<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>머신러닝 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/</link>
    <description>Recent content in 머신러닝 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Sat, 26 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>머신 러닝에서 벡터 행렬의 그래디언트</title>
      <link>https://freshrimpsushi.github.io/posts/gradient-for-vector-and-matrix-in-machine-learning/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gradient-for-vector-and-matrix-in-machine-learning/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 **스칼라 함수의 그래디언트: $$ \frac{ \partial f(\mathbf{w})}{ \partial \mathbf{w} } :=\nabla_{\mathbf{w}}f(\mathbf{w})=\begin{bmatrix} \frac{ \partial f(\mathbf{w})}{ \partial w_{1} },\frac{ \partial f(\mathbf{w})}{ \partial w_{2} },\cdots,\frac{ \partial f(\mathbf{w})}{ \partial w_{n} } \end{bmatrix}^{T} $$ 내적의 그래디언트Gradient of the inner product: $f(\mathbf{w})=\mathbf{w}^{T}\mathbf{x}$라고 하면 $$ \begin{align} \frac{ \partial f(\mathbf{w})}{ \partial \mathbf{w} } =\frac{ \partial (\mathbf{w}^{T}\mathbf{x})}{ \partial \mathbf{w} } &amp;amp;=\begin{bmatrix} \frac{ \partial \left( \sum _{i=1} ^{n} w_{i}x_{i}\right)}{ \partial</description>
    </item>
    
    <item>
      <title>머신 러닝에서 선형 회귀</title>
      <link>https://freshrimpsushi.github.io/posts/linear-regression-in-machine-learning/</link>
      <pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/linear-regression-in-machine-learning/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 머신 러닝에서 선형 회귀 란 주어진 작업에 대한 실제 함수를 가장 잘 근사하는 선형 함수를 찾는 알고리즘이다. 어떤 트레이닝 샘플 $D=\left\{ (\mathbf{x}_{1},y_{1}),\dots,(\mathbf{x}_{n},y_{n }) \right\}$가 있다고 하자. 이때 인풋 $x_{i}$에 대해서 아웃풋 $y_{i}$를 주는 함수를 $f$라고 하자. $$ f(\mathbf{x}_{i})=y_{i}\quad (i=1,\dots,n) $$ 즉 $f$는 모든 데이터 $\</description>
    </item>
    
    <item>
      <title>머신 러닝에서 벡터 행렬 표기법</title>
      <link>https://freshrimpsushi.github.io/posts/vector-and-matrix-notation-in-machine-learning/</link>
      <pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/vector-and-matrix-notation-in-machine-learning/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 선형대수를 잘 알지 못하거나, 잘 알아도 실제로 행렬 계산을 많이 해보지 않은 경우에 머신 러닝을 공부하면서 벡터와 행렬 표기법 때문에 힘들 수 있다. 해당 값이 스칼라인지, 벡터인지, 행렬인지 잘 구분해야하는데 실제로 손 계산을 해보면 익숙해지는데에 도움이 된다. 본 글의 표기법은 비숍의 &amp;lsqu</description>
    </item>
    
    <item>
      <title>로젠블렛의 단층 퍼셉트론</title>
      <link>https://freshrimpsushi.github.io/posts/rosenblatts-perceptron/</link>
      <pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/rosenblatts-perceptron/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 퍼셉트론이란? 퍼셉트론 은 1957년 로젠블렛에 의해 고안됐으며 최초의 지도 학습 모델이다. 위 그림1과 같이 단일 층으로 구성돼있고 활성화 함수 $\varphi$는 입력 데이터와 가중치의 곱을 $+1$ 혹은 $-1$로 반환한다. $-1$대신 $0$이라 두기도 하는데 본질적으로 차이는 없다. 간</description>
    </item>
    
    <item>
      <title>컴퓨터 비전이란</title>
      <link>https://freshrimpsushi.github.io/posts/computer-vision/</link>
      <pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/computer-vision/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 컴퓨터 비전 이란 주로 사람의 시각에 해당하는 기능을 컴퓨터가 수행할 수 있도록 하는 연구 분야이며 이미지나 영상을 다룬다. 컴퓨터 비전을 전문적으로 다루는 컨퍼런스로는 ICCV(International Conference on Computer Vision), ECCV(European Conference on Computer Vision), CVPR(Conference on Computer Vision and Pattern Recognition)등이 있다. 컴퓨터 비전에서 주로 다루는 문제는 아래의 사진과 같</description>
    </item>
    
    <item>
      <title>딥러닝에서 연속 학습이란</title>
      <link>https://freshrimpsushi.github.io/posts/continual-learning-lifelong-learning-incremental-learning/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/continual-learning-lifelong-learning-incremental-learning/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 딥러닝에서 연속 학습이란 평생 학습, 점진적 학습과 같은 말로서 인공 신경망이 순차적으로 여러 작업을 학습하는 것을 말한다. 인간의 경우 새로운 지식을 학습한다고 해서 기존의 지식을 잊어버리지 않는다. 물론 시간이 지나면 기존의 지식을 잊기도 하지만 이 원인이 새로운 지식을 학습했기 때문은 아니다.</description>
    </item>
    
    <item>
      <title>머신러닝에서 플루딩이란</title>
      <link>https://freshrimpsushi.github.io/posts/flooding-do-we-need-zero-training-loss-after-achieving-zero-training-error-review/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/flooding-do-we-need-zero-training-loss-after-achieving-zero-training-error-review/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 플루딩 은 ICML 2020에서 발표된 &amp;lsquo;Do We Need Zero Training Loss After Achieving Zero Training Error?&amp;lsquo;에서 소개한 레귤라이제이션의 한 종류이다. 이 논문의 저자는 오버 피팅이 일어나는 이유가 아래 그림과 같이 지나치게 작은 트레이닝 로스라고 말한다.따라서 아래 그림과 같이 학습 과정에서 트레이닝 로스가 특정한 값</description>
    </item>
    
    <item>
      <title>머신러닝에서 많이 쓰이는 데이터 셋</title>
      <link>https://freshrimpsushi.github.io/posts/popular-datasets-in-machine-learning/</link>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/popular-datasets-in-machine-learning/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 MNIST 머신 러닝을 공부할 때 가장 먼저 접할 데이터 셋이다. [엠니스트]라고 읽으며 $28\times 28$ 크기의 손글씨 사진 데이터이다. 학습 데이터 60,000개, 테스트 데이터 10,000개가 포함되어 있다1 2. CIFAR-10, CIFAR-100 CIFAR-10은 [싸이파-텐]이라고 읽으며, 10가지 카테고리 대한 $32\times 32$ 크기의 컬러</description>
    </item>
    
    <item>
      <title>머신러닝에서 레귤라이제이션이란</title>
      <link>https://freshrimpsushi.github.io/posts/regularization-in-machine-learning/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/regularization-in-machine-learning/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 트레이닝 로스가 아닌 테스트 로스를 줄이기 위해 알고리즘을 수정하는 모든 방법을 레귤라이제이션 이라 한다1Goodfellow defines regularization as &amp;ldquo;any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.&amp;rdquo; 즉, 오버피팅을 막기 위한 모든 방법을 묶어서 레귤라이제이션이라 한다. 따라서 딥러닝 교재에서 많이 소개되는</description>
    </item>
    
  </channel>
</rss>
