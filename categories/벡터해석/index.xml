<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>벡터해석 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EB%B2%A1%ED%84%B0%ED%95%B4%EC%84%9D/</link>
    <description>Recent content in 벡터해석 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Wed, 14 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EB%B2%A1%ED%84%B0%ED%95%B4%EC%84%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>전도함수: 다변수 벡터함수의 도함수</title>
      <link>https://freshrimpsushi.github.io/posts/total-derivative-of-multivariable-vector-function/</link>
      <pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/total-derivative-of-multivariable-vector-function/</guid>
      <description>빌드업1 일변수함수의 도함수의 정의를 떠올려보자. $$ \lim \limits_{h\to 0} \dfrac{f(x+h) - f(x)}{h} = f&#39;(x) $$ 여기서 좌변의 분자를 아래와 같이 $h$에 대한 선형함수로 근사하면 다음과 같다. $$ \begin{equation} f(x+h) - f(x) = a h + r(h) \label{1} \end{equation} $$ 여기서 $r(h)$는 다음과 같은 조건을 만족하는 나머지remainder, 잔차라고 하자. $$ \lim \limits_{h \to 0} \dfrac{r(h)}{h}=0 $$ 그러면 $\eqref{1}$의 양변을 $h</description>
    </item>
    
    <item>
      <title>델 연산자가 포함된 곱셈 규칙</title>
      <link>https://freshrimpsushi.github.io/posts/product-rule-with-del-operator/</link>
      <pubDate>Thu, 07 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/product-rule-with-del-operator/</guid>
      <description>공식 $f=f(x,y,z)$를 스칼라 함수라고 하자. $\mathbf{A} = A_{x}\hat{\mathbf{x}} + A_{y}\hat{\mathbf{y}} + A_{z}\hat{\mathbf{z}}, \mathbf{B} = B_{x}\hat{\mathbf{x}} + B_{y}\hat{\mathbf{y}} + B_{z}\hat{\mathbf{z}}$를 벡터 함수라고 하자. 그러면 다음의 식들이 성립한다. 그래디언트(기울기) (a) $\nabla{(fg)}=f\nabla{g}+g\nabla{f}$ (b) $\nabla(\mathbf{A} \cdot \mathbf{B}) = \mathbf{A} \times (\nabla \times \mathbf{B}) + \mathbf{B} \times (\nabla \times \mathbf{A})+(\mathbf{A} \cdot \nabla)\mathbf{B}+(\mathbf{B} \cdot \nabla) \mathbf{A}$ 다이벌전스(발산) (c) $\nabla \cdot (f\mathbf{A}) = f(\nabla \cdot \mathbf{A}) + \mathbf{A} \cdot (\nabla f)$ (d) $\nabla \cdot (\mathbf{A} \times \mathbf{B}) = \mathbf{B} \cdot (\nabla \times</description>
    </item>
    
    <item>
      <title>3차원 데카르트 좌표계에서 스칼라 함수의 라플라시안</title>
      <link>https://freshrimpsushi.github.io/posts/laplacian-of-scalar-function-in-cartesian-coordinate-system/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/laplacian-of-scalar-function-in-cartesian-coordinate-system/</guid>
      <description>정의 3차원 스칼라 함수 $f=f(x,y,z)$의 그래디언트의 다이벌전스를 $f$의 라플라시안Laplacian이라 하고 $\nabla^{2}$로 표기한다. $$ \nabla ^{2} f := \nabla \cdot(\nabla f)= \frac{ \partial^{2} f}{ \partial x^{2} }+\frac{ \partial^{2} f}{ \partial y^{2}}+\frac{ \partial^{2} f}{ \partial z^{2}} $$ 설명 라플라시안이라는 이름은 프랑스 수학자 라플라스 에서 따온 것이다. $\nabla^{2}$라는 표현은 편의를 위</description>
    </item>
    
    <item>
      <title>벡터 필드에서의 다이벌전스</title>
      <link>https://freshrimpsushi.github.io/posts/divergence-in-vector-field/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/divergence-in-vector-field/</guid>
      <description>정의 유클리드 공간에서 정의된 벡터 필드 $\textbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ 을 $\textbf{f} = (f_{1} , \cdots , f_{n})$ 과 같이 나타내고 축의 방향을 $u_{1} , \cdots , u_{n}$ 이라고 할 때, $\textbf{f}$ 의 다이벌전스 를 다음과 같이 정의한다. $$ \text{div} \textbf{f} := \nabla \cdot \textbf{f} = \sum_{k=1}^{n} {{ \partial f_{k} } \over { \partial u_{k} }} $$ 설명 벡터 필드의 다이벌전스는 다음과 다음과 같이 한 점 $\textbf{v} \in \mathbb{R}^{n}$ 가 주어져 있을 때 그 점에서 벡터들이 모이는지, 퍼지는지에 대한 하나의 척도가 된</description>
    </item>
    
    <item>
      <title>벡터 필드에서의 볼륨</title>
      <link>https://freshrimpsushi.github.io/posts/volume-in-vector-field/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/volume-in-vector-field/</guid>
      <description>정의 유클리드 공간의 부분공간 $D \subset \mathbb{R}^{n}$ 의 볼륨 $V$ 는 직교좌표 $\textbf{u} = (u_{1}, u_{2}, \cdots , u_{n})$ 으로 나타낼 때 다음과 같이 정의된다. $$ V(D) = \int_{D} du_{1} du_{2} \cdots d u_{n} $$ $\textbf{u} \in \mathbb{R}^{n}$ 가 벡터 함수 $\textbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ 에 의해 $\textbf{f} \left( \textbf{u} \right) = \left( f_{1} (\textbf{u}) , \cdots , f_{n} (\textbf{u}) \right)$ 와 같이 변환될 때, $D$ 의 볼륨 은 다음과 같다. $$ V(D) = \int_{D} \left| {{ \partial \textbf{f} (\textbf{u}) } \over { \partial \textbf{u} }} \right| d u_{1} d u_{2} \cdots d u_{n} $$ $\displaystyle \left| {{ \partial \textbf{f} (\textbf{u}) } \over { \partial \textbf{u} }} \right|$ 는 다음과 같이</description>
    </item>
    
    <item>
      <title>전미분, 완전미분</title>
      <link>https://freshrimpsushi.github.io/posts/total-differential/</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/total-differential/</guid>
      <description>정의 다변수 함수 $f : \mathbb{R}^{n} \to \mathbb{R}$가 주어졌다고 하자. 변수 $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n})$의 변화에 따른 $f(\mathbf{x})$의 변화를 다음과 같이 $df$로 표기하고 이를 $f$의 전미분total differential 혹은 완전미분exact differential이라 한다. $$ \begin{equation} df = \frac{ \partial f}{ \partial x_{1} }dx_{1} + \frac{ \partial f}{ \partial x_{2} }dx_{2} + \cdots + \frac{ \partial f}{</description>
    </item>
    
    <item>
      <title>3차원 데카르트 좌표계에서 벡터 함수의 다이벌전스(발산)</title>
      <link>https://freshrimpsushi.github.io/posts/divergence-of-fector-function-in-cartesian-cooridenates-system/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/divergence-of-fector-function-in-cartesian-cooridenates-system/</guid>
      <description>정의 벡터 함수 $\mathbf{F}(x,y,z)=F_{x}\hat{\mathbf{x}}+F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$에 대해서 다음과 같은 스칼라값을 $\mathbf{F}$ 다이벌전스divergence라고 정의하고 $\nabla \cdot \mathbf{F}$라고 표기한다. $$ \begin{equation} \nabla \cdot \mathbf{F} := \frac{ \partial F_{x}}{ \partial x} + \frac{ \partial F_{y}}{ \partial y }+ \frac{ \partial F_{z}}{ \partial z} \label{divergence} \end{equation} $$ 기하학적으로 $\nabla \cdot \mathbf{F}&amp;gt;0$이면 $\mathbf{F}$</description>
    </item>
    
    <item>
      <title>원통 좌표계의 변수로 r, 세타를 쓰면 안되는 이유</title>
      <link>https://freshrimpsushi.github.io/posts/reason-not-to-use-rtheta-as-a-variable-in-the-cylindrical-coordinate-system/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/reason-not-to-use-rtheta-as-a-variable-in-the-cylindrical-coordinate-system/</guid>
      <description>극좌표계와 원통좌표계의 차이 원통 좌표계는 아래와 같이 3차원 공간의 점을 $(\rho,\phi,z)$로 표현하는 좌표계를 말한다. 그런데 원통 좌표계를 $(r,\theta, z)$와 같이 표기한 것을 볼 수 있다. 극좌표계 $(r,\theta)$에서 높이 $z$가 추가되었으니 아무 생각 없이 $(r,\theta, z)$와 같이 표기한 것으로 보이는데 이는 각 기호의 의미를 생각</description>
    </item>
    
    <item>
      <title>3차원 데카르트 좌표계에서 스칼라 함수의 그래디언트(기울기)</title>
      <link>https://freshrimpsushi.github.io/posts/gradient-of-scalar-function-in-cartesian-coordinate-system/</link>
      <pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gradient-of-scalar-function-in-cartesian-coordinate-system/</guid>
      <description>정의 3변수 스칼라 함수 $f=f(x,y,z)$의 그래프의 각 점에서 기울기와 증가하는 방향을 나타내는 벡터를 $\nabla f$라고 표기하며 그래디언트gradient라고 부른다. $$ \mathrm{grad}f=\nabla f = \frac{ \partial f}{ \partial x }\hat{\mathbf{x}}+\frac{ \partial f}{ \partial y}\hat{\mathbf{y}}+\frac{ \partial f}{ \partial z}\hat{\mathbf{z}} $$ 설명 그래디언트는 기울기, 구배, 물매 등으로 번역된다. 구매, 물매는 그래디언트의 옛날식 번역이고 최근에는 잘 쓰이지 않는</description>
    </item>
    
    <item>
      <title>구좌표계에서의 미소부피</title>
      <link>https://freshrimpsushi.github.io/posts/volume-in-polar-coordinates/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/volume-in-polar-coordinates/</guid>
      <description>공식 구좌표계에서 미소 부피는 아래와 같다. $$ dV=r^{2}\sin\theta dr d\theta d\phi $$ 구 표면 위의 미소 면적은 $dr$을 곱하지 않음으로써 얻을 수 있다. $$ da=\color{blue}{rd\theta} \cdot \color{red}{r\sin\theta d \phi}=r^{2}\sin\theta d\theta d\phi $$ 설명 그림을 통한 이해 구좌표계에서 미소부피는 위 그림에서 보이는 바와 같이 (초록선의 길이)$\times$(파란선의 길이)$\times$(빨간선의 길이)로 나타남을 알 수 있다. 원점에서 세</description>
    </item>
    
    <item>
      <title>극 좌표계에서 미소 면적 원통 좌표계에서 미소 부피</title>
      <link>https://freshrimpsushi.github.io/posts/volume-in-cylindrical-coordinates/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/volume-in-cylindrical-coordinates/</guid>
      <description>공식 극 좌표계에서 미소 면적은 다음과 같다. $$ dA=rdrd\theta $$ 원통 좌표계에서 미소 부피와 원통 표면의 미소 면적은 다음과 같다. $$ dV=\rho d\rho d\phi dz \\ dA=\rho d\phi dz $$ 설명 극 좌표계 $\mathbf{r}=\mathbf{r}(r,\theta)$ 미소 면적은 그림에서와 같이 (초록선의 길이)$\times$(파란선의 길이)이다. 초록색 선은 지름 방향의 미소 변화량이므로 $\color{green}{dr}$이다. 파란색 선</description>
    </item>
    
    <item>
      <title>3차원 데카르트 좌표계에서 벡터 함수의 컬(회전)</title>
      <link>https://freshrimpsushi.github.io/posts/curl-of-vector-function-in-cartesian-coordinate-system/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/curl-of-vector-function-in-cartesian-coordinate-system/</guid>
      <description>정의 3차원 벡터 $\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\hat{\mathbf{x}} + F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$에 대해서 다음과 같은 벡터를 $\mathbf{F}$의 컬curl이라 정의하고 $\nabla \times \mathbf{F}$라고 표기한다. $$ \begin{align} \nabla \times \mathbf{F} &amp;amp;= \left( \dfrac{ \partial F_{z}}{ \partial y }-\dfrac{ \partial F_{y}}{ \partial z} \right)\hat{\mathbf{x}}+ \left( \dfrac{ \partial F_{x}}{ \partial z }-\dfrac{ \partial F_{z}}{ \partial x} \right)\hat{\mathbf{y}}+ \left( \dfrac{ \partial F_{y}}{ \partial x }-\dfrac{ \partial F_{x}}{ \partial y} \right)\hat{\mathbf{z}} \label{def1} \\ &amp;amp;=\begin{vmatrix} \hat{\mathbf{x}} &amp;amp; \hat{\mathbf{y}} &amp;amp; \hat{\mathbf{z}} \\ \dfrac{ \partial }{ \partial x} &amp;amp; \dfrac{ \partial }{ \partial</description>
    </item>
    
    <item>
      <title>두 벡터의 외적의 크기는 두 벡터가 만드는 평행사변형의 넓이와 같다</title>
      <link>https://freshrimpsushi.github.io/posts/properties-of-outer-product/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/properties-of-outer-product/</guid>
      <description>정리 두 벡터 $\mathbf{A}$, $\mathbf{B}$ 사이의 각도가 $\theta$일 때 두 벡터의 외적의 크기는 다음과 같다. $$ \left| \mathbf{A}\times \mathbf{B}\right| =\left|\mathbf{A}\right|\left| \mathbf{B} \right|\sin \theta $$ 그리고 이는 두 벡터가 만드는 평행사변형의 넓이와 같다. 증명 두 벡터 $\mathbf{A}=(A_{x},A_{y},A_{z})$, $\mathbf{B}=(B_{x},B_{y},B_{z})$가 위 그림과 같다고 하자. 그러면 part 1. 평행사변형의 넓이 평행사변형의 넓이는 밑변과 높이의 곱</description>
    </item>
    
    <item>
      <title>물리학에서 델 연산자란</title>
      <link>https://freshrimpsushi.github.io/posts/del-operator-in-physics/</link>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/del-operator-in-physics/</guid>
      <description>설명 물리학에서 연산자는 함수를 함수에 대응시키는 함수를 말한다. 그 중에서도 델 연산자del operator란 어떤 함수가 주어졌을 때, 그 함수의 도함수를 함숫값으로 가지는 함수이다. 연산자라는 말이 생소하다면 그냥 대상을 계산하는 규칙이라고 이해하면 된다. 예를 들어 $\dfrac{d}{dx}$라는 함수에 $f$를 집어넣으면 $f</description>
    </item>
    
    <item>
      <title>스칼라 필드의 그래디언트</title>
      <link>https://freshrimpsushi.github.io/posts/gradient-in-scalar-field/</link>
      <pubDate>Sun, 28 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gradient-in-scalar-field/</guid>
      <description>정의 스칼라 필드 $f : \mathbb{R}^{n} \to \mathbb{R}$ 에 대해 한 점 $\mathbb{x}_{0} \in \mathbb{R}^{n}$ 에서 함수의 증가율이 가장 큰 방향을 나타낸 벡터를 $\mathbb{x}_{0}$ 에서 $f$ 의 그래디언트 라고 한다. 설명 스칼라장의 예시로써 위의 그림을 생각해보자. 위 그림은 $z(x,y) = x^2 - y^2$ 와 같이 정의된 함수 $z : \mathbb{R}^{2} \to \mathbb{R}$ 을 시각적으로 나타낸 것이다. $y= f(x)$ 와 같은 곡선에서는 변화율을 구할만할 방향이 $x$ 축 뿐이었기 때문에 접선의 기울기를</description>
    </item>
    
    <item>
      <title>헤세 행렬이란</title>
      <link>https://freshrimpsushi.github.io/posts/hessian-matrix/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/hessian-matrix/</guid>
      <description>정의 $D \subset \mathbb{R}^{n}$ 에서 정의된 다변수 스칼라 함수 $f : D \to \mathbb{R}$ 에 대해 다음과 같은 행렬 $H \in \mathbb{R}^{n \times n}$ 을 $f$ 의 헤세 행렬이라 한다. $$ H := \begin{bmatrix} {{\partial^2 f } \over {\partial x_{1}^2 }} &amp;amp; \cdots &amp;amp; {{\partial^2 f } \over { \partial x_{1} \partial x_{n} }} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ {{\partial^2 f } \over {\partial x_{n} \partial x_{1} }} &amp;amp; \cdots &amp;amp; {{\partial^2 f_{m} } \over {\partial x_{n}^2 }} \end{bmatrix} $$ 설명 야코비 행렬이 함수의 고차원적인 도함수에 해당한다면, 헤세 행렬은 고차원적인 이계도함수라고 볼 수 있다.</description>
    </item>
    
    <item>
      <title>야코비 행렬 혹은 자코비 행렬이란</title>
      <link>https://freshrimpsushi.github.io/posts/jacobian-matrix/</link>
      <pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/jacobian-matrix/</guid>
      <description>정의 $D \subset \mathbb{R}^{n}$ 에서 정의된 다변수 벡터 함수 $\mathbb{f} : D \to \mathbb{R}^{m}$ 가 각각의 스칼라 함수 $f_{1} , \cdots , f_{m} : D \to \mathbb{R}$ 에 대해 $$ \mathbb{f} ( x_{1} , \cdots , x_{n} ) : = \begin{bmatrix} f_{1} ( x_{1} , \cdots , x_{n} ) \\ \vdots \\ f_{m} ( x_{1} , \cdots , x_{n} ) \end{bmatrix} $$ 과 같이 정의되었다고 하자. $$ J := \begin{bmatrix} {{\partial f_{1} } \over {\partial x_{1} }} &amp;amp; \cdots &amp;amp; {{\partial f_{1} } \over {\partial x_{n} }} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ {{\partial f_{m} } \over {\partial x_{1} }} &amp;amp; \cdots &amp;amp; {{\partial f_{m} } \over {\partial x_{n} }} \end{bmatrix} $$ 을 $\mathbb{f}$ 의 야코비 행렬이라 한다. 설명</description>
    </item>
    
    <item>
      <title>스칼라 함수와 벡터 함수</title>
      <link>https://freshrimpsushi.github.io/posts/scalar-function-and-vector-function/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/scalar-function-and-vector-function/</guid>
      <description>정의 $D\subset \mathbb{R}^{n}$ 이라고 하자. 1. $D$ 를 정의역으로 갖는 함수를 다변수 함수라 한다. 2. $f : D \to \mathbb{R}$ 을 스칼라 함수라 한다. 3. 스칼라 함수 $f_{1} , \cdots , f_{m} : D \to \mathbb{R}$ 에 대해 $$ \mathbb{f} ( x_{1} , \cdots , x_{n} ) : = \begin{bmatrix} f_{1} ( x_{1} , \cdots , x_{n} ) \\ \vdots \\ f_{m} ( x_{1} , \cdots , x_{n} ) \end{bmatrix} $$ 과 같이 정의된 $\mathbb{f} : D \to \mathbb{R}^{m}$ 를 벡터 함수라 한다. 설명 1. 다변수 함수라는 표현은 특히 미적분학을 위시한 해석학에서 쓰는 표</description>
    </item>
    
    <item>
      <title>델 연산자가 포함된 식의 부분적분</title>
      <link>https://freshrimpsushi.github.io/posts/integral-by-part-with-del-operator/</link>
      <pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/integral-by-part-with-del-operator/</guid>
      <description>공식 델 연산자가 포함된 벡터 적분에 대해서 다음의 식이 성립한다. (a) $$ \int_{\mathcal{V}}\mathbf{A} \cdot (\nabla f)d\tau = \oint_{\mathcal{S}}f\mathbf{A} \cdot d \mathbf{a}-\int_{\mathcal{V}}f(\nabla \cdot \mathbf{A})d\tau $$ (b) $$ \int_{\mathcal{S}} f \left( \nabla \times \mathbf{A} \right)\mathbf{A} \cdot d \mathbf{a} = \int_{\mathcal{S}} \left[ \mathbf{A} \times \left( \nabla f \right) \right] \cdot d\mathbf{a} + \oint_{\mathcal{P}} f\mathbf{A} \cdot d\mathbf{l} $$ (c) $$ \int_{\mathcal{V}} \mathbf{B} \cdot \left( \nabla \times \mathbf{A} \right) d\tau = \int_{\mathcal{V}} \mathbf{A} \cdot \left( \nabla \times \mathbf{B} \right) d\tau + \oint_{\mathcal{S}} \left( \mathbf{A} \times \mathbf{B} \right) \cdot d \mathbf{a} $$ 설명 부분적분은 어떤 함수$(f\ or\ \mathbf{A}$)와 어떤 함수의 도함수$(\nabla f\</description>
    </item>
    
    <item>
      <title>스토크스 정리</title>
      <link>https://freshrimpsushi.github.io/posts/stokes-theorem-for-physics/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/stokes-theorem-for-physics/</guid>
      <description>정리1 $\mathbf{v}, \mathcal{S}$를 각각 3차원 공간에서 어떤 벡터, 면적이라고 하자. $\mathcal{S}$의 면적 벡터를 $d\mathbf{a}$, $\mathcal{S}$의 테두리를 $\mathcal{P}$, $\mathcal{P}$를 따라 움직이는 경로를 $d\mathbf{l}$이라고 하자. 그러면 다음의 식이 성립한다. $$ \int_{\mathcal{S}} (\nabla \times \mathbf{v} )\cdot d\mathbf{a} = \oint_{\mathcal{P}} \mathbf{v} \cdot d\mathbf{l} $$ 이를 스토</description>
    </item>
    
    <item>
      <title>분리벡터의 회전</title>
      <link>https://freshrimpsushi.github.io/posts/curl-of-separation-vector-1r2/</link>
      <pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/curl-of-separation-vector-1r2/</guid>
      <description>공식 $\nabla \times \dfrac{\hat{\boldsymbol{\eta}} }{\eta ^2} =0$ 설명 이 식이 특별한 의미를 가지는 것은 아니다. 자기장의 발산을 구하는 과정에서 나오는데 계산이 간단하지 않아 따로 설명한다. 증명 $\boldsymbol{\eta}=(x-x&#39;)\hat{\mathbf{x}} + (y-y&#39;)\hat{\mathbf{y}} + (z-z&#39;)\hat{\mathbf{z}}$를 분리벡터라고 하면 다음과 같다. $$ | \boldsymbol{\eta} |=\eta=\sqrt{(x-x&#39;)^2+(y-y&#39;)^2 + (z-z&#39;)^2} $$ $$ \hat{ \boldsymbol{\eta}}=\dfrac{ \boldsymbol{\eta} } { \eta}=\dfrac{(x-x&#39;)\hat{\mathbf{x}} + (y-y&#39;)\hat{\mathbf{y}} + (z-z&#39;)\hat{\mathbf{z}}}{\sqrt{(x-x&#39;)^2+(y-y&#39;)^2 + (z-z&#39;)^2}} $$ $$ \dfrac{\hat{\boldsymbol{\eta}}}{\eta^2}=\dfrac{1}{(x-x&#39;)^2+(y-y&#39;)^2 + (z-z&#39;)^2}\dfrac{(x-x&#39;)\hat{\mathbf{x}} + (y-y&#39;)\hat{\mathbf{y}} + (z-z&#39;)\hat{\mathbf{z}}}{\sqrt{(x-x&#39;)^2+(y-y&#39;)^2 + (z-z&#39;)^2}} $$ 계산의 편의를 위해 $$ \dfrac{\hat{\boldsymbol{\eta}} }{\eta ^2}</description>
    </item>
    
    <item>
      <title>델 연산자가 두 번 들어간 수식, 2계 도함수</title>
      <link>https://freshrimpsushi.github.io/posts/second-derivative-with-del-operator/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/second-derivative-with-del-operator/</guid>
      <description>$T$를 스칼라 함수, $\mathbf{A}$를 벡터 함수라고하자. 그래디언트의 다이벌전스: $\nabla \cdot (\nabla T) = \dfrac{\partial^{2} T}{\partial x^{2}} + \dfrac{\partial ^{2} T} {\partial y^{2}} + \dfrac{\partial ^{2} T}{\partial z^{2}}$ 그래디언트의 컬: $\nabla \times (\nabla T)=0$ 다이벌전스의 그래디언트: $\nabla (\nabla \cdot \mathbf{A} )$ 컬의 다이벌전스: $\nabla \cdot (\nabla \times \mathbf{A})=0$ 컬의 컬: $\nabla \times (\nabla \times \mathbf{A})=\nabla ( \nabla \cdot \mathbf{A}) - \nabla ^{2} \mathbf{A}$ 설명 그래디언트와 컬의 결과가 벡터이고, 다이벌전스의 결과가 스</description>
    </item>
    
    <item>
      <title>헤비사이드 계단 함수를 미분하면 디랙 델타 함수가 됨을 증명</title>
      <link>https://freshrimpsushi.github.io/posts/derivative-of-heaviside-function-is-dirac-delta-function/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/derivative-of-heaviside-function-is-dirac-delta-function/</guid>
      <description>정리 헤비사이드 계단 함수의 도함수는 디랙 델타 함수이다. $$ \dfrac{dH}{dx}=\delta (x) $$ 이때 $H=H(x)$는 헤비사이드 계단 함수Heaviside step function 혹은 단위 계단 함수unit step function $$ H(x)=\begin{cases} 1 &amp;amp; x&amp;gt;0 \\ 0 &amp;amp; x \le 0 \end{cases} $$ 디랙 델타 함수 아래의 두 조건을 만족하는 함수를 디랙 델타 함수라고 한다. $$ \begin{equation} \delta (x) = \begin{cases} 0, &amp;amp; x\neq 0 \\ \infty , &amp;amp; x=0 \end{cases} \label{condition1} \end{equation} $$ $$ \begin{equation} \int_{-\infty}^{\infty}{\delta (x) dx}=1 \label{condition2} \end{equation} $$ 증명 $\d</description>
    </item>
    
    <item>
      <title>분리벡터의 발산</title>
      <link>https://freshrimpsushi.github.io/posts/the-divergence-of-1r2/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/the-divergence-of-1r2/</guid>
      <description>공식 $\nabla \cdot \left( \dfrac{1}{r^2}\hat{ \mathbf{r} } \right) = 4\pi \delta^3(\mathbf{r})$ $\nabla \cdot \left( \dfrac{1}{\eta^2}\hat{ \boldsymbol{\eta} } \right) = 4\pi \delta^3(\boldsymbol{\eta})$ $\nabla^2 \left(\dfrac{1}{\eta} \right) =-4\pi \delta^3 ( \mathbf{r} )$ 설명 벡터 함수 $\mathbf{v} = \dfrac{1}{r^2}\hat{\mathbf{r}}$이 있다고 하자. 크기는 거리 제곱에 반비례하고 방향은 반지름 방향이다.이제부터 이 함수의 발산을 계산해보자. 구좌표계에서의 기울기 공식을 사용하면 $$ \nabla \cdot \mathbf{v} = \dfrac{1}{r^2}\dfrac{\partial}{\partial r}\left( r^2\dfrac{1}{r^2} \right) = \dfrac{1}{r^2}\dfrac{\partial}{\partial r}(1)=0 $$ 그런데</description>
    </item>
    
    <item>
      <title>기울기의 기본 정리</title>
      <link>https://freshrimpsushi.github.io/posts/fundamental-theorem-for-gradient/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/fundamental-theorem-for-gradient/</guid>
      <description>정리 $T$를 3차원 스칼라 함수라고 하자. $a, b$를 3차원 공간상의 임의의 점이라고 하자. $a$에서 점 $b$로 가는 임의의 경로에 따른 $T$의 총 변화량은 다음과 같다. $$ \begin{equation} T(b)-T(a) = \int _{a}^{b} (\nabla T) \cdot d\mathbf{l} \label{1} \end{equation} $$ 이를 기울기의 기본 정리fundamental theorem for gradients 혹은 기울기 정리gradients theorem라고 한다. 참고로 생새우초밥집에</description>
    </item>
    
    <item>
      <title>파푸스-굴딘 정리 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-pappus-guldinus-theorem/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-pappus-guldinus-theorem/</guid>
      <description>정리 $yz$-평면 상의 도형 $F$ 의 넓이를 $A$ 라고 하고 $F$ 를 $z$-축으로 회전시켜서 얻은 회전체 $W$ 의 부피를 $V$ 라고 하자. $z$-축과 $F$ 의 무게중심 사이의 거리를 $r$ 이라고 하면 $$ V = 2 \pi r A $$ 설명 파푸스-굴딘 정리는 고등학교 수준으로는 증명할 수 없지만 회전체에 대해 배울때 선생님들이 심심찮게 언급하는 정리다. 막상 학부수준의 수학을 공부</description>
    </item>
    
    <item>
      <title>유사벡터란</title>
      <link>https://freshrimpsushi.github.io/posts/pseudovector/</link>
      <pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/pseudovector/</guid>
      <description>설명 물리학 공부를 하다 보면 유사벡터 혹은 수도벡터라는 말을 접할 수 있다. 중요한 점은 유사벡터를 접하기만 할 뿐 어떤 녀석인지 알기는 힘들다는 거다. 유사벡터가 뭔지 몰라도 학부 물리학을 공부하는데 아무 지장은 없다지만 제대로 설명해놓은 교재를 본 적이 없다. 나는 유사벡터의 특징을 배울 수 있도록 한 그리피스 전자기학의 연습문제에서 준벡터(Pse</description>
    </item>
    
    <item>
      <title>가우스 정리, 발산 정리</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-gauss-theorem-divergence-theorem/</link>
      <pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-gauss-theorem-divergence-theorem/</guid>
      <description>정리1 3차원 $$ \begin{equation} \int_\mathcal{V} \nabla \cdot \mathbf{ F} dV = \oint _\mathcal{S} \mathbf{F} \cdot d \mathbf{S} \label{1} \end{equation} $$ 이를 가우스 정리Gauss&amp;rsquo;s theorem, 그린 정리Green&amp;rsquo;s theorem, 혹은 발산 정리divergence theorem라고 한다. 설명1 발산 정리는 특히 전자기학에서 많이 사용된다. 수식적 의미 수식적으로는 면적분을 부피적분으로, 부피적분을 면적분으로 바꾸어 표</description>
    </item>
    
    <item>
      <title>직교좌표계에서의 벡터, 내적, 외적의 미분</title>
      <link>https://freshrimpsushi.github.io/posts/derivative-of-a-vector-in-cartesian-coordinate/</link>
      <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/derivative-of-a-vector-in-cartesian-coordinate/</guid>
      <description>공식 $\mathbf{A} = A_{x}\hat{\mathbf{x}} + A_{y}\hat{\mathbf{y}} + A_{z}\hat{\mathbf{z}}, \mathbf{B} = B_{x}\hat{\mathbf{x}} + B_{y}\hat{\mathbf{y}} + B_{z}\hat{\mathbf{z}}$를 3차원 직교좌표계에서의 벡터라고 하자. $n$을 임의의 스칼라라고 하자. 그러면 다음의 식이 성립한다. (a) $\dfrac{ d \left( n \mathbf{A} \right) }{dt} = \dfrac{ dn }{dt} \mathbf{A} + n\dfrac{ d\mathbf{A}}{dt}$ (b) $\dfrac{ d ( \mathbf{A} \cdot \mathbf{B} )}{dt} = \dfrac{ \mathbf{A} }{dt} \cdot \mathbf{B} + \mathbf{A} \cdot \dfrac{ d\mathbf{B}}{dt}$ (c) $\dfrac{ d ( \mathbf{A} \times \mathbf{B}) }{dt} = \dfrac{ d \mathbf{A} } {dt} \times \mathbf{B} + \mathbf{A} \times \dfrac{ d \mathbf{B} } {dt}$ 설명 고등학생 때</description>
    </item>
    
    <item>
      <title>직교 원통 구면 좌표계에 대한 기울기 발산 회전 라플라스 연산</title>
      <link>https://freshrimpsushi.github.io/posts/del-operator-in-several-coordinates/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/del-operator-in-several-coordinates/</guid>
      <description>설명 직교좌표계, 원통좌표계, 구면좌표계에서의 그래디언트, 다이벌전스, 컬, 라플라시안을 한데 모아 정리했다. 이렇게 정리한 이유는 보고 암기하라는 것이 아니라 갑자기 필요한데 생각이 안나거나 할 때 검색해서 보고 참고하라고 정리한 것이다. 굳이 외우려하지 않아도 어차피 공부 열심히하다보면 외워진다. $f$는 스칼라 함수, $\mathbf A$는 벡터 함</description>
    </item>
    
    <item>
      <title>직교좌표계 단위벡터를 구면좌표계의 단위벡터로 나타내기</title>
      <link>https://freshrimpsushi.github.io/posts/cartesian-coordinate-system-unit-vectors-and-spherical-coordinate-system-unit-vectors/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/cartesian-coordinate-system-unit-vectors-and-spherical-coordinate-system-unit-vectors/</guid>
      <description>공식 직교좌표계의 단위벡터를 구면좌표계의 단위벡터로 나타낸 식은 아래와 같다. $$ \begin{align*} \hat{ \mathbf{x} }&amp;amp;= \cos \phi \sin \theta \hat{ \mathbf{r} } + \cos \phi \cos \theta \hat{ \boldsymbol{\theta} } - \sin\phi\hat{ \boldsymbol{\phi} } \\ \hat{ \mathbf{y} } &amp;amp;= \sin\phi\sin\theta \hat{ \mathbf{r} } + \sin\phi\cos\theta\hat{ \boldsymbol{\theta} } + \cos\phi\hat{ \boldsymbol{\phi} } \\ \hat{ \mathbf{z} } &amp;amp;= \cos\theta\hat{ \mathbf{r} } - \sin\theta\hat{ \boldsymbol{\theta} } \end{align*} $$ 구면좌표계의 단위벡터를 직교좌표계의 단위벡터로 나타내면 아래와 같다. (구면좌표계와 직교좌표계의 관계) $$ \begin{align*} \hat{ \mathbf{r} } &amp;amp;= \cos\phi \sin\theta \hat{ \mathbf{x} } +</description>
    </item>
    
    <item>
      <title>삼차원 유클리드 공간에서 외적이란</title>
      <link>https://freshrimpsushi.github.io/posts/outer-product-in-three-dimension/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/outer-product-in-three-dimension/</guid>
      <description>정의 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^3$ 에 대해 다음을 $\mathbf{x}$와 $\mathbf{y}$ 의 외적 으로 정의한다. $$ \begin{align*} \mathbf{x} \times \mathbf{y} =&amp;amp; (x_{2}y_{3} - x_{3}y_{2}, x_{3}y_{1} - x_{1}y_{3}, x_{1}y_{2} - x_{2}y_{1}) \\ =&amp;amp; \det \begin{bmatrix} \mathbf{i} &amp;amp; \mathbf{j} &amp;amp; \mathbf{k} \\ x_{1} &amp;amp; x_{2} &amp;amp; x_{3} \\ y_{1} &amp;amp; y_{2} &amp;amp; y_{3} \end{bmatrix} \\ =&amp;amp; \begin{bmatrix} 0 &amp;amp; -x_{3} &amp;amp; x_{2} \\ x_{3} &amp;amp; 0 &amp;amp; -x_{1} \\ -x_{2} &amp;amp; x_{1} &amp;amp; 0 \end{bmatrix} \begin{bmatrix} y_{1} \\ y_{2} \\ y_{3} \end{bmatrix} \end{align*} $$ 설명 참고로 $\mathbf{i} = (1,0,0)$ , $ \mathbf{j} = (0,1,0)$ , $\mathbf{k} = (0,0,1)$ 이다. 내적과 마찬가지로 외적 역시 더욱 일반적인 정의는 가능하지만 실</description>
    </item>
    
    <item>
      <title>유클리드 공간에서 내적이란</title>
      <link>https://freshrimpsushi.github.io/posts/inner-product-in-euclidean-space/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/inner-product-in-euclidean-space/</guid>
      <description>정의 벡터공간 $V = \mathbb{R}^n$ 에 대해 $\mathbb{x}, \mathbb{y}, \mathbb{z} \in V$ 그리고 $k \in \mathbb{R}$ 이라고 하자. $\left&amp;lt; \cdot , \cdot \right&amp;gt; : V^2 \to \mathbb{R}$ 가 아래 네 조건들을 만족시킬 때 $\left&amp;lt; \cdot , \cdot \right&amp;gt;$ 를 $V$ 상에서의 내적 으로 정의한다. (1) 대칭성: $\left&amp;lt; \mathbb{x} , \mathbb{y} \right&amp;gt; = \left&amp;lt; \mathbb{y}, \mathbb{x} \right&amp;gt;$ (2) 가산성: $\left&amp;lt; \mathbb{x} + \mathbb{y} , \mathbb{z} \right&amp;gt; = \left&amp;lt; \mathbb{x}, \mathbb{z} \right&amp;gt; + \left&amp;lt; \mathbb{y}, \mathbb{z} \right&amp;gt;$ (3) 동질성: $\left&amp;lt; k \mathbb{x} , \mathbb{y} \right&amp;gt; = k \left&amp;lt; \mathbb{x}, \mathbb{y} \right&amp;gt;$ (4) 정부호: $\left&amp;lt; \mathbb{x} , \mathbb{x} \right&amp;gt; \ge 0$ 그리고 $\left&amp;lt; \mathbb{x} , \mathbb{x} \right&amp;gt; =0 \iff \mathbb{x}=\mathbb{0}$ 특히</description>
    </item>
    
    <item>
      <title>컬의 다이벌전스는 항상 0이다</title>
      <link>https://freshrimpsushi.github.io/posts/proof-that-divergence-of-curl-is-always-0/</link>
      <pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-that-divergence-of-curl-is-always-0/</guid>
      <description>공식 벡터 함수 $\mathbf{A}$의 컬의 다이벌전스는 항상 $0$이다. $$ \nabla \cdot (\nabla \times \mathbf{A}) = 0 $$ 증명 $\mathbf{A}$의 컬은 다음과 같다. $$ \begin{align*} \nabla \times \mathbf{A} &amp;amp;= \begin{vmatrix} \hat{\mathbf{x}} &amp;amp; \hat{\mathbf{y}} &amp;amp; \hat{\mathbf{z}} \\ \displaystyle \frac{\partial}{\partial x} &amp;amp; \displaystyle \frac{\partial}{\partial y} &amp;amp; \displaystyle \frac{\partial}{\partial z} \\ A_{x} &amp;amp; A_{y} &amp;amp; A_{z} \end{vmatrix} \\ &amp;amp;= \hat{\mathbf{x}} \left( \frac{\partial A_{z}}{\partial y} - \frac{\partial A_{y}}{\partial z} \right) + \hat{\mathbf{y}} \left( \frac{\partial A_{x}}{\partial z} - \frac{ \partial A_{z}}{\partial x} \right) + \hat{\mathbf{z}} \left( \frac{\partial A_{y}}{\partial x}-\frac{\partial A_{x}}{\partial y} \right) \end{align*} $$ 어떤 벡터 함수 $\mathbf{F}</description>
    </item>
    
    <item>
      <title>벡터 함수의 컬의 컬</title>
      <link>https://freshrimpsushi.github.io/posts/the-curl-of-curl-of-vector-function/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/the-curl-of-curl-of-vector-function/</guid>
      <description>공식 벡터 함수의 컬의 컬은 다음과 같다. $$ \nabla \times (\nabla \times \mathbf{A}) = \nabla (\nabla \cdot \mathbf{A}) - \nabla^2 \mathbf{A} $$ 설명 첫번째 항인 $\nabla(\nabla \cdot \mathbf{A})$는 다이벌전스의 그래디언트이며 따로 붙여진 이름은 없다. 두번째 항은 중요해서 이름이 있다. $\nabla \cdot \nabla$를 라플라시안이라 하는데, 정확하게는 벡터 함수의 라플라시안이다. 컬의 컬에 특별한 의미가 있는 것은 아니고,</description>
    </item>
    
    <item>
      <title>그래디언트의 컬은 항상 0이다</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-that-the-curl-of-a-gradient-is-always-0/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-that-the-curl-of-a-gradient-is-always-0/</guid>
      <description>공식 스칼라 함수 $T$의 그래디언트의 컬은 항상 $\mathbf{0}$이다 $$ \nabla \times (\nabla T)=0 $$ 증명 직교 좌표계에서 $T$의 그래디언트는 다음과 같다. $$ \nabla T = \frac{\partial T}{\partial x}\hat{\mathbf{ x}} +\frac{\partial T}{\partial y}\hat{\mathbf{y}} +\frac{\partial T}{\partial z}\hat{\mathbf{z}} $$ $\nabla T$의 컬을 구하면 다음과 같다. $$ \begin{align*} \nabla \times (\nabla T) &amp;amp;= \begin{vmatrix} \displaystyle \hat{\mathbf{x}} &amp;amp;\hat{\mathbf{y}} &amp;amp; \hat{\mathbf{z}} \\ \displaystyle \frac{\partial }{\partial x} &amp;amp; \displaystyle \frac{\partial }{\partial y} &amp;amp; \displaystyle \frac{\partial }{\partial z} \\ \displaystyle \frac{\partial T}{\partial x} &amp;amp; \displaystyle \frac{\partial T}{\partial y} &amp;amp; \displaystyle\frac{\partial T}{\partial z} \end{vmatrix} \\ &amp;amp;= \left( \frac{\partial^2 T}{\partial y \partial z}-\frac{\partial^2 T}{\partial z \partial</description>
    </item>
    
    <item>
      <title>유클리드 공간이란</title>
      <link>https://freshrimpsushi.github.io/posts/what-is-an-euclidean-space/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/what-is-an-euclidean-space/</guid>
      <description>정의 자연수 $n \in \mathbb{N}$ 에 대해 실수 집합 $\mathbb{R}$ 의 데카르트 곱 $\mathbb{R}^{n}$ 을 유클리드 공간 이라고 한다. $$ \mathbb{R}^{n} = \mathbb{R} \times \cdots \times \mathbb{R} $$ $\mathbb{R}^{1}$ 을 실수 공간 혹은 수직선 이라고 한다. $\mathbb{R}^{2}$ 을 평면 이라고 한다. $\mathbb{R}^{3}$ 을 $3$차원 공간 이라고 한다. 이때 $\mathbb{N} := \left\{ 1, 2, 3, \cdots \right\}$은 자연수를 모두 모아놓은 집합을 의미한다. $\mathbb{R}$ 은 실수를 모두 모아놓은 집합을 의미한다. 설명 유클리드 공</description>
    </item>
    
    <item>
      <title>직교좌표계의 단위벡터로 표현한 구면좌표계의 단위벡터</title>
      <link>https://freshrimpsushi.github.io/posts/relationship-between-cartesian-coordinate-system-unit-vectors-and-spherical-coordinate-system-unit-vectors/</link>
      <pubDate>Sun, 16 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/relationship-between-cartesian-coordinate-system-unit-vectors-and-spherical-coordinate-system-unit-vectors/</guid>
      <description>구면좌표계의 단위벡터 $$ \begin{align*} \hat{\mathbf{r}} &amp;amp;= \cos\phi \sin\theta\hat{\mathbf{x}} + \sin\phi \sin\theta\hat{\mathbf{y}} + \cos\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\theta}} &amp;amp;= \cos\phi \cos\theta \hat{\mathbf{x}} + \sin\phi \cos\theta \hat{\mathbf{y}} - \sin\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\phi}} &amp;amp;= -\sin\phi \hat{\mathbf{x}} + \cos\phi \hat{\mathbf{y}} \end{align*} $$ 유도 $\hat{\mathbf{r}}$을 먼저 구한 뒤 이를 이용해서 나머지 둘을 구한다. 반지름 방향 단위벡터 $\hat{\mathbf{r}}$ $$ \hat{\mathbf{r}}=r\hat{\mathbf{r}}=x\hat{\mathbf{x}}+y\hat{\mathbf{y}}+z\hat{\mathbf{z}} $$ 이므로 양변을 $r$로 나누면 다음과 같다. $$ \begin{align*} \hat{\mathbf{r}}&amp;amp;=\frac{x}{r}\hat{\mathbf{x}}+\frac{y}{r}\hat{\mathbf{y}}+\frac{z}{r}\hat{\mathbf{z}} \\ &amp;amp;= \frac{x}{r \sin\theta}\sin\theta\hat{\mathbf{x}}+\frac{y}{r \sin\theta}\sin\theta\hat{\mathbf{y}}+\cos\theta\hat{\mathbf{z}} \\ &amp;amp;= \cos\phi \sin\theta \hat{\mathbf{x}} + \sin\phi \sin\theta\hat{\mathbf{y}} + \cos\theta\hat{\mathbf{z}} =\hat{\mathbf{r}}(\theta,\phi) \end{align*} $$ 극각 방향 단위벡터 $\hat{\boldsymbol{\theta}}$ $\ha</description>
    </item>
    
    <item>
      <title>스칼라 삼중곱</title>
      <link>https://freshrimpsushi.github.io/posts/properties-of-scalar-triple-product/</link>
      <pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/properties-of-scalar-triple-product/</guid>
      <description>스칼라 삼중곱 $$ \mathbf{A}\cdot (\mathbf{B} \times \mathbf{C} ) $$ 설명 위 식을 스칼라 삼중곱scalar triple product이라 한다. 스칼라 삼중곱은 벡터 3개를 곱하는 연산 중에서 결과가 스칼라인 것을 말한다. 결과가 벡터인 것은 벡터 삼중곱이라 한다. 결과가 스칼라로 나오기 위해서는 우선 두 벡터를 외적해서 나온 벡터와 다른 벡터를 내적해야한다. 스칼라 삼중곱의 특징을 하나씩 살펴</description>
    </item>
    
    <item>
      <title>분리벡터</title>
      <link>https://freshrimpsushi.github.io/posts/separation-vector/</link>
      <pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/separation-vector/</guid>
      <description>정의1 원천점에서 관찰점까지의 벡터를 분리벡터separation vector라 한다. $$ \boldsymbol{\eta} = \mathbf{r} - \mathbf{r}&#39; $$ 설명 원천벡터source vector $\mathbf{r}&#39;$: 전하나 전류가 있는 곳. 즉, 전자기장을 만드는 근원지의 좌표를 나타내는 벡터이다. 위치벡터position vector $\mathbf{r}$: 전기장 $\mathbf{E}$나 자기장 $\mathbf{B}$ 등을 측정하는 곳의 좌표를 나타내는 벡터이</description>
    </item>
    
    <item>
      <title>분리벡터의 크기의 그래디언트</title>
      <link>https://freshrimpsushi.github.io/posts/gradient-of-separation-vector/</link>
      <pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gradient-of-separation-vector/</guid>
      <description>공식 분리벡터 $\boldsymbol{\eta}$의 크기의 $n$ 제곱, $\eta ^{n}$의 그래디언트는 다음과 같다. $$ \nabla (\eta^n)=n\eta^{n-1}\hat{\boldsymbol{\eta}} $$ 다항함수의 미분과 같은 방식으로 계산한 뒤에 단위벡터인 $\hat{\boldsymbol{\eta}}$만 붙여주면 된다. 설명 분리벡터는 $\boldsymbol{\eta}=\mathbf{r}-</description>
    </item>
    
    <item>
      <title>벡터 삼중곱, BAC-CAB 공식</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-vector-triple-productbac-cab-rule/</link>
      <pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-vector-triple-productbac-cab-rule/</guid>
      <description>공식 $$ \mathbf{A} \times (\mathbf{B} \times \mathbf{C} ) = \mathbf{B}(\mathbf{A} \cdot \mathbf{C} )-\mathbf{C}(\mathbf{A} \cdot \mathbf{B}) $$ 설명 위 공식의 좌변을 벡터 삼중곱vector triple product이라 한다. 우변의 결과를 간단하게 **BAC-CAB(백캡)**이라고 한다. 벡터 삼중곱은 벡터를 3번 곱하는 연산 중에서 그 결과가 벡터인 것이다. 결과가 벡터로 나오기 위해서 식에는 외적만 두 번 들어간다. 두 벡터의 외적은 여전히 벡터이므</description>
    </item>
    
  </channel>
</rss>
