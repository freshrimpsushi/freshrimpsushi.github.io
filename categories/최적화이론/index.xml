<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>최적화이론 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EC%B5%9C%EC%A0%81%ED%99%94%EC%9D%B4%EB%A1%A0/</link>
    <description>Recent content in 최적화이론 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Mon, 23 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EC%B5%9C%EC%A0%81%ED%99%94%EC%9D%B4%EB%A1%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>선형 계획 문제의 정의</title>
      <link>https://freshrimpsushi.github.io/posts/definition-of-linear-programming-problem/</link>
      <pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/definition-of-linear-programming-problem/</guid>
      <description>정의 1 목적 함수Objective Function와 제약Constraint들이 리니어한 최적화 문제를 선형 계획 문제Linear Programming Problem, 짧게는 선형 문제LP라 한다. 쉽게 말해, 선형 문제란 선형성을 가지는 목적 함수 $f: \mathbb{R}^{n} \to \mathbb{R}$ 가 주어진 벡터 $\mathbf{c} \in \mathbb{R}^{n}$ 에 대해 $$ f \left( \mathbf{x} \right) := \mathbf{c}^{T} \mathbf{x} $$ 이고 주어진 행렬 $A \in \mathbb{R}^{m \times n}$ 과 $\mathbf{b} \in \mathbb{R}^{m \times 1}$ 에 대해 $$ A \mathbf{x} \le \mathbf{b} $$ 을</description>
    </item>
    
    <item>
      <title>최적해: 최대인수와 최소인수</title>
      <link>https://freshrimpsushi.github.io/posts/optimizer-argmax-and-argmin/</link>
      <pubDate>Sat, 05 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/optimizer-argmax-and-argmin/</guid>
      <description>어려운 정의 임의의 집합 $X$ 과 전순서집합 $\left( Y, \le \right)$ 가 주어져 있다고 하자. $X$ 의 부분집합 $S \subset X$ 에 대해 함수 $f : X \to Y$ 의 최대인수Argument of Maxima $\argmax_{S} : Y^{X} \to 2^{X}$ 와 최소인수Argument of Minima $\argmin_{S} : Y^{X} \to 2^{X}$ 는 다음과 같이 정의된다. $$ \argmax_{S} f := \left\{ x_{\ast} \in S : f \left( x_{\ast} \right) \ge f(x) , \forall x \in X \right\} \\ \argmin_{S} f := \left\{ x_{\ast} \in S : f \left( x_{\ast} \right) \le f(x) , \forall x \in X \right\} $$ $2^{X}$ 은 $X$ 의 멱집</description>
    </item>
    
    <item>
      <title>최적값: 최대값과 최소값</title>
      <link>https://freshrimpsushi.github.io/posts/optimum-maximum-minimum/</link>
      <pubDate>Fri, 28 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/optimum-maximum-minimum/</guid>
      <description>쉬운 정의 최대값Maximum과 최소값Minimum을 통틀어 최적값Optimum이라 한다. 집합 $X$ 에서 가장 큰 원소를 최대값 $\max X$, 가장 작은 원소를 최소값 $\min X$ 과 같이 나타낸다. 함수 $f : X \to \mathbb{R}$ 의 가장 큰 함수값을 $\max_{X} f$, 가장 작은 함수값을 $\min_{X} f$ 와 같이 나타낸다. $\mathbb{R}$ 은 실수 전체의 집합을 나타낸다. 최대, 최소는 한자어고 값은 순우리말이므로 사</description>
    </item>
    
    <item>
      <title>확률적 경사 하강법</title>
      <link>https://freshrimpsushi.github.io/posts/stochastic-gradient-descent-method/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/stochastic-gradient-descent-method/</guid>
      <description>정의 목적 함수 $Q$ 와 러닝 레이트 $\alpha &amp;gt; 0$, 배치사이즈 $m$ 과 $i$ 번째 데이터에 대해 $$ \omega_{n+1} := \omega_{n} - \alpha {{ 1 } \over { n }} \sum_{i=1}^{m} \nabla Q_{i} ( \omega_{n} ) $$ 를 확률적 경사 하강법이라 한다. 설명 머신러닝 확률적 경사 하강법은 데이터를 다루는만큼 필연적으로 머신러닝과 깊은 관계를 가지고 있을 수밖에 없다. 몇몇 단어가 익숙하지 않더라도 일단 예시를 통해 이해해보는 게 좋다: $x_{1}$ 와 $x_{2}$ 가 주</description>
    </item>
    
    <item>
      <title>수학에서의 최적화 기법</title>
      <link>https://freshrimpsushi.github.io/posts/optimization-method/</link>
      <pubDate>Sat, 25 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/optimization-method/</guid>
      <description>정의 함수 $f : \mathbb{R}^{n} \to \mathbb{R}$ 의 함수값이 최소가 되도록 하는 $x^{ \ast } = \argmin_{x} f(x)$ 를 구하는 문제를 최적화 문제Optimization Problem라 하고, 그 문제를 푸는 알고리즘을 최적화 기법이라 부른다. 최적화 문제에서 주어진 함수 $f$ 를 특히 목적 함수Objective Function라 한다. 정의역의 모든 $x$ 에 대해 $f(x^{ \ast }) \le f(x)$ 를 만족하는 $x^{ \ast }$ 를</description>
    </item>
    
    <item>
      <title>수학에서의 경사하강법</title>
      <link>https://freshrimpsushi.github.io/posts/gradient-descent-method/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gradient-descent-method/</guid>
      <description>정의 1 스칼라 함수 $\varphi : \mathbb{R}^{n} \to \mathbb{R}$ 을 비용 함수Cost Function이라 한다. 비용 함수 $ \varphi ( \mathbb{x} )$ 의 극소값을 구하기 위해 $\mathbb{x} = \mathbb{x}_{n}$ 에서 $\varphi ( \mathbb{x}_{n+1} ) &amp;lt; \varphi ( \mathbb{x}_{n} )$ 를 만족시키는 $\mathbb{x}_{n+1}$ 를 찾는 알고리즘을 하강법Descent Method이라 한다. 설명 $\varphi$ 를 비용 함수라고 부를만한 예로써 집을 한 채 짓는다고 하자. 집 한 채에 들어가는 자원은 목재, 석재, 철</description>
    </item>
    
    <item>
      <title>최적화이론의 라그랑주 승수법</title>
      <link>https://freshrimpsushi.github.io/posts/lagrangian-multiplier-in-optimization-theory/</link>
      <pubDate>Fri, 21 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/lagrangian-multiplier-in-optimization-theory/</guid>
      <description>⚡ 이 포스트는 패스트 트랙Fast Track으로 작성되었습니다. 설명 비선형인 목적 함수를 가지는 비선형 최적화 문제에서 제약조건에 라그랑주 승수Lagrangian Multiplier라는 것을 곱해서 목적 함수에 반영시키는 풀이법을 라그랑주 승수법이라 한다. $$ \begin{matrix} \text{Maximize} &amp;amp; f(x) \\ \text{subject to} &amp;amp; g(x) = 0 \end{matrix} $$ 가령 미분가능한 스칼라함수 $f,g : \mathbb{R}^{p} \to \mathbb{R}$ 에</description>
    </item>
    
  </channel>
</rss>
