<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>알고리즘 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/</link>
    <description>Recent content in 알고리즘 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Sat, 02 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>문자열의 편집 거리</title>
      <link>https://freshrimpsushi.github.io/posts/edit-distance-of-strings/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/edit-distance-of-strings/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 편집 방법 1문자열에는 다음과 같이 네가지 작용이 있다. 삽입** : 문자열에 새로운 문자를 끼워넣는다. 제거** : 문자열에서 문자 하나를 없앤다. 교체** : 문자열에서 문자 하나를 다른 문자로 바꾼다. 전치** : 두 문자의 위치를 서로 바꾼다. 편집 거리는 문자열간의 거리 함수로써 편집 방법을</description>
    </item>
    
    <item>
      <title>그리디 알고리즘</title>
      <link>https://freshrimpsushi.github.io/posts/greedy-algorithm/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/greedy-algorithm/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 그리디 알고리즘 이란 어떤 선택을 할 때 그 순간만을 고려해서 가장 좋은 경우를 고르는 방법이다. 그리드 알고리즘은 탐욕Greed 이라는 이름대로 길게 보지 않고 그 순간만을 생각한다. 좋게 말하면 항상 최선을 다하는 것이지만, 크게 보았을 때 이는 현명하지 못할 수도 있다. 다음의 예시를 보자 : 왼쪽 0</description>
    </item>
    
    <item>
      <title>레벤슈타인 알고리즘</title>
      <link>https://freshrimpsushi.github.io/posts/levensteins-algorithm/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/levensteins-algorithm/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 문자열 $A,B$ 를 $A=[a_{i}]=(a_{1}, a_{2} , \cdots, a_{n})$ 과 $B=[b_{j}]=(b_{1}, b_{2} , \cdots, b_{m})$ 로 표현하자.**Step 1. 행렬 $M_{(n+1) \times (m+1)} = [m_{x y }]$ 를 만들고 $M_{11} = 0$ 을 대입한다.**Step 2. $m_{(i+1) 1} = i$ 그리고 $m_{ 1 (j+1)} = j$ 을 대입한다.Step 3. $i = 1, 2, \cdots , n$ 그리고 $j=1,2, \cdots , m$if $a_{i}==b_{j} $$ M_{i,j} = M_{(i-1)(j-1)}$ 을 대입한다.else$M_{i,j} = \min \left\{ M_{(i-1)(j)}, M_{(i)(j-1)}, M_{(i-1)(j-1)}\right\} + 1 $</description>
    </item>
    
    <item>
      <title>기수 정렬</title>
      <link>https://freshrimpsushi.github.io/posts/radix-sort/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/radix-sort/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 **기수 정렬 자리수가 $k$ 로 제한된 $n$ 개의 자연수로 이루어진 데이터가 주어져있다고 하자. 그러면 데이터는 다음의 알고리즘에 따라 정렬되며 그 시간 복잡도는 $O (n)$ 이다.$i = 1 , \cdots , k$ 번째 자리수들끼리 비교해서 정렬한다. 기수 정렬은 자리수의 제한이 있기 때문에 부동소수점이 있는 데이터에 적</description>
    </item>
    
    <item>
      <title>프로그래밍 패러다임</title>
      <link>https://freshrimpsushi.github.io/posts/programming-paradigm/</link>
      <pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/programming-paradigm/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 프로그래밍 패러다임 이란 주어진 문제를 해결하는 프로그램을 작성할 때의 관점 내지 방법론을 말한다. 어떠한 패러다임에 알맞는 프로그래밍 언어는 그러한 프로그래밍 패러다임을 갖는다 고 말하며, 대개의 언어는 하나의 패러다임을 갖는다. 여러 패러다임을 갖는 언어를 멀티 패러다임 언어 라고 한다. 언</description>
    </item>
    
    <item>
      <title>비교 정렬 알고리즘 시간 복잡도의 하한</title>
      <link>https://freshrimpsushi.github.io/posts/lower-bound-of-time-complexity-of-comparison-sort-algorithms/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/lower-bound-of-time-complexity-of-comparison-sort-algorithms/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 비교 정렬 알고리즘의 시간복잡도는 아무리 좋아도 $\Omega ( n \log n )$ 알고리즘이 원래 신기한 것이지만, 삽입 정렬과 같은 효율적인 알고리즘도 퀵 정렬에 밀리는 것을 보면 그 이상의 알고리즘도 있지 않을까 궁금할 수밖에 없다. 다행인지 아닌지는 모르겠으나, 이 증명에 따라 그보다 효율적인 알고리즘을 생각할</description>
    </item>
    
    <item>
      <title>비교 정렬 알고리즘들의 시간 복잡도</title>
      <link>https://freshrimpsushi.github.io/posts/time-complexity-of-comparison-sort-algorithms/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/time-complexity-of-comparison-sort-algorithms/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $n$ 개의 데이터가 주어져 있을 때, 비교 정렬 알고리즘들의 시간 복잡도는 다음과 같다. [1] 버블 정렬** :$$ \Theta ( n^2 ) \\ O ( n^2 ) $$ [2] 선택 정렬** :$$ \Theta ( n^2 ) \\ O ( n^2 ) $$ [3] 삽입 정렬** :$$ \Theta ( n^2 ) \\ O ( n^2 ) $$ [4] 힙 정렬 :$$ \Theta ( n \log n ) \\ O ( n \log n ) $$ [5] 합병 정렬** :$$ \Theta ( n \log n ) \\ O ( n</description>
    </item>
    
    <item>
      <title>동적 프로그래밍</title>
      <link>https://freshrimpsushi.github.io/posts/dynamic-programing/</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/dynamic-programing/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 문제를 풀 때, 큰 문제의 해답에 그보다 작은 문제의 해답이 포함되어 있으면 최적 부분 구조Optimal Substructure 를 가진다고 한다. 최적 부분 구조를 갖춘 문제의 예로써 가장 쉬운 것이 바로 피보나치 수를 구하는 것이다. $n$ 번째 피보나치 수는 $a_{n} = a_{n-1} + a_{n-2}$ 와 같이 구해지므로, 큰 문제 $a_{n}$ 에 작은 문제 $a_{n-1}$, $a_{n-2}$ 가 포함되</description>
    </item>
    
    <item>
      <title>재귀함수를 쓸 때 주의해야하는 이유</title>
      <link>https://freshrimpsushi.github.io/posts/why-you-watch-out-when-you-using-recurrence/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/why-you-watch-out-when-you-using-recurrence/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 프로그래밍을 처음 배우면 그것이 어떤 언어든지 &amp;lsquo;재귀함수는 조심해서 써야한다&amp;rsquo;는 경고가 함께한다. 사실 재귀함수라는 게 그렇게 빈번하게 사용되는 테크닉이 아니기 때문에 그 이유는 설명하지 않는 경우가 많은데, 배우는 입장에선 이 좋은 걸 왜 꺼리는지 이해가 잘 되지 않을</description>
    </item>
    
    <item>
      <title>시간복잡도와 공간복잡도</title>
      <link>https://freshrimpsushi.github.io/posts/time-complexity-and-space-complexity/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/time-complexity-and-space-complexity/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 주어진 문제를 풀 때의 걸리는 시간을 시간복잡도 , 메모리 소요를 공간복잡도 라고 한다. 점근적 표기법은 이들을 표현하는데에 굉장히 유용한 수단이 된다. 시간복잡도에 대한 예시를 살펴보자. (0) 상수 시간** : $O(1) $$ n$ 에 관계없이 끝낼 수 있는 알고리즘으로, 사실상 시간이 걸리지 않는 것이다. 가령 $\mathbb{x}</description>
    </item>
    
    <item>
      <title>알고리즘의 비용에 대한 점근적 표기법</title>
      <link>https://freshrimpsushi.github.io/posts/asymptotic-notation/</link>
      <pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/asymptotic-notation/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 크기가 $n$ 인 데이터에 대해 알고리즘의 비용을 다음과 같이 나타낸다. $O$ 표기법** : $O(g(n)) := \left\{ f(n) , | , \exists c &amp;gt; 0, n_{0} \in \mathbb{N} : \forall n \ge n_{0} \implies f(n) \le c g(n) \right\}$ $\Omega$ 표기법** : $\Omega (g(n)) := \left\{ f(n) , | , \exists c &amp;gt; 0, n_{0} \in \mathbb{N} : \forall n \ge n_{0} \implies f(n) \ge c g(n) \right\}$ $\Theta$ 표기법** : $\Theta (g(n)) := O (g(n)) \cap \Omega (g(n))$ 점근적 표기법은 알고리즘의 비용을 수리적</description>
    </item>
    
    <item>
      <title>지프의 법칙</title>
      <link>https://freshrimpsushi.github.io/posts/zipfs-law/</link>
      <pubDate>Sat, 19 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/zipfs-law/</guid>
      <description>법칙 코퍼스에서 $k$ 번째로 자주 나타나는 단어의 상대빈도를 $f_{k}$ 라고 하면 $$ f_{k} = {{C} \over {k}} $$ 설명 여기서 $C$ 는 $\displaystyle \sum_{k} f_{k} = 1$ 이 되도록하는 정규화계수다. 히스토그램으로 나타내보면 대략 위와 같은 모양이되 넓이의 합이 정확하게 $1$ 이 되도록 스케일을 조정해준 것이다. 오른쪽에 생기는 두꺼운 꼬리 모양을 롱테일이라고 부른다. 힙스의 법칙과 마찬가지로 경험적으</description>
    </item>
    
    <item>
      <title>힙스의 법칙</title>
      <link>https://freshrimpsushi.github.io/posts/heaps-law/</link>
      <pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/heaps-law/</guid>
      <description>법칙 코퍼스에서 어휘의 갯수를 $M$, 토큰의 갯수를 $T$ 라고 하면 $$ M = kT^{b} $$ 설명 코퍼스가 영어일 경우 보통 상수 $k,b$ 는 $10 \le k \le 100$, 그리고 $b = 0.5$ 정도로 나타난다고 한다. 힙스의 법칙은 수학적인 근거를 두고 유도된 것이 아니라 경험적으로 얻어진 법칙이다. 수식은 언뜻 굉장히 복잡해 보이지만 양변에 로그를 취하면 $\log M = \log k + b \log T$ 가 되고, 다음과 같이 선형적</description>
    </item>
    
    <item>
      <title>n-그램과 자카드 계수</title>
      <link>https://freshrimpsushi.github.io/posts/n-gram-and-jaccard-coefficient/</link>
      <pubDate>Sat, 28 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/n-gram-and-jaccard-coefficient/</guid>
      <description>정의 n-그램n-gram이란 어떠한 문자열을 n개씩 끊어서 자른 것을 말한다. 자카드 계수Jaccard Coefficient란 두 집합이 얼마나 비슷한지에 대한 척도로써 $0$ 부터 $1$ 사이의 값을 가진다. 수식으로 표현하면 다음과 같다. $$ JC(A,B) = {{| A \cap B|} \over {| A \cup B| }} = {{| A \cap B|} \over { |A|+ |B| -| A \cap B| }} $$ 예시 예를 들어 &amp;lsquo;오마이갓&amp;</description>
    </item>
    
  </channel>
</rss>
