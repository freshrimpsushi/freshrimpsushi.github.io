<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>양자정보이론 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EC%96%91%EC%9E%90%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0/</link>
    <description>Recent content in 양자정보이론 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Mon, 15 Apr 2013 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EC%96%91%EC%9E%90%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>부울함수의 정의</title>
      <link>https://freshrimpsushi.github.io/posts/definition-of-boolean-function/</link>
      <pubDate>Mon, 15 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/definition-of-boolean-function/</guid>
      <description>정의1 다음과 같은 [함수]를 부울 함수Boolean function라 한다. $n \in \mathbb{N}$에 대해서, $$ f : \left\{ 0, 1 \right\}^{n} \to \left\{ 0, 1 \right\} $$ 여기서 $1 =$ 참(True), $0 =$ 거짓(False)을 의미한다. 설명 영국의 수학자 조지 부울George Boole의 이름을 딴 것이다. 조지 부울은 부울 대수의 창시자로 논리학을 대수적</description>
    </item>
    
    <item>
      <title>정보이론에서 엔트로피란?</title>
      <link>https://freshrimpsushi.github.io/posts/shannon-entropy-in-information-theory/</link>
      <pubDate>Sat, 13 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/shannon-entropy-in-information-theory/</guid>
      <description>정의1 2 확률변수 $X$의 엔트로피Shannon entropy $H$를 다음과 같이 정의한다. $$ \begin{equation} H(X) := \sum_{i=1}^{N} P(x_{i}) I(x_{i}) = -\sum_{i=1}^{N} P(x_{i}) \log_{2}P(x_{i}) \end{equation} $$ 이때 $I$는 정보량이고, $P(x_{i}) = P(X=x_{i})$이다. 설명 쉽게 말해서 엔트로피란 정보의 기대값(평균)이다. 엔트로피를 통해 [부호화]의 효율과 통신의 한계에 대해서 수학적으로 다룰 수 있다. 엔트로피는 흔히 무질서</description>
    </item>
    
    <item>
      <title>정보이론에서 정보량이란?</title>
      <link>https://freshrimpsushi.github.io/posts/shannon-information-in-information-theory/</link>
      <pubDate>Tue, 09 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/shannon-information-in-information-theory/</guid>
      <description>정의1 확률변수 $X$에 대해서, $X=x$인 사건의 정보(량)information $I$를 다음과 같이 정의한다. $$ \begin{equation} I(x) = -\log_{2} P(X=x) \end{equation} $$ 설명 추상적 개념인 정보에 대한 정량적인 정의를 제시한 사람은 디지털 논리회로 이론과 정보이론을 창시한 클래드 섀넌Claude Shannon이다. 정보량를 &#39;확률의 마이너스 로그&#39;로 정의한 것을 처음</description>
    </item>
    
  </channel>
</rss>
