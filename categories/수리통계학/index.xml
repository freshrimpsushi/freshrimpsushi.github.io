<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>수리통계학 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/</link>
    <description>Recent content in 수리통계학 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Sun, 15 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>수리통계학에서의 확률 수렴</title>
      <link>https://freshrimpsushi.github.io/posts/convergence-in-probability/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/convergence-in-probability/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 측도론으로 정의되는 확률 수렴 **확률 수렴의 쉬운 정의1 확률변수 $X$ 와 확률 변수의 시퀀스 $\left\{ X_{n} \right\}$ 가 다음을 만족하면 $n \to \infty$ 일 때 $X_{n}$ 이 $X$ 로 확률 수렴 한다고 말하고, $X_{n} \overset{P}{\to} X$ 와 같이 나타낸다. $$ \forall \varepsilon &amp;gt; 0 , \lim_{n \to \infty} P \left[ \left| X_{n} - X \right| &amp;lt; \varepsilon \right] = 1 $$ 확률 수렴의 조건은 수식 그대로 확률의 센스에서 수렴을 정</description>
    </item>
    
    <item>
      <title>순서통계량</title>
      <link>https://freshrimpsushi.github.io/posts/order-statistics/</link>
      <pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/order-statistics/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 **순서통계량의 확률밀도함수1 랜덤 샘플 $X_{1} , \cdots , X_{n}$ 가 서포트 $\mathcal{S} =(a,b)$ 인 확률밀도함수 $f(x)$ 를 가지는 연속확률분포를 따른다고 하자. 이들을 크기 순으로 나열한 확률 변수들을 $Y_{1} &amp;lt; \cdots &amp;lt; Y_{n}$ 와 같이 나타내도록 하면 그 조인트, 마지널 확률밀도함수들은 다음과 같다. [1] 조인트** : $$ g \left( y_{1} , \cdots , y_{n} \right) =</description>
    </item>
    
    <item>
      <title>표본 분산을 n-1으로 나누는 이유 Why Sample Variance is Divided by n-1</title>
      <link>https://freshrimpsushi.github.io/posts/1747/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/1747/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $X_{i} \sim \left( \mu , \sigma^{2} \right)$ 이라고 하면 표본 분산 $S^{2}$ 는 다음과 같다. $$ S^{2} := {{1} \over {n-1}} \sum_{i=1}^{n} \left( X_{i} - \overline{X} \right)^{2} $$ 알다시피 표본 평균과 달리 표본 분산은 편차의 제곱을 모두 더한 후 표본 크기인 $n$ 이 아니라 $n-1$ 로 나눈다. 당연히 이를 이상하게 느껴야한다고는 말하지 않겠지만, 수식에 대한 보편적인 감성이 있다면 $n$ 개를 더하고 $n-1$ 로</description>
    </item>
    
    <item>
      <title>불편추정량</title>
      <link>https://freshrimpsushi.github.io/posts/unbiased-estimator/</link>
      <pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/unbiased-estimator/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 **불편성1 $\theta$ 의 추정량 $T$ 가 다음을 만족하면 $T$ 를 $\mu$ 의 불편추정량 이라고 한다. $$ E T = \theta $$ 불편성이란 편의를 가지지 않는 성질을 말한다. 가령 $X_{i} \sim \left( \mu , \sigma^{2} \right)$ 라고 할 때 $\mu$ 의 추정량으로써 표본평균 $\displaystyle \overline{X} = {{ 1 } \over { n }} \sum_{i} X_{i} $ 를 사용한다면 $\displaystyle E \overline{X} = \mu$ 이므로 $\overline{X}$ 는 $\mu$ 의 불편추정량이 된다. 이</description>
    </item>
    
    <item>
      <title>편의-분산 트레이드 오프</title>
      <link>https://freshrimpsushi.github.io/posts/bias-variance-trade-off/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bias-variance-trade-off/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $$ \text{MSE} \left( \widehat{\theta} \right) = \text{Var} \widehat{\theta} + \left( \text{Bias} \widehat{\theta} \right)^{2} $$ 평균제곱오차 $\text{MSE}$ 는 통계 모형의 평가나 머신 러닝에서의 손실 함수로써 즐겨쓰이는 척도로써, 특히 편의와 분산에 대한 트레이드 오프로 나타난다.통계학도에게 있어서 편의를 다루는 것은 다소 어색할지 모르겠다. 적절한 확률 분포를 가정하고 그에 따른 수학적 이론을 토대</description>
    </item>
    
    <item>
      <title>수리통계학에서의 편의</title>
      <link>https://freshrimpsushi.github.io/posts/bias-in-mathematical-statistics/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bias-in-mathematical-statistics/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 모수 $\theta$ 에 대한 추정량 $\widehat{\theta}$ 에 대해 다음과 같이 정의된 $\text{Bias}$ 를 편의 라고 한다. $$ \text{Bias} ( \theta ) = E(\widehat{\theta}) - \theta $$ Bias는 편의 또는 편향으로 순화되지만, 역시 가장 많이 쓰이는 말은 발음 그대로 읽은 [바이어스]다. 한국어에서 편의는 Convenience인 경우가 압도적으로 많고 수식적으로나 실제 쓰임새로</description>
    </item>
    
    <item>
      <title>수리통계학에서의 신뢰 구간</title>
      <link>https://freshrimpsushi.github.io/posts/confidence-interval/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/confidence-interval/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 **신뢰 구간1 확률 밀도 함수 $f (x; \theta)$ 를 가지는 확률 변수 $X$ 의 샘플 $X_{1} , \cdots , X_{n}$ 와 신용 계수Confidence Coefficient $\alpha \in (0,1)$ 가 주어져 있다고 하자. $$ L := L \left( X_{1} , \cdots , X_{n} \right) \\ U := U \left( X_{1} , \cdots , X_{n} \right) $$ 통계량 $L,U$ 가 위와 같이 정의되어있다고 할 때, 다음을 만족하는 구간 $(L,U) \subset \mathbb{R}$ 을 모수 $\theta$ 에 대한 $( 1 - \alpha)100%$</description>
    </item>
    
    <item>
      <title>수리통계학에서의 통계량과 추정량</title>
      <link>https://freshrimpsushi.github.io/posts/statistic-and-estimator/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/statistic-and-estimator/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 **통계량1 1. 확률 변수 $X$ 의 샘플 $X_{1} , \cdots , X_{n}$ 의 함수 $T$ 를 통계량 이라 한다. $$ T := T \left( X_{1} , \cdots , X_{n} \right) $$ 2.** $X$ 의 분포 함수가 $f(x; \theta)$ 혹은 $p(x; \theta)$ 와 같이 나타날 때, $T$ 가 $\theta$ 를 파악하기 위한 통계량이면 $T$ 를 $\theta$ 의 **추정량** 이라고 한다. 3.** 통계량과 모수에 대한 함수를 **피벗**Pivot 이라고 한</description>
    </item>
    
    <item>
      <title>수리통계학에서의 랜덤 샘플</title>
      <link>https://freshrimpsushi.github.io/posts/random-sample/</link>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/random-sample/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 **랜덤 샘플1 1. 확률 변수 $X$ 가 실제로 뽑힌 것을 실현Realization 이라 하고 보통 소문자 $x$ 로 나타낸다. 2.** 확률 변수 $X$ 와 같은 확률 분포에서 **샘플 사이즈**Sample Size $n$ 만큼 얻어낸 확률 변수들을 **샘플**Sample 이라 하고 다음과 같이 나타낸다. $$ X_{1} , X_{2} , \cdots , X_{n}</description>
    </item>
    
    <item>
      <title>확률 변수들의 선형 결합</title>
      <link>https://freshrimpsushi.github.io/posts/linear-combinations-of-random-variable/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/linear-combinations-of-random-variable/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 특정한 분포를 따르는 확률변수들의 덧셈 확률 변수 $X_{1} , \cdots , X_{n}$ 가 어떤 $(a_{1}, \cdots , a_{n}) \in \mathbb{R}^{n}$ 에 대해 $\displaystyle T := \sum_{i=1}^{n} a_{i} X_{i}$ 를 선형 결합 이라고 한다. 특히 $X_{1} , \cdots , X_{n}$ 이 iid면 사이즈 $n$ 의 랜덤 샘플Random Sample 이라고도 부른다. 통계학의 맥락이라면 모든 관측값에 같은 가중치가 곱해진 $a_{1} = \cdots = a_{n} $ 을 생각할 것이</description>
    </item>
    
    <item>
      <title>정규분포를 따르는 두 분포가 독립인 것과 공분산이 0인 것이 동치임을 증명</title>
      <link>https://freshrimpsushi.github.io/posts/591/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/591/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $X_{1} \sim N ( \mu_{1} , \sigma_{1} ) , X_{2} \sim N ( \mu_{2} , \sigma_{2} ) $ 면 $X_{1} \perp X_{2} \iff \text{cov} (X_{1} , X_{2} ) = 0$ 일반적으로 상관관계가 없다고 독립인 것은 아니다. 하지만 분포들이 정규분포를 따른다는 가정이 있다면 공분산이 $0$ 인 것이 독립임을 보장해준다. 증명 $$ \displaystyle M_{X_{1}} (t_{1} ) = \exp \left[ \mu_{1} t_{1} + {{1} \over {2}} \sigma_{1} t_{1}^{2} \right] M_{X_{2}} (t_{2} ) = \exp \left[ \mu_{2} t_{2} + {{1} \over {2}} \sigma_{2} t_{2}^{2} \right] $$</description>
    </item>
    
    <item>
      <title>짝으로 독립이라고 상호 독립은 아니다  번스타인 분포</title>
      <link>https://freshrimpsushi.github.io/posts/sbernstein-distribution/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/sbernstein-distribution/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $\displaystyle p(x,y,z) = {{1} \over {4} }$ 여기서 $(x,y,z) \in \left\{ (1,0,0), (0,1,0), (0,0,1), (1,1,1) \right\}$ 다. 번스타인 분포는 분포의 조건을 모두 만족시키고는 있지만 자연계에 실재하는 분포라고 보기는 어렵다. &amp;lsquo;짝으로 독립이면 상호 독립이다&amp;rsquo;라는 명제의 반례로 제시된 것으로, 그 외엔 아무런 의미가 없다. 다만 그 반례로써는 상당</description>
    </item>
    
    <item>
      <title>확률 변수들의 상호 독립과</title>
      <link>https://freshrimpsushi.github.io/posts/iid-mutual-independence-and-iidindependent-and-identically-distributed/</link>
      <pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/iid-mutual-independence-and-iidindependent-and-identically-distributed/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 1. 확률 변수 $X_{1} , \cdots , X_{n}$ 가 $i \ne j \implies X_{i} \perp X_{j}$ 를 만족하면 $X_{1} , \cdots , X_{n}$ 이 짝으로 독립Pairwise Independent 이라고 한다. 2.** 연속 확률 변수 $X_{1} , \cdots , X_{n}$ 의 조인트 확률 밀도 함수 $f$ 가 각각의 확률 밀도 함수 $f_{1} , \cdots , f_{n}$ 에 대해 다음을 만족하면 $X_{1} , \cdots , X_{n}$ 가 **상호 독립** 이라고 한다. $$ f(x_{1} , \cdots , x_{n} ) \equiv f_{1} (x_{1})</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률 변수의 독립</title>
      <link>https://freshrimpsushi.github.io/posts/independence-of-random-variable/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/independence-of-random-variable/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 측도론으로 정의되는 확률 변수의 독립 두 확률 변수 $X_{1}, X_{2}$ 의 조인트 확률 밀도 함수 $f$ 혹은 확률 질량 함수 $p$ 에 대해 $X_{1}, X_{2}$ 의 확률 밀도 함수들 $f_{1}, f_{2}$ 혹은 확률 질량 함수 $p_{1}, p_{2}$ 가 다음을 만족하면 $X_{1}, X_{2}$ 가 독립 이라고 하고, $X_{1} \perp X_{2}$ 와 같이 나타낸다. $$ f(x_{1} , x_{2} ) \equiv f_{1}(x_{1})f_{2}(x_{2}) \\ p(x_{1} , x_{2} ) \equiv p_{1}(x_{1})p_{2}(x_{2}) $$ 아래의 정리는 이산 확률 변수에 대</description>
    </item>
    
    <item>
      <title>수리통계학에서의 조건부 확률 분포</title>
      <link>https://freshrimpsushi.github.io/posts/conditional-probability/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/conditional-probability/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 측도론으로 정의되는 조건부 확률 분포측도론으로 정의되는 조건부 기대값 1. 이산 확률 변수 $X_{1}, X_{2}, \cdots , X_{n}$ 에 대해 다음의 $p_{2, \cdots , n \mid 1}$ 를 $X_{1} = x_{1}$ 이 주어졌을 때의 $ X_{2}, \cdots , X_{n}$ 의 조인트 조건부 확률 질량 함수 라고 한다. $$ p_{2, \cdots , n \mid 1} ( x_{2} , \cdots ,x_{n} \mid X_{1} = x_{1} ) = {{ p_{1, \cdots , n}(x_{1} , x_{2} , \cdots , x_{n}) } \over { p_{1}( X_{1} = x_{1} ) }}</description>
    </item>
    
    <item>
      <title>확률 변수의 변환</title>
      <link>https://freshrimpsushi.github.io/posts/transform-of-random-variable/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/transform-of-random-variable/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 다변량 확률 변수 $X = ( X_{1} , \cdots , X_{n} )$ 의 조인트 확률밀도함수 $f$ 가 $f(x_{1} , \cdots , x_{n})$ 와 같이 주어져있다고 하고 다음과 같은 변환을 생각해보자. $$ y_{1} = u_{1} (x_{1} , \cdots , x_{n}) \\ \vdots \\ y_{n} = u_{n} (x_{1} , \cdots , x_{n}) $$ 이러한 변환 $u_{1} , \cdots , u_{n}$ 는 단사가 아닐 수 있다. 따라서 $X$ 의 서포트 $S_{X}$ 는 $k$ 개의 파티션 $A_{1} , \cdots , A_{i} , \cdots , A_{k}$ 으로 나</description>
    </item>
    
    <item>
      <title>수리통계학에서의 다변량 확률 분포</title>
      <link>https://freshrimpsushi.github.io/posts/multivariate-distribution/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/multivariate-distribution/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 1. 표본 공간 $\Omega$ 에서 정의된 $n$ 개의 확률 변수 $X_{i}$ 에 대해 $X = (X_{1} , \cdots , X_{n})$ 를 $n$차원 랜덤 벡터Random Vector 라고 한다. $X$ 의 치역 $X(\Omega)$ 를 공간 이라고도 부른다. 2.** 다음을 만족하는 함수 $F_{X} : \mathbb{R}^{n} \to [0,1]$ 을 $X$ 의 **조인트 누적 분포 함수** 라고 한다. $$ F_{X}\left( x_{1}, \cdots , x_{n} \right) := P \left[ X_{1} \le x_{1} , \cdots , X_{n} \le x_{n} \right] $$ 3.** 어떤</description>
    </item>
    
    <item>
      <title>옌센 부등식의 기댓값 폼 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-expectation-form-of-jensens-inequality/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-expectation-form-of-jensens-inequality/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 옌센 부등식의 유한 폼 보기옌센 부등식의 적분 폼 보기조건부 옌센 부등식 개구간 $I$ 에서 함수 $\phi$ 가 컨벡스하고 두 번 미분가능, 확률변수 $X$의 기댓값 $\mu$ 가 존재하며 $X \subset I $ 면 $$ \phi [ E(X) ] \le E [ \phi(X)] $$ 적분 폼과는 상당히 유사한 형태를 가지고 있다. 잘 생각해보면 유한 폼 역시 항이 무한하지는 않지만 가중</description>
    </item>
    
    <item>
      <title>체비셰프 부등식 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-chebyshevs-inequality/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-chebyshevs-inequality/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 확률변수 $X$ 의 분산 $\sigma^2 &amp;lt; \infty$ 가 존재하면 $\mu := E(X)$ 와 어떤 양수 $k&amp;gt;0$ 에 대해 $$ \displaystyle P(|X-\mu| \ge k\sigma) \le {1 \over k^2} $$ 비교적 형태가 간단하고 식의 조작이 쉬운데다 결과도 한 눈에 들어오기 때문에 보조정리로써 많이 쓰인다. 다만 마코프 부등식과 비교하자면 분산이 존재해야한다는 조건이 하나 더 있다.조건에서 $2$차 적률이 존</description>
    </item>
    
    <item>
      <title>마코프 부등식 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-markovs-inequality/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-markovs-inequality/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 확률변수 $X$ 에 대해 함수 $u(X) \ge 0$ 를 정의하자. $E(u(X))$ 가 존재하면 $c &amp;gt; 0$ 에 대해 $$ \displaystyle P(u(X) \ge c) \le {E(u(X)) \over c} $$ 수많은 증명에 사용되는 보조정리로써 이를 좀 더 편리하게 만든 체비셰프 부등식이 있다.조건에서 $1$차 적률이 존재해야하는 것을 보고 너무 쉽고 당연한 조건으로 여길지 모르겠다. 뭐 어느정도는 맞는 말</description>
    </item>
    
    <item>
      <title>n차 적률이 존재하면 차수가 n보다 작은 적률도 존재한다</title>
      <link>https://freshrimpsushi.github.io/posts/247/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/247/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 확률변수 $X$와 자연수 $n$에 대해 $E( X^n )$ 이 존재하면 $E( X^m ), m=1,2,3,\cdots, n$ 도 존재한다. 어떤 차수의 적률이든 존재하기만 한다면 그보다 작은 차수의 적률은 항상 존재하지만, 당연히 역은 성립하지 않는다. 물론 실제로 문제를 접해보면 높은 차수의 적률이 먼저 주어지는 경우는 거의 없으나, 어떤 정리의 조건</description>
    </item>
    
    <item>
      <title>적률생성함수란</title>
      <link>https://freshrimpsushi.github.io/posts/moment-generating-function/</link>
      <pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/moment-generating-function/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 확률변수 $X$ 와 어떤 양수 $h&amp;gt;0$ 대해 $E(e^{tX})$ 이 $-h&amp;lt; t &amp;lt; h$ 에서 존재하면 $M(t) = E( e^{tX} )$ 를 $X$ 의 적률생성함수 라고 정의한다. 적률생성함수는 흔히 mgf 라는 약어로 많이 쓰인다. 수리통계학에서는 비교적 초반에 배우는데, 생소한 정의와 맥락 없는 등장 때문에 수리통계학을 싫어지게 만드는 주범 중 하나다. 적률생성함수</description>
    </item>
    
    <item>
      <title>수리통계학에서의 첨도</title>
      <link>https://freshrimpsushi.github.io/posts/kurtosis/</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/kurtosis/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 1. 확률변수 $X$ 의 평균이 $\mu$, 분산이 $\sigma^2$ 라고 할 때 다음과 같이 정의된 $\gamma_{2}$ 를 $X$ 의 첨도라고 한다. $$ \gamma_{2} := {{ E \left( X - \mu \right)^4 } \over { \sigma^4 }} $$ 2.** 데이터 $\left\{ X_{i} \right\}_{i}^{n}$ 의 표본평균이 $\overline{X}$, 표본분산이 $\widehat{\sigma}^2$ 이라고 할 때 표본첨도 $g_{2}$ 는 다음과 같이 구해진다. $$ g_{2} := \sum_{i=1}^{n} = {{ \left( X - \overline{X} \right)^4 } \over { n \widehat{\sigma}^4 }} - 3 $$ 첨도는 4차 적률로 구해지며</description>
    </item>
    
    <item>
      <title>수리통계학에서의 왜도</title>
      <link>https://freshrimpsushi.github.io/posts/skewness/</link>
      <pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/skewness/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 1. 확률변수 $X$ 의 평균이 $\mu$, 분산이 $\sigma^2$ 라고 할 때 다음과 같이 정의된 $\gamma_{1}$ 를 $X$ 의 왜도라고 한다. $$ \gamma_{1} := {{ E \left( X - \mu \right)^3 } \over { \sigma^3 }} $$ 2.** 데이터 $\left\{ X_{i} \right\}_{i}^{n}$ 의 표본평균이 $\overline{X}$, 표본분산이 $\widehat{\sigma}^2$ 이라고 할 때 표본왜도 $g_{1}$ 은 다음과 같이 구해진다. $$ g_{1} := \sum_{i=1}^{n} {{ \left( X - \overline{X} \right)^3 } \over { n \widehat{\sigma}^3 }} $$ 왜도는 3차 적률로 구해지며, 확률</description>
    </item>
    
    <item>
      <title>공분산의 여러가지 성질들</title>
      <link>https://freshrimpsushi.github.io/posts/properties-of-covariance/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/properties-of-covariance/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 평균이 각각 $\mu_{X}$, $\mu_{Y}$ 인 확률 변수 $X$, $Y$ 에 대해 $\text{Cov} (X ,Y) : = E \left[ ( X - \mu_{X} ) ( Y - \mu_{Y} ) \right] $ 을 $X$ 와 $Y$ 의 공분산 이라고 정의한다. 공분산은 아래의 성질들을 가진다. [1]** $\text{Var} (X) = \text{Cov} (X,X)$ [2]** $\text{Cov} (X,Y) = \text{Cov} (Y, X) $ [3]** $\text{Var} (X + Y) = \text{Var} (X) + \text{Var} (Y) + 2 \text{Cov} (X,Y) $ [4]** $\text{Cov} (X + Y , Z ) = \text{Cov}(X,Z) + \text{Cov}(Y,Z) $ [5]** $\text{Cov} (aX + b , cY + d ) = ac \text{Cov}(X,Y) $ 공분산</description>
    </item>
    
    <item>
      <title>피어슨 상관계수</title>
      <link>https://freshrimpsushi.github.io/posts/pearson-correlation-coefficient/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/pearson-correlation-coefficient/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 다음과 같이 정의된 $\rho = \rho(X,Y)$ 를 피어스 상관계수라고 한다. $$ \displaystyle \rho = { {\text{Cov} (X,Y)} \over {\sigma_X \sigma_Y} } $$ (피어슨) 상관 계수** 는 두 변수가 서로 **(선형) 상관 관계** 를 가지고 있는지를 확인하는 척도가 된다. $1$ 이나 $–1$ 에 가까우면 상관관계가 있다고 보고 $0$ 이면 없다고 본다. $[-1, 1]$ 를 벗어나지는 않는다.</description>
    </item>
    
    <item>
      <title>평균과 분산의 성질들</title>
      <link>https://freshrimpsushi.github.io/posts/properties-of-mean-and-variance/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/properties-of-mean-and-variance/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 평균 $E ( X ) = \mu_{X} $ 과 분산 $\text{Var} (X) = E [ ( X - \mu_{X} )^2 ] $ 은 아래의 성질들을 가진다. [1]** $E(X + Y) = E(X) + E(Y)$ [2]** $E(aX + b) = a E(X) + b $ [3]** $\text{Var} (X) \ge 0$ [4]** $ \text{Var} ( X ) = E(X^2) - \mu_{X}^2 $ [5]** $ \text{Var} (aX + b) = a^2 \text{Var} (X)$ 평균과 분산에 관한 것이니만큼 아주 중요한 성질들이다. 특히 [2] 는 이른바 선형성Linearity 이라 불리</description>
    </item>
    
    <item>
      <title>대표값의 수리적 성질 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-mathematical-property-of-representative-value/</link>
      <pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-mathematical-property-of-representative-value/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 통계학의 세가지 대표값 : 최빈값, 중앙값, 평균 데이터 $X = \left\{ x_{1} , \cdots , x_{n} \right\} $ 가 주어져 있다고 하자. [0]** $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{0}$ 가 최소가 되도록 하는 $\theta$ 는 $\text{mode}(X)$ [1]** $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{1}$ 가 최소가 되도록 하는 $\theta$ 는 $\text{median}(X)$ [2]** $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{2}$ 가 최소가 되도록 하는 $\theta$ 는 $\text{mean}(X)$ 선형대수의 용어로 어렵게 말해보자면 다음과 같다 : (0) $l^{0}</description>
    </item>
    
    <item>
      <title>수리통계학에서의 기대값 평균 분산 적률의 정의</title>
      <link>https://freshrimpsushi.github.io/posts/expectation-mean-variance-moment/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/expectation-mean-variance-moment/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 대표값으로써의 평균측도론으로 정의되는 기대값 확률 변수 $X$ 가 주어져 있다고 하자. 1.** 연속 확률 변수 $X$ 의 확률 밀도 함수 $f(x)$ 가 $\displaystyle \int_{-\infty}^{\infty} |x| f(x) dx &amp;lt; \infty$ 를 만족하면 다음과 같이 정의된 $E(X)$ 를 $X$ 의 **기대값** 이라고 한다. $$ E(X) := \int_{-\infty}^{\infty} x f(x) dx $$ 2.** 이산 확률 변수 $X$ 의 확률 질량 함수 $p(x)$ 가 $\displaystyle \sum_{x} |x| p(x) &amp;lt; \infty$ 를 만족하면 다</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률 변수와 확률 분포</title>
      <link>https://freshrimpsushi.github.io/posts/1433/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/1433/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 측도론으로 정의되는 확률 변수와 확률 분포측도론으로 정의되는 누적분포함수 표본 공간 $\Omega$ 에서 확률 $P$ 가 정의되어 있다고 하자. 1.** 정의역이 표본 공간인 함수 $X : \Omega \to \mathbb{R}$ 을 **확률 변수** 라고 한다. 확률 변수의 치역 $X(\Omega)$ 을 **공간**Space 이라고도 부른다. 2.** 다음을 만족하는 함수 $F_{X} : \mathbb{R} \to</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률과 확률의 덧셈법칙</title>
      <link>https://freshrimpsushi.github.io/posts/1431/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/1431/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 측도론으로 엄밀하게 정의되는 확률 1. 같은 조건 하에서 반복할 수 있는 시행을 임의 시행Random Experiment 이라고 한다. 임의 시행에서 얻을 수 있는 모든 결과Outcome 를 모아놓은 집합 $\Omega$ 를 표본 공간Sample Space 이라고 한다. 표본 공간에서 우리가 관심을 가지는 결과들의 집합, 즉 $B \subset \Omega$ 를 사건E</description>
    </item>
    
    <item>
      <title>스튜던트의 정리 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-students-theorem/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-students-theorem/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $X_{i} \sim N(\mu,\sigma)$이라고 하자.(a) $\displaystyle \overline{X} \sim N\left( \mu , { {\sigma^2} \over {n} } \right) $(b) $\overline{X} \perp S^2$(c) $\displaystyle (n-1) { {S^2} \over {\sigma^2} } \sim \chi^2 (n-1) $(d) $\displaystyle T = { {\overline{X} - \mu } \over {S / \sqrt{n}} } \sim t(n-1)$ 통계학을 하는 사람들은 당연한듯 쓰고 있지만 사실 여기에도 이름이 있다.네 개의 파트로 나뉘어져 있어 구체적으로 인용하기도 어려운 것도 한몫</description>
    </item>
    
  </channel>
</rss>
