<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>수리통계학 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/</link>
    <description>Recent content in 수리통계학 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Sun, 07 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>다변량 확률 변수의 확률 수렴</title>
      <link>https://freshrimpsushi.github.io/posts/convergence-in-probability-of-random-vector/</link>
      <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/convergence-in-probability-of-random-vector/</guid>
      <description>정의 1 $p$차원 랜덤 벡터 $\mathbf{X}$ 와 랜덤 벡터의 시퀀스 $\left\{ \mathbf{X}_{n} \right\}$ 가 다음을 만족하면 $n \to \infty$ 일 때 $\mathbf{X}_{n}$ 이 $\mathbf{X}$ 로 확률 수렴Convergence in Probability한다고 말하고, $\mathbf{X} _ {n} \overset{P}{\to} \mathbf{X}$ 와 같이 나타낸다. $$ \forall \varepsilon &amp;gt; 0 , \lim_{n \to \infty} P \left[ \left\| \mathbf{X}_{n} - \mathbf{X} \right\| &amp;lt; \varepsilon \right] = 1 $$ $\| \cdot \|$ 는 유클리드 놈으로써, $\left\| \left( x_{1} , \cdots , x_{n} \right) \right\| = \sqrt{ x_{1}^{2} + \cdots + x_{n}^{2}}$ 와 같이 정의된다. 정리</description>
    </item>
    
    <item>
      <title>공분산 행렬</title>
      <link>https://freshrimpsushi.github.io/posts/covariance-matrix/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/covariance-matrix/</guid>
      <description>정의1 $p$차원 랜덤 벡터 $\mathbf{X} = \left( X_{1}, \cdots , X_{p} \right)$ 에 대해 다음과 같이 정의된 $\text{Cov} (\mathbf{X})$ 를 공분산 행렬Covariance Matrix이라 한다. $$ \left( \text{Cov} \left( \mathbf{X} \right) \right)_{ij} := \text{Cov} \left( X_{i} , X_{j} \right) $$ $\text{Cov}$ 는 공분산이다. 설명 정의를 더 쉽게 풀어 적어보면 다음과 같다. $$ \text{Cov} \left( \mathbf{X} \right) := \begin{pmatrix} \text{Var} \left( X_{1} \right) &amp;amp; \text{Cov} \left( X_{1} , X_{2} \right) &amp;amp; \cdots &amp;amp; \text{Cov} \left( X_{1} , X_{p} \right) \\ \text{Cov} \left( X_{2} , X_{1} \right) &amp;amp; \text{Var} \left( X_{2} \right) &amp;amp; \cdots &amp;amp; \text{Cov} \left( X_{2} , X_{p}</description>
    </item>
    
    <item>
      <title>중심극한 정리 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-central-limit-theorem/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-central-limit-theorem/</guid>
      <description>정리 1 ${X_n}$가 iid 확률 변수들이고 확률분포 $ \left( \mu, \sigma^2 \right) $를 따른다고 하면 $n \to \infty$ 일 때 $$ \displaystyle \sqrt{n} {{ \overline{X}_n - \mu } \over {\sigma}} \overset{D}{\to} \text{N} (0,1) $$ $\overset{D}{\to}$ 는 분포 수렴을 의미한다. 설명 통계학에선 대수의 법칙과 더불어 정말 그 명성이 자자한 정리로 꼽힌다. 수없이 듣고 쓰는 정리지만 막상 증명은 수리통계학을 배우면서 한번 해볼까말까다. 하지만 실제로는 활용도를 떠나 증명 자</description>
    </item>
    
    <item>
      <title>약한 대수의 법칙 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-the-weak-law-of-large-numbers/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-the-weak-law-of-large-numbers/</guid>
      <description>법칙 ${X_n}$가 iid 확률 변수들이고 확률분포 $ \left( \mu, \sigma^2 \right) $를 따른다고 하면 $n \to \infty$ 일 때 $$ \overline{X}_n \overset{P}{\to} \mu $$ $\overset{P}{\to}$ 는확률 수렴을 의미한다. 설명 이 정리는 그 어떤 분포든 &amp;lsquo;표본평균은 모평균으로 수렴한다&amp;rsquo;는 팩트를 함의한다. 생각해보면 당연할 수도 있지만, 자연과학에서 &amp;lsquo;당연하다&amp;rsquo;는 말만큼 중요</description>
    </item>
    
    <item>
      <title>분포수렴하면 확률유계다</title>
      <link>https://freshrimpsushi.github.io/posts/if-convergence-in-distribution-then-bounded-in-probability/</link>
      <pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/if-convergence-in-distribution-then-bounded-in-probability/</guid>
      <description>정리 확률변수의 시퀀스 $\left\{ X_{n} \right\} $ 가 분포수렴하면 확률유계다. $\overset{D}{\to}$ 는 분포 수렴을 의미한다. 설명 앞서 확률수렴하면 분포수렴함을 보였으므로, 이 대우 명제를 생각해보면 &amp;lsquo;확률유계가 아니면 확률수렴하지 않는다&amp;rsquo;는 상식적인 따름정리도 얻을 수 있다. 증명 $\epsilon&amp;gt;0$ 가 주어져 있고 $X_{n}$ 이 확률변수 $X$ 로 분포수렴하며 그 누적분포함수가</description>
    </item>
    
    <item>
      <title>확률수렴하면 분포수렴한다</title>
      <link>https://freshrimpsushi.github.io/posts/if-convergence-in-probability-then-convergence-in-distribution/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/if-convergence-in-probability-then-convergence-in-distribution/</guid>
      <description>정리1 확률변수 $X$ 와 확률변수의 시퀀스 $\left\{ X_{n} \right\} $ 에 대해 $$ X_{n} \overset{P}{\to} X \implies X_{n} \overset{D}{\to} X $$ $\overset{P}{\to}$ 는 확률 수렴을 의미한다. $\overset{D}{\to}$ 는 분포 수렴을 의미한다. 설명 직관적인 단어로 다시 말하자면, 분포만 수렴하는 것이 정확히 수렴하는 것보다는 훨씬 쉽다는 말이다. 확률변수라는 것 자체를 함수로써 정확하게 이해하고 있다면 받아들이기 어렵지 않을 것이다. 증명 전략: 사건을 둘</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률 유계</title>
      <link>https://freshrimpsushi.github.io/posts/bounded-in-probability/</link>
      <pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bounded-in-probability/</guid>
      <description>정의 1 확률 변수의 시퀀스 $\left\{ X_{n} \right\}$ 가 주어져 있다고 하자. 모든 $\varepsilon &amp;gt; 0$ 에 대해 다음을 만족시키는 $N_{\varepsilon} \in \mathbb{N}$ 과 상수 $B_{\varepsilon} &amp;gt; 0$ 가 존재하면 $\left\{ X_{n} \right\}$ 가 확률 유계Bounded in Probability라고 한다. $$ n \ge N_{\varepsilon} \implies P \left[ \left| X_{n} \right| \le B_{\varepsilon} \right] \ge 1 - \varepsilon $$ 설명 생각해보면 일상생활에서 실제로 접하는 많은 확률 분포 함수들의 정의역이 무한히 넓다. 표준정규분포 $N(0,1)$</description>
    </item>
    
    <item>
      <title>수리통계학에서의 분포 수렴</title>
      <link>https://freshrimpsushi.github.io/posts/convergence-in-distribution/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/convergence-in-distribution/</guid>
      <description>정의 1 확률변수 $X$ 와 확률 변수의 시퀀스 $\left\{ X_{n} \right\}$ 가 다음을 만족하면 $n \to \infty$ 일 때 $X_{n}$ 이 $X$ 로 분포 수렴Convergence in Distribution한다고 말하고, $X_{n} \overset{D}{\to} X$ 와 같이 나타낸다. $$ \lim_{n \to \infty} F_{X_{n}} (x) = F_{X} (x) \qquad, \forall x \in C_{F_{X}} $$ $F_{X}$ 는 확률변수 $X$ 의 누적분포함수다. $C_{F_{X}}$ 는 함수 $F_{X}$ 가 연속인 점들의 집합을 나타낸다. 설명 분포 수렴 은 확률 수렴과 마찬가지</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률 수렴</title>
      <link>https://freshrimpsushi.github.io/posts/convergence-in-probability/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/convergence-in-probability/</guid>
      <description>정의 1 확률변수 $X$ 와 확률 변수의 시퀀스 $\left\{ X_{n} \right\}$ 가 다음을 만족하면 $n \to \infty$ 일 때 $X_{n}$ 이 $X$ 로 확률 수렴Convergence in Probability한다고 말하고, $X_{n} \overset{P}{\to} X$ 와 같이 나타낸다. $$ \forall \varepsilon &amp;gt; 0 , \lim_{n \to \infty} P \left[ \left| X_{n} - X \right| &amp;lt; \varepsilon \right] = 1 $$ 설명 확률 수렴의 조건은 수식 그대로 확률의 센스에서 수렴을 정의한 것으로, 쉽게 말해 $n$ 이 커지면 두 확률 변수</description>
    </item>
    
    <item>
      <title>순서통계량</title>
      <link>https://freshrimpsushi.github.io/posts/order-statistics/</link>
      <pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/order-statistics/</guid>
      <description>정리1 랜덤 샘플 $X_{1} , \cdots , X_{n}$ 가 서포트 $\mathcal{S} =(a,b)$ 인 확률밀도함수 $f(x)$ 를 가지는 연속확률분포를 따른다고 하자. 이들을 크기 순으로 나열한 확률 변수들을 $Y_{1} &amp;lt; \cdots &amp;lt; Y_{n}$ 와 같이 나타내도록 하면 그 조인트, 마지널 확률밀도함수들은 다음과 같다. [1] 조인트: $$ g \left( y_{1} , \cdots , y_{n} \right) = \begin{cases} n! f (y_{1}) \cdots f (y_{n}) &amp;amp;, a &amp;lt; y_{1} &amp;lt; \cdots &amp;lt; y_{n} &amp;lt; b \\ 0 &amp;amp; , \text{elsewhere} \end{cases} $$ [2] 마지널: $Y_{k}$ 의 누적밀도함수</description>
    </item>
    
    <item>
      <title>표본 분산을 n-1으로 나누는 이유</title>
      <link>https://freshrimpsushi.github.io/posts/why-sample-variance-is-divided-by-n-1/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/why-sample-variance-is-divided-by-n-1/</guid>
      <description>왜 n-1로 나누지? $X_{i} \sim \left( \mu , \sigma^{2} \right)$ 이라고 하면 표본 분산 $S^{2}$ 는 다음과 같다. $$ S^{2} := {{1} \over {n-1}} \sum_{i=1}^{n} \left( X_{i} - \overline{X} \right)^{2} $$ 알다시피 표본 평균과 달리 표본 분산은 편차의 제곱을 모두 더한 후 표본 크기인 $n$ 이 아니라 $n-1$ 로 나눈다. 당연히 이를 이상하게 느껴야한다고는 말하지 않겠지만, 수식에 대한 보편적인 감성이 있다면 $n$ 개를 더하고 $n-1$ 로 나누는 것에서 강렬한 띠꺼움을 느</description>
    </item>
    
    <item>
      <title>불편추정량</title>
      <link>https://freshrimpsushi.github.io/posts/unbiased-estimator/</link>
      <pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/unbiased-estimator/</guid>
      <description>정의 1 $\theta$ 의 추정량 $T$ 가 다음을 만족하면 $T$ 를 $\mu$ 의 불편추정량Unbiased Estimator이라고 한다. $$ E T = \theta $$ 설명 불편성이란 편의를 가지지 않는 성질을 말한다. 가령 $X_{i} \sim \left( \mu , \sigma^{2} \right)$ 라고 할 때 $\mu$ 의 추정량으로써 표본평균 $\displaystyle \overline{X} = {{ 1 } \over { n }} \sum_{i} X_{i} $ 를 사용한다면 $\displaystyle E \overline{X} = \mu$ 이므로 $\overline{X}$ 는 $\mu$ 의 불편추정량이 된다. 이는 언뜻 당연해보</description>
    </item>
    
    <item>
      <title>편의-분산 트레이드 오프</title>
      <link>https://freshrimpsushi.github.io/posts/bias-variance-trade-off/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bias-variance-trade-off/</guid>
      <description>정의 $$ \text{MSE} \left( \widehat{\theta} \right) = \text{Var} \widehat{\theta} + \left( \text{Bias} \widehat{\theta} \right)^{2} $$ 평균 평균제곱오차 $\text{MSE}$ 는 통계 모형의 평가나 머신 러닝에서의 손실 함수로써 즐겨쓰이는 척도로써, 특히 편의와 분산에 대한 트레이드 오프로 나타난다.통계학도에게 있어서 편의를 다루는 것은 다소 어색할지 모르겠다. 적절한 확률 분포를 가정하고 그에 따른 수학적 이론을 토대로 데이터를 다루는 입장에서 분산은 손에 잡힐</description>
    </item>
    
    <item>
      <title>수리통계학에서의 편의</title>
      <link>https://freshrimpsushi.github.io/posts/bias-in-mathematical-statistics/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bias-in-mathematical-statistics/</guid>
      <description>정의 모수 $\theta$ 에 대한 추정량 $\widehat{\theta}$ 에 대해 다음과 같이 정의된 $\text{Bias}$ 를 편의 라고 한다. $$ \text{Bias} ( \theta ) = E(\widehat{\theta}) - \theta $$ 설명 Bias는 편의 또는 편향으로 순화되지만, 역시 가장 많이 쓰이는 말은 발음 그대로 읽은 [바이어스]다. 한국어에서 편의는 Convenience인 경우가 압도적으로 많고 수식적으로나 실제 쓰임새로나 &amp;lsquo;편향&amp;rsquo;으로</description>
    </item>
    
    <item>
      <title>수리통계학에서의 신뢰 구간</title>
      <link>https://freshrimpsushi.github.io/posts/confidence-interval/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/confidence-interval/</guid>
      <description>정의 1 확률 밀도 함수 $f (x; \theta)$ 를 가지는 확률 변수 $X$ 의 샘플 $X_{1} , \cdots , X_{n}$ 와 신뢰 계수Confidence Coefficient $\alpha \in (0,1)$ 가 주어져 있다고 하자. $$ L := L \left( X_{1} , \cdots , X_{n} \right) \\ U := U \left( X_{1} , \cdots , X_{n} \right) $$ 통계량 $L,U$ 가 위와 같이 정의되어있다고 할 때, 다음을 만족하는 구간 $(L,U) \subset \mathbb{R}$ 을 모수 $\theta$ 에 대한 $( 1 - \alpha)100 \%$ 신뢰구간이라 한다. $$ 1-\alpha = P \left[ \theta \in \left( L,U \right) \right] $$ 설명 사실 신</description>
    </item>
    
    <item>
      <title>수리통계학에서의 통계량과 추정량</title>
      <link>https://freshrimpsushi.github.io/posts/statistic-and-estimator/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/statistic-and-estimator/</guid>
      <description>정의 1 확률 변수 $X$ 의 샘플 $X_{1} , \cdots , X_{n}$ 의 함수 $T$ 를 통계량Statistic이라 한다. $$ T := T \left( X_{1} , \cdots , X_{n} \right) $$ $X$ 의 분포 함수가 $f(x; \theta)$ 혹은 $p(x; \theta)$ 와 같이 나타날 때, $T$ 가 $\theta$ 를 파악하기 위한 통계량이면 $T$ 를 $\theta$ 의 추정량Estimator이라고 한다. 통계량과 모수에 대한 함수를 피벗Pivot이라고 한다. 설명 2: 추정량(Estimator)</description>
    </item>
    
    <item>
      <title>수리통계학에서의 랜덤 샘플</title>
      <link>https://freshrimpsushi.github.io/posts/random-sample/</link>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/random-sample/</guid>
      <description>정의 1 확률 변수 $X$ 가 실제로 뽑힌 것을 실현Realization 이라 하고 보통 소문자 $x$ 로 나타낸다. 확률 변수 $X$ 와 같은 확률 분포에서 샘플 사이즈Sample Size $n$ 만큼 얻어낸 확률 변수들을 샘플Sample 이라 하고 다음과 같이 나타낸다. $$ X_{1} , X_{2} , \cdots , X_{n} $$ 확률 변수 $X_{1} , \cdots , X_{n}$ 이 iid면 사이즈 $n$ 의 랜덤 샘플 이라 부른다. 설명 이러한 정의</description>
    </item>
    
    <item>
      <title>확률 변수들의 선형 결합</title>
      <link>https://freshrimpsushi.github.io/posts/linear-combinations-of-random-variable/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/linear-combinations-of-random-variable/</guid>
      <description>정의 1 확률 변수 $X_{1} , \cdots , X_{n}$ 가 어떤 $(a_{1}, \cdots , a_{n}) \in \mathbb{R}^{n}$ 에 대해 $\displaystyle T := \sum_{i=1}^{n} a_{i} X_{i}$ 를 선형 결합Linear Combinations이라고 한다. 설명 특히 $X_{1} , \cdots , X_{n}$ 이 iid면 사이즈 $n$ 의 랜덤 샘플Random Sample 이라고도 부른다. 통계학의 맥락이라면 모든 관측값에 같은 가중치가 곱해진 $a_{1} = \cdots = a_{n} = {{ 1 } \over { n } } $ 을 생각할 것이다. 해석학과 선형</description>
    </item>
    
    <item>
      <title>정규분포를 따르는 두 확률 변수가 독립인 것과 공분산이 0인 것은 동치다</title>
      <link>https://freshrimpsushi.github.io/posts/two-normal-distributions-are-independent-iff-their-covariance-is-equal-to-zero/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/two-normal-distributions-are-independent-iff-their-covariance-is-equal-to-zero/</guid>
      <description>정리 $$ X_{1} \sim N ( \mu_{1} , \sigma_{1} ) \\ X_{2} \sim N ( \mu_{2} , \sigma_{2} ) $$ 면 $$ X_{1} \perp X_{2} \iff \text{cov} (X_{1} , X_{2} ) = 0 $$ 설명 일반적으로 상관관계가 없다고 독립인 것은 아니다. 하지만 분포들이 정규분포를 따른다는 가정이 있다면 공분산이 $0$ 인 것이 독립임을 보장해준다. 증명 $( \Leftarrow ) $ $$ \displaystyle M_{X_{1}} (t_{1} ) = \exp \left[ \mu_{1} t_{1} + {{1} \over {2}} \sigma_{1} t_{1}^{2} \right] M_{X_{2}} (t_{2} ) = \exp \left[ \mu_{2} t_{2} + {{1} \over {2}} \sigma_{2} t_{2}^{2} \right] $$ $\sigma_{12} : = \text{cov} (X_{1} , X_{2} )$ 그리고 $\sigma_{21}</description>
    </item>
    
    <item>
      <title>번스타인 분포: 짝으로 독립이라고 상호 독립은 아니다</title>
      <link>https://freshrimpsushi.github.io/posts/bernstein-distribution/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bernstein-distribution/</guid>
      <description>정의 $(x,y,z) \in \left\{ (1,0,0), (0,1,0), (0,0,1), (1,1,1) \right\}$ 에 대해 다음과 같은 확률질량함수를 가지는 분포를 번스타인 분포Bernstein Distribution라고 한다. $$ p(x,y,z) = {{1} \over {4} } $$ 설명 번스타인 분포는 분포의 조건을 모두 만족시키고는 있지만 자연계에 실재하는 분포라고 보기는 어렵다. &amp;lsquo;짝으로 독립이면 상호 독립이다&amp;rsquo;라는 명제의 반례</description>
    </item>
    
    <item>
      <title>확률 변수들의 상호 독립과 iid</title>
      <link>https://freshrimpsushi.github.io/posts/mutual-independence-and-iid-independent-and-identically-distributed/</link>
      <pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/mutual-independence-and-iid-independent-and-identically-distributed/</guid>
      <description>정의 1 확률 변수 $X_{1} , \cdots , X_{n}$ 가 다음을 만족하면 $X_{1} , \cdots , X_{n}$ 이 짝으로 독립Pairwise Independent 이라고 한다. $$ i \ne j \implies X_{i} \perp X_{j} $$ 연속 확률 변수 $X_{1} , \cdots , X_{n}$ 의 조인트 확률 밀도 함수 $f$ 가 각각의 확률 밀도 함수 $f_{1} , \cdots , f_{n}$ 에 대해 다음을 만족하면 $X_{1} , \cdots , X_{n}$ 가 상호 독립 이라고 한다. $$ f(x_{1} , \cdots , x_{n} ) \equiv f_{1} (x_{1}) \cdots f_{n} (x_{n}) $$ 이산 확률 변수 $X_{1} , \cdots , X_{n}$ 의 조인트 확률 질</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률 변수의 독립</title>
      <link>https://freshrimpsushi.github.io/posts/independence-of-random-variable/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/independence-of-random-variable/</guid>
      <description>정의 1 두 확률 변수 $X_{1}, X_{2}$ 의 조인트 확률 밀도 함수 $f$ 혹은 확률 질량 함수 $p$ 에 대해 $X_{1}, X_{2}$ 의 확률 밀도 함수들 $f_{1}, f_{2}$ 혹은 확률 질량 함수 $p_{1}, p_{2}$ 가 다음을 만족하면 $X_{1}, X_{2}$ 가 독립 이라고 하고, $X_{1} \perp X_{2}$ 와 같이 나타낸다. $$ f(x_{1} , x_{2} ) \equiv f_{1}(x_{1})f_{2}(x_{2}) \\ p(x_{1} , x_{2} ) \equiv p_{1}(x_{1})p_{2}(x_{2}) $$ 정리 아래의 정리는 이산 확률 변수에 대해서도 같지만, 편의상 연속 확률 변수인 경우만 언급한다. 다음은 모두 동치다.</description>
    </item>
    
    <item>
      <title>수리통계학에서의 조건부 확률 분포</title>
      <link>https://freshrimpsushi.github.io/posts/conditional-probability/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/conditional-probability/</guid>
      <description>정의 이산 확률 변수 $X_{1}, X_{2}, \cdots , X_{n}$ 에 대해 다음의 $p_{2, \cdots , n \mid 1}$ 를 $X_{1} = x_{1}$ 이 주어졌을 때의 $ X_{2}, \cdots , X_{n}$ 의 조인트 조건부 확률 질량 함수라고 한다. $$ p_{2, \cdots , n \mid 1} ( x_{2} , \cdots ,x_{n} \mid X_{1} = x_{1} ) = {{ p_{1, \cdots , n}(x_{1} , x_{2} , \cdots , x_{n}) } \over { p_{1}( X_{1} = x_{1} ) }} $$ 연속 확률 변수 $X_{1}, X_{2}, \cdots , X_{n}$ 에 대해 다음의 $f_{2, \cdots , n \mid 1}$ 를 $X_{1} = x_{1}$ 이 주어졌을 때의 $ X_{2}, \cdots , X_{n}$ 의 조인트 조건부 확률 밀도 함수</description>
    </item>
    
    <item>
      <title>다변량 확률 변수의 변환</title>
      <link>https://freshrimpsushi.github.io/posts/transform-of-random-variable/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/transform-of-random-variable/</guid>
      <description>공식 다변량 확률 변수 $X = ( X_{1} , \cdots , X_{n} )$ 의 조인트 확률밀도함수 $f$ 가 $f(x_{1} , \cdots , x_{n})$ 와 같이 주어져있다고 하고 다음과 같은 변환을 생각해보자. $$ y_{1} = u_{1} (x_{1} , \cdots , x_{n}) \\ \vdots \\ y_{n} = u_{n} (x_{1} , \cdots , x_{n}) $$ 이러한 변환 $u_{1} , \cdots , u_{n}$ 는 단사가 아닐 수 있다. 따라서 $X$ 의 서포트 $S_{X}$ 는 $k$ 개의 파티션 $A_{1} , \cdots , A_{i} , \cdots , A_{k}$ 으로 나누어지고, 다음과 같은 역변환 $w_{ji} \mid_{i=1,\cdots,k \\ j=1,\cdots,n}$ 들을 생각</description>
    </item>
    
    <item>
      <title>수리통계학에서의 다변량 확률 분포</title>
      <link>https://freshrimpsushi.github.io/posts/multivariate-distribution/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/multivariate-distribution/</guid>
      <description>정의 1 표본 공간 $\Omega$ 에서 정의된 $n$ 개의 확률 변수 $X_{i}$ 에 대해 $X = (X_{1} , \cdots , X_{n})$ 를 $n$차원 랜덤 벡터Random Vector라고 한다. $X$ 의 치역 $X(\Omega)$ 를 공간이라고도 부른다. 다음을 만족하는 함수 $F_{X} : \mathbb{R}^{n} \to [0,1]$ 을 $X$ 의 조인트Joint 누적 분포 함수라고 한다. $$ F_{X}\left( x_{1}, \cdots , x_{n} \right) := P \left[ X_{1} \le x_{1} , \cdots , X_{n} \le x_{n} \right] $$ 어떤 $h_{1} , \cdots , h_{n} &amp;gt;0$ 들에 대해 다음을 만족하는</description>
    </item>
    
    <item>
      <title>옌센 부등식의 기댓값 폼 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-expectation-form-of-jensens-inequality/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-expectation-form-of-jensens-inequality/</guid>
      <description>정리 1 개구간 $I$ 에서 함수 $\phi$ 가 컨벡스하고 두 번 미분가능, 확률변수 $X$의 기댓값 $\mu$ 가 존재하며 $X \subset I $ 면 $$ \phi [ E(X) ] \le E [ \phi(X)] $$ 다른 형태 옌센 부등식의 유한 폼 옌센 부등식의 적분 폼 조건부 옌센 부등식 적분 폼과는 상당히 유사한 형태를 가지고 있다. 잘 생각해보면 유한 폼 역시 항이 무한하지는 않지만 가중평균의 부등식이라는 센스에서 기댓값이라고 볼</description>
    </item>
    
    <item>
      <title>체비셰프 부등식 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-chebyshevs-inequality/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-chebyshevs-inequality/</guid>
      <description>정리 1 확률변수 $X$ 의 분산 $\sigma^2 &amp;lt; \infty$ 가 존재하면 $\mu := E(X)$ 와 어떤 양수 $k&amp;gt;0$ 에 대해 $$ \displaystyle P(|X-\mu| \ge k\sigma) \le {1 \over k^2} $$ 설명 비교적 형태가 간단하고 식의 조작이 쉬운데다 결과도 한 눈에 들어오기 때문에 보조정리로써 많이 쓰인다. 다만 마코프 부등식과 비교하자면 분산이 존재해야한다는 조건이 하나 더 있다. 조건에서 $2$차 적률이 존재해야하는 것을 보고 너무 쉽고 당연한 조건</description>
    </item>
    
    <item>
      <title>마코프 부등식 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-markovs-inequality/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-markovs-inequality/</guid>
      <description>정리 1 확률변수 $X$ 에 대해 함수 $u(X) \ge 0$ 를 정의하자. $E \left( u(X) \right)$ 가 존재하면 $c &amp;gt; 0$ 에 대해 $$ \displaystyle P(u(X) \ge c) \le {E \left( u(X) \right) \over c} $$ 설명 수많은 증명에 사용되는 보조정리로써 이를 좀 더 편리하게 만든 체비셰프 부등식이 있다. 조건에서 $1$차 적률이 존재해야하는 것을 보고 너무 쉽고 당연한 조건으로 여길지 모르겠다. 뭐 어느정도는 맞는 말이지만, 학부생 정도 됐다면</description>
    </item>
    
    <item>
      <title>n차 적률이 존재하면 차수가 n보다 작은 적률도 존재한다</title>
      <link>https://freshrimpsushi.github.io/posts/if-nth-moment-exists-then-moments-with-less-degree-are-exist/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/if-nth-moment-exists-then-moments-with-less-degree-are-exist/</guid>
      <description>정리 확률변수 $X$와 자연수 $n$에 대해 $E( X^n )$ 이 존재하면 $E( X^m ), m=1,2,3,\cdots, n$ 도 존재한다. 설명 어떤 차수의 적률이든 존재하기만 한다면 그보다 작은 차수의 적률은 항상 존재하지만, 당연히 역은 성립하지 않는다. 물론 실제로 문제를 접해보면 높은 차수의 적률이 먼저 주어지는 경우는 거의 없으나, 어떤 정리의 조건을 나열할 때 지면을 상당히 절약할 수 있게 해주</description>
    </item>
    
    <item>
      <title>적률생성함수란?</title>
      <link>https://freshrimpsushi.github.io/posts/moment-generating-function/</link>
      <pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/moment-generating-function/</guid>
      <description>정의 1 확률변수 $X$ 와 어떤 양수 $h&amp;gt;0$ 대해 $E(e^{tX})$ 이 $-h&amp;lt; t &amp;lt; h$ 에서 존재하면 $M(t) = E( e^{tX} )$ 를 $X$ 의 적률생성함수Moment Generating Function라고 정의한다. 설명 적률생성함수는 흔히 mgf라는 약어로 많이 쓰인다. 수리통계학에서는 비교적 초반에 배우는데, 생소한 정의와 맥락 없는 등장 때문에 수리통계학을 싫어지게 만드는 주범 중 하나다. 적률생성함수를</description>
    </item>
    
    <item>
      <title>수리통계학에서의 첨도</title>
      <link>https://freshrimpsushi.github.io/posts/kurtosis/</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/kurtosis/</guid>
      <description>첨도 확률변수 $X$ 의 평균이 $\mu$, 분산이 $\sigma^2$ 라고 할 때 다음과 같이 정의된 $\gamma_{2}$ 를 $X$ 의 첨도kurtosis라고 한다. $$ \gamma_{2} := {{ E \left( X - \mu \right)^4 } \over { \sigma^4 }} $$ 데이터 $\left\{ X_{i} \right\}_{i}^{n}$ 의 표본평균이 $\overline{X}$, 표본분산이 $\widehat{\sigma}^2$ 이라고 할 때 표본첨도 $g_{2}$ 는 다음과 같이 구해진다. $$ g_{2} := \sum_{i=1}^{n} = {{ \left( X - \overline{X} \right)^4 } \over { n \widehat{\sigma}^4 }} - 3 $$ 설명 첨도는 4차 적률로 구해지며, 확률변수의 분포함수가 얼마</description>
    </item>
    
    <item>
      <title>수리통계학에서의 왜도</title>
      <link>https://freshrimpsushi.github.io/posts/skewness/</link>
      <pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/skewness/</guid>
      <description>정의 확률변수 $X$ 의 평균이 $\mu$, 분산이 $\sigma^2$ 라고 할 때 다음과 같이 정의된 $\gamma_{1}$ 를 $X$ 의 왜도Skewness라고 한다. $$ \gamma_{1} := {{ E \left( X - \mu \right)^3 } \over { \sigma^3 }} $$ 데이터 $\left\{ X_{i} \right\}_{i}^{n}$ 의 표본평균이 $\overline{X}$, 표본분산이 $\widehat{\sigma}^2$ 이라고 할 때 표본왜도 $g_{1}$ 은 다음과 같이 구해진다. $$ g_{1} := \sum_{i=1}^{n} {{ \left( X - \overline{X} \right)^3 } \over { n \widehat{\sigma}^3 }} $$ 설명 왜도는 3차 적률로 구해지며, 확률변수의 분포함수가 어떻게 치우</description>
    </item>
    
    <item>
      <title>공분산의 여러가지 성질들</title>
      <link>https://freshrimpsushi.github.io/posts/properties-of-covariance/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/properties-of-covariance/</guid>
      <description>정의와 성질 평균이 각각 $\mu_{X}$, $\mu_{Y}$ 인 확률 변수 $X$, $Y$ 에 대해 $\text{Cov} (X ,Y) : = E \left[ ( X - \mu_{X} ) ( Y - \mu_{Y} ) \right] $ 을 $X$ 와 $Y$ 의 공분산Covariance이라고 정의한다. 공분산은 아래의 성질들을 가진다. [1]: $\text{Var} (X) = \text{Cov} (X,X)$ [2]: $\text{Cov} (X,Y) = \text{Cov} (Y, X) $ [3]: $\text{Var} (X + Y) = \text{Var} (X) + \text{Var} (Y) + 2 \text{Cov} (X,Y) $ [4]: $\text{Cov} (X + Y , Z ) = \text{Cov}(X,Z) + \text{Cov}(Y,Z) $ [5]: $\text{Cov} (aX + b , cY + d ) = ac \text{Cov}(X,Y) $ 설명 공분산은 두 변수의 선</description>
    </item>
    
    <item>
      <title>피어슨 상관계수</title>
      <link>https://freshrimpsushi.github.io/posts/pearson-correlation-coefficient/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/pearson-correlation-coefficient/</guid>
      <description>정의 1 다음과 같이 정의된 $\rho = \rho(X,Y)$ 를 피어스 상관계수라고 한다. $$ \rho = { {\text{Cov} (X,Y)} \over {\sigma_X \sigma_Y} } $$ 설명 (피어슨) 상관 계수(Pearson) Correlation Coefficient는 두 변수가 서로 (선형) 상관 관계 를 가지고 있는지를 확인하는 척도가 된다. $1$ 이나 $–1$ 에 가까우면 상관관계가 있다고 보고 $0$ 이면 없다고 본다. 주의할 것은 상관관계와 독립이 같은 개</description>
    </item>
    
    <item>
      <title>평균과 분산의 성질들</title>
      <link>https://freshrimpsushi.github.io/posts/properties-of-mean-and-variance/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/properties-of-mean-and-variance/</guid>
      <description>정리 평균 $E ( X ) = \mu_{X} $ 과 분산 $\text{Var} (X) = E [ ( X - \mu_{X} )^2 ] $ 은 아래의 성질들을 가진다. [1]: $E(X + Y) = E(X) + E(Y)$ [2]: $E(aX + b) = a E(X) + b$ [3]: $\text{Var} (X) \ge 0$ [4]: $\text{Var} ( X ) = E(X^2) - \mu_{X}^2$ [5]: $\text{Var} (aX + b) = a^2 \text{Var} (X)$ 설명 평균과 분산에 관한 것이니만큼 아주 중요한 성질들이다. 특히 [1]과 [2]는 이른바 선형성Linearity이라 불리우는 성질로써, 수식을 다룰 때 무척 편리</description>
    </item>
    
    <item>
      <title>대표값의 수리적 성질 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-mathematical-property-of-representative-value/</link>
      <pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-mathematical-property-of-representative-value/</guid>
      <description>정리 데이터 $X = \left\{ x_{1} , \cdots , x_{n} \right\} $ 가 주어져 있다고 하자. [0]: $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{0}$ 가 최소가 되도록 하는 $\theta$ 는 $$ \text{argmin}_{\theta} h \left( \theta \right) = \text{mode}(X) $$ [1]: $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{1}$ 가 최소가 되도록 하는 $\theta$ 는 $$ \text{argmin}_{\theta} h \left( \theta \right) = \text{median}(X) $$ [2]: $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{2}$ 가 최소가 되도록 하는 $\theta$ 는 $$ \text{argmin}_{\theta} h \left( \theta \right) = \text{mean}(X) $$ 설명 선형대수의 용어로 어렵게 말해보자면 다음과 같다 : [0]: $l^{0}$-놈을 최소화하는 것은 최빈값이다</description>
    </item>
    
    <item>
      <title>수리통계학에서의 기대값, 평균, 분산, 적률의 정의</title>
      <link>https://freshrimpsushi.github.io/posts/expectation-mean-variance-moment/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/expectation-mean-variance-moment/</guid>
      <description>정의: 기대값, 평균, 분산 확률 변수 $X$ 가 주어져 있다고 하자. 연속 확률 변수 $X$ 의 확률 밀도 함수 $f(x)$ 가 $\displaystyle \int_{-\infty}^{\infty} |x| f(x) dx &amp;lt; \infty$ 를 만족하면 다음과 같이 정의된 $E(X)$ 를 $X$ 의 기대값Expectation이라고 한다. $$ E(X) := \int_{-\infty}^{\infty} x f(x) dx $$ 이산 확률 변수 $X$ 의 확률 질량 함수 $p(x)$ 가 $\displaystyle \sum_{x} |x| p(x) &amp;lt; \infty$ 를 만족하면 다음과 같이 정의된 $E(X)$ 를 $X$ 의 기대값Expectation이라</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률 변수와 확률 분포</title>
      <link>https://freshrimpsushi.github.io/posts/random-variable-and-probability-distribution/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/random-variable-and-probability-distribution/</guid>
      <description>정의 1 표본 공간 $\Omega$ 에서 확률 $P$ 가 정의되어 있다고 하자. 정의역이 표본 공간인 함수 $X : \Omega \to \mathbb{R}$ 을 확률 변수Random Variable라고 한다. 확률 변수의 치역 $X(\Omega)$ 을 공간Space이라고도 부른다. 다음을 만족하는 함수 $F_{X} : \mathbb{R} \to [0,1]$ 을 $X$ 의 누적분포함수(Cummulative Distribution Function, cdf) 라고 한다. $$ F_{X}(x) = P_{X}\left( (-\infty,x] \right) = P \left( \left\{ \omega \in \Omega : X(\omega) \le x \right\} \right)</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률과 확률의 덧셈법칙</title>
      <link>https://freshrimpsushi.github.io/posts/probability-and-additive-law-of-probability/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/probability-and-additive-law-of-probability/</guid>
      <description>정의 1 같은 조건 하에서 반복할 수 있는 시행을 임의 시행Random Experiment이라고 한다. 임의 시행에서 얻을 수 있는 모든 결과Outcome를 모아놓은 집합 $\Omega$ 를 표본 공간Sample Space이라고 한다. 표본 공간에서 우리가 관심을 가지는 결과들의 집합, 즉 $B \subset \Omega$ 를 사건Event이라 하고 이들의 집합을 $\mathcal{B}$ 와 같이 나타낸다.</description>
    </item>
    
    <item>
      <title>통계학의 세가지 대표값: 최빈값, 중앙값, 평균</title>
      <link>https://freshrimpsushi.github.io/posts/mode-median-mean/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/mode-median-mean/</guid>
      <description>개요 대표값은 데이터를 설명하는 대표적인 값을 말한다. 수천 수만에 달하는 데이터가 있어도 일일이 다 살펴볼 게 아니라면 결국 중요한 것은 데이터가 무엇을 의미하느냐고, 대표값은 이를 효과적으로 요약한다. 그 중 가장 자주 쓰이는 세가지 대표값으로써 최빈값, 중앙값, 평균이 있다. (0) 최빈값: 표본에서 가장 자주 발생한 값 (1) 중앙값: 표본에서 중앙에 위</description>
    </item>
    
    <item>
      <title>몬테카를로 방법과 부트스트랩의 차이점</title>
      <link>https://freshrimpsushi.github.io/posts/difference-between-monte-carlo-method-and-bootstrap/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/difference-between-monte-carlo-method-and-bootstrap/</guid>
      <description>개요 몬테카를로 방법은 작위적인 데이터로 시뮬레이션을 반복해 새로운 기법을 확인하는 방법이고 부트스트랩은 실제 데이터에서 재표본 추출을 통해 비용을 절감하며 문제를 해결하려는 방법이다. 정의 몬테카를로 방법Monte Carlo Method이란 난수 추출을 통해 관심 있는 대상에 대해 점추정량을 찾는 방법이다. 부트스트랩Bootstrap이란 표</description>
    </item>
    
    <item>
      <title>표본표준편차와 표준오차의 구분</title>
      <link>https://freshrimpsushi.github.io/posts/how-different-between-sample-standard-deviation-and-standard-error/</link>
      <pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/how-different-between-sample-standard-deviation-and-standard-error/</guid>
      <description>정의 $X$ 로부터 얻은 데이터를 $\mathbb{x} = ( x_{1}, x_{2}, \cdots , x_{n} )$ 라고 하자. 표본평균: $$ \overline{x} = {{1} \over {n}} \sum_{i=1}^{n} x_{i} $$ 표본표준편차: $$ s_{x} = \sqrt { {{1} \over {n-1}} \sum_{i=1}^{n} ( x_{i} - \overline{x} )^2 } $$ 표준오차: $$ \text{s.e.}( \hat{x} ) = {{ s_{x} } \over { \sqrt{n} }} $$ 설명 말이 비슷해서인지 의외로 많은 사람들이 표본표준편차와 표준오차를 구분하지 못한다. 사실상 통계를 글로만 배우는 고등학생들은 물론이고 심하게는 통계학과</description>
    </item>
    
    <item>
      <title>특정한 분포를 따르는 확률변수들의 덧셈 총정리</title>
      <link>https://freshrimpsushi.github.io/posts/sum-of-some-probability-distribution/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/sum-of-some-probability-distribution/</guid>
      <description>정리 확률 변수 $X_{1} , \cdots , X_{n}$ 들이 상호 독립이라고 하자. [1] 이항 분포: $X_i \sim \text{Bin} ( n_{i}, p)$ 이면 $$ \displaystyle \sum_{i=1}^{m} X_{i} \sim \text{Bin} \left( \sum_{i=1}^{m} n_{i} , p \right) $$ [2] 푸아송 분포: $X_i \sim \text{Poi}( m_{i} )$ 이면 $$ \displaystyle \sum_{i=1}^{n} X_{i} \sim \text{Poi} \left( \sum_{i=1}^{n} m_{i} \right) $$ [3] 감마 분포: $X_i \sim \Gamma( k_{i}, \theta)$ 이면 $$ \displaystyle \sum_{i=1}^{n} X_{i} \sim \Gamma \left( \sum_{i=1}^{n} k_{i} , \theta \right) $$ [4] 카이제곱 분포: $X_i \sim \chi^2 ( r_{i} )$ 이면 $$ \displaystyle \sum_{i=1}^{n} X_{i} \sim \chi ^2 \left( \sum_{i=1}^{n} r_{i} \right) $$ [5] 정규 분포: $X_i \sim N( \mu_{i}, \sigma_{i}^{2} )$ 이면 주어진 벡터 $(a_{1} ,</description>
    </item>
    
  </channel>
</rss>
