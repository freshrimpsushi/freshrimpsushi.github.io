<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>수리통계학 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/</link>
    <description>Recent content in 수리통계학 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Sun, 15 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>수리통계학에서의 확률 수렴</title>
      <link>https://freshrimpsushi.github.io/posts/convergence-in-probability/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/convergence-in-probability/</guid>
      <description>측도론으로 정의되는 확률 수렴**확률 수렴의 쉬운 정의1 확률변수 $X$ 와 확률 변수의 시퀀스 $\left\{ X_{n} \right\}$ 가 다음을 만족하면 $n \to \infty$ 일 때 $X_{n}$ 이 $X$ 로 **확률 수렴** 한다고 말하고, $X_{n} \overset{P}{\to} X$ 와 같이 나타낸다. $$ \forall \varepsilon &amp;gt; 0 , \lim_{n \to \infty} P \left[ \left| X_{n} - X \right| &amp;lt; \varepsilon \right] = 1 $$ 확률 수렴의 조건은 수식 그대로 확률의 센스에서 수렴을 정의한 것으로, 쉽게 말해 $n$ 이 커지면 두 확률 변</description>
    </item>
    
    <item>
      <title>순서통계량</title>
      <link>https://freshrimpsushi.github.io/posts/order-statistics/</link>
      <pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/order-statistics/</guid>
      <description>**순서통계량의 확률밀도함수1 랜덤 샘플 $X_{1} , \cdots , X_{n}$ 가 서포트 $\mathcal{S} =(a,b)$ 인 확률밀도함수 $f(x)$ 를 가지는 연속확률분포를 따른다고 하자. 이들을 크기 순으로 나열한 확률 변수들을 $Y_{1} &amp;lt; \cdots &amp;lt; Y_{n}$ 와 같이 나타내도록 하면 그 조인트, 마지널 확률밀도함수들은 다음과 같다.**[1] 조인트** : $$ g \left( y_{1} , \cdots , y_{n} \right) = \begin{cases} n! f (y_{1}) \cdots f (y_{n}) &amp;amp; a &amp;lt; y_{1} &amp;lt; \cdots &amp;lt; y_{n} &amp;lt; b \\ 0</description>
    </item>
    
    <item>
      <title>표본 분산을 n-1으로 나누는 이유 Why Sample Variance is Divided by n-1</title>
      <link>https://freshrimpsushi.github.io/posts/1747/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/1747/</guid>
      <description>$X_{i} \sim \left( \mu , \sigma^{2} \right)$ 이라고 하면 표본 분산 $S^{2}$ 는 다음과 같다. $$ S^{2} := {{1} \over {n-1}} \sum_{i=1}^{n} \left( X_{i} - \overline{X} \right)^{2} $$ 알다시피 표본 평균과 달리 표본 분산은 편차의 제곱을 모두 더한 후 표본 크기인 $n$ 이 아니라 $n-1$ 로 나눈다. 당연히 이를 이상하게 느껴야한다고는 말하지 않겠지만, 수식에 대한 보편적인 감성이 있다면 $n$ 개를 더하고 $n-1$ 로 나누는 것에서 강렬한 띠꺼움을 느끼는 것 역시 정상이다</description>
    </item>
    
    <item>
      <title>불편추정량</title>
      <link>https://freshrimpsushi.github.io/posts/unbiased-estimator/</link>
      <pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/unbiased-estimator/</guid>
      <description>**불편성1 $\theta$ 의 추정량 $T$ 가 다음을 만족하면 $T$ 를 $\mu$ 의 불편추정량 이라고 한다. $$ E T = \theta $$ 불편성이란 편의를 가지지 않는 성질을 말한다. 가령 $X_{i} \sim \left( \mu , \sigma^{2} \right)$ 라고 할 때 $\mu$ 의 추정량으로써 표본평균 $\displaystyle \overline{X} = {{ 1 } \over { n }} \sum_{i} X_{i} $ 를 사용한다면 $\displaystyle E \overline{X} = \mu$ 이므로 $\overline{X}$ 는 $\mu$ 의 불편추정량이 된다. 이는 언뜻 당연해보이지만 추정량이 모수를 정확하게 찍어</description>
    </item>
    
    <item>
      <title>편의-분산 트레이드 오프</title>
      <link>https://freshrimpsushi.github.io/posts/bias-variance-trade-off/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bias-variance-trade-off/</guid>
      <description>$$ \text{MSE} \left( \widehat{\theta} \right) = \text{Var} \widehat{\theta} + \left( \text{Bias} \widehat{\theta} \right)^{2} $$ 평균제곱오차 $\text{MSE}$ 는 통계 모형의 평가나 머신 러닝에서의 손실 함수로써 즐겨쓰이는 척도로써, 특히 편의와 분산에 대한 트레이드 오프로 나타난다.통계학도에게 있어서 편의를 다루는 것은 다소 어색할지 모르겠다. 적절한 확률 분포를 가정하고 그에 따른 수학적 이론을 토대로 데이터를 다루는 입장에서 분산은 손에 잡힐듯이 친숙</description>
    </item>
    
    <item>
      <title>수리통계학에서의 편의</title>
      <link>https://freshrimpsushi.github.io/posts/bias-in-mathematical-statistics/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bias-in-mathematical-statistics/</guid>
      <description>모수 $\theta$ 에 대한 추정량 $\widehat{\theta}$ 에 대해 다음과 같이 정의된 $\text{Bias}$ 를 편의 라고 한다. $$ \text{Bias} ( \theta ) = E(\widehat{\theta}) - \theta $$ Bias는 편의 또는 편향으로 순화되지만, 역시 가장 많이 쓰이는 말은 발음 그대로 읽은 [바이어스]다. 한국어에서 편의는 Convenience인 경우가 압도적으로 많고 수식적으로나 실제 쓰임새로나 &amp;lsquo;편향&amp;rsquo;으로 순화하는</description>
    </item>
    
    <item>
      <title>수리통계학에서의 랜덤 샘플</title>
      <link>https://freshrimpsushi.github.io/posts/random-sample/</link>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/random-sample/</guid>
      <description>**랜덤 샘플1 1. 확률 변수 $X$ 가 실제로 뽑힌 것을 실현Realization 이라 하고 보통 소문자 $x$ 로 나타낸다.2. 확률 변수 $X$ 와 같은 확률 분포에서 샘플 사이즈Sample Size $n$ 만큼 얻어낸 확률 변수들을 샘플Sample 이라 하고 다음과 같이 나타낸다. $$ X_{1} , X_{2} , \cdots , X_{n} $$ **3.** 확률 변수 $X_{1} , \cdots , X_{n}$ 이 iid면 사이즈 $n$ 의 **랜덤 샘플** 이</description>
    </item>
    
    <item>
      <title>확률 변수들의 선형 결합</title>
      <link>https://freshrimpsushi.github.io/posts/linear-combinations-of-random-variable/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/linear-combinations-of-random-variable/</guid>
      <description>특정한 분포를 따르는 확률변수들의 덧셈확률 변수 $X_{1} , \cdots , X_{n}$ 가 어떤 $(a_{1}, \cdots , a_{n}) \in \mathbb{R}^{n}$ 에 대해 $\displaystyle T := \sum_{i=1}^{n} a_{i} X_{i}$ 를 **선형 결합** 이라고 한다.특히 $X_{1} , \cdots , X_{n}$ 이 iid면 사이즈 $n$ 의 **랜덤 샘플Random Sample** 이라고도 부른다. 통계학의 맥락이라면 모든 관측값에 같은 가중치가 곱해진 $a_{1} = \cdots = a_{n} $ 을 생각할 것이다. 해석학과 선형대수, 분포 이론을 넘</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률 변수의 독립</title>
      <link>https://freshrimpsushi.github.io/posts/independence-of-random-variable/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/independence-of-random-variable/</guid>
      <description>측도론으로 정의되는 확률 변수의 독립두 확률 변수 $X_{1}, X_{2}$ 의 조인트 확률 밀도 함수 $f$ 혹은 확률 질량 함수 $p$ 에 대해 $X_{1}, X_{2}$ 의 확률 밀도 함수들 $f_{1}, f_{2}$ 혹은 확률 질량 함수 $p_{1}, p_{2}$ 가 다음을 만족하면 $X_{1}, X_{2}$ 가 **독립** 이라고 하고, $X_{1} \perp X_{2}$ 와 같이 나타낸다. $$ f(x_{1} , x_{2} ) \equiv f_{1}(x_{1})f_{2}(x_{2}) \\ p(x_{1} , x_{2} ) \equiv p_{1}(x_{1})p_{2}(x_{2}) $$ 아래의 정리는 이산 확률 변수에 대해서도 같지만, 편의상 연속 확률 변수인 경우</description>
    </item>
    
    <item>
      <title>수리통계학에서의 조건부 확률 분포</title>
      <link>https://freshrimpsushi.github.io/posts/conditional-probability/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/conditional-probability/</guid>
      <description>측도론으로 정의되는 조건부 확률 분포측도론으로 정의되는 조건부 기대값1. 이산 확률 변수 $X_{1}, X_{2}, \cdots , X_{n}$ 에 대해 다음의 $p_{2, \cdots , n \mid 1}$ 를 $X_{1} = x_{1}$ 이 주어졌을 때의 $ X_{2}, \cdots , X_{n}$ 의 **조인트 조건부 확률 질량 함수** 라고 한다. $$ p_{2, \cdots , n \mid 1} ( x_{2} , \cdots ,x_{n} \mid X_{1} = x_{1} ) = {{ p_{1, \cdots , n}(x_{1} , x_{2} , \cdots , x_{n}) } \over { p_{1}( X_{1} = x_{1} ) }} $$ **2.** 연속 확률 변수 $X_{1}, X_{2}, \cdots , X_{n}$ 에 대해 다음</description>
    </item>
    
    <item>
      <title>확률 변수의 변환</title>
      <link>https://freshrimpsushi.github.io/posts/transform-of-random-variable/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/transform-of-random-variable/</guid>
      <description>다변량 확률 변수 $X = ( X_{1} , \cdots , X_{n} )$ 의 조인트 확률밀도함수 $f$ 가 $f(x_{1} , \cdots , x_{n})$ 와 같이 주어져있다고 하고 다음과 같은 변환을 생각해보자. $$ y_{1} = u_{1} (x_{1} , \cdots , x_{n}) \\ \vdots \\ y_{n} = u_{n} (x_{1} , \cdots , x_{n}) $$ 이러한 변환 $u_{1} , \cdots , u_{n}$ 는 단사가 아닐 수 있다. 따라서 $X$ 의 서포트 $S_{X}$ 는 $k$ 개의 파티션 $A_{1} , \cdots , A_{i} , \cdots , A_{k}$ 으로 나누어지고, 다음과 같은 역변환 $w_{ji} \mid_{i=1,\cdots,k \\ j=1,\cdots,n}$ 들을 생각할 수</description>
    </item>
    
    <item>
      <title>마코프 부등식 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-markovs-inequality/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-markovs-inequality/</guid>
      <description>확률변수 $X$ 에 대해 함수 $u(X) \ge 0$ 를 정의하자. $E(u(X))$ 가 존재하면 $c &amp;gt; 0$ 에 대해 $$ \displaystyle P(u(X) \ge c) \le {E(u(X)) \over c} $$ 수많은 증명에 사용되는 보조정리로써 이를 좀 더 편리하게 만든 체비셰프 부등식이 있다.조건에서 $1$차 적률이 존재해야하는 것을 보고 너무 쉽고 당연한 조건으로 여길지 모르겠다. 뭐 어느정도는 맞는 말이지만, 학부생 정도 됐다면 그 존재성이라는 게 아주 당</description>
    </item>
    
    <item>
      <title>n차 적률이 존재하면 차수가 n보다 작은 적률도 존재한다</title>
      <link>https://freshrimpsushi.github.io/posts/247/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/247/</guid>
      <description>확률변수 $X$와 자연수 $n$에 대해 $E( X^n )$ 이 존재하면 $E( X^m ), m=1,2,3,\cdots, n$ 도 존재한다.어떤 차수의 적률이든 존재하기만 한다면 그보다 작은 차수의 적률은 항상 존재하지만, 당연히 역은 성립하지 않는다. 물론 실제로 문제를 접해보면 높은 차수의 적률이 먼저 주어지는 경우는 거의 없으나, 어떤 정리의 조건을 나열할 때 지면을 상당히 절약할 수 있게 해주는 정리긴</description>
    </item>
    
    <item>
      <title>적률생성함수란</title>
      <link>https://freshrimpsushi.github.io/posts/moment-generating-function/</link>
      <pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/moment-generating-function/</guid>
      <description>확률변수 $X$ 와 어떤 양수 $h&amp;gt;0$ 대해 $E(e^{tX})$ 이 $-h&amp;lt; t &amp;lt; h$ 에서 존재하면 $M(t) = E( e^{tX} )$ 를 $X$ 의 적률생성함수 라고 정의한다.적률생성함수는 흔히 mgf 라는 약어로 많이 쓰인다. 수리통계학에서는 비교적 초반에 배우는데, 생소한 정의와 맥락 없는 등장 때문에 수리통계학을 싫어지게 만드는 주범 중 하나다. 적률생성함수를 이해하는게 어려운 것은 보통 교재의 구성 상 대뜸 정의</description>
    </item>
    
    <item>
      <title>수리통계학에서의 첨도</title>
      <link>https://freshrimpsushi.github.io/posts/kurtosis/</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/kurtosis/</guid>
      <description>1. 확률변수 $X$ 의 평균이 $\mu$, 분산이 $\sigma^2$ 라고 할 때 다음과 같이 정의된 $\gamma_{2}$ 를 $X$ 의 첨도라고 한다. $$ \gamma_{2} := {{ E \left( X - \mu \right)^4 } \over { \sigma^4 }} $$ **2.** 데이터 $\left\{ X_{i} \right\}_{i}^{n}$ 의 표본평균이 $\overline{X}$, 표본분산이 $\widehat{\sigma}^2$ 이라고 할 때 표본첨도 $g_{2}$ 는 다음과 같이 구해진다. $$ g_{2} := \sum_{i=1}^{n} = {{ \left( X - \overline{X} \right)^4 } \over { n \widehat{\sigma}^4 }} - 3 $$ 첨도는 4차 적률로 구해지며, 확률변수의 분포함수가 얼마나 뾰족하게 생겼는지에</description>
    </item>
    
    <item>
      <title>수리통계학에서의 왜도</title>
      <link>https://freshrimpsushi.github.io/posts/skewness/</link>
      <pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/skewness/</guid>
      <description>1. 확률변수 $X$ 의 평균이 $\mu$, 분산이 $\sigma^2$ 라고 할 때 다음과 같이 정의된 $\gamma_{1}$ 를 $X$ 의 왜도라고 한다. $$ \gamma_{1} := {{ E \left( X - \mu \right)^3 } \over { \sigma^3 }} $$ **2.** 데이터 $\left\{ X_{i} \right\}_{i}^{n}$ 의 표본평균이 $\overline{X}$, 표본분산이 $\widehat{\sigma}^2$ 이라고 할 때 표본왜도 $g_{1}$ 은 다음과 같이 구해진다. $$ g_{1} := \sum_{i=1}^{n} {{ \left( X - \overline{X} \right)^3 } \over { n \widehat{\sigma}^3 }} $$ 왜도는 3차 적률로 구해지며, 확률변수의 분포함수가 어떻게 치우쳐져 있는지에 대한 척도</description>
    </item>
    
    <item>
      <title>공분산의 여러가지 성질들</title>
      <link>https://freshrimpsushi.github.io/posts/properties-of-covariance/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/properties-of-covariance/</guid>
      <description>평균이 각각 $\mu_{X}$, $\mu_{Y}$ 인 확률 변수 $X$, $Y$ 에 대해 $\text{Cov} (X ,Y) : = E \left[ ( X - \mu_{X} ) ( Y - \mu_{Y} ) \right] $ 을 $X$ 와 $Y$ 의 **공분산** 이라고 정의한다. 공분산은 아래의 성질들을 가진다.**[1]** $\text{Var} (X) = \text{Cov} (X,X)$**[2]** $\text{Cov} (X,Y) = \text{Cov} (Y, X) $**[3]** $\text{Var} (X + Y) = \text{Var} (X) + \text{Var} (Y) + 2 \text{Cov} (X,Y) $**[4]** $\text{Cov} (X + Y , Z ) = \text{Cov}(X,Z) + \text{Cov}(Y,Z) $**[5]** $\text{Cov} (aX + b , cY + d ) = ac \text{Cov}(X,Y) $공분산은 두 변수의 선형상관관계를 나타내며,</description>
    </item>
    
    <item>
      <title>피어슨 상관계수</title>
      <link>https://freshrimpsushi.github.io/posts/pearson-correlation-coefficient/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/pearson-correlation-coefficient/</guid>
      <description>다음과 같이 정의된 $\rho = \rho(X,Y)$ 를 피어스 상관계수라고 한다. $$ \displaystyle \rho = { {\text{Cov} (X,Y)} \over {\sigma_X \sigma_Y} } $$ (피어슨) 상관 계수 는 두 변수가 서로 (선형) 상관 관계 를 가지고 있는지를 확인하는 척도가 된다. $1$ 이나 $–1$ 에 가까우면 상관관계가 있다고 보고 $0$ 이면 없다고 본다. $[-1, 1]$ 를 벗어나지는 않는다. $$ –1 \le \rho \le 1 $$ 주의할 것은 상관관계와 독립이 같은 개념이 아니라</description>
    </item>
    
    <item>
      <title>평균과 분산의 성질들</title>
      <link>https://freshrimpsushi.github.io/posts/properties-of-mean-and-variance/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/properties-of-mean-and-variance/</guid>
      <description>평균 $E ( X ) = \mu_{X} $ 과 분산 $\text{Var} (X) = E [ ( X - \mu_{X} )^2 ] $ 은 아래의 성질들을 가진다.**[1]** $E(X + Y) = E(X) + E(Y)$**[2]** $E(aX + b) = a E(X) + b $**[3]** $\text{Var} (X) \ge 0$**[4]** $ \text{Var} ( X ) = E(X^2) - \mu_{X}^2 $**[5]** $ \text{Var} (aX + b) = a^2 \text{Var} (X)$평균과 분산에 관한 것이니만큼 아주 중요한 성질들이다. 특히 **[2]** 는 이른바 **선형성**Linearity 이라 불리우는 성질로써, 수식을 다룰 때 무</description>
    </item>
    
    <item>
      <title>대표값의 수리적 성질 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-mathematical-property-of-representative-value/</link>
      <pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-mathematical-property-of-representative-value/</guid>
      <description>통계학의 세가지 대표값 : 최빈값, 중앙값, 평균데이터 $X = \left\{ x_{1} , \cdots , x_{n} \right\} $ 가 주어져 있다고 하자.**[0]** $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{0}$ 가 최소가 되도록 하는 $\theta$ 는 $\text{mode}(X)$**[1]** $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{1}$ 가 최소가 되도록 하는 $\theta$ 는 $\text{median}(X)$**[2]** $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{2}$ 가 최소가 되도록 하는 $\theta$ 는 $\text{mean}(X)$선형대수의 용어로 어렵게 말해보자면 다음과 같다 :**(0)** $l^{0}$-놈을 최</description>
    </item>
    
    <item>
      <title>수리통계학에서의 기대값 평균 분산 적률의 정의</title>
      <link>https://freshrimpsushi.github.io/posts/expectation-mean-variance-moment/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/expectation-mean-variance-moment/</guid>
      <description>대표값으로써의 평균측도론으로 정의되는 기대값확률 변수 $X$ 가 주어져 있다고 하자.1. 연속 확률 변수 $X$ 의 확률 밀도 함수 $f(x)$ 가 $\displaystyle \int_{-\infty}^{\infty} |x| f(x) dx &amp;lt; \infty$ 를 만족하면 다음과 같이 정의된 $E(X)$ 를 $X$ 의 **기대값** 이라고 한다. $$ E(X) := \int_{-\infty}^{\infty} x f(x) dx $$ **2.** 이산 확률 변수 $X$ 의 확률 질량 함수 $p(x)$ 가 $\displaystyle \sum_{x} |x| p(x) &amp;lt; \infty$ 를 만족하면 다음과 같이 정의된 $E(X)$ 를 $X$ 의 **기대값** 이라고 한</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률 변수와 확률 분포</title>
      <link>https://freshrimpsushi.github.io/posts/1433/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/1433/</guid>
      <description>측도론으로 정의되는 확률 변수와 확률 분포측도론으로 정의되는 누적분포함수표본 공간 $\Omega$ 에서 확률 $P$ 가 정의되어 있다고 하자.1. 정의역이 표본 공간인 함수 $X : \Omega \to \mathbb{R}$ 을 확률 변수 라고 한다. 확률 변수의 치역 $X(\Omega)$ 을 공간Space 이라고도 부른다.2. 다음을 만족하는 함수 $F_{X} : \mathbb{R} \to [0,1]$ 을 $X$ 의 **누적분포함수(Cummulative Distribution Function, cdf)** 라고</description>
    </item>
    
    <item>
      <title>수리통계학에서의 확률과 확률의 덧셈법칙</title>
      <link>https://freshrimpsushi.github.io/posts/1431/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/1431/</guid>
      <description>측도론으로 엄밀하게 정의되는 확률1. 같은 조건 하에서 반복할 수 있는 시행을 임의 시행Random Experiment 이라고 한다. 임의 시행에서 얻을 수 있는 모든 결과Outcome 를 모아놓은 집합 $\Omega$ 를 표본 공간Sample Space 이라고 한다. 표본 공간에서 우리가 관심을 가지는 결과들의 집합, 즉 $B \subset \Omega$ 를 사건Event 이라 하고 이들의 집합을 $\mathcal{B}$ 와 같이 나타낸다</description>
    </item>
    
    <item>
      <title>수리통계학에서의 다변량 확률 분포</title>
      <link>https://freshrimpsushi.github.io/posts/multivariate-distribution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/multivariate-distribution/</guid>
      <description>1. 표본 공간 $\Omega$ 에서 정의된 $n$ 개의 확률 변수 $X_{i}$ 에 대해 $X = (X_{1} , \cdots , X_{n})$ 를 **$**n$차원 랜덤 벡터Random Vector 라고 한다. $X$ 의 치역 $X(\Omega)$ 를 **공간** 이라고도 부른다.**2.** 다음을 만족하는 함수 $F_{X} : \mathbb{R}^{n} \to [0,1]$ 을 $X$ 의 **조인트 누적 분포 함수** 라고 한다. $$ F_{X}\left( x_{1}, \cdots , x_{n} \right) := P \left[ X_{1} \le x_{1} , \cdots , X_{n} \le x_{n} \right] $$ **3.** 어떤 $h_{1} , \cdots , h_{n} &amp;gt;0$ 들에 대해</description>
    </item>
    
    <item>
      <title>수리통계학에서의 신뢰 구간</title>
      <link>https://freshrimpsushi.github.io/posts/confidence-interval/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/confidence-interval/</guid>
      <description>**신뢰 구간1 확률 밀도 함수 $f (x; \theta)$ 를 가지는 확률 변수 $X$ 의 샘플 $X_{1} , \cdots , X_{n}$ 와 신용 계수Confidence Coefficient $\alpha \in (0,1)$ 가 주어져 있다고 하자. $$ L := L \left( X_{1} , \cdots , X_{n} \right) \\ U := U \left( X_{1} , \cdots , X_{n} \right) $$ 통계량 $L,U$ 가 위와 같이 정의되어있다고 할 때, 다음을 만족하는 구간 $(L,U) \subset \mathbb{R}$ 을 모수 $\theta$ 에 대한 $( 1 - \alpha)100%$ 신뢰구간이라 한다. $$ 1-\alpha = P \left[ \theta \in \left( L,U \right) \right] $$ 사실</description>
    </item>
    
    <item>
      <title>수리통계학에서의 통계량과 추정량</title>
      <link>https://freshrimpsushi.github.io/posts/statistic-and-estimator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/statistic-and-estimator/</guid>
      <description>**통계량1 1. 확률 변수 $X$ 의 샘플 $X_{1} , \cdots , X_{n}$ 의 함수 $T$ 를 **통계량** 이라 한다. $$ T := T \left( X_{1} , \cdots , X_{n} \right) $$ **2.** $X$ 의 분포 함수가 $f(x; \theta)$ 혹은 $p(x; \theta)$ 와 같이 나타날 때, $T$ 가 $\theta$ 를 파악하기 위한 통계량이면 $T$ 를 $\theta$ 의 **추정량** 이라고 한다.**3.** 통계량과 모수에 대한 함수를 **피벗**Pivot 이라고 한다.**2.** 추정량(Es</description>
    </item>
    
    <item>
      <title>스튜던트의 정리Students Theorem 증명</title>
      <link>https://freshrimpsushi.github.io/posts/203/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/203/</guid>
      <description>$X_{i} \sim N(\mu,\sigma)$이라고 하자.**(a)** $\displaystyle \overline{X} \sim N\left( \mu , { {\sigma^2} \over {n} } \right) $**(b)** $\overline{X} \perp S^2$**(c)** $\displaystyle (n-1) { {S^2} \over {\sigma^2} } \sim \chi^2 (n-1) $**(d)** $\displaystyle T = { {\overline{X} - \mu } \over {S / \sqrt{n}} } \sim t(n-1)$통계학을 하는 사람들은 당연한듯 쓰고 있지만 사실 여기에도 이름이 있다.네 개의 파트로 나뉘어져 있어 구체적으로 인용하기도 어려운 것도 한몫을 한 것 같다.**증명(a)</description>
    </item>
    
    <item>
      <title>옌센 부등식의 기댓값 폼 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-expectation-form-of-jensens-inequality/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-expectation-form-of-jensens-inequality/</guid>
      <description>옌센 부등식의 유한 폼 보기옌센 부등식의 적분 폼 보기조건부 옌센 부등식개구간 $I$ 에서 함수 $\phi$ 가 컨벡스하고 두 번 미분가능, 확률변수 $X$의 기댓값 $\mu$ 가 존재하며 $X \subset I $ 면 $$ \phi [ E(X) ] \le E [ \phi(X)] $$ 적분 폼과는 상당히 유사한 형태를 가지고 있다. 잘 생각해보면 유한 폼 역시 항이 무한하지는 않지만 가중평균의 부등식이라는 센스에서 기댓값이라고 볼 수 있겠</description>
    </item>
    
    <item>
      <title>정규분포를 따르는 두 분포가 독립인 것과 공분산이 0인 것이 동치임을 증명</title>
      <link>https://freshrimpsushi.github.io/posts/591/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/591/</guid>
      <description>$X_{1} \sim N ( \mu_{1} , \sigma_{1} ) , X_{2} \sim N ( \mu_{2} , \sigma_{2} ) $ 면 $X_{1} \perp X_{2} \iff \text{cov} (X_{1} , X_{2} ) = 0$일반적으로 상관관계가 없다고 독립인 것은 아니다. 하지만 분포들이 정규분포를 따른다는 가정이 있다면 공분산이 $0$ 인 것이 독립임을 보장해준다. 증명 $$ \displaystyle M_{X_{1}} (t_{1} ) = \exp \left[ \mu_{1} t_{1} + {{1} \over {2}} \sigma_{1} t_{1}^{2} \right] M_{X_{2}} (t_{2} ) = \exp \left[ \mu_{2} t_{2} + {{1} \over {2}} \sigma_{2} t_{2}^{2} \right] $$ $\sigma_{12} : = \text{cov} (X_{1} , X_{2} )$ 그리고 $\sigma_{21} : = \text{cov} (X_{2} , X_{1} ) $ 이라</description>
    </item>
    
    <item>
      <title>짝으로 독립이라고 상호 독립은 아니다  번스타인 분포</title>
      <link>https://freshrimpsushi.github.io/posts/sbernstein-distribution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/sbernstein-distribution/</guid>
      <description>$\displaystyle p(x,y,z) = {{1} \over {4} }$ 여기서 $(x,y,z) \in \left\{ (1,0,0), (0,1,0), (0,0,1), (1,1,1) \right\}$ 다.번스타인 분포는 분포의 조건을 모두 만족시키고는 있지만 자연계에 실재하는 분포라고 보기는 어렵다. &amp;lsquo;짝으로 독립이면 상호 독립이다&amp;rsquo;라는 명제의 반례로 제시된 것으로, 그 외엔 아무런 의미가 없다. 다만 그 반례로써는 상당히 직관적이라 팩트를 숙지하는데 큰 도움이 된다.**</description>
    </item>
    
    <item>
      <title>체비셰프 부등식 증명</title>
      <link>https://freshrimpsushi.github.io/posts/proof-of-chebyshevs-inequality/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/proof-of-chebyshevs-inequality/</guid>
      <description>확률변수 $X$ 의 분산 $\sigma^2 &amp;lt; \infty$ 가 존재하면 $\mu := E(X)$ 와 어떤 양수 $k&amp;gt;0$ 에 대해 $$ \displaystyle P(|X-\mu| \ge k\sigma) \le {1 \over k^2} $$ 비교적 형태가 간단하고 식의 조작이 쉬운데다 결과도 한 눈에 들어오기 때문에 보조정리로써 많이 쓰인다. 다만 마코프 부등식과 비교하자면 분산이 존재해야한다는 조건이 하나 더 있다.조건에서 $2$차 적률이 존재해야하는 것을 보고 너무 쉽고 당연한 조건으로 여길지</description>
    </item>
    
    <item>
      <title>확률 변수들의 상호 독립과</title>
      <link>https://freshrimpsushi.github.io/posts/iid-mutual-independence-and-iidindependent-and-identically-distributed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/iid-mutual-independence-and-iidindependent-and-identically-distributed/</guid>
      <description>1. 확률 변수 $X_{1} , \cdots , X_{n}$ 가 $i \ne j \implies X_{i} \perp X_{j}$ 를 만족하면 $X_{1} , \cdots , X_{n}$ 이 **짝으로 독립**Pairwise Independent 이라고 한다.**2.** 연속 확률 변수 $X_{1} , \cdots , X_{n}$ 의 조인트 확률 밀도 함수 $f$ 가 각각의 확률 밀도 함수 $f_{1} , \cdots , f_{n}$ 에 대해 다음을 만족하면 $X_{1} , \cdots , X_{n}$ 가 **상호 독립** 이라고 한다. $$ f(x_{1} , \cdots , x_{n} ) \equiv f_{1} (x_{1}) \cdots f_{n} (x_{n}) $$ **3.** 이산 확률 변수 $X_{1} , \cdots</description>
    </item>
    
  </channel>
</rss>
