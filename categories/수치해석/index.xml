<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>수치해석 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/</link>
    <description>Recent content in 수치해석 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Wed, 29 Jan 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>확률적 경사 하강법</title>
      <link>https://freshrimpsushi.github.io/posts/stochastic-gradient-descent-method/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/stochastic-gradient-descent-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 대상 함수 $Q$ 와 러닝 레이트 $\alpha &amp;gt; 0$, 배치사이즈 $m$ 과 $i$ 번째 데이터에 대해 $$ \omega_{n+1} := \omega_{n} - \alpha {{ 1 } \over { n }} \sum_{i=1}^{m} \nabla Q_{i} ( \omega_{n} ) $$ 를 확률적 경사 하강법 이라고 한다. 확률적 경사 하강법은 데이터를 다루는만큼 필연적으로 머신러닝과 깊은 관계를 가지고 있을 수밖에 없다. 몇몇 단어가 익숙하지 않더라도 일단 예시를</description>
    </item>
    
    <item>
      <title>수학에서의 최적화 기법</title>
      <link>https://freshrimpsushi.github.io/posts/optimization-method/</link>
      <pubDate>Sat, 25 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/optimization-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 1. 함수 $f : \mathbb{R}^{n} \to \mathbb{R}$ 의 함수값이 최소가 되도록 하는 $x^{ \ast } = \text{argmin}_{x} f(x)$ 를 구하는 문제를 최적화 문제Optimization Problem라 하고, 그 문제를 푸는 알고리즘을 최적화 기법 이라고 부른다. 최적화 문제에서 주어진 함수 $f$ 를 특히 대상 함수Objective Function라 한다. 2.**</description>
    </item>
    
    <item>
      <title>디리클레 경계 조건이 주어진 열방정식에 대한 초기값 문제의 수치해석적 풀이</title>
      <link>https://freshrimpsushi.github.io/posts/numerical-solution-for-heat-equation-with-dirichlet-boundary-condition/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/numerical-solution-for-heat-equation-with-dirichlet-boundary-condition/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 대수적 풀이 $\begin{cases} u_{t} = \gamma u_{xx} \\ u(t,0) = u(t,l) = 0 \\ u(0,x) = f(x) \end{cases}$ 주어진 문제는 대수적 풀이가 있을 정도로 쉽고 간단하지만, 미분방정식을 푸는 방법으로써의 수치해석을 왜 배우는지 명쾌하게 알려주는 예시가 되기도 한다. 단순히 $y&#39; = f(x,y)$ 꼴의 미분방정식을 푸는 게 편미분방정식의 풀이로도 이어지는 것이다.**풀이</description>
    </item>
    
    <item>
      <title>룽게-쿠타 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/runge-kutta-method/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/runge-kutta-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b $ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값 $y_{0} = Y_{0}$ 에 대해 $$ \displaystyle y_{n+1} = y_{n-1} + h \sum_{j=1}^{p} \gamma_{j} V_{j} $$ 룽게-쿠타 메소드</description>
    </item>
    
    <item>
      <title>A-스테이블</title>
      <link>https://freshrimpsushi.github.io/posts/a-stable/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/a-stable/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 미드포인트 메소드를 비롯한 멀티스텝 메소드는 $h$ 가 충분히 작지 않을 때 패러사이틱 솔루션이 있을 수 있다. 충분히 작지 않다는 건 $ y&#39; = \lambda y$ 와 같은 문제가 있을 때 $| 1 + h \lambda| &amp;lt;1$ 과 같은 조건을 만족하지 못하는 등의 경우를 말한다. $z : = h \lambda \in \mathbb{C}$ 라고 할 때 위의 조건을 복소평면 상에 나타내보면 아래의 그</description>
    </item>
    
    <item>
      <title>일관성을 가지는 멀티스텝 메소드의 수렴성과 루트 컨디션</title>
      <link>https://freshrimpsushi.github.io/posts/convergence-and-root-condition-of-multi-step-method-with-consistency/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/convergence-and-root-condition-of-multi-step-method-with-consistency/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 멀티스텝 메소드가 일관성을 가진다고 하자.메소드는 수렴성을 가진다 $\iff$ 메소드는 루트 컨디션을 만족 시킨다 폐구간 $[x_{0} , b]$ 에 대해 $h$ 를 단위로 잘라서 노드 포인트를 만들 때, $x_{0} \le x_{1} \le \cdots \le x_{N(h) -1} \le x_{N(h) } \le b$ 라고 하자. 여기서 $N(h)$ 는 $h$ 에 따라 변하는 마지막 노드 포인트의 인덱스를 나타낸다.메소드가 수렴</description>
    </item>
    
    <item>
      <title>일관성을 가지는 멀티스텝 메소드의 안정성과 루트 컨디션</title>
      <link>https://freshrimpsushi.github.io/posts/stability-and-root-condition-of-multi-step-method-with-consistency/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/stability-and-root-condition-of-multi-step-method-with-consistency/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 멀티스텝 메소드가 일관성을 가진다고 하자.메소드는 안정성을 가진다 $\iff$ 메소드는 루트 컨디션을 만족 시킨다 폐구간 $[x_{0} , b]$ 에 대해 $h$ 를 단위로 잘라서 노드 포인트를 만들 때, $x_{0} \le x_{1} \le \cdots \le x_{N(h) -1} \le x_{N(h) } \le b$ 라고 하자. 여기서 $N(h)$ 는 $h$ 에 따라 변하는 마지막 노드 포인트의 인덱스를 나타낸다.원래 주어진 초</description>
    </item>
    
    <item>
      <title>멀티스텝 메소드의 루트 컨디션</title>
      <link>https://freshrimpsushi.github.io/posts/root-conditions-of-multistep-method/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/root-conditions-of-multistep-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 멀티스텝 메소드$D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b $ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값과 $0 \le p \le m$ 에</description>
    </item>
    
    <item>
      <title>아담스 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/adams-method/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/adams-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 멀티스텝 메소드$D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b $ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값과 $0 \le p \le m$ 에</description>
    </item>
    
    <item>
      <title>리차드슨 오차 추정</title>
      <link>https://freshrimpsushi.github.io/posts/richardson-error-estimation/</link>
      <pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/richardson-error-estimation/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 미분방정식을 푸는 메소드의 퍼포먼스를 확인하는 방법으로 참값과 비교할 수 있다면 가장 좋겠지만, 당장 참값을 구하기 귀찮은 경우부터 시작해서 아예 트루 솔루션을 구하기 곤란한 경우도 많다. 이땐 $y_{h} (x_{n} )$ 과 스탭사이즈 $h$ 를 두배로 늘렸을 때의 $y_{2h} (x_{n} )$ 을 비교함으로써 오차를 추정할 수 있다.예를 들어 사</description>
    </item>
    
    <item>
      <title>사다리꼴 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/trapezoidal-method/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/trapezoidal-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b $ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값 $y_{0} = Y_{0}$ 에 대해 $$ \displaystyle y_{n+1} = y_{n-1} + {{h} \over {2}} [ f ( x_{n} , y_{n} ) + f (</description>
    </item>
    
    <item>
      <title>패러사이틱 솔루션</title>
      <link>https://freshrimpsushi.github.io/posts/parasitic-solution/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/parasitic-solution/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 패러사이틱 솔루션Parasitic Solution 이란 직역했을 때 &amp;lsquo;기생하는 해&amp;rsquo;라는 뜻으로 메소드가 진행될수록 크기가 커지며 부호가 바뀌는 등의 항을 말한다. $a_{n} = 2^{-n} + (-2)^{n}$ 이라는 수열이 $ (-2)^{n}$ 때문에 수렴하지 않는 걸 상상하면 좋다. 이런 항에다 &amp;lsquo;패러사이틱&amp;r</description>
    </item>
    
    <item>
      <title>미드포인트 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/midpoint-method/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/midpoint-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ ( y( x_{0} ), y (x_{1}) ) = ( Y_{0} , Y_{1} ) \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 를 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b $ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값 $y_{0} = Y_{0}$ 에 대해 $$ y_{n+1} := y_{n-1} + 2 h f ( x_{n} ,</description>
    </item>
    
    <item>
      <title>멀티스텝 메소드의 수렴성과 오차</title>
      <link>https://freshrimpsushi.github.io/posts/error-and-stability-analysis-for-multi-step-method/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/error-and-stability-analysis-for-multi-step-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) )= ( Y_{0} , \cdots , Y_{p} ) \end{cases}$ 에 대해 멀티스텝 메소드 $\displaystyle y_{n+1} = \sum_{j=0}^{p} a_{j} y_{n-j} + h \sum_{j = -1}^{p} b_{j} f (x_{n-j} , y_{n-j} ) $ 가 일관성을 가지고, 초기 오차 $\displaystyle \eta (h) : = \max_{ 0 \le i \le p} | Y (x_{i} ) - y_{h} (x_{i} ) | $ 가 $\displaystyle \lim_{ h \to 0} \eta (h) = 0$ 를 만족하고, $j = 0, 1, \cdots , p$ 에 대해 $a_{j} \ge 0$ 이고 $f$ 가 립시츠 조건</description>
    </item>
    
    <item>
      <title>멀티스텝 메소드의 일관성과 수렴차수</title>
      <link>https://freshrimpsushi.github.io/posts/consistency-and-convergence-order-of-multistep-method/</link>
      <pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/consistency-and-convergence-order-of-multistep-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) )= ( Y_{0} , \cdots , Y_{p} ) \end{cases}$ 에 대해 멀티스텝 메소드 $\displaystyle y_{n+1} = \sum_{j=0}^{p} a_{j} y_{n-j} + h \sum_{j = -1}^{p} b_{j} f (x_{n-j} , y_{n-j} ) $ 가 일관성을 가지는 필요충분조건은 (i) 이고, 수렴차수 $m \in \mathbb{N}$ 을 갖는 필요충분조건은 (ii) 다. (i)** $\begin{cases} \displaystyle \sum_{j = 0}^{p} a_{j} = 1 \\ \displaystyle - \sum_{j = 0}^{p} j a_{j} + \sum_{j = -1}^{p} b_{j} = 1 \end{cases}$ (ii)** $\displaystyle \sum_{j=0}^{p} (-j)^{i} a_{j} + i \sum_{j=-1}^{p}</description>
    </item>
    
    <item>
      <title>멀티스텝 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/multistep-method/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/multistep-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b $ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값과 $0 \le p \le m$ 에 대해 $a_{p} \ne 0$ 혹은 $b_{p}</description>
    </item>
    
    <item>
      <title>초기값이 조금 달라졌을 때 오일러 메소드의 오차</title>
      <link>https://freshrimpsushi.github.io/posts/error-analysis-for-euler-methods-with-perturbed-initial-valued/</link>
      <pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/error-analysis-for-euler-methods-with-perturbed-initial-valued/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $[x_{0} , b] \times \mathbb{R}$ 에서 정의된 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 의 해 $Y(x)$ 가 $Y \in C^{3} [ x_{0} , b ]$ 이고 $\displaystyle f_{y} (x,y) = {{ \partial f (x,y) } \over { \partial y }} $ 와 $\displaystyle f_{yy} (x,y) = {{ \partial^{2} f (x,y) } \over { \partial y^{2} }} $ 가 연속이면서 바운디드라고 하자. 초기값 $y_{h} (x_{0} ) $ 가 $Y_{0} - y_{h} (x_{0} ) = \delta_{0} h + O ( h^2 ) $ 을 만족시킨다고 하자. 그러면 오일</description>
    </item>
    
    <item>
      <title>강한 립시츠 조건과 오일러 메소드의 오차</title>
      <link>https://freshrimpsushi.github.io/posts/stronger-lipschitz/</link>
      <pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/stronger-lipschitz/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 강한 립시츠 조건 $\implies$ 립시츠 조건 $\implies$ 국소 립시츠 조건 $[x_{0} , b] \times \mathbb{R}$ 에서 정의된 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 의 해 $Y(x)$ 가 $[x_{0} , b]$ 에서 두 번 미분가능하다고 하자. $f$ 가 모든 $x_{0} \le x \le b$ 와 $ y_{1} , y_{2} \in \mathbb{R}$, 그리고 $K \ge 0$ 에 대해 **강한 립시츠 조건 $$ |f(x,y_{1} ) - f(x,y_{2}) | \le K | y_{1} - y_{2} | $$ 을 만족하면 오일</description>
    </item>
    
    <item>
      <title>수치해석에서의 오일러 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/euler-method-in-numerical-analysis/</link>
      <pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/euler-method-in-numerical-analysis/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b $ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값 $y_{0} \simeq Y_{0}$ 에 대해 $$ y_{n+1} = y_{n} + h f ( x_{n} , y_{n} ) $$ 오일러 메소드</description>
    </item>
    
    <item>
      <title>립시츠 조건</title>
      <link>https://freshrimpsushi.github.io/posts/lipschitz-condition/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/lipschitz-condition/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 강한 립시츠 조건 $\implies$ 립시츠 조건 $\implies$ 국소 립시츠 조건 1계 미분방정식에 대한 존재성-유일성 정리$D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y&#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 가 주어져 있다. $f$ 가 모든 $(x,y_{1}) , (x , y_{2} ) \in D$ 와 $K &amp;gt; 0$ 에 대해 립시츠 조건 $|f(x,y_{1} ) - f(x,y_{2}) | \le K | y_{1} - y_{2} |$ 을 만족하면 $(x_{0} , Y_{0}) \in D^{\circ}$ 에</description>
    </item>
    
    <item>
      <title>수치적으로 이상적분을 계산하기 위한 가우스 구적법</title>
      <link>https://freshrimpsushi.github.io/posts/gaussian-quadrature-to-calculate-numerically-improper-integration/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gaussian-quadrature-to-calculate-numerically-improper-integration/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 가우스 구적법은 그 자체로 아주 탁월할뿐 아니라, 노드를 잘 고름으로써 적분범위가 무한하게 주어져도 계산을 수행해낼 수 있다. 가우스-체비셰프 구적법 $$ \int_{-1}^{1} {{ 1 } \over { \sqrt{1 - x^2 } }} f(x) dx \approx \sum_{i=1}^{n} w_{i} f( x_{i} ) $$ $$ w_{i} = {{ \pi } \over { n }} $$ 여기서 $x_{i}$ 들은 $T_{n}(x) = 0$ 를 만족하는 체비셰프 노드다. 가우스-라게르 구</description>
    </item>
    
    <item>
      <title>에르미트 다항함수</title>
      <link>https://freshrimpsushi.github.io/posts/hermite-polynomial/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/hermite-polynomial/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 1. 확률론자의 에르미트 다항함수 : $H_{e_{n}} = (-1)^{n} e^{{x^2} \over {2}} {{d^{n}} \over {dx^{n}}} e^{- {{x^2} \over {2}}}$2. 물리학자의 에르미트 다항함수** : $H_{n} = (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2}$ 에르미트 다항함수 는 두가지 꼴이 쓰이며, $H_{n} (x) = 2^{{n} \over {2}} H_{e_{n}} \left( \sqrt{2} x \right)$ 와 같은 관계를 갖는다. [0] $\displaystyle H_{n+1} (x) = 2x H_{n} (x) - H_{n}&#39; (X)$ [1]** 함수의 내적 $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx $ 에 대해 웨이트 $w$ 를 $\displaystyle w(x) :=</description>
    </item>
    
    <item>
      <title>라게르 다항함수</title>
      <link>https://freshrimpsushi.github.io/posts/laguerre-polynomial/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/laguerre-polynomial/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $\displaystyle L_{n} := {{ e^{x} } \over { n! }} {{ d^{n} } \over { dx^{n} }} \left( e^{-x} x^{n} \right)$ 을 라게르 다항함수라 한다. [0]** $\displaystyle L_{n+1} (x) = {{ 1 } \over { n+1 }} \left[ \left( 2n + 1 - x \right) L_{n} (x) - n L_{n-1} (x) \right]$ [1]** 함수의 내적 $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx $ 에 대해 웨이트 $w$ 를 $\displaystyle w(x) := e^{-x}$ 와 같이 주면 $\left\{ L_{0} , L_{1}, L_{2}, \cdots \right\} $ 은 직교 집합이 된다. $n = 0, \cdots , 3$ 에 대한 라게르 다항함수는 다음</description>
    </item>
    
    <item>
      <title>수치적으로 이상적분을 계산하기 위한 변수 치환 트릭</title>
      <link>https://freshrimpsushi.github.io/posts/substitution-trick-to-calculate-numerically-improper-integration/</link>
      <pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/substitution-trick-to-calculate-numerically-improper-integration/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $0 &amp;lt; a &amp;lt; b &amp;lt; \infty$ 라고 하자. [1]** $ 0 &amp;lt; p &amp;lt; 1$ 면 $\displaystyle \int_{0}^{b} {{ f(x) } \over {x^{p} }} dx = \int_{0}^{{{ 1 } \over { 1-p }} b^{1-p} } f \left( \left[ ( 1- p ) m \right]^{{{ 1 } \over { 1-p }}} \right) dm$ [2]** $ 1 &amp;lt; p$ 면 $\displaystyle \int_{a}^{ \infty } {{ f(x) } \over {x^{p} }} dx = \int_{0}^{{{ 1 } \over { p-1 }} a^{1-p}}f \left( \left[ ( p-1 ) m \right]^{{{ 1 } \over { 1-p }}} \right) dm$ 이상적분은 언제나 골칫덩이지만 풀이를 포기할 수는 없다. 보통 적분이</description>
    </item>
    
    <item>
      <title>가우스 구적법</title>
      <link>https://freshrimpsushi.github.io/posts/gaussian-quadrature/</link>
      <pubDate>Thu, 04 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gaussian-quadrature/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 $a = x_{1} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. $$ \displaystyle I_{n} (f) := \sum_{j=1}^{n} w_{j} f ( x_{j} ) \approx \int_{a}^{b} w(x) f(x) dx = I ( f ) $$ 위와 같이 정의된 $I_{n}$ 의 가중치 $w_{j}$ 들을 구해서 수치적 적분을 계산하는 것을 가우스 구적법 이라고 한다. $f$ 를 잘 근사하는 다항함수 $p_{n-1}$ 이 존재하는 것은 보</description>
    </item>
    
    <item>
      <title>뉴턴-코테스 적분 공식</title>
      <link>https://freshrimpsushi.github.io/posts/newton-cotes-integration-formulas/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/newton-cotes-integration-formulas/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 간격이 $\displaystyle h:= {{b-a} \over {n}}$ 로 일정한 $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 다음과 같이 정의된 $I_{n}^{p}$ 을 뉴턴-코테스 공식 이라고 한다. $$ \displaystyle I_{n}^{p} (f) := \sum_{i=0}^{n} w_{i} f ( x_{i} ) $$ $i=0,1,\cdots , n$ 에 대해 $x_{i} := a + i h $ 이고, $l_{i}$ 는 라그랑주 공식에서 쓰이는 다항함수 $\displaystyle l_{i} (x)</description>
    </item>
    
    <item>
      <title>심슨 룰</title>
      <link>https://freshrimpsushi.github.io/posts/simpson-rule/</link>
      <pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/simpson-rule/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 간격이 $\displaystyle h:= {{b-a} \over {n}}$ 로 일정한 $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 다음과 같이 정의된 $I_{n}^{2}$ 을 심슨 룰 이라고 한다. $$ \displaystyle I_{n}^{2} (f) := \sum_{k=1}^{n/2} {{h} \over {3}} \left[ f(x_{2k-2}) + 4 f( x_{2k-1} ) + f(x_{2k} ) \right] $$ $f \in C^4 [a,b]$ 이라고 하자.[1] $\displaystyle E_{1}^{2} (f) = - {{h^5} \over {90}} f^{(4)} ( \xi )$[2] $\displaystyle \tilde{E}_{n}^{2} (f) = - {{ h^4</description>
    </item>
    
    <item>
      <title>사다리꼴 룰</title>
      <link>https://freshrimpsushi.github.io/posts/trapezoidal-rule/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/trapezoidal-rule/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 간격이 $\displaystyle h:= {{b-a} \over {n}}$ 로 일정한 $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 다음과 같이 정의된 $I_{n}^{1}$ 을 사다리꼴 룰 이라고 한다. $$ I_{n}^{1} (f) := \displaystyle \sum_{k=1}^{n} {{h} \over {2}} \left( f(x_{k-1}) + f(x_{k} ) \right) $$ $f \in C^2 [a,b]$ 이라고 하자.[1] $\displaystyle E_{1}^{1} (f) = - {{1} \over {12}} h^{3} f&#39;&#39; ( \xi )$[2] $\displaystyle \tilde{E}_{n}^{1} (f) = - {{ h^2 } \over</description>
    </item>
    
    <item>
      <title>수치적 적분</title>
      <link>https://freshrimpsushi.github.io/posts/numerical-intergration/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/numerical-intergration/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. (1)** 적분 오퍼레이터 $I$ 를 $\displaystyle I(f) := \int_{a}^{b} f(x) dx$ 와 같이 정의한다. (2)** 적분 오퍼레이터 $I_{n}$ 을 $\displaystyle I_{n} (f) := \sum_{k=1}^{n} \int_{x_{k-1}}^{x_{k}} f(x) dx $ 와 같이 정의한다. (3)** 에러 $E_{n}$ 을 $E_{n} (f) := I (f) - I_{n} ( f )$ 와 같이 정의한다. (4)** $\displaystyle \lim_{n \to \infty} {{\tilde{E}_{n} (f) } \over {</description>
    </item>
    
    <item>
      <title>체비셰프 노드</title>
      <link>https://freshrimpsushi.github.io/posts/chebyshev-node/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/chebyshev-node/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $[-1,1]$ 에서 $\displaystyle x_{k} = \cos \left( {{2k-1} \over {2n}} \pi \right)$, $k=1, \cdots , n$ 을 체비셰프 노드라 한다. 체비셰프 노드는 일반적으로 사용하듯 일정한 간격의 노드 포인트와 달리 반원의 호를 일정한 크기로 자르고 그 점들을 $x$ 축으로 사영시킨 노드 포인트를 말한다. 점들의 분포는 가운데보다 양 끝에 조금 몰리는 모양새를 이룬다. 밑에서 다시 설</description>
    </item>
    
    <item>
      <title>체비셰프 전개</title>
      <link>https://freshrimpsushi.github.io/posts/chebyshev-expansion/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/chebyshev-expansion/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 체비셰프 전개를 이해하기 위해서는 어떻게 체비셰프 전개가 나오는지를 먼저 알아야한다. 우선 최소극대화 문제를 푸는 대신 최소제곱 문제를 푼다고 생각해보자. $$ \displaystyle M_{n} (f) := \inf_{\deg(r) \le n } \left\| f - r \right\|_{2} $$ $f : [a,b] \to \mathbb{R}$ 에 대해 위와 같이 최소제곱 문제가 주어져있다고 하자. 목표는 $M_{n} (f)$ 를 최소화하는 $n$ 차 이하의</description>
    </item>
    
    <item>
      <title>수치해석학에서의 최소극대화 근사와 최소제곱 근사</title>
      <link>https://freshrimpsushi.github.io/posts/minimax-approximation-and-least-squares-approximation/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/minimax-approximation-and-least-squares-approximation/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 주어진 함수 $f : [a,b] \to \mathbb{R}$ 를 근사하는 문제가 주어져 있다고 하자. 계산은 컴퓨터의 몫이므로 다항함수로 $f$ 를 근사하는 것이 목표다. 함수를 근사시킨다는 것은 한 점에서의 계산이 아니라 정의역 $[a,b]$ 전체에서 $f$ 와 비슷한 함수를 사용하고 싶은 것이므로 가장 크게 틀리는 부분을 줄이는 것이 목표다. 이러한 상황</description>
    </item>
    
    <item>
      <title>함수 근사</title>
      <link>https://freshrimpsushi.github.io/posts/approximation-of-functions/</link>
      <pubDate>Mon, 10 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/approximation-of-functions/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 수치적인 계산을 할 때 컴퓨터가 인간보다 압도적으로 빠른 것은 사실이지만, 딱히 컴퓨터가 초월함수와 무리수를 이해했기 때문은 아니다. 가령 $\displaystyle \sin {{ \pi } \over {6}} = {{1} \over { 2 }}$ 을 계산시킨다면 삼각함수의 기하학적인 정의를 이용해서 직각삼각형을 그려보고 빗변과 높이의 비를 구하는 게 아니라, 다항함수</description>
    </item>
    
    <item>
      <title>수치해석학에서의 B-스플라인 B-Spline in Numerical Analysis</title>
      <link>https://freshrimpsushi.github.io/posts/1045/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/1045/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 함수해석학에서의 B-스플라인* 글과 수식이 읽기 싫으면 그냥 그림으로 보고 이해해도 무방하다. 구간 $[a,b]$ 를 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b $ 와 같은 노드 포인트들로 쪼갰다고 하자. 주어진 자유도 $K$ 에 대해서 $x_{-K} &amp;lt; x_{-K + 1} &amp;lt; \cdots &amp;lt; x_{-1} &amp;lt; x_{0}$ 과 $x_{N} &amp;lt; x_{N + 1} &amp;lt; \cdots &amp;lt; x_{N+K-1} &amp;lt; x_{N+K}$ 의 추가적인 노드를 생각한다. $i</description>
    </item>
    
    <item>
      <title>수치해석에서의 스플라인</title>
      <link>https://freshrimpsushi.github.io/posts/spline-in-numerical-analysis/</link>
      <pubDate>Thu, 16 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/spline-in-numerical-analysis/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 인터폴레이션이란 정확한 함수를 복원하는 게 아니라 그와 유사하면서도 다루기 편한 함수를 구하는 것이 목적이다. 물론 익스플릭시트Explicit하고 계산이 쉬워지도록 구할 수 있다면야 제일 좋겠지만, 이 우주는 그렇게 만만한 곳이 아니다.문제에 따라서는 간단한 부분을 빨리 풀고 복잡한 부분을</description>
    </item>
    
    <item>
      <title>에르미트 인터폴레이션</title>
      <link>https://freshrimpsushi.github.io/posts/hermite-interpolation/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/hermite-interpolation/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 서로 다른 $x_{1} , \cdots , x_{n}$ 의 데이터 $(x_{1}, y_{1} , y&#39;_{1}) , \cdots , (x_{n} , y_{n}, y&#39;_{n})$ 에 대해 $\begin{cases} p (x_{i} ) = y_{i} \\ p&#39;(x_{i} ) = y&#39;_{i} \end{cases}$ 와 $\deg H \le 2n-1$ 을 만족하는 폴리노미얼 $H$ 를 에르미트 인터폴레이션이라고 한다. [1]** 존재성과 유일성 : 주어진 데이터에 대해서 $H$ 는 유일하게 존재한다. [2]** 라그랑주 폼 : $\displaystyle H_{n} (x) = \sum_{i=1}^{n} y_{i} h_{i} (x) + \sum_{i=1}^{n} y&#39;_{i} \tilde{h}_{i} (x)$ [3]** 뉴턴 계차</description>
    </item>
    
    <item>
      <title>에르미트-제노키 공식</title>
      <link>https://freshrimpsushi.github.io/posts/hermite-genocchi-formula/</link>
      <pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/hermite-genocchi-formula/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 서로 다른 $x_{0}, \cdots , x_{n}$ 에 대해 $f \in C^{n} \left( \mathscr{H} \left\{ x_{0}, \cdots , x_{n} \right\} \right) $ 이라고 하자. 그러면 $\displaystyle \tau_{n} := \left\{ ( t_{1} , \cdots , t_{n} ) : t_{i} \ge 0 \land \sum_{i=1}^{t} t_{i} \le 1 \right\}$ 과 $\displaystyle t_{0} = 1 - \sum_{i=1}^{n} t_{i}$ 에 대해 $\displaystyle f [ x_{0}, \cdots , x_{n} ] = \int \cdots \int_{\tau_{n}} f^{(n)} ( t_{0} x_{0} + \cdots + t_{n} x_{n} ) dt_{1} \cdots dt_{n}$ $\mathscr{H} \left\{ a,b,c, \cdots \right\} $ 는 $a,b,c, \cdots$ 를 포함하는 가장 작은 구간을 나타낸다.에르미트-제노키 공식은</description>
    </item>
    
    <item>
      <title>뉴턴 계차상 공식 유도</title>
      <link>https://freshrimpsushi.github.io/posts/derivation-of-newton-divided-difference-formula/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/derivation-of-newton-divided-difference-formula/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 서로 다른 $x_{0} , \cdots , x_{n}$ 의 데이터 $(x_{0}, f(x_{0} )) , \cdots , (x_{n} , f( x_{n} ) )$ 에 대해 $\displaystyle p_{n} (x) =\sum_{i=0}^{n} f [ x_{0} , \cdots , x_{i} ] \prod_{j=0}^{i-1} (x - x_{j} ) $ 복잡해보이지만 $n=1,2,3$ 에 대해서 실제로 전개를 해보면 다음과 같이 단순하게 나타난다. $$ p_{0} (x) = f(x_{0}) $$ $$ p_{2} (1) = f( x_{0} ) + (x - x_{0} ) f [ x_{0} , x_{1} ] $$ $$ p_{2} (x) = f( x_{0} ) + (x - x_{0} ) f [ x_{0} , x_{1} ] + ( x</description>
    </item>
    
    <item>
      <title>라그랑주 공식 유도</title>
      <link>https://freshrimpsushi.github.io/posts/derivation-of-lagrange-formula/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/derivation-of-lagrange-formula/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 서로 다른 $x_{0} , \cdots , x_{n}$ 의 데이터 $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$ 에 대해 $\displaystyle l_{i} (x) := \prod_{i \ne j} \left( {{ x - x_{j} } \over { x_{i} - x_{j} }} \right)$ 이라고 하면 $\displaystyle p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X)$ 라그랑주 공식은 폴리노미얼 인터폴레이션을 찾는 방법 중 가장 심플한 공식이다. Strategy : $l_{i}$ 이 인덱스에 대해 크로데커 델타 함수임을 보인다.유도 $$ \displaystyle l_{i} (x_{i}) = \prod_{i \ne j} \left( {{</description>
    </item>
    
    <item>
      <title>폴리노미얼 인터폴레이션</title>
      <link>https://freshrimpsushi.github.io/posts/polynomial-interpolation/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/polynomial-interpolation/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 서로 다른 $x_{0} , \cdots , x_{n}$ 의 데이터 $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$ 에 대해 $p (x_{i} ) = y_{i}$ 와 $\deg p \le n$ 을 만족하는 폴리노미얼 $p$ 를 폴리노미얼 인터폴레이션 이라고 한다. [1] 존재성과 유일성** : 주어진 데이터에 대해서 $p$ 는 유일하게 존재한다. [2] 라그랑주 공식** : $\displaystyle p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X)$ [3] 뉴턴 계차상 공식** : $\displaystyle p_{n} (x)</description>
    </item>
    
    <item>
      <title>수치해석에서의 인터폴레이션</title>
      <link>https://freshrimpsushi.github.io/posts/interpolation-in-numerical-analysis/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/interpolation-in-numerical-analysis/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 주어진 $(n+1)$쌍의 데이터 $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$ 에 대해 $f (x_{i} ) = y_{i}$ 를 만족하면서 어떤 특정한 성질을 가지는 $f$ 를 찾는 방법을 보간법 혹은 내삽법 이라고 한다. 예를 들어 위와 같이 데이터가 있긴한데 가운데 데이터가 비어있는 상황을 생각해보자. 물론 실제 데이터가 있는게 가장 좋지만, 없으면 예측</description>
    </item>
    
    <item>
      <title>수치해석에서의 경사하강법</title>
      <link>https://freshrimpsushi.github.io/posts/gradient-descent-method/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gradient-descent-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 머신러닝에서의 경사하강법 스칼라 함수 $\varphi : \mathbb{R}^{n} \to \mathbb{R}$ 을 비용 함수Cost Function 이라고 한다. 비용 함수 $ \varphi ( \mathbb{x} ) $ 의 극소값을 구하기 위해 $\mathbb{x} = \mathbb{x}_{n}$ 에서 $\varphi ( \mathbb{x}_{n+1} ) &amp;lt; \varphi ( \mathbb{x}_{n} ) $ 를 만족시키는 $\mathbb{x}_{n+1}$ 를 찾는 알고리즘을 하강법Descent Method 이라고 한다. $\varphi$ 를 비용 함수라고 부를만한 예로써 집을 한 채 짓는다</description>
    </item>
    
    <item>
      <title>넌리니어 시스템을 풀기 위한 뉴턴 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/newton-method-for-nonlinear-system/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/newton-method-for-nonlinear-system/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 1차원에서의 뉴턴-랩슨 메소드 $\mathbb{f} ( \mathbb{x} ) := \begin{bmatrix} f_{1}( \mathbb{x} ) \\ \vdots \\ f_{N} ( \mathbb{x} ) \end{bmatrix} $ 와 같은 다변수 함수 $\mathbb{f} : \mathbb{R}^{N} \to \mathbb{R}^{N}$ 가 $\mathbb{f} \in C^{2} \left( N ( \alpha ) \right)$ 이고 $\mathbb{f} ( \alpha ) = \mathbb{0}$, $\left[ D \mathbb{f} ( \alpha ) \right]^{-1}$ 이 존재한다고 하자. $\alpha$ 와 충분히 가까운 초기값 $\mathbb{x}_{0}$ 에 대해 $\displaystyle \mathbb{x}_{n+1} := \mathbb{x}_{n} - \left[ D \mathbb{f} ( \mathbb{x}_{n} ) \right]^{-1} f ( \mathbb{x}_{n} ) $ 과 같이 정의된 수열 $\left\{ \mathbb{x}_{n} \right\}$ 은 $n \to \infty$ 일</description>
    </item>
    
    <item>
      <title>뮬러 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/muller-method/</link>
      <pubDate>Sat, 13 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/muller-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $f (\alpha) = 0$ 이라고 하자. 초기값 $x_{0} , x_{1} , x_{2}$ 과 $w_{n} := f [x_{n} , x_{n-1} ] + f [ x_{n} , x_{n-2} ] - f [ x_{n-2} , x_{n-1} ]$ 에 대해 $\displaystyle x_{n+1} : = x_{n} - {{ 2 f ( x_{n} ) } \over { w_{n} \pm \sqrt{ w_{n}^{2} - 4 f (x_{n} ) f [ x_{n} , x_{n-1} , x_{n-2} ] } }}$ 과 같이 정의된 수열 $\left\{ x_{n} \right\}$ 은 $n \to \infty$ 일 때 $\alpha$ 로 $\displaystyle p \approx 1.84 $ 차 수렴한다.단, $ \left( w_{n} \pm \sqrt{ w_{n}^{2} - 4 f (x_{n} ) f [ x_{n} , x_{n-1} , x_{n-2}</description>
    </item>
    
    <item>
      <title>시컨트 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/secant-method/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/secant-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $f,f&#39;,f&#39;&#39;$ 가 $\alpha$ 의 근방에서 연속이고 $f(\alpha) = 0, f&#39;(\alpha) \ne 0$ 이라고 하자. $\alpha$ 와 충분히 가까운 초기값 $x_{0} , x_{1}$ 에 대해 $\displaystyle x_{n+1} := x_{n} - f ( x_{n} ) {{ x_{n} - x_{n-1} } \over { f ( x_{n} ) - f ( x_{n-1} ) }}$ 과 같이 정의된 수열 $\left\{ x_{n} \right\}$ 은 $n \to \infty$ 일 때 $\alpha$ 로 $\displaystyle {{1 + \sqrt{5} } \over {2}} $ 차 수렴한다. 수렴차수가 상당히 낯이 익을 것이다. 바로 황금비인 $\displaystyle {{1 + \sqrt{5} }</description>
    </item>
    
    <item>
      <title>수치해석학에서의 계차상</title>
      <link>https://freshrimpsushi.github.io/posts/divided-difference/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/divided-difference/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 함수 $f : \mathbb{R} \to \mathbb{R}$ 와 서로 다른 $x_{1} , \cdots , x_{n}$ 에 대해 다음을 $f$ 의 계차상이라고 한다. $f[x_{0}] := f( x_{0} ) $$ \displaystyle f [ x_{0} , x_{1} ] : = {{ f ( x_{1} ) - f ( x_{0} ) } \over { x_{1} - x_{0} }} $$ \displaystyle f [ x_{0} , x_{1} , x_{2} ] : = {{ f [ x_{1} , x_{2} ] - f [ x_{0} , x_{1} ] } \over { x_{2} - x_{0} }} $$ \displaystyle f [ x_{0} , \cdots , x_{n} ] : = {{ f [ x_{1} , \cdots , x_{n} ] - f [ x_{0} , x_{n-1} ]</description>
    </item>
    
    <item>
      <title>뉴턴-랩슨 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/newton-raphson-method/</link>
      <pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/newton-raphson-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 고차원에 대해 일반화된 뉴턴-랩슨 메소드 $f,f&#39;,f&#39;&#39;$ 가 $\alpha$ 의 근방에서 연속이고 $f(\alpha) = 0, f&#39;(\alpha) \ne 0$ 이라고 하자. $\alpha$ 와 충분히 가까운 초기값 $x_{0}$ 에 대해 $\displaystyle x_{n+1} := x_{n} - {{ f ( x_{n} ) } \over { f&#39; ( x_{n} ) }}$ 과 같이 정의된 수열 $\left\{ x_{n} \right\}$ 은 $n \to \infty$ 일 때 $\alpha$ 로 쿼드러틱하게 수렴한다. 뉴턴-랩슨 메소드는 그냥 뉴턴 메소드라고 불리기도</description>
    </item>
    
    <item>
      <title>바이섹션 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/bisection-method/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bisection-method/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 연속함수 $f$ 가 폐구간 $[a,b]$ 에서 $f(a) f(b) &amp;lt; 0$ 이라고 하자. 허용오차는 $\varepsilon$ 이다. $f(c) = 0$ 를 만족하는 $c \in [a,b]$ 는 다음과 같이 구할 수 있다.**Step 1. $\displaystyle c:= {{a+b} \over {2}} $**Step 2. $b-c \le \varepsilon$ 이면 $c$ 를 반환한다.**Step 3. $f(b) f(c) &amp;lt; 0 $ 이면 $a:=c$, 아니면 $b:=c$그리고 Step 1. 으로 돌아간다. 중간값정리의 대표적인 응</description>
    </item>
    
    <item>
      <title>수치해석에서의 수렴률</title>
      <link>https://freshrimpsushi.github.io/posts/rate-of-convergence-in-numerical-analysis/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/rate-of-convergence-in-numerical-analysis/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $\alpha$ 로 수렴하는 수열 $\left\{ x_{n} \right\} $ 이 $p \ge 1$ 과 $c \ge 0$ 에 대해 $| \alpha - x_{n+1} | \le c | \alpha - x_{n} | ^{p}$ 이면 차수 $p$ 을 만족하면 $\left\{ x_{n} \right\} $ 이 수렴률 $c$ 로 $\alpha$ 에 $p$ 차 수렴한다 고 한다. 특히 $c &amp;lt; 1$ 이라는 조건과 함께 $p=1$ 이면 선형 수렴Linear Convergence 이라고 부른다. 비슷하게 $p=2$ 일 때는 Quadratic Convergence , $p=3$ 일 때는 Cubic Convergen</description>
    </item>
    
    <item>
      <title>제1종 제2종 체비셰프 다항함수의 관계</title>
      <link>https://freshrimpsushi.github.io/posts/relations-between-chebyshev-polynomials/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/relations-between-chebyshev-polynomials/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 제1종 체비셰프 다항함수제2종 체비셰프 다항함수체비셰프 미분방정식의 해로써의 체비셰프 다항함수 $T_{n} (x) = \cos \left( n \cos^{-1} x \right)$ 과 $\displaystyle U_{n} (x) = {{1} \over {n+1} } T_{n+1} &#39; (X)$ 은 다음의 관계를 가진다. [1]** $U_{n} (x) - U_{n-2} (x) = 2 T_{n} (X)$ [2]** $T_{n} (x) - T_{n-2} (x) = 2( x^2 - 1 ) U_{n-2} (x) $ *보통 $0 \le \theta \le \pi$ 에 대해 $\theta := \cos^{-1} x $ 라고 둔다. 위 등식들을</description>
    </item>
    
    <item>
      <title>제2종 체비셰프 다항함수</title>
      <link>https://freshrimpsushi.github.io/posts/chebyshev-polynomial-of-the-second-kind/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/chebyshev-polynomial-of-the-second-kind/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 제1종 체비셰프 다항함수제1종, 제2종 체비셰프 다항함수의 관계체비셰프 미분방정식의 해로써의 체비셰프 다항함수 $\displaystyle U_{n} (x) := {{1} \over {n+1} } T_{n+1} &#39; (x) = {{\sin \left( ( n +1 ) \theta \right)} \over { \sin \theta }} $ 을 제2종 체비셰프 다항함수라 한다. [0]** $U_{n+1} (x) = 2x U_{n} (x) - U_{n-1} (X)$ [1]** 함수의 내적 $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx $ 에 대해 웨이트 $w$ 를 $\displaystyle w(x)</description>
    </item>
    
    <item>
      <title>제1종 체비셰프 다항함수</title>
      <link>https://freshrimpsushi.github.io/posts/chebyshev-polynomial-of-the-first-kind/</link>
      <pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/chebyshev-polynomial-of-the-first-kind/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 제2종 체비셰프 다항함수제1종, 제2종 체비셰프 다항함수의 관계체비셰프 미분방정식의 해로써의 체비셰프 다항함수 $T_{n} (x) = \cos \left( n \cos^{-1} x \right)$ 을 제1종 체비셰프 다항함수라 한다. [0]** $T_{n+1} (x) = 2x T_{n} (x) - T_{n-1} (X)$ [1]** 함수의 내적 $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx $ 에 대해 웨이트 $w$ 를 $\displaystyle w(x) := {{1} \over { \sqrt{1 - x^2} }}$ 와 같이 주면 $\left\{ T_{0} , T_{1},</description>
    </item>
    
    <item>
      <title>빅오 노테이션이 분모에 있을 때 분자로 올리는 법</title>
      <link>https://freshrimpsushi.github.io/posts/trick-for-big-o-notation-in-denominator/</link>
      <pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/trick-for-big-o-notation-in-denominator/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 $a \ne 0$ 와 $p&amp;gt;0$, $n \in \mathbb{N}$ 에 대해 $\displaystyle {{1} \over { \sqrt[p]{a + O ( h^n ) } }} = {{1} \over { \sqrt[p]{a } }}+ O(h^n)$ 복잡하게 생긴 분모를 깔끔한 형태로 바꿔주는 렘마로써 요긴하게 쓰일 수 있다.상수항 $a$ 이 없다면 렘마 없이도 $\displaystyle {{1} \over { \sqrt[p]{ O ( h^n ) } }} = O \left( h^{ - {{n} \over {p}} } \right) $ 으로 깔끔하게 올라오지만 보통 쓸모가 없다. 증명 $\displaystyle {{1} \over { \sqrt[p]{a + O</description>
    </item>
    
    <item>
      <title>수치해석에서의 차분</title>
      <link>https://freshrimpsushi.github.io/posts/difference-in-numerical-analysis/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/difference-in-numerical-analysis/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 시계열분석에서의 차분 **1. ** 전방차분 : ** $\Delta f(x) = f(x+h) - f(x)$ 그리고 $\Delta^{r+1} f(x) =\Delta^{r} f(x+h) - \Delta^{r} f(x)$2. 후방차분 : $\nabla f(x) = f(x) - f(x- h)$ 그리고 $\nabla^{r+1} f(x) = \nabla^{r} f(x) - \nabla^{r} f(x- h)$ 일반적으로 계차Difference 란 수열 전반에서 사용하는 말이지만 수치해석에선 특히 두 노드포인트의 함숫값의 차를 말한다. 사실 고등학교때부터 계속 봐</description>
    </item>
    
  </channel>
</rss>
