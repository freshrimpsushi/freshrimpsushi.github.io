<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>수치해석 on 생새우초밥집</title>
    <link>https://freshrimpsushi.github.io/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/</link>
    <description>Recent content in 수치해석 on 생새우초밥집</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Fri, 16 Aug 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>디리클레 경계 조건이 주어진 열방정식에 대한 초기값 문제의 수치해석적 풀이</title>
      <link>https://freshrimpsushi.github.io/posts/numerical-solution-for-heat-equation-with-dirichlet-boundary-condition/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/numerical-solution-for-heat-equation-with-dirichlet-boundary-condition/</guid>
      <description>예제 1 $$ \begin{cases} u_{t} = \gamma u_{xx} \\ u(t,0) = u(t,l) = 0 \\ u(0,x) = f(x) \end{cases} $$ 주어진 문제는 대수적 풀이가 있을 정도로 쉽고 간단하지만, 미분방정식을 푸는 방법으로써의 수치해석을 왜 배우는지 명쾌하게 알려주는 예시가 되기도 한다. 단순히 $y&amp;rsquo; = f(x,y)$ 꼴의 미분방정식을 푸는 게 편미분방정식의 풀이로도 이어지는 것이다. 풀이 대수적 풀이에서와 달리 $\gamma$ 의 값이 중요한 것은 아니므로 편의</description>
    </item>
    
    <item>
      <title>룽게-쿠타 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/runge-kutta-method/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/runge-kutta-method/</guid>
      <description>메소드 1 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값 $y_{0} = Y_{0}$ 에 대해 $$ y_{n+1} = y_{n-1} + h \sum_{j=1}^{p} \gamma_{j} V_{j} $$ 설명 룽게-쿠타 메소드는 아담스 메소드처럼 여러가지 형태를 가지</description>
    </item>
    
    <item>
      <title>A-스테이블</title>
      <link>https://freshrimpsushi.github.io/posts/a-stable/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/a-stable/</guid>
      <description>빌드업 미드포인트 메소드를 비롯한 멀티스텝 메소드는 $h$ 가 충분히 작지 않을 때 패러사이틱 솔루션이 있을 수 있다. 충분히 작지 않다는 건 $ y &#39; = \lambda y$ 와 같은 문제가 있을 때 $| 1 + h \lambda| &amp;lt;1$ 과 같은 조건을 만족하지 못하는 등의 경우를 말한다. $z : = h \lambda \in \mathbb{C}$ 라고 할 때 위의 조건을 복소평면 상에 나타내보면 아래의 그림과 같다. $z$ 이 이 영역에 속하지 못하면 메소</description>
    </item>
    
    <item>
      <title>일관성을 가지는 멀티스텝 메소드의 수렴성과 루트 컨디션</title>
      <link>https://freshrimpsushi.github.io/posts/convergence-and-root-condition-of-multi-step-method-with-consistency/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/convergence-and-root-condition-of-multi-step-method-with-consistency/</guid>
      <description>정리 만약 멀티스텝 메소드가 일관성을 가진다고 하면, 메소드는 수렴성을 가진다 $\iff$ 메소드는 루트 컨디션을 만족 시킨다 설명 폐구간 $[x_{0} , b]$ 에 대해 $h$ 를 단위로 잘라서 노드 포인트를 만들 때, $x_{0} \le x_{1} \le \cdots \le x_{N(h) -1} \le x_{N(h) } \le b$ 라고 하자. 여기서 $N(h)$ 는 $h$ 에 따라 변하는 마지막 노드 포인트의 인덱스를 나타낸다. 메소드가 수렴성을 가진다는 것은 $h \to 0$ 일 때 $\displaystyle \eta (h) :</description>
    </item>
    
    <item>
      <title>일관성을 가지는 멀티스텝 메소드의 안정성과 루트 컨디션</title>
      <link>https://freshrimpsushi.github.io/posts/stability-and-root-condition-of-multi-step-method-with-consistency/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/stability-and-root-condition-of-multi-step-method-with-consistency/</guid>
      <description>정리 만약 멀티스텝 메소드가 일관성을 가진다고 하면, 메소드는 안정성을 가진다 $\iff$ 메소드는 루트 컨디션을 만족 시킨다 설명 폐구간 $[x_{0} , b]$ 에 대해 $h$ 를 단위로 잘라서 노드 포인트를 만들 때, $x_{0} \le x_{1} \le \cdots \le x_{N(h) -1} \le x_{N(h) } \le b$ 라고 하자. 여기서 $N(h)$ 는 $h$ 에 따라 변하는 마지막 노드 포인트의 인덱스를 나타낸다. 원래 주어진 초기값 $y_{0} , \cdots , y_{p}$ 에 대해 아주 조금 변화를</description>
    </item>
    
    <item>
      <title>멀티스텝 메소드의 루트 컨디션</title>
      <link>https://freshrimpsushi.github.io/posts/root-conditions-of-multistep-method/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/root-conditions-of-multistep-method/</guid>
      <description>정의 1 멀티스텝 메소드: $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값과 $0 \le p \le m$ 에 대해 $a_{p} \ne 0$ 혹은 $b_{p} \ne 0$ 이면 다음을 $(p+1</description>
    </item>
    
    <item>
      <title>아담스 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/adams-method/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/adams-method/</guid>
      <description>정의 1 멀티스텝 메소드: $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값과 $0 \le p \le m$ 에 대해 $a_{p} \ne 0$ 혹은 $b_{p} \ne 0$ 이면 다음을 $(p+1</description>
    </item>
    
    <item>
      <title>리차드슨 오차 추정</title>
      <link>https://freshrimpsushi.github.io/posts/richardson-error-estimation/</link>
      <pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/richardson-error-estimation/</guid>
      <description>빌드업 미분방정식을 푸는 메소드의 퍼포먼스를 확인하는 방법으로 참값과 비교할 수 있다면 가장 좋겠지만, 당장 참값을 구하기 귀찮은 경우부터 시작해서 아예 트루 솔루션을 구하기 곤란한 경우도 많다. 이땐 $y_{h} (x_{n} )$ 과 스탭사이즈 $h$ 를 두배로 늘렸을 때의 $y_{2h} (x_{n} )$ 을 비교함으로써 오차를 추정할 수 있다. 예를 들어 사다리꼴 메소드라면 $h$ 를 고침으로써 아래의 두 식</description>
    </item>
    
    <item>
      <title>사다리꼴 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/trapezoidal-method/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/trapezoidal-method/</guid>
      <description>정의 1 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값 $y_{0} = Y_{0}$ 에 대해 $$ y_{n+1} = y_{n-1} + {{h} \over {2}} [ f ( x_{n} , y_{n} ) + f ( x_{n+1} , y_{n+1} ) ] $$ 설명 예측자-수정자 알고리즘 오일</description>
    </item>
    
    <item>
      <title>패러사이틱 솔루션</title>
      <link>https://freshrimpsushi.github.io/posts/parasitic-solution/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/parasitic-solution/</guid>
      <description>정의 1 패러사이틱 솔루션Parasitic Solution 이란 직역했을 때 &amp;lsquo;기생하는 해&amp;rsquo;라는 뜻으로 메소드가 진행될수록 크기가 커지며 부호가 바뀌는 등의 항을 말한다. $a_{n} = 2^{-n} + (-2)^{n}$ 이라는 수열이 $ (-2)^{n}$ 때문에 수렴하지 않는 걸 상상하면 좋다. 이런 항에다 &amp;lsquo;패러사이틱&amp;rsquo;이라는 표현을 쓰는것은 수렴을 방해</description>
    </item>
    
    <item>
      <title>미드포인트 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/midpoint-method/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/midpoint-method/</guid>
      <description>메소드 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ ( y( x_{0} ), y (x_{1}) ) = ( Y_{0} , Y_{1} ) \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 를 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값 $y_{0} = Y_{0}$ 에 대해 $$ y_{n+1} := y_{n-1} + 2 h f ( x_{n} , y_{n} ) $$ 유도 1 $Y&amp;rsquo;(t)$ 를 $x_{n}$ 에 대해 테일러 전개하면 $$</description>
    </item>
    
    <item>
      <title>멀티스텝 메소드의 수렴성과 오차</title>
      <link>https://freshrimpsushi.github.io/posts/error-and-stability-analysis-for-multi-step-method/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/error-and-stability-analysis-for-multi-step-method/</guid>
      <description>정리 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) )= ( Y_{0} , \cdots , Y_{p} ) \end{cases}$ 에 대해 멀티스텝 메소드 $$ \displaystyle y_{n+1} = \sum_{j=0}^{p} a_{j} y_{n-j} + h \sum_{j = -1}^{p} b_{j} f (x_{n-j} , y_{n-j} ) $$ 가 일관성을 가지고, 초기 오차 $\displaystyle \eta (h) : = \max_{ 0 \le i \le p} | Y (x_{i} ) - y_{h} (x_{i} ) |$ 가 $\displaystyle \lim_{ h \to 0} \eta (h) = 0$ 를 만족하고, $j = 0, 1, \cdots , p$ 에 대해 $a_{j} \ge 0$ 이고 $f$ 가 립시츠 조건을 만족하면 메소드는 수렴하고 적절한 상수 $c_{1} ,</description>
    </item>
    
    <item>
      <title>멀티스텝 메소드의 일관성과 수렴차수</title>
      <link>https://freshrimpsushi.github.io/posts/consistency-and-convergence-order-of-multistep-method/</link>
      <pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/consistency-and-convergence-order-of-multistep-method/</guid>
      <description>정리 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) )= ( Y_{0} , \cdots , Y_{p} ) \end{cases}$ 에 대해 멀티스텝 메소드 $$ \displaystyle y_{n+1} = \sum_{j=0}^{p} a_{j} y_{n-j} + h \sum_{j = -1}^{p} b_{j} f (x_{n-j} , y_{n-j} ) $$ 가 일관성을 가지는 필요충분조건은 (i) 이고, 수렴차수 $m \in \mathbb{N}$ 을 갖는 필요충분조건은 (ii) 다. (i): $$\begin{cases} \displaystyle \sum_{j = 0}^{p} a_{j} = 1 \\ \displaystyle - \sum_{j = 0}^{p} j a_{j} + \sum_{j = -1}^{p} b_{j} = 1 \end{cases}$$ (ii): $i = 0, 1 , \cdots , m$ 에 대해 $$\sum_{j=0}^{p} (-j)^{i} a_{j} + i \sum_{j=-1}^{p} (- j )^{i-1} b_{j} = 1$$ 설명 멀</description>
    </item>
    
    <item>
      <title>멀티스텝 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/multistep-method/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/multistep-method/</guid>
      <description>정의 1 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값과 $0 \le p \le m$ 에 대해 $a_{p} \ne 0$ 혹은 $b_{p} \ne 0$ 이면 다음을 $(p+1)$-스텝 메소드</description>
    </item>
    
    <item>
      <title>초기값이 조금 달라졌을 때 오일러 메소드의 오차</title>
      <link>https://freshrimpsushi.github.io/posts/error-analysis-for-euler-methods-with-perturbed-initial-valued/</link>
      <pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/error-analysis-for-euler-methods-with-perturbed-initial-valued/</guid>
      <description>정리 $[x_{0} , b] \times \mathbb{R}$ 에서 정의된 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 의 해 $Y(x)$ 가 $Y \in C^{3} [ x_{0} , b ]$ 이고 $\displaystyle f_{y} (x,y) = {{ \partial f (x,y) } \over { \partial y }}$ 와 $\displaystyle f_{yy} (x,y) = {{ \partial^{2} f (x,y) } \over { \partial y^{2} }}$ 가 연속이면서 바운디드라고 하자. 초기값 $y_{h} (x_{0} )$ 가 $Y_{0} - y_{h} (x_{0} ) = \delta_{0} h + O ( h^2 )$ 을 만족시킨다고 하자. 그러면 오일러 메소드로 생기는 오차는 선형 초기값 문제 $$ \begin{cases} \displaystyle D&amp;rsquo; (x) =</description>
    </item>
    
    <item>
      <title>강한 립시츠 조건과 오일러 메소드의 오차</title>
      <link>https://freshrimpsushi.github.io/posts/stronger-lipschitz/</link>
      <pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/stronger-lipschitz/</guid>
      <description>정리 $[x_{0} , b] \times \mathbb{R}$ 에서 정의된 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 의 해 $Y(x)$ 가 $[x_{0} , b]$ 에서 두 번 미분가능하다고 하자. $f$ 가 모든 $x_{0} \le x \le b$ 와 $ y_{1} , y_{2} \in \mathbb{R}$, 그리고 $K \ge 0$ 에 대해 **강한 립시츠 조건 $$ |f(x,y_{1} ) - f(x,y_{2}) | \le K | y_{1} - y_{2} | $$ 을 만족하면 오일러 메소드로 얻은 해 $\left\{ y_{n} ( x_{ n } ) \ : \ x_{0} \le x_{n} \le b \right\} $ 에 대해 $$ \max_{ x_{0 } \le x_{n} \le b } | Y_{x_{n}} - y_{h}</description>
    </item>
    
    <item>
      <title>수치해석에서의 오일러 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/euler-method-in-numerical-analysis/</link>
      <pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/euler-method-in-numerical-analysis/</guid>
      <description>메소드 1 $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 가 주어져 있다. 구간 $(a,b)$ 을 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 특히 충분히 작은 $h &amp;gt; 0$ 에 대해 $x_{j} = x_{0} + j h$ 이라고 하면 초기값 $y_{0} \simeq Y_{0}$ 에 대해 $$ y_{n+1} = y_{n} + h f ( x_{n} , y_{n} ) $$ 설명 오일러 메소드는 개념적으로 아주 간단한 방법이지만 수</description>
    </item>
    
    <item>
      <title>립시츠 조건</title>
      <link>https://freshrimpsushi.github.io/posts/lipschitz-condition/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/lipschitz-condition/</guid>
      <description>정의 우리는 1계 미분방정식에 대한 존재성-유일성 정리의 스테이트먼트에서 립시츠 조건Lipschitz Condition을 발견할 수 있다. $D \subset \mathbb{R}^2$ 에서 정의된 연속함수 $f$ 에 대해 초기값 문제 $\begin{cases} y &#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ 가 주어져 있다. $f$ 가 모든 $(x,y_{1}) , (x , y_{2} ) \in D$ 와 $K &amp;gt; 0$ 에 대해 립시츠 조건 $$ |f(x,y_{1} ) - f(x,y_{2}) | \le K | y_{1} - y_{2} | $$ 을 만족하면 $(x_{0} , Y_{0})</description>
    </item>
    
    <item>
      <title>수치적으로 이상적분을 계산하기 위한 가우스 구적법</title>
      <link>https://freshrimpsushi.github.io/posts/gaussian-quadrature-to-calculate-numerically-improper-integration/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gaussian-quadrature-to-calculate-numerically-improper-integration/</guid>
      <description>정의 1 가우스-체비셰프 구적법 $$ \int_{-1}^{1} {{ 1 } \over { \sqrt{1 - x^2 } }} f(x) dx \approx \sum_{i=1}^{n} w_{i} f( x_{i} ) $$ $$ w_{i} = {{ \pi } \over { n }} $$ 여기서 $x_{i}$ 들은 $T_{n}(x) = 0$ 를 만족하는 체비셰프 노드다. 가우스-라게르 구적법 $$ \int_{0}^{\infty} e^{-x} f(x) dx \approx \sum_{i=1}^{n} w_{i} f( x_{i} ) $$ $$ w_{i} = {{ x_{i} } \over { (n+1)^2 \left[ L_{n+1} (x_{i} ) \right]^2 }} $$ 여기서 $x_{i}$ 들은 $L_{n}(x) = 0$ 를 만족하는 라게르 노드다. 가우스-에르미트 구적법 $$ \int_{-\infty}^{\infty} e^{-x^2} f(x) dx \approx \sum_{i=1}^{n} w_{i} f( x_{i} ) $$</description>
    </item>
    
    <item>
      <title>에르미트 다항함수</title>
      <link>https://freshrimpsushi.github.io/posts/hermite-polynomial/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/hermite-polynomial/</guid>
      <description>정의 확률론자의 에르미트 다항함수 $$ H_{e_{n}} := (-1)^{n} e^{{x^2} \over {2}} {{d^{n}} \over {dx^{n}}} e^{- {{x^2} \over {2}}} $$ 물리학자의 에르미트 다항함수 $$ H_{n} := (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2} $$ 기초 성질 에르미트 다항함수는 두가지 꼴이 쓰이며, $H_{n} (x) = 2^{{n} \over {2}} H_{e_{n}} \left( \sqrt{2} x \right)$ 와 같은 관계를 갖는다. 재귀 공식 H_{n+1} (x) = 2x H_{n} (x) - H_{n}&amp;rsquo; (X) $$ 직교 집합 [1] 함수의 내적: $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx$ 에 대해 웨이트 $w$ 를 $\displaystyle w(x) := e^{-x^2}$ 와 같이 주면 $\left\{ H_{0} , H_{1}, H_{2},</description>
    </item>
    
    <item>
      <title>라게르 다항함수</title>
      <link>https://freshrimpsushi.github.io/posts/laguerre-polynomial/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/laguerre-polynomial/</guid>
      <description>정의 $\displaystyle L_{n} := {{ e^{x} } \over { n! }} {{ d^{n} } \over { dx^{n} }} \left( e^{-x} x^{n} \right)$ 을 라게르 다항함수Laguerre Polynomial라 한다. 기초 성질 재귀 공식 [0]: $$L_{n+1} (x) = {{ 1 } \over { n+1 }} \left[ \left( 2n + 1 - x \right) L_{n} (x) - n L_{n-1} (x) \right]$$ 직교 집합 [1] 함수의 내적: $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx$ 에 대해 웨이트 $w$ 를 $\displaystyle w(x) := e^{-x}$ 와 같이 주면 $\left\{ L_{0} , L_{1}, L_{2}, \cdots \right\}$ 은 직교 집합이 된다. 설명 $n = 0, \cdots , 3$ 에 대한 라</description>
    </item>
    
    <item>
      <title>수치적으로 이상적분을 계산하기 위한 변수 치환 트릭</title>
      <link>https://freshrimpsushi.github.io/posts/substitution-trick-to-calculate-numerically-improper-integration/</link>
      <pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/substitution-trick-to-calculate-numerically-improper-integration/</guid>
      <description>정리 1 $0 &amp;lt; a &amp;lt; b &amp;lt; \infty$ 라고 하자. [1]: $ 0 &amp;lt; p &amp;lt; 1$ 면 $$\int_{0}^{b} {{ f(x) } \over {x^{p} }} dx = \int_{0}^{{{ 1 } \over { 1-p }} b^{1-p} } f \left( \left[ ( 1- p ) m \right]^{{{ 1 } \over { 1-p }}} \right) dm$$ [2]: $ 1 &amp;lt; p$ 면 $$\int_{a}^{ \infty } {{ f(x) } \over {x^{p} }} dx = \int_{0}^{{{ 1 } \over { p-1 }} a^{1-p}}f \left( \left[ ( p-1 ) m \right]^{{{ 1 } \over { 1-p }}} \right) dm$$ 설명 이상적분은 언제나 골칫덩이지만 풀이를 포기할 수는 없다. 보통 적분이 어려운 이유는 어지간한 트릭을 써봐도 정작 그 트</description>
    </item>
    
    <item>
      <title>가우스 구적법</title>
      <link>https://freshrimpsushi.github.io/posts/gaussian-quadrature/</link>
      <pubDate>Thu, 04 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/gaussian-quadrature/</guid>
      <description>정의 1 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 $a = x_{1} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. $$ I_{n} (f) := \sum_{j=1}^{n} w_{j} f ( x_{j} ) \approx \int_{a}^{b} w(x) f(x) dx = I ( f ) $$ 위와 같이 정의된 $I_{n}$ 의 가중치 $w_{j}$ 들을 구해서 수치적 적분을 계산하는 것을 가우스 구적법이라 한다. 설명 $f$ 를 잘 근사하는 다항함수 $p_{n-1}$ 이 존재하는 것은 보장되어있기 때문에, $f$ 대신 $p_{n-1}$ 을 생각해보려 한</description>
    </item>
    
    <item>
      <title>뉴턴-코테스 적분 공식</title>
      <link>https://freshrimpsushi.github.io/posts/newton-cotes-integration-formulas/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/newton-cotes-integration-formulas/</guid>
      <description>정의 1 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 간격이 $\displaystyle h:= {{b-a} \over {n}}$ 로 일정한 $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 다음과 같이 정의된 수치적 적분 오퍼레이터 $I_{n}^{p}$ 을 뉴턴-코테스 공식이라 한다. $$ I_{n}^{p} (f) := \sum_{i=0}^{n} w_{i} f ( x_{i} ) $$ $i=0,1,\cdots , n$ 에 대해 $x_{i} := a + i h$ 이고, $l_{i}$ 는 라그랑주 공식에서 쓰이는 다항함수 $\displaystyle l_{i} (x) := \prod_{i \ne j} \left( {{ x - x_{j} } \over { x_{i}</description>
    </item>
    
    <item>
      <title>심슨 룰</title>
      <link>https://freshrimpsushi.github.io/posts/simpson-rule/</link>
      <pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/simpson-rule/</guid>
      <description>정의 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 간격이 $\displaystyle h:= {{b-a} \over {n}}$ 로 일정한 $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 다음과 같이 정의된 수치적 적분 오퍼레이터 $I_{n}^{2}$ 을 심슨 룰이라 한다. $$ I_{n}^{2} (f) := \sum_{k=1}^{n/2} {{h} \over {3}} \left[ f(x_{2k-2}) + 4 f( x_{2k-1} ) + f(x_{2k} ) \right] $$ 정리 $f \in C^4 [a,b]$ 이라고 하자. 심슨 룰의 에러 $E_{1}^{2}$ 와 어심토틱 에러 $\tilde{E}_{n}^{2}$ 는 다음과 같다. [1]: $$E_{1}^{2} (f) = - {{h^5} \over {90}} f^{(4)} ( \xi</description>
    </item>
    
    <item>
      <title>사다리꼴 룰</title>
      <link>https://freshrimpsushi.github.io/posts/trapezoidal-rule/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/trapezoidal-rule/</guid>
      <description>정의 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 간격이 $\displaystyle h:= {{b-a} \over {n}}$ 로 일정한 $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 다음과 같이 정의된 수치적 적분 오퍼레이터 $I_{n}^{1}$ 을 사다리꼴 룰이라 한다. $$ I_{n}^{1} (f) := \displaystyle \sum_{k=1}^{n} {{h} \over {2}} \left( f(x_{k-1}) + f(x_{k} ) \right) $$ 정리 $f \in C^2 [a,b]$ 이라고 하자. 사다리꼴 룰의 에러 $E_{1}^{1}$ 와 어심토틱 에러 $\tilde{E}_{n}^{1}$ 는 다음과 같다. [1]: $$E_{1}^{1} (f) = - {{1} \over {12}} h^{3} f &#39;&amp;rsquo;</description>
    </item>
    
    <item>
      <title>수치적 적분</title>
      <link>https://freshrimpsushi.github.io/posts/numerical-intergration/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/numerical-intergration/</guid>
      <description>정의 1 $f : [a,b] \to \mathbb{R}$ 가 $[a,b]$ 에서 적분가능하고 $[a,b]$ 를 $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 적분 오퍼레이터 $I$ 를 $\displaystyle I(f) := \int_{a}^{b} f(x) dx$ 와 같이 정의한다. 적분 오퍼레이터 $I_{n}$ 을 $\displaystyle I_{n} (f) := \sum_{k=1}^{n} \int_{x_{k-1}}^{x_{k}} f(x) dx$ 와 같이 정의한다. 에러 $E_{n}$ 을 $E_{n} (f) := I (f) - I_{n} ( f )$ 와 같이 정의한다. $\displaystyle \lim_{n \to \infty} {{\tilde{E}_{n} (f) } \over { E_{n} (f) }} = 1$ 을 만족하는 $\tilde{E}_{n}$ 을 $E_{n}$ 에 대한 어심토틱 에러라고 한</description>
    </item>
    
    <item>
      <title>체비셰프 노드</title>
      <link>https://freshrimpsushi.github.io/posts/chebyshev-node/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/chebyshev-node/</guid>
      <description>정의 $[-1,1]$ 에서 $\displaystyle x_{k} = \cos \left( {{2k-1} \over {2n}} \pi \right)$, $k=1, \cdots , n$ 을 체비셰프 노드라 한다. 설명 체비셰프 노드는 일반적으로 사용하듯 일정한 간격의 노드 포인트와 달리 반원의 호를 일정한 크기로 자르고 그 점들을 $x$ 축으로 사영시킨 노드 포인트를 말한다. 점들의 분포는 가운데보다 양 끝에 조금 몰리는 모양새를 이룬다. 밑에서 다시 설명하겠지만, 이 점이 바로 체비셰프 노드의 장</description>
    </item>
    
    <item>
      <title>체비셰프 전개</title>
      <link>https://freshrimpsushi.github.io/posts/chebyshev-expansion/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/chebyshev-expansion/</guid>
      <description>빌드업 1 체비셰프 전개를 이해하기 위해서는 어떻게 체비셰프 전개가 나오는지를 먼저 알아야한다. 우선 최소극대화 문제를 푸는 대신 최소제곱 문제를 푼다고 생각해보자. $$ M_{n} (f) := \inf_{\deg(r) \le n } \left\| f - r \right\|_{2} $$ $f : [a,b] \to \mathbb{R}$ 에 대해 위와 같이 최소제곱 문제가 주어져있다고 하자. 목표는 $M_{n} (f)$ 를 최소화하는 $n$ 차 이하의 다항함수 $r_{n}^{ \ast }$ 를 찾는 것이다. 다행스럽게도</description>
    </item>
    
    <item>
      <title>수치해석학에서의 최소극대화 근사와 최소제곱 근사</title>
      <link>https://freshrimpsushi.github.io/posts/minimax-approximation-and-least-squares-approximation/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/minimax-approximation-and-least-squares-approximation/</guid>
      <description>빌드업 1 주어진 함수 $f : [a,b] \to \mathbb{R}$ 를 근사하는 문제가 주어져 있다고 하자. 계산은 컴퓨터의 몫이므로 다항함수로 $f$ 를 근사하는 것이 목표다. 함수를 근사시킨다는 것은 한 점에서의 계산이 아니라 정의역 $[a,b]$ 전체에서 $f$ 와 비슷한 함수를 사용하고 싶은 것이므로 가장 크게 틀리는 부분을 줄이는 것이 목표다. 이러한 상황을 최소극대화Minimax 문제라고 한다</description>
    </item>
    
    <item>
      <title>수치해석에서의 함수 근사</title>
      <link>https://freshrimpsushi.github.io/posts/approximation-of-functions-in-numerical-analysis/</link>
      <pubDate>Mon, 10 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/approximation-of-functions-in-numerical-analysis/</guid>
      <description>빌드업 수치적인 계산을 할 때 컴퓨터가 인간보다 압도적으로 빠른 것은 사실이지만, 딱히 컴퓨터가 초월함수와 무리수를 이해했기 때문은 아니다. 가령 $\displaystyle \sin {{ \pi } \over {6}} = {{1} \over { 2 }}$ 을 계산시킨다면 삼각함수의 기하학적인 정의를 이용해서 직각삼각형을 그려보고 빗변과 높이의 비를 구하는 게 아니라, 다항함수로 급수전개해서 사칙연산으로 구하는 식이다.</description>
    </item>
    
    <item>
      <title>수치해석학에서의 B-스플라인 </title>
      <link>https://freshrimpsushi.github.io/posts/b-spline-in-numerical-analysis/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/b-spline-in-numerical-analysis/</guid>
      <description>글과 수식이 읽기 싫으면 그냥 그림으로 보고 이해해도 무방하다. 정의 1 구간 $[a,b]$ 를 $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$ 와 같은 노드 포인트들로 쪼갰다고 하자. 주어진 자유도 $K$ 에 대해서 $x_{-K} &amp;lt; x_{-K + 1} &amp;lt; \cdots &amp;lt; x_{-1} &amp;lt; x_{0}$ 과 $x_{N} &amp;lt; x_{N + 1} &amp;lt; \cdots &amp;lt; x_{N+K-1} &amp;lt; x_{N+K}$ 의 추가적인 노드를 생각한다. $i$번째 노드 포인트와 자유도 $1 \le k \le K$ 에 디펜드하는 재귀함수 $$ B_{i,k}(x) := \begin{cases} \chi_{[x_{i} , x_{i+1} )} (x)</description>
    </item>
    
    <item>
      <title>수치해석에서의 스플라인</title>
      <link>https://freshrimpsushi.github.io/posts/spline-in-numerical-analysis/</link>
      <pubDate>Thu, 16 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/spline-in-numerical-analysis/</guid>
      <description>빌드업 인터폴레이션이란 정확한 함수를 복원하는 게 아니라 그와 유사하면서도 다루기 편한 함수를 구하는 것이 목적이다. 물론 익스플릭시트Explicit하고 계산이 쉬워지도록 구할 수 있다면야 제일 좋겠지만, 이 우주는 그렇게 만만한 곳이 아니다. 문제에 따라서는 간단한 부분을 빨리 풀고 복잡한 부분을 정교하게 풀어야할 수도 있고, 연속성조차 보장</description>
    </item>
    
    <item>
      <title>에르미트 인터폴레이션</title>
      <link>https://freshrimpsushi.github.io/posts/hermite-interpolation/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/hermite-interpolation/</guid>
      <description>정의 1 서로 다른 $x_{1} , \cdots , x_{n}$ 의 데이터 $(x_{1}, y_{1} , y&amp;rsquo;_{1}) , \cdots , (x_{n} , y_{n}, y&amp;rsquo;_{n})$ 에 대해 $\begin{cases} p (x_{i} ) = y_{i} \\ p&amp;rsquo;(x_{i} ) = y&amp;rsquo;_{i} \end{cases}$ 와 $\deg H \le 2n-1$ 을 만족하는 인터폴레이션인 다항 함수 $H$ 를 에르미트 인터폴레이션Hermite Interpolation이라고 한다. 정리 존재성과 유일성 [1]: 주어진 데이터에 대해서 $H$ 는 유일하게 존재한다. 라그랑주 폼 [2]: $$H_{n} (x) = \sum_{i=1}^{n} y_{i} h_{i} (x) + \sum_{i=1}^{n} y&amp;rsquo;_{i} \tilde{h}_{i} (x)$$</description>
    </item>
    
    <item>
      <title>에르미트-제노키 공식</title>
      <link>https://freshrimpsushi.github.io/posts/hermite-genocchi-formula/</link>
      <pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/hermite-genocchi-formula/</guid>
      <description>공식 서로 다른 $x_{0}, \cdots , x_{n}$ 에 대해 $f \in C^{n} \left( \mathscr{H} \left\{ x_{0}, \cdots , x_{n} \right\} \right)$ 이라 하자. 그러면 표준 심플렉스 $$ \tau_{n} := \left\{ ( t_{1} , \cdots , t_{n} ) : t_{i} \ge 0 \land \sum_{i=1}^{t} t_{i} \le 1 \right\} $$ 과 $\displaystyle t_{0} = 1 - \sum_{i=1}^{n} t_{i}$ 에 대해 다음이 성립한다. $$ f [ x_{0}, \cdots , x_{n} ] = \int \cdots \int_{\tau_{n}} f^{(n)} ( t_{0} x_{0} + \cdots + t_{n} x_{n} ) dt_{1} \cdots dt_{n} $$ $\mathscr{H} \left\{ a,b,c, \cdots \right\}$ 는 $a,b,c, \cdots$ 를 포함하는 가장 작은 구간을 나타낸다. 설명 에르미트-제노키 공식Hermite</description>
    </item>
    
    <item>
      <title>뉴턴 계차상 공식 유도</title>
      <link>https://freshrimpsushi.github.io/posts/derivation-of-newton-divided-difference-formula/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/derivation-of-newton-divided-difference-formula/</guid>
      <description>공식 서로 다른 $x_{0} , \cdots , x_{n}$ 의 데이터 $(x_{0}, f(x_{0} )) , \cdots , (x_{n} , f( x_{n} ) )$ 에 대해 $$ p_{n} (x) =\sum_{i=0}^{n} f [ x_{0} , \cdots , x_{i} ] \prod_{j=0}^{i-1} (x - x_{j} ) $$ 설명 복잡해보이지만 $n=0,1,2$ 에 대해서 실제로 전개를 해보면 다음과 같이 단순하게 나타난다. $$ \begin{align*} p_{0} (x) =&amp;amp; f(x_{0}) \\ p_{1} (x) =&amp;amp; f( x_{0} ) + (x - x_{0} ) f [ x_{0} , x_{1} ] \\ p_{2} (x) =&amp;amp; f( x_{0} ) + (x - x_{0} ) f [ x_{0} , x_{1} ] + ( x - x_{0} ) ( x - x_{1} ) f [ x_{0} , x_{1} , x_{2} ] \end{align*} $$ 뉴턴</description>
    </item>
    
    <item>
      <title>라그랑주 공식 유도</title>
      <link>https://freshrimpsushi.github.io/posts/derivation-of-lagrange-formula/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/derivation-of-lagrange-formula/</guid>
      <description>공식 1 서로 다른 $x_{0} , \cdots , x_{n}$ 의 데이터 $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$ 에 대해 $\displaystyle l_{i} (x) := \prod_{i \ne j} \left( {{ x - x_{j} } \over { x_{i} - x_{j} }} \right)$ 이라고 하면 $$ p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X) $$ 설명 라그랑주 공식은 폴리노미얼 인터폴레이션을 찾는 방법 중 가장 심플한 공식이다. 유도 전략: $l_{i}$ 이 인덱스에 대해 크로데커 델타 함수임을 보인다. $$ l_{i} (x_{i}) = \prod_{i \ne j} \left( {{ x_{i} - x_{j} } \over { x_{i} - x_{j} }} \right) = 1 $$ $$ l_{i} (x_{j})</description>
    </item>
    
    <item>
      <title>폴리노미얼 인터폴레이션</title>
      <link>https://freshrimpsushi.github.io/posts/polynomial-interpolation/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/polynomial-interpolation/</guid>
      <description>정의 1 서로 다른 $x_{0} , \cdots , x_{n}$ 의 데이터 $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$ 에 대해 $p (x_{i} ) = y_{i}$ 와 $\deg p \le n$ 을 만족하는 인터폴레이션인 다항 함수 $p$ 를 폴리노미얼 인터폴레이션Polynomial Interpolation이라 한다. 정리 존재성과 유일성 [1]: 주어진 데이터에 대해서 $p$ 는 유일하게 존재한다. 라그랑주 공식 [2]: $$p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X)$$ 뉴턴 계차상 공식 [3]: $$p_{n} (x) =</description>
    </item>
    
    <item>
      <title>수치해석에서의 인터폴레이션</title>
      <link>https://freshrimpsushi.github.io/posts/interpolation-in-numerical-analysis/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/interpolation-in-numerical-analysis/</guid>
      <description>정의 1 주어진 $(n+1)$쌍의 데이터 $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$ 에 대해 $f (x_{i} ) = y_{i}$ 를 만족하면서 어떤 특정한 성질을 가지는 $f$ 를 찾는 방법이나 그 함수 자체를 보간법 혹은 내삽법이라 한다. 설명 예를 들어 위와 같이 데이터가 있긴한데 가운데 데이터가 비어있는 상황을 생각해보자. 물론 실제 데이터가 있는게 가장 좋지만, 없으면 예측이라도 해서 써야할 상황이 있</description>
    </item>
    
    <item>
      <title>넌리니어 시스템을 풀기 위한 뉴턴 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/newton-method-for-nonlinear-system/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/newton-method-for-nonlinear-system/</guid>
      <description>메소드 1 $\mathbb{f} ( \mathbb{x} ) := \begin{bmatrix} f_{1}( \mathbb{x} ) \\ \vdots \\ f_{N} ( \mathbb{x} ) \end{bmatrix}$ 와 같은 다변수 함수 $\mathbb{f} : \mathbb{R}^{N} \to \mathbb{R}^{N}$ 가 $\mathbb{f} \in C^{2} \left( N ( \alpha ) \right)$ 이고 $\mathbb{f} ( \alpha ) = \mathbb{0}$, $\left[ D \mathbb{f} ( \alpha ) \right]^{-1}$ 이 존재한다고 하자. $\alpha$ 와 충분히 가까운 초기값 $\mathbb{x}_{0}$ 에 대해 $$ \mathbb{x}_{n+1} := \mathbb{x}_{n} - \left[ D \mathbb{f} ( \mathbb{x}_{n} ) \right]^{-1} f ( \mathbb{x}_{n} ) $$ 과 같이 정의된 수열 $\left\{ \mathbb{x}_{n} \right\}$ 은 $n \to \infty$ 일 때 $\alpha$ 로 쿼드러틱하게 수렴한다. $\mathbb{f} \in C^{2} \left( N ( \alpha ) \right)$ 이라는 것은 $\alpha$ 의 근방에서</description>
    </item>
    
    <item>
      <title>뮬러 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/muller-method/</link>
      <pubDate>Sat, 13 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/muller-method/</guid>
      <description>메소드 $f (\alpha) = 0$ 이라고 하자. 초기값 $x_{0} , x_{1} , x_{2}$ 과 $$ w_{n} := f [x_{n} , x_{n-1} ] + f [ x_{n} , x_{n-2} ] - f [ x_{n-2} , x_{n-1} ] $$ 에 대해 $$ x_{n+1} : = x_{n} - {{ 2 f ( x_{n} ) } \over { w_{n} \pm \sqrt{ w_{n}^{2} - 4 f (x_{n} ) f [ x_{n} , x_{n-1} , x_{n-2} ] } }} $$ 과 같이 정의된 수열 $\left\{ x_{n} \right\}$ 은 $n \to \infty$ 일 때 $\alpha$ 로 $p \approx 1.84$ 차 수렴한다. 단, $\left( w_{n} \pm \sqrt{ w_{n}^{2} - 4 f (x_{n} ) f [ x_{n} , x_{n-1} , x_{n-2} ] } \right) \in \mathbb{C}$ 는 $+$ 와 $-$ 둘 중 $\left| w_{n} \pm \sqrt{ w_{n}^{2} - 4 f (x_{n}</description>
    </item>
    
    <item>
      <title>시컨트 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/secant-method/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/secant-method/</guid>
      <description>메소드 $f,f&amp;rsquo;,f&amp;rsquo;&amp;rsquo;$ 가 $\alpha$ 의 근방에서 연속이고 $f(\alpha) = 0, f &#39;(\alpha) \ne 0$ 이라고 하자. $\alpha$ 와 충분히 가까운 초기값 $x_{0} , x_{1}$ 에 대해 $$ x_{n+1} := x_{n} - f ( x_{n} ) {{ x_{n} - x_{n-1} } \over { f ( x_{n} ) - f ( x_{n-1} ) }} $$ 과 같이 정의된 수열 $\left\{ x_{n} \right\}$ 은 $n \to \infty$ 일 때 $\alpha$ 로 $\displaystyle {{1 + \sqrt{5} } \over {2}}$ 차 수렴한다. 설명 황금비 수렴차수가 상당히 낯이 익을 것이다. 바로 황금비인 $\displaystyle {{1 + \sqrt{5} } \over {2}} = 1.618 \cdots$ 인데, 수열의 정의에</description>
    </item>
    
    <item>
      <title>수치해석학에서의 계차상</title>
      <link>https://freshrimpsushi.github.io/posts/divided-difference/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/divided-difference/</guid>
      <description>정의 함수 $f : \mathbb{R} \to \mathbb{R}$ 와 서로 다른 $x_{1} , \cdots , x_{n}$ 에 대해 다음을 $f$ 의 계차상Divided Difference이라고 한다. $$ \begin{align*} f[x_{0}] :=&amp;amp; f( x_{0} ) \\ f [ x_{0} , x_{1} ] :=&amp;amp; {{ f ( x_{1} ) - f ( x_{0} ) } \over { x_{1} - x_{0} }} \\ f [ x_{0} , x_{1} , x_{2} ] :=&amp;amp; {{ f [ x_{1} , x_{2} ] - f [ x_{0} , x_{1} ] } \over { x_{2} - x_{0} }} \\ f [ x_{0} , \cdots , x_{n} ] :=&amp;amp; {{ f [ x_{1} , \cdots , x_{n} ] - f [ x_{0} , \cdots , x_{n-1} ] } \over { x_{n} - x_{0}</description>
    </item>
    
    <item>
      <title>뉴턴-랩슨 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/newton-raphson-method/</link>
      <pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/newton-raphson-method/</guid>
      <description>메소드 1 $f,f&amp;rsquo;,f&amp;rsquo;&amp;rsquo;$ 가 $\alpha$ 의 근방에서 연속이고 $f(\alpha) = 0, f &#39;(\alpha) \ne 0$ 이라고 하자. $\alpha$ 와 충분히 가까운 초기값 $x_{0}$ 에 대해 $$ x_{n+1} := x_{n} - {{ f ( x_{n} ) } \over { f &#39; ( x_{n} ) }} $$ 과 같이 정의된 수열 $\left\{ x_{n} \right\}$ 은 $n \to \infty$ 일 때 $\alpha$ 로 쿼드러틱하게 수렴한다. 설명 뉴턴-랩슨 메소드는 그냥 뉴턴 메소드라고 불리기도 한다. 미분가능성이나 연속성과 같은 조건이 있긴 하지만 그래도 간단하고 수</description>
    </item>
    
    <item>
      <title>바이섹션 메소드</title>
      <link>https://freshrimpsushi.github.io/posts/bisection-method/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/bisection-method/</guid>
      <description>메소드 1 연속함수 $f$ 가 폐구간 $[a,b]$ 에서 $f(a) f(b) &amp;lt; 0$ 이라고 하자. 허용오차는 $\varepsilon$ 이다. $f(c) = 0$ 를 만족하는 $c \in [a,b]$ 는 다음과 같이 구할 수 있다. Step 1. $$c:= {{a+b} \over {2}}$$ Step 2. $b-c \le \varepsilon$ 이면 $c$ 를 반환한다. Step 3. $f(b) f(c) &amp;lt; 0$ 이면 $a:=c$, 아니면 $b:=c$ 이라 둔다. 그 후 Step 1. 으로 돌아간다. 설명 중간값 정리의 대표적인 응용으로써, 해가 존재하는 구간을 계속 절반으로 줄여가며 방정식 $f(x) = 0$</description>
    </item>
    
    <item>
      <title>수치해석에서의 수렴률</title>
      <link>https://freshrimpsushi.github.io/posts/rate-of-convergence-in-numerical-analysis/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/rate-of-convergence-in-numerical-analysis/</guid>
      <description>정의 1 $\alpha$ 로 수렴하는 수열 $\left\{ x_{n} \right\}$ 이 차수Order $p \ge 1$ 에 대해 $$ | \alpha - x_{n+1} | \le c | \alpha - x_{n} | ^{p} $$ 을 만족시키는 $c \ge 0$ 가 존재하면 $\left\{ x_{n} \right\}$ 이 수렴률 $c$ 로 $\alpha$ 에 $p$ 차 수렴한다고 한다. 설명 특히 $c &amp;lt; 1$ 이라는 조건과 함께 $p=1$ 이면 선형 수렴Linear Convergence이라 부른다. 비슷하게 $p=2$ 일 때는 Quadratic Convergence , $p=3$ 일 때는 Cubic Convergence</description>
    </item>
    
    <item>
      <title>제1종 제2종 체비셰프 다항함수의 관계</title>
      <link>https://freshrimpsushi.github.io/posts/relations-between-chebyshev-polynomials/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/relations-between-chebyshev-polynomials/</guid>
      <description>정리 제1종 체비셰프 다항함수 $T_{n} (x) = \cos \left( n \cos^{-1} x \right)$ 과 제2종 체비셰프 다항함수 $\displaystyle U_{n} (x) = {{1} \over {n+1} } T_{n+1} &amp;rsquo; (X)$ 은 다음의 관계를 가진다. [1]: $$U_{n} (x) - U_{n-2} (x) = 2 T_{n} (X)$$ [2]: $$T_{n} (x) - T_{n-2} (x) = 2( x^2 - 1 ) U_{n-2} (x)$$ 보통 $0 \le \theta \le \pi$ 에 대해 $\theta := \cos^{-1} x $ 라고 둔다. 증명 위 등식들을 증명하는 데에는 아래의 팩트가 필수적이다. 제2종 체비셰프 다항함수의 다른 표현: $$U_{n} (x) = {{\sin \left( ( n</description>
    </item>
    
    <item>
      <title>제2종 체비셰프 다항함수</title>
      <link>https://freshrimpsushi.github.io/posts/chebyshev-polynomial-of-the-second-kind/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/chebyshev-polynomial-of-the-second-kind/</guid>
      <description>정의 $$U_{n} (x) := {{1} \over {n+1} } T_{n+1} &amp;rsquo; (x) = {{\sin \left( ( n +1 ) \theta \right)} \over { \sin \theta }} $$ 을 제2종 체비셰프 다항함수라 한다. 기초 성질 재귀 공식 [0]: $$U_{n+1} (x) = 2x U_{n} (x) - U_{n-1} (X)$$ 직교 집합 [1] 함수의 내적: $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx$ 에 대해 웨이트 $w$ 를 $\displaystyle w(x) := \sqrt{1 - x^2}$ 와 같이 주면 $\left\{ U_{0} , U_{1}, U_{2}, \cdots \right\}$ 은 직교 집합이 된다. 체비셰프 노드 [2]: $\displaystyle U_{n} (X)$ 의 근은 $k=1, \cdots , n$ 에 대해 다음과 같다. $$x_{k} = \cos \left( {{k} \over {n+1}} \pi \right)$$ 기</description>
    </item>
    
    <item>
      <title>제1종 체비셰프 다항함수</title>
      <link>https://freshrimpsushi.github.io/posts/chebyshev-polynomial-of-the-first-kind/</link>
      <pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/chebyshev-polynomial-of-the-first-kind/</guid>
      <description>정의 1 $T_{n} (x) = \cos \left( n \cos^{-1} x \right)$ 을 제1종 체비셰프 다항함수라 한다. 기초 성질 재귀 공식 [0]: $$T_{n+1} (x) = 2x T_{n} (x) - T_{n-1} (X)$$ 직교 집합 [1] 함수의 내적: $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx$ 에 대해 웨이트Weight $w$ 를 $\displaystyle w(x) := {{1} \over { \sqrt{1 - x^2} }}$ 와 같이 주면 $\left\{ T_{0} , T_{1}, T_{2}, \cdots \right\}$ 은 직교 집합이 된다. 체비셰프 노드 [2]: $T_{n} (x)$ 의 근은 $k=1, \cdots , n$ 에 대해 다음과 같은 체비셰프 노드다. $$x_{k} = \cos \left( {{2k-1} \over {2n}} \pi \right)$$</description>
    </item>
    
    <item>
      <title>수치해석에서의 차분</title>
      <link>https://freshrimpsushi.github.io/posts/difference-in-numerical-analysis/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://freshrimpsushi.github.io/posts/difference-in-numerical-analysis/</guid>
      <description>정의 1 전방차분: $$ \begin{align*} \Delta f(x) =&amp;amp; f(x+h) - f(x) \\ \Delta^{r+1} f(x) =&amp;amp; \Delta^{r} f(x+h) - \Delta^{r} f(x) \end{align*} $$ 후방차분: $$ \begin{align*} \nabla f(x) =&amp;amp; f(x) - f(x- h) \\ \nabla^{r+1} f(x) =&amp;amp; \nabla^{r} f(x) - \nabla^{r} f(x- h) \end{align*} $$ 설명 일반적으로 계차Difference란 수열 전반에서 사용하는 말이지만 수치해석에선 특히 두 노드포인트의 함숫값의 차를 말한다. 사실 고등학교때부터 계속 봐왔기 때문에 익숙하다면 익숙한 연산자인데 수치해석에서 자주</description>
    </item>
    
  </channel>
</rss>
