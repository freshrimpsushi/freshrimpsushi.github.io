<!doctype html><html class=blog lang=ko><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=preload href=https://freshrimpsushi.github.io/ko/css/style.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=preload href=https://freshrimpsushi.github.io/ko/css/comment.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=icon href=https://freshrimpsushi.github.io/ko/logo/favicon.ico><meta name=msapplication-TileColor content="#FFFFFF"><meta name=msapplication-TileImage content="logo/basic.png"><meta name=NaverBot content="All"><meta name=NaverBot content="index,follow"><meta name=Yeti content="All"><meta name=Yeti content="index,follow"><meta name=google-site-verification content="KYAokS7-6C5YuXOjatJQsiK1T0O8x4YncYFIF4tneYI"><meta name=naver-site-verification content="e5651d6f97899061897203413efc84994f04bbba"><link rel=alternate type=application/rss+xml title=생새우초밥집 href=https://freshrimpsushi.github.io/ko/index.xml><title>머신러닝에서 강화학습이란</title></head><meta name=title content="머신러닝에서 강화학습이란"><meta name=description content="국내 최대의 수학, 물리학, 통계학 블로그"><meta property="og:title" content="머신러닝에서 강화학습이란"><meta property="og:description" content><meta property="og:image" content="https://freshrimpsushi.github.io/ko/logo/basic.png/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"j_","name":"머신러닝에서 강화학습이란","headline":"머신러닝에서 강화학습이란","alternativeHeadline":"","description":"정의 강화학습이란, 에이전트가 환경과 상호작용하여 누적 보상을 최대화하는 정책을 찾을 수 있도록 하는 것이다. 설명1 강화학습을 이루고 있는 요소들은 다음과 같다","inLanguage":"ko","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/freshrimpsushi.github.io\/ko\/posts\/3029\/"},"author":{"@type":"Person","name":"전기현","url":"https://math.stackexchange.com/users/459895/ryu-dae-sick"},"creator":{"@type":"Person","name":"전기현"},"accountablePerson":{"@type":"Person","name":"전기현"},"copyrightHolder":"생새우초밥집","copyrightYear":"2021","dateCreated":"2021-02-04T00:00:00.00Z","datePublished":"2021-02-04T00:00:00.00Z","dateModified":"2021-02-04T00:00:00.00Z","publisher":{"@type":"Organization","name":"생새우초밥집","url":"https://freshrimpsushi.github.io/ko/","logo":{"@type":"ImageObject","url":"https:\/\/freshrimpsushi.github.io\/ko\/logo\/basic.png","width":"32","height":"32"}},"image":"https://freshrimpsushi.github.io/ko/logo/basic.png","url":"https:\/\/freshrimpsushi.github.io\/ko\/posts\/3029\/","wordCount":"3706","genre":[],"keywords":[]}</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/mhchem.min.js integrity=sha384-F2ptQFZqNJuqfGGl28mIXyQ5kXH48spn7rcoS0Y9psqIKAcZPLd1NzwFlm/bl1mH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>function renderKaTex(e){renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],trust:!0,trust:e=>["\\htmlId","\\href","\\includegraphics"].includes(e.command),macros:{"\\eqref":"(\\text{#1})","\\ref":"\\href{###1}{\\text{#1}}","\\label":"\\htmlId{#1}{}","\\sech":"\\operatorname{sech}","\\csch":"\\operatorname{csch}","\\sgn":"\\operatorname{sgn}","\\sign":"\\operatorname{sign}","\\sinc":"\\operatorname{sinc}","\\diag":"\\operatorname{diag}","\\diam":"\\operatorname{diam}","\\trace":"\\operatorname{trace}","\\Tr":"\\operatorname{Tr}","\\tr":"\\operatorname{tr}","\\re":"\\operatorname{Re}","\\im":"\\operatorname{Im}","\\Var":"\\operatorname{Var}","\\Poi":"\\operatorname{Poi}","\\Cov":"\\operatorname{Cov}","\\span":"\\operatorname{span}","\\supp":"\\operatorname{supp}","\\rank":"\\operatorname{rank}","\\nullity":"\\operatorname{nullity}","\\ad":"\\operatorname{ad}","\\Ric":"\\operatorname{Ric}","\\i":"\\mathrm{i}","\\d":"\\mathrm{d}","\\cR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5863.png}","\\acR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.4371.png}","\\bcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5899.png}","\\abcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.0596.png}","\\crH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\smallcrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\acrH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}","\\smallacrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}"},throwOnError:!1})}</script><body class=main><header><a href=https://freshrimpsushi.github.io/ko/ rel=home><p style=text-align:center;font-size:1rem;color:#000><img src=https://freshrimpsushi.github.io/ko/logo/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D.png style=height:80px alt=logo></p></a></header><form method=get action=/ko/search style=border:1px;text-align:center><div class=field><input type=text id=searchtext placeholder=🔍︎ class=input_text name=s style=background-color:#eee;text-align:center;width:200px;font-size:1.5rem;border:1px;border-radius:5px;padding-top:5px;padding-bottom:5px;margin-bottom:1.5rem></div></form><aside style=text-align:center;margin-bottom:1rem><a href=https://freshrimpsushi.github.io/ko//posts/3029/>한국어</a> |
<a href=https://freshrimpsushi.github.io/en//posts/3029/>English</a> |
<a href=https://freshrimpsushi.github.io/jp//posts/3029/>日本語</a></aside><div class=wrapper><div class=content><div class=content-box><title>머신러닝에서 강화학습이란</title>
<a href=https://freshrimpsushi.github.io/ko/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/ style=background-color:rgba(0,0,0,.8);color:orange;border-radius:10px;padding:5px>📂머신러닝</a><h1>머신러닝에서 강화학습이란</h1><aside><div class=innerheader><div class=innertoc><b>목차</b><nav id=TableOfContents><ul><li><a href=#정의>정의</a></li><li><a href=#설명1>설명</a></li><li><a href=#강화학습의-문제-격자-모델>강화학습의 문제: 격자 모델</a><ul><li><a href=#에이전트>에이전트</a></li><li><a href=#상태>상태</a></li><li><a href=#행동>행동</a></li><li><a href=#정책>정책</a></li><li><a href=#보상>보상</a></li><li><a href=#환경>환경</a></li><li><a href=#에피소드>에피소드</a></li></ul></li></ul></nav></div></div></aside><h2 id=정의>정의</h2><p>강화학습이란, <strong>에이전트</strong>가 <strong>환경</strong>과 상호작용하여 <strong>누적 보상</strong>을 최대화하는 <strong>정책</strong>을 찾을 수 있도록 하는 것이다.</p><h2 id=설명1>설명<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></h2><p>강화학습을 이루고 있는 요소들은 다음과 같다.</p><ul><li><strong>에이전트<sup>agent</sup></strong>: 주어진 상태에 대해서, 정책에 따라 행동을 결정한다.</li><li><strong>스테이트<sup>state, 상태</sup></strong>: 에이전트가 처한 상황을 말한다.</li><li><strong>액션<sup>action, 행동</sup></strong>: 에이전트가 주어진 상태에서 취할 수 있는 선택지를 말한다.</li><li><strong>폴리시<sup>policy, 정책</sup></strong>: 에이전트가 주어진 상태에서 행동을 결정하는 전략을 말한다.</li><li><strong>리워드<sup>reward, 보상</sup></strong>: 에이전트가 주어진 상태에서 선택한 행동에 따라서 받는 점수를 말한다. 에이전트가 달성해야할 목표라고 볼 수 있다.</li><li><strong>환경<sup>environment</sup></strong>: 에이전트가 주어진 상태에서 어떠한 행동을 결정하면, MDP에 따라 다음 상태와 그에 따른 보상을 결정한다.</li><li><strong>에피소드<sup>episode</sup></strong>: 에이전트와 환경의 상호작용이 시작된 때부터 끝날 때까지를 말한다.</li></ul><p>이를 여러 상황에 비유하면 다음과 같다.</p><table><thead><tr><th style=text-align:center>강화학습</th><th style=text-align:center>시험공부</th><th style=text-align:center>바둑</th></tr></thead><tbody><tr><td style=text-align:center>에이전트</td><td style=text-align:center>학생</td><td style=text-align:center>바둑기사</td></tr><tr><td style=text-align:center>스테이트</td><td style=text-align:center>시험까지남은날짜</td><td style=text-align:center>바둑판</td></tr><tr><td style=text-align:center>액션</td><td style=text-align:center>공부, 음주, 게임 등</td><td style=text-align:center>수 놓기</td></tr><tr><td style=text-align:center>폴리시</td><td style=text-align:center>날짜별 공부 계획</td><td style=text-align:center>전략</td></tr><tr><td style=text-align:center>리워드</td><td style=text-align:center>시험점수</td><td style=text-align:center>승패</td></tr><tr><td style=text-align:center>에피소드</td><td style=text-align:center>시험기간</td><td style=text-align:center>한 판</td></tr></tbody></table><h2 id=강화학습의-문제-격자-모델>강화학습의 문제: 격자 모델</h2><p>강화학습을 설명하기 위한 대표적인 예시로 격자 모델<sup>grid world</sup>이 있다(영어로 말할 땐 보통 grid 'world'라 하고, 한국어로 말할 땐 격자 '모델'이라는 표현을 많이 쓴다). 이제 다음의 격자 모델을 예시로 들어 각 요소들을 구체적으로 설명하겠다. 한 번에 상하좌우 네 방향 중 하나로 한 칸씩 움직일 수 있는 로봇이 아래와 같은 $4 \times 4$ 격자에서 움직이는 경우를 생각해보자. 시작칸은 $\boxed{\ 2\ }$부터 $\boxed{15}$까지 임의로 정해지고, 로봇이 $\fcolorbox{black}{darkgray}{\ 1\ }$ 혹은 $\fcolorbox{black}{darkgray}{16}$까지 최단거리로 가는 것이 목표라고 하자.</p><h3 id=에이전트>에이전트</h3><p>강화학습에서 에이전트란 학습을 하는 주체로 설명되지만, 실재하지는 않는다. 후술할 다른 개념들이 확률변수 등으로 정의되는 것과는 달리, 에이전트는 명확한 수학적인 정의가 없다. 따라서 강화학습에 대한 이론 공부는 에이전트라는 대상이 없이도 가능하고, 실제로도 그렇다. 강화학습 이론에서 본질적으로 에이전트를 의미하는 것은 정책이다. 다만 직관적으로는 학습을 하는 대상이 있다고 생각하는 것이 편하므로, &ldquo;에이전트가 행동한다&rdquo;, &ldquo;에이전트의 상태가 바뀌었다&rdquo; 등과 같이 표현한다. 에이전트는 단지 컴퓨터 시뮬레이션(특히 게임)에서 캐릭터와 같이 <strong>학습하는 것처럼 보이는 것</strong>일 뿐이다. 가령 격자 모델에서 에이전트가 아래의 오른쪽 그림과 같이 이동하는 것은 단순히 상태의 나열으로도 표현할 수 있다.
$$
\boxed{\ 3\ } \to \boxed{\ 2\ } \to \fcolorbox{black}{darkgray}{\ 1\ }
$$
$3, 2, 1$을 순서대로 <code>print</code>하기만 해도 된다. 강화학습의 끝에 결국 우리가 얻고자 하는 것은 본질적으로 정책이니까, 에이전트라는 것을 정의하지 않아도 학습할 수 있다. 한마디로 에이전트란 <b>정책의 시각화(실재화)</b>라고 할 수 있다.</p><p>물론 위의 얘기는 이론과 컴퓨터 시뮬레이션에서 그렇다는거고, 자율주행과 같은 실제 응용에서는 정책에 따라 실제로 움직이는 드론이나 자동차가 필요하다. 이 때 드론, 자동차와 같은 로봇이나 기계들이 에이전트가 되고, 없으면 정책의 학습이 불가능하다.</p><h3 id=상태>상태</h3><p><img src=1.PNG#center alt=1.PNG></p><p><strong>상태</strong><sup>state</sup>는 <a href=../1433>확률변수</a>이며 state의 앞글자를 따서 $S$라고 표기한다. 에피소드는 시간에 따라 순차적으로 진행되기 때문에 인덱스로 $t$를 쓴다. 따라서 타임스텝이 $t$일 때의 스테이트 함수를 $S_{t}$라고 표기한다. 처음 스테이트는 보통 $t=0$으로 나타낸다. 정리하면 $S_{t}$는 시간이 $t$일 때 각각의 격자들에 대해서 다음과 같이 함숫값을 주는 함수이다.</p><p>$$
S_{t} \left( \boxed{ N } \right) = n,\quad 1\le n \le 16
$$</p><p>이때 가능한 모든 상태값(상태함수의 함숫값)들의 집합을 $\mathcal{S}\subset \mathbb{R}$라고 표기하고, 이의 원소를 $s$로 표기한다.</p><p>$$
\mathcal{S} = \left\{ s_{1}, s_{2},\dots \right\}
$$</p><p>그러면 위 격자모델에 대한 상태함수는 다음과 같다.</p><p>$$
S_{t} : \left\{ \fcolorbox{black}{darkgray}{\ 1\ } , \boxed{\ 2\ }, \dots, \boxed{15}, \fcolorbox{black}{darkgray}{16} \right\} \to \mathcal{S}
\\ S_{t} \left( \boxed{\ n\ } \right) = s_{n} = n,\quad 1\le n \le 16
$$</p><p>그러면 시간이 $t$일 때 상태값이 $s_{6}$였다가 다음 타임스텝에서 상태값이 $s_{10}$으로 바뀔 <a href=../1431>확률</a>은 다음과 같다.</p><p>$$
P \left( S_{t+1} = s_{10} | S_{t} = s_{6} \right)
$$</p><p>도달한 순간 에피소드가 끝나게 되는 상태를 <strong>터미널 스테이트</strong><sup>terminal state</sup>라고 한다. 위 격자 모델에서 터미널 스테이트는 $\fcolorbox{black}{darkgray}{1}, \fcolorbox{black}{darkgray}{16}$이다.</p><h3 id=행동>행동</h3><p><strong>행동</strong><sup>action</sup>이란 에이전트가 현재 상태에서 취할 수 있는 선택지를 말하며 이 또한 확률변수이다. action의 앞글자를 따와서 $A_{t}$로 표기한다. 위의 격자모델 예시에서는 $\boxed{2}$ ~ $\boxed{15}$에서 각각 상하좌우를 선택할 수 있다. 가능한 모든 행동값(행동함수의 함숫값)들의 집합을 $\mathcal{A}\subset \mathbb{R}$라고 표기하고, 이의 원소를 $a$라고 표기한다.</p><p>$$
\mathcal{A} = \left\{ a_{1}, a_{2}, \dots \right\}
$$</p><p>그러면 타임스텝 $t$에서의 행동함수는 다음과 같다.</p><p>$$
A_{t} : \left\{ \uparrow, \rightarrow, \downarrow, \leftarrow \right\} \to \mathcal{A} \\
\begin{cases}
A_{t}(\uparrow) = a_{1} \\
A_{t}(\rightarrow) = a_{2} \\
A_{t}(\downarrow) = a_{3} \\
A_{t}(\leftarrow) = a_{4}
\end{cases}
$$</p><p>에이전트는 주어진 상태에서 확률에 따라 행동을 결정한다. 예를들어 타임스텝이 $t$일 때 상태값이 $s_{6}$인 상황에서 행동 $a_{1}$을 선택한 확률은 다음과 같다.</p><p>$$
P(A_{t} = a_{1} | S_{t} = s_{6})
$$</p><h3 id=정책>정책</h3><p><strong>정책</strong><sup>policy</sup>은 상태 $s$에서 행동 $a$를 결정할 확률을 모든 $s$와 $a$에 대해서 명시한 것을 말하며 $\pi$로 표기한다. 게임이나 전쟁으로 비유하자면 전략이다. 격자모델 예시에서 행동을 결정할 확률이 $\dfrac{1}{4}$로 모두 같다고 하면 정책 $\pi$는 다음과 같다.</p><p>$$
\pi
\begin{cases}
P(a_{1} | s_{2}) = \dfrac{1}{4} \\
P(a_{2} | s_{2}) = \dfrac{1}{4} \\
P(a_{3} | s_{2}) = \dfrac{1}{4} \\
\vdots \\
P(a_{2} | s_{15}) = \dfrac{1}{4} \\
P(a_{3} | s_{15}) = \dfrac{1}{4} \\
P(a_{4} | s_{15}) = \dfrac{1}{4}
\end{cases} \quad \text{or} \quad
\pi : \mathcal{S} \times \mathcal{A} \to [0,1]
$$</p><p>물론 이는 최적화된 정책은 아니다. 간단히 $\boxed{2}$의 경우만 생각해봐도 위쪽으로 가면 격자 밖으로 벗어난 것이므로 위쪽으로 갈 확률 자체가 아예 없는 것이 더 나은 정책이다. 따라서 아래의 그림에서 $\pi_{1}$보다 $\pi_{2}$가 더 좋은 정책이라고 말할 수 있다.</p><p><img src=2.PNG#center alt=2.PNG></p><p>강화학습 알고리즘의 목표는 최적의 정책을 찾는 것이다. 그러면 최적의 정책은 어떻게 찾느냐 하는건데, 정책의 좋은 정도를 평가하는 <strong>가치함수</strong><sup>value function</sup>를 통해서 찾을 수 있다.</p><h3 id=보상>보상</h3><p><strong>보상</strong><sup>reward</sup>이란 주어진 상태에서 에이전트가 선택한 행동에 따라 실수를 매핑하는 함수이며 reward의 앞글자를 따서 $R_{t}$라 표기한다. 모든 보상값(보상함수의 함숫값)들의 집합을 $\mathcal{R} \subset \mathbb{R}$이라 표기하며, 이의 원소를 $r$으로 표기한다.</p><p>$$
\mathcal{R} = \left\{ r_{1}, r_{2}, \dots \right\} \\
R_{t} = \mathcal{S} \times \mathcal{A} \to \mathcal{R}
$$</p><p>보상은 한 번의 타임스텝마다 한 번씩 받게 되며 한 번의 에피소드에서 받은 총 보상, 그러니까 누적된 보상이 가장 크게 끔 하는 정책을 찾는 것이 강화학습의 궁극적인 목표이다.</p><p>그렇다면 왜 각각의 타임스텝에서의 보상보다 누적 보상이 커지도록 하는 것인지 궁금할 수 있다. 이는 시험공부로 비유하면 쉽게 이해가 갈 것이다. 시험기간동안 매일 저녁에 공부 대신 술 마시고 놀거나 게임을 하면 당장은 공부를 하는 것보다 더 즐거울 것이다. 하지만 누적보상, 그러니까 시험 성적은 개판일 것이다. 따라서 당장은 공부를 하는 것이 지치고 힘들지라도 미래의 큰 보상을 위해서 공부를 하는 것이 낫다고 판단하여 시험공부를 하는 것이다.</p><p>보상은 사람이 정하는 하이퍼 파라매터이다. 따라서 에이전트가 수행해야할 일에 따라서 적절하게 정해야한다. 가령 격자모델 예시에서 격자가 미로이고 에이전트가 미로를 탈출하는 로봇이라면 한 칸을 이동할 때마다 $-1$의 보상, 터미널 스테이트에 도착하면 $+10$의 보상을 주는 식으로 정할 수 있다. 격자가 공원이고 에이전트가 애완동물을 산책시키는 로봇이라면 한 칸을 이동할 때 마다 $0$의 보상, 터미널 스테이트에 도착하면 $+10$의 보상을 주는 식으로 정할 수 있다.</p><h3 id=환경>환경</h3><p><strong>환경</strong><sup>environment</sup>은 에이전트가 주어진 상태에서 선택한 행동에 따라 다음 상태와 보상을 결정하는 함수, 즉 $f : (s,a) \mapsto (s^{\prime},r)$이다. 따라서 항상 현실에 딱 들어맞는 비유를 찾기는 어렵다.</p><p>타임스텝이 $t$일 때의 상태를 $s_{t}$, $s_{t}$에서 선택한 행동을 $a_{t}$라고 하자. 이에 따라 환경이 결정한 다음 상태를 $s_{t+1}$, 보상을 $r_{t+1}$라고 하면 다음과 같이 나타낸다.</p><p>$$
f(s_{t}, a_{t}) = (s_{t+1}, r_{t+1})
$$</p><p>격자모델 예시에 대해서 구체적으로 설명하면 다음과 같다. 에이전트가 $\boxed{7}$에서 $\uparrow$를 선택하여 환경이 다음 상태 $\boxed{3}$와 보상 $-1$를 결정했다면 다음과 같은 수식으로 나타낸다.</p><p>$$
f(s_{7}, a_{1}) = (s_{3}, -1)
$$</p><p>에이전트가 행동을 결정하는 전략을 <strong>정책</strong>이라고 부른다면, 환경이 다음 상태와 보상을 결정하는 것을 <strong>MDP</strong><sup>markov decision process, 마코프 결정 과정</sup>라 한다. 에이전트와 환경의 상호작용을 그림으로 나타내면 다음과 같다.</p><p><img src=3.PNG#center alt=3.PNG></p><h3 id=에피소드>에피소드</h3><p>에이전트와 환경이 상호작용하면서 결정된 상태, 행동, 보상의 수열을 <strong>경로</strong><sup>trajectory, 궤적</sup> 혹은 <strong>히스토리</strong><sup>history</sup> 이라고 한다. 경로가 유한한 경우를 <strong>episode task</strong>라고 한다. 위에서 예로든 시험기간, 바둑, 격자모델 모두 이에 해당한다.</p><p>$$
s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \dots, a_{T-1}, s_{T}, r_{T} \\
\text{or} \\
(s_{0},) \overset{a_{0}}{\to} (s_{1}, r_{1}) \overset{a_{1}}{\to} (s_{2}, r_{2}) \overset{a_{2}}{\to} \cdots \overset{a_{T-1}}{\to} (s_{T}, r_{T})
$$</p><p>경로가 무한한 경우를 <strong>continuing task</strong>라고 한다. 다만 아주 긴 시간동안 지속되는 에피소드는 무한한 경우라고 간주 하기도 한다.</p><p>$$
s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \dots, a_{t-1}, s_{t}, r_{t}, a_{t}, s_{t+1}, r_{t+1},\dots \\
\text{or} \\
(s_{0},) \overset{a_{0}}{\to} (s_{1}, r_{1}) \overset{a_{1}}{\to} (s_{2}, r_{2}) \overset{a_{2}}{\to} \cdots \overset{a_{t-1}}{\to} (s_{t}, r_{t}) \overset{a_{t}}{\to} (s_{t+1}, r_{t+1}) \overset{a_{t+1}}{\to} \cdots
$$</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>오일석, 기계 학습(MACHINE LEARNING). 2017, p466-480&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><aside style=text-align:right>2021-02-04&emsp;
전기현&emsp;
<a href=../733>🎲 3029</a></aside><script>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="머신러닝에서 강화학습이란",c="https://freshrimpsushi.github.io/ko/posts/3029/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script></div><aside><h2>댓글</h2><div class=area-reply><div class=list-reply></div></div><div class=write-box><div class="write-author-info box-account"><input type=hidden name=cmt_idx>
<input class=ai-author type=text placeholder=Name>
<input class=ai-password type=password maxlength=20 placeholder=Password></div><div class=write-content><textarea class=ai-content value placeholder=내용 style=ime-mode:active></textarea></div><button class=write-button onclick=write_comment() aria-label=send><i class='fa-solid fa-paper-plane'></i></button><aside class=tex>댓글에도 $\TeX$이 적용됩니다.</aside></div></aside><script>let commentRows=[];const listReply=document.querySelector(".list-reply");document.addEventListener("DOMContentLoaded",()=>{get_all_comment()});function get_all_comment(){fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=getAllComment";return postFetch(t,{board_idx:"3029",user_ip:e})}).then(e=>e.json()).then(e=>{e.ok&&(commentRows=e.rows,render_comment(commentRows))}).catch(e=>console.error(e))}function render_comment(e){const n=["류대식","전기현","ㅇㅇ","질문"],s=["류대식","전기현"];render_blank();let t="";e.map(e=>{t+=`<div id="comment${e.cmt_idx}" class="parents">`,t+=`<div class="content-info">`;let o="";e.cmt_cnt>125?o="🥇":e.cmt_cnt>25?o="🥈":e.cmt_cnt>5&&(o="🥉"),n.includes(e.author)&&(o="");let i="";e.ip_address==null?i="(-)":(ipParts=e.ip_address.split("."),i=`(${ipParts[0]}.${ipParts[1]})`),s.includes(e.author)&&(i=""),t+=`<div class="list-author">${o} ${e.author} ${i}</div>`,t+=`<sup class="list-date">${e.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,e.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> 관리자의 승인을 기다리고 있습니다</div>`);let a=e.content;a=a.replace(/\n/g,"<br />"),t+=`<div class="content-text">${a} <span class="re-comment-button" onclick="re_comment('${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`,e.child.map(o=>{let c="child";e.ip_address&&o.ip_address===e.ip_address&&(c="parentself"),t+=`<div id="comment${o.cmt_idx}" class="${c}">`,t+=`<div class="content-info">`;let i="";o.cmt_cnt>5?i="🥉":o.cmt_cnt>25?i="🥈":o.cmt_cnt>125&&(i="🥇"),n.includes(o.author)&&(i="");let a="";o.ip_address==null?a="(-)":a=`(${o.ip_address})`,s.includes(o.author)&&(a=""),t+=`<div class="list-author">${i} ${o.author} ${a}</div>`,t+=`<sup class="list-date">${o.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,o.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> 관리자의 승인을 기다리고 있습니다</div>`);let r=o.content;r=r.replace(/\n/g,"<br />"),t+=`<div class="content-text">${r} <span class="re-comment-button" onclick="re_comment('${o.cmt_idx}', '${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`})}),listReply.innerHTML=t,renderKaTex(listReply),location.href.indexOf("#")!=-1&&(location.href=location.href.substr(location.href.indexOf("#")))}function render_blank(){listReply.innerHTML=""}function write_comment(){const e=document.querySelector(".ai-author"),t=document.querySelector(".ai-password"),n=document.querySelector(".ai-content"),s=e.value,o=t.value,i=n.value;if(s===""||o===""||i===""){alert("빈칸을 채워주세요");return}const a="3029",r="",c="머신러닝에서 강화학습이란";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeComment";return postFetch(t,{board_idx:a,board_slug:r,board_title:c,author:s.replace(/\+/g,"%2B"),password:o,content:i.replace(/\+/g,"%2B"),user_ip:e})}).then(e=>e.json()).then(s=>{s.ok?(commentRows.push(s.row),render_comment(commentRows),e.value="",t.value="",n.value=""):s.status===606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),e.value="",t.value="",n.value=""):s.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}async function update_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 수정",input:"password",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(s=>{if(s.ok){const s=document.querySelector(`#comment${e}`),o="https://freshrimpsushi.com/blog/ajax/comment.php?action=getComment";postFetch(o,{cmt_idx:e}).then(e=>e.json()).then(o=>{if(o.ok){const a=o.row,r=a.author,c=a.content;let i="";i+=`<div class="update-author-info">`,i+=`<input class="update-author" type="text" value="${r}" placeholder="이름" />`,i+=`<input class="update-password" type="password" value="${n}" placeholder="비밀번호" disabled />`,i+="</div>",i+=`<textarea class="update-content" value="" style="IME-MODE:active;">${c}</textarea>`,i+=`<input class="update_comment-button" type="submit" value="수정" onclick="update_comment_click(${e}, '${t}')" />`,i+=`<input class="update_comment-button" type="submit" value="취소" onclick="cancel_click(${e})" />`,s.innerHTML=i}}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}async function delete_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 삭제",text:"삭제하면 되돌릴 수 없습니다.",input:"password",icon:"warning",showCancelButton:!0,confirmButtonColor:"#3085d6",cancelButtonColor:"#d33",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(n=>{if(n.ok){const n="https://freshrimpsushi.com/blog/ajax/comment.php?action=deleteComment";postFetch(n,{cmt_idx:e}).then(e=>e.json()).then(n=>{Swal.fire("삭제되었습니다."),t==="parents"?commentRows=commentRows.filter(t=>t.cmt_idx!==e):t==="child"&&commentRows.map(t=>{t.child=t.child.filter(t=>t.cmt_idx!==e),t.child_cnt=t.child.length}),render_comment(commentRows)}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}function update_comment_click(e,t){const i=document.querySelector(`#comment${e} .update-author`),a=document.querySelector(`#comment${e} .update-password`),r=document.querySelector(`#comment${e} .update-content`),n=i.value,s=a.value,o=r.value;(n===""||s===""||o==="")&&alert("빈칸을 채워주세요");const c="https://freshrimpsushi.com/blog/ajax/comment.php?action=updateComment";postFetch(c,{cmt_idx:e,author:n.replace(/\+/g,"%2B"),password:s,content:o.replace(/\+/g,"%2B")}).then(e=>e.json()).then(n=>{n.ok&&(Swal.fire("수정되었습니다."),t==="parents"?commentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}):t==="child"&&commentRows.map(t=>{t.cmt_idx==n.parent_cmt_idx&&t.child.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)})}),render_comment(commentRows),recentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}),render_recent_comment(recentRows))}).catch(e=>console.error(e))}function cancel_click(){render_comment(commentRows)}function re_comment(e,t){const s=document.querySelector(`#comment${e} .re-comment`);let n="";n+='<div class="re-comment-box">',n+='<div class="re-comment-author-info">',n+='<input class="re-comment-author" type="text" value="" placeholder="이름" />',n+='<input class="re-comment-password" type="password" value="" placeholder="비밀번호" />',n+="</div>",n+='<textarea class="re-comment-content" placeholder="내용" value="" style="IME-MODE:active;"></textarea>',t===void 0?n+=`<button class="write-button" onclick="re_comment_click(${e})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`:n+=`<button class="write-button" onclick="re_comment_click(${e}, ${t})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`,n+=`<button class="write-button" onclick="cancel_click(${e})" value="close"><i class='fa-solid fa-solid fa-ban'></i></button>`,n+="</div>",s.innerHTML=n}function re_comment_click(e,t){const n=document.querySelector(`#comment${e} .re-comment-author`),s=document.querySelector(`#comment${e} .re-comment-password`),o=document.querySelector(`#comment${e} .re-comment-content`),i=n.value,a=s.value,r=o.value;if(i===""||a===""||r===""){alert("빈칸을 채워주세요");return}const c="3029",l="",d="머신러닝에서 강화학습이란";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(n=>{const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeReComment";return postFetch(s,{board_idx:c,board_slug:l,board_title:d,cmt_idx:t===void 0?e:t,author:i.replace(/\+/g,"%2B"),password:a,content:r.replace(/\+/g,"%2B"),user_ip:n})}).then(e=>e.json()).then(i=>{i.ok?(commentRows.map(n=>{t===void 0?n.cmt_idx==e&&n.child.push(i.row):n.cmt_idx==t&&n.child.push(i.row)}),render_comment(commentRows)):i.status==606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),n.value="",s.value="",o.value=""):i.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}</script></div><aside class=sidebar><aside><style>.reddot a:link{color:#67d10f;text-shadow:0 0 10px #d3d3d3;font-weight:700}.reddot a:visited{color:#f5f5f5}</style><p class=reddot style=text-align:center;margin:0><a href=https://freshrimpsushi.github.io/ko/posts/2707/>여름 특선 오마카세<br>「상상 속의 수」</a></p></aside><br><div class=category></div><div style=display:flex>● 를 클릭해서 관심있는 분야만 강조해보세요.<div class=resetmute><button class="sb-btn btnReset" style=border-radius:5px;border:0>reset</button>
<button class="sb-btn btnMute" style=border-radius:5px;border:0>mute</button></div></div><script defer>const color={green:"#33cc33",yellow:"#ffcc00",red:"#ff3333",black:"#000000"},categoryRows=[[{idx:1,name:"함수",color:color.green,show:"함수",size:"146"},{idx:2,name:"보조정리",color:color.green,show:"보조정리",size:"55"},{idx:3,name:"미분적분학",color:color.green,show:"미분적분학",size:"45"},{idx:4,name:"행렬대수",color:color.green,show:"행렬대수",size:"117"}],[{idx:1,name:"정수론",color:color.green,show:"정수론",size:"90"},{idx:2,name:"집합론",color:color.green,show:"집합론",size:"49"},{idx:3,name:"그래프이론",color:color.green,show:"그래프이론",size:"65"},{idx:4,name:"선형대수",color:color.green,show:"선형대수",size:"97"},{idx:5,name:"해석개론",color:color.green,show:"해석개론",size:"84"},{idx:6,name:"추상대수",color:color.green,show:"추상대수",size:"98"},{idx:7,name:"위상수학",color:color.green,show:"위상수학",size:"64"},{idx:8,name:"기하학",color:color.green,show:"기하학",size:"167"}],[{idx:1,name:"다변수벡터해석",color:color.green,show:"다변수벡터해석",size:"36"},{idx:2,name:"복소해석",color:color.green,show:"복소해석",size:"71"},{idx:3,name:"측도론",color:color.green,show:"측도론",size:"53"},{idx:4,name:"푸리에해석",color:color.green,show:"푸리에해석",size:"54"},{idx:5,name:"표현론",color:color.red,show:"표현론",size:"7"},{idx:6,name:"초함수론",color:color.green,show:"초함수론",size:"22"},{idx:7,name:"단층촬영",color:color.green,show:"단층촬영",size:"20"}],[{idx:1,name:"거리공간",color:color.green,show:"거리공간",size:"38"},{idx:2,name:"바나흐공간",color:color.green,show:"바나흐공간",size:"38"},{idx:3,name:"힐베르트공간",color:color.green,show:"힐베르트공간",size:"31"},{idx:4,name:"르벡공간",color:color.green,show:"르벡공간",size:"33"}],[{idx:1,name:"상미분방정식",color:color.green,show:"상미분방정식",size:"58"},{idx:2,name:"편미분방정식",color:color.green,show:"편미분방정식",size:"60"},{idx:3,name:"확률미분방정식",color:color.green,show:"확률미분방정식",size:"26"}],[{idx:1,name:"줄리아",color:color.green,show:"줄리아",size:"233"},{idx:2,name:"알고리즘",color:color.green,show:"알고리즘",size:"28"},{idx:3,name:"수치해석",color:color.green,show:"수치해석",size:"63"},{idx:4,name:"최적화이론",color:color.green,show:"최적화이론",size:"37"},{idx:5,name:"머신러닝",color:color.green,show:"머신러닝",size:"114"},{idx:6,name:"프로그래밍",color:color.yellow,show:"프로그래밍",size:"120"},{idx:7,name:"세이버메트릭스",color:color.green,show:"세이버메트릭스",size:"233",size:"13"}],[{idx:1,name:"물리학",color:color.green,show:"물리학",size:"30"},{idx:2,name:"수리물리",color:color.green,show:"수리물리",size:"78"},{idx:3,name:"고전역학",color:color.green,show:"고전역학",size:"48"},{idx:4,name:"전자기학",color:color.green,show:"전자기학",size:"51"},{idx:5,name:"양자역학",color:color.green,show:"양자역학",size:"57"},{idx:6,name:"열물리학",color:color.green,show:"열물리학",size:"29"}],[{idx:1,name:"R",color:color.green,show:"R",size:"54"},{idx:2,name:"데이터확보",color:color.green,show:"데이터확보",size:"29"},{idx:3,name:"데이터과학",color:color.green,show:"데이터과학",size:"41"},{idx:4,name:"통계적검정",color:color.green,show:"통계적검정",size:"33"},{idx:5,name:"통계적분석",color:color.green,show:"통계적분석",size:"76"},{idx:6,name:"수리통계학",color:color.green,show:"수리통계학",size:"123"},{idx:7,name:"확률분포론",color:color.green,show:"확률분포론",size:"84"},{idx:8,name:"확률론",color:color.green,show:"확률론",size:"80"},{idx:9,name:"위상데이터분석",color:color.green,show:"위상데이터분석",size:"40"}],[{idx:1,name:"논문작성",color:color.red,show:"논문작성",size:"63"},{idx:2,name:"생새우초밥지",color:color.black,show:"생새우초밥지",size:"7"}]];document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("blindList"));(e==""||e==null||e==null||e==0||e==NaN)&&localStorage.setItem("blindList",null),render_category(categoryRows),blind_category(e);const t=document.querySelector(".btnReset");t.addEventListener("click",()=>{let e=new Array;localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)});const n=document.querySelector(".btnMute");n.addEventListener("click",()=>{let e=new Array;for(let t=0;t<categoryRows.length;t++)categoryRows[t].map(n=>{const s={mainIdx:t,subIdx:n.idx};e.push(s)});localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)})});function render_category(e){const n=document.querySelector(".category");let t="";for(let n=0;n<e.length;n++)e[n].map(e=>{t+=`<span id="cate${n}-${e.idx}" class="cate etewsert">`,t+=`<span onclick="check_blind(${n}, ${e.idx});" style="cursor: pointer; color: ${e.color}">●</span>`,t+=`<a href="https://freshrimpsushi.github.io/ko/categories/${e.name.toLowerCase()}/">`,t+=` ${e.show} (${e.size})</a>`,t+="</span>",screen.width>954?t+="<br>":t+=" "}),t+="<hr>";n.innerHTML=t}function check_blind(e,t){const o=document.querySelector(`#cate${e}-${t}`);let n=new Array;const i={mainIdx:e,subIdx:t};let s=JSON.parse(localStorage.getItem("blindList"));if(s==""||s==null||s==null||s==0||s==NaN)n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind";else{n=s;let a=null;for(let s=0;s<n.length;s++)if(n[s].mainIdx==e&&n[s].subIdx==t){a=n.filter(n=>n.mainIdx!=e||n.subIdx!=t);break}a===null?(n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind"):(localStorage.setItem("blindList",JSON.stringify(a)),o.className="cate")}}function blind_category(e){const t=document.querySelectorAll(".cate");t.forEach(e=>{e.className="cate"}),e!==null&&e.map(e=>{const t=document.querySelector(`#cate${e.mainIdx}-${e.subIdx}`);t.className+=" blind"})}</script><br><aside><img src=https://freshrimpsushi.github.io/ko/banner/%EC%A4%84%EB%A6%AC%EC%95%84%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D.png alt=줄리아프로그래밍>
저희들의 저서 「줄리아 프로그래밍」이 2024 세종도서 학술부문에 선정되었습니다!<p style=margin:0><a href=https://product.kyobobook.co.kr/detail/S000213090376><img src=https://freshrimpsushi.github.io/ko/banner/%EA%B5%90%EB%B3%B4.png style=width:70px alt=교보문고></a>
<a href=https://www.yes24.com/Product/Goods/126164843><img src=https://freshrimpsushi.github.io/ko/banner/%EC%98%88%EC%8A%A424.png style=width:70px alt=예스24></a>
<a href="https://www.aladin.co.kr/shop/wproduct.aspx?ISBN=K532930200&start=pnaver_02"><img src=https://freshrimpsushi.github.io/ko/banner/%EC%95%8C%EB%9D%BC%EB%94%98.png style=width:70px alt=알라딘></a></p></aside><br><b>최근 본 포스트</b><div class=lately-viewed-list style=padding-left:4px></div><script defer>document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("latelyViewPostList")),n=document.querySelector(".lately-viewed-list");let t="";if(e==""||e==null||e==null||e==0||e==NaN)localStorage.setItem("latelyViewPostList",null),t+='<div class="lv-list">',t+=" · 열람한 포스트가 없습니다.",t+="</div>",n.innerHTML=t;else{for(let n=e.length-1;n>=0;n--)t+='<div class="lv-list">',t+=`<a href="${e[n].link}"> · ${e[n].title}</a>`,t+="</div>";n.innerHTML=t}})</script><script defer>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="머신러닝에서 강화학습이란",c="https://freshrimpsushi.github.io/ko/posts/3029/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script><br><b>최신 댓글</b><div class=current-reply></div><script defer>const url="https://freshrimpsushi.com/blog/ajax/recent_comment.php?action=getCurrentComment";let recentRows=[];fetch(url).then(e=>e.json()).then(e=>{e.ok&&(recentRows=e.rows,render_recent_comment(recentRows))}).catch(e=>console.error(e));function render_recent_comment(e){const n=document.querySelector(".current-reply");let t="";e.map(e=>{let n="https://freshrimpsushi.github.io/ko/";e.board_idx>-1?n+=`posts/${e.board_idx}#comment${e.cmt_idx}`:e.board_idx==-1?n+=`#comment${e.cmt_idx}`:n+=`categories/${e.board_title}#comment${e.cmt_idx}`,t+='<div class="current-reply-list">',t+=`<a href="${n}">`,t+=`<b> - ${e.author}</b>: `,t+=`${e.content}`,t+=`</a>`,t+=`</div>`}),n.innerHTML=t,renderKaTex(n)}</script><br></aside></div><footer><aside><div><p id=mirror-link style=text-align:center><a style=cursor:text href=http://localhost:1313//ko/posts/3029/>© 생새우초밥집 / Powered by 류대식, 전기현</a><br>Contact:
<img src=https://freshrimpsushi.github.io/ko/logo/gmail.png width=12px alt=mail> freshrimpsushi@gmail.com
<a href=https://freshrimpsushi.github.io/ko/index.xml><img src=https://freshrimpsushi.github.io/ko/logo/RSS.png width=12px alt=RSS> RSS</a></p></div><script type=text/javascript>var goIndex=function(){var e=document.getElementsByName("idx")[0].value,t="https://freshrimpsushi.github.io/ko/posts/"+e;location.replace(t)};document.addEventListener("keydown",function(e){const n=document.getElementById("navigator");if(e.altKey&&e.ctrlKey&&e.key==="l"){e.preventDefault();var t=document.querySelector("#mirror-link a");t?t.click():console.log("거울 링크를 찾지 못했습니다.")}})</script></aside></footer></body><script async src="https://www.googletagmanager.com/gtag/js?id=G-NLV8Y9PRK1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NLV8Y9PRK1")</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4751085325232621" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/sweetalert2@11></script><script src=https://freshrimpsushi.github.io/ko/js/fontawsome.min.js></script><script src=https://freshrimpsushi.github.io/ko/js/common.js></script><script>document.addEventListener("DOMContentLoaded",()=>{fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/ip_checker.php";return postFetch(t,{user_ip:e})}).then(e=>e.json()).then(e=>{e.ok||(alert(`차단된 IP입니다.
Contact:
freshrimpsushi@gmail.com`),window.location.href="https://google.com")}).catch(e=>console.error(e))})</script></html>