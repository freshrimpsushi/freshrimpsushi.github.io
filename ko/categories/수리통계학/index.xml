<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>수리통계학 on 생새우초밥집</title><link>https://freshrimpsushi.github.io/ko/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/</link><description>Recent content in 수리통계학 on 생새우초밥집</description><generator>Hugo -- gohugo.io</generator><language>ko</language><lastBuildDate>Thu, 07 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/ko/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/index.xml" rel="self" type="application/rss+xml"/><item><title>유한 모집단 보정 계수 유도</title><link>https://freshrimpsushi.github.io/ko/posts/2691/</link><pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2691/</guid><description>공식 모분산이 $\sigma^{2}$ 인 랜덤샘플 $X_{1} , \cdots , X_{N}$ 이 주어져 있다고 할 때, 전체 샘플에 대한 표본평균 $\overline{X}_{N}$ 의 분산은 $\sigma^{2} / N$ 이다. 그 중 $n \le N$ 개만큼 비복원추출한 표본에 표본평균</description></item><item><title>확률 벡터</title><link>https://freshrimpsushi.github.io/ko/posts/3665/</link><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3665/</guid><description>정의 다음을 만족하는 벡터 $\mathbf{p} = \begin{bmatrix}p_{1} &amp;amp; \cdots &amp;amp; p_{n} \end{bmatrix}^{\mathsf{T}}$을 확률 벡터probability/stochast</description></item><item><title>코크란 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2655/</link><pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2655/</guid><description>정리 샘플 $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ 이 $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$ 와 같이 iid로 정규분포를 따른다고 하자. 랭크가 $r_{j}$ 인 대칭행렬 $A_{1} , \cdots , A_{k} \in \mathbb{R}^{n \times n}$ 에 대해 확률변수 $Q_{1}</description></item><item><title>호그-크레이그 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2651/</link><pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2651/</guid><description>정리 샘플 $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ 이 $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$ 와 같이 iid로 정규분포를 따른다고 하자. 대칭행렬 $A_{1} , \cdots , A_{k} \in \mathbb{R}^{n \times n}$ 에 대해 확률변수 $Q_{1} , \cdots , Q_{k}$ 가</description></item><item><title>크레이그 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2647/</link><pubDate>Sun, 11 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2647/</guid><description>정리 샘플 $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ 이 $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$ 와 같이 iid로 정규분포를 따른다고 하자. 대칭행렬 $A, B \in \mathbb{R}^{n \times n}$ 에 대해 확률변수 $Q_{1}$ 과 $Q_{2}$ 가 랜덤벡터 이</description></item><item><title>혼합 분포</title><link>https://freshrimpsushi.github.io/ko/posts/3639/</link><pubDate>Tue, 06 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3639/</guid><description>도입1 아래 그림과 같은 확률밀도함수를 갖는 확률분포를 근사하고 싶다고 하자. 확률분포를 근사하는 기본적인 방법 중 하나는 근사하고자 하는 분포와 가장 닮은 정규분</description></item><item><title>정규분포 랜덤벡터 이차형식의 카이제곱성의 동치조건</title><link>https://freshrimpsushi.github.io/ko/posts/2639/</link><pubDate>Fri, 25 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2639/</guid><description>정리 샘플 $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ 이 $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$ 와 같이 iid로 정규분포를 따른다고 하자. 랭크가 $r \le n$ 인 대칭행렬 $A \in \mathbb{R}^{n \times n}$ 에 대해 랜덤벡터 이차형</description></item><item><title>정규분포 랜덤벡터 이차형식의 적률생성함수</title><link>https://freshrimpsushi.github.io/ko/posts/2637/</link><pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2637/</guid><description>정리 샘플 $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ 이 $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$ 와 같이 iid로 정규분포를 따른다고 하자. 랭크가 $r \le n$ 인 대칭행렬 $A \in \mathbb{R}^{n \times n}$ 에 대해 랜덤벡터 이차형</description></item><item><title>랜덤벡터 이차형식으로 나타낸 편차제곱합</title><link>https://freshrimpsushi.github.io/ko/posts/2635/</link><pubDate>Thu, 17 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2635/</guid><description>공식 랜덤벡터 $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ 와 항등행렬 $I_{n} \in \mathbb{R}^{n \times n}$ 과 모든 성분이 $1$ 인 일행렬 $J_{n} \in \mathbb{R}^{n \times n}$ 에 대해 다음이 성립한다. $$ \mathbf{X}^{T} \left( I_{n} - {\frac{ 1 }{ n }} J_{n} \right) \mathbf{X} = ( n - 1</description></item><item><title>랜덤벡터 이차형식의 기대값</title><link>https://freshrimpsushi.github.io/ko/posts/2633/</link><pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2633/</guid><description>공식 모평균 벡터 $\mu \in \mathbb{R}^{n}$ 와 공분산행렬 $\Sigma \in \mathbb{R}^{n \times n}$ 에 대해 랜덤벡터 $\mathbf{X}$ 가 $\mathbf{X} \sim \left( \mu , \Sigma \right)$ 라 하자. 대칭행렬 $A$ 에 대해 랜덤벡터 이차형식 $Q = \mathbf{X}^{T} A \mathbf{X}$ 의 기대값은 다음과</description></item><item><title>랜덤벡터 이차형식</title><link>https://freshrimpsushi.github.io/ko/posts/2631/</link><pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2631/</guid><description>정의 1 랜덤벡터 $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ 와 대칭행렬 $A \in \mathbb{R}^{n \times n}$ 에 대해 $Q = \mathbf{X}^{T} A \mathbf{X}$ 를 이차형식quadratic form in $\mathbf{X}$이라 한다. 설명 이</description></item><item><title>수리통계학에서의 주성분 분석 PCA</title><link>https://freshrimpsushi.github.io/ko/posts/2606/</link><pubDate>Fri, 02 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2606/</guid><description>개요 주성분 분석은 회귀분석에서 다중공선성을 회피하고 데이터를 요약하는 등 통계학에서 많은 쓰임새가 있고, 머신러닝에서도 차원축소라는 중요한 의미를 가진다.</description></item><item><title>랜덤 벡터의 기대값</title><link>https://freshrimpsushi.github.io/ko/posts/2555/</link><pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2555/</guid><description>정의 1 $$ E \left( X \right) := \begin{bmatrix} E \left( X_{1} \right) \\ \vdots \\ E \left( X_{n} \right) \end{bmatrix} $$ 랜덤벡터 $X = \left( X_{1} , \cdots , X_{n} \right)$ 의 기대값expectation은 위와 같이 각 성분의 기대값의 벡터로 정의된</description></item><item><title>조건부 기대값은 편차제곱합을 최소화한다</title><link>https://freshrimpsushi.github.io/ko/posts/3514/</link><pubDate>Wed, 29 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3514/</guid><description>정리 다음이 성립한다. $$ \begin{equation} E\left[ Y | X \right] = \argmin_{f(X)} E\left[ (Y - f(X))^{2} | X \right] \end{equation} $$ $$ \begin{equation} E\left[ Y | X \right] = \argmin_{f(X)} E\left[ (Y - f(X))^{2} \right] \end{equation} $$ 증명 (1) $$ \begin{align*} &amp;amp; \argmin_{f(X)} E\left[ (Y - f(X))^{2} | X \right] \\ &amp;amp;= \argmin_{f(X)} E\left[ Y^{2} - 2Yf(X) + f(X)^{2} | X \right]</description></item><item><title>합동분산의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2472/</link><pubDate>Sat, 04 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2472/</guid><description>빌드업 분포가 $X \sim \left( \mu , \sigma^{2} \right)$ 인 모집단에서 상호독립으로 뽑은 $n$개의 샘플들이 실제로는 $m$가지의 모집단 $\left( \mu_{1} , \sigma_{1}^{2} \right), \cdots , \left( \mu_{m} , \sigma_{m}^{2} \right)$ 에서 $n_{1} , \cdots , n_{</description></item><item><title>가중평균의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2470/</link><pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2470/</guid><description>정의 데이터 $\mathbf{x} = \left\{ x_{1} , \cdots , x_{n} \right\}$ 와 벡터 $\mathbf{w} = \left( w_{1} , \cdots , w_{n} \right) \in \mathbb{R}^{n}$ 에 대해 다음을 가중평균weighted mean이라 한다. $$ {{ \sum_{k=1}^{n} w_{k} x_{k} } \over { \sum_{k=1}^{n} w_{k} }} = {{ w_{1} x_{1}</description></item><item><title>표준오차의 일반적인 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2462/</link><pubDate>Sun, 15 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2462/</guid><description>정의 1 어떤 추정량estimator $T$ 에 대해, $T$ 의 표준편차의 추정치estimate를 표준오차standard error라 한다. $$ \text{s.e.} \left( T \right) := \sqrt{ \widehat{</description></item><item><title>유니모달 분포의 최단 신뢰구간</title><link>https://freshrimpsushi.github.io/ko/posts/2337/</link><pubDate>Tue, 07 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2337/</guid><description>정리 유니모달 함수의 정의 함수 $f : \mathbb{R} \to \mathbb{R}$ 이 $x \le x^{\ast}$ 에서 감소하지 않고, $x \ge x^{\ast}$ 에서 증가하지 않게끔 하는 모드mode $x^{\ast}$ 가 존재하면 $f$ 를 유니모달unimoda</description></item><item><title>확률적 증감함수와 신뢰구간</title><link>https://freshrimpsushi.github.io/ko/posts/2335/</link><pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2335/</guid><description>정리 1 확률적 증감함수의 정의 누적분포함수 $F \left( t ; \theta \right)$ 가 $\theta$ 에 대해 증가(감소) 함수면 확률적 증가(감소) 함수stochastic Increasing</description></item><item><title>최정확 신뢰집합</title><link>https://freshrimpsushi.github.io/ko/posts/2333/</link><pubDate>Mon, 30 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2333/</guid><description>정의 1 $\theta$ 에 대한 가설검정의 $1 - \alpha$ 신뢰집합을 $C \left( \mathbf{x} \right)$ 이라 하고, 채택역을 $A \left( \theta \right) = C \left( \mathbf{x} \right)^{c}$ 이라 하자. $P_{\theta} \left( \theta ' \in C \left( \mathbf{X} \right) \right)$ 를 $\theta ' \ne \theta$ 에 대한 펄스 커버리</description></item><item><title>가설검정과 신뢰집합의 일대일 대응관계</title><link>https://freshrimpsushi.github.io/ko/posts/2332/</link><pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2332/</guid><description>정리 모수공간 $\Theta$ 와 공간 $\mathcal{X}$ 가 주어져 있다고 하자. 각각의 $\theta_{0} \in \Theta$ 에 대해 $A \left( \theta_{0} \right)$ 을 가설검정 $H_{0} : \theta = \theta_{0}$ 의 레벨 $\alpha$ 채택역이라 하자. 각각의 $\mathbf{x} \in \mathcal{X}$ 에 대해 다음과 같이</description></item><item><title>수리통계학에서 피벗의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2331/</link><pubDate>Thu, 26 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2331/</guid><description>정의 1 확률변수 $Q \left( \mathbf{X} ; \theta \right) := Q \left( X_{1} , \cdots , X_{n} ; \theta \right)$ 의 확률분포가 모든 모수 $\theta$ 에 독립이면 $Q$ 를 피벗pivot 혹은 피버탈 퀀터티pivotal quanti</description></item><item><title>수리통계적인 신뢰집합의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2329/</link><pubDate>Sun, 22 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2329/</guid><description>정의 1 모수 $\theta$ 의 구간추정량 $\left[ L \left( \mathbf{X} \right), U \left( \mathbf{X} \right) \right]$ 에 대해 다음을 커버리지 확률coverage Probability라 한다. $$ P_{\theta} \left( \theta \in \left[ L \left( \mathbf{X} \right), U \left( \mathbf{X}</description></item><item><title>구간추정량</title><link>https://freshrimpsushi.github.io/ko/posts/2327/</link><pubDate>Wed, 18 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2327/</guid><description>정의 1 모수 $\theta \in \mathbb{R}$ 에 대해 순서쌍 $\left( L \left( x_{1} , \cdots , x_{n} \right), U \left( x_{1} , \cdots , x_{n} \right) \right)$ 이 모든 $\mathbf{x} \in \mathcal{X}$ 에 대해 $L \left( \mathbf{x} \right) \le U \left( \mathbf{x} \right)$ 을 만족하면 $\theta$ 의 구간추정치interval</description></item><item><title>수리통계적인 유의확률의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2304/</link><pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2304/</guid><description>정의 1 가설검정 $H_{0} \text{ vs } H_{1}$ 이 주어져 있다고 하자. 모든 실현 $\mathbf{x} \in \Omega$ 에 대해 $0 \le p \left( \mathbf{x} \right) \le 1$ 를 만족시키는 검정 통계량 $p \left( \mathbf{X} \right)$ 를 유의확률 혹은 p-밸류p-va</description></item><item><title>충분통계량이 포함된 최강력검정</title><link>https://freshrimpsushi.github.io/ko/posts/2301/</link><pubDate>Sun, 27 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2301/</guid><description>정리 가설검정: $$ \begin{align*} H_{0} :&amp;amp; \theta = \theta_{0} \\ H_{1} :&amp;amp; \theta = \theta_{1} \end{align*} $$ 위와 같은 가설검정에서 $\theta$ 에 대한 충분통계량 $T$ 의 $\theta_{0}, \theta_{1}$ 에 대한 확률밀도함수 혹은 확률질량함수를 $g \left( t | \theta_{0} \right), g</description></item><item><title>칼린-루빈 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2299/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2299/</guid><description>정리 가설검정: $$ \begin{align*} H_{0} :&amp;amp; \theta \le \theta_{0} \\ H_{1} :&amp;amp; \theta &amp;gt; \theta_{0} \end{align*} $$ 위와 같은 가설검정에서 $T$ 를 $\theta$ 의 충분통계량이라 하고, $t$ 의 확률밀도함수 혹은 확률질량함수의 패밀리 $\left\{ g(t | \theta)</description></item><item><title>단조우도비의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2297/</link><pubDate>Sat, 19 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2297/</guid><description>정의 모수 $\theta \in \mathbb{R}$ 와 일변량 확률변수 $T$ 에 대한 확률질량함수 혹은 확률밀도함수의 패밀리를 $G := \left\{ g ( t | \theta) : \theta \in \Theta \right\}$ 라 하자. 모든 $\theta_{2} &amp;gt; \theta_{1}$ 에 대해 $$ {{ g \left( t | \theta_{2}</description></item><item><title>네이만-피어슨 보조정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2295/</link><pubDate>Tue, 15 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2295/</guid><description>정리 가설검정: $$ \begin{align*} H_{0} :&amp;amp; \theta = \theta_{0} \\ H_{1} :&amp;amp; \theta = \theta_{1} \end{align*} $$ 위와 같은 가설검정에서 $\theta_{0}, \theta_{1}$ 에 대한 확률밀도함수 혹은 확률질량함수를 $f \left( \mathbf{x} | \theta_{0} \right), f \left( \mathbf{x} | \theta_{1} \right)$ 이라 하고 기각</description></item><item><title>일변량 확률 변수 샘플링하는 법</title><link>https://freshrimpsushi.github.io/ko/posts/2294/</link><pubDate>Sun, 13 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2294/</guid><description>개요 확률변수의 구체적인 실현을 구하는 방법이다. 정리 일변량 확률변수 $T$ 의 누적분포함수 $F = F_{T}$ 가 증가 함수라 하자. 그러면 일양분포를 따르는 확률 변수 $U \sim U \left(</description></item><item><title>불편 검정력 함수와 최강력검정</title><link>https://freshrimpsushi.github.io/ko/posts/2293/</link><pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2293/</guid><description>정의 1 가설검정: $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ 검정력 함수 $\beta (\theta)$가 모든 $\theta_{0} \in \Theta_{0}$ 와 $\theta_{1} \in \Theta_{0}^{c}$ 에 대해 다음을 만족하면 불편unbiased 검정</description></item><item><title>가설 검정의 검정력 함수</title><link>https://freshrimpsushi.github.io/ko/posts/2291/</link><pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2291/</guid><description>정의 1 가설검정: $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ 위와 같은 가설검정이 주어져 있고 $\alpha \in [0,1]$ 이라 하자. 모수 $\theta$ 에 대해 기각역이 $R$ 인 함수 $\beta (\theta) := P_{\theta} \left( \mathbf{X} \in \mathbb{R} \right)$ 을 검</description></item><item><title>충분통계량이 포함된 우도비검정</title><link>https://freshrimpsushi.github.io/ko/posts/2289/</link><pubDate>Thu, 03 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2289/</guid><description>정리 가설검정: $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ 우도비검정통계량: $$ \lambda \left( \mathbf{x} \right) := {{ \sup_{\Theta_{0}} L \left( \theta \mid \mathbf{x} \right) } \over { \sup_{\Theta} L \left( \theta \mid \mathbf{x} \right) }} $$ 만약 $T \left( \mathbf{X} \right)$ 가 모수 $\theta$ 의 충분</description></item><item><title>수리통계적인 우도비검정의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2287/</link><pubDate>Sun, 30 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2287/</guid><description>정의 1 $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ 위와 같은 가설검정에 대해 다음의 통계량 $\lambda$ 를 우도비검정 통계량likelihood Ratio test statistic이라 한다</description></item><item><title>로케이션 패밀리의 충분통계량과 최대우도추정량</title><link>https://freshrimpsushi.github.io/ko/posts/2285/</link><pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2285/</guid><description>정리 확률밀도함수가 $f_{X} \left( x ; \theta \right) = f_{X} \left( x - \theta \right)$ 인 로케이션 패밀리에서 얻은 랜덤샘플 $X_{1} , \cdots , X_{n} \sim X$ 이 주어져 있다고 하자. 충분통계량과 최대우도추정량은 $X$</description></item><item><title>수리통계적인 가설검정의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2283/</link><pubDate>Sat, 22 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2283/</guid><description>정의 1 모수에 관한 명제를 가설hypothesis이라 한다. 주어진 샘플에 따라 가설 $H_{0}$ 을 참으로 받아들이게 만들거나 가설 $H_{0}$ 을 기각하고 $H_{1}$ 을 채택하는 문제를 가</description></item><item><title>랜덤샘플의 표본평균의 평균과 분산</title><link>https://freshrimpsushi.github.io/ko/posts/2281/</link><pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2281/</guid><description>공식 랜덤샘플 $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} X$ 이 주어져 있다고 하면 그 표본평균 $\bar{X}$ 의 평균과 분산은 다음과 같다. $$ \begin{align*} E \bar{X} =&amp;amp; E X \\ \Var \bar{X} =&amp;amp; {{ 1 } \over { n }} \Var X \end{align*} $$ 설명 너무 쉬워서</description></item><item><title>유일한 최대우도추정량은 충분통계량에 종속된다</title><link>https://freshrimpsushi.github.io/ko/posts/2279/</link><pubDate>Fri, 14 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2279/</guid><description>정리 만약 모수 $\theta$ 에 대한 충분통계량 $T$ 가 존재하고 $\theta$ 의 최대우도추정량 $\hat{\theta}$ 가 유일하게 존재한다면, $\hat{\theta}$ 는 $T$ 에 대한 함수로 나타난다. 증명 1 확률밀도함수 $f \left( x ; \theta</description></item><item><title>레만-셰페 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2277/</link><pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2277/</guid><description>정리 1 2 완비 충분 통계량에 종속된 불편추정량은 유일하다. 다시 말해, $\theta$ 의 완비충분통계량 $T$ 에 대해 만약 $E \left[ \phi (T) \right] = \tau (\theta)$ 면 $\phi (T)$ 는 $\tau (\theta)$ 의 유일한 불편추정량,</description></item><item><title>최소분산불편추정량의 유일성</title><link>https://freshrimpsushi.github.io/ko/posts/2275/</link><pubDate>Thu, 06 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2275/</guid><description>정리 1 만약 $W$ 가 $\tau (\theta)$ 의 최선불편추정량이라면, $W$ 는 유일하다. 증명 코시-슈바르츠 부등식: 확률변수 $X, Y$ 에 대해 다음이 성립한다. $$ \operatorname{Cov} (X,Y) \le \Var X \Var Y $$ 등호가</description></item><item><title>최선불편추정량, 최소분산불편추정량 UMVUE</title><link>https://freshrimpsushi.github.io/ko/posts/2273/</link><pubDate>Sun, 02 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2273/</guid><description>정의 1 모수 $\theta$ 가 주어져 있다고 하자. 불편추정량 $W^{\ast}$ 가 다른 모든 불편추정량 $W$ 에 대해 다음을 만족하면 최선불편추정량best unbiased estimator 혹은 최소분산불편추정량UMV</description></item><item><title>불편추정량의 라오-크래머 하한</title><link>https://freshrimpsushi.github.io/ko/posts/2271/</link><pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2271/</guid><description>정리 정칙조건: (R0): 확률밀도함수 $f$ 는 $\theta$ 에 대해 단사다. 수식으로는 다음을 만족시킨다. $$ \theta \ne \theta ' \implies f \left( x_{k} ; \theta \right) \ne f \left( x_{k} ; \theta ' \right) $$ (R1): 확률밀도함수 $f$ 는 모든</description></item><item><title>최대우도추정량의 불변성질 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2269/</link><pubDate>Sat, 24 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2269/</guid><description>정리 최대우도추정량은 함수를 취하는 것에 대해 불변invariant 하다. 다시 말해 만약 $\hat{\theta}$ 가 모수 $\theta$ 의 최대우도추정량면, 모든 함수 $\tau$ 에 대해 $\tau \left( \hat{\theta} \right)$ 역시 $\tau</description></item><item><title>새터스화이트 근사</title><link>https://freshrimpsushi.github.io/ko/posts/2267/</link><pubDate>Tue, 20 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2267/</guid><description>빌드업 자유도가 $r_{k}$ 인 카이제곱분포를 따르는 독립적인 $n$ 개의 확률변수 $Y_{k} \sim \chi_{r_{k}}^{2}$ 가 주어져 있다고 하자. 널리 알려진대로, 이들의 합인 $\sum_{k=1}^{n} Y_{k}$ 는 자유도가 $\sum_{k=1}^{n} r_{k}$ 인 카이제곱</description></item><item><title>로케이션-스케일 패밀리의 보조통계량</title><link>https://freshrimpsushi.github.io/ko/posts/2265/</link><pubDate>Fri, 16 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2265/</guid><description>정리 1 $X_{1} , \cdots , X_{n}$ 가 로케이션 패밀리면서 스케일 패밀리에서 나온 랜덤샘플이라 하자. 두 통계량 $T_{1} \left( X_{1} , \cdots, X_{n} \right)$ 과 $T_{2} \left( X_{1} , \cdots , X_{n} \right)$ 가 모든 $x_{1} , \cdots , x_{n}$ 와 모든 상수</description></item><item><title>최소충분통계량이 주어진 불편추정량의 분산은 최소가 된다</title><link>https://freshrimpsushi.github.io/ko/posts/2263/</link><pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2263/</guid><description>정리 1 모수 $\theta$ 가 주어져 있다고 하자. $U$ 는 그 불편추정량, $T_{1}$ 은 충분통계량이고 $T_{2}$ 은 최소충분통계량이고 다음과 같이 $$ \begin{align*} U_{1} :=&amp;amp; E \left( U | T_{1} \right) \\ U_{2} :=&amp;amp; E \left( U | T_{2} \right) \end{align*}</description></item><item><title>지수족 확률분포의 완비통계량</title><link>https://freshrimpsushi.github.io/ko/posts/2261/</link><pubDate>Thu, 08 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2261/</guid><description>정리 1 모수 $\mathbf{\theta} = \left( \theta_{1} , \cdots , \theta_{k} \right)$ 가 주어져 있고, 랜덤샘플 $X_{1} , \cdots , X_{n}$ 의 확률밀도함수 혹은 확률질량함수가 다음과 같이 지수족확률분포를 따른다고 하자. $$ f(x; \mathbf{\theta}) =</description></item><item><title>모먼트 메소드</title><link>https://freshrimpsushi.github.io/ko/posts/2259/</link><pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2259/</guid><description>정의 1 주어진 분포의 모수를 모를 때, 적률로써 모수에 대한 연립방정식을 세우고 그 해를 모수의 추정량으로 보는 방법을 모먼트 메소드moment method라 한</description></item><item><title>바수 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2257/</link><pubDate>Wed, 31 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2257/</guid><description>정리 만약 $T \left( \mathbf{X} \right)$ 이 완비통계량이면서 최소충분통계량이면, $T \left( \mathbf{X} \right)$ 은 모든 보조통계량과 독립이다. 설명 바수 정리는 충분통계량에 관한 정리 중에 가장 중요한 정</description></item><item><title>완비통계량</title><link>https://freshrimpsushi.github.io/ko/posts/2255/</link><pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2255/</guid><description>정의 1 $\Omega$ 를 모수의 집합이라고 하자. 샘플 $\mathbf{X}$ 의 통계량 $T := T \left( \mathbf{X} \right)$ 의 확률밀도함수 혹은 확률질량함수 $f \left( t ; \theta \right)$ 들을 모아놓은 패밀리 $\left\{ f \left( t ; \theta \right) : \theta \in \Theta</description></item><item><title>스케일 패밀리</title><link>https://freshrimpsushi.github.io/ko/posts/2253/</link><pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2253/</guid><description>정의 누적분포함수 $F$ 에 대해 $F_{\sigma}$ 는 모든 $x$ 에 대해 $F_{\sigma} (x) = F \left( x / \sigma \right)$ 를 만족한다고 하자. $\left\{ F_{\sigma} : \sigma &amp;gt; 0 \right\}$ 을 스케일 패밀리scale Family라 한다. 예시 1</description></item><item><title>로케이션 패밀리</title><link>https://freshrimpsushi.github.io/ko/posts/2251/</link><pubDate>Fri, 19 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2251/</guid><description>정의 누적분포함수 $F$ 에 대해 $F_{\theta}$ 는 모든 $x$ 에 대해 $F_{\theta} (x) = F \left( x - \theta \right)$ 를 만족한다고 하자. $\left\{ F_{\theta} : \theta \in \mathbb{R} \right\}$ 을 로케이션 패밀리location Family라 한다</description></item><item><title>보조통계량</title><link>https://freshrimpsushi.github.io/ko/posts/2249/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2249/</guid><description>정의 1 $S$ 가 샘플 $\mathbf{X}$ 의 통계량이라고 하자. $S \left( \mathbf{X} \right)$ 의 분포가 모수 $\theta$ 에 종속되지 않으면 보조통계량ancillary statistic이라 한다. 설명 사실 말</description></item><item><title>최소충분통계량</title><link>https://freshrimpsushi.github.io/ko/posts/2247/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2247/</guid><description>정의 1 $T \left( \mathbf{X} \right)$ 가 충분통계량이라고 하자. 모든 다른 충분통계량 $T ' \left( \mathbf{X} \right)$ 에 대해 $T \left( \mathbf{x} \right)$ 가 $T ' \left( \mathbf{x} \right)$ 의 함수로 나타나면 $T \left( \mathbf{X} \right)$ 를 최소충분통계량mini</description></item><item><title>우도함수의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/2239/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2239/</guid><description>정의 1 샘플 $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right)$ 의 조인트 확률밀도함수 혹은 확률질량함수를 $f(\mathbf{x}|\theta)$ 라 하자. 그 실현 $\mathbf{x}$ 가 주어져 있을 때, $f(\mathbf{x}|\theta)$ 를 $\theta$ 에 대한 함수로 본 $$ L \left( \theta | \mathbf{x} \right) := f \left(</description></item><item><title>수리통계학에서의 델타 메소드</title><link>https://freshrimpsushi.github.io/ko/posts/2237/</link><pubDate>Fri, 22 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2237/</guid><description>정리 상수 $\theta \in \mathbb{R}$ 와 확률변수의 시퀀스 $\left\{ Y_{n} \right\}_{n \in \mathbb{N}}$ 에 대해 $\sqrt{n} \left( Y_{n} - \theta \right)$ 가 정규분포 $N \left(0, \sigma^{2} \right)$ 로 분포수렴한다고 하자. $1$계 델타 메소드 1 $g ' (\theta) \ne 0$ 이 존재한다</description></item><item><title>스털링 공식의 수리통계적 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2235/</link><pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2235/</guid><description>정리 $$ \lim_{n \to \infty} {{n!} \over {e^{n \ln n - n} \sqrt{ 2 \pi n} }} = 1 $$ 설명 스털링 근사 혹은 스털링 공식stirling formula은 통계학이나 물리학 등 여러 곳에서 유용하게 쓰</description></item><item><title>지수족 확률분포</title><link>https://freshrimpsushi.github.io/ko/posts/2213/</link><pubDate>Sat, 04 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2213/</guid><description>정의 1 2 모수 $\theta$ 인 확률분포의 확률질량함수 혹은 확률밀도함수가 어떤 함수 $p,K,H,q,h,c,w_{i},t_{i}$ 들에 대해 다음과 같이 나타낼 수 있으면 지수족exponential Family 혹은 익스포넨</description></item><item><title>확률 밀도 함수의 컨볼루션 공식</title><link>https://freshrimpsushi.github.io/ko/posts/2211/</link><pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2211/</guid><description>공식 1 독립인 두 연속확률변수 $X, Y$ 의 확률밀도함수가 $f_{X}, f_{Y}$ 로 주어져 있다고 하자. 그러면 $Z := X + Y$ 의 확률밀도함수는 두 확률밀도함수의 합성곱 $f_{Z} = f_{X} \ast f_{Y}$ 이다.</description></item><item><title>함수를 취한 확률변수꼴 합의 기대값</title><link>https://freshrimpsushi.github.io/ko/posts/2209/</link><pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2209/</guid><description>정리 1 $X_{1} , \cdots , X_{n}$ 이 랜덤 샘플이고, $E g \left( X_{1} \right)$ 과 $\Var g \left( X_{1} \right)$ 가 존재하게끔 하는 함수 $g : \mathbb{R} \to \mathbb{R}$ 가 주어져 있다고 하자. 그러면 다음이 성립한다. [1] 평균: $$ E \left(</description></item><item><title>라오-블랙웰 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2107/</link><pubDate>Thu, 04 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2107/</guid><description>정리 1 2 모수 $\theta$ 가 주어져 있다고 하자. $T$ 가 $\theta$ 의 충분통계량이고 $W$ 가 $\tau \left( \theta \right)$ 의 불편추정량이라고 할 때 $\phi \left( T \right) := E \left( W | T \right)$ 를 정의하면 모든 $\theta$ 에 대해 다음이</description></item><item><title>네이만 인수분해 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2084/</link><pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2084/</guid><description>정리 랜덤 샘플 $X_{1} , \cdots , X_{n}$ 이 모수 $\theta \in \Theta$ 에 대해 같은 확률질량/밀도함수 $f \left( x ; \theta \right)$ 를 가진다고 하자. 통계량 $Y = u_{1} \left( X_{1} , \cdots , X_{n} \right)$ 이 $\theta$ 의 충분통계량인 것은 다</description></item><item><title>충분통계량</title><link>https://freshrimpsushi.github.io/ko/posts/2061/</link><pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2061/</guid><description>정의 수식적인 정의 1 모수 $\theta \in \Theta$ 에 대해 랜덤 샘플 $X_{1} , \cdots , X_{n}$ 의 확률질량/밀도함수를 $f(x;\theta)$, 통계량 $Y_{1} := u_{1} \left( X_{1} , \cdots , X_{n} \right)$ 의 확률질량/밀도함수를 $f_{Y_{1}} \left( y_{1}; \theta \right)$ 이라 하</description></item><item><title>효율적추정량</title><link>https://freshrimpsushi.github.io/ko/posts/2059/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2059/</guid><description>정의 1 $Y$ 가 모수 $\theta$ 에 대한 불편추정량이라고 하자. 라오-크래머 하한 $\text{RC}$ 에 대해 다음을 추정량 $Y$ 의 효율성efficiency이라고 한다. $$ {{ \text{RC} } \over { \Var (Y) }}</description></item><item><title>라오-크래머 하한</title><link>https://freshrimpsushi.github.io/ko/posts/2057/</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2057/</guid><description>정리 1 정칙조건: (R0): 확률밀도함수 $f$ 는 $\theta$ 에 대해 단사다. 수식으로는 다음을 만족시킨다. $$ \theta \ne \theta ' \implies f \left( x_{k} ; \theta \right) \ne f \left( x_{k} ; \theta ' \right) $$ (R1): 확률밀도함수 $f$ 는 모</description></item><item><title>바틀렛 항등식</title><link>https://freshrimpsushi.github.io/ko/posts/2055/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2055/</guid><description>정리 정칙조건: (R0): 확률밀도함수 $f$ 는 $\theta$ 에 대해 단사다. 수식으로는 다음을 만족시킨다. $$ \theta \ne \theta ' \implies f \left( x_{k} ; \theta \right) \ne f \left( x_{k} ; \theta ' \right) $$ (R1): 확률밀도함수 $f$ 는 모든</description></item><item><title>피셔 정보</title><link>https://freshrimpsushi.github.io/ko/posts/2034/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2034/</guid><description>빌드업 스코어 함수 모수 $\theta \in \Theta$ 에 대해 확률밀도함수가 $f \left( x ; \theta \right)$ 인 확률변수 $X$ 를 생각해보자. 로그우도함수가 가장 커지는 추정량인 최대우도추정량은 다음과 같</description></item><item><title>수리통계학에서의 정칙성 조건</title><link>https://freshrimpsushi.github.io/ko/posts/2029/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2029/</guid><description>개요 수학을 사용하는 과목에서 대개 정칙성regularity conditions이란 대개 응용될 구석이 많으면서 이론적인 전개가 편해지는 조건들을 말하며</description></item><item><title>최대우도추정량</title><link>https://freshrimpsushi.github.io/ko/posts/2026/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2026/</guid><description>빌드업 모수 $\theta \in \Theta$ 에 대해 확률밀도함수가 $f \left( x ; \theta \right)$ 인 확률변수 $X$ 를 생각해보자. $X$ 와 같은 분포로 iid하게 뽑은 랜덤샘플 $X_{1} , \cdots , X_{n}$ 는 같은 확률밀도함수 $f(x</description></item><item><title>일치추정량</title><link>https://freshrimpsushi.github.io/ko/posts/2021/</link><pubDate>Sun, 16 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2021/</guid><description>정의 1 확률변수 $X$ 가 누적분포함수 $F ( x ; \theta), \theta \in \Theta$ 를 가진다고 하자. $X_{1} , \cdots , X_{n}$ 을 $X$ 에서 뽑은 샘플이라고 할 때, 통계량 $T_{n}$ 이 다음을 만족하면 모수 $\theta$ 에 대한 일치</description></item><item><title>스튜던트의 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/203/</link><pubDate>Fri, 30 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/203/</guid><description>정리 1 확률 변수 $X_{1} , \cdots , X_{n}$ 들이 iid로 정규분포 $N\left( \mu,\sigma^{2} \right)$ 를 따른다고 하면 (a): $$ \overline{X} \sim N\left( \mu , { {\sigma^2} \over {n} } \right) $$ (b): $$ \overline{X} \perp S^2 $$ (c): $$ (n-1) { {S^2} \over {\sigma^2} } \sim \chi^2 (n-1) $$ (d): $$ T = { {\overline{X}</description></item><item><title>다변량 확률 변수의 분포 수렴</title><link>https://freshrimpsushi.github.io/ko/posts/2009/</link><pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2009/</guid><description>정의1 $p$차원 랜덤 벡터 $\mathbf{X}$ 와 랜덤 벡터의 시퀀스 $\left\{ \mathbf{X}_{n} \right\}$ 가 다음을 만족하면 $n \to \infty$ 일 때 $\mathbf{X}_{n}$ 이 $\mathbf{X}$ 로 분포 수렴한다고 말하고, $\mathbf{X}_{n} \overset{D}{\to} \mathbf{X}$ 와 같이 나타낸다. $$\lim_{n \to \infty} F_{\mathbf{X}_{n}} (x) =</description></item><item><title>다변량 확률 변수의 확률 수렴</title><link>https://freshrimpsushi.github.io/ko/posts/1952/</link><pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1952/</guid><description>정의 1 $p$차원 랜덤 벡터 $\mathbf{X}$ 와 랜덤 벡터의 시퀀스 $\left\{ \mathbf{X}_{n} \right\}$ 가 다음을 만족하면 $n \to \infty$ 일 때 $\mathbf{X}_{n}$ 이 $\mathbf{X}$ 로 확률 수렴convergence in Probability한</description></item><item><title>공분산 행렬</title><link>https://freshrimpsushi.github.io/ko/posts/1950/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1950/</guid><description>정의1 $p$차원 랜덤 벡터 $\mathbf{X} = \left( X_{1}, \cdots , X_{p} \right)$ 에 대해 다음과 같이 정의된 $\operatorname{Cov} (\mathbf{X})$ 를 공분산 행렬covariance matrix이라 한다. $$ \left( \operatorname{Cov} \left( \mathbf{X} \right) \right)_{ij} := \operatorname{Cov} \left(</description></item><item><title>중심극한 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/43/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/43/</guid><description>정리 1 $\left\{ X_{k} \right\}_{k=1}^{n}$ 이 iid 확률 변수들이고 확률분포 $\left( \mu, \sigma^2 \right) $를 따른다고 하면 $n \to \infty$ 일 때 $$ \sqrt{n} {{ \overline{X}_n - \mu } \over {\sigma}} \overset{D}{\to} N (0,1) $$ $\overset{D}{\to}$ 는 분포 수렴을 의미한다. 설명 통계학에선 대수</description></item><item><title>약한 대수의 법칙 증명</title><link>https://freshrimpsushi.github.io/ko/posts/32/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/32/</guid><description>법칙 $\left\{ X_{k} \right\}_{k=1}^{n}$ 이 iid 확률 변수들이고 확률분포 $\left( \mu, \sigma^2 \right) $를 따른다고 하면 $n \to \infty$ 일 때 $$ \overline{X}_n \overset{P}{\to} \mu $$ $\overset{P}{\to}$ 는확률 수렴을 의미한다. 설명 중심극한정리와 더불어 통계학에서 가</description></item><item><title>분포수렴하면 확률유계다</title><link>https://freshrimpsushi.github.io/ko/posts/176/</link><pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/176/</guid><description>정리 확률변수의 시퀀스 $\left\{ X_{n} \right\}$ 가 분포수렴하면 확률유계다. $\overset{D}{\to}$ 는 분포 수렴을 의미한다. 설명 앞서 확률수렴하면 분포수렴함을 보였으므로, 이 대우 명제를 생각해보</description></item><item><title>확률수렴하면 분포수렴한다</title><link>https://freshrimpsushi.github.io/ko/posts/175/</link><pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/175/</guid><description>정리1 확률변수 $X$ 와 확률변수의 시퀀스 $\left\{ X_{n} \right\}$ 에 대해 $$ X_{n} \overset{P}{\to} X \implies X_{n} \overset{D}{\to} X $$ $\overset{P}{\to}$ 는 확률 수렴을 의미한다. $\overset{D}{\to}$ 는 분포 수렴을 의미한다. 설명 직관적인 단어로 다시 말하자</description></item><item><title>수리통계학에서의 확률 유계</title><link>https://freshrimpsushi.github.io/ko/posts/1922/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1922/</guid><description>정의 1 확률 변수의 시퀀스 $\left\{ X_{n} \right\}$ 가 주어져 있다고 하자. 모든 $\varepsilon &amp;gt; 0$ 에 대해 다음을 만족시키는 $N_{\varepsilon} \in \mathbb{N}$ 과 상수 $B_{\varepsilon} &amp;gt; 0$ 가 존재하면 $\left\{ X_{n} \right\}$ 가 확률 유계bounded in</description></item><item><title>수리통계학에서의 분포 수렴</title><link>https://freshrimpsushi.github.io/ko/posts/1888/</link><pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1888/</guid><description>정의 1 확률변수 $X$ 와 확률 변수의 시퀀스 $\left\{ X_{n} \right\}$ 가 다음을 만족하면 $n \to \infty$ 일 때 $X_{n}$ 이 $X$ 로 분포 수렴convergence in distribution한다고 말하</description></item><item><title>수리통계학에서의 확률 수렴</title><link>https://freshrimpsushi.github.io/ko/posts/1789/</link><pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1789/</guid><description>정의 1 확률변수 $X$ 와 확률 변수의 시퀀스 $\left\{ X_{n} \right\}$ 가 다음을 만족하면 $n \to \infty$ 일 때 $X_{n}$ 이 $X$ 로 확률 수렴convergence in Probability한다고 말하고</description></item><item><title>순서통계량</title><link>https://freshrimpsushi.github.io/ko/posts/1757/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1757/</guid><description>정리1 랜덤 샘플 $X_{1} , \cdots , X_{n}$ 가 서포트 $\mathcal{S} =(a,b)$ 인 확률밀도함수 $f(x)$ 를 가지는 연속확률분포를 따른다고 하자. 이들을 크기 순으로 나열한 확률 변수들을 $Y_{1} &amp;lt; \cdots &amp;lt; Y_{n}$ 와 같이 나</description></item><item><title>표본 분산을 n-1으로 나누는 이유</title><link>https://freshrimpsushi.github.io/ko/posts/1747/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1747/</guid><description>왜 n-1로 나누지? $X_{i} \sim \left( \mu , \sigma^{2} \right)$ 이라고 하면 표본분산 $S^{2}$ 는 다음과 같다. $$ S^{2} := {{1} \over {n-1}} \sum_{i=1}^{n} \left( X_{i} - \overline{X} \right)^{2} $$ 알다시피 표본 평균과 달리 표본 분산은 편차의 제곱을 모두</description></item><item><title>불편추정량</title><link>https://freshrimpsushi.github.io/ko/posts/1745/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1745/</guid><description>정의 1 $\theta$ 의 추정량 $T$ 가 다음을 만족하면 $T$ 를 $\theta$ 의 불편추정량unbiased estimator이라고 한다. $$ E T = \theta $$ 설명 특히 $\theta$ 에 대한 불편추정량 중 가</description></item><item><title>편의-분산 트레이드 오프</title><link>https://freshrimpsushi.github.io/ko/posts/1739/</link><pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1739/</guid><description>정의 $$ \text{MSE} \left( \widehat{\theta} \right) = \Var \widehat{\theta} + \left( \text{Bias} \widehat{\theta} \right)^{2} $$ 설명 평균제곱오차 $\text{MSE}$ 는 통계 모형의 평가나 머신 러닝에서의 손실 함수로써 즐겨쓰이는 척도로써, 특히 편의와 분산에 대한 트레이</description></item><item><title>수리통계학에서의 편의</title><link>https://freshrimpsushi.github.io/ko/posts/1735/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1735/</guid><description>정의 모수 $\theta$ 에 대한 추정량 $\widehat{\theta}$ 에 대해 다음과 같이 정의된 $\text{Bias}$ 를 편의라 한다. $$ \text{Bias} ( \theta ) = E(\widehat{\theta}) - \theta $$ 설명 Bias는 편의 또는 편향으로 순화되지만, 역시 가장 많이 쓰이</description></item><item><title>신뢰구간의 쉬운 정의</title><link>https://freshrimpsushi.github.io/ko/posts/1732/</link><pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1732/</guid><description>정의 1 확률 밀도 함수 $f (x; \theta)$ 를 가지는 확률 변수 $X$ 의 샘플 $X_{1} , \cdots , X_{n}$ 와 신뢰 계수confidence coefficient $\alpha \in (0,1)$ 가 주어져 있다고 하자. $$ L := L \left( X_{1} , \cdots , X_{n} \right) \\</description></item><item><title>수리통계학에서의 통계량과 추정량</title><link>https://freshrimpsushi.github.io/ko/posts/1730/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1730/</guid><description>정의 1 2 확률 변수 $X$ 의 샘플 $X_{1} , \cdots , X_{n}$ 의 함수 $T$ 를 통계량statistic이라 한다. $$ T := T \left( X_{1} , \cdots , X_{n} \right) $$ $X$ 의 분포 함수가 $f(x; \theta)$ 혹은 $p(x; \theta)$ 와 같이 나타날</description></item><item><title>수리통계학에서의 랜덤 샘플</title><link>https://freshrimpsushi.github.io/ko/posts/1715/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1715/</guid><description>정의 1 확률 변수 $X$ 가 실제로 뽑힌 것을 실현realization이라 하고 보통 소문자 $x$ 로 나타낸다. 확률 변수 $X$ 와 같은 확률 분포에서 샘플 사이즈sample</description></item><item><title>확률 변수들의 선형 결합</title><link>https://freshrimpsushi.github.io/ko/posts/1479/</link><pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1479/</guid><description>정의 1 확률변수 $X_{1} , \cdots , X_{n}$ 가 주어져 있다고 하자. 어떤 $(a_{1}, \cdots , a_{n}) \in \mathbb{R}^{n}$ 에 대해 $\displaystyle T := \sum_{i=1}^{n} a_{i} X_{i}$ 를 선형 결합linear Combinations이라고 한다. 설명</description></item><item><title>정규분포를 따르는 두 확률 변수가 독립인 것과 공분산이 0인 것은 동치다</title><link>https://freshrimpsushi.github.io/ko/posts/591/</link><pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/591/</guid><description>정리 $$ X_{1} \sim N ( \mu_{1} , \sigma_{1} ) \\ X_{2} \sim N ( \mu_{2} , \sigma_{2} ) $$ 면 $$ X_{1} \perp X_{2} \iff \text{cov} (X_{1} , X_{2} ) = 0 $$ 설명 일반적으로 상관관계가 없다고 독립인 것은 아니다. 하지만 분포들이 정규분포</description></item><item><title>번스타인 분포: 짝으로 독립이라고 상호 독립은 아니다</title><link>https://freshrimpsushi.github.io/ko/posts/206/</link><pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/206/</guid><description>정의 $(x,y,z) \in \left\{ (1,0,0), (0,1,0), (0,0,1), (1,1,1) \right\}$ 에 대해 다음과 같은 확률질량함수를 가지는 분포를 번스타인 분포bernstein distribution라고 한다. $$ p(x,y,z) = {{1} \over {4} }</description></item><item><title>확률 변수들의 상호 독립과 iid</title><link>https://freshrimpsushi.github.io/ko/posts/1469/</link><pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1469/</guid><description>정의 1 확률 변수 $X_{1} , \cdots , X_{n}$ 가 다음을 만족하면 $X_{1} , \cdots , X_{n}$ 이 짝으로 독립pairwise independent이라 한다. $$ i \ne j \implies X_{i} \perp X_{j} $$ 연속 확률 변수</description></item><item><title>수리통계학에서의 확률 변수의 독립</title><link>https://freshrimpsushi.github.io/ko/posts/1461/</link><pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1461/</guid><description>정의 1 두 확률 변수 $X_{1}, X_{2}$ 의 조인트 확률 밀도 함수 $f$ 혹은 확률 질량 함수 $p$ 에 대해 $X_{1}, X_{2}$ 의 확률 밀도 함수들 $f_{1}, f_{2}$ 혹은 확률 질량 함수 $p_{1}, p_{2}$ 가 다음을 만족하면 $X_{1}, X_{2}$ 가 독립이라</description></item><item><title>수리통계학에서의 조건부 확률 분포</title><link>https://freshrimpsushi.github.io/ko/posts/1458/</link><pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1458/</guid><description>정의1 이산 랜덤 벡터 $(X, Y)$에 대해서, $p_{X, Y}$를 $(X, Y)$의 결합 확률질량함수라고 하자. $p_{X}$를 $X$의 주변 확률질량함수라고 하자. 이 때, 다</description></item><item><title>다변량 확률 변수의 변환</title><link>https://freshrimpsushi.github.io/ko/posts/1455/</link><pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1455/</guid><description>공식 다변량 확률 변수 $X = ( X_{1} , \cdots , X_{n} )$ 의 조인트 확률밀도함수 $f$ 가 $f(x_{1} , \cdots , x_{n})$ 와 같이 주어져있다고 하고 다음과 같은 변환을 생각해보자. $$ y_{1} = u_{1} (x_{1} , \cdots , x_{n}) \\</description></item><item><title>수리통계학에서의 다변량 확률 분포</title><link>https://freshrimpsushi.github.io/ko/posts/1449/</link><pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1449/</guid><description>정의 1 표본 공간 $\Omega$ 에서 정의된 $n$ 개의 확률 변수 $X_{i}$ 에 대해 $X = (X_{1} , \cdots , X_{n})$ 를 $n$차원 랜덤 벡터random vector라고 한다. $X$ 의 치역 $X(\Omega)$ 를 공간이라고도</description></item><item><title>n차 적률이 존재하면 차수가 n보다 작은 적률도 존재한다</title><link>https://freshrimpsushi.github.io/ko/posts/247/</link><pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/247/</guid><description>정리 확률변수 $X$와 자연수 $n$ 에 대해 $E( X^n )$ 이 존재하면 $E( X^m ), m=1,2,3,\cdots, n$ 도 존재한다. 설명 어떤 차수의 적률이든 존재하기만 한다면 그보다 작은 차수의 적률은 항상 존재</description></item><item><title>적률생성함수란?</title><link>https://freshrimpsushi.github.io/ko/posts/248/</link><pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/248/</guid><description>정의 1 확률변수 $X$ 와 어떤 양수 $h&amp;gt;0$ 대해 $E(e^{tX})$ 이 $-h&amp;lt; t &amp;lt; h$ 에서 존재하면 $M(t) = E( e^{tX} )$ 를 $X$ 의 적률생성함수moment Generating function라고 정의한다. 설명 적률생성</description></item><item><title>수리통계학에서의 첨도</title><link>https://freshrimpsushi.github.io/ko/posts/1271/</link><pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1271/</guid><description>첨도 확률변수 $X$ 의 평균이 $\mu$, 분산이 $\sigma^2$ 라고 할 때 다음과 같이 정의된 $\gamma_{2}$ 를 $X$ 의 첨도kurtosis라고 한다. $$ \gamma_{2} := {{ E \left( X - \mu \right)^4 } \over { \sigma^4 }} $$ 데이터 $\left\{ X_{i} \right\}_{i}^{n}$ 의</description></item><item><title>수리통계학에서의 왜도</title><link>https://freshrimpsushi.github.io/ko/posts/1268/</link><pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1268/</guid><description>정의 확률변수 $X$ 의 평균이 $\mu$, 분산이 $\sigma^2$ 라고 할 때 다음과 같이 정의된 $\gamma_{1}$ 를 $X$ 의 왜도skewness라고 한다. $$ \gamma_{1} := {{ E \left( X - \mu \right)^3 } \over { \sigma^3 }} $$ 데이터 $\left\{ X_{i} \right\}_{i}^{n}$ 의</description></item><item><title>공분산의 여러가지 성질들</title><link>https://freshrimpsushi.github.io/ko/posts/425/</link><pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/425/</guid><description>정의와 성질 평균이 각각 $\mu_{X}$, $\mu_{Y}$ 인 확률 변수 $X$, $Y$ 에 대해 $\operatorname{Cov} (X ,Y) : = E \left[ ( X - \mu_{X} ) ( Y - \mu_{Y} ) \right]$ 을 $X$ 와 $Y$ 의 공분산covariance이라고 정의한다. 공분산은</description></item><item><title>피어슨 상관계수</title><link>https://freshrimpsushi.github.io/ko/posts/57/</link><pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/57/</guid><description>정의 1 두 확률변수 $X, Y$ 에 대해 다음과 같이 정의된 $\rho = \rho (X,Y)$ 를 피어슨 상관계수pearson Correlation라고 한다. $$ \rho = { {\operatorname{Cov} (X,Y)} \over {\sigma_X \sigma_Y} } $$ $\sigma_{X}$, $\sigma_{Y}$ 는</description></item><item><title>평균과 분산의 성질들</title><link>https://freshrimpsushi.github.io/ko/posts/424/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/424/</guid><description>정리 평균 $E ( X ) = \mu_{X}$ 과 분산 $\Var (X) = E [ ( X - \mu_{X} )^2 ]$ 은 아래의 성질들을 가진다. [1]: $E(X + Y) = E(X) + E(Y)$ [2]: $E(aX + b) = a E(X) + b$ [3]: $\Var (X) \ge 0$ [4]: $\Var ( X ) = E(X^2) - \mu_{X}^2$ [5]: $\Var (aX</description></item><item><title>대표값의 수리적 성질 증명</title><link>https://freshrimpsushi.github.io/ko/posts/49/</link><pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/49/</guid><description>정리 데이터 $X = \left\{ x_{1} , \cdots , x_{n} \right\}$ 가 주어져 있다고 하자. [0]: $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{0}$ 가 최소가 되도록 하는 $\theta$ 는 $$ \argmin_{\theta} h \left( \theta \right) = \text{mode}(X) $$ [1]: $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{1}$ 가 최소가 되도록 하는 $\theta$ 는 $$ \argmin_{\theta} h</description></item><item><title>수리통계학에서의 기대값, 평균, 분산, 적률의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/246/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/246/</guid><description>정의: 기대값, 평균, 분산 확률 변수 $X$ 가 주어져 있다고 하자. 연속 확률 변수 $X$ 의 확률 밀도 함수 $f(x)$ 가 $\displaystyle \int_{-\infty}^{\infty} |x| f(x) dx &amp;lt; \infty$ 를 만족하면 다음과 같이 정의된 $E(X)$ 를 $X$ 의 기대값</description></item><item><title>수리통계학에서의 확률 변수와 확률 분포</title><link>https://freshrimpsushi.github.io/ko/posts/1433/</link><pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1433/</guid><description>정의 1 표본 공간 $\Omega$ 에서 확률 $P$ 가 정의되어 있다고 하자. 정의역이 표본 공간인 함수 $X : \Omega \to \mathbb{R}$ 을 확률 변수random variable라고 한다. 확률 변수의 치</description></item><item><title>수리통계학에서의 확률과 확률의 덧셈법칙</title><link>https://freshrimpsushi.github.io/ko/posts/1431/</link><pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1431/</guid><description>정의 1 같은 조건 하에서 반복할 수 있는 시행을 임의 시행random experiment이라고 한다. 임의 시행에서 얻을 수 있는 모든 결과outcome를 모아</description></item><item><title>베이즈 인자를 통한 가설검정</title><link>https://freshrimpsushi.github.io/ko/posts/782/</link><pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/782/</guid><description>빌드업 고전적인 가설검정을 쓸 수 있게 되려면 기각역, 유의확률과 같은 개념에 대한 수학적인 이해를 포함해서 이를 직관적으로 받아들일 수 있을 정도의 통계학적 센스까</description></item><item><title>최고사후밀도 신용구간</title><link>https://freshrimpsushi.github.io/ko/posts/769/</link><pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/769/</guid><description>정의 1 모수공간 $\Theta$ 의 부분집합 $C \subset \Theta$ 가 유의수준 $\alpha$ 에 대해 $C : = \left\{ \theta \in \Theta \ | \ p ( \theta | y ) \ge k (\alpha) \right\}$ 를 자료 $y$ 가 주어졌을 때 $\theta$ 에 대한 $100(1 - \alpha) % $ 최고사후밀도</description></item><item><title>신용구간과 신뢰구간의 차이</title><link>https://freshrimpsushi.github.io/ko/posts/752/</link><pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/752/</guid><description>요약 신용구간과 신뢰구간의 차이는 실로 베이지안과 프리퀀티스트의 차이라고 볼 수 있다. 신뢰구간(프리퀀티스트): 모수는 고정된 상수고, 신뢰구간이 랜덤으로</description></item><item><title>신용구간</title><link>https://freshrimpsushi.github.io/ko/posts/741/</link><pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/741/</guid><description>정의 1 모수공간 $\Theta$ 의 부분집합 $C \subset \Theta$ 가 유의수준 $\alpha$ 에 대해 $P ( \theta \in C | y ) \ge 1 - \alpha$ 를 만족할 때, $C$ 를 자료 $y$ 가 주어졌을 때 $\theta$ 에 대한 $100(1 - \alpha) % $ 신용구간cre</description></item><item><title>통계학의 세가지 대표값: 최빈값, 중앙값, 평균</title><link>https://freshrimpsushi.github.io/ko/posts/740/</link><pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/740/</guid><description>개요 대표값은 데이터를 설명하는 대표적인 값을 말한다. 수천 수만에 달하는 데이터가 있어도 일일이 다 살펴볼 게 아니라면 결국 중요한 것은 데이터가 무엇을 의미하느냐</description></item><item><title>제프리 사전분포</title><link>https://freshrimpsushi.github.io/ko/posts/716/</link><pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/716/</guid><description>정의 1 자료의 분포 $p( y | \theta)$ 에 대해 $\pi ( \theta ) \propto I^{1/2} ( \theta )$ 를 제프리 사전분포jeffreys prior라 한다. $I$ 는 피셔정보fishser Informat</description></item><item><title>라플라스 사전분포</title><link>https://freshrimpsushi.github.io/ko/posts/714/</link><pubDate>Tue, 06 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/714/</guid><description>빌드업 모수에 대한 정보가 거의 없다면 구태여 복잡한 사전분포를 생각할 이유는 없다: 예시1: 내년 모 대학의 통계학과 신입생의 성비를 추측해보라고 했을 때, 통계학</description></item><item><title>켤레사전분포</title><link>https://freshrimpsushi.github.io/ko/posts/712/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/712/</guid><description>정의 1 사전분포와 사후분포가 동일한 분포족에 속하면 사전분포를 켤레사전분포conjugate prior 혹은 공액사전분포라고 한다. 설명 베이지안이란 본래 사전분</description></item><item><title>라플라스 계승 법칙</title><link>https://freshrimpsushi.github.io/ko/posts/710/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/710/</guid><description>정리 1 이항모형 $\displaystyle p(y | \theta) = \binom{ n }{ y} \theta^{y} (1- \theta)^{n-y}$ 의 사전분포가 일양 분포 $U (0,1)$ 를 따르고 사후분포가 베타 분포 $\beta (y+1 , n-y+1)$ 을 따라 $p( \theta | y ) \sim \theta^{y} (1- \theta)^{n-y}$ 이라고 하자. 그러면 이</description></item><item><title>베이지안 패러다임</title><link>https://freshrimpsushi.github.io/ko/posts/702/</link><pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/702/</guid><description>빌드업 통계학이란 &amp;lsquo;모수를 파악하는 방법을 연구하는 학문&amp;rsquo;이라고 할 수 있다. 어떤 물리량을 측정하는 것처럼 공식이나 법칙을 통해 정확</description></item><item><title>베이즈 정리로 보는 몬티홀 딜레마</title><link>https://freshrimpsushi.github.io/ko/posts/697/</link><pubDate>Mon, 08 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/697/</guid><description>설명 알다시피 몬티홀 게임은 실제로 경품이 어디있든 관계 없이 선택을 바꾸는 것이 유리하다. 이것을 팩트로써 받아들이냐와 별개로 몬티홀 게임을 직관적으로 이해하지</description></item><item><title>몬테카를로 방법과 부트스트랩의 차이점</title><link>https://freshrimpsushi.github.io/ko/posts/551/</link><pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/551/</guid><description>개요 몬테카를로 방법은 작위적인 데이터로 시뮬레이션을 반복해 새로운 기법을 확인하는 방법이고 부트스트랩은 실제 데이터에서 재표본 추출을 통해 비용을 절감하며 문</description></item><item><title>표본표준편차와 표준오차의 구분</title><link>https://freshrimpsushi.github.io/ko/posts/541/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/541/</guid><description>정의 $X$ 로부터 얻은 데이터를 $\mathbf{x} = ( x_{1}, x_{2}, \cdots , x_{n} )$ 라고 하자. 표본평균: $$ \overline{x} = {{1} \over {n}} \sum_{i=1}^{n} x_{i} $$ 표본표준편차: $$ s_{x} = \sqrt { {{1} \over {n-1}} \sum_{i=1}^{n} ( x_{i} - \overline{x} )^2 } $$ 표준오차: $$ \text{s.e.}</description></item><item><title>상관관계가 없다고 독립인 것은 아니다</title><link>https://freshrimpsushi.github.io/ko/posts/536/</link><pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/536/</guid><description>설명 독립이면 상관관계가 없지만, 상관관계가 없다고 독립인 것은 아니다. 상관관계가 없을 때 독립인 경우, 즉 필요충분조건이 되는 경우는 확률변수가 정규분포를 따</description></item><item><title>특정한 분포를 따르는 확률변수들의 덧셈 총정리</title><link>https://freshrimpsushi.github.io/ko/posts/202/</link><pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/202/</guid><description>정리 확률 변수 $X_{1} , \cdots , X_{n}$ 들이 상호 독립이라고 하자. [1] 이항 분포: $X_i \sim \text{Bin} ( n_{i}, p)$ 이면 $$ \sum_{i=1}^{m} X_{i} \sim \text{Bin} \left( \sum_{i=1}^{m} n_{i} , p \right) $$ [2] 푸아송 분포: $X_i \sim \text{Poi}( m_{i} )$ 이면 $$ \sum_{i=1}^{n} X_{i} \sim \text{Poi} \left( \sum_{i=1}^{n}</description></item><item><title>베이즈 정리의 증명과 사전분포, 사후분포</title><link>https://freshrimpsushi.github.io/ko/posts/29/</link><pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/29/</guid><description>정리 1 표본공간 $S$ 와 사건 $A$, 확률 $P$에 대해서 $\left\{ S_1, S_2, \cdots ,S_n \right\}$ 가 $S$ 의 분할이면 다음이 성립한다. $$ P(S_k|A)=\frac { P(S_k)P(A|S_k) }{ \sum _{ k=1 }^{ n }{ P(S_k)P(A|S_k) } } $$ 정의 베이즈 정리의 우변에 있는</description></item></channel></rss>