<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>머신러닝 on 생새우초밥집</title><link>https://freshrimpsushi.github.io/ko/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/</link><description>Recent content in 머신러닝 on 생새우초밥집</description><generator>Hugo -- gohugo.io</generator><language>ko</language><lastBuildDate>Mon, 28 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/ko/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/index.xml" rel="self" type="application/rss+xml"/><item><title>샘플링, 복원추출과 비복원추출</title><link>https://freshrimpsushi.github.io/ko/posts/2686/</link><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2686/</guid><description>정의 표본sample을 뽑는 행위를 샘플링sampling 혹은 더 간단히 추출이라 한다. 샘플링 과정 중 이미 뽑힌 표본을 모집단에 다시 넣는 것을 복원repla</description></item><item><title>논문 리뷰: 스코어 매칭(Score Matching)</title><link>https://freshrimpsushi.github.io/ko/posts/3676/</link><pubDate>Mon, 21 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3676/</guid><description>개요 스코어 매칭score matching은 2005년에 발표된 Aapo Hyvarinen의 논문 Estimation of Non-Normalized Statistical Models by Score Matching에서 소개된 통계적 기법으로</description></item><item><title>에너지 기반 모델</title><link>https://freshrimpsushi.github.io/ko/posts/3664/</link><pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3664/</guid><description>개요1 2 3 에너지 기반 모델이란, 데이터의 에너지energy라는 함수를 정의하여 에너지가 낮은 데이터일 수록 그럴 듯한 데이터(=확률이 높은 데이터)라 간주</description></item><item><title>기계학습에서 선형회귀모델의 최대우도 추정</title><link>https://freshrimpsushi.github.io/ko/posts/3643/</link><pubDate>Wed, 14 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3643/</guid><description>정리 데이터 $\mathbf{x}_{i} \in \mathbb{R}^{n}$와 이의 레이블 $y_{i} \in \mathbb{R}$ 사이의 관계가 다음과 같은 선형모델이라 가정하자. $$ y_{i} = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{i} + \epsilon_{i}, \qquad i = 1, \ldots, K \tag{1} $$ $K &amp;gt;</description></item><item><title>기계학습에서 베이즈 추론</title><link>https://freshrimpsushi.github.io/ko/posts/3642/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3642/</guid><description>개요 베이즈 추론Bayesian inference이란, 베이즈 정리를 기반으로 사전 지식과 관측된 데이터를 통해 모수의 분포를 추정하는 통계적 방법이다. 설</description></item><item><title>기계학습에서 선형회귀모델의 최대사후확률 추정</title><link>https://freshrimpsushi.github.io/ko/posts/3641/</link><pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3641/</guid><description>정리 데이터 $\mathbf{x}_{i} \in \mathbb{R}^{n}$와 이의 레이블 $y_{i} \in \mathbb{R}$ 사이의 관계가 다음과 같은 선형모델이라 가정하자. $$ y_{i} = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{i} + \epsilon_{i}, \qquad i = 1, \ldots, K \tag{1} $$ 사후</description></item><item><title>논문 리뷰: Denoising Diffusion Probabilistic Models (DDPM)</title><link>https://freshrimpsushi.github.io/ko/posts/3638/</link><pubDate>Sun, 04 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3638/</guid><description>개요 및 요약 생성 모델이란, 주어진 랜덤 샘플 $\left\{ y_{i} \right\}$이 따르는 확률분포 $Y$ 혹은 그것을 찾는 방법을 말한다. 맨땅에서 이를 찾는 것은 매우 어려우므로</description></item><item><title>생성 모델</title><link>https://freshrimpsushi.github.io/ko/posts/3637/</link><pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3637/</guid><description>개요 우리가 갖고 있는 데이터가 따르는 확률 분포를 정확하게 알아내는 일은 많은 응용 분야에서 중요한 문제이지만, 매우 어려운 문제이기도 하다. 가령 사람 얼굴 사진의</description></item><item><title>최적화기</title><link>https://freshrimpsushi.github.io/ko/posts/1019/</link><pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1019/</guid><description>정의 최적화 문제란, 함수 $f : \mathbb{R}^{n} \to \mathbb{R}$의 함숫값이 최소가 되도록 하는 $x_{\ast}$를 찾는 것을 말한다. $$ x_{\ast} = \argmin\limits_{x} f(x) $$ 최적화 문제를</description></item><item><title>머신러닝에서 실루 혹은 스위시 함수 SiLU, Swish</title><link>https://freshrimpsushi.github.io/ko/posts/883/</link><pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/883/</guid><description>정의 1 2 실루SiLU, Sigmoid-weighted Linear Unit 혹은 스위시Swish 함수는 다음과 같이 정의된다. $$ \operatorname{SiLU}(x) = x \cdot \sigma(x) $$ 여기서 $\sigma$ 는 시그모이드 함수 중 특히 로지스틱 함수 $\sigma(x) = \left( 1 +</description></item><item><title>오토인코더 (자동부호기)</title><link>https://freshrimpsushi.github.io/ko/posts/1181/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1181/</guid><description>정의 두 자연수 $m \ll n$에 대해서, 함수 $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$를 부호기encoder, 인코더라 한다. 함수 $g : \mathbb{R}^{m} \to \mathbb{R</description></item><item><title>이미지(신호, 데이터)에서 노이즈와 아티팩트의 차이</title><link>https://freshrimpsushi.github.io/ko/posts/1192/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1192/</guid><description>개요 노이즈와 아티팩트는 공통적으로 원본 신호(데이터)를 훼손시키는 요소이며 제거해야할 대상이다. 본 문서에서는 이 둘이 가지는 특징과, 서로 어떤 차이가 있는</description></item><item><title>인터넷에 무료 공개된 인공지능, 머신러닝, 딥러닝 교재</title><link>https://freshrimpsushi.github.io/ko/posts/1253/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1253/</guid><description>설명 인공지능, 머신러닝, 딥러닝 분야의 교재들은 인터넷에 무료로 공개되어있는 경우가 많다. 특히 유명한 교재인 경우에도 무료 공개인 경우가 꽤 있다. 몇가지를 소</description></item><item><title>줄리아에서 이원수를 이용하여 자동미분 전진모드 구현하기</title><link>https://freshrimpsushi.github.io/ko/posts/1498/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1498/</guid><description>개요 자동미분의 전진모드는 이원수를 이용하면 쉽게 구현할 수 있다. 줄리아에서 전진모드를 구현하는 방법을 설명한다. 이원수와 자동미분에 대한 배경지식을 위해 아</description></item><item><title>자동미분과 이원수</title><link>https://freshrimpsushi.github.io/ko/posts/1501/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1501/</guid><description>개요 이원수란, 두 실수 $a, b \in \mathbb{R}$에 대해서 다음과 같은 꼴로 표현되는 수를 말한다. $$ a + b\epsilon, \quad (\epsilon^{2} = 0,\ \epsilon \neq 0) $$ 이원수의 덧셈과 곱셈 체계는 자</description></item><item><title>줄리아 플럭스에서 구조체를 이용하여 함수형 API로 신경망 정의하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/1539/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1539/</guid><description>설명 간단한 구조의 신경망은 Flux.Chain을 사용하여 정의할 수 있지만, 복잡한 구조의 신경망은 Chain으로 정의하기 어렵다. 이 때는 @functor 매크로를 사용</description></item><item><title>소금 후추 노이즈</title><link>https://freshrimpsushi.github.io/ko/posts/76/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/76/</guid><description>정의 이미지에 작은 점으로 흰 색이나 검은 색으로 나타나는 노이즈를 소금 후추 노이즈salt-and pepper noise라 한다. 예시 예로써 위의 이미지에 소금 후추 노이</description></item><item><title>논문 리뷰: 콜모고로프-아놀드 신경망(KAN)</title><link>https://freshrimpsushi.github.io/ko/posts/322/</link><pubDate>Sun, 29 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/322/</guid><description>개요 및 요약 Kolmogorov–Arnold Networks(KAN)은 그 이름 그대로 콜모고로프-아놀드 표현 정리Kolmogorov–Arno</description></item><item><title>DeepONet 논문 구현 무작정 따라하기: 선형 상미분 방정식 풀이 (PyTorch)</title><link>https://freshrimpsushi.github.io/ko/posts/1153/</link><pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1153/</guid><description>개요 DeepONet은 비선형 연산자를 학습하기위한 신경망구조로 논문이 공개된 이후 편미분 방정식의 풀이 등 많은 분야에서 응용되고 있다. 본 글에서는 PyTo</description></item><item><title>파이토치에서 자동미분하는 법</title><link>https://freshrimpsushi.github.io/ko/posts/1966/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1966/</guid><description>설명 파이토치에서 자동미분하는 방법을 소개한다. 파이토치에서 자동 미분은 torch.autograd.grad 함수로 구현되어있다. torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=None, is_grads_batched=False, materialize_grads=False) outputs: 그래디언트(자동미분)를 계산할 함</description></item><item><title>파이토치로 PINN 논문 구현하기</title><link>https://freshrimpsushi.github.io/ko/posts/1967/</link><pubDate>Mon, 16 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1967/</guid><description>설명 PINN은 Physics-Informed Neural Networks의 약자로 자동미분과 인공신경망을 이용하여 미분방정식의 해를 수치적으로 구하는 것을 말한다. PINN 논문에서는 이를 텐서플로</description></item><item><title>줄리아 Flux에서 신경망 훈련모드, 테스트모드 설정하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/1975/</link><pubDate>Sat, 31 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1975/</guid><description>설명 신경망의 구조 중에서 훈련과정과 테스트과정에서 다르게 작동해야하는 부분이 있다. 예를 들어, 드롭 아웃은 훈련 중에는 적용되어야하지만, 훈련이 끝난 뒤 테스</description></item><item><title>줄리아 플럭스에서 GPU 사용하는 법</title><link>https://freshrimpsushi.github.io/ko/posts/2611/</link><pubDate>Mon, 12 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2611/</guid><description>개요 줄리아의 머신러닝 라이브러리인 Flux.jl을1 통해 딥러닝을 구현하되, GPU를 통해 성능을 학습을 가속시키는 방법에 대해 소개한다. GPU를 사용하</description></item><item><title>논문 리뷰: DeepONet</title><link>https://freshrimpsushi.github.io/ko/posts/1180/</link><pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1180/</guid><description>개요 및 요약 레퍼런스, 수식의 번호, 표기법 등은 가능한 논문을 따른다. 접근성을 위해서 저널에 출간된 버전이 아닌 아카이브에 올라와있는 버전을 기준으로 리뷰한다</description></item><item><title>대학원생 하강법</title><link>https://freshrimpsushi.github.io/ko/posts/2598/</link><pubDate>Wed, 17 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2598/</guid><description>빌드업 냉장코-코끼리 문제 전통적으로 코끼리를 냉장고에 넣는 방법은 대학원생에게 의존해왔다. 얼마나 어려울지, 힘들지, 어떤 방법이 좋은지는 잘 모르겠지만 그</description></item><item><title>그리드 서치, 브루트 포스, 노가다</title><link>https://freshrimpsushi.github.io/ko/posts/2596/</link><pubDate>Sat, 13 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2596/</guid><description>용어 그리드 서치 주로 최적화 문제에서, 유클리드 공간 $\mathbb{R}^{n}$ 을 격자grid로 나누고 가능한 많은 점에 대해 시도를 반복해 최적해를 찾는 방식을 그리드 서치grid se</description></item><item><title>파이토치에서 torch.nn과 torch.nn.functional의 차이</title><link>https://freshrimpsushi.github.io/ko/posts/3626/</link><pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3626/</guid><description>설명 파이토치에는 신경망과 관련된 많은 함수들이 torch.nn과 torch.nn.functional에 같은 이름으로 포함되어있다. nn의 함수들은</description></item><item><title>머신러닝에서 가중치란?</title><link>https://freshrimpsushi.github.io/ko/posts/3625/</link><pubDate>Mon, 08 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3625/</guid><description>정의 머신러닝에서는 최적화해야할 파라매터를 가중치weight라 부른다.</description></item><item><title>인공신경망에서 스킵 커넥션이란?</title><link>https://freshrimpsushi.github.io/ko/posts/3624/</link><pubDate>Sat, 06 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3624/</guid><description>정의 $\mathbf{W}$를 가중치, $\mathbf{x}$를 입력, $\sigma$를 비선형 활성화함수라고 하자. 레이어 $L_{\mathb</description></item><item><title>파이토치에서 AdaBelief 옵티마이저 사용하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3620/</link><pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3620/</guid><description>설명 AdaBelief는 2020년 J. Zhuang 외 6명에 의해 소개된 옵티마이저로 Adam의 변형 중 하나이다1. 파이토치에서 이 옵티마이저를 기본적으로 제공하지</description></item><item><title>잠재 변수와 잠재 공간</title><link>https://freshrimpsushi.github.io/ko/posts/3589/</link><pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3589/</guid><description>정의 데이터 집합 $X \subset \mathbb{R}^{n}$이 주어졌다고 하자. 데이터 집합을 정의역으로 갖는 함수를 인코더라 한다. $$ \begin{align*} f : X &amp;amp;\to Z \\ \mathbf{x} &amp;amp;\mapsto \mathbf{z} = f(\mathbf{x}) \end{align*}</description></item><item><title>텐서플로-케라스에서 시퀄션 모델, 함수형 API로 MLP 정의하고 훈련하는 법</title><link>https://freshrimpsushi.github.io/ko/posts/3562/</link><pubDate>Mon, 04 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3562/</guid><description>개요 텐서플로에서는 케라스를 이용하여 쉽게 신경망을 정의할 수 있다. 아래에서는 Sequential()과 함수형 API로 간단한 MLP를 정의하고 훈련하는</description></item><item><title>적응적 학습률: AdaGrad, RMSProp, Adam</title><link>https://freshrimpsushi.github.io/ko/posts/3529/</link><pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3529/</guid><description>개요1 2 경사하강법에서 사용되는 적응적 학습률과 이를 적용한 모델인 AdaGrad, RMSProp, Adam에 대해 설명한다. 설명 경사하강법에서 학습률learning rate은 파</description></item><item><title>경사하강법에서 모멘텀 기법</title><link>https://freshrimpsushi.github.io/ko/posts/3528/</link><pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3528/</guid><description>개요1 2 경사하강법에서 모멘텀 기법이란, 파라매터를 업데이트할 때 이전의 그래디언트도 모두 사용하는 것이다. 이것이 본질이며 이것만이 설명의 끝이다. 그런데</description></item><item><title>파이토치에서 모듈러 연산</title><link>https://freshrimpsushi.github.io/ko/posts/3527/</link><pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3527/</guid><description>설명 모듈러 연산이란, 나머지 연산이라고도 불리며 $a$를 $b$로 나누었을 때의 나머지를 반환하는 함수를 말한다. 파이토치에는 두 가지 함수가 있다. torch.remainder(a,b) torch.fmod(a,b) 둘 다</description></item><item><title>머신러닝에서 온라인 러닝과 배치 러닝이란?</title><link>https://freshrimpsushi.github.io/ko/posts/3526/</link><pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3526/</guid><description>개요 온라인 러닝, 배치 러닝, 미니배치 러닝에 대해서 설명한다. 이들의 이름과 차이점이 실제로 중요한 것은 아니며, 별거 아니라는 점을 말하고자 한다. 실제로 최근</description></item><item><title>데이터 증강이란?</title><link>https://freshrimpsushi.github.io/ko/posts/3522/</link><pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3522/</guid><description>정의 데이터 집합 $X = \left\{ x \in \mathbb{R}^{n} \right\}$가 주어졌다고 하자. 적당한 변환 $f_{i} : \mathbb{R}^{n} \to \mathbb{R}^{n}$들을 사용하여 $X$로부터 $X^</description></item><item><title>몬테카를로 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3521/</link><pubDate>Wed, 13 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3521/</guid><description>정의 쉽게 말해서 몬테카를로 방법Monte-Carlo method이란, 무작위로 많이 해보는 것이다. 설명 단순한 방법이지만, 많이 해보는 것만큼이나 쉽고</description></item><item><title>기각 샘플링</title><link>https://freshrimpsushi.github.io/ko/posts/3518/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3518/</guid><description>개요 1 기각 샘플링은 몬테카를로 방법 중 하나로, 주어진 분포 $p$로 샘플링하기 어려울 때 샘플링하기 쉬운 제안분포 $q$를 이용하여 $p$를 따르는 샘플을 얻는</description></item><item><title>중요도 샘플링</title><link>https://freshrimpsushi.github.io/ko/posts/3516/</link><pubDate>Sun, 03 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3516/</guid><description>개요1 중요도 샘플링은 몬테카를로 방법 중 하나로, 유한합으로 적분(기댓값)을 근사할 때 사용할 수 있는 일종의 샘플링 트릭이다. 도입 표준정규분포에서 $z$ 값이 $</description></item><item><title>몬테 카를로 적분</title><link>https://freshrimpsushi.github.io/ko/posts/3515/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3515/</guid><description>개요 몬테 카를로 적분은 주어진 함수의 적분을 계산하기 어려울 때 사용하는 수치적인 근사 방법 중 하나이다. 다음과 같은 상황을 가정하자. 주어진 $[0, 1]$혹은 일반적</description></item><item><title>파이토치에서 모델 저장할 때 'RuntimeError: Parent directory does not exists' 에러 해결법</title><link>https://freshrimpsushi.github.io/ko/posts/3502/</link><pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3502/</guid><description>오류 파이토치에서 모델이나 가중치를 저장할 때, 분명 존재하는 경로임에도 불구하고 다음과 같은 오류를 마주할 수 있다. &amp;gt;&amp;gt;&amp;gt; print(&amp;#34;Is exists path?: &amp;#34;, os.path.exists(directory)) Is exists path?: True &amp;gt;&amp;gt;&amp;gt; torch.save(model.state_dict(), directory + &amp;#39;weights.pt&amp;#39;) RuntimeError: Parent directory _____</description></item><item><title>플럭스-파이토치-텐서플로우 치트 시트</title><link>https://freshrimpsushi.github.io/ko/posts/3489/</link><pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3489/</guid><description>개요 플럭스, 파이토치, 텐서플로우에서 같은 기능을 하는 코드를 정리한다. 줄리아-매트랩-파이썬-R 치트시트 플럭스에 대해서 다음과 같은 환경이라고 하자.</description></item><item><title>파이토치에서 텐서 정렬에 관한 함수</title><link>https://freshrimpsushi.github.io/ko/posts/3487/</link><pubDate>Fri, 06 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3487/</guid><description>torch.sort() torch.sort()에 텐서를 입력하면, 정렬된 값과 인덱스를 반환한다. 1차원 텐서 &amp;gt;&amp;gt;&amp;gt; x = torch.tensor([1, 3, -2, 5, -1, 7, 0]) &amp;gt;&amp;gt;&amp;gt; values, indices = torch.sort(x) &amp;gt;&amp;gt;&amp;gt; values tensor([-2, -1, 0, 1, 3, 5, 7]) &amp;gt;&amp;gt;&amp;gt; indices tensor([2, 4, 6,</description></item><item><title>파이토치에서 'RuntimeError: Boolean value of Tensor with more than one value is ambiguous' 에러 해결 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3486/</link><pubDate>Wed, 04 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3486/</guid><description>에러 손실함수인 nn.MESLoss()를 사용했을 때 다음과 같은 에러가 발생했다. RuntimeError Traceback (most recent call last) &amp;lt;ipython-input-75-8c6e9ea829d4&amp;gt; in &amp;lt;module&amp;gt; ----&amp;gt; 1 nn.MSELoss(y_pred, y) 2 frames /usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py in legacy_get_string(size_average, reduce, emit_warning) 33 reduce = True 34 ---&amp;gt; 35 if size_average and reduce: 36</description></item><item><title>파이토치에서 주어진 분포로 랜덤 샘플링하는 법</title><link>https://freshrimpsushi.github.io/ko/posts/3485/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3485/</guid><description>개요 파이토치에서 주어진 분포로 랜덤 샘플링하는 방법을 소개한다. 베타, 베르누이, 코시, 감마, 파레토, 푸아송 등 여러 분포가 구현되어있다. 본 글에서는 균등</description></item><item><title>딥러닝에서 레이어란?</title><link>https://freshrimpsushi.github.io/ko/posts/3484/</link><pubDate>Sat, 30 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3484/</guid><description>정의 딥러닝에서, 선형변환 $L^{mn} : \mathbb{R}^{n} \to \mathbb{R}^{m}$을 레이어layer, 층라 한다. 일반화 딥러닝에서, 고정된 $\mathbf{b} \in \mathbb{R}</description></item><item><title>합성곱 신경망</title><link>https://freshrimpsushi.github.io/ko/posts/3449/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3449/</guid><description>정의 합성곱층, 풀링층, 활성화함수 등을 적절하게 합성한 합성함수를 합성곱 신경망convolutional neural network, CNN이라 한다. 설명 합성곱층과 활성화함</description></item><item><title>딥러닝에서 풀링층이란?</title><link>https://freshrimpsushi.github.io/ko/posts/3448/</link><pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3448/</guid><description>개요 인공신경망에서 풀링층이란, 입력 데이터의 차원을 국소적인 단위로 줄이는 함수를 말한다. 지정된 영역에서 최댓값만을 남기는 것을 최대값풀링, 지정된 영역의</description></item><item><title>다층 퍼셉트론(MLP), 완전연결 신경망(FCNN)</title><link>https://freshrimpsushi.github.io/ko/posts/3447/</link><pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3447/</guid><description>정의 $L_{i} : \mathbb{R}^{n_{i}} \to \mathbb{R}^{n_{i+1}}$을 완전연결층이라 하자. $\sigma : \mathbb{R} \to \mathbb{R}$을 활성화함수라 하자. 이들의 합성을 다</description></item><item><title>Iris 데이터 셋</title><link>https://freshrimpsushi.github.io/ko/posts/3445/</link><pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3445/</guid><description>개요1 Iris 데이터 셋이란, 미국의 식물학자 에드가 앤더슨Edgar Anderson이 만들고 영국의 통계학자 로널드 피셔Ronald Fisher에 의해 소개</description></item><item><title>MNIST 데이터베이스</title><link>https://freshrimpsushi.github.io/ko/posts/3444/</link><pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3444/</guid><description>개요1 $\includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png}$ MNISTmodified national institute of standards and technology 데이터베이스란, 미국의 고등학생과 인구조사국직원의 숫자 손글씨에 대한 데이터 셋을 말한다. 흔히 [엠니스트]라 부른다. 공식 홈페이</description></item><item><title>줄리아의 여러가지 딥러닝 프레임워크</title><link>https://freshrimpsushi.github.io/ko/posts/3443/</link><pubDate>Sat, 08 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3443/</guid><description>개요 마지막 수정 날짜: 2022년 11월 22일 줄리아의 대표적인 딥러닝 프레임워크로는 Flux.jl이 있다. 이와 함께 또 다른 프레임워크인 Knet.jl</description></item><item><title>자동 미분</title><link>https://freshrimpsushi.github.io/ko/posts/3442/</link><pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3442/</guid><description>정의1 2 자동 미분automatic differentiation이란, 컴퓨터 프로그래밍 코드로 정의된 함수의 도함수를 구하는 방법을 말한다. AD, autodiff 등으</description></item><item><title>표현자 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/2408/</link><pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2408/</guid><description>정리 인풋 집합input set $X \ne \emptyset$ 과 양정부호 커널 $k: X \times X \to \mathbb{R}$ 이 주어져 있다고 하자. 학습데이터셋training dataset을 $$ D := \left\{ \left( x_{i} , y_{i} \right) \right\}_{i=1}^{m}</description></item><item><title>머신러닝에서 원-핫 인코딩이란?</title><link>https://freshrimpsushi.github.io/ko/posts/3438/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3438/</guid><description>정의 주어진 데이터 집합 $X \subset \mathbb{R}^{n}$의 부분집합 $X_{i}$들이 다음을 만족한다고 하자. $$ X = X_{1} \cup \cdots \cup X_{N} \quad \text{and} \quad X_{i} \cap X_{j} = \varnothing \enspace (i</description></item><item><title>머신러닝에서의 정부호 커널과 재생 커널 힐베르트 공간</title><link>https://freshrimpsushi.github.io/ko/posts/2406/</link><pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2406/</guid><description>정의 1 2 인풋 공간input space $X \ne \emptyset$ 이 정의역이고 공역이 복소수의 집합 $\mathbb{C}$ 인 사상 $f: X \to \mathbb{C}$ 들로 이루어진 함수들의 공간 $\left( H , \left&amp;lt; \cdot , \cdot \right&amp;gt; \right) \subset \mathbb{C}^{X}$ 가 힐베르트공간</description></item><item><title>줄리아에서 U-net 구현하기</title><link>https://freshrimpsushi.github.io/ko/posts/3434/</link><pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3434/</guid><description>개요 논문 &amp;quot;U-Net: Convolutional networks for Biomedical Image Segmentation&amp;quot;에서 소개된 U-Net을 줄리아로 구현하는 방법을 소개한다. 코드 U-Net의 구조는 크게 두 부</description></item><item><title>서포트 벡터 머신</title><link>https://freshrimpsushi.github.io/ko/posts/2402/</link><pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/2402/</guid><description>모델 1 쉬운 정의 이진분류binary classification 가능한 데이터를 가장 잘 구분하도록하는 직선이나 평면을 구하는 방법을 서포트 벡터 머신이라 한다. 어려운 정의 내적공간 $X</description></item><item><title>파이토치에서 모델/텐서가 올라간 디바이스 확인하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3364/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3364/</guid><description>코드1 2 get_device()로 확인할 수 있다. &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import torch.nn as nn &amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available() True &amp;gt;&amp;gt;&amp;gt; Device = torch.device(&amp;#34;cuda:0&amp;#34; if torch.cuda.is_available() else &amp;#34;cpu&amp;#34;) # Model &amp;gt;&amp;gt;&amp;gt; model = nn.Sequential(nn.Linear(5,10), nn.ReLU(), nn.Linear(10,10), nn.ReLU(), nn.Linear(10,1)) &amp;gt;&amp;gt;&amp;gt; next(model.parameters()).get_device() -1 &amp;gt;&amp;gt;&amp;gt; model.to(Device) Sequential( (0): Linear(in_features=5, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=10, bias=True) (3):</description></item><item><title>소프트플러스 함수란?</title><link>https://freshrimpsushi.github.io/ko/posts/3396/</link><pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3396/</guid><description>정의1 다음의 함수를 소프트플러스softplus라고 한다. $$ \zeta (x) = \ln (1 + e^{x}) $$ 설명 2001년에 Dugas 외 4명의 논문 『Incorporating Second-Order Functional Knowledge for</description></item><item><title>완전연결층(선형층, 밀집층)</title><link>https://freshrimpsushi.github.io/ko/posts/3384/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3384/</guid><description>정의 $L: \mathbb{R}^{n} \to \mathbb{R}^{m}$을 레이어라고 하자. $\mathbf{W}$를 $L$의 행렬표현이라 하자. $\mathbf{W}$가 $0</description></item><item><title>머신러닝에서 훈련/검증/테스트 집합이란?</title><link>https://freshrimpsushi.github.io/ko/posts/3382/</link><pubDate>Wed, 08 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3382/</guid><description>정의 훈련 중에 쓰이는 데이터 집합: 모델의 파라매터를 최적화하는데 쓰이는 데이터 집합을 트레이닝 셋training set, 훈련 집합이라 한다. 모델의 하이퍼 파라매</description></item><item><title>기계학습에서 인코더와 디코더</title><link>https://freshrimpsushi.github.io/ko/posts/3380/</link><pubDate>Sat, 04 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3380/</guid><description>정의 데이터 집합 $X \subset \mathbb{R}^{n}$이 주어졌다고 하자. 기계학습의 맥락에서 인코더encoder, 부호기란, 적당한 집합 $Z \subset \mathbb{R}^{m}$ ($m \le n$</description></item><item><title>기계학습에서 ReLU란?</title><link>https://freshrimpsushi.github.io/ko/posts/3362/</link><pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3362/</guid><description>정의 기계학습에서, 다음의 함수를 정류 선형 단위Rectified Linear Unit(ReLU), 렐루라 한다. $$ f(x) = x^{+} := \max \left\{ 0, x \right\} $$ 설명 전기전자공학에서는 이를 램프 함수ramp</description></item><item><title>딥러닝에서 인공신경망(ANN), 심층신경망(DNN), 순방향신경망(FNN)의 뜻과 차이점</title><link>https://freshrimpsushi.github.io/ko/posts/3446/</link><pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3446/</guid><description>개요 인공신경망, 심층신경망, 순방향신경망 등 딥러닝에서 사용되는 용어들에 대해서 정리한다. 이 용어들은 명확한 정의도 없고, 혼용되는 일도 잦아서 입문자가 헷</description></item><item><title>합성곱층</title><link>https://freshrimpsushi.github.io/ko/posts/3386/</link><pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3386/</guid><description>정의 $\mathbf{W}$를 $k \times k$ 행렬이라고 하자. $M^{n\times n} = M^{n\times n}(\mathbb{R})$을 크기가 $n \times n$인 실수행렬들의 집합이라고 하자. 합성</description></item><item><title>논문 리뷰: 물리정보기반 신경망(PINN)</title><link>https://freshrimpsushi.github.io/ko/posts/3313/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3313/</guid><description>개요 및 요약 레퍼런스, 수식의 번호, 표기법 등은 논문을 그대로 따른다. Physics-informed neural networks (PINN[핀]이라 읽는다)는 미분 방정식을 수치적으로 풀기 위해 고안된 인공신</description></item><item><title>머신러닝에서 선형회귀모델의 최소제곱법 학습</title><link>https://freshrimpsushi.github.io/ko/posts/3263/</link><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3263/</guid><description>개요1 선형회귀모델의 학습 방법 중 하나인 최소제곱법least squares을 이용한 방법을 소개한다. 설명 데이터 집합을 $X = \left\{ \mathbf{x}_{i} \right\}_{i=1}^{N}$, 레이블 집합을 $Y = \left\{ y_{i}</description></item><item><title>머신러닝에서 선형회귀모델의 경사하강법 학습</title><link>https://freshrimpsushi.github.io/ko/posts/3261/</link><pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3261/</guid><description>개요1 선형회귀모델의 학습 방법 중 하나인 경사하강법gradient descent을 이용한 방법을 소개한다. 설명 데이터 셋을 $X = \left\{ \mathbf{x}_{i} \right\}_{i=1}^{N}$, 레이블 셋을 $Y = \left\{</description></item><item><title>계층적 군집화</title><link>https://freshrimpsushi.github.io/ko/posts/3253/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3253/</guid><description>알고리즘 Input $p$ 차원의 데이터 $N$개와 거리 $d$가 주어져있다고 하자. Step 1. 각각의 점을 하나의 군집으로 생각한다. 가장 가까운 두 군집을 하나의 군집으로 묶는다</description></item><item><title>줄리아 플럭스에서 MLP 구현해서 비선형함수 근사하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3227/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3227/</guid><description>시작 using Flux using Plots using Distributions using Flux: @epochs function f(x) if -5 ≤ x &amp;lt; -2 y = -3x + 2 elseif -2 ≤ x &amp;lt; 1 y = x + 10 elseif 1 ≤ x &amp;lt; 2 y = -2x + 13 elseif 2 ≤ x &amp;lt; 3 y = 4x + 1 elseif 3 ≤ x ≤ 5 y = -4x + 25 end return</description></item><item><title>파이토치에서 리스트에 대한 타입 에러 'TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.' 해결법</title><link>https://freshrimpsushi.github.io/ko/posts/3225/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3225/</guid><description>에러 TypeError: can&amp;#39;t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first. 분명히 파이토치 텐서나 넘파이 배열이 아니라 리스트를 다루고 있음에도 불구하고 위와 같은 에러가 나올 수 있다. 시키</description></item><item><title>줄리아 플럭스에서 MLP 구현하고 MNIST 학습하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3221/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3221/</guid><description>MNIST 데이터 셋 불러오기 오래된 예제의 경우 이 부분에서 Flux.Data를 사용하는 코드를 볼 수 있는데, 이제는 플럭스에서 지원하지 않는다. julia&amp;gt; Flux.Data.MNIST.images() ┌ Warning: Flux&amp;#39;s datasets are deprecated, please</description></item><item><title>줄리아 플럭스에서 원-핫 인코딩하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3219/</link><pubDate>Thu, 14 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3219/</guid><description>개요 원-핫 인코딩이란, 데이터를 분류class에 따라 표준기저벡터 로 매핑하는 것을 말한다. Flux에서는 이를 위한 함수를 제공한다. 코드1 onehot() onehot(x, labels, [default]) x .==</description></item><item><title>줄리아 플럭스에서 MLP 구현하고 경사하강법으로 최적화하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3211/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3211/</guid><description>MLP 구현 우선 줄리아의 머신러닝 패키지인 Flux.jl와 옵티마이저 업데이트 메소드 update!를 불러오자. using Flux using Flux: update! Dense() 함수로 선형층을 쓸 수 있다. Chain() 함</description></item><item><title>줄리아 플럭스에서 은닉층 다루는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3209/</link><pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3209/</guid><description>선형1 플럭스에서 선형층은 Dense()로 구현할 수 있다. Dense(in, out, σ=identity; bias=true, init=glorot_uniform) Dense(W::AbstractMatrix, [bias, σ] 활성화함수에 대한 기본값은 항등함수이다. relu나</description></item><item><title>파이토치에서 텐서의 차원, 크기 다루기</title><link>https://freshrimpsushi.github.io/ko/posts/3205/</link><pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3205/</guid><description>정의 $A$를 파이토치 텐서라고 하자. 다음과 같은 순서쌍 $(a_{0}, a_{1}, \dots, a_{n-1})$을 $A$의 사이즈라고 한다. $$ \text{A.size() = torch.Size}([a_{0}, a_{1}, \dots, a_{n-1} ]) $$ $\prod \limits_{i=0}^{n-1} a_{i} = a_{0} \times a_{1} \times \cdots a_</description></item><item><title>파이토치에서 텐서 패딩하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3199/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3199/</guid><description>코드 1 torch.nn.functional.pad(input, pad, mode='constant', value=0.0) input: 패딩할 텐서 pad: 패딩할 위치 mode: 패딩할 방법 mode: 패딩할 값 설명 pad $n$차원 텐서를 input으로 쓸 때, 최대 $2n-$순서쌍을 인자를 입력할 수 있</description></item><item><title>줄리아에서 머신러닝 데이터 셋 사용하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3191/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3191/</guid><description>설명 MLDatasets.jl1 2 패키지로 아래와 같은 데이터 셋을 사용할 수 있다. 링크가 있는 데이터셋은 각 문서에서 사용법을 설명한다. Vision CIFAR10 CIFAR100 EMNIST FashionMNIST MNIST Omniglot SVHN2 convert2image Mesh FAUST Miscellaneous BostonHousing Iris Mutagenesis Titanic Text PTBLM SMSSpamCollection UD_English Graphs</description></item><item><title>파이토치에서 텐서 붙이거나 쌓는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3187/</link><pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3187/</guid><description>텐서 붙이기 cat()1 cat(tensors, dim=0)는 2개 이상의 텐서들을 지정한 차원을 기준으로 붙인다. 기준이 된다는 말은 지정한 차원의 크기가 늘어나도록 붙인다는 뜻이다. 따라서</description></item><item><title>파이토치에서 모델의 가중치 값을 얻는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3183/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3183/</guid><description>설명 다음과 같은 모델을 정의하자. import torch import torch.nn as nn class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.linear = nn.Linear(3, 3, bias=True) self.conv = nn.Conv2d(3, 5, 2) f = Model() 그러면 .weight와 .bias 메소드로 각 층의 가중치와 바이어스에 접근</description></item><item><title>파이토치에서 텐서 깊은 복사하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3177/</link><pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3177/</guid><description>설명 파이토치 텐서도 다른 객체와 마찬가지로 copy.deepcopy()를 이용해 깊은 복사를 할 수 있다. &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import copy &amp;gt;&amp;gt;&amp;gt; a = torch.ones(2,2) &amp;gt;&amp;gt;&amp;gt; b = a &amp;gt;&amp;gt;&amp;gt; c = copy.deepcopy(a) &amp;gt;&amp;gt;&amp;gt; a += 1</description></item><item><title>파이토치에서 리스트와 반복문으로 인공 신경망 레이어 정의하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3173/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3173/</guid><description>설명 쌓아야할 층이 많거나, 신경망 구조를 자주 바꿔야하는 등의 이유로 인공 신경망의 정의를 자동화하고 싶은 경우가 있다. 이럴 때 다음과 같은 for문으로 정의하고</description></item><item><title>논문 리뷰: Neural Ordinary Differential Equations</title><link>https://freshrimpsushi.github.io/ko/posts/3159/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3159/</guid><description>개요 및 요약 「Neural Ordinary Differential Equations」는 Ricky T. Q. Chen 외 3명이 2018년에 발표한 논문으로, 2018 NeurIPS Best Papers에 선정되었다. 다음과 같이 간단</description></item><item><title>파이토치에서 랜덤 순열 만들고 텐서 순서 섞는 법</title><link>https://freshrimpsushi.github.io/ko/posts/3140/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3140/</guid><description>torch.randperm()1 torch.randperm(n): 은 0부터 n-1개의 랜덤한 정수 순열을 리턴한다. 당연하게도 정수형이 아니면 입력으로 쓸 수 없다. &amp;gt;&amp;gt;&amp;gt; torch.randperm(4) tensor([2, 1, 0, 3]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(8) tensor([4, 0, 1, 3, 2, 5, 6, 7]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(16) tensor([12, 5, 6, 3, 15, 13, 2,</description></item><item><title>파이토치에서 가중치, 모델, 옵티마이저 저장하고 불러오는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3114/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3114/</guid><description>재학습하지 않는 경우1 2 3 저장하기 재학습하지 않는 경우라면 간단하게 가중치 혹은 모델만 저장해도 된다. 아래에서 말하겠지만 재학습을 할거라면 옵티마이저까지</description></item><item><title>파이토치에서 Numpy 배열로 커스텀 데이터 셋 만들고 사용하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3108/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3108/</guid><description>설명 &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; from torch.utils.data import TensorDataset, DataLoader $32\times 32$의 크기로 된 &amp;lsquo;흑백&amp;rsquo;사진 100장을 쌓아올린 numpy 배열 $X$와 이에 대한 레이블 $Y$가</description></item><item><title>파이토치에서 가중치 초기화 하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3104/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3104/</guid><description>코드1 다음과 같이 뉴럴 네트워크를 정의했다고 하자. forward 부분은 생략하였다. import torch import torch.nn as nn class Custom_Net(nn.Module): def __init__(self): super(Custom_Net, self).__init__() self.linear_1 = nn.Linear(1024, 1024, bias=False) self.linear_2 = nn.Linear(1024, 512, bias=False) self.linear_3 = nn.Linear(512, 10, bias=True) torch.nn.init.constant_(self.linear_1.weight.data, 0) torch.nn.init.unifiom_(self.linear_2.weight.data) torch.nn.init.xavier_normal_(self.linear_3.weight.data) torch.nn.init.xavier_normal_(self.linear_3.bias.data) def forward(self, x): ... 가</description></item><item><title>파이토치에서 MLP 구현하는 방법</title><link>https://freshrimpsushi.github.io/ko/posts/3103/</link><pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3103/</guid><description>라이브러리 import torch import torch.nn as nn import torch.nn.functional as F nn와 nn.functional에는 뉴럴 네트워크를 구성하는 여러가지 레이어, 손실함수, 활성화 함수 등이 포함되어있다</description></item><item><title>파이토치 RuntimeError: grad can be implicitly created only for scalar outputs 해결법</title><link>https://freshrimpsushi.github.io/ko/posts/3095/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3095/</guid><description>사례1 만약 손실함수를 loss = sum(a,b)와 같이 뒀다면 loss.backward()에서 백프로파게이션할 때 해당 오류가 날 수 있다. 이때 loss = torc</description></item><item><title>역 전파 알고리즘</title><link>https://freshrimpsushi.github.io/ko/posts/3077/</link><pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3077/</guid><description>이 글은 역전파 알고리즘의 원리를 수학 전공자가 이해하기 쉽도록 작성되었다. 표기법 위 그림과 같은 인공 신경망이 주어졌다고 하자. $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n_{0}}</description></item><item><title>퍼셉트론 수렴 정리</title><link>https://freshrimpsushi.github.io/ko/posts/3023/</link><pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3023/</guid><description>$X^{+}$, $X^{-}$가 선형 분리 가능한 트레이닝 셋이라고 하자. $y$를 다음과 같은 레이블이라고 하자. $$ y_{i} = \pm 1\ (\mathbf{x}_{i} \in X^{\pm}) $$ 전체 트레이닝 셋 $X = X^{+} \cup X^{-}</description></item><item><title>머신러닝에서 강화학습이란</title><link>https://freshrimpsushi.github.io/ko/posts/3029/</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/3029/</guid><description>정의 강화학습이란, 에이전트가 환경과 상호작용하여 누적 보상을 최대화하는 정책을 찾을 수 있도록 하는 것이다. 설명1 강화학습을 이루고 있는 요소들은 다음과 같다</description></item><item><title>딥러닝의 수학적 근거, 시벤코 정리 증명</title><link>https://freshrimpsushi.github.io/ko/posts/1853/</link><pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1853/</guid><description>정리 $\sigma$ 가 연속 시그모이달 함수라고 하면 $$ S := \left\{ G(x) = \sum_{k=1}^{N} \alpha_{k} \sigma \left( y_{k}^{T} x+ \theta_{k} \right) : y_{k} \in \mathbb{R}^{n} \land \alpha_{k} , \theta_{k} \in \mathbb{R} \land N \in \mathbb{N} \right\} $$ 는 $C\left( I_{n} \right)$ 에서 균등 조밀하다. 달리 말하자면, 모든 $f</description></item><item><title>시그모이달 함수란?</title><link>https://freshrimpsushi.github.io/ko/posts/1851/</link><pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1851/</guid><description>정의 다음을 만족하는 함수 $\sigma : \mathbb{R} \to \mathbb{R}$ 을 시그모이달 함수sigmoidal function라 한다. $$ \sigma (t) \to \begin{cases} 1 &amp;amp; \text{as } t \to + \infty \\ 0 &amp;amp; \text{as } t \to - \infty \end{cases} $$ 정</description></item><item><title>차별 함수란?</title><link>https://freshrimpsushi.github.io/ko/posts/1838/</link><pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1838/</guid><description>정의 모든 $y \in \mathbb{R}^{n}$ 과 $\theta \in \mathbb{R}$ 와 어떤 $\mu \in M \left( I_{n} \right)$ 에 대해 $$ \int_{I_{n}} \sigma \left( y^{T} x + \theta \right) d \mu (x) = 0 \implies \mu =0 $$ 를 만족하는 함수 $\sigma : \mathbb{R} \to \mathbb{R}$ 를 차별적 함수discriminat</description></item><item><title>머신 러닝에서 회귀를 위한 선형 모델</title><link>https://freshrimpsushi.github.io/ko/posts/1887/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1887/</guid><description>정의1 단순 모델 데이터 집합 $X = \left\{ \mathbf{x}_{i} \right\}$와 레이블 집합 $Y = \left\{ y_{i} \right\}$ 사이의 타겟 함수target function $f : X \to Y$를 다음과 같이 정의하자. $$ y_{i} =</description></item><item><title>로지스틱 함수란?</title><link>https://freshrimpsushi.github.io/ko/posts/1775/</link><pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1775/</guid><description>정의 1 로지스틱 함수란 미분 방정식의 해 $y ' = y(1-y)$ 로써, 다음과 같이 구해진다. $$ y(t) = {{ 1 } \over { 1 + e^{-t} }} $$ 설명 조금 더 일반적인 형태로써 $\displaystyle f(x) := {{ L } \over { 1 +</description></item><item><title>시그모이드 함수란?</title><link>https://freshrimpsushi.github.io/ko/posts/1769/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1769/</guid><description>정의 1 유계 미분가능 스칼라 함수 $\sigma : \mathbb{R} \to \mathbb{R}$ 이 모든 $x \in \mathbb{R}$ 에서 $\sigma ' (x) \ge 0$ 이고 단 하나의 변곡점을 가지면 시그모이드 함수sigmoid function라고 한</description></item><item><title>퍼셉트론의 정의</title><link>https://freshrimpsushi.github.io/ko/posts/1846/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1846/</guid><description>정의 선형 함수 $f(x) = wx + b$와 단위 계단 함수 $H$의 합성을 퍼셉트론perceptron이라 정의한다. $$ \text{Perceptron} := H \circ f (x) = H(wx + b) $$ 다변수 함수의 경우, $f(\mathbf{x}) =</description></item><item><title>컴퓨터 비전이란</title><link>https://freshrimpsushi.github.io/ko/posts/1839/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1839/</guid><description>설명 컴퓨터 비전 이란 주로 사람의 시각에 해당하는 기능을 컴퓨터가 수행할 수 있도록 하는 연구 분야이며 이미지나 영상을 다룬다. 컴퓨터 비전을 전문적으로 다루는 컨퍼</description></item><item><title>딥러닝에서 연속 학습이란</title><link>https://freshrimpsushi.github.io/ko/posts/1837/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1837/</guid><description>설명 딥러닝에서 연속 학습이란 평생 학습, 점진적 학습과 같은 말로서 인공 신경망이 순차적으로 여러 작업을 학습하는 것을 말한다. 인간의 경우 새로운 지식을 학습한다</description></item><item><title>논문 리뷰: Do We Need Zero Training Loss After Achieving Zero Training Error?</title><link>https://freshrimpsushi.github.io/ko/posts/1809/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1809/</guid><description>논문 리뷰 플루딩flooding은 ICML 2020에서 발표된 Do We Need Zero Training Loss After Achieving Zero Training Error?에서 소개된 레귤라이제이션 기법을 말한다. 이 논문의 저자는 오버</description></item><item><title>머신러닝에서 많이 쓰이는 데이터 셋</title><link>https://freshrimpsushi.github.io/ko/posts/1808/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1808/</guid><description>컴퓨터 비전 MNIST 머신 러닝을 공부할 때 가장 먼저 접할 데이터 셋이다. [엠니스트]라고 읽으며 $28\times 28$ 크기의 손글씨 사진 데이터이다. 학습 데이터 60,000개, 테스</description></item><item><title>머신러닝에서 오버피팅과 레귤러라이제이션이란?</title><link>https://freshrimpsushi.github.io/ko/posts/1807/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1807/</guid><description>오버피팅 트레이닝 로스는 줄어들지만, 테스트 로스(밸리데이션 로스)가 줄어들지 않거나 오히려 증가하는 현상을 오버피팅over fitting, 과적합이라 한다. 설명 이와</description></item><item><title>k-평균 군집화</title><link>https://freshrimpsushi.github.io/ko/posts/1365/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1365/</guid><description>알고리즘 Input $p$차원의 데이터 $N$ 개와 자연수 $k$ 가 주어져있다고 하자. Step 1. 초기화 $k$ 개의 점 $\mu_{1} , \cdots , \mu_{k}$ 을 랜덤하게 정한다. 각각의 $\mu_{j}$ 는 군집 $M_{j}$ 의 평균이 될 것이다</description></item><item><title>지도학습과 비지도학습</title><link>https://freshrimpsushi.github.io/ko/posts/1013/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1013/</guid><description>정의 머신러닝에서 종속변수가 정해진 경우를 지도학습, 그렇지 않은 경우를 비지도학습이라고 한다. 예시 지도학습과 비지도학습의 차이는 쉽게 비유하자면 객관식과</description></item><item><title>딥러닝에서의 드롭아웃</title><link>https://freshrimpsushi.github.io/ko/posts/1004/</link><pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/1004/</guid><description>정의 드롭아웃dropout이란 인공 신경망의 뉴런을 확률적으로 사용하지 않음으로써 과적합을 방지하는 기법이다. 설명 언뜻 생각하면 그냥 학습을 덜 하는 것이고</description></item><item><title>딥러닝에서의 소프트맥스 함수</title><link>https://freshrimpsushi.github.io/ko/posts/993/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/993/</guid><description>정의 $\mathbf{x} := (x_{1} , \cdots , x_{n}) \in \mathbb{R}^{n}$ 이라고 하자. $\displaystyle \sigma_{j} ( \mathbf{x} ) = {{ e^{x_{j}} } \over {\sum_{i=1}^{n} e^{x_{i}} }}$ 에 대해 $\sigma ( \mathbf{x} ) := \left( \sigma_{1} (\mathbf{x}) , \cdots , \sigma_{n} (\mathbf{x} ) \right)$ 와 같이 정의된 $\sigma : \mathbb{R}^{n} \to (0,1)^{n}$ 을 소프트맥스softm</description></item><item><title>딥러닝에서의 활성화 함수</title><link>https://freshrimpsushi.github.io/ko/posts/991/</link><pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/991/</guid><description>정의 실제 생물의 역치를 모방한 비선형 함수를 활성화 함수activation function라 한다. 수학적 정의 딥러닝에서 비선형 스칼라 함수 $\sigma : \mathbb{R} \to \m</description></item><item><title>딥러닝이란?</title><link>https://freshrimpsushi.github.io/ko/posts/996/</link><pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/996/</guid><description>정의 딥러닝은 인공 신경망을 이용한 머신러닝의 일종으로, 특히 인공 신경망을 구성할 때 복수의 레이어를 사용하는 기법을 말한다. 모티브 인간의 두뇌가 뉴런들의 복잡</description></item><item><title>머신러닝에서의 경사하강법, 확률적경사하강법</title><link>https://freshrimpsushi.github.io/ko/posts/987/</link><pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/987/</guid><description>개요 손실 함수의 기울기를 이용해 손실 함수의 극소값을 찾는 알고리즘 중 가장 간단한 방법으로 경사하강법gradient Descent algorithm이 있다. 설명 단,</description></item><item><title>머신러닝에서의 손실 함수</title><link>https://freshrimpsushi.github.io/ko/posts/967/</link><pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/967/</guid><description>정의 데이터 $Y = \begin{bmatrix} y_{1} \\ \vdots \\ y_{n} \end{bmatrix}$ 에 대한 추정치가 $\widehat{Y} = \begin{bmatrix} \widehat{ y_{1} } \\ \vdots \\ \widehat{y_{n}} \end{bmatrix}$ 와 같이 주어져 있을 때 데이터와 추정치의 괴리도를 나태는 스칼라 함수 $L : \mathbb{R}^{n} \to [ 0 , \infty )$</description></item><item><title>인공 신경망이란?</title><link>https://freshrimpsushi.github.io/ko/posts/962/</link><pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/ko/posts/962/</guid><description>정의 실제 생물의 신경계를 모방한 네트워크를 인공 신경망artificial neural network (ANN) 이라 한다. 수학적 정의 스칼라 함수 $\sigma : \mathbb{R} \to \mathbb{R}$에 대해</description></item></channel></rss>