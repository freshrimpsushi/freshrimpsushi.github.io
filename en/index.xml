<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/</link><description>Recent content on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 28 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/index.xml" rel="self" type="application/rss+xml"/><item><title>Example of Using Cookies in JavaScript to Maintain Page State</title><link>https://freshrimpsushi.github.io/en/posts/2717/</link><pubDate>Sun, 28 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2717/</guid><description>Showcase A B C D E F G H I J The checkboxes on this page retain their state even after refreshing the page or navigating to another page. For most web pages, this feature is not merely useful but can be considered almost essential. Code HTML &amp;lt;div align=&amp;#34;center&amp;#34; style=&amp;#34;user-select: none;&amp;#34;&amp;gt; &amp;lt;input type=&amp;#34;checkbox&amp;#34; id=&amp;#34;A&amp;#34;&amp;gt;&amp;lt;label for=&amp;#34;A&amp;#34;&amp;gt;A&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;#34;checkbox&amp;#34; id=&amp;#34;B&amp;#34;&amp;gt;&amp;lt;label for=&amp;#34;B&amp;#34;&amp;gt;B&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;#34;checkbox&amp;#34; id=&amp;#34;C&amp;#34;&amp;gt;&amp;lt;label for=&amp;#34;C&amp;#34;&amp;gt;C&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;#34;checkbox&amp;#34; id=&amp;#34;D&amp;#34;&amp;gt;&amp;lt;label for=&amp;#34;D&amp;#34;&amp;gt;D&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;#34;checkbox&amp;#34; id=&amp;#34;E&amp;#34;&amp;gt;&amp;lt;label</description></item><item><title>Example of Implementing Multiple Image Toggle Functionality in JavaScript</title><link>https://freshrimpsushi.github.io/en/posts/2716/</link><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2716/</guid><description>Showcase A B C D A toggle refers to a feature that can switch between two or more states or options1. In the example above, pressing the buttons changes the image as follows: A: Pikachu B: Bulbasaur C: Charmander D: Squirtle Code HTML &amp;lt;button onclick=&amp;#34;toggle_A()&amp;#34;&amp;gt;A&amp;lt;/button&amp;gt; &amp;lt;button onclick=&amp;#34;toggle_B()&amp;#34;&amp;gt;B&amp;lt;/button&amp;gt; &amp;lt;button onclick=&amp;#34;toggle_C()&amp;#34;&amp;gt;C&amp;lt;/button&amp;gt; &amp;lt;button onclick=&amp;#34;toggle_D()&amp;#34;&amp;gt;D&amp;lt;/button&amp;gt; &amp;lt;img id=&amp;#34;img&amp;#34; src=&amp;#34;A.webp&amp;#34; width=&amp;#34;280px&amp;#34; frameborder=&amp;#34;0&amp;#34;&amp;gt;&amp;lt;/img&amp;gt; button onclick=&amp;quot;toggle_*()&amp;quot;: Specifies the JavaScript function that will respond to the button. img id=&amp;quot;img&amp;quot;</description></item><item><title>Reading and Writing JSON Files in Julia (JSON3.jl)</title><link>https://freshrimpsushi.github.io/en/posts/3709/</link><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3709/</guid><description>Overview Packages that facilitate handling JSON files in Julia include JSON.jl, JSON3.jl, and Serde.jl. This document provides an explanation of JSON3.jl. JSON3.jl is a package written purely in Julia, offering functionality to parse and output JSON files. Unlike JSON.jl, it is not grouped under JuliaIO on GitHub, but is managed in a personal repository. JSON3.jl GitHub Repository Official Documentation Comprehensive Guide to File I/O in Julia Code Writing Saving a</description></item><item><title>JavaScript Example for Selecting, Deselecting, and Toggling a Series of Checkboxes with Shift Click</title><link>https://freshrimpsushi.github.io/en/posts/2715/</link><pubDate>Wed, 24 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2715/</guid><description>Showcase A B C D E F G H I J When you select one of the checkboxes above, hold Shift and click another checkbox; all checkboxes between the two will be toggled (checked ↔ unchecked). This feature is useful when dealing with a large number of checkboxes, and is particularly handy in contexts such as lists of posts or file listings where items do not immediately appear as checkboxes.</description></item><item><title>How to Read and Write JSON Files in Julia (JSON.jl)</title><link>https://freshrimpsushi.github.io/en/posts/3708/</link><pubDate>Tue, 23 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3708/</guid><description>Overview Packages that help handle JSON files in Julia include JSON.jl, JSON3.jl, and Serde.jl. This document will explain JSON.jl. JSON.jl is a package written in pure Julia that provides functionality for parsing and outputting JSON files. Although it is not part of Julia&amp;rsquo;s standard library, since the package is grouped under the JuliaIO group on GitHub and managed by a Julia developer, it can be considered almost an official package.</description></item><item><title>Example of Selecting, Deselecting, and Inverting All Checkboxes in JavaScript</title><link>https://freshrimpsushi.github.io/en/posts/2714/</link><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2714/</guid><description>Showcase Æ ∅ ∀ ¬ A B C D E F G H I J The checkboxes above can be toggled individually, but four special buttons provide the following functions. This kind of implementation is especially useful when dealing with a large number of checkboxes. Æ: Select only the A, B, C, D, E checkboxes ∅: Deselect all checkboxes ∀: Select all checkboxes ¬: Invert all</description></item><item><title>How to Send Server-Unrelated Values from an HTML Form</title><link>https://freshrimpsushi.github.io/en/posts/2713/</link><pubDate>Sat, 20 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2713/</guid><description>Showcase If you enter &amp;ldquo;your message&amp;rdquo; in the form above and submit it, then check the address bar, you&amp;rsquo;ll see that &amp;ldquo;subject=statistics&amp;rdquo; is included irrespective of the value you entered. This is useful when the same form is used in different contexts (i.e., the page&amp;rsquo;s purpose differs) or when there is a parameter the server must know but the user does not need to be aware of. Code HTML &amp;lt;form</description></item><item><title>Comprehension Syntax in Python</title><link>https://freshrimpsushi.github.io/en/posts/3706/</link><pubDate>Fri, 19 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3706/</guid><description>Description When using a for loop to create an object, the syntax where the loop itself is written within the object is called a comprehension. It&amp;rsquo;s a method with good readability, and Python supports list, set, and dictionary comprehensions. Rather than a long explanation, let&amp;rsquo;s look at the examples below. Code List The most basic way to create a list of squares is as follows. &amp;gt;&amp;gt;&amp;gt; squares = [] &amp;gt;&amp;gt;&amp;gt;</description></item><item><title>How to Display a Tooltip on Mouse Hover Using CSS</title><link>https://freshrimpsushi.github.io/en/posts/2712/</link><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2712/</guid><description>Showcase mouse hover If you hover the mouse over &amp;lsquo;mouse hover&amp;rsquo;, a tooltip appears. Code HTML &amp;lt;div class=&amp;#34;container&amp;#34; tooltip=&amp;#34;This is tooltip&amp;#34;&amp;gt;mouse hover&amp;lt;/div&amp;gt; class=&amp;quot;container&amp;quot;: Acts as the container wrapping the element that displays the tooltip. tooltip: Specifies the content to be displayed in the tooltip. CSS The following CSS is written inline as in the HTML code for simple testing. In practice, it is preferable to place it in a separate</description></item><item><title>Crystal Structure</title><link>https://freshrimpsushi.github.io/en/posts/3705/</link><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3705/</guid><description>Definition An arrangement of atoms, molecules, ions, etc., that periodically repeats in a three-dimensional space is called a crystal. Description1 A crystal structure refers to the basic structural unit of solid materials, where atoms or molecules are arranged in a regular pattern. This structure greatly influences the physical and chemical properties of the material. For instance, in the illustration below, on the left, carbon atoms $\ce{C}$ are repeated in a</description></item><item><title>How to View Folder Tree Structures in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3704/</link><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3704/</guid><description>Code A function to output the folder tree structure is not provided by default. You can use the FileTree(path) function provided by the FileTrees.jl package. Environment OS: Windows11 Version: Julia 1.11.3, FileTrees v0.3.10</description></item><item><title>How to Solve the Error: externally-managed-environment When Installing Python Packages</title><link>https://freshrimpsushi.github.io/en/posts/2710/</link><pubDate>Sun, 14 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2710/</guid><description>Issue error: externally-managed-environment is an error that mainly occurs in Python environments installed by the system package manager on Linux-based systems. It happens because installation of packages from external sources is restricted for security reasons. Solution sudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED If the installed Python version is 3.11, you can remove the EXTERNALLY-MANAGED file as shown above. Alternative python3 -m venv proj source proj/bin/activate However, in any case this error is raised</description></item><item><title>How to Browse Files and Paths in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3703/</link><pubDate>Sat, 13 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3703/</guid><description>Code The function used to traverse a folder and obtain a list of subfolders and files is walkdir(path). Suppose a folder structure is as follows. A/ ├─ B/ │ ├─ BB/ │ │ └─ bbb.py │ ├─ b.py │ └─ bb.csv ├─ C/ │ ├─ CC/ │ │ └─ c.txt │ └─ c.jl ├─ a.txt └─ aa.py With the</description></item><item><title>How to Force Empty the Trash on Linux</title><link>https://freshrimpsushi.github.io/en/posts/2709/</link><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2709/</guid><description>Issue On Linux, when attempting to empty the Trash because of insufficient storage capacity, you may be unable to permanently delete items, receiving the message &amp;lsquo;Failed to delete the item from the trash&amp;rsquo;. Solution You can force-delete with the following command. 1 sudo rm -rf ~/.local/share/Trash/files/ https://forums.linuxmint.com/viewtopic.php?p=2169086&amp;amp;sid=171cda4fb134c3ed7bbce5f0bc442046#p2169086&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Meter</title><link>https://freshrimpsushi.github.io/en/posts/3702/</link><pubDate>Thu, 11 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3702/</guid><description>Definition1 The distance that light travels in $\dfrac{1}{299\ 792\ 458}$ seconds in a vacuum is defined as $1$ meters, and it is denoted by $\text{m}$. Explanation Conversely, the speed of light $c$ is $299\ 792\ 458$ meters per second. This is an exact figure, not an approximation. $$ c = 299\ 792\ 458\ \text{m/s} $$ It is the most popular unit of length and one of the SI base units.</description></item><item><title>Angstrom</title><link>https://freshrimpsushi.github.io/en/posts/3701/</link><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3701/</guid><description>Definition Ångström is one of the units of length and represents $10^{-10}$ meters. The symbol $\text{\AA}$ is used for it. $$ 1\ \text{\AA} = 10^{-10}\ \text{m} = 10^{-8}\ \text{cm} = 10^{-7}\ \text{mm} = 10^{-4}\ \mu\text{m} $$ Description It is named after the Swedish physicist Anders Ångström. It is mainly used to denote distances (lengths) in</description></item><item><title>2025 Summer Omakase: Imaginary Numbers</title><link>https://freshrimpsushi.github.io/en/posts/2707/</link><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2707/</guid><description>Introduction We call the solution $i$ of the quadratic equation $x^{2} + 1 = 0$ an imaginary number in the sense of &amp;ldquo;a number in the imagination.&amp;rdquo; However, even before irrational numbers were accepted, numbers were essentially imagined entities. This season, we&amp;rsquo;ll lightly explore the world of complex numbers. Menu We&amp;rsquo;ve prepared the content to be deeper than a standard curriculum yet not as deep as undergraduate complex analysis. Geometry</description></item><item><title>How to Hide Only the Box in a Checkbox Using CSS</title><link>https://freshrimpsushi.github.io/en/posts/2706/</link><pubDate>Sat, 06 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2706/</guid><description>Showcase R G B Recently, in search or filtering settings, it&amp;rsquo;s common to indicate ON/OFF by changing only the background without showing the checkbox box. The R, G, B above are checkboxes implemented in that way. Code HTML &amp;lt;input type=&amp;#34;checkbox&amp;#34; id=&amp;#34;box_R&amp;#34; hidden checked&amp;gt; &amp;lt;label for=&amp;#34;box_R&amp;#34; class=&amp;#34;option&amp;#34;&amp;gt;R&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;#34;checkbox&amp;#34; id=&amp;#34;box_G&amp;#34; hidden checked&amp;gt; &amp;lt;label for=&amp;#34;box_G&amp;#34; class=&amp;#34;option&amp;#34;&amp;gt;G&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;#34;checkbox&amp;#34; id=&amp;#34;box_B&amp;#34; hidden&amp;gt; &amp;lt;label for=&amp;#34;box_B&amp;#34; class=&amp;#34;option&amp;#34;&amp;gt;B&amp;lt;/label&amp;gt; hidden: Hides the checkbox box. If you just remove</description></item><item><title>Summary of PBS Job Commands</title><link>https://freshrimpsushi.github.io/en/posts/3699/</link><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3699/</guid><description>Overview PBS stands for Portable Batch System, which is software used to manage jobs on large computer clusters. PBS supports job scheduling, resource management, and job monitoring, enabling users to perform tasks efficiently on a cluster. Job Script A job script is a file defining the job to be submitted to PBS. Note that comments start with #, which is different from #! or #PBS. #!/bin/sh: Specifies the interpreter for</description></item><item><title>Shortcut to Save As in Excel, PowerPoint, Word</title><link>https://freshrimpsushi.github.io/en/posts/2705/</link><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2705/</guid><description>Overview Although I don&amp;rsquo;t know the exact reason, in the MS Office suite the commonly used &amp;lsquo;Save As&amp;rsquo; feature was removed and replaced with a &amp;lsquo;Save a Copy&amp;rsquo; feature. Considering compatibility with OneDrive, it&amp;rsquo;s not entirely unintelligible, but there are still many cases where &amp;lsquo;Save As&amp;rsquo; is more convenient. Guide excel2.mp4 You can use the &amp;lsquo;Save As&amp;rsquo; functionality like in other programs by pressing the F12 key. This key behaves</description></item><item><title>Volume Formula of a Parallelepiped</title><link>https://freshrimpsushi.github.io/en/posts/3698/</link><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3698/</guid><description>Formula The volume of a parallelepiped $V$ can be determined as follows: [1]: When the area of the base $A$ and the height $h$ are known, it is given by: $$ V = A \times h $$ [2]: When you know the three vectors $\mathbf{a}$, $\mathbf{b}$, $\mathbf{c}$ that form the parallelepiped in a coordinate space, it is given by: $$ V = |\mathbf{a} \cdot (\mathbf{b} \times \mathbf{c})| $$ Alternatively, it</description></item><item><title>Shortcut to Enter Current Date and Time in Excel</title><link>https://freshrimpsushi.github.io/en/posts/2704/</link><pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2704/</guid><description>Overview Introduces keyboard shortcuts that insert the current date and time directly into a cell in Excel. Particularly useful when you need to manually record logs. Guide excel1.mp4 ctrl;: Inserts the current date into the selected cell. ctrl:: Inserts the current time into the selected cell. Note that ctrl: is equivalent to ctrlshift;. Notepad tip As a side note, in Notepad, pressing the F5 key inserts the current date and</description></item><item><title>Trapezoid</title><link>https://freshrimpsushi.github.io/en/posts/3697/</link><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3697/</guid><description>Definition A quadrilateral with one pair of opposite sides parallel to each other is called a trapezoid. Explanation According to the definition, if it&amp;rsquo;s a parallelogram, then it&amp;rsquo;s a trapezoid. Square $\implies$ Rectangle $\implies$ $\implies$ Rhombus $\implies$ Parallelogram $\implies$ Trapezoid</description></item><item><title>How to Reference Environment Variables in Windows</title><link>https://freshrimpsushi.github.io/en/posts/2703/</link><pubDate>Sun, 31 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2703/</guid><description>Guide ${env:변수명}을 통해 &amp;lsquo;변수명&amp;rsquo;이라는 이름을 가진 환경변수를 참조할 수 있다. 예를 들어, 윈도우즈에서 사용자 이름을 가져</description></item><item><title>Parallelepiped</title><link>https://freshrimpsushi.github.io/en/posts/3696/</link><pubDate>Sat, 30 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3696/</guid><description>Definition Simple Definition A hexahedron in which all faces are parallelograms is called a parallelepiped. Linear Algebraic Definition For three distinct vectors $\mathbf{a}$, $\mathbf{b}$, $\mathbf{c}$ in a 3-dimensional coordinate space, the following set is called a parallelepiped. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} + \lambda_{3} \mathbf{c} : 0\leq \lambda_{1},\lambda_{2},\lambda_{3}\leq 1 \right\} $$ Explanation It is a 3-dimensional extension of a parallelogram. It is defined in the same way</description></item><item><title>How to Set Git Username and Email</title><link>https://freshrimpsushi.github.io/en/posts/2702/</link><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2702/</guid><description>Guide git config --global user.name &amp;#34;Your Name&amp;#34; git config --global user.email &amp;#34;Your Email&amp;#34; It may seem obvious, but many actions that use Git require a GitHub username and email. If you configure them once as above, it’s convenient because you won’t need to set them again on the same machine. Issue In fact, there are countless posts introducing git config, and the true</description></item><item><title>Rhombus</title><link>https://freshrimpsushi.github.io/en/posts/3695/</link><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3695/</guid><description>Definition Simple Definition A quadrilateral with all sides of equal length is called a rhombus. Linear Algebraic Definition For two distinct vectors $\mathbf{a}$ and $\mathbf{b}$ of equal magnitude on a 2D Cartesian plane, the following set is called a rhombus. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} : 0\leq \lambda_{1},\lambda_{2}\leq 1, \quad |\mathbf{a}| = |\mathbf{b}|, \quad \mathbf{a} \ne \mathbf{b} \right\} $$ Explanation By definition, if it is a square,</description></item><item><title>JavaScript Multi Dropdown Implementation Example</title><link>https://freshrimpsushi.github.io/en/posts/2701/</link><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2701/</guid><description>Showcase Kingdom Animal Plant Species Subspecies From left to right, this implements cascading dropdowns that go from a broad category to a more specific one. When an upper dropdown is selected, the lower dropdown is dynamically updated via JavaScript. The key point of this example is that you can create the most common multi-level dropdowns (two levels or more) in a very simple way. Code HTML &amp;lt;select id=&amp;#34;dropdown1&amp;#34; onchange=&amp;#34;updateDropdown(1)&amp;#34;&amp;gt; &amp;lt;option</description></item><item><title>Parallelogram</title><link>https://freshrimpsushi.github.io/en/posts/3694/</link><pubDate>Tue, 26 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3694/</guid><description>Definition Simple Definition A quadrilateral with two pairs of opposite sides that are each parallel to each other is called a parallelogram. Linear Algebraic Definition For two distinct vectors $\mathbf{a}$ and $\mathbf{b}$ on a 2-dimensional coordinate plane, the following set is called a parallelogram. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} : 0\leq \lambda_{1},\lambda_{2}\leq 1, \quad \mathbf{a} \ne \mathbf{b} \right\} $$ Explanation By definition, a parallelogram is also a</description></item><item><title>Example of Sending Data to a Julia Server via Localhost in JavaScript</title><link>https://freshrimpsushi.github.io/en/posts/2700/</link><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2700/</guid><description>Showcase Send &amp;lsquo;A&amp;rsquo; Send &amp;lsquo;B&amp;rsquo; Clicking the left button sends the message &amp;lsquo;A&amp;rsquo; and clicking the right button sends &amp;lsquo;B&amp;rsquo; to the Julia server. The Julia server receives this message via the HTTP.jl package and, via , outputs it to the console. showcase.mp4 Code HTML &amp;lt;button onclick=&amp;#34;sendData(&amp;#39;A&amp;#39;)&amp;#34;&amp;gt;Send &amp;#39;A&amp;#39;&amp;lt;/button&amp;gt; &amp;lt;button onclick=&amp;#34;sendData(&amp;#39;B&amp;#39;)&amp;#34;&amp;gt;Send &amp;#39;B&amp;#39;&amp;lt;/button&amp;gt; onclick: When the button is clicked, the sendData function is executed. JavaScript &amp;lt;script&amp;gt; function sendData(message) { fetch(&amp;#39;http://localhost:8000&amp;#39;, { method:</description></item><item><title>Square</title><link>https://freshrimpsushi.github.io/en/posts/3693/</link><pubDate>Sun, 24 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3693/</guid><description>Definition Simple Definition A quadrilateral with all sides of equal length and all angles of equal measure is called a square. Linear Algebraic Definition On a 2-dimensional coordinate plane, for two perpendicular and equal magnitude vectors $\mathbf{a}$ and $\mathbf{b}$, the following set is called a square. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} : 0\leq \lambda_{1},\lambda_{2}\leq 1, \quad \mathbf{a} \cdot \mathbf{b} = 0, \quad |\mathbf{a}| = |\mathbf{b}| \right\} $$</description></item><item><title>How to Change the Mouse Cursor Shape in HTML</title><link>https://freshrimpsushi.github.io/en/posts/2699/</link><pubDate>Sat, 23 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2699/</guid><description>Showcase auto | default | none | context-menu | help | pointer | progress | wait | cell | crosshair | text | vertical-text | alias | copy | move | no-drop | not-allowed | grab | grabbing | all-scroll | col-resize | row-resize | n-resize | e-resize | s-resize | w-resize | ne-resize | nw-resize | se-resize | sw-resize | ew-resize | ns-resize | nesw-resize | nwse-resize | zoom-in</description></item><item><title>Rectangle</title><link>https://freshrimpsushi.github.io/en/posts/3692/</link><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3692/</guid><description>Definition Simple Definition A quadrilateral where all four angles are equal is called a rectangle. Linear Algebraic Definition On a 2D coordinate plane, for two perpendicular vectors $\mathbf{a}$ and $\mathbf{b}$, the set defined as follows is called a rectangle. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} : 0\leq \lambda_{1},\lambda_{2}\leq 1, \quad \mathbf{a} \cdot \mathbf{b} = 0 \right\} $$ Explanation By definition, if it is a square, then it is</description></item><item><title>Comprehensive Summary of Julia String Syntax and Functions</title><link>https://freshrimpsushi.github.io/en/posts/3691/</link><pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3691/</guid><description>Overview This document provides a brief overview of the syntax and functions related to characters/strings used in Julia. The version reference is Julia v1.11.5. Official Documentation Manual&amp;gt;Strings: https://docs.julialang.org/en/v1/manual/strings/ Base&amp;gt;Strings: https://docs.julialang.org/en/v1/base/strings/ Summary Definition 'x' &amp;quot;x&amp;quot; repr(foo) Char(decimal) Char(hex) Operations &amp;quot;foo&amp;quot; &amp;lt; &amp;quot;bar&amp;quot; &amp;quot;foo&amp;quot; &amp;gt; &amp;quot;bar&amp;quot; &amp;quot;foo&amp;quot; == &amp;quot;bar&amp;quot; &amp;quot;foo&amp;quot; != &amp;quot;bar&amp;quot; isless(&amp;quot;foo&amp;quot;, &amp;quot;bar&amp;quot;) Predicates isspace(' ') isletter('x') isuppercase('X') islowercase('x') isdigit('1') isxdigit('x') isnumeric('1') ispunct('!') isascii(&amp;quot;foo&amp;quot;) iscntrl('x') isprint('x') General length(&amp;quot;foo bar&amp;quot;) sizeof(&amp;quot;foo bar&amp;quot;)</description></item><item><title>Orthogonal Similarity and Orthogonal Diagonalization of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3690/</link><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3690/</guid><description>Definition1 Orthogonal similarity For two square matrices $A$ and $B$, if there exists an orthogonal matrix $P$ that satisfies the following, then $A$ and $B$ are said to be orthogonally similar. $$ A = P^{\mathsf{T}}BP $$ Orthogonal diagonalization A square matrix $A$ is said to be orthogonally diagonalizable (or $P$ is said to orthogonally diagonalize $A$) if it is orthogonally similar to some diagonal matrix $D$. In other words, $A$</description></item><item><title>Diagonalization of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3689/</link><pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3689/</guid><description>Definition1 If a square matrix $A$ is similar to some diagonal matrix $D$ via similarity, we say $A$ is diagonalizable, or that $P$ diagonalizes $A$. In other words, $A$ is called a diagonalizable matrix if there exists an invertible matrix $P$ satisfying the following. $$ A = P^{-1}DP $$ Explanation If the matrix $A$ is diagonalizable, computing its powers becomes very easy. If one computes $A^{k}$ directly, one must perform</description></item><item><title>Topological Group</title><link>https://freshrimpsushi.github.io/en/posts/3687/</link><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3687/</guid><description>Definition1 A group $\braket{G, \cdot}$ that is a topological space and satisfies the following is called a topological group. The group multiplication $\cdot : G \times G \to G$, $\quad (g, h) \mapsto g \cdot h$ is continuous. The map to inverses $i : G \to G$, $\quad g \mapsto g^{-1}$ is continuous. From the topological viewpoint A Hausdorff space $G$ is called a topological group if it forms a</description></item><item><title>Differentiation of Polynomial Functions</title><link>https://freshrimpsushi.github.io/en/posts/3686/</link><pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3686/</guid><description>Formula The derivative of a polynomial function is as follows. $$ \dfrac{d x^{n}}{dx} = n x^{n-1} $$ If $n \in \mathbb{N}$, it holds in $x \in \mathbb{R}$. If $n \in \mathbb{Z}$, it holds in $x \ne 0$. If $n \in \mathbb{R}$, it holds in $x \gt 0$. Proof $n \in \mathbb{N}$ By the definition of the derivative, $$ \dfrac{d x^{n}}{dx} = \lim_{h \to 0} \dfrac{(x+h)^{n} - x^{n}}{h} $$ Binomial theorem:</description></item><item><title>Representation of Groups</title><link>https://freshrimpsushi.github.io/en/posts/3684/</link><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3684/</guid><description>Definition1 2 Let a group $G$ and a finite-dimensional vector space $V$ be given. Call $\operatorname{GL}(V)$ the general linear group. A homomorphism $\rho$ as below is called a representation of $G$ on $V$. $$ \rho : G \to \operatorname{GL}(V) $$ By definition one may also say “the map $\rho$ endows the vector space $V$ with the structure of a $G$-modu</description></item><item><title>Equivariant Map of Group Action</title><link>https://freshrimpsushi.github.io/en/posts/3683/</link><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3683/</guid><description>Definition1 2 Suppose a group $G$, and two actions $\ast_{1} : G \times X \to X$, $\ast_{2} : G \times Y \to Y$ are given. A function $f : X \to Y$ between two $G$-sets is said to be equivariant concerning $G$ if it satisfies the following condition. $$ f(g \ast_{1} x) = g \ast_{2} f(x), \qquad \forall g \in G, x \in X $$ Explanation Simply speaking, the result</description></item><item><title>Kronecker Delta through Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3682/</link><pubDate>Sat, 02 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3682/</guid><description>Definition We define $\delta_{ij}$, as shown below, to be the Kronecker delta. $$ \delta_{ij} = \begin{cases} 1 &amp;amp; \text{if } i = j \\ 0 &amp;amp; \text{if } i \ne j \end{cases} $$ Explanation The Kronecker delta is typically encountered around the second year of undergraduate studies in science and engineering when vector calculus becomes prominent. While it is a useful tool for converting complex vector calculations into simpler scalar</description></item><item><title>Relative Entropy (Kullback-Leibler Divergence) between Two Normal Distributions</title><link>https://freshrimpsushi.github.io/en/posts/3681/</link><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3681/</guid><description>Formula The relative entropy (KLD) between two normal distributions $N(\mu, \sigma^{2})$ and $N(\mu_{1}, \sigma_{1}^{2})$ is given by the following expression. $$ D_{\text{KL}}\big( N(\mu, \sigma^{2}) \| N(\mu_{1}, \sigma_{1}^{2}) \big) = \log \left( \dfrac{\sigma_{1}}{\sigma} \right) + \dfrac{\sigma^{2} + (\mu - \mu_{1})^{2}}{2\sigma_{1}^{2}} - \dfrac{1}{2} $$ The relative entropy between two multivariate normal distributions $N(\boldsymbol{\mu}, \Sigma)$ and $N(\boldsymbol{\mu_{1}}, \Sigma_{1})$ is given by the following. $$ \begin{array}{l} D_{\text{KL}}\big( N(\boldsymbol{\mu}, \Sigma) \| N(\boldsymbol{\mu_{1}}, \Sigma_{1}) \big) \\[1em]</description></item><item><title>How to input physical units in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3680/</link><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3680/</guid><description>Code Unit Plain Text Inside an Equation Using siunitx Ångström Å \AA \mathrm{\AA} \si{\angstrom}</description></item><item><title>Inverse Function</title><link>https://freshrimpsushi.github.io/en/posts/3679/</link><pubDate>Sun, 27 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3679/</guid><description>Definition For a given surjective function $f: X \to Y$, the inverse function of $f$ is defined as follows. $$ f^{-1} : Y \to X, \quad f^{-1}(y) = x \iff f(x) = y $$ A function for which an inverse function exists is called an invertible function. Explanation By definition, $f$ is the inverse function of $f^{-1}$. $$ f = (f^{-1})^{-1} $$ $f \circ f^{-1}$ and $f^{-1} \circ f$ are</description></item><item><title>Radian</title><link>https://freshrimpsushi.github.io/en/posts/3678/</link><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3678/</guid><description>Definition The angle of a sector with a radius $r$ and an arc length $\ell$ is called $\theta$ $\text{rad}$. Here, $\text{rad}$ is read as radian. Explanation Since it is a value derived from dividing a length by a length, it is a dimensionless unit. Therefore, this unit is often omitted in usage. An angle without a unit is fundamentally assumed to be in radians. In the unit circle, where the</description></item><item><title>Geometric Mean</title><link>https://freshrimpsushi.github.io/en/posts/3677/</link><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3677/</guid><description>Definition The value for two positive numbers $a, b$ is called the geometric mean of $a$ and $b$. $$ \sqrt{ab} $$ Generalization For $n$ positive numbers $a_{1}, \dots, a_{n}$, the following value is called the geometric mean of $a_{1}, \dots, a_{n}$. $$ \sqrt[n]{a_{1}a_{2}\cdots a_{n}} $$ Explanation If one considers an extension to complex numbers, then $a_{i}$ do not necessarily have to be positive. One who encounters the geometric mean for</description></item><item><title>Paper Review: Score Matching</title><link>https://freshrimpsushi.github.io/en/posts/3676/</link><pubDate>Mon, 21 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3676/</guid><description>Overview Score Matching is a statistical technique introduced in the 2005 paper by Aapo Hyvarinen, Estimation of Non-Normalized Statistical Models by Score Matching, which provides a method for estimating non-normalized models without considering the normalization constant. 1. Introduction In many cases, probabilistic models are given as non-normalized models containing a normalization constant $Z$. For instance, a probability density function $p_{\boldsymbol{\theta}}$ with parameters $\boldsymbol{\theta}$ is defined as follows: $$ p(\boldsymbol{\xi}; \boldsymbol{\theta})</description></item><item><title>Matrix Differentiation Table for Scalar Functions</title><link>https://freshrimpsushi.github.io/en/posts/3675/</link><pubDate>Sat, 19 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3675/</guid><description>Explanation The formulas for matrix calculus have been summarized in the table below. The notation used throughout the document is as follows: $\mathbf{a}, \mathbf{b} \in \mathbb{R}^{n}$: A constant vector independent of $\mathbf{x}$ or $\mathbf{X}$ $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{n \times n}$: A constant matrix independent of $\mathbf{x}$ or $\mathbf{X}$ $\mathbf{x} \in \mathbb{R}^{n}$: A variable vector $\mathbf{X} \in \mathbb{R}^{n \times n}$: A variable matrix An interesting aspect of differentiation rules is</description></item><item><title>Trace Trick</title><link>https://freshrimpsushi.github.io/en/posts/3674/</link><pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3674/</guid><description>Equation Suppose a scalar function $f: \mathbb{R}^{n \times n} \to \mathbb{R}$ is defined over matrix space $M(\mathbb{R}^{n \times n})$. Let $\mathrm{d}f$ be the total differential of $f$. The following holds. $$ \mathrm{d}f = \Tr (\mathrm{d}f) = \mathrm{d}\Tr (f) \tag{1} $$ $\Tr$ is the trace. Proof The function value of $f$ is a scalar, so taking the trace yields the same result. Therefore, we obtain: $$ f = \Tr(f) \implies \mathrm{d}f</description></item><item><title>Matrix Differential Dissection</title><link>https://freshrimpsushi.github.io/en/posts/3673/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3673/</guid><description>Definition $n \times n$ The differential of a matrix $\mathbf{X} = [x_{ij}] a$ is defined as follows. $$ \mathrm{d} \mathbf{X} = \begin{bmatrix} \mathrm{d} x_{11} &amp;amp; \mathrm{d} x_{12} &amp;amp; \cdots &amp;amp; \mathrm{d} x_{1n} \\ \mathrm{d} x_{21} &amp;amp; \mathrm{d} x_{22} &amp;amp; \cdots &amp;amp; \mathrm{d} x_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \mathrm{d} x_{n1} &amp;amp; \mathrm{d} x_{n2} &amp;amp; \cdots &amp;amp; \mathrm{d} x_{nn} \end{bmatrix} $$ Explanation If the generalization of the</description></item><item><title>Total Differential of Function of a Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3672/</link><pubDate>Sun, 13 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3672/</guid><description>Background Scalar Differentiation Let&amp;rsquo;s consider a scalar function $f : \mathbb{R} \to \mathbb{R}$ and ordinary differentiation. $$ \dfrac{d f}{d x} \tag{1} $$ This notation resembles a fraction, and it&amp;rsquo;s assured that it can indeed be treated as such for calculations. For example, the chain rule, which is the differentiation rule for composite functions, can be intuitively calculated as if canceling fractions, as shown below. $$ \dfrac{d f}{d t} = \dfrac{d</description></item><item><title>Inner Product of Matrices (Frobenius Inner Product)</title><link>https://freshrimpsushi.github.io/en/posts/3671/</link><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3671/</guid><description>Definition The inner product or dot product of two matrices $m \times n$, $X = [x_{ij}]$, $Y=[Y]_{ij}$ is defined as follows. $$ X \cdot Y = \braket{X, Y} = \sum_{i=1}^{m}\sum\limits_{j=1}^{n} x_{ij}y_{ij} $$ In the case where the components are complex numbers in a complex matrix, it is defined as follows. $$ X \cdot Y = \braket{X, Y} = \sum_{i=1}^{m}\sum\limits_{j=1}^{n} \overline{x}_{ij}y_{ij} $$ Here, $\overline{x}$ denotes the complex conjugate. Explanation Since the</description></item><item><title>Rank in Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2677/</link><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2677/</guid><description>Definition In statistics, the rank $R : \mathbb{R} \to \mathbb{N}$ is commonly used as a function to indicate the order of data when sorted in ascending order. Explanation $$ x_{3} &amp;lt; x_{1} &amp;lt; x_{2} \implies x_{(1)} = x_{3}, x_{(2)} = x_{1}, x_{(3)} = x_{2} $$ In statistics, the method of placing parentheses in the subscript of data is used when the order of data is required. The rank introduced in</description></item><item><title>Matrix Calculus of Trace</title><link>https://freshrimpsushi.github.io/en/posts/3670/</link><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3670/</guid><description>Formula Let $\mathbf{X}$ be $n \times n$ matrix. Define $\dfrac{\partial }{\partial \mathbf{X}} = \nabla_{\mathbf{X}}$ as the matrix gradient. Then, the following formula holds: $$ \dfrac{\partial \Tr(\mathbf{X})}{\partial \mathbf{X}} = I, \qquad \dfrac{\partial \Tr(a\mathbf{X})}{\partial \mathbf{X}} = aI \tag{1} $$ Here, $a \in \mathbb{R}$ is a constant (scalar), and $I$ is an identity matrix. Suppose $\mathbf{A} \in \mathbb{R}^{n \times p}$ and $\mathbf{X} \in \mathbb{R}^{p \times n}$. Then, the following holds: $$ \dfrac{\partial \Tr(\mathbf{A}\mathbf{X})}{\partial</description></item><item><title>Consistent with, agree with</title><link>https://freshrimpsushi.github.io/en/posts/2676/</link><pubDate>Tue, 08 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2676/</guid><description>Vocabulary consistent with: An idiom meaning &amp;lsquo;in agreement with&amp;rsquo;, where consistent is not used in the sense of &amp;lsquo;steadfast&amp;rsquo; or &amp;lsquo;coherent&amp;rsquo;. agree with: Another expression meaning &amp;lsquo;in agreement with&amp;rsquo;, where agree is not used in the sense of &amp;lsquo;consent&amp;rsquo; or &amp;lsquo;concur&amp;rsquo;. Examples consistent with &amp;ldquo;그림 2는 예측된 헤논 맵에서의 바이퍼케이션</description></item><item><title>Outer Product of Two Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3669/</link><pubDate>Mon, 07 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3669/</guid><description>Definition The outer product of two column vectors $\mathbf{u} = \begin{bmatrix} u_{1} \\ \vdots \\ u_{n} \end{bmatrix}$ and $\mathbf{v} = \begin{bmatrix} v_{1} \\ \vdots \\ v_{n} \end{bmatrix}$ is defined as follows. $$ \mathbf{u} \otimes \mathbf{v} = \mathbf{u}\mathbf{v}^{\mathsf{T}} = \begin{bmatrix} u_{1} \\ u_{2} \\ \vdots \\ u_{n} \end{bmatrix} \begin{bmatrix} v_{1} &amp;amp; v_{2} &amp;amp; \cdots &amp;amp; v_{n} \end{bmatrix}= \begin{bmatrix} u_{1}v_{1} &amp;amp; u_{1}v_{2} &amp;amp; \cdots &amp;amp; u_{1}v_{n} \\ u_{2}v_{1} &amp;amp; u_{2}v_{2} &amp;amp; \cdots</description></item><item><title>What is Non-parametric Statistics?</title><link>https://freshrimpsushi.github.io/en/posts/2675/</link><pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2675/</guid><description>Definition 1 In statistics, nonparametric statistics refers to statistical methodologies that generally do not assume a specific distribution for the population. They are particularly known for having minimal conditions for hypothesis testing. Description As an example, let&amp;rsquo;s see how hypothesis testing is conducted in analysis of variance. One-way ANOVA: Consider that there are $k$ treatments in an experimental design, and $n_{j}$ samples are drawn from each treatment, with a total</description></item><item><title>Formula for Matrix Power Form</title><link>https://freshrimpsushi.github.io/en/posts/3668/</link><pubDate>Sat, 05 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3668/</guid><description>Formula For the matrix $X = [x_{ij}] \in \mathbb{R}^{n \times n}$, the following holds. $$ [XX]_{ij} = \sum_{k=1}^{n} x_{ik} x_{kj} $$ $$ XX = X^{2} = \begin{bmatrix} \sum\limits_{k=1}^{n} x_{1k} x_{k1} &amp;amp; \cdots &amp;amp; \sum\limits_{k=1}^{n} x_{1k} x_{kn} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \sum\limits_{k=1}^{n} x_{nk} x_{k1} &amp;amp; \cdots &amp;amp; \sum\limits_{k=1}^{n} x_{nk} x_{kn} \end{bmatrix} $$ If $X$ is a symmetric matrix, $$ X^{2} = \begin{bmatrix} \sum\limits_{k=1}^{n} (x_{1k})^{2} &amp;amp; \cdots &amp;amp; \sum\limits_{k=1}^{n}</description></item><item><title>How to Move the Screen Using WASD in Hearts of Iron 4</title><link>https://freshrimpsushi.github.io/en/posts/2674/</link><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2674/</guid><description>Overview An alternate title for this post could be &amp;lsquo;How to Set Key Bindings in Windows&amp;rsquo;. Unlike Linux, Windows has made it traditionally difficult to change key bindings natively. However, by using the PowerToys app provided by Microsoft, you can easily remap key inputs. Example HOI4 is a strategic simulation game provided by Paradox Interactive, which has a large fanbase. However, new users might find intuitive gameplay hindered as WASD</description></item><item><title>Matrix Calculus of Quadratic and Bilinear Forms</title><link>https://freshrimpsushi.github.io/en/posts/3667/</link><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3667/</guid><description>Formula For two vectors $\mathbf{a} \in \mathbb{R}^{m}$, $\mathbf{b} \in \mathbb{R}^{n}$ and a matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$, the gradient matrix of the bilinear form $\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{b}$ is as follows. $$ \nabla_{\mathbf{X}} (\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{b}) = \dfrac{\partial (\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{b})}{\partial \mathbf{X}} = \mathbf{a}\mathbf{b}^{\mathsf{T}} \tag{1} $$ As a corollary, for quadratic form $\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{a}$, the following holds. $$ \dfrac{\partial (\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{a})}{\partial \mathbf{X}} = \mathbf{a}\mathbf{a}^{\mathsf{T}} $$ For two vectors</description></item><item><title>A Trick to Solve Ordinary Differential Equations Backwards in Time</title><link>https://freshrimpsushi.github.io/en/posts/2673/</link><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2673/</guid><description>Method $$ {\frac{ d y }{ d t }} = f(y) $$ Consider a numerical solver $K$ for solving the given ordinary differential equation using a future time point $T &amp;gt; 0$ and initial value $y_{0}$. Assume it operates as follows: $$ K \left( f , T , y_{0} \right) = \left\{ y(t) : t \in [0, T] \right\} $$ Conversely, to obtain $\left\{ y(s) : s \in [-S, 0]</description></item><item><title>Probability Vector</title><link>https://freshrimpsushi.github.io/en/posts/3665/</link><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3665/</guid><description>Definition A vector that satisfies the following condition $\mathbf{p} = \begin{bmatrix}p_{1} &amp;amp; \cdots &amp;amp; p_{n} \end{bmatrix}^{\mathsf{T}}$ is called a probability vector. $$ 0 \le p_{i} \le 1 \quad (1 \le i \le n)\quad \text{and} \quad \sum_{i=1}^{n} p_{i} = 1 $$ Explanation A probability vector is a vector that represents the probabilities of each state when there are $n$ states. Conceptually, it is analogous to a probability mass function. If the</description></item><item><title>"Small, Minuscule: Tiny, Minuscule"</title><link>https://freshrimpsushi.github.io/en/posts/2672/</link><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2672/</guid><description>Vocabulary tiny: &amp;ldquo;very small&amp;rdquo;, referring to quantities or sizes in a mathematical context rather than colloquially. minuscule: &amp;ldquo;very small&amp;rdquo;, a more sophisticated expression. Examples tiny &amp;ldquo;However, detecting tiny objects (for example tiny persons less than 20 pixels) in large-scale images remains not well investigated.&amp;rdquo; 1 minuscule &amp;ldquo;In many cases, we may need to normalize the columns of $\Theta (X)$ first to ensure that the restricted isometry property holds; this is</description></item><item><title>Energy-Based Model</title><link>https://freshrimpsushi.github.io/en/posts/3664/</link><pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3664/</guid><description>Overview1 2 3 Energy-based models are a methodology where a function of data referred to as energy is defined, with the notion that data with lower energy is considered more plausible (i.e., more probable). Unlike assuming previously known distributions, energy-based models directly model the distribution of the data, making them useful for handling complex datasets. Build-up In traditional probabilistic models, the approach to handling data involves assuming that the data</description></item><item><title>Situations Where Implicit Methods Are Preferred Over Explicit Methods in Solving Ordinary Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/2671/</link><pubDate>Sat, 28 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2671/</guid><description>Overview $$ \dot{y} = f(y) $$ The numerical solver used to solve the given ordinary differential equation above is categorized into explicit methods and implicit methods based on their calculation techniques. Generally, explicit methods are easy to implement and use, while implicit methods—although more complex to solve—ten</description></item><item><title>How to Use Dice Images in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3663/</link><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3663/</guid><description>\usepackage{epsdice} This package provides black and white images of dice with faces numbered 1 through 6. \documentclass{article} \usepackage{epsdice} \begin{document} \epsdice{1} \epsdice{2} \epsdice{3} \epsdice[white]{4} \epsdice[white]{5} \epsdice[white]{6} \epsdice[black]{1} \epsdice[black]{2} \epsdice[black]{3} \epsdice{[black]4} \epsdice[black]{5} \epsdice[black]{6} \end{document} \usepackage{customdice} This package provides the functionality to draw user-defined dice images. The number of faces, color, characters to be displayed, and size can all be customized according to user preference. For more details, refer to the official documentation.</description></item><item><title>Brace terms in Tex</title><link>https://freshrimpsushi.github.io/en/posts/2670/</link><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2670/</guid><description>Code In TeX, when indicating the number of terms, the \overbrace and \underbrace commands are commonly used to group multiple terms. To use these two commands, you need to load the amsmath package via \usepackage{amsmath}. \documentclass{article} \usepackage{amsmath} \begin{document} $$ x + n = x + \overbrace{1 + \cdots + 1}^{n \text{ times}} $$ $$ xn = \underbrace{x + \cdots + x}_{n \text{ times}} $$ \end{document} Both commands take the formula</description></item><item><title>How to Define and Use Your Colors in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3662/</link><pubDate>Wed, 25 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3662/</guid><description>Description The xcolor package allows users to employ predefined colors, but users can also define and use their own colors. By including \definecolor{color name}{color space}{values} in the preamble, you can define custom colors. The color spaces include rgb, cmyk, HTML, gray, among others. \definecolor{color name}{gray}{gray}: Definition of grayscale color $[0, 1]$ \definecolor{color name}{rgb}{r,g,b}: Definition of RGB color $[0, 1]^{3}$ \definecolor{color name}{cmyk}{c,m,y,k}: Definition of CMYK color $[0, 1]^{4}$ \definecolor{color name}{RGB}{R, G,</description></item><item><title>Two-way ANOVA</title><link>https://freshrimpsushi.github.io/en/posts/2669/</link><pubDate>Tue, 24 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2669/</guid><description>Hypothesis Testing 1 Suppose there are $k$ treatments and $b$ blocks in an experimental design, from which $n = bk$ samples are obtained. Assume that the samples of the $j = 1 , \cdots , k$th treatment are independently and randomly distributed following a normal distribution $N \left( \mu_{j} , \sigma_{j}^{2} \right)$, with the population variance of each normal distribution being equal, i.e., $\sigma^{2} = \sigma_{1}^{2} = \cdots = \sigma_{k}^{2}$.</description></item><item><title>How to Use Colors in LaTeX: The xcolor Package</title><link>https://freshrimpsushi.github.io/en/posts/3661/</link><pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3661/</guid><description>Explanation \usepackage{color} In $\LaTeX$, the most basic package that allows the use of colors is color. You can add \usepackage{color} to the preamble and use it as {\color{color}content}. The available colors are as follows. $$ \begin{array}{lll} \color{black}\colorbox{black}{\phantom{colorbox}}black&amp;amp; \color{blue}\colorbox{blue}{\phantom{colorbox}}blue&amp;amp; \color{cyan}\colorbox{cyan}{\phantom{colorbox}}cyan\\ \color{green}\colorbox{green}{\phantom{colorbox}}green&amp;amp; \color{magenta}\colorbox{magenta}{\phantom{colorbox}}magenta&amp;amp; \color{red}\colorbox{red}{\phantom{colorbox}}red\\ \color{white}\colorbox{white}{\phantom{colorbox}} \color{black}{white}&amp;amp; \color{yellow}\colorbox{yellow}{\phantom{colorbox}}yellow \end{array} $$ \documentclass{article} \usepackage{color} \begin{document} This is {\color{red}red} text. \end{document} $$ \text{This is } \textcolor{red}{\text{red}} \text{ text.} $$ \usepackage{xcolor} By using xcolor, you can utilize</description></item><item><title>How to Use Lorem Ipsum in Text</title><link>https://freshrimpsushi.github.io/en/posts/2668/</link><pubDate>Sun, 22 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2668/</guid><description>Code Lorem Ipsum or Lipsum is a placeholder made of meaningless strings to focus on the document&amp;rsquo;s design. In $\TeX$, it can be used by utilizing the lipsum package. \lipsum 1 The command \lipsum can be handled as if it were a two-dimensional array. \lipsum[x]: Outputs the $x$-th paragraph. \lipsum[x][y]: Outputs the $x$-th paragraph&amp;rsquo;s $y$-th sentence. \lipsum[x][y-z]: Outputs from the $y$-th to the $z$-th sentence in the $x$-th paragraph. \documentclass{article}</description></item><item><title>How to Automatically Number Figures in LaTeX Beamer</title><link>https://freshrimpsushi.github.io/en/posts/3660/</link><pubDate>Sat, 21 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3660/</guid><description>Code In the case of the article document style in $\LaTeX$, when inserting a photo and writing \caption{}, numbering is automatically applied as follows. However, in the case of beamer, numbering is not applied as follows. By adding \setbeamertemplate{caption}[numbered] to the preamble, numbering will be applied.</description></item><item><title>One-way Analysis of Variance</title><link>https://freshrimpsushi.github.io/en/posts/2667/</link><pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2667/</guid><description>Hypothesis Testing 1 In the experimental design where there are $k$ treatments, suppose that we obtain $n_{j}$ samples from each treatment for a total of $n = n_{1} + \cdots + n_{k}$ samples. Assume that the samples from the $j = 1 , \cdots , k$-th treatment are independent and randomly follow a normal distribution with $N \left( \mu_{j} , \sigma_{j}^{2} \right)$, and that the population variance of each normal</description></item><item><title>Professor Choi Byung-sun's Freely Distributed Textbooks on Mathematics, Statistics, and Economics</title><link>https://freshrimpsushi.github.io/en/posts/3659/</link><pubDate>Thu, 19 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3659/</guid><description>Description Professor Byung-Sun Choi, a former professor at Yonsei University in the Department of Applied Statistics and a current professor in the Department of Economics at Seoul National University, has published 12 textbooks on mathematics, statistics, and economics for free online. Each can be downloaded as a PDF from the links below. List 1995 Multivariate Time Series Analysis (Link) 1995 Introduction to SAS/IML (Link) 1997 Regression Analysis Volume I (Link)</description></item><item><title>Expansion of Input Parameters for Batch Command</title><link>https://freshrimpsushi.github.io/en/posts/2666/</link><pubDate>Wed, 18 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2666/</guid><description>Code Consider that one of several input parameters, the first argument C:\data\ABC.csv, is passed as %1 with a tilde included. Various functionalities of this input argument are introduced1. Syntax Origin Description Result %~f1 Full path Full path C:\data\ABC.csv %~d1 Drive Drive C: %~p1 Path Path \data\ %~n1 Name Name ABC %~x1 eXtension Extension .csv Combination The input argument can also be used in combination. Although combinations like %~dn1 may not</description></item><item><title>Free Online Available R Textbook (Original Version)</title><link>https://freshrimpsushi.github.io/en/posts/3658/</link><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3658/</guid><description>Description Hadley Wickham hails from the University of Auckland in New Zealand and is the creator of most of the current tools for big data analysis in R, including the visualization package ggplot2, the preprocessing package dplyr, and the string package stringr.1 The four books introduced below are R texts authored by Hadley Wickham, which are available for free on the web. List R for Data Science (1e, 2e) ggplot2</description></item><item><title>F-test in Analysis of Variance</title><link>https://freshrimpsushi.github.io/en/posts/2665/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2665/</guid><description>Hypothesis Testing 1 Assume that there are $k$ treatments in experimental design, and that in each treatment there are $n_{j}$ samples, totaling $n = n_{1} + \cdots + n_{k}$ samples. Assume that each sample in the $j = 1 , \cdots , k$th treatment is independently and randomly drawn from a normal distribution $N \left( \mu_{j} , \sigma_{j}^{2} \right)$, and their population variance is equal such that $\sigma^{2} = \sigma_{1}^{2}</description></item><item><title>List of Available Fonts in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3657/</link><pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3657/</guid><description>Basic Fonts These are the default fonts that can be used without installing any packages. Roman Used with \mathrm{words} or \rm{words}. \mathrm{ABCDEFGHIJKLMNOPQRSTUVWXYZ} $$\rm{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$$ \mathrm{abcdefghijklmnopqrstuvwxyz} $$\rm{abcdefghijklmnopqrstuvwxyz}$$ \mathrm{0123456789} $$\rm{0123456789}$$ Caligraphic Used with \mathcal{words} or \cal{words}. In mathematics, it is commonly used to denote operators such as Fourier transform and Laplace transform. Only uppercase letters are possible; lowercase letters are rendered as entirely different characters. \mathcal{ABCDEFGHIJKLMNOPQRSTUVWXYZ} $$\cal{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$$ Sans-serif Used with \mathsf{words} or \sf{words}.</description></item><item><title>How to receive multiple input parameters in a batch command</title><link>https://freshrimpsushi.github.io/en/posts/2664/</link><pubDate>Sat, 14 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2664/</guid><description>Code In batch command, for a natural number $m$, %n refers to the $n$th parameter, and the array of all parameters is represented as %*1. For example: Let&amp;rsquo;s assume there is a batch file named multi.bat in the directory as shown above. @echo off echo %%1: %1 echo %%3: %3 echo %%*: %* pause The result of executing multi.bat by drag-and-dropping the three folders A, B, and C is as</description></item><item><title>How to Use Hyperlinks in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3656/</link><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3656/</guid><description>Code You can load the hyperref package and use the \href{URL}{text} command. It can be used to wrap text or equations and can be applied even in display mode. \documentclass{article} \usepackage{hyperref} \hypersetup{ colorlinks=true, urlcolor=blue, } \begin{document} unlinked Freshrimpsushi Blog linked \href{https://freshrimpsushi.github.io/ko/}{Freshrimpsushi Blog} The Fourier transform \href{https://freshrimpsushi.github.io/ko/posts/1086/}{$\hat{f}$} is defined as $$ \href{https://freshrimpsushi.github.io/ko/posts/1086/}{\hat{f}(\xi) = \frac{1}{2\pi}\int_{-\infty}^{\infty} f(x) e^{-i \xi x} dx} $$ \end{document} Through \hypersetup, you can also specify the colors for URLs,</description></item><item><title>ANOVA Table</title><link>https://freshrimpsushi.github.io/en/posts/2663/</link><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2663/</guid><description>Definition 1 A table summarizing the results from analysis of variance (ANOVA) is called an ANOVA table. The format of the ANOVA table may vary slightly depending on the experimental design. Completely Randomized Design Source df SS MS F Treatments $k-1$ SST MST MST/MSE Error $n-k$ SSE MSE Total $n-1$ TSS Randomized Block Design Source df SS MS F Treatments $k-1$ SST MST MST/MSE Blocks $b-1$ SSB MSB Error $(k-1)(b-1)$</description></item><item><title>Differentiation of the Absolute Value Function</title><link>https://freshrimpsushi.github.io/en/posts/3655/</link><pubDate>Wed, 11 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3655/</guid><description>Theorem The derivative of the absolute value function is as follows. $$ \frac{ d |x| } {d x} = \dfrac{1}{|x|}x = \begin{cases} 1 &amp;amp; x &amp;gt; 0 \\ -1 &amp;amp; x &amp;lt; 0 \end{cases}, \qquad x \neq 0 $$ Explanation In fact, the absolute value function is not differentiable over the entire set of real numbers because of its sharp point at $x = 0$. However, excluding just one point</description></item><item><title>How to Run Programs in the Background Using the Batch Command</title><link>https://freshrimpsushi.github.io/en/posts/2662/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2662/</guid><description>Code Let&amp;rsquo;s say you want to run a program called program. Windows START /B program You just need to add START /B before program1. For instance, assume you have the following files: REPL.jl, REPL.py, and REPL.bat. REPL.py: import time while True: print(&amp;#34;It&amp;#39;s from python&amp;#34;) time.sleep(1) REPL.jl: while true println(&amp;#34;It&amp;#39;s from julia&amp;#34;) sleep(1) end REPL.bat: START /B julia REPL.jl START /B python REPL.py The Julia and Python files are programs that</description></item><item><title>The Mean and Variance of the Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3654/</link><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3654/</guid><description>Formula Given $X \sim$ when $\operatorname{Bin}(1, p)$, the mean and variance of $X$ are as follows. $$ E(X) = p $$ $$ \Var(X) = p(1-p) = pq, \qquad q = 1 - p $$ Proof For $p \in [0, 1]$, a discrete probability distribution with the following probability mass function is called a Bernoulli distribution. $$ f(x) = p^{x}(1-p)^{1-x}, \qquad x = 0, 1 $$ Direct Calculation By the definition</description></item><item><title>What is Analysis of Variance or ANOVA in Statistics?</title><link>https://freshrimpsushi.github.io/en/posts/2661/</link><pubDate>Sun, 08 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2661/</guid><description>Definition 1 2 Analyzing variance to compare the population means of two or more groups is called analysis of variance, and it&amp;rsquo;s often abbreviated as ANOVA. Explanation Intuitively, one might think that comparing the sample means would suffice to compare population means, but in the realm of statistics, simply comparing numerical values has little significance. Mean of A Mean of B 0.0142 0.0271 For example, consider the histograms of samples</description></item><item><title>Moment Generating Function of Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3653/</link><pubDate>Sat, 07 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3653/</guid><description>Formula $X \sim$ When $\operatorname{Bin}(1, p)$, the moment generating function of $X$ is given below. $$ m(t) = 1 - p + pe^{t} = q + pe^{t}, \qquad q = 1 - p $$ Proof For $p \in [0, 1]$, a discrete probability distribution with the following probability mass function is referred to as a Bernoulli distribution. $$ f(x) = p^{x}(1-p)^{1-x}, \qquad x = 0, 1 $$ From the Definition</description></item><item><title>Difference Between '>' and '>>' in Batch Command</title><link>https://freshrimpsushi.github.io/en/posts/2660/</link><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2660/</guid><description>Overview In batch commands, &amp;gt; overwrites a string, while &amp;gt;&amp;gt; appends a string. Code For instance, suppose there is a file named example.bat written as follows. @echo off echo 1 &amp;gt; A.txt echo 2 &amp;gt;&amp;gt; A.txt echo 3 &amp;gt;&amp;gt; B.txt echo 4 &amp;gt; B.txt The contents of the resulting A.txt and B.txt upon execution are as follows. Trick to Empty a File echo. &amp;gt; file.txt The echo. with a period</description></item><item><title>Categorical Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3652/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3652/</guid><description>Definition1 Given a sample space with $k (\ge 2)$ categories, $\Omega = \left\{ 1, 2, \dots, k \right\}$, and a probability vector $\mathbf{p} = (p_{1}, \dots, p_{k})$, the discrete probability distribution with the following probability mass function is called the Categorical distribution. $$ p(x = i) = p_{i}, \qquad x \in \left\{ 1, 2, \dots, k \right\} $$ Description The probability of each of the $k$ categories occurring is represented</description></item><item><title>Experimental Design in Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2659/</link><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2659/</guid><description>Definition 1 The subject on which measurement or observation is performed is known as an experimental unit. The independent variable manipulated and varied by the experimenter is referred to as a factor. The intensity at which a factor is set is called a level. The combination of levels of factors is termed as a treatment. The dependent variable measured by the experimenter is called a response. Completely Randomized Design 2</description></item><item><title>How to Input Consecutive Numbers Conveniently in VSCode with Multi-Cursor</title><link>https://freshrimpsushi.github.io/en/posts/2658/</link><pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2658/</guid><description>Guide Even when working with programming languages, using arrays might be cumbersome, or there might be scenarios when using markup languages where numbers need to be sequentially entered starting from 0 or 1. Here, I introduce a method to quickly and easily input multiple numbers using multi-cursor1. Setting Up New Shortcuts Open the Keyboard Shortcut settings in vscode through File/Preferences/Keyboard Shortcut or Ctrl + k → Ctrl + s, then</description></item><item><title>Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3651/</link><pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3651/</guid><description>Definition1 For $p \in [0, 1]$, a discrete probability distribution with the following probability mass function is referred to as a Bernoulli distribution. $$ f(x) = p^{x}(1-p)^{1-x}, \qquad x = 0, 1 $$ Description This distribution is used when describing an experiment with only two possible outcomes, such as a coin toss. Because there are two possible outcomes, $x = 1$ is commonly referred to as a success and $x</description></item><item><title>Estimation of Population Variance for Normally Distributed Groups</title><link>https://freshrimpsushi.github.io/en/posts/2657/</link><pubDate>Sat, 31 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2657/</guid><description>Hypothesis Testing 1 Assume the distribution of a population with a sample size of $n$ follows a normal distribution $N \left( \mu , \sigma^{2} \right)$. The hypothesis test for the candidate $\sigma_{0}$ of the population variance is as follows. $H_{0}$: $\sigma^{2} = \sigma_{0}^{2}$ $H_{1}$: $\sigma^{2} \neq \sigma_{0}^{2}$ Test Statistic The test statistic for the sample variance $S^{2}$ is as follows. $$ \mathcal{X}^{2} = \frac{ \left( n - 1 \right) S^{2}</description></item><item><title>Generalizing Differentiation: Gradient Matrices and Matrix Calculus</title><link>https://freshrimpsushi.github.io/en/posts/3666/</link><pubDate>Fri, 30 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3666/</guid><description>Definition We define the gradient matrix $\nabla_{\mathbf{X}} f$ for a scalar function $f : \mathbb{R}^{n \times n} \to \mathbb{R}$ and a matrix $\mathbf{X} = [x_{ij}] \in \mathbb{R}^{n \times n}$ as follows. $$ [\nabla_{\mathbf{X}} f]_{ij} = \dfrac{\partial f}{\partial x_{ij}} \quad (i,j=1,\dots,n) $$ $$ \nabla_{\mathbf{X}} f = \dfrac{\partial f}{\partial \mathbf{X}} = \begin{bmatrix} \dfrac{\partial f}{\partial x_{11}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f}{\partial x_{1n}} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \dfrac{\partial f}{\partial x_{n1}} &amp;amp;</description></item><item><title>The meaning of the tilde in the input parameters of the batch command</title><link>https://freshrimpsushi.github.io/en/posts/2656/</link><pubDate>Thu, 29 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2656/</guid><description>Overview When receiving input parameters for a batch command, the tilde symbol as in %1 and %~1 determines whether or not to remove double quotes &amp;quot; from the input1. Code For instance, let&amp;rsquo;s say there&amp;rsquo;s an RPEL.bat file written as follows. @echo off echo %1 &amp;gt; result.txt echo %~1 &amp;gt;&amp;gt; result.txt pause When you execute it with the input parameter &amp;quot;space bar&amp;quot; which includes spaces, you obtain the following result.</description></item><item><title>Differences between Python special methods __str__ and __repr__</title><link>https://freshrimpsushi.github.io/en/posts/3650/</link><pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3650/</guid><description>Description __str__ and __repr__ are two special methods in Python used to define the string representation of an object. While these two methods serve similar roles, their purposes and usage contexts differ. Firstly, the name __str__ is derived from &amp;ldquo;string,&amp;rdquo; and it literally means returning the object or its value itself as a string. On the other hand, the name __repr__ comes from &amp;ldquo;representation,&amp;rdquo; and it returns a string that</description></item><item><title>Proof of Cochran's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2655/</link><pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2655/</guid><description>Theorem Let Sample $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ be iid and follow a Normal distribution like $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$. For a symmetric matrix $A_{1} , \cdots , A_{k} \in \mathbb{R}^{n \times n}$ with rank $r_{j}$, suppose the random variable $Q_{1} , \cdots , Q_{k}$ is expressed as a random vector quadratic form $Q_{i} := \mathbf{X}^{T} A_{i} \mathbf{X}$, and</description></item><item><title>How to Mute VSCode (Not Audio Cues)</title><link>https://freshrimpsushi.github.io/en/posts/3649/</link><pubDate>Mon, 26 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3649/</guid><description>Solution When using VSCode and you hear sounds like &amp;lsquo;boop&amp;rsquo;, &amp;lsquo;bop&amp;rsquo;, or &amp;lsquo;beep&amp;rsquo;, you can change the setting in the configuration (Ctrl + ,) by turning off the editor.accessibility support option. Upon searching, you might find suggestions to change the audio cues option, but it seems that this option is not available anymore. Environment Version: 1.98.2 (user setup) Commit: ddc367ed5c8936efe395cffeec279b04ffd7db78 Date: 2025-03-12T13:32:45.399Z Electron: 34.2.0 ElectronBuildId: 11161602 Chromium: 132.0.6834.196 Node.js: 20.18.2</description></item><item><title>How to Add a New Column to the First Column in a Julia DataFrame</title><link>https://freshrimpsushi.github.io/en/posts/2654/</link><pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2654/</guid><description>Code Adding a new column itself is not particularly difficult, but it can be tricky to add it at a specific position. insertcols! julia&amp;gt; df = DataFrame(a = 1:3, b = 4:6) 3×2 DataFrame Row │ a b │ Int64 Int64 ─────┼────────────── 1 │ 1 4 2 │ 2</description></item><item><title>Summary of Python Special Methods</title><link>https://freshrimpsushi.github.io/en/posts/3648/</link><pubDate>Sat, 24 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3648/</guid><description>Overview In Python, you can define operations on a class&amp;rsquo;s instance by defining methods with specific names. These methods are called special methods, also known as magic methods. By prefixing and suffixing these names with __, they are sometimes referred to as dunder methods (double underscore method). __init__ and __call__ __init__: A method for initializing an instance. Its contents are automatically executed when defining an instance. __call__: Executed when the</description></item><item><title>Proof of the Subadditivity of Matrix Rank: rank(A+B) ≤ rankA + rankB</title><link>https://freshrimpsushi.github.io/en/posts/2653/</link><pubDate>Fri, 23 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2653/</guid><description>Theorem The rank of a matrix possesses a quasi-additive property. In other words, for two matrices $A, B$, the following holds. $$ \rank \left( A + B \right) \le \rank A + \rank B $$ Explanation This theorem is used in the proof of Cochran&amp;rsquo;s theorem. Proof 1 Bases for Row Space, Column Space, and Null Space: (a1) Two row equivalent matrices have the same row space, meaning that elementary</description></item><item><title>Maximum Likelihood Estimator of the Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3647/</link><pubDate>Thu, 22 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3647/</guid><description>Theorem Suppose we are given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \operatorname{Laplace}(\mu, b)$ that follows a Laplace distribution. The maximum likelihood estimator $(\hat{\mu}, \hat{b})$ for $(\mu, b)$ is as follows. $$ \hat{\mu} = \text{median}(\mathbf{x}_{1}, \cdots, \mathbf{x}_{n}) $$ $$ \hat{b} = \dfrac{1}{n} \sum\limits_{k=1}^{n} |x_{k} - \mu| $$ Proof Laplace Distribution: The Laplace distribution with parameters $\mu \in \mathbb{R}$ and $b &amp;gt; 0$ is a</description></item><item><title>How to Recursively Retrieve a List of Files under a Specific Path in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2652/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2652/</guid><description>Code walkdir julia&amp;gt; walkdir(&amp;#34;D:\\U\\B&amp;#34;) Channel{Tuple{String, Vector{String}, Vector{String}}}(0) (1 item available) julia&amp;gt; collect(walkdir(&amp;#34;D:\\U\\B&amp;#34;)) 2-element Vector{Tuple{String, Vector{String}, Vector{String}}}: (&amp;#34;D:\\U\\B&amp;#34;, [&amp;#34;b&amp;#34;], [&amp;#34;alpha.txt&amp;#34;, &amp;#34;beta.txt&amp;#34;]) (&amp;#34;D:\\U\\B\\b&amp;#34;, [], [&amp;#34;m.txt&amp;#34;]) walkdir is a built-in function that explores all files under a given path. By itself, it is an iterator containing tuples with file information, which can be accessed as an array through functions like collect. However, dealing with this function without any knowledge can be quite tricky,</description></item><item><title>Moment Generating Function of the Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3646/</link><pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3646/</guid><description>Formula $X \sim$ When $\operatorname{Laplace}(\mu, b)$, the moment-generating function of $X$ is as follows. $$ m(t) = \dfrac{1}{1 - b^{2}t^{2}} e^{\mu t} \qquad \text{for } |t| &amp;lt; \dfrac{1}{b} $$ Proof By the definition of the moment-generating function, $$ \begin{align*} E(e^{tX}) &amp;amp;= \int\limits_{-\infty}^{\infty} e^{tx} f(x) dx \\ &amp;amp;= \int\limits_{-\infty}^{\infty} e^{tx} \dfrac{1}{2b} e^{-|x - \mu|/b} dx \\ &amp;amp;= \dfrac{a}{2}e^{\mu t} \int\limits_{-\infty}^{\infty} e^{ty} e^{-a|y|} dx \qquad (y \equiv x - \mu, \quad a</description></item><item><title>Proof of the Hogg-Craig Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2651/</link><pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2651/</guid><description>Theorem Let sample $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ follow iid Normal distribution like $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$. Consider a symmetric matrix $A_{1} , \cdots , A_{k} \in \mathbb{R}^{n \times n}$ and a random variable $Q_{1} , \cdots , Q_{k}$ represented as a random vector quadratic form $Q_{i} := \mathbf{X}^{T} A_{i} \mathbf{X}$. Define symmetric matrix $A$ and random variable $Q$</description></item><item><title>Mean and Variance of Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3645/</link><pubDate>Sun, 18 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3645/</guid><description>Formula $X \sim$ When $\operatorname{Laplace}(\mu, b)$, the mean and variance of $X$ are as follows. $$ E(X) = \mu $$ $$ \Var(X) = 2b^{2} $$ Proof Direct Calculation By the definition of expectation and integration by parts, $$ \begin{align*} E(X) &amp;amp;= \int\limits_{-\infty}^{\infty} x \dfrac{1}{2b} e^{-|x - \mu|/b} dx \\ &amp;amp;= \int\limits_{-\infty}^{\mu} \dfrac{x}{2b} e^{(x - \mu)/b} dx + \int\limits_{\mu}^{\infty} \dfrac{x}{2b} e^{-(x - \mu)/b} dx \\ &amp;amp;= \left( \left[\dfrac{x}{2b}\left(b e^{(x - \mu)/b}\right)\right]_{-\infty}^{\mu}</description></item><item><title>Tricks for Concatenating Arrays of Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2650/</link><pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2650/</guid><description>Overview In Julia, although you can concatenate arrays using functions like append!, the bang convention implies that the original array is modified as a side effect. To avoid this, we introduce a trick using the splat operator .... Code Merging Arrays of Different Lengths julia&amp;gt; y = [[3, 1, 4], [1, 5], [9, 2]] 3-element Vector{Vector{Int64}}: [3, 1, 4] [1, 5] [9, 2] julia&amp;gt; [y...;] 7-element Vector{Int64}: 3 1 4</description></item><item><title>Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3644/</link><pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3644/</guid><description>Definition1 For $\mu \in \mathbb{R}$ and $b &amp;gt; 0$, a continuous probability distribution $\operatorname{Laplace}(\mu, b)$ with the following probability density function is called the Laplace distribution. $$ f(x) = \dfrac{1}{2b} \exp \left( -\dfrac{|x - \mu|}{b} \right) $$ Explanation Relationship with the Normal Distribution Although it looks similar to the normal distribution, it has an absolute value $| x - \mu |$ instead of a square, giving it a sharper shape.</description></item><item><title>Properties of the Main Diagonal Elements of Positive Definite Matrices</title><link>https://freshrimpsushi.github.io/en/posts/2649/</link><pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2649/</guid><description>Theorem Suppose a positive definite matrix is given as $A = \left( a_{ij} \right) \in \mathbb{C}^{n \times n}$. Sign of the Main Diagonal Elements The sign of the main diagonal elements in $A$, denoted as $a_{ii}$, is the same as the sign of $A$. If $A$ is positive definite, then $a_{ii} &amp;gt; 0$ If $A$ is positive semidefinite, then $a_{ii} \ge 0$ If $A$ is negative definite, then $a_{ii} &amp;lt;</description></item><item><title>Maximum Likelihood Estimation for Linear Regression Model in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3643/</link><pubDate>Wed, 14 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3643/</guid><description>Summary Assume the relationship between data $\mathbf{x}_{i} \in \mathbb{R}^{n}$ and its labels $y_{i} \in \mathbb{R}$ is described by the following linear model. $$ y_{i} = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{i} + \epsilon_{i}, \qquad i = 1, \ldots, K \tag{1} $$ When $K &amp;gt; n$, the parameter $\mathbf{w}_{\text{ML}}$ that maximizes the likelihood is as follows. $$ \mathbf{w}_{\text{ML}} = (\mathbf{X}^{\mathsf{T}} \mathbf{X})^{-1} \mathbf{X}^{\mathsf{T}} \mathbf{y} $$ Here, $\mathbf{y} = \begin{bmatrix} y_{1} &amp;amp; \cdots &amp;amp; y_{K} \end{bmatrix}^{\mathsf{T}}$ and</description></item><item><title>How to suppress output in the batch command</title><link>https://freshrimpsushi.github.io/en/posts/2648/</link><pubDate>Tue, 13 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2648/</guid><description>Code output &amp;gt; nul To omit results written to a file or displayed on the screen, you can redirect the output to a null device. Windows In Windows, instead of null, you use nul with a single letter l, or the uppercase NUL1. @echo off echo 1 &amp;gt; nul echo 2 echo 3 &amp;gt; NUL echo 4 pause For instance, if you execute an example.bat file written as above, the</description></item><item><title>Bayesian Inference in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3642/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3642/</guid><description>Overview Bayesian inference is a statistical method for estimating the distribution of parameters using prior knowledge and observed data based on Bayes&amp;rsquo; theorem. Explanation Assume that a random variable $\mathbf{x}$ follows a probability distribution with parameter $\theta$. The purpose of Bayesian inference is to estimate the distribution of $\theta$ by examining the samples drawn from $\mathbf{x}$. The key point is not the value of $\theta$, but estimating the &amp;ldquo;distribution&amp;rdquo; of</description></item><item><title>Proof of Craig's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2647/</link><pubDate>Sun, 11 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2647/</guid><description>Theorem Let the sample $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ be iid and follow a normal distribution like $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$. For a symmetric matrix $A, B \in \mathbb{R}^{n \times n}$, with respect to the random variables $Q_{1}$ and $Q_{2}$, which are defined as quadratic forms in random vectors like $Q_{1} := \sigma^{-2} \mathbf{X}^{T} A \mathbf{X}$ and $Q_{2} :=</description></item><item><title>Maximum a Posteriori Estimation for Linear Regression Model in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3641/</link><pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3641/</guid><description>Theorem Assume that the relationship between data $\mathbf{x}_{i} \in \mathbb{R}^{n}$ and its labels $y_{i} \in \mathbb{R}$ can be expressed by the following linear model. $$ y_{i} = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{i} + \epsilon_{i}, \qquad i = 1, \ldots, K \tag{1} $$ The parameter $\mathbf{w}_{\text{MAP}}$ that maximizes the posterior probability is as follows. For $\mathbf{y} = \begin{bmatrix} y_{1} &amp;amp; \cdots &amp;amp; y_{K} \end{bmatrix}^{\mathsf{T}}$ and $\mathbf{X} = \begin{bmatrix} \mathbf{x}_{1} &amp;amp; \cdots &amp;amp; \mathbf{x}_{K} \end{bmatrix}^{T}</description></item><item><title>How to Check the Integrated Development Environment (IDE) for Running Julia</title><link>https://freshrimpsushi.github.io/en/posts/2646/</link><pubDate>Fri, 09 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2646/</guid><description>Guide When executing code in an IDE (Integrated Development Environment) such as vscode, it&amp;rsquo;s common to change settings based on the development stage. To check this, you can refer to environment variables like ENV[&amp;quot;TERM_PROGRAM&amp;quot;] == &amp;quot;vscode&amp;quot;. Precautions When executed directly in the terminal, no indication of a program is present, leading to an error being raised as shown above. Hence, when development and execution must be combined, it is advisable</description></item><item><title>Prove that the trace of powers of a diagonalizable matrix is equal to the sum of powers of its eigenvalues</title><link>https://freshrimpsushi.github.io/en/posts/2645/</link><pubDate>Wed, 07 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2645/</guid><description>Theorem Suppose a diagonalizable matrix $A \in \mathbb{C}^{n \times n}$ and a natural number $k \in \mathbb{N}$ are given. Let the eigenvalue of $A$ be $\lambda_{1} , \cdots , \lambda_{n}$, then the following holds. $$ \operatorname{tr} A^{k} = \sum_{i=1}^{n} \lambda_{i}^{k} $$ Here, $\operatorname{tr}$ is the trace. Explanation Although it is not quite a corollary, it is useful to note that when $A \in \mathbb{R}^{n \times n}$ is a symmetric matrix,</description></item><item><title>Mixture Distributions</title><link>https://freshrimpsushi.github.io/en/posts/3639/</link><pubDate>Tue, 06 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3639/</guid><description>Build-up1 Suppose we want to approximate a probability distribution with the probability density function as shown in the image below. One of the basic methods to approximate a probability distribution is to find a normal distribution that closely resembles the distribution we aim to approximate. However, as the following figures show, the distribution we want to approximate has three peaks, making it challenging to approximate using a normal distribution. Here,</description></item><item><title>How to View the First and Last Parts of a DataFrame in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2644/</link><pubDate>Mon, 05 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2644/</guid><description>Overview In Julia, DataFrames do not have head or tail functions. Although it&amp;rsquo;s a bit annoying because it requires an extra step compared to other languages, you can use the first and last functions. Why does DataFrames.jl stubbornly ignore industry conventions? This can roughly be summarized as follows1: : There&amp;rsquo;s already a Base.tail function in Base, and they want to avoid conflicts. first and last were already implemented for the</description></item><item><title>Paper Review: Denoising Diffusion Probabilistic Models (DDPM)</title><link>https://freshrimpsushi.github.io/en/posts/3638/</link><pubDate>Sun, 04 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3638/</guid><description>Overview and Summary A generative model refers to a method to find a probability distribution $Y$ that a given random sample $\left\{ y_{i} \right\}$ follows. As it&amp;rsquo;s highly challenging to directly find this from scratch, it&amp;rsquo;s common to use well-known distributions to approximate the desired distribution. Thus, if a dataset $\left\{ x_{i} \right\}$ following a well-known distribution $X$ is given, the generative model can be regarded as a function $f$,</description></item><item><title>Proof that if all eigenvalues of a symmetric real matrix are either 0 or 1, it is an idempotent matrix</title><link>https://freshrimpsushi.github.io/en/posts/2643/</link><pubDate>Sat, 03 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2643/</guid><description>Theorem If all the eigenvalues of the symmetric matrix $A \in \mathbb{R}^{n \times n}$ are either $0$ or $1$, then $A$ is an idempotent matrix. Explanation This lemma is used in the proof of the equivalence conditions for the chi-square-ness of quadratic forms of normally distributed random vectors and the proof of Cochran’s theorem. The converse does not hold. Proof Spectral Theory:</description></item><item><title>Generative Model</title><link>https://freshrimpsushi.github.io/en/posts/3637/</link><pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3637/</guid><description>Overview Determining the exact probability distribution that our data follows is a crucial yet very challenging problem in many application fields. For instance, if we precisely know the probability distribution of human face photographs and the method to sample from this distribution, we can obtain plausible human face images every time we sample data from this distribution. Obviously, this task is nearly impossible. Much like how many difficult problems begin</description></item><item><title>How to Install unofficial Packages in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2642/</link><pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2642/</guid><description>Guide For unofficial packages, it is sometimes explained as a complex process requiring registration and so on, but in reality, it can be installed just by having the GitHub address1. Suppose the GitHub remote repository address of the package you want to install is https://github.com/JuliaLang/Example.jl. Pkg using Pkg; Pkg.add(url=&amp;#34;https://github.com/JuliaLang/Example.jl&amp;#34;) This is the method using the Pkg package. Package Management Mode ] add https://github.com/JuliaLang/Example.jl This is a method to enter the</description></item><item><title>A Comprehensive Compilation of Mathematical and Scientific Terms (Korean, English, Japanese)</title><link>https://freshrimpsushi.github.io/en/posts/3636/</link><pubDate>Wed, 30 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3636/</guid><description>Overview For Korean terminology, we follow the Korean Mathematical Society(KMS). However, for expressions that are rarely used—for example, using “옹골찬” for “compact”—we instead transliterate the English term and write “컴팩트.” For English and Japanese, we consulted Wikipedia. Click the table&amp;rsquo;s header row (first</description></item><item><title>Proof that the eigenvalues of an idempotent matrix are either 0 or 1</title><link>https://freshrimpsushi.github.io/en/posts/2641/</link><pubDate>Tue, 29 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2641/</guid><description>Theorem The eigenvalues of an idempotent matrix are only $0$ or $1$. Explanation This lemma is used in the proof of the equivalence condition for the chi-squared property of a quadratic form of a normally distributed random vector. For the converse of this theorem to hold, the given idempotent matrix must be a real symmetric matrix. Proof 1 Let $A$ be an idempotent matrix, in other words, $A^{2} = A$.</description></item><item><title>How to Add a Column Filled with the Same Value to a Julia DataFrame</title><link>https://freshrimpsushi.github.io/en/posts/2640/</link><pubDate>Sun, 27 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2640/</guid><description>Code Fundamentally, this is similar to the method to add a new column, but initialization can be easily performed without creating a separate column by applying broadcasting to the assignment operation = using .=. julia&amp;gt; df = DataFrame(rand(3, 4), :auto) 3×4 DataFrame Row │ x1 x2 x3 x4 │ Float64 Float64 Float64 Float64 ─────┼─────────</description></item><item><title>gravitational acceleration</title><link>https://freshrimpsushi.github.io/en/posts/3633/</link><pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3633/</guid><description>Definition The acceleration of an object moving under the influence of gravity is called gravitational acceleration. When an object moves solely under the effect of Earth&amp;rsquo;s gravity, without other external forces such as friction or air resistance, this motion is termed free-fall motion. Explanation When gravity or gravitational field is mentioned simply, it usually refers to Earth&amp;rsquo;s gravity. Earth&amp;rsquo;s gravitational acceleration is approximately $9.8 \mathrm{m/s^{2}}$, commonly denoted as $g$. This</description></item><item><title>Conditions for Equivalence of Chi-Squared Nature in Quadratic Forms of Normal Distribution Random Vectors</title><link>https://freshrimpsushi.github.io/en/posts/2639/</link><pubDate>Fri, 25 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2639/</guid><description>Theorem Let sample $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ follow a normal distribution as iid as $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$. For a symmetric matrix $A \in \mathbb{R}^{n \times n}$ with rank $r \le n$, define the quadratic form of a random vector as $Q = \sigma^{-2} \mathbf{X}^{T} A \mathbf{X}$, then the following holds. $$ Q \sim \chi^{2} (r) \iff A^{2}</description></item><item><title>How to Remove Tick Guides in MATLAB Plots</title><link>https://freshrimpsushi.github.io/en/posts/2638/</link><pubDate>Wed, 23 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2638/</guid><description>Code &amp;gt;&amp;gt; scatter(rand(10, 1), rand(10, 1)) To completely remove the ticks themselves, you can use xtick([]) or ytick([]). However, if you want to remove just the tick guides while keeping the values, you need to adjust the TickLength as shown below1. &amp;gt;&amp;gt; set(gca, &amp;#39;TickLength&amp;#39;, [0, 0]) This trick involves setting the length of the tick guides to zero. According to other documentation, in [x y], x represents the length of</description></item><item><title>The Moment Generating Function of a Quadratic Form of a Normally Distributed Random Vector</title><link>https://freshrimpsushi.github.io/en/posts/2637/</link><pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2637/</guid><description>Theorem Let Sample $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ be iid following a Normal Distribution such as $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} N \left( 0, \sigma^{2} \right)$. Consider a Symmetric Matrix $A \in \mathbb{R}^{n \times n}$ with Rank $r \le n$. The Moment Generating Function of the Quadratic Form of a Random Vector $Q = \sigma^{-2} \mathbf{X}^{T} A \mathbf{X}$ is expressed as follows: $$ M_{Q} (t)</description></item><item><title>How to Add Borders to Figures in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/2636/</link><pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2636/</guid><description>Code &amp;gt;&amp;gt; scatter(rand(10, 1), rand(10, 1)) For example, when plotting scatter plots using MATLAB, the default setting leaves the upper and right edges open, making the border invisible. To make the border visible, use the box on command as shown below1. &amp;gt;&amp;gt; box on Environment OS: Windows MATLAB: 2023b https://kr.mathworks.com/help/matlab/ref/box.html&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Sum of Squares Decomposition Represented by Quadratic Form of a Random Vector</title><link>https://freshrimpsushi.github.io/en/posts/2635/</link><pubDate>Thu, 17 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2635/</guid><description>Formula For a random vector $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$, an identity matrix $I_{n} \in \mathbb{R}^{n \times n}$, and an all-ones matrix $J_{n} \in \mathbb{R}^{n \times n}$ whose elements are all $1$, the following holds: $$ \mathbf{X}^{T} \left( I_{n} - {\frac{ 1 }{ n }} J_{n} \right) \mathbf{X} = ( n - 1 ) S^{2} $$ Here, $S^{2}$ is the sample variance. Derivation $$ \begin{align*} \overline{X}</description></item><item><title>Graph Laplacian</title><link>https://freshrimpsushi.github.io/en/posts/925/</link><pubDate>Wed, 16 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/925/</guid><description>Definition Let the degree matrix of a graph be $D$ and the adjacency matrix be $A$. Then, the graph Laplacian $L$ of $G$ is defined as follows: $$ L := D - A $$ Explanation Unlike the Laplacian $\Delta = \sum\limits_{i} \frac{\partial^{2}}{\partial x_{i}^{2}}$ defined for functions over Euclidean space, it depends on the given graph. For the graph $G = (V, E)$, denote $f : V \to \mathbb{R}$ as the</description></item><item><title>Toeplitz Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2634/</link><pubDate>Tue, 15 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2634/</guid><description>Definition A component $\left( A \right)_{ij}$ of a matrix $A \in \mathbb{R}^{m \times n}$ is said to satisfy $\left( A \right)_{i, j} = \left( A \right)_{i+1, j+1}$ for all $i, j$ if it is called a Toeplitz matrix. In other words, a Toeplitz matrix is a matrix where all elements along a specific diagonal are the same. $$ A = \begin{bmatrix} a_{0} &amp;amp; a_{-1} &amp;amp; a_{-2} &amp;amp; \cdots &amp;amp; a_{-n+1}</description></item><item><title>Graph Embedding, Node Embedding, Edge Embedding</title><link>https://freshrimpsushi.github.io/en/posts/975/</link><pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/975/</guid><description>Definition1 Let the graph $G(V, E)$ be given. The function $f: V \to \mathbb{R}^{n}$ is called node embedding, and the function $g: E \to \mathbb{R}^{m}$ is called edge embedding. For the set of graphs $\mathcal{G} = \left\{ G_{i} \right\}$, $h: \mathcal{G} \to \mathbb{R}^{k}$ is referred to as graph embedding. Explanation Graph/node/edge embedding is a function that maps a graph or its constituents to Euclidean space. To handle the abstract entity</description></item><item><title>Expected Value of the Quadratic Form of a Random Vector</title><link>https://freshrimpsushi.github.io/en/posts/2633/</link><pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2633/</guid><description>Formula Let the population mean vector $\mu \in \mathbb{R}^{n}$ and the covariance matrix $\Sigma \in \mathbb{R}^{n \times n}$ be given such that the random vector $\mathbf{X}$ is $\mathbf{X} \sim \left( \mu , \Sigma \right)$. For a symmetric matrix $A$, the expected value of the quadratic form of a random vector is as follows. $$ E (Q) = \operatorname{tr} A \Sigma + \mu^{T} A \mu $$ Here, $\mu^{T}$ is the transpose</description></item><item><title>Birthday Problem: Probability of Sharing the Same Birthday</title><link>https://freshrimpsushi.github.io/en/posts/998/</link><pubDate>Sat, 12 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/998/</guid><description>Formula Disregarding leap years, let $1$ be the number of years and $365$ be the number of days in a year. The probability that there is at least one pair of people with the same birthday among $n$ people is given by the following formula. $$ p(n) = 1 - \dfrac{365!}{365^n(365-n)!} $$ Explanation I recall that during my middle school years, this formula was introduced in a particular chapter of</description></item><item><title>How to Use l1 Trend Filtering in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2632/</link><pubDate>Fri, 11 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2632/</guid><description>Overview L1TrendFiltering.jl is a Julia package that translates the $\ell_{1}$ trend filtering code originally implemented in MATLAB1. The $\ell_{1}$ finds the solution $x = \left\{ x_{t} \right\}_{t=1}^{n}$ to the following optimization problem for the given time series data $y = \left\{ y_{t} \right\}_{t = 1}^{n}$ and $\lambda \ge 0$2. $$ \argmin_{x} {\frac{ 1 }{ 2 }} \left\| y - x \right\|_{2}^{2} + \lambda \left\| D x \right\|_{1} $$ Here, the</description></item><item><title>Optimizer</title><link>https://freshrimpsushi.github.io/en/posts/1019/</link><pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1019/</guid><description>Definition An optimization problem refers to finding $x_{\ast}$ such that the value of function $f : \mathbb{R}^{n} \to \mathbb{R}$ is minimized. $$ x_{\ast} = \argmin\limits_{x} f(x) $$ A series of algorithms used to solve an optimization problem is called an optimizer. Explanation In machine learning and deep learning, the function $f$ is referred to as the loss function, and in this context, $x$ becomes the parameters of the neural network,</description></item><item><title>Quadratic Form of Random Vector</title><link>https://freshrimpsushi.github.io/en/posts/2631/</link><pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2631/</guid><description>Definition 1 For a random vector $\mathbf{X} = \left( X_{1} , \cdots , X_{n} \right)$ and a symmetric matrix $A \in \mathbb{R}^{n \times n}$, $Q = \mathbf{X}^{T} A \mathbf{X}$ is called a quadratic form. Explanation Since the quadratic form $A = \left( a_{ij} \right)$ is a symmetric matrix, it can be expressed in several ways, as shown below, and is useful in many applications. $$ \begin{align*} &amp;amp; Q \\ =&amp;amp;</description></item><item><title>Complex Step Derivative Approximation</title><link>https://freshrimpsushi.github.io/en/posts/2630/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2630/</guid><description>Formula The derivative $f ' (x)$ of the function $f : \mathbb{R} \to \mathbb{R}$ can be approximated using complex numbers as follows. $$ f ' (x) \approx \frac{\im \left( f \left( x + i h \right) \right)}{h} $$ Here, $\im$ represents the imaginary part of the complex number, and $h$ is a sufficiently small real number. Explanation Complex analysis continues to be fascinating the more you learn about it. Unlike</description></item><item><title>Parity Plot</title><link>https://freshrimpsushi.github.io/en/posts/1053/</link><pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1053/</guid><description>Definition A plot representing the ordered pairs of true values and predicted values as a scatter plot is called a parity plot. Explanation Phonetically, it would sound more like [parety plot], but in this article, it will be referred to as a parity plot in accordance with foreign word notation rules. The word &amp;ldquo;parity&amp;rdquo; implies &amp;lsquo;agreement&amp;rsquo;, so a parity plot can be described as a visual representation showing how well</description></item><item><title>Example of Changing Output with a Button in JavaScript</title><link>https://freshrimpsushi.github.io/en/posts/2629/</link><pubDate>Sat, 05 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2629/</guid><description>Showcase 0 This example starts with a number at 0, increases the number by 1 each time the + button is pressed, and decreases the number by 1 each time the - button is pressed. Code HTML &amp;lt;input type=&amp;#39;button&amp;#39; onclick=&amp;#39;count(&amp;#34;+&amp;#34;)&amp;#39; value=&amp;#39;+&amp;#39;/&amp;gt; &amp;lt;input type=&amp;#39;button&amp;#39; onclick=&amp;#39;count(&amp;#34;-&amp;#34;)&amp;#39; value=&amp;#39;-&amp;#39;/&amp;gt; &amp;lt;div id=&amp;#39;result&amp;#39;&amp;gt;0&amp;lt;/div&amp;gt; Displays the initialized number in &amp;lt;div id='result'&amp;gt;0&amp;lt;/div&amp;gt;. Determines which action will be executed by giving different arguments to onclick='count() according to the button.</description></item><item><title>Test of Homogeneity of Population</title><link>https://freshrimpsushi.github.io/en/posts/894/</link><pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/894/</guid><description>Hypothesis Testing 1 Assume that the categories in multinomial experiments are obtained from $R$ populations, each with $C$ categorical data. When the probability that an element in the $i = 1 , \cdots , R$-th population falls into the $j = 1 , \cdots , C$-th category is $p_{ij}$, denote the proportion vector of the $i$-th population as $\mathbf{p}_{i} = \left( p_{i1} , \cdots , p_{iC} \right)$. The following hypothesis</description></item><item><title>SiLU or Swish Function in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/883/</link><pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/883/</guid><description>Definition 1 2 The SiLU or Swish function is defined as follows. $$ \operatorname{SiLU}(x) = x \cdot \sigma(x) $$ Here, $\sigma$ is a particular case of the sigmoid function, specifically the logistic function $\sigma(x) = \left( 1 + e^{-x} \right)^{-1}$. Explanation The SiLU resembles ReLU in shape, but unlike ReLU, it is not a monotonic function and is smooth. The logistic function has a problem known as gradient vanishing, which</description></item><item><title>Inner Product of Dual Space</title><link>https://freshrimpsushi.github.io/en/posts/1064/</link><pubDate>Mon, 31 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1064/</guid><description>Introduction For the vector space $V$, let&amp;rsquo;s denote $(V, \braket{\cdot, \cdot}_{V})$ as a Hilbert space. Let $V^{\ast}$ represent the dual space of $V$. By the Riesz Representation Theorem, any $f \in V^{\ast}$ can be expressed uniquely in terms of $\mathbf{v}_{f} \in V$ as follows: $$ f = \braket{\cdot, \mathbf{v}_{f}}_{V}; \quad f(\mathbf{x}) = \braket{\mathbf{x}, \mathbf{v}_{f}}_{V} \tag{1} $$ That is, $f \in V^{\ast}$ and $\mathbf{v}_{f} \in V$ are in a one-to-one correspondence.</description></item><item><title>Test of Independence</title><link>https://freshrimpsushi.github.io/en/posts/882/</link><pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/882/</guid><description>Hypothesis Testing 1 In a multinomial experiment, assume we have categorical data with two characteristics $X$ and $Y$, where $X$ has $R$ categories and $Y$ has $C$ categories obtained from $n$ independent trials. The following hypothesis test using the Pearson chi-square test statistic is called the test of independence. $H_{0}$: The two categories are independent. $H_{1}$: The two categories are dependent. Test Statistic The test statistic is the Pearson chi-square</description></item><item><title>How to Interpolate Strings in Python</title><link>https://freshrimpsushi.github.io/en/posts/1068/</link><pubDate>Sat, 29 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1068/</guid><description>f-string f-strings can be used from Python 3.6 and are the simplest and most convenient method of string interpolation. By prefixing the string with an f and using variables within the string as {variable}, you can achieve this. &amp;gt;&amp;gt;&amp;gt; name = &amp;#39;An, Yujin&amp;#39; &amp;gt;&amp;gt;&amp;gt; birthday = &amp;#39;2003&amp;#39; &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print(f&amp;#39;The leader of IVE is {name}, and she was born in {birthday}.&amp;#39;) The leader of IVE is An, Yujin, and she</description></item><item><title>Equivalent Conditions for Discontinuity in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/869/</link><pubDate>Fri, 28 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/869/</guid><description>Theorem 1 A function $f : \mathbb{R} \to \mathbb{R}$ is not continuous at $x_{0}$ if and only if: $$ \exists \epsilon &amp;gt; 0 , \forall \delta &amp;gt; 0 : \exists x ( \delta ) \in \mathbb{R} \left( \left| x ( \delta ) - x_{0} \right| &amp;lt; \delta \land \left| f \left( x ( \delta ) - f \left( x_{0} \right) \right) \right| \ge \varepsilon \right) $$ Explanation While it&amp;rsquo;s not</description></item><item><title>How to Check the PC Name in Python</title><link>https://freshrimpsushi.github.io/en/posts/1069/</link><pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1069/</guid><description>Code Use socket.gethostname(). &amp;gt;&amp;gt;&amp;gt; import socket &amp;gt;&amp;gt;&amp;gt; socket.gethostname() &amp;#39;lab1&amp;#39; See Also Check Operating System platform.system() Check PC Username os.getlogin Check PC Name socket.gethostname() Environment OS: Windows11 Version: Python 3.11.5</description></item><item><title>로치 맵</title><link>https://freshrimpsushi.github.io/en/posts/855/</link><pubDate>Wed, 26 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/855/</guid><description>Definitions 1 A map defined as follows is referred to as the Lozi map. $$ \begin{align*} x \mapsto&amp;amp; 1 - a \left| x \right| + y \\ y \mapsto&amp;amp; b x \end{align*} $$ Description The Lozi map is a map where only $x^{2}$ is replaced by $\left| x \right|$ from the Hénon map. It is continuous but has points $x = 0$ that are non-differentiable. Its</description></item><item><title>Avoid Starting Sentences with And, But, and So When Writing English Papers</title><link>https://freshrimpsushi.github.io/en/posts/1543/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1543/</guid><description>Description When writing academic papers, it is not advisable to start sentences with &amp;ldquo;and,&amp;rdquo; &amp;ldquo;but,&amp;rdquo; or &amp;ldquo;so.&amp;rdquo; First of all, these three words are all conjunctions, so starting a sentence with them is grammatically incorrect. Conjunctions are used to connect two sentences into one, so if the previous sentence ends with a period, the next sentence should not start with and/but/so. While these words are frequently used in speech, colloquial,</description></item><item><title>Julia's Permutation Dimension Function and Its Application permutedims</title><link>https://freshrimpsushi.github.io/en/posts/834/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/834/</guid><description>Overview The built-in function permutedims in Julia can be seen as a generalization of transpose matrix and is useful for handling dimensions of multi-dimensional arrays. Code Tensor Dimension Transformation julia&amp;gt; A = reshape(Vector(1:8), (2,2,2)) 2×2×2 Array{Int64, 3}: [:, :, 1] = 1 3 2 4 [:, :, 2] = 5 7 6 8 julia&amp;gt; B = permutedims(A, (3, 1, 2)) 2×2×2 Array{Int64,</description></item><item><title>Numerical solution of ordinary differential equations with external forces (Julia DifferentialEquations Package)</title><link>https://freshrimpsushi.github.io/en/posts/1083/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1083/</guid><description>Explanation DifferentialEquations.jl is a Julia package for numerical solutions of differential equations. In this article, we will introduce how to find a numerical solution of an ordinary differential equation with a forcing term using DifferentialEquations.jl. Reading the tutorial might be helpful. First-order ODE with Forcing Term Consider the following simple problem. $$ \begin{align*} \dfrac{du(t)}{dt} &amp;amp;= u(t) + \sin(t) \qquad t \in [0, 1] \\ u(0) &amp;amp;= 0.0 \end{align*} $$ Solving</description></item><item><title>Strange Attractor</title><link>https://freshrimpsushi.github.io/en/posts/816/</link><pubDate>Sat, 22 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/816/</guid><description>Definition Chaos 1 A closed set that is invariant and topologically transitive is called an attractor. A compact invariant set that is sensitive to initial conditions and topologically transitive is referred to as chaotic. A chaotic attractor is known as a strange attractor. Sensitive Dependence on Initial Conditions 2 Consider a dynamical system in a metric space $\left( X, d \right)$ defined by a differential equation as shown in $\dot{x}</description></item><item><title>What is a Differential Equation Solver?</title><link>https://freshrimpsushi.github.io/en/posts/1093/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1093/</guid><description>Definition An algorithm used to numerically solve differential equations is called a solver. Explanation For example, if a differential equation is solved using the 4th-order Runge-Kutta method (RK4), one might say, &amp;ldquo;RK4 was chosen as the solver for solving the differential equation.&amp;rdquo;</description></item><item><title>Tangent Points and Transversal Points in Geometry</title><link>https://freshrimpsushi.github.io/en/posts/815/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/815/</guid><description>Terminology Let&amp;rsquo;s denote the intersection of two manifolds as $p$. If two manifolds merely brush against each other, this point is called a tangent point, and if they intersect transversely, it is called a transversal point. Explanation The reason for explaining tangent and transversal points as &amp;rsquo;terms&amp;rsquo; without rigorously defining them mathematically is because these concepts are often used intuitively across mathematics without a need for separate definitions. For instance,</description></item><item><title>Quasiperiodic Orbit</title><link>https://freshrimpsushi.github.io/en/posts/768/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/768/</guid><description>Definition If a system is bounded and its orbits aren&amp;rsquo;t asymptotically periodic, and it does not exhibit sensitive dependence on initial conditions, then the orbit is said to be quasiperiodic1. Alternatively, if the flow $\phi (t)$ of a dynamical system represented by ordinary differential equations is a quasiperiodic function over time $t$, then any orbit passing through any point of $\phi (t)$ can also be termed as a quasiperiodic orbit2.</description></item><item><title>The Net, Moore-Smith Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1133/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1133/</guid><description>Definition1 Given a set $X$ and a directed set $A$, a function $f : A \to X$ from $A$ to $X$ is called a net. Notation For each $a \in A$, if we denote it by $x_{a} = f(a) \in X$, the net $f$ is represented as $(x_{a})_{a \in A}$ or $x_{\centerdot}$2. In other words, $$ x_{\centerdot} : A \to X \\ x_{\centerdot}(a) = x_{a} = f(a) $$ Explanation A</description></item><item><title>Quasiperiodic function</title><link>https://freshrimpsushi.github.io/en/posts/759/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/759/</guid><description>Definition 1 A function $h : \mathbb{R} \to \mathbb{R}^{n}$ is said to be quasiperiodic if it can be expressed in terms of a basic frequency $\omega_{1} , \cdots , \omega_{n}$ and for each $x_{1} , \cdots , x_{n}$ there exists a $2 \pi$-periodic function $H$ such that the following holds $h$. $$ h(t) = H \left( \omega_{1} t , \cdots , \omega_{n} t \right) $$ Explanation A quasiperiodic function is</description></item><item><title>Directed Set</title><link>https://freshrimpsushi.github.io/en/posts/1134/</link><pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1134/</guid><description>Definition 1 Let $(A, \le)$ be a partially ordered set. If for any $a, b \in A$ there exists $c \in A$ satisfying $a \le c$ and $b \le c$, then $(A, \le)$ is called a directed set . Explanation A totally ordered set is a directed set. For the set $X$, the power set $P(X)$ with the subset relation $\subset$ as a partial order on $(P(X), \subset)$ is a</description></item><item><title>The Structure Remains in a State of A</title><link>https://freshrimpsushi.github.io/en/posts/726/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/726/</guid><description>Sentence Structure Let&amp;rsquo;s assume that subject S and adjective A are given. &amp;ldquo;S remains A&amp;rdquo; $\iff$ &amp;ldquo;S가 A한 채로 남아 있다.&amp;rdquo; Examples Adjective &amp;ldquo;그럼에도 불구하고, 작은 사람을 탐지하는 것은</description></item><item><title>The Mandelbrot Set and the Julia Set</title><link>https://freshrimpsushi.github.io/en/posts/718/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/718/</guid><description>Definition $$ P_{c} : z \mapsto z^{2} + c $$ Consider the dynamical system defined by a map and its orbit $P_{c}^{n} (z)$ in the complex plane $\mathbb{C}$ for a given $c \in \mathbb{C}$. Mandelbrot Set 1 The Mandelbrot set is the set $M$ of parameters $c$ for which the orbit $P_{c}^{n} (z)$ does not diverge, given an initial condition $z = 0$. $$ M = \mathbb{C} \setminus \left\{ c</description></item><item><title>Curse of Dimensionality</title><link>https://freshrimpsushi.github.io/en/posts/708/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/708/</guid><description>Definition 1 In Euclidean space $\mathbb{R}^{d}$, the exponential increase in the amount of memory or computation required to solve a problem as the dimension $d$ increases is referred to as the curse of dimensionality. Explanation In many problems, increasing dimensionality means an increase in the number of variables considered. For instance, if the purity $c$ of a chemical substance depends on a model $f$ and oxygen concentration $x$ as in</description></item><item><title>Approximation, Best Approximation</title><link>https://freshrimpsushi.github.io/en/posts/1152/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1152/</guid><description>Definition1 Let $(X, d)$ be a metric space. For a subset $U \subset X$, a mapping $X \to U$ is called an approximation (method). The best approximation for $f \in X$ is defined as follows: $$ u^{\ast} = \argmin_{u \in U} d(f, u) $$ Explanation The term &amp;ldquo;approximation&amp;rdquo; refers to something close, thus &amp;ldquo;approximation to $f$ (an approximation of $f$)&amp;rdquo; means something close to $f$. Mathematically, the concepts of being</description></item><item><title>Ikeda Map</title><link>https://freshrimpsushi.github.io/en/posts/671/</link><pubDate>Sat, 08 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/671/</guid><description>Definition A map that defines a dynamical system in the complex plane $\mathbb{C}$ is known as the Ikeda map. $$ z \mapsto \mu z \exp \left( i \left[ a + {\frac{ b }{ |z|^{2} + 1 }} \right] \right) + c $$ When a complex number is set as $z = x + iy$, the Ikeda map is generally expressed in terms of the real number $x, y$ and parameter</description></item><item><title>Zero-function</title><link>https://freshrimpsushi.github.io/en/posts/1155/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1155/</guid><description>Definition In the Real Space $0 : \mathbb{R} \to \mathbb{R}$ defined as follows is called the zero function. $$ 0(x) = 0 \quad \text{for all } x \in \mathbb{R} $$ In the Vector Space Let the zero vector of the vector space $V$ be denoted by $\mathbf{0}_{V}$. The zero function $\mathbf{0} : V \to V$ defined on $V$ is as follows. $$ \mathbf{0}(\mathbf{v}) = \mathbf{0}_{V} \quad \text{for all } \mathbf{v}</description></item><item><title>2025 Winter Omakase: A Concept Overwhelmed by Form</title><link>https://freshrimpsushi.github.io/en/posts/639/</link><pubDate>Thu, 06 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/639/</guid><description>Introduction The year 2025, the Year of the Blue Snake, has arrived. In this course, instead of introducing difficult and complex new content, we will present examples where the form of equations that repeatedly appear throughout the history of science has swallowed intuition. Menu Vectors What is the first definition of a vector you encountered? As geometry and physics developed after Descartes&amp;rsquo; coordinate system, scholars felt the need to handle</description></item><item><title>Differentiation of Constant Functions</title><link>https://freshrimpsushi.github.io/en/posts/1175/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1175/</guid><description>Formula The derivative of the constant function $C$ is $0$. $$ \dfrac{d C}{dx} = 0 $$ Explanation To be precise, the derivative being a function means &amp;ldquo;the derivative of the constant function is the zero function.&amp;rdquo; Since the zero function is also a constant function, the derivative of the constant function is a constant function. Derivation For all $x \in \mathbb{R}$, let it be $C(x) = c$ ($c \in \mathbb{R}$</description></item><item><title>Riddled Basin</title><link>https://freshrimpsushi.github.io/en/posts/617/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/617/</guid><description>Definition 1 In a dynamical system, assume there are $n$ basins for $R_{1} , \cdots , R_{n}$ different attractors. A set $\mathcal{R}$ is termed a riddled basin if for all $\mathbf{x} \in \mathcal{R}$ and for all $\varepsilon &amp;gt; 0$, the open ball $B \left( \mathbf{x} ; \varepsilon \right)$ is not disjoint from all $R_{1} , \cdots , R_{n}$: $$ \forall \mathbf{x} \in \mathcal{R} , \varepsilon &amp;gt; 0 , k =</description></item><item><title>Antiderivatives and Indefinite Integrals</title><link>https://freshrimpsushi.github.io/en/posts/1177/</link><pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1177/</guid><description>Definition A function $F$ is called an antiderivative of another function $f$ if it satisfies $F^{\prime} = f$. Explanation An antiderivative is translated as 원시함수 (primitive function), 역도함수 (reverse derivative), etc. The process of finding a function $F$ that satisfies $F^{\prime} = f$ for a given $f$, or the function $F$ itself, is called an indefinite integral. The indefinite integral or antiderivative of $f$</description></item><item><title>Another Definition of Mean and Variance</title><link>https://freshrimpsushi.github.io/en/posts/610/</link><pubDate>Sun, 02 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/610/</guid><description>Definition Euclidean Space $\mathbb{R}$ 1 For a random variable $X :\Omega \to \mathbb{R}$, the infimum of the squared deviations&amp;rsquo; expectation $\sigma^{2} (X) \in \mathbb{R}$ is defined as the variance of $X$. $$ \sigma^{2} \left( X \right) := \inf_{a \in \mathbb{R}} E \left[ \left( X - a \right)^{2} \right] $$ The value that minimizes the expectation of squared deviations $\mu (X) \in \mathbb{R}$ is defined as the mean. $$ \mu \left(</description></item><item><title>Subgroup Test</title><link>https://freshrimpsushi.github.io/en/posts/3474/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3474/</guid><description>Theorem One-step subgroup test Let the group $G$ and let $H$ be a nonempty subset of $G$ (empty set가 아닌 부분집합). If whenever $a$ and $b$ are elements of $H$ then $ab^{-1}$ is also an element of $H$, then $H$ is a subgroup of $G$. Equivalently, if whenever $a$ and $b$ are in $H$ then $a-b$ is also in $H$, then</description></item><item><title>Correlation Dimension</title><link>https://freshrimpsushi.github.io/en/posts/606/</link><pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/606/</guid><description>Definition 1 2 In a metric space $\left( X , d \right)$, let&amp;rsquo;s express elements of a set $S = \left\{ x_{1} , \cdots , x_{n} \right\} \subset X$ as $x \in S$. The number of elements in an open ball $B \left( x ; \varepsilon \right)$ with center $x$ and radius $\varepsilon &amp;gt; 0$ is $N_{x} ( \varepsilon )$: $$ N_{x} ( \varepsilon ) := \left| B \left( x</description></item><item><title>Autoencoder</title><link>https://freshrimpsushi.github.io/en/posts/1181/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1181/</guid><description>Definition For two natural numbers $m \ll n$, the function $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is called an encoder. The function $g : \mathbb{R}^{m} \to \mathbb{R}^{n}$ is called a decoder. If $h = g \circ f$ satisfies $h(x) = x$, it is called an autoencoder. Explanation Since the encoder&amp;rsquo;s output dimension is smaller than the input dimension, it can be considered as performing data compression and encryption. On the other</description></item><item><title>Definition of Random Fields</title><link>https://freshrimpsushi.github.io/en/posts/595/</link><pubDate>Wed, 26 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/595/</guid><description>Definition 1 Set-based Definition A stochastic process is defined as a set of random variables. For a parameter set or an index set $T$, a stochastic process over $T$, denoted as $f$, is defined as a set of random variables over $t \in T$, denoted as $f(t)$. If $T$ is $n$-dimensional and $f$ is a $d$-dimensional vector-valued process, it is called a $\left( n , d \right)$ random field. Functional</description></item><item><title>How to Assign Numbers Arbitrarily in LaTeX Enumerate</title><link>https://freshrimpsushi.github.io/en/posts/1188/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1188/</guid><description>Code Using \setcounter{enumi}{n}, the following list starts from $n+1$. \documentclass{article} \begin{document} \begin{enumerate} \setcounter{enumi}{2} \item IVE \item LE SSERAFIM \setcounter{enumi}{7} \item IZ*ONE \setcounter{enumi}{10} \item LOVELYZ \end{enumerate} \end{document}</description></item><item><title>Box-Counting Dimension</title><link>https://freshrimpsushi.github.io/en/posts/584/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/584/</guid><description>Definition 1 2 Suppose there is a bounded set $S \subset \mathbb{R}^{n}$. Define the minimum number of hypercubes with a side length of $\varepsilon$ needed to cover $S$ as $N \left( \varepsilon \right)$. If $d = d \left( S \right)$, defined as follows, exists, then we call $d$ the box-counting dimension of $S$. $$ d \left( S \right) := \lim_{\varepsilon \to \infty} {\frac{ \log N \left( \varepsilon \right) }{ \log</description></item><item><title>Difference between Noise and Artifacts in Images (Signals, Data)</title><link>https://freshrimpsushi.github.io/en/posts/1192/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1192/</guid><description>Overview Noise and artifacts are common factors that degrade the original signal (data) and need to be removed. This document explains the characteristics of these two elements and how they differ from each other. Definition In an image, noise refers to any values that degrade the original image, typically caused by random, unpredictable, and unremovable factors. On the other hand, the degradation caused by removable, regular, or predictable factors is</description></item><item><title>Proof of SST = SSR + SSE in Linear Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/511/</link><pubDate>Sat, 22 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/511/</guid><description>Summary $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ Given $n$ data and $p</description></item><item><title>How to Use Square Brackets in LaTeX Enumerate</title><link>https://freshrimpsushi.github.io/en/posts/1193/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1193/</guid><description>Code In an enumerate environment, round brackets (, ) are used by default. If you want to use square brackets [, ], you should load the package with \usepackage{enumitem} and use it with \begin{enumerate}[label={[}\arabic*{]}]...\end{enumerate}. \documentclass{article} \usepackage{enumitem} \begin{document} \begin{enumerate}[label={[}\arabic*{]}] \item IVE \item LE SSERAFIM \item IZ*ONE \item LOVELYZ \end{enumerate} \end{document}</description></item><item><title>What is a Fractal in Dynamics?</title><link>https://freshrimpsushi.github.io/en/posts/442/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/442/</guid><description>Terminology A fractal is generally referred to as a geometric object possessing self-similarity, and while its examples and concepts are widely recognized, a universally accepted definition remains elusive1. Easy Agreement A complex geometric form possessing intricate structures at arbitrary small scales is called a fractal. Most fractals exhibit some degree of self-similarity2. Difficult Agreement A structure with complex patterns repeated over a wide range of scales, or having a fractal</description></item><item><title>Website for Easily Creating LaTeX Tables</title><link>https://freshrimpsushi.github.io/en/posts/1194/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1194/</guid><description>Description Creating tables in a LaTeX document is quite cumbersome. Unlike typical document editing programs, it&amp;rsquo;s not easy to construct them. Particularly, attempting to draw tables like the one above, where rows or columns are merged, can be quite frustrating. At times like these, you can use the following website to effortlessly create tables. Tables Generator By constructing tables as shown in the image below, similar to editing in Hangul</description></item><item><title>Divide: be divided into</title><link>https://freshrimpsushi.github.io/en/posts/441/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/441/</guid><description>Vocabulary be divided into: This phrase literally means to be separated into parts. The important point is that it uses the preposition &amp;ldquo;into.&amp;rdquo; Example Sentence &amp;ldquo;우선, 패 $T$ 는 서로 다른 연결되지 않은 블럭들로 나뉜다.&amp;rdquo; &amp;ldquo;First, the hand $T$ is divided into</description></item><item><title>How to Make References Appear in Numerical Order When Using \cite{} in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1215/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1215/</guid><description>Code When writing a TeX document, if the bibliography is sorted by &amp;lsquo;order of citation,&amp;rsquo; using \cite{} will automatically arrange them in numerical order (as that is the order in which they are cited). However, if sorted by other criteria like last name, the bibliography numbers will appear in the order in which the code is written, not in numerical order. In this case, using \usepackage{cite} will automatically sort them</description></item><item><title>Similarity Dimension</title><link>https://freshrimpsushi.github.io/en/posts/397/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/397/</guid><description>Definition Let&amp;rsquo;s say a set $\displaystyle A := \lim_{n \to \infty} A_{n}$ is a self-similar set. When a subset of $A_{0}$ similar to $A_{0}$ is called a copy of $A_{0}$, let&amp;rsquo;s call $r$, which when multiplied by the volume of the copy of $A_{0}$ equates to the volume of $A_{0}$, the scale factor. If $A_{1}$ has $m$ mutually disjoint copies of $A_{0}$, then $d$ defined as follows is called the</description></item><item><title>How to Add Hyperlinks to Bibliography Numbers in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1232/</link><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1232/</guid><description>Code Add \usepackage{hyperref} to the preamble of the document. This package automatically generates links within the document and provides various linking functionalities, including referencing. You can specify detailed options, for example, in the following code: colorlinks=true: Apply colors to links (the default is a box display), linkcolor=blue: Set the color of internal links to blue, citecolor=green: Set the color of bibliography (reference) links to green. \usepackage[colorlinks=true, linkcolor=blue, citecolor=green]{hyperref} (Left) Before</description></item><item><title>Role of A in Conducting V</title><link>https://freshrimpsushi.github.io/en/posts/393/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/393/</guid><description>Sentence Structure Let&amp;rsquo;s assume that the verb V and the adjective A are given. &amp;ldquo;play a A role V-ing&amp;rdquo; $\iff$ play an A role in V-ing Example Sentence &amp;ldquo;추가적으로, 0에 근접하는 RMSE를 얻는 것은 더욱 믿을만한 모델을 세우는데에</description></item><item><title>Jacobi's Formula</title><link>https://freshrimpsushi.github.io/en/posts/1234/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1234/</guid><description>Formula Let $A = A(t)$ be a differentiable matrix function. The derivative of the determinant $\det A(t)$ is given by: $$ \dfrac{\mathrm{d}}{\mathrm{d} t} \det A(t) = \Tr \Big( (\operatorname{adj}A(t)) \dfrac{\mathrm{d}A(t)}{\mathrm{d}t} \Big) = \det A(t) \cdot \Tr\left( A^{-1}(t) \dfrac{\mathrm{d}A(t)}{\mathrm{d}t} \right) $$ This is known as Jacobi&amp;rsquo;s formula. In the form of a total differential, it can be written as follows. The second equality holds when $A$ is an invertible matrix. $$</description></item><item><title>von Koch Curve</title><link>https://freshrimpsushi.github.io/en/posts/387/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/387/</guid><description>Definition 1 The $K_{n+1}$ is defined as follows: Divide every line segment of length $K_{n}$ into three equal parts of length $l$. Add an equilateral triangle at the middle section with a side length of $l/3$. Remove the overlapping part of the equilateral triangle and the $K_{n}$. The limit $K \subset \mathbb{R}^{2}$ of such a set is defined as the von Koch Curve. $$ K := \lim_{n \to \infty} K_{n}</description></item><item><title>Implementing a Discrete Fourier Transform Matrix in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1249/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1249/</guid><description>Explanation The Discrete Fourier Transform of an $N$-dimensional vector $\mathbf{x}$ can be represented as a matrix multiplication as follows: $$ \widehat{\mathbf{x}} = F \mathbf{x} $$ Here, if we express $F$ as $\omega = e^{-i2\pi/N}$, $$ F = \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \\ 1 &amp;amp; \omega &amp;amp; \omega^2 &amp;amp; \cdots &amp;amp; \omega^{N-1} \\ 1 &amp;amp; \omega^2 &amp;amp; \omega^4 &amp;amp; \cdots &amp;amp; \omega^{2(N-1)} \\ \vdots &amp;amp;</description></item><item><title>Consideration of Quantum Entropy</title><link>https://freshrimpsushi.github.io/en/posts/386/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/386/</guid><description>Vocabulary take into account: An idiom meaning &amp;rsquo;to consider,&amp;rsquo; frequently used in models or methods to indicate that certain factors have been taken into account. consider: A verb meaning &amp;rsquo;to consider,&amp;rsquo; often used when discussing specific examples or situations. Examples take into account &amp;ldquo;또한 빠른 인구성장으로 인한 환경적 변화에 대한</description></item><item><title>Discrete Fourier Transform Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1250/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1250/</guid><description>Description The discrete Fourier transform is an adaptation of the Fourier transform for finite-dimensional vectors or digital signals. The definition of the discrete Fourier transform is as follows. The linear transformation defined as $\mathcal{F}_{N} : \mathbb{C}^{N} \to \mathbb{C}^{N}$ is called the discrete Fourier transform (DFT). $$ \mathcal{F}_{N}(\mathbf{a}) = \hat{\mathbf{a}},\quad \hat{a}_{m}=\sum_{n=0}^{N-1}e^{-i2\pi mn /N} a_{n}\quad (0\le m &amp;lt; N) $$ Here, $\mathbf{a} = (a_{0}, a_{1}, \dots, a_{N-1})$, $\hat{\mathbf{a}} = (\hat{a}_{0}, \hat{a}_{1}, \dots,</description></item><item><title>Hausdorff Dimension</title><link>https://freshrimpsushi.github.io/en/posts/370/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/370/</guid><description>Definition 1 Assume a metric space $\left( X, d \right)$ is given. The diameter of $S \subset X$ $\diam S$ is defined as follows. $$ \diam S := \sup \left\{ d (x, y) : x, y \in S \right\} $$ Hausdorff Outer Measure Let $S$ be a subset of $X$. For a positive $\delta &amp;gt; 0$, if the union of $U_{k}$ whose diameters are less than $\delta$ $\cup_{k=1}^{\infty} U_{k}$ is</description></item><item><title>Free Online Resources for Artificial Intelligence, Machine Learning, and Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/1253/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1253/</guid><description>Description Textbooks in the fields of artificial intelligence, machine learning, and deep learning are often freely available on the internet. Even in the case of well-known textbooks, there are quite a few that are freely accessible. Here are a few introductions. List Pattern Recognition and Machine Learning (Link) Authored by Christopher Bishop, this book is also well-known as PRML. The Korean translation title is &amp;ldquo</description></item><item><title>영어: X Corresponding to That</title><link>https://freshrimpsushi.github.io/en/posts/369/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/369/</guid><description>Sentence Structure &amp;ldquo;the corresponding X&amp;rdquo; $\iff$ &amp;ldquo;그에 해당하는 X&amp;rdquo; Example Sentence &amp;ldquo;$g$ 가 연속인 도함수를 가진다고 할 때, 이계 미분 방정식 $\ddot{x} + g(x) = 0$ 혹은 그에 대응되는 시스템 $\dot{x} = y , \dot{y} = g(x)$ 을 생</description></item><item><title>Hiding Specific Fields in Biblatex</title><link>https://freshrimpsushi.github.io/en/posts/1275/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1275/</guid><description>Code To exclude specific fields from the information displayed in biblatex, you can set them using the \AtEveryBibitem command. Below are the BibTeX information and rendered result for Walter Rudin&amp;rsquo;s book &amp;ldquo;Principles of Mathematical Analysis.&amp;rdquo; @book{rudin1964principles, title={Principles of mathematical analysis}, author={Rudin, Walter and others}, volume={3}, year={1964}, publisher={McGraw-hill New York} } If you want to exclude the volume and year fields from the above reference, you can write it as follows.</description></item><item><title>Self-Similar Set</title><link>https://freshrimpsushi.github.io/en/posts/368/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/368/</guid><description>Definition 1 For two sets $A, B$, if a bijection $f$ exists that satisfies $f(A) = B$, we say that the two sets $A, B$ are similar. For a set $X$, if there exists a subset $S \subset X$ that is similar to $X$, then $X$ is called a self-similar set. Explanation Note that the definition of &amp;lsquo;similar&amp;rsquo; is somewhat arbitrary and invented by the author. The reference material uses</description></item><item><title>Customizing the Order of References in BibLaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1276/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1276/</guid><description>Code In the preamble of the document (\begin{document} section), you can add the \DeclareSortingScheme command to arbitrarily set the sorting order of the references. To set the first sorting criterion as ascending by name, the second criterion as descending by year, and the third criterion as by title, write as follows. \documentclass{article} \usepackage[backend=biber,sorting=mycustomscheme]{biblatex} % BibLaTeX 설정 \DeclareSortingTemplate{mycustomscheme}{ \sort[direction=ascending]{\field{author}} \sort[direction=descending]{\field{year}} \sort[direction=descending]{\field{title}} } \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography[heading=none] \end{refsection} \end{document} This time,</description></item><item><title>Affine Transformation</title><link>https://freshrimpsushi.github.io/en/posts/365/</link><pubDate>Sun, 02 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/365/</guid><description>Definition Simple Definition Suppose a matrix $A$ and a vector $\mathbf{b}$ are given. The transformation that multiplies the matrix $A$ to the vector $\mathbf{x}$ and adds $\mathbf{b}$ is called an affine transformation. $$ \mathbf{x} \mapsto A \mathbf{x} + \mathbf{b} $$ Complex Definition 1 If $f : V \to V$ defined on a vector space $V$ satisfies the following condition for any scalar $\lambda$, then $f$ is called an affine transformation.</description></item><item><title>How to Set a LaTeX Document Horizontally</title><link>https://freshrimpsushi.github.io/en/posts/1345/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1345/</guid><description>Code If you want to set the document in landscape orientation, you can add \usepackage[landscape]{geometry}. This is useful when creating presentation materials. \documentclass{article} \usepackage[a6paper, landscape]{geometry} \begin{document} with landscape \end{document} \documentclass{article} \usepackage[a6paper]{geometry} \begin{document} without landscape \end{document}</description></item><item><title>Sensitivity to Initial Conditions</title><link>https://freshrimpsushi.github.io/en/posts/364/</link><pubDate>Fri, 31 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/364/</guid><description>Definition 1 Let space $X = \left( \mathbb{R}^{n} , \left\| \cdot \right\| \right)$ and a smooth function $f,g : X \to X$ be given, expressing a vector field and map as follows. $$ \dot{x} = f(x) \\ x \mapsto g(x) $$ $\phi (t, \cdot)$ is the flow of the vector field $\dot{x} = f(x)$, $g^{n}$ is the map applied $n$ times to the map $g$, and let $\Lambda \subset X$</description></item><item><title>IguanaTeX: How to Input TeX in PowerPoint</title><link>https://freshrimpsushi.github.io/en/posts/1366/</link><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1366/</guid><description>Description The native equation editor in PowerPoint is quite crude. Although it somewhat follows TeX syntax, it&amp;rsquo;s virtually unusable, and the most significant issue is that the equations do not look good. For presentation materials, unattractive equations are a critical flaw. Consequently, for presentation materials requiring a lot of equations, TeX&amp;rsquo;s beamer format is often used. However, because TeX was not developed for creating presentation materials, using beamer is essentially</description></item><item><title>X, a Combination of A and B</title><link>https://freshrimpsushi.github.io/en/posts/363/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/363/</guid><description>Sentence Structure &amp;ldquo;a X coupling A and B&amp;rdquo; $\iff$ &amp;ldquo;X, which is a combination of A and B&amp;rdquo; Example &amp;ldquo;이러한 센스에서, 인공신경망과 유전알고리즘을 섞은 메서드는 이질적인 광촉매가 포함된 시뮬레이션과</description></item><item><title>How to Automatically Fill Blank Spaces in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1372/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1372/</guid><description>Code If you want to place any equations or images at the bottom of the document, you can use \vfill. This will fill the empty space vertically. If you want to fill the empty space horizontally, use \hfill. \documentclass{article} \usepackage[a6paper]{geometry} \begin{document} aaaaaaaaaa \hfill bbbbbbbbbbbb \vfill cccccccccc \end{document}</description></item><item><title>Chaos in Systems Described by Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/337/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/337/</guid><description>Definition 1 Consider the following vector field given as a differential equation for the function $f : \mathbb{R}^{n} \to \mathbb{R}^{n}$. $$ \dot{x} = f(x) $$ The orbit $\phi_{t} ( x_{0} )$ of this system at a point $x_{0} \in X$ is said to be chaotic if it satisfies the following conditions: (i): $\phi_{t} ( x_{0} )$ is bounded for $t \ge 0$. (ii): At least one of the Lyapunov exponents</description></item><item><title>Angle Between Two Vectors in an n-Dimensional Euclidean Space</title><link>https://freshrimpsushi.github.io/en/posts/1408/</link><pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1408/</guid><description>Definition1 $n$Vector space of dimension n, for two vectors $\mathbf{v}, \mathbf{u} \in \mathbb{R}^{n}$, the angle between them is defined as the $\theta$ satisfying the following condition. $$ \cos \theta = \dfrac{\mathbf{v} \cdot \mathbf{u}}{\|\mathbf{v}\| \|\mathbf{u}\|} \tag{1} $$ Here, $\cdot$ is the inner product. Explanation In 2D or 3D spaces, since vectors can be visualized as arrows, the concept of the &amp;ldquo;angle between two vectors&amp;rdquo; can be understood intuitively and geometrically represented.</description></item><item><title>In the Vicinity of X</title><link>https://freshrimpsushi.github.io/en/posts/335/</link><pubDate>Sat, 25 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/335/</guid><description>Sentence Structure &amp;ldquo;in the vicinity of X&amp;rdquo; $\iff$ &amp;ldquo;X의 인근에&amp;rdquo; Example Sentence &amp;ldquo;일반적으로 이러한 절차는 트래젝터리 시뮬레이션에서, 특히 불연속점 인근에서</description></item><item><title>A Constant-Magnitude Vector-Valued Function is Orthogonal to Its Derivative</title><link>https://freshrimpsushi.github.io/en/posts/1411/</link><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1411/</guid><description>Theorem1 For a vector function $\mathbf{r} : \mathbb{R} \to \mathbb{R}^{n}$, if $|\mathbf{r}(t)| = c$ then the following holds. ($c$ is a constant) $$ \mathbf{r}(t) \perp \mathbf{r}^{\prime}(t) \quad \forall t $$ Explanation An example can be given for uniform circular motion with a constant radius. In this case, the velocity vector and the acceleration vector are always perpendicular to each other. Proof By the property of the dot product, $$ \mathbf{r}</description></item><item><title>Hénon Map</title><link>https://freshrimpsushi.github.io/en/posts/334/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/334/</guid><description>Definition A Hénon map is defined as a map of a dynamical system as follows. $$ \begin{align*} x \mapsto&amp;amp; 1 - a x^{2} + y \\ y \mapsto&amp;amp; b x \end{align*} $$ Explanation The French astronomer Hénon demonstrated in 1975 that the intriguing phenomena shown by the Poincaré map of a dynamical system expressed as</description></item><item><title>Limits and Continuity of Vector-Valued Functions</title><link>https://freshrimpsushi.github.io/en/posts/1418/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1418/</guid><description>Definition1 Let the vector function $\mathbf{r} : \mathbb{R} \to \mathbb{R}^{3}$ for three scalar functions $f, g, h: \mathbb{R} \to \mathbb{R}$ be given as follows. $$ \mathbf{r}(t) = \left( f(t), g(t), h(t) \right) $$ Define the limit of $\mathbf{r}$ at $a$ as follows. $$ \lim\limits_{t \to a} \mathbf{r}(t) = \left( \lim\limits_{t \to a} f(t), \lim\limits_{t \to a} g(t), \lim\limits_{t \to a} h(t) \right) $$ We say that $\mathbf{r}$ is continuous at</description></item><item><title>Derivative of a Vector-Valued Function</title><link>https://freshrimpsushi.github.io/en/posts/1419/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1419/</guid><description>Definition1 For the vector function $\mathbf{r} : I \subset \mathbb{R} \to \mathbb{R}^{3}$, if the following limit exists, then we say that $\mathbf{r}$ is differentiable at $t$ and its value is called the derivative of $\mathbf{r}$ at $t$. $$ \dfrac{d \mathbf{r}}{d t} = \mathbf{r}^{\prime}(t) := \lim_{h \to 0} \dfrac{\mathbf{r}(t+h) - \mathbf{r}(t)}{h} $$ If for all $t \in I$ there exists $\mathbf{r}^{\prime}(t)$, then we say that $\mathbf{r}$ is differentiable at $I$. When</description></item><item><title>Strange Nonchaotic Attractor (SNA)</title><link>https://freshrimpsushi.github.io/en/posts/326/</link><pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/326/</guid><description>Definition 1 2 A Strange Nonchaotic Attractor is an attractor that possesses the structure of fractal geometry but is not chaotic in a dynamical sense. Explanation In a dynamical sense, something being chaotic means that the largest number in the Lyapunov spectrum is negative, indicating that, although it is a strange attractor, it does not have the important property of being sensitive to initial conditions. Consider, for instance, a gear</description></item><item><title>The Relationship between the Dot Product of Two Vectors and the Angle Between Them</title><link>https://freshrimpsushi.github.io/en/posts/1435/</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1435/</guid><description>Theorem Let the angle between two vectors $\mathbf{a} = (a_{1}, a_{2}, a_{3})$ and $\mathbf{b} = (b_{1}, b_{2}, b_{3})$ be $\theta$. Then, the following holds. $$ \mathbf{a} \cdot \mathbf{b} = |\mathbf{a}| |\mathbf{b}| \cos \theta $$ Here, $\mathbf{a} \cdot \mathbf{b}$ is the dot product (inner product) of the two vectors. Corollary The necessary and sufficient condition for two non-zero vectors $\mathbf{a}$ and $\mathbf{b}$ to be orthogonal is as follows. $$ \mathbf{a} \cdot</description></item><item><title>Mean Arctangent Absolute Percentage Error MAAPE</title><link>https://freshrimpsushi.github.io/en/posts/320/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/320/</guid><description>Definition 1 In a regression problem, for a data point $\left\{ x_{k} \right\}_{k=1}^{n}$ and its prediction $\left\{ \widehat{x}_{k} \right\}_{k=1}^{n}$, the Mean Arctangent Absolute Percentage Error is defined as follows: $$ \text{MAAPE} = {{ 1 } \over { n }} \sum_{k=1}^{n} \arctan \left| {{ x_{k} - \widehat{x}_{k} } \over { x_{k} }} \right| $$ Explanation MAAPE is a metric that resolves the critical flaw of MAPE, which may diverge to infinity</description></item><item><title>How to Highlight (with a Marker) Text and Equations in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1438/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1438/</guid><description>Using the soul package and the \hl command, it is possible to highlight texts, equations, and more. In this case, the xcolor package must also be loaded for the highlight color to appear. Applying this to display mode can be somewhat complex. Refer to the following code: \documentclass{article} \usepackage{xcolor} \usepackage{soul} \begin{document} \hl{asdfasdf} \\ \begin{center} \hl{$\displaystyle \int\limits_{-\infty}^{\infty} f(x) e^{-i\xi x} dx$} \end{center} \end{document}</description></item><item><title>A trajectory without a fixed point has at least one zero Lyapunov exponent.</title><link>https://freshrimpsushi.github.io/en/posts/313/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/313/</guid><description>Theorem Let there be a space $X = \mathbb{R}^{n}$ and a continuous function $f : X \to X$ such that the following vector field is given by the differential equation: $$ \dot{x} = f(x) $$ Assume that the trajectory $x$ of this system is bounded with respect to $t \in [0, \infty)$. If $\left\{ x(t) \right\}$ does not contain a fixed point, then at least one of the Lyapunov exponents</description></item><item><title>How to Merge Multiple TeX Files into One</title><link>https://freshrimpsushi.github.io/en/posts/1440/</link><pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1440/</guid><description>Code \include Use the \include command. Create a new tex file in the same folder as the two files, file1.tex and file2.tex, and write as follows. However, note that a page break will be forced between the two files. \documentclass{article} \usepackage[a5paper]{geometry} \usepackage{kotex} \begin{document} \include{file1} \include{file2} \end{document} \input If you want the contents of the two files to seamlessly continue on the same page, use the \input command. \documentclass{article} \usepackage[a5paper]{geometry} \usepackage{kotex}</description></item><item><title>Mean Absolute Percentage Error MAPE</title><link>https://freshrimpsushi.github.io/en/posts/312/</link><pubDate>Mon, 13 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/312/</guid><description>Definition 1 In a regression problem, for a data point $\left\{ x_{k} \right\}_{k=1}^{n}$ and its prediction $\left\{ \widehat{x}_{k} \right\}_{k=1}^{n}$, the Mean Absolute Percentage Error is defined as follows. $$ \text{MAPE} = {{ 1 } \over { n }} \sum_{k=1}^{n} \left| {{ x_{k} - \widehat{x}_{k} } \over { x_{k} }} \right| $$ Explanation Advantages MAPE provides a highly intuitive interpretation because it explains how well the predictions describe the data in</description></item><item><title>Azimuth and Direction Cosines</title><link>https://freshrimpsushi.github.io/en/posts/1468/</link><pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1468/</guid><description>Definition1 Suppose we are given a three-dimensional vector $\mathbf{a} = (a_{1}, a_{2}, a_{3})$. The angles that $\mathbf{a}$ forms with the $x$-, $y$-, and $z$-axes are denoted as $\alpha$, $\beta$, and $\gamma$, respectively. These are called direction angles. The cosines of the direction angles $\cos \alpha$, $\cos \beta$, and $\cos \gamma$ are called direction cosines. Properties From the definition of direction angles and the properties of the dot product, the direction</description></item><item><title>Food Chain System in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/302/</link><pubDate>Sat, 11 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/302/</guid><description>Model 1 $$ \begin{align*} \dot{R} =&amp;amp; R \left( 1 - {\frac{ R }{ K }} \right) - {\frac{ x_{c} y_{c} C R }{ R + R_{0} }} \\ \dot{C} =&amp;amp; x_{c} C \left( {\frac{ y_{c} R }{ R + R_{0} }} - 1 \right) - {\frac{ x_{p} y_{p} P C }{ C + C_{0} }} \\ \dot{P} =&amp;amp; x_{p} P \left( {\frac{ y_{p} C }{ C + C_{0} }} -</description></item><item><title>Expectation of the Power of Normally Distributed Random Variables with Mean Zero</title><link>https://freshrimpsushi.github.io/en/posts/300/</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/300/</guid><description>Official Random Variable $X$ follows a Normal Distribution $N \left( 0 , \sigma^{2} \right)$, then the expectation of its powers $X^{n}$ can be recursively expressed as follows1. $$ E \left( X^{n} \right) = (n - 1) \sigma^{2} E \left( X^{n-2} \right) $$ $E \left( X^{n} \right)$ is $0$ when $n$ is odd, and for even it is given by the following2. $$ E \left( X^{2n} \right) = \left( 2n -</description></item><item><title>Implementing Automatic Differentiation Forward Mode Using Dual Numbers in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1498/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1498/</guid><description>Overview The forward mode of automatic differentiation can be easily implemented using dual numbers. This document explains how to implement the forward mode in Julia. For background knowledge on dual numbers and automatic differentiation, the following articles are recommended: Dual Numbers Differentiable Real Functions Defined on Dual Number Fields Automatic Differentiation Automatic Differentiation and Dual Numbers Code 1 An example of calculating the automatic differentiation for function $y(x) = \ln</description></item><item><title>Holling type functional response</title><link>https://freshrimpsushi.github.io/en/posts/294/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/294/</guid><description>Overview When assuming the mass action law applied between predator and prey in models like the ordinary differential equations that model an ecosystem, it is often referred to as functional response. Generally, the number of per consumed by unit predator $x$ is expressed as a function against prey density consumption $f(x)$. One can choose from the following three options, known as Holling&amp;rsquo;s type functional responses, types 1, 2, and 3.</description></item><item><title>Differentiable Real Functions Defined on a Dual Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1500/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1500/</guid><description>Build-up1 Assume the given smooth function $f : \mathbb{R} \to \mathbb{R}$. The Taylor series of $f$ at $a$ is as follows: $$ \begin{align*} f(x) &amp;amp;= f(a) + f^{\prime}(a)(x - a) + \dfrac{f^{\prime \prime}(a)}{2!}(x - a)^{2} + \cdots \\ &amp;amp;= f(a) + f^{\prime}(a)(x - a) + \sum_{n=2}^{\infty} \dfrac{f^{(n)}(a)}{n!}(x - a)^{n} \end{align*} $$ Although the above equation is obtained for a function defined on the real space, let&amp;rsquo;s substitute the dual number</description></item><item><title>How to Exit vi or vim on Linux</title><link>https://freshrimpsushi.github.io/en/posts/292/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/292/</guid><description>Overview vi or vim is a text editor renowned for its longevity in Linux systems. While it is admittedly less user-friendly compared to gedit, its prevalence stems from the nature of Linux, where handling files via CLI, often in server environments, is common. Alongside emacs, it remains popular with many users. The issue, much like with scientific calculators, is that its operation can be unintuitive, starting with the challenge of</description></item><item><title>Automatic Differentiation and Dual Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1501/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1501/</guid><description>Overview Dual numbers are numbers that can be expressed in the following form for two real numbers $a, b \in \mathbb{R}$. $$ a + b\epsilon, \quad (\epsilon^{2} = 0,\ \epsilon \neq 0) $$ The addition and multiplication system of dual numbers is useful for implementing the forward mode of automatic differentiation. Description1 In automatic differentiation, especially the forward mode, when computing the function value $f$, the derivative is calculated simultaneously.</description></item><item><title>Systems Represented by Differential Equations and Their Lyapunov Spectrum with Numerical Calculation Methods</title><link>https://freshrimpsushi.github.io/en/posts/287/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/287/</guid><description>Algorithm Assume that a vector field $X = \mathbb{R}^{n}$ for space and function $f : X \to X$ is given by a differential equation $$ \dot{x} = f(x) $$. Define a variational equation for the Jacobian matrix $J$ of $f$ as follows $$ \dot{Y} = J Y $$. Here, the initial condition for the matrix function $Y = Y(t) \in \mathbb{R}^{n \times n}$ is set as the identity matrix $Y(0)</description></item><item><title>Dual Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1502/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1502/</guid><description>Definition The following form for $\epsilon$ satisfying $\epsilon^{2} = 0 (\epsilon \neq 0)$ is called dual numbers. $$ a + b\epsilon,\qquad a, b \in \mathbb{R} $$ Explanation As can be seen from the definition, $\epsilon$ plays a role similar to the $i$ of complex numbers in that it creates the second dimension in the ordered pair. Of course, its properties are entirely different. $$ x + yi = (x, y)</description></item><item><title>Definition of a Normal Matrix</title><link>https://freshrimpsushi.github.io/en/posts/286/</link><pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/286/</guid><description>Definition A square matrix $A \in \mathbb{C}^{n \times n}$ is called a normal matrix ▷normal matrix◁ if it satisfies the following condition: $$ A A^{\ast} = A^{\ast} A $$ Here, $X^{\ast}$ is the conjugate transpose of the matrix $X$. Properties Assume $A$ is a square matrix. The necessary and sufficient condition for the triangular matrix $A$ to be</description></item><item><title>Introduction to KOSIS National Data Portal</title><link>https://freshrimpsushi.github.io/en/posts/284/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/284/</guid><description>Introduction KOSIS National Data Portal is a one-stop statistical service provided by Statistics Korea, allowing users to find desired statistics at once by gathering major domestic, international, and North Korean statistics in one place. It includes all nationally approved statistics on the economy, society, and environment, compiled by over 400 organizations. Requirements You can download data without any requirements and without limits, but creating an account is advisable as being</description></item><item><title>How to not omit author names in BibTeX</title><link>https://freshrimpsushi.github.io/en/posts/1519/</link><pubDate>Sun, 29 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1519/</guid><description>Code In biblatex, the authors are abbreviated as et al. when there are four or more authors. \documentclass[10pt]{article} \usepackage{biblatex} \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography \end{refsection} \end{document} By placing a sufficiently large number in the maxbibnames option, you can display all author names. \documentclass[10pt]{article} \usepackage[maxbibnames=99]{biblatex} \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography \end{refsection} \end{document}</description></item><item><title>Lyapunov Spectrum of Linear Systems</title><link>https://freshrimpsushi.github.io/en/posts/283/</link><pubDate>Sat, 28 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/283/</guid><description>Theorem For the matrix $A \in \mathbb{R}^{n \times n}$, let&amp;rsquo;s assume the following vector field is given as a linear differential equation. $$ \dot{\mathbf{x}} = A \mathbf{x} \qquad , \mathbf{x} \in \mathbb{R}^{n} $$ If $A$ is a symmetric matrix, the Lyapunov spectrum of this system is equal to the eigenvalues of ${\frac{ 1 }{ 2 }} \left( A + A^{T} \right)$. Here, $A^{T}$ is the transpose matrix of matrix $A$.</description></item><item><title>How to Sort Bibliographies Chronologically or Reverse Chronologically in BibTeX</title><link>https://freshrimpsushi.github.io/en/posts/1535/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1535/</guid><description>Code By default, when no settings are applied, the output is sorted in ascending order by first name. \documentclass[10pt]{article} \usepackage{kotex} \usepackage{biblatex} \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography \end{refsection} \end{document} When the package is loaded with the option sorting=ynt, it is sorted in ascending order of year, name (title), and title. \documentclass[10pt]{article} \usepackage{kotex} \usepackage[sorting=ynt]{biblatex} \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography \end{refsection} \end{document} If the option sorting=ydnt is added, it is sorted in descending order. \documentclass[10pt]{article}</description></item><item><title>Introduction to the Administrative Standard Code Management System</title><link>https://freshrimpsushi.github.io/en/posts/280/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/280/</guid><description>Introduction This post introduces the Administrative Standard Code Management System, focusing on the Jurisdiction Code List Inquiry. Though you may not need it immediately, it&amp;rsquo;s beneficial to know the existence of such a system when working with Korean data. Requirements You can download data unlimitedly without any specific requirements. Guide Suppose you need the jurisdiction code for Daegu City: Select &amp;lsquo;Daegu Metropolitan City&amp;rsquo; from the &amp;lsquo;Region Selection&amp;rsquo; dropdown menu. Click</description></item><item><title>How to Choose Coefficients in the Runge-Kutta Method</title><link>https://freshrimpsushi.github.io/en/posts/1536/</link><pubDate>Wed, 25 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1536/</guid><description>Overview1 Let’s consider the ordinary differential equation given as follows. $y$ is a function of $t$, and prime$(^{\prime})$ denotes the derivative with respect to $t$. $$ y^{\prime} = f(t,y),\quad t \ge t_{0},\quad y(t_{0}) = y_{0}, \quad t_{n+1} = t_{n} + h $$ The explicit Runge-Kutta method is a way to approximate $y_{n+1} = y(t_{n+1})$ for a given $y_{n} = y(t_{n})$ as follows: $$ y_{n+1} = y_{n}</description></item><item><title>Definition of Lyapunov Spectrum</title><link>https://freshrimpsushi.github.io/en/posts/272/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/272/</guid><description>Definition Given a space $X = \mathbb{R}^{n}$ and a function $f : X \to X$, suppose that the following vector field is given by a differential equation. $$ \dot{x} = f(x) $$ Simple Definition The Lyapunov numbers and Lyapunov exponents for a multi-dimensional map with respect to the time-$1$ map of the flow $F_{T} (v)$ are each defined as the Lyapunov number and Lyapunov exponent of $F_{T} (v)$1. Difficult Definition</description></item><item><title>How to Define Neural Networks Using Functional API in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/1539/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1539/</guid><description>Description A simple neural network structure can be defined using Flux.Chain, but it is difficult to define a neural network with a complex structure using Chain. In such cases, you can define the neural network using a functional API with the @functor macro. The @functor allows the parameters of a struct-defined neural network to be tracked for performing backpropagation. Code Let&amp;rsquo;s define a neural network consisting of four linear layers.</description></item><item><title>Introduction to Network Data Repository</title><link>https://freshrimpsushi.github.io/en/posts/271/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/271/</guid><description>Introduction The Network Data Repository is a site that provides thousands of diverse networks across more than 30 different topics. Requirements There are no requirements, and you can download without any limitations. Data Example For instance, when examining data such as &amp;lsquo;Bird-Sparrow-Social&amp;rsquo;, it offers a brief description of the data along with tags and a detailed definition of the network elements. Networks are provided by saving the text with the</description></item><item><title>Additive Group of Integer Modulo n</title><link>https://freshrimpsushi.github.io/en/posts/1088/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1088/</guid><description>Definition For an arbitrary natural number $n$, let the set $\mathbb{Z}_{n}$ and the binary operation $+$ be defined as follows. $$ \mathbb{Z}_{n} = \{ 0, 1, 2, \cdots, n-1 \} \\ a + b = (a + b) \mod n $$ Here $\operatorname{mod}$ is the modulo operation. The binary operation structure $(\mathbb{Z}_{n}, +)$ is called the integer modulo (additive) group. It is denoted simply by $\mathbb{Z}_{n}$. Explanation The binary operation</description></item><item><title>Variational Equations</title><link>https://freshrimpsushi.github.io/en/posts/270/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/270/</guid><description>Definition 1 2 Let the space $X = \mathbb{R}^{n}$ and the function $f : X \to X$ be given such that the following vector field is represented by a differential equation: $$ \dot{x} = f(x) $$ For the Jacobian matrix $J$ of $f$, the following is called the variational equation: $$ \dot{Y} = J Y $$ Here, the initial condition of the matrix function $Y = Y(t) \in \mathbb{R}^{n \times</description></item><item><title>Introduction to Baseball Savant</title><link>https://freshrimpsushi.github.io/en/posts/269/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/269/</guid><description>Introduction Baseball Savant is a website that provides data on Major League Baseball (MLB). Requirements There are no specific requirements, and you can download data unlimitedly. Data Example Depending on how you filter, widely known stats can be easily downloaded in *.csv format simply by clicking the &amp;lsquo;Download CSV&amp;rsquo; button, without any significant restrictions. Links Download Fileset: https://baseballsavant.mlb.com/leaderboard/custom?year=2024&amp;amp;type=batter&amp;amp;filter=&amp;amp;min=q&amp;amp;selections=pa%2Ck_percent%2Cbb_percent%2Cwoba%2Cxwoba%2Csweet_spot_percent%2Cbarrel_batted_rate%2Chard_hit_percent%2Cavg_best_speed%2Cavg_hyper_speed%2Cwhiff_percent%2Cswing_percent&amp;amp;chart=false&amp;amp;x=pa&amp;amp;y=pa&amp;amp;r=no&amp;amp;chartType=beeswarm&amp;amp;sort=xwoba&amp;amp;sortDir=desc</description></item><item><title>Memristive Hindmarsh-Rose Neuron Model as a Dynamical System</title><link>https://freshrimpsushi.github.io/en/posts/268/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/268/</guid><description>Overview The Hindmarsh-Rose model is a model that explains the changes in voltage related to ion channels in neurons. The term memristor is a concept combining memory and resistor. This document describes a system that adds a memristor to the Hindmarsh-Rose model. Model 1 The dimensionless system of the memristor-based Hindmarsh-Rose circuit and the equivalent differential equations can be represented as the following non-autonomous non-smooth system. $$ \begin{align*} \dot{x} =&amp;amp;</description></item><item><title>Do not use articles for Figure, Table, and Appendice in English papers</title><link>https://freshrimpsushi.github.io/en/posts/1597/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1597/</guid><description>Description In English academic papers, the first letters of Figure, Table, Appendix, and Diagram are always capitalized. Additionally, articles such as a/an/the are not attached to them. Remember especially not to attach &amp;ldquo;the&amp;rdquo; to terms like Definition, Theorem, and Lemma in mathematics papers. Examples $\sout{\text{The figure}}\text{ 1 shows the relationship between A and B. (X)}$ $\text{Figure 1 shows the relationship between A and B. (O)}$ $\text{See } \sout{\text{the appendix}} \text{</description></item><item><title>Introduction to the Web of Life</title><link>https://freshrimpsushi.github.io/en/posts/261/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/261/</guid><description>Introduction Web of Life is a site offered by the University of Zurich that provides ecosystem network data and related information. Requirements Data can be downloaded without any limitations or specific requirements. Data Examples As the database is global, the network varies depending on the location. Network data is available for download in widely recognized formats such as *.csv, *.json, etc. It also provides histories and brief visualizations of the</description></item><item><title>Difference between Abbreviation and Acronym</title><link>https://freshrimpsushi.github.io/en/posts/1602/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1602/</guid><description>Description The act of shortening long English words or sentences into succinct forms is referred to as abbreviation or acronym. An abbreviation is a reduced form of a word or phrase by omitting some of its letters, and it is translated to Korean as 약어 or 축약어. In the case of abbreviations, a period (.) is placed to indicate the omission of letters. Doctor $\textbf{D}\text{octo}\textbf{r} \mapsto</description></item><item><title>Double Pendulum as a Dynamical System</title><link>https://freshrimpsushi.github.io/en/posts/259/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/259/</guid><description>Model In a plane, a pendulum with another pendulum attached to its end is called a double pendulum1. $$ \begin{align*} \ddot{\phi} =&amp;amp; \left[ 1 - \mu \cos^{2} \left( \psi - \phi \right) \right]^{-1} \left[ \mu g_{1} \sin \psi \cos \left( \psi - \phi \right) \right. \\ &amp;amp; \left. + \mu \dot{\phi}^{2} \sin \left( \psi - \phi \right) \cos \left( \psi - \phi \right) - g_{1} \sin \phi + {\frac{ \mu</description></item><item><title>Numerical Solution of the Wave Equation: K-Space Method</title><link>https://freshrimpsushi.github.io/en/posts/1627/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1627/</guid><description>Method Assume we are given the following wave equation. $$ \partial_{t}^{2} u(\mathbf{x}, t) = \Delta_{\mathbf{x}} u (\mathbf{x}, t), \qquad (\mathbf{x}, t) \in \mathbb{R}^{n} \times \mathbb{R} $$ By taking the Fourier transform of both sides with respect to the variable $\mathbf{x}$, we obtain the following. $$ \partial_{t}^{2} \mathcal{F}_{\mathbf{x}}u(\mathbf{k}, t) = \mathcal{F}_{\mathbf{x}}[\Delta u (\cdot, t)]u(\mathbf{k}, t) = -|\mathbf{k}|^{2} \mathcal{F}_{\mathbf{x}}u(\mathbf{k}, t) $$ Additionally, the left-hand side of the above equation can be approximated</description></item><item><title>How to Read Excel XLSX Files in Julia</title><link>https://freshrimpsushi.github.io/en/posts/242/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/242/</guid><description>Overview Introducing how to read *.XLSX files, which are Excel workbook extensions, in Julia. Although it is somewhat more complex than reading CSV files, in scenarios where you must use *.XLSX files due to the nature of the data or the sheer number of such files, it&amp;rsquo;s inevitable to use them1. Code For instance, assume that the example.xlsx file contains alpha, beta sheets as follows. Specifically, beta has a detached</description></item><item><title>Notation of Expectation Values in Physics</title><link>https://freshrimpsushi.github.io/en/posts/1992/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1992/</guid><description>Definition When the probability density function of a random variable $X$ is $f(x)$, the following value is referred to as the expectation. $$ \braket{X} = \braket{x} = \int_{-\infty}^{\infty} x f(x) dx \tag{1} $$ Explanation In statistics, the expectation is usually denoted as follows: $$ E(X) = \int_{-\infty}^{\infty} x f(x) dx $$ However, in physics, it is common to use single angle brackets to denote it as $(1)$. This is called</description></item><item><title>Feigenbaum Universality</title><link>https://freshrimpsushi.github.io/en/posts/241/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/241/</guid><description>Conjecture $$ x \mapsto f_{\alpha} (x) \qquad , x \in \mathbb{R}^{1} $$ Let us consider a dynamical system represented as a map, as defined above, which exhibits a period-doubling bifurcation with $\alpha$ as the bifurcation parameter. Suppose the sequence of parameters at which the $k$th period-doubling occurs is denoted by $\left\{ \alpha_{k} \right\}_{k=1}^{\infty}$. The ratio of the lengths between them will converge to some constant $\mu_{F} \approx 4.6692 \ldots$1: $$</description></item><item><title>Numerical Solution of the Wave Equation: Finite Difference Method (FDM)</title><link>https://freshrimpsushi.github.io/en/posts/1628/</link><pubDate>Sat, 07 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1628/</guid><description>Method Assume we are given the following one-dimensional wave equation. $$ \dfrac{\partial^{2} u}{\partial t^{2}} = c^{2} \dfrac{\partial^{2} u}{\partial x^{2}}, \qquad 0 \le x \le 1, \quad t \ge 0 \tag{1} $$ Our goal is to approximate the above solution using a finite number of points. Let&amp;rsquo;s discretize the spacetime domain as follows. $$ \left\{ (\ell \Delta x, n\Delta t) : \ell=0,1,\dots,d+1,\ n\ge 0 \right\}\quad \text{ where } \Delta x =</description></item><item><title>How to Check the Operating System in Julia</title><link>https://freshrimpsushi.github.io/en/posts/233/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/233/</guid><description>Overview Julia is a language specialized for scientific computing, and thus you often need to deploy and execute programs on multiple servers, necessitating writing code that can handle various operating systems. Code To check the operating system, use the Sys module1. Sys.iswindows julia&amp;gt; Sys.islinux() false julia&amp;gt; Sys.iswindows() true In addition to the most commonly used islinux and iswindows, there are also isapple, isbsd, isfreebsd, and isunix. Notably, isunix encompasses all</description></item><item><title>Absorbing Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/1631/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1631/</guid><description>Overview Methods for numerically calculating the propagation of waves include the Finite Element Method (FEM) and the $k$-space method. These methods assume that waves propagate infinitely, but in actual simulations, waves propagate within a finite grid. This can cause reflections at the boundaries. To solve this, grids can be set much larger than the actual region of interest, or boundary conditions can be applied to prevent wave reflections. These boundary</description></item><item><title>Neimark-Sacker Bifurcation</title><link>https://freshrimpsushi.github.io/en/posts/190/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/190/</guid><description>Definition Simple Definition Neimark-Sacker Bifurcation is a bifurcation in which an invariant closed curve arises or disappears at a fixed point as a parameter changes in a dynamical system1. Complex Definition Let&amp;rsquo;s denote $n \ge 2$. $$ \dot{x} \mapsto f \left( x , \alpha \right) \qquad , x \in \mathbb{R}^{n} , \alpha \in \mathbb{R}^{1} $$ Given that the dynamical system $f$ is smooth under $x$ and $\alpha$. Suppose $\bar{x}$ is</description></item><item><title>Integration of Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1647/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1647/</guid><description>Summary Suppose the power series $\sum\limits_{n = 0}^{\infty} c_{n}x^{n}$ converges at $\left| x \right| \lt R$. We then define the function $f$ as follows: $$ f(x) = \sum\limits_{n = 0}^{\infty} c_{n}x^{n} \qquad \left| x \right| \lt R \tag{1} $$ Then the function $f$ is integrable at $(-R, R)$, and its indefinite integral is as follows: $$ \int f(x) dx = C + \sum\limits_{n = 0}^{\infty} \dfrac{c_{n}}{n + 1} x^{n+1} \qquad</description></item><item><title>How to Swap the Horizontal and Vertical Axes in Julia</title><link>https://freshrimpsushi.github.io/en/posts/179/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/179/</guid><description>Overview This article introduces a method to swap the horizontal and vertical axes in Julia. Code By using permute = (:y, :x) as an argument in the plot function, you can change the order of the axes. bar(name, height) bar(name, height, permute = (:y, :x)) Full Code using Plots name = [&amp;#34;Gaeul&amp;#34;, &amp;#34;Yujin&amp;#34;, &amp;#34;Rei&amp;#34;, &amp;#34;Wonyoung&amp;#34;, &amp;#34;Liz&amp;#34;, &amp;#34;Leeseo&amp;#34;] height = [164, 173, 170, 173, 171, 165] bar(name, height) bar(name, height, permute</description></item><item><title>The limit of n^(1/n)</title><link>https://freshrimpsushi.github.io/en/posts/1649/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1649/</guid><description>공식 $$ \lim \limits_{n \to \infty} \sqrt[n]{n} = 1 $$ $$ \lim \limits_{n \to \infty} \sqrt[n]{\dfrac{1}{n}} = 1 $$ 증명 $\sqrt[n]{n}$ 대신 $\ln \sqrt[n]{n}$의 극한을 구하면 쉽다. $$ \lim\limits_{n \to \infty} \ln \sqrt[n]{n} = \lim\limits_{n \to \infty} \dfrac{\ln n}{n} $$ $\dfrac{\infty}{\infty}$ 꼴이므로 로피탈 정</description></item><item><title>Period Doubling Bifurcation</title><link>https://freshrimpsushi.github.io/en/posts/146/</link><pubDate>Sat, 30 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/146/</guid><description>Definition Easy Definition Period-doubling bifurcation is a bifurcation where the period of a periodic orbit doubles or halves depending on the change of a parameter in a dynamical system. Hard Definition $$ \dot{x} \mapsto f \left( x , r \right) \qquad , x \in \mathbb{R}^{n} , r \in \mathbb{R}^{1} $$ Let&amp;rsquo;s assume the dynamical system $f$ is $x$ and $\alpha$ smooth. If $\bar{x}$ is a hyperbolic fixed point of this</description></item><item><title>Convergence of Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1665/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1665/</guid><description>Theorem1 Power Series $\sum\limits_{n = 0}^{\infty} c_{n} (x - a)^{n}$&amp;rsquo;s radius of convergence is $R$. Then, The series converges absolutely within $x \in (a - R, a + R)$. The series converges uniformly within any closed interval $[b, d] \subset (a - R, a + R)$. Regarding $(R \lt \infty$, the series diverges beyond $x \notin [a - R, a + R]$. Explanation Refer to here for the proof of</description></item><item><title>How to Access a Windows Server from Linux Clients using SAMBA</title><link>https://freshrimpsushi.github.io/en/posts/143/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/143/</guid><description>Overview Samba is software that provides file-sharing capabilities between Windows and Linux. This post explains how to access shared files by connecting to a Windows server from a Linux client. Guide Windows (Server) 1 Step 1. Enable Samba Features Navigate to &amp;lsquo;Control Panel - Programs - Programs and Features - Turn Windows features on or off&amp;rsquo;. Find and enable &amp;lsquo;SMB 1.0/CIFS File Sharing Support&amp;rsquo;. Step 2. Share Directory In the</description></item><item><title>Differentiation of Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1704/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1704/</guid><description>정리1 멱급수 $\sum\limits_{n = 0}^{\infty} c_{n}x^{n}$이 $\left| x \right| \lt R$에서 수렴한다고 하자. 그리고 함수 $f$를 다음과 같이 정의하자. $$ f(x) = \sum\limits_{n = 0}^{\infty} c_{n}x^{n} \qquad \left| x \right| \lt R \tag{1}</description></item><item><title>Hyperbolicity of Limit Cycles in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/140/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/140/</guid><description>Definition Euclidean space $\mathbb{R}^{n}$ and open set $U \subset \mathbb{R}^{n}$, consider a continuous function $f : U \to \mathbb{R}^{n}$ with the following vector field given by a differential equation. $$ \dot{x} = f(x) $$ Let $P : \Sigma \to \Sigma$ denote the Poincaré map defined on a manifold $\Sigma$ that is traversed by the limit cycle $L_{0}$ of this system. Assume that a</description></item><item><title>Convergence Properties of Series</title><link>https://freshrimpsushi.github.io/en/posts/1737/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1737/</guid><description>정리 두 급수 $\sum a_{n}$과 $\sum b_{n}$이 수렴하면, 급수 $\sum c a_{n}$ ($c$는 상수), $\sum (a_{n} \pm b_{n})$도 수렴하고 다음이 성립한다. $\sum\limits_{n = 1}^{\infty} c a_{n} = c</description></item><item><title>Accessing Linux Remotely via Chrome and Solutions to the Black Screen Issue</title><link>https://freshrimpsushi.github.io/en/posts/139/</link><pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/139/</guid><description>Overview In the case of Linux, Chrome Remote Desktop is not as straightforward as it is on Windows. There is an issue where only a black screen and cursor are visible, making it unusable. Let&amp;rsquo;s explore ways to resolve this. Although it&amp;rsquo;s not a simple task, this guide is written to be as easy to follow as possible, even for beginners. Guide We will refer to the client attempting to</description></item><item><title>p-Series and the p-Series Test</title><link>https://freshrimpsushi.github.io/en/posts/1754/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1754/</guid><description>정의1 다음과 같은 급수를 $p$-급수라고 한다. $$ \sum\limits_{n=1}^{\infty} \dfrac{1}{n^{p}} $$ 설명 역제곱수의 무한합에 대한 일반화이다. 아래에서 소개할 판정법은 $p$-급수에 대해서만 쓸 수 있</description></item><item><title>Hyperbolicity of Fixed Point in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/128/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/128/</guid><description>Definition 1 Let&amp;rsquo;s consider a space $\mathbb{R}^{p}$ and a smooth function $f , g : \mathbb{R}^{p} \to \mathbb{R}^{p}$, such that the dynamical system can be represented as follows by a vector field or a map. $$ \dot{x} = f(x) \\ x \mapsto g(x) $$ We denote the eigenvalues obtained from the Jacobian matrix at a fixed point $\overline{x}$ as $\lambda_{1} , \cdots , \lambda_{p}$. Depending on whether it is a</description></item><item><title>Limit Comparison Test</title><link>https://freshrimpsushi.github.io/en/posts/1756/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1756/</guid><description>Summary1 Given two series $\sum a_{n}$ and $\sum b_{n}$, let us assume $a_{n}, b_{n} \gt 0$. If there exists a positive number $c \gt 0$ such that $$ \lim\limits_{n \to \infty} \dfrac{a_{n}}{b_{n}} = c $$ is satisfied, then either both series converge, or both diverge. Explanation This is called the limit comparison test. The comparison test is intuitive and useful, but it can only determine the convergence of a series</description></item><item><title>How to Use the Symbol of Pi on the Axis in Julia</title><link>https://freshrimpsushi.github.io/en/posts/127/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/127/</guid><description>Overview In Julia, this document explains how to directly use the pi symbol on ticks. It doesn&amp;rsquo;t necessarily have to be pi; you can apply in various forms by passing the tick locations and labels as tuples. Code For xticks, provide the tick locations and labels in the form (A, B) where A indicates the location and B represents the label. Full Code using Plots piticks = [0, π,</description></item><item><title>Comparison Test</title><link>https://freshrimpsushi.github.io/en/posts/1759/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1759/</guid><description>발췌한 문서: 정리1 두 급수 $\sum a_{n}$과 $\sum b_{n}$에 대해서 $a_{n}, b_{n} \gt 0$이라 하자. 그러면 다음이 성립한다. 만약 $\forall n \ a_{n} \le b_{n}$이고 $\sum b_</description></item><item><title>Hopf Bifurcation</title><link>https://freshrimpsushi.github.io/en/posts/125/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/125/</guid><description>Definition Hopf bifurcation is a bifurcation in which the stability of a fixed point changes as a parameter varies, leading to the appearance or disappearance of a new periodic orbit in a dynamical system. Normal Form 1 Let&amp;rsquo;s consider the complex number $z$ represented as $z = x + iy$ or in polar coordinates as $z = r e^{i \phi}$. The Hopf bifurcation can be categorized into two types: supercritical</description></item><item><title>Absolute Convergence and Conditional Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1760/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1760/</guid><description>정의1 급수 $\sum\limits_{n=0}^{\infty} a_{n}$에 대해서, $\sum\limits_{n=0}^{\infty} |a_{n}|$이 수렴하면 $\sum\limits_{n=0}^{\infty} a_{n}$이 절대수렴한다고 말한다. 설명 주의할 점은 주어진 급수가 아니라 &amp;ldq</description></item><item><title>Country-Specific ISO3 and Latitude Longitude Data</title><link>https://freshrimpsushi.github.io/en/posts/124/</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/124/</guid><description>Introduction Rather than being a specific site, this is a Gist page where you can get ISO alpha-2, alpha-3 codes as well as the average latitude and longitude for the entire territory. Although maintained by an individual, it is extremely useful for simple visualizations or data handling. Requirements There are no requirements; it can be downloaded without any limitations. Example of Data Download You can receive a backup file as</description></item><item><title>Ratio Test</title><link>https://freshrimpsushi.github.io/en/posts/1771/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1771/</guid><description>Theorem1 For the series $\sum\limits_{n=0}^{\infty} a_{n}$, let $\lim\limits_{n \to \infty} \left| \dfrac{a_{n+1}}{a_{n}} \right| = L$. (a) If $L &amp;lt; 1$, the series absolutely converges. (b) If $L &amp;gt; 1$ or $L = \infty$, the series diverges. (c) If $L = 1$, the convergence cannot be determined. Explanation If $L = 1$, the convergence cannot be determined, and another method must be used to judge whether the series converges or diverges.</description></item><item><title>Infinite Period Bifurcation</title><link>https://freshrimpsushi.github.io/en/posts/123/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/123/</guid><description>Definition Infinite-period bifurcation is a bifurcation where the limit cycle, including a saddle point and a stable node, appears or disappears as the parameter of a dynamical system changes. The period converging to this limit cycle must diverge to infinity with the changing parameter. Description An infinite-period bifurcation is a bifurcation in which the flow diverges to infinity for the period as it approaches the limit cycle. This is a</description></item><item><title>Root Test</title><link>https://freshrimpsushi.github.io/en/posts/1779/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1779/</guid><description>Summary1 For the series Series $\sum\limits_{n=0}^{\infty} a_{n}$, let $\lim\limits_{n \to \infty} \sqrt[n]{\left| a_{n} \right|} = L$ be given. (a) If $L &amp;lt; 1$, the series absolutely converges. (b) If $L &amp;gt; 1$ or $L = \infty$, the series diverges. (c) If $L = 1$, it cannot be determined. Explanation If $L = 1$, it cannot be determined, so other methods must be used to judge whether the series converges or</description></item><item><title>How to Easily Create Large Rectangular Matrices in PowerPoint</title><link>https://freshrimpsushi.github.io/en/posts/120/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/120/</guid><description>Overview When you want to insert a rectangular matrix in PowerPoint, it is often difficult to determine if there is a formal method available before resorting to manual effort. We have discovered a helpful tip related to two-dimensional formats and will introduce it here. Guide Syntax In an expression, after inputting a special black square character ■ followed by a number 8 or larger, open a parenthesis and follow the</description></item><item><title>Radius of Convergence of Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1791/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1791/</guid><description>Summary1 For the given power series $\sum\limits_{n=0}^{\infty} c_{n}(x - a)^{n}$, let $\alpha$ and $R$ be defined as follows. $$ \alpha = \limsup\limits_{n \to \infty} \sqrt[n]{|c_{n}|}, \qquad R = \dfrac{1}{\alpha} $$ Then the series converges if $\left| x - a \right| \lt R$, and diverges if $\left| x - a \right| \gt R$. If $\alpha = 0$, let $R = \infty$, and if $\alpha = \infty$, let $R = 0$. Definition</description></item><item><title>Heteroclinic Bifurcation</title><link>https://freshrimpsushi.github.io/en/posts/119/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/119/</guid><description>Definition Heteroclinic Bifurcation is a bifurcation in which a heteroclinic orbit appears or disappears as the parameters of a dynamical system change. Explanation The heteroclinic bifurcation, as its name suggests, involves heteroclinic orbits. It is helpful to imagine it as a scenario where the manifold connecting two fixed points joins or breaks as parameters change. Since the indication of being part of a heteroclinic orbit cannot be observed by examining</description></item><item><title>Notation and Naming Conventions of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1825/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1825/</guid><description>Definition The hyperbolic sine function is defined as the linear combination of two exponential functions $\frac{1}{2}e^{x} - \frac{1}{2}e^{-x}$ and is denoted as follows: $$ \sinh x := \dfrac{e^{x} - e^{-x}}{2} $$ Similarly, the hyperbolic cosine function is defined as the linear combination of two exponential functions $\frac{1}{2}e^{x} + \frac{1}{2}e^{-x}$ and is denoted as follows: $$ \cosh x := \dfrac{e^{x} + e^{-x}}{2} $$ Explanation The names and notations of $\sinh$ and</description></item><item><title>How to Easily Create Large Square Matrices in PowerPoint</title><link>https://freshrimpsushi.github.io/en/posts/115/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/115/</guid><description>Overview In PowerPoint, when you want to include a large square matrix, you had no choice but to do it manually. However, I&amp;rsquo;ve discovered a tip related to the two-dimensional format that I would like to introduce. Guide Enter the size N of the matrix by typing a special character ■ followed by the matrix size N in the equation, then click the [Equation]-[2D format] button. As an example that</description></item><item><title>Uniform Convergence and Integrability of Function Series</title><link>https://freshrimpsushi.github.io/en/posts/1829/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1829/</guid><description>정리1 구간 $[a, b]$에서 적분가능한 함수들의 수열 $\left\{ f_{n} : f_{n} \text{ is integrable on } [a, b] \right\}$이 $[a, b]$에서 $f$로 균등 수렴한다고 하자. $$ f_{n} \rightrightarrows f $$ 그러</description></item><item><title>호모클리닉 바이퍼케이션</title><link>https://freshrimpsushi.github.io/en/posts/114/</link><pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/114/</guid><description>Definition Homoclinic Bifurcation is a bifurcation where homoclinic orbits appear or disappear as the parameters of the dynamical system change. Description Homoclinic bifurcation, as its name suggests, is related to homoclinic orbits. It can be imagined as an image where a limit cycle and saddle point meet as their positions change with the parameters. Since one cannot deduce the inclusion of homoclinic orbits by merely observing the vicinity of the</description></item><item><title>Uniform Convergence and Differentiation</title><link>https://freshrimpsushi.github.io/en/posts/1836/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1836/</guid><description>Theorem1 $$ f^{\prime} (x) = \lim_{n \to \infty} f_{n}^{\prime} (x) \quad a \le x \le b $$ Explanation2 The result of the theorem can be summarized as &amp;ldquo;the limit of the derivatives is equal to the derivative of the limits&amp;rdquo;. In other words, it is possible to interchange the limit symbol and the differentiation symbol. $$ \dfrac{d}{dx} \lim\limits_{n \to \infty} f_{n} (x) = \lim\limits_{n \to \infty} \dfrac{d}{dx} f_{n} (x) \quad</description></item><item><title>How to Align Based on the Equal Sign in PowerPoint</title><link>https://freshrimpsushi.github.io/en/posts/113/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/113/</guid><description>Overview This guide introduces the method to align equations by the equal sign in PowerPoint. Guide 1. Equation Let&amp;rsquo;s assume that the first line of the equation is given as above. 2. Line Break Use shift + enter to maintain the equation while breaking the line. If you don&amp;rsquo;t break the line this way, it will be treated as two separate equations. 3. Ampersand &amp;amp; Indicate the part to be</description></item><item><title>Special Angles of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/1849/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1849/</guid><description>공식 몇몇의 특수한 각도에 대한 삼각함수의 함숫값은 다음과 같다. 라디안(각도) $0$ $\frac{\pi}{12} (15^{\circ})$ $\frac{\pi}{6} (30^{\circ})$ $\frac{\pi}{4} (45^{\circ})$ $\frac{\pi}{3} (60^{\circ})$ $\frac{\pi}{2} (90^{\circ})$ $\sin$ $0$ $\dfrac{\sqrt{6} - \sqrt{2}}{4}$ $\dfrac{1}{2}$ $\dfrac{\sqrt{2}}{2}$ $\dfrac{\sqrt{3}}{2}$ $1$ $\cos$ $1$ $ \dfrac{\sqrt{6} + \sqrt{2}}{4}$ $\dfrac{\sqrt{3}}{2}$ $\dfrac{\sqrt{2}}{2}$ $\dfrac{1}{2}$ $0$ $\tan$ $0$ $2 - \sqrt{3}$</description></item><item><title>Homoclinic Orbit and Heteroclinic Orbit</title><link>https://freshrimpsushi.github.io/en/posts/107/</link><pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/107/</guid><description>Definition1 Let the space $X$ and the function $f : X \to X$ be given. Consider the following vector field presented in the form of a differential equation. $$ \dot{x} = f(x) $$ A flow of this system can be represented as $\phi (t, x)$. Homoclinic Orbit For a certain fixed point $x_{0}$, a curve satisfying the following $\phi$ is called a homoclinic orbit. $$ \lim_{t \to \pm \infty} \phi</description></item><item><title>How to Edit and Save Text (txt) Files in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3728/</link><pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3728/</guid><description>write() write(path, string) The basic function write() saves a .txt file whose contents are the input string. julia&amp;gt; fm = &amp;#34;프로미스나인\n송하영\n박지원\n이채영\n이나경\n백지헌&amp;#34; &amp;#34;프로미스</description></item><item><title>Taylor Series and Maclaurin Series</title><link>https://freshrimpsushi.github.io/en/posts/1854/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1854/</guid><description>Build-Up1 Assume that the given function $f$ is expressed as a power series. $$ f(x) = c_{0} + c_{1}(x - a) + c_{2}(x - a)^{2} + c_{3}(x - a)^{3} + \cdots \qquad |x - a| \lt R \tag{1} $$ In this context, finding the power series representation of the function $f$ is equivalent to determining the coefficients of each term $c_{n}$. By substituting $x = a$ into both sides, we</description></item><item><title>How to Read Text (txt) Files as Strings in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3727/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3727/</guid><description>Description There are several ways to load a text file. A simple approach is to use the functions read(), readline(), and readlines(). Among these, read() reads the entire file and returns a string, readline() reads the first line from the file and returns a string, and readlines() reads all lines of the file and returns a vector whose elements are the strings of each line. If you do not specify</description></item><item><title>Knuth's Up-Arrow Notation: Why ^ is Used for Exponentiation in Programming</title><link>https://freshrimpsushi.github.io/en/posts/105/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/105/</guid><description>Definition Knuth&amp;rsquo;s up-arrow notation is a notation devised to represent large integers. Addition, Multiplication, and Exponentiation Addition $x + n$ is defined as follows: $$ x + n = x + \underbrace{1 + \cdots + 1}_{n \text{ times}} $$ Multiplication $xn$ is defined as follows: $$ xn = \underbrace{x + \cdots + x}_{n \text{ times}} $$ Exponentiation $x^{n}$ is defined as follows: $$ \begin{align*} x^{n} =&amp;amp; x \uparrow n \\</description></item><item><title>Applications of Taylor Series</title><link>https://freshrimpsushi.github.io/en/posts/1861/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1861/</guid><description>Explanation Taylor series (Maclaurin series) is an approximation of a given function as a power series, and the Taylor series of function $f$ is as follows. $$ \sum\limits_{n=0}^{\infty} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} = f(a) + f^{\prime}(a)(x-a) + \dfrac{f^{\prime \prime}(a)}{2!} (x-a)^{2} + \cdots $$ Under good conditions, $f$ and its Taylor series are indeed the same. $$ f(x) = \sum\limits_{n=0}^{\infty} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} = f(a) + f^{\prime}(a)(x-a) + \dfrac{f^{\prime \prime}(a)}{2!} (x-a)^{2} + \cdots $$</description></item><item><title>Nullclines in Mathematical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/99/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/99/</guid><description>Definition Isocline Given a set of curves, the set of points with the same slope is called an isocline. Nullcline Given an $p$-dimensional ordinary differential equation as in $\dot{\mathbf{x}} = \mathbf{f} \left( \mathbf{x} \right)$, the isocline with the slope $0$ for the $k$ coordinate $x_{k}$ is called a nullcline. More specifically, the $N_{k}$ nullcline for the $k$ coordinate can be defined as follows: $$ N_{k} := \left\{ \mathbf{x} = \left(</description></item><item><title>Recurrence Relation</title><link>https://freshrimpsushi.github.io/en/posts/1865/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1865/</guid><description>Definition Let&amp;rsquo;s consider a sequence $\left\{ a_{n} \right\}$. At this time, expressing $a_{n}$ as a function of $a_{n-1}$, $a_{n-2}$, $\cdots$, and $a_{1}$ is called a recurrence relation. Explanation For instance, the sequence of natural numbers $\left\{ 1, 2, 3, 4, \dots \right\}$ can be expressed by the following recurrence relation. $$ a_{n} = a_{n-1} + 1, \qquad a_{1} = 1 $$ The coefficients of the Legendre polynomial are expressed by</description></item><item><title>Hellinger Distance of Probability Distributions</title><link>https://freshrimpsushi.github.io/en/posts/91/</link><pubDate>Sun, 27 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/91/</guid><description>Definition The following distance function defined on probability distributions themselves is called the Hellinger distance. Discrete 1 Let $p, q$ be a probability mass function. The Hellinger distance of $p, q$ is defined as: $$ H \left( p , q \right) := \sqrt{ \frac{1}{2} \sum_{k} \left( \sqrt{p_{k}} - \sqrt{q_{k}} \right)^{2} } $$ Continuous 2 Let $f, g$ be a probability density function. The Hellinger distance of $f, g$ is defined</description></item><item><title>Continued Fraction</title><link>https://freshrimpsushi.github.io/en/posts/1876/</link><pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1876/</guid><description>Definition A fraction of the form as shown below is called a continued fraction. $$ a_{0} + \dfrac{1}{a_{1} + \dfrac{1}{a_{2} + \dfrac{1}{a_{3} + \dfrac{1}{\ddots + \dfrac{1}{a_{n}}}}}} \tag{1} $$ Explanation1 2 $(1)$ is denoted as $[a_{1}, a_{2}, \dots, a_{n}]$. Naturally, one can also consider taking the limit of it. For example, let&amp;rsquo;s consider a sequence with the recurrence relation given by $a_{n+1} = 1 + \dfrac{1}{1 + a_{n}}$ and $a_{1} =</description></item><item><title>Tipping Points in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/87/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/87/</guid><description>Terminology In dynamical systems, experiencing a sudden transition from one state to another is referred to as the phenomenon of tipping point1. Description The phenomenon of tipping point is discussed primarily in contexts where returning to the original state after a transition is impossible. Examples include a sudden deterioration from a healthy state (in physiology), abrupt extinction of species (in ecology), or the melting of Arctic glaciers (in climatology). Although</description></item><item><title>Integral Test</title><link>https://freshrimpsushi.github.io/en/posts/1900/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1900/</guid><description>빌드업1 $$ \sum\limits_{n=1}^{\infty} \dfrac{1}{n^{2}} = 1 + \dfrac{1}{2^{2}} + \dfrac{1}{3^{2}} + \dfrac{1}{4^{2}} + \cdots $$ 위과 같은 급수가 수렴하는지 발산하는지 알고싶은 상황이라고 하자. 이를 위해 $\dfrac{1}{n^{2}} = f(n)$을 만족하는 함수를 생각하자</description></item><item><title>Graph Edit Distance Between Graphs</title><link>https://freshrimpsushi.github.io/en/posts/85/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/85/</guid><description>Definition 1 Let&amp;rsquo;s denote a finite set of vertices as $X$ and a set of a finite alphabet as $\alpha$, where the alphabet is considered to include spaces or nulls. A graph is defined as a triple $G = (X, V, E)$ concerning vertex labeling $V : X \to \alpha$ and edge labeling $E : X \times X \to \alpha$. That $\hat{G} = \left( \hat{X}, \hat{V}, \hat{E} \right)$ is a</description></item><item><title>Subsequence</title><link>https://freshrimpsushi.github.io/en/posts/1905/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1905/</guid><description>정의 수열 $\left\{ a_{n} \right\}$이 주어졌다고 하자. 자연수의 수열 $\left\{ n_{k} : n_{i} \lt n_{i+1} \right\}_{ k \in \mathbb{N}}$ 에 대해 $\left\{ a_{n_{k}} \right\}_{ k \in \mathbb{N}}$ 를 $\left\{ a_{n} \right\}_{ n \in \mathbb{N}}$ 의 부분수열이라 한다. 만약 부분</description></item><item><title>Hysteresis Phenomena in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/81/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/81/</guid><description>Definition The phenomenon where changes in a dynamical system due to parameter variations are irreversible is called hysteresis1. Explanation As an example, consider the fold bifurcation shown above. For simplicity, let&amp;rsquo;s imagine $x$ represents the population of bees, and $r$ represents the average temperature. The bifurcation diagram reveals that the system can have up to three fixed points, with two of them being stable fixed points, meaning the bee population</description></item><item><title>Limits of Subsequences and Convergence of Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1911/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1911/</guid><description>정리 수열 $\left\{ a_{n} \right\}$이 주어졌다고 하자. 두 부분수열 $\left\{ a_{2n} \right\}$과 $\left\{ a_{2n+1} \right\}$에 대해서, $\lim\limits_{n \to \infty} a_{2n} = L$이고 $\lim\limits_{n \to \infty} a_{2n+1}</description></item><item><title>Spectral Distance Between Graphs</title><link>https://freshrimpsushi.github.io/en/posts/80/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/80/</guid><description>Definition Let us consider two graphs $G_{1}$ and $G_{2}$ with the number of vertices being $n$, and their adjacency matrices denoted as $A_{1}$ and $A_{2}$ respectively. We represent their eigenvalues, which are sorted in descending order, i.e., their spectra, as follows. $$ A_{1} \mapsto \lambda_{1}^{(1)} \ge \cdots \ge \lambda_{n}^{(1)} \\ A_{2} \mapsto \lambda_{1}^{(2)} \ge \cdots \ge \lambda_{n}^{(2)} $$ The spectral distance between the two graphs is defined as follows1 2.</description></item><item><title>Alternating Series</title><link>https://freshrimpsushi.github.io/en/posts/1925/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1925/</guid><description>Definition A series in which the sign of each term alternates is called an alternating series. In other words, for $b_{n} \gt 0$, a series whose general term is expressed in the following form. $$ a_{n} = (-1)^{n-1}b_{n} \qquad \text{ or } \qquad a_{n} = (-1)^{n}b_{n} $$ Explanation One method to determine the convergence of an alternating series is the Alternating Series Test. Alternating Series Test An alternating series $\sum\limits_{n</description></item><item><title>Saddle-Node Bifurcation</title><link>https://freshrimpsushi.github.io/en/posts/79/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/79/</guid><description>Definition Simple Definition Saddle-node bifurcation is a bifurcation in a dynamical system where fixed points are created or annihilated as a parameter changes1. Complex Definition $$ \dot{x} = f \left( x , r \right) \qquad , x \in \mathbb{R}^{n} , r \in \mathbb{R}^{1} $$ Let us assume that in a given dynamical system, $f$ is smooth with respect to $x$ and $\alpha$. If $\bar{x}$ is a hyperbolic fixed point of</description></item><item><title>Alternating Series Test</title><link>https://freshrimpsushi.github.io/en/posts/1927/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1927/</guid><description>Summary1 The alternating series satisfying the following conditions $\sum\limits_{n = 1}^{\infty} (-1)^{n-1}b_{n}$ $(b_{n} \gt 0)$ converges. $b_{n+1} \le b_{n} \quad \forall n$. $\lim\limits_{n \to \infty} b_{n} = 0$. Proof First, consider the partial sum up to the even terms. $$ \begin{align*} s_{2} &amp;amp;= b_{1} - b_{2} \ge 0 \\ s_{4} &amp;amp;= s_{2} + (b_{3} - b_{4}) \ge s_{2} \\ s_{6} &amp;amp;= s_{4} + (b_{5} - b_{6}) \ge s_{4} \\ &amp;amp;\quad</description></item><item><title>Hamming Distance</title><link>https://freshrimpsushi.github.io/en/posts/78/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/78/</guid><description>Definition For a natural number $n \in \mathbb{N}$, the Hamming distance is defined in the set $\left\{ 0, 1 \right\}^{n}$ of code points of length $n$ as the distance function $H$ given below1. $$ H \left( \mathbf{x} , \mathbf{y} \right) = \operatorname{card} \left\{ k : x_{k} \ne y_{k} \right\} $$ Here, code points are represented by $\mathbf{x} = \left( x_{1} , \cdots , x_{n} \right)$ and $\mathbf{y} = \left( y_{1}</description></item><item><title>Alternating Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/1928/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1928/</guid><description>Definition The following series is called the alternating harmonic series. $$ \sum\limits_{n = 1}^{\infty} (-1)^{n-1}\dfrac{1}{n} = 1 - \dfrac{1}{2} + \dfrac{1}{3} - \dfrac{1}{4} + \cdots $$ Convergence The alternating harmonic series converges. $$ \sum\limits_{n = 1}^{\infty} (-1)^{n-1}\dfrac{1}{n} = \ln 2 $$ Explanation On the other hand, the harmonic series diverges. $$ \sum\limits_{n = 1}^{\infty} \dfrac{1}{n} = 1 + \dfrac{1}{2} + \dfrac{1}{3} + \dfrac{1}{4} + \cdots = \infty $$</description></item><item><title>Transcritical Bifurcation</title><link>https://freshrimpsushi.github.io/en/posts/77/</link><pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/77/</guid><description>Definition 1 2 Transcritical Bifurcation is a bifurcation in a dynamical system where two fixed points exchange stability as a parameter of the system varies. Normal Form The normal form of a transcritical bifurcation is given by: $$ \dot{x} = rx - x^{2} $$ Diagram The bifurcation diagram of a transcritical bifurcation is as follows: Explanation The normal form of a transcritical bifurcation always has two fixed points, $x_{1} =</description></item><item><title>Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/1938/</link><pubDate>Sat, 12 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1938/</guid><description>Definition The following series is called the harmonic series. $$ \sum\limits_{n = 1}^{\infty} \dfrac{1}{n} = 1 + \dfrac{1}{2} + \dfrac{1}{3} + \dfrac{1}{4} + \cdots $$ Explanation It is a representative counterexample to the divergence test. That is, the harmonic sequence converges, but the harmonic series diverges. $$ \lim\limits_{n \to \infty} \dfrac{1}{n} = 0 \quad \text{ but } \quad \sum\limits_{n = 1}^{\infty} \dfrac{1}{n} = \infty $$ On the other hand, the</description></item><item><title>Salt and Pepper Noise</title><link>https://freshrimpsushi.github.io/en/posts/76/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/76/</guid><description>Definition The noise that appears as small dots in an image in white or black is called salt-and-pepper noise. Example For example, the occurrence of salt-and-pepper noise in the image above means that small dots are scattered throughout the image, as shown below. Description Unlike the Gaussian noise, which typically makes the image look blurry, salt-and-pepper noise occurs when extreme values of either $0$ or $1$ are assigned regardless of</description></item><item><title>Divergence Test</title><link>https://freshrimpsushi.github.io/en/posts/1940/</link><pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1940/</guid><description>Summary If the series $\sum\limits_{n = 1}^{\infty} a_{n}$ converges, then the sequence $\{a_{n}\}$ converges to $0$. $$ \sum\limits_{n = 1}^{\infty} a_{n} \text{ is convergent } \implies \lim\limits_{n \to \infty} a_{n} = 0 $$ Proof Let the sum of the series be $\sum\limits_{n = 1}^{\infty} a_{n} = s$. That is, for the partial sum $s_{n}$, it is $\lim\limits_{n \to \infty} s_{n} = s$. Then, since $a_{n} = s_{n} - s_{n-1}$, $$</description></item><item><title>Positive Semidefinite Matrices and Their Real Powers</title><link>https://freshrimpsushi.github.io/en/posts/69/</link><pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/69/</guid><description>Definition For a positive-definite matrix $A \ge 0$ and a real number $t \in \mathbb{R}$, the $t$-power of $A$ is defined as follows. $$ A^{t} := \exp \left( t \log A \right) $$ Here, $\exp$ and $\log$ are respectively the matrix exponential and matrix logarithm. Explanation Generally, the power of a matrix $A^{t}$ is defined as taking the product of the matrix $t$ times for a natural number $t \in</description></item><item><title>Limits of Geometric Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1949/</link><pubDate>Tue, 08 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1949/</guid><description>Summary The geometric sequence $\left\{ r^{n} \right\}$ converges to $-1 \lt r \le 1$, and its value is as follows: $$ \lim\limits_{n \to \infty} r^{n} = \begin{cases} 0 &amp;amp; \text{if } -1 \lt r \lt 1 \\ 1 &amp;amp; \text{if } r = 1 \end{cases} $$ Proof $r = 1$ If $r = 1$, $$ \lim\limits_{n \to \infty} 1^{n} = \lim\limits_{n \to \infty} 1 = 1 $$ ■ $-1 \lt</description></item><item><title>Mathematical Graph Layouts</title><link>https://freshrimpsushi.github.io/en/posts/68/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/68/</guid><description>Overview In mathematics, a layout of a graph or network can be summarized as an algorithm that dictates how vertices and edges are arranged when visualized in 2D or 3D. Code For convenience, all code is written based on Julia which is publicly available on GitHub1. The example graph used is generated by the Barabási-Albert model.</description></item><item><title>Geometric Series</title><link>https://freshrimpsushi.github.io/en/posts/1951/</link><pubDate>Sun, 06 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1951/</guid><description>Definition1 The following series is called a geometric series for $a \ne 0$. $$ a + ar + ar^{2} + ar^{3} + \cdots = \sum_{n=0}^{\infty} ar^{n} $$ Explanation It is the infinite sum of a geometric sequence with the first term $a$ and the common ratio $r$. The ▷eq04 Definition1 The following series is called a geometric series for $a \ne 0$. $$ a + ar</description></item><item><title>Matrix Unwinding Function</title><link>https://freshrimpsushi.github.io/en/posts/67/</link><pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/67/</guid><description>Definition Aiming to generalize the exponential function $\exp$ and the logarithmic function $\log$ to matrices. Matrix Exponential The generalization of the exponential function to matrices $\exp : \mathbb{C}^{n \times n} \to \mathbb{C}^{n \times n}$ is defined for a matrix $A \in \mathbb{C}^{n \times n}$ as follows: $$ \exp A := \sum_{k=0}^{\infty} \frac{A^{k}}{k!} $$ The expression $\exp A$ can also be simply written as $e^{A}$, and it is called the matrix</description></item><item><title>How to Remove Indentation in the Entire Document in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1953/</link><pubDate>Fri, 04 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1953/</guid><description>Code Add \setlength{\parindent}{0pt}. \documentclass{article} \begin{document} Me and my girlies. We gon party til its early. Got me feeling otherworldly tonight. Caught in some traffic. But the radio is blasting. Drop a red light and we&amp;#39;ll sing it goodbye. Ooh. By the morning, feel like magic. I got all I need you know nothing else can beat. The way that I feel when I&amp;#39;m dancing with my girls. Perfect energy yeah</description></item><item><title>Obtaining Colors with Values from 0 to 1 in the Julia Color Scheme</title><link>https://freshrimpsushi.github.io/en/posts/46/</link><pubDate>Thu, 03 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/46/</guid><description>Overview To obtain colors in the range from 0 to 1 in Julia&amp;rsquo;s color scheme, use the get function1. Although this function originally exists in Base, it has been overloaded through using ColorSchemes. Code The color scheme used in the example is colorschemes[:plasma]. colorschemes[:plasma] By inputting a value between 0 and 1 as the second argument to the get function, you can obtain the exact color at that position in</description></item><item><title>ulem: Packages Related to Underlining in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1959/</link><pubDate>Wed, 02 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1959/</guid><description>Description1 The ulem package is related to underline functions $\LaTeX$. It allows you to use underline, double underline, wavy underline, strikethrough, diagonal strikethrough, dashed underline, and dotted underline. Code \documentclass[10pt]{article} \usepackage{ulem} \begin{document} underlined text like \uline{important} \\ % 밑줄 double-underlined text like \uuline{urgent} \\ % 겹밑줄 wavy underline like \uwave{boat} \\ % 물결 밑줄 line struck through word like \sout{wrong} \\ % 취소선</description></item><item><title>Hermitian Matrix's Loewner Order</title><link>https://freshrimpsushi.github.io/en/posts/26/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/26/</guid><description>Definition Loewner Order Let&amp;rsquo;s assume two matrices $A, B \in \mathbb{C}^{n \times n}$ are Hermitian matrices. If $\left( A - B \right)$ is positive semidefinite, it is denoted as $A \ge B$, and if $\left( A - B \right)$ is positive definite, it is denoted as $A &amp;gt; B$. This kind of partial order $\ge$, $&amp;gt;$ are called the Loewner order. Explanation Unlike commonly considered scalars, and indeed even with</description></item><item><title>How to Use Strikethrough in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1960/</link><pubDate>Mon, 30 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1960/</guid><description>Description1 You can apply strikethrough using either the \sout from the ulem package or the st from the soul package. Code \sout It works well with equations, but it doesn&amp;rsquo;t seem to work properly in display mode. \documentclass[10pt]{article} \usepackage{amsmath, amssymb} \usepackage{ulem} \begin{document} \sout{The ulem package provides various types of underlining} that can stretch between words and be broken across lines. Use it with {\LaTeX} or plain \TeX. \\ \sout{$x^{2} +</description></item><item><title>Introduction to the Julia Natural Language Processing Package TextAnalysis.jl</title><link>https://freshrimpsushi.github.io/en/posts/24/</link><pubDate>Sun, 29 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/24/</guid><description>Overview This section introduces TextAnalysis.jl, a natural language processing package for Julia1. For practical natural language processing, packages like Snowball.jl for stemming are also used2. Code julia&amp;gt; using TextAnalysis julia&amp;gt; crps = Corpus(StringDocument.([ &amp;#34;To be or not to be&amp;#34;, &amp;#34;It should be a question to be&amp;#34;, &amp;#34;It is not a question&amp;#34; ])) A Corpus with 3 documents: * 3 StringDocument&amp;#39;s * 0 FileDocument&amp;#39;s * 0 TokenDocument&amp;#39;s * 0 NGramDocument&amp;#39;s Corpus&amp;#39;s</description></item><item><title>Paper Review: Kolmogorov-Arnold Neural Network (KAN)</title><link>https://freshrimpsushi.github.io/en/posts/322/</link><pubDate>Sun, 29 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/322/</guid><description>Overview and Summary Kolmogorov–Arnold Networks (KAN) are neural networks inspired by the Kolmogorov–Arnold representation theorem. Despite the idea having been discussed for decades, authors like Girosi and Poggio pointed out in the 1989 paper &amp;lsquo;Representation Properties of Networks:</description></item><item><title>How to Order Alphabetically with enumerate in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1961/</link><pubDate>Sat, 28 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1961/</guid><description>Code You need to add \usepackage{enumitem}. \begin{enumerate}[label=(\alph*)] \item Lorem Ipsum \item Lorem Ipsum is simply dummy text of the printing and typesetting industry. \item Lorem Ipsum has been the industry&amp;#39;s standard dummy text ever since the 1500s, \item when an unknown printer took a galley of type and scrambled it to make a type specimen book. \end{enumerate} \begin{enumerate}[label=(\Alph*)] \item Lorem Ipsum \item Lorem Ipsum is simply dummy text of the</description></item><item><title>Hermitian Matrix Spaces and Convex Cones of Positive Semidefinite Matrices</title><link>https://freshrimpsushi.github.io/en/posts/19/</link><pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/19/</guid><description>Definition Let&amp;rsquo;s denote as $n \in \mathbb{N}$. Hermitian Matrix Space The set of Hermitian matrices of size $n \times n$ is denoted as follows. $$ \mathbb{H}_{n} := \left\{ A \in \mathbb{C}^{n \times n} : A = A^{\ast} \right\} $$ Positive Definite Matrix Set The set of positive definite matrices of size $n \times n$ is denoted as $\mathbb{P}_{n}$. Theorem $\mathbb{H}_{n}$ is a Vector Space [1]: With respect to the scalar</description></item><item><title>How to Pass Multiple Keyword Arguments at Once Using a Dictionary in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1962/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1962/</guid><description>Explanation Using a dictionary and a splat operator, you can pass multiple keyword arguments to a function at once. This technique is useful when you need to apply the same options to multiple plots. If all the plots need the same options, using the function default is straightforward. However, if even one plot needs to be drawn with a different style, using default can become inconvenient. Code You can create</description></item><item><title>Introduction to Julia's Symbolic Computing Package Symbolics.jl</title><link>https://freshrimpsushi.github.io/en/posts/18/</link><pubDate>Wed, 25 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/18/</guid><description>Overview This introduces Symbolics.jl, a package that supports symbolic algebra systems in Julia1. This package provides an extremely intuitive and powerful interface along with Julia&amp;rsquo;s basic syntax. Difference from SymEngine.jl Symbolics.jl is implemented natively in Julia and has many better aspects in terms of performance and interface. SymEngine.jl, introduced in the post on how to do symbolic operations in Julia, is originally a library written in C++ and wrapped in</description></item><item><title>Parametric Equation</title><link>https://freshrimpsushi.github.io/en/posts/1963/</link><pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1963/</guid><description>Build-Up Let&amp;rsquo;s consider the situation of expressing the position of a particle on a two-dimensional plane in a formula. The path of the particle&amp;rsquo;s movement is shown in the following figure. It is impossible to represent the path in the above figure as a function of $x$, i.e., in the form of $y = f(x)$. This is because there are multiple $y$ values corresponding to points like $x_{0}$. (A function</description></item><item><title>Definition of Cone and Convex Cone</title><link>https://freshrimpsushi.github.io/en/posts/8/</link><pubDate>Mon, 23 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/8/</guid><description>Definition 1 Cone A cone is defined in a vector space $V$ as a subset $C \subset V$ that satisfies the following for all scalars $a &amp;gt; 0$ and $x \in C$: $$ ax \in C $$ Flat Cone and Salient Cone If cone $V$ satisfies $-\mathbf{v} \in V$ for some non-zero vector $\mathbf{v} \in V$, it is called a flat cone. If not, it is called a salient cone.</description></item><item><title>Implementing DeepONet Paper Step by Step (PyTorch)</title><link>https://freshrimpsushi.github.io/en/posts/1153/</link><pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1153/</guid><description>Overview DeepONet is a neural network architecture for learning nonlinear operators and has been applied in various fields such as solving partial differential equations since its paper was published. In this article, we introduce how to implement DeepONet using PyTorch and follow the problems presented in the paper. Paper Review Implementation in Julia DeepONet Theory Let $X$, $X^{\prime}$ be function spaces, and operator $G : X \to X^{\prime}$ be as</description></item><item><title>Comprehensive Guide to File I/O in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3707/</link><pubDate>Sat, 21 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3707/</guid><description>Overview This document summarizes packages and functions related to file I/O in Julia. Summary CSV: CSV.jl Reading CSV.read(&amp;quot;file_name.csv&amp;quot;, DataFrame) CSV.File(&amp;quot;file_name.csv&amp;quot;) Writing: CSV.write(&amp;quot;file_name.csv&amp;quot;, df_data) JSON: JSON3.jl, JSON.jl Reading String: read(&amp;quot;file_name.json&amp;quot;, String) JSON3.Object: JSON3.read(cd*&amp;quot;/wonnyo.json&amp;quot;) Dictionary: JSON3.read(cd*&amp;quot;/wonnyo.json&amp;quot;) |&amp;gt; Dict Writing JSON3.write(&amp;quot;file_name.json&amp;quot;, json_data) open(&amp;quot;file_name.json&amp;quot;, &amp;quot;w&amp;quot;) do io; JSON3.pretty(io, json_data); end Pickle: PyCall.jl Reading: open(&amp;quot;file_name.pkl&amp;quot;) do f; pickle.load(f); end Writing: open(&amp;quot;file_name.pkl&amp;quot;, &amp;quot;w&amp;quot;) do f; pickle.dump(dict_data, f); end Numpy: PyCall.jl Reading: open(joinpath(cd, &amp;quot;file_name.npy&amp;quot;)) do f; np.load(f);</description></item><item><title>How to Flip Images Vertically and Horizontally in Julia</title><link>https://freshrimpsushi.github.io/en/posts/7/</link><pubDate>Sat, 21 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/7/</guid><description>Overview This section introduces how to vertically or horizontally flip an image in Julia. It might seem intuitive to expect a function like flip in Images.jl, but it&amp;rsquo;s not present1. However, such functionality is implemented in the Augmentor.jl package2. Considering dependencies, it&amp;rsquo;s inefficient to use a package just for the flipping feature, so let&amp;rsquo;s implement it using only built-in functions. Code We will use this example image. Horizontal Flip rotr90(img')</description></item><item><title>How to Use Functions Outside a JavaScript Module</title><link>https://freshrimpsushi.github.io/en/posts/6/</link><pubDate>Thu, 19 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/6/</guid><description>Overview In JavaScript, functions within a module cannot be used externally. In the web environment, modules are defined with the &amp;lt;script type=&amp;quot;module&amp;quot;&amp;gt; tag, and while the module itself can be exported through export, to use a function defined within the module, it must be passed to the window DOM and redefined. Code &amp;lt;script type=&amp;#34;module&amp;#34;&amp;gt; function _f(){ return ... } window.f = _f; &amp;lt;script&amp;gt; By using window.f = _f, the function</description></item><item><title>How to Use Automatic Differentiation in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/1966/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1966/</guid><description>Explanation We introduce how to perform automatic differentiation in PyTorch. In PyTorch, automatic differentiation is implemented through the torch.autograd.grad function. torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=None, is_grads_batched=False, materialize_grads=False) outputs: The function value for which the gradient (automatic differentiation) is calculated. inputs: The point at which the gradient is calculated. grad_outputs: This is the element to be multiplied by the calculated gradient. Usually, it is set to torch.ones_like(outputs). For instance,</description></item><item><title>How to Reference Device Names and Account Names in Julia</title><link>https://freshrimpsushi.github.io/en/posts/902/</link><pubDate>Tue, 17 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/902/</guid><description>Overview Julia, aimed at high-performance computing, is likely to use multiple servers exceeding the usual level of computational demand. In such cases, it&amp;rsquo;s necessary to reference the unique name of each device for control, data transmission, or log creation. Code Device Name gethostname() julia&amp;gt; gethostname() &amp;#34;Sick3060&amp;#34; A hostname is the name of a local machine. It can be used whenever the name of the server that provides resources, regardless of</description></item><item><title>파이토치로 PINN 논문 구현하기</title><link>https://freshrimpsushi.github.io/en/posts/1967/</link><pubDate>Mon, 16 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1967/</guid><description>Description PINN stands for Physics-Informed Neural Networks and refers to numerically solving differential equations using automatic differentiation and artificial neural networks. In the PINN paper, it has been implemented with TensorFlow. This article explains how to implement it using PyTorch. It proceeds under the assumption that you have read the following two articles. Review of the PINN paper How to use automatic differentiation in PyTorch Schrö</description></item><item><title>How to Use Clipboard in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2628/</link><pubDate>Sun, 15 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2628/</guid><description>Overview Introducing how to use the clipboard in Julia. Code clipboard is implemented with two methods. clipboard(x) Copies x to the clipboard. clipboard() Pastes from the clipboard. How to Use Outside the REPL using InteractiveUtils: clipboard Outside the REPL, the clipboard function cannot be used, so you must import the clipboard function from InteractiveUtils as shown above1. Full Code using InteractiveUtils: clipboard clipboard(&amp;#34;julia&amp;#34;) x = clipboard() Environment OS: Windows julia:</description></item><item><title>NamedArrays.jl Package in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1968/</link><pubDate>Sat, 14 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1968/</guid><description>Description Julia&amp;rsquo;s NamedArrays.jl package allows the use of 2D arrays with named rows and columns. Although it is also applicable to arrays with three or more dimensions, this article focuses on 2D arrays. Code Definition When inputting an array of size $3 \times 4$ into the NamedArray function, it outputs the array with names attached to rows and columns. julia&amp;gt; using NamedArrays julia&amp;gt; X = reshape(1:12, (3, 4)) 3×</description></item><item><title>Changing Automatic Line Wrapping Settings in VSCode</title><link>https://freshrimpsushi.github.io/en/posts/2627/</link><pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2627/</guid><description>Guide When using both programming and markup languages, there may be instances where you need to set line breaks. This can be easily done through shortcuts. Alt+z ↕ Alt+z Environment OS: Windows vscode: v1.86.2</description></item><item><title>How to Define an Array by Specifying Its Type in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1969/</link><pubDate>Thu, 12 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1969/</guid><description>Code To specify the type in front of the brackets, simply write the type name. julia&amp;gt; [1, 2, 3] 3-element Vector{Int64}: 1 2 3 julia&amp;gt; Float64[1, 2, 3] 3-element Vector{Float64}: 1.0 2.0 3.0 julia&amp;gt; Complex{Float64}[1, 2, 3] 3-element Vector{ComplexF64}: 1.0 + 0.0im 2.0 + 0.0im 3.0 + 0.0im julia&amp;gt; Char[1, 2, 3] 3-element Vector{Char}: &amp;#39;\x01&amp;#39;: ASCII/Unicode U+0001 (category Cc: Other, control) &amp;#39;\x02&amp;#39;: ASCII/Unicode U+0002 (category Cc: Other, control) &amp;#39;\x03&amp;#39;: ASCII/Unicode</description></item><item><title>How to Assess Code Performance, Benchmark in julia</title><link>https://freshrimpsushi.github.io/en/posts/2626/</link><pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2626/</guid><description>Overview The primary reason for using Julia is its speed and performance, so writing code that is optimized from an engineering perspective is extremely important. BenchmarkTools.jl provides an interface for easy, convenient, and accurate performance evaluation1. Code julia&amp;gt; sinpi(0.5), cospi(0.5) (1.0, 0.0) julia&amp;gt; sincospi(0.5) (1.0, 0.0) Let&amp;rsquo;s compare the performance between computing a value at once with sincospi and using sinpi and cospi separately. @btime The evaluation shows that sincospi</description></item><item><title>Comprehensions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1970/</link><pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1970/</guid><description>Code In Julia, like in Python, comprehension is possible. Comprehension is a method of creating arrays that involves embedding conditional expressions directly into the array. For instance, if you want to define an array containing integers sequentially from $0$ to $9$, you can embed a for loop directly into the array. julia&amp;gt; [i for i ∈ 0:9] 10-element Vector{Int64}: 0 1 2 3 4 5 6 7 8 9 julia&amp;gt;</description></item><item><title>How to use multi window in vscode</title><link>https://freshrimpsushi.github.io/en/posts/2625/</link><pubDate>Mon, 09 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2625/</guid><description>Guide In vscode, you can pop out tabs and terminals into new windows to utilize multiple monitors. Tabs Right-click on any tab and select [Copy into New Window] to open it in a new window. Terminal In the terminal window, you can select different processes on the right side. If the right bar does not appear, you can press + or ++&amp;lt;`&amp;gt; to run any process and check. Right-click on</description></item><item><title>Methods for Calculating Arrays Column-wise in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1971/</link><pubDate>Sun, 08 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1971/</guid><description>Code There are methods like map, broadcast, and comprehension for calculating arrays by column or row. julia&amp;gt; using Statistics julia&amp;gt; X = stack([i*ones(8) for i ∈ 1:9], dims=2) 8×9 Matrix{Float64}: 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 1.0 2.0</description></item><item><title>Application for sortperm in julia</title><link>https://freshrimpsushi.github.io/en/posts/2624/</link><pubDate>Sat, 07 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2624/</guid><description>Code sortperm returns an array of indices that would sort the given array1. It might sound complicated when you hear it, but looking at an example makes it immediately clear. julia&amp;gt; foo = [&amp;#39;다&amp;#39;, &amp;#39;나&amp;#39;, &amp;#39;라&amp;#39;,</description></item><item><title>Differences Between Vectors and Tuples in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1972/</link><pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1972/</guid><description>Description Vectors and tuples may appear similar at first glance, and they indeed share several common points, which might lead one to think, &amp;quot;Are they just the same concept with different names?&amp;quot; However, there are a few important differences between vectors and tuples. This article will primarily focus on the similarities and differences from the perspective of writing and using code. The mathematical basis for tuples in Julia and how</description></item><item><title>Snippet Defualt Directory of vscode</title><link>https://freshrimpsushi.github.io/en/posts/2623/</link><pubDate>Thu, 05 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2623/</guid><description>Guide In vscode, the default snippet path is as follows: C:\Users\username\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions For example, if the username is ryu, it would be: C:\Users\ryu\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions</description></item><item><title>Comparing NaN in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1973/</link><pubDate>Wed, 04 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1973/</guid><description>Description In Julia, the standard for floating-point numbers follows the IEEE 754 standard. Under this rule, NaN always returns false when compared with all other values. It only returns true when using the != and !== operators. Code julia&amp;gt; NaN &amp;gt; 1 false julia&amp;gt; NaN ≥ 2 false julia&amp;gt; NaN == 3 false julia&amp;gt; NaN ≤ 4 false julia&amp;gt; NaN &amp;lt; 5 false julia&amp;gt; NaN != 6 true julia&amp;gt; NaN</description></item><item><title>Functions for accessing elements of a singleton set in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2622/</link><pubDate>Tue, 03 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2622/</guid><description>Overview Just as a singleton set and its element are distinctly different in set theory, as discussed with singleton set $\left\{ a \right\}$ and its element $a$, in programming, a collection [a] that contains only one element and the unique element a itself are different. While some environments like MATLAB may not distinguish between these in certain situations, Julia does so more strictly, and the function designed for this purpose</description></item><item><title>Fall 2024 Omakase: Dual Numbers and Automatic Differentiation</title><link>https://freshrimpsushi.github.io/en/posts/1484/</link><pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1484/</guid><description>Introduction Hello, esteemed guests. For those of you disappointed that the magical moments of Cheoseo haven&amp;rsquo;t arrived, we have prepared a special omakase menu just for you. This course will introduce a variety of writings under the theme of &amp;ldquo;Dual Numbers and Automatic Differentiation.&amp;rdquo; Just like enjoying an omakase meal, savor each one in order to deepen your mathematical thinking. Menu Dual Numbers First, as a starter for this omakase,</description></item><item><title>Simple Poles of Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/2621/</link><pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2621/</guid><description>Theorem The domain of the Gamma function $\Gamma$ as a complex function is as follows: $$ \mathbb{C} \setminus \left( \mathbb{Z} \setminus \mathbb{N} \right) = \mathbb{C} \setminus \left\{ 0 , -1, -2, \cdots \right\} $$ Moreover, the set of singularities of $\Gamma$, $\left( \mathbb{Z} \setminus \mathbb{N} \right)$, is a set of simple poles. $\mathbb{N}$ represents the set of natural numbers, $\mathbb{Z}$ represents the set of integers, and $\mathbb{C}$ represents the set</description></item><item><title>How to Set Training and Testing Modes for Neural Networks in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/1975/</link><pubDate>Sat, 31 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1975/</guid><description>Description In the architecture of a neural network, there are components that must operate differently during training and testing phases. For instance, dropout should be applied during training but must not be applied during testing or actual use after training is complete. In such cases, it is necessary to distinguish between training mode and testing mode. Code The function to switch the neural network to training mode is trainmode!, and</description></item><item><title>How to Preview a DataFrame in vscode julia</title><link>https://freshrimpsushi.github.io/en/posts/2620/</link><pubDate>Fri, 30 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2620/</guid><description>Guide 1 julia&amp;gt; using DataFrames julia&amp;gt; df = DataFrame(a = 10:-1:1, b = &amp;#39;a&amp;#39;:&amp;#39;j&amp;#39;) 10×2 DataFrame Row │ a b │ Int64 Char ─────┼───────────── 1 │ 10 a 2 │ 9 b 3 │ 8 c 4 │ 7 d 5 │ 6 e 6 │ 5 f 7</description></item><item><title>Derivative of the Inverse Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1976/</link><pubDate>Thu, 29 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1976/</guid><description>Formulas1 The derivatives of the inverse hyperbolic functions are as follows: $$ \begin{align*} \dfrac{d}{dx} (\sinh^{-1} x) &amp;amp;= \dfrac{1}{\sqrt{x^{2} + 1}} \qquad &amp;amp; \dfrac{d}{dx} (\csch^{-1} x) &amp;amp;= - \dfrac{1}{|x|\sqrt{x^{2} + 1}} \\ \dfrac{d}{dx} (\cosh^{-1} x) &amp;amp;= \dfrac{1}{\sqrt{x^{2} - 1}} \qquad &amp;amp; \dfrac{d}{dx} (\sech^{-1} x) &amp;amp;= - \dfrac{1}{x\sqrt{1 - x^{2}}} \\ \dfrac{d}{dx} (\tanh^{-1} x) &amp;amp;= \dfrac{1}{1 - x^{2}} \qquad &amp;amp; \dfrac{d}{dx} (\coth^{-1} x) &amp;amp;= \dfrac{1}{1 - x^{2}} \end{align*} $$ Description The closed</description></item><item><title>Definition of Hypergraph</title><link>https://freshrimpsushi.github.io/en/posts/2619/</link><pubDate>Wed, 28 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2619/</guid><description>Definition A finite set $V \ne \emptyset$ is called a Hypervertex Set. A Hyperedge refers to a subset of a hypervertex set, and the set of hyperedges $E$ is called a Hyperedge Set. In other words, a hyperedge set can be defined as a subset $E \subset 2^{V}$ of the power set of a hypervertex set. The pair of hypervertices and hyperedges $G = (V, E)$ is called a Hypergraph.</description></item><item><title>Inverse Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1977/</link><pubDate>Tue, 27 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1977/</guid><description>Definition1 The inverse functions of hyperbolic functions are called inverse hyperbolic functions. $$ \begin{align*} y = \sinh^{-1} x &amp;amp;\iff \sinh y = x \\ y = \cosh^{-1} x &amp;amp;\iff \cosh y = x \\ y = \tanh^{-1} x &amp;amp;\iff \tanh y = x \\ \end{align*} $$ Closed Form The values of the inverse hyperbolic functions are concretely as follows. $$ \begin{align*} \sinh^{-1} x &amp;amp;= \ln \left( x + \sqrt{x^{2} +</description></item><item><title>How to Use Decision Trees in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2618/</link><pubDate>Mon, 26 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2618/</guid><description>Overview Introducing the DecisionTree.jl package, which implements Decision Trees in Julia1. Code As an example, we use the iris dataset, a classic built-in dataset in R. Our goal is to create a decision tree that uses four variables SepalLength, SepalWidth, PetalLength, PetalWidth to predict Species and evaluate its performance. julia&amp;gt; iris = dataset(&amp;#34;datasets&amp;#34;, &amp;#34;iris&amp;#34;) 150×5 DataFrame Row │ SepalLength SepalWidth PetalLength PetalWidth Speci ⋯ │ Float64</description></item><item><title>Derivatives of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1978/</link><pubDate>Sun, 25 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1978/</guid><description>Formula1 The derivative of hyperbolic functions is as follows: $$ \begin{align*} \dfrac{d}{dx} (\sinh x) &amp;amp;= \cosh x \qquad &amp;amp; \dfrac{d}{dx} (\csch x) &amp;amp;= - \csch x \coth x \\ \dfrac{d}{dx} (\cosh x) &amp;amp;= \sinh x \qquad &amp;amp; \dfrac{d}{dx} (\sech x) &amp;amp;= - \sech x \tanh x \\ \dfrac{d}{dx} (\tanh x) &amp;amp;= \sech^{2} x \qquad &amp;amp; \dfrac{d}{dx} (\coth x) &amp;amp;= - \csch^{2} x \end{align*} $$ Proof Since hyperbolic functions are linear</description></item><item><title>Derivative of Gamma Function at 1</title><link>https://freshrimpsushi.github.io/en/posts/2617/</link><pubDate>Sat, 24 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2617/</guid><description>Theorem For the Gamma function $\Gamma$ and the Euler-Mascheroni constant $\gamma$, the following holds: $$ \Gamma ' (1) = - \gamma $$ Proof 1 The derivative of the Gamma function times its reciprocal: $$ {{ \Gamma ' (z) } \over { \Gamma (z) }} = - \gamma + \sum_{n=1}^{\infty} \left( {{ 1 } \over { n }} - {{ 1 } \over { z + n - 1 }} \right)</description></item><item><title>Trigonometric Identities</title><link>https://freshrimpsushi.github.io/en/posts/1979/</link><pubDate>Fri, 23 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1979/</guid><description>Formula The following identity holds for trigonometric functions. $$ \begin{align} \cos^{2} x + \sin^{2} x &amp;amp;= 1 \\ 1 + \tan^{2} x &amp;amp;= \sec^{2} x \\ 1 + \cot^{2} x &amp;amp;= \csc^{2} x \end{align} $$ Proof $(1)$ From the addition formula of trigonometric functions, $$ \cos (x - y) = \cos x \cos y + \sin x \sin y $$ Substituting $y= x$, $$ \cos 0 = \cos^{2} x +</description></item><item><title>How to Remove Duplicates from a Collection in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2616/</link><pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2616/</guid><description>Overview This section introduces how to remove and check for duplicates in collections in Julia. The unique() function, which eliminates duplicates, is algorithmically straightforward, but can be bothersome to implement on your own, and may not be efficient. The allunique() function, which checks for the absence of duplicate elements, is easy enough to implement that one might not have sought it out, so it’s worth getting familiar</description></item><item><title>Derivatives of Inverse Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/1980/</link><pubDate>Wed, 21 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1980/</guid><description>Formulas Inverse trigonometric functions&amp;rsquo; derivatives are as follows. $$ \begin{align*} \dfrac{d}{dx} \sin ^{-1} x &amp;amp;= \dfrac{1}{\sqrt{1-x^2}} \qquad &amp;amp; \dfrac{d}{dx} \csc ^{-1} x &amp;amp;= -\dfrac{1}{x\sqrt{x^2-1}} \\ \dfrac{d}{dx} \cos ^{-1} x &amp;amp;= -\dfrac{1}{\sqrt{1-x^2}} \qquad &amp;amp; \dfrac{d}{dx} \sec ^{-1} x &amp;amp;= \dfrac{1}{x\sqrt{x^2-1}} \\ \dfrac{d}{dx} \tan ^{-1} x &amp;amp;= \dfrac{1}{1+x^2} \qquad &amp;amp; \dfrac{d}{dx} \cot ^{-1} x &amp;amp;= -\dfrac{1}{1+x^2} \end{align*} $$ Proof Differentiation of trigonometric functions $$ \begin{align*} \dfrac{d}{dx} \sin x &amp;amp;= \cos x \qquad</description></item><item><title>Pitchfork Bifurcation</title><link>https://freshrimpsushi.github.io/en/posts/2614/</link><pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2614/</guid><description>Definition 1 2 Pitchfork Bifurcation refers to a type of bifurcation in dynamical systems where the stability of a fixed point is reversed, and two new fixed points emerge or disappear as a parameter changes. Normal Form Pitchfork bifurcation comes in two types: Supercritical and Subcritical, with the following normal forms: Supercritical: $$\dot{x} = rx - x^{3}$$ Subcritical: $$\dot{x} = rx + x^{3}$$ Diagrams The bifurcation diagrams for pitchfork bifurcation</description></item><item><title>Reciprocal times Derivative of Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/2615/</link><pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2615/</guid><description>Definition The derivative of the logarithm of the Gamma function is called the digamma function. $$ \psi_{0} (z) := \dfrac{d}{dz} \ln \Gamma (z) = \dfrac{\Gamma^{\prime}(z)}{\Gamma (z)} $$ Theorem For the Gamma function $\Gamma$ and the Euler-Mascheroni constant $\gamma$, the following holds: $$ {{ \Gamma ' (z) } \over { \Gamma (z) }} = - \gamma + \sum_{n=1}^{\infty} \left( {{ 1 } \over { n }} - {{ 1 } \over</description></item><item><title>줄리아 미분방정식 패키지 DiffetentialEquations 튜토리얼</title><link>https://freshrimpsushi.github.io/en/posts/1098/</link><pubDate>Sun, 18 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1098/</guid><description>Description DifferentialEquations.jl is one of the packages under the SciML group developed for the numerical solution of differential equations. The equations that can be solved with this package are as follows: Discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations) Ordinary Differential Equations (ODEs) Split and Partitioned ODEs (Symplectic integrators, IMEX Methods) Stochastic Differential Equations (SDEs) Stochastic differential-algebraic equations (SDAEs) Random differential equations (RODEs or RDEs) Differential algebraic equations (DAEs) Delay</description></item><item><title>How to Use Quotation Marks in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1982/</link><pubDate>Sat, 17 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1982/</guid><description>Code In $\LaTeX$, opening and closing quotation marks exist separately. The opening quotation mark is `, located to the left of the number key 1 and above the tab key. The closing quotation mark is &amp;rsquo;, located to the left of the Enter key. &amp;#39;Example with only closing quotation marks 1&amp;#39; &amp;#34;Example with only closing quotation marks 2&amp;#34; `Example with both opening and closing quotation marks used separately&amp;#39; ``To type</description></item><item><title>How to Use Clustering Packages in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2613/</link><pubDate>Fri, 16 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2613/</guid><description>Overview In Julia, the package for clustering offered is Clustering.jl1. The algorithms implemented include: K-means K-medoids Affinity Propagation Density-based spatial clustering of applications with noise (DBSCAN) Markov Clustering Algorithm (MCL) Fuzzy C-Means Clustering Hierarchical Clustering Single Linkage Average Linkage Complete Linkage Ward&amp;rsquo;s Linkage Code DBSCAN DBSCAN (Density-based spatial clustering of applications with noise) is implemented with the dbscan() function. If there are $n$ pieces of data in $p$ dimensions, a</description></item><item><title>How to render text, formulas, pictures, and more transparently in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1983/</link><pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1983/</guid><description>Code The part wrapped in \phantom{} is rendered transparently. It remains transparent while still occupying its original place. If the original text is as follows, Using \phantom results in the following. The full code is as follows. \documentclass{article} \usepackage{graphicx} \usepackage{amsmath} \usepackage{kotex} \begin{document} Let $f$ be defined \phantom{(and real-valued) on $[a, b ]$}. For any $x \in [a, b]$ form the quotient $$ \phi(t) = \dfrac{f(t) - f(x)}{t - x} \qquad</description></item><item><title>Vibro-impact Model as a Dynamical System</title><link>https://freshrimpsushi.github.io/en/posts/2612/</link><pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2612/</guid><description>Overview The Vibro-impact Model represents the motion of an object inside a vibrating cylindrical capsule in a nonsmooth dynamical system, which can broadly be classified into two types: Hard and Soft. Model 1 Let&amp;rsquo;s refer to the Cylindrical Capsule for convenience as a capsule. In the illustrations below, $s$ is the length of the capsule, $M$ is the mass of the capsule, $m_{b}$ is the mass of the object, and</description></item><item><title>양자 조화 진동자의 사다리 연산자</title><link>https://freshrimpsushi.github.io/en/posts/1984/</link><pubDate>Tue, 13 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1984/</guid><description>Definition Let&amp;rsquo;s define two operators $a_{+}$ and $a_{-}$ as follows. Then $a_{\pm}$ is the ladder operator of the Hamiltonian in a quantum harmonic oscillator. $$ \begin{align*} a_{+} &amp;amp;= \dfrac{1}{\sqrt{2\hbar m \omega}}(- \i P + m\omega X) \\ a_{-} &amp;amp;= \dfrac{1}{\sqrt{2\hbar m \omega}}(+ \i P + m\omega X) \end{align*} $$ Here, $P$ is the momentum operator, $X$ is the position operator, $\hbar$ is the Planck constant, $m$ is the mass of</description></item><item><title>How to Use GPU in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/2611/</link><pubDate>Mon, 12 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2611/</guid><description>Overview In this article, we introduce how to implement deep learning with Flux.jl,1 a machine learning library for Julia, and how to accelerate the learning performance through the GPU. To use the GPU, it is essential to utilize CUDA.jl2 and to have the CUDA settings properly configured in advance. The setup for CUDA is similar to that in Python, so refer to the following post for guidance: How to Install</description></item><item><title>Paper Review: DeepONet</title><link>https://freshrimpsushi.github.io/en/posts/1180/</link><pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1180/</guid><description>Overview and Summary Follow the references, equation numbers, and notation in the paper as closely as possible. For accessibility, this review is based on the version available on arXiv rather than the journal published version. Although the problems covered in the experimental section differ slightly, the core focus is not on the experimental results and performance but on the explanation of the DeepONet method itself. DeepONet is a deep learning</description></item><item><title>Normal Form of Vector Field in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/2610/</link><pubDate>Sat, 10 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2610/</guid><description>Definition Let $p(x;r)$ be some polynomial function. $$ \dot{x} = p(x; r) $$ To describe the properties of a dynamical system, the simplified vector field as above is called a Normal Form. Explanation One of the challenging aspects of studying dynamics, such as chaos, bifurcation, fractals, etc., is the feeling that there is a lack of mathematical rigor and generality. Whether this perception is due to one&amp;rsquo;s own lack of</description></item><item><title>Variables Separable Function</title><link>https://freshrimpsushi.github.io/en/posts/1985/</link><pubDate>Fri, 09 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1985/</guid><description>Definition For a multivariable function, if there exists an $g_{i}$ that satisfies the following equation $f$, it is said to be separable in variables. $$ f(x_{1}, x_{2}, \dots, x_{n}) = g_{1}(x_{1}) g_{2}(x_{2}) \cdots g_{n}(x_{n}) $$ Description In simple terms, separating variables means expressing something as a product of functions that depend only on each variable. This assumption is often made when solving differential equations.</description></item><item><title>Julia's Automatic Differentiation Package Zygote.jl</title><link>https://freshrimpsushi.github.io/en/posts/2609/</link><pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2609/</guid><description>Overview In Julia, the Zygote.jl package is used for automatic differentiation, especially in the field of machine learning, and particularly for deep learning. The developers promote this package as the next-generation automatic differentiation system that enables differentiable programming in Julia, and indeed, using it can be surprisingly intuitive. If you are curious about packages related to the derivative itself, not automatic differentiation, check out the Calculus.jl package. Code Univariate Functions</description></item><item><title>Cylindrical Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/1986/</link><pubDate>Wed, 07 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1986/</guid><description>Definition The coordinates of a point $p$ in coordinate space are represented as &amp;ldquo;the length $\rho$ of the line segment obtained by projecting the line between the origin and $p$ onto the $xy-$ plane&amp;rdquo; and &amp;ldquo;the angle $\phi$ formed by the line connecting the origin $O$ and the projection $p^{\prime}$ with the positive direction of the $x-$ axis&amp;rdquo; and &amp;ldquo;the value $z$ in the positive direction of the $z-$ axis</description></item><item><title>Bifurcation in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/2608/</link><pubDate>Tue, 06 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2608/</guid><description>Definition 1 2 In dynamical systems, a bifurcation occurs when a phase portrait exhibits a topological nonequivalence due to changes in parameters. A point in the parameter space where the system&amp;rsquo;s topological type changes, i.e., where a bifurcation occurs, is called a bifurcation point. Local and Global 3 A bifurcation that can be detected by examining a small neighborhood of a fixed point is said to be local. Conversely, a</description></item><item><title>How to Change the Default Save Location for Excel, PowerPoint, and Word</title><link>https://freshrimpsushi.github.io/en/posts/1987/</link><pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1987/</guid><description>Description Click the [File] tab. Click the [Options] tab. Click the [Save] tab. Change the Default Local File Location (I) to your desired path. Changing it to the relative path ./ will directly open the file location when F12 is pressed, and the path remains unchanged even when saving multiple image files.</description></item><item><title>Referencing Struct Properties as Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2607/</link><pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2607/</guid><description>Overview In Julia, there are mainly two ways to reference the properties of a structure. They should be used appropriately according to grammatical convenience or actual use. Code For example, in Julia, the // operator creates a Rational type of number as follows. The names of the properties that a rational number has include :num meaning numerator and :den meaning denominator. julia&amp;gt; q = 7 // 12 7//12 julia&amp;gt; q</description></item><item><title>Cylindrical Coordinates에서의 Del 연산자</title><link>https://freshrimpsushi.github.io/en/posts/1988/</link><pubDate>Sat, 03 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1988/</guid><description>Formula The del operator in the cylindrical coordinate system is as follows. $$ \nabla = \dfrac{\partial}{\partial \rho} \widehat{\boldsymbol{\rho}} + \dfrac{1}{\rho}\dfrac{\partial}{\partial \phi} \widehat{\boldsymbol{\phi}} + \dfrac{\partial}{\partial z} \widehat{\mathbf{z}} $$ Description The del operator is not a vector, but for convenience, it is represented as above. Gradient: $$ \nabla f = \frac{\partial f}{\partial \rho}\boldsymbol{\hat \rho} + \frac{1}{\rho}\frac{\partial f}{\partial \phi}\boldsymbol{\hat \phi} + \frac{\partial f}{\partial z}\mathbf{\hat{\mathbf{z}}} $$ Divergence: $$ \begin{align*} \nabla \cdot \mathbf{F} &amp;amp;= \frac{1}{\rho}</description></item><item><title>Principal Component Analysis (PCA) in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2606/</link><pubDate>Fri, 02 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2606/</guid><description>Overview Principal Component Analysis (PCA) has many uses in statistics, such as avoiding multicollinearity in regression analysis and summarizing data, and holds significant importance in machine learning as a way of dimensionality reduction. This post focuses on the theoretical foundations of deriving principal components rather than on practical usage. Definition 1 Principal Component Analysis Let&amp;rsquo;s assume a random vector $\mathbf{X} = \left( X_{1} , \cdots , X_{p} \right)$ is given.</description></item><item><title>Curvilinear Coordinate System and the Del Operator</title><link>https://freshrimpsushi.github.io/en/posts/1989/</link><pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1989/</guid><description>Definition In any curvilinear coordinate system where the coordinates are $(q_{1}, q_{2}, q_{3})$, the del operator is as follows. $$ \begin{align*} \nabla &amp;amp;= \dfrac{1}{h_{1}}\dfrac{\partial}{\partial q_{1}} \widehat{\mathbf{q}}_{1} + \dfrac{1}{h_{2}}\dfrac{\partial}{\partial q_{2}} \widehat{\mathbf{q}}_{2} + \dfrac{1}{h_{3}}\dfrac{\partial}{\partial q_{3}} \widehat{\mathbf{q}}_{3} \\ &amp;amp;= \sum\limits_{1}^{3} \dfrac{1}{h_{i}}\dfrac{\partial}{\partial q_{i}} \widehat{\mathbf{q}}_{i} \end{align*} $$ Here, $h_{i}$ represents the scale factor. Description The del operator is not a vector, but for convenience, it is represented as above. Cartesian Coordinate System In the Cartesian</description></item><item><title>How to Draw Vector Fields in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2605/</link><pubDate>Wed, 31 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2605/</guid><description>Code quiver(, quiver=) In Julia, the quiver() function can be used to visualize a vector field. θ = 0:0.2:2π quiver(cos.(θ),sin.(θ), quiver = (-sin.(θ), cos.(θ)),</description></item><item><title>Gradient, Divergence and Curl in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1990/</link><pubDate>Tue, 30 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1990/</guid><description>Formula The del operator in the spherical coordinates is as follows. $$ \nabla = \dfrac{\partial}{\partial r} \widehat{\mathbf{r}} + \dfrac{1}{r}\dfrac{\partial}{\partial \theta} \widehat{\boldsymbol{\theta}} + \dfrac{1}{r\sin\theta}\dfrac{\partial}{\partial \phi} \widehat{\boldsymbol{\phi}} $$ Description The del operator is not a vector, but for convenience, it is represented as above. Gradient: $$ \nabla f= \frac{\partial f}{\partial r} \mathbf{\hat r} + \frac{1}{r}\frac{\partial f}{\partial \theta} \boldsymbol{\hat \theta} + \frac{1}{r\sin\theta}\frac{\partial f}{\partial \phi}\boldsymbol{\hat \phi} $$ Divergence: $$ \begin{align*} \nabla \cdot \mathbf{F} &amp;amp;=</description></item><item><title>Eigenvalues of Positive Definite Matrices and the Maximum Value of Quadratic Forms</title><link>https://freshrimpsushi.github.io/en/posts/2604/</link><pubDate>Mon, 29 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2604/</guid><description>Theorem Let&amp;rsquo;s assume that the eigenpairs $A \in \mathbb{R}^{p \times p}$ of a positive definite matrix $\left\{ \left( \lambda_{k} , e_{k} \right) \right\}_{k=1}^{n}$ are ordered as $\lambda_{1} \ge \cdots \ge \lambda_{n} \ge 0$. On the Unit Sphere, the maximum and minimum values of the quadratic form $\mathbf{x}^{T} A \mathbf{x}$ are as follows. $$ \begin{align*} \max_{\left\| \mathbf{x} \right\| = 1} \mathbf{x}^{T} A \mathbf{x} =&amp;amp; \lambda_{1} &amp;amp; \text{, attained when } \mathbf{x}</description></item><item><title>What is Flux in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/1991/</link><pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1991/</guid><description>Definition1 In physics, flux refers to the number of particles (or physical quantities such as energy, momentum, etc.) passing through (colliding with) a unit area per unit time. Flux is typically denoted by the capital letter pi $\Phi$. $$ \Phi = \dfrac{\text{physical quantity}}{\text{area} \times \text{time}} $$ Explanation Electric field flux: The flux of an electric field $\mathbf{E}$ passing through surface $\mathcal S$ is as follows: $$ \Phi_{E} = \int_{\mathcal S}</description></item><item><title>Referencing Specific Positions in an Array with Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2603/</link><pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2603/</guid><description>Overview When multiple arrays are given, there are often situations where one wants to access a specific element of these arrays, for example, the third element in each array. In Julia, this can be implemented through broadcasting the getindex() function. Code getindex.() julia&amp;gt; seq_ = [collect(1:k:100) for k in 1:10] 10-element Vector{Vector{Int64}}: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 91, 92, 93, 94, 95, 96, 97,</description></item><item><title>Hamiltonian Operator</title><link>https://freshrimpsushi.github.io/en/posts/1993/</link><pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1993/</guid><description>정의 양자역학에서 해밀토니안 연산자 $H$란 다음과 같다. $$ H = \dfrac{P^{2}}{2m} + V $$ 여기서 $P$는 운동량 연산자, $m$은 입자의 질량, $V$는 퍼텐셜이다. 설명 흔</description></item><item><title>Positive Semidefinite Matrices and the Proof of the Extended Cauchy-Schwarz Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2602/</link><pubDate>Thu, 25 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2602/</guid><description>Theorem 1 For any two vectors $\mathbf{b}, \mathbf{d} \in \mathbf{R}^{p}$ and a positive definite matrix $A \in \mathbf{R}^{p \times p}$, the following inequality holds. $$ \left( \mathbf{b}^{T} \mathbf{d} \right)^{2} \le \left( \mathbf{b}^{T} A \mathbf{b} \right) \left( \mathbf{d}^{T} A^{-1} \mathbf{d} \right) $$ The equivalence conditions for this to be an equality are represented as either $\mathbf{b} = c A^{-1} \mathbf{d}$ or $\mathbf{d} = c A \mathbf{b}$ for some constant $c \in</description></item><item><title>What is a Ladder Operator in Quantum Mechanics?</title><link>https://freshrimpsushi.github.io/en/posts/1994/</link><pubDate>Wed, 24 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1994/</guid><description>Definition For an arbitrary operator $N$, let $n$ be its eigenvalue and $\ket{n}$ be the corresponding eigenfunction. $$ N \ket{n} = n \ket{n} $$ An operator $A$ that satisfies the following conditions is called the ladder operator corresponding to $N$. $$ \left[ N, A \right] = cA \tag{1} $$ Here, $c$ is a constant, and $[N, A]$ is the commutator. Explanation The reason why $A$ is called the ladder operator</description></item><item><title>How to Import Packages from Julia to R</title><link>https://freshrimpsushi.github.io/en/posts/2601/</link><pubDate>Tue, 23 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2601/</guid><description>Overview The way to load a package in Julia is to use using, but as the program grows, it becomes a task to individually write them each time. This introduces a method to load packages through a loop1. Code Metaprogramming packages = [:CSV, :DataFrames, :LinearAlgebra, :Plots] for package in packages @eval using ▷eq1◁(package) end In actual use,</description></item><item><title>Quantum Mechanics: The Position Operator</title><link>https://freshrimpsushi.github.io/en/posts/1995/</link><pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1995/</guid><description>Definition In quantum mechanics, the position operator $X$ is defined as follows for a wave function $\psi(x, t)$: $$ X \psi(x, t) = x \psi(x, t) $$ Description The equation where the position operator is applied to a wave function can be interpreted in quantum mechanics as the act of measuring (observing) the position of the wave function.</description></item><item><title>Normalization of Data</title><link>https://freshrimpsushi.github.io/en/posts/2600/</link><pubDate>Sun, 21 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2600/</guid><description>Terminology Normalization Normalization refers to transforming given data to a value between $0$ and $1$. It is commonly achieved through a transformation $T$ using the data&amp;rsquo;s maximum value $x_{\text{max}}$ and minimum value $x_{\text{min}}$. $$ T (x) = {{ x - x_{\text{min}}} \over { x_{\text{max}} - x_{\text{min}}}} $$ Sometimes, it&amp;rsquo;s simply called Scaling. Description Note that in linear algebra, the term normalized vector refers to a vector that has been orthonormalized,</description></item><item><title>Rodrigues' Formula for Hermite Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/1996/</link><pubDate>Sat, 20 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1996/</guid><description>Official The explicit form of the Hermite polynomial is as follows. Physicist&amp;rsquo;s Hermite Polynomial $$ H_{n} = (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2} \tag{1} $$ Probabilist&amp;rsquo;s Hermite Polynomial $$ H_{e_{n}} = (-1)^{n} e^{{x^2} \over {2}} {{d^{n}} \over {dx^{n}}} e^{- {{x^2} \over {2}}} $$ Derivation The solution to the differential equation below $$ y_{n}^{\prime \prime} - x^{2}y_{n} = -(2n+1)y_{n} \tag{2} $$ is called the Hermite function, and is as follows. $$</description></item><item><title>How to Normalize Matrices Column-wise in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2599/</link><pubDate>Fri, 19 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2599/</guid><description>Overview This document introduces a tip for easily normalizing matrices in Julia 1. At its core, it’s just mixing the method of scalar multiplying matrices by rows and columns, the eachcol() function, and the norm() function from the LinearAlgebra module, but it’s concise, ending in one line, and proving to be quite useful to memorize for frequent use. Code julia&amp;gt; using LinearAlgebra julia&amp;gt; X</description></item><item><title>Laguerre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/3630/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3630/</guid><description>정의 라게르 다항식은 다음과 같은 방법들로 정의된다. 미분방정식의 해로서 아래와 같은 라게르 미분방정식의 해를 라게르 다항함수라 한다. $$ xy^{\prime \prime} + (1-x)y^{\prime} + ny = 0, \quad n=0,1,2,\cdots</description></item><item><title>Graduate Student Descent Method</title><link>https://freshrimpsushi.github.io/en/posts/2598/</link><pubDate>Wed, 17 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2598/</guid><description>Buildup The Fridge-Elephant Problem Traditionally, the method for putting an elephant into a fridge has relied on graduate students. How difficult or challenging it could be, or what the best method might be, I&amp;rsquo;m not sure, but that&amp;rsquo;s something the graduate students would figure out, so no worries there. Then, what could be the first measure to try in solving nonlinear optimization problems, including deep learning? Terminology It involves trying</description></item><item><title>Customizing Chapter and Section Numbering in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3629/</link><pubDate>Tue, 16 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3629/</guid><description>Code The LaTeX code \setcounter{section}{n} sets the current section number to $n$. Since this counter increases by 1 with each added section, the order of the section created next becomes $n+1$. \documentclass{book} \begin{document} \chapter{Limits and Derivatives} \section{Continuity} \end{document} \documentclass{book} \begin{document} \setcounter{chapter}{1} \chapter{Limits and Derivatives} \setcounter{section}{4} \section{Continuity} \end{document}</description></item><item><title>Multiplying Matrices Row-wise and Column-wise with Scalar in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2597/</link><pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2597/</guid><description>Overview Introducing how to perform scalar multiplication by row and column on matrices in Julia. Code julia&amp;gt; d = 1:10 1:10 julia&amp;gt; X = ones(Int64, 10, 10) 10×10 Matrix{Int64}: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</description></item><item><title>프레드홀 적분 방정식</title><link>https://freshrimpsushi.github.io/en/posts/3628/</link><pubDate>Sun, 14 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3628/</guid><description>Definition1 The following integral equation is referred to as a Fredholm Integral Equation of the first kind. $$ g(s) = \int K(s, t) f(t) dt \tag{1} $$ Here, $K$ is called the kernel. The following form is referred to as the Fredholm integral equation of the second kind. $$ g(s) = f(s) + \int K(s, t) f(t) dt \tag{2} $$ Explanation Solving the integral equation $(1), (2)$ typically means finding</description></item><item><title>Grid Search, Brute Force, Hard Work</title><link>https://freshrimpsushi.github.io/en/posts/2596/</link><pubDate>Sat, 13 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2596/</guid><description>Terminology Grid Search In optimization problems, dividing the Euclidean space $\mathbb{R}^{n}$ into a grid and repeatedly trying as many points as possible to find the optimal solution is referred to as Grid Search. Brute Force In cryptographic problems, attempting every possible combination indiscriminately to crack codes is called Brute Force. Manual Labor To solve problems that are either difficult or undesirable to solve, repeating possible attempts without a particular strategy</description></item><item><title>Creating a New Command in LaTeX That Acts Like a Function Using the begin...end Structure</title><link>https://freshrimpsushi.github.io/en/posts/3627/</link><pubDate>Fri, 12 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3627/</guid><description>Code Writing TeX code with \begin{}...\end{} statements can be quite bothersome. \documentclass{article} \usepackage{amsthm} \newtheorem*{theorem*}{Theorem} \begin{document} \begin{theorem*} Let $f$ and $g$ be continuous functions on a metric space $X$. Then $f + g$, $fg$, and $f/g$ are continuous on $X$. \end{theorem*} \end{document} By adding a new command like \newcommand{\thm}[1]{\begin{theorem*}#1\end{theorem*}}, it can be conveniently used. \documentclass{article} \usepackage{amsthm} \newtheorem*{theorem*}{Theorem} \newcommand{\thm}[1]{\begin{theorem*}#1\end{theorem*}} \begin{document} \thm{ Let $f$ and $g$ be continuous functions on a metric space</description></item><item><title>PLU Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/2/</link><pubDate>Thu, 11 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2/</guid><description>Definition 1 2 For permutation matrices $P^{T}$ and invertible matrices $A \in \mathbb{R}^{n \times n}$, their matrix product $P^{T} A$&amp;rsquo;s LU decomposition is referred to as the PLU Decomposition $A$. Since $P$ is a permutation matrix, it&amp;rsquo;s an orthogonal matrix, i.e., $P^{-1} = P^{T}$, and thus can be represented as follows. $$ P^{T} A = LU \iff A = PLU $$ Description Algorithm of LU decomposition: Let&amp;rsquo;s assume $(a_{ij}) \in</description></item><item><title>Difference Between torch.nn and torch.nn.functional in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3626/</link><pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3626/</guid><description>Description PyTorch contains many functions related to neural networks, which are included under the same names in torch.nn and torch.nn.functional. The functions in nn return a neural network as a function, while those in nn.functional are the neural network itself. For instance, nn.MaxPool2d takes the kernel size as input and returns a pooling layer. import torch import torch.nn as nn pool = nn.MaxPool2d(kernel_size = 2) # MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,</description></item><item><title>Permutation Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1/</link><pubDate>Tue, 09 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1/</guid><description>Definition 1 $P \in \mathbb{R}^{n \times n}$ in which only one component in each row is $1$ and the rest are $0$ is called a Permutation Matrix. Basic Properties Orthogonality All permutation matrices are orthogonal matrices: $$P^{-1} = P^{T}$$ Sparseness For sufficiently large $n$, $P \in \mathbb{R}^{n \times n}$ is a sparse matrix. Explanation The Permutation Matrix gives a permutation of rows and columns through matrix multiplication. The following example</description></item><item><title>What are Weights in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/3625/</link><pubDate>Mon, 08 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3625/</guid><description>Definition In machine learning, the parameters to be optimized are called weights.</description></item><item><title>Dynamics in Atopic dermatitis Systems</title><link>https://freshrimpsushi.github.io/en/posts/2595/</link><pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2595/</guid><description>Model The following nonsmooth dynamic system is referred to as the Atopic dermatitis System. $$ \begin{align*} {{ d P (t) } \over { d t }} =&amp;amp; {{ P_{\text{env}} \kappa_{P} } \over { 1 + \gamma_{B} B (t) }} - \alpha_{I} R(t) P(t) - \delta_{P} P (t) \\ {{ d B (t) } \over { d t }} =&amp;amp; {{ \kappa_{B} \left[ 1 - B(t) \right] } \over { \left[</description></item><item><title>What is Skip Connection in Artificial Neural Networks?</title><link>https://freshrimpsushi.github.io/en/posts/3624/</link><pubDate>Sat, 06 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3624/</guid><description>Definition Let $\mathbf{W}$ be the weight, $\mathbf{x}$ the input, and $\sigma$ the nonlinear activation function. Let&amp;rsquo;s define layer $L_{\mathbf{w}}$ as follows. $$ L_{\mathbf{W}}(\mathbf{x}) := \sigma(\mathbf{W} \mathbf{x}) $$ A function in the form that adds an identity function to the layer like the following is called a skip connection. $$ L_{\mathbf{W}} + I : \mathbf{x} \mapsto \sigma(\mathbf{W} \mathbf{x}) + \mathbf{x} $$ Explanation Typically, the input $\mathbf{x}$ and the weight $\mathbf{W}$ are</description></item><item><title>Hyperparameters, What Are They?</title><link>https://freshrimpsushi.github.io/en/posts/2594/</link><pubDate>Fri, 05 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2594/</guid><description>Terminology Bayesian Statistics 1 In the Bayesian paradigm, the following is referred to as Bayesian Hierarchical Model: (1) It is assumed that data $y_{1} , \cdots , y_{n}$ is obtained according to parameters $\theta_{1} , \cdots , \theta_{n}$: $$y_{1} , \cdots , y_{n} | \theta_{1} , \cdots , \theta_{n} \sim p \left( y_{k} | \theta_{k} \right)$$ (2) Parameters $\theta_{1} , \cdots , \theta_{n}$ themselves are considered to be obtained by</description></item><item><title>Solution of Wave Equation with Zero Initial Condition</title><link>https://freshrimpsushi.github.io/en/posts/3623/</link><pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3623/</guid><description>Tidy up Let&amp;rsquo;s say that we have the following wave equation. where $\Delta_{\mathbf{x}}$ is Laplacian for the variable $\mathbf{x}$. $$ \begin{align} \partial_{t}^{2} p(\mathbf{x}, t) &amp;amp;= \Delta_{\mathbf{x}} p(\mathbf{x}, t) &amp;amp;\text{on } \mathbb{R} \times [0, \infty) \\ p(\mathbf{x}, 0) &amp;amp;= f(\mathbf{x}) &amp;amp;\text{on } \mathbb{R} \\ \partial_{t} p(\mathbf{x}, 0) &amp;amp;= 0 &amp;amp;\text{on } \mathbb{R} \end{align} $$ The solution of the above partial differential equation is as follows. $$ \begin{equation} p(\mathbf{x}, t) = \dfrac{1}{(2\pi)^{n}}</description></item><item><title>DC-DC Buck Converter as a Dynamical System</title><link>https://freshrimpsushi.github.io/en/posts/2593/</link><pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2593/</guid><description>Model The voltage $V$ and current $I$ of a circuit diagram like the one above 1 can be represented as a non-smooth, non-autonomous system. This is referred to as a DC-DC Buck Converter. $$ \begin{align*} \dot{V} =&amp;amp; - {{ 1 } \over { RC }} V + {{ 1 } \over { C }} I \\ \dot{I} =&amp;amp; - {{ V } \over { L }} + \begin{cases} 0 &amp;amp;</description></item><item><title>Rodrigues' Formula for Multiple Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/3622/</link><pubDate>Tue, 02 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3622/</guid><description>Description Rodrigues&amp;rsquo; formula originally referred to an explicit form of the Legendre polynomials, but later became the general term for formulae representing the explicit forms of special functions expressed in polynomials. Formulas Legendre Polynomials: $$ P_{l}(x)=\dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} $$ Laguerre Polynomials: $$ L_{n}(x) = \frac{1}{n!}e^{x}\frac{ d ^{n}}{ dx^{n} }(x^{n}e^{-x}) $$ Hermite Polynomials: $$ H_{n} = (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2} $$</description></item><item><title>Inverse and Square Root of Positive Definite Matrices</title><link>https://freshrimpsushi.github.io/en/posts/2592/</link><pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2592/</guid><description>Formulas 1 Let&amp;rsquo;s say the eigenpairs $\left\{ \left( \lambda_{k} , e_{k} \right) \right\}_{k=1}^{n}$ of a positive definite matrix $A$ are arranged in order $\lambda_{1} &amp;gt; \cdots &amp;gt; \lambda_{n} &amp;gt; 0$. Regarding the orthogonal matrix $P = \begin{bmatrix} e_{1} &amp;amp; \cdots &amp;amp; e_{n} \end{bmatrix} \in \mathbb{R}^{n \times n}$ and the diagonal matrix $\Lambda = \diag \left( \lambda_{1} , \cdots , \lambda_{n} \right)$, the inverse matrix $A^{-1}$ and the square root matrix</description></item><item><title>What is a Special Function?</title><link>https://freshrimpsushi.github.io/en/posts/3621/</link><pubDate>Sun, 30 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3621/</guid><description>Description Special functions, as called in mathematics, are usually the solutions to certain differential equations, defined by complex integrals, can&amp;rsquo;t be expressed by elementary functions, or have mathematically interesting properties. They often bear names of people, letters of the alphabet, or Greek letters, and it can be said that almost all named functions, except polynomial, trigonometric, exponential, and logarithmic functions, are referred to as special functions. As you can see</description></item><item><title>Slow-Fast Systems</title><link>https://freshrimpsushi.github.io/en/posts/2591/</link><pubDate>Sat, 29 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2591/</guid><description>Definition 1 Slow-Fast System Assuming an element of an open subset $W \subset \mathbb{R}^{m + n}$ of the Euclidean space is expressed as $\left( \mathbf{x}, \mathbf{y} \right)$. For $k \in \mathbb{N}$, two functions $\mathbf{f} : W \times [0,1] \to \mathbb{R}^{m}$ and $\mathbf{g} : W \times [0,1] \to \mathbb{R}^{n}$, in addition to $t = \xi \tau$, are given for $\xi \in (0,1)$ to satisfy the following $C^{k}$ class, $$ \begin{align*} {{</description></item><item><title>Using AdaBelief Optimizer in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3620/</link><pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3620/</guid><description>Description AdaBelief, introduced by J. Zhuang et al. in 2020, is one of the variations of Adam1. Since PyTorch does not natively provide this optimizer, it must be installed separately. Code2 Installation The following command can be used to install it via cmd. pip install adabelief-pytorch==0.2.0 Usage The code below can be used to import and utilize it. from adabelief_pytorch import AdaBelief optimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple =</description></item><item><title>Spectral Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/2590/</link><pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2590/</guid><description>Definition 1 In spectral theory, the statement that $A$ is a Hermitian matrix is equivalent to it being unitarily diagonalizable: $$ A = A^{\ast} \iff A = Q \Lambda Q^{\ast} $$ The term $A = Q \Lambda Q^{\ast}$ as mentioned in spectral theory is referred to as Spectral Decomposition, and is expressed as a series of eigenpairs $\left\{ \left( \lambda_{k} , e_{k} \right) \right\}_{k=1}^{n}$. $$ A = \sum_{k=1}^{n} \lambda_{k} e_{k}</description></item><item><title>Non-Smooth Systems in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/2589/</link><pubDate>Tue, 25 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2589/</guid><description>Terms A Nonsmooth Dynamical System is defined as a dynamical system expressed through $S_{k} \subset \mathbb{R}^{n}$ in terms of $f_{k} : S_{k} \to \mathbb{R}^{n}$ defined by a piecewise smooth system $$ \dot{x} = f_{k} (x) \qquad , k = 1, \cdots, s $$ or heteroclinic mapping $F : \mathbb{R}^{n} \rightrightarrows \mathbb{R}^{n}$ with respect to a differential inclusion $$ \dot{x} \in F(x) $$. Description Many definitions and theorems regarding dynamics, especially</description></item><item><title>Uniform Continuity in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3618/</link><pubDate>Mon, 24 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3618/</guid><description>Definition1 Given two metric spaces $(X, d_{X})$, $(Y, d_{Y})$ and a sequence of functions $\left\{ f_{n} : X \to Y \right\}$. If for every $\varepsilon \gt$ there exists $\delta (\varepsilon) \gt 0$ satisfying the following condition, then the sequence $\left\{ f_{n} \right\}$ is called equicontinuous. $$ \forall x_{1}, x_{2} \in X \text{ and } f_{n}\quad d_{X}(x_{1}, x_{2}) \lt \delta (\varepsilon) \implies d_{Y} \big( f_{n}(x_{1}), f_{n}(x_{2}) \big) \lt \varepsilon $$ Explanation</description></item><item><title>Toeplitz Matrices are Hermitian Matrices</title><link>https://freshrimpsushi.github.io/en/posts/2588/</link><pubDate>Sun, 23 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2588/</guid><description>Proof A positive-definite matrix $A \in \mathbb{C}^{n \times n}$ is a Hermitian matrix. Naturally, a positive semi-definite matrix is also a Hermitian matrix. Proof 1 $$ \mathbf{x}^{\ast} A \mathbf{x} = \lambda $$ If $A$ is a positive-definite matrix, for all $\mathbf{x} \in \mathbb{C}^{n}$, the quadratic form $\mathbf{x}^{\ast} A \mathbf{x}$ is expressed as some real number $\lambda \in \mathbb{R}$ as above. Taking the conjugate transpose on both sides, the complex conjugate</description></item><item><title>Compact Integral Operators</title><link>https://freshrimpsushi.github.io/en/posts/3617/</link><pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3617/</guid><description>Theorem1 For a given compact interval $J = [a, b]$, let $K$ be a function continuous on $J \times J$. Let $X = C[a, b]$ be the space of continuous functions. Then, the integral operator $T : X \to X$ with kernel $K$ is a compact linear operator. $$ (Tx)(s) = \int\limits_{a}^{b} K(s, t) x(t) dt,\qquad \forall x \in X $$ Proof Since the integral operator is linear and bounded,</description></item><item><title>Definition of Differential Inclusion</title><link>https://freshrimpsushi.github.io/en/posts/2587/</link><pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2587/</guid><description>Definition 1 Differential Inclusions For a multivalued mapping $F : \mathbb{R}^{n} \to \mathbb{R}^{n}$, the expression that the derivative $\dot{x} = dx/dt$ at $x \in \mathbb{R}^{n}$ is one of the elements of the set $F(x)$ is called a Differential Inclusion. $$ \dot{x} \in F(x) $$ Filippov Differential Inclusions When $f : \mathbb{R}^{n} \to \mathbb{R}^{n}$ is considered a bounded function, the differential inclusion defined for the initial time $t_{0} \in \mathbb{R}$ and</description></item><item><title>Integration Operator</title><link>https://freshrimpsushi.github.io/en/posts/3616/</link><pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3616/</guid><description>Definition1 For the space of continuous functions $C[0 ,1]$, the operator $T : C[0, 1] \to C[0, 1]$ defined as follows is called the integral operator. $$ y = Tx \qquad \text{where} \qquad y(s) = \int_{0}^{1} K(s, t) x(t) dt $$ Herein, $K$ is called the kernel of $T$. (It is assumed that the kernel $K$ is continuous on $[0, 1] \times [0, 1]$.) Explanation The integral operator is also</description></item><item><title>Necessary and Sufficient Conditions for a Quadratic Form to Be Zero</title><link>https://freshrimpsushi.github.io/en/posts/2586/</link><pubDate>Wed, 19 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2586/</guid><description>Theorem Matrix Form Let&amp;rsquo;s say $A \in \mathbb{C}^{n \times n}$ represents a matrix and $\mathbf{x} \in \mathbb{C}^{n}$ represents a vector. The necessary and sufficient condition for the quadratic form $\mathbf{x}^{\ast} A \mathbf{x}$ to be $0$ for all $\mathbf{x} \in \mathbb{C}^{n}$ is that $A$ is a zero matrix: $$ \mathbf{x}^{*} A \mathbf{x} = 0 , \forall \mathbf{x} \in \mathbb{C}^{n} \iff A = O $$ Linear Transformation Form When $\left( V, \mathbb{C}</description></item><item><title>Compact Operator Equivalence Conditions</title><link>https://freshrimpsushi.github.io/en/posts/3615/</link><pubDate>Tue, 18 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3615/</guid><description>Theorem1 Let $X$ and $Y$ be normed spaces. Let $T : X \to Y$ be a linear operator. Then the following two propositions are equivalent. $T$ is a compact operator. $T$ maps &amp;ldquo;every bounded sequence in $X$&amp;rdquo; to &amp;ldquo;a sequence in $Y$ that has a convergent subsequence&amp;rdquo;. Proof $1. \Longrightarrow 2.$ Assume $T : X \to Y$ is compact. Let $\left\{ x_{n} \right\}$ be a bounded sequence. By the definition</description></item><item><title>Smooth Systems in Each Segment of Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/2585/</link><pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2585/</guid><description>Definition Piecewise Smooth System Let&amp;rsquo;s consider a dynamical system whose state space is $\mathbb{R}^{n}$ with variables $x \in \mathbb{R}^{n}$ and parameters $\mu \in \mathbb{R}^{p}$ represented as follows: $$ \dot{x} = f \left( x ; \mu \right) \qquad x \in \mathbb{R}^{n} , \mu \in \mathbb{R}^{p} $$ If there exist finitely many open sets $S_{k} \subset \mathbb{R}^{n}$ and $f$ that satisfy the following for smooth functions $F_{k} : S_{k} \to \mathbb{R}^{n}$, then</description></item><item><title>The Spectrum and Decomposition Set of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3614/</link><pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3614/</guid><description>Definition1 The set $\sigma (A)$ of all eigenvalues of a square matrix $A$ is called the spectrum of $A$. The complement set $\rho (A) = \mathbb{C} \setminus \sigma (A)$ of the spectrum is called the resolvent set of $A$. Explanation $$ Ax = \lambda x $$ Consider the eigenvalue equation for matrix $A$. A vector $x$ that satisfies the above equation is called an eigenvector, and the constant $\lambda$ is</description></item><item><title>Partial Differential Rings and Differential Rings</title><link>https://freshrimpsushi.github.io/en/posts/2584/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2584/</guid><description>Definition 1 In a ring $R$, if the algebraic derivatives $\Delta = \left\{ \partial_{1} , \cdots , \partial_{n} \right\}$ defined satisfy $$ \partial_{i} \left( \partial_{j} (r) \right) = \partial_{j} \left( \partial_{i} (r) \right) \qquad \forall r \in R $$ for all $i,j = 1, \cdots, n$, then the ordered pair $\left( R , \Delta \right)$ is called a Partial Derivative Ring. Especially, if $\Delta$ is a Singleton set, that is</description></item><item><title>In a Metric Space, Compact Implies Closed and Bounded</title><link>https://freshrimpsushi.github.io/en/posts/3613/</link><pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3613/</guid><description>Theorem A compact subset $K$ of a metric space $(X, d)$ is bounded and a closed set. Description The converse does not generally hold. In Euclidean spaces, the converse holds. Proof Boundedness1 By contradiction, assume that $K$ is not bounded. Since being compact in a metric space is equivalent to being sequentially compact, $K$ is sequentially compact. Sequentially Compact A metric space $X$ being sequentially compact means that for every</description></item><item><title>General Polyhedral Mapping, Definition of Set-Valued Mapping</title><link>https://freshrimpsushi.github.io/en/posts/2583/</link><pubDate>Thu, 13 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2583/</guid><description>Definition 1 2 3 Given two sets $X, Y$ and $Y$, the power set $\mathcal{P} (Y)$ of, a function $f : X \to \mathcal{P} (Y)$ is referred to as Multivalued Mapping or Set-valued Mapping, and is also denoted as $f : X \rightrightarrows Y$. Description In notation, $f : X \rightrightarrows Y$ literally means that $f$ maps $x \in X$ to multiple $y \in Y$. See Also Multivalued functions in</description></item><item><title>Equivalence of Various Compactnesses in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3612/</link><pubDate>Wed, 12 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3612/</guid><description>Theorem1 Let $X$ be a metric space. The following propositions are equivalent. $X$ is compact. $X$ is countably compact. $X$ is limit point compact. $X$ is sequentially compact. Explanation This generally does not hold in topological spaces but holds in metric spaces. 박대희·안승호, 위상수학(하) (5/E, 2022), p503&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Differential Fields in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/2582/</link><pubDate>Tue, 11 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2582/</guid><description>Definition 1 Let $R$ be an (Abelian) ring. A function $d: R \to R$ that satisfies the following is called a derivation, $$ \begin{align*} d \left( x + y \right) =&amp;amp; d (x) + d(y) \\ d \left( x y \right) =&amp;amp; d (x) y + x d(y) \end{align*} $$ and the ordered pair $\left( R, d \right)$ is called a differential ring. Suppose $R$ has a unity $1$. The</description></item><item><title>What is Sequential Compactness in Topological Spaces?</title><link>https://freshrimpsushi.github.io/en/posts/3611/</link><pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3611/</guid><description>Definition1 A topological space $X$ is said to be sequentially compact if every sequence $\left\{ x_{n} \right\}$ in $X$ has a subsequence $\left\{ x_{n_{k}} \right\}$ that converges to a point in $X$. Explanation In general, in topological spaces, compactness and sequential compactness are independent. For example, there are spaces that are compact but not sequentially compact, and vice versa, spaces that are sequentially compact but not compact. In metric spaces,</description></item><item><title>What is the SINDy Algorithm?</title><link>https://freshrimpsushi.github.io/en/posts/2581/</link><pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2581/</guid><description>Algorithm 1 Let&amp;rsquo;s consider a dynamical system whose state space is $\mathbb{R}^{n}$ and is given by the smooth function $f : \mathbb{R}^{n} \to \mathbb{R}^{n}$ as follows. $$ \dot{\mathbf{x}} = f \left( \mathbf{x} \right) $$ Briefly representing as $\mathbf{x} = \left( x_{1} , \cdots , x_{n} \right)$ and $\dot{\mathbf{x}} = \left( \dot{x_{1}} , \cdots , \dot{x_{n}} \right)$, for $m \in \mathbb{N}$ time points $\left\{ t_{k} \right\}_{k=1}^{m}$, a design matrix is constructed</description></item><item><title>Necessary and Sufficient Conditions for a Subset of a Normed Space to be Bounded</title><link>https://freshrimpsushi.github.io/en/posts/3610/</link><pubDate>Sat, 08 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3610/</guid><description>Definition Diameter Given a normed space $(X, \left\| \cdot \right\|)$, the diameter $\diam M$ of a non-empty subset $M \subset X$ is defined as follows. $$ \diam M =: \sup\limits_{x, y \in M} \left\| x - y \right\| $$ Bounded If $\diam M \lt \infty$ is satisfied, then $M$ is said to be bounded. Explanation In a normed space, since the metric can be naturally induced as $d (x, y)</description></item><item><title>Differential Rings in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/2580/</link><pubDate>Fri, 07 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2580/</guid><description>Definition Differential Ring 1 Let $R$ be an (Abelian) ring. A function $d: R \to R$ that satisfies the following is called an (algebraic) derivation. $$ \begin{align*} d \left( x + y \right) =&amp;amp; d (x) + d(y) \\ d \left( x y \right) =&amp;amp; d (x) y + x d(y) \end{align*} $$ The ordered pair $\left( R, d \right)$ is called a Differential Ring. === 定義 微分</description></item><item><title>Compact Action Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3609/</link><pubDate>Thu, 06 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3609/</guid><description>Definition1 Let $X$ and $Y$ be normed spaces, and let $T : X \to Y$ be an operator between these spaces. If for every bounded subset $M \subset X$, the image $T(M)$ of the operator $T$ is precompact, then $T$ is called a compact operator. Explanation That $T(M)$ is precompact means its closure $\overline{T(M)}$ is compact. In other words, a compact operator is an operator that maps bounded sets to</description></item><item><title>What is STLSQ?</title><link>https://freshrimpsushi.github.io/en/posts/2579/</link><pubDate>Wed, 05 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2579/</guid><description>Algorithm STLSQ Given a full rank matrix $X \in \mathbb{R}^{m \times p}$ and $Y \in \mathbb{R}^{m \times n}$ for the matrix equation $$ Y = X B $$ , let&amp;rsquo;s consider the problem of finding as sparse an $B \in \mathbb{R}^{p \times n}$ as possible in the sparse regression problem. STLSQ is a method used in a paper by Brunton, who proposed the SINDy algorithm, and as the name suggests,</description></item><item><title>How to Use Color Gradients in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3608/</link><pubDate>Tue, 04 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3608/</guid><description>Description A color gradient is one of the two color schemes supported by Julia&amp;rsquo;s visualization package Plots.jl (the other is palette), which is what we commonly refer to as gradation. Simply put, a type that implements gradation is ColorGradient. Gradients are used to draw charts such as heatmap(), surface(), contour(). If you want to differentiate the colors of various graphs, use a palette instead of a gradient. Code Symbol It</description></item><item><title>Fraction Rings and Fraction Fields</title><link>https://freshrimpsushi.github.io/en/posts/2578/</link><pubDate>Mon, 03 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2578/</guid><description>Definition 1 For a ring $\left( A , + , \cdot \right)$, let&amp;rsquo;s say $S := A^{\ast} = A \setminus \left\{ 0 \right\}$ is a subset $S \subset A$ that excludes the identity element $0$ for addition $+$ in $A$. Field of Fractions If $A$ is an integral domain, $$ (a,s) \equiv (b,t) \iff at = bs $$ When we define the equivalence relation $\equiv$ on the Cartesian product $A</description></item><item><title>How to Use Palettes in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3607/</link><pubDate>Sun, 02 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3607/</guid><description>Explanation A palette refers to a board where paints are squeezed out in advance. Mathematically, it can be explained as a &amp;lsquo;set of colors&amp;rsquo; or a &amp;lsquo;sequence of colors&amp;rsquo;. When drawing multiple graphs in one picture, the most common way is to distinguish them by using different colors. For this purpose, Julia has implemented a type called ColorPalette that collects various colors. It can be comfortably understood as a vector</description></item><item><title>Uniform Uncertainty Principle: Restricted Isometry Condition</title><link>https://freshrimpsushi.github.io/en/posts/2577/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2577/</guid><description>Definition $$ \begin{align*} \left\| \mathbf{v} \right\|_{0} :=&amp;amp; \left| \operatorname{supp} \mathbf{v} \right| \\ =&amp;amp; \operatorname{card} \left\{ k : \left( \mathbf{v} \right) _{k} \ne 0 \right\} \end{align*} $$ For a vector $\mathbf{v} \in V$ in the vector space $V$, let&amp;rsquo;s define the $l_{0}$-norm $\left\| \mathbf{v} \right\|_{0} : V \to \mathbb{Z}$ as the cardinality of the support, that is, the number of non-zero components $0$. $\left\| \cdot \right\|_{1}$ is the $l_{1}$-norm, $\left\| \cdot</description></item><item><title>How to Plot Two Data Axes of Different Scales in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3606/</link><pubDate>Fri, 31 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3606/</guid><description>Code When plotting two data sets that have a large scale difference on the same plot, the one with the smaller scale gets completely ignored as shown in the figure below. using Plots x = 0:0.01:2π plot(x, sin.(x)) plot!(x, exp.(x)) When plotting the second data set, if you input twinx() as the first argument, it shares the $x$ axis and the graph</description></item><item><title>What is a Closed Form in Mathematics?</title><link>https://freshrimpsushi.github.io/en/posts/2576/</link><pubDate>Thu, 30 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2576/</guid><description>Glossary 1 In mathematics, a closed form expression of a function refers to an expression constructed from a finite number of symbols and arithmetic operations $+, -, \cdot, \div$, as well as several well-known functions. These well-known functions include radicals $\sqrt[n]{\cdot}$, exponential functions $\exp$, logarithmic functions $\log$, trigonometric functions $\sin , \cos$, factorials $\cdot !$, etc. Typically, sums $\sum$, products $\prod$, integrals $\int$, and limits $\lim$ are not included. Examples</description></item><item><title>List of Plot Properties in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3605/</link><pubDate>Wed, 29 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3605/</guid><description>Description In Julia&amp;rsquo;s Plots.jl, a plot is also an object. If you draw an empty plot to check its type, it looks like this. julia&amp;gt; using Plots julia&amp;gt; p = plot() julia&amp;gt; p |&amp;gt; typeof Plots.Plot{Plots.GRBackend} Removing Plots., it becomes Plot{GRBackend}, meaning the plot&amp;rsquo;s backend is GR, similar to how a vector with elements of type Float64 is denoted as Vector{Float64}. Checking the properties of Plot, we find the following.</description></item><item><title>What is Compressive Sensing?</title><link>https://freshrimpsushi.github.io/en/posts/2575/</link><pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2575/</guid><description>Definitions $$ \mathbf{y} = \Theta \mathbf{s} $$ When we have $\Theta \in \mathbb{R}^{n \times p}$ and $n \ll p$, meaning that there are infinitely many solutions $\mathbf{s}$ that satisfy the underdetermined system of matrix equations, then the optimization problem that finds the sparsest solution under the constraint of satisfying this matrix equation is referred to as Compressed Sensing. $$ \begin{matrix} \text{Minimize} &amp;amp; \left\| \mathbf{s} \right\|_{0} \\ \text{subject to} &amp;amp; \mathbf{y}</description></item><item><title>Decorating the Background Grid in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3604/</link><pubDate>Mon, 27 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3604/</guid><description>Overview Keywords related to the grid background in Plots.jl are as follows: Keyword Name Function grid Display grid gridalpha, ga, gα Specify grid transparency foreground_color_grid, fgcolor_grid Specify grid color gridlinewidth, grid_lw Specify grid thickness gridstyle, grid_ls Specify grid line style minorgrid Display minor grid minorgridalpha Specify minor grid transparency foreground_color_minor_grid, fgcolor_minorgrid Specify minor grid color minorgridlinewidth, minorgrid_lw Specify minor grid thickness minorgridstyle, minorgrid_ls Specify minor grid line style Code</description></item><item><title>How to Call a DataFrame without String7, String15 in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2574/</link><pubDate>Sun, 26 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2574/</guid><description>Error When using data frames in Julia, string data sometimes get read as String7, String15, String31, etc., causing various errors. Rather than specific errors occurring, the usual functions don&amp;rsquo;t work with these, causing all sorts of problems. Cause For performance reasons, String was changed to faster versions like String7, etc. It&amp;rsquo;s designed this way on purpose, so nothing much can be done about it. Solution Passing the option stringtype =</description></item><item><title>Specifying Background Color in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3603/</link><pubDate>Sat, 25 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3603/</guid><description>Overview The keywords related to the background color of figures in Plots.jl are as follows. Keyword Name Function background_color, bg_color Specify the color of the overall background background_color_outside, bg_color_outside Specify the color of the area outside where the graph is drawn background_subplot, bg_subplot Specify the color of the area where the graph is drawn background_inside, bg_inside Specify the color of the area where the graph is drawn, excluding the legend</description></item><item><title>Why Notation of Partial Differential is Different?</title><link>https://freshrimpsushi.github.io/en/posts/2573/</link><pubDate>Fri, 24 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2573/</guid><description>Question In partial derivatives, unlike the usual derivatives, expressions like $\displaystyle {{ \partial f } \over { \partial t }}$ are used instead of $\displaystyle {{ d f } \over { d t }}$. $\partial$ is read as &amp;ldquo;Round Dee&amp;rdquo; or &amp;ldquo;Partial,&amp;rdquo; and historically, it originated from &amp;ldquo;Curly Dee,&amp;rdquo; which is a cursive form of $d$1. In code, it&amp;rsquo;s \partial, and in Korea, some people even shorten it to just</description></item><item><title>How to Specify Graph Colors for Each Subplot in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3602/</link><pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3602/</guid><description>Overview This section introduces three methods for specifying graph colors for each subplot. To learn how to specify colors for graph elements, refer here. Method 1 The first way to specify the graph color for a subplot is to predefine the color when defining each subplot. In Julia, since a picture is an object itself, you can define multiple pictures with different attributes and then combine them into one plot.</description></item><item><title>How to Add a Main Title in Julia Subplots</title><link>https://freshrimpsushi.github.io/en/posts/2572/</link><pubDate>Wed, 22 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2572/</guid><description>Overview When drawing figures in Julia, to apply a title to all subplots, one should use plot_title instead of title1. This is because the arguments of the outermost plot() function, in the case of a plot with subplots like plot( plot1, plot2, ... ) inherit properties to the inner subplots. To clearly distinguish between them, title and plot_title are used separately. Code plot(p1, p2, title = &amp;#34;Two Plots&amp;#34;) As you</description></item><item><title>Specifying the Color of Graph Elements in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3601/</link><pubDate>Tue, 21 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3601/</guid><description>Overview In Plots.jl, the keywords for specifying the color of each graph component are as follows. Keyword Function markercolor, mc Specify the marker&amp;rsquo;s inside color markerstrokecolor, msc Specify the marker&amp;rsquo;s border color linecolor, lc Specify the line color fillcolor, fc Specify the fill color seriescolor, c Specify the color of all components Keyword Function markeralpha, ma, mα Specify the marker&amp;rsquo;s inside transparency markerstrokealpha, msa, msα Specify the</description></item><item><title>What is LASSO Regression?</title><link>https://freshrimpsushi.github.io/en/posts/2571/</link><pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2571/</guid><description>Definition $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ Given $n$ data points and</description></item><item><title>How to Use RGB Color Codes in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3600/</link><pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3600/</guid><description>Code The package provided in Julia for dealing with colors is Colors.jl. By importing the visualization package Plots.jl, the features within Colors.jl can also be used. The color codes representing the RGB space include RGB, BGR, RGB24, RGBX, XRGB, which are subtypes of AbstractRGB. RGBA adds transparency to RGB. julia&amp;gt; using Plots julia&amp;gt; subtypes(AbstractRGB) 5-element Vector{Any}: BGR RGB RGB24 RGBX XRGB julia&amp;gt; subtypes(AbstractRGBA) 2-element Vector{Any}: BGRA RGBA Strings For the</description></item><item><title>How to Remove Axis Values in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/2570/</link><pubDate>Sat, 18 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2570/</guid><description>Overview In Julia, there are ways to remove color bars, axes, ticks, grids, etc., but these involve graphic elements, so it&amp;rsquo;s not possible to cleanly remove numbers alone. You must use an option called formatter. formatter = (_...) -&amp;gt; &amp;quot;&amp;quot; By giving the option formatter = (_...) -&amp;gt; &amp;quot;&amp;quot; to the plot() function, it&amp;rsquo;s done. using Plots x = rand(10) y = rand(10) plot( plot(x,y) ,plot(x,y, formatter = (_...) -&amp;gt;</description></item><item><title>Package for Color Processing in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3599/</link><pubDate>Fri, 17 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3599/</guid><description>Introduction1 Introducing the capabilities of Colors.jl, a package for color processing in Julia. When using the visualization package Plots.jl, there&amp;rsquo;s no need to load Colors.jl separately. It provides the following functionalities: Color parsing and conversion Color maps Color scales Parsing and Conversion Assuming str is a string representing color information, you can parse the string into a color code of a specific color space using @colorant_str or parse(Colorant, str). Note</description></item><item><title>Hard Thresholding and Soft Thresholding as Functions</title><link>https://freshrimpsushi.github.io/en/posts/2569/</link><pubDate>Thu, 16 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2569/</guid><description>Definition 1 A threshold $\lambda \in \mathbb{R}$ is given. Hard Thresholding We define $\eta _{H} \left( x ; \lambda \right) : \mathbb{R} \to \mathbb{R}$ as Hard Thresholding as follows: $$ \begin{align*} \eta _{H} \left( x ; \lambda \right) =&amp;amp; x \cdot \mathbf{1}_{\left\{ \left| x \right| \ge \lambda \right\}} \\ =&amp;amp; \begin{cases} x &amp;amp; , \text{if } x \in [-\lambda, \lambda] \\ 0 &amp;amp; , \text{if } x \notin [-\lambda, \lambda]</description></item><item><title>How to Use Colors in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3598/</link><pubDate>Wed, 15 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3598/</guid><description>Overview The package that facilitates the convenient use of colors in Julia is Colors.jl. It can be used together just by importing the visualization package Plots.jl. Symbols and Strings The way to check the list of named colors is by entering Colors.color_names in the console window or checking the official documentation. julia&amp;gt; using Plots julia&amp;gt; Colors.color_names Dict{String, Tuple{Int64, Int64, Int64}} with 666 entries: &amp;#34;darkorchid&amp;#34; =&amp;gt; (153, 50, 204) &amp;#34;chocolate&amp;#34; =&amp;gt;</description></item><item><title>How to Use Finite Difference in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2568/</link><pubDate>Tue, 14 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2568/</guid><description>Overview To use Finite Differences in Julia, more specifically to calculate the coefficients of finite differences, it is advisable to use FiniteDifferences.jl1. In cases where there is susceptibility to noise, it’s possible to use Total Variation Regularized Numerical Differentiation, known as TVDiff, implemented in NoiseRobustDifferentiation.jl. Code FiniteDifferenceMethod() julia&amp;gt; f′ = FiniteDifferenceMethod([-2, 0, 5], 1) FiniteDifferenceMethod: order of method: 3 order of derivative: 1 grid: [-2, 0,</description></item><item><title>Decorating Text Output with Built-in Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3597/</link><pubDate>Mon, 13 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3597/</guid><description>Code Using the function printstyled(string; color = color) allows you to decorate the outputted function. As input for the keyword argument color, symbols and integers $(0 \le n \le 255)$ are possible. Note that strings are not allowed. The available symbols include not only colors but also options like :blink, :reverse, etc. These can also be applied by entering them as keyword arguments like blink = true, bold = true.</description></item><item><title>What is Ridge Regression?</title><link>https://freshrimpsushi.github.io/en/posts/2567/</link><pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2567/</guid><description>Definition $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ Given a set of data</description></item><item><title>Proximal Alternating Linearized Minimization Algorithm (PALM)</title><link>https://freshrimpsushi.github.io/en/posts/3596/</link><pubDate>Sat, 11 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3596/</guid><description>Overview Jérôme Bolte, Shoham Sabach, and Marc Teboulle introduced an optimization technique called Proximal Alternating Linearized Minimization (PALM) algorithm in their paper Proximal alternating linearized minimization for nonconvex and nonsmooth problems. Algorithm The method to solve optimization problems like $(1)$ is called the Proximal Alternating Linearized Minimization (PALM) algorithm. This, in simple terms, means performing alternating optimization for two variables using the proximal gradient method.</description></item><item><title>Numerical Interpolation in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2566/</link><pubDate>Fri, 10 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2566/</guid><description>Overview In Julia, the Interpolations.jl package is used for numerical interpolation1. Be cautious not to confuse it with the interpolation method used when printing the value of a variable in Julia[../2041]. Code Interpolate() julia&amp;gt; y = rand(10) 10-element Vector{Float64}: 0.8993801321974316 0.12988982511901515 0.49781160399025925 0.22555299914088356 0.4848674643768577 0.6089318286915111 0.10444895196527337 0.5921775799940143 0.2149546302906653 0.32749334953170317 julia&amp;gt; f = interpolate(y, BSpline(Linear())); julia&amp;gt; f(1.2) 0.7454820707817483 julia&amp;gt; f(0.1) ERROR: BoundsError: attempt to access 10-element interpolate(::Vector{Float64}, BSpline(Linear())) with element type</description></item><item><title>Proximal Gradient Method</title><link>https://freshrimpsushi.github.io/en/posts/3595/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3595/</guid><description>Definition 1 Let&amp;rsquo;s say the non-differentiable objective function $H(\mathbf{x}) : \mathbb{R}^{n} \to \mathbb{R}$ is decomposed into a differentiable function $f$ and a non-differentiable function $g$. $$ H(\mathbf{x}) = f(\mathbf{x}) + g(\mathbf{x}) $$ The method of solving the optimization problem for $H$ using the following iterative algorithm is called the proximal gradient method. $$ \mathbf{x}^{(k+1)} = \operatorname{prox}_{\lambda g}(\mathbf{x}^{(k)} - \lambda \nabla f(\mathbf{x}^{(k)})) $$ Explanation It is called the proximal gradient method</description></item><item><title>Sum of Squared Residuals' Gradient</title><link>https://freshrimpsushi.github.io/en/posts/2565/</link><pubDate>Wed, 08 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2565/</guid><description>Overview In many regression problems in statistics and machine learning, the sum of squared residuals is used as the objective function, especially when $f$ is a linear combination, it can be succinctly expressed in matrix form. $$ \begin{align*} RSS =&amp;amp; \sum_{k} \left( y_{k} - f \left( \mathbf{x}_{k} \right) \right)^{2} \\ =&amp;amp; \sum_{k} \left( y_{k} - \left( s_{0} + s_{1} x_{k1} + \cdots + s_{p} x_{kp} \right) \right)^{2} \\ =&amp;amp; \left(</description></item><item><title>Subgradient Method</title><link>https://freshrimpsushi.github.io/en/posts/3594/</link><pubDate>Tue, 07 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3594/</guid><description>Definition1 Let&amp;rsquo;s say the objective function $f : \mathbb{R}^{n} \to \mathbb{R}$ is a convex function. Let&amp;rsquo;s denote the subgradient of $f$ at point $\mathbf{x}^{(k)}$ as $\mathbf{g}^{(k)}$. The method of updating $\mathbf{x}^{(k)}$ in the following way to solve the optimization problem for $f$ is called the subgradient method. $$ \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \alpha \mathbf{g}^{(k)} $$ Description2 It&amp;rsquo;s a form where the gradient in the Gradient Descent is replaced with a</description></item><item><title>Calculating the Difference of Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2564/</link><pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2564/</guid><description>Overview In Julia, the diff() function is provided to calculate differences1. It&amp;rsquo;s also possible to use the circshift() function to easily create a similar effect, but dealing with end points and such can be somewhat inconvenient, so knowing how to use diff() can be much more comfortable. It can be used almost in the same way as the diff() functions in R and MATLAB, however, unlike these, Julia does not</description></item><item><title>Subgradient</title><link>https://freshrimpsushi.github.io/en/posts/3593/</link><pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3593/</guid><description>Definition 1 2 For a function $f : \mathbb{R}^{n} \to \mathbb{R}$, the $\mathbf{g} \in \mathbb{R}^{n}$ that satisfies the following is called a subgradient of $f$ at point $\mathbf{x}$. $$ f(\mathbf{y}) \ge f(\mathbf{x}) + \mathbf{g}^{T}(\mathbf{y} - \mathbf{x}) \qquad \forall \mathbf{y} \in \mathbb{R}^{n} $$ Explanation If the convex function $f$ is differentiable at $\mathbf{x}$, then $\mathbf{g} =$ $\nabla f(\mathbf{x})$ is unique. Conversely, if $\partial f(\mathbf{x}) = \left\{ \mathbf{g} \right\}$, then $f$ is</description></item><item><title>What is Sparse Regression?</title><link>https://freshrimpsushi.github.io/en/posts/2563/</link><pubDate>Sat, 04 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2563/</guid><description>Definition Let&amp;rsquo;s consider a matrix equation given by matrix $A \in \mathbb{R}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{R}^{m}$ as follows: $$ A \mathbf{x} = \mathbf{b} $$ Sparse Regression refers to methods that aim to find a solution or least squares solution to such matrix equations, maximizing the sparsity of $\mathbf{x}$, in other words, minimizing the number of non-zero elements in $\mathbf{x}$ as much as possible. When the solution $\mathbf{x}$</description></item><item><title>Alternating Optimization</title><link>https://freshrimpsushi.github.io/en/posts/3592/</link><pubDate>Fri, 03 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3592/</guid><description>Definition When optimizing a multivariate objective function, the practice of optimizing over one variable at a time, alternating between variables, is known as alternating optimization. Description Consider the following optimization problem where the objective function is $H(x,y)$. $$ \argmin\limits_{x,y} H(x,y) $$ This can be divided into two subproblems by fixing one variable and optimizing over the other. $$ \begin{cases} \argmin\limits_{x} H(x,y) \\ \argmin\limits_{y} H(x,y) \end{cases} $$ Alternating optimization is the</description></item><item><title>How to Use Circular Arrangements in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2562/</link><pubDate>Thu, 02 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2562/</guid><description>Overview Though Julia does not natively support Circular Arrays, it essentially allows for such functionality by providing the circshift() function, which pushes or pulls elements circularly1. It&amp;rsquo;s not particularly difficult to write this function yourself, but knowing it obviates the need. This function can be used almost exactly like the circshift() in MATLAB. Code This function has been introduced in the post about how to translate arrays in parallel as</description></item><item><title>Proximal Minimization Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/3591/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3591/</guid><description>Definition1 When solving the optimization problem for the objective function $f : \mathbb{R}^{n} \to \mathbb{R}^{n}$, the method of updating the optimal solution $\mathbf{x}^{(k)}$ by repeatedly applying the proximal operator is called the proximal minimization algorithm. $$ \mathbf{x}^{(k+1)} = \operatorname{prox}_{\lambda f}(\mathbf{x}^{(k)}) = \argmin\limits_{\mathbf{v}} \left\{ \lambda f(\mathbf{v}) + \dfrac{1}{2}\left\| \mathbf{v} - \mathbf{x}^{(k)} \right\|_{2}^{2} : \mathbf{v} \in \mathbb{R}^{n} \right\} $$ Description It is also known as the proximal point algorithm or proximal iteration.</description></item><item><title>Hankel Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2561/</link><pubDate>Tue, 30 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2561/</guid><description>Definition $$ H = \begin{bmatrix} h_{11} &amp;amp; h_{12} &amp;amp; h_{13} &amp;amp; \cdots &amp;amp; h_{1n} \\ h_{21} &amp;amp; h_{22} &amp;amp; h_{23} &amp;amp; \cdots &amp;amp; h_{2n} \\ h_{31} &amp;amp; h_{32} &amp;amp; h_{33} &amp;amp; \cdots &amp;amp; h_{3n} \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ h_{m1} &amp;amp; h_{m2} &amp;amp; h_{m3} &amp;amp; \cdots &amp;amp; h_{mn} \end{bmatrix} $$ A given matrix $H = \left( h_{ij} \right) \in \mathbb{R}^{m \times n}$ is called</description></item><item><title>Proximal Operator</title><link>https://freshrimpsushi.github.io/en/posts/3590/</link><pubDate>Mon, 29 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3590/</guid><description>Definition1 Let $\left\| \cdot \right\|_{X}$ be the norm of the vector space $X$. The proximal operator $\operatorname{prox}_{\lambda f} : X \to 2^{X}$ for a function $f : X \to \mathbb{R}$ is defined as follows, for $\lambda &amp;gt; 0$, $$ \begin{align} \operatorname{prox}_{\lambda f} (\mathbf{x}) &amp;amp;:= \argmin\limits_{\mathbf{v}} \left\{ \lambda f(\mathbf{v}) + \dfrac{1}{2}\left\| \mathbf{v} - \mathbf{x} \right\|_{X}^{2} : \mathbf{v} \in X \right\} \\ &amp;amp;= \argmin\limits_{\mathbf{v}} \left\{ f(\mathbf{v}) + \dfrac{1}{2\lambda}\left\| \mathbf{v} - \mathbf{x} \right\|_{X}^{2}</description></item><item><title>List of Markers and Line Styles in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2560/</link><pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2560/</guid><description>Code 1 No need for a lengthy description, it literally shows what the marker and line styles look like in reality. linesytle Choose one from [:auto, :solid, :dash, :dot, :dashdot, :dashdotdot]. shape Choose one from [:none, :auto, :circle, :rect, :star5, :diamond, :hexagon, :cross, :xcross, :utriangle, :dtriangle, :rtriangle, :ltriangle, :pentagon, :heptagon, :octagon, :star4, :star6, :star7, :star8, :vline, :hline, :+, :x]. Full Code ▷code1◁ Environment OS:</description></item><item><title>Latent Variable and Latent Space</title><link>https://freshrimpsushi.github.io/en/posts/3589/</link><pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3589/</guid><description>Definition Suppose a dataset $X \subset \mathbb{R}^{n}$ is given. A function with the dataset as its domain is called an encoder. $$ \begin{align*} f : X &amp;amp;\to Z \\ \mathbf{x} &amp;amp;\mapsto \mathbf{z} = f(\mathbf{x}) \end{align*} $$ The range of the encoder $Z \subset \mathbb{R}^{m}$ ($m \le n$) is called the latent space, and the elements of the latent space $\mathbf{z}$ are referred to as latent variables or feature vectors. For</description></item><item><title>Definition of the Arctan2 Function</title><link>https://freshrimpsushi.github.io/en/posts/2559/</link><pubDate>Fri, 26 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2559/</guid><description>Definition Arc Tangent 2 $\arctan 2 : \left( \mathbb{R}^{2} \setminus \left\{ (0,0) \right\} \right) \to \mathbb{R}$ is defined as follows. $$ \arctan 2 : \left( r \sin \theta , r \cos \theta \right) \mapsto \theta $$ $r &amp;gt; 0$ is any positive number. Description Arc Tangent 2 is used in fields such as mechanical engineering to supplement the information that is insufficient with Arc Tangent $\arctan$, considering it provides the</description></item><item><title>What are Generalized Coordinates in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/3588/</link><pubDate>Thu, 25 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3588/</guid><description>Definition1 The coordinates of a system of particles with $n$ degrees of freedom, expressed in $n$ variables (1) that are independent of constraints and (2) mutually independent, are called generalized coordinates. Generalized Coordinates In a three-dimensional space where the degrees of freedom of a particle are $3$, the position of the particle can be expressed by the generalized coordinates $q_{1}, q_{2}, q_{3}$ as follows: $$ \begin{align*} x &amp;amp;= x(q_{1}, q_{2},</description></item><item><title>Drawing a Regression Line on a Julia Plot</title><link>https://freshrimpsushi.github.io/en/posts/2558/</link><pubDate>Wed, 24 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2558/</guid><description>Code To insert a regression line in the scatter plot of Julia, simply use the option smooth = true. using Plots x = rand(100) scatter(x, 2x .+ 0.1randn(100), smooth = true) savefig(&amp;#34;plot.svg&amp;#34;) Environment OS: Windows julia: v1.8.3 Plots v1.38.5</description></item><item><title>What is a Constraint in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/3587/</link><pubDate>Tue, 23 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3587/</guid><description>Definition A particle, or a system of particles, is said to undergo constrained motion when it moves only within a geometrically confined area (such as given curves or surfaces), and such restrictions themselves are called constraints. Explanation In Korean, it is commonly referred to as 구속조건, but in English, it is just called a constraint, not a constraint condition. Simple examples of constrained motion include circular</description></item><item><title>3D Rotation Transformation Matrix: Roll, Pitch, Yaw</title><link>https://freshrimpsushi.github.io/en/posts/2557/</link><pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2557/</guid><description>Definition 1 In a 3-dimensional space $\mathbb{R}^{3}$, a matrix $R_{x}$, $R_{y}$, $R_{z}$ that rotates a vector around the $x$ axis, $y$ axis, and $z$ axis in a counterclockwise direction by $\theta$ is as follows. $$ \begin{align*} R_{x} =&amp;amp; \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; \cos \theta &amp;amp; - \sin \theta \\ 0 &amp;amp; \sin \theta &amp;amp; \cos \theta \end{bmatrix} \\ R_{y} =&amp;amp; \begin{bmatrix} \cos \theta &amp;amp; 0</description></item><item><title>What is Degrees of Freedom in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/3586/</link><pubDate>Sun, 21 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3586/</guid><description>Definition The number of independent coordinates among the total coordinates of a particle system is called the degree of freedom. Explanation To put it simply, the degree of freedom is the minimum number of variables needed to describe a particle system. Consider a particle moving freely in three-dimensional space. The position of this particle can be expressed as $r = (x,y,z)$, and since the variables for each axis $x, y,</description></item><item><title>The Difference Between Inf and NaN When Dividing by Zero in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2556/</link><pubDate>Sat, 20 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2556/</guid><description>Overview Explains how 0/0 and 1/0 are different in Julia. Code julia&amp;gt; 0/0 NaN julia&amp;gt; 1/0 Inf At least in programming, infinity Inf can be quite useful in terms of initialization or comparison of sizes, making division by 0 not so strange after all. The problem is ${{ 0 } \over { 0 }} \ne \infty$, and in Julia, it is treated as NaN. In other words, while dividing by</description></item><item><title>Differentiation Operators and Symbols</title><link>https://freshrimpsushi.github.io/en/posts/3585/</link><pubDate>Fri, 19 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3585/</guid><description>Definition1 For a natural number $m \in \mathbb{N}$, a differential operator refers to the following map $P$. $$ \begin{equation} P = \sum\limits_{\left| \alpha \right| \le m} a_{\alpha}(x) D^{\alpha},\qquad x = (x_{1}, \dots, x_{n}) \end{equation} $$ Here, $\alpha = (\alpha_{1}, \dots, \alpha_{n})$ is a multi-index. $D^{\alpha}$ is as follows. $$ \begin{align*} D^\alpha &amp;amp;:= \dfrac{\partial ^{|\alpha|} } {{\partial x_{1}}^{\alpha_{1}}\cdots {\partial x_{n}}^{\alpha_{n}}} \\ &amp;amp;= \left( \frac{ \partial }{ \partial x_{1}} \right)^{\alpha_{1}}\left( \frac{ \partial</description></item><item><title>Expectation of Random Vectors</title><link>https://freshrimpsushi.github.io/en/posts/2555/</link><pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2555/</guid><description>Definition 1 $$ E \left( X \right) := \begin{bmatrix} E \left( X_{1} \right) \\ \vdots \\ E \left( X_{n} \right) \end{bmatrix} $$ The expectation of a random vector $X = \left( X_{1} , \cdots , X_{n} \right)$ is defined as a vector of the expectations of its components, as shown above. Similarly, the matrix $\mathbf{X} = \left[ X_{ij} \right]$ of a random variable of size $m \times n$ is also</description></item><item><title>Polyharmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3584/</link><pubDate>Wed, 17 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3584/</guid><description>Definition Let $\Delta = \nabla^{2}$ be called a Laplacian. For a natural number $k \in \mathbb{N}$, $\Delta ^{k}$ is referred to as a polyharmonic operator or a polylaplacian. The equation below is called the polyharmonic equation. $$ \Delta^{k} f = 0 $$ The solutions to the polyharmonic equation are referred to as polyharmonic functions. Description It is a generalization of harmonic functions. See Also Harmonic functions Biharmonic functions Polyharmonic functions</description></item><item><title>Tips for passing Optional Arguments through the Julia Splatt Operator</title><link>https://freshrimpsushi.github.io/en/posts/2554/</link><pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2554/</guid><description>Overview In Julia, the most frequently used purpose of the ... splat is explained as the method of passing optional arguments. Basically, it uses the method of applying the splat operator to the tuple after determining in advance what options to put into which arguments, in the form of a named tuple. Code Passing to Multiple Functions args1 = (; dims = 1) The named tuple args1 above can be</description></item><item><title>Biharmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3583/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3583/</guid><description>Definition1 Let&amp;rsquo;s call $\Delta = \nabla^{2}$ the Laplacian. $\Delta ^{2}$ is called the biharmonic operator or bilaplacian. The following equation is called the biharmonic equation. $$ \Delta^{2} f = 0 $$ The solutions to the biharmonic equation are called biharmonic functions. Explanation Let&amp;rsquo;s say $\partial_{i} = \dfrac{\partial}{\partial x_{i}}$. In the Cartesian coordinate system, since $\Delta = \sum\limits_{i} \partial_{i}\partial_{i}$, $$ \Delta^{2} f = \sum\limits_{j} \sum\limits_{i} \partial_{j}\partial_{j} \partial_{i}\partial_{i} f $$ Especially in</description></item><item><title>Derivation of Finite Difference Using Multiple Points</title><link>https://freshrimpsushi.github.io/en/posts/2553/</link><pubDate>Sun, 14 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2553/</guid><description>Theorem Given a differentiable function eq01, at a point eq03 where the n-th derivative&amp;rsquo;s value is eq06, can be approximated by using the same number of points eq10 from a finite set eq09 with cardinality eq08 for sufficiently small eq07 as follows: eq01 Here, eq11 is determined as follows: eq02 The points eq10 are also referred to as Stencil Points. eq14 is the inverse matrix. eq15 is the factorial. eq16</description></item><item><title>파이썬에서 운영체제 확인하는 방법</title><link>https://freshrimpsushi.github.io/en/posts/3582/</link><pubDate>Sat, 13 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3582/</guid><description>Code You can check it with platform.system(). &amp;gt;&amp;gt;&amp;gt; import platform &amp;gt;&amp;gt;&amp;gt; platform.system() &amp;#39;Windows&amp;#39; # Windows의 경우 &amp;gt;&amp;gt;&amp;gt; platform.system() &amp;#39;Linux&amp;#39; # Ubuntu의 경우 See Also Check the operating system with platform.system() Check PC username with os.getlogin Check PC name with socket.gethostname()</description></item><item><title>Julia's Splat Operator</title><link>https://freshrimpsushi.github.io/en/posts/2552/</link><pubDate>Fri, 12 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2552/</guid><description>Overview In Julia, ... is called the splat operator. It is usefully employed when using functions or defining arrays1. This operator isn&amp;rsquo;t exclusive to Julia, but it&amp;rsquo;s defined in a more intuitive way compared to other languages, making it exceptionally easy to learn and understand. From personal experience, using ... seems to bring some sort of enlightenment regarding Julia programming. Code Function Input Primarily, ... is appended after an array</description></item><item><title>파이썬에서 PC 이름 얻는 방법</title><link>https://freshrimpsushi.github.io/en/posts/3581/</link><pubDate>Thu, 11 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3581/</guid><description>Code You can obtain it using os.getlogin. &amp;gt;&amp;gt;&amp;gt; import os &amp;gt;&amp;gt;&amp;gt; os.getlogin() &amp;#39;rydbr&amp;#39; See Also Check operating system using platform.system() Retrieve PC username using os.getlogin Check PC name using socket.gethostname() Environment OS: Windows 11 Version: Python 3.11.5</description></item><item><title>Rössler Attractor</title><link>https://freshrimpsushi.github.io/en/posts/2551/</link><pubDate>Wed, 10 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2551/</guid><description>Overview The Rössler Equation is one of the systems introduced to have a simple yet chaotic attractor. System1 $$ \begin{align*} {{dx} \over {dt}} =&amp;amp; - y - z \\ {{dy} \over {dt}} =&amp;amp; x + ay \\ {{dz} \over {dt}} =&amp;amp; b + (x-c) z \end{align*} $$ Variables $x(t)$: Represents the $x$ coordinate of a particle at time $t$. $y(t)$: Represents the $y$ coordinate</description></item><item><title>Sperical Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/3580/</link><pubDate>Tue, 09 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3580/</guid><description>Definition 1 The coordinates of a point $p$ in coordinate space are represented by &amp;ldquo;the distance $r$ between the origin and $p$,&amp;rdquo; &amp;ldquo;the angle $\theta$ between the line connecting the origin and $p$ and the positive direction of the $z-$axis,&amp;rdquo; and &amp;ldquo;the angle $\phi$ between the projection of the line connecting the origin and $p$ on the $xy-$plane and the positive direction of the $x-$axis.&amp;rdquo; This is called the spherical</description></item><item><title>Slicing Only a Part of Unicode Strings in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2550/</link><pubDate>Mon, 08 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2550/</guid><description>Overview As with many programming languages, in Julia, English is written in ASCII code and characters like Chinese and Korean are written in Unicode. The trouble, unlike with other languages, is that dealing with these strings is quite tricky, which is intended for performance reasons1, so one has no choice but to bear with it and use them as they are. Code julia&amp;gt; str1 = &amp;#34;English&amp;#34; &amp;#34;English&amp;#34; julia&amp;gt; str2 =</description></item><item><title>Coordinates of a Möbius Strip in Three-Dimensional Space</title><link>https://freshrimpsushi.github.io/en/posts/3579/</link><pubDate>Sun, 07 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3579/</guid><description>Definition1 A surface represented by the coordinate chart mapping $\mathbf{x}$ is called a Möbius band. $$ \mathbf{x}(\theta, v) = (\cos \theta, \sin \theta, 0) + v(\textstyle \sin\frac{\theta}{2}\cos\theta, \sin\frac{\theta}{2}\sin\theta, \cos\frac{\theta}{2}) $$ $$ \textstyle -\pi \lt \theta \lt \pi, \quad -\frac{1}{2} \lt v \lt \frac{1}{2} $$ Properties The Möbius band is not an orientable surface Richard S. Millman and George D. Parker,</description></item><item><title>What is an F1 Score in Data Science?</title><link>https://freshrimpsushi.github.io/en/posts/2549/</link><pubDate>Sat, 06 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2549/</guid><description>Definition Let’s assume a model is given for distinguishing Positive $P$ and Negative $N$ in a classification problem. The number of positives correctly identified as positive is referred to as True Positive $TP$, the number of negatives correctly identified as negative is referred to as True Negative $TN$, the number of positives incorrectly identified as negative is referred to as False Negative $FN$, and the number</description></item><item><title>Coordinate Space, Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/3578/</link><pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3578/</guid><description>Definition When the coordinate plane and the number line meet at the origin of the coordinate plane and are drawn orthogonally, it is called a coordinate space. The vertical line that is orthogonal to the coordinate plane is called the $z-$axis. The point determined by the three axes as shown above is called point $(a,b,c)$. Point $(0,0,0)$ is called the origin. The coordinate plane made by the $x$ axis and</description></item><item><title>Omitting DataFrame Names in Julia StatsPlots with Macro @df</title><link>https://freshrimpsushi.github.io/en/posts/2548/</link><pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2548/</guid><description>Overview In the StatsPlots package of Julia, the @df macro allows omitting the repeatedly mentioned dataframe name when plotting1. The syntax for using the macro, when using column a of dataframe X, is to specify which dataframe to use with @df X, followed immediately by passing the argument a as a symbol :a in the scope that follows, writing it as plot (:a). In summary, the code is written as</description></item><item><title>Polar Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/3577/</link><pubDate>Wed, 03 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3577/</guid><description>Definition The coordinates of a point on the coordinate plane are defined as &amp;ldquo;the distance $r$ from the origin&amp;rdquo; and &amp;ldquo;the angle $\theta$ made by the line connecting the point to the origin with the $x$ axis,&amp;rdquo; which is referred to as the polar coordinate system. Explanation It is useful for expressing functions that depend on the distance from the origin. For example, the position of an object performing circular</description></item><item><title>What is Reproducibility in Data Science?</title><link>https://freshrimpsushi.github.io/en/posts/2547/</link><pubDate>Tue, 02 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2547/</guid><description>Definition Let&amp;rsquo;s assume that in a classification problem distinguishing between Positive $P$ and Negative $N$, a model is given to assess positives and negatives. Let the numbers of correctly identified positives be called True Positive $TP$ and those of negatives be called True Negative $TN$, while incorrectly identified positives as False Negative $FN$ and negatives as False Positive $FP$. Mathematical Definition The following metric is referred to as the Recall</description></item><item><title>Definition of Coordinate Plane</title><link>https://freshrimpsushi.github.io/en/posts/3576/</link><pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3576/</guid><description>Definition The coordinate plane is created by drawing two perpendicular lines that intersect at $0$ in an orthogonal manner. These lines are referred to as axes. The horizontal line is called the $x$ axis, and the vertical line is called the $y$ axis. A line drawn orthogonally to the $x$ axis at the real number $a$ on the $x$ axis, and a line drawn orthogonally to the $y$ axis at</description></item><item><title>Why Factorial 0 is Defined as 0!=1</title><link>https://freshrimpsushi.github.io/en/posts/2546/</link><pubDate>Sun, 31 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2546/</guid><description>Definition For the factorial of $0 \notin \mathbb{N}$, it is defined as follows $0!$. $$ 0! := 1 $$ Explanation Why is $0!$ not $0$ but $1$? Originally, since $0! := 1$ is a definition, there is no need for proof, and the process of understanding why such a definition is valid is close to a request to &amp;lsquo;accept why this definition was made.&amp;rsquo; Let&amp;rsquo;s understand it step by step</description></item><item><title>Definition of a Line</title><link>https://freshrimpsushi.github.io/en/posts/3575/</link><pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3575/</guid><description>Definition The assigning of each point on a straight line to a real number is called the number line. Explanation It is called a number line even though it is always drawn horizontally because it is line of numbers, not a perpendicular line. An arrow is usually drawn in the direction that the numbers increase. If you draw two number lines, it becomes a coordinate plane. Drawing three lines results</description></item><item><title>What is Precision in Data Science?</title><link>https://freshrimpsushi.github.io/en/posts/2545/</link><pubDate>Fri, 29 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2545/</guid><description>Definitions Assuming we have a model that distinguishes between positive $P$ and negative $N$ in a classification problem, let&amp;rsquo;s define the number of correctly identified positives as true positive $TP$, correctly identified negatives as true negative $TN$, incorrectly identified positives (as negative) as false negative $FN$, and incorrectly identified negatives (as positive) as false positive $FP$. Mathematical Definition The following value is referred to as the model&amp;rsquo;s Precision. $$ \textrm{Precision}</description></item><item><title>Geodesic Coordinate Mapping and Gaussian Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3574/</link><pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3574/</guid><description>Theorem1 The metric matrix of the geodesic coordinate mapping $\mathbf{x} : U \to \mathbb{R}^{3}$ is as follows. $$ \left[ g_{ij} \right] = \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; h^{2} \end{bmatrix} \quad (h \gt 0) $$ Then, the Gaussian curvature of $\mathbf{x}$ is as follows. $$ K = -\dfrac{h_{11}}{h} $$ At this time, $(u^{1}, u^{2})$ is the coordinate of $U$, and $h_{i} = \dfrac{\partial h}{\partial u^{i}}$. Proof Gauss&amp;rsquo; Theorema Egregium</description></item><item><title>How to Use Functions Defined in Other Files in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2544/</link><pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2544/</guid><description>Overview Introducing the function include() which executes Julia code itself to use functions from another file. In MATLAB, if it&amp;rsquo;s in the same directory, it tends to automatically find the function, so some people think this process is hard. There is a way to properly modularize and export, but1 it is not recommended for beginners who urgently need functionality because it is difficult and complicated. It&amp;rsquo;s not too late to</description></item><item><title>Geodesic Coordinate Patch Mapping and Christoffel Symbols</title><link>https://freshrimpsushi.github.io/en/posts/3573/</link><pubDate>Tue, 26 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3573/</guid><description>Theorem1 $$ \left[ g_{ij} \right] = \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; h^{2} \end{bmatrix} \quad (h \gt 0) $$ Then, the Christoffel symbols of $\mathbf{x}$ are as follows, and except for the below, all are $0$. $$ \Gamma_{22}^{1} = -hh_{1},\quad \Gamma_{12}^{2} = \Gamma_{21}^{2} = \dfrac{h_{1}}{h},\quad \Gamma_{22}^{2} = \dfrac{h_{2}}{h} $$ At this time, $(u^{1}, u^{2})$ is the coordinate of $U$, and $h_{i} = \dfrac{\partial h}{\partial u^{i}}$ is valid. Proof Before</description></item><item><title>The Overestimation of Accuracy in Data Science</title><link>https://freshrimpsushi.github.io/en/posts/2543/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2543/</guid><description>Definition Let&amp;rsquo;s assume a given model that distinguishes positive $P$ from negative $N$ in a classification problem. Let&amp;rsquo;s denote the count of correctly identified positives as true positive $TP$, the count of correctly identified negatives as true negative $TN$, the count of positives incorrectly identified as negatives as false negative $FN$, and the count of negatives incorrectly identified as positives as false positive $FP$. The following figure is referred to</description></item><item><title>How to k-means cluster in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3572/</link><pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3572/</guid><description>Description k-means clustering is a clustering algorithm that divides the given $n$ data points into $k$ clusters. In Julia, it can be easily implemented using the Clustering.jl package. Code The following is a code to perform clustering with $k=3$ on the Iris dataset. Since data loaded from RDatasets.jl are by default data frames, they are converted into arrays, and transposed so that each column becomes a single data point. It</description></item><item><title>How to Perform System Restore on Windows</title><link>https://freshrimpsushi.github.io/en/posts/2542/</link><pubDate>Sat, 23 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2542/</guid><description>Guide Windows 11 is still experiencing many compatibility issues as of February 2023, and if you use your computer for professional purposes, you often have to do what&amp;rsquo;s colloquially known as a &amp;ldquo;clean reinstall.&amp;rdquo; However, there are many inconveniences such as the requirement for an internet connection during installation, so just doing a system restore is most convenient. Part 1. Commands Enter rstrui through cmd or wt, as shown. Part</description></item><item><title>Introduction to PROJ in Earth Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2541/</link><pubDate>Thu, 21 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2541/</guid><description>Build-up The Earth is round, and more precisely, it is considered an ellipsoid. A &amp;lsquo;globe&amp;rsquo;, which is a scaled-down model of the Earth, is an accurate model but not very widely used since, after all, humanity is still more comfortable with flat pictures. Thus, various coordinate systems have been devised, and there are&amp;hellip; quite a variety of them. 1 For example, the world map we commonly see uses the Mercator</description></item><item><title>Integration of 1/(1+x^2)</title><link>https://freshrimpsushi.github.io/en/posts/3570/</link><pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3570/</guid><description>Formulas Definite Integral $$ \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx = \pi $$ Indefinite Integral $$ \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx = \pi $$ $C$ is the integration constant. Proofs Definite Integral Let&amp;rsquo;s substitute with $x = \tan \theta$. Then, the range of integration becomes $\displaystyle \int_{-\infty}^{\infty} \to \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}$, and since $\tan ^{\prime} = \sec^{2}$, it results in $dx = \sec^{2} d\theta$. $$ \begin{align*} \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx &amp;amp;= \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \dfrac{1}{1 + \tan^{2}\theta} \sec^{2} \theta d\theta \\ &amp;amp;= \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}</description></item><item><title>Definitions of Classification and Regression Problems in Data Science</title><link>https://freshrimpsushi.github.io/en/posts/2540/</link><pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2540/</guid><description>Terms In [Data Science](../../categories/Data Science), problems are categorized based on what the dependent variable is, as follows. Classification Problem A problem is termed a classification problem if the dependent variable is a qualitative variable. Specifically, if the qualitative variable&amp;rsquo;s categories are only two, it is called a binary classification. Regression Problem Contrastingly, if the dependent variable is a quantitative variable, the problem is referred to as a regression, originating from</description></item><item><title>Differentiation of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/3569/</link><pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3569/</guid><description>Formula1 The derivatives of trigonometric functions are as follows. $$ \begin{align*} \dfrac{d}{dx} \sin x &amp;amp;= \cos x \qquad &amp;amp; \dfrac{d}{dx} \csc x &amp;amp;= -\csc x \cot x \\[1em] \dfrac{d}{dx} \cos x &amp;amp;= - \sin x \qquad &amp;amp; \dfrac{d}{dx} \sec x &amp;amp;= \sec x \tan x \\[1em] \dfrac{d}{dx} \tan x &amp;amp;= \sec^{2} x \qquad &amp;amp; \dfrac{d}{dx} \cot x &amp;amp;= -\csc^{2} x \end{align*} $$ Proof Sum formulas for trigonometric functions $$ \sin\left(</description></item><item><title>Population Balance Equation</title><link>https://freshrimpsushi.github.io/en/posts/2539/</link><pubDate>Sun, 17 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2539/</guid><description>Definition 1 Let&amp;rsquo;s denote the population at time $t$ as $P_{t}$. The population born from time $t$ to the next time point $(t+1)$ as $B_{t}$, the population that dies as $D_{t}$, the population of immigrants as $I_{t}$, and the population of emigrants as $E_{t}$. Then, the following equation is called the Demographic balancing Equation. $$ P_{t+1} = P_{t} + B_{t} - D_{t} + I_{t} - E_{t} $$ Here, $\left( B_{t}</description></item><item><title>The Limit of 1-cos(x)/x</title><link>https://freshrimpsushi.github.io/en/posts/3568/</link><pubDate>Sat, 16 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3568/</guid><description>Formula $$ \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} = 0 $$ Proof $$ \begin{align*} \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} &amp;amp;= \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} \dfrac{1 + \cos x}{1 + \cos x} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{1 - \cos^{2} x}{x(1+\cos x)} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{\sin^{2}x}{x(1+\cos x)} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{\sin x}{x} \dfrac{\sin x}{1+\cos x}</description></item><item><title>Independent and Dependent Variables in Data Science</title><link>https://freshrimpsushi.github.io/en/posts/2538/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2538/</guid><description>Terminology In the field of Statistics, Machine Learning, and other areas of Data Science, the variables in a model can generally be divided into two categories. Dependent Variables A Dependent Variable is typically the variable that is expected to be the result or output of the model, also called Output or Predictor Variable. Independent Variables An Independent Variable is the input variable that affects the dependent variable, also referred to</description></item><item><title>The limit of sin(x)/x</title><link>https://freshrimpsushi.github.io/en/posts/3567/</link><pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3567/</guid><description>공식 $$ \lim \limits_{x \to 0} \dfrac{\sin x}{x} = 1 $$ 증명 반지름이 $1$인 부채꼴 $OAB$가 주어졌다고 하자. 점 $B$에서 선분 $\overline{OA}$로 내린 수선의 발을</description></item><item><title>How to Color Markers in a Julia Fractal</title><link>https://freshrimpsushi.github.io/en/posts/2537/</link><pubDate>Wed, 13 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2537/</guid><description>Code using Plots x, y = rand(100), rand(100) Given the data above, depending on whether the data is continuous or categorical, the shape of the plot and the method of plotting differ. Continuous scatter(marker_z=) z = x + y scatter(x, y, marker_z = z) Categorical scatter(group=) 1 team = rand(&amp;#39;A&amp;#39;:&amp;#39;C&amp;#39;, 100) scatter(x, y, group = team) Environment OS: Windows julia: v1.8.3 https://stackoverflow.com/a/60846501/12285249&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>What are Frequency and Oscillation?</title><link>https://freshrimpsushi.github.io/en/posts/3566/</link><pubDate>Tue, 12 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3566/</guid><description>Definition The number of oscillations per unit time is called the frequency. Description Frequency refers to the same concept as oscillation frequency. Its unit is $\mathrm{Hz}$[Hertz], and it is commonly denoted by $f$, $\nu$, $k$. When saying $k \mathrm{Hz}$ is &amp;ldquo;1 second with $k번 진동한다는 뜻이다. 주기 $T$는 한 번 진동하는데 걸리</description></item><item><title>Brook's Auxiliary Lemma Proof</title><link>https://freshrimpsushi.github.io/en/posts/2536/</link><pubDate>Mon, 11 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2536/</guid><description>Theorem 1 Let&amp;rsquo;s represent the support of the probability mass function $p : \mathbb{R}^{n} \to \mathbb{R}$ of the random vector $Z : \Omega \to \mathbb{R}^{n}$ as follows. $$ S_{Z} = \left\{ \left( z_{1} , \cdots , z_{n} \right) \in \mathbb{R}^{n} : p \left( z_{1} , \cdots , z_{n} \right) &amp;gt; 0 \right\} \subset \Omega $$ For all $\mathbf{x} := \left( x_{1} , \cdots , x_{n} \right) \in S_{Z}$ and $\mathbf{y}</description></item><item><title>Sampling with Replacement and without Replacement in Python</title><link>https://freshrimpsushi.github.io/en/posts/3565/</link><pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3565/</guid><description>Code To perform sampling with replacement/replacement in Python, one can use the np.random.choice() function of numpy. random.choice(a, size=None, replace=True, p=None) a: 1-dimensional array or integer Represents the set from which to sample. If a is an integer, sampling is done from np.arange(a). size: An integer or a tuple of integers Represents the size of the output sample. replace: Boolean T for sampling with replacement, F for sampling without replacement. p:</description></item><item><title>How to Connect to an SSH Server via CLI</title><link>https://freshrimpsushi.github.io/en/posts/2535/</link><pubDate>Sat, 09 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2535/</guid><description>Guide Let&amp;rsquo;s say the IP of the server you&amp;rsquo;re trying to connect to is 192.168.1.1. ssh user@192.168.1.1 Enter the above command in the terminal1. Caution for Windows Servers Using Windows as a server, unlike Linux, there&amp;rsquo;s a bit annoying and concerning part. user: It implies the name of the account that has permission to access the server. It might be the first five letters of an account existing on that</description></item><item><title>Integrability of 1/x^p</title><link>https://freshrimpsushi.github.io/en/posts/3564/</link><pubDate>Fri, 08 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3564/</guid><description>Theorem The integrability of function $f(x) = \dfrac{1}{x^{p}}$ is as follows: when $x \in (0,1]$, if $p \lt 1$, then $f$ is integrable. when $x \in [1, \infty)$, if $p \gt 1$, then $f$ is integrable. Explanation If $x$ is less than $1$, then $p$ must also be less than $1$, and if $x$ is greater than $1$, then $p$ must also be greater than $1$. Just remember this. Proof</description></item><item><title>Resolving not defined because of singularities in R Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/2534/</link><pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2534/</guid><description>If you are a major in statistics or mathematics, it is strongly recommended not to stop at roughly identifying the cause and solving the faced problem but also to understand the mathematical proof. Error Diagnosis Coefficients: (1 not defined because of singularities) Estimate Std. Error t value Pr(&amp;gt;|t|) (Intercept) 0.5723 0.1064 5.381 4.98e-05 *** 최고기온 -0.3528 0.1490 -2.368 0.030 * 최저기온 0.2982 0.1955 1.525</description></item><item><title>Dimensionality Reduction in Data Science</title><link>https://freshrimpsushi.github.io/en/posts/3563/</link><pubDate>Wed, 06 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3563/</guid><description>Definition Let&amp;rsquo;s assume a data set $X \subset \mathbb{R}^{n}$ is given. The following mapping for $m \lt n$ is called dimension reduction. $$ r : X \to \mathbb{R}^{m} $$ Or more commonly in machine learning, any method that reduces the number of input variables in a way that retains as much of the performance as possible is called a dimension reduction technique. Explanation Dimension reduction, as the name suggests, refers</description></item><item><title>Inverse Matrix of X^T X: Necessary and Sufficient Conditions</title><link>https://freshrimpsushi.github.io/en/posts/2533/</link><pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2533/</guid><description>Theorem Suppose that the matrix $X \in \mathbb{R}^{m \times n}$ is given and $m \ge n$, then the following holds: $$ \exists \left( X^{T} X \right)^{-1} \iff \text{rank} X = n $$ In other words, the necessary and sufficient condition for the inverse matrix of $X^{T} X$ to exist is that $X$ has full rank. $X^{T}$ is the transpose of $X$. Explanation The reason this fact is important is that</description></item><item><title>How to Define and Train MLP with the Sequence Model and Functional API in TensorFlow and Keras</title><link>https://freshrimpsushi.github.io/en/posts/3562/</link><pubDate>Mon, 04 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3562/</guid><description>Overview In TensorFlow, neural networks can be easily defined using Keras. Below, we introduce how to define and train a simple MLP using Sequential() and the functional API. However, Sequential() is only easy for defining models and can be challenging to use for designing complex structures. Similarly, if you plan to design complex structures using the functional API, it’s better to use the keras.Model class, and for</description></item><item><title>Windows 11 Initial Setup</title><link>https://freshrimpsushi.github.io/en/posts/2532/</link><pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2532/</guid><description>Guide Detailed explanations are omitted. All commands must be entered in Windows Terminal running as administrator. Reset Context Menu 1 reg.exe add &amp;#34;HKCU\Software\Classes\CLSID\{86ca1aa0-34aa-4e8b-a509-50c905bae2a2}\InprocServer32&amp;#34; /f /ve taskkill /f /im explorer.exe explorer.exe Ungroup Taskbar 2 Download and run ep_setup.exe from the latest stable version. KMS 3 slmgr /ipk TX9XD-98N7V-6WMQ6-BX7FG-H8Q99 slmgr /skms kms8.msguides.com slmgr /ato slmgr -xpr https://playcraft.tistory.com/472&amp;#160;&amp;#x21a9;&amp;#xfe0e; https://github.com/valinet/ExplorerPatcher/releases&amp;#160;&amp;#x21a9;&amp;#xfe0e; https://extrememanual.net/38878&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Python's Command Line Parsing Module argparse</title><link>https://freshrimpsushi.github.io/en/posts/3561/</link><pubDate>Sat, 02 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3561/</guid><description>Description argparse is one of the Python standard libraries that helps in handling arguments passed from the command-line. While it is possible to receive input arguments using sys.argv, using argparse allows for advanced features such as specifying the type and whether an argument is mandatory. Code argparse.ArgumentParser: Creates an object. add_argument(): Method to register an argument to be accepted. parse_args(): Method to parse the received arguments. # 테스트</description></item><item><title>Introduction to GIS Developer</title><link>https://freshrimpsushi.github.io/en/posts/2531/</link><pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2531/</guid><description>Introduction This blog is run by Hyungjun Kim, a GIS expert and developer. It’s not an exaggeration to say that, at least when dealing with Korean data analysis, one couldn’t do anything without his help. Requirements There are no specific requirements, and the downloads can be unlimited. Data Example Processing and cleansing unstructured files like *.shp are typically not easy</description></item><item><title>How to Pass Arguments When Executing a Python File</title><link>https://freshrimpsushi.github.io/en/posts/3560/</link><pubDate>Thu, 29 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3560/</guid><description>Code To pass arguments to a Python file when executing, you just need to input the path to the Python file followed by the arguments separated by spaces. If you want to pass the variables 1, x, &amp;ldquo;3&amp;rdquo;, and 3 to a Python file named text.py, you would enter the following in the command line: python test.py 1 x &amp;#34;3&amp;#34; 3 To use the passed arguments inside the Python file,</description></item><item><title>Harmonic Mean</title><link>https://freshrimpsushi.github.io/en/posts/2530/</link><pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2530/</guid><description>Definition For a positive number $a,b &amp;gt; 0$, the following is called the Harmonic Mean. $$ H (a,b) := 2 \left( {{ 1 } \over { a }} + {{ 1 } \over { b }} \right)^{-1} = {{ 2 ab } \over { a + b }} $$ The generalized form with $n$ terms $x_{1} , \cdots , x_{n}$ is as follows. $$ H \left( x_{1} , \cdots ,</description></item><item><title>How to Adjust the Size and Resolution of an Image in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3559/</link><pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3559/</guid><description>Code Size plot(x, y, size=(600,400)) In Julia, the size of a plot is set using the size option. It must be input as a Tuple{Integer, Integer}, where each integer represents the width and height in pixels, respectively. The default value is (600,400). using Plots x = rand(10) plot(x) savefig(&amp;#34;size_default.png&amp;#34;) plot(x, size=(1200,800)) savefig(&amp;#34;size_(1200,800).png&amp;#34;) 1800x1200 image (left), 600x400 image (right) Resolution plot(x, y, dpi=100) The resolution of an image is set using</description></item><item><title>Conditional Mean and Variance of the Multivariate Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2529/</link><pubDate>Mon, 26 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2529/</guid><description>Formulas Bivariate Normal Distribution $$ \left( X, Y \right) \sim N_{2} \left( \begin{bmatrix} \mu_{1} \\ \mu_{n} \end{bmatrix} , \begin{bmatrix} \sigma_{X}^{2} &amp;amp; \rho \sigma_{X} \sigma_{Y} \\ \rho \sigma_{X} \sigma_{Y} &amp;amp; \sigma_{Y}^{2} \end{bmatrix} \right) $$ When a random vector $\left( X,Y \right)$ follows a bivariate normal distribution as above, $X | Y$ follows a univariate normal distribution and the conditional mean and variance are as follows. $$ \begin{align*} E \left( X |</description></item><item><title>Drawing Arrows in Graphics with Julia</title><link>https://freshrimpsushi.github.io/en/posts/3558/</link><pubDate>Sun, 25 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3558/</guid><description>Code plot!([x1, x2], [y1, y2], arrow=:true) This code plots an arrow from point $(x1, y1)$ to point $(x2, y2)$ on the plot. Naturally, the tip of the arrow is at the terminal point $(x2, y2)$. The maximum value of the sine function can be shown as follows. using Plots x = range(0, 2π, 100) plot(x, sin.(x), label=&amp;#34;&amp;#34;, ylims=(-1.3,1.3)) plot!([π/2</description></item><item><title>Eigenvector Centrality in Network Theory</title><link>https://freshrimpsushi.github.io/en/posts/2528/</link><pubDate>Sat, 24 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2528/</guid><description>Definition 1 The nth component of the eigenvector corresponding to the spectral radius $\lambda_{1}$ of the adjacency matrix $A$ of a network $\left( V , E \right)$ is called the Eigenvector Centrality of the nth node $v_{i}$. Newman. (2010). Networks: An Introduction: p170.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>What is a Box Plot?</title><link>https://freshrimpsushi.github.io/en/posts/3557/</link><pubDate>Fri, 23 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3557/</guid><description>Definition1 A box plot is a diagram that represents the median, first quartile, third quartile, maximum, and minimum of data as shown below. Explanation The third quartile, median, and first quartile are denoted as $Q3$, $Q2$, and $Q1$, respectively. The difference between $Q3$ and $Q1$ is called the IQR. The maximum and minimum values are denoted as $Q4$ and $Q0$, respectively. The rectangle in the middle is called the box,</description></item><item><title>Independence and Zero Correlation are Equivalent in Multivariate Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2527/</link><pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2527/</guid><description>Theorem 1 $$ \begin{align*} \mathbf{X} =&amp;amp; \begin{bmatrix} \mathbf{X}_{1} \\ \mathbf{X}_{2} \end{bmatrix} &amp;amp; : \Omega \to \mathbb{R}^{n} \\ \mu =&amp;amp; \begin{bmatrix} \mu_{1} \\ \mu_{2} \end{bmatrix} &amp;amp; \in \mathbb{R}^{n} \\ \Sigma =&amp;amp; \begin{bmatrix} \Sigma_{11} &amp;amp; \Sigma_{12} \\ \Sigma_{21} &amp;amp; \Sigma_{22} \end{bmatrix} &amp;amp; \in \mathbb{R}^{n \times n} \end{align*} $$ Let&amp;rsquo;s assume that a random vector $\mathbf{X} \sim N_{n} \left( \mu , \Sigma \right)$, which follows a multivariate normal distribution, is given as shown</description></item><item><title>How to Adjust Camera Position for 3D Plots in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3556/</link><pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3556/</guid><description>Explanation Unlike 2D plots such as line graphs and heatmaps, 3D plots have different appearances depending on the viewing angle. The viewpoint of a 3D plot can be set using the camera=(azimuth, altitude) option. Azimuth represents the compass direction and corresponds to the angle measured from the $xz$-plane. Altitude represents the elevation and corresponds to the angle measured from the $xy$-plane. The default value is camera=(30, 30). Drawing a spiral</description></item><item><title>Proximity Centrality in Network Theory</title><link>https://freshrimpsushi.github.io/en/posts/2526/</link><pubDate>Tue, 20 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2526/</guid><description>Definition 1 Let&amp;rsquo;s call the geodesic distance of a network $\left( V, E \right)$ as $d$. The following defined $C_{C} : V \to \mathbb{R}$ is referred to as the Closeness Centrality of node $v \in V$. $$ C_{C} := {{ 1 } \over { \sum_{u \ne v} d \left( u, v \right) }} $$ Description Intuitive Meaning Closeness Centrality serves as a measure of &amp;lsquo;how easily can one node reach</description></item><item><title>How to Fix the Random Seed in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3555/</link><pubDate>Mon, 19 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3555/</guid><description>Explanation1 In Julia, the random seed can be fixed as follows: seed!([rng=default_rng()], seed) -&amp;gt; rng seed!([rng=default_rng()]) -&amp;gt; rng The input variable rng stands for Random Number Generator, which refers to the algorithm used for drawing random numbers. The Random package offers the following options: TaskLocalRNG: This is the default setting. Xoshiro RandomDevice MersenneTwister Code By fixing the seed to 0, drawing three times, and then fixing it again to 0</description></item><item><title>Linear Transformations of Multivariate Normal Distributions</title><link>https://freshrimpsushi.github.io/en/posts/2525/</link><pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2525/</guid><description>정리 1 Linear Transformations&amp;rsquo; Normality Regarding matrix $A \in \mathbb{R}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{R}^{m}$, a random vector $\mathbf{X} \sim N_{n} \left( \mu , \Sigma \right)$ following a multivariate normal distribution undergoes a linear transformation $\mathbf{Y} = A \mathbf{X} + \mathbf{b}$ will still follow a multivariate normal distribution $N_{m} \left( A \mu + \mathbf{b} , A \Sigma A^{T} \right)$. Normality of Marginal Distributions $$ \begin{align*} \mathbf{X} =&amp;amp;</description></item><item><title>How to Draw a Box Plot in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3554/</link><pubDate>Sat, 17 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3554/</guid><description>Description plt.boxplot() can draw a Box Plot. If you draw it with the default settings, it appears as a white box with black lines only, so if you want to make it prettier, you have to manually adjust the settings. Code Basic import numpy as np import matplotlib.pyplot as plt x = 100*np.random.random_sample(100) y = 50*np.random.random_sample(100) + 50 z = np.concatenate((x,y)) plt.boxplot([x,y,z]) plt.show() Legend fig, ax = plt.subplots() bp1 =</description></item><item><title>Network Mediation Centrality in Network Theory</title><link>https://freshrimpsushi.github.io/en/posts/2524/</link><pubDate>Fri, 16 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2524/</guid><description>Definition 1 Stress Centrality In a network $\left( V, E \right)$, let&amp;rsquo;s denote by $\sigma_{st} = \sigma_{ts}$ the number of shortest paths between two nodes $s,t \in V$, and specifically, let&amp;rsquo;s denote by $\sigma_{st} (v)$ the number of paths including another node $v \in V$ among the paths connecting $s,t$. The following defined $C_{S} : V \to \mathbb{Z}$ is called the Stress Centrality of node $v$. $$ C_{S} (v) :=</description></item><item><title>How to Draw a Box Plot in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3553/</link><pubDate>Thu, 15 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3553/</guid><description>English Translation Description To draw a box plot, the statistical visualization package StatsPlots.jl must be used. boxplot([data], labels=[label]) Code using StatsPlots x = rand(0:100, 100) y = rand(50:100, 100) z = cat(x,y, dims=1) boxplot(x, label=&amp;#34;x&amp;#34;) boxplot!(y, label=&amp;#34;y&amp;#34;) boxplot!(z, label=&amp;#34;z&amp;#34;) Or boxplot([x,y,z], label=[&amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;]) will draw the same figure. Note that there should be no commas in lable. That is, it needs to be an array, not an $3 \times</description></item><item><title>Universal Kriging</title><link>https://freshrimpsushi.github.io/en/posts/2523/</link><pubDate>Wed, 14 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2523/</guid><description>Model Ordinary Kriging In Spatial Data Analysis, for a Random Field $\mathbf{Y} = \left( Y \left( s_{1} \right) , \cdots , Y \left( s_{n} \right) \right)$ that follows a Multivariate Normal Distribution $\varepsilon \sim N_{n} \left( \mathbf{0} , \Sigma \right)$ with Mean $\mu \in \mathbb{R}$ and Covariance Matrix $\Sigma \in \mathbb{R}^{n \times n}$, the following model $$ \mathbf{Y} = \mu \mathbf{1} + \varepsilon $$ is used to estimate ■c</description></item><item><title>The Hyperfunctional Derivative of Brownian Motion is White Noise</title><link>https://freshrimpsushi.github.io/en/posts/3552/</link><pubDate>Tue, 13 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3552/</guid><description>Summary The distributional derivative of Brownian motion is white noise. Description Brownian motion $B_{t}$ does not have a derivative in the traditional sense. Therefore, it can be defined as a stochastic process that satisfies the following condition $\xi$, which is defined as white noise: $$ \begin{align} E[\xi_{t}] &amp;amp;= 0, &amp;amp; \forall t \\ \Cov(\xi_{t}, \xi_{s}) &amp;amp;= \delta_{0} \end{align} $$ Here, $\Cov$ is the covariance, and $\delta$ is the Dirac delta</description></item><item><title>Stress Centrality in Network Theory</title><link>https://freshrimpsushi.github.io/en/posts/2522/</link><pubDate>Mon, 12 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2522/</guid><description>Definition 1 In a network $\left( V, E \right)$, the number of shortest paths connecting two nodes $s,t \in V$ is denoted by $\sigma_{st} = \sigma_{ts}$, and specifically, the number of paths among those that include another node $v \in V$ is denoted by $\sigma_{st} (v)$. The following defined $C_{S} : V \to \mathbb{Z}$ is referred to as the Stress Centrality of node $v$. $$ C_{S} (v) := \sum_{s \ne</description></item><item><title>Drawing Subplots at Desired Arbitrary Locations or Overlapping in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3551/</link><pubDate>Sun, 11 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3551/</guid><description>Overview Using plt.subplot or gridspec, you can create subplots in simple or complex grid-based layouts. This article introduces a method to draw subplots at completely arbitrary positions, independent of the grid system. Code fig.add_axes([left, bottom, width, height]) By using add_axes, you can create a new subplot at a desired location on the figure. The area of the subplot is determined by the input arguments [left, bottom, width, height]. Each item</description></item><item><title>Kringing in Spatial Data Analysis</title><link>https://freshrimpsushi.github.io/en/posts/2521/</link><pubDate>Sat, 10 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2521/</guid><description>Model Ordinary Kriging In Spatial Data Analysis, for a Random Field $\mathbf{Y} = \left( Y \left( s_{1} \right) , \cdots , Y \left( s_{n} \right) \right)$ following a Multivariate Normal Distribution with Mean $\mu \in \mathbb{R}$ and Covariance Matrix $\Sigma \in \mathbb{R}^{n \times n}$, the value estimated for a new site $s_{0}$ using the model $$ \mathbf{Y} = \mu \mathbf{1} + \varepsilon $$ is called the Ordinary Kriging Estimate. The</description></item><item><title>Drawing Subplots with Complex Layouts in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3550/</link><pubDate>Fri, 09 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3550/</guid><description>Overview Essentially, grids of subplots can be drawn using plt.subplot(nrows, ncols, index) or plt.subplots(nrows, ncols). This article introduces methods to draw more complex subplot arrangements. For methods on overlaying or truly drawing as one pleases, independent of a grid, see here. Code plt.subplot(nrows, ncols, index) Using plt.subplot, placing a tuple instead of an integer in the index position allows for spanning the drawing over multiple grids. The frustrating part here</description></item><item><title>Degree Centrality in Network Theory</title><link>https://freshrimpsushi.github.io/en/posts/2520/</link><pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2520/</guid><description>Definition 1 In a given network $G (V, E)$, the following is called Degree Centrality for each node $v \in V$. $$ \deg v $$ Description Centrality Centrality refers to the concept of &amp;lsquo;how important a node is within a given network&amp;rsquo;, and there are various definitions and methods of calculation depending on the problem of interest. Among many methods, the simplest is evaluating the degree itself as centrality, known</description></item><item><title>How to Draw Subplots in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3549/</link><pubDate>Wed, 07 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3549/</guid><description>Overview This document introduces several ways to draw multiple figures within one figure. The methods mentioned below are for creating simple layouts, and for more complex layouts, refer to the following. Creating Complex Layouts Drawing or Overlapping Subplots at Arbitrary Locations Code plt.subplot(nrows, ncols, index) Entering plt.subplot(r, c, n) divides the entire figure into a grid of $r$ rows and $c$ columns, allowing you to draw a picture in the</description></item><item><title>Empirical Variogram</title><link>https://freshrimpsushi.github.io/en/posts/2519/</link><pubDate>Tue, 06 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2519/</guid><description>Buildup Definition of the Variogram: Consider a spatial process $\left\{ Y(s) \right\}_{s \in D}$ which is a set of random variables $Y(s) : \Omega \to \mathbb{R}^{1}$ in a fixed subset $D \subset \mathbb{R}^{r}$ of the Euclidean space with direction vectors $\mathbf{h} \in \mathbb{R}^{r}$. Specifically, represent $n \in \mathbb{N}$ sites as follows $\left\{ s_{1} , \cdots , s_{n} \right\} \subset D$ and assume that $Y(s)$ has a variance for all $s</description></item><item><title>신호의 교차상관함수</title><link>https://freshrimpsushi.github.io/en/posts/3548/</link><pubDate>Mon, 05 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3548/</guid><description>Definition1 Analog Signal For an Energy Signal $f \in L^{2}(\mathbb{R})$, $f \star g$ defined as follows is called the cross-correlation function defined by $f$ and $g$. $$ (f \star g)(\tau) = R_{fg}(\tau) := \int_{-\infty}^{\infty} \overline{f(t)} g(t + \tau) dt $$ Here, $\overline{f(t)}$ is the conjugate complex number of $f(t)$. Digital Signal The cross-correlation function of energy signal $\left\{ x_{n} \right\} \in \ell^{2}$ is defined as follows. $$ (x\star y)[n] =</description></item><item><title>Nonsymmetric F-distribution</title><link>https://freshrimpsushi.github.io/en/posts/2518/</link><pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2518/</guid><description>Definition Singly Non-central F-distribution 1 The degrees of freedom $r_{1} , r_{2} &amp;gt; 0$ and non-centrality $\lambda_{1} \ge 0$ define the probability density function of a continuous probability distribution $F \left( r_{1} , r_{2} , \lambda_{1} \right)$, known as the Singly Non-central F-distribution. $$ f(x) = \sum_{k=0}^{\infty} {{ e^{ - \lambda / 2 } \left( \lambda / 2 \right)^{k} } \over { B \left( {{ r_{2} } \over { 2</description></item><item><title>EasyDict: A Python Package for Convenient Dictionary Use</title><link>https://freshrimpsushi.github.io/en/posts/3547/</link><pubDate>Sat, 03 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3547/</guid><description>Overview1 Introducing the easydict package that makes it convenient to use dictionaries. By using EasyDict within the package, you can access the values of a dictionary as if they were attributes, and this works recursively. Code A standard Python dictionary is defined as follows. 김채원 = { &amp;#39;국적&amp;#39;: &amp;#39;대한민</description></item><item><title>Finding the Inverse Matrix Using Gaussian Elimination Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/2517/</link><pubDate>Fri, 02 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2517/</guid><description>Algorithm Input A invertible matrix $M \in \mathbb{R}^{n \times n}$ is given. Step 1. Initialization Create an identity matrix $X$ of the same size as $M$. Step 2. Echelon Form Through Gaussian elimination, transform $M$ into echelon form. Normally, it would be sufficient for it to be in the form of an upper triangular matrix, but in the algorithm for calculating the inverse matrix, we go as far as calculating</description></item><item><title>L² Space</title><link>https://freshrimpsushi.github.io/en/posts/3546/</link><pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3546/</guid><description>Definition1 The set of sequences that are square-convergent is denoted as $\ell^{2}(\mathbb{N})$. $$ \ell^{2}(\mathbb{N}) := \left\{ \left\{ x_{k} \right\}_{k \in \mathbb{N}} : x \in \mathbb{C}(\text{or } \mathbb{R}),\quad \sum\limits_{k \in \mathbb{N}} \left| x_{k} \right|^{2} \lt \infty \right\} $$ It can also be simply denoted as follows. $$ \mathbf{x} = \left\{ x_{k} \right\}_{k \in \mathbb{N}} = (x_{1}, x_{2}, \dots, x_{n}, \dots) $$ Description $\ell^{2}$ space is a special case when $\ell^{p}$ space</description></item><item><title>Noncentral Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2516/</link><pubDate>Wed, 31 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2516/</guid><description>Definition The continuous probability distribution that has the probability density function as follows with respect to degrees of freedom $r &amp;gt; 0$ and non-centrality $\lambda \ge 0$ is called the Noncentral Chi-squared Distribution. $$ f(x) = {{ 1 } \over { 2 }} e^{- \left( x + \lambda \right) / 2 } \left( {{ x } \over { \lambda }} \right)^{k/4 - 1/2} I_{r/2 - 1} \left( \sqrt{\lambda x} \right)</description></item><item><title>신호의 자기상관함수</title><link>https://freshrimpsushi.github.io/en/posts/3545/</link><pubDate>Tue, 30 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3545/</guid><description>Definition1 Analog Signal For an energy signal $f \in L^{2}(\mathbb{R})$, $R_{f}$ defined as follows is called the auto-correlation function. $$ R_{f}(\tau) := \int_{-\infty}^{\infty} \overline{f(t)} f(t + \tau) dt $$ Here $\overline{f(t)}$ is the complex conjugate of $f(t)$. Digital Signal The auto-correlation function for the energy signal $\left\{ x_{n} \right\} \in \ell^{2}$ is defined as follows. $$ R_{x}(m) := \sum\limits_{n \in \mathbb{N}} \overline{x_{n}}x_{n+m} $$ Explanation Expressed in terms of inner product,</description></item><item><title>Square Root Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2515/</link><pubDate>Mon, 29 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2515/</guid><description>Definition 1 A matrix $A$ is called the Square Root Matrix of $B$ if it satisfies the following condition and is denoted by $\sqrt{A} := B$. $$ B^{2} = A $$ Description The concept of square roots becomes more interesting in the context of matrices. For instance, $$ A = \begin{bmatrix} 2 &amp;amp; 2 \\ 2 &amp;amp; 2 \end{bmatrix} $$ if there is a matrix like this, its square root</description></item><item><title>신호의 에너지와 평균 전력</title><link>https://freshrimpsushi.github.io/en/posts/3544/</link><pubDate>Sun, 28 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3544/</guid><description>Definition1 Analog Signal The energy of an analog signal $f \in L^{p}$ $E_{f}$ is defined as follows. $$ E_{f} := \int_{-\infty}^{\infty} \left| f(t) \right|^{2} dt = \left\| f \right\|_{2}^{2} $$ If $E_{f} \lt \infty$, then $f$ is called an energy signal. For $f$, which is not an energy signal, the mean power $P_{f}$ is defined as follows. $$ P_{f} := \lim\limits_{T \to \infty} \dfrac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} \left| f(t) \right|^{2}dt $$ If $P_{f}</description></item><item><title>Pythagorean Winning Percentage Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2514/</link><pubDate>Sat, 27 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2514/</guid><description>Formulas Let&amp;rsquo;s assume we have a team from a certain sports league. The Team Scores $S$ and Team Allows $A$ are random variables that each follow a Weibull distribution, $$ \begin{align*} S &amp;amp; \sim \text{Weibull} \left( \alpha_{S} , \beta , \gamma \right) \\ A &amp;amp; \sim \text{Weibull} \left( \alpha_{A} , \beta , \gamma \right) \end{align*} $$ and are also independent of each other independently. The team&amp;rsquo;s expected winning percentage $p$</description></item><item><title>아날로그신호와 디지털신호의 정의</title><link>https://freshrimpsushi.github.io/en/posts/3543/</link><pubDate>Fri, 26 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3543/</guid><description>Definition1 The elements of the $L^{p}$ space are referred to as analog signals or continuous-time signals. $$ f \in L^{p}(\mathbb{R}) $$ The elements of the $\ell^{p}$ space are referred to as digital signals or discrete-time signals. $$ x_{n} = \left\{ x_{n} \right\} \in \ell^{p}(\mathbb{N}) $$ Explanation By definition, analog signals and digital signals possess almost identical characteristics, with the primary difference being whether they are functions or sequences. It is</description></item><item><title>Sparse Matrices</title><link>https://freshrimpsushi.github.io/en/posts/2513/</link><pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2513/</guid><description>Definition In natural language, sparse refers to being thin or scarce in a way that a value is considered virtually non-existent if it is $0$. Sparsity indicates the degree to which something is made up of such $0$ values. Sparse Matrix A matrix whose elements are mostly $0$ is referred to as a Sparse Matrix. $S$-Sparse 1 Even if there are many values, when there are only $S \ll d$</description></item><item><title>Absolutely Continuous Real Function</title><link>https://freshrimpsushi.github.io/en/posts/3542/</link><pubDate>Wed, 24 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3542/</guid><description>Definition1 Let&amp;rsquo;s say a function $f : \mathbb{R} \to \mathbb{R}( \text{or } \mathbb{C})$ is given. If for any finite number of mutually disjoint intervals $(a_{i}, b_{i}) \sub [a,b]$, the following condition is satisfied, then it is said to be absolutely continuous on $[a, b]$. $$ \forall \epsilon \gt 0 \quad \exist \delta \gt 0 \text{ such that } \sum\limits_{i=1}^{N} (b_{i} - a_{i}) \lt \delta \implies \sum\limits_{i=1}^{N} \left| f(b_{j}) - f(a_{j})</description></item><item><title>Kent Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2512/</link><pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2512/</guid><description>Definition 1 Concentration $\kappa &amp;gt; 0$ and $\beta \in \mathbb{R}$, Mean $\gamma_{1} \in S^{p-1}$, Major Axis $\gamma_{2} \in S^{p-1}$, Minor Axis $\gamma_{3} \in S^{p-1}$ are characterized by the following probability density function for the multivariate distribution $\text{FB}_{5} \left( \left( \gamma_{1} , \gamma_{2} , \gamma_{3} \right) , \kappa , \beta \right)$, known as the Kent Distribution. $$ f \left( \mathbf{x} \right) = {{ 1 } \over { c \left( \kappa ,</description></item><item><title>Lipschitz Continuity</title><link>https://freshrimpsushi.github.io/en/posts/3541/</link><pubDate>Mon, 22 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3541/</guid><description>Definition1 For two metric spaces $(X, d_{X})$ and $(Y, d_{Y})$, let&amp;rsquo;s assume that a function $f : X \to Y$ is given. If there exists a constant $K$ such that the following holds for all $x_{1}, x_{2} \in X$, then $f$ is called $K$-Lipschitz continuous. $$ d_{Y} \big( f(x_{1}), f(x_{2}) \big) \le K d_{X} \big( x_{1}, x_{2} \big) $$ Such a constant $K$ is called the Lipschitz constant. Explanation It</description></item><item><title>How to Use Infinite Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2511/</link><pubDate>Sun, 21 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2511/</guid><description>Overview InfiniteArrays.jl is a package that enables the use of arrays of infinite size1, and is, in fact, closely related to Lazy Arrays. Lazy Evaluation refers to the method where the computer knows what needs to be computed but postpones the calculation until it is absolutely necessary. Obviously, computers cannot understand infinity, but this method allows for the implementation of infinite arrays on computers. Code ∞ julia&amp;gt; using InfiniteArrays julia&amp;gt;</description></item><item><title>Homogeneous Function</title><link>https://freshrimpsushi.github.io/en/posts/3540/</link><pubDate>Sat, 20 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3540/</guid><description>Definition For a constant $a$ and a function $f$, if there exists a $k \in \mathbb{N}$ that satisfies the following conditions, then $f$ is called a $k$-th degree homogeneous function. $$ f(ax) = a^{k}f(x) $$ In the case of a multivariable function, $$ f(ax_{1}, ax_{2}, \dots, ax_{n}) = a^{k}f(x_{1}, x_{2}, \dots, x_{n}) $$ Explanation In the case of a univariate function, it is similar to a polynomial function that only</description></item><item><title>Bingham-Mardia Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2510/</link><pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2510/</guid><description>Definition 1 The Bingham-Mardia Distribution is a multivariate distribution $\text{BM}_{p} \left( \mu , \kappa, \nu \right)$ that has a probability density function for Unique Mode $\mu \in S^{p-1}$, Concentration $\kappa &amp;gt; 0$, and radius $\nu &amp;gt; 0$ as follows. $$ f \left( \mathbf{x} \right) = {{ 1 } \over { \alpha \left( \kappa , \nu \right) }} \exp \left( - \kappa \left( \mu^{T} \mathbf{x} - \nu \right)^{2} \right) \qquad ,</description></item><item><title>Reading and Writing mat Files in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2509/</link><pubDate>Wed, 17 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2509/</guid><description>Overview MAT.jl is a library for reading and writing *.mat files, which are the data storage format used in MATLAB1. As is typical of Julia, this package does not force users to abandon their existing programming languages and habits; instead, it aims to secure users by providing an environment that is as familiar as possible. While the speed and convenience of Julia are significant advantages, MATLAB offers unique benefits for</description></item><item><title>Origin of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/3538/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3538/</guid><description>Explanation In mathematics, there are functions called trigonometric functions. These include $\sin, \cos, \tan, \sinh, \sec, \dots$ and others. They are collectively referred to as trigonometric functions because they are fundamentally related to triangles. Each of their names also originates from geometric meanings related to triangles. $\sin$ Let&amp;rsquo;s assume a right triangle like the one shown in the figure above. If the angle between the base and the hypotenuse is</description></item><item><title>Why Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2508/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2508/</guid><description>Definition 1 The probability distribution which has the following probability density function in terms of location $\xi \in \mathbb{R}$, scale $\omega &amp;gt; 0$, and shape $\alpha \in \mathbb{R}$ parameters is called the Skew Normal Distribution. $$ \begin{align*} f(x) =&amp;amp; {{ 2 } \over { \omega }} \phi \left( {{ x - \xi } \over { \omega }} \right) \Phi \left( \alpha {{ x - \xi } \over { \omega }}</description></item><item><title>Origin of the arc notation for inverse trigonometric functions</title><link>https://freshrimpsushi.github.io/en/posts/3537/</link><pubDate>Sun, 14 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3537/</guid><description>Definition The inverse function of a trigonometric function is called the inverse trigonometric function. Explanation Inverse trigonometric functions are often denoted using standard inverse notation $\sin^{-1}$, $\cos^{-1}$, but they are also frequently denoted with the prefix arc-, as in $\arcsin$, $\arccos$. $$ \begin{align*} \arcsin x &amp;amp;= \sin^{-1} x \qquad &amp;amp; \operatorname{arccsc} x &amp;amp;= \csc^{-1} x \\ \arccos x &amp;amp;= \cos^{-1} x \qquad &amp;amp; \operatorname{arcsec} x &amp;amp;= \sec^{-1} x \\ \arctan</description></item><item><title>How to Output Simple Graphics in the Julia Console</title><link>https://freshrimpsushi.github.io/en/posts/2507/</link><pubDate>Sat, 13 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2507/</guid><description>Overview UnicodePlots.jl is a library that uses Unicode characters to print graphics in the Julia REPL1, enabling lightweight yet high-quality visualization as the program runs. Code using UnicodePlots p1 = lineplot(100 |&amp;gt; randn |&amp;gt; cumsum) p1 = lineplot!(p1, 100 |&amp;gt; randn |&amp;gt; cumsum); p1 UnicodePlots.heatmap(cumsum(abs.(randn(100,100)), dims=2)) The result of running the above example code is as follows. Environment OS: Windows julia: v1.7.3 UnicodePlots v3.0.4 https://github.com/JuliaPlots/UnicodePlots.jl&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Notation for Vectors Entering and Exiting the Plane</title><link>https://freshrimpsushi.github.io/en/posts/3536/</link><pubDate>Fri, 12 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3536/</guid><description>Definition As depicted above, from the observer&amp;rsquo;s perspective, the direction that goes into the plane is denoted as $\otimes$. Conversely, the direction that comes out of the plane is denoted as $\odot$. Explanation Looking at the $xy$-plane as shown in the figure above, the $\hat{z}$ direction is $=\odot$ and the $-\hat{z}$ direction is $=\otimes$. This is primarily used in physics, engineering, etc. The notation is not based on circles and</description></item><item><title>폰 푀르스터 방정식</title><link>https://freshrimpsushi.github.io/en/posts/2506/</link><pubDate>Thu, 11 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2506/</guid><description>Model 1 $$ {{ \partial n } \over { \partial t }} + {{ \partial n } \over { \partial a }} = - \mu \left( a \right) n \qquad t, a \in (0, \infty) $$ The above partial differential equation is referred to as the von Foerster equation and possesses the following two Dirichlet boundary conditions: Initial conditions of age structure $$ n \left( 0, a \right) = f(a)</description></item><item><title>Curl of a Vector Function in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/3535/</link><pubDate>Wed, 10 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3535/</guid><description>Theorem In the curvilinear coordinate system, the curl of the vector function $\mathbf{F}=\mathbf{F}(q_{1},q_{2},q_{3})=F_{1}\hat{\mathbf{q}}_{1}+F_{2}\hat{\mathbf{q}}_{2}+F_{3}\hat{\mathbf{q}}_{3}$ is as follows. $$ \begin{align*} \nabla \times \mathbf{F} &amp;amp;= \frac{\hat{\mathbf{q}}_{1}}{h_{2}h_{3}}\left( \dfrac{\partial (F_{3}h_{3})}{\partial q_{2}} - \dfrac{\partial (F_{2}h_{2})}{\partial q_{3}} \right) + \frac{\hat{\mathbf{q}}_{2}}{h_{1}h_{3}}\left( \dfrac{\partial (F_{1}h_{1})}{\partial q_{3}} - \dfrac{\partial (F_{3}h_{3})}{\partial q_{1}} \right) \\ &amp;amp;\quad+ \frac{\hat{\mathbf{q}}_{3}}{h_{1}h_{2}}\left( \dfrac{\partial (F_{2}h_{2})}{\partial q_{1}} - \dfrac{\partial (F_{1}h_{1})}{\partial q_{2}} \right) \\ &amp;amp;= \frac{1}{h_{1}h_{2}h_{3}} \begin{vmatrix} h_{1}\hat{\mathbf{q}}_{1} &amp;amp; h_{2}\hat{\mathbf{q}}_{2} &amp;amp; h_{3}\hat{\mathbf{q}}_{3} \\[0.5em] \dfrac{\partial }{\partial q_{1}} &amp;amp; \dfrac{\partial }{\partial q_{2}} &amp;amp;</description></item><item><title>How to Initialize the Console in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2505/</link><pubDate>Tue, 09 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2505/</guid><description>Method In the console, pressing Ctrl + L appears to clear the console completely, but in some environments, it does not actually reset but rather scrolls the window as if it were pushed up. To cleanly remove or not rely on keyboard input, printing ASCII character \033c can be used1 2. print(&amp;#34;\033c&amp;#34;) Also, printing \007 will play a notification sound3. It&amp;rsquo;s surprisingly useful when you want to hear the end</description></item><item><title>First-Order Necessary Conditions for Extrema of Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/3534/</link><pubDate>Mon, 08 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3534/</guid><description>Theorem1 Let&amp;rsquo;s assume the function $f : \mathbb{R}^{n} \to \mathbb{R}$ is given. If $x^{\ast}$ is a local optimizer and $f \in C^{1}$ in the vicinity of $x^{\ast}$, then, $$ \nabla f(x^{\ast}) = 0 $$ $\nabla f$ is the gradient of $f$. Note here that $0$ is not the numeric zero, but a zero vector. Explanation The first-order necessary condition tells us about the property of the gradient, which is the</description></item><item><title>Von Mises-Fisher Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2504/</link><pubDate>Sun, 07 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2504/</guid><description>Definition 1 The Von Mises-Fisher Distribution is defined as the multivariate distribution $\text{vMF}_{p} \left( \mu , \kappa \right)$ with the following probability density function for Unique Mode $\mu \in S^{p-1}$ and Concentration $\kappa &amp;gt; 0$. $$ f \left( \mathbf{x} \right) = \left( {{ \kappa } \over { 2 }} \right)^{p/2-1} {{ 1 } \over { \Gamma \left( p/2 \right) I_{p/2-1} \left( \kappa \right) }} \exp \left( \kappa \mu^{T} \mathbf{x} \right)</description></item><item><title>Second Order Necessary/Sufficient Conditions for the Extreme Values of Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/3533/</link><pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3533/</guid><description>Theorem1 Let&amp;rsquo;s say the function $f : \mathbb{R}^{n} \to \mathbb{R}$ is given. $\nabla f$, $\nabla^{2}f$ are the gradient and Hessian of $f$, respectively. Second-order necessary conditions If $x^{\ast}$ is a local optimizer and $\nabla^{2}f$ exists and is continuous in the neighborhood of $x^{\ast}$, $$ \nabla f(x^{\ast}) = 0 $$ and $\nabla^{2} f(x^{\ast})$ is positive semidefinite. Note that $0$ is not the number zero, but the zero vector. Second-order sufficient conditions</description></item><item><title>Removing Missing Values in DataFrames in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2503/</link><pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2503/</guid><description>Overview 1 In Julia, you can easily remove missing values using the dropmissing() function. Code julia&amp;gt; df = DataFrame(x = [&amp;#34;i&amp;#34;, missing, &amp;#34;k&amp;#34;, &amp;#34;j&amp;#34;], y = [1, 2, 3, missing]) 4×2 DataFrame Row │ x y │ String? Int64? ─────┼────────────────── 1 │ i 1 2</description></item><item><title>Taylor's Theorem Rest Term</title><link>https://freshrimpsushi.github.io/en/posts/3532/</link><pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3532/</guid><description>Definition1 2 For a differentiable function $f$, the $P_{k}$ defined below is called the Taylor polynomial of $f$ at point $a$. $$ P_{k} (x) := f(a) + f^{\prime}(a) (x-a) + \dfrac{f^{\prime \prime}(a)}{2!}(x-a)^{2} + \cdots + \dfrac{f^{(k)}(a)}{k!}(x-a)^{k} $$ The difference between $f$ and $P_{k}$ is called the remainder term. $$ R_{k}(x) = f(x) - P_{k}(x) $$ Explanation $$ f(x) = P_{k}(x) + R_{k}(x) = \sum \limits_{n=0}^{k} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} + R_{k}(x) $$</description></item><item><title>Models of Semivariograms</title><link>https://freshrimpsushi.github.io/en/posts/2502/</link><pubDate>Wed, 03 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2502/</guid><description>Overview In Spatial Statistics Analysis, if a Spatial Process is Isotropic and the Semivariogram satisfies $\gamma \left( \left\| \mathbf{h} \right\| \right) = \gamma (d)$, then $\gamma$ can be expressed not as a complex matrix form but as a one-dimensional scalar function, that is, $\gamma : \mathbb{R} \to \mathbb{R}$. This means that the correlation between point reference data $Y(s), Y(s + d)$ can be plotted as a line graph. Models 1</description></item><item><title>2nd Imaginary Matrix Shrimp Sushi Restaurant Contest</title><link>https://freshrimpsushi.github.io/en/posts/2501/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2501/</guid><description>Imaginary Matrix $$ x^{2} = -1 \longrightarrow X^{2} = - E_{p} $$ The second contest addressed the Imaginary Matrix, a generalization of complex numbers to matrices as $\mathbb{R}^{p \times p}$. Results Participants included uriel, jryoungw, and Trakatus, with the scorecard available here. Research From the organizer&amp;rsquo;s perspective, a surprising aspect was the rapid rate of abstraction, sooner than expected. It was assumed that a winning strategy would involve initially exploring</description></item><item><title>Secant Method: Newton's Method</title><link>https://freshrimpsushi.github.io/en/posts/3530/</link><pubDate>Sun, 31 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3530/</guid><description>Definition1 2 In the problem of optimizing the objective function $J : \mathbb{R}^{n} \to \mathbb{R}$, the following iterative algorithm is called Newton&amp;rsquo;s method. $$ \begin{equation} \mathbf{x}_{n+1} = \mathbf{x}_{n} - H^{-1}(\mathbf{x}_{n}) \nabla J(\mathbf{x}_{n}) \end{equation} $$ $\nabla J$ is the gradient, and $H$ is the Hessian of $J$. Derivation By approximating $J$ with up to the second derivative term using the Taylor expansion, it looks like this: $$ J(\mathbf{x}) \approx J(\mathbf{x}_{0}) +</description></item><item><title>Isotropy of Variogram</title><link>https://freshrimpsushi.github.io/en/posts/2500/</link><pubDate>Sat, 30 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2500/</guid><description>Definition 1 The semivariogram $\gamma \left( \mathbf{h} \right)$ of a spatial process can be expressed as depending solely on the magnitude $d := \left\| \mathbf{h} \right\|$ rather than on the direction vector $\mathbf{h} \in \mathbb{R}^{r}$, in which case the variogram $2 \gamma$ is said to be isotropic. If it is not isotropic, it is referred to as anisotropic. $$ \gamma \left( \left\| \mathbf{h} \right\| \right) = \gamma (d) $$ Especially</description></item><item><title>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</title><link>https://freshrimpsushi.github.io/en/posts/3529/</link><pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3529/</guid><description>Overview1 2 Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam. Explanation In gradient descent, the learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\alpha$, $\eta$, and determines how much of the gradient is taken into account when updating the parameter. Optimization</description></item><item><title>How to Reference Environment Variables in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2499/</link><pubDate>Thu, 28 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2499/</guid><description>Overview This document explains how to reference environment variables in Julia1. Code Base.ENV Base.ENV[&amp;#34;JULIA_NUM_THREADS&amp;#34;] As you can see, accessing environment variables does not require loading any separate package; you can directly access them through Base.ENV. Since they are read as a dictionary, using the name of the desired environment variable as a key will return the environment variable as a string. The results of executing the above two lines of</description></item><item><title>Momentum Method in Gradient Descent</title><link>https://freshrimpsushi.github.io/en/posts/3528/</link><pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3528/</guid><description>Overview1 2 In gradient descent, the momentum technique involves using all previous gradients when updating parameters. This is its essence and the end of the explanation. However, explanations involving strange update equations, the motivation from physics&amp;rsquo; momentum, setting the mass to $1$ and the initial velocity to $0$, only complicate understanding. This article explains the momentum technique as plainly as possible. Build-Up Let&amp;rsquo;s denote the parameters as $\boldsymbol{\theta}$ and the</description></item><item><title>Definition of Variogram</title><link>https://freshrimpsushi.github.io/en/posts/2498/</link><pubDate>Tue, 26 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2498/</guid><description>Definition 1 In a fixed subset $D \subset \mathbb{R}^{r}$ of Euclidean space , consider a space process $\left\{ Y(s) \right\}_{s \in D}$ which is a set of random variables $Y(s) : \Omega \to \mathbb{R}^{1}$ and a direction vector $\mathbf{h} \in \mathbb{R}^{r}$. Specifically, represent $n \in \mathbb{N}$ sites as $\left\{ s_{1} , \cdots , s_{n} \right\} \subset D$, and assume that $Y(s)$ has variance existing for all $s \in D$. The</description></item><item><title>Modular Arithmetic in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3527/</link><pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3527/</guid><description>Explanation Modular arithmetic, also known as the remainder operation, is a function that returns the remainder when dividing $a$ by $b$. In PyTorch, there are two functions available: torch.remainder(a,b) torch.fmod(a,b) Both provide the remainder when $a$ is divided by $b$, but the outcomes are slightly different. If you&amp;rsquo;re curious about the specific formulas, refer to the official documents for remainder and fmod. Simply put, in remainder, the sign of the</description></item><item><title>How to use progress bars in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2497/</link><pubDate>Sun, 24 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2497/</guid><description>Overview In Julia, you can easily use a progress bar to indicate the progress of a program. Code ProgressMeter.jl By placing the @showprogress macro from the ProgressMeter.jl package in a for loop, you can display the progress1. using ProgressMeter chi2 = [] @showprogress for n in 1:20000 push!(chi2, sum(randn(n) .^ 2)) end Compared to ProgressBars.jl below, the use of a macro makes the code more concise. ProgressBars.jl You can wrap</description></item><item><title>Online Learning vs. Batch Learning in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3526/</link><pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3526/</guid><description>Overview This document explains online learning, batch learning, and mini-batch learning. It emphasizes that the names and differences among them are not actually crucial, but rather inconsequential. In fact, it can be assumed that only mini-batch learning is used in recent deep learning. Definition Let&amp;rsquo;s suppose we have a true value $\mathbf{y} = \left\{ y_{i} \right\}_{i=1}^{N}$ and an estimated value $\hat{\mathbf{y}} = \left\{ \hat{y}_{i} \right\}_{i=1}^{N}$. Assuming the loss function for</description></item><item><title>Stationarity of Spatial Processes</title><link>https://freshrimpsushi.github.io/en/posts/2496/</link><pubDate>Fri, 22 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2496/</guid><description>Definitions 1 Consider a spatial process $\left\{ Y(s) \right\}_{s \in D}$ and direction vector $\mathbf{h} \in \mathbb{R}^{r}$, which is a set of random variables $Y(s) : \Omega \to \mathbb{R}^{1}$ in a fixed subset $D \subset \mathbb{R}^{r}$ of Euclidean space. Specifically, represent $n \in \mathbb{N}$ number of sites as $\left\{ s_{1} , \cdots , s_{n} \right\} \subset D$, and assume that $Y(s)$ has a variance for all $s \in D$. $\left\{</description></item><item><title>How to Prevent the Date from Appearing when Creating a Title with maketitle in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3525/</link><pubDate>Thu, 21 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3525/</guid><description>Explanation When creating a title with \maketitle in $\LaTeX$, how can one prevent the date from being displayed? Many would intuitively try to comment out \date{2023.12.21}. However, commenting out will not stop the date from being printed, and in fact, you just have to not enter a date at all (the frustrating part is that you can stop the display by commenting out \author{}). Code \title{Using maketitle in LaTeX...} \author{Jeon</description></item><item><title>Calculating the Mean Excluding 0 or Missing Values in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2495/</link><pubDate>Wed, 20 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2495/</guid><description>Overview R language has options within functions like sum() or mean() to ignore missing values directly, whereas Julia lacks such options but actively employs Functional Programming approaches instead. Code julia&amp;gt; data = [0,1,2,3,0] 5-element Vector{Int64}: 0 1 2 3 0 julia&amp;gt; sum(data) / length(data) 1.2 julia&amp;gt; sum(data) / sum(!iszero, data) 2.0 The top portion results in 1.2, dividing by the total number of samples including up to $0$, while the</description></item><item><title>How to Insert Code Blocks in LaTeX Documents</title><link>https://freshrimpsushi.github.io/en/posts/3524/</link><pubDate>Tue, 19 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3524/</guid><description>Description $\TeX$ There are two methods you can use when you want to add a code block to a document. $\text{\textbackslash begin}\{\text{verbatim}\} &amp;hellip; \text{\textbackslash end}\{\text{verbatim}\}$ $\text{\textbackslash begin}\{\text{lstlisting}\} &amp;hellip; \text{\textbackslash end}\{\text{lstlisting}\}$ Among these, verbatim is a basic feature that doesn&amp;rsquo;t require the use of a package, whereas lstlisting requires \usepackage{listings}. Code \documentclass{article} \usepackage[utf8]{inputenc} \usepackage{listings} \title{How to Insert a Code Block in \LaTeX?} \date{} \begin{document} \maketitle verbatim style: \begin{verbatim} def add(x, y)</description></item><item><title>Spatial Processes</title><link>https://freshrimpsushi.github.io/en/posts/2494/</link><pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2494/</guid><description>Definition 1 Especially when it is $r &amp;gt; 1$, for a fixed subset $D \in \mathbb{R}^{r}$ of the Euclidean space, the following set of $p$-variate random vectors $Y(s) : \Omega \to \mathbb{R}^{p}$ is also referred to as a Spatial Process. $$ \left\{ Y(s) : s \in D \right\} $$ Especially when the spatial process is a finite set and represented as a vector like the following, it is also referred</description></item><item><title>자기장의 기호로 B를 사용하는 이유</title><link>https://freshrimpsushi.github.io/en/posts/3523/</link><pubDate>Sun, 17 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3523/</guid><description>Question Electromagnetism is literally the study of electric fields $\mathbf{E}$ and magnetic fields $\mathbf{B}$. While studying electromagnetism, one might have wondered the following at least once. Why is the symbol for magnetic fields $\mathbf{B}$ used? It's understandable that the electric field is $\mathbf{E}$, derived from the Electric field, but why is the magnetic field $\mathbf{B}$ when it should be from Magnetic field? This notation might feel oddly placed, and it's</description></item><item><title>How to Perform Regression Analysis in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2493/</link><pubDate>Sat, 16 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2493/</guid><description>Overview This brief introduction presents the GLM.jl package for conducting regression analysis in Julia, emphasizing its similarity to the interface in R and thus, skipping detailed explanations1. Code Julia using GLM, RDatasets faithful = dataset(&amp;#34;datasets&amp;#34;, &amp;#34;faithful&amp;#34;) out1 = lm(@formula(Waiting ~ Eruptions), faithful) The result of running the above code is as follows: julia&amp;gt; out1 = lm(@formula(Waiting ~ Eruptions), faithful) StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}} Waiting ~ 1 + Eruptions Coefficients:</description></item><item><title>What is Data Augmentation?</title><link>https://freshrimpsushi.github.io/en/posts/3522/</link><pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3522/</guid><description>Definition Suppose a set of data $X = \left\{ x \in \mathbb{R}^{n} \right\}$ is given. Using appropriate transformations $f_{i} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ to obtain $X^{\prime}$ from $X$ is called data augmentation. $$ X^{\prime} = X \cup \left( \bigcup_{i} \left\{ f_{i}(x) : x \in X \right\} \right) $$ Description Simply put, for example with images, this means adding new images to the dataset, which are obtained by applying modifications like</description></item><item><title>What is Spatial Data Analysis?</title><link>https://freshrimpsushi.github.io/en/posts/2492/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2492/</guid><description>Explanation 1 Spatial Data refers to data that includes information about space, and Spatial Statistics is a branch of statistics that analyzes Euclidean space $\mathbb{R}^{r}$ as &amp;lsquo;space&amp;rsquo; in the true dictionary sense. While time series analysis analyses data that changes over the time axis $t$, spatial data analysis analyses data that changes depending on the given $D \subset \mathbb{R}^{r}$, (usually when $r = 2$) location. Even at first thought, the</description></item><item><title>Monte Carlo Method</title><link>https://freshrimpsushi.github.io/en/posts/3521/</link><pubDate>Wed, 13 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3521/</guid><description>Definition Simply put, the Monte Carlo method is about doing something many times at random. Description Although it&amp;rsquo;s a simple method, there are few methods as easy and certain as trying many times. Of course, &amp;ldquo;easy&amp;rdquo; refers to when utilizing a computer. Monte Carlo refers to the name of an area famous for its casinos and hotels in northern Monaco. The name &amp;ldquo;Monte Carlo method&amp;rdquo; also comes from the Monte</description></item><item><title>Fitness Test of a group</title><link>https://freshrimpsushi.github.io/en/posts/2491/</link><pubDate>Tue, 12 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2491/</guid><description>Hypothesis Testing 1 Given categorical data obtained from $n$ independent trials in a multinomial experiment where $k$ categories are each theoretically drawn with a probability of $p_{j} &amp;gt; 0$, the following hypothesis test using the Pearson&amp;rsquo;s chi-square test statistic is known as a goodness of fit test. $H_{0}$: The given data has been sampled to conform to the theoretical probabilities. $H_{1}$: The given data has not been sampled to conform</description></item><item><title>Bivariate von Mises Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2490/</link><pubDate>Sun, 10 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2490/</guid><description>Definition 1 The mean direction $\mu, \nu \in \mathbb{R}$ and concentration $\kappa_{1}, \kappa_{2} &amp;gt; 0$ with respect to a certain matrix $A \in \mathbb{R}^{2 \times 2}$ have a continuous probability distribution $\text{vM}^{2} \left( \mu , \nu , \kappa_{1} , \kappa_{2} \right)$ with a probability density function that is proportional to the following, which is called the Bivariate von Mises Distribution: $$ \exp \left[ \kappa_{1} \cos \left( \theta - \mu \right)</description></item><item><title>Square Root Expansion Formula</title><link>https://freshrimpsushi.github.io/en/posts/3519/</link><pubDate>Sat, 09 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3519/</guid><description>Formula When $a \gt b$, $$ \sqrt{a + b \pm 2\sqrt{a b}} = \sqrt{a} \pm \sqrt{b} $$ Explanation It might seem incredibly difficult to solve with two roots, but it can be directly solved in a perfect square form. Example $$ \begin{align*} \sqrt{13 - 2\sqrt{12}} &amp;amp;= \sqrt{12 + 1 - 2\sqrt{12 \cdot 1}} \\ &amp;amp;= \sqrt{(\sqrt{12})^{2} + (\sqrt{1})^{2} - 2\sqrt{12}\sqrt{1}} \\ &amp;amp;= \sqrt{(\sqrt{12} - \sqrt{1})^{2}} \\ &amp;amp;= \sqrt{12} - \sqrt{1}</description></item><item><title>Polynomial Experiments and Contingency Tables</title><link>https://freshrimpsushi.github.io/en/posts/2489/</link><pubDate>Fri, 08 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2489/</guid><description>Definition 1 Multinomial Experiment An experiment that has the following characteristics and has three or more possible outcomes or categories is called a Multinomial Experiment. It consists of $n$ identical trials. Each trial&amp;rsquo;s outcome is one of $k&amp;gt;2$ possible outcomes or categories. Each trial is independent. The probabilities of various outcomes remain constant throughout the trials. Contingency Table When there is information about more than one variable for an element,</description></item><item><title>Rejection Sampling</title><link>https://freshrimpsushi.github.io/en/posts/3518/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3518/</guid><description>Overview 1 Rejection sampling is one of the Monte Carlo methods, where a proposal distribution $q$, easy to sample from, is used to obtain samples following a given distribution $p$, especially when it&amp;rsquo;s difficult to sample from $p$ directly. Build-up Let&amp;rsquo;s assume we are given a random variable $X$ with a probability density function $p$. We want to sample from the distribution of $p$, but it&amp;rsquo;s challenging. (For instance, if</description></item><item><title>Von Mises Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2488/</link><pubDate>Wed, 06 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2488/</guid><description>Definition 1 2 The von Mises Distribution is a continuous probability distribution with the probability density function given for Mean Direction $\mu \in \mathbb{R}$ and Concentration $\kappa &amp;gt; 0$ as follows: $$ f(x) = {{ 1 } \over {2 \pi I_{0} \left( \kappa \right) }} \exp \left( \kappa \cos \left( x - \mu \right) \right) \qquad , x \in \mathbb{R} \pmod{2 \pi} $$ $I_{\nu}$ is a modified Bessel function of</description></item><item><title>Bounded Function</title><link>https://freshrimpsushi.github.io/en/posts/3517/</link><pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3517/</guid><description>Definition1 For a function $f : X \to \mathbb{R}^{n}$, if there exists a constant $M \in \mathbb{R}$ that satisfies the following, $f$ is said to be bounded. $$ \left| f(x) \right| \le M \quad \text{for all $x \in X, $} $$ 설명 다시말해 $f(x)$의 치역이 유계 집합이면 $f$ is called a bounded function. Walter</description></item><item><title>Pearson Chi-Square test statistic</title><link>https://freshrimpsushi.github.io/en/posts/2487/</link><pubDate>Mon, 04 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2487/</guid><description>Definition 1 Consider a multinomial experiment where $k$ categories are drawn each with a probability of $p_{j} &amp;gt; 0$, and we obtain categorical data from $n$ independent trials. The frequency of data belonging to the $j$-th category $O_{j}$ is termed the observed cell count, while the expected value under the null hypothesis of hypothesis testing $E_{j}$ is called the expected cell count. The test statistic $$ \mathcal{X}^{2} := \sum_{j=1}^{k} {{</description></item><item><title>중요도 샘플링</title><link>https://freshrimpsushi.github.io/en/posts/3516/</link><pubDate>Sun, 03 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3516/</guid><description>Overview1 Importance sampling is one of the Monte Carlo methods, and it is a sampling trick that can be used when approximating integrations (expectations) with finite sums. Build-up The probability that a value from a standard normal distribution exceeds $z$ is about $4$. $$ \begin{equation} \int_{4}^{\infty} \dfrac{1}{\sqrt{2\pi}} e^{-z^{2}/2}dz \approx 3.167 \times 10^{-5} \end{equation} $$ The Julia code to calculate this integral using Monte Carlo integration is as follows. using Distributions</description></item><item><title>Proof of Pearson's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2486/</link><pubDate>Sat, 02 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2486/</guid><description>Theorem Let&amp;rsquo;s say that the random vector $\left( N_{1} , \cdots , N_{k} \right)$ follows the multinomial distribution $M_{k} \left( n ; \mathbf{p} \right)$ for $\mathbf{p} = \left( p_{1} , \cdots , p_{k} \right) \in [0,1]^{k}$ that satisfies $$ \sum_{i=1}^{k} N_{i} = n \qquad \&amp;amp; \qquad \sum_{i=1}^{k} p_{i} = 1 $$, and the sample sizes $n \in \mathbb{N}$ and $k \in \mathbb{N}$ categories. Then, when $n \to \infty$, the statistic</description></item><item><title>Monte Carlo Integration</title><link>https://freshrimpsushi.github.io/en/posts/3515/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3515/</guid><description>Overview Monte Carlo integration is a numerical approximation method used when calculating the integral of a given function is difficult. Consider the following scenario: For an integrable function $f$ in the given $[0, 1]$, we know the formula of $f(x)$ but calculating its integral is not straightforward. However, we want to calculate the integral $I[f]$ of $f$. $$ \begin{equation} I[f] = \int_{[0,1]} f(x) dx \end{equation} $$ Definition Monte Carlo integration</description></item><item><title>Determinant of a Triangular Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2485/</link><pubDate>Thu, 30 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2485/</guid><description>English Translation Theorem The determinant of a triangular matrix is expressed as the product of its diagonal elements. Proof 1 Without loss of generality, assume that $A$ is an upper triangular matrix. $$ A := \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} &amp;amp; \cdots &amp;amp; a_{1n} \\ 0 &amp;amp; a_{22} &amp;amp; a_{23} &amp;amp; \cdots &amp;amp; a_{2n} \\ 0 &amp;amp; 0 &amp;amp; a_{33} &amp;amp; \cdots &amp;amp; a_{3n} \\ \vdots &amp;amp; \vdots &amp;amp;</description></item><item><title>Conditional Expectation Minimizes the Sum of Squared Deviations</title><link>https://freshrimpsushi.github.io/en/posts/3514/</link><pubDate>Wed, 29 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3514/</guid><description>Summary The following holds true: $$ \begin{equation} E\left[ Y | X \right] = \argmin_{f(X)} E\left[ (Y - f(X))^{2} | X \right] \end{equation} $$ $$ \begin{equation} E\left[ Y | X \right] = \argmin_{f(X)} E\left[ (Y - f(X))^{2} \right] \end{equation} $$ Proof (1) $$ \begin{align*} &amp;amp; \argmin_{f(X)} E\left[ (Y - f(X))^{2} | X \right] \\ &amp;amp;= \argmin_{f(X)} E\left[ Y^{2} - 2Yf(X) + f(X)^{2} | X \right] \\ &amp;amp;= \argmin_{f(X)} \left( E\left[ Y^{2}</description></item><item><title>Derivation of the Covariance Matrix of the Multinomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2484/</link><pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2484/</guid><description>Formula If a random vector $\mathbf{X} := \left( X_{1} , \cdots , X_{k} \right)$ follows a multinomial distribution $M_{k} \left( n, \mathbf{p} \right)$, then its covariance matrix is as follows. $$ \operatorname{Cov} \left( \mathbf{X} \right) = n \begin{bmatrix} p_{1} \left( 1 - p_{1} \right) &amp;amp; - p_{1} p_{2} &amp;amp; \cdots &amp;amp; - p_{1} p_{k} \\ - p_{2} p_{1} &amp;amp; p_{2} \left( 1 - p_{2} \right) &amp;amp; \cdots &amp;amp; - p_{2}</description></item><item><title>Bilinear Forms and Hermitian Forms</title><link>https://freshrimpsushi.github.io/en/posts/3513/</link><pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3513/</guid><description>Definition1 Let’s say we have two vectors $\mathbf{x}, \mathbf{u} \in \mathbb{R}^{n}$ as follows. $$ \mathbf{x}=\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix},\quad \mathbf{u}^{T} = \begin{bmatrix} u_{1} &amp;amp; u_{2} &amp;amp; \cdots &amp;amp; u_{n} \end{bmatrix} $$ For a real constant $a_{ij} \in \mathbb{R} (1\le i,j \le n)$, the function $A : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$, defined as follows, is called the bilinear form. $$ A(\mathbf{u},\mathbf{x}):=\sum \limits_{i,k=1}^{n}</description></item><item><title>Eigenvalues and Eigenvectors</title><link>https://freshrimpsushi.github.io/en/posts/2483/</link><pubDate>Sun, 26 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2483/</guid><description>Definition Let&amp;rsquo;s assume that a square matrix $A_{n \times n} = (a_{ij})$ is given. The determinant $M_{ij}$ of the matrix obtained by removing the $i$-th row and the $j$-th row from $A$ is called the minor. $C_{ij} := (-1)^{i + j} M_{ij}$ is referred to as the cofactor. The matrix of cofactors $C = \left( C_{ij} \right)$ and its transpose $C^{T}$ is called the classical adjugate matrix, represented by $\operatorname{adj}</description></item><item><title>Quadratic Form</title><link>https://freshrimpsushi.github.io/en/posts/3512/</link><pubDate>Sat, 25 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3512/</guid><description>Definition $V$ is called a $n$dimensional vector space. For a given constant $a_{ij} \in \mathbb{R}(\text{or } \mathbb{C})$, the following second order homogeneous function $A : V \to \mathbb{R}(\text{or } \mathbb{C})$ is called a quadratic form. $$ A(\mathbf{x}) := \sum\limits_{i,j=1}^{n} a_{ij}x_{i}x_{j},\qquad (a_{ij} = a_{ji}) $$ Here, $\mathbf{x} = \begin{bmatrix} x_{1} &amp;amp; \cdots &amp;amp; x_{n} \end{bmatrix}^{T}$ holds. The term $i \ne j$ for $a_{ij}x_{i}x_{j}$ is called the cross product terms. Explanation According</description></item><item><title>Levy's Continuity Theorem in Probability Theory</title><link>https://freshrimpsushi.github.io/en/posts/2482/</link><pubDate>Fri, 24 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2482/</guid><description>Theorem 1 Let a measurable space $\left( \mathbb{R}^{d} , \mathcal{B} \left( \mathbb{R}^{d} \right) \right)$ be given. Denote by $n \in \overline{\mathbb{N}}$ a probability measure with $\mu_{n}$, and the corresponding characteristic function by $\varphi_{n}$. The following are equivalent: (a): $\mu_{n}$ weakly converges to $\mu_{\infty}$. (b): For all $t \in \mathbb{R}^{d}$, $$\lim_{n \to \infty} \varphi_{n} (t) = \varphi_{\infty} (t)$$ $\overline{\mathbb{N}} = \mathbb{N} \cup \left\{ \infty \right\}$ is a set that includes natural</description></item><item><title>Hyperbolic Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/3511/</link><pubDate>Thu, 23 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3511/</guid><description>Definition1 2 Consider the following 2nd order linear partial differential equation for $u(t,x)$. $$ Au_{tt} + Bu_{tx} + Cu_{xx} + Du_{t} + Eu_{x} + Fu + G = 0\qquad (ABC \ne 0) \tag{1} $$ Here, the coefficients $A, \dots, G$ are functions of $(t,x)$. $\Delta = B^{2} - 4AC$ is called the discriminant. A partial differential equation $(1)$ with a positive discriminant is called a hyperbolic PDE. $$ (1) \text{</description></item><item><title>Derivation of the Sherman-Morrison Formula</title><link>https://freshrimpsushi.github.io/en/posts/2481/</link><pubDate>Wed, 22 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2481/</guid><description>Theorem Invertible matrix for $A \in \mathbb{R}^{n \times n}$ and $\mathbf{u} , \mathbf{v} \in \mathbb{R}^{n}$, the following holds true. $$ 1 + \mathbf{v}^{T} A^{-1} \mathbf{u} \ne 0 \iff \exists : \left( A + \mathbf{u} \mathbf{v}^{T} \right)^{-1} $$ Sherman-Morrison Formula When $\left( A + \mathbf{u} \mathbf{v}^{T} \right)^{-1}$ exists, the concrete formula is as follows. $$ \left( A + \mathbf{u} \mathbf{v}^{T} \right)^{-1} = A^{-1} - {{ A^{-1} \mathbf{u} \mathbf{v}^{T} A^{-1} } \over</description></item><item><title>Entropy of Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3510/</link><pubDate>Tue, 21 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3510/</guid><description>Theorem The entropy of the normal distribution $N(\mu, \sigma^{2})$ (when using natural logarithms) is as follows. $$ H = \dfrac{1}{2} \ln (2\pi e \sigma^{2}) = \ln \sqrt{2\pi e \sigma^{2}} $$ The entropy of the multivariate normal distribution $N_{p}(\boldsymbol{\mu}, \Sigma)$ is as follows. $$ H = \dfrac{1}{2}\ln \left[ (2 \pi e)^{p} \left| \Sigma \right| \right] = \dfrac{1}{2}\ln (\det (2\pi e \Sigma)) $$ $\left| \Sigma \right|$ is the determinant of the covariance</description></item><item><title>Polynomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2480/</link><pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2480/</guid><description>Definition Let a random vector composed of $n \in \mathbb{N}$ and $k \in \mathbb{N}$ counts of random variables be denoted as $\left( X_{1} , \cdots , X_{k} \right)$. $$ \sum_{i=1}^{k} X_{i} = n \qquad \&amp;amp; \qquad \sum_{i=1}^{k} p_{i} = 1 $$ For $\mathbf{p} = \left( p_{1} , \cdots , p_{k} \right) \in [0,1]^{k}$ that satisfies this, a multivariate probability distribution $M_{k} \left( n, \mathbf{p} \right)$ with the following probability mass</description></item><item><title>Drawing Subplots in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3509/</link><pubDate>Sun, 19 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3509/</guid><description>설명 Putting multiple photos in a single line can actually be done just by resizing them as shown above. ![3509_image1.png](3509_image1.png#center) To attach captions to each subplot and group them into a single Figure with numbering, the `subfig` package can be used. - `\subfloat[caption]{figure}` Note that it is `\subfloat` not `\subplot`. ```code2 ![3509_image3.png](3509_image3.png#center) ## Full Code ```code3</description></item><item><title>Proof of the Matrix Determinant Lemma</title><link>https://freshrimpsushi.github.io/en/posts/2479/</link><pubDate>Sat, 18 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2479/</guid><description>Theorem An invertible matrix satisfies the following for $A \in \mathbb{R}^{n \times n}$ and $\mathbf{u} , \mathbf{v} \in \mathbb{R}^{n}$. $$ \det \left( A + \mathbf{u} \mathbf{v}^{T} \right) = \left( 1 + \mathbf{v}^{T} A^{-1} \mathbf{u} \right) \det A $$ Particularly, for the classical adjoint matrix $\operatorname{adj} (A) = A^{-1} \det A$, it can be represented as follows. $$ \det \left( A + \mathbf{u} \mathbf{v}^{T} \right) = \det A + \mathbf{v}^{T} \operatorname{adj}</description></item><item><title>How to Insert GIF Animations into a PDF File Using LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3508/</link><pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3508/</guid><description>Description Using \animategraphics from the animate package allows you to compile a pdf file with GIF-like animations inserted. Such animations can be viewed on JavaScript-supporting pdf viewers like Adobe Acrobat or KDE Ocular. \animategraphics[options]{frame}{file name}{start}{end} options: Defines settings like size or autoplay. See the homepage for more details. autoplay: Autoplay loop: Loop playback frame: Frame rate (number of images played per second) file name: Enter the file name. The file</description></item><item><title>Reasons Why the Modified Bessel Function of the First Kind Appears in Directional Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2478/</link><pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2478/</guid><description>Buildup Modified Bessel Functions $$ J_{\nu}(x) = \sum \limits_{n=0}^{\infty} \frac{(-1)^{n} }{\Gamma (n+1) \Gamma (n+\nu+1)} \left(\frac{x}{2} \right)^{2n+\nu} $$ The $I_{\nu}$ defined as follows for the Bessel function of the first kind $J_{\nu}$ is called the modified Bessel function of the first kind1. $$ \begin{align*} I_{\nu} (z) :=&amp;amp; i^{-\nu} J_{\nu} \left( iz \right) \\ =&amp;amp; \left( {{ z } \over { 2 }} \right)^{\nu} \sum_{k=0}^{\infty} {{ {{ z } \over { 2</description></item><item><title>How to Replace Existing Output with New Output in Python and Display Progress</title><link>https://freshrimpsushi.github.io/en/posts/3507/</link><pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3507/</guid><description>English Translation When you want to find out the progress while the code is running, using the print() function, it will be printed like this: for i in range(1,6): print(f&amp;#34;진행 경과[{i}/5]&amp;#34;) At this time, you can add \r in front of the string</description></item><item><title>Fixed Points in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/2477/</link><pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2477/</guid><description>Definition A Fixed Point is said to be a $x_{0} \in X$ that satisfies the following condition for the function $f : X \to X$. $$ f \left( x_{0} \right) = x_{0} $$ When the derivative $f '$ of $f$ is given, the following is also referred to as a fixed point. $$ f ' \left( x_{0} \right) = 0 $$ Explanation In universal mathematics, the concept of fixed points</description></item><item><title>Existence of a Sequence of Simple Functions Converging to a Measurable Function</title><link>https://freshrimpsushi.github.io/en/posts/3506/</link><pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3506/</guid><description>Theorem1 Let $(X, \mathcal{E})$ be a measurable space. If $f : X \to [0, \infty]$ is a measurable function, then there exists a sequence of simple functions $\left\{ \phi_{n} \right\}$ satisfying the following: $$ 0 \le \phi_{1} \le \phi_{2} \le \cdots \le f \quad \text{and} \quad \phi \to f $$ If $f$ is bounded, $$ \phi \rightrightarrows f $$ Here, $\phi \to f$ denotes pointwise convergence, and $\phi \rightrightarrows f$</description></item><item><title>Small-Sample Hypothesis Testing for the Difference Between Two Population Means</title><link>https://freshrimpsushi.github.io/en/posts/2476/</link><pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2476/</guid><description>Hypothesis Testing 1 Assume that two independent populations, each following a normal distribution $N \left( \mu_{1} , \sigma_{1}^{2} \right)$ and $N \left( \mu_{2} , \sigma_{2}^{2} \right)$ with $\sigma_{1}^{2} = \sigma^{2} = \sigma_{2}^{2}$, i.e., the population variances are unknown but assumed to be equal. When the samples are small, meaning the number of samples is $n_{1} , n_{2} &amp;lt; 30$, the hypothesis testing for the difference between two population means $D_{0}$</description></item><item><title>Converting Strings like 'False', 'True' to Bool Type in Python</title><link>https://freshrimpsushi.github.io/en/posts/3505/</link><pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3505/</guid><description>Code When you want to convert the string &amp;quot;False&amp;quot; into a boolean False in Python, the first code you might try looks like this. &amp;gt;&amp;gt;&amp;gt; bool(&amp;#34;False&amp;#34;) True &amp;gt;&amp;gt;&amp;gt; int(&amp;#34;False&amp;#34;) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; ValueError: invalid literal for int() with base 10: &amp;#39;False&amp;#39; However, in this case, since &amp;quot;False&amp;quot; is a non-empty string, bool(&amp;quot;False&amp;quot;) will return True. The function to return False from the</description></item><item><title>Difference Between Root and Solution</title><link>https://freshrimpsushi.github.io/en/posts/2475/</link><pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2475/</guid><description>Definitions A point of the domain that makes the function value of a given function become $0$ is called a root. Something that satisfies the conditions of a given problem is called a solution. Explanation In short, a root is something formal, and a solution is something conceptual. Many people not interested in mathematics get confused by these terms because in many cases, they are used interchangeably. $$ f(x) =</description></item><item><title>What is the Dot Product in Three-Dimensional Space?</title><link>https://freshrimpsushi.github.io/en/posts/3504/</link><pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3504/</guid><description>Definition The inner product of two 3-dimensional vectors $\mathbf{A} = (A_{x}, A_{y}, A_{z})$ and $\mathbf{B} = (B_{x}, B_{y}, B_{z})$ is defined as follows. $$ \mathbf{A} \cdot \mathbf{B} := A_{x}B_{x} + A_{y}B_{y} + A_{z}B_{z} $$ Description In fact, the above definition specifically refers to the dot product. The term inner product is a translation of inner product, which is often used to refer to a more general concept. However, in high</description></item><item><title>Hypothesis Testing for the Population Mean with a Small Sample</title><link>https://freshrimpsushi.github.io/en/posts/2474/</link><pubDate>Wed, 08 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2474/</guid><description>Hypothesis Testing 1 Assuming the population distribution follows a normal distribution $N \left( \mu , \sigma^{2} \right)$ but the population variance $\sigma^{2}$ is unknown. When the sample size is $n &amp;lt; 30$, a small sample, the hypothesis test about the candidate $\mu_{0}$ for the population mean proceeds as follows. $H_{0}$: $\mu = \mu_{0}$. That is, the population mean is $\mu_{0}$. $H_{1}$: $\mu \ne \mu_{0}$. That is, the population mean is</description></item><item><title>Python Matplotlib Basics &amp; Custom Line Styles</title><link>https://freshrimpsushi.github.io/en/posts/3503/</link><pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3503/</guid><description>Basic Configuration1 &amp;lsquo;-&amp;rsquo; or &amp;lsquo;solid&amp;rsquo;: Solid &amp;lsquo;&amp;ndash;&amp;rsquo; or &amp;lsquo;dashed&amp;rsquo;: Dashed &amp;lsquo;-.&amp;rsquo; or &amp;lsquo;dashdot&amp;rsquo;: Dash-Dot &amp;lsquo;:&amp;rsquo; or &amp;lsquo;dotted&amp;rsquo;: Dotted &amp;rsquo;none&amp;rsquo;, &amp;lsquo;None&amp;rsquo;, &amp;rsquo; &amp;lsquo;, or &amp;lsquo;&amp;rsquo;: No line linestyle or ls can be used to set the line style. import numpy as np import matplotlib.pyplot as plt solid = np.ones(10) dashed = 2*solid dashdot = 3*solid dotted = 4*solid none = 5*solid plt.plot(solid, ls=&amp;#34;-&amp;#34;, label=&amp;#34;-&amp;#34;) plt.plot(dashed, ls=&amp;#34;--&amp;#34;, label=&amp;#34;--&amp;#34;) plt.plot(dashdot, ls=&amp;#34;-.&amp;#34;, label=&amp;#34;-.&amp;#34;) plt.plot(dotted,</description></item><item><title>Definition of a Complex Function</title><link>https://freshrimpsushi.github.io/en/posts/2473/</link><pubDate>Mon, 06 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2473/</guid><description>Definition 1 For a non-empty subset $A,B \subset \mathbb{C}$ of the set of complex numbers $\mathbb{C}$, $f : A \to B$ is called a Complex Valued Function. On the other hand, when $A, B \subset \mathbb{R}$, $f : A \to B$ is also referred to as a Real Valued Function to distinguish it from complex functions. Explanation The above definition actually means nothing. You might wonder what all this is</description></item><item><title>Solutions to 'RuntimeError: Parent directory does not exist' Error When Saving Models in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3502/</link><pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3502/</guid><description>Error When saving a model or weights in PyTorch, you may encounter the following error, even though the path definitely exists. &amp;gt;&amp;gt;&amp;gt; print(&amp;#34;Is exists path?: &amp;#34;, os.path.exists(directory)) Is exists path?: True &amp;gt;&amp;gt;&amp;gt; torch.save(model.state_dict(), directory + &amp;#39;weights.pt&amp;#39;) RuntimeError: Parent directory _____ does not exist. Solution In my case, the file path contained the special character deltaΔ, and removing it solved the problem. However, this is</description></item><item><title>Definition of Congruent Covariance</title><link>https://freshrimpsushi.github.io/en/posts/2472/</link><pubDate>Sat, 04 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2472/</guid><description>Buildup Let&amp;rsquo;s say we have samples drawn independently from a population with distribution $X \sim \left( \mu , \sigma^{2} \right)$, but these samples are actually composed of $m$ different populations, $\left( \mu_{1} , \sigma_{1}^{2} \right), \cdots , \left( \mu_{m} , \sigma_{m}^{2} \right)$, with $n_{1} , \cdots , n_{m}$ samples drawn from each, creating a collection of random samples. $$ \begin{align*} \left\{ X_{1} \right\}_{n_{1}} \overset{\text{iid}}{\sim} &amp;amp; \left( \mu_{1} , \sigma_{1}^{2} \right)</description></item><item><title>How to Neatly Print without Axes, Scales, etc. in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3501/</link><pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3501/</guid><description>Code Plots.jl essentially outputs everything including grids, ticks, axes, and color bars by default, but if you want to make it clean without these, you can add the following options. colorbar=:none: Removes the color bar. showaxis = false: Removes the axes and ticks. grid=false: Removes the background grid. ticks=false: Removes both background grid and ticks. framestyle=:none: Removes both background grid and axes. using Plots surface(L, title=&amp;#34;default&amp;#34;) surface(L, title=&amp;#34;colorbar=:none&amp;#34;, colorbar=:none) surface(L,</description></item><item><title>Definition of a Riemann Sphere</title><link>https://freshrimpsushi.github.io/en/posts/2471/</link><pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2471/</guid><description>Definition 1 The Riemann Sphere refers to the two-dimensional unit sphere, $2$-sphere $S^{2} \subset \mathbb{R}^{3}$, that is homeomorphic to the extended complex plane $\overline{\mathbb{C}} = \mathbb{C} \cup \left\{ \infty \right\}$ through one-point compactification. $$ \widetilde{\mathbb{C}} := S^{2} \simeq \overline{\mathbb{C}} $$ Osborne (1999). Complex variables and their applications: p13.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>How to Create a Meshgrid in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3500/</link><pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3500/</guid><description>Overview There is no direct equivalent to the meshgrid() function used in Python and MATLAB. If you only want to obtain the function values on a grid, there is a simpler method that does not require creating a grid. Code 2D Multiplying a column vector by a row vector gives the same result as taking the Kronecker product of a column vector and a row vector. U(t,x) = si</description></item><item><title>Definition of Weighted Average</title><link>https://freshrimpsushi.github.io/en/posts/2470/</link><pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2470/</guid><description>Definition The following is called the Weighted Mean for data $\mathbf{x} = \left\{ x_{1} , \cdots , x_{n} \right\}$ and vector $\mathbf{w} = \left( w_{1} , \cdots , w_{n} \right) \in \mathbb{R}^{n}$. $$ {{ \sum_{k=1}^{n} w_{k} x_{k} } \over { \sum_{k=1}^{n} w_{k} }} = {{ w_{1} x_{1} + \cdots + w_{n} x_{n} } \over { w_{1} + \cdots + w_{n} }} $$ Meanwhile, $\mathbf{w}$ is also called a weighted vector</description></item><item><title>Broadcasting of Multivariable Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3499/</link><pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3499/</guid><description>Overview Introducing how to broadcast multivariable functions in Julia. Like in Python, you can create a meshgrid, or you can easily calculate by creating vectors for each dimension. Bivariate Functions $$ u(t,x) = \sin(\pi x) e^{-\pi^{2}t} $$ To plot the function $(t,x) \in [0, 0.35] \times [-1,1]$ as above, the function values can be calculated like this: x = LinRange(-1., 1, 100) t = LinRange(0., 0.35, 200)&amp;#39; u1 = @.</description></item><item><title>Non-Openness of the Real Axis in the Complex Plane</title><link>https://freshrimpsushi.github.io/en/posts/2469/</link><pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2469/</guid><description>Theorem $$ \mathbb{R}^{\circ} \ne \mathbb{R} \subset \mathbb{C} $$ The real axis $\mathbb{R} \subset \mathbb{C}$ is not open in the complex plane $\mathbb{C}$. Explanation Therefore, the real axis is not a complex domain in the complex space. Considering that from $\mathbb{R}$ to $\mathbb{R}$ is open, this might seem somewhat contrary to intuition. This is essentially because the concept of being open or closed in a set is a relative concept that</description></item><item><title>Lusin's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3498/</link><pubDate>Sat, 28 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3498/</guid><description>Theorem 1 Let $f : E \to \mathbb{R}$ be a Lebesgue measurable function defined on the measurable set $E \subset \mathbb{R}$. Then, for any given positive number $\epsilon \gt 0$, there exists a measurable set $A \subset \mathbb{R}$ that satisfies the following. $$ m(A) \le \epsilon \quad \text{ and } \quad g = f|_{E\setminus A} \text{ is continuous.} $$ Here, $m$ is the Lebesgue measure. Generalization2 If $f$ is a</description></item><item><title>Large Sample Hypothesis Testing for the Difference Between Two Population Means</title><link>https://freshrimpsushi.github.io/en/posts/2468/</link><pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2468/</guid><description>Hypothesis Testing 1 Let&amp;rsquo;s say two independent populations follow distributions $\left( \mu_{1} , \sigma_{1}^{2} \right)$ and $\left( \mu_{2} , \sigma_{2}^{2} \right)$, respectively. In the case of a large sample, meaning the sample size is $n_{1} , n_{2} &amp;gt; 30$, the hypothesis test about the difference between the two population means against candidate $D_{0}$ is as follows: $H_{0}$: $\mu_{1} - \mu_{2} = D_{0}$. That is, the difference in population means is</description></item><item><title>Egorov's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3497/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3497/</guid><description>Theorem1 2 Let a measure space $( X , \mathcal{E} , \mu)$ be given, and let $\mu$ be a finite measure. If a sequence of measurable functions $\left\{ f_{n} : X \to \mathbb{R} \right\}_{n \in \mathbb{N}}$ converges to a measurable function $f$ almost everywhere on $X$, then $f_{n}$ converges to $f$ almost uniformly and in measure. Explanation This theorem essentially states that for measurable functions, pointwise convergence and uniform convergence</description></item><item><title>Topology of Complex Spaces</title><link>https://freshrimpsushi.github.io/en/posts/2467/</link><pubDate>Wed, 25 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2467/</guid><description>Overview We introduce definitions for dealing with the set of complex numbers $\mathbb{C}$ as a topological space. Although it is referred to as a topological space, most of the definitions are specializations of the definitions in a metric space for complex sets. If you have studied introductory analysis diligently, you will be able to understand these without much difficulty. Definitions 1 Let&amp;rsquo;s assume $\alpha \in \mathbb{C}$, $\delta &amp;gt; 0$ and</description></item><item><title>Hypothesis Testing for Population Mean</title><link>https://freshrimpsushi.github.io/en/posts/2466/</link><pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2466/</guid><description>Hypothesis Testing 1 Suppose the population distribution follows $\left( \mu , \sigma^{2} \right)$. When the sample is a large sample, i.e., when the number of samples is $n &amp;gt; 30$, the hypothesis testing for the candidate of population mean $\mu_{0}$ is as follows: $H_{0}$: $\mu = \mu_{0}$. That is, the population mean is $\mu_{0}$. $H_{1}$: $\mu \ne \mu_{0}$. That is, the population mean is not $\mu_{0}$. test statistic The test</description></item><item><title>Numerical Solutions of Heat Equations: Finite Difference Method</title><link>https://freshrimpsushi.github.io/en/posts/3495/</link><pubDate>Sun, 22 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3495/</guid><description>Numerical Solution of Heat Equation1 Let&amp;rsquo;s assume we are given a one-dimensional heat equation as follows. $$ \dfrac{\partial u}{\partial t} = \dfrac{\partial^{2} u}{\partial x^{2}},\qquad 0\le x \le 1,\quad t \ge 0 \tag{1} $$ Our goal is to approximate the solution using finite points. Let’s divide the space-time domain as follows. $$ \left\{ (\ell \Delta x, n\Delta t) : \ell=0,1,\dots,d+1,\ n\ge 0 \right\}\quad \text{ where } \Delta</description></item><item><title>Definition of a Constant Function</title><link>https://freshrimpsushi.github.io/en/posts/2465/</link><pubDate>Sat, 21 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2465/</guid><description>Definition A function $c : X \to Y$ is called a Constant Function if it satisfies the following for all $x_{1} , x_{2} \in X$. $$ c \left( x_{1} \right) = c \left( x_{2} \right) $$ Explanation Typically, the starting point where one first &amp;lsquo;recognizes&amp;rsquo; a constant function as a function is when learning about the differentiation of constant functions. $$ \lim_{h \to 0} {{ c \left( x + h</description></item><item><title>Finite Difference Method</title><link>https://freshrimpsushi.github.io/en/posts/3494/</link><pubDate>Fri, 20 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3494/</guid><description>Definition1 2 The finite difference method is a numerical method for computing derivatives, approximating the derivative as the average rate of change over a short interval. Explanation The key to deriving the formula is the Taylor expansion. $$ f(x+h) = f(x) + f^{\prime}(x)h + \dfrac{f^{\prime \prime}(x)}{2!}h^{2} + \dfrac{f^{\prime \prime \prime}}{3!}h^{3} + \cdots \tag{1} $$ Rearranging to have only the derivative on the left side, $$ \begin{align*} f^{\prime}(x) &amp;amp;= \dfrac{f(x+h) -</description></item><item><title>Estimation of the Variance of Residuals and Standard Errors of Regression Coefficients in Multiple Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/2464/</link><pubDate>Thu, 19 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2464/</guid><description>Theorem $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ When there are $p$ independent</description></item><item><title>Radial Functions</title><link>https://freshrimpsushi.github.io/en/posts/3493/</link><pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3493/</guid><description>Definition1 If the function defined above by $\mathbb{R}^{n}$ satisfies the following, it is called radial. $$ f(R\mathbf{x}) = f(\mathbf{x}) \text{ for all rotations } R $$ Explanation Directly translated as a radial function, but hardly anyone calls it that. The function value depends only on the distance $\left| x \right|$ from the origin. In physics, it is often referred to as spherical symmetry. Examples include gravity, the electric field created</description></item><item><title>Definition of a Rational Function</title><link>https://freshrimpsushi.github.io/en/posts/2463/</link><pubDate>Tue, 17 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2463/</guid><description>Definition 1 For any two polynomial functions $P_{1}(z), P_{2}(z) : \mathbb{C} \to \mathbb{C}$, the following function $Q$ that maps every $z \in \mathbb{C}$ for which $P_{2} (z) \ne 0$ into $\left( P_{1} / P_{2} \right) (z)$ is called a Rational Function or an Algebraic Fraction. $$ Q (z) := {{ P_{1} (z) } \over { P_{2} (z) }} \qquad \text{where } P_{2} (z) \ne 0 $$ Osborne (1999). Complex variables</description></item><item><title>The Fast Fourier Transform Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/3492/</link><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3492/</guid><description>Overview1 The Discrete Fourier Transform (DFT), when computed naively following its mathematical definition, has a time complexity of $\mathcal{O}(N^{2})$. However, by using the algorithm described below, the time complexity can be reduced to $\mathcal{O}(N\log_{2}N)$. This efficient computation method of the Discrete Fourier Transform is known as the Fast Fourier Transform (FFT). Buildup Let&amp;rsquo;s define multiplying two numbers and then adding them to another number as one operation. To compute the</description></item><item><title>Standard Definition of Standard Error</title><link>https://freshrimpsushi.github.io/en/posts/2462/</link><pubDate>Sun, 15 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2462/</guid><description>Definition 1 For a given estimator $T$, the estimated standard deviation of $T$ is called the standard error. $$ \text{s.e.} \left( T \right) := \sqrt{ \widehat{ \Var \left( T \right) } } $$ Explanation The reason why it is precisely defined as an estimator in the definition, not a statistic, is because the standard error becomes meaningless unless we are discussing whether it &amp;lsquo;matches or not&amp;rsquo; with the parameter $\theta$</description></item><item><title>Meaning of Weak and Strong in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/3491/</link><pubDate>Sat, 14 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3491/</guid><description>Description In mathematics, &amp;quot;weak&amp;quot; means &amp;quot;(logically) loose, less strict, less rigorous&amp;quot;. Being &amp;quot;less&amp;quot; something is to be understood in a relative sense. Conversely, &amp;quot;strong&amp;quot; means that the condition is (relatively) strict. In simple terms, &amp;quot;weak&amp;quot; can be translated as &amp;quot;in effect, frankly&amp;quot;. For example, if we talk about college entrance examination scores, the top cumulative 4% of scores are awarded the 1st grade. Strictly, accurately speaking, it&amp;rsquo;s correct that only</description></item><item><title>How to Calculate the Difference Between Two Times in Seconds in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2461/</link><pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2461/</guid><description>Overview To achieve this, one can use the canonicalize() function of the Dates module1. Code using Dates tic = DateTime(2022,3,7,7,1,11) toc = now() Dates.canonicalize(toc-tic) The result of executing the above code is as follows. julia&amp;gt; using Dates julia&amp;gt; tic = DateTime(2022,3,7,7,1,11) 2022-03-07T07:01:11 julia&amp;gt; toc = now() 2022-07-19T22:26:22.070 julia&amp;gt; Dates.canonicalize(toc-tic) 19 weeks, 1 day, 15 hours, 25 minutes, 11 seconds, 70 milliseconds It automatically calculates and outputs up to weeks, precisely</description></item><item><title>Specifying the Color of Axes, Axis Names, Ticks, and Tick Values in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3490/</link><pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3490/</guid><description>Overview The keywords related to specifying the color of axes and ticks in Plots.jl are as follows. Keyword Name Function guidefontcolor Specify axis name color foreground_color_border, fgcolor_border Specify axis color foreground_color_axis, fgcolor_axis Specify tick color foreground_color_text, fgcolor_text Specify tick value color Adding x_ or y_ in front of the keyword name applies it to the respective axis only. Code1 Axis Names The keyword to specify the color of axis names</description></item><item><title>Proof of Normality of Regression Coefficients</title><link>https://freshrimpsushi.github.io/en/posts/2460/</link><pubDate>Wed, 11 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2460/</guid><description>Theorem $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ Given $p$ independent variables and</description></item><item><title>Flux-PyTorch-TensorFlow Cheat Sheet</title><link>https://freshrimpsushi.github.io/en/posts/3489/</link><pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3489/</guid><description>Overview This document organizes code that performs the same functions in Flux, PyTorch, and TensorFlow. Julia-Matlab-Python-R Cheat Sheet Let&amp;rsquo;s assume the following environment for Flux. using Flux Let&amp;rsquo;s assume the following environment for PyTorch. import torch import torch.nn as nn import torch.nn.functional as F Let&amp;rsquo;s assume the following environment for TensorFlow. import tensorflow as tf from tensorflow import keras 1-Dimensional Tensor 줄리아Julia 파</description></item><item><title>How to Convert between 2D Arrays and Matrices in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2459/</link><pubDate>Mon, 09 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2459/</guid><description>Overview Introducing tips for switching between 2D arrays and matrices in Julia, which may be the simplest, fastest, and most beautiful way to do it, especially in environments of Julia 1.7 or lower1. Code There are countless ways to switch between matrices and 2D arrays, not just the method introduced here. Since the goal itself is not difficult whether you code haphazardly or not, it&amp;rsquo;s better to consider not only</description></item><item><title>The Definition of Regression Coefficients and Derivation of Estimator Formulas</title><link>https://freshrimpsushi.github.io/en/posts/2458/</link><pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2458/</guid><description>Definition 1 $$ Y = \beta_{0} + \beta_{1} X_{1} + \cdots + \beta_{p} X_{p} + \varepsilon $$ In multiple regression analysis, when establishing the above linear model for given $p$ independent variables $X_{1} , \cdots , X_{p}$, $\beta_{0} , \beta_{1} , \cdots , \beta_{p}$ is called the regression coefficient. $Y$ represents the dependent variable, and $\varepsilon$ indicates randomly distributed errors. Formula $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n}</description></item><item><title>Functions for Tensor Sorting in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3487/</link><pubDate>Fri, 06 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3487/</guid><description>torch.sort() torch.sort() takes a tensor as input and returns sorted values and indices. 1-dimensional tensor &amp;gt;&amp;gt;&amp;gt; x = torch.tensor([1, 3, -2, 5, -1, 7, 0]) &amp;gt;&amp;gt;&amp;gt; values, indices = torch.sort(x) &amp;gt;&amp;gt;&amp;gt; values tensor([-2, -1, 0, 1, 3, 5, 7]) &amp;gt;&amp;gt;&amp;gt; indices tensor([2, 4, 6, 0, 1, 3, 5]) Multi-dimensional tensor If only the tensor is inputted, it sorts each row. That is, torch.sort(x)=torch.sort(x, dim=1). If a dimension is specified, it</description></item><item><title>How to Send an Email via Naver in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2457/</link><pubDate>Thu, 05 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2457/</guid><description>Overview This document introduces how to send emails from Naver using the SMTPClient.jl package with SMTP 1. I use it to send reports to Kakao Mail when long-running simulations are finished, which helps to speed up my research. This way, knowing when simulations are finished without having to check the server myself, as Jordy notifies me via personal chat. Code Regardless of the programming language, the first thing to do</description></item><item><title>Solving 'RuntimeError: Boolean value of Tensor with more than one value is ambiguous' Error in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3486/</link><pubDate>Wed, 04 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3486/</guid><description>Error When using the loss function nn.MESLoss(), the following error occurred. RuntimeError Traceback (most recent call last) &amp;lt;ipython-input-75-8c6e9ea829d4&amp;gt; in &amp;lt;module&amp;gt; ----&amp;gt; 1 nn.MSELoss(y_pred, y) 2 frames /usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py in legacy_get_string(size_average, reduce, emit_warning) 33 reduce = True 34 ---&amp;gt; 35 if size_average and reduce: 36 ret = &amp;#39;mean&amp;#39; 37 elif reduce: RuntimeError: Boolean value of Tensor with more than one value is ambiguous Solution1 The code can be fixed by changing it</description></item><item><title>Degrees of Freedom in Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2456/</link><pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2456/</guid><description>Terminology When computing a statistic, the number of independent data values that can vary is called degree of freedom1. Explanation Why It’s Hard to Explain Degrees of Freedom When you start studying statistics as a freshman, you&amp;rsquo;re likely to find the concept of &amp;lsquo;degrees of freedom&amp;rsquo; quite frustrating. It’s not only difficult and frequently used but almost impossible to find a textbook that provides</description></item><item><title>Sampling Randomly from a Given Distribution in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3485/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3485/</guid><description>Overview Introducing how to random sample from a given distribution in PyTorch. Various distributions such as Beta, Bernoulli, Cauchy, Gamma, Pareto, and Poisson are implemented. This article explains using the uniform distribution as an example. Code1 The code to random sample from the uniform distribution from $0$ to $5$ in PyTorch is as follows: &amp;gt;&amp;gt;&amp;gt; m = torch.distributions.Uniform(0.0, 5) &amp;gt;&amp;gt;&amp;gt; m.sample() tensor(1.6371) To sample a tensor of size $2 \times</description></item><item><title>Cumulative Average Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2455/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2455/</guid><description>Formula Given a sample $x_{1} , \cdots , x_{n}$ with a sample mean of $y_{n}$, when a new sample $x_{n+1}$ is provided, the overall sample mean $y_{n+1}$ is as follows. $$ y_{n+1} := {{ n } \over {n + 1}} y_{n} + {{1} \over {n+1}} x_{n+1} $$ Description Cumulative Average is also called Moving Average or Running Average. It’s a mistake anyone can make at least once</description></item><item><title>딥러닝에서 레이어란?</title><link>https://freshrimpsushi.github.io/en/posts/3484/</link><pubDate>Sat, 30 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3484/</guid><description>Definition In deep learning, a linear transformation $L^{mn} : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is called a layer. Generalization In deep learning, for a fixed $\mathbf{b} \in \mathbb{R}^{m}$, an affine transformation $\mathbf{x} \mapsto L^{mn}(\mathbf{x}) + \mathbf{b}$ is also called a layer. Description In other words, a layer refers to a linear vector function. On the other hand, a non-linear scalar function is called an activation function. The reason it is called a</description></item><item><title>How to Draw a Map in Excel</title><link>https://freshrimpsushi.github.io/en/posts/2454/</link><pubDate>Fri, 29 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2454/</guid><description>Overview We will be drawing a map in Excel, coloring specific areas based on given numbers. You can learn the details later by yourself, but just knowing this method can dramatically improve your visualization skills. Try including a custom-made map in your proposals or papers. Guide Step 1. Data Preparation Prepare your data in Excel. It should be organized like the above, with place names and corresponding figures. Step 2.</description></item><item><title>Definition of Intervals in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/2453/</link><pubDate>Wed, 27 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2453/</guid><description>Definition $$ [a,b] := \left\{ x \in \mathbb{R} : a \le x \le b \right\} \subset \mathbb{R} $$ For two real numbers $a \le b$, the set as described above is called an Interval. In particular, if both endpoints $a,b$ are included, it is notated as $\left[ a,b \right]$ using square brackets [] and is said to be Closed. If both endpoints $a,b$ are not included, it is notated as</description></item><item><title>Percentiles and Outliers</title><link>https://freshrimpsushi.github.io/en/posts/2452/</link><pubDate>Mon, 25 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2452/</guid><description>Definitions 1 Given quantitative data, A value that is greater than $p \%$ but less than $(100-p) \%$ is called the $p$-percentile. The $100$-percentile and $0$-percentile (the largest and smallest values in the data) are referred to as the maximum, minimum values, respectively. The difference between the maximum and minimum values is called the data&amp;rsquo;s range $R$. The $25$-percentile is called the first quartile $Q_{1}$, and the $75$-percentile is called</description></item><item><title>Definition of Pi</title><link>https://freshrimpsushi.github.io/en/posts/2451/</link><pubDate>Sat, 23 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2451/</guid><description>Definitions Geometric Definition A circle is defined as the set of points in a plane that are at a given distance $r &amp;gt; 0$ from a given point. The ratio of a circle&amp;rsquo;s circumference $l$ to its diameter $2r$ is defined as the Pi $\pi$. $$ \pi := {{ l } \over { 2r }} $$ Analytical Definition 1 $$ E (z) := \sum_{k=0}^{\infty} {{ z^{k} } \over { k!</description></item><item><title>Quadratic Curve</title><link>https://freshrimpsushi.github.io/en/posts/3480/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3480/</guid><description>Definition Quadratic curves are planar curves represented by quadratic equations with two variables, such as: Parabola Ellipse Hyperbola Description A circle can be considered a special case of an ellipse, hence it is not specifically mentioned when generally speaking about quadratic curves.</description></item><item><title>Z-Score and Standardization</title><link>https://freshrimpsushi.github.io/en/posts/2450/</link><pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2450/</guid><description>Definition 1 For a random variable $X$ that follows a distribution with a population mean of $\mu$ and a population standard deviation of $\sigma$, the following transformation is called standardization: $$ Z = {{ X - \mu } \over { \sigma }} $$ Let&amp;rsquo;s assume we&amp;rsquo;re given quantitative data $\left\{ x \right\}$. For the sample mean $\overline{x}$ and the sample standard deviation $s$, the following statistic $z$ is known as</description></item><item><title>Definition of Circle</title><link>https://freshrimpsushi.github.io/en/posts/3479/</link><pubDate>Wed, 20 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3479/</guid><description>Definition A set of points on a plane whose distance from a single point $O$ is constant is called a circle. The point $O$ is called the center of the circle. The distance $r$ from the center is called the radius. Description It can be considered as a special case of an ellipse. An ellipse where the two foci are the same point becomes a circle.</description></item><item><title>Implementation of Zomorodian's Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/2449/</link><pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2449/</guid><description>Overview This section explains and implements the pseudocode of an algorithm introduced in the paper &amp;ldquo;Computing Persistent Homology&amp;rdquo; by Zomorodian and Carlsson1. It takes a filtered complex constructed from an abstract simplicial complex and returns $\mathcal{P}$-intervals, omitting the construction of computationally challenging persistent modules and calculating persistent homology through matrix reduction. Furthermore, the actual implementation does not even use matrix operations. Derivation Derivation of Zomorodian&amp;rsquo;s algorithm: It&amp;rsquo;s certain that without</description></item><item><title>Hyperbola</title><link>https://freshrimpsushi.github.io/en/posts/3478/</link><pubDate>Mon, 18 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3478/</guid><description>Definition 1 The set of points on a plane whose difference in distances to two distinct points $F$, $F^{\prime}$ is constant is called a hyperbola. $\\ $ The points $F$, $F^{\prime}$ are called focus. The midpoint of segment $\overline{FF^{\prime}}$ is called center. The two points where the hyperbola intersects segment $\overline{FF^{\prime}}$, $A$, $A^{\prime}$, are called vertices. The segment $\overline{AA^{\prime}}$ is called major axis. Description Method of Discrimination For a given</description></item><item><title>Definition of Variance in Basic Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2448/</link><pubDate>Sun, 17 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2448/</guid><description>Definition 1 Let&amp;rsquo;s assume we are given $n$ quantitative data. The difference $\left( \overline{x} - x_{i} \right)$ between the sample mean $\overline{x}$ and the data is called the deviation. The value $s^{2}$, which is the sum of the squares of deviations divided by $n-1$, is known as the variance of a sample. $$ s^{2} := {{ \sum \left( x_{i} - \overline{x} \right)^{2} } \over { n-1 }} $$ Taking the</description></item><item><title>Parabola</title><link>https://freshrimpsushi.github.io/en/posts/3477/</link><pubDate>Sat, 16 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3477/</guid><description>Definition 1 For a point $F$ on the plane and a line $l$ that does not pass through it, the set of points equidistant from $F$ and $l$ is called a parabola. $\\ $ $F$ is called the focus. $l$ is called the directrix. The line passing through $F$ and perpendicular to $l$ is called the axis of the parabola. The intersection of the axis and the parabola is called</description></item><item><title>Derivation of Zomorodian's Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/2447/</link><pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2447/</guid><description>Overview The paper &amp;ldquo;Computing Persistent Homology&amp;rdquo; by Zomorodian and Carlsson introduces an algorithm for deriving $\mathcal{P}$-intervals from a Filtered Complex created by an Abstract Simplicial Complex, bypassing the construction of Persistent Modules that are challenging to handle computationally, and computing persistent homology through matrix reduction1. Derivation Part 0. Preliminary Investigation The derivation of the algorithm starts by examining the form of a Persistence Complex depicted as above, crucial for understanding</description></item><item><title>Definition of the Mode in Basic Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2446/</link><pubDate>Wed, 13 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2446/</guid><description>Definitions1 When given qualitative data, the category with the highest frequency is called the Mode. In the case of quantitative data, the class with the highest frequency is called the Modal Class. Description Literally meaning &amp;rsquo;the most frequent value&amp;rsquo;, the mode disregards all information except for that singular value. Unless dealing with qualitative data, its significance greatly diminishes, making it not so important when handling real data. When viewed through</description></item><item><title>Persistent Modules</title><link>https://freshrimpsushi.github.io/en/posts/2445/</link><pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2445/</guid><description>Definition 1 Let $R$ be called a ring and let the extended set of integers $\mathbb{Z}$ be denoted by $\overline{\mathbb{Z}} := \mathbb{Z} \cup \left\{ \pm \infty \right\}$. Given a chain map $f^{i} : \mathsf{C}_{\ast}^{i} \to \mathsf{C}_{\ast}^{i+1}$ between the chain complexes $\mathsf{C}_{\ast}^{i}$ as follows, $\mathcal{C} := \left\{ \left( \mathsf{C}_{\ast}^{i} , \partial_{i} , f^{i} \right) : i \ge 0 \right\}$ is called a Persistent Complex. $$ \mathsf{C}_{\ast}^{0} \overset{f^{0}}{\longrightarrow} \mathsf{C}_{\ast}^{1} \overset{f^{1}}{\longrightarrow} \mathsf{C}_{\ast}^{2} \overset{f^{2}}{\longrightarrow}</description></item><item><title>Definition of Median in Basic Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2444/</link><pubDate>Sat, 09 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2444/</guid><description>Definition 1 Given $n$ quantitative data in ascending order, the value located in the middle of all the data is called the median $m$. If $n$ is odd, $m := x_{(n+1)/2}$ is used, and if $n$ is even, any values that satisfy the following are considered the median. $$ x_{1} \le \cdots \le x_{ \lceil {{ n+1 } \over { 2 }} \rceil } \le m \le x_{ \lceil {{</description></item><item><title>Summary of Measure Theory and Probability Theory</title><link>https://freshrimpsushi.github.io/en/posts/3473/</link><pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3473/</guid><description>Overview This is a summary of definitions and concepts for those who have already studied measure theory and probability. It is intended to be viewed when definitions are confusing or unrecognizable, and when a general review is needed. Measure Theory Algebras An algebra of sets on nonempty set $X$ is a nonempty collection $\mathcal{A}$ of subsets of $X$ is colsed under finite unions ans complements. $\sigma$-algebra is an algebra that</description></item><item><title>Definition of Persistent Homology groups</title><link>https://freshrimpsushi.github.io/en/posts/2443/</link><pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2443/</guid><description>Build Up In the Free group obtained from an Oriented Simplex $k$-Simplex $K$, the boundary operator $\partial_{k} : \mathsf{C}_{k} \to \mathsf{C}_{k-1}$ that satisfies $\partial_{k} \circ \partial_{k+1} = 0$ constitutes a Chain Complex. The Homology group defined as the quotient group of the Cycle group $\mathsf{Z}_{k} := \ker \partial_{k}$ and the Boundary group $\mathsf{B}_{k} := \operatorname{Im} \partial_{k+1}$ is called the $k$th Homology group. $$ \mathsf{H}_{k} := \mathsf{Z}_{k} / \mathsf{B}_{k} $$ Meanwhile,</description></item><item><title>Physics에서 Coordinate Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3472/</link><pubDate>Wed, 06 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3472/</guid><description>Definition Two-dimensional coordinate systems $(x_{1}, \dots, x_{n})$ and $(x_{1}^{\prime}, \dots, x_{n}^{\prime})$ are related by the following functions $f_{1}, \dots, f_{n}$ (or the act of changing coordinates itself) is referred to as a coordinate transformation from $(x_{1}, \dots, x_{n})$ coordinate system to $(x_{1}^{\prime}, \dots, x_{n}^{\prime})$ coordinate system. $$ \begin{align*} x_{1}^{\prime} &amp;amp;= f_{1}(x_{1}, \dots, x_{n}) \\ x_{2}^{\prime} &amp;amp;= f_{2}(x_{1}, \dots, x_{n}) \\ &amp;amp;\vdots \\ x_{n}^{\prime} &amp;amp;= f_{n}(x_{1}, \dots, x_{n}) \\ \end{align*} $$</description></item><item><title>Simplified Definition of Hypothesis Testing</title><link>https://freshrimpsushi.github.io/en/posts/2442/</link><pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2442/</guid><description>Definition 1 2 In science, a statistical hypothesis refers to an assumption about a population, and the statistical decision-making process of accepting or rejecting this hypothesis is called statistical hypothesis testing. This process involves two competing hypotheses, where the hypothesis that the researcher wishes to support is called the alternative hypothesis $H_{1}$, and the hypothesis accepted when there is no substantial evidence to claim that the alternative hypothesis is true</description></item><item><title>Coordinate Systems and Coordinates in Physics</title><link>https://freshrimpsushi.github.io/en/posts/3471/</link><pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3471/</guid><description>Definition When each pair of $n$-ordered pairs $(a_{1}, a_{2}, \dots, a_{n})$ uniquely determines a point in a $n$-dimensional space, the set of these $n$-ordered pairs is called a ($n$-dimensional) coordinate system, and the element $(a_{1}, a_{2}, \dots, a_{n})$ of the coordinate system is called the coordinate of that point. Description In physics, it is generally $n \le 4$. The above definition is nothing more than a reorganization of concepts that</description></item><item><title>Filtration of Complexes</title><link>https://freshrimpsushi.github.io/en/posts/2441/</link><pubDate>Sun, 03 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2441/</guid><description>Definition 1 Let $K$ be a simplicial complex. A subset $L \subset K$ is a Subcomplex of $K$ if it is a simplicial complex itself. $$ \emptyset = K^{0} \subset K^{1} \subset \cdots \subset K^{m} = K $$ A Nested Sequence of subcomplexes of $K$ is called the Filtration of $K$. Generally, for all $i \ge m$, it is presumed that $K^{i} = K^{m}$. When such a filtration exists, $K$</description></item><item><title>Parabolic Partial Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/3470/</link><pubDate>Sat, 02 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3470/</guid><description>Definition1 2 Consider the following second-order linear partial differential equation for $u(t,x)$. $$ Au_{tt} + Bu_{tx} + Cu_{xx} + Du_{t} + Eu_{x} + Fu + G = 0\qquad (ABC \ne 0) \tag{1} $$ Here, the coefficients $A, \dots, G$ are functions of $(t,x)$. $\Delta = B^{2} - 4AC$ is called the discriminant. A partial differential equation $(1)$ with a discriminant of $0$ is referred to as a parabolic PDE. $$</description></item><item><title>Parameter and Statistic in Basic Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2440/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2440/</guid><description>Definition 1 A Parameter is a numerical descriptive measure associated with a population, while something computed from a sample is referred to as a Statistic. Explanation Although there can be various definitions of statistics, fundamentally, it is considered a field of study which primarily focuses on &amp;lsquo;what is a Parameter&amp;rsquo; especially in inferential statistics, a field slightly more abstract from general understanding and favored by majors, including Mathematical Statistics. From</description></item><item><title>Elliptic Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/3469/</link><pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3469/</guid><description>Definition1 2 Consider the following second-order linear partial differential equation for $u(x,y)$. $$ Au_{xx} + Bu_{xy} + Cu_{yy} + Du_{x} + Eu_{y} + Fu + G = 0\qquad (ABC \ne 0) \tag{1} $$ Here, the coefficients $A, \dots, G$ are functions of $(x,y)$. $\Delta = B^{2} - 4AC$ is called the discriminant. A partial differential equation $(1)$ with a negative discriminant is called an elliptic PDE. $$ (1) \text{ is</description></item><item><title>Definition of a Grading Module</title><link>https://freshrimpsushi.github.io/en/posts/2439/</link><pubDate>Wed, 30 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2439/</guid><description>빌드업 Let&amp;rsquo;s denote it as $n,m,i \in \mathbb{Z}$. Graded Ring A Graded Ring equipped with a direct sum $\left( R , \otimes \right) \simeq \bigoplus_{i} R_{i}$ of an Abelian group $R$ that Link $\left( R , + , \cdot \right)$ is defined by the multiplication $\otimes$ between $R_{i}$ being $$ R_{n} \otimes R_{m} \to R_{n+m} $$ Elements within each part of the direct sum, $R_{i}$, are called Homogeneous</description></item><item><title>Inequalities for the Logarithmic Function 1-1/x &lt; log x &lt; x-1</title><link>https://freshrimpsushi.github.io/en/posts/3468/</link><pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3468/</guid><description>Theorem For a logarithmic function with base $e$, the following inequality holds: $$ 1 - \dfrac{1}{x} \le \ln x \le x - 1\qquad \text{ for } x \gt 0 $$ Proof1 Part 1. $\ln x \le x - 1$ Let&amp;rsquo;s set it as $f(x) = x - 1 - \ln x$. Differentiating it gives, $f^{\prime}(x) = 1 - \dfrac{1}{x}$ $(x&amp;gt;0)$. At $0 \lt x \lt 1$, it is $f^{\prime} \lt</description></item><item><title>Definition of Mean in Basic Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2438/</link><pubDate>Mon, 28 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2438/</guid><description>Definition 1 $$ \overline{x} := {{ 1 } \over { n }} \sum_{k=1}^{n} x_{k} $$ When $n$ quantitative data are given, the value obtained by adding all those values and dividing by $n$, denoted as $\overline{x}$, is called the sample mean, arithmetic mean, or average. Description There&amp;rsquo;s no need to explicitly explain how averages can efficiently summarize data. Anyone studying statistics beyond the undergraduate level should be able to address</description></item><item><title>Mutual Information</title><link>https://freshrimpsushi.github.io/en/posts/3467/</link><pubDate>Sun, 27 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3467/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Retracts in Topology</title><link>https://freshrimpsushi.github.io/en/posts/2437/</link><pubDate>Sat, 26 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2437/</guid><description>Definition 1 2 Let&amp;rsquo;s consider a subspace $A \subset X$ of a topological space $X$ and denote the identity function by $\text{id}$. If there exists a continuous surjective function $r : X \to A$ that satisfies $$ r \circ i = \text{id}_{A} : A \to A $$ for an inclusion $i : A \to X$, then $r$ is called a Retraction, and $A$ is called a Retract of $X$. In</description></item><item><title>Relative Entropy (Kullback-Leibler Divergence) in Classical Information Theory</title><link>https://freshrimpsushi.github.io/en/posts/3466/</link><pubDate>Fri, 25 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3466/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Scatter Plot of Multivariate Data</title><link>https://freshrimpsushi.github.io/en/posts/2436/</link><pubDate>Thu, 24 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2436/</guid><description>Definition 1 Given multivariate data, select two quantitative data, one for the horizontal axis (x-axis) and the other for the vertical axis (y-axis), and plot the points. This is referred to as a Scatter Plot. Explanation Below is the multivariate data recording the birth year and height of the girl group &amp;lsquo;WJSN&amp;rsquo; members. Typically, scatter plots are drawn to check for any correlation between two variables. In data science, the</description></item><item><title>What is Conditional Entropy in Classical Information Theory?</title><link>https://freshrimpsushi.github.io/en/posts/3465/</link><pubDate>Wed, 23 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3465/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Definition of Compactifiable Spaces</title><link>https://freshrimpsushi.github.io/en/posts/2435/</link><pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2435/</guid><description>Definition 1 2 Homotopy Type: For two topological spaces $X, Y$, if there exist continuous functions $f : X \to Y$, $g: Y \to X$ satisfying the following, then $X, Y$ are said to have the same Homotopy Type. $X, Y$ or $f, g$ is also referred to as Homotopy Equivalence. $$ \begin{align*} g \circ f \simeq&amp;amp; \text{id}_{X} \\ f \circ g \simeq&amp;amp; \text{id}_{Y} \end{align*} $$ Here, $\text{id}_{\cdot}$ is the</description></item><item><title>What is Joint Entropy in Classical Information Theory?</title><link>https://freshrimpsushi.github.io/en/posts/3464/</link><pubDate>Mon, 21 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3464/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Line Charts of Time Series Data</title><link>https://freshrimpsushi.github.io/en/posts/2434/</link><pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2434/</guid><description>Definition 1 Given time series data, if we represent time changes on the horizontal axis (x-axis) and connect the changes in values with a line, such a graph is called a Line Chart. Mendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p19.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Sampling Randomly from a Given Distribution in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3463/</link><pubDate>Sat, 19 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3463/</guid><description>설명 Using the Distributions.jl package, you can randomly sample from a given distribution.</description></item><item><title>The Fundamental group of a Torus is Isomorphic to the Product of Two Integer groups</title><link>https://freshrimpsushi.github.io/en/posts/2433/</link><pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2433/</guid><description>Theorem 1 $$ \pi_{1} \left( T^{2} \right) \simeq \mathbb{Z} \times \mathbb{Z} $$ The fundamental group of a torus $T^{2}$ is $\mathbb{Z} \times \mathbb{Z}$. Proof Properties of induced homomorphisms: [2]: If $\varphi : X \to Y$ is a homeomorphism, then $\varphi_{\ast} : \pi_{1} \left( X, x \right) \to \pi_{1} \left( Y, \varphi (x) \right)$ is an isomorphism. Fundamental group of a product space: $$ \pi_{1} \left( X \times Y \right) \simeq</description></item><item><title>Sampling Randomly in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3462/</link><pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3462/</guid><description>Description1 In Julia, the function for random sampling is as follows: rand([rng=default_rng()], [S], [dims...]) rng stands for Random Number Generator, which specifies the random number generation algorithm. If you don&amp;rsquo;t understand what this means, it&amp;rsquo;s okay to leave it untouched. S likely stands for Set, and it is a variable that specifies the set from which the random sampling will occur. The variables that can be input for S include</description></item><item><title>Histograms of Quantitative Data</title><link>https://freshrimpsushi.github.io/en/posts/2432/</link><pubDate>Wed, 16 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2432/</guid><description>Definitions 1 2 Complex Definition A bar chart made from the frequency distribution of quantitative data is called a histogram. Simple Definition A histogram is a bar chart where numerical data is divided into intervals, and the frequency of data within those intervals is counted, with the sizes represented as the heights of the bars. Explanation Histograms are an indispensable visualization technique in scientific literature, especially used to represent probability</description></item><item><title>Writing TeX Code in Gmail (Email)</title><link>https://freshrimpsushi.github.io/en/posts/3461/</link><pubDate>Tue, 15 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3461/</guid><description>Description Installing the Chrome extension TeX for Gmail allows you to use TeX code in Gmail. It&amp;rsquo;s also available for the Naver Whale browser. The usage is straightforward. Write your TeX code and press F8, then all the mathematical formulas in the email get rendered. You can copy and paste this into another email as well. As shown in the GIF below, you don&amp;rsquo;t necessarily have to highlight anything; simply</description></item><item><title>Homotopy Type</title><link>https://freshrimpsushi.github.io/en/posts/2431/</link><pubDate>Mon, 14 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2431/</guid><description>Definition 1 For two topological spaces $X, Y$, if there exist continuous functions $f : X \to Y$, $g: Y \to X$ that satisfy the following, then $X, Y$ is said to have the same Homotopy Type and $X, Y$ or $f, g$ is also referred to as Homotopy Equivalent. $$ \begin{align*} g \circ f \simeq&amp;amp; \text{id}_{X} \\ f \circ g \simeq&amp;amp; \text{id}_{Y} \end{align*} $$ Here, $\text{id}_{\cdot}$ is the identity</description></item><item><title>Bar Graphs of Qualitative Data</title><link>https://freshrimpsushi.github.io/en/posts/2430/</link><pubDate>Sat, 12 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2430/</guid><description>Definition 1 Assume that the frequency distribution of qualitative data is given. A graph that represents the frequency with the height of the bars is called a Bar Chart. A graph that represents the relative frequency with the area of the sectors is called a Pie Chart. Description Differences between the Two Charts Obviously, the shapes are dissimilar, but the purpose and advantages/disadvantages of the two graphs differ. Unlike the</description></item><item><title>The Fundamental group of a Product Space is Isomorphic to the Product of the Fundamental groups</title><link>https://freshrimpsushi.github.io/en/posts/2429/</link><pubDate>Thu, 10 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2429/</guid><description>Theorem Let $X, Y$ be a topological space. The fundamental group of the (topology) Cartesian product is isomorphic to the Cartesian product of the groups of each respective component. $$ \pi_{1} \left( X \times Y , \left( x_{0} , y_{0} \right) \right) \simeq \pi_{1} \left( X, x_{0} \right) \times \pi_{1} \left( Y, y_{0} \right) $$ In particular, if $X, Y$ are all path-connected, then one can omit the base point</description></item><item><title>CSS color name tags</title><link>https://freshrimpsushi.github.io/en/posts/3459/</link><pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3459/</guid><description>Overview1 140+ CSS color palettes with names. Code</description></item><item><title>Quantitative Data Classification</title><link>https://freshrimpsushi.github.io/en/posts/2428/</link><pubDate>Tue, 08 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2428/</guid><description>Definition 1 The intervals created by setting upper and lower limits according to the values of quantitative data are called classes. The number of data that belongs to each class is called frequency. Explanation Just as the academic term Class is translated into class in a straightforward manner in statistics, the part that might seem grating, like the term &amp;lsquo;class&amp;rsquo; rather than &amp;lsquo;category&amp;rsquo; which might come up in areas such</description></item><item><title>Drawing Diagrams of Universal Properties with TikZ</title><link>https://freshrimpsushi.github.io/en/posts/3458/</link><pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3458/</guid><description>Code \documentclass[12pt]{article} \usepackage{tikz} \begin{document} $$ \begin{tikzpicture}[node distance=3.5cm, auto] \node (Vtimes) {$V_{1} \times V_{2}$}; \node (Votimes) [right of = Vtimes] {$V_{1} \otimes V_{2}$}; \node (W) [node distance=2.0cm, below of = Votimes] {$W$}; \draw[-&amp;gt;] (Vtimes) to node [swap] {$\phi$} (W); \draw[-&amp;gt;] (Vtimes) to node {$f$} (Votimes); \draw[-&amp;gt;, dashed] (Votimes) to node {$\psi$} (W); \end{tikzpicture} $$ \end{document} 설명 $\KaTeX$ is still lacking the ability to draw diagrams. To draw diagrams like the</description></item><item><title>The Fundamental group of a Circle is Isomorphic to the Integer group</title><link>https://freshrimpsushi.github.io/en/posts/2427/</link><pubDate>Sun, 06 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2427/</guid><description>Theorem The fundamental group $\pi_{1} \left( S^{1}, 1 \right)$ of the unit circle $S^{1}$ is isomorphic to the integer group $\mathbb{Z}$. $$ \pi_{1} \left( S^{1}, 1 \right) \simeq \mathbb{Z} $$ Here, $(1,0) \in \mathbb{R}^{2}$ is also simply denoted as $1$. Explanation Application Studying homotopy, one of the biggest and most important results that can be obtained is that the fundamental group of $S^{1}$ is the integer group. Known applications include:</description></item><item><title>Matrix Representation of the Sum and Scalar Multiplication of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3457/</link><pubDate>Sat, 05 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3457/</guid><description>Theorem Let $V, W$ be a finite-dimensional vector space with a given ordered basis $\beta, \gamma$. Also, let $T, U : V \to W$. Then, the following hold:$\\[0.5em]$ $[T + U]_{\beta}^{\gamma} = [T]_{\beta}^{\gamma} + [U]_{\beta}^{\gamma}$ $[aT]_{\beta}^{\gamma} = a[T]_{\beta}^{\gamma}$ Here, $[T]_{\beta}^{\gamma}$ is the matrix representation of $T$. Proof Since the proofs are similar, we will only prove the first equation. Let $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ and $\gamma =</description></item><item><title>Qualitative Data Frequency</title><link>https://freshrimpsushi.github.io/en/posts/2426/</link><pubDate>Fri, 04 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2426/</guid><description>Definition 1 The frequency at which each observation of qualitative data appears is referred to as Frequency. Dividing the frequency by the total number of data and multiplying the relative frequency by 100 is called Relative Frequency and Percentage, respectively. How each frequency or relative frequency is distributed across several categories is known as Frequency Distribution. Explanation Frequency, often translated as frequency in the context of physics, in the context</description></item><item><title>Continuity of Relative Homotopy 연속함수의 상대적 호모토피</title><link>https://freshrimpsushi.github.io/en/posts/2425/</link><pubDate>Wed, 02 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2425/</guid><description>Definition 1 Generalization of Homotopy Let $I = [0,1]$ be the unit interval and $X, Y$ a topological space. For two continuous mappings $f_{0} , f_{1} : X \to Y$, if there exists a continuous mapping $F : X \times Y$ satisfying $$ F (x , 0) = f_{0} (x) \\ F (x , 1) = f_{1} (x) $$ then $f_{0}, f_{1}$ is said to be homotopic and $F$ is</description></item><item><title>Definition of Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2424/</link><pubDate>Mon, 31 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2424/</guid><description>Definition 1 Statistics is a collection of methods for collecting, analyzing, representing, interpreting, and making decisions about data. Descriptive statistics consist of methods that use charts or graphs and summary measures to organize, present, and describe data. Inferential statistics consist of methods for making decisions or predictions about a population from a sample. Commentary Below is a story beyond the textbook. I personally would like to define statistics as &amp;ldquo;a</description></item><item><title>Special Unitary Group</title><link>https://freshrimpsushi.github.io/en/posts/3454/</link><pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3454/</guid><description>Definition The set of unitary matrices whose determinant is $1$ is denoted by $\operatorname{SU}(n)$ and called the special unitary group of degree $n$. $$ \operatorname{SU}(n) := \left\{ n \times n \text{ unitary matrix} \right\} = {\left\{ A \in M_{n \times n}(\mathbb{C}) : A A^{\ast} = I \right\}} $$ Here, $A^{\ast}$ is the conjugate transpose matrix. Explanation Since it consists only of unitary matrices, it forms a group with respect to</description></item><item><title>Induced Homomorphism in Algebraic Topology</title><link>https://freshrimpsushi.github.io/en/posts/2423/</link><pubDate>Sat, 29 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2423/</guid><description>Definition 1 2 Let $X,Y$ be a topological space and $\varphi : X \to Y$ be continuous. The homomorphism defined as $$ \varphi_{\ast} [f] := \left[ \varphi f \right] = \left[ \varphi \circ f \right] \qquad , \forall f : I \to X $$ and $$ \varphi_{\ast} : \pi_{1} \left( X, x \right) \to \pi_{1} \left( Y, \varphi (x) \right) $$ is called Induced Homomorphism. Theorem [1]: If both $\phi</description></item><item><title>Orthogonal Group</title><link>https://freshrimpsushi.github.io/en/posts/3453/</link><pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3453/</guid><description>Definition $n \times n$ The set of orthogonal matrices is denoted by $\operatorname{O}(n)$ and is called the $n$-dimensional orthogonal group. $$ \operatorname{O}(n) := {\left\{ A \in M_{n \times n}(\mathbb{R}) : AA^{T} = I \right\}} $$ Description Since it is a set of orthogonal matrices, only invertible matrices exist. Hence, it forms a group with respect to matrix multiplication, and is a subgroup of the general linear group $\mathrm{GL}(n, \mathbb{R})$. It</description></item><item><title>Scales in Statistics: Nominal, Ordinal, Interval, Ratio</title><link>https://freshrimpsushi.github.io/en/posts/2422/</link><pubDate>Thu, 27 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2422/</guid><description>Overview Generally, creating data in the real world refers to the action of observing a phenomenon or experiment and recording it, which is called measuring. Definition 1 There are four known scales for measuring data: Nominal: The purpose is classification itself. Ordinal: Order is assigned. Interval: It is determined within a certain range. Ratio: It is expressed in proportion to a certain unit. Explanation Nominal and ordinal scales are used</description></item><item><title>Special Linear Group</title><link>https://freshrimpsushi.github.io/en/posts/3452/</link><pubDate>Wed, 26 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3452/</guid><description>Definition The set of matrices with determinant $1$ is denoted by $\mathrm{SL}(n, \mathbb{R})$ and called the special linear group of degree $n$. $$ \mathrm{SL}(n, \mathbb{R}) := {\left\{ A \in M_{n \times n}(\mathbb{R}) : \det{A} = 1 \right\}} $$ Description Since it is a set of matrices with determinant $1$, only invertible matrices exist. Thus, it forms a group under matrix multiplication and is a subgroup of the general linear group</description></item><item><title>Monodromy Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/2421/</link><pubDate>Tue, 25 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2421/</guid><description>Theorem 1 Let&amp;rsquo;s say there are two paths $f_{0} \simeq f_{1}$ that are equivalent starting at point $1 := (1,0)$ on the sphere $S^{1}$. If each of their lifts satisfies $\widetilde{f}_{0}, \widetilde{f}_{1}$ equals $\widetilde{f}_{0} (0) = \widetilde{f}_{1} (0)$, then it follows that $\widetilde{f}_{0} (1) = \widetilde{f}_{1} (1)$. Description $1 = (1,0)$ Strictly speaking, $1 = (1,0)$ should be distinguished, but for convenience, if we simply state $1$ as being from</description></item><item><title>Unitary Group</title><link>https://freshrimpsushi.github.io/en/posts/3451/</link><pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3451/</guid><description>Definition $n \times n$ The set of unitary matrices is denoted as $\mathrm{U}(n)$ and is called the unitary group of degree $n$. $$ \mathrm{U}(n) := \left\{ n \times n \text{ unitary matrix} \right\} = {\left\{ A \in M_{n \times n}(\mathbb{C}) : A A^{\ast} = I \right\}} $$ Here, $A^{\ast}$ is the conjugate transpose matrix. Explanation Since it only collects unitary matrices, it forms a group under matrix multiplication. It is</description></item><item><title>Qualitative Variable and Quantitative Variable</title><link>https://freshrimpsushi.github.io/en/posts/2420/</link><pubDate>Sun, 23 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2420/</guid><description>Definition 1 Qualitative Variables Variables that measure qualitative characteristics are called qualitative variables. The food is&amp;hellip; delicious / so-so / tasteless The color is&amp;hellip; red / blue / yellow The major is&amp;hellip; mathematics / statistics / physics Such qualitative variables are often referred to as categorical data. Quantitative Variables Variables that measure quantitative characteristics are called quantitative variables. Age is&amp;hellip; 20 years old / 31 years old / 11 years</description></item><item><title>General Linear Group</title><link>https://freshrimpsushi.github.io/en/posts/3450/</link><pubDate>Sat, 22 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3450/</guid><description>Definition The set of real invertible $n \times n$ matrices is denoted by $\mathrm{GL}(n, \mathbb{R})$ or $\mathrm{GL}_{n}(\mathbb{R})$ and is called the general linear group of degree $n$. $$ \mathrm{GL}(n, \mathbb{R}) := \left\{ n \times n \text{ invertible matrix} \right\} = M_{n \times n}(\mathbb{R}) \setminus {\left\{ A \in M_{n \times n}(\mathbb{R}) : \det{A} = 0 \right\}} $$ Explanation Since it consists only of invertible matrices, it forms a group with respect</description></item><item><title>Proof of the Lifting Theorem in Algebraic Topology</title><link>https://freshrimpsushi.github.io/en/posts/2419/</link><pubDate>Fri, 21 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2419/</guid><description>Theorem 1 2 Definitions of Covering and Lifting: Let&amp;rsquo;s denote the unit interval as $I = [0,1]$. An open set $U \subset X$ of $X$ is evenly covered by $p$ if for every $\alpha \in \forall$, all corresponding restricted functions $p |_{\widetilde{U}_{\alpha}}$ are homeomorphisms, and $$ \alpha_{1} \ne \alpha_{2} \implies \widetilde{U}_{\alpha_{1}} \cap \widetilde{U}_{\alpha_{2}} = \emptyset $$ holds, meaning there exist disjoint open sets $\widetilde{U}_{\alpha} \subset \widetilde{X}$ in $\widetilde{X}$ such that</description></item><item><title>Convolutional Neural Network (CNN)</title><link>https://freshrimpsushi.github.io/en/posts/3449/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3449/</guid><description>Definition A composite function obtained by appropriately combining a convolutional layer, pooling layer, and activation function is called a convolutional neural network. Description A function composed of a convolutional layer and an activation function is called a convolutional neural network. CNNs demonstrate excellent performance predominantly in image-related tasks. In the case of MLP, when values pass through each layer, they are sent to a fully connected layer, which can lead</description></item><item><title>Definition and Etymology of Data</title><link>https://freshrimpsushi.github.io/en/posts/2418/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2418/</guid><description>Overview In modern society, there is no intellectual who knows absolutely nothing about data. Even non-specialists with no interest can easily think of synonyms such as &amp;lsquo;knowledge about something&amp;rsquo; or &amp;lsquo;resources for communication&amp;rsquo; like data or information, to the extent that the concept of data has become universal and popularized. The following descriptions are merely attempts to define data a little more strictly from the perspective of data science. Definition</description></item><item><title>딥러닝에서 풀링층이란?</title><link>https://freshrimpsushi.github.io/en/posts/3448/</link><pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3448/</guid><description>Overview In artificial neural networks, a pooling layer is a function that reduces the dimension of input data into smaller local units. Keeping only the maximum value within a specified region is called max pooling, while retaining the average value of a specified region is referred to as average pooling. Definition For $m \times m$ matrix $\mathbf{X} = [X_{ij}]$, the following function is called max pooling. $$ \begin{align*} P_{\text{max}} :</description></item><item><title>Covering and Lifting in Algebraic Topology</title><link>https://freshrimpsushi.github.io/en/posts/2417/</link><pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2417/</guid><description>Definitions 1 2 Let $p : \widetilde{X} \to X$ be a continuous function between two topological spaces $\widetilde{X}, X$. Denote any index set as $\forall$, and let&amp;rsquo;s write the restriction function from $\widetilde{U}_{\alpha} \subset \widetilde{X}$ to $p$ simply as $p |_{\widetilde{U}_{\alpha}} : \widetilde{U}_{\alpha} \to U$. $I = [0,1]$ is the unit interval from $0$ to $1$. $\bigsqcup$ represents the union of disjoint sets. Covering An open set $U \subset X$</description></item><item><title>Multilayer Perceptron (MLP), Fully Connected Neural Network (FCNN)</title><link>https://freshrimpsushi.github.io/en/posts/3447/</link><pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3447/</guid><description>Definition Let $L_{i} : \mathbb{R}^{n_{i}} \to \mathbb{R}^{n_{i+1}}$ be referred to as a fully connected layer. Let $\sigma : \mathbb{R} \to \mathbb{R}$ be referred to as an activation function. The composition of these is called a multilayer perceptron. $$ \operatorname{MLP}(\mathbf{x}) = T_{N} \circ \overline{\sigma} \circ T_{N-1} \circ \overline{\sigma} \circ \cdots \circ T_{1} (\mathbf{x}) $$ Here, $\overline{\sigma}$ is a function that applies $\sigma$ to each component. $$ \overline{\sigma}(\mathbf{x}) = \begin{bmatrix} \sigma(x_{1}) \\</description></item><item><title>How to Print Without Omitting Data in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2416/</link><pubDate>Sat, 15 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2416/</guid><description>Overview Originally, Julia formats the data output to fit the size of the REPL beautifully, but sometimes we want to see the entire data comfortably. If the data is foo, you can print the entire data using show(stdout, &amp;quot;text/plain&amp;quot;, foo)1. Code julia&amp;gt; foo = rand(100,2) 100×2 Matrix{Float64}: 0.956438 0.663427 0.790117 0.472821 0.976134 0.198475 0.727601 0.472336 0.0469046 0.991999 0.625807 0.26634 0.490773 0.588481 0.352966 0.426474 0.585632 0.00185974 ⋮</description></item><item><title>Total Variation in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3232/</link><pubDate>Fri, 14 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3232/</guid><description>Definition1 A regular curve $\gamma$ is considered piecewise simple curve with a period of $L$ being a closed curve. Let $\mathbf{Z}(t)$ be a continuous vector field along $\gamma$. Suppose the vector field $\mathbf{V}$ satisfies $\left| \mathbf{V}(p) \right| = 1$ $\forall p \in U$. Let $\alpha$ be a function mapping the angle between $\mathbf{Z}$ and $\mathbf{V}$. $$ \alpha (t) = \angle \left( \mathbf{V}(\gamma (t)), \mathbf{Z}(t) \right) $$ Now, the total angular</description></item><item><title>Fundamental group in Algebraic Topology</title><link>https://freshrimpsushi.github.io/en/posts/2415/</link><pubDate>Thu, 13 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2415/</guid><description>Definition 1 Given a topological space $X$ and a unit interval $I = [0,1]$, For paths $f, g : I \to X$ in $X$, when $f (1) = g(0)$, the product or composition $f \cdot g$ of two paths is defined as: $$ f \cdot g (s) := \begin{cases} f \left( 2s \right) &amp;amp; , \text{if } s \in [0, 1/2] \\ g \left( 2s - 1 \right) &amp;amp; ,</description></item><item><title>Iris Dataset</title><link>https://freshrimpsushi.github.io/en/posts/3445/</link><pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3445/</guid><description>Overview1 The Iris dataset refers to a dataset about the observation records of iris flowers, created by the American botanist, Edgar Anderson, and introduced by the British statistician, Ronald Fisher2. Description It is the most commonly used dataset in machine learning and data analysis practice.3 It consists of data from observing 50 flowers each of the three species of iris: setosa, versicolor, and virginica. It measures each flo</description></item><item><title>1st Shrimp Sushi Restaurant Competition: Graph group</title><link>https://freshrimpsushi.github.io/en/posts/2414/</link><pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2414/</guid><description>In celebration of 2022, Fresh Shrimp Sushi Shop is holding a periodic event where readers can participate. We will release one challenge each quarter. The section for this competition is Pure Mathematics, where we explore the structure of sets of graphs given certain operations. The expected difficulty level is for undergraduate students in their 2nd to 3rd year. Graph Group Let&amp;rsquo;s denote a set of $n$ labeled simple graphs as</description></item><item><title>MNIST Database</title><link>https://freshrimpsushi.github.io/en/posts/3444/</link><pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3444/</guid><description>Overview1 $$ \includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$ The MNIST database refers to a dataset of digit handwriting from American high school students and Census Bureau employees. It is commonly known as [MNIST]. Official Website Description This dataset is frequently used as an example for beginners in machine learning/deep learning. NIST originally collected handwritten data in the following format for the evaluation of character recognition technology for automated sorting of handwritten postal codes. Yann</description></item><item><title>Homotopy Classes</title><link>https://freshrimpsushi.github.io/en/posts/2413/</link><pubDate>Sun, 09 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2413/</guid><description>Theorem Brief Description In any topological space, the relation of a homotopy defined between any two fixed points is an equivalence relation. Detailed Description Given a topological space $X$ and two points $x_{0}, x_{1} \in X$, if the paths $f, g : I \to X$ between two points are homotopic, as expressed by $f \simeq g$, then this binary relation $\simeq$ is an equivalence relation. Moreover, the equivalence classes created</description></item><item><title>Various Deep Learning Frameworks of Julia</title><link>https://freshrimpsushi.github.io/en/posts/3443/</link><pubDate>Sat, 08 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3443/</guid><description>Overview Last modified date: November 22, 2022 Among Julia&amp;rsquo;s representative deep learning frameworks, there is Flux.jl. Along with it, other frameworks such as Knet.jl and Lux.jl will be briefly introduced. Description Flux Flux is the official deep learning framework of Julia. Various packages of GraphNeuralNetworks.jl and SciML are implemented based on Flux. Although Flux&amp;rsquo;s functionality and features are still lacking compared to TensorFlow and PyTorch, it might eventually catch up</description></item><item><title>How to Use the Linear Algebra Package in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2412/</link><pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2412/</guid><description>Overview Julia supports [linear algebra](../../categories/Linear Algebra) as well as MATLAB does, if not better. The intuitive and elegant syntax of Julia gives a feeling that it has been well-designed since its inception1. Code julia&amp;gt; A = [ 1 0 3 0 5 1 3 1 9 ] 3×3 Matrix{Int64}: 1 0 3 0 5 1 3 1 9 As you can see, defining matrices is intuitive and easy</description></item><item><title>Automatic differentiation</title><link>https://freshrimpsushi.github.io/en/posts/3442/</link><pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3442/</guid><description>Definition1 2 Automatic differentiation refers to a method for obtaining the derivative of a function defined by computer programming code. It is also abbreviated as AD or autodiff. Explanation Automatic differentiation involves using the chain rule to compute the derivative of a composite function composed of functions whose derivatives are already known. In simple terms, it is the chain rule itself. Implementing the chain rule in programming code constitutes automatic</description></item><item><title>Definition of Homotopy</title><link>https://freshrimpsushi.github.io/en/posts/2411/</link><pubDate>Wed, 05 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2411/</guid><description>Definitions 1 Let&amp;rsquo;s assume that the closed unit interval $I := [0,1]$ and the topological space $X$ are given. A continuous function $p : I \to X$ from $x_{0}$ to $x_{1}$ satisfying the following for fixed points $x_{0} , x_{1} \in X$ is called a path or path. $$ \begin{align*} p(0) =&amp;amp; x_{0} \\ p(1) =&amp;amp; x_{1} \end{align*} $$ For two paths $f \equiv h_{0}$ and $g \equiv h_{1}$, the</description></item><item><title>Using Date and Time Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2410/</link><pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2410/</guid><description>Overview 1 Dates is a module that collects functions related to dates and times. It is inevitably useful not only for general programming but also for handling a lot of data, whether it&amp;rsquo;s related to time series or not1. Code Full Code using Dates 오늘 = DateTime(2022,3,10) typeof(오늘) propertyname</description></item><item><title>How to Use Fast Fourier Transform (FFT) in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3440/</link><pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3440/</guid><description>Overview 1 2 The Fastest Fourier Transform in the West (FFTW) is a software library developed by Matteo Frigo and Steven G. Johnson at the Massachusetts Institute of Technology (MIT) for computing the Discrete Fourier Transform. While there exists a Julia package named AbstractFFTs.jl for FFT implementation, it is not intended to be used on its own but rather to aid in the implementation of fast Fourier transforms, such as</description></item><item><title>Abstract Simplicial Complexes: Definitions</title><link>https://freshrimpsushi.github.io/en/posts/2409/</link><pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2409/</guid><description>Definition 1 Let&amp;rsquo;s say an arbitrary set $X$ is given. A (Abstract Simplicial) Complex $A \subset 2^{X}$ that satisfies the following among the finite subsets of the power set $2^{X}$ of $X$ is defined as: $$ \alpha \in A \land \beta \subset \alpha \implies \beta \in A $$ The elements $\alpha \in A$ of the complex $A$ are called Simplices. The Dimension of a Simplex $\alpha$ $\dim$ is defined as</description></item><item><title>How to Change Basic Data Types in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3439/</link><pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3439/</guid><description>Overview In fields like machine learning, 32-bit floating point numbers are used instead of 64-bit ones for improving computation speed and saving memory. Therefore, in PyTorch, when tensors are created, their data type is fundamentally 32-bit floating point numbers by default. In Julia, there&amp;rsquo;s a machine learning package called Flux.jl, which takes Julia&amp;rsquo;s standard arrays as input for the neural networks it implements. The fact that it does not use</description></item><item><title>Proof of the Representation Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2408/</link><pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2408/</guid><description>Theorem Let&amp;rsquo;s assume we are given an input set $X \ne \emptyset$ and a positive definite kernel $k: X \times X \to \mathbb{R}$. Define the Training Dataset as $$ D := \left\{ \left( x_{i} , y_{i} \right) \right\}_{i=1}^{m} \subset X \times \mathbb{R} $$ and a class in the Reproducing Kernel Hilbert Space $H_{k}$ as $$ \mathcal{F} := \left\{ f \in \mathbb{R}^{X} : f \left( \cdot \right) = \sum_{i=1}^{\infty} \beta_{i} k</description></item><item><title>What is One-Hot Encoding in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/3438/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3438/</guid><description>Definition Given a set $X \subset \mathbb{R}^{n}$, suppose its subsets $X_{i}$ satisfy the following. $$ X = X_{1} \cup \cdots \cup X_{N} \quad \text{and} \quad X_{i} \cap X_{j} = \varnothing \enspace (i \ne j) $$ Let&amp;rsquo;s call $\beta = \left\{ e_{1}, \dots, e_{N} \right\}$ the standard basis of $\mathbb{R}^{N}$. Then, the following function, or mapping $x \in X$ itself, is called one-hot encoding. $$ \begin{align*} f : X &amp;amp;\to \beta</description></item><item><title>Euler Characteristic in Algebraic Topology</title><link>https://freshrimpsushi.github.io/en/posts/2407/</link><pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2407/</guid><description>Definition 1 Let us assume a simplex $\Delta$ is given. When the number of vertices of $\Delta$ is $n$, the number of edges is $m$, and the number of faces is $f$, the Euler Characteristic $\chi$ of $\Delta$ is defined as follows. $$ \chi := n - m + f $$ Theorem Given the simplicial homology group formed from a simplicial complex $K$ for a topological space $X$, the Euler</description></item><item><title>Encoding and Decoding in Information Theory</title><link>https://freshrimpsushi.github.io/en/posts/3437/</link><pubDate>Mon, 26 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3437/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Definite Kernel and Reproducing Kernel Hilbert Space in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/2406/</link><pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2406/</guid><description>Definition 1 2 Input Space $X \ne \emptyset$ is the domain and the codomain is the set of complex numbers $\mathbb{C}$, and let&amp;rsquo;s denote the space of functions $\left( H , \left&amp;lt; \cdot , \cdot \right&amp;gt; \right) \subset \mathbb{C}^{X}$ composed of mappings $f: X \to \mathbb{C}$ as a Hilbert space. Reproducing Kernel Hilbert Space For a fixed datum $x \in X$, the functional $\delta_{x} : H \to \mathbb{C}$, which takes</description></item><item><title>Hadamard Product of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3436/</link><pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3436/</guid><description>Definition The Hadamard product $A \odot B$ of two matrices $A, B \in M_{m \times n}$ is defined as follows. $$ A \odot B = \begin{bmatrix} a_{11} &amp;amp; \cdots &amp;amp; a_{1n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} \odot\begin{bmatrix} b_{11} &amp;amp; \cdots &amp;amp; b_{1n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ b_{m1} &amp;amp; \cdots &amp;amp; b_{mn} \end{bmatrix} := \begin{bmatrix} a_{11}b_{11} &amp;amp; \cdots &amp;amp; a_{1n}b_{1n}</description></item><item><title>Matrix Boundaries in Computational Topology</title><link>https://freshrimpsushi.github.io/en/posts/2405/</link><pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2405/</guid><description>Definition 1 Given a simplicial complex $K$, let us denote the number of $p$-simplices as $n_{p}$ and the number of $P-1$-simplices as $n_{p-1}$. Assume that the $p-1$th $i$-simplex of dimension $(p-1)$ is a face $a_{i}^{j} = 1$ of the $j$th $p$-simplex and otherwise is $a_{i}^{j} = 0$. The matrix $\partial_{p} := \left[ a_{i}^{j} \right]_{i = 1 , \cdots , n_{p-1}}^{j = 1 , \cdots , n_{p}}$ is called the Boundary</description></item><item><title>Optimization Theory: Method of Lagrange Multipliers</title><link>https://freshrimpsushi.github.io/en/posts/2404/</link><pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2404/</guid><description>⚡ 이 포스트는 패스트 트랙Fast Track으로 작성되었습니다. Description In nonlinear optimization problems that have a nonlinear objective function, a method called Lagrangian Multiplier Method involves multiplying constraints by something called Lagrangian Multipliers and integrating them into the objective function. $$ \begin{matrix} \text{Maximize} &amp;amp; f(x) \\ \text{subject to}</description></item><item><title>줄리아에서 U-net 구현하기</title><link>https://freshrimpsushi.github.io/en/posts/3434/</link><pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3434/</guid><description>Overview This document introduces how to implement the U-Net presented in the paper &amp;ldquo;U-Net: Convolutional networks for Biomedical Image Segmentation&amp;rdquo; using Julia. Code The structure of U-Net is divided into two main parts: a contracting path where the encoder compresses the input data, and an expansive path where the compressed data is restored. In the image below, the left half is the contracting path, and the right is the expansive</description></item><item><title>The Definition of Chech Complexes</title><link>https://freshrimpsushi.github.io/en/posts/2403/</link><pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2403/</guid><description>Definition 1 2 Given a metric space $\left( X, d \right)$ and a positive number $\varepsilon &amp;gt; 0$, for a finite set $S \subset X$, the abstract simplicial complex $\check{C}_{\varepsilon} (S)$ defined as follows is called a Čech Complex. $$ \check{C}_{\varepsilon} (S) := \left\{ \sigma \subset S : \bigcap_{x \in \sigma} B_{d} \left( x ; \varepsilon \right) \ne \emptyset \right\} $$ Here, $B_{d} (c; r)$ is an</description></item><item><title>Impossibility of Cloning Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3433/</link><pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3433/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Support Vector Machine</title><link>https://freshrimpsushi.github.io/en/posts/2402/</link><pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2402/</guid><description>Model 1 Simple Definition The method of finding a Support Vector Machine is to find a line or plane that best separates binary classifiable data. Complex Definition For an inner product space $X = \mathbb{R}^{p}$ and labeling $Y = \left\{ -1, +1 \right\}$, let&amp;rsquo;s denote the Training Dataset composed of $n$ pieces of data as $D = \left\{ \left( \mathbf{x}_{k} , y_{k} \right) \right\}_{k=1}^{n} \subset X \times Y$, and $$</description></item><item><title>Solvay-Kitaev Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3432/</link><pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3432/</guid><description>Theorem1 Let&amp;rsquo;s consider $\mathcal{G}$ as a finite subset of the special unitary group $\operatorname{SU}(2)$ that is closed under taking inverses. $$ \mathcal{G} \subset \operatorname{SU}(2), \qquad g \in \mathcal{G} \implies g^{-1} \in \mathcal{G} $$ Then, the free group $\braket{\mathcal{G}}$ generated by $\mathcal{G}$ is dense in $\operatorname{SU}(2)$. Specialization2 By composing the Hadamard gate $H$, phase gate $R_{\pi/4}$, and $\operatorname{CNOT}_{q}$gate, any quantum gate can be approximated as closely as desired. Description In classical</description></item><item><title>Definition of Vietoris-Rips Complex</title><link>https://freshrimpsushi.github.io/en/posts/2401/</link><pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2401/</guid><description>Definition 1 2 Simple Definition Let us assume a Euclidean space $\left( \mathbb{R}^{d} , \left\| \cdot \right\|_{2} \right)$ and a positive number $\varepsilon &amp;gt; 0$ are given. For a finite set $S \subset \mathbb{R}^{d}$, a simplicial complex $\text{VR}_{\varepsilon} (S)$ that satisfies the following two conditions is called a Vietoris-Rips Complex. (i): It has $S$ as the set of vertices. (ii): A simplex $\left[ v_{0} , v_{1} , \cdots, v_{k} \right]$</description></item><item><title>Quantum Fredkin/CSWAP Gate</title><link>https://freshrimpsushi.github.io/en/posts/3431/</link><pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3431/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Definition of a General Convex Function</title><link>https://freshrimpsushi.github.io/en/posts/2400/</link><pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2400/</guid><description>Definitions A function $f : V \to \mathbb{R}$ defined in a vector space $V$ is called a convex function if it satisfies the following for all $\mathbf{x}, \mathbf{y} \in V$ and all $t \in [0,1]$. $$ f \left( (1-t) \mathbf{x} + t \mathbf{y} \right) \le (1-t) f \left( \mathbf{x} \right) + t f \left( \mathbf{y} \right) $$ Explanation In fact, once you study mathematics beyond the undergraduate level, you stop</description></item><item><title>Quantum Topology/CCNOT Gate</title><link>https://freshrimpsushi.github.io/en/posts/3430/</link><pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3430/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Betti Number of Homology group</title><link>https://freshrimpsushi.github.io/en/posts/2399/</link><pubDate>Sun, 11 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2399/</guid><description>Overview Without considering the geometric meaning, if we just define it plainly, in Algebraic Topology, the Betti Number is merely the rank of the homology group in a chain complex. The problem is that such an explanation does not help those curious about the meaning of Betti numbers, and it&amp;rsquo;s also difficult to learn through examples because the specific calculation is daunting. In this post, we introduce a theorem that</description></item><item><title>Exchange Gate</title><link>https://freshrimpsushi.github.io/en/posts/3429/</link><pubDate>Sat, 10 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3429/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Distributed Computing in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2398/</link><pubDate>Fri, 09 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2398/</guid><description>Overview In Julia, this introduces how to schedule computations across multiple devices1. Honestly, I&amp;rsquo;m not quite sure myself. Code using Distributed ip_ = [] for last in [160,161,162,163,164,32,33,34,35,36,43,44,45,46,47] push!(ip_, join([155,230,211,last],&amp;#39;.&amp;#39;)) end sort!(ip_) for ip in ip_ addprocs([(&amp;#34;chaos@&amp;#34; * ip, 8)]; dir =&amp;#34;/home/chaos&amp;#34;, exename = &amp;#34;julia&amp;#34;) #add slave node\&amp;#39;s workers println(&amp;#34;ip $ip&amp;#34; * &amp;#34; passed&amp;#34;) end nworkers() @everywhere function f(n) return n^2 - n end A = pmap(f,1:20000) X = []</description></item><item><title>Quantum CNOT Gate</title><link>https://freshrimpsushi.github.io/en/posts/3428/</link><pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3428/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Proof of the Fundamental Theorem of Finitely generated Abelian groups</title><link>https://freshrimpsushi.github.io/en/posts/2397/</link><pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2397/</guid><description>Theorem A finitely generated free group $G$ that is an Abelian group, and let $T \subset G$ be the torsion subgroup of $G$. (1): There exists a free Abelian group $H \subset G$ with finite rank $\beta \ge 0$ satisfying the following: $$ G = H \oplus T $$ (2): Let $T_{i}$ be a finite cyclic group of order $t_{i} &amp;gt; 1$. Then there exists $T_{1} , \cdots , T_{k}$</description></item><item><title>Pauli Gates</title><link>https://freshrimpsushi.github.io/en/posts/3427/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3427/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Euclidean Graph</title><link>https://freshrimpsushi.github.io/en/posts/2396/</link><pubDate>Mon, 05 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2396/</guid><description>Definitions 1 Simple Definition Let us assume that we have a finite subset $V \subset \mathbb{R}^{p}$ of the Euclidean space and a cutoff $\delta \ge 0$. A Euclidean Graph is defined as a graph that connects an edge between two points $u,v \in V$ only when the distance between them is less than $\delta$, with $V$ as vertices. Complex Definition For a finite subset $V \subset \mathbb{R}^{p}$ of the Euclidean</description></item><item><title>Topological Gates</title><link>https://freshrimpsushi.github.io/en/posts/3426/</link><pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3426/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Homomorphism Smith Normal Form</title><link>https://freshrimpsushi.github.io/en/posts/2395/</link><pubDate>Sat, 03 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2395/</guid><description>Theorem 1 Let free Abelian group $G$ and $G'$ have bases $a_{1} , \cdots , a_{n}$ and $a_{1}' , \cdots , a_{m}'$, respectively. If function $f : G \to G'$ is a homomorphism, then there exists a unique set of integers $\left\{ \lambda_{ij} \right\} \subset \mathbb{Z}$ satisfying the following. $$ f \left( a_{j} \right) = \sum_{i=1}^{m} \lambda_{ij} a_{i}' $$ The matrix $\left( \lambda_{ij} \right) \in \mathbb{Z}^{m \times n}$ is referred</description></item><item><title>Hadamard Gate</title><link>https://freshrimpsushi.github.io/en/posts/3425/</link><pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3425/</guid><description>English Translation 양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양</description></item><item><title>Julia's Multidimensional Indices</title><link>https://freshrimpsushi.github.io/en/posts/2394/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2394/</guid><description>Overview Julia provides a type of index that can reference multi-dimensional arrays, known as CatesianIndex1. Naturally, the naming Catesian comes from the Cartesian product, which is the product of sets. Code julia&amp;gt; M = rand(0:9, 4,4) 4×4 Matrix{Int64}: 9 3 7 0 8 6 2 1 3 8 4 9 5 6 8 2 For example, let&amp;rsquo;s assume you want to access the element 9, which is in</description></item><item><title>Quantum Gates and Quantum Circuits</title><link>https://freshrimpsushi.github.io/en/posts/3424/</link><pubDate>Wed, 31 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3424/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Definition of Torsion Subgroups</title><link>https://freshrimpsushi.github.io/en/posts/2393/</link><pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2393/</guid><description>Definition 1 Let $G$ be an Abelian group. If $g \in G$ satisfies $ng = 0$ for some $n \in \mathbb{N}$, then $g$ is said to have a Finite Order. If the set $T \subset G$ of all elements of $G$ with finite order is a subgroup of $G$, then $T$ is called the Torsion Subgroup of $G$. If the torsion subgroup of $G$, $T$, virtually Vanishes, meaning $T =</description></item><item><title>Qubits: The Basic Unit of Information in Quantum Computers</title><link>https://freshrimpsushi.github.io/en/posts/3423/</link><pubDate>Mon, 29 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3423/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Julia's Short Circuit</title><link>https://freshrimpsushi.github.io/en/posts/2392/</link><pubDate>Sun, 28 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2392/</guid><description>Overview In Julia, &amp;amp;&amp;amp; and || not only perform logical AND and OR operations but also execute short-circuit evaluation1. For instance, A &amp;amp;&amp;amp; B returns true only if both A and B are true, but in reality, if A is false, there is no need to check whether B is true or false; A &amp;amp;&amp;amp; B is false. Short-circuit evaluation essentially skips checking B. Skipping the calculation for B can</description></item><item><title>Bit: Unit of Information Classical Computer</title><link>https://freshrimpsushi.github.io/en/posts/3422/</link><pubDate>Sat, 27 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3422/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Free groups and Their Subgroups</title><link>https://freshrimpsushi.github.io/en/posts/2391/</link><pubDate>Fri, 26 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2391/</guid><description>Theorem Let $F$ be a free Abelian group. [1]: Every subgroup $R$ of $F$ is a free group. [2]: If $F$ has rank $n$, then the subgroup $R \subset F$ of $F$ is a free Abelian group with rank $r \le n$. [3]: Moreover, there exists a basis $e_{1} , \cdots , e_{n}\in F$ and natural numbers $t_{1} , \cdots , t_{k}$ for $F$ that satisfy the following three conditions:</description></item><item><title>Julia's find functions</title><link>https://freshrimpsushi.github.io/en/posts/2390/</link><pubDate>Wed, 24 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2390/</guid><description>Overview Julia&amp;rsquo;s basic built-in functions are increasingly useful the more you know them. Without further ado, let&amp;rsquo;s learn through examples. Code x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4] argmin(x) argmax(x) findmin(x) findmax(x) extrema(x) findfirst(x .== 3) findlast(x .== 3) findall(x .== 3) findnext(x .== 3, 5) findprev(x .== 3, 5) Optimal solutions argmin(),argmax(),findmin(),findmax(),extrema() Finding the optimal solutions. x = [3, 7, 4, 5, 10, 3,</description></item><item><title>Proof of the Existence of Smith Normal Form</title><link>https://freshrimpsushi.github.io/en/posts/2389/</link><pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2389/</guid><description>Algorithm $R$ is a principal ideal domain, for every matrix $A \in R^{m \times n}$, there exists a unique Smith normal form. In other words, for the matrix $A \in R^{m \times n}$, there exists $d_{1} , \cdots , d_{r} \in R$ and invertible matrices $P \in R^{m \times m}$, $Q \in R^{n \times n}$ that satisfy $$ PAQ = \begin{bmatrix} d_{1} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots</description></item><item><title>Matrix Representation of Tensor Product</title><link>https://freshrimpsushi.github.io/en/posts/3419/</link><pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3419/</guid><description>Buildup1 Choose bases $\mathcal{V}, {\mathcal{V}}^{\prime}$ respectively for the finite-dimensional vector spaces $V, V^{\prime}$. Then, there exists a matrix equivalent to the linear transformation $\phi : V \to V^{\prime}$, called its matrix representation $\phi$. Now assume we have the finite-dimensional vector space $V, V^{\prime}, W, W^{\prime}$ and its ordered basis $\mathcal{V}, {\mathcal{V}}^{\prime}, \mathcal{W}, {\mathcal{W}}^{\prime}$, as well as two linear transformations $\phi : V \to V^{\prime}$ and $\psi : W \to W^{\prime}$.</description></item><item><title>Julia's Exclamation Point Convention</title><link>https://freshrimpsushi.github.io/en/posts/2388/</link><pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2388/</guid><description>Overview 1 In Julia, appending an exclamation mark ! at the very end of a function name is referred to as the bang convention. Such functions are characterized by modifying the arguments they are given. Code function add_1!(x) x .+= 1 return x end foo = [2,5,-1] add_1!(foo) foo For example, executing the code above yields the following result. julia&amp;gt; foo = [2,5,-1] 3-element Vector{Int64}: 2 5 -1 julia&amp;gt; add_1!(foo)</description></item><item><title>Kronecker Product of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3418/</link><pubDate>Fri, 19 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3418/</guid><description>Definition1 The Kronecker product of two matrices $A = [a_{ij}] \in M_{m \times n}$, $B \in M_{p \times q}$ is defined as follows. $$ A \otimes B := \begin{bmatrix} a_{11} B &amp;amp; \cdots &amp;amp; a_{1n} B \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} B &amp;amp; \cdots &amp;amp; a_{mn} B \end{bmatrix} \in M_{mp \times nq} $$ Explanation The matrix representation of the tensor product of two linear transformations is defined</description></item><item><title>Smith Normal Form of a Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2387/</link><pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2387/</guid><description>Definition 1 2 Given a PID, or Principal Ideal Domain $R$, if there exist $d_{1} , \cdots , d_{r} \in R$ and invertible matrices $P \in R^{m \times m}$, $Q \in R^{n \times n}$ that satisfy the following for a matrix $A \in R^{m \times n}$, then $PAQ \in R^{m \times n}$ is called the Smith Normal Form of $A$. $$ PAQ = \begin{bmatrix} d_{1} &amp;amp; 0 &amp;amp; 0 &amp;amp;</description></item><item><title>Tensor Product of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3417/</link><pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3417/</guid><description>Buildup1 Given finite-dimensional vector spaces $V_{1}, V_{2}, W_{1}, W_{2}$ and linear transformations $\phi_{1} : V_{1} \to W_{1}$, $\phi_{2} : V_{2} \to W_{2}$, one can consider the following bilinear transformation. $$ V_{1} \times V_{2} \to W_{1} \otimes W_{2} $$ $$ (v_{1}, v_{2}) \mapsto \phi_{1}(v_{1}) \otimes \phi_{2}(v_{2}) $$ $\phi_{1}, \phi_{2}$ is a linear transformation, and it is easy to see that this function is bilinear due to the definition of product vectors.</description></item><item><title>Bezout's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2386/</link><pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2386/</guid><description>Definition In integral domain $D$, the following equation is called the Bézout&amp;rsquo;s identity. $$ m a + n b = \gcd \left( a, b \right) $$ If there exists $m,n \in D$ satisfying the Bézout&amp;rsquo;s identity for all $a, b \in D$, then $D$</description></item><item><title>Universal Properties of Tensor Products</title><link>https://freshrimpsushi.github.io/en/posts/3416/</link><pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3416/</guid><description>Buildup1 Given a finite-dimensional vector space $V_{1}, \dots, V_{r}$. If $n_{i} = \dim V_{i}$, and we select a basis for each vector space, we obtain the following coordinate vector as a bijective function $f_{i}$. $$ \begin{align*} f _{i}: &amp;amp; V_{i} \to \mathbb{C}^{n_{i}} \\ &amp;amp; v_{i} \mapsto (a_{i1}, \dots, a_{i n_{i}}) \end{align*} $$ From this, the following multilinear transformation $f$ is naturally defined. $$ \begin{align*} f : V_{1} \times \cdots \times</description></item><item><title>Welzl Algorithm: Solution to Smallest Enclosing Disk Problem</title><link>https://freshrimpsushi.github.io/en/posts/2385/</link><pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2385/</guid><description>Problem Smallest Enclosing Disk Let&amp;rsquo;s denote it as $n &amp;gt; d$. In a $d$-dimensional Euclidean space, given a finite set $P = \left\{ p_{k} \right\}_{k=1}^{n} \subset \mathbb{R}^{d}$, the following optimization problem is referred to as the Smallest Enclosing Disk Problem: $$ \begin{matrix} \text{Minimize} &amp;amp; r \ge 0 \\ \text{subject to} &amp;amp; \left\| c - p_{k} \right\|_{2} \le r \end{matrix} \\ c \in \mathbb{R}^{d} , k = 1, \cdots , n</description></item><item><title>Tensor Product of Product Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3415/</link><pubDate>Sat, 13 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3415/</guid><description>Buildup For convenience, we will develop the concept in the complex number space $\mathbb{C}$, but $\mathbb{R}$ or any vector space is also applicable. Let&amp;rsquo;s denote the set of functions from the finite set $\Gamma$ to the complex number space as indicated by $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ When $\Gamma = \mathbf{n} = \left\{ 1, \dots, n \right\}$, it essentially becomes $\mathbb{C}^{\mathbf{n}} = \mathbb{C}^{n}$,</description></item><item><title>How to Quickly Reference Subarrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2384/</link><pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2384/</guid><description>Overview In Julia, view is a data structure that quickly refers to a subarray of an array, making it seem cumbersome from a user&amp;rsquo;s perspective, although there might seem to be no difference. However, it returns a lighter array as it is lazily referenced. Therefore, in Julia code that is optimized even at a very basic level, it is easy to find the macro @views. Code Let&amp;rsquo;s refer to a</description></item><item><title>Tensor Product of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3414/</link><pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3414/</guid><description>Buildup1 For convenience, this exposition is developed for the complex number space $\mathbb{C}$, but it is applicable to $\mathbb{R}$ or any vector space as well. Let&amp;rsquo;s denote the set of functions from a finite set $\Gamma$ to the complex number space as described by $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ Let&amp;rsquo;s set $\Gamma$ as $\mathbf{n} = \left\{ 1, 2, \dots, n \right\}$. A function</description></item><item><title>Definition of Simplicial Homology group</title><link>https://freshrimpsushi.github.io/en/posts/2383/</link><pubDate>Wed, 10 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2383/</guid><description>Buildup Despite the complexity of the content, I made sure to leave detailed calculations and explanations to make it as understandable as possible. If you&amp;rsquo;re interested in homology, I highly recommend reading this. Consider a topological space $X$ of interest, represented through a $\Delta$-complex structure according to a specific simplicial complex. As a small example, in the image on the right, the torus represents $X$, and the left side corresponds</description></item><item><title>Shadow and Injection</title><link>https://freshrimpsushi.github.io/en/posts/3413/</link><pubDate>Tue, 09 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3413/</guid><description>Definition1 For $n \in \mathbb{N}$ and $0 \le i \le n$, the following function $p_{i}$ $$ \begin{align*} p_{i} : &amp;amp;\left\{ 0, 1 \right\}^{n+1} \to \left\{ 0, 1 \right\}^{n} \\ &amp;amp; (a_{0}, \dots, a_{n}) \mapsto (a_{0}, \dots, a_{i-1}, a_{i+1}, \dots, a_{n}) \end{align*} $$ is called a projection. The following two functions $\imath_{i}$, $\jmath_{i}$ $$ \begin{align*} \imath : &amp;amp;\left\{ 0, 1 \right\}^{n} \to \left\{ 0, 1 \right\}^{n+1} \\ &amp;amp; (a_{0}, \dots, a_{n-1})</description></item><item><title>Julia's Broadcasting Syntax</title><link>https://freshrimpsushi.github.io/en/posts/2382/</link><pubDate>Mon, 08 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2382/</guid><description>Overview Broadcasting is one of the most important concepts in Julia, offering a convenient syntax for writing vectorized code1. It is used by placing a dot . before a binary operation or after a function. This represents the application of a function in a pointwise manner, which is a perfect expression of its purpose. From a programming perspective, broadcasting can be viewed as a simplification of using Map in Map</description></item><item><title>Fredkin/CSWAP Gate</title><link>https://freshrimpsushi.github.io/en/posts/3412/</link><pubDate>Sun, 07 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3412/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Definition of a Delta-Complex</title><link>https://freshrimpsushi.github.io/en/posts/2381/</link><pubDate>Sat, 06 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2381/</guid><description>Definition 1 Definition of a simplex: The $n$-simplex $\Delta^{n}$ is called the convex hull of affinely independent $v_{0}, v_{1} , \cdots , v_{n} \in \mathbb{R}^{n+1}$, and the vectors $v_{k}$ are called Vertices. Formally, it is as follows. $$ \Delta^{n} := \left\{ \sum_{k} t_{k} v_{k} : v_{k} \in \mathbb{R}^{n+1} , t_{k} \ge 0 , \sum_{k} t_{k} = 1 \right\} $$ The $n-1$-simplexes $\Delta^{n-1}$ created by removing a vertex from $\Delta^{n}$ are</description></item><item><title>Toffoli/CCNOT Gate</title><link>https://freshrimpsushi.github.io/en/posts/3411/</link><pubDate>Fri, 05 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3411/</guid><description>Definition1 The following vector-valued Boolean function is called a Toffoli gate. $$ T : \left\{ 0, 1 \right\}^{3} \to \left\{ 0, 1 \right\}^{3} $$ $$ T (a, b, c) = (a, b, (a \land b) \oplus c) $$ The $\text{CCNOT}$ gate is also known as. Description In the Toffoli gate, if the first two inputs are both $1$, the third input is inverted. In all other cases, the input and</description></item><item><title>Programming with Map and Reduce</title><link>https://freshrimpsushi.github.io/en/posts/2380/</link><pubDate>Thu, 04 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2380/</guid><description>Definitions Map Let us denote a set of functions as $f : X \to Y$. The higher-order function defined as follows, $\text{map}$, is called a map. $$ \text{map} : F \times 2^{X} \to 2^{Y} \\ \text{map} \left( f , \left\{ x_{1} , \cdots , x_{n} \right\} \right) := \left\{ f \left( x_{1} \right) , \cdots, f \left( x_{n} \right) \right\} $$ Fold Let us denote a set of functions as</description></item><item><title>Controlled NOT(CNOT) Gate</title><link>https://freshrimpsushi.github.io/en/posts/3410/</link><pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3410/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Definition of Simplicial Complexes</title><link>https://freshrimpsushi.github.io/en/posts/2379/</link><pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2379/</guid><description>Definition Difficult Definition 1 $$ \Delta^{k} \in K $$ A complex is called a Simplicial Complex if a finite set of simplices $K$ satisfies the following two conditions: (i): If $\sigma \in K$ and $\tau$ is a face of $\sigma$, then $\tau \in K$. $$ \sigma \in K \land \tau \le \sigma \implies \tau \in K $$ (ii): If $\sigma_{1}, \sigma_{2} \in K$, then $\sigma_{1} \cap \sigma_{2}$ is either an</description></item><item><title>Replication Function</title><link>https://freshrimpsushi.github.io/en/posts/3409/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3409/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Higher-Order Functions in Programming</title><link>https://freshrimpsushi.github.io/en/posts/2378/</link><pubDate>Sun, 30 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2378/</guid><description>Definition In programming, when a function is a first-class object, the function that takes or returns a function is called a Higher-order Function. Conversely, a function that is not a higher-order function is called a First-order Function. The functions referred to in this definition are not the functions discussed in mathematics, but functions in computer science, that is, functions that might have side effects. Explanation Mathematics&amp;rsquo; Functionals Occasionally, there&amp;rsquo;s mention</description></item><item><title>What is a Functionally Complete Set?</title><link>https://freshrimpsushi.github.io/en/posts/3408/</link><pubDate>Sat, 29 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3408/</guid><description>Definition1 Let&amp;rsquo;s assume that a set of Boolean functions $\left\{ f_{k} \right\} = \left\{ f_{k} : \left\{ 0, 1 \right\}^{n_{k}} \to \left\{ 0, 1 \right\} \right\}_{k\in \Gamma}$ is given. $\Gamma$ is a finite set. If any Boolean function $$ \left\{ 0, 1 \right\}^{n} \to \left\{ 0, 1 \right\}\quad (n \in \mathbb{N}) $$ can be expressed by the compositions of $\left\{ f_{k} \right\}$, then the set $\left\{ f_{k} \right\}$ is called</description></item><item><title>What is a Torus in Mathematics?</title><link>https://freshrimpsushi.github.io/en/posts/2377/</link><pubDate>Fri, 28 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2377/</guid><description>Definition The square of the sphere $1$, denoted as $S^{1} \times S^{1} = [0,1] \times [0,1]$, and the quotient space $T$ that is homeomorphic to it according to the map shown above, is called a Torus. In the figure, the donut shape on the far right is an example of a torus. Description The torus is a space - more specifically, a figure - that is treated very preciously throughout</description></item><item><title>NOR Gate</title><link>https://freshrimpsushi.github.io/en/posts/3407/</link><pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3407/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Definition of CW Complex</title><link>https://freshrimpsushi.github.io/en/posts/2376/</link><pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2376/</guid><description>Overview 1 CW complexes are a type of complex also known as Cell Complexes, constructed through the following recursive procedure. Definition A discrete set $X^{0} \ne \emptyset$ is considered as a $0$-cell. A $n$-skeleton $X^{n}$ is created by attaching $n$-cells $e_{\alpha}^{n}$ into $\phi_{\alpha} : S^{n-1} \to X^{n-1}$ from $X^{n-1}$. When $X := \bigcup_{n \in \mathbb{N}} X^{n}$ becomes a topological space with a weak topology, $X$ is called a cell complex.</description></item><item><title>NAND Gate</title><link>https://freshrimpsushi.github.io/en/posts/3406/</link><pubDate>Tue, 25 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3406/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Creating Dictionaries from Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2375/</link><pubDate>Mon, 24 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2375/</guid><description>Code 1 julia&amp;gt; Dict([&amp;#34;a&amp;#34;, &amp;#34;bc&amp;#34;] .=&amp;gt; [2,8]) Dict{String, Int64} with 2 entries: &amp;#34;a&amp;#34; =&amp;gt; 2 &amp;#34;bc&amp;#34; =&amp;gt; 8 Given two arrays you want to use as keys and values, you can create a dictionary using Dict(Key .=&amp;gt; Value). Essentially, it&amp;rsquo;s nothing more than broadcasting the =&amp;gt; operator to create pairs. Environment OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/create-a-dictionary-from-arrays-of-keys-and-values/13908/3&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Exclusive Disjuction, XOR Gate</title><link>https://freshrimpsushi.github.io/en/posts/3405/</link><pubDate>Sun, 23 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3405/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>What is a Complex in Topology?</title><link>https://freshrimpsushi.github.io/en/posts/2374/</link><pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2374/</guid><description>Overview In mathematics, the term &amp;ldquo;Complex&amp;rdquo; usually refers to complex numbers, but in geometry or topology, &amp;ldquo;Complex&amp;rdquo; signifies something like the following terms. Terminology A Complex is made up of topologically simple $S$s, whose intersections are of a lower dimension but of the same kind as $S$. Description As this ambiguous expression suggests, there isn&amp;rsquo;t exactly a &amp;lsquo;definition&amp;rsquo;. Whether we call the simple things a Simplex or refer to Complex</description></item><item><title>Negation, NOT Gate</title><link>https://freshrimpsushi.github.io/en/posts/3404/</link><pubDate>Fri, 21 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3404/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>How to Use Complex Numbers in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2373/</link><pubDate>Thu, 20 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2373/</guid><description>Overview In Julia, complex numbers are natively supported, similar to R. Code Imaginary unit im julia&amp;gt; z = 3 + 4im 3 + 4im im represents the pure imaginary unit $i = \sqrt{-1}$. All the common arithmetic operations that we are familiar with can be used. julia&amp;gt; typeof(z) Complex{Int64} julia&amp;gt; typeof(3.0 + 4.0im) ComplexF64 (alias for Complex{Float64}) When checking the type, even though it&amp;rsquo;s the same complex number, the type</description></item><item><title>Disjunction, OR Gate</title><link>https://freshrimpsushi.github.io/en/posts/3403/</link><pubDate>Wed, 19 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3403/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Topology in Mathematics: Discs and Spheres</title><link>https://freshrimpsushi.github.io/en/posts/2372/</link><pubDate>Tue, 18 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2372/</guid><description>Definition 1 In the Euclidean space $\left( \mathbb{R}^{n} , \left\| \cdot \right\| \right)$, the following shapes are defined. Defined as $D^{n} \subset \mathbb{R}^{n}$, this is called $n$-Unit Disk. $$ D^{n} := \left\{ \mathbf{x} \in \mathbb{R}^{n} : \left\| \mathbf{x} \right\| \le 1 \right\} $$ Defined as $S^{n} \subset \mathbb{R}^{n+1}$, this is called $n$-Unit Sphere. $$ S^{n} := \left\{ \mathbf{x} \in \mathbb{R}^{n+1} : \left\| \mathbf{x} \right\| = 1 \right\} $$ An open</description></item><item><title>Conjunction, AND Gate</title><link>https://freshrimpsushi.github.io/en/posts/3402/</link><pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3402/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Introduction to Network Data by Mark Newman</title><link>https://freshrimpsushi.github.io/en/posts/2371/</link><pubDate>Sun, 16 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2371/</guid><description>Introduction Mark Newman is a physicist renowned for his significant contributions to the fields of complex systems and networks. His reputation in these areas is well deserved and absolutely prestigious. Links You can access the famous Mark Newman&amp;rsquo;s network data sets here. There are 23 types of networks related to research published in papers available for download. Fileset download: http://www-personal.umich.edu/~mejn/netdata/ Requirements There are no requirements; downloads are available without limitation.</description></item><item><title>Boolean Functions</title><link>https://freshrimpsushi.github.io/en/posts/3401/</link><pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3401/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Definition of Weak Topology</title><link>https://freshrimpsushi.github.io/en/posts/2370/</link><pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2370/</guid><description>Definition 1 Let $X$ be a set with two topologies $\mathscr{T}_{1}$, $\mathscr{T}_{2}$. If $\mathscr{T}_{1} \subset \mathscr{T}_{2}$, then $\mathscr{T}_{1}$ is said to be weaker than $\mathscr{T}_{2}$, and $\mathscr{T}_{2}$ is said to be stronger than $\mathscr{T}_{1}$. Consider the set $\mathscr{F} := \left\{ f_{\alpha} : X \hookrightarrow X_{\alpha} , \alpha \in \mathscr{A} \right\}$ of injections from the set $X$ to the topological space $X_{\alpha}$. $$ \mathscr{S} := \left\{ f_{\alpha}^{-1} \left( O_{\alpha} \right) \subset</description></item><item><title>How to Check the Device on which the PyTorch Model/Tensor is loaded</title><link>https://freshrimpsushi.github.io/en/posts/3364/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3364/</guid><description>Code1 2 It can be checked with get_device(). &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import torch.nn as nn &amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available() True &amp;gt;&amp;gt;&amp;gt; Device = torch.device(&amp;#34;cuda:0&amp;#34; if torch.cuda.is_available() else &amp;#34;cpu&amp;#34;) # Model &amp;gt;&amp;gt;&amp;gt; model = nn.Sequential(nn.Linear(5,10), nn.ReLU(), nn.Linear(10,10), nn.ReLU(), nn.Linear(10,1)) &amp;gt;&amp;gt;&amp;gt; next(model.parameters()).get_device() -1 &amp;gt;&amp;gt;&amp;gt; model.to(Device) Sequential( (0): Linear(in_features=5, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=10, bias=True) (3): ReLU() (4): Linear(in_features=10, out_features=1, bias=True) ) &amp;gt;&amp;gt;&amp;gt; next(model.parameters()).get_device() 0 # Tensor &amp;gt;&amp;gt;&amp;gt; A = torch.rand(5) &amp;gt;&amp;gt;&amp;gt;</description></item><item><title>Introduction to OpenFlights</title><link>https://freshrimpsushi.github.io/en/posts/2369/</link><pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2369/</guid><description>Introduction Open Flights provides data on airports and airline networks around the world. Although it requires some preprocessing, network data of this quality is rare, so let&amp;rsquo;s appreciate and use it wisely. The real issue, however, is that it has been left unupdated on GitHub since 2020, so be cautious. Requirements There are no requirements and the data can be downloaded unlimitedly. Data Example airports.dat routes.dat They&amp;rsquo;re not in csv</description></item><item><title>Lagrangian Motion Model</title><link>https://freshrimpsushi.github.io/en/posts/2368/</link><pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2368/</guid><description>Definition 1 The meta-population model that deals with short-term interactions between groups is called the Eulerian Movement Model. Example Let&amp;rsquo;s consider an extension of a simple SIR model. There are two patches, a home location $1$ and a vacation spot $2$, and the model that describes diseases that couldn&amp;rsquo;t possibly originate from the home location but are brought in from the vacation spot can be represented as a coupled dynamic</description></item><item><title>고전정보이론에서 정보량이란?</title><link>https://freshrimpsushi.github.io/en/posts/3398/</link><pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3398/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Introduction to the Stanford Network Analysis Project</title><link>https://freshrimpsushi.github.io/en/posts/2367/</link><pubDate>Sat, 08 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2367/</guid><description>Introduction SNAP (Stanford Network Analysis Project) is a network analysis/mining library maintained by Stanford University, offering network data sizable enough to be considered massive networks. For instance, the network created using Twitter includes 17,069,982 users as nodes, with 476,553,560 tweets as links. To be honest, there isn&amp;rsquo;t much data that would be useful for research or any practical applications, but it serves as an excellent practice for big data or</description></item><item><title>What is a Flag in Linear Algebra?</title><link>https://freshrimpsushi.github.io/en/posts/3397/</link><pubDate>Fri, 07 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3397/</guid><description>Definition1 2 $n$Dimension Vector space $V$Subspaces sequences $\left\{ W_{i} \right\}$ satisfying the following equations are termed flags. $$ \left\{ \mathbf{0} \right\} = W_{0} \lneq W_{1} \lneq W_{2} \lneq \cdots \lneq W_{k-1} \lneq W_{k} = V $$ By definition, the following holds. $$ 0 = \dim V_{0} \lt \dim V_{1} \lt \dim V_{2} \lt \cdots \lt \dim V_{k-1} \lt \dim V_{k} = n $$ Explanation The term flag is used because,</description></item><item><title>Euler's Motion Model</title><link>https://freshrimpsushi.github.io/en/posts/2366/</link><pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2366/</guid><description>Definition 1 The meta-population model that deals with the medium to long-term migration between groups is called the Eulerian Movement Model. Citron. (2021). Comparing metapopulation dynamics of infectious diseases under different models of human movement. https://doi.org/10.1073/pnas.2007488118&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>What is a Softplus Function?</title><link>https://freshrimpsushi.github.io/en/posts/3396/</link><pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3396/</guid><description>Definition1 The function defined below is called a softplus. $$ \zeta (x) = \ln (1 + e^{x}) $$ Ian Goodfellow, Deep Learning, p68&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Introduction to World Pop</title><link>https://freshrimpsushi.github.io/en/posts/2365/</link><pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2365/</guid><description>Introduction This site provides data on the global air network, migration statistics between countries, urbanization, and the age and gender composition. The data covered are in line with the site&amp;rsquo;s name, focusing on Population, offering exactly the kind of data needed for research. Requirements There are no specific requirements, allowing for unlimited downloads. Guide For example, the link to the Global Flight Data https://www.worldpop.org/geodata/summary?id=1287 has a download button that makes</description></item><item><title>Direct Sum of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3395/</link><pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3395/</guid><description>Definition1 The direct sum of two matrices $B \in M_{m\times n}$, $C \in M_{p\times q}$ is defined as matrix $A$ of the following $(m+p) \times (n+q)$, and is denoted by $B \oplus C$. $$ A = B \oplus C := \begin{bmatrix} b_{11} &amp;amp; \cdots &amp;amp; b_{1n} &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ b_{m1} &amp;amp; \cdots &amp;amp; b_{mn}</description></item><item><title>Metapopulation Model</title><link>https://freshrimpsushi.github.io/en/posts/2364/</link><pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2364/</guid><description>Glossary 1 A model describing multiple populations that are spatially separated is known as the Metapopulation Model. In this context, the separated spaces are also referred to as Patches. Description Meta? In general, in the scientific community, Meta is taken to mean &amp;lsquo;about itself&amp;rsquo;. For example, if the ability to understand some object is called cognitive ability, then understanding about one’s cognitive ability is called metacognitive</description></item><item><title>Zero Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3394/</link><pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3394/</guid><description>Definition The matrix of size $m\times n$ with all elements being $0$ is called a zero matrix, and is denoted as $O_{m\times n}$ or simply as $O$. Description Other notations include $Z_{m \times n}$, $Z$, $\mathbf{0}_{m\times n}$, or $\mathbf{0}$. It is better to write the number $0$ in bold to avoid confusion (actually, it is better just to use $O$). A zero matrix is the identity element for matrix addition.</description></item><item><title>SEES:lab Introduction</title><link>https://freshrimpsushi.github.io/en/posts/2363/</link><pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2363/</guid><description>Introduction The name of the laboratory, SEES, is derived from the initials of Science and Engineering of Emerging Systems. The data on networks such as airports, emails, etc., is neatly organized. Most of the content is related to actual research, which ensures reliability and the papers provide good detail. The primary focus of the research is on complex networks, but there seems to be an interest in drugs and bio</description></item><item><title>Direct Sum of Invariant Subspaces and Its Characteristic Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/3393/</link><pubDate>Thu, 30 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3393/</guid><description>Theorem1 Let $T : V \to V$ be a linear transformation on a finite-dimensional vector space $V$ above. Assume that $V$ is the direct sum of the $T$-invariant subspaces $W_{i}$. $$ V = W_{1} \oplus W_{2} \oplus \cdots \oplus W_{k} $$ Let $f_{i}(t)$ be the characteristic polynomial of the restriction $T|_{W_{i}}$. Then, the characteristic polynomial of $T$, $f(t)$, is as follows. $$ f(t) = f_{1}(t) \cdot f_{2}(t) \cdot \cdots \cdot</description></item><item><title>Solving Linear Programming Problems with R</title><link>https://freshrimpsushi.github.io/en/posts/2362/</link><pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2362/</guid><description>Overview You can use the lpSolve package1. It is used by inputting $A, \mathbf{b}, \mathbf{c}$ of the Linear Programming Problem expressed in matrix form. Code $$ \begin{matrix} \text{Maximize} &amp;amp; &amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} \\ \text{subject to} &amp;amp;-&amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} &amp;amp; \le &amp;amp; 1 \\ &amp;amp; &amp;amp; x_{1} &amp;amp; &amp;amp; &amp;amp; \le &amp;amp; 3 \\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; x_{2} &amp;amp; \le &amp;amp; 2 \end{matrix} $$</description></item><item><title>Types of Cross Sections in Medical Imaging</title><link>https://freshrimpsushi.github.io/en/posts/3392/</link><pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3392/</guid><description>Overview The planes dividing the human body can be categorized into three types based on their orientation: coronal plane, sagittal plane, and axial plane. Distinguishing these planes is crucial, especially in medical imaging, as the information derived from an image can significantly vary depending on the plane it is associated with. Description In the descriptions below, it is assumed that the human body is standing on the ground. Coronal Plane</description></item><item><title>Introduction to Our World in Data</title><link>https://freshrimpsushi.github.io/en/posts/2361/</link><pubDate>Mon, 27 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2361/</guid><description>Introduction Our World in Data offers, as the site name suggests, hundreds of types of data on population, age, economy, politics, energy, gender, diseases, and more by country and year without any charge. It is the best in the world in terms of diversity and volume. A downside, if it can be considered a downside, is that most data is only provided on an annual basis. Those seeking more detailed</description></item><item><title>Cayley-Hamilton Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3391/</link><pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3391/</guid><description>Definition1 Let $T : V \to V$ be a linear transformation on a finite-dimensional vector space $V$. Let $f(t)$ be the characteristic polynomial of $T$. Then, the following holds: $$ f(T) = T_{0} $$ Here, $T_{0}$ is the zero transformation. In other words, a linear transformation satisfies its own characteristic polynomial. Rewriting this theorem from the perspective of matrices, Corollary Square matrices satisfy their own characteristic equations. $$ f(A) =</description></item><item><title>Solving Linear Programming Problems with MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/2360/</link><pubDate>Sat, 25 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2360/</guid><description>Overview You can use the Optimization Toolbox1. Insert the $A, \mathbf{b}, \mathbf{c}$ in matrix form for the linear programming problem. Code $$ \begin{matrix} \text{Maximize} &amp;amp; &amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} \\ \text{subject to} &amp;amp;-&amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} &amp;amp; \le &amp;amp; 1 \\ &amp;amp; &amp;amp; x_{1} &amp;amp; &amp;amp; &amp;amp; \le &amp;amp; 3 \\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; x_{2} &amp;amp; \le &amp;amp; 2 \end{matrix} $$ As a simple example,</description></item><item><title>Structural Medical Imaging and Functional Medical Imaging</title><link>https://freshrimpsushi.github.io/en/posts/3390/</link><pubDate>Fri, 24 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3390/</guid><description>Structural Imaging Photos taken to obtain structural/anatomical information inside the human body are called structural imaging. They are taken to check for lesions such as fractures or tumors. Usually, when people mention taking photos at a hospital, it refers to this. Functional Imaging Photos taken to obtain functional information about the body are called functional imaging. Here, functional does not relate to functions or [functionals] as mentioned in mathematics, but</description></item><item><title>Chi-Square Distribution's Sufficient Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2359/</link><pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2359/</guid><description>Theorem Let&amp;rsquo;s assume we have a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \chi^{2} (r)$ that follows a chi-squared distribution. The sufficient statistic $T$ for $r$ is as follows. $$ T = \left( \prod_{i} X_{i} \right) $$ Proof Relationship between gamma distribution and chi-squared distribution: $$ \Gamma \left( { r \over 2 } , 2 \right) \iff \chi ^2 (r) $$ Sufficient statistic for the</description></item><item><title>Cyclic Subspaces of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3389/</link><pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3389/</guid><description>Definition1 Let us consider $T : V \to V$ as a linear transformation on the vector space $V$. Suppose $\mathbf{v} \ne \mathbf{0} \in V$. The following subspace $$ W = \span\left( \left\{ \mathbf{v}, T\mathbf{v}, T^{2}\mathbf{v}, \dots \right\} \right) $$ is called the $V$ $T$-cyclic subspace generated by $\mathbf{v}$. Description The $T$-cyclic subspace is trivially a $T$-invariant subspace. Also, it is the smallest $T$-invariant subspace that includes $\mathbf{v}$. Theorem1 Let $T</description></item><item><title>Solving Linear Programming Problems with Python</title><link>https://freshrimpsushi.github.io/en/posts/2358/</link><pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2358/</guid><description>Overview You can use the scipy package1. Input the linear programming problem expressed in matrix form, as in $A, \mathbf{b}, \mathbf{c}$. Code $$ \begin{matrix} \text{Maximize} &amp;amp; &amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} \\ \text{subject to} &amp;amp;-&amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} &amp;amp; \le &amp;amp; 1 \\ &amp;amp; &amp;amp; x_{1} &amp;amp; &amp;amp; &amp;amp; \le &amp;amp; 3 \\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; x_{2} &amp;amp; \le &amp;amp; 2 \end{matrix} $$ As a simple</description></item><item><title>Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3388/</link><pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3388/</guid><description>Definition1 A matrix whose elements are all $1$ is called an $1$ matrix. Explanation The sushi restaurant owner majored in physics, so he preferred to call it an $1$ matrix because &amp;ldquo;work matrix&amp;rdquo; sounded technical. There doesn&amp;rsquo;t seem to be a rigid convention, and absolutely $I$, $E$ or $O$ should be avoided. If notation is necessary, perhaps $\mathbf{1}_{m\times n}$ would be better. Conceptually, it is almost never needed when studying</description></item><item><title>Sufficient Statistics for the Beta Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2357/</link><pubDate>Sun, 19 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2357/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \text{Beta} \left( \alpha, \beta \right)$ that follows a beta distribution, the sufficient statistic $T$ for $\left( \alpha, \beta \right)$ is as follows. $$ T = \left( \prod_{i} X_{i}, \prod_{i} \left( 1 - X_{i} \right) \right) $$ Proof $$ \begin{align*} f \left( \mathbf{x} ; \alpha, \beta \right) =&amp;amp; \prod_{k=1}^{n} f \left( x_{k} ; \alpha, \beta \right)</description></item><item><title>Null Space of Power Maps</title><link>https://freshrimpsushi.github.io/en/posts/3387/</link><pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3387/</guid><description>Theorem1 $n$ Let&amp;rsquo;s say that the linear transformation $T : V \to V$ on the dimension vector space is nilpotent. $$ T^{p} = T_{0} $$ Here, $T_{0}$ is the zero transformation. Let&amp;rsquo;s call $N(T)$ the null space of $T$. Then, the following holds: For all $i \in \mathbb{N}$, it is $N(T^{i}) \subset N(T^{i+1})$. For $1 \le i \le p-1$, there exists a sequence basis $N(T^{i})$ of $\beta_{i}$ such that the</description></item><item><title>Solving Linear Programming Problems with Julia</title><link>https://freshrimpsushi.github.io/en/posts/2356/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2356/</guid><description>Overview To solve optimization problems, one can use the JuMP package[^1]. JuMP stands for Julia Mathematical Programming. Compared to other languages, coding in Julia is almost like directly transcribing mathematical formulas, which is very intuitive. Code $$ \begin{matrix} \text{Maximize} &amp;amp; &amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} \\ \text{subject to} &amp;amp;-&amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} &amp;amp; \le &amp;amp; 1 \\ &amp;amp; &amp;amp; x_{1} &amp;amp; &amp;amp; &amp;amp; \le &amp;amp; 3 \\</description></item><item><title>Sufficient Statistics of the Gamma Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2355/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2355/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \Gamma \left( k, \theta \right)$ that follows the Gamma distribution. The sufficient statistic $T$ for $\left( k, \theta \right)$ is as follows. $$ T = \left( \prod_{i} X_{i}, \sum_{i} X_{i} \right) $$ Proof $$ \begin{align*} f \left( \mathbf{x} ; k, \theta \right) =&amp;amp; \prod_{k=1}^{n} f \left( x_{k} ; k, \theta \right) \\ =&amp;amp; \prod_{i=1}^{n} {{</description></item><item><title>Orthogonal Triangular Matrices are Nilpotent</title><link>https://freshrimpsushi.github.io/en/posts/3385/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3385/</guid><description>Theorem1 $n \times n$ Upper triangular matrix $A$ is a nilpotent matrix. Explanation The converse is not true. A simple counterexample is when $A = \begin{bmatrix} 1 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix}$, $$ A^{2} = \begin{bmatrix} 1 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix}\begin{bmatrix} 1 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix} = \begin{bmatrix} 0 &amp;amp; 0 \\ 0 &amp;amp; 0 \end{bmatrix} $$ The method of proof is</description></item><item><title>Solving Linear Programming Problems with Excel</title><link>https://freshrimpsushi.github.io/en/posts/2354/</link><pubDate>Mon, 13 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2354/</guid><description>Guide Step 1. Activate the Solver Add-in In the File/Options/Add-ins tab, click the Go(G) button next to Excel Add-ins in Manage(A). A window named Add-ins will pop up like below. Check the Solver Add-in and click OK. The Solver feature has been activated under the Data tab in Analysis. Step 2. Transcribe the Linear Programming Problem $$ \begin{matrix} \text{Maximize} &amp;amp; &amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} \\ \text{subject to} &amp;amp;-&amp;amp;</description></item><item><title>Fully Connected Layer (Linear Layer, Dense Layer)</title><link>https://freshrimpsushi.github.io/en/posts/3384/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3384/</guid><description>Definition Let&amp;rsquo;s refer to $L: \mathbb{R}^{n} \to \mathbb{R}^{m}$ as a layer. Consider $\mathbf{W}$ as the matrix representation of $L$. When $\mathbf{W}$ consists only of components that are not $0$, $L$ is called a fully connected layer. Explanation A fully connected layer is the most fundamental layer in an artificial neural network. In deep learning, most layers are linear; however, the term linear layer often refers to a fully connected layer.</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators of a Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2353/</link><pubDate>Sat, 11 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2353/</guid><description>Theorem A given random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim N \left( \mu , \sigma^{2} \right)$ follows a normal distribution. The sufficient statistic $T$ and maximum likelihood estimator $\left( \hat{\mu}, \widehat{\sigma^{2}} \right)$ for $\left( \mu, \sigma^{2} \right)$ are as follows: $$ \begin{align*} T =&amp;amp; \left( \sum_{k} X_{k}, \sum_{k} X_{k}^{2} \right) \\ \left( \hat{\mu}, \widehat{\sigma^{2}} \right) =&amp;amp; \left( {{ 1 } \over { n }} \sum_{k}</description></item><item><title>Proof of Strong Duality Theorem in Linear Programming</title><link>https://freshrimpsushi.github.io/en/posts/2352/</link><pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2352/</guid><description>Theorem $$ \begin{align*} \text{Maximize} &amp;amp; \sum_{j=1}^{n} c_{j} x_{j} &amp;amp; \text{(Primal)} \\ \text{subject to} &amp;amp; \sum_{j=1}^{n} a_{ij} x_{j} \le b_{i} &amp;amp; i = 1 ,\cdots , m \\ &amp;amp; x_{j} \ge 0 &amp;amp; j = 1, \cdots , n \end{align*} $$ $$ \begin{align*} \text{Minimize} &amp;amp; \sum_{i=1}^{m} b_{i} y_{i} &amp;amp; \text{(Dual)} \\ \text{subject to} &amp;amp; \sum_{i=1}^{m} y_{i} a_{ij} \ge c_{j} &amp;amp; j = 1 ,\cdots , n \\ &amp;amp; y_{i} \ge 0</description></item><item><title>Training/Validation/Test Sets in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3382/</link><pubDate>Wed, 08 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3382/</guid><description>Definitions During training, data sets used are: The data set used to optimize the model&amp;rsquo;s parameters is called the training set. The data set used to optimize the model&amp;rsquo;s hyperparameters is called the validation set. After training, the data set used: The data set used to evaluate the model&amp;rsquo;s performance after training is called the test set. The function value of the loss function for the training/validation/test sets is called</description></item><item><title>The Eigenvalues of the Null Matrix are Only Zero</title><link>https://freshrimpsushi.github.io/en/posts/3381/</link><pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3381/</guid><description>Theorem1 English Translation: Let&amp;rsquo;s consider $V$ as a finite-dimensional vector space, and $T : V \to V$ as a nilpotent linear transformation. Then, the eigenvalues of $T$ are exclusively $0$. Japanese Translation: $V$を有限次元のベクトル空間、$T : V \to V$を冪零の線形変換としよう。する</description></item><item><title>Proof of Weak Duality Theorem in Linear Programming</title><link>https://freshrimpsushi.github.io/en/posts/2350/</link><pubDate>Sun, 05 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2350/</guid><description>Theorem $$ \begin{align*} \text{Maximize} &amp;amp; \sum_{j=1}^{n} c_{j} x_{j} &amp;amp; \text{(Primal)} \\ \text{subject to} &amp;amp; \sum_{j=1}^{n} a_{ij} x_{j} \le b_{i} &amp;amp; i = 1 ,\cdots , m \\ &amp;amp; x_{j} \ge 0 &amp;amp; j = 1, \cdots , n \end{align*} $$ $$ \begin{align*} \text{Minimize} &amp;amp; \sum_{i=1}^{m} b_{i} y_{i} &amp;amp; \text{(Dual)} \\ \text{subject to} &amp;amp; \sum_{i=1}^{m} y_{i} a_{ij} \ge c_{j} &amp;amp; j = 1 ,\cdots , n \\ &amp;amp; y_{i} \ge 0</description></item><item><title>Encoder and Decoder in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3380/</link><pubDate>Sat, 04 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3380/</guid><description>Definition Suppose a dataset $X \subset \mathbb{R}^{n}$ is given. In the context of machine learning, an encoder is defined as a function for an appropriate set $Z \subset \mathbb{R}^{m}$ ($m \le n$) such as the following: $$ \begin{align*} f : X &amp;amp;\to Z \\ \mathbf{x} &amp;amp;\mapsto \mathbf{z} = f(\mathbf{x}) \end{align*} $$ The following $g$ is called a decoder: $$ \begin{align*} g : Z &amp;amp;\to X \\ \mathbf{z} &amp;amp;\mapsto \mathbf{x} =</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators of Exponential Distributions</title><link>https://freshrimpsushi.github.io/en/posts/2349/</link><pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2349/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \exp \left( \lambda \right)$ that follows an exponential distribution. The sufficient statistic $T$ and maximum likelihood estimator $\hat{\lambda}$ for $\lambda$ are as follows. $$ \begin{align*} T =&amp;amp; \sum_{k=1}^{n} X_{k} \\ \hat{\lambda} =&amp;amp; {{ n } \over { \sum_{k=1}^{n} X_{k} }} \end{align*} $$ Proof Sufficient Statistic $$ \begin{align*} f \left( \mathbf{x} ; \lambda \right) =&amp;amp; \prod_{k=1}^{n}</description></item><item><title>Power Series Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3379/</link><pubDate>Thu, 02 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3379/</guid><description>Definition1 A linear transformation $T : V \to V$ on a vector space $V$ is called nilpotent if there exists a positive number $k$ such that $T^{k} = T_{0}$, where $T_{0}$ is the zero transformation. Stephen H. Friedberg, Linear Algebra (4th Edition, 2002), p512&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Linear Programming Duality</title><link>https://freshrimpsushi.github.io/en/posts/2348/</link><pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2348/</guid><description>Buildup For $x_{1} , x_{2} \ge 0$, let&amp;rsquo;s say we have the following linear programming problem. $$ \begin{matrix} \text{Maximize} &amp;amp; &amp;amp; 2x_{1} &amp;amp; + &amp;amp; 3x_{2} \\ \text{subject to} &amp;amp; &amp;amp; 4x_{1} &amp;amp; + &amp;amp; 8x_{2} &amp;amp; \le &amp;amp; 12 \\ &amp;amp; &amp;amp; 2x_{1} &amp;amp; + &amp;amp; x_{2} &amp;amp; \le &amp;amp; 3 \\ &amp;amp; &amp;amp; 3x_{1} &amp;amp; + &amp;amp; 2x_{2} &amp;amp; \le &amp;amp; 4 \end{matrix} $$ Our goal is to</description></item><item><title>Spring 2025 Omakase: Your Name</title><link>https://freshrimpsushi.github.io/en/posts/3378/</link><pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3378/</guid><description>Introduction In Korea, the new academic year starts in March, so around this time, you have many new names to learn. Therefore, we have prepared a menu related to names today. Menu Trigonometric Functions Trigonometric functions are some of the most encountered functions when studying STEM fields. However, not many may be well-versed with their names. As I recall, I first learned about them in middle school, but perhaps because</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators of the Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2347/</link><pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2347/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \text{Poi} \left( \lambda \right)$ following a Poisson distribution, the sufficient statistic $T$ and the maximum likelihood estimator $\hat{\lambda}$ for $\lambda$ are as follows: $$ \begin{align*} T =&amp;amp; \sum_{k=1}^{n} X_{k} \\ \hat{\lambda} =&amp;amp; {{ 1 } \over { n }} \sum_{k=1}^{n} X_{k} \end{align*} $$ Proof Sufficient Statistic $$ \begin{align*} f \left( \mathbf{x} ; \lambda \right) =&amp;amp;</description></item><item><title>Diagonalizability of Linear Transformations, Multiplicity of Eigenvalues, and the Relationship with Eigenspaces</title><link>https://freshrimpsushi.github.io/en/posts/3377/</link><pubDate>Sun, 26 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3377/</guid><description>Theorem1 Let $T : V \to V$ be a linear transformation on a finite-dimensional vector space $V$. Suppose the characteristic polynomial of $T$ splits and $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$ are distinct eigenvalues of $T$. Then, $T$ is diagonalizable if and only if, for all $i$, the multiplicity of $\lambda_{i}$ and the dimension $\dim(E_{\lambda_{i}})$ of the eigenspace are the same. $$ T \text{ is diagobalizable. } \iff \text{multiplicity of } \lambda_{i}</description></item><item><title>Linear Programming: Proof of the Fundamental Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2346/</link><pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2346/</guid><description>Theorem For a linear programming problem in the form of equation form, one of the following three is true: (1): If an optimal solution does not exist, then the problem is inherently infeasible or unbounded. (2): If a feasible solution exists, then a feasible basic solution also exists. (3): If an optimal solution exists, then an optimal basic solution also exists. Proof Strategy: Given that it&amp;rsquo;s named the Fundamental Theorem,</description></item><item><title>How to Change Axis Style in Julia Plots `framestyle`e`</title><link>https://freshrimpsushi.github.io/en/posts/3376/</link><pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3376/</guid><description>Overview1 The framestyle attribute allows changing the style of the plot&amp;rsquo;s axes and border. The possible options are as follows: :box :semi :axes :origin :zerolines :grid :none Code The default setting is :axes. ▷code1◁ The styles for each attribute are as follows. ▷code2◁ https://docs.juliaplots.org/latest/generated/attributes_subplot/&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators for the Geometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2345/</link><pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2345/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \text{Geo} \left( p \right)$ that follows a geometric distribution, the sufficient statistic $T$ and the maximum likelihood estimator $\hat{p}$ for $p$ are as follows. $$ \begin{align*} T =&amp;amp; \sum_{k=1}^{n} X_{k} \\ \hat{p} =&amp;amp; {{ n } \over { \sum_{k=1}^{n} X_{k} }} \end{align*} $$ Proof Sufficient Statistic $$ \begin{align*} f \left( \mathbf{x} ; p \right) =&amp;amp;</description></item><item><title>The Creation of Unions is Equal to the Sum of Creations</title><link>https://freshrimpsushi.github.io/en/posts/3375/</link><pubDate>Wed, 22 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3375/</guid><description>Theorem1 Let $S_{1}, S_{2}$ be a subset of the vector space $V$. Then, the following holds. $$ \span(S_{1} \cup S_{2}) = \span(S_{1}) + \span(S_{2}) $$ Here, $\span$ means generation, and $+$ means the sum of sets. Proof $\span(S_{1} \cup S_{2}) \subset \span(S_{1}) + \span(S_{2})$ Let $v \in \span(S_{1} \cup S_{2})$. Then, $v$ can be expressed as follows: $$ v = \sum\limits_{i=1}^{n}a_{i}x_{i} + \sum\limits_{j=1}^{m}b_{j}y_{j},\quad x_{i}\in S_{1},\ y_{j} \in S_{2} $$ The</description></item><item><title>Simplex Method's Bland's Rule</title><link>https://freshrimpsushi.github.io/en/posts/2344/</link><pubDate>Tue, 21 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2344/</guid><description>Theorem A system of equations of the following form for Dictionary: $i = 1 , \cdots , m$ is called a Dictionary. $$ \begin{align*} \zeta &amp;amp;=&amp;amp; &amp;amp; &amp;amp; \sum_{j=1}^{n} c_{j} x_{j} \\ x_{n+i} &amp;amp;=&amp;amp; b_{i} &amp;amp;-&amp;amp; \sum_{j=1}^{n} a_{ij} x_{j} \end{align*} $$ Variables on the left side of $\zeta$, excluding the one variable, are called basic variables, and variables on the right side are called nonbasic variables. Their indices are denoted</description></item><item><title>The Coordinate Patch Mapping of a Torus in Three-Dimensional Space</title><link>https://freshrimpsushi.github.io/en/posts/3374/</link><pubDate>Mon, 20 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3374/</guid><description>Formula1 The coordinate chart mapping of a torus with a distance from the center to the tube of $R$ and the diameter of the tube of $r$ is as follows. $$ \mathbf{x}(u_{1}, u_{2}) = \left( (R + r\cos u_{2})\cos u_{1}, (R + r\cos u_{2})\sin u_{1}, r\sin u_{1} \right) $$ Where $(u_{1}, u_{2}) \in [0, 2\pi) \times [0, 2\pi)$. Code The Julia code to draw the above figure is as follows.</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators for the Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2343/</link><pubDate>Sun, 19 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2343/</guid><description>Theorem Let&amp;rsquo;s assume we have a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim U \left( 0 , \theta \right)$ following a uniform distribution. The sufficient statistic $T$ and maximum likelihood estimator $\hat{\theta}$ for $\theta$ are as follows: $$ \begin{align*} T =&amp;amp; \max_{k=1 , \cdots , n} X_{k} \\ \hat{\theta} =&amp;amp; \max_{k=1 , \cdots , n} X_{k} \end{align*} $$ Proof Strategy: The sufficient statistic and maximum</description></item><item><title>직합의 성질</title><link>https://freshrimpsushi.github.io/en/posts/3373/</link><pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3373/</guid><description>Theorem1 Let $W_{1}, W_{2}, \dots, W_{k}$ be subspaces of a finite-dimensional vector space $V$. The following propositions are equivalent: $V = W_{1} \oplus W_{2} \oplus \cdots \oplus W_{k}$ It is $V = \sum\limits_{i=1}^{k}W_{i}$, and for any vectors $v_{i} \in W_{i}(1 \le i \le k)$, if $v_{1} + \cdots v_{k} = 0$, then for all $i$, $v_{i} = 0$ holds. All $v \in V$ can be uniquely expressed in the form</description></item><item><title>Simplex Method Cycling</title><link>https://freshrimpsushi.github.io/en/posts/2342/</link><pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2342/</guid><description>Definition $$ \begin{matrix} \text{Maximize} &amp;amp; \mathbf{c}^{T} \mathbf{x} \\ \text{subject to} &amp;amp; A \mathbf{x} = \mathbf{b} \\ &amp;amp; \mathbf{x} \ge \mathbf{0} \end{matrix} $$ For the matrix $A \in \mathbb{R}^{m \times n}$, $\mathbf{b} \in \mathbb{R}^{m \times 1}$, and $\mathbf{c} \in \mathbb{R}^{n}$, let us say the linear programming problem is expressed in the equation form as above, and for $i = 1 , \cdots , m$, let us represent its dictionary as follows.</description></item><item><title>Hyperbolic Functions Composite Formula</title><link>https://freshrimpsushi.github.io/en/posts/3372/</link><pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3372/</guid><description>Formulas The following holds true. $$ c_{1} \cosh x + c_{2} \sinh x = \begin{cases} A\cosh(x + y_{1}) &amp;amp; \text{if } c_{1} \gt c_{2} \\ B e^{x} &amp;amp; \text{if } c_{1} = c_{2} = B \\ C\sinh(x + y_{2}) &amp;amp; \text{if } c_{1} \lt c_{2} \end{cases} $$ $A = \sqrt{c_{2}^{2} - c_{1}^{2}}$ $C = \sqrt{c_{2}^{2} - c_{1}^{2}}$ $y_{1} = \cosh^{-1}\left( \dfrac{c_{1}}{\sqrt{c_{1}^{2} - c_{2}^{2}}} \right) = \sinh^{-1} \left( \dfrac{c_{2}}{\sqrt{c_{1}^{2} - c_{2}^{2}}}</description></item><item><title>How to Write Conditional Statements Concisely in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2341/</link><pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2341/</guid><description>Overview In Julia, &amp;lt;condition&amp;gt; &amp;amp;&amp;amp; &amp;lt;statement&amp;gt; executes &amp;lt;statement&amp;gt; when &amp;lt;condition&amp;gt; is true. As a function, it returns the result of &amp;lt;statement&amp;gt; if true, and if false, &amp;lt;statement&amp;gt; is not even evaluated. While it allows writing code efficiently and concisely, it may reduce readability. Moreover, even if you don&amp;rsquo;t use it frequently, you should understand it to read the code written by others. Without any context, encountering such syntax can be</description></item><item><title>Sum of Subspaces in a Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3371/</link><pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3371/</guid><description>Definition1 Let&amp;rsquo;s refer to $W_{1}, W_{2}$ as a subspace of the vector space $V$. The sum of $W_{1}$ and $W_{2}$ is denoted as $W_{1} + W_{2}$ and defined as follows. $$ W_{1} + W_{2} := \left\{ x + y : x\in W_{1}, y \in W_{2} \right\} $$ Generalization2 Let $W_{1}, W_{2}, \dots, W_{k}$ be a subspace of the vector space $V$. The sum of these subspaces is denoted as $W_{1}</description></item><item><title>Infinity of the Objective Function in Linear Programming</title><link>https://freshrimpsushi.github.io/en/posts/2340/</link><pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2340/</guid><description>Description $$ \begin{matrix} \text{Maximize} &amp;amp; \mathbf{c}^{T} \mathbf{x} \\ \text{subject to} &amp;amp; A \mathbf{x} = \mathbf{b} \\ &amp;amp; \mathbf{x} \ge \mathbf{0} \end{matrix} $$ Given matrix $A \in \mathbb{R}^{m \times n}$, $\mathbf{b} \in \mathbb{R}^{m \times 1}$, and $\mathbf{c} \in \mathbb{R}^{n}$, let&amp;rsquo;s say a linear programming problem is represented in an equation form as above. Despite following the constraints, the objective function can become unbounded. A picture is worth a thousand words1. Geometrically</description></item><item><title>Sum and Difference Identities for Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/3370/</link><pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3370/</guid><description>Formulas Composition into sine $$ A \cos \theta + B \sin \theta = C\sin(\theta + \phi) $$ Here, $C = \sqrt{A^{2} + B^{2}}$, $\phi = \sin^{-1} \left( \dfrac{A}{\sqrt{A^{2} + B^{2}}} \right) = \cos^{-1} \left( \dfrac{B}{\sqrt{A^{2} + B^{2}}} \right)$ are given. Composition into cosine $$ A \cos \theta + B \sin \theta = C\cos(\theta - \phi) $$ Here, $C = \sqrt{A^{2} + B^{2}}$, $\phi = \sin^{-1} \left( \dfrac{B}{\sqrt{A^{2} + B^{2}}} \right)</description></item><item><title>Transformations on the Quotient Space of Diagonalizable Linear Transformations are also Diagonalizable</title><link>https://freshrimpsushi.github.io/en/posts/3369/</link><pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3369/</guid><description>Theorem1 Let $V$ be a dimension vector space, $T : V \to V$ be a linear transformation, and $W$ be an $T$-invariant subspace. If $T$ is diagonalizable, then $\overline{T} : V/W \to V/W$ is also diagonalizable. In this case, $V/W$ is the quotient space of $V$. Proof If $T$ is diagonalizable, so is $T|_{W}$, there exists a basis $\gamma = \left\{ v_{1}, v_{2}, \dots, v_{k} \right\}$ of $W$ such that</description></item><item><title>Initialization and Auxiliary Problem in Simplex Method</title><link>https://freshrimpsushi.github.io/en/posts/2338/</link><pubDate>Thu, 09 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2338/</guid><description>Buildup $$ \begin{matrix} \text{Maximize} &amp;amp; \mathbf{c}^{T} \mathbf{x} \\ \text{subject to} &amp;amp; A \mathbf{x} = \mathbf{b} \\ &amp;amp; \mathbf{x} \ge \mathbf{0} \end{matrix} $$ Let&amp;rsquo;s say the linear programming problem is represented in the equation form as above with respect to matrices $A \in \mathbb{R}^{m \times n}$, $\mathbf{b} \in \mathbb{R}^{m \times 1}$, and $\mathbf{c} \in \mathbb{R}^{n}$. And for all $j = 1 , \cdots , n+m$, $x_{k} \ge 0$ holds, and for</description></item><item><title>Gaussian Curvature with Negative Values on Rotational Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3368/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3368/</guid><description>Overview1 The document explains rotation surfaces with negative Gaussian curvature. Description The curvature of the rotation surface is $K = - \dfrac{r^{\prime \prime}}{r}$, therefore $r^{\prime \prime} - a^{2}r = 0$. The solution of this differential equation is as follows: $$ r(s) = c_{1} \cosh(as) + c_{2}\sinh(as) $$ This can be written for some appropriate constant $B, b, C, c \in \mathbb{R}$ as follows: $$ r(s) = c_{1} \cosh (as) +</description></item><item><title>Unimodal Distribution's Shortest Confidence Interval</title><link>https://freshrimpsushi.github.io/en/posts/2337/</link><pubDate>Tue, 07 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2337/</guid><description>Theorem Definition of Unimodal Functions If there exists a mode $x^{\ast}$ such that the function $f : \mathbb{R} \to \mathbb{R}$ does not decrease in $x \le x^{\ast}$ and does not increase in $x \ge x^{\ast}$, then $f$ is called unimodal. Especially, if the probability density function of $f$ is unimodal, we call that probability distribution a unimodal distribution. Shortest Confidence Interval Let&amp;rsquo;s consider $f(x)$ to be a unimodal probability density</description></item><item><title>Characteristics of the Mapping into the Quotient Space via Linear Transformations between Polynomial Relations</title><link>https://freshrimpsushi.github.io/en/posts/3367/</link><pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3367/</guid><description>Theorem1 Let $V$ be a $n$-dimensional vector space, $T : V \to V$ a linear transformation, $W \le V$ a $T$-invariant subspace, $T|_{W}$ a contraction mapping, and $\overline{T}$ a linear transformation on the quotient space. $$ T|_{W} : W \to W \\ \overline{T} : V/W \to V/W $$ Let $f(t), g(t), h(t)$ be the characteristic polynomial of $T, T|_{W}, \overline{T}$, respectively. Then, the following holds. $$ f(t) = g(t)h(t) $$</description></item><item><title>Linear Programming: The Simplex Method</title><link>https://freshrimpsushi.github.io/en/posts/2336/</link><pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2336/</guid><description>Buildup 1 Consider the following Linear Programming Problem for $x_{1} , x_{2} \ge 0$. $$ \begin{matrix} \text{Maximize} &amp;amp; &amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} \\ \text{subject to} &amp;amp;-&amp;amp; x_{1} &amp;amp; + &amp;amp; x_{2} &amp;amp; \le &amp;amp; 1 \\ &amp;amp; &amp;amp; x_{1} &amp;amp; &amp;amp; &amp;amp; \le &amp;amp; 3 \\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; x_{2} &amp;amp; \le &amp;amp; 2 \end{matrix} $$ In other words, we want to maximize $x_{1} + x_{2}$ while</description></item><item><title>Various Properties of Convex Functions</title><link>https://freshrimpsushi.github.io/en/posts/3366/</link><pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3366/</guid><description>Theorem1 All convex functions are continuous. If $f$ is an increasing convex function, and $g$ is a convex function, then $f \circ g$ is also a convex function. If $f$ is convex in $(a, b)$, and if $a \lt s \lt t \lt u \lt b$, then $$ \dfrac{f(t) - f(s)}{t-s} \le \dfrac{ f(u) - f(s) }{ u - s } \le \dfrac{ f(u) - f(t) }{ u - t</description></item><item><title>Stochastic Increment and Decrement Functions and Confidence Intervals</title><link>https://freshrimpsushi.github.io/en/posts/2335/</link><pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2335/</guid><description>Theorem 1 Definition of Stochastic Monotone Functions If the cumulative distribution function $F \left( t ; \theta \right)$ is a monotone (increasing or decreasing) function for $\theta$, it is called a Stochastic Increasing(Decreasing) Function. Pivoting a Continuous Cumulative Distribution Function Let&amp;rsquo;s say a statistic $T$ has a continuous cumulative distribution function $F_{T} \left( t ; \theta \right)$. For a fixed $\alpha \in (0,1)$, let $\alpha_{1} + \alpha_{2} = \alpha$, and</description></item><item><title>Linear Transformations on the Quotient Space</title><link>https://freshrimpsushi.github.io/en/posts/3365/</link><pubDate>Thu, 02 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3365/</guid><description>Definition 1 Let $V$ be a vector space and $T : V \to V$ be a linear transformation. Let $W \le V$ be a $T$-invariant subspace. The linear transformation on quotient space $\overline{T}$ is defined as follows: $$ \begin{align*} \overline{T} : V/W &amp;amp;\to V/W \\ v + W &amp;amp;\mapsto T(v) + W \end{align*} $$ Here, $V/W$ is the quotient space. Theorem (a) $\overline{T}$ is well-defined. (b) $\overline{T}$ is indeed a</description></item><item><title>Linear Programming: Dictionaries and Tableau</title><link>https://freshrimpsushi.github.io/en/posts/2334/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2334/</guid><description>Notation $$ \begin{matrix} \text{Maximize} &amp;amp; \mathbf{c}^{T} \mathbf{x} \\ \text{subject to} &amp;amp; A \mathbf{x} = \mathbf{b} \\ &amp;amp; \mathbf{x} \ge \mathbf{0} \end{matrix} $$ For the matrix $A \in \mathbb{R}^{m \times n}$, $\mathbf{b} \in \mathbb{R}^{m \times 1}$, and $\mathbf{c} \in \mathbb{R}^{n}$, it is said that the linear programming problem is represented in the form of equation form as above, and let&amp;rsquo;s denote its components as follows. $$ \begin{align*} A =&amp;amp; \left( a_{ij}</description></item><item><title>Shannon Entropy in Classical Information Theory</title><link>https://freshrimpsushi.github.io/en/posts/3400/</link><pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3400/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Most Accurate Confidence Set</title><link>https://freshrimpsushi.github.io/en/posts/2333/</link><pubDate>Mon, 30 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2333/</guid><description>Definition 1 For the hypothesis test for $\theta$, the confidence set of $1 - \alpha$ is called $C \left( \mathbf{x} \right)$, and let the acceptance region be $A \left( \theta \right) = C \left( \mathbf{x} \right)^{c}$. The probability of False Coverage for $P_{\theta} \left( \theta ' \in C \left( \mathbf{X} \right) \right)$ against $\theta ' \ne \theta$ is called. The original coverage probability of $P_{\theta} \left( \theta \in C \left(</description></item><item><title>Mapping to the Quotient Space</title><link>https://freshrimpsushi.github.io/en/posts/3363/</link><pubDate>Sun, 29 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3363/</guid><description>Theorem1 Let $V$ be a vector space, and $W \le V$ a subspace. Define the function $\eta$ as follows. $$ \begin{align*} \eta : V &amp;amp;\to V/W \\ v &amp;amp;\mapsto v + W \end{align*} $$ In this case, $V/W$ is the quotient space of $V$. Then $\eta$ is a linear transformation and its null space is $N(\eta) = W$. If $V$ is finite-dimensional, then $$ \begin{equation} \dim(W) + \dim(V/W) = \dim(V)</description></item><item><title>Hypothesis Testing and the One-to-One Correspondence of Confidence Sets</title><link>https://freshrimpsushi.github.io/en/posts/2332/</link><pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2332/</guid><description>Theorem Let&amp;rsquo;s assume we have parameter space $\Theta$ and space $\mathcal{X}$ given. For each $\theta_{0} \in \Theta$, let $A \left( \theta_{0} \right)$ be the rejection region $\alpha$ of the hypothesis test $H_{0} : \theta = \theta_{0}$. For each $\mathbf{x} \in \mathcal{X}$, let&amp;rsquo;s define the set $C \left( \mathbf{x} \right) \subset \Theta$ as follows: $$ C \left( \mathbf{x} \right) := \left\{ \theta_{0} : \mathbf{x} \in A \left( \theta_{0} \right) \right\} $$</description></item><item><title>What is ReLU in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/3362/</link><pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3362/</guid><description>Definition In machine learning, the following function is referred to as a Rectified Linear Unit. $$ f(x) = x^{+} := \max \left\{ 0, x \right\} $$ Description In electrical engineering, this is known as a ramp function. For a description and properties of the function itself, not from the perspective of its use as an activation function in machine learning, refer to the ramp function article. See Also Dirac delta</description></item><item><title>Definition of a Pivot in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2331/</link><pubDate>Thu, 26 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2331/</guid><description>Definition 1 A random variable $Q \left( \mathbf{X} ; \theta \right) := Q \left( X_{1} , \cdots , X_{n} ; \theta \right)$ whose probability distribution is independent of all parameters $\theta$ is called a pivot or pivotal quantity. Description Naturally, $Q$ is a statistic. The statement that the probability distribution is independent of all parameters $\theta$ means that the cumulative distribution function $F \left( \mathbf{x} ; \theta \right)$ of $Q</description></item><item><title>몫공간의 기저와 차원</title><link>https://freshrimpsushi.github.io/en/posts/3361/</link><pubDate>Wed, 25 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3361/</guid><description>Theorem1 Let $V$ be a $n$-dimensional vector space, and let $W \le V$ be a $k (\lt n)$-dimensional subspace. Let $\left\{ u_{1}, \dots, u_{k} \right\}$ be a basis of $W$. And let $\left\{ u_{1}, \dots, u_{k}, u_{k+1}, \dots, u_{n} \right\}$ be a basis of $V$ obtained by extending this basis. Then $\left\{ u_{k+1} + W, \dots, u_{n} + W \right\}$ is a basis of the quotient space $V/W$. $\dim(V) =</description></item><item><title>How to Replace NaN with 0 in Julia DataFrames</title><link>https://freshrimpsushi.github.io/en/posts/2330/</link><pubDate>Tue, 24 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2330/</guid><description>Overview The method of replacing with a specific value is inconvenient because it changes one column at a time, and when dealing with NaN throughout the dataframe, it seems more practical to use a better trick. Code julia&amp;gt; df = DataFrame(rand(1:9,3,3), :auto) ./ DataFrame(rand(0:1,3,3), :auto) 3×3 DataFrame Row │ x1 x2 x3 │ Float64 Float64 Float64 ─────┼──────</description></item><item><title>Ramp Function</title><link>https://freshrimpsushi.github.io/en/posts/3360/</link><pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3360/</guid><description>Definition The following function is called a ramp function. $$ R(x) := \begin{cases} x &amp;amp; x \gt 0 \\ 0 &amp;amp; x \le 0 \end{cases} $$ Various Definitions1 It can be defined in several ways as follows. $$ \begin{align*} R(x) &amp;amp;:= \begin{cases} x &amp;amp; x \gt 0 \\ 0 &amp;amp; x \le 0 \end{cases} \\[1em] &amp;amp;= \max \left\{ 0, x \right\} \\[1em] &amp;amp;= x H(x) \\[1em] &amp;amp;= \dfrac{x + \left|</description></item><item><title>Definition of a Mathematical-Statistical Confidence Set</title><link>https://freshrimpsushi.github.io/en/posts/2329/</link><pubDate>Sun, 22 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2329/</guid><description>Definition 1 The following is referred to as the coverage probability for the interval estimator $\left[ L \left( \mathbf{X} \right), U \left( \mathbf{X} \right) \right]$ of parameter $\theta$. $$ P_{\theta} \left( \theta \in \left[ L \left( \mathbf{X} \right), U \left( \mathbf{X} \right) \right] \right) = P \left( \theta \in \left[ L \left( \mathbf{X} \right), U \left( \mathbf{X} \right) \right] | \theta \right) $$ The infimum of the coverage probability is</description></item><item><title>Residual Classes and Quotient Spaces in Linear Algebra</title><link>https://freshrimpsushi.github.io/en/posts/3359/</link><pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3359/</guid><description>Definition1 $V$ is a $F$-vector space and $W \le V$ is a subspace. For $v \in V$, the set $$ \left\{ v \right\} + W := \left\{ v + w : w \in W \right\} $$ is called the coset of $W$ containing $v$ . The left-hand $+$ is the sum of sets. Explanation Often $\left\{ v \right\} + W$ is simply denoted by $v + W$. Let $\left\{ v</description></item><item><title>Julia's Ternary Operator ? :</title><link>https://freshrimpsushi.github.io/en/posts/2328/</link><pubDate>Fri, 20 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2328/</guid><description>Overview In Julia, A ? B : C is known as the Ternary Operator, which returns B if A is true and C otherwise. Just like binary operations are defined as functions in mathematics, the ternary operation is also a function. It&amp;rsquo;s similar to an if statement but has this fundamental difference, making it very useful once you&amp;rsquo;re accustomed to it. However, it can make the code less readable, so</description></item><item><title>Free Fall Motion</title><link>https://freshrimpsushi.github.io/en/posts/3358/</link><pubDate>Thu, 19 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3358/</guid><description>Definition When a body moves only under the influence of gravitational acceleration, without any other external force, this is called free fall motion.</description></item><item><title>Interval Estimator</title><link>https://freshrimpsushi.github.io/en/posts/2327/</link><pubDate>Wed, 18 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2327/</guid><description>Definition 1 For a parameter $\theta \in \mathbb{R}$, the ordered pair $\left( L \left( x_{1} , \cdots , x_{n} \right), U \left( x_{1} , \cdots , x_{n} \right) \right)$ is called an Interval Estimate if it satisfies $L \left( \mathbf{x} \right) \le U \left( \mathbf{x} \right)$ for all $\mathbf{x} \in \mathcal{X}$. The random interval $\left[ L \left( \mathbf{X} \right), U \left( \mathbf{X} \right) \right]$ is referred to as an Interval</description></item><item><title>대각화가능한 선형변환의 불변부분공간으로의 축소사상도 대각화가능하다</title><link>https://freshrimpsushi.github.io/en/posts/3357/</link><pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3357/</guid><description>Theorem1 Let $V$ be a vector space, and let $T : V \to V$ be a diagonalizable linear transformation. Suppose $\left\{ \mathbf{0} \right\} \ne W \le V$ denotes a non-trivial $T$-invariant subspace. Then the restriction map $T|_{W}$ is also diagonalizable. A trivial $T$-invariant subspace refers to the zero vector set $\left\{ \mathbf{0} \right\}$, the entire set $V$, the range $R(T)$, the null space $N(T)$, and the eigenspace $E_{\lambda}$. Proof Since</description></item><item><title>How to Change a Specific Value in a DataFrame in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2326/</link><pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2326/</guid><description>Overview To make replacements, use the replace!() method1. The first argument should be the column of the dataframe you want to change, and the second argument takes a pair A =&amp;gt; B. It&amp;rsquo;s important that it&amp;rsquo;s the dataframe&amp;rsquo;s column being specified here. Code julia&amp;gt; WJSN 10×4 DataFrame Row │ member birth height unit │ String Int64 Int64 String ─────┼───</description></item><item><title>Position, Velocity, and Acceleration</title><link>https://freshrimpsushi.github.io/en/posts/3356/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3356/</guid><description>Position Definition The function representing an object&amp;rsquo;s position is referred to as the position function, or simply position. Description In physics, since we consider the change of position over time (referred to as motion), position is a function of time, as shown by $x = x(t)$. When assuming a one-dimensional space, it is commonly denoted as $x$. In the case of two-dimensional or three-dimensional spaces, it is usually denoted in</description></item><item><title>SIRD Model: Death and Fatality Rate</title><link>https://freshrimpsushi.github.io/en/posts/2325/</link><pubDate>Sat, 14 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2325/</guid><description>Overview The SIRD model is an epidemiological compartment model that adds death to the SIR model. Model $$ \begin{align*} {{d S} \over {d t}} =&amp;amp; - {{ \beta } \over { N }} I S \\ {{d I} \over {d t}} =&amp;amp; {{ \beta } \over { N }} S I - \mu I \\ {{d R} \over {d t}} =&amp;amp; \left( 1 - \delta \right) \mu I \\ {{d</description></item><item><title>The Relationship between Invariant Subspaces and Eigenvectors</title><link>https://freshrimpsushi.github.io/en/posts/3355/</link><pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3355/</guid><description>Theorem1 Let $V$ be a $n$-dimension vector space, $T : V \to V$ a linear transformation, and $W \le V$ a $T$-invariant subspace. Let $v_{1}, \dots, v_{k}$ be $T$&amp;rsquo;s eigenvectors corresponding to distinct eigenvalues. If $v_{1} + \cdots + v_{k} \in W$, then for every $i$ we have $v_{i} \in W$. Explanation Since $W$ is a subspace, if $v_{i} \in W$ then $\sum_{i}v_{i} \in W$ holds. However, the converse does</description></item><item><title>How to Calculate Frequency in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2324/</link><pubDate>Thu, 12 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2324/</guid><description>Overview 1 Use the freqtable() function from the FreqTables.jl package. It provides a similar functionality to the freq() function in R. Code Arrays julia&amp;gt; compartment = rand([&amp;#39;S&amp;#39;,&amp;#39;I&amp;#39;,&amp;#39;R&amp;#39;], 1000); julia&amp;gt; freqtable(compartment) 3-element Named Vector{Int64} Dim1 │ ──────┼──── &amp;#39;I&amp;#39; │ 316 &amp;#39;R&amp;#39; │ 342 &amp;#39;S&amp;#39; │ 342 By inserting an array like shown above, it will count the frequency for each class.</description></item><item><title>Mass Dependent on Position: The Motion of Balls Connected by Chains</title><link>https://freshrimpsushi.github.io/en/posts/3354/</link><pubDate>Wed, 11 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3354/</guid><description>Overview1 Consider the case where a ball with a chain moves in the vertical direction. If the chain is long enough that the ball moves up and down, causing the length of the chain that is floating in the air to continuously change, then the total mass of the object changes according to the position of the ball. In other words, the mass of the object becomes dependent on its</description></item><item><title>SIRV Model: Vaccines and Breakthrough Infections</title><link>https://freshrimpsushi.github.io/en/posts/2323/</link><pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2323/</guid><description>Overview The SIRV model is an epidemiological compartment model that adds vaccination to the SIR model. Model $$ \begin{align*} {{d S} \over {d t}} =&amp;amp; - {{ \beta } \over { N }} I S - vS \\ {{d I} \over {d t}} =&amp;amp; {{ \beta } \over { N }} S I - \mu I \\ {{d R} \over {d t}} =&amp;amp; \mu I \\ {{d V} \over {d</description></item><item><title>Invariant Subspaces of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3353/</link><pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3353/</guid><description>Overview Let $\beta = v_{1}, \dots, v_{k}$ be the set of eigenvectors of the linear transformation $T : V \to V$. Then, it can be understood that $T$ maps $\span{\beta}$ to $\span{\beta}$. A subspace that maps itself to itself in this manner is defined as an invariant subspace. Definition1 Let $V$ be a vector space, and $T : V \to V$ a linear transformation. A subspace $W$ is called an</description></item><item><title>Reading Only Columns from a CSV File in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2322/</link><pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2322/</guid><description>Guide Let&amp;rsquo;s say we have an example.csv file like the one above. When loading it into a dataframe, sometimes we want to create an entirely empty dataframe that only retains the column names, without importing all the data. This is necessary in cases where an empty dataframe is needed. using CSV # Loading the dataframe with no rows df_empty = CSV.read(&amp;#34;example.csv&amp;#34;, DataFrame; limit = 0) In the resulting dataframe created</description></item><item><title>Reading and Writing GEXF Files in NetworkX</title><link>https://freshrimpsushi.github.io/en/posts/3352/</link><pubDate>Sat, 07 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3352/</guid><description>Explanation1 2 GEXF stands for Graph Exchange XML Format, a language for describing graph structures. Considering the explanation that it started together with the Gephi project, it seems to be designed to be easily handled by Gephi. Code Writing Let&amp;rsquo;s create the following graph with NetworkX. import networkx as nx from itertools import combinations &amp;gt;&amp;gt;&amp;gt; G = nx.Graph() &amp;gt;&amp;gt;&amp;gt; IVE = [&amp;#34;가을&amp;</description></item><item><title>SEIR Model: Latent Period and Incubation Period</title><link>https://freshrimpsushi.github.io/en/posts/2321/</link><pubDate>Fri, 06 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2321/</guid><description>Overview The SEIR model is an epidemiological compartment model that adds an exposed group to the SIR model. Model 1 2 $$ \begin{align*} {{d S} \over {d t}} =&amp;amp; - {{ \beta } \over { N }} I S \\ {{d E} \over {d t}} =&amp;amp; {{ \beta } \over { N }} S I - {{ 1 } \over { \tau }} E \\ {{d I} \over {d t}}</description></item><item><title>The Union of Linearly Independent Sets from Different Eigenspaces is Linearly Independent</title><link>https://freshrimpsushi.github.io/en/posts/3351/</link><pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3351/</guid><description>Theorem1 Let $V$ be a vector space, $T : V \to V$ a linear transformation, and $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$ and $T$ different eigenvalues of $T$. For each $i = 1, \dots, k$, let $S_{i}$ be a linearly independent subset of the eigenspace $E_{\lambda_{i}}$. Then, ▶Eq1◀ is a linearly independent subset of $V$. Proof Lemma Following the notation of the theorem, let $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$</description></item><item><title>How to View Data Frame Summaries in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2320/</link><pubDate>Wed, 04 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2320/</guid><description>Guide 1 using RDatasets iris = dataset(&amp;#34;datasets&amp;#34;, &amp;#34;iris&amp;#34;) describe(iris) describe() function can be used. Let&amp;rsquo;s summarize the iris data. julia&amp;gt; describe(iris) 5×7 DataFrame Row │ variable mean min median max nmissing eltype │ Symbol Union… Any Union… Any Int64 DataType ─────┼────────────</description></item><item><title>Julia's Graph Analysis Package Graphs.jl</title><link>https://freshrimpsushi.github.io/en/posts/3350/</link><pubDate>Tue, 03 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3350/</guid><description>Introduction Graphs.jl is a package for graph (network) analysis, similar to Python&amp;rsquo;s NetworkX. It was created by rebooting the LightGraphs.jl package. The goal of Graphs.jl is to offer a performant platform for network and graph analysis in Julia, following the example of libraries such as NetworkX in Python. The Graphs.jl project is a reboot of the LightGraphs.jl package (archived in October 2021), which remains available on GitHub at sbromberger/LightGraphs.jl. Code</description></item><item><title>Row-wise and Column-wise Scalar Multiplication of Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2319/</link><pubDate>Mon, 02 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2319/</guid><description>Theorem Given a diagonal matrix $D := \text{diag} \left( d_{1} , \cdots , d_{n} \right)$ and a matrix $D := \text{diag} \left( d_{1} , \cdots , d_{n} \right)$, the following holds. $$ \begin{align*} D A =&amp;amp; \begin{bmatrix} d_{1} a_{11} &amp;amp; d_{1} a_{12} &amp;amp; \cdots &amp;amp; d_{1} a_{1n} \\ d_{2} a_{21} &amp;amp; d_{2} a_{22} &amp;amp; \cdots &amp;amp; d_{2} a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ d_{n} a_{n1} &amp;amp;</description></item><item><title>Eigen Spaces of Linear Transformations and Geometric Multiplicity</title><link>https://freshrimpsushi.github.io/en/posts/3349/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3349/</guid><description>Definition1 Let&amp;rsquo;s define $V$ and $n$ as dimension vector spaces, $T : V \to V$ as linear transformation. Let&amp;rsquo;s also define $\lambda$ as the eigenvalue of $T$. The set defined as follows, $E_{\lambda}$, is called the eigenspace of $T$ corresponding to the eigenvalue $\lambda$. $$ E_{\lambda} = V_{\lambda} := \left\{ x \in V : Tx = \lambda x \right\} = N(T - \lambda I) $$ In this case, $N$ is</description></item><item><title>Julia's Categorical Array</title><link>https://freshrimpsushi.github.io/en/posts/2318/</link><pubDate>Sat, 31 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2318/</guid><description>Overview The CategoricalArrays.jl package in Julia serves a similar function to factor in R. Code julia&amp;gt; A = [&amp;#34;red&amp;#34;, &amp;#34;blue&amp;#34;, &amp;#34;red&amp;#34;, &amp;#34;green&amp;#34;] 4-element Vector{String}: &amp;#34;red&amp;#34; &amp;#34;blue&amp;#34; &amp;#34;red&amp;#34; &amp;#34;green&amp;#34; julia&amp;gt; B = categorical(A) 4-element CategoricalArray{String,1,UInt32}: &amp;#34;red&amp;#34; &amp;#34;blue&amp;#34; &amp;#34;red&amp;#34; &amp;#34;green&amp;#34; julia&amp;gt; levels(B) 3-element Vector{String}: &amp;#34;blue&amp;#34; &amp;#34;green&amp;#34; &amp;#34;red&amp;#34; categorical() The categorical() function allows for casting a regular array to a categorical array. levels() With the levels() function, one can view the categories. Naturally,</description></item><item><title>Graph (Network) Analysis Package NetworkX in Python</title><link>https://freshrimpsushi.github.io/en/posts/3348/</link><pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3348/</guid><description>Introduction NetworkX is a Python package for analyzing graphs (networks). Code Installation Enter the following in the terminal. #설치 &amp;gt; pip install networkx #버전 업데이트 &amp;gt; pip install --upgrade networkx Importing and Checking Version networkx is abbreviated as nx. &amp;gt;&amp;gt;&amp;gt; import networkx as nx &amp;gt;&amp;gt;&amp;gt; nx.__version__ &amp;#39;2.8.6&amp;#39; Graph Creation Create a null graph with the following code. &amp;gt;&amp;gt;&amp;gt; G = nx.Graph() &amp;gt;&amp;gt;&amp;gt; nx.info(G)</description></item><item><title>Definition of Simplex</title><link>https://freshrimpsushi.github.io/en/posts/2317/</link><pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2317/</guid><description>Definition 1 A $n$-simplex $\Delta^{n}$, whose convex hull consists of affinely independent $v_{0}, v_{1} , \cdots , v_{n} \in \mathbb{R}^{n+1}$, has vertices $v_{k}$. Formally, it is defined as follows: $$ \Delta^{n} := \left\{ \sum_{k} t_{k} v_{k} : v_{k} \in \mathbb{R}^{n+1} , t_{k} \ge 0 , \sum_{k} t_{k} = 1 \right\} $$ The faces of a $\Delta^{n}$ are the $n-1$-simplices $\Delta^{n-1}$ formed by removing a single vertex from $\Delta^{n}$. The boundary</description></item><item><title>Multiplicity of Eigenvalues of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3347/</link><pubDate>Wed, 28 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3347/</guid><description>Definition1 Let $V$ be a finite-dimensional vector space, and let $T : V \to V$ be a linear transformation. Let $f(t)$ be the characteristic polynomial of $T$, and let $\lambda$ be an eigenvalue of $T$. The highest power $k$ of the factor $(t - \lambda)^{k}$ in $f(t)$ is called the (algebraic) multiplicity of $\lambda$. Explanation Simply put, the multiplicity of an eigenvalue refers to how many times $\lambda$ is a</description></item><item><title>How to Load a Built-in Dataset in R Used in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2316/</link><pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2316/</guid><description>Guide Using the RDatasets.jl package should do the trick. The following is an example of how to load the simplest iris dataset. It includes a variety of datasets beyond the basic built-in ones, so make sure to check out GitHub1. julia&amp;gt; using RDatasets julia&amp;gt; iris = dataset(&amp;#34;datasets&amp;#34;, &amp;#34;iris&amp;#34;) 150×5 DataFrame Row │ SepalLength SepalWidth PetalLength PetalWidth Species │ Float64 Float64 Float64 Float64 Cat…</description></item><item><title>Graph (Network) Visualization and Analysis Program Gephi</title><link>https://freshrimpsushi.github.io/en/posts/3346/</link><pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3346/</guid><description>Introduction Gephi is an open-source, free program designed for graph (network) analysis, with a particular strength in visualization. It supports Windows, Mac, and Linux. Basic Usage Let&amp;rsquo;s create a graph using NetworkX and save it as a gexf file. &amp;gt;&amp;gt;&amp;gt; import networkx as nx &amp;gt;&amp;gt;&amp;gt; G = nx.gnm_random_graph(100,200) &amp;gt;&amp;gt;&amp;gt; nx.info(G) &amp;#39;Graph with 100 nodes and 200 edges&amp;#39; &amp;gt;&amp;gt;&amp;gt; nx.write_gexf(G, &amp;#39;graph.gexf&amp;#39;) When you select a gexf file, information about the graph</description></item><item><title>Definition of Affine Independence</title><link>https://freshrimpsushi.github.io/en/posts/2315/</link><pubDate>Sun, 25 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2315/</guid><description>Definition 1 A set of vectors $S := \left\{ v_{0} , v_{1}, \cdots , v_{n} \right\} \subset V$ is said to be Affinely Independent if the vectors in $S$ or $S$ itself are linearly independent. $$ v_{1} - v_{0} , v_{2} - v_{0} , \cdots , v_{n} - v_{0} $$ https://glossary.informs.org/ver2/mpgwiki/index.php?title=Affine_independence&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>The Characteristic Polynomial of a Diagonalizable Linear Transformation Is Factorizable</title><link>https://freshrimpsushi.github.io/en/posts/3345/</link><pubDate>Sat, 24 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3345/</guid><description>Definition1 The polynomial $P(F)$ being split over $F$ means that there exists a constant $c, a_{1}, \dots, a_{n} \in F$ that satisfies the following. $$ f(t) = c(t - a_{1})(t - a_{2})\cdots(t - a_{n}) $$ If the polynomial $f(t)$ that is decomposed is the characteristic polynomial of some linear transformation $T$ or matrix $A$, we say that $T$(or $A$) is decomposed. Explanation By definition, if the characteristic polynomial of $T:</description></item><item><title>Leslie age structure Model</title><link>https://freshrimpsushi.github.io/en/posts/2314/</link><pubDate>Fri, 23 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2314/</guid><description>Definition 1 Suppose there are $m \in \mathbb{N}$ stages of age in a population, and let&amp;rsquo;s represent the population size at stage $a \in \left\{ 1 , \cdots , m \right\}$ at time $t$ as $x_{a}(t)$. If we denote the average number of offspring produced by individuals at age stage $a$ as $b_{a}$, and the survival rate of individuals moving from age stage $a$ to $a+1$ as $s_{a}$, then it</description></item><item><title>Tsiolkovsky Rocket Equation</title><link>https://freshrimpsushi.github.io/en/posts/3344/</link><pubDate>Thu, 22 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3344/</guid><description>Formulas1 The equation depicting the velocity of a rocket ejecting fuel in one-dimensional space without external forces is known as the Tsiolkovsky rocket equation. $$ v = v_{0} + V \ln \dfrac{m_{0}}{m} $$ Here, $v$ represents the final velocity of the rocket, $v_{0}$ the initial velocity of the rocket, $V$ the relative ejection speed of the fuel to the rocket, $m$ the final mass of the rocket, and $m_{0}$ the</description></item><item><title>How to Check Package Versions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2313/</link><pubDate>Wed, 21 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2313/</guid><description>Guide For example, let&amp;rsquo;s check the version of the Plots.jl package. Press the ] key in the REPL to enter the package mode. Here, if you type status foo, you can check the version of the foo package as follows. Environment OS: Windows julia: v1.6.3</description></item><item><title>Polynomial Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3343/</link><pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3343/</guid><description>Definition1 Polynomial A polynomial with coefficients from a field $F$ is defined for a non-negative integer $n$ as the following form. $$ \begin{equation} f(x) = a_{n}x^{n} + a_{n-1}x^{n-1} + \cdots + a_{1}x + a_{0} \end{equation} $$ Here, each $a_{k} \in F$ is called the coefficient of $x^{k}$. If $a_{n}=a_{n-1}=\cdots=a_{0}=0$, then $f(x)$ is called a zero polynomial. The degree of a polynomial is the largest power of $x$ with a coefficient</description></item><item><title>Poker-Plank Equation Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2312/</link><pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2312/</guid><description>Theorem $$ d X_{t} = f \left( t, X_{t} \right) dt + g \left( t , X_{t} \right) d W_{t} \qquad , t \in \left[ t_{0} , T \right] $$ Given a stochastic differential equation as above, and let $F \in C_{0}^{\infty} \left( \mathbb{R} \right)$. Then, at time point $t$, the probability density function $p(t,x)$ of $X_{t}$ follows the next partial differential equation. $$ {{ \partial p(t,x) } \over {</description></item><item><title>Equations of Motion for a Variable Mass System</title><link>https://freshrimpsushi.github.io/en/posts/3342/</link><pubDate>Sun, 18 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3342/</guid><description>Overview1 In physics, mass is often treated as a fixed constant in many situations. However, there are many cases where this is not true. For example, a falling raindrop absorbs smaller droplets in the atmosphere, increasing its mass. A rocket accelerates by expelling gases from burning fuel, thereby reducing its mass as the fuel is consumed. Formulas An Object Moving with Increasing Mass Consider a case where an object moves</description></item><item><title>How to Check if an Array is Empty in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2311/</link><pubDate>Sat, 17 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2311/</guid><description>Overview Use the isempty() function. Code julia&amp;gt; isempty([]) true julia&amp;gt; isempty(Set()) true julia&amp;gt; isempty(&amp;#34;&amp;#34;) true Though it&amp;rsquo;s mentioned as an array in the title, it actually could be a set or a string. Optimization Of course, checking if an array is empty by seeing if length() is $0$ might be fine too. julia&amp;gt; @time for t in 1:10^6 isempty([]) end 0.039721 seconds (1000.00 k allocations: 76.294 MiB, 27.85% gc time)</description></item><item><title>Eigenvectors Corresponding to Distinct Eigenvalues are Linearly Independent</title><link>https://freshrimpsushi.github.io/en/posts/3341/</link><pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3341/</guid><description>Theorem1 Let $V$ be a vector space, $T : V \to V$ be a linear transformation, and $\lambda_{1}, \dots, \lambda_{k}$ be distinct eigenvalues of $T$. If $\mathbf{v}_{1}, \dots, \mathbf{v}_{k}$ are eigenvectors of $T$ corresponding to the eigenvalue $\lambda_{1}, \dots, \lambda_{k}$, then $\left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{k} \right\}$ are linearly independent. Corollary Diagonalization T is diagonalizable if there exist $n$ linearly independent eigenvectors of $T$. If $T : V \to V$ is</description></item><item><title>Definition of Homology groups</title><link>https://freshrimpsushi.github.io/en/posts/2310/</link><pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2310/</guid><description>Definitions 1 2 Let&amp;rsquo;s denote by $n \in \mathbb{N}_{0}$. A chain of Abelian groups $C_{n}$ and homomorphisms $\partial_{n} : C_{n} \longrightarrow C_{n-1}$ $$ \cdots \longrightarrow C_{n+1} \overset{\partial_{n+1}}{\longrightarrow} C_{n} \overset{\partial_{n}}{\longrightarrow} C_{n-1} \longrightarrow \cdots \longrightarrow C_{1} \overset{\partial_{1}}{\longrightarrow} C_{0} \overset{\partial_{0}}{\longrightarrow} 0 $$ that satisfies $$ \partial_{n} \circ \partial_{n+1} = 0 $$ for all $n$ is called a Chain Complex. The quotient group $H_{n} := \ker \partial_{n} / \operatorname{Im} \partial_{n+1}$ is called the $n$-th</description></item><item><title>Diagonalizable Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3335/</link><pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3335/</guid><description>Definition 1 Let $V$ be called a finite-dimensional vector space. Let $T : V \to V$ be called a linear transformation. If there exists an ordered basis $\beta$ for which the matrix representation $\begin{bmatrix} T \end{bmatrix}_{\beta}$ of $T$ becomes a diagonal matrix, $T$ is said to be diagonalizable. For a square matrix $A$, if the $L_{A}$ is diagonalizable, then the matrix $A$ is said to be diagonalizable. Explanation Suppose the</description></item><item><title>How to Handle Exceptions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2309/</link><pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2309/</guid><description>Overview People who have struggled with severe loneliness know, oh they know Anyone who has struggled with unknown errors while coding understands the critical importance of errors in programming&amp;hellip; In Julia, errors can be thrown using the error() function or the @error macro. As of Julia v1.63, 25 types of built-in exceptions are defined1. Code julia&amp;gt; log(1 + 2im) 0.8047189562170501 + 1.1071487177940904im Consider, for instance, when using the logarithmic function</description></item><item><title>Characteristics Polynomial of Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3339/</link><pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3339/</guid><description>Overview The characteristic polynomial of linear transformation is defined. From the theorem below, it can be seen that solving equation $\det(A - \lambda I) = 0$ is equivalent to finding the eigenvalues. Therefore, it is quite natural to name $\det(A - \lambda I)$ the characteristic polynomial. Theorem1 Let&amp;rsquo;s say $F$ is any field, and $A \in M_{n\times n}(F)$. That $\lambda \in F$ is an eigenvalue of $A$ is equivalent to</description></item><item><title>Zero Morphism</title><link>https://freshrimpsushi.github.io/en/posts/2308/</link><pubDate>Sun, 11 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2308/</guid><description>Definition 1 $$ W \to X \overset{f}{\to} Y \to Z $$ Consider a morphism $f : X \to Y$. A morphism $g,h : W \to X$ is called a constant morphism if $fg = fh$ implies $f$. A morphism $g,h : Y \to Z$ is called a coconstant morphism if $gf = hf$ implies $f$. A morphism $f$ that is both a constant morphism and a coconstant morphism is called</description></item><item><title>딥러닝에서 인공신경망(ANN), 심층신경망(DNN), 순방향신경망(FNN)의 뜻과 차이점</title><link>https://freshrimpsushi.github.io/en/posts/3446/</link><pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3446/</guid><description>Overview This document summarizes terms used in deep learning, such as artificial neural networks, deep neural networks, and feedforward neural networks. These terms are often used interchangeably without clear definitions and can be confusing for beginners, but essentially, they can be considered the same. The origins and historical contexts of the terms explained below are not based on exhaustive research and are the author&amp;rsquo;s own hypotheses. Artificial Neural Networks and</description></item><item><title>How to Check DataFrame Size in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2307/</link><pubDate>Fri, 09 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2307/</guid><description>Overview nrow(), ncol(), and size() can be used. Unlike with R, length() results in an error. Code julia&amp;gt; df = DataFrame(rand(100000,5), :auto) 100000×5 DataFrame Row │ x1 x2 x3 x4 x5 │ Float64 Float64 Float64 Float64 Float64 ────────┼─────────────────</description></item><item><title>Eigenvalues and Eigenvectors of Finite-Dimensional Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3337/</link><pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3337/</guid><description>Definition1 Let $V$ be a finite-dimensional $F$-vector space. Let $T : V \to V$ be a linear transformation. For $\lambda \in F$, $$ Tx = \lambda x $$ a non-zero vector $x \in V$ satisfying this is called an eigenvector of $T$. The scalar $\lambda \in F$ is called the eigenvalue corresponding to the eigenvector $x$. Explanation Although one might find the term eigenvector replaced by the terms characteristic vector</description></item><item><title>Sets of Differentiable Real-Valued Functions on a Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3340/</link><pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3340/</guid><description>Definition1 Let $M$ be a differentiable manifold. The set of differentiable functions $f : M \to \mathbb{R}$ at point $p \in M$ is denoted as $\mathcal{D}$. $$ \mathcal{D} := \left\{ \text{all real-valued functions on } M \text{ that are differentialable at } p \right\} $$ On $M$, the set of differentiable functions $f : M \to \mathbb{R}$ is denoted as $\mathcal{D}(M)$. $$ \mathcal{D}(M) := \left\{ \text{all real-valued functions of class</description></item><item><title>Free groups in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/2306/</link><pubDate>Wed, 07 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2306/</guid><description>Definition 1 Given an index set $I \ne \emptyset$, let&amp;rsquo;s refer to the set $A := \left\{ a_{i} : i \in I \right\}$ as the alphabet, and its elements $a_{i} \in A$ as letters. For an integer $n \in \mathbb{Z}$, expressions like $a_{i}^{n}$ are referred to as syllables. A finite juxtaposition of these, $w$, is called a word. A syllable $a_{i}^{n} a_{i}^{m}$ can be represented as $a_{i}^{n+m}$, this is called</description></item><item><title>Differentiable Vector Fields on a Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3338/</link><pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3338/</guid><description>Definition1 Let&amp;rsquo;s call $M$ a differentiable manifold. The set of all differentiable vector fields on $M$ is denoted as $\frak{X}(M)$. $$ \frak{X}(M) := \left\{ \text{all vector fileds of calss } C^{\infty} \text{ on } M \right\} $$ Explanation $\frak{X}(M)$ is a module over the ring $\mathcal{D}(M)$ of $\mathcal{D}(M)$. In other words, for a differentiable function $f \in \mathcal{D}(M)$ and a vector field $X \in \frak{X}(M)$, $fX$ is (pointwise) well defined.</description></item><item><title>How to Create a DataFrame with Variable Names as Column Names in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2305/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2305/</guid><description>Overview Named tuples can be used. The way to create a named tuple is by attaching a semicolon ; right after the left parenthesis. For example, if you say DataFrame(; x, y), a DataFrame is created with column names :x and :y, and the contents are x and y respectively. Code julia&amp;gt; MyCol7 = rand(5); B = 1:5; julia&amp;gt; DataFrame(; MyCol7, B) 5×2 DataFrame Row │ MyCol7 B</description></item><item><title>Mathematical Definition of Statistical Significance</title><link>https://freshrimpsushi.github.io/en/posts/2304/</link><pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2304/</guid><description>Definition 1 Let&amp;rsquo;s assume that there is a given hypothesis test $H_{0} \text{ vs } H_{1}$. For all realizations $\mathbf{x} \in \Omega$, a test statistic $p \left( \mathbf{X} \right)$ that satisfies $0 \le p \left( \mathbf{x} \right) \le 1$ is called the significance probability or p-value. If $p \left( \mathbf{X} \right)$ satisfies the following for all $\theta \in \Theta_{0}$ and all $\alpha \in [0,1]$, it is said to be valid.</description></item><item><title>Tensors Defined on Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3334/</link><pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3334/</guid><description>Definition1 Let $M$ be a $n$-dimensional differentiable manifold, $\mathcal{D}(M)$ be the set of differentiable functions on $M$, $\mathfrak{X}(M)$ be the set of all vector fields on $M$. $$ \mathcal{D}(M) := \left\{ \text{all real-valued functions of class } C^{\infty} \text{ defined on } M \right\} $$ $$ \frak{X}(M) := \left\{ \text{all vector fileds of calss } C^{\infty} \text{ on } M \right\} $$ A multilinear function $T$ as follows is called</description></item><item><title>Julia's Named Tuples</title><link>https://freshrimpsushi.github.io/en/posts/2303/</link><pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2303/</guid><description>Overview Named tuples are tuples that, unlike regular tuples, can be used like dictionaries or structures. They have an array of symbols as keys and allow access to values via those keys, all the while retaining their tuple-like usage. Code x = rand(Bool, 5); y = rand(Bool, 5); z = (; x, y) typeof(z) z.x Let&amp;rsquo;s run the above code to see how named tuples are used. julia&amp;gt; z =</description></item><item><title>Basis Transformation (Coordinate Transformation) of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3333/</link><pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3333/</guid><description>Overview1 Let $V$ to $n$ dimensional vector space, and call it $\mathbf{v} \in V$. Let $\beta, \beta^{\prime}$ be the ordered basis of $V$. Then, the two coordinates $[\mathbf{v}]_{\beta}$ and $[\mathbf{v}]_{\beta^{\prime}}$ of $\mathbf{v}$ are transformed by the coordinate transformation matrix $Q$ as follows. $$ [\mathbf{v}]_{\beta} = Q [\mathbf{v}]_{\beta^{\prime}} $$ Now, suppose a linear transformation $T : V \to V$ is given. Then, for each ordered basis, there exist matrix representations $\begin{bmatrix}</description></item><item><title>Definition of Poisson Processes through Differential Operator Matrices</title><link>https://freshrimpsushi.github.io/en/posts/2302/</link><pubDate>Tue, 29 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2302/</guid><description>Definition Let us assume that $\lambda &amp;gt; 0$ is given. If it satisfies $X(0) = 0$ and has infinitesimal probabilities like $\left\{ X(t) : t \in [0,\infty) \right\}$, then it is called a Poisson Process. $$ \begin{align*} p_{ij} \left( \Delta t \right) := &amp;amp; P \left( X \left( t + \Delta t = j | X(t) = i \right) \right) \\ =&amp;amp; \begin{cases} \lambda \Delta + o \left( \Delta t</description></item><item><title>The Relationship between Covariant Derivative and Riemann Curvature Tensor</title><link>https://freshrimpsushi.github.io/en/posts/3332/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3332/</guid><description>Theorem1 Let $f : A \subset \mathbb{R}^{2} \to M$ be a parametrized surface. Let $(s, t)$ be the standard coordinates of $\mathbb{R}^{2}$. Let $V = V(s,t)$ be a vector field following $f$. At each point $(s, t)$, the following holds: $$ \dfrac{D }{\partial t} \dfrac{D }{\partial s}V - \dfrac{D }{\partial s} \dfrac{D }{\partial t}V = R(\dfrac{\partial f}{\partial s}, \dfrac{\partial f}{\partial t})V $$ Description Einstein notation is used. Proof Choose a</description></item><item><title>Most Powerful Test Containing Sufficient Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2301/</link><pubDate>Sun, 27 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2301/</guid><description>Theorem Hypothesis Testing: $$ \begin{align*} H_{0} :&amp;amp; \theta = \theta_{0} \\ H_{1} :&amp;amp; \theta = \theta_{1} \end{align*} $$ In such hypothesis testing, let us call the probability density function or probability mass function for $\theta_{0}, \theta_{1}$ of sufficient statistic $T$ considering $\theta$ as $g \left( t | \theta_{0} \right), g \left( t | \theta_{1} \right)$. Then, given a rejection region $S$ and a certain constant $k \ge 0$, all hypothesis</description></item><item><title>Coordinate Transformation of Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3331/</link><pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3331/</guid><description>Overview1 2 Let $V$ be $n$dimensional vector space, and let us call $\mathbf{v} \in V$ as such. Let $\beta$ be some ordered basis of $V$. Then, $\mathbf{v}$ is expressed as the coordinate vector $[\mathbf{v}]_{\beta}$. Given another ordered basis $\beta ^{\prime}$, $\mathbf{v}$ can also be expressed as the coordinate vector $[\mathbf{v}]_{\beta^{\prime}}$ with respect to it. Coordinate transformation of vectors refers to the equation relating these two coordinate vectors. Build-up For convenience,</description></item><item><title>F-vector space in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/2300/</link><pubDate>Fri, 25 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2300/</guid><description>Definition A $F$-vector space is a $R$-module that is a field in the context of a ring $R = F$. Explanation This is a definition that even feels sophisticated, considering that a module is a generalization of a vector field, it makes sense. See Also Vector spaces in linear algebra Vector spaces in abstract algebra The $F$-vector spaces discussed in the documents below are essentially no different from the vector</description></item><item><title>Convolutional Layer</title><link>https://freshrimpsushi.github.io/en/posts/3386/</link><pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3386/</guid><description>Definition Let $\mathbf{W}$ be $k \times k$ matrix. Define $M^{n\times n} = M^{n\times n}(\mathbb{R})$ to be the set of real matrices of size $n \times n$. A convolutional layer $C_{\mathbf{W}} : M^{nn} \to M^{(n-k+1) \times (n-k+1)}$ is a function defined as follows. For $\mathbf{X} \in M^{n\times n}$ and $\mathbf{Y} = C_{\mathbf{W}}(\mathbf{X})$, $$ \begin{align*} Y_{ij} &amp;amp;= \begin{bmatrix} w_{11} &amp;amp; w_{12} &amp;amp; \cdots &amp;amp; w_{1k} \\ w_{21} &amp;amp; w_{22} &amp;amp; \cdots &amp;amp;</description></item><item><title>Calin-Rubin Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/2299/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2299/</guid><description>Theorem Hypothesis Testing: $$ \begin{align*} H_{0} :&amp;amp; \theta \le \theta_{0} \\ H_{1} :&amp;amp; \theta &amp;gt; \theta_{0} \end{align*} $$ In such a hypothesis test, $T$ is called a sufficient statistic for $\theta$, and the family $\left\{ g(t | \theta) : \theta \in \Theta \right\}$ of the probability density function or probability mass function of $t$ possesses a Monotone Likelihood Ratio (MLR). Then, for $\forall t_{0}$, $$ H_{0} \text{ is rejected if</description></item><item><title>Lie Groups</title><link>https://freshrimpsushi.github.io/en/posts/3330/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3330/</guid><description>Definition1 A group $G$ is called a Lie group if it satisfies the following conditions: It has a differentiable structure. The binary operation $\cdot : G \times G \to G$ defined in $G$ is differentiable. The inverse ${}^{-1} : G \to G$ is differentiable. Explanation Simply put, a Lie group is a differentiable group. Examples $(\mathbb{R}, +)$ Euclidean space has a differentiable structure. $f : (x,y) \mapsto x+y \in C^{\infty}$</description></item><item><title>Abstract Algebra in R-modules</title><link>https://freshrimpsushi.github.io/en/posts/2298/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2298/</guid><description>Definitions 1 An Abelian group $(G,+)$ with the identity element for multiplication $1 \ne 0$, and a ring $(R,+,\cdot)$ satisfy the following three conditions for the binary operation $$ \mu : R \times G \to G $$, then $\left( G, +, R, \cdot ; \mu \right)$ is called an $R$-module: (M1) Bi-additivity: For $\forall \alpha, \beta \in R$ and $\forall x,y \in G$, $$ \begin{align*} \mu \left( \alpha + \beta</description></item><item><title>Left Multiplication Transformation (Matrix Transformation)</title><link>https://freshrimpsushi.github.io/en/posts/3329/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3329/</guid><description>Definition1 Regarding the field $F$, let&amp;rsquo;s say $A \in M_{m \times n}(F)$. The following $L_{A}$ is defined as the left-multiplication transformation. $$ \begin{align*} L_{A} : F^{n} &amp;amp;\to F^{m} \\ x &amp;amp;\mapsto Ax \end{align*} $$ Here, $Ax$ is the matrix product of $A$ and $x$. Description This is a more abstract description of matrix transformation using the concept of a field. Theorems Let&amp;rsquo;s say $A \in M_{m \times n}(F)$. Then, $L_{A}$</description></item><item><title>Definition of Monotonic Probability</title><link>https://freshrimpsushi.github.io/en/posts/2297/</link><pubDate>Sat, 19 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2297/</guid><description>Definition Let&amp;rsquo;s define a family of probability mass functions or probability density functions for a parameter $\theta \in \mathbb{R}$ and a univariate random variable $T$ as $G := \left\{ g ( t | \theta) : \theta \in \Theta \right\}$. If for all $\theta_{2} &amp;gt; \theta_{1}$, $$ {{ g \left( t | \theta_{2} \right) } \over { g \left( t | \theta_{1} \right) }} $$ is a monotonic function in $\left\{</description></item><item><title>Matrix Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3327/</link><pubDate>Fri, 18 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3327/</guid><description>Definitions For a field $F$, let us define the set of $m \times n$ matrices whose components are elements of $F$ as $M_{m \times n}(F)$. $$ M_{m \times n}(F) := \left\{ \begin{bmatrix} a_{11} &amp;amp; \cdots &amp;amp; a_{1n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} : a_{ij} \in F \right\} $$ Then, with respect to matrix addition and scalar multiplication, $M_{m \times n}(F)$ is a</description></item><item><title>Elementary Notation in Enumerating Elements</title><link>https://freshrimpsushi.github.io/en/posts/2296/</link><pubDate>Thu, 17 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2296/</guid><description>Explanation $$ \left\{ 1, \cdots , \hat{k} , \cdots , n \right\} $$ When representing a set, there are instances where a hat is placed on a specific index as shown above, which means that the element is to be removed. For instance, in $\left\{ 0,1,2,3,4 \right\}$, if we consider the collection of sets with exactly one element removed, it can be represented as $$ \left\{ \left\{ 0, \cdots, \hat{k},</description></item><item><title>Scalar Curvature of Differential Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3328/</link><pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3328/</guid><description>Definition1 Riesz Representation Theorem Let $\left( H, \left\langle \cdot,\cdot \right\rangle \right)$ be an inner product space. For linear functionals $f \in H^{ \ast }$ and $\mathbf{x} \in H$ on $H$, there exists a unique $\mathbf{w} \in H$ that satisfies $f ( \mathbf{x} ) = \left\langle \mathbf{w} , \mathbf{x} \right\rangle$. Let $M$ be a differentiable manifold. Let $T_{p}M$ be the tangent space at point $p\in M$. Now, for a fixed $X</description></item><item><title>Proof of the Neyman-Pearson Lemma</title><link>https://freshrimpsushi.github.io/en/posts/2295/</link><pubDate>Tue, 15 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2295/</guid><description>Theorem Hypothesis Testing: $$ \begin{align*} H_{0} :&amp;amp; \theta = \theta_{0} \\ H_{1} :&amp;amp; \theta = \theta_{1} \end{align*} $$ In the hypothesis testing above, let $\theta_{0}, \theta_{1}$ have a probability density function or probability mass function denoted by $f \left( \mathbf{x} | \theta_{0} \right), f \left( \mathbf{x} | \theta_{1} \right)$, and let the rejection region be $R$, and some constant $k \ge 0$, then if (i): $f \left( \mathbf{x} | \theta_{1}</description></item><item><title>Matrices of Linear Transformations from a Basis of a Subspace to an Extended Basis</title><link>https://freshrimpsushi.github.io/en/posts/3325/</link><pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3325/</guid><description>Theorem Let █eq01█ be a subspace in █eq02█ dimension vector space called █eq03█. Let █eq04█ be the ordered basis of █eq05█. Consider █eq06█ as an extension of the basis of █eq03█ from █</description></item><item><title>How to Sample Univariate Probability Variables</title><link>https://freshrimpsushi.github.io/en/posts/2294/</link><pubDate>Sun, 13 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2294/</guid><description>Overview A method for obtaining a specific realization of a random variable. Theorem Let the cumulative distribution function $F = F_{T}$ of a univariate random variable $T$ be an increasing function. Then, for a probability variable $U \sim U \left( 0,1 \right)$ that follows a uniform distribution, the following holds: $$ T = F^{-1} \left( U \right) $$ Explanation Considering that the range of the cumulative distribution function is always</description></item><item><title>Ricci Curvature of Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3326/</link><pubDate>Sat, 12 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3326/</guid><description>Definition1 1 Given a differentiable manifold $M$ and the tangent space $T_{p}M$ at point $p \in M$, let&amp;rsquo;s have the function $f$ as follows. For a given $X, Y \in T_{p}M$, $$ \begin{align*} f : T_{p}M &amp;amp;\to T_{p}M \\ Z &amp;amp;\mapsto R(X,Z)Y \end{align*} $$ where $R$ is the Riemann curvature. Then, the Ricci curvature $\Ric : T_{p}M \times T_{p}M \to \mathbb{R}$ at point $p$ is defined as $$ \Ric (X,Y)</description></item><item><title>Power of a Nuisance Test and the Most Powerful Test</title><link>https://freshrimpsushi.github.io/en/posts/2293/</link><pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2293/</guid><description>Definition 1 Hypothesis Testing: $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ A power function $\beta (\theta)$ is said to be unbiased if it satisfies the following for all $\theta_{0} \in \Theta_{0}$ and $\theta_{1} \in \Theta_{0}^{c}$: $$ \beta \left( \theta_{0} \right) \le \beta \left( \theta_{1} \right) $$ Let $\mathcal{C}$ be a set comprising such hypothesis tests. A hypothesis test $A$ that has a</description></item><item><title>Block Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3323/</link><pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3323/</guid><description>Definition Let&amp;rsquo;s say $A$ is a matrix $m \times n$. $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \\ \end{bmatrix} $$ Consider any vertical and horizontal lines that cut the matrix as follows. $$ A = \left[ \begin{array}{cc|ccc|c|} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}</description></item><item><title>Definition of Spectral Radius</title><link>https://freshrimpsushi.github.io/en/posts/2292/</link><pubDate>Wed, 09 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2292/</guid><description>Definition The eigenvalue $\lambda_{1} , \cdots , \lambda_{n}$ of the matrix $A \in \mathbb{C}^{n \times n}$ with the largest modulus $\rho (A) = \argmax_{\lambda} \left| \lambda \right|$ is called the Spectral Radius. Explanation Usually, the spectra of a matrix refers to both its eigenvalues and eigenvectors collectively, and in the absence of a matrix, it denotes the matrix or the abstractly defined eigenvalues and eigenvectors algebraically1. Gantmacher. (2000). The Theory</description></item><item><title>If the Sectional Curvature is the Same, the Riemannian Curvature is also the Same.</title><link>https://freshrimpsushi.github.io/en/posts/3324/</link><pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3324/</guid><description>Theorem1 Let $V$ be defined in a vector space of dimension $2$ or higher, and let $\left\langle \cdot, \cdot \right\rangle$ be the inner product defined on $V$. Let $R : V \times V \times V \times V \to V$ and $R^{\prime} : V \times V \times V \times V \to V$ be multilinear functions satisfying the conditions below. $$ R(x,y,z,w) = \left\langle R(x,y)z, w \right\rangle,\quad R^{\prime}(x,y,z,w) = \left\langle R^{\prime}(x,y)z, w</description></item><item><title>Power Function of Hypothesis Testing</title><link>https://freshrimpsushi.github.io/en/posts/2291/</link><pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2291/</guid><description>Definition 1 Hypothesis Testing: $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ Given the hypothesis testing above, let&amp;rsquo;s denote it as $\alpha \in [0,1]$. For the parameter $\theta$, the function $\beta (\theta) := P_{\theta} \left( \mathbf{X} \in \mathbb{R} \right)$ with the rejection region $R$ is called the Power Function. If $\sup_{\theta \in \Theta_{0}} \beta (\theta) = \alpha$, then the given hypothesis test is</description></item><item><title>Expansion and Contraction of the Basis</title><link>https://freshrimpsushi.github.io/en/posts/3321/</link><pubDate>Sun, 06 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3321/</guid><description>Theorem1 Let $S$ be a finite subset of the finite-dimensional vector space $V$. (a) If $S$ generates $V$ but is not a basis of $V$, then elements of $S$ can be appropriately removed to reduce it to a basis of $V$. (b) If $S$ is linearly independent but not a basis of $V$, then elements can be suitably added to $S$ to extend it to a basis of $V$. Corollary</description></item><item><title>Gillespie Stochastic Simulation Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/2290/</link><pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2290/</guid><description>Algorithm 1 For $\mathbf{x}_{t} = i$, $$ P \left( \mathbf{X}_{t + \tau} = j | \mathbf{x}_{t} = i \right) = a_{j} \left( \mathbf{x}_{t} \right) \qquad , j = 1, \cdots , n $$ the method for simulating the realization of a continuous Markov chain $\left\{ \mathbf{X}_{t} \right\}$ is called the Gillespie Stochastic Simulation Algorithm, or simply SSA. Input Receives initial values $t=0$ and $\mathbf{X}_{0} = \mathbf{x}$. Step 1. Calculate $\sum_{j}</description></item><item><title>Coordinate Representation of the Riemann Curvature Tensor</title><link>https://freshrimpsushi.github.io/en/posts/3320/</link><pubDate>Fri, 04 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3320/</guid><description>Explanation1 Given a Riemannian manifold $(M, g)$, let&amp;rsquo;s say the coordinate system at $p$ is referred to as $(U, \mathbf{x})$. And let the tangent vector be denoted as follows. $$ \dfrac{\partial }{\partial x_{i}} \overset{\text{denote}}{=} X_{i} $$ Now, consider $R(X_{i}, X_{j})X_{k}$. By the definition of the Riemann curvature tensor $R$, it is also a vector field. Therefore, it can be expressed as follows. $$ R(X_{i}, X_{j})X_{k} = \sum_{l}R_{ijk}^{l}X_{l} $$ The coefficient</description></item><item><title>Likelihood Ratio Test Including Sufficient Statistic</title><link>https://freshrimpsushi.github.io/en/posts/2289/</link><pubDate>Thu, 03 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2289/</guid><description>Theorem Hypothesis Testing: $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ Likelihood Ratio test statistic: $$ \lambda \left( \mathbf{x} \right) := {{ \sup_{\Theta_{0}} L \left( \theta \mid \mathbf{x} \right) } \over { \sup_{\Theta} L \left( \theta \mid \mathbf{x} \right) }} $$ If $T \left( \mathbf{X} \right)$ is a sufficient statistic for the parameter $\theta$, and $\lambda^{\ast} (t)$ is a likelihood ratio test statistic</description></item><item><title>Implicit Runge-Kutta Methods</title><link>https://freshrimpsushi.github.io/en/posts/3319/</link><pubDate>Wed, 02 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3319/</guid><description>Overview This document introduces the Runge-Kutta method, an Ordinary Differential Equation (ODE) solver. The commonly used fourth-order Runge-Kutta method, RK4, is a type of explicit Runge-Kutta method. This document explains the implicit Runge-Kutta method. Buildup1 $$ y^{\prime} = f(t,y),\quad t \ge t_{0},\quad y(t_{0}) = y_{0} $$ Consider the given ordinary differential equation as above. $y$ is a function of $t$, and $^{\prime}$ represents the derivative with respect to $t$. When</description></item><item><title>Kolmogorov Differential Equation Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2288/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2288/</guid><description>Theorem A differential equation holds for the transition probability matrix $P(t)$ and the differential matrix $Q$. $$ {{ d P(t) } \over { dt }} = Q P(t) = P(t) Q $$ Explanation If one were to make a distinction, $dP/dt = P(t) Q$ is referred to as the backward Kolmogorov differential equation, and $dP/dt = Q P(t)$ as the forward Kolmogorov differential equation or Stochastic governing equation. Derivation According</description></item><item><title>Symmetry of the Riemann Curvature Tensor</title><link>https://freshrimpsushi.github.io/en/posts/3318/</link><pubDate>Mon, 31 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3318/</guid><description>Definition1 The Riemann curvature tensor $R$ is defined as follows for $R: \frak{X}(M) \times \frak{X}(M) \times \frak{X}(M) \times \frak{X}(M) \to \mathcal{D}(M)$. $$ R(X, Y, Z, W) := g(R(X, Y)Z, W) = \left\langle R(X, Y)Z, W \right\rangle $$ Here, $\frak{X}(M)$ is the set of all vector fields defined on $M$, $\mathcal{D}(M)$ is the set of differentiable functions defined on $M$, and $g$ is the Riemannian metric. Description Note that notation is</description></item><item><title>Definition of Likelihood Ratio Test in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2287/</link><pubDate>Sun, 30 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2287/</guid><description>Definition 1 $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ For the hypothesis test described above, the statistic $\lambda$ is called the Likelihood Ratio test statistic. $$ \lambda \left( \mathbf{x} \right) := {{ \sup_{\Theta_{0}} L \left( \theta \mid \mathbf{x} \right) } \over { \sup_{\Theta} L \left( \theta \mid \mathbf{x} \right) }} $$ A hypothesis test that has a rejection region $\left\{ \mathbf{x} :</description></item><item><title>Explicit Runge-Kutta Methods</title><link>https://freshrimpsushi.github.io/en/posts/3317/</link><pubDate>Sat, 29 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3317/</guid><description>Overview Introducing the Runge-Kutta method, an Ordinary Differential Equation (ODE) solver. A separate article is published for a detailed explanation of the commonly used 4th Order Runge-Kutta method RK4. Buildup Consider the following ordinary differential equation. $y$ is a function of $t$, and $^{\prime}$ means the derivative with respect to $t$. $$ y^{\prime} = f(t,y),\quad t \ge t_{0},\quad y(t_{0}) = y_{0} $$ Integrating this from $t_{n}$ to $t_{n+1} = t_{n}</description></item><item><title>Continuous Markov Chain</title><link>https://freshrimpsushi.github.io/en/posts/2286/</link><pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2286/</guid><description>Definition A Continuous-Time Markov Chain (CTMC) refers to a continuous stochastic process $\left\{ X_{t} \right\}$ that satisfies the following conditions for all finite sequences of time points $0 \le t_{0} \le \cdots \le t_{n} \le t_{n+1}$, where the state space is a countable set: $$ P \left( X_{t_{n+1}} = j \mid X_{t_{n}} = i , X_{t_{n-1}} = k , \cdots , X_{t_{0}} = l \right) = P \left( X_{t_{n+1}} =</description></item><item><title>Bianchi Identity</title><link>https://freshrimpsushi.github.io/en/posts/3316/</link><pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3316/</guid><description>Theorem1 Let&amp;rsquo;s call $R$ the Riemann curvature. Then, the following holds. $$ R(X, Y) Z + R(Y, Z) X + R(Z, X) Y = 0 $$ Proof It is proven by a straightforward, though complex, calculation without any special techniques. By the definition of Riemann curvature, $$ \begin{align*} R(X, Y) Z + R(Y, Z) X + R(Z, X) Y &amp;amp;= \nabla_{Y} \nabla_{X} Z - \nabla_{X} \nabla_{Y} Z + \nabla_{[X,Y]}Z \\</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimates of the Location Family</title><link>https://freshrimpsushi.github.io/en/posts/2285/</link><pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2285/</guid><description>Theorem Given a random sample $X_{1} , \cdots , X_{n} \sim X$ obtained from a location family with the probability density function $f_{X} \left( x ; \theta \right) = f_{X} \left( x - \theta \right)$, the sufficient statistic and maximum likelihood estimator depend on if the support of $X$ is upper bounded, then $\max X_{k}$ if the support of $X$ is lower bounded, then $\min X_{k}$. The support of a</description></item><item><title>Differences in Array Dimensions in Julia, Python (NumPy, PyTorch)</title><link>https://freshrimpsushi.github.io/en/posts/3315/</link><pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3315/</guid><description>Overview When dealing with high-dimensional arrays in Julia and NumPy, PyTorch (hereinafter referred to collectively as Python for simplicity), it is important to pay attention to what each dimension signifies as they differ. This distinction arises because Julia&amp;rsquo;s arrays are column-major, whereas Python&amp;rsquo;s arrays are row-major. Note that Matlab, being column-major like Julia, does not have this discrepancy, so those familiar with Matlab need not be overly cautious, but those</description></item><item><title>Transition Probabilities of Stochastic Processes</title><link>https://freshrimpsushi.github.io/en/posts/2284/</link><pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2284/</guid><description>Definition Let us assume there is a stochastic process $\left\{ X_{t} \right\}$ with a countable set as its state space. For two points in time $t_{1} &amp;lt; t_{2}$, the transition probability $p_{ij} \left( t_{1} , t_{2} \right)$ is defined as follows: $$ p_{ij} \left( t_{1} , t_{2} \right) := P \left( X_{t_{2}} = j \mid X_{t_{1}} = i \right) $$ Here, the (current) state represented by $i$ is referred to</description></item><item><title>Sectional Curvature of Differential Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3322/</link><pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3322/</guid><description>Theorem1 Let $\sigma \subset T_{p}M$ be a 2-dimensional subspace of the tangent space $T_{p}M$. Suppose that $x, y \in \sigma$ are linearly independent. Then, the following $K$ does not depend on the choice of $x, y$. $$ K(x, y) = \dfrac{R(x,y,x,y)}{\left\| x \times y \right\|^{2}} $$ Here, $R$ is the Riemann curvature tensor. Explanation According to the above theorem, if $\sigma$ is given, the value of $K$ is the same</description></item><item><title>Mathematical Statistical Hypothesis Testing Definition</title><link>https://freshrimpsushi.github.io/en/posts/2283/</link><pubDate>Sat, 22 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2283/</guid><description>Definition A proposition about a parameter is called a Hypothesis. The problem of accepting hypothesis $H_{0}$ as true based on a given sample, or rejecting hypothesis $H_{0}$ and adopting hypothesis $H_{1}$ is called a Hypothesis Test. In hypothesis testing, the complementary hypotheses $H_{0}$, $H_{1}$ are called the Null Hypothesis and the Alternative Hypothesis, respectively. The subset $R \subset \Omega$ of the sample space $\Omega$ that leads to the rejection of</description></item><item><title>Differential Geometry of Curved Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3314/</link><pubDate>Fri, 21 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3314/</guid><description>Definition1 Let&amp;rsquo;s denote $M$ as a Riemannian manifold, and $\frak{X}(M)$ as the set of all vector fields on $M$. $$ \frak{X}(M) = \text{the set of all vector fileds of calss } C^{\infty} \text{ on } M $$ The curvature $R$ of $M$ is a function that maps $X, Y \in \frak{X}(M)$ to $R(X, Y) : \frak{X}(M) \to \frak{X}(M)$. In this context, $R(X, Y)$ is given as follows. $$ \begin{equation} R(X,</description></item><item><title>How to Check Text File Encoding at the Terminal</title><link>https://freshrimpsushi.github.io/en/posts/2282/</link><pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2282/</guid><description>Guide 1 This method requires Python to be installed. By entering pip install chardet in the terminal to install chardet, and running the following command in the directory where the file to check the encoding is located, it shows what the encoding is along with its confidence. PS C:\Users\rmsms\Downloads&amp;gt; chardetect .\example.csv .\example.csv: EUC-KR with confidence 0.99 In this example, the encoding of example.csv is EUC-KR. It is important to note</description></item><item><title>Paper Review: Physics-Informed Neural Networks</title><link>https://freshrimpsushi.github.io/en/posts/3313/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3313/</guid><description>Overview The notation and numbering of references and formulas follow the conventions of the original paper. Physics-informed neural networks (referred to as PINN) are artificial neural networks designed to numerically solve differential equations, introduced in the 2018 Journal of Computational Physics paper Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. The authors of the paper are M. Raissi, P. Perdikaris,</description></item><item><title>Random Sample's Sample Mean Average and Variance</title><link>https://freshrimpsushi.github.io/en/posts/2281/</link><pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2281/</guid><description>Formulas Given a random sample $X_{1} , \cdots , X_{n} \overset{\text{iid}}{\sim} X$, the mean and variance of its sample mean $\bar{X}$ are as follows. $$ \begin{align*} E \bar{X} =&amp;amp; E X \\ \Var \bar{X} =&amp;amp; {{ 1 } \over { n }} \Var X \end{align*} $$</description></item><item><title>Poincaré Metric</title><link>https://freshrimpsushi.github.io/en/posts/3312/</link><pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3312/</guid><description>1 Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p73-74&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>How to Insert Command Line Arguments in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2280/</link><pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2280/</guid><description>English Translation Code println(ARGS[1] * &amp;#34; + &amp;#34; * ARGS[2] * &amp;#34; = &amp;#34; * string(parse(Float64, ARGS[1]) + parse(Float64, ARGS[2]))) Let&amp;rsquo;s say we have a file named example.jl that consists of a single line as shown above. In Julia, we can receive command line arguments as an array through ARGS, similar to how sys.argv works with command line arguments in Python. The code written is a program that takes two</description></item><item><title>Methods for Symbolic Computation in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3311/</link><pubDate>Sat, 15 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3311/</guid><description>Overview Symbolic operations in Julia can be used through the SymEngine.jl1 package. Code Defining Symbols Symbols can be defined in the following way. julia&amp;gt; using SymEngine julia&amp;gt; x = symbols(:x) x julia&amp;gt; x, y = symbols(&amp;#34;x y&amp;#34;) (x, y) julia&amp;gt; @vars x, y (x, y) julia&amp;gt; x = symbols(:x) x julia&amp;gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x) Vectors and Matrices julia&amp;gt; v = [</description></item><item><title>The Unique Maximum Likelihood Estimator Depends on the Sufficient Statistic</title><link>https://freshrimpsushi.github.io/en/posts/2279/</link><pubDate>Fri, 14 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2279/</guid><description>Theorem If a sufficient statistic $T$ exists for a parameter $\theta$ and a unique maximum likelihood estimator $\hat{\theta}$ for $\theta$ exists, then $\hat{\theta}$ can be represented as a function of $T$. Proof 1 Consider a random sample $X_{1} , \cdots , X_{n}$ with a probability density function $f \left( x ; \theta \right)$ and its sufficient statistic $T := T \left( X_{1} , \cdots , X_{n} \right)$ and probability density</description></item><item><title>Exponential Mapping and Normal Neighborhood</title><link>https://freshrimpsushi.github.io/en/posts/3310/</link><pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3310/</guid><description>1 Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p71-73&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Executing External Programs in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2278/</link><pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2278/</guid><description>Code In Julia, you can execute a string wrapped in backticks using the run() function. This is similar to using os.system() from the os module in Python. julia&amp;gt; txt = &amp;#34;helloworld&amp;#34; &amp;#34;helloworld&amp;#34; julia&amp;gt; typeof(`echo $txt`) Cmd As shown above, a string wrapped in backticks has the type Cmd and can be executed with the run() function. julia&amp;gt; run(`cmd /C echo $txt`) helloworld Process(`cmd /C echo helloworld`, ProcessExited(0)) In this example,</description></item><item><title>Binomial operation's Jacobi Identity</title><link>https://freshrimpsushi.github.io/en/posts/3309/</link><pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3309/</guid><description>Definition Set $S$ and binary operations $\ast : S \times S \to S$, commutative binary operations $+ : S \times S \to S$, are considered. The following form of equation is called the Jacobi identity. $$ a \ast (b \ast c) + c \ast (a \ast b) + b \ast (c \ast a) = 0,\quad a,b,c \in S $$ If the above equation holds, $\ast$ satisfies the Jacobi identity. Description</description></item><item><title>Lemmas-Schep Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/2277/</link><pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2277/</guid><description>Theorem 1 2 A unique unbiased estimator dependent on a complete sufficient statistic exists. That is, for the complete sufficient statistic $T$, if $E \left[ \phi (T) \right] = \tau (\theta)$, then $\phi (T)$ is the unique unbiased estimator for $\tau (\theta)$, namely the best unbiased estimator. Explanation The Lehmann-Scheffé theorem is a powerful theorem that</description></item><item><title>Minimizing Geodesics</title><link>https://freshrimpsushi.github.io/en/posts/3308/</link><pubDate>Sun, 09 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3308/</guid><description>English Translation 1 Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p70-71&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Converting String to Number in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2276/</link><pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2276/</guid><description>Code To convert a string str to a number of type type, use parse(type, str). julia&amp;gt; parse(Int, &amp;#34;21&amp;#34;) 21 julia&amp;gt; parse(Float64, &amp;#34;3.14&amp;#34;) 3.14 You might wonder why we can&amp;rsquo;t do something like Int64(&amp;quot;21&amp;quot;) as in Python&amp;hellip; That&amp;rsquo;s because changing &amp;lsquo;&amp;ldquo;21&amp;rdquo;&amp;rsquo; into 21 is not about changing types but interpreting the string &amp;quot;21&amp;quot; as a number, which justifies the use of parse1. Environment OS: Windows julia: v1.6.3 https://discourse.julialang.org/t/why-are-there-all-these-strange-stumbling-blocks-in-julia/92644/2&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Nilpotent Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3307/</link><pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3307/</guid><description>Definition1 $n \times n$ For a matrix $A$, if there exists a positive integer $k$ that satisfies $A^{k} = O$, then we call $A$ a nilpotent matrix. In this case, $O$ is the zero matrix of $n \times n$. Explanation &amp;ldquo;Nil&amp;rdquo; means &amp;lsquo;zero&amp;rsquo; or &amp;rsquo;none.&amp;rsquo; &amp;ldquo;Potent&amp;rdquo; means &amp;lsquo;powerful,&amp;rsquo; and is the root of the word &amp;ldquo;potential.&amp;rdquo; Therefore, the term &amp;ldquo;nilpotent&amp;rdquo; can be understood as &amp;lsquo;having the potential/power to become $0$.&amp;rsquo;</description></item><item><title>Minimum Variance Unbiased Estimator Uniqueness</title><link>https://freshrimpsushi.github.io/en/posts/2275/</link><pubDate>Thu, 06 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2275/</guid><description>Theorem 1 If $W$ is the Best Unbiased Estimator for $\tau (\theta)$, then $W$ is unique. Proof Cauchy-Schwarz Inequality: For the Random Variable $X, Y$, the following holds: $$ \operatorname{Cov} (X,Y) \le \Var X \Var Y $$ The necessary and sufficient condition for equality to hold is as follows: $$ \exist a \ne 0 , b \in \mathbb{R} : a X + b = Y $$ Assuming $w '$ is</description></item><item><title>Gauss's Lemma in Riemannian Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3306/</link><pubDate>Wed, 05 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3306/</guid><description>1 Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p68-70&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Proof of the Cauchy-Schwarz Inequality in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2274/</link><pubDate>Tue, 04 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2274/</guid><description>Theorem For a random variable $X, Y$, the following holds. $$ \operatorname{Cov} (X,Y) \le \Var X \Var Y $$ The necessary and sufficient condition for the equality to hold is as follows1. $$ \exist a \ne 0 , b \in \mathbb{R} : a X + b = Y $$ Proof Let&amp;rsquo;s denote the population means of $X,Y$ as $\mu_{X}$ and $\mu_{Y}$, respectively. $$ \begin{align*} h(t) :=&amp;amp; E \left( \left[ \left(</description></item><item><title>Triangular Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3305/</link><pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3305/</guid><description>Definition1 A matrix $A = [a_{ij}]$ with all elements above the main diagonal being $0$ is called a lower triangular matrix. $$ A \text{ is lower triangluar matrix if } a_{ij} = 0 \text{ whenever } i \lt j $$ A matrix $A = [a_{ij}]$ with all elements below the main diagonal being $0$ is called an upper triangular matrix. $$ A \text{ is upper triangluar matrix if } a_{ij}</description></item><item><title>Best Unbiased Estimator, Minimum Variance Unbiased Estimator UMVUE</title><link>https://freshrimpsushi.github.io/en/posts/2273/</link><pubDate>Sun, 02 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2273/</guid><description>Definition 1 Let us assume that parameter $\theta$ is given. If an unbiased estimator $W^{\ast}$ satisfies the following condition over all other unbiased estimators $W$, it is called the Best Unbiased Estimator or the Uniform Minimum Variance Unbiased Estimator (UMVUE). $$ \Var_{\theta} W^{\ast} \le \Var_{\theta} W \qquad , \forall \theta $$ Explanation UMVUE is sometimes simply referred to as MVUE, dropping the initial Uniform part. The term UMVUE might be</description></item><item><title>Parameterized Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3304/</link><pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3304/</guid><description>Definition1 For an open set $U \subset \mathbb{R}^{2}$, let&amp;rsquo;s say the connected set $A\subset \mathbb{R}^{2}$ satisfies the following: $$ U \subset A \subset \overline{U} \quad \text{and} \quad \partial A \text{ is piecewise differentiable} $$ $s : A \to M$ is called a parameterized surface within the manifold $M$. A vector field $V$ along $s$, maps each $q \in A$ to $V(q) \in T_{s(q)}M$ and is a differentiable function in the</description></item><item><title>Galton-Watson Process</title><link>https://freshrimpsushi.github.io/en/posts/2272/</link><pubDate>Fri, 30 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2272/</guid><description>Definition 1 The Basic Reproductive Rate $m = EX &amp;lt; \infty$ of a random variable $X$ is given. If we denote the $X_{n,i}$th offspring of the $i$th particle in the $n$rd generation of a branching process by $X_{n,i}$, then the stochastic process represented by a random sample $\left\{ X_{n,i} : (n,i) \in \mathbb{N}^{2} \right\} \overset{\text{iid}}{\sim} X$ is called the Galton-Watson Process. $$ Z_{n+1} = \sum_{i=1}^{Z_{n}} X_{n,i} $$ If the mean</description></item><item><title>What is a commutator in field theory?</title><link>https://freshrimpsushi.github.io/en/posts/3303/</link><pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3303/</guid><description>Definition For a ring $(R, +, \cdot)$, the commutator of two elements $a, b \in R$ is defined as follows. $$ [a, b] := a \cdot b - b \cdot a = ab - ba $$ If $[a, b] = 0$, then $a, b$ are said to commute. The anticommutator of $a, b$ is defined as follows. $$ \left\{a, b\right\} = ab + ba $$ Explanation While similar to the</description></item><item><title>Unbiased Estimators and the Cramér-Rao Bound</title><link>https://freshrimpsushi.github.io/en/posts/2271/</link><pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2271/</guid><description>Theorem Regularity Conditions: (R0): The probability density function $f$ is injective with respect to $\theta$. It satisfies the following equation. $$ \theta \ne \theta ' \implies f \left( x_{k} ; \theta \right) \ne f \left( x_{k} ; \theta ' \right) $$ (R1): The probability density function $f$ has the same support for all $\theta$. (R2): The true value $\theta_{0}$ is an interior point of $\Omega$. (R3): The probability density function</description></item><item><title>Differentiable Curves and Minimization</title><link>https://freshrimpsushi.github.io/en/posts/3302/</link><pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3302/</guid><description>본 문서의 내용이 없어 번역 진행이 불가능합니다. 구체적인 내용을 제공해주세요.</description></item><item><title>Branching Process</title><link>https://freshrimpsushi.github.io/en/posts/2270/</link><pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2270/</guid><description>Definition 1 A branching process refers to a stochastic process where entities such as individuals, cells, molecules, or other types of particles have characteristics like lifespan or basic reproductive rate that are considered random variables. Kimmel, Axelrod. (2006). Branching Processes in Biology: p1.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>What is a Commutator in Group Theory?</title><link>https://freshrimpsushi.github.io/en/posts/3301/</link><pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3301/</guid><description>Definition For two elements $g, h \in G$ of a group $G$, the commutator of $g, h$ is defined as follows. $$ [g,h] := ghg^{-1}h^{-1} $$ Explanation According to the definition, it is equivalent that the commutator of all elements is the identity element $e$ and that $G$ is an Abelian group. The commutator mentioned in group theory and the commutators that appear in quantum mechanics, differential geometry, etc., are</description></item><item><title>Proof of the Invariance Property of the Maximum Likelihood Estimator</title><link>https://freshrimpsushi.github.io/en/posts/2269/</link><pubDate>Sat, 24 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2269/</guid><description>Theorem The Maximum Likelihood Estimator (MLE) is invariant with respect to transformation of function. In other words, if $\hat{\theta}$ is the MLE of the parameter $\theta$, then for any function $\tau$, $\tau \left( \hat{\theta} \right)$ is also the MLE of $\tau \left( \theta \right)$. Proof 1 Let $\eta := \tau \left( \theta \right)$ and define a new function $L^{\ast}$ for the likelihood function $L = L \left( \theta | \mathbf{x}</description></item><item><title>Stalin Sort</title><link>https://freshrimpsushi.github.io/en/posts/2268/</link><pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2268/</guid><description>Algorithm 1 Let&amp;rsquo;s say we have an array of length $n$. If we read the array from the beginning to the end and repeatedly remove elements where the element behind is greater than the one in front, we &amp;lsquo;in order&amp;rsquo; obtain a sorted array. Its time complexity is $O(n)$, and the pseudocode is as follows. FUNCTION stalinSort(A : list OF sortable items) n := length(A) bigger := 0 B SET</description></item><item><title>Using Tuples for Indexing in Python</title><link>https://freshrimpsushi.github.io/en/posts/3299/</link><pubDate>Wed, 21 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3299/</guid><description>2D Arrays Let&amp;rsquo;s call $A$ a 2D array. Then $A[(y,x)]$ and $A[y,x]$ perform the same function. &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; A = np.arange(16).reshape(4,4) &amp;gt;&amp;gt;&amp;gt; A array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) &amp;gt;&amp;gt;&amp;gt; A[2,3] 11 &amp;gt;&amp;gt;&amp;gt; A[(2,3)] 11 &amp;gt;&amp;gt;&amp;gt; Using a tuple of tuples allows you to reference multiple specific indexes. It&amp;rsquo;s important to note that</description></item><item><title>Satterthwaite Approximation</title><link>https://freshrimpsushi.github.io/en/posts/2267/</link><pubDate>Tue, 20 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2267/</guid><description>Buildup Let&amp;rsquo;s assume that we have $n$ independent random variables $Y_{k} \sim \chi_{r_{k}}^{2}$, each following a chi-squared distribution with degrees of freedom $r_{k}$. As is well-known, the sum of these, $\sum_{k=1}^{n} Y_{k}$, follows a chi-squared distribution with degrees of freedom $\sum_{k=1}^{n} r_{k}$. This insight can be particularly useful when looking at the denominator of $\displaystyle {{W} \over {\sqrt{V / r}}}$, which follows a t-distribution. Unfortunately, there&amp;rsquo;s an issue when this</description></item><item><title>Homogeneity of Geodesics</title><link>https://freshrimpsushi.github.io/en/posts/3298/</link><pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3298/</guid><description>I&amp;rsquo;m sorry for any confusion, but it seems like the provided prompt does not contain any specific Korean document for translation into English and Japanese. Can you please provide the content you wish to have translated?</description></item><item><title>How to Define Variadic Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2266/</link><pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2266/</guid><description>Overview 1 A Varargs Function, commonly mentioned in programming, is a function that can receive an unlimited number of arguments. In Julia, you can simply set a variable to accept variadic arguments by appending ... after it. Let&amp;rsquo;s understand this with an example code. Additionally, this ... is called splat operator2. Code Isaac Newton famously discovered that adding the reciprocals of factorials simply converges to $e$ with the following theorem.</description></item><item><title>Geodesic Flow</title><link>https://freshrimpsushi.github.io/en/posts/3296/</link><pubDate>Sat, 17 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3296/</guid><description>죄송하지만, 제공된 한국어 문서의 내용이 보이지 않습니다. 정확한 내용을 제공해 주시면, 해당 내용을 영어와 일본어로 번역하는데 도움을 줄 수 있겠습니다.</description></item><item><title>Location-Scale Family Auxiliary Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2265/</link><pubDate>Fri, 16 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2265/</guid><description>Theorem 1 Let $X_{1} , \cdots , X_{n}$ be a random sample from both a location family and a scale family. If the two statistics $T_{1} \left( X_{1} , \cdots, X_{n} \right)$ and $T_{2} \left( X_{1} , \cdots , X_{n} \right)$ satisfy $$ T_{i} \left( a x_{1} + b , \cdots , a x_{n} + b \right) = a T_{i} \left( x_{1} , \cdots , x_{n} \right) $$ for all</description></item><item><title>How to Draw a Histogram in Excel</title><link>https://freshrimpsushi.github.io/en/posts/3297/</link><pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3297/</guid><description>Overview This introduces how to draw a histogram through Excel. Description Click on the [File] tab, then click on [Options] below to bring up the [Excel Options] window. In [Add-Ins], press [Go] at [Analysis ToolPak] and then press [OK]. Now you can confirm that the [Data Analysis] item has been added to the [Data] tab. Press it and select [Histogram]. Choose the range of values and the intervals of the</description></item><item><title>How to Check the Element Type Inside a Julia Container</title><link>https://freshrimpsushi.github.io/en/posts/2264/</link><pubDate>Wed, 14 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2264/</guid><description>Overview To achieve this, use the eltype() function. It likely gets its name from element type. Code julia&amp;gt; set_primes = Set([2,3,5,7,11,13]) Set{Int64} with 6 elements: 5 13 7 2 11 3 julia&amp;gt; arr_primes = Array([2,3,5,7,11,13]) 6-element Vector{Int64}: 2 3 5 7 11 13 Consider two types of containers that hold prime numbers up to $13$. Honestly, they contain the same data, but one is a set while the other is</description></item><item><title>The Equivalence Condition When the Range of a Linear Transformation is Smaller than the Kernel</title><link>https://freshrimpsushi.github.io/en/posts/3295/</link><pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3295/</guid><description>Theorem1 Let $V$ be a vector space, and $T : V \to V$ be a linear transformation. Then, the following holds: $$ T^{2} = T_{0} \iff R(T) \subset N(T) $$ Here, $T_{0}$ is the zero transformation, and $R(T), N(T)$ are the respective range and null space of $T$. Generalization Let $U, V, W$ be a vector space, and $T_{1} : U \to V$, $T_{2} : V \to W$ be linear</description></item><item><title>The Variance of an Unbiased Estimator Given a Sufficient Statistic is Minimized</title><link>https://freshrimpsushi.github.io/en/posts/2263/</link><pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2263/</guid><description>Theorem 1 Let&amp;rsquo;s say we have a parameter $\theta$. $U$ is an unbiased estimator, $T_{1}$ is a sufficient statistic, and $T_{2}$ is a minimal sufficient statistic, defined as follows: $$ \begin{align*} U_{1} :=&amp;amp; E \left( U | T_{1} \right) \\ U_{2} :=&amp;amp; E \left( U | T_{2} \right) \end{align*} $$ it holds that: $$ \Var U_{2} \le \Var U_{1} $$ Explanation Whether $T_{1}$ or $T_{2}$ is given, $U$ being an</description></item><item><title>Geodesics on a Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3294/</link><pubDate>Sun, 11 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3294/</guid><description>Definition1 On the manifold $M$, for a curve $\gamma : I \to M$ at point $t_{0} \in I$, if $\dfrac{D}{dt}\left( \dfrac{d \gamma}{d t} \right) = 0$, then $\gamma$ is a geodesic at $t_{0}$. If for all points $t \in I$, $\gamma$ is a geodesic at $t$, then $\gamma$ is called a geodesic. If $[a,b] \subset I$ and $\gamma : I \to M$ is a geodesic, a contraction mapping $\gamma|_{[a,b]}$ connecting</description></item><item><title>How to Change the Basic Settings of a Julia Plot</title><link>https://freshrimpsushi.github.io/en/posts/2262/</link><pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2262/</guid><description>Code You can use the default() function. using Plots default(size = (400,400), color = :red) default(:size, (400,400)) for key in [:size, :color], value in [(400,400), :red] default(key, value) end There is a way to set it up like the ordinary plot() function, and there is a way to change them one by one by giving key and value. Usually, the former is more convenient, but in the case of very</description></item><item><title>Dual Pair Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3293/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3293/</guid><description>Definition1 Let us call $X$ a vector space. Let $X^{\ast\ast}$ be the dual space of $X$&amp;rsquo;s dual space, $X^{\ast}$. $$ X^{\ast\ast} = (X^{\ast})^{\ast} $$ This is called the bidual space of $X$. Theorem If $X$ is a finite-dimensional vector space, then $X$ and $X^{\ast\ast}$ are isomorphic. $$ X \approx X^{\ast\ast} $$ Explanation Bidual, double dual, and second dual all mean the same thing. The above theorem holds only when $X$</description></item><item><title>Complete Statistics of the Exponential Family of Probability Distributions</title><link>https://freshrimpsushi.github.io/en/posts/2261/</link><pubDate>Thu, 08 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2261/</guid><description>Theorem 1 Given a parameter $\mathbf{\theta} = \left( \theta_{1} , \cdots , \theta_{k} \right)$ and the probability density function or probability mass function of a random sample $X_{1} , \cdots , X_{n}$ follows an exponential family distribution as shown below. $$ f(x; \mathbf{\theta}) = h(x) c (\mathbf{\theta}) \exp \left( \sum_{i=1}^{k} w_{i} \left( \theta_{j} \right) t_{i} (x) \right) $$ Then the following statistic $T$ is a complete statistic. $$ T \left(</description></item><item><title>Levi-Civita Connection, Riemannian Connection, Coefficients of Connection, Christoffel Symbols</title><link>https://freshrimpsushi.github.io/en/posts/3292/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3292/</guid><description>Theorem1 Let $(M,g)$ be a Riemannian manifold. Then, there uniquely exists an affine connection $\nabla$ on $M$ satisfying the following: $\nabla$ is symmetric. $\nabla$ is compatible with $g$. Such $\nabla$ specifically satisfies the following equation: $$ \begin{align*} g(Z, \nabla_{Y}X) =&amp;amp;\ \dfrac{1}{2}\Big( X g(Y, Z) + Y g(Z, X) - Z g(X, Y) \\ &amp;amp;\ - g([X, Z], Y) - g([Y, Z], X) - g([X, Y], Z) \Big) \tag{1} \end{align*} $$</description></item><item><title>How to Remove Specific Rows from a DataFrame in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2260/</link><pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2260/</guid><description>Overview When indexing, you can use the Not() function1. If you input the symbol of the column name as is, or an array of symbols, those columns are excluded from the indexing. Code using DataFrames WJSN = DataFrame( member = [&amp;#34;다영&amp;#34;,&amp;#34;다원&amp;#34;,&amp;#3</description></item><item><title>Transpose of Linear Transformations Defined by Dual Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3291/</link><pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3291/</guid><description>Theorem1 Let&amp;rsquo;s denote the ordered bases of two finite-dimensional vector spaces $V, W$ as $\beta, \gamma$, respectively. For any linear transformation $T : V \to W$, the following defined function $U$ is a linear transformation and satisfies $[U]_{\gamma^{\ast}}^{\beta^{\ast}} = ([T]_{\beta}^{\gamma})^{t}$. $$ U : W^{\ast} \to V^{\ast} \quad \text{ by } \quad U(g) = gT \quad \forall g \in W^{\ast} $$ Here, $[T]_{\beta}^{\gamma}$ is the matrix representation of $T$, ${}^{t}$ is</description></item><item><title>Moment Method</title><link>https://freshrimpsushi.github.io/en/posts/2259/</link><pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2259/</guid><description>Definition 1 When the parameters of a given distribution are unknown, the method of forming simultaneous equations for the parameters using moments and considering the solution to these equations as estimates of the parameters is known as the Moment Method. Description The moment method has been a point estimation technique used for a long time since the 1800s by Karl Pearson and others. Although it has not always produced very</description></item><item><title>Symmetry of Connection</title><link>https://freshrimpsushi.github.io/en/posts/3290/</link><pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3290/</guid><description>Definition1 An affine connection $\nabla$ on a differentiable manifold $M$ is said to be symmetric if it satisfies the following. $$ \nabla_{X}Y - \nabla_{Y} X = \left[ X, Y \right] \quad \forall X,Y \in \mathfrak{X}(M) $$ Here $\mathfrak{X}(M)$ is the set of vector fields on $M$, and $[ \cdot, \cdot]$ is the Lie bracket. Explanation Let&amp;rsquo;s take Euclidean space as an example. Consider a coordinate system $(U, \mathbf{x})$ of $\mathbb{R}^{n}$.</description></item><item><title>How to Insert Vertical and Horizontal Lines in Figures in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2258/</link><pubDate>Fri, 02 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2258/</guid><description>Overview To draw vertical and horizontal lines, use the vline!() and hline!() functions respectively. Code @time using Plots plot(rand(100)) hline!([0.5], linewidth = 2) vline!([25, 75], linewidth = 2) png(&amp;#34;result&amp;#34;) The positions where lines are drawn should be passed as an array. If the array contains multiple elements, multiple lines will be drawn at once. Environment OS: Windows julia: v1.6.3</description></item><item><title>Linear Transformation Spaces and Their Matrix Representation Spaces are Isomorphic</title><link>https://freshrimpsushi.github.io/en/posts/3289/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3289/</guid><description>Theorem1 Let&amp;rsquo;s assume that two vector spaces $V, W$ have dimensions $n, m$, respectively. And let $\beta, \gamma$ be the ordered bases for each. Then the function defined as follows $\Phi$ is an isomorphism. $$ \Phi : L(V, W) \to M_{m\times n}(\mathbb{R}) \quad \text{ by } \quad \Phi (T) = [T]_{\beta}^{\gamma} $$ $[T]_{\beta}^{\gamma}$ is the matrix representation of $T$. Corollary A necessary and sufficient condition for a linear transformation to</description></item><item><title>Proof of Bézout's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2257/</link><pubDate>Wed, 31 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2257/</guid><description>Theorem If $T \left( \mathbf{X} \right)$ is a complete statistic as well as a minimal sufficient statistic, then $T \left( \mathbf{X} \right)$ is independent of all ancillary statistics. Description Basu&amp;rsquo;s theorem is one of the most important results, among those related to sufficient statistics, allowing for a very strong conclusion that certain statistics are independent. Intuitively, a sufficient statistic contains all the information about a parameter $\theta$, and since ancillary</description></item><item><title>Coexistence Compatible Connection</title><link>https://freshrimpsushi.github.io/en/posts/3288/</link><pubDate>Tue, 30 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3288/</guid><description>Definition1 Let&amp;rsquo;s assume that an affine connection $M$ and a Riemannian metric $\nabla$ are given on a differentiable manifold. For all differentiable curves $g$, if any parallel vector fields $c$ along any two $g$ satisfy $c$, then the connection $M$ is said to be compatible with the metric $\nabla$. Explanation The following corollary is sometimes stated as the definition of compatibility. The definition given above is easy to conceptualize because</description></item><item><title>How to Create Art Styles in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2256/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2256/</guid><description>Overview RecipesBase.jl is a package that allows users to create their own styles for new plots, similar to how ggplot works in the R programming language, with its own unique syntax1 separate from the base Julia. Let&amp;rsquo;s learn through examples. Code using Plots using DataFrames df = DataFrame(x = 1:10, y = rand(10)) plot(df) @userplot TimeEvolution @recipe function f(te::TimeEvolution) df = te.args[1] linealpha --&amp;gt; 0.5 column_names = names(df) for (column_index,</description></item><item><title>Homomorphism</title><link>https://freshrimpsushi.github.io/en/posts/3287/</link><pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3287/</guid><description>Definition1 For two vector spaces $V, W$, if there exists an invertible linear transformation $T : V \to W$, then $V$ is said to be isomorphic to $W$, and is denoted as follows. $$ V \cong W $$ Furthermore, $T$ is called an isomorphism. Explanation By the equivalence condition of being invertible, saying $T$ is an isomorphism means that $T$ is a bijective function. Therefore, if there exists a bijective</description></item><item><title>Sufficient Statistic</title><link>https://freshrimpsushi.github.io/en/posts/2255/</link><pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2255/</guid><description>Definition 1 Let us define the set of parameters as $\Omega$. The family $\left\{ f \left( t ; \theta \right) : \theta \in \Theta \right\}$ that collects all probability density functions or probability mass functions $f \left( t ; \theta \right)$ of the statistic $T := T \left( \mathbf{X} \right)$ from the sample $\mathbf{X}$, $$ \forall \theta, E_{\theta} g (T) = 0 \implies \forall \theta, P_{\theta} \left( g(T) = 0</description></item><item><title>Parallel Vector Fields on Differential Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3286/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3286/</guid><description>Definition1 Let us consider a differentiable manifold with given affine connection $M$ and $\nabla$. A vector field $V$ following the curve $c : I \to M$ is said to be parallel if it satisfies the following condition. $$ \dfrac{DV}{dt} = 0,\quad \forall i \in I $$ Theorem Let us consider a differentiable manifold with given affine connection $M$ and $\nabla$. Let $c : T \to M (t\in I)$ be a</description></item><item><title>grouping and Calculating DataFrames in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2254/</link><pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2254/</guid><description>Overview Using groupby() to divide by group and combine() for calculation is the way to go1. groupby(df, :colname) Returns a groupedDataFrame based on :colname. combine(gdf, :colname =&amp;gt; fun) gdf is a groupedDataFrame divided by groups. :colname =&amp;gt; fun represents a pair of the symbol :colname, which is the name of the column containing the values to be calculated, and the calculation function fun. Code using DataFrames using StatsBase WJSN =</description></item><item><title>Inverse of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3285/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3285/</guid><description>Definition1 Let $V, W$ be a vector space, and $T : V \to W$ be a linear transformation. If the linear transformation $U : W \to V$ satisfies the following, then $U$ is called the inverse or inverse transformation of $T$. $$ TU = I_{W} \quad \text{and} \quad UT = I_{V} $$ $TU$ is the composition of $U$ and $T$, $I_{X} : X \to X$ is the identity transformation. If</description></item><item><title>Scale Families</title><link>https://freshrimpsushi.github.io/en/posts/2253/</link><pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2253/</guid><description>Definition The cumulative distribution function $F$ is said to satisfy $F_{\sigma}$ for all $x$ if $F_{\sigma} (x) = F \left( x / \sigma \right)$ holds. $\left\{ F_{\sigma} : \sigma &amp;gt; 0 \right\}$ is called a Scale Family. Example 1 Consider a random sample $X_{1} , \cdots , X_{n}$ with parameter $\sigma$ having a cumulative distribution function $F_{1} (x) = F ( x / 1) = F(x)$, then for the random</description></item><item><title>Covariant Derivative of Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/3284/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3284/</guid><description>Theorem1 Let $M$ be a differentiable manifold, and let $\nabla$ be an affine connection on $M$. Then, there exists a unique function $\dfrac{D}{dt} : V \mapsto \dfrac{DV}{dt}$ that maps a vector field $V$ following a differentiable curve $c : I \to M(t\in I)$ to another vector field $\dfrac{D V}{dt}$ following $c$. Such a mapping is referred to as the covariant derivative of $V$ along $c$, and it has the following</description></item><item><title>How to Delete Duplicate Rows in DataFrames in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2252/</link><pubDate>Sun, 21 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2252/</guid><description>Overview To achieve this, we can use unique(). More precisely, it leaves only one of the duplicates rather than deleting duplicated rows. Code using DataFrames WJSN = DataFrame( member = [&amp;#34;다영&amp;#34;,&amp;#34;다원&amp;#34;,&amp;#34;루다&amp;#34;,</description></item><item><title>Linear Transformation Space</title><link>https://freshrimpsushi.github.io/en/posts/3283/</link><pubDate>Sat, 20 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3283/</guid><description>Definition1 The set of all linear transformations from the vector space $V$ to $W$ is denoted as $L(V,W)$. $$ L(V, W) = \mathcal{L}(V, W) := \left\{ T : V \to W\enspace |\enspace T \text{ is linear } \right\} $$ This is also expressed as follows, referred to as the homomorphism space. $$ \operatorname{Hom}(V,W) = L(V, W) = \left\{ T : V \to W \text{ is linear} \right\} $$ Additionally, when</description></item><item><title>Location Family</title><link>https://freshrimpsushi.github.io/en/posts/2251/</link><pubDate>Fri, 19 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2251/</guid><description>Definition For a given cumulative distribution function $F$, suppose $F_{\theta}$ satisfies $F_{\theta} (x) = F \left( x - \theta \right)$ for all $x$. $\left\{ F_{\theta} : \theta \in \mathbb{R} \right\}$ is referred to as a Location Family. Example 1 Considering a random sample $X_{1} , \cdots , X_{n}$ with parameter $\theta$ that possesses a cumulative distribution function $F_{0} (x) = F (x - 0) = F(x)$, the sample $Z_{1} ,</description></item><item><title>Affine Connection</title><link>https://freshrimpsushi.github.io/en/posts/3282/</link><pubDate>Thu, 18 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3282/</guid><description>Buildup Given a vector field $\mathbf{V}$ on a differentiable manifold, we can differentiate functions defined on the manifold using the vector field. Naturally, one might also want to differentiate the vector field itself. However, approaching the differentiation of the vector field $\mathbb{R}^{3}$ in the sense of differential geometry proves to be impossible as follows. First Case Let&amp;rsquo;s consider $S \subset \mathbb{R}^{3}$ as a surface and $c : I \to S$</description></item><item><title>Drawing Subplots with a Layout in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2250/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2250/</guid><description>Overview In Julia, options related to subplots can be controlled through the layout option. Entering an integer automatically creates a grid of that many plots. Entering a 2-tuple of integers creates a grid exactly as specified. The @layout macro is used to create complex layouts of the Plots.GridLayout type. Code using Plots left = plot(randn(100), color = :red) right = plot(randn(100), color = :blue) plot(left, right) png(&amp;#34;easyone&amp;#34;) data = rand(10,</description></item><item><title>Linear Functional</title><link>https://freshrimpsushi.github.io/en/posts/3281/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3281/</guid><description>Definitions1 Let&amp;rsquo;s call $V$ a vector space. A mapping $f$ from $V$ to $\mathbb{C}$ (or $\mathbb{R}$) is called a functional. $$ f : V \to \mathbb{C} $$ If $f$ is linear, it is called a linear functional. More Detailed Definitions2 Let&amp;rsquo;s call $V$ a vector space over the field $F$. Here, the field $F$ itself becomes a $1$-dimensional vector space over $F$. A linear transformation $f : V \to F$</description></item><item><title>Auxiliary Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2249/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2249/</guid><description>Definition 1 Let $S$ be a statistic of sample $\mathbf{X}$. If the distribution of $S \left( \mathbf{X} \right)$ does not depend on the parameter $\theta$, it is called an Ancillary Statistic. Description Actually, nobody says ancillary statistic in conversation, they pronounce it as [ancillary statistic]. If a sufficient statistic has all the information about $\theta$, then an ancillary statistic can be thought of as a statistic that has no information</description></item><item><title>Vector Fields Along Curves on Differential Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3280/</link><pubDate>Sun, 14 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3280/</guid><description>Definition1 Let $M$ be a differentiable manifold. A $c : I\subset \mathbb{R} \to M$ that is a (parameterized) curve is called a differentiable function. A differentiable $V$ that satisfies the following is called a vector field along the curve $c : I \to M$. Being differentiable means that for a differentiable function $f$ on $M$, the function $t \mapsto V(t)f$ is differentiable on $I$. $$ V : I \to T_{c(t)}M</description></item><item><title>Adjusting the Position of the Legend in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/2248/</link><pubDate>Sat, 13 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2248/</guid><description>Overview 1 The position of the legend can be freely adjusted with the legend option of the plot() function. Giving a 2-tuple comprised of values between $0$ and $1$ will exactly place it at that location, otherwise, it can be controlled by symbols. Symbols combine top/bottom and left/right in order. Adding an outer at the very beginning places the legend outside of the plot. Examples of symbols that can be</description></item><item><title>Order Basis and Coordinate Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3279/</link><pubDate>Fri, 12 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3279/</guid><description>Definition1 Let&amp;rsquo;s say $V$ is a finite-dimensional vector space. When a specific order is assigned to a basis of $V$, it is called an ordered basis. Let&amp;rsquo;s say $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ is an ordered basis of $V$. Then, due to the uniqueness of basis representation, for $\mathbf{v} \in V$, scalars $a_{i}$ uniquely exist as follows. $$ \mathbf{v} = a_{1}\mathbf{v}_{1} + \dots a_{n}\mathbf{v}_{n} $$ $a_{1},\dots,a_{n}$ is called</description></item><item><title>Minimum Sufficient Statistic</title><link>https://freshrimpsushi.github.io/en/posts/2247/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2247/</guid><description>Definition 1 Let $T \left( \mathbf{X} \right)$ be a sufficient statistic. If for every other sufficient statistic $T ' \left( \mathbf{X} \right)$, $T \left( \mathbf{x} \right)$ can be expressed as a function of $T ' \left( \mathbf{x} \right)$, then $T \left( \mathbf{X} \right)$ is called a Minimal Sufficient Statistic. Theorem Let $f \left( \mathbf{x} ; \theta \right)$ be the probability density function or probability mass function of a sample $\mathbf{X}$.</description></item><item><title>Isometries and Local Isometries on Riemann Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3278/</link><pubDate>Wed, 10 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3278/</guid><description>Isometry Given a Riemannian manifold $(M, g), (N, h)$, a diffeomorphism $f : M \to N$ is called an isometry if the following holds for $f$: $$ \begin{equation} g(u, v)_{p} = h\left( df_{p}(u), df_{p}(v) \right)_{f(p)},\quad \forall p\in M,\quad u,v\in T_{p}M \end{equation} $$ or $$ \left\langle u, v \right\rangle_{p} = \left\langle df_{p}(u), df_{p}(v) \right\rangle_{f(p)},\quad \forall p\in M,\quad u,v\in T_{p}M $$ Here $df_{p} : T_{p}M \to T_{f(p)}N$ is the derivative of $f$.</description></item><item><title>How to Adjust the Aspect Ratio of a Julia Set Picture</title><link>https://freshrimpsushi.github.io/en/posts/2246/</link><pubDate>Tue, 09 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2246/</guid><description>Overview 1 To adjust the width and height of the figure, you can include the ratio option. Other recommended aliases include aspect_ratios, axis_ratio. ratio = :none: The default value, where the figure&amp;rsquo;s size is adjusted to fit the ratio. ratio = :equal: Regardless of the figure&amp;rsquo;s size, the x and y axes are adjusted to a one-to-one ratio. ratio = Number: The ratio is adjusted according to Number. Number is</description></item><item><title>Linear Transformations Between Finite-Dimensional Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3277/</link><pubDate>Mon, 08 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3277/</guid><description>Theorem1 Let $V, W$ be a vector space. Let $\left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ and $\left\{ \mathbf{w}_{1}, \mathbf{w}_{2}, \dots, \mathbf{w}_{n} \right\}$ be bases of $V, W$, respectively. Then there exists a unique linear transformation $T : V \to W$ that satisfies $T(\mathbf{v}_{i}) = \mathbf{w}_{i}$. Corollary2 Let $V, W$ be a vector space. Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ be a basis of $V$. If $U, T</description></item><item><title>Perron-Frobenius Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2245/</link><pubDate>Sun, 07 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2245/</guid><description>Theorem Auxiliary Definitions To apply a permutation on a matrix literally means to change the order of rows and columns. A matrix $A$ is reducible if, after applying a permutation to $\tilde{A}$, it can be represented in the form of a Jordan block with respect to some square matrices $B, D$, a zero matrix $O$, and matrix $C$1. If not, it is said to be irreducible. $$ \tilde{A} = \begin{bmatrix}</description></item><item><title>Riemann Metric and Riemann Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3276/</link><pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3276/</guid><description>Definition1 A Riemannian metric $g$ on a $n$-dimensional differentiable manifold $M$ is a function that maps each point $p \in M$ to $g_{p}$. Here, $g_{p}$ is an inner product defined in the tangent space $p$ over $T_{p}M$. $$ \begin{align*} g : M &amp;amp;\to \left\{ \text{all inner products on tangent space } T_{p}M \right\} \\ p &amp;amp;\mapsto g_{p}=\left\langle \cdot, \cdot \right\rangle_{p} \end{align*} $$ $$ \begin{align*} g_{p} : T_{p}M \times T_{p}M &amp;amp;\to</description></item><item><title>Solving Broken Characters when Exporting CSV in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2244/</link><pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2244/</guid><description>Error using DataFrames, CSV example = DataFrame(x = 1:10, 가 = &amp;#34;나다&amp;#34;) CSV.write(&amp;#34;example.csv&amp;#34;, example) When outputting to a CSV file in Julia, you can see a phenomenon where the Korean text becomes garbled, as shown above. Cause The garbling isn&amp;rsquo;t actually due to the Korean text itself but a Unicode encoding issue, especially due to the UTF-8 encoding&amp;rsquo;s</description></item><item><title>Linear Transformation Trace</title><link>https://freshrimpsushi.github.io/en/posts/3275/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3275/</guid><description>Definition Let $V$ be a $n$-dimensional vector space. Let $f : V \to V$ be a linear transformation. Let $B = \left\{ e_{i} \right\}$ be a basis of $V$. Let $n \times n$ matrix $A$ be the matrix representation of $f$ with respect to $B$. $$ A = [f]_{B} $$ Since $f(e_{i}) \in V$, we represent it as $f(e_{i}) = \sum f_{j}(e_{i})e_{j}$. Then, $$ A = \begin{bmatrix} f_{1}(e_{1}) &amp;amp; f_{2}(e_{1})</description></item><item><title>Product of Indicator Functions</title><link>https://freshrimpsushi.github.io/en/posts/2243/</link><pubDate>Wed, 03 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2243/</guid><description>Theorem For $x_{1} , \cdots , x_{n} \in \mathbb{R}$ and constant $\theta \in \mathbb{R}$, the product of $I_{\cdot} \left( x_{i} \right)$ is as follows: $$ \prod_{i=1}^{n} I_{[\theta,\infty)} \left( x_{i} \right) = I_{[\theta,\infty)} \left( \min_{i \in [n]} x_{i} \right) $$ $I_{A}$ is the indicator function for the set $A$. $$ I_{A} (x) = \begin{cases} 1 &amp;amp; , x \in A \\ 0 &amp;amp; , x \notin A \end{cases} $$ Proof Regardless</description></item><item><title>- Text Formatting Package in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2242/</link><pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2242/</guid><description>Overview The Crayons.jl package is known for decorating text output in Julia1. If you want to decorate using only built-in functions, you can use printstyled(). Code using Crayons print(Crayon(background = :red), &amp;#34;빨강&amp;#34;) print(Crayon(foreground = :blue), &amp;#34;파랑&amp;#34;) print(Crayon(bold = true), &amp;#34;볼드</description></item><item><title>Adding a New Column to a DataFrame in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3273/</link><pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3273/</guid><description>Code Let&amp;rsquo;s say we are given the Cosmic Girls dataframe as follows. WJSN = DataFrame( member = [&amp;#34;다영&amp;#34;,&amp;#34;다원&amp;#34;,&amp;#34;루다&amp;#34;,&amp;#34;소정&amp;#34;,</description></item><item><title>Summarizing Inequalities in the Form of an Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2241/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2241/</guid><description>Theorem Let $x_{1} , \cdots , x_{n}$ and positive $a_{1} , \cdots , a_{n} &amp;gt; 0$ along with constant $\theta \in \mathbb{R}$ be given. $$ \forall i \in [n] : x_{i} &amp;lt; a_{i} \theta \iff \max_{i \in [n]} {{ x_{i} } \over { a_{i} }} &amp;lt; \theta $$ Proof The fact that $(\implies)$ holds for all $i \in [n]$ implies that even the largest $x_{i} / a_{i}$ is smaller than</description></item><item><title>Lie Brackets of Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/3272/</link><pubDate>Fri, 29 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3272/</guid><description>Definition1 On two differentiable vector fields $X, Y$ on a differentiable manifold $M$, $[X, Y]$ is defined as follows, and is called the (Lie-)bracket or Lie algebra. $$ \begin{equation} [X, Y] := XY - YX \end{equation} $$ Explanation Vector field $X, Y$ can be seen as an operator acting on $\mathcal{D}(M)$, and $XY$ although not a vector field, $[X, Y] = XY - YX$ becomes a vector field. $(1)$ satisfying</description></item><item><title>Inserting a Line into a Julia Set Image</title><link>https://freshrimpsushi.github.io/en/posts/2240/</link><pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2240/</guid><description>Code using Plots scatter(rand(100), randn(100)) plot!([0,1],[0,1]) png(&amp;#34;example1&amp;#34;) plot!([.00,.25,.50],[-2,0,-2]) png(&amp;#34;example2&amp;#34;) θ = 0:0.01:2π plot!(.5 .+ cos.(θ)/3, 1.5sin.(θ)) png(&amp;#34;example3&amp;#34;) Let&amp;rsquo;s learn how to insert line segments into the diagram by executing the code above. Line Segment plot!([0,1],[0,1]) Whether you draw just one line segment or</description></item><item><title>Properties of Full Rank Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3271/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3271/</guid><description>Theorem1 Let&amp;rsquo;s refer to $A$ as matrix $m \times n$. Then, the necessary and sufficient condition for $A$ to have a full rank is for $A^{T}A$ to be an invertible matrix. Proof $(\Longrightarrow)$ Assume that $A$ has a full rank. Since $A^{T}A$ is a square matrix $n \times n$, showing that the linear system $A^{T}A \mathbf{x} = \mathbf{0}$ only has the trivial solution, according to the equivocal condition of being</description></item><item><title>Definition of Likelihood Function</title><link>https://freshrimpsushi.github.io/en/posts/2239/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2239/</guid><description>Definition 1 Let&amp;rsquo;s denote the joint probability density function or probability mass function of a sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right)$ as $f(\mathbf{x}|\theta)$. When a realization $\mathbf{x}$ is given, regarding $f(\mathbf{x}|\theta)$ as a function of $\theta$ $$ L \left( \theta | \mathbf{x} \right) := f \left( \mathbf{x} | \theta \right) $$ is called the Likelihood Function. Explanation In the context of discussing maximum likelihood estimators, it</description></item><item><title>Vector Field on Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3270/</link><pubDate>Mon, 25 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3270/</guid><description>Buildup1 Consider the easy definition of a vector field. In 3-dimensional space, a vector field is a function $X : \mathbb{R}^{3} \to \mathbb{R}^{3}$ that maps a 3-dimensional vector to another 3-dimensional vector. When considering this in the context of manifolds, $X$ maps a point $\mathbb{R}^{3}$ on the differential manifold $p$ to a vector $\mathbb{R}^{3}$ in $\mathbf{v}$, treating this vector $\mathbf{v}$ as an operator to consider as a directional derivative (=</description></item><item><title>How to Sort Dataframe in julia</title><link>https://freshrimpsushi.github.io/en/posts/2238/</link><pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2238/</guid><description>Code using DataFrames Unit1 = DataFrame( member = [&amp;#34;다영&amp;#34;,&amp;#34;루다&amp;#34;,&amp;#34;수빈&amp;#34;,&amp;#34;진숙&amp;#34;], birth = [99,97,96,99], height = [161,157,159,162] ) Unit2</description></item><item><title>Fundamental Spaces of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3269/</link><pubDate>Sat, 23 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3269/</guid><description>Explanation1 Let&amp;rsquo;s assume that the matrix $A$ is given. Then, we can think of the following 6 spaces for $A$. Row space of $A$, Row space of $A^{T}$ Column space of $A$, Column space of $A^{T}$ Null space of $A$, Null space of $A^{T}$ However, since the row vectors of $A$ are the column vectors of $A^{T}$, and the column vectors of $A$ are the row vectors of $A^{T}$, the</description></item><item><title>Delta Method in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2237/</link><pubDate>Fri, 22 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2237/</guid><description>Theorem Let&amp;rsquo;s assume that the constant $\theta \in \mathbb{R}$ and the sequence of random variables $\left\{ Y_{n} \right\}_{n \in \mathbb{N}}$ converge in distribution to a normal distribution $\sqrt{n} \left( Y_{n} - \theta \right)$ following $N \left(0, \sigma^{2} \right)$. $1$-Order Delta Method 1 If $g ' (\theta) \ne 0$ exists, $$ \sqrt{n} \left[ g \left( Y_{n} \right) - g(\theta) \right] \overset{D}{\to} N \left( 0, \sigma^{2} \left[ g ' (\theta) \right]^{2} \right)</description></item><item><title>Tangent Bundles on Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3268/</link><pubDate>Thu, 21 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3268/</guid><description>Definition1 Let&amp;rsquo;s call a $M$ a $n$-dimensional differentiable manifold. Let&amp;rsquo;s denote the tangent space at point $p \in M$ as $T_{p}M$. The tangent bundle $TM$ of $M$ is defined as follows. $$ \begin{align*} TM &amp;amp;:= \bigsqcup \limits_{p \in M } T_{p}M \\ &amp;amp;= \bigcup_{p \in M} \left\{ p \right\} \times T_{p}M \\ &amp;amp;= \left\{ (p, v) : p \in M, v \in T_{p}M \right\} \end{align*} $$ Here, $\bigsqcup$ is a</description></item><item><title>Inserting a New Row into a DataFrame in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2236/</link><pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2236/</guid><description>Code using DataFrames Unit1 = DataFrame( member = [&amp;#34;다영&amp;#34;,&amp;#34;루다&amp;#34;,&amp;#34;수빈&amp;#34;,&amp;#34;진숙&amp;#34;], birth = [99,97,96,99], height = [161,157,159,162] ) Unit2</description></item><item><title>Lagrange Multiplier Method</title><link>https://freshrimpsushi.github.io/en/posts/3267/</link><pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3267/</guid><description>Definition1 The optimal value of the multivariable function $f(x_{1}, \dots, x_{n})$ Description Let&amp;rsquo;s assume the graph of $y = 2 - x^{2}$ is given as shown in the figure above. Let&amp;rsquo;s denote the distance between the origin and the graph as $d$. $$ d(x,y) = \sqrt{x^{2} + y^{2}} $$ Then, the problem of finding the point that minimizes the distance $d$ is the same as finding the point where the</description></item><item><title>Statistical Proof of Stirling's Formula</title><link>https://freshrimpsushi.github.io/en/posts/2235/</link><pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2235/</guid><description>정리 $$ \lim_{n \to \infty} {{n!} \over {e^{n \ln n - n} \sqrt{ 2 \pi n} }} = 1 $$ 설명 The Stirling approximation, or Stirling&amp;rsquo;s formula, is usefully applied in various fields, including statistics and physics. If one is well-versed in probability theory, the mathematical statistical proof can be more intuitive and easier to understand compared to other proofs. 같이보기 Proof using Gaussian integral</description></item><item><title>Emulsions are locally embedded.</title><link>https://freshrimpsushi.github.io/en/posts/3266/</link><pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3266/</guid><description>3173) An immersion is locally embedded. Theorem1 Proof Assuming that $\phi$ is an immersion, it follows that $d\phi{p}$ is injective. To demonstrate embedding, it is necessary for $\phi|_{V}$ and $(\phi|_{V})^{-1}$ to be bijections, hence only up to $\mathbb{R}^{m}$ coordinates are considered for $$ \begin{bmatrix} \dfrac{\partial y_{1}}{\partial x_{1}} &amp;amp; \dfrac{\partial y_{1}}{\partial x_{2}} &amp;amp; \dots &amp;amp; \dfrac{\partial y_{1}}{\partial x_{n}} \\[1em] \dfrac{\partial y_{2}}{\partial x_{1}} &amp;amp; \dfrac{\partial y_{2}}{\partial x_{2}} &amp;amp; \dots &amp;amp; \dfrac{\partial y_{2}}{\partial</description></item><item><title>Using Infinity in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2234/</link><pubDate>Sat, 16 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2234/</guid><description>Overview Infinities.jl is a package that aids in using infinity symbols in Julia1. Surprisingly, infinity is quite useful in scientific computing coding. Code julia&amp;gt; 8 &amp;lt; Inf true The reason it&amp;rsquo;s mentioned that it helps in using infinity symbols, not just infinity, in the introduction is that you can actually use them without the package. julia&amp;gt; using Infinities julia&amp;gt; 8 &amp;lt; ∞ true julia&amp;gt; -∞ &amp;lt; 8 true julia&amp;gt;</description></item><item><title>Supersaturated and Undersaturated Systems</title><link>https://freshrimpsushi.github.io/en/posts/3265/</link><pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3265/</guid><description>Definition1 Consider the following linear system for matrix $A$: $$ A \mathbf{x} = \mathbf{b} $$ If $m \gt n$, then there are more constraints than unknowns, and such a linear system is called an overdetermined system. If $m \lt n$, then there are less constraints than unknowns, and such a linear system is called an underdetermined system. Theorem 1 Consider the linear system for matrix $A$ with rank $r$ of</description></item><item><title>Derivation of the F-distribution from the t-distribution</title><link>https://freshrimpsushi.github.io/en/posts/2233/</link><pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2233/</guid><description>Theorem 1 A random variable $X \sim t(\nu)$ that follows a t-distribution with degrees of freedom $\nu &amp;gt; 0$ is defined as $Y$, which follows an F-distribution $F (1,\nu)$. $$ Y := X^{2} \sim F (1,\nu) $$ Proof Via Chi-Square Distribution $X \sim t(\nu)$, which follows a standard normal distribution, and $W$, which follows a chi-square distribution with degrees of freedom $\nu$, are related as $$ X^{2} = \left( {{</description></item><item><title>Differential Forms of Type k</title><link>https://freshrimpsushi.github.io/en/posts/3264/</link><pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3264/</guid><description>Definition1 Let $\omega = \sum\limits_{I} a_{I} dx_{I}$ be a $k$-form on the $n$-dimensional differential manifold $M$. The exterior differential $d\omega$ of $\omega$ is defined as follows: $$ d\omega := \sum\limits_{I} da_{I} \wedge dx_{I} $$ Here, $\wedge$ is the wedge product. Description Since $da_{I}$ is a $1$-form and $dx_{I}$ is a $k$-form, $d\omega$ is a $(k+1)$-form. Example Let $\omega$ be a $1$-form given in $\mathbb{R}^{3}$ as follows: $$ \omega = xyz</description></item><item><title>How to Install a Specific Version of a Package in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2232/</link><pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2232/</guid><description>Guide 1 (@v1.6) pkg&amp;gt; status JuMP Status `C:\Users\rmsms\.julia\environments\v1.6\Project.toml` [4076af6c] JuMP v0.20.0 By pressing the ] key in the REPL, you enter the package mode. For example, if you want to upgrade a package version from v0.20.0 to v0.21, you can do so by appending @x.yy to the package as follows. (@v1.6) pkg&amp;gt; add JuMP@0.21 Resolving package versions... ... (@v1.6) pkg&amp;gt; status JuMP Status `C:\Users\rmsms\.julia\environments\v1.6\Project.toml` [4076af6c] JuMP v0.21.4 If you check</description></item><item><title>머신러닝에서 선형회귀모델의 최소제곱법 학습</title><link>https://freshrimpsushi.github.io/en/posts/3263/</link><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3263/</guid><description>Overview1 We introduce a method using the least squares, a learning method for linear regression models. Description Let the dataset be $X = \left\{ \mathbf{x}_{i} \right\}_{i=1}^{N}$, and the label set be $Y = \left\{ y_{i} \right\}_{i=1}^{N}$. Assume the following linear regression model: $$ \hat{y} = \sum\limits_{j=0}^{n-1} w_{j}x_{j} = \mathbf{w}^{T} \mathbf{x} $$ Here, $\mathbf{x} = \begin{bmatrix} x_{0} &amp;amp; \dots &amp;amp; x_{n-1} \end{bmatrix}^{T}$ and $\mathbf{w} = \begin{bmatrix} w_{0} &amp;amp; \dots &amp;amp; w_{n-1}</description></item><item><title>Derivation of Beta Distribution from F-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2231/</link><pubDate>Sun, 10 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2231/</guid><description>Theorem 1 A random variable $X \sim F \left( r_{1}, r_{2} \right)$ following an F-distribution with degrees of freedom $r_{1} , r_{2}$ is defined as follows $Y$ and follows a beta distribution $\text{Best} \left( {{ r_{1} } \over { 2 }} , {{ r_{2} } \over { 2 }} \right)$. $$ Y := {{ \left( r_{1} / r_{2} \right) X } \over { 1 + \left( r_{1} / r_{2} \right)</description></item><item><title>Pull Back in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3262/</link><pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3262/</guid><description>Overview We define the pullback on a differential manifold. If differential manifolds are complex, one can think of $M = \mathbb{R}^{m}$ and $N = \mathbb{R}^{n}$. Definition1 Given two differential manifolds $M, N$ and a differentiable function $f : M \to N$, we can consider a function $f^{\ast}$ that maps $N$&amp;rsquo;s $k$-forms to $M$&amp;rsquo;s $k$-forms. Let $\omega$ be a $k$-form on the manifold $N$, then a $k$-form $f^{\ast}\omega$ on the manifold</description></item><item><title>How to Make Empty DataFrame in julia</title><link>https://freshrimpsushi.github.io/en/posts/2230/</link><pubDate>Fri, 08 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2230/</guid><description>Overview Despite many languages supporting dataframes, surprisingly, creating empty arrays can be a new and annoying task each time. Code Type Specification julia&amp;gt; using DataFrames julia&amp;gt; df1 = DataFrame(x = Int64[], y = String[]) 0×2 DataFrame You actually just need to insert an empty array as data. At this point, a type is specified, but when there&amp;rsquo;s absolutely no data, neither column names nor types are visible. julia&amp;gt;</description></item><item><title>Gradient Descent Learning of Linear Regression Models in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3261/</link><pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3261/</guid><description>Overview1 Introducing a method using gradient descent, one of the learning methods of the linear regression model. Description Let&amp;rsquo;s assume that the data set is $X = \left\{ \mathbf{x}_{i} \right\}_{i=1}^{N}$ and the label set is $Y = \left\{ y_{i} \right\}_{i=1}^{N}$. And let&amp;rsquo;s assume the following linear regression model. $$ \hat{y} = \sum\limits_{j=0}^{n} w_{j}x_{j} = \mathbf{w}^{T} \mathbf{x} $$ At this time, $\mathbf{x} = \begin{bmatrix} x_{0} &amp;amp; \dots &amp;amp; x_{n} \end{bmatrix}^{T}$ and</description></item><item><title>If an Optimal Solution Exists in Linear Programming Problems, One of Them is a Basic Feasible Solution</title><link>https://freshrimpsushi.github.io/en/posts/2229/</link><pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2229/</guid><description>Theorem 1 $$ \begin{matrix} \text{Maximize} &amp;amp; \mathbf{c}^{T} \mathbf{x} \\ \text{subject to} &amp;amp; A \mathbf{x} = \mathbf{b} \\ &amp;amp; \mathbf{x} \ge \mathbf{0} \end{matrix} $$ Given matrices $A \in \mathbb{R}^{m \times n}$, $\mathbf{b} \in \mathbb{R}^{m \times 1}$, and $\mathbf{c} \in \mathbb{R}^{n}$, let&amp;rsquo;s assume a linear programming problem is represented in the form of an equation as above. If an optimal solution exists, then an optimal basic feasible solution also exists. $\mathbf{c}^{T}$ represents</description></item><item><title>Operations on Differential Forms: Sum and Wedge Product</title><link>https://freshrimpsushi.github.io/en/posts/3260/</link><pubDate>Tue, 05 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3260/</guid><description>Definition1 Sum $+$ Suppose $\omega = \sum\limits_{I} a_{I} dx_{I}, \varphi = \sum\limits_{I} b_{I} dx_{I}$ is in $k$ notation. Then, the sum of these two is defined as follows. $$ \omega + \varphi := \sum\limits_{I}\left( a_{I} + b_{I} \right)dx_{I} $$ Wedge Product $\wedge$ Let $\omega = \sum\limits_{I} a_{I}dx_{I}$, $\varphi = \sum\limits_{J} b_{J}dx_{J}$ be in $k$ notation and $s$ notation, respectively. Then, the wedge product of these two is defined as: $$</description></item><item><title>Definition of Convex Hull</title><link>https://freshrimpsushi.github.io/en/posts/2228/</link><pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2228/</guid><description>Definition 1 The convex hull $C$ of a subset $X$ of a vector space $V$ refers to the intersection of all convex sets that contain $X$, and is mathematically defined as follows. $$ C = \left\{ \sum_{k} t_{k} \mathbf{x}_{k} : \mathbf{x}_{k} \in X, t_{k} \ge 0 , \sum_{k} t_{k} = 1 \right\} $$ Explanation The equation mentioned in the definition isn&amp;rsquo;t exactly a definition per se. The expression $$ \sum_{k}</description></item><item><title>How to Perform Hierarchical Clustering in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3259/</link><pubDate>Sun, 03 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3259/</guid><description>Explanation Use the hclust() function from the Clustering.jl package. hclust(d::AbstractMatrix; [linkage], [uplo], [branchorder]) It takes a distance matrix as input and returns the result of hierarchical clustering. The default method for calculating distances between clusters is single linkage. To plot a dendrogram, use StatsPlots.jl instead of Plots.jl. Code using StatsPlots using Clustering using Distances using Distributions a = rand(Uniform(-1,1), 2, 25) scatt = scatter(a[1,:], a[2,:], label=false) savefig(scatt, &amp;#34;julia_hclust_scatter.png&amp;#34;) D_a =</description></item><item><title>Proof of the Existence of an Optimal Solution in the Equation Form of Linear Programming Problems</title><link>https://freshrimpsushi.github.io/en/posts/2227/</link><pubDate>Sat, 02 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2227/</guid><description>Theorem 1 $$ \begin{matrix} \text{Maximize} &amp;amp; \mathbf{c}^{T} \mathbf{x} \\ \text{subject to} &amp;amp; A \mathbf{x} = \mathbf{b} \\ &amp;amp; \mathbf{x} \ge \mathbf{0} \end{matrix} $$ Let&amp;rsquo;s say the linear programming problem is represented in the form of equation form for matrices $A \in \mathbb{R}^{m \times n}$, $\mathbf{b} \in \mathbb{R}^{m \times 1}$, and $\mathbf{c} \in \mathbb{R}^{n}$. If there exists at least one feasible solution, and the set of feasible solutions is Bounded Above</description></item><item><title>kth Order Differential Forms</title><link>https://freshrimpsushi.github.io/en/posts/3258/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3258/</guid><description>Overview Just as we defined the second-order differential form, we generalize to define the k-th order forms on the differential manifold $M$. If the concept of a differential manifold is challenging, it can be simply thought of as $M = \mathbb{R}^{n}$. Build-up Let&amp;rsquo;s say $M$ is a $n$-dimensional differential manifold. $p \in M$ is a point in $M$, and $T_{p}M$ is the tangent space at point $p$ in $M$. $T_{p}^{\ast}M$</description></item><item><title>How to Find a Specific Pattern Location in Julia Strings</title><link>https://freshrimpsushi.github.io/en/posts/2226/</link><pubDate>Thu, 30 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2226/</guid><description>Code julia&amp;gt; findfirst(&amp;#34;li&amp;#34;, &amp;#34;multicolinearlity&amp;#34;) 8:9 julia&amp;gt; findlast(&amp;#34;li&amp;#34;, &amp;#34;multicolinearlity&amp;#34;) 14:15 julia&amp;gt; findnext(&amp;#34;l&amp;#34;, &amp;#34;multicolinearlity&amp;#34;, 1) 3:3 julia&amp;gt; findnext(&amp;#34;l&amp;#34;, &amp;#34;multicolinearlity&amp;#34;, 4) 8:8 julia&amp;gt; findnext(&amp;#34;l&amp;#34;, &amp;#34;multicolinearlity&amp;#34;, 9) 14:14 julia&amp;gt; findfirst(r&amp;#34;t.+t&amp;#34;, &amp;#34;multicolinearlity&amp;#34;) 4:16 findfirst(pattern, A) Returns a Range representing the interval matching pattern in the string A. The pattern can include regular expressions. In the last example, it finds and returns the interval from the first t to the last t.</description></item><item><title>How to Draw a Dendrogram in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3257/</link><pubDate>Wed, 29 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3257/</guid><description>Explanation When attempting to draw a dendrogram by using the plot() function after performing hierarchical clustering with hclust() on the given data, the following error occurs. using Clustering using Distances using Plots a = rand(2, 10) D_a = pairwise(Euclidean(), a, a) SL = hclust(D_a, linkage=:single) dendrogram = plot(SL) ERROR: LoadError: Cannot convert Hclust{Float64} to series data for plotting To draw a dendrogram, one should use StatsPlots.jl instead of Plots.jl. using</description></item><item><title>Proof of the Uniqueness of Base Solubility</title><link>https://freshrimpsushi.github.io/en/posts/2225/</link><pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2225/</guid><description>Theorem 1 $$ \begin{matrix} \text{Maximize} &amp;amp; \mathbf{c}^{T} \mathbf{x} \\ \text{subject to} &amp;amp; A \mathbf{x} = \mathbf{b} \\ &amp;amp; \mathbf{x} \ge \mathbf{0} \end{matrix} $$ Given a matrix $A \in \mathbb{R}^{m \times n}$ and $\mathbf{b} \in \mathbb{R}^{m \times 1}$ and $\mathbf{c} \in \mathbb{R}^{n}$, if the linear programming problem is expressed in the form of equations as above, the basic feasible solution is uniquely determined by the basis $B$. $\mathbf{c}^{T}$ means the transpose.</description></item><item><title>Second-Order Differential Form</title><link>https://freshrimpsushi.github.io/en/posts/3256/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3256/</guid><description>Overview We define the binary operation $\wedge$ and, in the sense that we defined the first-order differential form, we define a second-order form for the differential manifold $M$. If differential manifolds seem difficult, one can think of them as $M = \mathbb{R}^{n}$. Buildup1 Let’s consider the first-order form $\omega$. $$ \begin{align*} \omega : M &amp;amp;\to T^{\ast}M \\ p &amp;amp;\mapsto \omega_{p} \end{align*} $$ This maps a point</description></item><item><title>How to Check if a Specific String is Contained in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2224/</link><pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2224/</guid><description>Code julia&amp;gt; contains(&amp;#34;qwerty&amp;#34;, &amp;#34;er&amp;#34;) true julia&amp;gt; contains(&amp;#34;qwerty&amp;#34;, &amp;#34;et&amp;#34;) false julia&amp;gt; contains(&amp;#34;qwerty&amp;#34;, r&amp;#34;q?&amp;#34;) true contains(haystack::AbstractString, needle) Returns a boolean indicating whether needle is contained in haystack. needle can include regular expressions, such as r&amp;quot;...&amp;quot;. Note that &amp;lsquo;haystack&amp;rsquo; means a hay pile, referring to the idiom &amp;ldquo;a needle in a haystack&amp;rdquo;. Environment OS: Windows julia: v1.6.2</description></item><item><title>What is a Dendrogram?</title><link>https://freshrimpsushi.github.io/en/posts/3255/</link><pubDate>Sat, 25 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3255/</guid><description>Definition A dendrogram is a tree-like diagram that shows the hierarchical clustering of given data (left side) as a picture (right side).</description></item><item><title>Linear Programming Problem Basis Solution</title><link>https://freshrimpsushi.github.io/en/posts/2223/</link><pubDate>Fri, 24 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2223/</guid><description>Definition 1 $$ \begin{matrix} \text{Maximize} &amp;amp; \mathbf{c}^{T} \mathbf{x} \\ \text{subject to} &amp;amp; A \mathbf{x} = \mathbf{b} \\ &amp;amp; \mathbf{x} \ge \mathbf{0} \end{matrix} $$ Given matrices $A \in \mathbb{R}^{m \times n}$, $\mathbf{b} \in \mathbb{R}^{m \times 1}$, and $\mathbf{c} \in \mathbb{R}^{n}$ for a Linear Programming Problem represented in the equation form, a set $B \subseteq [n]$ with cardinality $m$ exists for the feasible solution $\mathbf{x} \in \mathbb{R}^{n}$, satisfying the following two conditions,</description></item><item><title>Cotangent Space and First-Order Differential Forms</title><link>https://freshrimpsushi.github.io/en/posts/3254/</link><pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3254/</guid><description>Overview We define the cotangent space and the differential 1-form. If differential manifolds are challenging, one can think of it as $M = \mathbb{R}^{n}$. We use Einstein notation. Cotangent Space1 Let&amp;rsquo;s consider a $M$ as a $n$-dimensional differential manifold. Then, the tangent space $T_{p}M$ at point $p \in M$ becomes a $n$-dimensional vector space (function space), with the basis being $\left\{ \mathbf{e}_{i} = \left. \frac{\partial }{\partial x_{i}}\right|_{p} \right\}_{i}$. At this</description></item><item><title>How to Use Factorization and Prime Number Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2222/</link><pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2222/</guid><description>Overview Primes.jl is a package that deals with functions related to primes and prime factorization. The implementation of functions related to analytic number theory is still lacking. This is not a comprehensive list of all the features of the package, but rather a selection of the useful ones. For more details, check the repository1. Types Prime Factorization Primes.Factorization julia&amp;gt; factor(12) 2^2 * 3 julia&amp;gt; factor(12)[1] 0 julia&amp;gt; factor(12)[2] 2 julia&amp;gt;</description></item><item><title>Hierarchical Clustering</title><link>https://freshrimpsushi.github.io/en/posts/3253/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3253/</guid><description>Algorithm Input Given data of dimension $p$ and $N$ instances with a distance $d$. Step 1. Consider each point as a single cluster. Combine the two closest clusters into one. Step 2. Again, combine the two closest clusters into one. Step 3. Repeat until only one cluster remains. Output Returns which cluster each data belongs to and the distance between clusters. The tree structure obtained on the right is called</description></item><item><title>Linear Programming Problem in Equation Form</title><link>https://freshrimpsushi.github.io/en/posts/2221/</link><pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2221/</guid><description>Definition 1 For matrices $A \in \mathbb{R}^{m \times n}$, $\mathbf{b} \in \mathbb{R}^{m \times 1}$, and $\mathbf{c} \in \mathbb{R}^{n}$, the following linear programming problem is called in standard form or equational form. $$ \begin{matrix} \text{Maximize} &amp;amp; \mathbf{c}^{T} \mathbf{x} \\ \text{subject to} &amp;amp; A \mathbf{x} = \mathbf{b} \\ &amp;amp; \mathbf{x} \ge \mathbf{0} \end{matrix} $$ $\mathbf{c}^{T}$ means transpose. Optimization refers to maximization or minimization. Description Conversion to Standard Form In principle, any linear</description></item><item><title>How to Use Polynomials in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2220/</link><pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2220/</guid><description>Overview Polynomials.jl is a package that includes the representation and computation of polynomial functions. Since polynomials are mathematically simple, there&amp;rsquo;s a tendency to think their coding should be straightforward, too. However, once you start implementing the necessary features, it can become quite cumbersome. Of course, it&amp;rsquo;s not extremely difficult, but it&amp;rsquo;s generally better to use a package whenever possible. This is not an exhaustive list of all features of the</description></item><item><title>Angular Momentum Operator in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/3251/</link><pubDate>Fri, 17 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3251/</guid><description>Build-Up The angular momentum operator is naturally derived from the classical definition of angular momentum. $$ \mathbf{l} = \mathbf{r} \times \mathbf{p} \tag{1} $$ Let’s denote this as $\mathbf{r} = (x, y, z)$ and $\mathbf{p} = (p_{x}, p_{y}, p_{z})$. Then each component of the angular momentum $\mathbf{l} = (l_{x}, l_{y}, l_{z})$ is described as follows. $$ l_{x} = yp_{z} - zp_{y},\quad l_{y} = zp_{x} - xp_{z},\quad l_{z}</description></item><item><title>Weibull Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2219/</link><pubDate>Thu, 16 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2219/</guid><description>Definition A Weibull Distribution is a probability distribution with the following probability density function, given scale parameter $\lambda &amp;gt; 0$ and shape parameter $k &amp;gt; 0$. $$ f(x) = {{ k } \over { \lambda }} \left( {{ x } \over { \lambda }} \right)^{k-1} e^{-(x/\lambda)^{k}} \qquad , x \ge 0 $$ Theorems [1] A Generalization of the Exponential Distribution: The Weibull Distribution becomes the Exponential Distribution when $k=1$. [2]</description></item><item><title>Concatenating Strings in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2218/</link><pubDate>Tue, 14 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2218/</guid><description>Code Concatenate Strings * julia&amp;gt; &amp;#34;oh&amp;#34; * &amp;#34;my&amp;#34; * &amp;#34;girl&amp;#34; &amp;#34;ohmygirl&amp;#34; Corresponds to the + in Python. Concatenate Multiple Strings string() julia&amp;gt; string(&amp;#34;oh&amp;#34;,&amp;#34;my&amp;#34;, &amp;#34;girl&amp;#34;) &amp;#34;ohmygirl&amp;#34; Corresponds to paste0() in R. Joining Items of a List of Strings join() julia&amp;gt; OMG = [&amp;#34;oh&amp;#34;,&amp;#34;my&amp;#34;, &amp;#34;girl&amp;#34;] 3-element Vector{String}: &amp;#34;oh&amp;#34; &amp;#34;my&amp;#34; &amp;#34;girl&amp;#34; julia&amp;gt; join(OMG) &amp;#34;ohmygirl&amp;#34; Corresponds to join() in Python. Repeat the Same String ^ julia&amp;gt; &amp;#34;=-&amp;#34; ^ 10 &amp;#34;=-=-=-=-=-=-=-=-=-=-&amp;#34; Corresponds to * in</description></item><item><title>Convolution Support</title><link>https://freshrimpsushi.github.io/en/posts/3249/</link><pubDate>Mon, 13 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3249/</guid><description>Theorem Given two sets of real numbers $A, B$, we define $A + B$ as follows. $$A + B := \left\{ a + b : \forall a \in A, \forall b \in \supp B \right\}$$ For two functions $f, g$, the following holds. $$\supp f \ast g \subset \supp f + \supp g$$ Here, $\supp$ is the function&amp;rsquo;s support, and $\ast$ is the convolution. Proof1 Assume $x \notin \supp f</description></item><item><title>Pythagorean Winning Percentage</title><link>https://freshrimpsushi.github.io/en/posts/2217/</link><pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2217/</guid><description>Formula 1 Given a team from a particular sports league, let&amp;rsquo;s discuss Team Scores $S$ and Team Allows $A$. The expected winning percentage of the season for this team $p$ is as follows. $$ p = {{ S^{2} } \over { S^{2} + A^{2} }} = {{ 1 } \over { 1 + (A/S)^{2} }} $$ Explanation The Pythagorean Expectation, proposed by Bill James, is a nonlinear model that uses</description></item><item><title>Hiding Specific Data Labels in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/2216/</link><pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2216/</guid><description>Code 1 using Plots x = rand(30) y = rand(30) z = rand(30) plot(x) plot!(y) plot!(z) png(&amp;#34;result1&amp;#34;) In some cases, you might want to make only certain data labels appear in the legend, as shown above. label = &amp;quot;&amp;quot; plot(x, label = &amp;#34;&amp;#34;) plot!(y) png(&amp;#34;result2&amp;#34;) In such cases, you can set the option label = &amp;quot;&amp;quot;. As you can see, the first data is displayed in the figure, but it</description></item><item><title>How to Change the Default Terminal in VS Code with Ctrl+Shift+C</title><link>https://freshrimpsushi.github.io/en/posts/2215/</link><pubDate>Wed, 08 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2215/</guid><description>Guide In Visual Studio Code, you can press Ctrl + Shift + C to bring up the terminal window. This is a feature that ends up being used quite often for various tests and reasons, but more often than not, you might want to use powershell or windows terminal instead of cmd. Step 1. Open the settings window by going to File/Preferences/Setting or pressing Ctrl+,. Step 2. External: Windows Exec</description></item><item><title>How to Insert Text into a Julia Plot</title><link>https://freshrimpsushi.github.io/en/posts/2214/</link><pubDate>Mon, 06 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2214/</guid><description>Code 1 You can use annotate!(). The following code is for drawing a picture that marks the maximum and minimum points in Brownian motion. using Plots cd(@__DIR__) data = cumsum(randn(100)) plot(data, color = :black, legend = :none) annotate!(argmax(data), maximum(data), &amp;#34;max\n&amp;#34;) annotate!(argmin(data), minimum(data), &amp;#34;\nmin&amp;#34;) png(&amp;#34;result&amp;#34;) Environment OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/plots-annotate/37784&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Radon Transform and Product Integration, Convolution</title><link>https://freshrimpsushi.github.io/en/posts/3245/</link><pubDate>Sun, 05 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3245/</guid><description>Summary1 Let&amp;rsquo;s call $\mathcal{R}$ the Radon Transform. $$ \mathcal{R} f (s, \boldsymbol{\theta}) = \int\limits_{\mathbf{x} \cdot \boldsymbol{\theta} = s} f(\mathbf{x}) d \mathbf{x} $$ Let&amp;rsquo;s say $\mathcal{R}_{\boldsymbol{\theta}}f(s) = \mathcal{R} f (s, \boldsymbol{\theta})$. The following formulas hold. Radon Transform and Product Integration $$ \int\limits_{-\infty}^{\infty} \mathcal{R}_{\boldsymbol{\theta}}f(s)g(s) ds = \int \limits_{\mathbb{R}^{n}} f(\mathbf{x}) g(\mathbf{x} \cdot \boldsymbol{\theta}) d \mathbf{x} $$ Corollary $$ \int\limits_{-\infty}^{\infty} \mathcal{R}_{\boldsymbol{\theta}}f(t - s)g(s) ds = \int \limits_{\mathbb{R}^{n}} f(\mathbf{x}) g(-\mathbf{x} \cdot \boldsymbol{\theta} + t) d</description></item><item><title>Exponential Family of Probability Distributions</title><link>https://freshrimpsushi.github.io/en/posts/2213/</link><pubDate>Sat, 04 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2213/</guid><description>Definition 1 2 If the probability mass function or probability density function of a probability distribution with parameter $\theta$ can be expressed in terms of some functions $p,K,H,q,h,c,w_{i},t_{i}$ as follows, it is said to belong to the Exponential Family or Exponential Class. $$ \begin{align*} f \left( x ; \theta \right) =&amp;amp; \exp \left( p (\theta) K (x) + H(x) + q(\theta) \right) \\ =&amp;amp; h(x) c (\theta) \exp \left( \sum_{i=1}^{k}</description></item><item><title>Inserting Korean Text into Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/2212/</link><pubDate>Thu, 02 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2212/</guid><description>Environment OS: Windows julia: v1.6.2 Error julia&amp;gt; plot(data, color = :black, label = &amp;#34;값&amp;#34;, title = &amp;#34;브라운모션&amp;#34;) GKS: glyph missing from current font: 48652 GKS: glyph missing from current font: 46972 GKS: glyph missing from current font: 50868 GKS: glyph missing from current font:</description></item><item><title>Writing Korean in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3243/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3243/</guid><description>Description Using the kotex package allows for the use of Korean. With kotex \documentclass{article} \usepackage[utf8]{inputenc} \usepackage{kotex} \title{TeX에서 한글 쓰는 방법} \author{전기현} \date{2022년 06월 01일} \begin{document} \maketitle TeX</description></item><item><title>Convolution Formula of Probability Density Functions</title><link>https://freshrimpsushi.github.io/en/posts/2211/</link><pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2211/</guid><description>Formula 1 Given two independent continuous random variables $X, Y$, their probability density functions are given by $f_{X}, f_{Y}$. Then the probability density function of $Z := X + Y$ is the convolution of the two probability density functions $f_{Z} = f_{X} \ast f_{Y}$. $$ f_{Z} (z) = \left( f_{X} \ast f_{Y} \right) (z) = \int_{-\infty}^{\infty} f_{X} (w) f_{Y} (z-w) dw $$ Derivation If we let $W := X$, the</description></item><item><title>Euler Characteristics in Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3242/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3242/</guid><description>Definition Simple Definition Let&amp;rsquo;s assume we are given a shape. Let&amp;rsquo;s call the number of vertices $V$, the number of edges $E$, and the number of faces $F$. The Euler characteristic $\chi$ of this shape is defined as follows. $\chi := V - E + F$ Complex Definition1 For a surface $M$ and its region $\mathscr{R}$, the term $\chi(\mathscr{R}) \in \mathbb{Z}$ that satisfies the Gauss-Bonnet theorem is called the Euler</description></item><item><title>Rayleigh Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2210/</link><pubDate>Sun, 29 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2210/</guid><description>Definition 1 A continuous probability distribution with the probability density function given for the scale parameter $\sigma &amp;gt; 0$ is called the Rayleigh Distribution. $$ f(x) = {{ x } \over { \sigma^{2} }} e^{ - x^{2} / (2 \sigma^{2})} \qquad , x \ge 0 $$ Theorem [1]: If $X, Y \sim N \left( 0, \sigma^{2} \right)$ then $\sqrt{X^{2} + Y^{2}}$ follows a Rayleigh Distribution with $\sigma &amp;gt; 0$. Explanation</description></item><item><title>How to Install TeX and Use It in VS Code</title><link>https://freshrimpsushi.github.io/en/posts/3241/</link><pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3241/</guid><description>Installation Download TeX Live from here. Windows users should click the link pointed to in the screenshot above. Follow the installation steps in order. A dog will pop up; just let it be. Then, the installation window shown below will appear. Click &amp;ldquo;Install.&amp;rdquo; Be aware that it might take a long time. It took me 2 hours and 11 seconds. VS Code Install TeX Workshop from extensions. If you modify</description></item><item><title>Expectation of the Sum of Random Variables in the Form of Functions</title><link>https://freshrimpsushi.github.io/en/posts/2209/</link><pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2209/</guid><description>Theorem 1 Given that $X_{1} , \cdots , X_{n}$ is a random sample, and there exist functions $E g \left( X_{1} \right)$ and $\Var g \left( X_{1} \right)$ such that $g : \mathbb{R} \to \mathbb{R}$ is given, then the following hold: [1] Mean: $$ E \left( \sum_{k = 1}^{n} g \left( X_{k} \right) \right) = n E g \left( X_{1} \right) $$ [2] Variance: $$ \Var \left( \sum_{k = 1}^{n}</description></item><item><title>Definition of OPS in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2208/</link><pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2208/</guid><description>Definition 1 The sum of On-base Percentage (OBP) and Slugging Percentage (SLG) is known as OPS. $$ OPS := OBP + SLG $$ Description A high on-base percentage has been considered to increase the expected score for the team, and a high slugging percentage is seen as an indicator of the ability to realize these expectations. The argument that batting average has become worthless as a quantitative record in modern</description></item><item><title>Principles of Photoacoustic Tomography</title><link>https://freshrimpsushi.github.io/en/posts/3239/</link><pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3239/</guid><description>Photoacoustic Effect The photoacoustic (or optoacoustic) effect is a physical phenomenon discovered1 by Alexander Graham Bell, who is erroneously known2 as the inventor of the telephone in 1880. When a material is exposed to light (electromagnetic waves), it absorbs the light, leading to an increase in temperature and thermal expansion. Once the radiation ceases, the material cools and contracts. This expansion and contraction of the material result in pressure changes,</description></item><item><title>Definition of Linear Programming Problem</title><link>https://freshrimpsushi.github.io/en/posts/2207/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2207/</guid><description>Definition 1 A linear programming problem, shortly referred to as LP, is an optimization problem that is linear in both its objective function and constraints. Simply put, a linear problem looks to find $\mathbf{x} \in \mathbb{R}^{n}$ for which the objective function $f: \mathbb{R}^{n} \to \mathbb{R}$, given vectors $\mathbf{c} \in \mathbb{R}^{n}$, is $$ f \left( \mathbf{x} \right) := \mathbf{c}^{T} \mathbf{x} $$ and for given matrices $A \in \mathbb{R}^{m \times n}$ and</description></item><item><title>Gauss-Bonnet Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3238/</link><pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3238/</guid><description>Gauss-Bonnet Theorem Let&amp;rsquo;s consider $\mathbf{x} : U \to \mathbb{R}^{3}$ as a simple connected geodesic coordinate chart, and $\boldsymbol{\gamma}(I) \subset \mathbf{x}(U)$, which is $\boldsymbol{\gamma}$, as piecewise regular curves. Also, let&amp;rsquo;s say that $\boldsymbol{\gamma}$ surrounds some region $\mathscr{R}$. Then, the following holds true. $$ \iint_{\mathscr{R}} K dA + \int_{\boldsymbol{\gamma}} \kappa_{g} ds + \sum \alpha_{i} = 2\pi $$ Here, $K$ denotes the Gaussian curvature, $\kappa_{g}$ denotes the geodesic curvature, and $\alpha_{i}$ denotes the</description></item><item><title>On-base Percentage in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2206/</link><pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2206/</guid><description>Definition 1 2 The ratio that indicates how often a batter gets on base is called On-base Percentage, abbreviated as OBP. It is defined in relation to Hits H, Walks BB and Hit By Pitch HBP, At Bats AB, Sacrifice Flies SF as follows: $$ \begin{align*} OBP :=&amp;amp; {{ H + (BB + HBP) } \over { AB + (BB + HBP) + SF }} \\ 출루율 :=&amp;amp;</description></item><item><title>What is a Sinogram in Tomography?</title><link>https://freshrimpsushi.github.io/en/posts/3237/</link><pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3237/</guid><description>Definition The heat map of the Radon transform is called a sinogram. Description The name sinogram comes from the fact that, as can be seen in the image below, it appears similar to a sine graph for small phantoms.1 (Above) Phantom (Below) The sinogram of the phantom above Peter Kuchment, The Radon Transform and Medical Imaging (2014), p28&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Handling Strings in Julia like in Python</title><link>https://freshrimpsushi.github.io/en/posts/2205/</link><pubDate>Thu, 19 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2205/</guid><description>Code 1 2 3 julia&amp;gt; replace(&amp;#34;qwerty&amp;#34;, &amp;#34;q&amp;#34;=&amp;gt;&amp;#34;Q&amp;#34;) &amp;#34;Qwerty&amp;#34; julia&amp;gt; join(&amp;#34;qwerty&amp;#34;, &amp;#34;,&amp;#34;) &amp;#34;q,w,e,r,t,y&amp;#34; julia&amp;gt; split(&amp;#34;qwerty&amp;#34;, &amp;#34;&amp;#34;) 6-element Vector{SubString{String}}: &amp;#34;q&amp;#34; &amp;#34;w&amp;#34; &amp;#34;e&amp;#34; &amp;#34;r&amp;#34; &amp;#34;t&amp;#34; &amp;#34;y&amp;#34; Julia is not particularly outstanding in string processing, but maybe because of that, it has followed Python closely making it easy and quick to learn. Most of the already known functionalities are implemented, so apart from whether it&amp;rsquo;s a module or not, the usage is almost similar.</description></item><item><title>Simple Connected Region</title><link>https://freshrimpsushi.github.io/en/posts/3236/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3236/</guid><description>Definitions Let $\mathscr{R}$ be a region of the surface $M$. If every closed curve within $\mathscr{R}$ is null-homotopic, then $\mathscr{R}$ is said to be simply connected. Description Easy examples such as $\mathbb{R}^{2}$, disk $\left\{ x^{2} + y^{2} = r^{2} \right\}$, and sphere $\mathbb{S}^{2}$ are immediately thought to be simply connected. However, as shown in the figure below, one can see that the torus $T^{2}$ is not simply connected. Unlike $\gamma$,</description></item><item><title>Definition of Slugging Percentage in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2204/</link><pubDate>Tue, 17 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2204/</guid><description>Definition 1 2 The total number of bases achieved through a batter&amp;rsquo;s hit is referred to as Total Bases, abbreviated as TB. The ratio of Total Bases TB to At Bats AB is called the Slugging Percentage, abbreviated as SLG. In other words, the slugging percentage is defined as follows with respect to total bases from hits $H_{k}$ and Home Runs HR. $$ SLG := {{ H_{1} + 2 H_{2}</description></item><item><title>What is a Phantom in Tomography?</title><link>https://freshrimpsushi.github.io/en/posts/3235/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3235/</guid><description>Definition A hypothetical image used for the numerical simulation of tomography is called a phantom. Description The problem dealt with in tomography is, given a function $f$ and an operator $A$, finding $f$ when $Af$ is given. For example, in the case of CT imaging, $Af$ is the Radon transform $\mathcal{R}f$, which refers to the data obtained by a CT scanner passing radiation through our body. Specifically, the brain CT</description></item><item><title>How to Check Approximate Values in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2203/</link><pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2203/</guid><description>Code Using the comparison operator $\approx$, it only returns true when two values are sufficiently similar. ≈ can be used by entering \approx and then pressing Tab, just as in $\TeX$. julia&amp;gt; π ≈ 3.141592653 true julia&amp;gt; π ≈ 3.14159265 true julia&amp;gt; π ≈ 3.1415926 false julia&amp;gt; π ≈ 3.141592 false Environment OS: Windows julia: v1.7.0</description></item><item><title>Homotopy to Null in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3234/</link><pubDate>Sat, 14 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3234/</guid><description>Definition1 Let&amp;rsquo;s say that a closed curve $\gamma$ encloses a region $\mathscr{R}$ on a surface $M$. Suppose $\sigma$ is a closed curve or a loop with period $L$ placed on $\mathscr{R}$. And let $\sigma (0) = x_{0}$. If there exists a closed curve $\sigma_{s}$ on the surface $M$ that satisfies the following conditions for $s \in [0,1]$, then $\sigma$ is said to be null-homotopic. $\sigma_{s}(0) = x_{0}$ $\sigma_{0}(t) = \sigma</description></item><item><title>Definition of Batting Average in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2202/</link><pubDate>Fri, 13 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2202/</guid><description>Definition 1 The ratio of hits H divided by at-bats AB is called the batting average, abbreviated as AVG. In other words, batting average is defined as follows. $$ AVG := {{ H } \over { AB }} $$ Theory [1]: Batting average does not have a significant correlation with the batter&amp;rsquo;s run production ability. Description The term batting average, originating from baseball, has become a word frequently used in</description></item><item><title>From Julia: Dictionaries and Pairs</title><link>https://freshrimpsushi.github.io/en/posts/2201/</link><pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2201/</guid><description>Code 1 julia&amp;gt; d = Dict(&amp;#34;A&amp;#34;=&amp;gt;1, &amp;#34;B&amp;#34;=&amp;gt;2) Dict{String, Int64} with 2 entries: &amp;#34;B&amp;#34; =&amp;gt; 2 &amp;#34;A&amp;#34; =&amp;gt; 1 julia&amp;gt; push!(d,(&amp;#34;C&amp;#34;,3)) ERROR: MethodError: no method matching push!(::Dict{String, Int64}, ::Tuple{String, Int64}) julia&amp;gt; push!(d,&amp;#34;C&amp;#34; =&amp;gt; 3) Dict{String, Int64} with 3 entries: &amp;#34;B&amp;#34; =&amp;gt; 2 &amp;#34;A&amp;#34; =&amp;gt; 1 &amp;#34;C&amp;#34; =&amp;gt; 3 julia&amp;gt; typeof(&amp;#34;C&amp;#34; =&amp;gt; 3) Pair{String, Int64} Dictionaries in Julia are data types that pair Keys and Values, much like in other programming languages.</description></item><item><title>What is a Heatmap?</title><link>https://freshrimpsushi.github.io/en/posts/3233/</link><pubDate>Tue, 10 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3233/</guid><description>Definition The graph of a function $f : \mathbb{R}^{2} \to \mathbb{R}$, when projected onto the $xy-$ plane and its value represented as color, is called a heatmap. Explanation The image above shows both the graph of $z = -4x e^{-(x^2+y^2)}$ (top) and its heatmap (bottom) in one picture. A heatmap, like this, turns a 3D graph into a 2D format using colors for ease of understanding. It is easier to</description></item><item><title>The Definition of a Sacrifice Hit in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2200/</link><pubDate>Mon, 09 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2200/</guid><description>Definition 1 2 The number of times a batter is out but successfully advances a runner with a bunt is called a Sacrifice Bunt, abbreviated as SH. The number of times a batter flies out but enables a runner to score is called a Sacrifice Fly, abbreviated as SF. Description 3 A bunt refers to lightly touching the ball with the bat as shown above. The ball doesn&amp;rsquo;t go far,</description></item><item><title>n-Dimensional Radon Transform</title><link>https://freshrimpsushi.github.io/en/posts/3231/</link><pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3231/</guid><description>Definition1 For $s \in \mathbb{R}^{1}$, $\boldsymbol{\theta} \in S^{n-1}$, the Radon transform $\mathcal{R} : L^{2}(\mathbb{R}^{n}) \to L^{2}(Z_{n})$ is defined as follows. $$ \mathcal{R} f (s, \boldsymbol{\theta}) = \int\limits_{\mathbf{x} \cdot \boldsymbol{\theta} = s} f(\mathbf{x}) d \mathbf{x} $$ Here, $Z_{n} := \mathbb{R}^{1} \times S^{n-1}$ is a unit cylinder in $n+1$ dimensions. Description The geometric meaning of $\mathcal{R} f (s, \boldsymbol{\theta})$ is to integrate over all points that are $s$ away from the origin</description></item><item><title>How to Store Data like .mat in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2199/</link><pubDate>Sat, 07 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2199/</guid><description>Overview JLD.jl is a package that allows the storage of temporary data created while using Julia1. It is useful for managing the input and output of data in pure Julia projects. On the other hand, JLD2.jl, which further improves the intuitiveness of the JLD.jl interface, is also available. 2 The content introduced in this post should be taken as a rough understanding of these functionalities, and it is recommended to</description></item><item><title>Differentiable Surfaces and Boundaries of Regions in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3230/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3230/</guid><description>Region1 Consider a subset $\mathscr{R}$ of a surface $M$. If $\mathscr{R}$ is an open set, and for any two points in $\mathscr{R}$ there exists a curve on $\mathscr{R}$ containing both, then $\mathscr{R}$ is called a region of $M$. Boundary For a region $\mathscr{R}$ of a surface $M$, the following set $\partial \mathscr{R}$ is called the boundary of $\mathscr{R}$. $$ \partial \mathscr{R} = \left\{ p \notin \mathscr{R} : \exists \left\{ p_{j}</description></item><item><title>Definition of Walks and Hit-by-Pitches in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2198/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2198/</guid><description>Definition 1 The term Base on Balls (BB), also known as a walk or 4-ball, refers to the number of times a batter reaches base due to four pitches being thrown out of the strike zone. The number of times a batter is hit by a pitch is referred to as Hit-by-Pitch (HBP). Description In Asia (Korea, Japan), a walk is still called a 4-ball, and being hit by a</description></item><item><title>n-Dimensional Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/3229/</link><pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3229/</guid><description>Definition1 Let&amp;rsquo;s say the Cartesian coordinates of point $x \in \mathbb{R}^{n}$ are $x_{1}, \dots, x_{n}$. Then, the relationship with its polar coordinates $r, \varphi_{1}, \dots, \varphi_{n-1}$ is as follows. $$ \begin{align*} x_{n} &amp;amp;= r \cos \varphi_{1} \\ x_{n-1} &amp;amp;= r \sin \varphi_{1} \sin \varphi_{2} \\ x_{n-2} &amp;amp;= r \sin \varphi_{1} \cos \varphi_{2} \\ \vdots&amp;amp; \\ x_{4} &amp;amp;= r \sin \varphi_{1} \sin \varphi_{2} \cdots \sin \varphi_{n-3} \sin \varphi_{n-2} \\ x_{3} &amp;amp;=</description></item><item><title>How to Refer to Both Index and Value in Julia's Loops</title><link>https://freshrimpsushi.github.io/en/posts/2197/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2197/</guid><description>Code 1 Base.Iterators.enumerate() returns an iterator that allows referencing both the index and value of an array, similar to Python. julia&amp;gt; x = [3,5,4,1,2] 5-element Vector{Int64}: 3 5 4 1 2 julia&amp;gt; for (idx, value) in enumerate(x) println(&amp;#34;x[▷eq1◁value&amp;#34;) end x[1]:</description></item><item><title>Gaussian Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3228/</link><pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3228/</guid><description>Definition1 Mapping a point $M$ of a surface to the normal vector at $p$ $$ \nu : M \to S^{2} \text{ with } \nu (p) \text{ normal to } M \text{ at } p $$ If it is continuous at every point, $M$ is called an orientable surface. Description $\nu$ is called the Gauss map. Examples Sphere $S^{2}$ If $\nu (p)$ is called the outward normal vector at $p$, since</description></item><item><title>Definition of a Home Run in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2196/</link><pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2196/</guid><description>Definition 1 The number of times a batter hits a fair ball and scores by hitting the ball is called a Home Run, abbreviated as HR. https://www.mlb.com/glossary/standard-stats/home-run&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Implementing MLP in Julia Flux to Approximate Nonlinear Functions</title><link>https://freshrimpsushi.github.io/en/posts/3227/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3227/</guid><description>Start Import the necessary packages and define the nonlinear function we want to approximate. Creating the Training Set From the domain of the function $[-5, 5]$, 1024 random points were selected. These points are of type Float64, but deep learning typically handles the Float32 data type, so it was converted. Of course, the model can also automatically convert and run data types like Float64 or Int64 when used as input.</description></item><item><title>Symbols in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2195/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2195/</guid><description>Overview When first encountering Julia, one might be perplexed by the symbol data type. Symbols are used with a preceding :, functioning simply by their name without any internal data. They are commonly used as names, labels, or dictionary keys1. Explanation In other programming languages, when giving options to a function, they are often provided as numbers or strings to clarify the meaning. For example, the following two functions illustrate</description></item><item><title>Definition of a Hit in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2194/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2194/</guid><description>Definition 1 The number of balls hit by the batter that land in the valid area, excluding fielder&amp;rsquo;s choices and errors, is called a hit, abbreviated as H. Hits are classified into four types: singles, doubles, triples, and home runs. Theorem [1]: For Plate Appearances PA, At Bats AB, and Hits H, the following inequality is established: $$ \begin{align*} H \le AB \le PA \\ 안타 \le 타수</description></item><item><title>Resolving 'TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.' with Lists in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3225/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3225/</guid><description>Error TypeError: can&amp;#39;t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first. Despite dealing with a list, not a PyTorch tensor or a NumPy array, the above error can occur. If you follow the instruction and use the .cpu() or .numpy() methods, you will encounter the following error. AttributeError: &amp;#39;list&amp;#39; object has no attribute &amp;#39;cpu&amp;#39; Solution The reason this error occurs is</description></item><item><title>How to Check if Elements of an Array Belong to a List in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2193/</link><pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2193/</guid><description>Guide 1 julia&amp;gt; x = rand(&amp;#39;a&amp;#39;:&amp;#39;c&amp;#39;, 10) 10-element Vector{Char}: &amp;#39;a&amp;#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase) &amp;#39;a&amp;#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase) &amp;#39;b&amp;#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase) &amp;#39;c&amp;#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase) &amp;#39;b&amp;#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase) &amp;#39;c&amp;#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase) &amp;#39;c&amp;#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase) &amp;#39;c&amp;#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase) &amp;#39;b&amp;#39;: ASCII/Unicode U+0062 (category Ll:</description></item><item><title>Differentiable Manifolds in Compact Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3224/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3224/</guid><description>Definition1 Let us say $\overline{B}_{r} = \left\{ p \in \mathbb{R}^{3} : \left| p \right| \le r \right\}$. If for a surface $M \subset \mathbb{R}^{3}$, there exists $r$ that satisfies the following, then $M$ is said to be bounded. $$ M \subset \overline{B}_{r} $$ If every sequence of points $\left\{ p_{n} \right\}$ on $M$ satisfies the following equation, in other words, converges to point $p$ on $M$, then $M$ is said</description></item><item><title>Definition of At-bats in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2192/</link><pubDate>Sat, 23 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2192/</guid><description>Definition 1 The term At Bat, abbreviated as AB, refers to the number of times a batter hits a ball, reaches base on a fielder&amp;rsquo;s choice, reaches base on an error, or gets out (excluding sacrifice hits). Theorem [1]: The equation for a batter&amp;rsquo;s plate appearances (PA), at-bats (AB), walks (BB), hit by pitch (HBP), sacrifice bunts (SH), sacrifice fly (SF), and reaching base due to batter or runner interference</description></item><item><title>How to Directly Define Multidimensional Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3223/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3223/</guid><description>Explanation 1D arrays (vectors) are defined as follows. julia&amp;gt; A = [1; 2; 3] 3-element Vector{Int64}: 1 2 3 Here, ; signifies moving to the next element based on the first dimension. By generalizing this, ;; signifies moving to the next element based on the second dimension. julia&amp;gt; A = [1; 2; 3;; 4; 5; 6] 3×2 Matrix{Int64}: 1 4 2 5 3 6 Similarly, arrays of three</description></item><item><title>How to Use Elegant Loops in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2191/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2191/</guid><description>Guide while The while loop is no different from other languages. julia&amp;gt; while x &amp;lt; 10 x += 1 print(&amp;#34;▷eq1◁i - &amp;#34;) end 1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 - julia&amp;gt; for i = 1:10 print(</description></item><item><title>The Rotational Surface with Zero Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3222/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3222/</guid><description>Theorem1 Let $M$ be the unit speed curve of the rotational surface $\boldsymbol{\alpha}$ and let the Gaussian curvature be $K=0$. Then, $M$ satisfies one of the following conditions. It is a part of a cylinder. It is a part of a plane. It is a part of a cone. Moreover, these surfaces are locally isometric. Proof Let the Gaussian curvature of the rotational surface be $K = 0$. Since the</description></item><item><title>Definition of Batting in Baseball</title><link>https://freshrimpsushi.github.io/en/posts/2190/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2190/</guid><description>Definition The number of times a batter stands in the batter&amp;rsquo;s box and completes their batting turn (in any way leaves the batter&amp;rsquo;s box) is called a Plate Appearance, abbreviated as PA. Theorems [1]: The equation for a batter&amp;rsquo;s plate appearances PA, at-bats AB, walks BB, hit-by-pitches HBP, sacrifice bunts SH, sacrifice flies SF, and reaching base due to batter/runner interference $X$ is as follows: $$ \begin{align*} PA =&amp;amp; AB</description></item><item><title>Implementing MLP in Julia Flux and Learning with MNIST</title><link>https://freshrimpsushi.github.io/en/posts/3221/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3221/</guid><description>Loading the MNIST Dataset In older examples, you might see code using Flux.Data, but this is no longer supported in Flux. julia&amp;gt; Flux.Data.MNIST.images() ┌ Warning: Flux&amp;#39;s datasets are deprecated, please use the package MLDatasets.jl The official documentation1 advises using the `MLDatasets.jl&amp;rsquo; package. julia&amp;gt; using Flux julia&amp;gt; using MLDatasets julia&amp;gt; imgs = MLDatasets.MNIST.traintensor() 28×28×60000 reinterpret(FixedPointNumbers.N0f8, ::Array{UInt8, 3}) julia&amp;gt; labs = MLDatasets.MNIST.trainlabels()</description></item><item><title>Hub Nodes in Network Theory</title><link>https://freshrimpsushi.github.io/en/posts/2189/</link><pubDate>Sun, 17 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2189/</guid><description>Definition 1 A node that is connected to many other nodes in a network is called a hub. Description In network theory centrality, one of the answers to the question, &amp;ldquo;What constitutes an important node?&amp;rdquo; could be a node with a higher degree $\deg v$, i.e., a node that is connected to many other nodes. This intuition is considered to be commonsensical. The above picture is a log-log histogram plotting</description></item><item><title>Two Rotational Surfaces with Positive Curvature are Locally Isometric</title><link>https://freshrimpsushi.github.io/en/posts/3220/</link><pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3220/</guid><description>Theorem1 Let $M_{1}$, $M_{2}$ be the unit speed curves $\boldsymbol{\alpha}_{1}$, $\boldsymbol{\alpha}_{2}$ of the surfaces of revolution respectively. If $M_{1}$, $M_{2}$ have a constant curvature $a^{2} \gt 0$, then $M_{1}$, $M_{2}$ are locally isometric. Proof Lemma The following two propositions are equivalent: The two surfaces $M$ and $N$ are locally isometric. For all $p \in M$, there exist two coordinate patch mappings $\mathbf{x} : U \to M$, $\mathbf{y} : U \to</description></item><item><title>Introduction to the Korea Data Exchange (KDX)</title><link>https://freshrimpsushi.github.io/en/posts/2188/</link><pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2188/</guid><description>Introduction Unlike typical data hubs, this company sells data for a fee. As it is paid, it offers the highest level of data quality and quantity suitable for the Korean context, and a significant amount of data is also sold for free. Requirements It costs money. For example, the Daegu area real estate listing price data costs ten million won, and the July 2021 comprehensive e-commerce product-specific purchase and interest</description></item><item><title>How to Perform One-Hot Encoding in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/3219/</link><pubDate>Thu, 14 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3219/</guid><description>Overview One-hot encoding is the process of mapping data to standard basis vectors based on its classification. Flux provides functions for this. Code1 onehot() onehot(x, labels, [default]) Returns x .== labels. However, it does not return the exact same result, but returns a type called OneHotVector. For encoding multiple data points, use onehotbatch() below. julia&amp;gt; 3 .== [1,3,4] 3-element BitVector: 0 1 0 julia&amp;gt; Flux.onehot(3, [1,3,4]) 3-element OneHotVector(::UInt32) with eltype</description></item><item><title>Barabási-Albert Model</title><link>https://freshrimpsushi.github.io/en/posts/2187/</link><pubDate>Wed, 13 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2187/</guid><description>Algorithm 1 Input Given the link parameter $m \in \mathbb{N}$ and the network size $N$. Step 1. Initialization Construct the initial network with $m$ nodes. Unless otherwise specified, this network is a complete graph. Step 2. Adding Nodes Assuming the current number of nodes is $n$. A new node is added, which connects to each of the existing $m$ nodes. The probability of connecting to each $k = 1 ,</description></item><item><title>Positive Gaussian Curvature Surfaces of Revolution</title><link>https://freshrimpsushi.github.io/en/posts/3218/</link><pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3218/</guid><description>Overview1 The surface of revolution created by the unit speed curve $\boldsymbol{\alpha}(s) = \left( r(s), z(s) \right)$ is called $M$. $$ M = \left\{ \left( r(s)\cos\theta, r(s)\sin\theta, z(s) \right) : 0 \le \theta \le 2\pi, s \in (s_{0}, s_{1}) \right\} $$ The coordinate patch mapping $\mathbf{x}$ of $M$ is as follows. $$ \mathbf{x}(s, \theta) = \left( r(s)\cos\theta, r(s)\sin\theta, z(s) \right) $$ At this time, the Gaussian curvature of this surface</description></item><item><title>Introduction to the ITS National Transportation Information Center</title><link>https://freshrimpsushi.github.io/en/posts/2186/</link><pubDate>Mon, 11 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2186/</guid><description>Introduction The National Traffic Information Center provides domestic traffic flow, construction accidents, CCTV, traffic forecasts, vehicle detectors, VMS, traffic safety assistants, variable speed signs, vulnerable section information, and national standard node links in South Korea. In particular, the national standard node link, which represents all domestic road networks as a network, can be very useful for most data scientists. Requirements You can download without any requirements unlimitedly. Data Example National</description></item><item><title>List of Available Commands in Julia Package Management Mode</title><link>https://freshrimpsushi.github.io/en/posts/3217/</link><pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3217/</guid><description>Description By typing the right bracket ] in the Julia REPL, you can switch to package management mode. The available commands in package management mode are as follows. Command Function add foo Adds the package foo. free foo Unpins the package version. help, ? Shows these commands. pin foo Pins the version of the package foo. remove foo, rm foo Removes the package foo. test foo Test-runs the package foo.</description></item><item><title>Blue-Loo Fitness Model</title><link>https://freshrimpsushi.github.io/en/posts/2185/</link><pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2185/</guid><description>Definition A Fitness Model refers to a random network where weights are assigned to each node, and the probability of links being formed is varied according to these weights. Algorithm Input A null graph with $n \in \mathbb{N}$ nodes, denoted as $G$, is given. Chung-Lu model 1 Step 1. A degree sequence $\displaystyle \mathbf{w} := \left( w_{1} , \cdots , w_{n} \right)$, which satisfies $\displaystyle \max w_{k}^{2} &amp;lt; \sum_{k=1}^{n} w_{k}$,</description></item><item><title>Fundamental Theorem of Curved Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3216/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3216/</guid><description>Theorem1 For an open set $U \subset \mathbb{R}^{2}$, suppose any two points within $U$ are connected by a curve within $U$. Also, let the function $L_{ij}, g_{ij} : U \to \mathbb{R}\ (i,j = 1,2)$ be differentiable and have the following properties: $L_{12} = L_{21}$, $g_{12} = g_{21}$, $g_{11}, g_{22} &amp;gt; 0$, and $g_{11}g_{22} - (g_{12})^{2} &amp;gt; 0$ Assume that $L_{ij}, g_{ij}$ satisfies the Gauss equation and the Codazzi-Mainardi equation. $$</description></item><item><title>Introduction to the Meteorological Data Open Portal</title><link>https://freshrimpsushi.github.io/en/posts/2184/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2184/</guid><description>Introduction The Weather Data Portal offers a variety of datasets and open APIs including synoptic meteorological observation, climate, and seismic volcanic data. It contains everything that one would need for weather data, such as temperature, precipitation, fine dust, wind speed, humidity, and sunlight. Requirements Non-members can download living weather information and predictions by sea area, with a limit of up to 31 daily records and up to 24 hourly records</description></item><item><title>How to load a npy file in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3215/</link><pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3215/</guid><description>설명 This document outlines the process of calculating the Radon transform $\mathcal{R}f$ of a phantom $f$ in Python and saving the results as a *.npy file. To load this file in Julia, one can use the PyCall.jl package. using PyCall np = pyimport(&amp;#34;numpy&amp;#34;) The above code is equivalent to executing import numpy as np in Python. This allows one to directly use the code written for numpy in Python</description></item><item><title>Scale-Free Network</title><link>https://freshrimpsushi.github.io/en/posts/2183/</link><pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2183/</guid><description>Definition 1 A random network whose degree distribution follows a Pareto distribution is known as a Scale-free Network. Description The term Scale-free (SF) network comes from the scale-invariance of the Pareto distribution. Being defined by its degree distribution, it strongly inherits the properties of that distribution. Mathematically, the degree $v \in V(G)$ of nodes in a scale-free network $G$ can be described by some parameter $\gamma &amp;gt; 0$ as follows.</description></item><item><title>Differentiable Geometry: Local Isometries</title><link>https://freshrimpsushi.github.io/en/posts/3214/</link><pubDate>Mon, 04 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3214/</guid><description>Definition1 Let&amp;rsquo;s suppose a function $f : M \to N$ defined between two surfaces is given. If there exists an open set $$ U, V \ \text{ such that }\ p \in U \subset M, V \subset N $$ such that for all points $p \in M$, the contraction mapping $f|_{U} : U \to V$ becomes an isometry, then $M$ and $N$ are said to be locally isometric. Furthermore, such</description></item><item><title>Introduction to the Environmental Big Data Platform</title><link>https://freshrimpsushi.github.io/en/posts/2182/</link><pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2182/</guid><description>Introduction The Environmental Big Data Platform offers an environmental data market, visualization, educational services, and competitions. Although it&amp;rsquo;s called a market, there&amp;rsquo;s a lot of free data available, and the UI and data descriptions are clear, making it user-friendly. It uniquely holds a comprehensive and diverse range of environmental data. Requirements Login is required, and users must state their purpose each time they obtain data. While it seems that one</description></item><item><title>Overlaying Plots on Heatmaps in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3213/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3213/</guid><description>Code Let&amp;rsquo;s say we want to draw a sine curve from $0$ to $2\pi$ on the heatmap of the array $(5,5)$. You might want to write the code like this, but as you can see in the figure, it doesn&amp;rsquo;t output as desired. using Plots A = rand(Bool, 5,5) heatmap(A, color=:greens) x = range(0, 2pi, length=100) y = sin.(x) plot!(x, y, color=:red, width=3) This is because the horizontal and vertical</description></item><item><title>Pareto Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2181/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2181/</guid><description>Definition 1 For the scale parameter $x_{0} &amp;gt; 0$ and the shape parameter $\alpha &amp;gt; 0$, the following probability function is referred to as the Pareto Distribution, Power Law, or Scale-free Distribution: Continuous: For a constant $C$ that satisfies constant $\displaystyle \int_{x_{0}}^{\infty} p(x) dx = 1$ $$ p(x) = C x^{-\alpha} \qquad , x &amp;gt; x_{0} $$ Discrete: For the Riemann zeta function $\zeta$ $$ p_{k} = {{ 1 }</description></item><item><title>Differential Geometry: Isometric Mappings</title><link>https://freshrimpsushi.github.io/en/posts/3212/</link><pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3212/</guid><description>Definition1 Suppose a function $f : M \to N$ is given between two surfaces $M, N$. Then $f$ is called an isometry if it satisfies the following conditions: $f$ is differentiable. $f$ is bijective. For every curve $\boldsymbol{\gamma}:[c,d] \to M$, the length of $\boldsymbol{\gamma}$ and the length of $f \circ \boldsymbol{\gamma}$ are the same. If there exists an isometry $f$ between $M$ and $N$, then $M$ and $N$ are said</description></item><item><title>Using TeX in Plots in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2180/</link><pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2180/</guid><description>Code 1 To use the LaTeXStrings library, prefix strings with L, like so L&amp;quot;...&amp;quot;. @time using Plots @time using LaTeXStrings plot(0:0.1:2π, sin.(0:0.1:2π), xlabel = L&amp;#34;x&amp;#34;, ylabel = L&amp;#34;y&amp;#34;) title!(L&amp;#34;\mathrm{TeX\,representation:\,} y = \sin x , x \in [0, 2 \pi]&amp;#34;) Note that the package</description></item><item><title>Implementing MLP in Julia Flux and Optimizing with Gradient Descent</title><link>https://freshrimpsushi.github.io/en/posts/3211/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3211/</guid><description>MLP Implementation First, let&amp;rsquo;s load the machine learning package in Julia, Flux.jl, and the optimizer update method update!. using Flux using Flux: update! We can use the Dense() function for linear layers. The Chain() function stacks these linear layers, similar to the Sequential() feature in Keras and PyTorch. julia&amp;gt; model = Chain( Dense(10, 5, relu), Dense(5, 5, relu), Dense(5, 2) ) Chain( Dense(10, 5, relu), # 55 parameters Dense(5, 5,</description></item><item><title>A verb always follows "Let."</title><link>https://freshrimpsushi.github.io/en/posts/2179/</link><pubDate>Mon, 28 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2179/</guid><description>Grammar Let the verb in the clause &amp;ldquo;Let S V ~&amp;rdquo; be in its base form when referring to the subject S and the verb V. Example Sentences &amp;ldquo;Come on! Let&amp;rsquo;s start the killing.&amp;rdquo; &amp;ldquo;Let the killing begin.&amp;rdquo; - 복한규, former LOL pro gamer &amp;ldquo;Let&amp;rsquo;s suppose the infectious contact rate is $b=0.2 (&amp;gt;k)$.&amp;rdquo; &amp;ldquo;Let us further assume that the infectious contact rate is $b=0.2 (&amp;gt;k)$.&amp;rdquo; 1 Description</description></item><item><title>Differentiable Functions Between Two Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3210/</link><pubDate>Sun, 27 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3210/</guid><description>Definition1 Let&amp;rsquo;s say we are given a function $f : M \to N$ between two surfaces $M, N$. Suppose $\mathbf{x} : U \to M$ and $\mathbf{y} : V \to N$ are coordinate chart mappings that include the points $p \in M$, $f(p) \in N$, respectively. If $\mathbf{y}^{-1} \circ f \circ \mathbf{x} : U \to V$ is differentiable, then we say $f$ is differentiable at point $p$. Explanation Why don&amp;rsquo;t we</description></item><item><title>Introduction to investing.com</title><link>https://freshrimpsushi.github.io/en/posts/2178/</link><pubDate>Sat, 26 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2178/</guid><description>Introduction Investing.com is a global financial information site that offers free and easy access to chart information for stocks like KOSPI, KOSDAQ, etc. However, if you need real-time data or a more diverse set of data, you might consider using brokerage APIs such as CYBOS Plus. For those who only need a basic dataset for testing purposes, Investing.com is overwhelmingly convenient. Requirements Although it asks for login details, it&amp;rsquo;s practically</description></item><item><title>Handling Hidden Layers in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/3209/</link><pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3209/</guid><description>Linear1 In Flux, a linear layer can be implemented as Dense(). Dense(in, out, σ=identity; bias=true, init=glorot_uniform) Dense(W::AbstractMatrix, [bias, σ] The default value for the activation function is the identity function. Well-known functions like relu, tanh, and sigmoid can be used. julia&amp;gt; Dense(5, 2) Dense(5, 2) # 12 parameters julia&amp;gt; Dense(5, 2, relu) Dense(5, 2, relu) # 12 parameters julia&amp;gt;</description></item><item><title>Probability of Doing V at Rate a</title><link>https://freshrimpsushi.github.io/en/posts/2177/</link><pubDate>Thu, 24 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2177/</guid><description>Sentence Structure Assume that the verb V and the probability $a \in [0,1]$ are given. &amp;ldquo;It V at the rate $a$.&amp;rdquo; $\iff$ &amp;ldquo;$a$의 확률로 V하다.&amp;rdquo; Example Sentences &amp;ldquo;During the disease spread, each infected node transmits the disease to its susceptible neighbor nodes at the infection rate $\beta$, and</description></item><item><title>Gauss's Great Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3208/</link><pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3208/</guid><description>정리[^1] Gaussian curvature $K$ is intrinsic, and the following holds. $$ K = \dfrac{\sum\limits_{l} R_{121}^{l}g_{l2}}{g} $$ Here, $R_{ijk}^{l}$ are the coefficients of the Riemann curvature tensor, and $g$ and $g_{ij}$ are the coefficients of the Riemann metric. Corollary Since $R_{ijk}^{l} = \dfrac{\partial \Gamma_{ik}^{l}}{\partial u^{j}} - \dfrac{\partial \Gamma_{ij}^{l}}{\partial u^{k}} + \sum_{p} \left( \Gamma_{ik}^{p} \Gamma_{pj}^{l} - \Gamma_{ij}^{p}\Gamma_{pk}^{l}\right) \text{ for }$, the following holds. $$ K = \dfrac{1}{g}\left(</description></item><item><title>Introduction to Changwon City Big Data Portal</title><link>https://freshrimpsushi.github.io/en/posts/2176/</link><pubDate>Tue, 22 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2176/</guid><description>Introduction The Changwon Big Data Portal is a public data portal centered on Changwon city, offering over 900 million pieces of data, and services such as Big Data Studio and Commercial Zone Analysis. It also actively encourages data science by hosting analysis contests and competitions. However, it should be noted that, upon verification through a phone call, the data counts each cell individually, which means, contrary to expectations, most of</description></item><item><title>Performing Operations on Vectors of Different Sizes Component-wise in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3207/</link><pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3207/</guid><description>Description julia&amp;gt; x = [1 2 3] 1×3 Matrix{Int64}: 1 2 3 julia&amp;gt; y = [1 2 3 4] 1×4 Matrix{Int64}: 1 2 3 4 julia&amp;gt; x .+ y ERROR: DimensionMismatch Two vectors of different sizes cannot perform element-wise operations by default. To implement this manually, one would have to use a double for loop, but fortunately, it can be easily calculated by treating one as</description></item><item><title>Coupled Dynamic Systems</title><link>https://freshrimpsushi.github.io/en/posts/2175/</link><pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2175/</guid><description>Definition 1 Let us have a state space $X$. When the adjacency matrix of a network $\Gamma$ with $N$ nodes is denoted by $A$, and the state of node $i \in V \left( \Gamma \right)$ is represented as $x_{i} \in X$, then the following differential equation-based dynamical system is called a Coupled Dynamical System. $$ \dot{x_{i}} = f_{i} \left( x_{i} \right) + \sum_{i=1}^{N} A_{ji} c_{ji} \left( x_{j} , x_{i} \right)</description></item><item><title>Riemann Curvature Tensor, Gauss Equation, and Codazzi-Mainardi Equation in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3206/</link><pubDate>Sat, 19 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3206/</guid><description>Definition1 The coefficients of the Riemannian curvature tensor $R_{ijk}^{l}$ are defined as follows. $$ R_{ijk}^{l} = \dfrac{\partial \Gamma_{ik}^{l}}{\partial u^{j}} - \dfrac{\partial \Gamma_{ij}^{l}}{\partial u^{k}} + \sum_{p} \left( \Gamma_{ik}^{p} \Gamma_{pj}^{l} - \Gamma_{ij}^{p}\Gamma_{pk}^{l}\right) \text{ for } 1 \le i,j,k,l \le 2 $$ Here, $\Gamma_{ij}^{k}$ is the Christoffel symbol. Explanation Since Christoffel symbols are intrinsic, the Riemann curvature tensor is also intrinsic. The so-called coefficients that appear in differential geometry do not depend on</description></item><item><title>How to Fetch Short Selling Trends with CYBOS Plus</title><link>https://freshrimpsushi.github.io/en/posts/2174/</link><pubDate>Fri, 18 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2174/</guid><description>Code CpSysDib.CpSvr7238 requests and receives the trend of short selling by stock. Here’s a Python example to learn how to fetch short selling data of Seegene Inc. If you&amp;rsquo;re not familiar with CYBOS API, refer to the following guide first. How to Fetch Stock Prices with CYBOS Plus &amp;gt;&amp;gt;&amp;gt; import win32com.client &amp;gt;&amp;gt;&amp;gt; instCpStockCode = win32com.client.Dispatch(&amp;#34;CpUtil.CpStockCode&amp;#34;) &amp;gt;&amp;gt;&amp;gt; item = instCpS</description></item><item><title>Handling the Dimensions and Sizes of PyTorch Tensors</title><link>https://freshrimpsushi.github.io/en/posts/3205/</link><pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3205/</guid><description>Definition Let&amp;rsquo;s call $A$ a PyTorch tensor. The following pair $(a_{0}, a_{1}, \dots, a_{n-1})$ is called the size of $A$. $$ \text{A.size() = torch.Size}([a_{0}, a_{1}, \dots, a_{n-1} ]) $$ Let&amp;rsquo;s refer to $\prod \limits_{i=0}^{n-1} a_{i} = a_{0} \times a_{1} \times \cdots a_{n-1}$ as the dimension of $A$. Call $A$ a $n$-dimensional tensor. $a_{i}$ are the sizes of the respective $i$th dimensions, which are integers greater than $1$. Since this is</description></item><item><title>How to Make a Transparent Background in Graphics with Julia</title><link>https://freshrimpsushi.github.io/en/posts/2173/</link><pubDate>Wed, 16 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2173/</guid><description>Code 1 If the browser is in dark mode, you can clearly see that the background has been rendered transparent. You just need to insert the :transparent symbol into the background_color option. It saves well as *.png, but it is said that it doesn&amp;rsquo;t save well as *.pdf. using Plots plot(rand(10), background_color = :transparent) png(&amp;#34;example&amp;#34;) As you can guess from the option name, if you put in a color symbol,</description></item><item><title>Necessary and Sufficient Conditions for a Vector Field to be Parallel Along a Curve</title><link>https://freshrimpsushi.github.io/en/posts/3204/</link><pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3204/</guid><description>Theorem1 Let $\boldsymbol{\gamma}(t) = \mathbf{x}\left( \gamma^{1}(t), \gamma^{2}(t) \right)$ be a regular curve on $\mathbf{x}$, the coordinate patch. Let $\mathbf{X}(t)$ be a differentiable vector field along the curve $\boldsymbol{\gamma}$. $$ \mathbf{X} = X^{1}\mathbf{x}_{1} + X^{2}\mathbf{x}_{2} $$ Then, the necessary and sufficient condition for $\mathbf{X}(t)$ to be parallel along $\boldsymbol{\gamma}$ is as follows. $$ 0 = \dfrac{d X^{k}}{d t} + \sum_{i,j} \Gamma_{ij}^{k} X^{i}\dfrac{d \gamma^{j}}{d t},\quad k=1,2 $$ Proof The definition of $\mathbf{X}$</description></item><item><title>How to Import Institutional and Foreign Trade Volume with CYBOS Plus</title><link>https://freshrimpsushi.github.io/en/posts/2172/</link><pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2172/</guid><description>Guide1 CpSysDib.CpSvr7254 is used to check the status of investment by party on a daily-period basis, net purchase-trading proportion, and amount-value on a daily basis. Let&amp;rsquo;s learn how to use it with a Python example that fetches data by investor type for Seegene Inc. If you&amp;rsquo;re not familiar with the CYBOS API, refer to the following guide first. How to Fetch Stock Prices using CYBOS Plus Highlights &amp;gt;&amp;gt;&amp;gt; data7254 종</description></item><item><title>Methods for Coloring Up to a Certain Value from Curves in Julia / Between Two Curves / Inside a Closed Curve</title><link>https://freshrimpsushi.github.io/en/posts/3203/</link><pubDate>Sun, 13 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3203/</guid><description>Fill up to a Specific Value1 Using attributes fillrange=a, fillalpha=b, fillcolor=:color in plot(), it colors with :color to the value a from the plotted curve with the transparency b. It works the same by writing fill=(a,b,:color). That is, the following two codes are the same. plot(x,y, fillrange=a, fillalpha=b, fillcolor=:color) plot(x,y, fill=(a,b,:color)) It seems to be a bug, but selecting the value of fillrange as $(0,1)$ does not get colored. using</description></item><item><title>All is Followed by Either a Plural Countable Noun or a Singular Uncountable Noun</title><link>https://freshrimpsushi.github.io/en/posts/2171/</link><pubDate>Sat, 12 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2171/</guid><description>Grammar All followed by a noun, referred to as N. The N following All must be either a plural countable noun or a singular uncountable noun, and the number of &amp;ldquo;All N&amp;rdquo; must match the number of N. Examples Plural &amp;ldquo;All red tiles indicate chimera states that emerge in both layers.&amp;rdquo; 1 Since tile is a countable noun, it comes after All in the plural form tiles. The subject &amp;ldquo;All</description></item><item><title>Definition and Relationship between the Gaussian Map and Gaussian Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3202/</link><pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3202/</guid><description>Definition1 A function $\nu$ that maps each point $p$ on a surface $M$ to a unit normal is called the Gaussian map. $$ \nu : M \to \mathbb{S}^{2} \quad \text{and} \quad \nu (p) = \mathbf{n}_{p} $$ Description The Gaussian map is also referred to as the normal spherical image. Theorem Let us call the area of any region $\mathscr{R}$ on the surface $A(\mathscr{R})$ as the area of $\mathscr{R}$. Then the</description></item><item><title>How to Fetch Stock Prices for Securities Using CYBOS Plus CpSysDib.StockChart</title><link>https://freshrimpsushi.github.io/en/posts/2170/</link><pubDate>Thu, 10 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2170/</guid><description>Guide 1 CpSysDib.StockChart receives chart data for stocks, sectors, and ELW. If you&amp;rsquo;re not familiar with using the API, understanding this system can be quite challenging. Let&amp;rsquo;s run a Python example that fetches the stock price of Seegene Inc. and understand how it works. SetInputValue(): Specifically designates the data you want. BlockRequest(): Requests reception of the designated data. GetHeaderValue(): Returns the received header data. GetDataValue(): Returns the received data. The</description></item><item><title>A Singular Noun Must Always Follow "Every" and "Each"</title><link>https://freshrimpsushi.github.io/en/posts/2169/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2169/</guid><description>Grammar E stands for Every or Each, and let&amp;rsquo;s denote the noun following E as N. The N following E must be singular, and &amp;ldquo;E N&amp;rdquo; as a whole is treated as singular. Examples Every &amp;ldquo;In consequence, every coupling function in an undirected graph has to be called twice, once with $x_i$ as its first and $x_j$ as its second argument and once the other way round.&amp;rdquo; 1 Instead of</description></item><item><title>Gaussian Curvature and Mean Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3200/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3200/</guid><description>Definition1 Let&amp;rsquo;s consider the principal curvature at a point $p$ on a surface $M$ be denoted as $\kappa_{1}, \kappa_{2}$. Let $L$ be referred to as the Weingarten map. The Gaussian curvature $K$ is defined as follows: $$ K := \kappa_{1} \kappa_{2} = \det L = \det ([{L^{i}}_{j}]) $$ where ${L^{i}}_{j} = \sum \limits_{k} L_{kj}g^{ki}$ applies. Formula The product of the principal curvatures $$ K = \kappa_{1} \kappa_{2} $$ Gaussian curvature</description></item><item><title>How to Load Stock Codes with CYBOS Plus CpUtil.CpStockCode</title><link>https://freshrimpsushi.github.io/en/posts/2168/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2168/</guid><description>Guide 1 CpUtil.CpStockCode provides methods related to the stock codes. NameToCode(): Takes a stock name as a string and returns the code as a string. CodeToName(): Takes a code as a string and returns the stock name as a string. Let’s run the following Python code using Seegene (096530) as an example. &amp;gt;&amp;gt;&amp;gt; import win32com.client &amp;gt;&amp;gt;&amp;gt; instCpStockCode = win32com.client.Dispatch(&amp;#34;CpUtil.CpStockCode&amp;#34;) &amp;gt;&amp;gt;&amp;gt; instCp</description></item><item><title>How to Pad PyTorch Tensors</title><link>https://freshrimpsushi.github.io/en/posts/3199/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3199/</guid><description>Code 1 torch.nn.functional.pad(input, pad, mode='constant', value=0.0) input: The tensor to pad pad: Where to pad mode: Method of padding value: Value for padding Description pad When using a tensor of dimension $n$ as an input, you can pass up to $2n-$pairs of arguments. $$ ((n-1)_{\text{low}}, (n-1)_{\text{up}}, (n-2)_{\text{low}}, (n-2)_{\text{up}}, \dots, 1_{\text{low}}, 1_{\text{up}}, 0_{\text{low}}, 0_{\text{up}}) $$ $i_{\text{low}}$ indicates how many values to pad before the $i$th dimension&amp;rsquo;s lower index, i.e., before the</description></item><item><title>Fractal Brownian Motion</title><link>https://freshrimpsushi.github.io/en/posts/2167/</link><pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2167/</guid><description>Definition $E \left( X_{t} \right) = 0$ given $X_{t}$ is a Gaussian process and let&amp;rsquo;s denote it by $H \in (0, 1)$. The Fractional Brownian motion can be defined in the following two ways. Definition through Covariance 1 The covariance at time point $t, s$ of $X_{t}$, if it follows the expression below, is referred to as Fractional Brownian Motion. $$ \operatorname{Cov} \left( X_{t}, X_{s} \right) = {{ 1 }</description></item><item><title>Properties of Vector Fields Parallel to a Curve</title><link>https://freshrimpsushi.github.io/en/posts/3198/</link><pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3198/</guid><description>Properties Let $\mathbf{X}(t)$ and $\mathbf{Y}(t)$ be vectors parallel to a regular curve $\alpha (t)$ on the surface $M$. Then the angle between $\mathbf{X}$ and $\mathbf{X}(t), \mathbf{Y}(t)$, and the magnitude of $\left\| \mathbf{X}(t) \right\|$ are constants. Description In other words, both the angle and magnitude are conserved. Proof Let $f(t) = \left\langle \mathbf{X}(t), \mathbf{Y}(t) \right\rangle$. Differentiating $f$, by the differentiation of inner products, we get: $$ \dfrac{d f}{d t} = \left\langle</description></item><item><title>CYBOS Plus Installation Tutorial</title><link>https://freshrimpsushi.github.io/en/posts/2166/</link><pubDate>Wed, 02 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2166/</guid><description>Environment OS: Windows IDE: VS code Guide This tutorial is for installing CYBOS Plus on Windows and checking its connection status with Python. It is assumed that the user is somewhat familiar with Python&amp;rsquo;s object-oriented nature and dealing with an IDE. Step 1. Install CYBOS Plus Let&amp;rsquo;s download CYBOS Plus from the Daishin Securities Download Center. In fact, it&amp;rsquo;s not something you can download separately, as CYBOS Plus is integrated</description></item><item><title>How to Remove Axes in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3197/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3197/</guid><description>Code1 If you want to hide the axes on a graph, you can use plt.axis('off') (or False instead of &amp;lsquo;off&amp;rsquo;). This will make both the x and y axes disappear. If you wish to remove only one of the axes, you can apply a method like the example below to the current figure&amp;rsquo;s axis object, plt.gca(). import numpy as np import matplotlib.pyplot as plt plt.subplots(figsize=(15,3)) plt.subplot(1,4,1) plt.plot(x,y) plt.subplot(1,4,2) plt.plot(x,y) plt.gca().axes.xaxis.set_visible(False)</description></item><item><title>How to Place a Large Bracket on Only One Side in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/2165/</link><pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2165/</guid><description>Code $$ \left. \int {{ 1 } \over { g (u) }} du \right|\_{u = X\_{t}} $$ $$ \left. \int {{ 1 } \over { g (u) }} du \right|_{u = X_{t}} $$ In large brackets, if one side (left or right) is unnecessary, you can maintain the size while omitting the rendering by using \left., \right..</description></item><item><title>Euler's Theorem in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3196/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3196/</guid><description>Theorem1 Let&amp;rsquo;s define the unit tangent vector to the surface $M$ at point $p$ as $\mathbf{Y}$. $$ \mathbf{Y} \in T_{p}M \quad \text{and} \quad \left\| \mathbf{Y} \right\| = 1 $$ Let $\kappa_{1} \ge \kappa_{2}$ represent the principal curvature at $p$. Then, the following equation holds: $$ II(\mathbf{Y}, \mathbf{Y}) = \kappa_{1} \cos^{2} \theta + \kappa_{2} \sin^{2} \theta $$ In this context, $II$ represents the second fundamental form, $\mathbf{X}_{1}$ represents the principal direction</description></item><item><title>Introduction to Investment Information Open API CYBOS Plus</title><link>https://freshrimpsushi.github.io/en/posts/2164/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2164/</guid><description>Introduction CYBOS is a trading system developed by Daishin Securities, offering securities data through CYBOS Plus as an Open API. Requirements A Daishin Securities account and ID are needed, along with a joint authentication certificate to request data while logged into the service. Daewoo Securities accounts can be opened non-face-to-face via the CYBOS Touch app in the app store. An ID card, smartphone, and an account with another financial institution</description></item><item><title>Drawing Vertical and Horizontal Lines in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3195/</link><pubDate>Fri, 25 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3195/</guid><description>CODE1 2 import numpy as np import matplotlib.pyplot as plt x = np.linspace(0,2*np.pi,num=1000) y = np.sin(x) plt.plot(x,y) plt.show() axhline(y=0, xmin=0, xmax=1, **kwargs) axvline(x=0, ymin=0, ymax=1, **kwargs) Horizontal lines can be added with plt.axhline(), and vertical lines with plt.axvline(). The min/max values for the range to draw these lines are not real values but should be entered as a ratio between 0 and 1. The major options include: color or c</description></item><item><title>Prime and Composite Numbers</title><link>https://freshrimpsushi.github.io/en/posts/2163/</link><pubDate>Thu, 24 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2163/</guid><description>Definition 1 A prime number is a natural number $p \ge 2$ that has only $1$ and $p$ as its divisors. A natural number $m \ge 2$ that is not a prime number is called a composite number. Explanation According to the definition, $2$ is naturally a prime number. The scope of numbers dealt with in Number Theory is quite broad, extending to rational numbers, but the actual subject of</description></item><item><title>Curvature of a Principal Curve</title><link>https://freshrimpsushi.github.io/en/posts/3194/</link><pubDate>Wed, 23 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3194/</guid><description>Buildup1 To know in which direction and how much a surface $M$ is curved, it is sufficient to know the normal curvatures $\kappa_{n}$ in each direction. In other words, knowing all $\kappa_{n}$ at point $p$ allows us to understand how $M$ is bent. The first step towards this is to think about the maximum and minimum values of $\kappa_{n}$. The following theorem applies to the unit tangent curve $\boldsymbol{\gamma}$: Lemma</description></item><item><title>Introduction to Kaggle</title><link>https://freshrimpsushi.github.io/en/posts/2162/</link><pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2162/</guid><description>Introduction Kaggle is the world&amp;rsquo;s most famous open data hub, offering an endless variety of data and hosting numerous competitions. While not every dataset is available on Kaggle, there is no type of data that isn&amp;rsquo;t. If you&amp;rsquo;re tackling a project for a statistics or machine learning class, Kaggle should be your first stop. Requirements Although registration is required, it&amp;rsquo;s practically unrestricted thanks to Google integration. English might be a</description></item><item><title>The Relationship between the Fundamental Form and Coordinate Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3193/</link><pubDate>Mon, 21 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3193/</guid><description>Overview1 Given the coordinate transformation $f : V \to U$, this explains the relationship between the metric $g$ on $U$ and the metric $\overline{g}$ on $V$. Einstein notation is used. Formulas For the metric $g$ of coordinate patch mapping $\mathbf{x} : U \to \mathbb{R}^{3}$ and the metric $\overline{g}$ of $\mathbf{y} = \mathbf{x} \circ f : V \to \mathbb{R}^{3}$, and the tangent vector $\mathbf{X} = X^{i}\mathbf{x}_{i} = \overline{X}^{\alpha} \mathbf{y}_{\alpha}$, the following</description></item><item><title>Self-similarity and the Hurst Index of Stochastic Processes</title><link>https://freshrimpsushi.github.io/en/posts/2161/</link><pubDate>Sun, 20 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2161/</guid><description>Definition 1 2 A stochastic process $\left\{ X_{t} \right\}$ is said to be $H$-self-similar if for all $a &amp;gt; 0$, it satisfies the following equation. $$ X_{at} \overset{D}{=} a^{H} X_{t} $$ Here, $\overset{D}{=}$ denotes equality in distribution, and the parameter $H&amp;gt;0$ is referred to as the Hurst Index. Example Considering the Brownian motion $W_{t}$, where $W_{t} \sim N(0,t)$ applies. For instance, regarding a random variable $Z$ that follows a normal</description></item><item><title>The Relationship between the Second Normal Form and the Vingarten Map</title><link>https://freshrimpsushi.github.io/en/posts/3192/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3192/</guid><description>Theorem1 Let&amp;rsquo;s call the tangent vector for a point $p$ on a surface $M$ as $\mathbf{X}, \mathbf{Y} \in T_{p}M$. Then the following holds. $$ II(\mathbf{X}, \mathbf{Y}) = \left\langle L(\mathbf{X}), \mathbf{Y} \right\rangle = \left\langle \mathbf{X}, L(\mathbf{Y}) \right\rangle $$ Here, $L$ is the Weingarten map. Description In other words, the Weingarten map $L$ is a self-adjoint linear transformation. Proof Properties of the Weingarten Map If we define ${L^{l}}_{k} = \sum \limits_{i} L_{ik}g^{il}$,</description></item><item><title>How to Download Data Using Kaggle API, Solving OSError: Could Not Find kaggle.json.</title><link>https://freshrimpsushi.github.io/en/posts/2160/</link><pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2160/</guid><description>Overview Example Data Link The data on Kaggle can be substantial in size, making it somewhat risky to rely solely on a web browser&amp;rsquo;s download functionality. Therefore, an API that allows for the stable download of high-capacity data is provided, and the code at the very top of the above screenshot pertains to it. By entering pip install kaggle in the terminal to install kaggle and then inputting kaggle competitions</description></item><item><title>Using Machine Learning Datasets in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3191/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3191/</guid><description>Description The MLDatasets.jl1 2 package allows for the use of the following datasets. Datasets with links have their usage explained in their respective documents. Vision CIFAR10 CIFAR100 EMNIST FashionMNIST MNIST Omniglot SVHN2 convert2image Mesh FAUST Miscellaneous BostonHousing Iris Mutagenesis Titanic Text PTBLM SMSSpamCollection UD_English Graphs CiteSeer Cora Graph HeteroGraph KarateClub MovieLens OGBDataset OrganicMaterialsDB PolBlogs PubMed Reddit TUDataset For one-hot encoding this data or training methods, refer to the following. How</description></item><item><title>가우스 과정</title><link>https://freshrimpsushi.github.io/en/posts/2159/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2159/</guid><description>Definition 1 For every finite subset $S = \left\{ X_{t_{k}} \right\}_{k=1}^{n} \subset \left\{ X_{t} \right\}$ of a stochastic process $\left\{ X_{t} \right\}$, all linear combinations $S$ of the elements $$ \sum_{k=1}^{n} a_{k} X_{t_{k}} \qquad , \left\{ a_{k} \right\}_{k=1}^{n} \subset \mathbb{R} $$ are said to follow a multivariate normal distribution. If $\left\{ X_{t} \right\}$ satisfies this condition, it is called a Gaussian process. Explanation To non-specialists, the definition might appear excessively</description></item><item><title>Bingarten Equation</title><link>https://freshrimpsushi.github.io/en/posts/3190/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3190/</guid><description>Theorem1 On the surface $M$, the following equation holds. $$ \mathbf{n}_{j} = - \sum_{k} {L^{k}}_{j}\mathbf{x}_{k} $$ Here, $\mathbf{x} : U \to M$ is a coordinate chart mapping, $\mathbf{n}$ is the unit normal, and ${L^{k}}_{j} = \sum\limits_{i}L_{ij}g^{ik}$ is. Explanation Consider the Frenet-Serret frame $\left\{ \mathbf{T}, \mathbf{N}, \mathbf{B} \right\}$ of a curve. Since these are three mutually orthogonal vectors, they form a basis of $\mathbb{R}^{3}$. Moreover, the derivative of each is expressed</description></item><item><title>Introduction to AI Hub</title><link>https://freshrimpsushi.github.io/en/posts/2158/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2158/</guid><description>Introduction It handles various formats such as images, videos, texts, audios, 3D, and sensor data in fields like voice, natural language, vision, healthcare, autonomous driving, safety, agriculture and fisheries, national land environments, and education. It seems to be provided by the Korea Institute of Intelligent Information Society located in Dong-gu, Daegu Metropolitan City. The description of the data&amp;rsquo;s form, structure, and application area is very detailed, and in addition, it</description></item><item><title>How to Get Column and Row Labels of Data Frame in Python Pandas</title><link>https://freshrimpsushi.github.io/en/posts/3189/</link><pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3189/</guid><description>Code import pandas as pd data = { &amp;#39;나이&amp;#39; : [26,23,22,22,21,21,20,20,20,20,18,17], &amp;#39;키&amp;#39; : [160, 163, 163, 162, 164, 163, 164, 150, 158, 162, 172, 173], &amp;#39;별명&amp;#39; : [&amp;#39;땡모&amp;#3</description></item><item><title>The Difference Between == and === in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2157/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2157/</guid><description>Code 1 == compares whether values are the same, and === operates differently depending on whether the values to be compared are Mutable or not. Mutable: Checks if both terms refer to the same object, in other words, it returns whether the two variables can be programmatically distinguished or not. Immutable: Checks if both terms are of the same type, Checks if both terms have the same structure, And recursively</description></item><item><title>Bingarten Map</title><link>https://freshrimpsushi.github.io/en/posts/3188/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3188/</guid><description>Definition1 Let $M$ be a surface, and $p \in M$ be a point on the surface. The map $L : T_{p}M \to \mathbb{R}^{3}$, defined as follows, is called the Weingarten map. $$ L (\mathbf{X}) = - \mathbf{X}\mathbf{n} $$ Here, $\mathbf{X} \in T_{p}M$ is a tangent vector, $\mathbf{n}$ is a unit normal, and $\mathbf{X}\mathbf{n}$ is the directional derivative of $\mathbf{n}$. Properties $L$ is a linear transformation that is $L : T_{p}M</description></item><item><title>Derivation of Black-Scholes Model</title><link>https://freshrimpsushi.github.io/en/posts/2156/</link><pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2156/</guid><description>Model 1 At time point $t$, let&amp;rsquo;s say the price of $S_{t}$ units of the underlying asset $1$, and assume that $S_{t}$ undergoes Geometric Brownian Motion. That is, for Standard Brownian Motion $W_{t}$, drift $\mu \in \mathbb{R}$, and diffusion $\sigma^{2} &amp;gt; 0$, $S_{t}$ is the solution to the following Stochastic Differential Equation. $$ d S_{t} = S_{t} \left( \mu dt + \sigma d W_{t} \right) $$ When a risk-free rate</description></item><item><title>How to Concatenate or Stack Tensors in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3187/</link><pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3187/</guid><description>Concatenate Tensors cat()1 cat(tensors, dim=0) concatenates two or more tensors along a specified dimension. This means that the size of the specified dimension increases when the tensors are concatenated. Therefore, it is natural that the sizes of the other dimensions need to be the same. For example, if there are tensors $(2,2)$ and $(2,3)$, they cannot be concatenated along the 0th dimension but can be concatenated along the 1st dimension.</description></item><item><title>How to Check Python Package, Library, and Module Versions</title><link>https://freshrimpsushi.github.io/en/posts/2155/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2155/</guid><description>Guide You can check the version with the following commands in the console. Overall, list is cleaner to look at, but freeze is easier to read when checking the version of a specific package.</description></item><item><title>Definition of Normal Sections and Menelaus's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3186/</link><pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3186/</guid><description>Definition1 Let&amp;rsquo;s suppose a curve $\boldsymbol{\gamma}$ is given on a surface $M$. We denote by $\Pi$ the plane generated by the normal $\mathbf{n}(p)$ and $\boldsymbol{\gamma}^{\prime}(p) \in T_{p}M$ at $p \in M$. The normal section at $M \cap \Pi$ in the direction from $p$ to $\boldsymbol{\gamma}^{\prime}$ on $M$ is referred to as $M \cap \Pi$. Theorem2 Let&amp;rsquo;s denote by $\boldsymbol{\gamma}(s)$ the unit-speed curve on the surface $M$, which has the normal</description></item><item><title>Geometric Brownian Motion</title><link>https://freshrimpsushi.github.io/en/posts/2154/</link><pubDate>Sun, 06 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2154/</guid><description>Definition 1 Let&amp;rsquo;s say the following Stochastic Differential Equation (SDE) is given by $\mu \in \mathbb{R}$ and $\sigma^{2} &amp;gt; 0$. $$ d X_{t} = X_{t} \left( \mu dt + \sigma d B_{t} \right) $$ The solution of this SDE is found as a Stochastic Process for the initial value $X_{0}$, which is referred to as Geometric Brownian Motion. $$ X_{t} = X_{0} \exp \left[ \left( \mu - {{ \sigma^{2} }</description></item><item><title>How to Set Plot Scale Range in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3185/</link><pubDate>Sat, 05 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3185/</guid><description>Code1 You can fix the scale of the image using plt.clim(). However, plt.imshow() does not print the color bar by default, so you need to add plt.colorbar() to see it. import numpy as np import matplotlib.pyplot as plt A = np.random.rand(4,4) plt.imshow(A) plt.colorbar() plt.show() plt.imshow(A) plt.colorbar() plt.clim(0,1) plt.show() The results are as follows. Unlike the first image, in the second image, you can see that the range of the color</description></item><item><title>Log-Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2153/</link><pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2153/</guid><description>Definition 1 The continuous probability distribution $\log N \left( \mu,\sigma^{2} \right)$, which has the probability density function given for $\mu \in \mathbb{R}$ and $\sigma^{2} &amp;gt; 0$, is known as the log-normal distribution. $$ f(x) = {{ 1 } \over { x \sigma \sqrt{2 \pi}}} \exp \left[ - {{ \left( \log x - \mu \right)^{2} } \over { 2 \sigma^{2} }} \right] \qquad, x &amp;gt; 0 $$ Description In fact, the</description></item><item><title>Properties of the Second Normal Form</title><link>https://freshrimpsushi.github.io/en/posts/3184/</link><pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3184/</guid><description>Definition The second fundamental form is defined as a bilinear form on the tangent space $T_{p}M$. For two tangent vectors $\mathbf{X}=\sum X^{i}\mathbf{x}_{i}$ and $\mathbf{Y} = \sum Y^{j}\mathbf{x}_{j}$, it is defined as: $$ II ( \mathbf{X}, \mathbf{Y}) = \sum _{i,j} L_{ij} X^{i} Y^{j} $$ where the coefficients $L_{ij}$ are as follows. $$ L_{ij} = \left\langle \mathbf{x}_{ij}, \mathbf{n} \right\rangle $$ Properties1 $II$ is symmetric. If $\mathbf{T}$ is the tangent field of a</description></item><item><title>Shoji-Ozaki Local Linearization Method</title><link>https://freshrimpsushi.github.io/en/posts/2152/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2152/</guid><description>Build-up1 $$ d X_{t} = f \left( t, X_{t} \right) dt + g \left( X_{t} \right) d W_{t} $$ Let&amp;rsquo;s assume that the diffusion $g$ is dependent only on $X_{t}$ and independent of time $t$ as given by the following stochastic differential equation. If $Y_{t}$ represents some constant $\sigma$ such that $\phi &amp;rsquo; \left( X_{t} \right) g \left( X_{t} \right) = \sigma$ is $\phi \in C^{2}$, which is then expressed</description></item><item><title>How to Obtain the Weight Values of a Model in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3183/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3183/</guid><description>Explanation Let&amp;rsquo;s define a model as follows. import torch import torch.nn as nn class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.linear = nn.Linear(3, 3, bias=True) self.conv = nn.Conv2d(3, 5, 2) f = Model() Then, you can access the weights and biases of each layer with the .weight and .bias methods, respectively. Note that the values obtained through .weight (.bias) are not tensors but Parameter objects. So, if you want to get</description></item><item><title>Python shutil Module Summary</title><link>https://freshrimpsushi.github.io/en/posts/2151/</link><pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2151/</guid><description>Overview 1 shutil is a standard module that gathers high-level commands for files and directories. One of the reasons to use Python is its capability to write programs quickly and conveniently. However, it&amp;rsquo;s quite inconvenient as the functionalities that logically should be available are scattered between the shutil module and os module. When handling the file system, it&amp;rsquo;s necessary to use both modules wisely. Even though this is a comprehensive</description></item><item><title>Geodesic Coordinate Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3182/</link><pubDate>Sun, 30 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3182/</guid><description>Definition1 Let&amp;rsquo;s define $U \subset \mathbb{R}^{2}$ as an open set. Define $\mathbf{x} : U \to \mathbb{R}^{3}$ as a coordinate chart that satisfies the following: $$ g_{11} = 1 \quad \text{and} \quad g_{12} = g_{21} = 0 $$ $$ \left[ g_{ij} \right] = \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; g_{22} \end{bmatrix} $$ In this case, $g_{ij}$ is the coefficient of the first fundamental form. Such $\mathbf{x}$ is called a geodesic</description></item><item><title>Lambert Transformation</title><link>https://freshrimpsushi.github.io/en/posts/2150/</link><pubDate>Sat, 29 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2150/</guid><description>Definition 1 $$ d X_{t} = f \left( t , X_{t} \right) dt + g \left( X_{t} \right) d W_{t} $$ Let&amp;rsquo;s assume that the diffusion $g$ is dependent only on $X_{t}$ and independent of time $t$, given the stochastic differential equation (SDE) as shown above. The transformation $F : X_{t} \mapsto Y_{t}$ is called the Lamperti Transformation. $$ Y_{t} := F \left( X_{t} \right) = \left. \int {{ 1</description></item><item><title>Solutions When Python npy Files Won't Open</title><link>https://freshrimpsushi.github.io/en/posts/3181/</link><pubDate>Fri, 28 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3181/</guid><description>Error # code1 Solution If you encounter the above error when opening a npy file, you can solve it by adding allow_pickle=True as follows. # code2</description></item><item><title>How to Invert a Bit Array in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2149/</link><pubDate>Thu, 27 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2149/</guid><description>Code 1 It&amp;rsquo;s quite simple, but a common mistake is to treat the negation operators ! and ~ not as unary operators but as functions, and use !. or ~.. They should be written as .! or .~ instead. julia&amp;gt; a = rand(1,10) .&amp;lt; 0.5 1×10 BitMatrix: 1 1 0 0 1 0 1 0 0 0 julia&amp;gt; .!(a) 1×10 BitMatrix: 0 0 1</description></item><item><title>Directional Derivatives in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3180/</link><pubDate>Wed, 26 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3180/</guid><description>Definition1 Let $\mathbf{X} \in T_{p}M$ be a tangent vector, and let $\alpha (t)$ be a curve on the surface $M$. Then, we have $\alpha : (-\epsilon, \epsilon) \to M$ and it satisfies $\alpha (0) = p$. In other words, $\mathbf{X} = \dfrac{d \alpha}{d t} (0)$. Now, let&amp;rsquo;s say the function $f$ is a differentiable function defined in some neighborhood of point $p \in M$ on the surface $M$. Then, the</description></item><item><title>Milstein Method Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2148/</link><pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2148/</guid><description>Method 1 $$ d X(t) = f \left( X_{t} \right) dt + g \left( X_{t} \right) d W_{t} \qquad , t \in [t_{0}, T] $$ Given that the Ito process is a solution to the autonomous stochastic differential equation described above. For equally spaced intervals of $h$, the calculation expressed for the equally spaced points $\left\{ t_{i} \le T : t_{i+1} = t_{i} + h \right\}_{i=0}^{N}$ is a numerical solution</description></item><item><title>Writing Multiple for Loops in One Line Using Python</title><link>https://freshrimpsushi.github.io/en/posts/3179/</link><pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3179/</guid><description>Code To repeat for index i up to 2 and j up to 4, you can write the for loop as follows: &amp;gt;&amp;gt;&amp;gt; for i in range(3): ... for j in range(5): ... if j == 4: ... print((i,j)) ... else : ... print((i,j), end=&amp;#34;&amp;#34;) ... (0, 0)(0, 1)(0, 2)(0, 3)(0, 4) (1, 0)(1, 1)(1, 2)(1, 3)(1, 4) (2, 0)(2, 1)(2, 2)(2, 3)(2, 4) Using the product() from the standard</description></item><item><title>Python OS Module Summary</title><link>https://freshrimpsushi.github.io/en/posts/2147/</link><pubDate>Sun, 23 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2147/</guid><description>Overview 1 os is a base module that collects simple commands for operating system-dependent features. One of the reasons to use Python is that it is easy and quick to write programs, but it is very inconvenient that features that logically should be there are mixed with the shutil module. When handling the file system, you need to use both modules equally. It&amp;rsquo;s a summary, but it&amp;rsquo;s not just scraped</description></item><item><title>If It's the Shortest Curve, It's a Geodesic</title><link>https://freshrimpsushi.github.io/en/posts/3178/</link><pubDate>Sat, 22 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3178/</guid><description>Theorem1 Let&amp;rsquo;s say $\boldsymbol{\gamma}$ is a unit speed curve connecting two points $P = \boldsymbol{\gamma}(a), Q = \boldsymbol{\gamma}(b)$ on surface $M$. If $\boldsymbol{\gamma}$ is the shortest distance curve connecting $P$ and $Q$, then $\boldsymbol{\gamma}$ is a geodesic. Explanation The converse does not hold. In other words, a geodesic is not necessarily the shortest distance curve. Proof Strategy: Prove by contradiction. What needs to be shown is $\kappa_{g} = 0$, so</description></item><item><title>Euler-Maruyama Method Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2146/</link><pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2146/</guid><description>Methods 1 $$ d X(t) = f \left( X_{t} \right) dt + g \left( X_{t} \right) d W_{t} \qquad , t \in [t_{0}, T] $$ Let&amp;rsquo;s say that the Itô process is given as a solution to the autonomous stochastic differential equation as shown above. For the equidistant points $\left\{ t_{i} \le T : t_{i+1} = t_{i} + h \right\}_{i=0}^{N}$ with a constant interval of $h$, $Y_{i} :=</description></item><item><title>How to Deep Copy Tensors in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3177/</link><pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3177/</guid><description>Description PyTorch tensors, like other objects, can be deep-copied using copy.deepcopy(). &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import copy &amp;gt;&amp;gt;&amp;gt; a = torch.ones(2,2) &amp;gt;&amp;gt;&amp;gt; b = a &amp;gt;&amp;gt;&amp;gt; c = copy.deepcopy(a) &amp;gt;&amp;gt;&amp;gt; a += 1 &amp;gt;&amp;gt;&amp;gt; a tensor([[2., 2.], [2., 2.]]) &amp;gt;&amp;gt;&amp;gt; b tensor([[2., 2.], [2., 2.]]) &amp;gt;&amp;gt;&amp;gt; c tensor([[1., 1.], [1., 1.]]) However, this is only possible for tensors that are explicitly defined by the user. For instance, attempting to deep-copy</description></item><item><title>Three Ways to Pause a Program in Python</title><link>https://freshrimpsushi.github.io/en/posts/2145/</link><pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2145/</guid><description>Code 1 input() input() No need to import a module, and as it does not display any message, it is the most convenient and frequently used method. The program pauses while waiting for input, but there&amp;rsquo;s no issue with its pause functionality as the input does not need to be stored. time.sleep() import time time_duration = 3.5 time.sleep(time_duration) Pauses for the specified amount of time. It is used when writing</description></item><item><title>Uniqueness Theorem of Geodesics</title><link>https://freshrimpsushi.github.io/en/posts/3176/</link><pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3176/</guid><description>Theorem1 Let&amp;rsquo;s say point $p$ is on the surface $M$ and let $\mathbf{X} \in T_{p}M$ be the unit tangent vector at point $p$. Then, there exists a unique geodesic $\boldsymbol{\gamma} : (-\epsilon, \epsilon) \to M$ that satisfies the following initial value condition. $$ \boldsymbol{\gamma} (0) = p \quad \text{and} \quad \boldsymbol{\gamma}^{\prime}(0) = \mathbf{X} $$ Description This theorem states that, at least locally, there exists a straight line of shortest distance</description></item><item><title>Ito-Taylor Expansion Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2144/</link><pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2144/</guid><description>Theorem 1 $$ d X(t) = f \left( X_{t} \right) dt + g \left( X_{t} \right) d W_{t} \qquad , t \in [0, T] $$ Let&amp;rsquo;s assume the Ito Process is given as the solution to the above Autonomous Stochastic Differential Equation. If $f,g : \mathbb{R} \to \mathbb{R}$ satisfies the Linear Growth Condition, i.e., for some constant $K$, $\begin{cases} \left| f \left( X_{t} \right) \right| \le K \left( 1 +</description></item><item><title>Differences among \mathrm, \text, and \operatorname in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3175/</link><pubDate>Sun, 16 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3175/</guid><description>\mathrm, \text code1 $$ \begin{array}{cc} \mathrm{sin}(\mathrm{mathrm}) &amp;amp; \text{sin}(\text{text}) \end{array} \\[1em] \text{basic font: abcdefghijklmnopqrstuvwxyz} $$ When you look at the default setting, there seems to be no difference in rendering between \mathrm and \text. That is because the default font itself is exactly the same as \mathrm. \mathrm is precisely rendered in Roman type, while \text follows the font that has been set. Therefore, changing the font would result in the</description></item><item><title>How to Open a Dialog Box and Select a File Like file.choose() in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2143/</link><pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2143/</guid><description>Code 1 using Gtk file_name = open_dialog(&amp;#34;파일 열기&amp;#34;) The string given as the first argument is the title of the dialog. When executed, you can see a &amp;lsquo;Open File&amp;rsquo; dialog popping up like this. Environment OS: Windows julia: v1.6.0 https://discourse.julialang.org/t/choose-a-file-interactively/10910/3&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Definition of Parallel Vector Field along a Curve on Surface</title><link>https://freshrimpsushi.github.io/en/posts/3174/</link><pubDate>Fri, 14 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3174/</guid><description>Vector Field Along a Curve1 Definition Given a surface $M$ and a curve $\alpha : \left[ a, b \right] \to M$, let us consider a function $\mathbf{X}$ that maps each $t \in \left[ a,b \right]$ to a tangent vector at point $\alpha (t)$ on surface $M$. This function $\mathbf{X}$ is called a vector field along curve $\alpha$. $$ \mathbf{X} : \left[ a, b \right] \to \mathbb{R}^{3} \\ \mathbf{X}(t) \in T_{\alpha</description></item><item><title>Strong and Weak Convergence of Numerical Solutions to SDEs</title><link>https://freshrimpsushi.github.io/en/posts/2142/</link><pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2142/</guid><description>Buildup $$ d X_{t} = f \left( t, X_{t} \right) dt + g \left( t , X_{t} \right) d W_{t} \qquad , t \in \left[ t_{0} , T \right] $$ Given a Stochastic Differential Equation as above, let&amp;rsquo;s assume that the time is discretized as $t_{0} &amp;lt; t_{1} &amp;lt; \cdots &amp;lt; t_{N}$. Choosing a sufficiently large $N \in \mathbb{N}$ and setting $\Delta = \left( T - t_{0} \right) / N</description></item><item><title>How to Define Artificial Neural Network Layers with Lists and Loops in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3173/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3173/</guid><description>Explanation If there are many layers to stack or if there is a need to frequently change the structure of the neural network, one might want to automate the definition of the artificial neural network. In such cases, you might think of defining it using the following for loop. class Model(nn.Module): def __init__(self): super(Model, self).__init__() fc_ = [nn.Linear(n,n) for i in range(m)] def forward(self, x): for i in range(m): x</description></item><item><title>Definition of Even Numbers</title><link>https://freshrimpsushi.github.io/en/posts/2141/</link><pubDate>Tue, 11 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2141/</guid><description>Definition Simple Definition A number is said to be even if it has a remainder of $0$ when divided by $2$. Complex Definition $$ a = 2 \cdot k $$ For any integer $a$, if there exists an integer $k$ that satisfies the above, then $a$ is considered Even. Integers that are not even are called Odd. Explanation There&amp;rsquo;s a question that reignites internet forums every cooldown period. &amp;ldquo;Is $0$</description></item><item><title>Geodesic on a Surface of Revolution</title><link>https://freshrimpsushi.github.io/en/posts/3172/</link><pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3172/</guid><description>Theorem1 Let us assume that $M$ is a surface of revolution generated by the unit-speed curve $\alpha (t) = (r(t), z(t))$. Then (a) All meridians are geodesics. (b) The condition for parallels to be geodesics is that $\mathbf{x}_{t}$ is parallel to the axis of rotation at all points on the parallel. $$ \text{The circle of latitude is a geodesic.} \\ \iff \mathbf{x}_{t} \text{ is parallel to the axis of revolution</description></item><item><title>CKLS Mean Reverting Gamma Stochastic Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/2140/</link><pubDate>Sun, 09 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2140/</guid><description>Model 1 $$ d X_{t} = \left( \alpha - \beta X_{t} \right) dt + \sigma X_{t}^{\gamma} d W_{t} \qquad , X_{0} &amp;gt; 0 $$ Let&amp;rsquo;s assume $\alpha, \beta, \sigma, \gamma &amp;gt; 0$. This stochastic differential equation is called the CKLS Mean Reverting Gamma Stochastic Differential Equation. Variables $X_{t}$: Represents the Interest Rate or the Gene Frequency. Parameters $\alpha / \beta$: The Mean Reversion, towards which $X_{t}$ tends to revert over</description></item><item><title>Emersion and Embedding on a Differential Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3171/</link><pubDate>Sat, 08 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3171/</guid><description>Definition1 Let us consider $M^{m}, N^{m}$ as an $m, n$-dimensional differentiable manifold, and $\phi : M \to N$ as a differentiable function. If the derivative $d\phi_{p}$ is a one-to-one function at every point $p \in M$, then $\phi$ is called an immersion. If $\phi$ is both an immersion and a homeomorphic, then $\phi$ is called an embedding. If the inclusion function $i : M \subset N$ is an embedding, then</description></item><item><title>Quotient and Remainder</title><link>https://freshrimpsushi.github.io/en/posts/2139/</link><pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2139/</guid><description>Definition 1 Given two integers $A$ and $B \ne 0$, suppose there exist integers $Q$ and $R$ that satisfy $$ A = Q \cdot B + R $$ . In this case, $Q$ is called the Quotient, and $R$ is called the Remainder. Explanation I&amp;rsquo;m not sure how quotients and remainders are defined in elementary schools these days, but there might be differences from the strict levels of discrete mathematics</description></item><item><title>Rotational Surfaces in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3170/</link><pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3170/</guid><description>Definition1 Let $z$ be the variable on the given axis, and $r&amp;gt;0$ be the distance from the $z-$ axis. Then, one can consider the curve $\alpha$ on the $rz-$ plane as shown in the figure below. As shown in the figure below, the surface obtained by rotating the curve $\alpha$ about the $z-$ axis is called a surface of revolution. The surface of revolution is expressed as follows. $$ \mathbf{x}(t,</description></item><item><title>Cox-Ingersoll-Ross Model, CIR Model</title><link>https://freshrimpsushi.github.io/en/posts/2138/</link><pubDate>Wed, 05 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2138/</guid><description>Model 1 $$ d X_{t} = \left( \alpha - \beta X_{t} \right) dt + \sigma \sqrt{X_{t}} d W_{t} \qquad , X_{0} &amp;gt; 0 $$ Assume $\alpha, \beta, \sigma &amp;gt; 0$ satisfies $2 \alpha &amp;gt; \sigma^{2}$. The stochastic differential equation mentioned above is called the CIR model. $$ X_{t} = {{ \alpha } \over { \beta }} + e^{-\beta t} \left( X_{0} - {{ \alpha } \over { \beta }} \right)</description></item><item><title>Embedding in Topology</title><link>https://freshrimpsushi.github.io/en/posts/3169/</link><pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3169/</guid><description>Definition1 Let $X, Y$ be a topological space. If $f : X \to Y$ satisfies the following, $f$ is called an embedding and denoted as $f : X \hookrightarrow Y$. $X$ is a subspace of $Y$. $f : X \to f(X)$ is a homeomorphism. See also Embedding in Functional Analysis Embedding in Differential Manifolds Daehie Park &amp;amp; Seungho Ahn, Topology (4th Edition, 2018), p232&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Greatest Common Divisor and Coprime</title><link>https://freshrimpsushi.github.io/en/posts/2137/</link><pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2137/</guid><description>Definition 1 For two integers $n$ and $m \ne 0$, if there exists an integer $k$ that satisfies the following, $n$ is divisible by $m$. $$ n = mk $$ In this case, $n$ is called a Multiple of $m$, and $m$ is called a Divisor of $n$, as indicated below. $$ m \mid n $$ If $m$ cannot divide $n$, it is denoted by striking through as $m \nmid</description></item><item><title>Definition of a Straight Line (Geodesic) in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3168/</link><pubDate>Sun, 02 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3168/</guid><description>Buildup1 2 Let&amp;rsquo;s assume there is an object moving along a certain curve on the surface $M \subset \mathbb{R}^{3}$. Even if the line looks curved from the perspective of the entire space $\mathbb{R}^{3}$, an object moving on the surface can be thought of as moving straight ahead. Then, such a line can be defined as a straight line (geodesic) on the surface. First, let&amp;rsquo;s consider the properties of a straight</description></item><item><title>Ornstein-Uhlenbeck Equation</title><link>https://freshrimpsushi.github.io/en/posts/2136/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2136/</guid><description>Definition 1 $$ d X_{t} = a X_{t} dt + \sigma d W_{t} $$ Let $a , \sigma \in \mathbb{R}$. The stochastic differential equation given above is called the Ornstein-Uhlenbeck Equation, and its solution, the stochastic process $X_{t}$, is called the Ornstein-Uhlenbeck Process. $$ X_{t} = X_{0} e^{a t} + \sigma \int_{0}^{t} e^{a (t-s)} d W_{s} $$ Description 2 The Ornstein-Uhlenbeck equation is also known as the Langevin Equation. If</description></item><item><title>Identity Function</title><link>https://freshrimpsushi.github.io/en/posts/3167/</link><pubDate>Fri, 31 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3167/</guid><description>Definition1 Given a set $X$, the following function $I_{X} : X \to X$ is called the identity function. $$ I_{X}(x) = x,\quad \forall x \in X $$ Explanation The following notations are commonly used. $$ I,\quad \text{id},\quad \text{1} $$ Tangent vectors on a differentiable manifold are defined as follows in $\dfrac{d (f\circ \alpha)}{d t}$, where the function to be differentiated $$ f \circ \alpha = f \circ I \circ \alpha</description></item><item><title>The Equivalence between Compact Metric Spaces and Complete, Totally Bounded Spaces</title><link>https://freshrimpsushi.github.io/en/posts/2135/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2135/</guid><description>Theorem 1 A metric space is compact if and only if it is complete and totally bounded. Proof $(\Rightarrow)$ Suppose the metric space $X$ is compact. Properties of complete metric spaces: If $(X,d)$ is a metric space and $K \subset X$, [1]: $K$ is a complete subspace. $\iff$ $X$ where $K$ is a closed set. [2]: $K$ is a totally bounded space $\iff$ $X$ where the closed set $K$ is</description></item><item><title>Geodesic Curvature is Intrinsic</title><link>https://freshrimpsushi.github.io/en/posts/3166/</link><pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3166/</guid><description>Theorem1 The geodesic curvature $\kappa_{g}$ of a curve on a surface is intrinsic. Description In other words, $\kappa_{g}$ can be calculated solely using the coefficients of the Riemannian metric, without the unit normal $\mathbf{n}$. Of course, it can also be expressed using the extrinsic formula, as $\kappa \mathbf{N} = \mathbf{T}^{\prime} = \alpha^{\prime \prime} = \kappa_{n}\mathbf{n}+ \kappa_{g}\mathbf{S}$, so $$ \begin{align*} \kappa_{g} =&amp;amp;\ \left\langle \mathbf{T}^{\prime}, \mathbf{S} \right\rangle \\ =&amp;amp;\ \left\langle \mathbf{T}^{\prime}, \mathbf{n}</description></item><item><title>2021 Reader Major Survey Results</title><link>https://freshrimpsushi.github.io/en/posts/2351/</link><pubDate>Tue, 28 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2351/</guid><description>Reader Survey Results The Shrimp Sushi House aims to be the largest formal science blog in the country, and in 2022, we are preparing events such as small competitions in conjunction with the community. From December 29th, 2020, to November 28th, 2021, for about 11 months, we have asked our visitors for a brief survey on their majors. Let&amp;rsquo;s look at a brief summary together. A total of 799 readers</description></item><item><title>Bridge of Brown</title><link>https://freshrimpsushi.github.io/en/posts/2134/</link><pubDate>Tue, 28 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2134/</guid><description>Definition 1 2 $$ d Y_{t} = {{ b - Y_{t} } \over { 1 - t }} dt + d W_{t} \qquad, t \in [0,1), Y_{0} = a $$ Let&amp;rsquo;s denote it as $a, b \in \mathbb{R}$. The stochastic process $Y_{t}$, which is a solution of the $1$-dimensional stochastic differential equation, from ($a$ to $b$) is called a Brownian Bridge. $$ Y_{t} = a (1-t) + bt + (1-t)</description></item><item><title>The Tangent Space on an n-Dimensional Differentiable Manifold is an n-Dimensional Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3165/</link><pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3165/</guid><description>Overview Let $M$ be a $n$-dimensional differential manifold, and let $T_{p}M$ be the tangent space at point $p\in M$. The tangent space becomes a vector space, specifically, a $n$-dimensional vector space. The following set becomes the basis of the tangent space, which is very useful in the study of differential manifolds. $$ \mathcal{B} = \left\{ \left. \dfrac{\partial }{\partial x_{i}} \right|_{p} : 1 \le i \le n \right\} $$ Theorem 11</description></item><item><title>How to Round to a Specific Decimal Place in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2133/</link><pubDate>Sun, 26 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2133/</guid><description>Code In fact, Julia is not a very convenient language for things like string formatting. While there are methods that utilize the intrinsic capabilities of strings for printing to the console, often it&amp;rsquo;s more convenient to use the round() function&amp;rsquo;s default option, digits. julia&amp;gt; for k in 0:8 println(round(π, digits = k)) end 3.0 3.1</description></item><item><title>The Christoffel Symbols are Intrinsic</title><link>https://freshrimpsushi.github.io/en/posts/3164/</link><pubDate>Sat, 25 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3164/</guid><description>Theorem1 The Christoffel symbols $\Gamma_{ij}^{k}$ satisfy the following equation. In other words, they are intrinsic. $$ \Gamma_{ij}^{k} = \dfrac{1}{2} \sum \limits_{l=1}^{2} g^{lk} \left( \dfrac{\partial g_{lj}}{\partial u_{i}} - \dfrac{\partial g_{ij}}{\partial u_{l}} + \dfrac{\partial g_{il}}{\partial u_{j}} \right) $$ Explanation Gauss proved it. The Christoffel symbols depend only on the Riemann metric and are independent of the normal vector. Therefore, by using Christoffel symbols, one can understand the structure of a surface without</description></item><item><title>Solutions to Typical Stochastic Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/2132/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2132/</guid><description>Equations 1 (G) General Form: $$ d X_{t} = f \left( t , X_{t} \right) dt + g \left( t , X_{t} \right) d W_{t} $$ (L) Linear: $\begin{cases} f \left( t , X_{t} \right) = a_{t} + b_{t} X_{t} \\ g \left( t , X_{t} \right) = c_{t} + e_{t} X_{t} \end{cases}$ $$ d X_{t} = \left( a_{t} + b_{t} X_{t} \right) dt + \left( c_{t} + e_{t} X_{t}</description></item><item><title>Taylor's Theorem for Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/3163/</link><pubDate>Thu, 23 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3163/</guid><description>Theorem1 Let $f : \mathbb{R}^{n} \to \mathbb{R}$ be $C^{k}$ function, and call it $\mathbf{a} = (a_{1}, \dots, a_{n}) \in \mathbb{R}^{n}$. Then, there exists $C^{k-2}$ function $h_{ij}$ that satisfies the following. $$ f(\mathbf{x}) = f(\mathbf{a}) + \sum_{i} (x_{i} - a_{i})\dfrac{\partial f}{\partial x_{i}}(\mathbf{a}) + \sum_{i,j}h_{ij}(\mathbf{x})(x_{i} - a_{i}) (x_{j} - a_{j}) $$ Description It generalizes the Taylor theorem to functions of several variables. second-order $$ \begin{align*} f(\mathbf{x}) &amp;amp;= f(\mathbf{a}) + \sum\limits_{i=1}^{n} (x_{i} -</description></item><item><title>Proof of Gronwall's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2131/</link><pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2131/</guid><description>Theorem Let&amp;rsquo;s assume there are two continuous functions $f,w : I \to \mathbb{R}$ defined in an interval $I \subset \mathbb{R}$ that contains the minimum value $a \in \mathbb{R}$. If $w$ is $\forall t \in I$ in $w(t) \ge 0$ and for some constant $C \in \mathbb{R}$, $$ f(t) \le C + \int_{a}^{t} w(s) f(s) ds \qquad , \forall t \in I $$ then the following holds. $$ f(t) \le C</description></item><item><title>Definition of Intrinsic in Differntial Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3162/</link><pubDate>Tue, 21 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3162/</guid><description>Definition1 In differential geometry, a function that depends only on the coefficients of the first fundamental form $g_{ij}$, and not on the unit normal $\mathbf{n}$, is called intrinsic. Explanation2 3 If the coefficients of the Riemann metric $g_{ij}$ are known, then the length of curves on the surface and the area of the surface can be calculated without leaving the surface as follows can be calculated. $$ \text{length of }</description></item><item><title>Linear, Homogeneous, Autonomous Stochastic Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/2130/</link><pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2130/</guid><description>Definition 1 Let&amp;rsquo;s assume we have a probability space $( \Omega , \mathcal{F} , P)$ and a filtration $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$. Consider the following $n$-dimensional stochastic differential equation with respect to two functions $f$, $g$, and $\mathcal{F}_{t}$-adapted $m$-dimensional Wiener process $W_{t}$: $$ \begin{align*} d X_{t} =&amp;amp; f \left( t, X_{t} \right) dt + g \left( t, X_{t} \right) d W_{t} \\ f =&amp;amp; a(t) + A(t) X_{t} \\</description></item><item><title>Silver-Müller Radiation Condition</title><link>https://freshrimpsushi.github.io/en/posts/3161/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3161/</guid><description>Definition1 Let&amp;rsquo;s define $E$ as the electric field. The following condition is called the Silver-Mueller radiation condition. When $x\in \mathbb{R}^{3}, r = \left| x \right|$, $$ \lim \limits_{r \to \infty} r \left[ (\nabla \times E) \times x - r E \right] = 0 $$ Explanation The Silver-Mueller radiation condition is a necessary condition for the scattering problem of electromagnetic waves for electromagnetic waves to satisfy. Explanation The Sommerfeld radiation condition,</description></item><item><title>Frobenius Norm</title><link>https://freshrimpsushi.github.io/en/posts/2129/</link><pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2129/</guid><description>Definition 1 For a matrix $A = \left( a_{ij} \right) \in \mathbb{C}^{m \times n}$, the matrix norm $\left\| \cdot \right\|_{F}$ is defined as follows and is called the Frobenius norm. $$ \left\| A \right\|_{F} = \sqrt{ \sum_{ij} \left| a_{ij} \right|^{2} } = \sqrt{ \text{Tr} \left( A^{\ast} A \right) } $$ Explanation The Frobenius norm is also called the Hilbert–Schm</description></item><item><title>Gauss's Theorem in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3160/</link><pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3160/</guid><description>정리1 Let&amp;rsquo;s call $\mathbf{x} : U \to \R^{3}$ the coordinate patch. Let $(u_{1}, u_{2})$ be the coordinates of $U$. Let $\mathbf{n}$ be the unit normal, $L_{ij} = \left\langle \mathbf{x}_{ij}, \mathbf{n} \right\rangle$ the coefficients of the second fundamental form, and $\Gamma_{ij}^{k} = \sum \limits_{l=1}^{2} \left\langle \mathbf{x}_{ij}, \mathbf{x}_{l} \right\rangle g^{lk} = \left\langle \mathbf{x}_{ij}, \mathbf{x}_{l} \right\rangle g^{lk}$ the Christoffel symbols. Then, the following are true: (a) Gauss&amp;rsquo;s formulas: $$ \mathbf{x}_{ij} =</description></item><item><title>Existence and Uniqueness of Solutions to Stochastic Differential Equations, Strong and Weak Solutions</title><link>https://freshrimpsushi.github.io/en/posts/2128/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2128/</guid><description>Definition 1 Let us have a probability space $( \Omega , \mathcal{F} , P)$ and a filtration $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$. $$ \begin{align*} f &amp;amp;: [0,T] \times \mathbb{R}^{n} \to \mathbb{R}^{n} \\ g &amp;amp;: [0,T] \times \mathbb{R}^{n} \to \mathbb{R}^{n \times m} \end{align*} $$ Consider the following $n$-dimensional stochastic differential equation for two functions $f$, $g$ and $\mathcal{F}_{t}$-adapted $m$-dimensional Wiener process $W_{t}$: $$ d X_{t} = f \left( t, X_{t} \right)</description></item><item><title>Paper Review: Neural Ordinary Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/3159/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3159/</guid><description>Overview and Summary &amp;ldquo;Neural Ordinary Differential Equations&amp;rdquo; is a paper published in 2018 by Ricky T. Q. Chen and three others, and it was selected for 2018 NeurIPS Best Papers. It proposes a method to approximate a simple first-order differential equation, which is a non-autonomous system, using neural networks. $$ \dfrac{\mathrm{d}y}{\mathrm{d}t} = f(y, t) $$ Notably, what the neural network approximates (predicts) is not the $y$ but the rate of</description></item><item><title>Introduction to D-Data Hub</title><link>https://freshrimpsushi.github.io/en/posts/2127/</link><pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2127/</guid><description>Introduction D-Data Hub is a public data portal centered around Daegu Metropolitan City, offering more than 4,000 datasets and over 13,000 services. True to its regional data focus, it provides detailed and diverse information broken down by district/county. Requirements There are no specific requirements, allowing for unlimited downloads. Data Examples Daegu Metropolitan City_Covid-19 Confirmed Cases by Basic Local Government Unit_20210309.csv Daegu Metropolitan City_Nam District_Administrative Office Situation_20201023.csv Free Meal Distribution.shp Categories</description></item><item><title>Christoffel Symbols in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3158/</link><pubDate>Mon, 13 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3158/</guid><description>Buildup Let $\mathbf{x} : U \to \mathbb{R}^{3}$ represent a coordinate mapping. In differential geometry, the characteristics and properties of geometric objects are described through differentiation. Therefore, the derivatives of the coordinate fragments $\mathbf{x}$ appear in various theorems and formulas. For instance, the first-order derivatives $\left\{ \mathbf{x}_{1}, \mathbf{x}_{2} \right\}$ become the basis of the tangent space $T_{p}M$. Hence, any tangent vector $\mathbf{X} \in T_{p}M$ can be expressed as follows: $$ \mathbf{X}</description></item><item><title>How to Specify Heatmap Color Ranges in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2126/</link><pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2126/</guid><description>Code 1 When drawing a heatmap, sometimes it&amp;rsquo;s essential to fix the scale of values, so they don&amp;rsquo;t adjust with the numerical values. You can fix the color range using the clim option in the basic heatmap function. using Plots cd(@__DIR__) heatmap(rand(4,4)); png(&amp;#34;1.png&amp;#34;) heatmap(rand(4,4), clim = (0,1)); png(&amp;#34;2.png&amp;#34;) The results are as follows. The first heatmap has no fixed range, but the second heatmap&amp;rsquo;s range is fixed between 0 and</description></item><item><title>What is a Stochastic Differential Equation?</title><link>https://freshrimpsushi.github.io/en/posts/2125/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2125/</guid><description>Definition 1 $$ d X(t) = f \left( t, X(t) \right) dt + g \left( t, X(t) \right) d W_{t} \qquad , t \in \left[ t_{0} , T \right], T &amp;gt; 0 $$ Equations of the form above are called Stochastic Differential Equations, abbreviated as SDEs. Here, $f$ and $g$ are called the drift and diffusion coefficient functions, respectively. For the initial condition $X_{0} := X \left( t_{0} \right)$, the</description></item><item><title>Second Fundamental Form in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3156/</link><pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3156/</guid><description>Build-up Let $\mathbf{x} : U \to \mathbb{R}^{3}$ be referred to as a chart. In differential geometry, the characteristics and properties of geometric objects are explained through differentiation. Hence, the derivatives of coordinate charts $\mathbf{x}$ appear in various theorems and formulas. For instance, the first-order derivatives $\left\{ \mathbf{x}_{1}, \mathbf{x}_{2} \right\}$ become the basis of the tangent space $T_{p}M$. Therefore, any tangent vector $\mathbf{X} \in T_{p}M$ can be expressed as follows. $$</description></item><item><title>How to Use zfill() in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2124/</link><pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2124/</guid><description>Overview 1 In Python, zfill() actually serves as a method of the string class, filling the left side with zeros. Julia, on the other hand, offers the lpad() as a more versatile and widely applicable built-in function. While zfill() means to fill with zeros, lpad() signifies padding to the left. Code julia&amp;gt; lpad(&amp;#34;12&amp;#34;, 4, &amp;#34;0&amp;#34;) &amp;#34;0012&amp;#34; julia&amp;gt; lpad(12, 4, &amp;#34;0&amp;#34;) &amp;#34;0012&amp;#34; Continuing from the overview, lpad() in Julia is more</description></item><item><title>Robin Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/3155/</link><pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3155/</guid><description>Definition1 Let&amp;rsquo;s assume that a partial differential equation is defined in an open set $\Omega$. The following boundary conditions are called Robin boundary conditions. $$ u + \dfrac{\partial u}{\partial \nu} = 0 \quad \text{on }\partial \Omega $$ Here, $\nu$ represents the outward unit normal vector. Description Example For instance, solving the Poisson&amp;rsquo;s equation with given Robin boundary conditions is to find $u$ that satisfies the following. $$ \left\{ \begin{align*} -\Delta</description></item><item><title>Ito Formula and Martingale Representation Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2123/</link><pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2123/</guid><description>Theorem 1 2 Given a probability space $( \Omega , \mathcal{F} , P)$ and a filtration $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$, let&amp;rsquo;s say that a Wiener process $\left\{ W_{t} \right\}_{t \ge 0}$ is $\mathcal{F}_{t}$-adapted. Itô&amp;rsquo;s Lemma If $f \in \mathcal{L}^{2} (P)$, then there exists a unique stochastic process $X (t,\omega) \in m^{2}(0,T)$ satisfying: $$ f (\omega) = E (f) +</description></item><item><title>Gauss Curvature and Geodesic Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3154/</link><pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3154/</guid><description>Buildup1 $$ \left\{ T(s), N(s), B(s), \kappa (s), \tau (s) \right\} $$ Recall how we used the Frenet-Serret apparatus when analyzing curves. When studying surfaces, we will consider similar concepts. When $\boldsymbol{\alpha}$ is the unit speed curve, the curvature of the curve was defined as the magnitude of acceleration $\kappa = \left| T^{\prime} \right| = \left| \boldsymbol{\alpha}^{\prime \prime} \right|$. It is natural to think about how curved a surface is</description></item><item><title>Distribution of Degrees in Networks</title><link>https://freshrimpsushi.github.io/en/posts/2122/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2122/</guid><description>Buildup Random networks are random elements whose function values are networks, therefore, every time we sample, we get different networks. Depending on the method of constructing the network, or in other words, the model, we may have some consistent properties, but the realizations obtained are each unique. According to this, the degree of each node $\deg$ also changes with every sampling, and since the degree is an extremely important element</description></item><item><title>Neumann Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/3153/</link><pubDate>Fri, 03 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3153/</guid><description>Definition1 Let&amp;rsquo;s assume a partial differential equation is given, defined on an open set $\Omega$. The following boundary condition is called the Neumann boundary condition. The problem of finding the solution to the partial differential equation with the Neumann boundary condition is referred to as the Neumann problem. $$ \dfrac{\partial u}{\partial \nu} = 0 \quad \text{on } \partial \Omega $$ Here, $\nu$ represents the outward unit normal vector. Description Nonhomogeneous</description></item><item><title>Ito's Formula</title><link>https://freshrimpsushi.github.io/en/posts/2121/</link><pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2121/</guid><description>Theorem 1 Given an Itô process $\left\{ X_{t} \right\}_{t \ge 0}$, $$ d X_{t} = u dt + v d W_{t} $$ for a function $V \left( t, X_{t} \right) = V \in C^{2} \left( [0,\infty) \times \mathbb{R} \right)$, let $Y_{t} := V \left( t, X_{t} \right)$, then $\left\{ Y_{t} \right\}$ is also an Itô process and the following holds: $$ \begin{align*} d Y_{t} =&amp;amp; V_{t}</description></item><item><title>Parametric Curves on a Simple Surface</title><link>https://freshrimpsushi.github.io/en/posts/3152/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3152/</guid><description>Definition1 2 Let $\mathbf{x} : U \to \R^{3}$ be called a simple surface. Let the coordinates of $U$ be called $(u, v)$. For any point $(u_{0}, v_{0})$, the following curve is called the $u-$parameter curve at $v = v_{0}$ of $\mathbf{x}$. $$ u \mapsto \mathbf{x}(u, v_{0}) $$ The following curve is called the $v-$parameter curve at $u = u_{0}$ of $\mathbf{x}$. $$ v \mapsto \mathbf{x}(u_{0}, v) $$ The velocity vectors</description></item><item><title>Checking Struct Properties in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2120/</link><pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2120/</guid><description>Code propertynames() You can check with the propertynames() function1. Since Julia has no classes, only structs2, all symbols returned by this function are precisely the names of properties only. The following is code for creating an Erdős–Rényi network in the Graphs package, checking the number of nodes, and each node&amp;rsquo;s neighborhood. The propertynames() function was used on this network, and</description></item><item><title>Boundary Value Problems in Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/3151/</link><pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3151/</guid><description>Definition Given a partial differential equation defined in an open set $\Omega$, let&amp;rsquo;s assume the values of the unknown $u$ are given on the boundary $\partial \Omega$ of $\Omega$. This is called a boundary condition. The partial differential equation together with the boundary condition is referred to as a boundary value problem. Description The abbreviation BVP is commonly used. To solve a boundary value problem means to find a solution</description></item><item><title>Ito Process</title><link>https://freshrimpsushi.github.io/en/posts/2119/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2119/</guid><description>Definition 1 Given a probability space $( \Omega , \mathcal{F} , P)$ and a filtration $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$, suppose that a Wiener process $\left\{ W_{t} \right\}_{t \ge 0}$ is $\mathcal{F}_{t}$-adapted, and for $f \in \mathcal{L}^{1} [0 , \infty)$ and $g \in \mathcal{L}^{2} [0 , \infty)$, we define a $1$-dimensional continuous $\mathcal{F}_{t}$-adapted stochastic process $\left\{ X_{t} \right\}_{t \ge 0}$ as a $1$-dimensional Itô Process. $$ X (t)</description></item><item><title>Specific Examples of Calculations Using the Riemann Metric</title><link>https://freshrimpsushi.github.io/en/posts/3150/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3150/</guid><description>Notation Let&amp;rsquo;s say we have coordinates for $(u,v)$ as $U$ on a simple surface $\mathbf{x} : U \to \mathbb{R}^{3}$. $$ \mathbf{x}_{1} := \dfrac{\partial \mathbf{x}}{\partial u}\quad \text{and} \quad \mathbf{x}_{2} := \dfrac{\partial \mathbf{x}}{\partial v} $$ Let&amp;rsquo;s represent the coefficients of the Riemannian metric as follows. $$ \begin{align*} g_{ij} =&amp;amp;\ \left\langle \mathbf{x}_{i}, \mathbf{x}_{j} \right\rangle \\ g_{11} =&amp;amp;\ \left\langle \mathbf{x}_{1}, \mathbf{x}_{1} \right\rangle = E \\ g_{12} =&amp;amp;\ g_{21} = \left\langle \mathbf{x}_{1}, \mathbf{x}_{2} \right\rangle =</description></item><item><title>Gilbert Model</title><link>https://freshrimpsushi.github.io/en/posts/2118/</link><pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2118/</guid><description>Definition 1 2 Simple Definition A random network where links of a simple network are connected independently according to probability $p \in [0,1]$ is called the Gilbert Model $\mathbb{G}_{n,p}$. Complicated Definition Given a probability space $( \Omega , \mathcal{F} , P)$, and a network&amp;rsquo;s properties $2^{\binom{n}{2}} \subseteq 2^{\binom{n}{2}}$ with $n$ labeled nodes. A function that is measurable with respect to $\mathcal{F}$ and has the following probability mass function for the</description></item><item><title>Potential, A General Definition of Potential Energy</title><link>https://freshrimpsushi.github.io/en/posts/3149/</link><pubDate>Thu, 25 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3149/</guid><description>Definition1 Scalar Potential Let&amp;rsquo;s assume that the vector field $\mathbf{V}$ is a conservative field. In other words, let&amp;rsquo;s say $\nabla \times \mathbf{V} = \mathbf{0}$. Then, there exists a scalar field $W$ that satisfies $\mathbf{V} = -\nabla W$, and this is called the scalar potential of $\mathbf{V}$. Vector Potential Assume the vector field $\mathbf{V}$ satisfies $\nabla \cdot \mathbf{V} = 0$. Then, there exists a vector field $\mathbf{A}$ that satisfies $\mathbf{V} =</description></item><item><title>Integration by Parts</title><link>https://freshrimpsushi.github.io/en/posts/2117/</link><pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2117/</guid><description>Theorem 1 If the bounded continuous function $f(s,\omega) = f(s)$ in $[0,t]$ depends only on $s$, then $$ \int_{0}^{t} f(s) d W_{s} = f (t) W_{t} - \int_{0}^{t} W_{s} d f (s) $$ $W_{t}$ is a Wiener process. Description It&amp;rsquo;s a theorem about Itô integration, not much different from the integration by parts we commonly know. It’s important to note that the integrand has changed.</description></item><item><title>First Basic Forms, Riemannian Metrics</title><link>https://freshrimpsushi.github.io/en/posts/3148/</link><pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3148/</guid><description>Buildup Riemannian metric is a concept that comes from the process of calculating the length of curves on a surface, and the process is as follows. Let&amp;rsquo;s say $\boldsymbol{\alpha}(t)$ is a regular curve moving on a simple surface $\mathbf{x} : U \to \mathbb{R}^{3}$. Let&amp;rsquo;s say $(u_{1}, u_{2})$ are the coordinates in $U$. Then, $\boldsymbol{\alpha}$ can be expressed as follows. $$ \boldsymbol{\alpha}(t) = \mathbf{x}(u_{1}(t), u_{2}(t)) $$ At this point, the length</description></item><item><title>Erdős–Rényi Model</title><link>https://freshrimpsushi.github.io/en/posts/2116/</link><pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2116/</guid><description>Buildup Let us consider a simple graph with $n$ labeled vertices and $m$ edges, which has the property $\mathscr{G}_{n,m} \subset 2^{\binom{n}{2}}$. A random graph with exactly $m$ links can be represented as follows $\mathbb{G}_{n, m} : \Omega \to \mathscr{G}_{n,m}$. The graph produced in this manner does not care who makes it or what probability it is made with, as long as it only has $n$ nodes and $m$ links. In</description></item><item><title>Sign function</title><link>https://freshrimpsushi.github.io/en/posts/3147/</link><pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3147/</guid><description>Definition Sign function $\mathrm{sgn} : \mathbb{R} \to \mathbb{R}$ is defined as follows. $$ \mathrm{sgn}(x) :=\begin{cases} 1 &amp;amp; x&amp;gt;0 \\ 0 &amp;amp; x=0 \\ -1 &amp;amp; x&amp;lt;0 \end{cases} $$ Explanation It is mainly used to simplify the notation of equations or definitions. It is also written as $\mathrm{sign}$. See Also Sign of complex numbers</description></item><item><title>Ito Multiplication Table</title><link>https://freshrimpsushi.github.io/en/posts/2115/</link><pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2115/</guid><description>Build-up $s&amp;lt; t &amp;lt; t+u$ Suppose, meet the following conditions for a stochastic process $\left\{ W_{t} \right\}$ to be called a Wiener process. (i): $W_{0} = 0$ (ii): $\left( W_{t+u} - W_{t} \right) \perp W_{s}$ (iii): $\left( W_{t+u} - W_{t} \right) \sim N ( 0, u )$ (iv): Sample paths of $W_{t}$ are almost surely continuous. The Wiener process has the following properties: [1]: $\displaystyle W_{t} \sim N ( 0</description></item><item><title>Definition of Surfaces in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3146/</link><pubDate>Fri, 19 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3146/</guid><description>Definition1 If for every point $M \subset \R^{3}$ of $P \in M$, there exists an $C^{k}$ diffeomorphism $\mathbf{x} : U \subset \R^{2} \to M$ such that the image $\mathbf{x}(U)$ contains some $\epsilon-$neighborhood $N_{p}$ of $P$, then $M$ is called a $\R^{3}$ surface. Moreover, for such two diffeomorphisms $\mathbf{x} : U \to \R^{3}$ and $\mathbf{y} : V \to \R^{3}$, $$ \mathbf{y}^{-1} \circ \mathbf{x} : \mathbf{x}^{-1}\left( \mathbf{x}(U) \cap \mathbf{y}(V) \right) \to \mathbf{y}^{-1}\left(</description></item><item><title>Random Graphs</title><link>https://freshrimpsushi.github.io/en/posts/2114/</link><pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2114/</guid><description>Definitions Simple Definition A graph that is created by a nondeterministic procedure or expressed according to some probability distribution is called a Random Graph. Complex Definition Given a probability space $( \Omega , \mathcal{F} , P)$, let $2^{\binom{n}{2}}$ represent the collection of all labeled graphs with $n$ vertices, known as a graph family. A function $\mathbb{G} : \Omega \to 2^{\binom{n}{2}}$, which is $\mathcal{F}$-measurable, is called a Random Graph. In other</description></item><item><title>How to Read and Write Greek Characters and Their Meaning in Mathematics and Science</title><link>https://freshrimpsushi.github.io/en/posts/3145/</link><pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3145/</guid><description>Alpha $\Alpha, \alpha$ Alpha is read as &amp;ldquo;alpha&amp;rdquo;. The TeX codes are \Alpha, \alpha respectively. It is the first letter of the Greek alphabet, and the phrase &amp;ldquo;alpha and omega&amp;rdquo; means &amp;ldquo;the beginning and the end.&amp;rdquo; Index of an index set $\alpha$ In differential geometry, a curve $\alpha$ Curve used to define tangent vectors on a differential manifold $\alpha$ Beta $\Beta, \beta$ Beta is read as &amp;ldquo;beta&amp;rdquo;. The TeX codes</description></item><item><title>Isometric Equality of Ito</title><link>https://freshrimpsushi.github.io/en/posts/2113/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2113/</guid><description>Theorem 1 For all $f \in m^{2}[a,b]$, the following equation holds. $$ E \left[ \left( \int_{a}^{b} f d W_{t} \right)^{2} \right] = E \left[ \int_{a}^{b} f^{2} dt \right] $$ Explanation While it is correct that the power outside of the integral sign $^{2}$ crosses over, attention should also be paid to the change in the integrands $d W_{t}$ and $dt$. Proof 1 Strategy: Since it suffices to show for sequences</description></item><item><title>Eigen Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/3144/</link><pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3144/</guid><description>Definition1 Let us assume that $M \subset \mathbb{R}^{3}$ and $\epsilon &amp;gt;0$ are given. Let&amp;rsquo;s call $d$ the Euclidean distance. The set defined as follows is called the $\epsilon -$ neighborhood of point $P \in M$. $$ N_{p} := \left\{ Q \in M : d(P,Q) &amp;lt; \epsilon \right\} $$ Let&amp;rsquo;s say $M \subset \mathbb{R}^{3}$. Given a function $g : M \to \R^{2}$. For all open sets $U \subset \R^{2}$ containing $g(P)$,</description></item><item><title>Graph Families and Properties</title><link>https://freshrimpsushi.github.io/en/posts/2112/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2112/</guid><description>Buildup Consider a simple graph with a set of $n$ labeled vertices $V = [n] = \left\{ 1 ,\cdots , n \right\}$. The graph can have edges equal to the number of ways to choose two distinct vertices, thus there are exactly $\binom{n}{2}$ such edges. Let&amp;rsquo;s think about this not just in terms of each edge, but the entire graph. With $n$ vertices fixed, each of the $\binom{n}{2}$ possible edges</description></item><item><title>Points to Note When Slicing in Python</title><link>https://freshrimpsushi.github.io/en/posts/3143/</link><pubDate>Sat, 13 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3143/</guid><description>Explanation When indexing strings, lists, etc. in Python, entering an index that exceeds the last index results in the following error. &amp;gt;&amp;gt;&amp;gt; list = [0, 1, 2, 3] &amp;gt;&amp;gt;&amp;gt; list[4] Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; IndexError: list index out of range &amp;gt;&amp;gt;&amp;gt; string = &amp;#39;abcde&amp;#39; &amp;gt;&amp;gt;&amp;gt; string[5] Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; IndexError: string index out of range</description></item><item><title>Ito Calculus</title><link>https://freshrimpsushi.github.io/en/posts/2111/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2111/</guid><description>Buildup Before discussing stochastic integrals, it is crucial to define an essential probabilistic process called the Elementary Process. Elementary processes play a similar role to simple functions, which were necessary for defining the Lebesgue integral in [Measure Theory](../../categories/Measure Theory). $$ a = t_{0} &amp;lt; t_{1} &amp;lt; \cdots &amp;lt; t_{k} = b $$ Considering such a partition in the Natural Domain $[a,b]$, an Elementary Process is defined as follows for indicator</description></item><item><title>Tangent Vectors on Simple Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3142/</link><pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3142/</guid><description>Definition1 Consider a point $p = \mathbf{x}(a,b)$ on a coordinate patch $\mathbf{x} : U \to \mathbb{R}^{3}$. If a vector $\mathbf{X}$ is the velocity vector at $p$ of some curve $\mathbf{x}(U)$ on the curve passing through $p$, then $\mathbf{X}$ is defined as the tangent vector to the simple surface $\mathbf{x}$. In other words, if for any arbitrary $\epsilon &amp;gt; 0$, there exists a suitably short curve $\boldsymbol{\alpha} : (-\epsilon, \epsilon) \to</description></item><item><title>Intersection of a Plane and a Normal Vector</title><link>https://freshrimpsushi.github.io/en/posts/2110/</link><pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2110/</guid><description>Definition 1 Let a subset of the Euclidean space $2$ have coordinates $U \subset \mathbb{R}^{2}$ and $u_{1}$, then the directional derivatives $\mathbf{x}_{1}$ and $\mathbf{x}_{2}$ can be referred to as follows on a simple surface $\mathbf{x} : U \to \mathbb{R}^{3}$. $$ \begin{align*} \mathbf{x}_{1} := {{ \partial \mathbf{x} } \over { \partial u_{1} }} &amp;amp; , &amp;amp; \mathbf{x}_{2} := {{ \partial \mathbf{x} } \over { \partial u_{2} }} \end{align*} $$ The plane</description></item><item><title>Differentiable Homomorphism</title><link>https://freshrimpsushi.github.io/en/posts/3141/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3141/</guid><description>Definition1 Let&amp;rsquo;s call $M_{1}, M_{2}$ a differential manifold. A function $\varphi : M_{1} \to M_{2}$ is called a diffeomorphism if it satisfies the following conditions: $\varphi$ is differentiable. $\varphi$ is a bijective function. $\varphi ^{-1}$ is differentiable. If for the neighborhoods $U$ and $V$ of points $p \in M_{1}$ and $\varphi(p)$, the contraction mapping $\varphi|_{U} : U \to V$ is a diffeomorphism, then $\varphi$ is called a local diffeomorphism. Theorem</description></item><item><title>m2 Space</title><link>https://freshrimpsushi.github.io/en/posts/2109/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2109/</guid><description>Definition 1 2 Given that there is a probability space $( \Omega , \mathcal{F} , P)$, A sequence of sub sigma fields $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$ of $\mathcal{F}$ is called a Filtration if it satisfies the following: $$ \forall s &amp;lt; t, \mathcal{F}_{s} \subset \mathcal{F}_{t} $$ A stochastic process $g(t,\omega) : [0,\infty) \times \Omega \to \mathbb{R}^{n}$ is said to be $\mathcal{F}_{t}$-Adapted if for all $t \ge 0$, $\omega \mapsto</description></item><item><title>Creating Random Permutations and Shuffling Tensor Order in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3140/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3140/</guid><description>torch.randperm()1 torch.randperm(n): Returns a random permutation of integers from 0 to n-1. Of course, non-integer types cannot be used as input. &amp;gt;&amp;gt;&amp;gt; torch.randperm(4) tensor([2, 1, 0, 3]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(8) tensor([4, 0, 1, 3, 2, 5, 6, 7]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(16) tensor([12, 5, 6, 3, 15, 13, 2, 4, 7, 11, 1, 0, 9, 10, 14, 8]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(4.0) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; TypeError: randperm():</description></item><item><title>Coordinate Transformation in Curved Surface Theory</title><link>https://freshrimpsushi.github.io/en/posts/2108/</link><pubDate>Sat, 06 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2108/</guid><description>Definition 1 Assuming that $2$th-dimensional Euclidean space $U \subset \mathbb{R}^{2}$ is a set, let&amp;rsquo;s say $k \in \mathbb{N}$. If for $k \in \mathbb{N}$ a bijective function $f : U \to \mathbb{R}^{3}$ and its inverse function $f^{-1}$ are both $C^{k}$ functions, it is called a Coordinate Transformation. Explanation The definition of coordinate transformation reminds us of the homeomorphism and diffeomorphism discussed in general topology. It is specifically about dealing with $3$-dimensional</description></item><item><title>Inverse Function Theorem in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/3139/</link><pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3139/</guid><description>Theorem1 Let&amp;rsquo;s say a function $\mathbf{f} : E \subset \mathbb{R}^{n} \to \mathbb{R}^{n}$ defined in an open set $E$ is a $C^{1}$-function. For $\mathbf{a} \in E$, let&amp;rsquo;s assume that $\mathbf{f}^{\prime}(\mathbf{a})$ is invertible and $\mathbf{b} = \mathbf{f}(\mathbf{a})$. Then, the following holds. (a) There exists an open set $U, V \subset \mathbb{R}^{n}$ where $\mathbf{a} \in U, \mathbf{b} \in V$, and over $U$, $\mathbf{f}$ is one-to-one and $\mathbf{f}(U) = V$. (b) If $\mathbf{g}$ is</description></item><item><title>Rao-Blackwell Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/2107/</link><pubDate>Thu, 04 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2107/</guid><description>Theorem 1 2 Description To put the Rao-Blackwell Theorem into simple terms, it could be summarized as a theorem that &amp;rsquo;tells why sufficient statistics are useful.&amp;rsquo; An unbiased estimator becomes more effective, as in having a reduced variance, when information about the sufficient statistic is provided. Especially if $T$ is the minimum sufficient statistic, then $\phi \left( T \right)$ becomes the best unbiased estimator, as proven by the theorem. Proof</description></item><item><title>Jacobian of Composite Functions</title><link>https://freshrimpsushi.github.io/en/posts/3138/</link><pubDate>Wed, 03 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3138/</guid><description>Theorem Let&amp;rsquo;s assume we have two functions $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ and $g : \mathbb{R}^{m} \to \mathbb{R}^{k}$. We denote the Jacobian of $f$ as $J(f)$. Then, the following holds. $$ J(g \circ f) = J(g) J(f) $$ Explanation Since the Jacobian is the most generalized derivative, the above theorem is a generalization of the chain rule. Proof By definition of the Jacobian, $$ J(g \circ f) = \begin{bmatrix} \dfrac{\partial</description></item><item><title>Simple Surfaces, Coordinate Mapping</title><link>https://freshrimpsushi.github.io/en/posts/2106/</link><pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2106/</guid><description>Definition 1 1 Let&amp;rsquo;s consider subsets $U \subset \mathbb{R}^{2}$ of a $2$-dimensional Euclidean space with coordinates $u_{1}$, $u_{2}$ to be open sets. If there exists a $C^{k}$ injective function $\mathbf{x} : U \to \mathbb{R}^{3}$ that satisfies the following for all $p \in U$, it is called a Simple Surface. $$ {{ \partial \mathbf{x} } \over { \partial u_{1} }} (p) \times {{ \partial \mathbf{x} } \over { \partial u_{2} }}</description></item><item><title>Alternating Function</title><link>https://freshrimpsushi.github.io/en/posts/3137/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3137/</guid><description>Definition Let set $X$ be given. A function that satisfies the following is called an alternating function. $$ \phi : \overbrace{X \times X \times \cdots \times X}^{n} \to \mathbb{R} \\ \phi (x_{1}, \dots, x_{i}, x_{i+1}, \dots, x_{n}) = - \phi (x_{1}, \dots, x_{i+1}, x_{i}, \dots, x_{n}) $$ Explanation It is a function whose sign changes when two adjacent variables are swapped. Of course, it can also be shown that this</description></item><item><title>Topological Equivalence between Dynamical Systems</title><link>https://freshrimpsushi.github.io/en/posts/2105/</link><pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2105/</guid><description>Definition 1 $$ \left\{ T , \mathbb{R}^{n} , \varphi^{t} \right\} \\ \left\{ T , \mathbb{R}^{n} , \psi^{t} \right\} $$ Suppose we are given two dynamical systems as shown above. If there exists a homeomorphism $h : \mathbb{R}^{n} \to \mathbb{R}^{n}$ that maps each orbit of the first system to all orbits of the second system while preserving the direction of time, then the two systems are said to be topologically equivalent.</description></item><item><title>Differentiation of Functions Defined on Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3136/</link><pubDate>Sat, 30 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3136/</guid><description>Theorem1 Let&amp;rsquo;s call $M_{1}^{n}, M_{2}^{m}$ and $m, n$, respectively, $m, n$-dimensional differentiable manifolds. Let&amp;rsquo;s say $\varphi : M_{1} \to M_{2}$ is a differentiable function. And for every point $p \in M_{1}$ and tangent vector $v \in T_{p}M$, choose a differentiable curve $$\alpha : (-\epsilon, \epsilon) \to M_{1} \text{ with } \alpha (0) = p,\ \alpha^{\prime}(0)=v$$ Let&amp;rsquo;s set it as $\beta = \varphi \circ \alpha$. Then, the following mapping $$ d\varphi_{p}</description></item><item><title>Proof of the Rearrangement Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2104/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2104/</guid><description>Theorem 1 Suppose there is a plane regular simple closed curve $\alpha$ with length $L$. If the area enclosed by $\alpha$ is denoted as $A$, then $$ L^{2} \ge 4 \pi A $$ In particular, the condition for $L^{2} = 4 \pi A$ is that $\alpha$ is a circle. Description In fact, the fact itself mentioned in this theorem is known to many people, whether intuitively or otherwise, because we</description></item><item><title>How to Find Derivatives in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3135/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3135/</guid><description>Overview1 The package is named Calculus.jl, but it does not support integration. If automatic differentiation, as discussed in machine learning, is needed, refer to the Zygote.jl package. Differentiation of Single Variable Function Derivative function derivative() It calculates the derivative of $f : \R \to \R$. derivative(f) or derivative(f, :x): Returns the derivative $f^{\prime}$. derivative(f, a): Returns the differential coefficient $f^{\prime}(a)$. julia&amp;gt; f(x) = 1 + 2x + 3x^2 f (generic</description></item><item><title>Stochastic Differential Equations with White Noise</title><link>https://freshrimpsushi.github.io/en/posts/2103/</link><pubDate>Wed, 27 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2103/</guid><description>Motivation $$ \xi (t) \overset{?}{:=} \dot{W}(t) = {{d W (t)} \over {dt}} $$ Imagine a $\xi$ as the derivative of a Wiener process as shown above. When thinking of Brownian motion, this $\xi (t)$ would be a noise representing random fluctuations at the time point $t$. Although it seems very intuitive and not at all awkward, regrettably, there is an issue with the existence of $\dot{W}(t)$ in the universal sense.</description></item><item><title>Chain Rule for Multivariable Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/3134/</link><pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3134/</guid><description>Theorem Let&amp;rsquo;s assume that two functions $\mathbf{g} : D \subset \mathbb{R}^{m} \to \mathbb{R}^{k}$, $\mathbf{f} : \mathbf{g}(\mathbb{R}^{k}) \subset \mathbb{R}^{k} \to \mathbb{R}^{n}$ are differentiable. Then, the composition of these two functions $\mathbf{F} = \mathbf{f} \circ \mathbf{g} : \mathbb{R}^{m} \to \mathbb{R}^{n}$ is also differentiable, and the (total) derivative of $\mathbf{F}$ satisfies the following. $$ \mathbf{F}^{\prime}(\mathbf{x}) = \mathbf{f}^{\prime}\left( \mathbf{g}(\mathbf{x}) \right) \mathbf{g}^{\prime}(\mathbf{x}) $$ Explanation This is called the chain rule. If we denote $\mathbf{x} =</description></item><item><title>Derivation of the Area Formula for a Region Enclosed by a Simple Closed Plane Curve</title><link>https://freshrimpsushi.github.io/en/posts/2102/</link><pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2102/</guid><description>Formula 1 If a simple closed curve $\alpha$ surrounds the region $R$ and rotates in a counterclockwise direction, $$ V (R) = \int_{\alpha} x dy = - \int_{\alpha} y dx $$ $V(R)$ represents the volume of the region $R$, or in other words, the area of $R$. Proof According to Green&amp;rsquo;s Theorem, let&amp;rsquo;s assume that a simple planar $C^{2}$ closed curve $\mathcal{C}$, which is piecewise smooth, encloses a bounded region</description></item><item><title>How to Create an Array Filled with a Specific Value in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2101/</link><pubDate>Sat, 23 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2101/</guid><description>Code fill() function can be used. It serves a similar purpose to the rep() function in R.</description></item><item><title>Tangent Vector on Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3132/</link><pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3132/</guid><description>Buildup1 To define a tangent vector at each point on a differentiable manifold $M$, let&amp;rsquo;s assume a differentiable curve $\alpha : (-\epsilon , \epsilon) \to M$ is given. We would like to define the derivative $\dfrac{d \alpha}{dt}(0)$ at $t=0$ in $\alpha$ as a tangent vector, like in differential geometry, but since the range of $\alpha$ is $M$ (since it&amp;rsquo;s not guaranteed to be a metric space), we cannot speak of</description></item><item><title>Proof of the Rotation Number Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2100/</link><pubDate>Thu, 21 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2100/</guid><description>Theorem 1 The winding number of a plane simple closed curve is $i_{\alpha} = \pm 1$. Explanation It&amp;rsquo;s a brief but very intuitive and important theorem. The proof is somewhat unique. Proof Let&amp;rsquo;s say $\alpha (s)$ is a curve that satisfies the condition of the theorem with length $L$. $$ 0 \le u &amp;lt; v \le L $$ Define two points $u, v$ appearing according to the reparameterization of arc</description></item><item><title>Scattering Problem of Sound Waves</title><link>https://freshrimpsushi.github.io/en/posts/3131/</link><pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3131/</guid><description>Explanation1 The canonical problem of scattering theory is, given the incident field $u^{i}$, to find the scattered field $u^{s}$ in the situation where the total field is as per $u = u^{i} + u^{s}$. For acoustic waves, it is assumed that the incident field is given as the following time-harmonic plane wave. $$ u^{i} (x,t) = e^{i(k x\cdot d - \omega t)},\quad x\in \mathbb{R}^{3} $$ Here, $k = \dfrac{\omega}{c_{0}}$ denotes</description></item><item><title>Installing TensorFlow GPU on Windows with Python</title><link>https://freshrimpsushi.github.io/en/posts/2099/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2099/</guid><description>Guide Installing TensorFlow GPU isn&amp;rsquo;t as straightforward as installing regular programs with just an installer. If there are various issues, it might be more convenient to wipe your computer and start from scratch, and if it&amp;rsquo;s your first time, be prepared to do so about ten times. Step 1. NVIDIA GPU Driver Installation Check your graphics specifications in the NVIDIA control panel and select the appropriate driver. Although setting the</description></item><item><title>Differentiable Functions from a Differentiable Manifold to a Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3130/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3130/</guid><description>Definition1 Given that $M_{1}, M_{2}$ are each a $n, m$-dimensional differentiable manifold, a mapping $\varphi : M_{1} \to M_{2}$ is defined to be differentiable at $p \in M_{1}$ if it satisfies the following conditions: Whenever a coordinate system $\mathbf{y} : V \subset \mathbb{R}^{m} \to M_{2}$ is given in $\varphi(p)$, there exists a coordinate system $\mathbf{x} : U \subset \mathbb{R}^{n} \to M_{1}$ in $p$ such that $\varphi\left( \mathbf{x}(U) \right) \subset \mathbf{y}(V)$</description></item><item><title>Rotation Number of Plane Curves</title><link>https://freshrimpsushi.github.io/en/posts/2098/</link><pubDate>Sun, 17 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2098/</guid><description>Buildup Before discussing how much the tangent of a plane curve rotates, let&amp;rsquo;s first consider something like the appropriate angle function. In the plane, let&amp;rsquo;s represent the size of the angle formed at point $p$ from the horizontal line (x-axis) to the tangent $t$ as $\overline{\theta} (p)$. The problem is that its value is $0 \le \overline{\theta} \le 2\pi$ so it is discontinuous from $0$ to $\overline{\theta}$. To overcome this,</description></item><item><title>Sommerfeld Radiation Condition</title><link>https://freshrimpsushi.github.io/en/posts/3129/</link><pubDate>Sat, 16 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3129/</guid><description>Definition1 Let&amp;rsquo;s refer to $u$ as a time-harmonic wave. The following condition is known as the Sommerfeld radiation condition. $$ \lim \limits_{r \to \infty} r \left( \dfrac{\partial u}{\partial r} - ik u \right) = 0 $$ Explanation The Sommerfeld radiation condition is a criterion that physically feasible solutions of the Helmholtz equation must satisfy. It was proposed by the German physicist Sommerfeld in his 1912 paper Die greensche Funktion der</description></item><item><title>How to Read SHP Files in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2097/</link><pubDate>Fri, 15 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2097/</guid><description>Code The code to read a shp file named XsDB_주거인구_100M_TM.shp is as follows. using Shapefile cd(@__DIR__) path = &amp;#34;XsDB_주거인구_100M_TM.shp&amp;#34; table</description></item><item><title>Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3128/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3128/</guid><description>Definition1 In a Euclidean space $\mathbb{R}^{n+1}$, the set of all lines passing through the origin $\mathbf{0}$ is denoted by $\mathbb{P}^{n}$ and referred to as the projective space. $$ \mathbb{P}^{n} := \left\{ \text{all straight lines passing through in } \mathbb{R}^{n+1} \right\} $$ Explanation An easy example of a moduli space. $$ (x_{1}, \dots, x_{n+1}) \sim (\lambda x_{1}, \dots, \lambda x_{n+1}),\quad \lambda \in \mathbb{R}\setminus \left\{ 0 \right\} $$ Since the points on</description></item><item><title>Winding Number of a Closed Curve</title><link>https://freshrimpsushi.github.io/en/posts/2096/</link><pubDate>Wed, 13 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2096/</guid><description>Buildup Before discussing how much the tangent of a plane curve rotates, let&amp;rsquo;s first think about something like an appropriate angle function. In the plane, let&amp;rsquo;s denote the size of the angle created by the tangent $t$ from point $p$ to the horizontal line (x-axis) as $\overline{\theta} (p)$. The issue is that since the value is $0 \le \overline{\theta} \le 2\pi$, it&amp;rsquo;s not continuous from $0$ to $\overline{\theta}$. To overcome</description></item><item><title>What is Scattering Theory?</title><link>https://freshrimpsushi.github.io/en/posts/3127/</link><pubDate>Tue, 12 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3127/</guid><description>Explanation1 What is Scattering Theory? Scattering theory studies the effects that an inhomogeneous medium has on incoming particles or waves. It explains why the sky is blue and Rutherford&amp;rsquo;s alpha particle scattering experiment, and is also applied in medical imaging technologies such as tomography. Problems Addressed Scattering theory can broadly be divided into quantum scattering theory and classical scattering theory. A fundamental problem in classical scattering theory involves &amp;ldquo;boundaries with</description></item><item><title>How to truncate decimal points and convert to an integer in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2095/</link><pubDate>Mon, 11 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2095/</guid><description>Overview To use the trunc function, simply pass Int as the first argument. Code julia&amp;gt; @time for t in 1:10^8 Int64(ceil(t/1000)) end 0.189653 seconds julia&amp;gt; @time for t in 1:10^8 trunc(Int64, ceil(t/1000)) end 0.128472 seconds The two loops perform the identical task but with about a 1.5 times speed difference. The former drops the decimal points using ceil and type casts to Int64, whereas the latter returns an integer natively</description></item><item><title>Moduli Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3126/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3126/</guid><description>Definition Simple Definition A collection of geometric objects is called a moduli space. Description For example, there is the projective space, which is the set of all lines passing through the origin.</description></item><item><title>Definition of a Simple Curve</title><link>https://freshrimpsushi.github.io/en/posts/2094/</link><pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2094/</guid><description>Definition 1 A regular curve $\beta (t)$ is said to be simple if $\beta$ is an injective function or it is a closed curve with period $a &amp;gt; 0$ that satisfies the following for some integer $n \in \mathbb{Z}$: $$ \beta \left( t_{1} \right) = \beta \left( t_{2} \right) \iff t_{1} - t_{2} = na $$ Example Cases like the above, which cannot be represented as an injective function but</description></item><item><title>What is an Inverse Problem?</title><link>https://freshrimpsushi.github.io/en/posts/3125/</link><pubDate>Fri, 08 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3125/</guid><description>Definition When there is a physical phenomenon expressed by a formula, the process of solving the formula based on a cause to find information about the result is called a direct problem or forward problem. Conversely, the process of finding information about the cause from information about the result is called an inverse problem. Explanation Many problems are direct problems, for example, when an object with an initial velocity of</description></item><item><title>Renaming Column Names of a DataFrame in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2093/</link><pubDate>Thu, 07 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2093/</guid><description>Overview To rename columns, you can use the rename!() function1. You can rename them all at once by providing a list of strings, or individually. Code using DataFrames df = DataFrame(rand(1:9, 10, 3), :auto) rename!(df, [&amp;#34;X&amp;#34;, &amp;#34;Y&amp;#34;, &amp;#34;Z&amp;#34;]) rename!(df, :X =&amp;gt; :A) When executed, it first creates the following dataframe: julia&amp;gt; df = DataFrame(rand(1:9, 10, 3), :auto) 10×3 DataFrame Row │ x1 x2 x3 │ Int64 Int64</description></item><item><title>Including Functions</title><link>https://freshrimpsushi.github.io/en/posts/3124/</link><pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3124/</guid><description>Definition Let&amp;rsquo;s denote $X \subset Y$. A function that satisfies the following is called an inclusion function. $$ i : X \to Y, \quad \text{and} \quad i(x) = x,\quad \forall x\in X $$ Explanation Simply put, it&amp;rsquo;s an identity function whose codomain could be larger than its domain. $i : X \hookrightarrow Y$1 or $i : X \subset Y$2 notation is also used. 박대희·안승호</description></item><item><title>Definition of a Closed Curve</title><link>https://freshrimpsushi.github.io/en/posts/2092/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2092/</guid><description>Definition 1 A regular curve $\beta (t)$ being a closed curve is equivalent to being a periodic function $\beta$. Formula: Length of a Closed Curve If $\alpha (s)$ is the arc length parameterization of a closed curve $\beta (t)$ with period $a&amp;gt;0$, then $\alpha$ is a closed curve with period $L = \int_{0}^{a} |d \beta / dt| dt$. In other words, the length of the closed curve $\beta$ is $L$.</description></item><item><title>Expansion and Contraction of a Function</title><link>https://freshrimpsushi.github.io/en/posts/3123/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3123/</guid><description>Definition1 Let&amp;rsquo;s assume that function $f : X \to Y$ is given. Let&amp;rsquo;s also assume that $U \subset X \subset V$ holds. Contraction Mapping We call $f |_{U}$ a contraction mapping of $f$ if it satisfies the following. $$ f|_{U} : U \to Y \quad \text{and} \quad f|_{U}(x) = f (x),\quad \forall x \in U $$ Extension We call $\tilde{f}$ an extension of $f$ if it satisfies the following. $$</description></item><item><title>Inverse Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/2091/</link><pubDate>Sun, 03 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2091/</guid><description>Definition1 The inverse functions of trigonometric functions are called inverse trigonometric functions, and they are denoted by adding arc- to the name of the trigonometric function. $$ \begin{align*} \arcsin x &amp;amp;= \sin^{-1} x \qquad &amp;amp; \operatorname{arccsc} x &amp;amp;= \csc^{-1} x \\ \arccos x &amp;amp;= \cos^{-1} x \qquad &amp;amp; \operatorname{arcsec} x &amp;amp;= \sec^{-1} x \\ \arctan x &amp;amp;= \tan^{-1} x \qquad &amp;amp; \operatorname{arccot} x &amp;amp;= \cot^{-1} x \end{align*} $$ Description Since</description></item><item><title>Helmholtz Equation</title><link>https://freshrimpsushi.github.io/en/posts/3122/</link><pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3122/</guid><description>Definition The following partial differential equation is called the Helmholtz equation. $$ \nabla^{2}u(x) + k^{2} u(x) = \Delta u(x) + k^{2} u(x) = (\Delta + k^{2} )u(x) = 0,\quad x \in \mathbb{R}^{n} $$ Here, $\nabla ^{2} = \Delta$ is the Laplacian. Explanation It can also be expressed in the form of $-\Delta u = \lambda u$. Hence it is sometimes called the eigenvalue equation for the Laplace operator. It can</description></item><item><title>Tangents, Normals, and Curvature of Plane Curves</title><link>https://freshrimpsushi.github.io/en/posts/2090/</link><pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2090/</guid><description>Definitions 1 Let us assume that a unit speed plane curve $\alpha : (a,b) \to \mathbb{R}^{2}$ is given. The tangent (vector field) is defined as $t (s) := \alpha^{\prime} (s)$. The unique vector field $n(s)$ that makes $\left\{ t(s), n(s) \right\}$ the counterclockwise basis of $\mathbb{R}^{2}$ is defined as normal (vector field). The plane curvature is defined as $k(s) := \left&amp;lt; t^{\prime}(s) , n (s) \right&amp;gt;$. Basic Properties [1] $$</description></item><item><title>Differential Equations: Fundamental Solutions and Green's Functions</title><link>https://freshrimpsushi.github.io/en/posts/3121/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3121/</guid><description>Definition A solution $u$ of a nonhomogeneous differential equation with a nonhomogeneous term of $f$, expressed as a function of $\Phi$ and $f$, is called the fundamental solution of the differential equation. $$ u = u\left( \Phi, f \right) $$ Description Note that this is not a strict definition. It is also called Green&amp;rsquo;s function. Both terms refer to the same concept, but Green&amp;rsquo;s function usually implies that boundary conditions</description></item><item><title>How to Open a shp File with QGIS</title><link>https://freshrimpsushi.github.io/en/posts/2089/</link><pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2089/</guid><description>Overview The shp extension represents a Shapefile. Many geographic information data are managed in formats accompanied by *.shp files and others such as *.dbf, *.sbn, *.sbx, *.shx. The most baffling thing upon receiving data is not knowing how to view it. Guide Step 1. Install QGIS https://qgis.org/en/site/forusers/download.html QGIS is an application software equipped with the ability to view and edit GIS (Geographic Information System) data. Download the installer appropriate for</description></item><item><title>Coordinates of a Three-Dimensional Unit Sphere</title><link>https://freshrimpsushi.github.io/en/posts/3120/</link><pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3120/</guid><description>Formulas 3D space&amp;rsquo;s unit sphere can be represented by the following six coordinate patch mappings. For $(u,v) \in U = \left\{ (u,v) : u^{2} + v^{2} \lt 1 \right\}$, $$ \begin{align*} \mathbf{x} _{(0,0,1)}(u, v) &amp;amp;= \left( u, v , \sqrt{1- u^{2} -v^{2} } \right) \\ \mathbf{x}_{(0,0,-1)}(u, v) &amp;amp;= \left( u, v , -\sqrt{1- u^{2} -v^{2} } \right) \\ \mathbf{x}_{(0,1,0)}(u, v) &amp;amp;= \left( u, \sqrt{1- u^{2} -v^{2}}, v \right) \\ \mathbf{x}_{(0,-1,0)}(u,</description></item><item><title>How to Calculate Distance using NearstNeibors.jl in julia</title><link>https://freshrimpsushi.github.io/en/posts/2088/</link><pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2088/</guid><description>Overview To calculate the distance between $n$ points, if there&amp;rsquo;s no need to create a matrix but just to compute the distance, using a k-d tree1, a data structure advantageous for multi-dimensional search, can enhance speed. All related algorithms are implemented in NearestNeighbors.jl, so refer to the official GitHub page. Speed Comparison Let&amp;rsquo;s compare with the technique optimized for calculating distance matrices using the pairwise() function. using Distances using StatsBase</description></item><item><title>Extensions of Bounded Linear Operators</title><link>https://freshrimpsushi.github.io/en/posts/3119/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3119/</guid><description>Theorem1 2 Let $V_{1}, V_{2}$ be a Banach space. Let $W \subset V_{1}$ be a dense subspace. And let $T : W \to V_{2}$ be a bounded linear operator. Then for all $\mathbf{v} \in W$, there exists a unique bounded linear operator that satisfies $$ \widetilde{T} : V_{1} \to V_{2} $$. Moreover, the following holds: $$ \| \widetilde{T} \| = \left\| T \right\| $$. Explanation $\widetilde{T}$ is called the extension</description></item><item><title>Proof of the Fundamental Theorem of Curves</title><link>https://freshrimpsushi.github.io/en/posts/2087/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2087/</guid><description>Theorem 1 $a,b$ contains $0$ as an interval. Let&amp;rsquo;s assume the following holds true: (i): $\overline{\kappa}(s) &amp;gt; 0$ is $(a,b)$ at $C^{1}$ (ii): $\overline{\tau}(s)$ is continuous at $(a,b)$ (iii): $\mathbf{x}_{0}$ is a fixed point of $\mathbb{R}^{3}$ (iv): $\left\{ D,E,F \right\}$ is the right-handed orthonormal basis of $\mathbb{R}^{3}$ Then there exists a unique $C^{3}$ regular curve $\alpha : (a,b) \to \mathbb{R}^{3}$ that satisfies the conditions with the parameter being the arc</description></item><item><title>Properties of Bounded Linear Operators</title><link>https://freshrimpsushi.github.io/en/posts/3118/</link><pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3118/</guid><description>Theorem1 Let&amp;rsquo;s denote $V$ as normed space, $T$ as bounded linear operator, and $W \subset V$. Then, the following holds: (a) $$ T\left( \overline{W} \right) \subset \overline{T(W)} $$ Moreover, if $T$ is invertible, and $T^{-1}$ is also a bounded linear operator, then the following is true: $$ T\left( \overline{W} \right) = \overline{T(W)} $$ Here, $\overline{W}$ is the closure of $W$. (b) Let $\left\{ \mathbf{v}_{k} \right\}$ be a sequence in $V$,</description></item><item><title>How to Initialize Swap Memory in Linux</title><link>https://freshrimpsushi.github.io/en/posts/2086/</link><pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2086/</guid><description>Guide code1 While using swap memory can temporarily solve the issue of insufficient memory, failing to clear the swap memory once the related work is done can lead to severe lagging. It&amp;rsquo;s normal for the swap memory to decrease gradually rather than disappearing instantly upon turning it off, so be patient even if it takes some time. Environment Ubuntu 20.04 LTS</description></item><item><title>Dense Subsets and Closures</title><link>https://freshrimpsushi.github.io/en/posts/3117/</link><pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3117/</guid><description>Dense Subsets Definition1 Let $W \subset V$ be a subset of the normed space $V$. For any $\mathbf{v} \in V$ and $\epsilon \gt 0$, if there always exists $\mathbf{w} \in W$ satisfying the following, then $W$ is called a dense subset in $V$. $$ \left\| \mathbf{v} - \mathbf{w} \right\| \le \epsilon $$ Explanation If $W$ is a dense subspace of $V$, it implies that any element within $V$ can be</description></item><item><title>Reparameterization and the Tools of Frenet-Serret</title><link>https://freshrimpsushi.github.io/en/posts/2085/</link><pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2085/</guid><description>Definition Let&amp;rsquo;s call $\beta : [a,b] \to \mathbb{R}^{3}$ a regular curve. The arc length reparametrization $t = t(s)$ satisfies $s(t) = \int_{a}^{t} \left| \beta^{\prime}(t) \right| dt$, and the Frenet-Serre apparatus of the unit speed curve $\alpha (s) := \beta \left( t (s) \right)$ $$ \left\{ \kappa_{\alpha} \left( s(t) \right), \tau_{\alpha} \left( s(t) \right) , T_{\alpha} \left( s(t) \right) , N_{\alpha} \left( s(t) \right), B_{\alpha} \left( s(t) \right) \right\} $$ is</description></item><item><title>Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3116/</link><pubDate>Mon, 20 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3116/</guid><description>Definition1 Let $M$ be an arbitrary set and $U_{\alpha} \subset \mathbb{R}^{n}$ an open set. For a one-to-one function $\mathbf{x}_{\alpha} : U_{\alpha} \to M$, define the ordered pair $\left( M, \left\{ \mathbf{x}_{\alpha} \right\}_{\alpha\in \mathscr{A}} \right)$, or simply $M$, that satisfies the following conditions as a differentiable manifold of dimension $n$. $\bigcup \limits_{\alpha} \mathbf{x}_{\alpha} \left( U_{\alpha} \right) = M$ The map $\mathbf{x}_{\beta}^{-1} \circ \mathbf{x}_{\alpha} : \mathbf{x}_{\alpha}^{-1}(W) \to \mathbf{x}_{\beta}^{-1}(W)$ is differentiable for $\varnothing</description></item><item><title>Neumann Factorization Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/2084/</link><pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2084/</guid><description>Theorem Let&amp;rsquo;s say a random sample $X_{1} , \cdots , X_{n}$ has the same probability mass/density function $f \left( x ; \theta \right)$ for a parameter $\theta \in \Theta$. Statistic $Y = u_{1} \left( X_{1} , \cdots , X_{n} \right)$ is a sufficient statistic for $\theta$ if there exist two non-negative functions $k_{1} , k_{2} \ge 0$ that satisfy the following. $$ f \left( x_{1} ; \theta \right) \cdots f</description></item><item><title>Increasing Line Spacing in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3115/</link><pubDate>Sat, 18 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3115/</guid><description>Code $$ \begin{vmatrix} \dfrac{ \partial x}{ \partial r} &amp;amp; \dfrac{ \partial x}{ \partial \theta} \\[1em] \dfrac{ \partial y}{ \partial r} &amp;amp; \dfrac{ \partial y}{ \partial \theta} \end{vmatrix} $$ $$ \begin{align*} \begin{vmatrix} \frac{ \partial x}{ \partial r} &amp;amp; \frac{ \partial x}{ \partial \theta} \\ \frac{ \partial y}{ \partial r} &amp;amp; \frac{ \partial y}{ \partial \theta} \end{vmatrix} &amp;amp;&amp;amp; \begin{vmatrix} \dfrac{ \partial x}{ \partial r} &amp;amp; \dfrac{ \partial x}{ \partial \theta} \\ \dfrac{</description></item><item><title>Formulas for Curves on a Sphere</title><link>https://freshrimpsushi.github.io/en/posts/2083/</link><pubDate>Fri, 17 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2083/</guid><description>Formula 1 Let&amp;rsquo;s say a unit speed curve $\alpha : I \to \mathbb{R}^{3}$ is placed on a sphere with center $m$ and radius $r$. That is, $$ \alpha (I) \subset S_{r,m} = \left\{ x \in \mathbb{R}^{3} : \left&amp;lt; x - m , x - m \right&amp;gt; = r^{2} \right\} $$ then it follows that $\kappa \ne 0$. If $\tau \ne 0$, then with respect to $\rho = 1/\kappa$ and $\sigma</description></item><item><title>Saving and Loading Weights, Models, and Optimizers in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3114/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3114/</guid><description>Not Re-training1 2 3 Saving If you&amp;rsquo;re not planning to re-train, you can simply save the weights or the entire model. However, as mentioned below, if you&amp;rsquo;re planning to re-train, you also need to save the optimizer. Weights can be easily saved as follows: # 모델 정의 class CustomModel(nn.module): ...(이하생략) model = CustomModel() # 가중치 저장 torch.save(model.state_dict(),</description></item><item><title>Integration of Complex Functions</title><link>https://freshrimpsushi.github.io/en/posts/2082/</link><pubDate>Wed, 15 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2082/</guid><description>Definition 1 $$ g(t) := p(t) + i q(t) \qquad , t \in [a,b] $$ Let&amp;rsquo;s assume for a real function $p, q : [a,b] \to \mathbb{R}$, a complex function $g : [a,b] \to \mathbb{C}$ is expressed as above. The definite integral from $[a,b]$ to $g$ is defined as follows. $$ \int_{a}^{b} g(t) dt = \int_{a}^{b} p(t) dt + i \int_{a}^{b} q(t) dt $$ For $t \in [a,b]$, the complex</description></item><item><title>Line Integrals of Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/3113/</link><pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3113/</guid><description>Definition1 Let a vector field $\mathbf{F} : \mathbb{R}^{3} \to \mathbb{R}^{3}$ and a curve $C$ in 3-dimensional space be given as $\mathbf{r}(t)$. Let $\mathbf{T}$ be called the tangent field of the vector field. Then, the $\mathbf{F}$ line integral along the curve $C$ is defined as follows. $$ \int_{C} \mathbf{F} \cdot d \mathbf{r} = \int_{a}^{b} \mathbf{F}\left( \mathbf{r}(t) \right) \cdot \mathbf{r}^{\prime}(t) dt = \int_{C} \mathbf{F} \cdot \mathbf{T} ds $$ Explanation The buildup to</description></item><item><title>Tangent Plane and Normal Plane</title><link>https://freshrimpsushi.github.io/en/posts/2081/</link><pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2081/</guid><description>Definition 1 A curve $\alpha$ is given. The plane $\text{span} \left\{ T, N \right\}$ perpendicular to $B$ is called the Osculating Plane. The plane $\text{span} \left\{ N, B \right\}$ perpendicular to $T$ is called the Normal Plane. The plane $\text{span} \left\{ B, T \right\}$ perpendicular to $N$ is called the Rectifying Plane. $T,N,B$ represents the Tangent, Normal, Binormal respectively. $\text{span}$ represents the space generated by vectors. Explanation These planes should</description></item><item><title>Scalar Field Line Integral</title><link>https://freshrimpsushi.github.io/en/posts/3112/</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3112/</guid><description>Line Integral over a Plane Curve1 Buildup Given a function as in $y = f(x)$, its definite integral is defined by the idea of adding up all the function values $f(x)$ along the $x$ axis. Thus, the integral value is obtained along a straight line on the $x$ axis. Now, consider a two-variable function $z=f(x,y)$. Unlike in the case of single-variable functions, since the variable moves over the $xy-$ plane,</description></item><item><title>Zeros in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/2080/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2080/</guid><description>Definition 1 $\alpha \in \mathbb{C}$ being a zero of order $n$ of the function $f : \mathbb{C} \to \mathbb{C}$ means that for some function $g$, where $\displaystyle \lim_{z \to \alpha} g(z) \ne 0$, it can be expressed as follows: $$ f(z) = (z-\alpha)^{n} g(z) $$ Theorem Zeros are isolated: For a zero $f$, we can take a radius such that no other zeros exist around it. For the zero $\alpha$</description></item><item><title>Length of a Curve</title><link>https://freshrimpsushi.github.io/en/posts/3111/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3111/</guid><description>Length of a Plane Curve1 Buildup Suppose we have a smooth function $y=f(x)$ given as in figure (a) above, with $n+1$ points on it. The total length $s$ of the curve can be obtained by summing up the lengths $s_{k}$ of each arc divided by points. Moreover, the length of each arc can be approximated by the length between two points as shown in figure (b). As the number of</description></item><item><title>Lanczos Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/2079/</link><pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2079/</guid><description>Theorem 1 $\kappa \ne 0$ for a given unit speed curve $\alpha$ being a helix is equivalent to it being a certain constant $c \in \mathbb{R}$ for $\tau = c \kappa$. $\tau, \kappa$ is the torsion, curvature. Proof Definition of a Helix: A regular curve $\alpha$ is called a helix if for some fixed unit vector $\mathbf{u}$, $\left&amp;lt; T, \mathbf{u} \right&amp;gt;$ is a constant, and $\mathbf{u}$ is called the axis.</description></item><item><title>Smooth Functions</title><link>https://freshrimpsushi.github.io/en/posts/3110/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3110/</guid><description>Definition If a function $f$ is infinitely differentiable, then $f$ is called a smooth function. If a function $f$ is differentiable and $f^{\prime}$ is continuous, then $f$ is called a smooth function. Explanation In analysis and functional analysis, the term smooth likely refers to the first definition. The phrase &amp;lsquo;infinitely differentiable&amp;rsquo; may seem ambiguous, but it can be understood as follows: $$ \text{For any natural number $n$, the $n$th derivative</description></item><item><title>Harmonic Function</title><link>https://freshrimpsushi.github.io/en/posts/2078/</link><pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2078/</guid><description>Definition 1 If a function $\phi (x,y)$ has a continuous second derivative in the region $\mathscr{R}$ and is a solution to the Laplace&amp;rsquo;s equation, it is said to be harmonic. In other words, a harmonic function satisfies the following. $$ \Delta \phi = \nabla^{2} \phi = \phi_{xx} + \phi_{yy} = 0 $$ Especially, if a function $u(x,y), v(x,y)$ is harmonic and $u,v$ satisfies the Cauchy-Riemann equations, then $v(x,y)$ is referred</description></item><item><title>Definition of Directional Derivative</title><link>https://freshrimpsushi.github.io/en/posts/3109/</link><pubDate>Mon, 06 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3109/</guid><description>Buildup Let&amp;rsquo;s say a multivariable function $f = \mathbb{R}^{n} \to \mathbb{R}$ is given. When trying to calculate the derivative of $f$, unlike the case with a univariable function, one must consider the rate of change in &amp;lsquo;which direction&amp;rsquo;. A familiar example is the partial derivative. The partial derivative considers the rate of change with respect to only one variable. For instance, the partial derivative $\dfrac{\partial f}{\partial y}$ of $f=f(x,y,z)$ with</description></item><item><title>Definition of a General Spiral</title><link>https://freshrimpsushi.github.io/en/posts/2077/</link><pubDate>Sun, 05 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2077/</guid><description>Definition 1 A helix is defined as a regular curve $\alpha$ that, for some fixed unit vector $\mathbf{u}$, satisfies $\left&amp;lt; T, \mathbf{u} \right&amp;gt;$ as a constant, and $\mathbf{u}$ is called the axis. $T$ is a tangent. $\left&amp;lt; \cdot , \cdot \right&amp;gt;$ is an inner product. Description According to the definition, since $\mathbb{R}^{3}$ always satisfies $\left&amp;lt; T, \mathbf{u} \right&amp;gt; = 0$ when $\mathbf{u} = B$, all regular curves lying in a</description></item><item><title>Creating and Using Custom Datasets from Numpy Arrays in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3108/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3108/</guid><description>Description &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; from torch.utils.data import TensorDataset, DataLoader Assuming that a stack of 100 &amp;lsquo;black and white&amp;rsquo; photographs of size $32\times 32$ represented as a numpy array $X$, along with their labels $Y$, has been prepared. Let&amp;rsquo;s say it was imported with the following code. &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#34;X.npy&amp;#34;) &amp;gt;&amp;gt;&amp;gt; X.shape (100, 32, 32) &amp;gt;&amp;gt;&amp;gt; Y = np.load(&amp;#34;Y.npy&amp;#34;) &amp;gt;&amp;gt;&amp;gt; Y.shape (100) In order</description></item><item><title>Limits of Complex Functions</title><link>https://freshrimpsushi.github.io/en/posts/2076/</link><pubDate>Fri, 03 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2076/</guid><description>Definition 1 Let $f : \mathbb{C} \to \mathbb{C}$ be a complex function $f : A \to \mathbb{C}$ defined on an open set $A \subset \mathbb{C}$ where $\alpha \in \overline{A}$. To say that $f(z)$ converges to the limit $l$ when $z \to \alpha$ means for all $\varepsilon &amp;gt; 0$, there exists $\delta &amp;gt; 0$ such that $$ 0 &amp;lt; \left| z - \alpha \right| &amp;lt; \delta \implies \left| f(z) - l</description></item><item><title>Integrals of Trigonometric Functions Table</title><link>https://freshrimpsushi.github.io/en/posts/3107/</link><pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3107/</guid><description>Formulas $$ \begin{equation} \int_{0}^{\pi / 2} \sin \theta \cos \theta d \theta = \dfrac{1}{2} \end{equation} $$ $$ \begin{equation} \begin{aligned} \int \cos^{2}\theta d\theta &amp;amp;= \dfrac{1}{2}\theta + \dfrac{1}{4}\sin 2\theta + C \\ \int_{0}^{2\pi} \cos^{2}\theta d\theta &amp;amp;= \pi \\ \int_{0}^{\pi} \cos^{2}\theta d\theta &amp;amp;= \dfrac{\pi}{2} \end{aligned} \end{equation} $$ $$ \begin{equation} \begin{aligned} \int \sin^{2}\theta d\theta &amp;amp;= \dfrac{1}{2}\theta - \dfrac{1}{4}\sin 2\theta + C \\ \int_{0}^{2\pi} \sin^{2}\theta d\theta &amp;amp;= \pi \\ \int_{0}^{\pi} \sin^{2}\theta d\theta &amp;amp;= \dfrac{\pi}{2} \end{aligned}</description></item><item><title>Conditions for a Curve to Lie in a Plane in Three-Dimensional Euclidean Space</title><link>https://freshrimpsushi.github.io/en/posts/2075/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2075/</guid><description>Theorem 1 Given a unit speed curve $\kappa \ne 0$, the following three are equivalent: (a): $\alpha$ lies in a plane. (b): $B$ is a constant. (c): $\tau = 0$ holds. Explanation This is a corollary of the Frenet-Serret formulas, allowing us to understand why torsion is defined in such a bizarre way as $\tau := \left&amp;lt; B^{\prime}, N \right&amp;gt;$. Proof Frenet-Serret Formulas: If $\alpha$ is a unit speed curve</description></item><item><title>Formulas Related to Factorials</title><link>https://freshrimpsushi.github.io/en/posts/3106/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3106/</guid><description>Product of Consecutive Odd Numbers For an integer $n \ge 0$, the following holds. $$ (2n-1) \cdot (2n-3) \cdots 5 \cdot 3 \cdot 1 = \dfrac{(2n)!}{2^{n} (n!)} = (2n-1)!! $$ Here, $n!!$ refers to the double factorial. Proof A detailed explanation is omitted. $$ \begin{align*} 3 \cdot 1 =&amp;amp;\ \dfrac{4 \cdot 3 \cdot 2 \cdot 1}{4 \cdot 2} = \dfrac{4!}{2^{2}(2 \cdot 1)} = \dfrac{(2 \cdot 2)!}{2^{2}(2!)} \\ 5 \cdot 3</description></item><item><title>Orbits and Phase Portraits in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/2074/</link><pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2074/</guid><description>Definition 1 $$ O \left( x_{0} \right) := \left\{ x \in X : x = \varphi^{t} x_{0} , \forall t \in T \right\} $$ In the dynamical system given by $\left( T, X, \varphi^{t} \right)$, let&amp;rsquo;s denote the orbit of $x_{0} \in X$ as shown above. The partition of $X$ consisting of orbits is called the Phase Portrait of the dynamical system. Kuznetsov. (1998). Elements of Applied Bifurcation Theory(2nd Edition):</description></item><item><title>Generalization of Gaussian Integrals</title><link>https://freshrimpsushi.github.io/en/posts/3105/</link><pubDate>Sun, 29 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3105/</guid><description>Formulas1 For an integer $n \ge 0$, the following expressions are true. When multiplied by an even degree polynomial $$ \int_{-\infty}^{\infty} x^{2n} e^{-\alpha x^{2}}dx = \dfrac{(2n)!}{n! 2^{2n}}\sqrt{\dfrac{\pi}{\alpha^{2n+1}}} $$ $$ \int_{0}^{\infty} x^{2n} e^{-\alpha x^{2}}dx = \dfrac{(2n)!}{n! 2^{2n+1}}\sqrt{\dfrac{\pi}{\alpha^{2n+1}}} $$ When multiplied by an odd degree polynomial $$ \int_{-\infty}^{\infty} x^{2n+1} e^{-\alpha x^{2}}dx = 0 $$ $$ \int_{0}^{\infty} x^{2n+1} e^{-\alpha x^{2}}dx = \dfrac{n!}{2 \alpha^{n+1}} $$ Explanation Gaussian Integral $$ \int_{-\infty}^{\infty} e^{-\alpha x^2} dx= \sqrt{\dfrac{\pi}{\alpha}}</description></item><item><title>How to Output a 2D Array to a CSV File in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2073/</link><pubDate>Sat, 28 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2073/</guid><description>Code using CSV, DataFrames A = rand(1:10, 10) B = zeros(10) AB = DataFrame(hcat(A,B), [&amp;#34;A&amp;#34;, &amp;#34;B&amp;#34;]) CSV.write(&amp;#34;AB.csv&amp;#34;, AB) The write function of the CSV package allows you to easily output a two-dimensional array. A, B are one-dimensional arrays, which are combined using the hcat function to transform into a dataframe. Execution Result julia&amp;gt; using CSV, DataFrames julia&amp;gt; A = rand(1:10, 10) 10-element Array{Int64,1}: 8 5 4 3 6 4 10</description></item><item><title>Initializing Weights in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3104/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3104/</guid><description>Code1 Assuming we have defined a neural network as follows. The forward part is omitted. import torch import torch.nn as nn class Custom_Net(nn.Module): def __init__(self): super(Custom_Net, self).__init__() self.linear_1 = nn.Linear(1024, 1024, bias=False) self.linear_2 = nn.Linear(1024, 512, bias=False) self.linear_3 = nn.Linear(512, 10, bias=True) torch.nn.init.constant_(self.linear_1.weight.data, 0) torch.nn.init.unifiom_(self.linear_2.weight.data) torch.nn.init.xavier_normal_(self.linear_3.weight.data) torch.nn.init.xavier_normal_(self.linear_3.bias.data) def forward(self, x): ... Weight initialization can be set through nn.init. For layers with bias, this also needs to be specifically set. Basics</description></item><item><title>Frenet-Serret Formulas</title><link>https://freshrimpsushi.github.io/en/posts/2072/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2072/</guid><description>Formulas 1 If $\alpha$ is a unit speed curve with $\kappa (s) \ne 0$ then $$ \begin{align*} T^{\prime}(s) =&amp;amp; \kappa (s) N(s) \\ N^{\prime}(s) =&amp;amp; - \kappa (s) T(s) + \tau (s) B(s) \\ B^{\prime}(s) =&amp;amp; - \tau (s) N(s) \end{align*} $$ Description In matrix form, it can be expressed as follows. $$ \begin{bmatrix} T \\ N \\ B \end{bmatrix} ^{\prime} = \begin{bmatrix} 0 &amp;amp; \kappa &amp;amp; 0 \\ -</description></item><item><title>How to Implement MLP in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3103/</link><pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3103/</guid><description>Library import torch import torch.nn as nn import torch.nn.functional as F nn and nn.functional include various layers, loss functions, activation functions, etc., for constructing neural networks.</description></item><item><title>Rigorous Definition of Dynamical Systems</title><link>https://freshrimpsushi.github.io/en/posts/2071/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2071/</guid><description>Definition 1 For a space $X$ and a moment $t \in T$, an operator $\varphi^{t}$ is called a Flow. If the set of flows $F := \left\{ \varphi^{t} \right\}_{t \in T}$ satisfies $\left( F , \circ \right)$ with respect to the function composition operation $\circ$, then the triple $\left( T, X, \varphi^{t} \right)$ is called a Dynamic System. $$ \begin{align*} \varphi^{0} =&amp;amp; \text{id} \\ \varphi^{t+s} =&amp;amp; \varphi^{t} \circ \varphi^{s} \end{align*}</description></item><item><title>Conformal Mapping</title><link>https://freshrimpsushi.github.io/en/posts/3102/</link><pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3102/</guid><description>Definition1 Assuming the mapping $\mathbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is given as follows. $$ \mathbf{f}(\mathbf{x}) = \left( f_{1}(\mathbf{x}), f_{2}(\mathbf{x}), \dots, f_{m}(\mathbf{x}) \right),\quad \mathbf{x}\in \R^{n} $$ The total derivative, or Jacobian matrix of $\mathbf{f}$ is as follows. $$ \mathbf{f}^{\prime} = J = \begin{bmatrix} \dfrac{\partial f_{1}}{\partial x_{1}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_{1}}{\partial x_{n}} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \dfrac{\partial f_{m}}{\partial x_{1}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_{m}}{\partial x_{n}} \end{bmatrix} $$ If</description></item><item><title>Frenet-Serret Formulas: Curvature, Tangent, Normal, Binormal, Torsion</title><link>https://freshrimpsushi.github.io/en/posts/2070/</link><pubDate>Sun, 22 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2070/</guid><description>Definition 1 Let&amp;rsquo;s say $\alpha$ is a unit speed curve. The speed $\kappa (s) := \left| T^{\prime}(s) \right|$ of the tangent $T(s) = \alpha^{\prime} (s)$ is called the curvature $\alpha (s)$. The function obtained by dividing the velocity $T^{\prime}(s)$ of the tangent of $\alpha$ by the curvature $\kappa (s)$, namely, defined as $N$, is referred to as the Normal vector field. $$ N(s) := {{ T^{\prime}(s) } \over { \left|</description></item><item><title>Derivatives of 3D Scalar/Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/3101/</link><pubDate>Sat, 21 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3101/</guid><description>Theorem When the 3D scalar function $f : \mathbb{R}^{3} \to \mathbb{R}^{1}$ is $f(x(t), y(t), z(t)) = f$, $\dfrac{df}{dt}$ is as follows. $$ \dfrac{d f}{d t} = \dfrac{\partial f}{\partial x}\dfrac{dx}{dt} + \dfrac{\partial f}{\partial y}\dfrac{dy}{dt} + \dfrac{\partial f}{\partial z}\dfrac{dz}{dt} $$ When the 3D vector function $\mathbf{f} : \mathbb{R}^{3} \to \mathbb{R}^{3}$ is $\mathbf{f}(x(t), y(t), z(t)) = (f_{1}, f_{2}, f_{3})$, $\dfrac{d \mathbf{f}}{dt}$ is as follows. $$ \begin{align*} \dfrac{d \mathbf{f}}{d t} =&amp;amp;\ \left( \dfrac{d f_{1}}{d</description></item><item><title>Embedding Theorems in Lp Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3100/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3100/</guid><description>Theorem1 If $\Omega \subset \mathbb{R}^{n}$ is an open set and let&amp;rsquo;s assume $\text{vol}(\Omega) = \int_{\Omega} 1 dx \lt \infty$. (a) For $1 \le p \le q \le \infty$, if $u \in L^{q}(\Omega)$ then, $u \in L^{p}(\Omega)$ and $$ \begin{equation} \left\| u \right\|_{p} \le \left( \text{vol}(\Omega) \right)^{\frac{1}{p} - \frac{1}{q}} \left\| u \right\|_{q} \end{equation} $$ And $L^{q}$ is embedded into $L^{p}$. $$ \begin{equation} L^{q}(\Omega) \to L^{p}(\Omega) \end{equation} $$ (b) For $1 \le</description></item><item><title>The Definition of Strings</title><link>https://freshrimpsushi.github.io/en/posts/2068/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2068/</guid><description>Definition 1 Let there be a given curve $\alpha : (c,d) \to \mathbb{R}^{3}$. When $c &amp;lt; a &amp;lt; b &amp;lt; d$, for all $t \in [a,b]$ that satisfies $\alpha (t) = \gamma (t)$, $\gamma : [a,b] \to \mathbb{R}^{3}$ is called a Chord or Curve Segment. The length of chord $\gamma : [a,b] \to \mathbb{R}^{3}$, denoted $l_{[a,b]}(\gamma)$, is defined as follows. $$ l_{[a,b]}(\gamma) := \int_{a}^{b} \left| {{ d \gamma } \over</description></item><item><title>L infinity space</title><link>https://freshrimpsushi.github.io/en/posts/3099/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3099/</guid><description>Definitions1 Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. For a measurable function $u$ on $\Omega$, if there exists a constant $K$ that satisfies the following condition for $u$, then $u$ is said to be essentially bounded on $\Omega$. $$ \left| u(x) \right| \le K \text{ a.e. on } \Omega $$ Here, $\text{a.e.}$ means almost everywhere. The supremum of such $K$ is called the essential supremum of $\left| u</description></item><item><title>Inverse Holder's Inequality: A Sufficient Condition for Lp Functions</title><link>https://freshrimpsushi.github.io/en/posts/3098/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3098/</guid><description>Theorem1 Let&amp;rsquo;s consider $\Omega \subset \mathbb{R}^{n}$ as an open set. The necessary and sufficient condition for the measurable function $u$ to be included in the $L^{p}$ space is $$ \sup \left\{ \int_{\Omega} \left| u(x) \right| v(x) dx : v(x) \ge 0 \text{ on } \Omega, \left\| v \right\|_{p^{\prime}} \le 1 \right\} \lt \infty $$ Furthermore, the above supremum is as follows $\left\| u \right\|_{p}$. Here, $p^{\prime} = \dfrac{p}{p-1}$ is the</description></item><item><title>Tangent Lines and Tangent Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/2066/</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2066/</guid><description>Definition Let there be a given regular curve $\alpha (t)$. The vector field $\displaystyle T(t) := {{ d \alpha / d t } \over { \left| d \alpha / d t \right| }}$ is called the Tangent Vector Field. The line $l$ defined as follows in $t = t_{0}$ to $\alpha$ is called the Tangent Line. $$ l := \left\{ \mathbf{w} \in \mathbb{R}^{3} : \mathbf{w} = \alpha \left( t_{0} \right)</description></item><item><title>Definition and Properties of Vector Areas</title><link>https://freshrimpsushi.github.io/en/posts/3097/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3097/</guid><description>Definition For a given surface $S$, the following integral is called the vector area of $S$. $$ \mathbf{a} := \int_{\mathcal{S}} d \mathbf{a} $$ Description As an example, let&amp;rsquo;s calculate the vector area of a hemisphere with a radius of $R$. It is $d \mathbf{a} = R^{2}\sin\theta d\theta d\phi \hat{\mathbf{r}}$. Here, $$ \hat{\mathbf{r}} = \cos\phi \sin\theta \hat{\mathbf{x}} + \sin\phi \sin\theta\hat{\mathbf{y}} + \cos\theta\hat{\mathbf{z}} $$ when integrated over the region of the northern</description></item><item><title>Writing Greek Characters and Subscripts in Julia Variable Names</title><link>https://freshrimpsushi.github.io/en/posts/2065/</link><pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2065/</guid><description>Overview In Julia, Unicode (UTF-8) is allowed for variable names. Therefore, you can use not only Greek letters but also superscripts, subscripts, and even Korean or emojis. There&amp;rsquo;s no particular need to use them, but odd codes like the following work fine. julia&amp;gt; α₁ = 2 2 julia&amp;gt; α₂ = 1 1 julia&amp;gt; println(α₁ \ast\ α₂) 2</description></item><item><title>Various Formulas of Vector Integration Involving the Del Operator</title><link>https://freshrimpsushi.github.io/en/posts/3096/</link><pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3096/</guid><description>Formulas1 Let&amp;rsquo;s designate $T, U$ as a scalar function, and $\mathbf{v}$ as a vector function. Then, the following equations hold: $$ \begin{equation} \int_{\mathcal{V}} (\nabla T) d \tau = \oint_{\mathcal{S}} T d \mathbf{a} \end{equation} $$ $$ \begin{equation} \int_{\mathcal{V}} (\nabla \times \mathbf{v}) d \tau = - \oint_{\mathcal{S}} \mathbf{v} \times d \mathbf{a} \end{equation} $$ $$ \begin{equation} \int_{\mathcal{V}} \left[ T \nabla^{2} U + (\nabla T) \cdot (\nabla U) \right] d \tau = \oint_{\mathcal{S}} (T</description></item><item><title>Reparameterization</title><link>https://freshrimpsushi.github.io/en/posts/2064/</link><pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2064/</guid><description>Definition 1 Let $k \in \mathbb{N}$ and a curve $\alpha : (a,b) \to \mathbb{R}^{3}$ be given. If a bijection $g: (c,d) \to (a,b)$ satisfies $g , g^{-1} \in C^{k}$, then $g$ is referred to as the Reparameterization of $\alpha$. $C^{k}$ is $k$ times differentiable and its derivative is a set of continuous functions. Explanation Pronouncing it is hard, so even in English, it&amp;rsquo;s read as [Reparameterization]; it&amp;rsquo;s just that the</description></item><item><title>PyTorch RuntimeError: "grad can be implicitly created only for scalar outputs" Solution</title><link>https://freshrimpsushi.github.io/en/posts/3095/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3095/</guid><description>Example 1 If you have set the loss function as loss = sum(a,b), an error might occur during backpropagation when loss.backward() is called. Changing it to loss = torch.sum(a,b) will prevent the error.</description></item><item><title>Definition of Logarithmic Functions</title><link>https://freshrimpsushi.github.io/en/posts/2063/</link><pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2063/</guid><description>Definition The inverse function of the exponential function is defined as the logarithmic function $\log : (0,\infty) \to \mathbb{R}$. If for all $x \in (0,\infty)$, $x = e^y$ holds, then the logarithmic function is expressed as follows: $$ \log x := y $$ Explanation Despite its simple definition, logarithms hold significant meaning across mathematics. The base can be any positive number, but it is typically set as Euler&amp;rsquo;s constant $e$</description></item><item><title>Definition of Improper Integrals</title><link>https://freshrimpsushi.github.io/en/posts/3094/</link><pubDate>Sat, 07 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3094/</guid><description>Definition1 Assume that the function $f$ is integrable over every interval $[a,b]$ with fixed $a$ and $b&amp;gt;a$. If the following limit exists, then it is defined as the improper integral of $f$. $$ \int _{a}^{\infty} f(x) dx = \lim \limits_{b \to \infty} \int _{a}^{b} f(x)dx $$ In this case, if the integration on the left-hand side converges, and if replacing $f$ with $\left| f \right|$ the limit still exists, it</description></item><item><title>Definition of a Curve</title><link>https://freshrimpsushi.github.io/en/posts/2062/</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2062/</guid><description>Definitions 1 The mapping $\alpha : (a,b) \to \mathbb{R}^{3}$ is referred to as a Curve. A point $t = t_{0}$ at the curve where $\alpha^{\prime} = \dfrac{d \alpha}{d t} = \mathbf{0}$ is called a Singular Point. A curve $\alpha \in C^{k}$ for some $k \in \mathbb{N}$ where at all $t \in (a,b)$, $\displaystyle {{ d \alpha } \over { d t }} \ne \mathbf{0}$ is known as a Regular Curve.</description></item><item><title>Cauchy Problem, Initial Value Problem</title><link>https://freshrimpsushi.github.io/en/posts/3093/</link><pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3093/</guid><description>Definition1 Let&amp;rsquo;s say we have a partial differential equation defined on an open set $\Omega=\mathbb{R}^{n}$. When the time is $t=0$, the value of the unknown $u$ at $\Omega$, that is, the initial value, is given. The problem of finding solutions to such partial differential equations is called the Cauchy problem or the initial value problem. Explanation The acronym IVP is commonly used. Example Solving the Cauchy problem for the heat</description></item><item><title>Sufficient Statistic</title><link>https://freshrimpsushi.github.io/en/posts/2061/</link><pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2061/</guid><description>Definitions Mathematical Definition 1 Let the probability mass/density function of a random sample $X_{1} , \cdots , X_{n}$ for parameter $\theta \in \Theta$ be $f(x;\theta)$, and let the probability mass/density function for statistic $Y_{1} := u_{1} \left( X_{1} , \cdots , X_{n} \right)$ be $f_{Y_{1}} \left( y_{1}; \theta \right)$. For $H \left( x_{1} , \cdots , x_{n} \right)$, which does not depend on $\theta \in \Theta$, $$ {{ f \left(</description></item><item><title>Wave Equation</title><link>https://freshrimpsushi.github.io/en/posts/3092/</link><pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3092/</guid><description>Definition1 The following partial differential equation is called the wave equation.wave equation. $$ u_{tt} - \Delta u =0 $$ This equation assumes the propagation speed of the wave as a constant $1$. If the propagation speed of the wave is denoted as $c$, then the wave equation becomes, $$ u_{tt} - c^{2}\Delta u =0 $$ In the case of being nonhomogeneousnonhomogeneous, $$ u_{tt} - \Delta u = f $$ $U</description></item><item><title>Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/2060/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2060/</guid><description>Overview The Exponential Function is a generalization of exponentiation that appears universally across all branches of mathematics. Although in original exponentiations the base $a &amp;gt; 0$ does not necessarily have to be $a = e$, the existence of the base change formula means that essentially, it doesn’t matter which base is used. For convenience, when referring to an exponential function, its base is commonly</description></item><item><title>How to Solve the "Fail to create pixmap with TK_GetPixmap in TKImgPhotoInstanceSetSize" Error in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3091/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3091/</guid><description>Problem import matplotlib.pyplot as plt import numpy as np for i in range(400): fig = plt.figure(figsize=(12, 12)) plt.plot(np.random.rand(10)) plt.savefig(&amp;#34;./plt_test/no_%d&amp;#34; %i) print(&amp;#34;test_%d&amp;#34; %i) Let&amp;rsquo;s say you are running a code in Python using matplotlib.pyplot to draw a graph and save it, like the one above. The moment you generate the 369th figure, the following error occurs in the output window. test_366 test_367 test_368 Fail to create pixmap with TK_GetPixmap in TKImgPhotoInstanceSetSize</description></item><item><title>Efficient Estimator</title><link>https://freshrimpsushi.github.io/en/posts/2059/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2059/</guid><description>Definition 1 Let&amp;rsquo;s say $Y$ is an unbiased estimator for the parameter $\theta$. The efficiency of estimator $Y$ with respect to the Cramér-Rao lower bound $\text{RC}$ is defined as: $$ {{ \text{RC} } \over { \Var (Y) }} $$ An estimator with efficiency $1$ is called an Efficient Estimator. Description Cramér-Rao Inequality:</description></item><item><title>What is a State Function in Thermophysics?</title><link>https://freshrimpsushi.github.io/en/posts/3090/</link><pubDate>Fri, 30 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3090/</guid><description>Definition1 A state function or state variable is a property that has a fixed value independent of the path taken and can be measured macroscopically. Explanation Let&amp;rsquo;s explain this more mathematically. Consider a function $f(\mathbf{x})$ that has a value in three dimensions. When $\mathbf{x}$ changes from $\mathbf{x}_{1}=a$ to $\mathbf{x}=b$, if the difference in the value of $f$ is independent of the path, then $f$ is called a state function. $$</description></item><item><title>Polynomial Function</title><link>https://freshrimpsushi.github.io/en/posts/2058/</link><pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2058/</guid><description>Definition 1 A Polynomial of degree $n$ is defined for $n \in \mathbb{N}_{0}$ and $\left\{ a_{k} \right\}_{k=0}^{n} \subset \mathbb{C}$ as follows: $$ P(z) := a_{0} + a_{1} z + \cdots a_{n} z^{n} \qquad , a_{n} \ne 0 $$ Explanation A polynomial function is one of the most basic functions that can be considered in all of mathematics, and it has been proven by the Fundamental Theorem of Algebra that there</description></item><item><title>How to Use the Shepp-Logan Phantom in Julia, MATLAB, and Python</title><link>https://freshrimpsushi.github.io/en/posts/3089/</link><pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3089/</guid><description>Julia1 To use the Tomography package Tomography.jl, you can use the phantom() function. phantom(m,n=1): Generates a Shepp-Logan phantom of size $m\times m$. When n=1, it generates a Shepp-Logan phantom, and when n=2, it generates a Modified Shepp-Logan phantom. using Tomography using Plots # 팬텀 생성 p = phantom(256,2) # 그림 출력 heatmap(reverse(p, dims=1)) Environment OS: Windows10 Version: Julia 1.7.1, Tomography 0.1.5 MATLAB2 You can create a</description></item><item><title>Rao-Blackwell-Kolmogorov Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2057/</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2057/</guid><description>Theorem 1 Regular Conditions: (R0): The probability density function $f$ is injective with respect to $\theta$. In formula, it satisfies: $$ \theta \ne \theta ' \implies f \left( x_{k} ; \theta \right) \ne f \left( x_{k} ; \theta ' \right) $$ (R1): The probability density function $f$ has the same support for all $\theta$. (R2): The true value $\theta_{0}$ is an interior point of $\Omega$. (R3): The probability density function</description></item><item><title>How to Use Radon Transform in Julia, MATLAB, Python</title><link>https://freshrimpsushi.github.io/en/posts/3088/</link><pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3088/</guid><description>Julia1 In the Tomography package Tomography.jl, one can use the radon() function. radon(f, θ): f stands for the image, θ for the projection angle. If not specified, it calculates from 0 degrees to $\pi$ by default. using Tomography using Plots # 팬텀 생성 f = phantom(256,2) # 라돈 변환 계산 ℛf = radon(f) # 그림 출력 h1 = heatmap(reverse(f, dims=1),</description></item><item><title>Definition of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/2056/</link><pubDate>Sun, 25 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2056/</guid><description>Overview Trigonometric functions are functions that associate the angles of a right triangle with their trigonometric ratios. Definition The trigonometric functions sine and cosine $\sin, \cos : \mathbb{R} \to \mathbb{R}$ are defined as follows. $$ \sin \theta := {{ y } \over { \sqrt{x^{2} + y^{2}} }} \\ \cos \theta := {{ x } \over { \sqrt{x^{2} + y^{2}} }} $$ Consequently, secant, cosecant, tangent, and cotangent are defined as</description></item><item><title>Dirichlet Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/3087/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3087/</guid><description>Definition1 Let us assume that a partial differential equation is given on an open set $\Omega$. The following boundary conditions are referred to as Dirichlet boundary conditions. The problem of finding solutions to partial differential equations with Dirichlet boundary conditions is called the Dirichlet problem. $$ u = 0 \quad \text{on } \partial \Omega $$ Explanation Nonhomogeneous Conditions The following boundary conditions are referred to as nonhomogeneous Dirichlet conditions, although,</description></item><item><title>Bartlett's Identity</title><link>https://freshrimpsushi.github.io/en/posts/2055/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2055/</guid><description>Theorem Regular Conditions: (R0): The probability density function $f$ is injective with respect to $\theta$. Mathematically, it satisfies the following. $$ \theta \ne \theta ' \implies f \left( x_{k} ; \theta \right) \ne f \left( x_{k} ; \theta ' \right) $$ (R1): The probability density function $f$ has the same support for all $\theta$. (R2): The true value $\theta_{0}$ is an interior point of $\Omega$. (R3): The probability density function</description></item><item><title>Fundamental Solution of the Laplace Equation</title><link>https://freshrimpsushi.github.io/en/posts/3086/</link><pubDate>Thu, 22 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3086/</guid><description>Buildup1 Laplace&amp;rsquo;s equation is invariant under rotation transformations, so consider changing $u(x)$&amp;rsquo;s variables to radii. This allows simplifying the differential equation as follows. Let&amp;rsquo;s assume that $u=u(x)$ is a solution to Laplace&amp;rsquo;s equation. $$ \Delta u = 0 $$ And let&amp;rsquo;s set $r=|x|=(x_{1}^{2} + \cdots + x_{n}^{2})^{1/2}$ and assume $v\in C^2$ and $u(x) = v(|x|) = v(r) (x\in \mathbb{R}^{n} \setminus \left\{ 0 \right\})$. $$ \begin{align*} v(r) &amp;amp;= u(x) \\ \Delta</description></item><item><title>Law of Mass Action in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/2054/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2054/</guid><description>Law 1 The extent of a chemical reaction is proportional to the concentration of the reacting substances, each of which exerts the same force on the number of various molecules involved. https://terms.naver.com/entry.nhn?docId=1594878&amp;amp;cid=50314&amp;amp;categoryId=50314&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Smoothness of Boundaries</title><link>https://freshrimpsushi.github.io/en/posts/3085/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3085/</guid><description>Definition1 Let&amp;rsquo;s call $U \subset \mathbb{R}^{n}$ a bounded open set. Let $\partial U$ be the boundary of $U$. If there exists a $C^{k}$ function $\gamma = \mathbb{R}^{n-1} \to \mathbb{R}$ satisfying the following for each point $x = (x_{1}, \dots, x_{n}) \in \partial U$ on the boundary, then we say &amp;rsquo;the boundary $\partial U$ is $C^{k}$'. $$ \gamma (x_{1}, x_{2}, \dots, x_{n-1}) = x_{n} $$ Explanation To rephrase the condition in</description></item><item><title>Survival Function</title><link>https://freshrimpsushi.github.io/en/posts/2053/</link><pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2053/</guid><description>Definition 1 If a function $S(0)=1$ is non-increasing, it is defined as a Survival Function. Description The survival function, in simple terms, is a function that maps the probability $S(t) \in [0,1]$ of being alive at time $t$. In mathematics, survival doesn&amp;rsquo;t necessarily stick to the meaning of &amp;lsquo;being alive&amp;rsquo; but is abstracted to the period until a certain event occurs, and since it&amp;rsquo;s about mapping probabilities, it naturally has</description></item><item><title>Rotation Transformation, Rotation Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3084/</link><pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3084/</guid><description>Definition In the two-dimensional plane $\mathbb{R}^{2}$, the transformation that rotates an arbitrary vector counterclockwise by $\theta$ is given by $$ \begin{bmatrix} x^{\prime} \\ y^{\prime} \end{bmatrix} = \begin{bmatrix} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} $$ Explanation The matrix $\begin{bmatrix} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \end{bmatrix}$ is called the rotation matrix or the rotation transformation.</description></item><item><title>AIDS Transmission Model</title><link>https://freshrimpsushi.github.io/en/posts/2052/</link><pubDate>Sat, 17 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2052/</guid><description>Overview AIDS, also known as Acquired Immunodeficiency Syndrome, is a contagious disease that has been tormenting humanity for decades, caused by the virus HIV. The transmission routes of AIDS include homosexual and heterosexual interactions, drug use, among others, making the mathematical modeling of its spread inevitably include the structure of the entire population. However, the simplest model introduced by Castillo, Chavez, and others, which is the [ODE](../../categories/Differential Equations) model, will</description></item><item><title>Absolute Value Function</title><link>https://freshrimpsushi.github.io/en/posts/3083/</link><pubDate>Fri, 16 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3083/</guid><description>Definition A function defined as $f$ is called the absolute value function, and its values are denoted as shown in $|x|$. $$ |x| := f(x) = \begin{cases} x &amp;amp;\text{if } x&amp;gt;0 \\ 0 &amp;amp;\text{if } x=0 \\ -x &amp;amp;\text{if } x&amp;lt;0 \end{cases},\quad x\in \mathbb{R} $$ Explanation Absolute value refers to the magnitude of a real number, and a generalization of this is the norm. The triangle inequality holds. $$ |x</description></item><item><title>Representation of Complex Numbers in Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/2051/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2051/</guid><description>Definition 1 A complex number $z \ne 0$ corresponds to a point $P(x,y)$ on the complex plane, and can be expressed in polar coordinates through the length $r := |z|$ of segment $\overline{OP}$ and the counterclockwise angle $\theta$ made by the segment $\overline{OP}$ and the $x$ axis. $$ z = r \left( \cos \theta + i \sin \theta \right) $$ In this context, $\theta$ is called the Argument and is</description></item><item><title>Partial Derivatives: Derivatives of Multivariable Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/3082/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3082/</guid><description>Buildup[^1] Recall the definition of the derivative of a univariate function. $$ \lim \limits_{h\to 0} \dfrac{f(x+h) - f(x)}{h} = f^{\prime}(x) $$ By approximating the numerator on the left-hand side as a linear function of $h$, we get the following. $$ \begin{equation} f(x+h) - f(x) = a h + r(h) \label{1} \end{equation} $$ Let&amp;rsquo;s call $r(h)$ the remainder, satisfying the condition below. $$ \lim \limits_{h \to 0} \dfrac{r(h)}{h}=0 $$ Then, dividing</description></item><item><title>Periodic Function</title><link>https://freshrimpsushi.github.io/en/posts/2050/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2050/</guid><description>Definition A function $f : \mathbb{R} \to \mathbb{R}$ is called a $T$-periodic function if it satisfies the following for some constant $T \ne 0$ and for all $t \in \mathbb{R}$: $$ f(t + T) = f(t) $$ Example Sine $\sin$ and cosine $\cos$ are typical periodic functions, and according to the above definition, they are $2\pi$-periodic functions.</description></item><item><title>Laplacian of a Scalar Field</title><link>https://freshrimpsushi.github.io/en/posts/3081/</link><pubDate>Mon, 12 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3081/</guid><description>Definition The divergence of the gradient of the scalar function $u : \mathbb{R}^{n} \to \mathbb{R}$ is called the Laplacian and is denoted as follows. $$ \begin{align*} \Delta u :&amp;amp;= \mathrm{div}(\nabla (u)) \\ &amp;amp;= \mathrm{div} \left( \left( u_{x_{1}}, u_{x_{2}}, \dots, u_{x_{n}} \right) \right) \\ &amp;amp;= u_{x_{1}x_{1}} + u_{x_{2}x_{2}} + \cdots + u_{x_{n}x_{n}} \\ &amp;amp;= \sum _{i=1}^{n} u_{x_{i}x_{i}} \end{align*} $$ Here, $u_{x_{i}}=\dfrac{\partial u}{\partial x_{i}}$ is. Explanation In mathematics, the divergence is often</description></item><item><title>Gibbs' Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2049/</link><pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2049/</guid><description>Theorem Gibbs Inequality describes the relationship between Shannon Entropy and Cross Entropy, ensuring the lower bound of Kullback-Leibler Divergence. Theorem $$ H(P) \le H(P,Q) $$ Proof 1 The proof is provided only for the discrete case, assuming for all $k$ that $p_{k} &amp;gt; 0$. The equation of the tangent line at $x=1$ on the curve $y = \ln x$ is $y = x - 1$. Since the logarithm is a</description></item><item><title>Transport Equation</title><link>https://freshrimpsushi.github.io/en/posts/3080/</link><pubDate>Sat, 10 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3080/</guid><description>Definition1 Below is referred to as a transport equation. $$ \begin{equation} u_{t} + b \cdot Du=0\quad \text{in }\mathbb{R}^n \times (0,\ \infty) \end{equation} $$ $b=(b_{1}, b_2, \cdot, b_{n}) \in \mathbb{R}^n$ is a fixed vector $u=u(x,t)$ is $u:\mathbb{R}^n \times [0,\infty) \rightarrow \mathbb{R}$ $x=(x_{1}, \cdots , x_{n})\in \mathbb{R}^n$ $t \ge 0$ is time $Du=D_{x}u=(u_{x_{1}}, \cdots ,u_{x_{n}})$ is the gradient of $u$ with respect to the spatial variable $x$ Explanation Assume $u \in C^1$</description></item><item><title>Diagonal Matrix as a Function, Diagonal Elements</title><link>https://freshrimpsushi.github.io/en/posts/2048/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2048/</guid><description>Definition Diagonal Elements The $\text{diag} : \mathbb{R}^{n \times n} \to \mathbb{R}^{n}$ for a matrix is defined as the vector consisting of the diagonal elements of the matrix. $$ \text{diag} A = \begin{bmatrix} A_{11} \\ A_{22} \\ \vdots \\ A_{nn} \end{bmatrix} $$ Diagonal Matrix The $\text{diag} : \mathbb{R}^{n} \to \mathbb{R}^{n \times n}$ for a vector is defined as the matrix that has the vector as its diagonal elements. $$ \text{diag} \begin{bmatrix}</description></item><item><title>Every n-dimensional Real Vector Space is Isomorphic to R^n</title><link>https://freshrimpsushi.github.io/en/posts/3079/</link><pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3079/</guid><description>Definition1 Let $V$ and $W$ be called a vector space. If there exists an invertible (bijective) linear transformation $T : V \to W$, then $V$ and $W$ are said to be isomorphic. $T$ is called an isomorphism. Theorem Every $n$-dimentional real vector space is isomorphic to $\mathbb{R}^{n}$. Explanation Another way to express the theorem is as follows. &amp;ldquo;A $\mathbb{R}$-vector space $V$ being isomorphic to $\mathbb{R}^{n}$&amp;rdquo; is equivalent to &amp;ldquo;being $\dim{V}=n$&amp;rdquo;.</description></item><item><title>Relative Entropy, Kullback-Leibler Divergence</title><link>https://freshrimpsushi.github.io/en/posts/2047/</link><pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2047/</guid><description>Buildup When there are two probability distributions $P$ and $Q$, it&amp;rsquo;s easy to imagine situations where one might wonder how different these two are. For instance, consider the situation of guessing exactly what number is captured by a camera. The numbers 6 and 9 can be quite confusing without clear indication of their top and bottom, and since there are people who draw a line below 9 and those who</description></item><item><title>The Matrix Representation of a Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3078/</link><pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3078/</guid><description>Definition1 Let&amp;rsquo;s call $V, W$ a finite-dimensional vector space. Let&amp;rsquo;s call $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ and $\gamma = \left\{ \mathbf{w}_{1}, \dots, \mathbf{w}_{m} \right\}$ the ordered bases for $V$ and $W$, respectively. Let&amp;rsquo;s call $T : V \to W$ a linear transformation. Then, by the uniqueness of the basis representation, there exists a unique scalar $a_{ij}$ satisfying the following. $$ T(\mathbf{v}_{j}) = \sum_{i=1}^{m}a_{ij}\mathbf{w}_{i} = a_{1j}\mathbf{w}_{1} + \cdots +</description></item><item><title>Definition of Complex Numbers</title><link>https://freshrimpsushi.github.io/en/posts/2046/</link><pubDate>Mon, 05 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2046/</guid><description>Definition 1 The solution $x = \sqrt{-1}$ of the quadratic equation $x^{2} +1 = 0$ is called an Imaginary Number. A number in the form of $z = x + iy$ for two real numbers $x,y \in \mathbb{R}$ is called a Complex Number, and is also denoted as $(x,y)$. Here, $\operatorname{Re} (z) = x$ and $\operatorname{Im} (z) = y$ are called the Real Part and Imaginary Part of $z$, respectively.</description></item><item><title>Uniform Convergence and Continuity of Function Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1893/</link><pubDate>Sun, 04 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1893/</guid><description>정리1 거리공간 $E$위에서 함수열 $\left\{ f_{n} \right\}$이 $f$로 균등 수렴한다고 하자. $$ f_{n} \rightrightarrows f $$ $E$의 집적점 $x$에 수열 $A_{n}(x)$</description></item><item><title>Cross Entropy</title><link>https://freshrimpsushi.github.io/en/posts/2045/</link><pubDate>Sat, 03 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2045/</guid><description>Overview Cross Entropy is the average number of bits required to distinguish between two probability distributions, commonly defined between the assumed true (reference) probability distribution $p$ and the estimated (expected) probability distribution $q$. Definition 1 Discrete Given the probability mass functions $p,q$ of two discrete probability distributions. The cross entropy $H (p,q)$ of the two distributions is defined as follows: $$ H (p,q) := - \sum p(x) \log_{2} q(x) $$</description></item><item><title>Back Propagation Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/3077/</link><pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3077/</guid><description>This article is written for math majors to understand the principles of the backpropagation algorithm. Notation Given an artificial neural network like the one shown above. Let $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n_{0}})$ be the input, $y_{j}^{l}$ be the $j$th node of the $l$th layer, $\hat{\mathbf{y}} = (\hat{y}_{1}, \hat{y}_{2}, \dots, \hat{y}_{\hat{n}})$ is the output . Let $L \in \mathbb{N}$ be the number of hidden layers, and the components of $\mathbf{n}=(n_{0},</description></item><item><title>Poincaré Map</title><link>https://freshrimpsushi.github.io/en/posts/2044/</link><pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2044/</guid><description>Definition 1 Let us consider a vector field in Euclidean space $\mathbb{R}^{n}$ and open sets $U \subset \mathbb{R}^{n}$ defined in the function continuous $f : U \to \mathbb{R}^{n}$ as given by the following differential equation. $$ \dot{x} = f(x) $$ Represent the flow as $\phi_t \left( \cdot \right)$ and consider a $\left( n-1 \right)$-dimensional surface $\Sigma$ that intersects the vector field. For an open set $V \subset \Sigma$, we define</description></item><item><title>Properties of the Space of Invertible Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3076/</link><pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3076/</guid><description>Theorem1 Let&amp;rsquo;s call the set of all invertible linear transformations on $\Omega$. $$ \Omega = \left\{ \text{all invertible linear operator on } \mathbb{R}^{n} \right\} $$ (a) If the following holds for $T_{1} \in \Omega$ and $T_{2} \in L(\mathbb{R}^{n})$, then $T_{2} \in \Omega$ is true. $$ \| T_{2} - T_{1} \| \| T_{1}^{-1} \| &amp;lt; 1 $$ Here, $\| T \|$ is the norm of the linear transformation. (b) The following</description></item><item><title>Conditional Entropy</title><link>https://freshrimpsushi.github.io/en/posts/2043/</link><pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2043/</guid><description>Definition 1 A joint probability mass function $p$ or a joint probability density function $f$ is given for the random variable $X_{1}, \cdots , X_{n}$. The conditional entropy of $X_{1}, \cdots , X_{n}$ given that $X_{k}$ is given can be stated as $H \left( X_{1}, \cdots , X_{n} | X_{k} \right)$. Discrete $$ H \left( X_{1}, \cdots , X_{n} | X_{k} \right) := - \sum_{x_{1}} \cdots \sum_{x_{n}} p \left( x_{1}</description></item><item><title>Norm of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3075/</link><pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3075/</guid><description>Definition1 Define the norm of the linear transformation $T \in L(\mathbb{R}^{n}, \mathbb{R}^{m})$ as follows. $$ \begin{equation} \| T \| := \sup \limits_{\| \mathbf{x} \| = 1} \| T(\mathbf{x}) \| \end{equation} $$ Explanation (a) From (a) we have the following equality, so $\| T \|$ can be interpreted as the ratio by which $T$ changes the magnitude of elements of $\mathbb{R}^{n}$ when mapping them into $\mathbb{R}^{m}$. In other words, no matter</description></item><item><title>General Definitions of Lines, Planes, and Spheres</title><link>https://freshrimpsushi.github.io/en/posts/2042/</link><pubDate>Sun, 27 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2042/</guid><description>Definition 1 Let&amp;rsquo;s assume that a vector space $X$ is given. The set of points satisfying the following equation $L \subset X$ or $\alpha (t)$ itself is defined as a Line that passes through point $\mathbf{x}_{0} \in X$ and is parallel to vector $\mathbf{v} \ne 0$. $$ \alpha (t) = \mathbf{x}_{0} + t \mathbf{v} \qquad , t \in \mathbb{R} $$ The set of points satisfying the following equation $P \subset</description></item><item><title>Composition of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3074/</link><pubDate>Sat, 26 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3074/</guid><description>Definition1 Given linear transformations $T_{1} : V \to W$ and $T_{2} : W \to Z$, the transformation defined by $T_{2} T_{1}$ is called the composition of $T_{1}$ and $T_{2}$. $$ (T_{2} \circ T_{1})(\mathbf{x}) = T_{2}\left( T_{1}(\mathbf{x}) \right) \quad \mathbf{x} \in V $$ Explanation The composition of linear transformations is often denoted simply as follows: $$ T_{2}T_{1}\mathbf{x} = (T_{2} \circ T_{1}) (\mathbf{x}) $$ In finite dimensions, this is essentially the same</description></item><item><title>How to Conveniently Print Variable Values in Julia, Interpolation</title><link>https://freshrimpsushi.github.io/en/posts/2041/</link><pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2041/</guid><description>Overview This section explains the convenient feature of interpolation in Julia. By using interpolation effectively, you can make output statements simple and clean, which is very handy. Although it is not related to interpolation in numerical analysis, the meaning of the word is similar. For features related to numerical analysis interpolation, refer to the usage of Interpolations.jl. Code The usage is very simple. You can append the dollar sign $</description></item><item><title>Necessary and Sufficient Conditions for Linear Transformations to be Surjective and Injective</title><link>https://freshrimpsushi.github.io/en/posts/3073/</link><pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3073/</guid><description>Theorem 11 The following two propositions are equivalent concerning a linear transformation $T: V \to W$. $T$ is one-to-one. $N(T) = \text{ker}(T) = \left\{ \mathbf{0} \right\}$ Explanation This means that understanding the kernel of $T$ is a method to determine whether $T$ is one-to-one or not. According to the theorem, a linear transformation being one-to-one is equivalent to the following condition. $$ \mathbf{x} \ne \mathbf{0} \implies T(\mathbf{x}) \ne \mathbf{0} $$</description></item><item><title>Basis Orientation Defined in a Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/2040/</link><pubDate>Wed, 23 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2040/</guid><description>Definition 1 $$ U = \left\{ \mathbf{u}_{1}, \cdots, \mathbf{u}_{n} \right\} \\ V = \left\{ \mathbf{v}_{1}, \cdots, \mathbf{v}_{n} \right\} $$ Let $U,V$ be a basis of the vector space $X$, and define the matrix $\left( a_{ij} \right) \in \mathbb{C}^{n \times n}$ such that the following equation is satisfied. $$ \mathbf{v}_{j} = \sum_{i=1}^{n} a_{ij} \mathbf{u}_{i} $$ If $\det \left( a_{ij} \right) &amp;gt; 0$ then $U,V$ is said to have the same Orientation.</description></item><item><title>Rank, Nullity, and Dimension Theorems of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3072/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3072/</guid><description>Definition1 Let $T : V \to W$ be a linear transformation. If the range $R(T)$ of $T$ is finite-dimensional, the dimension of $R(T)$ is called the rank of $T$, denoted by: $$ \mathrm{rank}(T) := \dim (R(T)) $$ If the null space $N(T)$ of $T$ is finite-dimensional, the dimension of $N(T)$ is called the nullity of $T$, denoted by: $$ \mathrm{nullity}(T) := \dim\left( N(T) \right) $$ Explanation This is a generalization</description></item><item><title>Ceiling Function and Floor Function</title><link>https://freshrimpsushi.github.io/en/posts/2039/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2039/</guid><description>Definition 1 The Ceiling function $\lceil \cdot \rceil : \mathbb{R} \to \mathbb{Z}$ and the Floor function $\lfloor \cdot \rfloor : \mathbb{R} \to \mathbb{Z}$ are defined as follows. $$ \lceil x \rceil := \min \left\{ n \in \mathbb{Z} : x \le n \right\} \\ \lfloor x \rfloor := \max \left\{ n \in \mathbb{Z} : n \le x \right\} $$ Description In domestic terms, the Floor function $\lfloor \cdot \rfloor$ is also</description></item><item><title>Kernel and Range of Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3071/</link><pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3071/</guid><description>Definition1 Let&amp;rsquo;s say $T : V \to W$ is a linear transformation. The set of elements of $V$ that are mapped to $\mathbf{0}$ by $T$ is called the kernel or null space, and is denoted as follows. $$ \text{ker}(T) = N(T) := \left\{ \mathbf{v} \in V : T( \mathbf{v} ) = \mathbf{0} \right\} $$ The set of images under $\mathbf{v} \in V$ by $T$ is called the range or image</description></item><item><title>Definition of General Angles and Perpendicularity</title><link>https://freshrimpsushi.github.io/en/posts/2038/</link><pubDate>Sat, 19 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2038/</guid><description>Definition 1 Let&amp;rsquo;s say $V$ is a vector space. For two vectors $\mathbf{u}, \mathbf{v} \in V$, $\theta$ is defined as the angle between two vectors if it satisfies the following. $$ \cos \theta = {{ \left&amp;lt; \mathbf{u}, \mathbf{v} \right&amp;gt; } \over { \left| \mathbf{u} \right| \left| \mathbf{v} \right| }} $$ If two vectors $\mathbf{u}, \mathbf{v}$ satisfy $\left&amp;lt; \mathbf{u}, \mathbf{v} \right&amp;gt; = 0$, then $\mathbf{u}$ is said to be orthogonal or</description></item><item><title>The Basis of the Domain Generates the Image of the Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3070/</link><pubDate>Fri, 18 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3070/</guid><description>Theorem1 Let&amp;rsquo;s suppose we have a given linear transformation $T : V \to W$. Assume $V$ is finite-dimensional, and let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ be a basis of $V$. Then, the image of any $\mathbf{v} \in V$ can be represented as follows. $$ T(\mathbf{v}) = c_{1}T(\mathbf{v}_{1}) + c_{2}T(\mathbf{v}_{2}) + \cdots c_{n}T(\mathbf{v}_{n}) $$ Here, $c_{i}$ are coefficients that satisfy $\mathbf{v} = \sum c_{i}\mathbf{v}_{i}$. In other words, $\left\{</description></item><item><title>Joint Entropy</title><link>https://freshrimpsushi.github.io/en/posts/2037/</link><pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2037/</guid><description>Definition Let us assume we have the joint probability mass function $X_{1}, \cdots , X_{n}$ or joint probability density function $f$ given for random variables. Discrete $$ H \left( X_{1}, \cdots , X_{n} \right) := - \sum_{x_{1}} \cdots \sum_{x_{n}} p \left( x_{1} , \cdots , x_{n} \right) \log_{2} p \left( x_{1} , \cdots , x_{n} \right) $$ Continuous $$ H \left( X_{1}, \cdots , X_{n} \right) := - \int_{\mathbb{R}} \cdots</description></item><item><title>Distributional Convolution Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1895/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1895/</guid><description>Theorem1 Let&amp;rsquo;s say $\phi$ is a test function that satisfies $\int_{\mathbb{R}^{n}}\phi (\mathbf{x})d\mathbf{x}=1$. And let $\phi_{\epsilon}(\mathbf{x})=\epsilon^{-n}\phi (\epsilon^{-1}\mathbf{x})$ be given. Then, for any distribution $F$ and regular distribution $T_{F*\phi_{\epsilon}}$, when $\epsilon \to 0$, $T_{F*\phi_{\epsilon}}$ converges to $F$. $$ T_{F * \phi_{\epsilon}} \overset{\text{w}}{\to} F\quad \text{as } \epsilon \to 0 $$ Description The name &amp;lsquo;The Convolution Convergence Theorem for Distributions&amp;rsquo; is arbitrarily given as there was no specific name attached to the content above.</description></item><item><title>Using Julia in Windows CMD and PowerShell</title><link>https://freshrimpsushi.github.io/en/posts/2036/</link><pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2036/</guid><description>Guide Step 0. Install julia 1.6 or higher From version 1.6, you can add it to the environment variables during the installation process. Just check the indicated option and install. If using an older version, either install version 1.6 or higher, or follow the instructions below. Step 1. Check the Julia installation path Check the installation path of Julia. If you haven&amp;rsquo;t altered anything, it should be stored in the</description></item><item><title>Discrete Fourier Inversion</title><link>https://freshrimpsushi.github.io/en/posts/3069/</link><pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3069/</guid><description>Formula1 Let&amp;rsquo;s denote the Discrete Fourier Transform of $\mathbf{a} = (a_{0}, a_{1}, \dots, a_{N-1}) \in \mathbb{C}^{N}$ as $\hat{\mathbf{a}} = (\hat{a}_{0}, \hat{a}_{1}, \dots, \hat{a}_{N-1}) \in \mathbb{C}^{N}$. $$ \mathcal{F}_{N}(\mathbf{a}) = \hat{\mathbf{a}},\quad \hat{a}_{m}=\sum_{n=0}^{N-1}e^{-i2\pi mn /N}a_{n} $$ Then, the following holds. $$ a_{n} = \dfrac{1}{N} \sum \limits_{m=0}^{N-1} e^{i 2 \pi m n / N} \hat{a}_{m} $$ Explanation This is known as the inverse formula for discrete Fourier transform. Proof Lemma For $m = 0,</description></item><item><title>Shannon Entropy: Entropy Defined by Random Variables</title><link>https://freshrimpsushi.github.io/en/posts/2035/</link><pubDate>Sun, 13 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2035/</guid><description>Overview Shannon Entropy or Information Entropy is a measure of disorder defined by a probability variable, and can be viewed as a quantification of how uncertain it is in a probability distribution. Easy and Complex Definitions Discrete Entropy 1 When the probability mass function of a discrete random variable $X$ is $p(x)$, the entropy of $X$ is represented as follows. $$ H(X) := - \sum p(x) \log_{2} p(x) $$ Continuous</description></item><item><title>Fisher Information</title><link>https://freshrimpsushi.github.io/en/posts/2034/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2034/</guid><description>Buildup Score Function Consider a random variable $X$ whose probability density function is $f \left( x ; \theta \right)$ for a parameter $\theta \in \Theta$. The estimator that maximizes the log-likelihood function, known as the maximum likelihood estimator, can be found as $\widehat{\theta}$, which satisfies the following partial differential equation. $$ \sum_{k=1}^{n} {{ \partial \log f \left( x_{k} ; \theta \right) } \over { \partial \theta }} = 0 $$</description></item><item><title>Shannon Information: Information Defined by Probability Theory</title><link>https://freshrimpsushi.github.io/en/posts/2033/</link><pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2033/</guid><description>Buildup Card Matching Game Imagine Alice and Bob are betting on which one of the 52 cards in a deck (excluding Jokers) they have drawn facedown. Alice: The card I drew is not a Joker. Bob frowns immediately upon hearing this. While it&amp;rsquo;s technically true, it&amp;rsquo;s such an obvious statement that it holds no meaningful insight for the bet. A discussion on the stakes seems necessary before proceeding with the</description></item><item><title>Inter-Species Transmission Model: Disease Spread among Three Populations</title><link>https://freshrimpsushi.github.io/en/posts/2032/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2032/</guid><description>Overview Species Barrier refers to the phenomenon where it&amp;rsquo;s difficult for a pathogen to transmit from its original host to another species. The event where a disease crosses this barrier is called Cross-species Transmission, for which we introduce the Host-vector-host model as a mathematical representation. Model 1 $$ \begin{align*} \\ {{d I_{1}} \over {d t}} =&amp;amp; \beta_{12} I_{2} S_{1} - \gamma I_{1} \\ {{d I_{2}} \over {d t}} =&amp;amp; \left(</description></item><item><title>Optimal Solution: Maximum and Minimum Factors</title><link>https://freshrimpsushi.github.io/en/posts/2031/</link><pubDate>Sat, 05 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2031/</guid><description>Difficult Definitions Let&amp;rsquo;s assume an arbitrary set $X$ and a totally ordered set $\left( Y, \le \right)$ are given. For a subset $S \subset X$ of $X$, the argument of maxima $\argmax_{S} : Y^{X} \to 2^{X}$ and argument of minima $\argmin_{S} : Y^{X} \to 2^{X}$ of the function $f : X \to Y$ are defined as follows. $$ \argmax_{S} f := \left\{ x_{\ast} \in S : f \left( x_{\ast} \right)</description></item><item><title>Gas Flux</title><link>https://freshrimpsushi.github.io/en/posts/3063/</link><pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3063/</guid><description>Definition1 In physics, flux is the number of particles (or a physical quantity such as energy, momentum, etc.) that pass through (collide with) a unit area per unit time. Flux is often denoted by the capital letter Phi (Φ) $\Phi$. $$ \Phi = \dfrac{\text{physical quantity}}{\text{area} \times \text{time}} $$ Explanation According to the definition, the flux of a gas refers to the number of gas molecules passing through a</description></item><item><title>Sexually Transmitted Diseases Model: Disease Transmission between Two Populations</title><link>https://freshrimpsushi.github.io/en/posts/2030/</link><pubDate>Thu, 03 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2030/</guid><description>Overview Cooke and Yorke&amp;rsquo;s proposed mathematical model for the spread of sexually transmitted diseases is explored. In references, gonorrhea is considered as a specific example of a sexually transmitted disease. Model 1 $$ \begin{align*} {{d S_{1}} \over {d t}} =&amp;amp; - \beta_{12} S_{1} I_{2} + \gamma_{1} I_{1} \\ {{d I_{1}} \over {d t}} =&amp;amp; \beta_{12} S_{1} I_{2} - \gamma_{1} I_{1} \\ {{d S_{2}} \over {d t}} =&amp;amp; - \beta_{21} S_{2}</description></item><item><title>Dalton's Law</title><link>https://freshrimpsushi.github.io/en/posts/3062/</link><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3062/</guid><description>Law1 $n$ Let&amp;rsquo;s consider a mixture of gases composed of ▷eq.◁ different types of molecules. The total pressure of this gas mixture is equal to the sum of the pressures of each individual gas. $$ p = \sum \limits_{i=1}^{n}N_{i} k_{B} T = \sum \limits_{i=1}^{n}p_{i} $$ Explanation This law was named after the British chemist John Dalton (1766~1844). Stephen J. Blundell and Katherine M. Blundell, Concepts in</description></item><item><title>Regularity Conditions in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2029/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2029/</guid><description>Overview In subjects that utilize mathematics, the term Regularity Conditions usually refers to conditions that allow for a wide range of applications and make theoretical developments more comfortable. In mathematical statistics, they are as follows. Assumptions 1 Consider a random variable $X$ with probability density function $f \left( x ; \theta \right)$ for a parameter $\theta \in \Theta$. The random sample $X_{1} , \cdots , X_{n}$ drawn iid from the</description></item><item><title>Solid Angle of a Sphere</title><link>https://freshrimpsushi.github.io/en/posts/3061/</link><pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3061/</guid><description>Definition1 A solid angle $\Omega$ of a 3-dimensional sector with a radius of $r$ and a surface area of $A$ is defined as follows: $$ \Omega := \dfrac{A}{r^{2}} $$ The unit is called steradian and is denoted as $\mathrm{sr}$. Explanation Considering how the radian angle in a circle is defined as the ratio of the arc length to the radius, this definition seems natural. $$ \theta := \dfrac{s}{r} $$ However,</description></item><item><title>SIS Model: Reinfection and Chronic Disease</title><link>https://freshrimpsushi.github.io/en/posts/2028/</link><pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2028/</guid><description>Overview The SIS model is one that does not take into account immunity or indifference in the spread of contagions or information. It is primarily applied to diseases that are endemic rather than epidemic, such as colds, influenza, sexually transmitted infections, and malaria. Model 1 $$ \begin{align*} {{d S} \over {d t}} =&amp;amp; - {{ \beta } \over { N }} I S + \gamma I \\ {{d I} \over</description></item><item><title>Derivation of the Ideal Gas Equation through Kinetic Theory of Gases</title><link>https://freshrimpsushi.github.io/en/posts/3060/</link><pubDate>Sat, 29 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3060/</guid><description>Definition[^1] Pressure $p$ of a fluid acting on an area $M$ is defined as the ratio of the force $F$ the fluid exerts perpendicularly on area $M$ to the area $M$ itself. $$ p:=\frac{F}{M} \left[ \mathrm{N/m^{2}} \right] $$ Formula Let the volume of the gas be $V$, the temperature be $T$, and the number of molecules be $N$. Then, the pressure $p$ of the gas satisfies the following equation. $$</description></item><item><title>Optimal Value: Maximum and Minimum</title><link>https://freshrimpsushi.github.io/en/posts/2027/</link><pubDate>Fri, 28 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2027/</guid><description>Simple Definitions Maximum and Minimum collectively are called the Optimum. In the set $X$, the largest element is denoted as the maximum $\max X$, and the smallest element as the minimum $\min X$. For the function $f : X \to \mathbb{R}$, the largest function value is denoted as $\max_{X} f$, and the smallest function value as $\min_{X} f$. $\mathbb{R}$ denotes the entire set of real numbers. Maximum and Minimum are</description></item><item><title>Expectation Value of Speed and Velocity of Gas Molecules</title><link>https://freshrimpsushi.github.io/en/posts/3059/</link><pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3059/</guid><description>Formulas1 Let&amp;rsquo;s denote the velocity of gas molecules as $\mathbf{v} = (v_{x}, v_{y}, v_{z})$ and their speed as $v = | \mathbf{v} |$. The expected values of velocity and speed of gas molecules are as follows. $$ \begin{align*} \left\langle v_{x} \right\rangle &amp;amp;= 0 \\ \left\langle |v_{x}| \right\rangle &amp;amp;= \sqrt{\dfrac{2 k_{B} T}{\pi m}} \\ \left\langle v_{x} ^{2} \right\rangle &amp;amp;= \dfrac{k_{B} T}{\pi m} \\ \left\langle v \right\rangle &amp;amp;= \sqrt{\dfrac{8 k_{B} T}{\pi m}}</description></item><item><title>Maximum Likelihood Estimator</title><link>https://freshrimpsushi.github.io/en/posts/2026/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2026/</guid><description>Buildup Consider a random variable $X$ with a probability density function (pdf) $f \left( x ; \theta \right)$ for parameter $\theta \in \Theta$. A random sample $X_{1} , \cdots , X_{n}$ drawn identically and independently (iid) from the same distribution as $X$ has the same pdf $f(x ; \theta)$ and realization $\mathbf{x} := \left( x_{1} , \cdots , x_{n} \right)$. The function $L$ defined for this is called the Likelihood</description></item><item><title>Discrete Fourier Transform Properties</title><link>https://freshrimpsushi.github.io/en/posts/3058/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3058/</guid><description>Properties1 Let&amp;rsquo;s denote the Discrete Fourier Transform as $\mathscr{F}_{N}$ or $\hat{\mathbf{a}}$ given $\mathbf{a} \in \mathbb{C}^{N}$. Convolution $$ \mathscr{F}_{N}(\mathbf{a} \ast \mathbf{b}) = \hat{\mathbf{a}} \hat{\mathbf{b}} = (\hat{a}_{0}\hat{b}_{0}, \dots, \hat{a}_{N-1}\hat{b}_{N-1}) $$ In this case, $\ast$ is the discrete convolution. Explanation The Discrete Fourier Transform also satisfies the properties that the Fourier Transform does. Gerald B. Folland, Fourier Analysis and Its Applications (1992), p251&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>SIR Model: The Most Basic Diffusion Model</title><link>https://freshrimpsushi.github.io/en/posts/2025/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2025/</guid><description>Overview The SIR model is one of the simplest and most widely varied compartmental models in epidemiology, offering a straightforward and intuitive explanation of the spread of diseases or information. Model 1 $$ \begin{align*} {{d S} \over {d t}} =&amp;amp; - {{ \beta } \over { N }} I S \\ {{d I} \over {d t}} =&amp;amp; {{ \beta } \over { N }} S I - \mu I \\</description></item><item><title>Convergence in Schwartz Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1897/</link><pubDate>Sun, 23 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1897/</guid><description>Definition Let us assume that a sequence in the Schwartz space is denoted by $\left\{ \phi_{n} \right\}$. If for all multi-indexes $\alpha$, $\beta$, the sequence $\left\{ \mathbf{x}^{\beta}D^{\alpha}\phi_{n}(\mathbf{x}) \right\}$ converges uniformly to $0$, then we define that $\left\{ \phi_{n} \right\}$ converges to $0$ and denote it as follows. $$ \phi_{n} \overset{\mathcal{S}}{\to} 0 $$ Explanation By generalizing the above definition, if $\left\{ \phi_{n}-\phi \right\}$ converges to $0$, we can say that $\left\{</description></item><item><title>Metaprogramming in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2024/</link><pubDate>Sat, 22 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2024/</guid><description>Code 1 Julia supports meta programming at the language level. The following is the result of reading and executing a string as code itself. julia&amp;gt; text = &amp;#34;f(x) = 2x + 1; f(2)&amp;#34; &amp;#34;f(x) = 2x + 1; f(2)&amp;#34; julia&amp;gt; code = Meta.parse(text) :($(Expr(:toplevel, :(f(x) = begin #= none:1 =# 2x + 1 end), :(f(2))))) julia&amp;gt; eval(code) 5 Meta.Parse(): This function converts the input string into an expression and returns</description></item><item><title>Picard's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3057/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3057/</guid><description>Buildup1 Consider the following ODE system. $$ \begin{equation} \begin{aligned} x_{1}^{\prime}(t) =&amp;amp;\ F_{1}(t,x_{1},x_{2},\cdots,x_{n}) \\ x_{2}^{\prime}(t) =&amp;amp;\ F_{2}(t,x_{1},x_{2},\cdots,x_{n}) \\ \vdots &amp;amp; \\ x_{n}^{\prime}(t) =&amp;amp;\ F_{n}(t,x_{1},x_{2},\cdots,x_{n}) \end{aligned} \end{equation} $$ Assume the values of $x_{i}$ are as follows when $t=t_{0}$. $$ \begin{equation} x_{1}(t_{0}) = x_{1}^{0}, x_{2}(t_{0}) = x_{2}^{0}, \dots, x_{n}(t_{0}) = x_{n}^{0} \end{equation} $$ Combining $(1)$ and $(2)$ into an initial value problem of a system of first-order differential equations, and finding the solution</description></item><item><title>What is the Basic Reproduction Number in Epidemic Spread Models?</title><link>https://freshrimpsushi.github.io/en/posts/2023/</link><pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2023/</guid><description>Definition Basic Reproduction Number $\mathcal{R}_{0}$ represents the speed at which an epidemic spreads, and is essentially the expected number of people that one infected person will infect. Epidemiological Compartment Models 1 In a dynamical system expressed by differential equations, the largest real part of the eigenvalue of the Jacobian matrix is referred to as $\mathcal{R}_{0}$. Explanation Although the definition alone may seem difficult to understand, thinking about it in terms</description></item><item><title>Proof of Heisenberg's Uncertainty Principle</title><link>https://freshrimpsushi.github.io/en/posts/3056/</link><pubDate>Wed, 19 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3056/</guid><description>정리1 두 연산자 $A$와 $B$에 대하여 다음이 성립한다. $$ \sigma_{A}^{2}\sigma_{B}^{2} \ge \left( \dfrac{1}{2\i} \braket{[A, B]} \right)^{2} $$ 이때 $\sigma_{A}^{2}$는 $A$의 분산, $[A, B]$는 $A$와</description></item><item><title>How to Flatten an Array in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2022/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2022/</guid><description>Code Use the vec() function. julia&amp;gt; A = rand(0:9, 3,4) 3×4 Array{Int64,2}: 6 8 7 3 2 9 3 2 5 0 6 7 julia&amp;gt; vec(A) 12-element Array{Int64,1}: 6 2 5 8 9 0 7 3 6 3 2 7 To the human eye, it appears the same as a 1-dimensional array, but it&amp;rsquo;s actually a 2-dimensional array by type, which can cause errors. This method can solve</description></item><item><title>Heisenberg Uncertainty Principle</title><link>https://freshrimpsushi.github.io/en/posts/3055/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3055/</guid><description>Buildup There exists a special relationship between $f$ and its Fourier transform $\hat{f}$. If for some constant $\Omega$, $\hat{f} (\omega) = 0\ for\ | \omega | \ge \Omega$ holds, then it is not possible for $f$ to exhibit the same property. In other words, it indicates that both $f$ and $\hat{f}$ cannot be concentrated in a narrow location simultaneously, mathematically speaking, this means $f$ and $\hat{f}$ cannot both have a</description></item><item><title>Consistent Estimator</title><link>https://freshrimpsushi.github.io/en/posts/2021/</link><pubDate>Sun, 16 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2021/</guid><description>Definition 1 Let the random variable $X$ have a cumulative distribution function $F ( x ; \theta), \theta \in \Theta$. When $X_{1} , \cdots , X_{n}$ is drawn from $X$, the statistic $T_{n}$ satisfies the following for the parameter $\theta$, it is said to be a Consistent Estimator. $$ T_{n} \overset{P}{\to} \theta \quad \text{as } n \to \infty $$ $\overset{P}{\to}$ is probabilistic convergence. Explanation If the unbiased estimator discusses the</description></item><item><title>Properties of Divergent Real Sequences</title><link>https://freshrimpsushi.github.io/en/posts/3052/</link><pubDate>Sat, 15 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3052/</guid><description>Summary1 Let $\left\{ x_{n} \right\}$, $\left\{ y_{n} \right\}$ be real sequences and let $\lim \limits_{n\to\infty} x_{n}=\infty(-\infty)$. Then the following hold: (a) If $\left\{ y_{n} \right\}$ is bounded below (bounded above), then $\lim \limits_{n\to\infty}(x_{n}+y_{n}) = \infty(-\infty)$. (b) $\forall \alpha &amp;gt; 0,\quad \lim \limits_{n\to\infty} \alpha x_{n} = \infty (-\infty)$. (c) If for every $n\in \mathbb{N}$, there exists a $M_{0} &amp;gt;0$ such that $y_{n} &amp;gt; M_{0}$, then $\lim \limits_{n\to\infty} x_{n}y_{n} = \infty(-\infty)$. (d)</description></item><item><title>Optimizing Distance Matrix Calculations in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2020/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2020/</guid><description>Conclusion Let&amp;rsquo;s calculate the distance between $n$ coordinates. If it&amp;rsquo;s not necessary to calculate the distance between all coordinates, divide them into groups and create a rectangular distance matrix. The rectangular distance matrix can be calculated quickly and easily with the pairwise() function. Speed Comparison Let&amp;rsquo;s imagine doing a moving agent-based simulation for the SIR model. The original time complexity is $O \left( n^{2} \right)$, but if you divide it</description></item><item><title>Sampling Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3054/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3054/</guid><description>Buildup[^1] Consider a physical signal $f$ being measured over time $t_{1} &amp;lt; t_{2} &amp;lt; t_{3} &amp;lt; \cdots$. Even if we know $f(t_{1}), f(t_{2}), \dots$, we generally cannot know the values for arbitrary $f(t)$. However, let&amp;rsquo;s assume that signal $f$ contains only frequencies within a certain range. That is, we consider a signal $f$ that contains only frequencies smaller than some constant $\Omega$, which is referred to as a band-limited signal.</description></item><item><title>Dynamics Compartment Model</title><link>https://freshrimpsushi.github.io/en/posts/2019/</link><pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2019/</guid><description>Overview 1 Compartmental models in epidemiology serve as models for epidemic outbreaks, incorporating infectious diseases into population dynamics and dividing the &amp;lsquo;population&amp;rsquo; into several compartments. Epidemiology is the study of epidemics, unrelated to the mechanics discussed at shrimp sushi restaurants. Description Ever since Kermack and McKendrick devised what&amp;rsquo;s known as the SIR model, there have been numerous modifications and developments. All models that originate from this idea are essentially considered</description></item><item><title>Limits of Exponential and Logarithmic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3053/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3053/</guid><description>Formulas Exponential functions and logarithmic functions satisfy the following equations. $$ \begin{equation} \lim \limits_{x \to 0} \dfrac{\log (x + 1) }{x} = 1 \end{equation} $$ $$ \begin{equation} \lim \limits_{x \to 0} \dfrac{ e^{x} - 1}{x} = 1 \end{equation} $$ Proof $(1)$ $$ \begin{align*} \lim \limits_{x \to 0} \dfrac{\log (x + 1) }{x} &amp;amp;= \lim \limits_{x \to 0} \dfrac{1}{x} \log ( x + 1) \\ &amp;amp;= \lim \limits_{x \to 0} \log</description></item><item><title>How to Weight and Random Sample in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2018/</link><pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2018/</guid><description>Overview The usage of the function sample() which serves a similar role to R&amp;rsquo;s sample() or Python package numpy&amp;rsquo;s random.choice(), and the Weights function in Julia. Code 1 using StatsBase items = 0:5 weights = 0:5 sample(items, Weights(weights)) # With replacement my_samps = sample(items, Weights(weights), 10) # Without replacement my_samps = sample(items, Weights(weights), 2, replace=false) Execution Result julia&amp;gt; using StatsBase julia&amp;gt; items = 0:5 0:5 julia&amp;gt; weights = 0:5 0:5</description></item><item><title>Monotone Sequence and Monotone Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3051/</link><pubDate>Sun, 09 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3051/</guid><description>Definition1 For the sequence of real numbers $\left\{ s_{n} \right\}$, If $s_{n} \le s_{n+1}$ holds, it is called monotonically increasing. If $s_{n} \ge s_{n+1}$ holds, it is called monotonically decreasing. A sequence that is either monotonically increasing or monotonically decreasing is called monotonic. If $s_{n} \lt s_{n+1}$ holds, it is called an increasing sequence. If $s_{n} \gt s_{n+1}$ holds, it is called a decreasing sequence. Explanation Since a sequence is</description></item><item><title>Smith-Waterman Alignment: Local Sequence Alignment</title><link>https://freshrimpsushi.github.io/en/posts/2017/</link><pubDate>Sat, 08 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2017/</guid><description>Overview Finding the alignment of the most similar parts from two base sequences is called local alignment, and the most widely used method for this is introducing the Smith-Waterman Algorithm. There are so many possible cases in Sequence Alignment, so there is a need to calculate efficiently and quickly through Dynamic Programming. Algorithm1 Input Given two strings $\textbf{v}, \textbf{w}$ and Substitution Matrix $\delta$, let&amp;rsquo;s say they are displayed as follows.</description></item><item><title>Integration of Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/1902/</link><pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1902/</guid><description>Definition1 Let $I^{k}$ be a k-cell, and assume $\mathbf{x} \in I^{k}$. $$ \mathbf{x} = (x_{1},\dots,x_{k}),\quad a_{i} \le x_{i} \le b_{i} (i=1,\dots,k) $$ Suppose $f: I^{k} \to \mathbb{R}$ is continuous. Then, since it is integrable, let us set it as $f=f_{k}$, and define $f_{k-1} : I^{k-1} \to \mathbb{R}$ as follows: $$ f_{k-1} (x_{1}, \dots, x_{k-1}) = \int_{a_{k}}^{b_{k}} f_{k}(x_{1}, \dots, x_{k}) dx_{k} $$ Then, by the Leibniz Rule, $f_{k-1}$ is continuous in</description></item><item><title>Comparison of the Speed of the Equality Operator == for Characters and Integers in Julia</title><link>https://freshrimpsushi.github.io/en/posts/2016/</link><pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2016/</guid><description>Conclusion Comparing each element in an array using the Equal Operator == shows that Char is faster than integers. Speed Comparison julia&amp;gt; integer = rand(1:5, N); print(typeof(integer)) Array{Int64,1} julia&amp;gt; character = rand([&amp;#39;S&amp;#39;,&amp;#39;E&amp;#39;,&amp;#39;I&amp;#39;,&amp;#39;R&amp;#39;,&amp;#39;D&amp;#39;], N); print(typeof(character)) Array{Char,1} julia&amp;gt; @time integer .== 1; 0.009222 seconds (6 allocations: 1.196 MiB) julia&amp;gt; @time character .== &amp;#39;S&amp;#39;; 0.005266 seconds (7 allocations: 1.196 MiB) The above code identifies where 1 and S are located in an array</description></item><item><title>Convolution of Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/1903/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1903/</guid><description>Definition Let&amp;rsquo;s say we have $f,g:\mathbb{R}^{n}\to \mathbb{C}$ and $\mathbf{x},\mathbf{y} \in \mathbb{R}^{n}$. Then the convolution of these two multivariable functions is as follows: $$ f \ast g(\mathbf{x})=\int f(\mathbf{y})g(\mathbf{x}-\mathbf{y})d\mathbf{y} $$ In this case, the integral mentioned above is the integral of a multivariable function. Properties The convolution of multivariable functions also satisfies the same desirable properties as the convolution of single-variable functions. (a) Commutative Law $$ f \ast g = g \ast</description></item><item><title>Approximation of Normal Distribution Variance Stabilization from a Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2015/</link><pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2015/</guid><description>Example 1 If $Y = Y_{n}$ follows a binomial distribution $\text{Bin} (n,p)$, $$ \arcsin \sqrt{ {{ Y } \over { n }} } \overset{D}{\to} N \left( \arcsin \sqrt{p} , n/4 \right) $$ $N \left( \mu , \sigma^{2} \right)$ refers to the normal distribution. $\overset{D}{\to}$ refers to convergence in distribution. Explanation The binomial distribution $\text{Bin} (n, p )$ converges to the normal distribution $N \left( np, np(1-p) \right)$ when $n \to</description></item><item><title>Weighted Lp Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1856/</link><pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1856/</guid><description>Definition1 A function space defined as follows is called a weighted $L^{p}$ space or specifically a $w$-weighted $L^{p}$ space. $$ L_{w}^{p}(a,b):= \left\{ f : \mathbb{R}\to \mathbb{C}\ \big|\ \int_{a}^{b} \left| f(x) \right|^{p}w(x)dx &amp;lt;\infty \right\} $$ Here, $w:\mathbb{R}\to[0,\infty)$ is called a weight function. Description It is one of the spaces that generalizes the $L^{p}$ space. When it is $w(x)=1$, $L_{w}^{p}=L^{p}$ holds. The norm of the weighted $L^{p}$ space is defined as follows</description></item><item><title>Row-major and Column-major of 2D Arrays</title><link>https://freshrimpsushi.github.io/en/posts/2014/</link><pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2014/</guid><description>Overview 1 This document explains row-major and column-major order of matrices or 2-dimensional arrays. Simply put, whether row-major or column-major, it&amp;rsquo;s about the preferred direction of reading through an array. Differences According to Wikipedia, some programming languages and libraries are designed with their native preference for either row or column-major order as follows: Row-major: A method where the rows of a matrix are accessed first, reading from left to right.</description></item><item><title>The Definition of Euler's Constant, the Natural Number e</title><link>https://freshrimpsushi.github.io/en/posts/3050/</link><pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3050/</guid><description>Definition1 The limit of the series below is defined as constant $e$. $$ e: = \sum \limits_{n=0}^{\infty} \dfrac{1}{n!} $$ Explanation Even if we do not know what that value is right away, it can be easily shown that the series mentioned above converges to some limit. The partial sum $s_{n}$, as it is bounded and increasing, converges. $$ \begin{align*} s_{n} &amp;amp;= 1 + 1 + \dfrac{1}{2} + \dfrac{1}{2 \cdot 3}</description></item><item><title>Student's t-test Proof</title><link>https://freshrimpsushi.github.io/en/posts/203/</link><pubDate>Fri, 30 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/203/</guid><description>Theorem 1 If random variables $X_{1} , \cdots , X_{n}$ are iid and follow a normal distribution $N\left( \mu,\sigma^{2} \right)$, then (a): $$ \overline{X} \sim N\left( \mu , { {\sigma^2} \over {n} } \right) $$ (b): $$ \overline{X} \perp S^2 $$ (c): $$ (n-1) { {S^2} \over {\sigma^2} } \sim \chi^2 (n-1) $$ (d): $$ T = { {\overline{X} - \mu } \over {S / \sqrt{n}} } \sim t(n-1) $$</description></item><item><title>Orthogonal Basis and Its Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/3040/</link><pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3040/</guid><description>Definition1 An inner product space basis $V$ that is an orthogonal set is called an orthogonal basis. If $S$ is an orthonormal set, it is called an orthonormal basis. Theorem If $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ is an orthogonal basis of the inner product space $V$, and let $\mathbf{u} \in V$. Then, the following equation holds. $$ \begin{equation} \begin{aligned} \mathbf{u} &amp;amp;= \dfrac{\langle \mathbf{u}, \mathbf{v}_{1} \rangle}{\| \mathbf{v}_{1} \|^{2}}</description></item><item><title>RGB Color Cheat Sheet</title><link>https://freshrimpsushi.github.io/en/posts/2013/</link><pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2013/</guid><description>Overview A commonly used RGB color palette. Code</description></item><item><title>Basis Addition/Subtraction Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3028/</link><pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3028/</guid><description>Theorem1 Let $S$ be a non-empty subset of vector space $V$. (a) If $S$ is linearly independent and if $\mathbf{v} \in V$ equals $\mathbf{v} \notin \text{span}(S)$, then $S \cup \left\{ \mathbf{v} \right\}$ remains linearly independent. (b) If $\mathbf{v} \in S$ can be represented as a linear combination of other vectors in $S$, then $S$ and $S \setminus \left\{ \mathbf{v} \right\}$ span the same space. That is, the following holds: $$</description></item><item><title>Needleman-Wunsch Algorithm: Global Sequence Alignment</title><link>https://freshrimpsushi.github.io/en/posts/2012/</link><pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2012/</guid><description>Overview Finding an alignment between two sequences that maximizes the number of common parts is called Global Alignment, and the most widely used method for this is the Needleman-Wunsch Algorithm. There are too many possible alignments in sequence alignment, so it&amp;rsquo;s necessary to compute them efficiently and quickly through dynamic programming. Algorithm1 Input Given two strings $\textbf{v}, \textbf{w}$ and a substitution matrix $\delta$, let&amp;rsquo;s represent them as follows. $$ \textbf{v}</description></item><item><title>Differentiation of Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/3049/</link><pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3049/</guid><description>Formulas The derivative of the exponential function is as follows. $$ \begin{equation} \dfrac{d e^{x}}{dx} = e^{x} \label{fml1} \end{equation} $$ The derivative of the exponential composite function is as follows. $$ \begin{equation} \dfrac{d \left( e^{f(x)} \right)}{dx} = f^{\prime}(x)e^{f(x)} \label{fml2} \end{equation} $$ Description The exponential function is the only function that is equal to its own derivative. Derivation (1) Using the definition of the derivative, the calculation is as follows. $$ \begin{align*}</description></item><item><title>Pseudo Inverse Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2011/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2011/</guid><description>Overview The Pseudoinverse Matrix generalizes the concept of an inverse matrix to matrices $A \in \mathbb{R}^{m \times n}$ that are not square matrices because their rows and columns are not of the same size. It refers to a matrix that acts as an &amp;rsquo;effective&amp;rsquo; inverse matrix for them. If a matrix transformation $T_{A} : \mathcal{N} (A) \to \mathcal{C} (A)$ satisfies $$ T_{A} \mathbf{x} = A \mathbf{x} $$ for all $\mathbf{x}</description></item><item><title>Relationship Between Orthogonality and Linear Independence</title><link>https://freshrimpsushi.github.io/en/posts/3045/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3045/</guid><description>Definition1 An inner product space $V$&amp;rsquo;s two vectors $\mathbf{u}, \mathbf{v}$ are said to be orthogonal if they satisfy $\langle \mathbf{u}, \mathbf{v} \rangle = 0$. A set made up of elements of $V$ where each element is orthogonal to every other element is called an orthogonal set. If the norm of every element in an orthogonal set is $1$, then it is called an orthonormal set. Theorem A subset $S =</description></item><item><title>Simultaneous Firing Combat Model</title><link>https://freshrimpsushi.github.io/en/posts/2010/</link><pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2010/</guid><description>Overview If Lanchester&amp;rsquo;s Laws are a model describing the aspects of modern and contemporary warfare, the Salvo Combat Model specifically depicts large-scale naval battles among them. In naval warfare, the means of attack are often large and powerful, such as missiles, and conversely, there are missiles for intercepting these missiles, which is a difference. Model1 $$ \begin{align*} \Delta A =&amp;amp; - { { 1 } \over { H_{A} } }</description></item><item><title>Basis of Row Space, Column Space, and Null Space</title><link>https://freshrimpsushi.github.io/en/posts/3027/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3027/</guid><description>Overview1 The concepts such as row space, column space, null space were created to solve linear systems $A \mathbf{x} = \mathbf{b}$. Linear systems can be solved through basic row operations, and indeed, the row space and null space are invariant under basic row operations, indicating their relationship with linear systems. It is important to note here that the column space is not invariant under basic row operations. Theorem 1 (a1)</description></item><item><title>Distribution Convergence of Multivariate Random Variables</title><link>https://freshrimpsushi.github.io/en/posts/2009/</link><pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2009/</guid><description>Definition1 When a $p$-dimensional random vector $\mathbf{X}$ and the sequence of random vectors $\left\{ \mathbf{X}_{n} \right\}$ satisfies the following condition for $n \to \infty$, it is said that $\mathbf{X}_{n}$ converges in distribution to $\mathbf{X}$, denoted as $\mathbf{X}_{n} \overset{D}{\to} \mathbf{X}$. $$\lim_{n \to \infty} F_{\mathbf{X}_{n}} (x) = F_{\mathbf{X}} (x) \qquad, \forall x \in C_{F_{\mathbf{X}}}$$ $F_{X}$ is the cumulative distribution function of the random variable $X$. $C_{F_{\mathbf{X}}}$ represents the set of points where</description></item><item><title>Composition of Functions</title><link>https://freshrimpsushi.github.io/en/posts/3048/</link><pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3048/</guid><description>Definitions A function $f: X \to Y$, $g: f(X) \to Z$ is defined as follows: the composition of $g$ with $f$ is called $h: X \to Z$, and it is denoted by $h=g \circ f$. $$ h(x) = (g\circ f) (x) := g\left( f(x) \right) $$</description></item><item><title>Lanchester's laws</title><link>https://freshrimpsushi.github.io/en/posts/2008/</link><pubDate>Sun, 18 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2008/</guid><description>Laws First Law In melee or close combat, combat power is proportional to the size of the force. Second Law In modern or long-range combat, combat power is proportional to the square of the size of the force. Description Lanchester&amp;rsquo;s Laws describe the number of casualties in combat between two groups, categorized into the First Law (linear law) and the Second Law (square law). Linear Law: War, before the distribution</description></item><item><title>What is Inner Product in Real Vector Spaces?</title><link>https://freshrimpsushi.github.io/en/posts/3044/</link><pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3044/</guid><description>Definition1 Let $V$ be a real vector space. An inner product on $V$ is a function that maps two vectors in $V$ to a single real number $\langle \mathbf{u}, \mathbf{v} \rangle$, satisfying the following conditions: When $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k \in \mathbb{R}$, $\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle$ $\langle \mathbf{u} + \mathbf{v}, \mathbf{w} \rangle = \langle \mathbf{u}, \mathbf{w} \rangle + \langle \mathbf{v}, \mathbf{w} \rangle$</description></item><item><title>Solution to the git warning: LF will be replaced by CRLF in …</title><link>https://freshrimpsushi.github.io/en/posts/2007/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2007/</guid><description>Commands git config --global core.safecrlf false This is a warning due to the differences between Linux and Windows, but it can be ignored. Just type as shown above.</description></item><item><title>Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3026/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3026/</guid><description>Definition1 A transformation is when a function $T : V \to W$ maps from one vector space to another, that is $V$, $W$ are both vector spaces, we call $T$ a transformation. If the transformation $T$ is a linear function, satisfying the following two conditions for any $\mathbf{v},\mathbf{u} \in V$ and scalar $k$, it is called a linear transformation: $T(k \mathbf{u}) = k T(\mathbf{u})$ $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) +</description></item><item><title>Derivatives of Logarithmic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3047/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3047/</guid><description>Formulas The derivative of a logarithmic function with base $e$ is as follows. $$ \begin{equation} \dfrac{d \log x}{dx}=\dfrac{1}{x} \end{equation} $$ The derivative of a composite logarithmic function is as follows. $$ \begin{equation} \dfrac{d \left( \log f(x) \right)}{dx} = \dfrac{f^{\prime}(x)}{f(x)} \end{equation} $$ Explanation Especially, $(2)$ is used as a useful substitution trick. Derivation $(1)$ By the definition of logarithmic functions, the following equation holds. $$ x = e^{\log x} $$ Differentiating</description></item><item><title>May-Leonard Competition Model</title><link>https://freshrimpsushi.github.io/en/posts/2006/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2006/</guid><description>Overview The May-Leonard competition model is a population dynamics model in the state of competition involving three groups, depicting a triangular relationship where each group preys on and is preyed on by the others. These three parties could be political parties, companies, or actual competing species with natural advantages and disadvantages. Model1 $$ \begin{align*} \dot{x_{1}} =&amp;amp; x_{1} \left( 1 - x_{1} - b x_{2} - a x_{3} \right) \\ \dot{x_{2}}</description></item><item><title>Necessary and Sufficient Conditions for a Basis in Finite-Dimensional Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3043/</link><pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3043/</guid><description>Theorem1 Let $V$ be a $n$-dimensional vector space. Suppose a subset $S\subset V$ has $n$ elements. A necessary and sufficient condition for $S$ to be a basis of $V$ is that $V = \text{span}(S)$ or $S$ is linearly independent. Explanation Vector space, dimension, basis, span, independence - all these fundamental concepts of linear algebra appear here. For a set to be a basis of a vector space, it must be</description></item><item><title>Analytic Proof of 1+2+3+4+5+⋯=-1/12</title><link>https://freshrimpsushi.github.io/en/posts/2005/</link><pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2005/</guid><description>Theorem $$ \begin{align*} &amp;amp; 1 + 2 + 3 + 4 + 5 + \cdots \\ =&amp;amp; \sum_{n \in \mathbb{N}} {{ 1 } \over { n^{-1} }} \\ =&amp;amp; \zeta (-1) \\ =&amp;amp; -{{ 1 } \over { 12 }} \end{align*} $$ Description If you only focus on how summing positive numbers can result in a negative, you will never understand this post. The key point is that $\sum_{n \in</description></item><item><title>Inverse Matrices and Systems of Linear Equations</title><link>https://freshrimpsushi.github.io/en/posts/3024/</link><pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3024/</guid><description>Theorem: Equivalent Conditions for an Invertible Matrix1 Let $A$ be a square matrix of size $n\times n$. Then the following statements are equivalent. (a) $A$ is an invertible matrix. (e) $A\mathbf{x}=\mathbf{b}$ has a solution for all matrices $n\times 1$ of size $\mathbf{b}$. (f) $A\mathbf{x}=\mathbf{b}$ has exactly one solution for all matrices $n\times 1$ of size $\mathbf{b}$. That is, $\mathbf{x}=A^{-1}\mathbf{b}$ holds. Description (e) and (f) being equivalent means that if the</description></item><item><title>Projection Theorem in Linear Algebra</title><link>https://freshrimpsushi.github.io/en/posts/3046/</link><pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3046/</guid><description>Theorem1 If $W$ is a subspace of a finite-dimensional inner product space $V$, then every $\mathbf{u} \in V$ is uniquely represented by the following formula. $$ \begin{equation} \mathbf{u} = \mathbf{w}_{1} + \mathbf{w}_{2} \end{equation} $$ Here, $\mathbf{w}_{1} \in W$ and $\mathbf{w}_{2} \in W^{\perp}$ apply. Explanation The notations $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$ in the theorem are also marked as follows. $$ \mathbf{w}_{1} = \mathrm{proj}_{W} \mathbf{u} \quad \text{and} \quad \mathbf{w}_{2} = \mathrm{proj}_{W^{\perp}} \mathbf{u} $$</description></item><item><title>Perceptron Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3023/</link><pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3023/</guid><description>Let&amp;rsquo;s say we have a training set that is linearly separable as represented by $X^{+}$, $X^{-}$. Consider $y$ as the following labels. $$ y_{i} = \pm 1\ (\mathbf{x}_{i} \in X^{\pm}) $$ Suppose the entire training set $X = X^{+} \cup X^{-}$ has $N$ data points. Then, let&amp;rsquo;s say we insert input values in the following order. $$ \mathbf{x}(1), \mathbf{x}(2), \cdots \mathbf{x}(N), \mathbf{x}(1), \mathbf{x}(2), \cdots \mathbf{x}(N),\mathbf{x}(1), \mathbf{x}(2), \cdots $$ That is,</description></item><item><title>Lotka-Volterra Competition Model</title><link>https://freshrimpsushi.github.io/en/posts/2004/</link><pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2004/</guid><description>Overview The Lotka-Volterra competition model is a model capable of explaining the Principle of Competitive Exclusion, particularly describing situations where two groups inhibit each other. For instance, it can be applied to the relationship between rabbits and sheep sharing the same pasture or to the massacres between two rival tribes. Model1 $$ \begin{align*} \dot{x_{1}} =&amp;amp; r_{1} x_{1} {{ K_{1} - x_{1} - \beta_{12} x_{2} } \over { K_{1} }} \\</description></item><item><title>Basic Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3022/</link><pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3022/</guid><description>Definition[^1] If two matrices $A$ and $B$ can be derived from each other through basic row operations, these matrices are said to be row equivalent. A matrix that can be obtained by performing a single basic row operation on an identity matrix is called an elementary matrix, generally denoted by $E$.</description></item><item><title>Multivariate t-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2003/</link><pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2003/</guid><description>Definition Given a location vector $\mathbf{\mu} \in \mathbb{R}^{p}$ and a scale matrix $\Sigma \in \mathbb{R}^{p \times p}$ that is positive definite, the multivariate distribution $t_{p} \left(\nu; \mu , \Sigma \right)$ with the following probability density function is referred to as the Multivariate t-distribution. $$ f (\textbf{x}) = {{ \Gamma \left[ (\nu + p) / 2 \right] } \over { \Gamma ( \nu / 2) \sqrt{ \nu^{p} \pi^{p} \det \Sigma }</description></item><item><title>Various Meanings of the Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/3039/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3039/</guid><description>The Fourier transform is widely treated across various fields such as mathematics, physics, and engineering, and thus it comes to have different meanings depending on the perspective from which it is viewed. Here, its meanings in the context of mathematics, quantum mechanics, and signal processing are introduced. Let&amp;rsquo;s first define the Fourier transform and inverse transform, as they are defined in various forms in this document. $$ \hat{f}(\xi) := \int_{-\infty}^{\infty}</description></item><item><title>Lotka-Volterra Predator-Prey Model</title><link>https://freshrimpsushi.github.io/en/posts/2002/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2002/</guid><description>Overview The Lotka-Volterra predator-prey model models the interaction between species as a system, and the predator-prey model in particular represents the predatory relationship between two species. Since dealing with only two species can be extended indefinitely, it is sufficient to represent a food chain. Model1 $$ \begin{align*} \dot{x} =&amp;amp; a x - b y \cdot x \\ \dot{y} =&amp;amp; c x \cdot y - d y \end{align*} $$ Variables $x(t)$:</description></item><item><title>Definition of Heat in Physics</title><link>https://freshrimpsushi.github.io/en/posts/3038/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3038/</guid><description>Definition1 The energy transferred between two interacting systems is defined as heat. Description To understand the definition of heat, it is useful to recall the concept of work. Consider a situation where a person applies a force to move an object at rest. When the object moves, it is said that the force has done work on the object. From the object&amp;rsquo;s perspective, it has gained kinetic energy, which can</description></item><item><title>How to Save a Git Password</title><link>https://freshrimpsushi.github.io/en/posts/2001/</link><pubDate>Mon, 29 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2001/</guid><description>Command git config credential.helper store Just input as above.</description></item><item><title>Simultaneous Homogeneous Linear Equations</title><link>https://freshrimpsushi.github.io/en/posts/3020/</link><pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3020/</guid><description>Definition1 In a linear system, if the constant terms are all $0$, it is called homogeneous. $$ \begin{align*} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= 0 \\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= 0 \\ &amp;amp;\vdots \\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= 0 \end{align*} $$ Unlike general linear systems, every homogeneous linear system always has a solution because if the constant terms are $0$,</description></item><item><title>Wirtinger Derivatives of Complex Functions</title><link>https://freshrimpsushi.github.io/en/posts/3035/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3035/</guid><description>Buildup Let&amp;rsquo;s assume the complex function $f : \mathbb{C} \to \mathbb{C}$ is given. The complex number $z=x+iy$, being a linear combination of two real numbers $x,y \in \mathbb{R}$, allows us to consider the function $f$ as a function of two real variables. Moreover, using two real functions $u,v : \mathbb{R}^{2} \to \mathbb{R}$, the value of function $f$ can be divided into real and imaginary parts as follows. $$ f(z) =</description></item><item><title>Translation of Vector Field Translation:</title><link>https://freshrimpsushi.github.io/en/posts/1632/</link><pubDate>Wed, 24 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1632/</guid><description>Theorem Let&amp;rsquo;s call a regular curve on the surface $C^{2}$ through $\boldsymbol{\alpha} (t)$. Let $\tilde{\mathbf{X}} = (\tilde{X}^{1}, \tilde{X}^{2})$ be a vector tangent to $M$ at point $\boldsymbol{\alpha}(t_{0})$. Then, there exists a unique vector field $\mathbf{X}(t)$ parallel to $\boldsymbol{\alpha}(t)$ that satisfies $\mathbf{X}(t_{0}) =\tilde{\mathbf{X}}$. Definition The unique vector field $X(t)$ is called the parallel translate of $\tilde{X}$ along $\alpha$. Proof Let $\mathbf{x}$ be the coordinate chart mapping for $\boldsymbol{\alpha}(t_{0})$. It can be</description></item><item><title>Gaussian-Jordan Elimination</title><link>https://freshrimpsushi.github.io/en/posts/3019/</link><pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3019/</guid><description>Definition1 An augmented matrix is said to be in echelon form if it satisfies the following conditions: In rows that have a non-zero element, the first non-zero number is a 1, referred to as the leading 1. Rows where all elements are zero are placed at the bottom. For consecutive rows that contain non-zero elements, the leading 1 in the upper row must be to the left of the leading</description></item><item><title>Solving \General\registry.toml: No such file or directory when Installing Julia Packages</title><link>https://freshrimpsushi.github.io/en/posts/2069/</link><pubDate>Sat, 20 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2069/</guid><description>Error ERROR: SystemError: opening file &amp;quot;C:\\Users\\rmsms\\.julia\\registries\\General\\registry.toml&amp;quot;: No such file or directory Cause It&amp;rsquo;s a really frustrating error, which, as the message indicates, occurs because the Registry.toml file does not exist at the specified path. Solution Delete the C:\Users\사용자이름\.julia\registries\</description></item><item><title>Basis of Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3017/</link><pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3017/</guid><description>Definition1 Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$ be a subset of vector space $V$. If $S$ satisfies the following two conditions, then $S$ is called a basis of $V$. $S$ spans $V$. $$ V = \text{span}(S) $$ $S$ is linearly independent. Explanation As the name suggests, the concept of a basis corresponds to &amp;rsquo;the smallest thing that can create a vector space&amp;rsquo;. The condition of spanning has</description></item><item><title>Matrix Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3025/</link><pubDate>Wed, 17 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3025/</guid><description>Definition A function from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$ is called a matrix transformation with respect to the matrix $m \times n$ $A$ if it maps as follows, and is denoted as $T_{A} : \mathbb{R}^{n} \to \mathbb{R}^{m}$. $$ \mathbf{w} = T_{A} (\mathbf{x}) = A\mathbf{x}\quad \left( \mathbf{x} \in \mathbb{R}^{n}, \mathbf{w} \in \mathbb{R}^{m} \right) $$ It can also be represented as $\mathbf{x} \overset{T_{A}}{\to} \mathbf{w}$. This mapping can be represented in matrix form as follows.</description></item><item><title>How to Install the Latest Version of Julia on Windows</title><link>https://freshrimpsushi.github.io/en/posts/2067/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2067/</guid><description>Guide Julia Version After 1.10 In the terminal, enter the command winget install julia -s msstore as shown above. Julia Version Before 1.10 Step 1. Install Julia Download and run the installation file from the Julia Downloads Page. Step 2. Install VS Code Download and run the installation file from the Visual Studio Code Download Page. Step 3. Install Julia Extension Open Extensions using the fifth icon from the left</description></item><item><title>Matrix Similarity</title><link>https://freshrimpsushi.github.io/en/posts/3016/</link><pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3016/</guid><description>Definition1 A square matrix $A$, $B$ and an invertible matrix $P$ are said to be $B$ is similar to $A$ if the following equation holds. $$ B = P^{-1} A P $$ Description The reason why it is called similar is because similar matrices share many important properties. This is called similarity invariant or invariant under similarity. Conjugate When the given equation is expressed for $B$, $$ B = P^{-1}</description></item><item><title>Various Function Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3032/</link><pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3032/</guid><description>Definition A set of functions $X$ is called a function space if it forms a vector space. Explanation In the function space $X$, the inner product is defined by integration as follows. $$ \langle f, g \rangle = \int f(x) g(x) dx,\quad f,g\in X $$ The main function spaces considered include the following. Space of continuous functions $C^{m}$ $$ C^{m}(\mathbb{R}) : =\left\{ f \in C(\mathbb{R}) : f^{(n)} \text{ is continuous</description></item><item><title>Eigenvalues and Eigenvectors</title><link>https://freshrimpsushi.github.io/en/posts/319/</link><pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/319/</guid><description>Definition1 Given a matrix $n\times n$ $A$, for a non-zero column vector $\mathbf{0}$ $n\times 1$ and a constant $\mathbf{x}$, the following equation is referred to as the eigenvalue equation or the eigenvalue problem. $$ \begin{equation} A \mathbf{x} = \lambda \mathbf{x} \end{equation} $$ For a given $A$, a $\mathbf{x}$ that satisfies the eigenvalue equation above is called the eigenvalue of $A$, and $n\times 1$ is called the eigenvector corresponding to the</description></item><item><title>Referencing Equations in TeX (Hyperlink)</title><link>https://freshrimpsushi.github.io/en/posts/3201/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3201/</guid><description>Code \label{} and \eqref{} can be used to reference equations. It is also possible with \ref{}, but the difference from \eqref{} is that it does not automatically put parentheses. $$ \begin{equation} (a+b)^{2} = a^{2} + 2ab + b^{2} \label{a} \end{equation} $$ 곱셈공식 $\eqref{a}$를 적용하면, The code like $\TeX$ above is displayed as</description></item><item><title>Partial Derivatives</title><link>https://freshrimpsushi.github.io/en/posts/3036/</link><pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3036/</guid><description>Definitions1 Let us define $E\subset \mathbb{R}^{n}$ as an open set, and $\mathbf{x}\in E$, and $\mathbf{f} : E \to \mathbb{R}^{m}$. Let $\left\{ \mathbf{e}_{1}, \mathbf{e}_{2}, \dots, \mathbf{e}_{n} \right\}$, and $\left\{ \mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{m} \right\}$ be the standard basis of $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$, respectively. Then, the components $f_{i} : \mathbb{R}^{n} \to \mathbb{R}$ of $\mathbf{f}$ are defined as follows. $$ \mathbf{f} (\mathbf{x}) = \sum_{i=1}^{m} f_{i}(\mathbf{x})\mathbf{u}_{i}, \quad \mathbf{x} \in E $$ or $$ f_{i}</description></item><item><title>Definite matrix</title><link>https://freshrimpsushi.github.io/en/posts/336/</link><pubDate>Sat, 06 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/336/</guid><description>Definition1 Positive Definite Matrix A quadratic form $\mathbf{x}^{\ast} A \mathbf{x}$ is called a positive definite matrix or quadratic form if it satisfies $\mathbf{x}^{\ast} A \mathbf{x} &amp;gt; 0$ for all $\mathbf{x} \ne \mathbf{0}$. called a negative definite matrix or quadratic form if it satisfies $\mathbf{x}^{\ast} A \mathbf{x} &amp;lt; 0$ for all $\mathbf{x} \ne \mathbf{0}$. called indefinite if it sometimes satisfies $\mathbf{x}$ for the same quadratic form or matrix $A$. For real</description></item><item><title>Linear Forms</title><link>https://freshrimpsushi.github.io/en/posts/1734/</link><pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1734/</guid><description>Definition Let $V$ be a $n$dimensional vector space. For a given constant $a_{i} \in \mathbb{R}(\text{or } \mathbb{C})$, the following linear transformation $A : V \to \mathbb{R}(\text{or } \mathbb{C})$ is called a linear form. $$ A(\mathbf{x}) := \sum\limits_{i=1}^{n} a_{i}x_{i} $$ In this case, $\mathbf{x} = \begin{bmatrix} x_{1} &amp;amp; \cdots &amp;amp; x_{n} \end{bmatrix}^{T}$. Generalization For a given inner product space $(V, \left&amp;lt; \cdot, \cdot \right&amp;gt;)$ and $\mathbf{a} \in V$, the following linear</description></item><item><title>Properties of Rotational Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3042/</link><pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3042/</guid><description>Overview Consider a surface obtained by rotating the curve $\boldsymbol{\alpha}(t) = \left( r(t), z(t) \right)$ about the $z-$ axis, denoted $\mathbf{x}$. $$ \mathbf{x}(t, \theta) = \left( r(t)\cos \theta, r(t)\sin \theta, z(t) \right) $$ This document discusses various properties of the rotational surface. Properties For better readability, let&amp;rsquo;s denote $r = r(t)$, $z = z(t)$ as follows: Partial Derivations $$ \begin{align*} \mathbf{x}_{1} &amp;amp;= \mathbf{x}_{t} = \left( \dot{r}\cos\theta, \dot{r}\sin\theta, \dot{z} \right) \\</description></item><item><title>Properties of Determinants</title><link>https://freshrimpsushi.github.io/en/posts/3015/</link><pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3015/</guid><description>Properties Let $A,B$ be a $n\times n$ matrix and $k$ be a constant. The determinant satisfies the following properties: (a) $\det(kA) = k^{n}\det(A)$ (b) $\det(AB) = \det(A)\det(B)$ (c) $\det(AB)=\det(BA)$ (d) If $A$ is an invertible matrix, then $\det(A^{-1}) = \dfrac{1}{\det(A)}$ (e) $\det(A^{T}) = \det(A)$. Here, $A^{T}$ is the transpose of $A$.</description></item><item><title>Classification of Surfaces of Revolution According to Gaussian Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3034/</link><pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3034/</guid><description>Overview1 Rotational surfaces are classified into three types according to the sign of the Gaussian curvature. Within each classification, surfaces with the same curvature share the same local intrinsic characteristics, even though they might have different global, extrinsic properties. In other words, they are locally isometric. Richard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p153-154&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Determinants</title><link>https://freshrimpsushi.github.io/en/posts/252/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/252/</guid><description>Definition Let $A$ be the following $2 \times 2$ matrix. $$ A = \begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \end{bmatrix} $$ The determinant of $A$ is defined as follows and is denoted by $\det(A)$. $$ \det(A) := ad - bc $$ Explanation To discuss determinants, we cannot ignore the very purpose of linear algebra. Most problems in mathematics can be summarized as &amp;lsquo;Can we solve the equation?&amp;rsquo; A</description></item><item><title>Augmented Matrices and Elementary Row Operations</title><link>https://freshrimpsushi.github.io/en/posts/3014/</link><pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3014/</guid><description>Definition1 Let&amp;rsquo;s assume a linear system is given as follows. $$ \begin{equation} \begin{aligned} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= b_{1}\\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= b_{2}\\ &amp;amp;\vdots\\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= b_{m} \end{aligned} \end{equation} $$ The representation of constants of a linear system in a matrix is called an augmented matrix. $$ \begin{equation} \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}</description></item><item><title>Area of a Surface in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3033/</link><pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3033/</guid><description>Definition1 Let&amp;rsquo;s say $\mathbf{x} : U \to \mathbb{R}^{3}$ is the coordinate chart mapping of a surface. The area of any region $\mathscr{R} \subset \mathbf{x}(U)$ on the surface is defined as follows. $$ \begin{align*} A(\mathscr{R}) &amp;amp;:= \int\int_{\mathbf{x}^{-1}(\mathscr{R})} [\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{n}] du^{1}du^{2} \\ &amp;amp;= \int\int_{\mathbf{x}^{-1}(\mathscr{R})} \sqrt{g} du^{1}du^{2} \end{align*} $$ Here, $(u^{1}, u^{2})$ are the coordinates of $U$, $\mathbf{x}_{i} = \dfrac{\partial \mathbf{x}}{\partial u^{i}}$ is the partial derivative of the $i$-th coordinate, $[\mathbf{x}_{1}, \mathbf{x}_{2},</description></item><item><title>Simultaneous Linear Equations</title><link>https://freshrimpsushi.github.io/en/posts/3013/</link><pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3013/</guid><description>Definition1 For constants $a_{1}$, $a_{2}$, $\dots$, $a_{n}$, $b$, we define a linear equation for variables $x_{1}$, $x_{2}$, $\dots$, $x_{n}$ as follows. $$ \begin{equation} a_{1}x_{1} + a_{2}x_{2} + \cdots + a_{n}x_{n} = b \label{lineq} \end{equation} $$ At least one of $a$ is not $0$. In other words, not &amp;ldquo;all $a$ are $0$&amp;rdquo;. A finite set of linear equations is called a system of linear equations or simply a linear system, and</description></item><item><title>Linear Function</title><link>https://freshrimpsushi.github.io/en/posts/3037/</link><pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3037/</guid><description>Definition A function $f : X \to Y$ is called linear if it satisfies the following two conditions for $x,x_{1},x_{2}\in X$ and $a \in \mathbb{R}$, $f(ax) = af(x)$ $f(x_{1} + x_{2}) = f(x_{1}) + f(x_{2})$ Explanation If it is not linear, it is called nonlinear. The two conditions are sometimes combined as follows $$ f(ax_{1} + x_{2}) = af(x_{1}) + f(x_{2}) $$ If in 2., instead of being equal, it</description></item><item><title>Equivalents Codes in Julia, MATLAB, Python, and R</title><link>https://freshrimpsushi.github.io/en/posts/3031/</link><pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3031/</guid><description>Overview Organizing code that performs the same function in Julia, Matlab, Python, and R. Flux-PyTorch-TensorFlow Cheat Sheet Let&amp;rsquo;s assume the following environment for Python. import numpy as np General Julia Matlab Python R comment #comment %comment #comment #comment 2d grid X = kron(x, ones(size(y)))Y = kron(ones(size(x)), y) [X,Y] = meshgrid(x,y) np.meshgrid(x,y) How to make an n-dimensional meshgrid in Julia Type 줄리아Julia 매트</description></item><item><title>The eigenvectors of distinct eigenvalues of Hermitian matrices are orthogonal.</title><link>https://freshrimpsushi.github.io/en/posts/330/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/330/</guid><description>Theorem Let $A$ be a Hermitian matrix of size $n \times n$. Let the eigenvectors corresponding to two distinct eigenvalues $\lambda , \mu$ of $A$ be $\mathbf{x}$ and $\mathbf{y}$, that is, $$ \begin{align*} A \mathbf{x} =&amp;amp; \lambda \mathbf{x} \quad \\ A \mathbf{y} =&amp;amp; \mu \mathbf{y} \end{align*} $$ Then, the two eigenvectors are orthogonal to each other. $$ \mathbf{x} \perp \mathbf{y} $$ Explanation Hermitian matrices have the property that not only</description></item><item><title>Unitary Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3008/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3008/</guid><description>Definition Unitary Matrix Let $A$ be a square complex matrix. $A$ is called a unitary matrix if it satisfies the following equation: $$ A^{-1}=A^{\ast} $$ Here, $A^{-1}$ is the inverse of $A$, $A^{\ast}$ is the conjugate transpose of $A$. Unitary Diagonalization1 Consider a square matrix $A$ of size $n \times n$. $A$ is said to be unitarily diagonalizable if it satisfies the following equation for some diagonal matrix $D$ and</description></item><item><title>The eigenvalues of a Hermitian matrix are always real</title><link>https://freshrimpsushi.github.io/en/posts/310/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/310/</guid><description>Theorem Let $A$ be a Hermitian matrix of size $n \times n$. Then, the eigenvalues of $A$ are all real. Explanation In general, there is no guarantee that the eigenvalues of a matrix are real, but for Hermitian matrices, this can be verified through proof. It may not be intuitively obvious, but the proof itself is relatively simple, and it is quite useful as a fact. It yields various good</description></item><item><title>Hermitian Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3007/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3007/</guid><description>Definition Let $A$ be a square complex matrix. If $A$ satisfies the following equation, it is called a Hermitian matrix or self-adjoint matrix. $$ A^{\ast}=A $$ Here, $A^{\ast}$ is the conjugate transpose of $A$. If $A$ satisfies the following equation, it is called a skew-Hermitian matrix . $$ A^{\ast}=-A $$ Explanation If it is a real matrix, since $A^{\ast}=A^{T}$, if it is a symmetric matrix, it is a Hermitian matrix.</description></item><item><title>Equivalence Conditions for Orthogonal Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3012/</link><pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3012/</guid><description>Theorem For a real matrix $A$, the following propositions are all equivalent. (a) $A$ is an orthogonal matrix. (b) The set of row vectors of $A$ forms a normal orthogonal set in $\mathbb{R}^n$. (c) The set of column vectors of $A$ forms a normal orthogonal set in $\mathbb{R}^n$. (d) $A$ preserves inner product, i.e., for all $\mathbf{x},\mathbf{y}\in \mathbb{R}^{n}$, the following holds: $$ (A \mathbf{x}) \cdot (A\mathbf{y}) = \mathbf{x} \cdot \mathbf{y}</description></item><item><title>대각합</title><link>https://freshrimpsushi.github.io/en/posts/1924/</link><pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1924/</guid><description>Definition Let the $n\times n$ matrix $A$ be given as follows. $$ A= \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{n1} &amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{bmatrix} $$ The sum of the diagonal entries of $A$ is defined to be the trace of $A$ and is denoted as follows. $$ \text{tr}(A)=\text{Tr}(A)=a_{11}+a_{22}+\cdots</description></item><item><title>Multivariate Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1954/</link><pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1954/</guid><description>Definition The multivariate distribution $N_{p} \left( \mu , \Sigma \right)$ with the following probability density function, given the population mean vector $\mathbf{\mu} \in \mathbb{R}^{p}$ and the covariance matrix $\Sigma \in \mathbb{R}^{p \times p}$, is called the multivariate normal distribution. $$ f (\textbf{x}) = \left( (2\pi)^{p} \det \Sigma \right)^{-1/2} \exp \left[ - {{ 1 } \over { 2 }} \left( \textbf{x} - \mathbf{\mu} \right)^{T} \Sigma^{-1} \left( \textbf{x} - \mathbf{\mu} \right) \right]</description></item><item><title>Multivariate Random Variables Probability Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1952/</link><pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1952/</guid><description>Definition 1 $p$-dimensional random vector $\mathbf{X}$ and a sequence of random vectors $\left\{ \mathbf{X}_{n} \right\}$ are said to converge in probability to $n \to \infty$ as $\mathbf{X}_{n}$ if they satisfy the following. It is denoted by $\mathbf{X} _ {n} \overset{P}{\to} \mathbf{X}$. $$ \forall \varepsilon &amp;gt; 0 , \lim_{n \to \infty} P \left[ \left\| \mathbf{X}_{n} - \mathbf{X} \right\| &amp;lt; \varepsilon \right] = 1 $$ $\| \cdot \|$ is defined as the</description></item><item><title>Analytic Proof that 1+1+1+1+1+⋯=-1/12</title><link>https://freshrimpsushi.github.io/en/posts/1944/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1944/</guid><description>Theorem $$ \begin{align*} &amp;amp; 1 + 1 + 1 + 1 + 1 + \cdots \\ =&amp;amp; \sum_{n \in \mathbb{N}} {{ 1 } \over { n^{0} }} \\ =&amp;amp; \zeta (0) \\ =&amp;amp; -{{ 1 } \over { 2 }} \end{align*} $$ Explanation If you only focus on how adding positive numbers results in a negative number, you will never understand this post. The key is that $\sum_{n \in \mathbb{N}}</description></item><item><title>Bass Diffusion Model: Innovation and Imitation</title><link>https://freshrimpsushi.github.io/en/posts/1946/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1946/</guid><description>Model 1 2 $$ \dot{N} = \left( p + q {{ N } \over { K }} \right) \left( 1 - {{ N } \over { K }} \right) $$ Variables $N(t)$: Represents the number of entities in a population at time point $t$. Parameters $K$: Carrying Capacity, describes the size of the environment that can support the population. The number of entities cannot grow beyond the carrying capacity. $p$:</description></item><item><title>Covariance Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1950/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1950/</guid><description>Definition1 $p$ For a dimensional random vector $\mathbf{X} = \left( X_{1}, \cdots , X_{p} \right)$, the following defined ▶eq03◀ is called the covariance matrix. $$ \left( \operatorname{Cov} \left( \mathbf{X} \right) \right)_{ij} := \operatorname{Cov} \left( X_{i} , X_{j} \right) $$ $\operatorname{Cov}$ is covariance. Explanation To put the definition more simply, it can be stated as follows: $$ \operatorname{Cov} \left( \mathbf{X} \right) := \begin{pmatrix} \Var \left( X_{1}</description></item><item><title>Properties of Orthogonal Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3010/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3010/</guid><description>Properties1 An orthogonal matrix has the following properties: (a) The transpose of an orthogonal matrix is also an orthogonal matrix. (b) The inverse of an orthogonal matrix is an orthogonal matrix. (c) The product of two orthogonal matrices is an orthogonal matrix. (d) The determinant of an orthogonal matrix is either $1$ or $-1$. $$ \det(A)=\pm 1 $$ Proof (a) Let&amp;rsquo;s say $A$ is an orthogonal matrix. Let&amp;rsquo;s say $B$</description></item><item><title>Gompertz Growth Model: Time-dependent Growth Deceleration</title><link>https://freshrimpsushi.github.io/en/posts/1943/</link><pubDate>Fri, 05 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1943/</guid><description>Model 1 $$ {{ d N } \over { dt }} = r e^{ - \alpha t} N \qquad, \alpha 0 $$ Variables $N(t)$: Represents the population size at time $t$. Parameters $r \in \mathbb{R}$: Intrinsic Rate of Increase, it grows when greater than $0$ and declines when less than $0$. It can also be defined as the difference $r:=b-d$ between Birth Rate $b$ and Death Rate $d$. $\alpha&amp;gt;0$: A</description></item><item><title>Allee Effect in Mathematical Biology</title><link>https://freshrimpsushi.github.io/en/posts/1941/</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1941/</guid><description>What is the Allee Effect? 1 The Allee effect refers to the phenomenon where the population decreases when the density is low. Mathematically, this is represented in models by setting function $a: \mathbb{R} \to \mathbb{R}$ for $N$ as an upward convex convex function. $$ \dot{N} = a(N) N $$ Variables $N(t)$: Represents the number of individuals in the group at time $t$. Examples The Allee effect can be illustrated, for</description></item><item><title>What is Reinforcement Learning in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3029/</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3029/</guid><description>Definition Reinforcement learning is the process where an agent interacts with the environment to find a policy that maximizes the cumulative reward. Description1 The elements comprising reinforcement learning are as follows: Agent: Decides actions based on a policy, given a state. State: Refers to the situation in which the agent is placed. Action: Refers to the choices available to the agent in a given state. Policy: Refers to the strategy</description></item><item><title>Central Limit Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/43/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/43/</guid><description>Theorem 1 If $\left\{ X_{k} \right\}_{k=1}^{n}$ are iid random variables following the probability distribution $\left( \mu, \sigma^2 \right) $, then when $n \to \infty$ $$ \sqrt{n} {{ \overline{X}_n - \mu } \over {\sigma}} \overset{D}{\to} N (0,1) $$ $\overset{D}{\to}$ means convergence in distribution. Explanation This theorem is widely acclaimed in statistics, along with the Law of Large Numbers. Despite being frequently discussed and applied, many encounter its proof only upon studying</description></item><item><title>Diffusion in Lattice Model Simulations</title><link>https://freshrimpsushi.github.io/en/posts/1939/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1939/</guid><description>Simulation In this post, we aim to mimic the diffusion of an ingredient in a lattice space. This simultaneously serves as a simulation of the SI disease spreading model and can also be considered an SIR model due to the limited space. Variables $t$: Represents the current turn. $I(t) \in \mathbb{N}$: The amount of the ingredient being diffused at turn $t$. $S(t) \in \mathbb{N}$: The amount of empty space at</description></item><item><title>Orthogonal Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3009/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3009/</guid><description>Definition Let $A$ be a square real matrix. $A$ is called an orthogonal matrix if it satisfies the following equation: $$ A^{-1} = A^{\mathsf{T}} $$ Another way to express this condition is as follows: $$ AA^{\mathsf{T}} = A^{\mathsf{T}}A =I $$ Explanation To put the definition in words, an orthogonal matrix is a matrix whose row vectors or column vectors are orthogonal unit vectors to each other. When extended to complex</description></item><item><title>Ramanujan Sum</title><link>https://freshrimpsushi.github.io/en/posts/1936/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1936/</guid><description>Definition Assigning values to diverging series is called Ramanujan summation, and it is represented through the symbol $\re$. Theorem [1] The Grandi Series 1: $$ 1-1+1-1+ \cdots = {{ 1 } \over { 2 }} \qquad ( \operatorname{Re} ) $$ [2] $$ 1-2+3-4+ \cdots = {{ 1 } \over { 4 }} \qquad ( \operatorname{Re} ) $$ [2]' $$ 1+2+3+4+ \cdots = - {{ 1 } \over { 12 }}</description></item><item><title>Analytic Continuation</title><link>https://freshrimpsushi.github.io/en/posts/1931/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1931/</guid><description>Definition 1 For an analytic function $f_{1}: \mathscr{R}_{1} \to \mathbb{C}$ if there exists an analytic function $f_{2}: \mathscr{R}_{2} \to \mathbb{C}$ in $\mathscr{R}_{2} \subset \mathbb{C}$ that satisfies $$ \mathscr{S} := \mathscr{R}_{1} \cap \mathscr{R}_{2} \ne \emptyset \\ f_{1} (z) = f_{2} (z) \qquad , z \in \mathscr{S} $$ then $f_{2}$ is called the Analytic Continuation of $\mathscr{R}_{2}$ in $f_{1}$. Explanation Although this article is written in a complicated manner, if we read</description></item><item><title>Derivation of the Laurent Expansion of the Riemann Zeta Function</title><link>https://freshrimpsushi.github.io/en/posts/1934/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1934/</guid><description>Theorem The Laurent expansion of the Riemann zeta function $\zeta$ is as follows: $$ \zeta (s) = {{ 1 } \over { s-1 }} + \sum_{n=0}^{\infty} \gamma_{n} {{ (1-s)^{n} } \over { n! }} \qquad , s &amp;gt; 1 $$ Here, $\gamma_{n}$ is the nth Stieltjes constant, defined as follows: $$ \gamma_{n} := \lim_{m \to \infty} \sum_{k=1}^{m} \left( {{ \left( \log k \right)^{n} } \over { k }} - {{</description></item><item><title>First Steps in Lattice Model Simulation: Representing with Heatmaps</title><link>https://freshrimpsushi.github.io/en/posts/1932/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1932/</guid><description>Simulation Code Review Step 1. Creating a Lattice Space julia&amp;gt; colormap\_SI = [colorant&amp;#34;#EEEEEE&amp;#34;, colorant&amp;#34;#111111&amp;#34;] julia&amp;gt; row\_size = 5 5 julia&amp;gt; column\_size = 5 5 julia&amp;gt; Random.seed!(3); julia&amp;gt; stage\_lattice = rand([&amp;#39;S&amp;#39;], row\_size, column\_size) 5×5 Array{Char,2}: &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; &amp;#39;S&amp;#39; The code above creates an empty lattice space of</description></item><item><title>Proof of the Weak Law of Large Numbers</title><link>https://freshrimpsushi.github.io/en/posts/32/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/32/</guid><description>Law Given $\left\{ X_{k} \right\}_{k=1}^{n}$ are iid random variables with distribution $\left( \mu, \sigma^2 \right) $, then when $n \to \infty$ $$ \overline{X}_n \overset{P}{\to} \mu $$ $\overset{P}{\to}$ means convergence in probability. Explanation Ranked among the most important theorems in statistics, alongside the Central Limit Theorem. This theorem implies that no matter the distribution, &amp;rsquo;the sample mean converges to the population mean&amp;rsquo;. While this might seem obvious, in natural sciences, the</description></item><item><title>Inner Product with Vector</title><link>https://freshrimpsushi.github.io/en/posts/3011/</link><pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3011/</guid><description>Definition: Inner Product of Two Column Vectors1 The inner product of two column vectors of size $n \times 1$, $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{R}^{n}$ is defined as follows. $$ \begin{equation} \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{T}\mathbf{v}=u_{1}v_{1} + u_{2}v_{2} + \cdots + u_{n}v_{n} \label{EuclideanIP} \end{equation} $$ In the case where $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{C}^{n}$, it is as follows. $$ \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{\ast}\mathbf{v}=u^{\ast}_{1}v_{1}^{\ } + u_{2}^{\ast}v_{2}^{\ } + \cdots + u_{n}^{\ast}v_{n}^{\ }</description></item><item><title>Analytic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1929/</link><pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1929/</guid><description>Definition Given open sets $A \subset \mathbb{C}$ and $f: A \to \mathbb{C}$, let us assume $\alpha \in A$. If $\displaystyle \lim_{z \to \alpha } f(z) = f (\alpha)$, then $f$ is said to be continuous at $\alpha$, and if $f$ is continuous at every point in the complex domain $\mathscr{R}$, then $f$ is said to be continuous on $\mathscr{R}$. Especially, if $f$ is continuous throughout its domain, it is called</description></item><item><title>Conjugate Transpose Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3006/</link><pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3006/</guid><description>Definition Let $A$ be a complex matrix of size $m \times n $. Define $\overline{A}$ as follows, and call it the conjugate matrix of $A$. $$ \overline{A} :=\begin{bmatrix} \overline{a_{11}} &amp;amp; \overline{a_{12}} &amp;amp; \cdots &amp;amp; \overline{a_{1n}} \\ \overline{a_{21}} &amp;amp; \overline{a_{22}} &amp;amp; \cdots &amp;amp; \overline{a_{2n}} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \overline{a_{m1}} &amp;amp; \overline{a_{m2}} &amp;amp; \cdots &amp;amp; \overline{a_{mn}} \end{bmatrix} = \left[ \overline{a_{ij}} \right] $$ Here, $\overline{a}$ is the conjugate</description></item><item><title>Convergence in Distribution Implies Probability Bound</title><link>https://freshrimpsushi.github.io/en/posts/176/</link><pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/176/</guid><description>Theorem A sequence of random variables $\left\{ X_{n} \right\}$ is probabilistically bounded if it converges in distribution. $\overset{D}{\to}$ means convergence in distribution. Explanation Since we have shown that convergence almost surely implies convergence in distribution, by considering the contrapositive proposition, we can also obtain the common-sense corollary that &amp;lsquo;if it is not probabilistically bounded, it does not converge almost surely&amp;rsquo;. Proof Given $\epsilon&amp;gt;0$ and assuming that $X_{n}$ converges in distribution</description></item><item><title>Convergence in Probability Implies Convergence in Distribution</title><link>https://freshrimpsushi.github.io/en/posts/175/</link><pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/175/</guid><description>Theorem1 Given a random variable $X$ and its sequence $\left\{ X_{n} \right\}$, $$ X_{n} \overset{P}{\to} X \implies X_{n} \overset{D}{\to} X $$ $\overset{P}{\to}$ denotes convergence in probability. $\overset{D}{\to}$ denotes convergence in distribution. Explanation In simpler terms, it’s much easier to have convergence in distribution than exact convergence. Understanding a random variable as a function in its own right should make this concept not too difficult to grasp. Proof</description></item><item><title>Probability Bounds in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1922/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1922/</guid><description>Definition 1 Let&amp;rsquo;s assume that a sequence of random variables $\left\{ X_{n} \right\}$ is given. If for all $\varepsilon &amp;gt; 0$, there exists a $N_{\varepsilon} \in \mathbb{N}$ and a constant $B_{\varepsilon} &amp;gt; 0$ such that the following is satisfied, then $\left\{ X_{n} \right\}$ is said to be Bounded in Probability. $$ n \ge N_{\varepsilon} \implies P \left[ \left| X_{n} \right| \le B_{\varepsilon} \right] \ge 1 - \varepsilon $$ Explanation If</description></item><item><title>Symmetric Matrices, Skew-Symmetric Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3005/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3005/</guid><description>Definition1 A square matrix $A$ is called a symmetric matrix if it satisfies the following equation: $$ A=A^{T} $$ Here, $A^{T}$ is the transpose of $A$. $A$ is called an anti-symmetric matrix if it satisfies the following equation: $$ A =-A^{T} $$ Explanation By the definition of the transpose, matrices that are not square cannot be symmetric or anti-symmetric. If $A$ is an anti-symmetric matrix, it follows from the definition</description></item><item><title>Deriving Standard Normal Distribution as a Limiting Distribution of Student's t-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/195/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/195/</guid><description>Theorem If $T_n \sim t(n)$ then $$ T_n \ \overset{D}{\to} N(0,1) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with mean $\mu$ and variance $\sigma^{2}$. $t(r)$ is a t-distribution with degrees of freedom $r$. $\overset{D}{\to}$ respectively imply distribution convergence. Originally, the Student t-distribution was created for statistical analysis when the sample size is small. As the sample size increases, it becomes similar to the standard normal distribution,</description></item><item><title>How to Use Hexadecimal RGB Codes (HEX) in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1921/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1921/</guid><description>Code The package provided for handling colors in Julia is Colors.jl. By loading the visualization package Plots.jl, one can also use the functionality within Colors.jl. Color codes representing the RGB space, such as RGB, BGR, RGB24, RGBX, XRGB, are supported and are subtypes of AbstractRGB. RGBA is RGB with added transparency. julia&amp;gt; using Plots julia&amp;gt; subtypes(AbstractRGB) 5-element Vector{Any}: BGR RGB RGB24 RGBX XRGB julia&amp;gt; subtypes(AbstractRGBA) 2-element Vector{Any}: BGRA RGBA Strings</description></item><item><title>Conditions for a Matrix Being Invertible</title><link>https://freshrimpsushi.github.io/en/posts/3004/</link><pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3004/</guid><description>Theorem1 Let $A$ be a square matrix of size $n\times n$. Then the following propositions are all equivalent. (a) $A$ is an invertible matrix. (b) The homogeneous linear system $A\mathbf{x}=\mathbf{0}$ has only the trivial solution. (c) The reduced row echelon form of $A$ is $I_{n}$. (d) $A$ can be expressed as a product of elementary matrices. (e) $A\mathbf{x}=\mathbf{b}$ has a solution for all $n\times 1$ matrices $\mathbf{b}$. (f) $A\mathbf{x}=\mathbf{b}$ has</description></item><item><title>Converting Between DataFrames and 2D Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1930/</link><pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1930/</guid><description>Code Matrix(df) or Array(df) functions can be used to convert a DataFrame into an array of the same size. To create a DataFrame from an array, use DataFrame(array, :auto). In the past, the convert function was used, but it&amp;rsquo;s not applicable anymore, so be careful. using DataFrames julia&amp;gt; A = rand(5,3) 5×3 Matrix{Float64}: 0.678876 0.10431 0.827079 0.621647 0.372007 0.29346 0.756844 0.171237 0.0732631 0.922519 0.0535938 0.121689 0.164058 0.0684278 0.68446</description></item><item><title>Derivation of the Standard Normal Distribution as the Limiting Distribution of the Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/197/</link><pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/197/</guid><description>Theorem If $X_{n} \sim \text{Poi} \left( n \right)$ and $\displaystyle Y_{n} := {{ X_{n} - n } \over { \sqrt{n} }}$ are given $$ Y_{n} \overset{D}{\to} N(0,1) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with a mean of $\mu$ and a variance of $\sigma^{2}$. $\text{Poi} (\lambda)$ is a Poisson distribution with mean and variance of $\lambda$. Explanation Considering the approximation of the binomial distribution to the</description></item><item><title>How to Read *.csv Files in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1923/</link><pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1923/</guid><description>Guide Old Version In julia v1.5.0, *.csv files were read as follows: In fact, Julia is not yet a language notably convenient for data input. However, if one desires speed, there may come a time when Julia should be chosen over Python, R, or Matlab. For instance, if one wants to load a *.csv file located just under the E drive, it can be entered as follows. using CSV data</description></item><item><title>Riemann Hypothesis</title><link>https://freshrimpsushi.github.io/en/posts/1920/</link><pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1920/</guid><description>Conjecture All non-trivial solutions of $\zeta (s) = 0$ will satisfy $\displaystyle \operatorname{Re} (s) = {{ 1 } \over { 2 }}$. $\zeta$ is the Riemann zeta function. $\re(z)$ denotes the real part of the complex number $z \in \mathbb{C}$. Description The Riemann Hypothesis is still an unsolved Millennium problem, and for those not familiar with mathematics, its significance, let alone the hypothesis itself, could be difficult to comprehend. If</description></item><item><title>Changing the Number of Threads for parallel Computing in Julia on Windows</title><link>https://freshrimpsushi.github.io/en/posts/1933/</link><pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1933/</guid><description>Guide In Julia, parallel computing is commonly used, so it may be necessary to focus all of a computer&amp;rsquo;s resources on computation depending on the situation. Although there are various ways to change the number of threads, the most static and convenient method is to edit environmental variables. Step 1. Edit System Environmental Variables Press the Windows key or Windows+S to search for &amp;lsquo;Edit system environment variables&amp;rsquo;. When the System</description></item><item><title>Inverse Matrix, Reversible Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3003/</link><pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3003/</guid><description>Definition Let $A$ be an arbitrary square matrix of size $n\times n$. A matrix $L$ is called the left inverse matrix of $A$ if it satisfies the following equation with $A$ in a matrix multiplication. $$ LA=I_{n} $$ Here, $I_{n}$ is the identity matrix of size $n\times n$. A matrix $R$ that is capable of matrix multiplication with $A$ and satisfies the following equation is called the right inverse matrix</description></item><item><title>Logistic Growth Model: The Limits of Population Growth</title><link>https://freshrimpsushi.github.io/en/posts/1915/</link><pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1915/</guid><description>Model $$ \dot{N} = {{ r } \over { K }} N ( K - N) $$ Variables $N(t)$: Represents the population size of a group at time $t$. Parameters $r \in \mathbb{R}$ : Intrinsic Rate of Increase, growth occurs if it is greater than $0$, and decline occurs if it is less than $0$. It can also be defined by the difference $r:=b-d$ between the Birth Rate $b$ and</description></item><item><title>Derivation of the Standard Normal Distribution as a Limiting Distribution of the Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/196/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/196/</guid><description>Theorem De Moivre-Laplace Theorem If $X_i \sim B(1,p)$ and $Y_n = X_1 + X_2 + \cdots + X_n$, then $Y_n \sim B(n,p)$ and $$ { { Y_n - np } \over {\sqrt{ np(1-p) } } }\overset{D}{\to} N(0,1) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with mean $\mu$ and variance $\sigma^{2}$. $B(n,p)$ is a binomial distribution with $n$ trials and probability $p$. $\overset{D}{\to}$ denotes convergence in distribution.</description></item><item><title>How to Determine the Location of Code Files Executed in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1935/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1935/</guid><description>Guide If you are someone who uses Julia, it&amp;rsquo;s likely that you&amp;rsquo;re comfortable with using multiple operating systems or computers, including servers. If there is file input/output involved, having to adjust the path each time the development environment changes can be quite bothersome. This is where the @__DIR__ macro comes in handy. Suppose you have a Julia code file like the following. Typically, when executed from the terminal, pwd() and</description></item><item><title>Changing the Number of Threads for parallel Computing in Julia on Linux</title><link>https://freshrimpsushi.github.io/en/posts/1937/</link><pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1937/</guid><description>Guide In Julia, parallel computing is routinely used, so sometimes it&amp;rsquo;s necessary to focus all of a computer&amp;rsquo;s resources on the computation. There are several ways to change the number of threads, but the most static and convenient method is to edit the environment variables. Step 1. Edit System Environment Variables Press Ctrl + Alt + T to open the terminal and type gedit ~/.bashrc. A window for editing environment</description></item><item><title>The Poisson Distribution as a Limiting Distribution of the Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/198/</link><pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/198/</guid><description>Theorem Let&amp;rsquo;s say $X_{n} \sim B(n,p)$. If $\mu \approx np$ then $$ X_{n} \overset{D}{\to} \text{Poi} (\mu) $$ $B(n,p)$ is a binomial distribution with trials $n$ and probability $p$. $\text{Poi} (\lambda)$ is a Poisson distribution with mean and variance $\lambda$. $\overset{D}{\to}$ means distribution convergence. Description Note that the condition $\mu \approx np$ is necessary here. Since $ np \approx npq$, it implies $q = (1-p) \approx 1$, i.e., $p \approx 0$.</description></item><item><title>Transpose Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3002/</link><pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3002/</guid><description>Definition1 Let&amp;rsquo;s consider a matrix of size $m\times n$ as $A$. The matrix obtained by swapping the rows and columns of $A$ is called the transpose of $A$ and is denoted by $A^{\mathsf{T}}$ or $A^{T}$, $A^{t}$. Description Following the definition, if $A$ is a $m \times n$ matrix then $A^{\mathsf{T}}$ will be a $n \times m$ matrix. Also, the $i$th row of $A$ is the same as the $i$th column</description></item><item><title>How to Use Composite Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1942/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1942/</guid><description>Code julia&amp;gt; f(x) = 2x + 1 f (generic function with 1 method) julia&amp;gt; g(x) = x^2 g (generic function with 1 method) julia&amp;gt; (g ∘ f)(3) 49 Description In Julia, function composition is similar to the pipe operator in programming. The main advantage of this composition is that it makes it easier for mathematicians to express formulas as code. The example above is simply a translation of the following</description></item><item><title>Identity Matrix, Unit Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3001/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3001/</guid><description>Definition A diagonal matrix of size $n\times n$ with all diagonal elements being $1$ is called an identity matrix or unit matrix, denoted as $I_{n}$ or $I_{n\times n}$. $$ I_{n\times n}= \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \end{bmatrix} $$ Description The identity matrix is</description></item><item><title>Convergence of Distributions in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1888/</link><pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1888/</guid><description>Definition 1 Given a random variable $X$ and a sequence of random variables $\left\{ X_{n} \right\}$, if the following condition is satisfied when $n \to \infty$, we say that $X$ converges in distribution to $X_{n}$ and represent it as $X_{n} \overset{D}{\to} X$. $$ \lim_{n \to \infty} F_{X_{n}} (x) = F_{X} (x) \qquad, \forall x \in C_{F_{X}} $$ $F_{X}$ is the cumulative distribution function of the random variable $X$. $C_{F_{X}}$ represents</description></item><item><title>Diagonal Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1958/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1958/</guid><description>Diagonal Matrix1 Let&amp;rsquo;s consider a matrix $A$ of size $n\times m$. The elements whose row and column numbers are the same, that is, $a_{ii} (1 \le i \le \min(n,m))$, are called the main diagonal elements. The imaginary line connecting the main diagonal elements is referred to as the main diagonal, or principal diagonal. A matrix $A$, in which all elements except for the main diagonal elements are $0$, is called</description></item><item><title>Agent-based Model Simulation of Mortality</title><link>https://freshrimpsushi.github.io/en/posts/1884/</link><pubDate>Sat, 16 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1884/</guid><description>Simulation This post aims to simulate the degrowth of a population from a macroscopic perspective by implementing an action in which generated agents die. Everything related to space or movement in this simulation is merely for visualization and has no actual purpose. Variables $t$: Represents the current turn. $N(t)$: Represents the number of agents at turn $t$. Parameters $N_{0} \in \mathbb{N}$: Represents the number of agents at the start of</description></item><item><title>Square Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1956/</link><pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1956/</guid><description>Definition A matrix $A$ is called a square matrix if the number of rows and columns of the matrix $A$ are equal. Explanation It is also referred to as a regular square matrix. Square matrices are easy to handle and possess various favourable properties. Examples Identity matrix Invertible matrix Elementary matrix Symmetric matrix Orthogonal matrix Hermitian matrix Unitary matrix</description></item><item><title>Matrix Operations: Scalar Multiplication, Addition, and Multiplication</title><link>https://freshrimpsushi.github.io/en/posts/1957/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1957/</guid><description>Scalar Multiplication The multiplication of an arbitrary matrix $A$ of size $m \times n$ by a scalar $k$ is defined as multiplying each element of $A$ by $k$ and is denoted as follows: $$ kA = k\begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} :=</description></item><item><title>Agent-based Model Simulation of Reproduction</title><link>https://freshrimpsushi.github.io/en/posts/1880/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1880/</guid><description>Simulation In this post, we attempt to emulate the macroscopic growth of a population by giving generated agents the action to replicate themselves. Everything related to space or movement in this simulation is merely for visualization purposes and has no actual relevance to the objective. Variables $t$: Represents the current turn. $N(t)$: Indicates the number of agents at turn $t$. Parameters $N_{0} \in \mathbb{N}$: Represents the number of agents at</description></item><item><title>How to Print and Save a 2D Array as a Heatmap Image in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1948/</link><pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1948/</guid><description>Imagesc imagesc function allows you to display a 2D array as a heatmap. colorbar is a setting that outputs a color bar indicating the scale. N=2^8; p=phantom(&amp;#39;Modified Shepp-Logan&amp;#39;,N); figure() imagesc(p) colorbar Saving Method 1 You can use the saveas function to save the figure displayed above. The setting gcf refers to the current figure. Then, the picture below is saved. N=2^8; p=phantom(&amp;#39;Modified Shepp-Logan&amp;#39;,N); figure() imagesc(p) colorbar saveas(gcf,&amp;#39;phantom.png&amp;#39;) Method 2 You</description></item><item><title>code summary</title><link>https://freshrimpsushi.github.io/en/posts/10/</link><pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/10/</guid><description>matplotlib Image Size Create a figure with the specified size: plt.figure(figsize=(w,h)) Default size: plt.figure(figsize=(6.4,4.8)) Adjust default plot size: import matplotlib plt.rcParams[&amp;#39;figure.figsize&amp;#39;] = [width, height] tmux tmux attach -t 0: Attach to session 0 Ctrl + b + Arrow Keys: Switch to the session in the specified direction Ctrl + b + Number: Switch to the session with the specified number Ctrl + b + d: Return to shell while keeping</description></item><item><title>Sequence Alignment Scores and Gap Penalty</title><link>https://freshrimpsushi.github.io/en/posts/1878/</link><pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1878/</guid><description>Definitions Let&amp;rsquo;s assume we are given a reference sequence and a query sequence. A Sequence Alignment Score is the numerical representation and methodology of how well two sequences match when compared. Scoring is calculated based on weights assigned to the following: Match: The number of times the two sequences align. Mismatch: The number of times the two sequences do not align. Example For instance, consider the two nucleotide sequences shown</description></item><item><title>Substitution Matrix in Sequence Alignment</title><link>https://freshrimpsushi.github.io/en/posts/1881/</link><pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1881/</guid><description>Definition A Substitution Matrix is the matrix used as a criterion for matches and mismatches when scoring sequence alignment. Example using BioAlignments EDNAFULL BLOSUM45 PAM30 Let’s dive straight into the examples. In Julia, there is a package called BioAlignments available, which allows one to easily load the desired substitution matrix. When loading matrices frequently used for DNA analysis like EDNAFULL or BLOSUM (BLOcks SUbstitution Matrix), and</description></item><item><title>Matrix Definitions</title><link>https://freshrimpsushi.github.io/en/posts/1955/</link><pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1955/</guid><description>Definition1 A matrix is an arrangement of numbers in the shape of a rectangle as follows: $$ A=\begin{bmatrix} 10 &amp;amp; 0 &amp;amp; 3 \\ 0 &amp;amp; 8 &amp;amp; 22 \end{bmatrix} $$ Each of the arranged numbers is called an entry or element. A horizontal line is called a row, and a vertical line is called a column. Moreover, if a certain matrix has $m$ rows and $n$ columns, its size</description></item><item><title>Definition of Vectors</title><link>https://freshrimpsushi.github.io/en/posts/1947/</link><pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1947/</guid><description>Definition A sequence of numbers is called a vector. Description In the general curriculum, a vector is learned as a &amp;lsquo;geometric object with magnitude and direction&amp;rsquo;. Since it&amp;rsquo;s the concept you first come across in physics, you inevitably become familiar with vectors of $3$ dimensions or less. $$ (3,4) = \begin{bmatrix} 3 \\ 4 \end{bmatrix} $$ $$ (x,y,z) = \begin{bmatrix} x \\ y \\ z \end{bmatrix} $$ However, vectors can</description></item><item><title>First Steps in Agent-Based Simulation: Representing with Scatter Plots</title><link>https://freshrimpsushi.github.io/en/posts/1875/</link><pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1875/</guid><description>Simulation Code Review Step 1. Load Packages, Set Initial Parameters code1 The code above loads packages and sets the initial number of agents, along with movement rules that allow them to wander around each turn. The amount of movement is defined by the following $2$-dimensional normal distribution. eq1 Step 2. Agent Creation Talking big, calling it agent creation, but in fact, it&amp;rsquo;s nothing much. Drawing samples equal to the number</description></item><item><title>Product Rule Involving the Del Operator</title><link>https://freshrimpsushi.github.io/en/posts/93/</link><pubDate>Thu, 07 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/93/</guid><description>Formulas Let&amp;rsquo;s call $f=f(x,y,z)$ a scalar function. Let&amp;rsquo;s call $\mathbf{A} = A_{x}\hat{\mathbf{x}} + A_{y}\hat{\mathbf{y}} + A_{z}\hat{\mathbf{z}}, \mathbf{B} = B_{x}\hat{\mathbf{x}} + B_{y}\hat{\mathbf{y}} + B_{z}\hat{\mathbf{z}}$ a vector function. Then, the following equations hold. Gradient (a) $\nabla{(fg)}=f\nabla{g}+g\nabla{f}$ (b) $\nabla(\mathbf{A} \cdot \mathbf{B}) = \mathbf{A} \times (\nabla \times \mathbf{B}) + \mathbf{B} \times (\nabla \times \mathbf{A})+(\mathbf{A} \cdot \nabla)\mathbf{B}+(\mathbf{B} \cdot \nabla) \mathbf{A}$ Divergence (c) $\nabla \cdot (f\mathbf{A}) = f(\nabla \cdot \mathbf{A}) + \mathbf{A} \cdot (\nabla f)$ (d)</description></item><item><title>What is Sequence Alignment?</title><link>https://freshrimpsushi.github.io/en/posts/1874/</link><pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1874/</guid><description>Definition Sequence Alignment refers to the process of arranging sequences based on their similarity among base sequences. [^1]</description></item><item><title>Radon Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1945/</link><pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1945/</guid><description>Definition Let&amp;rsquo;s assume that a function $f :D \to \mathbb{R}$ is defined on some 2D domain $D\subset \mathbb{R}^{2}$. The Radon transform $\mathcal{R}f$ of $f$ is defined as follows, for $s \in \mathbb{R}$, $\boldsymbol{\theta} = (\cos \theta, \sin \theta) \in S^{1}$, $$ \begin{align*} \mathcal{R} f(s, \boldsymbol{\theta}):=&amp;amp;\ \int \limits_{t=-\infty}^{\infty} f ( s \boldsymbol{\theta} + t \boldsymbol{\theta}^{\perp} )dt \\ =&amp;amp;\ \int \limits_{t=-\infty} ^{\infty} f \left( s\cos\theta-t\sin\theta, s\sin\theta + t\cos\theta \right)dt \end{align*} $$ Explanation</description></item><item><title>Dynamical Model Simulation</title><link>https://freshrimpsushi.github.io/en/posts/1873/</link><pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1873/</guid><description>Description The gif above visualizes the Malthusian growth model through agent-based simulation. Simulation refers to the practice of experimentally implementing a model that explains a phenomenon in a virtual setting. In the context of dynamical models, simulation typically involves methods such as: Agent-based Model: An agent-based model is an approach that seeks to emulate the macro world through the micro-actions of each actor (agent). Agents act according to their programming,</description></item><item><title>Solution of Differential Equations Using Series Solutions</title><link>https://freshrimpsushi.github.io/en/posts/888/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/888/</guid><description>Description Differential equations with constant coefficients can be relatively easily solved using methods such as separation of variables or integrating factor method. However, differential equations with coefficients that include the independent variable, as shown below, cannot be easily solved. $$ \begin{equation} P(x)\dfrac{d^2 y}{dx^2} + Q(x)\dfrac{dy}{dx}+R(x)y=0 \label{1}\end{equation} $$ Here, $P$, $Q$, and $R$ are assumed to be polynomials without common factors. Equations of the above form include Bessel&amp;rsquo;s equation $$ x^2</description></item><item><title>String Edit Distance</title><link>https://freshrimpsushi.github.io/en/posts/1872/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1872/</guid><description>Buildup 1 Strings have the following four operations: Insertion: Insert a new character into the string. Deletion: Remove a character from the string. Replacement: Replace one character in the string with another. Transposition: Swap the positions of two characters. Definition The edit distance is a distance function between strings, categorized into the following types by allowing or forbidding certain editing methods: (1) Hamming distance: Hamming distance is the simplest, allowing</description></item><item><title>Malthus Growth Model: Ideal Population Growth</title><link>https://freshrimpsushi.github.io/en/posts/1871/</link><pubDate>Thu, 31 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1871/</guid><description>Model $$ \dot{N} = rN $$ Variables $N(t)$: Represents the population size of a group at time $t$. Parameters $r \in \mathbb{R}$ : The Intrinsic Rate of Increase, if greater than $0$, the population grows, if less than $0$, it declines. It can also be defined by the difference $r:=b-d$ between Birth Rate $b$ and Death Rate $d$. Description Population Dynamics is the first pathway through which dynamics leads to</description></item><item><title>Generalized Fourier Coefficients and Fourier Series in Hilbert Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1913/</link><pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1913/</guid><description>Definition[^1] Let $H$ be a Hilbert space, and let $\left\{ u_{\alpha} \right\}_{\alpha\in A}$ be a normal orthogonal system in $H$. Then, for a fixed $x\in H$, let&amp;rsquo;s define the complex function $\hat{x} :A\to \mathbb{C}$ as follows. $$ \hat{x}(\alpha)=\left\langle x,u_{\alpha} \right\rangle $$ The values above are referred to as the Fourier coefficients of $x$ with respect to $\left\{ u_{\alpha} \right\}$.</description></item><item><title>Genomics and Genes in Bioinformatics</title><link>https://freshrimpsushi.github.io/en/posts/1864/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1864/</guid><description>Definition The entire collection of genetic material in an organism is called a genome. A segment of the genome that serves as the unit of genetic traits is called a gene. Specifically, in eukaryotes, it consists of introns and exons. Explanation In fact, the term genome is seldom used as a pure Korean term, and it is commonly referred to as 게놈 or 지놈. Both genome and</description></item><item><title>Inner Product is a Continuous Mapping</title><link>https://freshrimpsushi.github.io/en/posts/1916/</link><pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1916/</guid><description>Theorem1 Let&amp;rsquo;s assume $\left( X, \left\langle \cdot,\cdot \right\rangle \right)$ is an inner product space and $\left\{ \mathbf{x}_{n} \right\}$, $\left\{ \mathbf{y}_{n} \right\}$ are sequences in $X$ converging to $\mathbf{x}$ and $\mathbf{y}$, respectively. Then, the following holds. $$ \left\langle \mathbf{x}_{n},\mathbf{y}_{n} \right\rangle \to \left\langle \mathbf{x},\mathbf{y} \right\rangle \text{ as } n \to \infty $$ Since the limit can move inside and outside of the inner product, we obtain the following corollary. Corollary Assuming that</description></item><item><title>Making GIFs in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1863/</link><pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1863/</guid><description>Code Although the original Sakura Sushi restaurant tends to add much more detailed explanations, to emphasize how easy it is to create animated GIFs in Julia, we will keep this explanation as brief as possible. Even setting aside simulating a random walk, creating an animated GIF like the one above can be very difficult and demanding, depending on the language. However, Julia makes this incredibly easy with the @animate macro</description></item><item><title>Derivatives of Vectors and Matrices</title><link>https://freshrimpsushi.github.io/en/posts/1926/</link><pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1926/</guid><description>Gradient of a Scalar Function Scalar function $f : \mathbb{R}^{n} \to \mathbb{R}$&amp;rsquo;s gradient is as follows. $$ \frac{ \partial f(\mathbf{x})}{ \partial \mathbf{x} } := \nabla f(\mathbf{x}) = \begin{bmatrix} \dfrac{ \partial f(\mathbf{x})}{ \partial x_{1} } &amp;amp; \dfrac{ \partial f(\mathbf{x})}{ \partial x_{2} } &amp;amp; \cdots &amp;amp; \dfrac{ \partial f(\mathbf{x})}{ \partial x_{n} } \end{bmatrix}^{T} $$ Here, $\dfrac{ \partial f(\mathbf{x})}{ \partial x_{i} }$ is the partial derivative of $f$ with respect to $x_{i}$. Inner</description></item><item><title>Intron and Exon in Bioinformatics</title><link>https://freshrimpsushi.github.io/en/posts/1862/</link><pubDate>Fri, 25 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1862/</guid><description>Definition The part of DNA in eukaryotes that actually involves in the synthesis of proteins is referred to as exon, while the part that does not is called intron. Explanation Prokaryotes and eukaryotes are differentiated by the presence or absence of a nuclear membrane in the cell nucleus, but an important difference from the perspective of bioinformatics is whether the process of splicing, which occurs after mRNA is transcribed due</description></item><item><title>Properties of Adjoint Operators</title><link>https://freshrimpsushi.github.io/en/posts/1919/</link><pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1919/</guid><description>Theorem1 Let&amp;rsquo;s call $H,K$ a Hilbert space. For a bounded linear operator $T : K \to H$, $T^{\ast} : H \to K$ is called the adjoint operator of $T$ if it satisfies the following. $$ \left\langle T \textbf{v} , \textbf{w} \right\rangle_{H} = \left\langle \textbf{v} , T^{\ast} \textbf{w} \right\rangle_{K},\quad \forall \textbf{v} \in K $$ The adjoint operator has the following properties. (a) $T^{\ast}$ is linear and bounded. (b) $\left( T^{\ast} \right)^{\ast}</description></item><item><title>Solving Error: 'C:\U' used without hex digits in character string starting 'C:\U' when Reading R Files or Changing Paths</title><link>https://freshrimpsushi.github.io/en/posts/1860/</link><pubDate>Wed, 23 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1860/</guid><description>If you only seek a solution, you might only need to see how to fix it, but if you don&amp;rsquo;t want to encounter the same error again, it is recommended to read everything. Error Diagnosis For instance, when trying to read an example.csv file on the desktop as shown above, the following error may occur. Error: &amp;#39;\U&amp;#39; used without hex digits in character string starting &amp;#34;&amp;#34;C:\U&amp;#34; Despite looking over all</description></item><item><title>Convex Sets in Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1914/</link><pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1914/</guid><description>Definition A subset $M$ of a vector space $V$ is called a convex set if the following equation holds: $$ \lambda x +(1-\lambda)y \in M,\quad \forall \lambda\in[0,1],\ \forall x,y \in M $$ Description Verbally, this equation means &amp;quot;$M$ is a convex set implies that every vector lying between any two vectors in $M$ also belongs to $M$&amp;quot;. Also, if $M$ is a subspace, it is closed under addition and scalar</description></item><item><title>Upstream and Downstream of the Base Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1855/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1855/</guid><description>Buildup 1 The orientation of a nucleotide sequence can be represented by numbering based on the position of carbon atoms in the sugar as shown in the image above. RNA and DNA specifically form chains through the phosphodiester bond between the 3&amp;rsquo; carbon $3&amp;rsquo;$ and the 5&amp;rsquo; carbon $5&amp;rsquo;$. Let&amp;rsquo;s suppose four bases are given with their carbon positions as follows. $$ 3&amp;rsquo;C5' \\ 5&amp;rsquo;A3' \\ 5&amp;rsquo;G3' \\ 5&amp;rsquo;T3' $$</description></item><item><title>Norm Space에서의 Infinite Series Span Total Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1918/</link><pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1918/</guid><description>Infinite Series1 Definition Let $(X, \left\| \cdot \right\|)$ be a normed space. For a sequence $X$ of $\left\{ \mathbf{x}_{k}\right\}_{k\in \mathbb{N}}$, let&amp;rsquo;s define the partial sum as follows. $$ \mathbf{S}_{N} := \sum \limits_{k=1}^{N}\mathbf{x}_{k} $$ If the limit of the partial sum $\mathbf{S}_{N}$ is $\mathbf{x} \in X$, i.e., if it satisfies the following equation $$ \lim \limits_{N\to \infty}\left\| \mathbf{x}-\sum \limits_{k=1}^{N}\mathbf{x}_{k} \right\|=0 $$ then the infinite series $\sum_{k=1}^{\infty}\mathbf{x}_{k}$ is said to converge to</description></item><item><title>딥러닝의 수학적 근거, 시벤코 정리 증명</title><link>https://freshrimpsushi.github.io/en/posts/1853/</link><pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1853/</guid><description>Theorem $\sigma$ is said to be a continuous sigmoidal function if $$ S := \left\{ G(x) = \sum_{k=1}^{N} \alpha_{k} \sigma \left( y_{k}^{T} x+ \theta_{k} \right) : y_{k} \in \mathbb{R}^{n} \land \alpha_{k} , \theta_{k} \in \mathbb{R} \land N \in \mathbb{N} \right\} $$ is uniformly dense in $C\left( I_{n} \right)$. In other words, for all $f \in C \left( I_{n} \right)$ and $\varepsilon &amp;gt; 0$, there exists a $G \in S$ that</description></item><item><title>Properties of Zero in Inner Product Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1917/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1917/</guid><description>Theorem Let&amp;rsquo;s call $\left( X, \left\langle \cdot,\cdot \right\rangle \right)$ an inner space. (a) For all $\mathbf{x}\in X$, the following holds: $$ \left\langle \mathbf{0},\mathbf{x} \right\rangle = 0 $$ (b) For all $\mathbf{x}\in X$, there exists only one element in $X$ that satisfies the following equation: $$ \forall \mathbf{x}\in X,\ \left\langle \mathbf{x},\mathbf{y} \right\rangle = 0 \implies \mathbf{y}=\mathbf{0} $$ (c) Let&amp;rsquo;s call it $\mathbf{y}, \mathbf{\mathbf{z}} \in X$. And $$ \begin{equation} \left\langle \mathbf{x},\mathbf{y} \right\rangle</description></item><item><title>Codon and Amino Acid Genetic Code in Bioinformatics</title><link>https://freshrimpsushi.github.io/en/posts/1852/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1852/</guid><description>Definition A unit consisting of three bases of DNA in sequence is called a triplet code. The triplet code of transcribed mRNA, according to the Central Dogma, is called a codon. Chemically, a molecule that includes an amino group and a carboxyl group, and which serves as the building block of proteins, is called an amino acid. The correspondence between the sequence of codons and amino acids is called the</description></item><item><title>B-spline Scaling Equation</title><link>https://freshrimpsushi.github.io/en/posts/1910/</link><pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1910/</guid><description>Formulas1 (a) B-spline scaling equation: For a B-spline of order $m\in N$, the following equation holds: $$ \widehat{N_{m}}(2\gamma)=H_{0}(\gamma)\widehat{N_{m}}(\gamma),\quad \forall \gamma \in \mathbb{R} $$ Where $H_{0}$ is a function with period $1$, which is described as follows: $$ H_{0}(\gamma)=\left( \frac{1+e^{-2\pi i \gamma}}{2} \right)^{m} $$ Also, the definition of the Fourier transform of $f$, $\widehat{f}$, is as follows: $$ \widehat{f}(\gamma):=\int _{-\infty} ^{\infty} f(x)e^{-2\pi i x \gamma}dx $$ (b) Central B-spline scaling equation:</description></item><item><title>What is a Sigmoid Function?</title><link>https://freshrimpsushi.github.io/en/posts/1851/</link><pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1851/</guid><description>Definition A function $\sigma : \mathbb{R} \to \mathbb{R}$ is called a Sigmoidal Function if it satisfies the following. $$ \sigma (t) \to \begin{cases} 1 &amp;amp; \text{as } t \to + \infty \\ 0 &amp;amp; \text{as } t \to - \infty \end{cases} $$ Explanation of the Definition In the definition of a sigmoidal function, whether it&amp;rsquo;s $0$ or $1$ is not really important, but it&amp;rsquo;s important that it converges to a</description></item><item><title>Central B-spline</title><link>https://freshrimpsushi.github.io/en/posts/1909/</link><pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1909/</guid><description>Definition1 For $m\in \mathbb{N}$, the centered B-spline $B_{m}$ is defined as follows: $$ B_{m}(x):= T_{-\frac{m}{2}}N_{m}(x)=N_{m}(x+{\textstyle \frac{1}{2}}) $$ Here, $T$ is the translation of the space $L^{2}$ space. Description It can also be defined as follows: $$ B_{1}:= \chi_{[-1/2,1/2]},\quad B_{m+1}:=B_{m}*B_{1},\ m\in\mathbb{N} $$ Both definitions actually mean the same function. The key point here is that $B_{m}$ is defined to be an even function. As with the B-spline, it is easy to</description></item><item><title>Principles of Molecular Biology</title><link>https://freshrimpsushi.github.io/en/posts/1850/</link><pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1850/</guid><description>Principles The central principle or the Central Dogma of molecular biology proposes that genetic information is transferred from DNA to RNA, and from RNA to protein, which is comprised of the following three phenomena: Replication: DNA replicates itself. Transcription: RNA, which contains the same information as DNA, is synthesized. Translation: Proteins are synthesized according to the information in RNA. Explanation The Central Dogma was an hypothesis proposed by Francis Crick</description></item><item><title>Regularity of B-Splines</title><link>https://freshrimpsushi.github.io/en/posts/1908/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1908/</guid><description>Theorem1 For $m=2,3,\dots$, the B-spline $N_{m}$ has the following properties. (a) $N_{m}\in C^{m-2}(\mathbb{R})$ (b) For $k\in \mathbb{Z}$, in each interval $[k,k+1]$, $N_{m}$ is at most a polynomial of degree $m-1$. Explicit formula of B-spline $$ N_{m}(x) = \frac{1}{(m-1)!}\sum \limits_{j=0}^{m} \left( -1 \right)^{j}\binom{m}{j}\left( x-j \right)_{+}^{m-1},\quad x\in \mathbb{R} $$ Where $$ f(x)_{+}:=\max \left( 0,f(x) \right) \quad \&amp;amp; \quad f(x)_{+}^{n}:=\left( f(x)_{+} \right)^{n} $$ Lemma For $m=2,3,\cdots$, $x_{+}^{m-1}$ is differentiable up to $m-2$ times,</description></item><item><title>What is a Discriminant Function?</title><link>https://freshrimpsushi.github.io/en/posts/1838/</link><pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1838/</guid><description>Definition A function $\sigma : \mathbb{R} \to \mathbb{R}$ that satisfies $$ \int_{I_{n}} \sigma \left( y^{T} x + \theta \right) d \mu (x) = 0 \implies \mu =0 $$ for all $y \in \mathbb{R}^{n}$ and $\theta \in \mathbb{R}$ and some $\mu \in M \left( I_{n} \right)$ is called a Discriminatory Function. $I_{n} := [0,1]^{n}$ is the $n$-dimensional unit cube, which is the Cartesian product of $n$ unit closed intervals $[0,1]$. $M</description></item><item><title>Explicit Formulas of B-splines</title><link>https://freshrimpsushi.github.io/en/posts/1907/</link><pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1907/</guid><description>Formulas1 For the function $f: \mathbb{R}\to \mathbb{R}$, let&amp;rsquo;s say $$ f(x)_{+}:=\max \left( 0,f(x) \right) $$ That is, $f_{+}$ is a function that replaces all parts of $f$ where the function value is less than $0$, with $0$. Also, let&amp;rsquo;s define $$ f(x)_{+}^{n}:=\left( f(x)_{+} \right)^{n} $$ Then, for each $m=2,3,\dots$, the B-spline $N_{m}$ can be expressed as follows. $$ N_{m}(x) = \frac{1}{(m-1)!}\sum \limits_{j=0}^{m} \left( -1 \right)^{j}\binom{m}{j}\left( x-j \right)_{+}^{m-1},\quad x\in \mathbb{R} $$</description></item><item><title>Regular Measure</title><link>https://freshrimpsushi.github.io/en/posts/1834/</link><pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1834/</guid><description>Definition: Regularity of a Measure 1 Let $\mu$ be a measure defined on a measurable space $(X, \Sigma)$. A measurable set $A \in \Sigma$ is said to be Inner Regular if it satisfies the following: $$ \mu (A) = \sup \left\{ \mu (F) : F \subset A, F \in \Sigma \text{ is compact} \right\} $$ A measurable set $A \in \Sigma$ is said to be Outer Regular if it satisfies</description></item><item><title>Fourier Transform of B-Splines</title><link>https://freshrimpsushi.github.io/en/posts/1906/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1906/</guid><description>The Equation1 The Fourier transform of a B-spline of order $m \in \mathbb{N}$ is given as follows. $$ \widehat{N_{m}}(\gamma)=\left( \frac{1-e^{-2\pi i\gamma}}{2\pi i \gamma} \right)^{m} $$ Here, the definition of the Fourier transform of $f$ is as follows. $$ \widehat{f}(\gamma):=\int _{-\infty} ^{\infty} f(x)e^{-2\pi i x\gamma}dx $$ Explanation Using the properties of B-splines, Fourier transforms, and convolutions, the calculation can be done without much difficulty. Proof First, computing the Fourier transform of</description></item><item><title>Key Bases and Base Pairs in Bioinformatics</title><link>https://freshrimpsushi.github.io/en/posts/1832/</link><pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1832/</guid><description>Definition The following five bases are referred to as the Canonical Bases: Purine bases: Adenine $A$, Guanine $G$ Pyrimidine bases: Cytosine $C$, Thymine $T$, Uracil $U$ Description Thymine is only used in DNA, while Uracil is used in RNA. Therefore, by checking whether $T$ or $U$ is used in the data, one can tell whether it is a DNA or RNA base sequence. A Base Pair is formed by two</description></item><item><title>Orthogonality, Orthogonal Sets, and Orthonormal Sets in Inner Product Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1912/</link><pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1912/</guid><description>Definition1 Let $\left( X, \left\langle \cdot, \cdot \right\rangle \right)$ be an inner product space. If two elements $\mathbf{x}, \mathbf{y}\in X$ satisfy $\left\langle \mathbf{x}, \mathbf{y} \right\rangle =0$, then $\mathbf{y}$ and $\mathbf{x}$ are said to be orthogonal and denoted as follows. $$ \mathbf{x} \perp \mathbf{y} $$ If the set of elements $X$, $\left\{ \mathbf{x}_{k} \right\}_{k\in \mathbb{N}}$, satisfies the following equation, it is called an orthogonal system or an orthogonal set. $$ \left\langle</description></item><item><title>Properties of B-Splines</title><link>https://freshrimpsushi.github.io/en/posts/1904/</link><pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1904/</guid><description>Properties1 The B-spline of order $m\in \mathbb{N}$, denoted $N_{m}$, satisfies the following properties. (a) $\mathrm{supp}N_{m}=[0,m] \quad \text{and} \quad N_{m}(x)&amp;gt;0 \text{ for } x\in(0,m)$ (b) $\displaystyle \int _{-\infty} ^{\infty} N_{m}(x)dx=1$ (c) For $m\ge 2$, the following equation holds. $$ \begin{equation} \sum \limits_{k \in \mathbb{Z}} N_{m}(x-k)=1,\quad \forall x\in \mathbb{R} \end{equation} $$ (c&amp;rsquo;) When $m=1$, the above equation holds for $x\in \mathbb{R}\setminus \mathbb{Z}$. Explanation (c) In other words, $\left\{ N_{m}(x-k) \right\}_{k}$ means that</description></item><item><title>Proof of the LaSalle Invariance Principle</title><link>https://freshrimpsushi.github.io/en/posts/1831/</link><pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1831/</guid><description>Principle Buildup Let&amp;rsquo;s consider a vector field given by the following differential equation for space $X$ and function $f : X \to X$. $$ \dot{x} = f(x) $$ Let’s call the compact positively invariant set under the flow $\phi_t \left( \cdot \right)$ as $\mathcal{M} \subset \mathbb{R}^{n}$. When a Lyapunov function $V : \mathcal{M} \to \mathbb{R}$ is defined as in $\mathcal{M}$, consider the following two sets. $$</description></item><item><title>Attracting Set's Basin</title><link>https://freshrimpsushi.github.io/en/posts/978/</link><pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/978/</guid><description>Definition 1 Let&amp;rsquo;s say the vector field and map for the space $X$ and function $f,g : X \to X$ are represented as follows. $$ \dot{x} = f(x) \\ x \mapsto g(x) $$ Let $\phi (t, \cdot)$ be the flow of the vector field $\dot{x} = f(x)$ and $g^{n}$ be the map $g$ applied $n$ times. The following defined sets are called the Basin of attracting set $A$: Vector Field</description></item><item><title>Properties of Convolution</title><link>https://freshrimpsushi.github.io/en/posts/1901/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1901/</guid><description>Theorem Convolution satisfies the following properties. (a) Commutative Law $$ f \ast g = g \ast f $$ (b) Distributive Law $$ f \ast (g+h) = f \ast g + f \ast h $$ (c) Associative Law $$ f \ast (g \ast h) = (f \ast g) \ast h $$ (d) Scalar Multiplication Associative Law $$ a(f \ast g)=(af\ast g)=(f\ast ag) $$ (e) Differentiation $$ (f\ast g)^{\prime}=f^{\prime}\ast g=f\ast g^{\prime} $$</description></item><item><title>Bioinformatics: DNA Sequencing</title><link>https://freshrimpsushi.github.io/en/posts/1828/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1828/</guid><description>Build-up A polymer is a large molecule composed of repeating monomeric units linked by chemical synthesis. Phosphoric Acid is a type of inorganic oxyacid, with the chemical formula $H_{3}PO_{4}$. A monosaccharide with five carbon atoms is called Pentose. The molecule that functions as the basic unit of genetic information is known as a Nitrogenous base or simply Base. A nucleotide is a molecule that consists of phosphate-pentose-base and becomes the</description></item><item><title>Attractors in Dynamical Systems</title><link>https://freshrimpsushi.github.io/en/posts/1493/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1493/</guid><description>Buildup Let&amp;rsquo;s assume that the vector field and maps are represented as follows for the space $X$ and function $f,g : X \to X$. $$ \dot{x} = f(x) \\ x \mapsto g(x) $$ $\phi (t, \cdot)$ denotes the flow of vector field $\dot{x} = f(x)$, and $g^{n}$ denotes the map obtained by taking map $g$ $n$ times. Definition of Nonwandering1 A point $x_{0} \in X$ is called a nonwandering point,</description></item><item><title>Several Definitions and Notations of Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/1898/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1898/</guid><description>Overview The definition and notation of the Fourier transform vary depending on the needs and preferences of the author. Therefore, before dealing with the Fourier transform in textbooks, lectures, research papers, etc., it is common to clarify the definition and notation. If you skip over the definition thinking it is a concept you know, you might find the equations odd, so it is necessary to check carefully. Of course, the</description></item><item><title>Plancherel's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1899/</link><pubDate>Mon, 30 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1899/</guid><description>Theorem For all $f,g \in L^{2}$, the following equation holds. $$ \begin{align} \langle \hat{f},\hat{g} \rangle &amp;amp;= 2\pi \left\langle f,g \right\rangle \\[1em] \| \hat{f} \|_{2}^{2} &amp;amp;= 2\pi \| f \|_{2}^{2} \end{align} $$ Here $\hat{f}$ is the Fourier transform of $f$. Explanation If expressed in integral form, it is as follows. $$ \begin{align} \int \overline{f(x)}g(x)dx &amp;amp;= \dfrac{1}{2\pi} \int \overline{\hat{f}(\xi)} \hat{g}(\xi) d\xi \tag{1} \\[1em] \int \left| f(x) \right|^{2} dx &amp;amp;= \dfrac{1}{2\pi} \int |</description></item><item><title>Biomedical Informatics: DNA, RNA, Chromosomes</title><link>https://freshrimpsushi.github.io/en/posts/1827/</link><pubDate>Sun, 29 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1827/</guid><description>Buildup Polymers are high molecular compounds formed by the repeating connection of monomers through chemical synthesis. Phosphoric Acid is a type of inorganic oxyacid, with the chemical formula $H_{3}PO_{4}$. Monosaccharides with five carbon atoms are called Pentoses. Molecules that function as the basic unit of genetic information are called Nitrogenous bases, or simply Bases. A molecule composed of phosphate, pentose, and a base, serving as the building block of nucleic</description></item><item><title>Proof That the Space of Test Functions is a Proper Subset of the Schwartz Space</title><link>https://freshrimpsushi.github.io/en/posts/1896/</link><pubDate>Sat, 28 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1896/</guid><description>Theorem1 Let $\mathcal{D}$ be the space of test functions, and let $\mathcal{S}$ be the Schwartz space. Then the following equation holds. $$ \mathcal{D} \subsetneq \mathcal{S} $$ Proof Strategy: First, we show that all test functions belong to the Schwartz space, and then by providing an example of a Schwartz function that is not a test function, we prove the theorem. Schwartz Functions Define $\phi$ as a Schwartz function if it</description></item><item><title>Omega Limit Sets of Autonomous Systems</title><link>https://freshrimpsushi.github.io/en/posts/1822/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1822/</guid><description>Definition For a metric space $X$ and function $f : X \to X$, let&amp;rsquo;s say the following vector field is given by a differential equation. $$ \dot{x} = f(x) $$ Regarding the flow of this system $\phi ( t, x )$ and a point $x_{0} \in X$, if $t_{i} \to \infty$ holds, $$ \phi \left( t_{i} , x_{0} \right) \to x $$ if there exists a sequence of times $\left\{</description></item><item><title>Distributional Convolution Lemma</title><link>https://freshrimpsushi.github.io/en/posts/1894/</link><pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1894/</guid><description>Theorem1 Let $F$ be a distribution, and $\phi,\psi$ be a test function. Then $F \ast \phi$ is a function defined in the real space and is locally integrable. Therefore, there exists a corresponding regular distribution $T$ as follows: $$ T_{F \ast \phi}(\psi)=F(\tilde{\phi} \ast \psi) $$ Here, $\tilde{\phi}(x)=\phi (-x)$. Description The name &amp;lsquo;distribution convolution lemma&amp;rsquo; is arbitrarily given as there&amp;rsquo;s no specific name attached to the content above. Proof Case 1.</description></item><item><title>Prokaryotes and Eukaryotes in Bioinformatics</title><link>https://freshrimpsushi.github.io/en/posts/1810/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1810/</guid><description>Definitions Organisms without a nuclear membrane are called Prokaryotes. Organisms consisting of nuclei with nuclear membranes are called Eukaryotes. Description In eukaryotic organisms, the part containing the genetic material, the Nucleus, and the Cytoplasm, where various metabolisms occur, are distinguished by the Nuclear Envelope. However, prokaryotic cells do not have a distinct boundary called a nuclear membrane but have a Nucleoid. The English notation of prokaryotic organisms signifies that the</description></item><item><title>Convolution of Distributions, Distributions as Functions Defined on Real Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1892/</link><pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1892/</guid><description>Buildup1 The goal of distribution theory is to rigorously define entities like the naively defined Dirac delta function in mathematical terms. As such, it becomes necessary to treat distributions, defined in the function space, as functions defined over the real number space. Initially, let&amp;rsquo;s consider how the differentiation, translation, etc., of distributions have been defined. Since the domain of a distribution is a function space, it is thought that actions</description></item><item><title>How to Compute Distance Matrices in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1799/</link><pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1799/</guid><description>Overview The Distance Matrix is commonly used in simulations based on Particle Dynamics and Moving Agents, but it is often difficult to find a ready-made function for this purpose, and coding it from scratch can be daunting. In Julia, you can easily calculate a distance matrix using the pairwise() function and the Euclidean() function from the Distances package1. The dims option allows you to specify the direction of rows and</description></item><item><title>Necessary and Sufficient Condition for Uniform Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1891/</link><pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1891/</guid><description>Theorem 1 Let us suppose that a sequence of functions $\left\{ f_{n} \right\}$ defined on the metric space $E$ is given. The following two conditions are equivalent. $\left\{ f_{n} \right\}$ converges uniformly on $E$. For all $\varepsilon&amp;gt;0$, there exists a natural number $N$ such that the following equation holds. $$ \begin{equation} \quad m,n\ge N,\ x\in E \implies \left| f_{n}(x)-f_{m}(x) \right| \le \varepsilon \end{equation} $$ Explanation In other words, for all</description></item><item><title>Differentiability of Distributions is Continuous with Respect to Weak Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1890/</link><pubDate>Sun, 22 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1890/</guid><description>Theorem1 The differentiation of a distribution is continuous with respect to weak convergence. In other words, if $T_{k}$ weakly converges to $T$, then $\partial ^{\alpha} T_{k}$ weakly converges to $\partial ^{\alpha}T$. $$ T_{k} \to T \quad \text{weakly} \implies \partial ^{\alpha} T_{k}\to \partial ^{\alpha}T \quad \text{weakly} $$ Here, $\alpha$ is any multi-index. Explanation It means that the differential operator of a distribution satisfies the condition for being continuous with respect to</description></item><item><title>Convergence of Distributions to the Dirac Delta Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1889/</link><pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1889/</guid><description>Theorem1 Let&amp;rsquo;s assume that $f$ is a function satisfying $\int_{\mathbb{R}^{n}} f(\mathbf{x})d\mathbf{x}=1$. And let $f_{\epsilon}(\mathbf{x})=\dfrac{ 1 }{ \epsilon^{n} }f\left( \dfrac{\mathbf{x}}{\epsilon} \right)$. Then, the corresponding regular antifunction $T_{\epsilon}=T_{f_{\epsilon}}$ of $f$ weakly converges to the Dirac delta antifunction. That is, the following holds. $$ \lim \limits_{\epsilon \to 0} T_{\epsilon}=\delta $$ Proof Let $\tilde{f}(\mathbf{x})=f(-\mathbf{x})$. Then, the following holds. $$ \tilde{f_{\epsilon}}(\mathbf{x})=\frac{1}{\epsilon}f\left( -\frac{\mathbf{x}}{\epsilon} \right) \quad \text{and} \quad \int_{\mathbb{R}^{n}}\tilde{f_{\epsilon}}d\mathbf{x}=1 $$ Furthermore, the test function $\phi$ is a</description></item><item><title>Poincaré Recurrence Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/1798/</link><pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1798/</guid><description>Theorem In the Euclidean space where a multidimensional map $g : \mathbb{R}^{n} \to \mathbb{R}^{n}$ is defined to be injective and continuous, and $D \subset \mathbb{R}^{n}$ is a compact invariant set, let&amp;rsquo;s say $g(D) = D$. For any $\overline{x} \in D$, given any neighborhood $U$, there exists $x \in U$ such that for some $n \in$, $g^{n} (x) \in U$ is satisfied. Explanation The statement is simple: if $D$ is a</description></item><item><title>Linear Models for Regression in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/1887/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1887/</guid><description>Definition1 Simple Model Let&amp;rsquo;s define the target function $f : X \to Y$ between the data set $X = \left\{ \mathbf{x}_{i} \right\}$ and the label set $Y = \left\{ y_{i} \right\}$ as follows. $$ y_{i} = f(\mathbf{x}_{i}) $$ In machine learning, linear regression refers to finding a linear function $\hat{f}$ that satisfies the following equation for $\mathbf{w}$. $$ y_{i} \approx \hat{y}_{i} = \hat{f}(\mathbf{x}_{i}, \mathbf{w}) = w_{0} + w_{1}x_{1} + \cdots</description></item><item><title>How to Create an Empty Array in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1797/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1797/</guid><description>Code Size Specification julia&amp;gt; empty = Array{Float64, 2}(undef, 3, 4) 3×4 Array{Float64,2}: 3.39519e-313 3.18299e-313 4.66839e-313 1.061e-313 4.03179e-313 5.51719e-313 1.6976e-313 4.24399e-314 2.97079e-313 4.66839e-313 7.00259e-313 5.0e-324 Executing the code above results in an empty array being created. Occasionally, it may seem like a strange value such as 1.76297e-315 is entered, but this is a value very close to 0, so it&amp;rsquo;s not a major issue for initialization. Array{X, Y}(undef, ...)</description></item><item><title>Operations and Notation Table of Vectors and Matrices</title><link>https://freshrimpsushi.github.io/en/posts/1886/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1886/</guid><description>Overview This document summarizes various notations and operations for vectors and matrices. Vector Vectors are usually denoted in lower case bold type, and unless otherwise stated, they refer to a $n\times 1$ matrix, that is, a column vector. The $i$rd component of vector $\mathbf{x}$ is represented as $x_{i}$. $$ \mathbf{x}=\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix},\quad\mathbf{y}=\begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} $$ Row vectors are</description></item><item><title>Inverse Fourier Transform Theorem for Smooth Functions</title><link>https://freshrimpsushi.github.io/en/posts/1885/</link><pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1885/</guid><description>Theorem Assuming that $f$ is integrable on $\mathbb{R} $ and piecewise smooth, then the following equation holds: $$ \lim \limits_{r\to \infty} \frac{1}{2\pi} \int_{-r}^{r}e^{i\xi x} \hat{f}(\xi)d\xi= \frac{1}{2}\big[f(x-)+f(x+) \big],\quad \forall x\in \mathbb{R} $$ Here, $f(x+)$ and $f(x-)$ are respectively the right-hand limit and left-hand limit of $f$ at $x$. Description The inverse Fourier transform theorem used a cutoff function instead of requiring a relatively weak condition for $f$. The theorem above is</description></item><item><title>Convolution Norm Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1883/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1883/</guid><description>Theorem Let the function $g \in L^{1}$ be bounded and satisfy $\int_{\mathbb{R}}g(y)dy=1$. Assuming $f\in L^{2}$ and that the convolution $f \ast g$ of $f$ and $g$ is well-defined for all $x\in \mathbb{R}$, then $f \ast g_{\epsilon}$ converges in norm to $f$. $$ \begin{equation} \lim \limits_{\epsilon \to 0} \left\| f \ast g_{\epsilon} -f \right\| = 0 \end{equation} $$ In this case, $g_{\epsilon}(y)=\frac{1}{\epsilon}g \left( \frac{y}{\epsilon} \right)$. The name &amp;lsquo;Convolution Norm Convergence Theorem&amp;rsquo;</description></item><item><title>Proof of Liouville's Theorem in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/1792/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1792/</guid><description>Theorem Consider the Euclidean space $\mathbb{R}^{n}$ and the function $f : \mathbb{R}^{n} \to \mathbb{R}^{n}$ given the following vector field represented by a differential equation. $$ \dot{x} = f(x) $$ For the flow $\phi_t ( \cdot )$ of this system and the region $D_{0} \subset \mathbb{R}^{n}$, let $D_{t} := \phi_{t} \left( D_{0} \right)$ denote the region displaced by the flow after time $t$, whose volume is represented as $V(t) \equiv V</description></item><item><title>Laplacian of a Scalar Function in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1882/</link><pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1882/</guid><description>Theorem In the curvilinear coordinate system, the Laplacian of a scalar function $f=f(q_{1},q_{2},q_{3})$ is as follows. $$ \nabla ^{2}f= \frac{1}{h_{1}h_{2}h_{3}}\left[\frac{ \partial }{ \partial q_{1} } \left( \frac{h_{2}h_{3}}{h_{1}} \frac{ \partial f}{ \partial q_{1}}\right)+\frac{ \partial }{ \partial q_{2} } \left( \frac{h_{1}h_{3}}{h_{2}} \frac{ \partial f}{ \partial q_{2}}\right)+\frac{ \partial }{ \partial q_{3} } \left( \frac{h_{1}h_{2}}{h_{3}} \frac{ \partial f}{ \partial q_{3}}\right) \right] $$ Formulas Cartesian coordinates: $$ h_{1}=h_{2}=h_{3}=1 $$ $$ \nabla ^2 f= \frac{ \partial^2</description></item><item><title>Laplacian of a Scalar Function in the Three-Dimensional Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/1879/</link><pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1879/</guid><description>Definition The Laplacian of a 3D scalar function $f=f(x,y,z)$ is the divergence of its gradient $f$ and is denoted by $\nabla^{2}$. $$ \nabla ^{2} f := \nabla \cdot(\nabla f)= \frac{ \partial^{2} f}{ \partial x^{2} }+\frac{ \partial^{2} f}{ \partial y^{2}}+\frac{ \partial^{2} f}{ \partial z^{2}} $$ Explanation The name Laplacian comes from the French mathematician Laplace. The notation $\nabla^{2}$ is used for convenience. In mathematics (theory of partial differential equations), the notation</description></item><item><title>Probability Convergence in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1789/</link><pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1789/</guid><description>Definition 1 A random variable $X$ and a sequence of random variables $\left\{ X_{n} \right\}$ are said to converge in probability to $X$ as $n \to \infty$ if they satisfy the following, and it is denoted by $X_{n} \overset{P}{\to} X$. $$ \forall \varepsilon &amp;gt; 0 , \lim_{n \to \infty} P \left[ \left| X_{n} - X \right| &amp;lt; \varepsilon \right] = 1 $$ Explanation The condition for convergence in probability is</description></item><item><title>Convolution Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1877/</link><pubDate>Sat, 14 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1877/</guid><description>Theorem Let&amp;rsquo;s assume that a function $g \in L^{1}$ satisfies the following condition. $$ \begin{align*} \int_{\mathbb{R}}g(y)dy &amp;amp;= 1 \\ \int_{-\infty}^{0}g(y)dy &amp;amp;= \alpha \\ \int_{0}^{\infty}g(y)dy &amp;amp;=\beta \\ \alpha+\beta &amp;amp;= 1 \end{align*} $$ Furthermore, let&amp;rsquo;s say $f$ is piecewise continuous on $\mathbb{R}$. And either $f$ is bounded, or $g$ outside any interval $[-a,a]$ is $g=0$. That is, the convolution $f \ast g(x)$ is well-defined for all $x\in \mathbb{R}$. Now, let&amp;rsquo;s assume for</description></item><item><title>Conservation Quantities of Autonomous Systems</title><link>https://freshrimpsushi.github.io/en/posts/1770/</link><pubDate>Fri, 13 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1770/</guid><description>Definition Let&amp;rsquo;s assume that we have a vector field given by a differential equation related to a space $X$ and a function $f : X \to X$. $$ \dot{x} = f(x) $$ If there exists a constant function $h : X \to \mathbb{R}$ that is dependent on the given system, it is called a conservation quantity. Explanation If one is familiar with the mechanical sense in physics, the concept of</description></item><item><title>If the Derivative of a Curve is Continuous, the Curve Can Be Measured</title><link>https://freshrimpsushi.github.io/en/posts/1870/</link><pubDate>Thu, 12 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1870/</guid><description>Theorem1 If $\gamma ^{\prime}$ is continuous on the interval $[a,b]$, then $\gamma$ forms a rectifiable curve, and the following equation holds: $$ \Lambda (\gamma) = \int _{a} ^{b} \left| \gamma^{\prime}(t) \right| dt $$ Proof Part 1. Let $P=\left\{ a=x_{0},\dots,x_{n}=b \right\}$ be any partition of the interval $[a,b]$. If we state $a\le x_{i-1}&amp;lt;x_{i}\le b$, then the following is true: $$ \begin{align*} \left| \gamma (x_{i})-\gamma (x_{i-1}) \right| &amp;amp;= \left| \int_{x_{i-1}}^{x_{i}}\gamma^{\prime} (t)dt \right|</description></item><item><title>Measuring Curves: A Guide to Length</title><link>https://freshrimpsushi.github.io/en/posts/1869/</link><pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1869/</guid><description>Definition1 A curve $\gamma : [a,b] \to \mathbb{R}^{k}$ on $\mathbb{R}^{k}$ or simply on $[a,b]$ is called a continuous function. If the curve $\gamma$ is a one-to-one function, it is called an arc. If $\gamma (a)=\gamma (b)$, then $\gamma$ is called a closed curve. Explanation The important point is that the curve is defined as a mapping, not as a collection of points. Now let&amp;rsquo;s define $\Lambda$ for the partition $P=\left\{</description></item><item><title>Proof of the Continuity Mapping Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1787/</link><pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1787/</guid><description>Theorem 1 The following is a measure-theoretic description of the continuous mapping theorem. For metric spaces $\left( S , d \right)$ and $\left( S' , d&amp;rsquo; \right)$, let us say $g : S \to S'$ is continuous from $C_{g} \subset S$. For a random element $X$ in $S$, concerning a sequence of random elements converging to $X$ in $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$, the following holds if $P \left( X</description></item><item><title>Integration of Vector-Valued Functions</title><link>https://freshrimpsushi.github.io/en/posts/1868/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1868/</guid><description>Definition1 Let $f_{1}$, $f_{2}$, $\dots$, $f_{k}$ be functions taking real values on the interval $[a,b]$. And suppose $\mathbf{f} : [a,b] \to \mathbb{R}^{k}$ is defined as follows. $$ \mathbf{f}(x)=\left( f_{1}(x),\dots,f_{k}(x) \right),\quad x\in [a,b] $$ If each $f_{k}$ is integrable on the interval $[a,b]$, then the integral of $\mathbf{f}$ is defined as follows. $$ \int _{a} ^{b} \mathbf{f}dx = \left( \int _{a} ^{b}f_{1} dx, \dots, \int _{a} ^{b}f_{k} dx \right) $$ Theorem</description></item><item><title>Divergence in Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/1777/</link><pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1777/</guid><description>Definition In a Euclidean space, a vector field $\textbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ represented as $\textbf{f} = (f_{1} , \cdots , f_{n})$ and the direction of the axis as $u_{1} , \cdots , u_{n}$, the divergence of $\textbf{f}$ is defined as follows. $$ \operatorname{div} \textbf{f} := \nabla \cdot \textbf{f} = \sum_{k=1}^{n} {{ \partial f_{k} } \over { \partial u_{k} }} $$ Explanation The divergence of a vector field serves as</description></item><item><title>Integration by Parts</title><link>https://freshrimpsushi.github.io/en/posts/1867/</link><pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1867/</guid><description>Theorem 1 Assuming $F$, $G$ are differentiable in the interval $[a,b]$, and $F^{\prime}=f$, $G^{\prime}=g$ are integrable. Then, the following equation holds: $$ \begin{align*} \int _{a} ^{b} F(x)g(x)dx &amp;amp;= F(b)G(b)-F(a)G(a)-\int _{a} ^{b}f(x)G(x)dx \\ &amp;amp;= \left[ F(x)G(x) \right]_{a}^{b} -\int _{a} ^{b}f(x)G(x)dx \end{align*} $$ Description This result is called the integration by parts. Memorizing it as Integration-Differential-Integration makes it easy. What to integrate is kept on both sides as is, and what to</description></item><item><title>The Fundamental Theorem of Calculus in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1866/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1866/</guid><description>Theorem1 Given that function $f$ is Riemann integrable on the interval $[a,b]$, and there exists a function $F$ that is differentiable on $[a,b]$, satisfying $F^{\prime}=f$. Then, the following holds true. $$ \int_{a}^{b} f(x) dx= F(b)-F(a) $$ Explanation This theorem is famously known as the Fundamental Theorem of Calculus Part 2, often abbreviated as FTC2[^Funcamental Theorem of Calculus1]. It implies that the definite integral of $f$ is represented by the difference</description></item><item><title>What is a Logistic Function?</title><link>https://freshrimpsushi.github.io/en/posts/1775/</link><pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1775/</guid><description>Definition [^1] The logistic function is derived as $y ' = y(1-y)$, which is a solution to the differential equation. $$ y(t) = {{ 1 } \over { 1 + e^{-t} }} $$ Explanation In a more general form, it can also be expressed as $\displaystyle f(x) := {{ L } \over { 1 + e^{-k(x-x_{0})} }}$. The logistic function, which is a sigmoid function, is widely mentioned in various</description></item><item><title>Orthogonality of Solutions to the Regular Sturm-Liouville Problem</title><link>https://freshrimpsushi.github.io/en/posts/1859/</link><pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1859/</guid><description>Theorem1 Assume that $\lambda_{n}, \lambda_{m}$ are distinct eigenvalues of the regular S-L problem, and $u_{n}, u_{m}$ are the eigenfunctions corresponding to each eigenvalue with real values. Then, $u_{n}, u_{m}$ are orthogonal to each other in the $L_{w}^{2}(a,b)$ space. That is, $$ \int _{a} ^{b} u_{n}(x)u_{m}(x)w(x)dx=0 $$ Explanation Regular Sturm-Liouville Problem The differential equation $(1)$ is defined on the interval $[a,b]$ and is called a regular Sturm-Liouville problem when it satisfies</description></item><item><title>Volume in Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/1772/</link><pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1772/</guid><description>Definition The volume $V$ of a subspace $D \subset \mathbb{R}^{n}$ in a Euclidean space is defined as follows when expressed in Cartesian coordinates $\textbf{u} = (u_{1}, u_{2}, \cdots , u_{n})$. $$ V(D) = \int_{D} du_{1} du_{2} \cdots d u_{n} $$ When $\textbf{u} \in \mathbb{R}^{n}$ is transformed by a vector function $\textbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ as $\textbf{f} \left( \textbf{u} \right) = \left( f_{1} (\textbf{u}) , \cdots , f_{n} (\textbf{u}) \right)$,</description></item><item><title>Eigenvalues and Eigenfunctions in S-L Problems</title><link>https://freshrimpsushi.github.io/en/posts/1858/</link><pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1858/</guid><description>Definition1 If the Sturm-Liouville differential equation $$ \begin{equation} \left[ p(x)u^{\prime}(x) \right]^{\prime}+\left[ q(x) +\lambda w(x) \right]u(x)=0 \end{equation} $$ has a solution $u \in L_{r}^{2}(a,b)$ different from $0$, then $\lambda$ is called an eigenvalue, and the corresponding $u$ is referred to as the eigenfunction. Explanation Let&amp;rsquo;s assume the weighting function is $w(x)=1$. Then, $(1)$ can be written as follows. $$ \begin{equation} \begin{aligned} &amp;amp;&amp;amp; p(x)u^{\prime \prime}(x) +p^{\prime}(x)u^{\prime}(x)+q(x)u(x)+\lambda u(x) =&amp;amp;\ 0 \\ \implies &amp;amp;&amp;amp;</description></item><item><title>Proof of Poincaré bendixson Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1788/</link><pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1788/</guid><description>Theorem $2$ Consider a manifold $\mathcal{P}$ and a function $f,g \in C^{r} \left( \mathcal{P} \right)$ such that the following vector field is given as a differential equation: $$ \dot{x} = f(x,y) \\ \dot{y} = g(x,y) $$ If $\mathcal{M}$ represents an invariant set with a finite number of fixed points, then the omega limit set $\omega (p)$ of $p \in \mathcal{M}$ satisfies one of the following three conditions: (1): $\omega (p)$</description></item><item><title>Sturm-Liouville Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1857/</link><pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1857/</guid><description>Definition1 Let $p\in$, $C^{1}(\mathbb{R})$(../1594) and assume $q,r\in C(\mathbb{R})$, $\lambda \in \mathbb{R}$. The differential equation of the following form is called a Sturm-Liouville differential equation. $$ \begin{equation} \left[ p(x)u^{\prime}(x) \right]^{\prime}+\left[ q(x) +\lambda w(x) \right]u(x)=0 \end{equation} $$ or $$ p(x)u^{\prime \prime}(x)+p^{\prime}(x)u^{\prime}(x)+\left[ q(x)+\lambda w(x) \right]u(x)=0 $$ Explanation It is also referred to as the S-L problem. Here, $w$ is called the weight function, because it becomes the weight for the inner product in</description></item><item><title>What is a Sigmoid Function?</title><link>https://freshrimpsushi.github.io/en/posts/1769/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1769/</guid><description>Definition 1 A Sigmoid Function is defined as a bounded, differentiable scalar function $\sigma : \mathbb{R} \to \mathbb{R}$ that is defined for all $x \in \mathbb{R}$, is $\sigma ' (x) \ge 0$, and has exactly one inflection point. Sigmoidal functions are defined differently. Types Examples of sigmoid functions include: Logistic function: $\displaystyle f(x) := {{ 1 } \over { 1 + e^{-x} }}$ Hyperbolic tangent: $\tanh x$ Arctangent: $\arctan x$</description></item><item><title>Convolution's General Definition</title><link>https://freshrimpsushi.github.io/en/posts/1848/</link><pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1848/</guid><description>Definition Given the integral transform $J$ and two functions $f$, $g$, a function $f \ast g$ fulfilling the conditions below is defined as the convolution of $f$ and $g$ with respect to $J$. $$ J(f \ast g)=(Jf)(Jg) $$ Explanation According to the definition, the convolution, being the integral transform of a product, can be divided into the product of integral transforms. This means that two functions, which were bound in</description></item><item><title>Absence of Periodic Orbits in Two-Dimensional Autonomous Systems</title><link>https://freshrimpsushi.github.io/en/posts/1761/</link><pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1761/</guid><description>Considerations on Periodic Orbits The question of whether periodic orbits exist in an autonomous system is generally quite complicated, but if we&amp;rsquo;re talking about a $1,2$-dimensional space, we can discuss their absence relatively simply. Let&amp;rsquo;s say we have the following vector field given by a differential equation for spaces $X = \mathbb{R}$ or $X = \mathbb{R}^{2}$ and function $f : X \to X$: $$ \dot{x} = f(x) $$ 1 Dimension</description></item><item><title>Integral Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1847/</link><pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1847/</guid><description>Definition If a map $J$ from a function space to a function space is defined as the following integral, then $J$ is called an integral transform. $$ (Jf) (x) = \int_{a}^{b} K(x,t)f(t)dt $$ $$ J : f(\cdot) \mapsto \int_{a}^{b} K(\cdot,t)f(t)dt $$ In this case, $K$ is referred to as the kernel of $J$. If a map from $Jf$ to $f$ exists, it is denoted as $J^{-1}$ and called the inverse</description></item><item><title>Perceptron Definition</title><link>https://freshrimpsushi.github.io/en/posts/1846/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1846/</guid><description>Definition A perceptron is defined as the composition of a linear function $f(x) = wx + b$ and a unit step function $H$. $$ \text{Perceptron} := H \circ f (x) = H(wx + b) $$ In the case of a multivariable function, $f(\mathbf{x}) = \mathbf{w}\cdot \mathbf{x} + b = w_{1}x_{1} + \cdots w_{n}x_{n} + b$ and, $$ \text{Perceptron} := H \circ f (\mathbf{x}) = H(\mathbf{w} \cdot \mathbf{x} + b) $$</description></item><item><title>Order Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1757/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1757/</guid><description>Theorem1 Let&amp;rsquo;s say that a random sample $X_{1} , \cdots , X_{n}$ has a probability density function $f(x)$ with support $\mathcal{S} =(a,b)$, following a continuous probability distribution. If we arrange these according to their size as $Y_{1} &amp;lt; \cdots &amp;lt; Y_{n}$, then their joint and marginal probability density functions are as follows: [1] Joint: $$ g \left( y_{1} , \cdots , y_{n} \right) = \begin{cases} n! f (y_{1}) \cdots f</description></item><item><title>Prove that Norm is a Continuous Mapping</title><link>https://freshrimpsushi.github.io/en/posts/1845/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1845/</guid><description>Theorem Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ a norm space. Then for a sequence $\left\{ x_{k} \right\}$ of $X$ which is $\lim \limits_{k\to\infty} x_{k} = x$, the following equation holds. $$ \lim \limits_{k \to\infty} \left\| x_{k} \right\| = \left\| x \right\| $$ Explanation $\left\| \cdot \right\|$ means that it is a continuous function. The limit symbol can freely enter and exit the continuous function, which is a very good property.</description></item><item><title>Properties of the Norm Associated with the Inner Product Defined in Inner Space</title><link>https://freshrimpsushi.github.io/en/posts/1844/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1844/</guid><description>Theorem1 Given an inner space $\left( X, \langle \cdot,\cdot \rangle \right)$, one can naturally define the [norm] as in $\left\| \cdot \right\|:=\sqrt{\left\langle \cdot,\cdot \right\rangle }$ and the following properties hold. (a) The Cauchy-Schwarz Inequality: For any $\mathbf{x}, \mathbf{y}\in X$, $$ \left| \langle \mathbf{x},\mathbf{y} \rangle \right| \le \left\| \mathbf{x} \right\| \left\| \mathbf{y} \right\| $$ (b) The Parallelogram Law: For any $\mathbf{x},\mathbf{y}\in X$, $$ \left\| \mathbf{x} + \mathbf{y} \right\|^{2} + \left\| \mathbf{x}</description></item><item><title>Bendixson's Criterion</title><link>https://freshrimpsushi.github.io/en/posts/1751/</link><pubDate>Mon, 26 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1751/</guid><description>Bendixson&amp;rsquo;s Criterion Given a space $\mathbb{R}^{2}$ and a function $f,g \in C^{1} \left( \mathbb{R}^{2} \right)$, let us consider the following vector field as a differential equation: $$ \dot{x} = f(x,y) \\ \dot{y} = g(x,y) $$ In the simply connected region $D \subset \mathbb{R}^{2}$, if $$ {{ \partial f } \over { \partial x }} + {{ \partial g } \over { \partial y }} \ne 0 $$ the sign does</description></item><item><title>Inner Product Spaces and the Cauchy-Schwarz Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1843/</link><pubDate>Mon, 26 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1843/</guid><description>Theorem1 Let $(H, \langle \cdot ,\cdot \rangle)$ be an inner product space. Then, the following inequality holds, and it is called the Cauchy-Schwarz inequality. $$ \left| \langle x,y \rangle \right| \le \langle x,x \rangle^{1/2} \langle y,y \rangle ^{1/2},\quad \forall x,y \in H $$ Explanation Since a norm can be defined from the inner product, it can also be expressed as the following equation. $$ \left| \left\langle x, y \right\rangle \right|</description></item><item><title>Inner product spaces</title><link>https://freshrimpsushi.github.io/en/posts/1842/</link><pubDate>Sun, 25 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1842/</guid><description>Definition1 Let&amp;rsquo;s consider $X$ as a vector space. For $\mathbf{x}, \mathbf{y}, \mathbf{z} \in X$ and $\alpha, \beta \in \mathbb{C}$(or $\mathbb{R}$), the following conditions satisfied by a function $$ \langle \cdot , \cdot \rangle : X \times X \to \mathbb{C} $$ are defined as the inner product, and $\left( X, \langle \cdot ,\cdot \rangle \right)$ is called an inner product space. Linearity: $$\langle \alpha \mathbf{x} + \beta \mathbf{y} ,\mathbf{z} \rangle =\alpha</description></item><item><title>Reason for Dividing the Sample Variance by n-1</title><link>https://freshrimpsushi.github.io/en/posts/1747/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1747/</guid><description>Why divide by n-1? If we denote it as $X_{i} \sim \left( \mu , \sigma^{2} \right)$, the sample variance $S^{2}$ can be represented as follows. $$ S^{2} := {{1} \over {n-1}} \sum_{i=1}^{n} \left( X_{i} - \overline{X} \right)^{2} $$ As is well known, unlike the sample mean, the sample variance sums up the squares of the deviations and then divides not by the sample size $n$, but by $n-1$. While I</description></item><item><title>Semi-Linear (Conjugate Linear) Functions</title><link>https://freshrimpsushi.github.io/en/posts/1841/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1841/</guid><description>Definition Assuming that a function $f : X \to \mathbb{C}$ is given. If the following equation holds for $x,y\in X$, $a,b \in \mathbb{C}$, then $f$ is called antilinear or conjugate linear. $$ f(ax + by)=\overline{a}f(x)+\overline{b}f(y) $$ Explanation Unlike linear functions, where the multiplied constant is the same inside and outside the function, it refers to a function in which the constant is the conjugate complex number inside and outside the</description></item><item><title>Relations among Inner Product Spaces, Normed Spaces, and Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1840/</link><pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1840/</guid><description>Description Let&amp;rsquo;s say an inner space $\left( X, \langle\cdot, \cdot\rangle \right)$ is given. Then, one can naturally define a norm as follows from the inner product. $$ \begin{equation} \left\| x \right\| := \sqrt{ \langle x, x\rangle},\quad x\in X \end{equation} $$ Hence, if it is an inner space, then it&amp;rsquo;s a normed space. Subsequently, one can define a distance from the norm thus defined. $$ \begin{equation} d(x,y):=\left\| x -y \right\| =\sqrt{</description></item><item><title>Stability of Invariant Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/1746/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1746/</guid><description>Definition Stability and Instability of Invariant Sets 1 Let&amp;rsquo;s define the two invariant sets with respect to a fixed point $\overline{x}$ of a dynamical system $\left( T, X , \varphi^{t} \right)$ as follows. $$ \begin{align*} W^{s} \left( \overline{x} \right) :=&amp;amp; \left\{ x : \varphi^{t} x \to \overline{x} , t \to + \infty \right\} \\ W^{u} \left( \overline{x} \right) :=&amp;amp; \left\{ x : \varphi^{t} x \to \overline{x} , t \to -</description></item><item><title>What is Computer Vision</title><link>https://freshrimpsushi.github.io/en/posts/1839/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1839/</guid><description>Explanation Computer vision is the research area that allows computers to perform functions corresponding to human vision, mainly dealing with images and videos. The conferences specialized in computer vision include ICCV (International Conference on Computer Vision), ECCV (European Conference on Computer Vision), and CVPR (Conference on Computer Vision and Pattern Recognition). The problems primarily handled in computer vision can be classified into three major categories, as shown in the picture</description></item><item><title>Continuous Learning in Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/1837/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1837/</guid><description>Explanation Continual learning in deep learning refers to the sequential learning of multiple tasks by artificial neural networks, synonymous with lifelong learning or incremental learning. Unlike humans, who do not forget existing knowledge simply by learning something new – though they may forget over time, not due to the acquisition of new knowledge – artificial neural networks exhibit a decline in performance on previously learned tasks after sufficiently learning one</description></item><item><title>Unbiased Estimator</title><link>https://freshrimpsushi.github.io/en/posts/1745/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1745/</guid><description>Definition 1 If the estimator $T$ of $\theta$ satisfies the following, then $T$ is called the unbiased estimator of $\theta$. $$ E T = \theta $$ Explanation Especially, among the unbiased estimators for $\theta$, the one with the smallest variance is called the minimum variance unbiased estimator. Unbiasedness refers to the property of not having any bias. For example, when we assume $X_{i} \sim \left( \mu , \sigma^{2} \right)$, if</description></item><item><title>Mellin Transform Convolution</title><link>https://freshrimpsushi.github.io/en/posts/1835/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1835/</guid><description>Definition The convolution of the Mellin transform is as follows. $$ (f \times g) (y) = \int _{0}^{\infty} f(x)g \left(\frac{y}{x} \right)\frac{dx}{x} $$ Explanation It is also called multiplicative convolution1. Proof $$ \mathcal{M}(f \times g)=(\mathcal{M}f)(\mathcal{M}g) $$ It suffices to show that the above equation holds. $$ \begin{align*} \mathcal{M}(f\times g)(s) &amp;amp;= \int _{0} ^{\infty} x^{s-1} (f\times g)(x)dx \\ &amp;amp;= \int _{0} ^{\infty} x^{s-1} (f\times g)(x)dx \\ &amp;amp;= \int _{0} ^{\infty} x^{s-1} \left(</description></item><item><title>Classification of Discontinuities</title><link>https://freshrimpsushi.github.io/en/posts/1833/</link><pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1833/</guid><description>Definition1 Let&amp;rsquo;s assume the function $f :X \to \mathbb{R}$ is given in a metric space $X$. If $f$ is not continuous at $x\in X$, it is said that $f$ is discontinuous at $x$ or has a discontinuity at $x$. Let&amp;rsquo;s say $f: (a,b) \to \mathbb{R}$. If $f$ is discontinuous at $x\in (a,b)$ and the left/right limits $f(x-)$, $f(x+)$ at $x$ exist, it is said that $f$ has a discontinuity of</description></item><item><title>Invariant Sets in Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/1079/</link><pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1079/</guid><description>Definition Rigorous Definition 1 In a dynamical system $\left( T, X , \varphi^{t} \right)$, if a subset $S \subset X$ satisfies the following conditions $S$, it is called an invariant set. $$ x_{0} \in S \implies \varphi^{t} x_{0} \in S \qquad , \forall t \in T $$ Vector Fields and Maps 2 Let us denote a space $X$ and a function $f,g : X \to X$, and consider that a</description></item><item><title>Limits from the Left and the Right Strictly Defined in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1830/</link><pubDate>Sat, 17 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1830/</guid><description>Definition Let&amp;rsquo;s assume a function $f :X \to \mathbb{R}$ is given in a metric space $X$. If $f$ is not continuous at $x\in X$, $f$ is said to be discontinuous at $x$ or to have a discontinuity at $x$. $f: (a,b) \to \mathbb{R}$ is assumed. For any point $x$, let $a \le x &amp;lt;b$. Consider a sequence of points $(x,b)$ that converges to $x$ and call it $\left\{ t_{n} \right\}$.</description></item><item><title>Convenience-Variance Trade-off</title><link>https://freshrimpsushi.github.io/en/posts/1739/</link><pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1739/</guid><description>Definition $$ \text{MSE} \left( \widehat{\theta} \right) = \Var \widehat{\theta} + \left( \text{Bias} \widehat{\theta} \right)^{2} $$ Description Mean Squared Error (MSE) $\text{MSE}$ is frequently used as a measure for evaluating statistical models or as a loss function in machine learning, represented especially in terms of trade-offs between bias and variance. Handling bias may seem somewhat uncomfortable for a statistician. While dealing with variance feels almost tangible, based on the assumption of</description></item><item><title>The Relationship between Derivatives and the Increasing/Decreasing of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1826/</link><pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1826/</guid><description>정리 Let the function $f$ be differentiable at $(a,b)$. If for all $x\in (a,b)$, $f^{\prime}(x) \ge 0$ holds, then $f$ is monotonically increasing. If for all $x\in (a,b)$, $f^{\prime}(x)=0$ holds, then $f$ is a constant function. If for all $x\in (a,b)$, $f^{\prime}(x) \le 0$ holds, then $f$ is monotonically decreasing. Proof From the Mean Value Theorem, it follows that for all $x_{1},x_{2}\in (a,b)$ and $x \in (x_{1},x_{2})$ the following</description></item><item><title>Mean Value Theorem in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1824/</link><pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1824/</guid><description>Theorem Let the functions $f$ and $g$ be continuous on the interval $[a,b]$ and differentiable on $(a,b)$. Then there exists $x \in (a,b)$ that satisfies the following equation. $$ [f(b)-f(a)]g^{\prime}(x)=[g(b)-g(a)]f^{\prime}(x) $$ Note that differentiability is not necessary at the endpoints $a$ and $b$. Explanation This is a generalization of the Mean Value Theorem learned in high school and in calculus. If we set it as $g(x)=x$, it becomes the familiar</description></item><item><title>Monotonic Functions, Increasing Functions, Decreasing Functions</title><link>https://freshrimpsushi.github.io/en/posts/848/</link><pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/848/</guid><description>Definition Let&amp;rsquo;s assume the function $f:[a,b] \rightarrow \mathbb{R}$ is given. For $x_{1}$, $x_{2}$, $\in [a,b]$ $$ x_{1} \lt x_{2} \ \implies f(x_{1}) \le f(x_{2}) $$ If it satisfies, then $f$ is said to be monotonically increasing or $f$ is called a monotone increasing function. Conversely, $$ x_{1} \lt x_{2} \ \implies f(x_{1}) \ge f(x_{2}) $$ If it satisfies, then $f$ is said to be monotonically decreasing or $f$ is called</description></item><item><title>Definition and Relationship of Extremum in Analysis and Differential Coefficients</title><link>https://freshrimpsushi.github.io/en/posts/1699/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1699/</guid><description>Definition Let $(X,d)$ be a metric space. If there exists a positive real number $\delta &amp;gt;0$ such that the function $f : X \rightarrow \mathbb{R}$ satisfies the condition below, then $f$ has a local maximum at point $p \in X$. $$ \forall q\in X,\quad f(q)\le f(p)\ \mathrm{with}\ d(p,q)&amp;lt;\delta $$ Explanation To put it in words: If $f(p)$ is the largest within a distance of $\delta$ from $p$, then $f(p)$ is</description></item><item><title>Lyapunov Function</title><link>https://freshrimpsushi.github.io/en/posts/1738/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1738/</guid><description>Definition1 Given a space $X$ and a function $f : X \to X$, suppose the following vector field is given by a differential equation. $$ \dot{x} = f(x) $$ For a point $x_{0} \in X$ in the above autonomous system, a scalar function $V \in C^{1} \left( \mathcal{N} (x_{0}) , \mathbb{R} \right)$ defined in the neighborhood $\mathcal{N} \left( x_{0} \right)$ of $x_{0}$ is called a Liapunov function if it satisfies</description></item><item><title>The Chain Rule of Differentiation in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1823/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1823/</guid><description>Theorem1 If $f :[a,b] \to \mathbb{R}$ is a continuous function and is differentiable at $x\in [a,b]$, and if $g : f([a,b])\to \mathbb{R}$ is differentiable at $f (x)\in f([a,b])$, and if we define $h : [a,b] \to \mathbb{R}$ as follows. $$ h(t)=g\left( f(t) \right)\quad (a\le t \le b) $$ Then, $h$ is differentiable at $x$ and its value is as follows. $$ h^{\prime}(x)=g^{\prime}(f(x))f^{\prime}(x) $$ Using the composite function symbol, it can</description></item><item><title>Differentiable Function Properties</title><link>https://freshrimpsushi.github.io/en/posts/1821/</link><pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1821/</guid><description>Theorem1 Let&amp;rsquo;s say $f, g : [a,b] \to \mathbb{R}$. If $f,g$ is differentiable at $x\in [a,b]$, then $f+g$, $fg$, and $f/g$ are also differentiable at $x$ and the following equation holds. $$ \begin{align} (f+g)^{\prime}(x) &amp;amp;=f^{\prime}(x)+g^{\prime}(x) \\ (fg)^{\prime}(x) &amp;amp;= f^{\prime}(x)g(x)+f(x)g^{\prime}(x) \\ \left( \frac{f}{g} \right)^{\prime}(x) &amp;amp;= \frac{f^{\prime}(x)g(x)-f(x)g^{\prime}(x)}{g^{2}(x)} \end{align} $$ However, $(3)$ holds when $g(x)\ne 0$. Description $(2)$ is commonly referred to as the product rule of differentiation. Proof $(1)$ By the definition</description></item><item><title>Convenience in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1735/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1735/</guid><description>Definition A bias $\text{Bias}$ is defined as follows for an estimator $\widehat{\theta}$ of a parameter $\theta$. $$ \text{Bias} ( \theta ) = E(\widehat{\theta}) - \theta $$ Description While the term Bias can be purified into bias or tendency, the most commonly used term is Bias, pronounced as it is. In Korean, &amp;ldquo;편의(Convenien</description></item><item><title>If Differentiable, Then Continuous</title><link>https://freshrimpsushi.github.io/en/posts/1820/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1820/</guid><description>Theorem1 Let&amp;rsquo;s say $f : [a,b] \to \mathbb{R}$. If $f$ is differentiable at $p \in [a,b]$, then $f$ is continuous at $p$. Explanation Note that the converse &amp;lsquo;if it is continuous, it is differentiable&amp;rsquo; does not hold. In the past, there was a pun called Simple Integration (simply put, if it&amp;rsquo;s differentiable, then it&amp;rsquo;s continuous) among older students, but I wonder if this pun has become unused as current students</description></item><item><title>Regulating Supersaturation</title><link>https://freshrimpsushi.github.io/en/posts/1819/</link><pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1819/</guid><description>Definition1 A continuous linear functional $T:\mathcal{S}(\mathbb{R}^{n}) \to \mathbb{C}$ on the Schwartz space is called a tempered distribution. In other words, a tempered distribution is an element of the dual space of the Schwartz space. Therefore, $$ T \in \mathcal{S}^{ \ast } $$ is denoted as such, and $\mathcal{S}^{ \ast }$ is called the space of tempered distributions. Description Since a tempered distribution $T$ is linear, the following holds true. $$</description></item><item><title>Classification of Fixed Points in Autonomous Systems</title><link>https://freshrimpsushi.github.io/en/posts/1733/</link><pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1733/</guid><description>Definition Given a space $X$ and a function $f \in C^{1}(X,X)$, consider the following vector field given as a differential equation: $$ \dot{x} = f(x) $$ Let $\overline{x}$ be a fixed point of this autonomous system and the eigenvalues of $D f \left( \overline{x} \right)$ be described as $\lambda_{1} , \cdots , \lambda_{m}$. Hyperbolic: Hyperbolic Fixed Points1 Hyperbolic: If the real parts of all eigenvalues of $D f \left( \overline{x}</description></item><item><title>Fourier Series in Complex Notation</title><link>https://freshrimpsushi.github.io/en/posts/964/</link><pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/964/</guid><description>Formulas The complex Fourier series of a function $f$ defined over the interval $[-L,\ L)$ is given by: $$ f(t) = \sum \limits_{n=-\infty}^{\infty} c_{n} e^{i\frac{n\pi t}{L}} $$ Here, the complex Fourier coefficients are as follows: $$ c_{n} = \dfrac{1}{2L}\int_{-L}^{L}f(t)e^{-i\frac{n \pi t}{L} }dt $$ The Fourier coefficients satisfy the following equation: $$ \begin{align*} a_{0} &amp;amp; = 2 c_{0} \\ a_{n} &amp;amp;= c_{n}+c_{-n} \\ b_{n} &amp;amp;= i(c_{n}-c_{-n}) \\ c_{n} &amp;amp;= \frac{1}{2} (a_{n}-ib_{n})</description></item><item><title>Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1818/</link><pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1818/</guid><description>Definitions1 Partial Differential Equations For a natural number $k \in \mathbb{N}$ and an open set $U \subset \mathbb{R}^{n}$, the following expression is called a $k$-order partial differential equation. $$ \begin{equation} F(D^{k}u(x), D^{k-1}u(x),\cdots,Du(x),u(x),x)=0\quad (x\in U) \end{equation} $$ Here, $D^{k}u$ is the multi-index notation. $F$ is given as follows, and the unknown $u$ is as follows. $$ F : {\mathbb{R}}^{n^{k}}\times{\mathbb{R}}^{n^{k-1}}\times \cdots \times \mathbb{R}^{n}\times \mathbb{R}\times U \to \mathbb{R} \\ u : U \to</description></item><item><title>Divergence of Vector Functions in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1817/</link><pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1817/</guid><description>Theorem The divergence of the vector function $\mathbf{F}=\mathbf{F}(q_{1},q_{2},q_{3})=F_{1}\hat{\mathbf{q}}_{1}+F_{2}\hat{\mathbf{q}}_{2}+F_{3}\hat{\mathbf{q}}_{3}$ in curvilinear coordinates is as follows. $$ \nabla \cdot \mathbf{F}=\frac{1}{h_{1}h_{2}h_{3}}\left[ \frac{ \partial }{ \partial q_{1} }(h_{2}h_{3}F_{1})+\frac{ \partial }{ \partial q_{2} }(h_{1}h_{3}F_{2})+\frac{ \partial }{ \partial q_{3} }(h_{1}h_{2}F_{3}) \right] $$ $h_{i}$ is the scale factor. Formulas Cartesian coordinates: $$ h_{1}=h_{2}=h_{3}=1 $$ $$ \begin{align*} \nabla \cdot \mathbf{F} =\frac{\partial F_{x}}{\partial x}+\frac{\partial F_{y}}{\partial y}+\frac{\partial F_{z}}{\partial z} \end{align*} $$ Cylindrical coordinates: $$ h_{1}=1,\quad h_{2}=\rho,\quad h_{3}=1 $$ $$ \begin{align*}</description></item><item><title>Easy Definition of Confidence Intervals</title><link>https://freshrimpsushi.github.io/en/posts/1732/</link><pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1732/</guid><description>Definition 1 Let the probability density function $f (x; \theta)$ of the random variable $X$ and the samples $X_{1} , \cdots , X_{n}$ with a Confidence Coefficient $\alpha \in (0,1)$ be given. $$ L := L \left( X_{1} , \cdots , X_{n} \right) \\ U := U \left( X_{1} , \cdots , X_{n} \right) $$ It is said that the statistic $L &amp;lt; U$ is defined as above, then the</description></item><item><title>Gradient of a Scalar Function in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1816/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1816/</guid><description>Theorem In a curvilinear coordinate system, the gradient of a scalar function $f=f(q_{1},q_{2},q_{3})$ is as follows. $$ \nabla f= \frac{1}{h_{1}}\frac{ \partial f }{ \partial q_{1} } \hat{\mathbf{q}}_{1} + \frac{1}{h_{2}}\frac{ \partial f }{ \partial q _{2}}\hat{\mathbf{q}}_{2}+\frac{1}{h_{3}}\frac{ \partial f }{ \partial q_{3} } \hat{\mathbf{q}}_{3}=\sum \limits _{i=1} ^{3}\frac{1}{h_{i}}\frac{ \partial f}{ \partial q_{i}}\hat{\mathbf{q}}_{i} $$ $h_{i}$ is the scale factor. Formulas Cartesian Coordinate System: $$ h_{1}=h_{2}=h_{3}=1 $$ $$ \nabla f= \frac{\partial f}{\partial x}\mathbf{\hat{\mathbf{x}} }+ \frac{\partial</description></item><item><title>Spline, B-Spline in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1815/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1815/</guid><description>Definition1 If the function $f:\mathbb{R} \to \mathbb{R}$ is a piecewise polynomial on interval $\mathbb{R}$, it is called a spline on $\mathbb{R}$. The points where the polynomial changes are called knots. Explanation As can be seen from the definition, a spline does not have to be a continuous function. The following function $f$ is an example of a spline. $$ f(x) = \begin{cases} 0 &amp;amp; x\in[\infty,0] \\ 2x^{2}&amp;amp;x\in(0,1] \\ 2-x &amp;amp;</description></item><item><title>The Van der Pol Oscillator</title><link>https://freshrimpsushi.github.io/en/posts/1731/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1731/</guid><description>Duffing Equation1 $$ \ddot{x} + \delta \dot{x} + \alpha x + \beta x^{3} = \gamma \cos \left( \omega t \right) $$ Variables $t$: Represents time. $x$: Represents position in (for example, a particle&amp;rsquo;s) $1$-dimensional space. $\dot{x}$: Represents (a particle&amp;rsquo;s) velocity. $\ddot{x}$: Represents (a particle&amp;rsquo;s) acceleration. Parameters $\delta$: Controls damping, similar to friction. $\alpha$: Controls stiffness, similar to a spring constant, affecting potential energy. $\beta$: Controls nonlinear restoring force; when $\beta</description></item><item><title>Unit Partition in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1814/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1814/</guid><description>Definition A set $\left\{ u_{\alpha} \right\}_{\alpha \in \Lambda}$ of continuous functions $u_{\alpha} : X \to [0,1], \alpha \in \Lambda$ that satisfies the conditions below is called a Partition of Unity. $$ \sum _{\alpha \in \Lambda}u_{\alpha}(x)=1 $$</description></item><item><title>Statistical Measures and Estimators in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1730/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1730/</guid><description>Definition 1 2 A function $T$ of a sample $X_{1} , \cdots , X_{n}$ from a random variable $X$ is called a Statistic. $$ T := T \left( X_{1} , \cdots , X_{n} \right) $$ When the distribution function of $X$ is expressed as $f(x; \theta)$ or $p(x; \theta)$, if $T$ serves to capture $\theta$, then $T$ is referred to as an Estimator of $\theta$. The probability distribution of a</description></item><item><title>Zeeman Effect</title><link>https://freshrimpsushi.github.io/en/posts/1813/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1813/</guid><description>설명 1897년 네덜란드 물리학자 피테르 제이만이 발견한 현상으로 원자가 자기장 내에 있을 때 방출 스펙트럼 선이 갈라지는 것을 말한다. 패러데이가 1860년에 나</description></item><item><title>Spectrum and Fraunhofer Lines</title><link>https://freshrimpsushi.github.io/en/posts/1812/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1812/</guid><description>Explanation1 2 3 4 A spectrum refers to the object of study in spectroscopy, which is light decomposed into various colors. The first spectrum picture one often sees is a line spectrum like the one above, either long horizontally or vertically. However, the spectrum was not always in this form. The first person to discover that light could be decomposed into various colors through a prism was Newton. Because Newton</description></item><item><title>Lyapunov Stability and Orbit Stability</title><link>https://freshrimpsushi.github.io/en/posts/1716/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1716/</guid><description>Definition Liapunov Stability 1 Given a metric space $\left( X , \left\| \cdot \right\| \right)$ and a function $f : X \to X$, consider the following vector field presented as a differential equation: $$ \dot{x} = f(x) $$ Let $t_{0} \in \mathbb{R}$. If for every solution $\overline{x}(t)$ of the given differential equation, when $\varepsilon &amp;gt; 0$ is given, $$ \left\| \overline{x} \left( t_{0} \right) - y \left( t_{0} \right) \right\|</description></item><item><title>Spectroscopy</title><link>https://freshrimpsushi.github.io/en/posts/1811/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1811/</guid><description>Explanation1 2 As shown in the above gif, breaking light into various colors is called a spectrum, or spectroscopy in Korean. Spectroscopy is a branch of optics that focuses on observing and studying visible light that has been dispersed according to wavelength. However, its meaning has recently expanded to include the measurement and study of any physical quantity based on wavelength or frequency. The concept of the spectrum has also</description></item><item><title>Paper Review: Do We Need Zero Training Loss After Achieving Zero Training Error?</title><link>https://freshrimpsushi.github.io/en/posts/1809/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1809/</guid><description>Paper Review Flooding refers to a regularization technique introduced in Do We Need Zero Training Loss After Achieving Zero Training Error?, presented at ICML 2020. According to the authors of the paper, the root cause of overfitting is the excessively low training loss as illustrated below. Thus, the core idea of the paper is that controlling the training loss not to fall below a certain value during the learning process,</description></item><item><title>Commonly Used Datasets in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/1808/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1808/</guid><description>Computer Vision MNIST This is the first dataset that one encounters when studying machine learning. It is pronounced as [em-nist] and consists of hand-written digit images of size $28\times 28$. The dataset includes 60,000 training images and 10,000 testing images[^1]. CIFAR-10, CIFAR-100 CIFAR-10, pronounced as [cypher-ten], includes 60,000 images in 10 different categories, with images of size $32\times 32$. It is composed of 50,000 training images and 10,000 testing images.</description></item><item><title>Random Sampling in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1715/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1715/</guid><description>Definitions 1 The actual outcome of a random variable $X$ is called its realization and is usually represented by the lowercase letter $x$. A set of random variables from the same probability distribution as $X$, with a sample size of $n$, is called a sample, represented as follows: $$ X_{1} , X_{2} , \cdots , X_{n} $$ If the random variable $X_{1} , \cdots , X_{n}$ is iid, then a</description></item><item><title>What is Overfitting and Regularization in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/1807/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1807/</guid><description>Overfitting The phenomenon where the training loss decreases, but the test loss (or validation loss) does not decrease or rather increases is called overfitting. Explanation There is also a term called underfitting, which basically means the opposite, but frankly, it&amp;rsquo;s a meaningless term and not often used in practice. A crucial point in machine learning is that the function trained with the available data must also work well with new</description></item><item><title>Curved Coordinate Systems: Coordinate Transformations and Jacobians</title><link>https://freshrimpsushi.github.io/en/posts/1806/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1806/</guid><description>Formulas The volume in a 3-dimensional Cartesian coordinate system is represented for any curvilinear coordinate system as follows. $$ dxdydz =\begin{vmatrix} \dfrac{ \partial x}{ \partial q_{1}} &amp;amp; \dfrac{ \partial y}{ \partial q_{1}} &amp;amp; \dfrac{ \partial z}{ \partial q_{1} } \\[1em] \dfrac{ \partial x}{ \partial q_{2}} &amp;amp; \dfrac{ \partial y}{ \partial q_{2}} &amp;amp; \dfrac{ \partial z}{ \partial q_{2} } \\[1em] \dfrac{ \partial x}{ \partial q_{3}} &amp;amp; \dfrac{ \partial y}{ \partial q_{3}}</description></item><item><title>Linearization of Nonlinear Systems</title><link>https://freshrimpsushi.github.io/en/posts/1709/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1709/</guid><description>Buildup Let&amp;rsquo;s say a vector field is given as a differential equation with respect to space $\left( X, \left\| \cdot \right\| \right)$ and function $f : X \to X$. $$ \dot{x} = f(x) $$ When the fixed point of such an autonomous system $\overline{x}$ is given, it is essential to use a method called linearization to understand the stability around it. The idea is to analyze the system linearly around</description></item><item><title>Schwartz Space and Schwartz Functions</title><link>https://freshrimpsushi.github.io/en/posts/1805/</link><pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1805/</guid><description>Definition The set of functions $\phi : \mathbb{R}^{n} \to \mathbb{C}$ that satisfy the following two conditions is called the Schwartz space, denoted by $\mathcal{S}(\mathbb{R}^{n})$. An element $\phi$ of the Schwartz space is called a Schwartz function. (a) $\phi \in $ $C^{\infty}$ (b) For any multi-index $\alpha$, $\beta$, the following holds: $\left| \mathbf{x}^{\beta}D^{\alpha}\phi (\mathbf{x}) \right| &amp;lt;\infty$. Here, for $\beta=(\beta_{1}, \beta_{2},\dots,\beta_{n})$, $$ \mathbf{x}^{\beta}=x_{1}^{\beta_{1}}x_{2}^{\beta_{2}}\dots x_{n}^{\beta_{n}} $$ (b) can be rewritten as: $$ \mathbf{x}^{</description></item><item><title>Curvilinear Coordinates in Three-Dimensional Space</title><link>https://freshrimpsushi.github.io/en/posts/1774/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1774/</guid><description>Buildup The most common way to express a position in three-dimensional space is the Cartesian coordinate system. Named after Descartes, who devised it, it is also widely known as the orthogonal coordinate system. However, in specific situations, it might be difficult to represent the position using the Cartesian coordinate system. For instance, let&amp;rsquo;s consider an object performing rotational motion on a two-dimensional plane. Then, it would be much simpler to</description></item><item><title>Derivative Approximation</title><link>https://freshrimpsushi.github.io/en/posts/1085/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1085/</guid><description>Buildup Let&amp;rsquo;s recall the idea of defining the differentiation of distributions. There exists a regular distribution $T_{u}$ for $u \in {L}_{\mathrm{loc}}^1(\Omega)$. If $u$ is differentiable, by applying the integration by parts, the following equation holds, and the derivative of $T_{u}$ is defined as $T_{u^{\prime}}$, which corresponds to the derivative of $u$, $u^{\prime}$. $$ \begin{align*} T_{u}^{\prime}(\phi) &amp;amp;:= T_{u^{\prime}}(\phi) \\ &amp;amp;= \int u^{\prime}(x)\phi (x)dx \\ &amp;amp;= \left[ u(x) \phi (x) \right]_{-\infty}^{\infty} -\int</description></item><item><title>Differentiation of the Product of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1804/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1804/</guid><description>Theorem1 Let&amp;rsquo;s denote $T\in D^{\ast}$ as a distribution, and $f \in C^{\infty}$ as a smooth function. Then, the following equation holds. $$ (fT)^{\prime}= f^{\prime}T+fT^{\prime} $$ Explanation It fits perfectly with the existing product rule, so one can feel that the differentiation of a distribution and product of distributions have been plausibly defined. Proof By the definition of distribution differentiation and product, the following is true. $$ \begin{align*} D( fT (\phi)</description></item><item><title>Multiplication of a Distribution with a Smooth Function</title><link>https://freshrimpsushi.github.io/en/posts/1803/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1803/</guid><description>Buildup A distribution cannot be multiplied with a function defined on the real space since its domain is a function space. However, in the case of regular distributions, there exists a corresponding locally integrable function $u\in L_{\mathrm{loc}}^{1}$, so it is represented as follows. $$ T_{u}(\phi)=\int u(x)\phi (x) dx,\quad \phi \in \mathcal{D} $$ Therefore, considering some action $S$ applied to $u$ by which we can obtain $Su=u^{\prime}$, if $u^{\prime}$ remains a</description></item><item><title>Scaling Factors of Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1776/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1776/</guid><description>Buildup In the curvilinear coordinate system, the scale factor is an element that multiplies each component so that they have dimensions of length. For instance, the polar coordinate system is represented by $(r,\theta)$, where the distance the coordinates move as $\theta$ changes is the length of the arc, which is $l=r\theta$. Here, things like $r$ are called scale factors. Let&amp;rsquo;s say the variable of an arbitrary coordinate system is $(q_{1},q_{2},q_{3})$.</description></item><item><title>Convergence of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1802/</link><pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1802/</guid><description>Definition1 Let&amp;rsquo;s say $D^{\ast}$ is a distribution space, $\left\{ T_{n} \right\}$ a sequence of distributions in $D^{\ast}$. If for all test functions $\phi$, the following equation holds, then $\left\{ T_{n} \right\}$ is said to weakly converge to $T$. $$ T_{n}(\phi) \to T(\phi) ,\quad \forall \phi \in \mathcal{D} $$ Explanation The convergence of distributions is referred to as weak convergence because, in the case that $T$, $T_{n}$ are regular distributions, it</description></item><item><title>Convergence of Sequences in Normed Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1800/</link><pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1800/</guid><description>Definition Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ as a normed space. For a sequence $\left\{ x_{n} \right\}$ of $X$, $$ \lim \limits_{n \to \infty} \left\| x - x_{n} \right\| = 0,\quad x\in X $$ it is said to converge to $x$ if it satisfies the following condition, and it is represented as follows. $$ x_{n} \to x \text { as } n \to \infty \quad \text{or} \quad x=\lim \limits_{n\to\infty}x_{n} $$</description></item><item><title>Weak Convergence in Hilbert Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1801/</link><pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1801/</guid><description>Definition Let $(H,\langle \cdot \rangle)$ be a Hilbert space, and let $\left\{ x_{n} \right\}$ be a sequence in $H$. For all $y\in H$, if the following equation holds, $\left\{ x_{n} \right\}$ is said to converge weakly and is denoted as $x_{n} \rightharpoonup x$. $$ \langle x_{n}, y \rangle \to \langle x , y \rangle ,\quad \forall y\in H $$ Following the &amp;lsquo;w&amp;rsquo; in weak, it can also be denoted as:</description></item><item><title>Total Differentiation, Exact Differentiation</title><link>https://freshrimpsushi.github.io/en/posts/1773/</link><pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1773/</guid><description>Definition Let&amp;rsquo;s assume that a multivariable function $f : \mathbb{R}^{n} \to \mathbb{R}$ is given. The change of $f(\mathbf{x})$ according to the change of variable $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n})$ is denoted as $df$, and this is called the total differential or exact differential of $f$. $$ \begin{equation} df = \frac{ \partial f}{ \partial x_{1} }dx_{1} + \frac{ \partial f}{ \partial x_{2} }dx_{2} + \cdots + \frac{ \partial f}{ \partial</description></item><item><title>Cauchy Distribution: A Distribution Without a Mean</title><link>https://freshrimpsushi.github.io/en/posts/147/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/147/</guid><description>Definition The continuous probability distribution with the following probability density function is called a Cauchy distribution. $C$ $$ f(x) = {1 \over \pi} {1 \over {x^2 + 1}} \qquad , x \in \mathbb{R} $$ Explanation It may seem like all probability distributions would have a mean and variance, but in reality, that&amp;rsquo;s not always the case. A prime example of this is the Cauchy distribution, which at a glance resembles</description></item><item><title>Differentiation of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1084/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1084/</guid><description>Buildup The distribution cannot be differentiated in the same manner as functions defined over real numbers since its domain is a function space. However, for regular distributions, there is a corresponding locally integrable function $u\in L_{\mathrm{loc}}^{1}$, expressed as follows. $$ T_{u}(\phi) =\int u(x)\phi (x) dx,\quad \phi \in \mathcal{D} $$ Hence, the action $S$ on $u$ could yield $Su=u^{\prime}$; if $u^{\prime}$ remains a locally integrable function, then there exists a corresponding</description></item><item><title>Divergence of Vector Function in Cartesian Cooridenates System</title><link>https://freshrimpsushi.github.io/en/posts/1796/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1796/</guid><description>Definition For a vector function $\mathbf{F}(x,y,z)=F_{x}\hat{\mathbf{x}}+F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$, the following scalar function is defined as the divergence $\mathbf{F}$ of $\mathbf{F}(x,y,z)=F_{x}\hat{\mathbf{x}}+F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$ and is denoted by $\nabla \cdot \mathbf{F}$. $$ \begin{equation} \nabla \cdot \mathbf{F} := \frac{ \partial F_{x}}{ \partial x} + \frac{ \partial F_{y}}{ \partial y }+ \frac{ \partial F_{z}}{ \partial z} \label{divergence} \end{equation} $$ Explanation Geometrically, if $\nabla \cdot \mathbf{F}&amp;gt;0$, it means that $\mathbf{F}$ is spreading out or diverging.</description></item><item><title>Reasons Not to Use r, Theta as Variables in Cylindrical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1795/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1795/</guid><description>Notation of Cylindrical Coordinates The cylindrical coordinate system represents points in 3D space as shown in $(\rho, \phi, z)$. Here, $\rho$: the magnitude of the vector projection of the position vector $\mathbf{r}$ onto the $xy$-plane $\phi$: the angle between the projected vector and the $x$ axis $z$: the magnitude of the vector projection of the position vector $\mathbf{r}$ onto the $z$ axis However, the notation $(r,\theta, z)$ is also often</description></item><item><title>Autonomous Systems: Orbits and Limit Cycles</title><link>https://freshrimpsushi.github.io/en/posts/1693/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1693/</guid><description>Definition Let&amp;rsquo;s assume we have a vector field given by a differential equation with respect to space $X$ and function $f : X \to X$ as follows: $$ \dot{x} = f(x) $$ Consider the flow of the autonomous system at the initial time $t_{0}$ and the initial point $x_{0}$ to be represented as $x(t,t_{0},x_{0})$. Then, an Orbit passing through $x_{0} \in X$ is represented as follows[^1]: $$ O(x_{0}) := \left\{</description></item><item><title>Dilation of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1794/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1794/</guid><description>Buildup The distribution cannot be dilated in the same manner as functions defined in real space because its domain is a function space. However, for regular distributions, there exists a corresponding locally integrable function $u\in L_{\mathrm{loc}}^{1}$, allowing it to be expressed as follows. $$ T_{u}(\phi) =\int u(x)\phi (x) dx,\quad \phi \in \mathcal{D} $$ Therefore, through some action $S$ on $u$, we can obtain $Su=u^{\prime}$, and if $u^{\prime}$ remains a locally</description></item><item><title>Fourier Transform of the Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/1793/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1793/</guid><description>Formulas Let&amp;rsquo;s assume that the Fourier transform of the function $f(x)$ is $\hat{f}(\xi) = \mathcal{F}[f] (x) = \displaystyle \int_{-\infty}^{\infty} f(x)e^{-i \xi x}dx$. The Fourier transform of the Dirac delta function $\delta (x)$ is as follows. $$ \hat{\delta}(\xi) = \mathcal{F}[\delta] (\xi) = 1 $$ The Fourier transform of $\delta (x - y)$ is $$ \mathcal{F}[\delta (\cdot - y)] (\xi) = e^{-i\xi y} $$ Explanation Depending on how the Fourier transform is</description></item><item><title>Proof that the Dirac Delta Function is Not a Regularized Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1785/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1785/</guid><description>Theorem1 $$ \delta (\phi) := \phi (0), \quad \phi \in \mathcal{D} $$ As defined above, the Dirac delta function is not a regular distribution. Distributions that are not regular are called singular distributions. Description A regular distribution refers to a distribution that is defined with the existence of a locally integrable function $u$ as follows: $$ T_{u}(\phi) := \int u(x) \phi (x) dx,\quad \phi \in \mathcal{D} $$ The statement that</description></item><item><title>Characteristic Function, Indicator Function</title><link>https://freshrimpsushi.github.io/en/posts/1790/</link><pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1790/</guid><description>Definition For $A \subset X$, the function defined as $\chi_{A} : X \to \mathbb{R}$ is referred to as the characteristic function or the indicator function. $$ \chi _{A}(x) := \begin{cases} 1, &amp;amp; x\in A \\ 0 ,&amp;amp; x \notin A \end{cases} $$ Explanation $\chi$ is the Greek letter chi. The reason our math teacher used to say you should not write the letter x as $\chi$ but should instead use</description></item><item><title>Mean and Variance of the t-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1669/</link><pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1669/</guid><description>Formulas $X \sim t (\nu)$ if $$ E(X) = 0 \qquad , \nu &amp;gt;1 \\ \Var(X) = {{ \nu } \over { \nu - 2 }} \qquad , \nu &amp;gt; 2 $$ Derivation Strategy: Similar to the chi-squared distribution, the t-distribution also has known moment-generating functions, which we utilize. Moment of the t-distribution: Assume two random variables $W,V$ are independent and $W \sim N(0,1)$, $V \sim \chi^{2} (r)$. If $k</description></item><item><title>Riemann Hypothesis and Trivial Roots of the Riemann Zeta Function</title><link>https://freshrimpsushi.github.io/en/posts/1668/</link><pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1668/</guid><description>Formula The following is called the Riemann functional equation. $$ \zeta (s) = 2^{s} \pi^{s - 1} \sin \left( {{ \pi s } \over { 2 }} \right) \Gamma (1-s) \zeta (1-s) $$ $\Gamma$ is the Gamma function. $\zeta$ is the Riemann zeta function. Description In the Riemann functional equation, if $s \in 2 \mathbb{Z}$ then $\displaystyle \sin \left( {{ \pi s } \over { 2 }} \right) = 0$,</description></item><item><title>The Dirac Delta Function Rigorously Defined through Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1784/</link><pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1784/</guid><description>Definition1 Let&amp;rsquo;s define the functional of the space of test functions $\mathcal{D}(\mathbb{R}^{n})$ as follows and call it the Dirac delta function. $$ \delta_{a}(\phi):=\phi (a) $$ Then, the Dirac delta function becomes a distribution. It is briefly represented as follows if $a=0$. $$ \delta=\delta_{0} $$ Explanation The Dirac delta function, which was not strictly defined due to having divergent values and was roughly termed as a function, is rigorously defined by</description></item><item><title>Convergence in the Space of Test Functions</title><link>https://freshrimpsushi.github.io/en/posts/1077/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1077/</guid><description>In the test function space, &amp;lsquo;convergence&amp;rsquo; is defined in a special way. Normally, when a space $X$ is given, convergence is defined using the norm or distance defined in $X$. However, in the test function space, convergence is defined under stronger conditions to properly define and handle distributions. Definition Let $\Omega \subset \mathbb{R}^n$ be an open set, and $\left\{ \phi _{j} \right\}$ be a sequence of test functions. We say</description></item><item><title>Distributions, Generalized Functions</title><link>https://freshrimpsushi.github.io/en/posts/1009/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1009/</guid><description>Definition1 2 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. A continuous linear functional $T : \mathcal{D}(\Omega) \to \mathbb{C}$ on the space of test functions is defined as a distribution. That is, a distribution is an element of the dual space of the test function space. Thus $$ T \in \mathcal{D}^{\ast} $$ and we call $D^{\ast}$ the (Schwartz) distribution space. Explanation The name distribution seems to be influenced by the</description></item><item><title>Locally Integrable Function</title><link>https://freshrimpsushi.github.io/en/posts/1783/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1783/</guid><description>Definition Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. Definition 11 For every bounded measurable set $K \subset \Omega$, $$ \int_{K} \left| u(x) \right| dx \lt \infty $$ a function $u : \Omega \to \mathbb{C}$ satisfying this is said to be locally integrable with respect to (the Lebesgue measure). Definition 22 Let the function $u$ be defined almost everywhere on $\Omega$. For every open set $U \Subset \Omega$ when</description></item><item><title>Proving that All Locally Integrable Functions Can Be Extended to Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1078/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1078/</guid><description>Theorem1 For every $u \in L_{\mathrm{loc} }^1(\Omega) $, there exists a distribution $T_{u} \in D^{\ast}(\Omega)$ defined as follows: $$ T_{u} (\phi) := \int_{\Omega} u(x)\phi (x)dx, \quad \phi \in D(\Omega) $$ Description $\mathcal{D}(\Omega)$ is the space of test functions. The distribution defined as above is called a regular distribution. Moreover, the above expression can be regarded as the inner product of $u$ and $\phi$ from the viewpoint of inner product spaces,</description></item><item><title>t-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1667/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1667/</guid><description>Definition 1 A continuous probability distribution $t \left( \nu \right)$, known as the t-distribution, is defined for degrees of freedom $\nu &amp;gt; 0$ as having the following probability density function: $$ f(x) = {{ \Gamma \left( {{ \nu + 1 } \over { 2 }} \right) } \over { \sqrt{\nu \pi} \Gamma \left( {{ \nu } \over { 2 }} \right) }} \left( 1 + {{ x^{2} } \over {</description></item><item><title>Test Functions and Test Function Space</title><link>https://freshrimpsushi.github.io/en/posts/1782/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1782/</guid><description>Definition1 Let an open set $\Omega \subset \mathbb{R}^{n}$ and a function $\phi : \Omega \to \mathbb{C}$ be given. If $\phi$ is infinitely differentiable, and all its derivatives are continuous and have a compact support, it is called a test function. The function space of test functions is denoted by $C_{c}^{\infty}(\Omega)$ or simply as $\mathcal{D}(\Omega)$. Explanation It is also called a test function or testing function. The reason $\phi$ is named</description></item><item><title>Translation of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1786/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1786/</guid><description>Buildup Distribution cannot be translated in the same manner as functions defined on the real space because their domain is a function space. However, for regular distributions, there is a corresponding locally integrable function $u\in L_{\mathrm{loc}}^{1}$, which can be represented as follows. $$ T_{u}(\phi) =\int u(x)\phi (x) dx,\quad \phi \in \mathcal{D}(\mathbb{R}^{n}) $$ Thus, some action $S$ on $u$ would yield $Su=u^{\prime}$, and if $u^{\prime}$ is still a locally integrable function,</description></item><item><title>The History of the Delta Function and Why Dirac Used the Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/1781/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1781/</guid><description>The History of the Delta Function1 2 3 The delta function started to appear in the works of scholars such as Poisson (1815), Fourier (1822), and Cauchy (1823, 1827) who made significant contributions to mathematics and physics in the early 19th century. However, at that time, there was not a focus on rigorously defining the delta function as we do today. Later, Kirchhoff (1882, 1891) and Heaviside (1893, 1899) were</description></item><item><title>Riemann Zeta Function</title><link>https://freshrimpsushi.github.io/en/posts/1664/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1664/</guid><description>Definition The function defined as $\xi$ is called the Riemann xi Function. $$ \xi (s) := {{ 1 } \over { 2 }} s ( s-1) \pi^{-s/2} \zeta (s) \Gamma \left( {{ s } \over { 2 }} \right) $$ $\zeta$ is the Riemann Zeta Function. $\Gamma$ is the Gamma Function. Explanation The Riemann xi function was originally defined in a slightly different form, but Edmund Landau redefined it with</description></item><item><title>Why Functional is Named Functional</title><link>https://freshrimpsushi.github.io/en/posts/1780/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1780/</guid><description>The term &amp;ldquo;functional analysis&amp;rdquo; is indeed intriguing, especially when considering the word &amp;ldquo;functional&amp;rdquo; instead of merely &amp;ldquo;function analysis.&amp;rdquo; At first glance, &amp;ldquo;functional&amp;rdquo; appears to be an adjective form of &amp;ldquo;function,&amp;rdquo; suggesting meanings like &amp;ldquo;function-like&amp;rdquo; or &amp;ldquo;pertaining to functions.&amp;rdquo; This notion can also be found in another name for functionals, &amp;ldquo;generalized functions.&amp;rdquo; The question arises as to why these are not simply called functions. To understand this, let&amp;rsquo;s look at the</description></item><item><title>Derivation of the Student's t-Distribution from Independent Normal Distributions and the Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/204/</link><pubDate>Sun, 13 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/204/</guid><description>Theorem Two independent random variables $W,V$ where $W \sim N(0,1)$ and $V \sim \chi^{2} (r)$, then $$ T = { {W} \over {\sqrt{V/r} } } \sim t(r) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with mean $\mu$ and variance $\sigma^{2}$. $\chi^{2} \left( r \right)$ is a chi-squared distribution with degrees of freedom $r$. $t(r)$ is a t-distribution with degrees of freedom $r$. Description If this theorem</description></item><item><title>Gradient of Scalar Function in Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/1778/</link><pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1778/</guid><description>Definition For a scalar function $f=f(x,y,z)$, the following vector function is defined as the gradient of $f$, denoted by $\nabla f$: $$ \nabla f := \frac{ \partial f}{ \partial x }\hat{\mathbf{x}}+\frac{ \partial f}{ \partial y}\hat{\mathbf{y}}+\frac{ \partial f}{ \partial z}\hat{\mathbf{z}} = \left( \dfrac{\partial f}{\partial x}, \dfrac{\partial f}{\partial y}, \dfrac{\partial f}{\partial z} \right) $$ Explanation The gradient is translated into English as gradient, slope, or incline. The terms &amp;lsquo;slope&amp;rsquo; and &amp;lsquo;incline&amp;rsquo; are</description></item><item><title>Perfect Graph</title><link>https://freshrimpsushi.github.io/en/posts/1662/</link><pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1662/</guid><description>Definition A perfect graph is a graph $G$ where every induced subgraph $H$ satisfies the following. $$ \chi (H) = \omega (H) $$ $\chi (H)$ is the chromatic number of graph $H$. $\omega (H)$ is the clique number of graph $H$. Explanation The world of graph theory, like many branches of mathematics, is staggeringly vast, honestly a bit more so. This is because there are so many different ways to</description></item><item><title>The Square of a Standard Normal Distribution Follows a Chi-Square Distribution with One Degree of Freedom</title><link>https://freshrimpsushi.github.io/en/posts/148/</link><pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/148/</guid><description>Theorem If $X \sim N(\mu,\sigma ^2)$ then $$ V=\left( { X - \mu \over \sigma} \right) ^2 \sim \chi ^2 (1) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with mean $\mu$ and variance $\sigma^{2}$. $\chi^{2} \left( 1 \right)$ is a chi-squared distribution with degrees of freedom $1$. Description In general, Student&amp;rsquo;s theorem is widely used to generalize this. Anyone studying statistics must always know as a</description></item><item><title>Normal Distribution: Mean and Variance</title><link>https://freshrimpsushi.github.io/en/posts/1661/</link><pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1661/</guid><description>Formula $X \sim N\left( \mu , \sigma^{2} \right)$ Plane $$ E(X) = \mu \\ \Var (X) = \sigma^{2} $$ Derivation Strategy: The normal distribution has a moment-generating function that is easy to differentiate, so we just directly derive it. Moment-generating function of normal distribution: $$ m(t) = \exp \left( \mu t + {{ \sigma^{2} t^{2} } \over { 2 }} \right) \qquad , t \in \mathbb{R} $$ $$ m '</description></item><item><title>Coupled Oscillations</title><link>https://freshrimpsushi.github.io/en/posts/1767/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1767/</guid><description>Simple Coupled Oscillations Let&amp;rsquo;s say we have two objects, $m_{1}$ and $m_{2}$, connected by two springs as shown in the above figure. Let the distance from the equilibrium point to object $m_{1}$ be $x_{1}$, and to object $m_{2}$ be $x_{2}$. The restoring force exerted by a spring on an object is the product of the spring constant and the stretch (or compression) of the spring, so the force exerted by</description></item><item><title>Fluids and the Definition of Fluid Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/1768/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1768/</guid><description>Definition Fluid refers to: A term that combines both liquid and gas A collection of molecules that are randomly arranged and clustered together1 In a stationary state, it is subject to vertical stress, and in a flow state, it undergoes continuous deformation and flows when subjected to shear forces2 Description Although it&amp;rsquo;s called a definition, it&amp;rsquo;s not that strict. However, without pushing for a strict definition, we naturally understand what</description></item><item><title>Multiple Spring Oscillation</title><link>https://freshrimpsushi.github.io/en/posts/1766/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1766/</guid><description>When springs are connected on both sides of an object Let $x$ be the distance the object has moved. Since the restoring force of the spring is $-kx$, the object receives a force of $-k_{1}x$ from the left spring and $-k_{2}x$ from the right spring. Therefore, the equation of motion is as follows. $$ \begin{align*} &amp;amp;&amp;amp; m\ddot{x}&amp;amp;=-k_{1}x-k_{2}x \\ \implies &amp;amp;&amp;amp;m\ddot{x}+(k_{1}+k_{2})x&amp;amp;=0 \\ \implies &amp;amp;&amp;amp; \ddot{x}+\frac{k_{1}+k_{2}}{m}x &amp;amp;=0 \end{align*} $$ This is the</description></item><item><title>Compiling C Code with GCC Compiler on Linux</title><link>https://freshrimpsushi.github.io/en/posts/1653/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1653/</guid><description>Guide Normally, for program development using C/C++, it&amp;rsquo;s recommended to use Visual Studio on Windows. However, when proceeding with simple tests, numerical calculations, simulations, etc., on Linux, the unique lightness of Linux can come as a significant advantage. For example, if there is a C source code named infection_modified_200428.c, try entering the following after moving to the corresponding path in the terminal. gcc -o infection infection\_modified\_200428.c -lm Then an executable</description></item><item><title>Frequently Used Symbols and Abbreviations in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1764/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1764/</guid><description>for all, exist, such that Example1 For every $\varepsilon \gt 0$, there is an integer N such that $n \ge N$ implies that $d(p_{n},p)&amp;lt;\varepsilon$. Every positive real number $\varepsilon$, there exists an integer $N$ such that whenever $n$ is greater than some integer $N$, $d(p_{n},p) \lt \varepsilon$ holds. $$ \forall \varepsilon \gt 0, \exists N \in \mathbb{N}\quad \text{s.t. } n\ge N \implies d(p_{n},p) \lt \varepsilon $$ Explanation $\forall$ It means</description></item><item><title>Physical Pendulum</title><link>https://freshrimpsushi.github.io/en/posts/1765/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1765/</guid><description>Definition1 A physical pendulum refers to a rigid body swinging about a fixed horizontal axis due to gravity. Physical Pendulum Pendulum motion is a type of harmonic oscillation. The magnitude of the torque acting on the center of mass is as follows: $$ \begin{align*} N &amp;amp;=\left| \mathbf{r} \times \mathbf{F} \right| \\ &amp;amp;= rF\sin\theta \\ &amp;amp;=lmg \sin\theta \end{align*} $$ Expressing the torque in terms of moment of inertia, we get the</description></item><item><title>Multi-Resolution Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1762/</link><pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1762/</guid><description>Definition If a sequence of closed subspaces $L^{2}(\mathbb{R})$ and a function $\phi \in V_{0}$ satisfy the following conditions, then $\left( \left\{ V_{j} \right\}, \phi \right)$ is called a multiresolution analysis. (a) For each $V_{j}$, $\cdots V_{-1} \subset V_{0} \subset V_{1}\cdots$ holds. (b) $\overline{\cup_{j\in\mathbb{Z}}V_{j}}=L^{2}(\mathbb{R})$ and $\cap_{j\in\mathbb{Z}}V_{j}=\left\{ 0\right\}$. (c) $\forall j\in \mathbb{Z}$, $V_{j+1}=D(V_{j})$. (d) If $\forall k \in \mathbb{Z}$, $f \in V_{0}$, then $T_{k}f \in V_{0}$. (e) $\left\{ T_{k} \phi\right\}_{k\in \mathbb{Z}}$ is</description></item><item><title>Multi-Resolution Analysis Scaling Equation</title><link>https://freshrimpsushi.github.io/en/posts/1763/</link><pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1763/</guid><description>정리 함수 $\phi \in L^{2}(\mathbb{R})$가 multiresolution analysis을 생성한다고 하자. 그러면 아래의 식을 만족하는 주기가 $1$인 함수 $H_{0}\in L^{2}</description></item><item><title>How to Initialize the Workspace and Remove All Variables in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1758/</link><pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1758/</guid><description>Method clear Command Typing clear in the command window will reset the workspace. Clearing Workspace (Alt+T+O) Right-clicking on the workspace window allows you to select &amp;lsquo;Clear Workspace (O)&amp;rsquo;. Pressing it resets the workspace. This can also be done using the shortcut Alt+T+O, but it does not work when an editor is opened. Deleting by Selection You can delete by dragging to select everything or by pressing Ctrl+a to select all</description></item><item><title>Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1645/</link><pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1645/</guid><description>Definition A continuous probability distribution $N \left( \mu,\sigma^{2} \right)$ with a probability density function having a mean of $\mu \in \mathbb{R}$ and a variance of $\sigma^{2} &amp;gt; 0$ is called a normal distribution. $$ f(x) = {{ 1 } \over { \sqrt{2 \pi} \sigma }} \exp \left[ - {{ 1 } \over { 2 }} \left( {{ x - \mu } \over { \sigma }} \right)^{2} \right] \qquad, x \in</description></item><item><title>Differential Volume in Cylindrical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1753/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1753/</guid><description>Formulas The infinitesimal volume in spherical coordinates is given by: $$ dV=r^{2}\sin\theta dr d\theta d\phi $$ The infinitesimal surface area on a sphere can be obtained without multiplying $dr$ by: $$ da=\color{blue}{rd\theta} \cdot \color{red}{r\sin\theta d \phi}=r^{2}\sin\theta d\theta d\phi $$</description></item><item><title>Infinitesimal Area in Polar Coordinates, Infinitesimal Volume in Cylindrical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1755/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1755/</guid><description>Formula In polar coordinates, the infinitesimal area is as follows. $$ dA=rdrd\theta $$ In cylindrical coordinates, the infinitesimal volume and the infinitesimal surface area of a cylinder are as follows. $$ dV=\rho d\rho d\phi dz \\ dA=\rho d\phi dz $$ Description Polar Coordinates $\mathbf{r}=\mathbf{r}(r,\theta)$ The infinitesimal area, as shown in the figure, is (length of the green line)$\times$(length of the blue line). The green line represents the infinitesimal change in</description></item><item><title>Jacobi Theta Function</title><link>https://freshrimpsushi.github.io/en/posts/1644/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1644/</guid><description>Definition The function defined as follows $\vartheta$ is called the Jacobi theta Function. $$ \vartheta (\tau) := \sum_{n \in \mathbb{Z}} e^{-\pi n^{2} \tau } $$ Description While Jacobi functions can originally be more generally defined, it is common to use a special form of them depending on the needs. Note that the Jacobi theta function introduced here does not cover all contexts in its exact meaning. The following property is</description></item><item><title>Curl of Vector Functions in 3D Cartesian Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1752/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1752/</guid><description>Definition For a vector function $\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\hat{\mathbf{x}} + F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$, the following vector is defined as the curl of $\mathbf{F}$, denoted as $\nabla \times \mathbf{F}$. $$ \begin{align} \nabla \times \mathbf{F} &amp;amp;= \left( \dfrac{ \partial F_{z}}{ \partial y }-\dfrac{ \partial F_{y}}{ \partial z} \right)\hat{\mathbf{x}}+ \left( \dfrac{ \partial F_{x}}{ \partial z }-\dfrac{ \partial F_{z}}{ \partial x} \right)\hat{\mathbf{y}}+ \left( \dfrac{ \partial F_{y}}{ \partial x }-\dfrac{ \partial F_{x}}{ \partial y} \right)\hat{\mathbf{z}} \label{def1} \\ &amp;amp;=\begin{vmatrix}</description></item><item><title>Double and Half Angle Formulas for Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1748/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1748/</guid><description>Formulas Double Angle Formula: $$ \begin{align} \sinh (2x) =&amp;amp;\ 2\sinh x \cosh x \label{1} \\ \cosh (2x) =&amp;amp;\ \cosh^{2} x + \sinh^{2} x = 2\cosh ^{2 } x -1 = 2\sinh ^{2} x +1 \\ \tanh (2x) =&amp;amp;\ \frac{2\tanh x}{1+\tanh^{2}x} \end{align} $$ Half Angle Formula: $$ \begin{align} \sinh^{2} \frac{x}{2} =&amp;amp;\ \frac{\cosh x -1 }{2} \\ \cosh^{2} \frac{x}{2} =&amp;amp;\ \frac{\cosh x +1 }{2} \\ \tanh ^{2} \frac{x}{2} =&amp;amp;\ \frac{\cosh x -1}{\cosh</description></item><item><title>Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1749/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1749/</guid><description>Definition Regarding $z \in \mathbb{C}$, $$ \begin{align*} \sinh z &amp;amp;:= \frac{e^{z}-e^{-z}}{2} \\ \cosh z &amp;amp;:= \frac{e^{z}+e^{-z}}{2} \\ \tanh z &amp;amp;:= \frac{\sinh z}{\cosh z} \end{align*} $$ $$ \begin{align*} \mathrm{csch}x&amp;amp;=\frac{1}{\sinh x} \\ \mathrm{sech} x&amp;amp;=\frac{1}{\cosh x} \\ \coth x &amp;amp;=\frac{1}{\tanh x} \end{align*} $$ Relationship with Trigonometric Functions $$ \begin{align*} \sinh (iz) &amp;amp;= i\sin z \\ \sin (iz) &amp;amp;= i\sinh z \\ \cosh (iz) &amp;amp;= \cos z \\ \cos (iz) &amp;amp;= \cosh z \end{align*}</description></item><item><title>Sum and Difference Formulas and Multiplication Formulas for Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1750/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1750/</guid><description>Formulas Sum and Difference Formulas: $$ \begin{align} \sinh x +\sinh y =&amp;amp;\ 2\sinh \left(\frac{x+y}{2}\right) \cosh \left(\frac{x-y}{2}\right) \\[1em] \sinh x -\sinh y =&amp;amp;\ 2\sinh \left(\frac{x-y}{2}\right) \cosh \left( \frac{x+y}{2} \right) \\[1em] \cosh x + \cosh y =&amp;amp;\ 2 \cosh \left(\frac{x+y}{2}\right) \cosh \left(\frac{x-y}{2}\right) \\[1em] \cosh x -\cosh y =&amp;amp;\ 2 \sinh \left( \frac{x+y}{2} \right) \sinh \left(\frac{x-y}{2}\right) \end{align} $$ Product Formulas: $$ \begin{align} \sinh x \sinh y =&amp;amp;\ \frac{\cosh (x+y)-\cosh (x-y)}{2} \\ \sinh x</description></item><item><title>Derivation of F-distribution from Two Independent Chi-squared Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1643/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1643/</guid><description>Theorem If two random variables $U,V$ are independent and it is assumed that $U \sim \chi^{2} ( r_{1})$, $V \sim \chi^{2} ( r_{2})$ then $$ {{ U / r_{1} } \over { V / r_{2} }} \sim F \left( r_{1} , r_{2} \right) $$ Explanation If two data follow the Chi-squared distribution and are independent, it might be possible to explain their ratio using distribution theory. In statistics in general,</description></item><item><title>Forced Harmonic Vibration and Resonant Frequency</title><link>https://freshrimpsushi.github.io/en/posts/1742/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1742/</guid><description>Forced Harmonic Oscillation1 Harmonic motion, such as the movement of an object hanging from a spring, occurs when no external forces other than the spring constant $k$&amp;rsquo;s restoring force are acting. This is called simple harmonic oscillation when excluding other external forces like air resistance or any friction. If there is an external force proportional to the velocity, like friction, it&amp;rsquo;s referred to as damped harmonic oscillation. When an external</description></item><item><title>Hyperbolic Functions' Identities</title><link>https://freshrimpsushi.github.io/en/posts/1744/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1744/</guid><description>Formulas $$ \begin{align} \sinh(-x) =&amp;amp;\ -\sinh x \\ \cosh(-x) =&amp;amp;\ \cosh x \\ \tanh(-x) =&amp;amp;\ - \tanh x \\ \cosh x + \sinh x =&amp;amp;\ e^{x} \\ \cosh x - \sinh x =&amp;amp;\ e^{-x} \\ \cosh^{2}x -\sinh^{2}x =&amp;amp;\ 1 \end{align} $$ Explanation There&amp;rsquo;s really no proof needed. This can be directly known from the definition. Proof Proof of $(1)$ $$ \begin{align*} \sinh(-x) =&amp;amp;\ \frac{e^{-x}-e^{x}}{2} \\ =&amp;amp;-\frac{e^{x}-e^{-x}}{2} \\ =&amp;amp;-\sinh x \end{align*}</description></item><item><title>Proof of the Addition Formulas for Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1743/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1743/</guid><description>Formula $$ \begin{align} \sinh (x\pm y) =&amp;amp;\ \sinh x \cosh y \pm \sinh y \cosh x \\ \cosh (x \pm y) =&amp;amp;\ \cosh x \cosh y \pm \sinh x \sinh y \\ \tanh{x \pm y}&amp;amp;=\frac{\tanh x \pm \tanh y}{1 \pm \tanh x \tanh y} \end{align} $$ Description Thinking about the relationship between hyperbolic and trigonometric functions makes it natural that their forms are similar to the addition theorem of trigonometric</description></item><item><title>Derivation of the Poisson Summation Formula</title><link>https://freshrimpsushi.github.io/en/posts/1642/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1642/</guid><description>Formula Let $f : \mathbb{R} \to \mathbb{C}$ be a Schwartz function. Then, $$ \sum_{n \in \mathbb{Z}} f(n) = \sum_{k \in \mathbb{Z}} \widehat{f}(k) $$ Schwartz functions $f \in C^{\infty}(\mathbb{R})$ are functions whose magnitude $\left| f (x) \right|$ converges rapidly to $0$ when $x \to \pm \infty$. For $f$ and $\gamma \in \mathbb{R}$, $\widehat{f}(\gamma)$ represents the following Fourier transform: $$ \widehat{f} ( \gamma ) = \int_{\mathbb{R}} f(x) e^{-2 \pi i \gamma x}</description></item><item><title>Principle of the Compton Camera</title><link>https://freshrimpsushi.github.io/en/posts/1740/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1740/</guid><description>Principles The Compton camera uses Compton scattering to locate materials emitting gamma rays. It is also called the Compton telescope or Compton imager. On the right side of the figure, the Compton camera is simply represented by two detectors. The detectors measure the energy of the gamma rays, with scattering of the gamma rays occurring at the first detector. The black square on the left side of the figure is</description></item><item><title>What is Tomography?</title><link>https://freshrimpsushi.github.io/en/posts/1741/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1741/</guid><description>What is Tomography?1 Tomography, derived from the Greek words τoμoσ and γραψετε, translates to slicing and recording in Korean. The images we see after a CT scan are essentially cross-sectional pictures2 of the body. In tomography, opaque objects are penetrated with signals in the form of waves or particles to gather information about their internal structure. The signals commonly</description></item><item><title>Damped Harmonic Oscillation</title><link>https://freshrimpsushi.github.io/en/posts/1736/</link><pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1736/</guid><description>Damped Harmonic Oscillation1 When the spring constant is denoted as $k$, the equation of motion for a simple harmonic oscillator is as follows. $$ m \ddot {x}+kx=0 $$ The simple harmonic motion only considers the restoring force by the spring. However, in reality, other external forces such as frictional forces also affect the motion of the object, so they cannot be ignored. So, let&amp;rsquo;s assume there is a frictional force</description></item><item><title>Relationship between the Gamma Function and the Riemann Zeta Function and the Dirichlet Eta Function</title><link>https://freshrimpsushi.github.io/en/posts/1641/</link><pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1641/</guid><description>정리 If $\operatorname{Re} (s) &amp;gt; 1$ then $$ \zeta (s) \Gamma (s) = \mathcal{M} \left[ {{ 1 } \over { e^{x} - 1 }} \right] (s) = \int_{0}^{\infty} {{ x^{s-1} } \over { e^{x} - 1 }} dx \\ \eta (s) \Gamma (s) = \mathcal{M} \left[ {{ 1 } \over { e^{x} + 1 }} \right] (s) = \int_{0}^{\infty} {{ x^{s-1} } \over { e^{x} + 1 }} dx</description></item><item><title>Operators as Fourier Transforms</title><link>https://freshrimpsushi.github.io/en/posts/1640/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1640/</guid><description>Definition1 The Fourier Transform of the function $f$ $$ \widehat{f} (\gamma ) := \int_{\mathbb{R}} f(x) e^{-2 \pi i x \gamma} dx, \quad \gamma \in \mathbb{R} $$ can also be represented as the following operator $\mathcal{F}$. $$ (\mathcal{F} f) (\gamma ) := \widehat{f} ( \gamma ) $$ Description Fourier Transform is widely used throughout analysis and the two expressions $\widehat{f}$ and $\mathcal{F} f$ essentially do not differ, but there is a</description></item><item><title>Translation, Modulation, and Dilation Commutation Relations in L2 Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1639/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1639/</guid><description>Theorem1 For all $a, b \in \mathbb{R}$ and $c &amp;gt; 0$, $T_{a}, E_{b}, D_{c}$ has the following relationship: $$ \begin{equation} (T_{a} E_{b} f ) (x) = e^{- 2 \pi i b a} (E_{b} T_{a} f ) (x) \end{equation} $$ $$ \begin{equation} (T_{a} D_{c} f ) (x) = (D_{c} T_{a/c} f ) (x) \end{equation} $$ $$ \begin{equation} (D_{c} E_{b} f ) (x) = (E_{b/c} D_{c} f ) (x) \end{equation} $$ Here,</description></item><item><title>Borel-Cantelli Lemma</title><link>https://freshrimpsushi.github.io/en/posts/1422/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1422/</guid><description>Theorem Distance spaces $(X, \rho)$ satisfy all of the following equivalent conditions. (a) $X$ is a compact space. (b) $X$ is sequentially compact. (c) $X$ is both a complete space and a totally bounded space. Description A sequentially compact space in distance spaces $X$ means that every sequence in $X$ has a subsequence that converges to a point within $X$. The Borel-Lebesgue theorem provides several necessary and sufficient conditions for</description></item><item><title>Connected Sets in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1729/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1729/</guid><description>Definition If two subsets $A$ and $B$ of a metric space $X$ satisfy $$ A \cap \overline{B}= \varnothing \quad \text{and} \quad \overline{A}\cap B= \varnothing $$ then $A$ and $B$ are said to be separated. In other words, there is no point of $A$ included in the closure of $B$, and there is no point of $B$ included in the closure of $A$. A subset $E \subset X$ that cannot be</description></item><item><title>Continuity and Compactness in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1724/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1724/</guid><description>Theorem Let $X$ be a compact metric space, $Y$ be a metric space, and $f:X\to Y$ be continuous. Then $f(X)$ is compact. The compactness condition cannot be omitted. Proof Let $\left\{ O_\alpha \right\}$ be an open cover of $f(X)$. Since $f$ is continuous, by the equivalence condition, each preimage $f^{-1}(O_{\alpha})$ is also an open set in $X$. Therefore, $\left\{ f^{-1}(O_{\alpha}) \right\}$ is an open cover of $X$, and since $X$</description></item><item><title>Continuous and Uniformly Continuous in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/384/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/384/</guid><description>Definitions Let&amp;rsquo;s define a function $f : E \to Y$ for two metric spaces $\left( X , d_{X} \right)$, $\left( Y , d_{Y} \right)$ and a subset $E\subset X$. Let&amp;rsquo;s say $p \in E$. For any $\varepsilon &amp;gt; 0$, if there exists $\delta&amp;gt;0$ such that $$ x \in E \quad \text{and} \quad d_{X}(p, x ) &amp;lt; \delta \implies d_{Y}(f(p) , f(x) ) &amp;lt; \varepsilon $$ is satisfied, then $f$ is</description></item><item><title>Limit of Functions in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1719/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1719/</guid><description>Definition Let $(X,d_{X})$, $(Y,d_{Y})$ be a metric space. Suppose $E\subset X$, $f: E\rightarrow Y$, and that $p$ is a limit point of $E$. Then, for every positive number $\varepsilon$, $$ x \in E \ \text{and} \ d_{X}(x,p)&amp;lt;\delta \implies d_{Y}(f(x),q) &amp;lt;\varepsilon $$ exists $\delta&amp;gt;0$ such that, $$ f(x)\rightarrow q\ \mathrm{as}\ x\to p $$ or $$ \lim \limits_{x\to p}f(x)=q $$ is denoted and $f$ is said to have the limit $q$ at</description></item><item><title>Maximum and Minimum Theorem in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1725/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1725/</guid><description>Theorem Let $X$ be a compact metric space, and let $f : X \to \mathbb{R}$ be continuous. Then, it is as follows. $$ M = \sup \limits_{x\in X} f(x),\quad m=\inf \limits_{x \in X}f(x) $$ Then, $$ M=f(p),\quad m=f(q) $$ there exists a $q,p\in X$ that satisfies this. In other words: for every $x$, $$ f(q)\le f(x) \le f(p) $$ there exists a $q,p \in X$ that satisfies this. This is</description></item><item><title>Proof that Continuous Functions on Compact Metric Spaces are Uniformly Continuous</title><link>https://freshrimpsushi.github.io/en/posts/1727/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1727/</guid><description>Theorem Let $(X,d_{X})$ be a compact metric space, $(Y,d_{Y})$ a metric space, and $f:X\to Y$ continuous. Then, $f$ is uniformly continuous on $X$. Explanation The condition of being compact cannot be omitted. Proof Suppose we are given any positive number $\varepsilon &amp;gt;0$. Since $f$ is assumed to be continuous, by definition, for each point $p\in X$, there exists a positive number $\delta_{p}$ satisfying the following equation: $$ \forall q\in X,\quad</description></item><item><title>Properties of Complete Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1425/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1425/</guid><description>Properties $(X,d)$ is a metric space and let $K \subset X$. [1]: $K$ is a complete subspace. $\iff$ $X$ implies $K$ is a closed set. [2]: $K$ is a closed set in a totally bounded space $\iff$ $X$, hence $K$ is compact. Description A complete metric space possesses all the usual properties one might expect from a space with completeness, being a metric space with completeness. If it becomes a</description></item><item><title>Properties of Continuous Functions in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1723/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1723/</guid><description>Theorem: Real Functions Let two functions $f$, $g$ be functions from a metric space $X$ to the complex numbers. $$ f:X \to \mathbb{C},\quad g:X \to \mathbb{C} $$ If the two functions are continuous, then $f+g$, $fg$, $f/g$ are also continuous. However, in the last case, it only holds for $g(x)\ne 0$ being $x\in X$. Proof Lemma 1 Let $(X,d)$ be a metric space, $E\subset X$ a subset, and $p$ an</description></item><item><title>Properties of Limits of Functions in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1720/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1720/</guid><description>Theorem 1 Let $(X,d)$ be a metric space, $E\subset X$ a subset, and $p$ an accumulation point of $E$. Suppose two complex-valued functions defined on $E$, $f:E\to \mathbb{C}$ and $g: E\to \mathbb{C}$, are given. Furthermore, assume that these two functions have the following limits at $p$. $$ \begin{equation} \lim \limits_{x \to p}f(x)=A \quad \text{and} \quad \lim \limits_{x \to p}g(x)=B \tag{1} \label{thm1} \end{equation} $$ Then, the following holds. (a) $\lim \limits_{x</description></item><item><title>The Composition of Continuous Functions in Metric Spaces Preserves Continuity</title><link>https://freshrimpsushi.github.io/en/posts/1721/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1721/</guid><description>Theorem Let there be three metric spaces $(X,d_{X})$, $(Y,d_{Y})$, $(Z,d_{Z})$. Assume that $E\subset X$ and there are two functions $f:E\to Y$, $g:f(E) \to Z$. Also, let $h : E \to Z$ defined in $E$ be as follows. $$ h(x) = g(f(x))\quad \forall x \in E $$ If $f$ is continuous at $p\in E$ and $g$ is continuous at $f(p)\in f(E)$, then $h$ is also continuous at $p$. Here, $h$ is</description></item><item><title>The Importance of Compactness Conditions in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1728/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1728/</guid><description>Overview Many theorems in analysis require compactness as a necessary condition (Reference1, Reference2, Reference3, Reference4). While it&amp;rsquo;s taken for granted that &amp;lsquo;if it&amp;rsquo;s compact, then it&amp;rsquo;s fine&amp;rsquo; due to the assumption of compactness in the proof process, one might wonder why &amp;lsquo;if it&amp;rsquo;s not compact, then it&amp;rsquo;s not fine&amp;rsquo;. If the condition of being compact is absent, the following undesirable situations can occur. Theorem1 Let&amp;rsquo;s say $E\subset \mathbb{R}$ is a</description></item><item><title>The inverse of a continuous bijection on a compact metric space is continuous.</title><link>https://freshrimpsushi.github.io/en/posts/1726/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1726/</guid><description>Theorem Let $X$ be a compact metric space, and let $Y$ be a metric space. Assume that $f : X \to Y$ is a bijective continuous function. Then, the inverse $f^{-1}$ of $f$, defined as follows, is bijective and continuous. $$ f^{-1} (f(x))=x, \quad x\in X $$ The compactness condition is essential Proof Equivalent conditions for continuity in metric spaces For two metric spaces $(X,d_{X})$ and $(Y,d_{Y})$, let $f :</description></item><item><title>거리공간에서 연속함수일 동치 조건</title><link>https://freshrimpsushi.github.io/en/posts/1722/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1722/</guid><description>Theorem 1 For two metric spaces $(X,d_{X})$, $(Y,d_{Y})$, let $E\subset X$ and $p \in E$, $f : E \to Y$. Then, the following three statements are equivalent. (1a) $f$ is continuous at $p$. (1b) $ \lim \limits_{x \to p} f(x)=f(p)$. (1c) For $\lim \limits_{n\to\infty} p_{n}=p$, $\left\{ p_{n} \right\}$, it follows that $\lim \limits_{n\to\infty} f(p_{n})=f(p)$. Proof (1a) $\iff$ (1b) By the definition of a limit and continuity, it is trivial. ■</description></item><item><title>Dirichlet eta function</title><link>https://freshrimpsushi.github.io/en/posts/1635/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1635/</guid><description>Definition The function $\eta : \mathbb{C} \to \mathbb{C}$ defined as below is called the Dirichlet eta Function. $$ \eta (s) := \sum_{n \in \mathbb{N}} (-1)^{n-1} n^{-s} $$ The Dirichlet eta function is defined as the alternating Riemann zeta function. Theorems [1] Relationship with the Riemann zeta function: $$ \eta (s) = \left( 1 - 2^{1-s} \right) \zeta (s) $$ [2] Relationship with the gamma function: If $\operatorname{Re} (s) &amp;gt; 1$,</description></item><item><title>Translation, Modulation, and Dilation Operators in L2 Space</title><link>https://freshrimpsushi.github.io/en/posts/1630/</link><pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1630/</guid><description>Theorem1 $T_{a}, E_{b}, D_{c}$ is unitary, and its inverse operator is as follows. $$ T_{a}^{-1} = T_{-a} = \left( T_{a} \right)^{ \ast } $$ $$ E_{b}^{-1} = E_{-b} = \left( E_{b} \right)^{ \ast } $$ $$ D_{c}^{-1} = D_{1/c} = \left( D_{c} \right)^{ \ast } $$ Here, $T_{a}, E_{b}, D_{c}$ is defined respectively in $L^{2}$ as translation, modulation, dilation. Proof Translation By substituting with $t := x - a$, $$</description></item><item><title>Convergence of Cauchy Sequences in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1718/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1718/</guid><description>Definition Let $\left\{ p_{n} \right\}$ be a sequence of points in a metric space $(X,d)$. If for every positive number $\varepsilon$, there exists a positive number $N$ such that $$ n\ge N,\ m\ge N \implies d(p_{n},p_{m})&amp;lt;\varepsilon $$ is satisfied, then $\left\{ p_{n} \right\}$ is called a Cauchy sequence. If every Cauchy sequence in a metric space $X$ converges to a point in $X$, then $X$ is called a complete space.</description></item><item><title>Convergence of Sequences in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1713/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1713/</guid><description>Definitions1 If there exists a point $p \in X$ such that the sequence $\left\{ p_{n} \right\}$ of points in a metric space $(X,d)$ satisfies the following condition, the sequence $\left\{ p_{n} \right\}$ is said to converge to $p$, and it is denoted by $p_{n} \rightarrow p$ or $\lim \limits_{n\to \infty}p_{n}=p$. $$ \forall \varepsilon &amp;gt;0,\ \exists N\in \mathbb{N}\ \mathrm{s.t}\ n\ge N \implies d(p_{n},p)&amp;lt;\varepsilon $$ If $\left\{ p_{n} \right\}$ does not converge,</description></item><item><title>Diameter of a Set in a Metric Space</title><link>https://freshrimpsushi.github.io/en/posts/1717/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1717/</guid><description>Definition1 Let $E$ be a subset of the metric space $(X,d)$. And suppose $S$ is as follows. $$ S=\left\{ d(p, q) : \forall p, q \in E\right\} $$ Then, the least upper bound $\sup S$ of $S$ is called the diameter of $E$ and is denoted by $\operatorname{diam} E$. Explanation Let $\left\{ p_{n} \right\}$ be a sequence in the metric space $X$, called $E_{N}=\left\{ p_{N},p_{N+1},p_{N+2},\cdots \right\}$. Then, by the definition</description></item><item><title>Heine-Borel Theorem</title><link>https://freshrimpsushi.github.io/en/posts/377/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/377/</guid><description>Theorem Definition A set $\mathcal{O} = \left\{ ( x , y ) \ | \ x &amp;lt; y \right\}$ of open intervals that satisfies $\displaystyle E \subset \bigcup_{\alpha \in \forall} ( x_{\alpha} , y_{\alpha})$ for a subset $E \subset \mathbb{R}$ of real numbers is called an open covering of $E$. That $E$ is compact means for every open covering $\mathcal{O}$ of $E$, there exists a finite subset $\left\{ O_{1} ,</description></item><item><title>Every Non-Empty Perfect Set in Euclidean Space is Uncountable</title><link>https://freshrimpsushi.github.io/en/posts/1712/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1712/</guid><description>Definition Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. The set that includes all $q$s that satisfy $d(q,p)&amp;lt;r$ is defined as the neighborhood of point $p$ and denoted by $N_{r}(p)$. In this case, $r$ is called the radius of $N_{r}(p)$. It can also be denoted as $N_{p}$ when it&amp;rsquo;s okay to omit the distance. If every neighborhood of $p$ contains a $q$ that</description></item><item><title>Properties of Converging Real Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1714/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1714/</guid><description>Theorem 1[^1] Let $\left\{ s_{n} \right\}$, $\left\{ t_{n} \right\}$ be sequences of real (or complex) numbers and assume that $\lim \limits_{n\to\infty} s_{n}=s$, $\lim\limits_{n\to\infty}t_{n}=t$. Then (a) $\lim \limits_{n\to\infty}(s_{n}+t_{n})=s+t$ (b) $\forall c \in \mathbb{C},\quad\lim \limits_{n\to\infty} cs_{n}=cs \quad \text{and} \quad \lim \limits_{n\to\infty} (c+s_{n})=c+s$ (c) $\lim \limits_{n\to\infty} s_{n}t_{n}=st$ (d) $\forall s_{n}\ne 0,s\ne0,\quad \lim \limits_{n\to\infty}\frac{1}{s_{n}}=\frac{1}{s}$ Of course, this can be extended to $\mathbb{R}^{k}$ as well. See Theorem 2 for a deeper look. Proof (a) Given</description></item><item><title>Riemann Zeta Function</title><link>https://freshrimpsushi.github.io/en/posts/1626/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1626/</guid><description>Definition The function defined as $\zeta : \mathbb{C} \setminus \left\{ 1 \right\} \to \mathbb{C}$ is called the Riemann Zeta Function. $$ \zeta (s) := \sum_{n \in \mathbb{N}} n^{-s} = \prod_{p : \text{prime}} \left( 1- {p^{-s}} \right)^{-1} $$ Related Theorems [0] Ramanujan Sum: If $\displaystyle \sum_{n \in \mathbb{N}} x^{n-1} = {{ 1 } \over { 1-x }}$ is accepted to hold even at $|x| = 1$, $$ \zeta (0) = 1</description></item><item><title>Every k Cell is Compact</title><link>https://freshrimpsushi.github.io/en/posts/1711/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1711/</guid><description>Definition For $a_{i},b_{i} \in \mathbb{R} (1\le i \le k)$, the set $I=[a_{1},b_{1}] \times [a_{2},b_{2}]\times \cdots \times [a_{k},b_{k}]$ is called a $k$-cell. Here, $\times$ represents the Cartesian product of sets. Theorem 1 Let&amp;rsquo;s assume a sequence of closed intervals on $\mathbb{R}$, $\left\{ I_{n} \right\}$, satisfies $I_{n}\supset I_{n+1}\ (n=1,2,\cdots)$. Then, the following holds true. $$ \bigcap_{i=1}^{\infty}I_{n}\ne \varnothing $$ Proof Let&amp;rsquo;s denote $I_{n}=[a_{n},b_{n}]$. Also, let $E=\left\{ a_{n} : n=1,2,\cdots \right\}$. Then, $E\ne \varnothing$</description></item><item><title>Generalized Cantor's Intersection Theorem in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1710/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1710/</guid><description>Theorem 1 Let&amp;rsquo;s assume that $(X,d)$ is a metric space. $K_{n}\subset X (n=1,2,\cdots)$ is a non-empty compact subset. In this case, if $\left\{ K_{n} \right\}$ $$ K_{n}\supset K_{n+1}\ (n=1,2,\cdots) $$ is satisfied, then $\bigcap _{i=1}^{\infty} K_{n} \ne \varnothing$ is true. If we set $\left\{ K_{n} \right\}$ as above, it has the finite intersection property, and therefore it immediately applies as a corollary of the theorem shown below. When set to</description></item><item><title>P-adic Numbers in Number Theory</title><link>https://freshrimpsushi.github.io/en/posts/1707/</link><pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1707/</guid><description>Definition 1 A $p$-adic valuation for a prime number $p$ and an integer $a \in \mathbb{Z}$ is defined as follows with respect to $v_{p}$ and is represented as $a$ of $p$-adic valuation. $$ v_{p} (a) := \sup \left\{ e \in \mathbb{Z} : p^{e} \mid a \right\} $$ Theorem 2 [0]: For every prime number $p$ $$ v_{p} (0) = \infty $$ [1]: $$v_{p} (xy) = v_{p}(x) + v_{p}(y)$$ [2]: $$v_{p}</description></item><item><title>Proof of Exponential Auxiliary Lemma</title><link>https://freshrimpsushi.github.io/en/posts/1708/</link><pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1708/</guid><description>Theorem $n \in \mathbb{N}$, $x , y \in \mathbb{Z}$, prime numbers $p \ne 2$ satisfy $$ \gcd (n,p) = 1 \\ p \mid (x - y) \\ p \nmid x \\ p \nmid y $$ then $$ v_{p} \left( x^{n} - y^{n} \right) = v_{p} \left( x - y \right) + v_{p} (n) $$ $v_{p} (a)$ represents the $p$-ary expansion of $a$. Proof 1 Strategy: It naturally follows from the</description></item><item><title>Translation in L2 Spaces: Translations, Modulations, Dilations</title><link>https://freshrimpsushi.github.io/en/posts/1616/</link><pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1616/</guid><description>Definition1 Translation is defined as $T_{a} : L^{2} \to L^{2}$ for $a \in \mathbb{R}$ as follows. $$ \left( T_{a} f \right) (x) := f(x-a) $$ Modulation is defined as $E_{b} : L^{2} \to L^{2}$ for $b \in \mathbb{R}$ as follows. $$ \left( E_{b} f \right) (x) := e^{2 \pi i b x} f(x) $$ Dilation is defined as $D_{c} : L^{2} \to L^{2}$ for $c &amp;gt; 0$ as follows. $$</description></item><item><title>Closed Subsets of Compact Sets in Metric Spaces are Compact</title><link>https://freshrimpsushi.github.io/en/posts/1706/</link><pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1706/</guid><description>Theorem1 In a metric space $X$, a closed (relative to $X$) subset of a compact set $K$ is compact. Proof Given a metric space $X$ where $F\subset K \subset X$ and assuming $F$ is a closed set in $X$ and $K$ is a compact set. Let $\left\{ V_{\alpha}\right\}$ be an arbitrary open cover of $F$. By adding $F^{c}$, let&amp;rsquo;s denote it as $\Omega=\left\{ V_\alpha \right\}\cup \left\{ F^{c} \right\}$. Then $\Omega$</description></item><item><title>Compactness in Metric Space</title><link>https://freshrimpsushi.github.io/en/posts/1705/</link><pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1705/</guid><description>Definition Open Cover Given a metric space $(X,d)$ and a subset $E\subset X$, a set of open sets $\left\{ O_{\alpha} \right\}$ that satisfies the following equation $X$ is called an open cover of $E$. $$ E\subset \bigcup _{\alpha} O_{\alpha} $$ A subset of an open cover is called a subcover. Specifically, a subcover with a finite number of elements is called a finite subcover. Compact Given a subset $K$ of</description></item><item><title>Mean and Variance of the F-distribution</title><link>https://freshrimpsushi.github.io/en/posts/1608/</link><pubDate>Sat, 15 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1608/</guid><description>Formulas $X \sim F ( r_{1} , r_{2})$ Surface Area $$ E(X) = {{ r_{2} } \over { r_{2} - 2 }} \qquad , r_{2} &amp;gt; 2 \\ \Var(X) = {{ 2 d_{2}^{2} (d_{1} + d_{2} - 2) } \over { d_{1} (d_{2} -2)^{2} (d_{2} - 4) }} \qquad , r_{2} &amp;gt; 4 $$ Derivation Strategy: Like the chi-squared distribution, the F-distribution also has known moment-generating functions, which we will</description></item><item><title>Relatively Open Sets in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1703/</link><pubDate>Sat, 15 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1703/</guid><description>Explanation Let&amp;rsquo;s say there are two metric spaces $Y\subset X$. And suppose a subset $E \subset Y \subset X$ is given. If $E$ is open with respect to the entire space $X$, then by the definition of being open and interior points, $E$ remains an open set even when $Y$ is considered the entire space. This is because the situation involves a reduction of the whole set hence there is</description></item><item><title>Proof of the Anderson-Livingston Theorem</title><link>https://freshrimpsushi.github.io/en/posts/20/</link><pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/20/</guid><description>Theorem 1 $R$ If a commutative ring has a unity $1$ and the set of its zero divisors is denoted as $Z(R)$, then its zero divisor graph $\Gamma (R)$ is a connected graph and $\text{diam}(\Gamma (R)) \le 3$ $\text{diam}$ represents the diameter of a graph. Explanation Anderson and Livingstone have made significant contributions to the study of zero divisor graphs, particularly this theorem that specifies the upper limit of graph</description></item><item><title>Properties of Open and Closed Sets in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1702/</link><pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1702/</guid><description>Let $(X,d)$ be a metric space. Suppose $p \in X$ and $E \subset X$. The set that contains all $q$s satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhood of point $p$ and is denoted by $N_{r}(p)$. Here $r$ is called the radius of $N_{r}(p)$. When it&amp;rsquo;s possible to omit the metric, it can also be denoted as $N_{p}$. If every neighborhood of $p$ contains $q$s with $q\ne p$ and $q\in E$,</description></item><item><title>Closure and Derived Set in Metric Space</title><link>https://freshrimpsushi.github.io/en/posts/1701/</link><pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1701/</guid><description>Definitions Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. A set that contains all $q$ satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhood of point $p$, denoted by $N_{r}(p)$. In this case, $r$ is called the radius of $N_{r}(p)$. If it&amp;rsquo;s permissible to omit the distance, it can also be denoted as $N_{p}$. If every neighborhood of $p$ includes a $q$ that is $q\ne</description></item><item><title>Generalized Dirichlet Product Representation for partial Sums of Arithmetic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1607/</link><pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1607/</guid><description>Lemma 1 Let&amp;rsquo;s define $h = f \ast g$ for the arithmetic function $f,g,h$ as follows: $$ F (x) := \sum_{n \le x} f(x) \\ G (x) := \sum_{n \le x} g(x) \\ H (x) := \sum_{n \le x} h(x) $$ Then, $$ H = f \circ G = g \circ F $$ where the operation $\circ$ refers to the generalized convolution. In other words, the following is true: $$</description></item><item><title>Neighborhood, Limit Point, Open, Closed in Metric Space</title><link>https://freshrimpsushi.github.io/en/posts/1700/</link><pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1700/</guid><description>Definition Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. The set that includes all $q$s satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhood of point $p$ and is denoted as $N_{r}(p)$. Here, $r$ is called the radius of $N_{r}(p)$. If the distance can be omitted, it may also be denoted as $N_{p}$. If all neighborhoods of $p$ contain $q$, which is $q\ne p$ and</description></item><item><title>F-distribution</title><link>https://freshrimpsushi.github.io/en/posts/1606/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1606/</guid><description>Definition 1 The continuous probability distribution $F \left( r_{1} , r_{2} \right)$, which has the following probability density function for degrees of freedom $r_{1}, r_{2} &amp;gt; 0$, is called the F-distribution. $$ f(x) = {{ 1 } \over { B \left( r_{1}/2 , r_{2} / 2 \right) }} \left( {{ r_{1} } \over { r_{2} }} \right)^{r_{1} / 2} x^{r_{1} / 2 - 1} \left( 1 + {{ r_{1} }</description></item><item><title>Integrable Functions and Absolute Values</title><link>https://freshrimpsushi.github.io/en/posts/1697/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1697/</guid><description>This article is based on Riemann-Stieltjes integration. If set as $\alpha=\alpha (x)=x$, it equals the Riemann integration. Theorem1 Let function $f$ be Riemann(-Stieltjes) integrable over the interval $[a,b]$. Then (a) $\left|f\right|$ is also integrable over $[a,b]$. (b) Furthermore, the following inequality holds: $$ \left|\int_{a}^{b}fd\alpha \right| \le \int_{a}^{b}\left| f\right| d\alpha $$ Proof (a) Integrability is defined for bounded functions. Hence, assuming that $f$ is integrable implies that $f$ is bounded. Let&amp;rsquo;s</description></item><item><title>Riemann-Stieltjes Integrability is Preserved within an Interval</title><link>https://freshrimpsushi.github.io/en/posts/1695/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1695/</guid><description>The following document is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem 1 Let function $f$ be Riemann(-Stieltjes) integrable on $[a,b]$. Let us also say $a&amp;lt;c&amp;lt;b$. Then, $f$ is also integrable on $[a,c]$ and $[c,b]$, and the sum of the integration values is equal to the integral on $[a,b]$. $$ \int_{a}^{c}fd\alpha + \int_{c}^{b}fd\alpha=\int_{a}^{b}f d\alpha $$ Proof In the first</description></item><item><title>The Fundamental Theorem of Calculus in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1698/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1698/</guid><description>Theorem1 Let&amp;rsquo;s say $f$ is a function that is Riemann integrable over the interval $[a,b]$. And let&amp;rsquo;s define $F$ for $a\le x \le b$ as follows. $$ F(x) = \int _{a} ^{x} f(t)dt $$ (a) Then, $F$ is continuous over $[a,b]$. (b) If $f$ is continuous over $x_{0}\in [a,b]$, then $F$ is differentiable over $x_{0}$ and satisfies $F^{\prime}(x_{0})=f(x_{0})$. Explanation This is known by the name Fundamental Theorem of Calculus 1,</description></item><item><title>The Relationship Between the Size of Integrals Based on the Order of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1696/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1696/</guid><description>This article is based on Riemann-Stieltjes integration. If we set it as $\alpha=\alpha (x)=x$, it is the same as Riemann integration. Theorem1 Let us assume that two functions $f_{1}, f_{2}$ are Riemann(-Stieltjes) integrable over the interval $[a,b]$. Also, assume that $f_{1} \le f_{2}$ in $[a,b]$. Then, the following inequality holds. $$ \int_{a}^{b}f_{1}d\alpha \le \int_{a}^{b}f_{2}d\alpha $$ Proof Let there be a positive $\varepsilon &amp;gt;0$. Since $f_{2}$ is integrable, by the necessary</description></item><item><title>Erdős–Rényi Graph</title><link>https://freshrimpsushi.github.io/en/posts/1604/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1604/</guid><description>Definition Let&amp;rsquo;s assume a commutative ring $R$ is given. The set of zero divisors within $R$ is denoted as $Z(R)$. The graph $\Gamma (R)$ defined below is referred to as the Zero Divisor Graph for $R$. $$ V \left( \Gamma (R) \right) = Z(R) \\ E( \Gamma (R)) = \left\{ ab : ab=0 \right\} $$ Description As is known, the product of zero divisors does not necessarily yield $0$. For</description></item><item><title>Generalized Dirichlet Product</title><link>https://freshrimpsushi.github.io/en/posts/1603/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1603/</guid><description>Definition 1 Let $F : \mathbb{R}^{+} \to \mathbb{C}$ be a function where $x \in (0,1)$, and let $F(x) = 0$ apply. For any arithmetic function $\alpha$, the following operation $\circ$ is defined as the generalized Dirichlet product. $$ (\alpha \circ F)(x) := \sum_{n \le x} \alpha (n) F \left( {{ x } \over { n }} \right) $$ Basic Properties Let $\alpha$ and $\beta$ be arithmetic functions, and let $F</description></item><item><title>Day, Work-Energy Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1694/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1694/</guid><description>Definition A work is defined as the product of the magnitude of a force $\mathbf{F}$ and the distance $s$ by which an object has moved in the same direction as the force $W=Fs$, when the force $\mathbf{F}$ acts on the object. Description In the distance-force graph, the area under the graph is equal to the amount of work. Since the direction of movement and the direction of the force must</description></item><item><title>The Mean and Variance of the Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1601/</link><pubDate>Wed, 05 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1601/</guid><description>Formula If $X \sim \chi^{2} (r)$ then $$ E(X) = r \\ \Var (X) = 2r $$ Derivation Strategy: Fortunately, the moment generating function of the chi-squared distribution is known. Moment of the chi-squared distribution: Let&amp;rsquo;s say $X \sim \chi^{2} (r)$. If $k &amp;gt; - r/ 2$, then there exists the $k$th moment $$ E X^{k} = {{ 2^{k} \Gamma (r/2 + k) } \over { \Gamma (r/2) }} $$</description></item><item><title>Formula for the Roots of a Cubic Equation</title><link>https://freshrimpsushi.github.io/en/posts/1692/</link><pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1692/</guid><description>Formulas The solution of the cubic equation $t^{3}+pt+q = 0$ is as follows. $$ \begin{cases} t_{1}=u_{1}+v_{1}=\sqrt[3]{-\frac{q}{2}+\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}+\sqrt[3]{-\frac{q}{2}-\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}} \\ t_{2}=u_{2}+v_{3}=\sqrt[3]{-\frac{q}{2}+\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}\omega+\sqrt[3]{-\frac{q}{2}-\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}\omega^{2} \\ t_{3}=u_{3}+v_{2}=\sqrt[3]{-\frac{q}{2}+\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}\omega^{2}+\sqrt[3]{-\frac{q}{2}-\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}\omega\end{cases} $$ Here $\omega = e^{i\frac{2}{3}\pi}$. Proof Cardano&amp;rsquo;s Method Let us consider a cubic equation $ax^{3}+bx^{2}+cx+d=0(a\ne0)$ is given. To simplify the solution, without loss of generality, it is shown as follows. $$ \begin{equation} x^{3}+ax^{2} +bx+c=0 \end{equation} $$ To eliminate the quadratic term, substitute $x=t-{\textstyle \frac{a}{3}}$. Then it becomes: $$ \begin{align*} &amp;amp;&amp;amp;\left(</description></item><item><title>Relationships between the Roots and Coefficients of Quadratic/Tertiary/nth Degree Equations</title><link>https://freshrimpsushi.github.io/en/posts/1691/</link><pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1691/</guid><description>Formulas Relationship Between Roots and Coefficients of a Quadratic Equation Let&amp;rsquo;s consider the roots of the quadratic equation $ax^{2}+bx+c=0$ to be $\alpha$ and $\beta$. Then the following relationship holds true. $$ \alpha+\beta=-\frac{b}{a}\quad \&amp;amp; \quad \alpha\beta= \frac{ c}{a} $$ Relationship Between Roots and Coefficients of a Cubic Equation Let&amp;rsquo;s consider the roots of the cubic equation $ax^{3}+bx^{2}+cx+d=0$ to be $\alpha$, $\beta$, and $\gamma$. Then the following relationship holds true. $$ \alpha</description></item><item><title>Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1600/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1600/</guid><description>Definition 1 The chi-square distribution refers to a continuous probability distribution $\chi^{2} (r)$ with the following probability density function, defined over the degrees of freedom $r &amp;gt; 0$. $$ f(x) = {{ 1 } \over { \Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} \qquad , x \in (0, \infty) $$ $\Gamma$ represents the gamma function. Basic Properties Moment Generating Function [1]: $$m(t) = (1-2t)^{-r/2} \qquad , t &amp;lt; {{ 1 }</description></item><item><title>Kepler's First Law: The Law of Elliptical Orbits</title><link>https://freshrimpsushi.github.io/en/posts/1689/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1689/</guid><description>Kepler&amp;rsquo;s First Law: The Law of Elliptical Orbits Planets revolve in elliptical orbits with the Sun at one focus. This is the first law among Kepler&amp;rsquo;s laws of planetary motion. Proof1 The equation of orbit for a particle moving under a central force is as follows. $$ \frac{ d ^{2}u}{ d \theta^{2} } + u=-\frac{1}{ml^{2}u^{2}}F(u^{-1}) $$ Here, $u={\textstyle \frac{1}{r}}$. Since we want to solve the problem with respect to gravity,</description></item><item><title>Kepler's Third Law: The Harmony of the Worlds</title><link>https://freshrimpsushi.github.io/en/posts/1690/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1690/</guid><description>Kepler&amp;rsquo;s Third Law: The Law of Harmonies The square of the orbital period of a planet is proportional to the cube of the semi-major axis of its orbit. Kepler&amp;rsquo;s third law among Kepler&amp;rsquo;s laws of planetary motion. When approximating the orbit of a planet as a circle, it becomes &amp;lsquo;The square of the orbital period is proportional to the cube of the distance to the sun&amp;rsquo;. Proof1 Let the area</description></item><item><title>Elliptic Integral of the Second Kind</title><link>https://freshrimpsushi.github.io/en/posts/1687/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1687/</guid><description>Definition The integral below is referred to as the complete elliptic integral of the second kind. $$ E(k)=\int_{0}^{{\textstyle \frac{\pi}{2}}}\sqrt{1-k^{2} \sin ^{2} \theta} d\theta $$ The integral below is referred to as the incomplete elliptic integral of the second kind. $$ E(\phi, k)=\int_{0}^{\phi}\sqrt{1-k^{2} \sin ^{2} \theta}d\theta $$ Explanation The reason why the above two integrals are named elliptic integrals is that they emerge from the process of calculating the perimeter of</description></item><item><title>Perimeter of an Ellipse</title><link>https://freshrimpsushi.github.io/en/posts/1688/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1688/</guid><description>Most materials do not provide a detailed explanation of how the elliptic integral of the second kind is derived. Even if they do, many are incorrect1 so I wrote the &amp;lsquo;accurate&amp;rsquo; and &amp;lsquo;detailed&amp;rsquo; content myself. For reference, the content in Boas&amp;rsquo; Mathematical Methods in the Physical Sciences, 3rd edition, is also incorrect. Formula The perimeter of an ellipse with semi-major axis $a$, semi-minor axis $b$, and eccentricity $k^{2}$ is calculated</description></item><item><title>Derivation of the Beta Distribution from Two Independent Gamma Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1596/</link><pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1596/</guid><description>Theorem If two random variables $X_{1},X_{2}$ are independent and $X_{1} \sim \Gamma ( \alpha_{1} , 1)$, $X_{2} \sim \Gamma ( \alpha_{2} , 1)$, then $$ {{ X_{1} } \over { X_{1} + X_{2} }} \sim \text{beta} \left( \alpha_{1} , \alpha_{2} \right) $$ Explanation If two data points follow the gamma distribution and are independent, it could be possible to explain the ratio of their sum using probability distribution theory. Specifically,</description></item><item><title>Ellipse</title><link>https://freshrimpsushi.github.io/en/posts/1685/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1685/</guid><description>Definition The set of points on a plane whose sum of distances to two fixed points $F$, $F^{\prime}$ is constant, are called an ellipse. The components of an ellipse are as follows. $F$, $F^{\prime}$ are called foci. $a$ is called the semimajor axis, and $b$ is called the semiminor axis. $b=\sqrt{1-\epsilon^{2}}a$ is satisfied. $\epsilon$ is called the eccentricity of the ellipse. It represents how ellipsed is compressed, and the foci</description></item><item><title>Equation of an Ellipse with the Focus at the Origin in Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1686/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1686/</guid><description>Theorem The equation of an ellipse in polar coordinates is given as follows. $$ r=\frac{\alpha}{1+\epsilon \cos \theta}\tag{a} $$ or $$ r=\frac{b^{2}/a}{1+\frac{\sqrt{a^{2}-b^{2}}}{a}\cos\theta} \tag{b} $$ Where $\alpha$ is the focal parameter, $\epsilon$ is the eccentricity, $a$ is the semi-major axis, and $b$ is the semi-minor axis. Explanation The two proofs below are essentially the same. Proof High School Level The definition of an ellipse is the set of points where the sum</description></item><item><title>Hilbert Space Frames</title><link>https://freshrimpsushi.github.io/en/posts/1595/</link><pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1595/</guid><description>Definition1 A sequence $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}}$ in a Hilbert space $H$ is called a frame if there exists a $A,B &amp;gt; 0$ satisfying the following, and especially when $A = B$, this frame is said to be tight. $$ A \left\| \mathbf{v} \right\|^{2} \le \sum_{k \in \mathbb{N}} \left| \left\langle \mathbf{v} , \mathbf{v}_{k} \right\rangle \right|^{2} \le B \left\| \mathbf{v} \right\|^{2} \qquad , \forall \mathbf{v} \in H $$ Explanation Unlike</description></item><item><title>The Support of Functions and the Class of Continuous Function Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1594/</link><pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1594/</guid><description>Definitions Let&amp;rsquo;s consider a function $f : \mathbb{R} \to \mathbb{C}$ in the function space $\mathbb{C}^{\mathbb{R}}$. The support of a function $f$ is defined as the closed set obtained by taking the closure of the set of points where the function value is not $0$. $$ \text{supp} f = \overline{\left\{ x \in \mathbb{R} : f(x) \ne 0 \right\}} $$ If $\text{supp} f$ is bounded, then $f$ is said to have a</description></item><item><title>Orbit Equation of a Particle under Central Force</title><link>https://freshrimpsushi.github.io/en/posts/1684/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1684/</guid><description>Orbit Equation of a Particle Subject to Central Force1 A particle of mass $m$ subject to a central force can be described by its motion equation in polar coordinates as follows. $$ \begin{equation} m\ddot{\mathbf{r}}=F(r)\hat{\mathbf{r}} \label{eq1} \end{equation} $$ $F(r)$ represents the central force acting on the particle. The acceleration in polar coordinates can be expressed as: $$ \ddot{\mathbf{r}}=\mathbf{a}=\left( \ddot{r}-r\dot{\theta}{}^{2} \right)\hat{\mathbf{r}} +\left(2\dot{r}\dot{\theta}+r\ddot{\theta} \right)\hat{\boldsymbol{\theta}} $$ Hence, separating the motion equation $\eqref{eq1}$ into components</description></item><item><title>Derivation of the Equation of an Ellipse</title><link>https://freshrimpsushi.github.io/en/posts/1683/</link><pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1683/</guid><description>Formulas The equation of an ellipse with the center at $(x_{0},y_{0})$, major axis $a$, and minor axis $b$ is as follows. $$ \frac{(x-x_{0})^{2}}{a^{2}}+\frac{(y-y_{0})^{2}}{b^{2}}=1 $$ Description An ellipse is a set of points where the sum of the distances to two foci is constant. Derivation Let&amp;rsquo;s consider an ellipse as shown in the figure above. Based on the definition of an ellipse, we can establish the following equation. $$ \begin{align*} \overline{F^{\prime}P}</description></item><item><title>Hilbert Space's Orthonormal Basis and Unitary Operator</title><link>https://freshrimpsushi.github.io/en/posts/1593/</link><pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1593/</guid><description>Definition If a Schauder basis $\left\{ \mathbf{e}_{k} \right\}_{k \in \mathbb{N}}$ of a Hilbert space $H$ is a normal orthogonal system, then $\left\{ \mathbf{e}_{k} \right\}_{k \in \mathbb{N}}$ is called the Orthonormal Basis of $H$. Theorem1 Equivalent Conditions for Orthonormal Basis [1]: Assuming $H$ is a Hilbert space. For the normal orthogonal system $\left\{ \mathbf{e}_{k} \right\}_{k \in \mathbb{N}} \subset H$ of $H$, the following are all equivalent. (i): $\left\{ \mathbf{e}_{k} \right\}_{k \in</description></item><item><title>Kepler's Second Law: The Law of Equal Areas</title><link>https://freshrimpsushi.github.io/en/posts/1682/</link><pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1682/</guid><description>Kepler&amp;rsquo;s Second Law: Law of Equal Areas1 A line segment joining a planet and the Sun sweeps out equal areas during equal intervals of time. The Law of Equal Areas is the second of Kepler&amp;rsquo;s laws of planetary motion. However, this is not a special law that occurs only between the Sun and the planets, but a general law that is valid for any object (particle) moving under a central</description></item><item><title>The Magnitude of the Cross Product of Two Vectors is Equal to the Area of the Parallelogram They Form</title><link>https://freshrimpsushi.github.io/en/posts/1681/</link><pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1681/</guid><description>Theorem The magnitude of the cross product of two vectors $\mathbf{A}$ and $\mathbf{B}$, when the angle between them is $\theta$, is as follows: $$ \left| \mathbf{A}\times \mathbf{B}\right| =\left|\mathbf{A}\right|\left| \mathbf{B} \right|\sin \theta $$ And this is equal to the area of the parallelogram that the two vectors form. Proof Let&amp;rsquo;s say the two vectors $\mathbf{A}=(A_{x},A_{y},A_{z})$ and $\mathbf{B}=(B_{x},B_{y},B_{z})$ are as shown in the figure above. Then part 1. Area of the parallelogram</description></item><item><title>Kepler's Laws of Planetary Motion</title><link>https://freshrimpsushi.github.io/en/posts/1680/</link><pubDate>Tue, 21 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1680/</guid><description>Kepler&amp;rsquo;s Laws of Planetary Motion1 The Kepler&amp;rsquo;s laws are empirical laws formulated by compiling and analyzing the observation records of astronomers such as Tycho Brahe and Hipparchus. They greatly contributed to the derivation of Newton&amp;rsquo;s Law of Universal Gravitation. Conversely, Kepler&amp;rsquo;s laws could be mathematically explained through the Law of Universal Gravitation and Newton&amp;rsquo;s Laws of Motion. First Law: The Law of Ellipses The orbit of a planet is an</description></item><item><title>Uniform Sphere Shell and the Gravity of a Separated Particle</title><link>https://freshrimpsushi.github.io/en/posts/1679/</link><pubDate>Tue, 21 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1679/</guid><description>Uniform Spherical Shell and a Displaced Particle&amp;rsquo;s Gravity1 Let&amp;rsquo;s assume there is a uniform spherical shell with a total mass of $M$ and a radius of $R$. And there exists a particle with a mass of $m$, displaced at a distance of $r$ from the center $O$ of the spherical shell. In this case, $R&amp;lt;r$ holds. Let&amp;rsquo;s first calculate the force exerted by a portion of the spherical shell on</description></item><item><title>Centrifugal Force</title><link>https://freshrimpsushi.github.io/en/posts/1677/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1677/</guid><description>Definition A force is called a central force if its direction is always towards the same point, regardless of the position of the object it acts upon. Gravity is a typical example. No matter where on Earth we are, the gravity acting on us is directed towards the center of the Earth. It can be mathematically demonstrated that a particle moving under a central force conserves angular momentum and maintains</description></item><item><title>Four Color Map Problem</title><link>https://freshrimpsushi.github.io/en/posts/1592/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1592/</guid><description>Buildup The Four Color Map Problem asks whether four colors are sufficient to color any given map so that adjacent regions are distinguishable. It might seem like complex maps require more colors, but since only adjacent regions need to be different, it&amp;rsquo;s not as many as one might think. For example, the following is a world map colored with just $4$ colors. Historically, the Four Color Map Problem was first</description></item><item><title>Kinetic Energy of Particle Systems</title><link>https://freshrimpsushi.github.io/en/posts/1676/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1676/</guid><description>Particle System&amp;rsquo;s Kinetic Energy1 The kinetic energy of a particle system, like the linear momentum and angular momentum we defined before, can also be naturally defined as the sum of the kinetic energy of each particle. $$ \begin{equation} T=\sum \limits _{i=1} ^{n} \frac{ 1}{ 2 }m_{i}v_{i}^{2} \label{kinetic} \end{equation} $$ Now, we will do the same operation for the particle system&amp;rsquo;s linear and angular momentum, representing each particle&amp;rsquo;s position vector with</description></item><item><title>Law of Universal Gravitation: Gravity</title><link>https://freshrimpsushi.github.io/en/posts/1678/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1678/</guid><description>Law of Universal Gravity1 The law of universal gravity, announced by Newton through his Principia in 1687, is a physical law that simply states &amp;ldquo;every object attracts every other object&amp;rdquo;. To describe this concept in detail: Every particle of matter in the universe with mass attracts every other particle with a force that is directly proportional to the product of their masses and inversely proportional to the square of the</description></item><item><title>Angular Momentum of Particle Systems</title><link>https://freshrimpsushi.github.io/en/posts/1675/</link><pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1675/</guid><description>Formulas The torque of a particle system equals the sum of the torques of each particle. $$ \mathbf{N}=\frac{ d \mathbf{L}}{ d t }=\sum \limits _{i=1} ^{n} \mathbf{r}_{i}\times \mathbf{F}_{i} $$ Derivation1 The linear momentum of a particle system was defined as the sum of the linear momenta of each particle. Similarly, the angular momentum of a particle system is defined as the sum of the angular momenta of each particle. $$</description></item><item><title>Proving that Every Separable Hilbert Space is Isometric to the l^2 space</title><link>https://freshrimpsushi.github.io/en/posts/1591/</link><pubDate>Sat, 18 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1591/</guid><description>Theorem1 Every infinite-dimensional separable Hilbert space $H$ is isometrically isomorphic to $\ell^{2}$. Description The fact that a separable Hilbert space is isometrically isomorphic to $\ell^{2}$ essentially means that in the study of Hilbert spaces, one can focus solely on $\ell^{2}$. Proof Gram-Schmidt Orthogonalization of Separable Hilbert Spaces Every separable Hilbert space has an orthonormal basis. Corollary of Bessel&amp;rsquo;s Inequality If $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}}$ is an orthonormal set in</description></item><item><title>Angular Momentum and Torque</title><link>https://freshrimpsushi.github.io/en/posts/1674/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1674/</guid><description>Angular Momentum1 Momentum is a physical quantity that represents the state of motion of a moving object. The larger the mass and the faster the speed, the greater the momentum. In physics, there is an interest in how the motion of an object changes. Therefore, the force, which is the cause of changing the state of motion of an object, is expressed as a change in momentum. $$ \mathbf{F}=\frac{d \mathbf{p}}{dt}</description></item><item><title>Newton's Laws of Motion</title><link>https://freshrimpsushi.github.io/en/posts/1671/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1671/</guid><description>Newton&amp;rsquo;s Laws of Motion 1 English mathematician and physicist Isaac Newton presented three laws about motion in 1687 in Principia as follows: An object not subjected to an external force does not change its state of motion. The change in motion is proportional to the applied force on the object. When object 1 applies a force to object 2, object 2 simultaneously applies a force equal in magnitude and opposite</description></item><item><title>Physics: The Definition of Mass, Force, and Momentum</title><link>https://freshrimpsushi.github.io/en/posts/1673/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1673/</guid><description>Mass1 In Newton&amp;rsquo;s laws of motion, inertia is described as the property that resists changes in motion. That is, the greater the inertia, the harder it is to move, and the smaller the inertia, the easier it is to move. This exactly aligns with our experience that it&amp;rsquo;s harder to push a heavier object than a lighter one. Hence, the magnitude of inertia can be expressed by the magnitude of</description></item><item><title>Proof of the Five Color Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1590/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1590/</guid><description>Theorem 1 Every simple plane graph is $5$-colorable. Wilson. (1970). Introduction to Graph Theory: p83.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>The Reason Why Momentum is Denoted by p</title><link>https://freshrimpsushi.github.io/en/posts/1672/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1672/</guid><description>Explanation Many of the symbols used in physics can be easily understood why they were chosen without much thought. For example, the symbols for force, mass, velocity, and acceleration denoted by $\mathbf{F}$, $m$, $\mathbf{v}$, and $\mathbf{a}$ respectively, can be easily guessed to originate from the first letter of the corresponding English words force, mass, velocity, acceleration. However, the symbol for momentum is $\mathbf{p}$, even though momentum is the term used</description></item><item><title>Gram-Schmidt Orthogonalization in Separable Hilbert Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1589/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1589/</guid><description>Theorem1 Every separable Hilbert space has an orthonormal basis. Proof Strategy: Essentially the same as Gram-Schmidt orthonormalization in finite-dimensional vector spaces. Since the existence of a basis is not guaranteed in general Hilbert spaces unlike finite-dimensional vector spaces, it is necessary to first choose an orthogonal basis $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}}$ due to separability, before proceeding with orthonormalization. $$ \overline{\text{span}} \left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}} = H $$ If</description></item><item><title>Center of Mass and Linear Momentum of a Particle System</title><link>https://freshrimpsushi.github.io/en/posts/1670/</link><pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1670/</guid><description>Definition A system of particles is referred to as a particle system. Description 1 When the position vectors of particles with masses $m_{1}$, $m_2$, $\cdots$, and $m_{n}$ are $\mathbf{r}_{1}$, $\mathbf{r}_{2}$, $\cdots$, and $\mathbf{r}_{n}$, respectively, the center of mass of this particle system is defined as follows. $$ \mathbf{r}_{cm}=\frac{m_{1}\mathbf{r}_{1}+m_{2}\mathbf{r}_{2}+\cdots + m_{n}\mathbf{r}_{n}}{m_{1}+ m_{2}+ \cdots+ m_{n}}=\frac{\sum m_{i}\mathbf{r}_{i}}{m} $$ Here, $m=\sum \limits_{i}m_{i}$ denotes the total mass of the particle system. The subscript $cm$ stands</description></item><item><title>Definition of Maps in Graph Theory</title><link>https://freshrimpsushi.github.io/en/posts/1586/</link><pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1586/</guid><description>Definition 1 A planar graph connected by $3$ is defined as a map. A map that can be colored with $k$ different colors in such a way that adjacent faces across the same edge have different colors is called a $k$-face colorable map. The existing $k$-colorable graph is called a $k$-vertex colorable graph. The regions distinguished on the plane while drawing a planar graph are called faces. Description $3$-Graph is</description></item><item><title>Reordering of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1585/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1585/</guid><description>Definition 1 Let us suppose a sequence $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}}$ in a vector space $V$ is given. For a given bijection $\sigma : \mathbb{N} \to \mathbb{N}$, the following is called the reordering of $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}}$. $$ \left\{ \mathbf{v}_{\sigma (k) } \right\}_{k \in \mathbb{N}} = \left\{ \mathbf{v}_{\sigma (1)} , \mathbf{v}_{\sigma (2)} , \cdots \right\} $$ Explanation Reordering is also called a Permutation, and as you can</description></item><item><title>Simple Properties of Planar Graphs</title><link>https://freshrimpsushi.github.io/en/posts/1584/</link><pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1584/</guid><description>Theorem 1 Assume that $G$ is a simple planar graph. [1]: If $G$ is a connected graph with $n \ge 3$ vertices and $m$ edges, then $m \le 3n - 6$ [2]: Every simple planar graph $G$ has at least one vertex $v \in V(G)$ with $\deg v \le 5$. Proof [1] Let&amp;rsquo;s assume every face of a planar graph is surrounded by at least three edges. For the simplest</description></item><item><title>Infinite-Dimensional Vector Spaces and Schauder Bases</title><link>https://freshrimpsushi.github.io/en/posts/1583/</link><pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1583/</guid><description>Definition1 Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ a normed space. If there exists a unique sequence of scalars $\left\{ a_{k} \right\}_{k \in \mathbb{N}}$ that satisfies the following for every element $\mathbf{x}\in X$ in $X$, then $\left\{ \mathbf{e}_{k} \right\}_{k \in \mathbb{N}} \subset X$ is called the Schauder basis of $X$. $$ \mathbf{x}= \sum_{k \in \mathbb{N}} a_{k} \mathbf{e}_{k} $$ Description The basis of a vector space is called the Schauder basis, especially</description></item><item><title>Abstract Dual Graphs</title><link>https://freshrimpsushi.github.io/en/posts/1582/</link><pubDate>Sat, 04 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1582/</guid><description>Buildup Properties of Geometric Dual Graphs [3]: For a planar graph $G$ and its geometric dual graph $G^{ \ast }$, if $C \subset E(G)$ is a cycle then $\iff$ $C^{ \ast } \subset E \left( G^{ \ast } \right)$ is a cutset Abstract Dual Graphs are defined abstractly for general graphs unlike geometric dual graphs which are intuitive for planar graphs. A graph is called a dual graph if it</description></item><item><title>Bessel Sequences in Hilbert Spaces with Dense Subspaces</title><link>https://freshrimpsushi.github.io/en/posts/1581/</link><pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1581/</guid><description>Theorem1 Given a Hilbert space $H$, suppose that $V \subset H$, which are $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}} \subset H$ and $\overline{V} = H$, satisfy the following. $$ \sum_{k \in \mathbb{N}} \left| \left\langle \mathbf{v} , \mathbf{v}_{k} \right\rangle \right|^{2} \le B \left\| \mathbf{v} \right\|^{2} \qquad , \mathbf{v} \in V $$ Then, $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}}$ is a Bessel sequence with Bessel bound $B$. Explanation Originally, Bessel sequences had to satisfy</description></item><item><title>Linearity of Riemann(-Stieltjes) Iintegral</title><link>https://freshrimpsushi.github.io/en/posts/1666/</link><pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1666/</guid><description>Theorem1 This article is based on the Riemann-Stieltjes integral. If set to $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Let&amp;rsquo;s say $f$ is integrable by Riemann(-Stieltjes) from $[a,b]$. Then, for a constant $c\in \mathbb{R}$, $cf$ is also integrable from $[a,b]$, and its value is as follows. $$ \int_{a}^{b}cf d\alpha = c\int_{a}^{b}f d\alpha $$ Let two functions $f_{1}$, $f_{2}$ be integrable by Riemann(-Stieltjes) from $[a,b]$. Then, $f_{1}+f_{2}$ is</description></item><item><title>Geometric Dual Graphs</title><link>https://freshrimpsushi.github.io/en/posts/1570/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1570/</guid><description>Definitions 1 For a given planar graph $G$, the geometric dual graph $G^{ \ast }$ is constructed as follows: Step 1. Place a vertex $v^{ \ast }$ corresponding to each face $f$ of $G$. Step 2. Draw an edge $e^{ \ast }$ corresponding to each edge $e$ of $G$, such that it overlaps. Step 3. Erase the original graph and use the vertices $v^{ \ast }$ and edges $e^{ \ast</description></item><item><title>Proof of the Generalized Bessel's Inequality in Hilbert Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1578/</link><pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1578/</guid><description>Theorem1 If $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}}$ is a regular orthogonal set in the Hilbert space $H$, the following holds. (a) For all $\left\{ c_{k} \right\}_{k \in \mathbb{N}} \in \ell^{2}$, the infinite series $\sum_{k \in \mathbb{N}} c_{k} \mathbf{v}_{k}$ converges. (b) For all $\mathbf{v} \in H$, $$ \sum_{k \in \mathbb{N}} \left| \left\langle \mathbf{v} , \mathbf{v}_{k} \right\rangle \right|^{2} \le \left\| \mathbf{v} \right\|^{2} $$ Explanation A $\ell^{2}$ space is a function space consisting</description></item><item><title>Graph k-connectivity and Menger's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1576/</link><pubDate>Fri, 26 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1576/</guid><description>Definition For a given graph $G$, let the number of components be denoted by $\text{comp} (G)$. Edge-Connectivity A set of edges satisfying the following conditions is called a disconnecting set of $G$. $$ \text{comp} \left( G \setminus D \right) &amp;gt; \text{comp}(G) $$ A disconnecting set of $G$ that does not have a proper subset which is also a disconnecting set is called a cutset of $G$. If $G$ is a</description></item><item><title>Hilbert Space Bessel Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1569/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1569/</guid><description>Definition1 A sequence $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}} \subset H$ in a Hilbert space $H$ is called a Bessel sequence and $B$ is called the Bessel bound if there exists $B &amp;gt; 0$ that satisfies the following $$ \sum_{k=1}^{\infty} \left| \left\langle \mathbf{v} , \mathbf{v}_{k} \right\rangle \right|^{2 } \le B \left\| \mathbf{v} \right\|^{2}, \quad \forall \mathbf{v} \in H $$ Explanation Intuitively, a Bessel sequence can be seen as a sequence that</description></item><item><title>Proof of Euler's Polyhedron Formula</title><link>https://freshrimpsushi.github.io/en/posts/82/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/82/</guid><description>Overview Euler&amp;rsquo;s polyhedron theorem is also called Euler&amp;rsquo;s characteristic; in graph theory, it is simply referred to as Euler’s formula. Geometrically, it signifies that the relationship between vertices, edges, and faces of a spatial figure follows #vertices-#edges+#faces=2. For instance, considering a cube, it has $8$ vertices, $12$ edges, and $6$ faces, thus $8-12+6=2$ holds. Theorem 1 Link Planar graph For $G$, let $n:=|V(G)|$, $m:=|E(G)|$,</description></item><item><title>Definition of Wavelets</title><link>https://freshrimpsushi.github.io/en/posts/1663/</link><pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1663/</guid><description>Definition Let&amp;rsquo;s denote as $\psi \in L^{2}(\mathbb{R})$. When $\psi$ satisfies the following two conditions, the function $\psi$ is called a wavelet. (a) For an integer $j,k \in \mathbb{Z}$, $\psi_{j,k}$ is defined as follows. $$ \psi_{j,k} (x):=2^{\frac{j}{2}}\psi (2^{j}x-k),\quad x\in \mathbb{R} $$ (b) $\left\{ \psi _{j,k}\right\}_{j,k\in \mathbb{Z}}$ is an orthonormal basis of $L^{2}(\mathbb{R})$ space. $\psi_{j,k}$ can also be represented by dilation D and translation $T_{k}$ as follows. $$ \psi_{j,k}=D^{j}T_{k}\psi,\quad j,k\in\mathbb{Z} $$ Explanation</description></item><item><title>Lies Basis</title><link>https://freshrimpsushi.github.io/en/posts/1568/</link><pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1568/</guid><description>Definition1 Let&amp;rsquo;s assume that a normalized orthogonal basis $\left\{ \mathbf{e}_{k} \right\}_{k \in \mathbb{N}}$ of a Hilbert space $H$ is given. If surjective $U : H \to H$ is a linear and bounded operator for all $k \in \mathbb{N}$, then $\mathbf{v}_{k} := U \mathbf{e}_{k}$ implies that $\left\{ \mathbf{v}_{k} \right\}_{k \in \mathbb{N}}$ forms a basis of $H$, and the following holds. $$ \mathbf{v} = \sum_{k \in \mathbb{N}} \left\langle \mathbf{v} , \left( U^{-1}</description></item><item><title>Hilbert Space to L2 Space: The Adjoint Operator</title><link>https://freshrimpsushi.github.io/en/posts/1566/</link><pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1566/</guid><description>Theorem1 Let $\left\{ \mathbf{v}_k \right\}_{k \in \mathbb{N}}$ be a sequence defined in a Hilbert space $H$. Let a bounded linear operator $T : \ell^{2} \to H$ be defined as follows. $$ T \left\{ c_{k} \right\}_{k \in \mathbb{N}} := \sum_{k=1}^{\infty} c_{k} \mathbf{v}_{k} $$ Then, the adjoint operator $T^{ \ast } : H \to \ell^{2}$ of $T$ is represented as follows. $$ T^{ \ast } \mathbf{v} = \left\{ \left\langle \mathbf{v} , \mathbf{v}_{k}</description></item><item><title>Planar Graphs and Kuratowski's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1565/</link><pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1565/</guid><description>Definition Planar Graph A planar graph is a graph that can be drawn on a plane without any edges crossing each other. Explanation When a planar graph is drawn, the regions that are demarcated on the plane are called faces. The following planar graph $K_{4}$ has four faces $f_{1}, f_{2}, f_{3}, f_{4}$, and among them, the one that is not bounded $f_{4}$ is called an infinite face. Planar Graphs, as</description></item><item><title>Orthogonal Projection in Hilbert Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1564/</link><pubDate>Sun, 14 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1564/</guid><description>Definition1 Let&amp;rsquo;s assume a closed subspace $V$ of a Hilbert space $H$ is given. When $\mathbf{v} \in H$ is represented as $\mathbf{v} = \mathbf{v}_{1} + \mathbf{v}_{2}$ with respect to $\mathbf{v}_{1} \in V$ and $\mathbf{v}_{2} \in V^{\perp}$, a surjection $P :H \to V$ that satisfies the following is called an orthogonal projection. $$ P \mathbf{v} = \mathbf{v}_{1} $$ Explanation Orthogonal projection has the following properties: $P$ is linear, bounded, and $|</description></item><item><title>Graph Homomorphism</title><link>https://freshrimpsushi.github.io/en/posts/1563/</link><pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1563/</guid><description>Definition 1 Given two graphs $G_{1}$ and $G_{2}$, if there exists a graph isomorphism between some subdivision $G_{1} ' $ of $G_{1}$ and some subdivision $G_{2} ' $ of $G_{2}$, then $G_{1}$ and $G_{2}$ are said to be homeomorphic. A graph $G$ with vertices $w$ added in succession that satisfy the following condition is called the subdivision $G'$ of $G$. $$ \begin{align*} u \sim_{G} v &amp;amp; \implies \begin{cases} u \nsim_{G'}</description></item><item><title>지수성장방정식/상수 계수를 갖는 1계 선형 동차 미분 방정식</title><link>https://freshrimpsushi.github.io/en/posts/1660/</link><pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1660/</guid><description>Definition In a first-order ordinary differential equation where the independent variable $t$ is not explicitly included in $f$, it is called an autonomous system or autonomous differential equation. $$ \dfrac{\mathrm{d}y}{\mathrm{d}t} = f(y) $$ Conversely, an equation in the following form is called a non-autonomous system. $$ \dfrac{\mathrm{d}y}{\mathrm{d}t} = f(y, t) $$ Explanation The term autonomous system has a more dynamical sense, whereas autonomous differential equation feels more focused on the</description></item><item><title>First-Order Linear Differential Equation System</title><link>https://freshrimpsushi.github.io/en/posts/1659/</link><pubDate>Thu, 11 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1659/</guid><description>Buildup1 When the mass is $m$, the damping factor is $\gamma$, and the spring constant is $k$, the equation of motion representing the vibration of an object hung on a spring is as follows. $$ m x^{\prime \prime} + \gamma x^{\prime} + kx = F $$ Letting $x_{1}=x$, $x_{2}=x_{1}^{\prime}$, the above equation of motion can be expressed as the following system. $$ \begin{align*} x_{1}^{\prime}(t) =&amp;amp;\ x_{2}(t) \\ x_{2}^{\prime} (t) =&amp;amp;\</description></item><item><title>Hilbert Space Adjoint Operators</title><link>https://freshrimpsushi.github.io/en/posts/1562/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1562/</guid><description>Buildup1 Let us assume that we are given bounded linear operators $T : K \to H$ in Hilbert spaces $\left( H, \left\langle \cdot , \cdot \right\rangle_{H} \right)$ and $\left( K, \left\langle \cdot , \cdot \right\rangle_{K} \right)$. Then, for any fixed element $\mathbf{w} \in H$, the following defined $\Phi : K \to \mathbb{C}$ becomes a linear functional $\Phi \in K^{ \ast }$. $$ \Phi \mathbf{v} := \left\langle T \mathbf{v} , \mathbf{w}</description></item><item><title>Laguerre Polynomials' Rodrigues' Formula</title><link>https://freshrimpsushi.github.io/en/posts/1658/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1658/</guid><description>Formulas The explicit formula for the Laguerre polynomials is as follows. $$ L_{n}(x) = \frac{1}{n!}e^{x}\frac{ d ^{n}}{ dx^{n} }(x^{n}e^{-x}) \tag{1} $$ Description The formula above is referred to as the Rodrigues&amp;rsquo; formula for Laguerre polynomials. Originally, the term Rodrigues&amp;rsquo; formula denoted the explicit form of the Legendre polynomials, but it later became a general term for formulas expressing the explicit form of special functions represented by polynomials. Writing down the</description></item><item><title>Hermite Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1655/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1655/</guid><description>Description Hermite Polynomials are defined in several ways as follows. As solutions to a differential equation Hermite polynomials are defined as the solutions to the following Hermite Differential Equation. $$ y^{\prime \prime} -2xy^{\prime} +2ny=0,\quad n=0,1,2,\cdots $$ Rodrigues&amp;rsquo; formula The following function $H_{n}$ is called the Hermite polynomial. $$ H_{n}(x)=(-1)^{n}e^{x^{2}}\frac{ d ^{n}}{ dx^{n} }e^{-x^{2}} $$ This is known as the Rodrigues&amp;rsquo; formula. Meanwhile, the above function is referred to as the</description></item><item><title>Hermite Polynomials' Generating Function</title><link>https://freshrimpsushi.github.io/en/posts/1654/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1654/</guid><description>Formulas The generating function of Hermite Polynomials is as follows. $$ \Phi (x,t)=\sum \limits _{n=0}^{\infty} \frac{H_{n}(x)}{n!}t^{n}= e^{2xt-t^{2}} $$ Explanation The generating function of Hermite Polynomials, simply put, is a polynomial that uses Hermite Polynomials as its coefficients. $H_{n}(x)$ is a Hermite Polynomial, and can be obtained by multiplying Hermite function $y_{n}=e^{\frac{x^{2}}{2}}\frac{ \d ^{n} }{ \d x^{n} }e^{-x^{2}}$ with $(-1)^{n}e^{\frac{x^{2}}{2}}$ or by solving the Hermite Differential Equation. $$ H_{n}(x)=(-1)^{n}e^{x^{2}}\frac{ \d ^{n}}{</description></item><item><title>Hermite Polynomials' Recursive Relations</title><link>https://freshrimpsushi.github.io/en/posts/1656/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1656/</guid><description>Theorem The Hermite polynomials satisfy the following recursive relation. $$ \begin{align} H_{n}^{\prime}(x) &amp;amp;= 2nH_{n-1}(x) \\ H_{n+1}(x) &amp;amp;= 2xH_{n}(x)-2nH_{n-1}(x) \\ &amp;amp;= 2xH_{n}(x)-H_{n}^{\prime}(x) \nonumber \end{align} $$ Proof $(1)$ Solving using the Generating Function Generating function of the Hermite polynomials $$ \Phi (x,t) = e^{2xt-t^{2}}=\sum \limits _{n=0}^{\infty} H_{n}(x)\frac{t^{n}}{n!} $$ Differentiating the generating function of the Hermite polynomials gives, $$ 2te^{2xt-t^{2}} = \sum \limits _{n=0}^{\infty}H_{n}^{\prime}(x)\frac{t^{n}}{n!} $$ Then, the left side, by the definition of</description></item><item><title>Orthogonality of Hermite Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1657/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1657/</guid><description>Theorem The Hermite polynomials $\left\{ H_{n} \right\}_{n=0}^{\infty}$ are orthogonal with respect to the weight function $w(x)=e^{-x^{2}}$ over the interval $(-\infty, \infty)$. $$ \braket{ H_{n} | H_{m} }_{e^{-x^{2}}} =\int_{-\infty}^{\infty}e^{-x^{2}}H_{n}(x)H_{m}(x)dx=\sqrt{\pi}2^{n}n!\delta_{nm} $$ Here, $\delta_{nm}$ is the Kronecker delta. Proof Case 1: $n=m$ Let&amp;rsquo;s denote the differential operator as $D = \dfrac{d}{dx}$. $$ \int_{-\infty}^{\infty} e^{-x^{2}}H_{n}(x)H_{n}(x)dx $$ Hermite polynomials $$ H_{n}(x) = (-1)^{n}e^{x^{2}}\frac{d^{n}}{dx^{n}}e^{-x^{2}} = (-1)^{n}e^{x^{2}}D^{n}e^{-x^{2}} $$ If we solve the front part $H_{n}(x)$ of the</description></item><item><title>Graph Coloring and Brooks' Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1561/</link><pubDate>Mon, 08 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1561/</guid><description>Definition A function $f : V(G) \to [k]$ for a loop-free graph $G$ is called a $k$-coloring of $G$. $$ u \sim v \implies f(u) \ne f(v) $$ If a graph $G$ has a $k$-coloring, it is also said to be $k$-colorable. If it is $k$-colorable but not $(k-1)$-colorable, then that $k$ is called the Chromatic Number of $G$, denoted as $\chi(G) = k$. A graph whose chromatic number is</description></item><item><title>Selberg Identity Proof</title><link>https://freshrimpsushi.github.io/en/posts/1560/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1560/</guid><description>Theorem 1 $$ \Lambda (n) \log n + \sum_{d \mid n } \Lambda (d) \Lambda \left( {{ n } \over { d }} \right) = \sum_{d \mid n} \mu (d) \log^{2} {{ n } \over { d }} $$ Proof Strategy: Not as hard as it looks. With just the differentiation of arithmetic functions, it can be derived very simply. Mangoldt function: $$ \sum_{d \mid n} \Lambda ( d )</description></item><item><title>Foehammer Symbol</title><link>https://freshrimpsushi.github.io/en/posts/1652/</link><pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1652/</guid><description>Definitions The Pochhammer symbol has two kinds of representations as follows. The following equation is defined as the falling factorial. $$ \begin{align*} x^{\underline{n}} := (x)_{n}&amp;amp;=x(x-1)(x-2)\cdots(x-n+1) \\ &amp;amp;=\frac{x!}{(x-n)!}=\frac{\Gamma (x+1) }{ \Gamma (x-n+1)} \\ &amp;amp;=\prod \limits_{k=0}^{n-1}(x-k) \end{align*} $$ The following equation is defined as the raising factorial. $$ \begin{align*} x^{\overline{n}} := x^{(n)}&amp;amp;=x(x+1)(x+2)\cdots(x+n-1) \\ &amp;amp;=\frac{(x+n-1)!}{(x-1)!}=\frac{\Gamma (x+n) }{ \Gamma (x)} \\ &amp;amp;=\prod \limits_{k=0}^{n-1}(x+k) \end{align*} $$ $x^{\overline{0}}$ and $x^{\underline{0}}$ are defined as $1$. $$ x^{\overline{0}}=x^{\underline{n}}=1</description></item><item><title>Havel-Hakimi Algorithm Proof</title><link>https://freshrimpsushi.github.io/en/posts/1559/</link><pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1559/</guid><description>Theorem Let&amp;rsquo;s assume we are given a non-increasing sequence $D = (d_{1} , \cdots , d_{n})$. If $D$ is graphic, we can find a realization $G$ of $D$ using the following method: Step 1. Create a null graph with $n$ vertices $v_{1} , \cdots , v_{n}$. Step 2. $k = 1, \cdots , n$ Step 2-1. Connect $v_{k}$ to $v_{k+1} , \cdots , v_{d_{k} + 1}$. Step 2-2. Decrease $d_{k+1}</description></item><item><title>Series Solution of Laguerre Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1651/</link><pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1651/</guid><description>Definition The following differential equation is referred to as the Laguerre differential equation. $$ xy^{\prime \prime}+(1-x)y^{\prime}+ny=0,\quad n=0,1,2,\cdots $$ Description The solution to the Laguerre differential equation is called Laguerre polynomials, and the first few Laguerre polynomials are as follows. $$ \begin{align*} L_{0}(x) &amp;amp;= 1 \\ L_{1}(x) &amp;amp;= -x+1 \\ L_{2}(x) &amp;amp;= \frac{1}{2}\left( x^{2}-4x+2 \right) \\ L_{3}(x) &amp;amp;= \frac{1}{6}\left( -x^{3}+9x^{2}-18x+6 \right) \\ \vdots &amp;amp; \end{align*} $$ Examining the equation to solve</description></item><item><title>Hermite Differential Equations and Series Solutions</title><link>https://freshrimpsushi.github.io/en/posts/1650/</link><pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1650/</guid><description>Definition The differential equation given below is referred to as the Hermite Differential Equation. $$ y^{\prime \prime}-2xy^{\prime}+2ny=0,\quad n=0,1,2,\cdots $$ The solution to the Hermite Differential Equation is called the Hermite Polynomial, and it is commonly denoted as $H_{n}(x)$. $$ \begin{align*} H_{0}(x) &amp;amp;= 1 \\ H_{1}(x) &amp;amp;= 2x \\ H_{2}(x) &amp;amp;= 4x^{2} - 2 \\ H_{3}(x) &amp;amp;= 8x^{3} - 12x \\ H_{4}(x) &amp;amp;= 16x^{4} - 48x^{2} + 12 \\ H_{5}(x) &amp;amp;=</description></item><item><title>Differentiation of Arithmetic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1558/</link><pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1558/</guid><description>Definition 1 The derivative or differential $f '$ of an arithmetic function $f$ is defined as follows. $$ f ' (n) := f(n) \log n \qquad , n \in \mathbb{N} $$ Basic Properties [1] Sum rule of derivatives: $(f+g)' = f '+g'$ [2] Product rule of derivatives: $\left( f \ast g \right)' = f '\ast g + f \ast g'$ [3] Quotient rule of derivatives: If $f(1) \ne 0$, then</description></item><item><title>Hermite Functions</title><link>https://freshrimpsushi.github.io/en/posts/1646/</link><pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1646/</guid><description>Definitions Hermite functions are defined as follows: $$ \begin{align} y_{n} &amp;amp;= \left( D-x \right)^{n} e^{-\frac{x^{2}}{2}} \\ &amp;amp;=e^{\frac{x^{2}}{2}} D^{n} e^{-x^{2}} \end{align} $$ where $D=\frac{d}{dx}$ is the differential operator. Description Hermite functions are solutions to the differential equation $$ y_{n}^{\prime \prime}-x^{2}y_{n}=-(2n+1)y_{n},\quad n=0,1,2,\cdots $$ and represent the solution to the one-dimensional harmonic oscillator Schrödinger equation in physics, i.e., the wave function of a</description></item><item><title>Operator Solution of the Differential Equation Satisfied by Hermite Functions</title><link>https://freshrimpsushi.github.io/en/posts/1648/</link><pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1648/</guid><description>Theorem Given the differential equation $$ y_{n}^{\prime \prime}-x^{2}y_{n}=-(2n+1)y_{n},\quad n=0,1,2,\cdots \tag{1} \label{eq1} $$ The solution to $(1)$ is as follows, known as the Hermite function. $$ \begin{align*} y_{n} &amp;amp;= \left( D-x \right)^{n} e^{-\frac{x^{2}}{2}} \\ &amp;amp;= e^{\frac{x^{2}}{2}} D^{n} x^{-x^{2}} \end{align*} $$ Here, $D$ is the differential operator $D=\frac{ d }{ dx }$. Explanation The first equation of $y_{n}$ can be directly obtained by solving the differential equation. That the second equation is</description></item><item><title>What is a Differential Operator in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/1638/</link><pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1638/</guid><description>Explanation One of the methods to solve differential equations is to use the differential operator. Let&amp;rsquo;s define the differential operator $D$ as follows. $$ D:= \frac{d}{dx} $$ When explicitly expressing the variable being differentiated, it is also denoted as $D_{x}$. For partial differentiation, it is represented as follows. $$ \partial _{x}:=\frac{ \partial }{ \partial x},\quad \partial_{y}=\frac{ \partial }{ \partial y} $$ Using the differential operator, the differential equation is expressed</description></item><item><title>Erdős–Gallai Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1557/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1557/</guid><description>Buildup A set that includes duplicates of the degrees of a graph $G$ is called a Graph Score, and the sequence $G$ sorted in descending order (non-increasing) of a graph score is called the Degree Sequence of $G$. If there exists a graph $G$ that satisfies the following, with a sequence $D = (d_{1} , \cdots , d_{n})$ of non-increasing natural numbers and $n$ vertices $v_{1} , \cdots , v_{n}$,</description></item><item><title>Wave Function and Hilbert Space in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/1637/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1637/</guid><description>Build-Up In Classical Mechanics, the main concern is finding the position function $\mathbf{r}(t)$ that satisfies Newton&amp;rsquo;s Second Law $\mathbf{F} = m \mathbf{a}$ given specific conditions. For instance, the position of an object launched with an initial velocity $\mathbf{v}_{0} = (v_{0}\cos\theta, v_{0}\sin\theta)$ in 2-dimensional space over time can be found by solving the following system of equations, and this is called Projectile Motion. $$ \begin{align*} m \dfrac{d^{2}\mathbf{r}}{dt^{2}} &amp;amp;= -mg\hat{\mathbf{y}} \\ \mathbf{v}(0)</description></item><item><title>Angular Momentum and Position/Momentum Commutation Relations</title><link>https://freshrimpsushi.github.io/en/posts/1636/</link><pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1636/</guid><description>Formula The commutator of angular momentum and position is as follows. $$ \begin{align*} [L_{z}, x] &amp;amp;= \i \hbar y \\ [L_{z}, y] &amp;amp;= -\i \hbar x \\ [L_{z}, z] &amp;amp;= 0 \end{align*} $$ The commutator of angular momentum and momentum is as follows. $$ \begin{align*} [L_{z}, p_{x}] &amp;amp;= \i \hbar p_{y} \\ [L_{z}, p_{y}] &amp;amp;= -\i \hbar p_{x} \\ [L_{z}, p_{z}] &amp;amp;= 0 \end{align*} $$ The squares of angular momentum</description></item><item><title>Arithmetic Functions' Bell Series</title><link>https://freshrimpsushi.github.io/en/posts/1556/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1556/</guid><description>Definition 1 Given the arithmetic function $f$ and prime number $p$, the following $f_{p}(x)$ is defined as the Bell series of $f$ modulo $p$. $$ f_{p}(x) := \sum_{n=0}^{\infty} f \left( p^{n} \right) x^{n} $$ Apostol. (1976). Introduction to Analytic Number Theory: p42.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Ladder Operators of Angular Momentum in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1634/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1634/</guid><description>공식 구 좌표계에서 각운동량의 사다리 연산자는 다음과 같다. $$ \begin{align*} L_{+} &amp;amp;= \hbar e^{\i\phi}\left( \dfrac{\partial }{\partial \theta} + \i\cot\theta \dfrac{\partial }{\partial \phi}\right) \\ L_{-} &amp;amp;= -\hbar e^{-\i\phi}\left( \dfrac{\partial }{\partial \theta} - \i\cot\theta \dfrac{\partial }{\partial \phi}\right) \\ L_{+}L_{-} &amp;amp;= -\hbar ^{2} \left( \frac{ \partial ^{2}}{ \partial \theta ^{2} } + \cot \theta \frac{</description></item><item><title>The eigenfunctions of the angular momentum operator are spherical harmonics.</title><link>https://freshrimpsushi.github.io/en/posts/1633/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1633/</guid><description>Summary The angular momentum operator $L^{2}$ and $L_{z}$ have simultaneous eigenfunctions determined by constants $l$, $m$ $\ket{\ell, m}$. $$ \begin{align*} L^{2}\ket{\ell, m} &amp;amp;= \hbar^{2}\ell(\ell+1)\ket{\ell, m} \\ L_{z}\ket{\ell, m} &amp;amp;= m\hbar\ket{\ell, m} \end{align*} $$ Here, the eigenfunction of the angular momentum operator $\ket{\ell, m}$ is actually spherical harmonics $Y_{l}^{m}$. $$ \ket{\ell, m} = Y_{l}^{m} $$ Proof In spherical coordinates, the angular momentum operator $L_{z}$ is as follows: $$ L_{z} = -\i\hbar\frac{\partial}{\partial</description></item><item><title>Label Tree and Cayley's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1555/</link><pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1555/</guid><description>Definition A labeled tree is a tree in which each vertex is assigned a distinct number. Description The concept of labeling is different from simply determining whether the elements in a set of vertices are the same or different. For example, the following two graphs are essentially considered the same labeled tree, despite having different labels written on them. $$ 1-2-3 \\ a-b-c $$ Of course, because they are graphs,</description></item><item><title>Analytic Number Theory and the Liouville Function</title><link>https://freshrimpsushi.github.io/en/posts/1553/</link><pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1553/</guid><description>Definition 1 Let&amp;rsquo;s consider a prime number $p_{1} , \cdots , p_{k}$ and a natural number $n$ represented as $n = p_{1}^{a_{1}} \cdots p_{k}^{a_{k}}$. The arithmetic function $\lambda$ defined as follows is called the Liouville function. $$ \lambda (n) = (-1)^{a_{1} + \cdots a_{k}} $$ Basic Properties [1] Liouville series: $n$ is a perfect square, then $1$, otherwise $0$. In other words, $$ \sum_{d \mid n} \lambda (d) = \begin{cases}</description></item><item><title>The Airy Function</title><link>https://freshrimpsushi.github.io/en/posts/1629/</link><pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1629/</guid><description>Definition The function below is referred to as the Airy function. $$ \begin{align*} \operatorname{Ai}(x) &amp;amp;= \frac{1}{\pi}\sqrt{\frac{x}{3}}K_{1/3}\left( \frac{2}{3}x^{2/3} \right) \\ \operatorname{Bi}(x) &amp;amp;= \sqrt{\frac{x}{3}}\left[ I_{-1/3}\left( \frac{2}{3}x^{3/2} \right) + I_{1/3} \left( \frac{2}{3}x^{2/3} \right) \right] \end{align*} $$ Here, $I_{\nu}$ and $K_{\nu}$ are modified Bessel functions. Description The Airy function represents the solution to the Airy differential equation using Bessel functions. Integral Form The Airy function has the following integral form: $$ \begin{align*} \operatorname{Ai}(x) &amp;amp;=</description></item><item><title>Tree Graph</title><link>https://freshrimpsushi.github.io/en/posts/1552/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1552/</guid><description>Definition 1 A tree is a connected graph that does not contain any cycles. Description The concept of trees is commonly seen in areas such as data structures in computer science. For those in STEM fields who have even a cursory involvement with computers, they may have heard of heap sorting. The heap mentioned here is indeed a type of tree. The union of trees is intuitively called a Forest.</description></item><item><title>Series Solutions to the Airy Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1625/</link><pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1625/</guid><description>Definition The following differential equation is called the Airy differential equation. $$ y^{\prime \prime}-xy=0,\quad -\infty&amp;lt;x&amp;lt;\infty $$ Explanation The name originates from the British astronomer George Biddell Airy. It is also called the Stokes equation. Solution Since the coefficient of $y^{\prime \prime}$ is $1$, all points are ordinary points. Among them, let&amp;rsquo;s find the power series solution around $x=0$. Assume that the solution of the Airy equation is as follows and</description></item><item><title>Analytic Number Theory and the Mangoldt Function</title><link>https://freshrimpsushi.github.io/en/posts/1551/</link><pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1551/</guid><description>Definition 1 The arithmetic function defined as follows $\Lambda$ is called the Mangoldt function. $$ \Lambda (n) := \begin{cases} \log p &amp;amp; n = p^{m} , p \text{ is prime}, m \in \mathbb{N} \\ 0 &amp;amp; \text{otherwise} \end{cases} $$ Basic Properties [1] Mangoldt series: equals the logarithmic function $\log$. In other words, $$ \sum_{d \mid n} \Lambda ( d ) = \log n $$ Explanation $$ \begin{matrix} n &amp;amp; 1</description></item><item><title>Modified Bessel Equation and Modified Bessel Function</title><link>https://freshrimpsushi.github.io/en/posts/1624/</link><pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1624/</guid><description>Buildup The differential equation below is referred to as the modified Bessel equation. $$ x^2 y^{\prime \prime} + xy^{\prime}-(x^2-\nu^2)y=0 $$ It is a form of the Bessel equation where the sign of the term $y$ has been changed to $+ \rightarrow -$. The solution to this differential equation is given by the formula for differential equations that have Bessel equation solutions, as follows. $$ y=Z_{\nu}(ix)=AJ_{\nu}(ix)+BN_{\nu}(ix) $$ The two commonly used</description></item><item><title>Bessel Functions</title><link>https://freshrimpsushi.github.io/en/posts/1622/</link><pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1622/</guid><description>Definition Bessel Equation The differential equation below is called the $\nu$ order Bessel equation. $$ \begin{align*} x^2 y^{\prime \prime} +xy^{\prime} +(x^2-\nu^2)y&amp;amp;=0 \\ x(xy^{\prime})^{\prime}+(x^2- \nu ^2) y&amp;amp;=0 \\ y^{\prime \prime}+\frac{1}{x} y^{\prime} + \left( 1-\frac{\nu^{2}}{x^{2}} \right)y&amp;amp;=0 \end{align*} $$ Description Functions Related First Kind Bessel Function First Kind Bessel Function The first solution of the Bessel equation is written as $J_{\nu}(x)$ and is called the first kind Bessel function. $$ J_{\nu}(x)=\sum \limits_{n=0}^{\infty} \frac{(-1)^{n}</description></item><item><title>Hankel Functions, Bessel Functions of the Third Kind</title><link>https://freshrimpsushi.github.io/en/posts/1623/</link><pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1623/</guid><description>Definition A Hankel function, also known as a Bessel function of the third kind, is defined as the following two linear combinations of the Bessel function of the first kind $J_{\nu}$ and the Bessel function of the second kind $N_{\nu}$. $$ H_{\nu}^{(1)}(x) = J_{\nu}(x)+iN_{\nu}(x) $$ $$ H_{\nu}^{(2)}(x) = J_{\nu}(x)-iN_{\nu}(x) $$ Explanation It was introduced by the German mathematician Hermann Hankel in 1869. Specifically, $H_{\nu}^{(1)}$ is called the Hankel function of</description></item><item><title>Proof of Dirac's Theorem in Graph Theory</title><link>https://freshrimpsushi.github.io/en/posts/1550/</link><pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1550/</guid><description>Theorem 1 Let&amp;rsquo;s suppose that $G$ is a simple graph with $n ( \ge 3)$ vertices. [1] Dirac&amp;rsquo;s Theorem: If every vertex $v$ of $G$ satisfies $\deg (v) \ge n / 2$, then $G$ is a Hamiltonian graph. [2] Ore&amp;rsquo;s Theorem: If for every pair of non-adjacent vertices $(v ,w)$ of $G$, $\deg (v) + \deg(w) \ge n$ is satisfied, then $G$ is a Hamiltonian graph. Explanation Dirac&amp;rsquo;s Theorem identifies</description></item><item><title>Orthogonality of Bessel Functions</title><link>https://freshrimpsushi.github.io/en/posts/1621/</link><pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1621/</guid><description>Theorem Let&amp;rsquo;s assume that the roots of the first kind Bessel function $\alpha, \beta$ are $J_{\nu}(x)$. Then, in the interval $[0,1]$, $\sqrt{x}J_{\nu}(x)$ forms an orthogonal set. $$ \int_{0}^{1} x J_{\nu}(\alpha x) J_{\nu}(\beta x)dx = \begin{cases} 0 &amp;amp;\alpha\ne \beta \\ \frac{1}{2}J^{2}_{\nu+1}(\alpha)=\frac{1}{2}J_{\nu-1}^{2}(\alpha)=\frac{1}{2}J_{\nu^{\prime}}^{2}(\alpha) &amp;amp;\alpha=\beta \end{cases} $$ Description The above content can also be expressed as &amp;lsquo;Bessel function $J_{\nu}(x)$ is orthogonal in the interval $[0,1]$ with respect to the weight function $x$&amp;rsquo;. Proof $\alpha</description></item><item><title>Bessel Functions as Solutions to Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1620/</link><pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1620/</guid><description>Theorem1 Theorem 1 Given a differential equation slightly different from the Bessel equation as follows: $$ \begin{equation} \begin{aligned} &amp;amp;&amp;amp; y^{\prime \prime}+\frac{1-2a}{x}y^{\prime}+\left[ (bcx^{c-1})^{2}+\frac{a^{2}-\nu^{2}c^{2}}{x^{2}} \right]y =&amp;amp;\ 0 \\ \text{or} &amp;amp;&amp;amp; x^{2}y^{\prime \prime}+(1-2a)xy^{\prime}+\left[ b^{2}c^{2}x^{2c}+(a^{2}-\nu^{2}c^{2}) \right]y =&amp;amp;\ 0 \end{aligned} \label{1} \end{equation} $$ And let $Z_{\nu}(x)$ be any linear combination of $J_{\nu}(x)$ and $N_{\nu}(x)$. Then, the solution to the given differential equation is as follows: $$ y=x^{a}Z_{\nu}(bx^{c})=x^{a}[AJ_{\nu}(bx^{c})+BN_{\nu}(bx^{c})] $$ $\nu$, $a$, $b$, $c$, $A$, $B$ are</description></item><item><title>Mobius Inversion Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/1549/</link><pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1549/</guid><description>Formulas 1 $f$ and $g$ are arithmetic functions and $\mu$ is the Möbius function. $$ f(n) = \sum_{d \mid n} g(d) \iff g(n) = \sum_{d \mid n} f(d) \mu \left( {{ n } \over { d }} \right) $$ Explanation The Möbius function might seem unnatural at first glance, but it actually appears in the core formulas that permeate all</description></item><item><title>Bessel Function's Recursive Relations</title><link>https://freshrimpsushi.github.io/en/posts/1619/</link><pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1619/</guid><description>Theorem $$ J_{\nu}(x)=\sum \limits_{n=0}^{\infty} \frac{(-1)^{n} }{\Gamma (n+1) \Gamma (n+\nu+1)} \left(\frac{x}{2} \right)^{2n+\nu} \tag{1} $$ The function above is called the first kind Bessel function of order $\nu$. The first kind Bessel function $J_{\nu}(x)$ satisfies the following equation. $$ \begin{align*} \frac{d}{dx}[x^{\nu} J_{\nu}(x)] &amp;amp;= x^{\nu}J_{\nu-1}(x) \tag{a} \\ \frac{d}{dx}[x^{-\nu}J_{\nu}(x)] &amp;amp;= -x^{-\nu}J_{\nu+1}(x) \tag{b} \\ J_{\nu-1}(x)+J_{\nu+1}(x) &amp;amp;= \frac{2\nu}{x}J_{\nu}(x) \tag{c} \\ J_{\nu-1}(x)-J_{\nu+1}(x) &amp;amp;= 2J^{\prime}_{\nu}(x) \tag{d} \\ J_{\nu}^{\prime}(x) = -\frac{\nu}{x}J_{\nu}(x)+J_{\nu-1}(x) &amp;amp;= \frac{\nu}{x}J_{\nu}(x)-J_{\nu+1}(x) \tag{e} \end{align*} $$ Proof $(a)$ By</description></item><item><title>Hamiltonian Graph</title><link>https://freshrimpsushi.github.io/en/posts/1548/</link><pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1548/</guid><description>Definition 1 Let $G$ be a connected graph. If there exists a closed path that includes all vertices of $G$, then $G$ is called a Hamiltonian graph, and that cycle is called a Hamiltonian cycle. If there exists a path that includes all vertices but is not closed, then $G$ is called a semi-Hamiltonian graph. Explanation Just as Eulerian graphs are interested in trails that pass through all edges, Hamiltonian</description></item><item><title>The Second Series Solution of the Bessel Equation: Bessel Functions of the Second Kind, Neumann Functions, Weber Functions</title><link>https://freshrimpsushi.github.io/en/posts/1618/</link><pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1618/</guid><description>Definition[^1] A second solution of the Bessel equation is called the Neumann function, denoted by $N_{\nu}(x)$ or $Y_{\nu}(x)$. For non-integer $\nu$, $$ N_{\nu}(x)=Y_{\nu}(x)=\frac{\cos (\nu \pi)J_{\nu}(x)-J_{-\nu}(x)}{\sin (\nu\pi)} $$ For integer $\nu$, it is defined by the limit. For $n\in \mathbb{Z}$, $\nu \in \mathbb{R}\setminus\mathbb{Z}$, $$ N_{n}(x)=\lim \limits_{\nu \rightarrow n}N_{\nu}(x) $$ Here, $J_{\pm \nu}(x)$ is the first kind Bessel function. Thus, the general solution of the Bessel equation is as follows. $$ y(x)=AJ_{\nu}(x)+BN_{\nu}(x)</description></item><item><title>Schrödinger Equation in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1617/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1617/</guid><description>Equation In the spherical coordinate system, the Schrödinger equation is as follows. $$ -\frac{\hbar^{2}}{2M}\left[\frac{1}{r^2}\frac{\partial}{\partial r} \left( r^2\frac{\partial \psi}{\partial r} \right) + \frac{1}{r^2\sin\theta}\frac{\partial}{\partial\theta}\left( \sin\theta \frac{\partial \psi}{\partial \theta} \right) + \frac{1}{r^2\sin^2\theta}\frac{\partial^2 \psi}{\partial^2 \phi} \right]+V\psi=E\psi \tag{1} $$ Explanation The time-independent Schrödinger equation in three dimensions is as follows. $$ -\frac{\hbar^{2}}{2M}\nabla^{2}\psi+V\psi=E\psi $$ Here, $M$</description></item><item><title>Associated Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1615/</link><pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1615/</guid><description>Definition Associated Legendre polynomials are defined in the following ways. As a Solution to a Differential Equation The solutions to the associated Legendre differential equation below are referred to as associated Legendre polynomials. $$ \begin{align*} &amp;amp;&amp;amp; (1-x^{2}) \frac{d^{2}y}{dx^{2}} - 2x \frac{dy}{dx} + \left[l(l+1) - \frac{m^{2}}{1-x^{2}}\right] y &amp;amp;= 0 \\ \text{or} &amp;amp;&amp;amp; \frac{d}{dx} \left[(1-x^{2})y^{\prime}\right] + \left[l(l+1) - \frac{m^{2}}{1-x^{2}}\right] y &amp;amp;= 0 \end{align*} $$ Rodrigues&amp;rsquo; Formula The polynomial function $P_{l}^{m}$ below is</description></item><item><title>In Analytic Number Theory</title><link>https://freshrimpsushi.github.io/en/posts/1547/</link><pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1547/</guid><description>Definition 1 The arithmetic function defined as follows $u$ is called the unit function. $$ u(n) := 1 $$ Basic Properties [1] Unit series: Equals the number of divisors $\sigma_{0}$. In other words, $$ \sum_{d \mid n} u(d) = \sigma_{0} (n) $$ [2] Completely multiplicative: For all $m,n \in \mathbb{N}$, $u(mn) = u(m) u(n)$ Explanation $$ \begin{matrix} n &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5 &amp;amp; 6</description></item><item><title>Normalization of Spherical Harmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1614/</link><pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1614/</guid><description>Theorem The standardized spherical harmonics are as follows. $$ Y_{l}^{m}(\theta,\phi)=\sqrt{\frac{2l+1}{4\pi}\frac{(l-m)!}{(l+m)!}}P_{l}^{m}(\cos\theta)e^{im\phi} $$ $$ \nabla ^2 f = \frac{1}{r^2}\frac{\partial}{\partial r} \left( r^2\frac{\partial f}{\partial r} \right) + \frac{1}{r^2\sin\theta}\frac{\partial}{\partial\theta}\left( \sin\theta \frac{\partial f}{\partial \theta} \right) + \frac{1}{r^2\sin^2\theta}\frac{\partial^2 f}{\partial^2 \phi}=0 $$ $$ f(r,\theta,\phi)=R(r)\Theta (\theta)\Phi (\phi) $$ Description In the Laplace equation on spherical coordinates, solutions for the polar angle $\theta$ and the azimuthal angle $\phi$ are referred to as spherical harmonics. $$ \Theta (\theta)\Phi (\phi)=Y_{l}^{m}(\theta,\phi)=e^{im\phi}P_{l}^{m}(\cos \theta)</description></item><item><title>Fleury's Algorithm Proof</title><link>https://freshrimpsushi.github.io/en/posts/1546/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1546/</guid><description>Definition 1 Let $G$ be an Euler graph. Then, an Euler trail can be made in the following way. Start from an arbitrary vertex $u$ and follow these two rules to make the trail: (i): Already passed edges are removed. If removing an edge results in an isolated vertex, that vertex is also removed. (ii): At each step, bridges are only crossed if there are no other alternatives. If removing</description></item><item><title>Orthogonality of Associated Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1613/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1613/</guid><description>Theorem The associated Legendre polynomials over the interval $[-1,1]$ for a fixed $m$ form an orthogonal set. $$ \int_{-1}^{1} P_{l}^{m}(x)P_{k}^{m}(x)dx =\frac{ 2}{ 2l+1 }\frac{(l+m)!}{(l-m)!}\delta_{lk} $$ In the case of $x=\cos \theta$, $$ \int_{0}^{\pi} P_{l}^{m}(\cos \theta)P_ {k}^{m}(\cos\theta)\sin \theta d\theta =\frac{ 2}{ 2l+1 }\frac{(l+m)!}{(l-m)!}\delta_{lk} $$ Associated Legendre Polynomials $$ P_{l}^{m}(x) = (1-x ^{2})^{\frac{m}{2}} \dfrac{1}{2^l l!} \dfrac{d^{l+m}}{dx^{l+m}}(x^2-1)^l $$ Proof For convenience, let&amp;rsquo;s briefly denote it as $P_{lm} = P_{l}^{m}(x)$. The associated Legendre differential</description></item><item><title>Associated Legendre Polynomials for Negative Index m</title><link>https://freshrimpsushi.github.io/en/posts/1612/</link><pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1612/</guid><description>Formulas The Associated Legendre Polynomials obey the proportionality relation below, depending on the sign of $m$. $$ P_{l}^{-m}(x)=(-1)^{m}\frac{(l-m)!}{(l+m)!}P_{l}^{m}(x) $$ $$ (1-x^{2})\frac{ d^{2}y }{ dx^{2} }-2x \frac{dy}{dx}+\left( \frac{-m^{2}}{1-x^{2}}+l(l+1) \right)y=0 $$ Explanation Looking at the associated Legendre differential equation, the section regarding $m$ is shown as $m^2$, so whether $m$ is positive or negative doesn&amp;rsquo;t affect the solution. Thus, the associated Legendre polynomials can be derived as follows. $$ \begin{align*} P_{l}^{m}(x)&amp;amp;= (1-x</description></item><item><title>Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1611/</link><pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1611/</guid><description>Definition Legendre polynomials are defined in various ways. As Solutions to a Differential Equation The solutions to the following Legendre differential equation are called Legendre polynomials. $$ (1-x^{2}) \dfrac{d^{2} y}{dx^{2}} -2x\dfrac{dy}{dx} + l(l+1) y = 0 $$ Rodrigues&amp;rsquo; Formula The following function $P_{l}$ is called a Legendre polynomial. $$ P_{l}(x) = \dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} $$ This is known as Rodrigues&amp;rsquo; formula. Explanation By definition, $P_{n}$ is technically a polynomial &amp;lsquo;function&amp;rsquo;,</description></item><item><title>Generating Functions of Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1610/</link><pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1610/</guid><description>Theorem The generating function of Legendre polynomials is as follows. $$ \Phi (x,t) = \frac{1}{\sqrt{1-2xt+t^{2}}} = \sum \limits_{l=0}^{\infty}P_{l}(x)t^{l},\quad |t|&amp;lt;1 $$ Description The generating function of Legendre polynomials is, put simply, a polynomial that has the Legendre polynomial $P_{l}(x)$ as its coefficients. Lemma The function $\Phi (x,t) = \dfrac{1}{\sqrt{1-2xt+t^{2}}}$ is a solution to the differential equation below. $$ \begin{equation} (1-x^{2})\frac{ \partial ^{2} \Phi}{ \partial x^{2} }-2x\frac{ \partial \Phi}{ \partial x }+t\frac{</description></item><item><title>Recursive Relations of Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1609/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1609/</guid><description>Theorem $$ P_{l}(x)=\dfrac{1}{2^l l!} \dfrac{d^l}{dx^l}(x^2-1)^l \tag{1} $$ Such a Legendre Polynomial $P_{l}$ satisfies the following recursive relation: $$ P^{\prime}_{l+1}(x)-P^{\prime}_{l-1}(x)=(2l+1)P_{l}(x) \tag{a} $$ $$ lP_{l}(x)=(2l-1)xP_{l-1}(x)-(l-1)P_{l-2}(x) \tag{b} $$ $$ xP^{\prime}_{l}(x)-P^{\prime}_{l-1}(x)=lP_{l}(x)\tag{c} $$ Proof $(a)$ First, if we calculate the derivative of $P_{l}(x)$, $$ \begin{align*} \frac{d}{dx}P_{l}(x) &amp;amp;= \frac{1}{2^l l!}\frac{d}{dx} \dfrac{d^l}{dx^l}(x^2-1)^l \\ &amp;amp;= \frac{1}{ 2^{l}l! }\frac{ d ^{l} }{ dx^{l} }\frac{d}{dx}(x^{2}-1)^{l} \\ &amp;amp;= \frac{2l}{ 2^{l}l! }\frac{ d ^{l} }{ dx^{l} }\left[ x(x^{2}-1)^{l-1} \right] \\ &amp;amp;= \frac{1}{</description></item><item><title>The Solution to the Bridges of Königsberg Problem</title><link>https://freshrimpsushi.github.io/en/posts/1542/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1542/</guid><description>Problem 1 The Seven Bridges of Königsberg is a problem about whether it is possible to traverse each of the city’s seven bridges exactly once and return to the starting point. At first glance, without knowing the solution, it seems like a daunting problem that requires checking every possible route. It doesn</description></item><item><title>Mean and Variance of the Beta Distribution</title><link>https://freshrimpsushi.github.io/en/posts/97/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/97/</guid><description>Formula $X \sim \text{Beta}(\alpha,\beta)$ Surface $$ E(X)={\alpha \over {\alpha + \beta} } \\ \Var (X)={ { \alpha \beta } \over {(\alpha + \beta + 1) { ( \alpha + \beta ) }^2 } } $$ Derivation Strategy: Direct deduction using the definition of the beta distribution and the basic properties of the gamma function. Definition of the Beta Distribution: $\alpha , \beta &amp;gt; 0$ A continuous probability distribution $\text{Beta}(\alpha,\beta)$ with</description></item><item><title>Euler Graph</title><link>https://freshrimpsushi.github.io/en/posts/1541/</link><pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1541/</guid><description>Definition Let $G$ be a connected graph. If there exists a closed trail that includes all edges of $G$, then $G$ is called an Eulerian graph, and the trail is called an Eulerian trail. If there exists a trail that includes all edges but is not closed, then $G$ is called a semi-Eulerian graph. Explanation This concept is also familiar to us as the problem of drawing with one stroke.</description></item><item><title>Beta Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1540/</link><pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1540/</guid><description>Definition 1 For $\alpha , \beta &amp;gt; 0$, the continuous probability distribution $\text{Beta}(\alpha,\beta)$, called the beta Distribution, has the following probability density function: $$ f(x) = {{ 1 } \over { B(\alpha,\beta) }} x^{\alpha - 1} (1-x)^{\beta - 1} \qquad , x \in [0,1] $$ $B$ represents the beta function. Basic Properties Moment Generating Function [1]: $$m(t) = 1 + \sum_{k=1}^{\infty} \left( \prod_{r=0}^{k-1} {{ \alpha + r } \over {</description></item><item><title>Associated Legendre Differential Equations and Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1605/</link><pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1605/</guid><description>Definition1 The differential equation given below is called the associated Legendre differential equation. $$ \begin{equation} \begin{aligned} &amp;amp;&amp;amp;(1-x^{2})\frac{ d^{2}y }{ dx^{2} }-2x \frac{dy}{dx}+\left[ +l(l+1)-\frac{m^{2}}{1-x^{2}} \right]y =&amp;amp;\ 0 \\ \mathrm{or} &amp;amp;&amp;amp; \frac{ d }{ dx } \left[ (1-x^{2})y^{\prime} \right] +\left[ l(l+1)-\frac{m^{2}}{1-x^{2}} \right]y =&amp;amp;\ 0 \end{aligned} \label{1} \end{equation} $$ The solution to the associated Legendre differential equation is denoted as $P_{l}^{m}(x)$, and this is called the associated Legendre polynomial or the generalized Legendre</description></item><item><title>Proof of Kőnig's theorem</title><link>https://freshrimpsushi.github.io/en/posts/1534/</link><pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1534/</guid><description>Theorem 1 Let $G$ be a locally finite connected graph. Then, for every $v \in V(G)$, there exists a one-way infinite path with $v$ as its starting point. Proof Since $G$ is a connected graph, for all $z \in V(G)$ other than $v$, there are infinitely many paths from $v$ to $z$. And since $G$ is locally finite, among these infinitely many paths, infinitely many must start with the same</description></item><item><title>Derivation of the Schrödinger Equation</title><link>https://freshrimpsushi.github.io/en/posts/1598/</link><pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1598/</guid><description>Overview Time-independent Schrodinger equation $$ H\psi=\left(-\frac{\hbar^{2}}{2m}\frac{ d ^{2} }{ d x^{2} }+V\right)\psi=E\psi \\ H\psi=\left(-\frac{\hbar^{2}}{2m}\nabla^{2}+V\right)\psi=E\psi $$ Time-dependent Schrodinger equation $$ i\hbar\frac{ \partial \psi}{ \partial t}=\left(-\frac{\hbar^{2}}{2m}\frac{ \partial ^{2} }{\partial x^{2} }+V\right)\psi \\ i\hbar\frac{ \partial \psi}{ \partial t}=\left(-\frac{\hbar^{2}}{2m}\nabla^{2}+V\right)\psi $$ The Schrodinger equation is a partial differential equation related to the energy, position, and time of a complex wave function. In simpler terms, it&amp;rsquo;s like the following in classical mechanics: $$ F=ma $$ Using</description></item><item><title>Euler's Totient Function in Analytic Number Theory</title><link>https://freshrimpsushi.github.io/en/posts/1533/</link><pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1533/</guid><description>Definition 1 The arithmetic function defined as follows $\varphi$ is called the totient function. $$ \varphi (n) := \sum_{\gcd ( k , n ) = 1} 1 $$ Basic Properties [1] Totient series: the norm $N$. That is, $$ \sum_{d \mid n } \varphi (d) = N(n) $$ [2] Multiplicativity: For all $m, n \in \mathbb{N}$ that satisfy $\gcd (m,n) = 1$, $\varphi (mn) = \varphi (m) \varphi (n)$ Explanation</description></item><item><title>Solutions to Euler's Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1599/</link><pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1599/</guid><description>Definition The differential equation of the following form is called the Euler differential equation or Euler-Cauchy equation. $$ \begin{equation} a_{2}x^{2}\frac{ d ^{2 }y}{ dx^{2} }+a_{1}x\frac{ d y}{ d x }+a_{0}y=0 \end{equation} $$ Explanation For a non-homogeneous equation where the right side is not $0$, it can be solved by substituting it with $x=e^{z}$. Solution For convenience of calculation, both sides of $(1)$ are divided by $a_{2}$, and let&amp;rsquo;s call the</description></item><item><title>General Solution to the Laplace Equation in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1588/</link><pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1588/</guid><description>Theorem In spherical coordinates, the Laplace equation is as follows: $$ \nabla ^2 f = \frac{1}{r^2}\frac{\partial}{\partial r} \left( r^2\frac{\partial f}{\partial r} \right) + \frac{1}{r^2\sin\theta}\frac{\partial}{\partial\theta}\left( \sin\theta \frac{\partial f}{\partial \theta} \right) + \frac{1}{r^2\sin^2\theta}\frac{\partial^2 f}{\partial^2 \phi}=0 $$ Explanation Assuming that $f$ can be separated into variables as $f(r,\theta,\phi)=R(r)\Theta (\theta)\Phi (\phi)$, the general solution for the radial component can be derived by solving the Euler differential equation as follows: $$ R(r)=\sum \limits_{l=0}^{\infty}R_{l}(r)=\sum \limits_{l=0}^{\infty}\left( A_{l}r^{l}+\frac{</description></item><item><title>Orientation of Graphs</title><link>https://freshrimpsushi.github.io/en/posts/1532/</link><pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1532/</guid><description>Build-up Let&amp;rsquo;s assume a directed graph $D$ is given. A finite sequence of arcs is called a Directed Walk and is represented as follows: $$ v_{0} v_{1} , v_{1} v_{2} , \cdots , v_{m-1} v_{m} \\ v_{0} \rightarrow v_{1} \rightarrow v_{2} \rightarrow \cdots \rightarrow v_{m-1} \rightarrow v_{m} $$ In this case, $v_{0}$ is called the Initial Vertex, $v_{m}$ is called the Final Vertex, and $m$ is referred to as the</description></item><item><title>General Solution to the Radial Component Equation in the Laplace's Equation in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1587/</link><pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1587/</guid><description>Theorem The general solution of the radial part equation in the spherical coordinate system Laplace&amp;rsquo;s equation is given below. $$ R(r)=\sum \limits_{l=0}^{\infty}R_{l}(r)=\sum \limits_{l=0}^{\infty}\left( A_{l}r^{l}+\frac{ B_{l}}{r^{l+1}} \right) $$ Here, $l$ is a non-negative integer, and $A_{l}$, $B_{l}$ are constants. Description The process of finding this is relatively simple compared to the solution for polar and azimuthal angles. Proof In the spherical coordinate system Laplace&amp;rsquo;s equation, the solutions for the polar angle</description></item><item><title>Spherical Harmonics: General Solutions for the Polar and Azimuthal Angles in the Spherical Coordinate Laplace's Equation</title><link>https://freshrimpsushi.github.io/en/posts/1580/</link><pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1580/</guid><description>Definition The general solution for the polar and azimuthal angles in the spherical coordinate system for the Laplace equation is as follows, and this is called Spherical harmonics. $$ Y_{l}^{m}(\theta,\phi)=e^{im\phi}P_{l}^{m}(\cos \theta) $$ Here, $l$ is $l=0,1,2\cdots$ and $m$ is an integer that satisfies $ -l \le m \le l$. Also, $P_{l}^{m}(\cos\theta)$ is as follows. $$ \begin{align*} P_{l}^{m}(\cos \theta)&amp;amp;= (1-\cos ^{2}\theta)^{\frac{|m|}{2}} \frac{ d^{|m|} }{ dx^{|m|} } P_{l}(x) \\ &amp;amp; =(1-\cos ^{2}\theta)^{\frac{|m|}{2}}</description></item><item><title>The Moebius Function in Analytic Number Theory</title><link>https://freshrimpsushi.github.io/en/posts/1531/</link><pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1531/</guid><description>Definition 1 For a prime number $p_{1} , \cdots , p_{k}$, let&amp;rsquo;s express a natural number $n$ as follows. The arithmetic function $\mu$ defined as such is called the Möbius function. $$ \mu (n) := \begin{cases} 1 &amp;amp;, n=1 \\ (-1)^{k} &amp;amp;, a_{1} = \cdots = a_{k} = 1 \\ 0 &amp;amp; , \text{otherwise} \end{cases} $$ Basic Properties [1] Möbius</description></item><item><title>Principles of CT (Computed Tomography)</title><link>https://freshrimpsushi.github.io/en/posts/1579/</link><pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1579/</guid><description>Principle CT stands for Computerized Tomography, a technology that captures cross-sectional images of the body, similarly well-known as MRI among the public. Our bodies are composed of various materials such as bones, muscles, and water. CT uses the difference in how these materials absorb X-rays1 to obtain cross-sectional images. Let&amp;rsquo;s look at the diagram below. In (a), let&amp;rsquo;s say the white square represents a material (for example, bones) that absorbs</description></item><item><title>Distance, Neighborhood, Diameter, Perimeter in a Graph</title><link>https://freshrimpsushi.github.io/en/posts/1530/</link><pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1530/</guid><description>Definition In the graph $G$, the set of paths whose origin is $v \in V(G)$ and destination is $w \in V(G)$ is represented as $P(v,w)$, and let&amp;rsquo;s denote the set of cycles that include $v \in V(G)$ as $C(v)$. Also, let&amp;rsquo;s present the length of a walk $x$ as $l(x)$. The distance $d$ between two vertices $v,w \in V(G)$ is defined as the smallest value among the lengths of paths</description></item><item><title>Magnetic Field Created by a Moving Point Charge</title><link>https://freshrimpsushi.github.io/en/posts/1577/</link><pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1577/</guid><description>Overview 1 The electromagnetic field created by a moving point charge is as follows. $$ \begin{align*} \mathbf{E}(\mathbf{r}, t) &amp;amp;= \frac{q}{4\pi\epsilon_{0}} \frac{\cR} {( \bcR\cdot \mathbf{u} )^3 } \left[(c^2-v^2)\mathbf{u} +\bcR\times (\mathbf{u} \times \mathbf{a} ) \right] \\ \mathbf{B} (\mathbf{ r}, t) &amp;amp;=\frac{1}{c} \crH\times \mathbf{ E } (\mathbf{ r}, t) \end{align*} $$ Description The formula for the magnetic field is specifically as follows. $$ \mathbf{B}=-\frac{1}{c}\frac{1}{4\pi \epsilon_{0}} \frac{q}{ (\mathbf{u}\cdot \bcR)^{3}} \bcR \times \left[ (c^{2}-v^{2})\mathbf{v}+(\bcR \cdot</description></item><item><title>Physics에서의 Del 연산자</title><link>https://freshrimpsushi.github.io/en/posts/1575/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1575/</guid><description>Explanation In physics, an operator refers to a function that maps a function to another function. Among these, the del operator refers to a function that, given a function, results in a function that has the derivative of the given function as its function value. If the term operator is unfamiliar, you can simply understand it as a rule that computes a target. For example, if you insert $f$ into</description></item><item><title>Analytic Number Theory: Norms</title><link>https://freshrimpsushi.github.io/en/posts/1529/</link><pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1529/</guid><description>Definition 1 The arithmetic function defined as below $N$ is called a norm. $$ N(n) := n $$ Basic Properties [1] Norm Series: Sigma function $\sigma = \sigma_{1}$. In other words, $$ \sum_{d \mid n } N(d) = \sigma_{1}(n) $$ [2] Complete Multiplicativity: For all $m,n \in \mathbb{N}$, $N(mn) = N(m) N (n)$ Explanation $$ \begin{matrix} n &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5 &amp;amp; 6 &amp;amp;</description></item><item><title>In Quantum Mechanics, what is a commutator?</title><link>https://freshrimpsushi.github.io/en/posts/1574/</link><pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1574/</guid><description>Definition For the given two operators $A, B$, $AB - BA$ is defined as the commutator of $A, B$ and is denoted as follows: $$ [A,B]=AB-BA $$ Explanation Upon first encountering the definition of a commutator, one might wonder if it is not $AB - BA = 0$. However, since operators are expressed as matrices and the product of two matrices does not satisfy the commutative law, different results can</description></item><item><title>de Broglie Equation and Matter Waves</title><link>https://freshrimpsushi.github.io/en/posts/1573/</link><pubDate>Sun, 19 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1573/</guid><description>Description The question of whether light is a wave or a particle has been a major interest in the history of physics. In the early 20th century, several experiments revealed that light possesses both particle and wave properties. $$ \begin{align} E=\sqrt{p^2c^2+m_{0}^{2}c^{4}} \\ E=h\nu= \frac{hc}{\lambda} \end{align} $$ From the equation $(1)$ that expresses the relativistic energy of a particle and the equation $(2)$ derived from the photoelectric effect, it is understood</description></item><item><title>Expectation Value in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/1572/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1572/</guid><description>Definition The expectation value of a normalized wavefunction $\psi$ for an operator $A$ is defined as follows. $$ \braket{A} =\int_{-\infty}^{\infty} \psi^{\ast}A\psi dx $$ Explanation To put it simply, it is the same expectation value that you learned in high school statistics. When studying quantum mechanics, if you find it difficult to understand the expectation value, the difficulties can be broadly classified into two types. The first is the difficulty in</description></item><item><title>Walks, Trails, Paths, and Cycles in Graph Theory</title><link>https://freshrimpsushi.github.io/en/posts/1528/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1528/</guid><description>Definition 1 Let there be a given graph $G$. A finite sequence of edges is called a walk and is denoted as follows: $$ v_{0} v_{1} , v_{1} v_{2} , \cdots , v_{m-1} v_{m} \\ v_{0} \rightarrow v_{1} \rightarrow v_{2} \rightarrow \cdots \rightarrow v_{m-1} \rightarrow v_{m} $$ Here, $v_{0}$ is called the initial vertex, $v_{m}$ is called the final vertex, and $m$ is called the length. If all edges in</description></item><item><title>Moving Point Charges and the Electric Fields They Create</title><link>https://freshrimpsushi.github.io/en/posts/1273/</link><pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1273/</guid><description>Overview 1 The electromagnetic field created by a moving point charge is as follows. $$ \begin{align*} \mathbf{E}(\mathbf{r}, t) &amp;amp;= \frac{q}{4\pi\epsilon_{0}} \frac{\cR} {( \bcR\cdot \mathbf{u} )^3 } \left[(c^2-v^2)\mathbf{u} +\bcR\times (\mathbf{u} \times \mathbf{a} ) \right] \\ \mathbf{B} (\mathbf{ r}, t) &amp;amp;=\frac{1}{c} \crH\times \mathbf{ E } (\mathbf{ r}, t) \end{align*} $$ Description An introduction to the induction process for electric fields. Induction The electric and magnetic fields created by a moving point charge</description></item><item><title>Divisor Function in Analytic Number Theory</title><link>https://freshrimpsushi.github.io/en/posts/1527/</link><pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1527/</guid><description>Definition 1 For $\alpha \in \mathbb{C}$, the following $\sigma_{\alpha} : \mathbb{N} \to \mathbb{C}$ is defined as a divisor function. $$ \sigma_{\alpha} (n) := \sum_{d \mid n} d^{\alpha} $$ Basic Properties [1] Multiplicativity: For all $m, n \in \mathbb{N}$ that satisfy $\gcd (m,n) = 1$, $\sigma_{\alpha} (mn) = \sigma_{\alpha} (m) \sigma_{\alpha} (n)$ [2]: For a prime $p$ and natural number $a$, $$ \sigma_{\alpha} \left( p^{a} \right) = \begin{cases} a +1 &amp;amp;</description></item><item><title>Parity Operator</title><link>https://freshrimpsushi.github.io/en/posts/1571/</link><pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1571/</guid><description>Definition The operator $P$, defined as follows, is called the parity operator. $$ P\psi (x) = \psi (-x) $$ Description It is an operator that translates the position variable of a wave function symmetrically. The parity operator $P$ is used in quantum mechanics to distinguish between two degenerate eigenfunctions. Let&amp;rsquo;s suppose there are two degenerate wave functions as follows. $$ \psi_{1}(x)=e^{ikx},\quad \psi_{2}(x)=e^{-ikx} $$ Then, solving the eigenvalue equation for the</description></item><item><title>Infinite Graph</title><link>https://freshrimpsushi.github.io/en/posts/1526/</link><pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1526/</guid><description>Definitions 1 If the vertex set $V(G)$ or the edge set $E(G)$ of a graph $G$ is an infinite set, then $G$ is called an infinite graph. An infinite graph $G$, whose $V(G)$ and $E(G)$ are both countable sets, is called a Countable Graph. Let&amp;rsquo;s define $A(v)$ for a vertex $v \in V(G)$ of an infinite graph $G$ as follows. $$ A(v) := \left\{ w : vw \in E(G) \right\}</description></item><item><title>Multiplicative Function's Abelian group</title><link>https://freshrimpsushi.github.io/en/posts/1525/</link><pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1525/</guid><description>Theorem 1 The set of multiplicative functions $M$ and the binary operation $\ast$ satisfy that $(M,*)$ is an Abelian group. Description Just as the set of arithmetic functions $A$ becomes an Abelian group $(A,*)$ with convolution $\ast$, so does the set of multiplicative functions. Of course, $M \le A$, that is $M$ becomes a subgroup of $A$. Proof A $\left&amp;lt; G, \ast\ \right&amp;gt;$, a monoid, is defined as a group</description></item><item><title>Bipartite Graph</title><link>https://freshrimpsushi.github.io/en/posts/1524/</link><pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1524/</guid><description>Definition 1 A graph $G$ is called a bipartite graph and is also represented as $G = G(A,B)$ if there exists a partition $\left\{ A,B \right\}$ of the vertices $V(G)$ such that for every $xy \in E(G)$, it is either $x \in A, y \in B$ or $x \in B , y \in A$. Explanation As the name suggests, a bipartite graph is a graph where the vertices are divided</description></item><item><title>Dirichlet Product and Multiplicative Properties</title><link>https://freshrimpsushi.github.io/en/posts/1523/</link><pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1523/</guid><description>Theorem 1 [1]: If $f$ and $g$ are multiplicative functions, then $f \ast\ g$ is also a multiplicative function. [2]: If $g$ and $f \ast g$ are multiplicative functions, then $f$ is also a multiplicative function. Description These properties can be used immediately when discussing the algebraic properties of multiplicative functions: Theorem [1] means, in other words, that multiplicative functions are closed under the convolution $\ast$. Theorem [2] allows proving</description></item><item><title>The Relationship Between the Gamma Distribution and the Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/135/</link><pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/135/</guid><description>Theorem $$ \Gamma \left( { r \over 2 } , 2 \right) \iff \chi ^2 (r) $$ Description The gamma distribution and the chi-square distribution have the following properties. Proof Strategy: It is shown that the moment-generating functions of the two distributions can be represented in the same form. The moment-generating function of the chi-square distribution $\chi ^2 (r)$ is $\displaystyle m_{1}(t) = (1- 2t)^{- {r \over 2} }$, and</description></item><item><title>Regular Graph</title><link>https://freshrimpsushi.github.io/en/posts/1522/</link><pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1522/</guid><description>Definition 1 A graph is called a Regular Graph if all vertices have the same degree. Specifically, if all vertices have a degree of $r$, it is called a $r$-Regular Graph. In other words, a graph $G$ that satisfies the following is referred to as a $r$-Regular Graph. $$ \deg (v) = r \qquad , \forall v \in V(G) $$ A $2$-Regular connected graph is called a Cycle. Examples Regular</description></item><item><title>Relationship between Gamma Distribution and Exponential Distribution</title><link>https://freshrimpsushi.github.io/en/posts/133/</link><pubDate>Sun, 05 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/133/</guid><description>Theorem $$ \Gamma \left(1, { 1 \over \lambda } \right) \iff \text{exp} (\lambda) $$ Description If we think about the intuitive definition of the exponential distribution, it&amp;rsquo;s about the interest in the amount of time it takes for a certain event to occur. If we were to relate this to a discrete probability distribution, the geometric distribution would correspond to this. In this sense, the generalization of the geometric distribution</description></item><item><title>Arithmetic Functions' Multiplicative Properties</title><link>https://freshrimpsushi.github.io/en/posts/1521/</link><pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1521/</guid><description>Definition 1 For $\forall n \in \mathbb{N}$, if an arithmetic function $f$ that is not $f(n) = 0$ satisfies the following, it is called a multiplicative function. $$ f(mn) = f(m) f(n) \qquad,\gcd(m,n)=1 $$ If a multiplicative function satisfies the following condition, it is called a completely multiplicative function. $$ f(mn) = f(m) f(n) \qquad,m,n \in \mathbb{N} $$ Basic Properties [1]: If $f$ is multiplicative, then $f(1) = 1$. [2]:</description></item><item><title>The Relationship between Gamma Distribution and Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/98/</link><pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/98/</guid><description>Theorem For all natural numbers $k$, the following holds. $$ \int_{\mu}^{\infty} { { z^{k-1} e^{-z} } \over { \Gamma (k) } } dz = \sum_{x=0}^{k-1} { { {\mu}^{x} e^{-\mu} } \over {x!} } $$ $\Gamma$ is the gamma function. Description $k, \theta &amp;gt; 0$ For the following probability density function for continuous probability distribution $\Gamma ( k , \theta )$ is called Gamma Distribution. $$ f(x) = {{ 1 }</description></item><item><title>Null Graphs and Complete Graphs</title><link>https://freshrimpsushi.github.io/en/posts/1520/</link><pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1520/</guid><description>Definition 1 Given a simple graph $G$. If $E(G) = \emptyset$, then $G$ is called a null graph. If $E \left( \overline{G} \right) = \emptyset$, then $G$ is called a complete graph. Description A null graph is literally an empty graph. The reason why we use the term Null instead of Empty is that even if $G \ne \emptyset$, it has no meaning as a graph. For example, if there</description></item><item><title>Mean and Variance of Gamma Distribution</title><link>https://freshrimpsushi.github.io/en/posts/132/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/132/</guid><description>Formula Random Variable $X$ is said to be $X \sim \Gamma \left( k , \theta \right)$ with respect to the Gamma Distribution $\Gamma \left( k , \theta \right)$. $$ E(X) = k \theta \\ \Var (X) = k \theta^{2} $$ Derivation Strategy: Directly infer using the definition of the gamma distribution and the basic properties of the gamma function. Use a trick to balance the numerator and denominator of the</description></item><item><title>Gamma Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1517/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1517/</guid><description>Definition 1 For $k, \theta &amp;gt; 0$, it is called the Gamma Distribution which has the following probability density function $\Gamma ( k , \theta )$. $$ f(x) = {{ 1 } \over { \Gamma ( k ) \theta^{k} }} x^{k - 1} e^{ - x / \theta} \qquad , x &amp;gt; 0 $$ $\Gamma$ represents the Gamma function. The probability density function of the Gamma distribution can also be</description></item><item><title>Graph Complement</title><link>https://freshrimpsushi.github.io/en/posts/1515/</link><pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1515/</guid><description>Definition 1 For a simple graph $G$, a graph $\overline{G}$ that satisfies the following conditions is called the complement of $G$. $$ V \left( \overline{G} \right) = V(G) \\ vw \in E \left( \overline{G} \right) \iff vw \notin E(G) $$ Description As with the concept of Complement in mathematics, the complement of a graph implies the concept of supplement. If we were to translate it into more native Korean, it</description></item><item><title>Arithmetic Functions of Abelian groups</title><link>https://freshrimpsushi.github.io/en/posts/1514/</link><pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1514/</guid><description>Theorem 1 Given a set of arithmetic functions $A = \left\{ f : \mathbb{N} \to \mathbb{C} \mid f(1) \ne 0 \right\}$ other than $f(1) \ne 0$ and binary operation $\ast$, $(A,*)$ is an Abelian group. Description Strictly speaking, not all sets of arithmetic functions can be Abelian groups, due to the last condition for an algebraic structure to be a group, the existence of an inverse element. Fortunately, this condition</description></item><item><title>Geometric Distribution's Memorylessness</title><link>https://freshrimpsushi.github.io/en/posts/217/</link><pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/217/</guid><description>Theorem $X \sim \text{Geo} ( m )$ implies $P(X \ge s+ t ,|, X \ge s) = P(X \ge t)$ Explanation The geometric distribution concerns the number of trials needed for an event to occur and is a discrete probability distribution. Thinking of it as a discretization of the exponential distribution, it&amp;rsquo;s natural to consider the memorylessness of the geometric distribution. Here, the memoryless property refers to the characteristic where</description></item><item><title>Subgraph</title><link>https://freshrimpsushi.github.io/en/posts/1513/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1513/</guid><description>Definition 1 For a given graph $G$, graph $H$ is said to be a subgraph of $G$ if it satisfies $V(H) \subset V(G)$ and $ E(H) \subset E(G)$. Explanation It is important not to denote $H$ being a subgraph of $G$ as $H \subset G$. The concept of a subgraph serves not so much as a focus of interest in graph theory itself but rather as a natural and common</description></item><item><title>Exponential Distribution's Memorylessness</title><link>https://freshrimpsushi.github.io/en/posts/216/</link><pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/216/</guid><description>Properties If $X \sim \exp{ ( \lambda ) }$ then $P(X \ge s+ t ,|, X \ge s) = P(X \ge t)$ Explanation The exponential distribution is a continuous probability distribution that focuses on the timeframe within which a certain event occurs. It&amp;rsquo;s easy to assume that it can be applied in predicting lifespans or in insurance. The Memoryless Property means that future events are not influenced by the amount</description></item><item><title>Graphical Set Notation</title><link>https://freshrimpsushi.github.io/en/posts/1512/</link><pubDate>Mon, 23 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1512/</guid><description>Definition 1 Let&amp;rsquo;s consider two graphs $G_{1}$ and $G_{2}$ and let $V(G_{1}) \cap V(G_{2}) = \emptyset$. The union $G = G_{1} \cup G_{2}$ of two graphs is a graph that has a vertex set $V(G_{1}) \cup V(G_{2})$ and an edge set $E (G_{1}) \cup E ( G_{2} )$. If graph $H$ cannot be represented as the union of other graphs, then $H$ is said to be connected; otherwise, it is</description></item><item><title>The Relationship between Exponential Distribution and Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/296/</link><pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/296/</guid><description>Theorem When the time it takes for an event to occur is given by $X_{k}$, and if $X_{k} \sim \exp (\lambda)$, then the number of occurrences of an event per unit time is given by $N$, and $\displaystyle N \sim \text{Poi} (\lambda)$</description></item><item><title>Mean and Variance of Exponential Distribution</title><link>https://freshrimpsushi.github.io/en/posts/62/</link><pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/62/</guid><description>Formula $X \sim \exp ( \lambda)$ Surface $$ E(X) = {{ 1 } \over { \lambda }} \\ \Var (X) = {{ 1 } \over { \lambda^{2} }} $$ Proof Strategy: Deduce directly from the definition of the exponential distribution. Definition of the Exponential Distribution: For $\lambda &amp;gt; 0$, continuous probability distribution $\exp ( \lambda)$ with the following probability density function is called the exponential distribution. $$ f(x) = \lambda</description></item><item><title>Exponential Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1510/</link><pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1510/</guid><description>Definition 1 The continuous probability distribution $\exp ( \lambda)$ with the following probability density function, for $\lambda &amp;gt; 0$, is called an Exponential Distribution. $$ f(x) = \lambda e^{-\lambda x} \qquad , x \ge 0 $$ Depending on the book, the parameter might be its reciprocal, $\displaystyle \theta = {{ 1 } \over { \lambda }}$. Basic Properties Moment Generating Function [1]: $$m(t) = {{ \lambda } \over { \lambda</description></item><item><title>Proof of Darboux's Intermediate Value Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1554/</link><pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1554/</guid><description>정리 If a function $f : [a,b] \to \mathbb{R}$ is differentiable at $[a,b]$, there exists a $c \in (a,b)$ such that $y_{0} = f ' (c)$ is satisfied between $f ' (a)$ and $f ' (b)$ for some $y_{0}$.</description></item><item><title>Matrix Representation of Graphs</title><link>https://freshrimpsushi.github.io/en/posts/1499/</link><pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1499/</guid><description>Definition 1 Let&amp;rsquo;s assume that a graph $G(V,E)$ is given. Degree Matrix Let&amp;rsquo;s denote the degree $d(v_{i})$ of each vertex $v_{i}\in V$ simply as $d_{i}$. The following matrix is called the degree matrix of $G$ and is denoted as $D(G)$ or simply $D$. $$ D(G) = \mathrm{diag} (d_{1}, \dots, d_{n}) = \begin{bmatrix} d_{1} &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; d_{2} &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp;</description></item><item><title>Autonomous Systems: Flow and Time-T Maps</title><link>https://freshrimpsushi.github.io/en/posts/1507/</link><pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1507/</guid><description>Definition 1 Flow Given a space $X$ and a function $f : X \to X$, suppose we have the following vector field presented as a differential equation. $$ \dot{x} = f(x) $$ For a time variable $t$ and an initial value $x_{0}$, the solution to the autonomous differential equation is called a flow, which is denoted as $F(t, x_{0})$. For a fixed unit time $t = T$, $F_{T}(x) := F(T,x)$</description></item><item><title>Shaking Hands Dilemma Proof</title><link>https://freshrimpsushi.github.io/en/posts/1506/</link><pubDate>Fri, 13 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1506/</guid><description>Theorem 1 In any directed graph, the sum of the in-degrees and the sum of the out-degrees are equal. Explanation The handshake dilemma can be considered the handshake lemma for directed graphs. Proof In a directed graph, the sum of the out-degrees is equal to the number of arcs. Since an arc comes out from one vertex and enters another, the sum of the out-degrees and in-degrees are equal. ■</description></item><item><title>Dynamical Systems Described by Differential Equations and Equilibrium Points</title><link>https://freshrimpsushi.github.io/en/posts/1505/</link><pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1505/</guid><description>Definition 1 Given a space $V$ and function $f : V \to V$, assume the following vector field is given as a differential equation: $$ \dot{v} = f(v) $$ If variable $t$ is included in the differential equation and $t$ is not explicitly shown, it is referred to as an Autonomous Differential Equation. If a constant function $f_{0} (v)$ is a solution to the autonomous differential equation $\dot{v} = f(v)$,</description></item><item><title>Handshaking Lemma Proof</title><link>https://freshrimpsushi.github.io/en/posts/1504/</link><pubDate>Mon, 09 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1504/</guid><description>Theorem 1 In any given graph, the sum of the degrees of all vertices is even. Description The term &amp;ldquo;handshake&amp;rdquo; obviously refers to the scenario where each vertex &amp;ldquo;shakes hands&amp;rdquo; with its adjacent vertices, and thus the total number of these handshakes corresponds to the sum of the degrees. Proof For a graph $G$, the sum of all degrees must exactly be twice the number of edges as per $$</description></item><item><title>Attractors in Chaos</title><link>https://freshrimpsushi.github.io/en/posts/1497/</link><pubDate>Sat, 07 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1497/</guid><description>Definition 1 Let the space be $X = \left( \mathbb{R}^{n} , \left\| \cdot \right\| \right)$ and a smooth function $f,g : X \to X$. Consider that a vector field and map are expressed as follows: $$ \dot{x} = f(x) \\ x \mapsto g(x) $$ Let $\phi (t, \cdot)$ be the flow of the vector field $\dot{x} = f(x)$, $g^{n}$ be the $g$-th iterate of a map, and $\Lambda \subset X$</description></item><item><title>Graph Theory: Degree</title><link>https://freshrimpsushi.github.io/en/posts/1496/</link><pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1496/</guid><description>Definition 1 Let&amp;rsquo;s assume a directed graph $G$ is given. If an edge $vw$ exists, we say that the edge leaves from $v$ and enters into $w$. The number of edges entering vertex $v$ is called the Indegree and is denoted as $\deg^{-} (v)$. The number of edges leaving vertex $v$ is called the Outdegree and is denoted as $\deg^{+}(v)$. A vertex that is $\deg^{-} (v) = 0$ is called</description></item><item><title>Inverse of Dirichlet Products</title><link>https://freshrimpsushi.github.io/en/posts/1494/</link><pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1494/</guid><description>Definition 1 An arithmetic function $f^{-1}$ is said to be the (Dirichlet) inverse of another arithmetic function $f$ if there exists a unique arithmetic function $f^{-1}$ satisfying the following equation $f$. $$ f \ast\ f^{-1} = f^{-1} \ast\ f = I $$ Here, $I$ is the identity function with respect to convolution. Theorem [1]: If an arithmetic function $f$ is $f(1) \ne 0$, then its inverse $f^{-1}$ uniquely exists and</description></item><item><title>Graph Isomorphism</title><link>https://freshrimpsushi.github.io/en/posts/1492/</link><pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1492/</guid><description>Definition 1 Let two graphs $G_{1}$ and $G_{2}$ be given. If there exists a bijective function between $V(G_{1})$ and $V(G_{2})$, and the number of edges between vertices of $G_{1}$ matches with the corresponding vertices of $G_{2}$, then this bijective function is called an isomorphism, and the two graphs are said to be isomorphic. In other words, a bijective function $\phi : G_{1} \to G_{2}$ satisfying the following is called an</description></item><item><title>Mean and Variance of the Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/61/</link><pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/61/</guid><description>Formulas $X \sim \text{Poi}(\lambda)$ Surface $$ E(X) = \lambda \\ \Var(X) = \lambda $$ Derivation Strategy: Directly deduce from the definition of the Poisson distribution. The trick of splitting factorials and series is important. Definition of Poisson Distribution: For $\lambda &amp;gt; 0$, an discrete probability distribution that has the following probability mass function $\text{Poi} ( \lambda )$ is called a Poisson distribution. $$ p(x) = {{ e^{-\lambda} \lambda^{x} } \over</description></item><item><title>Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1491/</link><pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1491/</guid><description>Definitions 1 For $\lambda &amp;gt; 0$, we refer to the following probability mass function as the Poisson Distribution that has a discrete probability distribution $\text{Poi} ( \lambda )$. $$ p(x) = {{ e^{-\lambda} \lambda^{x} } \over { x! }} \qquad , x = 0 , 1 , 2, \cdots $$ Basic Properties Moment Generating Function [1]: $$m(t) = \exp \left[ \lambda \left( e^{t} - 1 \right) \right] \qquad , t</description></item><item><title>Time Derivatives of the Lienard-Wiechert Potentials</title><link>https://freshrimpsushi.github.io/en/posts/1544/</link><pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1544/</guid><description>Overview The time derivative of the Liénard-Wiechert potential is as follows. $$ \begin{align*} \frac{ \partial V}{ \partial t} &amp;amp;= \frac{qc}{4\pi \epsilon_{0}} \frac{1}{(\cR c -\bcR \cdot \mathbf{v})^{2}} \left( c^{2} -c^{2}\frac{ \partial t}{ \partial t_{r}}-v^{2}+\bcR\cdot \mathbf{a} \right)\frac{ \partial t_{r}}{ \partial t } \\ \frac{ \partial \mathbf{A}}{ \partial t } &amp;amp;= \frac{qc}{4\pi \epsilon_{0}} \frac{1}{(\cR c -\bcR \cdot \mathbf{v})^{3}}\left[</description></item><item><title>Identity for Dirichlet Products</title><link>https://freshrimpsushi.github.io/en/posts/1490/</link><pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1490/</guid><description>Definition 1 A arithmetic function defined as follows $I$ is called the identity function. $$ I(n) := \left[ {{ 1 } \over { n }} \right] $$ [1] Identity series: This is the unit function $u$. In other words, $$ \sum_{d \mid n}I(d) = u(n) = 1 $$ [2] Completely multiplicative: For all $n , m \in \mathbb{N}$, $I (mn) = I(m) I(n)$ [a] Identity element for convolution: For all</description></item><item><title>Differential Equations for Physics: Solutions to Commonly Encountered Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1538/</link><pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1538/</guid><description>Differential Equations This has been explained as intuitively as possible for those studying undergraduate physics. A differential equation is, simply put, an equation that involves derivatives. Without any complications, since acceleration is the second derivative of position, the most famous physics formula $F=ma$ is also a differential equation. The polynomial $x^{3}+3x+1=0$ is called a third-degree equation because its highest order is 3. Similarly, when the maximum number of times differentiated</description></item><item><title>Mean and Variance of the Negative Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/94/</link><pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/94/</guid><description>Formula $X \sim \text{NB}(r, p)$ Plane $$ E(X) = {{ r (1-p) } \over { p }} \\ \Var(X) = {{ r (1-p) } \over { p^{2} }} $$ Proof Strategy: Use the fact that the negative binomial distribution is a generalization of the geometric distribution. [b] Generalization of Geometric Distribution: If $Y = X_{1} + \cdots + X_{r}$ and $X_{i} \overset{\text{iid}}{\sim} \text{Geo}(p)$ then $Y \sim \text{NB}(r,p)$ Here, the definition</description></item><item><title>Negative Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1489/</link><pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1489/</guid><description>Definition 1 Given $r \in \mathbb{N}$ and $p \in (0,1]$, a discrete probability distribution $\text{NB}(r,p)$ with the following probability mass function is called the Negative Binomial Distribution. $$ p(x) = \binom{r+x-1}{x-1} p^{r}(1-p)^{x} \qquad, x = 0,1,2,\cdots $$ Basic Properties Moment Generating Function [1]: $$m(t) = \left[ {{ p } \over { 1 - (1-p) e^{t} }} \right]^{r} \qquad , t &amp;lt; -\log (1-P)$$ Mean and Variance [2]: If $X \sim</description></item><item><title>Trigonometric Form of the Legendre Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1537/</link><pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1537/</guid><description>Definition The associated Legendre differential equation in the form of a trigonometric function is as follows. $$ \begin{align} \frac{ d^{2} y}{ d \theta^{2} }+\cot \theta \frac{ d y}{ d \theta}+ \left( l(l+1) -\frac{m^{2}}{\sin ^{2 }\theta} \right)y=0 \\ \mathrm{or} \quad\frac{1}{\sin \theta}\left(\sin \theta \frac{dy}{d\theta} \right)+ \left(l(l+1) -\frac{ m^{2}}{\sin ^{2} \theta} \right)y=0 \end{align} $$ Explanation Useful for solving spherical coordinate Laplace&amp;rsquo;s equation in electromagnetics, quantum mechanics, etc. The solutions are as follows. $$</description></item><item><title>Arithmetic Functions' Dirichlet Convolution</title><link>https://freshrimpsushi.github.io/en/posts/1488/</link><pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1488/</guid><description>Definition 1 For two arithmetic functions $f$, $g$, the arithmetic function $h$ satisfying the following is called the Dirichlet product of $f$ and $g$. $$ h(n) = \sum_{d \mid n} f(d) g \left( {{ n } \over { d }} \right) $$ The Dirichlet product can be represented as either $h (n) = \left( f \ast g \right) (n) $ or $h = f \ast g$. Explanation The Dirichlet product,</description></item><item><title>Differences Between the Two Definitions of the Geometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/295/</link><pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/295/</guid><description>Description While studying the geometric distribution, the most perplexing and confusing aspect is the differing explanations across textbooks, blogs, and wikis. Some sources mention the mean as $\displaystyle {{1} \over {p}} $, while others use $\displaystyle {{1-p} \over {p}}$. This discrepancy arises because there are two ways to define the geometric distribution. The probability mass function of the geometric distribution $\text{Geo}(p)$ is defined either as $$ p_{1}(x) = p(1-p)^{x-1} ,</description></item><item><title>Arithmetic Functions in Analytic Number Theory</title><link>https://freshrimpsushi.github.io/en/posts/1487/</link><pubDate>Tue, 18 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1487/</guid><description>Definition 1 A function whose domain is the set of natural numbers $\mathbb{N}$ and whose range is the set of real numbers $\mathbb{R}$ or the set of complex numbers $\mathbb{C}$ is called an arithmetic function. Description In analytic number theory, there is interest in the properties and relationships of various arithmetic functions, including examples such as: Identity function $I$ Divisor function $\sigma_{\alpha}$ Norm $N$ Divisor function $\sigma_{\alpha}$ Möb</description></item><item><title>Installing the Latest Version of Julia on Linux</title><link>https://freshrimpsushi.github.io/en/posts/1511/</link><pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1511/</guid><description>Guide After Julia Version 1.10 In the terminal, enter the command curl -fsSL https://install.julialang.org | sh as shown above. Before Julia Version 1.10 Step 1. Download Julia Download the file that matches your CPU&amp;rsquo;s bit from Generic Linux Binaries for x86. Step 2. Extract and Move Extract the file. Move the folder to the location where Julia will be stored. You can place it anywhere you like, but in this</description></item><item><title>Mean and Variance of the Geometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/63/</link><pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/63/</guid><description>Formula $X \sim \text{Geo} (p)$ Area $$ E(X) = {{ 1 } \over { p }} \\ \Var(X) = {{ 1-p } \over { p^{2} }} $$ Derivation The mean and variance of the Geometric Distribution are not as easily calculated as one might think. This post introduces two interesting and useful proofs. The discrete probability distribution that has the following probability mass function is called a Geometric Distribution, according</description></item><item><title>Geometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1486/</link><pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1486/</guid><description>Definition 1 For $p \in (0,1]$, the discrete probability distribution $\text{Geo}(p)$ that follows the probability mass function as shown above, is called the Geometric Distribution. $$ p(x) = p (1 - p)^{x-1} \qquad , x = 1 , 2, 3, \cdots $$ Take special care with the domain and the formula as there are two definitions used. Basic Properties Moment Generating Function [1]: $$m(t) = {{ p e^{t} } \over</description></item><item><title>Multidimensional Map Chaos</title><link>https://freshrimpsushi.github.io/en/posts/1485/</link><pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1485/</guid><description>Definition1 An orbit of the map $f : \mathbb{R}^{m} \to \mathbb{R}^{m}$ is called chaotic if it satisfies the following conditions: (i): It is not asymptotically periodic. (ii): For every $i = 1,\cdots , m$, $h_{i} ( \mathbf{v}_{0} ) \ne 0$ (iii): $h_{1} ( \mathbf{v}_{0}) &amp;gt; 0$ Saying an orbit is bounded indicates the existence of $M \in \mathbb{R}$ that satisfies $\left\| \mathbf{v}_{n} \right\| &amp;lt; M$ for every $n \in \mathbb{N}_{0}$.</description></item><item><title>Mean and Variance of the Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/60/</link><pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/60/</guid><description>Formulas $\displaystyle X \sim \text{Bin} (n,p)$ Surface $$ E(X)=np \\ \Var(X)=npq $$ Where $q : = 1-p$ stands. Derivation Strategy: Directly unravel the combinations. The expression might be quite messy, but it&amp;rsquo;s entirely digestible at the high school level. It&amp;rsquo;s worth trying out at least once. Upon encountering mathematical statistics, one can prove it using a much shorter and simpler method. Be it the mean or variance, start with the</description></item><item><title>Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1480/</link><pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1480/</guid><description>Definition 1 The discrete probability distribution $\text{Bin}(n,p)$ with the following probability mass function for $n \in \mathbb{N}$ and $p \in [0,1]$ is called the Binomial Distribution. $$ p(x) = \binom{n}{x} p^{x} (1-p)^{n-x} \qquad , x = 0 , 1, \cdots n $$ Basic Properties Moment Generating Function [1]: $$m(t) = \left[ (1-p) + pe^{t} \right]^{n} \qquad , t \in \mathbb{R}$$ Mean and Variance [2]: If $X \sim \text{Bin}(n,p)$ then $$</description></item><item><title>List of decimals to the 10,000th</title><link>https://freshrimpsushi.github.io/en/posts/2339/</link><pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2339/</guid><description>Prime numbers A list of primes up to the 10,000th. Download 2 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97 101 103 107 109 113 127 131 137 139 149 151 157 163 167 173 179 181 191 193 197 199 211 223 227 229 233 239 241 251 257 263 269 271 277</description></item><item><title>Velocity and Acceleration of an Object Moving in a Rotating Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/906/</link><pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/906/</guid><description>Formulas The velocity and acceleration of an object in a rotating coordinate system are as follows. $$ \mathbf{v} = \mathbf{v}^{\prime} + \boldsymbol{\omega} \times \mathbf{r}^{\prime} +\mathbf{V}_{0} $$ $$ \mathbf{a} = \mathbf{a}^{\prime} + \dot{\boldsymbol{\omega}} \times \mathbf{r}^{\prime}+ 2\boldsymbol{\omega} \times \mathbf{v}^{\prime}+\boldsymbol{\omega} \times ( \boldsymbol{\omega} \times \mathbf{r}^{\prime}) + \mathbf{A}_{0} $$ Rotating Coordinate System1 Imagine a train in motion and a fly buzzing around inside it. For a person inside the train, considering only the movement</description></item><item><title>Linear Combinations of Random Variables</title><link>https://freshrimpsushi.github.io/en/posts/1479/</link><pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1479/</guid><description>Definition 1 Let $X_{1} , \cdots , X_{n}$ be given as a random variable. For some $(a_{1}, \cdots , a_{n}) \in \mathbb{R}^{n}$, $\displaystyle T := \sum_{i=1}^{n} a_{i} X_{i}$ is referred to as Linear Combinations. Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p136.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>For an Angle Small Enough</title><link>https://freshrimpsushi.github.io/en/posts/1516/</link><pubDate>Sun, 09 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1516/</guid><description>Description Physics makes use of the approximation $\sin x\approx x$ in many places. The reason this approximation can be used is because the following equation holds: $$ \lim \limits_{x\rightarrow 0}\frac{\sin x}{x}=1 $$ Since this equation is first introduced in high school, college students might feel it is obvious enough to not question the validity of such approximation. However, how small should something be to be considered similar? For instance, when</description></item><item><title>How to Parallel Process in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1474/</link><pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1474/</guid><description>Code While the original sushi restaurant with fresh shrimp includes detailed explanations, Julia wants to omit explanations on purpose to emphasize how easy it is to do parallel processing. using Base.Threads for i in 1:10 println(i^2) end If you want to parallelize the above loop, you just need to prepend @threads to the for loop. @threads for i in 1:10 println(i^2) end However, if I must add one piece of</description></item><item><title>Vectors and Inner Products in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/1509/</link><pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1509/</guid><description>Generalization of Vectors For science students who have not learned linear algebra, a vector is a physical quantity with magnitude and direction, meaning a point in 3-dimensional space, and it is usually represented as in $\vec{x} = (x_{1}, x_{2}, x_{3})$. This definition poses no significant problem in studying classical mechanics and electromagnetism. However, in quantum mechanics, concepts like Fourier analysis and inner product of functions arise, so if one does</description></item><item><title>Lyapunov Numbers and Their Numerical Calculation Methods for Multidimensional Maps</title><link>https://freshrimpsushi.github.io/en/posts/1472/</link><pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1472/</guid><description>Definition 1 Given a smooth map $\mathbf{f} : \mathbb{R}^{m} \to \mathbb{R}^{m}$ and initial value $\mathbf{v}_{0} \in \mathbb{R}^{m}$, let&amp;rsquo;s say $J_{n} := D \mathbf{f}^{n} ( \mathbf{v}_{0}) \in \mathbb{R}^{m \times m}$. For $k = 1 , \cdots , m$, consider the length of the $k$-th longest axis of the ellipsoid $J_{n} N$, which is the unit sphere $N := \left\{ \mathbf{x} \in \mathbb{R}^{m} : \left\| \mathbf{x} \right\|_{2} = 1 \right\}$ in $m$</description></item><item><title>Frobenius Method</title><link>https://freshrimpsushi.github.io/en/posts/1508/</link><pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1508/</guid><description>Explanation1 There are various methods to solve differential equations. One of them involves assuming the solution as a power series, as follows. $$ y=\sum \limits_{n=0}^{\infty} a_{n}x^{n} $$ However, some series cannot be represented in the form above. For example, as follows. $$ \frac{\cos x}{x^{2}}=\frac{1}{x^{2}}-\frac{1}{2!}+\frac{ x^{2}}{4!}-\cdots $$ $$ \sqrt{x} \sin x = x^{\frac{1}{2}}\left( x - \frac{x^{3}}{3!}+\cdots \right) $$ In such cases, the solution is assumed to be in the following form.</description></item><item><title>The Equivalence Between Two Normally Distributed Random Variables Being Independent and Having a Covariance of Zero</title><link>https://freshrimpsushi.github.io/en/posts/591/</link><pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/591/</guid><description>Theorem $$ X_{1} \sim N ( \mu_{1} , \sigma_{1} ) \\ X_{2} \sim N ( \mu_{2} , \sigma_{2} ) $$ Surface $$ X_{1} \perp X_{2} \iff \text{cov} (X_{1} , X_{2} ) = 0 $$ Description Generally, being uncorrelated does not imply independence. However, if there is an assumption that the distributions follow a normal distribution, then having a covariance of $0$ guarantees independence. Proof $( \implies )$ $$ M_{X_{1}} (t_{1}</description></item><item><title>Generalization of the Ellipse: Ellipsoid</title><link>https://freshrimpsushi.github.io/en/posts/1471/</link><pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1471/</guid><description>Definition For a linear transformation $A \in \mathbb{R}^{m \times m}$, the image $AN$ of a $m$-dimensional unit sphere $N := \left\{ \mathbf{x} \in \mathbb{R}^{m} : \left\| \mathbf{x} \right\|_{2} = 1 \right\}$ is called an ellipsoid. The eigenvalues $\sigma_{1}^{2} &amp;gt; \cdots \ge \sigma_{m}^{2} \ge 0$ of $A$ and the corresponding unit eigenvectors $u_{1} , \cdots , u_{m}$ are referred to as the axes of the ellipsoid for $\sigma_{i} u_{i}$. Explanation A</description></item><item><title>Bernstein Distributions: Pairwise Independence Does Not Imply Mutual Independence</title><link>https://freshrimpsushi.github.io/en/posts/206/</link><pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/206/</guid><description>Definition For $(x,y,z) \in \left\{ (1,0,0), (0,1,0), (0,0,1), (1,1,1) \right\}$, the distribution with the following probability mass function is called the Bernstein Distribution. $$ p(x,y,z) = {{1} \over {4} } $$ Explanation Although the Bernstein Distribution satisfies all the conditions for a distribution, it is hard to consider it as a distribution that actually exists in nature. It is presented as a counterexample to the proposition that &amp;lsquo;if pairs are</description></item><item><title>Series Solution of the Bessel Equation: Bessel Functions of the First Kind</title><link>https://freshrimpsushi.github.io/en/posts/1503/</link><pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1503/</guid><description>Definition1 For $\nu \in \mathbb{R}$, a differential equation of the following form is called a $\nu$ order Bessel equation. $$ \begin{align*} &amp;amp;&amp;amp; x^{2} y^{\prime \prime} +xy^{\prime}+(x^{2}-\nu^{2})y &amp;amp;= 0 \\ \text{or} &amp;amp;&amp;amp; y^{\prime \prime}+\frac{1}{x} y^{\prime} + \left( 1-\frac{\nu^{2}}{x^{2}} \right)y &amp;amp;= 0 \end{align*} $$ Explanation The Bessel equation emerges when solving the wave equation in spherical coordinates. The coefficients are not constant but depend on the independent variable $x$. Since, at $x=0$,</description></item><item><title>Independence and iid of Random Variables</title><link>https://freshrimpsushi.github.io/en/posts/1469/</link><pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1469/</guid><description>Definition 1 A random variable $X_{1} , \cdots , X_{n}$ is said to be pairwise independent if it satisfies the following. $$ i \ne j \implies X_{i} \perp X_{j} $$ A continuous random variable $X_{1} , \cdots , X_{n}$ whose joint probability density function $f$ satisfies the condition with respect to each of its probability density functions $f_{1} , \cdots , f_{n}$ is said to be mutually independent. $$ f(x_{1}</description></item><item><title>Angular Momentum Operator in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1495/</link><pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1495/</guid><description>Formula The angular momentum operator is expressed in spherical coordinates as follows. $$ \begin{align*} L_{x} &amp;amp;= \i\hbar \left(\sin\phi\dfrac{\partial }{\partial \theta} + \cos\phi \cot\theta \dfrac{\partial }{\partial \phi}\right) \\ L_{y} &amp;amp;= -\i\hbar \left( \cos\phi \dfrac{\partial }{\partial \theta} - \sin\phi \cot\theta \dfrac{\partial }{\partial \phi}\right) \\ L_{z} &amp;amp;= -\i\hbar \dfrac{\partial }{\partial \phi} \end{align*} $$ Derivation The definition of the angular momentum operator is as follows. $$ L = \mathbf{r} \times P = - \i\hbar</description></item><item><title>Stochastic Gradient Descent</title><link>https://freshrimpsushi.github.io/en/posts/1464/</link><pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1464/</guid><description>Definition The method known as Stochastic Gradient Descent refers to, given the objective function $Q$, learning rate $\alpha &amp;gt; 0$, batch size $m$, and for the $i$th data, $$ \omega_{n+1} := \omega_{n} - \alpha {{ 1 } \over { n }} \sum_{i=1}^{m} \nabla Q_{i} ( \omega_{n} ) $$. Explanation Machine Learning Stochastic Gradient Descent is inevitably deeply related to machine learning as it deals with data. Even if some terms</description></item><item><title>Probability Variables Independence in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1461/</link><pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1461/</guid><description>Definition 1 If for two random variables $X_{1}, X_{2}$, the joint probability density function $f$ or the probability mass function $p$ satisfies the following conditions for the probability density functions $f_{1}, f_{2}$ or the probability mass functions $p_{1}, p_{2}$ of $X_{1}, X_{2}$, then $X_{1}, X_{2}$ are said to be independent, and is denoted as $X_{1} \perp X_{2}$. $$ f(x_{1} , x_{2} ) \equiv f_{1}(x_{1})f_{2}(x_{2}) \\ p(x_{1} , x_{2} ) \equiv</description></item><item><title>Second Derivative, Higher Order Derivative</title><link>https://freshrimpsushi.github.io/en/posts/1094/</link><pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1094/</guid><description>Definition1 Let the function $f$ have a derivative $f^{\prime}$ on the interval $I$. If $f^{\prime}$ itself also has a derivative, we call it the second derivative of $f$ and denote it by $f^{\prime\prime}$. If $f^{\prime\prime}$ also has a derivative, we denote it as $f^{\prime \prime \prime}$, or simply as $f^{(3)}$. In the same manner, the $n$th derivative of $f$ is denoted as $f^{(n)}$. $$ f,\ f^{\prime},\ f^{\prime\prime},\ f^{(3)},\ \dots,\ f^{(n)}</description></item><item><title>Optimization Techniques in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1463/</link><pubDate>Sat, 25 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1463/</guid><description>Definition The problem of finding $x^{ \ast } = \argmin_{x} f(x)$ that makes the function value of function $f : \mathbb{R}^{n} \to \mathbb{R}$ minimum is known as the Optimization Problem, and the algorithm to solve this problem is called an Optimization Technique. The given function $f$ in the optimization problem is specifically referred to as the Objective Function. $x^{ \ast }$ is called the Global Optimizer if for all $x$,</description></item><item><title>Ordinary Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1097/</link><pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1097/</guid><description>Definition1 For the univariate function $u(t)$, the following form is called an ordinary differential equation (ODE). $$ F(t, u(t), u^{\prime}(t), \dots, u^{(n)}(t)) = 0 \tag{1} $$ Here, $u^{\prime}$ is the derivative of $u$, and $u^{(n)}$ is the $n$-th order derivative of $u$, or simply referred to as $y = u(t)$, $$ F(t, y, y^{\prime}, \dots, y^{(n)}) = 0 $$ Explanation In $(1)$, $n$ is referred to as the order of</description></item><item><title>Probability Distributions under Conditional Probability in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1458/</link><pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1458/</guid><description>Definition1 For a discrete random vector $(X, Y)$, let $p_{X, Y}$ be the joint probability mass function of $(X, Y)$. Let $p_{X}$ be the marginal probability mass function of $X$. In this case, the following $p_{Y | X}$, given $Y = y$, is called the conditional probability mass function of $X$. $$ p_{Y | X} (y | x) = \dfrac{p_{X, Y}(x, y)}{p_{X}(x)} $$ For a continuous random vector $(X, Y)$,</description></item><item><title>Euler Integrals: Beta Function and Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/1483/</link><pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1483/</guid><description>Definition Euler Integrals The following two integrals are referred to as Euler integrals. $(a)$ Euler integral of the first kind: Beta function $$ B(p,q)=\int_{0}^1 t^{p-1}(1-t)^{q-1}dt,\quad p&amp;gt;0,\quad q&amp;gt;0 $$ $(b)$ Euler integral of the second kind: Gamma function $$ \Gamma (p) = \int_{0}^\infty t^{p-1}e^{-t}dt,\quad p&amp;gt;0 $$ Explanation Euler Integral of the First Kind 1-1. Beta Function: If the gamma function is considered a generalization of the factorial, then the beta function</description></item><item><title>Meta-Programming</title><link>https://freshrimpsushi.github.io/en/posts/1457/</link><pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1457/</guid><description>What is Metaprogramming? Metaprogramming, in simple terms, can be thought of as programming that allows a program to modify code. Rather than being a specific technique, metaprogramming refers to the entire concept wherein a program opens code written in another language and edits it as if modifying a &amp;lsquo;string&amp;rsquo;, or even when code modifications occur within the same language or by the program itself. Many of the programming languages developed</description></item><item><title>Representation of the Beta Function in the Form of an Improper Integral</title><link>https://freshrimpsushi.github.io/en/posts/1482/</link><pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1482/</guid><description>Theorem Beta Function: $$ B(p,q)=\int_{0}^{1}t^{p-1}(1-t)^{q-1}dt\quad \cdots (1) $$ The beta function can be expressed as an improper integral as follows: $$ B(p,q)=\int_{0}^{\infty}\frac{ t^{p-1} }{ (1+t)^{p+q}}dt\quad \cdots (2) $$ Explanation Using the above formula makes it easier to obtain difficult integral values. The proof is not difficult. Proof Let&amp;rsquo;s substitute $(1)$ with $t=\frac{x}{1+x}$. Then, $1-t=\frac{1}{1+x}$, and the range of integration changes to $\int_{0}^{1}\rightarrow \int_{0}^{\infty}$. Also, since $ \displaystyle \frac{ d t</description></item><item><title>Uploading and Downloading Files to and from a Server using SCP</title><link>https://freshrimpsushi.github.io/en/posts/1456/</link><pubDate>Sun, 19 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1456/</guid><description>Commands scp is probably an abbreviation for server copy. It&amp;rsquo;s a command used for uploading and downloading when using an ssh server. Pay attention to the spaces and the positions of @ and :. Let&amp;rsquo;s call the server account serverACC, the server address serverADD, the directory on the server where files will be uploaded or downloaded serverDIR, the file or directory I want to transfer Object, and the directory where</description></item><item><title>Relationship between Beta Function and Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/1481/</link><pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1481/</guid><description>Theorem $$ B(p,q) = {{\Gamma (p) \Gamma (q)} \over {\Gamma (p+q) }} $$ Explanation The Beta function is defined as $\displaystyle B(p,q) := \int_{0}^{1} t^{p-1} (1-t)^{q-1} dt $, and, like the Gamma function, it is an important function applied in many fields. Since the Gamma function can be easily calculated using the recursive relationship, the Beta function can also be calculated easily using the above relation. Intuitively, it can be</description></item><item><title>Transformation of Multivariate Random Variables</title><link>https://freshrimpsushi.github.io/en/posts/1455/</link><pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1455/</guid><description>Formulas The joint probability density function of a multivariate random variable $X = ( X_{1} , \cdots , X_{n} )$, given by $f$, is assumed to be as follows: $$ y_{1} = u_{1} (x_{1} , \cdots , x_{n}) \\ \vdots \\ y_{n} = u_{n} (x_{1} , \cdots , x_{n}) $$ Consider the following transformation $u_{1} , \cdots , u_{n}$, which might not be injective. Thus, the support $X$ of $S_{X}$</description></item><item><title>Various Important Formulas Involving the Gamma Function and Factorials</title><link>https://freshrimpsushi.github.io/en/posts/1478/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1478/</guid><description>Formulas $$ \Gamma (\frac{1}{2})=\sqrt{\pi} \tag{a} $$ - Euler&amp;rsquo;s Reflection Formula: $$ \Gamma (p)\Gamma (1-p)=\dfrac{\pi}{\sin(\pi p)} \tag{b} $$ $$ \Gamma (n+\frac{1}{2})=\frac{1\cdot 3\cdot \cdot5 \cdots (2n-1)}{2^{n}}\sqrt{\pi}=\frac{(2n-1)!!}{2^n}\sqrt{\pi}=\frac{(2n)!}{4^{n}n!}\sqrt{\pi},\quad n\in \mathbb{N} \tag{c} $$ $!!$ is the Double Factorial. - Binomial Coefficient: $$ \begin{pmatrix} n \\ k \end{pmatrix}=\frac{\Gamma (n+1)}{k! \Gamma (n-k+1)} \tag{d} $$ - Euler-Mascheroni Constant: $$ \gamma=-\Gamma^{\prime} (1) \tag{e} $$ - Beta Function: $$ B(p,q)=\frac{\Gamma (p) \Gamma (q)}{\Gamma (p+q)} \tag{f} $$ Proofs Gamma Function: $$</description></item><item><title>Julia's Powerful Convenience Features, Macros</title><link>https://freshrimpsushi.github.io/en/posts/1454/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1454/</guid><description>Overview Macros in Julia provide convenience features when coding, being executed in front of a scope. For example, if you want to know how much time your program is consuming, you can write it as follows. @time for t in 1:10 foo() bar() end Examples There are many types, but the following macros are especially widely used: @time: Measures the execution time of the function or scope that follows. When</description></item><item><title>Factorial, Double Factorial, and Multifactorial</title><link>https://freshrimpsushi.github.io/en/posts/1477/</link><pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1477/</guid><description>Factorial For a natural number $n$, $n!$ is read as $n$factorial and is defined as follows. $$ n!=n\cdot(n-1)\cdot(n-2)\cdots 2\cdot 1 =\prod\limits_{k=1}^n k $$ Description It is used in many places to neatly express equations. The factorial of $0$ is defined as $0!:=1$. By generalizing the domain of definition of factorial, one can also define something called a Gamma function. Double Factorial For a natural number $n$, $n!!$ is read as</description></item><item><title>How to Get a List of Files in a Folder in R</title><link>https://freshrimpsushi.github.io/en/posts/1451/</link><pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1451/</guid><description>Code setwd(&amp;#34;F:\\dsr\\project&amp;#34;) getwd() list.files(getwd()) list.files(getwd(),pattern=&amp;#34;*.csv&amp;#34;) list.files() is a function that is useful for compiling data divided into several files or for meta-programming: path: As the first argument, if you specify a directory, it returns a list of files in that folder. pattern: As the second argument, it accepts a rule as a regular expression and returns a list of files that meet the condition. In the example, a wildcard *</description></item><item><title>Derivation of the Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/1476/</link><pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1476/</guid><description>Non-negative Integers and the Gamma Function For $\alpha &amp;gt;0$, $$ \int_{0}^{\infty} e^{-\alpha x} dx=\left[-\frac{1}{\alpha}e^{-\alpha x}\right]_{0}^{\infty}=\frac{1}{\alpha} $$ Differentiating both sides with respect to $\alpha$, according to the Leibniz integral rule, allows the differentiation to move under the integration sign, thus giving $$ \begin{align*} &amp;amp;&amp;amp;\int_{0}^\infty -xe^{-\alpha x}dx&amp;amp;=-\frac{1}{\alpha^2} \\ \implies &amp;amp;&amp;amp; \int_{0}^\infty xe^{-\alpha x}dx &amp;amp;= \frac{1}{\alpha ^2} \end{align*} $$ Continuing to differentiate gives $$ \begin{align*} \int_{0}^\infty x^2e^{-\alpha x}dx&amp;amp;=\frac{2}{\alpha^3} \\ \int_{0}^\infty x^3e^{-\alpha x}dx&amp;amp;=\frac{3\cdot 2}{\alpha^4}</description></item><item><title>Leibniz Integral Rule</title><link>https://freshrimpsushi.github.io/en/posts/1475/</link><pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1475/</guid><description>Theorem Let&amp;rsquo;s assume that $f(x,t)$ and $\dfrac{\partial f}{\partial x}(x,t)$ are consecutive. Then, the following equation holds. $$ \frac{d}{dx} \int_{a}^b f(x,t)dt = \int_{a}^b\frac{\partial f}{\partial x}(x,t)dt $$ Description Being able to interchange the order of differentiation and integration is undoubtedly useful. Besides, there are many theorems or formulas related to differentiation and integration named after Leibniz. Proof Since if continuous, then integrable, let&amp;rsquo;s assume $u$ as follows. $$ u(x):=\int_{a}^b f(x,t)dt $$ Then,</description></item><item><title>How to Use Pipe Operators in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1450/</link><pubDate>Sat, 11 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1450/</guid><description>Overview Julia supports the pipeline operator, highlighting its strength in handling data. Code julia&amp;gt; (1:5) .|&amp;gt; (x -&amp;gt; sqrt(x+2)) .|&amp;gt; sin |&amp;gt; minimum 0.4757718381527513 julia&amp;gt; minimum(sin.((x -&amp;gt; sqrt(x+2)).(1:5))) 0.4757718381527513 The example code above puts the array $[1,2,3,4,5]$ into $\sqrt{x + 2}$, and then puts the result into $\sin$ to obtain the smallest value. The code above and below produces exactly the same results. It goes without saying how useful the</description></item><item><title>Multivariate Probability Distributions in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1449/</link><pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1449/</guid><description>Definition 1 A Random Vector is defined as $X = (X_{1} , \cdots , X_{n})$ for $n$ number of probability variables $X_{i}$ defined in sample space $\Omega$. The range $X(\Omega)$ of $X$ is also called a space. A function that satisfies the following $F_{X} : \mathbb{R}^{n} \to [0,1]$ is called the Joint Cumulative Distribution Function of $X$. $$ F_{X}\left( x_{1}, \cdots , x_{n} \right) := P \left[ X_{1} \le x_{1}</description></item><item><title>Properties of the Interior in Topological Spaces and Subspaces</title><link>https://freshrimpsushi.github.io/en/posts/1473/</link><pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1473/</guid><description>Theorem Given a topological space $(X,\mathcal{T})$ and a subset $A,B,A_{\alpha}\subset X\ (\alpha \in \Lambda)$, then: $(a1)$: If $A\subset B$, then $A^{\circ} \subset B^{\circ}$. $(b1)$: $A^{\circ}\cup B^{\circ} \subset (A\cup B)^{\circ}$ $(c1)$: $A^{\circ} \cap B^{\circ} = (A\cap B)^{\circ}$ $(d1)$: $(\cap_{\alpha\in\Lambda}A_{\alpha})^{\circ} \subset \cap _{\alpha \in \Lambda} A_{\alpha}^{\circ}$ Interior of Subspace Even if it&amp;rsquo;s the same set, depending on how the whole space is given, it may or may not become an open set.</description></item><item><title>Lambda Expressions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1448/</link><pubDate>Tue, 07 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1448/</guid><description>Overview In Julia, lambdas are defined as follows: (x -&amp;gt; 3x^2 - 2x + 3)(1) This corresponds to defining the anonymous function $\lambda : \mathbb{Z} \to \mathbb{Z}$, substituting $1$ into it, and obtaining the function value $4$. $$ \lambda : x \mapsto ( 3 x^{2} - 2 x + 3 ) \\ \lambda (1) = 4 $$ Indeed, lambda expressions themselves are not a Julia-specific feature but almost naturally supported,</description></item><item><title>Generating Topology from a Basis</title><link>https://freshrimpsushi.github.io/en/posts/1470/</link><pubDate>Mon, 06 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1470/</guid><description>Buildup Topology For a set $X$, a collection $\mathscr{T}$ of subsets of $X$ that satisfies the following three conditions is called the topology on set $X$. $(T1)$ $\varnothing, X \in \mathscr{T}$ If $(T2)$ $U_{\alpha} \in \mathscr{T} (\alpha \in \Lambda)$, then $\bigcup_{\alpha \in \Lambda} U_{\alpha} \in \mathscr{T}$. If $(T3)$ $U_{1},\cdots,U_{n} \in \mathscr{T}$, then $\bigcap_{i=1}^{n}U_{i} \in \mathscr{T}$. In simple terms, a collection of subsets that contains the empty set and the whole</description></item><item><title>Don'sker's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1447/</link><pubDate>Sun, 05 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1447/</guid><description>Theorem Let&amp;rsquo;s say $\left\{ \xi_i \right\}_{i \in \mathbb{N}}$ is a probability process defined in $(0,1)$. Suppose in the function space $C[0,1]$, the probability function $X_{n}$ is defined as follows: $$ X_{n}:= {{ 1 } \over { \sqrt{n} }} \sum_{i=1}^{\lfloor nt \rfloor} \xi_{i} + \left( nt - \lfloor nt \rfloor \right) {{ 1 } \over { \sqrt{n} }} \xi_{\lfloor nt \rfloor + 1} $$ $X_{n}$ converges in distribution to the Wiener</description></item><item><title>Proof of the Expected Form of Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/266/</link><pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/266/</guid><description>Theorem 1 In the open interval $I$, if the function $\phi$ is convex and twice differentiable, the expected value $X$ of the random variable exists, and $X \subset I $ then $$ \phi [ E(X) ] \le E [ \phi (X)] $$ Other Forms Jensen&amp;rsquo;s Inequality in Finite Form Jensen&amp;rsquo;s Inequality in Integral Form Conditional Jensen&amp;rsquo;s Inequality It has a form quite similar to the integral form. Upon closer inspection,</description></item><item><title>How to Change Image Size in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1466/</link><pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1466/</guid><description>Resizing Images To resize images, you can use the imresize function from the Images package. The function name is the same as in Matlab. imresize(X, ratio=a): Returns the image of array X scaled by a factor of a. Unlike Matlab, you must explicitly write ratio=a. imresize(X, m, n): Returns the image of array X resized to m rows and n columns. Below are example codes and their results. using Images</description></item><item><title>How to Load Images in Julia and Save Them as Matrices</title><link>https://freshrimpsushi.github.io/en/posts/1446/</link><pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1446/</guid><description>Code using Images cd(&amp;#34;C:/Users/rmsms/OneDrive/examples&amp;#34;) pwd() example = load(&amp;#34;example.jpg&amp;#34;) typeof(example) size(example) gray1 = Gray.(example) typeof(gray1) size(gray1) M = convert(Array{Float64},gray1) typeof(M) size(M) colorview(Gray, M.^(1/2)) save(&amp;#34;rgb.png&amp;#34;, colorview(RGB, example)) save(&amp;#34;gray1.png&amp;#34;, colorview(Gray, gray1)) save(&amp;#34;gray2.png&amp;#34;, colorview(Gray, transpose(gray1))) save(&amp;#34;gray3.png&amp;#34;, colorview(Gray, M.^(1/2))) Let&amp;rsquo;s briefly understand the example code from top to bottom: cd() : Change Directory, changes the working directory to the desired location. pwd() : Print Working Directory, prints the working directory. If you want to follow</description></item><item><title>Measuring Code Execution Time in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1467/</link><pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1467/</guid><description>Methods tic X1=rand(2^7); X2=rand(2^8); X3=rand(2^9); X4=rand(2^10); X5=rand(2^11); toc Y1=imrotate(X1,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y2=imrotate(X2,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y3=imrotate(X3,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y4=imrotate(X4,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y5=imrotate(X5,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc tic: Starts a stopwatch for measuring execution time. toc: Returns the current time on the stopwatch. Note that it&amp;rsquo;s not measuring the time between toc and toc. To measure the computation time of calculating Y1~Y6 in the example code above, you should enter the code as follows. tic X1=rand(2^7); X2=rand(2^8); X3=rand(2^9); X4=rand(2^10);</description></item><item><title>Chebyshev's Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/34/</link><pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/34/</guid><description>Theorem 1 If the variance $\sigma^2 &amp;lt; \infty$ of a random variable $X$ exists for some positive number $k&amp;gt;0$, then $$ P(|X-\mu| \ge k\sigma) \le {1 \over k^2} $$ Explanation It is relatively simple in form and easy to manipulate, and the results are immediately apparent, making it widely used as a lemma. However, compared to Markov&amp;rsquo;s inequality, there is one more condition that the variance must exist. One might</description></item><item><title>How to Rotate Image Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1462/</link><pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1462/</guid><description>Image Rotation imrotate(X, theta): Rotates array X by theta radians. Note that, unlike in MATLAB where the angle unit is degrees ($^{\circ})$, the angle unit here is radians. Additionally, unlike MATLAB, it rotates clockwise. If no other variables are inputted, the interpolation method defaults to bilinear, and the rotated image is not cropped. Examples of rotating the original image X by $90^\circ=\pi/2$, $180^\circ=\pi$, and $270^\circ=\frac{3}{2}\pi$, along with their results, are</description></item><item><title>Resizing Images in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1465/</link><pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1465/</guid><description>Methods imresize(A, scale): Returns a new image by adjusting the size of A by a factor of scale. If A is a 10x10 image and a scale of 0.5 is input, it returns a 5x5 image. You can also adjust the size directly as follows. imresize(A, [m n]): Returns an image with m rows and n columns. Below are example codes and their results. X=imread(&amp;#39;test\_{i}mage.jpg&amp;#39;); figure() imshow(X) saveas(gcf,&amp;#39;X.png&amp;#39;) title(&amp;#39;X&amp;#39;) Y1=imresize(X,0.5);</description></item><item><title>Building an SSH Server on Windows</title><link>https://freshrimpsushi.github.io/en/posts/1445/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1445/</guid><description>Overview Windows now provides many Linux-specific convenience features including PowerShell with the release of version 10. The ssh server, for instance, can be very easily installed through the GUI. Guide Step 1. Apps and Features Press Win+S to search for Add or remove programs or Apps and features, then click on Optional features. Step 2. Install OpenSSH Server Click on Add a feature and install OpenSSH Server. Step 3. PowerShell</description></item><item><title>Functions for 2D Array Operations in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1460/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1460/</guid><description>Let&amp;rsquo;s say $A = \begin{pmatrix} 1 &amp;amp; 2 &amp;amp; 1 \\ 0 &amp;amp; 3 &amp;amp; 0 \\ 2 &amp;amp; 3 &amp;amp; 4\end{pmatrix}$. Transpose Matrix julia&amp;gt; A =[1 2 1; 0 3 0; 2 3 4] 3×3 Array{Int64,2}: 1 2 1 0 3 0 2 3 4 julia&amp;gt; transpose(A) 3×3 LinearAlgebra.Transpose{Int64,Array{Int64,2}}: 1 0 2 2 3 3 1 0 4 julia&amp;gt; A&amp;#39; 3×3 LinearAlgebra.Adjoint{Int64,Array{Int64,2}}: 1</description></item><item><title>How to output and save arrays as heatmap images in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1459/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1459/</guid><description>Heatmap Using the heatmap function from the Plots package, you can output a 2D array as a heatmap image, and with the savefig function, you can save the resulting image. The @__DIR__ macro tells you the location of the Julia code file. # code1 However, if you compare array A with the heatmap image, you may notice that the top and bottom of the array are flipped in the heatmap</description></item><item><title>Differences between is and == in Python</title><link>https://freshrimpsushi.github.io/en/posts/1444/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1444/</guid><description>Code if type(150421) is int : print(&amp;#34;!&amp;#34;) else : print(&amp;#34;?&amp;#34;) x = [1,2] y = [1,2] x == y x is y Description When looking at Python code on GitHub, sometimes you see is. Apart from the fact that the code reads smoothly like a sentence, there are clear differences from == and it&amp;rsquo;s good to use it appropriately: == simply compares values. It&amp;rsquo;s intuitive because it compares the appearance</description></item><item><title>Markov Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/33/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/33/</guid><description>Theorem 1 Let&amp;rsquo;s define a function $u(X) \ge 0$ for the random variable $X$. If $E \left( u(X) \right)$ exists, then for $c &amp;gt; 0$ $$ P(u(X) \ge c) \le {E \left( u(X) \right) \over c} $$ Explanation There is the Chebyshev&amp;rsquo;s inequality, which makes it more convenient to use as an auxiliary lemma in numerous proofs. You might consider the condition that the $1$th moment must exist as too</description></item><item><title>If an nth Moment Exists, Moments of Lower Orders than n Also Exist</title><link>https://freshrimpsushi.github.io/en/posts/247/</link><pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/247/</guid><description>Theorem If there exists $E( X^n )$ for a random variable $X$ and a natural number $n$, then $E( X^m ), m=1,2,3,\cdots, n$ also exists. Description Regardless of the degree, if a certain moment exists, moments of lower degrees always exist, although naturally the converse is not true. Of course, in practice, it is rare for higher-order moments to be given first, but this theorem does save a significant amount</description></item><item><title>Tight Probability Processes</title><link>https://freshrimpsushi.github.io/en/posts/1443/</link><pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1443/</guid><description>Definition Let us say in a probability space $( \Omega , \mathcal{F} , P)$, a stochastic process $\left\{ X_n \right\}_{n \in \mathbb{N}}$ is defined. If for every $\varepsilon &amp;gt; 0$, there exists a compact set $K \subset \Omega$ such that $$\displaystyle \inf_{n \in \mathbb{N}} P\left( X_{n} \in K \right) &amp;gt; 1 - \varepsilon$$ is satisfied, then $\left\{ X_{n} \right\}$ is said to be tight. Explanation In mathematical statistics, it corresponds</description></item><item><title>Sets and Operators in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1442/</link><pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1442/</guid><description>Overview Julia, like Python, supports the set data type. As with any set data type, it is incredibly useful for those who use it and utterly ignored by those who don&amp;rsquo;t. Given that Julia&amp;rsquo;s design is closely aligned with mathematics, its implementation of set concepts and operations is robust, making it an important feature to understand. Perhaps the most distinct difference from other languages, especially Python, is the ability to</description></item><item><title>What is the Moment Generating Function?</title><link>https://freshrimpsushi.github.io/en/posts/248/</link><pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/248/</guid><description>Definition 1 For a random variable $X$ and some positive number $h&amp;gt;0$, if $E(e^{tX})$ exists in $-h&amp;lt; t &amp;lt; h$, then $M(t) = E( e^{tX} )$ is defined as the Moment Generating Function of $X$. Explanation The moment generating function (mgf) is a concept often encountered relatively early in mathematical statistics, yet its unfamiliar definition and seemingly contextless introduction can make it a source of dislike for the subject. The</description></item><item><title>How to Swap the Values of Two Variables in Python</title><link>https://freshrimpsushi.github.io/en/posts/1441/</link><pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1441/</guid><description>Code Swapping variables is commonly implemented by creating a temporary variable and moving values accordingly. However, as someone dealing with multiple programming languages, it&amp;rsquo;s hard to be sure if this method works well in Python due to its characteristic of exchanging pointers when binding variables. Moreover, writing a function to swap variables for each case can be tedious. Let&amp;rsquo;s solve this easily with Python&amp;rsquo;s syntax itself. x = 3 y</description></item><item><title>Kurtosis in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1271/</link><pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1271/</guid><description>Kurtosis Given a random variable $X$ with mean $\mu$ and variance $\sigma^2$, the kurtosis of $X$ is defined as follows: $$ \gamma_{2} := {{ E \left( X - \mu \right)^4 } \over { \sigma^4 }} $$ For data $\left\{ X_{i} \right\}_{i}^{n}$, with sample mean $\overline{X}$ and sample variance $\widehat{\sigma}^2$, sample kurtosis $g_{2}$ is obtained as follows: $$ g_{2} := \sum_{i=1}^{n} {{ \left( X - \overline{X} \right)^4 } \over { n</description></item><item><title>Skewness in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1268/</link><pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1268/</guid><description>Definition When the mean of a random variable $X$ is $\mu$, and its variance is $\sigma^2$, the following defined $\gamma_{1}$ is called the Skewness of $X$. $$ \gamma_{1} := {{ E \left( X - \mu \right)^3 } \over { \sigma^3 }} $$ When the sample mean of data $\left\{ X_{i} \right\}_{i}^{n}$ is $\overline{X}$, and the sample variance is $\widehat{\sigma}^2$, the sample skewness $g_{1}$ is calculated as follows. $$ g_{1} :=</description></item><item><title>Pearson Correlation Coefficient</title><link>https://freshrimpsushi.github.io/en/posts/57/</link><pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/57/</guid><description>Definition 1 For two random variables $X, Y$, the term $\rho = \rho (X,Y)$ defined as follows is called the Pearson correlation coefficient. $$ \rho = { {\operatorname{Cov} (X,Y)} \over {\sigma_X \sigma_Y} } $$ $\sigma_{X}$ and $\sigma_{Y}$ are the standard deviations of $X$ and $Y$, respectively. Explanation The (Pearson) Correlation coefficient is a measure used to determine whether two variables have a (linear) correlation. If it is close to $1$</description></item><item><title>Slicing and Indexing of Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1437/</link><pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1437/</guid><description>Overview Julia is a language that mixes the advantages of R, Python, and Matlab. Arrays are fundamental to programming, and their usage reveals traces of these languages. Code Matrix julia&amp;gt; M = [1. 2. ; 3. 4.] 2×2 Array{Float64,2}: 1.0 2.0 3.0 4.0 julia&amp;gt; size(M) (2, 2) julia&amp;gt; length(M) 4 For matrices, the syntax is defined and used almost exactly like Matlab. The size() function is used just</description></item><item><title>Various Properties of Covariance</title><link>https://freshrimpsushi.github.io/en/posts/425/</link><pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/425/</guid><description>Definitions and Properties The covariance of probability variables $X$ and $Y$, whose means are $\mu_{X}$ and $\mu_{Y}$ respectively, is defined as $\operatorname{Cov} (X ,Y) : = E \left[ ( X - \mu_{X} ) ( Y - \mu_{Y} ) \right]$. Covariance has the following properties: [1]: $\Var (X) = \operatorname{Cov} (X,X)$ [2]: $\operatorname{Cov} (X,Y) = \operatorname{Cov} (Y, X)$ [3]: $\Var (X + Y) = \Var (X) + \Var (Y) + 2</description></item><item><title>Precompact Stochastic Process</title><link>https://freshrimpsushi.github.io/en/posts/1436/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1436/</guid><description>Theorem Let&amp;rsquo;s define a function space consisting of continuous functions going from measurable space $(S, \mathcal{S})$ to $(S ', \mathcal{S} ')$ as $\mathscr{H}:= C \left( S,S&amp;rsquo; \right)$, and say that $\left\{ h^{-1}(A&amp;rsquo;): h \in \mathscr{H} , A ' \in \mathcal{S} ' \right\}$ is a separating class of $(S , \mathcal{S})$. Here, $X$ is a probability element defined in $S$, and $\left\{ X_n \right\}_{n \in \mathbb{N}}$ is a stochastic process defined</description></item><item><title>Properties of Mean and Variance</title><link>https://freshrimpsushi.github.io/en/posts/424/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/424/</guid><description>Theorem The mean $E ( X ) = \mu_{X}$ and variance $\Var (X) = E [ ( X - \mu_{X} )^2 ]$ have the following properties: [1]: $E(X + Y) = E(X) + E(Y)$ [2]: $E(aX + b) = a E(X) + b$ [3]: $\Var (X) \ge 0$ [4]: $\Var ( X ) = E(X^2) - \mu_{X}^2$ [5]: $\Var (aX + b) = a^2 \Var (X)$ Explanation As they relate</description></item><item><title>Mathematical Proof of the Properties of Representative Values</title><link>https://freshrimpsushi.github.io/en/posts/49/</link><pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/49/</guid><description>Theorem Let&amp;rsquo;s assume that we have given data $X = \left\{ x_{1} , \cdots , x_{n} \right\}$. [0]: The $\theta$ that minimizes $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{0}$ is $$ \argmin_{\theta} h \left( \theta \right) = \text{mode}(X) $$ [1]: The $\theta$ that minimizes $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{1}$ is $$ \argmin_{\theta} h \left( \theta \right) = \text{median}(X) $$ [2]: The $\theta$ that minimizes $\displaystyle h(\theta)=\sum_{i=1}^{n} {|x_i - \theta|}^{2}$ is $$ \argmin_{\theta}</description></item><item><title>Expectation, Mean, Variance, and Moments in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/246/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/246/</guid><description>Definition: Expectation, Mean, and Variance Let&amp;rsquo;s assume that we have a given random variable $X$. If the probability density function $f(x)$ of a continuous random variable $X$ satisfies $\displaystyle \int_{-\infty}^{\infty} |x| f(x) dx &amp;lt; \infty$, then $E(X)$, defined as follows, is called the Expectation of $X$. $$ E(X) := \int_{-\infty}^{\infty} x f(x) dx $$ If the probability mass function $p(x)$ of a discrete random variable $X$ satisfies $\displaystyle \sum_{x} |x|</description></item><item><title>Greedy Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/1434/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1434/</guid><description>Definition Greedy algorithm is a method of making choices that only considers the current moment and selects the best possible option. Description As its name suggests, the greedy algorithm focuses on the immediate without taking a long-term perspective. Speaking positively, it&amp;rsquo;s always trying to do its best, but this may not always be wise when looking at the bigger picture. Consider the following example: Let&amp;rsquo;s say there&amp;rsquo;s a problem of</description></item><item><title>Probability Variables and Probability Distribution in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1433/</link><pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1433/</guid><description>Definition 1 Let us assume that probability $P$ is defined in the sample space $\Omega$. A function $X : \Omega \to \mathbb{R}$ whose domain is the sample space is called a Random Variable. The range $X(\Omega)$ of a random variable is also called its Space. A function $F_{X} : \mathbb{R} \to [0,1]$ that satisfies the following is called the Cumulative Distribution Function (cdf) of $X$. $$ F_{X}(x) = P_{X}\left( (-\infty,x]</description></item><item><title>Convergence of Distributions Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1432/</link><pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1432/</guid><description>Definition Let&amp;rsquo;s define a measurable space $(S,\mathcal{S})$ with respect to the Borel sigma field $\mathcal{S}:= \mathcal{B}(S)$ of a metric space $S$. When random variables $X$ and stochastic processes $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$ defined in a probability space $(\Omega, \mathcal{F}, P)$ are $n \to \infty$, for all $f \in C_{b}(S)$, if the following is satisfied, then it is said to Converge in Distribution $X$ and is denoted as $X_{n} \overset{D}{\to}</description></item><item><title>Probability and the Addition Law of Probability in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1431/</link><pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1431/</guid><description>Definition 1 An experiment that can be repeated under the same conditions is referred to as a Random Experiment. The set $\Omega$ of all possible outcomes that can be obtained from a random experiment is called the Sample Space. The set of outcomes in the sample space that we are interested in, i.e., $B \subset \Omega$ is called an Event, and these sets are represented as $\mathcal{B}$. A function $P</description></item><item><title>Proof of the Mixing Theorem in Probability Theory</title><link>https://freshrimpsushi.github.io/en/posts/1430/</link><pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1430/</guid><description>Theorem Let the space $S$ be both a metric space $( S , \rho)$ and a measurable space $(S,\mathcal{B}(S))$. The following are all equivalent: (1): $P_{n} \overset{W}{\to} P$ (2): For every bounded, uniformly continuous function $f$, there exists $\displaystyle \int_{S} f dP_{n} \to \int_{S}f d P$ (3): For every closed set $F$, there exists $\displaystyle \limsup_{n\to\infty} P_{n}(F) \le P(F)$ (4): For every open set $G$, there exists $\displaystyle P(G) \le</description></item><item><title>Projection Mapping in Stochastic Processes</title><link>https://freshrimpsushi.github.io/en/posts/1429/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1429/</guid><description>Definition Let $S$ be a space that is both a metric space $( S , \rho)$ and a measurable space $(S,\mathcal{B}(S))$, and consider $k \in \mathbb{N}$. Discrete Projection Mapping: For (discrete time) $N = \left\{ n \in \mathbb{N}: n \le \xi, \xi \in [0,\infty] \right\}\subset \mathbb{N}$ and an element $x:= (x_{1} , x_{2} , \cdots )$ of $\displaystyle S^{\sup N}:= \prod_{n \in N} S$ in $S$, the following defined $\pi_{k}:</description></item><item><title>Probability Measures Defined on Polish Spaces are Tight</title><link>https://freshrimpsushi.github.io/en/posts/1428/</link><pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1428/</guid><description>Theorem Let&amp;rsquo;s say that a metric space $(S,\rho)$ is a Polish space. Then, all probability measures defined in $S$ are tight. Explanation A Polish space refers to a separable complete metric space. The reason we discuss the tightness of probability measures is precisely that, under these conditions, most probabilities are tight. This, conversely, implies the need to study probabilities defined in non-Polish spaces. Proof Strategy: We need to bring in</description></item><item><title>Polish Space</title><link>https://freshrimpsushi.github.io/en/posts/1427/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1427/</guid><description>Definition A topological space $X$ is called a Polish space if it satisfies the following conditions: (i): $X$ is a metrizable space. (ii): $X$ is a separable space. (iii): $X$ is a complete space. Explanation The term Polish Space directly comes from &amp;lsquo;Poland&amp;rsquo;, as you might guess from the naturalized expression. The reason behind this naming is that this concept was first actively researched by topologists and logicians from Poland.</description></item><item><title>Translating Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1453/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1453/</guid><description>Description Using circshifr(A, (n,m)), you can shift the rows of the array A $n$ positions down, and the columns $m$ positions to the right. (n,m) must be a tuple of integers, and negative numbers are also possible. If negative, it shifts in the opposite direction. For arrays of 3 dimensions or more, it is applied to each smallest 2-dimensional array respectively. Code 2D array julia&amp;gt; A = transpose(reshape(1:25,5,5)) 5×</description></item><item><title>Various Methods of Creating Vectors in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1452/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1452/</guid><description>Code julia&amp;gt; x1=[1 2 3] 1×3 Array{Int64,2}: 1 2 3 julia&amp;gt; x2=[1, 2, 3] 3-element Array{Int64,1}: 1 2 3 julia&amp;gt; x3=[i for i in 1:3] 3-element Array{Int64,1}: 1 2 3 julia&amp;gt; x4=[i for i in 1:3:10] 4-element Array{Int64,1}: 1 4 7 10 julia&amp;gt; x5=[i for i in 1:3:11] 4-element Array{Int64,1}: 1 4 7 10 x1 is a 2-dimensional array. Since it looks like a row vector, if you</description></item><item><title>Continuum Hypothesis</title><link>https://freshrimpsushi.github.io/en/posts/1426/</link><pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1426/</guid><description>Hypothesis Continuum Hypothesis: There does not exist a cardinal number $x$ that satisfies $\aleph_{0} &amp;lt; x &amp;lt; 2^{\aleph_{0}}$ for given $\aleph_{0} = |\mathbb{N}|$. Generalized Continuum Hypothesis: For any infinite cardinal number $a = |A|$, there does not exist a cardinal number $x$ that satisfies $a &amp;lt; x &amp;lt; 2^{a}$. Explanation Cantor demonstrated that not all infinities are equal using methods similar to the diagonal argument. He showed that even though</description></item><item><title>Russell's Paradox</title><link>https://freshrimpsushi.github.io/en/posts/1423/</link><pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1423/</guid><description>Paradox 1 If there exists a set of all sets $\mathscr{U}$, then there exists some set $R$ that is both a member of $\mathscr{U}$ and not a member of itself. Description In the 6th century BC, the philosopher from Crete, Epimenides, declared: &amp;ldquo;All Cretans are liars!&amp;rdquo; If Epimenides&amp;rsquo; statement is true, then, being a Cretan himself, his statement would be false. However, if his statement is false, then Epimenides is</description></item><item><title>부분순서 집합</title><link>https://freshrimpsushi.github.io/en/posts/1421/</link><pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1421/</guid><description>Definition 1 A relation $\le$ on a set $A$ is called a partial order if it is reflexive, transitive, and antisymmetric, and $(A,\le)$ is referred to as a partially ordered set. In other words, that $A$ is a partially ordered set means that for all elements $a,b,c \in A$, the following holds: $$ \begin{align*} a \le a &amp;amp; \text{ (reflexivity) } \\ a \le b \land b \le c \implies</description></item><item><title>Completely Bounded Space</title><link>https://freshrimpsushi.github.io/en/posts/1420/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1420/</guid><description>Definition 1 Given a metric space $(X,d)$ and $\varepsilon&amp;gt;0$, A finite set $A_{\varepsilon} \subset X$ that satisfies $B_{d}(x,\varepsilon) \cap A_{\varepsilon} \ne \emptyset$ for all $x \in X$ is called a $\varepsilon$-net for $X$. If for all $\varepsilon &amp;gt; 0$, there exists a $\varepsilon$-net $A_{\varepsilon}$ for $X$, then $X$ is said to be Totally Bounded. Explanation Totally bounded spaces are often also called precompact spaces. $\varepsilon$-Net Calling $A_{\varepsilon}$ a net is</description></item><item><title>Tight Probability Measures</title><link>https://freshrimpsushi.github.io/en/posts/1417/</link><pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1417/</guid><description>Definition Let the space $S$ be a metric space $( S , \rho)$ and a measurable space $(S,\mathcal{B}(S))$. Let $P$ be a probability measure defined on $S$. It is said to be tight if for all $\varepsilon &amp;gt; 0$, there exists a compact set $K$ such that $P(K) &amp;gt; 1 - \varepsilon$ is satisfied. Explanation Generally, in undergraduate level probability, one rarely encounters a probability measure that is not tight.</description></item><item><title>Comparison of the Cardinality of Real Numbers and the Cardinality of Rational Numbers</title><link>https://freshrimpsushi.github.io/en/posts/110/</link><pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/110/</guid><description>Theorem 1 Regarding $\operatorname{card}(\mathbb{Q})={{ \aleph }_{ 0 }}, \operatorname{card}(\mathbb{R})=c$, $$ { 2 }^{ {{ \aleph }_{ 0 }} } =c \\ {{ \aleph }_{ 0 }}&amp;lt;c $$ Explanation As you might guess from seeing Cantor&amp;rsquo;s diagonal argument, the set of real numbers has far more elements than the set of rational numbers. The cardinality of these can be explicitly shown by establishing an inequality. Proof Part 1. $c \le 2^{\aleph_{0}}$</description></item><item><title>Installing and Using Packages in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1416/</link><pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1416/</guid><description>Method 1 using LinearAlgebra using Pkg Pkg.add(&amp;#34;Plots&amp;#34;) Pkg.add(&amp;#34;Distributions&amp;#34;) using Plots The above code demonstrates importing the LinearAlgebra and Pkg packages and installing the Plots, Distribution packages using the .add() function. The keyword using to import packages is somewhat reminiscent of the language used in mathematics when applying a theorem or argument. Installing packages is more akin to R than Python, and its usage closely resembles that of Python. Similar to</description></item><item><title>Conditions for Two Probability Measures to Coincide</title><link>https://freshrimpsushi.github.io/en/posts/1415/</link><pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1415/</guid><description>Theorem Let space $S$ be a metric space $( S , \rho)$ and a measurable space $(S,\mathcal{B}(S))$. $\mathcal{O}$ is the set of all open sets, $\mathcal{C}$ is the set of all closed sets, and $P$ and $Q$ are probability measures defined in $(S,\mathcal{B}(S))$. [1]: For every open set $O \in \mathcal{O} \subset S$, if $P(O) = Q(O)$ then $P=Q$. In other words, $\mathcal{O}$ is a separating class. [2]: For every</description></item><item><title>Resolving Warning in install.packages lib = C:\Program Files\R\R-3.6.1\library is not writable during R Package Installation</title><link>https://freshrimpsushi.github.io/en/posts/1414/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1414/</guid><description>Overview This documentation is tailored to users who are completely new to R, to the point of not even being familiar with computers, let alone programming, and are in urgent need to use R. Therefore, the explanations may be overly detailed. WARNING: Rtools is required to build R packages but is not currently installed. Please download and install the appropriate version of Rtools before proceeding: https://cran.rstudio.com/bin/windows/Rtools/ If you get a</description></item><item><title>Subspace Topology, Relative Topology</title><link>https://freshrimpsushi.github.io/en/posts/1439/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1439/</guid><description>Definition 1 Let&amp;rsquo;s assume that a topological space $(X,\mathscr{T})$ and a subset $A \subset X$ are given. Then, the following set $$ \mathscr{T}_{A} =\left\{ A\cap U\ :\ U\in \mathscr{T} \right\} $$ is a topology on $A$. In this case, $\mathscr{T}_{A}$ is referred to as the Subspace Topology or Relative Topology. Moreover, the topological space $(A, \mathscr{T}_{A})$ is called the Subspace of $(X,\mathscr{T})$. Theorem [0]: For a topological space $(X, \mathscr{T}$)</description></item><item><title>In Probability Theory: Separating Classes</title><link>https://freshrimpsushi.github.io/en/posts/1413/</link><pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1413/</guid><description>Theorem A Separating Class in a measurable space $(S, \mathcal{B}(S))$ defined for two probabilities $P$, $Q$ is said to satisfy the following $\mathcal{C}$. $$ P(A) = Q(A), \forall A \in \mathcal{C} \implies P(A) = Q(A), \forall A \in \mathcal{B}(S) $$ Explanation The existence of a separating class implies that to check if two measures are the same, one does not need to examine the entire measurable space but only a</description></item><item><title>Proof of Cantor's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/108/</link><pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/108/</guid><description>Theorem 1 For an arbitrary set $X$ and its power set $\mathscr{P} (X)$, $$ \operatorname{card}(X)&amp;lt;\operatorname{card}(\mathscr{P} (X)) $$ 이흥천 역, You-Feng Lin. (2011). 집합론(Set Theory: An Intuitive Approach): p251.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Proof of the Cantor-Bernstein Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1412/</link><pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1412/</guid><description>Theorem 1 A set $A$, $B$ is equivalent to a subset of $B$ being equivalent to a subset of $A$ if and only if $A$ and $B$ are equivalent. Two sets are equivalent if and only if there exists a bijective function between them. Proof Strategy: We attempt to express taking function $f$ on $x$ $k$ times as $f^{k}(x)$. For all $k \in \mathbb{N}$, it should be representable as $f^{k}(x)</description></item><item><title>Weak Convergence of Measures</title><link>https://freshrimpsushi.github.io/en/posts/1410/</link><pubDate>Tue, 03 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1410/</guid><description>Definition Let space $S$ be both a metric space $( S , \rho)$ and a measurable space $(S,\mathcal{B}(S))$. Measure Theory When a measure $\mu$ defined on $S$ and a sequence of measures $\left\{ \mu_n \right\}_{n \in \mathbb{N}}$ is $n \to \infty$, it is said to converge weakly to measure $\mu$ if it satisfies the following for all $f \in C_{b}(S)$: $$ \int_{S} f d\mu_{n} \to \int_{S} f d\mu $$ It</description></item><item><title>Merging Rows and Columns in numpy array in Python</title><link>https://freshrimpsushi.github.io/en/posts/1409/</link><pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1409/</guid><description>Code import numpy as np a = np.array([[1,2,3]]) b = np.array([[4,5,6]]) print(a) print(b) print(np.c_[a,b]) print(np.r_[a,b]) Python&amp;rsquo;s numpy package offers many convenient features. As seen in the following screenshot, objects numpy.c_ and numpy.r_ merge arrays contained within brackets [] as columns and rows, respectively. It&amp;rsquo;s important to clarify that these are not methods. They may be used like methods, but they are simply arrays that have already been merged using brackets</description></item><item><title>Proof of Lévy's Theorem in Probability Theory</title><link>https://freshrimpsushi.github.io/en/posts/1406/</link><pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1406/</guid><description>Theorem Let&amp;rsquo;s assume that a probability space $( \Omega , \mathcal{F} , P)$ is given. If $\eta$ is an integrable random variable and $\left\{ \mathcal{F}_{n} \right\}_{n \in \mathbb{N}}$ is a sequence of sigma fields where $\left\{ \mathcal{F}_{n} \right\}_{n \in \mathbb{N}}$ is $\mathcal{F}_{n} \subset \mathcal{F}_{n+1}$, then $n \to \infty$ when $$ E \left( \eta | \mathcal{F}_{n} \right) \to E \left( \eta | \mathcal{F}_{\infty} \right) $$ $\displaystyle \mathcal{F}_{\infty} = \bigotimes_{n=1}^{\infty} \mathcal{F}_{n}$ does</description></item><item><title>Dinkin's Pi-Lambda Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1405/</link><pubDate>Sat, 30 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1405/</guid><description>Theorem If the pi system $\mathcal{P}$ is a subset of the lambda system $\mathcal{L}$, then there exists a sigma field $\sigma ( \mathcal{P} )$ that satisfies $\mathcal{P} \subset \sigma ( \mathcal{P} ) \subset \mathcal{L}$. $\sigma ( \mathcal{P} )$ represents the smallest sigma field that contains all elements of $\mathcal{P}$. Explanation At first glance, the statement might look rather simple, but as with such theorems, its proof is quite long and</description></item><item><title>Reading Large CSV Files in Python at Once</title><link>https://freshrimpsushi.github.io/en/posts/1403/</link><pubDate>Fri, 29 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1403/</guid><description>Code y_test=[] y_csv = open(&amp;#39;y_test.csv&amp;#39;, &amp;#39;r&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) rdr = csv.reader(y_csv) for line in rdr: y_test.append(line[0]) y_csv.close() Normally, when reading a csv file, one would open it using the built-in Python function open and process it line by line, as shown above. However, this approach is not well-suited for handling big data as soon as loops are introduced. For instance, a file exceeding 700MB might not seem very large, but reading</description></item><item><title>Pi System and Lambda System</title><link>https://freshrimpsushi.github.io/en/posts/1402/</link><pubDate>Thu, 28 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1402/</guid><description>Definitions An entity satisfying $\mathcal{P}$ is called a $\pi$-system. $$ A, B \in \mathcal{P} \implies A \cap B \in \mathcal{P} $$ An entity satisfying the following conditions is called a $\lambda$-system. (i): $\emptyset \in \mathcal{L}$ (ii): $A \in \mathcal{L} \implies A^{c} \in \mathcal{L}$ (iii): For all $i \ne j$, when $\displaystyle A_{i} \cap A_{j} = \emptyset$, then $\displaystyle \left\{ A_{n} \right\}_{n \in \mathbb{N}} \subset \mathcal{L} \implies \bigcup_{n \in \mathbb{N}} A_{n}</description></item><item><title>If L1 Convergent, Then Martingale is Closable</title><link>https://freshrimpsushi.github.io/en/posts/1401/</link><pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1401/</guid><description>Theorem Given a probability space $( \Omega , \mathcal{F} , P)$ and a martingale $\left\{ ( X_{n} , \mathcal{F}_{n} ) \right\}$, if a stochastic process $\left\{ ( X_{n} , \mathcal{F}_{n} ) \right\}$ converges to a random variable $Y$ through $\mathcal{L}_{1}$, then $\left\{ ( X_{n} , \mathcal{F}_{n} ): n = 1 , \cdots , \infty \right\}$ is a closable martingale. Description Even if $X_{n}$ converges to $Y$ through $\mathcal{L}_{1}$ and almost</description></item><item><title>How to install the cv2 and PIL packages with pip in Python</title><link>https://freshrimpsushi.github.io/en/posts/1400/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1400/</guid><description>Guide The openCV and PIL packages are useful for image processing. The problem is that when importing these packages in example codes, they are usually referred to as cv2 and PIL, but attempting to install them with pip in Python throws an error. This is due to the difference between the names used for importing and those used for installing. openCV As seen in the screenshot, to install openCV, one</description></item><item><title>Uniformly Integrable Martingales are L1 Convergent Martingales</title><link>https://freshrimpsushi.github.io/en/posts/1399/</link><pubDate>Mon, 25 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1399/</guid><description>Definition Let&amp;rsquo;s assume we have a probability space $( \Omega , \mathcal{F} , P)$. A stochastic process $\left\{ X_{n} \right\}$ is said to converge to a random variable $X_{\infty}$ in the sense of $\mathcal{L}_{p}$, if it satisfies the following. $$ \lim_{n \to \infty} \| X_{n} - X_{\infty} \|_{p} = 0 $$ If a stochastic process $\left\{ X_{n} \right\}$ converges in the sense of $\mathcal{L}_{p}$, then the martingale $\left\{ ( X_{n}</description></item><item><title>Vitali Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1398/</link><pubDate>Sun, 24 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1398/</guid><description>Theorem 1 Let&amp;rsquo;s assume that a measure space $( X , \mathcal{E} , \mu)$ is given. When we say $1 \le p &amp;lt; \infty$, the sequence of functions $\left\{ f_{n} \right\}_{n \in \mathbb{N}} \subset \mathcal{L}^{p}$ converging to $f$ in $\mathcal{L}_{p}$ is equivalent to the necessity and sufficiency of all three conditions being satisfied: (i): $\left\{ f_{n} \right\}$ converges in measure to $f$. (ii): $\left\{ | f_{n} |^{p} \right\}$ is uniformly</description></item><item><title>Convergence of Probabilities Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1397/</link><pubDate>Sat, 23 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1397/</guid><description>Probability Convergence Defined Rigorously Given a probability space $( \Omega , \mathcal{F} , P)$. A sequence of random variables $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$ is said to converge in probability to a random variable $X$ if it converges in measure to $X$, denoted as $X_{n} \overset{P}{\to} X$. If you&amp;rsquo;re not yet familiar with measure theory, the term probability space can be disregarded. Explanation The convergence of $\left\{ X_{n} \right\}_{n \in</description></item><item><title>Convergence in Measure</title><link>https://freshrimpsushi.github.io/en/posts/1396/</link><pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1396/</guid><description>Definition 1 Suppose a measure space $( X , \mathcal{E} , \mu)$ is given. A sequence of measurable functions $\left\{ f_{n} : X \to \mathbb{R} \right\}_{n \in \mathbb{N}}$ is said to converge in measure to a measurable function $f : X \to \mathbb{R}$ if for all $M &amp;gt;0$ it satisfies the following. $$ \lim_{n \to \infty} \mu \left( \left\{ x \in X : | f_{n}(x) - f(x) | \ge M</description></item><item><title>Cardinality of a Set</title><link>https://freshrimpsushi.github.io/en/posts/1395/</link><pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1395/</guid><description>Definition 1 For any given set $X$, $\operatorname{card} X$ that satisfies the following properties is defined as the Cardinality of $X$. (i): $X = \emptyset \iff \operatorname{card} X = 0$ (ii): $A \sim B \iff \operatorname{card} A = \operatorname{card} B$ (iii): For some natural number $k$, if $X \sim \left\{ 1 , 2, \cdots , k \right\}$ then $\operatorname{card} X = k$ Specifically, the cardinality of a finite set is</description></item><item><title>If It Is a Regular Martingale, It Is a Uniformly Integrable Martingale</title><link>https://freshrimpsushi.github.io/en/posts/1393/</link><pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1393/</guid><description>Definition Let there be a given probability space $( \Omega , \mathcal{F} , P)$. When a set of random variables $\Phi$ is given, if for all $\varepsilon&amp;gt;0$ there exists a $k \in \mathbb{N}$ that satisfies $$ \sup_{ X \in \Phi } \int_{ \left( \left| X \right| \ge k \right) } \left| X \right| dP &amp;lt; \varepsilon $$, then $\Phi$ is said to be uniformly integrable. If a stochastic process $\left\{</description></item><item><title>Several Equivalent Conditions for the Interior in a Topological Space</title><link>https://freshrimpsushi.github.io/en/posts/1424/</link><pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1424/</guid><description>Definition 1 Let us consider a topological space $(X,\mathcal{T})$ and a subspace $A$. The union of all open sets contained in $A$ is called the interior of $A$, denoted by $A^{\circ}$ or $\mathrm{int}(A)$. $$ A^{\circ} = \cup \left\{ U \in \mathcal{T} \ :\ U \subset A\right\} $$ Furthermore, if there exists an open set $U$ satisfying $x \in U \subset A$ with respect to $x \in X$, then $x$ is</description></item><item><title>Uniform Integrability</title><link>https://freshrimpsushi.github.io/en/posts/1392/</link><pubDate>Tue, 19 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1392/</guid><description>Definition Let&amp;rsquo;s assume that a measure space $( X , \mathcal{E} , \mu)$ is given. Given a set of Lebesgue integrable functions $\Phi \subset \mathcal{L}^{1}$, if for every $\varepsilon&amp;gt;0$, there exists $\delta &amp;gt; 0$ that satisfies $$ \mu (E) &amp;lt; \delta \implies \sup_{f \in \Phi} \int_{ E } \left| f \right| d \mu &amp;lt; \varepsilon $$ then $\Phi$ is said to be uniformly integrable. Explanation The concept of uniform integrability</description></item><item><title>Cantor's Diagonal Argument</title><link>https://freshrimpsushi.github.io/en/posts/109/</link><pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/109/</guid><description>Theorem 1 The open interval $(0,1)$ is an uncountable set. Proof The set of real numbers $\mathbb{R}$ is not a countable set, which is shown by the absence of a &amp;lsquo;one-to-one correspondence&amp;rsquo; between the set of real numbers and any countable set. This demonstrates that there is no one-to-one correspondence between the set of natural numbers and the open interval $(0,1)$, which can be obtained as a corollary. Cantor proved</description></item><item><title>Regular Martingales and Closable Martingales</title><link>https://freshrimpsushi.github.io/en/posts/1384/</link><pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1384/</guid><description>Definition Let us assume a probability space $( \Omega , \mathcal{F} , P)$ and a martingale $\left\{ ( X_{n} , \mathcal{F}_{n} ) \right\}$ are given. If for some integrable random variable $\eta$, $X_{n} = E ( \eta | \mathcal{F}_{n} )$ holds, then $\left\{ ( X_{n} , \mathcal{F}_{n} ) \right\}$ is called a regular martingale. If there exists some integrable random variable $X_{\infty}$ that makes $\left\{ ( X_{n} , \mathcal{F}_{n} ):</description></item><item><title>Countable and Uncountable Sets</title><link>https://freshrimpsushi.github.io/en/posts/1383/</link><pubDate>Sun, 17 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1383/</guid><description>Definitions 1 A set $X$ is called a countable set if it is either a finite set or $X \sim \mathbb{N}$. A set that is not countable is called an uncountable set. $\mathbb{N}$ is the set of natural numbers. $X \sim Y$&amp;rsquo;s $\sim$ denotes the equivalence of sets. Explanation The concept of countable sets might not be intuitively accepted by Easterners, including Koreans. This comes from a fundamental difference in</description></item><item><title>Proof of the Submartingale Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1382/</link><pubDate>Sat, 16 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1382/</guid><description>Theorem Given a probability space $( \Omega , \mathcal{F} , P)$ and a submartingale $\left\{ ( X_{n} , \mathcal{F}_{n} ) \right\}$, if we assume $\displaystyle \sup_{n \in \mathbb{N}} E X_{n}^{+} &amp;lt; \infty$, then $X_{n}$ converges almost surely to some random variable $X_{\infty}: \Omega \to \mathbb{R}$. $$E X_{\infty} &amp;lt; E X_{\infty}^{+} &amp;lt; \infty$$ Proof Strategy: Use the properties of limit supremum and limit infimum. $$ X^{\ast}:= \limsup_{n \in \mathbb{N}} X_{n} \\</description></item><item><title>Finite Sets and Infinite Sets Strictly Defined by Set Theory</title><link>https://freshrimpsushi.github.io/en/posts/1381/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1381/</guid><description>Definition 1 If there exists a bijection $f : X \to Y$ between two sets $X,Y$, then $X$ and $Y$ are said to be equipotent and denoted as $X \sim Y$. If for some non-empty set $X$, any proper subset $Y \subsetneq X$ satisfies $X \sim Y$, then $X$ is called an infinite set. Any set that is not infinite is called a finite set. Explanation When trying to explain</description></item><item><title>Crossings in Stochastic Processes</title><link>https://freshrimpsushi.github.io/en/posts/1380/</link><pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1380/</guid><description>Definition Let&amp;rsquo;s assume that we have a probability space $( \Omega , \mathcal{F} , P)$ and a submartingale $\left\{ ( X_{n} , \mathcal{F}_{n} ) \right\}$. An upcrossing occurs when, over a closed interval $[a,b]$, the value changes from being $X_{t_{1}} \le a$ to $X_{t_{2}} \ge b$. The number of upcrossings observed up to time $N \in \mathbb{N}$ is represented as follows: $$ \beta_{N} (a,b): = \text{A number of upcrossing of</description></item><item><title>What is Resonance?</title><link>https://freshrimpsushi.github.io/en/posts/399/</link><pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/399/</guid><description>Theorem Given the propositions $p$, contradiction $c$, and $A_{\alpha} \subset X$, the following hold: [1] Vacuous Truth: $c \implies p$ [2] Union: $\displaystyle \bigcup_{\alpha \in \emptyset} A_{\alpha} = \emptyset$ [3] Intersection: $\displaystyle \bigcap_{\alpha \in \emptyset} A_{\alpha} = X$ Explanation For instance, in the statement &amp;ldquo;God is dead&amp;rdquo;, if God does not exist, the premise is already flawed. If God doesn&amp;rsquo;t exist, then it implies $0$ gods have died, so it</description></item><item><title>Easy Ways to Memorize Surjections, Injections, Ranges, and Domains, Explained</title><link>https://freshrimpsushi.github.io/en/posts/690/</link><pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/690/</guid><description>Explanation I found it really hard to memorize the names when I first encountered injective, surjective, and codomain. It was easier to distinguish them in English, but in Korean, it didn&amp;rsquo;t stick well. &amp;lsquo;Is this injective or surjective?&amp;rsquo; &amp;lsquo;Was the codomain the larger one? What was it?&amp;rsquo;. Since the names sounded similar, I was always confused when I needed to use them. I don&amp;rsquo;t know how many people are like</description></item><item><title>줄리아의 타입과 애노테이션</title><link>https://freshrimpsushi.github.io/en/posts/1379/</link><pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1379/</guid><description>Code julia&amp;gt; typeof(0) Int64 julia&amp;gt; typeof(0.0) Float64 julia&amp;gt; typeof(0 == 0.0) Bool julia&amp;gt; typeof(Bool) DataType julia&amp;gt; typeof(NaN) Float64 julia&amp;gt; typeof(Inf) Float64 julia&amp;gt; typeof(&amp;#39;O&amp;#39;) Char julia&amp;gt; typeof(&amp;#34;Ohmygirl&amp;#34;) String julia&amp;gt; typeof(&amp;#34;O&amp;#34;) String Julia implements a variety of types. $0$ and $0.0$ are the same $0$ but have different types, and as you can see, even the type Bool has a type called DataType. Similar to the C language, String is an array</description></item><item><title>Dob's Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/1375/</link><pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1375/</guid><description>Theorem Given a probability space $( \Omega , \mathcal{F} , P)$ and a submartingale $\left\{ ( X_{n} , \mathcal{F}_{n} ) \right\}$. If for some $N \in \mathbb{N}$ and $p&amp;gt;1$, $X_{n} \ge 0 (n \le N)$, $E X_{N}^{p} &amp;lt; \infty$ then $$ E \left( \max_{n \le N} X_{n}^{p} \right) \le \left( {{ p } \over { p-1 }} \right)^{p} E X_{N}^{p} \text{ a.s.} $$ Explanation The form of the equation can</description></item><item><title>Injection, Surjection, Bijection, Inverse Function</title><link>https://freshrimpsushi.github.io/en/posts/471/</link><pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/471/</guid><description>Definition 1 Let&amp;rsquo;s say $x \in X$, $y \in Y$, and $f: X \to Y$ are functions. For every $x_{1}, x_{2} \in X$, if $x_{1} \ne x_{2} \implies f(x_{1}) \ne f(x_{2})$ then $f$ is called injective. If $f(X) = Y$, then $f$ is called surjective. If $f$ is both injective and surjective, it is called bijective. $I : X \to X$ that satisfies $I(x) = x$ is called an Identity</description></item><item><title>Julia Programming Language</title><link>https://freshrimpsushi.github.io/en/posts/1374/</link><pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1374/</guid><description>Overview Julia has been developed at MIT and publicly released in 2012, aiming for a language that is both highly productive and fast. It achieves speeds comparable to C or Fortran while also providing a high-level syntax similar to Python or R, among absorbing benefits from various other languages. As of November 2019, it&amp;rsquo;s true that Julia is somewhat lagging due to the rapid advancement of GPUs and the prevalence</description></item><item><title>The Original Image of a Function</title><link>https://freshrimpsushi.github.io/en/posts/472/</link><pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/472/</guid><description>Definition 1 For functions $f: X \to Y$ and $B \subset Y$, $f^{-1}(B): = \left\{ x \in X \ | \ f(x) \in B \right\}$ is called the preimage or inverse image according to $f$ of $B$. Explanation Though the notation is similar, one cannot say that the inverse image and the inverse function are related just by the definitions alone, and one should not confuse them. Some people might</description></item><item><title>Functions and Mappings Rigorously Defined by Set Theory, Sequences</title><link>https://freshrimpsushi.github.io/en/posts/470/</link><pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/470/</guid><description>Definitions 1 Let&amp;rsquo;s assume two sets $X$, $Y$ that are not empty sets are given. A binary relation $f \subset (X,Y)$ is called a function if it satisfies the following and is denoted as $f : X \to Y$. $$ (x ,y_{1}) \in f \land (x,y_{2}) \in f \implies y_{1} = y_{2} $$ For the function $f : X \to Y$, $\text{Dom} (f) = X$ is called the domain of</description></item><item><title>Inequalities of Martingales</title><link>https://freshrimpsushi.github.io/en/posts/1370/</link><pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1370/</guid><description>Theorem Let $\left\{ (X_{n} , \mathcal{F}_{n}) \right\}$ be a supermartingale. [1]: For all $\lambda &amp;gt; 0$, $$ \begin{align*} \lambda P \left( \max_{n \le N} X_{n} \ge \lambda \right) \le &amp;amp; E X_{1} - \int_{(\max_{n \le N} X_{n} &amp;lt; \lambda)} X_{N} dP \\ \le &amp;amp; E X_{1} + E X_{N}^{-} \text{ a.s.} \end{align*} $$ [2]: For all $\lambda &amp;gt; 0$, $$ \begin{align*} \lambda P \left( \min_{n \le N} X_{n} \le -</description></item><item><title>Equivalence Relations and Set Partitions</title><link>https://freshrimpsushi.github.io/en/posts/1051/</link><pubDate>Sat, 09 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1051/</guid><description>Theorem 1 A partition of a set $X$ with respect to an equivalence relation $R$ is $X / R$ of $X$. Description Though this theorem might seem trivial, it&amp;rsquo;s widely used across all areas of mathematics, including topology and abstract algebra. An equivalence relation, simply put, is viewing things as &amp;rsquo;the same,&amp;rsquo; and ironically, the concept of &amp;rsquo;not being the same&amp;rsquo; accompanies the establishment of an equivalence relation. The entire</description></item><item><title>First-Class Objects in Programming</title><link>https://freshrimpsushi.github.io/en/posts/1368/</link><pubDate>Sat, 09 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1368/</guid><description>Definition In programming, a First Class Object is an element that satisfies the following conditions: (i) Can be a real argument of a function. (ii) Can be a return value of a function. (iii) Can be the target of an assignment statement. (iv) Can be the subject of equality comparison. Example Simply put, something that can be treated like a regular number is called a first class object, precisely because</description></item><item><title>Homotopy Type</title><link>https://freshrimpsushi.github.io/en/posts/1050/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1050/</guid><description>Definition 1 Let&amp;rsquo;s say a equivalence relation $R$ is defined on a set $X$. For $x \in X$, $x / R := \left\{ y \in X : y R x \right\}$ is called the equivalence class of $x$. The set of all equivalence classes given by $X$ is represented as $X / R := \left\{ x / R : x \in X \right\}$. Explanation Though the expression might look a</description></item><item><title>k-Means Clustering</title><link>https://freshrimpsushi.github.io/en/posts/1365/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1365/</guid><description>Algorithm Input Given data of dimension $p$, $N$ pieces, and a natural number $k$. Step 1. Initialization Randomly set $k$ points $\mu_{1} , \cdots , \mu_{k}$. Each $\mu_{j}$ will become the mean of cluster $M_{j}$. Step 2. Distance Calculation Calculate $\| x_{i} - \mu_{j} \|$ for the $i$th data $x_{i}$ and $j = 1 , \cdots , k$. Choose the smallest one to make it $x_{i} \in M_{j}$. Repeat this</description></item><item><title>Levenshtein Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/510/</link><pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/510/</guid><description>Algorithm Input Let&amp;rsquo;s denote the string as $A,B$, $A=[a_{i}]=(a_{1}, a_{2} , \cdots, a_{n})$, and $B=[b_{j}]=(b_{1}, b_{2} , \cdots, b_{m})$. Step 1. Initialization Create the matrix $M_{(n+1) \times (m+1)} = [m_{x y }]$ and assign $M_{11} ← 0$. Then fill the $1$ row and $1$ column as follows: $$ M_{(i+1) 1} ← i \\ M_{ 1 (j+1)} ← j $$ Step 2. Dynamic Programming for $i = 1, 2, \cdots ,</description></item><item><title>Partition of a Set</title><link>https://freshrimpsushi.github.io/en/posts/1049/</link><pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1049/</guid><description>Definition 1 A partition of a set $X$ consists of all subsets $A,B,C$ of $X$ that satisfy the following conditions: (i): $$A,B \in \mathscr{P} \land A \ne B \implies A \cap B = \emptyset$$ (ii): $$\bigcup_{C \in \mathscr{P} } C = X$$ Explanation Although the mathematical expression might seem complex, simply put, it&amp;rsquo;s just about dividing the entire set into several parts without omission. If there&amp;rsquo;s leeway to delve into</description></item><item><title>Programming Types</title><link>https://freshrimpsushi.github.io/en/posts/1364/</link><pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1364/</guid><description>The Birth of Types If you&amp;rsquo;ve ever written code in a language that requires specifying types when declaring variables, you almost certainly felt a bit of frustration along with it. Some languages perform calculations without needing to explicitly define types, and writing what seems like messy and meaningless code feels like a waste of time and energy. Imagine programming environments before the concept of types existed. The data computers understand</description></item><item><title>Equivalence Relations in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1033/</link><pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1033/</guid><description>Definition 1 A binary relation that is reflexive, symmetric, and transitive is called an equivalence relation. Explanation To put the concept of an equivalence relation in non-mathematical terms, it&amp;rsquo;s like saying &amp;ldquo;it&amp;rsquo;s all the same.&amp;rdquo; While it&amp;rsquo;s not always necessary to have a reason when studying mathematics, if there were to be a practical reason for studying mathematics, it could be said to &amp;ldquo;simplify complex concepts into easier, more manageable</description></item><item><title>Radix Sort</title><link>https://freshrimpsushi.github.io/en/posts/1363/</link><pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1363/</guid><description>Algorithm Assuming we have data comprised of numbers with digits limited to $k$. In this case, the data will be sorted according to the following algorithm, and its time complexity is $O (n)$. Sort by comparing the $i = 1 , \cdots , k$th digits of each number. Explanation Radix Sort is a clever method that can sort data in linear time without comparing the data elements, overcoming the limits</description></item><item><title>Mathematical Binary Relations</title><link>https://freshrimpsushi.github.io/en/posts/960/</link><pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/960/</guid><description>Definition 1 For two sets $X,Y$, $$ R := \left\{ (x,y): x \in X , y \in Y \right\} \subset X \times Y $$ is defined as a (binary) relation and is represented as follows: $$ (x,y) \in R \iff x R y $$ $x R y \iff y R^{-1} x$ satisfying $$ R^{-1} : \left\{ (y,x): (x,y) \in R \right\} $$ is called the inverse relation of $R$. For</description></item><item><title>Programming Paradigm</title><link>https://freshrimpsushi.github.io/en/posts/1361/</link><pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1361/</guid><description>Definition Programming Paradigm refers to the perspective or methodology in solving problems when writing a program. A programming language that fits a particular paradigm is said to have that programming paradigm, and most languages have one paradigm. Languages that have multiple paradigms are called multi-paradigm languages. Having one paradigm does not mean excluding the concepts of other paradigms, but rather that the potential performance of that language can be fully</description></item><item><title>Cartesian Product of Sets</title><link>https://freshrimpsushi.github.io/en/posts/1360/</link><pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1360/</guid><description>Definition 1 For any two objects $a$, $b$, $(a,b)$ is called an Ordered Pair. For any two sets $A$, $B$, the set of ordered pairs $(a,b)$ of $a \in A$, $b \in B$ is called the Cartesian Product of $A$ and $B$ and is represented as follows. $$ A \times B := \left\{ (a,b): a \in A \land b \in B \right\} $$ Explanation The reason why the term &amp;lsquo;product&amp;rsquo;</description></item><item><title>Lower Bound on the Time Complexity of Comparison Sorting Algorithms</title><link>https://freshrimpsushi.github.io/en/posts/1359/</link><pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1359/</guid><description>Theorem The time complexity of comparison-based sorting algorithms cannot be better than $\Omega ( n \log n )$. Explanation Algorithms are inherently fascinating, yet seeing efficient ones like insertion sort being outperformed by quick sort makes one wonder if there exists an even more efficient algorithm. Fortunately or unfortunately, based on this proof, there is no need to consider algorithms more efficient than this. Of course, there might exist faster</description></item><item><title>Homomorphism Preserves Basis</title><link>https://freshrimpsushi.github.io/en/posts/1407/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1407/</guid><description>Theorem Let&amp;rsquo;s suppose two topological spaces $X$, $Y$ and a homeomorphism $f$ between them are given. $$ f\ :\ X \rightarrow Y $$ If $\mathcal{B}_{X}$ is called a basis of $X$, then $f(\mathcal{B}_{X})$ becomes a basis of $Y$. Description Simply put, a homeomorphic mapping preserves the basis. Proof A collection $\mathcal{B}$ of subsets of $X$ that satisfies the following two conditions is called a basis for the topology on $X$:</description></item><item><title>Sets and Indices</title><link>https://freshrimpsushi.github.io/en/posts/1358/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1358/</guid><description>Definitions A set whose elements are sets themselves is called a Family. The elements of a family are called Members. When each element of a set $\Gamma$ corresponds to a set $A_{\gamma}$ with $\gamma$ as the index, $\Gamma$ as the index set, and $\left\{ A_{\gamma} : \gamma \in \Gamma \right\}$ as the indexed family. Explanation Though the term &amp;lsquo;Family&amp;rsquo; was originally introduced as a refined term for &amp;lsquo;set of sets&amp;rsquo;,</description></item><item><title>Time Complexity of Comparison Sorting Algorithms</title><link>https://freshrimpsushi.github.io/en/posts/1357/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1357/</guid><description>Theorem Given $n$ pieces of data, the time complexity of comparison sorting algorithms is as follows: [1] Bubble sort: $$ \Theta ( n^2 ) \\ O ( n^2 ) $$ [2] Selection sort: $$ \Theta ( n^2 ) \\ O ( n^2 ) $$ [3] Insertion sort: $$ \Theta ( n^2 ) \\ O ( n^2 ) $$ [4] Heap sort: $$ \Theta ( n \log n ) \\ O</description></item><item><title>In Topology, What is a Coordinate System?</title><link>https://freshrimpsushi.github.io/en/posts/1404/</link><pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1404/</guid><description>Definition Let us consider $M$ to be a $n$-dimensional manifold. Suppose two open sets $U\subset M$, $\tilde{U} \subset \mathbb{R}^n$ and a homeomorphism $\phi\ :\ U \rightarrow \tilde{U}$ are given. Then, the ordered pair $(U, \phi)$ is called the coordinate system on $M$, or simply coordinates$(\mathrm{Chart})$. Explanation If $p \in U$, $\phi (p)=0$, then $(U,\phi)$ is called the center in $p$. Moreover, $U$ is referred to as the coordinate domain or</description></item><item><title>Zermelo-Fraenkel Set Theory with the Axiom of Choice</title><link>https://freshrimpsushi.github.io/en/posts/1356/</link><pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1356/</guid><description>Zermelo&amp;rsquo;s Axiomatic System [1] Axiom of Extensionality: $$ \forall A \forall B ( \forall x ( x \in A \iff x \in B) ) $$ It is said that two sets $A$, $B$ are equal if they have the same elements, and this is expressed as $A = B$. [2] Axiom of the Empty Set: $$ \exists X \forall x \left( \lnot \left( x \in X \right) \right) $$ There</description></item><item><title>Selective Sampling Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/1355/</link><pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1355/</guid><description>Theorem Let&amp;rsquo;s assume there is a probability space $( \Omega , \mathcal{F} , P)$ and a supermartingale $\left\{ ( X_{n} , \mathcal{F}_{n} ) \right\}$. If $\tau$ and $\sigma$ are bounded stopping times with respect to $\sigma \le \tau$ and $\mathcal{F}_{n}$ then $$ E \left( X_{\tau} | \mathcal{F}_{\sigma} \right) \le X_{\sigma} \text{ a.s.} $$ Being bounded with respect to $\mathcal{F}_{n}$ for $\tau$ means, quite literally, that there exists a $N \in</description></item><item><title>Axiom of Choice</title><link>https://freshrimpsushi.github.io/en/posts/1354/</link><pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1354/</guid><description>Axioms 1 $$ \forall U \left( \emptyset \notin U \implies \exists f : U \to \bigcup_{X \in U \\ f(X) \in X } U \right) $$ For every set of sets $U$ that is not the empty set, there exists a selection function $f$ that selects exactly one element from each element of $U$. Explanation The axiom of choice guarantees the existence of a selection function $f$ that, for example,</description></item><item><title>Properties of Stopping Times</title><link>https://freshrimpsushi.github.io/en/posts/1353/</link><pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1353/</guid><description>Theorem Let&amp;rsquo;s assume we have a probability space $( \Omega , \mathcal{F} , P)$ and a martingale $\left\{ ( X_{n} , \mathcal{F}_{n} ) \right\}$. For a given stopping time $\tau$, $\mathcal{F}_{\tau}:= \left\{ A \in \mathcal{F}: A \cap ( \tau = n ) \in \mathcal{F}_{n} \right\}$ is referred to as the sigma field induced by $\tau$. [1]: $\mathcal{F}_{\tau}$ is a sigma field. [2]: $\tau$ is a $\mathcal{F}_{\tau}$-measurable function. [3]: For a</description></item><item><title>Substitution Axiom Form</title><link>https://freshrimpsushi.github.io/en/posts/1352/</link><pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1352/</guid><description>Axiom $$ \forall X \left( \forall x \in X \exists ! y \left( p(x,y) \right) \implies \exists Y \forall x \in X \exists y \in Y \left( p(x,y) \right) \right) $$ The range exists for all functions. The symbol $\exists !$ signifies uniqueness. Here, $p(x,y)$ is a propositional function in $X \times Y$. Explanation Although propositional function $p(x,y)$ is a function, strictly speaking, it has not yet been defined as</description></item><item><title>Stopping Times in Stochastic Processes</title><link>https://freshrimpsushi.github.io/en/posts/1351/</link><pubDate>Sat, 26 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1351/</guid><description>Definitions Let&amp;rsquo;s assume a probability space $( \Omega , \mathcal{F} , P)$ is given. A random variable $\tau$ with an integer value greater than or equal to $0$ for all $n \in \mathbb{N}_{0}$ that satisfies $(\tau = n) \in \mathcal{F}_{n}$ with respect to the filtration $\left\{ \mathcal{F}_{n} \right\}$ is called a Stopping Time. For a Borel set $B \in \mathcal{B}(\mathbb{R})$, $(\tau \in B) = \tau^{-1} (B)$ is, therefore, the same</description></item><item><title>Axiom of Regularity</title><link>https://freshrimpsushi.github.io/en/posts/1350/</link><pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1350/</guid><description>Axioms $$ \forall X \left( \exists x_{0} ( x_{0} \in X ) \implies \exists y ( y \in X \land \lnot \exists x ( x \in y \land x \in X )) \right) $$ Every set $X \ne \emptyset$ has an element that is mutually exclusive with itself.</description></item><item><title>Lp Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1394/</link><pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1394/</guid><description>Definition 1 If a sequence of functions $\left\{ f_{n} \right\}_{n \in \mathbb{N}}$ satisfies the following for some function $f$, then $\left\{ f_{n} \right\}$ is said to converge to $f$ in $L^{p}$. $$ \lim_{n \to \infty} \left\| f_{n} - f \right\|_{p} = 0 $$ The sequence $\left\{ f_{n} \right\}_{n \in \mathbb{N}}$ is said to be Cauchy in $L^{p}$ if it satisfies the following. $$ \lim_{n, m \to \infty} \left\| f_{n} -</description></item><item><title>The Definition of Martingale</title><link>https://freshrimpsushi.github.io/en/posts/1349/</link><pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1349/</guid><description>Definition Let&amp;rsquo;s assume that a probability space $( \Omega , \mathcal{F} , P)$ is given. A sequence $\left\{ \mathcal{F}_{n} \right\}_{n \in \mathbb{N}}$ of sub-σ-fields of $\mathcal{F}$ is called a filtration if it satisfies the following: $$ \forall n \in \mathbb{N}, \mathcal{F}_{n} \subset \mathcal{F}_{n+1} $$ Given a filtration $\left\{ \mathcal{F}_{n} \right\}_{n \in \mathbb{N}}$, a sequence $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$</description></item><item><title>Axiom of Infinity</title><link>https://freshrimpsushi.github.io/en/posts/1348/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1348/</guid><description>Axioms $$ \exists U \left( \emptyset \in U \land \forall X ( X \in U \implies S(X) \in U) \right) $$ There exists a set $U$ that contains the empty set and $X$ as elements, and also contains $S(X)$ as an element. For a set $X$, $S(X)$ is defined as a set that is equivalent to $S(X):= X \cup \left\{ X \right\}$. Explanation Rather than tediously explaining why this is</description></item><item><title>Maximal Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1390/</link><pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1390/</guid><description>Theorem1 For every $f \in L^1_{\mathrm{loc}}$ and every $\alpha &amp;gt;0$, there exists a constant $C&amp;gt;0$ that satisfies the following condition. $$ \mu \big( \left\{ x\ :\ Hf(x)&amp;gt;\alpha \right\}\big) \le \frac{C}{\alpha} \int |f(y)| dy $$ This inequality is called the Hardy-Littlewood maximal inequality. The Hardy-Littlewood maximal function $$ Hf (x) = \sup \limits_{r&amp;gt;0} A_{r} |f|(x) = \sup \limits_{r&amp;gt;0} \frac{1}{\mu \big( B(r,x) \big)}\int_{B(r,x)}|f(y)|dy $$ Proof Let&amp;rsquo;s say $E_\alpha =\left\{ x\ |\ Hf(x)</description></item><item><title>Proof of Conditional Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1347/</link><pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1347/</guid><description>Theorem Given a probability space $( \Omega , \mathcal{F} , P)$ and a sub-sigma field $\mathcal{G} \subset \mathcal{F}$, let&amp;rsquo;s assume $X$ is a random variable. Regarding the convex function $\phi : \mathbb{R} \to \mathbb{R}$ and $\phi (X) \in \mathcal{L}^{1} ( \Omega ) $, $$ \phi \left( E \left( X | \mathcal{G} \right) \right) \le E \left( \phi (X) | \mathcal{G} \right) $$ A function is said to be convex if,</description></item><item><title>The Mean Value of Locally Integrable Functions Converges to the Value of the Function at the Center.</title><link>https://freshrimpsushi.github.io/en/posts/1391/</link><pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1391/</guid><description>Theorem1 Let&amp;rsquo;s say $f \in L^1_{\mathrm{loc}}$. Then, the following is true. $$ \lim \limits_{r \rightarrow 0} A_{r} f(x)=f(x) \text{ a.e. } x\in \mathbb{R}^n $$ Here, $\text{ a.e. }$ is almost everywhere. Description The message here is that the limit of the average value of a locally integrable function&amp;rsquo;s value over the volume $B(r,x)$ as the radius goes to $0$ equals the function value at the center of the volume. Proof</description></item><item><title>Hardy-Littlewood Maximal Function</title><link>https://freshrimpsushi.github.io/en/posts/1389/</link><pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1389/</guid><description>Definition1 Let&amp;rsquo;s denote $ f \in L^1_{\mathrm{loc}}$. Then, the Hardy-Littlewood maximal function $Hf$ is defined as follows: $$ Hf (x) := \sup \limits_{r&amp;gt;0} A_{r} |f|(x) = \sup \limits_{r&amp;gt;0} \frac{1}{\mu \big( B(r,x) \big)}\int_{B(r,x)}|f(y)|dy $$ $A_{r}f(x)$ represents the average of the function values of $B_{r}(x)$ on the top of $f$. $H$ is called the maximal operator. Theorem $Hf$ is a Lebesgue measurable function. If $f \in L^1_{\mathrm{loc}}$, then $A_{r}f(x)$ is continuous with</description></item><item><title>Maximal Lemma</title><link>https://freshrimpsushi.github.io/en/posts/1388/</link><pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1388/</guid><description>Theorem1 Let&amp;rsquo;s call a collection of open balls at $\mathbb{R}^n$ given $\mathcal{B}$. Let&amp;rsquo;s say $U=\bigcup \limits_ { B\in \mathcal{B}} B$. Then, for some constant $c \lt m (U)$, there exist a finite number of mutually disjoint $B_{j} \in \mathcal{B}$ that satisfy the following condition. $$ \dfrac{c}{3^{n}} \lt \sum \limits_{j=1}^{k} m(B_{j}) $$ Here, $m$ is the $n$-dimensional Lebesgue measure. Description Actually, this theorem is not officially named the maximal lemma, but</description></item><item><title>Power Set Axiom</title><link>https://freshrimpsushi.github.io/en/posts/1346/</link><pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1346/</guid><description>Axioms 1 $$ \forall X \exists P \forall A ( A \subset X \implies A \in P) $$ For any set $X$, there exists a set $P$ that contains every subset of $X$ as an element. Explanation The power set of $X$ is generally denoted by $\mathcal{P} (X)$ or $2^{X}$, the reason being if the number of elements of a finite set $X$ is denoted by $|X|$, then $\left| \mathcal{P}</description></item><item><title>Union axiom</title><link>https://freshrimpsushi.github.io/en/posts/1344/</link><pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1344/</guid><description>Axiom $$ \forall X \left( \exists U \left( \forall a \left( a \in x \land x \in X \implies a \in U \right) \right) \right) $$ For any set $X$, there exists a set $U$ that contains all the elements of the elements of $X$. Definition of Union 1 The axiom of union guarantees the existence of the union defined as follows: $$ x \in A \lor x \in B</description></item><item><title>Conditional Variance Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1343/</link><pubDate>Fri, 18 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1343/</guid><description>Definition Given a probability space $( \Omega , \mathcal{F} , P)$ and a sub sigma field $\mathcal{G} \subset \mathcal{F}$, let $X$ and $Y$ be random variables. The following defined $\Var$ is called the variance of $X$ given $\mathcal{G}$. $$ \Var ( X | \mathcal{G}) := E \left[ (X - E(X | \mathcal{G}))^2 | \mathcal{G} \right] $$ That $\mathcal{G}$ is a sub sigma field of $\mathcal{F}$ means both are sigma fields</description></item><item><title>Classification Axiomatic Form</title><link>https://freshrimpsushi.github.io/en/posts/1341/</link><pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1341/</guid><description>Axioms 1 $$ \forall X \exists A \forall a \left( a \in A \iff ( a \in X \land p(a)) \right) $$ For any set $X$, there exists a subset $A$ composed of elements that have property $p$. $p(x)$ is a propositional function in $X$. Explanation The reason why $A$ is limited to a subset of $X$ is to prevent problems like Russell&amp;rsquo;s paradox. The reason it is not called</description></item><item><title>Saturation and Definition of Fibers in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1387/</link><pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1387/</guid><description>Definitions Given two sets $X$, $Y$ and a function $\pi\ :\ X\rightarrow Y$. If $\pi^{-1}\big( \pi (u) \big)=u$ holds, then $u\subset X$ is called saturation. The set $\pi^{-1}(y) \subset X$ is called the fiber or stalk over the point $y\in Y$ in $\pi$. Description $\pi^{-1}$ is a preimage. Let&amp;rsquo;s easily understand through the pictures below. Saturation $u$ is always less than or equal to $\pi^{-1} \big( \pi (u) \big)$. Thus,</description></item><item><title>Disjoint Union: Disjoint Unions</title><link>https://freshrimpsushi.github.io/en/posts/1385/</link><pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1385/</guid><description>Definition Let $\left\{ X_{\alpha} \right\} _{\alpha\in A}$ be an arbitrary index family. Then, the set of ordered pairs defined as follows is called the $\left\{ X_{\alpha}\right\}$ disjoint union. $$ \bigsqcup \limits_{\alpha \in A} X_{\alpha} := \left\{ (x,\alpha)\ |\ x\in X_{\alpha},\ \alpha \in A \right\} $$ Explanation Instead of $\bigsqcup$, $\amalg$, $\biguplus$, etc., are also used. Note that $\amalg$ is not the capital Pi $\Pi$. It is the mirrored version of</description></item><item><title>Separated Union Topological Space</title><link>https://freshrimpsushi.github.io/en/posts/1386/</link><pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1386/</guid><description>Definition Let $\left\{ X_\alpha \right\}_{\alpha \in A}$ be an arbitrary topological space index family. Let us say $u \subset \bigsqcup \limits_{\alpha \in A} X_\alpha$. Then, for all $\alpha \in A$, if $u \cap X_\alpha$ is an open set in $ X_\alpha$, then $u$ is said to be an open set $^{\ast}$ in $\bigsqcup \limits_{\alpha \in A} X_\alpha$. The so-called open$^{\ast}$ here is not exactly open in the sense of topology.</description></item><item><title>Smoothing Properties of Conditional Expectation</title><link>https://freshrimpsushi.github.io/en/posts/1340/</link><pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1340/</guid><description>Theorem Given a probability space $( \Omega , \mathcal{F} , P)$ and a sub-sigma field $\mathcal{G}, \mathcal{G} ' \subset \mathcal{F}$, assume $X$ and $Y$ are random variables. [1]: If $X$ is $\mathcal{G}$-measurable $$ E(XY | \mathcal{G}) = X E (Y | \mathcal{G}) \text{ a.s.} $$ [2]: If $\mathcal{G} ' \subset \mathcal{G}$ then $$ \begin{align*} E (X | \mathcal{G} ') =&amp;amp; E \left( E ( X | \mathcal{G}) | \mathcal{G} '</description></item><item><title>Axiom of Pairs</title><link>https://freshrimpsushi.github.io/en/posts/1339/</link><pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1339/</guid><description>Axioms $$ \forall A \forall B \exists U ( A \in U \land B \in U ) $$ For any two sets $A$ and $A$, there exists a set $U$ that has $A$ and $B$ as elements. Explanation When first encountering the axiom of pairs (and indeed, this applies to most axioms), one might wonder why such an axiom is necessary at all. However, the pairing axiom can actually be</description></item><item><title>Conditional Properties of Probability</title><link>https://freshrimpsushi.github.io/en/posts/1338/</link><pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1338/</guid><description>Theorem Let&amp;rsquo;s say we have a probability space $( \Omega , \mathcal{F} , P)$ and a sub-sigma field $\mathcal{G} \subset \mathcal{F}$. [1] For all $B \in \mathcal{G}$, there is $0 \le P(B | \mathcal{G}) \le 1$. [2] Continuity of probability: For a nested sequence $\left\{ B_{n} \right\}_{n \in \mathbb{N}} \subset \mathcal{G}$, $$ \lim_{n \to \infty} B_{n} = B \implies P ( B_{n} | \mathcal{G} ) \to P ( B |</description></item><item><title>Empty Set Axiom</title><link>https://freshrimpsushi.github.io/en/posts/1337/</link><pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1337/</guid><description>Axioms 1 $$ \exists X \forall x \left( \lnot \left( x \in X \right) \right) $$ A set $X$ that does not contain any elements exists, and this set $X$ is defined as the empty set. Explanation The empty set is typically denoted as $\emptyset$. Meanwhile, the empty set can also be viewed as a set with $0$ elements, and sets that can be defined in terms of the number</description></item><item><title>Proof of the Dominated Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1336/</link><pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1336/</guid><description>Theorem Given a probability space $( \Omega , \mathcal{F} , P)$. If a sequence of random variables $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$ satisfies $n \in \mathbb{N}$ for all $Y \in \mathcal{L}^{1} (\Omega)$ and some $Y \in \mathcal{L}^{1} (\Omega)$, then $$ X_{n} \to X \text{ a.s.} \implies E( X_{n} | \mathcal{G} ) \to \mathcal{G} ) \text{ a.s.} $$ $\text{a.s.}$ means almost surely. Description The Dominated Convergence Theorem for conditional expectations, also</description></item><item><title>Complex Measures, Vector Measures</title><link>https://freshrimpsushi.github.io/en/posts/1378/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1378/</guid><description>Definition1 Let $(X,\mathcal{E})$ be a measurable space. A function $\nu : \mathcal{E} \to \mathbb{C}$ that satisfies the following conditions is called a complex measure or vector measure on $(X,\mathcal{E})$. (a) $\nu (\varnothing) = 0$ (b) For mutually disjoint $E_{j} \in \mathcal{E}$, $$ \nu \left( \bigcup \limits_{j=1}^\infty E_{j} \right) = \sum \limits_{1} ^\infty \nu (E_{j}) $$ Explanation (b) signifies countable additivity. Unlike measures and signed measures, complex measures are defined not</description></item><item><title>Extensionality Axiom</title><link>https://freshrimpsushi.github.io/en/posts/1335/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1335/</guid><description>Axiom 1 $$ \forall A \forall B ( \forall x ( x \in A \iff x \in B) ) $$ Two sets $A$, $B$ are said to be equal if every element of the first is an element of the second and vice versa. This is represented as $A = B$. Explanation If $A$ and $B$ are not equal, it is denoted as $A \ne B$. The equality of two</description></item><item><title>Proof of the Monotone Convergence Theorem for Conditional Cases</title><link>https://freshrimpsushi.github.io/en/posts/1331/</link><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1331/</guid><description>Theorem Let&amp;rsquo;s assume that a probability space $( \Omega , \mathcal{F} , P)$ is given. Considering the sequence of random variables $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$ and $X \in \mathcal{L}^{1} (\Omega)$, we have $$ X_{1} \le X_{2} \le \cdots \le X \\ X_{n} \to X \text{ a.s.} $$ then $$ \lim_{n \to \infty} E( X_{n} | \mathcal{G} ) = E( \lim_{n \to \infty} X_{n} | \mathcal{G} ) \text{ a.s.} $$</description></item><item><title>Algebra, Quasi-measure</title><link>https://freshrimpsushi.github.io/en/posts/1377/</link><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1377/</guid><description>Definition A collection $\mathcal{A}$ of subsets of a set $X \ne \varnothing$ is called the algebra of sets on $X$ if it satisfies the following three conditions: (a) If $E_{1}$, $\cdots$, and $E_{n}\in \mathcal{A}$, then $\bigcup \nolimits_{1}^n E_{n} \in \mathcal{A}$ is true. (b) If $E_{1}$, $\cdots$, and $E_{n}\in \mathcal{A}$, then $\bigcap \nolimits_{1}^n E_{n} \in \mathcal{A}$ is true. (c) If $E \in \mathcal{A}$, then $E^c\in \mathcal{A}$ is true. Let&amp;rsquo;s refer to</description></item><item><title>Set Inclusion</title><link>https://freshrimpsushi.github.io/en/posts/1329/</link><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1329/</guid><description>Definition 1 $$ A \subset B \iff \forall x (x\in A \implies x \in B) $$ For any set $A$, $B$, if all elements of $A$ are also elements of $B$, then $A$ is a Subset of $B$, and $B$ is a Superset of $A$, denoted as $A \subset B$. Explanation If it&amp;rsquo;s $A \subset B$ and $B \not\subset A$, then $A$ is called a Proper Subset of $B$, and</description></item><item><title>How to Create Equally Spaced Row Vectors in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1376/</link><pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1376/</guid><description>Method linspace(a,b,n): Returns a row vector of $[a,b]$ divided into $n$ equal intervals. If the number of elements is not specified, it returns a $1\times 100$ vector. It is used when the number of intervals is important, not the length of the intervals. a: m :b : Returns a row vector of $[a,b]$ divided by equal intervals of $m$. If the interval is not specified, the interval is set to</description></item><item><title>Methods for Expressing the Absolute Value of an Arbitrary Function as Two Non-negative Functions</title><link>https://freshrimpsushi.github.io/en/posts/1325/</link><pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1325/</guid><description>Theorem Basic The absolute value $|f|$ of the function $f : X \to \mathbb{R}$ is represented with respect to the positive part $f^{+}$ and the negative part $f^{-}$ as follows. $$ |f| = f^{+} + f^{-} $$ Advanced Let’s say the function $g : X \to \mathbb{R}$ is almost everywhere $g \ge 0$. [1] Inside the absolute value: $$ f^{+} = |f^{+}| \\ f^{-} = |f^{-}|</description></item><item><title>Properties of Conditional Expectation</title><link>https://freshrimpsushi.github.io/en/posts/1322/</link><pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1322/</guid><description>Theorem Given a probability space $( \Omega , \mathcal{F} , P)$: [1] From measure theory: If measurable functions $f$, $g$ are $\mathcal{F}$-measurable, then there exists a Borel function $h : \mathbb{R} \to \mathbb{R}$ satisfying $g = h (f)$. [2] Application in probability theory: If random variables $X$, $Y$ are $\sigma (X)$-measurable, then there exists a Borel function $h : \mathbb{R} \to \mathbb{R}$ satisfying $E(Y | X) = h(X)$. [3]: If</description></item><item><title>Quantifiers over Propositional Functions</title><link>https://freshrimpsushi.github.io/en/posts/1321/</link><pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1321/</guid><description>Definition 1 Let&amp;rsquo;s assume the propositional function $P(x)$ of the universe $U$ is given. Universal Quantifier: It is written as $\forall x$ to mean &amp;lsquo;for all $x \in U$&amp;rsquo; and is called the universal sign. Existential Quantifier: It is written as $\exists x$ to mean &amp;rsquo;there exists at least one $x \in U$ such that&amp;rsquo; and is called the existential sign. Explanation For instance, regarding the set of natural numbers</description></item><item><title>Relationship between Absolutely Continuous and Integrable Functions</title><link>https://freshrimpsushi.github.io/en/posts/1373/</link><pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1373/</guid><description>Buildup Consider the following proposition. Given a measure $\mu$ and $\mu-$ integrable function $f$ on a measurable space $(X,\mathcal{E})$, then there exists a $\nu \ll\mu$ $\nu$ depending on $f$. It&amp;rsquo;s hardly a proof to show it. If we define $\nu$ as follows, we know that such a $\nu$ exists because it satisfies the above conditions and thus is a $\nu \ll\mu$. $$ \nu (E):=\int_{E} f d\mu,\quad E \in \mathcal{E} $$</description></item><item><title>Conditional Probability of Random Variables Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1320/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1320/</guid><description>Definition Given a probability space $( \Omega , \mathcal{F} , P)$. When $\mathcal{G}$ is a sub sigma field of $\mathcal{F}$, the conditional probability of an event $F \in \mathcal{F}$ with respect to $\mathcal{G}$ is defined as $$ P(F | \mathcal{G}) := E ( \mathbb{1}_{F} | \mathcal{G}) $$. The conditional density of $Y$ when $f_{Y | X =x}$ is defined as follows and given $X=x$ is $$ f_{Y | X =</description></item><item><title>Dynamic Programming</title><link>https://freshrimpsushi.github.io/en/posts/1262/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1262/</guid><description>Buildup When solving a problem, if the solution to a larger problem includes the solution to a smaller problem, it is said to have an Optimal Substructure. An easy example of a problem with an optimal substructure is calculating the Fibonacci numbers. The $n$-th Fibonacci number is calculated as $a_{n} = a_{n-1} + a_{n-2}$, thus, the larger problem $a_{n}$ includes the smaller problems $a_{n-1}$ and $a_{n-2}$. A simple way to</description></item><item><title>Lebesgue-Radon-Nikodym Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1371/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1371/</guid><description>Theorem1 A finite measure $\mu$, $\nu$ on a measurable space $(X, \mathcal{E})$ is given. Then, either $\mu \perp \nu$ or there exist $\epsilon&amp;gt;0$, $E \in \mathcal{E}$ satisfying the conditions below. $$ \mu (E) &amp;gt;0 \quad \text{and} \quad \nu (E) \ge \epsilon \mu (E) $$ Explanation Although this theorem does not have a specific name, it is used as an auxiliary lemma when proving the Lebesgue-Radon-Nikodym theorem. It contains quite a</description></item><item><title>Absolute Continuity of the Sign Measure</title><link>https://freshrimpsushi.github.io/en/posts/1369/</link><pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1369/</guid><description>Definitions1 Given a signed measure $\nu$ and a positive measure $\mu$ on a measurable space $(X, \mathcal{E})$, for all $E \in \mathcal{E}$, $$ \mu (E) = 0 \implies \nu (E) = 0 $$ then $\nu$ is absolutely continuous with respect to $\mu$, denoted as $\nu \ll \mu$. Explanation Absolute continuity This is a generalization of absolute continuity for measures. Like measures that are absolutely continuous, the following equivalent condition holds.</description></item><item><title>Definition of Sets and Propositional Functions</title><link>https://freshrimpsushi.github.io/en/posts/1316/</link><pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1316/</guid><description>Definitions 1 Set: A collection of distinct objects that are the subjects of our intuition or thought is called a set. Element: An object that belongs to a set is called an element. Propositional Function: For an element $x$ of the set $U$, a proposition $p(x)$ that is either true or false is called a propositional function in $U$. Explanation In mathematics, the concept of a set is as important</description></item><item><title>Conditional Expectation of Random Variables Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1315/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1315/</guid><description>Definition Let&amp;rsquo;s assume a probability space $( \Omega , \mathcal{F} , P)$ is given. If $\mathcal{G}$ is a sub sigma-field of $\mathcal{F}$ and the random variable $X \in \mathcal{L}^{1} ( \Omega )$ is integrable, for all $A \in \mathcal{G}$, $$ \int_{A} Y d P = \int_{A} X d P $$ a $\mathcal{G}$-measurable random variable $Y$ uniquely exists satisfying the above, then $Y := E ( X | \mathcal{G} )$ is</description></item><item><title>Radon-Nikodym Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/1312/</link><pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1312/</guid><description>Theorem 1 If two sigma-finite measure $\nu$, $\mu$ in a measure space $( \Omega , \mathcal{F} )$ satisfy $\nu \ll \mu$, then for all $A \in \mathcal{F}$, there exists a unique $\mathcal{F}$-measurable function $h$ satisfying $h \ge 0$ almost everywhere according to $\mu$ and $$ \nu (A) = \int_{A} h d \mu $$. That $h$ is almost everywhere according to $\mu$ means $\mu \left( h^{-1} ( -\infty , 0 )</description></item><item><title>Total Variation</title><link>https://freshrimpsushi.github.io/en/posts/1367/</link><pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1367/</guid><description>Definition1 A total variation $| \nu |$ of a signed measure $\nu$ on a measurable space $(X, \mathcal{E})$ is defined as follows. $$ |\nu |= \nu^{+} +\nu^{-} $$ Here, $\nu=\nu^{+}-\nu^{-}$ is the Jordan decomposition of $\nu$. Explanation $\nu^{+}$ and $\nu^{-}$ are called the positive variation and negative variation of $\nu$, respectively. The Jordan decomposition and the total variation for a measure are exactly the same as the method of expressing</description></item><item><title>Radon-Nikodym Derivative</title><link>https://freshrimpsushi.github.io/en/posts/1307/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1307/</guid><description>Theorem 1 Assuming a measurable space $( \Omega , \mathcal{F} )$ is given with measures $\mu$, $\nu$ satisfying $\mu ( \Omega ) = 1$ for all $F \in \mathcal{F}$ and $0 \le \nu (F) \le \mu (F)$, there exists a $\mathcal{F}$-measurable function $h : \Omega \to \mathbb{R}$ for all $F \in \mathcal{F}$ that satisfies $$ \nu (F) = \int_{F} h d \mu $$ This $h$, denoted as $\displaystyle h :=</description></item><item><title>Reasons to Be Careful When Using Recursive Functions</title><link>https://freshrimpsushi.github.io/en/posts/1254/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1254/</guid><description>Caution When you first learn programming, regardless of the language, there&amp;rsquo;s a common caution that &amp;lsquo;recursion should be used carefully.&amp;rsquo; This is because recursion isn&amp;rsquo;t as frequently utilized technique, and the reason behind this caution is often left unexplained. This can lead to confusion for learners on why such a useful method is discouraged. Let&amp;rsquo;s delve into the examples to understand it better. Example def fibo1(n) : if n==1 or</description></item><item><title>Finite Sigma Measures</title><link>https://freshrimpsushi.github.io/en/posts/1314/</link><pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1314/</guid><description>Definitions 1 Let a measurable space $( X , \mathcal{E} )$ be given. If $\mu (X) &amp;lt; \infty$, then $\mu$ is called a finite measure. When $$\displaystyle X = \bigcup_{i=1}^{\infty} E_{i} \qquad , E_{i} \in \mathcal{E}$$ for all $i \in \mathbb{N}$ such that $\mu ( E_{i} ) &amp;lt; \infty$, it is called a sigma-finite measure. Also, the ordered pair $(X, \mathcal{E}, \mu)$ is called a sigma-finite measure space. If for</description></item><item><title>Selecting Specific Rows and Columns in a Matrix in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1362/</link><pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1362/</guid><description>Methods $m \times n$ Given a data in the form of a matrix, let&amp;rsquo;s call it $A$. If you want to use only a specific part of matrix $A$, you can use the following method. B=A(a:b, c:d) Running the code as above, $B$ becomes a $(b-a) \times (d-c)$ matrix containing the data from row $a$ to $b$, column $c$ to $d$ of matrix $A$. Below is the example code and</description></item><item><title>Mathematical Induction</title><link>https://freshrimpsushi.github.io/en/posts/118/</link><pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/118/</guid><description>Law 1 $$ \left[ p(1) \land \left( p(n) \implies p(n+1) \right) \right] \implies \forall n \in \mathbb{N} : p(n) $$ Regarding the proposition $p(n) (n=1,2,3, \cdots )$, if $p(1)$ is true and assuming $p(n)$, then if $p(n+1)$ holds, $p(n)$ is true. Explanation When a formula holds for natural numbers, a powerful proof method called Peano&amp;rsquo;s Fifth Axiom is especially potent, or simply called induction without the &amp;lsquo;mathematical&amp;rsquo; prefix. Originally, induction</description></item><item><title>Mathematical Logic Proof of Syllogism</title><link>https://freshrimpsushi.github.io/en/posts/191/</link><pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/191/</guid><description>Law 1 The following hypothetical proposition is called a syllogism. $$ ( p \to q ) \land ( q \to r ) \implies p \to r $$ Explanation There is hardly anyone who does not know about the syllogism, and it is believed there&amp;rsquo;s no need for explicit explanation. Except in ancient philosophical debates, it&amp;rsquo;s rare to use the phrase &amp;lsquo;by syllogism.&amp;rsquo; That&amp;rsquo;s because it is a familiar method of</description></item><item><title>Matrix Functions, Definition of Matrix Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/1342/</link><pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1342/</guid><description>Definitions1 $$ \mathbf{x}(t) = \begin{pmatrix} x_{1}(t) \\ \vdots \\ x_{n}(t) \end{pmatrix},\quad \mathbf{A}(t) = \begin{pmatrix} a_{11}(t) &amp;amp; \cdots &amp;amp; a_{1m}(t) \\ \vdots &amp;amp; &amp;amp; \vdots \\ a_{n1}(t) &amp;amp; \cdots &amp;amp; a_{nm}(t) \end{pmatrix} $$ If each element of a matrix is a function of variable $t$, it is called a matrix function. All elements of $\mathbf{A}(t)$, i.e., all $a_{ij}$ being continuous at a given point (or interval) means $\mathbf{A}(t)$ is continuous. If</description></item><item><title>Absolute Continuity of Measures</title><link>https://freshrimpsushi.github.io/en/posts/1309/</link><pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1309/</guid><description>Definition 1 Let&amp;rsquo;s assume given a measurable space $( \Omega , \mathcal{F} )$. If measures $\nu$, $\mu$ satisfy $$ \mu (A) = 0 \implies \nu (A) = 0 $$ for all $A \in \mathcal{F}$, then $\nu$ is said to be absolutely continuous with respect to $\mu$ and is denoted by $\nu \ll \mu$. Explanation As the notation $\nu \ll \mu$ suggests, $\mu$ has a strong sense of &amp;lsquo;dominating&amp;rsquo; over $\nu$.</description></item><item><title>Chain Rule for Fréchet Derivatives</title><link>https://freshrimpsushi.github.io/en/posts/1334/</link><pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1334/</guid><description>Theorem Let&amp;rsquo;s assume $(X, \left\| \cdot \right\|_{X}), (Y, \left\| \cdot \right\|_{Y}), (Z, \left\| \cdot \right\|_{Z})$ is a Banach space. Let $\Omega \subset X$, $U \subset Y$ be open sets. And functions $F : \Omega \to Y$, $G : U \to Z$ are given. Then, $F(\Omega) \subset U$ is satisfied. Now, let&amp;rsquo;s assume $F$ is differentiable at $x\in\Omega$ in the sense of [Fréchet]</description></item><item><title>Mathematical Logic Proof by Contradiction</title><link>https://freshrimpsushi.github.io/en/posts/117/</link><pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/117/</guid><description>Law 1 $$ (p \land \lnot q) \to c \iff p \to q $$ $c$ implies a contradiction. Explanation Reductio ad absurdum or proof by contradiction is a proof technique that is widely used throughout mathematics. However, those who encounter this method for the first time may find the term unfamiliar and may resist it. Or there might be people who, although they have become accustomed to it, do not</description></item><item><title>Mathematical Proof of the Counterfactual</title><link>https://freshrimpsushi.github.io/en/posts/116/</link><pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/116/</guid><description>Law 1 $$ p \to q \iff \lnot q \to \lnot p $$ Explanation If a proposition is true, then its contrapositive is also true; if a proposition is false, then its contrapositive is also false. Of course, if the converse holds, then the inverse of the original proposition also holds through contraposition. These expressions might be too difficult for those not familiar with mathematics. Let&amp;rsquo;s understand it through an</description></item><item><title>Partition and Refinement of Measurable Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1313/</link><pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1313/</guid><description>Definition Given a measurable space $( \Omega , \mathcal{F} )$. A finite (measurable) partition of the measurable space $\Omega$ is defined as $$\mathcal{P} : = \left\{ A_{i} \in \mathcal{F} : i_{1} \ne i_{2} \implies A_{i_{1}} \cap A_{i_{2}} = \emptyset \right\}_{i=1}^{k}$$ that satisfies $\displaystyle \bigsqcup_{i=1}^{k} A_{i} = \Omega$ for $( \Omega , \mathcal{F} )$. If there exist $B_{j} \in \mathcal{P} ' $ that satisfy $\displaystyle A_{i} = \bigsqcup_{j \in J} B_{j}$</description></item><item><title>Fréchet Derivative</title><link>https://freshrimpsushi.github.io/en/posts/1332/</link><pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1332/</guid><description>Definition Given two Banach spaces $X, Y$ and an open set $\Omega \subset X$. Then a function $F : \Omega \to Y$ is said to be Frechet differentiable at $x\in \Omega$ if there exists a bounded linear operator $L : X \to Y$ that satisfies the following condition: $$ \lim \limits_{ \left\| y \right \| \to 0} \frac{\| F(x+y) -F(x)-Ly \|}{\|y\|}=0 $$ In this case, such a linear transformation $L$</description></item><item><title>How to Specify Colors, Line Styles, and Marker Types in MATLAB Graphs</title><link>https://freshrimpsushi.github.io/en/posts/1330/</link><pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1330/</guid><description>Properties The properties of a graph can be specified as follows. Graph Color Marker Line Style Red r Dot . Solid - Green g Star * Dotted : Blue b X x Dash-dot -. Black k Circle o (letter o) Dashed -- Yellow y Plus + Magenta m Square s White w Diamond d Cyan c Star p Triangle down v Triangle up ^ Triangle left &amp;lt; Triangle right &amp;gt;</description></item><item><title>Proof of De Morgan's Laws</title><link>https://freshrimpsushi.github.io/en/posts/1306/</link><pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1306/</guid><description>Theorem 1 [1] De Morgan&amp;rsquo;s Laws: $$ \lnot (p \land q) \iff \lnot p \lor \lnot q \\ \lnot(p \lor q) \iff \lnot p \land \lnot q $$ [2] De Morgan&amp;rsquo;s Theorem: $$ (A \cup B)^{c} = A^{c} \cap B^{c} \\ (A \cap B)^{c} = A^{c} \cup B^{c} $$ Description De Morgan&amp;rsquo;s Laws and De Morgan&amp;rsquo;s Theorems refer to propositions and sets, respectively, but in everyday language, they are often</description></item><item><title>Joint and Marginal Distributions Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1305/</link><pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1305/</guid><description>Definition 1 Let&amp;rsquo;s assume that a probability space $( \Omega , \mathcal{F} , P)$ is given. Joint Distribution: If there are two random variables $X$ and $Y$ defined in $( \Omega , \mathcal{F} , P)$, the distribution of the random vector $(X,Y) : \Omega \to \mathbb{R}^2$ for a Borel set $B \subset \mathcal{B} \left( \mathbb{R}^2 \right)$ is defined as $$ \begin{align*} P_{(X,Y)} (B) :=&amp;amp; P \left( (X,Y) \in B \right)</description></item><item><title>Rotating an Image in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1328/</link><pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1328/</guid><description>Method imrotate(I,angle,method,bbox) I: Image to be rotated. angle: The angle of rotation in degrees. method: The interpolation method. Options are &amp;rsquo;nearest&amp;rsquo;, &amp;lsquo;bilinear&amp;rsquo;, &amp;lsquo;bicubic&amp;rsquo;. If nothing is specified, &amp;rsquo;nearet&amp;rsquo; is applied. X = phantom(&amp;#39;Modified Shepp-Logan&amp;#39;,64); figure() imagesc(X) title(&amp;#39;X&amp;#39;) Y1=imrotate(X,30,&amp;#39;nearest&amp;#39;,&amp;#39;crop&amp;#39;); Y2=imrotate(X,30,&amp;#39;bilinear&amp;#39;,&amp;#39;crop&amp;#39;); Y3=imrotate(X,30,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); figure() subplot(1,3,1) imagesc(Y1) title(&amp;#39;Y1 - nearest&amp;#39;) subplot(1,3,2) imagesc(Y2) title(&amp;#39;Y2 - bilinear&amp;#39;) subplot(1,3,3) imagesc(Y3) title(&amp;#39;Y3 - bicubic&amp;#39;) bbox: Specifies the size of the output image. &amp;rsquo;loose&amp;rsquo; enlarges the size of the</description></item><item><title>Contrapositive and Converse Propositions</title><link>https://freshrimpsushi.github.io/en/posts/1304/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1304/</guid><description>Definition 1 A proposition that is true for all logical possibilities is called a Tautology. A proposition that is false for all logical possibilities is called a Contradiction. For $p$, $q$, if the conditional statement $p \to q$ is a tautology, it is called an Implication and represented as follows: $$ p \implies q $$ For $p$, $q$, if the biconditional statement $p \leftrightarrow q$ is a tautology, it is</description></item><item><title>Creating Special Matrices in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1327/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1327/</guid><description>Zero Matrix zeros(): Returns a zero matrix. zeros(n): Returns a $n\times n$ zero matrix. zeros(m,n): Returns a $n\times m$ zero matrix. zeros(size(A)): Returns a zero matrix of the same size as matrix A. Matrix with All Elements as 1 ones(): Returns a matrix where all elements are 1. However, for operations between two matrices, it&amp;rsquo;s more convenient to just use 1. It&amp;rsquo;s obvious that the code below is much simpler</description></item><item><title/><link>https://freshrimpsushi.github.io/en/posts/1300/</link><pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1300/</guid><description>Definition 1 Probability Space $( \Omega , \mathcal{F} , P)$ is given. For Random Variables $X$ and $t \in \mathbb{R}$, the function defined as follows $\varphi_{X} (t)$ is called the characteristic function of $X$. $$ \varphi_{X} (t) := E \left( e^{i t X} \right) = \int_{\mathbb{R}} e^{it x} f_{X} (x) dx $$ If you haven&amp;rsquo;t encountered measure theory yet, you can ignore the term &amp;ldquo;probability space.&amp;rdquo; Explanation The random variable</description></item><item><title>How to Perform Element-wise Operations on Two Matrices in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1326/</link><pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1326/</guid><description>Multiplication times(), .*: Returns the result of multiplying each element of two matrices. The operation can only proceed if the two matrices are of the exact same size, or one of them is a scalar, or if one is a row vector with the same row size, or a column vector with the same column size. If the sizes are different, the smaller matrix is treated as if it were</description></item><item><title>Propositions and Connectives, Truth Tables</title><link>https://freshrimpsushi.github.io/en/posts/1299/</link><pubDate>Sun, 22 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1299/</guid><description>Definitions 1 A proposition is a statement that is either true or false. A proposition has one of two truth values, true or false. Two propositions $p$, $q$ are said to be (logically) equivalent if they have the same truth value, and this is represented as $p \equiv q$. The following symbols are referred to as connectives and are methods for forming compound propositions: Negation: $\lnot$ Conjunction: $\land$ Disjunction: $\lor$</description></item><item><title>Uniform C^m-Regularity Condition</title><link>https://freshrimpsushi.github.io/en/posts/1324/</link><pubDate>Sun, 22 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1324/</guid><description>Definition1 If there exists a locally finite open cover $\left\{ U_{j} \right\}$ of $\mathrm{bdry}\Omega$, and a sequence $\left\{ \Phi_{j} \right\}$ of $m$-smooth transformations taking $U_{j}$ onto the ball $B=\left\{ y\in \mathbb{R}^n : |y| \lt 1 \right\}$, with an inverse transformation $\Psi _{j}=\Phi_{j}^{-1}$ existing and satisfying $\text{(i)}$ ~ $\text{(iv)}$, then the open set $\Omega \subset \mathbb{R}^n$ satisfies the uniform $C^{m}$-regularity condition. $\text{(i)}$ For any $\delta &amp;gt;0$, $\Omega_{&amp;lt;\delta}$$\subset \bigcup \nolimits_{j=1}^\infty \Psi \Big(</description></item><item><title>Expected Value Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1294/</link><pubDate>Sat, 21 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1294/</guid><description>Definition 1 Let us assume that a probability space $( \Omega , \mathcal{F} , P)$ is given. The $E(X)$, defined as follows for a random variable $X$, is referred to as the (mathematical) expected value of $X$. $$ E(X) := \int_{\Omega} X d P $$ If you haven&amp;rsquo;t encountered measure theory yet, you can disregard the term probability space. Explanation The definition of expected value, however complex it might seem</description></item><item><title>Matrix Size and Related Functions in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1323/</link><pubDate>Sat, 21 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1323/</guid><description>Functions size(): Returns a row vector that contains the lengths of the rows and columns of the matrix. It is useful for creating a zero matrix of the same size as the matrix being dealt with. zeros(size(A)): Returns a zero matrix of the same size as A. length(): Returns the larger number among the rows and columns. In the case of row vectors and column vectors, it is the same</description></item><item><title>Dirac Measure and Discrete Probability Distribution Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/879/</link><pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/879/</guid><description>Overview In basic probability theory, a probability distribution was either discrete or continuous, and its explanation had to rely somewhat on intuition. However, with the introduction of measure theory, discrete probability distributions can be defined cleanly without any mathematical ambiguity. Discrete Probability Distribution 1 Assume that a probability space $( \Omega , \mathcal{F} , P)$ is given. Step 1. When the random variable $X$ takes only one value When considering</description></item><item><title>Strong Local Lipschitz Condition</title><link>https://freshrimpsushi.github.io/en/posts/1319/</link><pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1319/</guid><description>Definition1 If there exists a locally finite open cover $\left\{ U_{j} \right\}$ of $\delta \gt 0$, $M \gt 0$, and $\mathrm{bdry}\Omega$, such that for each $j$, there is a real-valued function $f_{j}$ with $n-1$ variables satisfying $\text{(i)}$ ~ $\text{(iv)}$, then the open set $\Omega \subset \mathbb{R}^n$ satisfies the strong local Lipschitz condition. For all pairs $x,y\in$ $\Omega_{\lt \delta}$ that satisfy $|x-y| \lt \delta$, there exists $j$ that satisfies the condition</description></item><item><title>Eisenstein Prime Number Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/1293/</link><pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1293/</guid><description>Theorem An Eisenstein prime is an irreducible element of the Eisenstein ring. An Eisenstein integer $\pi \in \mathbb{Z}[ \omega ]$ is an Eisenstein prime if it satisfies one of the following conditions: (i): $\pi = 1 + \omega 2$ (ii): For some prime $p \in \mathbb{Z}$, $p \equiv 2 \pmod{3}$ of $\pi = p$ (iii): For some prime $p \in \mathbb{Z}$, when $p \equiv 1 \pmod{3}$, $p = u^2 -</description></item><item><title>Uniform Cone Condition</title><link>https://freshrimpsushi.github.io/en/posts/1318/</link><pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1318/</guid><description>Definition1 If there exists a locally finite open cover $\left\{ U_{j} \right\}$ of the boundary of $\Omega$ and a corresponding sequence of finite cones $\left\{ C_{j} \right\}$ that satisfy $\text{(i)}$ ~ $\text{(iv)}$, then the open set $\Omega \subset \mathbb{R}^n$ is said to satisfy the uniform cone condition. $\text{(i)}$ There exists $M \lt \infty$ such that every $U_{j}$ has a diameter smaller than $M$. $\text{(ii)}$ For some $\delta \gt 0$ $\Omega_{\lt</description></item><item><title>Density and Cumulative Distribution Functions of Random Variables Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1292/</link><pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1292/</guid><description>Definition 1 A probability space $( \Omega , \mathcal{F} , P)$ is given, and let&amp;rsquo;s say $m$ is a measure. For an integrable $f \ge 0$, if measure $P : \mathcal{F} \to \mathbb{R}$ has the form of $$ A \mapsto P(A) = \int_{A} f dm $$ then $P$ is said to be absolutely continuous. In particular, such $f$ is called the density of $P$ with respect to measure $m$. The</description></item><item><title>Sets Outside/Inside a Certain Distance from the Boundary of a Set</title><link>https://freshrimpsushi.github.io/en/posts/1317/</link><pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1317/</guid><description>Definition Let us assume an open set $\Omega \subset \mathbb{R}^n$ is given. Then, $\Omega_{&amp;lt;\delta}$ and $\Omega_{&amp;gt;\delta}$ are defined as follows. $$ \begin{align*} \Omega_{&amp;lt;\delta} :=&amp;amp; \left\{ x\in\Omega : \mathrm{dist}(x, \mathrm{bdry}\Omega)&amp;lt;\delta \right\} \\ \Omega_{&amp;gt;\delta} :=&amp;amp; \left\{ x\in\Omega : \mathrm{dist}(x, \mathrm{bdry}\Omega)&amp;gt;\delta \right\} \end{align*} $$ Explanation Such sets are usefully employed in partial differential equations, functional analysis, etc. Depending on the textbook, there are cases where it&amp;rsquo;s $\Omega_\delta=\Omega_{&amp;lt;\delta}$1 and cases where it&amp;rsquo;s $\Omega_\delta=\Omega_{&amp;gt;\delta}$2. In</description></item><item><title>Eisenstein Ring's Norm</title><link>https://freshrimpsushi.github.io/en/posts/1291/</link><pubDate>Tue, 17 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1291/</guid><description>Theorem Let&amp;rsquo;s consider a function on the Eisenstein ring $\mathbb{Z}[ \omega ]$. [1]: If defined as $N(x + \omega y) := x^2 - xy + y^2$, then $N$ becomes the multiplicative norm of $\mathbb{Z}[ \omega ]$. [2]: $\mathbb{Z}[ \omega ]$ is a Euclidean domain. [3]: The only unit of $\mathbb{Z}[ \omega ]$ is $\pm 1, \pm \omega, \pm \omega^2 $. Description The study of Eisenstein integers can be greatly facilitated</description></item><item><title>Jordan Decomposition Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1311/</link><pubDate>Tue, 17 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1311/</guid><description>Theorem Let&amp;rsquo;s assume a measurable space $(X,\mathcal{E})$ and a signed measure $\nu$ defined on it. Then, there uniquely exist two positive measures $\nu^{+}$, $\nu^{-}$ that satisfy the following conditions, and $\nu=\nu^{+}-\nu^{-}$ is called the Jordan decomposition of $\nu$. $$ \nu=\nu^{+}-\nu^{-} $$ $$ \nu^{+} \perp \nu^{-} $$ If $X=P \cup N$ is called a partition, then $\nu^{+}, \nu^{-}$ is as follows. $$ \begin{align*} \nu^{+} (E) &amp;amp;= \nu ( E \cap P)</description></item><item><title>Mutually Singular</title><link>https://freshrimpsushi.github.io/en/posts/1310/</link><pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1310/</guid><description>Definition1 Given two signed measures $\nu$, $\nu$. If there exists a $E,F\ \in \mathcal{E}$ that satisfies the following three conditions for $\nu$, $\mu$, we say that the two signed measures $\nu$, $\mu$ are and denote it as $\nu \perp \mu$ or $\mu \perp \nu$: $E \cup F=X$ $E \cap F=\varnothing$ $E$ is a null set with respect to $\nu$, and $F$ is a null set with respect to $\mu$. Also,</description></item><item><title>Random Variables Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1290/</link><pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1290/</guid><description>Definition 1 Let there be a given probability space $( \Omega , \mathcal{F} , P)$. Probability variables $X$, $Y$ are said to be independent if for all Borel sets $B_{1} , B_{2} \in \mathcal{B} ( \mathbb{R} )$ the following holds true: $$ P \left( X^{-1} (B_{1} ) \cap Y^{-1} (B_{2} ) \right) = P \left( X^{-1} (B_{1}) \right) P \left( Y^{-1} (B_{2}) \right) $$ If you have not yet encountered</description></item><item><title>Eisenstein Integer</title><link>https://freshrimpsushi.github.io/en/posts/1289/</link><pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1289/</guid><description>Definition Equation $\mathbb{Z} [ \omega ] := \left\{ a + \omega b : a, b \in \mathbb{Z} \right\}$ is called the Eisenstein Ring, and its elements are referred to as Eisenstein Integers. Theorem [1]: $\overline{ \omega } = \omega^{2} = - (1 + \omega)$ [2]: $( a \pm \omega b ) + ( c \pm \omega d) = (a \pm c) + \omega (b \pm d)$ [3]: $( a +</description></item><item><title>Hahn Decomposition Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1308/</link><pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1308/</guid><description>Theorem1 (a) Let $\nu$ be a signed measure defined on a measurable space $(X, \mathcal{E})$. Then there exist a positive set $P$ and a negative set $N$ for $\nu$, satisfying the following: $$ P \cup N=X \quad \text{and} \quad P \cap N =\varnothing $$ Such a $X=P \cup N$ is called a Hahn decomposition for $\nu$. (b) Let $P^{\prime}, N^{\prime}$ be another pair of sets satisfying (a). Then the following</description></item><item><title>Positive Set, Negative Set, Null Set</title><link>https://freshrimpsushi.github.io/en/posts/1303/</link><pubDate>Sat, 14 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1303/</guid><description>Definition1 Let us call $\nu$ on $(X,\mathcal{E})$ a sign measure. And let us denote $E,F \in \mathcal{E}$. Then When $\nu (F) \ge 0,\ \forall F\subset E$, we call $E$ regarding $\nu$ a positive set or simply positive. When $\nu (F) \le 0,\ \forall F\subset E$, we call $E$ regarding $\nu$ a negative set or simply negative. When $\nu (F)=0,\ \forall F\subset E$, we call $E$ regarding $\nu$ a null set</description></item><item><title>Probability Variables and Probability Distributions Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/1288/</link><pubDate>Sat, 14 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1288/</guid><description>Definition 1 Let&amp;rsquo;s assume a Probability Space $( \Omega , \mathcal{F} , P)$ is given. A function $X : \Omega \to \mathbb{R}$ that satisfies $X^{-1} (B) \in \mathcal{F}$ for every Borel Set $B \in \mathcal{B} (\mathbb{R})$ is called a Random Variable. $\mathcal{F}_{X}$ defined as follows is called the Sigma Field generated by $X$. $$ \mathcal{F}_{X} := X^{-1} ( \mathcal{B} ) = \sigma (X) = \left\{ X^{-1} (B) \in \Omega :</description></item><item><title>Proof of the Gaussian Prime Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1285/</link><pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1285/</guid><description>Theorem 1 A Gaussian integer $\pi \in \mathbb{Z}[i]$ is called a Gaussian prime if it satisfies one of the following conditions. (i): $\pi = 1 + i$ (ii): For a prime $p \in \mathbb{Z}$, $p \equiv 3 \pmod{4}$ for $\pi = p$ (iii): For a prime $p \in \mathbb{Z}$, when $p \equiv 1 \pmod{4}$, $p = u^2 + v^2$ for $\pi = u + iv$ (iv): For $\pi$ corresponding to</description></item><item><title>Signed Measures</title><link>https://freshrimpsushi.github.io/en/posts/1301/</link><pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1301/</guid><description>Definition1 Let $(X, \mathcal{E})$ be a measurable space. A function $\nu : \mathcal{E} \to \overline{\mathbb{R}}$ which takes extended real values and satisfies the conditions below is called a signed measure. $\nu ( \varnothing ) = 0$ At most one of $\pm \infty$ can have a function value of $\nu$. In other words, if $-\infty \in \nu (\mathcal{E})$ then $+\infty \notin \nu (\mathcal{E})$, and if $+\infty \in \nu (\mathcal{E})$ then $-\infty</description></item><item><title>General Definitions of Measure</title><link>https://freshrimpsushi.github.io/en/posts/1302/</link><pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1302/</guid><description>Definition Let $(X,\mathcal{E})$ be a measurable space. A function $\mu : \mathcal{E} \to \overline{\mathbb{R}}$ that takes extended real values is called a measure if it satisfies the following three conditions: (a) $\mu ( \varnothing ) = 0$ (b) $\mu (E) \ge 0,\quad \forall E\in \mathcal{E}$ (c) Suppose $\left\{E_{j}\right\}$ are sequences of mutually disjoint sets in $\mathcal{E}$. Then the following holds: $$ \mu \left( \bigcup _{j=1}^\infty E_{j} \right) =\sum \limits_{j=1}^\infty \mu</description></item><item><title>Strassen Algorithm Proof</title><link>https://freshrimpsushi.github.io/en/posts/1284/</link><pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1284/</guid><description>Algorithm Let&amp;rsquo;s say for $k \in \mathbb{N}$ there is $n=2^{k}$. For $A, B \in \mathbb{R}^{n \times n}$, using the Jordan block matrix representation, let&amp;rsquo;s consider the following 8 matrices ${{n} \over {2}} \times {{n} \over {2}}$, $A_{i}$, and $B_{i}$. $$ AB= \begin{bmatrix} A_{1} &amp;amp; A_{2} \\ A_{3} &amp;amp; A_{4} \end{bmatrix} \begin{bmatrix} B_{1} &amp;amp; B_{2} \\ B_{3} &amp;amp; B_{4} \end{bmatrix} = \begin{bmatrix} C_{1} &amp;amp; C_{2} \\ C_{3} &amp;amp; C_{4} \end{bmatrix} =</description></item><item><title>Line Segment Conditions</title><link>https://freshrimpsushi.github.io/en/posts/1298/</link><pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1298/</guid><description>Definition1 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. For all $x \in \mathrm{bdry}\Omega$, if there exists a neighborhood $x$ of $U_{x}$ and a nonzero vector $y_{x}$ such that the following condition is satisfied, then $\Omega$ satisfies the segment condition. $$ z\in \overline{\Omega}\cap U_{x} \quad \implies \quad z+ty_{x} \in \Omega, 0 \lt t \lt 1 $$ Robert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p82&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Time Complexity and Space Complexity</title><link>https://freshrimpsushi.github.io/en/posts/1283/</link><pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1283/</guid><description>Definition The time it takes to solve a given problem is called time complexity, and the memory requirements are referred to as space complexity. Example Asymptotic notation is a very useful means for expressing these. Let&amp;rsquo;s take a look at an example of time complexity. Constant Time $O(1)$ Algorithms that can finish regardless of $n$, essentially taking no time. For example, finding the third element in $\mathbf{x} = [4,3,8,-1,-9,0,5,7,2,6]$ just</description></item><item><title>Analyzing Time Series with Valuation Models in R</title><link>https://freshrimpsushi.github.io/en/posts/1282/</link><pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1282/</guid><description>Practice Value model serves as a useful tool for explaining archetypes, and the analytical procedure itself is similar to that of the ARMA model. The graph above shows the German DAX index from 1991 to 1999, drawn by extracting only DAX from EuStockMarkets. When we look at the square of returns, there seems to be an ARCH effect almost certainly. To check if the squared returns follow the ARMA model,</description></item><item><title>Weak Cone Condition</title><link>https://freshrimpsushi.github.io/en/posts/1297/</link><pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1297/</guid><description>Definition1 Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. Suppose a point $x \in \Omega$ is given. Let $R(x)$ be the set of all $y$ that ensure the line segment from $x$ to $y \in \Omega$ is included within $\Omega$ again. That is, $R(x)$ is the set of points on all lines starting from $x$ within $\Omega$. And let $\Gamma (x)$ be defined as follows. $$ \begin{align*} \Gamma (x)</description></item><item><title>Asymptotic Notation of the Cost of Algorithms</title><link>https://freshrimpsushi.github.io/en/posts/1281/</link><pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1281/</guid><description>Definition The cost of an algorithm for data of size $n$ is denoted as follows: $O$ notation: $$ O(g(n)) := \left\{ f(n) \ | \ \exists c &amp;gt; 0, n_{0} \in \mathbb{N} : \forall n \ge n_{0} \implies f(n) \le c g(n) \right\} $$ $\Omega$ notation: $$ \Omega (g(n)) := \left\{ f(n) \ | \ \exists c &amp;gt; 0, n_{0} \in \mathbb{N} : \forall n \ge n_{0} \implies f(n) \ge</description></item><item><title>Locally Finite Covers</title><link>https://freshrimpsushi.github.io/en/posts/1295/</link><pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1295/</guid><description>Definition1 English An open cover $\mathcal{O}$ of a set $S \subset \mathbb{R}^n$ is said to be locally finite if any compact set in $\mathbb{R}^n$ can intersect at most finitely many members of $\mathcal{O}$. Explanation Even an infinite cover can be locally finite. By definition, a locally finite cover is at most countable, and a finite set is naturally locally finite. Furthermore, if $S$ is closed, then any open cover of</description></item><item><title>Under Cone Conditions</title><link>https://freshrimpsushi.github.io/en/posts/1296/</link><pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1296/</guid><description>Definition1 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. If there exists some finite cone such that for each $x \in \Omega$, there exists a finite cone $C_{x} \subset \Omega$ having $x$ as its vertex, then $\Omega$ satisfies the cone condition. Explanation For all $x\in \Omega$, if there exists $C_{x} \in \Omega$ as shown in the figure above, then $\Omega$ satisfies the cone condition. If $\Omega$ includes a pointed part</description></item><item><title>Time Series Analysis in Valuation Models</title><link>https://freshrimpsushi.github.io/en/posts/1280/</link><pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1280/</guid><description>Model 1 The Value-at-Risk (VaR) Model generalizes the ARCH model and is used for time series analysis to detect heteroskedasticity. $$ (1 - \beta{1} B - \cdots - \beta_{p} B^p) \sigma_{t | t-1}^2 = \omega + (\alpha_{1} B + \cdots + \alpha_{q} B^q) r_{t}^{2} $$ Derivation Let&amp;rsquo;s start with the simplest $ARCH(1)$ model. 2 Given time series data $\left\{ p_{t} \right\}$ and its returns $\left\{ r_{t} \right\}$, the data exhibiting</description></item><item><title>Finite Cone</title><link>https://freshrimpsushi.github.io/en/posts/1287/</link><pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1287/</guid><description>Definition1 Let $v$ be a unit vector2 in $\mathbb{R}^n$. For each non-zero $x\in \mathbb{R}^n$, let $\angle(x,v)$ be the angle between two vectors $v,x$. Then, for given $v$, $\rho \gt 0$, $0 \lt \kappa \le \pi$, the set $C$ is called a finite cone of height $\rho$, axis direction $v$, and aperture angle $\kappa$ with the vertex at the origin. $$ C= \left\{ x \in \mathbb{R}^n \ \ \big| \ \</description></item><item><title>McLeod-Li Test</title><link>https://freshrimpsushi.github.io/en/posts/1279/</link><pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1279/</guid><description>Hypothesis Testing Given the time series data returns $\left\{ r_{t} \right\}$, let&amp;rsquo;s assume: $H_{0}$: The data does not exhibit autoregressive conditional heteroscedasticity (ARCH) effect at lag $k$. $H_{1}$: The data exhibits ARCH effect at lag $k$. Explanation McLeod-Li Test is used to check for ARCH effects in the given returns. Code Practice Fortunately, in R, the TSA package’s McLeod.Li.test() function allows for</description></item><item><title>Arch Effect</title><link>https://freshrimpsushi.github.io/en/posts/1278/</link><pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1278/</guid><description>Definition 1 The term ARCH effect refers to the &amp;lsquo;AutoRegressive Conditional Heteroscedasticity,&amp;rsquo; which literally translates to &amp;lsquo;autoregressive conditional heteroscedastic effect.&amp;rsquo; Therefore, it is not neutralized because it is interpreted as such. Description In simpler terms, if the volatility of data changes and can be explained by previous data, it is said that the data exhibits the ARCH effect. The model that statistically explains this ARCH effect is called the ARCH</description></item><item><title>Definition of a General Parallelepiped</title><link>https://freshrimpsushi.github.io/en/posts/1286/</link><pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1286/</guid><description>Definition Given $n$ linearly independent vectors $y_{1},\ \cdots,\ y_{n} \in \mathbb{R}^n$, the set $P$ is known as a parallelepiped. $$ P = \left\{ \sum \limits_{j=1}^{n} \lambda_{j} y_{j} \ \ \Big| \quad 0\le \lambda_{j} \le 1 \right\} $$ Description As defined, it includes the origin as a vertex. Simply put, it is the set of all linear combinations with coefficients up to 1. For $n=3$, it forms a parallelepiped, and for</description></item><item><title>Gaussian Ring Norm</title><link>https://freshrimpsushi.github.io/en/posts/1277/</link><pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1277/</guid><description>Theorem 1 Consider the function $N : \mathbb{Z}[i] \to \mathbb{Z}$ for the Gaussian Ring $\mathbb{Z}[i]$. [1]: If defined as $N(x + iy) := x^2 + y^2$, $N$ becomes the multiplicative norm of $\mathbb{Z}[i]$. [2]: $\mathbb{Z}[i]$ is a Euclidean domain. [3]: The only unit of $\mathbb{Z}[i]$ is $1,-1,i,-i$. Description Gaussian integers can be much more comfortably studied with the help of [abstract algebra](../../categories/abstract algebra). By proving [2] with the norm $N$</description></item><item><title>Heteroskedasticity and Volatility Clustering in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1272/</link><pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1272/</guid><description>Definition 1 Given a time series data $\left\{ p_{t} \right\}$. When the variance of $\left\{ p_{t} \right\}$ depends on $t$, $\left\{ p_{t} \right\}$ is said to have Heteroscedasticity. The phenomenon of the variance of $\left\{ p_{t} \right\}$, which has Heteroscedasticity, increasing and decreasing repeatedly is referred to as Volatility Clustering. The following defined $r_{t}$ is referred to as (Log) Return in $t$. $$ r_{t} := \nabla \log p_{t} = \log</description></item><item><title>Gradient of Time Delay</title><link>https://freshrimpsushi.github.io/en/posts/1274/</link><pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1274/</guid><description>Overview The gradient of time delay is as follows. $$ \nabla t_{r}=-\frac{1}{c} \crH $$ Proof Given $\acR = c(t -t_{r})$ and since $t$ is independent of spatial variables, $$ \nabla \cR =\nabla(-c t_{r})=-c \nabla t_{r} $$ Therefore, the gradient of time delay can be computed by calculating $\nabla \cR$. $$ \begin{align} \nabla \cR &amp;amp;= \nabla \sqrt{\bcR \cdot \bcR} \nonumber \\ &amp;amp;= \frac{1}{2\sqrt{\bcR\cdot \bcR}} \nabla (\bcR \cdot \bcR ) \nonumber \\</description></item><item><title>Reading Data Files Quickly in R</title><link>https://freshrimpsushi.github.io/en/posts/1270/</link><pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1270/</guid><description>Overview R essentially provides the read.csv() function for reading csv data, but if you&amp;rsquo;re engaged in practical analysis, its performance is so lacking that it&amp;rsquo;s not worth using. As an alternative, it is highly recommended to use read_csv() from the readr package. read_csv() is written in c++ and can read csv files at a very high speed. Code The following is a measurement of the time taken for read.csv() and</description></item><item><title>Liénard-Wiechert Potentials</title><link>https://freshrimpsushi.github.io/en/posts/1269/</link><pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1269/</guid><description>Overview1 The Liénard-Wiechert potentials for a point charge $q$ moving at a speed $\mathbf{v}$ at the retarded time $t_{r}$ are as follows. $$ \begin{align*} V(\mathbf{r}, t) &amp;amp;= \frac{1}{4\pi \epsilon_{0}} \frac{qc}{ (\cR c -\bcR\cdot \mathbf{v})} \\ \mathbf{A}(\mathbf{r}, t) &amp;amp;= \frac{\mu_{0}}{4 \pi}\frac{qc \mathbf{v} }{(\cR c - \bcR\cdot \mathbf{v} )}=\frac{\mathbf{v}}{c^2}V(\mathbf{r}, t) \end{align*} $$ Here, $\bcR=\mathbf{r} -\mathbf{w}(t_{r})$ is the</description></item><item><title>Gaussian Integers</title><link>https://freshrimpsushi.github.io/en/posts/1267/</link><pubDate>Sat, 31 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1267/</guid><description>Definition 1 $\mathbb{Z} [i] := \left\{ a + i b : a, b \in \mathbb{Z} \right\}$ is called a Gaussian Ring, and its elements are called Gaussian Integers. Theorems [1]: $\overline{i} = i^{3}$ [2]: $( a \pm ib ) + ( c \pm id) = (a \pm c) + i (b \pm d)$ [3]: $( a + ib )( c + id) = (ac - bd) + i (ad +</description></item><item><title>How to Parallel Process in R</title><link>https://freshrimpsushi.github.io/en/posts/1266/</link><pubDate>Fri, 30 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1266/</guid><description>Overview While R is not a language used for its speed, there are definitely times when speed is necessary. Even if the code is well written and tidy, if it takes too long, parallel processing or the use of GPUs is usually considered. At first glance, it might seem like there&amp;rsquo;s not much need for parallel processing in R, but when dealing with big data or conducting large-scale simulations, parallel</description></item><item><title>Dynamic Regression Models</title><link>https://freshrimpsushi.github.io/en/posts/1265/</link><pubDate>Thu, 29 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1265/</guid><description>Model Dynamic regression models are, simply put, models that combine ARIMA models with regression models. Description It is also called ARIMAX $ARIMAX$, which means adding independent variables other than ARIMA $X$. In programming, especially as explained below in the practice section, in R, $X$ is emphasized with xreg. In fact, at this point, formulas are more comfortable than words. Let&amp;rsquo;s say the time series data to be analyzed as the</description></item><item><title>The Zhemengko Equation</title><link>https://freshrimpsushi.github.io/en/posts/1264/</link><pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1264/</guid><description>Overview1 When the distribution of continuous charge changes over time, the electric field is as follows. $$ \mathbf{E} (\mathbf{r},t)=\frac{1}{4\pi \epsilon_{0}} \int \left[ \frac{ \rho (\mathbf{r}^{\prime}, t_{r}) }{\cR ^2} \crH + \frac{ \dot{\rho}(\mathbf{r}^{\prime}, t_{r})}{c\cR}\crH-\frac{\dot{\mathbf{J}}(\mathbf{r}^{\prime},t_{r}) }{c^2 \cR} \right]d\tau^{\prime} $$ When the distribution of continuous current changes over time, the magnetic field is as follows. $$ \mathbf{B}( \mathbf{r}, t) = \dfrac{\mu_{0}}{4\pi} \int \left[ \frac{\mathbf{J}(\mathbf{r}^{\prime},t_{r})}{\cR^2} + \dfrac{ \dot{\mathbf{J}}(\mathbf{r}^{\prime}, t_{r}) } {c\cR} \right]\times \crH d\tau^{\prime}</description></item><item><title>Electric Potential and Electromagnetic Fields</title><link>https://freshrimpsushi.github.io/en/posts/1263/</link><pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1263/</guid><description>Overview1 When the charge and current distribution change over time, the electric field and magnetic field are as follows. $$ \mathbf{E}= -\nabla V-\frac{\partial \mathbf{A}}{\partial t} $$ $$ \mathbf{B} = \nabla \times \mathbf{A} $$ $V$ is the scalar potential, and $\mathbf{A}$ is the vector potential. Description When the charge density $\rho (\mathbf{r}, t)$ and current density $\mathbf{J}(\mathbf{r},t)$ are constant1, knowing the Coulomb&amp;rsquo;s law and the Biot-Savart law allows us to find</description></item><item><title>Time Series Analysis and Innovative Outliers</title><link>https://freshrimpsushi.github.io/en/posts/1260/</link><pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1260/</guid><description>Build-up In the graph above, a significant outlier can be found in September 2001. However, unlike additive outliers, it continues to have an effect thereafter. The number of air passengers was steadily increasing, showing seasonality, but the fear from the 9/11 terrorist attacks can be interpreted as sharply reducing the number of users itself. Definition 1 Outliers that change the landscape of analysis itself are called Innovative Outliers. Practice Such</description></item><item><title>Integral Domain Norm</title><link>https://freshrimpsushi.github.io/en/posts/1259/</link><pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1259/</guid><description>Definition 1 An multiplicative norm $N : D \to \mathbb{Z}$ on an integral domain $D$ with respect to all $\alpha , \beta \in D$ is defined by the following conditions: (i): $N (\alpha) = 0 \iff \alpha = 0$ (ii): $N ( \alpha \beta ) = N ( \alpha ) N ( \beta )$ Theorem Let $p \in \mathbb{Z}$ be a prime. [1]: If a multiplicative norm $N$ is defined</description></item><item><title>Physics Appendix</title><link>https://freshrimpsushi.github.io/en/posts/1257/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1257/</guid><description>Fundamentals Appendix A-1 $$ \begin{align*} \frac{ d |x| } {d x} &amp;amp;= \frac{d \sqrt{x^2} }{d x} \\ &amp;amp;= \frac{d \sqrt{x^2}}{d x^2} \frac{d x^2}{dx} \\ &amp;amp;= \frac{1}{2}\frac{1}{\sqrt{x^2}} \cdot 2x \\ &amp;amp;= \dfrac{1}{|x|}x \end{align*} $$ Electromagnetism Appendix E-1 Go to the original article $$ \delta \big( f(x) \big) =\sum \limits_{x_{0}} \frac{\delta (x-x_{0})}{ \frac{\partial f}{\partial x}\Big|_{x=x_{0}} } $$ In this case, $x_{0}$ is a solution to $f(x)$. Using the fact above, $$ \delta</description></item><item><title>Solution of Schrödinger Equation for Finite Square Well Potential</title><link>https://freshrimpsushi.github.io/en/posts/1261/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1261/</guid><description>Overview Let’s examine how a particle moves when the potential takes the shape of a finite square well as shown in the figure above. The potential $U$ is $$ U(x) = \begin{cases} 0 &amp;amp; x&amp;lt;-a \\ U_{0} &amp;amp; -a &amp;lt; x &amp;lt;a \\ 0 &amp;amp;a&amp;lt;x \end{cases} $$ When the potential is $U(x)$, the time-independent Schrödinger equation</description></item><item><title>Solution of the Schrödinger Equation for a Potential Barrier</title><link>https://freshrimpsushi.github.io/en/posts/1256/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1256/</guid><description>Overview Let’s explore how a particle behaves when the potential is in the form of a wall as shown in the figure above. The potential $U$ is $$ U(x) = \begin{cases} 0 &amp;amp; x&amp;lt;-a \\ U_{0} &amp;amp; -a &amp;lt; x &amp;lt;a \\ 0 &amp;amp;a&amp;lt;x \end{cases} $$ The time-independent Schrödinger equation when the potential is $U(x)$ is</description></item><item><title>Time Series Analysis of Additive Outliers</title><link>https://freshrimpsushi.github.io/en/posts/1258/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1258/</guid><description>Buildup The most noticeable point in the graph above is the huge outlier near February 2015. Such extreme values can have a detrimental effect on analysis. Luckily, it was just a brief, momentary outlier. Definition 1 Outliers that do not change the fluctuation of data itself are called additive outliers. Practice Such additive outliers can be intuitively found, or you can use the detectAO() function of the TSA package. The</description></item><item><title>Conditions for a Function with Extended Real Values to be Measurable</title><link>https://freshrimpsushi.github.io/en/posts/1255/</link><pubDate>Sat, 24 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1255/</guid><description>Theorem1 The necessary and sufficient condition for a function $f : X\rightarrow \overline{\mathbb{R}}$ with values in the extended real numbers to be measurable is as follows, given that σ-algebra $(X,\mathcal{E})$. $$ f\text{ is measurable} \iff \begin{align} &amp;amp; \left\{ x \in X : f(x)=-\infty \right\} \in \mathcal{E} \\ &amp;amp; \left\{ x \in X : \alpha &amp;lt; f(x) &amp;lt; +\infty \right\} \in \mathcal{E}\quad (\forall</description></item><item><title>Step Function and Pulse Function</title><link>https://freshrimpsushi.github.io/en/posts/1248/</link><pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1248/</guid><description>Definition 1 The function defined as follows $S_{t}^{(T)}$ is called a step function. $$ S_{t}^{(T)} := \begin{cases} 1 &amp;amp; , t \le T \\ 0 &amp;amp; , \text{otherwise} \end{cases} $$ The function defined as follows $P_{t}^{(T)}$ is called a pulse function. $$ \begin{align*} P_{t}^{(T)} :=&amp;amp; \nabla S_{t}^{(T)} \\ =&amp;amp; S_{t}^{(T)} - S_{t-1}^{(T)} \end{align*} $$ Description Step functions and pulse functions are useful for representing equations used in intervention analysis, and</description></item><item><title>Extended Real Number System</title><link>https://freshrimpsushi.github.io/en/posts/1252/</link><pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1252/</guid><description>Definition The set defined as follows is called the extended real number system. $$ \overline{ \mathbb{R} } := \mathbb{R} \cup \left\{ -\infty, +\infty\right\} $$ Explanation In fields such as analysis, for convenience, the set $\mathbb{R}$ is often replaced with $\overline{ \mathbb{R} }$. $\pm \infty$ is not a number, but for convenience, it is treated as one and added to $\mathbb{R}$. Within the extended real number system, the rules for comparison</description></item><item><title>Measuring and Benchmarking Code Execution Time in R</title><link>https://freshrimpsushi.github.io/en/posts/1246/</link><pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1246/</guid><description>Overview Matlab R is certainly a programming language specialized in statistical analysis, but like all languages, it is not indifferent to speed. Even if speed is not a forte, one should be able to benchmark. In R, you can simply measure time by putting the entire code inside system.time({}). Example Below is the code that implements the Sieve of Eratosthenes in R and checks if numbers less than or equal</description></item><item><title>Borel Sigma-Algebra, Borel Measurable Space</title><link>https://freshrimpsushi.github.io/en/posts/1251/</link><pubDate>Wed, 21 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1251/</guid><description>Theorem Let $X$ be an arbitrary set. Given a non-empty set $A \subset \mathcal{P}(X)$, there exists the smallest $\sigma$-algebra, $\mathcal{E}_{A}$, that contains $A$. Proof We define $\mathcal{E}_{A}$ and show that it is an $\sigma$-algebra and then prove that it is the smallest1. Let $S$ be the set of all $\sigma$-algebras that contain $A$. $$ S:= \left\{ \mathcal{E} \subset \mathcal{P}(X)\ :\ \mathcal{E}\ \mathrm{is\ } \sigma \mathrm{-algebra, \ } A \subset \mathcal{E}</description></item><item><title>Intervention Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1243/</link><pubDate>Wed, 21 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1243/</guid><description>Buildup The graph above represents the actual time series data of the fine dust concentration in Seoul in the year 2015. What immediately stands out to anyone looking at it is that there was a day, around the 50th marker or so, which means the end of February, when the fine dust concentration exceeded 500. Anyone somewhat familiar with handling data might first suspect this to be a measurement error,</description></item><item><title>Differences in the start and end Options of the ts Function and the window Function in R</title><link>https://freshrimpsushi.github.io/en/posts/1242/</link><pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1242/</guid><description>Description In R, you often use the ts() and window() functions when dealing with time series data. ts() is used to create time series data that R can accept, and window() is used to extract a portion of the time series data. Both functions have options for start and end, and their differences are as follows. ts() This is an option for giving indices. start: Assigns an index to the</description></item><item><title>Time Series Regression and Spurious Correlation</title><link>https://freshrimpsushi.github.io/en/posts/1238/</link><pubDate>Mon, 19 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1238/</guid><description>Definition 1 Spurious correlation refers to a relationship where two datasets appear to have a plausible correlation but actually do not. Cryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p260.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>How to Plot Points with Colored Borders in R</title><link>https://freshrimpsushi.github.io/en/posts/1237/</link><pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1237/</guid><description>Code To change the border color or fill out the inner part in the scatter plot, the following options can be modified: pch: Change the symbol to fill out the color. Use numbers from 21 to 25. bg: Stands for background color, determining the color filled inside. In the figure above, it is light green. col: The color of the symbol itself, which actually refers to the border. In the</description></item><item><title>Encyclopedia</title><link>https://freshrimpsushi.github.io/en/posts/1236/</link><pubDate>Sat, 17 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1236/</guid><description>Definition Prewhitening is a method that transforms time series data into white noise when calculating the Cross-Correlation Function (CCF) to more accurately identify the correlation between two datasets. Practical Exercise 1 If possible, it is recommended to fully understand mathematically how this is achievable. As an example, let&amp;rsquo;s look at the following data. bluebird consists of two time series data including the average price and sales volume of potato chips</description></item><item><title>How to Output Multiple Figures on One Page in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1247/</link><pubDate>Sat, 17 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1247/</guid><description>Method The subplot() function can be used to print multiple figures on one page. The first and second parameters respectively indicate the rows and columns of the chessboard on which images will be displayed, deciding the layout of the figures. The third parameter determines the sequence in which the specific figure will be placed. Below is the code and the actual output. X1=Phantom(); X2=radon(X1); X3=fft(X2); X4=iradon(X2,0:179); subplot(2,2,1) imagesc(X1) title(&amp;#34;Phantom&amp;#34;); subplot(2,2,2)</description></item><item><title>Fourth-order Runge-Kutta method</title><link>https://freshrimpsushi.github.io/en/posts/796/</link><pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/796/</guid><description>Method 1 Given the initial value problem $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ for the continuous function $f$ defined in $D \subset \mathbb{R}^2$, let&amp;rsquo;s assume the interval $(a,b)$ is divided into nodes as in $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Particularly, for sufficiently small $h &amp;gt; 0$, if $x_{j} = x_{0} + j h$, then for</description></item><item><title>How to Use Pipe Operator %>% in R</title><link>https://freshrimpsushi.github.io/en/posts/1235/</link><pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1235/</guid><description>Overview In R, the %&amp;gt;% is known as the Pipe Operator, and like all other operators, it performs a binary operation. The pipe operator, true to its name, allows values to travel through the pipeline, navigating through functions and enabling seamless data manipulation. To truly understand its utility, consider the following example. Example The example above calculates the square root of numbers from $1$ to $10$, takes the logarithm of</description></item><item><title>Numerical Solution to the Initial Value Problem for the Heat Equation Given Dirichlet Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/790/</link><pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/790/</guid><description>Example 1 $$ \begin{cases} u_{t} = \gamma u_{xx} \\ u(t,0) = u(t,l) = 0 \\ u(0,x) = f(x) \end{cases} $$ The given problem has an algebraic solution simple enough to solve, yet it serves as a clear example of why we learn numerical methods to solve differential equations. It shows how solving a simple differential equation of the form $y ' = f(x,y)$ leads to the solution of partial differential</description></item><item><title>A-Stable</title><link>https://freshrimpsushi.github.io/en/posts/774/</link><pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/774/</guid><description>Buildup Multistep methods including the midpoint method might have parasitic solutions when $h$ is not sufficiently small. Being not sufficiently small refers to situations such as when there is a problem like $ y ' = \lambda y$ and it fails to meet conditions like $| 1 + h \lambda| &amp;lt;1$. When we say $z : = h \lambda \in \mathbb{C}$ and represent the condition on the complex plane, it</description></item><item><title>Cross-Correlation Function</title><link>https://freshrimpsushi.github.io/en/posts/1227/</link><pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1227/</guid><description>Definition 1 Let&amp;rsquo;s say $\left\{ X_{t} \right\}_{t=1}^{n}$, $\left\{ Y_{t} \right\}_{t=1}^{n}$ are stochastic processes. The following defined $\rho_{k}$ is called the cross-correlation function at lag $k$. $$ \rho_{k} (X,Y) := \text{cor} \left( X_{t} , Y_{t-k} \right) = \text{cor} \left( X_{t+k} , Y_{t} \right) $$ The following defined $r_{k}$ is called the sample cross-correlation function at lag $k$. $$ r_{k} := {{ \sum \left( X_{t} - \overline{X} \right) \left( Y_{t-k} - \overline{Y}</description></item><item><title>Solution of the Schrödinger Equation for a Step Potential</title><link>https://freshrimpsushi.github.io/en/posts/1245/</link><pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1245/</guid><description>Overview Let&amp;rsquo;s examine how a particle moves when the potential is a step function as shown in the figure above. The potential $U$ is $$ U(x) = \begin{cases} 0 &amp;amp; x&amp;lt;0 \\ U_{0} &amp;amp; x&amp;gt;0 \end{cases} $$ The time-independent Schrödinger equation for this potential $U(x)$ is $$ \dfrac{d^2 u(x)}{dx^2}+\frac{2m}{\hbar ^2} \Big[ E-U(x) \Big]u(x)=0 $$ Solution1 $E&amp;lt;0$ No solution exists if</description></item><item><title>Convergence and Root Condition of Consistent Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/754/</link><pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/754/</guid><description>Theorem If a multistep method is consistent, then the method is convergent $\iff$ The method satisfies the root condition Explanation For a closed interval $[x_{0} , b]$, when cutting into units of $h$ to create node points, let&amp;rsquo;s call it $x_{0} \le x_{1} \le \cdots \le x_{N(h) -1} \le x_{N(h) } \le b$. Where $N(h)$ represents the index of the last node point that varies according to $h$. The method</description></item><item><title>How to Define the Operator %% in R</title><link>https://freshrimpsushi.github.io/en/posts/1224/</link><pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1224/</guid><description>Overview In R, it is possible to define a function as a binary operator right away. The remainder operation %%, the quotient operation %/%, dot product %*%, %o% or inclusion %in%, and the pipe operator %&amp;gt;% that are already defined in R also belong to these binary operators. Code For instance, in a language like Python, adding strings together concatenates them, which is very convenient, but R is somewhat inconvenient</description></item><item><title>Proving that Lp Spaces are Uniformly Convex and Reflexive</title><link>https://freshrimpsushi.github.io/en/posts/1244/</link><pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1244/</guid><description>Theorem1 Let&amp;rsquo;s say $1 \lt p \lt \infty$. Then, the $L^{p}$ space is uniformly convex and reflexive. Explanation It can be proven using the definition of uniform convexity and the Clarkson inequality. Thanks to the Clarkson inequality, the proof ends easily and briefly. It feels like a finishing move. Uniformly convex On the norm space $X$, the norm $\left\| \cdot \right\|$ is said to be uniformly convex if for all</description></item><item><title>Reflection and Transmission of Wave Functions</title><link>https://freshrimpsushi.github.io/en/posts/1241/</link><pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1241/</guid><description>Definition The reflection coefficient $R$ and transmission coefficient $T$ of a wave function are defined as follows. $$ R = \left| \frac{j_{\text{ref}}}{j_{\text{inc}}} \right|,\quad T = \left| \frac{j_{\text{trans}}}{j_{\text{inc}}}\right| \tag{1} $$ Here, $j_{\text{inc}}$ represents the probability flux of the incident wave, $j_{\text{ref}}$ represents the probability flux of the reflection wave, and $j_{\text{trans}}$ represents the probability flux of the transmission wave. Explanation When a particle with energy $E$ encounters a potential barrier higher</description></item><item><title>Time Series Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1223/</link><pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1223/</guid><description>Definition Time series regression analysis refers precisely to the technique of performing regression analysis using time series data. It&amp;rsquo;s true that regression analysis is not inherently well-suited to handling time series data, yet, when dealing with multiple series of time-series data, it can be beneficial to borrow the ideas and tools of regression analysis. Practice Suppose we are given two types of data, x and y, as shown above. Of</description></item><item><title>Probability Flow</title><link>https://freshrimpsushi.github.io/en/posts/1240/</link><pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1240/</guid><description>정의1 2 Wave Function $\psi (x, t)$ of Probability Current is defined as follows. $$ j(x,t) := \frac{\hbar}{2m\i}\left( \psi^{\ast}\dfrac{\partial \psi}{\partial x} - \psi\frac{\partial \psi^{\ast}}{\partial x}\right) \tag{1} $$ Formula The rate of change of the probability current is equal to the time derivative of the Probability Density. That is, the following equation holds. $$ \dfrac{\partial \left| \psi(x, t) \right|^{2}}{\partial t} = - \dfrac{\partial j(x,t)}{\partial x} \tag{2} $$ Explanation In</description></item><item><title>Run-Test</title><link>https://freshrimpsushi.github.io/en/posts/1219/</link><pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1219/</guid><description>Hypothesis Testing Let us denote the ARMA model obtained from time series analysis as $ARMA(p,q)$ to be $M$. $H_{0}$: $M$ is fit. $H_{1}$: $M$ is not fit. Explanation The Ljung-Box Test, also abbreviated as LBQ, is a test for determining the goodness-of-fit of an ARIMA model. In 1970, Box and Pierce proposed the following test statistic $Q$, through the sACF $\hat{r}_{1} , \cdots , \widehat{r}_{k}$ of residuals obtained from ARIMA</description></item><item><title>Gram-Schmidt Orthogonalization Process in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/1239/</link><pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1239/</guid><description>Definition The Gram-Schmidt orthogonalization procedure is a method for creating an orthogonal set from vectors that are not orthogonal to each other. Formula Suppose there are two time-independent one-dimensional wave functions $u_{1}$, $u_{2}$. Let’s assume that $u_{1}$ and $u_{2}$ are normalized and not orthogonal to each other. Then, the following wave function $u$ is a normalized wave function orthogonal to $u_{1}$. $$ \begin{align*} u &amp;amp;= \dfrac</description></item><item><title>Residual Analysis of ARIMA Models</title><link>https://freshrimpsushi.github.io/en/posts/1218/</link><pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1218/</guid><description>Explanation Like regression analysis, time series analysis also involves residual analysis. According to the assumptions of the ARIMA model, residuals are all white noise, thus they should follow linearity, homoscedasticity, independence, and normality. Compared to regression analysis, it&amp;rsquo;s generally not as strict, but independence is rigorously checked. After all, the purpose of time series analysis is to understand autocorrelation; if residuals still lack independence, it means the analysis is incomplete.</description></item><item><title>Durbin-Watson Test</title><link>https://freshrimpsushi.github.io/en/posts/1217/</link><pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1217/</guid><description>Hypothesis Testing After performing a regression analysis, let&amp;rsquo;s assume we are given a residual $\left\{ e_{t} \right\}_{t=1}^{n}$ and express it as $e_{t} := \rho e_{t-1} + \nu_{t}$. $H_{0}$: $\rho = 0$ i.e., the residuals do not exhibit autocorrelation. $H_{1}$: $\rho \ne 0$ i.e., the residuals exhibit autocorrelation. Explanation Empirical Interpretation The Durbin-Watson test is used to check the independence of residuals after regression analysis, determining whether or not the residuals</description></item><item><title>Natural Embeddings and Reflexive Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1233/</link><pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1233/</guid><description>Definition1 Let us call $\left( X, \left\| \cdot \right\|_{X} \right)$ a normed space. And let us define $X^{\ast \ast}=(X^{\ast})^{\ast}$ as the bidual of $X$. Define the function $J : X \to X^{\ast \ast}$ as follows. $$ J(x)=J_{x},\quad x\in X $$ Here, $J_{x} \in X^{\ast \ast}$ is specifically given as follows. $$ J_{x} : X^{\ast} \to \mathbb{C} \quad \text{and} \quad J_{x}(x^{\ast})=x^{\ast}(x) $$ In this case, $J$ becomes an embedding. Such $J$</description></item><item><title>Selecting an ARMA Model Using EACF in R</title><link>https://freshrimpsushi.github.io/en/posts/1216/</link><pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1216/</guid><description>Practice 1 PACF is very helpful in determining the order of $AR(p)$, while ACF aids in determining the order of $MA(q)$. Let&amp;rsquo;s directly observe an example. ma1.2.s data comes from a $MA(1)$ model, and ar1.s data comes from a $AR(1)$ model, both from the TSA package. By using the acf() and pacf() functions from the TSA package, it generates a Correlogram for various lags $k$ as follows. Merely looking at</description></item><item><title>Stability and Root Conditions of Consistent Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/734/</link><pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/734/</guid><description>Theorem If a multistep method is consistent, the method is stable $\iff$ the method satisfies the root condition. Explanation When creating node points by cutting the closed interval $[x_{0} , b]$ at intervals of $h$, let&amp;rsquo;s call it $x_{0} \le x_{1} \le \cdots \le x_{N(h) -1} \le x_{N(h) } \le b$. Here, $N(h)$ represents the index of the last node point that changes according to $h$. Consider $z_{0} , \cdots</description></item><item><title>Extended Autocorrelation Function</title><link>https://freshrimpsushi.github.io/en/posts/1213/</link><pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1213/</guid><description>Buildup PACF helps in determining the order of $AR(p)$, while ACF is helpful in setting the order for $MA(q)$. However, when applying to $ARMA(p,q)$ model, due to the invertibility of ARMA models, $AR(p)$ may appear as $MA(\infty)$, and $MA(q)$ may appear as $AR(\infty)$. Therefore, various methods have been devised to circumvent these issues and find the ARMA model. Definition The Extended Autocorrelation Function is one such method, defined as EACF</description></item><item><title>Hahn-Banach Extension Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1231/</link><pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1231/</guid><description>Theorem1 Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ a normed space. Let $Y \subset X$. Also, given a linear functional $y^{\ast} \in Y^{\ast}$ of $Y$. Then there exists a linear functional $x^{\ast} \in X^{\ast}$ of $X$ that satisfies the following equation. $$ \begin{equation} x^{\ast}(y)=y^{\ast}(y),\quad \forall y \in Y \end{equation} $$ $$ \begin{equation} \| x^{\ast}\|_{X^{\ast}} = \| y^{\ast}\|_{Y^{\ast}} \end{equation} $$ Explanation In simple terms, this means that the dual of a subspace</description></item><item><title>Multistep Methods' Root Condition</title><link>https://freshrimpsushi.github.io/en/posts/732/</link><pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/732/</guid><description>Definition 1 Multi-step method: Given the continuous function defined by $D \subset \mathbb{R}^2$ for $f$ and the initial value problem $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$ on the interval $(a,b)$. Let&amp;rsquo;s divide the interval into nodes as $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Especially, for a</description></item><item><title>Adams Method</title><link>https://freshrimpsushi.github.io/en/posts/724/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/724/</guid><description>Definition 1 Multistep methods: Given a continuous function $D \subset \mathbb{R}^2$ for $f$ and an initial value problem $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$. Let&amp;rsquo;s divide interval $(a,b)$ into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Especially, for a sufficiently small $h &amp;gt; 0$, if</description></item><item><title>Autocorrelation Function</title><link>https://freshrimpsushi.github.io/en/posts/1211/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1211/</guid><description>Definition 1 Let $\left\{ Y_{t} \right\}_{t=1}^{n}$ be a stochastic process, and for lag $k$, let the residuals obtained by regressing $Y_{t}$ on $Y_{t-1}, \cdots , Y_{t-(k-1)}$ be $\widehat{e_{t}}$, and the residuals obtained by regressing $Y_{t-k}$ on $Y_{t-1}, \cdots , Y_{t-(k-1)}$ be $\widehat{e_{t-k}}$. The following defined $\phi_{kk}$ is referred to as the partial autocovariance function at lag $k$. $$ \phi_{kk} := \text{cor} ( \widehat{e_{t}} , \widehat{e_{t-k}} ) $$ The following defined</description></item><item><title>Hahn Banach Theorem for Real, Complex, Seminorm</title><link>https://freshrimpsushi.github.io/en/posts/1230/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1230/</guid><description>The Hahn-Banach Theorem for Real Numbers1 Let $X$ be a $\mathbb{R}$-vector space and assume that $Y \subset X$. Let us define $p : X \to \mathbb{ R}$ as a sublinear linear functional of $X$. Now, assume that $y^{\ast} : Y \to \mathbb{ R}$ satisfies the following condition as a $\mathbb{R}$-linear functional of $Y$. $$ y^{\ast}(y) \le p(y)\quad \forall y\in Y $$ Then, there exists a linear functional $x^{\ast} : X</description></item><item><title>Differentiation of Functions Defined in Real Number Space</title><link>https://freshrimpsushi.github.io/en/posts/1210/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1210/</guid><description>Definition1 If in some $E$ containing $a$, $f$ is defined and the limit $$ f^{\prime} (a) := \lim_{h \to 0} {{ f (a + h ) - f(a) } \over { h }}=\lim \limits_{x\rightarrow a}\frac{f(x)-f(a)}{x-a} $$ exists, then $f$ is said to be differentiable at $a$, and $f^{\prime} (a)$ is called the derivative of $f$ at $a$. If $f$ is differentiable at every point $a \in E$, then $f$ is</description></item><item><title>Richardson Error Estimation</title><link>https://freshrimpsushi.github.io/en/posts/706/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/706/</guid><description>Buildup Comparing with the true value is the best method to check the performance of a method for solving differential equations, but there are cases where it is bothersome to immediately find the true value, and other cases where it is difficult to find the true solution at all. In such cases, one can estimate the error by comparing $y_{h} (x_{n} )$ and the result of doubling the step size</description></item><item><title>Semi Norm</title><link>https://freshrimpsushi.github.io/en/posts/1229/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1229/</guid><description>Definition1 Let $X$ be a vector space. A function $\left\| \cdot \right\| : X \to \mathbb{R}$ is called a semi norm of $X$ if it satisfies the following three conditions: (a) $\left\| x \right\| \ge 0,\quad \forall\ x \in X$ (b) $|cx|=|c|\left\| x \right\|,\quad \forall\ x\in X,\ \forall\ c \in\mathbb{C}$ (c) $\left\| x + y \right\| \le \left\| x \right\| + \left\| y \right\|,\quad \forall\ x,y\in X$ Explanation The definition</description></item><item><title>Semi-linear Function</title><link>https://freshrimpsushi.github.io/en/posts/1333/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1333/</guid><description>Definition1 If a function $f : X \to Y$ satisfies the following two conditions, it is called sublinear. For $x,x_{1},x_{2}\in X$ and $a \in \mathbb{R}$, $f(ax) = af(x)$ $f(x_{1} + x_{2}) \le f(x_{1}) + f(x_{2})$ Explanation If the second condition holds as an equality, it is linear, and if it holds as an inequality, it is sublinear. Robert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003),</description></item><item><title>Topological Isomorphism in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/385/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/385/</guid><description>Definition Given two metric spaces $\left( X, d_{1} \right)$ and $\left( Y, d_{2} \right)$, if there exists a bijective $f : X \to Y$ such that both $f$ and its inverse function $f^{-1}$ are continuous functions, then $f$ is called a homeomorphism, and the two metric spaces are said to be homeomorphic. Explanation The definition of homeomorphism for metric spaces might seem vacuous at first glance. Indeed, since metric spaces</description></item><item><title>Autocorrelation Function</title><link>https://freshrimpsushi.github.io/en/posts/1209/</link><pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1209/</guid><description>Definition 1 Let&amp;rsquo;s say $\left\{ Y_{t} \right\}_{t=1}^{n}$ is a stochastic process. $\mu_{t} := E ( Y_{t} )$ is called the mean function. The following defined $\gamma_{ t , s }$ is called the autocovariance function. $$ \gamma_{t , s} : = \text{cov} ( Y_{t} , Y_{s} ) = E ( Y_{t} - \mu_{t} ) E ( Y_{s} - \mu_{s} ) $$ The following defined $\rho_{ t , s }$ is</description></item><item><title>The Riesz Representation Theorem for Lp Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1228/</link><pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1228/</guid><description>Theorem[^1] ${L}^{\ p}$ Representation Theorem for Spaces Let&amp;rsquo;s say $1&amp;lt;p&amp;lt;\infty$ and $L\in \big( {L}^{\ p} \big)^{\ast}$. Then, $({L}^{\ p})^{\ast}$ is the dual of the ${L}^{\ p}$ space. Therefore, for every $u\in {L}^{\ p}$, there exists a $v \in {L}^{\ p^{\prime}}$ that satisfies the following equation. $$ L(u)=L_{v}(u)=\int_{\Omega} u(x)v(x)dx $$ Explanation Note that the case where $p=1$ is not included. $\left\| v \right\|_{p^{\prime}} =\left\| L\ ; ({L}^{\ p})^{\ast}\right\|$ satisfies that $f\</description></item><item><title>Trapezoidal Method</title><link>https://freshrimpsushi.github.io/en/posts/704/</link><pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/704/</guid><description>Definition 1 Given an initial value problem $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ for a continuous function defined in $D \subset \mathbb{R}^2$ on the interval $(a,b)$, let&amp;rsquo;s say this interval is divided into nodes as described in $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Particularly, for sufficiently small $h &amp;gt; 0$, if we assume $x_{j} = x_{0}</description></item><item><title>Proof that All Isometric Mappings are Embeddings</title><link>https://freshrimpsushi.github.io/en/posts/1226/</link><pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1226/</guid><description>Theorem Let&amp;rsquo;s call $(X, \left\| \cdot \right\|_{X}), (Y, \left\| \cdot \right\|_{Y})$ a norm space. And let&amp;rsquo;s say $f : X \to Y$ is an isometry. Then, $f$ is an embedding. In other words, $f$ satisfies the following two conditions. (a) $f(X) \subset Y$ (b) $f : X \to f(X)$ is a homeomorphism. Proof Strategy: Prove $(b)$ first, then prove $(a)$. Although there isn&amp;rsquo;t anything particularly difficult in each proof process,</description></item><item><title>Uniform Continuity of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1207/</link><pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1207/</guid><description>Definition1 Let us assume $E \subset \mathbb{R}$ is a non-empty set and define $f : E \to \mathbb{R}$. If for every $\varepsilon &amp;gt; 0$, $$ | x_{1} - x_{2} | &amp;lt; \delta \land x_{1} , x_{2} \in E \implies | f(x_{1}) - f(x_{2}) | &amp;lt; \varepsilon $$ there exists a $\delta&amp;gt;0$ satisfying the above equation, then $f$ is said to be uniformly continuous on $E$. $\land$ represents the logical &amp;lsquo;and&amp;rsquo;</description></item><item><title>Parasitic Solution</title><link>https://freshrimpsushi.github.io/en/posts/701/</link><pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/701/</guid><description>Definition 1 A Parasitic Solution refers to a term whose magnitude grows and whose sign changes as the method progresses. It&amp;rsquo;s useful to imagine a sequence $a_{n} = 2^{-n} + (-2)^{n}$ that does not converge due to $ (-2)^{n}$. The term &amp;lsquo;parasitic&amp;rsquo; is quite intuitive and appropriate in describing these terms as they hinder convergence. Example: Dahlquist Problem Consider, for example, $\begin{cases} y ' = \lambda y \\ y(0) =</description></item><item><title>Reversibility of ARMA Models</title><link>https://freshrimpsushi.github.io/en/posts/1208/</link><pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1208/</guid><description>Definition 1 In the ARMA model, having invertibility means that $AR(p)$ and $MA(q)$ can represent each other. Examples Although it is not a formula development for the general $ARMA ( p , q)$, let&amp;rsquo;s examine the examples of $AR(1)$ and $MA(1)$. Autoregressive Model $AR(1) \implies MA( \infty )$ Considering the following autoregressive model $AR(1)$ for $| \phi | &amp;lt; 1$: $$ Y_{t} = \phi Y_{t-1} + e_{t} $$ $Y_{t-1}$ can</description></item><item><title>What is a Norm Space?</title><link>https://freshrimpsushi.github.io/en/posts/1225/</link><pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1225/</guid><description>Definition1 Let&amp;rsquo;s call $X$ a vector space. If there exists a function $\left\| \cdot \right\| : X \to \mathbb{R}$ that satisfies the following three conditions, then $\left\| \cdot \right\|$ is called the norm of $X$, and $(X,\left\| \cdot \right\| )$ is called a normed space. (a) $\left\| x \right\| \ge 0,\quad \forall\ x \in X$ and $\left\| x \right\|=0 \iff x = 0$ (b) $\|cx\|=|c|\left\| x \right\|,\quad \forall\ x\in X,\</description></item><item><title>Midpoint Method</title><link>https://freshrimpsushi.github.io/en/posts/700/</link><pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/700/</guid><description>Methods In the continuous function defined by $D \subset \mathbb{R}^2$ for the initial value problem $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ), y (x_{1}) ) = ( Y_{0} , Y_{1} ) \end{cases}$ given for $f$, assume the interval $(a,b)$ is divided into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Particularly, for sufficiently small $h &amp;gt; 0$ if $x_{j}</description></item><item><title>Newly Defined Continuous Functions in University Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1206/</link><pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1206/</guid><description>Definition Let&amp;rsquo;s say a set that is not an empty set is called $E \subset \mathbb{R}$, and $f : E \to \mathbb{R}$. If there exists $\delta&amp;gt;0$ for every $\varepsilon &amp;gt; 0$ such that $$ | x - a | &amp;lt; \delta \implies | f(x) - f(a) | &amp;lt; \varepsilon $$ is satisfied, $f$ is said to be continuous at $a \in E$, and if it is continuous at every point</description></item><item><title>Sobolev Spaces are Separable, Uniformly Convex, and Reflexive: A Proof</title><link>https://freshrimpsushi.github.io/en/posts/1222/</link><pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1222/</guid><description>Theorem1 When $1\le p &amp;lt;\infty$, the Sobolev space $W^{m, p}$ is separable. Moreover, when $1&amp;lt; p &amp;lt; \infty$, the Sobolev space is reflexive and uniformly convex. Description A vector space in which an inner product is defined is called an inner product space, and a complete inner product space is specially called a Hilbert space. Since $W^{m, p}$ is complete, if the inner product is defined as below, $W^{m,\ 2}$</description></item><item><title>Convergence and Error of Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/698/</link><pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/698/</guid><description>Theorem Given the initial value problem $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) )= ( Y_{0} , \cdots , Y_{p} ) \end{cases}$, a multistep method $$ \displaystyle y_{n+1} = \sum_{j=0}^{p} a_{j} y_{n-j} + h \sum_{j = -1}^{p} b_{j} f (x_{n-j} , y_{n-j} ) $$ is consistent, and if the initial error $\displaystyle \eta (h) : = \max_{ 0 \le i \le p} |</description></item><item><title>Predicting with ARIMA Models in R</title><link>https://freshrimpsushi.github.io/en/posts/1205/</link><pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1205/</guid><description>Practice The R built-in data UKDriverDeaths contains monthly data on road casualties in the UK from 1969 to 1984. It obviously follows a seasonal ARIMA model, and finding the actual model is not very difficult. However, performing calculations directly using the formula from the final model is quite laborious and complex. Therefore, we use the predict() function. You can set how far into the future you would like to predict</description></item><item><title>Sobolev Spaces are Banach Spaces: A Proof</title><link>https://freshrimpsushi.github.io/en/posts/1221/</link><pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1221/</guid><description>Theorem1 The Sobolev space $W^{m, p}$ is a Banach space. Description A Banach space is defined as a space where a norm is defined and is complete. As the norm is also defined when defining the Sobolev space, we only need to verify that it is complete. Therefore, it suffices to show that the Cauchy sequence within $W^{m, p}$ converges within $W^{m, p}$. The proof is relatively straightforward. Proof Let</description></item><item><title>Epsilon-Delta Argument</title><link>https://freshrimpsushi.github.io/en/posts/1204/</link><pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1204/</guid><description>Definition1 Let $I$ be an interval containing $a \in \mathbb{R}$, and suppose that $f$ is a function defined at $I \setminus \left\{ a \right\}$. If for every $\epsilon &amp;gt; 0$, there exists a $\delta&amp;gt;0$ such that $$ 0 &amp;lt; | x - a | &amp;lt; \delta \implies | f(x) - L | &amp;lt; \varepsilon $$ is satisfied, then we say that $f(x)$ converges to $L \in \mathbb{R}$ as $x \to</description></item><item><title>Sobolev Norm and Sobolev Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1220/</link><pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1220/</guid><description>Definition1 Sobolev Space Let $\Omega \subset \mathbb{R}^{n}$ be an open set. For positive integers $m$ and $1\le p \le \infty$, the function space defined as follows is called the Sobolev space. $$ W^{m, p}(\Omega):=\left\{ u \in L^{p}(\Omega) : D^\alpha u \in L^{p}(\Omega)\quad \forall 0\le |\alpha | \le m \right\} $$ Here, $\alpha$ is a multi-index, $D^\alpha u$ is a weak derivative, and $L^{p}$ is a Lebesgue space. Sobolev Norm For</description></item><item><title>Consistency and Order of Convergence of Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/694/</link><pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/694/</guid><description>Theorem The necessary and sufficient condition for a multistep method $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) )= ( Y_{0} , \cdots , Y_{p} ) \end{cases}$ to be consistent is (i), and the necessary and sufficient condition to have convergence order $m \in \mathbb{N}$ is (ii). (i): $$\begin{cases} \displaystyle \sum_{j = 0}^{p} a_{j} = 1 \\ \displaystyle - \sum_{j = 0}^{p} j a_{j}</description></item><item><title>How to View Time Series Analysis Results Obtained with ARIMA Model in R</title><link>https://freshrimpsushi.github.io/en/posts/1200/</link><pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1200/</guid><description>Practice The built-in R dataset AirPassenger consists of monthly airline passenger numbers from 1949 to 1960. (1) Model: In fact, if the coefficients can be precisely identified, that&amp;rsquo;s not the most critical aspect. Represents the Seasonal ARIMA model $ARIMA(p,d,q)\times(P,D,Q)_{s}$. For instance, the result of the above analysis ARIMA(0,1,1)(0,1,1)[12] means $ARIMA(0,1,1)\times(0,1,1)_{12}$. (2) Coefficients: Represents the coefficients that fit the model. ma1 is the moving average process coefficient $\theta_{1}$, and sma1 is</description></item><item><title>Embeddings in Mathematics, Insertion Mappings</title><link>https://freshrimpsushi.github.io/en/posts/1214/</link><pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1214/</guid><description>imbedding and embedding mean the same thing. Embedding is translated as insertion, embedding, incorporating, burying, etc. Definition1 Let $(X, \left\| \cdot \right\|_{X}), (Y, \left\| \cdot \right\|_{Y})$ be a normed space. If the following two conditions are satisfied for $X$ and $Y$, then $X$ is said to be embedded into $Y$, and $I : X \to Y$ is called the embedding. $X$ is a subspace of $Y$. For all $x \in</description></item><item><title>Limits Supremum and Limits Infimum</title><link>https://freshrimpsushi.github.io/en/posts/1198/</link><pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1198/</guid><description>Definition Let $\left\{ x_{n} \right\}_{n \in \mathbb{N}}$, $\left\{ y_{n} \right\}_{n \in \mathbb{N}}$ be sequences of real numbers. $\displaystyle \limsup_{n \to \infty} x_{n} := \lim_{n \to \infty} \left( \sup_{k \ge n} x_{k} \right)$ is called the limit supremum of $\left\{ x_{n} \right\}$. $\displaystyle \liminf_{n \to \infty} y_{n} := \lim_{n \to \infty} \left( \inf_{k \ge n} y_{k} \right)$ is called the limit infimum of $\left\{ y_{n} \right\}$. Where $\displaystyle \sup_{k \ge n}</description></item><item><title>How to Analyze Time Series with ARIMA Models in R</title><link>https://freshrimpsushi.github.io/en/posts/1197/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1197/</guid><description>Practice Let&amp;rsquo;s load the built-in data WWWusage in R and draw a graph to check it. WWWusage represents a time series data indicating the number of internet users a long time ago. To understand its trend, time series analysis is necessary. Among the time series analysis models, the most representative one is the ARIMA model. However, even within the ARIMA models, there are various methods to find the appropriate model.</description></item><item><title>Minkowski's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1212/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1212/</guid><description>Theorem1 Let&amp;rsquo;s denote $\Omega \subset \mathbb{R}^{n}$ as an open set, and $0 \lt p \lt 1$. If $u, v \in L^p(\Omega)$ then $u+v \in L^p(\Omega)$. Explanation This is called the reverse Minkowski&amp;rsquo;s inequality. It&amp;rsquo;s not the converse of the Minkowski&amp;rsquo;s inequality proposition, but the direction of the inequality is reversed. The Minkowski inequality shows that when $1 \le p \lt \infty$, the defined $\left\| \cdot \right\|_{p}$ satisfies the triangle inequality</description></item><item><title>Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/693/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/693/</guid><description>Definition 1 Given a continuous function defined in $D \subset \mathbb{R}^2$ for the initial value problem given in $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$, let&amp;rsquo;s say we have broken down interval $(a,b)$ into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Especially for a sufficiently small</description></item><item><title>Cauchy Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1190/</link><pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1190/</guid><description>Definition For all $\varepsilon &amp;gt; 0$, if there exists $N \in \mathbb{N}$ such that $n , m \ge N \implies | x_{n} - x_{m} | &amp;lt; \varepsilon$ is satisfied, then the sequence $\left\{ x_{n} \right\}_{n \in \mathbb{N}}$ is said to be Cauchy. Theorem It is equivalent for a sequence to be Cauchy and to converge. Explanation Considering that there are hardly any important sequences that diverge, it can be surmised</description></item><item><title>Holder's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1203/</link><pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1203/</guid><description>Theorem1 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. Assume $0 &amp;lt; p &amp;lt; 1$ and $p^{\prime} = \dfrac{p}{p-1} &amp;lt; 0$. If $u \in$ $L^{p}(\Omega)$, $uv\in$ $L^{1}(\Omega)$, then $$ \begin{equation} 0 \lt \int_{\Omega} |v(x)|^{p^{\prime}}dx \lt \infty \end{equation} $$ the following inequality is established: $$ \int_{\Omega} |u(x)v(x)|dx \ge \left( \int_{\Omega} |u(x)|^{p} dx \right)^{1/p} \left( \int_{\Omega} |v(x)|^{p^{\prime}} dx \right) ^{1/p^{\prime}} $$ Explanation This is called the reverse Höeld</description></item><item><title>Initial Value Variation and the Error in the Euler Method</title><link>https://freshrimpsushi.github.io/en/posts/692/</link><pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/692/</guid><description>Theorem The solution $Y(x)$ to the initial value problem $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ defined in $[x_{0} , b] \times \mathbb{R}$ with respect to $f$, is $Y \in C^{3} [ x_{0} , b ]$ if $\displaystyle f_{y} (x,y) = {{ \partial f (x,y) } \over { \partial y }}$ and $\displaystyle f_{yy} (x,y) = {{ \partial^{2} f (x,y) } \over { \partial y^{2}</description></item><item><title>Factorization of Semiprimes: Conditions for Easy Resolution</title><link>https://freshrimpsushi.github.io/en/posts/1189/</link><pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1189/</guid><description>Theorem 1 The problem of factorizing semiprimes $N = pq$ becomes relatively easy under the following conditions: (i): $p$ is a smooth prime. (ii): $p \approx q$ Explanation The meaning of condition (ii) is that when the difference between $p$ and $q$ is small, it becomes easier to solve, which requires explanation: At first glance, if $p,q \in \mathbb{N}$ is constrained by $(p + q)$, the best way to maximize</description></item><item><title>Proof of Clarkson's Inequalities</title><link>https://freshrimpsushi.github.io/en/posts/1202/</link><pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1202/</guid><description>Theorem1 Let&amp;rsquo;s say that $\Omega \subset \mathbb{R}^{n}$ is an open set. Let&amp;rsquo;s say that $u,v\in {L}^{p}(\Omega)$. Also, let&amp;rsquo;s say it satisfies $\frac{1}{p}+\frac{1}{p^{\prime}}=1$. If $2 \le p \lt \infty$, then the following two inequalities hold. $$ \begin{equation} \left\| \frac{u+v}{2}\right\|_{p}^{p}+ \left\| \frac{u-v}{2} \right\|_{p}^{p} \le \frac{1}{2}\left\| u \right\|_{p}^{p} + \frac{1}{2}\left\| v \right\|_{p}^{p} \end{equation} $$ $$ \begin{equation} \left\| \frac{u+v}{2}\right\|_{p}^{p^{\prime}}+ \left\| \frac{u-v}{2} \right\|_{p}^{p^{\prime}} \ge \left( \frac{1}{2}\left\| u \right\|_{p}^{p} + \frac{1}{2}\left\| v \right\|_{p}^{p}\right)^{p^{\prime}-1} \end{equation} $$ If</description></item><item><title>Strong Lipschitz Condition and Error of the Euler Method</title><link>https://freshrimpsushi.github.io/en/posts/689/</link><pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/689/</guid><description>Theorem Let&amp;rsquo;s assume that the solution $Y(x)$ to the initial value problem $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$, defined in $[x_{0} , b] \times \mathbb{R}$ for $f$, is twice differentiable in $[x_{0} , b]$. If $f$ satisfies strong Lipschitz condition $$ |f(x,y_{1} ) - f(x,y_{2}) | \le K | y_{1} - y_{2} | $$ for all $x_{0} \le x \le b$, $ y_{1} ,</description></item><item><title>Bolzano-Weierstrass Theorem</title><link>https://freshrimpsushi.github.io/en/posts/380/</link><pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/380/</guid><description>Theorem If a given infinite set $E \subset \mathbb{R}$ is bounded, then there is an accumulation point $p \in \mathbb{R}$ of $E$. Explanation Or one could also say, &amp;lsquo;A bounded sequence has a converging subsequence.&amp;rsquo; It&amp;rsquo;s important to note that $E$ does not need to be closed under the condition. Proof Part 1. $\displaystyle \bigcap_{n=1}^{\infty} I_{n} = \left\{ x \right\}$ Since $E$ is bounded by assumption, there exists a closed</description></item><item><title>Euler Method in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/687/</link><pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/687/</guid><description>Method 1 $D \subset \mathbb{R}^2$ defines a continuous function $f$ for the initial value problem given by $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$. Suppose the interval $(a,b)$ is divided into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Particularly for a sufficiently small $h &amp;gt; 0$, if it is assumed that $x_{j} = x_{0} + j</description></item><item><title>Interpolation Inequalities in Lebesgue Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1201/</link><pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1201/</guid><description>Theorem1 Let&amp;rsquo;s say $\Omega \subset \mathbb{R}^{n}$ is an open set. Suppose that for some $\theta$, $1 \le p \lt q\lt r \le \infty$ satisfies the following equation being $0 \lt \theta \lt 1$. $$ \dfrac{1}{q} = \frac{\theta}{p} + \frac{1-\theta}{r} $$ Assume that $u \in L^p(\Omega) \cap L^r(\Omega)$. Then, $u\in L^{q}(\Omega)$ holds, and the following inequality is established. $$ \left\| u \right\|_{q} \le \left\| u \right\|_{p}^{\theta} \left\| u \right\|_{r}^{1-\theta} $$ This</description></item><item><title>Proof of the Pollard p-1 Factoring Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/1187/</link><pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1187/</guid><description>Algorithm 1 Given a semiprime $N$, if $p$ is a smooth prime, then the factorization $N = pq$ of $N$ can be found as follows: Step 1. $a := 2$ and $L := 1$ are set. Step 2. $d := \gcd ( a^{L} - 1 , N )$ is calculated. Step 3. If $1&amp;lt; d &amp;lt; N$, then $d = p$, a divisor of $N$, is found and we are</description></item><item><title>Cantor's Intersection Theorem</title><link>https://freshrimpsushi.github.io/en/posts/376/</link><pubDate>Mon, 22 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/376/</guid><description>Definition1 A sequence $\left\{ S_{n} \right\}_{n=1}^{\infty}$ of a set is said to be nested if for every natural number $n$, $S_{n+1} \subset S_{n}$ holds. Explanation The translation of nested might not be smooth, but since there is no better alternative, it is recommended to just memorize it as &amp;ldquo;Nested.&amp;rdquo; Theorem For the nested interval $[a_{n}, b_{n}]$, the following holds: (a) $\displaystyle \bigcap_{n=1}^{\infty} [a_{n}, b_{n}] \ne \emptyset$ (b) Specifically, if $\displaystyle</description></item><item><title>Linear Functionals on Lp Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1196/</link><pubDate>Mon, 22 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1196/</guid><description>Definition1 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. Suppose that $1 \le p \le \infty$ and $p^{\prime}=\frac{p}{p-1}$. For each $v \in L^{p^{\prime}}(\Omega)$, define the linear functional $L_{v}\ :\ L^p(\Omega) \rightarrow \mathbb{C}$ on the space $L^p(\Omega)$ as follows. $$ L_{v}(u) = \int_{\Omega} u(x)v(x)dx, \quad u\in L^p(\Omega) $$ Theorem Let the norm on the space $L^p$ be denoted by $\| \cdot \|_{p}$. Then, by the Hölder</description></item><item><title>Proof of the Goldwasser-Micali Probabilistic Key Cryptosystem</title><link>https://freshrimpsushi.github.io/en/posts/1185/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1185/</guid><description>Buildup Let&amp;rsquo;s name them Alice, Bob, and Eve from left to right. Alice and Bob are the parties exchanging messages, and Eve is the passive attacker interested in the message. The orange box represents information known only to Alice, the sky-blue box represents information known only to Bob, and the black box represents information that is public (also known to Eve). Alice has a message $m \in \left\{ 0 ,</description></item><item><title>The Reason for Intricately Defining the Convergence of Sequences in University Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1186/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1186/</guid><description>Definition Let $\left\{ x_{n } \right\}_{n = 1}^{\infty}$ be a sequence of real numbers. If for every $\varepsilon &amp;gt; 0$, there exists $N \in \mathbb{N}$ such that $n \ge N \implies | x_{n} - a | &amp;lt; \varepsilon$ is satisfied, then we say that $\left\{ x_{n } \right\}$ converges to $a \in \mathbb{R}$. $$ \lim_{n \to \infty} x_{n} = a \iff \forall \varepsilon &amp;gt; 0 , \exists N \in \mathbb{N}</description></item><item><title>Uniform Convexity</title><link>https://freshrimpsushi.github.io/en/posts/1199/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1199/</guid><description>Definitions1 Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ a normed space. We say that the norm $\left\| \cdot \right\|$ of $X$ is uniformly convex if it satisfies the following condition: For every $\epsilon$ where $0 \lt \epsilon \le 2$, there exists a positive number $\delta (\epsilon) \gt 0$ such that if $x,y \in X$ and $\| x \| = \|y\| = 1$, $\| x-y\| \ge \epsilon$ then it satisfies $\|( x+y)/2</description></item><item><title>Derivation of Bessel's Equation</title><link>https://freshrimpsushi.github.io/en/posts/1195/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1195/</guid><description>Definition The differential equation below is called the $\nu$th order Bessel&amp;rsquo;s equation. $$ \begin{align*} x^2 y^{\prime \prime} +xy^{\prime} +(x^2-\nu^2)y =&amp;amp;\ 0 \\ x(xy^{\prime})^{\prime} + (x^2- \nu ^2) y =&amp;amp;\ 0 \\ y^{\prime \prime}+\frac{1}{x} y^{\prime} + \left( 1-\frac{\nu^{2}}{x^{2}} \right)y =&amp;amp;\ 0 \end{align*} $$ Description The solution to the Bessel&amp;rsquo;s equation is called the Bessel function. Bessel functions are often seen in physics, engineering, and more, especially in problems involving cylindrical symmetry.</description></item><item><title>Redefining the Limits of Sequences in University Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1184/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1184/</guid><description>Definitions1 2 $\mathbb{N}$ represents the set of natural numbers, and $\mathbb{R}$ represents the set of real numbers. A function with a domain of $\mathbb{N}$ is called a sequence. For a sequence of natural numbers $\left\{ n_{k} \right\}_{ k \in \mathbb{N}}$, $\left\{ x_{n_{k}} \right\}_{ k \in \mathbb{N}}$ is called a subsequence of $\left\{ x_{n} \right\}_{ n \in \mathbb{N}}$. If for every $x \in \left\{ x_{n} \right\}_{ n \in \mathbb{N}}$ there exists</description></item><item><title>Proof of the RSA Public Key Cryptosystem</title><link>https://freshrimpsushi.github.io/en/posts/1173/</link><pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1173/</guid><description>Buildup Let&amp;rsquo;s call them Alice, Bob, and Eve from left to right. Alice and Bob are the parties exchanging messages, and Eve is a passive attacker interested in the message. The orange box represents information only known by Alice, the sky blue box is information only known by Bob, and the black box represents information that is public (also known by Eve). Alice has a message $m \in \mathbb{N}$ she</description></item><item><title>List of Special Symbols Available for Graphs in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1191/</link><pubDate>Wed, 17 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1191/</guid><description>Method When labeling graphs in MATLAB to indicate what each axis represents, you can use xlabel and ylabel. It&amp;rsquo;s also possible to use special symbols, bold, and italic styles. x=-3*pi:0.2:3* pi; y=sin(x-pi/6); plot(x,y); xlabel(&amp;#39;\beta&amp;#39;), ylabel(&amp;#39;\nabla f(x)&amp;#39;),; x=-3*pi:0.2:3* pi; y=sin(x-pi/6); plot(x,y); xlabel(&amp;#39;진폭{\bf Volt}&amp;#39;), ylabel(&amp;#39</description></item><item><title>Prime Factorization</title><link>https://freshrimpsushi.github.io/en/posts/775/</link><pubDate>Wed, 17 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/775/</guid><description>Definition For a given natural number $N$, finding prime numbers $p_{1} , \cdots , p_{n}$ and natural numbers $r_{1} , \cdots , r_{n}$ that satisfy $N = p_{1}^{r_{1}} \cdots p_{n}^{r_{n}}$ is called prime factorization.</description></item><item><title>Continuous but Not Differentiable Functions: Weierstrass Function</title><link>https://freshrimpsushi.github.io/en/posts/1169/</link><pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1169/</guid><description>Theorem There exists a continuous function that cannot be differentiated anywhere. Proof Strategy: Consider continuous functions $g_{1} (x) := | x - 1 |$ and $g_{2} (x) := | x - 2 |$. $g_{1}$ is not differentiable at $x=1$, and $g_{2}$ is not differentiable at $x=2$. $(g_{1} + g_{2})$ is not differentiable at both points $x = 1$ and $x = 2$. In this way, if we construct $\displaystyle G:</description></item><item><title>Lipschitz Condition</title><link>https://freshrimpsushi.github.io/en/posts/684/</link><pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/684/</guid><description>Definition We can find the Lipschitz condition in the statement of Existence-Uniqueness Theorem for First Order Differential Equations. For a continuous function defined in $D \subset \mathbb{R}^2$ with an initial value problem given by $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$, if $f$ satisfies the Lipschitz condition for all $(x,y_{1}) , (x , y_{2} ) \in D$ and $K &amp;gt; 0$, $$ |f(x,y_{1} ) -</description></item><item><title>Euler-Lagrange Equation in Physics</title><link>https://freshrimpsushi.github.io/en/posts/1183/</link><pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1183/</guid><description>Overview This article assumes that the reader has already gone through the category of classical mechanics, specifically Lagrangian Mechanics and Hamilton&amp;rsquo;s Principle of Least Action. While this article will attempt to re-explain notations and content, even if they have been covered before, it is recommended to refer to the linked document for any unexplained notation. The integral of the Lagrangian over a path of motion is called the action and</description></item><item><title>Gaussian Quadrature for Numerically Computing Improper Integrals</title><link>https://freshrimpsushi.github.io/en/posts/1161/</link><pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1161/</guid><description>Definition 1 Gauss-Chebyshev Quadrature $$ \int_{-1}^{1} {{ 1 } \over { \sqrt{1 - x^2 } }} f(x) dx \approx \sum_{i=1}^{n} w_{i} f( x_{i} ) $$ $$ w_{i} = {{ \pi } \over { n }} $$ Here, $x_{i}$s are the Chebyshev nodes that satisfy $T_{n}(x) = 0$. Gauss-Laguerre Quadrature $$ \int_{0}^{\infty} e^{-x} f(x) dx \approx \sum_{i=1}^{n} w_{i} f( x_{i} ) $$ $$ w_{i} = {{ x_{i} } \over { (n+1)^2</description></item><item><title>Functions of Series</title><link>https://freshrimpsushi.github.io/en/posts/1160/</link><pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1160/</guid><description>Definitions Let&amp;rsquo;s define the series of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$. (1) If $\displaystyle \sum_{k=1}^{n} f_{k} (X)$ when $n \to \infty$, then the series $\displaystyle \sum_{k=1}^{ \infty } f_{k}$ is said to converge pointwise in $E$ if it converges pointwise. (2) If $\displaystyle \sum_{k=1}^{n} f_{k} (X)$ when $n \to \infty$, then the series $\displaystyle \sum_{k=1}^{ \infty } f_{k}$ is said to converge uniformly in $E$ if it</description></item><item><title>Lagrangian Mechanics and Hamiltons Variational Principle</title><link>https://freshrimpsushi.github.io/en/posts/1182/</link><pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1182/</guid><description>Overview Hamilton&amp;rsquo;s principle, functionals, action, and variation are explained here in a way that is as simple as possible. If you have not found a satisfactory explanation elsewhere, it is recommended to read through to the end. This has been written so that even freshmen and sophomores in college can understand it. Lagrangian Mechanics1 When an object moves from time $t_{1}$ to $t_{2}$, the integral of the Lagrangian over the</description></item><item><title>Hermite Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1159/</link><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1159/</guid><description>Definition Probabilist&amp;rsquo;s Hermite Polynomial $$ H_{e_{n}} := (-1)^{n} e^{{x^2} \over {2}} {{d^{n}} \over {dx^{n}}} e^{- {{x^2} \over {2}}} $$ Physicist&amp;rsquo;s Hermite Polynomial $$ H_{n} := (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2} $$ Basic Properties Hermite polynomials are used in two forms, having a relationship as shown in $H_{n} (x) = 2^{{n} \over {2}} H_{e_{n}} \left( \sqrt{2} x \right)$. Recurrence Relation [0]: $$H_{n+1} (x) = 2x H_{n} (x) - H_{n} '</description></item><item><title>Proof that the Hopf-Lax Formula Satisfies the Hamilton-Jacobi Equation</title><link>https://freshrimpsushi.github.io/en/posts/1178/</link><pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1178/</guid><description>Theorem 1 Hopf-Lax Formula $$ u(x,t) = \min \limits_{y \in \mathbb{R}^n} \left\{ tL\left( \dfrac{x-y}{t} \right) +g(y) \right\} $$ Let&amp;rsquo;s denote $x \in \mathbb{R}^n$ and $t&amp;gt;0$. And suppose that $u$ defined by the Hopf-Lax formula is differentiable at point $(x,t)$. Then, $u$ satisfies the Hamilton-Jacobi Equation. $$ u_{t}(x, t) + H\big( Du(x, t) \big) =0 $$ Proof Lemma: Generalization of Hopf-Lax Formula Let&amp;rsquo;s denote $t&amp;gt;0$. Then, for any given $x \in</description></item><item><title>The Difference between Pointwise Convergence and Uniform Convergence of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1158/</link><pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1158/</guid><description>Let&amp;rsquo;s define the function $f : E \to \mathbb{R}$ and the sequence of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$ for the subset $E \ne \emptyset$ of $\mathbb{R}$. Pointwise Convergence It is said that $f_{n}$ converges pointwise to $f$ in $E$ if, for every $\varepsilon &amp;gt; 0$ and $x \in E$, there exists $N \in \mathbb{N}$ satisfying $n \ge N \implies | f_{n} (x) - f(x) | &amp;lt; \varepsilon$,</description></item><item><title>Fundamental Theorem of Algebra</title><link>https://freshrimpsushi.github.io/en/posts/1179/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1179/</guid><description>Theorem1 Suppose that $p, q, r \ge 1$ satisfies $\dfrac{1}{p} + \dfrac{1}{q} + \dfrac{1}{r} = 2$. Then, for all ${u \in L^{p}(\mathbb{R}^{n})}$, ${v \in L^{q}(\mathbb{R}^{n})}$, ${w \in L^{r}(\mathbb{R}^{n})}$, the following equation holds. $$ \begin{equation} \left| \int_{\mathbb{R}^{n}} (u \ast v)(x)w(x)dx \right| \le \left\| u \right\|_{p} \left\| v \right\|_{q} \left\| w \right\|_{r} \end{equation} $$ Here, $u \ast v$ is the convolution of $u$ and $v$. Description This is called Young&amp;rsquo;s theorem. The</description></item><item><title>Inner Enclosure Boundary in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/383/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/383/</guid><description>Definition Let&amp;rsquo;s say $\left( X, d \right)$ for a metric space. When there exists an open set $O$ that satisfies $x \in O \subset A$, $x$ is called an Interior Point of $A$. The set of interior points of $A$, $A^{\circ}$, is called the Interior of $A$. The union $\overline{A} : = A \cup a '$ of $A$ and its codomain is called the Closure of $A$. When it</description></item><item><title>Laguerre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1156/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1156/</guid><description>Definition $\displaystyle L_{n} := {{ e^{x} } \over { n! }} {{ d^{n} } \over { dx^{n} }} \left( e^{-x} x^{n} \right)$ is called Laguerre Polynomial. Basic Properties Recursion Formula [0]: $$L_{n+1} (x) = {{ 1 } \over { n+1 }} \left[ \left( 2n + 1 - x \right) L_{n} (x) - n L_{n-1} (x) \right]$$ Orthogonal Set [1] Inner Product of Functions: Given the weight $w$ as $\displaystyle w(x)</description></item><item><title>The Accumulation Point in the Set of Real Numbers</title><link>https://freshrimpsushi.github.io/en/posts/379/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/379/</guid><description>Definition Given a point $x \in \mathbb{R}$ on the real line and a subset $A \subset \mathbb{R}$, if for any open set $O$ containing $x$, $O \cap ( A \setminus \left\{ x \right\} ) \ne \emptyset$ holds, then $x$ is defined as a Limit Point. The set of limit points of $A$ is called the Derived set of $A$, and is denoted by $A '$. Explanation In the above definition,</description></item><item><title>The set of real numbers and the empty set are both open and closed.</title><link>https://freshrimpsushi.github.io/en/posts/378/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/378/</guid><description>Theorem $\mathbb{R}$ and $\emptyset$ are both open and closed. Description On the set of real numbers $\mathbb{R}$, the union of multiple intervals is called an open set. For example, $(-1,0) \cup (2,3)$ is obviously an open set, and so are $(0,1)$ and $\mathbb{R}$. Meanwhile, being closed is defined through being open. For any subset of real numbers $C$, if $R \setminus C$ is open, then $C$ is called a closed</description></item><item><title>Hopf-Lax Formula</title><link>https://freshrimpsushi.github.io/en/posts/1174/</link><pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1174/</guid><description>Buildup1 Let&amp;rsquo;s consider the initial value problem of the Hamilton-Jacobi equation that depends only on $H$ as $Du$ for the Hamilton-Jacobi equation. $$ \begin{equation} \left\{ \begin{aligned} u_{t} + H(Du)&amp;amp;=0 &amp;amp;&amp;amp; \text{in } \mathbb{R}^n \times (0,\infty) \\ u&amp;amp;=g &amp;amp;&amp;amp; \text{on } \mathbb{R}^n \times \left\{ t=0 \right\} \end{aligned} \right. \end{equation} $$ Generally, the Hamiltonian depends on the spatial variables as in the form of $H(Du, x)$, but let&amp;rsquo;s say here it is</description></item><item><title>Uniform Convergence of Function Series</title><link>https://freshrimpsushi.github.io/en/posts/1154/</link><pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1154/</guid><description>Definition Let&amp;rsquo;s define a subset $E \ne \emptyset$ of $\mathbb{R}$, function $f : E \to \mathbb{R}$, and sequence of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$. If there exists $N \in \mathbb{N}$ for every $\varepsilon &amp;gt; 0$ satisfying $n \ge N \implies | f_{n} (x) - f(x) | &amp;lt; \varepsilon$, then sequence $f_{n}$ converges uniformly to $f$ in $E$, denoted by: $$ f_n \rightrightarrows f $$ or $$ f_{n}</description></item><item><title>Mellin Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1176/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1176/</guid><description>Definition For a given function $f : [0, \infty) \to \mathbb{C}$, if the following integral exists, it is called the Mellin transform of $f$, and such integral transformations are denoted by $\mathcal{M}$. $$ \mathcal{M}f (s) = \int_{0}^{\infty} x^{s-1}f(x)dx = \phi (s),\quad s \in \mathbb{C} $$ The inverse Mellin transform is as follows. $$ \mathcal{M}^{-1}\phi (x) = \dfrac{1}{2\pi i }\int_{c-i\infty}^{c+i\infty} x^{-s}\phi (s) ds $$ Explanation It is a kind of integral</description></item><item><title>Pointwise Convergence of Function Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1148/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1148/</guid><description>Definition Let us define a function $f : E \to \mathbb{R}$ for the subset $E \ne \emptyset$ of $\mathbb{R}$. If the sequence of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$ satisfies $f(x) = \lim \limits_{n \to \infty} f_{n} (X)$ for each $x \in E$, then it is said to converge pointwise to $f_{n}$ in $E$, denoted by: $$ f_{n} \to f $$ Explanation Rewriting the above definition using the</description></item><item><title>Hamiltonian and Lagrangian Convex Duality</title><link>https://freshrimpsushi.github.io/en/posts/1172/</link><pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1172/</guid><description>Theorem1 Legendre Transform $L$ is a convex function. $\lim \limits_{ |v|\to \infty} \dfrac{ L(v) }{ |v| }=+\infty$ Given the conditions, the Legendre transform $L^{\ast} : \mathbb{R}^{n} \to \mathbb{R}$ of $L$ for the Lagrangian $L : \mathbb{R}^{n} \to \mathbb{R}$ is defined as follows: $$ L^{\ast} (p) := \sup \limits_{v \in \mathbb{R}^{n}} \big( p\cdot v -L(v) \big) \quad \forall \ p \in \mathbb{R}^{n} $$ Let&amp;rsquo;s assume the Lagrangian $L$ satisfies the conditions</description></item><item><title>Tricks for Variable Substitution to Compute Improper Integrals Numerically</title><link>https://freshrimpsushi.github.io/en/posts/1147/</link><pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1147/</guid><description>Theorem 1 Let&amp;rsquo;s say $0 &amp;lt; a &amp;lt; b &amp;lt; \infty$. [1]: If $ 0 &amp;lt; p &amp;lt; 1$ then $$\int_{0}^{b} {{ f(x) } \over {x^{p} }} dx = \int_{0}^{{{ 1 } \over { 1-p }} b^{1-p} } f \left( \left[ ( 1- p ) m \right]^{{{ 1 } \over { 1-p }}} \right) dm$$ [2]: If $ 1 &amp;lt; p$ then $$\int_{a}^{ \infty } {{ f(x) } \over {x^{p}</description></item><item><title>Functions That Cannot Be Integrated over a Closed Interval: The Dirichlet Function</title><link>https://freshrimpsushi.github.io/en/posts/1146/</link><pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1146/</guid><description>Definition The Dirichlet function is defined as follows: $f$ Explanation The Dirichlet function is a classic example of a function that can&amp;rsquo;t be integrated using Riemann integration. Unless one advances in the study beyond analysis, it&amp;rsquo;s unlikely to even imagine such a peculiar example. The specific mention that it can&amp;rsquo;t be integrated using Riemann integration is because it may be integrable by methods other than Riemann integration. Theorem The Dirichlet</description></item><item><title>Legendre Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1171/</link><pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1171/</guid><description>Regarding x and p, when emphasizing that they are variables of a partial differential equation, it is indicated with regular font $x,p \in \mathbb{R}^{n}$, and when emphasizing that it is a function of $s$, it is indicated with bold font $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Definition 1 For simplicity, let&amp;rsquo;s say the Lagrangian is a function of variable $v\in \mathbb{R}^{n}$ only. $$ L(v) = L : \mathbb{R}^{n} \to \mathbb{R} $$ Let&amp;rsquo;s</description></item><item><title>Gaussian Quadrature</title><link>https://freshrimpsushi.github.io/en/posts/1144/</link><pubDate>Thu, 04 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1144/</guid><description>Definition 1 Let $f : [a,b] \to \mathbb{R}$ be integrable over $[a,b]$ and divide $[a,b]$ into nodes such as $a = x_{1} &amp;lt; \cdots &amp;lt; x_{n} = b$. $$ I_{n} (f) := \sum_{j=1}^{n} w_{j} f ( x_{j} ) \approx \int_{a}^{b} f(x) dx = I ( f ) $$ Calculating the weights $w_{j}$ of the defined $I_{n}$ and computing the numerical integration this way is called Gaussian quadrature. Explanation As there</description></item><item><title>Laplace Transform Convolution</title><link>https://freshrimpsushi.github.io/en/posts/1170/</link><pubDate>Thu, 04 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1170/</guid><description>Definitions1 Let&amp;rsquo;s call $\mathcal{L}$ the Laplace transform. The function $f*g$ that satisfies the following expression is called the convolution with respect to the Laplace transforms of $f$ and $g$. $$ \mathcal{L}(f*g) = \mathcal{L}(f) \cdot \mathcal{L}(g) $$ Theorem The convolution of $f$ and $g$ with respect to their Laplace transforms $h=f*g$ is as follows. $$ h(t) = f*g(t) = \int_{0}^t f(t-\tau)g(\tau)d\tau = \int_{0}^t f(\tau) g(t-\tau)d\tau $$ Proof $$ \begin{align*} \mathcal{L} \left\{</description></item><item><title>Hamiltonian Equations Derived from Variational Calculus and Euler-Lagrange Equation</title><link>https://freshrimpsushi.github.io/en/posts/1168/</link><pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1168/</guid><description>For variables x and p, when emphasizing that they are variables in partial differential equations, they are denoted in regular font $x,p \in \mathbb{R}^{n}$, and when emphasizing as a function of $s$, they are denoted in bold $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Similarly, v is denoted in regular font $v \in \mathbb{R}^{n}$ to emphasize it as a variable, and in bold $\mathbf{v} \in \mathbb{R}^{n}$ to emphasize it as a function. There</description></item><item><title>Negative Binomial Coefficient</title><link>https://freshrimpsushi.github.io/en/posts/1143/</link><pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1143/</guid><description>Definition For $r,k \in \mathbb{N}$, $\displaystyle \binom{-r}{k}$ is called the Negative Binomial Coefficient. Explanation As the name suggests, the negative binomial coefficient is an extension of the binomial coefficient to negative numbers. Mathematically, there&amp;rsquo;s no reason not to compute it for $\alpha \in \mathbb{Z}$ as shown in $\displaystyle \binom{\alpha}{k} = {{ \alpha ( \alpha - 1 ) \cdots ( \alpha - k + 1 ) } \over { k! }}$.</description></item><item><title>Hilbert Transform</title><link>https://freshrimpsushi.github.io/en/posts/1167/</link><pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1167/</guid><description>Buildup Radon Inversion $$ f(x,y)=\dfrac{1}{2} \mathcal{B} \left\{ \mathcal{F}^{-1} \Big[ |S|\mathcal{F} (\mathcal{R}f) (S,\ \theta) \Big]&amp;gt; \right\} (x,y) $$ This formula is for obtaining $f$ from Radon transform $\mathcal{R}f$ of $f$. First, recall the following property of the Fourier transform. $$ \mathcal{F} [f^{\prime} ] (\xi) = i\xi \mathcal{F}(\xi) $$ Here, if we substitute $\mathcal{R}f$ for $f$, we get the following. $$ \begin{equation} \mathcal{F} \left( \dfrac{\partial (\mathcal{R}f)(t,\ \theta) } {\partial t} \right) (S,\</description></item><item><title>The Euler Constant e is an Irrational Number</title><link>https://freshrimpsushi.github.io/en/posts/1141/</link><pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1141/</guid><description>Theorem $\mathbb{Q}$ represents the set of rational numbers. Proof Using Maclaurin Expansion1 Strategy: Split $e^{-1}$ into two parts using the Maclaurin expansion and derive a contradiction. This proof cannot be conducted within the high school curriculum due to the necessity of the Maclaurin expansion. $\mathbb{N}$ represents the set of natural numbers, $\mathbb{Z}$ represents the set of integers. Part 1. $x_{1} = x_{2}$ Assuming $e \in \mathbb{Q}$, the Euler&amp;rsquo;s constant $e$</description></item><item><title>Radon Inverse Transform: Filtered Back Projection (FBP)</title><link>https://freshrimpsushi.github.io/en/posts/1166/</link><pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1166/</guid><description>Theorem There is a formula that holds for $f : \mathbb{R}^{2} \to \mathbb{R}$. Description Also known as The filtered back projection formula. Given the Radon transform $\mathcal{R}f$ of $f$, it is said that $f$ can be obtained using the Fourier transform and back projection. This means, applying a Fourier transform to the Radon transform, multiplying by $|S|$, then applying the inverse Fourier transform, and finally, back projection, is known as</description></item><item><title>Back Projection: The Dual of the Radon Transform</title><link>https://freshrimpsushi.github.io/en/posts/1164/</link><pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1164/</guid><description>Definition1 2 The dual operator $\mathcal{R}^{\#} : L^{2}(Z_{n}) \to L^{2}(\mathbb{R}^{n})$ of the Radon transform $\mathcal{R} : L^{2}(\mathbb{R}^{n}) \to L^{2}(Z_{n})$ is referred to as back projection. $$ \left\langle \mathcal{R}f ,g \right\rangle_{L^{2}(Z_{n})} = \left\langle f , \mathcal{R}^{\#}g \right\rangle_{L^{2}(\mathbb{R}^{n})} $$ Here, $Z_{n} := \mathbb{R}^{1} \times S^{n-1}$ is the unit cylinder of $\mathbb{R}^{n+1}$. Theorem Formula Specifically, back projection is as follows. $$ \mathcal{R}^{\#} g (\mathbf{x}) = \int_{S^{n-1}} g (\mathbf{x} \cdot \boldsymbol{\theta}, \boldsymbol{\theta}) d\boldsymbol{\theta} $$</description></item><item><title>Fourier Slice Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1165/</link><pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1165/</guid><description>Theorem For $f : \mathbb{R}^{2} \to \mathbb{R}$, the following equation holds: $$ \begin{equation} \mathcal{F}_2 f(\xi \cos\theta,\ \xi \sin\theta)=\mathcal{F}(\mathcal{R}f)(\xi ,\ \theta) \label{thm1} \end{equation} $$ Here, $\mathcal{F}$ represents the 1-dimensional Fourier transform, $\mathcal{F}_2$ represents the 2-dimensional Fourier transform, and $\mathcal{R}$ is the Radon transform. $$ \begin{align*} \mathcal{F}f (y) &amp;amp;= \int f(x) e^{-i xy } dx \\ \mathcal{F}_{2} f (y_{1}, y_{2}) &amp;amp;= \int \int f(x_{1}, x_{2}) e^{-i (x_{1}, x_{2}) \cdot (y_{1}, y_{2})} dx_{1}</description></item><item><title>Proof that Pi is an Irrational Number</title><link>https://freshrimpsushi.github.io/en/posts/1139/</link><pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1139/</guid><description>Theorem $$\pi \notin \mathbb{Q}$$ $\mathbb{Q}$ represents the set of rational numbers. Proof Strategy: Utilize the fact that integers do not have the property of being dense. Define the functions $f$, $F$ very cleverly, applying various tricks. This method was devised by Ivan Niven and is among the easiest to prove that $\pi$ is irrational, but unfortunately, because it employs the epsilon-delta argument, it cannot be proven within the scope of</description></item><item><title>Newton-Cotes Integration Formulas</title><link>https://freshrimpsushi.github.io/en/posts/1138/</link><pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1138/</guid><description>Definition 1 Assume that $f : [a,b] \to \mathbb{R}$ is integrable over $[a,b]$ and $[a,b]$ is divided into nodes with a constant interval of $\displaystyle h:= {{b-a} \over {n}}$, like in $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. The numerical integration operator $I_{n}^{p}$ defined as follows is called the Newton-Cotes formula. $$ I_{n}^{p} (f) := \sum_{i=0}^{n} w_{i} f ( x_{i} ) $$ For $i=0,1,\cdots , n$, $x_{i} :=</description></item><item><title>Hamilton-Jacobi Equation and Hamiltonian Equation</title><link>https://freshrimpsushi.github.io/en/posts/1162/</link><pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1162/</guid><description>There are two ways to derive the Hamilton equations. One is from the Euler-Lagrangian equations, and the other, which will be introduced in this article, is from the characteristic equations of the Hamilton-Jacobi equation. Definition1 The following partial differential equation is called the general Hamilton-Jacobi equation. $$ G(Du, u_{t}, u, x, t)=u_{t}+H(Du, x)=0 $$ $t &amp;gt;0 \in \mathbb{R}$ $x \in \mathbb{R}^{n}$ $u : \mathbb{R}^{n} \to \mathbb{R}$ Here, the differential operator</description></item><item><title>Importing Excel Data into MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1163/</link><pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1163/</guid><description>Method Matlab provides the functionality to import data from Excel files. First, click on &amp;lsquo;Import Data&amp;rsquo; from the Home menu. Select the Excel file that contains the data you want to import. Then, you can select which data to import, which is automatically selected initially. Confirm and click &amp;lsquo;Import Selected&amp;rsquo;. From &amp;lsquo;Import Selected&amp;rsquo;, click on &amp;lsquo;Import Data&amp;rsquo;. Then, the data from the Excel file is input into a variable with</description></item><item><title>Proof that the Square Root of 2 is Irrational</title><link>https://freshrimpsushi.github.io/en/posts/1137/</link><pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1137/</guid><description>Theorem $\sqrt{2}$ is irrational. Proof Strategy: To show $\sqrt{2}$ is irrational, we assume it can be expressed as a fraction in lowest terms and derive a contradiction. This method can be used to prove that $\sqrt{n}$ is irrational for every $n$ that is not a perfect square. Assuming $\sqrt{2}$ is rational, it can be expressed as a fraction of two natural numbers, $a,b$ and $\displaystyle \sqrt{2} = {{ a }</description></item><item><title>Lagrangians and Euler-Lagrange Equations in Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1157/</link><pubDate>Thu, 27 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1157/</guid><description>Definition1 Lagrangian Let&amp;rsquo;s assume a smooth function $L : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ is given. This is called the Lagrangian and is denoted as follows. $$ L = L(v,x)=L(v_{1}, \dots, v_{n}, x_{1}, \dots, x_{n}) \quad v,x\in \mathbb{R}^{n} \\ D_{v}L = (L_{v_{1}}, \dots, L_{v_{n}}), \quad D_{x}L = (L_{x_{1}}, \dots, L_{x_{n}}) $$ The reason for using variables $v, x$ is because, in physics, each variable actually signifies velocity and position. Action,</description></item><item><title>Simpson's Rule</title><link>https://freshrimpsushi.github.io/en/posts/1132/</link><pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1132/</guid><description>Definition Let $f : [a,b] \to \mathbb{R}$ be integrable on $[a,b]$ and divide $[a,b]$ into nodes with equal intervals of $\displaystyle h:= {{b-a} \over {n}}$ like $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. Then, the numerical integration operator $I_{n}^{2}$ defined as follows is called the Simpson&amp;rsquo;s Rule. $$ I_{n}^{2} (f) := \sum_{k=1}^{n/2} {{h} \over {3}} \left[ f(x_{2k-2}) + 4 f( x_{2k-1} ) + f(x_{2k} ) \right] $$ Theorem</description></item><item><title>How to Draw a Log-Log Scale Plot in R</title><link>https://freshrimpsushi.github.io/en/posts/1131/</link><pubDate>Tue, 25 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1131/</guid><description>Not a Good Approach win.graph(7,4); par(mfrow=c(1,2)) plot(pressure,main=&amp;#39;Pressure\&amp;#39;) y&amp;lt;-pressure[-1,]$pressure; logtemp&amp;lt;-log(y) x&amp;lt;-pressure[-1,]$temperature; logpress&amp;lt;-log(x) plot(logpress,logtemp,main=&amp;#39;log scale\&amp;#39;) The easiest way to draw a log-log scaled graph is to take the log of the data itself. If you are drawing a log-log plot for the first time, it&amp;rsquo;s definitely worthwhile to get familiar with this method. This approach works with R or any other language, so it can be used in a pinch. However, as</description></item><item><title>One-Dimensional D'Alembert's Formula</title><link>https://freshrimpsushi.github.io/en/posts/1151/</link><pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1151/</guid><description>Review1 Suppose the Cauchy problem for the wave equation is given as follows. $$ \begin{align*} u_{tt}-u_{xx}&amp;amp;= 0 &amp;amp;&amp;amp; \text{in } \mathbb{R}^2=\mathbb{R}_{x} \times \mathbb{R}_{t} \\ u=g,\quad u_{t}=&amp;amp;\h &amp;amp;&amp;amp; \text{on } \mathbb{R}\times\left\{t=0\right\} \end{align*} $$ Then $g \in C^2(\mathbb{R}), h\in C^1(\mathbb{R})$. Let&amp;rsquo;s define $u(x,t)$ as follows. $$ \begin{equation} u(x,t)=\dfrac{1}{2} \left[ g(x+t)+g(x-t) \right] + \dfrac{1}{2} \int_{x-t}^{x+t}h(y)dy \quad \forall\ (x,t)\in \mathbb{R}^2 \end{equation} $$ Then, $u\in C^2(\mathbb{R}^2)$ is the solution to the given Cauchy problem. Explanation</description></item><item><title>Trapezoidal Rule</title><link>https://freshrimpsushi.github.io/en/posts/1130/</link><pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1130/</guid><description>Definition Let&amp;rsquo;s assume $f : [a,b] \to \mathbb{R}$ is integrable over $[a,b]$ and $[a,b]$ is divided into nodes at intervals of $\displaystyle h:= {{b-a} \over {n}}$, like $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. The numerical integration operator $I_{n}^{1}$, defined as follows, is called the trapezoidal rule. $$ I_{n}^{1} (f) := \displaystyle \sum_{k=1}^{n} {{h} \over {2}} \left( f(x_{k-1}) + f(x_{k} ) \right) $$ Theorem Let&amp;rsquo;s say $f \in</description></item><item><title>How to Insert a Legend in R</title><link>https://freshrimpsushi.github.io/en/posts/1129/</link><pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1129/</guid><description>Code Data representation is just as important as data analysis. The more complex a figure becomes, the more meticulous annotations and clear legends help in understanding the data. The legend() function has a plethora of options, yet employing just the essential elements as below is beneficial. The first option for location can simply be input as a combination of &amp;ldquo;top&amp;rdquo;, &amp;ldquo;bottom&amp;rdquo; + &amp;ldquo;left&amp;rdquo;, &amp;ldquo;right&amp;rdquo;, or &amp;ldquo;center&amp;rdquo;, and specific coordinates can</description></item><item><title>How to Save Data Calculated in MATLAB to an Excel File</title><link>https://freshrimpsushi.github.io/en/posts/1150/</link><pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1150/</guid><description>Method When you want to organize the data calculated in MATLAB into Excel and the amount of data is not too much, you can manually copy and paste. However, for a matrix of data like 128*128 shown in the picture above, that method is not feasible. In this case, you can use xlswrite to save the data into an Excel file. Compared to the picture above, xlswrite('test', Y) has been</description></item><item><title>How to Comment and Uncomment Multiple Lines at Once in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1149/</link><pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1149/</guid><description>Method To comment out a section that you want, drag to select the area and then press Ctrl+R. This will comment out the entire selected portion. To undo, drag to select the same area and press Ctrl+T, which will remove the % from each line.</description></item><item><title>Numerical Integration</title><link>https://freshrimpsushi.github.io/en/posts/1128/</link><pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1128/</guid><description>Definition 1 Let&amp;rsquo;s assume that $f : [a,b] \to \mathbb{R}$ is integrable over $[a,b]$, and $[a,b]$ is divided into nodes like $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. The integral operator $I$ is defined as $\displaystyle I(f) := \int_{a}^{b} f(x) dx$. The integral operator $I_{n}$ is defined as $\displaystyle I_{n} (f) := \sum_{k=1}^{n} \int_{x_{k-1}}^{x_{k}} f(x) dx$. The error $E_{n}$ is defined as $E_{n} (f) := I (f) -</description></item><item><title>Referencing Metadata and attr in R</title><link>https://freshrimpsushi.github.io/en/posts/1127/</link><pubDate>Fri, 21 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1127/</guid><description>Overview When using various functions in R, you might occasionally come across data like attr(,&amp;quot;something&amp;quot;). The term Attribute literally means property. Unlike languages like Python, in R, attributes can be thought of as metadata or a form of annotation within the data. Sometimes, while using R, you might find the need to refer to this data. Example For instance, after performing standardization, the mean is stored as attr(,&amp;quot;scaled:center&amp;quot;) and the</description></item><item><title>Methods of Expressing an Arbitrary Function as Two Non-negative Functions</title><link>https://freshrimpsushi.github.io/en/posts/1145/</link><pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1145/</guid><description>Definitions1 Let&amp;rsquo;s define $f^{+}$ and $f^{-}$ for a function $f : X \to \mathbb{R}$ as follows. $$ \begin{align*} f^{+} (x) &amp;amp;:= \max \left\{ f(x),\ 0 \right\} \\ f^{-} (x) &amp;amp;:= \max \left\{ -f(x),\ 0 \right\} \end{align*} $$ We call $f^{+}$ the positive part of $f$, and $f^{-}$ the negative part of $f$. Description Despite their names, both $f^{+}$ and $f^{-}$ are non-negative functions. It might not be immediately clear why</description></item><item><title>Proof of Bernoulli's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1126/</link><pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1126/</guid><description>Theorem If we call it $\alpha &amp;gt; 0$, for all $x \in [ - 1, \infty )$, the following two inequalities hold: [1]: $\alpha \in (0, 1] \implies (1 + x )^{\alpha } \le 1 + \alpha x $ [2] $\alpha \in (1, \infty] \implies (1 + x )^{\alpha } \ge 1 + \alpha x $ Explanation Looking closely at the shape of the inequalities, although it depends on the</description></item><item><title>How to Concatenate a Vector of Strings into One String in R</title><link>https://freshrimpsushi.github.io/en/posts/1125/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1125/</guid><description>Overview R is a very convenient language for handling data, but if you are already familiar with other programming languages, the way R handles strings might feel somewhat unfamiliar. Unlike C or Python, R has a lot of built-in functionalities, and conversely, you often need to use these functionalities to handle data comfortably. It can be frustrating when built-in functions do not work as expected. Example For instance, let&amp;rsquo;s say</description></item><item><title>Properties of Measurable Functions with Real Values</title><link>https://freshrimpsushi.github.io/en/posts/1136/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1136/</guid><description>Theorem1 If two functions defined in a measurable space $(X,\mathcal{E})$ are measurable functions, then the following functions are also measurable: $$ cf,\quad f^2,\quad f+g,\quad fg,\quad |f| $$ Proof Measurable Function A function $f : X \to \overline{\mathbb{R}}$ is called a measurable function if it satisfies the following equation for all $\alpha \in \mathbb{R}$: $$ S_{f}(\alpha):=\left\{ x\in X\ |\ f(x) &amp;gt;\alpha \right\} \in \mathcal{E},\quad \forall \alpha \in \mathbb{R} $$ $cf$ Case</description></item><item><title>Chebyshev Nodes</title><link>https://freshrimpsushi.github.io/en/posts/1124/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1124/</guid><description>Definition From $[-1,1]$ to $\displaystyle x_{k} = \cos \left( {{2k-1} \over {2n}} \pi \right)$, $k=1, \cdots , n$ are called Chebyshev nodes. Explanation Unlike the equidistant node points typically used, Chebyshev nodes refer to the node points obtained by dividing the arc of a semicircle into equal sizes and projecting these points onto the $x$ axis. The distribution of points tends to gather a bit more at the extremes than</description></item><item><title>Accessing Lists in R</title><link>https://freshrimpsushi.github.io/en/posts/1123/</link><pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1123/</guid><description>Overview R offers many great features for handling data, and among them, the list is one of the primary reasons for using R. Although list data types are implemented in many other languages like Python, none offer the ease and intuitive manipulation of data that R provides. Mastering the use of lists allows for handling tasks that would be more complex and tedious in other programming languages with ease. Example</description></item><item><title>Chebyshev Expansion</title><link>https://freshrimpsushi.github.io/en/posts/1122/</link><pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1122/</guid><description>Buildup 1 To understand Chebyshev expansions, it&amp;rsquo;s essential first to grasp how Chebyshev expansions arise. Consider instead solving a least squares problem as opposed to solving a minimax problem. $$ M_{n} (f) := \inf_{\deg(r) \le n } \left\| f - r \right\|_{2} $$ Given the least squares problem as described above in $f : [a,b] \to \mathbb{R}$, the goal is to find a polynomial of degree $n$ or less $r_{n}^{</description></item><item><title>Finding the Location of Maximum and Minimum Values in R</title><link>https://freshrimpsushi.github.io/en/posts/1120/</link><pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1120/</guid><description>Code set.seed(150421) x&amp;lt;-sample(100,10); x which.max(x) which.min(x) When working with data for statistical purposes, it’s not only important to know what the maximum and minimum values are, but also necessary to identify their positions. This is particularly true for time series data. Of course, R doesn’t need such functions for manipulation to be easy, but it&amp;rsquo;s generally better to avoid complex code</description></item><item><title>Predictable Functions</title><link>https://freshrimpsushi.github.io/en/posts/1135/</link><pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1135/</guid><description>Definition1 Let’s call $(X, \mathcal{E})$ a measurable space. Let&amp;rsquo;s define the set $S_{f}(\alpha)$ as follows. $$ S_{f}(\alpha):=\left\{ x\in X\ |\ f(x) &amp;gt;\alpha \right\} = f^{-1}\left( (\alpha, \infty) \right),\quad \forall \alpha \in \mathbb{R} $$ If for every real number $\alpha \in \mathbb{R}$, $S_{f}(\alpha) \in \mathcal{E}$ holds, then the function $f : X \to \overline{\mathbb{R}}$ taking extended real values is called $\mathcal{E}$-measurable or simply measurable. Explanation Especially, if</description></item><item><title>Proof of the Stone-Weierstrass Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1117/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1117/</guid><description>Theorem1 Auxiliary Definitions Let&amp;rsquo;s say $A \subset C(X)$ for $X$. If for any distinct $x_{1}, x_{2} \in X$, there always exists $f \in A$ that satisfies $f(x_{1}) \ne f(x_{2})$, then we say $A$ separates the points of $X$. If $X$ is a metric space and for all $\varepsilon &amp;gt; 0$ and $f \in C(X)$ there exists $g \in A$ that satisfies $| g - f | &amp;lt; \varepsilon$, then $A$</description></item><item><title>Minimization and Maximization Approximations and Least Squares Approximations in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1116/</link><pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1116/</guid><description>Buildup 1 Let&amp;rsquo;s assume we are given the problem of approximating a given function $f : [a,b] \to \mathbb{R}$. Since computation is the responsibility of computers, our goal is to approximate it with a polynomial function $f$. Approximating a function means that we want to use a function similar to $f$ not just at a single point but across its entire domain $[a,b]$, so the goal is to reduce the</description></item><item><title>Algebra of the Space of Continuous Functions</title><link>https://freshrimpsushi.github.io/en/posts/1113/</link><pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1113/</guid><description>Definitions1 A set $A$ that satisfies the following three conditions is called the algebra of $C(X)$: (i): $\emptyset \ne A \subset C(X)$ (ii): $f,g \in A \implies (f+g) , fg \in A$ (iii): $f \in A , c \in \mathbb{R} \implies cf \in A$ Let&amp;rsquo;s say $X$ is for metric space $A \subset C(X)$. If every sequence $\left\{ f_{n} \in A : n \in \mathbb{N} \right\}$ of $A$ for some</description></item><item><title>Function Approximation in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1107/</link><pubDate>Mon, 10 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1107/</guid><description>Buildup Although it is true that computers are overwhelmingly faster at numerical calculations than humans, it isn&amp;rsquo;t because they understand transcendental functions or irrational numbers. For instance, when asked to calculate $\displaystyle \sin {{ \pi } \over {6}} = {{1} \over { 2 }}$, instead of drawing a right triangle and finding the ratio of the hypotenuse to the height using the geometric definition of trigonometric functions, it uses polynomial</description></item><item><title>Binomial Series Derivation</title><link>https://freshrimpsushi.github.io/en/posts/1103/</link><pubDate>Sun, 09 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1103/</guid><description>Formulas $|x| &amp;lt; 1$ implies $\alpha \in \mathbb{C}$, then $$ \begin{align*} (1 + x )^{\alpha} =&amp;amp; \sum_{k=0}^{\infty} \binom{\alpha}{k} x^{k} \\ =&amp;amp; 1 + \alpha x + \dfrac{\alpha (\alpha-1)}{2!}x^{2} + \dfrac{\alpha (\alpha-1)(\alpha-2)}{3!}x^{3} + \cdots \end{align*} $$ Negative Binomial Series $$ \begin{align*} (1 - x)^{-\alpha} &amp;amp;= \sum\limits_{k=0}^{\infty} \binom{\alpha + k - 1}{k} x^{k} \\ &amp;amp;= 1 + \alpha x + \dfrac{\alpha(\alpha+1)}{2!} x^{2} + \dfrac{\alpha(\alpha+1)(\alpha+2)}{3!} x^{3} + \cdots \end{align*} $$ Description Known as</description></item><item><title>Properties of Continuous Functions with Additivity</title><link>https://freshrimpsushi.github.io/en/posts/1102/</link><pubDate>Sat, 08 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1102/</guid><description>Theorem [1] If a continuous function $f : \mathbb{R} \to \mathbb{R}$ satisfies $f(x + y) = f(x) + f(y)$ for all $x, y \in \mathbb{R}$ $$ f(x) = f(1) x $$ [2] If a continuous function $g : \mathbb{R} \to ( 0 , \infty )$ satisfies $g(x + y) = g(x) g(y)$ for all $x, y \in \mathbb{R}$ $$ g(x) = \left( g(1) \right)^x $$ Explanation The property where addition</description></item><item><title>Generalized Binomial Coefficients for Complex Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1100/</link><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1100/</guid><description>Definition For a complex number $\alpha \in \mathbb{C}$, the following is called a Binomial Coefficient. $$ \binom{\alpha}{k} := \begin{cases} \displaystyle {{ \alpha ( \alpha - 1 ) \cdots ( \alpha - k + 1 ) } \over { k! }} &amp;amp; , k \in \mathbb{N} \\ 1 &amp;amp; ,k=0 \end{cases} $$ Description Originally, binomial coefficients have an intuitive meaning only when $\alpha \in \mathbb{N}$, but if we consider just the</description></item><item><title>Cauchy Product: The Product of Two Convergent Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1099/</link><pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1099/</guid><description>Theorem 1 If the convergence interval for $f(x) : = \sum _{k=0}^{\infty} a_{k} x^{k}$ and $g(x) : = \sum_{k=0}^{\infty} b_{k} x^{k}$ is $(-r,r)$ and assuming $c_{k} := \sum_{j=0}^{k} a_{j} b_{k-j}$, then $\sum_{k=0}^{\infty} c_{k} x^{k}$ converges to $f(x)g(x)$ within the convergence interval $(-r,r)$. Description The fact that the products of coefficients converge to the product of the coefficients of the two functions on their own is quite fascinating. It would be</description></item><item><title>Discrete Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/1121/</link><pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1121/</guid><description>Buildup1 Let&amp;rsquo;s assume that $f$ is some physical signal measured at specific intervals. Considering realistic conditions, there will be a moment when the measurement of the signal starts, $t=0$, and a moment when the measurement ends, $t=\Omega$. Thus, it can be assumed that the function value of $f$ is $0$ everywhere except for $[0, \Omega]$. Sampling Theorem Let&amp;rsquo;s assume that $\hat{f} \in L^{2}$ and $f (t) = 0\ \text{for }</description></item><item><title>Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1090/</link><pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1090/</guid><description>Definition A power series is denoted by $S(x) : = \sum \limits_{k=0}^{\infty} a_{k} ( x - x_{0} )^{k}$, and the Center of $S(x)$ is denoted by $x_{0}$. When $S(x)$ converges absolutely for $|x - x_{0}| &amp;lt; R$ and diverges for $|x - x_{0}| &amp;gt; R$, $R$ is called the Radius of Convergence of $S(x)$. The largest interval on which $S(x)$ converges is called the Interval of Convergence. If there exists</description></item><item><title>Calculating the Dot Product of Vectors in R</title><link>https://freshrimpsushi.github.io/en/posts/1089/</link><pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1089/</guid><description>Code x&amp;lt;-1:10; x y&amp;lt;-(-1)^(1:10); y sum(x*y) x %*% y x %o% y When analyzing or simulating in R, it&amp;rsquo;s common to calculate the weighted average. Of course, mathematically, $\displaystyle \left&amp;lt; \mathbf{x}, \mathbf{y} \right&amp;gt; = \sum_{i=1}^{n} x_{i} y_{i}$ is very simple, and since vector operations in R itself are very convenient, one can easily perform inner products just by using the sum() function. However, this can reduce the readability of the</description></item><item><title>Solving Differential Equations Using Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/1119/</link><pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1119/</guid><description>Description Fourier series and Fourier transforms emerged to solve heat equations. Of course, they can be used to solve other differential equations too, provided the conditions are met. Fourier series, in particular, are used in quantum physics to calculate the energy of particles through the Schrödinger equation. Many physics students use it without knowing it&amp;rsquo;s a Fourier series, but they</description></item><item><title>Properties of Radon Transform</title><link>https://freshrimpsushi.github.io/en/posts/1118/</link><pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1118/</guid><description>Properties1 The Radon transform $\mathcal{R} : L^{2}(\mathbb{R}^{n}) \to L^{2}(\Lambda)$ has the following properties. Linearity For $\alpha, \beta \in \mathbb{R}$ and $f, g \in L^{2}(\mathbb{R}^{2})$, the following holds. $$ \mathcal{R} \left( \alpha f + \beta g \right) = \alpha \mathcal{R}f + \beta \mathcal{R}g $$ Shift Invariance Let $T_{\mathbf{a}}$ be a translation for $\mathbf{a} \in \mathbb{R}^{n}$. $$ T_{\mathbf{a}}f(\mathbf{x}) := f(\mathbf{x}-\mathbf{a}) \text{ for } f\in L^{2}(\mathbb{R}^{n})\quad \text{and} \quad T_{t}g(s,\boldsymbol{\theta}) := g(s-t, \boldsymbol{\theta}) \text{</description></item><item><title>The Relationship between L1 Space and L2 Space</title><link>https://freshrimpsushi.github.io/en/posts/1114/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1114/</guid><description>Definition $L^{1}$ Space A function $f$ is said to be (absolutely) integrable on the interval $[a,\ b]$ if it satisfies the following equation. $$ \int_{a}^b |f(x)| dx &amp;lt; \infty $$ The set of functions that are integrable on the interval $[a,b]$ is denoted as $L^{1}(a,b)$. $$ L^{1}(a,b)= \left\{ f : \int_{-a}^{b} |f(x)| dx &amp;lt; \infty \right\} $$ $L^{2}$ Space A function satisfying the following equation is said to be square-integrable.</description></item><item><title>Drift in the ARIMA Model</title><link>https://freshrimpsushi.github.io/en/posts/1115/</link><pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1115/</guid><description>Explanation When analyzing time series, one often comes across a coefficient called Drift. Of course, in the case above, the coefficient is too small compared to the standard error to matter. However, if you actually have a significant coefficient and need to write it down in a formula, it&amp;rsquo;s necessary to understand what a drift is. Unfortunately, there are no good explanations about what drift actually is domestically, and without</description></item><item><title>Fourier Inversion Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1112/</link><pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1112/</guid><description>Buildup The process of deriving the Fourier transform also derived the definition of the inverse transform. However, this was simply explained to aid understanding, and the transformation formula was not accurately derived. The Fourier inverse transformation is as follows: $$ \begin{equation} f(x) =\dfrac{1}{2\pi} \int \hat{f}(\xi) e^{i\xi x}d\xi \end{equation} $$ This equation implies that from $f$, we can obtain $\hat{f}$ and from $\hat{f}$, we can retrieve $f$ again. This might seem</description></item><item><title>How to Get a List of Files from Command Prompt in Windows</title><link>https://freshrimpsushi.github.io/en/posts/1072/</link><pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1072/</guid><description>Overview When putting together a program that aggregates multiple sets of data, having too many files can be problematic. Naturally, every programming language has its own solutions, but temporary fixes often greatly assist when there is no need to repeat or when using a wide variety of languages. Let&amp;rsquo;s see how to do this quickly and easily using Windows Command Prompt. Guide Step 1. Copy the Address Enter the folder</description></item><item><title>Convergence of Mollification</title><link>https://freshrimpsushi.github.io/en/posts/1109/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1109/</guid><description>Theorem Let&amp;rsquo;s assume the following holds for a Mollifier $\eta_{\epsilon}$. $\displaystyle \alpha= \int_{-\infty}^{0} \eta_{\epsilon}(x) dx$ $\displaystyle \beta=\int_{0}^{\infty} \eta_{\epsilon} (x) dx$ Let&amp;rsquo;s say $\alpha + \beta = 1$. And say $f$ is piecewise continuous and bounded. Then, the mollification of $f$ converges as follows. $$ \lim \limits_{\epsilon \rightarrow 0} f \ast \eta_{\epsilon}(x) = \alpha f(x+) + \beta f(x-) $$ If $\eta_{\epsilon}(x)$ is an even function, $$ \lim \limits_{\epsilon \rightarrow 0} f</description></item><item><title>Exponential Function Set and Trigonometric Function Set are Orthogonal Bases</title><link>https://freshrimpsushi.github.io/en/posts/1108/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1108/</guid><description>Theorem Both sets $\left\{ e^{inx} \right\}_{n=-\infty}^\infty$ and $\left\{ \cos nx\ \right\}_{n=0}^\infty \cup \left\{ \sin nx \right\}_{n=1}^\infty$ are the orthonormal bases of $L^{2}(-\pi,\ \pi)$. Furthermore, $\left\{ \cos nx \right\}_{n=0}^{\infty}$ and $\left\{ \sin nx \right\}_{n=1}^{\infty}$ are the orthonormal bases of $L^{2}(0,\ \pi)$. Explanation This fact explains the reason why it is valid to express a given function as a series of trigonometric functions in Fourier series. Proof Let $\phi_{n}(x)=e^{inx}$. And assume $f</description></item><item><title>Lines Determined by Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1110/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1110/</guid><description>Description Lines Determined by Polar Coordinates A line, as shown in Figure (1), is determined by the slope $a$ and the $y$ intercept $b$. It seems that all lines on a plane can be represented only by their slope and intercept, but this is not the case. Precisely, only lines can be depicted as functions. Therefore, a line perpendicular to the $x$ axis, as shown in Figure (2), cannot be</description></item><item><title>Seasonal ARIMA Model</title><link>https://freshrimpsushi.github.io/en/posts/1067/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1067/</guid><description>Model 1 An operator defined as $\nabla_{s} Y_{t} := Y_{t} - Y_{t-s}$ is called the Seasonal Difference. If $W_{t} := \nabla^{d} \nabla_{s}^{D} Y_{t}$ is defined as $\left\{ W_{t} \right\}_{t \in \mathbb{N}}$, and if $ARMA(P,Q)$ and $\left\{ Y_{t} \right\}_{t \in \mathbb{N}}$ is $ARMA(p,q)$, then $\left\{ Y_{t} \right\}_{t \in \mathbb{N}}$ is called a Seasonal ARIMA process $ARIMA(p,d,q)\times(P,D,Q)_{s}$. This form is called the Seasonal ARIMA model. Explanation Today&amp;rsquo;s temperature is, of course, mostly</description></item><item><title>Box-Cox Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1065/</link><pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1065/</guid><description>Buildup $x &amp;gt; 0$ is referred to as a Box-Cox transformation on $g(x) := \begin{cases} \displaystyle {{ x^{\lambda} - 1 } \over { \lambda }} &amp;amp; , \lambda \ne 0 \\ \log x &amp;amp; , \lambda = 0 \end{cases}$. $g$, originally known as Power Transformation, was introduced by Box and Cox, hence it is also called the Box-Cox transformation. The main uses of the Box-Cox transformation are to make data</description></item><item><title>Fourier Transform of Characteristic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1105/</link><pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1105/</guid><description>Formulas The Fourier transform of the characteristic function is as follows: $$ \mathcal{F} \left[ \chi_{[-a,a]}(x) \right] = \dfrac{2 \sin(a\xi) }{\xi} $$ Proof $$ \begin{align*} \mathcal{F} \left[ \chi_{[-a,a]}(x) \right] &amp;amp;= \int_{-\infty}^{\infty} \chi_{[-a,a]}(x)e^{-i \xi x } dx \\ &amp;amp;= \int_{-a}^{a}e^{-i \xi x} dx \\ &amp;amp;= \dfrac{1}{-i\xi} \left. e^{-i\xi x}\right]_{-a}^{a} \\ &amp;amp;= \dfrac{1}{-i\xi} \left( e^{-i a \xi} - e^{i a \xi} \right) \\ &amp;amp;= \dfrac{2}{\xi} \dfrac{e^{ia\xi} -e^{-ia\xi}}{2i} \\ &amp;amp;= \dfrac{2}{\xi} \sin (a\xi) \end{align*}</description></item><item><title>Riemann-Lebesgue Lemma</title><link>https://freshrimpsushi.github.io/en/posts/1106/</link><pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1106/</guid><description>Theorem1 Let&amp;rsquo;s assume $f \in$ $L^{1}$ is given. Then, the following equation holds. $$ \lim \limits_{n \to \pm \infty} \hat{f}(\xi) = 0 $$ Here, $\hat{f}$ is the Fourier transform of $f$. Proof step 1 Proofs for the step function $f$, and generalization in step 2. Note that $f$ in step 1 and step 2 are not the same. case 1 Assume $f$ is the following step function. $$ f(x) =</description></item><item><title>Fourier Transform of Gaussian Functions</title><link>https://freshrimpsushi.github.io/en/posts/1104/</link><pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1104/</guid><description>Formulas The Fourier transform of a Gauss function $f(x)=e^{-Ax^2}$ is given as follows. $$ \mathcal{F}[f] (\xi) = \mathcal{F} \left[ e^{-Ax^2} \right] (\xi)=\sqrt{\frac{\pi}{A}}e^{-\frac{\xi ^2}{4A}} $$</description></item><item><title>Multidimensional Nonlinear Maps</title><link>https://freshrimpsushi.github.io/en/posts/1052/</link><pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1052/</guid><description>Definition of Nonlinear Maps If the map $\mathbf{f} : \mathbb{R}^{m} \to \mathbb{R}^{m}$ is not linear, it is said to be nonlinear. Buildup 1 It is hard to prove that a map is linear, but easy to show it is nonlinear; linear problems are easy, but nonlinear problems are challenging. Almost everything in this universe is nonlinear, and because nonlinearity is difficult, humans start by trying to convert nonlinearities into linearities.</description></item><item><title>Properties of Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/1101/</link><pubDate>Thu, 23 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1101/</guid><description>Theorem[^1] Let&amp;rsquo;s consider $\cal{F}f, \hat{f}$ as the Fourier transform of $f$. Let $f \in L^{1}$. Then, the following properties hold for the Fourier transform: (a) For any real number $a$, $$ \mathcal{F} \left[ f(x-a) \right] ( \xi ) = e^{-ia\xi}\hat{f}(\xi) \quad \mathrm{and} \quad \mathcal{F} \left[ e^{iax}f(x)\right] (\xi) = \hat{f}(\xi-a) $$ (b) Define $f_\delta (x) := \frac{1}{\delta}f ( \frac{x}{\delta} )$ for $\delta &amp;gt;0$. Then, $$ \mathcal{F}\left[ f_\delta \right] (\xi ) =</description></item><item><title>Additive and Multiplicative Functions</title><link>https://freshrimpsushi.github.io/en/posts/1096/</link><pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1096/</guid><description>Given a function $f : X \to Y$, let $a, b \in X$, $a_{i} \in X\ (i=1,\cdots)$. Subadditive Function A function $f$ is called a subadditive function when it satisfies the following equation: $$ f(a+b) \le f(a)+f(b) $$ The absolute value is an example. $$ |3+(-4)| \le |3|+|-4| $$ Another example, if we have $f(x)=2x+3$ then $$ 13=f(2+3) \le f(2)+f(3)=7+9=16 $$ Additive Function A function $f$ is called an additive</description></item><item><title>Proof of the Karatheodory's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1095/</link><pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1095/</guid><description>Definition1 For all $A \subset X$, if the following equation holds, then $E \subset X$ is said to satisfy the Caratheodory condition, or $E$ is said to be $\mu^{\ast}$-measurable. $$ \begin{equation} \mu^{\ast}(A) = \mu^{\ast}(A\cap E) + \mu^{\ast}(A \cap E^{c}) \label{def1} \end{equation} $$ $\mu^{\ast}$ is an outer measure. Theorem Let $L$ be the set that includes all $E \subset X$ that satisfy the Caratheodory condition. Then, $L$ is a $\sigma$-algebra. Also,</description></item><item><title>Linearization of Boundaries in Nonlinear First-Order Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1092/</link><pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1092/</guid><description>Buildup One method to easily solve the characteristic equation of a nonlinear first-order differential equation is to straighten out a small portion, $\Gamma$, of the boundary, $\partial \Omega$, of the domain, $\Omega$. Since this is always possible, in the vicinity of a point $x^{0}$ on the boundary, one can assume from the start that the boundary is a straight line and approach the problem. This is called straightening the boundary.</description></item><item><title>Multidimensional Linear Maps</title><link>https://freshrimpsushi.github.io/en/posts/1048/</link><pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1048/</guid><description>Definition 1 If a map $T_{A} : \mathbb{R}^{m} \to \mathbb{R}^{m}$ satisfies $$ T_{A} ( a \mathbf{x} + b \mathbf{y} ) = a T_{A} ( \mathbf{x} ) + b T_{A} ( \mathbf{y} ) $$ for all $a,b \in \mathbb{R}$ and $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{m}$, then $T_{A}$ is said to be linear. Let&amp;rsquo;s refer to the eigenvalues of $A$ as $\lambda_{1} , \cdots , \lambda_{m}$. If $| \lambda_{1} | \ne 1, \cdots</description></item><item><title>Generalized Hölder's Inequality, Corollaries of Hölder's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1091/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1091/</guid><description>Description Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. Suppose we are given two constants $1 \lt p \lt \infty, 1 \lt p^{\prime} \lt \infty$ that satisfy the following equation: $$ \dfrac{1}{p} + \dfrac{1}{p^{\prime}} = 1 \left(\text{or } p^{\prime} = \frac{p}{p-1} \right) $$ If $u \in L^p(\Omega)$, $v\in L^{p^{\prime}}(\Omega)$, then $uv \in L^1(\Omega)$ and the inequality below holds. $$ \| uv \|_{1} = \int_{\Omega} |u(x)v(x)| dx \le \| u</description></item><item><title>B-Splines in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1045/</link><pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1045/</guid><description>If you dislike reading texts and formulas, it&amp;rsquo;s perfectly fine to just look at the images to understand. Definition 1 Let&amp;rsquo;s divide the interval $[a,b]$ into node points such as $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Considering additional nodes $x_{-K} &amp;lt; x_{-K + 1} &amp;lt; \cdots &amp;lt; x_{-1} &amp;lt; x_{0}$ and $x_{N} &amp;lt; x_{N + 1} &amp;lt; \cdots &amp;lt; x_{N+K-1} &amp;lt; x_{N+K}$</description></item><item><title>Holder Continuous Function Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1087/</link><pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1087/</guid><description>Definitions1 Space of Continuous Functions Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. For a non-negative integer $m$, for all multi-indices $\alpha$ where $|\alpha| \le m$, the set of $\phi$ that are continuous in $\Omega$ for $D^{\alpha}\phi$ is called the space of continuous functions. $$ C^{m}\left( \Omega \right) := \left\{ \phi : D^{\alpha} \phi \text{ is continuous on } \Omega, \forall \left| \alpha \right| \lt m \right\} $$ In</description></item><item><title/><link>https://freshrimpsushi.github.io/en/posts/1086/</link><pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1086/</guid><description>Definition Fourier Transform as a Function We define the Fourier transform of function $f \in$ $L^{1}$ as follows: $$ \hat{f}(\xi) := \int_{-\infty}^{\infty} f(t) e^{-i \xi t}dt $$ Fourier Transform as an Operator An operator defined as $\mathcal{F} : L^{1} \to$ $C_{0}$ is called the Fourier transform. $$ \mathcal{F}[f] (\xi) = \int_{-\infty}^{\infty} f(t) e^{-i \xi t}dt $$ Explanation As seen in the definition, the term Fourier transform can refer to the</description></item><item><title>Sorting Data Frames by Columns in R</title><link>https://freshrimpsushi.github.io/en/posts/1043/</link><pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1043/</guid><description>Overview Sorting data in R can be easily accomplished by using the sort() function, which by default only sorts vectors. However, in practice, dealing with numerous categories in a data frame often necessitates the ability to sort based on columns as well. Code x&amp;lt;-c(pi,3,99,0,-1) order(x) x[order(x)] head(iris) head(iris[order(iris$Petal.Length)]) This demonstrates how computing the &amp;lsquo;position&amp;rsquo; vector in ascending order from the beginning for every component of the vector can be achieved,</description></item><item><title>Numerical Analysis in Splines</title><link>https://freshrimpsushi.github.io/en/posts/1036/</link><pubDate>Thu, 16 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1036/</guid><description>Buildup Interpolation is not about restoring the exact function but finding a similar yet more manageable function. Of course, it would be ideal if we could find one that is explicit and easy to calculate, but the universe is not so simple. Depending on the problem, it might be necessary to quickly solve simple parts and meticulously solve complex parts, and continuity might not even be guaranteed. In this way,</description></item><item><title>How to See Histograms in More Detail in R</title><link>https://freshrimpsushi.github.io/en/posts/1035/</link><pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1035/</guid><description>Code In R, you can easily draw a histogram using the hist() function. The size of the class intervals is determined automatically by R, but if you want a more detailed look, you can use the nclalss option. set.seed(150421) x&amp;lt;-runif(50) win.graph(7,4); par(mfrow=c(1,2)) hist(x) hist(x,nclass=20) The result of executing the above code is as follows.</description></item><item><title>Complete Orthonormal Basis and Complete Orthonormal Set</title><link>https://freshrimpsushi.github.io/en/posts/1082/</link><pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1082/</guid><description>Theorem: Equivalence Conditions of an Orthonormal Set Let $\left\{ \phi_{n} \right\}_{1}^\infty$ be an orthonormal set of $L^2(a,b)$ and denote $f \in L^2(a,b)$. Then, the following conditions are equivalent. $(a)$ For all $n$, if $\left\langle f, \phi_{n} \right\rangle=0$ then $f=0$. $(b)$ For all $f\in L^2(a,b)$, the series $\sum_{1}^\infty \left\langle f,\phi_{n}\right\rangle\phi_{n}$ converges to $f$ in the norm sense. That is, the following equation holds: $$ f=\sum_{1}^\infty \left\langle f,\phi_{n}\right\rangle\phi_{n} $$ $(c)$ For all</description></item><item><title>Hermite Interpolation</title><link>https://freshrimpsushi.github.io/en/posts/1034/</link><pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1034/</guid><description>Definition 1 For data $(x_{1}, y_{1} , y&amp;rsquo;_{1}) , \cdots , (x_{n} , y_{n}, y&amp;rsquo;_{n})$ of different $x_{1} , \cdots , x_{n}$ that satisfies $\begin{cases} p (x_{i} ) = y_{i} \\ p '(x_{i} ) = y&amp;rsquo;_{i} \end{cases}$ and $\deg H \le 2n-1$, the polynomial function $H$ is referred to as Hermite Interpolation. Theorem Existence and Uniqueness [1]: For the given data, $H$ uniquely exists. Lagrange Form [2]: $$H_{n} (x) =</description></item><item><title>Bessel's Inequality in L2 Space</title><link>https://freshrimpsushi.github.io/en/posts/1081/</link><pubDate>Mon, 13 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1081/</guid><description>Theorem Let&amp;rsquo;s say $\left\{ \phi_{n} \right\}_{n=1}^{\infty}$ is an orthonormal set in $L^{2}(a,b)$. And let $f\in L^{2}(a,b)$. Then, the following inequality holds. $$ \sum \limits_{n=1}^\infty \left| \left\langle f, \phi_{n} \right\rangle \right|^{2} \le \| f \|^{2} $$ Explanation This is called Bessel&amp;rsquo;s inequality. $L^2$ space A function that satisfies the following equation is called square-integrable from $하다고 한다. $$ \int_{a}^b |f(x)|^2 dx &amp;lt; \infty $$ 구간</description></item><item><title>Convergence of Norms of Function Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1080/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1080/</guid><description>Definitions Suppose a sequence of functions $\left\{ f_{n} \right\}$ is given. If $\| f_{n} - f \|$ converges to $0$, then $f_{n}$ is said to converge in norm, denoted as follows. $$ f_{n} \to f \text{ in norm } $$ or $$ \| f_{n} - f\| \to 0 $$ or $$ \lim \limits_{n \to 0} \| f_{n}-f\|=0 $$ Explanation To define the limit of a sequence, the concept of distance</description></item><item><title>Hermite-Genocchi Formula</title><link>https://freshrimpsushi.github.io/en/posts/1031/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1031/</guid><description>Formulas Let&amp;rsquo;s say we have different $x_{0}, \cdots , x_{n}$ for $f \in C^{n} \left( \mathscr{H} \left\{ x_{0}, \cdots , x_{n} \right\} \right)$. Then, for the standard simplex $$ \tau_{n} := \left\{ ( t_{1} , \cdots , t_{n} ) : t_{i} \ge 0 \land \sum_{i=1}^{t} t_{i} \le 1 \right\} $$ and $\displaystyle t_{0} = 1 - \sum_{i=1}^{n} t_{i}$, the following is true. $$ f [ x_{0}, \cdots , x_{n} ]</description></item><item><title>Standardizing Data in R: Viewing Standardized Residuals</title><link>https://freshrimpsushi.github.io/en/posts/1026/</link><pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1026/</guid><description>Code R is specialized for statistics, so there often comes a need to calculate the Z-score $\displaystyle z:= {{x - \mu} \over {\sigma}}$. For this, using the built-in scale() function can be very convenient. Let&amp;rsquo;s standardize a vector, for example, $\mathbf{x} = ( 1, \cdots , 10 )$. If you dislike seeing messy outputs like center(mean) or scale(standard deviation), just take the vector. On the other hand, one of the</description></item><item><title>Derivation of Newton's Forward Difference Formula</title><link>https://freshrimpsushi.github.io/en/posts/1025/</link><pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1025/</guid><description>Formulas For different data $x_{0} , \cdots , x_{n}$ of $(x_{0}, f(x_{0} )) , \cdots , (x_{n} , f( x_{n} ) )$, $$ p_{n} (x) =\sum_{i=0}^{n} f [ x_{0} , \cdots , x_{i} ] \prod_{j=0}^{i-1} (x - x_{j} ) $$ Description Though it seems complicated, when actually expanding for $n=0,1,2$, it simplifies as follows. $$ \begin{align*} p_{0} (x) =&amp;amp; f(x_{0}) \\ p_{1} (x) =&amp;amp; f( x_{0} ) + (x -</description></item><item><title>Delayed Potential on Continuous Distribution of Delay Times</title><link>https://freshrimpsushi.github.io/en/posts/1075/</link><pubDate>Thu, 09 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1075/</guid><description>Overview1 The scalar and vector potentials for a moving point charge are called retarded potentials, and they are as follows. $$ \begin{align*} V(\mathbf{r},\ t) &amp;amp;= \dfrac{1}{4\pi\epsilon_{0}} \int \dfrac{ \rho (\mathbf{r}^{\prime},\ t_{r}) }{ \cR } d\tau^{\prime} \\[1em] \mathbf{A}( \mathbf{r},\ t) &amp;amp;= \dfrac{\mu_{0}}{4\pi} \int \dfrac{\mathbf{J}(\mathbf{r}^{\prime},\ t_{r})}{\cR}d\tau^{\prime} \end{align*} $$ Here, $t_{r}$ is the retarded time. Retarded Time If the charge and current distribution do not change over time, the scalar and vector potentials</description></item><item><title>Lagrange's Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/1023/</link><pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1023/</guid><description>Formula 1 Given different $x_{0} , \cdots , x_{n}$ data $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$, let&amp;rsquo;s say $\displaystyle l_{i} (x) := \prod_{i \ne j} \left( {{ x - x_{j} } \over { x_{i} - x_{j} }} \right)$, then $$ p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X) $$ Description Lagrange&amp;rsquo;s formula is the simplest method among those to find polynomial interpolation. Derivation Strategy: Prove that $l_{i}$ is the</description></item><item><title>Solution of Nonlinear First Order PDE Using Characteristic Equations</title><link>https://freshrimpsushi.github.io/en/posts/1074/</link><pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1074/</guid><description>Explanation1 When emphasizing that x and p are variables of a partial differential equation, they are denoted in normal font as $x,p \in \mathbb{R}^{n}$, and when emphasizing them as functions of $s$, they are denoted in bold font as $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Characteristic Equations $$ \begin{cases} \dot{\mathbf{p}} (s) = -D_{x}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big)-D_{z}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big)\mathbf{p}(s) \\ \dot{z}(s) = D_{p}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big) \cdot \mathbf{p}(s) \\ \dot{\mathbf{x}}(s) = D_{p}F\big(\mathbf{p}(s),\</description></item><item><title>Characteristics of Nonlinear First-Order Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1073/</link><pubDate>Tue, 07 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1073/</guid><description>Regarding x and p, when emphasizing that they are variables of a partial differential equation, they are denoted in normal font as $x,p \in \mathbb{R}^{n}$, and when emphasizing that they are functions of $s$, they are denoted in bold font as $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Characteristic Method1 Let&amp;rsquo;s suppose that an open set $\Omega \subset \mathbb{R}^{n}$ is given. Assume that $u\in C^{2}(\Omega)$ is a solution to the following nonlinear first-order</description></item><item><title>Notation for Nonlinear First-Order Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1071/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1071/</guid><description>Notation1 A nonlinear first-order partial differential equation is denoted as follows. $$ \begin{equation} F(Du, u, x) = F(p, z, x) = 0 \label{eq1} \end{equation} $$ $\Omega \subset \mathbb{R}^{n}$ is an open set $x\in \Omega$ $F : \mathbb{R}^n \times \mathbb{R}^n \times \bar{ \Omega } \to \mathbb{R}$ is the given function $u : \bar{ \Omega } \to \mathbb{R}$ is the variable of $F$ Description Solving a nonlinear first-order partial differential equation $F$</description></item><item><title>Polynomial Interpolation</title><link>https://freshrimpsushi.github.io/en/posts/1021/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1021/</guid><description>Definition 1 For different $x_{0} , \cdots , x_{n}$ data $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$, a polynomial function $p$ that satisfies $p (x_{i} ) = y_{i}$ and $\deg p \le n$ is called Polynomial Interpolation. Theorem Existence and Uniqueness [1]: For the given data, there exists a unique $p$. Lagrange&amp;rsquo;s Formula [2]: $$p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X)$$ Newton&amp;rsquo;s Divided Difference Formula [3]: $$p_{n} (x) =</description></item><item><title>Checking the Current Date and Time in R</title><link>https://freshrimpsushi.github.io/en/posts/1020/</link><pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1020/</guid><description>Code Not only in R but in many tasks that require the use of programming languages, it&amp;rsquo;s necessary to write logs and to have information about the time at which they were written. In R, you can check the date with the Sys.Date() function, and you can know the accurate time down to seconds with the Sys.time() function. Be careful with the case sensitivity, and if you don&amp;rsquo;t need the</description></item><item><title>Coulomb Gauge and Lorentz Gauge</title><link>https://freshrimpsushi.github.io/en/posts/134/</link><pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/134/</guid><description>Overview1 A relationship exists between the potential and the charge density, current density as follows. $$ \begin{align*} \nabla ^2 V +\dfrac{\partial }{\partial t}(\nabla \cdot \mathbf{A}) &amp;amp;= -\frac{1}{\epsilon_{0}}\rho \\ \left( \nabla ^2 \mathbf{A}-\mu_{0}\epsilon_{0} \dfrac{\partial ^2 \mathbf{A} }{\partial t^2} \right) -\nabla\left( \nabla \cdot \mathbf{A} +\mu_{0}\epsilon_{0} \dfrac{\partial V}{\partial t}\right) &amp;amp;= -\mu_{0} \mathbf{J} \end{align*} $$ Depending on how assumptions about the potential are made, the expression changes. Coulomb Gauge As in magnetostatics, the divergence</description></item><item><title>Gauge Transformation</title><link>https://freshrimpsushi.github.io/en/posts/129/</link><pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/129/</guid><description>Overview1 A given scalar potential $V$ and vector potential $\mathbf{A}$ uniquely determine the electric field $\mathbf{E}$ and the magnetic field $\mathbf{B}$, but the converse is not true. In other words, there are multiple potentials $V$, $\mathbf{A}$ that can represent a single electromagnetic field $\mathbf{E}$, $\mathbf{B}$. Therefore, within the changes that do not alter $\mathbf{E}$ and $\mathbf{B}$, $V$ and $\mathbf{A}$ can be changed freely. Gauge Transformation There exist two pairs of</description></item><item><title>Vertical Waves, Parallel Waves, Plane Polarization</title><link>https://freshrimpsushi.github.io/en/posts/1070/</link><pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1070/</guid><description>Definition A wave whose direction of propagation and direction of vibration are perpendicular to each other is called a transverse wave. Conversely, a wave whose direction of propagation and direction of vibration are parallel to each other is called a longitudinal wave. Explanation The phenomenon of a wave vibrating in a specific direction is called polarization. Since there are two directions perpendicular to the direction of propagation for a transverse</description></item><item><title>Interpolation in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1016/</link><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1016/</guid><description>Definition 1 For a given pair of data $(n+1)$ and $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$, the method or the function itself that satisfies $f (x_{i} ) = y_{i}$ while possessing some specific property is called interpolation. Description For example, consider the situation where there&amp;rsquo;s data available as shown above, but the middle part is missing. Of course, it&amp;rsquo;s best to have actual data, but if not, there</description></item><item><title>Smoothing Effect of Harmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1063/</link><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1063/</guid><description>Theorem Mean Value Property $$ \begin{align*} u(x) = -\!\!\!\!\!\! \int_{\partial B(x,r)} udS = -\!\!\!\!\!\! \int _{B(x,r)} udy \end{align*} $$ Assuming $u \in C(\Omega)$ satisfies the mean value property in each open ball $B(x,r)\subset \Omega$, then the following holds. $$ u \in C^{\infty}(\Omega) $$ Description If it&amp;rsquo;s Harmonic, it means it&amp;rsquo;s smooth inside. It is important to note that smoothness or continuity is not guaranteed at the boundary $\partial \Omega$. Proof</description></item><item><title>Chaotic Transition</title><link>https://freshrimpsushi.github.io/en/posts/1014/</link><pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1014/</guid><description>Definition The phenomenon where a system becomes chaotic or remains non-chaotic depending on the change of parameters is called chaotic transition. Examples For example, if we consider the logistic family, a system formed by $g_{a} = ax (1-x)$ shows different behaviors depending on the parameter $a$, and it can be observed that it has a chaotic orbit when $a=4$. The next question then is &amp;lsquo;what happens when it&amp;rsquo;s $a&amp;gt;4$?&amp;rsquo; Firstly,</description></item><item><title>Mollification</title><link>https://freshrimpsushi.github.io/en/posts/1060/</link><pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1060/</guid><description>Definition 1 $f \in {L^1_{\mathrm{Loc}}( \Omega)}$ and $\epsilon&amp;gt;0$ with respect to $f$, the $\epsilon$-mollification is defined as follows. $$ f^{\epsilon}(x) := \eta_{\epsilon} * f (x) =\int_{\mathbb{R}^{n}} \eta_{\epsilon}(x-y)f(y)dy, \quad x\in \Omega_{&amp;gt;\epsilon} $$ Here, $f$ is a function defined as $0$ outside of $\Omega$. $\eta_\epsilon$ is a mollifier. $\ast$ is a convolution. $\Omega_{&amp;gt;\epsilon} := \left\{ x \in \Omega : \mathrm{dist}(x, \partial \Omega) &amp;gt; \epsilon \right\}$ Properties (i) $f^{\epsilon} \in C^\infty( \Omega_{&amp;gt;\epsilon})$ (ii)</description></item><item><title>Multi Index Notation</title><link>https://freshrimpsushi.github.io/en/posts/1062/</link><pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1062/</guid><description>Definition[^1] A multi-index with order $|\alpha|$ is a tuple $\alpha=(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n})$ whose components are non-negative integers. Here, $| \alpha|$ is defined as follows. $$ |\alpha| = \sum _{i}^{n} \alpha_{i} = \alpha_{1} + \cdots + \alpha_{n} $$ Notation For $x = (x_{1}, x_{2}, \dots, x_{n}) \in \mathbb{R}^{n}$, $x^{\alpha}$ is defined as follows. $$ x^{\alpha} := x_{1}^{\alpha_{1}} x_{2}^{\alpha_{2}} \cdots x_{n}^{\alpha_{n}} $$ The multi-index is often used to represent partial derivatives</description></item><item><title>Dickey-Fuller Test</title><link>https://freshrimpsushi.github.io/en/posts/921/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/921/</guid><description>Hypothesis Testing Given that we have time series data eq1. eq2: The data eq1 does not have stationarity. eq4: The data eq1 has stationarity. Explanation The Dickey-Fuller test is a hypothesis test used to determine whether time series data has stationarity or not. If it does not have stationarity, differencing must be used to make the mean constant. It is important to note that this diagnosis only occurs for the</description></item><item><title>Mollifiers</title><link>https://freshrimpsushi.github.io/en/posts/1059/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1059/</guid><description>Definition1 Let&amp;rsquo;s define the function $\eta \in C^{\infty}(\mathbb{R}^{n})$ as follows. $$ \begin{equation} \eta (x) := \begin{cases} C \exp \left( \dfrac{1}{|x|^2-1} \right) &amp;amp; |x|&amp;lt;1 \\ 0 &amp;amp; |x| \ge 1\end{cases} \label{1} \end{equation} $$ Such $\eta$ is called a mollifier. In particular, when $C&amp;gt;0$ is a constant satisfying $\displaystyle \int_{\mathbb{R}^{n}} \eta dx=1$, $\eta$ is called a standard mollifier. Let&amp;rsquo;s define $\eta_{\epsilon}$ for $\epsilon&amp;gt;0$ as follows. $$ \eta_\epsilon (x) := \dfrac{1}{\epsilon^n}\eta\left( \dfrac{x}{\epsilon} \right)</description></item><item><title>Sine Waves and Complex Wave Functions</title><link>https://freshrimpsushi.github.io/en/posts/1066/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1066/</guid><description>Definitions A wave expressed as a sine function is called a sine wave. Description The general form of a sine wave is as follows. The reason why it&amp;rsquo;s referred to as a sine wave even though the equation is $\cos$ is explained below, as the real part of the complex wave function is $\cos$. $\sin$ is the imaginary part. $$ f(x,t) = A \cos \big( k(x-vt)+\delta \big) $$ Here, $A$</description></item><item><title>Supervised and Unsupervised Learning</title><link>https://freshrimpsushi.github.io/en/posts/1013/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1013/</guid><description>Definition In machine learning, the case where the dependent variable is specified is called supervised learning, and the case where it is not specified is called unsupervised learning. Example The difference between supervised and unsupervised learning can be simply compared to the difference between multiple-choice and essay questions. For example, let&amp;rsquo;s say there is a classification problem asking for the color of 6 tiles like above. Supervised Learning But here,</description></item><item><title>Wave Boundary Conditions: Reflection and Transmission</title><link>https://freshrimpsushi.github.io/en/posts/1058/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1058/</guid><description>Let us consider a situation where two different strings are tied together, and a wave propagates from left to right along string 1. Since the propagation speed of the wave is related to the mass, the speed changes as the wave passes through the point where the strings are tied. For convenience, let&amp;rsquo;s denote the location of the knot as $x=0$ and assume that the wave enters from the left.</description></item><item><title>Derivation of the One-Dimensional Wave Equation</title><link>https://freshrimpsushi.github.io/en/posts/1057/</link><pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1057/</guid><description>Overview The one-dimensional wave equation is as follows. $$ \dfrac{\partial ^{2} f }{\partial x^{2}} = \dfrac{1}{v^{2}}\dfrac{\partial ^{2} f}{\partial t^{2}} $$ Here, $v$ represents the propagation speed of the wave. Characteristics of Waves Let&amp;rsquo;s assume there is a wave with a constant speed of $v$ as shown in Figure 1. Let the displacement of the point at $x$ at time $t$ be $f(x,t)$. Assuming the initial displacement of the string is</description></item><item><title>Gradient Descent in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1012/</link><pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1012/</guid><description>Definition 1 A scalar function $\varphi : \mathbb{R}^{n} \to \mathbb{R}$ is called a Cost Function. An algorithm that finds $\mathbf{x}_{n+1}$ that satisfies $\varphi ( \mathbf{x}_{n+1} ) &amp;lt; \varphi ( \mathbf{x}_{n} )$ in $\mathbf{x} = \mathbf{x}_{n}$ to minimize the cost function $ \varphi ( \mathbf{x} )$ is called the Descent Method. Explanation Let’s consider building a house as an example for a cost function, $\varphi$. The resources</description></item><item><title>Angular Momentum of the Electromagnetic Field</title><link>https://freshrimpsushi.github.io/en/posts/1056/</link><pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1056/</guid><description>Overview1 The angular momentum stored in the electromagnetic field is as follows. $$ \mathbf{\ell} = \mathbf{r} \times \mathbf{g}=\epsilon_{0}\big( \mathbf{r} \times (\mathbf{E} \times \mathbf{B} )\big) $$ $\mathbf{g}$ is the momentum density stored in the electromagnetic field. Description The electromagnetic field is not only a mediator of the electromagnetic forces acting between charges but also possesses energy itself. $$ u =\dfrac{1}{2} \left( \epsilon_{0} E^2 + \dfrac{1}{\mu_{0}} B^2 \right) $$ It also possesses</description></item><item><title>Conservation of Momentum in Electrodynamics</title><link>https://freshrimpsushi.github.io/en/posts/1055/</link><pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1055/</guid><description>Overview1 In electrodynamics, the law of conservation of momentum is as follows. $$ \dfrac{d \mathbf{p}}{dt} =-\epsilon_{0}\mu_{0}\dfrac{d}{dt}\int_{\mathcal{V}} \mathbf{S} d\tau + \oint_{\mathcal{S}} \mathbf{T} \cdot d\mathbf{a} $$ Explanation According to Newton&amp;rsquo;s second law, the force acting on an object and the change in the object&amp;rsquo;s momentum are equal. $$ \mathbf{F} = \dfrac{d \mathbf{p}}{dt} $$ $\mathbf{p}$ is the total mechanical momentum of particles within volume $\mathcal{V}$. To distinguish it from the momentum stored in</description></item><item><title>Natural Invariant Measure</title><link>https://freshrimpsushi.github.io/en/posts/1011/</link><pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1011/</guid><description>Definition 1 In a chaotic dynamical system, the distribution function that probabilistically represents the state after a sufficient amount of time has passed is called a natural (invariant) measure. Yorke. (1996). CHAOS: An Introduction to Dynamical Systems: p249~255.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Gradient of Scalar Field</title><link>https://freshrimpsushi.github.io/en/posts/1010/</link><pubDate>Sun, 28 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1010/</guid><description>Definition A gradient, specifically referred to as the total derivative of a scalar field $f : \mathbb{R}^{n} \to \mathbb{R}$, is denoted by $\nabla f$. $$ \begin{align*} \nabla f := f^{\prime} =&amp;amp; \begin{bmatrix} D_{1}f &amp;amp; D_{2}f &amp;amp; \cdots &amp;amp; D_{n}f\end{bmatrix} \\ =&amp;amp; \begin{bmatrix} \dfrac{\partial f}{\partial x_{1}} &amp;amp; \dfrac{\partial f}{\partial x_{2}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f}{\partial x_{n}} \end{bmatrix} \\ =&amp;amp; \dfrac{\partial f}{\partial x_{1}}\hat{x}_{1} + \dfrac{\partial f}{\partial x_{2}}\hat{x}_{2} + \dots + \dfrac{\partial f}{\partial</description></item><item><title>Bifurcation Diagram</title><link>https://freshrimpsushi.github.io/en/posts/1006/</link><pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1006/</guid><description>Definition A Bifurcation Diagram denotes a graph showing the changes occurring with the variation of parameters in a dynamical system, illustrating bifurcation. Example1 Considering the case of the Logistic family $$ x_{n+1} = a x_{n} \left( 1 - x_{n} \right) $$ , it is presumed that the values of $x_{N}$ for a sufficiently large $N$, given the changes in parameter $a$, fall within the black region of the following bifurcation</description></item><item><title>Newton's Method for Solving Nonlinear Systems</title><link>https://freshrimpsushi.github.io/en/posts/1005/</link><pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1005/</guid><description>Methods 1 Let&amp;rsquo;s assume that a multivariable function $\mathbf{f} : \mathbb{R}^{N} \to \mathbb{R}^{N}$ is defined as $\mathbf{f} \in C^{2} \left( N ( \alpha ) \right)$ with $\mathbf{f} ( \alpha ) = \mathbb{0}$, $\left[ D \mathbf{f} ( \alpha ) \right]^{-1}$ existing. For an initial value $\mathbf{x}_{0}$ sufficiently close to $\alpha$, the sequence $\left\{ \mathbf{x}_{n} \right\}$ defined by $$ \mathbf{x}_{n+1} := \mathbf{x}_{n} - \left[ D \mathbf{f} ( \mathbf{x}_{n} ) \right]^{-1} f (</description></item><item><title>Dropout in Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/1004/</link><pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1004/</guid><description>Definition Dropout is a technique to prevent overfitting by stochastically not using neurons in an artificial neural network. Explanation At first glance, it might seem like it&amp;rsquo;s just learning less, and to some extent, that&amp;rsquo;s true. By not using neurons with a certain probability, it&amp;rsquo;s possible to ignore neurons that are &amp;rsquo;too influential&amp;rsquo;. Being too influential can be seen as being too confident about the training data. If there are</description></item><item><title>Calculating Jacobian and Hessian Matrices in R</title><link>https://freshrimpsushi.github.io/en/posts/994/</link><pubDate>Mon, 22 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/994/</guid><description>Code To calculate the Jacobian matrix and Hessian matrix in R, you use the jacobian() and hessian() functions from the numDeriv package. install.packages(&amp;#34;numDeriv&amp;#34;) library(numDeriv) f &amp;lt;- function(v) {c(v[1]^2 + v[2]^2 - 1, sin(pi*v[1]/2) + v[2]^3)} g &amp;lt;- function(v) {(v[1])^3+(v[2])^2} jacobian(f, c(1,1)) hessian(g, c(1,1)) The results of executing the code are as follows. The top is the result of substituting $x=y=1$ into the Jacobian matrix of $f(x,y) := \begin{bmatrix} x^2 +</description></item><item><title>Uniqueness of the Solution to the Dirichlet Problem for the Poisson Equation</title><link>https://freshrimpsushi.github.io/en/posts/1046/</link><pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1046/</guid><description>Theorem1 Let&amp;rsquo;s assume that $\Omega \subset \mathbb{R}^n$ is open and bounded. And let $g \in C(\partial \Omega)$, $f \in C(\Omega)$. Then in the Dirichlet problem of the Poisson equation as below, the solution $u \in C^2(\Omega) \cap C(\bar{\Omega})$, if exists, is unique (=at most one exists). $$ \begin{equation} \left\{ \begin{aligned} -\Delta u &amp;amp;= f &amp;amp;&amp;amp; \text{in } \Omega \\ u &amp;amp;= g &amp;amp;&amp;amp; \text{on }\partial \Omega \end{aligned} \right. \label{eq1} \end{equation}</description></item><item><title>Maximum Principle of Harmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1044/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1044/</guid><description>Theorem1 Let $\Omega \subset \mathbb{R}^n$ be open and bounded. Also, suppose $u : \Omega \to \mathbb{R}$ is equal to $u \in C^2(\Omega) \cap C(\bar \Omega)$ and satisfies the Laplace equation. Then, the following holds: (i) Maximum Principle $$ \max \limits_{\bar \Omega} u = \max \limits_{\partial \Omega} u \quad \left( \mathrm{or} \ \ \min \limits_{\bar \Omega} u= \min \limits_{\partial \Omega} u \right) $$ (ii) Strong Maximum Principle If $\Omega$ is a</description></item><item><title>Softmax Function in Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/993/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/993/</guid><description>Definition Let&amp;rsquo;s refer to it as $\mathbf{x} := (x_{1} , \cdots , x_{n}) \in \mathbb{R}^{n}$. For $\displaystyle \sigma_{j} ( \mathbf{x} ) = {{ e^{x_{j}} } \over {\sum_{i=1}^{n} e^{x_{i}} }}$, $\sigma ( \mathbf{x} ) := \left( \sigma_{1} (\mathbf{x}) , \cdots , \sigma_{n} (\mathbf{x} ) \right)$ is defined as $\sigma : \mathbb{R}^{n} \to (0,1)^{n}$, which is called the softmax. Explanation The softmax function is a type of activation function characterized by its</description></item><item><title>What is a Hessian Matrix?</title><link>https://freshrimpsushi.github.io/en/posts/992/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/992/</guid><description>Definition $D \subset \mathbb{R}^{n}$ is defined as the matrix $H \in \mathbb{R}^{n \times n}$ for a multivariate scalar function $f : D \to \mathbb{R}$ is called the Hessian matrix of $f$. $$ H := \begin{bmatrix} {{\partial^2 f } \over {\partial x_{1}^2 }} &amp;amp; \cdots &amp;amp; {{\partial^2 f } \over { \partial x_{1} \partial x_{n} }} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ {{\partial^2 f } \over {\partial x_{n} \partial x_{1}</description></item><item><title>Activation Functions in Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/991/</link><pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/991/</guid><description>Definition An non-linear function that mimics the threshold of real-life organisms is known as an activation function. Mathematical Definition In deep learning, a non-linear scalar function $\sigma : \mathbb{R}^{n} \to \mathbb{R}$ is referred to as an activation function. Of course, there are exceptions like the softmax which don&amp;rsquo;t fit into this definition. Explanation On the other hand, a vector function is called a layer. If there is an expression or</description></item><item><title>Electromagnetic Force on a Charge inside a Volume</title><link>https://freshrimpsushi.github.io/en/posts/1042/</link><pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1042/</guid><description>Overview1 The electromagnetic force experienced by all charges in volume $\mathcal{V}$ is as follows. $$ \mathbf{F} =\oint_{\mathcal{S}} \mathbf{T} \cdot d\mathbf{a} -\epsilon_{0}\mu_{0}\dfrac{d}{dt}\int_{\mathcal{V}} \mathbf{S} d\tau $$ $\mathcal{S}$ is the boundary surface of volume $\mathcal{V}$, $\mathbf{T}$ is the Maxwell stress tensor, $\mathbf{S}$ is the Poynting vector. Derivation Part 1. By Lorentz force law, the force experienced by a charge is $$ \mathbf{F}=q(\mathbf{E} + \mathbf{v} \times \mathbf{B}) $$ If we express the charge quantity</description></item><item><title>Jacobian Matrix or Jacobi Matrix</title><link>https://freshrimpsushi.github.io/en/posts/989/</link><pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/989/</guid><description>Definition Let a multivariable vector function $\mathbf{f} : D \to \mathbb{R}^{m}$ defined by $D \subset \mathbb{R}^{n}$ be defined for each scalar function $f_{1} , \cdots , f_{m} : D \to \mathbb{R}$ as follows: $$ \mathbf{f} ( x_{1} , \cdots , x_{n} ) : = \begin{bmatrix} f_{1} ( x_{1} , \cdots , x_{n} ) \\ \vdots \\ f_{m} ( x_{1} , \cdots , x_{n} ) \end{bmatrix} $$ It is called the</description></item><item><title>Maxwell's Stress Tensor</title><link>https://freshrimpsushi.github.io/en/posts/1041/</link><pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1041/</guid><description>Definition1 The tensor $\mathbf{T}$ below is called the Maxwell stress tensor. $$ \mathbf{T}=\overleftrightarrow{\mathbf{T}}=\begin{pmatrix} T_{xx} &amp;amp; T_{xy} &amp;amp; T_{xz} \\ T_{yx} &amp;amp; T_{yy} &amp;amp; T_{yz} \\ T_{zx} &amp;amp; T_{zy} &amp;amp; T_{zz} \end{pmatrix} $$ $$ T_{ij}=\epsilon_{0} \left( E_{i}E_{j}-\dfrac{1}{2}\delta_{ij}E^2 \right) + \dfrac{1}{\mu_{0}}\left(B_{i}B_{j}-\dfrac{1}{2}\delta_{ij}B^2 \right) $$ Here, $\delta_{ij}$ is the Kronecker delta. Description $2$th order tensor is defined as above. It appears in the process of deriving the force experienced by a charge in a</description></item><item><title>What is Deep Learning?</title><link>https://freshrimpsushi.github.io/en/posts/996/</link><pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/996/</guid><description>Definition Deep learning is a type of machine learning that uses artificial neural networks, especially employing multiple layers when constructing these networks. Motivation Just like the human brain is composed of a complex network of neurons, deep learning also enhances performance by making the connections in artificial neural networks more complex. Similar to how the stimuli received by sensory cells are transmitted to the brain through the spinal cord, artificial</description></item><item><title>Gradient Descent and Stochastic Gradient Descent in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/987/</link><pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/987/</guid><description>Overview The Gradient Descent Algorithm is the simplest method among algorithms that find the local minimum of the loss function by using the gradient of the loss function. Description Here, the loss function $L$ is considered a function of weights and biases with the dataset $X$ being fixed. If the input data looks like $\mathbf{x} \in \mathbb{R}^{m}$, then $L$ becomes a function of $(w_{1} , w_{2} , \cdots , w_{m}</description></item><item><title>In Physics, What is a Tensor</title><link>https://freshrimpsushi.github.io/en/posts/1040/</link><pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1040/</guid><description>Overview Without a doubt, this is the easiest explanation about tensors, so if you&amp;rsquo;re an undergraduate in physics who came here because you don&amp;rsquo;t know what a tensor is, I highly recommend reading this. We&amp;rsquo;re not accepting corrections about mathematical inaccuracies. Teaching someone who hasn&amp;rsquo;t learned about negative numbers that &amp;lsquo;you can&amp;rsquo;t subtract a larger number from a smaller number&amp;rsquo;, or someone who hasn&amp;rsquo;t learned about complex numbers that &amp;lsquo;you</description></item><item><title>Schwarzschild Derivative</title><link>https://freshrimpsushi.github.io/en/posts/1047/</link><pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1047/</guid><description>Definition1 Let $p$ be a fixed point or a periodic point of the smooth map $f : \mathbb{R} \to \mathbb{R}$. $f ' (c) = 0$ being $c$ is called the critical point of $f$. If the basin of $p$ includes an interval of infinite length, it is called an infinite basin. $\displaystyle S(f)(x) := {{f ''' (x) } \over { f '(x) }} - {{3} \over {2}} \left( {{f '''</description></item><item><title>Poynting's Theorem and Poynting Vector</title><link>https://freshrimpsushi.github.io/en/posts/1039/</link><pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1039/</guid><description>Theorem1 The work done by the electromagnetic force on a charge is equal to the decrease in energy stored in the electromagnetic field plus the energy that has leaked out through the boundary. This is called Poynting&amp;rsquo;s theorem. $$ \begin{align*} \dfrac{dW}{dt} &amp;amp;= -\dfrac{d}{dt} \int_{\mathcal{V}} \dfrac{1}{2} \left( \epsilon_{0} E^2 + \dfrac{1}{\mu_{0}} B^2 \right) d\tau - \dfrac{1}{\mu_{0}} \oint_{\mathcal{S}} (\mathbf{E} \times \mathbf{B}) \cdot d \mathbf{a} \\ &amp;amp;= -\dfrac{d}{dt} \int_{\mathcal{V}} u d\tau - \oint_{\mathcal{S}}\mathbf{S}</description></item><item><title>Using Complex Numbers in R</title><link>https://freshrimpsushi.github.io/en/posts/981/</link><pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/981/</guid><description>Overview R includes the complex number data type. There&amp;rsquo;s no need to implement it yourself; you just need to use it. In addition to basic arithmetic, it also includes several functions essential for handling complex numbers. Code Let&amp;rsquo;s say $z_{1} : = 1- i$ and $z_{2} := 1+ i$. z_1 = 1-1i z_2 = 1+1i z_1 + z_2 z_1 - z_2 z_1 * z_2 z_1 / z_2 Re(z_1) Im(z_1) Mod(z_1)</description></item><item><title>Maxwell's Equations</title><link>https://freshrimpsushi.github.io/en/posts/1038/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1038/</guid><description>Formulas Maxwell&amp;rsquo;s Equations $(\text{i}) \quad \nabla \cdot \mathbf{E}=\dfrac{1}{\epsilon_{0}}\rho$ (Gauss&amp;rsquo;s Law) $(\text{ii}) \quad \nabla \cdot \mathbf{B}=0$ (Gauss&amp;rsquo;s Law for Magnetism) $(\text{iii}) \quad \nabla \times \mathbf{E} = -\dfrac{\partial \mathbf{B}}{\partial t}$ (Faraday&amp;rsquo;s Law) $(\text{iv}) \quad \nabla \times \mathbf{B} = \mu_{0} \mathbf{J}+\mu_{0}\epsilon_{0}\dfrac{\partial \mathbf{E}}{\partial t}$ (Ampère&amp;rsquo;s Law) Description1 Before Maxwell completed the Maxwell&amp;rsquo;s equations, the four equations concerning the electric field</description></item><item><title>Calculating Definite Integrals in R</title><link>https://freshrimpsushi.github.io/en/posts/977/</link><pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/977/</guid><description>Overview In R, you can use the integrate() function to calculate definite integrals. For example, Code To calculate $\displaystyle \int_{0}^{3} \left( x^2 + 4x + 1 \right) dx$ and $\displaystyle \int_{0}^{\infty} e^{-x} dx$, you can use the following. Notably, by including inf in the integration interval, it is possible to perform improper integrals as well. f&amp;lt;-function(x) {x^2 + 4*x + 1} g&amp;lt;-function(x) {exp(-x)} integrate(f,0,3) integrate(g,0,Inf) Upon actual calculation, $$ \int_{0}^{3}</description></item><item><title>The Mean Value Theorem for Laplace's Equation</title><link>https://freshrimpsushi.github.io/en/posts/1037/</link><pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1037/</guid><description>Theorem1 Let us assume that an open set $\Omega \subset \mathbb{R}^{n}$ is given. Also, let&amp;rsquo;s assume that $u \in C^2(\Omega)$ satisfies the Laplace equation. Then, for each open ball $B(x,r)\subset \subset \Omega$, the following holds. $$ \begin{align*} u(x) &amp;amp;= \dfrac{1}{n \alpha (n)r^{n-1}} \int _{\partial B(x,r)} udS =: -\!\!\!\!\!\! \int_{\partial B(x,r)} udS \\ &amp;amp;= \dfrac{1}{\alpha (n)r^n}\int_{B(x,r)}udy =: -\!\!\!\!\!\! \int _{B(x,r)} udy \end{align*} $$ It is denoted as $V\subset \subset U$ when</description></item><item><title>Continuity Equations in Electromagnetism</title><link>https://freshrimpsushi.github.io/en/posts/1032/</link><pubDate>Sat, 13 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1032/</guid><description>Formulas The following formula is known as the continuity equation. $$ \dfrac{\partial \rho}{\partial t}=-\nabla \cdot \mathbf{J} $$ Explanation1 The continuity equation mathematically expresses the law of conservation of charge in a local region. The law of conservation of charge states that the original amount of charge does not suddenly disappear or newly appear; the initial amount of charge is maintained. This is true not only for the entire universe but</description></item><item><title>Mueller Method</title><link>https://freshrimpsushi.github.io/en/posts/976/</link><pubDate>Sat, 13 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/976/</guid><description>Method Let $f (\alpha) = 0$. With the initial value $x_{0} , x_{1} , x_{2}$ and $$ w_{n} := f [x_{n} , x_{n-1} ] + f [ x_{n} , x_{n-2} ] - f [ x_{n-2} , x_{n-1} ] $$ , define the sequence $\left\{ x_{n} \right\}$ as $$ x_{n+1} : = x_{n} - {{ 2 f ( x_{n} ) } \over { w_{n} \pm \sqrt{ w_{n}^{2} - 4 f (x_{n}</description></item><item><title>How to Calculate the Derivative in R</title><link>https://freshrimpsushi.github.io/en/posts/971/</link><pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/971/</guid><description>Overview To compute derivatives in R, one can use the grad() function from the numDeriv package. Code For example, the derivatives of $f(x) = x^2 + 4x + 1$ and $g(x) = e^{-x}$ can be computed as follows. install.packages(&amp;#34;numDeriv&amp;#34;) library(numDeriv) f&amp;lt;-function(x) {x^2 + 4*x + 1} g&amp;lt;-function(x) {exp(-x)} grad(f,2) grad(g,0) Upon actual computation, it is confirmed that it is $f ' (2) = 2 \cdot 2 + 4 = 8$</description></item><item><title>The sufficient condition for the Fourier series of a function to converge absolutely and uniformly to the function</title><link>https://freshrimpsushi.github.io/en/posts/1030/</link><pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1030/</guid><description>Theorem The function $f$, defined in $[L, -L)$, is continuous and piecewise smooth. Therefore, the Fourier series of $f$ absolutely and uniformly converges to $f$. When $f$ is piecewise smooth, its Fourier series converges pointwise to $f$. If the condition that $f$ is continuous is strengthened by removing the discontinuity points of $f$, then the Fourier series of $f$ absolutely and uniformly converges to $f$. The proof uses the Cauchy-Schwarz</description></item><item><title>Energy in a Magnetic Field</title><link>https://freshrimpsushi.github.io/en/posts/1029/</link><pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1029/</guid><description>Explanation1 Just like we considered the energy of the electric field created by a charge distribution in the energy of the electric field created by a charge distribution, we can think about the energy of the magnetic field created by a current distribution. When current flows through a circuit, energy is inputted. The identity of this energy is exactly the work done against electromotive force (EMF). Because of EMF, it&amp;rsquo;s</description></item><item><title>Scalar Functions and Vector-valued Functions</title><link>https://freshrimpsushi.github.io/en/posts/970/</link><pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/970/</guid><description>Definition Let $D$ be a subset $D\subset \mathbb{R}^{n}$ of the $n$-dimensional Euclidean space. Functions having $D$ as their domain are called function of several variables. $f : D \to \mathbb{R}$ is called a scalar function. For a scalar function $f_{1} , \cdots , f_{m} : D \to \mathbb{R}$, $\mathbf{f} : D \to \mathbb{R}^{m}$ defined as follows is called a vector-valued function. $$ \mathbf{f} ( x_{1} , \cdots , x_{n} )</description></item><item><title>Self-Inductance</title><link>https://freshrimpsushi.github.io/en/posts/1028/</link><pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1028/</guid><description>Explanation1 In the situation shown in the figure above, if current $I_{1}$ flows through loop 1, magnetic field $\mathbf{B}_{1}$ will flow and the magnetic flux passing through loop 2 can be calculated as follows. $$ \Phi_{2} = M_{21}I_{1} $$ At this point, $M_{21}$ is referred to as mutual inductance. Now, let&amp;rsquo;s say the current $I_{1}$ flowing through loop 1 changes over time. Then, the magnetic flux passing through loop 2</description></item><item><title>Conjugate Maps in Chaos Theory</title><link>https://freshrimpsushi.github.io/en/posts/968/</link><pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/968/</guid><description>Overview In chaos theory, the conjugacy of maps is akin to isometry and isomorphism, and indeed, in a more general context of dynamics, it is synonymous with homeomorphism itself. 1 Although not exactly the same according to some textbooks, their purpose is precisely the same. As is the case with mathematics, the idea is to verify a property in a simpler computation context first and then preserve that property where</description></item><item><title>Mutual Inductance</title><link>https://freshrimpsushi.github.io/en/posts/1027/</link><pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1027/</guid><description>Explanation1 Consider two fixed conducting loops as shown in the figure above. If a steady current $I_{1}$ flows through loop 1, it generates a magnetic field $\mathbf{B}_{1}$.(Ampère&amp;rsquo;s law) Some of the magnetic field lines $\mathbf{B}_{1}$ will pass through loop</description></item><item><title>Loss Functions in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/967/</link><pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/967/</guid><description>Definition When an estimate for the data $Y = \begin{bmatrix} y_{1} \\ \vdots \\ y_{n} \end{bmatrix}$ is given as $\widehat{Y} = \begin{bmatrix} \widehat{ y_{1} } \\ \vdots \\ \widehat{y_{n}} \end{bmatrix}$, the scalar function $L : \mathbb{R}^{n} \to [ 0 , \infty )$ that represents the discrepancy between the data and its estimate is called a loss function. Alternate Names The loss function is used as an indicator to evaluate how</description></item><item><title>Graphs and Networks in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/966/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/966/</guid><description>Definitions 1 A set comprising vertices and lines connecting vertices is called a graph or a network. Let&amp;rsquo;s denote the set of vertices as $V$ and the set of lines as $E$. Elements of $V(G) := V$ are called vertices or nodes of $G$. Elements of $E(G) := E$ are called edges or links of $G$. An edge that connects to the same vertex is called a loop. If two</description></item><item><title>Logistic Family</title><link>https://freshrimpsushi.github.io/en/posts/860/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/860/</guid><description>Definition 1 Equation $a \ge 0$ is referred to as the Logistic Map, and $\left\{ g_{a} \mid a &amp;gt; 0 \right\}$ is called the Logistic Family. Properties [1]: $x \in [0,1] \iff g_{a} (x) \ge 0$ [2]: $g'_{a} (x) = a ( 1 - 2x)$ [3]: If $1 &amp;lt; a \le 4$, then $\displaystyle x_{1} = {{ a - 1} \over { a }}$ is a fixed point of $g_{a}</description></item><item><title>Sharkovsky's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1022/</link><pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1022/</guid><description>Theorem 1 $$ 3 \prec 5 \prec 7 \prec 9 \prec \cdots \prec \\ 2\cdot 3 \prec 2 \cdot 5 \prec \cdots \prec \\ 2^2 3 \prec 2^2 5 \prec \cdots \prec \\ 2^3 3 \prec 2^3 5^2 \prec \cdots \prec \\ 2^3 \prec 2^2 \prec 2^1 \prec 2^0 $$ The order mentioned above for a transitive relation $\prec$ is known as Sharkovs</description></item><item><title>인공 신경망이란?</title><link>https://freshrimpsushi.github.io/en/posts/962/</link><pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/962/</guid><description>Definition A network that mimics the nervous system of living organisms is called an artificial neural network. Mathematical Definition For a scalar function $\sigma : \mathbb{R} \to \mathbb{R}$, the notation $\overline{\sigma}$ is defined as follows: $$ \overline{\sigma}(\mathbf{x}) = \begin{bmatrix} \sigma(x_{1}) \\ \sigma(x_{2}) \\ \vdots \\ \sigma(x_{n}) \end{bmatrix} \qquad \text{where } \mathbf{x} = \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} $$ In deep learning, a linear vector function $L</description></item><item><title>Proof of the Lee-Yang Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1017/</link><pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1017/</guid><description>Theorem If a periodic-$3$ orbit exists for the continuous map $f: [a,b] \to [a,b]$, then $f$ is chaotic. Description The Li-Yorke Theorem also known as the Period-$3$ Theorem, is often mentioned as a statement that periodic-$3$ induces chaos. Although this theorem seems to be limited to $1$-dimensional maps, the mere existence of a periodic-$3$ orbit ensuring the existence of all periodic orbits is mathematically astonishing. Typically in mathematics, if a</description></item><item><title>Chaos in One-Dimensional Maps</title><link>https://freshrimpsushi.github.io/en/posts/864/</link><pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/864/</guid><description>Definition Chaotic Orbit1 An orbit of map $f : \mathbb{R} \to \mathbb{R}$ is said to be chaotic if it satisfies the following conditions: (i) It is not asymptotically periodic. (ii): $h (x_{1} ) &amp;gt; 0$ A bounded orbit is an $M \in \mathbb{R}$ for which there exists a $|x_{n} | &amp;lt; M$ that satisfies all $n \in \mathbb{N}$. $h(x_{1} )$ refers to the Lyapunov exponent. Chaotic Map A map $f$</description></item><item><title>Faraday's Law and Lenz's Law</title><link>https://freshrimpsushi.github.io/en/posts/1018/</link><pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1018/</guid><description>Faraday&amp;rsquo;s Law A changing magnetic field induces an electric field. $$ \nabla \times \mathbf{E} = -\dfrac{\partial \mathbf{B}}{\partial t} $$ Explanation1 In 1831, Faraday announced the results of experiments as follows: A wire loop placed in a magnetic field was pulled to the right. A current flowed through the loop. With the wire loop fixed in the magnetic field, a magnet was pushed to the left. A current flowed through the</description></item><item><title>How to Install TensorFlow for Python on Windows</title><link>https://freshrimpsushi.github.io/en/posts/958/</link><pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/958/</guid><description>Guide When installing TensorFlow, problems usually arise due to an incorrect installation of Python. It&amp;rsquo;s recommended to uninstall Python and start over from the beginning, or if possible, to give your computer a fresh start. Step 1. Python Step 1-1. Checking the Bit Version Check the system information through Control Panel/All Control Panel Items/System or My Computer(right-click)-Properties. These days, it&amp;rsquo;s mostly 64-bit, so if it&amp;rsquo;s hard to confirm, it&amp;rsquo;s safe</description></item><item><title>Lyapunov Exponents of One-Dimensional Maps</title><link>https://freshrimpsushi.github.io/en/posts/862/</link><pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/862/</guid><description>Definition1 Let&amp;rsquo;s assume an orbit $\left\{ x_{1} , x_{2} , x_{3} , \cdots \right\}$ of a smooth $1$-dimensional map $f : \mathbb{R} \to \mathbb{R}$ is given. Lyapunov Number The following is called the Lyapunov Number: $$L ( x_{1} ) : = \lim_{ n \to \infty } \left( \prod_{i = 1}^{n} | f ' (x_{i} ) | \right)^{1/n}$$ Lyapunov Exponent The following is called the Lyapunov Exponent: $$h ( x_{1} )</description></item><item><title>Poisson's Equation Fundamental Solution</title><link>https://freshrimpsushi.github.io/en/posts/1015/</link><pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1015/</guid><description>Buildup1 Fundamental Solution to the Laplace Equation Let&amp;rsquo;s define the function $\Phi$ as the fundamental solution to the Laplace equation, where $x \in \mathbb{R}^{n}$ and $x \ne 0$. $$ \Phi (x) := \begin{cases} -\frac{1}{2\pi}\log |x| &amp;amp; n=2 \\ \frac{1}{n(n-2)\alpha (n)} \frac{1}{|x|^{n-2}} &amp;amp; n \ge 3 \end{cases} $$ Consider a function that maps as $x \mapsto \Phi (x)$. It is harmonic at places where $x \ne 0$. Suppose we symmetrize the</description></item><item><title>Wiener Process</title><link>https://freshrimpsushi.github.io/en/posts/957/</link><pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/957/</guid><description>Definition When $s&amp;lt; t &amp;lt; t+u$, a stochastic process $\left\{ W_{t} \right\}$ that satisfies the following conditions is called a Wiener Process: (i): $W_{0} = 0$ (ii): $\left( W_{t+u} - W_{t} \right) \perp W_{s}$ (iii): $\left( W_{t+u} - W_{t} \right) \sim N ( 0, u )$ (iv): The sample paths of $W_{t}$ are almost surely continuous. Basic Properties [1]: $\displaystyle W_{t} \sim N ( 0 , t )$ [2]: $\displaystyle</description></item><item><title>Map System's Orbit</title><link>https://freshrimpsushi.github.io/en/posts/858/</link><pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/858/</guid><description>Definition1 Let the smallest natural number satisfying $f^{k} (p) = p$ for maps $f : X \to X$ and $p \in X$ be $k \in \mathbb{N}$. For map $f : X \to X$ and point $x \in X$, the set $\left\{ x , f(x) , f^{2} , \cdots \right\}$ under $f$ is called the orbit of $x$. Here, $x$ is called the initial value of the orbit. An orbit $\left\{</description></item><item><title>Semi-prime</title><link>https://freshrimpsushi.github.io/en/posts/950/</link><pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/950/</guid><description>Definition Two product of prime numbers is called a Semiprime. Description Examples of semiprimes include $4 = 2 \cdot 2$, $21 = 3 \cdot 7$, $673703 = 719 \cdot 937$, etc. In Japanese translation, it is also called 半素数 but it is hard to find either semiprime or 半素数 in the Korean documents. Essentially, semiprime is not a semiprime itself, but it somewhat inherits the properties</description></item><item><title>Fourier Coefficients of Odd Functions</title><link>https://freshrimpsushi.github.io/en/posts/1008/</link><pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1008/</guid><description>Theorem If a function $f$ with a period of $2L$ is antisymmetric, then the Fourier coefficients of $f$ are as follows. $$ \begin{align*} a_{0} &amp;amp;= 0 \\ a_{n} &amp;amp;= \begin{cases} \dfrac{2}{L} {\displaystyle \int_{0}^{L}} f(t) \cos \frac{n \pi t}{L} dt &amp;amp; (n=1, 3, \cdots ) \\ 0 &amp;amp; (n=0, 2, \cdots )\end{cases} \\ b_{n} &amp;amp;= \begin{cases} \dfrac{2}{L} {\displaystyle \int_{0}^{L}} f(t) \sin \frac{n \pi t}{L} dt &amp;amp; (n=1, 3, \cdots ) \\</description></item><item><title>Identification of Sinks and Sources in One-Dimensional Maps</title><link>https://freshrimpsushi.github.io/en/posts/999/</link><pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/999/</guid><description>Theorem1 Let&amp;rsquo;s say some $p \in \mathbb{R}$ is a fixed point for a smooth map $f : \mathbb{R} \to \mathbb{R}$. [1] If $| f ' (p) | &amp;lt; 1$, then $p$ is a sink. [2] If $| f ' (p) | &amp;gt; 1$, then $p$ is a source. Example As an example of a $1$-dimensional map, consider $f(x) = x^3$ which makes it easy to see that the fixed point</description></item><item><title>Jarque–Bera test Test</title><link>https://freshrimpsushi.github.io/en/posts/949/</link><pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/949/</guid><description>Hypothesis Testing Given that we have quantitative data $\left\{ x_{i} \right\}_{i = 1}^{n}$. $H_{0}$: Data $\left\{ x_{i} \right\}_{i = 1}^{n}$ follows a normal distribution. $H_{1}$: Data $\left\{ x_{i} \right\}_{i = 1}^{n}$ does not follow a normal distribution. Explanation The Jarque-Bera test is used to test for normality as a hypothesis test, typically to demonstrate the presence of normality. This is one of the rare cases where the acceptance of the</description></item><item><title>Banach Fixed-Point Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/948/</link><pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/948/</guid><description>Definition Let $(X, \left\| \cdot \right\|)$ be a Banach space. A $T : X \to X$ that satisfies $\| T(x) - T ( \tilde{x} ) \| \le r \| x - \tilde{x} \|$ for all $x, \tilde{x} \in X$ and $0 \le r &amp;lt; 1$ is defined as a contraction mapping. A $\alpha \in X$ that satisfies $T ( \alpha ) = \alpha$ is called a fixed point. Theorem 1</description></item><item><title>Fourier Cosine Series, Sine Series, Fourier Coefficients of Even and Odd Functions</title><link>https://freshrimpsushi.github.io/en/posts/1007/</link><pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1007/</guid><description>Definition Let&amp;rsquo;s say $f$ is a piecewise smooth function on the interval $[0,L)$. The following defined $f_{e}$ on the interval $[-L, L)$ is called the even extension of $f$. $$ f_{e}(t) := \begin{cases} f(t) &amp;amp; -L \le t &amp;lt;0 \\ f(-t) &amp;amp; 0 \le t &amp;lt;L\end{cases} $$ Similarly, the following defined $f_{o}$ on the interval $[-L, L)$ is called the odd extension of $f$. $$ f_{o}(t) := \begin{cases} -f(-t) &amp;amp;</description></item><item><title>Half-Wave Symmetric Function</title><link>https://freshrimpsushi.github.io/en/posts/1003/</link><pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1003/</guid><description>Definition A periodic function $f$ with a period of $2L$ is said to have half-wave symmetry when it satisfies the following equation for all $t$. $$ f(t)=-f(t+L) $$ Description Expanding on the definition above, it means &amp;lsquo;when a wave is on the $xy$ plane, the pattern of the wave alternates symmetrically around the $y$ axis based on the midpoint of the period.&amp;rsquo; Example As the name implies, it means being</description></item><item><title>How to Check Current OS Information in R</title><link>https://freshrimpsushi.github.io/en/posts/947/</link><pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/947/</guid><description>Overview R is frequently used on Linux for various reasons. One notable use case is for handling big data using Hadoop. Although R itself doesn’t differ much between Windows and Linux, the change in the working environment, including the working directory, can make file I/O operations a bit cumbersome. To easily set the working directory regardless of the environment, it&amp;rsquo;s necessary to check the</description></item><item><title>Any Function Can Always Be Expressed as the Sum of Odd and Even Functions</title><link>https://freshrimpsushi.github.io/en/posts/1002/</link><pubDate>Fri, 29 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1002/</guid><description>Theorem The arbitrary function $f$ defined in $\mathbb{R}$ can always be expressed as a sum of an even function and an odd function. Proof Let $f_{e}(t)$ and $f_o(t)$ be as follows. $$ f_{e}(t)=\dfrac{ f(t)+f(-t)}{2},\ \ \ f_o(t)=\dfrac{ f(t)-f(-t)}{2} $$ Then, $f_{e}(t)$ is an even function, and $f_o(t)$ is an odd function, and the following equation holds. $$ f_{e}(x)+f_o(x)=f(x) $$ ■</description></item><item><title>Derivation of the Base Change Formula for Logarithms</title><link>https://freshrimpsushi.github.io/en/posts/944/</link><pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/944/</guid><description>Formula For any positive number $c&amp;gt;0$, $$ \log_{a} b = {{ \log_{c} b } \over { \log_{c} a }} $$ Explanation Nowadays, the formula itself has become less meaningful, but it still holds significant value for entrance exams. It is recommended to solve many practice problems suitable for the title of &amp;lsquo;formula&amp;rsquo;, rather than underestimating it simply because it appears to be a simple property. Derivation If we say $x</description></item><item><title>Heat Equation, Diffusion Equation</title><link>https://freshrimpsushi.github.io/en/posts/1001/</link><pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1001/</guid><description>Definition1 2 The following partial differential equation is referred to as the heat equation or the diffusion equation. $$ \dfrac{\partial u}{\partial t} = \dfrac{\partial^{2} u}{\partial x^{2}} $$ When the spatial coordinate is $n$-dimensional, $$ \dfrac{\partial u}{\partial t} = \Delta u = \nabla^{2}u $$ here, $\Delta = \nabla^{2} = \sum\limits_{i=1}^{n} \dfrac{\partial^{2} }{\partial x_{i}^{2}}$ refers to the Laplacian. When there is an external force $f = f(x,t)$, $$ \dfrac{\partial u}{\partial t} =</description></item><item><title>Definition of Convolution</title><link>https://freshrimpsushi.github.io/en/posts/1000/</link><pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1000/</guid><description>Definition Let&amp;rsquo;s assume that two functions $f$ and $g$ defined in $\mathbb{R}$ are given. If the integral below exists, it is called the convolution of the two functions $f$ and $g$, and is denoted by $f \ast g$. $$ f \ast g(x):=\int _{-\infty} ^{\infty} f(y)g(x-y)dy $$ If $f$ and $g$ are discrete functions, they are defined as follows. $$ (f \ast g)(m)=\sum \limits_{n}f(n)g(m-n) $$ Explanation Although there is a translation</description></item><item><title>Increment of Stochastic Processes</title><link>https://freshrimpsushi.github.io/en/posts/943/</link><pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/943/</guid><description>Definition A stochastic process $\xi (t)$ is defined at time $T$ and let’s denote it by $t_{0} &amp;lt; t_{1} &amp;lt; \cdots &amp;lt; t_{n} \in T$. $\xi ( t ) - \xi ( s )$ is called an increment. If for all $i=1, \cdots , n$, $\xi ( t_{i} ) - \xi ( t_{i-1} )$ are mutually independent, $\xi (t)$ is said to have independent increments. If</description></item><item><title>Discrete Logarithm Problems Solved Easily Under Certain Conditions</title><link>https://freshrimpsushi.github.io/en/posts/942/</link><pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/942/</guid><description>Review 1 Assume element $g$ of group $G = F_{p}$ has an order of $N$. Then, the Discrete Logarithm problem $g^{x} = h$ becomes relatively easy to solve under the following conditions: (i): $p$ is a smooth prime number. (ii): $p \equiv 3 \pmod{4}$ and $a$ are quadratic residues modulo $p$. Proof (i) If $p$ is a smooth prime, the Pohlig-Hellman algorithm can be used, so the Discrete Logarithm problem</description></item><item><title>Laplace's Equation and Poisson's Equation</title><link>https://freshrimpsushi.github.io/en/posts/997/</link><pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/997/</guid><description>Definition1 $\ U \in \mathbb{R}^n$ is an open set $\ x\in U$ $u=u(x) : \overline{U} \rightarrow \mathbb{R}^n$ Laplace&amp;rsquo;s Equation The partial differential equation below is called Laplace&amp;rsquo;s equation. $$ \Delta u=0 $$ Here, $\Delta$ is the Laplacian. A $u$ that satisfies Laplace&amp;rsquo;s equation is specifically called a harmonic function. Poisson&amp;rsquo;s Equation The nonhomogeneous Laplace&amp;rsquo;s equation is called Poisson&amp;rsquo;s equation. $$ -\Delta u = f $$ Explanation Laplace&amp;rsquo;s equation appears in</description></item><item><title>Proving the Invariance of the Laplace Equation with Respect to Orthogonal Transformations</title><link>https://freshrimpsushi.github.io/en/posts/995/</link><pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/995/</guid><description>Theorem1 Let&amp;rsquo;s say $u$ satisfies the Laplace equation. And let&amp;rsquo;s define $v(x)$ as follows. $$ v(x) :=u(Rx) $$ Then, $R$ is a rotation transformation. Therefore, $v(x)$ also satisfies the Laplace equation. $$ \Delta v=0 $$ Explanation In fact, the content above holds for all orthogonal transformations. Thus, the fact that the Laplace equation is invariant under rotation transformation is a specific case of the fact that the Laplace equation is</description></item><item><title>Arima Model</title><link>https://freshrimpsushi.github.io/en/posts/941/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/941/</guid><description>Model 1 For the given white noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$, it is defined as $$ \nabla^{d} Y_{t} := \sum_{i = 1}^{p} \phi_{i} \nabla^{d} Y_{t-i} + e_{t} - \sum_{i = 1}^{q} \theta_{i} e_{t-i} $$ and this form is referred to as the $(p,d,q)$th ARIMA process $ARIMA (p,d,q)$. Such a form of time series analysis model is called ARIMA model. Explanation $ARI(p,d) \iff ARIMA(p,d,0)$ is referred to as AR model,</description></item><item><title>Exterior Unit Normal Vector</title><link>https://freshrimpsushi.github.io/en/posts/988/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/988/</guid><description>Definition1 Let $U\subset \mathbb{R}^{n}$ be an open set. Let the boundary of $U$ be $\partial U$, which is a $\partial U \in C^1$. Then, the following outward unit normal vector can be defined: $$ \boldsymbol{\nu}=(\nu^{1}, \nu^{2}, \dots, \nu^{n}) \quad \text{and} \quad |\boldsymbol{\nu}|=1 $$ $\boldsymbol{\nu}$ is a vector that touches a point on the boundary, has a magnitude of 1, and points outward. Let it be $u \in C^{1}(\bar{U})$. Then, the</description></item><item><title>Green's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/974/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/974/</guid><description>Theorem Let&amp;rsquo;s assume $u, v \in C^2( \bar{U})$. Then, the following expressions hold: (i) $\displaystyle \int_{U} \Delta u dx=\int_{\partial U} \dfrac{\partial u}{\partial \nu}dS$ (ii) $\displaystyle \int_{U} Dv \cdot Du dx = -\int_{U} u \Delta v dx+\int_{\partial U}\dfrac{\partial v}{\partial \nu}udS$ (iii) $\displaystyle \int_{U} (u\Delta v - v\Delta u )dx = \int_{\partial U} \left( \dfrac{\partial v}{\partial \nu}u - \dfrac{\partial u}{\partial \nu} v\right)dS$ These are collectively referred to as Green&amp;rsquo;s formula. $\Delta$ is</description></item><item><title>Green's Theorem, Integration by Parts Formula</title><link>https://freshrimpsushi.github.io/en/posts/990/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/990/</guid><description>Theorem Let $U\subset \mathbb{R}^{n}$ be an open set. Suppose $u : \bar{U} \to \mathbb{R}$ and $u \in C^1(\bar{U})$. Let $\nu$ be the outward unit normal vector. Then, the following equation holds: $$ \begin{equation} \int_{U} u_{x_{i}}dx=\int _{\partial U} u\nu^{i} dS\quad (i=1,\dots, n) \label{eq1} \end{equation} $$ Summing this over all $i$ gives the equation below. For each $u^{1} \in C^{1}(\bar{U})$, if we say $\mathbf{u} = (u^{1},\dots,u^{n}) : \bar{U} \to \mathbb{R}^{n}$, then: $$</description></item><item><title>Initial Value Problem and Inhomogeneous Problem Solutions for the Transport Equation</title><link>https://freshrimpsushi.github.io/en/posts/986/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/986/</guid><description>Equation Below partial differential equation is known as the transport equation. $$ u_{t} + b \cdot Du=0\quad \text{in }\mathbb{R}^n \times (0,\ \infty) $$ Solution1 Initial Value Problem Let&amp;rsquo;s suppose the initial value problem of the transport equation is given as follows. $$ \begin{equation} \left\{ \begin{aligned} u_{t}+b \cdot Du &amp;amp;= 0 &amp;amp;&amp;amp; \text{in } \mathbb{R}^n \times [0,\ \infty) \\ u &amp;amp;= g &amp;amp;&amp;amp; \text{on } \mathbb{R}^n\times \left\{ t=0 \right\} \end{aligned} \right.</description></item><item><title>Secant Method</title><link>https://freshrimpsushi.github.io/en/posts/682/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/682/</guid><description>Methods Let $f,f&amp;rsquo;,f&amp;rsquo;&amp;rsquo;$ be continuous near $\alpha$ and suppose $f(\alpha) = 0, f '(\alpha) \ne 0$. For a sufficiently close initial value $x_{0} , x_{1}$ to $\alpha$, the sequence $\left\{ x_{n} \right\}$ defined by $$ x_{n+1} := x_{n} - f ( x_{n} ) {{ x_{n} - x_{n-1} } \over { f ( x_{n} ) - f ( x_{n-1} ) }} $$ converges to $\alpha$ with order $\displaystyle {{1 + \sqrt{5}</description></item><item><title>Derivative's Fourier Coefficients</title><link>https://freshrimpsushi.github.io/en/posts/979/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/979/</guid><description>Formula Given that a function $f$ defined on the interval $[-L,\ L)$ is continuous and piecewise smooth, then the Fourier coefficients of $f^{\prime}$ are as follows. $$ a^{\prime}_{n}=\dfrac{n\pi}{L}b_{n} $$ $$ b^{\prime}_{n}=-\dfrac{n\pi}{L}a_{n} $$ $$ c^{\prime}_{n}=\dfrac{in\pi}{L}c_{n} $$ Here, $a_{n},\ b_{n}$ are the Fourier coefficients of $f$, and $c_{n}$ are the complex Fourier coefficients of $f$. Proof $$ \begin{align*} c^{\prime}_{n} &amp;amp;=\dfrac{1}{2L}\int _{-L}^{L} f^{\prime}(t)e^{-i\frac{n\pi t}{L}}dt \\ &amp;amp;= \dfrac{1}{2L}\left[ f(t)e^{-i\frac{n\pi t}{L}} \right]_{-L}^{L} +\dfrac{in \pi}{L}\dfrac{1}{2L}\int_{-L}^{L} f(t)e^{-i\frac{n</description></item><item><title>Differential Stages in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/969/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/969/</guid><description>Definition A Divided Difference of a function $f : \mathbb{R} \to \mathbb{R}$ for distinct $x_{1} , \cdots , x_{n}$ is defined as follows: $$ \begin{align*} f[x_{0}] :=&amp;amp; f( x_{0} ) \\ f [ x_{0} , x_{1} ] :=&amp;amp; {{ f ( x_{1} ) - f ( x_{0} ) } \over { x_{1} - x_{0} }} \\ f [ x_{0} , x_{1} , x_{2} ] :=&amp;amp; {{ f [ x_{1} ,</description></item><item><title>Fourier Series Integration</title><link>https://freshrimpsushi.github.io/en/posts/980/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/980/</guid><description>Theorem Let&amp;rsquo;s say a periodic function $f$ with a period $2L$ is piecewise continuous in the interval $[-L,\ L)$. Then, the definite integral of $f$ can be expressed as follows. $$ \int_{t_{1}}^{t_{2}} f(t) dt= c_{0}(t_{2}-t_{1}) +\sum \limits_{n \ne 0} \dfrac{L}{in\pi}c_{n}\left( e^{i\frac{n\pi t_{2}}{L}}-e^{i\frac{n\pi t_{1}}{L}} \right) $$ Here, $c_{0},\ c_{n}$ is the complex Fourier coefficient. Thus, the definite integral of $f(t)$ is the same as summing the definite integrals of each term</description></item><item><title>Mean of Function Values</title><link>https://freshrimpsushi.github.io/en/posts/983/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/983/</guid><description>Definition The average value of a function between $[a,\ b]$ and $f(x)$ is equivalent to dividing the integral of the function over the interval by the length of the interval. $$ \dfrac{1}{b-a}\int_{a}^bf(x)dx $$ Derivation Let&amp;rsquo;s denote a partition of the interval $[a,\ b]$ as $P$. $$ P=\left\{ x_{1},\ x_{2},\ \cdots ,\ x_{n} \right\} $$ In this case, $a=x_{1} &amp;lt; x_{2} &amp;lt; \cdots &amp;lt; x_{n}=b$ and the distance between each point</description></item><item><title>Proof of Pollard's Rho Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/940/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/940/</guid><description>Algorithm Let&amp;rsquo;s say that an element $g$ of a group $G$ has order $N = q_{1}^{r_{1}} q_{2}^{r_{2}} \cdots q_{t}^{r_{t}}$. Then, the discrete logarithm problem $g^{x} = h$ can be solved in no more than $\displaystyle O \left( \sum_{i=1}^{t} S_{q_{i}^{r_{i}}} + \log N \right)$ steps according to the following algorithm. Step 1. Compute $\displaystyle g_{i} : = g^{N / q_{i}^{r_{i}}}$ and $\displaystyle h_{i} := h^{N / q_{i}^{r_{i}}}$. Step 2. Find the</description></item><item><title>The constant term of the Fourier series is equal to the average of one period of the function.</title><link>https://freshrimpsushi.github.io/en/posts/984/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/984/</guid><description>Theorem The constant term of the Fourier series of a function with period $2L$, namely $f$, equals the average of one period of the function $f$. Proof By definition The integral over one period of $f(t)$ is $$ \dfrac{1}{2L}\int_{-L}^{L} f(t)dt $$ According to the definition of the Fourier coefficients, this is equal to $\dfrac{1}{2}a_{0}$. Therefore, the integral over one period of $f(t)$ is the same as the constant term of</description></item><item><title>The Integral of a Periodic Function Over One Period is Constant Regardless of the Integration Interval</title><link>https://freshrimpsushi.github.io/en/posts/982/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/982/</guid><description>Theorem Let&amp;rsquo;s call $f$ a periodic function with $2L$. Then, the value below remains constant regardless of the value of $a$. $$ \int_{a}^{a+2L}f(t)dt $$ Explanation By definition of periodic functions, this is obvious. From this fact, when integrating periodic functions, techniques such as changing the interval of integration can be applied. Furthermore, if you consider it in conjunction with the average value of a function, it means that the average</description></item><item><title>The Limit of Fourier Coefficients is Zero</title><link>https://freshrimpsushi.github.io/en/posts/985/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/985/</guid><description>Theorem The Fourier Coefficients $a_{n}, b_{n}$ and Complex Fourier Coefficients $c_{\pm n}$ are the limit $n \rightarrow \infty$ $$ \begin{align*} \lim \limits_{n \rightarrow \infty} a_{n} &amp;amp;= 0 \\ \lim \limits_{n \rightarrow \infty} b_{n} &amp;amp;= 0 \\ \lim \limits_{n \rightarrow \infty} c_{\pm n} &amp;amp;= 0 \end{align*} $$ Proof By the Bessel&amp;rsquo;s Inequality, we know that the sum of the Fourier coefficients converges. $$ \dfrac{1}{4}|a_{0}|^2 +\dfrac{1}{2}\sum\limits_{n=1}^{\infty} \left(|a_{n}|^2 + |b_{n}|^2 \right) =\sum \limits_{-\infty}^{\infty}</description></item><item><title>Newton-Raphson Method</title><link>https://freshrimpsushi.github.io/en/posts/678/</link><pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/678/</guid><description>Methods 1 Let&amp;rsquo;s suppose that $f,f&amp;rsquo;,f&amp;rsquo;&amp;rsquo;$ is continuous near $\alpha$ and define it as $f(\alpha) = 0, f '(\alpha) \ne 0$. For an initial value $x_{0}$, close enough to $\alpha$, the sequence $\left\{ x_{n} \right\}$ defined by $$ x_{n+1} := x_{n} - {{ f ( x_{n} ) } \over { f ' ( x_{n} ) }} $$ converges quadratically to $\alpha$ when $n \to \infty$. Explanation The Newton-Raphson method, also</description></item><item><title>Shapiro-Wilk Test</title><link>https://freshrimpsushi.github.io/en/posts/939/</link><pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/939/</guid><description>Hypothesis Testing Given quantitative data $\left\{ x_{i} \right\}_{i = 1}^{n}$. $H_{0}$: Data $\left\{ x_{i} \right\}_{i = 1}^{n}$ follows a normal distribution. $H_{1}$: Data $\left\{ x_{i} \right\}_{i = 1}^{n}$ does not follow a normal distribution. Description The Shapiro-Wilk test is a hypothesis test used to assess the normality of data, usually to demonstrate that normality is present. It&amp;rsquo;s one of the rare occasions where having the null hypothesis accepted matches &amp;rsquo;the</description></item><item><title>Bisection Method</title><link>https://freshrimpsushi.github.io/en/posts/676/</link><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/676/</guid><description>Method 1 Let&amp;rsquo;s assume that a continuous function $f$ is defined on the closed interval $[a,b]$ and is equal to $f(a) f(b) &amp;lt; 0$. The tolerance is $\varepsilon$. A $c \in [a,b]$ satisfying $f(c) = 0$ can be found as follows. Step 1. $$c:= {{a+b} \over {2}}$$ Step 2. If $b-c \le \varepsilon$, return $c$. Step 3. If $f(b) f(c) &amp;lt; 0$, then $a:=c$, else $b:=c$. Then return to Step</description></item><item><title>Transformation in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/938/</link><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/938/</guid><description>Buildup The reason why transformations are necessary in time series is to give a &amp;ldquo;penalty&amp;rdquo; for increasing variance over time, to keep the variance constant, and to achieve stationarity. The square root $\sqrt{}$ and log $\log$ are often used because the amount reduced is greater for larger values. Of course, when variance decreases, it means that the trend of data converges to some point, thus no time series analysis is</description></item><item><title>Rate of Convergence in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/674/</link><pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/674/</guid><description>Definition 1 If there exists a constant $c \ge 0$ such that the sequence $\left\{ x_{n} \right\}$ converging to $\alpha$ satisfies $$ | \alpha - x_{n+1} | \le c | \alpha - x_{n} | ^{p} $$ for the order of convergence $p \ge 1$, then $\left\{ x_{n} \right\}$ is said to converge to $\alpha$ at the rate of $c$ of order $p$. Explanation In particular, together with the condition $c</description></item><item><title>Smooth Primes</title><link>https://freshrimpsushi.github.io/en/posts/927/</link><pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/927/</guid><description>Definitions A prime number $p$ that has many divisors is called a smooth prime if $(p-1)$. A number that can be represented as a product of prime numbers less than or equal to $B$ is called a $B$-smooth number. $\psi ( X , B )$ represents the number of $B$-smooth numbers less than or equal to $X$. Description As an example of a smooth prime, consider $p=37$ where $(p-1)$ is</description></item><item><title>Continuity in Every Piece, Smoothness in Every Segment</title><link>https://freshrimpsushi.github.io/en/posts/972/</link><pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/972/</guid><description>Definition A function $f$ is said to be piecewise continuous on an interval $I$ if it satisfies the conditions below: It has a finite number of discontinuities $x_{1},\ x_{2},\ \cdots ,\ x_{n} \in I$. At each point of discontinuity, it has both a left-hand limit and a right-hand limit. $$ \left|\lim \limits_{x\rightarrow x_{i}^{+}} f(x) \right| &amp;lt; \infty \quad \text{and} \quad \left|\lim_{x \rightarrow x_{i}^{-}}f(x)\right|&amp;lt;\infty \quad (i=1,\ \cdots ,\ n) $$ If</description></item><item><title>Convergence of Fourier Series at Discontinuities</title><link>https://freshrimpsushi.github.io/en/posts/973/</link><pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/973/</guid><description>Theorem1 Let&amp;rsquo;s say the function $f(t)$, defined in the interval $[-L,\ L)$, is piecewise continuous. Denoting the points of discontinuity as $t_{i}\ (i=1,\ \cdots m )$ and assuming at each point of discontinuity, there exist left-hand derivative $f(a-)$ and right-hand derivative $f(a+)$. Then, the Fourier series of $f(t)$ converges to the midpoint of the left-hand and right-hand limits at the point of discontinuity $t_{i}$. $$ \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{\infty}\left( a_{n} \cos \dfrac{n</description></item><item><title>Shor's Algorithm Proof</title><link>https://freshrimpsushi.github.io/en/posts/917/</link><pubDate>Tue, 19 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/917/</guid><description>Algorithm 1 Let us say the element $g$ of the group $G$, with an identity element of $e$, has an order of $N$. Then, the discrete logarithm problem $g^{x} = h$ can be solved in at most $O \left( \sqrt{N} \log N \right)$ steps according to the following algorithm. Step 1. $n: = 1 + \lfloor \sqrt{N} \rfloor $ Step 2. Create two lists $A := \left\{ e , g</description></item><item><title>Differencing in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/916/</link><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/916/</guid><description>Definition 1 Define operator $B$ as $B Y_{t} = Y_{t-1}$, referred to as Backshift. Define operator $\nabla$ as $\nabla := 1 - B$ and $\nabla^{r+1} = \nabla \left( \nabla^{r} Y_{t} \right)$, referred to as Differencing. Explanation According to the definition of differencing, the $1$th difference is calculated as $$ \nabla Y_{t} = Y_{t} - Y_{t-1} $$, and the $2$th difference is calculated as $$ \begin{align*} \nabla^2 Y_{t} =&amp;amp; \nabla \left(</description></item><item><title>Wronskian of Two Solutions of a Second Order Linear Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/965/</link><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/965/</guid><description>Theorem 1 Suppose $y_{1}$ and $y_{2}$ are solutions to the second-order linear differential equation $y^{\prime \prime}+p(t)y^{\prime}+q(t)y=0$. Then, The Wronskian of $y_{1}$ and $y_{2}$ is expressed in the form of an exponential function. $$ W [y_{1}, y_{2}] (t)=c e^{-\int p(t) dt} $$ Where $c$ is a constant that depends on $y_{1},\ y_{2}$. $W[y_{1},y_{2}] (t)$ is either always $0$ or never $0$ at all points. Explanation Also known as Abel&amp;rsquo;s theorem. Although</description></item><item><title>Electromotive Force and Kinetic Electromotive Force</title><link>https://freshrimpsushi.github.io/en/posts/963/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/963/</guid><description>Electromotive Force1 Let us denote the force that moves charges and generates current in a circuit as $\mathbf{f}$. This $\mathbf{f}$ can be divided into two types. One is the force of the circuit&amp;rsquo;s power source, $\mathbf{f}_{s}$, and the other is the electric force, $\mathbf{E}$, created by the charges accumulated in some part of the circuit. Here, the subscript $s$ stands for source. Therefore, $$ \mathbf{f}=\mathbf{f}_{s}+\mathbf{E} $$ The force of the</description></item><item><title>ElGamal Public Key Cryptosystem Proof</title><link>https://freshrimpsushi.github.io/en/posts/915/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/915/</guid><description>Buildup From left to right, let&amp;rsquo;s name them Alice, Bob, and Eve. Alice and Bob are the parties exchanging messages, and Eve is a passive attacker interested in the message. The orange box represents information known only to Alice, the sky-blue box represents information known only to Bob, and the black box represents public information (also known to Eve). Alice has a message $m \in \mathbb{N}$ to receive from Bob.</description></item><item><title>Partial Integration of Expressions Containing the Del Operator</title><link>https://freshrimpsushi.github.io/en/posts/959/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/959/</guid><description>Formulas The following expressions hold true for vector integration involving the del operator. (a) $$ \int_{\mathcal{V}}\mathbf{A} \cdot (\nabla f)d\tau = \oint_{\mathcal{S}}f\mathbf{A} \cdot d \mathbf{a}-\int_{\mathcal{V}}f(\nabla \cdot \mathbf{A})d\tau $$ (b) $$ \int_{\mathcal{S}} f \left( \nabla \times \mathbf{A} \right)\mathbf{A} \cdot d \mathbf{a} = \int_{\mathcal{S}} \left[ \mathbf{A} \times \left( \nabla f \right) \right] \cdot d\mathbf{a} + \oint_{\mathcal{P}} f\mathbf{A} \cdot d\mathbf{l} $$ (c) $$ \int_{\mathcal{V}} \mathbf{B} \cdot \left( \nabla \times \mathbf{A} \right) d\tau = \int_{\mathcal{V}}</description></item><item><title>Work and Energy in Electrostatics</title><link>https://freshrimpsushi.github.io/en/posts/961/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/961/</guid><description>Moving a Charge1 The following equation holds between potential and electric field. $$ -\int_\mathbf{a} ^\mathbf{b} \mathbf{E} \cdot d\mathbf{l} = \int_\mathbf{a} ^ \mathbf{b} \left( \nabla V \right) \cdot d\mathbf{l} = V(\mathbf{b}) - V(\mathbf{a}) $$ Thus, if there is a fixed source charge distribution, and we move a test charge $Q$ from point $\mathbf{a}$ to point $\mathbf{b}$, the work done is calculated as follows. $$ W=\int_{\mathbf{a}}^\mathbf{b} \mathbf{F} \cdot d\mathbf{l} = -Q\int_\mathbf{a}^\mathbf{b} \mathbf{E}</description></item><item><title>Autoregressive Moving Average Model</title><link>https://freshrimpsushi.github.io/en/posts/914/</link><pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/914/</guid><description>Model 1 White noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ is defined as $$ Y_{t} := \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots + \phi_{p} Y_{t-p} +e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2} - \cdots - \theta_{q} e_{t-q} $$ such a process is called a $(p,q)$th order autoregressive moving average process $ARMA(p,q)$. Explanation The ARMA model is simply a combination of the Moving Average Process and the Autoregressive Process. For instance,</description></item><item><title>Proof of the Diffie-Hellman Key Exchange Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/912/</link><pubDate>Fri, 15 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/912/</guid><description>Buildup From left to right, let&amp;rsquo;s call them Alice, Bob, and Eve. Alice and Bob are the parties exchanging messages, and Eve is a passive attacker interested in the messages. The orange box denotes information known only to Alice, the sky-blue box denotes information known only to Bob, and the black box denotes information that is public (also known to Eve). Alice and Bob intend to exchange messages under the</description></item><item><title>Autoregressive Process</title><link>https://freshrimpsushi.github.io/en/posts/910/</link><pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/910/</guid><description>Model 1 White noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ is defined as in $Y_{t} := \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots + \phi_{p} Y_{t-p} + e_{t}$ and is called an $p$th order autoregressive process $AR(p)$. (1): $AR(1) : Y_{t} = \phi Y_{t-1} + e_{t}$ (2): $AR(2) : Y_{t} = \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + e_{t}$ (p): $AR(p) : Y_{t} = \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots +</description></item><item><title>How to Install R on Ubuntu</title><link>https://freshrimpsushi.github.io/en/posts/946/</link><pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/946/</guid><description>Guide Step 1. Press Ctrl+Alt+T to open the console window. Step 2. Enter the following in the console window. sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 Administrator privileges are required, so you need to enter the password for your user account. Step 3. Enter the following in the console window. sudo add-apt-repository &amp;#39;deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/&amp;#39; Step 4. Enter the following in the console window. sudo apt update Step 5. Enter</description></item><item><title>Discrete Logarithms</title><link>https://freshrimpsushi.github.io/en/posts/911/</link><pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/911/</guid><description>Definition 1 Let&amp;rsquo;s say that for a prime number $p$, the identity element in the Galois Field $\mathbb{F}_{p} := \mathbb{Z} / p \mathbb{Z}$ is $0$. Then, for a primitive root $g \ne 0$ of $\mathbb{F}_{p}$, a function $\log_{g} : \mathbb{F}_{p}^{ \ast } \to \mathbb{Z} / (p-1) \mathbb{Z}$ defined on a cyclic group $\mathbb{F}_{p} ^{ \ast } := \mathbb{F}_{p} \setminus \left\{ 0 \right\} = \left&amp;lt; g \right&amp;gt;$ that satisfies the following</description></item><item><title>Moving Average Process</title><link>https://freshrimpsushi.github.io/en/posts/909/</link><pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/909/</guid><description>Model 1 The process defined as follows for white noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ and according to $Y_{t} := e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2} - \cdots - \theta_{q} e_{t-q}$, is called the $q$th order moving average process $MA(q)$. (1): $MA(1) : Y_{t} = e_{t} - \theta e_{t-1}$ (2): $MA(2) : Y_{t} = e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2}$ (q): $MA(q) : Y_{t} = e_{t} - \theta_{1}</description></item><item><title>Chebyshev Differential Equations and Chebyshev Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/956/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/956/</guid><description>Definition The following differential equation is referred to as the Chebyshev Differential Equation. $$ \begin{equation} (1-x^2)\dfrac{d^2 y}{dx^2} -x\dfrac{dy}{dx}+n^2 y=0 \label{def1} \end{equation} $$ The solution to the Chebyshev differential equation is known as Chebyshev polynomials, commonly denoted by $T_{n}(x)$. The general term of $T_{n}(x)$ is as follows: When $n$ is even $$ 1-\dfrac{\lambda^2}{2!}x^2+\dfrac{\lambda^2(\lambda^2-2^2)}{4!}x^4+\sum \limits_{m=3}^\infty (-1)^m \dfrac{\lambda^2(\lambda^2-2^2)\cdots(\lambda^2-(2m-2)^2)}{(2m)!} x^{2m} $$ When $n$ is odd $$ x-\dfrac{\lambda^2-1^2}{3!}x^3+\dfrac{(\lambda^2-1^2)(\lambda^2-3^2)}{5!}x^5+\sum \limits_{m=3}^\infty (-1)^m\dfrac{(\lambda^2-1^2)(\lambda^2-3^2) \cdots (\lambda^2-(2m-1)^2)}{(2m+1)!} x^{2m+1} $$ Especially,</description></item><item><title>Encryption and Decryption in Cryptography</title><link>https://freshrimpsushi.github.io/en/posts/908/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/908/</guid><description>Buildup Let&amp;rsquo;s imagine that Alice wants to convey a message to Bob. If there were only two people in the world, this message would be shared exclusively between them, with no reason to hide it. [ NOTE: In cryptography, Alice and Bob are names commonly used to represent $A$ and $B$, respectively. ] However, suppose there is a third person, Eve. Eve might not necessarily have bad intentions, but she</description></item><item><title>Series Solution of Chebyshev Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/955/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/955/</guid><description>Definition The following differential equation is referred to as the Chebyshev Differential Equation: $$ (1-x^2)\dfrac{d^2 y}{dx^2} -x\dfrac{dy}{dx}+n^2 y=0 $$ Description It&amp;rsquo;s a form that includes the independent variable $x$ in the coefficient, and assuming that the solution is in the form of a power series, it can be solved. The solution to the Chebyshev equation is called the Chebyshev polynomial, often denoted as $T_{n}(x)$. Solution $$ \begin{equation} (1-x^2)y^{\prime \prime} -xy^{\prime}+\lambda^2</description></item><item><title>Bound Current Density and the Vector Magnetic Field Created by a Magnetized Object</title><link>https://freshrimpsushi.github.io/en/posts/954/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/954/</guid><description>Explanation1 Suppose there is an object magnetized by an external magnetic field. This object will have a magnetization density $\mathbf{M}$, and this magnetization density will generate a new magnetic field. The vector potential created by a magnetic dipole is as follows. $$ \mathbf{A} (\mathbf{r}) = \dfrac{\mu_{0}}{4\pi}\dfrac{\mathbf{m} \times \crH }{\cR ^2} $$ Since magnetization density is the dipole moment per unit volume, $\mathbf{M}=\dfrac{\mathbf{m}}{d\tau}$. By substituting this into the above equation and</description></item><item><title>Changes in Electron Orbits Due to External Magnetic Fields and Diamagnetism</title><link>https://freshrimpsushi.github.io/en/posts/953/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/953/</guid><description>Explanation1 Let&amp;rsquo;s imagine an electron orbiting around the nucleus with a radius of $R$. Although the moving point charge does not become a steady current, it appears to do so because of its high velocity. The period is calculated by dividing the distance traveled by the speed, thus $$ T=\dfrac{2\pi R}{v} $$ The current $I$ is the amount of charge passing per unit time, and since the electron passes through</description></item><item><title>Self-Density and Ferromagnets</title><link>https://freshrimpsushi.github.io/en/posts/951/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/951/</guid><description>Description1 Let&amp;rsquo;s assume there is an object that appears to be non-magnetic at first glance. If we look into this object down to the atomic level, we find that tiny currents are created by electrons orbiting around the nucleus, which result in magnetic phenomena. This means that very small magnetic dipoles are generated in each atom. However, since the directions of the atoms are all different, summing all these dipole</description></item><item><title>Stability in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/907/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/907/</guid><description>Definition 1 Time series data is said to have stationarity when its mean and variance are constant over time. Description It&amp;rsquo;s not normal正常 as in standard, but stationarity定常. The fact that data is stationary means that its mean and variance are stabilized, making it easier to analyze. If the</description></item><item><title>The Torque on a Magnetic Dipole in an External Magnetic Field and Paramagnetism</title><link>https://freshrimpsushi.github.io/en/posts/952/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/952/</guid><description>Explanation1 Just like an electric dipole gains torque by an external electric field, a magnetic dipole does the same. Let&amp;rsquo;s assume there is a current loop in a uniform external magnetic field $\mathbf{B}=B\hat{\mathbf{z}}$ as shown below. Since a small rectangular current loop can be overlapped to approximate a current loop of any shape, let&amp;rsquo;s focus on the rectangular current loop. The magnetic force received by each side can be calculated</description></item><item><title>Adding Subscripts to Axis Labels in R Graphs</title><link>https://freshrimpsushi.github.io/en/posts/905/</link><pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/905/</guid><description>Code In R, it is allowed to include underscores _ in variable names, but if they are displayed like that on a graph, readability can be severely reduced. Using the expression() function as follows allows for nicely adding subscripts to axis names. data&amp;lt;-as.numeric(lynx) win.graph(4,4) plot(data[-1],data[-length(data)],type=&amp;#39;p \&amp;#39;,main=&amp;#39;아래첨자 사용&amp;#</description></item><item><title>Time Series Analysis: White Noise</title><link>https://freshrimpsushi.github.io/en/posts/904/</link><pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/904/</guid><description>Definition 1 A sequence $\left\{ e_{t} \right\}_{t = 1}^{\infty}$ of independent identically distributed (iid) random variables $e_{t}$ is called White Noise. iid stands for independent identically distributed, meaning that they are independent from each other and share the same distribution. Description Following the definition of a sequence of random variables, it is naturally a stochastic process. Particularly, if $E ( e_{t} ) = 0$, then the stochastic process $\left\{ Y_{t}</description></item><item><title>Definition of Poisson Process Through Exponential Distribution</title><link>https://freshrimpsushi.github.io/en/posts/903/</link><pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/903/</guid><description>Definition Let&amp;rsquo;s define $\tau_{1} , \tau_{2} , \cdots \sim \text{exp} ( \lambda )$. $\lambda$ is referred to as Intensity. $\displaystyle s_{n}:= \sum_{k=1}^{n} \tau_{k}$ is called the Arrival Time. A stochastic process defined as $N_{t}:= \begin{cases} 0 , &amp;amp; 0 \le t &amp;lt; s_{1} \\ k , &amp;amp; s_{k} \le t &amp;lt; s_{k+1} \end{cases}$, $\left\{ N_{t} \right\}_{t = 0}^{\infty}$ is called a Poisson Process. Basic Properties [1]: $\displaystyle p (N_{t} =</description></item><item><title>Probabilistic interpretation and normalization of the wave function in quantum mechanics</title><link>https://freshrimpsushi.github.io/en/posts/945/</link><pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/945/</guid><description>Wave Function The Wave Function is a function in quantum mechanics that represents the state of motion of a particle with respect to time and position. In a sushi restaurant, the wave function with respect to position and time is denoted as $\psi (x,t)$, and the wave function with respect to position and independent of time is denoted as $u(x)$. Probabilistic Interpretation The method of understanding the state of a</description></item><item><title>Hidden Markov Chain</title><link>https://freshrimpsushi.github.io/en/posts/901/</link><pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/901/</guid><description>Buildup Imagine a machine that produces certain items at fixed intervals as shown in the figure above. If green represents the normal quality goods $1$ and red represents defective goods that need to be discarded $0$, then the record so far would be $\left( 1, 0 , 1 \right)$. The actual visible results like this are referred to as Signal. However, the probability of producing a defective item varies depending</description></item><item><title>Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/900/</link><pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/900/</guid><description>Description Time Series can be simply seen as a stochastic process obtained from real data. The stock index is a good example of a time series as its value changes with uncertainty over time. Time series analysis aims to understand and predict the movement of a dependent variable over time. The biggest difference between time series analysis and regression analysis is that, while regression analysis assumes that the independent variables</description></item><item><title>Alignment of Polar Molecules by a Constant External Electric Field</title><link>https://freshrimpsushi.github.io/en/posts/817/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/817/</guid><description>Overview 1 When an electrically neutral atom is placed in an external electric field, it becomes polarized and acquires a dipole moment $\mathbf{p}$. However, some molecules have a dipole moment even without the influence of an external electric field. Such molecules are referred to as polar molecules. Polar Molecules An example of a polar molecule is a water molecule. Water molecules are bent as shown in $105^{\circ}$, resulting in a</description></item><item><title>Alignment of Polar Molecules by a Non-uniform Electric Field</title><link>https://freshrimpsushi.github.io/en/posts/818/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/818/</guid><description>Explanation 1 Polar molecules possess a dipole moment even in the absence of an external electric field. If there is a constant external electric field, the dipole moment aligns with the direction of the electric field. However, if the external electric field is not constant, $\mathbf{F}_+$ and $\mathbf{F}_-$ are not the same, resulting in a net force as well as a torque. The net force can be calculated as follows.</description></item><item><title>Ampère's Law and Applications</title><link>https://freshrimpsushi.github.io/en/posts/922/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/922/</guid><description>Formulas The magnetic field $\mathbf{B}$ that arises from the volume current density $\mathbf{J}$ rotates in the direction that satisfies the right-hand rule with the direction of $\mathbf{J}$ as the axis. $$ \nabla \times \mathbf{B}=\mu_{0} \mathbf{J} $$ Explanation1 Ampère&amp;rsquo;s law indicates a special relationship between the current flowing through a conductor and the magnetic field around it. A</description></item><item><title>Current and Current Density</title><link>https://freshrimpsushi.github.io/en/posts/898/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/898/</guid><description>Definition1 The electric current is defined as the amount of charge that passes through a given point in a conductor per unit of time, denoted by $I$. Thus, a negative charge moving to the left and a positive charge moving to the right constitute an electric current of the same sign. The amount of Coulomb passing per unit of time is called ampere. $$ 1 [A] = 1 [C/s] $$</description></item><item><title>Divergence (Divergence) and Curl of Magnetic Fields</title><link>https://freshrimpsushi.github.io/en/posts/920/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/920/</guid><description>Theorem The divergence and curl of a magnetic field are as follows: $$ \begin{align*} \nabla \cdot \mathbf{B} =&amp;amp;\ 0 \\ \nabla \times \mathbf{B} =&amp;amp;\ \mu_{0} \mathbf{J} \end{align*} $$ Description Just like the electric field was always a special vector function with a curl of $\mathbf{0}$, so is the magnetic field. Let&amp;rsquo;s calculate the divergence and curl using the law of Biot-Savart for volume currents. $$ \mathbf{B} (\mathbf{r}) = \dfrac{\mu_{0}}{4 \pi}</description></item><item><title>Electric Field Created by a Dipole</title><link>https://freshrimpsushi.github.io/en/posts/153/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/153/</guid><description>Explanation1 The potential due to an electric dipole $\mathbf{p}$ is as follows. $$ V_{\text{dip}}(\mathbf{r}) = \dfrac{1}{4\pi\epsilon_{0}}\dfrac{\mathbf{p}\cdot\hat{\mathbf{r}}}{r^2} = \dfrac{1}{4\pi\epsilon_{0}}\dfrac{p\cos\theta}{r^{2}} $$ Now, let&amp;rsquo;s assume that $\mathbf{p}$ is at the origin and parallel to the $z$ axis, as shown in the figure above. Since the electric field is the gradient of the potential, in spherical coordinates it is as follows. $$ \mathbf{E} = - \nabla V = -\left( \dfrac{\partial V}{\partial r}\hat{\mathbf{r}} + \frac{1}{r}\dfrac{\partial</description></item><item><title>Electric Field Created by Bound Charges and Polarized Objects</title><link>https://freshrimpsushi.github.io/en/posts/880/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/880/</guid><description>Bound Charges External electric fields cause the dipoles in a material to align in one direction, polarizing the material and giving it a dipole moment $\mathbf{p}$. The electric field produced by these dipole moments is calculated as follows. The potential created by the dipole moment $\mathbf{p}$ is as follows. $$ \begin{equation} V(\mathbf{r}) = \dfrac{1}{4 \pi \epsilon_{0}} \dfrac{ \mathbf{p} \cdot \crH } {\cR ^2} \label{1} \end{equation} $$ $\mathbf{r}^{\prime}$ is the position</description></item><item><title>Magnetic Fields Produced by Magnetic Dipoles</title><link>https://freshrimpsushi.github.io/en/posts/156/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/156/</guid><description>Description The vector potential due to a magnetic dipole $\mathbf{m}$ is as given in magnetic dipole moment. $$ \mathbf{A}_{\text{dip}}(\mathbf{r}) = \dfrac{\mu_{0}}{4 \pi} \dfrac{\mathbf{m} \times \hat{\mathbf{r}}}{r^2} = \dfrac{\mu_{0}}{4 \pi} \dfrac{m\sin\theta}{r^{2}} \hat{\boldsymbol{\phi}} $$ Now, let $\mathbf{m}$ be located at the origin and parallel to the $z$ axis, as shown in the figure above. Since the magnetic field is the curl of the vector potential, in spherical coordinates, it is as follows. $$</description></item><item><title>Magnetic Forces Do Not Work</title><link>https://freshrimpsushi.github.io/en/posts/897/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/897/</guid><description>Theorem1 Magnetic forces do not do work. Explanation In situations with magnetic forces, when particles or objects move, it might seem as though the magnetic forces are doing work. However, this is not the case. Proof Work is the product of force and displacement. $$ W=\int \mathbf{F} \cdot d\mathbf{l} $$ The work done by the magnetic force is $$ W_{\text{mag}}=\int \mathbf{F}_{\text{mag}} \cdot d\mathbf{l} = \int Q(\mathbf{v}\times \mathbf{B})\cdot d\mathbf{l} $$ Given</description></item><item><title>Magnetic Vector Potential</title><link>https://freshrimpsushi.github.io/en/posts/923/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/923/</guid><description>Explanation1 In electrostatics, the electric field is easily handled by using a property called $\nabla \times \mathbf{E} = \mathbf{0}$ to define the scalar potential $V$. Similarly, in magnetostatics, the vector potential $A$ is defined and used by utilizing a property called $\nabla \cdot \mathbf{B} = 0$. Let&amp;rsquo;s say the magnetic field $\mathbf{B}$ is the curl of some vector $\mathbf{A}$. $$ \mathbf{B}=\nabla \times \mathbf{A} $$ Since the divergence of a curl</description></item><item><title>Magnetism and the Law of Lorentz Force</title><link>https://freshrimpsushi.github.io/en/posts/896/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/896/</guid><description>Definition1 A moving charge (current) creates a magnetic field $\mathbf{B}$ around it. The force experienced by a charge $Q$ moving at velocity $\mathbf{v}$ in a magnetic field $\mathbf{B}$ is given by: $$ \begin{equation} \mathbf{F}_{m}=Q(\mathbf{v} \times \mathbf{B}) \end{equation} $$ This force is called the magnetic force, and the above formula is known as the Lorentz force law. Explanation As with the definition of an electric field, when a moving charge experiences</description></item><item><title>Multipole Expansion of Vector Potentials and Magnetic Dipole Moments</title><link>https://freshrimpsushi.github.io/en/posts/924/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/924/</guid><description>Multipole Expansion of Vector Potentials1 The multipole expansion of vector potentials refers to the expression of vector potentials as a power series approximation to $\dfrac{1}{r^{n}}$ when currents are concentrated, sufficiently far from the source. Initially, the vector potential due to a current loop is as follows. $$ \mathbf{A}(\mathbf{r})=\dfrac{\mu_{0} I}{4\pi}\oint \dfrac{1}{\cR}d\mathbf{l}^{\prime} $$ Under the conditions shown in the figure, the following equation holds. $$ \dfrac{1}{\cR} =\dfrac{1}{\sqrt{r^2+(r^{\prime})^2-2rr^{\prime}\cos\alpha}} = \dfrac{1}{r}\sum \limits_{n=0}^{\infty} \left( \dfrac{r^{\prime}}{r}</description></item><item><title>Polarization density and genomes</title><link>https://freshrimpsushi.github.io/en/posts/819/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/819/</guid><description>Overview1 2 Conductors contain a high concentration of free charges. This means that many electrons are not bound to any particular nucleus and freely roam the interior of the conductor. Conversely, in dielectrics or insulators, the situation is different. All electrons are bound to specific atoms (molecules). While they can move slightly within the molecule, they cannot move freely like free charges. An example of this slight movement is polarization.</description></item><item><title>Steady Current and Biot-Savart Law</title><link>https://freshrimpsushi.github.io/en/posts/899/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/899/</guid><description>Definition1 A steady current refers to the flow of charge that continues without changing in amount or direction. Description Since the current does not change over time, the magnetic field created by the steady current also does not change over time. The &amp;lsquo;direction of progress&amp;rsquo; mentioned here is a different concept from the direction of a vector we commonly think of. It means that as long as the flow continues</description></item><item><title>Stokes' Theorem</title><link>https://freshrimpsushi.github.io/en/posts/937/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/937/</guid><description>Theorem1 Let&amp;rsquo;s call something a vector and an area in 3D space as $\mathbf{v}, \mathcal{S}$, respectively. The area vector of $\mathcal{S}$ is denoted as $d\mathbf{a}$, the border of $\mathcal{S}$ as $\mathcal{P}$, and the path moving along $\mathcal{P}$ as $d\mathbf{l}$. Then, the following equation holds: $$ \int_{\mathcal{S}} (\nabla \times \mathbf{v} )\cdot d\mathbf{a} = \oint_{\mathcal{P}} \mathbf{v} \cdot d\mathbf{l} $$ This is called Stokes&amp;rsquo; theorem or the fundamental theorem for curl. Incidentally, outside</description></item><item><title>Multipole Expansion of Potential and Dipole Moments</title><link>https://freshrimpsushi.github.io/en/posts/936/</link><pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/936/</guid><description>Multiple Expansion When a distribution of charges is viewed from sufficiently far away, it appears almost as though it were a point charge. In other words, if the total charge of the charge distribution is $Q$, it would feel as if there&amp;rsquo;s a single point charge with charge $Q$ when viewed from afar. This means that the potential can be approximated as $\dfrac{1}{4\pi\epsilon_{0}} \dfrac{Q}{r}$. But if the total charge is</description></item><item><title>Proof of the Second Cosine Law Using the Definition of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/935/</link><pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/935/</guid><description>Formula For the triangle given above, the following equations hold true, and they are collectively known as the law of cosines. $$ \begin{cases} a^{2} =b^{2}+c^{2}-2bc\cos\alpha \\ b^{2}=a^{2}+c^{2}-2ac\cos\beta \\ c^{2}=a^{2}+b^{2}-2ab\cos\gamma \end{cases} $$ Proof From the triangle in the upper left corner of the diagram, we obtain the following equation. $$ \begin{align} a &amp;amp;= \overline{BH_{a}}+\overline{H_{a}C} \nonumber \\ &amp;amp;= c\cos\beta + b\cos\gamma \label{eq1} \end{align} $$ Multiplying both sides by $a$ yields: $$ a^{2}=ac\cos\beta</description></item><item><title>리만적분가능한 함수의 푸리에 급수는 수렴한다</title><link>https://freshrimpsushi.github.io/en/posts/934/</link><pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/934/</guid><description>Theorem1 Let the function $f$ be Riemann integrable on the interval $[-L,\ L)$. Then for a point $t$ at which the function is continuous, the Fourier series $\lim \limits_{N \to \infty }S^{f}_{N}(t)$ of $f$ converges to $f(t)$. $$ \lim \limits_{N \rightarrow \infty} S^{f}_{N}(t)=f(t) $$ In this case $$ \begin{align*} S^{f}_{N}(t)&amp;amp;=\dfrac{a_{0}}{2}+\sum \limits_{n=1}^{N} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t} {L} \right) \\ a_{0} &amp;amp;=\dfrac{1}{L}\int_{-L}^{L}f(t)dt \\ a_{n} &amp;amp;= \dfrac{1}{L}\int_{-L}^{L} f(t)\cos\dfrac{n\pi t}{L}</description></item><item><title>Derivation of Fourier Series</title><link>https://freshrimpsushi.github.io/en/posts/929/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/929/</guid><description>Definition The series for $2L$-periodic function $f$ is defined as the Fourier series of $f$ as follows: $$ \begin{align*} \lim \limits_{N \rightarrow \infty} S^{f}_{N}(t) &amp;amp;= \lim \limits_{N \to \infty}\left[ \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{N} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t}{L} \right) \right] \\ &amp;amp;= \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{\infty} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t}{L} \right) \end{align*} $$ Here, each coefficient $a_{0}, a_{n}, b_{n}$ is called the Fourier coefficient, and its value</description></item><item><title>Dirichlet Kernel</title><link>https://freshrimpsushi.github.io/en/posts/932/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/932/</guid><description>Definition Dirichlet Kernel $D_{n}$ is defined as follows. $$ \begin{equation} D_{n}(t) := \dfrac{1}{2}+\sum \limits_{k=1}^{n} \cos kt \end{equation} $$ Explanation The Dirichlet kernel is related to delta functions, exponential functions, etc., and appears in Fourier analysis. Here are some related theorems and their proofs. Theorem 1 The Dirichlet Kernel satisfies the following equation. $$ D_{n}(t)=\dfrac{\sin\left(n+\frac{1}{2}\right) t}{2\sin \frac{1}{2}t} $$ Proof If we express the cosine function as a complex exponential form, we</description></item><item><title>Legendre Polynomials are orthogonal to any lower degree polynomial</title><link>https://freshrimpsushi.github.io/en/posts/933/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/933/</guid><description>Theorem When $P_{l}(x)$ is a Legendre Polynomial and $f(x)$ is any polynomial of lower degree than $l$, then $P_{l}(x)$ and $f(x)$ are orthogonal to each other. $$ \int_{-1}^{1}P_{l}(x)f(x)dx = 0 $$ Explanation The following lemma is essentially equivalent to the proof of the theorem. Lemma Let $f(x)$ be any polynomial of degree $n$. $f(x)$ can be expressed as a linear combination of Legendre polynomials up to degree $l \le n$.</description></item><item><title>Orthogonal Functions and Orthogonal Sets</title><link>https://freshrimpsushi.github.io/en/posts/926/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/926/</guid><description>Definition Inner Product The inner product of two functions $f$ and $g$ defined on the interval $[a,b]$ is defined as follows. $$ \braket{f , g} := \int_{a}^b f(x) g(x) dx $$ When $f, g$ is a complex function, then, $$ \braket{f, g} := \int_{a}^{b} f(x) \overline{g(x)} dx $$ In this case, $\overline{z}$ is the conjugate complex of $z$. Orthogonal Functions Two complex functions $f$, $g$ are said to be orthogonal</description></item><item><title>Orthogonality of Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/931/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/931/</guid><description>Theorem In the interval $[-1,\ 1]$, Legendre polynomials form an orthogonal set. $$ \int_{-1}^{1} P_{l}(x)P_{m}(x) dx =\frac{2}{2l+1}\delta_{lm} \quad (l, m = 0, 1, 2, \dots) $$ Proof Case 1: $l \ne m$ Legendre Differential Equation The following differential equation is called the Legendre differential equation. $$ \dfrac{d}{d x}\left[ (1-x)^{2} \dfrac{d y}{d x} \right] +l(l+1)y = 0 $$ Since Legendre polynomials are solutions to the Legendre differential equation, they satisfy the</description></item><item><title>Proof of the Orthogonality of the Set of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/928/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/928/</guid><description>Theorem The set $\left\{ 1,\ \cos \dfrac{\pi x}{L},\ \cos \dfrac{2\pi x}{L}, \cdots ,\ \sin\dfrac{\pi x}{L},\ \sin\dfrac{2\pi x}{L},\ \cdots \right\}$ of functions $2L$ that are periodic functions is an orthogonal set in the interval $[-L,\ L)$. In other words, for $m,n = 1, 2, 3, \dots$, the following holds. $$ \begin{align} \dfrac{1}{L} \int _{-L}^{L} \cos\dfrac{m\pi x}{L} \cos\dfrac{n\pi x}{L} dx &amp;amp;= \delta_{mn} \label{eq1} \\ \dfrac{1}{L} \int _{-L}^{L} \sin \dfrac{m\pi x}{L}\sin \dfrac{n\pi x}{L}</description></item><item><title>Sum of Trigonometric Functions Orthogonal to Each Other</title><link>https://freshrimpsushi.github.io/en/posts/930/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/930/</guid><description>Formula Define $C_{n}$ and $S_{n}$ as follows. $$ \begin{align*} C_{n}: &amp;amp;= 1+\cos x + \cos 2x + \cdots +\cos nx \\ S_{n}: &amp;amp;= \sin x +\sin 2x + \cdots + \sin nx \end{align*} $$ Then, the following equation holds. $$ \begin{align*} C_{n} &amp;amp;= \dfrac{\sin \dfrac{n+1}{2}x}{\sin \dfrac{1}{2}x} \cos \dfrac{n}{2}x \\ S_{n} &amp;amp;= \dfrac{\sin \dfrac{n+1}{2}x}{\sin \dfrac{1}{2}x}\sin \dfrac{n}{2}x \end{align*} $$ Proof Use the Euler&amp;rsquo;s formula. $$ \begin{align*} &amp;amp; C_{n}+ i S_{n} \\ =&amp;amp;\</description></item><item><title>The Gambler's Ruin Problem</title><link>https://freshrimpsushi.github.io/en/posts/893/</link><pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/893/</guid><description>Problem The gambler&amp;rsquo;s ruin problem is a type of random walk where two players bet a finite amount of money and the game is repeated until one of them goes bankrupt. If you are one of the players, then according to the diagram above, the probability of you winning is $p$ and the probability of losing is $(1-p)$. State $0$ is when you go bankrupt, and state $N$ is when</description></item><item><title>Galois Theory</title><link>https://freshrimpsushi.github.io/en/posts/891/</link><pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/891/</guid><description>Theorem 1 Let $K$ be a Galois extension of $F$ with group $F \le E \le K$. Let us denote by $\lambda (E)$ the subgroup of $G ( K / F )$ that fixes $E$. Then, the map $\lambda$ becomes an isomorphism mapping all $E$ between $F$ and $K$ to all subgroups of $G ( K / F )$. $\lambda$ has the following properties: $\lambda ( E ) = G</description></item><item><title>Comparing Models Using the AUC of ROC Curves</title><link>https://freshrimpsushi.github.io/en/posts/887/</link><pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/887/</guid><description>Theorem ROC curves are essentially better as they fill up the rectangle $[0,1]^2$ more, and it is preferable for the curve&amp;rsquo;s turning point in the upper left to be closer to $(0,1)$. Description Given the two ROC curves above, the right side can comfortably be considered &amp;lsquo;better&amp;rsquo;. This &amp;lsquo;better&amp;rsquo; refers to the model generating the ROC curve being superior. Naturally, the two models being compared are derived from the same</description></item><item><title>Rotation of Separation Vector</title><link>https://freshrimpsushi.github.io/en/posts/919/</link><pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/919/</guid><description>Equation $$ \nabla \times \dfrac{\crH }{\cR ^2} = \mathbf{0} $$ Explanation There is nothing particularly special about this formula. It emerges in the process of calculating the divergence of a magnetic field, and its calculation is not straightforward, hence the separate explanation. Proof If we refer to $\bcR=(x-x^{\prime})\hat{\mathbf{x}} + (y-y^{\prime})\hat{\mathbf{y}} + (z-z^{\prime})\hat{\mathbf{z}}$ as a component vector, it can be represented as follows: $$ | \bcR |=\cR=\sqrt{(x-x^{\prime})^2+(y-y^{\prime})^2 + (z-z^{\prime})^2} $$ $$</description></item><item><title>Pell's equation</title><link>https://freshrimpsushi.github.io/en/posts/885/</link><pubDate>Mon, 25 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/885/</guid><description>Buildup $a_{n} : = n^2$ is referred to as a Square Number. $\displaystyle b_{m} : = {{ m ( m + 1 ) } \over {2}}$ is referred to as a Triangular Number. Considering if there are numbers that are both square and triangular, $a_{1} =b_{1} = 1$ and $\displaystyle a_{6} = 6 ^2 = 36 = {{ 8 \cdot 9 } \over {2}} = b_{8}$ immediately come to mind.</description></item><item><title>Finding the optimal cutoff using ROC curves</title><link>https://freshrimpsushi.github.io/en/posts/877/</link><pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/877/</guid><description>Overview Drawing the ROC Curve is useful as it provides a visual representation of how well a model derived from training data explains the test data. However, since this curve computes the classification rate for all cutoffs, it cannot definitively tell us &amp;lsquo;which cutoff to use for classifying 0 and 1&amp;rsquo;. To address this, let&amp;rsquo;s apply the methodology of cross-validation. Validation Data Finding the optimal cutoff that best classifies 0</description></item><item><title>Rationalizing Fractions Containing Roots Quickly</title><link>https://freshrimpsushi.github.io/en/posts/874/</link><pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/874/</guid><description>Formulas $$ {{ x } \over { \sqrt{a} \pm \sqrt{b} }} = {{ x \left( \sqrt{a} \mp \sqrt{b} \right) } \over { a - b }} $$ Explanation Rationalizing fractions may conceptually be simple, but it becomes difficult due to the extensive calculations involved in multiplying complex terms to both the numerator and denominator and then simplifying them. However, by using the formula above, one can rationalize quickly and easily,</description></item><item><title>Generalized Random Walk</title><link>https://freshrimpsushi.github.io/en/posts/870/</link><pubDate>Thu, 21 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/870/</guid><description>Definition A stochastic process $\left\{ X_{n} \right\}$ whose state space is the set of integers $\left\{ \cdots , -2 , -1, 0 , 1 , 2 , \cdots \right\}$ and starts from state $0$ is referred to as a generalized random walk if the probability of decreasing by $1$ in the next step is $p$, and the probability of increasing by $1$ is $(1-p)$. Explanation A random walk, which is</description></item><item><title>Drawing ROC Curves in R</title><link>https://freshrimpsushi.github.io/en/posts/868/</link><pubDate>Wed, 20 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/868/</guid><description>Definition The graph that plots the False Positive Rate and True Positive Rate of a confusion matrix is called the ROC curve (Receiver Operating Characteristic Curve). Explanation The ROC curve is extremely useful not only for providing a clear picture of a model&amp;rsquo;s performance but also for finding the optimal cutoff and comparing different models. Let&amp;rsquo;s understand its meaning and draw an ROC curve in R using an example. The</description></item><item><title>Proof that the Length of an Arc and the Length of a Chord are Approximately Equal When the Central Angle is Small</title><link>https://freshrimpsushi.github.io/en/posts/913/</link><pubDate>Wed, 20 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/913/</guid><description>Theorem When the central angle $\theta$ is sufficiently small, the length of the chord and the length of the arc approximate each other. When $\theta \rightarrow 0$, $$\overline{AB} \approx \stackrel\frown{AB}$$ Proof In the figure above, the length of the chord is $$\overline{AB} =2\overline{AM}=2r\sin \frac{\theta}{2}$$ The length of the arc with a central angle of $\theta$ and the radius length of $r$ is $$\stackrel\frown{AB}=r\theta$$ When the angle is sufficiently small, saying</description></item><item><title>Lorenz Attractor</title><link>https://freshrimpsushi.github.io/en/posts/867/</link><pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/867/</guid><description>Overview The Lorenz Equation is a mathematical model that represents atmospheric convection as a system of differential equations. System $$ \begin{align*} {{dx} \over {dt}} =&amp;amp; - \sigma x + \sigma y \\ {{dy} \over {dt}} =&amp;amp; - xz + \rho x - y \\ {{dz} \over {dt}} =&amp;amp; xy - \beta z \end{align*} $$ Variables $x(t)$: Represents the $x$ coordinate of a particle at time $t$. $y(t)$: Represents the $y$</description></item><item><title>Cross-validation</title><link>https://freshrimpsushi.github.io/en/posts/866/</link><pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/866/</guid><description>Model Validation Data analysis often results in a model that needs to be evaluated to determine if its performance is adequate. If the model only explains the given data well and is totally ineffective in practice, then the analysis is meaningless. For this reason, the entire data set is split into a dataset used for obtaining the model and another for evaluating the performance of the model. Obtaining a model</description></item><item><title>Galois Field</title><link>https://freshrimpsushi.github.io/en/posts/820/</link><pubDate>Sat, 16 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/820/</guid><description>Theorem 1 For a prime number $p$ and a natural number $n$, a finite field with cardinality $p^{n}$ is defined as the Galois Field of order $p^{n}$, denoted as $\text{GF} \left( p^{n} \right)$. Finite fields are exclusively Galois Fields, and for a given $p$ and $n$, the Galois Field is uniquely determined. The uniqueness implies that even if there are different fields, an isomorphism exists, making them essentially the same</description></item><item><title>Limit of Transition Probabilities</title><link>https://freshrimpsushi.github.io/en/posts/863/</link><pubDate>Sat, 16 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/863/</guid><description>Definition When the current state is $i$, after going through $k$ steps to reach $j$, the transition probability is denoted as $p_{ij}^{(k)}$. The transition probability after an infinite number of steps is represented as follows: $$ \pi_{j}:= \lim_{n \to \infty} p_{ij}^{ ( n ) } $$ Description Whether it&amp;rsquo;s statistics or applied mathematics, the main interest usually lies in predicting the future. What&amp;rsquo;s intriguing in the theory of stochastic processes</description></item><item><title>Scalable Divisible Body</title><link>https://freshrimpsushi.github.io/en/posts/890/</link><pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/890/</guid><description>Definition 1 Let&amp;rsquo;s say $E$ is an extension field of $F$. The number of automorphisms from $E$ to a subfield $\overline{F}$, leaving a fixed $F$, is called the index of $E$ over $F$, denoted as $\left\{ E : F \right\}$. If $E$ is a finite field and $\left\{ E : F \right\} = [ E : F ]$, $E$ is called a separable extension field of $F$. If $f (</description></item><item><title>Types of States in Stochastic Processes</title><link>https://freshrimpsushi.github.io/en/posts/861/</link><pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/861/</guid><description>Definition State space is a countable set of stochastic processes $\left\{ X_{t} \right\}$. Let $i,j$ be the states and let $p_{ij}$ be the transition probabilities. If there exists $p_{ij}^{ ( n ) } &amp;gt; 0$ that satisfies $n \ge 0$, then $j$ is said to be accessible from $i$. If $i$ and $j$ are mutually accessible, they are said to communicate. The largest set of states that communicate with each</description></item><item><title>Derivation of the Chapman-Kolmogorov Equation</title><link>https://freshrimpsushi.github.io/en/posts/47/</link><pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/47/</guid><description>Theorem Probability process establishes the following equations for transition probabilities $p_{ij}^{(n)}$, $p_{ij}(t)$ and transition probability matrices $P^{(n)}$, $P(t)$. Discrete Probability Process $$ \begin{align*} p_{ ij }^{ (n+m) } =&amp;amp; \sum _{ k } p_{ ik }^{ (n) } p _{ kj }^{ (m) } \\ P^{(n+m)} =&amp;amp; P^{(n)} P^{(m)} \end{align*} $$ Continuous Probability Process $$ \begin{align*} p_{ij} (t + s) =&amp;amp; \sum _{ k } p_{ ik } \left( t</description></item><item><title>Existence and Uniqueness of Solutions for Initial Value Problems of First-Order Ordinary Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/892/</link><pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/892/</guid><description>Theorem1 Let $E$ be open in $\mathbb{R}^{n}$ and given the following initial value problem concerning $f \in C^{1} (E)$ and $\phi_{0} \in E$. $$ \begin{cases} \dot{ \phi } = \mathbf{f} ( \phi ) \\ \phi (0) = \phi_{0} \end{cases} $$ Then, there exists a unique solution $\phi (t)$ to the given initial value problem in some interval $[-h,h] \subset \mathbb{R}$. $C^{1}$ is a set of functions with continuous derivatives. Proof</description></item><item><title>Rodrigues Formula for Legendre Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/895/</link><pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/895/</guid><description>Formula The explicit formula for the Legendre polynomials is as follows. $$ P_{l}(x)=\dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \tag{1} $$ Description This formula is used to obtain the $l$th Legendre polynomial, known as the Rodrigues&amp;rsquo; formula. Originally, it referred to the explicit form of the Legendre polynomials, but later it became a universal name for formulas representing the explicit form of special functions expressed as polynomials. Derivation The Legendre polynomial $P_{l}$ refers to</description></item><item><title>Discrete Markov Chains</title><link>https://freshrimpsushi.github.io/en/posts/859/</link><pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/859/</guid><description>Definition A discrete-time Markov chain (DTMC), or simply a Markov Chain, is a discrete stochastic process $\left\{ X_{n} \right\}$ satisfying the following, provided the state space is a countable set: $$ p \left( X_{n+1} = j \mid X_{n} = i , X_{n-1} = k , \cdots , X_{0} = l \right) = p \left( X_{n+1} = j \mid X_{n} = i \right) $$ See Also Continuous Markov Chain Description $p_{ij}:=</description></item><item><title>Local Lipschitz Condition</title><link>https://freshrimpsushi.github.io/en/posts/875/</link><pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/875/</guid><description>Definition If $E$ is open in $\mathbb{R}^{n}$ and let&amp;rsquo;s denote it $\mathbf{f} : E \to \mathbb{R}^{n}$. If for all $\mathbf{x} _{0} \in E$, there exists a $\varepsilon &amp;gt; 0$ that satisfies $B \left( \mathbf{x} _{0} ; \varepsilon \right) \subset E$ and for all $\mathbf{x} , \mathbf{y} \in B \left( \mathbf{x} _{0} ; \varepsilon \right)$, there exists a $K &amp;gt;0$ that satisfies $| \mathbf{f} ( \mathbf{x} ) - \mathbf{f} ( \mathbf{y}</description></item><item><title>Picard Method</title><link>https://freshrimpsushi.github.io/en/posts/881/</link><pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/881/</guid><description>Theorem1 If $E$ is open in $\mathbb{R}^{n}$ and the following initial value problem is given for $f \in C^{1} (E)$, $$ \begin{cases} \dot{ \phi } = f ( \phi ) \\ \phi (0) = \phi_{0} \end{cases} $$ let&amp;rsquo;s define the sequence of functions $\left\{ u_{k} (t) \right\} _{ k =0}^{ \infty }$ as follows. $$ \begin{cases} u_{0} (t) = \phi_{0} \\ u_{k+1} (t) = \phi_{0} + \int_{0}^{t} f \left( u_{k}</description></item><item><title>Series Solution of Legendre Differential Equation: Legendre Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/889/</link><pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/889/</guid><description>Definition1 The following differential equation is called the Legendre differential equation. $$ (1-x^2)\dfrac{d^2 y}{dx^2} -2x\dfrac{dy}{dx}+l(l+1) y=0 $$ The solution to the Legendre differential equation is called the Legendre polynomial, commonly denoted as $P_{l}(x)$. The first few Legendre polynomials according to $l$ are as follows. $$ \begin{align*} P_{0}(x) =&amp;amp;\ 1 \\ P_{1}(x) =&amp;amp;\ x \\ P_2(x) =&amp;amp;\ \dfrac{1}{2}(3x^2-1) \\ P_{3}(x) =&amp;amp;\ \dfrac{1}{2}(5x^3-3x) \\ P_{4}(x) =&amp;amp;\ \dfrac{1}{8}(35x^4-30x^2+3) \\ P_{5}(x) =&amp;amp;\ \dfrac{1}{8}(63x^5-70x^3+15x) \\</description></item><item><title>Proof of Leibniz's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/884/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/884/</guid><description>Theorem $$ \dfrac{d}{dx} (fg)=\dfrac{df}{dx}g+f\dfrac{dg}{dx} $$ $$ \begin{align*} \dfrac{d^n}{dx^n}(fg)&amp;amp;=\sum \limits_{k=0}^{n}\frac{n!}{(n-k)!k!}\dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n}{}_{n}\mathrm{C}_{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n} \binom{n}{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \end{align*} $$ Description Also known as Leibniz&amp;rsquo;s rule. The first equation is a well-known formula, often referred to as the product rule or the rule of product for differentiation. It simply expresses the result when the product of two functions is differentiated once. More generally, the equation below represents</description></item><item><title>Series, Infinite Series</title><link>https://freshrimpsushi.github.io/en/posts/886/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/886/</guid><description>Definition1 Let&amp;rsquo;s assume a sequence $\left\{ a_{n} \right\}$ is given. Then, let&amp;rsquo;s define the following notation. $$ \sum \limits_{n=p}^{q} a_{n} = a_{p} + a_{p+1} + \cdots + a_{q}\quad (p \le q) $$ Define the partial sum $s_{n}$ of $\left\{ a_{n} \right\}$ as follows. $$ s_{n} = \sum \limits_{k=1}^{n} a_{k} $$ Then, we can think of a sequence $\left\{ s_{n} \right\}$ of these $s_{n}$. The limit of sequence $\left\{ s_{n} \right\}$</description></item><item><title>What is a Stochastic Process?</title><link>https://freshrimpsushi.github.io/en/posts/857/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/857/</guid><description>Definition The range of the random variable $X: \Omega \to E$ is called the state space. The set of random variables $\left\{ X_{t} \mid t \in [ 0 , \infty ) \right\}$ is called a continuous stochastic process. The sequence of random variables $\left\{ X_{n} \mid n = 0, 1, 2, \cdots \right\}$ is called a discrete stochastic process. Explanation The term &amp;lsquo;process&amp;rsquo; in stochastic process often makes the concept</description></item><item><title>Representing Dynamical Systems and Fixed Points with Maps</title><link>https://freshrimpsushi.github.io/en/posts/856/</link><pubDate>Sat, 09 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/856/</guid><description>Definitions1 A function $f : X \to X$ whose domain and codomain are the same is called a Map. A map that is the composition of $f$ $k$ times is denoted as $f^{k}$. $p \in X$ that satisfies $f(p) = p$ is called a Fixed Point. If there exists a $\epsilon &amp;gt; 0$ that satisfies $\displaystyle \lim_{k \to \infty} f^{k} (x) = p$ for all $x \in N_{ \epsilon }</description></item><item><title>Equations Involving the Laplacian Operator, Second Order Partial Derivatives</title><link>https://freshrimpsushi.github.io/en/posts/876/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/876/</guid><description>Explanation Let&amp;rsquo;s call $T$ a scalar function and $\mathbf{A}$ a vector function. Divergence of the Gradient: $\nabla \cdot (\nabla T) = \dfrac{\partial^{2} T}{\partial x^{2}} + \dfrac{\partial ^{2} T} {\partial y^{2}} + \dfrac{\partial ^{2} T}{\partial z^{2}}$ Curl of the Gradient: $\nabla \times (\nabla T)= \mathbf{0}$ Gradient of the Divergence: $\nabla (\nabla \cdot \mathbf{A} )$ Divergence of the Curl: $\nabla \cdot (\nabla \times \mathbf{A})=0$ Curl of the Curl: $\nabla \times (\nabla \times</description></item><item><title>Fourier Series and Bessel's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/3041/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3041/</guid><description>Formulas If a function $f$ defined on the interval $[-L,L)$ is Riemann integrable, the following inequality holds, known as the Bessel&amp;rsquo;s inequality. $$ \dfrac{1}{4}|a_{0}|^{2} +\dfrac{1}{2}\sum\limits_{n=1}^{\infty} \left(|a_{n}|^{2} + |b_{n}|^{2} \right) =\sum \limits_{n=-\infty}^{\infty} | c_{n} |^{2} \le \dfrac{1}{2L}\int_{-L}^{L} | f(t)|^{2} dt $$ Here, $a_{0},\ a_{n},\ b_{n}$ is the Fourier coefficient of $f$, and $c_{n}$ is the complex Fourier coefficient of $f$. Proof For any complex number $z$, since $|z|^{2}= z \overline{z}$, $$</description></item><item><title>Hosmer-Lemeshow Goodness of Fit Test</title><link>https://freshrimpsushi.github.io/en/posts/852/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/852/</guid><description>Hypothesis Testing Let&amp;rsquo;s refer to the model obtained through logistic regression as $M$. $H_{0}$: $M$ is appropriate. $H_{1}$: $M$ is not appropriate. Description The Hosmer-Lemeshow goodness of fit test is a representative hypothesis test used to determine the adequacy of logistic regression models. Although it&amp;rsquo;s a very simple test, the null hypothesis and the alternative hypothesis can be confusing. While it’s true that there is no good</description></item><item><title>Proof that Differentiating the Heaviside Step Function Yields the Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/878/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/878/</guid><description>Theorem The derivative of the Heaviside step function is the Dirac delta function. $$ \dfrac{dH}{dx}=\delta (x) $$ Here, $H=H(x)$ refers to the Heaviside step function or unit step function $$ H(x)=\begin{cases} 1 &amp;amp; x&amp;gt;0 \\ 0 &amp;amp; x \le 0 \end{cases} $$ Dirac Delta Function A function that satisfies the following two conditions is called the Dirac delta function. $$ \begin{equation} \delta (x) = \begin{cases} 0, &amp;amp; x\neq 0 \\</description></item><item><title>Minimum Splitting Field</title><link>https://freshrimpsushi.github.io/en/posts/851/</link><pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/851/</guid><description>Definitions 1 Let&amp;rsquo;s denote it as $F \le E$. If $f(x) \in F [ x ]$ is factored into linear terms of $E [ x ]$, then $f(x)$ is said to split in $E$. When considering $\left\{ f_{i} (x) \mid i \in I \right\} \subset F [ x ]$, if $E$ contains all zeros of $f_{i} (x)$ and becomes the smallest subfield of $\overline{F}$, then $E$ is called the minimal</description></item><item><title>How to Read Logistic Regression Results in R</title><link>https://freshrimpsushi.github.io/en/posts/850/</link><pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/850/</guid><description>Practice Internal Data Let&amp;rsquo;s load turnout data. turnout is data pertaining to the 1992 U.S. general elections, which can identify the vote (whether voted) based on race, age, education level (educate), and income. Since this data is interested in the dependent variable of whether or not someone voted, logistic regression can be used. Logistic regression, unlike general regression analysis, builds a model through the glm() function. By inserting family=binomial() as</description></item><item><title>Automorphisms of a Body</title><link>https://freshrimpsushi.github.io/en/posts/843/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/843/</guid><description>Definition 1 Let $E$ be an extension field of $F$. An isomorphism $\sigma : E \to E$ of field $E$ to itself is called an automorphism, and the set of all automorphisms of $E$ is denoted as $\text{Auto} (E)$. If $\sigma \in \text{Auto} (E)$ then $\sigma ( a ) = a$, it is said that $\sigma$ fixes $a$. Let $S \subset \text{Auto} (E)$. If for all $a \in F$, all</description></item><item><title>Comparing Elements of Two Arrays in R</title><link>https://freshrimpsushi.github.io/en/posts/842/</link><pubDate>Mon, 04 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/842/</guid><description>Overview R is frequently used in fields more interested in the content of data rather than its form or structure, thus comparing this aspect is also useful. Inclusion (It&amp;rsquo;s not important at all, but in the examples, A represents triangular numbers $\displaystyle {{n(n+1)} \over {2}}$ and B represents square numbers $m^2$.) Using the binary operator %in% to compare two arrays returns true for elements of A that are also in</description></item><item><title>Proof of the Conjugate Isomorphism Theorem</title><link>https://freshrimpsushi.github.io/en/posts/841/</link><pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/841/</guid><description>Definition 1 Let&amp;rsquo;s say $\alpha$ is algebraic over the field $F$. A reduced polynomial for $\alpha$ over $F$, whose highest coefficient is $1$ and satisfies $p( \alpha ) = 0$, is represented as $\text{irr} ( \alpha , F) = p(x)$. The highest degree of $\text{irr} ( \alpha , F)$ is called the degree of $\alpha$ over $F$, and is represented as $\deg ( \alpha , F )$. For the algebraic</description></item><item><title>Solving the Laplace Equation Independent of the Jet Axis in Cylindrical Coordinates Using the Method of Separation of Variables</title><link>https://freshrimpsushi.github.io/en/posts/873/</link><pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/873/</guid><description>Theorem The general solution of the Laplace&amp;rsquo;s equation with cylindrical symmetry in the cylindrical coordinate system is as follows. $$ V(s,\phi) = A_{0} \ln s +B_{0} +\sum \limits _{k=1} ^\infty ( A_{k} s^k ++ B_{k} s^{-k} )( C_{k}\cos k\phi + D_{k}\sin k\phi) $$ Proof Step 0 When boundary conditions are more easily represented in cylindrical coordinates, one has to solve the Laplace&amp;rsquo;s equation for cylindrical coordinates. The Laplace&amp;rsquo;s equation in</description></item><item><title>Changing Column and Row Names in a DataFrame in R</title><link>https://freshrimpsushi.github.io/en/posts/840/</link><pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/840/</guid><description>Overview When writing complex code using data frames in R, there are situations where you need to change the default column names because they can become confusing. names() For example, looking at the data frame above, if no specific mention is made, vague and indistinct column names like V1, V2, V3 are given. By applying the names() function, you can return the column names, and conversely, you can change the</description></item><item><title>Solution of the Laplace Equation Independent of Azimuthal Angle in Spherical Coordinates using the Method of Separation of Variables</title><link>https://freshrimpsushi.github.io/en/posts/872/</link><pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/872/</guid><description>Theorem The general solution of the Laplace&amp;rsquo;s equation with azimuthal symmetry in spherical coordinates is as follows: $$ V(r,\theta) = \sum \limits_{l=0} ^\infty \left( A_{l} r^l + \dfrac{B_{l}}{r^{l+1} } \right) P_{l}(\cos \theta) $$ Proof Step 0 When finding the potential in cases where the boundary condition is easily expressed in spherical coordinates, one must solve the Laplace&amp;rsquo;s equation for spherical coordinates. The Laplace&amp;rsquo;s equation in spherical coordinates is as follows.</description></item><item><title>Euclidean Domain</title><link>https://freshrimpsushi.github.io/en/posts/838/</link><pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/838/</guid><description>Definition 1 In the domain $D$, if there exists a Euclidean Norm $\nu : D \setminus \left\{ 0 \right\} \to \mathbb{N}_{0}$ that satisfies the following two conditions, then $D$ is called a Euclidean Domain. (i): For all $a,b \in D (b \ne 0 )$ $$ a = bq + r $$ There exist $q$ and $r$ that satisfy this. At this time, it must be either $r = 0$ or</description></item><item><title>Logistic Regression</title><link>https://freshrimpsushi.github.io/en/posts/832/</link><pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/832/</guid><description>Buildup Let&amp;rsquo;s think about performing $Y \gets X_{1} , \cdots, X_{p}$. Here, $Y$ can be a categorical variable, particularly one with only two classes, such as male and female, success and failure, positive and negative, $0$ and $1$, etc. For convenience, let&amp;rsquo;s just call it $Y=0$ or $Y=1$. In cases where the dependent variable is binary, the interest is &amp;lsquo;what is $Y$ when we look at independent variables $ X_{1}</description></item><item><title>Solving Nonhomogeneous Euler Differential Equations Using Substitution</title><link>https://freshrimpsushi.github.io/en/posts/871/</link><pubDate>Wed, 30 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/871/</guid><description>Definition The differential equation given as follows is called the Euler differential equation. $$ \begin{align} &amp;amp;&amp;amp; a_2 x^2 \dfrac{d^2 y}{d x^2} + a_{1} x \dfrac{dy}{dx} + a_{0} y &amp;amp;= f(x) \label{eq1} \\ \mathrm{or}&amp;amp;&amp;amp; a_2 x^2 y^{\prime \prime} + a_{1} x y^{\prime} +a_{0} y &amp;amp;= f(x) \nonumber \\ \mathrm{or}&amp;amp;&amp;amp; x^2 y^{\prime \prime} + \alpha x y^{\prime} + \beta y &amp;amp;= f(x) \nonumber \end{align} $$ Explanation It is also referred to as</description></item><item><title>Unique Factorization Domain</title><link>https://freshrimpsushi.github.io/en/posts/827/</link><pubDate>Wed, 30 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/827/</guid><description>Definitions 1 A domain $D$, that is neither $0$ nor a field, in which every element (except for 0 and units) has a unique factorization into irreducible elements is said to be a Unique Factorization Domain. In a Unique Factorization Domain $D$, for $a_{1} , \cdots , a_{n}$ if $d \mid a_{i}$ and every divisor of $a_{i}$ divides $d$, then $d$ is called the Greatest Common Divisor of $a_{1} ,</description></item><item><title>Variable Selection Criteria in Statistical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/826/</link><pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/826/</guid><description>Overview The problem of variable selection inevitably involves the subjectivity of the analyst, but a numerical indicator that helps draw as objective a conclusion as possible was needed. If such values could be calculated, it would provide a clear answer to when to stop the variable selection procedure. However, there are various types of criteria, and applying different criteria can lead to different results. Indicators [^1] Explained Variance $R^2$ The</description></item><item><title>Principal Ideal Domain</title><link>https://freshrimpsushi.github.io/en/posts/825/</link><pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/825/</guid><description>Definition 1 Assume that $D$ of the domain is not a unit. PID If all ideals of $D$ are principal ideals, then $D$ is called a Principal Ideal Domain. Subsidiary Definitions Let the commutative ring $R$ have a unity $1$. If there exists an $c \in R$ that satisfies $b=ac$ for a given $a,b \in R$, then $a$ is said to divide $b$ or $a$ is a factor of $b$,</description></item><item><title>Primes Divisible by 3 with a Remainder of 1: Necessary and Sufficient Conditions</title><link>https://freshrimpsushi.github.io/en/posts/824/</link><pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/824/</guid><description>Theorem $p \ne 3$ is a prime number. $p \equiv 1 \pmod{3}$ $\iff$ for some $a,b \in \mathbb{Z}$ $p = a^2 - ab + b^2$ Description Though $p=3$ is excluded, in fact since $ 3= 2^2 - 2 \cdot 1 + 1^2$, it wouldn&amp;rsquo;t matter much if it was included in the theorem. For example, $13 \equiv 1 \pmod{4}$ is $$ 13 = 1 - 4 + 16 = 1^2</description></item><item><title>Brain Ventricular Enlargement</title><link>https://freshrimpsushi.github.io/en/posts/823/</link><pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/823/</guid><description>Definition $N$ is referred to as a ring. When the ideals of $N$ satisfy $S_{1} \le S_{2} \le \cdots$, it is called an Ascending Chain. For the ascending chain $\left\{ S_{i} \right\}_{i \in \mathbb{N} }$, if there exists $n \in \mathbf{n}$ that satisfies $S_{n} = S_{n+1} = \cdots$, it is said to be Stationary. In other words, in a stationary ascending chain, the ideal does not increase any further from</description></item><item><title>Primes That Leave a Remainder of 1 When Divided by 4</title><link>https://freshrimpsushi.github.io/en/posts/822/</link><pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/822/</guid><description>Theorem Let $p \ne 2$ be a prime number. For some $a,b \in \mathbb{Z}$, $p \equiv 1 \pmod{4}$ $\iff$ $p = a^2 + b^2$ Explanation While excluding $p=2$, in fact, since it is $ 2= 1^2 + 1^2$, it&amp;rsquo;s not a big deal to include it in the theorem. For example, $13 \equiv 1 \pmod{4}$ is $$ 13 = 4 + 9 = 2^2 + 3^2 $$ $37 \equiv 1</description></item><item><title>Variable Selection Procedures in Statistical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/821/</link><pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/821/</guid><description>Buildup Let&amp;rsquo;s consider doing a multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$. Here, if we have $p$ independent variables, it would be ideal if they satisfy the various assumptions of regression analysis well, there is no multicollinearity, and the explanatory power is high. Of course, the more information, the better, but a regression model obtained from too much data also requires a lot of data to use. Therefore,</description></item><item><title>How to Execute a.out after Compiling Fortran in Linux</title><link>https://freshrimpsushi.github.io/en/posts/865/</link><pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/865/</guid><description>Guide To compile a file with the extension .f90, such as example.f90, you need to enter gfortran example.f90 in the console. After the compilation, a file named a.out is created in the same directory. To run the program, you can enter ./a.out in the console to see the program being executed.</description></item><item><title>Integrability is Preserved in the Multiplication of Two Functions</title><link>https://freshrimpsushi.github.io/en/posts/854/</link><pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/854/</guid><description>Theorem1 If two functions $f$ and $g$ are Riemann(-Stieltjes) integrable over the interval $[a,b]$, then $fg$ is also integrable. Proof Assume that $f, g$ is integrable. Since integration is linear, $-g,\ f+g,\ f-g$ is also integrable. Let the function $\phi$ be defined as $\phi (x)=x^2$. Then $\phi$ is continuous over the entire domain. Since integrability is preserved under the composition with continuous functions, $\phi (f+g),\ \phi (f-g)$ is also integrable.</description></item><item><title>How to Perform Principal Component Regression in R</title><link>https://freshrimpsushi.github.io/en/posts/814/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/814/</guid><description>Overview Principal Components Regression (PCR) combines Principal Component Analysis and Multiple Regression Analysis. It involves using the principal components derived from PCA as new independent variables for regression analysis. From a statistical perspective, PCA itself might not be necessary, and its relevance usually comes into play for regression analysis. Practice (Following the method to detect multicollinearity) Although generating principal components involves complex computations including matrix decomposition, in R, this can</description></item><item><title>Integrability is Preserved in the Composition with Continuous Functions</title><link>https://freshrimpsushi.github.io/en/posts/853/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/853/</guid><description>This document is based on the Riemann-Stieltjes Integral. If we set it as $\alpha=\alpha (x)=x$, it equals the Riemann Integral. Theorem1 Suppose that the function $f$ is Riemann(-Stieltjes) integrable on the interval $[a,b]$ and let $m \le f \le M$. Let $\phi$ be a function that is continuous on the interval $[m,M]$. Let the function $h$ be defined as $h=\phi \circ f$. Then, $h$ is Riemann(-Stieltjes) integrable on the interval</description></item><item><title>Solid State Physics</title><link>https://freshrimpsushi.github.io/en/posts/795/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/795/</guid><description>Build-up A ring $R$ is defined to have a characteristic $n$ that is the smallest natural number such that all elements of $R$ satisfy $n \cdot r = 0$. If no such natural number exists, $0$ is defined as the characteristic of $R$. Rings that possess a multiplicative identity, that is, a unity element, exhibit the following properties: [1]: If the characteristic of $R$ with a unity element is $n&amp;gt;1$,</description></item><item><title>Monotone Functions are Riemann-Stieltjes Integrable</title><link>https://freshrimpsushi.github.io/en/posts/849/</link><pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/849/</guid><description>About Riemann Integration Let&amp;rsquo;s suppose that the function $f$ is monotonic over $[a,b]$. Then $f$ is Riemann integrable. Proof Assume that $f$ is a monotonically increasing function1. Let $\epsilon &amp;gt;0$ be given. Consider a partition $P= \left\{ x_{i} : a=x_{0} &amp;lt; x_{1} &amp;lt; x_{2} &amp;lt; \cdots &amp;lt; x_{n}=b \right\}$ of the interval $[a,b]$ that satifies the following for any natural number $n$: $$ \Delta x_{i} = x_{i}-x_{i-1} = \dfrac{b-a}{n},\quad (i=1,2,\dots,n)</description></item><item><title>Proof of the Three Classical Problems of Antiquity</title><link>https://freshrimpsushi.github.io/en/posts/813/</link><pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/813/</guid><description>Theorem 1 The following three constructions are impossible: [1] Squaring the circle: Construct a circle with the same area as a given square. [2] Doubling the cube: Construct a cube with twice the volume of a given cube. [3] Trisecting the angle: Divide a given angle into three equal parts. Disproof It&amp;rsquo;s truly remarkable that these long-standing problems of geometry are solved by algebra. Essentially, the contrapositive of the following</description></item><item><title>Principal Component Analysis in Statistics</title><link>https://freshrimpsushi.github.io/en/posts/812/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/812/</guid><description>Overview Think about performing Multiple Regression Analysis $Y \gets X_{1} , \cdots, X_{p}$. Principal Component Analysis, abbreviated as PCA in English, is, in simple terms, a method of &amp;lsquo;restructuring&amp;rsquo; quantitative variables so that they are properly independent for analysis. From the perspective of multivariate data analysis, it has the significance of &amp;lsquo;dimension reduction&amp;rsquo; as a means to explain phenomena with fewer variables. To properly understand the theoretical derivation of principal</description></item><item><title>Constructible Numbers</title><link>https://freshrimpsushi.github.io/en/posts/811/</link><pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/811/</guid><description>Definition A number that can be obtained by a finite number of operations including addition, subtraction, multiplication, division, and taking square roots, starting with $1$ is said to be Constructible. Explanation The concept of constructibility was originally discussed in the context of ancient Greek geometric arguments, but modern algebra renders the process of drawing circles with compasses and lines with rulers essentially unnecessary. Let&amp;rsquo;s examine how these operations replace construction.</description></item><item><title>Continuous Functions are Riemann-Stieltjes Integrable</title><link>https://freshrimpsushi.github.io/en/posts/847/</link><pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/847/</guid><description>= This article is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem If function $f$ is continuous on $[a,b]$, then it is Riemann(-Stieltjes) integrable on $[a,b]$. Proof Suppose $\epsilon &amp;gt;0$ is given. And let&amp;rsquo;s say we chose $\eta&amp;gt;0$ that satisfies $\left[ \alpha (b) - \alpha (a) \right] \eta &amp;lt; \epsilon$. Since $[a,b]$ is compact as it is closed and</description></item><item><title>Properties of Potential</title><link>https://freshrimpsushi.github.io/en/posts/846/</link><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/846/</guid><description>Reference Point of Potential1 The definition of potential is as follows. $$ V(\mathbf{r} ) \equiv - \int _\mathcal{O} ^{\mathbf{r}} \mathbf{E} \cdot d \mathbf{l} $$ Therefore, depending on the reference point $\mathcal{O}$, its value can vary. For instance, if a new reference point $\mathcal{O}^{\prime}$ is chosen, there will be a difference in value by a certain constant $K$. $$ \begin{align*} V^{\prime} (\mathbf{r} ) =&amp;amp;\ -\int _{\mathcal{O}^{\prime}}^\mathbf{r} \mathbf{E} \cdot d\mathbf{l} \\ =&amp;amp;\</description></item><item><title>Variance Inflation Factor VIF</title><link>https://freshrimpsushi.github.io/en/posts/810/</link><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/810/</guid><description>Definition 1 When performing multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$, let&amp;rsquo;s define the multiple regression coefficient for the $i$th independent variable as $R_{i}^2$. The following is called the Variance Inflation Factor for $X_{i}$. $$\displaystyle \text{VIF}_{i}: = {{1} \over {1 - R_{i}^{2} }}$$ Explanation First, it is recommended to read about multicollinearity. VIF is sometimes translated as the variance expansion index, but it is usually too long, so</description></item><item><title>Potential</title><link>https://freshrimpsushi.github.io/en/posts/845/</link><pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/845/</guid><description>Explanation1 The electric field is a special vector function that always has a curl (rotation) of $\mathbf{0}$. From this characteristic, we introduce a scalar function called electric potential. The potential is denoted as $V$ and has the following relationship with the electric field $\mathbf{E}$. $$ \mathbf{E} = -\nabla V $$ Therefore, if we know the potential $V$, we can know the electric field $\mathbf{E}$. Since the potential is a scalar</description></item><item><title>The Fundamental Theorem of Algebra Expressed in Terms of Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/809/</link><pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/809/</guid><description>Definition 1 Let $E$ be an extension field of field $F$. If all polynomials have zeros in $F$, then $F$ is said to be Algebraically Closed. $\overline{ F_{E}} : = \left\{ \alpha \in E \mid \alpha \text{ is algebraic over } F \right\}$ is called the Algebraic Closure of $F$ in $E$. Theorem [1]: If $F$ is algebraically closed, $\iff$ then every $f(x) \in F [ x ]$ is factored</description></item><item><title>Electric Field Curl</title><link>https://freshrimpsushi.github.io/en/posts/844/</link><pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/844/</guid><description>정리 Electric Field&amp;rsquo;s Curl is always $\mathbf{0}$. $$ \nabla \times \mathbf{E} = \mathbf{0} $$ Proof1 We will derive a general result from the special case where a point charge is located at the origin. The electric field due to a point charge at a distance of $r$ from the origin is as follows. $$ \mathbf{E}=\dfrac{1}{4 \pi \epsilon_{0} } \dfrac{q}{r^2} \hat{\mathbf{r}} $$ If we perform a path integral of the</description></item><item><title>Multicollinearity</title><link>https://freshrimpsushi.github.io/en/posts/808/</link><pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/808/</guid><description>Definition 1 Consider performing multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$. If among the independent variables $ X_{1} , \cdots, X_{p}$ there is a strong correlation between the independent variables, then it is said that there is multicollinearity. Practice Initially, the very idea that independent variables are dependent violates the assumptions of regression analysis and indeed leads to numerical problems that make the analysis results unreliable. It can</description></item><item><title>Proof of Gauss's Law of Quadratic Reciprocity</title><link>https://freshrimpsushi.github.io/en/posts/395/</link><pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/395/</guid><description>Theorem 1 For two different odd primes $p , q$, the following holds: (1): $$ \left( {{ q } \over { p }} \right) = \begin{cases} \left( {{ p } \over { q }} \right) &amp;amp; p \equiv 1 \pmod{4} \lor q \equiv 1 \pmod{4} \\ - \left( {{ p } \over { q }} \right) &amp;amp; p \equiv 3 \pmod{4} \land q \equiv 3 \pmod{4} \end{cases} $$ (2): $$</description></item><item><title>Algebraic Extension</title><link>https://freshrimpsushi.github.io/en/posts/807/</link><pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/807/</guid><description>Definition 1 Let $E$ be an extension of $F$ and call it $n \in \mathbb{N}$. If every element of $E$ is an algebraic number over $F$, then $E$ is called an algebraic extension of $F$. If $E$ is a $n$-dimensional vector space over $F$, then $E$ is called a $n$-dimensional finite extension over $F$. The degree of a finite extension $E$ over $F$ is denoted as $[ E : F</description></item><item><title>Applications of Gauss's Law in Integral Form</title><link>https://freshrimpsushi.github.io/en/posts/640/</link><pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/640/</guid><description>Explanation1 Gauss&amp;rsquo;s Law 1(../635) allows us to calculate the electric field very easily, but not always. Although Gauss&amp;rsquo;s Law itself always holds, its formulaic advantage can only be utilized in certain situations. As will be explained below, to easily calculate the electric field through Gauss&amp;rsquo;s Law, the magnitude of the electric field $\mathbf{E}$ must be constant, and its direction perpendicular, over the surface created by a specific coordinate system. In</description></item><item><title>Divergence of the Electric Field</title><link>https://freshrimpsushi.github.io/en/posts/839/</link><pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/839/</guid><description>Formulas1 The divergence of the electric field $\mathbf{E}$ produced by a volume charge with volume charge density $\rho$ is as follows. $$ \nabla \cdot \mathbf{E} = \dfrac{1}{\epsilon_{0}} \rho ( \mathbf{r} ) $$ Description The divergence of the electric field is also referred to as the differential form of Gauss&amp;rsquo;s law. Integrating both sides yields the integral form of Gauss&amp;rsquo;s law. Proof Electric field produced by the volume charge $$ \mathbf{E}(\mathbf</description></item><item><title>Electric Flux and Gauss's Law</title><link>https://freshrimpsushi.github.io/en/posts/635/</link><pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/635/</guid><description>Definition1 The flux of an electric field $\mathbf{E}$ passing through a surface $\mathcal S$ is defined as follows. $$ \Phi_{E} \equiv \int_{\mathcal S} \mathbf{E} \cdot d\mathbf{a} $$ Let&amp;rsquo;s consider $\mathcal{S}$ as some closed surface. Let the total charge inside the closed surface be $Q_{\text{in}}$. Then, the following equation holds. $$ \oint_{\mathcal{S}} \mathbf{E} \cdot d\mathbf{a} = \frac{1}{\epsilon_{0}}Q_{\mathrm{in}} $$ This is known as Gauss&amp;rsquo;s law. Flux Flux, or flux density, refers to</description></item><item><title>Euler's Criterion</title><link>https://freshrimpsushi.github.io/en/posts/188/</link><pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/188/</guid><description>Theorem 1 Regarding the prime number $p \ne 2$, $$ a^{{p-1} \over {2}} \equiv \left( {a \over p} \right) \pmod{p} $$ Explanation It means that, to see if $a$ is a quadratic residue (QR) or a non-quadratic residue (NQR), one could just calculate blindly. Of course, exponentiation is not always an easy task, but it&amp;rsquo;s better than calculating every number. The proof itself is not too difficult, but because many</description></item><item><title>Divergence of a Separation Vector</title><link>https://freshrimpsushi.github.io/en/posts/837/</link><pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/837/</guid><description>Formula $$ \begin{align*} \nabla \cdot \left( \dfrac{1}{r^2}\hat{ \mathbf{r} } \right) =&amp;amp;\ 4\pi \delta^3(\mathbf{r}) \\[1em] \nabla \cdot \left( \dfrac{1}{\cR^{2}} \crH \right) =&amp;amp;\ 4\pi \delta^3(\bcR) \\[1em] \nabla^2 \left(\dfrac{1}{\cR} \right) =&amp;amp;\ -4\pi \delta^3 ( \bcR ) \end{align*} $$ Here, $\mathbf{r}$ is the position vector, and $\bcR$ is the separation vector. Explanation Let&amp;rsquo;s assume there is a vector function $\mathbf{v} = \dfrac{1}{r^2}\hat{\mathbf{r}}$. Its magnitude is inversely proportional to the square of the distance, and</description></item><item><title>Proof of the Multiplicative Property of the Legendre Symbol</title><link>https://freshrimpsushi.github.io/en/posts/136/</link><pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/136/</guid><description>Definition QR and NR respectively stand for quadratic residues and non-quadratic residues. The Legendre Symbol for a natural number $a$ less than $p$ is defined as $$ \left( { a \over p } \right) = \begin{cases} 1 &amp;amp; a \text{: QR} \\ -1 &amp;amp; a \text{: NR} \end{cases} $$ In number theory, $\displaystyle \left( {{x} \over {y}} \right)$ is not a fraction but referred to as the Legendre Symbol, and</description></item><item><title>Vector Spaces in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/806/</link><pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/806/</guid><description>Definition 1 If all $\alpha , \beta \in F$ and $x, y \in V$ of the set $F$ and Abelian group $V$ satisfy the following conditions, then $V$ is called a vector space over $F$. The elements of $F$ are called scalars, and the elements of $V$ are called vectors. (i): $\alpha x \in V$ (ii): $\alpha ( \beta x) = ( \alpha \beta ) x$ (iii): $\left( \alpha +</description></item><item><title>Coulomb's Law and Electric Fields</title><link>https://freshrimpsushi.github.io/en/posts/836/</link><pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/836/</guid><description>Coulomb&amp;rsquo;s Law1 The force exerted on a test charge $Q$ placed at a distance $\cR$ away from a fixed point charge $q$ is known as the Coulomb force, and its equation is as follows. $$ \mathbf{F} = \dfrac{1}{4\pi \epsilon_{0}} \dfrac{qQ}{\cR ^2} \crH $$ This is referred to as Coulomb&amp;rsquo;s Law. Description Coulomb’s Law is an empirical law derived from repeated experiments. Therefore,</description></item><item><title>Nonlinear Regression Analysis: Variable Transformation in Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/805/</link><pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/805/</guid><description>Overview 1 Regression analysis is essentially a method to elucidate the linear relationships among variables; however, if necessary, the data can be &amp;lsquo;flattened&amp;rsquo; to analyze them linearly. This inherently involves explaining the dependent variable through a nonlinear combination of independent variables. Practice Let&amp;rsquo;s load the built-in data Pressure. Statistical analysis of the Pressure data is in fact unnecessary. This is merely a natural phenomenon which requires mathematical proof just as</description></item><item><title>Quadratic Residues and Non-Quadratic Residues</title><link>https://freshrimpsushi.github.io/en/posts/137/</link><pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/137/</guid><description>Definition 1 For primes $p \ne 2$ and $a &amp;lt; p$, if there exists a solution to the congruence equation $x^{2} \equiv a \pmod{p}$, then $a$ is called a Quadratic Residue QR modulo $p$. If $a$ is not a quadratic residue, it is called a Non-Quadratic Residue NR. Explanation Simply put, a quadratic residue is a number for which a square root exists in $\pmod{p}$. For example, consider the prime</description></item><item><title>Miller-Rabin Primality Test</title><link>https://freshrimpsushi.github.io/en/posts/804/</link><pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/804/</guid><description>Theorem 1 Let&amp;rsquo;s express an odd number $n,q$ as $n-1 = 2^{k} q$. $$ a \nmid n \\ a^{q} \not\equiv 1 \pmod{n} $$ And for all $i = 0, 1, \cdots , (k-1)$, $$ a^{2^{i} q} \not\equiv -1 \pmod{n} $$ if there exists a $ a$ that satisfies the above, then $n$ is a composite number. Explanation With the increased amount of computation, there&amp;rsquo;s a possibility to filter out composite</description></item><item><title>The Fundamental Theorem of Slopes</title><link>https://freshrimpsushi.github.io/en/posts/835/</link><pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/835/</guid><description>Theorem Let&amp;rsquo;s say $T$ is a scalar function in three dimensions. Let&amp;rsquo;s consider $a, b$ as an arbitrary point in three-dimensional space. The total change in $T$ along any path from point $a$ to point $b$ is given by: $$ \begin{equation} T(b)-T(a) = \int _{a}^{b} (\nabla T) \cdot d\mathbf{l} \label{1} \end{equation} $$ This is called the fundamental theorem for gradients or gradient theorem. Note that at Live Shrimp Sushi Restaurant,</description></item><item><title>Algebraic Methods to Construct the Field of Complex Numbers from the Field of Real Numbers</title><link>https://freshrimpsushi.github.io/en/posts/803/</link><pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/803/</guid><description>Theorem 1 $$ \mathbb{R} [x ] / \left&amp;lt; x^2 + 1 \right&amp;gt; \simeq \mathbb{C} $$ Explanation Considering just the facts, it&amp;rsquo;s obvious, and the process of creating the complex field from the real field is quite beautiful. Whether you cut $\mathbb{R} [x ]$ into $\left&amp;lt; x^2 \right&amp;gt;$ or into $\left&amp;lt; x^2 + x \right&amp;gt;$, the shape of the elements will come out in the form of $ax + b$, but</description></item><item><title>Coset Decision Method</title><link>https://freshrimpsushi.github.io/en/posts/802/</link><pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/802/</guid><description>Overview The theorem, which serves as a necessary and sufficient condition for determining if a number is a Carmichael number, can also be usefully applied. Theorem 1 Let an odd composite number be $n$. $n$ is a Carmichael number satisfying $\iff$ $p \mid n$ for all primes $p$ such that $p^2 \nmid n$ and $(p-1) \mid (n-1)$. Proof Definition of Carmichael Numbers: A natural number $n$ is called a Carmichael</description></item><item><title>Necessary and Sufficient Conditions for Riemann(-Stieltjes) Integrability</title><link>https://freshrimpsushi.github.io/en/posts/833/</link><pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/833/</guid><description>This article is based on the Riemann-Stieltjes integral. If we set $\alpha=\alpha (x)=x$, it is the same as Riemann integral. Theorem1 A necessary and sufficient condition for a function $f$ to be Riemann(-Stieltjes) integrable on $[a,b]$ is that for every $\epsilon &amp;gt;0$, there exists a partition $P$ of $[a,b]$ that satisfies $U(P,f,\alpha) - L(P,f,\alpha) &amp;lt; \epsilon$. $$ \begin{equation} f \in \mathscr{R} (\alpha) \text{ on } [a,b] \\ \iff \forall\epsilon &amp;gt;0,</description></item><item><title>Segmentation</title><link>https://freshrimpsushi.github.io/en/posts/830/</link><pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/830/</guid><description>This post is based on the Riemann-Stieltjes integral. If we set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Definition If $P^{\ast}$ and $P$ are partitions of $[a,b]$ and satisfy $P \subseteq P^{\ast}$, then $P^{\ast}$ is called a refinement of $P$. Hence, every point in $P$ is a point in $P^{\ast}$. For any two partitions $P_{1}$ and $P_{2}$, $P_{3}=P_{1} \cup P_{2}$ is called the common refinement of</description></item><item><title>Simple Enlargement Body</title><link>https://freshrimpsushi.github.io/en/posts/801/</link><pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/801/</guid><description>Definition 1 If an extension field $E$ of $F$ satisfies $E = F( \alpha )$ for some $\alpha \in E$, then $E$ is called a Simple Extension of $F$. Explanation Simply put, $F ( \alpha )$ can be seen as an expansion by adding just one $\alpha$ that was not in $F$. Speaking in terms of the field of real numbers $\mathbb{R}$, adding $i \in \mathbb{C}$ to its extension field</description></item><item><title>Upper integral is greater than or equal to lower integral.</title><link>https://freshrimpsushi.github.io/en/posts/831/</link><pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/831/</guid><description>This article is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem1 For any partition, the Riemann(-Stieltjes) upper sum is always greater than or equal to the Riemann(-Stieltjes) lower sum. $$ \underline { \int _{a} ^b} f d\alpha \le \overline {\int _{a}^b} f d\alpha $$ Proof Before proving, let&amp;rsquo;s assume the following: $f : [a,b] \to \mathbb{R}$ is bounded. $\alpha</description></item><item><title>Proof of the Primitive Element Theorem</title><link>https://freshrimpsushi.github.io/en/posts/800/</link><pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/800/</guid><description>Definition 1 If $1 \le a \le p$ satisfies $\text{ord}_{p} (a) = p-1$, it is defined as a primitive root modulo $p$. The order $\text{ord}_{p} (a) $ refers to the smallest natural number $e$ that satisfies $a^{e} \equiv 1 \pmod{p}$. Theorem Every prime number $p$ has $\phi ( p - 1)$ primitive roots. $\phi$ is the totient function. Explanation The reason why $a$ satisfying $\text{ord}_{p} (a) = p-1$ is called</description></item><item><title>Algebraic Numbers and Transcendental Numbers</title><link>https://freshrimpsushi.github.io/en/posts/799/</link><pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/799/</guid><description>Definition 1 Let&amp;rsquo;s call the field extension of field $F$ as $E$. For a non-constant function $f(x) \in F [ x ]$, if it satisfies $f( \alpha ) = 0$ for $\alpha \in E$, it is called algebraic over $F$, and if not, it is called transcendental. When $F = \mathbb{Q}$, $E = \mathbb{C}$, if $\alpha \in \mathbb{C}$ is algebraic, it is called an algebraic number, and if transcendental, a</description></item><item><title>Riemann-Stieltjes Integral</title><link>https://freshrimpsushi.github.io/en/posts/829/</link><pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/829/</guid><description>Overview The Riemann-Stieltjes integral is a generalization of the Riemann integral, sometimes simply referred to as Stieltjes integral. The Riemann integral is a special case of the Riemann-Stieltjes integral where $\alpha (x)=x$. The process of defining the Riemann-Stieltjes integral is the same as the process of defining the Riemann integral, so details on the notation and buildup are omitted here. Definition Let $\alpha : [a,b] \to \mathbb{R}$ be a monotonically</description></item><item><title>Orders in Number Theory</title><link>https://freshrimpsushi.github.io/en/posts/798/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/798/</guid><description>Definition 1 Theorem If $a^{n} \equiv 1 \pmod{p}$, then $\text{ord}_{p} (a) \mid n$. Explanation For example, consider $p=7$. $$ \begin{align*} 1^{1} \equiv &amp;amp; 1 \pmod{ 7 } \\ 2^{3} \equiv &amp;amp; 1 \pmod{ 7 } \\ 3^{6} \equiv &amp;amp; 1 \pmod{ 7 } \\ 4^{3} \equiv &amp;amp; 1 \pmod{ 7 } \\ 5^{6} \equiv &amp;amp; 1 \pmod{ 7 } \\ 6^{2} \equiv &amp;amp; 1 \pmod{ 7 } \end{align*} $$ Here,</description></item><item><title>Definition and Proof of Kronecker's Theorem for Extension Bodies</title><link>https://freshrimpsushi.github.io/en/posts/797/</link><pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/797/</guid><description>Definition of Extension Field 1 For a field $F$, if there exists $E$ such that $F \le E$, then $E$ is called the Extension Field of $F$. Kronecker&amp;rsquo;s Theorem Assuming $f(x) \in F [ x ]$ is not a constant, there exists an extension field $E$ of $F$ and $\alpha \in E$ such that $f ( \alpha ) = 0$. Description An example of an extension field is that $\mathbb{C}$</description></item><item><title>Partition, Riemann Sum, Riemann Integral</title><link>https://freshrimpsushi.github.io/en/posts/828/</link><pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/828/</guid><description>Partition1 Let&amp;rsquo;s assume the interval $[a,b]$ is given. The partition $P$ of $[a,b]$ is defined as follows. $$ P := \left\{ x_{0},\ x_{1},\ \cdots, x_{n}\right\},\quad a=x_{0} &amp;lt;x_{1}&amp;lt;\cdots &amp;lt; x_{n} =b $$ And $\Delta x_{i}$ is defined as follows. $$ \Delta x_{i} :=x_{i}-x_{i-1},\quad i=1,2,\cdots,n $$ Explanation Simply put, a partition is a set that contains all points at the ends of an interval and all boundary points within the interval when</description></item><item><title>Carmichael Numbers</title><link>https://freshrimpsushi.github.io/en/posts/794/</link><pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/794/</guid><description>Definition 1 An integer $n$ is called a Carmichael number if for all $1 \le a \le n$, it satisfies $a^{n} \equiv a \pmod{n}$. Theorem Every Carmichael number is a product of distinct primes, except for $2$. Description Carmichael numbers are composite numbers that pass the Fermat&amp;rsquo;s Little Theorem, meaning they appear to be prime. For example, $561=3 \cdot 11 \cdot 17$ is a composite number, but $a^{561} \equiv a</description></item><item><title>Main Ideals</title><link>https://freshrimpsushi.github.io/en/posts/793/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/793/</guid><description>Definition 1 A principal ideal is generated by an element $a$ with unity in a commutative ring $R$. The identity element for multiplication $1$ is called the unity. Description Although the notation $\left&amp;lt; a \right&amp;gt; := \left\{ r a \mid r\ \in R \right\}$ is similar to that of a cyclic group, it actually forms a slightly larger structure. For example, all ideals $n \mathbb{Z} = \left&amp;lt; n \right&amp;gt; =</description></item><item><title>Roots of Congruence Equations</title><link>https://freshrimpsushi.github.io/en/posts/792/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/792/</guid><description>Algorithm 1 The solution $x$ to the congruence equation $x^{k} \equiv b \pmod{ m }$, given that natural number $b,k,m$ satisfies $\gcd (b , m) = \gcd ( k , \phi (m) ) = 1$, can be calculated as follows: Step 1. Calculate $\phi (m)$. Step 2. Find $u, v$ that satisfies $ku - \phi (m) v =1$, and obtain $x^{ku} \equiv b^{u} \pmod{m}$ by raising both sides to the</description></item><item><title>Covariant Ideals</title><link>https://freshrimpsushi.github.io/en/posts/791/</link><pubDate>Sun, 30 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/791/</guid><description>Definition 1 A prime ideal $P \ne R$ of a commutative ring $R$ is an ideal that, given $a, b \in R$ and $ab \in P$, if $a \in P$ or $b \in P$ then $P$ is said to be a prime ideal in $R$. Explanation As the name prime suggests, it originates from the idea of breaking down the product of elements. For example, considering the ring of integers</description></item><item><title>Laplace Transform of Periodic Functions</title><link>https://freshrimpsushi.github.io/en/posts/773/</link><pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/773/</guid><description>Formulas Let $f$ be a periodic function with period $T$. Then $f(t+T)=f(t)$ and the Laplace transform of $f(t)$ is as follows. $$ \mathcal{L} \left\{ f(t) \right\} = \int_{0}^\infty e^{-st}f(t)dt = \frac{\displaystyle \int_{0}^T e^{-st}f(t)dt}{1-e^{-st}} $$ Derivation From the definition of Laplace transform, split the integral like this. $$ \int_{0}^\infty e^{-st}f(t)dt = \int_{0}^T e^{-st}f(t)dt + \int_{T}^{2T} e^{-st}f(t)dt + \int_{2T}^{3T}e^{-st}f(t)dt + \cdots $$ At this point, to make the integration range of the</description></item><item><title>Laplace Transform of the Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/772/</link><pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/772/</guid><description>Theorem1 The Laplace Transform of the Dirac Delta Function is as follows. $$ \mathcal{L} \left\{ \delta (t-t_{0}) \right\} = e^{-st_{0}} $$ Proof Let&amp;rsquo;s define as shown in the picture above $d_\tau (t) = \dfrac{1}{2\tau}$ $-\tau \le t \le \tau$. Then, the limit below is the same as the Dirac Delta Function. $$ \lim \limits_{\tau \to 0^+}d_\tau (t)=\delta (t) \\ \lim \limits_{\tau \to 0^+}d_\tau (t-t_{0})=\delta (t-t_{0}) $$ Thus $\mathcal{L} \left\{ \delta</description></item><item><title>Maximal Ideal</title><link>https://freshrimpsushi.github.io/en/posts/789/</link><pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/789/</guid><description>Definition 1 An ideal $M \ne R$ of a ring $R$ is called a maximal ideal of $R$ if it is not contained within any other ideal $N \ne R$ of $R$, other than $R$ itself. In other words, $M \subsetneq R$ being a maximal ideal means the following. $$ \nexists N : M \subsetneq N \subsetneq R $$ Explanation In algebra, &amp;lsquo;maximal&amp;rsquo; is almost the same as maximal in</description></item><item><title>Hilbert Spaces are Reflexive: A Proof</title><link>https://freshrimpsushi.github.io/en/posts/788/</link><pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/788/</guid><description>Theorem Hilbert Space $H$ is reflexive: $$ H^{\ast \ast} \approx H $$ $X^{\ast}$ is the dual space of $X$, and $X^{\ast \ast}$ represents the double dual. $X \approx Y$ means that $X$ and $Y$ are isometric. Description Although it is brief and straightforward, the fact that there is no need to consider anything larger than the dual space when studying Hilbert spaces is very good. Proof Part 1. $(H^{ \ast</description></item><item><title>Tri-diagonal Matrix Determinant Derivation</title><link>https://freshrimpsushi.github.io/en/posts/787/</link><pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/787/</guid><description>Formula $$ X_{n} := \begin{bmatrix} x &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\ 1 &amp;amp; x &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; x &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; x &amp;amp; 1 \\ 0 &amp;amp; 0 &amp;amp; 0</description></item><item><title>Proof of Liouville's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/786/</link><pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/786/</guid><description>Theorem[^1] Let $\left( H, \left\langle \cdot,\cdot \right\rangle \right)$ be a Hilbert space. For linear functionals $f \in H^{ \ast }$ and $\mathbf{x} \in H$ satisfying $f ( \mathbf{x} ) = \left\langle \mathbf{x} , \mathbf{w} \right\rangle$ and $\| f \|_{H^{\ast}} = \| \mathbf{w} \|_{H}$, there exists a unique $\mathbf{w} \in H$.</description></item><item><title>Units of an Ideal</title><link>https://freshrimpsushi.github.io/en/posts/785/</link><pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/785/</guid><description>Theorem 1 [1]: A ring $R$ with a unit element $1$ whose ideals $I$ have a unit is $I = R$ [2]: A field $F$ is $\left\{ 0 \right\}$, not having any ideals other than $F$. Explanation Summary [1] is an auxiliary theorem frequently used in proofs by contradiction, stating that having a unit in an ideal makes it whole, and from the point that the unit element is a</description></item><item><title>Orthogonal Decomposition Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/784/</link><pubDate>Sun, 23 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/784/</guid><description>Theorem1 Let $\left( H, \left\langle \cdot,\cdot \right\rangle \right)$ be a Hilbert space. Then for the closed subspace $M$ of $H$, $$ H = M \oplus M^{\perp} $$ Corollary $$ \left( M^{\perp} \right)^{\perp} = M $$ This fact can be demonstrated for $\left( M^{\perp} \right)^{\perp} := \left\{ \mathbf{x} \in H \mid \left\langle \mathbf{x} , \mathbf{m}^{\perp} \right\rangle = 0 , \mathbf{m}^{\perp} \in M^{\perp} \right\}$ as a corollary. Explanation $M^{\perp } := \left\{</description></item><item><title>Hypothesis Testing Through Bayesian Factors</title><link>https://freshrimpsushi.github.io/en/posts/782/</link><pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/782/</guid><description>Buildup To be able to use classical hypothesis testing, one must have a mathematical understanding of concepts such as rejection region, p-value, and even a statistical sense intuitive enough to understand them. It is no surprise that many students, even at the freshman college level, spend hours being taught and still fail to properly understand hypothesis testing. It is similar to how many students learn statistics in high school, find</description></item><item><title>Derivation of the Determinant of the Vandermonde Matrix</title><link>https://freshrimpsushi.github.io/en/posts/736/</link><pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/736/</guid><description>Definition For distinct $1, x_{1} , x_{2 } , \cdots , x_{n}$, the matrix $V_{n}$ defined as follows is called a Vandermonde Matrix. $$ V_{n} := \begin{bmatrix} 1 &amp;amp; x_{1} &amp;amp; x_{1}^{2} &amp;amp; \cdots &amp;amp; x_{1}^{n-1} \\ 1 &amp;amp; x_{2} &amp;amp; x_{2}^{2} &amp;amp; \cdots &amp;amp; x_{2}^{n-1} \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{n} &amp;amp; x_{n}^{2} &amp;amp; \cdots &amp;amp; x_{n}^{n-1} \end{bmatrix} $$ Formula The</description></item><item><title>Proof of Cramer's Rule</title><link>https://freshrimpsushi.github.io/en/posts/783/</link><pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/783/</guid><description>Overview Cramer&amp;rsquo;s Rule is not efficient for solving systems of equations, but if $A_{j}$ is an invertible matrix or $A$ itself is given under conditions that make it convenient to calculate determinants, it can be sufficiently useful to directly find the necessary answers. Theorem Let&amp;rsquo;s assume the system of equations $A \mathbf{x} = \mathbf{b}$ consists of an invertible matrix $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}</description></item><item><title>Relationship between the First and Second Kind Chebyshev Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/780/</link><pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/780/</guid><description>Theorem The first kind Chebyshev polynomials $T_{n} (x) = \cos \left( n \cos^{-1} x \right)$ and second kind Chebyshev polynomials $\displaystyle U_{n} (x) = {{1} \over {n+1} } T_{n+1} &amp;rsquo; (X)$ have the following relationship: [1]: $$U_{n} (x) - U_{n-2} (x) = 2 T_{n} (X)$$ [2]: $$T_{n} (x) - T_{n-2} (x) = 2( x^2 - 1 ) U_{n-2} (x)$$ Typically, for $0 \le \theta \le \pi$, it is set to</description></item><item><title>Second Kind Chebyshev Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/779/</link><pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/779/</guid><description>Definition $$U_{n} (x) := {{1} \over {n+1} } T_{n+1} &amp;rsquo; (x) = {{\sin \left( ( n +1 ) \theta \right)} \over { \sin \theta }} $$ is called the second kind Chebyshev polynomial. Basic Properties Recursive Formula [0]: $$U_{n+1} (x) = 2x U_{n} (x) - U_{n-1} (X)$$ Orthogonal Set [1] Inner product of functions: Given the weight $w$ for $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx$ as $\displaystyle w(x) :=</description></item><item><title>Proof of the Shortest Vector Theorem</title><link>https://freshrimpsushi.github.io/en/posts/778/</link><pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/778/</guid><description>Theorem1 Let $\left( H, \left\langle \cdot,\cdot \right\rangle \right)$ be a Hilbert space. Let $M \lneq H$ be a non-empty closed convex subset. Then, for $\mathbf{x} \in ( H \setminus M)$, $$ \delta := \| \mathbf{x} - \mathbf{m}_{0} \| = \inf_{\mathbf{m} \in M} \| \mathbf{x} - \mathbf{m} \| &amp;gt; 0 $$ there exists a unique $\mathbf{m}_{0} \in M$ that satisfies this. Explanation The fact that subspace $M$ is convex means for</description></item><item><title>First kind Chebyshev polynomials</title><link>https://freshrimpsushi.github.io/en/posts/777/</link><pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/777/</guid><description>Definition 1 $T_{n} (x) = \cos \left( n \cos^{-1} x \right)$ is called the first kind Chebyshev polynomial. Basic Properties Recurrence Formula [0]: $$T_{n+1} (x) = 2x T_{n} (x) - T_{n-1} (X)$$ Orthogonal Set [1] Inner product of functions: Given the weight $w$ as $\displaystyle w(x) := {{1} \over { \sqrt{1 - x^2} }}$, $\left\{ T_{0} , T_{1}, T_{2}, \cdots \right\}$ forms an orthogonal set. Chebyshev Nodes [2]: The roots</description></item><item><title>Hilbert Spaces in Functional Analysis</title><link>https://freshrimpsushi.github.io/en/posts/776/</link><pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/776/</guid><description>Definition1 A Hilbert space is a complete inner product space. It is commonly denoted by $H$ and named after Hilbert. Description A complete space is a space in which every Cauchy sequence converges. Since Banach spaces are also complete spaces, Hilbert spaces can be described as Banach spaces with an inner product. Examples include: Lebesgue spaces $L^{2}$ $\ell^{2}$ spaces Real number space $\mathbb{R}^{n}$ Complex number space $\mathbb{C}^{n}$ Properties Hilbert spaces</description></item><item><title>Reflexive of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/770/</link><pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/770/</guid><description>Definition 1 If $X$ is a vector space and $X^{\ast \ast}$ is its bidual, then $X$ is said to be reflexive if $X^{\ast \ast} \approx X$. Explanation Generally, the size of a vector space increases with each dual taken. However, reflexivity essentially means that the dual space does not continue to grow. Examples of reflexive spaces include: For $1 &amp;lt; p &amp;lt; \infty$, then ${\ell^{p}}^{\ast \ast} \approx \ell^{p}$ If $\dim</description></item><item><title>Highest Posterior Density Credible Interval</title><link>https://freshrimpsushi.github.io/en/posts/769/</link><pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/769/</guid><description>Definition 1 A subset $C \subset \Theta$ of the parameter space $\Theta$ is called the Highest Posterior Density (HPD) Credible Interval for $100(1 - \alpha) % $ given the data $y$ at the significance level $\alpha$ if it satisfies $C : = \left\{ \theta \in \Theta \ | \ p ( \theta | y ) \ge k (\alpha) \right\}$. Here $k(\alpha)$ is the largest constant that satisfies $p(\theta \in C</description></item><item><title>Isometric Mapping</title><link>https://freshrimpsushi.github.io/en/posts/756/</link><pubDate>Tue, 11 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/756/</guid><description>Definitions Given two metric spaces $(X,\ d_X), (Y,\ d_Y)$, if there exists a mapping $f : X \to Y$ that satisfies the conditions below, then $X$ and $Y$ are said to be isometric, denoted by $X \approx Y$. Furthermore, the mapping $f$ is called an isometric map or isometry. $$ d_X(x_1,\ x_2) =d_Y\big( f(x_1),\ f(x_2) \big),\quad \forall\ x_1,x_2\in X $$ Explanation As the name suggests, an isometric map is a</description></item><item><title>The Sign of Complex Numbers</title><link>https://freshrimpsushi.github.io/en/posts/755/</link><pubDate>Mon, 10 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/755/</guid><description>Definition 1 2 The sign of a complex number $\lambda \in \mathbb{C}$ is defined as follows. $$ \operatorname{sign} ( \lambda ) = \begin{cases} \displaystyle {{ \lambda } \over { \left| \lambda \right| }} &amp;amp;, \lambda \ne 0 \\ 0 &amp;amp;, \lambda = 0 \end{cases} $$ Description As an easily checkable example, the sign of real numbers $\operatorname{sign} ( +2 ) = 1$, $\operatorname{sign} ( -3 ) = -1$ can be</description></item><item><title>Dual Space</title><link>https://freshrimpsushi.github.io/en/posts/753/</link><pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/753/</guid><description>Dual Spaces Definition 11 The set of all continuous linear functionals of a vector space $X$ is denoted by $X^{ \ast }$ and is called the dual space of $X$, simply referred to as the dual of $X$, as denoted below. $$ X^{ \ast }:=\left\{ x^{ \ast }:X\to \mathbb{C}\ |\ x^{ \ast } \text{ is continuous and linear} \right\} $$ $$ X^{ \ast }:=B(X,\mathbb{C}) $$ $B \left( X, \mathbb{C} \right)$</description></item><item><title>Differences between Credit Intervals and Confidence Intervals</title><link>https://freshrimpsushi.github.io/en/posts/752/</link><pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/752/</guid><description>Theorem The difference between a credence interval and a confidence interval can indeed be considered the difference between Bayesian and frequentist approaches. Confidence Interval (Frequentist): The parameter is a fixed constant and the confidence interval is randomly determined. Credence Interval (Bayesian): The parameter is a variable with a distribution, and the credence interval is also determined from the posterior distribution. Confidence Interval In classical statistics, what a confidence interval $[a</description></item><item><title>Necessary and Sufficient Conditions for Linear Functionals to be Represented by Linearly Independent Combinations</title><link>https://freshrimpsushi.github.io/en/posts/748/</link><pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/748/</guid><description>Theorem Let $f, f_{1} , \cdots , f_{n}$ be a linear functional with domain $X$. (a) For $c_{1} , \cdots , c_{n} \in \mathbb{C}$, $\displaystyle f = \sum_{i=1}^{n} c_{i} f_{i}$ $\iff$ $\displaystyle \bigcap_{i=1}^{n} \ker ( f_{i} ) \subset \ker (f)$ (b) There exists $x_{1} , \cdots , x_{n}$ satisfying $f_{j} (x_{i} ) = \delta_{ij}$ with $f_{1} , \cdots , f_{n}$ being linearly independent. Here, $\delta_{ij}$ is the Kronecker delta. Explanation</description></item><item><title>Influence of Interaction in Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/696/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/696/</guid><description>Buildup It is recommended to read about regression analysis including qualitative variables first. Imagine guessing this year&amp;rsquo;s graduates&amp;rsquo; starting salaries based on their college entrance exam scores $X_{1}$, age $X_{2}$, gender $S$, and final educational attainment $E$. Firstly, with the presence of qualitative variables, gender is defined as $$ S = \begin{cases} 1 &amp;amp; ,\text{여성} \\ 0 &amp;amp; ,\t</description></item><item><title>Radicals and Nilradicals in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/744/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/744/</guid><description>Definition 1 Let $N$ be an ideal of $R$. $\text{rad} N := \left\{ a \in R \ | \ a^n \in N \right\}$ is called the radical of $N$. If there exists a $n \in \mathbb{N}$ that satisfies $a^{n} = 0$, then $a$ is called nilpotent. The set of nilpotent elements $\text{nil} R := \left\{ a \in R \ | \ a^n = 0 \right\}$ is called the nilradical of</description></item><item><title>Necessary and Sufficient Conditions for Linear Functionals to be Continuous</title><link>https://freshrimpsushi.github.io/en/posts/742/</link><pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/742/</guid><description>Theorem1 The linear functional $f$ is continuous. $\iff$ $\ker(f)$ is a closed set in $X$. Here, $\mathcal{N} (f) = \ker (f) = \left\{ x \in X \ | \ f(x) = 0 \right\}$ is the kernel of the linear transformation $f$. Proof Strategy: $(\implies)$ Direct deduction by the definition of the kernel. $(\impliedby)$ The necessary and sufficient condition for the continuity of a linear operator is boundedness. Showing that $f$</description></item><item><title>Regression Analysis Including Qualitative Variables</title><link>https://freshrimpsushi.github.io/en/posts/686/</link><pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/686/</guid><description>Overview Regression analysis does not always guarantee that quantitative variables are used as independent variables. There is also a need to reflect categorical data in the analysis, such as what gender someone is, which company they belong to, what color something is, whether it&amp;rsquo;s a metal, etc. Build-up 1 Imagine guessing the starting salary $Y$ with the nationwide exam score $X_{1}$, age $X_{2}$, gender $S$, and the highest education level</description></item><item><title>Checking the Normality of Residuals through Model Diagnostics</title><link>https://freshrimpsushi.github.io/en/posts/683/</link><pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/683/</guid><description>Diagnostics To determine if regression analysis was performed correctly, you can check using the standardized residual plot. Normality is better assessed using histograms rather than looking at the scatter of residuals, or by conducting a normality test. The left side shows a density that decreases towards the top and bottom from the center, whereas the right side is evenly distributed regardless of the direction. However, cases where the residuals actually</description></item><item><title>Confidence Intervals</title><link>https://freshrimpsushi.github.io/en/posts/741/</link><pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/741/</guid><description>Definition 1 When a subset $C \subset \Theta$ of the parameter space $\Theta$ satisfies $P ( \theta \in C | y ) \ge 1 - \alpha$ for a significance level $\alpha$, $C$ is called the Credible Interval for $\theta$ given data $y$. Explanation Interval estimation in Bayesian statistics is about finding intervals that are highly probable to contain the parameter $\theta$. The &amp;lsquo;Credible Interval&amp;rsquo; found in this manner corresponds to</description></item><item><title>Residual Independence Verified through Model Diagnosis</title><link>https://freshrimpsushi.github.io/en/posts/679/</link><pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/679/</guid><description>Diagnostic Methods Intuitive Pattern Recognition Using standardized residual plots, we can check if the regression analysis was performed correctly. To confirm independence, there should be no distinct patterns appearing in the residual plots. Unfortunately, diagnosing independence can be very subjective compared to other assumptions of regression analysis. A common example of lacking independence is seeing an unidentified straight line as shown above. It could be by chance, but usually, it</description></item><item><title>Three Representative Values of Statistics: Mode, Median, Average</title><link>https://freshrimpsushi.github.io/en/posts/740/</link><pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/740/</guid><description>Overview Measures of central tendency are statistics that summarize the data by identifying the central position within a data set. Even when dealing with thousands or millions of data points, it&amp;rsquo;s often not practical or necessary to examine each one individually. Instead, what&amp;rsquo;s important is understanding what the data represents, and measures of central tendency effectively condense this information. The three most commonly used measures of central tendency are mode,</description></item><item><title>Ideals in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/739/</link><pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/739/</guid><description>Definition 1 A subring $(I, +)$ that satisfies $a I \subset I$ and $I b \subset I$ for all $a,b \in R$ in a ring $(R , + , \cdot )$ is called an Ideal. Explanation As a simple example, $n \mathbb{Z}$ is an Ideal of $\mathbb{Z}$. The name Ideal literally comes from the word Ideal, as it is the perfect subring to deal with in abstract algebra. Especially if</description></item><item><title>Bounded Linear Operators Squared Norm</title><link>https://freshrimpsushi.github.io/en/posts/738/</link><pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/738/</guid><description>Terminology 1 Describing platform-independent and deeply technical code using natural language is referred to as pseudo code. Description Pseudo code is a representation that appears like code but is not actual code. It is used to illustrate algorithms without being constrained by any specific programming language. Pseudo code can utilize natural language as well as mathematical expressions, and as long as its definition is clear and not confusing, it can</description></item><item><title>Eisenstein's Criterion</title><link>https://freshrimpsushi.github.io/en/posts/737/</link><pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/737/</guid><description>Theorem 1 $f(x) = a_{n} x^{n} + \cdots + a_{0 } \in \mathbb{Z} [ x ]$, if it satisfies the following conditions for the primes $p \in \mathbb{Z}$ and $k = 0,1,2, \cdots , n-1$, then $f(x)$ is irreducible over $\mathbb{Q}$. (i): $a_{n} \not\equiv 0 \pmod{p}$ (ii): $a_{k} \equiv 0 \pmod{p} $ (iii): $a_{0} \not\equiv 0 \pmod{p^2}$ Description It has significance as a very simple judgment method for integer polynomials</description></item><item><title>Laplace Expansion</title><link>https://freshrimpsushi.github.io/en/posts/781/</link><pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/781/</guid><description>정리 A square matrix $A_{n \times n} = (a_{ij})$ is given. [1]: For the selected $i$ row $$ \det A = \sum_{j=1}^{n} a_{ij} C_{ij} $$ [2]: For the selected $j$ column $$ \det A = \sum_{i=1}^{n} a_{ij} C_{ij} $$ The determinant $M_{ij}$ of the matrix obtained by removing the $i$th row and $j$th column from the square matrix $A_{n \times n} = (a_{ij})$ is called a minor, and $C_{ij}</description></item><item><title>Irreducible Elements of Polynomial Functions</title><link>https://freshrimpsushi.github.io/en/posts/735/</link><pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/735/</guid><description>Definition 1 A non-constant function $f(x) \in F [ x ]$ is said to be an Irreducible Element on $F$ if it cannot be represented as a product $f(x) = g(x) h(x)$ of some $g(x) , h(x) \in F [ x ]$ with lower degree than $f(x)$. Explanation For example, consider $\mathbb{Q} [x ]$. Here, $x^2 - 2$ is an irreducible element over $\mathbb{Q}$, but in $\mathbb{R} [ x ]$,</description></item><item><title>Proof of the Factor Theorem</title><link>https://freshrimpsushi.github.io/en/posts/733/</link><pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/733/</guid><description>Theorem 1 Let&amp;rsquo;s say $f(x) \in F [ x ]$. $$ f(a) = 0 \iff f(x) = (x-a) q(x) $$ Explanation This is a theorem that guarantees the existence of factorization, which we have repeatedly done since middle school. It&amp;rsquo;s important to note that facts such as the Division Theorem or Factor Theorem only have meaning when the degree of a polynomial is finite. Proof $( \implies )$ Division Theorem:</description></item><item><title>Division Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/729/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/729/</guid><description>Theorem 1 Given $a_{n} \ne 0$ and $b_{m} \ne 0$, as well as $n &amp;gt; m &amp;gt; 0$, let the two elements of $F [ x ]$ be $$ f(x) = a_{n} x^{n} + \cdots + a_{1} x + a_{0} \\ g(x) = b_{m} x^{m} + \cdots + b_{1} x + b_{0} $$. Then, there exists a unique $q(x), r(x) \in F [ x ]$ that satisfies $f(x) = g(x)</description></item><item><title>Properties of Linear Operators</title><link>https://freshrimpsushi.github.io/en/posts/730/</link><pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/730/</guid><description>Theorem 1 $T : (X , \left\| \cdot \right\|_{X}) \to ( Y , \left\| \cdot \right\|_{Y} )$ is called a linear operator. (a) If $T$ is bounded, for all $x \in X$, $\left\| T(x) \right\|_{Y} \le \left\| T \right\| \left\| x \right\|_{X}$ (b) $T$ is continuous $\iff$ $T$ is bounded (c) If $X$ is a finite-dimensional space, then $T$ is continuous. (d) If $Y$ is a Banach space, then $(</description></item><item><title>Zeros of a Polynomial Function</title><link>https://freshrimpsushi.github.io/en/posts/723/</link><pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/723/</guid><description>Definition 1 $$ f(x) : = \sum_{k=0}^{n} a_{k} x^{k} = a_{0} + a_{1} x + \cdots + a_{n} x^{n} $$ Let us define the evaluation function $\phi_{\alpha} : F [ x ] \to E$ in $\alpha \in E$ for a polynomial function $f \in F [x]$ and a field $F \le E$ as follows. $$ \phi_{\alpha} ( f(x) ) : = a_{0} + a_{1} \alpha + \cdots + a_{n} \alpha^n</description></item><item><title>Operators in Functional Analysis</title><link>https://freshrimpsushi.github.io/en/posts/728/</link><pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/728/</guid><description>Definitions1 Let&amp;rsquo;s call $(X, \left\| \cdot \right\|_{X}), (Y, \left\| \cdot \right\|_{Y})$ a normed space. A mapping from a normed space to another normed space is called an operator. For $x,x_{1},x_{2}\in X$, if $T : X \to Y$ satisfies $$ T( x_{1} + x_{2} ) = T( x_{1} ) + T( x_{2} ) \quad \text{and} \quad T( a x ) = a T( x ) $$, it is called a linear</description></item><item><title>How to Move the Big O Notation from Denominator to Numerator</title><link>https://freshrimpsushi.github.io/en/posts/727/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/727/</guid><description>Theorem The following holds for $a \ne 0$, $p&amp;gt;0$, and $n \in \mathbb{N}$. $$ {{1} \over { \sqrt[p]{a + O ( h^n ) } }} = {{1} \over { \sqrt[p]{a } }}+ O(h^n) $$ Explanation It serves as a handy lemma for converting complex denominators into cleaner forms. If there were no constant term $a$, it could neatly rise to $\displaystyle {{1} \over { \sqrt[p]{ O ( h^n ) }</description></item><item><title>Lefschetz Fixed Point Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/725/</link><pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/725/</guid><description>Theorem 1 Let&amp;rsquo;s denote the scalar field of norm space $(X , \left\| \cdot \right\|)$ as $\mathbb{C}$. Then $X$ is finite-dimensional. $\iff$ $\overline{ B ( 0 ; 1 ) }$ is compact. Explanation $\overline{ B ( 0 ; 1 ) } := \left\{ x \in X : \| x \| \le 1 \right\}$ denotes the closed unit ball. According to the Riesz&amp;rsquo;s theorem, to determine whether the entire space is</description></item><item><title>Laplace Transform of t^{n}f(t)</title><link>https://freshrimpsushi.github.io/en/posts/771/</link><pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/771/</guid><description>Formulas Let&amp;rsquo;s say the Laplace transform of the function $f(t)$ is $\mathcal{L} \left\{ f(t) \right\} = \displaystyle \int _{0} ^\infty e^{-st}f(t)dt = F(s)$. Then, the Laplace transform of $t^{n}f(t)$ is as follows. $$ \mathcal{L} \left\{ t^n f(t) \right\} = (-1)^nF^{(n)}(s) $$ Derivation First, the Laplace transform of $t^nf(t)$, by definition, is as follows. $$ \int _{0} ^\infty e^{-st}tf(t) dt $$ If we look closely at the integral, it can be</description></item><item><title>Polynomial Rings</title><link>https://freshrimpsushi.github.io/en/posts/721/</link><pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/721/</guid><description>Definition 1 $$ f(x) : = \sum_{k=0}^{n} a_{k} x^{k} = a_{0} + a_{1} x + \cdots + a_{n} x^{n} $$ A polynomial $f(x)$ over a ring $R$ is defined as above. $a_{i} \in R$ are called the coefficients of $f(x)$. If $n &amp;lt; \infty$, then $n$ is called the degree of $f(x)$. $R[x]$ is the set of all polynomials with coefficients in $R$. $$ R[x] := \left\{ a_{0} + a_{1}</description></item><item><title>Numerical Analysis in Differences</title><link>https://freshrimpsushi.github.io/en/posts/722/</link><pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/722/</guid><description>Definition 1 Forward Difference: $$ \begin{align*} \Delta f(x) =&amp;amp; f(x+h) - f(x) \\ \Delta^{r+1} f(x) =&amp;amp; \Delta^{r} f(x+h) - \Delta^{r} f(x) \end{align*} $$ Backward Difference: $$ \begin{align*} \nabla f(x) =&amp;amp; f(x) - f(x- h) \\ \nabla^{r+1} f(x) =&amp;amp; \nabla^{r} f(x) - \nabla^{r} f(x- h) \end{align*} $$ Description Generally, the term Difference is used throughout sequences, but in numerical analysis, it specifically refers to the difference between the function values of</description></item><item><title>If the Unit of a Ring is Idempotent, It Can Be Expressed as a Direct Sum</title><link>https://freshrimpsushi.github.io/en/posts/731/</link><pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/731/</guid><description>Theorem Given a ring $R$ with a unity element $1$, if its zero divisors $a$ satisfy $a^2 = a$, that is, if it&amp;rsquo;s an idempotent, then $R$ is uniquely represented as the direct product of $aR$ and $(1-a)R$. $$ R = a R \times (1-a)R $$ Explanation This theorem is charmingly mathematical enough that one can understand it without defining the direct sum of rings separately. Proof Part (i). Existence</description></item><item><title>Unpacking Data Structures in R</title><link>https://freshrimpsushi.github.io/en/posts/720/</link><pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/720/</guid><description>Overview When using various functions in R, you will often see helpful results printed out as shown below. The problem arises when you want to take these results as output to use them further rather than just viewing them. Example For instance, if you need the maximum value of the residuals as shown in the screenshot above, you could just manually read and use the value 15.9719. However, if you</description></item><item><title>Reflection and Refraction</title><link>https://freshrimpsushi.github.io/en/posts/719/</link><pubDate>Sun, 11 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/719/</guid><description>Definitions 1 In a ring $R$, a $a,b \in R$ that is not $0$ and satisfies $ab = 0$ is called a Zero Divisor. A $D$ with a unit $1 \ne 0$ and without zero divisors is called an Integral Domain. Description Zero Divisors Examples of non-$0$ elements whose product is $0$ include $$ \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; 0 \end{bmatrix} \begin{bmatrix} 0 &amp;amp; 0 \\ 0 &amp;amp;</description></item><item><title>Inverse Laplace Transform of F(as+b)</title><link>https://freshrimpsushi.github.io/en/posts/767/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/767/</guid><description>공식1 Assuming that the Laplace transform $\mathcal{L} \left\{ f(t) \right\}= \displaystyle \int _{0} ^\infty e^{-st}f(t)dt =F(s)$ of function $f(t)$ exists as $s&amp;gt;\alpha \ge 0$, the inverse Laplace transform of $F(as+b)$ for constant $a&amp;gt;0 , b$ is as follows. $$ \mathcal{L^{-1}} \left\{ F(as+b) \right\} =\frac{1}{a}e^{-\frac{b}{a}t}f\left(\frac{t}{a}\right) $$ Derivation 1 Inverse Laplace transform of $F(ks)$: $$ \mathcal{L^{-1}} \left\{ F(ks) \right\} =\dfrac{1}{k}f\left(\frac{t}{k}\right) $$ Translation of Laplace transform: $$ \mathcal{L^{-1}} \left\{ F(s-c) \right\}=e^{ct}f(t)</description></item><item><title>Inverse Laplace Transform of F(ks)</title><link>https://freshrimpsushi.github.io/en/posts/766/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/766/</guid><description>Formulas1 Assuming that the Laplace transform $\mathcal{L} \left\{ f(t) \right\} = \displaystyle \int _{0} ^\infty e^{-st}f(t)dt = F(s)$ of the function $f(t)$ exists and is $s&amp;gt;a \ge 0$ for a positive number $k&amp;gt; 0$ then the inverse Laplace transform of $F(ks)$ is as follows. $$ \mathcal{L^{-1}} \left\{ F(ks) \right\} =\dfrac{1}{k}f\left(\frac{t}{k}\right),\quad s&amp;gt;\frac{a}{k} $$ Derivation 1 The Laplace transform of $f(ct)$ $$ \mathcal{L} \left\{ f(ct) \right\} =\dfrac{1}{c}F\left(\dfrac{s}{c}\right), \quad s&amp;gt;ca $$ By substituting</description></item><item><title>Laplace Transform of f(ct)</title><link>https://freshrimpsushi.github.io/en/posts/765/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/765/</guid><description>Formulas1 Let&amp;rsquo;s assume that the Laplace transform $\mathcal{L} \left\{ f(t) \right\} = \displaystyle \int _{0} ^\infty e^{-st}f(t)dt = F(s)$ of the function $f(t)$ exists and is $s&amp;gt;a \ge 0$. Then, for $c &amp;gt;0$, the Laplace transform of $f(ct)$ is as follows. $$ \mathcal{L} \left\{ f(ct) \right\} =\dfrac{1}{c}F\left(\dfrac{s}{c}\right), \quad s&amp;gt;ca $$ Derivation $$ \mathcal{L} \left\{ f(ct) \right\} = \int _{0} ^\infty e^{-st}f(ct)dt $$ Let&amp;rsquo;s substitute $ct=\tau$. Then, since $st=\dfrac{s}{c}\tau$ and $dt=\dfrac{1}{c}d\tau$,</description></item><item><title>Laplace Transform of the First Order Derivative</title><link>https://freshrimpsushi.github.io/en/posts/760/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/760/</guid><description>Theorem1 Let&amp;rsquo;s assume the following two conditions. Let function $f(t)$ be continuous on interval $0 \le t \le A$, and let its first derivative $f^{\prime}(t)$ be piecewise continuous. There exists real numbers $a$ and positive numbers $K$, $M$ such that when $t \ge M$, it satisfies $|f(t)| \le Ke^{at}$. Then, the first derivative of $f$&amp;rsquo;s Laplace transform $\mathcal{L} \left\{ f^{\prime}(t) \right\}$ exists when $s&amp;gt;a$ and its value is as follows.</description></item><item><title>Laplace Transform Translation</title><link>https://freshrimpsushi.github.io/en/posts/764/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/764/</guid><description>Formula1 Assuming the Laplace transform $F(s)=\mathcal{L} \left\{ f(t) \right\}$ of the function $f(t)$ exists as $s&amp;gt;a$. Then, the following holds for constant $c$. $$ \begin{align*} \mathcal{L} \left\{ e^{ct}f(t) \right\}&amp;amp;=F(s-c), &amp;amp;s&amp;gt;a+c \\ \mathcal{L^{-1}} \left\{ F(s-c) \right\}&amp;amp;=e^{ct}f(t) &amp;amp; \end{align*} $$ Explanation This means that multiplying an exponential function to $f$ is equivalent to translating $F$. Derivation $$ \begin{align*} \mathcal{L} \left\{ e^{ct}f(t) \right\} &amp;amp;=\int_{0}^\infty e^{-st}e^{ct}f(t)dt \\ &amp;amp;= \int_{0}^\infty e^{-(s-c)t}f(t)dt \\ &amp;amp;= F(s-c) \end{align*}</description></item><item><title>Solving Second-Order Linear Nonhomogeneous Differential Equations Using Laplace Transforms</title><link>https://freshrimpsushi.github.io/en/posts/763/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/763/</guid><description>Theorem1 $$ ay^{\prime \prime} + by^{\prime} + cy = g(t) $$ Let us assume that the above second-order linear inhomogeneous differential equation is given. And let&amp;rsquo;s say $\mathcal{L} \left\{ y \right\} =Y(s)$, $\mathcal{L} \left\{ g(t) \right\}=G(s)$. Then, $$ Y(s) = \dfrac{ (as + b)y(0) + ay^{\prime}(0) } {as^2+bs+c} + \dfrac{G(s) }{as^2+bs+c} $$ Explanation The above formula is easy to memorize if you remember the rules well. If you memorize according</description></item><item><title>The Laplace Transform of the n-th Order Derivative</title><link>https://freshrimpsushi.github.io/en/posts/762/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/762/</guid><description>Theorem1 Assuming the following two conditions: For any interval $0 \le t \le A$, let functions $f$, $f^{\prime}$, $\cdots$, $f^{(n-1)}$ be continuous and let the n-th derivative $f^{(n)}(t)$ be piecewise continuous. When $t \ge M$, there exist real numbers $a$ and positives $K$, $M$ satisfying $|f(t)| \le Ke^{at}$, $|f^{\prime}(t)| \le Ke^{at}$, $\cdots$, and $|f^{(n-1)}(t)| \le Ke^{at}$. Then, the Laplace transform of the n-th derivative of $f$, $\mathcal{L} \left\{ f^{(n)}(t) \right\}$,</description></item><item><title>Boolean Ring</title><link>https://freshrimpsushi.github.io/en/posts/717/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/717/</guid><description>Definition 1 Let&amp;rsquo;s call $R$ a ring. If $r \in R$ satisfies $r^2 = r$, then $r$ is called an Idempotent Element. If all elements of $R$ are idempotent, $R$ is called a Boolean Ring. Explanation Although &amp;lsquo;Boolean ring&amp;rsquo; could be translated phonetically in Korean, the term sounds awkward, hence the English pronunciation was used directly. The property of projection in linear algebra is known to be very useful, needless</description></item><item><title>Definition and Existence Proof of the Laplace Transform</title><link>https://freshrimpsushi.github.io/en/posts/761/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/761/</guid><description>Definition[^1] The Laplace transform of a function $f$ is defined as follows. $$ \mathcal{L} \left\{ f(t) \right\} := \int _{0}^\infty e^{-st}f(t) dt =F(s) $$</description></item><item><title>Laplace Transform of the Step Function</title><link>https://freshrimpsushi.github.io/en/posts/758/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/758/</guid><description>Definition1 Let&amp;rsquo;s denote the unit step function unit step function shifted by $c$ as follows: $$ u_{c}(t)=\begin{cases} 0 &amp;amp; t&amp;lt;c \\ 1 &amp;amp; t \ge c \end{cases} $$ Formula The Laplace transform of the step function $u_{c}(t)$ is as follows. $$ \begin{equation} \mathcal{L} \left\{ u_{c}(t) \right\} = \dfrac{e^{-cs}}{s},\quad s&amp;gt;0 \label{eq1} \end{equation} $$ Let&amp;rsquo;s assume that $c$ is an arbitrary constant, and when $s &amp;gt; a \ge 0$, the Laplace transform</description></item><item><title>Staircase Function</title><link>https://freshrimpsushi.github.io/en/posts/757/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/757/</guid><description>Definition A function that is a piecewise constant function is called a step function. Description As shown in the figure above, it looks like a staircase, hence the name step function. It is also known as the Heaviside function, named after Heaviside, who is known to be the first to propose it. Heaviside was the person who created a method for solving differential equations in electrical circuits, which is the</description></item><item><title>Jeffreys Prior Distribution</title><link>https://freshrimpsushi.github.io/en/posts/716/</link><pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/716/</guid><description>Definitions 1 The distribution $p( y | \theta)$ is called the Jeffreys prior for $\pi ( \theta ) \propto I^{1/2} ( \theta )$. $I$ refers to the Fisher information. $$ I ( \theta ) = E \left[ \left( \left. {{\partial \ln p (y | \theta) } \over {\partial \theta}} \right)^2 \right| \theta \right] = E \left[ \left. - {{\partial^2 \ln p (y | \theta) } \over { (\partial \theta )^2</description></item><item><title>Field Theory in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/715/</link><pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/715/</guid><description>Definition 1 If a ring $(R , + , \cdot)$ has an identity element $1 \in R$ for multiplication $\cdot$, $1$ is called a unity. In a ring $R$ with a unity, the element $r \ne 0$ that has a multiplicative inverse is called a unit. If every element other than $0$ is a unit in a ring $R$ with a unity, it is called a division ring. A division</description></item><item><title>Laplace Prior Distribution</title><link>https://freshrimpsushi.github.io/en/posts/714/</link><pubDate>Tue, 06 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/714/</guid><description>Buildup If there&amp;rsquo;s almost no information about the parameter, there&amp;rsquo;s no reason to consider a complex prior distribution: Example 1: If someone with a fair understanding of statistics is asked to guess the gender ratio of incoming freshmen for a certain university&amp;rsquo;s statistics department next year, they might make a reasonable guess based on the gender ratios in previous years. However, if someone with no relation or interest in the</description></item><item><title>Minkowski Inequality</title><link>https://freshrimpsushi.github.io/en/posts/288/</link><pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/288/</guid><description>Theorem For two vectors $\mathbf{x}= (x_{1} , x_{2} , \dots , x_{n} )$, $\mathbf{y} = (y_{1} , y_{2} , \dots , y_{n} )$ and a real number $1$ larger than $p$, the following equation holds. $$ \left( \sum_{k=1}^{n} | x_{k} + y_{k} |^{p} \right)^{{1} \over {p}} \le \left( \sum_{k=1}^{n} |x_{k}|^{p} \right)^{{1} \over {p}} + \left( \sum_{k=1}^{n} |y_{k}|^{p} \right)^{{1} \over {p}} $$ This is called the Minkowski&amp;rsquo;s inequality. Description Minkowski&amp;rsquo;s inequality</description></item><item><title>Proof of Lissajous's Auxiliary Lemma</title><link>https://freshrimpsushi.github.io/en/posts/713/</link><pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/713/</guid><description>Theorem 1 In the normed space $(X , \| \cdot \| )$, let&amp;rsquo;s say $Y$ is a closed set for the subspace $Y \subsetneq X$. For all $\theta \in (0,1)$ and $y \in Y$, there exists $x_{\theta} \in X$ that satisfies $\| x_{ \theta } \| = 1$ and $\| x_{ \theta } - y \| &amp;gt; \theta$. Proof Strategy: Show that a concrete $x_{\theta}$ exists and then $\| x_{</description></item><item><title>Conjugate Prior Distribution</title><link>https://freshrimpsushi.github.io/en/posts/712/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/712/</guid><description>Definition 1 If the prior and posterior distributions belong to the same family of distributions, the prior distribution is referred to as a Conjugate Prior. Explanation Even though Bayesian analysis is essentially about finding the parameters through updates regardless of the initial prior, using an appropriate prior can greatly simplify the mathematical computations and make the results easier to understand if there is some knowledge about the model. (1) The</description></item><item><title>Laplace Transform of Constant Functions</title><link>https://freshrimpsushi.github.io/en/posts/745/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/745/</guid><description>Formulas1 $$ \mathcal{L} \left\{ 1 \right\} = \dfrac{1}{s},\quad s&amp;gt;0 $$ Derivation $$ \begin{align*} \mathcal{L}\left\{ 1 \right\} &amp;amp;= \int _{0}^\infty e^{-st} \cdot 1 dt \\ &amp;amp;= \lim \limits_{A \to \infty} \left[ -\dfrac{e^{-st}}{s} \right]_{0}^A \\ &amp;amp;= \lim \limits_{A \to \infty} \left[ -\dfrac{e^{-sA}}{s} +\dfrac{e^{-0t}}{s} \right] \\ &amp;amp;= \dfrac{1}{s} \end{align*} $$ Since it must follow $\lim \limits_{A \to \infty}\dfrac{e^{-sA}}{s}=0$,2 the condition that $s&amp;gt;0$ is added. ■ See Also Table of Laplace Transforms William E.</description></item><item><title>Laplace Transform of Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/750/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/750/</guid><description>Formulas1 $$ \mathcal {L} \left\{ e^{at} \right\} = \dfrac{1}{s-a},\quad s&amp;gt;a $$ Description Let&amp;rsquo;s compare this with the result of the Laplace transform of a constant function (../745). $$ \mathcal{L} \left\{ 1 \right\} =\dfrac{1}{s} $$ The Laplace transform result of $e^{at}$ is the same as when $F(s)$ is shifted by $a$, when $f(t)=1$. This is inevitable because when $e^{at}$ is multiplied by the original function, $\displaystyle \int e^{-st}f(t) dt$ becomes $\displaystyle</description></item><item><title>Laplace Transform of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/751/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/751/</guid><description>Formulas1 The Laplace transforms of hyperbolic sine and hyperbolic cosine functions are as follows. $$ \mathcal{L} \left\{ \sinh (at) \right\} = \dfrac{a}{s^2-a^2},\quad s&amp;gt;|a| \\ \mathcal{L} \left\{ \cosh (at) \right\} = \dfrac{s}{s^2-a^2},\quad s&amp;gt;|a| $$ Description The definition of hyperbolic functions is as follows. $$ \sinh (ax) = \dfrac{ e^{ax} - e^{-ax} }{ 2 } \\ \cosh (ax) = \dfrac{ e^{ax} + e^{-ax} }{ 2 } $$ Derivation Use the results of</description></item><item><title>Laplace Transform of Polynomial Functions</title><link>https://freshrimpsushi.github.io/en/posts/747/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/747/</guid><description>Formulas1 $$ \mathcal{L} \left\{ t^p \right\} = \dfrac{ \Gamma (p+1) } {s^{p+1}},\quad s&amp;gt;0 $$ Explanation The Laplace transform of a polynomial is represented by the Gamma function. If we use $x^p$ instead of $t^p$, it would be easier to recognize at a glance. Usually, in differential equations, variables represent time, so $x$ is replaced with $t$. Derivation $$ \begin{align*} \mathcal{L} \left\{ t^p \right\} &amp;amp;= \int_{0}^\infty e^{-st}t^p dt \\ &amp;amp;= \lim</description></item><item><title>Laplace Transform of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/746/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/746/</guid><description>Formulas1 The Laplace transforms of sine and cosine are as follows. $$ \mathcal{L} \left\{ \sin (at) \right\} = \dfrac{a}{s^2+a^2},\quad s&amp;gt;0 $$ $$ \mathcal{L} \left\{ \cos (at) \right\} = \dfrac{s}{s^2+a^2},\quad s&amp;gt;0 $$ Derivation $\sin (at)$ $$ \begin{align*} \mathcal{L} \left\{ \sin (at) \right\}&amp;amp; =\displaystyle \int_{0}^\infty e^{-st}\sin(at)dt \\ &amp;amp;= \lim \limits_{A \to \infty} \left[-\dfrac{1}{a}e^{-st}\cos (at) \right]_{0}^A+ \lim \limits_{A \to \infty} \int _{0}^\infty -\dfrac{s}{a}e^{-st} \cos (at)dt \\ &amp;amp;= \dfrac{1}{a} - \lim \limits_{A \to \infty}</description></item><item><title>Laplace Transform Table</title><link>https://freshrimpsushi.github.io/en/posts/743/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/743/</guid><description>Formula1 This is table of Laplace transform. $f(t)=\mathcal{L^{-1}}$ $F(s)=\mathcal{L} \left\{ f(t) \right\}$ Derivation $1$ $\dfrac{1}{s}$ link $e^{at}$ $\dfrac{1}{s-a}$ link $t^n$ $\dfrac{n!}{s^{n+1}}$ link $t^{p}$ $\dfrac{ \Gamma (p+1) }{ s^{p+1}}$ link $t^{p}e^{at}$ $\dfrac{ \Gamma (p+1) }{ (s-a)^{p+1}}$ link $\sin (at)$ $\dfrac{a}{s^2+a^2}$ link $\cos (at)$ $\dfrac{s}{s^2+a^2}$ link $e^{at}\sin(bt)$ $\dfrac{b}{(s-a)^2 +b^2}$ link $e^{at}\cos(bt)$ $\dfrac{s-a}{(s-a)^2+b^2}$ link $\sinh (at)$ $\dfrac{a}{s^2-a^2}$ link $\cosh (at)$ $\dfrac{s}{s^2-a^2}$ link $e^{at} \sinh (bt)$ $\dfrac{b}{(s-a)^2-b^2}$ link $e^{at} \cosh (bt)$ $\dfrac{s-a}{(s-a)^2-b^2}$ link $u_{c}(t)=</description></item><item><title>Linearity of Laplace Transform</title><link>https://freshrimpsushi.github.io/en/posts/749/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/749/</guid><description>Theorem1 Let $f_{1}$ and $f_2$ be functions for which the Laplace transform exists. Also, let $c_{1}, c_2$ be an arbitrary constant. Then $$ \mathcal{L} \left\{ c_{1}f_{1} + c_2f_2 \right\} = c_{1}\mathcal{L} \left\{f_{1} \right\} + c_2\mathcal{L} \left\{f_2 \right\} $$ Explanation It is obvious that the Laplace transform is an integral transform. Proof $$ \begin{align*} \mathcal{L} \left\{ c_{1}f_{1}+c_2f_2 \right\} &amp;amp;= \int_{0}^\infty e^{-st} \left( c_{1}f_{1}+c_2f_2 \right) dt \\ &amp;amp;= \int_{0}^\infty e^{-st}c_{1}f_{1} dt +</description></item><item><title>Proof of the Completeness of Finite Dimensional Normed Spaces</title><link>https://freshrimpsushi.github.io/en/posts/711/</link><pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/711/</guid><description>Theorem 1 Finite-dimensional normed spaces are complete. Description Accordingly, finite-dimensional vector spaces become Banach spaces merely by the definition of a norm. This is particularly useful because of the frequent use of $\mathbb{R}^{n}$ or $\mathbb{C}^{n}$. Proof Strategy: Utilize the fact that we are dealing with a finite-dimensional vector space to break down every vector into basis units and define a convenient norm. Transform abstract calculations into direct calculations by leveraging</description></item><item><title>Laplace's Succession Rule</title><link>https://freshrimpsushi.github.io/en/posts/710/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/710/</guid><description>Theorem 1 Let&amp;rsquo;s say the prior distribution of the binomial model $\displaystyle p(y | \theta) = \binom{ n }{ y} \theta^{y} (1- \theta)^{n-y}$ follows a uniform distribution $U (0,1)$ and the posterior distribution follows a beta distribution $\beta (y+1 , n-y+1)$, hence $p( \theta | y ) \sim \theta^{y} (1- \theta)^{n-y}$. Then, for the data obtained so far $y$, the probability of observing a new $\tilde{y}$ being $1$ is $$</description></item><item><title>Proof that All Norms Defined on a Finite Dimensional Vector Space are Equivalent</title><link>https://freshrimpsushi.github.io/en/posts/709/</link><pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/709/</guid><description>Theorem 1 All norms defined on a finite-dimensional vector space are equivalent. Explanation The fact that all norms defined in Euclidean space are equivalent is a corollary of this theorem. Proof Strategy: If we can show that there exists $c , C &amp;gt;0$ satisfying $c \| v \| _{\alpha} \le \| v \| _{\beta} \le C \| v \| _{\alpha}$, then the two norms $\left\| \cdot \right\|_{\alpha}$ and $\left\| \cdot</description></item><item><title>Every Finite-Dimensional Normed Space Has a Basis</title><link>https://freshrimpsushi.github.io/en/posts/707/</link><pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/707/</guid><description>Theorem 1 Every finite-dimensional norm space has a basis. Description It might be unfamiliar to announce the existence of a basis not under specific conditions, but actually, the definition of a basis never stated that every vector space has a basis. Depending on how one defines finite dimensionality, it can also be a fact so obvious that it requires no separate proof. Proof Strategy: Use the fact that the space</description></item><item><title>Homogeneity of Norms</title><link>https://freshrimpsushi.github.io/en/posts/274/</link><pubDate>Sun, 28 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/274/</guid><description>Definition On a vector space $V$, if for two norms $\left\| \cdot \right\|_{\alpha}, \left\| \cdot \right\|_{\beta}$ defined and for any vector $\mathbf{v} \in V$ the following $$ c \left\| \mathbf{v} \right\|_{\alpha} \le \left\| \mathbf{v} \right\|_{\beta} \le C \left\| \mathbf{v} \right\|_{\alpha} $$ is satisfied with some constant $c , C &amp;gt;0$, the two norms are equivalent. Theorem Preservation of inequalities [1]: If norms $\left\| \cdot\right\|_{\alpha}$ and $\left\| \cdot \right\|_{\beta}$ defined on</description></item><item><title>Hamel Basis of Finite-Dimensional Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/705/</link><pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/705/</guid><description>Definition 1 Given a vector space $X$. For vectors $x_{1} , \dots , x_{n}$ and scalar $\alpha_{1} , \dots , \alpha_{n}$ in $X$, $\alpha_{1} x_{1} + \cdots + \alpha_{n} x_{n}$ is called the linear combination of vectors $x_{1} , \dots , x_{n}$. When it is $M =\left\{ x_{1} , \dots , x_{n} \right\}$, the set of all linear combinations of vectors of $M$ is called $\text{span} M$, which is a</description></item><item><title>Orthogonal Complement of a Subspace</title><link>https://freshrimpsushi.github.io/en/posts/273/</link><pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/273/</guid><description>Definition1 In the set $$ W^{\perp} = \left\{ \mathbf{v} \in V \ : \left\langle \mathbf{v} , \mathbf{w} \right\rangle = 0,\quad \forall \mathbf{w} \in W \right\} $$ for the subspace $W$ of a vector space $V$, it is called the orthogonal complement of $W$. Here $\langle , \rangle$ is the inner product. Explanation In other words, $W^{\perp}$ is a collection of vectors that are orthogonal to every element of $W$. The</description></item><item><title>Banach Space</title><link>https://freshrimpsushi.github.io/en/posts/703/</link><pubDate>Thu, 25 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/703/</guid><description>Definition1 A Banach space is a complete normed space. Explanation A complete space refers to a space in which every Cauchy sequence converges. Banach space is a space that satisfies all of the following conditions, making it an extremely useful space as it is defined with a distance function and possesses completeness: It is a vector space. It is a normed space. $\implies$ It is a metric space. It is</description></item><item><title>Holder Inequality</title><link>https://freshrimpsushi.github.io/en/posts/258/</link><pubDate>Thu, 25 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/258/</guid><description>Definition $\dfrac{1}{p} + \dfrac{1}{q} = 1$ satisfies and for two constants $p, q$ and $\mathbf{u}, \mathbf{v} \in \mathbb{C}^n$ larger than 1, the following inequality holds: $$ | \left\langle \mathbf{u}, \mathbf{v} \right\rangle | = |\mathbf{u} ^{\ast} \mathbf{v}| \le ||\mathbf{u}||_{p} ||\mathbf{v}||_{q} $$ This is called the Hölder&amp;rsquo;s inequality. Explanation Although it should be written as Höld</description></item><item><title>Bayesian Paradigm</title><link>https://freshrimpsushi.github.io/en/posts/702/</link><pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/702/</guid><description>Buildup Statistics can be defined as the study of methods to understand parameters. Just like measuring a physical quantity using formulas or laws, it would be ideal if parameters can be precisely estimated. However, due to the impractical nature of such precision, assumptions and samples are used to find &amp;lsquo;what is expected to be the parameter&amp;rsquo;. If interested in the average height of men in our country $X$, one might</description></item><item><title>Proof of Young's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/267/</link><pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/267/</guid><description>Theorem Given constants $\displaystyle {{1} \over {p}} + {{1} \over {q}} = 1$ that satisfy the condition and two positive numbers $p,q$ and $a,b$ which are greater than 1, $$ ab \le { {a^{p}} \over {p} } + {{b^{q}} \over {q}} $$ Description Apart from the aesthetically pleasing algebraic aspect, this inequality is not often mentioned except when proving the Hölder inequality. Proof Since both</description></item><item><title>Proof that the p-norm becomes the maximum norm when p=∞</title><link>https://freshrimpsushi.github.io/en/posts/699/</link><pubDate>Sun, 21 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/699/</guid><description>Theorem Let&amp;rsquo;s define the sequence spaces $l^{p}$ and $1 &amp;lt; p_{0} &amp;lt; \infty$ such that $\left\{ x_{n} \right\}_{n \in \mathbb{N} } \in \mathcal{l}^{p_{0}}$. $$ \lim_{p \to \infty} \left( \sum_{n \in \mathbb{N} } | x_{n} |^{p} \right)^{ {{1} \over {p}} } = \sup_{n \in \mathbb{N}} | x_{ n } | $$ Explanation Despite being encountered early in analysis or linear algebra, the reason why the maximum norm is related to $\infty$</description></item><item><title>Proof of the Finite Form of Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/264/</link><pubDate>Mon, 08 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/264/</guid><description>Theorem In $I \subset \mathbb{R}$, for the convex functions $f : I \to \mathbb{R}$ and $\displaystyle \sum_{k=1}^{n} \lambda_{k} = 1, \lambda_{k}&amp;gt;0$ $$ \begin{align*} f( \lambda_{1} x_{1} + \lambda_{2} x_{2} + \cdots + \lambda_{n} x_{n} ) &amp;amp; \le \lambda_{1} f( x_{1}) + \lambda_{2} f( x_{2}) + \cdots + \lambda_{n} f( x_{n} ) \\ f\left( \sum\limits_{k=1}^{n}\lambda_{k}x_{k} \right) &amp;amp;\le \sum\limits_{k=1}^{n} \lambda_{k} f(x_{k}) \end{align*} $$ If $f$ is a concave function, then the opposite</description></item><item><title>Viewing the Monty Hall Dilemma through Bayes' Theorem</title><link>https://freshrimpsushi.github.io/en/posts/697/</link><pubDate>Mon, 08 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/697/</guid><description>Explanation As is well known, in the Monty Hall problem it is advantageous to switch regardless of where the prize actually is. Setting aside whether one accepts this as a fact, there are people who do not grasp the Monty Hall game intuitively or who are uncomfortable with algebraic expressions. For convenience, imagine you are the player and you have chosen door 1. At this point we have no information</description></item><item><title>In Linear Algebra, What is a Norm?</title><link>https://freshrimpsushi.github.io/en/posts/257/</link><pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/257/</guid><description>Definition Let us define a vector space over $V$ as $\mathbb{F}$. $\left\| \cdot \right\| : V \to \mathbb{F}$ is defined as a norm on $V$ if it satisfies the following three conditions with respect to $\mathbf{u}, \mathbf{v} \in V$ and $k \in \mathbb{F}$: (i) Positive definiteness: $\left\| \mathbf{u} \right\| \ge 0$ and $\mathbf{u} = \mathbb{0} \iff \left\| \mathbf{u} \right\| = 0$ (ii) Homogeneity: $\left\|k \mathbf{u} \right\| = | k |</description></item><item><title>Definition and Criterion of Subrings</title><link>https://freshrimpsushi.github.io/en/posts/590/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/590/</guid><description>Definition 1 A subset $S$ of a ring $R$ is called a subring of $R$ if it satisfies the conditions of a ring with respect to the operations of $R$. Meanwhile, it is trivial that $\left\{ 0 \right\}$ and $R$ are subrings of the ring $R$, hence $\left\{ 0 \right\}$ and $R$ are referred to as trivial subrings. Theorem: Subring Criterion For a non-empty subset $S$ of a ring $R$,</description></item><item><title>Rings in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/587/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/587/</guid><description>Definition 1 A set $R$ satisfying the following rules for two binary operations, addition$+$ and multiplication$\cdot$, is defined as a Ring. When $a$, $b$, $c$ are elements of $R$, Commutative law holds for addition. $$a+b=b+a$$ Associative law holds for addition. $$(a+b)+c=a+(b+c)$$ There exists an identity element for addition. $$\forall a \ \exists 0\ \ \mathrm{s.t} \ a+0=a$$ There exists an additive inverse for every element. $$\forall a \ \exists -a\</description></item><item><title>Rules for Multiplication in a Ring</title><link>https://freshrimpsushi.github.io/en/posts/588/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/588/</guid><description>Theorem Given that $a,\ b,\ c$ is an element of the ring $R$ and $0$ is the identity element for addition, the following properties hold: $a0=0a=0$ $a(-b)=(-a)b=-(ab)$ $(-a)(-b)=ab$ $a(b-c)=ac-ac \ \ \And\ \ (b-c)a=ba-ca$ If there exists a multiplicative identity element $1$, then the following properties also hold: $(-1)a=-a$ $(-1)(-1)=1$ Proof 1. It&amp;rsquo;s about the property that multiplying any element with the additive identity still results in the additive identity.</description></item><item><title>Sequence Spaces (ℓp spaces)</title><link>https://freshrimpsushi.github.io/en/posts/695/</link><pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/695/</guid><description>Definition 1 For $1 \le p &amp;lt; \infty$, distance space $( \ell^{p} , d^{p} )$ is defined as follows: (i) Set of converging sequences: $$ \ell^{p} := \left\{ \left\{ x_{n} \right\}_{n \in \mathbb{N}} \subset \mathbb{C} \left| \left( \sum_{i=1}^{\infty} | x_{i} |^{p} \right)^{{1} \over {p}} &amp;lt; \infty \right. \right\} $$ (ii) Distance function: $$ d^{p} ( x_{n} , y_{n} ) := \left( \sum_{i = 1}^{\infty} | x_{i} - y_{i} |^{p} \right)^{</description></item><item><title>Dismantling a list in R, Removing Duplicate Elements</title><link>https://freshrimpsushi.github.io/en/posts/688/</link><pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/688/</guid><description>Overview Dealing with all sorts of unrefined data, the list data type in R is especially useful in organizing data. However, on the flip side, accessing data can be a bit cumbersome and disadvantageous in locating the desired content. In this case, breaking down the list data type through the unlist() function makes this manipulation much more convenient. The unique() function removes all duplicate elements in the received array, leaving</description></item><item><title>Properties and Proofs of Surplus Types</title><link>https://freshrimpsushi.github.io/en/posts/691/</link><pubDate>Fri, 21 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/691/</guid><description>Theorem Let $H$ be a subgroup of group $G$. Then, for an element $a$ of group $G$, set $aH= \left\{ ah | h\in H \right\}$ is called the left coset, and $Ha = \left\{ ha | h\in H \right\}$ is called the right coset. Let&amp;rsquo;s say $H &amp;lt; G,\enspace a,b \in G,\enspace h \in H$. Then, the following properties are satisfied: $a \in aH$ $aH=H \iff a \in H$ $aH=bH</description></item><item><title>What is a Manifold?</title><link>https://freshrimpsushi.github.io/en/posts/673/</link><pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/673/</guid><description>Definition 1 A topological space $X$ is called a $n$-dimensional manifold when it satisfies the following three conditions: (i): It is second-countable. (ii): It is Hausdorff. (iii): Every point of $X$ has a neighborhood homeomorphic to an open set in $\mathbb{R}^{n}$. A $n$-dimensional manifold $X$ is said to have a boundary when it has the following two types of points: (1) Interior points: Every neighborhood of $x \in X^{\circ}$ is</description></item><item><title>Homoscedasticity of Residuals Verified through Model Diagnostics</title><link>https://freshrimpsushi.github.io/en/posts/681/</link><pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/681/</guid><description>Diagnostic Methods 1 Standardized residual plots can be used to check if a regression analysis was conducted properly. To verify homoscedasticity, one should check if the scatter of the residuals is uniformly distributed overall. Common examples of lack of homoscedasticity include the following two cases. The variance increases towards the end, a situation that often requires a transformation or the introduction of weights to resolve. Regardless of how easy it</description></item><item><title>Derivation of the General Term of the Fibonacci Sequence</title><link>https://freshrimpsushi.github.io/en/posts/680/</link><pubDate>Sat, 15 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/680/</guid><description>Theorem Let&amp;rsquo;s say that the sequence sequence $\left\{ F_{n} \right\}_{n=0}^{\infty}$ is defined as $F_{n+1} := F_{n} + F_{n-1}$. If $F_{0} = F_{1} = 1$, then for $\displaystyle r_{0} : = {{1 + \sqrt{5} } \over {2}}$ and $\displaystyle r_{1} : = {{1 - \sqrt{5} } \over {2}}$, $$ F_{n} = {{ {r_{0}}^{n+1} - {r_{1}}^{n+1} } \over { r_{0} - r_{1} }} $$ Description Note that the Fibonacci sequence introduced above</description></item><item><title>Residual Linearity Verified through Model Diagnostics</title><link>https://freshrimpsushi.github.io/en/posts/677/</link><pubDate>Wed, 12 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/677/</guid><description>Diagnostic Techniques 1 Standardized residual plots can be used to check if the regression analysis was performed correctly. To check for linearity, see if the residuals are symmetrically distributed around $0$. Looking at the figure on the right, it is evident that there is a lack of linearity. If it were a simple regression analysis, it would result in an inability to explain the trend of the data at all.</description></item><item><title>Regression Model Diagnostics</title><link>https://freshrimpsushi.github.io/en/posts/675/</link><pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/675/</guid><description>Necessity In the case of simple regression analysis, since it involves only one independent variable and one dependent variable, making it a $2$ dimensional analysis, it is easy to visually confirm if the analysis was conducted properly. However, for multiple regression analyses that exceed $3$ dimensions, it becomes difficult to represent the data graphically, making it hard to verify the accuracy of the analysis. There are instances where the analysis</description></item><item><title>Proof of the Pappus-Guldin Theorem</title><link>https://freshrimpsushi.github.io/en/posts/685/</link><pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/685/</guid><description>Theorem Let the area of a shape $F$ on the plane be denoted as $A$, and let the volume of the solid of revolution $W$ obtained by rotating $F$ around axis $z$ be denoted as $V$. If the distance between the center of mass of $F$ and the axis $z$ is denoted as $r$, then $$ V = 2 \pi r A $$ Description The Pappus-Guldin Theorem is often mentioned</description></item><item><title>파푸스-굴딘 정리 증명</title><link>https://freshrimpsushi.github.io/en/posts/685/</link><pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/685/</guid><description>Theorem Let the area of a planar figure $F$ be $A$, and let the volume of the solid $W$ obtained by rotating $F$ about the $z$-axis be $V$. If the distance between the $z$-axis and the centroid of $F$ is $r$, then $$ V = 2 \pi r A $$ Explanation The Pappus-Guldinus theorem cannot be proven at the high school level, but it is a theorem often mentioned by</description></item><item><title>F-test for Regression Coefficients</title><link>https://freshrimpsushi.github.io/en/posts/672/</link><pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/672/</guid><description>Hypothesis Testing Assuming in the model diagnostics of the linear multiple regression model, the residuals satisfy linearity, homoscedasticity, independence, and normality. The hypothesis testing for the multiple regression analysis with $n$ observations and $p$ independent variables is as follows: $H_{0}$: $\beta_{1} = \beta_{2} = \cdots = \beta_{p} = 0$ i.e., all independent variables do not have a correlation with the dependent variable. $H_{1}$: At least one among $\beta_{1} , \beta_{2}</description></item><item><title>How to Interpret Multiple Regression Analysis Results in R</title><link>https://freshrimpsushi.github.io/en/posts/670/</link><pubDate>Wed, 05 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/670/</guid><description>Data Exploration tail(attitude) In R, let&amp;rsquo;s load the built-in data attitude and check it using the tail() function. We are interested in performing multiple regression analysis on this data. We are interested in how the other independent variables affect the rating, which is our dependent variable. It&amp;rsquo;s difficult to see if there is a linear relationship between rating and the other variables just by looking at the data, so let&amp;rsquo;s</description></item><item><title>Symbols Used in Plotting Graphs in R</title><link>https://freshrimpsushi.github.io/en/posts/669/</link><pubDate>Tue, 04 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/669/</guid><description>Code Various symbols can be changed in graph related functions by using the pch option. The image above especially shows at a glance the most frequently used symbols. There are many useful ones, but especially number 16 is often used, and after number 25, while the markers themselves are set, there&amp;rsquo;s nothing much useful. In the example code below, sym can be changed from 26 to 50 for checking. Meanwhile,</description></item><item><title>Shilov's theorem</title><link>https://freshrimpsushi.github.io/en/posts/668/</link><pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/668/</guid><description>Theorem 1 Let&amp;rsquo;s say we have a finite group that satisfies prime numbers $p$ and $\gcd (p, m) = 1$ for some natural number $m$ where $G$ is $|G| = p^{n} m$. A Sylow $p$-subgroup of $G$ is a $p$-subgroup that is not contained in any other $p$-subgroup. First Sylow Theorem: $G$ has a $p$-subgroup satisfying $|P| = p^{i}$ for $i=1, \cdots , n$. Second Sylow Theorem: For Sylow $p$-subgroups</description></item><item><title>How to Print Characters on a Plot in R</title><link>https://freshrimpsushi.github.io/en/posts/667/</link><pubDate>Sun, 02 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/667/</guid><description>Code You can use the text() function to place text on the graph. The first option is a vector of $x$ axis coordinates, the second option is a vector of $y$ axis coordinates, and the third option is a vector of strings to input. You will understand immediately if you try running the example code below with only changing t. win.graph(6,5) plot(x=0,y=0,xlim=c(-1,5),ylim=c(-1,4),xlab=&amp;#34;x&amp;#34;,ylab=&amp;#34;y&amp;#34;) points(4,3,col=&amp;#34;red&amp;#34;,pch=19) #1 abline(h=0) #2 abline(v=0) #3 abline(0,3/4) #4</description></item><item><title>Multiple Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/666/</link><pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/666/</guid><description>Overview Regression analysis is a method used to discover the relationships between variables, particularly useful for identifying linear relationships. Multiple Linear Regression refers to the regression analysis that determines the effects of multiple independent variables (explanatory variables) on a single dependent variable (response variable). Model 1 $$Y = \beta_{0} + \beta_{1} X_{1} + \cdots + \beta_{p} X_{p} + \varepsilon $$ We are interested in whether variables have a linear relationship</description></item><item><title>Quotient Space</title><link>https://freshrimpsushi.github.io/en/posts/665/</link><pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/665/</guid><description>Definitions 1 Let&amp;rsquo;s say the quotient class for the topological space $(X, \mathscr{T} )$ and the equivalence relation $\sim$ is $[x] = \left\{ y \in X \ | \ x \sim y \right\}$. $X / \sim$ is defined as the quotient set. If $q : X \to X / \sim$ is defined as $q(x) = [ x ]$, it is called the quotient function. For $U \in \mathscr{T}$, $$ q^{-1}</description></item><item><title>How to Draw Horizontal and Vertical Lines in R</title><link>https://freshrimpsushi.github.io/en/posts/664/</link><pubDate>Thu, 30 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/664/</guid><description>Example 1. abline(h=0) Draws a horizontal line. 2. abline(v=0) Draws a vertical line. 3. abline(0,3/4) Draws a line with an intercept of $y$ and a slope of $0$. Originally, the abline() function is named after $3/4$, which are the coefficients of $y=a+bx$. If you&amp;rsquo;re using R for statistics, you probably won&amp;rsquo;t find much use for it outside of drawing regression lines. segments(4,0,4,3) Draws a line segment from $a,b$ to $(4,0)$.</description></item><item><title>Proof of Cauchy's Theorem in group Theory</title><link>https://freshrimpsushi.github.io/en/posts/663/</link><pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/663/</guid><description>Theorem 1 For a finite group $G$, if a prime number $p$ is a divisor of $|G|$, then there exists a subgroup $H \leqslant G$ satisfying $|H| = p$. Explanation When people usually refer to Cauchy&amp;rsquo;s theorem, they don&amp;rsquo;t typically mean this theorem. Another Cauchy&amp;rsquo;s theorem, which is fundamental to complex analysis, is much more significant and often mentioned. More so because this theorem is generally superseded by the First</description></item><item><title>Drawing Graphs in R</title><link>https://freshrimpsushi.github.io/en/posts/662/</link><pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/662/</guid><description>Overview R has the advantage of making the representation of graphs very easy compared to other languages. When compared with other statistical packages, while easy drawings might be faster in those packages, R tends to become more convenient as the need for detailed expressions increases. Of course, R is not only for graphics, but since it is a very big advantage, it is good to practice to handle it freely.</description></item><item><title>In P-groups in abstract algebra</title><link>https://freshrimpsushi.github.io/en/posts/660/</link><pubDate>Mon, 27 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/660/</guid><description>Definition 1 Given that the identity of a finite group $G$ is $e$, if $g \in G$ satisfies $g^{n} = e$ for the smallest $n \in \mathbb{N}$, then it is denoted as $|g| = n$. For all $g \in G$ and a given prime number $p$, if there exists an integer $m \ge 0$ that satisfies $|g| = p^{m}$, the group $G$ is called a $p$-group. Explanation If $|G| =</description></item><item><title>How to Filter Data Conditionally in R</title><link>https://freshrimpsushi.github.io/en/posts/659/</link><pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/659/</guid><description>Overview R is primarily used in statistics, and its ability to select and edit necessary data is unparalleled. Becoming familiar with handling such data can be a bit difficult, but once perfectly mastered, other languages will feel incredibly inconvenient. In fact, these tips are not much helpful just by reading. (Indeed, the explanations can also be brief in the pursuit of accuracy.) Handling a lot of data and writing various</description></item><item><title>Wirtinger Inequality and Tietze Extension Theorem</title><link>https://freshrimpsushi.github.io/en/posts/658/</link><pubDate>Sat, 25 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/658/</guid><description>Theorem Urysohn&amp;rsquo;s Lemma 1 If $X$ is a normal space, then for all closed sets $A \cap B = \emptyset$ and $A, B \subset X$, there exists a continuous function $f:X \to [0,1]$ that satisfies $f(A) = \left\{ 0 \right\}$ and $f(B) = \left\{ 1 \right\}$. Tietze Extension Theorem 2 In a normal space $X$, for a closed set $C$, if $f : C \to \mathbb{R}$ is continuous, then there</description></item><item><title>Enthalpy, Helmholtz Free Energy, Gibbs Free Energy</title><link>https://freshrimpsushi.github.io/en/posts/657/</link><pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/657/</guid><description>Definition Enthalpy $H$ is defined as follows: $$ H := U + PV $$ Helmholtz function $F$ is defined as follows: $$ F := U - TS $$ Gibbs function $G$ is defined as follows: $$ G := H - TS $$ Explanation Enthalpy is a function almost as well-known as entropy, but its main area of importance is chemistry. For physics, it is advisable to view these three together</description></item><item><title>Computing Conditional Sums and Conditional Means in R</title><link>https://freshrimpsushi.github.io/en/posts/656/</link><pubDate>Thu, 23 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/656/</guid><description>Overview If we&amp;rsquo;re talking about Excel, sometimes we need functions like sumif() or averageif(). In R, while there&amp;rsquo;s no such simple function, the apply family of functions serves as a powerful superior alternative. It&amp;rsquo;s beneficial to meticulously learn these functions, but for the time being, let&amp;rsquo;s just focus on calculating conditional sums and averages. Example Let&amp;rsquo;s load the iris dataset. If we randomly look at the 10th, 50th, 90th, and</description></item><item><title>Gibbs' Representation of Entropy</title><link>https://freshrimpsushi.github.io/en/posts/655/</link><pubDate>Wed, 22 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/655/</guid><description>Formulas Given that the macrostate of a system is in the $i$ state, let&amp;rsquo;s denote the probability as $P_{i}$. The measured entropy $S$ of this system can be expressed as follows: $$ S = - k_{B} \sum_{i} P_{i} \ln P_{i} $$ Here, $k_{B}$ is the Boltzmann constant. Explanation Shannon Entropy: When the probability mass function of a discrete random variable $X$ is $p(x)$, the entropy of $X$ is expressed as</description></item><item><title>Regression Coefficient's t-test</title><link>https://freshrimpsushi.github.io/en/posts/654/</link><pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/654/</guid><description>Hypothesis Testing $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ When independent variables of</description></item><item><title>The Entropy of the Universe Does Not Decrease</title><link>https://freshrimpsushi.github.io/en/posts/653/</link><pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/653/</guid><description>Theorem The entropy of the universe does not decrease. Explanation The first thing that comes to mind upon seeing the statement above is &amp;lsquo;wow, that&amp;rsquo;s cool&amp;rsquo;. However, the truly cool people are those who can understand this equationally, and let&amp;rsquo;s strive to be one of those people. Proof This universe is unique, and thus it requires the assumption that there is no &amp;lsquo;outside&amp;rsquo; of this universe. Consider the cycles as</description></item><item><title>How to View Simple Regression Analysis Results in R</title><link>https://freshrimpsushi.github.io/en/posts/652/</link><pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/652/</guid><description>Practice How to Do Regression Analysis head(faithful) In R, load the built-in data faithful using head() function and check. It&amp;rsquo;s difficult to confirm if there&amp;rsquo;s a linear relationship between the two variables just by looking at the data, so let&amp;rsquo;s plot it to see. win.graph(6,3) par(mfrow=c(1,2)) plot(faithful, main =&amp;#34;faithful&amp;#34;,asp=T) plot(faithful, main =&amp;#34;faithful&amp;#34;) points(head(faithful),col=&amp;#39;red&amp;#39;,pch=19) The one on the left is with the aspect ratio kept constant, which is accurate but hard</description></item><item><title>In Thermodynamics, What is Entropy?</title><link>https://freshrimpsushi.github.io/en/posts/651/</link><pubDate>Sat, 18 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/651/</guid><description>Definition The quantity $S$ is defined as entropy if it satisfies the following equation. $$ dS = {{ \delta Q_{\text{rev} } } \over { T }} $$ Explanation Entropy is a physical quantity representing &amp;lsquo;disorder,&amp;rsquo; and it&amp;rsquo;s challenging to understand why it indicates disorder just by looking at its mathematical definition. Explanations for non-specialists like &amp;lsquo;messing up a room&amp;rsquo; or &amp;lsquo;dropping ink into a glass of water&amp;rsquo; can only explain</description></item><item><title>Simple Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/648/</link><pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/648/</guid><description>Overview Regression Analysis is a method for identifying relationships between variables, especially useful for elucidating linear relationships. Simple Linear Regression is the simplest among them, referring to regression analysis on one dependent (response) variable and one independent (explanatory) variable. Model 1 The statement that independent variable $x_{i}$ and dependent variable $y_{i}$ have a linear relationship means that for some $a,b$, it can be expressed as $y_{i} = ax_{i} + b$.</description></item><item><title>Clausius Inequality</title><link>https://freshrimpsushi.github.io/en/posts/649/</link><pubDate>Thu, 16 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/649/</guid><description>Theorem In a cyclic process, the following equation holds. $$ \oint {{\delta Q} \over {T}} \le 0 $$ Especially, if the process is reversible, the following is true. $$ \oint {{\delta Q_{\text{rev}}} \over {T}} = 0 $$ Explanation A cyclic process refers to a process in which the state of the system at the beginning and the end of the process is the same. If the entire process is reversible,</description></item><item><title>Fitted Values, Predicted Values, Residuals, Errors</title><link>https://freshrimpsushi.github.io/en/posts/650/</link><pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/650/</guid><description>Definition 1 Let the regression equation obtained by regression analysis $Y \gets X_{1} + X_{2} + \cdots + X_{n}$ be denoted as $y = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \cdots + \beta_{n} x_{n}$, and represent the $i$-th data as $(y_{i} , x_{i1} , x_{i2} , \cdots , x_{in})$. Mean: $$ \displaystyle \overline{y} := {{1} \over {n}} \sum_{i=1}^{n} y_{i} $$ Fitted Value: For the $i$-th data $y_{i}$ $$</description></item><item><title>Carnot's Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/647/</link><pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/647/</guid><description>Theorem No engine can be more efficient than a Carnot engine. Description Even though it&amp;rsquo;s impossible to actually implement a Carnot engine, the fact that it represents a theoretical limit is very meaningful. Proof Let&amp;rsquo;s assume that there is an engine $E$ more efficient than a Carnot engine $C$. $E$ takes in heat $Q_{h} ' $ and does work $W$, and $C$ takes in heat $Q_{l}$ and work $W$ to</description></item><item><title>Simplifying the Exponentiation of Two-digit Numbers Ending in 5</title><link>https://freshrimpsushi.github.io/en/posts/661/</link><pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/661/</guid><description>Formulas The square of a two-digit number whose ones place is 5 can be computed quickly and easily as shown in the photo above. It&amp;rsquo;s okay to just know and use the result, but some might be curious about why it works this way. Proof Let&amp;rsquo;s assume any two-digit number whose ones place is $5$ is $10a+5$. Then, the square can be calculated as follows. $$ \begin{align*} (10a+5)(10a+5) =&amp;amp;\ 100a^2+100a+25</description></item><item><title>Functional Spaces in Topology</title><link>https://freshrimpsushi.github.io/en/posts/646/</link><pubDate>Mon, 13 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/646/</guid><description>Definition 1 A function space is defined as the product space $Y^{X}$ for topological spaces $X$ and $Y$. $$ Y^{X} : = \prod_{x \in X} Y = \left\{ f \ | \ f : X \to Y \text{ is a function} \right\} $$ The topology for the function space can be: For an open set $U$ in $x \in X$ and $Y$, let $$ S (x , U) = \left\{</description></item><item><title>Carnot Engine</title><link>https://freshrimpsushi.github.io/en/posts/645/</link><pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/645/</guid><description>Definition A machine that performs the following four processes in order is called a Carnot engine: Step 1. Isothermal Expansion Process $A \to B$: The volume increases from $V_{A}$ to $V_{B}$ by absorbing thermal energy $Q_{h}$ while maintaining the temperature at $T_{h}$. Step 2. Adiabatic Expansion Process $B \to C$: The volume increases from $V_{B}$ to $V_{C}$ causing the temperature to decrease from $T_{h}$ to $T_{l}$ while the heat remains</description></item><item><title>Proof of the Third Isomorphism Theorem</title><link>https://freshrimpsushi.github.io/en/posts/644/</link><pubDate>Sat, 11 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/644/</guid><description>Theorem 1 Let $G,G'$ be a group. First Isomorphism Theorem: If there exists a homomorphism $\phi : G \to G'$ then $$ G / \ker ( \phi ) \simeq \phi (G) $$ Second Isomorphism Theorem: If $H \le G$ and $N \triangleleft G$ then $$ (HN) / N \simeq H / (H \cap N) $$ Third Isomorphism Theorem: If $H , K \triangleleft G$ and $K \leq H$ then $$</description></item><item><title>The Second Law of Thermodynamics</title><link>https://freshrimpsushi.github.io/en/posts/643/</link><pubDate>Fri, 10 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/643/</guid><description>Laws Clausius: There is no process that transfers heat from a colder body to a hotter body by itself. Kelvin: A process that converts all heat into work is impossible. Explanation The statements by German physicist Clausius and British physicist Kelvin on the Second Law of Thermodynamics are equivalent to each other. The most famous version is by the Greek mathematician Carathéod</description></item><item><title>Proof of the Second Isomorphism Theorem</title><link>https://freshrimpsushi.github.io/en/posts/642/</link><pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/642/</guid><description>Theorem 1 First Isomorphism Theorem: If there exists a homomorphism $\phi : G \to G'$, then $$ G / \ker ( \phi ) \simeq \phi (G) $$ Second Isomorphism Theorem: If $H \le G$ and $N \triangleleft G$, then $$ (HN) / N \simeq H / (H \cap N) $$ Third Isomorphism Theorem: If $H , K \triangleleft G$ and $K \leq H$, then $$ G/H \simeq (G/K) / (H/K)</description></item><item><title>Thermodynamic Derivation of the Heat Insulation Coefficient</title><link>https://freshrimpsushi.github.io/en/posts/638/</link><pubDate>Wed, 08 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/638/</guid><description>Formula Given that $m$ is the mass of a gas molecule, $h$ is the height, and $T$ is the temperature, the following equation holds: $$ \dfrac{dT}{dh} = - {{ \gamma -1} \over { \gamma }} \dfrac{ mg }{k_{B}} $$ Here, $\gamma = \dfrac{C_{p}}{C_{V}}$ is the ratio of the isobaric heat capacity to the isochoric heat capacity. Explanation As is well known, the higher the altitude, the lower the temperature, and</description></item><item><title>Proof of the First Isomorphism Theorem</title><link>https://freshrimpsushi.github.io/en/posts/637/</link><pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/637/</guid><description>Theorems 1 Let $G,G'$ be a group. First Isomorphism Theorem: If there exists a homomorphism $\phi : G \to G'$, then $$ G / \ker ( \phi ) \simeq \phi (G) $$ Second Isomorphism Theorem: If $H \le G$ and $N \triangleleft G$, then $$ (HN) / N \simeq H / (H \cap N) $$ Third Isomorphism Theorem: If $H , K \triangleleft G$ and $K \leq H$, then $$</description></item><item><title>Adiabatic Expansion of an Ideal Gas</title><link>https://freshrimpsushi.github.io/en/posts/636/</link><pubDate>Mon, 06 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/636/</guid><description>Theorem The number of moles being $1$ and in a system of an ideal gas undergoing adiabatic expansion, where the pressure is $p$, and the volume is $V$, then $p V^{\gamma}$ is a constant. At this time $\gamma = \dfrac{C_{p}}{C_{V}}$ is the ratio of the isobaric heat capacity to the isochoric heat capacity. Explanation Adiabatic expansion refers to the expansion where thermal energy does not change. Physically, $\gamma = \dfrac{C_{p}}{C_{V}}$</description></item><item><title>Burnside's Lemma Derivation</title><link>https://freshrimpsushi.github.io/en/posts/634/</link><pubDate>Sun, 05 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/634/</guid><description>Overview Burnside&amp;rsquo;s lemma is a representative application to the actions of a group and the isotropy subgroups that can be immediately used in combinatorics and other fields. Formula 1 For a finite group $G$ and a finite set $X$ that is a $G$-set, let $r$ be the number of orbits of $X$ under $G$. $$ r |G| = \sum_{g \in G} \left| X_{g} \right| $$ Derivation Let the cardinality of</description></item><item><title>Isothermal Expansion of an Ideal Gas</title><link>https://freshrimpsushi.github.io/en/posts/633/</link><pubDate>Sat, 04 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/633/</guid><description>Formulas The number of moles is $1$, and for an ideal gas undergoing isothermal expansion, the thermal energy is $Q$, the temperature is $T$, the volume before expansion is $V_{1}$, and the volume after expansion is $V_{2}$. Then, the following equation holds: $$ \Delta Q = RT \ln \dfrac{V_{2}}{V_{1}} $$ Explanation Isothermal expansion refers to expansion under a condition where the temperature does not change. In this case, the change</description></item><item><title>Isotropic Subgroups</title><link>https://freshrimpsushi.github.io/en/posts/632/</link><pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/632/</guid><description>Definition 1 Let&amp;rsquo;s consider $G$ as a group, and define $X$ as a $G$-set. For $x \in X$ and $g \in G$, let $X_{g} := \left\{ x \in X \ | \ gx = x \right\}$ and $G_{x} := \left\{ g \in G \ | \ gx = x \right\}$. We define the Isotropy Subgroup of $G$ with respect to $x$ as $G_{x}$. Explanation To get a grasp of what</description></item><item><title>Specific Heat Capacity and Heat Capacity at Constant Pressure</title><link>https://freshrimpsushi.github.io/en/posts/631/</link><pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/631/</guid><description>Formulas The following equation holds for an ideal gas system where the amount of substance is $1$, regarding the constant-volume heat capacity $C_{V}$ and constant-pressure heat capacity $C_{p}$. $$ C_{p} = C_{V} + R = {{5} \over {2}} R $$ Explanation Not only do the heat capacities differ depending on whether the process is isochoric or isobaric, but also their relationship fits perfectly in a mathematical sense. Especially, $\gamma :=</description></item><item><title>group Actions</title><link>https://freshrimpsushi.github.io/en/posts/630/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/630/</guid><description>Definition 1 An action of a group $G$ with identity element $e$ on a set $X$ is a binary operation $\ast : G \times X \to X$ that satisfies the following two conditions: (i): For all $x \in X$, there is $ex = x$ (ii): For all $x \in X$ and $g_{1} , g_{2} \in G$, there is $( g_{1} g_{2} ) (x) = g_{1} (g_{2} x)$ We refer to</description></item><item><title>First Law of Thermodynamics</title><link>https://freshrimpsushi.github.io/en/posts/629/</link><pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/629/</guid><description>Laws When work $W$ is applied to a system with thermal energy $Q$, the following equation holds for the internal energy $U$: $$ d U = \delta Q + \delta W $$ $\delta$ indicates an inexact differential. Explanation Since they do not have a primitive function in a clean form, it is necessary to calculate through line integration. It means that it is impossible to know exactly how much the</description></item><item><title>Quotient groups in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/628/</link><pubDate>Mon, 30 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/628/</guid><description>Definition 1 Let&amp;rsquo;s call the set of all cosets of $H \subset G$ as $G / H$. If there exists a well-defined binary operation $\ast$ like $(aH) \ast\ (bH) = (ab) H$, then $\left&amp;lt; G / H , * \right&amp;gt;$ is called a Factor group. Theorem Let&amp;rsquo;s assume $H \leqslant G$. That $H \triangleleft G$ and $G / H$ being a group are equivalent. Description That $H \triangleleft G$ means</description></item><item><title>Heat Capacity</title><link>https://freshrimpsushi.github.io/en/posts/627/</link><pubDate>Sun, 29 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/627/</guid><description>Definition1 The heat $dQ$ required to raise the temperature of an object by $dT$ is called the heat capacity, and it is denoted as follows by following the letter C of capacity. $$ C = \dfrac{dQ}{dT} [\text{J/K}] $$ Explanation Particularly in physics, the heat capacity per unit mass, called specific heat capacity, is not very important. Thermodynamics is more interested in the phenomena that occur in a system in general</description></item><item><title>Tikhonov's Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/626/</link><pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/626/</guid><description>Theorem An index set $\mathscr{A}$ is given. If $\left\{ X_{\alpha} \ | \ \alpha \in \mathscr{A} \right\}$ is a set of compact spaces, then $\displaystyle X : = \prod_{\alpha \in \mathscr{A}} X_{ \alpha}$ is compact. Explanation Though this theorem might seem like a trivial property at first glance, it&amp;rsquo;s actually the opposite. It appears trivial but is surprisingly difficult to prove, making it deserving of its own name. The fact</description></item><item><title>Average Kinetic Energy of Gas Molecules</title><link>https://freshrimpsushi.github.io/en/posts/625/</link><pubDate>Fri, 27 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/625/</guid><description>Formulas1 The average kinetic energy of gas molecules in a system at a temperature of $T$ is as follows. $$ \left\langle E_{K} \right\rangle = {{3} \over {2}} k_{B} T $$ Description Calculating the kinetic energy of each gas molecule and then averaging is not only inefficient but also practically impossible. However, according to this formula derived statistically, the kinetic energy depends solely on temperature, making it easier to obtain. The</description></item><item><title>Proof of the Alexander Subbase Theorem</title><link>https://freshrimpsushi.github.io/en/posts/624/</link><pubDate>Thu, 26 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/624/</guid><description>Theorem Let $X$ be a topological space. $X$ is compact. There exists some subbasis $\mathscr{S}$ for $X$, made up of members of $\iff$ $\mathscr{S}$, such that every open cover of $X$ composed of $X$ has a finite subcover. Explanation The importance of compactness goes without saying. This theorem was originally something Alexander&amp;rsquo;s teacher wanted to prove about bases. However, he couldn&amp;rsquo;t prove it for bases and it was Alexander who</description></item><item><title>Maxwell Distribution</title><link>https://freshrimpsushi.github.io/en/posts/623/</link><pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/623/</guid><description>Theorem1 The random variable $V$ representing the speed of gas molecules follows the probability density function described by the Maxwell distribution. $$ f(v) = \dfrac{4}{\sqrt{ \pi}} \left( \dfrac{m}{2 k_{B} T} \right)^{3/2} v^{2} e^{-mv^2 / 2k_{B}T } $$ Explanation The Maxwell distribution is derived from the Boltzmann distribution and is also called the Maxwell-Boltzmann speed distribution. It is a distribution rarely seen in statistics, so much so that the name statistical</description></item><item><title>Nucleus, Kernel in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/622/</link><pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/622/</guid><description>Definition The kernel of $G, G'$ with respect to the identity element $e, e'$ and the homomorphism $\phi : G \to G'$ is the preimage $ \phi^{-1} [ \left\{ e' \right\} ]$ of $\left\{ e' \right\}$ and is denoted as $\ker \phi $. Theorem [1]: For $g \in G$, $g ( \ker \phi ) = ( \ker \phi ) g$ [2]: $\ker \phi \triangleleft G$ [3]: $\ker \phi = \left\{</description></item><item><title>What is a Pseudovector?</title><link>https://freshrimpsushi.github.io/en/posts/641/</link><pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/641/</guid><description>Description Studying physics, one might come across the term pseudovector or axial vector. The tricky part is that you might encounter pseudovectors without really knowing what they are. It is said that one can study undergraduate physics without understanding pseudovectors, but I&amp;rsquo;ve never seen a textbook that explains it properly. I first encountered pseudovectors as Pseudovectors in a problem in Griffith&amp;rsquo;s electromagnetism. However, just solving problems was not enough to</description></item><item><title>Square-and-Multiply Algorithm Proof</title><link>https://freshrimpsushi.github.io/en/posts/621/</link><pubDate>Mon, 23 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/621/</guid><description>Algorithm Given a natural number $a,k,m$, $b \equiv a^{k} \pmod{m}$ can be computed as follows. Step 1. Binary Expansion of $k$ Represent $u_{i} = 0$ or $u_{i} = 1$ as follows. $$ k = \sum_{i=0}^{r} u_{i} 2^{i} = u_{0} + 2 u_{1} + \cdots + 2^r u_{r} $$ Step 2. Calculate $a^{2^{r}} \equiv ( a^{2^{r-1}} )^2 \equiv A_{r-1}^2 \equiv A_{r} \pmod{m}$ as follows: $$ \begin{align*} a &amp;amp; &amp;amp; &amp;amp; \equiv</description></item><item><title>Cartesian Product of Topological Spaces</title><link>https://freshrimpsushi.github.io/en/posts/620/</link><pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/620/</guid><description>Definition 1 For an index set $\mathscr{A}$, let $\left\{ X_{\alpha} \ | \ \alpha \in \mathscr{A} \right\}$ be a set of topological spaces, and let $O_{\alpha}$ be an open set in $X_{\alpha}$. For the Cartesian product $\displaystyle X := \prod_{\alpha \in \mathscr{A}} X_{ \alpha}$, $p_{\alpha} : X \to X_{\alpha}$ is called the projection. The topology generated by a subbasis $\mathscr{S} : = \left\{ p_{\alpha}^{-1} ( O_{\alpha} ) \ | \</description></item><item><title>The Cartesian Product of groups</title><link>https://freshrimpsushi.github.io/en/posts/619/</link><pubDate>Sat, 21 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/619/</guid><description>Definition 1 2 For groups $G_{1} , \cdots , G_{n}$ and elements $\displaystyle (a_{1},\cdots , a_{n}), (b_{1} , \cdots , b_{n} ) \in \prod_{i=1}^{n} G_{i}$ of their Cartesian product, $$ (a_{1},\cdots , a_{n}) (b_{1} , \cdots , b_{n} ) = (a_{1} b_{1},\cdots , a_{n} b_{n}) $$ then $\displaystyle \prod_{i=1}^{n} G_{i}$ is called the Direct Product of $G_{1} , \cdots , G_{n}$ groups. Especially, if $G_{1}, \cdots , G_{n}$ is an</description></item><item><title>Gas Molecule Count Formula Depending on Height in an Isothermal Atmosphere</title><link>https://freshrimpsushi.github.io/en/posts/618/</link><pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/618/</guid><description>Formulas1 Assuming the temperature $T$ is constant, let&amp;rsquo;s define the number of gas molecules per unit volume $V=1$ at height $h$ as $N(h)$. If the mass of the gas molecules is $m$ and the gravitational acceleration is $g$, the following formula holds. $$ N(h) = N(0) e^{- {{mgh} \over {k_{B} T}} } $$ Explanation Originally, this formula does not stand out in thermodynamics, but it&amp;rsquo;s interesting that there are two</description></item><item><title>Rigorous Proof of Stirling's Approximation Formula</title><link>https://freshrimpsushi.github.io/en/posts/616/</link><pubDate>Wed, 18 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/616/</guid><description>Theorem $$ \lim_{n \to \infty} {{n!} \over {e^{n \ln n - n} \sqrt{ 2 \pi n} }} = 1 $$ Description The Stirling approximation, also known as the Stirling&amp;rsquo;s formula, is widely used in various fields such as statistics and physics. It can also be expressed using the gamma function as follows: $$ \Gamma ( n ) \approx {e^{n \ln n - n} \sqrt{ 2 \pi n}} $$ This proof</description></item><item><title>Boltzmann Distribution</title><link>https://freshrimpsushi.github.io/en/posts/615/</link><pubDate>Tue, 17 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/615/</guid><description>Theorem1 The probability that a system with a temperature of $T$ has energy $\varepsilon$ is as follows. $$ P(\varepsilon) \propto e^{ - \frac{\varepsilon}{k_{B} T} } $$ This distribution is called the Boltzmann distribution. Derivation An ensemble is simply &amp;lsquo;a situation made up of systems&amp;rsquo;. Among them, a canonical ensemble is a situation with a large reservoir and a very small system as shown above. The reservoir is assumed to have</description></item><item><title>Proof of Minkowski's Inequality in Lebesgue Spaces</title><link>https://freshrimpsushi.github.io/en/posts/614/</link><pubDate>Mon, 16 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/614/</guid><description>Theorem[^1] Let&amp;rsquo;s call $\Omega \subset \mathbb{R}^{n}$ an open set. If $1 \le p &amp;lt; \infty$ and $u, v \in L^{p}(\Omega)$, then $$ \left\| u + v \right\|_{p} \le \left\| u \right\|_{p}+\left\| v \right\|_{p} $$ This is called the Minkowski inequality.</description></item><item><title>Definition of temperature in physicss</title><link>https://freshrimpsushi.github.io/en/posts/613/</link><pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/613/</guid><description>Definition1 2 Let&amp;rsquo;s assume there is a system with energy $E$. When the number of microstates for $E$ is denoted as $\Omega (E) = \Omega$, then $$ \dfrac{1}{k_{B} T} := \dfrac{d \ln ( \Omega )}{d E } $$ defines the temperature of the system as $T$, where $k_{B}$ is the Boltzmann constant. Microstates and Macrostates In statistical mechanics, the concepts of a system&amp;rsquo;s Macrostate and Microstate could be similar to</description></item><item><title>Thermodynamics Zeroth Law</title><link>https://freshrimpsushi.github.io/en/posts/612/</link><pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/612/</guid><description>Law1 If system $A,B,C$ is in thermodynamic equilibrium with $A$ and $B$, and $B$ is in thermodynamic equilibrium with $C$, then $A$ and $C$ are also in thermodynamic equilibrium. Description Mathematically, the Zeroth Law of Thermodynamics can be expressed as the &amp;rsquo;transitivity of thermodynamic equilibrium&amp;rsquo;. It is a fundamental law similar to the [First Postulate of Euclidean Geometry $A = B \land B=C \implies A = C$ and underpins many</description></item><item><title>Proof of Hölder's Inequality in Lebesgue Spaces</title><link>https://freshrimpsushi.github.io/en/posts/609/</link><pubDate>Thu, 12 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/609/</guid><description>Theorem1 Let us consider $\Omega \subset \mathbb{R}^{n}$ as an open set. Suppose we&amp;rsquo;re given two constants $1 \lt p \lt \infty, 1 \lt p^{\prime} \lt \infty$ satisfying the following equation. $$ \dfrac{1}{p}+\dfrac{1}{p^{\prime}} = 1 \left(\text{or } p^{\prime} = \frac{p}{p-1} \right) $$ If $u \in L^p(\Omega)$ and $v\in L^{p^{\prime}}(\Omega)$, then $uv \in L^1(\Omega)$, and the inequality below holds. $$ \| uv \|_{1} = \int_{\Omega} |u(x)v(x)| dx \le \| u \|_{p} \|</description></item><item><title>A Simple Derivation of the Stirling Formula</title><link>https://freshrimpsushi.github.io/en/posts/608/</link><pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/608/</guid><description>Formulas The following equation is referred to as Stirling&amp;rsquo;s formula. $$ \lim_{n \to \infty} {{n!} \over {e^{n \ln n - n} \sqrt{ 2 \pi n} }} = 1 $$ Description1 This approximation is useful in the aspect of calculating factorials for large numbers. In fields like thermodynamics and statistical mechanics, it&amp;rsquo;s essential to assume a large number of molecules, $$ \begin{align*} n! &amp;amp;\approx \sqrt{2\pi n}\left( \dfrac{n}{e} \right)^{n} \\[0.6em] \log_{2} n!</description></item><item><title>Cantor Set</title><link>https://freshrimpsushi.github.io/en/posts/607/</link><pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/607/</guid><description>Definition Let $$ \begin{align*} I =&amp;amp; \left[ 0, 1 \right] \\ C_{1} =&amp;amp; \left[ 0, {{1} \over {3}} \right] \cup \left[ {{2} \over {3}} , 1 \right] \\ C_{2} =&amp;amp; \left[ 0, {{1} \over {3^2}} \right] \cup \left[ {{2} \over {3^2}}, {{3} \over {3^2}} \right] \cup \left[ {{6} \over {3^2}}, {{7} \over {3^2}} \right] \cup \left[ {{8} \over {3^2}} , 1 \right] \\ &amp;amp;\vdots \\ C_{n} =&amp;amp; \left[ 0, {{1}</description></item><item><title>Distinguishing Between W and Omega in Mathematics and Physics Textbooks</title><link>https://freshrimpsushi.github.io/en/posts/611/</link><pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/611/</guid><description>Description When reading textbooks on physics and mathematics, it can be confusing to tell whether a symbol is omega or double-u. For sophomores who have just started studying their major subjects, some might not even be aware of omega and think it’s always double-u. In physics, it’s most likely omega when you encounter it as a symbol for angular frequency. Personally, I feel like</description></item><item><title>Lp Spaces, Lebesgue Spaces</title><link>https://freshrimpsushi.github.io/en/posts/605/</link><pubDate>Sun, 08 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/605/</guid><description>Definition1 2 3 Let $\Omega \subset \mathbb{R}^{n}$ be an open set, and $p$ be a positive real number. For all measurable functions $f$ defined on $\Omega$, define set $L^{p}(\Omega)$ as follows. $$ L^{p}(\Omega) := \left\{ f : \int_{\Omega} \left| f(x) \right|^{p} dx &amp;lt; \infty \right\} $$ This is called the Lp space or Lebesgue space and is briefly denoted as $L^{p}$. Typically, textbooks on functional analysis describe it as above,</description></item><item><title>The Proof of Baire's Category Theorem</title><link>https://freshrimpsushi.github.io/en/posts/604/</link><pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/604/</guid><description>Definition A Baire Space is a dense space for every sequence of all dense open subsets $\displaystyle \bigcap_{n=1}^{\infty} O_{n}$ of a topological space $X$.</description></item><item><title>Finding the Equivalent Resistance of Parallel Circuits Easily</title><link>https://freshrimpsushi.github.io/en/posts/603/</link><pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/603/</guid><description>Buildup Imagine calculating the equivalent resistance of the circuit shown above. Of course, if we change it into a parallel circuit like below, we can find the answer through the formula. When there are $n$ resistors, the formula for resistance in parallel is $\displaystyle {{1} \over {R}} = {{1} \over {R_{1}}} + {{1} \over {R_{2}}} + \cdots + {{1} \over {R_{n}}}$. Substituting the resistors into the formula gives us $$</description></item><item><title>Ideal Gas Equation</title><link>https://freshrimpsushi.github.io/en/posts/602/</link><pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/602/</guid><description>Formulas1 Let&amp;rsquo;s denote the number of molecules of a gas as $N$, volume as $V$, pressure as $p$, and absolute temperature as $T$. Then, the following equation holds, and this is called the ideal gas equation. $$ pV = N k_{B} T $$ Here, $k_{B} = 1.3807 \times 10^{-23} J / K$ is called the Boltzmann constant. Description Historically, it was derived from experimental laws and later derived mathematically from</description></item><item><title>Wallis Product</title><link>https://freshrimpsushi.github.io/en/posts/601/</link><pubDate>Wed, 04 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/601/</guid><description>Theorem $$ \prod_{n=1}^{\infty} {{4n^2} \over {4n^2 - 1}} = \lim_{n \to \infty} {{2 \cdot 2 } \over { 1 \cdot 3 } } \cdot {{4 \cdot 4 } \over { 3 \cdot 5 } } \cdot \cdots \cdot {{2n \cdot 2n } \over { (2n-1) \cdot (2n+1) } } = {{ \pi } \over {2}} $$ Explanation It is undeniably intriguing and useful to know that not only through series</description></item><item><title>Euler's Proof of the Perfect Number Theorem</title><link>https://freshrimpsushi.github.io/en/posts/600/</link><pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/600/</guid><description>Theorem 1 If an even number $n = 2^{p-1} (2^p - 1)$ is a perfect number, then $2^{p}-1$ is a Mersenne prime. Explanation At first glance, it might seem like the inverse of the Euclid&amp;rsquo;s perfect number formula, but the difference is that it only mentions even numbers. However, this theorem tells almost everything about perfect numbers, mainly because no odd perfect number has been discovered yet. The only known</description></item><item><title>Reason for Defining the Inner Product of Functions via Definite Integration</title><link>https://freshrimpsushi.github.io/en/posts/599/</link><pubDate>Mon, 02 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/599/</guid><description>Buildup The general definition of the inner product is as follows: Let $H$ be a vector space. For $x,y,z \in H$ and $\alpha, \beta \in \mathbb{C}$, a function that satisfies the following conditions $$ \langle \cdot , \cdot \rangle \ : \ H \times H \to \mathbb{C} $$ is defined as an inner product, and $\left( H, \langle \cdot ,\cdot \rangle \right)$ is called an inner product space. Linearity: $\langle</description></item><item><title>Point Compactification</title><link>https://freshrimpsushi.github.io/en/posts/598/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/598/</guid><description>Definition 1 Let&amp;rsquo;s assume we have a topological space $(X , \mathscr{T})$ and call it $\infty \notin X$. For $X_{\infty} := X \cup \left\{ \infty \right\}$, a topology $\mathscr{T}_{\infty}$ defined by $(X_{\infty } , \mathscr{T}_{\infty} )$, which satisfies the following two conditions, is called the One-Point Compactification of $(X, \mathscr{T})$. (i): $\infty \notin U \implies U \in \mathscr{T}_{\infty}$ and $U \in \mathscr{T}$ are equivalent. (ii): It is equivalent for $\infty</description></item><item><title>Proof of Lagrange's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/597/</link><pubDate>Sat, 30 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/597/</guid><description>Theorem 1 If $H$ is a subgroup of a finite group $G$, then $|H|$ is a divisor of $|G|$. Proof Upon slight consideration, it&amp;rsquo;s logically inevitable and the proof is accordingly simple. Each coset has the same number of elements. Since $H$ is also one of the cosets of $G$, the cardinality of the cosets of $H$ is $|H|$. As the cosets partition $G$, summing the cardinalities of all cosets</description></item><item><title>Cauchy-Schwarz Inequality in Lebesgue Spaces</title><link>https://freshrimpsushi.github.io/en/posts/596/</link><pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/596/</guid><description>Theorem1 If $f,g \in L^{2} (E)$ then $fg \in L^{1}(E)$ and the following holds: $$ \left| \int_{E} f \overline{g} dm \right| \le \left\| f g \right\|_{1} \le \left\| f \right\|_{2} \left\| g \right\|_{2} $$ Here, $\| \cdot \|_{2}$ is the norm of the $L^{2}$ space, $\| \cdot \|_{1}$ is the norm of the $L^{1}$ space. Explanation If one has studied functional analysis, the reason this inequality is named after Cauchy-Schwarz</description></item><item><title>L2 Space</title><link>https://freshrimpsushi.github.io/en/posts/594/</link><pubDate>Wed, 27 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/594/</guid><description>Definition 1 A function space $L^{2}$ is defined as follows. $$ L^{2} (E) := \left\{ f : \left( \int_{E} | f |^2 dm \right)^{{1} \over {2}} &amp;lt; \infty \right\} $$ Properties $L^{2}$ is a metric space. The metric is defined as following. $$ d(f,g) := \left( \int \left| f(x) - g(x) \right|^{2}dx \right)^{\frac{1}{2}} = \left\| f-g \right\|_{2} = \sqrt{\braket{f-g, f-g}} $$ $L^{2}$ is a vector space. $L^{2}$ is a normed</description></item><item><title>Euclid's Derivation of the Perfect Number Formula</title><link>https://freshrimpsushi.github.io/en/posts/593/</link><pubDate>Tue, 26 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/593/</guid><description>Formula 1 If $2^{p}-1$ is a prime number, then $2^{p-1}(2^{p} - 1)$ is a perfect number. Explanation It is not certain that all perfect numbers will have this form, but this form is definitely a perfect number. For example, for the prime number $(2^2 -1) = 3$, $2^{2-1}(2^2 -1) = 6$ is a perfect number.The fact that perfect numbers and Mersenne primes have this relationship could be somewhat inferred from</description></item><item><title>L1 Space</title><link>https://freshrimpsushi.github.io/en/posts/592/</link><pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/592/</guid><description>Definition1 A function space $L^{1}$ is defined as follows. $$ L^{1} (E) := \left\{ f : E \to \mathbb{R} \Big \vert \int_{E} | f | dm \lt \infty \right\} $$ Properties $L^{1}$ is a vector space. $L^{1}$ is a normed space. The norm is defined as follows: $$ \left\| f \right\|_{1} := \int \left| f(x) \right| dx $$ $L^{1}$ is a complete space. Explanation Space $L^{1}$ is a special case</description></item><item><title>Definition and Test Method of Subgroups</title><link>https://freshrimpsushi.github.io/en/posts/589/</link><pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/589/</guid><description>Definition1 group $G$의 subset $H$가 $G$의 연산에 대해서 group일 때, $H$를 $G$의 subgroup이라 하고 다음과 같이 표기한다. $$ H \le G $$ Explanation If $H$ is a subgroup</description></item><item><title>Proof of the Peano Space-Filling Theorem</title><link>https://freshrimpsushi.github.io/en/posts/586/</link><pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/586/</guid><description>Theorem 1 A surjective continuous function $f : I \to I \times I$ exists for $I = [0,1]$. Explanation This is a short yet shockingly profound theorem. If this theorem is true, it suggests that planes can be formed solely with lines, a concept that is tough to accept even after seeing the proof. The term &amp;ldquo;Space-filling curve theorem&amp;rdquo; is arbitrarily used here, initially translated in Japan. Proof Part 1.</description></item><item><title>Löb's Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/585/</link><pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/585/</guid><description>Definition Let $\mathscr{O}$ be an open cover of the metric space $(X,d)$. $\sup \left\{ d(a,b) \ | \ a,b \in A \right\} &amp;lt; \varepsilon$ is said to be the Lebesgue Number for $\mathscr{O}$ if every subset $A \subset X$ that satisfies $\sup \left\{ d(a,b) \ | \ a,b \in A \right\} &amp;lt; \varepsilon$ also satisfies $A \subset O$ for some $O \in \mathscr{O}$. Theorem 1 [1] Lebesgue&amp;rsquo;s Lemma: If $X$</description></item><item><title>Solution to the Initial Value Problem for the Wave Equation with Dirichlet Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/583/</link><pubDate>Wed, 20 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/583/</guid><description>Explanation $$ \begin{cases} u_{tt} = c^2 u_{xx} \\ u(0,x) = f(x) \\ u_{t}(0,x) = g(x) \\ \end{cases} $$ The equation above is in a $1$-dimensional space with a length of $l$, under the Dirichlet boundary condition from the wave equation. $$ \begin{cases} u(t,0) = \alpha (t) \\ u(t,l) = \beta (t) \end{cases} $$ It&amp;rsquo;s given when $\alpha = \beta = 0$ and there&amp;rsquo;s an initial condition regarding the waveform. Among</description></item><item><title>Solution to the Cauchy Problem for the Wave Equation</title><link>https://freshrimpsushi.github.io/en/posts/582/</link><pubDate>Tue, 19 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/582/</guid><description>Description $$ \begin{cases} u_{tt} = c^2 u_{xx} \\ u(0,x) = f(x) \\ u_{t}(0,x) = g(x) \end{cases} $$ This equation is a case where both the density $\rho (x) &amp;gt; 0$ and stiffness $\kappa (x) &amp;gt; 0$ are constants in the following wave equation, referring to $\displaystyle c : = {{\kappa} \over {\rho}}$ as the wave speed. Here, $t$ is time, $x$ is position, and $u(t,x)$ represents the waveform at time</description></item><item><title>Solution to the Initial Value Problem for the Heat Equation Given Dirichlet Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/581/</link><pubDate>Mon, 18 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/581/</guid><description>Description $$ \begin{cases} u_{t} = \gamma u_{xx} \\ u(t,0) = u(t,l) = 0 \\ u(0,x) = f(x) \end{cases} $$ The above equation is a Dirichlet boundary condition in a $1$-dimensional space with length of $l$ from Heat Equation $$ \begin{cases} u(t,0) = \alpha (t) \\ u(t,l) = \beta (t) \end{cases} $$ This is given by $\alpha = \beta = 0$ with initial conditions for the heat distribution. Among such problem</description></item><item><title>Solutions to Heat Equations</title><link>https://freshrimpsushi.github.io/en/posts/580/</link><pubDate>Sun, 17 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/580/</guid><description>Description $$ u_{t} = \gamma u_{xx} $$ The above equation is a specialized case of the generalized heat equation $$ {{\partial} \over {\partial t}} \left( \sigma (x) u \right) = {{\partial} \over {\partial x }} \left( \kappa (x) {{\partial u} \over {\partial x}} \right) $$ where the thermal conductivity $\kappa (x) &amp;gt; 0$ and the heat capacity $\sigma (x) &amp;gt; 0$ are both constants, leading to $\displaystyle \gamma : =</description></item><item><title>Solutions to Partial Differential Equations Using Fourier Series</title><link>https://freshrimpsushi.github.io/en/posts/579/</link><pubDate>Sat, 16 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/579/</guid><description>Definition For the function $f \in \mathcal{L}^{2} [- \pi , \pi] $ in the Hilbert space, and for $\displaystyle a_{k} = {{1} \over {\pi}} \int_{- \pi}^{\pi} f(x) \cos kx dx$ and $\displaystyle b_{k} = {{1} \over {\pi}} \int_{- \pi}^{\pi} f(x) \sin kx dx$, $$ f(x) \sim {{a_{0}} \over {2}} + \sum_{k=1}^{\infty} \left( a_{k} \cos kx + b_{x} \sin kx \right) $$ is called the Fourier series of $f$. Description Unlike</description></item><item><title>Solution to the Riemann Problem for the Burgers' Equation</title><link>https://freshrimpsushi.github.io/en/posts/575/</link><pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/575/</guid><description>Description $$ \begin{cases} u_{t} + u u_{x} = 0 &amp;amp; , t&amp;gt;0 \\ u(t,x) = \begin{cases} a &amp;amp; ,x&amp;lt;0 \\ b &amp;amp; ,x&amp;gt;0 \end{cases} &amp;amp; , t=0 \end{cases} $$ The Riemann problem refers to the case where the solution to the Burgers&amp;rsquo; equation, given an initial value, is expressed as a step function. In this case, if we have $a \ne b$, the obtained solution would have multiple or no</description></item><item><title>Various Distribution Functions in R</title><link>https://freshrimpsushi.github.io/en/posts/578/</link><pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/578/</guid><description>Description In R, functions related to specific distributions are made up of a combination of prefixes and suffixes. Prefixes Let the probability distribution function of distribution $X$ be called $f(x)$. r-: for random sampling, think of sampling $x_{1}, \cdots , x_{n}$ from distribution $X$. d-: for the density function, $f(x)$. p-: for the cumulative distribution function, $F(x) = \displaystyle \int_{\infty}^{x} f(t) dt$. q-: for the quantile function, $F^{-1}(\alpha)$. Suffixes Almost</description></item><item><title>Rounding Up, Down, and to a Specific Number of Digits in R</title><link>https://freshrimpsushi.github.io/en/posts/577/</link><pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/577/</guid><description>Overview The ceiling() function performs rounding up, while the floor() function performs rounding down. These functions might seem unnecessary in R, which is often used for statistics, but they are surprisingly convenient for data handling. Description The trunc() function drops everything below the decimal point in a similar manner but returns a value closer to $0$. Both the round() and signif() functions keep digits, but round() focuses on the digits</description></item><item><title>Bolzano-Weierstrass Property and Compactness of Accumulation Points</title><link>https://freshrimpsushi.github.io/en/posts/576/</link><pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/576/</guid><description>Definition 1 If every infinite subset of a topological space $X$ has its limit point in $X$, then $X$ is said to have the Bolzano-Weierstrass property or to be compactly accumulating points. Theorem [1]: Every compact space is a compactly accumulating points space. [2]: If $X$ is a metric space, then $X$ being compact is equivalent to being compactly accumulating points. $X$ is a metric space, then it being compact</description></item><item><title>Additive Compactness and Lindelöf Spaces</title><link>https://freshrimpsushi.github.io/en/posts/574/</link><pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/574/</guid><description>Definition 1 If every countable open cover of $X$ has a finite subcover, then $X$ is called countably compact. If every open cover of $X$ has a countable subcover, then $X$ is called Lindelöf. Theorem Countably Compact [1-1]: Every compact space is a countably compact space. [1-2]: Countable compactness is a topological property. Lindelöf [2-1]:</description></item><item><title>Lebesgue Integral as a Generalization of Riemann Integral</title><link>https://freshrimpsushi.github.io/en/posts/573/</link><pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/573/</guid><description>Theorem 1 Let&amp;rsquo;s denote bounded functions as $f : [a,b] \to \mathbb{R}$ and $g : \mathbb{R} \to [0,\infty)$. [1]: $f$ being Riemann integrable over $[a,b]$ is equivalent to $f$ being almost everywhere continuous with respect to the Lebesgue measure on $[a,b]$. [2]: If $\displaystyle \int_{a}^{b} f(x) dx$ exists, then $\displaystyle \int_{a}^{b} f(x) dx = \int_{[a,b]} f dm$ [3]: If $\displaystyle \int_{-\infty}^{\infty} g(x) dx$ exists, then $\displaystyle \int_{-\infty}^{\infty} g(x) dx =</description></item><item><title>Web Scraping and Removing Tags with Python</title><link>https://freshrimpsushi.github.io/en/posts/572/</link><pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/572/</guid><description>Overview Python is well equipped with packages for crawling, making it easy to follow along. Let&amp;rsquo;s try reading a web page and removing the html tags. Example Code import requests from bs4 import BeautifulSoup import re rq = requests.get(&amp;#34;https://ko.wikipedia.org/wiki/%EC%98%A4%EB%A7%88%EC%9D%B4%EA%B1%B8&amp;#34;) rqctnt = rq.content soup = BeautifulSoup(rqctnt,&amp;#34;html.parser&amp;#34;) OMG = str(soup.find\_all(&amp;#34;p&amp;#34;)) OMG = re.sub(&amp;#39;&amp;lt;.+?&amp;gt;&amp;#39;, &amp;#39;&amp;#39;, OMG, 0).strip() Result For an example, let&amp;rsquo;s read the Oh My Girl entry from Wikipedia. The necessary packages</description></item><item><title>Confusion Matrix, Sensitivity, and Specificity</title><link>https://freshrimpsushi.github.io/en/posts/571/</link><pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/571/</guid><description>Definition Let&amp;rsquo;s assume we have a model for distinguishing between positive $P$ and negative $N$ in a classification problem. We define the number of positives correctly predicted as true positives $TP$, the number of negatives correctly predicted as true negatives $TN$, the number of positives incorrectly predicted as negatives as false negatives $FN$, and the number of negatives incorrectly predicted as positives as false positives $FP$. Confusion Matrix In classification</description></item><item><title>Uniform Continuity Theorem</title><link>https://freshrimpsushi.github.io/en/posts/570/</link><pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/570/</guid><description>Definition Given a metric space $(X, d)$ and $(Y, d&amp;rsquo;)$, let&amp;rsquo;s say $f : X \to Y$. If for every $\varepsilon &amp;gt; 0$ and $x_{1}, x_{2} \in X$ there exists a $\delta &amp;gt; 0$ satisfying $$ d(x_{1}, x_{2}) &amp;lt; \delta \implies d&amp;rsquo;( f( x_{1} ) , f( x_{2} ) ) &amp;lt; \varepsilon $$ then $f$ is said to be Uniformly Continuous. Explanation Just like how the concept of continuity learned</description></item><item><title>Rankine-Hugoniot Condition and Entropy Condition</title><link>https://freshrimpsushi.github.io/en/posts/569/</link><pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/569/</guid><description>Definition $$ \begin{cases} u_{t} + u u_{x} = 0 &amp;amp; , t&amp;gt;0 \\ u(t,x) = f(x) &amp;amp; , t=0 \end{cases} $$ Let&amp;rsquo;s say the solution of the given inviscid Burgers&amp;rsquo; equation is $u$, and its rupture time is $t_{\ast}$. When the solution of the inviscid Burgers&amp;rsquo; equation ruptures, it is connected by a line segment so that the areas to the left and right become equal, as shown above. This</description></item><item><title>Handling Strings in R</title><link>https://freshrimpsushi.github.io/en/posts/568/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/568/</guid><description>Overview While not to the extent commonly seen in languages popular among developers, there is a surprisingly frequent need to handle strings in R. The more vast and unruly the data, the more crucial these minor techniques become. Tips The nchar() function simply returns the length of a string. Those familiar with other languages would probably have tried length first. The substring() function, as its name easily suggests, returns a</description></item><item><title>Sigma Functions in Number Theory</title><link>https://freshrimpsushi.github.io/en/posts/567/</link><pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/567/</guid><description>Theorem For $\displaystyle \sigma (n) : = \sum_{d \mid n} d$, the following holds: [1]: For a prime number $p$, $$\sigma ( p^k ) = {{p^{k+1} - 1} \over {p-1}}$$ [2]: If $\gcd (n , m ) = 1$, then $$\sigma (nm) = \sigma (n) \sigma (m)$$ Description The sigma function, simply put, is the sum of divisors, for example, for $6$, it is $\sigma (6) = 1 + 2</description></item><item><title>Mass Conservation Law in the Inviscid Burgers' Equation</title><link>https://freshrimpsushi.github.io/en/posts/566/</link><pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/566/</guid><description>Theorem $$ \begin{cases} u_{t} + u u_{x} = 0 &amp;amp; , t&amp;gt;0 \\ u(t,x) = f(x) &amp;amp; , t=0 \end{cases} $$ Considering the solution $u$ of the inviscid Burgers&amp;rsquo; equation up to the interval $[a,b]$, let&amp;rsquo;s define the mass $M$ in the following way. $$ M_{a,b}(t) := \int_{a}^{b} u(t,x) dx $$ And if we call the burst time $t_{\ast}$, the following holds for $t \in ( 0 , t_{\ast})$. $$</description></item><item><title>Proof of Levy's Theorem in Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/564/</link><pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/564/</guid><description>Theorem 1 If $\displaystyle \sum_{k=1}^{\infty} \int |f_{k}| dm &amp;lt; \infty$, then $\displaystyle \sum_{k=1}^{\infty} f_{k} (x)$ converges almost everywhere and is Lebesgue integrable with the integration explicitly as follows. $$ \int \sum_{k=1}^{\infty} f_{k} dm = \sum_{k=1}^{\infty} \int f_{k} dm $$ Proof Part 1. That $\displaystyle \sum_{k=1}^{\infty} f_{k} (x)$ converges almost everywhere and is Lebesgue integrable If we define $\displaystyle \phi (x) := \sum_{k=1}^{\infty} | f_{k} (x) |$, then $\phi$ is a</description></item><item><title>Proof of the Maximum and Minimum Value Theorem in Topological Spaces</title><link>https://freshrimpsushi.github.io/en/posts/563/</link><pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/563/</guid><description>Theorem 1 For a compact space $X$, if a function $f : X \to \mathbb{R}$ is continuous, then for every $x \in X$, there exists a $c,d \in X$ that satisfies $f(c) \le f(x) \le f(d)$. Explanation In $\mathbb{R}$, being compact is equivalent to being a closed interval $[a,b]$, so ultimately this generalizes the theorem we learned in high school and analysis. As much as it uses the difficult theories</description></item><item><title>Useful Properties of Compact Spaces and Continuous Functions</title><link>https://freshrimpsushi.github.io/en/posts/561/</link><pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/561/</guid><description>Theorems Let us assume that $f : X \to Y$, with $X$ being compact, and $f$ being continuous. [1]: If $f$ is surjective, $Y$ is compact. Even if $f$ is not surjective, $f(X)$ is still compact. [2]: If $Y$ is Hausdorff, then $f$ is a closed function. For a closed set $C \subset X$, $f(C) \subset Y$ is also a closed set. [3]: If $f$ is bijective and $Y$ is</description></item><item><title>Zipf's Law</title><link>https://freshrimpsushi.github.io/en/posts/560/</link><pubDate>Sat, 19 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/560/</guid><description>Laws Given a corpus, if the relative frequency of the $k$th most common word is denoted as $f_{k}$, then $$ f_{k} = {{C} \over {k}} $$ Explanation Here, $C$ is a normalization factor that makes $\displaystyle \sum_{k} f_{k} = 1$ possible. If shown as a histogram, the shape is roughly as described above, with the scale adjusted so the total area precisely equals $1$. The thick tail shape that appears</description></item><item><title>Heaps' Law</title><link>https://freshrimpsushi.github.io/en/posts/559/</link><pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/559/</guid><description>Law Given the number of unique words as $M$, and the number of tokens as $T$ in a corpus, $$ M = kT^{b} $$ Explanation When the corpus is in English, the constant $k,b$ is typically $10 \le k \le 100$, and about $b = 0.5$. Heaps&amp;rsquo; law is not derived from a mathematical foundation but empirically obtained. The formula may seem quite complex at first glance, but if both</description></item><item><title>How to Use Bootstrap Functions in R</title><link>https://freshrimpsushi.github.io/en/posts/558/</link><pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/558/</guid><description>Overview You can write your code to perform bootstrap in R, but you can also use the functions that are provided by default. The process is simple as below, but it has many differences in how to use it compared to other functions, so it might feel very unfamiliar at first. Guide Step 1. Define a function boot.fn() that returns the statistic you want to obtain. Of course, the name</description></item><item><title>Proof of the Dominated Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/557/</link><pubDate>Wed, 16 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/557/</guid><description>Theorem 1 Given measurable sets $E \in \mathcal{M}$ and $g \in \mathcal{L}^{1} (E)$, let the sequence of measurable functions $\left\{ f_{n} \right\}$ satisfy $|f_{n}| \le g$ almost everywhere in $E$. If, almost everywhere in $E$, $\displaystyle f = \lim_{n \to \infty} f_{n}$ then $f \in \mathcal{L}^{1}(E)$. $$ \lim_{ n \to \infty} \int_{E} f_{n} (x) dm = \int_{E} f dm $$ $f,g \in \mathcal{L}^{1} (E)$ means that $f$ and $g$ are</description></item><item><title>Mersenne Primes</title><link>https://freshrimpsushi.github.io/en/posts/552/</link><pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/552/</guid><description>Definition 1 If $M_{n} = 2^{n} - 1$ is a prime number, $M_{n}$ is called a Mersenne Prime. Explanation The discovery of Mersenne primes begins with the exploration of whether $p=x^{n}-1$ is a prime number. One immediately realizes that if $x$ is odd, except for $p=2$, it cannot be a prime. Also, since $$ x^{n}-1 = (x-1) ( x^{n-1} + x^{n-2} + \cdots + x^2 + x + 1 )</description></item><item><title>Differences between the Monte Carlo Method and Bootstrapping</title><link>https://freshrimpsushi.github.io/en/posts/551/</link><pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/551/</guid><description>Overview Monte Carlo methods involve repeating simulations with arbitrary data to test new techniques, while bootstrapping involves resampling from actual data to solve problems more cost-effectively. Definitions Monte Carlo Method is a method of finding point estimates for a target by drawing random samples. Bootstrap is a method that involves resampling from a sample to understand the distribution of a target. Explanation The primary confusion may come from the fact</description></item><item><title>Design Matrix</title><link>https://freshrimpsushi.github.io/en/posts/550/</link><pubDate>Sun, 13 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/550/</guid><description>Buildup Let&amp;rsquo;s load the built-in data faithful in R and check it with the head() function. Though only six, at a glance, eruptions and waiting seem to have a positive correlation. It would be nice if their relationship could be represented by some two constants $\beta_{0}, \beta_{1}$ such that $$\text{(eruptions)} = \beta_{0} + \beta_{1} \cdot \text{(waiting) }$$ The above equation represents the linear relationship between the two variables as the</description></item><item><title>Gauss's Theorem, Divergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/565/</link><pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/565/</guid><description>Theorem1 The following holds for a 3-dimensional vector function $\mathbf{F}$: $$ \begin{equation} \int_{\mathcal{V}} \nabla \cdot \mathbf{F} dV = \oint_{\mathcal{S}} \mathbf{F} \cdot d \mathbf{S} \label{1} \end{equation} $$ Here, $\nabla \cdot \mathbf{F}$ is divergence, $\int_{\mathcal{V}}$ is volume integration, and $\oint_{\mathcal{S}}$ is closed surface integration. Description This is called Gauss&amp;rsquo;s theorem, Green&amp;rsquo;s theorem, or divergence theorem. The divergence theorem is especially used in electromagnetics. Mathematical Meaning Mathematically, it means that a surface integral</description></item><item><title>Lebesgue Integrable</title><link>https://freshrimpsushi.github.io/en/posts/549/</link><pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/549/</guid><description>Definition 1 Basic Properties [1]: An integrable function is a measurable function. [2]: If $f \in \mathcal{L}^{1} (E)$ then $\displaystyle \left| \int_{E} f dm \right| \le \int_{E} | f | dm$ [3]: If $f \in \mathcal{L}^{1} (E) $ and $c \in \mathbb{R}$ then $\displaystyle \int_{E} (c f) dm = c \int_{E} f dm$ [4]: If $f,g \in \mathcal{L}^{1} (E) $ then $\displaystyle \int_{E} ( f + g ) dm =</description></item><item><title>What is Regression Analysis?</title><link>https://freshrimpsushi.github.io/en/posts/548/</link><pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/548/</guid><description>Description Regression analysis is so ubiquitous a foundation of nearly all statistical techniques that it is often described either too generally or too specifically. If one were to explain what regression analysis is in a sentence for someone curious, it could be described as a method for discovering the relationships between variables. This useful and astonishing method of analysis was born from the ideas of Francis Galton, the father of</description></item><item><title>Properties of a Line Passing Through the Focus of a Parabola</title><link>https://freshrimpsushi.github.io/en/posts/562/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/562/</guid><description>Theorem For the parabola $y^2 = 4px$, if a line passing through the focus $P(p,0)$ intersects the parabola at two points, respectively $A, B$, then $$ {{1} \over {\overline{PA}} } + {{1} \over {\overline{PB}} } = {{1} \over {p}} $$ Proof Case 1. $a=b$ The case where the line passing through the focus is $x = p$. Since $\overline{PA} = \overline{PB} = 2p$, $$ {{1} \over {\overline{PA}} } + {{1}</description></item><item><title>Sample Standard Deviation and Standard Error Distinguished</title><link>https://freshrimpsushi.github.io/en/posts/541/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/541/</guid><description>Definition Let&amp;rsquo;s call the data obtained from $X$ as $\mathbf{x} = ( x_{1}, x_{2}, \cdots , x_{n} )$. Sample Mean: $$ \overline{x} = {{1} \over {n}} \sum_{i=1}^{n} x_{i} $$ Sample Standard Deviation: $$ s_{x} = \sqrt { {{1} \over {n-1}} \sum_{i=1}^{n} ( x_{i} - \overline{x} )^2 } $$ Standard Error: $$ \text{s.e.} \left( \overline{X} \right) = {{ s_{x} } \over { \sqrt{n} }} $$ Explanation Because the terms sound similar,</description></item><item><title>Solution to Clairaut's Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/556/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/556/</guid><description>Definition The 1st order nonlinear differential equation below is called Clairaut&amp;rsquo;s equation. $$ y=xy^\prime+f(y^\prime ) $$ Explanation The Clairaut differential equation is comparatively easier to solve than other nonlinear differential equations such as the Bernoulli differential equation or the Riccati differential equation. Solution Differentiate both sides of the given differential equation $y=xy^\prime+f(y^\prime )$ and then organize. $$ \begin{align*} &amp;amp;&amp;amp; y^\prime = y^\prime+xy^{\prime \prime} + y^{\prime \prime}f^\prime(y^\prime ) \\ \implies &amp;amp;&amp;amp;</description></item><item><title>How to Draw a Power Function Graph in R</title><link>https://freshrimpsushi.github.io/en/posts/538/</link><pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/538/</guid><description>Overview This section introduces a simple way to draw the graph of a univariate function. As an appropriate example in statistics, let&amp;rsquo;s draw a power function. Definition Let&amp;rsquo;s define the rejection region $C_{\alpha}$ at the significance level $\alpha$ for the null hypothesis $H_{0} : \theta \in \Theta_{0}$ and the alternative hypothesis $H_{1} : \theta \in \Theta_{1}$. The function $\gamma_{C_{\alpha}}(\theta) : = P_{\theta} [ \mathbf{x} \in C_{\alpha} ]$ for the true</description></item><item><title>Riccati Differential Equation Solutions</title><link>https://freshrimpsushi.github.io/en/posts/555/</link><pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/555/</guid><description>Definition The first-order nonlinear differential equation below is called the Riccati equation. $$ y^\prime = P(x)y+Q(x)y^2+R(x) $$ Explanation If $y_{1}$ is known as a particular solution, the general solution is represented in the form of $y=y_{1}+u(x)$. Here, $u(x)$ is an arbitrary constant, and it can be obtained by solving the Bernoulli differential equation when $n=2$. Solution The Riccati equation looks too complicated at first glance to solve. Hence, we need</description></item><item><title>Easy Definition of P-Value or Significance Probability</title><link>https://freshrimpsushi.github.io/en/posts/537/</link><pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/537/</guid><description>Definition 1 The probability of rejecting the null hypothesis in hypothesis testing is called the p-value. Explanation If the p-value is lower than the significance level, it is considered that the null hypothesis has been rejected. A small p-value under the null hypothesis can be understood as &amp;rsquo;the evidence against the null hypothesis is too strong to be attributed to chance.' When terms like power curve or rejection region are</description></item><item><title>Independence Does Not Imply No Correlation</title><link>https://freshrimpsushi.github.io/en/posts/536/</link><pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/536/</guid><description>Description If variables are independent, it means there is no correlation, but lack of correlation does not necessarily imply independence. The case when variables are independent if there is no correlation, that is when it is a necessary and sufficient condition, is when the random variables follow a normal distribution. In the case on the left, there is positive correlation, and in the case on the right, there is negative</description></item><item><title>Solution to the Bernoulli Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/554/</link><pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/554/</guid><description>Definition The following first-order nonlinear differential equation is called the Bernoulli equation. $$ y^\prime + p(x)y = q(x)y^n $$ Here, $n$ is an integer greater than or equal to $2$, and when $n=0,\ 1$, it is a linear equation. Description It’s worth noting that the Bernoulli of the Bernoulli differential equation and the Bernoulli of the widely known Bernoulli&amp;rsquo;s principle in fluid dynamics are different people. The</description></item><item><title>Methods for Finding the Second Solution of Second Order Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/553/</link><pubDate>Sun, 06 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/553/</guid><description>Description1 $$ \begin{equation} y^{\prime \prime }+p(t)y^{\prime} + q(t)y=0 \end{equation} $$ Given the differential equation above, assume we know one solution $y_{1}$. Let&amp;rsquo;s assume the general solution is $y(t)=\nu (t) y_{1}(t)$. If we calculate the 1st and 2nd derivatives of $y$, we get the following. $$ \begin{align*} y^{\prime} &amp;amp;= \nu^{\prime} y_{1} + \nu y_{1}^{\prime} \\ y^{\prime \prime} &amp;amp;= \nu ^{\prime \prime}y_{1} + \nu^{\prime} y_{1}^{\prime} + \nu^ \prime y_{1}^{\prime} + \nu y_{1}^{\prime</description></item><item><title>Monotone Convergence Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/535/</link><pubDate>Sun, 06 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/535/</guid><description>Theorem 1 Let us assume that a sequence $\left\{ f_{n} \right\}$ of measurable functions with non-negative values satisfies $f_{n} \nearrow f$. Then $$ \lim_{n \to \infty} \int_{E} f_{n} dm = \int_{E} f dm $$ Explanation $f_{n} \nearrow f$ means that for all $x$, if $f_{n}(x) \le f_{n+1} (x)$ while $\displaystyle \lim_{n \to \infty} f_{n} = f$. The formula is too simple, so knowing this theorem means precisely understanding the &amp;lsquo;condition&amp;rsquo;.</description></item><item><title>Proof of Fatou's Lemma</title><link>https://freshrimpsushi.github.io/en/posts/534/</link><pubDate>Sat, 05 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/534/</guid><description>Theorem 1 For a sequence $\left\{ f_{n} \right\}$ of non-negative measurable functions, $$ \int_{E} \left( \liminf_{n \to \infty} f_{n} \right) dm \le \liminf_{n \to \infty} \int_{E} f_{n} dm $$ $\liminf$ is the limit inferior. Explanation A lemma necessary for proving the Monotone Convergence Theorem and the Dominated Convergence Theorem in real analysis. The version of Fatou&amp;rsquo;s lemma for series without the condition of being measurable functions is as follows. Series</description></item><item><title>General Solution to Second-Order Linear Nonhomogeneous Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/547/</link><pubDate>Fri, 04 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/547/</guid><description>Auxiliary Lemma1 Consider the following nonhomogeneous/homogeneous second-order linear differential equation: $$ \begin{align} y^{\prime \prime}+p(t)y^\prime + q(t)y &amp;amp;=g(t) \label{eq1} \\ y^{\prime \prime}+p(t)y^\prime + q(t)y &amp;amp;=0 \label{eq2} \end{align} $$ Assume that $y_{1} (t)$ and $y_{2} (t)$ are solutions to the nonhomogeneous differential equation $\eqref{eq1}$, and that $y_{1}(t)$ and $y_{2}(t)$ are the fundamental set of solutions to the homogeneous differential equation $\eqref{eq2}$. Then, the following equation holds: $$ y_{1} (t) – y_{2} (t)=</description></item><item><title>Solution to the Inviscid Burgers' Equation</title><link>https://freshrimpsushi.github.io/en/posts/532/</link><pubDate>Fri, 04 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/532/</guid><description>Definition The following quasilinear partial differential equation is called the Burgers&amp;rsquo; equation. $$ \begin{cases} u_{t} + u u_{x} = 0 &amp;amp; , t&amp;gt;0 \\ u(t,x) = f(x) &amp;amp; , t=0 \end{cases} $$ Here, $t$ represents time, $x$ represents position, and $u(t,x)$ represents the waveform at position $x$ at time $t$. $f$ represents the initial condition, specifically the waveform at $t=0$. Explanation Burgers&amp;rsquo; equation represents the case where the diffusion coefficient</description></item><item><title>Solution to the Inhomogeneous Progressive Wave Partial Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/531/</link><pubDate>Thu, 03 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/531/</guid><description>Definition The following formula is referred to as a non-uniform traveling wave if it satisfies $u$: $$ \begin{cases} u_{t} + c(x) u_{x} = 0 &amp;amp; , t&amp;gt;0 \\ u(t,x) = f(x) &amp;amp; , t=0 \end{cases} $$ Here, $t$ represents time, $x$ represents position, and $u(t,x)$ represents the waveform at position $x$ at time $t$. $f$ represents the initial condition, particularly the waveform at $t=0$. The function $c(x)$ represents the speed</description></item><item><title>Solutions to the Homogeneous Second-Order Linear Differential Equation and the Wronskian</title><link>https://freshrimpsushi.github.io/en/posts/546/</link><pubDate>Thu, 03 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/546/</guid><description>Definition[^1] $$ ay^{\prime \prime}+ by^\prime +cy=0 $$ Let&amp;rsquo;s consider the second-order linear homogeneous differential equation given above. Let&amp;rsquo;s call $W$ the Wronskian. If $W (y_{1}, y_{2}) \ne 0$, then we call $\left\{ y_{1}, y_{2} \right\}$ the fundamental set of solution for the given differential equation.</description></item><item><title>Homogeneous Meaning in Homogeneous Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/545/</link><pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/545/</guid><description>Description $$ a_{n}(x)\dfrac{d^ny}{dx^n}+a_{n-1}(x)\dfrac{d^{n-1}y}{dx^{n-1}}+ \cdots + a_{1}(x)\dfrac{dy}{dx}+a_{0}(x)y=f(x) $$ When a differential equation is as above, if $f(x)=0$, it is called homogeneous $f(x) \ne 0$ if not, it is called non-homogeneous or inhomogeneous. Consider the following simple example of a 2nd order linear differential equation. $$ ay^{\prime \prime}+by^\prime +cy=g(t) $$ Here, if $g(t)$ equals $0$, it is homogeneous; if $0$ is not met, it is non-homogeneous. To elaborate on the term homogeneous,</description></item><item><title>Solutions to the Partial Differential Equation of Standing Waves</title><link>https://freshrimpsushi.github.io/en/posts/529/</link><pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/529/</guid><description>Definition A wave that satisfies the following condition is referred to as a stationary wave. $$ \begin{cases} u_{t} = 0 &amp;amp; , t&amp;gt;0 \\ u(t,x) = f(x) &amp;amp; , t=0 \end{cases} $$ Explanation A stationary wave is a wave whose shape does not change over time. Here, $t$ represents time, $x$ represents position, and $u(t,x)$ represents the waveform at position $x$ when the time is $t$. $f$ represents an initial</description></item><item><title>Solutions to the Partial Differential Equations of Uniformly Progressive Waves</title><link>https://freshrimpsushi.github.io/en/posts/530/</link><pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/530/</guid><description>Definition A uniform traveling wave is defined by the following equation, $u$. $$ \begin{cases} u_{t} + c u_{x} + a u = 0 &amp;amp; , t&amp;gt;0 \\ u(t,x) = f(x) &amp;amp; , t=0 \end{cases} $$ Here, $t$ represents time, $x$ represents position, and $u(t,x)$ represents the waveform at position $x$ when the time is $t$. $f$ represents the initial condition, especially the waveform at $t=0$. The constant $c$ represents the</description></item><item><title>Removing Digit Limits in R</title><link>https://freshrimpsushi.github.io/en/posts/528/</link><pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/528/</guid><description>Overview R is a language for statistics, but the R console is not suitable for viewing data. Nevertheless, when dealing with big data with hundreds of thousands of observations or checking if handling has been done properly, simple printing is convenient. Tips When there are a bit too many observations, printing them on the console shows the bottom part cut off like above. In this case, in the console window,</description></item><item><title>Solution to Second Order Homogeneous Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/544/</link><pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/544/</guid><description>Theorem1 $$ ay^{\prime \prime} + by^\prime + cy=0 $$ Let&amp;rsquo;s say the solutions to the characteristic equation $ar^2+br+c=0$ given above are $r_{1}$ and $r_2$. Then, $\text{1.}$ If $r_{1}$ and $r_2$ are two distinct real numbers$(b^2-4ac&amp;gt;0)$, the general solution is as follows: $$ y(t)=c_{1}e^{r_{1}t}+c_2e^{r_2t} $$ $\text{2.}$ If $r_{1}$ and $r_2$ are complex conjugates $\lambda \pm i \mu$$(b^2-4ac&amp;lt;0)$, the general solution is as follows: $$ \begin{align*} y(t) &amp;amp;= c_{1}e^{(\lambda + i\mu)t} +</description></item><item><title>Lebesgue Integration</title><link>https://freshrimpsushi.github.io/en/posts/527/</link><pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/527/</guid><description>Build-up Before considering the generalization of the Riemann integral, it is necessary to define a simple function. Assume that the function values are non-negative and the codomain of $\phi : \mathbb{R} \to \mathbb{R}$ is a finite set $\left\{ a_{1} , a_{2}, \cdots , a_{n} \right\}$. If $A_{i} = \phi^{-1} \left( \left\{ a_{i} \right\} \right) \in \mathcal{M}$ satisfies, then $\phi$ is called a simple function. Simple functions have the following properties:</description></item><item><title>Restoring Force and One-Dimensional Simple Harmonic Oscillator</title><link>https://freshrimpsushi.github.io/en/posts/543/</link><pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/543/</guid><description>Simple Harmonic Motion1 Let&amp;rsquo;s consider the motion of an object hanging on a spring. It oscillates back and forth due to the restoring force of the spring. Such motion is called a harmonic oscillation. The functions representing harmonic oscillation, $\sin$ and $\cos$, were called harmonic functions a long time ago, which is why the motion is designated as such. Among harmonic oscillations, those with no friction or other external forces,</description></item><item><title>Remove NA in R</title><link>https://freshrimpsushi.github.io/en/posts/526/</link><pubDate>Sun, 29 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/526/</guid><description>Overview NA stands for Not Available in R programming, primarily indicating &amp;lsquo;missing values&amp;rsquo;. It is important to note that its meaning and usage differ entirely from null in other programming languages. Textbook examples are usually well-organized for analysis, but real-world data analysis often does not reflect this neatness. Handling such data frequently involves dealing with missing values. These missing values are either dealt with by imitating similarly trending data through</description></item><item><title>n-Gram and Jaccard Coefficient</title><link>https://freshrimpsushi.github.io/en/posts/525/</link><pubDate>Sat, 28 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/525/</guid><description>Definition An n-gram is a contiguous sequence of n items from a given sample of text or speech. The Jaccard Coefficient is a measure of how similar two sets are, ranging between $0$ and $1$. It can be mathematically represented as follows: $$ JC(A,B) = {{| A \cap B|} \over {| A \cup B| }} = {{| A \cap B|} \over { |A|+ |B| -| A \cap B| }} $$</description></item><item><title>In Measure Theory: Almost Everywhere and Almost Surely</title><link>https://freshrimpsushi.github.io/en/posts/524/</link><pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/524/</guid><description>Definition 1 If a function $f : E \to \overline{\mathbb{R}}$, excluding a set $E_{0} \subset E$ where $m(E_{0}) = 0$, has some property $P$, then $f$ is said to have property $P$ almost everywhere within $E$. Notation When talking about probability, almost everywhere is expressed as almost surely, and for conciseness, the abbreviation $$ f = g \text{ a.e.} \\ P(E) = 0 \text{ a.s.} $$ can be used. Explanation</description></item><item><title>Second-Order Linear Homogeneous Differential Equations with Constant Coefficients and Characteristic Equation</title><link>https://freshrimpsushi.github.io/en/posts/540/</link><pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/540/</guid><description>Theorem1 The general solution of a second-order linear homogeneous differential equation with constant coefficients $a y^{\prime \prime} + by^\prime +cy=0$ is as follows. $$ y(x)=A e^{r_{1} x}+Be^{r_2 x} $$ At this time, $r_{1,2}=\dfrac{-b \pm \sqrt{b^2-4ac}} {2a}$ Corollary The solution of $a y^{\prime \prime} + cy = 0$ is as follows. $$ y(x) = A e^{i\sqrt{\frac{c}{a}} x}+Be^{-i\sqrt{\frac{c}{a}} x} = C\cos{\textstyle (\sqrt{\frac{c}{a}}x)} + D\sin{\textstyle (\sqrt{\frac{c}{a}}x)} $$ Solution $$ \begin{equation} a\dfrac{d^2}{dx^2}y+b\dfrac{d}{dx}y+cy = 0</description></item><item><title>Linear Combination of Solutions to Homogeneous Linear Differential Equations is Also a Solution</title><link>https://freshrimpsushi.github.io/en/posts/542/</link><pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/542/</guid><description>Theorem1 If $y_{1}, y_{2}$ is a solution to $ay^{\prime \prime}+by^\prime +cy=0$, then $d_{1}y_{1} + d_{2}y_{2}$ is also a solution. Here, $d_{1}, d_{2}$ is any constant. Description As can be seen in the proof, it also holds for any $n$ order linear homogeneous differential equation. Proof Assume that $y_{1}, y_{2}$ is a solution to $ay^{\prime \prime}+by^\prime +cy=0$. Then the following two equations are satisfied. $$ \begin{align*} d_{1} (ay_{1}^{\prime \prime}+by_{1}^\prime + cy_{1}</description></item><item><title>Reasons for Using the Format Codes d and f for Integers and Floating-Point Numbers</title><link>https://freshrimpsushi.github.io/en/posts/523/</link><pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/523/</guid><description>Why Specifically d and f 1 In programming languages like C or Python, format codes used for string input/output include %s, %c, %d, %f, etc. As known, %s represents a string, and %c represents a character. However, unlike these taking the first letter of their meaning, for integers and floats, it’s not %i or %r but %d and %f. The reason is that %d is not just</description></item><item><title>Sum and Difference Formulas and Product-to-Sum Formulas of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/539/</link><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/539/</guid><description>The sum-to-product and product-to-sum formulas aren&amp;rsquo;t used as often as the double angle/half angle formulas, so they&amp;rsquo;re not considered as important. However, this doesn&amp;rsquo;t mean they&amp;rsquo;re entirely unnecessary. Since the derivation process is very simple, it&amp;rsquo;s good to be familiar with it and be able to derive it quickly whenever needed. They are derived using only the addition formulas. Addition Formulas $$ \begin{align*} \sin ( \theta_{1} \pm \theta_{2}) =&amp;amp;\ \sin</description></item><item><title>Why is "Implicit Function" a Misleading Translation?</title><link>https://freshrimpsushi.github.io/en/posts/522/</link><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/522/</guid><description>Definition The distinction between a positive and a negative function merely depends on how each is represented. Although it&amp;rsquo;s a somewhat unfamiliar expression in mathematics, the differentiation depends on how &amp;lsquo;independent variables&amp;rsquo; and &amp;lsquo;dependent variables&amp;rsquo; are represented. Simply put, it involves setting the independent variable as $x$ and the dependent variable as the changing $y$ and observing their forms. Examples For example, in the case of $y = x^2 +</description></item><item><title>Lebesgue Measurable Functions</title><link>https://freshrimpsushi.github.io/en/posts/518/</link><pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/518/</guid><description>Definition 1 A function $f: E \in \overline{ \mathbb{R} }$ is said to be (Lebesgue) measurable if for every interval $I \subset \overline{ \mathbb{R} }$, $$ f^{-1} (I) = \left\{ x \in \mathbb{R} \ | \ f(x) \in I \right\} \in \mathcal{M} $$ holds. $\overline{ \mathbb{R} } = \mathbb{R} \cup \left\{ - \infty , + \infty \right\}$ refers to the extended real number space, which includes positive and negative infinity,</description></item><item><title>Pressure of the Fluid Depending on the Depth when an Object is Placed on the Fluid</title><link>https://freshrimpsushi.github.io/en/posts/533/</link><pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/533/</guid><description>Explanation Simply put, when an object is on a fluid, the pressure depending on the depth can be obtained by substituting ${P_{0} }^\prime$ for $P_{0}$ in the case of calculating the pressure in a fluid depending on the depth. The original formula&amp;rsquo;s atmospheric pressure $P_{0}$ represented the pressure exerted from above the fluid. That is, if an object is placed on the fluid, adding the pressure due to the object</description></item><item><title>A Compact Hausdorff Space is a Normal Space</title><link>https://freshrimpsushi.github.io/en/posts/514/</link><pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/514/</guid><description>Theorem 1 [1]: A closed subset of a compact space is compact. [2]: A compact subset of a Hausdorff space is a closed set. [3]: For two compact subsets $A,B \subset X$ and $A \cap B = \emptyset$ of a Hausdorff space $X$, if $A \cap B = \emptyset$, there exists an open subset $U, V \subset X$ that satisfies: $$ A \subset U \\ B \subset V \\ U</description></item><item><title>Independence of Events and Conditional Probability</title><link>https://freshrimpsushi.github.io/en/posts/521/</link><pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/521/</guid><description>Definition 1 Let&amp;rsquo;s assume a probability space $(\Omega , \mathcal{F} , P)$ is given. For $P(B)&amp;gt;0$, $\displaystyle P (A | B) = {{P(A \cap B)} \over {P(B)}}$ is called the conditional probability of $A$ given $B$. If $P(A | B) = P(A)$, that is $P( A \cap B) = P(A) \cdot P(B)$, then $A, B$ are considered independent. If you haven&amp;rsquo;t yet encountered measure theory, you can ignore the term</description></item><item><title>The Formula to Calculate the Pressure of a Fluid Based on Depth</title><link>https://freshrimpsushi.github.io/en/posts/520/</link><pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/520/</guid><description>Formulas The pressure of a fluid at a vertical distance $h$ below the surface, or simply, the fluid pressure at a depth of $h$, $P_{h}$, is as follows. $$ P_{h}=P_{0}+\rho g h $$ Here, $P_{0}$ is the atmospheric pressure, $\rho$ is the density of the object, and $g$ is the gravitational acceleration. Explanation This formula applies only in static situations. It does not apply to moving fluids, i.e., fluids with</description></item><item><title>Fluid Pressure</title><link>https://freshrimpsushi.github.io/en/posts/519/</link><pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/519/</guid><description>Definition A pressure is the force acting per unit area by a fluid. Let&amp;rsquo;s denote $F$ as the force, $A$ as the area upon which the force acts, and $P$ as the pressure. Then, $$ P=\dfrac{F}{A} [\mathrm{N/m^{2}}] $$ Description The unit of pressure is newtons per square meter $\mathrm{N/m^{2}}$, known as Pascal, and denoted by $\mathrm{Pa}$. $$ 1 \mathrm{Pa} = 1 \mathrm{N/m^{2}} $$ A characteristic feature of pressure is that</description></item><item><title>Definition and Discrimination Method of an Exact Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/516/</link><pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/516/</guid><description>Definition The given differential equation $\psi=\psi (x,y)$ is said to be an exact differential equation if there exists $\psi=\psi (x,y)$ that satisfies $\psi (x,y)$. Explanation If the given differential equation is exact, it can be represented as a total differential with respect to $\psi (x,y)$. $d\psi (x,y)=\dfrac{\partial \psi }{\partial x}dx + \dfrac{\partial \psi }{\partial y}dy$ Since $d\psi (x,y)=\dfrac{\partial \psi }{\partial x}dx + \dfrac{\partial \psi }{\partial y}dy$, it follows that $d\psi</description></item><item><title>Integrating Factor Method for First Order Linear Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/515/</link><pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/515/</guid><description>Theorem1 The solution to the first order linear differential equation $\dfrac{dy}{dx}+p(x)y=q(x)$ is given as follows. $$ \begin{align*} y(x)&amp;amp;=\dfrac{1}{e^{\int p(x) dx}} \left[ \int e^{\int p(x) dx} q(x) dx +C \right] \\ &amp;amp;=e^{-\int p(x) dx}\int e^{\int p(x) dx} q(x) dx + e^{-\int p(x) dx}C \end{align*} $$ Description A differential equation of form $y^\prime+p(x)y=q(x)$ is called a first order linear differential equation. Here, if $q(x)=0$, we can directly apply variable separation and solve</description></item><item><title>Rejection Region and Significance Level</title><link>https://freshrimpsushi.github.io/en/posts/509/</link><pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/509/</guid><description>Definition 1 The error of rejecting the null hypothesis when it is actually true is called a Type I error. The error of failing to reject the null hypothesis when the alternative hypothesis is true is called a Type II error. The maximum probability of committing a Type I error is called the Significance Level. The statistic used for hypothesis testing is called the test statistic. The region of the</description></item><item><title>Solution to the Exact Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/517/</link><pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/517/</guid><description>Solution The solution to the given exact differential equation $M(x,y)+N(x,y)\dfrac{dy}{dx}=0$ is as follows. Step 0. Since the differential equation given is exact, an $\psi$ exists such that $\psi_{x}=M,\ \ \psi_{y}=N, \ \ \psi=c$. Step 1. Integrate $\psi_{x}$. Then, differentiate the obtained $\psi$ with respect to $y$ to find $h^\prime(y)$. The entire process including Step 1 can be done the opposite way with respect to $x$ and $y$ as well. $$</description></item><item><title>Moment of Inertia of a Spherical Shell</title><link>https://freshrimpsushi.github.io/en/posts/243/</link><pubDate>Sat, 07 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/243/</guid><description>Formulas The moment of inertia of a spherical shell with a radius of $a$ and a mass of $m$ is as follows. $$ I=\frac{2}{3}ma^{2} $$ Derivation Consider a uniform spherical shell with a radius of $a$ and a mass of $m$. The same idea is used as when calculating the moment of inertia of a sphere. However, there is a bit of a difference. Think of the spherical shell as</description></item><item><title>Type I and Type II Errors Difference</title><link>https://freshrimpsushi.github.io/en/posts/508/</link><pubDate>Sat, 07 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/508/</guid><description>Definition In hypothesis testing, an error of not accepting $H_{0}$ when $H_{0}$ is true is called a Type 1 Error, and an error of accepting $H_{0}$ when $H_{0}$ is false is called a Type 2 Error. Explanation We use the term &amp;lsquo;accept&amp;rsquo; for the null hypothesis and &amp;lsquo;reject&amp;rsquo; for the alternative hypothesis. If there is sufficient evidence supporting the null hypothesis, it means &amp;lsquo;rejecting the alternative hypothesis and accepting the</description></item><item><title>Finite Intersection Property</title><link>https://freshrimpsushi.github.io/en/posts/502/</link><pubDate>Fri, 06 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/502/</guid><description>Definition 1 Let&amp;rsquo;s say topological space is $X$ and $\mathscr{A} \subset \mathscr{P} (X)$. It is said to have the Finite Intersection Property if for all finite subsets $A \subset \mathscr{A}$, $\displaystyle \bigcap A \ne \emptyset$ implies $A$. Explanation That $A$ has the f.i.p. means the same as always meeting the following condition for open sets $U_{\alpha} \subset A$: $$ \bigcap_{i=1}^{n} \left( X \setminus U_{i} \right) \ne \emptyset \implies \bigcap_{\alpha \in</description></item><item><title>How to Set the Null Hypothesis and Alternative Hypothesis</title><link>https://freshrimpsushi.github.io/en/posts/500/</link><pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/500/</guid><description>Description Hypothesis Testing: Null Hypothesis $H_{0}$ vs Alternative Hypothesis $H_{1}$ As of April 2018, some textbooks and Wikipedia describe the null hypothesis as &amp;rsquo;the hypothesis that is initially assumed to be rejected in statistics&amp;rsquo;, and the alternative hypothesis as &amp;rsquo;the hypothesis that one hopes or expects to prove through research&amp;rsquo;. However, if you only study the necessary parts of statistics or lightly know about it, that might be fine, but</description></item><item><title>Derivation of the Legendre Duplication Formula</title><link>https://freshrimpsushi.github.io/en/posts/499/</link><pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/499/</guid><description>Formula $$ \Gamma (2r) = {{2^{ 2r - 1} } \over { \sqrt{ \pi } } } \Gamma \left( r \right) \Gamma \left( {{1} \over {2}} + r \right) $$ Description While the splitting shape may not be pretty, the fact that factors can be divided into smaller ones is certainly useful. The derivation itself is not very difficult if one uses an auxiliary lemma derived from the beta function.</description></item><item><title>Socks-Shoes Property: The Inverse of ab is Equal to the Product of the Inverse of b and the Inverse of a</title><link>https://freshrimpsushi.github.io/en/posts/513/</link><pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/513/</guid><description>Theorem 1 For any element $a,b$ of a group $G$, it follows that $(ab)^{-1}=b^{-1}a^{-1}$. Proof Since $(ab)^{-1}$ is the inverse of $ab$, $$ ab(ab)^{-1}=e $$ multiplying both sides by $a^{-1}$ gives $$ b(ab)^{-1}=a^{-1}e=a^{-1} $$ then multiplying both sides by $b^{-1}$ gives $$ (ab)^{-1}=b^{-1}a^{-1} $$ ■ Explanation This theorem is referred to as the Socks-Shoes Property, which is an analogy to the process of putting on socks and then shoes. If</description></item><item><title>Probability Defined by Measure Theory</title><link>https://freshrimpsushi.github.io/en/posts/498/</link><pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/498/</guid><description>Definition 1 Let&amp;rsquo;s say $\mathcal{F}$ is a sigma field of set $\Omega$. Measurable set $E \in \mathcal{F}$ is called an Event. On $\mathcal{F}$, if measure $P : \mathcal{F} \to \mathbb{R}$ satisfies $P(\Omega) = 1$, then $P$ is called Probability. $( \Omega, \mathcal{F} , P )$ is called the Probability Space. Explanation Borrowing the strength of measure theory, we can provide a mathematical foundation for various concepts of probability theory and</description></item><item><title>Converting Categorical Data to Numeric Data in R</title><link>https://freshrimpsushi.github.io/en/posts/497/</link><pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/497/</guid><description>Overview This post is for those who are unable to convert categorical data, which is read as numerical, into continuous data despite their intentions. The majority of this post is dedicated to explaining the principle, so if you only need the conclusion, it is recommended to start reading from the Practical Example section below. Note that the term &amp;ldquo;Cast&amp;rdquo; is commonly used when changing data types. Principle When conducting statistical</description></item><item><title>Differentiation of Vectors, Dot Product, and Cross Product in Cartesian Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/506/</link><pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/506/</guid><description>Formulas Let&amp;rsquo;s say $\mathbf{A} = A_{x}\hat{\mathbf{x}} + A_{y}\hat{\mathbf{y}} + A_{z}\hat{\mathbf{z}}, \mathbf{B} = B_{x}\hat{\mathbf{x}} + B_{y}\hat{\mathbf{y}} + B_{z}\hat{\mathbf{z}}$ is a vector in a 3-dimensional Cartesian coordinate system. Let $n$ be any scalar. Then, the following equations hold: (a) $\dfrac{ d \left( n \mathbf{A} \right) }{dt} = \dfrac{ dn }{dt} \mathbf{A} + n\dfrac{ d\mathbf{A}}{dt}$ (b) $\dfrac{ d ( \mathbf{A} \cdot \mathbf{B} )}{dt} = \dfrac{ \mathbf{A} }{dt} \cdot \mathbf{B} + \mathbf{A} \cdot \dfrac{</description></item><item><title>Kinetic and Potential Energy Definitions in Physics</title><link>https://freshrimpsushi.github.io/en/posts/507/</link><pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/507/</guid><description>Kinetic Energy1 When the force depends only on the position, i.e., it is independent of the velocity or time, the equation of motion (differential equation) for the straight-line motion of a particle is as follows. $$ \begin{equation} F(x)=m\ddot{x} \label{force1} \end{equation} $$ In this case, acceleration $\ddot{x}$ can be expressed in terms of velocity as follows. $$ \begin{align*} \ddot{x} &amp;amp;= \dfrac{d \dot{x}}{dt} \\ &amp;amp;=\dfrac{dv}{dt} \\ &amp;amp;=\dfrac{dv}{dx} \dfrac{dx}{dt} \\ &amp;amp;=v\dfrac{dv}{dx} \\ &amp;amp;=</description></item><item><title>Homogeneous Functions and First-Order Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/505/</link><pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/505/</guid><description>Definition When a function $f(x,y)$ satisfies $f(tx,ty)=t^nf(x,y)$ for any positive integer $n$, $f$ is called a $n$th degree homogeneous function.</description></item><item><title>Importing External Data in R</title><link>https://freshrimpsushi.github.io/en/posts/496/</link><pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/496/</guid><description>Overview R is fundamentally born for statistics, which is why data input is conveniently designed. read.table(file, header = FALSE, sep = &amp;#34;&amp;#34;, na.strings = &amp;#34;NA&amp;#34;, fileEncoding = &amp;#34;&amp;#34;) Function Introduction read.table() is a function used to import data tables, offering various useful options like those mentioned above. There are more options available, but the ones listed here are frequently used and essential to know. Below are the descriptions: (1) header:</description></item><item><title>Linear Expansion Coefficient and Volumetric Expansion Coefficient</title><link>https://freshrimpsushi.github.io/en/posts/504/</link><pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/504/</guid><description>Coefficient of Linear Expansion The coefficient of linear expansion refers to the change in length per unit length of a solid when it expands due to heat, as follows: $$ \alpha = \dfrac{\Delta L}{L} \dfrac{1}{\Delta T} \left[ ^\circ \mathrm{C} ^{-1} \right] $$ Here, $L$ is the original length of the solid, $\Delta T$ is the change in temperature, and $\Delta L$ is the change in length. Derivation Let&amp;rsquo;s assume that</description></item><item><title>Separable First-Order Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/503/</link><pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/503/</guid><description>Definition1 A first-order differential equation is said to be separable if it satisfies the following condition: $$ f(x)+g(y)\dfrac{dy}{dx}=0 \quad \text{or} \quad f(x)dx = -g(y)dy $$ Explanation It can be expressed in various forms, but the important point is that the variables on each side must be separated. The method of finding solutions by separating these two variables is called the method of separation of variables. The separability is a very</description></item><item><title>Wronskian Definition and Determination of Linear Independence</title><link>https://freshrimpsushi.github.io/en/posts/501/</link><pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/501/</guid><description>Definition1 Let us consider a set of functions that are differentiable up to n times, denoted by $S=\left\{ f_{1}, f_{2}, \dots, f_{n} \right\}$. The Wronskian $W$ of this set is defined by the following determinant. $$ W(x) = W(f_{1}, f_{2}, \dots, f_{n}) := \begin{vmatrix} f_{1} &amp;amp; f_{2} &amp;amp; \cdots &amp;amp; f_{n} \\ f_{1}^{\prime} &amp;amp; f_2^{\prime} &amp;amp; \cdots &amp;amp; f_{n}^{\prime} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ f_{1}^{(n-1)} &amp;amp;</description></item><item><title>Borel Set</title><link>https://freshrimpsushi.github.io/en/posts/495/</link><pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/495/</guid><description>Definition 1 Let $\mathcal{F}$ be called the sigma field of the Euclidean space $\mathbb{R}$. $\displaystyle \mathcal{B} : = \bigcap \left\{ \mathcal{F} : \mathcal{I} \subset \mathcal{F} \right\}$ is said to be generated by the set of all intervals $\mathcal{I}$. $B \in \mathcal{B}$ is called the Borel Set, and $\mathcal{B}$ is referred to as the Borel sigma field. $\mathcal{I}$ is the set of all intervals. Description In simple terms, it is the</description></item><item><title>Lebesgue Measure</title><link>https://freshrimpsushi.github.io/en/posts/494/</link><pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/494/</guid><description>Definition 1 Let us define the function $m : \mathcal{M} \to [0,\infty]$ with respect to $E \in \mathcal{M}$ as follows $m(E) := m^{ \ast } (E)$. $m$ is called the (Lebesgue) measure. $\mathcal{M}$ is a sigma-algebra, the set of measurable sets of $X = \mathbb{R}$. $m^{\ast}$ is an outer measure. Description The outer measure was neatly defined by $m^{ \ast } : \mathscr{P}( \mathbb{R} ) \to [0, \infty]$, but it</description></item><item><title>Proof of the Chinese Remainder Theorem</title><link>https://freshrimpsushi.github.io/en/posts/493/</link><pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/493/</guid><description>Theorem $\gcd(n,m) = 1$ and $\begin{cases} x \equiv b \pmod{n} \\ x \equiv c \pmod{m} \end{cases}$ have exactly one solution in $1 \le x \le nm$. Explanation It is said that a mathematics book written in China between the 3rd and 5th centuries AD contained this problem. If a number is grouped by threes, two remain; by fives, three remain; and by sevens, two remain. What is the number? -</description></item><item><title>Euler's Totient Summation Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/492/</link><pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/492/</guid><description>Formulas If $n$&amp;rsquo;s divisors are denoted by $d_{1}, d_{2} , \cdots , d_{r}$, then $$ n = \sum_{ i = 1 }^{r} \phi (d_{i}) = \phi (d_{1}) + \phi (d_{2}) + \cdots + \phi (d_{r}) $$ Explanation The totient function might feel somewhat unnatural from its definition. However, seeing Euler&amp;rsquo;s totient theorem and such formulas, one can&amp;rsquo;t help but acknowledge it as a necessary function somewhere within the truths of</description></item><item><title>Euler's Totient Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/491/</link><pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/491/</guid><description>Theorem 1 $$ \gcd(a,m) = 1 \implies a^{ \phi (m) } \equiv 1 \pmod{m} $$ Description One can immediately recognize that this theorem generalizes Fermat&amp;rsquo;s Little Theorem, and indeed, the proof is almost identical. Proof By the definition of the Euler&amp;rsquo;s totient function, there exactly $\phi (m)$ occurrences of $b_{i}$ satisfying $\gcd( b_{i} , m) =1$ among $1 \le b_{i} \le m$. Let&amp;rsquo;s call this set $$ B:= \left\{ b_{1},</description></item><item><title>Sigma Algebra and Measurable Spaces</title><link>https://freshrimpsushi.github.io/en/posts/490/</link><pubDate>Sun, 25 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/490/</guid><description>Definitions For a set $X \ne \emptyset$, $\mathcal{E} \subset \mathscr{P} (X)$ is called a Sigma Algebra or Sigma Field on $X$ if it satisfies the conditions below. The ordered pair $(X , \mathcal{E})$ of the set $X$ and the sigma field $\mathcal{E}$ is called a Measurable Space. (i): $\emptyset \in \mathcal{E}$ (ii): $E \in \mathcal{E} \implies E^{c} \in \mathcal{E}$ (iii): $\displaystyle \left\{ E_{n} \right\}_{n \in \mathbb{N}} \subset \mathcal{E} \implies \bigcup_{n=1}^{\infty}</description></item><item><title>What are Compact and Precompact in Topological Spaces?</title><link>https://freshrimpsushi.github.io/en/posts/489/</link><pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/489/</guid><description>Definition Let a topological space be $\left( X, \mathscr{T} \right)$, and denote $A \subset X$. A collection comprised of open sets of $X$ is called an open covering of $A$ if it satisfies the following condition: $$ A \subset \bigcup_{O \in \mathscr{O}} O $$ A subset $\mathscr{O} ' $ of $\mathscr{O}$ is called a subcover, especially when the cardinality of $\mathscr{O} ' $ is a natural number, it&amp;rsquo;s termed a</description></item><item><title>Topologist's Sine Curves and Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/488/</link><pubDate>Fri, 23 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/488/</guid><description>Definition 1 The curve defined as follows $S$ is called the Topologist&amp;rsquo;s Sine Curve. $$ S : = \left\{ (0,y) \ | \ y \in [-1,1] \right\} \cup \left\{ \left. \left( x, \sin {{1} \over {x}} \right) \ \right| \ x \in (0,1] \right\} $$ The space defined as follows $C$ is called the Topologist&amp;rsquo;s Comb Space. $$ C := \left\{ (0,y) \ | \ y \in [0,1] \right\} \cup</description></item><item><title>Indefinite Integral of the Form e^{x^2}</title><link>https://freshrimpsushi.github.io/en/posts/487/</link><pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/487/</guid><description>Theorem $$ \int e^{x^2}dx = \sum\limits_{n=0}^\infty \dfrac{x^{2n+1}}{(2n+1)n!}+C $$ Explanation Just like the form $e^{-x^{2}}$, it is difficult to integrate using general methods. There is a method to integrate by defining the error function, imaginary error function, erfi, but this article introduces solving it using Taylor series expansion. Proof By the method of Taylor series expansion, $$ e^{x} = \sum\limits_{n=0}^{\infty} \dfrac{x^{n}}{n!} = 1 + x + \dfrac{x^{2}}{2!} + \cdots + \dfrac{x^{n}}{n!}</description></item><item><title>Local Connectivity and Local Path Connectivity</title><link>https://freshrimpsushi.github.io/en/posts/486/</link><pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/486/</guid><description>Definition Let us call $X$ a topological space. If for all $U$ containing $x \in X$, there exists an open connected set $C$ that satisfies $x \in C \subset U$, then $X$ is said to be locally connected at $x$. If it is locally connected for all $x \in X$, then $X$ is termed a locally connected space. If for all $U$ containing $x \in X$, there exists an open</description></item><item><title>Path Connectivity Components</title><link>https://freshrimpsushi.github.io/en/posts/485/</link><pubDate>Wed, 21 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/485/</guid><description>Definition 1 A path connected component of a topological space $X$ is a path connected subset that has itself as the only connecting superSet. Specifically, the path connected component that includes $x \in X$ is written as $P_{x}$. Theorem [1]: $x \in X$ belongs to only one $P_{x}$. [2]: For $a,b \in X$, it is either $P_{a} = P_{b}$ or $P_{a} \cap P_{b} = \emptyset$. [3]: Every path connected space</description></item><item><title>Classification of Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/483/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/483/</guid><description>Description Differential equations can be classified by various criteria. They are broadly divided into ordinary differential equations and partial differential equations. Further classification can be made based on coefficients and order, and whether they are linear or nonlinear. The reason for classifying differential equations is obviously to solve them. The method of solving a differential equation varies depending on its classification. Ordinary Differential Equations and Partial Differential Equations Ordinary differential</description></item><item><title>Double Angle and Half Angle Formulas of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/481/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/481/</guid><description>Overview Back in the day, when the owners of sushi restaurants were high school students, there used to be formulas like angle addition, double angle, and sum-difference formulas in the curriculum, but nowadays, it&amp;rsquo;s understood they are not. All the following formulas can be derived from the sum formulas, so it&amp;rsquo;s better to learn the derivation process and derive them as needed rather than memorizing them all. Addition Theorem $$</description></item><item><title>Proof of the Adhesive Lemma</title><link>https://freshrimpsushi.github.io/en/posts/484/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/484/</guid><description>Theorem For a given topological space $X,Y$ and two closed sets $A,B \subset X$ satisfying $A \cup B = X$, let two continuous functions $f : A \to Y$ and $g : B \to Y$ satisfy $f(x) = g(x)$ for all $x \in A \cap B$. Then, the function defined as follows $h$ is a continuous function. $$ h(x) : = \begin{cases} f(x), &amp;amp; x \in A \\ g(x), &amp;amp;</description></item><item><title>Proof that Sine Squared Plus Cosine Squared Equals 1</title><link>https://freshrimpsushi.github.io/en/posts/482/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/482/</guid><description>Formulas $$ \sin^2\theta+\cos^2\theta=1 $$ Proof 1-Addition Formula for Cosine Using the Addition Theorem for Cosine, we can understand it very easily. $$ \cos(\theta_{1}-\theta_2)=\cos\theta_{1}\cos\theta_2 + \sin\theta_{1}\sin\theta_2 $$ Here, if we substitute $\theta$ instead of $\theta_{1}$, $\theta_2$ $$\cos(\theta-\theta)=\cos^2\theta + \sin^2\theta$$ $$\implies \cos(\theta-\theta)=\cos 0=1$$ $$\implies \sin^2\theta+\cos^2\theta=1$$ ■ 2-Pythagorean Theorem There is a unit circle with a radius of 1. Let&amp;rsquo;s look at the triangle formed by the unit circle&amp;rsquo;s radius, the perpendicular dropped</description></item><item><title>Exterior Measure</title><link>https://freshrimpsushi.github.io/en/posts/480/</link><pubDate>Mon, 19 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/480/</guid><description>Definition 1 Given $E \subset \mathbb{R}$, $\left\{ I_{n} \in \mathcal{I} \ | \ n \in \mathbb{N} \right\} $, $\left\{ E_{n} \in \mathscr{P} ( \mathbb{R} ) \ | \ n \in \mathbb{N} \right\}$, we define the function $m^{ \ast } (E) : = \inf Z_{E}$ as an Outer Measure. Fundamental Properties The outer measure has the following properties: [1] Generalization of Length: $I \in \mathcal{I} \implies m^{ \ast } (I) =</description></item><item><title>Path-Connectedness in Topology</title><link>https://freshrimpsushi.github.io/en/posts/478/</link><pubDate>Sun, 18 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/478/</guid><description>Definitions 1 Let&amp;rsquo;s call $X$ a topological space and let $C \subset \mathbb{R}^{n}$. A continuous function $p : [0,1] \to X$ from initial point $p(0)$ to terminal point $p(1)$ is called a path. $\overline{p}(t) = p(1-t)$ is called the reverse path of $p$. If for every $a,b \in X$ there exists a path $p$ satisfying both $p(0) = a$ and $p(1) = b$, then $X$ is called a path-connected space.</description></item><item><title>Definition and Examples of Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/479/</link><pubDate>Sat, 17 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/479/</guid><description>Definition A differential equation is an equation that includes derivatives of one or more dependent variables with respect to one or more independent variables. $$ \dfrac{dy}{dx}=y $$ $$ \dfrac{d^2y}{dx^2} = y $$ Explanation Most physical situations can be described by first-order or second-order differential equations. Falling Body $$ F=ma=mg $$ $$ v=\dfrac{dy}{dt} $$ $$ a=\dfrac{dv}{dt}=\dfrac{d}{dt} \left( \dfrac{dy}{dt} \right)=\dfrac{d^2y}{dt^2} $$ $$ \dfrac{d^2y}{dt^2}=g $$ Spring Mass System $$ F=ma=-ky $$ $$ a=</description></item><item><title>What is the Fixed Point Property in Topology?</title><link>https://freshrimpsushi.github.io/en/posts/477/</link><pubDate>Sat, 17 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/477/</guid><description>Definition A fixed point of a function $f : X \to X$ is $x_{0}$ that satisfies $f(x_{0}) = x_{0}$ for $f$. It is said that $X$ has the fixed point property if every continuous function $f$ has a fixed point. Explanation It is closely related to complete spaces. At least, in $\mathbb{R}$, it is possible to show that there always exists $c$ satisfying $f(c) = c$ for $f : [a,b]</description></item><item><title>Proof of the Intermediate Value Theorem</title><link>https://freshrimpsushi.github.io/en/posts/476/</link><pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/476/</guid><description>Definition 1 If $f : [a,b] \to \mathbb{R}$ is continuous, then there exists a $c \in (a,b)$ between $f(a)$ and $f(b)$ such that $y_{0} = f(c)$ is satisfied for $y_{0}$. Explanation Using the contrapositive, it can be shown that there is no curve connecting two specific shapes under condition $\mathbb{R}^2$. Corollary Meanwhile, the intermediate value theorem has several useful corollaries as follows: Existence judgment method for the solution of equation</description></item><item><title>Empty Set</title><link>https://freshrimpsushi.github.io/en/posts/475/</link><pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/475/</guid><description>Definition 1 For a set of real number intervals $\mathcal{I}$, define function $l : \mathcal{I} \to [ 0 , \infty )$ as $l( I ) := \sup{I} - \inf{I}$ and call it Length. If there exists a sequence of intervals $\left\{ I_{n} \ | \ n \in \mathbb{N} \right\}$ that satisfies $$ A \subset \bigcup_{n = 1}^{\infty} I_{n} \\ \sum_{n=1}^{\infty} l (I_{n}) &amp;lt; \varepsilon $$ for any $\varepsilon &amp;gt; 0$,</description></item><item><title>Connected Components and Totally Disconnected Spaces</title><link>https://freshrimpsushi.github.io/en/posts/474/</link><pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/474/</guid><description>Definition A connected component of a topological space $X$ is a connected subset that does not have a connected superset other than itself among the connected subsets of $X$. In particular, the connected component that includes $x \in X$ is denoted as $C_{x}$. If all connected components of $X$ are singleton sets, then $X$ is called a Totally Disconnected Space. Description Connected Components At first glance, the definition seems to</description></item><item><title>Properties of Subspaces in Connected Spaces</title><link>https://freshrimpsushi.github.io/en/posts/473/</link><pubDate>Tue, 13 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/473/</guid><description>Theorem Let&amp;rsquo;s say about a topological space $X$ that $Y \subset X$. [1]: If $Y$ is a connected space, then $\overline{Y}$ is also a connected space. [2]: That $Y$ is a disconnected space is equivalent to the existence of open sets $U$ and $V$ in $X$ that satisfy $$ U \cap Y \ne \emptyset \\ V \cap Y \ne \emptyset \\ U \cap V \cap Y = \emptyset \\ Y</description></item><item><title>Cosets and Normal Subgroups in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/469/</link><pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/469/</guid><description>Definition 1 The set $G$ and its subgroup $H$ such that $aH = \left\{ ah \ | \ h \in H \right\}$ is called the Left Coset and $Ha = \left\{ ha \ | \ h \in H \right\}$ is called the Right Coset. Here, $a \in G$ and $aH, Ha \subset G$. The number of left (right) cosets of $H \leqslant G$ is denoted as $(G : H)$ and</description></item><item><title>Alternating groups in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/468/</link><pubDate>Thu, 08 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/468/</guid><description>Definition 1 The group formed by the even permutations of the symmetric group $S_{n}$ is called the Alternating group and is denoted by $A_{n}$. Theorem For $n \ge 2$, $$ \left| A_{n} \right| = {{\left| S_{n} \right|} \over {2}} = {{ n! } \over {2}} $$ Description It is quite interesting that the order of $A_{n}$ is exactly half of $\left| S_{n} \right|$. The alternating group is considered to be</description></item><item><title>Perfectly Elastic Collisions and the Conservation of Kinetic Energy</title><link>https://freshrimpsushi.github.io/en/posts/466/</link><pubDate>Thu, 08 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/466/</guid><description>Theorem When the coefficient of restitution $e$ is $1$, it is said to be a perfectly elastic collision. There are two important characteristics of a perfectly elastic collision. (a) The sum of the kinetic energy of each object before and after the collision is conserved. (b) If the masses of the two objects are the same, their velocities are exchanged after the collision. Proof (a) By the law of conservation</description></item><item><title>Proof that a Permutation Cannot Be Both Even and Odd</title><link>https://freshrimpsushi.github.io/en/posts/467/</link><pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/467/</guid><description>Definition A permutation of a finite symmetric group can be expressed as a product of an even number of transpositions, making it even, or as a product of an odd number of transpositions, making it odd. The sign of a permutation $\sgn (\sigma)$ is defined as follows: $$ \sgn (\sigma) = \begin{cases} +1 &amp;amp; \text{if $\sigma$ is even} \\ -1 &amp;amp; \text{if $\sigma$ is odd} \end{cases} $$ Explanation The definition</description></item><item><title>Collision of Two Objects and the Coefficient of Restitution</title><link>https://freshrimpsushi.github.io/en/posts/465/</link><pubDate>Tue, 06 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/465/</guid><description>Explanation When two objects collide, there are three types of collisions. Perfectly elastic collision (Elastic collision) Inelastic collision Perfectly inelastic collision The classification is based on the coefficient of restitution $e$. The coefficient of restitution is the ratio of the relative speed of the two objects before collision to the relative speed after collision. In other words, it&amp;rsquo;s defined as the ratio of the speed at which the two objects</description></item><item><title>The Transfer Continuous Function Preserves Connectivity</title><link>https://freshrimpsushi.github.io/en/posts/462/</link><pubDate>Tue, 06 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/462/</guid><description>Theorem For a connected space $X$, if $f : X \to Y$ is a surjective continuous function, then $Y$ is a connected space. Explanation Terms like connected and continuous may seem confusing because they sound similar. Usually, memorizing them in English does help, but for this theorem, the relevant English terms are Connected and Continuous, so it doesn’t really help. Proof Assuming $Y$ is</description></item><item><title>Conservation of Momentum: An Easy Proof (High School Level)</title><link>https://freshrimpsushi.github.io/en/posts/464/</link><pubDate>Mon, 05 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/464/</guid><description>Theorem If no external force acts, the total momentum before and after the action of force (internal force) remains constant. In simpler terms, when two objects collide, the sum of each object&amp;rsquo;s momentum before and after the collision remains the same. $$ m_{1}v_{1}+m_2v_2=m_{1}{v_{1}}^{\prime}+m_2{v_2}^{\prime} $$ Proof (High School Level) When two objects $A, B$ collide, by the law of action and reaction, the force each exerts on the other is equal</description></item><item><title>Various Equivalent Conditions of Connected Spaces</title><link>https://freshrimpsushi.github.io/en/posts/461/</link><pubDate>Mon, 05 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/461/</guid><description>Definitions 1 For a topological space $X$, a subset $A \subset X$ is $$ A \ne \emptyset \\ A \ne X $$ then $A$ is called a Proper Subset of $X$. For two proper subsets $A,B \subset X$ if $$ \overline{A} \cap B = \emptyset \\ A \cap \overline{B} = \emptyset $$ then $A$ and $B$ are called Separated Sets or simply Separation. Equivalent Conditions for Connected Spaces Including the</description></item><item><title>Orbits, Cycles, and Permutations in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/460/</link><pubDate>Sun, 04 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/460/</guid><description>Definitions 1 The equivalence classes of $\sim$ are called the Orbits of $\sigma$. A permutation that has at most one orbit with more than one element is called a Cycle. Among the orbits a cycle has, the orbit with the largest cardinality is called the Length of the cycle. A cycle with length $2$ is called a Transposition. Orbits corresponding to a cycle that do not share elements are called</description></item><item><title>Connectivity in Topology</title><link>https://freshrimpsushi.github.io/en/posts/457/</link><pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/457/</guid><description>Definitions 1 In a topological space $X$, if there exist open sets $A \ne \emptyset$, $B \ne \emptyset$ that satisfy $A \cap B = \emptyset$ and $A \cup B = X$, then $X$ is called a Disconnected space. If it is not disconnected, it is called a Connected space. Theorems [1]: Connectedness is a topological property. [2]: Every trivial space is a connected space. [3]: Every discrete space is a</description></item><item><title>Matrix Representation of Angular Momentum Operator</title><link>https://freshrimpsushi.github.io/en/posts/463/</link><pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/463/</guid><description>Formula The matrix representation of the angular momentum operator is as follows. When $\ell = 1$, $m = 1, 0, -1$ holds, $$ \underset{\normalsize L_{z} = }{\scriptsize} \begin{array}{cccc} \scriptstyle{m=1} &amp;amp; \scriptstyle m=0 &amp;amp; \scriptstyle m=-1 &amp;amp; \\ \hbar\left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array} \right. &amp;amp; \begin{array}{c} 0 \\ 0 \\ 0 \end{array} &amp;amp; \left. \begin{array}{c} 0 \\ 0 \\ -1 \end{array} \right] &amp;amp; \begin{array}{l} \scriptstyle m=1 \\ \scriptstyle</description></item><item><title>In Hausdorff Spaces, Limits of Sequences Are Unique</title><link>https://freshrimpsushi.github.io/en/posts/456/</link><pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/456/</guid><description>Theorem In a $T_{2}$-space, a sequence in $X$ does not converge to two or more distinct points. Explanation Is it really necessary to paradoxically emphasize the importance of the uniqueness of limits? The mere fact that this property exists is evidence that Hausdorff spaces are useful. What needs attention is that there&amp;rsquo;s a slight difference from saying &amp;lsquo;it converges to a single point&amp;rsquo;. If one wishes to use that expression,</description></item><item><title>Being a Space and Having All Finite Subsets Closed are Equivalent</title><link>https://freshrimpsushi.github.io/en/posts/455/</link><pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/455/</guid><description>Theorem For $X$ to be a $T_{1}$-space, the necessary and sufficient condition is that every singleton set $\left\{ x \right\}$ is a closed set in $X$. Proof Given $(\Rightarrow)$, for space $X$ being a $T_{1}$-space, if we let $x \in X$, $x' \in X \setminus \left\{ x \right\}$, then $x \ne x '$ applies. Since $X$ is a $T_{1}$-space, there exists an open set $U_{x&amp;rsquo;} \subset X$ that is both</description></item><item><title>Cayley's Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/459/</link><pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/459/</guid><description>Theorem 1 Every group is isomorphic to some subgroup of the symmetric group. Explanation This concise yet significant theorem conveys the message that by studying symmetric groups, one can grasp all groups. Proof Although the proof may seem tedious at first glance, the techniques involved are quite interesting, so it is recommended to try and follow it at least once. Part 1. $f : G \to G'$ is injective then</description></item><item><title>Separation Properties in Topology</title><link>https://freshrimpsushi.github.io/en/posts/454/</link><pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/454/</guid><description>Definition 1 Let&amp;rsquo;s consider $X$ as a topological space. For $a,b \in X$, if $a \ne b$ and $U, V \subset X$ are open sets in $X$, then: $T_{0}$: If there exists $U$ that contains only one of $a$ and $b$, then $X$ is called a Kolmogorov space. $T_{1}$: For any $a,b$, if there exists $U,V$ satisfying $$ a \in U, b \notin U \\ a \notin V, b \in</description></item><item><title>Trigonometric Representation of the Beta Function</title><link>https://freshrimpsushi.github.io/en/posts/458/</link><pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/458/</guid><description>Theorem $$ B(p,q) = 2 \int_{0}^{{\pi} \over {2}} \left( \sin \theta \right) ^{2p-1} \left( \cos \theta \right) ^{2q-1} d \theta $$ Description No matter what kind of mathematics it is, being able to express a function in a different way is a good thing. Proof If we substitute from $\displaystyle B(p,q) = \int_{0}^{1} t^{p-1} (1-t)^{q-1} dt$ to $t = \sin^2 \theta$, $$ B(p,q) = \int_{0}^{{\pi} \over {2}} \left( \sin^2 \theta</description></item><item><title>Klein Four-group</title><link>https://freshrimpsushi.github.io/en/posts/453/</link><pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/453/</guid><description>Definition 1 Given $V = \left\{ e, a, b, c \right\}$ and the binary operation $\cdot$, $\left&amp;lt; V , \ \cdot \ \right&amp;gt;$ is referred to as the Klein four-group. Description As you can see, since the number of elements is only $4$, including the identity element, it does not possess very rich properties. However, it serves as a very good example for getting acquainted with the concept of groups</description></item><item><title>Proof of the Multiplicative Property of Totient Functions</title><link>https://freshrimpsushi.github.io/en/posts/452/</link><pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/452/</guid><description>Theorem 1 $$ \gcd (n , m) =1 \implies \phi ( n m ) = \phi (n) \phi (m) $$ Description It is an essential property to obtain various important results derived from the torsion function. There&amp;rsquo;s definitely a condition called $\gcd (n , m) =1$, so don&amp;rsquo;t be mistaken for it being almighty. Proof Without loss of generality, $$ nm = p_{1}^{{k}_{1}} p_{2}^{{k}_{2}} \cdots p_{r}^{{k}_{r}} \\ p_{1} &amp;lt; p_{2}</description></item><item><title>Generalization of Binomial Coefficients: Beta Function</title><link>https://freshrimpsushi.github.io/en/posts/450/</link><pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/450/</guid><description>Theorem: Binomial Coefficients Expressed Through the Beta Function $0 \le k\le n$ is satisfied for two natural numbers $k,n$, the following equation holds. $$ \binom{n}{k}={}_{n}C_{k}=C(n,k)=\frac{1}{(n+1)B(n-k+1,k+1)} $$ For two natural numbers $m,n$, the following equation holds. $$ B(m,n)=\left[ \frac{mn}{m+n} \begin{pmatrix} m+n \\ n \end{pmatrix}\right]^{-1} $$ Description The beta function, defined by $B(p,q):=\displaystyle \int_{0}^{1}t^{p-1}(1-t)^{q-1}dt$, can also be seen as a generalization of binomial coefficients. The proof is not difficult, but a lemma</description></item><item><title>Mean and Variance of the Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/444/</link><pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/444/</guid><description>Formulas $X \sim U[a,b]$ Surface $$ E(X) = {{ a+b } \over { 2 }} \\ \Var(X) = {{ (b-a)^{2} } \over { 12 }} $$ Derivation Strategy: Directly deduce from the definition of the uniform distribution. Definition of Uniform Distribution: For $[a,b] \subset \mathbb{R}$, a continuous probability distribution with the following probability density function]] $U[a,b]$ is called a uniform distribution. $$ f(x) = {{ 1 } \over { b</description></item><item><title>Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/443/</link><pubDate>Sat, 24 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/443/</guid><description>Definition 1 Continuous For $[a,b] \subset \mathbb{R}$, a continuous probability distribution $U(a,b)$ with the following probability density function is called the Uniform Distribution. $$ f(x) = {{ 1 } \over { b - a }} \qquad , x \in [a,b] $$ Discrete For a finite set $\left\{ x_{k} \right\}_{k=1}^{n}$, a discrete probability distribution with the following probability mass function is called the Uniform Distribution. $$ p \left( x_{k} \right) =</description></item><item><title>Schwarz-Christoffel Mapping</title><link>https://freshrimpsushi.github.io/en/posts/449/</link><pubDate>Sat, 24 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/449/</guid><description>Theorem 1 On the complex plane, a polygon with $n$ angles is called $\mathscr{P}$, and let&amp;rsquo;s denote these angles as $w_{r}$ and the size of its interior angles as $\psi_{r}$. Then, for $K, C, z_{0} \in \mathbb{C}$ and $x_{r} \in \mathbb{R}$, the transformation that satisfies $f(x_{r}) = w_{r}$ $$ w = f(z) = K \int_{z_{0}}^{z} \prod_{r = 1}^{n} ( \zeta - x_{r})^{ \psi_{r} / \pi - 1 } d \zeta</description></item><item><title>Joukowsky Transformation</title><link>https://freshrimpsushi.github.io/en/posts/448/</link><pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/448/</guid><description>Definition 1 Let&amp;rsquo;s assume $\displaystyle w = f(z) = a z + {{b} \over {z}}$. If $a=b$, then $f$ is called the Joukowski Transform, which maps a circle not centered at $0$ to an airfoil shape. [1]: $f$ maps a circle centered at $0$ to an ellipse. [2]: $f$ maps a semi-infinite line starting from $0$ to a hyperbola. Explanation Zhukovsky is a Soviet physicist who made contributions in the</description></item><item><title>Conformal Mapping by Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/447/</link><pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/447/</guid><description>Theorem 1 Conformal mapping $w = f(z) = \sin z$ maps vertical lines $y=k$ to ellipses and horizontal lines $x = k$ to hyperbolas. Proof Suppose $$ z = x + iy \\ w = u + i v $$, then $$ u = \sin x \cosh y \\ v = \cos x \sinh y $$. Let $y = k$, then $$ {{ u^2 } \over { \cosh^{2} k}} =</description></item><item><title>Conformal Mapping by Exponential Function</title><link>https://freshrimpsushi.github.io/en/posts/446/</link><pubDate>Wed, 21 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/446/</guid><description>Theorem 1 Conformal mapping $w = f(z) = e^{z} = e^{x} e^{i y}$ maps a rectangle to a sector or an annulus. Explanation $f(z) = e^{z}$ is clearly a conformal mapping, but since it is not injective, various restrictions are necessary when considering its inverse mapping. Osborne (1999). Complex variables and their applications: p217.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>What are Successive Properties in Topology?</title><link>https://freshrimpsushi.github.io/en/posts/451/</link><pubDate>Wed, 21 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/451/</guid><description>Buildup: Subspaces Let&amp;rsquo;s say for a topological space $(X, \mathscr{T})$, $Y \subset X$. If we set $\mathscr{T}&amp;rsquo; := \left\{ U \cap Y \ | \ U \in \mathscr{T} \right\}$, then $(Y , \mathscr{T}&amp;rsquo; )$ becomes a subspace of $X$ and $\mathscr{T} ' $ is called the subspace topology induced by $\mathscr{T}$ on $Y$. [1]: The necessary and sufficient condition for $A \subset Y$ to be a closed subset in $Y$</description></item><item><title>Topological Properties</title><link>https://freshrimpsushi.github.io/en/posts/440/</link><pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/440/</guid><description>Definition 1 Two spaces $X,Y$ that are homeomorphic share a property $X$. If $Y$ also has this property $P$, then $P$ is known as a Topological Property. Examples of topological properties include: [2]: First Countability [3]: Second Countability [7]: Fixed Point Property [9]: Countably Compactness Explanation Just as isomorphisms are important in algebra, homeomorphisms are crucial in topology for the same reason. Demonstrating the existence of a homeomorphism allows us</description></item><item><title>Torsion Function</title><link>https://freshrimpsushi.github.io/en/posts/445/</link><pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/445/</guid><description>See Also Totient Function in Analytic Number Theory Definition 1 The function defined as follows, $\phi$, is called the Euler&amp;rsquo;s totient function. $$ \phi ( m ) := \left| \left\{ a \ | \ 1 \le a \le m \land \gcd (a,m) = 1 \right\} \right| = m \prod_{p \mid m} \left( 1 - {{1} \over {p}} \right) $$ Explanation The term totient comes from the Tot- in Total meaning</description></item><item><title>Conformal Mapping of a Parabola onto a Half-Plane</title><link>https://freshrimpsushi.github.io/en/posts/437/</link><pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/437/</guid><description>Theorem 1 A conformal mapping $\displaystyle w = f(z) = z^{1/2}$ maps parabolas to half-planes. Explanation Considering what we learned from $\mathbb{R}^2$, it might seem obvious, but it&amp;rsquo;s necessary to check whether this holds in the complex plane as well. If you want to cleanly divide it along the y-axis, taking $\xi = w - a$ again will do the trick. Proof Given $$ z = x + i y</description></item><item><title>In English: Various Mappings in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/439/</link><pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/439/</guid><description>Definitions Let&amp;rsquo;s talk about a group $\left&amp;lt; G , \ast\ \right&amp;gt; , \left&amp;lt; G' , *' \right&amp;gt;$ and refer to it as $\phi : G \to G'$. If $\forall x ,y \in G $, $\phi (x \ast\ y) = \phi (x ) *' \phi ( y)$ then we call $\phi$ a Homomorphism. If a homomorphism $\phi$ is injective, then we call $\phi$ a Monomorphism and denote it $G \hookrightarrow G'$.</description></item><item><title>Homotopy in Topological Spaces</title><link>https://freshrimpsushi.github.io/en/posts/438/</link><pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/438/</guid><description>Definition 1 For two topological spaces $X,Y$, if a bijection $f : X \to Y$ exists such that both $f$ and its inverse function $f^{-1}$ are continuous functions, then $f$ is called a Homeomorphism, and the two topological spaces are said to be Homeomorphic. Theorem The following propositions are equivalent. (1): $f : X \to Y$ is a homeomorphism. (2): $f^{-1} : Y \to X$ is a homeomorphism. (3): $f</description></item><item><title>Mapping a Trapezoid to a Circle through Conformal Mapping</title><link>https://freshrimpsushi.github.io/en/posts/436/</link><pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/436/</guid><description>Theorem 1 A conformal mapping $\displaystyle w = f(z) = z^{n}$ maps a sector to a semicircle. Explanation If we consider the radius of the sector to be infinite, then $f$ can be thought of as mapping an angle to a straight angle and its interior to a half-plane. Similarly, since a semicircle is also a sector and a half-plane is also an angle, by applying $\xi = w^{2}$ once</description></item><item><title>Conformal Mapping of a Semicircle onto Quadrants</title><link>https://freshrimpsushi.github.io/en/posts/434/</link><pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/434/</guid><description>Theorem 1 The conformal mapping $\displaystyle w = f(z) = {{z - a} \over {z + a}}$ maps the semicircle to the quadrant. Explanation $\displaystyle w = {{z - a} \over {z + a}}$ is a function without a specific name but is very important and frequently used. Be sure to directly calculate $f(a) = 0$, $f(ai) = i$, $f(-a) = \infty$ to verify. Osborne (1999). Complex variables and their</description></item><item><title>Open Functions and Closed Functions</title><link>https://freshrimpsushi.github.io/en/posts/435/</link><pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/435/</guid><description>Definition Let&amp;rsquo;s say for a topological space $X,Y$ that $f : X \to Y$. For every open set $O \subset X$, if $f (O)$ is an open set in $Y$, then $f$ is called an open function. For every closed set $C \subset X$, if $f (C)$ is a closed set in $Y$, then $f$ is called a closed function. Theorem In particular, a continuous function has the following property:</description></item><item><title>Proof of Wilson's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/122/</link><pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/122/</guid><description>Theorem 1 For a prime number greater than 2, $p$, we have $(p-1)! \equiv -1 \pmod{p}$ Description While not quite as famous as Fermat&amp;rsquo;s Little Theorem, Wilson&amp;rsquo;s Theorem is also quite useful in various contexts. Its formulation is conveniently suited for calculating the product of consecutive numbers. Proof This section introduces two proofs for $\pmod{p}$: Proof 1 utilizing the existence and uniqueness of the multiplicative inverse, and Proof 2 which</description></item><item><title>Continuous in Topology</title><link>https://freshrimpsushi.github.io/en/posts/432/</link><pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/432/</guid><description>Definition For a topological space $(X, \mathscr{T}_{X} )$ and $(Y, \mathscr{T}_{Y} )$, let&amp;rsquo;s denote by $f: X \to Y$. A function $f$ is said to be continuous at $a$ if, for every neighborhood $V \in \mathscr{T}_{Y}$ containing $f(a)$, there exists a neighborhood $U \in \mathscr{T}_{X}$ containing $a$ such that $f(U) \subset V$ is satisfied. If $f$ is continuous at every point of $X$, it is called a continuous function and</description></item><item><title>Inversion in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/433/</link><pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/433/</guid><description>Definition 1 Consider a line $L: 2px + 2qy + c = 0$ and a circle $\mathscr{C}: |z - A | = r$, and a point $P: z = x + iy$ that is not on them. The inverse point is defined as follows. A $Q: z^{ \ast } = x^{ \ast } + i y^{ \ast }$ that satisfies $\displaystyle {{y - y^{ \ast }} \over {x - x^{</description></item><item><title>Proof of Fermat's Little Theorem</title><link>https://freshrimpsushi.github.io/en/posts/121/</link><pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/121/</guid><description>Theorem 1 Prime numbers $p$ and integers $a$ that are coprime to each other, $a^{p-1} \equiv 1 \pmod{p}$ Explanation Fermat&amp;rsquo;s little theorem is a straightforward yet highly influential theorem used in a variety of fields. Although there is a generalized theorem by Euler, Fermat&amp;rsquo;s little theorem is often sufficient. It is particularly essential in fields like cryptography that extensively deal with exponentiation in finite fields. Proof Strategy: The proof is</description></item><item><title>Infinite Cyclic groups in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/431/</link><pubDate>Sun, 11 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/431/</guid><description>Definition 1 A subgroup $D_{n} \leqslant S_{n}$ of the symmetric group comprised solely of permutations that rotate and reflect a $n$-sided polygon is defined as the Dihedral group. Description Since it is derived from geometrical figures, it&amp;rsquo;s hard to explain just with words. $D_{3} = S_{3}$ An example of the smallest dihedral group is the symmetric group $D_{3} = S_{3}$. $| D_{n} | =2n$ Such permutations for a $n$-sided polygon</description></item><item><title>Primitive Pythagorean triples are coprime.</title><link>https://freshrimpsushi.github.io/en/posts/429/</link><pubDate>Sun, 11 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/429/</guid><description>Theorem Given three natural numbers satisfying $a^2 + b^2 = c^2$, if $a,b,c$ then $$ \gcd (a,b) = 1 \\ \gcd (b,c) = 1 \\ \gcd (c,a) = 1 $$ Explanation At first glance, it might seem obvious whether it&amp;rsquo;s about Pythagorean triples or anything else, but it&amp;rsquo;s not necessarily the case when considering the greatest common divisor. For example, without the condition of being Pythagorean triples, $\gcd (6,10,15) =</description></item><item><title>Equivalence Conditions for Bases in Topology</title><link>https://freshrimpsushi.github.io/en/posts/430/</link><pubDate>Sat, 10 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/430/</guid><description>Definition 1 A basis $\mathscr{B}$ for the topology $\mathscr{T}$ and a basis $\mathscr{B} ' $ for the topology $\mathscr{T} ' $, given in the set $X$, are considered to be equivalent if $\mathscr{T} = \mathscr{T} ' $. Theorem The equivalence of bases is a necessary and sufficient condition for satisfying the following two conditions. (i): For all $B \in \mathscr{B}$ and $x \in B$, there exists $B ' \in \mathscr{B}</description></item><item><title>Primitive Pythagorean Triples Can Be Expressed Using Only Two Odd Numbers</title><link>https://freshrimpsushi.github.io/en/posts/428/</link><pubDate>Sat, 10 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/428/</guid><description>Theorem 1 For three natural numbers $a^2 + b^2 = c^2$ that satisfy $a,b,c$, there exist two coprime odd numbers $s&amp;gt;t$ that satisfy $$ \begin{align*} a =&amp;amp; st \\ b =&amp;amp; {{s^2 - t^2 } \over {2}} \\ c =&amp;amp; {{s^2 + t^2 } \over {2}} \end{align*} $$. Description According to this theorem, the term &amp;lsquo;Pythagorean triple&amp;rsquo; becomes somewhat of a misnomer. Being able to reduce the number of variables</description></item><item><title>Cross Ratio in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/426/</link><pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/426/</guid><description>Definition 1 On the extended complex plane, for four distinct points $ z_{1} , z_{2} , z_{3} , z_{4} \in \overline{ \mathbb{C} }$, the following is defined as the Cross Ratio: $$ (z_{1} , z_{2} , z_{3} , z_{4} ) = {{( z_{1} - z_{4})( z_{3} - z_{2})} \over {(z_{1} - z_{2}) ( z_{3} - z_{4}) } } $$ Description If we change the form a bit to $\displaystyle (z_{1}</description></item><item><title>Subbasis in Topology</title><link>https://freshrimpsushi.github.io/en/posts/427/</link><pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/427/</guid><description>Definition 1 Let&amp;rsquo;s say there is a topological space $\left( X , \mathscr{T} \right)$ and consider $\mathscr{S} \subset \mathscr{T}$. When $\displaystyle \mathscr{B} = \left\{ \left. B = \bigcap_{ i = 1}^{n} S_{i} \ \right| \ S_{i} \in \mathscr{S} \right\}$ becomes the basis for $\mathscr{T}$, then $\mathscr{S}$ is called a Subbasis for $\mathscr{T}$. Explanation The reason why accepting the concept of subbasis can be difficult is because the term &amp;lsquo;sub&amp;rsquo; in</description></item><item><title>One of the Pythagorean triples must be a multiple of three.</title><link>https://freshrimpsushi.github.io/en/posts/96/</link><pubDate>Thu, 08 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/96/</guid><description>Definition 1 A natural number $a,b,c$ that satisfies $a^2 + b^2 = c^2$, then either $a$ or $b$ is a multiple of $3$. Explanation Among the Pythagorean triples, not only is one of them always even, but at least one is also a multiple of $3$. Proof Consider a natural number $n$ and divide it into three cases based on the remainder $1, 2, 0$ when divided by $3$. Case</description></item><item><title>One of the Pythagorean Triples Must Be an Even Number</title><link>https://freshrimpsushi.github.io/en/posts/416/</link><pubDate>Wed, 07 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/416/</guid><description>Theorem 1 A natural number $a,b,c$ that satisfies $a^2 + b^2 = c^2$ implies that either $a$ or $b$ is even. Explanation Interestingly, one of the Pythagorean triples must always be even. Proof Since the square of an even number is even and the square of an odd number is odd, if $c^2$ is odd, then either $a^2$ or $b^2$ must be even. Assuming $c^2$ is even, then both $a^2$</description></item><item><title>Pythagorean Triple</title><link>https://freshrimpsushi.github.io/en/posts/415/</link><pubDate>Wed, 07 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/415/</guid><description>Definition 1 A set of three natural numbers $(a,b,c)$ that satisfies $a^2 + b^2 = c^2$ is called a Pythagorean triple. If the three natural numbers have no common factor, they are called a Primitive Pythagorean Triple. Description For convenience, let&amp;rsquo;s call the numbers included in a Pythagorean triple Pythagorean numbers. Examples of Pythagorean triples, as everyone well knows, include $(3, 4, 5)$ and $(5, 12, 13)$ among others. The</description></item><item><title>Circles are invariant under bilinear transformation in the extended complex plane.</title><link>https://freshrimpsushi.github.io/en/posts/423/</link><pubDate>Tue, 06 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/423/</guid><description>Theorem 1 Every bilinear transformation maps a circle described by $\overline { \mathbb{C} }$ to another circle described by $\overline { \mathbb{C} }$. Proof Let&amp;rsquo;s represent the general equation of a circle as $$ a ( x^2 + y^2 ) + 2p x + 2q y + c = 0 $$ and if we set $B := p - iq$, for the $z = x + i y$ on the</description></item><item><title>Euler's Proof: There are Infinitely Many Primes</title><link>https://freshrimpsushi.github.io/en/posts/420/</link><pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/420/</guid><description>정리 Primes are infinitely many.</description></item><item><title>Fundamental Theorem of Arithmetic Proof</title><link>https://freshrimpsushi.github.io/en/posts/419/</link><pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/419/</guid><description>Theorem 1 Every natural number $n &amp;gt;2$ has a unique prime factorization $n = p_{1} p_{2} \cdots p_{r}$. The order of the primes $p_{1} , p_{2} , \cdots , p_{r}$ is ignored. Explanation It may seem strange that a property we&amp;rsquo;ve naturally used since elementary school requires proof, but it is extremely important. The fact that this is so simple perhaps serves as evidence that it merits the name theorem.</description></item><item><title>Bilinear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/422/</link><pubDate>Sun, 04 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/422/</guid><description>Definition 1 A mapping $f$ that is conformal in its domain is called as follows: Translation $f(z) = z + \alpha$ Magnification: $f(z) = \rho z$ Rotation: $f(z) = e^{i \theta} z$ Inversion: $f(z) = {{1} \over {z}}$ Bilinear Transformation: $\displaystyle f(z) = {{ \alpha z + \beta } \over { \gamma z + \delta }}$ In translation, we have $\alpha \in \mathbb{C}$, and in magnification, we have $\rho \in</description></item><item><title>Prime Factorization Theorem</title><link>https://freshrimpsushi.github.io/en/posts/418/</link><pubDate>Sun, 04 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/418/</guid><description>Theorem 1 A prime number $p$ that divides a natural number $ n : = d_{1} d_{2} \cdots d_{r}$ into $p \mid n$ must divide at least one of $d_{1} , d_{2} , \cdots , d_{r}$. Description $p \mid n$ means that $n$ is a multiple of $p$, that is, $p$ divides $n$. At first glance, it might seem obvious, but it&amp;rsquo;s a property of prime numbers that definitely requires</description></item><item><title>Proof of the Extended Euclidean Theorem</title><link>https://freshrimpsushi.github.io/en/posts/417/</link><pubDate>Sun, 04 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/417/</guid><description>Theorem 1 For two integers $a,b$, there always exists an integer solution for $ax + by = \gcd (a,b)$. Description This theorem is also known as the Linear Congruence Theorem as it implies that $\gcd (a,b)$ can be expressed as a linear equation involving $a$ and $b$. Although its appearance seems somewhat complex and it only discusses existence, it is surprisingly widely used. It might not provide a specific solution</description></item><item><title>Symmetry groups in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/421/</link><pubDate>Sun, 04 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/421/</guid><description>Definition 1 A permutation is a bijection $\phi : A \to A$ for a set $A$. $S_{A}$ is the set of all permutations of $A$, which forms a group $\left&amp;lt; S_{A} , \circ \right&amp;gt;$ with respect to function composition $\circ$, and is called the symmetric group. Explanation The fact that symmetric groups indeed satisfy the conditions of a group can be easily ascertained, given that a permutation is defined as</description></item><item><title>First Countability and Second Countability of Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/414/</link><pubDate>Sat, 03 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/414/</guid><description>Theorem [1]: Every metric space is first-countable. [2]: Every separable metric space is second-countable. Description After seeing all sorts of abstract spaces in topology, one realizes how convenient and nice metric spaces are. Proof [1] For a metric space $\left( X , d \right)$, if we say $x \in X$, $$ \left\{ \left. B_{d} \left(x , {{1} \over {n}} \right) \ \right| \ n \in \mathbb{N} \right\} $$ is a</description></item><item><title>Bases and Local Bases in Topology</title><link>https://freshrimpsushi.github.io/en/posts/412/</link><pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/412/</guid><description>Definition Let&amp;rsquo;s say topological space $\left( X , \mathscr{T} \right)$ with respect to $\mathscr{B} , \mathscr{B}_{x} \subset \mathscr{T}$. When we say $B_{\lambda} \in \mathscr{B}$, if for every $U \in \mathscr{T}$ there exists a neighbourhood $\Lambda$ that satisfies $$ U = \bigcup_{\lambda \in \Lambda} B_{ \lambda } $$ then $\mathscr{B}$ is called a basis for $\mathscr{T}$. In this case, the topology $\mathscr{T}$ is said to be generated by $\mathscr{B}$. When we</description></item><item><title>The First Countable and the Second Countable</title><link>https://freshrimpsushi.github.io/en/posts/413/</link><pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/413/</guid><description>Definition 1 Let&amp;rsquo;s suppose that a topological space $X$ is given. If every point $x \in X$ has a countable local base, it is called a first-countable space. If $X$ has a countable base, it is called a second-countable space. Explanation Through the concepts of base and local base, a new branch of countability has been created. Examples of spaces not being first-countable Discrete spaces $\left( \mathbb{R} , \mathscr{T}_{f} \right)$</description></item><item><title>Conformal Mapping Preserves the Angles</title><link>https://freshrimpsushi.github.io/en/posts/410/</link><pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/410/</guid><description>Theorem 1 In the complex domain complex plane $\mathscr{R}$, let function $f$ be a conformal mapping, and let curves $\mathscr{C}_{1}$ and $\mathscr{C}_{2}$ meet at point $\alpha$ with an internal angle $\psi$. If $\mathscr{C}_{1} ' $ and $\mathscr{C}_{2} ' $ are the images of $\mathscr{C}_{1}$ and $\mathscr{C}_{2}$ under $f$, respectively, then the two curves meet at $\beta = f ( \alpha )$ and their internal angle is also $\psi$. Explanation Though</description></item><item><title>Proving That All Cyclic groups are Isomorphic to the Integer group</title><link>https://freshrimpsushi.github.io/en/posts/411/</link><pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/411/</guid><description>Theorem 1 If a cyclic group $\left&amp;lt; a \right&amp;gt;$ is a finite group, then $\left&amp;lt; a \right&amp;gt; \simeq \mathbb{Z}_{n}$, and if it is an infinite group, then $\left&amp;lt; a \right&amp;gt; \simeq \mathbb{Z}$. Explanation With this theorem, the exploration of cyclic groups is virtually complete. The fact that groups, which were only abstract, suddenly fall into the domain of number theory means that there is quite a lot that can be</description></item><item><title>Proof of the Inverse Function Theorem in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/408/</link><pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/408/</guid><description>Theorem 1 A function $f : \mathbb{C} \to \mathbb{C}$ that is analytic at $\alpha$ and satisfies $f ' (\alpha) \ne 0$ exists in the region $\mathcal{N} \left( f(\alpha) \right)$, where $f^{-1}$ exists. Explanation Consider the condition given by $f ' (\alpha) \ne 0$. When thought of as a real function, it implies the function is either increasing or decreasing, which is a condition for the existence of an inverse function.</description></item><item><title>What is a Conformal Mapping in Complex Analysis?</title><link>https://freshrimpsushi.github.io/en/posts/409/</link><pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/409/</guid><description>Definition 1 A function $f: A \subset \mathbb{C} \to \mathbb{C}$ that is analytic at $\mathscr{R} \subset A$ and for all $z \in \mathscr{R}$ satisfies $f ' (z) \ne 0$ is called a Conformal Mapping if $f$. Meanwhile, if there exists a point $\alpha$ that satisfies $f ' (\alpha) = 0$, then $\alpha$ is referred to as the Critical Point of $f$. Description As the Chinese characters for conformal (等</description></item><item><title>Limits of sequence are not unique in general Space</title><link>https://freshrimpsushi.github.io/en/posts/407/</link><pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/407/</guid><description>Theorem In general, in a topological space, the limit of a sequence is not unique. Explanation This might sound surprising, but it is indeed true. Until now, we have been accustomed to the image of intervals containing sequences in analysis narrowing down to converge to a point. However, according to the concept of convergence defined in topology, there is no reason for sequences to converge to a single point depending</description></item><item><title>Loose Topology and Leisurely Mountain Topology</title><link>https://freshrimpsushi.github.io/en/posts/406/</link><pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/406/</guid><description>Definition $X$ is said to be an infinite set. $\mathscr{T}_{f} : = \left\{ \emptyset , X \right\} \cup \left\{ U \subset X : | X \setminus U | &amp;lt; \infty \right\}$ is called the cofinite topology. $\mathscr{T}_{c} : = \left\{ \emptyset , X \right\} \cup \left\{ U \subset X : | X \setminus U | = \aleph_{0} \right\}$ is called the cocountable topology. Aleph null $\aleph_{0}$ refers to the cardinality</description></item><item><title>Self-Evident Topology and Discrete Topology</title><link>https://freshrimpsushi.github.io/en/posts/404/</link><pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/404/</guid><description>Definition 1 Given a set $X$, providing it with the trivial topology $\left\{ \emptyset , X \right\}$ makes it the smallest space, called the trivial space. Conversely, endowing it with the discrete topology $\mathscr{P}(X)$ makes it the largest space, known as the discrete space. Sierpinski Space If the topology of $S : = \left\{ 0, 1 \right\}$ is $\mathscr{T} : = \left\{ \emptyset , \left\{ 1 \right\} , \left\{ 0,</description></item><item><title>Topological Spaces: Separability and Closure</title><link>https://freshrimpsushi.github.io/en/posts/405/</link><pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/405/</guid><description>Definition 1 For a topological space $X$, let&amp;rsquo;s say $A \subset X$. When an open set $O$ exists that satisfies $x \in O \subset A$, $x$ is called the interior point of $A$. The set of interior points of $A$, $A^{\circ}$, is called the interior of $A$. The union of $A$ and its codomain $\overline{A} : = A \cup a '$ is called the closure of $A$. When $x \in</description></item><item><title>Isomorphism in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/403/</link><pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/403/</guid><description>Definition 1 For two binary operation structures $\left&amp;lt; S , * \right&amp;gt;$ and $\left&amp;lt; S' , *' \right&amp;gt;$, if there exists a bijective function $\phi : S \to S'$ such that for all $x , y \in S$, $$ \phi (x \ast\ y) = \phi ( x ) *' \phi ( y ) $$ is satisfied, then $\phi$ is called an isomorphism, and $S$ and $S'$ are said to be</description></item><item><title>Prove that a Subgroup of a Cyclic group is Cyclic</title><link>https://freshrimpsushi.github.io/en/posts/402/</link><pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/402/</guid><description>Definition 1 A subgroup $ H \leqslant G$ of a cyclic group $G$ is a cyclic group. Explanation Although it might seem obvious upon a bit of thought, it is a considerably important theorem and its proof is not as straightforward as it might appear. Proof If $H = \left\{ e \right\}$, then $H = \left&amp;lt; e \right&amp;gt;$ hence it is a cyclic group. If $H \ne \left\{ e \right\}$,</description></item><item><title>Limit Points and Convergence in Topological Spaces, Image Sets</title><link>https://freshrimpsushi.github.io/en/posts/400/</link><pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/400/</guid><description>Definition 1 Let&amp;rsquo;s assume a topological space $\left( X , \mathscr{T} \right)$ is given. If for $A \subset X$ any open set $O$ containing $x$ satisfies $O \cap ( A \setminus \left\{ x \right\} ) \ne \emptyset$, then $x$ is called a limit point of $A$, and the set of all limit points of $A$ is called the derived set of $A$. A sequence $\left\{ x_{n} \right\}$ in $X$ converges</description></item><item><title>Prove that All Cyclic groups are Abelian</title><link>https://freshrimpsushi.github.io/en/posts/401/</link><pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/401/</guid><description>Theorem 1 All cyclic groups are Abelian. Explanation It is also a fact that follows naturally without needing separate proof, if one shows that cyclic groups are isomorphic to the group of integers. Proof For a cyclic group $G := \left&amp;lt; a \right&amp;gt;$, let $g_{1} = a^{r}$ and $g_{2} = a^{s}$. $$ g_{1} g_{2} = a^{r} a^{s} = a^{r+s} = a^{s+r} = a^{s} a^{r} = g_{2} g_{1} $$ therefore, $G$</description></item><item><title>What is a Topological Space?</title><link>https://freshrimpsushi.github.io/en/posts/398/</link><pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/398/</guid><description>Definition Topological Space 1 Given a set $X$, if $\mathscr{T} \subset \mathscr{P} (X)$ satisfies the following three conditions for $T \in \mathscr{T}$, then $\mathscr{T}$ is called the topology of $X$, and $\left( X , \mathscr{T} \right)$ is called a Topological Space. (i): $$\emptyset , X \in \mathscr{T}$$ (ii): $$\displaystyle \bigcup_{ \alpha \in \forall } T_{\alpha} \in \mathscr{T}$$ (iii): $$\displaystyle \bigcap_{ i= 1}^{n} T_{i} \in \mathscr{T}$$ Explained in words, conditions (i)~(iii)</description></item><item><title>Completeness and Density in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/396/</link><pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/396/</guid><description>Definition 1 Let&amp;rsquo;s say for a metric space $\left( X , d \right)$. If for every sequence $\left\{ x_{n} \right\}$ in $X$ there exists a natural number $n_{0}$ that satisfies $d(x_{n} , x_{m}) &amp;lt; \varepsilon$ whenever $\varepsilon &amp;gt; 0$ for all $n,m &amp;gt; n_{0}$, it is called a Cauchy sequence. If the limiting points of Cauchy sequences in $\left( X , d \right)$ belong to $X$, then $\left( X ,</description></item><item><title>Gram-Schmidt Orthonormalization</title><link>https://freshrimpsushi.github.io/en/posts/394/</link><pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/394/</guid><description>Theorem Every finite-dimensional inner product space has an orthonormal basis. Explanation As with most existence proofs, it may not seem long or complex, but it is an extremely important theorem. Many of the logical foundations that support linear algebra rely on the existence of this orthonormal basis. Proof Let one of the bases generating the inner product space $(V, \left\langle \cdot , \cdot \right\rangle)$ be denoted as $\left\{ \mathbf{x}_{1} ,</description></item><item><title>Cyclic groups in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/392/</link><pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/392/</guid><description>Definition 1 An element $a$ of a group $G$ is termed a cyclic group if there exists an integer $n \in \mathbb{Z}$ such that for any $x \in G$, $x = a^{n}$ holds. It is denoted as $\braket{a}$. The element $a$ is called the generator. Explanation Simply put, a cyclic group is one where every element can be expressed as a power of the generator. Since all elements are represented</description></item><item><title>Euler's Proof: Finding the Sum of Reciprocals of Squares Using the Sinc Function</title><link>https://freshrimpsushi.github.io/en/posts/391/</link><pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/391/</guid><description>Theorem $$ \sum_{n =1 }^{\infty} {{1} \over {n^2}} = {{ \pi ^2 } \over { 6 }} $$ Proof Strategy: This proof, left by Euler, uses the Euler representation of the sinc function to provide a solution. The idea is quite fresh and interesting, making it harder to forget once you&amp;rsquo;ve seen it. Euler representation of the sinc function: $$ {{\sin x} \over {x}} = \prod_{n=1}^{\infty} \left( 1 - {{x^2}</description></item><item><title>Summation of Inverse Squares Using Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/390/</link><pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/390/</guid><description>Theorem 1 $$ \sum_{n =1 }^{\infty} {{1} \over {n^2}} = {{ \pi ^2 } \over { 6 }} $$ Euler&amp;rsquo;s approach is neat and elegant, but it&amp;rsquo;s so ingenious that there aren&amp;rsquo;t many practical applications. The joy of studying complex analysis is that such shortcuts to results are readily available. It&amp;rsquo;s a good example, so try solving it yourself. Proof If we define $\displaystyle f(z) : = {{1} \over {z^2}}$</description></item><item><title>Cotangent and Cosecant's Laurent Expansion</title><link>https://freshrimpsushi.github.io/en/posts/389/</link><pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/389/</guid><description>Formula $$ \cot z = {{1} \over {z}} - {{z} \over {3}} - {{z^{3}} \over {45}} - {{2 z^{5}} \over {945}} - \cdots \\ \csc z = {{1} \over {z}} + {{z} \over {6}} + {{7 z^{3}} \over {360}} + {{31 z^{5}} \over {15120}} + \cdots $$ Description In complex analysis, to use the sum of series formula, it is necessary to be able to find the residue of functions</description></item><item><title>Summation Formulas for All Integers Using Residue Theorem</title><link>https://freshrimpsushi.github.io/en/posts/388/</link><pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/388/</guid><description>Formulas The ratio of a polynomial function, that is, a rational function $f$, assumes $f(n) \ne 0$ at $n \in \mathbb{Z}$ while being $\lim_{z \to \infty} z f(z) = 0$. When $f$ has a finite singularity $z_{1}, \cdots , z_{m}$, $$ \sum_{n=-\infty}^{\infty} f(n) = - \sum_{n = 1}^{m} \text{Res}_{z_{n}} (\pi f(z) \cot \pi z) $$ Explanation It&amp;rsquo;s meaningful not just to sum up all natural numbers but to represent the</description></item><item><title>Balls and Open Sets, Closed Sets in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/382/</link><pubDate>Thu, 18 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/382/</guid><description>Definition Given a metric space where $\left( X, d \right)$, let $a \in X$ and $r &amp;gt; 0$. An open ball with center $a$ and radius $r$ is denoted by $B_{d} (a,r) = \left\{ x \in X \ | \ d(a,x) &amp;lt; r \right\}$. A closed ball with center $a$ and radius $r$ is denoted by $B_{d} [a,r] = \left\{ x \in X \ | \ d(a,x) \le r \right\}$.</description></item><item><title>Definition of a Metric Space</title><link>https://freshrimpsushi.github.io/en/posts/381/</link><pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/381/</guid><description>Definition A function $d : X \times X \to [0, \infty)$ on a set set $X$ is called a distance and $\left( X, d\right)$ is called a metric space if it satisfies the following conditions with respect to $x,y,z \in X$. If the distance is trivial, the metric space is also simply denoted by $X$. $d(x,y)=0 \iff x = y$ $d(x,y) = d(y,x)$ $d(x,y) + d(y,z) \ge d(x,z)$ Explanation As</description></item><item><title>Improper Integrals of Approach Functions</title><link>https://freshrimpsushi.github.io/en/posts/375/</link><pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/375/</guid><description>Buildup 1 The biggest problem when integrating a multivalued function is that the function value can change unexpectedly when the path encounters a branch cut. When integrating such a function, one uses the same trick as before to make the path circumvent the branch cut. Considering the representative multivalued function, log $\log$, with the negative real axis as the branch cut and the origin as the branching point, one can</description></item><item><title>Multiplicity and Branching in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/374/</link><pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/374/</guid><description>Definition 1 A mapping that associates the elements of $X = \mathbb{C}$ with multiple values of $Y$ is called a multifunction. For a multifunction $g$ defined in an open set $A \subset \mathbb{C}$, if there exists at least one closed curve $\mathscr{C}$ that wraps $\alpha \in \mathbb{C}$ and lies within $A$, along which $z-\alpha$ changes by $2\pi$ continuously, and the value $g(z)$ is not the same as the original value,</description></item><item><title>Evaluation of Improper Integrals through the Jordan Lemma</title><link>https://freshrimpsushi.github.io/en/posts/372/</link><pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/372/</guid><description>Description 1 First, similar to the divergent semicircular complex path integration for rational functions’ improper integrals, let&amp;rsquo;s start with two polynomials $p(z) , q(z)$, assuming $\displaystyle f(z) = {{q(z)} \over {p(z)}}$. If a real solution does not exist satisfying $p(z) = 0$, then $f$ would not have a real singularity. Considering an integral in the form of $\displaystyle \int_{- \infty}^{\infty} \sin{mx}f(x)</description></item><item><title>Singularities on the Real Axis and Improper Integrals via Jordan's Lemma</title><link>https://freshrimpsushi.github.io/en/posts/373/</link><pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/373/</guid><description>Buildup The overall flow is similar to Jordan&amp;rsquo;s Lemma for Improper Integrals. For two polynomials $p(z) , q(z)$, let&amp;rsquo;s say $\displaystyle f(z) = {{q(z)} \over {p(z)}}$. If there exists a real solution $a$ that satisfies $p(z) = 0$, then $f$ has a real singularity at $a$. The reason we haven&amp;rsquo;t dealt with such cases until now was to use the Residue Theorem. Of course, just because a singularity is added</description></item><item><title>Proof of Jordan's Lemma</title><link>https://freshrimpsushi.github.io/en/posts/371/</link><pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/371/</guid><description>Theorem 1 When the semicircle $\Gamma$ is represented as $z(\theta) = R e^{i \theta} , 0 \le \theta \le \pi$, if the function $f: \mathbb{C} \to \mathbb{C}$ is continuous on $\Gamma$ and $\displaystyle \lim_{z \to \infty} f(z) = 0$, then for any positive $m \in \mathbb{R}^{+}$, $$ \lim_{R \to \infty} \int_{\Gamma} e^{m i z } f(z) dz = 0 $$ Explanation The pronunciation [Jordan] is not Konglish but comes from</description></item><item><title>Singular Value Decomposition for Least Squares</title><link>https://freshrimpsushi.github.io/en/posts/359/</link><pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/359/</guid><description>Algorithm $A \in \mathbb{C}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{C}^{m}$, let&amp;rsquo;s say $\text{rank} A = n$ and the least squares solution of $A \mathbf{x} = \mathbf{b}$ is $\mathbf{x}_{\ast}$. Step 1. Singular Value Decomposition Find orthogonal matrix $\widehat{U}$, diagonal matrix $\widehat{\Sigma}$, and unitary matrix $V$ that satisfy $A = \widehat{U} \widehat{\Sigma} V^{\ast}$. Step 2. Using $\widehat{U}$ obtained from the singular value decomposition, compute the projection $P : = \widehat{U} \widehat{U}^{\ast}$.</description></item><item><title>Cholesky Decomposition for Least Squares Method</title><link>https://freshrimpsushi.github.io/en/posts/357/</link><pubDate>Fri, 05 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/357/</guid><description>Algorithm Given $A \in \mathbb{C}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{C}^{m}$ such that $\text{rank} A = n$ and the least-squares solution of $A \mathbf{x} = \mathbf{b}$ is denoted as $\mathbf{x}_{\ast}$. Step 1. Multiply both sides of the given equation by $A^{\ast}$ to form the normal equation $A^{\ast} A \mathbf{x} = A^{\ast} \mathbf{b}$. The solution of the normal equation is the least-squares solution of the original equation, hence, solving for</description></item><item><title>QR Decomposition for Least Squares Method</title><link>https://freshrimpsushi.github.io/en/posts/358/</link><pubDate>Fri, 05 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/358/</guid><description>Algorithm $A \in \mathbb{C}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{C}^{m}$, let $\text{rank} A = n$ and the least squares solution of $A \mathbf{x} = \mathbf{b}$ be $\mathbf{x}_{\ast}$. Step 1. QR decomposition Find the orthogonal matrix $\widehat{Q}$ and upper triangular matrix $\widehat{R}$ that satisfy $A = \widehat{Q} \widehat{R}$. Step 2. Using the obtained $\widehat{Q}$ from QR decomposition to compute the projection $P : = \widehat{Q} \widehat{Q}^{\ast}$. Since $A \mathbf{x}_{\ast} =</description></item><item><title>Least Squares Method</title><link>https://freshrimpsushi.github.io/en/posts/356/</link><pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/356/</guid><description>Definition1 Let&amp;rsquo;s say that a linear system $A\mathbf{x} = \mathbf{b}$ with a matrix $A \in \mathbb{C}^{m \times n}$ and a vector $\mathbf{b} \in \mathbb{C}^{m}$ is either overdetermined or underdetermined. In this case, the system either does not have a solution or has infinitely many. Here, consider the problem of minimizing the value of $$ \left\| A \mathbf{x} - \mathbf{b} \right\|_{2} $$ This is called the Least Squares Problem (LSP). The</description></item><item><title>Matrix QR Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/355/</link><pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/355/</guid><description>Overview Efficient matrix decomposition requires several conditions, but before efficiency, whether the decomposition itself is possible or not can be important. QR decomposition is a matrix decomposition method that does not require the condition of being a square matrix. Definition For a matrix $A := \begin{bmatrix} \mathbf{a}_{1} &amp;amp; \cdots &amp;amp; \mathbf{a}_{n} \end{bmatrix} \in \mathbb{C}^{m \times n}_{n}$ with coefficients $n$, let&amp;rsquo;s define the subspace generated by the column vectors up to</description></item><item><title>Matrix Algebra: Projections</title><link>https://freshrimpsushi.github.io/en/posts/354/</link><pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/354/</guid><description>Definition The projection $P \in \mathbb{C}^{m \times m}$ is called an orthogonal projection if it satisfies $\mathcal{C} (P) ^{\perp} = \mathcal{N} (P)$ and $P$. Explanation According to the property of projection $\mathbb{C}^{m } = \mathcal{C} (P) \oplus \mathcal{N} (P)$, it can be seen that $P$ divides $\mathbb{C}^{m}$ into exactly two subspaces, $\mathcal{C} (P)$ and $\mathcal{N} (P)$. The fact that this division satisfies the condition $\mathcal{N} (P) = \mathcal{C} (P) ^{\perp}$</description></item><item><title>Matrix Projection in Linear Algebra</title><link>https://freshrimpsushi.github.io/en/posts/352/</link><pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/352/</guid><description>Definition A square matrix $P \in \mathbb{C}^{m \times m}$ is a projector if $P^2 = P$. Explanation In algebraic terms, this is referred to as an idempotent, which similarly refers to an element like $a^2 = a$. If $P$ is a projection, then $(I-P)^2 = I - 2P + P^2 = I - 2P + P = (I-P)$, so it can be known that $(I-P)$ is also a projection. Such</description></item><item><title>Direct Sum in Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/353/</link><pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/353/</guid><description>Definition A vector space $V$ is said to be the direct sum of its two subspaces $W_{1}$ and $W_{2}$ if it satisfies the following, denoted by $V = W_{1} \oplus W_{2}$. (i) Existence: For any $\mathbf{v} \in V$, there exist $\mathbf{v}_{1} \in W_{1}$ and $\mathbf{v}_{2} \in W_{2}$ satisfying $\mathbf{v} = \mathbf{v}_{1} + \mathbf{v}_{2}$. (ii) Exclusivity: $W_{1} \cap W_{2} = \left\{ \mathbf{0} \right\}$ (iii) Uniqueness: For a given $\mathbf{v}$, there exists</description></item><item><title>Uniqueness Proof of Cholesky Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/351/</link><pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/351/</guid><description>Theorem $A&amp;gt;0$ has a unique Cholesky decomposition. Explanation Eigenvalue diagonalization, Singular value decomposition, Schur decomposition, LU decomposition, LDU decomposition all share the commonality of not being unique. This is because these methods either utilize the relationship between eigenvalues and eigenvectors, or because $1 = a \dfrac{1}{a}$ thus $L$ or $U$ can be divided into parts. However, Cholesky decomposition doesn&amp;rsquo;t use the concept of eigenvalues and is expressed as $A=LL^{T}$, so</description></item><item><title>Cholesky Decomposition of Positive Definite Matrices</title><link>https://freshrimpsushi.github.io/en/posts/350/</link><pubDate>Sat, 30 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/350/</guid><description>Overview Given a reversible matrix to a diagonal matrix, just as we could perform LDU decomposition, when the condition is strengthened to a positive definite matrix, an even more efficient matrix decomposition called Cholesky Decomposition can be done. Build-up Consider a $m \times m$ positive definite matrix $A : = \begin{bmatrix} a_{11} &amp;amp; \mathbf{w}^{T} \\ \mathbf{w} &amp;amp; K \end{bmatrix} &amp;gt; 0$. If $a_{11}$ is positive, $\mathbf{w} \in \mathbb{R}^{m-1}$ and $K</description></item><item><title>Decomposition of Symmetric Matrices into LDU</title><link>https://freshrimpsushi.github.io/en/posts/349/</link><pubDate>Fri, 29 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/349/</guid><description>Overview $L^{T}$ is an upper triangular matrix, so we can consider replacing $U$ in $A = LU$ with $U:= DL^{T}$. As general conditions become stricter than for the LU decomposition, the amount of computation significantly decreases. Theorem For the invertible symmetric matrix $A$, there exist a lower triangular matrix $L$ and a diagonal matrix $D$ satisfying $A = LDL^{T}$. Proof Since $A$ is an invertible matrix, there exist a lower</description></item><item><title>LU Decomposition of Invertible Matrices</title><link>https://freshrimpsushi.github.io/en/posts/345/</link><pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/345/</guid><description>Buildup When the matrix Matrix $A \in \mathbb{R}^{m \times m}$ is multiplied on the left side such that the component $(i, j)$ becomes $0$, the matrix $E_{ij}$ is defined as the $A$ Elimination Operator for $ij$. Specifically, for the square matrix Square Matrix $(a_{ij}) \in \mathbb{R}^{m \times m}$, $E_{ij}$ has the diagonal components as $1$, the $(i,j)$ component as $\displaystyle -m_{ij} = -{{a_{ij}} \over {a_{jj}}}$, and the remaining components as</description></item><item><title>Eigenvalue Diagonalization of Hermitian Matrices: Proof of Spectral Theory</title><link>https://freshrimpsushi.github.io/en/posts/346/</link><pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/346/</guid><description>Summary Let&amp;rsquo;s define the invertible matrix $A \in \mathbb{C}^{m \times m}$, the diagonal matrix composed of its eigenvalues $\lambda_{k}$ as $\Lambda : = \text{diag} ( \lambda_{1} , \cdots , \lambda_{m} )$, and the orthogonal matrix composed of the corresponding orthonormal eigenvectors $\mathbf{q}_{k}$ as $Q$. [1] Spectral Theory The necessary and sufficient condition for $A$ to be a normal matrix is that $A$ is unitarily diagonalizable. $$ A A^{\ast} = A^{\ast}</description></item><item><title>Schur Decomposition of Square Matrices</title><link>https://freshrimpsushi.github.io/en/posts/342/</link><pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/342/</guid><description>Definition For a given unitary matrix $Q$ and upper triangular matrix $T$, if $A = Q T Q^{\ast}$, then $A$ is said to have a Schur Factorization. Theorem Every square matrix $A \in \mathbb{C}^{ m \times m}$ has a Schur Factorization. Explanation The downside of eigenvalue diagonalization is that when it is decomposed into $A = S \Lambda S^{-1}$, it still requires the effort to find $S^{-1}$. Although it is</description></item><item><title>Proof of the Existence of a Complete Singular Value Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/341/</link><pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/341/</guid><description>Overview Eigenvalue diagonalization was limited to square matrices in application, but Singular Value Decomposition (SVD) had no such constraint. Determining whether such useful decomposition methods universally apply to all matrices, i.e., the existence of decomposition, is considered a significantly important issue. Theorem For three natural numbers $m \ge n \ge r = \text{rank} A$, the matrix $A \in \mathbb{R}^{m \times n}$ possesses an SVD. Proof For any vector $\mathbf{x} \ne</description></item><item><title>Singular Value Decomposition of a Matrix</title><link>https://freshrimpsushi.github.io/en/posts/340/</link><pubDate>Sat, 23 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/340/</guid><description>Overview While it would be great if every matrix could be decomposed through eigenvalue diagonalization, unfortunately, this method is limited by the requirement that the given matrix must be a square matrix. We aim to extend decomposability to matrices that are not square. Buildup Let us consider two natural numbers $m &amp;gt; n$ for a matrix $A \in \mathbb{C}^{ m \times n}$ whose coefficients are given by $\text{rank} A =</description></item><item><title>Eigenvalue Diagonalization of Invertible Matrices</title><link>https://freshrimpsushi.github.io/en/posts/339/</link><pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/339/</guid><description>Definition If there exists a unitary matrix $Q$ and a diagonal matrix $\Lambda$ that satisfy $A = Q^{ \ast } \Lambda Q$ for $A \in \mathbb{C}^{ m \times m }$, then the matrix $A$ is said to be unitarily diagonalizable.</description></item><item><title>Matrix Representation of the Harmonic Oscillator Operator</title><link>https://freshrimpsushi.github.io/en/posts/367/</link><pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/367/</guid><description>Explanation Harmonic 🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 The Schrödinger equation is a linear equation, so a linear combination of multiple wavefunctions that satisfy the equation also satisfies the equation. If the eigenfunctions of each state of the harmonic oscillator are $\ket{\psi_{0}}$, $\ket{\psi_{1}}$,</description></item><item><title>Matrix Representation of Operators in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/366/</link><pubDate>Sun, 17 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/366/</guid><description>Build-up Let&amp;rsquo;s consider two unit vectors $\widehat{\mathbf{x}} = (1, 0)$ and $\widehat{\mathbf{y}} = (0, 1)$ in a 2-dimensional space. The coordinate vector of an arbitrary point $(a, b)$ in this space can be expressed as a linear combination of these two unit vectors as follows. $$ (a, b) = a(1, 0) + b(0, 1) \implies \begin{bmatrix} a \\ b \end{bmatrix} = a\begin{bmatrix} 1 \\ 0 \end{bmatrix} + b\begin{bmatrix} 0 \\</description></item><item><title>Divergent Semicircle Complex Path Integral for the Improper Integral of Rational Functions</title><link>https://freshrimpsushi.github.io/en/posts/338/</link><pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/338/</guid><description>Buildup Let&amp;rsquo;s assume for two polynomial functions $p(z) , q(z)$ that $\displaystyle f(z) = {{q(z)} \over {p(z)}}$. If there are no real solutions satisfying $p(z) = 0$, then $f$ will not have a real singularity. The condition for the existence of an improper integral $\displaystyle \int_{-\infty}^{\infty} f(z) dz$ of such a rational function is from $\displaystyle f(z) \sim {{1} \over {z^{p}}}$ to $p &amp;gt; 1$. Thinking in terms of the</description></item><item><title>Solving Harmonic Oscillator Problems using the Operator Method: Definition of Ladder Operators</title><link>https://freshrimpsushi.github.io/en/posts/362/</link><pubDate>Wed, 13 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/362/</guid><description>Build-up The ladder operators of the harmonic oscillator are represented by $\mathrm{Ladder\ Operator}$. The Hamiltonian, which is the energy operator, $H$ can also be substituted, and the characteristic functions of the ladder operators can be used to find the eigenfunctions from the ground state. Hints for defining new operators can be obtained by factorizing the classical Hamiltonian of the harmonic oscillator $H$. $$ \begin{align*} H &amp;amp;= \frac{1}{2m}p^{2}+\frac{1}{2m}mw^{2}x^{2} \\ &amp;amp;= \frac{1}{2m}</description></item><item><title>Energy Levels in an Infinite Potential Well</title><link>https://freshrimpsushi.github.io/en/posts/361/</link><pubDate>Tue, 12 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/361/</guid><description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 To determine the wave functions (eigenfunctions) and energies (eigenvalues) in an infinite potential well, refer to here. Now, let&amp;rsquo;s bring in the results and examine their significance. Eigenfunction $\displaystyle \psi_{(x)} =\sqrt{\frac{2}{a}}\sin \frac{n\pi}{a}x$ Eigenvalue $\displaystyle E_{n}=\frac{n^2\pi^2\hbar^2}{2ma^2}$ For the wave function in an infinite potential well, the expectation value</description></item><item><title>What is Degeneracy of Wave Functions in Quantum Mechanics?</title><link>https://freshrimpsushi.github.io/en/posts/307/</link><pubDate>Mon, 11 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/307/</guid><description>Definition In quantum mechanics, degeneracy refers to the condition where two different (i.e., linearly independent) wave functions have the same eigenvalue. Explanation In simpler terms, when two wave functions are degenerate, it means that their energies are equal. Mathematically, this implies that the geometric multiplicity of the eigenvalue is 2 or more. In Griffiths&amp;rsquo; textbook, this is referred to as overlapping or overlapped states. According to the translator, before the</description></item><item><title>There is no solution to the time-independent Schrödinger equation when the energy is less than the potential.</title><link>https://freshrimpsushi.github.io/en/posts/360/</link><pubDate>Wed, 06 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/360/</guid><description>영어 및 일본어 번역 정리 에너지가 퍼텐셜보다 작은 구간에서는 시간에 무관한 슈뢰딩거 방정식의 해가 존재하지 않는다. 즉, 파동함수가 존재하지 않는다.</description></item><item><title>Relationship between simultaneous eigenfunctions of angular momentum and ladder operators</title><link>https://freshrimpsushi.github.io/en/posts/348/</link><pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/348/</guid><description>Summary Let&amp;rsquo;s denote the angular momentum operators $L^{2}$ and $L_{z}$, and their eigenvalues as $\ell(\ell+1)\hbar^{2}$ and $m\hbar$. The normalized simultaneous eigenfunctions corresponding to each eigenvalue are referred to as $\ket{\ell, m}$. $$ \begin{align*} L^{2} \ket{\ell, m} &amp;amp;= \ell(\ell+1)\hbar^{2}\ket{\ell, m} \\ L_{z}\ket{\ell, m} &amp;amp;= m\hbar\ket{\ell, m} \end{align*} $$ For the ladder operators for angular momentum $L_{\pm}$ and the eigenfunctions $\ket{\ell, m}$, the following relational expression holds. $$ \begin{align*} L_{+}\ket{\ell, m} &amp;amp;=</description></item><item><title>The condition for the product of two Hermitian operators to be a Hermitian operator</title><link>https://freshrimpsushi.github.io/en/posts/347/</link><pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/347/</guid><description>Summary Let the two operators $A$, $B$ be Hermitian operators. If $A$, $B$ are commutative, then $AB$ is also a Hermitian operator이다. 역도 성립한다. 설명 역의 대우를 생각해보면 교환 가능하지 않은 두 연산자를 곱하면 에르미</description></item><item><title>Definite Integration through Trigonometric Substitution on the Complex Plane</title><link>https://freshrimpsushi.github.io/en/posts/333/</link><pubDate>Sun, 26 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/333/</guid><description>Theorem $$ \int_{0}^{2 \pi} f( \cos \theta , \sin \theta ) d \theta = \int_{\mathscr{C}} f(z) dz = 2 \pi i \sum \text{Res} f(z) $$ Description Integrating real functions that are difficult to integrate can often be more straightforward through complex analysis. Among these, let&amp;rsquo;s look into integration techniques for integrands made up of trigonometric functions. The basic strategy involves changing the integration range to $z(\theta) = e^{ i \theta}</description></item><item><title>Ladder Operators for Angular Momentum</title><link>https://freshrimpsushi.github.io/en/posts/344/</link><pubDate>Sun, 26 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/344/</guid><description>Definition The Angular Momentum Operator corresponding to $L_{z}$ Ladder Operators are defined as follows. $$ L_{+} := L_{x} + \i L_{y} \\ L_{-} := L_{x} - \i L_{y} $$ $L_{+}$ is called the raising operator, and $L_{-}$ is called the lowering operator. Explanation 1 2 The names of the operators, raising/lowering, are due to the fact that $L_{\pm}$ raises or lowers the state of the simultaneous eigenfunction of the angular</description></item><item><title>Simultaneous Eigenfunctions of Angular Momentum</title><link>https://freshrimpsushi.github.io/en/posts/343/</link><pubDate>Sun, 26 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/343/</guid><description>Summary Angular momentum operator $L^{2}$ and $L_{z}$&amp;rsquo;s normalized simultaneous eigenfunctions are denoted as $\ket{l, m}$. The eigenvalue equations are given below. $$ \begin{align*} L^{2} \ket{\ell, m} &amp;amp;= \ell(\ell+1)\hbar^{2}\ket{\ell, m} \\ L_{z}\ket{\ell, m} &amp;amp;= m\hbar\ket{\ell, m} \end{align*} $$ In this case, $\ell$ can only be integers or half-integers. For a given $\ell$, the minimum value of $m$ is $-\ell$, and the maximum value is $\ell$. $$ \begin{align*} \ell &amp;amp;= 0, \frac{1}{2},</description></item><item><title>Swapping Rows and Columns in a Data Frame in R</title><link>https://freshrimpsushi.github.io/en/posts/332/</link><pubDate>Sat, 25 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/332/</guid><description>Overview One of the strengths of R is that it makes some operations, which can be quite difficult for those familiar with other programming languages, easy to implement. For instance, it automatically expands memory when using arrays without having to allocate memory in advance, and operations like changing the value of a variable are very easy. Example Let&amp;rsquo;s swap the Sepal.Width column with the Species column in the Iris dataset.</description></item><item><title>How to import built-in datasets in R</title><link>https://freshrimpsushi.github.io/en/posts/331/</link><pubDate>Fri, 24 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/331/</guid><description>Overview R is a representative statistical programming language that not only provides useful methods but also offers data sets that are good as examples. Without such data sets, one would have to download and load new data every time when giving lectures. Guide The method to load a data set is very simple. All you need to do is assign the name of the data set you want to load</description></item><item><title>The Algebraic Multiplicity of Eigenvalues is Greater Than or Equal to Their Geometric Multiplicity</title><link>https://freshrimpsushi.github.io/en/posts/328/</link><pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/328/</guid><description>Theorem A matrix matrix $A \in \mathbb{C}^{ m \times m}$ having an eigenvalue $\lambda$ with an algebraic multiplicity $a$ and a geometric multiplicity $g$ implies $a \ge g$. Explanation The algebraic and geometric multiplicities of an eigenvalue are not guaranteed to be the same. If they were, they wouldn&amp;rsquo;t have been defined differently in the first place. However, one certain thing is that regardless of how small the algebraic multiplicity</description></item><item><title>Similar Matrices Have the Same Eigenvalues</title><link>https://freshrimpsushi.github.io/en/posts/329/</link><pubDate>Mon, 20 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/329/</guid><description>Theorem If two matrices $A,B$ are similar, they have the same eigenvalues. $$ \det (A - \lambda I) = \det (B - \lambda I) $$ In this case, $\lambda$ is an eigenvalue of $A, B$. Description Having the same eigenvalues means that the characteristic equations are the same. Proof To show that the eigenvalues are the same, it is sufficient to show that the characteristic equations are the same. $$</description></item><item><title>Algebraic and Geometric Multiplicities of Eigenvalues</title><link>https://freshrimpsushi.github.io/en/posts/311/</link><pubDate>Sun, 19 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/311/</guid><description>Algebraic Multiplicity For a matrix $A \in \mathbb{R}^{m \times m}$, the eigenvalue is defined as $\lambda$ that satisfies $\det (A - \lambda I ) =0$. The characteristic equation is a polynomial equation of degree $m$ with respect to $\lambda$, which can be expressed as $$ \det (A - \lambda I ) = (-1)^m \lambda ^m + c_{m-1} \lambda ^{m-1} + \cdots + c_{1} \lambda + c_{0} = 0 $$ According</description></item><item><title>For any arbitrary operator, always in the Hermitian form</title><link>https://freshrimpsushi.github.io/en/posts/327/</link><pubDate>Sun, 19 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/327/</guid><description>공식 임의의 연산자 $A$에 대해서 아래의 꼴은 항상 에르미트 연산자이다. $$ A + A^{\dagger} \tag{1} $$ $$ \i (A - A^{\dagger}) \tag{2} $$ $$ A A^{\dagger} \tag{3} $$ 증명 원래의 식에 켤레전치 $^{\dagge</description></item><item><title>Flow in Simple Extremes</title><link>https://freshrimpsushi.github.io/en/posts/323/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/323/</guid><description>Theorem 1 It can be said that function $f$ can be expressed as $\displaystyle f(z) = {{g(z)} \over {h(z)}}$. Here, $g$ and $h$ are analytic in $\alpha$, and if we say $g(\alpha) \ne 0 , h(\alpha) = 0, h ' (\alpha) \ne 0$ then $\alpha$ is a simple pole of $f$ $$ \text{Res}_{\alpha} f(z) = {{g(\alpha)} \over {h ' (\alpha)}} $$ It&amp;rsquo;s not just that $h$ in the form of</description></item><item><title>Residue at Poles</title><link>https://freshrimpsushi.github.io/en/posts/324/</link><pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/324/</guid><description>Theorem 1 Let $\alpha$ be represented as the pole of order $m$, that is, $\displaystyle f(z) = {{g(z)} \over { (z - \alpha)^m }}$ of the function $f: A \subset \mathbb{C} \to \mathbb{C}$. Here, if $g$ is analytic within $\alpha$ and we denote $g(\alpha) \ne 0$ as $$ \text{Res}_{\alpha} f(z) = {{g^{(m-1)} (\alpha)} \over {(m-1)!} } $$ Although we can transform the integration problem into a problem of finding residues</description></item><item><title>The Relationship Between the Powers of i and the Powers of e</title><link>https://freshrimpsushi.github.io/en/posts/325/</link><pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/325/</guid><description>Theorem Natural constant $e$ and imaginary number $i$ raised to a power satisfy the following relationship. $$ e^{i\frac{l \pi}{2}} = i^{l} $$ Proof Since $e^{ i \frac{l \pi}{2}}=\cos\frac{l \pi}{2}+i\sin \frac{l \pi}{2}$, when $l=0$, $$ e^{ 0}= 1 =i^{0} $$ When $l=1$, $$ e^{ i \frac{\pi}{2}}=\cos\frac{\pi}{2}+i\sin \frac{\pi}{2}=i=i^{1} $$ When $l=2$, $$ e^{ i \pi}=\cos \pi+i\sin \pi=-1=i^{2} $$ When $l=3$, $$ e^{ i \frac{3\pi}{2}}=\cos\frac{3\pi}{2}+i\sin \frac{3\pi}{2}=-i=i^{3} $$ As it repeats thereafter, $$ e^{</description></item><item><title>Calculating the Product, Inverse, and Transpose of a Matrix in R</title><link>https://freshrimpsushi.github.io/en/posts/317/</link><pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/317/</guid><description>Overview The strengths of R lie in the simple manipulation of various datasets, including matrices, and the provision of a wealth of free statistical packages. It&amp;rsquo;s obvious, but the computation of matrices in statistical analysis is very important, and R excellently fulfills these needs. Unless it&amp;rsquo;s MATLAB or Julia, in other languages, you&amp;rsquo;ll have to annoyingly define the operations on matrices separately. Code Matrix Multiplication For example, let&amp;rsquo;s say we</description></item><item><title>Finding Quotients and Remainders in R</title><link>https://freshrimpsushi.github.io/en/posts/316/</link><pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/316/</guid><description>Overview In the syntax of programming languages, what really lacks unity are the quotient and remainder operators. At first glance, they all seem to look alike, which contributes to the confusion. C uses / for quotient and % for remainder, while Python uses // for quotient and % for remainder, and there are plenty more examples of this confusion. One might wonder where the need to calculate quotients and remainders</description></item><item><title>Two operators with simultaneous eigenfunctions are commutative.</title><link>https://freshrimpsushi.github.io/en/posts/321/</link><pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/321/</guid><description>Summary If two different operators have the same eigenfunction, then the two operators are commutative. In other words, if the following equation holds, it means $[A, B] = 0$. $$ \begin{cases} A\psi=a\psi \\ B\psi=b\psi \end{cases} $$ In this case, $\psi$ is the normalized eigenfunction. Converse The converse of the above theorem also holds. That is, the commutativity of two operators and having a common eigenfunction is a necessary and sufficient</description></item><item><title>Remove All Variables and Clear Console in R</title><link>https://freshrimpsushi.github.io/en/posts/315/</link><pubDate>Mon, 13 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/315/</guid><description>Overview R is an interpreted language, so you end up working continuously looking at the console. In this process, for debugging or other purposes, various tests must be done in the same working environment. There can be instances where a certain variable created during testing is very important but the programmer fails to recognize it and it does not get included in the final version. Situations like, &amp;ldquo;It definitely worked</description></item><item><title>Two eigenvectors with different eigenvalues are orthogonal.</title><link>https://freshrimpsushi.github.io/en/posts/318/</link><pubDate>Sun, 12 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/318/</guid><description>Summary Any two eigenfunctions corresponding to distinct eigenvalues of an Hermitian operator $A$ are orthogonal to each other. $$ \begin{cases} A\psi_{n}=a_{n}\psi_{n} \\ A\psi_{m}=a_{m}\psi_{m} \end{cases} $$ If $a_{n} \ne a_{m}$, $$ \braket{\psi_{n} | \psi_{m}} = 0 $$ Proof Since eigenvalues of Hermitian operators are always real, the following holds: $$ \braket{A\psi_{n}|\psi_{m} } ={a_{n}}^{\ast}\braket{\psi_{n}|\psi_{m} } =a_{n}\braket{\psi_{n}|\psi_{m}} $$ Moreover, according to the definition of Hermitian operator, $$ \braket{A\psi_{n}|\psi_{m}}=\braket{\psi_{n}|A^{\dagger}\psi_{m}}=\braket{\psi_{n}|A\psi_{m}} = a_{m} \braket{\psi_{n} | \psi_{m}}</description></item><item><title>Using else if Statements in R: Solving the Error: unexpected else in else Issue</title><link>https://freshrimpsushi.github.io/en/posts/314/</link><pubDate>Sun, 12 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/314/</guid><description>Overview R does not have a branching statement like a switch statement, so you need to divide branches by connecting multiple if statements. Here, this conditional statement can be the same for if and else in every programming language, but it&amp;rsquo;s peculiar that only else if can be different. They might be written together as elseif or even abbreviated like elif, and R uses properly spaced else if. Even as</description></item><item><title>Commutative groups in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/309/</link><pubDate>Sat, 11 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/309/</guid><description>Definition 1 A group $\left&amp;lt; G, \ast\ \right&amp;gt;$ is defined to be an Abelian group if for any two elements $a, b$ in $a \ast\ b = b \ast\ a$, $\left&amp;lt; G, \ast\ \right&amp;gt;$ satisfies the commutative property. Explanation The term &amp;ldquo;commutative&amp;rdquo; implies that the commutative law is applicable. In English, instead of Commutative, the term Abelian is used, named after the genius mathematician Abel. It is perfectly fine to</description></item><item><title>The Meaning of Eigenvalue Equations in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/306/</link><pubDate>Sat, 11 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/306/</guid><description>Definition Matrix $n\times n$ Let us assume a matrix $A$ is given. We call $a$, which satisfies the following equation, the eigenvalue. A non-zero $n\times 1$ vector $\mathbf{x}$ corresponding to $a$ is called an eigen vector. $$ A \mathbf{x} = a \mathbf{x} \tag{1} $$ Operator Let us assume an operator $A$ is given. We call $a$, which satisfies the following equation, eigenvalue, () non-zero $\ket{a}$, the eigenfunction corresponding to $a$.</description></item><item><title>Proof of the Residue Theorem</title><link>https://freshrimpsushi.github.io/en/posts/308/</link><pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/308/</guid><description>Theorem 1 Let the analytic function $f: A \subset \mathbb{C} \to \mathbb{C}$ have a finite number of singularities $z_{1} , z_{2} , \cdots , z_{m}$ inside a simple closed path $\mathscr{C}$. Then, $$ \int_{\mathscr{C}} f(z) dz = 2 \pi i \sum_{k=1}^{m} \text{Res}_{z_{k}} f(z) $$ Explanation At first glance, the theorem might seem quite confusing. One has to calculate the integral, but instead of calculus-like computations, there&amp;rsquo;s talk about singularities and</description></item><item><title>Proof that the expectation eigenvalue of a Hermitian operator is always real</title><link>https://freshrimpsushi.github.io/en/posts/305/</link><pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/305/</guid><description>정리 에르미트 연산자의 기댓값은 항상 실수이다. 증명 $A$를 에르미트 연산자라고 하자. $A$의 기댓값은 $$ \braket{A \rangle = \int \psi^{\ast}A\psi dx = \langle \psi | A\psi} $$ 실수임을 보이려먼 $\braket{\psi |</description></item><item><title>Hermitian Operator</title><link>https://freshrimpsushi.github.io/en/posts/304/</link><pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/304/</guid><description>Definition An operator $A$ is called a Hermitian operator if it satisfies the following equation. $$ A = A^{\dagger} $$ Here, $A^{\dagger}$ is the conjugate transpose of $A$. Explanation $A^{\dagger}$ is read as [A dagger], and a dagger means a small knife. It is named after the French mathematician Hermite. In English, it is called a Hermitian operator. All operators in quantum mechanics are Hermitian operators. The notation for the</description></item><item><title>Dimension of the Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3018/</link><pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3018/</guid><description>Definition1 The number of elements (vectors) of a basis for a vector space $V$ is defined as the dimension of $V$ and is denoted as follows. $$ \dim (V) $$ Explanation Such a generalization of dimensions goes beyond merely exploring vector spaces and is being applied to various technologies that support this society. It might seem pointless to consider dimensions higher than the $3$ dimensions of our world and the</description></item><item><title>What is Dirac Notation?</title><link>https://freshrimpsushi.github.io/en/posts/303/</link><pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/303/</guid><description>Definition In quantum mechanics, wave functions are vectors and are fundamentally considered column vectors. A column vector is denoted using a right single-angle bracket and is referred to as a ket vector. $$ \psi = \ket{\psi} = \begin{pmatrix} \psi_{1} \\ \psi_{2} \\ \vdots \\ \psi_{n} \end{pmatrix} $$ The conjugate transpose matrix of $\ket{\psi}$ is denoted using a left single-angle bracket and is referred to as a bra vector. $$ \psi^{\ast}</description></item><item><title>The Principal Part of Laurent Series and Classification of Singularities</title><link>https://freshrimpsushi.github.io/en/posts/293/</link><pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/293/</guid><description>Overview 1 If we closely examine the principal part of the Laurent expansion, we can identify the type of singularities. Let&amp;rsquo;s say $\alpha$ is an isolated singularity of the function $f:A\subset \mathbb{C} \to \mathbb{C}$. For its Laurent expansion $$ f(z) = \sum_{n = 0 }^{\infty} a_{n} (z-\alpha) ^{n} + \sum_{n = 1 }^{\infty} { {b_{n} } \over{ (z-\alpha) ^{n} } } $$ the sequence $b_{n}$ has the following properties: Theorem</description></item><item><title>What is an operator in physics (quantum mechanics)</title><link>https://freshrimpsushi.github.io/en/posts/301/</link><pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/301/</guid><description>영어 번역결과</description></item><item><title>What is the Laurent Series?</title><link>https://freshrimpsushi.github.io/en/posts/290/</link><pubDate>Mon, 06 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/290/</guid><description>Buildup Taylor&amp;rsquo;s theorem generalizes the mean value theorem regarding the number of differentiations. It expands from dealing with something differentiated $1$ times to $n \in \mathbb{N}$ times. But if it was possible to generalize it to natural numbers, could it not be generalized to all integers? Of course, it&amp;rsquo;s not possible to differentiate $-n$ times, but what about considering integration, which is the inverse operation of differentiation? Here we introduce</description></item><item><title>Gradient, Divergence, Curl, and Laplacian in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/299/</link><pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/299/</guid><description>Explanation In physics, the four operations involving the del operator $\nabla$, Gradient, Divergence, Curl, Laplacian, are very important. Therefore, one must know the operations in three coordinate systems. Of course, this does not mean that you have to memorize them. Since physics study is not about memorizing formulas, they will naturally be memorized as you study, so do not try to memorize them intentionally but instead keep a printout of</description></item><item><title>Commutation Relations of the Angular Momentum Operator</title><link>https://freshrimpsushi.github.io/en/posts/298/</link><pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/298/</guid><description>Formula The commutation relation of the angular momentum operators is as follows. $$ \left[L_{j}, L_{k} \right] = \i \hbar \epsilon_{jk\ell}L_{\ell} \tag{1} $$ Here, $\epsilon_{jk\ell}$ is the Levi-Civita symbol. When written out in full, $$ \left[ L_{x}, L_{y} \right] = \i \hbar L_{z} \\ \left[ L_{y}, L_{z} \right] = \i \hbar L_{x} \\ \left[ L_{z}, L_{x} \right] = \i \hbar L_{y} $$ Additionally, $L^{2} = L_{x}^{2} + L_{y}^{2} + L_{z}^{2}$ commutes</description></item><item><title>Types of Singularities in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/281/</link><pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/281/</guid><description>Definitions Singularity 1 If the function $f$ is differentiable at all points of $\mathcal{N}(\alpha)$ in $\alpha$, it is said to be analytic at $\alpha$. If the function $f$ is not analytic in $\alpha \in \mathbb{C}$ but is analytic at some points of $\mathcal{N}(\alpha)$, $\alpha$ is called a Singular Point of $f$. If there exists $\mathcal{N}(\alpha)$ that is analytic at all points except for $\alpha$, then $\alpha$ is said to be</description></item><item><title>Properties of Commutators</title><link>https://freshrimpsushi.github.io/en/posts/297/</link><pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/297/</guid><description>Definition For two operators $A, B$, $AB - BA$ is defined as the commutator of $A, B$ and is denoted as follows. $$ [A,B]=AB-BA $$ Properties $$ \begin{align} [A, A] &amp;amp;= 0 \\[1em] [A, B] &amp;amp;= -[B, A] \\[1em] [A+B, C] &amp;amp;= [A, C] + [B, C] \\[1em] [AB, C] &amp;amp;= A[B, C]+[A, C]B \\[1em] [A,BC] &amp;amp;= B[A,C]+ [A,B]C \end{align} $$ Explanation The main method of describing quantum mechanics is</description></item><item><title>Linear Independence and Linear Dependence</title><link>https://freshrimpsushi.github.io/en/posts/253/</link><pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/253/</guid><description>Definition1 Let&amp;rsquo;s denote a non-empty subset of vector space $V$ as $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$. For constants $k_{1}, k_{2}, \dots, k_{r}$, the following equation $$ k_{1} \mathbf{v}_{1} + k_{2} \mathbf{v}_{2} + \dots + k_{r} \mathbf{v}_{r} = \mathbf{0} $$ has at least one solution $$ k_{1} = 0,\ k_{2} = 0,\ \dots,\ k_{r} = 0 $$ This is called a trivial solution. If the trivial solution is</description></item><item><title>Understanding Ranks and Nullity through Systems of Equations</title><link>https://freshrimpsushi.github.io/en/posts/279/</link><pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/279/</guid><description>Historical Background Historically, the invention of matrices was primarily aimed at simplifying and conveniently notating systems of linear equations. For instance, observing the system of linear equations $$ \begin{cases} 2x_{1} &amp;amp; + &amp;amp; x_{2} &amp;amp; + &amp;amp; x_{3} =&amp;amp; 0 \\ &amp;amp; x_{2} &amp;amp; =&amp;amp; 0 \end{cases} $$ reveals the inconvenience of having to use the same variables multiple times. Representing this as a matrix leads to $$ \begin{bmatrix} 2</description></item><item><title>Conversion of Cartesian Coordinate System Unit Vectors to Spherical Coordinate System Unit Vectors</title><link>https://freshrimpsushi.github.io/en/posts/291/</link><pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/291/</guid><description>Formulas The expression for converting the unit vectors from the Cartesian coordinate system to the spherical coordinate system is as follows. $$ \begin{align*} \hat{ \mathbf{x} }&amp;amp;= \cos \phi \sin \theta \hat{ \mathbf{r} } + \cos \phi \cos \theta \hat{ \boldsymbol{\theta} } - \sin\phi\hat{ \boldsymbol{\phi} } \\ \hat{ \mathbf{y} } &amp;amp;= \sin\phi\sin\theta \hat{ \mathbf{r} } + \sin\phi\cos\theta\hat{ \boldsymbol{\theta} } + \cos\phi\hat{ \boldsymbol{\phi} } \\ \hat{ \mathbf{z} } &amp;amp;= \cos\theta\hat{ \mathbf{r} }</description></item><item><title>In group Theory in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/278/</link><pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/278/</guid><description>Definition 1 For elements $a$ and the identity element $e$ of a monoid $\left&amp;lt; G, \ast\ \right&amp;gt;$, if there exists $a '$ satisfying $a \ast\ a ' = a ' \ast\ a = e$, then $\left&amp;lt; G, \ast\ \right&amp;gt;$ is defined as a group. That is, a group is a binary operation structure that satisfies the following properties: (i): The operation is associative. (ii): An identity element exists. (iii): An</description></item><item><title>Proof of the Law of Cosines</title><link>https://freshrimpsushi.github.io/en/posts/131/</link><pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/131/</guid><description>Theorem 1 For an element $a,b,c$ of a group $\left&amp;lt;G, \ast \right&amp;gt;$, $$ a \ast b = a \ast c \implies b = c \\ b \ast a = c \ast a \implies b=c $$ Explanation When one encounters abstract algebra, they begin to learn what they have previously known in a new language. The cancellation property is probably among the first theorems one encounters. Normally, we simply say that</description></item><item><title>Uniqueness Proof of Identity Elements and Inverse Elements in groups</title><link>https://freshrimpsushi.github.io/en/posts/130/</link><pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/130/</guid><description>Theorem 1 For a group $\left&amp;lt;G, \ast \right&amp;gt;$, the identity element $e$ that satisfies $e \ast x = x \ast e = x$ for all elements $x$ of $G$ is unique. For any element $a$ of $G$, the inverse element $a^{\prime}$ that satisfies $a \ast {a^{\prime}} = {a^{\prime}} \ast a = e$ is unique with respect to $a$. Explanation Though everyone takes it for granted, the definition of a group</description></item><item><title>Monoids in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/277/</link><pubDate>Tue, 31 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/277/</guid><description>Definition 1 A semigroup $\left&amp;lt; M , \ast\ \right&amp;gt;$ is defined to be a monoid if there exists an element $e$ such that for all elements $a$ of $\left&amp;lt; M , \ast\ \right&amp;gt;$, $a \ast\ e = e \ast\ a = a$ is satisfied. Such an $e$ is called an identity. Explanation A monoid is a semigroup with an identity element. Introducing the concept of an identity element considerably broadens</description></item><item><title>Finding the Wave Function Eigenfunctions and Energy Eigenvalues in an Infinite Potential Well</title><link>https://freshrimpsushi.github.io/en/posts/289/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/289/</guid><description>Proposition When the potential takes the form of an infinite well over the interval $[0, a]$, the energy (eigenvalue) $E_{n}$ and the wave function (eigenstate) $\psi_{n}$ of the wave function are as follows. $$ \begin{align*} E_{n} &amp;amp;=\frac{n^{2}\pi^{2}\hbar^{2}}{2ma^{2}} \\[1em] \psi_{n}{(x)} &amp;amp;= \textstyle \sqrt{\frac{2}{a}}\sin \left( \frac{n\pi}{a}x \right) \end{align*} \qquad\qquad n = 0, 1, 2, \dots \tag{0} $$ Explanation $$ V(x) = \begin{cases} \infty, &amp;amp; -\infty \lt x \lt 0 \\ 0, &amp;amp;</description></item><item><title>Linear Combination, Span</title><link>https://freshrimpsushi.github.io/en/posts/512/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/512/</guid><description>Definition: Linear Combination1 Let $\mathbf{w}$ be a vector in the vector space $V$. If $\mathbf{w}$ can be expressed as follows for vectors $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$ in $V$ and arbitrary constants $k_{1}, k_{2}, \cdots, k_{r}$, then $\mathbf{w}$ is called a linear combination of $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$. $$ \mathbf{w} = k_{1}\mathbf{v}_{1} + k_{2}\mathbf{v}_{2} + \cdots + k_{r}\mathbf{v}_{r} $$ Additionally, in this case, the constants $k_{1}, k_{2}, \cdots, k_{r}$ are referred to as the coefficients</description></item><item><title>Semigroup in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/276/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/276/</guid><description>Definition 1 Associative property Let $S$ be a set and $\ast : S \times S \to S$ be a binary operation. The following expression is called the associative property: $$ (a \ast b) \ast c = a \ast (b \ast c) \qquad \forall a, b, c \in S $$ Semigroup For an element $a,b,c$ of magma $\left&amp;lt; S, *\right&amp;gt;$, if $(a \ast\ b) \ast\ c = a \ast\ (b \ast\</description></item><item><title>Binary Operations in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/275/</link><pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/275/</guid><description>Buildup Mathematics can be broadly divided into three categories: geometry, analysis, and algebra. Among these, algebra was a branch of mathematics dealing with binomials, reduction, etc., in the curriculum. Algebra essentially aimed to solve any equation using letters instead of numbers. It sought after a general and powerful method of solution, not limited to specific numbers, thus could be considered cutting-edge technology of the time. However, these mathematical techniques have</description></item><item><title>Subspace of Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/285/</link><pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/285/</guid><description>Definition1 Let $W$ be a non-empty subset of the vector space $V$. If $W$ satisfies the definition of a vector space with respect to the addition and scalar multiplication defined in $V$, then $W$ is called a subspace of the vector space $V$, and is denoted as follows: $$ W \le V $$ Explanation To determine whether a subset $W$ of a vector space $V$ is a subspace of $V$,</description></item><item><title>Definition of Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/282/</link><pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/282/</guid><description>Definition1 When the elements of a non-empty set $V$ satisfy the following ten rules for two operations, addition and scalar multiplication, $V$ is called a vector space over field2 $\mathbb{F}$, and the elements of $V$ are called vectors. For $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k, l \in \mathbb{F}$, (A1) If $\mathbf{u}, \mathbf{v}$ is an element of $V$, then $\mathbf{u}+\mathbf{v}$ is also an element of $V$. (A2) $\mathbf{u} + \mathbf{v}</description></item><item><title>Characteristics of Special Relativity due to Lorentz Transformation: Loss of Simultaneity</title><link>https://freshrimpsushi.github.io/en/posts/260/</link><pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/260/</guid><description>Characteristics of Lorentz Transformation The transformation between two coordinate systems in special relativity is different from classical transformation due to the principle that &amp;ldquo;the speed of light is the same for all observers&amp;rdquo;. Derived with this condition in mind, we get the Lorentz transformation. The Lorentz transformation introduces three new phenomena that do not appear in classical physics. Loss of simultaneity Time dilation Length contraction Loss of Simultaneity Among the</description></item><item><title>Convex Functions, Concave Functions</title><link>https://freshrimpsushi.github.io/en/posts/262/</link><pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/262/</guid><description>Definition Given an interval $I \subset \mathbb{R}$, two elements $x_{1} , x_{2}$ and functions $f : I \to \mathbb{R}$ and $0 \le t \le 1$, When $f( t x_{1} + (1-t) x_{2}) \le t f(x_{1}) + (1-t) f(x_{2})$, $f$ is defined as a convex function on $I$. When $f( t x_{1} + (1-t) x_{2}) \ge t f(x_{1}) + (1-t) f(x_{2})$, $f$ is defined as a concave function on $I$. Explanation</description></item><item><title>Implications of Special Relativity due to Lorentz Transformation: Length Contraction</title><link>https://freshrimpsushi.github.io/en/posts/263/</link><pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/263/</guid><description>Characteristics of the Lorentz Transformation In the theory of Special Relativity, the transformation between two coordinate systems is different from classical transformations. This is due to the fact that &amp;rsquo;the speed of light is the same for all observers&amp;rsquo;. Considering this condition, the Lorentz transformation was derived. As a result of the Lorentz transformation, there are three new phenomena that do not appear in classical physics. Loss of simultaneity Time</description></item><item><title>Proof of the Integral Form of Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/265/</link><pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/265/</guid><description>Theorem Given a convex function $ \phi : [a,b] \to \mathbb{R}$ and $f: [0,1] \to [a,b]$, if $\phi \circ f$ is integrable over $[0,1]$, then $$ \phi \left( \int_{0}^{1} f(x) dx \right) \le \int_{0}^{1} (\phi \circ f ) (x) dx $$ Explanation Of course, given the conditions, the integration interval can also be changed through substitution, etc. Unlike finite form, which generalizes the number of terms using definitions, integration form</description></item><item><title>Cross Product in Three-Dimensional Euclidean Space</title><link>https://freshrimpsushi.github.io/en/posts/256/</link><pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/256/</guid><description>Definition The cross product of $\mathbf{x}$ and $\mathbf{y}$ is defined in terms of $\mathbf{x}, \mathbf{y} \in \mathbb{R}^3$. $$ \begin{align*} \mathbf{x} \times \mathbf{y} =&amp;amp; (x_{2}y_{3} - x_{3}y_{2}, x_{3}y_{1} - x_{1}y_{3}, x_{1}y_{2} - x_{2}y_{1}) \\ =&amp;amp; \det \begin{bmatrix} \mathbf{i} &amp;amp; \mathbf{j} &amp;amp; \mathbf{k} \\ x_{1} &amp;amp; x_{2} &amp;amp; x_{3} \\ y_{1} &amp;amp; y_{2} &amp;amp; y_{3} \end{bmatrix} \\ =&amp;amp; \begin{bmatrix} 0 &amp;amp; -x_{3} &amp;amp; x_{2} \\ x_{3} &amp;amp; 0 &amp;amp; -x_{1} \\ -x_{2}</description></item><item><title>Inner product in Euclidean space</title><link>https://freshrimpsushi.github.io/en/posts/255/</link><pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/255/</guid><description>Definition Let&amp;rsquo;s say $V = \mathbb{R}^n$ for a vector space, and also $\mathbf{x}, \mathbf{y}, \mathbf{z} \in V$ and $k \in \mathbb{R}$. $\left&amp;lt; \cdot , \cdot \right&amp;gt; : V^2 \to \mathbb{R}$ is defined as the inner product on $V$ when it satisfies the following four conditions: (1) Symmetry: $\left&amp;lt; \mathbf{x} , \mathbf{y} \right&amp;gt; = \left&amp;lt; \mathbf{y}, \mathbf{x} \right&amp;gt;$ (2) Additivity: $\left&amp;lt; \mathbf{x} + \mathbf{y} , \mathbf{z} \right&amp;gt; = \left&amp;lt; \mathbf{x}, \mathbf{z}</description></item><item><title>Matrix Rank, Nullity</title><link>https://freshrimpsushi.github.io/en/posts/3021/</link><pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3021/</guid><description>Theorem1 The dimensions of the row space and column space of matrix $A$ are the same. Proof Let $R$ be the row echelon form matrix of $A$. Since basic row operations do not change the dimensions of the row space and column space of $A$, the following equation holds: $$ \begin{align*} \dim \big( \mathcal{R}(A) \big) &amp;amp;= \dim \big( \mathcal{R}(R) \big) \\ \dim \big( \mathcal{C}(A) \big) &amp;amp;= \dim \big( \mathcal{C}(R) \big)</description></item><item><title>Row Space, Column Space, Null Space</title><link>https://freshrimpsushi.github.io/en/posts/254/</link><pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/254/</guid><description>Definition1 $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} $$ For a matrix $A$, the $m$ number of $\mathbb{R}^{n}$ vectors made from the rows of $A$ $$ \begin{align*} \mathbf{r}_{1} =&amp;amp; \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \end{bmatrix} \\ \mathbf{r}_{2} =&amp;amp;</description></item><item><title>Lorentz Transformation Derivation</title><link>https://freshrimpsushi.github.io/en/posts/251/</link><pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/251/</guid><description>Derivation The text might be a bit long, but it&amp;rsquo;s written very simply, so don&amp;rsquo;t be afraid to dive in. Let&amp;rsquo;s think about light (photon) moving in the plane of $xy$ inertial system (coordinate system) when $t=0$. It starts from the origin and is advancing at an angle of $\theta$ with the $x$ axis. The new transformation that will replace the Galilean transformation can be said to look like the</description></item><item><title>Relativity Theory and Lorentz Transformation</title><link>https://freshrimpsushi.github.io/en/posts/249/</link><pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/249/</guid><description>Buildup Relativity theory starts from the completion of electromagnetism. To say electromagnetism is complete means Maxwell finished the four partial differential equations for the electric field $\mathbf{E}$ and the magnetic field $\mathbf{B}$. From Maxwell&amp;rsquo;s equations, we learn that the speed of electromagnetic waves is equal to the speed of light. This leads us to the following two facts: Light is an electromagnetic wave. The speed of light is $\dfrac{1}{\sqrt{\epsilon_o \mu_o}}=300,000</description></item><item><title>World Line and Galilean Transformation</title><link>https://freshrimpsushi.github.io/en/posts/250/</link><pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/250/</guid><description>Definition The line that represents the track of a particle in space and time is called a world line. Description Let&amp;rsquo;s think only about a coordinate system moving at a constant speed in one direction. In the $A$ coordinate system, there is a particle at rest at the origin. The world line of this particle is as follows. And there is a $A^{\prime}$ coordinate system moving at a speed of</description></item><item><title>Conjugate Complex Number</title><link>https://freshrimpsushi.github.io/en/posts/245/</link><pubDate>Sat, 23 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/245/</guid><description>Definition Let&amp;rsquo;s define a complex number as $z$ with $z=a+ib(a,b\in \mathbb{R})$. $\overline{z}$ is defined as follows and is called the Complex Conjugate of $z$. $$ \overline{z}:=\overline{a+ib}=a-ib $$ Explanation It can be explained as substituting $-i$ for $i$ in the original complex number, and as a reflection across the real axis on the complex plane. The term conjugate seems to be named because it forms a pair that produces a real</description></item><item><title>The Divergence of Curl is Always Zero</title><link>https://freshrimpsushi.github.io/en/posts/244/</link><pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/244/</guid><description>Formula The divergence of the curl of a vector function $\mathbf{A}$ is always $0$. $$ \nabla \cdot (\nabla \times \mathbf{A}) = 0 $$ Proof The curl of $\mathbf{A}$ is as follows. $$ \begin{align*} \nabla \times \mathbf{A} &amp;amp;= \begin{vmatrix} \hat{\mathbf{x}} &amp;amp; \hat{\mathbf{y}} &amp;amp; \hat{\mathbf{z}} \\ \displaystyle \frac{\partial}{\partial x} &amp;amp; \displaystyle \frac{\partial}{\partial y} &amp;amp; \displaystyle \frac{\partial}{\partial z} \\ A_{x} &amp;amp; A_{y} &amp;amp; A_{z} \end{vmatrix} \\ &amp;amp;= \hat{\mathbf{x}} \left( \frac{\partial A_{z}}{\partial y} -</description></item><item><title>Sphere's Moment of Inertia</title><link>https://freshrimpsushi.github.io/en/posts/240/</link><pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/240/</guid><description>Formula The moment of inertia of a sphere with radius $a$ and mass $m$ is as follows. $$ I=\frac{2}{5}ma^{2} $$ Proof The idea of finding the moment of inertia of a sphere is slightly different from other rigid bodies. The key idea is to think of the sphere as a sum of infinitely many discs, similar to the method of integration. Adding up the moment of inertia of these infinitely</description></item><item><title>Inertia Moments of Disks and Cylinders</title><link>https://freshrimpsushi.github.io/en/posts/239/</link><pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/239/</guid><description>Formula A disk with radius $a$ and mass $m$ has a moment of inertia perpendicular to the disk as $I=\dfrac{1}{2}ma^2$. parallel to the disk as $I=\dfrac{1}{4}ma^2$. Derivation When the axis of rotation passes through the center of the disk and is perpendicular to the disk Let $\rho$ be the mass per unit area. Then, the mass of the disk is $m=\rho \pi r^2$. Therefore, it follows that $$ dm=\rho \pi</description></item><item><title>Moment of Inertia of a Ring and Cylindrical Shell</title><link>https://freshrimpsushi.github.io/en/posts/238/</link><pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/238/</guid><description>Formulas The moment of inertia of a ring with radius $a$ and mass $m$, when the axis of rotation passes through the center of the ring, is: Perpendicular to the plane formed by the ring, it is $I=ma^{2}$. Parallel to the plane formed by the ring, it is $I=\dfrac{1}{2}ma^{2}$. Derivation Consider a thin, uniform circular ring (or cylindrical shell) with radius $a$ and mass $m$. There are cases where the</description></item><item><title>Parallel Axis Theorem</title><link>https://freshrimpsushi.github.io/en/posts/237/</link><pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/237/</guid><description>Parallel Axis Theorem The moment of inertia of a rigid body about any axis of rotation is equal to the sum of the moment of inertia about an axis parallel to it and passing through the center of mass and the product of the body&amp;rsquo;s mass and the square of the distance between the two axes. $$ \color{red}I=\color{blue}{I_{cm}}+\color{green}{md^{2}} $$ Proof Arbitrarily set the coordinate axis and let the moment of</description></item><item><title>Vertical Axis Theorem</title><link>https://freshrimpsushi.github.io/en/posts/236/</link><pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/236/</guid><description>Vertical Axis Theorem The moment of inertia about an axis perpendicular to a plane is equal to the sum of the moments of inertia about any two perpendicular axes lying in the plane and passing through the perpendicular axis. $$ \color{red}{I_{z}}=\color{blue}{I_{x}+I_{y}} $$ Proof $$ I_{z}=\sum\limits_{i} m_{i}{r_{i}}^{2} $$ By Pythagoras&amp;rsquo; theorem, since ${r_{i}}^{2}={x_{i}}^{2}+{y_{i}}^{2}$, substituting this into the above equation gives: $$ I_{z}=\sum\limits_{i} m_{i}({x_{i}}^{2}+{y_{i}}^{2})=\sum\limits_{i} m_{i}{x_{i}}^{2}+\sum\limits_{i} m_{i}{y_{i}}^{2} $$ $x$ is the distance from</description></item><item><title>Inertia Moment and Turning Radius</title><link>https://freshrimpsushi.github.io/en/posts/234/</link><pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/234/</guid><description>Moment of Inertia $$ \begin{align*} I &amp;amp;= \sum_{i} m_{i} {r_{i}}^2 \\ I &amp;amp;= \int r^2 dm \end{align*} $$ The moment of inertia is defined as the (mass of a particle)$\times$(distance from the rotation axis to the particle) and represents the physical quantity that indicates the characteristic of a body to continue rotating. Its symbol is $I$, which seems to be derived from the initial letter of the English word Inertia.</description></item><item><title>Inertia Moment of a Thin Rod</title><link>https://freshrimpsushi.github.io/en/posts/235/</link><pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/235/</guid><description>Formulas The moment of inertia for a rod with length $a$ and mass $m$ is: If the axis of rotation is at the end of the rod, it is $I=\dfrac{1}{3}ma^{2}$. If the axis of rotation is at the center of the rod, it is $I=\dfrac{1}{12}ma^{2}$. Derivation When the Axis of Rotation is at the End of the Rod If $\rho$ is defined as the mass per unit length, the mass</description></item><item><title>What is a Generating Function?</title><link>https://freshrimpsushi.github.io/en/posts/232/</link><pubDate>Wed, 30 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/232/</guid><description>Definition A function $g$, represented in the form such as $$ g(x) =\sum \limits _{n=0}^{\infty}a_{n}x^{n}= a_{0} + a_{1} x + a_{2} x^2 + \cdots $$ for a given sequence $\left\{ a_{n} \right\}$, is called the generating function of the sequence $\left\{ a_{n}\right\}$ or simply generating function. When the sequence is $a_{n}=a_{n}(x)$, it is also denoted as follows: $$ G(x,t)=\sum \limits _{n=0}^{\infty}a_{n}(x)t^{n} $$ Explanation As sharp readers may have noticed, the</description></item><item><title>Derivation of the Taylor Series Using Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/231/</link><pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/231/</guid><description>Theorem 1 If the function $f: A \subseteq \mathbb{C} \to \mathbb{C}$ is analytic in a circle $|z - \alpha| &amp;lt; r$, $$ f(z) = \sum_{n = 0} ^{\infty} {{f^{(n)} (\alpha)} \over {n!}} (z - \alpha)^n $$ Explanation One of the joys of mathematics is generalization. The Taylor&amp;rsquo;s Theorem is considered a generalization of the Mean Value Theorem, and now we extend from real numbers to complex numbers. Interestingly, despite the</description></item><item><title>Weierstrass M-test</title><link>https://freshrimpsushi.github.io/en/posts/230/</link><pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/230/</guid><description>Theorem 1 For functions $f_{n}$ and $z \in A$, if there exists a sequence of positive numbers $M_{n}$ that satisfies $|f_{n}(z)| \le M_{n}$ and $\displaystyle \sum_{n=1}^{\infty} M_{n}$ converges, then $\displaystyle \sum_{n=1}^{\infty} f_{n}$ absolutely converges and uniformly converges in $A$. Description The M-test is named after the sequence $M_{n}$. It is a useful theorem that allows us to show both absolute and uniform convergence at once if we can bring in</description></item><item><title>Proof of Roché's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/229/</link><pubDate>Wed, 16 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/229/</guid><description>Theorem 1 If $f$ and $g$ are analytic within a simple closed path $\mathscr{C}$ and its interior and satisfy $|g(z)| &amp;lt; |f(z)|$ on $\mathscr{C}$, then $f$ and $f + g$ have the same number of zeros in the interior of $\mathscr{C}$. Description This theorem involves considering the given function as $h = f + g$ and smartly dividing it into $f$ and $g$. Especially for polynomial functions, such manipulation is</description></item><item><title>Zeros and Poles of Meromorphic Functions</title><link>https://freshrimpsushi.github.io/en/posts/228/</link><pubDate>Wed, 16 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/228/</guid><description>Theorem 1 In a simple closed path $\mathscr{C}$, let an analytic function $f$ have $Z$ zeros and $P$ poles inside $\mathscr{C}$, and on $\mathscr{C}$, let it be $f(z) \ne 0$. Then, $$ {{1} \over {2 \pi i }} \int_{\mathscr{C}} {{f ' (z)} \over {f(z)}} dz = Z - P $$ $Z$ and $P$ are the sums including multiplicities. Explanation Analytic Number Theory If the function $f : \mathbb{C} \to \mathbb{C}$</description></item><item><title>Poisson Integral Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/226/</link><pubDate>Tue, 15 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/226/</guid><description>Formulas 1 Let&amp;rsquo;s assume the function $f : \mathbb{C} \to \mathbb{C}$ is analytic in a simply connected region that contains the circle $\mathscr{C}: |z| = r$. Then, for $0 &amp;lt; \rho &amp;lt; r$, we have $$ f( \rho e ^{i \phi} ) = {{1} \over { 2 \pi }} \int_{0}^{2 \pi} {{r^2 - \rho^2 } \over {r^2 - 2 r \rho \cos (\theta - \phi) + \rho ^2 }} f(r</description></item><item><title>Schwarz Lemma Proof</title><link>https://freshrimpsushi.github.io/en/posts/227/</link><pubDate>Tue, 15 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/227/</guid><description>Theorem 1 In the unit circle $|z| \le 1$, for an analytic function $f$, assume that $f(0) = 0$ and $0 &amp;lt; |z| &amp;lt; 1$ where $|f(z)| \le 1$. Then, from $0 &amp;lt; |z| &amp;lt; 1$, $$ |f ' (0)| \le 1 \\ |f(z)| \le |z| $$ Proof Without loss of generality, it can be expanded to $|z| \le r$ for convenience of the proof, but the unit circle is</description></item><item><title>Proof of Gauss's Mean Value Theorem</title><link>https://freshrimpsushi.github.io/en/posts/224/</link><pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/224/</guid><description>Theorem Let&amp;rsquo;s say the function $f$ is analytic on a closed circle $| z - z_{0} | \le r$. Then, $$ f(z_{0}) = {{1} \over {2 \pi}} \int_{0}^{2 \pi} f(z_{0} + r e ^{i \theta } ) d \theta $$ Description Just as the Mean Value Theorem for Derivatives evolved through generalizations and gave rise to various theorems named after mathematicians, the Mean Value Theorem for Integrals also has a</description></item><item><title>Proof of the Maximum Absolute Value Theorem</title><link>https://freshrimpsushi.github.io/en/posts/225/</link><pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/225/</guid><description>Theorem 1 Let us say the function $f$ is continuous on the simple closed path $\mathscr{C}$, analytic in the interior, and is not a constant function at any point. Then, the point $z = z_{0}$ that maximizes $|f(z)|$ on $\mathscr{C}$ exists on $\mathscr{C}$. Description Simply put, in complex analysis, the maximum value of $|f|$ within a closed path exists on its boundary. At this point, it becomes a level of</description></item><item><title>Proof of Liouville's Theorem in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/222/</link><pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/222/</guid><description>Theorem 1 If the function f $f : \mathbb{C} \to \mathbb{C}$ is an entire function and there exists a positive number $M$ for all $z \in \mathbb{C}$ such that $|f(z)| \le M$ is satisfied, then $f$ is a constant function. Explanation Saying that $f$ is an entire function means it is analytic throughout the entire complex plane. The contrapositive statement is that if it&amp;rsquo;s not a constant function, then its</description></item><item><title>Proof of the Fundamental Theorem of Algebra</title><link>https://freshrimpsushi.github.io/en/posts/223/</link><pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/223/</guid><description>Theorem 1 $n$th degree polynomial $P(x) = a_{0} + a_{1} x + a_{2} x^2 + \cdots + a_{n} x^{n}$ has exactly $n$ roots, including multiple roots. Explanation In fact, when we solve a polynomial, we usually assume that there exists a solution, but there&amp;rsquo;s no guarantee that this is always the case. For example, the quadratic polynomial $x^2+1 = 0$ does not have real roots. However, if complex numbers are</description></item><item><title>Fresnel Sine Integral's Maclaurin Series Expansion</title><link>https://freshrimpsushi.github.io/en/posts/220/</link><pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/220/</guid><description>Formula $$ S(x) = \sqrt{{2} \over {\pi}} \int_{0}^{x} \sin (w^2) dw = \sqrt{{2} \over {\pi}} \sum_{n=0}^{\infty} {{(-1)^{n}} \over {(2n+1)! (4n+3)}} x^{4n+3} $$ Description Fresnel was a physicist who studied optics, and his name is attached to most results involving trigonometric functions. Likely, trigonometric functions are deeply related to wave functions, which explains why he felt the need to vigorously study them, even if it meant creating formulas where none existed.</description></item><item><title>Proof of Fresnel Integrals</title><link>https://freshrimpsushi.github.io/en/posts/221/</link><pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/221/</guid><description>Theorem 1 $$ \int_{0}^{\infty} \cos x^2 dx = \int_{0}^{\infty} \sin x^2 dx = {{1}\over{2}} \sqrt{{\pi}\over{2}} $$ Description Fresnel integrals may appear simple at first glance, but the outcome is not as straightforward as it appears. It would be easier if it was merely the square of trigonometric functions, but it is due to the squaring of $x$ contained within them. Once you actually delve into it, you can appreciate how</description></item><item><title>Binomial Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/218/</link><pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/218/</guid><description>Definition A combination is a subset of a finite set. The number of subsets with cardinality $k$ from a set with cardinality $n$ is denoted by $\binom{n}{k}$ or $_{n}C_{k}$, and is called the binomial coefficient. $$ \binom{n}{k} = _{n}C_{k} = \frac{ n! }{ k! (n-k)! } $$ Theorem Binomial Theorem $$ (x+y)^{n} = \sum_{k=0}^{n} \binom{n}{k} x^{k} y^{n-k} $$ Pascal&amp;rsquo;s Identity $$ \binom{n}{k} + \binom{n}{k+1} = \binom{n+1}{k+1} $$ Binomial Coefficient Sum</description></item><item><title>Definite Integration of the form e^-x^2, Gaussian Integral, Euler-Poisson Integral</title><link>https://freshrimpsushi.github.io/en/posts/219/</link><pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/219/</guid><description>Theorem The Gaussian function $f(x) := e^{-x^2}$&amp;rsquo;s integral over the entire domain is as follows. $$ \int_{-\infty}^{\infty} e^{-x^2} dx= \sqrt{\pi} $$ Description Physicist Kelvin is said to have left the remark that &amp;ldquo;one who finds this integral obvious is a mathematician&amp;rdquo;. It is also known by other names such as Gaussian integral, or Euler-Poisson integral. It&amp;rsquo;s a shocking integration for high school students and especially crucial for statistics. That&amp;rsquo;s because,</description></item><item><title>Cauchy's Integral Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/215/</link><pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/215/</guid><description>Theorem 1 Let the complex function $f: A \subseteq \mathbb{C} \to \mathbb{C}$ be analytic in a simply connected region $\mathscr{R}$. If a simple closed path $\mathscr{C} \subset \mathscr{R}$ contained in $\mathscr{R}$ surrounds a point $\alpha$, then the following holds: $$ f(\alpha) = {{1} \over {2 \pi i }} \int_{\mathscr{C}} {{f(z)} \over { z - \alpha }} dz $$ Derivation First, let&amp;rsquo;s show that $\displaystyle 2 \pi i = \int_{\mathscr{C} '}</description></item><item><title>Proof of Morera's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/214/</link><pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/214/</guid><description>Theorem 1 A complex function $f : \mathbb{C} \to \mathbb{C}$ is analytic in a simply connected domain $\mathscr{R}$ if it is continuous in $\mathscr{R}$ and satisfies $\displaystyle \int_{\mathscr{C}} f(z) dz = 0$ for all closed paths $\mathscr{C} \subset \mathscr{R}$ contained in $\mathscr{R}$. Explanation This can be thought of as roughly the converse of Cauchy&amp;rsquo;s theorem. Interestingly, it&amp;rsquo;s common knowledge in analysis that &amp;lsquo;if it&amp;rsquo;s differentiable, then it&amp;rsquo;s continuous, and if</description></item><item><title>Mean Value Theorem for Integrals</title><link>https://freshrimpsushi.github.io/en/posts/212/</link><pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/212/</guid><description>Theorem If a function $f$ is continuous on a closed interval $[a,b]$, there exists at least one $c$ in $(a,b)$ that satisfies $\displaystyle f(c) = {{1}\over {b-a} } \int_{a}^{b} f(x) dx$. Description Similar to the Mean Value Theorem but as it is used for integration, it is named as such. The usage is very similar, and its utility is by no means inferior to the Mean Value Theorem. On the</description></item><item><title>Proof of the Fundamental Theorem of Calculus</title><link>https://freshrimpsushi.github.io/en/posts/213/</link><pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/213/</guid><description>Theorem1 Assume that the function $f$ is continuous on the closed interval $[a,b]$. (1) The function $\displaystyle F(x) = \int_{a}^{x} f(t) dt$ is continuous on $[a,b]$, differentiable on $(a,b)$, and satisfies $\displaystyle {{dF(x)} \over {dx}} = f(x)$. (2) For any antiderivative $F$ of $f$, $\displaystyle \int_{a}^{b} f(x) dx = F(b) - F(a)$ Explanation Of course, we use the words differentiation and integration so we can easily guess the relationship between</description></item><item><title>Contraction Lemma for Complex Path Integrals</title><link>https://freshrimpsushi.github.io/en/posts/211/</link><pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/211/</guid><description>Theorem 1 Let&amp;rsquo;s say in a simply connected domain containing a simple closed path $\mathscr{C}$, $f: A \subseteq \mathbb{C} \to \mathbb{C}$ is analytic at all points excluding point $\alpha$ inside $\mathscr{C}$. Then, for a closed curve $\mathscr{C} '$ centered at $\alpha$ inside $\mathscr{C}$, $$ \int_{\mathscr{C}} f(z) dz = \int_{\mathscr{C} '} f(z) dz $$ Explanation It&amp;rsquo;s a long way to say, but essentially, it means that when doing complex integration over</description></item><item><title>Proof of Cauchy's Theorem in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/210/</link><pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/210/</guid><description>Theorem 1 Let&amp;rsquo;s assume that $\mathscr{C}$ is a simple closed path and $f: A \subseteq \mathbb{C} \to \mathbb{C}$ is analytic in its interior and $f '$ is continuous. Then, $$ \int_{\mathscr{C}} f(z) dz = 0 $$ Proof For $a \le t \le b$, $$ z(t) = x(t) + i y(t) \\ f(z) = u(x,y) + i v(x,y) $$ then since $\displaystyle {{dz} \over {dt}} = x ' + i y</description></item><item><title>Curl of the Curl of Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/209/</link><pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/209/</guid><description>Formulas The curl of the curl of a vector function is as follows. $$ \nabla \times (\nabla \times \mathbf{A}) = \nabla (\nabla \cdot \mathbf{A}) - \nabla^2 \mathbf{A} $$ Explanation The first term, $\nabla(\nabla \cdot \mathbf{A})$, is the divergence of the gradient, which doesn&amp;rsquo;t have a specific name. The second term is important enough to have a name. $\nabla \cdot \nabla$ is called the Laplacian, specifically, the Laplacian of a vector</description></item><item><title>Finding the Speed of Electromagnetic Light from Maxwell's Equations</title><link>https://freshrimpsushi.github.io/en/posts/207/</link><pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/207/</guid><description>Formulas Vacuum Maxwell&amp;rsquo;s Equations $$ \begin{align} \nabla \cdot \mathbf{E} &amp;amp;= 0 \\[1em] \nabla \cdot \mathbf{B} &amp;amp;= 0 \\[1em] \nabla \times \mathbf{E} &amp;amp;= -\frac{\partial \mathbf{B}}{\partial t} \\[1em] \nabla \times \mathbf{B} &amp;amp;= \mu_{0}\epsilon_{0}\frac{\partial \mathbf{E}}{\partial t} \end{align} $$ One-dimensional wave equation $$ \frac{\partial^2 f}{\partial x^2}=\frac{1}{v^2}\frac{\partial^2 f}{\partial t^2} $$ Three-dimensional wave equation $$ \nabla ^2 f = \frac{1}{v^2}\frac{\partial ^2 f}{\partial t^2} $$ Derivation The goal is to derive a wave equation form from Maxwell&amp;rsquo;s</description></item><item><title>Summation Summary of Random Variables Following a Specific Distribution</title><link>https://freshrimpsushi.github.io/en/posts/202/</link><pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/202/</guid><description>Theorem Let&amp;rsquo;s say the random variables $X_{1} , \cdots , X_{n}$ are mutually independent. [1] Binomial distribution: If $X_i \sim \text{Bin} ( n_{i}, p)$, then $$ \sum_{i=1}^{m} X_{i} \sim \text{Bin} \left( \sum_{i=1}^{m} n_{i} , p \right) $$ [2] Poisson distribution: If $X_i \sim \text{Poi}( m_{i} )$, then $$ \sum_{i=1}^{n} X_{i} \sim \text{Poi} \left( \sum_{i=1}^{n} m_{i} \right) $$ [3] Gamma distribution: If $X_i \sim \Gamma ( k_{i}, \theta)$, then $$ \sum_{i=1}^{n}</description></item><item><title>The Curl of a Gradient is Always Zero</title><link>https://freshrimpsushi.github.io/en/posts/208/</link><pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/208/</guid><description>Formulas The curl of the gradient of a scalar function eq1 is always eq2. eq1 Proof In Cartesian coordinates, the gradient of eq1 is as follows. eq2 When we compute the curl of eq4, it is as follows. eq3 Since the result is eq6 for all components, regardless of eq1, the rotation of the gradient is always eq2. ■</description></item><item><title>Euclidean Space</title><link>https://freshrimpsushi.github.io/en/posts/205/</link><pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/205/</guid><description>Definition For a natural number $n \in \mathbb{N}$, the Cartesian product $\mathbb{R}$ of the set of real numbers is called the Euclidean space. $$ \mathbb{R}^{n} = \mathbb{R} \times \cdots \times \mathbb{R} $$ $\mathbb{R}^{1}$ is referred to as real space or number line. $\mathbb{R}^{2}$ is called a plane. $\mathbb{R}^{3}$ is called a $3$-dimensional space. Here, $\mathbb{N} := \left\{ 1, 2, 3, \cdots \right\}$ means the set that includes all natural numbers.</description></item><item><title>The Importance of the Relative Phase of the Wave Function</title><link>https://freshrimpsushi.github.io/en/posts/201/</link><pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/201/</guid><description>설명 The wave function is often expressed as a complex exponential function as follows. $$ \psi = R e^{\i\theta} $$ At this time, in the equation, what has physical significance is not $\psi$, but $\left| \psi \right|^{2} = R^{2}$, so the value of the phase $\theta$ is not important and can be treated interchangeably. However, the story is different when the wave function is represented as the sum of</description></item><item><title>Commutator of Momentum and Position</title><link>https://freshrimpsushi.github.io/en/posts/200/</link><pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/200/</guid><description>Formula The commutator of position and momentum operator is given by the equation below. $$ \begin{align} [p, x] &amp;amp;= -\i \hbar \\ [x, p] &amp;amp;= \i \hbar \end{align} $$ This equation is termed the canonical commutation relation. The commutator of the square of position and momentum is as follows. $$ \begin{align} [x^{2}, p] &amp;amp;= 2 \i \hbar x \\ [p, x^{2}] &amp;amp;= -2 \i \hbar x \end{align} $$ Explanation Since</description></item><item><title>Prove that the expectation value of momentum is always a real number.</title><link>https://freshrimpsushi.github.io/en/posts/199/</link><pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/199/</guid><description>Summary The expectation value of the momentum operator $\langle p \rangle$ is always a real number. Explanation In fact, not only the momentum operator but all Hermitian operators have eigenvalues that are always real. Proof The expectation value of momentum is as follows. $$ \displaystyle \langle p \rangle = \int \psi^{\ast} \left( \frac{\hbar}{i}\frac{\partial}{\partial x} \right) \psi dx $$ Additionally, the complex conjugate of the expectation value of momentum is as</description></item><item><title>Proof that the Partial Sums of a Geometric Sequence are Also Geometric</title><link>https://freshrimpsushi.github.io/en/posts/194/</link><pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/194/</guid><description>Theorem The geometric sequence $a_n = a r^{n-1}$, its partial sum $\displaystyle S_n = \sum_{k=1}^{n} a_k$, and some natural number $m$, such that $A_n = S_{mn} - S_{m(n-1)}$ is a geometric sequence. Explanation It&amp;rsquo;s really difficult if you don&amp;rsquo;t know. For example, consider the sequence obtained by summing sets of three powers of 2, $(1 + 2+ 4)= 7 $, $(8 + 16 + 32)=56$, $(64+128+256)=448 \cdots$ is a geometric</description></item><item><title>Proof that the Partial Sums of an Arithmetic Sequence Also Form an Arithmetic Sequence</title><link>https://freshrimpsushi.github.io/en/posts/193/</link><pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/193/</guid><description>Theorem An arithmetic sequence $a_n = a + (n-1)d$, its partial sum $\displaystyle S_n = \sum_{k=1}^{n} a_k $, and a certain natural number $m$ for $A_n = S_{mn} - S_{m(n-1)} $ form an arithmetic sequence. Explanation It&amp;rsquo;s really tough if you don&amp;rsquo;t know. For example, consider the sequence formed by summing every three natural numbers: $(1 + 2+ 3)= 6 $, $(4+5+6)=15$, $(7+8+9)=24 \cdots$ form an arithmetic sequence with the</description></item><item><title>Euler's Reflection Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/192/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/192/</guid><description>Formulas For non-integer $p$, $$ {\Gamma (1-p) \Gamma ( p )} = { {\pi} \over {\sin \pi p } } $$ Description It is the most famous formula among the formulas using the Gamma function. A useful result that can be obtained from the reflection formula is $ \Gamma ( { 1 \over 2} ) = \sqrt{\pi}$. Perhaps that’s why? The name &amp;ldquo;reflection formula&amp;rdquo; is</description></item><item><title>Proof of Euler's Representation of the Sinc Function</title><link>https://freshrimpsushi.github.io/en/posts/187/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/187/</guid><description>Definition Unnormalized Sinc Function The following function $\sinc : \mathbb{R} \to \mathbb{R}$ is called the sinc function. $$ \sinc x := \begin{cases} \displaystyle {{\sin x} \over {x}} &amp;amp; , \text{if } x \ne 0 \\ 1 &amp;amp; , \text{if } x = 0 \end{cases} $$ Normalized Sinc Function $$ \sinc x := \begin{cases} \displaystyle {{\sin \pi x} \over {\pi x}} &amp;amp; , \text{if } x \ne 0 \\ 1 &amp;amp;</description></item><item><title>Finding the Sum of Squares</title><link>https://freshrimpsushi.github.io/en/posts/189/</link><pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/189/</guid><description>Formula $$ \sum_{k=1}^{n} { k^2} = {{n(n+1)(2n+1)} \over {6}} $$ Derivation Let&amp;rsquo;s consider the difference between $k^3$ and $(k-1)^3$ which are of one higher order. $$ 1^3 - 0^3 = 3 \cdot 1^2 - 3 \cdot 1 + 1 \\ 2^3 - 1^3 = 3 \cdot 2^2 - 3 \cdot 2 + 1 \\ 3^3 - 2^3 = 3 \cdot 3^2 - 3 \cdot 3 + 1 \\ \vdots \\</description></item><item><title>Weierstrass's Infinite Product for the Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/150/</link><pubDate>Wed, 26 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/150/</guid><description>Theorem The following holds for the Gamma function $\Gamma : (0, \infty) \to \mathbb{R}$: $$ {1 \over \Gamma (x)} = x e^{\gamma x } \lim_{n \to \infty} \prod_{k=1}^{n} \left( 1 + {x \over k} \right) e^{- {x \over k} } $$ $\gamma$ is the Euler-Mascheroni constant. Explanation $$ \Gamma (x) = \int_{0}^{\infty} t^{x-1} e^{-t} dt $$ The Gamma function is defined as above, and also by Euler&amp;rsquo;s limit formula, $$</description></item><item><title>A Comprehensive Summary of Various Series Tests in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/186/</link><pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/186/</guid><description>Overview This post will introduce several series convergence tests without diving into their proofs. It is often more valuable to utilize these tests as facts, especially since the proofs can be quite tedious. In this post, we use the following notations: $\mathbb{N}$ is the set containing all natural numbers. $\mathbb{R}$ is the set containing all real numbers, and $\overline{\mathbb{R}}$ is the extended real number set that includes $\pm \infty$. $\left\{</description></item><item><title>Proof of the Density of Real Numbers</title><link>https://freshrimpsushi.github.io/en/posts/185/</link><pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/185/</guid><description>Theorem For two real numbers $a&amp;lt;b$, there exists a $r \in \mathbb{R}$ that satisfies $a&amp;lt;r&amp;lt;b$. Explanation In the real number space, no matter what interval you consider, there is always another real number in between. No matter how much you split it, there is a point that can be further divided. Although it seems obvious, keep in mind that this is not only non-obvious but also highly abstract. As an</description></item><item><title>Principle of Archimedes in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/181/</link><pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/181/</guid><description>Theorem For any positive number $a$ and real number $b$, there exists a natural number $n$ that satisfies $an&amp;gt;b$. Explanation This means that no matter what $b$ you take, you can always think of a multiple of $a$, which is $n$, that is greater than it. Simply put, ‘No matter how small a number is, if you keep adding to it, it will continue to gro</description></item><item><title>Three Axioms of Analysis: The Axiom of Completeness</title><link>https://freshrimpsushi.github.io/en/posts/180/</link><pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/180/</guid><description>Axioms1 A set $E \subset \mathbb{R}$ is not empty and if $E$ is bounded above, then a supremum $\sup(E) &amp;lt; \infty$ exists. Explanation The axioms of fields and orders might seem like complicating the known, but the completeness axiom does not seem so at a glance. Definitions for the terminology used here appear to be necessary first. Definitions For every element $a$ of $E$ if $a \le M$ is satisfied,</description></item><item><title>Electrons Cannot Be Constituents of the Nucleus</title><link>https://freshrimpsushi.github.io/en/posts/184/</link><pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/184/</guid><description>Theorem An electron cannot be a component of a nucleus. Explanation $10^{-14}\mathrm{m}$ A nucleus with a scale of $1\mathrm{MeV} \sim10\mathrm{MeV}$ emits an electron with energy in the range of $1\mathrm{MeV} \sim10\mathrm{MeV}$. In the early days of nuclear physics, it was believed that electrons existed within the nucleus. By using the uncertainty principle, it can be shown that an electron with such energy cannot be confined within the nucleus. Proof By</description></item><item><title>Momentum operator in quantum mechanics</title><link>https://freshrimpsushi.github.io/en/posts/100/</link><pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/100/</guid><description>Definition In quantum mechanics, the momentum operator is as follows: $$ P = \frac{\hbar}{\i}\frac{\partial}{\partial x} = -\i\hbar \dfrac{\partial }{\partial x} $$ Description The momentum operator is a function that allows one to calculate the momentum of a wave function. When a wave function with momentum $p = \hbar k$ is substituted, it satisfies the following equation. $$ P \psi = p \psi $$ In the case of dimensions higher than</description></item><item><title>Three Axioms of Analysis: 1 Field Axioms</title><link>https://freshrimpsushi.github.io/en/posts/178/</link><pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/178/</guid><description>Axioms1 Let&amp;rsquo;s accept the following properties for real numbers $a,b,c \in \mathbb{R}$ and operations $+,\cdot$. (A1) Closure under addition: $a+b \in \mathbb{R}$ (A2) Associative law for addition: $(a+b) + c = a + (b+c)$ (A3) Commutative law for addition: $ a+ b= b + a$ (A4) Identity element for addition: For every real number $a$, there exists a unique $0$ satisfying $a+0=0+a=a$. (A5) Inverse element for addition: For every real</description></item><item><title>Three Axioms of Analysis: The Second Order Axiom</title><link>https://freshrimpsushi.github.io/en/posts/177/</link><pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/177/</guid><description>Axioms1 For real numbers $ a,b,c \in \mathbb{R}$, the following properties are accepted: Trichotomy: For any given $a,b$, it must be that $a&amp;lt;b$ or $a&amp;gt;b$ or $a=b$ Transitivity: If $a&amp;lt;b$ and $b&amp;lt;c$ then $a&amp;lt;c$ Additivity: If $a&amp;lt;b$ and $c\in \mathbb{R}$ then $a+ c&amp;lt; b + c$ Multiplicativity: If $a&amp;lt;b$ and $c&amp;gt;0$ then $ac&amp;lt; bc$, or if $c&amp;lt;0$ then $ac&amp;gt; bc$ Description Although the terms are quite old-fashioned, they deal with</description></item><item><title>Compton scattering</title><link>https://freshrimpsushi.github.io/en/posts/182/</link><pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/182/</guid><description>Formula Let $\lambda$ be the wavelength of the incident light and $\lambda^{\prime}$ be the wavelength of the scattered photon. The following equation then holds true: $$ \lambda^{\prime} -\lambda = \frac{h}{m_{e}c}(1-\cos\theta) $$ Here, $h$ is Planck&amp;rsquo;s constant, $m_{e}$ is the mass of the electron, $c$ is the speed of light, and $\theta$ is the scattering angle. In terms of energy, we have: $$ \cos \theta=1-\frac{m_{e}c^{2}(E-E^{\prime})}{E^{\prime}E} $$ Explanation Compton scattering1 refers to</description></item><item><title>The rest mass of a photon is zero.</title><link>https://freshrimpsushi.github.io/en/posts/183/</link><pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/183/</guid><description>Summary Let the speed of the photon be $c = 299,792,458 \mathrm{m/s}$. Then the rest mass of the photon is $0$. Proof 1. Relationship between relativistic energy, momentum, and speed $$p=\gamma m_{0} v$$ $$E=\gamma m_{0} c^2$$ $$\implies \gamma m_{0}=\dfrac{E}{c^2}$$ By solving these equations simultaneously, $$p=\dfrac{E}{c^2}v$$ $$\implies v=\dfrac{pc^2}{E}$$ 2. Relativistic relationship between energy and momentum of a particle $$E=\sqrt{{m_{0}}^2c^4+p^2c^2}$$ 3. By 1 and 2 $$v=\frac{pc^2}{\sqrt{{m_{0}}^2c^4+p^2c^2}}$$ At this point, since the speed</description></item><item><title>Finding the Sum of a Geometric Sequence</title><link>https://freshrimpsushi.github.io/en/posts/170/</link><pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/170/</guid><description>Formula Given a geometric sequence $a_{n} = a r^{n-1}$ with the first term $a$ and common ratio $r$, $$ \sum_{k=1}^{n} a_{k}= {{a (1- r^{n} ) } \over {1-r}} $$ Proof Let&amp;rsquo;s denote it as $\displaystyle S= \sum_{k=1}^{n} a_{k}$. Then, $$ S= a + ar + \cdots + ar^{n-2} + ar^{n-1} $$ Multiplying both sides by $r$ gives $$ rS= ar + a r^2 + \cdots + ar^{n-1} + ar^{n} $$</description></item><item><title>Finding the Sum of an Arithmetic Sequence</title><link>https://freshrimpsushi.github.io/en/posts/169/</link><pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/169/</guid><description>Formula The arithmetic sequence $a_{n} = a+(n-1)d$ with the first term $a$ and the common difference $d$ $$ \sum_{k=1}^{n} a_{k}= {{n \left\{ 2a + (n-1)d \right\} } \over {2}} $$ Explanation Although this is a series that you might look at once and never write down again in this form, never forget its proof. Even if the proof is simple and straightforward, it&amp;rsquo;s crucial to write it down by hand</description></item><item><title>Velocity and Acceleration in Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/174/</link><pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/174/</guid><description>Velocity and Acceleration in Cartesian Coordinates $$ \begin{align*} \mathbf{r} &amp;amp;= x \hat{\mathbf{x}} + y \hat{\mathbf{y}} + z \hat{\mathbf{z}} \\ \mathbf{v} &amp;amp;= \dot{\mathbf{r}} = \dot{x} \hat{\mathbf{x}} + \dot{y} \hat{\mathbf{y}} + \dot{z} \hat{\mathbf{z}} \\ \mathbf{a} &amp;amp;= \dot{\mathbf{v}} = \ddot{\mathbf{r}} = \ddot{x} \hat{\mathbf{x}} + \ddot{y} \hat{\mathbf{y}} +\ddot{z}\hat{\mathbf{z}} \end{align*} $$ Derivation Determining velocity and acceleration in a Cartesian coordinate system is straightforward. Velocity Differentiating $\mathbf{r}$ with respect to $t$ yields the following. $$ \mathbf{v}=\frac{d}{dt}(x\hat{\mathbf{x}}</description></item><item><title>Differentiation of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/168/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/168/</guid><description>Theorem1 $$ \left( \sinh x \right)^{\prime} = \cosh x $$ $$ \left( \cosh x \right)^{\prime} = \sinh x $$ $$ \left( \tanh x \right)^{\prime} = \text{sech}^{2} x $$ Explanation The differentiation of hyperbolic functions actually doesn&amp;rsquo;t require much proof or memorization. The proofs simply use definitions, and the structures are almost identical to trigonometric functions, only with a change in signs. Using the method of proving hyperbolic sine, one can</description></item><item><title>Differentiation of Inverse Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/167/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/167/</guid><description>Theorem1 $$ \begin{align*} \left( \sin^{-1}x \right)^{\prime} &amp;amp;= {{1} \over {\sqrt{1-x^2}}} \\ \left( \cos^{-1}x \right)^{\prime} &amp;amp;= -{{1} \over {\sqrt{1-x^2}}} \\ \left( \tan^{-1}x \right)^{\prime} &amp;amp;= {{1} \over {1+x^2}} \end{align*} $$ Explanation They are read as arcsine, arccosine, and arctangent, respectively. It might seem surprising that these can be differentiated, but it turns out to be quite simple. As one can see on the right side, the shapes of the derivatives are not</description></item><item><title>Equation of the Tangent to a Circle with Slope m</title><link>https://freshrimpsushi.github.io/en/posts/172/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/172/</guid><description>Formula The equation of the tangent line to the circle $x^{2}+y^{2}=r^{2}$ with slope $m$ is as follows. $$ y=mx \pm r\sqrt{m^{2}+1} $$ Proof Let&amp;rsquo;s denote the equation of the line with slope $m$ as $y=mx+n$. Substituting into the equation of the circle and rearranging for x, we get $$ \begin{align*} x^2+(mx+n)^2 =&amp;amp;\ r^2 \\ x^2+m^2x^2+2mnx+n^2-r^2 =&amp;amp;\ 0 \\ (1+m^2)x^2+2mnx+n^2-r^2 =&amp;amp;\ 0 \end{align*} $$ Since the circle and the line are</description></item><item><title>Finding the Equation of the Tangent Line at a Point on a Circle</title><link>https://freshrimpsushi.github.io/en/posts/173/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/173/</guid><description>Explanation Let&amp;rsquo;s find the equation of the tangent line at a point $(x_{1},y_{1})$ on the circle $x^2+y^2=r^2$. This can be divided into cases when $y_{1}\neq 0$ and when $y_{1}=0$. $y_{1}\neq 0$ The slope from the center of the circle to the tangent point is $\dfrac{y_{1}}{x_{1}}$. Since the product of the slopes of two perpendicular lines is -1, the slope of the tangent line is $-\dfrac{x_{1}}{y_{1}}$. The equation of the line</description></item><item><title>Multiplication Formula Table</title><link>https://freshrimpsushi.github.io/en/posts/171/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/171/</guid><description>Overview Introducing commonly used multiplication formulas. Formulas $$ \begin{align} (a+b)^2 &amp;amp;=a^2+2ab+b^2 \\ (a-b)^2 &amp;amp;= a^2-2ab+b^2 \end{align} $$ $$ \begin{equation} (a+b)(a-b)=a^2-b^2 \end{equation} $$ $$ \begin{align} (a+b)^3 &amp;amp;= a^3+3a^2b+3ab^2+b^3 \\ (a-b)^3 &amp;amp;= a^3-3a^2b+3ab^2-b^3 \end{align} $$ $$ \begin{equation} (a+b+c)^2=a^2+b^2+c^2+2ab+2bc+2ca \end{equation} $$ $$ \begin{align} (a+b)(a^2-ab+b^2) &amp;amp;= a^3+b^3 \\ (a-b)(a^2+ab+b^2) &amp;amp;= a^3-b^3 \end{align} $$ Proof (1), (2) $$ \begin{align*} (a \pm b)^{2} &amp;amp;= (a \pm b)(a \pm b) \\ &amp;amp;= a^{2} \pm ab \pm ba</description></item><item><title>Proof of Fubini's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/165/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/165/</guid><description>Theorem1 Let&amp;rsquo;s define the function $f : R \to \mathbb{R}$ on the 2-dimensional domain $R : [a,b] \times [c,d]$. If $f(x,\cdot)$ is integrable over $[c,d]$, and $f(\cdot,y)$ is integrable over $[a,b]$, and $f$ is integrable over $R$, then $$ \iint _{R} f dA = \int_{a}^{b} \int_{c}^{d} f(x,y) dy dx = \int_{c}^{d} \int_{a}^{b} f(x,y) dx dy $$ Explanation The integration domain $R$ obviously comes from a Rectangle. As it is always</description></item><item><title>Proof of Green's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/166/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/166/</guid><description>Theorem1 Let the curve $\mathcal{C}$ be a simple, smooth, closed path in the plane $S = [a,b] \times [c,d]$, moving counterclockwise. If the function $P,Q : \mathbb{R}^2 \to \mathbb{R}$ is continuous on $\mathcal{C}$ and its derivative is also continuous, $$ \int_{\mathcal{C}} (Pdx + Qdy) = \iint_{S} (Q_{x} - P_{y}) dx dy $$ Explanation This can be thought of as a theorem that converts line integrals into surface integrals. It&amp;rsquo;s widely</description></item><item><title>Velocity and Acceleration in a Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/164/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/164/</guid><description>Spherical Coordinates: Velocity and Acceleration $$ \begin{align*} \mathbf{v} &amp;amp;=\dot{r} \hat {\mathbf{r}} +r \dot{\theta} \hat{ \boldsymbol{\theta}}+ r \dot{\phi} \sin{\theta} \hat{ \boldsymbol{\phi}} \\ \mathbf{a} &amp;amp;= (\ddot{r}-r\dot\theta^2-r\dot\phi^2\sin^2\theta)\hat{\mathbf{r}}+(r\ddot\theta+2\dot{r}\dot\theta-r\dot\phi^2\sin\theta\cos\theta)\hat{\boldsymbol{\theta}} \\ &amp;amp;\quad+(r\ddot\phi\sin\theta+2\dot{r}\dot\phi\sin\theta+2r\dot\theta\dot\phi\cos\theta)\hat{\boldsymbol{\phi}} \end{align*} $$ Derivation Unit vectors in spherical coordinates are as follows. $$ \begin{align*} \hat{\mathbf{r}} &amp;amp;= \cos \phi \sin \theta \hat{\mathbf{x}} + \sin \phi \sin \theta \hat{\mathbf{y}} + \cos\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\theta}} &amp;amp;= \cos\phi \cos\theta \hat{\mathbf{x}} + \sin\phi \cos\theta \hat{\mathbf{y}} - \sin\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\phi}} &amp;amp;= -\sin\phi</description></item><item><title>Proof of ML Auxiliary Lemma</title><link>https://freshrimpsushi.github.io/en/posts/162/</link><pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/162/</guid><description>Theorem 1 Let&amp;rsquo;s say the function $f$ is piecewise continuous on the integration path $\mathscr{C}: z = z(t), t \in [a,b]$. If the positive number $\displaystyle L = \int_{a}^{b} |z&amp;rsquo;(t)| dt$ is the length of $\mathscr{C}$, and for all points on $\mathscr{C}$ there exists a positive number $M$ satisfying $|f(z)| \le M$, then $$ \left| \int_{\mathscr{C}} f(z) dz \right| \le ML $$ Proof For the function $z&amp;rsquo;: [a,b] \to \mathbb{C}$</description></item><item><title>Pythagorean Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/161/</link><pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/161/</guid><description>Theorem Given a right-angled triangle, if we call the length of the hypotenuse $c$, and the lengths of the other two sides $a,b$, then the following equation holds. $$ a^2 + b^2 = c^2 $$ Explanation Apart from its wide applications, this theorem is very practical in itself. It&amp;rsquo;s named after Pythagoras for leaving behind the oldest &amp;lsquo;proof&amp;rsquo;, but it is speculated that most ancient civilizations, which could be considered</description></item><item><title>The Conditions for the Inverses of Cauchy-Riemann Equations to Hold</title><link>https://freshrimpsushi.github.io/en/posts/160/</link><pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/160/</guid><description>Theorem In the complex domain □ref.□ defined by complex function □ref.□ that takes real values for function □ref.□, $$ f(z) = f(x+iy) = u(x,y) + iv(x,y) $$ can be represented, and if □ref.□ has a continuous first-order partial derivative with respect to □ref.□ and</description></item><item><title>Velocity and Acceleration in Cylindrical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/163/</link><pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/163/</guid><description>Velocity and Acceleration in Cylindrical Coordinates $$ \begin{align*} \mathbf{v}&amp;amp;=\dot{r} \hat{\mathbf{r}} + r \dot{\phi} \hat{\boldsymbol{\phi}}+\dot{z} \hat{\mathbf{z}} \\ \mathbf{a} &amp;amp;= (\ddot r -r\dot{\phi} ^2)\hat{\mathbf{r}} + (2\dot{r} \dot{\phi} + r\ddot{\phi})\hat{\boldsymbol{\phi}} + \ddot{z}\hat{\mathbf{z}} \end{align*} $$ Derivation In cylindrical coordinates, the unit vectors are as follows. $$ \begin{align*} \boldsymbol{\rho}&amp;amp;=x\hat{\mathbf{x}}+y \hat{\mathbf{y}} +z\hat{\mathbf{z}}=r\hat{\mathbf{r}} +z\hat{\mathbf{z}} \\ \hat{\mathbf{r}} &amp;amp;= \hat{\mathbf{r}}(\phi) = \cos\phi \hat{\mathbf{x}} + \sin\phi \hat{\mathbf{y}} \\ \hat{\boldsymbol{\phi}} &amp;amp;= \hat{\mathbf{r}}(\phi+\pi/2) = -\sin\phi \hat{\mathbf{x}} + \cos\phi \hat{\mathbf{y}} \\ \hat{\mathbf{z}} &amp;amp;=</description></item><item><title>Cauchy-Riemann Equations</title><link>https://freshrimpsushi.github.io/en/posts/159/</link><pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/159/</guid><description>Theorem 1 Let the function $f: A \subseteq \mathbb{C} \to \mathbb{C}$ be analytic at $\mathscr{R}$. If for the real function $u,v$ we have $$ f(z) = f(x+iy) = u(x,y) + iv(x,y) $$ then $u,v$ has a first-order partial derivative with respect to $x,y$ and satisfies the following system of partial differential equations at every point on $\mathscr{R}$. $$ \begin{cases} u_{x} (x,y) = v_{y} (x,y) \\ u_{y} (x,y) = -v_{x} (x,y)</description></item><item><title>Derivation of Triple Angle Formulas for Trigonometric Functions Using De Moivre's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/154/</link><pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/154/</guid><description>Formulas $$ \sin 3\theta = 3 \sin \theta - 4 \sin^{3} {\theta} \\ \cos 3\theta = 4 \cos^{3} {\theta} - 3 \cos \theta $$ Description Traditional transformation formulas usually can be derived by using the addition formulas for trigonometric functions multiple times. For instance, the double angle formula is derived by substituting $b=a$ into $\sin(a + b ) = \sin {a} \cos {b} + \sin {b} \cos {a}$ to get</description></item><item><title>The Relationship between Trigonometric and Hyperbolic Functions in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/157/</link><pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/157/</guid><description>Definition 1 Let&amp;rsquo;s define the hyperbolic function as a complex function $\sinh, \cosh : \mathbb{C} \to \mathbb{C}$ as follows. $$ \sinh z := { {e^{z} - e^{-z}} \over 2 } \\ \cosh z := { {e^{z} + e^{-z}} \over 2 } $$ Theorem 2 $$ \begin{align*} \sinh (iz) =&amp;amp; i \sin z \\ \sin (iz) =&amp;amp; i \sinh z \\ \cosh (iz) =&amp;amp; \cos z \\ \cos (iz) =&amp;amp; \cosh</description></item><item><title>The Relationship between Trigonometric Functions and Exponential Functions in Complex Analysis</title><link>https://freshrimpsushi.github.io/en/posts/155/</link><pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/155/</guid><description>Theorem 1 The sine, cosine functions as complex functions $\sin , \cos : \mathbb{C} \to \mathbb{C}$ are as follows. $$ \sin z = { {e^{iz} - e^{-iz}} \over 2 i } \\ \cos z = { {e^{iz} + e^{-iz}} \over 2 } $$ Description It&amp;rsquo;s actually okay to think of this more as a definition than a theorem. The purpose is to demonstrate that defining it this way does not</description></item><item><title>Velocity and Acceleration in Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/158/</link><pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/158/</guid><description>Velocity and Acceleration in Polar Coordinates $$ \begin{align*} \mathbf{v}&amp;amp;=\dot{r} \hat{\mathbf{r}} + r \dot{\theta} \hat{\boldsymbol{\theta}} \\ \mathbf{a}&amp;amp;= (\ddot r -r\dot{\theta} ^2)\hat{\mathbf{r}} + (2\dot{r} \dot{\theta} + r\ddot{\theta})\hat{\boldsymbol{\theta}} \end{align*} $$ Derivation In the polar coordinate system, unit vectors can be described as follows. $$ \begin{align*} &amp;amp;&amp;amp; \mathbf{r}&amp;amp;=r\hat{\mathbf{r}}=x\hat{\mathbf{x}} + y \hat{\mathbf{y}} \\ \implies &amp;amp;&amp;amp; \hat{\mathbf{r}} &amp;amp;= \frac{x}{r}\hat{\mathbf{x}} +\frac{y}{r} \hat{\mathbf{y}}=\cos\theta \hat{\mathbf{x}} + \sin\theta \hat{\mathbf{y}} = \hat{\mathbf{r}} (\theta) \\ {} \\ &amp;amp;&amp;amp; \hat \theta &amp;amp;= \hat{\mathbf{r}}(\theta+\pi/2)=</description></item><item><title>Proof of the Convergence of the Euler-Mascheroni Constant</title><link>https://freshrimpsushi.github.io/en/posts/151/</link><pubDate>Sun, 16 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/151/</guid><description>Theorem $$ \gamma = \lim_{n \to \infty} \left( \sum_{k=1}^{n} \left( { 1 \over k } \right) - \ln{n} \right) = 0.577215664 \cdots $$ Description When associated with the Riemann zeta function, it also serves as the $\gamma$ $0$’th Stieltjes constant $\gamma_{0}$. $\gamma$ is briefly known as the Euler&amp;rsquo;s constant, which has a deep relationship with the Gamma function. Setting the exact value aside, does it</description></item><item><title>Unit Vectors of the Spherical Coordinate System Expressed in Terms of Unit Vectors of the Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/152/</link><pubDate>Sun, 16 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/152/</guid><description>Spherical Coordinate System&amp;rsquo;s Unit Vectors $$ \begin{align*} \hat{\mathbf{r}} &amp;amp;= \cos\phi \sin\theta\hat{\mathbf{x}} + \sin\phi \sin\theta\hat{\mathbf{y}} + \cos\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\theta}} &amp;amp;= \cos\phi \cos\theta \hat{\mathbf{x}} + \sin\phi \cos\theta \hat{\mathbf{y}} - \sin\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\phi}} &amp;amp;= -\sin\phi \hat{\mathbf{x}} + \cos\phi \hat{\mathbf{y}} \end{align*} $$ Derivation First, calculate $\hat{\mathbf{r}}$ and then use it to derive the other two. Radial Direction Unit Vector $\hat{\mathbf{r}}$ $$ \hat{\mathbf{r}}=r\hat{\mathbf{r}}=x\hat{\mathbf{x}}+y\hat{\mathbf{y}}+z\hat{\mathbf{z}} $$ Therefore, dividing both sides by $r$ gives: $$ \begin{align*} \hat{\mathbf{r}}&amp;amp;=\frac{x}{r}\hat{\mathbf{x}}+\frac{y}{r}\hat{\mathbf{y}}+\frac{z}{r}\hat{\mathbf{z}} \\</description></item><item><title>Euler's Limit Formula Derivation for the Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/149/</link><pubDate>Sat, 15 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/149/</guid><description>Formula 1 The following holds for the Gamma function $\Gamma : (0, \infty) \to \mathbb{R}$: $$ \Gamma (x) = \lim_{n \to \infty} {{n^x n!} \over {x(x+1)(x+2) \cdots (x+n) }} $$ Explanation The Gamma function that we previously knew in the form of integral $$ \Gamma (x) = \int_{0}^{\infty} t^{x-1} e^{-t} dt $$ looks entirely different, yet in 1729 Euler proved that both expressions are exactly the same. The derivation introduced</description></item><item><title>Calculating the Area of an Ellipse Using Integration</title><link>https://freshrimpsushi.github.io/en/posts/145/</link><pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/145/</guid><description>Formula The area of the ellipse $\displaystyle {x^2 \over a^2} + {y^2 \over b^2} = 1$ is $ab \pi$. Explanation Especially $a=b=r$, that is, the area of a circle $x^2 + y^2=r^2$ with radius $r$ is as well known $r^2 \pi$. Proof To obtain the area of an ellipse, it is sufficient to calculate only the area of the shaded region. The area of the region is given by $$</description></item><item><title>Scalar Triple Product</title><link>https://freshrimpsushi.github.io/en/posts/144/</link><pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/144/</guid><description>Definition The following expression is called the scalar triple product. $$ \mathbf{A}\cdot (\mathbf{B} \times \mathbf{C} ) $$ Explanation A scalar triple product is an operation involving the product of three vectors, where the result is a scalar. The operation resulting in a vector is called vector triple product. To get a scalar result, one must first cross multiply two vectors to produce another vector and then dot multiply it with</description></item><item><title>Finding Internal and External Division Points on a Line</title><link>https://freshrimpsushi.github.io/en/posts/138/</link><pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/138/</guid><description>Theorem The coordinates of the point $A(x_{1})$ and the point $B(x_{2})$ on the number line, dividing internally at $m:n$, are $\displaystyle x=\frac{mx_{2}+nx_{1}}{m+n}$, and the coordinates of the point that divides externally at $m:n$ are $\displaystyle x=\frac{mx_{2}-nx_{1}}{m-n}$. Explanation By examining the formulas for the internal and external division point, one can see that only the signs are different. There is no need to memorize both expressions; simply memorize the internal division</description></item><item><title>Gradient of the Magnitude of Separation Vectors</title><link>https://freshrimpsushi.github.io/en/posts/142/</link><pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/142/</guid><description>Formulas The square of the magnitude of the separation vector $\bcR$ and the gradient of $\cR ^{n}$ are as follows. $$ \nabla (\cR^n)=n\cR^{n-1}\crH $$ Explanation It is calculated in the same way as the derivative of a polynomial function, and then just attach the unit vector $\crH$. Since the separation vector is $\bcR=\mathbf{r}-\mathbf{r}^{\prime}$, it has variables $(x,y,z)$ and $(x^{\prime},y^{\prime},z^{\prime})$. Therefore, attention must be paid when differentiating. Gradients for coordinates with</description></item><item><title>Separation Vector</title><link>https://freshrimpsushi.github.io/en/posts/141/</link><pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/141/</guid><description>Definition1 The vector from the source point to the observation point is called the separation vector. $$ \bcR = \mathbf{r} - \mathbf{r}^{\prime} $$ Description Source vector $\mathbf{r}^{\prime}$: The place where there is a charge or current. That is, it represents the coordinates of the origin of the electromagnetic field. Position vector $\mathbf{r}$: Represents the coordinates of where the electric field $\mathbf{E}$ or magnetic field $\mathbf{B}$ is measured. Separation vector $\bcR$:</description></item><item><title>The Magnitude of the Imaginary Power of a Real Number is Always 1</title><link>https://freshrimpsushi.github.io/en/posts/126/</link><pubDate>Thu, 29 Jun 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/126/</guid><description>Theorem $$ \left| r^{i \theta} \right| = 1 $$ For any real number $r, \theta$ that is not $0$, the magnitude of its imaginary power is $1$.</description></item><item><title>Calculus and the Euler Formula</title><link>https://freshrimpsushi.github.io/en/posts/112/</link><pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/112/</guid><description>Theorem Euler&amp;rsquo;s Formula: $$ { e }^{ ix }= \cos x + i \sin x $$ Euler&amp;rsquo;s Identity: $$ { e }^{ i\pi }+1=0 $$ Explanation Euler&amp;rsquo;s Formula is in itself so peculiar that even Euler did not know where it might be used, but nowadays, it is utilized in so many fields that it is difficult to summarize its usefulness. It is even more astonishing when considering it was</description></item><item><title>Prove that the Product of the Slopes of Two Perpendicular Lines is Always -1</title><link>https://freshrimpsushi.github.io/en/posts/111/</link><pubDate>Sat, 20 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/111/</guid><description>Theorem The product of the slopes of two perpendicular lines is always $-1$. Explanation This is a fact that can be very useful in many problems. We introduce two methods of proof. Proof 1 Use Pythagoras&amp;rsquo; theorem. See the figure below. Suppose the slopes of two perpendicular lines are $a$, $a^{\prime}$. Then, considering the right triangle $\triangle OAA^{\prime}$ as shown above, we obtain the following result by Pythagoras&amp;rsquo; theorem. $$</description></item><item><title>Congruences in Number Theory</title><link>https://freshrimpsushi.github.io/en/posts/106/</link><pubDate>Thu, 11 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/106/</guid><description>Definition 1 Given integers $a \equiv b \pmod{m}$, $\iff$, $a$, $b$, $m$, there exists an integer $k$ that satisfies $a = b + mk$. Theorem Assuming $a_{1} \equiv b_{1} \pmod{m}$ and $a_{2} \equiv b_{2} \pmod{m}$ are true: [1] Addition: $a_{1} + a_{2} \equiv b_{1} + b_{2} \pmod{m}$ [2] Subtraction: $a_{1} - a_{2} \equiv b_{1} - b_{2} \pmod{m}$ [3] Multiplication: $a_{1} a_{2} \equiv b_{1} b_{2} \pmod{m}$ [4] Division: if $\gcd (</description></item><item><title>Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/103/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/103/</guid><description>Definition A function that satisfies the following two conditions is called the Dirac delta function. $$ \delta (x) = \begin{cases} 0, &amp;amp; x\neq 0 \\ \infty , &amp;amp; x=0 \end{cases} $$ $$ \int_{-\infty}^{\infty}{\delta (x) dx}=1 $$ Description ※Be careful not to confuse it with the Kronecker delta. In engineering, it is called the unit impulse function. Strictly speaking, mathematically, the Dirac delta function is not a function because</description></item><item><title>Properties of the Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/104/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/104/</guid><description>Properties $$ \begin{equation} \delta (-x) =\delta (x) \end{equation} $$ $$ \begin{equation} \delta (kx)= \frac{1}{|k|} \delta (x) \end{equation} $$ Proof Proof of $(1)$ Substituting $\int_{-\infty }^ { \infty } f(x) \delta (-x) dx$ with $-x \equiv y$ gets us $x=-y$ and $dx=-dy$, $$ \begin{align*} \int_{-\infty } ^{ \infty } f(x) \delta (-x) dx =&amp;amp;\ -\int_{ \infty }^{-\infty} f(-y) \delta (y) dy \\ =&amp;amp;\ \int_{-\infty } ^{\infty } f(-y) \delta (y) dy</description></item><item><title>The Period of Simple Pendulum Motion is Independent of the Pendulum's Mass</title><link>https://freshrimpsushi.github.io/en/posts/102/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/102/</guid><description>Theorem The period $T$ of a simple pendulum motion is independent of the mass of the pendulum $m$. Description Therefore, the period $T$ of a simple pendulum motion is independent of the pendulum&amp;rsquo;s mass, the amplitude&amp;rsquo;s size, etc., and depends solely on the pendulum&amp;rsquo;s length and the acceleration due to gravity. Proof The restoring force of the pendulum is as follows: $$ F=-mg\sin\theta $$ Since $x=l\theta$, when $\theta$ is sufficiently</description></item><item><title>규격화된 파동함수의 상태는 시간의 변화에 무관하다</title><link>https://freshrimpsushi.github.io/en/posts/101/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/101/</guid><description>Theorem1 A normalized wave function remains in a normalized state even as time changes. Explanation Let us assume the wave function is normalized at time $t=0$. According to the theorem, it is guaranteed to remain in a normalized state as time progresses. This is a very crucial fact that allows us to treat the wave function as a probability density function. Proof Strategy: To show that it remains constant over</description></item><item><title>Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/95/</link><pubDate>Sun, 30 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/95/</guid><description>Definition The function defined as follows $\Gamma : (0, \infty) \to \mathbb{R}$ is called the Gamma function. $$ \Gamma (x) := \int_{0}^{\infty} t^{x-1} e^{-t} dt $$ Description Focusing on the integral in the equation above, it is also referred to as Euler&amp;rsquo;s integral. The Gamma function is famous as an exceedingly important function not just in pure mathematics but also in physics, statistics, etc. It possesses a plethora of interesting</description></item><item><title>Einstein Notation</title><link>https://freshrimpsushi.github.io/en/posts/90/</link><pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/90/</guid><description>Notation The summation sign $\sum$ is omitted when a subscript is repeated two or more times. Description Also referred to as the Einstein summation convention. It&amp;rsquo;s not really a formula but rather a rule. When doing vector calculations, there are often cases where one needs to write the summation sign $\sum$ multiple times in a single formula, which can make the equation look cluttered and is very annoying to write</description></item><item><title>Parabolic Motion: Horizontal Range and Maximum Height Angle</title><link>https://freshrimpsushi.github.io/en/posts/89/</link><pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/89/</guid><description>Definition1 2 An object launched with angle $\alpha$ and initial speed $v_{0}$ performs a motion known as parabolic motion. Description It&amp;rsquo;s also referred to as projectile motion. Typically, external forces like air resistance are ignored, so the motion is uniform in the horizontal direction and free fall in the vertical direction. Analysis Motion in the $x$ direction (horizontal) is independent of gravity, while motion in the $y$ direction (vertical) is</description></item><item><title>Product of Two Levi-Civita Symbols</title><link>https://freshrimpsushi.github.io/en/posts/88/</link><pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/88/</guid><description>Theorem The $\epsilon_{ijk}$, defined as follows, is referred to as the Levi-Civita symbol. $$ \epsilon_{ijk} = \begin{cases} +1 &amp;amp; \text{if} \ \epsilon_{123}, \epsilon_{231}, \epsilon_{312} \\ -1 &amp;amp; \text{if} \ \epsilon_{132}, \epsilon_{213}, \epsilon_{321} \\ 0 &amp;amp; \text{if} \ i=j \ \text{or} \ j=k \ \text{or} \ k=i \end{cases} $$ The $\delta_{ij}$, defined as follows, is referred to as the Kronecker delta. $$ \delta_{ij} := \begin{cases} 1,&amp;amp;i=j \\ 0, &amp;amp; i\ne j</description></item><item><title>The minimum energy of a hydrogen atom in quantum mechanics</title><link>https://freshrimpsushi.github.io/en/posts/92/</link><pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/92/</guid><description>정리 수소원자의 최소 에너지는 다음과 같다. $$ E_{min}=-\frac{1}{2}mc^2\alpha^{2} $$ $m$은 수소원자의 질량, $c$는 광속, $\alpha$는 미세구조상수이다. 설명 여기서 $\alph</description></item><item><title>Series Expansion of the Arctangent Function</title><link>https://freshrimpsushi.github.io/en/posts/86/</link><pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/86/</guid><description>Theorem1 $$ \tan ^{ -1 } x = \sum _{ n=0 }^{ \infty }{ \frac { (-1) ^{ n } { x } ^ { 2n+1 } } { 2n+1 } } $$ Description Whether it is written as $\arctan$ or as $\tan ^{-1}$ does not matter. Among the several inverse trigonometric functions, arctan is particularly interesting because it provides a series that converges to $\pi$. When $x=1$ is substituted,</description></item><item><title>Kronecker Delta</title><link>https://freshrimpsushi.github.io/en/posts/84/</link><pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/84/</guid><description>Definition We define $\delta_{ij}$ as the Kronecker delta. $$ \delta_{ij} := \begin{cases} 1,&amp;amp;i=j \\ 0, &amp;amp; i\ne j \end{cases} $$ Explanation The Kronecker delta is used in many places, primarily to highlight the desired components (elements, possibilities, etc.) among all possible options. Physics students often encounter it in the context of dot products. If this concept isn&amp;rsquo;t immediately clear, consider the following example: Example Suppose we are given two vectors</description></item><item><title>Levi-Civita Symbol</title><link>https://freshrimpsushi.github.io/en/posts/83/</link><pubDate>Tue, 25 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/83/</guid><description>Definition The $\epsilon_{ijk}$ defined as follows is called the Levi-Civita symbol. $$ \epsilon_{ijk} = \begin{cases} +1 &amp;amp; \text{if} \ \epsilon_{123}, \epsilon_{231}, \epsilon_{312} \\ -1 &amp;amp; \text{if} \ \epsilon_{132}, \epsilon_{213}, \epsilon_{321} \\ 0 &amp;amp; \text{if} \ i=j \ \text{or} \ j=k \ \text{or} \ k=i \end{cases} $$ Description While the Kronecker delta only considers whether the indices are equal, the Levi-Civita symbol, as shown in its definition, is also affected by</description></item><item><title>Features of Special Relativity due to Lorentz Transformation: Time Dilation</title><link>https://freshrimpsushi.github.io/en/posts/75/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/75/</guid><description>Characteristics of Lorentz Transformation In special relativity, the transformation between two coordinate systems differs from the classical transformation. This is because &amp;rsquo;the speed of light is the same for all observers&amp;rsquo;. Taking this condition into account leads to the derivation of the Lorentz transformation. The Lorentz transformation introduces three new phenomena that do not appear in classical physics. Loss of simultaneity Time dilation Length contraction Time Dilation Simply put, time</description></item><item><title>Momentum and Impulse Relationship</title><link>https://freshrimpsushi.github.io/en/posts/73/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/73/</guid><description>Definition Momentum The product of an object&amp;rsquo;s mass and velocity is called momentum, denoted by $p$. While in high school physics, velocity $v$ is often used to represent the state of motion of an object, in college physics, momentum $p$ is more commonly used. $$ \vec{p}=m\vec{v}[kg\cdot m/s] $$ Since velocity $v$ is a vector, so is momentum. Since mass $m$ is always positive, the direction of velocity and momentum are</description></item><item><title>Uniform Acceleration Linear Motion and its Graphs</title><link>https://freshrimpsushi.github.io/en/posts/72/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/72/</guid><description>Definition When an object&amp;rsquo;s acceleration does not change over time $t$, it is said to undergo uniform acceleration. $$ a(t)=a $$ Uniformly accelerated motion refers to moving in a straight line with constant, unchanging acceleration. What matters here is whether the acceleration $a$ is positive or negative: if $a&amp;gt;0$, it will move faster in the initial direction, and if $a&amp;lt;0$, it will slow down to a velocity of 0 and</description></item><item><title>Uniform Motion and Graphs</title><link>https://freshrimpsushi.github.io/en/posts/74/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/74/</guid><description>Description Uniform motion, when the term is dissected, stands for &amp;lsquo;uniform (equal) + speed&amp;rsquo;. This implies motion where the speed, which is a vector, is kept constant. Both the magnitude (velocity) and direction must remain constant for it to be considered uniform motion. In other words, uniform motion refers to motion where both the speed and direction are constant. Graph The graph of uniform motion is as follows: In the</description></item><item><title>Vector Triple Product, BAC-CAB Rule</title><link>https://freshrimpsushi.github.io/en/posts/71/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/71/</guid><description>Formulas $$ \mathbf{A} \times (\mathbf{B} \times \mathbf{C} ) = \mathbf{B}(\mathbf{A} \cdot \mathbf{C} )-\mathbf{C}(\mathbf{A} \cdot \mathbf{B}) $$</description></item><item><title>A Simple Formula to Calculate the Sum of Elements of the Product of Second-Order Matrices</title><link>https://freshrimpsushi.github.io/en/posts/70/</link><pubDate>Sat, 22 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/70/</guid><description>Formulas The sum of the elements of the quadratic matrix $\begin{bmatrix} { a }&amp;amp;{ b } \\ { c }&amp;amp;{ d } \end{bmatrix} \begin{bmatrix} { p }&amp;amp;{ q } \\ { r }&amp;amp;{ s } \end{bmatrix}$ is as follows. $$ {(a+c)(p+q)}+{(b+d)(r+s)} $$ Description You may have encountered many problems asking to find the sum of the elements of the product of two quadratic matrices. As everyone knows, though multiplying matrices</description></item><item><title>Euclidean Algorithm Proof</title><link>https://freshrimpsushi.github.io/en/posts/65/</link><pubDate>Thu, 20 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/65/</guid><description>Algorithm For two integers $a \ge b$, $\text{gcd}(a,b)$ can be calculated as follows. Pseudocode Algorithm: Euclidean method In Given integers $a, b$. 1. $r_{0} \gets a$ 2. $r_{1} \gets b$ 3. while $r_{i+1} \ne 0$ 4. Continue to find $q_i$ and a new $r_{i+1}$ such that the following is satisfied.$$r_{i-1} = r_i \cdot q_i + r_{i+1} \qquad , (r_i&amp;gt;r_{i+1})$$ 5. $i \gets i+1$ 6. end while Out Obtain the greatest</description></item><item><title>Proof of the Fundamental Theorem of Algebra for Congruent Equations</title><link>https://freshrimpsushi.github.io/en/posts/66/</link><pubDate>Thu, 20 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/66/</guid><description>Theorem 1 For a given prime number $p$, if we denote $p\nmid a_{ 0 }$, polynomials $$ f(x)=a_{ 0 }x^{ d }+a_{ 1 }x^{ d-1 }+ \cdots +a_{ d-1 }x+a_{ d } $$ with all integer coefficients have at most $d$ distinct solutions to the equation $f(x)\equiv 0 \pmod{p}$. Explanation Just as commonly known, for polynomials with real coefficients, a $n$-degree equation includes $n$ solutions, including multiplicity. From the perspective</description></item><item><title>Euclid's Proof: There are Infinitely Many Primes</title><link>https://freshrimpsushi.github.io/en/posts/64/</link><pubDate>Sat, 15 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/64/</guid><description>Theorem 1 Prime numbers exist infinitely. Explanation There are several methods to prove that prime numbers are infinite. Among them, the simplest is Euclid&amp;rsquo;s method. This proof is not only simple but also famously elegant. Proof Assume that there are $n$ prime numbers. Let&amp;rsquo;s call these primes $p_1, p_2, \cdots , p_n$ and consider $p_{n+1}=p_1 p_2 \cdots p_n + 1$. If $p_{n+1}$ is a prime number, then $p_{n+1}$ is a</description></item><item><title>Derivation of the Series Form of the Natural Logarithm and Proof of the Convergence of the Alternating Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/58/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/58/</guid><description>Theorem $$ \ln(1-x)=\sum _{ n=0 }^{ \infty }{ \frac { -{ x }^{ n+1 } }{ n+1 } } $$ Description The series form of $\ln(1-x)$ can be relatively easily obtained. For $\ln(1+x)$, it is enough to substitute $-x$ for $x$ as a result of the theorem. $$ -\ln(1-x)=x+\frac { { x }^{ 2 } }{ 2 }+\frac { { x }^{ 3 } }{ 3 }+\frac { { x</description></item><item><title>Exponential, Sine, and Cosine Functions' Taylor Series Expansion</title><link>https://freshrimpsushi.github.io/en/posts/59/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/59/</guid><description>Theorem1 $$ \begin{equation} { { e ^ x } }=\sum _{ n=0 }^{ \infty }{ \frac { { x } ^{ n } }{ n! } } \end{equation} $$ $$ \begin{equation} \sin x=\sum _{ n=0 }^{ \infty }{ \frac { { x } ^{ 2n+1 } }{ (2n+1)! }{ { (-1) }^{ n } } } \end{equation} $$ $$ \begin{equation} \cos x=\sum _{ n=0 }^{ \infty }{ \frac { {</description></item><item><title>Derivation of the Quadratic Formula Step by Step</title><link>https://freshrimpsushi.github.io/en/posts/56/</link><pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/56/</guid><description>Formulas For the quadratic equation $ax^{2}+bx+c=0$ (where $a\neq 0$): $$ x=\dfrac{ -b\pm \sqrt { b^{2}-4ac } }{2a} $$ Explanation Given a quadratic equation, its roots can be easily found through the formula. Derivation Strategy: The key to deriving the formula is to convert it into a complete square form. This is explained in great detail for children who are not familiar with math. Simply follow along without questioning, and try</description></item><item><title>If an Infinite Series Converges, Then the Infinite Sequence Converges to 0</title><link>https://freshrimpsushi.github.io/en/posts/54/</link><pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/54/</guid><description>Theorem If $\displaystyle \sum _{ n=1 }^{ \infty }{ { a }_{ n }}$ converges, then $\displaystyle \lim _{ n\to \infty }{ { a }_{ n }}=0$ Explanation This theorem might be a bit surprising and counterintuitive at first. You might wonder why the converse doesn’t hold. A classic counterexample involves considering the following sequences: $$ \begin{align*} { a }_{ n }&amp;amp;=\frac { 1</description></item><item><title>Proof of the Power Formula for Rotation Transformation Matrices</title><link>https://freshrimpsushi.github.io/en/posts/55/</link><pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/55/</guid><description>Theorem For every natural number $n$, the following holds. $$ \begin{bmatrix} { \cos \theta }&amp;amp;{ -\sin \theta } \\ { \sin \theta }&amp;amp;{ \cos \theta } \end{bmatrix} ^{n} = \begin{bmatrix} { \cos n\theta }&amp;amp;{ -\sin n\theta } \\ { \sin n\theta }&amp;amp;{ \cos n\theta } \end{bmatrix} $$ Explanation A linear transformation matrix that rotates by $\theta$ around the origin, when squared, results in a linear transformation that rotates by $n\theta$.</description></item><item><title>Equation of the Tangent to a Parabola</title><link>https://freshrimpsushi.github.io/en/posts/52/</link><pubDate>Wed, 05 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/52/</guid><description>Derivation In the Case Where the Slope is Given Let&amp;rsquo;s first look at the case where the slope is given. When the equation of the line tangent to the parabola $y^{ 2 }=4px$ is $y=mx+n$, the two shapes must meet at only one point, thus $$ (mx+n)^{ 2 }=4px \implies m^{ 2 }x^{ 2 }+2(mn-2p)x+n^{ 2 }=0 $$ by the quadratic formula, $$ \frac { D }{ 4 }=m^{ 2</description></item><item><title>Inverse Functions of Fractional Functions and the Shape of the Inverse Matrix of a Quadratic Square Matrix</title><link>https://freshrimpsushi.github.io/en/posts/53/</link><pubDate>Wed, 05 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/53/</guid><description>Theorem The inverse function of the fractional function $\displaystyle f(x)=\frac { ax+b }{ cx+d }$ is $$ f^{ -1 }(x)=\frac { dx-b }{ -cx+a } $$ The inverse matrix of the 2x2 square matrix $\begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \end{bmatrix}$ is $$ \frac { 1 }{ ad-bc } \begin{bmatrix} d &amp;amp; -b \\ -c &amp;amp; a \end{bmatrix} $$ Explanation It might just be a coincidence, but finding</description></item><item><title>Area of a Triangle Enclosed by a Straight Line and the x and y Axes</title><link>https://freshrimpsushi.github.io/en/posts/50/</link><pubDate>Mon, 03 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/50/</guid><description>Overview Questions asking whether it&amp;rsquo;s possible to find the maximum or minimum value, or the tangent line often involve calculating the area of such triangles $S$. Of course, finding the area of a triangle is not difficult, but it would be even better if one could remember a simple formula and solve it immediately. Theorem The $y$ intercept of the line $y=mx+n$ is $n$, and the $x$ intercept is $-\frac</description></item><item><title>Cauchy-Schwarz Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/51/</link><pubDate>Mon, 03 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/51/</guid><description>Theorem $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\ge { (ax+by) }^{ 2 } $$ Proof $$ \begin{align*} &amp;amp; ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})-{ (ax+by) }^{ 2 } \\ =&amp;amp; {a}^{2}{x}^{2}+{b}^{2}{x}^{2}+{a}^{2}{y}^{2}+{b}^{2}{y}^{2}-{ (ax+by) }^{ 2 } \\ =&amp;amp; {b}^{2}{x}^{2}+{a}^{2}{y}^{2}-2axby \\ =&amp;amp; { (ay-bx) }^{ 2 } \\ \ge&amp;amp; 0 \end{align*} $$ Thus, we can summarize as follows. $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\ge { (ax+by) }^{ 2 } $$ ■ Explanation This inequality, which can be encountered as early as high school, is used widely</description></item><item><title>Calculating the Probability of Poker Hands</title><link>https://freshrimpsushi.github.io/en/posts/48/</link><pubDate>Wed, 29 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/48/</guid><description>Definition of Suits and Ranks Before getting into probabilities, it is recommended to look up poker hands if you are not familiar with poker itself. Before calculating probabilities, let&amp;rsquo;s define two things: Suits: Element of the set {♠,◇,♤,♣} Ranks: Element of the set {A,2,3,4,5,6,7,8,9,10,J,Q,K} If two or more hands are satisfied at the same time, follow the higher one. The probabilities below</description></item><item><title>Addition Formula for Trigonometric Functions: Various Proofs</title><link>https://freshrimpsushi.github.io/en/posts/44/</link><pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/44/</guid><description>Theorem $$ \sin\left( \alpha +\beta \right) =\sin\alpha \cos\beta +\cos\alpha \sin\beta \\ \sin\left( \alpha -\beta \right) =\sin\alpha \cos\beta -\cos\alpha \sin\beta \\ \cos\left( \alpha +\beta \right) =\cos\alpha \cos\beta -\sin\alpha \sin\beta \\ \cos\left( \alpha -\beta \right) =\cos\alpha \cos\beta +\sin\alpha \sin\beta \\ \tan\left( \alpha +\beta \right) =\frac { \tan\alpha +\tan\beta }{ 1-\tan\alpha \tan\beta } \\ \tan\left( \alpha -\beta \right) =\frac { \tan\alpha -\tan\beta }{ 1+\tan\alpha \tan\beta } $$ Proof Proof using the Law of</description></item><item><title>Integration of the Natural Logarithm Raised to a Power</title><link>https://freshrimpsushi.github.io/en/posts/45/</link><pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/45/</guid><description>Formulas $$ \int {{(\ln x)}^{ n }} dx=x{{(\ln x)}^{ n }}-\int n{{(\ln x)}^{ n-1 }}dx $$ Explanation When solving integral problems, it&amp;rsquo;s not uncommon to encounter this type of problem. Solving these problems directly through integration by parts can be extremely time-consuming. First, let&amp;rsquo;s try to find a rule. Given $f(n)=\int {{(\ln x)}^{ n }} dx$ (where $n=1,2,3&amp;hellip;$), $$ \begin{align*} f(1) =&amp;amp; x(\ln|x|-1)+C \\ f(2) =&amp;amp; x{(\ln|x|)^{ 2 }-2\ln|x|+2}+C \\</description></item><item><title>Conditions for a function and its Taylor series to be equal</title><link>https://freshrimpsushi.github.io/en/posts/42/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/42/</guid><description>Theorem1 A function $f$ that is infinitely differentiable around point $a$, a necessary and sufficient condition for $\displaystyle f(x) = \sum_{n=0}^{\infty} {{f^{(n)} (a)}\over{n!}} {(x-a)}^n$ is that for some $\xi \in \mathscr{H} \left\{ x , a \right\}$ $$ \lim_{n \to \infty} {{f^{(n)} (\xi)}\over{n!}} {(x-a)}^n = 0 $$ where $\xi \in \mathscr{H} \left\{ x , a \right\}$ means that $\xi$ is in either $(x,a)$ or $(a,x)$. Explanation The Taylor theorem often represents</description></item><item><title>Fermat's Last Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/35/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/35/</guid><description>Theorem1 If the function $f(x)$ is either a maximum or a minimum at $x=c$ and $f ' (c)$ exists, then $f ' (c) = 0$ Explanation While high school textbooks generally only introduce Rolle&amp;rsquo;s Theorem up to Rolle&amp;rsquo;s Theorem, to rigorously prove Rolle&amp;rsquo;s Theorem, one must be able to show why the derivative at a critical point is $0$, and Fermat&amp;rsquo;s Theorem guarantees that. Proof Strategy: Divide the proof into</description></item><item><title>Odd Functions and Even Functions</title><link>https://freshrimpsushi.github.io/en/posts/40/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/40/</guid><description>Definitions A function $f(x)$ that satisfies $f(-x) = f(x)$ is called an Even function. A function $f(x)$ that satisfies $f(-x) = -f(x)$ is called an Odd function. Description Even functions are symmetric about the $y$ axis in the coordinate plane, while Odd functions are symmetric about the origin $O$. For example, among the trigonometric functions, $\sin$ is Odd and $\cos$ is Even. Differentiating $\sin$ yields $\cos$, and differentiating $\cos$ yields</description></item><item><title>Proof of Cauchy's Mean Value Theorem</title><link>https://freshrimpsushi.github.io/en/posts/38/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/38/</guid><description>Theorem1 Let&amp;rsquo;s say $a &amp;lt; b$. If the function $f,g : \mathbb{R} \to \mathbb{R}$ is continuous at all points of $[a,b]$, differentiable at all $x \in (a,b)$, and if $g ' (x) \ne 0$, then there exists at least one $c \in (a,b)$ that satisfies the following: $$ {{f ' (c)}\over{g ' (c)}}={{f(b)-f(a)}\over{g(b)-g(a)}} $$ Explanation If there&amp;rsquo;s any difference from the mean value theorem, it&amp;rsquo;s just that there&amp;rsquo;s one more</description></item><item><title>Proof of L'Hôpital's Rule</title><link>https://freshrimpsushi.github.io/en/posts/39/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/39/</guid><description>Theorem1 Given that $f(x)$ and $g(x)$ are differentiable near $x=a$ and that $g ' (x) \ne 0$ and $\displaystyle \lim _{x \to a} f(x) = \lim _{x \to a} g(x) = 0$, $$ \lim _{x \to a} {{f(x)} \over {g(x)}} = \lim _{x \to a} {{f ' (x)} \over {g ' (x)}} $$ Explanation Though this theorem may seem like a magic wand to many students, who have learned and</description></item><item><title>Proof of Rolle's Theorem in Calculus</title><link>https://freshrimpsushi.github.io/en/posts/36/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/36/</guid><description>Theorem1 If the function $f(x)$ is continuous at $[a,b]$ and differentiable at $(a,b)$ and if $f(a)=f(b)$, then there exists at least one $c$ in $(a,b)$ that satisfies $f ' (c)=0$. Description In high school courses, it is introduced only as an auxiliary lemma to prove the mean value theorem and is not used at all otherwise. However, beyond the high school level, it is sometimes used as an auxiliary lemma.</description></item><item><title>Proof of Taylor's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/41/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/41/</guid><description>Theorem1 If a function $f(x)$ is continuous at $[a,b]$ and differentiable up to $n$ times at $(a,b)$, then there exists $\xi \in (a,b)$ that satisfies $$ \begin{align*} f(b) =&amp;amp; \sum_{k=0}^{n-1} {{(b-a)^{k}\over{k!}}{f^{(k)}( a )}} + {(b-a)^{n}\over{n!}}{f^{(n)}(\xi)} \\ =&amp;amp; {f(a)} + {(b-a)f ' (a)} + \cdots + {(b-a)^{n-1}\over{(n-1)!}}{f^{(n-1)}(a)} + {(b-a)^{n}\over{(n)!}}{f^{(n)}(\xi)} \end{align*} $$ Explanation This theorem, which is widely used throughout mathematics, has lent its name to the Taylor series. In terms of</description></item><item><title>Proof of the Mean Value Theorem in Calculus</title><link>https://freshrimpsushi.github.io/en/posts/37/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/37/</guid><description>Theorem1 If the function $f(x)$ is continuous at $[a,b]$ and differentiable at $(a,b)$, then there exists at least one $c$ in $(a,b)$ that satisfies $\displaystyle f '(c)={{f(b)-f(a)}\over{b-a}}$. Description It&amp;rsquo;s not just commonly used; it&amp;rsquo;s so famous that it&amp;rsquo;s abbreviated as MVT. The term &amp;lsquo;mean value&amp;rsquo; comes from the idea that there is a point where the derivative equals the average rate of change over the entire interval. The concept of</description></item><item><title>Finding the Extremum of a Quadratic Function Quickly</title><link>https://freshrimpsushi.github.io/en/posts/30/</link><pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/30/</guid><description>Formula The vertex of the quadratic function $f(x)=c(x-a)(x-b)$ is $\frac { a+b }{ 2 }$ (where $c\neq 0$) For factorable quadratic functions, the vertex can be found without bothering with various calculations. It seems obvious, but knowing or not knowing this fact can make a difference in whether or not you can reduce one step in the calculation process. Derivation $$ \begin{align*} &amp;amp; f(x) = c(x-a)(x-b) = c x^2 -c(a+b)x+cab</description></item><item><title>Integration Techniques for Various Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/31/</link><pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/31/</guid><description>Overview When solving integration problems, you often have to integrate trigonometric functions. And, getting familiar with these integration methods, trigonometric functions can be integrated as quickly as polynomial functions. Integration of Secant Functions, Integration of Cosecant Functions $$ \begin{align*} \int \sec x dx =&amp;amp; \int \frac { \sec x (\sec x +\tan x ) }{ (\sec x +\tan x ) }dx \\ =&amp;amp; \int \frac { \sec^{ 2 }x+\sec x</description></item><item><title>Proof of Bayes' Theorem and Prior, Posterior Distributions</title><link>https://freshrimpsushi.github.io/en/posts/29/</link><pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/29/</guid><description>Theorem 1 Sample Space $S$ and Event $A$, Probability $P$ If $\left\{ S_1, S_2, \cdots ,S_n \right\}$ is a partition of $S$, then the following holds. $$ P(S_k|A)=\frac { P(S_k)P(A|S_k) }{ \sum _{ k=1 }^{ n }{ P(S_k)P(A|S_k) } } $$ Definition The right-hand side of Bayes&amp;rsquo; theorem, $P \left( S_{k} \right)$, is called the Prior Probability, and the left-hand side, $P \left( S_{k} | A \right)$, is called the</description></item><item><title>Proof that if Two Events are Mutually Exclusive, They are Dependent</title><link>https://freshrimpsushi.github.io/en/posts/27/</link><pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/27/</guid><description>Theorem For two events $A,B$, if $B=A^c$ then $P(A\cap B) \neq P(A)P(B)$ Explanation Even without a formal proof through equations, it is common sense that if events are mutually exclusive, they cannot be independent. If one event occurring means the other cannot, this already implies an influence. However, knowing or not knowing this makes a big difference when solving problems regarding true or false judgments. Proof Let&amp;rsquo;s say for two</description></item><item><title>Two Events Being Independent Proves that Their Complements Are Also Independent</title><link>https://freshrimpsushi.github.io/en/posts/28/</link><pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/28/</guid><description>Theorem The following are equivalent. $$ P(A \cap B) = P(A)P(B) \\ P(A \cap B^c)=P(A)P(B^c) \\ P(A^c \cap B)=P(A^c)P(B) \\ P(A^c \cap B^c)=P(A^c)P(B^c) $$ Explanation Not only is this fact highly beneficial to know, but it is also useful as a formula. Proof Let&amp;rsquo;s assume $P(A \cap B) = P(A)P(B)$. In other words, events $A$ and $B$ are independent. According to the property of the complement, $$ P(A)=1-P(A^{ c })</description></item><item><title>A Simpler Proof of the Divisibility Test for 11</title><link>https://freshrimpsushi.github.io/en/posts/23/</link><pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/23/</guid><description>Buildup In this post, we use the following notations for convenience regarding numerical bases. $$ [a_{n} a_{n-1} &amp;hellip; a_{1} a_{0}] := a_{n} \cdot 10^{n} + a_{n-1} \cdot 10^{n-1} +&amp;hellip;+ a_{1} \cdot 10^{1} + a_{0} \cdot 10^{0} $$ For example, $5714$ can be represented as follows. $$ \begin{align*} [5714] =&amp;amp; 5000+700+10+4 \\ =&amp;amp; 5\cdot 10^{3} +7\cdot 10^{2} +1\cdot 10^{1} +4\cdot 10^{0} \end{align*} $$ Theorem If $a_{n} - a_{n-1} + &amp;hellip; +</description></item><item><title>Division Test for 3 and Proof of the Division Test for 9</title><link>https://freshrimpsushi.github.io/en/posts/21/</link><pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/21/</guid><description>Theorem If the sum of all digits is a multiple of $3$, it is a multiple of $3$, and if it is a multiple of $9$, it is a multiple of $9$. Explanation For example, $8142$ is a multiple of $3$ by $8142=3 \cdot 2714$, and indeed $8+1+4+2=15$ is a multiple of $3$. $1945125$ is a multiple of $9$ by $1945125=9 \cdot 216125$, and indeed $1+9+4+5+1+2+5=27$ is a multiple of</description></item><item><title>The Number of Subsets of a Finite Set with n Elements</title><link>https://freshrimpsushi.github.io/en/posts/25/</link><pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/25/</guid><description>Formula Sum of Binomial Coefficients 1 The sum of binomial coefficients is as follows. $$ 2^{n} = \sum_{k=0}^{n} \binom{n}{k} $$ Corollary: Cardinality of the Power Set If the cardinality of a finite set $S$ is $n = |S|$, then the cardinality of its power set $2^{S}$ is $2^{n}$. Derivation Binomial Theorem: $$ (x+y)^{n} = \sum_{k=0}^{n} \binom{n}{k} x^{k} y^{n-k} $$ Substituting $x = y = 1$ gives $2^{n} = \sum_{k=0}^{n} \binom{n}{k}</description></item><item><title>The Proof of the Divisibility Test for 7 and 13</title><link>https://freshrimpsushi.github.io/en/posts/22/</link><pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/22/</guid><description>Buildup In this post, for convenience in discussing number systems, we use the following notation. $$ [a_{n} a_{n-1} &amp;hellip; a_{1} a_{0}] := a_{n} \cdot 10^{n} + a_{n-1} \cdot 10^{n-1} +&amp;hellip;+ a_{1} \cdot 10^{1} + a_{0} \cdot 10^{0} $$ For example, $5714$ can be represented as follows. $$ \begin{align*} [5714] =&amp;amp; 5000+700+10+4 \\ =&amp;amp; 5\cdot 10^{3} +7\cdot 10^{2} +1\cdot 10^{1} +4\cdot 10^{0} \end{align*} $$ Theorem $$ a_{n} a_{n-1} a_{n-2} - a_{n-3}</description></item><item><title>Derivation of the Formula for the Velocity of a Falling Object</title><link>https://freshrimpsushi.github.io/en/posts/13/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/13/</guid><description>Formulas $g$ Let&amp;rsquo;s denote gravity acceleration. If an object falls from a high place to a lower one due to gravity, the formula for the velocity of this object as a function of the fallen distance $h$ is as follows. $$ v=\sqrt { 2gh } $$ Derivation Let&amp;rsquo;s ignore any conditions that are irrelevant to what we&amp;rsquo;re trying to find. Assuming an object was at rest and then falls a</description></item><item><title>Derivation of the Formula to Calculate the Distance Between Two Parallel Lines</title><link>https://freshrimpsushi.github.io/en/posts/4/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/4/</guid><description>Formulas $$ d=\frac { |2k| }{ \sqrt { m^{ 2 }+1 } } $$ Explanation When solving problems involving the tangent to a conic section, one often needs to calculate the distance between two tangents. While it&amp;rsquo;s not particularly challenging, thanks to the formula for the distance from a given point to a line, having an easy and quick formula for this distance can help to reduce calculation time. Derivation</description></item><item><title>Ernestrom-Kakeya Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/5/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/5/</guid><description>Theorem 1 Let $\left\{ a_{i} \right\}_{i=0}^{n} \subset \mathbb{R}$ such that $a_0 &amp;gt; a_1 &amp;gt; \cdots &amp;gt; a_n &amp;gt; 0$. Then for the polynomial function $$ P(z) := a_0 + a_1 z + \cdots + a_{n-1} z^{n-1} + a_n z^n $$ all roots $z \in \mathbb{C}$ satisfy $|z| \ge 1$. Proof If there is a root of $P(z) = 0$ at $z=1$, then we have $\displaystyle 0 = P(1) = \sum_{i=0}^{n}</description></item><item><title>Euler's Proof of the Divergence of the Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/17/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/17/</guid><description>Theorem The harmonic series diverges. $$ \sum _{ n=1 }^{ \infty }{ \frac { 1 }{ n } }=\infty $$ Description At first glance, the harmonic series appears as if it would converge since its terms continue to decrease in value. However, Oresme elegantly and simply proved that it diverges. This fact is often used as an example to explain the concept of absolute convergence, where the alternating harmonic series</description></item><item><title>Finding the Average Speed Using the Harmonic Mean</title><link>https://freshrimpsushi.github.io/en/posts/14/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/14/</guid><description>Formula When traveling a distance $S$ at speed $a$ and returning at speed $b$, the average speed $v$ can be represented as the harmonic mean of the two speeds. $$ v = \frac { 2ab }{ a+b } $$</description></item><item><title>Frequently Used Definite Integrals of Quadratic Functions</title><link>https://freshrimpsushi.github.io/en/posts/15/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/15/</guid><description>Formula $$ \int _{ \alpha }^{ \beta }{ (x-\alpha )(x-\beta )dx }=-\frac { { (\beta -\alpha ) } ^ { 3 } }{ 6 } $$ Description As you solve problems, you often find yourself calculating definite integrals of this sort more than you&amp;rsquo;d expect. This formula is entirely useless aside from making solutions quicker, and its derivation is just calculation. Just memorize the form so you can use it</description></item><item><title>Gabi's Proof of Li</title><link>https://freshrimpsushi.github.io/en/posts/16/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/16/</guid><description>Theorem If $bdf(b+d)\neq 0$ then $$ \frac { a }{ b }=\frac { c }{ d }=\frac { e }{ f } \implies \frac { a+c }{ b+d }=\frac { e }{ f } $$ Description &amp;ldquo;Gabi&amp;rdquo; is nothing else but a word made from two Hanja characters: add 加 and compare 比. Here, the compare 比 is the same as the &amp;lsquo;ratio&amp;rsquo; in ratios, making it a theorem</description></item><item><title>Proof of De Moivre's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/9/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/9/</guid><description>Theorem If $z = r \text{cis} \theta$, then for all natural numbers $n$, $z^n = r^n \text{cis} n\theta$ holds. $\text{cis} \theta: = \cos \theta + i \sin \theta$ Proof Let&amp;rsquo;s use mathematical induction. For $n=1$, it is obvious, and assuming it holds for $n=k$, $$ z^{k+1} = z z^k = (r \text{cis} \theta)(r^k \text{cis} k\theta) $$ Meanwhile, since $z_1 z_2 = r_1 r_2 \text{cis} (\theta_1 + \theta_2)$, $$ z^{k+1} =</description></item><item><title>The Relationship between Areas Calculated by Riemann Sums and Definite Integrals</title><link>https://freshrimpsushi.github.io/en/posts/12/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/12/</guid><description>Formulas $$ \begin{align*} &amp;amp; \lim _{ n\to \infty }{ \sum _{ k=1 }^{ n }{ f\left( a+\frac { p }{ n }k \right) \frac { p }{ n } } } \\ =&amp;amp; \int _{ a }^{ a+p }{ f(x)dx } \\ =&amp;amp; \int _{ 0 }^{ p }{ f(a+x)dx } \\ =&amp;amp; \int _{ 0 }^{ 1 }{ pf(a+px)dx } \end{align*} $$ Explanation Sometimes, you&amp;rsquo;ll encounter integration problems that</description></item><item><title>The Relationship between the Translation of Trigonometric Functions and Their Derivatives</title><link>https://freshrimpsushi.github.io/en/posts/11/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/11/</guid><description>Formula [1] Sine: $$\sin{\left( \theta +\frac { n }{ 2 }\pi \right)}={ \sin }^{ (n) }\theta$$ [2] Cosine: $$\cos{\left( \theta +\frac { n }{ 2 }\pi \right)}={ \cos }^{ (n) }\theta$$ $(n)$ means differentiating $n$ times. Explanation Simply put, you differentiate once every time you move by 90˚. Let&amp;rsquo;s actually calculate for $n=3$. Method Using Addition Theorem $$ \begin{align*} \cos \left( \theta +{3 \over 2}\pi \right) =&amp;amp;</description></item><item><title>Arithmetic, Geometric, and Harmonic Means Inequality</title><link>https://freshrimpsushi.github.io/en/posts/3/</link><pubDate>Tue, 14 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3/</guid><description>Definitions For $n$ positive numbers ${x}_1,{x}_2,\cdots,{x}_n$, the arithmetic mean, geometric mean, and harmonic mean are defined as: Arithmetic Mean : $$ \sum_{ k=1 }^{ n }{ \frac { {x}_k }{ n } }=\frac { {x}_1+{x}_2+\cdots+{x}_n }{ n } $$ Geometric Mean : $$ \prod_{ k=1 }^{ n }{ { {x}_k }^{ \frac { 1 }{ n } } }=\sqrt [ n ]{ {x}_1{x}_2\cdots{x}_n } $$ Harmonic Mean : $$ \left(</description></item><item><title>Search Results</title><link>https://freshrimpsushi.github.io/en/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/search/</guid><description>This file exists solely to respond to /search URL with the related search layout template.
No content shown here is rendered, all content is based in the template layouts/page/search.html
Setting a very low sitemap priority will tell search engines this is not important content.
This implementation uses Fusejs, jquery and mark.js
Initial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [&amp;ldquo;HTML&amp;rdquo;, &amp;ldquo;JSON&amp;rdquo;] ```</description></item></channel></rss>