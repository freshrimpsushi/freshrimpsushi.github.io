<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Analysis on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%ED%95%B4%EC%84%9D%EA%B0%9C%EB%A1%A0/</link><description>Recent content in Analysis on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 04 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%95%B4%EC%84%9D%EA%B0%9C%EB%A1%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>Taylor's Theorem Rest Term</title><link>https://freshrimpsushi.github.io/en/posts/3532/</link><pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3532/</guid><description>Definition1 2 For a differentiable function $f$, the $P_{k}$ defined below is called the Taylor polynomial of $f$ at point $a$. $$ P_{k} (x) := f(a) + f^{\prime}(a) (x-a) + \dfrac{f^{\prime \prime}(a)}{2!}(x-a)^{2} + \cdots + \dfrac{f^{(k)}(a)}{k!}(x-a)^{k} $$ The difference between $f$ and $P_{k}$ is called the remainder term. $$ R_{k}(x) = f(x) - P_{k}(x) $$ Explanation $$ f(x) = P_{k}(x) + R_{k}(x) = \sum \limits_{n=0}^{k} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} + R_{k}(x) $$</description></item><item><title>Line Integrals of Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/3113/</link><pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3113/</guid><description>Definition1 Let a vector field $\mathbf{F} : \mathbb{R}^{3} \to \mathbb{R}^{3}$ and a curve $C$ in 3-dimensional space be given as $\mathbf{r}(t)$. Let $\mathbf{T}$ be called the tangent field of the vector field. Then, the $\mathbf{F}$ line integral along the curve $C$ is defined as follows. $$ \int_{C} \mathbf{F} \cdot d \mathbf{r} = \int_{a}^{b} \mathbf{F}\left( \mathbf{r}(t) \right) \cdot \mathbf{r}^{\prime}(t) dt = \int_{C} \mathbf{F} \cdot \mathbf{T} ds $$ Explanation The buildup to</description></item><item><title>Scalar Field Line Integral</title><link>https://freshrimpsushi.github.io/en/posts/3112/</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3112/</guid><description>Line Integral over a Plane Curve1 Buildup Given a function as in $y = f(x)$, its definite integral is defined by the idea of adding up all the function values $f(x)$ along the $x$ axis. Thus, the integral value is obtained along a straight line on the $x$ axis. Now, consider a two-variable function $z=f(x,y)$. Unlike in the case of single-variable functions, since the variable moves over the $xy-$ plane,</description></item><item><title>Length of a Curve</title><link>https://freshrimpsushi.github.io/en/posts/3111/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3111/</guid><description>Length of a Plane Curve1 Buildup Suppose we have a smooth function $y=f(x)$ given as in figure (a) above, with $n+1$ points on it. The total length $s$ of the curve can be obtained by summing up the lengths $s_{k}$ of each arc divided by points. Moreover, the length of each arc can be approximated by the length between two points as shown in figure (b). As the number of</description></item><item><title>매끄러운 함수의 정의</title><link>https://freshrimpsushi.github.io/en/posts/3110/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3110/</guid><description>Definition If a function $f$ is infinitely differentiable, then $f$ is called a smooth function. If a function $f$ is differentiable and $f^{\prime}$ is continuous, then $f$ is called a smooth function. Explanation In analysis and functional analysis, the term smooth likely refers to the first definition. The phrase &amp;lsquo;infinitely differentiable&amp;rsquo; may seem ambiguous, but it can be understood as follows: $$ \text{For any natural number $n$, the $n$th derivative</description></item><item><title>Definition of Improper Integrals</title><link>https://freshrimpsushi.github.io/en/posts/3094/</link><pubDate>Sat, 07 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3094/</guid><description>Definition1 Assume that the function $f$ is integrable over every interval $[a,b]$ with fixed $a$ and $b&amp;gt;a$. If the following limit exists, then it is defined as the improper integral of $f$. $$ \int _{a}^{\infty} f(x) dx = \lim \limits_{b \to \infty} \int _{a}^{b} f(x)dx $$ In this case, if the integration on the left-hand side converges, and if replacing $f$ with $\left| f \right|$ the limit still exists, it</description></item><item><title>Uniform Convergence Preserves Continuity</title><link>https://freshrimpsushi.github.io/en/posts/1893/</link><pubDate>Sun, 04 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1893/</guid><description>Theorem1 Let&amp;rsquo;s consider a sequence of functions $\left\{ f_{n} \right\}$ on a metric space $E$ that uniformly converges to $f$. $$ f_{n} \rightrightarrows f $$ If each $f_{n}$ is continuous at $x \in E$, then $f$ is also continuous at $x$. That is, the following holds. $$ \lim \limits_{t \to x }f_{n}(t)=f_{n}(x) \implies \lim \limits_{t \to x }f(t)=f(x) $$ Or $$ \lim \limits_{t\to x}\lim \limits_{n\to \infty}f_{n}(t)=\lim \limits_{n\to \infty}\lim \limits_{t\to x}f_{n}(t)</description></item><item><title>Properties of Divergent Series of Real Numbers</title><link>https://freshrimpsushi.github.io/en/posts/3052/</link><pubDate>Sat, 15 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3052/</guid><description>Theorem1 Let $\left\{ x_{n} \right\}$ and $\left\{ y_{n} \right\}$ be sequences of real numbers and define $\lim \limits_{n\to\infty} x_{n}=\infty(-\infty)$. Then the following hold: (a) If $\left\{ y_{n} \right\}$ is bounded below (above), then $\lim \limits_{n\to\infty}(x_{n}+y_{n}) = \infty(-\infty)$. (b) $\forall \alpha &amp;gt; 0,\quad \lim \limits_{n\to\infty} \alpha x_{n} = \infty (-\infty)$. (c) If for all $n\in \mathbb{N}$, there exists $M_{0} &amp;gt;0$ such that $y_{n} &amp;gt; M_{0}$, then $\lim \limits_{n\to\infty} x_{n}y_{n} = \infty(-\infty)$.</description></item><item><title>Monotone Sequence, Monotone Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3051/</link><pubDate>Sun, 09 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3051/</guid><description>Definition1 For a sequence of real numbers $\left\{ s_{n} \right\}$, It is called monotonically increasing if $s_{n} \le s_{n+1}$ holds. It is called monotonically decreasing if $s_{n} \ge s_{n+1}$ holds. Sequences that are either increasing or decreasing are referred to as monotonic. Explanation Since a sequence is defined as a function with the natural numbers as its domain, saying that a sequence is increasing means the same thing as saying</description></item><item><title>The Definition of Euler's Constant, the Natural Number e</title><link>https://freshrimpsushi.github.io/en/posts/3050/</link><pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3050/</guid><description>Definition1 The limit of the series below is defined as constant $e$. $$ e: = \sum \limits_{n=0}^{\infty} \dfrac{1}{n!} $$ Explanation Even if we do not know what that value is right away, it can be easily shown that the series mentioned above converges to some limit. The partial sum $s_{n}$, as it is bounded and increasing, converges. $$ \begin{align*} s_{n} &amp;amp;= 1 + 1 + \dfrac{1}{2} + \dfrac{1}{2 \cdot 3}</description></item><item><title>Necessary and Sufficient Conditions for Uniform Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1891/</link><pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1891/</guid><description>Theorem 1 Let us assume a sequence of functions $\left\{ f_{n} \right\}$ is given as defined in $E\subset \mathbb{R}$. The following two conditions are equivalent: $\left\{ f_{n} \right\}$ converges uniformly on $E$. For all $\varepsilon&amp;gt;0$, there exists a natural number $N$ that satisfies the following equation. $$ \begin{equation} \quad m,n\ge N,\ x\in E \implies \left| f_{n}(x)-f_{m}(x) \right| \le \varepsilon \end{equation} $$ In other words, for all $x \in E$, the</description></item><item><title>If the Derivative of a Curve is Continuous, the Curve Can Be Measured</title><link>https://freshrimpsushi.github.io/en/posts/1870/</link><pubDate>Thu, 12 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1870/</guid><description>Theorem1 If $\gamma ^{\prime}$ is continuous on the interval $[a,b]$, then $\gamma$ forms a rectifiable curve, and the following equation holds: $$ \Lambda (\gamma) = \int _{a} ^{b} \left| \gamma^{\prime}(t) \right| dt $$ Proof Part 1. Let $P=\left\{ a=x_{0},\dots,x_{n}=b \right\}$ be any partition of the interval $[a,b]$. If we state $a\le x_{i-1}&amp;lt;x_{i}\le b$, then the following is true: $$ \begin{align*} \left| \gamma (x_{i})-\gamma (x_{i-1}) \right| &amp;amp;= \left| \int_{x_{i-1}}^{x_{i}}\gamma^{\prime} (t)dt \right|</description></item><item><title>Measuring Curves: A Guide to Length</title><link>https://freshrimpsushi.github.io/en/posts/1869/</link><pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1869/</guid><description>Definition1 A curve $\gamma : [a,b] \to \mathbb{R}^{k}$ on $\mathbb{R}^{k}$ or simply on $[a,b]$ is called a continuous function. If the curve $\gamma$ is a one-to-one function, it is called an arc. If $\gamma (a)=\gamma (b)$, then $\gamma$ is called a closed curve. Explanation The important point is that the curve is defined as a mapping, not as a collection of points. Now let&amp;rsquo;s define $\Lambda$ for the partition $P=\left\{</description></item><item><title>Integration by Parts</title><link>https://freshrimpsushi.github.io/en/posts/1867/</link><pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1867/</guid><description>Theorem 1 Assuming $F$, $G$ are differentiable in the interval $[a,b]$, and $F^{\prime}=f$, $G^{\prime}=g$ are integrable. Then, the following equation holds: $$ \begin{align*} \int _{a} ^{b} F(x)g(x)dx &amp;amp;= F(b)G(b)-F(a)G(a)-\int _{a} ^{b}f(x)G(x)dx \\ &amp;amp;= \left[ F(x)G(x) \right]_{a}^{b} -\int _{a} ^{b}f(x)G(x)dx \end{align*} $$ Description This result is called the integration by parts. Memorizing it as Integration-Differential-Integration makes it easy. What to integrate is kept on both sides as is, and what to</description></item><item><title>The Fundamental Theorem of Calculus in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1866/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1866/</guid><description>Theorem1 Given that function $f$ is Riemann integrable on the interval $[a,b]$, and there exists a function $F$ that is differentiable on $[a,b]$, satisfying $F^{\prime}=f$. Then, the following holds true. $$ \int_{a}^{b} f(x) dx= F(b)-F(a) $$ Explanation This theorem is famously known as the Fundamental Theorem of Calculus Part 2, often abbreviated as FTC2[^Funcamental Theorem of Calculus1]. It implies that the definite integral of $f$ is represented by the difference</description></item><item><title>Classification of Discontinuities</title><link>https://freshrimpsushi.github.io/en/posts/1833/</link><pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1833/</guid><description>Definition1 Let&amp;rsquo;s assume the function $f :X \to \mathbb{R}$ is given in a metric space $X$. If $f$ is not continuous at $x\in X$, it is said that $f$ is discontinuous at $x$ or has a discontinuity at $x$. Let&amp;rsquo;s say $f: (a,b) \to \mathbb{R}$. If $f$ is discontinuous at $x\in (a,b)$ and the left/right limits $f(x-)$, $f(x+)$ at $x$ exist, it is said that $f$ has a discontinuity of</description></item><item><title>Limits from the Left and the Right Strictly Defined in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1830/</link><pubDate>Sat, 17 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1830/</guid><description>Definition Let&amp;rsquo;s assume a function $f :X \to \mathbb{R}$ is given in a metric space $X$. If $f$ is not continuous at $x\in X$, $f$ is said to be discontinuous at $x$ or to have a discontinuity at $x$. $f: (a,b) \to \mathbb{R}$ is assumed. For any point $x$, let $a \le x &amp;lt;b$. Consider a sequence of points $(x,b)$ that converges to $x$ and call it $\left\{ t_{n} \right\}$.</description></item><item><title>The Relationship between Derivatives and the Increasing/Decreasing of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1826/</link><pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1826/</guid><description>정리 Let the function $f$ be differentiable at $(a,b)$. If for all $x\in (a,b)$, $f^{\prime}(x) \ge 0$ holds, then $f$ is monotonically increasing. If for all $x\in (a,b)$, $f^{\prime}(x)=0$ holds, then $f$ is a constant function. If for all $x\in (a,b)$, $f^{\prime}(x) \le 0$ holds, then $f$ is monotonically decreasing. Proof From the Mean Value Theorem, it follows that for all $x_{1},x_{2}\in (a,b)$ and $x \in (x_{1},x_{2})$ the following</description></item><item><title>Mean Value Theorem in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1824/</link><pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1824/</guid><description>Theorem Let the functions $f$ and $g$ be continuous on the interval $[a,b]$ and differentiable on $(a,b)$. Then there exists $x \in (a,b)$ that satisfies the following equation. $$ [f(b)-f(a)]g^{\prime}(x)=[g(b)-g(a)]f^{\prime}(x) $$ Note that differentiability is not necessary at the endpoints $a$ and $b$. Explanation This is a generalization of the Mean Value Theorem learned in high school and in calculus. If we set it as $g(x)=x$, it becomes the familiar</description></item><item><title>Definition and Relationship of Extremum in Analysis and Differential Coefficients</title><link>https://freshrimpsushi.github.io/en/posts/1699/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1699/</guid><description>Definition Let $(X,d)$ be a metric space. If there exists a positive real number $\delta &amp;gt;0$ such that the function $f : X \rightarrow \mathbb{R}$ satisfies the condition below, then $f$ has a local maximum at point $p \in X$. $$ \forall q\in X,\quad f(q)\le f(p)\ \mathrm{with}\ d(p,q)&amp;lt;\delta $$ Explanation To put it in words: If $f(p)$ is the largest within a distance of $\delta$ from $p$, then $f(p)$ is</description></item><item><title>The Chain Rule of Differentiation in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1823/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1823/</guid><description>Theorem1 If $f :[a,b] \to \mathbb{R}$ is a continuous function and is differentiable at $x\in [a,b]$, and if $g : f([a,b])\to \mathbb{R}$ is differentiable at $f (x)\in f([a,b])$, and if we define $h : [a,b] \to \mathbb{R}$ as follows. $$ h(t)=g\left( f(t) \right)\quad (a\le t \le b) $$ Then, $h$ is differentiable at $x$ and its value is as follows. $$ h^{\prime}(x)=g^{\prime}(f(x))f^{\prime}(x) $$ Using the composite function symbol, it can</description></item><item><title>Differentiable Function Properties</title><link>https://freshrimpsushi.github.io/en/posts/1821/</link><pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1821/</guid><description>Theorem1 Let&amp;rsquo;s say $f, g : [a,b] \to \mathbb{R}$. If $f,g$ is differentiable at $x\in [a,b]$, then $f+g$, $fg$, and $f/g$ are also differentiable at $x$ and the following equation holds. $$ \begin{align} (f+g)^{\prime}(x) &amp;amp;=f^{\prime}(x)+g^{\prime}(x) \\ (fg)^{\prime}(x) &amp;amp;= f^{\prime}(x)g(x)+f(x)g^{\prime}(x) \\ \left( \frac{f}{g} \right)^{\prime}(x) &amp;amp;= \frac{f^{\prime}(x)g(x)-f(x)g^{\prime}(x)}{g^{2}(x)} \end{align} $$ However, $(3)$ holds when $g(x)\ne 0$. Description $(2)$ is commonly referred to as the product rule of differentiation. Proof $(1)$ By the definition</description></item><item><title>If Differentiable, Then Continuous</title><link>https://freshrimpsushi.github.io/en/posts/1820/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1820/</guid><description>Theorem1 Let&amp;rsquo;s say $f : [a,b] \to \mathbb{R}$. If $f$ is differentiable at $p \in [a,b]$, then $f$ is continuous at $p$. Explanation Note that the converse &amp;lsquo;if it is continuous, it is differentiable&amp;rsquo; does not hold. In the past, there was a pun called Simple Integration (simply put, if it&amp;rsquo;s differentiable, then it&amp;rsquo;s continuous) among older students, but I wonder if this pun has become unused as current students</description></item><item><title>Properties of Converging Real Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1714/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1714/</guid><description>Theorem 1[^1] Let $\left\{ s_{n} \right\}$, $\left\{ t_{n} \right\}$ be sequences of real (or complex) numbers and assume that $\lim \limits_{n\to\infty} s_{n}=s$, $\lim\limits_{n\to\infty}t_{n}=t$. Then (a) $\lim \limits_{n\to\infty}(s_{n}+t_{n})=s+t$ (b) $\forall c \in \mathbb{C},\quad\lim \limits_{n\to\infty} cs_{n}=cs \quad \text{and} \quad \lim \limits_{n\to\infty} (c+s_{n})=c+s$ (c) $\lim \limits_{n\to\infty} s_{n}t_{n}=st$ (d) $\forall s_{n}\ne 0,s\ne0,\quad \lim \limits_{n\to\infty}\frac{1}{s_{n}}=\frac{1}{s}$ Of course, this can be extended to $\mathbb{R}^{k}$ as well. See Theorem 2 for a deeper look. Proof (a) Given</description></item><item><title>Integrable Functions and Absolute Values</title><link>https://freshrimpsushi.github.io/en/posts/1697/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1697/</guid><description>This article is based on Riemann-Stieltjes integration. If set as $\alpha=\alpha (x)=x$, it equals the Riemann integration. Theorem1 Let function $f$ be Riemann(-Stieltjes) integrable over the interval $[a,b]$. Then (a) $\left|f\right|$ is also integrable over $[a,b]$. (b) Furthermore, the following inequality holds: $$ \left|\int_{a}^{b}fd\alpha \right| \le \int_{a}^{b}\left| f\right| d\alpha $$ Proof (a) Integrability is defined for bounded functions. Hence, assuming that $f$ is integrable implies that $f$ is bounded. Let&amp;rsquo;s</description></item><item><title>Riemann-Stieltjes Integrability is Preserved within an Interval</title><link>https://freshrimpsushi.github.io/en/posts/1695/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1695/</guid><description>The following document is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem 1 Let function $f$ be Riemann(-Stieltjes) integrable on $[a,b]$. Let us also say $a&amp;lt;c&amp;lt;b$. Then, $f$ is also integrable on $[a,c]$ and $[c,b]$, and the sum of the integration values is equal to the integral on $[a,b]$. $$ \int_{a}^{c}fd\alpha + \int_{c}^{b}fd\alpha=\int_{a}^{b}f d\alpha $$ Proof In the first</description></item><item><title>The Fundamental Theorem of Calculus in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1698/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1698/</guid><description>Theorem1 Let&amp;rsquo;s say $f$ is a function that is Riemann integrable over the interval $[a,b]$. And let&amp;rsquo;s define $F$ for $a\le x \le b$ as follows. $$ F(x) = \int _{a} ^{x} f(t)dt $$ (a) Then, $F$ is continuous over $[a,b]$. (b) If $f$ is continuous over $x_{0}\in [a,b]$, then $F$ is differentiable over $x_{0}$ and satisfies $F^{\prime}(x_{0})=f(x_{0})$. Explanation This is known by the name Fundamental Theorem of Calculus 1,</description></item><item><title>The Relationship Between the Size of Integrals Based on the Order of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1696/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1696/</guid><description>This article is based on Riemann-Stieltjes integration. If we set it as $\alpha=\alpha (x)=x$, it is the same as Riemann integration. Theorem1 Let us assume that two functions $f_{1}, f_{2}$ are Riemann(-Stieltjes) integrable over the interval $[a,b]$. Also, assume that $f_{1} \le f_{2}$ in $[a,b]$. Then, the following inequality holds. $$ \int_{a}^{b}f_{1}d\alpha \le \int_{a}^{b}f_{2}d\alpha $$ Proof Let there be a positive $\varepsilon &amp;gt;0$. Since $f_{2}$ is integrable, by the necessary</description></item><item><title>Linearity of Riemann(-Stieltjes) Iintegral</title><link>https://freshrimpsushi.github.io/en/posts/1666/</link><pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1666/</guid><description>Theorem1 This article is based on the Riemann-Stieltjes integral. If set to $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Let&amp;rsquo;s say $f$ is integrable by Riemann(-Stieltjes) from $[a,b]$. Then, for a constant $c\in \mathbb{R}$, $cf$ is also integrable from $[a,b]$, and its value is as follows. $$ \int_{a}^{b}cf d\alpha = c\int_{a}^{b}f d\alpha $$ Let two functions $f_{1}$, $f_{2}$ be integrable by Riemann(-Stieltjes) from $[a,b]$. Then, $f_{1}+f_{2}$ is</description></item><item><title>Proof of Darboux's Intermediate Value Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1554/</link><pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1554/</guid><description>정리 If a function $f : [a,b] \to \mathbb{R}$ is differentiable at $[a,b]$, there exists a $c \in (a,b)$ such that $y_{0} = f ' (c)$ is satisfied between $f ' (a)$ and $f ' (b)$ for some $y_{0}$.</description></item><item><title>Leibniz Integral Rule</title><link>https://freshrimpsushi.github.io/en/posts/1475/</link><pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1475/</guid><description>Theorem Let&amp;rsquo;s assume that $f(x,t)$ and $\dfrac{\partial f}{\partial x}(x,t)$ are consecutive. Then, the following equation holds. $$ \frac{d}{dx} \int_{a}^b f(x,t)dt = \int_{a}^b\frac{\partial f}{\partial x}(x,t)dt $$ Description Being able to interchange the order of differentiation and integration is undoubtedly useful. Besides, there are many theorems or formulas related to differentiation and integration named after Leibniz. Proof Since if continuous, then integrable, let&amp;rsquo;s assume $u$ as follows. $$ u(x):=\int_{a}^b f(x,t)dt $$ Then,</description></item><item><title>Extended Real Number System</title><link>https://freshrimpsushi.github.io/en/posts/1252/</link><pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1252/</guid><description>Definition The set defined as follows is called the extended real number system. $$ \overline{ \mathbb{R} } := \mathbb{R} \cup \left\{ -\infty, +\infty\right\} $$ Explanation In fields such as analysis, for convenience, the set $\mathbb{R}$ is often replaced with $\overline{ \mathbb{R} }$. $\pm \infty$ is not a number, but for convenience, it is treated as one and added to $\mathbb{R}$. Within the extended real number system, the rules for comparison</description></item><item><title>Differentiation of Functions Defined in Real Number Space</title><link>https://freshrimpsushi.github.io/en/posts/1210/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1210/</guid><description>Definition1 If in some $E$ containing $a$, $f$ is defined and the limit $$ f^{\prime} (a) := \lim_{h \to 0} {{ f (a + h ) - f(a) } \over { h }}=\lim \limits_{x\rightarrow a}\frac{f(x)-f(a)}{x-a} $$ exists, then $f$ is said to be differentiable at $a$, and $f^{\prime} (a)$ is called the derivative of $f$ at $a$. If $f$ is differentiable at every point $a \in E$, then $f$ is</description></item><item><title>Uniform Continuity of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1207/</link><pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1207/</guid><description>Definition1 Let us assume $E \subset \mathbb{R}$ is a non-empty set and define $f : E \to \mathbb{R}$. If for every $\varepsilon &amp;gt; 0$, $$ | x_{1} - x_{2} | &amp;lt; \delta \land x_{1} , x_{2} \in E \implies | f(x_{1}) - f(x_{2}) | &amp;lt; \varepsilon $$ there exists a $\delta&amp;gt;0$ satisfying the above equation, then $f$ is said to be uniformly continuous on $E$. $\land$ represents the logical &amp;lsquo;and&amp;rsquo;</description></item><item><title>Newly Defined Continuous Functions in University Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1206/</link><pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1206/</guid><description>Definition Let&amp;rsquo;s say a set that is not an empty set is called $E \subset \mathbb{R}$, and $f : E \to \mathbb{R}$. If there exists $\delta&amp;gt;0$ for every $\varepsilon &amp;gt; 0$ such that $$ | x - a | &amp;lt; \delta \implies | f(x) - f(a) | &amp;lt; \varepsilon $$ is satisfied, $f$ is said to be continuous at $a \in E$, and if it is continuous at every point</description></item><item><title>Epsilon-Delta Argument</title><link>https://freshrimpsushi.github.io/en/posts/1204/</link><pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1204/</guid><description>Definition1 Let $I$ be an interval containing $a \in \mathbb{R}$, and suppose that $f$ is a function defined at $I \setminus \left\{ a \right\}$. If for every $\epsilon &amp;gt; 0$, there exists a $\delta&amp;gt;0$ such that $$ 0 &amp;lt; | x - a | &amp;lt; \delta \implies | f(x) - L | &amp;lt; \varepsilon $$ is satisfied, then we say that $f(x)$ converges to $L \in \mathbb{R}$ as $x \to</description></item><item><title>Limits Supremum and Limits Infimum</title><link>https://freshrimpsushi.github.io/en/posts/1198/</link><pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1198/</guid><description>Definition Let $\left\{ x_{n} \right\}_{n \in \mathbb{N}}$, $\left\{ y_{n} \right\}_{n \in \mathbb{N}}$ be sequences of real numbers. $\displaystyle \limsup_{n \to \infty} x_{n} := \lim_{n \to \infty} \left( \sup_{k \ge n} x_{k} \right)$ is called the limit supremum of $\left\{ x_{n} \right\}$. $\displaystyle \liminf_{n \to \infty} y_{n} := \lim_{n \to \infty} \left( \inf_{k \ge n} y_{k} \right)$ is called the limit infimum of $\left\{ y_{n} \right\}$. Where $\displaystyle \sup_{k \ge n}</description></item><item><title>Cauchy Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1190/</link><pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1190/</guid><description>Definition For all $\varepsilon &amp;gt; 0$, if there exists $N \in \mathbb{N}$ such that $n , m \ge N \implies | x_{n} - x_{m} | &amp;lt; \varepsilon$ is satisfied, then the sequence $\left\{ x_{n} \right\}_{n \in \mathbb{N}}$ is said to be Cauchy. Theorem It is equivalent for a sequence to be Cauchy and to converge. Explanation Considering that there are hardly any important sequences that diverge, it can be surmised</description></item><item><title>Bolzano-Weierstrass Theorem</title><link>https://freshrimpsushi.github.io/en/posts/380/</link><pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/380/</guid><description>Theorem If a given infinite set $E \subset \mathbb{R}$ is bounded, then there is an accumulation point $p \in \mathbb{R}$ of $E$. Explanation Or one could also say, &amp;lsquo;A bounded sequence has a converging subsequence.&amp;rsquo; It&amp;rsquo;s important to note that $E$ does not need to be closed under the condition. Proof Part 1. $\displaystyle \bigcap_{n=1}^{\infty} I_{n} = \left\{ x \right\}$ Since $E$ is bounded by assumption, there exists a closed</description></item><item><title>Cantor's Intersection Theorem</title><link>https://freshrimpsushi.github.io/en/posts/376/</link><pubDate>Mon, 22 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/376/</guid><description>Definition1 A sequence $\left\{ S_{n} \right\}_{n=1}^{\infty}$ of a set is said to be nested if for every natural number $n$, $S_{n+1} \subset S_{n}$ holds. Explanation The translation of nested might not be smooth, but since there is no better alternative, it is recommended to just memorize it as &amp;ldquo;Nested.&amp;rdquo; Theorem For the nested interval $[a_{n}, b_{n}]$, the following holds: (a) $\displaystyle \bigcap_{n=1}^{\infty} [a_{n}, b_{n}] \ne \emptyset$ (b) Specifically, if $\displaystyle</description></item><item><title>The Reason for Intricately Defining the Convergence of Sequences in University Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1186/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1186/</guid><description>Definition Let $\left\{ x_{n } \right\}_{n = 1}^{\infty}$ be a sequence of real numbers. If for every $\varepsilon &amp;gt; 0$, there exists $N \in \mathbb{N}$ such that $n \ge N \implies | x_{n} - a | &amp;lt; \varepsilon$ is satisfied, then we say that $\left\{ x_{n } \right\}$ converges to $a \in \mathbb{R}$. $$ \lim_{n \to \infty} x_{n} = a \iff \forall \varepsilon &amp;gt; 0 , \exists N \in \mathbb{N}</description></item><item><title>Redefining the Limits of Sequences in University Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1184/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1184/</guid><description>Definitions1 2 $\mathbb{N}$ represents the set of natural numbers, and $\mathbb{R}$ represents the set of real numbers. A function with a domain of $\mathbb{N}$ is called a sequence. For a sequence of natural numbers $\left\{ n_{k} \right\}_{ k \in \mathbb{N}}$, $\left\{ x_{n_{k}} \right\}_{ k \in \mathbb{N}}$ is called a subsequence of $\left\{ x_{n} \right\}_{ n \in \mathbb{N}}$. If for every $x \in \left\{ x_{n} \right\}_{ n \in \mathbb{N}}$ there exists</description></item><item><title>Continuous but Not Differentiable Functions: Weierstrass Function</title><link>https://freshrimpsushi.github.io/en/posts/1169/</link><pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1169/</guid><description>Theorem There exists a continuous function that cannot be differentiated anywhere. Proof Strategy: Consider continuous functions $g_{1} (x) := | x - 1 |$ and $g_{2} (x) := | x - 2 |$. $g_{1}$ is not differentiable at $x=1$, and $g_{2}$ is not differentiable at $x=2$. $(g_{1} + g_{2})$ is not differentiable at both points $x = 1$ and $x = 2$. In this way, if we construct $\displaystyle G:</description></item><item><title>Functions of Series</title><link>https://freshrimpsushi.github.io/en/posts/1160/</link><pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1160/</guid><description>Definitions Let&amp;rsquo;s define the series of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$. (1) If $\displaystyle \sum_{k=1}^{n} f_{k} (X)$ when $n \to \infty$, then the series $\displaystyle \sum_{k=1}^{ \infty } f_{k}$ is said to converge pointwise in $E$ if it converges pointwise. (2) If $\displaystyle \sum_{k=1}^{n} f_{k} (X)$ when $n \to \infty$, then the series $\displaystyle \sum_{k=1}^{ \infty } f_{k}$ is said to converge uniformly in $E$ if it</description></item><item><title>The Difference between Pointwise Convergence and Uniform Convergence of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1158/</link><pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1158/</guid><description>Let&amp;rsquo;s define the function $f : E \to \mathbb{R}$ and the sequence of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$ for the subset $E \ne \emptyset$ of $\mathbb{R}$. Pointwise Convergence It is said that $f_{n}$ converges pointwise to $f$ in $E$ if, for every $\varepsilon &amp;gt; 0$ and $x \in E$, there exists $N \in \mathbb{N}$ satisfying $n \ge N \implies | f_{n} (x) - f(x) | &amp;lt; \varepsilon$,</description></item><item><title>The Accumulation Point in the Set of Real Numbers</title><link>https://freshrimpsushi.github.io/en/posts/379/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/379/</guid><description>Definition Given a point $x \in \mathbb{R}$ on the real line and a subset $A \subset \mathbb{R}$, if for any open set $O$ containing $x$, $ O \cap ( A \setminus \left\{ x \right\} ) \ne \emptyset $ holds, then $x$ is defined as a Limit Point. The set of limit points of $A$ is called the Derived set of $A$, and is denoted by $a '$. Explanation In the</description></item><item><title>The set of real numbers and the empty set are both open and closed.</title><link>https://freshrimpsushi.github.io/en/posts/378/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/378/</guid><description>Theorem $\mathbb{R}$ and $\emptyset$ are both open and closed. Description On the set of real numbers $\mathbb{R}$, the union of multiple intervals is called an open set. For example, $(-1,0) \cup (2,3)$ is obviously an open set, and so are $(0,1)$ and $\mathbb{R}$. Meanwhile, being closed is defined through being open. For any subset of real numbers $C$, if $R \setminus C$ is open, then $C$ is called a closed</description></item><item><title>Uniform Convergence of Function Series</title><link>https://freshrimpsushi.github.io/en/posts/1154/</link><pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1154/</guid><description>Definition Let&amp;rsquo;s define a subset $E \ne \emptyset$ of $\mathbb{R}$, function $f : E \to \mathbb{R}$, and sequence of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$. If there exists $N \in \mathbb{N}$ for every $\varepsilon &amp;gt; 0$ satisfying $n \ge N \implies | f_{n} (x) - f(x) | &amp;lt; \varepsilon$, then sequence $f_{n}$ converges uniformly to $f$ in $E$, denoted by: $$ f_n \rightrightarrows f $$ or $$ f_{n}</description></item><item><title>Pointwise Convergence of Function Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1148/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1148/</guid><description>Definition Let us define a function $f : E \to \mathbb{R}$ for the subset $E \ne \emptyset$ of $\mathbb{R}$. If the sequence of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$ satisfies $f(x) = \lim \limits_{n \to \infty} f_{n} (X)$ for each $x \in E$, then it is said to converge pointwise to $f_{n}$ in $E$, denoted by: $$ f_{n} \to f $$ Explanation Rewriting the above definition using the</description></item><item><title>Functions That Cannot Be Integrated over a Closed Interval: The Dirichlet Function</title><link>https://freshrimpsushi.github.io/en/posts/1146/</link><pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1146/</guid><description>Definition The Dirichlet function is defined as follows: $f$ Explanation The Dirichlet function is a classic example of a function that can&amp;rsquo;t be integrated using Riemann integration. Unless one advances in the study beyond analysis, it&amp;rsquo;s unlikely to even imagine such a peculiar example. The specific mention that it can&amp;rsquo;t be integrated using Riemann integration is because it may be integrable by methods other than Riemann integration. Theorem The Dirichlet</description></item><item><title>The Euler Constant e is an Irrational Number</title><link>https://freshrimpsushi.github.io/en/posts/1141/</link><pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1141/</guid><description>Theorem $\mathbb{Q}$ represents the set of rational numbers. Proof Using Maclaurin Expansion1 Strategy: Split $e^{-1}$ into two parts using the Maclaurin expansion and derive a contradiction. This proof cannot be conducted within the high school curriculum due to the necessity of the Maclaurin expansion. $\mathbb{N}$ represents the set of natural numbers, $\mathbb{Z}$ represents the set of integers. Part 1. $x_{1} = x_{2}$ Assuming $e \in \mathbb{Q}$, the Euler&amp;rsquo;s constant $e$</description></item><item><title>Proof that Pi is an Irrational Number</title><link>https://freshrimpsushi.github.io/en/posts/1139/</link><pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1139/</guid><description>Theorem $$\pi \notin \mathbb{Q}$$ $\mathbb{Q}$ represents the set of rational numbers. Proof Strategy: Utilize the fact that integers do not have the property of being dense. Define the functions $f$, $F$ very cleverly, applying various tricks. This method was devised by Ivan Niven and is among the easiest to prove that $\pi$ is irrational, but unfortunately, because it employs the epsilon-delta argument, it cannot be proven within the scope of</description></item><item><title>Proof of the Stone-Weierstrass Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1117/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1117/</guid><description>Theorem1 Auxiliary Definitions Let&amp;rsquo;s say $A \subset C(X)$ for $X$. If for any distinct $x_{1}, x_{2} \in X$, there always exists $f \in A$ that satisfies $f(x_{1}) \ne f(x_{2})$, then we say $A$ separates the points of $X$. If $X$ is a metric space and for all $\varepsilon &amp;gt; 0$ and $f \in C(X)$ there exists $g \in A$ that satisfies $| g - f | &amp;lt; \varepsilon$, then $A$</description></item><item><title>Algebra of the Space of Continuous Functions</title><link>https://freshrimpsushi.github.io/en/posts/1113/</link><pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1113/</guid><description>Definitions1 A set $A$ that satisfies the following three conditions is called the algebra of $C(X)$: (i): $\emptyset \ne A \subset C(X)$ (ii): $f,g \in A \implies (f+g) , fg \in A$ (iii): $f \in A , c \in \mathbb{R} \implies cf \in A$ Let&amp;rsquo;s say $X$ is for metric space $A \subset C(X)$. If every sequence $\left\{ f_{n} \in A : n \in \mathbb{N} \right\}$ of $A$ for some</description></item><item><title>Binomial Series Derivation</title><link>https://freshrimpsushi.github.io/en/posts/1103/</link><pubDate>Sun, 09 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1103/</guid><description>Formulas $|x| &amp;lt; 1$ implies $\alpha \in \mathbb{C}$, then $$ \begin{align*} (1 + x )^{\alpha} =&amp;amp; \sum_{k=0}^{\infty} \binom{\alpha}{k} x^{k} \\ =&amp;amp; 1 + \alpha x + \dfrac{\alpha (\alpha-1)}{2!}x^{2} + \dfrac{\alpha (\alpha-1)(\alpha-2)}{3!}x^{3} + \cdots \end{align*} $$ Negative Binomial Series $$ \begin{align*} (1 - x)^{-\alpha} &amp;amp;= \sum\limits_{k=0}^{\infty} \binom{\alpha + k - 1}{k} x^{k} \\ &amp;amp;= 1 + \alpha x + \dfrac{\alpha(\alpha+1)}{2!} x^{2} + \dfrac{\alpha(\alpha+1)(\alpha+2)}{3!} x^{3} + \cdots \end{align*} $$ Description Known as</description></item><item><title>Cauchy Product: The Product of Two Convergent Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1099/</link><pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1099/</guid><description>Theorem 1 If the convergence interval for $f(x) : = \sum _{k=0}^{\infty} a_{k} x^{k}$ and $g(x) : = \sum_{k=0}^{\infty} b_{k} x^{k}$ is $(-r,r)$ and assuming $c_{k} := \sum_{j=0}^{k} a_{j} b_{k-j}$, then $\sum_{k=0}^{\infty} c_{k} x^{k}$ converges to $f(x)g(x)$ within the convergence interval $(-r,r)$. Description The fact that the products of coefficients converge to the product of the coefficients of the two functions on their own is quite fascinating. It would be</description></item><item><title>Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1090/</link><pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1090/</guid><description>Definition A power series is denoted by $S(x) : = \sum \limits_{k=0}^{\infty} a_{k} ( x - x_{0} )^{k}$, and the Center of $S(x)$ is denoted by $x_{0}$. When $S(x)$ converges absolutely for $|x - x_{0}| &amp;lt; R$ and diverges for $|x - x_{0}| &amp;gt; R$, $R$ is called the Radius of Convergence of $S(x)$. The largest interval on which $S(x)$ converges is called the Interval of Convergence. If there exists</description></item><item><title>Convergence of Norms of Function Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1080/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1080/</guid><description>Definitions Suppose a sequence of functions $\left\{ f_{n} \right\}$ is given. If $\| f_{n} - f \|$ converges to $0$, then $f_{n}$ is said to converge in norm, denoted as follows. $$ f_{n} \to f \text{ in norm } $$ or $$ \| f_{n} - f\| \to 0 $$ or $$ \lim \limits_{n \to 0} \| f_{n}-f\|=0 $$ Explanation To define the limit of a sequence, the concept of distance</description></item><item><title>Mean of Function Values</title><link>https://freshrimpsushi.github.io/en/posts/983/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/983/</guid><description>Definition The average value of a function between $[a,\ b]$ and $f(x)$ is equivalent to dividing the integral of the function over the interval by the length of the interval. $$ \dfrac{1}{b-a}\int_{a}^bf(x)dx $$ Derivation Let&amp;rsquo;s denote a partition of the interval $[a,\ b]$ as $P$. $$ P=\left\{ x_{1},\ x_{2},\ \cdots ,\ x_{n} \right\} $$ In this case, $a=x_{1} &amp;lt; x_{2} &amp;lt; \cdots &amp;lt; x_{n}=b$ and the distance between each point</description></item><item><title>Continuity in Every Piece, Smoothness in Every Segment</title><link>https://freshrimpsushi.github.io/en/posts/972/</link><pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/972/</guid><description>Definition A function $f$ is said to be piecewise continuous on an interval $I$ if it satisfies the conditions below: It has a finite number of discontinuities $x_{1},\ x_{2},\ \cdots ,\ x_{n} \in I$. At each point of discontinuity, it has both a left-hand limit and a right-hand limit. $$ \left|\lim \limits_{x\rightarrow x_{i}^{+}} f(x) \right| &amp;lt; \infty \quad \text{and} \quad \left|\lim_{x \rightarrow x_{i}^{-}}f(x)\right|&amp;lt;\infty \quad (i=1,\ \cdots ,\ n) $$ If</description></item><item><title>Local Lipschitz Condition</title><link>https://freshrimpsushi.github.io/en/posts/875/</link><pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/875/</guid><description>Definition If $E$ is open in $\mathbb{R}^{n}$ and let&amp;rsquo;s denote it $\mathbf{f} : E \to \mathbb{R}^{n}$. If for all $\mathbf{x} _{0} \in E$, there exists a $\varepsilon &amp;gt; 0$ that satisfies $B \left( \mathbf{x} _{0} ; \varepsilon \right) \subset E$ and for all $\mathbf{x} , \mathbf{y} \in B \left( \mathbf{x} _{0} ; \varepsilon \right)$, there exists a $K &amp;gt;0$ that satisfies $| \mathbf{f} ( \mathbf{x} ) - \mathbf{f} ( \mathbf{y}</description></item><item><title>Proof of Leibniz's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/884/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/884/</guid><description>Theorem $$ \dfrac{d}{dx} (fg)=\dfrac{df}{dx}g+f\dfrac{dg}{dx} $$ $$ \begin{align*} \dfrac{d^n}{dx^n}(fg)&amp;amp;=\sum \limits_{k=0}^{n}\frac{n!}{(n-k)!k!}\dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n}{}_{n}\mathrm{C}_{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n} \binom{n}{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \end{align*} $$ Description Also known as Leibniz&amp;rsquo;s rule. The first equation is a well-known formula, often referred to as the product rule or the rule of product for differentiation. It simply expresses the result when the product of two functions is differentiated once. More generally, the equation below represents</description></item><item><title>Series, Infinite Series</title><link>https://freshrimpsushi.github.io/en/posts/886/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/886/</guid><description>Definition1 Let&amp;rsquo;s assume a sequence $\left\{ a_{n} \right\}$ is given. Then, let&amp;rsquo;s define the following notation. $$ \sum \limits_{n=p}^{q} a_{n} = a_{p} + a_{p+1} + \cdots + a_{q}\quad (p \le q) $$ Define the partial sum $s_{n}$ of $\left\{ a_{n} \right\}$ as follows. $$ s_{n} = \sum \limits_{k=1}^{n} a_{k} $$ Then, we can think of a sequence $\left\{ s_{n} \right\}$ of these $s_{n}$. The limit of sequence $\left\{ s_{n} \right\}$</description></item><item><title>Integrability is Preserved in the Multiplication of Two Functions</title><link>https://freshrimpsushi.github.io/en/posts/854/</link><pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/854/</guid><description>Theorem1 If two functions $f$ and $g$ are Riemann(-Stieltjes) integrable over the interval $[a,b]$, then $fg$ is also integrable. Proof Assume that $f, g$ is integrable. Since integration is linear, $-g,\ f+g,\ f-g$ is also integrable. Let the function $\phi$ be defined as $\phi (x)=x^2$. Then $\phi$ is continuous over the entire domain. Since integrability is preserved under the composition with continuous functions, $\phi (f+g),\ \phi (f-g)$ is also integrable.</description></item><item><title>Integrability is Preserved in the Composition with Continuous Functions</title><link>https://freshrimpsushi.github.io/en/posts/853/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/853/</guid><description>This document is based on the Riemann-Stieltjes Integral. If we set it as $\alpha=\alpha (x)=x$, it equals the Riemann Integral. Theorem1 Suppose that the function $f$ is Riemann(-Stieltjes) integrable on the interval $[a,b]$ and let $m \le f \le M$. Let $\phi$ be a function that is continuous on the interval $[m,M]$. Let the function $h$ be defined as $h=\phi \circ f$. Then, $h$ is Riemann(-Stieltjes) integrable on the interval</description></item><item><title>Monotone Functions are Riemann-Stieltjes Integrable</title><link>https://freshrimpsushi.github.io/en/posts/849/</link><pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/849/</guid><description>About Riemann Integration Let&amp;rsquo;s suppose that the function $f$ is monotonic over $[a,b]$. Then $f$ is Riemann integrable. Proof Assume that $f$ is a monotonically increasing function1. Let $\epsilon &amp;gt;0$ be given. Consider a partition $P= \left\{ x_{i} : a=x_{0} &amp;lt; x_{1} &amp;lt; x_{2} &amp;lt; \cdots &amp;lt; x_{n}=b \right\}$ of the interval $[a,b]$ that satifies the following for any natural number $n$: $$ \Delta x_{i} = x_{i}-x_{i-1} = \dfrac{b-a}{n},\quad (i=1,2,\dots,n)</description></item><item><title>Continuous Functions are Riemann-Stieltjes Integrable</title><link>https://freshrimpsushi.github.io/en/posts/847/</link><pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/847/</guid><description>= This article is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem If function $f$ is continuous on $[a,b]$, then it is Riemann(-Stieltjes) integrable on $[a,b]$. Proof Suppose $\epsilon &amp;gt;0$ is given. And let&amp;rsquo;s say we chose $\eta&amp;gt;0$ that satisfies $\left[ \alpha (b) - \alpha (a) \right] \eta &amp;lt; \epsilon$. Since $[a,b]$ is compact as it is closed and</description></item><item><title>Necessary and Sufficient Conditions for Riemann(-Stieltjes) Integrability</title><link>https://freshrimpsushi.github.io/en/posts/833/</link><pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/833/</guid><description>This article is based on the Riemann-Stieltjes integral. If we set $\alpha=\alpha (x)=x$, it is the same as Riemann integral. Theorem1 A necessary and sufficient condition for a function $f$ to be Riemann(-Stieltjes) integrable on $[a,b]$ is that for every $\epsilon &amp;gt;0$, there exists a partition $P$ of $[a,b]$ that satisfies $U(P,f,\alpha) - L(P,f,\alpha) &amp;lt; \epsilon$. $$ \begin{equation} f \in \mathscr{R} (\alpha) \text{ on } [a,b] \\ \iff \forall\epsilon &amp;gt;0,</description></item><item><title>Segmentation</title><link>https://freshrimpsushi.github.io/en/posts/830/</link><pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/830/</guid><description>This post is based on the Riemann-Stieltjes integral. If we set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Definition If $P^{\ast}$ and $P$ are partitions of $[a,b]$ and satisfy $P \subseteq P^{\ast}$, then $P^{\ast}$ is called a refinement of $P$. Hence, every point in $P$ is a point in $P^{\ast}$. For any two partitions $P_{1}$ and $P_{2}$, $P_{3}=P_{1} \cup P_{2}$ is called the common refinement of</description></item><item><title>Upper integral is greater than or equal to lower integral.</title><link>https://freshrimpsushi.github.io/en/posts/831/</link><pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/831/</guid><description>This article is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem1 For any partition, the Riemann(-Stieltjes) upper sum is always greater than or equal to the Riemann(-Stieltjes) lower sum. $$ \underline { \int _{a} ^b} f d\alpha \le \overline {\int _{a}^b} f d\alpha $$ Proof Before proving, let&amp;rsquo;s assume the following: $f : [a,b] \to \mathbb{R}$ is bounded. $\alpha</description></item><item><title>Riemann-Stieltjes Integral</title><link>https://freshrimpsushi.github.io/en/posts/829/</link><pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/829/</guid><description>Overview The Riemann-Stieltjes integral is a generalization of the Riemann integral, sometimes simply referred to as Stieltjes integral. The Riemann integral is a special case of the Riemann-Stieltjes integral where $\alpha (x)=x$. The process of defining the Riemann-Stieltjes integral is the same as the process of defining the Riemann integral, so details on the notation and buildup are omitted here. Definition Let $\alpha : [a,b] \to \mathbb{R}$ be a monotonically</description></item><item><title>Partition, Riemann Sum, Riemann Integral</title><link>https://freshrimpsushi.github.io/en/posts/828/</link><pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/828/</guid><description>Partition1 Let&amp;rsquo;s assume the interval $[a,b]$ is given. The partition $P$ of $[a,b]$ is defined as follows. $$ P := \left\{ x_{0},\ x_{1},\ \cdots, x_{n}\right\},\quad a=x_{0} &amp;lt;x_{1}&amp;lt;\cdots &amp;lt; x_{n} =b $$ And $\Delta x_{i}$ is defined as follows. $$ \Delta x_{i} :=x_{i}-x_{i-1},\quad i=1,2,\cdots,n $$ Explanation Simply put, a partition is a set that contains all points at the ends of an interval and all boundary points within the interval when</description></item><item><title>Fresnel Sine Integral's Maclaurin Series Expansion</title><link>https://freshrimpsushi.github.io/en/posts/220/</link><pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/220/</guid><description>Formula $$ S(x) = \sqrt{{2} \over {\pi}} \int_{0}^{x} \sin (w^2) dw = \sqrt{{2} \over {\pi}} \sum_{n=0}^{\infty} {{(-1)^{n}} \over {(2n+1)! (4n+3)}} x^{4n+3} $$ Description Fresnel was a physicist who studied optics, and his name is attached to most results involving trigonometric functions. Likely, trigonometric functions are deeply related to wave functions, which explains why he felt the need to vigorously study them, even if it meant creating formulas where none existed.</description></item><item><title>Mean Value Theorem for Integrals</title><link>https://freshrimpsushi.github.io/en/posts/212/</link><pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/212/</guid><description>Theorem If a function $f$ is continuous on a closed interval $[a,b]$, there exists at least one $c$ in $(a,b)$ that satisfies $\displaystyle f(c) = {{1}\over {b-a} } \int_{a}^{b} f(x) dx$. Description Similar to the Mean Value Theorem but as it is used for integration, it is named as such. The usage is very similar, and its utility is by no means inferior to the Mean Value Theorem. On the</description></item><item><title>Proof of the Fundamental Theorem of Calculus</title><link>https://freshrimpsushi.github.io/en/posts/213/</link><pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/213/</guid><description>Theorem1 Assume that the function $f$ is continuous on the closed interval $[a,b]$. (1) The function $\displaystyle F(x) = \int_{a}^{x} f(t) dt$ is continuous on $[a,b]$, differentiable on $(a,b)$, and satisfies $\displaystyle {{dF(x)} \over {dx}} = f(x)$. (2) For any antiderivative $F$ of $f$, $\displaystyle \int_{a}^{b} f(x) dx = F(b) - F(a)$ Explanation Of course, we use the words differentiation and integration so we can easily guess the relationship between</description></item><item><title>A Comprehensive Summary of Various Series Tests in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/186/</link><pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/186/</guid><description>Overview This post will introduce several series convergence tests without diving into their proofs. It is often more valuable to utilize these tests as facts, especially since the proofs can be quite tedious. In this post, we use the following notations: $\mathbb{N}$ is the set containing all natural numbers. $\mathbb{R}$ is the set containing all real numbers, and $\overline{\mathbb{R}}$ is the extended real number set that includes $\pm \infty$. $\left\{</description></item><item><title>Proof of the Density of Real Numbers</title><link>https://freshrimpsushi.github.io/en/posts/185/</link><pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/185/</guid><description>Theorem For two real numbers $a&amp;lt;b$, there exists a $r \in \mathbb{R}$ that satisfies $a&amp;lt;r&amp;lt;b$. Explanation In the real number space, no matter what interval you consider, there is always another real number in between. No matter how much you split it, there is a point that can be further divided. Although it seems obvious, keep in mind that this is not only non-obvious but also highly abstract. As an</description></item><item><title>Principle of Archimedes in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/181/</link><pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/181/</guid><description>Theorem For any positive number $a$ and real number $b$, there exists a natural number $n$ that satisfies $an&amp;gt;b$. Explanation This means that no matter what $b$ you take, you can always think of a multiple of $a$, which is $n$, that is greater than it. Simply put, ‘No matter how small a number is, if you keep adding to it, it will continue to gro</description></item><item><title>Three Axioms of Analysis: The Axiom of Completeness</title><link>https://freshrimpsushi.github.io/en/posts/180/</link><pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/180/</guid><description>Axioms1 A set $E \subset \mathbb{R}$ is not empty and if $E$ is bounded above, then a supremum $\sup(E) &amp;lt; \infty$ exists. Explanation The axioms of fields and orders might seem like complicating the known, but the completeness axiom does not seem so at a glance. Definitions for the terminology used here appear to be necessary first. Definitions For every element $a$ of $E$ if $a \le M$ is satisfied,</description></item><item><title>Three Axioms of Analysis: 1 Field Axioms</title><link>https://freshrimpsushi.github.io/en/posts/178/</link><pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/178/</guid><description>Axioms1 Let&amp;rsquo;s accept the following properties for real numbers $a,b,c \in \mathbb{R}$ and operations $+,\cdot$. (A1) Closure under addition: $a+b \in \mathbb{R}$ (A2) Associative law for addition: $(a+b) + c = a + (b+c)$ (A3) Commutative law for addition: $ a+ b= b + a$ (A4) Identity element for addition: For every real number $a$, there exists a unique $0$ satisfying $a+0=0+a=a$. (A5) Inverse element for addition: For every real</description></item><item><title>Three Axioms of Analysis: The Second Order Axiom</title><link>https://freshrimpsushi.github.io/en/posts/177/</link><pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/177/</guid><description>Axioms1 For real numbers $ a,b,c \in \mathbb{R}$, the following properties are accepted: Trichotomy: For any given $a,b$, it must be that $a&amp;lt;b$ or $a&amp;gt;b$ or $a=b$ Transitivity: If $a&amp;lt;b$ and $b&amp;lt;c$ then $a&amp;lt;c$ Additivity: If $a&amp;lt;b$ and $c\in \mathbb{R}$ then $a+ c&amp;lt; b + c$ Multiplicativity: If $a&amp;lt;b$ and $c&amp;gt;0$ then $ac&amp;lt; bc$, or if $c&amp;lt;0$ then $ac&amp;gt; bc$ Description Although the terms are quite old-fashioned, they deal with</description></item><item><title>Differentiation of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/168/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/168/</guid><description>Theorem1 $$ \left( \sinh x \right)^{\prime} = \cosh x $$ $$ \left( \cosh x \right)^{\prime} = \sinh x $$ $$ \left( \tanh x \right)^{\prime} = \text{sech}^{2} x $$ Explanation The differentiation of hyperbolic functions actually doesn&amp;rsquo;t require much proof or memorization. The proofs simply use definitions, and the structures are almost identical to trigonometric functions, only with a change in signs. Using the method of proving hyperbolic sine, one can</description></item><item><title>Differentiation of Inverse Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/167/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/167/</guid><description>Theorem1 $$ \begin{align*} \left( \sin^{-1}x \right)^{\prime} &amp;amp;= {{1} \over {\sqrt{1-x^2}}} \\ \left( \cos^{-1}x \right)^{\prime} &amp;amp;= -{{1} \over {\sqrt{1-x^2}}} \\ \left( \tan^{-1}x \right)^{\prime} &amp;amp;= {{1} \over {1+x^2}} \end{align*} $$ Explanation They are read as arcsine, arccosine, and arctangent, respectively. It might seem surprising that these can be differentiated, but it turns out to be quite simple. As one can see on the right side, the shapes of the derivatives are not</description></item><item><title>Proof of Fubini's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/165/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/165/</guid><description>Theorem1 2 Let&amp;rsquo;s define the function $f : R \to \mathbb{R}$ on the 2-dimensional domain $R : [a,b] \times [c,d]$. If $f(x,\cdot)$ is integrable over $[c,d]$, and $f(\cdot,y)$ is integrable over $[a,b]$, and $f$ is integrable over $R$, then $$ \iint _{R} f dA = \int_{a}^{b} \int_{c}^{d} f(x,y) dy dx = \int_{c}^{d} \int_{a}^{b} f(x,y) dx dy $$ Explanation The integration domain $R$ obviously comes from a Rectangle. As it is</description></item><item><title>Proof of Green's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/166/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/166/</guid><description>Theorem1 Let the curve $\mathcal{C}$ be a simple, smooth, closed path in the plane $S = [a,b] \times [c,d]$, moving counterclockwise. If the function $P,Q : \mathbb{R}^2 \to \mathbb{R}$ is continuous on $\mathcal{C}$ and its derivative is also continuous, $$ \int_{\mathcal{C}} (Pdx + Qdy) = \iint_{S} (Q_{x} - P_{y}) dx dy $$ Explanation This can be thought of as a theorem that converts line integrals into surface integrals. It&amp;rsquo;s widely</description></item><item><title>Calculus and the Euler Formula</title><link>https://freshrimpsushi.github.io/en/posts/112/</link><pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/112/</guid><description>Theorem Euler&amp;rsquo;s Formula: $$ { e }^{ ix }= \cos x + i \sin x $$ Euler&amp;rsquo;s Identity: $$ { e }^{ i\pi }+1=0 $$ Explanation Euler&amp;rsquo;s Formula is in itself so peculiar that even Euler did not know where it might be used, but nowadays, it is utilized in so many fields that it is difficult to summarize its usefulness. It is even more astonishing when considering it was</description></item><item><title>Series Expansion of the Arctangent Function</title><link>https://freshrimpsushi.github.io/en/posts/86/</link><pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/86/</guid><description>Theorem1 $$ \tan ^{ -1 } x = \sum _{ n=0 }^{ \infty }{ \frac { (-1) ^{ n } { x } ^ { 2n+1 } } { 2n+1 } } $$ Description Whether it is written as $\arctan$ or as $\tan ^{-1}$ does not matter. Among the several inverse trigonometric functions, arctan is particularly interesting because it provides a series that converges to $\pi$. When $x=1$ is substituted,</description></item><item><title>Derivation of the Series Form of the Natural Logarithm and Proof of the Convergence of the Alternating Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/58/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/58/</guid><description>Theorem $$ \ln(1-x)=\sum _{ n=0 }^{ \infty }{ \frac { -{ x }^{ n+1 } }{ n+1 } } $$ Description The series form of $\ln(1-x)$ can be relatively easily obtained. For $\ln(1+x)$, it is enough to substitute $-x$ for $x$ as a result of the theorem. $$ -\ln(1-x)=x+\frac { { x }^{ 2 } }{ 2 }+\frac { { x }^{ 3 } }{ 3 }+\frac { { x</description></item><item><title>Exponential, Sine, and Cosine Functions' Taylor Series Expansion</title><link>https://freshrimpsushi.github.io/en/posts/59/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/59/</guid><description>Theorem1 $$ \begin{equation} { { e ^ x } }=\sum _{ n=0 }^{ \infty }{ \frac { { x } ^{ n } }{ n! } } \end{equation} $$ $$ \begin{equation} \sin x=\sum _{ n=0 }^{ \infty }{ \frac { { x } ^{ 2n+1 } }{ (2n+1)! }{ { (-1) }^{ n } } } \end{equation} $$ $$ \begin{equation} \cos x=\sum _{ n=0 }^{ \infty }{ \frac { {</description></item><item><title>Fermat's Last Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/35/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/35/</guid><description>Theorem1 If the function $f(x)$ is either a maximum or a minimum at $x=c$ and $f ' (c)$ exists, then $f ' (c) = 0$ Explanation While high school textbooks generally only introduce Rolle&amp;rsquo;s Theorem up to Rolle&amp;rsquo;s Theorem, to rigorously prove Rolle&amp;rsquo;s Theorem, one must be able to show why the derivative at a critical point is $0$, and Fermat&amp;rsquo;s Theorem guarantees that. Proof Strategy: Divide the proof into</description></item><item><title>Proof of Cauchy's Mean Value Theorem</title><link>https://freshrimpsushi.github.io/en/posts/38/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/38/</guid><description>Theorem1 Let&amp;rsquo;s say $a &amp;lt; b$. If the function $f,g : \mathbb{R} \to \mathbb{R}$ is continuous at all points of $[a,b]$, differentiable at all $x \in (a,b)$, and if $g ' (x) \ne 0$, then there exists at least one $c \in (a,b)$ that satisfies the following: $$ {{f ' (c)}\over{g ' (c)}}={{f(b)-f(a)}\over{g(b)-g(a)}} $$ Explanation If there&amp;rsquo;s any difference from the mean value theorem, it&amp;rsquo;s just that there&amp;rsquo;s one more</description></item><item><title>Proof of L'Hôpital's Rule</title><link>https://freshrimpsushi.github.io/en/posts/39/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/39/</guid><description>Theorem1 Given that $f(x)$ and $g(x)$ are differentiable near $x=a$ and that $g ' (x) \ne 0$ and $\displaystyle \lim _{x \to a} f(x) = \lim _{x \to a} g(x) = 0$, $$ \lim _{x \to a} {{f(x)} \over {g(x)}} = \lim _{x \to a} {{f ' (x)} \over {g ' (x)}} $$ Explanation Though this theorem may seem like a magic wand to many students, who have learned and</description></item><item><title>Proof of Rolle's Theorem in Calculus</title><link>https://freshrimpsushi.github.io/en/posts/36/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/36/</guid><description>Theorem1 If the function $f(x)$ is continuous at $[a,b]$ and differentiable at $(a,b)$ and if $f(a)=f(b)$, then there exists at least one $c$ in $(a,b)$ that satisfies $f ' (c)=0$. Description In high school courses, it is introduced only as an auxiliary lemma to prove the mean value theorem and is not used at all otherwise. However, beyond the high school level, it is sometimes used as an auxiliary lemma.</description></item><item><title>Proof of Taylor's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/41/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/41/</guid><description>Theorem1 If a function $f(x)$ is continuous at $[a,b]$ and differentiable up to $n$ times at $(a,b)$, then there exists $\xi \in (a,b)$ that satisfies $$ \begin{align*} f(b) =&amp;amp; \sum_{k=0}^{n-1} {{(b-a)^{k}\over{k!}}{f^{(k)}( a )}} + {(b-a)^{n}\over{n!}}{f^{(n)}(\xi)} \\ =&amp;amp; {f(a)} + {(b-a)f ' (a)} + \cdots + {(b-a)^{n-1}\over{(n-1)!}}{f^{(n-1)}(a)} + {(b-a)^{n}\over{(n)!}}{f^{(n)}(\xi)} \end{align*} $$ Explanation This theorem, which is widely used throughout mathematics, has lent its name to the Taylor series. In terms of</description></item><item><title>Proof of the Mean Value Theorem in Calculus</title><link>https://freshrimpsushi.github.io/en/posts/37/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/37/</guid><description>Theorem1 If the function $f(x)$ is continuous at $[a,b]$ and differentiable at $(a,b)$, then there exists at least one $c$ in $(a,b)$ that satisfies $\displaystyle f '(c)={{f(b)-f(a)}\over{b-a}}$. Description It&amp;rsquo;s not just commonly used; it&amp;rsquo;s so famous that it&amp;rsquo;s abbreviated as MVT. The term &amp;lsquo;mean value&amp;rsquo; comes from the idea that there is a point where the derivative equals the average rate of change over the entire interval. The concept of</description></item><item><title>Taylor Series and Maclaurin Series</title><link>https://freshrimpsushi.github.io/en/posts/42/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/42/</guid><description>Theorem1 A function $f$ that is infinitely differentiable around point $a$, a necessary and sufficient condition for $\displaystyle f(x) = \sum_{n=0}^{\infty} {{f^{(n)} (a)}\over{n!}} {(x-a)}^n$ is that for some $\xi \in \mathscr{H} \left\{ x , a \right\}$ $$ \lim_{n \to \infty} {{f^{(n)} (\xi)}\over{n!}} {(x-a)}^n = 0 $$ where $\xi \in \mathscr{H} \left\{ x , a \right\}$ means that $\xi$ is in either $(x,a)$ or $(a,x)$. Explanation The Taylor theorem often represents</description></item><item><title>Euler's Proof of the Divergence of the Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/17/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/17/</guid><description>Theorem The harmonic series diverges. $$ \sum _{ n=1 }^{ \infty }{ \frac { 1 }{ n } }=\infty $$ Description At first glance, the harmonic series appears as if it would converge since its terms continue to decrease in value. However, Oresme elegantly and simply proved that it diverges. This fact is often used as an example to explain the concept of absolute convergence, where the alternating harmonic series</description></item></channel></rss>