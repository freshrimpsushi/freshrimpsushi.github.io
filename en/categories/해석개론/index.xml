<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>해석개론 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%ED%95%B4%EC%84%9D%EA%B0%9C%EB%A1%A0/</link>
    <description>Recent content in 해석개론 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 08 Sep 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%95%B4%EC%84%9D%EA%B0%9C%EB%A1%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Definition of Smooth Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3110/</link>
      <pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3110/</guid>
      <description>Definition If a function $f$ is infinitely differentiable, then $f$ is referred to as a smooth function. If a function $f$ is differentiable and $f^{\prime}$ is continuous, then $f$ is referred to as a smooth function. Explanation $y= \left| x \right|$ is sharp in $x=0$, making it impossible to differentiate in $x=0$. Therefore, it is described as smooth when differentiation works well at every point, and the differentiated function is</description>
    </item>
    <item>
      <title>Integration by Parts</title>
      <link>https://freshrimpsushi.github.io/en/posts/1867/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1867/</guid>
      <description>Theorem 1 Assuming $F$, $G$ are differentiable in the interval $[a,b]$, and $F^{\prime}=f$, $G^{\prime}=g$ are integrable. Then, the following equation holds: $$ \begin{align*} \int _{a} ^{b} F(x)g(x)dx &amp;amp;= F(b)G(b)-F(a)G(a)-\int _{a} ^{b}f(x)G(x)dx \\ &amp;amp;= \left[ F(x)G(x) \right]_{a}^{b} -\int _{a} ^{b}f(x)G(x)dx \end{align*} $$ Description This result is called the integration by parts. Memorizing it as Integration-Differential-Integration makes it easy. What to integrate is kept on both sides as is, and what to</description>
    </item>
    <item>
      <title>The Fundamental Theorem of Calculus in Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1866/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1866/</guid>
      <description>Theorem1 Given that function $f$ is Riemann integrable on the interval $[a,b]$, and there exists a function $F$ that is differentiable on $[a,b]$, satisfying $F^{\prime}=f$. Then, the following holds true. $$ \int_{a}^{b} f(x) dx= F(b)-F(a) $$ Explanation This theorem is famously known as the Fundamental Theorem of Calculus Part 2, often abbreviated as FTC2[^Funcamental Theorem of Calculus1]. It implies that the definite integral of $f$ is represented by the difference</description>
    </item>
    <item>
      <title>Limits from the Left and the Right Strictly Defined in Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1830/</link>
      <pubDate>Sat, 17 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1830/</guid>
      <description>Definition Let&amp;rsquo;s assume a function $f :X \to \mathbb{R}$ is given in a metric space $X$. If $f$ is not continuous at $x\in X$, $f$ is said to be discontinuous at $x$ or to have a discontinuity at $x$. $f: (a,b) \to \mathbb{R}$ is assumed. For any point $x$, let $a \le x &amp;lt;b$. Consider a sequence of points $(x,b)$ that converges to $x$ and call it $\left\{ t_{n} \right\}$.</description>
    </item>
    <item>
      <title>The Chain Rule of Differentiation in Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1823/</link>
      <pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1823/</guid>
      <description>Theorem1 If $f :[a,b] \to \mathbb{R}$ is a continuous function and is differentiable at $x\in [a,b]$, and if $g : f([a,b])\to \mathbb{R}$ is differentiable at $f (x)\in f([a,b])$, and if we define $h : [a,b] \to \mathbb{R}$ as follows. $$ h(t)=g\left( f(t) \right)\quad (a\le t \le b) $$ Then, $h$ is differentiable at $x$ and its value is as follows. $$ h^{\prime}(x)=g^{\prime}(f(x))f^{\prime}(x) $$ Using the composite function symbol, it can</description>
    </item>
    <item>
      <title>Differentiable Function Properties</title>
      <link>https://freshrimpsushi.github.io/en/posts/1821/</link>
      <pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1821/</guid>
      <description>Theorem1 Let&amp;rsquo;s say $f, g : [a,b] \to \mathbb{R}$. If $f,g$ is differentiable at $x\in [a,b]$, then $f+g$, $fg$, and $f/g$ are also differentiable at $x$ and the following equation holds. $$ \begin{align} (f+g)^{\prime}(x) &amp;amp;=f^{\prime}(x)+g^{\prime}(x) \\ (fg)^{\prime}(x) &amp;amp;= f^{\prime}(x)g(x)+f(x)g^{\prime}(x) \\ \left( \frac{f}{g} \right)^{\prime}(x) &amp;amp;= \frac{f^{\prime}(x)g(x)-f(x)g^{\prime}(x)}{g^{2}(x)} \end{align} $$ However, $(3)$ holds when $g(x)\ne 0$. Description $(2)$ is commonly referred to as the product rule of differentiation. Proof $(1)$ By the definition</description>
    </item>
    <item>
      <title>Linearity of Riemann(-Stieltjes) Iintegral</title>
      <link>https://freshrimpsushi.github.io/en/posts/1666/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1666/</guid>
      <description>Theorem1 This article is based on the Riemann-Stieltjes integral. If set to $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Let&amp;rsquo;s say $f$ is integrable by Riemann(-Stieltjes) from $[a,b]$. Then, for a constant $c\in \mathbb{R}$, $cf$ is also integrable from $[a,b]$, and its value is as follows. $$ \int_{a}^{b}cf d\alpha = c\int_{a}^{b}f d\alpha $$ Let two functions $f_{1}$, $f_{2}$ be integrable by Riemann(-Stieltjes) from $[a,b]$. Then, $f_{1}+f_{2}$ is</description>
    </item>
    <item>
      <title>Leibniz Integral Rule</title>
      <link>https://freshrimpsushi.github.io/en/posts/1475/</link>
      <pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1475/</guid>
      <description>Summary Let&amp;rsquo;s assume that $f(x,t)$ and $\dfrac{\partial f}{\partial x}(x,t)$ are consecutive. Then, the following equation holds. $$ \frac{d}{dx} \int_{a}^b f(x,t)dt = \int_{a}^b\frac{\partial f}{\partial x}(x,t)dt $$ Description Being able to interchange the order of differentiation and integration is undoubtedly useful. Besides, there are many theorems or formulas related to differentiation and integration named after Leibniz. Proof Since if continuous, then integrable, let&amp;rsquo;s assume $u$ as follows. $$ u(x):=\int_{a}^b f(x,t)dt $$ Then,</description>
    </item>
    <item>
      <title>Extended Real Number System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1252/</link>
      <pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1252/</guid>
      <description>Definition The set defined as follows is called the extended real number system. $$ \overline{ \mathbb{R} } := \mathbb{R} \cup \left\{ -\infty, +\infty\right\} $$ Explanation In fields such as analysis, for convenience, the set $\mathbb{R}$ is often replaced with $\overline{ \mathbb{R} }$. $\pm \infty$ is not a number, but for convenience, it is treated as one and added to $\mathbb{R}$. Within the extended real number system, the rules for comparison</description>
    </item>
    <item>
      <title>Differentiation of Functions Defined in Real Number Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/1210/</link>
      <pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1210/</guid>
      <description>Definition1 If in some $E$ containing $a$, $f$ is defined and the limit $$ f^{\prime} (a) := \lim_{h \to 0} {{ f (a + h ) - f(a) } \over { h }}=\lim \limits_{x\rightarrow a}\frac{f(x)-f(a)}{x-a} $$ exists, then $f$ is said to be differentiable at $a$, and $f^{\prime} (a)$ is called the derivative of $f$ at $a$. If $f$ is differentiable at every point $a \in E$, then $f$ is</description>
    </item>
    <item>
      <title>Uniform Continuity of Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1207/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1207/</guid>
      <description>Definition1 Let us assume $E \subset \mathbb{R}$ is a non-empty set and define $f : E \to \mathbb{R}$. If for every $\varepsilon &amp;gt; 0$, $$ | x_{1} - x_{2} | &amp;lt; \delta \land x_{1} , x_{2} \in E \implies | f(x_{1}) - f(x_{2}) | &amp;lt; \varepsilon $$ there exists a $\delta&amp;gt;0$ satisfying the above equation, then $f$ is said to be uniformly continuous on $E$. $\land$ represents the logical &amp;lsquo;and&amp;rsquo;</description>
    </item>
    <item>
      <title>Newly Defined Continuous Functions in University Mathematics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1206/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1206/</guid>
      <description>Definition Let&amp;rsquo;s say a set that is not an empty set is called $E \subset \mathbb{R}$, and $f : E \to \mathbb{R}$. If there exists $\delta&amp;gt;0$ for every $\varepsilon &amp;gt; 0$ such that $$ | x - a | &amp;lt; \delta \implies | f(x) - f(a) | &amp;lt; \varepsilon $$ is satisfied, $f$ is said to be continuous at $a \in E$, and if it is continuous at every point</description>
    </item>
    <item>
      <title>Epsilon-Delta Argument</title>
      <link>https://freshrimpsushi.github.io/en/posts/1204/</link>
      <pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1204/</guid>
      <description>Definition1 Let $I$ be an interval containing $a \in \mathbb{R}$, and suppose that $f$ is a function defined at $I \setminus \left\{ a \right\}$. If for every $\epsilon &amp;gt; 0$, there exists a $\delta&amp;gt;0$ such that $$ 0 &amp;lt; | x - a | &amp;lt; \delta \implies | f(x) - L | &amp;lt; \varepsilon $$ is satisfied, then we say that $f(x)$ converges to $L \in \mathbb{R}$ as $x \to</description>
    </item>
    <item>
      <title>Limits Supremum and Limits Infimum</title>
      <link>https://freshrimpsushi.github.io/en/posts/1198/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1198/</guid>
      <description>Definition Let $\left\{ x_{n} \right\}_{n \in \mathbb{N}}$, $\left\{ y_{n} \right\}_{n \in \mathbb{N}}$ be sequences of real numbers. $\displaystyle \limsup_{n \to \infty} x_{n} := \lim_{n \to \infty} \left( \sup_{k \ge n} x_{k} \right)$ is called the limit supremum of $\left\{ x_{n} \right\}$. $\displaystyle \liminf_{n \to \infty} y_{n} := \lim_{n \to \infty} \left( \inf_{k \ge n} y_{k} \right)$ is called the limit infimum of $\left\{ y_{n} \right\}$. Where $\displaystyle \sup_{k \ge n}</description>
    </item>
    <item>
      <title>Redefining the Limits of Sequences in University Mathematics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1184/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1184/</guid>
      <description>Definitions1 2 $\mathbb{N}$ represents the set of natural numbers, and $\mathbb{R}$ represents the set of real numbers. A function with a domain of $\mathbb{N}$ is called a sequence. For a sequence of natural numbers $\left\{ n_{k} \right\}_{ k \in \mathbb{N}}$, $\left\{ x_{n_{k}} \right\}_{ k \in \mathbb{N}}$ is called a subsequence of $\left\{ x_{n} \right\}_{ n \in \mathbb{N}}$. If for every $x \in \left\{ x_{n} \right\}_{ n \in \mathbb{N}}$ there exists</description>
    </item>
    <item>
      <title>The Accumulation Point in the Set of Real Numbers</title>
      <link>https://freshrimpsushi.github.io/en/posts/379/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/379/</guid>
      <description>Definition Given a point $x \in \mathbb{R}$ on the real line and a subset $A \subset \mathbb{R}$, if for any open set $O$ containing $x$, $ O \cap ( A \setminus \left\{ x \right\} ) \ne \emptyset $ holds, then $x$ is defined as a Limit Point. The set of limit points of $A$ is called the Derived set of $A$, and is denoted by $a &#39;$. Explanation In the</description>
    </item>
    <item>
      <title>Uniform Convergence of Function Series</title>
      <link>https://freshrimpsushi.github.io/en/posts/1154/</link>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1154/</guid>
      <description>Definition Let&amp;rsquo;s define a subset $E \ne \emptyset$ of $\mathbb{R}$, function $f : E \to \mathbb{R}$, and sequence of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$. If there exists $N \in \mathbb{N}$ for every $\varepsilon &amp;gt; 0$ satisfying $n \ge N \implies | f_{n} (x) - f(x) | &amp;lt; \varepsilon$, then sequence $f_{n}$ converges uniformly to $f$ in $E$, denoted by: $$ f_n \rightrightarrows f $$ or $$ f_{n}</description>
    </item>
    <item>
      <title>Pointwise Convergence of Function Sequences</title>
      <link>https://freshrimpsushi.github.io/en/posts/1148/</link>
      <pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1148/</guid>
      <description>Definition Let us define a function $f : E \to \mathbb{R}$ for the subset $E \ne \emptyset$ of $\mathbb{R}$. If the sequence of functions $\left\{ f_{n} : E \to \mathbb{R} \right\}_{n=1}^{\infty}$ satisfies $f(x) = \lim \limits_{n \to \infty} f_{n} (X)$ for each $x \in E$, then it is said to converge pointwise to $f_{n}$ in $E$, denoted by: $$ f_{n} \to f $$ Explanation Rewriting the above definition using the</description>
    </item>
    <item>
      <title>Proof of the Stone-Weierstrass Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/1117/</link>
      <pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1117/</guid>
      <description>Theorem1 Auxiliary Definitions Let&amp;rsquo;s say $A \subset C(X)$ for $X$. If for any distinct $x_{1}, x_{2} \in X$, there always exists $f \in A$ that satisfies $f(x_{1}) \ne f(x_{2})$, then we say $A$ separates the points of $X$. If $X$ is a metric space and for all $\varepsilon &amp;gt; 0$ and $f \in C(X)$ there exists $g \in A$ that satisfies $| g - f | &amp;lt; \varepsilon$, then $A$</description>
    </item>
    <item>
      <title>Power Series</title>
      <link>https://freshrimpsushi.github.io/en/posts/1090/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1090/</guid>
      <description>Definition A power series is denoted by $S(x) : = \sum \limits_{k=0}^{\infty} a_{k} ( x - x_{0} )^{k}$, and the Center of $S(x)$ is denoted by $x_{0}$. When $S(x)$ converges absolutely for $|x - x_{0}| &amp;lt; R$ and diverges for $|x - x_{0}| &amp;gt; R$, $R$ is called the Radius of Convergence of $S(x)$. The largest interval on which $S(x)$ converges is called the Interval of Convergence. If there exists</description>
    </item>
    <item>
      <title>Mean of Function Values</title>
      <link>https://freshrimpsushi.github.io/en/posts/983/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/983/</guid>
      <description>Definition The average value of a function between $[a,\ b]$ and $f(x)$ is equivalent to dividing the integral of the function over the interval by the length of the interval. $$ \dfrac{1}{b-a}\int_{a}^bf(x)dx $$ Derivation Let&amp;rsquo;s denote a partition of the interval $[a,\ b]$ as $P$. $$ P=\left\{ x_{1},\ x_{2},\ \cdots ,\ x_{n} \right\} $$ In this case, $a=x_{1} &amp;lt; x_{2} &amp;lt; \cdots &amp;lt; x_{n}=b$ and the distance between each point</description>
    </item>
    <item>
      <title>Continuity in Every Piece, Smoothness in Every Segment</title>
      <link>https://freshrimpsushi.github.io/en/posts/972/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/972/</guid>
      <description>Definition A function $f$ is said to be piecewise continuous on an interval $I$ if it satisfies the conditions below: It has a finite number of discontinuities $x_{1},\ x_{2},\ \cdots ,\ x_{n} \in I$. At each point of discontinuity, it has both a left-hand limit and a right-hand limit. $$ \left|\lim \limits_{x\rightarrow x_{i}^{+}} f(x) \right| &amp;lt; \infty \quad \text{and} \quad \left|\lim_{x \rightarrow x_{i}^{-}}f(x)\right|&amp;lt;\infty \quad (i=1,\ \cdots ,\ n) $$ If</description>
    </item>
    <item>
      <title>Proof of Leibniz&#39;s Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/884/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/884/</guid>
      <description>Summary $$ \dfrac{d}{dx} (fg)=\dfrac{df}{dx}g+f\dfrac{dg}{dx} $$ $$ \begin{align*} \dfrac{d^n}{dx^n}(fg)&amp;amp;=\sum \limits_{k=0}^{n}\frac{n!}{(n-k)!k!}\dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n}{}_{n}\mathrm{C}_{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n} \binom{n}{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \end{align*} $$ Description Also known as Leibniz&amp;rsquo;s rule. The first equation is a well-known formula, often referred to as the product rule or the rule of product for differentiation. It simply expresses the result when the product of two functions is differentiated once. More generally, the equation below represents</description>
    </item>
    <item>
      <title>Series, Infinite Series</title>
      <link>https://freshrimpsushi.github.io/en/posts/886/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/886/</guid>
      <description>Definition1 Let&amp;rsquo;s assume a sequence $\left\{ a_{n} \right\}$ is given. Then, let&amp;rsquo;s define the following notation. $$ \sum \limits_{n=p}^{q} a_{n} = a_{p} + a_{p+1} + \cdots + a_{q}\quad (p \le q) $$ Define the partial sum $s_{n}$ of $\left\{ a_{n} \right\}$ as follows. $$ s_{n} = \sum \limits_{k=1}^{n} a_{k} $$ Then, we can think of a sequence $\left\{ s_{n} \right\}$ of these $s_{n}$. The limit of sequence $\left\{ s_{n} \right\}$</description>
    </item>
    <item>
      <title>Continuous Functions are Riemann-Stieltjes Integrable</title>
      <link>https://freshrimpsushi.github.io/en/posts/847/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/847/</guid>
      <description>= This article is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem If function $f$ is continuous on $[a,b]$, then it is Riemann(-Stieltjes) integrable on $[a,b]$. Proof Suppose $\epsilon &amp;gt;0$ is given. And let&amp;rsquo;s say we chose $\eta&amp;gt;0$ that satisfies $\left[ \alpha (b) - \alpha (a) \right] \eta &amp;lt; \epsilon$. Since $[a,b]$ is compact as it is closed and</description>
    </item>
    <item>
      <title>Necessary and Sufficient Conditions for Riemann(-Stieltjes) Integrability</title>
      <link>https://freshrimpsushi.github.io/en/posts/833/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/833/</guid>
      <description>This article is based on the Riemann-Stieltjes integral. If we set $\alpha=\alpha (x)=x$, it is the same as Riemann integral. Theorem1 A necessary and sufficient condition for a function $f$ to be Riemann(-Stieltjes) integrable on $[a,b]$ is that for every $\epsilon &amp;gt;0$, there exists a partition $P$ of $[a,b]$ that satisfies $U(P,f,\alpha) - L(P,f,\alpha) &amp;lt; \epsilon$. $$ \begin{equation} f \in \mathscr{R} (\alpha) \text{ on } [a,b] \\ \iff \forall\epsilon &amp;gt;0,</description>
    </item>
    <item>
      <title>Segmentation</title>
      <link>https://freshrimpsushi.github.io/en/posts/830/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/830/</guid>
      <description>This post is based on the Riemann-Stieltjes integral. If we set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Definition If $P^{\ast}$ and $P$ are partitions of $[a,b]$ and satisfy $P \subseteq P^{\ast}$, then $P^{\ast}$ is called a refinement of $P$. Hence, every point in $P$ is a point in $P^{\ast}$. For any two partitions $P_{1}$ and $P_{2}$, $P_{3}=P_{1} \cup P_{2}$ is called the common refinement of</description>
    </item>
    <item>
      <title>Riemann-Stieltjes Integral</title>
      <link>https://freshrimpsushi.github.io/en/posts/829/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/829/</guid>
      <description>Overview The Riemann-Stieltjes integral is a generalization of the Riemann integral, sometimes simply referred to as Stieltjes integral. The Riemann integral is a special case of the Riemann-Stieltjes integral where $\alpha (x)=x$. The process of defining the Riemann-Stieltjes integral is the same as the process of defining the Riemann integral, so details on the notation and buildup are omitted here. Definition Let $\alpha : [a,b] \to \mathbb{R}$ be a monotonically</description>
    </item>
    <item>
      <title>Partition, Riemann Sum, Riemann Integral</title>
      <link>https://freshrimpsushi.github.io/en/posts/828/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/828/</guid>
      <description>Partition1 Let&amp;rsquo;s assume the interval $[a,b]$ is given. The partition $P$ of $[a,b]$ is defined as follows. $$ P := \left\{ x_{0},\ x_{1},\ \cdots, x_{n}\right\},\quad a=x_{0} &amp;lt;x_{1}&amp;lt;\cdots &amp;lt; x_{n} =b $$ And $\Delta x_{i}$ is defined as follows. $$ \Delta x_{i} :=x_{i}-x_{i-1},\quad i=1,2,\cdots,n $$ Explanation Simply put, a partition is a set that contains all points at the ends of an interval and all boundary points within the interval when</description>
    </item>
    <item>
      <title>Mean Value Theorem for Integrals</title>
      <link>https://freshrimpsushi.github.io/en/posts/212/</link>
      <pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/212/</guid>
      <description>Theorem If a function $f$ is continuous on a closed interval $[a,b]$, there exists at least one $c$ in $(a,b)$ that satisfies $\displaystyle f(c) = {{1}\over {b-a} } \int_{a}^{b} f(x) dx$. Description Similar to the Mean Value Theorem but as it is used for integration, it is named as such. The usage is very similar, and its utility is by no means inferior to the Mean Value Theorem. On the</description>
    </item>
    <item>
      <title>Proof of the Fundamental Theorem of Calculus</title>
      <link>https://freshrimpsushi.github.io/en/posts/213/</link>
      <pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/213/</guid>
      <description>Theorem1 Assume that the function $f$ is continuous on the closed interval $[a,b]$. (1) The function $\displaystyle F(x) = \int_{a}^{x} f(t) dt$ is continuous on $[a,b]$, differentiable on $(a,b)$, and satisfies $\displaystyle {{dF(x)} \over {dx}} = f(x)$. (2) For any antiderivative $F$ of $f$, $\displaystyle \int_{a}^{b} f(x) dx = F(b) - F(a)$ Explanation Of course, we use the words differentiation and integration so we can easily guess the relationship between</description>
    </item>
    <item>
      <title>A Comprehensive Summary of Various Series Tests in Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/186/</link>
      <pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/186/</guid>
      <description>Overview This post will introduce several series convergence tests without diving into their proofs. It is often more valuable to utilize these tests as facts, especially since the proofs can be quite tedious. In this post, we use the following notations: $\mathbb{N}$ is the set containing all natural numbers. $\mathbb{R}$ is the set containing all real numbers, and $\overline{\mathbb{R}}$ is the extended real number set that includes $\pm \infty$. $\left\{</description>
    </item>
    <item>
      <title>Proof of the Density of Real Numbers</title>
      <link>https://freshrimpsushi.github.io/en/posts/185/</link>
      <pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/185/</guid>
      <description>Theorem For two real numbers $a&amp;lt;b$, there exists a $r \in \mathbb{R}$ that satisfies $a&amp;lt;r&amp;lt;b$. Explanation In the real number space, no matter what interval you consider, there is always another real number in between. No matter how much you split it, there is a point that can be further divided. Although it seems obvious, keep in mind that this is not only non-obvious but also highly abstract. As an</description>
    </item>
    <item>
      <title>Three Axioms of Analysis: The Axiom of Completeness</title>
      <link>https://freshrimpsushi.github.io/en/posts/180/</link>
      <pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/180/</guid>
      <description>Axioms1 A set $E \subset \mathbb{R}$ is not empty and if $E$ is bounded above, then a supremum $\sup(E) &amp;lt; \infty$ exists. Explanation The axioms of fields and orders might seem like complicating the known, but the completeness axiom does not seem so at a glance. Definitions for the terminology used here appear to be necessary first. Definitions For every element $a$ of $E$ if $a \le M$ is satisfied,</description>
    </item>
    <item>
      <title>Three Axioms of Analysis: 1 Field Axioms</title>
      <link>https://freshrimpsushi.github.io/en/posts/178/</link>
      <pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/178/</guid>
      <description>Axioms1 Let&amp;rsquo;s accept the following properties for real numbers $a,b,c \in \mathbb{R}$ and operations $+,\cdot$. (A1) Closure under addition: $a+b \in \mathbb{R}$ (A2) Associative law for addition: $(a+b) + c = a + (b+c)$ (A3) Commutative law for addition: $ a+ b= b + a$ (A4) Identity element for addition: For every real number $a$, there exists a unique $0$ satisfying $a+0=0+a=a$. (A5) Inverse element for addition: For every real</description>
    </item>
    <item>
      <title>Proof of Fubini&#39;s Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/165/</link>
      <pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/165/</guid>
      <description>Theorem1 2 Let&amp;rsquo;s define the function $f : R \to \mathbb{R}$ on the 2-dimensional domain $R : [a,b] \times [c,d]$. If $f(x,\cdot)$ is integrable over $[c,d]$, and $f(\cdot,y)$ is integrable over $[a,b]$, and $f$ is integrable over $R$, then $$ \iint _{R} f dA = \int_{a}^{b} \int_{c}^{d} f(x,y) dy dx = \int_{c}^{d} \int_{a}^{b} f(x,y) dx dy $$ Explanation The integration domain $R$ obviously comes from a Rectangle. As it is</description>
    </item>
    <item>
      <title>Proof of Green&#39;s Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/166/</link>
      <pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/166/</guid>
      <description>Theorem1 Let the curve $\mathcal{C}$ be a simple, smooth, closed path in the plane $S = [a,b] \times [c,d]$, moving counterclockwise. If the function $P,Q : \mathbb{R}^2 \to \mathbb{R}$ is continuous on $\mathcal{C}$ and its derivative is also continuous, $$ \int_{\mathcal{C}} (Pdx + Qdy) = \iint_{S} (Q_{x} - P_{y}) dx dy $$ Explanation This can be thought of as a theorem that converts line integrals into surface integrals. It&amp;rsquo;s widely</description>
    </item>
    <item>
      <title>Calculus and the Euler Formula</title>
      <link>https://freshrimpsushi.github.io/en/posts/112/</link>
      <pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/112/</guid>
      <description>Theorem Euler&amp;rsquo;s Formula: $$ { e }^{ ix }= \cos x + i \sin x $$ Euler&amp;rsquo;s Identity: $$ { e }^{ i\pi }+1=0 $$ Explanation Euler&amp;rsquo;s Formula is in itself so peculiar that even Euler did not know where it might be used, but nowadays, it is utilized in so many fields that it is difficult to summarize its usefulness. It is even more astonishing when considering it was</description>
    </item>
    <item>
      <title>Exponential, Sine, and Cosine Functions&#39; Taylor Series Expansion</title>
      <link>https://freshrimpsushi.github.io/en/posts/59/</link>
      <pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/59/</guid>
      <description>Theorem1 $$ \begin{equation} { { e ^ x } }=\sum _{ n=0 }^{ \infty }{ \frac { { x } ^{ n } }{ n! } } \end{equation} $$ $$ \begin{equation} \sin x=\sum _{ n=0 }^{ \infty }{ \frac { { x } ^{ 2n+1 } }{ (2n+1)! }{ { (-1) }^{ n } } } \end{equation} $$ $$ \begin{equation} \cos x=\sum _{ n=0 }^{ \infty }{ \frac { {</description>
    </item>
    <item>
      <title>Proof of Rolle&#39;s Theorem in Calculus</title>
      <link>https://freshrimpsushi.github.io/en/posts/36/</link>
      <pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/36/</guid>
      <description>Summary1 If the function $f(x)$ is continuous at $[a,b]$ and differentiable at $(a,b)$ and if $f(a)=f(b)$, then there exists at least one $c$ in $(a,b)$ that satisfies $f &#39; (c)=0$. Description In high school courses, it is introduced only as an auxiliary lemma to prove the mean value theorem and is not used at all otherwise. However, beyond the high school level, it is sometimes used as an auxiliary lemma.</description>
    </item>
    <item>
      <title>Proof of Taylor&#39;s Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/41/</link>
      <pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/41/</guid>
      <description>Theorem1 If a function $f(x)$ is continuous at $[a,b]$ and differentiable up to $n$ times at $(a,b)$, then there exists $\xi \in (a,b)$ that satisfies $$ \begin{align*} f(b) =&amp;amp; \sum_{k=0}^{n-1} {{(b-a)^{k}\over{k!}}{f^{(k)}( a )}} + {(b-a)^{n}\over{n!}}{f^{(n)}(\xi)} \\ =&amp;amp; {f(a)} + {(b-a)f &#39; (a)} + \cdots + {(b-a)^{n-1}\over{(n-1)!}}{f^{(n-1)}(a)} + {(b-a)^{n}\over{(n)!}}{f^{(n)}(\xi)} \end{align*} $$ Explanation This theorem, which is widely used throughout mathematics, has lent its name to the Taylor series. In terms of</description>
    </item>
    <item>
      <title>Proof of the Mean Value Theorem in Calculus</title>
      <link>https://freshrimpsushi.github.io/en/posts/37/</link>
      <pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/37/</guid>
      <description>Theorem1 If the function $f(x)$ is continuous at $[a,b]$ and differentiable at $(a,b)$, then there exists at least one $c$ in $(a,b)$ that satisfies $\displaystyle f &#39;(c)={{f(b)-f(a)}\over{b-a}}$. Description It&amp;rsquo;s not just commonly used; it&amp;rsquo;s so famous that it&amp;rsquo;s abbreviated as MVT. The term &amp;lsquo;mean value&amp;rsquo; comes from the idea that there is a point where the derivative equals the average rate of change over the entire interval. The concept of</description>
    </item>
    <item>
      <title>Taylor Series and Maclaurin Series</title>
      <link>https://freshrimpsushi.github.io/en/posts/42/</link>
      <pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/42/</guid>
      <description>Theorem1 A function $f$ that is infinitely differentiable around point $a$, a necessary and sufficient condition for $\displaystyle f(x) = \sum_{n=0}^{\infty} {{f^{(n)} (a)}\over{n!}} {(x-a)}^n$ is that for some $\xi \in \mathscr{H} \left\{ x , a \right\}$ $$ \lim_{n \to \infty} {{f^{(n)} (\xi)}\over{n!}} {(x-a)}^n = 0 $$ where $\xi \in \mathscr{H} \left\{ x , a \right\}$ means that $\xi$ is in either $(x,a)$ or $(a,x)$. Explanation The Taylor theorem often represents</description>
    </item>
    <item>
      <title>Euler&#39;s Proof of the Divergence of the Harmonic Series</title>
      <link>https://freshrimpsushi.github.io/en/posts/17/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/17/</guid>
      <description>Theorem The harmonic series diverges. $$ \sum _{ n=1 }^{ \infty }{ \frac { 1 }{ n } }=\infty $$ Description At first glance, the harmonic series appears as if it would converge since its terms continue to decrease in value. However, Oresme elegantly and simply proved that it diverges. This fact is often used as an example to explain the concept of absolute convergence, where the alternating harmonic series</description>
    </item>
  </channel>
</rss>
