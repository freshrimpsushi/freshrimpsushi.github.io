<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Stochastic Differential Equations on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%ED%99%95%EB%A5%A0%EB%AF%B8%EB%B6%84%EB%B0%A9%EC%A0%95%EC%8B%9D/</link><description>Recent content in Stochastic Differential Equations on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 13 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%99%95%EB%A5%A0%EB%AF%B8%EB%B6%84%EB%B0%A9%EC%A0%95%EC%8B%9D/index.xml" rel="self" type="application/rss+xml"/><item><title>The Hyperfunctional Derivative of Brownian Motion is White Noise</title><link>https://freshrimpsushi.github.io/en/posts/3552/</link><pubDate>Tue, 13 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3552/</guid><description>Summary The distributional derivative of Brownian motion is white noise. Description Brownian motion $B_{t}$ does not have a derivative in the traditional sense. Therefore, it can be defined as a stochastic process that satisfies the following condition $\xi$, which is defined as white noise: $$ \begin{align} E[\xi_{t}] &amp;amp;= 0, &amp;amp; \forall t \\ \Cov(\xi_{t}, \xi_{s}) &amp;amp;= \delta_{0} \end{align} $$ Here, $\Cov$ is the covariance, and $\delta$ is the Dirac delta</description></item><item><title>Poker-Plank Equation Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2312/</link><pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2312/</guid><description>Theorem $$ d X_{t} = f \left( t, X_{t} \right) dt + g \left( t , X_{t} \right) d W_{t} \qquad , t \in \left[ t_{0} , T \right] $$ Given a stochastic differential equation as above, and let $F \in C_{0}^{\infty} \left( \mathbb{R} \right)$. Then, at time point $t$, the probability density function $p(t,x)$ of $X_{t}$ follows the next partial differential equation. $$ {{ \partial p(t,x) } \over {</description></item><item><title>Derivation of Black-Scholes Model</title><link>https://freshrimpsushi.github.io/en/posts/2156/</link><pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2156/</guid><description>Model 1 At time point $t$, let&amp;rsquo;s say the price of $S_{t}$ units of the underlying asset $1$, and assume that $S_{t}$ undergoes Geometric Brownian Motion. That is, for Standard Brownian Motion $W_{t}$, drift $\mu \in \mathbb{R}$, and diffusion $\sigma^{2} &amp;gt; 0$, $S_{t}$ is the solution to the following Stochastic Differential Equation. $$ d S_{t} = S_{t} \left( \mu dt + \sigma d W_{t} \right) $$ When a risk-free rate</description></item><item><title>Shoji-Ozaki Local Linearization Method</title><link>https://freshrimpsushi.github.io/en/posts/2152/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2152/</guid><description>Build-up1 $$ d X_{t} = f \left( t, X_{t} \right) dt + g \left( X_{t} \right) d W_{t} $$ Let&amp;rsquo;s assume that the diffusion $g$ is dependent only on $X_{t}$ and independent of time $t$ as given by the following stochastic differential equation. If $Y_{t}$ represents some constant $\sigma$ such that $\phi &amp;rsquo; \left( X_{t} \right) g \left( X_{t} \right) = \sigma$ is $\phi \in C^{2}$, which is then expressed</description></item><item><title>Lambert Transformation</title><link>https://freshrimpsushi.github.io/en/posts/2150/</link><pubDate>Sat, 29 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2150/</guid><description>Definition 1 $$ d X_{t} = f \left( t , X_{t} \right) dt + g \left( X_{t} \right) d W_{t} $$ Let&amp;rsquo;s assume that the diffusion $g$ is dependent only on $X_{t}$ and independent of time $t$, given the stochastic differential equation (SDE) as shown above. The transformation $F : X_{t} \mapsto Y_{t}$ is called the Lamperti Transformation. $$ Y_{t} := F \left( X_{t} \right) = \left. \int {{ 1</description></item><item><title>Milstein Method Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2148/</link><pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2148/</guid><description>Method 1 $$ d X(t) = f \left( X_{t} \right) dt + g \left( X_{t} \right) d W_{t} \qquad , t \in [t_{0}, T] $$ Given that the Ito process is a solution to the autonomous stochastic differential equation described above. For equally spaced intervals of $h$, the calculation expressed for the equally spaced points $\left\{ t_{i} \le T : t_{i+1} = t_{i} + h \right\}_{i=0}^{N}$ is a numerical solution</description></item><item><title>Euler-Maruyama Method Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2146/</link><pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2146/</guid><description>Methods 1 $$ d X(t) = f \left( X_{t} \right) dt + g \left( X_{t} \right) d W_{t} \qquad , t \in [t_{0}, T] $$ Let&amp;rsquo;s say that the It√¥ process is given as a solution to the autonomous stochastic differential equation as shown above. For the equidistant points $\left\{ t_{i} \le T : t_{i+1} = t_{i} + h \right\}_{i=0}^{N}$ with a constant interval of $h$, $Y_{i} :=</description></item><item><title>Ito-Taylor Expansion Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2144/</link><pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2144/</guid><description>Theorem 1 $$ d X(t) = f \left( X_{t} \right) dt + g \left( X_{t} \right) d W_{t} \qquad , t \in [0, T] $$ Let&amp;rsquo;s assume the Ito Process is given as the solution to the above Autonomous Stochastic Differential Equation. If $f,g : \mathbb{R} \to \mathbb{R}$ satisfies the Linear Growth Condition, i.e., for some constant $K$, $\begin{cases} \left| f \left( X_{t} \right) \right| \le K \left( 1 +</description></item><item><title>Strong and Weak Convergence of Numerical Solutions to SDEs</title><link>https://freshrimpsushi.github.io/en/posts/2142/</link><pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2142/</guid><description>Buildup $$ d X_{t} = f \left( t, X_{t} \right) dt + g \left( t , X_{t} \right) d W_{t} \qquad , t \in \left[ t_{0} , T \right] $$ Given a Stochastic Differential Equation as above, let&amp;rsquo;s assume that the time is discretized as $t_{0} &amp;lt; t_{1} &amp;lt; \cdots &amp;lt; t_{N}$. Choosing a sufficiently large $N \in \mathbb{N}$ and setting $\Delta = \left( T - t_{0} \right) / N</description></item><item><title>CKLS Mean Reverting Gamma Stochastic Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/2140/</link><pubDate>Sun, 09 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2140/</guid><description>Model 1 $$ d X_{t} = \left( \alpha - \beta X_{t} \right) dt + \sigma X_{t}^{\gamma} d W_{t} \qquad , X_{0} &amp;gt; 0 $$ Let&amp;rsquo;s assume $\alpha, \beta, \sigma, \gamma &amp;gt; 0$. This stochastic differential equation is called the CKLS Mean Reverting Gamma Stochastic Differential Equation. Variables $X_{t}$: Represents the Interest Rate or the Gene Frequency. Parameters $\alpha / \beta$: The Mean Reversion, towards which $X_{t}$ tends to revert over</description></item><item><title>Cox-Ingersoll-Ross Model, CIR Model</title><link>https://freshrimpsushi.github.io/en/posts/2138/</link><pubDate>Wed, 05 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2138/</guid><description>Model 1 $$ d X_{t} = \left( \alpha - \beta X_{t} \right) dt + \sigma \sqrt{X_{t}} d W_{t} \qquad , X_{0} &amp;gt; 0 $$ Assume $\alpha, \beta, \sigma &amp;gt; 0$ satisfies $2 \alpha &amp;gt; \sigma^{2}$. The stochastic differential equation mentioned above is called the CIR model. $$ X_{t} = {{ \alpha } \over { \beta }} + e^{-\beta t} \left( X_{0} - {{ \alpha } \over { \beta }} \right)</description></item><item><title>Ornstein-Uhlenbeck Equation</title><link>https://freshrimpsushi.github.io/en/posts/2136/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2136/</guid><description>Definition 1 $$ d X_{t} = a X_{t} dt + \sigma d W_{t} $$ Let $a , \sigma \in \mathbb{R}$. The stochastic differential equation given above is called the Ornstein-Uhlenbeck Equation, and its solution, the stochastic process $X_{t}$, is called the Ornstein-Uhlenbeck Process. $$ X_{t} = X_{0} e^{a t} + \sigma \int_{0}^{t} e^{a (t-s)} d W_{s} $$ Description 2 The Ornstein-Uhlenbeck equation is also known as the Langevin Equation. If</description></item><item><title>Bridge of Brown</title><link>https://freshrimpsushi.github.io/en/posts/2134/</link><pubDate>Tue, 28 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2134/</guid><description>Definition 1 2 $$ d Y_{t} = {{ b - Y_{t} } \over { 1 - t }} dt + d W_{t} \qquad, t \in [0,1), Y_{0} = a $$ Let&amp;rsquo;s denote it as $a, b \in \mathbb{R}$. The stochastic process $Y_{t}$, which is a solution of the $1$-dimensional stochastic differential equation, from ($a$ to $b$) is called a Brownian Bridge. $$ Y_{t} = a (1-t) + bt + (1-t)</description></item><item><title>Solutions to Typical Stochastic Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/2132/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2132/</guid><description>Equations 1 (G) General Form: $$ d X_{t} = f \left( t , X_{t} \right) dt + g \left( t , X_{t} \right) d W_{t} $$ (L) Linear: $\begin{cases} f \left( t , X_{t} \right) = a_{t} + b_{t} X_{t} \\ g \left( t , X_{t} \right) = c_{t} + e_{t} X_{t} \end{cases}$ $$ d X_{t} = \left( a_{t} + b_{t} X_{t} \right) dt + \left( c_{t} + e_{t} X_{t}</description></item><item><title>Linear, Homogeneous, Autonomous Stochastic Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/2130/</link><pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2130/</guid><description>Definition 1 Let&amp;rsquo;s assume we have a probability space $( \Omega , \mathcal{F} , P)$ and a filtration $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$. Consider the following $n$-dimensional stochastic differential equation with respect to two functions $f$, $g$, and $\mathcal{F}_{t}$-adapted $m$-dimensional Wiener process $W_{t}$: $$ \begin{align*} d X_{t} =&amp;amp; f \left( t, X_{t} \right) dt + g \left( t, X_{t} \right) d W_{t} \\ f =&amp;amp; a(t) + A(t) X_{t} \\</description></item><item><title>Existence and Uniqueness of Solutions to Stochastic Differential Equations, Strong and Weak Solutions</title><link>https://freshrimpsushi.github.io/en/posts/2128/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2128/</guid><description>Definition 1 Let us have a probability space $( \Omega , \mathcal{F} , P)$ and a filtration $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$. $$ \begin{align*} f &amp;amp;: [0,T] \times \mathbb{R}^{n} \to \mathbb{R}^{n} \\ g &amp;amp;: [0,T] \times \mathbb{R}^{n} \to \mathbb{R}^{n \times m} \end{align*} $$ Consider the following $n$-dimensional stochastic differential equation for two functions $f$, $g$ and $\mathcal{F}_{t}$-adapted $m$-dimensional Wiener process $W_{t}$: $$ d X_{t} = f \left( t, X_{t} \right)</description></item><item><title>What is a Stochastic Differential Equation?</title><link>https://freshrimpsushi.github.io/en/posts/2125/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2125/</guid><description>Definition 1 $$ d X(t) = f \left( t, X(t) \right) dt + g \left( t, X(t) \right) d W_{t} \qquad , t \in \left[ t_{0} , T \right], T &amp;gt; 0 $$ Equations of the form above are called Stochastic Differential Equations, abbreviated as SDEs. Here, $f$ and $g$ are called the drift and diffusion coefficient functions, respectively. For the initial condition $X_{0} := X \left( t_{0} \right)$, the</description></item><item><title>Ito Formula and Martingale Representation Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2123/</link><pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2123/</guid><description>Theorem 1 2 Given a probability space $( \Omega , \mathcal{F} , P)$ and a filtration $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$, let&amp;rsquo;s say that a Wiener process $\left\{ W_{t} \right\}_{t \ge 0}$ is $\mathcal{F}_{t}$-adapted. It√¥&amp;rsquo;s Lemma If $f \in \mathcal{L}^{2} (P)$, then there exists a unique stochastic process $X (t,\omega) \in m^{2}(0,T)$ satisfying: $$ f (\omega) = E (f) +</description></item><item><title>Ito's Formula</title><link>https://freshrimpsushi.github.io/en/posts/2121/</link><pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2121/</guid><description>Theorem 1 Given an It√¥ process $\left\{ X_{t} \right\}_{t \ge 0}$, $$ d X_{t} = u dt + v d W_{t} $$ for a function $V \left( t, X_{t} \right) = V \in C^{2} \left( [0,\infty) \times \mathbb{R} \right)$, let $Y_{t} := V \left( t, X_{t} \right)$, then $\left\{ Y_{t} \right\}$ is also an It√¥ process and the following holds: $$ \begin{align*} d Y_{t} =&amp;amp; V_{t}</description></item><item><title>Ito Process</title><link>https://freshrimpsushi.github.io/en/posts/2119/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2119/</guid><description>Definition 1 Given a probability space $( \Omega , \mathcal{F} , P)$ and a filtration $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$, suppose that a Wiener process $\left\{ W_{t} \right\}_{t \ge 0}$ is $\mathcal{F}_{t}$-adapted, and for $f \in \mathcal{L}^{1} [0 , \infty)$ and $g \in \mathcal{L}^{2} [0 , \infty)$, we define a $1$-dimensional continuous $\mathcal{F}_{t}$-adapted stochastic process $\left\{ X_{t} \right\}_{t \ge 0}$ as a $1$-dimensional It√¥ Process. $$ X (t)</description></item><item><title>Integration by Parts</title><link>https://freshrimpsushi.github.io/en/posts/2117/</link><pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2117/</guid><description>Theorem 1 If the bounded continuous function $f(s,\omega) = f(s)$ in $[0,t]$ depends only on $s$, then $$ \int_{0}^{t} f(s) d W_{s} = f (t) W_{t} - \int_{0}^{t} W_{s} d f (s) $$ $W_{t}$ is a Wiener process. Description It&amp;rsquo;s a theorem about It√¥ integration, not much different from the integration by parts we commonly know. It‚Äôs important to note that the integrand has changed.</description></item><item><title>Ito Multiplication Table</title><link>https://freshrimpsushi.github.io/en/posts/2115/</link><pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2115/</guid><description>Build-up $s&amp;lt; t &amp;lt; t+u$ Suppose, meet the following conditions for a stochastic process $\left\{ W_{t} \right\}$ to be called a Wiener process. (i): $W_{0} = 0$ (ii): $\left( W_{t+u} - W_{t} \right) \perp W_{s}$ (iii): $\left( W_{t+u} - W_{t} \right) \sim N ( 0, u )$ (iv): Sample paths of $W_{t}$ are almost surely continuous. The Wiener process has the following properties: [1]: $\displaystyle W_{t} \sim N ( 0</description></item><item><title>Isometric Equality of Ito</title><link>https://freshrimpsushi.github.io/en/posts/2113/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2113/</guid><description>Theorem 1 For all $f \in m^{2}[a,b]$, the following equation holds. $$ E \left[ \left( \int_{a}^{b} f d W_{t} \right)^{2} \right] = E \left[ \int_{a}^{b} f^{2} dt \right] $$ Explanation While it is correct that the power outside of the integral sign $^{2}$ crosses over, attention should also be paid to the change in the integrands $d W_{t}$ and $dt$. Proof 1 Strategy: Since it suffices to show for sequences</description></item><item><title>Ito Calculus</title><link>https://freshrimpsushi.github.io/en/posts/2111/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2111/</guid><description>Buildup Before discussing stochastic integrals, it is crucial to define an essential probabilistic process called the Elementary Process. Elementary processes play a similar role to simple functions, which were necessary for defining the Lebesgue integral in [Measure Theory](../../categories/Measure Theory). $$ a = t_{0} &amp;lt; t_{1} &amp;lt; \cdots &amp;lt; t_{k} = b $$ Considering such a partition in the Natural Domain $[a,b]$, an Elementary Process is defined as follows for indicator</description></item><item><title>m2 Space</title><link>https://freshrimpsushi.github.io/en/posts/2109/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2109/</guid><description>Definition 1 2 Given that there is a probability space $( \Omega , \mathcal{F} , P)$, A sequence of sub sigma fields $\left\{ \mathcal{F}_{t} \right\}_{t \ge 0}$ of $\mathcal{F}$ is called a Filtration if it satisfies the following: $$ \forall s &amp;lt; t, \mathcal{F}_{s} \subset \mathcal{F}_{t} $$ A stochastic process $g(t,\omega) : [0,\infty) \times \Omega \to \mathbb{R}^{n}$ is said to be $\mathcal{F}_{t}$-Adapted if for all $t \ge 0$, $\omega \mapsto</description></item><item><title>Stochastic Differential Equations with White Noise</title><link>https://freshrimpsushi.github.io/en/posts/2103/</link><pubDate>Wed, 27 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2103/</guid><description>Motivation $$ \xi (t) \overset{?}{:=} \dot{W}(t) = {{d W (t)} \over {dt}} $$ Imagine a $\xi$ as the derivative of a Wiener process as shown above. When thinking of Brownian motion, this $\xi (t)$ would be a noise representing random fluctuations at the time point $t$. Although it seems very intuitive and not at all awkward, regrettably, there is an issue with the existence of $\dot{W}(t)$ in the universal sense.</description></item></channel></rss>