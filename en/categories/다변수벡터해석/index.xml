<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>다변수벡터해석 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%EB%8B%A4%EB%B3%80%EC%88%98%EB%B2%A1%ED%84%B0%ED%95%B4%EC%84%9D/</link>
    <description>Recent content in 다변수벡터해석 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 04 May 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%EB%8B%A4%EB%B3%80%EC%88%98%EB%B2%A1%ED%84%B0%ED%95%B4%EC%84%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>n-Dimensional Polar Coordinates</title>
      <link>https://freshrimpsushi.github.io/en/posts/3229/</link>
      <pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3229/</guid>
      <description>Definition1 Let&amp;rsquo;s say the Cartesian coordinates of point $x \in \mathbb{R}^{n}$ are $x_{1}, \dots, x_{n}$. Then, the relationship with its polar coordinates $r, \varphi_{1}, \dots, \varphi_{n-1}$ is as follows. $$ \begin{align*} x_{n} &amp;amp;= r \cos \varphi_{1} \\ x_{n-1} &amp;amp;= r \sin \varphi_{1} \sin \varphi_{2} \\ x_{n-2} &amp;amp;= r \sin \varphi_{1} \cos \varphi_{2} \\ \vdots&amp;amp; \\ x_{4} &amp;amp;= r \sin \varphi_{1} \sin \varphi_{2} \cdots \sin \varphi_{n-3} \sin \varphi_{n-2} \\ x_{3} &amp;amp;=</description>
    </item>
    <item>
      <title>Taylor&#39;s Theorem for Multivariable Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3163/</link>
      <pubDate>Thu, 23 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3163/</guid>
      <description>Theorem1 Let $f : \mathbb{R}^{n} \to \mathbb{R}$ be $C^{k}$ function, and call it $\mathbf{a} = (a_{1}, \dots, a_{n}) \in \mathbb{R}^{n}$. Then, there exists $C^{k-2}$ function $h_{ij}$ that satisfies the following. $$ f(\mathbf{x}) = f(\mathbf{a}) + \sum_{i} (x_{i} - a_{i})\dfrac{\partial f}{\partial x_{i}}(\mathbf{a}) + \sum_{i,j}h_{ij}(\mathbf{x})(x_{i} - a_{i}) (x_{j} - a_{j}) $$ Description It generalizes the Taylor theorem to functions of several variables. second-order $$ \begin{align*} f(\mathbf{x}) &amp;amp;= f(\mathbf{a}) + \sum\limits_{i=1}^{n} (x_{i} -</description>
    </item>
    <item>
      <title>Inverse Function Theorem in Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/3139/</link>
      <pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3139/</guid>
      <description>Summary1 Let&amp;rsquo;s say a function $\mathbf{f} : E \subset \mathbb{R}^{n} \to \mathbb{R}^{n}$ defined in an open set $E$ is a $C^{1}$-function. For $\mathbf{a} \in E$, let&amp;rsquo;s assume that $\mathbf{f}^{\prime}(\mathbf{a})$ is invertible and $\mathbf{b} = \mathbf{f}(\mathbf{a})$. Then, the following holds. (a) There exists an open set $U, V \subset \mathbb{R}^{n}$ where $\mathbf{a} \in U, \mathbf{b} \in V$, and over $U$, $\mathbf{f}$ is one-to-one and $\mathbf{f}(U) = V$. (b) If $\mathbf{g}$ is</description>
    </item>
    <item>
      <title>Jacobian of Composite Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3138/</link>
      <pubDate>Wed, 03 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3138/</guid>
      <description>Summary Let&amp;rsquo;s assume we have two functions $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ and $g : \mathbb{R}^{m} \to \mathbb{R}^{k}$. We denote the Jacobian of $f$ as $J(f)$. Then, the following holds. $$ J(g \circ f) = J(g) J(f) $$ Explanation Since the Jacobian is the most generalized derivative, the above theorem is a generalization of the chain rule. Proof By definition of the Jacobian, $$ J(g \circ f) = \begin{bmatrix} \dfrac{\partial</description>
    </item>
    <item>
      <title>Chain Rule for Multivariable Vector Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3134/</link>
      <pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3134/</guid>
      <description>Theorem Let&amp;rsquo;s assume that two functions $\mathbf{g} : D \subset \mathbb{R}^{m} \to \mathbb{R}^{k}$, $\mathbf{f} : \mathbf{g}(\mathbb{R}^{k}) \subset \mathbb{R}^{k} \to \mathbb{R}^{n}$ are differentiable. Then, the composition of these two functions $\mathbf{F} = \mathbf{f} \circ \mathbf{g} : \mathbb{R}^{m} \to \mathbb{R}^{n}$ is also differentiable, and the (total) derivative of $\mathbf{F}$ satisfies the following. $$ \mathbf{F}^{\prime}(\mathbf{x}) = \mathbf{f}^{\prime}\left( \mathbf{g}(\mathbf{x}) \right) \mathbf{g}^{\prime}(\mathbf{x}) $$ Explanation This is called the chain rule. If we denote $\mathbf{x} =</description>
    </item>
    <item>
      <title>Definition of Directional Derivative</title>
      <link>https://freshrimpsushi.github.io/en/posts/3109/</link>
      <pubDate>Mon, 06 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3109/</guid>
      <description>Buildup Let&amp;rsquo;s say a multivariable function $f = \mathbb{R}^{n} \to \mathbb{R}$ is given. When trying to calculate the derivative of $f$, unlike the case with a univariable function, one must consider the rate of change in &amp;lsquo;which direction&amp;rsquo;. A familiar example is the partial derivative. The partial derivative considers the rate of change with respect to only one variable. For instance, the partial derivative $\dfrac{\partial f}{\partial y}$ of $f=f(x,y,z)$ with</description>
    </item>
    <item>
      <title>Conformal Mapping</title>
      <link>https://freshrimpsushi.github.io/en/posts/3102/</link>
      <pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3102/</guid>
      <description>Definition1 Assuming the mapping $\mathbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is given as follows. $$ \mathbf{f}(\mathbf{x}) = \left( f_{1}(\mathbf{x}), f_{2}(\mathbf{x}), \dots, f_{m}(\mathbf{x}) \right),\quad \mathbf{x}\in \R^{n} $$ The total derivative, or Jacobian matrix of $\mathbf{f}$ is as follows. $$ \mathbf{f}^{\prime} = J = \begin{bmatrix} \dfrac{\partial f_{1}}{\partial x_{1}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_{1}}{\partial x_{n}} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \dfrac{\partial f_{m}}{\partial x_{1}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_{m}}{\partial x_{n}} \end{bmatrix} $$ If</description>
    </item>
    <item>
      <title>Partial Derivatives: Derivatives of Multivariable Vector Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3082/</link>
      <pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3082/</guid>
      <description>Buildup[^1] Recall the definition of the derivative of a univariate function. $$ \lim \limits_{h\to 0} \dfrac{f(x+h) - f(x)}{h} = f^{\prime}(x) $$ By approximating the numerator on the left-hand side as a linear function of $h$, we get the following. $$ \begin{equation} f(x+h) - f(x) = a h + r(h) \label{1} \end{equation} $$ Let&amp;rsquo;s call $r(h)$ the remainder, satisfying the condition below. $$ \lim \limits_{h \to 0} \dfrac{r(h)}{h}=0 $$ Then, dividing</description>
    </item>
    <item>
      <title>Laplacian of a Scalar Field</title>
      <link>https://freshrimpsushi.github.io/en/posts/3081/</link>
      <pubDate>Mon, 12 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3081/</guid>
      <description>Definition The divergence of the gradient of the scalar function $u : \mathbb{R}^{n} \to \mathbb{R}$ is called the Laplacian and is denoted as follows. $$ \begin{align*} \Delta u :&amp;amp;= \mathrm{div}(\nabla (u)) \\ &amp;amp;= \mathrm{div} \left( \left( u_{x_{1}}, u_{x_{2}}, \dots, u_{x_{n}} \right) \right) \\ &amp;amp;= u_{x_{1}x_{1}} + u_{x_{2}x_{2}} + \cdots + u_{x_{n}x_{n}} \\ &amp;amp;= \sum _{i=1}^{n} u_{x_{i}x_{i}} \end{align*} $$ Here, $u_{x_{i}}=\dfrac{\partial u}{\partial x_{i}}$ is. Explanation In mathematics, the divergence is often</description>
    </item>
    <item>
      <title>Integration of Multivariable Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1902/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1902/</guid>
      <description>Definition1 Let $I^{k}$ be a k-cell, and assume $\mathbf{x} \in I^{k}$. $$ \mathbf{x} = (x_{1},\dots,x_{k}),\quad a_{i} \le x_{i} \le b_{i} (i=1,\dots,k) $$ Suppose $f: I^{k} \to \mathbb{R}$ is continuous. Then, since it is integrable, let us set it as $f=f_{k}$, and define $f_{k-1} : I^{k-1} \to \mathbb{R}$ as follows: $$ f_{k-1} (x_{1}, \dots, x_{k-1}) = \int_{a_{k}}^{b_{k}} f_{k}(x_{1}, \dots, x_{k}) dx_{k} $$ Then, by the Leibniz Rule, $f_{k-1}$ is continuous in</description>
    </item>
    <item>
      <title>Partial Derivatives</title>
      <link>https://freshrimpsushi.github.io/en/posts/3036/</link>
      <pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3036/</guid>
      <description>Definitions1 Let us define $E\subset \mathbb{R}^{n}$ as an open set, and $\mathbf{x}\in E$, and $\mathbf{f} : E \to \mathbb{R}^{m}$. Let $\left\{ \mathbf{e}_{1}, \mathbf{e}_{2}, \dots, \mathbf{e}_{n} \right\}$, and $\left\{ \mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{m} \right\}$ be the standard basis of $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$, respectively. Then, the components $f_{i} : \mathbb{R}^{n} \to \mathbb{R}$ of $\mathbf{f}$ are defined as follows. $$ \mathbf{f} (\mathbf{x}) = \sum_{i=1}^{m} f_{i}(\mathbf{x})\mathbf{u}_{i}, \quad \mathbf{x} \in E $$ or $$ f_{i}</description>
    </item>
    <item>
      <title>Derivatives of Vectors and Matrices</title>
      <link>https://freshrimpsushi.github.io/en/posts/1926/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1926/</guid>
      <description>Gradient of a Scalar Function Scalar function $f : \mathbb{R}^{n} \to \mathbb{R}$&amp;rsquo;s gradient is as follows. $$ \frac{ \partial f(\mathbf{x})}{ \partial \mathbf{x} } := \nabla f(\mathbf{x}) = \begin{bmatrix} \dfrac{ \partial f(\mathbf{x})}{ \partial x_{1} } &amp;amp; \dfrac{ \partial f(\mathbf{x})}{ \partial x_{2} } &amp;amp; \cdots &amp;amp; \dfrac{ \partial f(\mathbf{x})}{ \partial x_{n} } \end{bmatrix}^{T} $$ Here, $\dfrac{ \partial f(\mathbf{x})}{ \partial x_{i} }$ is the partial derivative of $f$ with respect to $x_{i}$. Inner</description>
    </item>
    <item>
      <title>Integration of Vector-Valued Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1868/</link>
      <pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1868/</guid>
      <description>Definition1 Let $f_{1}$, $f_{2}$, $\dots$, $f_{k}$ be functions taking real values on the interval $[a,b]$. And suppose $\mathbf{f} : [a,b] \to \mathbb{R}^{k}$ is defined as follows. $$ \mathbf{f}(x)=\left( f_{1}(x),\dots,f_{k}(x) \right),\quad x\in [a,b] $$ If each $f_{k}$ is integrable on the interval $[a,b]$, then the integral of $\mathbf{f}$ is defined as follows. $$ \int _{a} ^{b} \mathbf{f}dx = \left( \int _{a} ^{b}f_{1} dx, \dots, \int _{a} ^{b}f_{k} dx \right) $$ Theorem</description>
    </item>
    <item>
      <title>Divergence in Vector Fields</title>
      <link>https://freshrimpsushi.github.io/en/posts/1777/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1777/</guid>
      <description>Definition In a Euclidean space, a vector field $\textbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ represented as $\textbf{f} = (f_{1} , \cdots , f_{n})$ and the direction of the axis as $u_{1} , \cdots , u_{n}$, the divergence of $\textbf{f}$ is defined as follows. $$ \operatorname{div} \textbf{f} := \nabla \cdot \textbf{f} = \sum_{k=1}^{n} {{ \partial f_{k} } \over { \partial u_{k} }} $$ Explanation The divergence of a vector field serves as</description>
    </item>
    <item>
      <title>Volume in Vector Fields</title>
      <link>https://freshrimpsushi.github.io/en/posts/1772/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1772/</guid>
      <description>Definition The volume $V$ of a subspace $D \subset \mathbb{R}^{n}$ in a Euclidean space is defined as follows when expressed in Cartesian coordinates $\textbf{u} = (u_{1}, u_{2}, \cdots , u_{n})$. $$ V(D) = \int_{D} du_{1} du_{2} \cdots d u_{n} $$ When $\textbf{u} \in \mathbb{R}^{n}$ is transformed by a vector function $\textbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ as $\textbf{f} \left( \textbf{u} \right) = \left( f_{1} (\textbf{u}) , \cdots , f_{n} (\textbf{u}) \right)$,</description>
    </item>
    <item>
      <title>Gradient of Scalar Field</title>
      <link>https://freshrimpsushi.github.io/en/posts/1010/</link>
      <pubDate>Sun, 28 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1010/</guid>
      <description>Definition A gradient, specifically referred to as the total derivative of a scalar field $f : \mathbb{R}^{n} \to \mathbb{R}$, is denoted by $\nabla f$. $$ \begin{align*} \nabla f := f^{\prime} =&amp;amp; \begin{bmatrix} D_{1}f &amp;amp; D_{2}f &amp;amp; \cdots &amp;amp; D_{n}f\end{bmatrix} \\ =&amp;amp; \begin{bmatrix} \dfrac{\partial f}{\partial x_{1}} &amp;amp; \dfrac{\partial f}{\partial x_{2}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f}{\partial x_{n}} \end{bmatrix} \\ =&amp;amp; \dfrac{\partial f}{\partial x_{1}}\hat{x}_{1} + \dfrac{\partial f}{\partial x_{2}}\hat{x}_{2} + \dots + \dfrac{\partial f}{\partial</description>
    </item>
    <item>
      <title>What is a Hessian Matrix?</title>
      <link>https://freshrimpsushi.github.io/en/posts/992/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/992/</guid>
      <description>Definition $D \subset \mathbb{R}^{n}$ is defined as the matrix $H \in \mathbb{R}^{n \times n}$ for a multivariate scalar function $f : D \to \mathbb{R}$ is called the Hessian matrix of $f$. $$ H := \begin{bmatrix} {{\partial^2 f } \over {\partial x_{1}^2 }} &amp;amp; \cdots &amp;amp; {{\partial^2 f } \over { \partial x_{1} \partial x_{n} }} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ {{\partial^2 f } \over {\partial x_{n} \partial x_{1}</description>
    </item>
    <item>
      <title>Jacobian Matrix or Jacobi Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/989/</link>
      <pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/989/</guid>
      <description>Definition Let a multivariable vector function $\mathbb{f} : D \to \mathbb{R}^{m}$ defined by $D \subset \mathbb{R}^{n}$ be defined for each scalar function $f_{1} , \cdots , f_{m} : D \to \mathbb{R}$ as follows: $$ \mathbb{f} ( x_{1} , \cdots , x_{n} ) : = \begin{bmatrix} f_{1} ( x_{1} , \cdots , x_{n} ) \\ \vdots \\ f_{m} ( x_{1} , \cdots , x_{n} ) \end{bmatrix} $$ It is called the</description>
    </item>
    <item>
      <title>Scalar Functions and Vector-valued Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/970/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/970/</guid>
      <description>Definition Let $D$ be a subset $D\subset \mathbb{R}^{n}$ of the $n$-dimensional Euclidean space. Functions having $D$ as their domain are called function of several variables. $f : D \to \mathbb{R}$ is called a scalar function. For a scalar function $f_{1} , \cdots , f_{m} : D \to \mathbb{R}$, $\mathbb{f} : D \to \mathbb{R}^{m}$ defined as follows is called a vector-valued function. $$ \mathbb{f} ( x_{1} , \cdots , x_{n} )</description>
    </item>
    <item>
      <title>Proof of the Pappus-Guldin Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/685/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/685/</guid>
      <description>Summary Let the area of a shape $F$ on the plane be denoted as $A$, and let the volume of the solid of revolution $W$ obtained by rotating $F$ around axis $z$ be denoted as $V$. If the distance between the center of mass of $F$ and the axis $z$ is denoted as $r$, then $$ V = 2 \pi r A $$ Description The Pappus-Guldin Theorem is often mentioned</description>
    </item>
    <item>
      <title>Inner product in Euclidean space</title>
      <link>https://freshrimpsushi.github.io/en/posts/255/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/255/</guid>
      <description>Definition Let&amp;rsquo;s say $V = \mathbb{R}^n$ for a vector space, and also $\mathbb{x}, \mathbb{y}, \mathbb{z} \in V$ and $k \in \mathbb{R}$. $\left&amp;lt; \cdot , \cdot \right&amp;gt; : V^2 \to \mathbb{R}$ is defined as the inner product on $V$ when it satisfies the following four conditions: (1) Symmetry: $\left&amp;lt; \mathbb{x} , \mathbb{y} \right&amp;gt; = \left&amp;lt; \mathbb{y}, \mathbb{x} \right&amp;gt;$ (2) Additivity: $\left&amp;lt; \mathbb{x} + \mathbb{y} , \mathbb{z} \right&amp;gt; = \left&amp;lt; \mathbb{x}, \mathbb{z}</description>
    </item>
    <item>
      <title>Why Notation of Partial Differential is Different?</title>
      <link>https://freshrimpsushi.github.io/en/posts/2573/</link>
      <pubDate>Mon, 24 May 2004 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2573/</guid>
      <description>Question In partial derivatives, unlike the usual derivatives, expressions like $\displaystyle {{ \partial f } \over { \partial t }}$ are used instead of $\displaystyle {{ d f } \over { d t }}$. $\partial$ is read as &amp;ldquo;Round Dee&amp;rdquo; or &amp;ldquo;Partial,&amp;rdquo; and historically, it originated from &amp;ldquo;Curly Dee,&amp;rdquo; which is a cursive form of $d$1. In code, it&amp;rsquo;s \partial, and in Korea, some people even shorten it to just</description>
    </item>
  </channel>
</rss>
