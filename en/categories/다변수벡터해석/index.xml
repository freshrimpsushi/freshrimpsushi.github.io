<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Vector Analysis on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%EB%8B%A4%EB%B3%80%EC%88%98%EB%B2%A1%ED%84%B0%ED%95%B4%EC%84%9D/</link><description>Recent content in Vector Analysis on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 19 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%EB%8B%A4%EB%B3%80%EC%88%98%EB%B2%A1%ED%84%B0%ED%95%B4%EC%84%9D/index.xml" rel="self" type="application/rss+xml"/><item><title>Matrix Differentiation Table for Scalar Functions</title><link>https://freshrimpsushi.github.io/en/posts/3675/</link><pubDate>Sat, 19 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3675/</guid><description>Explanation The formulas for matrix calculus have been summarized in the table below. The notation used throughout the document is as follows: $\mathbf{a}, \mathbf{b} \in \mathbb{R}^{n}$: A constant vector independent of $\mathbf{x}$ or $\mathbf{X}$ $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{n \times n}$: A constant matrix independent of $\mathbf{x}$ or $\mathbf{X}$ $\mathbf{x} \in \mathbb{R}^{n}$: A variable vector $\mathbf{X} \in \mathbb{R}^{n \times n}$: A variable matrix An interesting aspect of differentiation rules is</description></item><item><title>Trace Trick</title><link>https://freshrimpsushi.github.io/en/posts/3674/</link><pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3674/</guid><description>Equation Suppose a scalar function $f: \mathbb{R}^{n \times n} \to \mathbb{R}$ is defined over matrix space $M(\mathbb{R}^{n \times n})$. Let $\mathrm{d}f$ be the total differential of $f$. The following holds. $$ \mathrm{d}f = \Tr (\mathrm{d}f) = \mathrm{d}\Tr (f) \tag{1} $$ $\Tr$ is the trace. Proof The function value of $f$ is a scalar, so taking the trace yields the same result. Therefore, we obtain: $$ f = \Tr(f) \implies \mathrm{d}f</description></item><item><title>Matrix Differential Dissection</title><link>https://freshrimpsushi.github.io/en/posts/3673/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3673/</guid><description>Definition $n \times n$ The differential of a matrix $\mathbf{X} = [x_{ij}] a$ is defined as follows. $$ \mathrm{d} \mathbf{X} = \begin{bmatrix} \mathrm{d} x_{11} &amp;amp; \mathrm{d} x_{12} &amp;amp; \cdots &amp;amp; \mathrm{d} x_{1n} \\ \mathrm{d} x_{21} &amp;amp; \mathrm{d} x_{22} &amp;amp; \cdots &amp;amp; \mathrm{d} x_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \mathrm{d} x_{n1} &amp;amp; \mathrm{d} x_{n2} &amp;amp; \cdots &amp;amp; \mathrm{d} x_{nn} \end{bmatrix} $$ Explanation If the generalization of the</description></item><item><title>Total Differential of Function of a Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3672/</link><pubDate>Sun, 13 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3672/</guid><description>Background Scalar Differentiation Let&amp;rsquo;s consider a scalar function $f : \mathbb{R} \to \mathbb{R}$ and ordinary differentiation. $$ \dfrac{d f}{d x} \tag{1} $$ This notation resembles a fraction, and it&amp;rsquo;s assured that it can indeed be treated as such for calculations. For example, the chain rule, which is the differentiation rule for composite functions, can be intuitively calculated as if canceling fractions, as shown below. $$ \dfrac{d f}{d t} = \dfrac{d</description></item><item><title>Matrix Calculus of Trace</title><link>https://freshrimpsushi.github.io/en/posts/3670/</link><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3670/</guid><description>Formula Let $\mathbf{X}$ be $n \times n$ matrix. Define $\dfrac{\partial }{\partial \mathbf{X}} = \nabla_{\mathbf{X}}$ as the matrix gradient. Then, the following formula holds: $$ \dfrac{\partial \Tr(\mathbf{X})}{\partial \mathbf{X}} = I, \qquad \dfrac{\partial \Tr(a\mathbf{X})}{\partial \mathbf{X}} = aI \tag{1} $$ Here, $a \in \mathbb{R}$ is a constant (scalar), and $I$ is an identity matrix. Suppose $\mathbf{A} \in \mathbb{R}^{n \times p}$ and $\mathbf{X} \in \mathbb{R}^{p \times n}$. Then, the following holds: $$ \dfrac{\partial \Tr(\mathbf{A}\mathbf{X})}{\partial</description></item><item><title>Matrix Calculus of Quadratic and Bilinear Forms</title><link>https://freshrimpsushi.github.io/en/posts/3667/</link><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3667/</guid><description>Formula For two vectors $\mathbf{a} \in \mathbb{R}^{m}$, $\mathbf{b} \in \mathbb{R}^{n}$ and a matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$, the gradient matrix of the bilinear form $\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{b}$ is as follows. $$ \nabla_{\mathbf{X}} (\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{b}) = \dfrac{\partial (\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{b})}{\partial \mathbf{X}} = \mathbf{a}\mathbf{b}^{\mathsf{T}} \tag{1} $$ As a corollary, for quadratic form $\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{a}$, the following holds. $$ \dfrac{\partial (\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{a})}{\partial \mathbf{X}} = \mathbf{a}\mathbf{a}^{\mathsf{T}} $$ For two vectors</description></item><item><title>Generalizing Differentiation: Gradient Matrices and Matrix Calculus</title><link>https://freshrimpsushi.github.io/en/posts/3666/</link><pubDate>Fri, 30 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3666/</guid><description>Definition We define the gradient matrix $\nabla_{\mathbf{X}} f$ for a scalar function $f : \mathbb{R}^{n \times n} \to \mathbb{R}$ and a matrix $\mathbf{X} = [x_{ij}] \in \mathbb{R}^{n \times n}$ as follows. $$ [\nabla_{\mathbf{X}} f]_{ij} = \dfrac{\partial f}{\partial x_{ij}} \quad (i,j=1,\dots,n) $$ $$ \nabla_{\mathbf{X}} f = \dfrac{\partial f}{\partial \mathbf{X}} = \begin{bmatrix} \dfrac{\partial f}{\partial x_{11}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f}{\partial x_{1n}} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \dfrac{\partial f}{\partial x_{n1}} &amp;amp;</description></item><item><title>Jacobi's Formula</title><link>https://freshrimpsushi.github.io/en/posts/1234/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1234/</guid><description>Formula Let $A = A(t)$ be a differentiable matrix function. The derivative of the determinant $\det A(t)$ is given by: $$ \dfrac{\mathrm{d}}{\mathrm{d} t} \det A(t) = \Tr \Big( (\operatorname{adj}A(t)) \dfrac{\mathrm{d}A(t)}{\mathrm{d}t} \Big) = \det A(t) \cdot \Tr\left( A^{-1}(t) \dfrac{\mathrm{d}A(t)}{\mathrm{d}t} \right) $$ This is known as Jacobi&amp;rsquo;s formula. In the form of a total differential, it can be written as follows. The second equality holds when $A$ is an invertible matrix. $$</description></item><item><title>Angle Between Two Vectors in an n-Dimensional Euclidean Space</title><link>https://freshrimpsushi.github.io/en/posts/1408/</link><pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1408/</guid><description>Definition1 $n$Vector space of dimension n, for two vectors $\mathbf{v}, \mathbf{u} \in \mathbb{R}^{n}$, the angle between them is defined as the $\theta$ satisfying the following condition. $$ \cos \theta = \dfrac{\mathbf{v} \cdot \mathbf{u}}{\|\mathbf{v}\| \|\mathbf{u}\|} \tag{1} $$ Here, $\cdot$ is the inner product. Explanation In 2D or 3D spaces, since vectors can be visualized as arrows, the concept of the &amp;ldquo;angle between two vectors&amp;rdquo; can be understood intuitively and geometrically represented.</description></item><item><title>A Constant-Magnitude Vector-Valued Function is Orthogonal to Its Derivative</title><link>https://freshrimpsushi.github.io/en/posts/1411/</link><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1411/</guid><description>Theorem1 For a vector function $\mathbf{r} : \mathbb{R} \to \mathbb{R}^{n}$, if $|\mathbf{r}(t)| = c$ then the following holds. ($c$ is a constant) $$ \mathbf{r}(t) \perp \mathbf{r}^{\prime}(t) \quad \forall t $$ Explanation An example can be given for uniform circular motion with a constant radius. In this case, the velocity vector and the acceleration vector are always perpendicular to each other. Proof By the property of the dot product, $$ \mathbf{r}</description></item><item><title>Limits and Continuity of Vector-Valued Functions</title><link>https://freshrimpsushi.github.io/en/posts/1418/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1418/</guid><description>Definition1 Let the vector function $\mathbf{r} : \mathbb{R} \to \mathbb{R}^{3}$ for three scalar functions $f, g, h: \mathbb{R} \to \mathbb{R}$ be given as follows. $$ \mathbf{r}(t) = \left( f(t), g(t), h(t) \right) $$ Define the limit of $\mathbf{r}$ at $a$ as follows. $$ \lim\limits_{t \to a} \mathbf{r}(t) = \left( \lim\limits_{t \to a} f(t), \lim\limits_{t \to a} g(t), \lim\limits_{t \to a} h(t) \right) $$ We say that $\mathbf{r}$ is continuous at</description></item><item><title>Derivative of a Vector-Valued Function</title><link>https://freshrimpsushi.github.io/en/posts/1419/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1419/</guid><description>Definition1 For the vector function $\mathbf{r} : I \subset \mathbb{R} \to \mathbb{R}^{3}$, if the following limit exists, then we say that $\mathbf{r}$ is differentiable at $t$ and its value is called the derivative of $\mathbf{r}$ at $t$. $$ \dfrac{d \mathbf{r}}{d t} = \mathbf{r}^{\prime}(t) := \lim_{h \to 0} \dfrac{\mathbf{r}(t+h) - \mathbf{r}(t)}{h} $$ If for all $t \in I$ there exists $\mathbf{r}^{\prime}(t)$, then we say that $\mathbf{r}$ is differentiable at $I$. When</description></item><item><title>Why Notation of Partial Differential is Different?</title><link>https://freshrimpsushi.github.io/en/posts/2573/</link><pubDate>Fri, 24 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2573/</guid><description>Question In partial derivatives, unlike the usual derivatives, expressions like $\displaystyle {{ \partial f } \over { \partial t }}$ are used instead of $\displaystyle {{ d f } \over { d t }}$. $\partial$ is read as &amp;ldquo;Round Dee&amp;rdquo; or &amp;ldquo;Partial,&amp;rdquo; and historically, it originated from &amp;ldquo;Curly Dee,&amp;rdquo; which is a cursive form of $d$1. In code, it&amp;rsquo;s \partial, and in Korea, some people even shorten it to just</description></item><item><title>Sum of Squared Residuals' Gradient</title><link>https://freshrimpsushi.github.io/en/posts/2565/</link><pubDate>Wed, 08 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2565/</guid><description>Overview Many regression problems in statistics and machine learning use the sum of squared residuals as an objective function, and in particular when $f$ is a linear combination it can be compactly expressed in matrix form. $$ \begin{align*} RSS =&amp;amp; \sum_{k} \left( y_{k} - f \left( \mathbf{x}_{k} \right) \right)^{2} \\ =&amp;amp; \sum_{k} \left( y_{k} - \left( s_{0} + s_{1} x_{k1} + \cdots + s_{p} x_{kp} \right) \right)^{2} \\ =&amp;amp; \left(</description></item><item><title>n-Dimensional Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/3229/</link><pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3229/</guid><description>Definition1 Let&amp;rsquo;s say the Cartesian coordinates of point $x \in \mathbb{R}^{n}$ are $x_{1}, \dots, x_{n}$. Then, the relationship with its polar coordinates $r, \varphi_{1}, \dots, \varphi_{n-1}$ is as follows. $$ \begin{align*} x_{n} &amp;amp;= r \cos \varphi_{1} \\ x_{n-1} &amp;amp;= r \sin \varphi_{1} \sin \varphi_{2} \\ x_{n-2} &amp;amp;= r \sin \varphi_{1} \cos \varphi_{2} \\ \vdots&amp;amp; \\ x_{4} &amp;amp;= r \sin \varphi_{1} \sin \varphi_{2} \cdots \sin \varphi_{n-3} \sin \varphi_{n-2} \\ x_{3} &amp;amp;=</description></item><item><title>Taylor's Theorem for Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/3163/</link><pubDate>Thu, 23 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3163/</guid><description>Theorem1 Let $f : \mathbb{R}^{n} \to \mathbb{R}$ be $C^{k}$ function, and call it $\mathbf{a} = (a_{1}, \dots, a_{n}) \in \mathbb{R}^{n}$. Then, there exists $C^{k-2}$ function $h_{ij}$ that satisfies the following. $$ f(\mathbf{x}) = f(\mathbf{a}) + \sum_{i} (x_{i} - a_{i})\dfrac{\partial f}{\partial x_{i}}(\mathbf{a}) + \sum_{i,j}h_{ij}(\mathbf{x})(x_{i} - a_{i}) (x_{j} - a_{j}) $$ Description It generalizes the Taylor theorem to functions of several variables. second-order $$ \begin{align*} f(\mathbf{x}) &amp;amp;= f(\mathbf{a}) + \sum\limits_{i=1}^{n} (x_{i} -</description></item><item><title>Inverse Function Theorem in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/3139/</link><pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3139/</guid><description>Theorem1 Let&amp;rsquo;s say a function $\mathbf{f} : E \subset \mathbb{R}^{n} \to \mathbb{R}^{n}$ defined in an open set $E$ is a $C^{1}$-function. For $\mathbf{a} \in E$, let&amp;rsquo;s assume that $\mathbf{f}^{\prime}(\mathbf{a})$ is invertible and $\mathbf{b} = \mathbf{f}(\mathbf{a})$. Then, the following holds. (a) There exists an open set $U, V \subset \mathbb{R}^{n}$ where $\mathbf{a} \in U, \mathbf{b} \in V$, and over $U$, $\mathbf{f}$ is one-to-one and $\mathbf{f}(U) = V$. (b) If $\mathbf{g}$ is</description></item><item><title>Jacobian of Composite Functions</title><link>https://freshrimpsushi.github.io/en/posts/3138/</link><pubDate>Wed, 03 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3138/</guid><description>Theorem Let&amp;rsquo;s assume we have two functions $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ and $g : \mathbb{R}^{m} \to \mathbb{R}^{k}$. We denote the Jacobian of $f$ as $J(f)$. Then, the following holds. $$ J(g \circ f) = J(g) J(f) $$ Explanation Since the Jacobian is the most generalized derivative, the above theorem is a generalization of the chain rule. Proof By definition of the Jacobian, $$ J(g \circ f) = \begin{bmatrix} \dfrac{\partial</description></item><item><title>Chain Rule for Multivariable Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/3134/</link><pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3134/</guid><description>Theorem Let&amp;rsquo;s assume that two functions $\mathbf{g} : D \subset \mathbb{R}^{m} \to \mathbb{R}^{k}$, $\mathbf{f} : \mathbf{g}(\mathbb{R}^{k}) \subset \mathbb{R}^{k} \to \mathbb{R}^{n}$ are differentiable. Then, the composition of these two functions $\mathbf{F} = \mathbf{f} \circ \mathbf{g} : \mathbb{R}^{m} \to \mathbb{R}^{n}$ is also differentiable, and the (total) derivative of $\mathbf{F}$ satisfies the following. $$ \mathbf{F}^{\prime}(\mathbf{x}) = \mathbf{f}^{\prime}\left( \mathbf{g}(\mathbf{x}) \right) \mathbf{g}^{\prime}(\mathbf{x}) $$ Explanation This is called the chain rule. If we denote $\mathbf{x} =</description></item><item><title>Definition of Directional Derivative</title><link>https://freshrimpsushi.github.io/en/posts/3109/</link><pubDate>Mon, 06 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3109/</guid><description>Buildup Let&amp;rsquo;s say a multivariable function $f = \mathbb{R}^{n} \to \mathbb{R}$ is given. When trying to calculate the derivative of $f$, unlike the case with a univariable function, one must consider the rate of change in &amp;lsquo;which direction&amp;rsquo;. A familiar example is the partial derivative. The partial derivative considers the rate of change with respect to only one variable. For instance, the partial derivative $\dfrac{\partial f}{\partial y}$ of $f=f(x,y,z)$ with</description></item><item><title>Conformal Mapping</title><link>https://freshrimpsushi.github.io/en/posts/3102/</link><pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3102/</guid><description>Definition1 Assuming the mapping $\mathbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is given as follows. $$ \mathbf{f}(\mathbf{x}) = \left( f_{1}(\mathbf{x}), f_{2}(\mathbf{x}), \dots, f_{m}(\mathbf{x}) \right),\quad \mathbf{x}\in \R^{n} $$ The total derivative, or Jacobian matrix of $\mathbf{f}$ is as follows. $$ \mathbf{f}^{\prime} = J = \begin{bmatrix} \dfrac{\partial f_{1}}{\partial x_{1}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_{1}}{\partial x_{n}} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \dfrac{\partial f_{m}}{\partial x_{1}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_{m}}{\partial x_{n}} \end{bmatrix} $$ If</description></item><item><title>Partial Derivatives: Derivatives of Multivariable Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/3082/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3082/</guid><description>Buildup[^1] Recall the definition of the derivative of a univariate function. $$ \lim \limits_{h\to 0} \dfrac{f(x+h) - f(x)}{h} = f^{\prime}(x) $$ By approximating the numerator on the left-hand side as a linear function of $h$, we get the following. $$ \begin{equation} f(x+h) - f(x) = a h + r(h) \label{1} \end{equation} $$ Let&amp;rsquo;s call $r(h)$ the remainder, satisfying the condition below. $$ \lim \limits_{h \to 0} \dfrac{r(h)}{h}=0 $$ Then, dividing</description></item><item><title>Laplacian of a Scalar Field</title><link>https://freshrimpsushi.github.io/en/posts/3081/</link><pubDate>Mon, 12 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3081/</guid><description>Definition The divergence of the gradient of the scalar function $u : \mathbb{R}^{n} \to \mathbb{R}$ is called the Laplacian and is denoted as follows. $$ \begin{align*} \Delta u :&amp;amp;= \mathrm{div}(\nabla (u)) \\ &amp;amp;= \mathrm{div} \left( \left( u_{x_{1}}, u_{x_{2}}, \dots, u_{x_{n}} \right) \right) \\ &amp;amp;= u_{x_{1}x_{1}} + u_{x_{2}x_{2}} + \cdots + u_{x_{n}x_{n}} \\ &amp;amp;= \sum _{i=1}^{n} u_{x_{i}x_{i}} \end{align*} $$ Here, $u_{x_{i}}=\dfrac{\partial u}{\partial x_{i}}$ is. Explanation In mathematics, the divergence is often</description></item><item><title>Integration of Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/1902/</link><pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1902/</guid><description>Definition1 Let $I^{k}$ be a k-cell, and assume $\mathbf{x} \in I^{k}$. $$ \mathbf{x} = (x_{1},\dots,x_{k}),\quad a_{i} \le x_{i} \le b_{i} (i=1,\dots,k) $$ Suppose $f: I^{k} \to \mathbb{R}$ is continuous. Then, since it is integrable, let us set it as $f=f_{k}$, and define $f_{k-1} : I^{k-1} \to \mathbb{R}$ as follows: $$ f_{k-1} (x_{1}, \dots, x_{k-1}) = \int_{a_{k}}^{b_{k}} f_{k}(x_{1}, \dots, x_{k}) dx_{k} $$ Then, by the Leibniz Rule, $f_{k-1}$ is continuous in</description></item><item><title>Partial Derivatives</title><link>https://freshrimpsushi.github.io/en/posts/3036/</link><pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3036/</guid><description>Definitions1 Let us define $E\subset \mathbb{R}^{n}$ as an open set, and $\mathbf{x}\in E$, and $\mathbf{f} : E \to \mathbb{R}^{m}$. Let $\left\{ \mathbf{e}_{1}, \mathbf{e}_{2}, \dots, \mathbf{e}_{n} \right\}$, and $\left\{ \mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{m} \right\}$ be the standard basis of $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$, respectively. Then, the components $f_{i} : \mathbb{R}^{n} \to \mathbb{R}$ of $\mathbf{f}$ are defined as follows. $$ \mathbf{f} (\mathbf{x}) = \sum_{i=1}^{m} f_{i}(\mathbf{x})\mathbf{u}_{i}, \quad \mathbf{x} \in E $$ or $$ f_{i}</description></item><item><title>Derivatives of Vectors and Matrices</title><link>https://freshrimpsushi.github.io/en/posts/1926/</link><pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1926/</guid><description>Gradient of a Scalar Function Scalar function $f : \mathbb{R}^{n} \to \mathbb{R}$&amp;rsquo;s gradient is as follows. $$ \frac{ \partial f(\mathbf{x})}{ \partial \mathbf{x} } := \nabla f(\mathbf{x}) = \begin{bmatrix} \dfrac{ \partial f(\mathbf{x})}{ \partial x_{1} } &amp;amp; \dfrac{ \partial f(\mathbf{x})}{ \partial x_{2} } &amp;amp; \cdots &amp;amp; \dfrac{ \partial f(\mathbf{x})}{ \partial x_{n} } \end{bmatrix}^{T} $$ Here, $\dfrac{ \partial f(\mathbf{x})}{ \partial x_{i} }$ is the partial derivative of $f$ with respect to $x_{i}$. Inner</description></item><item><title>Integration of Vector-Valued Functions</title><link>https://freshrimpsushi.github.io/en/posts/1868/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1868/</guid><description>Definition1 Let $f_{1}$, $f_{2}$, $\dots$, $f_{k}$ be functions taking real values on the interval $[a,b]$. And suppose $\mathbf{f} : [a,b] \to \mathbb{R}^{k}$ is defined as follows. $$ \mathbf{f}(x)=\left( f_{1}(x),\dots,f_{k}(x) \right),\quad x\in [a,b] $$ If each $f_{k}$ is integrable on the interval $[a,b]$, then the integral of $\mathbf{f}$ is defined as follows. $$ \int _{a} ^{b} \mathbf{f}dx = \left( \int _{a} ^{b}f_{1} dx, \dots, \int _{a} ^{b}f_{k} dx \right) $$ Theorem</description></item><item><title>Divergence in Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/1777/</link><pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1777/</guid><description>Definition In a Euclidean space, a vector field $\textbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ represented as $\textbf{f} = (f_{1} , \cdots , f_{n})$ and the direction of the axis as $u_{1} , \cdots , u_{n}$, the divergence of $\textbf{f}$ is defined as follows. $$ \operatorname{div} \textbf{f} := \nabla \cdot \textbf{f} = \sum_{k=1}^{n} {{ \partial f_{k} } \over { \partial u_{k} }} $$ Explanation The divergence of a vector field serves as</description></item><item><title>Volume in Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/1772/</link><pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1772/</guid><description>Definition The volume $V$ of a subspace $D \subset \mathbb{R}^{n}$ in a Euclidean space is defined as follows when expressed in Cartesian coordinates $\textbf{u} = (u_{1}, u_{2}, \cdots , u_{n})$. $$ V(D) = \int_{D} du_{1} du_{2} \cdots d u_{n} $$ When $\textbf{u} \in \mathbb{R}^{n}$ is transformed by a vector function $\textbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ as $\textbf{f} \left( \textbf{u} \right) = \left( f_{1} (\textbf{u}) , \cdots , f_{n} (\textbf{u}) \right)$,</description></item><item><title>Total Differentiation, Exact Differentiation</title><link>https://freshrimpsushi.github.io/en/posts/1773/</link><pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1773/</guid><description>Definition Let&amp;rsquo;s assume that a multivariable function $f : \mathbb{R}^{n} \to \mathbb{R}$ is given. The change of $f(\mathbf{x})$ according to the change of variable $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n})$ is denoted as $df$, and this is called the total differential or exact differential of $f$. $$ \begin{equation} df = \frac{ \partial f}{ \partial x_{1} }dx_{1} + \frac{ \partial f}{ \partial x_{2} }dx_{2} + \cdots + \frac{ \partial f}{ \partial</description></item><item><title>Gradient of Scalar Field</title><link>https://freshrimpsushi.github.io/en/posts/1010/</link><pubDate>Sun, 28 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1010/</guid><description>Definition A gradient, specifically referred to as the total derivative of a scalar field $f : \mathbb{R}^{n} \to \mathbb{R}$, is denoted by $\nabla f$. $$ \begin{align*} \nabla f := f^{\prime} =&amp;amp; \begin{bmatrix} D_{1}f &amp;amp; D_{2}f &amp;amp; \cdots &amp;amp; D_{n}f\end{bmatrix} \\ =&amp;amp; \begin{bmatrix} \dfrac{\partial f}{\partial x_{1}} &amp;amp; \dfrac{\partial f}{\partial x_{2}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f}{\partial x_{n}} \end{bmatrix} \\ =&amp;amp; \dfrac{\partial f}{\partial x_{1}}\hat{x}_{1} + \dfrac{\partial f}{\partial x_{2}}\hat{x}_{2} + \dots + \dfrac{\partial f}{\partial</description></item><item><title>What is a Hessian Matrix?</title><link>https://freshrimpsushi.github.io/en/posts/992/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/992/</guid><description>Definition $D \subset \mathbb{R}^{n}$ is defined as the matrix $H \in \mathbb{R}^{n \times n}$ for a multivariate scalar function $f : D \to \mathbb{R}$ is called the Hessian matrix of $f$. $$ H := \begin{bmatrix} {{\partial^2 f } \over {\partial x_{1}^2 }} &amp;amp; \cdots &amp;amp; {{\partial^2 f } \over { \partial x_{1} \partial x_{n} }} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ {{\partial^2 f } \over {\partial x_{n} \partial x_{1}</description></item><item><title>Jacobian Matrix or Jacobi Matrix</title><link>https://freshrimpsushi.github.io/en/posts/989/</link><pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/989/</guid><description>Definition Let a multivariable vector function $\mathbf{f} : D \to \mathbb{R}^{m}$ defined by $D \subset \mathbb{R}^{n}$ be defined for each scalar function $f_{1} , \cdots , f_{m} : D \to \mathbb{R}$ as follows: $$ \mathbf{f} ( x_{1} , \cdots , x_{n} ) : = \begin{bmatrix} f_{1} ( x_{1} , \cdots , x_{n} ) \\ \vdots \\ f_{m} ( x_{1} , \cdots , x_{n} ) \end{bmatrix} $$ It is called the</description></item><item><title>Scalar Functions and Vector-valued Functions</title><link>https://freshrimpsushi.github.io/en/posts/970/</link><pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/970/</guid><description>Definition Let $D$ be a subset $D\subset \mathbb{R}^{n}$ of the $n$-dimensional Euclidean space. Functions having $D$ as their domain are called function of several variables. $f : D \to \mathbb{R}$ is called a scalar function. For a scalar function $f_{1} , \cdots , f_{m} : D \to \mathbb{R}$, $\mathbf{f} : D \to \mathbb{R}^{m}$ defined as follows is called a vector-valued function. $$ \mathbf{f} ( x_{1} , \cdots , x_{n} )</description></item><item><title>Proof of the Pappus-Guldin Theorem</title><link>https://freshrimpsushi.github.io/en/posts/685/</link><pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/685/</guid><description>Theorem Let the area of a shape $F$ on the plane be denoted as $A$, and let the volume of the solid of revolution $W$ obtained by rotating $F$ around axis $z$ be denoted as $V$. If the distance between the center of mass of $F$ and the axis $z$ is denoted as $r$, then $$ V = 2 \pi r A $$ Description The Pappus-Guldin Theorem is often mentioned</description></item><item><title>Inner product in Euclidean space</title><link>https://freshrimpsushi.github.io/en/posts/255/</link><pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/255/</guid><description>Definition Let&amp;rsquo;s say $V = \mathbb{R}^n$ for a vector space, and also $\mathbf{x}, \mathbf{y}, \mathbf{z} \in V$ and $k \in \mathbb{R}$. $\left&amp;lt; \cdot , \cdot \right&amp;gt; : V^2 \to \mathbb{R}$ is defined as the inner product on $V$ when it satisfies the following four conditions: (1) Symmetry: $\left&amp;lt; \mathbf{x} , \mathbf{y} \right&amp;gt; = \left&amp;lt; \mathbf{y}, \mathbf{x} \right&amp;gt;$ (2) Additivity: $\left&amp;lt; \mathbf{x} + \mathbf{y} , \mathbf{z} \right&amp;gt; = \left&amp;lt; \mathbf{x}, \mathbf{z}</description></item><item><title>Euclidean Space</title><link>https://freshrimpsushi.github.io/en/posts/205/</link><pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/205/</guid><description>Definition For a natural number $n \in \mathbb{N}$, the Cartesian product $\mathbb{R}$ of the set of real numbers is called the Euclidean space. $$ \mathbb{R}^{n} = \mathbb{R} \times \cdots \times \mathbb{R} $$ $\mathbb{R}^{1}$ is referred to as real space or number line. $\mathbb{R}^{2}$ is called a plane. $\mathbb{R}^{3}$ is called a $3$-dimensional space. Here, $\mathbb{N} := \left\{ 1, 2, 3, \cdots \right\}$ means the set that includes all natural numbers.</description></item></channel></rss>