<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>확률론 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%ED%99%95%EB%A5%A0%EB%A1%A0/</link>
    <description>Recent content in 확률론 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 08 Sep 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%99%95%EB%A5%A0%EB%A1%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Summary of Measure Theory and Probability Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/3473/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3473/</guid>
      <description>Overview This is a summary of definitions and concepts for those who have already studied measure theory and probability. It is intended to be viewed when definitions are confusing or unrecognizable, and when a general review is needed. Measure Theory Algebras An algebra of sets on nonempty set $X$ is a nonempty collection $\mathcal{A}$ of subsets of $X$ is colsed under finite unions ans complements. $\sigma$-algebra is an algebra that</description>
    </item>
    <item>
      <title>Transition Probabilities of Stochastic Processes</title>
      <link>https://freshrimpsushi.github.io/en/posts/2284/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2284/</guid>
      <description>Definition Let us assume there is a stochastic process $\left\{ X_{t} \right\}$ with a countable set as its state space. For two points in time $t_{1} &amp;lt; t_{2}$, the transition probability $p_{ij} \left( t_{1} , t_{2} \right)$ is defined as follows: $$ p_{ij} \left( t_{1} , t_{2} \right) := P \left( X_{t_{2}} = j \mid X_{t_{1}} = i \right) $$ Here, the (current) state represented by $i$ is referred to</description>
    </item>
    <item>
      <title>Geometric Brownian Motion</title>
      <link>https://freshrimpsushi.github.io/en/posts/2154/</link>
      <pubDate>Sun, 06 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2154/</guid>
      <description>Definition 1 Let&amp;rsquo;s say the following Stochastic Differential Equation (SDE) is given by $\mu \in \mathbb{R}$ and $\sigma^{2} &amp;gt; 0$. $$ d X_{t} = X_{t} \left( \mu dt + \sigma d B_{t} \right) $$ The solution of this SDE is found as a Stochastic Process for the initial value $X_{0}$, which is referred to as Geometric Brownian Motion. $$ X_{t} = X_{0} \exp \left[ \left( \mu - {{ \sigma^{2} }</description>
    </item>
    <item>
      <title>Shannon Entropy: Entropy Defined by Random Variables</title>
      <link>https://freshrimpsushi.github.io/en/posts/2035/</link>
      <pubDate>Sun, 13 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2035/</guid>
      <description>Overview Shannon Entropy or Information Entropy is a measure of disorder defined by a probability variable, and can be viewed as a quantification of how uncertain it is in a probability distribution. Easy and Complex Definitions Discrete Entropy 1 When the probability mass function of a discrete random variable $X$ is $p(x)$, the entropy of $X$ is represented as follows. $$ H(X) := - \sum p(x) \log_{2} p(x) $$ Continuous</description>
    </item>
    <item>
      <title>Proof of the Continuity Mapping Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/1787/</link>
      <pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1787/</guid>
      <description>Theorem 1 The following is a measure-theoretic description of the continuous mapping theorem. For metric spaces $\left( S , d \right)$ and $\left( S&#39; , d&amp;rsquo; \right)$, let us say $g : S \to S&#39;$ is continuous from $C_{g} \subset S$. For a random element $X$ in $S$, concerning a sequence of random elements converging to $X$ in $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$, the following holds if $P \left( X</description>
    </item>
    <item>
      <title>Convergence of Distributions Defined by Measure Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/1432/</link>
      <pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1432/</guid>
      <description>Definition Let&amp;rsquo;s define a measurable space $(S,\mathcal{S})$ with respect to the Borel sigma field $\mathcal{S}:= \mathcal{B}(S)$ of a metric space $S$. When random variables $X$ and stochastic processes $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$ defined in a probability space $(\Omega, \mathcal{F}, P)$ are $n \to \infty$, for all $f \in C_{b}(S)$, if the following is satisfied, then it is said to Converge in Distribution $X$ and is denoted as $X_{n} \overset{D}{\to}</description>
    </item>
    <item>
      <title>Convergence of Probabilities Defined by Measure Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/1397/</link>
      <pubDate>Sat, 23 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1397/</guid>
      <description>Probability Convergence Defined Rigorously Given a probability space $( \Omega , \mathcal{F} , P)$. A sequence of random variables $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$ is said to converge in probability to a random variable $X$ if it converges in measure to $X$, denoted as $X_{n} \overset{P}{\to} X$. If you&amp;rsquo;re not yet familiar with measure theory, the term probability space can be disregarded. Explanation The convergence of $\left\{ X_{n} \right\}_{n \in</description>
    </item>
    <item>
      <title>Stopping Times in Stochastic Processes</title>
      <link>https://freshrimpsushi.github.io/en/posts/1351/</link>
      <pubDate>Sat, 26 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1351/</guid>
      <description>Definitions Let&amp;rsquo;s assume a probability space $( \Omega , \mathcal{F} , P)$ is given. A random variable $\tau$ with an integer value greater than or equal to $0$ for all $n \in \mathbb{N}_{0}$ that satisfies $(\tau = n) \in \mathcal{F}_{n}$ with respect to the filtration $\left\{ \mathcal{F}_{n} \right\}$ is called a Stopping Time. For a Borel set $B \in \mathcal{B}(\mathbb{R})$, $(\tau \in B) = \tau^{-1} (B)$ is, therefore, the same</description>
    </item>
    <item>
      <title>The Definition of Martingale</title>
      <link>https://freshrimpsushi.github.io/en/posts/1349/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1349/</guid>
      <description>Definition Let&amp;rsquo;s assume that a probability space $( \Omega , \mathcal{F} , P)$ is given. A sequence $\left\{ \mathcal{F}_{n} \right\}_{n \in \mathbb{N}}$ of sub-σ-fields of $\mathcal{F}$ is called a filtration if it satisfies the following: $$ \forall n \in \mathbb{N}, \mathcal{F}_{n} \subset \mathcal{F}_{n+1} $$ Given a filtration $\left\{ \mathcal{F}_{n} \right\}_{n \in \mathbb{N}}$, a sequence $\left\{ X_{n} \right\}_{n \in \mathbb{N}}$</description>
    </item>
    <item>
      <title>Properties of Conditional Expectation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1322/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1322/</guid>
      <description>Theorem Given a probability space $( \Omega , \mathcal{F} , P)$: [1] From measure theory: If measurable functions $f$, $g$ are $\mathcal{F}$-measurable, then there exists a Borel function $h : \mathbb{R} \to \mathbb{R}$ satisfying $g = h (f)$. [2] Application in probability theory: If random variables $X$, $Y$ are $\sigma (X)$-measurable, then there exists a Borel function $h : \mathbb{R} \to \mathbb{R}$ satisfying $E(Y | X) = h(X)$. [3]: If</description>
    </item>
    <item>
      <title>Conditional Expectation of Random Variables Defined by Measure Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/1315/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1315/</guid>
      <description>Definition Let&amp;rsquo;s assume a probability space $( \Omega , \mathcal{F} , P)$ is given. If $\mathcal{G}$ is a sub sigma-field of $\mathcal{F}$ and the random variable $X \in \mathcal{L}^{1} ( \Omega )$ is integrable, for all $A \in \mathcal{G}$, $$ \int_{A} Y d P = \int_{A} X d P $$ a $\mathcal{G}$-measurable random variable $Y$ uniquely exists satisfying the above, then $Y := E ( X | \mathcal{G} )$ is</description>
    </item>
    <item>
      <title>Probability Variables and Probability Distributions Defined by Measure Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/1288/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1288/</guid>
      <description>Definition 1 Let&amp;rsquo;s assume a Probability Space $( \Omega , \mathcal{F} , P)$ is given. A function $X : \Omega \to \mathbb{R}$ that satisfies $X^{-1} (B) \in \mathcal{F}$ for every Borel Set $B \in \mathcal{B} (\mathbb{R})$ is called a Random Variable. $\mathcal{F}_{X}$ defined as follows is called the Sigma Field generated by $X$. $$ \mathcal{F}_{X} := X^{-1} ( \mathcal{B} ) = \sigma (X) = \left\{ X^{-1} (B) \in \Omega :</description>
    </item>
    <item>
      <title>Wiener Process</title>
      <link>https://freshrimpsushi.github.io/en/posts/957/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/957/</guid>
      <description>Definition When $s&amp;lt; t &amp;lt; t+u$, a stochastic process $\left\{ W_{t} \right\}$ that satisfies the following conditions is called a Wiener Process: (i): $W_{0} = 0$ (ii): $\left( W_{t+u} - W_{t} \right) \perp W_{s}$ (iii): $\left( W_{t+u} - W_{t} \right) \sim N ( 0, u )$ (iv): The sample paths of $W_{t}$ are almost surely continuous. Basic Properties [1]: $\displaystyle W_{t} \sim N ( 0 , t )$ [2]: $\displaystyle</description>
    </item>
    <item>
      <title>Generalized Random Walk</title>
      <link>https://freshrimpsushi.github.io/en/posts/870/</link>
      <pubDate>Thu, 21 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/870/</guid>
      <description>Definition A stochastic process $\left\{ X_{n} \right\}$ whose state space is the set of integers $\left\{ \cdots , -2 , -1, 0 , 1 , 2 , \cdots \right\}$ and starts from state $0$ is referred to as a generalized random walk if the probability of decreasing by $1$ in the next step is $p$, and the probability of increasing by $1$ is $(1-p)$. Explanation A random walk, which is</description>
    </item>
    <item>
      <title>Discrete Markov Chains</title>
      <link>https://freshrimpsushi.github.io/en/posts/859/</link>
      <pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/859/</guid>
      <description>Definition A discrete-time Markov chain (DTMC), or simply a Markov Chain, is a discrete stochastic process $\left\{ X_{n} \right\}$ satisfying the following, provided the state space is a countable set: $$ p \left( X_{n+1} = j \mid X_{n} = i , X_{n-1} = k , \cdots , X_{0} = l \right) = p \left( X_{n+1} = j \mid X_{n} = i \right) $$ See Also Continuous Markov Chain Description $p_{ij}:=</description>
    </item>
    <item>
      <title>What is a Stochastic Process?</title>
      <link>https://freshrimpsushi.github.io/en/posts/857/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/857/</guid>
      <description>Definition The range of the random variable $X: \Omega \to E$ is called the state space. The set of random variables $\left\{ X_{t} \mid t \in [ 0 , \infty ) \right\}$ is called a continuous stochastic process. The sequence of random variables $\left\{ X_{n} \mid n = 0, 1, 2, \cdots \right\}$ is called a discrete stochastic process. Explanation The term &amp;lsquo;process&amp;rsquo; in stochastic process often makes the concept</description>
    </item>
    <item>
      <title>Independence of Events and Conditional Probability</title>
      <link>https://freshrimpsushi.github.io/en/posts/521/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/521/</guid>
      <description>Definition 1 Let&amp;rsquo;s assume a probability space $(\Omega , \mathcal{F} , P)$ is given. For $P(B)&amp;gt;0$, $\displaystyle P (A | B) = {{P(A \cap B)} \over {P(B)}}$ is called the conditional probability of $A$ given $B$. If $P(A | B) = P(A)$, that is $P( A \cap B) = P(A) \cdot P(B)$, then $A, B$ are considered independent. If you haven&amp;rsquo;t yet encountered measure theory, you can ignore the term</description>
    </item>
    <item>
      <title>Probability Defined by Measure Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/498/</link>
      <pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/498/</guid>
      <description>Definition 1 Let&amp;rsquo;s say $\mathcal{F}$ is a sigma field of set $\Omega$. Measurable set $E \in \mathcal{F}$ is called an Event. On $\mathcal{F}$, if measure $P : \mathcal{F} \to \mathbb{R}$ satisfies $P(\Omega) = 1$, then $P$ is called Probability. $( \Omega, \mathcal{F} , P )$ is called the Probability Space. Explanation Borrowing the strength of measure theory, we can provide a mathematical foundation for various concepts of probability theory and</description>
    </item>
  </channel>
</rss>
