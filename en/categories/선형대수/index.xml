<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>선형대수 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98/</link>
    <description>Recent content in 선형대수 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 25 Nov 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Quadratic Form</title>
      <link>https://freshrimpsushi.github.io/en/posts/3512/</link>
      <pubDate>Sat, 25 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3512/</guid>
      <description>Definition $V$ is called a $n$dimensional vector space. For a given constant $a_{ij} \in \mathbb{R}(\text{or } \mathbb{C})$, the following second order homogeneous function $A : V \to \mathbb{R}(\text{or } \mathbb{C})$ is called a quadratic form. $$ A(\mathbf{x}) := \sum\limits_{i,j=1}^{n} a_{ij}x_{i}x_{j},\qquad (a_{ij} = a_{ji}) $$ Here, $\mathbf{x} = \begin{bmatrix} x_{1} &amp;amp; \cdots &amp;amp; x_{n} \end{bmatrix}^{T}$ holds. The term $i \ne j$ for $a_{ij}x_{i}x_{j}$ is called the cross product terms. Explanation According</description>
    </item>
    <item>
      <title>Tensor Product of Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/3414/</link>
      <pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3414/</guid>
      <description>Buildup1 For convenience, the discussion unfolds with respect to the complex number space $\mathbb{C}$, but it could equally apply to $\mathbb{R}$ or any arbitrary vector space. Let&amp;rsquo;s denote the set of functions from a finite set $\Gamma$ to a complex number space as $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ Let $\Gamma$ be $\mathbf{n} = \left\{ 1, 2, \dots, n \right\}$. If we denote a</description>
    </item>
    <item>
      <title>Invariant Subspaces of Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/3353/</link>
      <pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3353/</guid>
      <description>Overview Let $\beta = v_{1}, \dots, v_{k}$ be the set of eigenvectors of the linear transformation $T : V \to V$. Then, it can be understood that $T$ maps $\span{\beta}$ to $\span{\beta}$. A subspace that maps itself to itself in this manner is defined as an invariant subspace. Definition1 Let $V$ be a vector space, and $T : V \to V$ a linear transformation. A subspace $W$ is called an</description>
    </item>
    <item>
      <title>Characteristics Polynomial of Linear Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3339/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3339/</guid>
      <description>Overview The characteristic polynomial of linear transformation is defined. From the theorem below, it can be seen that solving equation $\det(A - \lambda I) = 0$ is equivalent to finding the eigenvalues. Therefore, it is quite natural to name $\det(A - \lambda I)$ the characteristic polynomial. Theorem1 Let&amp;rsquo;s say $F$ is any field, and $A \in M_{n\times n}(F)$. That $\lambda \in F$ is an eigenvalue of $A$ is equivalent to</description>
    </item>
    <item>
      <title>Eigenvalues and Eigenvectors of Finite-Dimensional Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3337/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3337/</guid>
      <description>Definition1 Let $V$ be a finite-dimensional $F$-vector space. Let $T : V \to V$ be a linear transformation. For $\lambda \in F$, $$ Tx = \lambda x $$ a non-zero vector $x \in V$ satisfying this is called an eigenvector of $T$. The scalar $\lambda \in F$ is called the eigenvalue corresponding to the eigenvector $x$. Explanation Although one might find the term eigenvector replaced by the terms characteristic vector</description>
    </item>
    <item>
      <title>Linear Functional</title>
      <link>https://freshrimpsushi.github.io/en/posts/3281/</link>
      <pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3281/</guid>
      <description>Definitions1 Let&amp;rsquo;s call $V$ a vector space. A mapping $f$ from $V$ to $\mathbb{C}$ (or $\mathbb{R}$) is called a functional. $$ f : V \to \mathbb{C} $$ If $f$ is linear, it is called a linear functional. More Detailed Definitions2 Let&amp;rsquo;s call $V$ a vector space over the field $F$. Here, the field $F$ itself becomes a $1$-dimensional vector space over $F$. A linear transformation $f : V \to F$</description>
    </item>
    <item>
      <title>Order Basis and Coordinate Vectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/3279/</link>
      <pubDate>Fri, 12 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3279/</guid>
      <description>Definition1 Let&amp;rsquo;s say $V$ is a finite-dimensional vector space. When a specific order is assigned to a basis of $V$, it is called an ordered basis. Let&amp;rsquo;s say $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ is an ordered basis of $V$. Then, due to the uniqueness of basis representation, for $\mathbf{v} \in V$, scalars $a_{i}$ uniquely exist as follows. $$ \mathbf{v} = a_{1}\mathbf{v}_{1} + \dots a_{n}\mathbf{v}_{n} $$ $a_{1},\dots,a_{n}$ is called</description>
    </item>
    <item>
      <title>Matrix Representation of Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3078/</link>
      <pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3078/</guid>
      <description>Definition1 Let us consider $V, W$ as a finite-dimensional vector space. Let&amp;rsquo;s say $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ and $\gamma = \left\{ \mathbf{w}_{1}, \dots, \mathbf{w}_{m} \right\}$ are the ordered bases for $V$ and $W$, respectively. Assume $T : V \to W$ to be a linear transformation. Due to the uniqueness of basis representation, there exist unique scalars $a_{ij}$ that satisfy the following. $$ T(\mathbf{v}_{j}) = \sum_{i=1}^{m}a_{ij}\mathbf{w}_{i} = a_{1j}\mathbf{w}_{1}</description>
    </item>
    <item>
      <title>Linear Transformation: Kernel and Range</title>
      <link>https://freshrimpsushi.github.io/en/posts/3071/</link>
      <pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3071/</guid>
      <description>Definition1 Let&amp;rsquo;s say $T : V \to W$ is a linear transformation. The set of elements of $V$ that are mapped to $\mathbf{0}$ by $T$ is called the kernel or null space, and is denoted as follows. $$ \text{ker}(T) = N(T) := \left\{ \mathbf{v} \in V : T( \mathbf{v} ) = \mathbf{0} \right\} $$ The set of images under $\mathbf{v} \in V$ by $T$ is called the range or image</description>
    </item>
    <item>
      <title>Linear Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3026/</link>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3026/</guid>
      <description>Definition1 A transformation is when a function $T : V \to W$ maps from one vector space to another, that is $V$, $W$ are both vector spaces, we call $T$ a transformation. If the transformation $T$ is a linear function, satisfying the following two conditions for any $\mathbf{v},\mathbf{u} \in V$ and scalar $k$, it is called a linear transformation: $T(k \mathbf{u}) = k T(\mathbf{u})$ $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) +</description>
    </item>
    <item>
      <title>Basis of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3017/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3017/</guid>
      <description>Definition1 Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$ be a subset of vector space $V$. If $S$ satisfies the following two conditions, then $S$ is called a basis of $V$. $S$ spans $V$. $$ V = \text{span}(S) $$ $S$ is linearly independent. Explanation As the name suggests, the concept of a basis corresponds to &amp;rsquo;the smallest thing that can create a vector space&amp;rsquo;. The condition of spanning has</description>
    </item>
    <item>
      <title>Dual Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/753/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/753/</guid>
      <description>Dual Spaces Definition 11 The set of all continuous linear functionals of a vector space $X$ is denoted by $X^{ \ast }$ and is called the dual space of $X$, simply referred to as the dual of $X$, as denoted below. $$ X^{ \ast }:=\left\{ x^{ \ast }:X\to \mathbb{C}\ |\ x^{ \ast } \text{ is continuous and linear} \right\} $$ $$ X^{ \ast }:=B(X,\mathbb{C}) $$ $B \left( X, \mathbb{C} \right)$</description>
    </item>
    <item>
      <title>In Linear Algebra, What is a Norm?</title>
      <link>https://freshrimpsushi.github.io/en/posts/257/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/257/</guid>
      <description>Definition Let us define a vector space over $V$ as $\mathbb{F}$. $\left\| \cdot \right\| : V \to \mathbb{F}$ is defined as a norm on $V$ if it satisfies the following three conditions with respect to $\mathbf{u}, \mathbf{v} \in V$ and $k \in \mathbb{F}$: (i) Positive definiteness: $\left\| \mathbf{u} \right\| \ge 0$ and $\mathbf{u} = \mathbb{0} \iff \left\| \mathbf{u} \right\| = 0$ (ii) Homogeneity: $\left\|k \mathbf{u} \right\| = | k |</description>
    </item>
    <item>
      <title>Direct Sum in Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/353/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/353/</guid>
      <description>Definition A vector space $V$ is said to be the direct sum of its two subspaces $W_{1}$ and $W_{2}$ if it satisfies the following, denoted by $V = W_{1} \oplus W_{2}$. (i) Existence: For any $\mathbf{v} \in V$, there exist $\mathbf{v}_{1} \in W_{1}$ and $\mathbf{v}_{2} \in W_{2}$ satisfying $\mathbf{v} = \mathbf{v}_{1} + \mathbf{v}_{2}$. (ii) Exclusivity: $W_{1} \cap W_{2} = \left\{ \mathbf{0} \right\}$ (iii) Uniqueness: For a given $\mathbf{v}$, there exists</description>
    </item>
    <item>
      <title>Dimension of the Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3018/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3018/</guid>
      <description>Definition1 The number of elements (vectors) of a basis for a vector space $V$ is defined as the dimension of $V$ and is denoted as follows. $$ \dim (V) $$ Explanation Such a generalization of dimensions goes beyond merely exploring vector spaces and is being applied to various technologies that support this society. It might seem pointless to consider dimensions higher than the $3$ dimensions of our world and the</description>
    </item>
    <item>
      <title>Linear Independence and Linear Dependence</title>
      <link>https://freshrimpsushi.github.io/en/posts/253/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/253/</guid>
      <description>Definition1 Let&amp;rsquo;s denote a non-empty subset of vector space $V$ as $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$. For constants $k_{1}, k_{2}, \dots, k_{r}$, the following equation $$ k_{1} \mathbf{v}_{1} + k_{2} \mathbf{v}_{2} + \dots + k_{r} \mathbf{v}_{r} = \mathbf{0} $$ has at least one solution $$ k_{1} = 0,\ k_{2} = 0,\ \dots,\ k_{r} = 0 $$ This is called a trivial solution. If the trivial solution is</description>
    </item>
    <item>
      <title>Linear Combination, Span</title>
      <link>https://freshrimpsushi.github.io/en/posts/512/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/512/</guid>
      <description>Definition: Linear Combination1 Let $\mathbf{w}$ be a vector in the vector space $V$. If $\mathbf{w}$ can be expressed as follows for vectors $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$ in $V$ and arbitrary constants $k_{1}, k_{2}, \cdots, k_{r}$, then $\mathbf{w}$ is called a linear combination of $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$. $$ \mathbf{w} = k_{1}\mathbf{v}_{1} + k_{2}\mathbf{v}_{2} + \cdots + k_{r}\mathbf{v}_{r} $$ Additionally, in this case, the constants $k_{1}, k_{2}, \cdots, k_{r}$ are referred to as the coefficients</description>
    </item>
    <item>
      <title>Subspace of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/285/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/285/</guid>
      <description>Definition1 Let $W$ be a non-empty subset of the vector space $V$. If $W$ satisfies the definition of a vector space with respect to the addition and scalar multiplication defined in $V$, then $W$ is called a subspacesubspace of the vector space $V$, and is denoted as follows: $$ W \le V $$ Explanation To determine whether a subset $W$ of a vector space $V$ is a subspace of $V$,</description>
    </item>
    <item>
      <title>Definition of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/282/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/282/</guid>
      <description>Definition1 When the elements of a non-empty set $V$ satisfy the following ten rules for two operations, additionaddition and scalar multiplicationscalar multiplication, $V$ is called a vector spacevector space over field2 $\mathbb{F}$, and the elements of $V$ are called vectorsvector. For $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k, l \in \mathbb{F}$, (A1) If $\mathbf{u}, \mathbf{v}$ is an element of $V$, then $\mathbf{u}+\mathbf{v}$ is also an element of $V$. (A2) $\mathbf{u}</description>
    </item>
  </channel>
</rss>
