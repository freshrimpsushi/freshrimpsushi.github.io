<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Linear Algebra on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98/</link><description>Recent content in Linear Algebra on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 23 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98/index.xml" rel="self" type="application/rss+xml"/><item><title>Definition of Cone and Convex Cone</title><link>https://freshrimpsushi.github.io/en/posts/8/</link><pubDate>Mon, 23 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/8/</guid><description>Definition 1 Cone A cone is defined in a vector space $V$ as a subset $C \subset V$ that satisfies the following for all scalars $a &amp;gt; 0$ and $x \in C$: $$ ax \in C $$ Flat Cone and Salient Cone If cone $V$ satisfies $-\mathbf{v} \in V$ for some non-zero vector $\mathbf{v} \in V$, it is called a flat cone. If not, it is called a salient cone.</description></item><item><title>Eigenvalues of Positive Definite Matrices and the Maximum Value of Quadratic Forms</title><link>https://freshrimpsushi.github.io/en/posts/2604/</link><pubDate>Mon, 29 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2604/</guid><description>Theorem Let&amp;rsquo;s assume that the eigenpairs $A \in \mathbb{R}^{p \times p}$ of a positive definite matrix $\left\{ \left( \lambda_{k} , e_{k} \right) \right\}_{k=1}^{n}$ are ordered as $\lambda_{1} \ge \cdots \ge \lambda_{n} \ge 0$. On the Unit Sphere, the maximum and minimum values of the quadratic form $\mathbf{x}^{T} A \mathbf{x}$ are as follows. $$ \begin{align*} \max_{\left\| \mathbf{x} \right\| = 1} \mathbf{x}^{T} A \mathbf{x} =&amp;amp; \lambda_{1} &amp;amp; \text{, attained when } \mathbf{x}</description></item><item><title>Necessary and Sufficient Conditions for a Quadratic Form to Be Zero</title><link>https://freshrimpsushi.github.io/en/posts/2586/</link><pubDate>Wed, 19 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2586/</guid><description>Theorem Matrix Form Let&amp;rsquo;s say $A \in \mathbb{C}^{n \times n}$ represents a matrix and $\mathbf{x} \in \mathbb{C}^{n}$ represents a vector. The necessary and sufficient condition for the quadratic form $\mathbf{x}^{\ast} A \mathbf{x}$ to be $0$ for all $\mathbf{x} \in \mathbb{C}^{n}$ is that $A$ is a zero matrix: $$ \mathbf{x}^{*} A \mathbf{x} = 0 , \forall \mathbf{x} \in \mathbb{C}^{n} \iff A = O $$ Linear Transformation Form When $\left( V, \mathbb{C}</description></item><item><title>Bilinear Forms and Hermitian Forms</title><link>https://freshrimpsushi.github.io/en/posts/3513/</link><pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3513/</guid><description>Definition1 Let’s say we have two vectors $\mathbf{x}, \mathbf{u} \in \mathbb{R}^{n}$ as follows. $$ \mathbf{x}=\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix},\quad \mathbf{u}^{T} = \begin{bmatrix} u_{1} &amp;amp; u_{2} &amp;amp; \cdots &amp;amp; u_{n} \end{bmatrix} $$ For a real constant $a_{ij} \in \mathbb{R} (1\le i,j \le n)$, the function $A : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$, defined as follows, is called the bilinear form. $$ A(\mathbf{u},\mathbf{x}):=\sum \limits_{i,k=1}^{n}</description></item><item><title>Quadratic Form</title><link>https://freshrimpsushi.github.io/en/posts/3512/</link><pubDate>Sat, 25 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3512/</guid><description>Definition $V$ is called a $n$dimensional vector space. For a given constant $a_{ij} \in \mathbb{R}(\text{or } \mathbb{C})$, the following second order homogeneous function $A : V \to \mathbb{R}(\text{or } \mathbb{C})$ is called a quadratic form. $$ A(\mathbf{x}) := \sum\limits_{i,j=1}^{n} a_{ij}x_{i}x_{j},\qquad (a_{ij} = a_{ji}) $$ Here, $\mathbf{x} = \begin{bmatrix} x_{1} &amp;amp; \cdots &amp;amp; x_{n} \end{bmatrix}^{T}$ holds. The term $i \ne j$ for $a_{ij}x_{i}x_{j}$ is called the cross product terms. Explanation According</description></item><item><title>Matrix Representation of the Sum and Scalar Multiplication of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3457/</link><pubDate>Sat, 05 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3457/</guid><description>Theorem Let $V, W$ be a finite-dimensional vector space with a given ordered basis $\beta, \gamma$. Also, let $T, U : V \to W$. Then, the following hold:$\\[0.5em]$ $[T + U]_{\beta}^{\gamma} = [T]_{\beta}^{\gamma} + [U]_{\beta}^{\gamma}$ $[aT]_{\beta}^{\gamma} = a[T]_{\beta}^{\gamma}$ Here, $[T]_{\beta}^{\gamma}$ is the matrix representation of $T$. Proof Since the proofs are similar, we will only prove the first equation. Let $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ and $\gamma =</description></item><item><title>Matrix Representation of Tensor Product</title><link>https://freshrimpsushi.github.io/en/posts/3419/</link><pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3419/</guid><description>Buildup1 Choose bases $\mathcal{V}, {\mathcal{V}}^{\prime}$ respectively for the finite-dimensional vector spaces $V, V^{\prime}$. Then, there exists a matrix equivalent to the linear transformation $\phi : V \to V^{\prime}$, called its matrix representation $\phi$. Now assume we have the finite-dimensional vector space $V, V^{\prime}, W, W^{\prime}$ and its ordered basis $\mathcal{V}, {\mathcal{V}}^{\prime}, \mathcal{W}, {\mathcal{W}}^{\prime}$, as well as two linear transformations $\phi : V \to V^{\prime}$ and $\psi : W \to W^{\prime}$.</description></item><item><title>Tensor Product of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3417/</link><pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3417/</guid><description>Buildup1 Given finite-dimensional vector spaces $V_{1}, V_{2}, W_{1}, W_{2}$ and linear transformations $\phi_{1} : V_{1} \to W_{1}$, $\phi_{2} : V_{2} \to W_{2}$, one can consider the following bilinear transformation. $$ V_{1} \times V_{2} \to W_{1} \otimes W_{2} $$ $$ (v_{1}, v_{2}) \mapsto \phi_{1}(v_{1}) \otimes \phi_{2}(v_{2}) $$ $\phi_{1}, \phi_{2}$ is a linear transformation, and it is easy to see that this function is bilinear due to the definition of product vectors.</description></item><item><title>Universal Properties of Tensor Products</title><link>https://freshrimpsushi.github.io/en/posts/3416/</link><pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3416/</guid><description>Buildup1 Given a finite-dimensional vector space $V_{1}, \dots, V_{r}$. If $n_{i} = \dim V_{i}$, and we select a basis for each vector space, we obtain the following coordinate vector as a bijective function $f_{i}$. $$ \begin{align*} f _{i}: &amp;amp; V_{i} \to \mathbb{C}^{n_{i}} \\ &amp;amp; v_{i} \mapsto (a_{i1}, \dots, a_{i n_{i}}) \end{align*} $$ From this, the following multilinear transformation $f$ is naturally defined. $$ \begin{align*} f : V_{1} \times \cdots \times</description></item><item><title>Tensor Product of Product Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3415/</link><pubDate>Sat, 13 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3415/</guid><description>Buildup For convenience, we will develop the concept in the complex number space $\mathbb{C}$, but $\mathbb{R}$ or any vector space is also applicable. Let&amp;rsquo;s denote the set of functions from the finite set $\Gamma$ to the complex number space as indicated by $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ When $\Gamma = \mathbf{n} = \left\{ 1, \dots, n \right\}$, it essentially becomes $\mathbb{C}^{\mathbf{n}} = \mathbb{C}^{n}$,</description></item><item><title>Tensor Product of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3414/</link><pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3414/</guid><description>Buildup1 For convenience, this exposition is developed for the complex number space $\mathbb{C}$, but it is applicable to $\mathbb{R}$ or any vector space as well. Let&amp;rsquo;s denote the set of functions from a finite set $\Gamma$ to the complex number space as described by $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ Let&amp;rsquo;s set $\Gamma$ as $\mathbf{n} = \left\{ 1, 2, \dots, n \right\}$. A function</description></item><item><title>What is a Flag in Linear Algebra?</title><link>https://freshrimpsushi.github.io/en/posts/3397/</link><pubDate>Fri, 07 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3397/</guid><description>Definition1 2 $n$Dimension Vector space $V$Subspaces sequences $\left\{ W_{i} \right\}$ satisfying the following equations are termed flags. $$ \left\{ \mathbf{0} \right\} = W_{0} \lneq W_{1} \lneq W_{2} \lneq \cdots \lneq W_{k-1} \lneq W_{k} = V $$ By definition, the following holds. $$ 0 = \dim V_{0} \lt \dim V_{1} \lt \dim V_{2} \lt \cdots \lt \dim V_{k-1} \lt \dim V_{k} = n $$ Explanation The term flag is used because,</description></item><item><title>Direct Sum of Invariant Subspaces and Its Characteristic Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/3393/</link><pubDate>Thu, 30 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3393/</guid><description>Theorem1 Let $T : V \to V$ be a linear transformation on a finite-dimensional vector space $V$ above. Assume that $V$ is the direct sum of the $T$-invariant subspaces $W_{i}$. $$ V = W_{1} \oplus W_{2} \oplus \cdots \oplus W_{k} $$ Let $f_{i}(t)$ be the characteristic polynomial of the restriction $T|_{W_{i}}$. Then, the characteristic polynomial of $T$, $f(t)$, is as follows. $$ f(t) = f_{1}(t) \cdot f_{2}(t) \cdot \cdots \cdot</description></item><item><title>Cayley-Hamilton Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3391/</link><pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3391/</guid><description>Definition1 Let $T : V \to V$ be a linear transformation on a finite-dimensional vector space $V$. Let $f(t)$ be the characteristic polynomial of $T$. Then, the following holds: $$ f(T) = T_{0} $$ Here, $T_{0}$ is the zero transformation. In other words, a linear transformation satisfies its own characteristic polynomial. Rewriting this theorem from the perspective of matrices, Corollary Square matrices satisfy their own characteristic equations. $$ f(A) =</description></item><item><title>Cyclic Subspaces of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3389/</link><pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3389/</guid><description>Definition1 Let us consider $T : V \to V$ as a linear transformation on the vector space $V$. Suppose $\mathbf{v} \ne \mathbf{0} \in V$. The following subspace $$ W = \span\left( \left\{ \mathbf{v}, T\mathbf{v}, T^{2}\mathbf{v}, \dots \right\} \right) $$ is called the $V$ $T$-cyclic subspace generated by $\mathbf{v}$. Description The $T$-cyclic subspace is trivially a $T$-invariant subspace. Also, it is the smallest $T$-invariant subspace that includes $\mathbf{v}$. Theorem1 Let $T</description></item><item><title>Null Space of Power Maps</title><link>https://freshrimpsushi.github.io/en/posts/3387/</link><pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3387/</guid><description>Theorem1 $n$ Let&amp;rsquo;s say that the linear transformation $T : V \to V$ on the dimension vector space is nilpotent. $$ T^{p} = T_{0} $$ Here, $T_{0}$ is the zero transformation. Let&amp;rsquo;s call $N(T)$ the null space of $T$. Then, the following holds: For all $i \in \mathbb{N}$, it is $N(T^{i}) \subset N(T^{i+1})$. For $1 \le i \le p-1$, there exists a sequence basis $N(T^{i})$ of $\beta_{i}$ such that the</description></item><item><title>The Eigenvalues of the Null Matrix are Only Zero</title><link>https://freshrimpsushi.github.io/en/posts/3381/</link><pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3381/</guid><description>Theorem1 English Translation: Let&amp;rsquo;s consider $V$ as a finite-dimensional vector space, and $T : V \to V$ as a nilpotent linear transformation. Then, the eigenvalues of $T$ are exclusively $0$. Japanese Translation: $V$を有限次元のベクトル空間、$T : V \to V$を冪零の線形変換としよう。する</description></item><item><title>Power Series Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3379/</link><pubDate>Thu, 02 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3379/</guid><description>Definition1 A linear transformation $T : V \to V$ on a vector space $V$ is called nilpotent if there exists a positive number $k$ such that $T^{k} = T_{0}$, where $T_{0}$ is the zero transformation. Stephen H. Friedberg, Linear Algebra (4th Edition, 2002), p512&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Diagonalizability of Linear Transformations, Multiplicity of Eigenvalues, and the Relationship with Eigenspaces</title><link>https://freshrimpsushi.github.io/en/posts/3377/</link><pubDate>Sun, 26 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3377/</guid><description>Theorem1 Let $T : V \to V$ be a linear transformation on a finite-dimensional vector space $V$. Suppose the characteristic polynomial of $T$ splits and $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$ are distinct eigenvalues of $T$. Then, $T$ is diagonalizable if and only if, for all $i$, the multiplicity of $\lambda_{i}$ and the dimension $\dim(E_{\lambda_{i}})$ of the eigenspace are the same. $$ T \text{ is diagobalizable. } \iff \text{multiplicity of } \lambda_{i}</description></item><item><title>The Creation of Unions is Equal to the Sum of Creations</title><link>https://freshrimpsushi.github.io/en/posts/3375/</link><pubDate>Wed, 22 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3375/</guid><description>Theorem1 Let $S_{1}, S_{2}$ be a subset of the vector space $V$. Then, the following holds. $$ \span(S_{1} \cup S_{2}) = \span(S_{1}) + \span(S_{2}) $$ Here, $\span$ means generation, and $+$ means the sum of sets. Proof $\span(S_{1} \cup S_{2}) \subset \span(S_{1}) + \span(S_{2})$ Let $v \in \span(S_{1} \cup S_{2})$. Then, $v$ can be expressed as follows: $$ v = \sum\limits_{i=1}^{n}a_{i}x_{i} + \sum\limits_{j=1}^{m}b_{j}y_{j},\quad x_{i}\in S_{1},\ y_{j} \in S_{2} $$ The</description></item><item><title>직합의 성질</title><link>https://freshrimpsushi.github.io/en/posts/3373/</link><pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3373/</guid><description>Theorem1 Let $W_{1}, W_{2}, \dots, W_{k}$ be subspaces of a finite-dimensional vector space $V$. The following propositions are equivalent: $V = W_{1} \oplus W_{2} \oplus \cdots \oplus W_{k}$ It is $V = \sum\limits_{i=1}^{k}W_{i}$, and for any vectors $v_{i} \in W_{i}(1 \le i \le k)$, if $v_{1} + \cdots v_{k} = 0$, then for all $i$, $v_{i} = 0$ holds. All $v \in V$ can be uniquely expressed in the form</description></item><item><title>Sum of Subspaces in a Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3371/</link><pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3371/</guid><description>Definition1 Let&amp;rsquo;s refer to $W_{1}, W_{2}$ as a subspace of the vector space $V$. The sum of $W_{1}$ and $W_{2}$ is denoted as $W_{1} + W_{2}$ and defined as follows. $$ W_{1} + W_{2} := \left\{ x + y : x\in W_{1}, y \in W_{2} \right\} $$ Generalization2 Let $W_{1}, W_{2}, \dots, W_{k}$ be a subspace of the vector space $V$. The sum of these subspaces is denoted as $W_{1}</description></item><item><title>Transformations on the Quotient Space of Diagonalizable Linear Transformations are also Diagonalizable</title><link>https://freshrimpsushi.github.io/en/posts/3369/</link><pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3369/</guid><description>Theorem1 Let $V$ be a dimension vector space, $T : V \to V$ be a linear transformation, and $W$ be an $T$-invariant subspace. If $T$ is diagonalizable, then $\overline{T} : V/W \to V/W$ is also diagonalizable. In this case, $V/W$ is the quotient space of $V$. Proof If $T$ is diagonalizable, so is $T|_{W}$, there exists a basis $\gamma = \left\{ v_{1}, v_{2}, \dots, v_{k} \right\}$ of $W$ such that</description></item><item><title>Characteristics of the Mapping into the Quotient Space via Linear Transformations between Polynomial Relations</title><link>https://freshrimpsushi.github.io/en/posts/3367/</link><pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3367/</guid><description>Theorem1 Let $V$ be a $n$-dimensional vector space, $T : V \to V$ a linear transformation, $W \le V$ a $T$-invariant subspace, $T|_{W}$ a contraction mapping, and $\overline{T}$ a linear transformation on the quotient space. $$ T|_{W} : W \to W \\ \overline{T} : V/W \to V/W $$ Let $f(t), g(t), h(t)$ be the characteristic polynomial of $T, T|_{W}, \overline{T}$, respectively. Then, the following holds. $$ f(t) = g(t)h(t) $$</description></item><item><title>Linear Transformations on the Quotient Space</title><link>https://freshrimpsushi.github.io/en/posts/3365/</link><pubDate>Thu, 02 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3365/</guid><description>Definition 1 Let $V$ be a vector space and $T : V \to V$ be a linear transformation. Let $W \le V$ be a $T$-invariant subspace. The linear transformation on quotient space $\overline{T}$ is defined as follows: $$ \begin{align*} \overline{T} : V/W &amp;amp;\to V/W \\ v + W &amp;amp;\mapsto T(v) + W \end{align*} $$ Here, $V/W$ is the quotient space. Theorem (a) $\overline{T}$ is well-defined. (b) $\overline{T}$ is indeed a</description></item><item><title>Mapping to the Quotient Space</title><link>https://freshrimpsushi.github.io/en/posts/3363/</link><pubDate>Sun, 29 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3363/</guid><description>Theorem1 Let $V$ be a vector space, and $W \le V$ a subspace. Define the function $\eta$ as follows. $$ \begin{align*} \eta : V &amp;amp;\to V/W \\ v &amp;amp;\mapsto v + W \end{align*} $$ In this case, $V/W$ is the quotient space of $V$. Then $\eta$ is a linear transformation and its null space is $N(\eta) = W$. If $V$ is finite-dimensional, then $$ \begin{equation} \dim(W) + \dim(V/W) = \dim(V)</description></item><item><title>몫공간의 기저와 차원</title><link>https://freshrimpsushi.github.io/en/posts/3361/</link><pubDate>Wed, 25 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3361/</guid><description>Theorem1 Let $V$ be a $n$-dimensional vector space, and let $W \le V$ be a $k (\lt n)$-dimensional subspace. Let $\left\{ u_{1}, \dots, u_{k} \right\}$ be a basis of $W$. And let $\left\{ u_{1}, \dots, u_{k}, u_{k+1}, \dots, u_{n} \right\}$ be a basis of $V$ obtained by extending this basis. Then $\left\{ u_{k+1} + W, \dots, u_{n} + W \right\}$ is a basis of the quotient space $V/W$. $\dim(V) =</description></item><item><title>Residual Classes and Quotient Spaces in Linear Algebra</title><link>https://freshrimpsushi.github.io/en/posts/3359/</link><pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3359/</guid><description>Definition1 $V$ is a $F$-vector space and $W \le V$ is a subspace. For $v \in V$, the set $$ \left\{ v \right\} + W := \left\{ v + w : w \in W \right\} $$ is called the coset of $W$ containing $v$ . The left-hand $+$ is the sum of sets. Explanation Often $\left\{ v \right\} + W$ is simply denoted by $v + W$. Let $\left\{ v</description></item><item><title>대각화가능한 선형변환의 불변부분공간으로의 축소사상도 대각화가능하다</title><link>https://freshrimpsushi.github.io/en/posts/3357/</link><pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3357/</guid><description>Theorem1 Let $V$ be a vector space, and let $T : V \to V$ be a diagonalizable linear transformation. Suppose $\left\{ \mathbf{0} \right\} \ne W \le V$ denotes a non-trivial $T$-invariant subspace. Then the restriction map $T|_{W}$ is also diagonalizable. A trivial $T$-invariant subspace refers to the zero vector set $\left\{ \mathbf{0} \right\}$, the entire set $V$, the range $R(T)$, the null space $N(T)$, and the eigenspace $E_{\lambda}$. Proof Since</description></item><item><title>The Relationship between Invariant Subspaces and Eigenvectors</title><link>https://freshrimpsushi.github.io/en/posts/3355/</link><pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3355/</guid><description>Theorem1 Let $V$ be a $n$-dimension vector space, $T : V \to V$ a linear transformation, and $W \le V$ a $T$-invariant subspace. Let $v_{1}, \dots, v_{k}$ be $T$&amp;rsquo;s eigenvectors corresponding to distinct eigenvalues. If $v_{1} + \cdots + v_{k} \in W$, then for every $i$ we have $v_{i} \in W$. Explanation Since $W$ is a subspace, if $v_{i} \in W$ then $\sum_{i}v_{i} \in W$ holds. However, the converse does</description></item><item><title>Invariant Subspaces of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3353/</link><pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3353/</guid><description>Overview Let $\beta = v_{1}, \dots, v_{k}$ be the set of eigenvectors of the linear transformation $T : V \to V$. Then, it can be understood that $T$ maps $\span{\beta}$ to $\span{\beta}$. A subspace that maps itself to itself in this manner is defined as an invariant subspace. Definition1 Let $V$ be a vector space, and $T : V \to V$ a linear transformation. A subspace $W$ is called an</description></item><item><title>The Union of Linearly Independent Sets from Different Eigenspaces is Linearly Independent</title><link>https://freshrimpsushi.github.io/en/posts/3351/</link><pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3351/</guid><description>Theorem1 Let $V$ be a vector space, $T : V \to V$ a linear transformation, and $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$ and $T$ different eigenvalues of $T$. For each $i = 1, \dots, k$, let $S_{i}$ be a linearly independent subset of the eigenspace $E_{\lambda_{i}}$. Then, ▶Eq1◀ is a linearly independent subset of $V$. Proof Lemma Following the notation of the theorem, let $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$</description></item><item><title>Eigen Spaces of Linear Transformations and Geometric Multiplicity</title><link>https://freshrimpsushi.github.io/en/posts/3349/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3349/</guid><description>Definition1 Let&amp;rsquo;s define $V$ and $n$ as dimension vector spaces, $T : V \to V$ as linear transformation. Let&amp;rsquo;s also define $\lambda$ as the eigenvalue of $T$. The set defined as follows, $E_{\lambda}$, is called the eigenspace of $T$ corresponding to the eigenvalue $\lambda$. $$ E_{\lambda} = V_{\lambda} := \left\{ x \in V : Tx = \lambda x \right\} = N(T - \lambda I) $$ In this case, $N$ is</description></item><item><title>Multiplicity of Eigenvalues of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3347/</link><pubDate>Wed, 28 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3347/</guid><description>Definition1 Let $V$ be a finite-dimensional vector space, and let $T : V \to V$ be a linear transformation. Let $f(t)$ be the characteristic polynomial of $T$, and let $\lambda$ be an eigenvalue of $T$. The highest power $k$ of the factor $(t - \lambda)^{k}$ in $f(t)$ is called the (algebraic) multiplicity of $\lambda$. Explanation Simply put, the multiplicity of an eigenvalue refers to how many times $\lambda$ is a</description></item><item><title>Definition of Affine Independence</title><link>https://freshrimpsushi.github.io/en/posts/2315/</link><pubDate>Sun, 25 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2315/</guid><description>Definition 1 A set of vectors $S := \left\{ v_{0} , v_{1}, \cdots , v_{n} \right\} \subset V$ is said to be Affinely Independent if the vectors in $S$ or $S$ itself are linearly independent. $$ v_{1} - v_{0} , v_{2} - v_{0} , \cdots , v_{n} - v_{0} $$ https://glossary.informs.org/ver2/mpgwiki/index.php?title=Affine_independence&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>The Characteristic Polynomial of a Diagonalizable Linear Transformation Is Factorizable</title><link>https://freshrimpsushi.github.io/en/posts/3345/</link><pubDate>Sat, 24 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3345/</guid><description>Definition1 The polynomial $P(F)$ being split over $F$ means that there exists a constant $c, a_{1}, \dots, a_{n} \in F$ that satisfies the following. $$ f(t) = c(t - a_{1})(t - a_{2})\cdots(t - a_{n}) $$ If the polynomial $f(t)$ that is decomposed is the characteristic polynomial of some linear transformation $T$ or matrix $A$, we say that $T$(or $A$) is decomposed. Explanation By definition, if the characteristic polynomial of $T:</description></item><item><title>Polynomial Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3343/</link><pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3343/</guid><description>Definition1 Polynomial A polynomial with coefficients from a field $F$ is defined for a non-negative integer $n$ as the following form. $$ \begin{equation} f(x) = a_{n}x^{n} + a_{n-1}x^{n-1} + \cdots + a_{1}x + a_{0} \end{equation} $$ Here, each $a_{k} \in F$ is called the coefficient of $x^{k}$. If $a_{n}=a_{n-1}=\cdots=a_{0}=0$, then $f(x)$ is called a zero polynomial. The degree of a polynomial is the largest power of $x$ with a coefficient</description></item><item><title>Eigenvectors Corresponding to Distinct Eigenvalues are Linearly Independent</title><link>https://freshrimpsushi.github.io/en/posts/3341/</link><pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3341/</guid><description>Theorem1 Let $V$ be a vector space, $T : V \to V$ be a linear transformation, and $\lambda_{1}, \dots, \lambda_{k}$ be distinct eigenvalues of $T$. If $\mathbf{v}_{1}, \dots, \mathbf{v}_{k}$ are eigenvectors of $T$ corresponding to the eigenvalue $\lambda_{1}, \dots, \lambda_{k}$, then $\left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{k} \right\}$ are linearly independent. Corollary Diagonalization T is diagonalizable if there exist $n$ linearly independent eigenvectors of $T$. If $T : V \to V$ is</description></item><item><title>Diagonalizable Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3335/</link><pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3335/</guid><description>Definition 1 Let $V$ be called a finite-dimensional vector space. Let $T : V \to V$ be called a linear transformation. If there exists an ordered basis $\beta$ for which the matrix representation $\begin{bmatrix} T \end{bmatrix}_{\beta}$ of $T$ becomes a diagonal matrix, $T$ is said to be diagonalizable. For a square matrix $A$, if the $L_{A}$ is diagonalizable, then the matrix $A$ is said to be diagonalizable. Explanation Suppose the</description></item><item><title>Characteristics Polynomial of Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3339/</link><pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3339/</guid><description>Overview The characteristic polynomial of linear transformation is defined. From the theorem below, it can be seen that solving equation $\det(A - \lambda I) = 0$ is equivalent to finding the eigenvalues. Therefore, it is quite natural to name $\det(A - \lambda I)$ the characteristic polynomial. Theorem1 Let&amp;rsquo;s say $F$ is any field, and $A \in M_{n\times n}(F)$. That $\lambda \in F$ is an eigenvalue of $A$ is equivalent to</description></item><item><title>Eigenvalues and Eigenvectors of Finite-Dimensional Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3337/</link><pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3337/</guid><description>Definition1 Let $V$ be a finite-dimensional $F$-vector space. Let $T : V \to V$ be a linear transformation. For $\lambda \in F$, $$ Tx = \lambda x $$ a non-zero vector $x \in V$ satisfying this is called an eigenvector of $T$. The scalar $\lambda \in F$ is called the eigenvalue corresponding to the eigenvector $x$. Explanation Although one might find the term eigenvector replaced by the terms characteristic vector</description></item><item><title>Basis Transformation (Coordinate Transformation) of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3333/</link><pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3333/</guid><description>Overview1 Let $V$ to $n$ dimensional vector space, and call it $\mathbf{v} \in V$. Let $\beta, \beta^{\prime}$ be the ordered basis of $V$. Then, the two coordinates $[\mathbf{v}]_{\beta}$ and $[\mathbf{v}]_{\beta^{\prime}}$ of $\mathbf{v}$ are transformed by the coordinate transformation matrix $Q$ as follows. $$ [\mathbf{v}]_{\beta} = Q [\mathbf{v}]_{\beta^{\prime}} $$ Now, suppose a linear transformation $T : V \to V$ is given. Then, for each ordered basis, there exist matrix representations $\begin{bmatrix}</description></item><item><title>Coordinate Transformation of Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3331/</link><pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3331/</guid><description>Overview1 2 Let $V$ be $n$dimensional vector space, and let us call $\mathbf{v} \in V$ as such. Let $\beta$ be some ordered basis of $V$. Then, $\mathbf{v}$ is expressed as the coordinate vector $[\mathbf{v}]_{\beta}$. Given another ordered basis $\beta ^{\prime}$, $\mathbf{v}$ can also be expressed as the coordinate vector $[\mathbf{v}]_{\beta^{\prime}}$ with respect to it. Coordinate transformation of vectors refers to the equation relating these two coordinate vectors. Build-up For convenience,</description></item><item><title>Left Multiplication Transformation (Matrix Transformation)</title><link>https://freshrimpsushi.github.io/en/posts/3329/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3329/</guid><description>Definition1 Regarding the field $F$, let&amp;rsquo;s say $A \in M_{m \times n}(F)$. The following $L_{A}$ is defined as the left-multiplication transformation. $$ \begin{align*} L_{A} : F^{n} &amp;amp;\to F^{m} \\ x &amp;amp;\mapsto Ax \end{align*} $$ Here, $Ax$ is the matrix product of $A$ and $x$. Description This is a more abstract description of matrix transformation using the concept of a field. Theorems Let&amp;rsquo;s say $A \in M_{m \times n}(F)$. Then, $L_{A}$</description></item><item><title>Matrix Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3327/</link><pubDate>Fri, 18 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3327/</guid><description>Definitions For a field $F$, let us define the set of $m \times n$ matrices whose components are elements of $F$ as $M_{m \times n}(F)$. $$ M_{m \times n}(F) := \left\{ \begin{bmatrix} a_{11} &amp;amp; \cdots &amp;amp; a_{1n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} : a_{ij} \in F \right\} $$ Then, with respect to matrix addition and scalar multiplication, $M_{m \times n}(F)$ is a</description></item><item><title>Matrices of Linear Transformations from a Basis of a Subspace to an Extended Basis</title><link>https://freshrimpsushi.github.io/en/posts/3325/</link><pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3325/</guid><description>Theorem Let █eq01█ be a subspace in █eq02█ dimension vector space called █eq03█. Let █eq04█ be the ordered basis of █eq05█. Consider █eq06█ as an extension of the basis of █eq03█ from █</description></item><item><title>Expansion and Contraction of the Basis</title><link>https://freshrimpsushi.github.io/en/posts/3321/</link><pubDate>Sun, 06 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3321/</guid><description>Theorem1 Let $S$ be a finite subset of the finite-dimensional vector space $V$. (a) If $S$ generates $V$ but is not a basis of $V$, then elements of $S$ can be appropriately removed to reduce it to a basis of $V$. (b) If $S$ is linearly independent but not a basis of $V$, then elements can be suitably added to $S$ to extend it to a basis of $V$. Corollary</description></item><item><title>The Equivalence Condition When the Range of a Linear Transformation is Smaller than the Kernel</title><link>https://freshrimpsushi.github.io/en/posts/3295/</link><pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3295/</guid><description>Theorem1 Let $V$ be a vector space, and $T : V \to V$ be a linear transformation. Then, the following holds: $$ T^{2} = T_{0} \iff R(T) \subset N(T) $$ Here, $T_{0}$ is the zero transformation, and $R(T), N(T)$ are the respective range and null space of $T$. Generalization Let $U, V, W$ be a vector space, and $T_{1} : U \to V$, $T_{2} : V \to W$ be linear</description></item><item><title>Dual Pair Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3293/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3293/</guid><description>Definition1 Let us call $X$ a vector space. Let $X^{\ast\ast}$ be the dual space of $X$&amp;rsquo;s dual space, $X^{\ast}$. $$ X^{\ast\ast} = (X^{\ast})^{\ast} $$ This is called the bidual space of $X$. Theorem If $X$ is a finite-dimensional vector space, then $X$ and $X^{\ast\ast}$ are isomorphic. $$ X \approx X^{\ast\ast} $$ Explanation Bidual, double dual, and second dual all mean the same thing. The above theorem holds only when $X$</description></item><item><title>Transpose of Linear Transformations Defined by Dual Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3291/</link><pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3291/</guid><description>Theorem1 Let&amp;rsquo;s denote the ordered bases of two finite-dimensional vector spaces $V, W$ as $\beta, \gamma$, respectively. For any linear transformation $T : V \to W$, the following defined function $U$ is a linear transformation and satisfies $[U]_{\gamma^{\ast}}^{\beta^{\ast}} = ([T]_{\beta}^{\gamma})^{t}$. $$ U : W^{\ast} \to V^{\ast} \quad \text{ by } \quad U(g) = gT \quad \forall g \in W^{\ast} $$ Here, $[T]_{\beta}^{\gamma}$ is the matrix representation of $T$, ${}^{t}$ is</description></item><item><title>Linear Transformation Spaces and Their Matrix Representation Spaces are Isomorphic</title><link>https://freshrimpsushi.github.io/en/posts/3289/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3289/</guid><description>Theorem1 Let&amp;rsquo;s assume that two vector spaces $V, W$ have dimensions $n, m$, respectively. And let $\beta, \gamma$ be the ordered bases for each. Then the function defined as follows $\Phi$ is an isomorphism. $$ \Phi : L(V, W) \to M_{m\times n}(\mathbb{R}) \quad \text{ by } \quad \Phi (T) = [T]_{\beta}^{\gamma} $$ $[T]_{\beta}^{\gamma}$ is the matrix representation of $T$. Corollary A necessary and sufficient condition for a linear transformation to</description></item><item><title>Homomorphism</title><link>https://freshrimpsushi.github.io/en/posts/3287/</link><pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3287/</guid><description>Definition1 For two vector spaces $V, W$, if there exists an invertible linear transformation $T : V \to W$, then $V$ is said to be isomorphic to $W$, and is denoted as follows. $$ V \cong W $$ Furthermore, $T$ is called an isomorphism. Explanation By the equivalence condition of being invertible, saying $T$ is an isomorphism means that $T$ is a bijective function. Therefore, if there exists a bijective</description></item><item><title>Inverse of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3285/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3285/</guid><description>Definition1 Let $V, W$ be a vector space, and $T : V \to W$ be a linear transformation. If the linear transformation $U : W \to V$ satisfies the following, then $U$ is called the inverse or inverse transformation of $T$. $$ TU = I_{W} \quad \text{and} \quad UT = I_{V} $$ $TU$ is the composition of $U$ and $T$, $I_{X} : X \to X$ is the identity transformation. If</description></item><item><title>Linear Transformation Space</title><link>https://freshrimpsushi.github.io/en/posts/3283/</link><pubDate>Sat, 20 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3283/</guid><description>Definition1 The set of all linear transformations from the vector space $V$ to $W$ is denoted as $L(V,W)$. $$ L(V, W) = \mathcal{L}(V, W) := \left\{ T : V \to W\enspace |\enspace T \text{ is linear } \right\} $$ This is also expressed as follows, referred to as the homomorphism space. $$ \operatorname{Hom}(V,W) = L(V, W) = \left\{ T : V \to W \text{ is linear} \right\} $$ Additionally, when</description></item><item><title>Linear Functional</title><link>https://freshrimpsushi.github.io/en/posts/3281/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3281/</guid><description>Definitions1 Let&amp;rsquo;s call $V$ a vector space. A mapping $f$ from $V$ to $\mathbb{C}$ (or $\mathbb{R}$) is called a functional. $$ f : V \to \mathbb{C} $$ If $f$ is linear, it is called a linear functional. More Detailed Definitions2 Let&amp;rsquo;s call $V$ a vector space over the field $F$. Here, the field $F$ itself becomes a $1$-dimensional vector space over $F$. A linear transformation $f : V \to F$</description></item><item><title>Order Basis and Coordinate Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3279/</link><pubDate>Fri, 12 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3279/</guid><description>Definition1 Let&amp;rsquo;s say $V$ is a finite-dimensional vector space. When a specific order is assigned to a basis of $V$, it is called an ordered basis. Let&amp;rsquo;s say $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ is an ordered basis of $V$. Then, due to the uniqueness of basis representation, for $\mathbf{v} \in V$, scalars $a_{i}$ uniquely exist as follows. $$ \mathbf{v} = a_{1}\mathbf{v}_{1} + \dots a_{n}\mathbf{v}_{n} $$ $a_{1},\dots,a_{n}$ is called</description></item><item><title>Linear Transformations Between Finite-Dimensional Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3277/</link><pubDate>Mon, 08 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3277/</guid><description>Theorem1 Let $V, W$ be a vector space. Let $\left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ and $\left\{ \mathbf{w}_{1}, \mathbf{w}_{2}, \dots, \mathbf{w}_{n} \right\}$ be bases of $V, W$, respectively. Then there exists a unique linear transformation $T : V \to W$ that satisfies $T(\mathbf{v}_{i}) = \mathbf{w}_{i}$. Corollary2 Let $V, W$ be a vector space. Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ be a basis of $V$. If $U, T</description></item><item><title>Linear Transformation Trace</title><link>https://freshrimpsushi.github.io/en/posts/3275/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3275/</guid><description>Definition Let $V$ be a $n$-dimensional vector space. Let $f : V \to V$ be a linear transformation. Let $B = \left\{ e_{i} \right\}$ be a basis of $V$. Let $n \times n$ matrix $A$ be the matrix representation of $f$ with respect to $B$. $$ A = [f]_{B} $$ Since $f(e_{i}) \in V$, we represent it as $f(e_{i}) = \sum f_{j}(e_{i})e_{j}$. Then, $$ A = \begin{bmatrix} f_{1}(e_{1}) &amp;amp; f_{2}(e_{1})</description></item><item><title>Every n-dimensional Real Vector Space is Isomorphic to R^n</title><link>https://freshrimpsushi.github.io/en/posts/3079/</link><pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3079/</guid><description>Definition1 Let $V$ and $W$ be called a vector space. If there exists an invertible (bijective) linear transformation $T : V \to W$, then $V$ and $W$ are said to be isomorphic. $T$ is called an isomorphism. Theorem Every $n$-dimentional real vector space is isomorphic to $\mathbb{R}^{n}$. Explanation Another way to express the theorem is as follows. &amp;ldquo;A $\mathbb{R}$-vector space $V$ being isomorphic to $\mathbb{R}^{n}$&amp;rdquo; is equivalent to &amp;ldquo;being $\dim{V}=n$&amp;rdquo;.</description></item><item><title>The Matrix Representation of a Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3078/</link><pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3078/</guid><description>Definition1 Let&amp;rsquo;s call $V, W$ a finite-dimensional vector space. Let&amp;rsquo;s call $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ and $\gamma = \left\{ \mathbf{w}_{1}, \dots, \mathbf{w}_{m} \right\}$ the ordered bases for $V$ and $W$, respectively. Let&amp;rsquo;s call $T : V \to W$ a linear transformation. Then, by the uniqueness of the basis representation, there exists a unique scalar $a_{ij}$ satisfying the following. $$ T(\mathbf{v}_{j}) = \sum_{i=1}^{m}a_{ij}\mathbf{w}_{i} = a_{1j}\mathbf{w}_{1} + \cdots +</description></item><item><title>Properties of the Space of Invertible Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3076/</link><pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3076/</guid><description>Theorem1 Let&amp;rsquo;s call the set of all invertible linear transformations on $\Omega$. $$ \Omega = \left\{ \text{all invertible linear operator on } \mathbb{R}^{n} \right\} $$ (a) If the following holds for $T_{1} \in \Omega$ and $T_{2} \in L(\mathbb{R}^{n})$, then $T_{2} \in \Omega$ is true. $$ \| T_{2} - T_{1} \| \| T_{1}^{-1} \| &amp;lt; 1 $$ Here, $\| T \|$ is the norm of the linear transformation. (b) The following</description></item><item><title>Norm of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3075/</link><pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3075/</guid><description>Definition1 Define the norm of the linear transformation $T \in L(\mathbb{R}^{n}, \mathbb{R}^{m})$ as follows. $$ \begin{equation} \| T \| := \sup \limits_{\| \mathbf{x} \| = 1} \| T(\mathbf{x}) \| \end{equation} $$ Explanation (a) From (a) we have the following equality, so $\| T \|$ can be interpreted as the ratio by which $T$ changes the magnitude of elements of $\mathbb{R}^{n}$ when mapping them into $\mathbb{R}^{m}$. In other words, no matter</description></item><item><title>Composition of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3074/</link><pubDate>Sat, 26 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3074/</guid><description>Definition1 Given linear transformations $T_{1} : V \to W$ and $T_{2} : W \to Z$, the transformation defined by $T_{2} T_{1}$ is called the composition of $T_{1}$ and $T_{2}$. $$ (T_{2} \circ T_{1})(\mathbf{x}) = T_{2}\left( T_{1}(\mathbf{x}) \right) \quad \mathbf{x} \in V $$ Explanation The composition of linear transformations is often denoted simply as follows: $$ T_{2}T_{1}\mathbf{x} = (T_{2} \circ T_{1}) (\mathbf{x}) $$ In finite dimensions, this is essentially the same</description></item><item><title>Necessary and Sufficient Conditions for Linear Transformations to be Surjective and Injective</title><link>https://freshrimpsushi.github.io/en/posts/3073/</link><pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3073/</guid><description>Theorem 11 The following two propositions are equivalent concerning a linear transformation $T: V \to W$. $T$ is one-to-one. $N(T) = \text{ker}(T) = \left\{ \mathbf{0} \right\}$ Explanation This means that understanding the kernel of $T$ is a method to determine whether $T$ is one-to-one or not. According to the theorem, a linear transformation being one-to-one is equivalent to the following condition. $$ \mathbf{x} \ne \mathbf{0} \implies T(\mathbf{x}) \ne \mathbf{0} $$</description></item><item><title>Rank, Nullity, and Dimension Theorems of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3072/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3072/</guid><description>Definition1 Let $T : V \to W$ be a linear transformation. If the range $R(T)$ of $T$ is finite-dimensional, the dimension of $R(T)$ is called the rank of $T$, denoted by: $$ \mathrm{rank}(T) := \dim (R(T)) $$ If the null space $N(T)$ of $T$ is finite-dimensional, the dimension of $N(T)$ is called the nullity of $T$, denoted by: $$ \mathrm{nullity}(T) := \dim\left( N(T) \right) $$ Explanation This is a generalization</description></item><item><title>Linear Transformation: Kernel and Range</title><link>https://freshrimpsushi.github.io/en/posts/3071/</link><pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3071/</guid><description>Definition1 Let&amp;rsquo;s say $T : V \to W$ is a linear transformation. The set of elements of $V$ that are mapped to $\mathbf{0}$ by $T$ is called the kernel or null space, and is denoted as follows. $$ \text{ker}(T) = N(T) := \left\{ \mathbf{v} \in V : T( \mathbf{v} ) = \mathbf{0} \right\} $$ The set of images under $\mathbf{v} \in V$ by $T$ is called the range or image</description></item><item><title>The Basis of the Domain Generates the Image of the Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3070/</link><pubDate>Fri, 18 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3070/</guid><description>Theorem1 Let&amp;rsquo;s suppose we have a given linear transformation $T : V \to W$. Assume $V$ is finite-dimensional, and let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ be a basis of $V$. Then, the image of any $\mathbf{v} \in V$ can be represented as follows. $$ T(\mathbf{v}) = c_{1}T(\mathbf{v}_{1}) + c_{2}T(\mathbf{v}_{2}) + \cdots c_{n}T(\mathbf{v}_{n}) $$ Here, $c_{i}$ are coefficients that satisfy $\mathbf{v} = \sum c_{i}\mathbf{v}_{i}$. In other words, $\left\{</description></item><item><title>Orthogonal Basis and Its Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/3040/</link><pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3040/</guid><description>Definition1 An inner product space basis $V$ that is an orthogonal set is called an orthogonal basis. If $S$ is an orthonormal set, it is called an orthonormal basis. Theorem If $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ is an orthogonal basis of the inner product space $V$, and let $\mathbf{u} \in V$. Then, the following equation holds. $$ \begin{equation} \begin{aligned} \mathbf{u} &amp;amp;= \dfrac{\langle \mathbf{u}, \mathbf{v}_{1} \rangle}{\| \mathbf{v}_{1} \|^{2}}</description></item><item><title>Basis Addition/Subtraction Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3028/</link><pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3028/</guid><description>Theorem1 Let $S$ be a non-empty subset of vector space $V$. (a) If $S$ is linearly independent and if $\mathbf{v} \in V$ equals $\mathbf{v} \notin \text{span}(S)$, then $S \cup \left\{ \mathbf{v} \right\}$ remains linearly independent. (b) If $\mathbf{v} \in S$ can be represented as a linear combination of other vectors in $S$, then $S$ and $S \setminus \left\{ \mathbf{v} \right\}$ span the same space. That is, the following holds: $$</description></item><item><title>Relationship Between Orthogonality and Linear Independence</title><link>https://freshrimpsushi.github.io/en/posts/3045/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3045/</guid><description>Definition1 An inner product space $V$&amp;rsquo;s two vectors $\mathbf{u}, \mathbf{v}$ are said to be orthogonal if they satisfy $\langle \mathbf{u}, \mathbf{v} \rangle = 0$. A set made up of elements of $V$ where each element is orthogonal to every other element is called an orthogonal set. If the norm of every element in an orthogonal set is $1$, then it is called an orthonormal set. Theorem A subset $S =</description></item><item><title>What is Inner Product in Real Vector Spaces?</title><link>https://freshrimpsushi.github.io/en/posts/3044/</link><pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3044/</guid><description>Definition1 Let $V$ be a real vector space. An inner product on $V$ is a function that maps two vectors in $V$ to a single real number $\langle \mathbf{u}, \mathbf{v} \rangle$, satisfying the following conditions: When $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k \in \mathbb{R}$, $\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle$ $\langle \mathbf{u} + \mathbf{v}, \mathbf{w} \rangle = \langle \mathbf{u}, \mathbf{w} \rangle + \langle \mathbf{v}, \mathbf{w} \rangle$</description></item><item><title>Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3026/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3026/</guid><description>Definition1 A transformation is when a function $T : V \to W$ maps from one vector space to another, that is $V$, $W$ are both vector spaces, we call $T$ a transformation. If the transformation $T$ is a linear function, satisfying the following two conditions for any $\mathbf{v},\mathbf{u} \in V$ and scalar $k$, it is called a linear transformation: $T(k \mathbf{u}) = k T(\mathbf{u})$ $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) +</description></item><item><title>Necessary and Sufficient Conditions for a Basis in Finite-Dimensional Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3043/</link><pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3043/</guid><description>Theorem1 Let $V$ be a $n$-dimensional vector space. Suppose a subset $S\subset V$ has $n$ elements. A necessary and sufficient condition for $S$ to be a basis of $V$ is that $V = \text{span}(S)$ or $S$ is linearly independent. Explanation Vector space, dimension, basis, span, independence - all these fundamental concepts of linear algebra appear here. For a set to be a basis of a vector space, it must be</description></item><item><title>Projection Theorem in Linear Algebra</title><link>https://freshrimpsushi.github.io/en/posts/3046/</link><pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3046/</guid><description>Theorem1 If $W$ is a subspace of a finite-dimensional inner product space $V$, then every $\mathbf{u} \in V$ is uniquely represented by the following formula. $$ \begin{equation} \mathbf{u} = \mathbf{w}_{1} + \mathbf{w}_{2} \end{equation} $$ Here, $\mathbf{w}_{1} \in W$ and $\mathbf{w}_{2} \in W^{\perp}$ apply. Explanation The notations $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$ in the theorem are also marked as follows. $$ \mathbf{w}_{1} = \mathrm{proj}_{W} \mathbf{u} \quad \text{and} \quad \mathbf{w}_{2} = \mathrm{proj}_{W^{\perp}} \mathbf{u} $$</description></item><item><title>Basis of Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3017/</link><pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3017/</guid><description>Definition1 Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$ be a subset of vector space $V$. If $S$ satisfies the following two conditions, then $S$ is called a basis of $V$. $S$ spans $V$. $$ V = \text{span}(S) $$ $S$ is linearly independent. Explanation As the name suggests, the concept of a basis corresponds to &amp;rsquo;the smallest thing that can create a vector space&amp;rsquo;. The condition of spanning has</description></item><item><title>Linear Forms</title><link>https://freshrimpsushi.github.io/en/posts/1734/</link><pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1734/</guid><description>Definition Let $V$ be a $n$dimensional vector space. For a given constant $a_{i} \in \mathbb{R}(\text{or } \mathbb{C})$, the following linear transformation $A : V \to \mathbb{R}(\text{or } \mathbb{C})$ is called a linear form. $$ A(\mathbf{x}) := \sum\limits_{i=1}^{n} a_{i}x_{i} $$ In this case, $\mathbf{x} = \begin{bmatrix} x_{1} &amp;amp; \cdots &amp;amp; x_{n} \end{bmatrix}^{T}$. Generalization For a given inner product space $(V, \left&amp;lt; \cdot, \cdot \right&amp;gt;)$ and $\mathbf{a} \in V$, the following linear</description></item><item><title>Convex Sets in Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1914/</link><pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1914/</guid><description>Definition A subset $M$ of a vector space $V$ is called a convex set if the following equation holds: $$ \lambda x +(1-\lambda)y \in M,\quad \forall \lambda\in[0,1],\ \forall x,y \in M $$ Description Verbally, this equation means &amp;quot;$M$ is a convex set implies that every vector lying between any two vectors in $M$ also belongs to $M$&amp;quot;. Also, if $M$ is a subspace, it is closed under addition and scalar</description></item><item><title>Convolution's General Definition</title><link>https://freshrimpsushi.github.io/en/posts/1848/</link><pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1848/</guid><description>Definition Given the integral transform $J$ and two functions $f$, $g$, a function $f \ast g$ fulfilling the conditions below is defined as the convolution of $f$ and $g$ with respect to $J$. $$ J(f \ast g)=(Jf)(Jg) $$ Explanation According to the definition, the convolution, being the integral transform of a product, can be divided into the product of integral transforms. This means that two functions, which were bound in</description></item><item><title>Integral Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1847/</link><pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1847/</guid><description>Definition If a map $J$ from a function space to a function space is defined as the following integral, then $J$ is called an integral transform. $$ (Jf) (x) = \int_{a}^{b} K(x,t)f(t)dt $$ $$ J : f(\cdot) \mapsto \int_{a}^{b} K(\cdot,t)f(t)dt $$ In this case, $K$ is referred to as the kernel of $J$. If a map from $Jf$ to $f$ exists, it is denoted as $J^{-1}$ and called the inverse</description></item><item><title>Why Functional is Named Functional</title><link>https://freshrimpsushi.github.io/en/posts/1780/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1780/</guid><description>The term &amp;ldquo;functional analysis&amp;rdquo; is indeed intriguing, especially when considering the word &amp;ldquo;functional&amp;rdquo; instead of merely &amp;ldquo;function analysis.&amp;rdquo; At first glance, &amp;ldquo;functional&amp;rdquo; appears to be an adjective form of &amp;ldquo;function,&amp;rdquo; suggesting meanings like &amp;ldquo;function-like&amp;rdquo; or &amp;ldquo;pertaining to functions.&amp;rdquo; This notion can also be found in another name for functionals, &amp;ldquo;generalized functions.&amp;rdquo; The question arises as to why these are not simply called functions. To understand this, let&amp;rsquo;s look at the</description></item><item><title>Reflexive of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/770/</link><pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/770/</guid><description>Definition 1 If $X$ is a vector space and $X^{\ast \ast}$ is its bidual, then $X$ is said to be reflexive if $X^{\ast \ast} \approx X$. Explanation Generally, the size of a vector space increases with each dual taken. However, reflexivity essentially means that the dual space does not continue to grow. Examples of reflexive spaces include: For $1 &amp;lt; p &amp;lt; \infty$, then ${\ell^{p}}^{\ast \ast} \approx \ell^{p}$ If $\dim</description></item><item><title>Dual Space</title><link>https://freshrimpsushi.github.io/en/posts/753/</link><pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/753/</guid><description>Dual Spaces Definition 11 The set of all continuous linear functionals of a vector space $X$ is denoted by $X^{ \ast }$ and is called the dual space of $X$, simply referred to as the dual of $X$, as denoted below. $$ X^{ \ast }:=\left\{ x^{ \ast }:X\to \mathbb{C}\ |\ x^{ \ast } \text{ is continuous and linear} \right\} $$ $$ X^{ \ast }:=B(X,\mathbb{C}) $$ $B \left( X, \mathbb{C} \right)$</description></item><item><title>Necessary and Sufficient Conditions for Linear Functionals to be Represented by Linearly Independent Combinations</title><link>https://freshrimpsushi.github.io/en/posts/748/</link><pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/748/</guid><description>Theorem Let $f, f_{1} , \cdots , f_{n}$ be a linear functional with domain $X$. (a) For $c_{1} , \cdots , c_{n} \in \mathbb{C}$, $\displaystyle f = \sum_{i=1}^{n} c_{i} f_{i}$ $\iff$ $\displaystyle \bigcap_{i=1}^{n} \ker ( f_{i} ) \subset \ker (f)$ (b) There exists $x_{1} , \cdots , x_{n}$ satisfying $f_{j} (x_{i} ) = \delta_{ij}$ with $f_{1} , \cdots , f_{n}$ being linearly independent. Here, $\delta_{ij}$ is the Kronecker delta. Explanation</description></item><item><title>Necessary and Sufficient Conditions for Linear Functionals to be Continuous</title><link>https://freshrimpsushi.github.io/en/posts/742/</link><pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/742/</guid><description>Theorem1 The linear functional $f$ is continuous. $\iff$ $\ker(f)$ is a closed set in $X$. Here, $\mathcal{N} (f) = \ker (f) = \left\{ x \in X \ | \ f(x) = 0 \right\}$ is the kernel of the linear transformation $f$. Proof Strategy: $(\implies)$ Direct deduction by the definition of the kernel. $(\impliedby)$ The necessary and sufficient condition for the continuity of a linear operator is boundedness. Showing that $f$</description></item><item><title>Minkowski Inequality</title><link>https://freshrimpsushi.github.io/en/posts/288/</link><pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/288/</guid><description>Theorem For two vectors $\mathbf{x}= (x_{1} , x_{2} , \dots , x_{n} )$, $\mathbf{y} = (y_{1} , y_{2} , \dots , y_{n} )$ and a real number $1$ larger than $p$, the following equation holds. $$ \left( \sum_{k=1}^{n} | x_{k} + y_{k} |^{p} \right)^{{1} \over {p}} \le \left( \sum_{k=1}^{n} |x_{k}|^{p} \right)^{{1} \over {p}} + \left( \sum_{k=1}^{n} |y_{k}|^{p} \right)^{{1} \over {p}} $$ This is called the Minkowski&amp;rsquo;s inequality. Description Minkowski&amp;rsquo;s inequality</description></item><item><title>Homogeneity of Norms</title><link>https://freshrimpsushi.github.io/en/posts/274/</link><pubDate>Sun, 28 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/274/</guid><description>Definition On a vector space $V$, if for two norms $\left\| \cdot \right\|_{\alpha}, \left\| \cdot \right\|_{\beta}$ defined and for any vector $\mathbf{v} \in V$ the following $$ c \left\| \mathbf{v} \right\|_{\alpha} \le \left\| \mathbf{v} \right\|_{\beta} \le C \left\| \mathbf{v} \right\|_{\alpha} $$ is satisfied with some constant $c , C &amp;gt;0$, the two norms are equivalent. Theorem Preservation of inequalities [1]: If norms $\left\| \cdot\right\|_{\alpha}$ and $\left\| \cdot \right\|_{\beta}$ defined on</description></item><item><title>Orthogonal Complement of a Subspace</title><link>https://freshrimpsushi.github.io/en/posts/273/</link><pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/273/</guid><description>Definition1 In the set $$ W^{\perp} = \left\{ \mathbf{v} \in V \ : \left\langle \mathbf{v} , \mathbf{w} \right\rangle = 0,\quad \forall \mathbf{w} \in W \right\} $$ for the subspace $W$ of a vector space $V$, it is called the orthogonal complement of $W$. Here $\langle , \rangle$ is the inner product. Explanation In other words, $W^{\perp}$ is a collection of vectors that are orthogonal to every element of $W$. The</description></item><item><title>Holder Inequality</title><link>https://freshrimpsushi.github.io/en/posts/258/</link><pubDate>Thu, 25 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/258/</guid><description>Definition $\dfrac{1}{p} + \dfrac{1}{q} = 1$ satisfies and for two constants $p, q$ and $\mathbf{u}, \mathbf{v} \in \mathbb{C}^n$ larger than 1, the following inequality holds: $$ | \left\langle \mathbf{u}, \mathbf{v} \right\rangle | = |\mathbf{u} ^{\ast} \mathbf{v}| \le ||\mathbf{u}||_{p} ||\mathbf{v}||_{q} $$ This is called the Hölder&amp;rsquo;s inequality. Explanation Although it should be written as Höld</description></item><item><title>In Linear Algebra, What is a Norm?</title><link>https://freshrimpsushi.github.io/en/posts/257/</link><pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/257/</guid><description>Definition Let us define a vector space over $V$ as $\mathbb{F}$. $\left\| \cdot \right\| : V \to \mathbb{F}$ is defined as a norm on $V$ if it satisfies the following three conditions with respect to $\mathbf{u}, \mathbf{v} \in V$ and $k \in \mathbb{F}$: (i) Positive definiteness: $\left\| \mathbf{u} \right\| \ge 0$ and $\mathbf{u} = \mathbb{0} \iff \left\| \mathbf{u} \right\| = 0$ (ii) Homogeneity: $\left\|k \mathbf{u} \right\| = | k |</description></item><item><title>Wronskian Definition and Determination of Linear Independence</title><link>https://freshrimpsushi.github.io/en/posts/501/</link><pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/501/</guid><description>Definition1 Let us consider a set of functions that are differentiable up to n times, denoted by $S=\left\{ f_{1}, f_{2}, \dots, f_{n} \right\}$. The Wronskian $W$ of this set is defined by the following determinant. $$ W(x) = W(f_{1}, f_{2}, \dots, f_{n}) := \begin{vmatrix} f_{1} &amp;amp; f_{2} &amp;amp; \cdots &amp;amp; f_{n} \\ f_{1}^{\prime} &amp;amp; f_2^{\prime} &amp;amp; \cdots &amp;amp; f_{n}^{\prime} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ f_{1}^{(n-1)} &amp;amp;</description></item><item><title>Gram-Schmidt Orthonormalization</title><link>https://freshrimpsushi.github.io/en/posts/394/</link><pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/394/</guid><description>Theorem Every finite-dimensional inner product space has an orthonormal basis. Explanation As with most existence proofs, it may not seem long or complex, but it is an extremely important theorem. Many of the logical foundations that support linear algebra rely on the existence of this orthonormal basis. Proof Let one of the bases generating the inner product space $(V, \left\langle \cdot , \cdot \right\rangle)$ be denoted as $\left\{ \mathbf{x}_{1} ,</description></item><item><title>Direct Sum in Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/353/</link><pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/353/</guid><description>Definition A vector space $V$ is said to be the direct sum of its two subspaces $W_{1}$ and $W_{2}$ if it satisfies the following, denoted by $V = W_{1} \oplus W_{2}$. (i) Existence: For any $\mathbf{v} \in V$, there exist $\mathbf{v}_{1} \in W_{1}$ and $\mathbf{v}_{2} \in W_{2}$ satisfying $\mathbf{v} = \mathbf{v}_{1} + \mathbf{v}_{2}$. (ii) Exclusivity: $W_{1} \cap W_{2} = \left\{ \mathbf{0} \right\}$ (iii) Uniqueness: For a given $\mathbf{v}$, there exists</description></item><item><title>Dimension of the Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3018/</link><pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3018/</guid><description>Definition1 The number of elements (vectors) of a basis for a vector space $V$ is defined as the dimension of $V$ and is denoted as follows. $$ \dim (V) $$ Explanation Such a generalization of dimensions goes beyond merely exploring vector spaces and is being applied to various technologies that support this society. It might seem pointless to consider dimensions higher than the $3$ dimensions of our world and the</description></item><item><title>Linear Independence and Linear Dependence</title><link>https://freshrimpsushi.github.io/en/posts/253/</link><pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/253/</guid><description>Definition1 Let&amp;rsquo;s denote a non-empty subset of vector space $V$ as $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$. For constants $k_{1}, k_{2}, \dots, k_{r}$, the following equation $$ k_{1} \mathbf{v}_{1} + k_{2} \mathbf{v}_{2} + \dots + k_{r} \mathbf{v}_{r} = \mathbf{0} $$ has at least one solution $$ k_{1} = 0,\ k_{2} = 0,\ \dots,\ k_{r} = 0 $$ This is called a trivial solution. If the trivial solution is</description></item><item><title>Linear Combination, Span</title><link>https://freshrimpsushi.github.io/en/posts/512/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/512/</guid><description>Definition: Linear Combination1 Let $\mathbf{w}$ be a vector in the vector space $V$. If $\mathbf{w}$ can be expressed as follows for vectors $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$ in $V$ and arbitrary constants $k_{1}, k_{2}, \cdots, k_{r}$, then $\mathbf{w}$ is called a linear combination of $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$. $$ \mathbf{w} = k_{1}\mathbf{v}_{1} + k_{2}\mathbf{v}_{2} + \cdots + k_{r}\mathbf{v}_{r} $$ Additionally, in this case, the constants $k_{1}, k_{2}, \cdots, k_{r}$ are referred to as the coefficients</description></item><item><title>Subspace of Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/285/</link><pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/285/</guid><description>Definition1 Let $W$ be a non-empty subset of the vector space $V$. If $W$ satisfies the definition of a vector space with respect to the addition and scalar multiplication defined in $V$, then $W$ is called a subspace of the vector space $V$, and is denoted as follows: $$ W \le V $$ Explanation To determine whether a subset $W$ of a vector space $V$ is a subspace of $V$,</description></item><item><title>Definition of Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/282/</link><pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/282/</guid><description>Definition1 When the elements of a non-empty set $V$ satisfy the following ten rules for two operations, addition and scalar multiplication, $V$ is called a vector space over field2 $\mathbb{F}$, and the elements of $V$ are called vectors. For $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k, l \in \mathbb{F}$, (A1) If $\mathbf{u}, \mathbf{v}$ is an element of $V$, then $\mathbf{u}+\mathbf{v}$ is also an element of $V$. (A2) $\mathbf{u} + \mathbf{v}</description></item></channel></rss>