<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>선형대수 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98/</link>
    <description>Recent content in 선형대수 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 27 Nov 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bilinear Forms and Hermitian Forms</title>
      <link>https://freshrimpsushi.github.io/en/posts/3513/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3513/</guid>
      <description>Definition1 Let’s say we have two vectors $\mathbf{x}, \mathbf{u} \in \mathbb{R}^{n}$ as follows. $$ \mathbf{x}=\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix},\quad \mathbf{u}^{T} = \begin{bmatrix} u_{1} &amp;amp; u_{2} &amp;amp; \cdots &amp;amp; u_{n} \end{bmatrix} $$ For a real constant $a_{ij} \in \mathbb{R} (1\le i,j \le n)$, the function $A : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$, defined as follows, is called the bilinear form. $$ A(\mathbf{u},\mathbf{x}):=\sum \limits_{i,k=1}^{n}</description>
    </item>
    <item>
      <title>Quadratic Form</title>
      <link>https://freshrimpsushi.github.io/en/posts/3512/</link>
      <pubDate>Sat, 25 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3512/</guid>
      <description>Definition $V$ is called a $n$dimensional vector space. For a given constant $a_{ij} \in \mathbb{R}(\text{or } \mathbb{C})$, the following second order homogeneous function $A : V \to \mathbb{R}(\text{or } \mathbb{C})$ is called a quadratic form. $$ A(\mathbf{x}) := \sum\limits_{i,j=1}^{n} a_{ij}x_{i}x_{j},\qquad (a_{ij} = a_{ji}) $$ Here, $\mathbf{x} = \begin{bmatrix} x_{1} &amp;amp; \cdots &amp;amp; x_{n} \end{bmatrix}^{T}$ holds. The term $i \ne j$ for $a_{ij}x_{i}x_{j}$ is called the cross product terms. Explanation According</description>
    </item>
    <item>
      <title>Tensor Product of Product Vectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/3415/</link>
      <pubDate>Sat, 13 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3415/</guid>
      <description>Buildup For convenience, we will develop the concept in the complex number space $\mathbb{C}$, but $\mathbb{R}$ or any vector space is also applicable. Let&amp;rsquo;s denote the set of functions from the finite set $\Gamma$ to the complex number space as indicated by $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ When $\Gamma = \mathbf{n} = \left\{ 1, \dots, n \right\}$, it essentially becomes $\mathbb{C}^{\mathbf{n}} = \mathbb{C}^{n}$,</description>
    </item>
    <item>
      <title>Tensor Product of Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/3414/</link>
      <pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3414/</guid>
      <description>Buildup1 For convenience, the discussion unfolds with respect to the complex number space $\mathbb{C}$, but it could equally apply to $\mathbb{R}$ or any arbitrary vector space. Let&amp;rsquo;s denote the set of functions from a finite set $\Gamma$ to a complex number space as $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ Let $\Gamma$ be $\mathbf{n} = \left\{ 1, 2, \dots, n \right\}$. If we denote a</description>
    </item>
    <item>
      <title>Sum of Subspaces in a Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3371/</link>
      <pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3371/</guid>
      <description>Definition1 Let&amp;rsquo;s refer to $W_{1}, W_{2}$ as a subspace of the vector space $V$. The sum of $W_{1}$ and $W_{2}$ is denoted as $W_{1} + W_{2}$ and defined as follows. $$ W_{1} + W_{2} := \left\{ x + y : x\in W_{1}, y \in W_{2} \right\} $$ Generalization2 Let $W_{1}, W_{2}, \dots, W_{k}$ be a subspace of the vector space $V$. The sum of these subspaces is denoted as $W_{1}</description>
    </item>
    <item>
      <title>Residual Classes and Quotient Spaces in Linear Algebra</title>
      <link>https://freshrimpsushi.github.io/en/posts/3359/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3359/</guid>
      <description>Definition1 Let&amp;rsquo;s refer to $V$ as $F$-vector space, and $W \le V$ as subspace. For $v \in V$, the following set $$ \left\{ v \right\} + W := \left\{ v + w : w \in W \right\} $$ is called the coset of $W$ containing $v$. $+$ is the sum of sets. Explanation We often abbreviate $\left\{ v \right\} + W$ as $v + W$. Considering the set of all</description>
    </item>
    <item>
      <title>Invariant Subspaces of Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/3353/</link>
      <pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3353/</guid>
      <description>Overview Let $\beta = v_{1}, \dots, v_{k}$ be the set of eigenvectors of the linear transformation $T : V \to V$. Then, it can be understood that $T$ maps $\span{\beta}$ to $\span{\beta}$. A subspace that maps itself to itself in this manner is defined as an invariant subspace. Definition1 Let $V$ be a vector space, and $T : V \to V$ a linear transformation. A subspace $W$ is called an</description>
    </item>
    <item>
      <title>Eigen Spaces of Linear Transformations and Geometric Multiplicity</title>
      <link>https://freshrimpsushi.github.io/en/posts/3349/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3349/</guid>
      <description>Definition1 Let&amp;rsquo;s define $V$ and $n$ as dimension vector spaces, $T : V \to V$ as linear transformation. Let&amp;rsquo;s also define $\lambda$ as the eigenvalue of $T$. The set defined as follows, $E_{\lambda}$, is called the eigenspace of $T$ corresponding to the eigenvalue $\lambda$. $$ E_{\lambda} = V_{\lambda} := \left\{ x \in V : Tx = \lambda x \right\} = N(T - \lambda I) $$ In this case, $N$ is</description>
    </item>
    <item>
      <title>Definition of Affine Independence</title>
      <link>https://freshrimpsushi.github.io/en/posts/2315/</link>
      <pubDate>Sun, 25 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2315/</guid>
      <description>Definition 1 A set of vectors $S := \left\{ v_{0} , v_{1}, \cdots , v_{n} \right\} \subset V$ is said to be Affinely Independent if the vectors in $S$ or $S$ itself are linearly independent. $$ v_{1} - v_{0} , v_{2} - v_{0} , \cdots , v_{n} - v_{0} $$ https://glossary.informs.org/ver2/mpgwiki/index.php?title=Affine_independence&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    <item>
      <title>Diagonalizable Linear Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3335/</link>
      <pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3335/</guid>
      <description>Definition 1 Let $V$ be called a finite-dimensional vector space. Let $T : V \to V$ be called a linear transformation. If there exists an ordered basis $\beta$ for which the matrix representation $\begin{bmatrix} T \end{bmatrix}_{\beta}$ of $T$ becomes a diagonal matrix, $T$ is said to be diagonalizable. For a square matrix $A$, if the $L_{A}$ is diagonalizable, then the matrix $A$ is said to be diagonalizable. Explanation Suppose the</description>
    </item>
    <item>
      <title>Characteristics Polynomial of Linear Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3339/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3339/</guid>
      <description>Overview The characteristic polynomial of linear transformation is defined. From the theorem below, it can be seen that solving equation $\det(A - \lambda I) = 0$ is equivalent to finding the eigenvalues. Therefore, it is quite natural to name $\det(A - \lambda I)$ the characteristic polynomial. Theorem1 Let&amp;rsquo;s say $F$ is any field, and $A \in M_{n\times n}(F)$. That $\lambda \in F$ is an eigenvalue of $A$ is equivalent to</description>
    </item>
    <item>
      <title>Eigenvalues and Eigenvectors of Finite-Dimensional Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3337/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3337/</guid>
      <description>Definition1 Let $V$ be a finite-dimensional $F$-vector space. Let $T : V \to V$ be a linear transformation. For $\lambda \in F$, $$ Tx = \lambda x $$ a non-zero vector $x \in V$ satisfying this is called an eigenvector of $T$. The scalar $\lambda \in F$ is called the eigenvalue corresponding to the eigenvector $x$. Explanation Although one might find the term eigenvector replaced by the terms characteristic vector</description>
    </item>
    <item>
      <title>Expansion and Contraction of the Basis</title>
      <link>https://freshrimpsushi.github.io/en/posts/3321/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3321/</guid>
      <description>Theorem1 Let $S$ be a finite subset of the finite-dimensional vector space $V$. (a) If $S$ generates $V$ but is not a basis of $V$, then elements of $S$ can be appropriately removed to reduce it to a basis of $V$. (b) If $S$ is linearly independent but not a basis of $V$, then elements can be suitably added to $S$ to extend it to a basis of $V$. Corollary</description>
    </item>
    <item>
      <title>Homomorphism</title>
      <link>https://freshrimpsushi.github.io/en/posts/3287/</link>
      <pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3287/</guid>
      <description>Definition1 For two vector spaces $V, W$, if there exists an invertible linear transformation $T : V \to W$, then $V$ is said to be isomorphic to $W$, and is denoted as follows. $$ V \cong W $$ Furthermore, $T$ is called an isomorphism. Explanation By the equivalence condition of being invertible, saying $T$ is an isomorphism means that $T$ is a bijective function. Therefore, if there exists a bijective</description>
    </item>
    <item>
      <title>Inverse of Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3285/</link>
      <pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3285/</guid>
      <description>Definition1 Let $V, W$ be a vector space, and $T : V \to W$ be a linear transformation. If the linear transformation $U : W \to V$ satisfies the following, then $U$ is called the inverse or inverse transformation of $T$. $$ TU = I_{W} \quad \text{and} \quad UT = I_{V} $$ $TU$ is the composition of $U$ and $T$, $I_{X} : X \to X$ is the identity transformation. If</description>
    </item>
    <item>
      <title>Linear Functional</title>
      <link>https://freshrimpsushi.github.io/en/posts/3281/</link>
      <pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3281/</guid>
      <description>Definitions1 Let&amp;rsquo;s call $V$ a vector space. A mapping $f$ from $V$ to $\mathbb{C}$ (or $\mathbb{R}$) is called a functional. $$ f : V \to \mathbb{C} $$ If $f$ is linear, it is called a linear functional. More Detailed Definitions2 Let&amp;rsquo;s call $V$ a vector space over the field $F$. Here, the field $F$ itself becomes a $1$-dimensional vector space over $F$. A linear transformation $f : V \to F$</description>
    </item>
    <item>
      <title>Order Basis and Coordinate Vectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/3279/</link>
      <pubDate>Fri, 12 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3279/</guid>
      <description>Definition1 Let&amp;rsquo;s say $V$ is a finite-dimensional vector space. When a specific order is assigned to a basis of $V$, it is called an ordered basis. Let&amp;rsquo;s say $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ is an ordered basis of $V$. Then, due to the uniqueness of basis representation, for $\mathbf{v} \in V$, scalars $a_{i}$ uniquely exist as follows. $$ \mathbf{v} = a_{1}\mathbf{v}_{1} + \dots a_{n}\mathbf{v}_{n} $$ $a_{1},\dots,a_{n}$ is called</description>
    </item>
    <item>
      <title>Matrix Representation of Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3078/</link>
      <pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3078/</guid>
      <description>Definition1 Let us consider $V, W$ as a finite-dimensional vector space. Let&amp;rsquo;s say $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ and $\gamma = \left\{ \mathbf{w}_{1}, \dots, \mathbf{w}_{m} \right\}$ are the ordered bases for $V$ and $W$, respectively. Assume $T : V \to W$ to be a linear transformation. Due to the uniqueness of basis representation, there exist unique scalars $a_{ij}$ that satisfy the following. $$ T(\mathbf{v}_{j}) = \sum_{i=1}^{m}a_{ij}\mathbf{w}_{i} = a_{1j}\mathbf{w}_{1}</description>
    </item>
    <item>
      <title>Composition of Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3074/</link>
      <pubDate>Sat, 26 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3074/</guid>
      <description>Definition1 Given linear transformations $T_{1} : V \to W$ and $T_{2} : W \to Z$, the transformation defined by $T_{2} T_{1}$ is called the composition of $T_{1}$ and $T_{2}$. $$ (T_{2} \circ T_{1})(\mathbf{x}) = T_{2}\left( T_{1}(\mathbf{x}) \right) \quad \mathbf{x} \in V $$ Explanation The composition of linear transformations is often denoted simply as follows: $$ T_{2}T_{1}\mathbf{x} = (T_{2} \circ T_{1}) (\mathbf{x}) $$ In finite dimensions, this is essentially the same</description>
    </item>
    <item>
      <title>Rank, Nullity, and Dimension Theorems of Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3072/</link>
      <pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3072/</guid>
      <description>Definition1 Let $T : V \to W$ be a linear transformation. If the range $R(T)$ of $T$ is finite-dimensional, the dimension of $R(T)$ is called the rank of $T$, denoted by: $$ \mathrm{rank}(T) := \dim (R(T)) $$ If the null space $N(T)$ of $T$ is finite-dimensional, the dimension of $N(T)$ is called the nullity of $T$, denoted by: $$ \mathrm{nullity}(T) := \dim\left( N(T) \right) $$ Explanation This is a generalization</description>
    </item>
    <item>
      <title>Linear Transformation: Kernel and Range</title>
      <link>https://freshrimpsushi.github.io/en/posts/3071/</link>
      <pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3071/</guid>
      <description>Definition1 Let&amp;rsquo;s say $T : V \to W$ is a linear transformation. The set of elements of $V$ that are mapped to $\mathbf{0}$ by $T$ is called the kernel or null space, and is denoted as follows. $$ \text{ker}(T) = N(T) := \left\{ \mathbf{v} \in V : T( \mathbf{v} ) = \mathbf{0} \right\} $$ The set of images under $\mathbf{v} \in V$ by $T$ is called the range or image</description>
    </item>
    <item>
      <title>Linear Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3026/</link>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3026/</guid>
      <description>Definition1 A transformation is when a function $T : V \to W$ maps from one vector space to another, that is $V$, $W$ are both vector spaces, we call $T$ a transformation. If the transformation $T$ is a linear function, satisfying the following two conditions for any $\mathbf{v},\mathbf{u} \in V$ and scalar $k$, it is called a linear transformation: $T(k \mathbf{u}) = k T(\mathbf{u})$ $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) +</description>
    </item>
    <item>
      <title>Basis of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3017/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3017/</guid>
      <description>Definition1 Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$ be a subset of vector space $V$. If $S$ satisfies the following two conditions, then $S$ is called a basis of $V$. $S$ spans $V$. $$ V = \text{span}(S) $$ $S$ is linearly independent. Explanation As the name suggests, the concept of a basis corresponds to &amp;rsquo;the smallest thing that can create a vector space&amp;rsquo;. The condition of spanning has</description>
    </item>
    <item>
      <title>Convex Sets in Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1914/</link>
      <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1914/</guid>
      <description>Definition A subset $M$ of a vector space $V$ is called a convex set if the following equation holds: $$ \lambda x +(1-\lambda)y \in M,\quad \forall \lambda\in[0,1],\ \forall x,y \in M $$ Description Verbally, this equation means &amp;quot;$M$ is a convex set implies that every vector lying between any two vectors in $M$ also belongs to $M$&amp;quot;. Also, if $M$ is a subspace, it is closed under addition and scalar</description>
    </item>
    <item>
      <title>Convolution&#39;s General Definition</title>
      <link>https://freshrimpsushi.github.io/en/posts/1848/</link>
      <pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1848/</guid>
      <description>Definition Given the integral transform $J$ and two functions $f$, $g$, a function $f \ast g$ fulfilling the conditions below is defined as the convolution of $f$ and $g$ with respect to $J$. $$ J(f \ast g)=(Jf)(Jg) $$ Explanation According to the definition, the convolution, being the integral transform of a product, can be divided into the product of integral transforms. This means that two functions, which were bound in</description>
    </item>
    <item>
      <title>Integral Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1847/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1847/</guid>
      <description>Definition If a map $J$ from a function space to a function space is defined as the following integral, then $J$ is called an integral transform. $$ (Jf) (x) = \int_{a}^{b} K(x,t)f(t)dt $$ $$ J : f(\cdot) \mapsto \int_{a}^{b} K(\cdot,t)f(t)dt $$ In this case, $K$ is referred to as the kernel of $J$. If a map from $Jf$ to $f$ exists, it is denoted as $J^{-1}$ and called the inverse</description>
    </item>
    <item>
      <title>Dual Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/753/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/753/</guid>
      <description>Dual Spaces Definition 11 The set of all continuous linear functionals of a vector space $X$ is denoted by $X^{ \ast }$ and is called the dual space of $X$, simply referred to as the dual of $X$, as denoted below. $$ X^{ \ast }:=\left\{ x^{ \ast }:X\to \mathbb{C}\ |\ x^{ \ast } \text{ is continuous and linear} \right\} $$ $$ X^{ \ast }:=B(X,\mathbb{C}) $$ $B \left( X, \mathbb{C} \right)$</description>
    </item>
    <item>
      <title>In Linear Algebra, What is a Norm?</title>
      <link>https://freshrimpsushi.github.io/en/posts/257/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/257/</guid>
      <description>Definition Let us define a vector space over $V$ as $\mathbb{F}$. $\left\| \cdot \right\| : V \to \mathbb{F}$ is defined as a norm on $V$ if it satisfies the following three conditions with respect to $\mathbf{u}, \mathbf{v} \in V$ and $k \in \mathbb{F}$: (i) Positive definiteness: $\left\| \mathbf{u} \right\| \ge 0$ and $\mathbf{u} = \mathbb{0} \iff \left\| \mathbf{u} \right\| = 0$ (ii) Homogeneity: $\left\|k \mathbf{u} \right\| = | k |</description>
    </item>
    <item>
      <title>Wronskian Definition and Determination of Linear Independence</title>
      <link>https://freshrimpsushi.github.io/en/posts/501/</link>
      <pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/501/</guid>
      <description>Definition1 Let us consider a set of functions that are differentiable up to n times, denoted by $S=\left\{ f_{1}, f_{2}, \dots, f_{n} \right\}$. The Wronskian $W$ of this set is defined by the following determinant. $$ W(x) = W(f_{1}, f_{2}, \dots, f_{n}) := \begin{vmatrix} f_{1} &amp;amp; f_{2} &amp;amp; \cdots &amp;amp; f_{n} \\ f_{1}^{\prime} &amp;amp; f_2^{\prime} &amp;amp; \cdots &amp;amp; f_{n}^{\prime} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ f_{1}^{(n-1)} &amp;amp;</description>
    </item>
    <item>
      <title>Direct Sum in Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/353/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/353/</guid>
      <description>Definition A vector space $V$ is said to be the direct sum of its two subspaces $W_{1}$ and $W_{2}$ if it satisfies the following, denoted by $V = W_{1} \oplus W_{2}$. (i) Existence: For any $\mathbf{v} \in V$, there exist $\mathbf{v}_{1} \in W_{1}$ and $\mathbf{v}_{2} \in W_{2}$ satisfying $\mathbf{v} = \mathbf{v}_{1} + \mathbf{v}_{2}$. (ii) Exclusivity: $W_{1} \cap W_{2} = \left\{ \mathbf{0} \right\}$ (iii) Uniqueness: For a given $\mathbf{v}$, there exists</description>
    </item>
    <item>
      <title>Dimension of the Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3018/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3018/</guid>
      <description>Definition1 The number of elements (vectors) of a basis for a vector space $V$ is defined as the dimension of $V$ and is denoted as follows. $$ \dim (V) $$ Explanation Such a generalization of dimensions goes beyond merely exploring vector spaces and is being applied to various technologies that support this society. It might seem pointless to consider dimensions higher than the $3$ dimensions of our world and the</description>
    </item>
    <item>
      <title>Linear Independence and Linear Dependence</title>
      <link>https://freshrimpsushi.github.io/en/posts/253/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/253/</guid>
      <description>Definition1 Let&amp;rsquo;s denote a non-empty subset of vector space $V$ as $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$. For constants $k_{1}, k_{2}, \dots, k_{r}$, the following equation $$ k_{1} \mathbf{v}_{1} + k_{2} \mathbf{v}_{2} + \dots + k_{r} \mathbf{v}_{r} = \mathbf{0} $$ has at least one solution $$ k_{1} = 0,\ k_{2} = 0,\ \dots,\ k_{r} = 0 $$ This is called a trivial solution. If the trivial solution is</description>
    </item>
    <item>
      <title>Linear Combination, Span</title>
      <link>https://freshrimpsushi.github.io/en/posts/512/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/512/</guid>
      <description>Definition: Linear Combination1 Let $\mathbf{w}$ be a vector in the vector space $V$. If $\mathbf{w}$ can be expressed as follows for vectors $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$ in $V$ and arbitrary constants $k_{1}, k_{2}, \cdots, k_{r}$, then $\mathbf{w}$ is called a linear combination of $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$. $$ \mathbf{w} = k_{1}\mathbf{v}_{1} + k_{2}\mathbf{v}_{2} + \cdots + k_{r}\mathbf{v}_{r} $$ Additionally, in this case, the constants $k_{1}, k_{2}, \cdots, k_{r}$ are referred to as the coefficients</description>
    </item>
    <item>
      <title>Subspace of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/285/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/285/</guid>
      <description>Definition1 Let $W$ be a non-empty subset of the vector space $V$. If $W$ satisfies the definition of a vector space with respect to the addition and scalar multiplication defined in $V$, then $W$ is called a subspacesubspace of the vector space $V$, and is denoted as follows: $$ W \le V $$ Explanation To determine whether a subset $W$ of a vector space $V$ is a subspace of $V$,</description>
    </item>
    <item>
      <title>Definition of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/282/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/282/</guid>
      <description>Definition1 When the elements of a non-empty set $V$ satisfy the following ten rules for two operations, additionaddition and scalar multiplicationscalar multiplication, $V$ is called a vector spacevector space over field2 $\mathbb{F}$, and the elements of $V$ are called vectorsvector. For $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k, l \in \mathbb{F}$, (A1) If $\mathbf{u}, \mathbf{v}$ is an element of $V$, then $\mathbf{u}+\mathbf{v}$ is also an element of $V$. (A2) $\mathbf{u}</description>
    </item>
  </channel>
</rss>
