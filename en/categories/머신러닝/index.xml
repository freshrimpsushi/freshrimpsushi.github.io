<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/</link><description>Recent content in Machine Learning on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 14 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/index.xml" rel="self" type="application/rss+xml"/><item><title>Maximum Likelihood Estimation for Linear Regression Model in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3643/</link><pubDate>Wed, 14 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3643/</guid><description>Summary Assume the relationship between data $\mathbf{x}_{i} \in \mathbb{R}^{n}$ and its labels $y_{i} \in \mathbb{R}$ is described by the following linear model. $$ y_{i} = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{i} + \epsilon_{i}, \qquad i = 1, \ldots, K \tag{1} $$ When $K &amp;gt; n$, the parameter $\mathbf{w}_{\text{ML}}$ that maximizes the likelihood is as follows. $$ \mathbf{w}_{\text{ML}} = (\mathbf{X}^{\mathsf{T}} \mathbf{X})^{-1} \mathbf{X}^{\mathsf{T}} \mathbf{y} $$ Here, $\mathbf{y} = \begin{bmatrix} y_{1} &amp;amp; \cdots &amp;amp; y_{K} \end{bmatrix}^{\mathsf{T}}$ and</description></item><item><title>Bayesian Inference in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3642/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3642/</guid><description>Overview Bayesian inference is a statistical method for estimating the distribution of parameters using prior knowledge and observed data based on Bayes&amp;rsquo; theorem. Explanation Assume that a random variable $\mathbf{x}$ follows a probability distribution with parameter $\theta$. The purpose of Bayesian inference is to estimate the distribution of $\theta$ by examining the samples drawn from $\mathbf{x}$. The key point is not the value of $\theta$, but estimating the &amp;ldquo;distribution&amp;rdquo; of</description></item><item><title>Maximum a Posteriori Estimation for Linear Regression Model in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3641/</link><pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3641/</guid><description>Theorem Assume that the relationship between data $\mathbf{x}_{i} \in \mathbb{R}^{n}$ and its labels $y_{i} \in \mathbb{R}$ can be expressed by the following linear model. $$ y_{i} = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{i} + \epsilon_{i}, \qquad i = 1, \ldots, K \tag{1} $$ The parameter $\mathbf{w}_{\text{MAP}}$ that maximizes the posterior probability is as follows. For $\mathbf{y} = \begin{bmatrix} y_{1} &amp;amp; \cdots &amp;amp; y_{K} \end{bmatrix}^{\mathsf{T}}$ and $\mathbf{X} = \begin{bmatrix} \mathbf{x}_{1} &amp;amp; \cdots &amp;amp; \mathbf{x}_{K} \end{bmatrix}^{T}</description></item><item><title>Generative Model</title><link>https://freshrimpsushi.github.io/en/posts/3637/</link><pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3637/</guid><description>Overview Determining the exact probability distribution that our data follows is a crucial yet very challenging problem in many application fields. For instance, if we precisely know the probability distribution of human face photographs and the method to sample from this distribution, we can obtain plausible human face images every time we sample data from this distribution. Obviously, this task is nearly impossible. Much like how many difficult problems begin</description></item><item><title>SiLU or Swish Function in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/883/</link><pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/883/</guid><description>Definition 1 2 The SiLU or Swish function is defined as follows. $$ \operatorname{SiLU}(x) = x \cdot \sigma(x) $$ Here, $\sigma$ is a particular case of the sigmoid function, specifically the logistic function $\sigma(x) = \left( 1 + e^{-x} \right)^{-1}$. Explanation The SiLU resembles ReLU in shape, but unlike ReLU, it is not a monotonic function and is smooth. The logistic function has a problem known as gradient vanishing, which</description></item><item><title>Autoencoder</title><link>https://freshrimpsushi.github.io/en/posts/1181/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1181/</guid><description>Definition For two natural numbers $m \ll n$, the function $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is called an encoder. The function $g : \mathbb{R}^{m} \to \mathbb{R}^{n}$ is called a decoder. If $h = g \circ f$ satisfies $h(x) = x$, it is called an autoencoder. Explanation Since the encoder&amp;rsquo;s output dimension is smaller than the input dimension, it can be considered as performing data compression and encryption. On the other</description></item><item><title>Difference between Noise and Artifacts in Images (Signals, Data)</title><link>https://freshrimpsushi.github.io/en/posts/1192/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1192/</guid><description>Overview Noise and artifacts are common factors that degrade the original signal (data) and need to be removed. This document explains the characteristics of these two elements and how they differ from each other. Definition In an image, noise refers to any values that degrade the original image, typically caused by random, unpredictable, and unremovable factors. On the other hand, the degradation caused by removable, regular, or predictable factors is</description></item><item><title>Free Online Resources for Artificial Intelligence, Machine Learning, and Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/1253/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1253/</guid><description>Description Textbooks in the fields of artificial intelligence, machine learning, and deep learning are often freely available on the internet. Even in the case of well-known textbooks, there are quite a few that are freely accessible. Here are a few introductions. List Pattern Recognition and Machine Learning (Link) Authored by Christopher Bishop, this book is also well-known as PRML. The Korean translation title is &amp;ldquo</description></item><item><title>Implementing Automatic Differentiation Forward Mode Using Dual Numbers in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1498/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1498/</guid><description>Overview The forward mode of automatic differentiation can be easily implemented using dual numbers. This document explains how to implement the forward mode in Julia. For background knowledge on dual numbers and automatic differentiation, the following articles are recommended: Dual Numbers Differentiable Real Functions Defined on Dual Number Fields Automatic Differentiation Automatic Differentiation and Dual Numbers Code 1 An example of calculating the automatic differentiation for function $y(x) = \ln</description></item><item><title>Automatic Differentiation and Dual Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1501/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1501/</guid><description>Overview Dual numbers are numbers that can be expressed in the following form for two real numbers $a, b \in \mathbb{R}$. $$ a + b\epsilon, \quad (\epsilon^{2} = 0,\ \epsilon \neq 0) $$ The addition and multiplication system of dual numbers is useful for implementing the forward mode of automatic differentiation. Description1 In automatic differentiation, especially the forward mode, when computing the function value $f$, the derivative is calculated simultaneously.</description></item><item><title>How to Define Neural Networks Using Functional API in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/1539/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1539/</guid><description>Description A simple neural network structure can be defined using Flux.Chain, but it is difficult to define a neural network with a complex structure using Chain. In such cases, you can define the neural network using a functional API with the @functor macro. The @functor allows the parameters of a struct-defined neural network to be tracked for performing backpropagation. Code Let&amp;rsquo;s define a neural network consisting of four linear layers.</description></item><item><title>Salt and Pepper Noise</title><link>https://freshrimpsushi.github.io/en/posts/76/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/76/</guid><description>Definition The noise that appears as small dots in an image in white or black is called salt-and-pepper noise. Example For example, the occurrence of salt-and-pepper noise in the image above means that small dots are scattered throughout the image, as shown below. Description Unlike the Gaussian noise, which typically makes the image look blurry, salt-and-pepper noise occurs when extreme values of either $0$ or $1$ are assigned regardless of</description></item><item><title>Paper Review: Kolmogorov-Arnold Neural Network (KAN)</title><link>https://freshrimpsushi.github.io/en/posts/322/</link><pubDate>Sun, 29 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/322/</guid><description>Overview and Summary Kolmogorov–Arnold Networks (KAN) are neural networks inspired by the Kolmogorov–Arnold representation theorem. Despite the idea having been discussed for decades, authors like Girosi and Poggio pointed out in the 1989 paper &amp;lsquo;Representation Properties of Networks:</description></item><item><title>Implementing DeepONet Paper Step by Step (PyTorch)</title><link>https://freshrimpsushi.github.io/en/posts/1153/</link><pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1153/</guid><description>Overview DeepONet is a neural network architecture for learning nonlinear operators and has been applied in various fields such as solving partial differential equations since its paper was published. In this article, we introduce how to implement DeepONet using PyTorch and follow the problems presented in the paper. Paper Review Implementation in Julia DeepONet Theory Let $X$, $X^{\prime}$ be function spaces, and operator $G : X \to X^{\prime}$ be as</description></item><item><title>How to Use Automatic Differentiation in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/1966/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1966/</guid><description>Explanation We introduce how to perform automatic differentiation in PyTorch. In PyTorch, automatic differentiation is implemented through the torch.autograd.grad function. torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=None, is_grads_batched=False, materialize_grads=False) outputs: The function value for which the gradient (automatic differentiation) is calculated. inputs: The point at which the gradient is calculated. grad_outputs: This is the element to be multiplied by the calculated gradient. Usually, it is set to torch.ones_like(outputs). For instance,</description></item><item><title>파이토치로 PINN 논문 구현하기</title><link>https://freshrimpsushi.github.io/en/posts/1967/</link><pubDate>Mon, 16 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1967/</guid><description>Description PINN stands for Physics-Informed Neural Networks and refers to numerically solving differential equations using automatic differentiation and artificial neural networks. In the PINN paper, it has been implemented with TensorFlow. This article explains how to implement it using PyTorch. It proceeds under the assumption that you have read the following two articles. Review of the PINN paper How to use automatic differentiation in PyTorch Schrö</description></item><item><title>How to Set Training and Testing Modes for Neural Networks in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/1975/</link><pubDate>Sat, 31 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1975/</guid><description>Description In the architecture of a neural network, there are components that must operate differently during training and testing phases. For instance, dropout should be applied during training but must not be applied during testing or actual use after training is complete. In such cases, it is necessary to distinguish between training mode and testing mode. Code The function to switch the neural network to training mode is trainmode!, and</description></item><item><title>How to Use GPU in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/2611/</link><pubDate>Mon, 12 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2611/</guid><description>Overview In this article, we introduce how to implement deep learning with Flux.jl,1 a machine learning library for Julia, and how to accelerate the learning performance through the GPU. To use the GPU, it is essential to utilize CUDA.jl2 and to have the CUDA settings properly configured in advance. The setup for CUDA is similar to that in Python, so refer to the following post for guidance: How to Install</description></item><item><title>Paper Review: DeepONet</title><link>https://freshrimpsushi.github.io/en/posts/1180/</link><pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1180/</guid><description>Overview and Summary Follow the references, equation numbers, and notation in the paper as closely as possible. For accessibility, this review is based on the version available on arXiv rather than the journal published version. Although the problems covered in the experimental section differ slightly, the core focus is not on the experimental results and performance but on the explanation of the DeepONet method itself. DeepONet is a deep learning</description></item><item><title>Graduate Student Descent Method</title><link>https://freshrimpsushi.github.io/en/posts/2598/</link><pubDate>Wed, 17 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2598/</guid><description>Buildup The Fridge-Elephant Problem Traditionally, the method for putting an elephant into a fridge has relied on graduate students. How difficult or challenging it could be, or what the best method might be, I&amp;rsquo;m not sure, but that&amp;rsquo;s something the graduate students would figure out, so no worries there. Then, what could be the first measure to try in solving nonlinear optimization problems, including deep learning? Terminology It involves trying</description></item><item><title>Grid Search, Brute Force, Hard Work</title><link>https://freshrimpsushi.github.io/en/posts/2596/</link><pubDate>Sat, 13 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2596/</guid><description>Terminology Grid Search In optimization problems, dividing the Euclidean space $\mathbb{R}^{n}$ into a grid and repeatedly trying as many points as possible to find the optimal solution is referred to as Grid Search. Brute Force In cryptographic problems, attempting every possible combination indiscriminately to crack codes is called Brute Force. Manual Labor To solve problems that are either difficult or undesirable to solve, repeating possible attempts without a particular strategy</description></item><item><title>Difference Between torch.nn and torch.nn.functional in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3626/</link><pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3626/</guid><description>Description PyTorch contains many functions related to neural networks, which are included under the same names in torch.nn and torch.nn.functional. The functions in nn return a neural network as a function, while those in nn.functional are the neural network itself. For instance, nn.MaxPool2d takes the kernel size as input and returns a pooling layer. import torch import torch.nn as nn pool = nn.MaxPool2d(kernel_size = 2) # MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,</description></item><item><title>What are Weights in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/3625/</link><pubDate>Mon, 08 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3625/</guid><description>Definition In machine learning, the parameters to be optimized are called weights.</description></item><item><title>What is Skip Connection in Artificial Neural Networks?</title><link>https://freshrimpsushi.github.io/en/posts/3624/</link><pubDate>Sat, 06 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3624/</guid><description>Definition Let $\mathbf{W}$ be the weight, $\mathbf{x}$ the input, and $\sigma$ the nonlinear activation function. Let&amp;rsquo;s define layer $L_{\mathbf{w}}$ as follows. $$ L_{\mathbf{W}}(\mathbf{x}) := \sigma(\mathbf{W} \mathbf{x}) $$ A function in the form that adds an identity function to the layer like the following is called a skip connection. $$ L_{\mathbf{W}} + I : \mathbf{x} \mapsto \sigma(\mathbf{W} \mathbf{x}) + \mathbf{x} $$ Explanation Typically, the input $\mathbf{x}$ and the weight $\mathbf{W}$ are</description></item><item><title>Using AdaBelief Optimizer in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3620/</link><pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3620/</guid><description>Description AdaBelief, introduced by J. Zhuang et al. in 2020, is one of the variations of Adam1. Since PyTorch does not natively provide this optimizer, it must be installed separately. Code2 Installation The following command can be used to install it via cmd. pip install adabelief-pytorch==0.2.0 Usage The code below can be used to import and utilize it. from adabelief_pytorch import AdaBelief optimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple =</description></item><item><title>Paper Review: Denoising Diffusion Probabilistic Models (DDPM)</title><link>https://freshrimpsushi.github.io/en/posts/3638/</link><pubDate>Sat, 04 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3638/</guid><description>Overview and Summary A generative model refers to a method to find a probability distribution $Y$ that a given random sample $\left\{ y_{i} \right\}$ follows. As it&amp;rsquo;s highly challenging to directly find this from scratch, it&amp;rsquo;s common to use well-known distributions to approximate the desired distribution. Thus, if a dataset $\left\{ x_{i} \right\}$ following a well-known distribution $X$ is given, the generative model can be regarded as a function $f$,</description></item><item><title>Latent Variable and Latent Space</title><link>https://freshrimpsushi.github.io/en/posts/3589/</link><pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3589/</guid><description>Definition Suppose a dataset $X \subset \mathbb{R}^{n}$ is given. A function with the dataset as its domain is called an encoder. $$ \begin{align*} f : X &amp;amp;\to Z \\ \mathbf{x} &amp;amp;\mapsto \mathbf{z} = f(\mathbf{x}) \end{align*} $$ The range of the encoder $Z \subset \mathbb{R}^{m}$ ($m \le n$) is called the latent space, and the elements of the latent space $\mathbf{z}$ are referred to as latent variables or feature vectors. For</description></item><item><title>How to Define and Train MLP with the Sequence Model and Functional API in TensorFlow and Keras</title><link>https://freshrimpsushi.github.io/en/posts/3562/</link><pubDate>Mon, 04 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3562/</guid><description>Overview In TensorFlow, neural networks can be easily defined using Keras. Below, we introduce how to define and train a simple MLP using Sequential() and the functional API. However, Sequential() is only easy for defining models and can be challenging to use for designing complex structures. Similarly, if you plan to design complex structures using the functional API, it’s better to use the keras.Model class, and for</description></item><item><title>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</title><link>https://freshrimpsushi.github.io/en/posts/3529/</link><pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3529/</guid><description>Overview1 2 Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam. Explanation In gradient descent, the learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\alpha$, $\eta$, and determines how much of the gradient is taken into account when updating the parameter. Optimization</description></item><item><title>Momentum Method in Gradient Descent</title><link>https://freshrimpsushi.github.io/en/posts/3528/</link><pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3528/</guid><description>Overview1 2 In gradient descent, the momentum technique involves using all previous gradients when updating parameters. This is its essence and the end of the explanation. However, explanations involving strange update equations, the motivation from physics&amp;rsquo; momentum, setting the mass to $1$ and the initial velocity to $0$, only complicate understanding. This article explains the momentum technique as plainly as possible. Build-Up Let&amp;rsquo;s denote the parameters as $\boldsymbol{\theta}$ and the</description></item><item><title>Modular Arithmetic in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3527/</link><pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3527/</guid><description>Explanation Modular arithmetic, also known as the remainder operation, is a function that returns the remainder when dividing $a$ by $b$. In PyTorch, there are two functions available: torch.remainder(a,b) torch.fmod(a,b) Both provide the remainder when $a$ is divided by $b$, but the outcomes are slightly different. If you&amp;rsquo;re curious about the specific formulas, refer to the official documents for remainder and fmod. Simply put, in remainder, the sign of the</description></item><item><title>Online Learning vs. Batch Learning in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3526/</link><pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3526/</guid><description>Overview This document explains online learning, batch learning, and mini-batch learning. It emphasizes that the names and differences among them are not actually crucial, but rather inconsequential. In fact, it can be assumed that only mini-batch learning is used in recent deep learning. Definition Let&amp;rsquo;s suppose we have a true value $\mathbf{y} = \left\{ y_{i} \right\}_{i=1}^{N}$ and an estimated value $\hat{\mathbf{y}} = \left\{ \hat{y}_{i} \right\}_{i=1}^{N}$. Assuming the loss function for</description></item><item><title>What is Data Augmentation?</title><link>https://freshrimpsushi.github.io/en/posts/3522/</link><pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3522/</guid><description>Definition Suppose a set of data $X = \left\{ x \in \mathbb{R}^{n} \right\}$ is given. Using appropriate transformations $f_{i} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ to obtain $X^{\prime}$ from $X$ is called data augmentation. $$ X^{\prime} = X \cup \left( \bigcup_{i} \left\{ f_{i}(x) : x \in X \right\} \right) $$ Description Simply put, for example with images, this means adding new images to the dataset, which are obtained by applying modifications like</description></item><item><title>Monte Carlo Method</title><link>https://freshrimpsushi.github.io/en/posts/3521/</link><pubDate>Wed, 13 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3521/</guid><description>Definition Simply put, the Monte Carlo method is about doing something many times at random. Description Although it&amp;rsquo;s a simple method, there are few methods as easy and certain as trying many times. Of course, &amp;ldquo;easy&amp;rdquo; refers to when utilizing a computer. Monte Carlo refers to the name of an area famous for its casinos and hotels in northern Monaco. The name &amp;ldquo;Monte Carlo method&amp;rdquo; also comes from the Monte</description></item><item><title>Rejection Sampling</title><link>https://freshrimpsushi.github.io/en/posts/3518/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3518/</guid><description>Overview 1 Rejection sampling is one of the Monte Carlo methods, where a proposal distribution $q$, easy to sample from, is used to obtain samples following a given distribution $p$, especially when it&amp;rsquo;s difficult to sample from $p$ directly. Build-up Let&amp;rsquo;s assume we are given a random variable $X$ with a probability density function $p$. We want to sample from the distribution of $p$, but it&amp;rsquo;s challenging. (For instance, if</description></item><item><title>중요도 샘플링</title><link>https://freshrimpsushi.github.io/en/posts/3516/</link><pubDate>Sun, 03 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3516/</guid><description>Overview1 Importance sampling is one of the Monte Carlo methods, and it is a sampling trick that can be used when approximating integrations (expectations) with finite sums. Build-up The probability that a value from a standard normal distribution exceeds $z$ is about $4$. $$ \begin{equation} \int_{4}^{\infty} \dfrac{1}{\sqrt{2\pi}} e^{-z^{2}/2}dz \approx 3.167 \times 10^{-5} \end{equation} $$ The Julia code to calculate this integral using Monte Carlo integration is as follows. using Distributions</description></item><item><title>Monte Carlo Integration</title><link>https://freshrimpsushi.github.io/en/posts/3515/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3515/</guid><description>Overview Monte Carlo integration is a numerical approximation method used when calculating the integral of a given function is difficult. Consider the following scenario: For an integrable function $f$ in the given $[0, 1]$, we know the formula of $f(x)$ but calculating its integral is not straightforward. However, we want to calculate the integral $I[f]$ of $f$. $$ \begin{equation} I[f] = \int_{[0,1]} f(x) dx \end{equation} $$ Definition Monte Carlo integration</description></item><item><title>Solutions to 'RuntimeError: Parent directory does not exist' Error When Saving Models in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3502/</link><pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3502/</guid><description>Error When saving a model or weights in PyTorch, you may encounter the following error, even though the path definitely exists. &amp;gt;&amp;gt;&amp;gt; print(&amp;#34;Is exists path?: &amp;#34;, os.path.exists(directory)) Is exists path?: True &amp;gt;&amp;gt;&amp;gt; torch.save(model.state_dict(), directory + &amp;#39;weights.pt&amp;#39;) RuntimeError: Parent directory _____ does not exist. Solution In my case, the file path contained the special character deltaΔ, and removing it solved the problem. However, this is</description></item><item><title>Flux-PyTorch-TensorFlow Cheat Sheet</title><link>https://freshrimpsushi.github.io/en/posts/3489/</link><pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3489/</guid><description>Overview This document organizes code that performs the same functions in Flux, PyTorch, and TensorFlow. Julia-Matlab-Python-R Cheat Sheet Let&amp;rsquo;s assume the following environment for Flux. using Flux Let&amp;rsquo;s assume the following environment for PyTorch. import torch import torch.nn as nn import torch.nn.functional as F Let&amp;rsquo;s assume the following environment for TensorFlow. import tensorflow as tf from tensorflow import keras 1-Dimensional Tensor 줄리아Julia 파</description></item><item><title>Functions for Tensor Sorting in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3487/</link><pubDate>Fri, 06 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3487/</guid><description>torch.sort() torch.sort() takes a tensor as input and returns sorted values and indices. 1-dimensional tensor &amp;gt;&amp;gt;&amp;gt; x = torch.tensor([1, 3, -2, 5, -1, 7, 0]) &amp;gt;&amp;gt;&amp;gt; values, indices = torch.sort(x) &amp;gt;&amp;gt;&amp;gt; values tensor([-2, -1, 0, 1, 3, 5, 7]) &amp;gt;&amp;gt;&amp;gt; indices tensor([2, 4, 6, 0, 1, 3, 5]) Multi-dimensional tensor If only the tensor is inputted, it sorts each row. That is, torch.sort(x)=torch.sort(x, dim=1). If a dimension is specified, it</description></item><item><title>Solving 'RuntimeError: Boolean value of Tensor with more than one value is ambiguous' Error in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3486/</link><pubDate>Wed, 04 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3486/</guid><description>Error When using the loss function nn.MESLoss(), the following error occurred. RuntimeError Traceback (most recent call last) &amp;lt;ipython-input-75-8c6e9ea829d4&amp;gt; in &amp;lt;module&amp;gt; ----&amp;gt; 1 nn.MSELoss(y_pred, y) 2 frames /usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py in legacy_get_string(size_average, reduce, emit_warning) 33 reduce = True 34 ---&amp;gt; 35 if size_average and reduce: 36 ret = &amp;#39;mean&amp;#39; 37 elif reduce: RuntimeError: Boolean value of Tensor with more than one value is ambiguous Solution1 The code can be fixed by changing it</description></item><item><title>Sampling Randomly from a Given Distribution in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3485/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3485/</guid><description>Overview Introducing how to random sample from a given distribution in PyTorch. Various distributions such as Beta, Bernoulli, Cauchy, Gamma, Pareto, and Poisson are implemented. This article explains using the uniform distribution as an example. Code1 The code to random sample from the uniform distribution from $0$ to $5$ in PyTorch is as follows: &amp;gt;&amp;gt;&amp;gt; m = torch.distributions.Uniform(0.0, 5) &amp;gt;&amp;gt;&amp;gt; m.sample() tensor(1.6371) To sample a tensor of size $2 \times</description></item><item><title>딥러닝에서 레이어란?</title><link>https://freshrimpsushi.github.io/en/posts/3484/</link><pubDate>Sat, 30 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3484/</guid><description>Definition In deep learning, a linear transformation $L^{mn} : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is called a layer. Generalization In deep learning, for a fixed $\mathbf{b} \in \mathbb{R}^{m}$, an affine transformation $\mathbf{x} \mapsto L^{mn}(\mathbf{x}) + \mathbf{b}$ is also called a layer. Description In other words, a layer refers to a linear vector function. On the other hand, a non-linear scalar function is called an activation function. The reason it is called a</description></item><item><title>Convolutional Neural Network (CNN)</title><link>https://freshrimpsushi.github.io/en/posts/3449/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3449/</guid><description>Definition A composite function obtained by appropriately combining a convolutional layer, pooling layer, and activation function is called a convolutional neural network. Description A function composed of a convolutional layer and an activation function is called a convolutional neural network. CNNs demonstrate excellent performance predominantly in image-related tasks. In the case of MLP, when values pass through each layer, they are sent to a fully connected layer, which can lead</description></item><item><title>Multilayer Perceptron (MLP), Fully Connected Neural Network (FCNN)</title><link>https://freshrimpsushi.github.io/en/posts/3447/</link><pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3447/</guid><description>Definition Let $L_{i} : \mathbb{R}^{n_{i}} \to \mathbb{R}^{n_{i+1}}$ be referred to as a fully connected layer. Let $\sigma : \mathbb{R} \to \mathbb{R}$ be referred to as an activation function. The composition of these is called a multilayer perceptron. $$ \operatorname{MLP}(\mathbf{x}) = T_{N} \circ \overline{\sigma} \circ T_{N-1} \circ \overline{\sigma} \circ \cdots \circ T_{1} (\mathbf{x}) $$ Here, $\overline{\sigma}$ is a function that applies $\sigma$ to each component. $$ \overline{\sigma}(\mathbf{x}) = \begin{bmatrix} \sigma(x_{1}) \\</description></item><item><title>Iris Dataset</title><link>https://freshrimpsushi.github.io/en/posts/3445/</link><pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3445/</guid><description>Overview1 The Iris dataset refers to a dataset about the observation records of iris flowers, created by the American botanist, Edgar Anderson, and introduced by the British statistician, Ronald Fisher2. Description It is the most commonly used dataset in machine learning and data analysis practice.3 It consists of data from observing 50 flowers each of the three species of iris: setosa, versicolor, and virginica. It measures each flo</description></item><item><title>MNIST Database</title><link>https://freshrimpsushi.github.io/en/posts/3444/</link><pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3444/</guid><description>Overview1 $$ \includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$ The MNIST database refers to a dataset of digit handwriting from American high school students and Census Bureau employees. It is commonly known as [MNIST]. Official Website Description This dataset is frequently used as an example for beginners in machine learning/deep learning. NIST originally collected handwritten data in the following format for the evaluation of character recognition technology for automated sorting of handwritten postal codes. Yann</description></item><item><title>Various Deep Learning Frameworks of Julia</title><link>https://freshrimpsushi.github.io/en/posts/3443/</link><pubDate>Sat, 08 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3443/</guid><description>Overview Last modified date: November 22, 2022 Among Julia&amp;rsquo;s representative deep learning frameworks, there is Flux.jl. Along with it, other frameworks such as Knet.jl and Lux.jl will be briefly introduced. Description Flux Flux is the official deep learning framework of Julia. Various packages of GraphNeuralNetworks.jl and SciML are implemented based on Flux. Although Flux&amp;rsquo;s functionality and features are still lacking compared to TensorFlow and PyTorch, it might eventually catch up</description></item><item><title>Automatic differentiation</title><link>https://freshrimpsushi.github.io/en/posts/3442/</link><pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3442/</guid><description>Definition1 2 Automatic differentiation refers to a method for obtaining the derivative of a function defined by computer programming code. It is also abbreviated as AD or autodiff. Explanation Automatic differentiation involves using the chain rule to compute the derivative of a composite function composed of functions whose derivatives are already known. In simple terms, it is the chain rule itself. Implementing the chain rule in programming code constitutes automatic</description></item><item><title>Proof of the Representation Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2408/</link><pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2408/</guid><description>Theorem Let&amp;rsquo;s assume we are given an input set $X \ne \emptyset$ and a positive definite kernel $k: X \times X \to \mathbb{R}$. Define the Training Dataset as $$ D := \left\{ \left( x_{i} , y_{i} \right) \right\}_{i=1}^{m} \subset X \times \mathbb{R} $$ and a class in the Reproducing Kernel Hilbert Space $H_{k}$ as $$ \mathcal{F} := \left\{ f \in \mathbb{R}^{X} : f \left( \cdot \right) = \sum_{i=1}^{\infty} \beta_{i} k</description></item><item><title>What is One-Hot Encoding in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/3438/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3438/</guid><description>Definition Given a set $X \subset \mathbb{R}^{n}$, suppose its subsets $X_{i}$ satisfy the following. $$ X = X_{1} \cup \cdots \cup X_{N} \quad \text{and} \quad X_{i} \cap X_{j} = \varnothing \enspace (i \ne j) $$ Let&amp;rsquo;s call $\beta = \left\{ e_{1}, \dots, e_{N} \right\}$ the standard basis of $\mathbb{R}^{N}$. Then, the following function, or mapping $x \in X$ itself, is called one-hot encoding. $$ \begin{align*} f : X &amp;amp;\to \beta</description></item><item><title>Definite Kernel and Reproducing Kernel Hilbert Space in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/2406/</link><pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2406/</guid><description>Definition 1 2 Input Space $X \ne \emptyset$ is the domain and the codomain is the set of complex numbers $\mathbb{C}$, and let&amp;rsquo;s denote the space of functions $\left( H , \left&amp;lt; \cdot , \cdot \right&amp;gt; \right) \subset \mathbb{C}^{X}$ composed of mappings $f: X \to \mathbb{C}$ as a Hilbert space. Reproducing Kernel Hilbert Space For a fixed datum $x \in X$, the functional $\delta_{x} : H \to \mathbb{C}$, which takes</description></item><item><title>Support Vector Machine</title><link>https://freshrimpsushi.github.io/en/posts/2402/</link><pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2402/</guid><description>Model 1 Simple Definition The method of finding a Support Vector Machine is to find a line or plane that best separates binary classifiable data. Complex Definition For an inner product space $X = \mathbb{R}^{p}$ and labeling $Y = \left\{ -1, +1 \right\}$, let&amp;rsquo;s denote the Training Dataset composed of $n$ pieces of data as $D = \left\{ \left( \mathbf{x}_{k} , y_{k} \right) \right\}_{k=1}^{n} \subset X \times Y$, and $$</description></item><item><title>How to Check the Device on which the PyTorch Model/Tensor is loaded</title><link>https://freshrimpsushi.github.io/en/posts/3364/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3364/</guid><description>Code1 2 It can be checked with get_device(). &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import torch.nn as nn &amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available() True &amp;gt;&amp;gt;&amp;gt; Device = torch.device(&amp;#34;cuda:0&amp;#34; if torch.cuda.is_available() else &amp;#34;cpu&amp;#34;) # Model &amp;gt;&amp;gt;&amp;gt; model = nn.Sequential(nn.Linear(5,10), nn.ReLU(), nn.Linear(10,10), nn.ReLU(), nn.Linear(10,1)) &amp;gt;&amp;gt;&amp;gt; next(model.parameters()).get_device() -1 &amp;gt;&amp;gt;&amp;gt; model.to(Device) Sequential( (0): Linear(in_features=5, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=10, bias=True) (3): ReLU() (4): Linear(in_features=10, out_features=1, bias=True) ) &amp;gt;&amp;gt;&amp;gt; next(model.parameters()).get_device() 0 # Tensor &amp;gt;&amp;gt;&amp;gt; A = torch.rand(5) &amp;gt;&amp;gt;&amp;gt;</description></item><item><title>What is a Softplus Function?</title><link>https://freshrimpsushi.github.io/en/posts/3396/</link><pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3396/</guid><description>Definition1 The function defined below is called a softplus. $$ \zeta (x) = \ln (1 + e^{x}) $$ Ian Goodfellow, Deep Learning, p68&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Fully Connected Layer (Linear Layer, Dense Layer)</title><link>https://freshrimpsushi.github.io/en/posts/3384/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3384/</guid><description>Definition Let&amp;rsquo;s refer to $L: \mathbb{R}^{n} \to \mathbb{R}^{m}$ as a layer. Consider $\mathbf{W}$ as the matrix representation of $L$. When $\mathbf{W}$ consists only of components that are not $0$, $L$ is called a fully connected layer. Explanation A fully connected layer is the most fundamental layer in an artificial neural network. In deep learning, most layers are linear; however, the term linear layer often refers to a fully connected layer.</description></item><item><title>Training/Validation/Test Sets in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3382/</link><pubDate>Wed, 08 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3382/</guid><description>Definitions During training, data sets used are: The data set used to optimize the model&amp;rsquo;s parameters is called the training set. The data set used to optimize the model&amp;rsquo;s hyperparameters is called the validation set. After training, the data set used: The data set used to evaluate the model&amp;rsquo;s performance after training is called the test set. The function value of the loss function for the training/validation/test sets is called</description></item><item><title>Encoder and Decoder in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3380/</link><pubDate>Sat, 04 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3380/</guid><description>Definition Suppose a dataset $X \subset \mathbb{R}^{n}$ is given. In the context of machine learning, an encoder is defined as a function for an appropriate set $Z \subset \mathbb{R}^{m}$ ($m \le n$) such as the following: $$ \begin{align*} f : X &amp;amp;\to Z \\ \mathbf{x} &amp;amp;\mapsto \mathbf{z} = f(\mathbf{x}) \end{align*} $$ The following $g$ is called a decoder: $$ \begin{align*} g : Z &amp;amp;\to X \\ \mathbf{z} &amp;amp;\mapsto \mathbf{x} =</description></item><item><title>What is ReLU in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/3362/</link><pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3362/</guid><description>Definition In machine learning, the following function is referred to as a Rectified Linear Unit. $$ f(x) = x^{+} := \max \left\{ 0, x \right\} $$ Description In electrical engineering, this is known as a ramp function. For a description and properties of the function itself, not from the perspective of its use as an activation function in machine learning, refer to the ramp function article. See Also Dirac delta</description></item><item><title>딥러닝에서 인공신경망(ANN), 심층신경망(DNN), 순방향신경망(FNN)의 뜻과 차이점</title><link>https://freshrimpsushi.github.io/en/posts/3446/</link><pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3446/</guid><description>Overview This document summarizes terms used in deep learning, such as artificial neural networks, deep neural networks, and feedforward neural networks. These terms are often used interchangeably without clear definitions and can be confusing for beginners, but essentially, they can be considered the same. The origins and historical contexts of the terms explained below are not based on exhaustive research and are the author&amp;rsquo;s own hypotheses. Artificial Neural Networks and</description></item><item><title>Convolutional Layer</title><link>https://freshrimpsushi.github.io/en/posts/3386/</link><pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3386/</guid><description>Definition Let $\mathbf{W}$ be $k \times k$ matrix. Define $M^{n\times n} = M^{n\times n}(\mathbb{R})$ to be the set of real matrices of size $n \times n$. A convolutional layer $C_{\mathbf{W}} : M^{nn} \to M^{(n-k+1) \times (n-k+1)}$ is a function defined as follows. For $\mathbf{X} \in M^{n\times n}$ and $\mathbf{Y} = C_{\mathbf{W}}(\mathbf{X})$, $$ \begin{align*} Y_{ij} &amp;amp;= \begin{bmatrix} w_{11} &amp;amp; w_{12} &amp;amp; \cdots &amp;amp; w_{1k} \\ w_{21} &amp;amp; w_{22} &amp;amp; \cdots &amp;amp;</description></item><item><title>Paper Review: Physics-Informed Neural Networks</title><link>https://freshrimpsushi.github.io/en/posts/3313/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3313/</guid><description>Overview The notation and numbering of references and formulas follow the conventions of the original paper. Physics-informed neural networks (referred to as PINN) are artificial neural networks designed to numerically solve differential equations, introduced in the 2018 Journal of Computational Physics paper Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. The authors of the paper are M. Raissi, P. Perdikaris,</description></item><item><title>머신러닝에서 선형회귀모델의 최소제곱법 학습</title><link>https://freshrimpsushi.github.io/en/posts/3263/</link><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3263/</guid><description>Overview1 We introduce a method using the least squares, a learning method for linear regression models. Description Let the dataset be $X = \left\{ \mathbf{x}_{i} \right\}_{i=1}^{N}$, and the label set be $Y = \left\{ y_{i} \right\}_{i=1}^{N}$. Assume the following linear regression model: $$ \hat{y} = \sum\limits_{j=0}^{n-1} w_{j}x_{j} = \mathbf{w}^{T} \mathbf{x} $$ Here, $\mathbf{x} = \begin{bmatrix} x_{0} &amp;amp; \dots &amp;amp; x_{n-1} \end{bmatrix}^{T}$ and $\mathbf{w} = \begin{bmatrix} w_{0} &amp;amp; \dots &amp;amp; w_{n-1}</description></item><item><title>Gradient Descent Learning of Linear Regression Models in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3261/</link><pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3261/</guid><description>Overview1 Introducing a method using gradient descent, one of the learning methods of the linear regression model. Description Let&amp;rsquo;s assume that the data set is $X = \left\{ \mathbf{x}_{i} \right\}_{i=1}^{N}$ and the label set is $Y = \left\{ y_{i} \right\}_{i=1}^{N}$. And let&amp;rsquo;s assume the following linear regression model. $$ \hat{y} = \sum\limits_{j=0}^{n} w_{j}x_{j} = \mathbf{w}^{T} \mathbf{x} $$ At this time, $\mathbf{x} = \begin{bmatrix} x_{0} &amp;amp; \dots &amp;amp; x_{n} \end{bmatrix}^{T}$ and</description></item><item><title>Hierarchical Clustering</title><link>https://freshrimpsushi.github.io/en/posts/3253/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3253/</guid><description>Algorithm Input Given data of dimension $p$ and $N$ instances with a distance $d$. Step 1. Consider each point as a single cluster. Combine the two closest clusters into one. Step 2. Again, combine the two closest clusters into one. Step 3. Repeat until only one cluster remains. Output Returns which cluster each data belongs to and the distance between clusters. The tree structure obtained on the right is called</description></item><item><title>Implementing MLP in Julia Flux to Approximate Nonlinear Functions</title><link>https://freshrimpsushi.github.io/en/posts/3227/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3227/</guid><description>Start Import the necessary packages and define the nonlinear function we want to approximate. Creating the Training Set From the domain of the function $[-5, 5]$, 1024 random points were selected. These points are of type Float64, but deep learning typically handles the Float32 data type, so it was converted. Of course, the model can also automatically convert and run data types like Float64 or Int64 when used as input.</description></item><item><title>Resolving 'TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.' with Lists in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3225/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3225/</guid><description>Error TypeError: can&amp;#39;t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first. Despite dealing with a list, not a PyTorch tensor or a NumPy array, the above error can occur. If you follow the instruction and use the .cpu() or .numpy() methods, you will encounter the following error. AttributeError: &amp;#39;list&amp;#39; object has no attribute &amp;#39;cpu&amp;#39; Solution The reason this error occurs is</description></item><item><title>Implementing MLP in Julia Flux and Learning with MNIST</title><link>https://freshrimpsushi.github.io/en/posts/3221/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3221/</guid><description>Loading the MNIST Dataset In older examples, you might see code using Flux.Data, but this is no longer supported in Flux. julia&amp;gt; Flux.Data.MNIST.images() ┌ Warning: Flux&amp;#39;s datasets are deprecated, please use the package MLDatasets.jl The official documentation1 advises using the `MLDatasets.jl&amp;rsquo; package. julia&amp;gt; using Flux julia&amp;gt; using MLDatasets julia&amp;gt; imgs = MLDatasets.MNIST.traintensor() 28×28×60000 reinterpret(FixedPointNumbers.N0f8, ::Array{UInt8, 3}) julia&amp;gt; labs = MLDatasets.MNIST.trainlabels()</description></item><item><title>How to Perform One-Hot Encoding in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/3219/</link><pubDate>Thu, 14 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3219/</guid><description>Overview One-hot encoding is the process of mapping data to standard basis vectors based on its classification. Flux provides functions for this. Code1 onehot() onehot(x, labels, [default]) Returns x .== labels. However, it does not return the exact same result, but returns a type called OneHotVector. For encoding multiple data points, use onehotbatch() below. julia&amp;gt; 3 .== [1,3,4] 3-element BitVector: 0 1 0 julia&amp;gt; Flux.onehot(3, [1,3,4]) 3-element OneHotVector(::UInt32) with eltype</description></item><item><title>Implementing MLP in Julia Flux and Optimizing with Gradient Descent</title><link>https://freshrimpsushi.github.io/en/posts/3211/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3211/</guid><description>MLP Implementation First, let&amp;rsquo;s load the machine learning package in Julia, Flux.jl, and the optimizer update method update!. using Flux using Flux: update! We can use the Dense() function for linear layers. The Chain() function stacks these linear layers, similar to the Sequential() feature in Keras and PyTorch. julia&amp;gt; model = Chain( Dense(10, 5, relu), Dense(5, 5, relu), Dense(5, 2) ) Chain( Dense(10, 5, relu), # 55 parameters Dense(5, 5,</description></item><item><title>Handling Hidden Layers in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/3209/</link><pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3209/</guid><description>Linear1 In Flux, a linear layer can be implemented as Dense(). Dense(in, out, σ=identity; bias=true, init=glorot_uniform) Dense(W::AbstractMatrix, [bias, σ] The default value for the activation function is the identity function. Well-known functions like relu, tanh, and sigmoid can be used. julia&amp;gt; Dense(5, 2) Dense(5, 2) # 12 parameters julia&amp;gt; Dense(5, 2, relu) Dense(5, 2, relu) # 12 parameters julia&amp;gt;</description></item><item><title>Handling the Dimensions and Sizes of PyTorch Tensors</title><link>https://freshrimpsushi.github.io/en/posts/3205/</link><pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3205/</guid><description>Definition Let&amp;rsquo;s call $A$ a PyTorch tensor. The following pair $(a_{0}, a_{1}, \dots, a_{n-1})$ is called the size of $A$. $$ \text{A.size() = torch.Size}([a_{0}, a_{1}, \dots, a_{n-1} ]) $$ Let&amp;rsquo;s refer to $\prod \limits_{i=0}^{n-1} a_{i} = a_{0} \times a_{1} \times \cdots a_{n-1}$ as the dimension of $A$. Call $A$ a $n$-dimensional tensor. $a_{i}$ are the sizes of the respective $i$th dimensions, which are integers greater than $1$. Since this is</description></item><item><title>How to Pad PyTorch Tensors</title><link>https://freshrimpsushi.github.io/en/posts/3199/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3199/</guid><description>Code 1 torch.nn.functional.pad(input, pad, mode='constant', value=0.0) input: The tensor to pad pad: Where to pad mode: Method of padding value: Value for padding Description pad When using a tensor of dimension $n$ as an input, you can pass up to $2n-$pairs of arguments. $$ ((n-1)_{\text{low}}, (n-1)_{\text{up}}, (n-2)_{\text{low}}, (n-2)_{\text{up}}, \dots, 1_{\text{low}}, 1_{\text{up}}, 0_{\text{low}}, 0_{\text{up}}) $$ $i_{\text{low}}$ indicates how many values to pad before the $i$th dimension&amp;rsquo;s lower index, i.e., before the</description></item><item><title>Using Machine Learning Datasets in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3191/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3191/</guid><description>Description The MLDatasets.jl1 2 package allows for the use of the following datasets. Datasets with links have their usage explained in their respective documents. Vision CIFAR10 CIFAR100 EMNIST FashionMNIST MNIST Omniglot SVHN2 convert2image Mesh FAUST Miscellaneous BostonHousing Iris Mutagenesis Titanic Text PTBLM SMSSpamCollection UD_English Graphs CiteSeer Cora Graph HeteroGraph KarateClub MovieLens OGBDataset OrganicMaterialsDB PolBlogs PubMed Reddit TUDataset For one-hot encoding this data or training methods, refer to the following. How</description></item><item><title>How to Concatenate or Stack Tensors in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3187/</link><pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3187/</guid><description>Concatenate Tensors cat()1 cat(tensors, dim=0) concatenates two or more tensors along a specified dimension. This means that the size of the specified dimension increases when the tensors are concatenated. Therefore, it is natural that the sizes of the other dimensions need to be the same. For example, if there are tensors $(2,2)$ and $(2,3)$, they cannot be concatenated along the 0th dimension but can be concatenated along the 1st dimension.</description></item><item><title>How to Obtain the Weight Values of a Model in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3183/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3183/</guid><description>Explanation Let&amp;rsquo;s define a model as follows. import torch import torch.nn as nn class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.linear = nn.Linear(3, 3, bias=True) self.conv = nn.Conv2d(3, 5, 2) f = Model() Then, you can access the weights and biases of each layer with the .weight and .bias methods, respectively. Note that the values obtained through .weight (.bias) are not tensors but Parameter objects. So, if you want to get</description></item><item><title>How to Deep Copy Tensors in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3177/</link><pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3177/</guid><description>Description PyTorch tensors, like other objects, can be deep-copied using copy.deepcopy(). &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import copy &amp;gt;&amp;gt;&amp;gt; a = torch.ones(2,2) &amp;gt;&amp;gt;&amp;gt; b = a &amp;gt;&amp;gt;&amp;gt; c = copy.deepcopy(a) &amp;gt;&amp;gt;&amp;gt; a += 1 &amp;gt;&amp;gt;&amp;gt; a tensor([[2., 2.], [2., 2.]]) &amp;gt;&amp;gt;&amp;gt; b tensor([[2., 2.], [2., 2.]]) &amp;gt;&amp;gt;&amp;gt; c tensor([[1., 1.], [1., 1.]]) However, this is only possible for tensors that are explicitly defined by the user. For instance, attempting to deep-copy</description></item><item><title>How to Define Artificial Neural Network Layers with Lists and Loops in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3173/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3173/</guid><description>Explanation If there are many layers to stack or if there is a need to frequently change the structure of the neural network, one might want to automate the definition of the artificial neural network. In such cases, you might think of defining it using the following for loop. class Model(nn.Module): def __init__(self): super(Model, self).__init__() fc_ = [nn.Linear(n,n) for i in range(m)] def forward(self, x): for i in range(m): x</description></item><item><title>Paper Review: Neural Ordinary Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/3159/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3159/</guid><description>Overview and Summary &amp;ldquo;Neural Ordinary Differential Equations&amp;rdquo; is a paper published in 2018 by Ricky T. Q. Chen and three others, and it was selected for 2018 NeurIPS Best Papers. It proposes a method to approximate a simple first-order differential equation, which is a non-autonomous system, using neural networks. $$ \dfrac{\mathrm{d}y}{\mathrm{d}t} = f(y, t) $$ Notably, what the neural network approximates (predicts) is not the $y$ but the rate of</description></item><item><title>Creating Random Permutations and Shuffling Tensor Order in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3140/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3140/</guid><description>torch.randperm()1 torch.randperm(n): Returns a random permutation of integers from 0 to n-1. Of course, non-integer types cannot be used as input. &amp;gt;&amp;gt;&amp;gt; torch.randperm(4) tensor([2, 1, 0, 3]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(8) tensor([4, 0, 1, 3, 2, 5, 6, 7]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(16) tensor([12, 5, 6, 3, 15, 13, 2, 4, 7, 11, 1, 0, 9, 10, 14, 8]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(4.0) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; TypeError: randperm():</description></item><item><title>Saving and Loading Weights, Models, and Optimizers in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3114/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3114/</guid><description>Not Re-training1 2 3 Saving If you&amp;rsquo;re not planning to re-train, you can simply save the weights or the entire model. However, as mentioned below, if you&amp;rsquo;re planning to re-train, you also need to save the optimizer. Weights can be easily saved as follows: # 모델 정의 class CustomModel(nn.module): ...(이하생략) model = CustomModel() # 가중치 저장 torch.save(model.state_dict(),</description></item><item><title>Creating and Using Custom Datasets from Numpy Arrays in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3108/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3108/</guid><description>Description &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; from torch.utils.data import TensorDataset, DataLoader Assuming that a stack of 100 &amp;lsquo;black and white&amp;rsquo; photographs of size $32\times 32$ represented as a numpy array $X$, along with their labels $Y$, has been prepared. Let&amp;rsquo;s say it was imported with the following code. &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#34;X.npy&amp;#34;) &amp;gt;&amp;gt;&amp;gt; X.shape (100, 32, 32) &amp;gt;&amp;gt;&amp;gt; Y = np.load(&amp;#34;Y.npy&amp;#34;) &amp;gt;&amp;gt;&amp;gt; Y.shape (100) In order</description></item><item><title>Initializing Weights in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3104/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3104/</guid><description>Code1 Assuming we have defined a neural network as follows. The forward part is omitted. import torch import torch.nn as nn class Custom_Net(nn.Module): def __init__(self): super(Custom_Net, self).__init__() self.linear_1 = nn.Linear(1024, 1024, bias=False) self.linear_2 = nn.Linear(1024, 512, bias=False) self.linear_3 = nn.Linear(512, 10, bias=True) torch.nn.init.constant_(self.linear_1.weight.data, 0) torch.nn.init.unifiom_(self.linear_2.weight.data) torch.nn.init.xavier_normal_(self.linear_3.weight.data) torch.nn.init.xavier_normal_(self.linear_3.bias.data) def forward(self, x): ... Weight initialization can be set through nn.init. For layers with bias, this also needs to be specifically set. Basics</description></item><item><title>How to Implement MLP in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3103/</link><pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3103/</guid><description>Library import torch import torch.nn as nn import torch.nn.functional as F nn and nn.functional include various layers, loss functions, activation functions, etc., for constructing neural networks.</description></item><item><title>PyTorch RuntimeError: "grad can be implicitly created only for scalar outputs" Solution</title><link>https://freshrimpsushi.github.io/en/posts/3095/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3095/</guid><description>Example 1 If you have set the loss function as loss = sum(a,b), an error might occur during backpropagation when loss.backward() is called. Changing it to loss = torch.sum(a,b) will prevent the error.</description></item><item><title>Back Propagation Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/3077/</link><pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3077/</guid><description>This article is written for math majors to understand the principles of the backpropagation algorithm. Notation Given an artificial neural network like the one shown above. Let $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n_{0}})$ be the input, $y_{j}^{l}$ be the $j$th node of the $l$th layer, $\hat{\mathbf{y}} = (\hat{y}_{1}, \hat{y}_{2}, \dots, \hat{y}_{\hat{n}})$ is the output . Let $L \in \mathbb{N}$ be the number of hidden layers, and the components of $\mathbf{n}=(n_{0},</description></item><item><title>Perceptron Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3023/</link><pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3023/</guid><description>Let&amp;rsquo;s say we have a training set that is linearly separable as represented by $X^{+}$, $X^{-}$. Consider $y$ as the following labels. $$ y_{i} = \pm 1\ (\mathbf{x}_{i} \in X^{\pm}) $$ Suppose the entire training set $X = X^{+} \cup X^{-}$ has $N$ data points. Then, let&amp;rsquo;s say we insert input values in the following order. $$ \mathbf{x}(1), \mathbf{x}(2), \cdots \mathbf{x}(N), \mathbf{x}(1), \mathbf{x}(2), \cdots \mathbf{x}(N),\mathbf{x}(1), \mathbf{x}(2), \cdots $$ That is,</description></item><item><title>What is Reinforcement Learning in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3029/</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3029/</guid><description>Definition Reinforcement learning is the process where an agent interacts with the environment to find a policy that maximizes the cumulative reward. Description1 The elements comprising reinforcement learning are as follows: Agent: Decides actions based on a policy, given a state. State: Refers to the situation in which the agent is placed. Action: Refers to the choices available to the agent in a given state. Policy: Refers to the strategy</description></item><item><title>딥러닝의 수학적 근거, 시벤코 정리 증명</title><link>https://freshrimpsushi.github.io/en/posts/1853/</link><pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1853/</guid><description>Theorem $\sigma$ is said to be a continuous sigmoidal function if $$ S := \left\{ G(x) = \sum_{k=1}^{N} \alpha_{k} \sigma \left( y_{k}^{T} x+ \theta_{k} \right) : y_{k} \in \mathbb{R}^{n} \land \alpha_{k} , \theta_{k} \in \mathbb{R} \land N \in \mathbb{N} \right\} $$ is uniformly dense in $C\left( I_{n} \right)$. In other words, for all $f \in C \left( I_{n} \right)$ and $\varepsilon &amp;gt; 0$, there exists a $G \in S$ that</description></item><item><title>What is a Sigmoid Function?</title><link>https://freshrimpsushi.github.io/en/posts/1851/</link><pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1851/</guid><description>Definition A function $\sigma : \mathbb{R} \to \mathbb{R}$ is called a Sigmoidal Function if it satisfies the following. $$ \sigma (t) \to \begin{cases} 1 &amp;amp; \text{as } t \to + \infty \\ 0 &amp;amp; \text{as } t \to - \infty \end{cases} $$ Explanation of the Definition In the definition of a sigmoidal function, whether it&amp;rsquo;s $0$ or $1$ is not really important, but it&amp;rsquo;s important that it converges to a</description></item><item><title>What is a Discriminant Function?</title><link>https://freshrimpsushi.github.io/en/posts/1838/</link><pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1838/</guid><description>Definition A function $\sigma : \mathbb{R} \to \mathbb{R}$ that satisfies $$ \int_{I_{n}} \sigma \left( y^{T} x + \theta \right) d \mu (x) = 0 \implies \mu =0 $$ for all $y \in \mathbb{R}^{n}$ and $\theta \in \mathbb{R}$ and some $\mu \in M \left( I_{n} \right)$ is called a Discriminatory Function. $I_{n} := [0,1]^{n}$ is the $n$-dimensional unit cube, which is the Cartesian product of $n$ unit closed intervals $[0,1]$. $M</description></item><item><title>Linear Models for Regression in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/1887/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1887/</guid><description>Definition1 Simple Model Let&amp;rsquo;s define the target function $f : X \to Y$ between the data set $X = \left\{ \mathbf{x}_{i} \right\}$ and the label set $Y = \left\{ y_{i} \right\}$ as follows. $$ y_{i} = f(\mathbf{x}_{i}) $$ In machine learning, linear regression refers to finding a linear function $\hat{f}$ that satisfies the following equation for $\mathbf{w}$. $$ y_{i} \approx \hat{y}_{i} = \hat{f}(\mathbf{x}_{i}, \mathbf{w}) = w_{0} + w_{1}x_{1} + \cdots</description></item><item><title>What is a Logistic Function?</title><link>https://freshrimpsushi.github.io/en/posts/1775/</link><pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1775/</guid><description>Definition [^1] The logistic function is derived as $y ' = y(1-y)$, which is a solution to the differential equation. $$ y(t) = {{ 1 } \over { 1 + e^{-t} }} $$ Explanation In a more general form, it can also be expressed as $\displaystyle f(x) := {{ L } \over { 1 + e^{-k(x-x_{0})} }}$. The logistic function, which is a sigmoid function, is widely mentioned in various</description></item><item><title>What is a Sigmoid Function?</title><link>https://freshrimpsushi.github.io/en/posts/1769/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1769/</guid><description>Definition 1 A Sigmoid Function is defined as a bounded, differentiable scalar function $\sigma : \mathbb{R} \to \mathbb{R}$ that is defined for all $x \in \mathbb{R}$, is $\sigma ' (x) \ge 0$, and has exactly one inflection point. Sigmoidal functions are defined differently. Types Examples of sigmoid functions include: Logistic function: $\displaystyle f(x) := {{ 1 } \over { 1 + e^{-x} }}$ Hyperbolic tangent: $\tanh x$ Arctangent: $\arctan x$</description></item><item><title>Perceptron Definition</title><link>https://freshrimpsushi.github.io/en/posts/1846/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1846/</guid><description>Definition A perceptron is defined as the composition of a linear function $f(x) = wx + b$ and a unit step function $H$. $$ \text{Perceptron} := H \circ f (x) = H(wx + b) $$ In the case of a multivariable function, $f(\mathbf{x}) = \mathbf{w}\cdot \mathbf{x} + b = w_{1}x_{1} + \cdots w_{n}x_{n} + b$ and, $$ \text{Perceptron} := H \circ f (\mathbf{x}) = H(\mathbf{w} \cdot \mathbf{x} + b) $$</description></item><item><title>What is Computer Vision</title><link>https://freshrimpsushi.github.io/en/posts/1839/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1839/</guid><description>Explanation Computer vision is the research area that allows computers to perform functions corresponding to human vision, mainly dealing with images and videos. The conferences specialized in computer vision include ICCV (International Conference on Computer Vision), ECCV (European Conference on Computer Vision), and CVPR (Conference on Computer Vision and Pattern Recognition). The problems primarily handled in computer vision can be classified into three major categories, as shown in the picture</description></item><item><title>Continuous Learning in Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/1837/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1837/</guid><description>Explanation Continual learning in deep learning refers to the sequential learning of multiple tasks by artificial neural networks, synonymous with lifelong learning or incremental learning. Unlike humans, who do not forget existing knowledge simply by learning something new – though they may forget over time, not due to the acquisition of new knowledge – artificial neural networks exhibit a decline in performance on previously learned tasks after sufficiently learning one</description></item><item><title>Paper Review: Do We Need Zero Training Loss After Achieving Zero Training Error?</title><link>https://freshrimpsushi.github.io/en/posts/1809/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1809/</guid><description>Paper Review Flooding refers to a regularization technique introduced in Do We Need Zero Training Loss After Achieving Zero Training Error?, presented at ICML 2020. According to the authors of the paper, the root cause of overfitting is the excessively low training loss as illustrated below. Thus, the core idea of the paper is that controlling the training loss not to fall below a certain value during the learning process,</description></item><item><title>Commonly Used Datasets in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/1808/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1808/</guid><description>Computer Vision MNIST This is the first dataset that one encounters when studying machine learning. It is pronounced as [em-nist] and consists of hand-written digit images of size $28\times 28$. The dataset includes 60,000 training images and 10,000 testing images[^1]. CIFAR-10, CIFAR-100 CIFAR-10, pronounced as [cypher-ten], includes 60,000 images in 10 different categories, with images of size $32\times 32$. It is composed of 50,000 training images and 10,000 testing images.</description></item><item><title>What is Overfitting and Regularization in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/1807/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1807/</guid><description>Overfitting The phenomenon where the training loss decreases, but the test loss (or validation loss) does not decrease or rather increases is called overfitting. Explanation There is also a term called underfitting, which basically means the opposite, but frankly, it&amp;rsquo;s a meaningless term and not often used in practice. A crucial point in machine learning is that the function trained with the available data must also work well with new</description></item><item><title>k-Means Clustering</title><link>https://freshrimpsushi.github.io/en/posts/1365/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1365/</guid><description>Algorithm Input Given data of dimension $p$, $N$ pieces, and a natural number $k$. Step 1. Initialization Randomly set $k$ points $\mu_{1} , \cdots , \mu_{k}$. Each $\mu_{j}$ will become the mean of cluster $M_{j}$. Step 2. Distance Calculation Calculate $\| x_{i} - \mu_{j} \|$ for the $i$th data $x_{i}$ and $j = 1 , \cdots , k$. Choose the smallest one to make it $x_{i} \in M_{j}$. Repeat this</description></item><item><title>Supervised and Unsupervised Learning</title><link>https://freshrimpsushi.github.io/en/posts/1013/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1013/</guid><description>Definition In machine learning, the case where the dependent variable is specified is called supervised learning, and the case where it is not specified is called unsupervised learning. Example The difference between supervised and unsupervised learning can be simply compared to the difference between multiple-choice and essay questions. For example, let&amp;rsquo;s say there is a classification problem asking for the color of 6 tiles like above. Supervised Learning But here,</description></item><item><title>Dropout in Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/1004/</link><pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1004/</guid><description>Definition Dropout is a technique to prevent overfitting by stochastically not using neurons in an artificial neural network. Explanation At first glance, it might seem like it&amp;rsquo;s just learning less, and to some extent, that&amp;rsquo;s true. By not using neurons with a certain probability, it&amp;rsquo;s possible to ignore neurons that are &amp;rsquo;too influential&amp;rsquo;. Being too influential can be seen as being too confident about the training data. If there are</description></item><item><title>Softmax Function in Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/993/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/993/</guid><description>Definition Let&amp;rsquo;s refer to it as $\mathbf{x} := (x_{1} , \cdots , x_{n}) \in \mathbb{R}^{n}$. For $\displaystyle \sigma_{j} ( \mathbf{x} ) = {{ e^{x_{j}} } \over {\sum_{i=1}^{n} e^{x_{i}} }}$, $\sigma ( \mathbf{x} ) := \left( \sigma_{1} (\mathbf{x}) , \cdots , \sigma_{n} (\mathbf{x} ) \right)$ is defined as $\sigma : \mathbb{R}^{n} \to (0,1)^{n}$, which is called the softmax. Explanation The softmax function is a type of activation function characterized by its</description></item><item><title>Activation Functions in Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/991/</link><pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/991/</guid><description>Definition An non-linear function that mimics the threshold of real-life organisms is known as an activation function. Mathematical Definition In deep learning, a non-linear scalar function $\sigma : \mathbb{R}^{n} \to \mathbb{R}$ is referred to as an activation function. Of course, there are exceptions like the softmax which don&amp;rsquo;t fit into this definition. Explanation On the other hand, a vector function is called a layer. If there is an expression or</description></item><item><title>What is Deep Learning?</title><link>https://freshrimpsushi.github.io/en/posts/996/</link><pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/996/</guid><description>Definition Deep learning is a type of machine learning that uses artificial neural networks, especially employing multiple layers when constructing these networks. Motivation Just like the human brain is composed of a complex network of neurons, deep learning also enhances performance by making the connections in artificial neural networks more complex. Similar to how the stimuli received by sensory cells are transmitted to the brain through the spinal cord, artificial</description></item><item><title>Gradient Descent and Stochastic Gradient Descent in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/987/</link><pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/987/</guid><description>Overview The Gradient Descent Algorithm is the simplest method among algorithms that find the local minimum of the loss function by using the gradient of the loss function. Description Here, the loss function $L$ is considered a function of weights and biases with the dataset $X$ being fixed. If the input data looks like $\mathbf{x} \in \mathbb{R}^{m}$, then $L$ becomes a function of $(w_{1} , w_{2} , \cdots , w_{m}</description></item><item><title>Loss Functions in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/967/</link><pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/967/</guid><description>Definition When an estimate for the data $Y = \begin{bmatrix} y_{1} \\ \vdots \\ y_{n} \end{bmatrix}$ is given as $\widehat{Y} = \begin{bmatrix} \widehat{ y_{1} } \\ \vdots \\ \widehat{y_{n}} \end{bmatrix}$, the scalar function $L : \mathbb{R}^{n} \to [ 0 , \infty )$ that represents the discrepancy between the data and its estimate is called a loss function. Alternate Names The loss function is used as an indicator to evaluate how</description></item><item><title>인공 신경망이란?</title><link>https://freshrimpsushi.github.io/en/posts/962/</link><pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/962/</guid><description>Definition A network that mimics the nervous system of living organisms is called an artificial neural network. Mathematical Definition For a scalar function $\sigma : \mathbb{R} \to \mathbb{R}$, the notation $\overline{\sigma}$ is defined as follows: $$ \overline{\sigma}(\mathbf{x}) = \begin{bmatrix} \sigma(x_{1}) \\ \sigma(x_{2}) \\ \vdots \\ \sigma(x_{n}) \end{bmatrix} \qquad \text{where } \mathbf{x} = \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} $$ In deep learning, a linear vector function $L</description></item><item><title>Comparing Models Using the AUC of ROC Curves</title><link>https://freshrimpsushi.github.io/en/posts/887/</link><pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/887/</guid><description>Theorem ROC curves are essentially better as they fill up the rectangle $[0,1]^2$ more, and it is preferable for the curve&amp;rsquo;s turning point in the upper left to be closer to $(0,1)$. Description Given the two ROC curves above, the right side can comfortably be considered &amp;lsquo;better&amp;rsquo;. This &amp;lsquo;better&amp;rsquo; refers to the model generating the ROC curve being superior. Naturally, the two models being compared are derived from the same</description></item><item><title>Finding the optimal cutoff using ROC curves</title><link>https://freshrimpsushi.github.io/en/posts/877/</link><pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/877/</guid><description>Overview Drawing the ROC Curve is useful as it provides a visual representation of how well a model derived from training data explains the test data. However, since this curve computes the classification rate for all cutoffs, it cannot definitively tell us &amp;lsquo;which cutoff to use for classifying 0 and 1&amp;rsquo;. To address this, let&amp;rsquo;s apply the methodology of cross-validation. Validation Data Finding the optimal cutoff that best classifies 0</description></item><item><title>Drawing ROC Curves in R</title><link>https://freshrimpsushi.github.io/en/posts/868/</link><pubDate>Wed, 20 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/868/</guid><description>Definition The graph that plots the False Positive Rate and True Positive Rate of a confusion matrix is called the ROC curve (Receiver Operating Characteristic Curve). Explanation The ROC curve is extremely useful not only for providing a clear picture of a model&amp;rsquo;s performance but also for finding the optimal cutoff and comparing different models. Let&amp;rsquo;s understand its meaning and draw an ROC curve in R using an example. The</description></item><item><title>Cross-validation</title><link>https://freshrimpsushi.github.io/en/posts/866/</link><pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/866/</guid><description>Model Validation Data analysis often results in a model that needs to be evaluated to determine if its performance is adequate. If the model only explains the given data well and is totally ineffective in practice, then the analysis is meaningless. For this reason, the entire data set is split into a dataset used for obtaining the model and another for evaluating the performance of the model. Obtaining a model</description></item><item><title>Confusion Matrix, Sensitivity, and Specificity</title><link>https://freshrimpsushi.github.io/en/posts/571/</link><pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/571/</guid><description>Definition Let&amp;rsquo;s assume we have a model for distinguishing between positive $P$ and negative $N$ in a classification problem. We define the number of positives correctly predicted as true positives $TP$, the number of negatives correctly predicted as true negatives $TN$, the number of positives incorrectly predicted as negatives as false negatives $FN$, and the number of negatives incorrectly predicted as positives as false positives $FP$. Confusion Matrix In classification</description></item></channel></rss>