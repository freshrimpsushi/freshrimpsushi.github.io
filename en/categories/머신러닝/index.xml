<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>머신러닝 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/</link>
    <description>Recent content in 머신러닝 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 29 Dec 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</title>
      <link>https://freshrimpsushi.github.io/en/posts/3529/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3529/</guid>
      <description>Overview1 2 Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam. Explanation In gradient descent, the learning rate learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\alpha$, $\eta$, and determines how much of the gradient is taken into account when updating the</description>
    </item>
    <item>
      <title>Momentum Method in Gradient Descent</title>
      <link>https://freshrimpsushi.github.io/en/posts/3528/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3528/</guid>
      <description>Overview1 2 In gradient descent, the momentum technique involves using all previous gradients when updating parameters. This is its essence and the end of the explanation. However, explanations involving strange update equations, the motivation from physics&amp;rsquo; momentum, setting the mass to $1$ and the initial velocity to $0$, only complicate understanding. This article explains the momentum technique as plainly as possible. Build-Up Let&amp;rsquo;s denote the parameters as $\boldsymbol{\theta}$ and the</description>
    </item>
    <item>
      <title>Monte Carlo Integration</title>
      <link>https://freshrimpsushi.github.io/en/posts/3515/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3515/</guid>
      <description>Overview Monte Carlo integration is a numerical approximation method used when calculating the integral of a given function is difficult. Consider the following scenario: For an integrable function $f$ in the given $[0, 1]$or generally $[0, 1]^{n}$, we know the formula of $f(x)$ but calculating its integral is not straightforward. However, we want to calculate the integral $I[f]$ of $f$. $$ \begin{equation} I[f] = \int_{[0,1]} f(x) dx \end{equation} $$ Definition</description>
    </item>
    <item>
      <title>Flux-PyTorch-TensorFlow Cheat Sheet</title>
      <link>https://freshrimpsushi.github.io/en/posts/3489/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3489/</guid>
      <description>Overview This document organizes code that performs the same functions in Flux, PyTorch, and TensorFlow. Julia-Matlab-Python-R Cheat Sheet Let&amp;rsquo;s assume the following environment for Flux. using Flux Let&amp;rsquo;s assume the following environment for PyTorch. import torch import torch.nn as nn import torch.nn.functional as F Let&amp;rsquo;s assume the following environment for TensorFlow. import tensorflow as tf from tensorflow import keras 1-Dimensional Tensor 줄리아Julia 파</description>
    </item>
    <item>
      <title>MNIST Database</title>
      <link>https://freshrimpsushi.github.io/en/posts/3444/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3444/</guid>
      <description>Overview1 $$ \includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$ The MNISTmodified national institute of standards and technology database refers to a dataset of digit handwriting from American high school students and Census Bureau employees. It is commonly known as [MNIST]. Official Website Description This dataset is frequently used as an example for beginners in machine learning/deep learning. NIST originally collected handwritten data in the following format for the evaluation of character recognition technology for automated</description>
    </item>
    <item>
      <title>Proof of the Representation Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/2408/</link>
      <pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2408/</guid>
      <description>Theorem Let&amp;rsquo;s assume we are given an input set $X \ne \emptyset$ and a positive definite kernel $k: X \times X \to \mathbb{R}$. Define the Training Dataset as $$ D := \left\{ \left( x_{i} , y_{i} \right) \right\}_{i=1}^{m} \subset X \times \mathbb{R} $$ and a class in the Reproducing Kernel Hilbert Space $H_{k}$ as $$ \mathcal{F} := \left\{ f \in \mathbb{R}^{X} : f \left( \cdot \right) = \sum_{i=1}^{\infty} \beta_{i} k</description>
    </item>
    <item>
      <title>Definite Kernel and Reproducing Kernel Hilbert Space in Machine Learning</title>
      <link>https://freshrimpsushi.github.io/en/posts/2406/</link>
      <pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2406/</guid>
      <description>Definition 1 2 Input Space $X \ne \emptyset$ is the domain and the codomain is the set of complex numbers $\mathbb{C}$, and let&amp;rsquo;s denote the space of functions $\left( H , \left&amp;lt; \cdot , \cdot \right&amp;gt; \right) \subset \mathbb{C}^{X}$ composed of mappings $f: X \to \mathbb{C}$ as a Hilbert space. Reproducing Kernel Hilbert Space For a fixed datum $x \in X$, the functional $\delta_{x} : H \to \mathbb{C}$, which takes</description>
    </item>
    <item>
      <title>Support Vector Machine</title>
      <link>https://freshrimpsushi.github.io/en/posts/2402/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2402/</guid>
      <description>Model 1 Simple Definition The method of finding a Support Vector Machine is to find a line or plane that best separates binary classifiable data. Complex Definition For an inner product space $X = \mathbb{R}^{p}$ and labeling $Y = \left\{ -1, +1 \right\}$, let&amp;rsquo;s denote the Training Dataset composed of $n$ pieces of data as $D = \left\{ \left( \mathbf{x}_{k} , y_{k} \right) \right\}_{k=1}^{n} \subset X \times Y$, and $$</description>
    </item>
    <item>
      <title>Paper Review: Physics-Informed Neural Networks</title>
      <link>https://freshrimpsushi.github.io/en/posts/3313/</link>
      <pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3313/</guid>
      <description>Overview The notation and numbering of references and formulas follow the conventions of the original paper. Physics-informed neural networks (referred to as PINN[pronounced &amp;lsquo;pin&amp;rsquo;]) are artificial neural networks designed to numerically solve differential equations, introduced in the 2018 Journal of Computational Physics paper Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. The authors of the paper are M. Raissi, P.</description>
    </item>
    <item>
      <title>Back Propagation Algorithm</title>
      <link>https://freshrimpsushi.github.io/en/posts/3077/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3077/</guid>
      <description>This article is written for math majors to understand the principles of the backpropagation algorithm. Notation Given an artificial neural network like the one shown above. Let $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n_{0}})$ be the inputinput, $y_{j}^{l}$ be the $j$th node of the $l$th layer, $\hat{\mathbf{y}} = (\hat{y}_{1}, \hat{y}_{2}, \dots, \hat{y}_{\hat{n}})$ is the output output. Let $L \in \mathbb{N}$ be the number of hidden layers, and the components of $\mathbf{n}=(n_{0},</description>
    </item>
    <item>
      <title>What is Reinforcement Learning in Machine Learning</title>
      <link>https://freshrimpsushi.github.io/en/posts/3029/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3029/</guid>
      <description>Definition Reinforcement learning is the process where an agent interacts with the environment to find a policy that maximizes the cumulative reward. Description1 The elements comprising reinforcement learning are as follows: Agentagent: Decides actions based on a policy, given a state. Statestate: Refers to the situation in which the agent is placed. Actionaction: Refers to the choices available to the agent in a given state. Policypolicy: Refers to the strategy</description>
    </item>
    <item>
      <title>What is a Logistic Function?</title>
      <link>https://freshrimpsushi.github.io/en/posts/1775/</link>
      <pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1775/</guid>
      <description>Definition [^1] The logistic function is derived as $y &#39; = y(1-y)$, which is a solution to the differential equation. $$ y(t) = {{ 1 } \over { 1 + e^{-t} }} $$ Explanation In a more general form, it can also be expressed as $\displaystyle f(x) := {{ L } \over { 1 + e^{-k(x-x_{0})} }}$. The logistic function, which is a sigmoid function, is widely mentioned in various</description>
    </item>
    <item>
      <title>What is Overfitting and Regularization in Machine Learning?</title>
      <link>https://freshrimpsushi.github.io/en/posts/1807/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1807/</guid>
      <description>Overfitting The phenomenon where the training loss decreases, but the test loss (or validation loss) does not decrease or rather increases is called overfitting. Explanation There is also a term called underfitting, which basically means the opposite, but frankly, it&amp;rsquo;s a meaningless term and not often used in practice. A crucial point in machine learning is that the function trained with the available data must also work well with new</description>
    </item>
    <item>
      <title>Activation Functions in Deep Learning</title>
      <link>https://freshrimpsushi.github.io/en/posts/991/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/991/</guid>
      <description>Definition An non-linear function that mimics the threshold of real-life organisms is known as an activation function. Mathematical Definition In deep learning, a non-linear scalar function $\sigma : \mathbb{R}^{n} \to \mathbb{R}$ is referred to as an activation function. Of course, there are exceptions like the softmax which don&amp;rsquo;t fit into this definition. Explanation On the other hand, a vector function is called a layer. If there is an expression or</description>
    </item>
    <item>
      <title>What is Deep Learning?</title>
      <link>https://freshrimpsushi.github.io/en/posts/996/</link>
      <pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/996/</guid>
      <description>Definition Deep learning is a type of machine learning that uses artificial neural networks, especially employing multiple layers when constructing these networks. Motivation Just like the human brain is composed of a complex network of neurons, deep learning also enhances performance by making the connections in artificial neural networks more complex. Similar to how the stimuli received by sensory cells are transmitted to the brain through the spinal cord, artificial</description>
    </item>
    <item>
      <title>Gradient Descent and Stochastic Gradient Descent in Machine Learning</title>
      <link>https://freshrimpsushi.github.io/en/posts/987/</link>
      <pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/987/</guid>
      <description>Overview The Gradient Descent Algorithm is the simplest method among algorithms that find the local minimum of the loss function by using the gradient of the loss function. Description Here, the loss function $L$ is considered a function of weights and biases with the dataset $X$ being fixed. If the input data looks like $\mathbb{x} \in \mathbb{R}^{m}$, then $L$ becomes a function of $(w_{1} , w_{2} , \cdots , w_{m}</description>
    </item>
    <item>
      <title>Loss Functions in Machine Learning</title>
      <link>https://freshrimpsushi.github.io/en/posts/967/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/967/</guid>
      <description>Definition When an estimate for the data $Y = \begin{bmatrix} y_{1} \\ \vdots \\ y_{n} \end{bmatrix}$ is given as $\widehat{Y} = \begin{bmatrix} \widehat{ y_{1} } \\ \vdots \\ \widehat{y_{n}} \end{bmatrix}$, the scalar function $L : \mathbb{R}^{n} \to [ 0 , \infty )$ that represents the discrepancy between the data and its estimate is called a loss function. Alternate Names The loss function is used as an indicator to evaluate how</description>
    </item>
    <item>
      <title>What is an Artificial Neural Network?</title>
      <link>https://freshrimpsushi.github.io/en/posts/962/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/962/</guid>
      <description>Definition An artificial neural network (ANN) is a network mimicking the nervous system of actual organisms. Mathematical Definition In deep learning, a vector function $W : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is referred to as a layer. In deep learning, a nonlinear scalar function $\sigma : \mathbb{R} \to \mathbb{R}$ is referred to as an activation function. The composition $\sigma \circ W$ of layers and activation functions is called an artificial neural network.</description>
    </item>
    <item>
      <title>Confusion Matrix, Sensitivity, and Specificity</title>
      <link>https://freshrimpsushi.github.io/en/posts/571/</link>
      <pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/571/</guid>
      <description>Definition Let&amp;rsquo;s assume we have a model for distinguishing between positive $P$ and negative $N$ in a classification problem. We define the number of positives correctly predicted as true positives $TP$, the number of negatives correctly predicted as true negatives $TN$, the number of positives incorrectly predicted as negatives as false negatives $FN$, and the number of negatives incorrectly predicted as positives as false positives $FP$. Confusion Matrix In classification</description>
    </item>
    <item>
      <title>Difference Between torch.nn and torch.nn.functional in PyTorch</title>
      <link>https://freshrimpsushi.github.io/en/posts/3626/</link>
      <pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3626/</guid>
      <description>Description PyTorch contains many functions related to neural networks, which are included under the same names in torch.nn and torch.nn.functional. The functions in nn return a neural network as a function, while those in nn.functional are the neural network itself. For instance, nn.MaxPool2d takes the kernel size as input and returns a pooling layer. import torch import torch.nn as nn pool = nn.MaxPool2d(kernel_size = 2) # MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,</description>
    </item>
    <item>
      <title>파이토치에서 AdaBelief 옵티마이저 사용하는 방법</title>
      <link>https://freshrimpsushi.github.io/en/posts/3620/</link>
      <pubDate>Sat, 28 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3620/</guid>
      <description>Description AdaBelief, introduced by J. Zhuang et al. in 2020, is one of the variations of Adam1. Since PyTorch does not natively provide this optimizer, it must be installed separately. Code2 Installation The following command can be used to install it via cmd. pip install adabelief-pytorch==0.2.0 Usage The code below can be used to import and utilize it. from adabelief_pytorch import AdaBelief optimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple =</description>
    </item>
    <item>
      <title>How to Define and Train MLP with the Sequence Model and Functional API in TensorFlow and Keras</title>
      <link>https://freshrimpsushi.github.io/en/posts/3562/</link>
      <pubDate>Tue, 04 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3562/</guid>
      <description>Overview In TensorFlow, neural networks can be easily defined using Keras. Below, we introduce how to define and train a simple MLP using Sequential() and the functional API. However, Sequential() is only easy for defining models and can be challenging to use for designing complex structures. Similarly, if you plan to design complex structures using the functional API, it’s better to use the keras.Model class, and for</description>
    </item>
  </channel>
</rss>
