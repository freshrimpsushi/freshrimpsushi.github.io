<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>수리통계학 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/</link>
    <description>Recent content in 수리통계학 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 15 Oct 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Standard Definition of Standard Error</title>
      <link>https://freshrimpsushi.github.io/en/posts/2462/</link>
      <pubDate>Sun, 15 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2462/</guid>
      <description>Definition 1 For a given estimator $T$, the estimated standard deviation of $T$ is called the standard error. $$ \text{s.e.} \left( T \right) := \sqrt{ \widehat{ \text{Var} \left( T \right) } } $$ Explanation The reason why it is precisely defined as an estimator in the definition, not a statistic, is because the standard error becomes meaningless unless we are discussing whether it &amp;lsquo;matches or not&amp;rsquo; with the parameter $\theta$</description>
    </item>
    <item>
      <title>Power of a Nuisance Test and the Most Powerful Test</title>
      <link>https://freshrimpsushi.github.io/en/posts/2293/</link>
      <pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2293/</guid>
      <description>Definition 1 Hypothesis Testing: $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ A power function $\beta (\theta)$ is said to be unbiased if it satisfies the following for all $\theta_{0} \in \Theta_{0}$ and $\theta_{1} \in \Theta_{0}^{c}$: $$ \beta \left( \theta_{0} \right) \le \beta \left( \theta_{1} \right) $$ Let $\mathcal{C}$ be a set comprising such hypothesis tests. A hypothesis test $A$ that has a</description>
    </item>
    <item>
      <title>Power Function of Hypothesis Testing</title>
      <link>https://freshrimpsushi.github.io/en/posts/2291/</link>
      <pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2291/</guid>
      <description>Definition 1 Hypothesis Testing: $$ \begin{align*} H_{0} :&amp;amp; \theta \in \Theta_{0} \\ H_{1} :&amp;amp; \theta \in \Theta_{0}^{c} \end{align*} $$ Given the hypothesis testing above, let&amp;rsquo;s denote it as $\alpha \in [0,1]$. For the parameter $\theta$, the function $\beta (\theta) := P_{\theta} \left( \mathbf{X} \in \mathbb{R} \right)$ with the rejection region $R$ is called the Power Function. If $\sup_{\theta \in \Theta_{0}} \beta (\theta) = \alpha$, then the given hypothesis test is</description>
    </item>
    <item>
      <title>Mathematical Statistical Hypothesis Testing Definition</title>
      <link>https://freshrimpsushi.github.io/en/posts/2283/</link>
      <pubDate>Sat, 22 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2283/</guid>
      <description>Definition A proposition about a parameter is called a Hypothesis. The problem of accepting hypothesis $H_{0}$ as true based on a given sample, or rejecting hypothesis $H_{0}$ and adopting hypothesis $H_{1}$ is called a Hypothesis Test. In hypothesis testing, the complementary hypotheses $H_{0}$, $H_{1}$ are called the Null Hypothesis and the Alternative Hypothesis, respectively. The subset $R \subset \Omega$ of the sample space $\Omega$ that leads to the rejection of</description>
    </item>
    <item>
      <title>Neumann Factorization Theorem Proof</title>
      <link>https://freshrimpsushi.github.io/en/posts/2084/</link>
      <pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2084/</guid>
      <description>Summary Let&amp;rsquo;s say a random sample $X_{1} , \cdots , X_{n}$ has the same probability mass/density function $f \left( x ; \theta \right)$ for a parameter $\theta \in \Theta$. Statistic $Y = u_{1} \left( X_{1} , \cdots , X_{n} \right)$ is a sufficient statistic for $\theta$ if there exist two non-negative functions $k_{1} , k_{2} \ge 0$ that satisfy the following. $$ f \left( x_{1} ; \theta \right) \cdots f</description>
    </item>
    <item>
      <title>Sufficient Statistic</title>
      <link>https://freshrimpsushi.github.io/en/posts/2061/</link>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2061/</guid>
      <description>Definitions Mathematical Definition 1 Let the probability mass/density function of a random sample $X_{1} , \cdots , X_{n}$ for parameter $\theta \in \Theta$ be $f(x;\theta)$, and let the probability mass/density function for statistic $Y_{1} := u_{1} \left( X_{1} , \cdots , X_{n} \right)$ be $f_{Y_{1}} \left( y_{1}; \theta \right)$. For $H \left( x_{1} , \cdots , x_{n} \right)$, which does not depend on $\theta \in \Theta$, $$ {{ f \left(</description>
    </item>
    <item>
      <title>Maximum Likelihood Estimator</title>
      <link>https://freshrimpsushi.github.io/en/posts/2026/</link>
      <pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2026/</guid>
      <description>Buildup Consider a random variable $X$ with a probability density function (pdf) $f \left( x ; \theta \right)$ for parameter $\theta \in \Theta$. A random sample $X_{1} , \cdots , X_{n}$ drawn identically and independently (iid) from the same distribution as $X$ has the same pdf $f(x ; \theta)$ and realization $\mathbf{x} := \left( x_{1} , \cdots , x_{n} \right)$. The function $L$ defined for this is called the Likelihood</description>
    </item>
    <item>
      <title>Covariance Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/1950/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1950/</guid>
      <description>Definition1 $p$-dimensional random vector $\mathbf{X} = \left( X_{1}, \cdots , X_{p} \right)$ is defined as follows for $\text{Cov} (\mathbf{X})$, which is called a Covariance Matrix. $$ \left( \text{Cov} \left( \mathbf{X} \right) \right)_{ij} := \text{Cov} \left( X_{i} , X_{j} \right) $$ $\text{Cov}$ is covariance. Explanation To put the definition in simpler words, it is as follows. $$ \text{Cov} \left( \mathbf{X} \right) := \begin{pmatrix} \text{Var} \left( X_{1} \right) &amp;amp; \text{Cov} \left( X_{1}</description>
    </item>
    <item>
      <title>Central Limit Theorem Proof</title>
      <link>https://freshrimpsushi.github.io/en/posts/43/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/43/</guid>
      <description>Summary 1 If $\left\{ X_{k} \right\}_{k=1}^{n}$ are iid random variables following the probability distribution $\left( \mu, \sigma^2 \right) $, then when $n \to \infty$ $$ \sqrt{n} {{ \overline{X}_n - \mu } \over {\sigma}} \overset{D}{\to} N (0,1) $$ $\overset{D}{\to}$ means convergence in distribution. Explanation This theorem is widely acclaimed in statistics, along with the Law of Large Numbers. Despite being frequently discussed and applied, many encounter its proof only upon studying</description>
    </item>
    <item>
      <title>Convergence of Distributions in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1888/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1888/</guid>
      <description>Definition 1 Given a random variable $X$ and a sequence of random variables $\left\{ X_{n} \right\}$, if the following condition is satisfied when $n \to \infty$, we say that $X$ converges in distribution to $X_{n}$ and represent it as $X_{n} \overset{D}{\to} X$. $$ \lim_{n \to \infty} F_{X_{n}} (x) = F_{X} (x) \qquad, \forall x \in C_{F_{X}} $$ $F_{X}$ is the cumulative distribution function of the random variable $X$. $C_{F_{X}}$ represents</description>
    </item>
    <item>
      <title>Unbiased Estimator</title>
      <link>https://freshrimpsushi.github.io/en/posts/1745/</link>
      <pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1745/</guid>
      <description>Definition 1 If the estimator $T$ of $\theta$ satisfies the following, then $T$ is called the unbiased estimator of $\theta$. $$ E T = \theta $$ Explanation Especially, among the unbiased estimators for $\theta$, the one with the smallest variance is called the minimum variance unbiased estimator. Unbiasedness refers to the property of not having any bias. For example, when we assume $X_{i} \sim \left( \mu , \sigma^{2} \right)$, if</description>
    </item>
    <item>
      <title>Statistical Measures and Estimators in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1730/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1730/</guid>
      <description>Definition 12 A function $T$ of a sample $X_{1} , \cdots , X_{n}$ from a random variable $X$ is called a Statistic. $$ T := T \left( X_{1} , \cdots , X_{n} \right) $$ When the distribution function of $X$ is expressed as $f(x; \theta)$ or $p(x; \theta)$, if $T$ serves to capture $\theta$, then $T$ is referred to as an Estimator of $\theta$. The probability distribution of a statistic</description>
    </item>
    <item>
      <title>Random Sampling in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1715/</link>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1715/</guid>
      <description>Definitions 1 The actual outcome of a random variable $X$ is called its realization and is usually represented by the lowercase letter $x$. A set of random variables from the same probability distribution as $X$, with a sample size of $n$, is called a sample, represented as follows: $$ X_{1} , X_{2} , \cdots , X_{n} $$ If the random variable $X_{1} , \cdots , X_{n}$ is iid, then a</description>
    </item>
    <item>
      <title>Independence and iid of Random Variables</title>
      <link>https://freshrimpsushi.github.io/en/posts/1469/</link>
      <pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1469/</guid>
      <description>Definition 1 A random variable $X_{1} , \cdots , X_{n}$ is said to be pairwise independent if it satisfies the following. $$ i \ne j \implies X_{i} \perp X_{j} $$ A continuous random variable $X_{1} , \cdots , X_{n}$ whose joint probability density function $f$ satisfies the condition with respect to each of its probability density functions $f_{1} , \cdots , f_{n}$ is said to be mutually independent. $$ f(x_{1}</description>
    </item>
    <item>
      <title>Probability Variables Independence in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1461/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1461/</guid>
      <description>Definition 1 If for two random variables $X_{1}, X_{2}$, the joint probability density function $f$ or the probability mass function $p$ satisfies the following conditions for the probability density functions $f_{1}, f_{2}$ or the probability mass functions $p_{1}, p_{2}$ of $X_{1}, X_{2}$, then $X_{1}, X_{2}$ are said to be independent, and is denoted as $X_{1} \perp X_{2}$. $$ f(x_{1} , x_{2} ) \equiv f_{1}(x_{1})f_{2}(x_{2}) \\ p(x_{1} , x_{2} ) \equiv</description>
    </item>
    <item>
      <title>Probability Distributions under Conditional Probability in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1458/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1458/</guid>
      <description>Definition For a discrete random variable $X_{1}, X_{2}, \cdots , X_{n}$, the following $p_{2, \cdots , n \mid 1}$, given $X_{1} = x_{1}$, is called the joint conditional probability mass function of $ X_{2}, \cdots , X_{n}$: $$ p_{2, \cdots , n \mid 1} ( x_{2} , \cdots ,x_{n} \mid X_{1} = x_{1} ) = {{ p_{1, \cdots , n}(x_{1} , x_{2} , \cdots , x_{n}) } \over { p_{1}(</description>
    </item>
    <item>
      <title>Multivariate Probability Distributions in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1449/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1449/</guid>
      <description>Definition 1 A Random Vector is defined as $X = (X_{1} , \cdots , X_{n})$ for $n$ number of probability variables $X_{i}$ defined in sample space $\Omega$. The range $X(\Omega)$ of $X$ is also called a space. A function that satisfies the following $F_{X} : \mathbb{R}^{n} \to [0,1]$ is called the Joint Cumulative Distribution Function of $X$. $$ F_{X}\left( x_{1}, \cdots , x_{n} \right) := P \left[ X_{1} \le x_{1}</description>
    </item>
    <item>
      <title>What is the Moment Generating Function?</title>
      <link>https://freshrimpsushi.github.io/en/posts/248/</link>
      <pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/248/</guid>
      <description>Definition 1 For a random variable $X$ and some positive number $h&amp;gt;0$, if $E(e^{tX})$ exists in $-h&amp;lt; t &amp;lt; h$, then $M(t) = E( e^{tX} )$ is defined as the Moment Generating Function of $X$. Explanation The moment generating function (mgf) is a concept often encountered relatively early in mathematical statistics, yet its unfamiliar definition and seemingly contextless introduction can make it a source of dislike for the subject. The</description>
    </item>
    <item>
      <title>Pearson Correlation Coefficient</title>
      <link>https://freshrimpsushi.github.io/en/posts/57/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/57/</guid>
      <description>Definition 1 For two random variables $X, Y$, the following $\rho = \rho (X,Y)$, defined as the Pearson Correlation Coefficient, is: $$ \rho = { {\text{Cov} (X,Y)} \over {\sigma_X \sigma_Y} } $$ $\sigma_{X}$, $\sigma_{Y}$ are the standard deviations of $X$, $Y$ respectively. Explanation The Pearson Correlation Coefficient is a measure of whether two variables have a (linear) correlation. If close to $1$ or $–1$, it is</description>
    </item>
    <item>
      <title>Various Properties of Covariance</title>
      <link>https://freshrimpsushi.github.io/en/posts/425/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/425/</guid>
      <description>Definitions and Properties The covariance of probability variables $X$ and $Y$, whose means are $\mu_{X}$ and $\mu_{Y}$ respectively, is defined as $\text{Cov} (X ,Y) : = E \left[ ( X - \mu_{X} ) ( Y - \mu_{Y} ) \right]$. Covariance has the following properties: [1]: $\text{Var} (X) = \text{Cov} (X,X)$ [2]: $\text{Cov} (X,Y) = \text{Cov} (Y, X)$ [3]: $\text{Var} (X + Y) = \text{Var} (X) + \text{Var} (Y) + 2</description>
    </item>
    <item>
      <title>Expectation, Mean, Variance, and Moments in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/246/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/246/</guid>
      <description>Definition: Expectation, Mean, and Variance Let&amp;rsquo;s assume that we have a given random variable $X$. If the probability density function $f(x)$ of a continuous random variable $X$ satisfies $\displaystyle \int_{-\infty}^{\infty} |x| f(x) dx &amp;lt; \infty$, then $E(X)$, defined as follows, is called the Expectation of $X$. $$ E(X) := \int_{-\infty}^{\infty} x f(x) dx $$ If the probability mass function $p(x)$ of a discrete random variable $X$ satisfies $\displaystyle \sum_{x} |x|</description>
    </item>
    <item>
      <title>Probability Variables and Probability Distribution in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1433/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1433/</guid>
      <description>Definition 1 Let us assume that probability $P$ is defined in the sample space $\Omega$. A function $X : \Omega \to \mathbb{R}$ whose domain is the sample space is called a Random Variable. The range $X(\Omega)$ of a random variable is also called its Space. A function $F_{X} : \mathbb{R} \to [0,1]$ that satisfies the following is called the Cumulative Distribution Function (cdf) of $X$. $$ F_{X}(x) = P_{X}\left( (-\infty,x]</description>
    </item>
    <item>
      <title>Probability and the Addition Law of Probability in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1431/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1431/</guid>
      <description>Definition 1 An experiment that can be repeated under the same conditions is referred to as a Random Experiment. The set $\Omega$ of all possible outcomes that can be obtained from a random experiment is called the Sample Space. The set of outcomes in the sample space that we are interested in, i.e., $B \subset \Omega$ is called an Event, and these sets are represented as $\mathcal{B}$. A function $P</description>
    </item>
  </channel>
</rss>
