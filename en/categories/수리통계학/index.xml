<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>수리통계학 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/</link>
    <description>Recent content in 수리통계학 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 04 Aug 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%EC%88%98%EB%A6%AC%ED%86%B5%EA%B3%84%ED%95%99/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sufficient Statistic</title>
      <link>https://freshrimpsushi.github.io/en/posts/2061/</link>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2061/</guid>
      <description>Definitions Mathematical Definition 1 Let the probability mass/density function of a random sample $X_{1} , \cdots , X_{n}$ for parameter $\theta \in \Theta$ be $f(x;\theta)$, and let the probability mass/density function for statistic $Y_{1} := u_{1} \left( X_{1} , \cdots , X_{n} \right)$ be $f_{Y_{1}} \left( y_{1}; \theta \right)$. For $H \left( x_{1} , \cdots , x_{n} \right)$, which does not depend on $\theta \in \Theta$, $$ {{ f \left(</description>
    </item>
    <item>
      <title>Statistical Measures and Estimators in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1730/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1730/</guid>
      <description>Definition 12 A function $T$ of a sample $X_{1} , \cdots , X_{n}$ from a random variable $X$ is called a Statistic. $$ T := T \left( X_{1} , \cdots , X_{n} \right) $$ When the distribution function of $X$ is expressed as $f(x; \theta)$ or $p(x; \theta)$, if $T$ serves to capture $\theta$, then $T$ is referred to as an Estimator of $\theta$. The probability distribution of a statistic</description>
    </item>
    <item>
      <title>Random Sampling in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1715/</link>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1715/</guid>
      <description>Definitions 1 The actual outcome of a random variable $X$ is called its realization and is usually represented by the lowercase letter $x$. A set of random variables from the same probability distribution as $X$, with a sample size of $n$, is called a sample, represented as follows: $$ X_{1} , X_{2} , \cdots , X_{n} $$ If the random variable $X_{1} , \cdots , X_{n}$ is iid, then a</description>
    </item>
    <item>
      <title>Multivariate Probability Distributions in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1449/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1449/</guid>
      <description>Definition 1 A Random Vector is defined as $X = (X_{1} , \cdots , X_{n})$ for $n$ number of probability variables $X_{i}$ defined in sample space $\Omega$. The range $X(\Omega)$ of $X$ is also called a space. A function that satisfies the following $F_{X} : \mathbb{R}^{n} \to [0,1]$ is called the Joint Cumulative Distribution Function of $X$. $$ F_{X}\left( x_{1}, \cdots , x_{n} \right) := P \left[ X_{1} \le x_{1}</description>
    </item>
    <item>
      <title>Expectation, Mean, Variance, and Moments in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/246/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/246/</guid>
      <description>Definition: Expectation, Mean, and Variance Let&amp;rsquo;s assume that we have a given random variable $X$. If the probability density function $f(x)$ of a continuous random variable $X$ satisfies $\displaystyle \int_{-\infty}^{\infty} |x| f(x) dx &amp;lt; \infty$, then $E(X)$, defined as follows, is called the Expectation of $X$. $$ E(X) := \int_{-\infty}^{\infty} x f(x) dx $$ If the probability mass function $p(x)$ of a discrete random variable $X$ satisfies $\displaystyle \sum_{x} |x|</description>
    </item>
    <item>
      <title>Probability Variables and Probability Distribution in Mathematical Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1433/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1433/</guid>
      <description>Definition 1 Let us assume that probability $P$ is defined in the sample space $\Omega$. A function $X : \Omega \to \mathbb{R}$ whose domain is the sample space is called a Random Variable. The range $X(\Omega)$ of a random variable is also called its Space. A function $F_{X} : \mathbb{R} \to [0,1]$ that satisfies the following is called the Cumulative Distribution Function (cdf) of $X$. $$ F_{X}(x) = P_{X}\left( (-\infty,x]</description>
    </item>
  </channel>
</rss>
