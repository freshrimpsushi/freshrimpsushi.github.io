<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Calculus on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%EB%AF%B8%EB%B6%84%EC%A0%81%EB%B6%84%ED%95%99/</link><description>Recent content in Calculus on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 25 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%EB%AF%B8%EB%B6%84%EC%A0%81%EB%B6%84%ED%95%99/index.xml" rel="self" type="application/rss+xml"/><item><title>Convergence Properties of Series</title><link>https://freshrimpsushi.github.io/en/posts/1737/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1737/</guid><description>정리 두 급수 $\sum a_{n}$과 $\sum b_{n}$이 수렴하면, 급수 $\sum c a_{n}$ ($c$는 상수), $\sum (a_{n} \pm b_{n})$도 수렴하고 다음이 성립한다. $\sum\limits_{n = 1}^{\infty} c a_{n} = c</description></item><item><title>p-Series and the p-Series Test</title><link>https://freshrimpsushi.github.io/en/posts/1754/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1754/</guid><description>정의1 다음과 같은 급수를 $p$-급수라고 한다. $$ \sum\limits_{n=1}^{\infty} \dfrac{1}{n^{p}} $$ 설명 역제곱수의 무한합에 대한 일반화이다. 아래에서 소개할 판정법은 $p$-급수에 대해서만 쓸 수 있</description></item><item><title>Limit Comparison Test</title><link>https://freshrimpsushi.github.io/en/posts/1756/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1756/</guid><description>Summary1 Given two series $\sum a_{n}$ and $\sum b_{n}$, let us assume $a_{n}, b_{n} \gt 0$. If there exists a positive number $c \gt 0$ such that $$ \lim\limits_{n \to \infty} \dfrac{a_{n}}{b_{n}} = c $$ is satisfied, then either both series converge, or both diverge. Explanation This is called the limit comparison test. The comparison test is intuitive and useful, but it can only determine the convergence of a series</description></item><item><title>Comparison Test</title><link>https://freshrimpsushi.github.io/en/posts/1759/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1759/</guid><description>발췌한 문서: 정리1 두 급수 $\sum a_{n}$과 $\sum b_{n}$에 대해서 $a_{n}, b_{n} \gt 0$이라 하자. 그러면 다음이 성립한다. 만약 $\forall n \ a_{n} \le b_{n}$이고 $\sum b_</description></item><item><title>Absolute Convergence and Conditional Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1760/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1760/</guid><description>정의1 급수 $\sum\limits_{n=0}^{\infty} a_{n}$에 대해서, $\sum\limits_{n=0}^{\infty} |a_{n}|$이 수렴하면 $\sum\limits_{n=0}^{\infty} a_{n}$이 절대수렴한다고 말한다. 설명 주의할 점은 주어진 급수가 아니라 &amp;ldq</description></item><item><title>Ratio Test</title><link>https://freshrimpsushi.github.io/en/posts/1771/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1771/</guid><description>Theorem1 For the series $\sum\limits_{n=0}^{\infty} a_{n}$, let $\lim\limits_{n \to \infty} \left| \dfrac{a_{n+1}}{a_{n}} \right| = L$. (a) If $L &amp;lt; 1$, the series absolutely converges. (b) If $L &amp;gt; 1$ or $L = \infty$, the series diverges. (c) If $L = 1$, the convergence cannot be determined. Explanation If $L = 1$, the convergence cannot be determined, and another method must be used to judge whether the series converges or diverges.</description></item><item><title>Root Test</title><link>https://freshrimpsushi.github.io/en/posts/1779/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1779/</guid><description>Summary1 For the series Series $\sum\limits_{n=0}^{\infty} a_{n}$, let $\lim\limits_{n \to \infty} \sqrt[n]{\left| a_{n} \right|} = L$ be given. (a) If $L &amp;lt; 1$, the series absolutely converges. (b) If $L &amp;gt; 1$ or $L = \infty$, the series diverges. (c) If $L = 1$, it cannot be determined. Explanation If $L = 1$, it cannot be determined, so other methods must be used to judge whether the series converges or</description></item><item><title>Taylor Series and Maclaurin Series</title><link>https://freshrimpsushi.github.io/en/posts/1854/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1854/</guid><description>Build-Up1 Assume that the given function $f$ is expressed as a power series. $$ f(x) = c_{0} + c_{1}(x - a) + c_{2}(x - a)^{2} + c_{3}(x - a)^{3} + \cdots \qquad |x - a| \lt R \tag{1} $$ In this context, finding the power series representation of the function $f$ is equivalent to determining the coefficients of each term $c_{n}$. By substituting $x = a$ into both sides, we</description></item><item><title>Applications of Taylor Series</title><link>https://freshrimpsushi.github.io/en/posts/1861/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1861/</guid><description>Explanation Taylor series (Maclaurin series) is an approximation of a given function as a power series, and the Taylor series of function $f$ is as follows. $$ \sum\limits_{n=0}^{\infty} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} = f(a) + f^{\prime}(a)(x-a) + \dfrac{f^{\prime \prime}(a)}{2!} (x-a)^{2} + \cdots $$ Under good conditions, $f$ and its Taylor series are indeed the same. $$ f(x) = \sum\limits_{n=0}^{\infty} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} = f(a) + f^{\prime}(a)(x-a) + \dfrac{f^{\prime \prime}(a)}{2!} (x-a)^{2} + \cdots $$</description></item><item><title>Integral Test</title><link>https://freshrimpsushi.github.io/en/posts/1900/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1900/</guid><description>빌드업1 $$ \sum\limits_{n=1}^{\infty} \dfrac{1}{n^{2}} = 1 + \dfrac{1}{2^{2}} + \dfrac{1}{3^{2}} + \dfrac{1}{4^{2}} + \cdots $$ 위과 같은 급수가 수렴하는지 발산하는지 알고싶은 상황이라고 하자. 이를 위해 $\dfrac{1}{n^{2}} = f(n)$을 만족하는 함수를 생각하자</description></item><item><title>Alternating Series</title><link>https://freshrimpsushi.github.io/en/posts/1925/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1925/</guid><description>Definition A series in which the sign of each term alternates is called an alternating series. In other words, for $b_{n} \gt 0$, a series whose general term is expressed in the following form. $$ a_{n} = (-1)^{n-1}b_{n} \qquad \text{ or } \qquad a_{n} = (-1)^{n}b_{n} $$ Explanation One method to determine the convergence of an alternating series is the Alternating Series Test. Alternating Series Test An alternating series $\sum\limits_{n</description></item><item><title>Alternating Series Test</title><link>https://freshrimpsushi.github.io/en/posts/1927/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1927/</guid><description>Summary1 The alternating series satisfying the following conditions $\sum\limits_{n = 1}^{\infty} (-1)^{n-1}b_{n}$ $(b_{n} \gt 0)$ converges. $b_{n+1} \le b_{n} \quad \forall n$. $\lim\limits_{n \to \infty} b_{n} = 0$. Proof First, consider the partial sum up to the even terms. $$ \begin{align*} s_{2} &amp;amp;= b_{1} - b_{2} \ge 0 \\ s_{4} &amp;amp;= s_{2} + (b_{3} - b_{4}) \ge s_{2} \\ s_{6} &amp;amp;= s_{4} + (b_{5} - b_{6}) \ge s_{4} \\ &amp;amp;\quad</description></item><item><title>Alternating Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/1928/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1928/</guid><description>Definition The following series is called the alternating harmonic series. $$ \sum\limits_{n = 1}^{\infty} (-1)^{n-1}\dfrac{1}{n} = 1 - \dfrac{1}{2} + \dfrac{1}{3} - \dfrac{1}{4} + \cdots $$ Convergence The alternating harmonic series converges. $$ \sum\limits_{n = 1}^{\infty} (-1)^{n-1}\dfrac{1}{n} = \ln 2 $$ Explanation On the other hand, the harmonic series diverges. $$ \sum\limits_{n = 1}^{\infty} \dfrac{1}{n} = 1 + \dfrac{1}{2} + \dfrac{1}{3} + \dfrac{1}{4} + \cdots = \infty $$</description></item><item><title>Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/1938/</link><pubDate>Sat, 12 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1938/</guid><description>Definition The following series is called the harmonic series. $$ \sum\limits_{n = 1}^{\infty} \dfrac{1}{n} = 1 + \dfrac{1}{2} + \dfrac{1}{3} + \dfrac{1}{4} + \cdots $$ Explanation It is a representative counterexample to the divergence test. That is, the harmonic sequence converges, but the harmonic series diverges. $$ \lim\limits_{n \to \infty} \dfrac{1}{n} = 0 \quad \text{ but } \quad \sum\limits_{n = 1}^{\infty} \dfrac{1}{n} = \infty $$ On the other hand, the</description></item><item><title>Divergence Test</title><link>https://freshrimpsushi.github.io/en/posts/1940/</link><pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1940/</guid><description>Summary If the series $\sum\limits_{n = 1}^{\infty} a_{n}$ converges, then the sequence $\{a_{n}\}$ converges to $0$. $$ \sum\limits_{n = 1}^{\infty} a_{n} \text{ is convergent } \implies \lim\limits_{n \to \infty} a_{n} = 0 $$ Proof Let the sum of the series be $\sum\limits_{n = 1}^{\infty} a_{n} = s$. That is, for the partial sum $s_{n}$, it is $\lim\limits_{n \to \infty} s_{n} = s$. Then, since $a_{n} = s_{n} - s_{n-1}$, $$</description></item><item><title>Limits of Geometric Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1949/</link><pubDate>Tue, 08 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1949/</guid><description>Summary The geometric sequence $\left\{ r^{n} \right\}$ converges to $-1 \lt r \le 1$, and its value is as follows: $$ \lim\limits_{n \to \infty} r^{n} = \begin{cases} 0 &amp;amp; \text{if } -1 \lt r \lt 1 \\ 1 &amp;amp; \text{if } r = 1 \end{cases} $$ Proof $r = 1$ If $r = 1$, $$ \lim\limits_{n \to \infty} 1^{n} = \lim\limits_{n \to \infty} 1 = 1 $$ ■ $-1 \lt</description></item><item><title>Geometric Series</title><link>https://freshrimpsushi.github.io/en/posts/1951/</link><pubDate>Sun, 06 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1951/</guid><description>Definition1 The following series is called a geometric series for $a \ne 0$. $$ a + ar + ar^{2} + ar^{3} + \cdots = \sum_{n=0}^{\infty} ar^{n} $$ Explanation It is the infinite sum of a geometric sequence with the first term $a$ and the common ratio $r$. The ▷eq04 Definition1 The following series is called a geometric series for $a \ne 0$. $$ a + ar</description></item><item><title>Parametric Equation</title><link>https://freshrimpsushi.github.io/en/posts/1963/</link><pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1963/</guid><description>Build-Up Let&amp;rsquo;s consider the situation of expressing the position of a particle on a two-dimensional plane in a formula. The path of the particle&amp;rsquo;s movement is shown in the following figure. It is impossible to represent the path in the above figure as a function of $x$, i.e., in the form of $y = f(x)$. This is because there are multiple $y$ values corresponding to points like $x_{0}$. (A function</description></item><item><title>Taylor's Theorem Rest Term</title><link>https://freshrimpsushi.github.io/en/posts/3532/</link><pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3532/</guid><description>Definition1 2 For a differentiable function $f$, the $P_{k}$ defined below is called the Taylor polynomial of $f$ at point $a$. $$ P_{k} (x) := f(a) + f^{\prime}(a) (x-a) + \dfrac{f^{\prime \prime}(a)}{2!}(x-a)^{2} + \cdots + \dfrac{f^{(k)}(a)}{k!}(x-a)^{k} $$ The difference between $f$ and $P_{k}$ is called the remainder term. $$ R_{k}(x) = f(x) - P_{k}(x) $$ Explanation $$ f(x) = P_{k}(x) + R_{k}(x) = \sum \limits_{n=0}^{k} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} + R_{k}(x) $$</description></item><item><title>Line Integrals of Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/3113/</link><pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3113/</guid><description>Definition1 Let a vector field $\mathbf{F} : \mathbb{R}^{3} \to \mathbb{R}^{3}$ and a curve $C$ in 3-dimensional space be given as $\mathbf{r}(t)$. Let $\mathbf{T}$ be called the tangent field of the vector field. Then, the $\mathbf{F}$ line integral along the curve $C$ is defined as follows. $$ \int_{C} \mathbf{F} \cdot d \mathbf{r} = \int_{a}^{b} \mathbf{F}\left( \mathbf{r}(t) \right) \cdot \mathbf{r}^{\prime}(t) dt = \int_{C} \mathbf{F} \cdot \mathbf{T} ds $$ Explanation The buildup to</description></item><item><title>Scalar Field Line Integral</title><link>https://freshrimpsushi.github.io/en/posts/3112/</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3112/</guid><description>Line Integral over a Plane Curve1 Buildup Given a function as in $y = f(x)$, its definite integral is defined by the idea of adding up all the function values $f(x)$ along the $x$ axis. Thus, the integral value is obtained along a straight line on the $x$ axis. Now, consider a two-variable function $z=f(x,y)$. Unlike in the case of single-variable functions, since the variable moves over the $xy-$ plane,</description></item><item><title>Length of a Curve</title><link>https://freshrimpsushi.github.io/en/posts/3111/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3111/</guid><description>Length of a Plane Curve1 Buildup Suppose we have a smooth function $y=f(x)$ given as in figure (a) above, with $n+1$ points on it. The total length $s$ of the curve can be obtained by summing up the lengths $s_{k}$ of each arc divided by points. Moreover, the length of each arc can be approximated by the length between two points as shown in figure (b). As the number of</description></item><item><title>Smooth Functions</title><link>https://freshrimpsushi.github.io/en/posts/3110/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3110/</guid><description>Definition If a function $f$ is infinitely differentiable, then $f$ is called a smooth function. If a function $f$ is differentiable and $f^{\prime}$ is continuous, then $f$ is called a smooth function. Explanation In analysis and functional analysis, the term smooth likely refers to the first definition. The phrase &amp;lsquo;infinitely differentiable&amp;rsquo; may seem ambiguous, but it can be understood as follows: $$ \text{For any natural number $n$, the $n$th derivative</description></item><item><title>Proof of Darboux's Intermediate Value Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1554/</link><pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1554/</guid><description>정리 If a function $f : [a,b] \to \mathbb{R}$ is differentiable at $[a,b]$, there exists a $c \in (a,b)$ such that $y_{0} = f ' (c)$ is satisfied between $f ' (a)$ and $f ' (b)$ for some $y_{0}$.</description></item><item><title>Proof of the Fundamental Theorem of Calculus</title><link>https://freshrimpsushi.github.io/en/posts/213/</link><pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/213/</guid><description>Theorem1 Assume that the function $f$ is continuous on the closed interval $[a,b]$. (1) The function $\displaystyle F(x) = \int_{a}^{x} f(t) dt$ is continuous on $[a,b]$, differentiable on $(a,b)$, and satisfies $\displaystyle {{dF(x)} \over {dx}} = f(x)$. (2) For any antiderivative $F$ of $f$, $\displaystyle \int_{a}^{b} f(x) dx = F(b) - F(a)$ Explanation Of course, we use the words differentiation and integration so we can easily guess the relationship between</description></item><item><title>A Comprehensive Summary of Various Series Tests in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/186/</link><pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/186/</guid><description>Overview This post will introduce several series convergence tests without diving into their proofs. It is often more valuable to utilize these tests as facts, especially since the proofs can be quite tedious. In this post, we use the following notations: $\mathbb{N}$ is the set containing all natural numbers. $\mathbb{R}$ is the set containing all real numbers, and $\overline{\mathbb{R}}$ is the extended real number set that includes $\pm \infty$. $\left\{</description></item><item><title>Differentiation of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/168/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/168/</guid><description>Theorem1 $$ \left( \sinh x \right)^{\prime} = \cosh x $$ $$ \left( \cosh x \right)^{\prime} = \sinh x $$ $$ \left( \tanh x \right)^{\prime} = \text{sech}^{2} x $$ Explanation The differentiation of hyperbolic functions actually doesn&amp;rsquo;t require much proof or memorization. The proofs simply use definitions, and the structures are almost identical to trigonometric functions, only with a change in signs. Using the method of proving hyperbolic sine, one can</description></item><item><title>Differentiation of Inverse Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/167/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/167/</guid><description>Theorem1 $$ \begin{align*} \left( \sin^{-1}x \right)^{\prime} &amp;amp;= {{1} \over {\sqrt{1-x^2}}} \\ \left( \cos^{-1}x \right)^{\prime} &amp;amp;= -{{1} \over {\sqrt{1-x^2}}} \\ \left( \tan^{-1}x \right)^{\prime} &amp;amp;= {{1} \over {1+x^2}} \end{align*} $$ Explanation They are read as arcsine, arccosine, and arctangent, respectively. It might seem surprising that these can be differentiated, but it turns out to be quite simple. As one can see on the right side, the shapes of the derivatives are not</description></item><item><title>Proof of Fubini's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/165/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/165/</guid><description>Theorem1 Let&amp;rsquo;s define the function $f : R \to \mathbb{R}$ on the 2-dimensional domain $R : [a,b] \times [c,d]$. If $f(x,\cdot)$ is integrable over $[c,d]$, and $f(\cdot,y)$ is integrable over $[a,b]$, and $f$ is integrable over $R$, then $$ \iint _{R} f dA = \int_{a}^{b} \int_{c}^{d} f(x,y) dy dx = \int_{c}^{d} \int_{a}^{b} f(x,y) dx dy $$ Explanation The integration domain $R$ obviously comes from a Rectangle. As it is always</description></item><item><title>Proof of Green's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/166/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/166/</guid><description>Theorem1 Let the curve $\mathcal{C}$ be a simple, smooth, closed path in the plane $S = [a,b] \times [c,d]$, moving counterclockwise. If the function $P,Q : \mathbb{R}^2 \to \mathbb{R}$ is continuous on $\mathcal{C}$ and its derivative is also continuous, $$ \int_{\mathcal{C}} (Pdx + Qdy) = \iint_{S} (Q_{x} - P_{y}) dx dy $$ Explanation This can be thought of as a theorem that converts line integrals into surface integrals. It&amp;rsquo;s widely</description></item><item><title>Calculus and the Euler Formula</title><link>https://freshrimpsushi.github.io/en/posts/112/</link><pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/112/</guid><description>Theorem Euler&amp;rsquo;s Formula: $$ { e }^{ ix }= \cos x + i \sin x $$ Euler&amp;rsquo;s Identity: $$ { e }^{ i\pi }+1=0 $$ Explanation Euler&amp;rsquo;s Formula is in itself so peculiar that even Euler did not know where it might be used, but nowadays, it is utilized in so many fields that it is difficult to summarize its usefulness. It is even more astonishing when considering it was</description></item><item><title>Series Expansion of the Arctangent Function</title><link>https://freshrimpsushi.github.io/en/posts/86/</link><pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/86/</guid><description>Theorem1 $$ \tan ^{ -1 } x = \sum _{ n=0 }^{ \infty }{ \frac { (-1) ^{ n } { x } ^ { 2n+1 } } { 2n+1 } } $$ Description Whether it is written as $\arctan$ or as $\tan ^{-1}$ does not matter. Among the several inverse trigonometric functions, arctan is particularly interesting because it provides a series that converges to $\pi$. When $x=1$ is substituted,</description></item><item><title>Derivation of the Series Form of the Natural Logarithm and Proof of the Convergence of the Alternating Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/58/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/58/</guid><description>Theorem $$ \ln(1-x)=\sum _{ n=0 }^{ \infty }{ \frac { -{ x }^{ n+1 } }{ n+1 } } $$ Description The series form of $\ln(1-x)$ can be relatively easily obtained. For $\ln(1+x)$, it is enough to substitute $-x$ for $x$ as a result of the theorem. $$ -\ln(1-x)=x+\frac { { x }^{ 2 } }{ 2 }+\frac { { x }^{ 3 } }{ 3 }+\frac { { x</description></item><item><title>Exponential, Sine, and Cosine Functions' Taylor Series Expansion</title><link>https://freshrimpsushi.github.io/en/posts/59/</link><pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/59/</guid><description>Theorem1 $$ \begin{equation} { { e ^ x } }=\sum _{ n=0 }^{ \infty }{ \frac { { x } ^{ n } }{ n! } } \end{equation} $$ $$ \begin{equation} \sin x=\sum _{ n=0 }^{ \infty }{ \frac { { x } ^{ 2n+1 } }{ (2n+1)! }{ { (-1) }^{ n } } } \end{equation} $$ $$ \begin{equation} \cos x=\sum _{ n=0 }^{ \infty }{ \frac { {</description></item><item><title>If an Infinite Series Converges, Then the Infinite Sequence Converges to 0</title><link>https://freshrimpsushi.github.io/en/posts/54/</link><pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/54/</guid><description>Theorem If $\displaystyle \sum _{ n=1 }^{ \infty }{ { a }_{ n }}$ converges, then $\displaystyle \lim _{ n\to \infty }{ { a }_{ n }}=0$ Explanation This theorem might be a bit surprising and counterintuitive at first. You might wonder why the converse doesn’t hold. A classic counterexample involves considering the following sequences: $$ \begin{align*} { a }_{ n }&amp;amp;=\frac { 1</description></item><item><title>Conditions for a function and its Taylor series to be equal</title><link>https://freshrimpsushi.github.io/en/posts/42/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/42/</guid><description>Theorem1 A function $f$ that is infinitely differentiable around point $a$, a necessary and sufficient condition for $\displaystyle f(x) = \sum_{n=0}^{\infty} {{f^{(n)} (a)}\over{n!}} {(x-a)}^n$ is that for some $\xi \in \mathscr{H} \left\{ x , a \right\}$ $$ \lim_{n \to \infty} {{f^{(n)} (\xi)}\over{n!}} {(x-a)}^n = 0 $$ where $\xi \in \mathscr{H} \left\{ x , a \right\}$ means that $\xi$ is in either $(x,a)$ or $(a,x)$. Explanation The Taylor theorem often represents</description></item><item><title>Fermat's Last Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/35/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/35/</guid><description>Theorem1 If the function $f(x)$ is either a maximum or a minimum at $x=c$ and $f ' (c)$ exists, then $f ' (c) = 0$ Explanation While high school textbooks generally only introduce Rolle&amp;rsquo;s Theorem up to Rolle&amp;rsquo;s Theorem, to rigorously prove Rolle&amp;rsquo;s Theorem, one must be able to show why the derivative at a critical point is $0$, and Fermat&amp;rsquo;s Theorem guarantees that. Proof Strategy: Divide the proof into</description></item><item><title>Proof of Cauchy's Mean Value Theorem</title><link>https://freshrimpsushi.github.io/en/posts/38/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/38/</guid><description>Theorem1 Let&amp;rsquo;s say $a &amp;lt; b$. If the function $f,g : \mathbb{R} \to \mathbb{R}$ is continuous at all points of $[a,b]$, differentiable at all $x \in (a,b)$, and if $g ' (x) \ne 0$, then there exists at least one $c \in (a,b)$ that satisfies the following: $$ {{f ' (c)}\over{g ' (c)}}={{f(b)-f(a)}\over{g(b)-g(a)}} $$ Explanation If there&amp;rsquo;s any difference from the mean value theorem, it&amp;rsquo;s just that there&amp;rsquo;s one more</description></item><item><title>Proof of L'Hôpital's Rule</title><link>https://freshrimpsushi.github.io/en/posts/39/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/39/</guid><description>Theorem1 Given that $f(x)$ and $g(x)$ are differentiable near $x=a$ and that $g ' (x) \ne 0$ and $\displaystyle \lim _{x \to a} f(x) = \lim _{x \to a} g(x) = 0$, $$ \lim _{x \to a} {{f(x)} \over {g(x)}} = \lim _{x \to a} {{f ' (x)} \over {g ' (x)}} $$ Explanation Though this theorem may seem like a magic wand to many students, who have learned and</description></item><item><title>Proof of Rolle's Theorem in Calculus</title><link>https://freshrimpsushi.github.io/en/posts/36/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/36/</guid><description>Theorem1 If the function $f(x)$ is continuous at $[a,b]$ and differentiable at $(a,b)$ and if $f(a)=f(b)$, then there exists at least one $c$ in $(a,b)$ that satisfies $f ' (c)=0$. Description In high school courses, it is introduced only as an auxiliary lemma to prove the mean value theorem and is not used at all otherwise. However, beyond the high school level, it is sometimes used as an auxiliary lemma.</description></item><item><title>Proof of Taylor's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/41/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/41/</guid><description>Theorem1 If a function $f(x)$ is continuous at $[a,b]$ and differentiable up to $n$ times at $(a,b)$, then there exists $\xi \in (a,b)$ that satisfies $$ \begin{align*} f(b) =&amp;amp; \sum_{k=0}^{n-1} {{(b-a)^{k}\over{k!}}{f^{(k)}( a )}} + {(b-a)^{n}\over{n!}}{f^{(n)}(\xi)} \\ =&amp;amp; {f(a)} + {(b-a)f ' (a)} + \cdots + {(b-a)^{n-1}\over{(n-1)!}}{f^{(n-1)}(a)} + {(b-a)^{n}\over{(n)!}}{f^{(n)}(\xi)} \end{align*} $$ Explanation This theorem, which is widely used throughout mathematics, has lent its name to the Taylor series. In terms of</description></item><item><title>Proof of the Mean Value Theorem in Calculus</title><link>https://freshrimpsushi.github.io/en/posts/37/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/37/</guid><description>Theorem1 If the function $f(x)$ is continuous at $[a,b]$ and differentiable at $(a,b)$, then there exists at least one $c$ in $(a,b)$ that satisfies $\displaystyle f '(c)={{f(b)-f(a)}\over{b-a}}$. Description It&amp;rsquo;s not just commonly used; it&amp;rsquo;s so famous that it&amp;rsquo;s abbreviated as MVT. The term &amp;lsquo;mean value&amp;rsquo; comes from the idea that there is a point where the derivative equals the average rate of change over the entire interval. The concept of</description></item><item><title>Euler's Proof of the Divergence of the Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/17/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/17/</guid><description>Theorem The harmonic series diverges. $$ \sum _{ n=1 }^{ \infty }{ \frac { 1 }{ n } }=\infty $$ Description At first glance, the harmonic series appears as if it would converge since its terms continue to decrease in value. However, Oresme elegantly and simply proved that it diverges. This fact is often used as an example to explain the concept of absolute convergence, where the alternating harmonic series</description></item><item><title>The Relationship between Areas Calculated by Riemann Sums and Definite Integrals</title><link>https://freshrimpsushi.github.io/en/posts/12/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/12/</guid><description>Formulas $$ \begin{align*} &amp;amp; \lim _{ n\to \infty }{ \sum _{ k=1 }^{ n }{ f\left( a+\frac { p }{ n }k \right) \frac { p }{ n } } } \\ =&amp;amp; \int _{ a }^{ a+p }{ f(x)dx } \\ =&amp;amp; \int _{ 0 }^{ p }{ f(a+x)dx } \\ =&amp;amp; \int _{ 0 }^{ 1 }{ pf(a+px)dx } \end{align*} $$ Explanation Sometimes, you&amp;rsquo;ll encounter integration problems that</description></item><item><title>Differentiation of Constant Functions</title><link>https://freshrimpsushi.github.io/en/posts/1175/</link><pubDate>Thu, 05 Mar 2015 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1175/</guid><description>Formula The derivative of the constant function $C$ is $0$. $$ \dfrac{d C}{dx} = 0 $$ Explanation To be precise, the derivative being a function means &amp;ldquo;the derivative of the constant function is the zero function.&amp;rdquo; Since the zero function is also a constant function, the derivative of the constant function is a constant function. Derivation For all $x \in \mathbb{R}$, let it be $C(x) = c$ ($c \in \mathbb{R}$</description></item><item><title>Antiderivatives and Indefinite Integrals</title><link>https://freshrimpsushi.github.io/en/posts/1177/</link><pubDate>Tue, 03 Mar 2015 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1177/</guid><description>Definition A function $F$ is called an antiderivative of another function $f$ if it satisfies $F^{\prime} = f$. Explanation An antiderivative is translated as 원시함수 (primitive function), 역도함수 (reverse derivative), etc. The process of finding a function $F$ that satisfies $F^{\prime} = f$ for a given $f$, or the function $F$ itself, is called an indefinite integral. The indefinite integral or antiderivative of $f$</description></item></channel></rss>