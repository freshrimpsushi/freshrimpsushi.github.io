<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Functions on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%ED%95%A8%EC%88%98/</link><description>Recent content in Functions on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 10 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%95%A8%EC%88%98/index.xml" rel="self" type="application/rss+xml"/><item><title>Differentiation of Polynomial Functions</title><link>https://freshrimpsushi.github.io/en/posts/3686/</link><pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3686/</guid><description>Formula The derivative of a polynomial function is as follows. $$ \dfrac{d x^{n}}{dx} = n x^{n-1} $$ If $n \in \mathbb{N}$, it holds in $x \in \mathbb{R}$. If $n \in \mathbb{Z}$, it holds in $x \ne 0$. If $n \in \mathbb{R}$, it holds in $x \gt 0$. Proof $n \in \mathbb{N}$ By the definition of the derivative, $$ \dfrac{d x^{n}}{dx} = \lim_{h \to 0} \dfrac{(x+h)^{n} - x^{n}}{h} $$ Binomial theorem:</description></item><item><title>Inverse Function</title><link>https://freshrimpsushi.github.io/en/posts/3679/</link><pubDate>Sun, 27 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3679/</guid><description>Definition For a given surjective function $f: X \to Y$, the inverse function of $f$ is defined as follows. $$ f^{-1} : Y \to X, \quad f^{-1}(y) = x \iff f(x) = y $$ A function for which an inverse function exists is called an invertible function. Explanation By definition, $f$ is the inverse function of $f^{-1}$. $$ f = (f^{-1})^{-1} $$ $f \circ f^{-1}$ and $f^{-1} \circ f$ are</description></item><item><title>Differentiation of the Absolute Value Function</title><link>https://freshrimpsushi.github.io/en/posts/3655/</link><pubDate>Wed, 11 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3655/</guid><description>Theorem The derivative of the absolute value function is as follows. $$ \frac{ d |x| } {d x} = \dfrac{1}{|x|}x = \begin{cases} 1 &amp;amp; x &amp;gt; 0 \\ -1 &amp;amp; x &amp;lt; 0 \end{cases}, \qquad x \neq 0 $$ Explanation In fact, the absolute value function is not differentiable over the entire set of real numbers because of its sharp point at $x = 0$. However, excluding just one point</description></item><item><title>Quasiperiodic function</title><link>https://freshrimpsushi.github.io/en/posts/759/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/759/</guid><description>Definition 1 A function $h : \mathbb{R} \to \mathbb{R}^{n}$ is said to be quasiperiodic if it can be expressed in terms of a basic frequency $\omega_{1} , \cdots , \omega_{n}$ and for each $x_{1} , \cdots , x_{n}$ there exists a $2 \pi$-periodic function $H$ such that the following holds $h$. $$ h(t) = H \left( \omega_{1} t , \cdots , \omega_{n} t \right) $$ Explanation A quasiperiodic function is</description></item><item><title>Zero-function</title><link>https://freshrimpsushi.github.io/en/posts/1155/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1155/</guid><description>Definition In the Real Space $0 : \mathbb{R} \to \mathbb{R}$ defined as follows is called the zero function. $$ 0(x) = 0 \quad \text{for all } x \in \mathbb{R} $$ In the Vector Space Let the zero vector of the vector space $V$ be denoted by $\mathbf{0}_{V}$. The zero function $\mathbf{0} : V \to V$ defined on $V$ is as follows. $$ \mathbf{0}(\mathbf{v}) = \mathbf{0}_{V} \quad \text{for all } \mathbf{v}</description></item><item><title>Notation and Naming Conventions of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1825/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1825/</guid><description>Definition The hyperbolic sine function is defined as the linear combination of two exponential functions $\frac{1}{2}e^{x} - \frac{1}{2}e^{-x}$ and is denoted as follows: $$ \sinh x := \dfrac{e^{x} - e^{-x}}{2} $$ Similarly, the hyperbolic cosine function is defined as the linear combination of two exponential functions $\frac{1}{2}e^{x} + \frac{1}{2}e^{-x}$ and is denoted as follows: $$ \cosh x := \dfrac{e^{x} + e^{-x}}{2} $$ Explanation The names and notations of $\sinh$ and</description></item><item><title>Special Angles of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/1849/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1849/</guid><description>공식 몇몇의 특수한 각도에 대한 삼각함수의 함숫값은 다음과 같다. 라디안(각도) $0$ $\frac{\pi}{12} (15^{\circ})$ $\frac{\pi}{6} (30^{\circ})$ $\frac{\pi}{4} (45^{\circ})$ $\frac{\pi}{3} (60^{\circ})$ $\frac{\pi}{2} (90^{\circ})$ $\sin$ $0$ $\dfrac{\sqrt{6} - \sqrt{2}}{4}$ $\dfrac{1}{2}$ $\dfrac{\sqrt{2}}{2}$ $\dfrac{\sqrt{3}}{2}$ $1$ $\cos$ $1$ $ \dfrac{\sqrt{6} + \sqrt{2}}{4}$ $\dfrac{\sqrt{3}}{2}$ $\dfrac{\sqrt{2}}{2}$ $\dfrac{1}{2}$ $0$ $\tan$ $0$ $2 - \sqrt{3}$</description></item><item><title>Simple Poles of Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/2621/</link><pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2621/</guid><description>Theorem The domain of the Gamma function $\Gamma$ as a complex function is as follows: $$ \mathbb{C} \setminus \left( \mathbb{Z} \setminus \mathbb{N} \right) = \mathbb{C} \setminus \left\{ 0 , -1, -2, \cdots \right\} $$ Moreover, the set of singularities of $\Gamma$, $\left( \mathbb{Z} \setminus \mathbb{N} \right)$, is a set of simple poles. $\mathbb{N}$ represents the set of natural numbers, $\mathbb{Z}$ represents the set of integers, and $\mathbb{C}$ represents the set</description></item><item><title>Derivative of the Inverse Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1976/</link><pubDate>Thu, 29 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1976/</guid><description>Formulas1 The derivatives of the inverse hyperbolic functions are as follows: $$ \begin{align*} \dfrac{d}{dx} (\sinh^{-1} x) &amp;amp;= \dfrac{1}{\sqrt{x^{2} + 1}} \qquad &amp;amp; \dfrac{d}{dx} (\csch^{-1} x) &amp;amp;= - \dfrac{1}{|x|\sqrt{x^{2} + 1}} \\ \dfrac{d}{dx} (\cosh^{-1} x) &amp;amp;= \dfrac{1}{\sqrt{x^{2} - 1}} \qquad &amp;amp; \dfrac{d}{dx} (\sech^{-1} x) &amp;amp;= - \dfrac{1}{x\sqrt{1 - x^{2}}} \\ \dfrac{d}{dx} (\tanh^{-1} x) &amp;amp;= \dfrac{1}{1 - x^{2}} \qquad &amp;amp; \dfrac{d}{dx} (\coth^{-1} x) &amp;amp;= \dfrac{1}{1 - x^{2}} \end{align*} $$ Description The closed</description></item><item><title>Inverse Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1977/</link><pubDate>Tue, 27 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1977/</guid><description>Definition1 The inverse functions of hyperbolic functions are called inverse hyperbolic functions. $$ \begin{align*} y = \sinh^{-1} x &amp;amp;\iff \sinh y = x \\ y = \cosh^{-1} x &amp;amp;\iff \cosh y = x \\ y = \tanh^{-1} x &amp;amp;\iff \tanh y = x \\ \end{align*} $$ Closed Form The values of the inverse hyperbolic functions are concretely as follows. $$ \begin{align*} \sinh^{-1} x &amp;amp;= \ln \left( x + \sqrt{x^{2} +</description></item><item><title>Derivatives of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1978/</link><pubDate>Sun, 25 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1978/</guid><description>Formula1 The derivative of hyperbolic functions is as follows: $$ \begin{align*} \dfrac{d}{dx} (\sinh x) &amp;amp;= \cosh x \qquad &amp;amp; \dfrac{d}{dx} (\csch x) &amp;amp;= - \csch x \coth x \\ \dfrac{d}{dx} (\cosh x) &amp;amp;= \sinh x \qquad &amp;amp; \dfrac{d}{dx} (\sech x) &amp;amp;= - \sech x \tanh x \\ \dfrac{d}{dx} (\tanh x) &amp;amp;= \sech^{2} x \qquad &amp;amp; \dfrac{d}{dx} (\coth x) &amp;amp;= - \csch^{2} x \end{align*} $$ Proof Since hyperbolic functions are linear</description></item><item><title>Derivative of Gamma Function at 1</title><link>https://freshrimpsushi.github.io/en/posts/2617/</link><pubDate>Sat, 24 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2617/</guid><description>Theorem For the Gamma function $\Gamma$ and the Euler-Mascheroni constant $\gamma$, the following holds: $$ \Gamma ' (1) = - \gamma $$ Proof 1 The derivative of the Gamma function times its reciprocal: $$ {{ \Gamma ' (z) } \over { \Gamma (z) }} = - \gamma + \sum_{n=1}^{\infty} \left( {{ 1 } \over { n }} - {{ 1 } \over { z + n - 1 }} \right)</description></item><item><title>Trigonometric Identities</title><link>https://freshrimpsushi.github.io/en/posts/1979/</link><pubDate>Fri, 23 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1979/</guid><description>Formula The following identity holds for trigonometric functions. $$ \begin{align} \cos^{2} x + \sin^{2} x &amp;amp;= 1 \\ 1 + \tan^{2} x &amp;amp;= \sec^{2} x \\ 1 + \cot^{2} x &amp;amp;= \csc^{2} x \end{align} $$ Proof $(1)$ From the addition formula of trigonometric functions, $$ \cos (x - y) = \cos x \cos y + \sin x \sin y $$ Substituting $y= x$, $$ \cos 0 = \cos^{2} x +</description></item><item><title>Derivatives of Inverse Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/1980/</link><pubDate>Wed, 21 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1980/</guid><description>Formulas Inverse trigonometric functions&amp;rsquo; derivatives are as follows. $$ \begin{align*} \dfrac{d}{dx} \sin ^{-1} x &amp;amp;= \dfrac{1}{\sqrt{1-x^2}} \qquad &amp;amp; \dfrac{d}{dx} \csc ^{-1} x &amp;amp;= -\dfrac{1}{x\sqrt{x^2-1}} \\ \dfrac{d}{dx} \cos ^{-1} x &amp;amp;= -\dfrac{1}{\sqrt{1-x^2}} \qquad &amp;amp; \dfrac{d}{dx} \sec ^{-1} x &amp;amp;= \dfrac{1}{x\sqrt{x^2-1}} \\ \dfrac{d}{dx} \tan ^{-1} x &amp;amp;= \dfrac{1}{1+x^2} \qquad &amp;amp; \dfrac{d}{dx} \cot ^{-1} x &amp;amp;= -\dfrac{1}{1+x^2} \end{align*} $$ Proof Differentiation of trigonometric functions $$ \begin{align*} \dfrac{d}{dx} \sin x &amp;amp;= \cos x \qquad</description></item><item><title>Reciprocal times Derivative of Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/2615/</link><pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2615/</guid><description>Definition The derivative of the logarithm of the Gamma function is called the digamma function. $$ \psi_{0} (z) := \dfrac{d}{dz} \ln \Gamma (z) = \dfrac{\Gamma^{\prime}(z)}{\Gamma (z)} $$ Theorem For the Gamma function $\Gamma$ and the Euler-Mascheroni constant $\gamma$, the following holds: $$ {{ \Gamma ' (z) } \over { \Gamma (z) }} = - \gamma + \sum_{n=1}^{\infty} \left( {{ 1 } \over { n }} - {{ 1 } \over</description></item><item><title>Variables Separable Function</title><link>https://freshrimpsushi.github.io/en/posts/1985/</link><pubDate>Fri, 09 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1985/</guid><description>Definition For a multivariable function, if there exists an $g_{i}$ that satisfies the following equation $f$, it is said to be separable in variables. $$ f(x_{1}, x_{2}, \dots, x_{n}) = g_{1}(x_{1}) g_{2}(x_{2}) \cdots g_{n}(x_{n}) $$ Description In simple terms, separating variables means expressing something as a product of functions that depend only on each variable. This assumption is often made when solving differential equations.</description></item><item><title>Rodrigues' Formula for Hermite Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/1996/</link><pubDate>Sat, 20 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1996/</guid><description>Official The explicit form of the Hermite polynomial is as follows. Physicist&amp;rsquo;s Hermite Polynomial $$ H_{n} = (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2} \tag{1} $$ Probabilist&amp;rsquo;s Hermite Polynomial $$ H_{e_{n}} = (-1)^{n} e^{{x^2} \over {2}} {{d^{n}} \over {dx^{n}}} e^{- {{x^2} \over {2}}} $$ Derivation The solution to the differential equation below $$ y_{n}^{\prime \prime} - x^{2}y_{n} = -(2n+1)y_{n} \tag{2} $$ is called the Hermite function, and is as follows. $$</description></item><item><title>Laguerre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/3630/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3630/</guid><description>정의 라게르 다항식은 다음과 같은 방법들로 정의된다. 미분방정식의 해로서 아래와 같은 라게르 미분방정식의 해를 라게르 다항함수라 한다. $$ xy^{\prime \prime} + (1-x)y^{\prime} + ny = 0, \quad n=0,1,2,\cdots</description></item><item><title>Rodrigues' Formula for Multiple Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/3622/</link><pubDate>Tue, 02 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3622/</guid><description>Description Rodrigues&amp;rsquo; formula originally referred to an explicit form of the Legendre polynomials, but later became the general term for formulae representing the explicit forms of special functions expressed in polynomials. Formulas Legendre Polynomials: $$ P_{l}(x)=\dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} $$ Laguerre Polynomials: $$ L_{n}(x) = \frac{1}{n!}e^{x}\frac{ d ^{n}}{ dx^{n} }(x^{n}e^{-x}) $$ Hermite Polynomials: $$ H_{n} = (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2} $$</description></item><item><title>What is a Special Function?</title><link>https://freshrimpsushi.github.io/en/posts/3621/</link><pubDate>Sun, 30 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3621/</guid><description>Description Special functions, as called in mathematics, are usually the solutions to certain differential equations, defined by complex integrals, can&amp;rsquo;t be expressed by elementary functions, or have mathematically interesting properties. They often bear names of people, letters of the alphabet, or Greek letters, and it can be said that almost all named functions, except polynomial, trigonometric, exponential, and logarithmic functions, are referred to as special functions. As you can see</description></item><item><title>General Polyhedral Mapping, Definition of Set-Valued Mapping</title><link>https://freshrimpsushi.github.io/en/posts/2583/</link><pubDate>Thu, 13 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2583/</guid><description>Definition 1 2 3 Given two sets $X, Y$ and $Y$, the power set $\mathcal{P} (Y)$ of, a function $f : X \to \mathcal{P} (Y)$ is referred to as Multivalued Mapping or Set-valued Mapping, and is also denoted as $f : X \rightrightarrows Y$. Description In notation, $f : X \rightrightarrows Y$ literally means that $f$ maps $x \in X$ to multiple $y \in Y$. See Also Multivalued functions in</description></item><item><title>Hard Thresholding and Soft Thresholding as Functions</title><link>https://freshrimpsushi.github.io/en/posts/2569/</link><pubDate>Thu, 16 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2569/</guid><description>Definition 1 A threshold $\lambda \in \mathbb{R}$ is given. Hard Thresholding We define $\eta _{H} \left( x ; \lambda \right) : \mathbb{R} \to \mathbb{R}$ as Hard Thresholding as follows: $$ \begin{align*} \eta _{H} \left( x ; \lambda \right) =&amp;amp; x \cdot \mathbf{1}_{\left\{ \left| x \right| \ge \lambda \right\}} \\ =&amp;amp; \begin{cases} x &amp;amp; , \text{if } x \in [-\lambda, \lambda] \\ 0 &amp;amp; , \text{if } x \notin [-\lambda, \lambda]</description></item><item><title>Definition of the Arctan2 Function</title><link>https://freshrimpsushi.github.io/en/posts/2559/</link><pubDate>Fri, 26 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2559/</guid><description>Definition Arc Tangent 2 $\arctan 2 : \left( \mathbb{R}^{2} \setminus \left\{ (0,0) \right\} \right) \to \mathbb{R}$ is defined as follows. $$ \arctan 2 : \left( r \sin \theta , r \cos \theta \right) \mapsto \theta $$ $r &amp;gt; 0$ is any positive number. Description Arc Tangent 2 is used in fields such as mechanical engineering to supplement the information that is insufficient with Arc Tangent $\arctan$, considering it provides the</description></item><item><title>Polyharmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3584/</link><pubDate>Wed, 17 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3584/</guid><description>Definition Let $\Delta = \nabla^{2}$ be called a Laplacian. For a natural number $k \in \mathbb{N}$, $\Delta ^{k}$ is referred to as a polyharmonic operator or a polylaplacian. The equation below is called the polyharmonic equation. $$ \Delta^{k} f = 0 $$ The solutions to the polyharmonic equation are referred to as polyharmonic functions. Description It is a generalization of harmonic functions. See Also Harmonic functions Biharmonic functions Polyharmonic functions</description></item><item><title>Biharmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3583/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3583/</guid><description>Definition1 Let&amp;rsquo;s call $\Delta = \nabla^{2}$ the Laplacian. $\Delta ^{2}$ is called the biharmonic operator or bilaplacian. The following equation is called the biharmonic equation. $$ \Delta^{2} f = 0 $$ The solutions to the biharmonic equation are called biharmonic functions. Explanation Let&amp;rsquo;s say $\partial_{i} = \dfrac{\partial}{\partial x_{i}}$. In the Cartesian coordinate system, since $\Delta = \sum\limits_{i} \partial_{i}\partial_{i}$, $$ \Delta^{2} f = \sum\limits_{j} \sum\limits_{i} \partial_{j}\partial_{j} \partial_{i}\partial_{i} f $$ Especially in</description></item><item><title>Why Factorial 0 is Defined as 0!=1</title><link>https://freshrimpsushi.github.io/en/posts/2546/</link><pubDate>Sun, 31 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2546/</guid><description>Definition For the factorial of $0 \notin \mathbb{N}$, it is defined as follows $0!$. $$ 0! := 1 $$ Explanation Why is $0!$ not $0$ but $1$? Originally, since $0! := 1$ is a definition, there is no need for proof, and the process of understanding why such a definition is valid is close to a request to &amp;lsquo;accept why this definition was made.&amp;rsquo; Let&amp;rsquo;s understand it step by step</description></item><item><title>Differentiation of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/3569/</link><pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3569/</guid><description>Formula1 The derivatives of trigonometric functions are as follows. $$ \begin{align*} \dfrac{d}{dx} \sin x &amp;amp;= \cos x \qquad &amp;amp; \dfrac{d}{dx} \csc x &amp;amp;= -\csc x \cot x \\[1em] \dfrac{d}{dx} \cos x &amp;amp;= - \sin x \qquad &amp;amp; \dfrac{d}{dx} \sec x &amp;amp;= \sec x \tan x \\[1em] \dfrac{d}{dx} \tan x &amp;amp;= \sec^{2} x \qquad &amp;amp; \dfrac{d}{dx} \cot x &amp;amp;= -\csc^{2} x \end{align*} $$ Proof Sum formulas for trigonometric functions $$ \sin\left(</description></item><item><title>Homogeneous Function</title><link>https://freshrimpsushi.github.io/en/posts/3540/</link><pubDate>Sat, 20 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3540/</guid><description>Definition For a constant $a$ and a function $f$, if there exists a $k \in \mathbb{N}$ that satisfies the following conditions, then $f$ is called a $k$-th degree homogeneous function. $$ f(ax) = a^{k}f(x) $$ In the case of a multivariable function, $$ f(ax_{1}, ax_{2}, \dots, ax_{n}) = a^{k}f(x_{1}, x_{2}, \dots, x_{n}) $$ Explanation In the case of a univariate function, it is similar to a polynomial function that only</description></item><item><title>Origin of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/3538/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3538/</guid><description>Explanation In mathematics, there are functions called trigonometric functions. These include $\sin, \cos, \tan, \sinh, \sec, \dots$ and others. They are collectively referred to as trigonometric functions because they are fundamentally related to triangles. Each of their names also originates from geometric meanings related to triangles. $\sin$ Let&amp;rsquo;s assume a right triangle like the one shown in the figure above. If the angle between the base and the hypotenuse is</description></item><item><title>Origin of the arc notation for inverse trigonometric functions</title><link>https://freshrimpsushi.github.io/en/posts/3537/</link><pubDate>Sun, 14 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3537/</guid><description>Definition The inverse function of a trigonometric function is called the inverse trigonometric function. Explanation Inverse trigonometric functions are often denoted using standard inverse notation $\sin^{-1}$, $\cos^{-1}$, but they are also frequently denoted with the prefix arc-, as in $\arcsin$, $\arccos$. $$ \begin{align*} \arcsin x &amp;amp;= \sin^{-1} x \qquad &amp;amp; \operatorname{arccsc} x &amp;amp;= \csc^{-1} x \\ \arccos x &amp;amp;= \cos^{-1} x \qquad &amp;amp; \operatorname{arcsec} x &amp;amp;= \sec^{-1} x \\ \arctan</description></item><item><title>Bounded Function</title><link>https://freshrimpsushi.github.io/en/posts/3517/</link><pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3517/</guid><description>Definition1 For a function $f : X \to \mathbb{R}^{n}$, if there exists a constant $M \in \mathbb{R}$ that satisfies the following, $f$ is said to be bounded. $$ \left| f(x) \right| \le M \quad \text{for all $x \in X, $} $$ 설명 다시말해 $f(x)$의 치역이 유계 집합이면 $f$ is called a bounded function. Walter</description></item><item><title>Definition of a Constant Function</title><link>https://freshrimpsushi.github.io/en/posts/2465/</link><pubDate>Sat, 21 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2465/</guid><description>Definition A function $c : X \to Y$ is called a Constant Function if it satisfies the following for all $x_{1} , x_{2} \in X$. $$ c \left( x_{1} \right) = c \left( x_{2} \right) $$ Explanation Typically, the starting point where one first &amp;lsquo;recognizes&amp;rsquo; a constant function as a function is when learning about the differentiation of constant functions. $$ \lim_{h \to 0} {{ c \left( x + h</description></item><item><title>Radial Functions</title><link>https://freshrimpsushi.github.io/en/posts/3493/</link><pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3493/</guid><description>Definition1 If the function defined above by $\mathbb{R}^{n}$ satisfies the following, it is called radial. $$ f(R\mathbf{x}) = f(\mathbf{x}) \text{ for all rotations } R $$ Explanation Directly translated as a radial function, but hardly anyone calls it that. The function value depends only on the distance $\left| x \right|$ from the origin. In physics, it is often referred to as spherical symmetry. Examples include gravity, the electric field created</description></item><item><title>Definition of a Rational Function</title><link>https://freshrimpsushi.github.io/en/posts/2463/</link><pubDate>Tue, 17 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2463/</guid><description>Definition 1 For any two polynomial functions $P_{1}(z), P_{2}(z) : \mathbb{C} \to \mathbb{C}$, the following function $Q$ that maps every $z \in \mathbb{C}$ for which $P_{2} (z) \ne 0$ into $\left( P_{1} / P_{2} \right) (z)$ is called a Rational Function or an Algebraic Fraction. $$ Q (z) := {{ P_{1} (z) } \over { P_{2} (z) }} \qquad \text{where } P_{2} (z) \ne 0 $$ Osborne (1999). Complex variables</description></item><item><title>Definition of a General Convex Function</title><link>https://freshrimpsushi.github.io/en/posts/2400/</link><pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2400/</guid><description>Definitions A function $f : V \to \mathbb{R}$ defined in a vector space $V$ is called a convex function if it satisfies the following for all $\mathbf{x}, \mathbf{y} \in V$ and all $t \in [0,1]$. $$ f \left( (1-t) \mathbf{x} + t \mathbf{y} \right) \le (1-t) f \left( \mathbf{x} \right) + t f \left( \mathbf{y} \right) $$ Explanation In fact, once you study mathematics beyond the undergraduate level, you stop</description></item><item><title>Hyperbolic Functions Composite Formula</title><link>https://freshrimpsushi.github.io/en/posts/3372/</link><pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3372/</guid><description>Formulas The following holds true. $$ c_{1} \cosh x + c_{2} \sinh x = \begin{cases} A\cosh(x + y_{1}) &amp;amp; \text{if } c_{1} \gt c_{2} \\ B e^{x} &amp;amp; \text{if } c_{1} = c_{2} = B \\ C\sinh(x + y_{2}) &amp;amp; \text{if } c_{1} \lt c_{2} \end{cases} $$ $A = \sqrt{c_{2}^{2} - c_{1}^{2}}$ $C = \sqrt{c_{2}^{2} - c_{1}^{2}}$ $y_{1} = \cosh^{-1}\left( \dfrac{c_{1}}{\sqrt{c_{1}^{2} - c_{2}^{2}}} \right) = \sinh^{-1} \left( \dfrac{c_{2}}{\sqrt{c_{1}^{2} - c_{2}^{2}}}</description></item><item><title>Sum and Difference Identities for Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/3370/</link><pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3370/</guid><description>Formulas Composition into sine $$ A \cos \theta + B \sin \theta = C\sin(\theta + \phi) $$ Here, $C = \sqrt{A^{2} + B^{2}}$, $\phi = \sin^{-1} \left( \dfrac{A}{\sqrt{A^{2} + B^{2}}} \right) = \cos^{-1} \left( \dfrac{B}{\sqrt{A^{2} + B^{2}}} \right)$ are given. Composition into cosine $$ A \cos \theta + B \sin \theta = C\cos(\theta - \phi) $$ Here, $C = \sqrt{A^{2} + B^{2}}$, $\phi = \sin^{-1} \left( \dfrac{B}{\sqrt{A^{2} + B^{2}}} \right)</description></item><item><title>Various Properties of Convex Functions</title><link>https://freshrimpsushi.github.io/en/posts/3366/</link><pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3366/</guid><description>Theorem1 All convex functions are continuous. If $f$ is an increasing convex function, and $g$ is a convex function, then $f \circ g$ is also a convex function. If $f$ is convex in $(a, b)$, and if $a \lt s \lt t \lt u \lt b$, then $$ \dfrac{f(t) - f(s)}{t-s} \le \dfrac{ f(u) - f(s) }{ u - s } \le \dfrac{ f(u) - f(t) }{ u - t</description></item><item><title>Ramp Function</title><link>https://freshrimpsushi.github.io/en/posts/3360/</link><pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3360/</guid><description>Definition The following function is called a ramp function. $$ R(x) := \begin{cases} x &amp;amp; x \gt 0 \\ 0 &amp;amp; x \le 0 \end{cases} $$ Various Definitions1 It can be defined in several ways as follows. $$ \begin{align*} R(x) &amp;amp;:= \begin{cases} x &amp;amp; x \gt 0 \\ 0 &amp;amp; x \le 0 \end{cases} \\[1em] &amp;amp;= \max \left\{ 0, x \right\} \\[1em] &amp;amp;= x H(x) \\[1em] &amp;amp;= \dfrac{x + \left|</description></item><item><title>Identity Function</title><link>https://freshrimpsushi.github.io/en/posts/3167/</link><pubDate>Fri, 31 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3167/</guid><description>Definition1 Given a set $X$, the following function $I_{X} : X \to X$ is called the identity function. $$ I_{X}(x) = x,\quad \forall x \in X $$ Explanation The following notations are commonly used. $$ I,\quad \text{id},\quad \text{1} $$ Tangent vectors on a differentiable manifold are defined as follows in $\dfrac{d (f\circ \alpha)}{d t}$, where the function to be differentiated $$ f \circ \alpha = f \circ I \circ \alpha</description></item><item><title>Sign function</title><link>https://freshrimpsushi.github.io/en/posts/3147/</link><pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3147/</guid><description>Definition Sign function $\mathrm{sgn} : \mathbb{R} \to \mathbb{R}$ is defined as follows. $$ \mathrm{sgn}(x) :=\begin{cases} 1 &amp;amp; x&amp;gt;0 \\ 0 &amp;amp; x=0 \\ -1 &amp;amp; x&amp;lt;0 \end{cases} $$ Explanation It is mainly used to simplify the notation of equations or definitions. It is also written as $\mathrm{sign}$. See Also Sign of complex numbers</description></item><item><title>Alternating Function</title><link>https://freshrimpsushi.github.io/en/posts/3137/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3137/</guid><description>Definition Let set $X$ be given. A function that satisfies the following is called an alternating function. $$ \phi : \overbrace{X \times X \times \cdots \times X}^{n} \to \mathbb{R} \\ \phi (x_{1}, \dots, x_{i}, x_{i+1}, \dots, x_{n}) = - \phi (x_{1}, \dots, x_{i+1}, x_{i}, \dots, x_{n}) $$ Explanation It is a function whose sign changes when two adjacent variables are swapped. Of course, it can also be shown that this</description></item><item><title>Including Functions</title><link>https://freshrimpsushi.github.io/en/posts/3124/</link><pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3124/</guid><description>Definition Let&amp;rsquo;s denote $X \subset Y$. A function that satisfies the following is called an inclusion function. $$ i : X \to Y, \quad \text{and} \quad i(x) = x,\quad \forall x\in X $$ Explanation Simply put, it&amp;rsquo;s an identity function whose codomain could be larger than its domain. $i : X \hookrightarrow Y$1 or $i : X \subset Y$2 notation is also used. 박대희·안승호</description></item><item><title>Expansion and Contraction of a Function</title><link>https://freshrimpsushi.github.io/en/posts/3123/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3123/</guid><description>Definition1 Let&amp;rsquo;s assume that function $f : X \to Y$ is given. Let&amp;rsquo;s also assume that $U \subset X \subset V$ holds. Contraction Mapping We call $f |_{U} \to Y$ a contraction mapping of $f$ if it satisfies the following. $$ f|_{U} : U \to Y \quad \text{and} \quad f|_{U}(x) = f (x),\quad \forall x \in U $$ Extension We call $\tilde{f} \to Y$ an extension of $f$ if it</description></item><item><title>Inverse Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/2091/</link><pubDate>Sun, 03 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2091/</guid><description>Definition1 The inverse functions of trigonometric functions are called inverse trigonometric functions, and they are denoted by adding arc- to the name of the trigonometric function. $$ \begin{align*} \arcsin x &amp;amp;= \sin^{-1} x \qquad &amp;amp; \operatorname{arccsc} x &amp;amp;= \csc^{-1} x \\ \arccos x &amp;amp;= \cos^{-1} x \qquad &amp;amp; \operatorname{arcsec} x &amp;amp;= \sec^{-1} x \\ \arctan x &amp;amp;= \tan^{-1} x \qquad &amp;amp; \operatorname{arccot} x &amp;amp;= \cot^{-1} x \end{align*} $$ Description Since</description></item><item><title>Harmonic Function</title><link>https://freshrimpsushi.github.io/en/posts/2078/</link><pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2078/</guid><description>Definition 1 If a function $\phi (x,y)$ has a continuous second derivative in the region $\mathscr{R}$ and is a solution to the Laplace&amp;rsquo;s equation, it is said to be harmonic. In other words, a harmonic function satisfies the following. $$ \Delta \phi = \nabla^{2} \phi = \phi_{xx} + \phi_{yy} = 0 $$ Especially, if a function $u(x,y), v(x,y)$ is harmonic and $u,v$ satisfies the Cauchy-Riemann equations, then $v(x,y)$ is referred</description></item><item><title>Formulas Related to Factorials</title><link>https://freshrimpsushi.github.io/en/posts/3106/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3106/</guid><description>Product of Consecutive Odd Numbers For an integer $n \ge 0$, the following holds. $$ (2n-1) \cdot (2n-3) \cdots 5 \cdot 3 \cdot 1 = \dfrac{(2n)!}{2^{n} (n!)} = (2n-1)!! $$ Here, $n!!$ refers to the double factorial. Proof A detailed explanation is omitted. $$ \begin{align*} 3 \cdot 1 =&amp;amp;\ \dfrac{4 \cdot 3 \cdot 2 \cdot 1}{4 \cdot 2} = \dfrac{4!}{2^{2}(2 \cdot 1)} = \dfrac{(2 \cdot 2)!}{2^{2}(2!)} \\ 5 \cdot 3</description></item><item><title>Definition of Logarithmic Functions</title><link>https://freshrimpsushi.github.io/en/posts/2063/</link><pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2063/</guid><description>Definition The inverse function of the exponential function is defined as the logarithmic function $\log : (0,\infty) \to \mathbb{R}$. If for all $x \in (0,\infty)$, $x = e^y$ holds, then the logarithmic function is expressed as follows: $$ \log x := y $$ Explanation Despite its simple definition, logarithms hold significant meaning across mathematics. The base can be any positive number, but it is typically set as Euler&amp;rsquo;s constant $e$</description></item><item><title>Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/2060/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2060/</guid><description>Overview The Exponential Function is a generalization of exponentiation that appears universally across all branches of mathematics. Although in original exponentiations the base $a &amp;gt; 0$ does not necessarily have to be $a = e$, the existence of the base change formula means that essentially, it doesn’t matter which base is used. For convenience, when referring to an exponential function, its base is commonly</description></item><item><title>Polynomial Function</title><link>https://freshrimpsushi.github.io/en/posts/2058/</link><pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2058/</guid><description>Definition 1 A Polynomial of degree $n$ is defined for $n \in \mathbb{N}_{0}$ and $\left\{ a_{k} \right\}_{k=0}^{n} \subset \mathbb{C}$ as follows: $$ P(z) := a_{0} + a_{1} z + \cdots a_{n} z^{n} \qquad , a_{n} \ne 0 $$ Explanation A polynomial function is one of the most basic functions that can be considered in all of mathematics, and it has been proven by the Fundamental Theorem of Algebra that there</description></item><item><title>Definition of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/2056/</link><pubDate>Sun, 25 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2056/</guid><description>Overview Trigonometric functions are functions that associate the angles of a right triangle with their trigonometric ratios. Definition The trigonometric functions sine and cosine $\sin, \cos : \mathbb{R} \to \mathbb{R}$ are defined as follows. $$ \sin \theta := {{ y } \over { \sqrt{x^{2} + y^{2}} }} \\ \cos \theta := {{ x } \over { \sqrt{x^{2} + y^{2}} }} $$ Consequently, secant, cosecant, tangent, and cotangent are defined as</description></item><item><title>Survival Function</title><link>https://freshrimpsushi.github.io/en/posts/2053/</link><pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2053/</guid><description>Definition 1 If a function $S(0)=1$ is non-increasing, it is defined as a Survival Function. Description The survival function, in simple terms, is a function that maps the probability $S(t) \in [0,1]$ of being alive at time $t$. In mathematics, survival doesn&amp;rsquo;t necessarily stick to the meaning of &amp;lsquo;being alive&amp;rsquo; but is abstracted to the period until a certain event occurs, and since it&amp;rsquo;s about mapping probabilities, it naturally has</description></item><item><title>Absolute Value Function</title><link>https://freshrimpsushi.github.io/en/posts/3083/</link><pubDate>Fri, 16 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3083/</guid><description>Definition A function defined as $f$ is called the absolute value function, and its values are denoted as shown in $|x|$. $$ |x| := f(x) = \begin{cases} x &amp;amp;\text{if } x&amp;gt;0 \\ 0 &amp;amp;\text{if } x=0 \\ -x &amp;amp;\text{if } x&amp;lt;0 \end{cases},\quad x\in \mathbb{R} $$ Explanation Absolute value refers to the magnitude of a real number, and a generalization of this is the norm. The triangle inequality holds. $$ |x</description></item><item><title>Periodic Function</title><link>https://freshrimpsushi.github.io/en/posts/2050/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2050/</guid><description>Definition A function $f : \mathbb{R} \to \mathbb{R}$ is called a $T$-periodic function if it satisfies the following for some constant $T \ne 0$ and for all $t \in \mathbb{R}$: $$ f(t + T) = f(t) $$ Example Sine $\sin$ and cosine $\cos$ are typical periodic functions, and according to the above definition, they are $2\pi$-periodic functions.</description></item><item><title>Diagonal Matrix as a Function, Diagonal Elements</title><link>https://freshrimpsushi.github.io/en/posts/2048/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2048/</guid><description>Definition Diagonal Elements The $\text{diag} : \mathbb{R}^{n \times n} \to \mathbb{R}^{n}$ for a matrix is defined as the vector consisting of the diagonal elements of the matrix. $$ \text{diag} A = \begin{bmatrix} A_{11} \\ A_{22} \\ \vdots \\ A_{nn} \end{bmatrix} $$ Diagonal Matrix The $\text{diag} : \mathbb{R}^{n} \to \mathbb{R}^{n \times n}$ for a vector is defined as the matrix that has the vector as its diagonal elements. $$ \text{diag} \begin{bmatrix}</description></item><item><title>Ceiling Function and Floor Function</title><link>https://freshrimpsushi.github.io/en/posts/2039/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2039/</guid><description>Definition 1 The Ceiling function $\lceil \cdot \rceil : \mathbb{R} \to \mathbb{Z}$ and the Floor function $\lfloor \cdot \rfloor : \mathbb{R} \to \mathbb{Z}$ are defined as follows. $$ \lceil x \rceil := \min \left\{ n \in \mathbb{Z} : x \le n \right\} \\ \lfloor x \rfloor := \max \left\{ n \in \mathbb{Z} : n \le x \right\} $$ Description In domestic terms, the Floor function $\lfloor \cdot \rfloor$ is also</description></item><item><title>Limits of Exponential and Logarithmic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3053/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3053/</guid><description>Formulas Exponential functions and logarithmic functions satisfy the following equations. $$ \begin{equation} \lim \limits_{x \to 0} \dfrac{\log (x + 1) }{x} = 1 \end{equation} $$ $$ \begin{equation} \lim \limits_{x \to 0} \dfrac{ e^{x} - 1}{x} = 1 \end{equation} $$ Proof $(1)$ $$ \begin{align*} \lim \limits_{x \to 0} \dfrac{\log (x + 1) }{x} &amp;amp;= \lim \limits_{x \to 0} \dfrac{1}{x} \log ( x + 1) \\ &amp;amp;= \lim \limits_{x \to 0} \log</description></item><item><title>Differentiation of Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/3049/</link><pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3049/</guid><description>Formulas The derivative of the exponential function is as follows. $$ \begin{equation} \dfrac{d e^{x}}{dx} = e^{x} \label{fml1} \end{equation} $$ The derivative of the exponential composite function is as follows. $$ \begin{equation} \dfrac{d \left( e^{f(x)} \right)}{dx} = f^{\prime}(x)e^{f(x)} \label{fml2} \end{equation} $$ Description The exponential function is the only function that is equal to its own derivative. Derivation (1) Using the definition of the derivative, the calculation is as follows. $$ \begin{align*}</description></item><item><title>Composition of Functions</title><link>https://freshrimpsushi.github.io/en/posts/3048/</link><pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3048/</guid><description>Definitions A function $f: X \to Y$, $g: f(X) \to Z$ is defined as follows: the composition of $g$ with $f$ is called $h: X \to Z$, and it is denoted by $h=g \circ f$. $$ h(x) = (g\circ f) (x) := g\left( f(x) \right) $$</description></item><item><title>Derivatives of Logarithmic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3047/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3047/</guid><description>Formulas The derivative of a logarithmic function with base $e$ is as follows. $$ \begin{equation} \dfrac{d \log x}{dx}=\dfrac{1}{x} \end{equation} $$ The derivative of a composite logarithmic function is as follows. $$ \begin{equation} \dfrac{d \left( \log f(x) \right)}{dx} = \dfrac{f^{\prime}(x)}{f(x)} \end{equation} $$ Explanation Especially, $(2)$ is used as a useful substitution trick. Derivation $(1)$ By the definition of logarithmic functions, the following equation holds. $$ x = e^{\log x} $$ Differentiating</description></item><item><title>Analytic Proof of 1+2+3+4+5+⋯=-1/12</title><link>https://freshrimpsushi.github.io/en/posts/2005/</link><pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2005/</guid><description>Theorem $$ \begin{align*} &amp;amp; 1 + 2 + 3 + 4 + 5 + \cdots \\ =&amp;amp; \sum_{n \in \mathbb{N}} {{ 1 } \over { n^{-1} }} \\ =&amp;amp; \zeta (-1) \\ =&amp;amp; -{{ 1 } \over { 12 }} \end{align*} $$ Description If you only focus on how summing positive numbers can result in a negative, you will never understand this post. The key point is that $\sum_{n \in</description></item><item><title>Linear Function</title><link>https://freshrimpsushi.github.io/en/posts/3037/</link><pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3037/</guid><description>Definition A function $f : X \to Y$ is called linear if it satisfies the following two conditions for $x,x_{1},x_{2}\in X$ and $a \in \mathbb{R}$, $f(ax) = af(x)$ $f(x_{1} + x_{2}) = f(x_{1}) + f(x_{2})$ Explanation If it is not linear, it is called nonlinear. The two conditions are sometimes combined as follows $$ f(ax_{1} + x_{2}) = af(x_{1}) + f(x_{2}) $$ If in 2., instead of being equal, it</description></item><item><title>Analytic Proof that 1+1+1+1+1+⋯=-1/12</title><link>https://freshrimpsushi.github.io/en/posts/1944/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1944/</guid><description>Theorem $$ \begin{align*} &amp;amp; 1 + 1 + 1 + 1 + 1 + \cdots \\ =&amp;amp; \sum_{n \in \mathbb{N}} {{ 1 } \over { n^{0} }} \\ =&amp;amp; \zeta (0) \\ =&amp;amp; -{{ 1 } \over { 2 }} \end{align*} $$ Explanation If you only focus on how adding positive numbers results in a negative number, you will never understand this post. The key is that $\sum_{n \in \mathbb{N}}</description></item><item><title>Ramanujan Sum</title><link>https://freshrimpsushi.github.io/en/posts/1936/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1936/</guid><description>Definition Assigning values to diverging series is called Ramanujan summation, and it is represented through the symbol $\re$. Theorem [1] The Grandi Series 1: $$ 1-1+1-1+ \cdots = {{ 1 } \over { 2 }} \qquad ( \operatorname{Re} ) $$ [2] $$ 1-2+3-4+ \cdots = {{ 1 } \over { 4 }} \qquad ( \operatorname{Re} ) $$ [2]' $$ 1+2+3+4+ \cdots = - {{ 1 } \over { 12 }}</description></item><item><title>Derivation of the Laurent Expansion of the Riemann Zeta Function</title><link>https://freshrimpsushi.github.io/en/posts/1934/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1934/</guid><description>Theorem The Laurent expansion of the Riemann zeta function $\zeta$ is as follows: $$ \zeta (s) = {{ 1 } \over { s-1 }} + \sum_{n=0}^{\infty} \gamma_{n} {{ (1-s)^{n} } \over { n! }} \qquad , s &amp;gt; 1 $$ Here, $\gamma_{n}$ is the nth Stieltjes constant, defined as follows: $$ \gamma_{n} := \lim_{m \to \infty} \sum_{k=1}^{m} \left( {{ \left( \log k \right)^{n} } \over { k }} - {{</description></item><item><title>Riemann Hypothesis</title><link>https://freshrimpsushi.github.io/en/posts/1920/</link><pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1920/</guid><description>Conjecture All non-trivial solutions of $\zeta (s) = 0$ will satisfy $\displaystyle \operatorname{Re} (s) = {{ 1 } \over { 2 }}$. $\zeta$ is the Riemann zeta function. $\re(z)$ denotes the real part of the complex number $z \in \mathbb{C}$. Description The Riemann Hypothesis is still an unsolved Millennium problem, and for those not familiar with mathematics, its significance, let alone the hypothesis itself, could be difficult to comprehend. If</description></item><item><title>Semi-Linear (Conjugate Linear) Functions</title><link>https://freshrimpsushi.github.io/en/posts/1841/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1841/</guid><description>Definition Assuming that a function $f : X \to \mathbb{C}$ is given. If the following equation holds for $x,y\in X$, $a,b \in \mathbb{C}$, then $f$ is called antilinear or conjugate linear. $$ f(ax + by)=\overline{a}f(x)+\overline{b}f(y) $$ Explanation Unlike linear functions, where the multiplied constant is the same inside and outside the function, it refers to a function in which the constant is the conjugate complex number inside and outside the</description></item><item><title>Monotonic Functions, Increasing Functions, Decreasing Functions</title><link>https://freshrimpsushi.github.io/en/posts/848/</link><pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/848/</guid><description>Definition Let&amp;rsquo;s assume the function $f:[a,b] \rightarrow \mathbb{R}$ is given. For $x_{1}$, $x_{2}$, $\in [a,b]$ $$ x_{1} \lt x_{2} \ \implies f(x_{1}) \le f(x_{2}) $$ If it satisfies, then $f$ is said to be monotonically increasing or $f$ is called a monotone increasing function. Conversely, $$ x_{1} \lt x_{2} \ \implies f(x_{1}) \ge f(x_{2}) $$ If it satisfies, then $f$ is said to be monotonically decreasing or $f$ is called</description></item><item><title>Characteristic Function, Indicator Function</title><link>https://freshrimpsushi.github.io/en/posts/1790/</link><pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1790/</guid><description>Definition For $A \subset X$, the function defined as $\chi_{A} : X \to \mathbb{R}$ is referred to as the characteristic function or the indicator function. $$ \chi _{A}(x) := \begin{cases} 1, &amp;amp; x\in A \\ 0 ,&amp;amp; x \notin A \end{cases} $$ Explanation $\chi$ is the Greek letter chi. The reason our math teacher used to say you should not write the letter x as $\chi$ but should instead use</description></item><item><title>Riemann Hypothesis and Trivial Roots of the Riemann Zeta Function</title><link>https://freshrimpsushi.github.io/en/posts/1668/</link><pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1668/</guid><description>Formula The following is called the Riemann functional equation. $$ \zeta (s) = 2^{s} \pi^{s - 1} \sin \left( {{ \pi s } \over { 2 }} \right) \Gamma (1-s) \zeta (1-s) $$ $\Gamma$ is the Gamma function. $\zeta$ is the Riemann zeta function. Description In the Riemann functional equation, if $s \in 2 \mathbb{Z}$ then $\displaystyle \sin \left( {{ \pi s } \over { 2 }} \right) = 0$,</description></item><item><title>The History of the Delta Function and Why Dirac Used the Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/1781/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1781/</guid><description>The History of the Delta Function1 2 3 The delta function started to appear in the works of scholars such as Poisson (1815), Fourier (1822), and Cauchy (1823, 1827) who made significant contributions to mathematics and physics in the early 19th century. However, at that time, there was not a focus on rigorously defining the delta function as we do today. Later, Kirchhoff (1882, 1891) and Heaviside (1893, 1899) were</description></item><item><title>Riemann Zeta Function</title><link>https://freshrimpsushi.github.io/en/posts/1664/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1664/</guid><description>Definition The function defined as $\xi$ is called the Riemann xi Function. $$ \xi (s) := {{ 1 } \over { 2 }} s ( s-1) \pi^{-s/2} \zeta (s) \Gamma \left( {{ s } \over { 2 }} \right) $$ $\zeta$ is the Riemann Zeta Function. $\Gamma$ is the Gamma Function. Explanation The Riemann xi function was originally defined in a slightly different form, but Edmund Landau redefined it with</description></item><item><title>Jacobi Theta Function</title><link>https://freshrimpsushi.github.io/en/posts/1644/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1644/</guid><description>Definition The function defined as follows $\vartheta$ is called the Jacobi theta Function. $$ \vartheta (\tau) := \sum_{n \in \mathbb{Z}} e^{-\pi n^{2} \tau } $$ Description While Jacobi functions can originally be more generally defined, it is common to use a special form of them depending on the needs. Note that the Jacobi theta function introduced here does not cover all contexts in its exact meaning. The following property is</description></item><item><title>Double and Half Angle Formulas for Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1748/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1748/</guid><description>Formulas Double Angle Formula: $$ \begin{align} \sinh (2x) =&amp;amp;\ 2\sinh x \cosh x \label{1} \\ \cosh (2x) =&amp;amp;\ \cosh^{2} x + \sinh^{2} x = 2\cosh ^{2 } x -1 = 2\sinh ^{2} x +1 \\ \tanh (2x) =&amp;amp;\ \frac{2\tanh x}{1+\tanh^{2}x} \end{align} $$ Half Angle Formula: $$ \begin{align} \sinh^{2} \frac{x}{2} =&amp;amp;\ \frac{\cosh x -1 }{2} \\ \cosh^{2} \frac{x}{2} =&amp;amp;\ \frac{\cosh x +1 }{2} \\ \tanh ^{2} \frac{x}{2} =&amp;amp;\ \frac{\cosh x -1}{\cosh</description></item><item><title>Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1749/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1749/</guid><description>Definition Regarding $z \in \mathbb{C}$, $$ \begin{align*} \sinh z &amp;amp;:= \frac{e^{z}-e^{-z}}{2} \\ \cosh z &amp;amp;:= \frac{e^{z}+e^{-z}}{2} \\ \tanh z &amp;amp;:= \frac{\sinh z}{\cosh z} \end{align*} $$ $$ \begin{align*} \mathrm{csch}x&amp;amp;=\frac{1}{\sinh x} \\ \mathrm{sech} x&amp;amp;=\frac{1}{\cosh x} \\ \coth x &amp;amp;=\frac{1}{\tanh x} \end{align*} $$ Relationship with Trigonometric Functions $$ \begin{align*} \sinh (iz) &amp;amp;= i\sin z \\ \sin (iz) &amp;amp;= i\sinh z \\ \cosh (iz) &amp;amp;= \cos z \\ \cos (iz) &amp;amp;= \cosh z \end{align*}</description></item><item><title>Sum and Difference Formulas and Multiplication Formulas for Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1750/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1750/</guid><description>Formulas Sum and Difference Formulas: $$ \begin{align} \sinh x +\sinh y =&amp;amp;\ 2\sinh \left(\frac{x+y}{2}\right) \cosh \left(\frac{x-y}{2}\right) \\[1em] \sinh x -\sinh y =&amp;amp;\ 2\sinh \left(\frac{x-y}{2}\right) \cosh \left( \frac{x+y}{2} \right) \\[1em] \cosh x + \cosh y =&amp;amp;\ 2 \cosh \left(\frac{x+y}{2}\right) \cosh \left(\frac{x-y}{2}\right) \\[1em] \cosh x -\cosh y =&amp;amp;\ 2 \sinh \left( \frac{x+y}{2} \right) \sinh \left(\frac{x-y}{2}\right) \end{align} $$ Product Formulas: $$ \begin{align} \sinh x \sinh y =&amp;amp;\ \frac{\cosh (x+y)-\cosh (x-y)}{2} \\ \sinh x</description></item><item><title>Hyperbolic Functions' Identities</title><link>https://freshrimpsushi.github.io/en/posts/1744/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1744/</guid><description>Formulas $$ \begin{align} \sinh(-x) =&amp;amp;\ -\sinh x \\ \cosh(-x) =&amp;amp;\ \cosh x \\ \tanh(-x) =&amp;amp;\ - \tanh x \\ \cosh x + \sinh x =&amp;amp;\ e^{x} \\ \cosh x - \sinh x =&amp;amp;\ e^{-x} \\ \cosh^{2}x -\sinh^{2}x =&amp;amp;\ 1 \end{align} $$ Explanation There&amp;rsquo;s really no proof needed. This can be directly known from the definition. Proof Proof of $(1)$ $$ \begin{align*} \sinh(-x) =&amp;amp;\ \frac{e^{-x}-e^{x}}{2} \\ =&amp;amp;-\frac{e^{x}-e^{-x}}{2} \\ =&amp;amp;-\sinh x \end{align*}</description></item><item><title>Proof of the Addition Formulas for Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1743/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1743/</guid><description>Formula $$ \begin{align} \sinh (x\pm y) =&amp;amp;\ \sinh x \cosh y \pm \sinh y \cosh x \\ \cosh (x \pm y) =&amp;amp;\ \cosh x \cosh y \pm \sinh x \sinh y \\ \tanh{x \pm y}&amp;amp;=\frac{\tanh x \pm \tanh y}{1 \pm \tanh x \tanh y} \end{align} $$ Description Thinking about the relationship between hyperbolic and trigonometric functions makes it natural that their forms are similar to the addition theorem of trigonometric</description></item><item><title>Derivation of the Poisson Summation Formula</title><link>https://freshrimpsushi.github.io/en/posts/1642/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1642/</guid><description>Formula Let $f : \mathbb{R} \to \mathbb{C}$ be a Schwartz function. Then, $$ \sum_{n \in \mathbb{Z}} f(n) = \sum_{k \in \mathbb{Z}} \widehat{f}(k) $$ Schwartz functions $f \in C^{\infty}(\mathbb{R})$ are functions whose magnitude $\left| f (x) \right|$ converges rapidly to $0$ when $x \to \pm \infty$. For $f$ and $\gamma \in \mathbb{R}$, $\widehat{f}(\gamma)$ represents the following Fourier transform: $$ \widehat{f} ( \gamma ) = \int_{\mathbb{R}} f(x) e^{-2 \pi i \gamma x}</description></item><item><title>Relationship between the Gamma Function and the Riemann Zeta Function and the Dirichlet Eta Function</title><link>https://freshrimpsushi.github.io/en/posts/1641/</link><pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1641/</guid><description>정리 If $\operatorname{Re} (s) &amp;gt; 1$ then $$ \zeta (s) \Gamma (s) = \mathcal{M} \left[ {{ 1 } \over { e^{x} - 1 }} \right] (s) = \int_{0}^{\infty} {{ x^{s-1} } \over { e^{x} - 1 }} dx \\ \eta (s) \Gamma (s) = \mathcal{M} \left[ {{ 1 } \over { e^{x} + 1 }} \right] (s) = \int_{0}^{\infty} {{ x^{s-1} } \over { e^{x} + 1 }} dx</description></item><item><title>Dirichlet eta function</title><link>https://freshrimpsushi.github.io/en/posts/1635/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1635/</guid><description>Definition The function $\eta : \mathbb{C} \to \mathbb{C}$ defined as below is called the Dirichlet eta Function. $$ \eta (s) := \sum_{n \in \mathbb{N}} (-1)^{n-1} n^{-s} $$ The Dirichlet eta function is defined as the alternating Riemann zeta function. Theorems [1] Relationship with the Riemann zeta function: $$ \eta (s) = \left( 1 - 2^{1-s} \right) \zeta (s) $$ [2] Relationship with the gamma function: If $\operatorname{Re} (s) &amp;gt; 1$,</description></item><item><title>Riemann Zeta Function</title><link>https://freshrimpsushi.github.io/en/posts/1626/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1626/</guid><description>Definition The function defined as $\zeta : \mathbb{C} \setminus \left\{ 1 \right\} \to \mathbb{C}$ is called the Riemann Zeta Function. $$ \zeta (s) := \sum_{n \in \mathbb{N}} n^{-s} = \prod_{p : \text{prime}} \left( 1- {p^{-s}} \right)^{-1} $$ Related Theorems [0] Ramanujan Sum: If $\displaystyle \sum_{n \in \mathbb{N}} x^{n-1} = {{ 1 } \over { 1-x }}$ is accepted to hold even at $|x| = 1$, $$ \zeta (0) = 1</description></item><item><title>Laguerre Polynomials' Rodrigues' Formula</title><link>https://freshrimpsushi.github.io/en/posts/1658/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1658/</guid><description>Formulas The explicit formula for the Laguerre polynomials is as follows. $$ L_{n}(x) = \frac{1}{n!}e^{x}\frac{ d ^{n}}{ dx^{n} }(x^{n}e^{-x}) \tag{1} $$ Description The formula above is referred to as the Rodrigues&amp;rsquo; formula for Laguerre polynomials. Originally, the term Rodrigues&amp;rsquo; formula denoted the explicit form of the Legendre polynomials, but it later became a general term for formulas expressing the explicit form of special functions represented by polynomials. Writing down the</description></item><item><title>Hermite Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1655/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1655/</guid><description>Description Hermite Polynomials are defined in several ways as follows. As solutions to a differential equation Hermite polynomials are defined as the solutions to the following Hermite Differential Equation. $$ y^{\prime \prime} -2xy^{\prime} +2ny=0,\quad n=0,1,2,\cdots $$ Rodrigues&amp;rsquo; formula The following function $H_{n}$ is called the Hermite polynomial. $$ H_{n}(x)=(-1)^{n}e^{x^{2}}\frac{ d ^{n}}{ dx^{n} }e^{-x^{2}} $$ This is known as the Rodrigues&amp;rsquo; formula. Meanwhile, the above function is referred to as the</description></item><item><title>Hermite Polynomials' Generating Function</title><link>https://freshrimpsushi.github.io/en/posts/1654/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1654/</guid><description>Formulas The generating function of Hermite Polynomials is as follows. $$ \Phi (x,t)=\sum \limits _{n=0}^{\infty} \frac{H_{n}(x)}{n!}t^{n}= e^{2xt-t^{2}} $$ Explanation The generating function of Hermite Polynomials, simply put, is a polynomial that uses Hermite Polynomials as its coefficients. $H_{n}(x)$ is a Hermite Polynomial, and can be obtained by multiplying Hermite function $y_{n}=e^{\frac{x^{2}}{2}}\frac{ \d ^{n} }{ \d x^{n} }e^{-x^{2}}$ with $(-1)^{n}e^{\frac{x^{2}}{2}}$ or by solving the Hermite Differential Equation. $$ H_{n}(x)=(-1)^{n}e^{x^{2}}\frac{ \d ^{n}}{</description></item><item><title>Hermite Polynomials' Recursive Relations</title><link>https://freshrimpsushi.github.io/en/posts/1656/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1656/</guid><description>Theorem The Hermite polynomials satisfy the following recursive relation. $$ \begin{align} H_{n}^{\prime}(x) &amp;amp;= 2nH_{n-1}(x) \\ H_{n+1}(x) &amp;amp;= 2xH_{n}(x)-2nH_{n-1}(x) \\ &amp;amp;= 2xH_{n}(x)-H_{n}^{\prime}(x) \nonumber \end{align} $$ Proof $(1)$ Solving using the Generating Function Generating function of the Hermite polynomials $$ \Phi (x,t) = e^{2xt-t^{2}}=\sum \limits _{n=0}^{\infty} H_{n}(x)\frac{t^{n}}{n!} $$ Differentiating the generating function of the Hermite polynomials gives, $$ 2te^{2xt-t^{2}} = \sum \limits _{n=0}^{\infty}H_{n}^{\prime}(x)\frac{t^{n}}{n!} $$ Then, the left side, by the definition of</description></item><item><title>Orthogonality of Hermite Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1657/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1657/</guid><description>Theorem The Hermite polynomials $\left\{ H_{n} \right\}_{n=0}^{\infty}$ are orthogonal with respect to the weight function $w(x)=e^{-x^{2}}$ over the interval $(-\infty, \infty)$. $$ \braket{ H_{n} | H_{m} }_{e^{-x^{2}}} =\int_{-\infty}^{\infty}e^{-x^{2}}H_{n}(x)H_{m}(x)dx=\sqrt{\pi}2^{n}n!\delta_{nm} $$ Here, $\delta_{nm}$ is the Kronecker delta. Proof Case 1: $n=m$ Let&amp;rsquo;s denote the differential operator as $D = \dfrac{d}{dx}$. $$ \int_{-\infty}^{\infty} e^{-x^{2}}H_{n}(x)H_{n}(x)dx $$ Hermite polynomials $$ H_{n}(x) = (-1)^{n}e^{x^{2}}\frac{d^{n}}{dx^{n}}e^{-x^{2}} = (-1)^{n}e^{x^{2}}D^{n}e^{-x^{2}} $$ If we solve the front part $H_{n}(x)$ of the</description></item><item><title>Foehammer Symbol</title><link>https://freshrimpsushi.github.io/en/posts/1652/</link><pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1652/</guid><description>Definitions The Pochhammer symbol has two kinds of representations as follows. The following equation is defined as the falling factorial. $$ \begin{align*} x^{\underline{n}} := (x)_{n}&amp;amp;=x(x-1)(x-2)\cdots(x-n+1) \\ &amp;amp;=\frac{x!}{(x-n)!}=\frac{\Gamma (x+1) }{ \Gamma (x-n+1)} \\ &amp;amp;=\prod \limits_{k=0}^{n-1}(x-k) \end{align*} $$ The following equation is defined as the raising factorial. $$ \begin{align*} x^{\overline{n}} := x^{(n)}&amp;amp;=x(x+1)(x+2)\cdots(x+n-1) \\ &amp;amp;=\frac{(x+n-1)!}{(x-1)!}=\frac{\Gamma (x+n) }{ \Gamma (x)} \\ &amp;amp;=\prod \limits_{k=0}^{n-1}(x+k) \end{align*} $$ $x^{\overline{0}}$ and $x^{\underline{0}}$ are defined as $1$. $$ x^{\overline{0}}=x^{\underline{n}}=1</description></item><item><title>Hermite Functions</title><link>https://freshrimpsushi.github.io/en/posts/1646/</link><pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1646/</guid><description>Definitions Hermite functions are defined as follows: $$ \begin{align} y_{n} &amp;amp;= \left( D-x \right)^{n} e^{-\frac{x^{2}}{2}} \\ &amp;amp;=e^{\frac{x^{2}}{2}} D^{n} e^{-x^{2}} \end{align} $$ where $D=\frac{d}{dx}$ is the differential operator. Description Hermite functions are solutions to the differential equation $$ y_{n}^{\prime \prime}-x^{2}y_{n}=-(2n+1)y_{n},\quad n=0,1,2,\cdots $$ and represent the solution to the one-dimensional harmonic oscillator Schrödinger equation in physics, i.e., the wave function of a</description></item><item><title>Operator Solution of the Differential Equation Satisfied by Hermite Functions</title><link>https://freshrimpsushi.github.io/en/posts/1648/</link><pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1648/</guid><description>Theorem Given the differential equation $$ y_{n}^{\prime \prime}-x^{2}y_{n}=-(2n+1)y_{n},\quad n=0,1,2,\cdots \tag{1} \label{eq1} $$ The solution to $(1)$ is as follows, known as the Hermite function. $$ \begin{align*} y_{n} &amp;amp;= \left( D-x \right)^{n} e^{-\frac{x^{2}}{2}} \\ &amp;amp;= e^{\frac{x^{2}}{2}} D^{n} x^{-x^{2}} \end{align*} $$ Here, $D$ is the differential operator $D=\frac{ d }{ dx }$. Explanation The first equation of $y_{n}$ can be directly obtained by solving the differential equation. That the second equation is</description></item><item><title>The Airy Function</title><link>https://freshrimpsushi.github.io/en/posts/1629/</link><pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1629/</guid><description>Definition The function below is referred to as the Airy function. $$ \begin{align*} \operatorname{Ai}(x) &amp;amp;= \frac{1}{\pi}\sqrt{\frac{x}{3}}K_{1/3}\left( \frac{2}{3}x^{2/3} \right) \\ \operatorname{Bi}(x) &amp;amp;= \sqrt{\frac{x}{3}}\left[ I_{-1/3}\left( \frac{2}{3}x^{3/2} \right) + I_{1/3} \left( \frac{2}{3}x^{2/3} \right) \right] \end{align*} $$ Here, $I_{\nu}$ and $K_{\nu}$ are modified Bessel functions. Description The Airy function represents the solution to the Airy differential equation using Bessel functions. Integral Form The Airy function has the following integral form: $$ \begin{align*} \operatorname{Ai}(x) &amp;amp;=</description></item><item><title>Modified Bessel Equation and Modified Bessel Function</title><link>https://freshrimpsushi.github.io/en/posts/1624/</link><pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1624/</guid><description>Buildup The differential equation below is referred to as the modified Bessel equation. $$ x^2 y^{\prime \prime} + xy^{\prime}-(x^2-\nu^2)y=0 $$ It is a form of the Bessel equation where the sign of the term $y$ has been changed to $+ \rightarrow -$. The solution to this differential equation is given by the formula for differential equations that have Bessel equation solutions, as follows. $$ y=Z_{\nu}(ix)=AJ_{\nu}(ix)+BN_{\nu}(ix) $$ The two commonly used</description></item><item><title>Bessel Functions</title><link>https://freshrimpsushi.github.io/en/posts/1622/</link><pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1622/</guid><description>Definition Bessel Equation The differential equation below is called the $\nu$ order Bessel equation. $$ \begin{align*} x^2 y^{\prime \prime} +xy^{\prime} +(x^2-\nu^2)y&amp;amp;=0 \\ x(xy^{\prime})^{\prime}+(x^2- \nu ^2) y&amp;amp;=0 \\ y^{\prime \prime}+\frac{1}{x} y^{\prime} + \left( 1-\frac{\nu^{2}}{x^{2}} \right)y&amp;amp;=0 \end{align*} $$ Description Functions Related First Kind Bessel Function First Kind Bessel Function The first solution of the Bessel equation is written as $J_{\nu}(x)$ and is called the first kind Bessel function. $$ J_{\nu}(x)=\sum \limits_{n=0}^{\infty} \frac{(-1)^{n}</description></item><item><title>Hankel Functions, Bessel Functions of the Third Kind</title><link>https://freshrimpsushi.github.io/en/posts/1623/</link><pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1623/</guid><description>Definition A Hankel function, also known as a Bessel function of the third kind, is defined as the following two linear combinations of the Bessel function of the first kind $J_{\nu}$ and the Bessel function of the second kind $N_{\nu}$. $$ H_{\nu}^{(1)}(x) = J_{\nu}(x)+iN_{\nu}(x) $$ $$ H_{\nu}^{(2)}(x) = J_{\nu}(x)-iN_{\nu}(x) $$ Explanation It was introduced by the German mathematician Hermann Hankel in 1869. Specifically, $H_{\nu}^{(1)}$ is called the Hankel function of</description></item><item><title>Orthogonality of Bessel Functions</title><link>https://freshrimpsushi.github.io/en/posts/1621/</link><pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1621/</guid><description>Theorem Let&amp;rsquo;s assume that the roots of the first kind Bessel function $\alpha, \beta$ are $J_{\nu}(x)$. Then, in the interval $[0,1]$, $\sqrt{x}J_{\nu}(x)$ forms an orthogonal set. $$ \int_{0}^{1} x J_{\nu}(\alpha x) J_{\nu}(\beta x)dx = \begin{cases} 0 &amp;amp;\alpha\ne \beta \\ \frac{1}{2}J^{2}_{\nu+1}(\alpha)=\frac{1}{2}J_{\nu-1}^{2}(\alpha)=\frac{1}{2}J_{\nu^{\prime}}^{2}(\alpha) &amp;amp;\alpha=\beta \end{cases} $$ Description The above content can also be expressed as &amp;lsquo;Bessel function $J_{\nu}(x)$ is orthogonal in the interval $[0,1]$ with respect to the weight function $x$&amp;rsquo;. Proof $\alpha</description></item><item><title>Bessel Function's Recursive Relations</title><link>https://freshrimpsushi.github.io/en/posts/1619/</link><pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1619/</guid><description>Theorem $$ J_{\nu}(x)=\sum \limits_{n=0}^{\infty} \frac{(-1)^{n} }{\Gamma (n+1) \Gamma (n+\nu+1)} \left(\frac{x}{2} \right)^{2n+\nu} \tag{1} $$ The function above is called the first kind Bessel function of order $\nu$. The first kind Bessel function $J_{\nu}(x)$ satisfies the following equation. $$ \begin{align*} \frac{d}{dx}[x^{\nu} J_{\nu}(x)] &amp;amp;= x^{\nu}J_{\nu-1}(x) \tag{a} \\ \frac{d}{dx}[x^{-\nu}J_{\nu}(x)] &amp;amp;= -x^{-\nu}J_{\nu+1}(x) \tag{b} \\ J_{\nu-1}(x)+J_{\nu+1}(x) &amp;amp;= \frac{2\nu}{x}J_{\nu}(x) \tag{c} \\ J_{\nu-1}(x)-J_{\nu+1}(x) &amp;amp;= 2J^{\prime}_{\nu}(x) \tag{d} \\ J_{\nu}^{\prime}(x) = -\frac{\nu}{x}J_{\nu}(x)+J_{\nu-1}(x) &amp;amp;= \frac{\nu}{x}J_{\nu}(x)-J_{\nu+1}(x) \tag{e} \end{align*} $$ Proof $(a)$ By</description></item><item><title>The Second Series Solution of the Bessel Equation: Bessel Functions of the Second Kind, Neumann Functions, Weber Functions</title><link>https://freshrimpsushi.github.io/en/posts/1618/</link><pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1618/</guid><description>Definition[^1] A second solution of the Bessel equation is called the Neumann function, denoted by $N_{\nu}(x)$ or $Y_{\nu}(x)$. For non-integer $\nu$, $$ N_{\nu}(x)=Y_{\nu}(x)=\frac{\cos (\nu \pi)J_{\nu}(x)-J_{-\nu}(x)}{\sin (\nu\pi)} $$ For integer $\nu$, it is defined by the limit. For $n\in \mathbb{Z}$, $\nu \in \mathbb{R}\setminus\mathbb{Z}$, $$ N_{n}(x)=\lim \limits_{\nu \rightarrow n}N_{\nu}(x) $$ Here, $J_{\pm \nu}(x)$ is the first kind Bessel function. Thus, the general solution of the Bessel equation is as follows. $$ y(x)=AJ_{\nu}(x)+BN_{\nu}(x)</description></item><item><title>Associated Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1615/</link><pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1615/</guid><description>Definition Associated Legendre polynomials are defined in the following ways. As a Solution to a Differential Equation The solutions to the associated Legendre differential equation below are referred to as associated Legendre polynomials. $$ \begin{align*} &amp;amp;&amp;amp; (1-x^{2}) \frac{d^{2}y}{dx^{2}} - 2x \frac{dy}{dx} + \left[l(l+1) - \frac{m^{2}}{1-x^{2}}\right] y &amp;amp;= 0 \\ \text{or} &amp;amp;&amp;amp; \frac{d}{dx} \left[(1-x^{2})y^{\prime}\right] + \left[l(l+1) - \frac{m^{2}}{1-x^{2}}\right] y &amp;amp;= 0 \end{align*} $$ Rodrigues&amp;rsquo; Formula The polynomial function $P_{l}^{m}$ below is</description></item><item><title>Orthogonality of Associated Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1613/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1613/</guid><description>Theorem The associated Legendre polynomials over the interval $[-1,1]$ for a fixed $m$ form an orthogonal set. $$ \int_{-1}^{1} P_{l}^{m}(x)P_{k}^{m}(x)dx =\frac{ 2}{ 2l+1 }\frac{(l+m)!}{(l-m)!}\delta_{lk} $$ In the case of $x=\cos \theta$, $$ \int_{0}^{\pi} P_{l}^{m}(\cos \theta)P_ {k}^{m}(\cos\theta)\sin \theta d\theta =\frac{ 2}{ 2l+1 }\frac{(l+m)!}{(l-m)!}\delta_{lk} $$ Associated Legendre Polynomials $$ P_{l}^{m}(x) = (1-x ^{2})^{\frac{m}{2}} \dfrac{1}{2^l l!} \dfrac{d^{l+m}}{dx^{l+m}}(x^2-1)^l $$ Proof For convenience, let&amp;rsquo;s briefly denote it as $P_{lm} = P_{l}^{m}(x)$. The associated Legendre differential</description></item><item><title>Associated Legendre Polynomials for Negative Index m</title><link>https://freshrimpsushi.github.io/en/posts/1612/</link><pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1612/</guid><description>Formulas The Associated Legendre Polynomials obey the proportionality relation below, depending on the sign of $m$. $$ P_{l}^{-m}(x)=(-1)^{m}\frac{(l-m)!}{(l+m)!}P_{l}^{m}(x) $$ $$ (1-x^{2})\frac{ d^{2}y }{ dx^{2} }-2x \frac{dy}{dx}+\left( \frac{-m^{2}}{1-x^{2}}+l(l+1) \right)y=0 $$ Explanation Looking at the associated Legendre differential equation, the section regarding $m$ is shown as $m^2$, so whether $m$ is positive or negative doesn&amp;rsquo;t affect the solution. Thus, the associated Legendre polynomials can be derived as follows. $$ \begin{align*} P_{l}^{m}(x)&amp;amp;= (1-x</description></item><item><title>Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1611/</link><pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1611/</guid><description>Definition Legendre polynomials are defined in various ways. As Solutions to a Differential Equation The solutions to the following Legendre differential equation are called Legendre polynomials. $$ (1-x^{2}) \dfrac{d^{2} y}{dx^{2}} -2x\dfrac{dy}{dx} + l(l+1) y = 0 $$ Rodrigues&amp;rsquo; Formula The following function $P_{l}$ is called a Legendre polynomial. $$ P_{l}(x) = \dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} $$ This is known as Rodrigues&amp;rsquo; formula. Explanation By definition, $P_{n}$ is technically a polynomial &amp;lsquo;function&amp;rsquo;,</description></item><item><title>Generating Functions of Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1610/</link><pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1610/</guid><description>Theorem The generating function of Legendre polynomials is as follows. $$ \Phi (x,t) = \frac{1}{\sqrt{1-2xt+t^{2}}} = \sum \limits_{l=0}^{\infty}P_{l}(x)t^{l},\quad |t|&amp;lt;1 $$ Description The generating function of Legendre polynomials is, put simply, a polynomial that has the Legendre polynomial $P_{l}(x)$ as its coefficients. Lemma The function $\Phi (x,t) = \dfrac{1}{\sqrt{1-2xt+t^{2}}}$ is a solution to the differential equation below. $$ \begin{equation} (1-x^{2})\frac{ \partial ^{2} \Phi}{ \partial x^{2} }-2x\frac{ \partial \Phi}{ \partial x }+t\frac{</description></item><item><title>Recursive Relations of Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1609/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1609/</guid><description>Theorem $$ P_{l}(x)=\dfrac{1}{2^l l!} \dfrac{d^l}{dx^l}(x^2-1)^l \tag{1} $$ Such a Legendre Polynomial $P_{l}$ satisfies the following recursive relation: $$ P^{\prime}_{l+1}(x)-P^{\prime}_{l-1}(x)=(2l+1)P_{l}(x) \tag{a} $$ $$ lP_{l}(x)=(2l-1)xP_{l-1}(x)-(l-1)P_{l-2}(x) \tag{b} $$ $$ xP^{\prime}_{l}(x)-P^{\prime}_{l-1}(x)=lP_{l}(x)\tag{c} $$ Proof $(a)$ First, if we calculate the derivative of $P_{l}(x)$, $$ \begin{align*} \frac{d}{dx}P_{l}(x) &amp;amp;= \frac{1}{2^l l!}\frac{d}{dx} \dfrac{d^l}{dx^l}(x^2-1)^l \\ &amp;amp;= \frac{1}{ 2^{l}l! }\frac{ d ^{l} }{ dx^{l} }\frac{d}{dx}(x^{2}-1)^{l} \\ &amp;amp;= \frac{2l}{ 2^{l}l! }\frac{ d ^{l} }{ dx^{l} }\left[ x(x^{2}-1)^{l-1} \right] \\ &amp;amp;= \frac{1}{</description></item><item><title>Euler Integrals: Beta Function and Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/1483/</link><pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1483/</guid><description>Definition Euler Integrals The following two integrals are referred to as Euler integrals. $(a)$ Euler integral of the first kind: Beta function $$ B(p,q)=\int_{0}^1 t^{p-1}(1-t)^{q-1}dt,\quad p&amp;gt;0,\quad q&amp;gt;0 $$ $(b)$ Euler integral of the second kind: Gamma function $$ \Gamma (p) = \int_{0}^\infty t^{p-1}e^{-t}dt,\quad p&amp;gt;0 $$ Explanation Euler Integral of the First Kind 1-1. Beta Function: If the gamma function is considered a generalization of the factorial, then the beta function</description></item><item><title>Representation of the Beta Function in the Form of an Improper Integral</title><link>https://freshrimpsushi.github.io/en/posts/1482/</link><pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1482/</guid><description>Theorem Beta Function: $$ B(p,q)=\int_{0}^{1}t^{p-1}(1-t)^{q-1}dt\quad \cdots (1) $$ The beta function can be expressed as an improper integral as follows: $$ B(p,q)=\int_{0}^{\infty}\frac{ t^{p-1} }{ (1+t)^{p+q}}dt\quad \cdots (2) $$ Explanation Using the above formula makes it easier to obtain difficult integral values. The proof is not difficult. Proof Let&amp;rsquo;s substitute $(1)$ with $t=\frac{x}{1+x}$. Then, $1-t=\frac{1}{1+x}$, and the range of integration changes to $\int_{0}^{1}\rightarrow \int_{0}^{\infty}$. Also, since $ \displaystyle \frac{ d t</description></item><item><title>Relationship between Beta Function and Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/1481/</link><pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1481/</guid><description>Theorem $$ B(p,q) = {{\Gamma (p) \Gamma (q)} \over {\Gamma (p+q) }} $$ Explanation The Beta function is defined as $\displaystyle B(p,q) := \int_{0}^{1} t^{p-1} (1-t)^{q-1} dt $, and, like the Gamma function, it is an important function applied in many fields. Since the Gamma function can be easily calculated using the recursive relationship, the Beta function can also be calculated easily using the above relation. Intuitively, it can be</description></item><item><title>Various Important Formulas Involving the Gamma Function and Factorials</title><link>https://freshrimpsushi.github.io/en/posts/1478/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1478/</guid><description>Formulas $$ \Gamma (\frac{1}{2})=\sqrt{\pi} \tag{a} $$ - Euler&amp;rsquo;s Reflection Formula: $$ \Gamma (p)\Gamma (1-p)=\dfrac{\pi}{\sin(\pi p)} \tag{b} $$ $$ \Gamma (n+\frac{1}{2})=\frac{1\cdot 3\cdot \cdot5 \cdots (2n-1)}{2^{n}}\sqrt{\pi}=\frac{(2n-1)!!}{2^n}\sqrt{\pi}=\frac{(2n)!}{4^{n}n!}\sqrt{\pi},\quad n\in \mathbb{N} \tag{c} $$ $!!$ is the Double Factorial. - Binomial Coefficient: $$ \begin{pmatrix} n \\ k \end{pmatrix}=\frac{\Gamma (n+1)}{k! \Gamma (n-k+1)} \tag{d} $$ - Euler-Mascheroni Constant: $$ \gamma=-\Gamma^{\prime} (1) \tag{e} $$ - Beta Function: $$ B(p,q)=\frac{\Gamma (p) \Gamma (q)}{\Gamma (p+q)} \tag{f} $$ Proofs Gamma Function: $$</description></item><item><title>Factorial, Double Factorial, and Multifactorial</title><link>https://freshrimpsushi.github.io/en/posts/1477/</link><pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1477/</guid><description>Factorial For a natural number $n$, $n!$ is read as $n$factorial and is defined as follows. $$ n!=n\cdot(n-1)\cdot(n-2)\cdots 2\cdot 1 =\prod\limits_{k=1}^n k $$ Description It is used in many places to neatly express equations. The factorial of $0$ is defined as $0!:=1$. By generalizing the domain of definition of factorial, one can also define something called a Gamma function. Double Factorial For a natural number $n$, $n!!$ is read as</description></item><item><title>Derivation of the Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/1476/</link><pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1476/</guid><description>Non-negative Integers and the Gamma Function For $\alpha &amp;gt;0$, $$ \int_{0}^{\infty} e^{-\alpha x} dx=\left[-\frac{1}{\alpha}e^{-\alpha x}\right]_{0}^{\infty}=\frac{1}{\alpha} $$ Differentiating both sides with respect to $\alpha$, according to the Leibniz integral rule, allows the differentiation to move under the integration sign, thus giving $$ \begin{align*} &amp;amp;&amp;amp;\int_{0}^\infty -xe^{-\alpha x}dx&amp;amp;=-\frac{1}{\alpha^2} \\ \implies &amp;amp;&amp;amp; \int_{0}^\infty xe^{-\alpha x}dx &amp;amp;= \frac{1}{\alpha ^2} \end{align*} $$ Continuing to differentiate gives $$ \begin{align*} \int_{0}^\infty x^2e^{-\alpha x}dx&amp;amp;=\frac{2}{\alpha^3} \\ \int_{0}^\infty x^3e^{-\alpha x}dx&amp;amp;=\frac{3\cdot 2}{\alpha^4}</description></item><item><title>Matrix Functions, Definition of Matrix Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/1342/</link><pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1342/</guid><description>Definitions1 $$ \mathbf{x}(t) = \begin{pmatrix} x_{1}(t) \\ \vdots \\ x_{n}(t) \end{pmatrix},\quad \mathbf{A}(t) = \begin{pmatrix} a_{11}(t) &amp;amp; \cdots &amp;amp; a_{1m}(t) \\ \vdots &amp;amp; &amp;amp; \vdots \\ a_{n1}(t) &amp;amp; \cdots &amp;amp; a_{nm}(t) \end{pmatrix} $$ If each element of a matrix is a function of variable $t$, it is called a matrix function. All elements of $\mathbf{A}(t)$, i.e., all $a_{ij}$ being continuous at a given point (or interval) means $\mathbf{A}(t)$ is continuous. If</description></item><item><title>Semi-linear Function</title><link>https://freshrimpsushi.github.io/en/posts/1333/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1333/</guid><description>Definition1 If a function $f : X \to Y$ satisfies the following two conditions, it is called sublinear. For $x,x_{1},x_{2}\in X$ and $a \in \mathbb{R}$, $f(ax) = af(x)$ $f(x_{1} + x_{2}) \le f(x_{1}) + f(x_{2})$ Explanation If the second condition holds as an equality, it is linear, and if it holds as an inequality, it is sublinear. Robert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003),</description></item><item><title>Properties of Continuous Functions with Additivity</title><link>https://freshrimpsushi.github.io/en/posts/1102/</link><pubDate>Sat, 08 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1102/</guid><description>Theorem [1] If a continuous function $f : \mathbb{R} \to \mathbb{R}$ satisfies $f(x + y) = f(x) + f(y)$ for all $x, y \in \mathbb{R}$ $$ f(x) = f(1) x $$ [2] If a continuous function $g : \mathbb{R} \to ( 0 , \infty )$ satisfies $g(x + y) = g(x) g(y)$ for all $x, y \in \mathbb{R}$ $$ g(x) = \left( g(1) \right)^x $$ Explanation The property where addition</description></item><item><title>Additive and Multiplicative Functions</title><link>https://freshrimpsushi.github.io/en/posts/1096/</link><pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1096/</guid><description>Given a function $f : X \to Y$, let $a, b \in X$, $a_{i} \in X\ (i=1,\cdots)$. Subadditive Function A function $f$ is called a subadditive function when it satisfies the following equation: $$ f(a+b) \le f(a)+f(b) $$ The absolute value is an example. $$ |3+(-4)| \le |3|+|-4| $$ Another example, if we have $f(x)=2x+3$ then $$ 13=f(2+3) \le f(2)+f(3)=7+9=16 $$ Additive Function A function $f$ is called an additive</description></item><item><title>Half-Wave Symmetric Function</title><link>https://freshrimpsushi.github.io/en/posts/1003/</link><pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1003/</guid><description>Definition A periodic function $f$ with a period of $2L$ is said to have half-wave symmetry when it satisfies the following equation for all $t$. $$ f(t)=-f(t+L) $$ Description Expanding on the definition above, it means &amp;lsquo;when a wave is on the $xy$ plane, the pattern of the wave alternates symmetrically around the $y$ axis based on the midpoint of the period.&amp;rsquo; Example As the name implies, it means being</description></item><item><title>Any Function Can Always Be Expressed as the Sum of Odd and Even Functions</title><link>https://freshrimpsushi.github.io/en/posts/1002/</link><pubDate>Fri, 29 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1002/</guid><description>Theorem The arbitrary function $f$ defined in $\mathbb{R}$ can always be expressed as a sum of an even function and an odd function. Proof Let $f_{e}(t)$ and $f_o(t)$ be as follows. $$ f_{e}(t)=\dfrac{ f(t)+f(-t)}{2},\ \ \ f_o(t)=\dfrac{ f(t)-f(-t)}{2} $$ Then, $f_{e}(t)$ is an even function, and $f_o(t)$ is an odd function, and the following equation holds. $$ f_{e}(x)+f_o(x)=f(x) $$ ■</description></item><item><title>Derivation of the Base Change Formula for Logarithms</title><link>https://freshrimpsushi.github.io/en/posts/944/</link><pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/944/</guid><description>Formula For any positive number $c&amp;gt;0$, $$ \log_{a} b = {{ \log_{c} b } \over { \log_{c} a }} $$ Explanation Nowadays, the formula itself has become less meaningful, but it still holds significant value for entrance exams. It is recommended to solve many practice problems suitable for the title of &amp;lsquo;formula&amp;rsquo;, rather than underestimating it simply because it appears to be a simple property. Derivation If we say $x</description></item><item><title>The Integral of a Periodic Function Over One Period is Constant Regardless of the Integration Interval</title><link>https://freshrimpsushi.github.io/en/posts/982/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/982/</guid><description>Theorem Let&amp;rsquo;s call $f$ a periodic function with $2L$. Then, the value below remains constant regardless of the value of $a$. $$ \int_{a}^{a+2L}f(t)dt $$ Explanation By definition of periodic functions, this is obvious. From this fact, when integrating periodic functions, techniques such as changing the interval of integration can be applied. Furthermore, if you consider it in conjunction with the average value of a function, it means that the average</description></item><item><title>Proof of the Second Cosine Law Using the Definition of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/935/</link><pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/935/</guid><description>Formula For the triangle given above, the following equations hold true, and they are collectively known as the law of cosines. $$ \begin{cases} a^{2} =b^{2}+c^{2}-2bc\cos\alpha \\ b^{2}=a^{2}+c^{2}-2ac\cos\beta \\ c^{2}=a^{2}+b^{2}-2ab\cos\gamma \end{cases} $$ Proof From the triangle in the upper left corner of the diagram, we obtain the following equation. $$ \begin{align} a &amp;amp;= \overline{BH_{a}}+\overline{H_{a}C} \nonumber \\ &amp;amp;= c\cos\beta + b\cos\gamma \label{eq1} \end{align} $$ Multiplying both sides by $a$ yields: $$ a^{2}=ac\cos\beta</description></item><item><title>Legendre Polynomials are orthogonal to any lower degree polynomial</title><link>https://freshrimpsushi.github.io/en/posts/933/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/933/</guid><description>Theorem When $P_{l}(x)$ is a Legendre Polynomial and $f(x)$ is any polynomial of lower degree than $l$, then $P_{l}(x)$ and $f(x)$ are orthogonal to each other. $$ \int_{-1}^{1}P_{l}(x)f(x)dx = 0 $$ Explanation The following lemma is essentially equivalent to the proof of the theorem. Lemma Let $f(x)$ be any polynomial of degree $n$. $f(x)$ can be expressed as a linear combination of Legendre polynomials up to degree $l \le n$.</description></item><item><title>Orthogonality of Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/931/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/931/</guid><description>Theorem In the interval $[-1,\ 1]$, Legendre polynomials form an orthogonal set. $$ \int_{-1}^{1} P_{l}(x)P_{m}(x) dx =\frac{2}{2l+1}\delta_{lm} \quad (l, m = 0, 1, 2, \dots) $$ Proof Case 1: $l \ne m$ Legendre Differential Equation The following differential equation is called the Legendre differential equation. $$ \dfrac{d}{d x}\left[ (1-x)^{2} \dfrac{d y}{d x} \right] +l(l+1)y = 0 $$ Since Legendre polynomials are solutions to the Legendre differential equation, they satisfy the</description></item><item><title>Rodrigues Formula for Legendre Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/895/</link><pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/895/</guid><description>Formula The explicit formula for the Legendre polynomials is as follows. $$ P_{l}(x)=\dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \tag{1} $$ Description This formula is used to obtain the $l$th Legendre polynomial, known as the Rodrigues&amp;rsquo; formula. Originally, it referred to the explicit form of the Legendre polynomials, but later it became a universal name for formulas representing the explicit form of special functions expressed as polynomials. Derivation The Legendre polynomial $P_{l}$ refers to</description></item><item><title>Staircase Function</title><link>https://freshrimpsushi.github.io/en/posts/757/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/757/</guid><description>Definition A function that is a piecewise constant function is called a step function. Description As shown in the figure above, it looks like a staircase, hence the name step function. It is also known as the Heaviside function, named after Heaviside, who is known to be the first to propose it. Heaviside was the person who created a method for solving differential equations in electrical circuits, which is the</description></item><item><title>Rigorous Proof of Stirling's Approximation Formula</title><link>https://freshrimpsushi.github.io/en/posts/616/</link><pubDate>Wed, 18 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/616/</guid><description>Theorem $$ \lim_{n \to \infty} {{n!} \over {e^{n \ln n - n} \sqrt{ 2 \pi n} }} = 1 $$ Description The Stirling approximation, also known as the Stirling&amp;rsquo;s formula, is widely used in various fields such as statistics and physics. It can also be expressed using the gamma function as follows: $$ \Gamma ( n ) \approx {e^{n \ln n - n} \sqrt{ 2 \pi n}} $$ This proof</description></item><item><title>Wallis Product</title><link>https://freshrimpsushi.github.io/en/posts/601/</link><pubDate>Wed, 04 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/601/</guid><description>Theorem $$ \prod_{n=1}^{\infty} {{4n^2} \over {4n^2 - 1}} = \lim_{n \to \infty} {{2 \cdot 2 } \over { 1 \cdot 3 } } \cdot {{4 \cdot 4 } \over { 3 \cdot 5 } } \cdot \cdots \cdot {{2n \cdot 2n } \over { (2n-1) \cdot (2n+1) } } = {{ \pi } \over {2}} $$ Explanation It is undeniably intriguing and useful to know that not only through series</description></item><item><title>Sum and Difference Formulas and Product-to-Sum Formulas of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/539/</link><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/539/</guid><description>The sum-to-product and product-to-sum formulas aren&amp;rsquo;t used as often as the double angle/half angle formulas, so they&amp;rsquo;re not considered as important. However, this doesn&amp;rsquo;t mean they&amp;rsquo;re entirely unnecessary. Since the derivation process is very simple, it&amp;rsquo;s good to be familiar with it and be able to derive it quickly whenever needed. They are derived using only the addition formulas. Addition Formulas $$ \begin{align*} \sin ( \theta_{1} \pm \theta_{2}) =&amp;amp;\ \sin</description></item><item><title>Why is "Implicit Function" a Misleading Translation?</title><link>https://freshrimpsushi.github.io/en/posts/522/</link><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/522/</guid><description>Definition The distinction between a positive and a negative function merely depends on how each is represented. Although it&amp;rsquo;s a somewhat unfamiliar expression in mathematics, the differentiation depends on how &amp;lsquo;independent variables&amp;rsquo; and &amp;lsquo;dependent variables&amp;rsquo; are represented. Simply put, it involves setting the independent variable as $x$ and the dependent variable as the changing $y$ and observing their forms. Examples For example, in the case of $y = x^2 +</description></item><item><title>Derivation of the Legendre Duplication Formula</title><link>https://freshrimpsushi.github.io/en/posts/499/</link><pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/499/</guid><description>Formula $$ \Gamma (2r) = {{2^{ 2r - 1} } \over { \sqrt{ \pi } } } \Gamma \left( r \right) \Gamma \left( {{1} \over {2}} + r \right) $$ Description While the splitting shape may not be pretty, the fact that factors can be divided into smaller ones is certainly useful. The derivation itself is not very difficult if one uses an auxiliary lemma derived from the beta function.</description></item><item><title>Double Angle and Half Angle Formulas of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/481/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/481/</guid><description>Overview Back in the day, when the owners of sushi restaurants were high school students, there used to be formulas like angle addition, double angle, and sum-difference formulas in the curriculum, but nowadays, it&amp;rsquo;s understood they are not. All the following formulas can be derived from the sum formulas, so it&amp;rsquo;s better to learn the derivation process and derive them as needed rather than memorizing them all. Addition Theorem $$</description></item><item><title>Proof that Sine Squared Plus Cosine Squared Equals 1</title><link>https://freshrimpsushi.github.io/en/posts/482/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/482/</guid><description>Formulas $$ \sin^2\theta+\cos^2\theta=1 $$ Proof 1-Addition Formula for Cosine Using the Addition Theorem for Cosine, we can understand it very easily. $$ \cos(\theta_{1}-\theta_2)=\cos\theta_{1}\cos\theta_2 + \sin\theta_{1}\sin\theta_2 $$ Here, if we substitute $\theta$ instead of $\theta_{1}$, $\theta_2$ $$\cos(\theta-\theta)=\cos^2\theta + \sin^2\theta$$ $$\implies \cos(\theta-\theta)=\cos 0=1$$ $$\implies \sin^2\theta+\cos^2\theta=1$$ ■ 2-Pythagorean Theorem There is a unit circle with a radius of 1. Let&amp;rsquo;s look at the triangle formed by the unit circle&amp;rsquo;s radius, the perpendicular dropped</description></item><item><title>Trigonometric Representation of the Beta Function</title><link>https://freshrimpsushi.github.io/en/posts/458/</link><pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/458/</guid><description>Theorem $$ B(p,q) = 2 \int_{0}^{{\pi} \over {2}} \left( \sin \theta \right) ^{2p-1} \left( \cos \theta \right) ^{2q-1} d \theta $$ Description No matter what kind of mathematics it is, being able to express a function in a different way is a good thing. Proof If we substitute from $\displaystyle B(p,q) = \int_{0}^{1} t^{p-1} (1-t)^{q-1} dt$ to $t = \sin^2 \theta$, $$ B(p,q) = \int_{0}^{{\pi} \over {2}} \left( \sin^2 \theta</description></item><item><title>Generalization of Binomial Coefficients: Beta Function</title><link>https://freshrimpsushi.github.io/en/posts/450/</link><pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/450/</guid><description>Theorem: Binomial Coefficients Expressed Through the Beta Function $0 \le k\le n$ is satisfied for two natural numbers $k,n$, the following equation holds. $$ \binom{n}{k}={}_{n}C_{k}=C(n,k)=\frac{1}{(n+1)B(n-k+1,k+1)} $$ For two natural numbers $m,n$, the following equation holds. $$ B(m,n)=\left[ \frac{mn}{m+n} \begin{pmatrix} m+n \\ n \end{pmatrix}\right]^{-1} $$ Description The beta function, defined by $B(p,q):=\displaystyle \int_{0}^{1}t^{p-1}(1-t)^{q-1}dt$, can also be seen as a generalization of binomial coefficients. The proof is not difficult, but a lemma</description></item><item><title>Euler's Proof: Finding the Sum of Reciprocals of Squares Using the Sinc Function</title><link>https://freshrimpsushi.github.io/en/posts/391/</link><pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/391/</guid><description>Theorem $$ \sum_{n =1 }^{\infty} {{1} \over {n^2}} = {{ \pi ^2 } \over { 6 }} $$ Proof Strategy: This proof, left by Euler, uses the Euler representation of the sinc function to provide a solution. The idea is quite fresh and interesting, making it harder to forget once you&amp;rsquo;ve seen it. Euler representation of the sinc function: $$ {{\sin x} \over {x}} = \prod_{n=1}^{\infty} \left( 1 - {{x^2}</description></item><item><title>Convex Functions, Concave Functions</title><link>https://freshrimpsushi.github.io/en/posts/262/</link><pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/262/</guid><description>Definition Given an interval $I \subset \mathbb{R}$, two elements $x_{1} , x_{2}$ and functions $f : I \to \mathbb{R}$ and $0 \le t \le 1$, When $f( t x_{1} + (1-t) x_{2}) \le t f(x_{1}) + (1-t) f(x_{2})$, $f$ is defined as a convex function on $I$. When $f( t x_{1} + (1-t) x_{2}) \ge t f(x_{1}) + (1-t) f(x_{2})$, $f$ is defined as a concave function on $I$. Explanation</description></item><item><title>What is a Generating Function?</title><link>https://freshrimpsushi.github.io/en/posts/232/</link><pubDate>Wed, 30 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/232/</guid><description>Definition A function $g$, represented in the form such as $$ g(x) =\sum \limits _{n=0}^{\infty}a_{n}x^{n}= a_{0} + a_{1} x + a_{2} x^2 + \cdots $$ for a given sequence $\left\{ a_{n} \right\}$, is called the generating function of the sequence $\left\{ a_{n}\right\}$ or simply generating function. When the sequence is $a_{n}=a_{n}(x)$, it is also denoted as follows: $$ G(x,t)=\sum \limits _{n=0}^{\infty}a_{n}(x)t^{n} $$ Explanation As sharp readers may have noticed, the</description></item><item><title>Euler's Reflection Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/192/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/192/</guid><description>Formulas For non-integer $p$, $$ {\Gamma (1-p) \Gamma ( p )} = { {\pi} \over {\sin \pi p } } $$ Description It is the most famous formula among the formulas using the Gamma function. A useful result that can be obtained from the reflection formula is $ \Gamma ( { 1 \over 2} ) = \sqrt{\pi}$. Perhaps that’s why? The name &amp;ldquo;reflection formula&amp;rdquo; is</description></item><item><title>Proof of Euler's Representation of the Sinc Function</title><link>https://freshrimpsushi.github.io/en/posts/187/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/187/</guid><description>Definition Unnormalized Sinc Function The following function $\sinc : \mathbb{R} \to \mathbb{R}$ is called the sinc function. $$ \sinc x := \begin{cases} \displaystyle {{\sin x} \over {x}} &amp;amp; , \text{if } x \ne 0 \\ 1 &amp;amp; , \text{if } x = 0 \end{cases} $$ Normalized Sinc Function $$ \sinc x := \begin{cases} \displaystyle {{\sin \pi x} \over {\pi x}} &amp;amp; , \text{if } x \ne 0 \\ 1 &amp;amp;</description></item><item><title>Weierstrass's Infinite Product for the Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/150/</link><pubDate>Wed, 26 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/150/</guid><description>Theorem The following holds for the Gamma function $\Gamma : (0, \infty) \to \mathbb{R}$: $$ {1 \over \Gamma (x)} = x e^{\gamma x } \lim_{n \to \infty} \prod_{k=1}^{n} \left( 1 + {x \over k} \right) e^{- {x \over k} } $$ $\gamma$ is the Euler-Mascheroni constant. Explanation $$ \Gamma (x) = \int_{0}^{\infty} t^{x-1} e^{-t} dt $$ The Gamma function is defined as above, and also by Euler&amp;rsquo;s limit formula, $$</description></item><item><title>Proof of the Convergence of the Euler-Mascheroni Constant</title><link>https://freshrimpsushi.github.io/en/posts/151/</link><pubDate>Sun, 16 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/151/</guid><description>Theorem $$ \gamma = \lim_{n \to \infty} \left( \sum_{k=1}^{n} \left( { 1 \over k } \right) - \ln{n} \right) = 0.577215664 \cdots $$ Description When associated with the Riemann zeta function, it also serves as the $\gamma$ $0$’th Stieltjes constant $\gamma_{0}$. $\gamma$ is briefly known as the Euler&amp;rsquo;s constant, which has a deep relationship with the Gamma function. Setting the exact value aside, does it</description></item><item><title>Euler's Limit Formula Derivation for the Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/149/</link><pubDate>Sat, 15 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/149/</guid><description>Formula 1 The following holds for the Gamma function $\Gamma : (0, \infty) \to \mathbb{R}$: $$ \Gamma (x) = \lim_{n \to \infty} {{n^x n!} \over {x(x+1)(x+2) \cdots (x+n) }} $$ Explanation The Gamma function that we previously knew in the form of integral $$ \Gamma (x) = \int_{0}^{\infty} t^{x-1} e^{-t} dt $$ looks entirely different, yet in 1729 Euler proved that both expressions are exactly the same. The derivation introduced</description></item><item><title>Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/103/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/103/</guid><description>Definition A function that satisfies the following two conditions is called the Dirac delta function. $$ \delta (x) = \begin{cases} 0, &amp;amp; x\neq 0 \\ \infty , &amp;amp; x=0 \end{cases} $$ $$ \int_{-\infty}^{\infty}{\delta (x) dx}=1 $$ Description ※Be careful not to confuse it with the Kronecker delta. In engineering, it is called the unit impulse function. Strictly speaking, mathematically, the Dirac delta function is not a function because</description></item><item><title>Properties of the Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/104/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/104/</guid><description>Properties $$ \begin{equation} \delta (-x) =\delta (x) \end{equation} $$ $$ \begin{equation} \delta (kx)= \frac{1}{|k|} \delta (x) \end{equation} $$ Proof Proof of $(1)$ Substituting $\int_{-\infty }^ { \infty } f(x) \delta (-x) dx$ with $-x \equiv y$ gets us $x=-y$ and $dx=-dy$, $$ \begin{align*} \int_{-\infty } ^{ \infty } f(x) \delta (-x) dx =&amp;amp;\ -\int_{ \infty }^{-\infty} f(-y) \delta (y) dy \\ =&amp;amp;\ \int_{-\infty } ^{\infty } f(-y) \delta (y) dy</description></item><item><title>Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/95/</link><pubDate>Sun, 30 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/95/</guid><description>Definition The function defined as follows $\Gamma : (0, \infty) \to \mathbb{R}$ is called the Gamma function. $$ \Gamma (x) := \int_{0}^{\infty} t^{x-1} e^{-t} dt $$ Description Focusing on the integral in the equation above, it is also referred to as Euler&amp;rsquo;s integral. The Gamma function is famous as an exceedingly important function not just in pure mathematics but also in physics, statistics, etc. It possesses a plethora of interesting</description></item><item><title>Addition Formula for Trigonometric Functions: Various Proofs</title><link>https://freshrimpsushi.github.io/en/posts/44/</link><pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/44/</guid><description>Theorem $$ \sin\left( \alpha +\beta \right) =\sin\alpha \cos\beta +\cos\alpha \sin\beta \\ \sin\left( \alpha -\beta \right) =\sin\alpha \cos\beta -\cos\alpha \sin\beta \\ \cos\left( \alpha +\beta \right) =\cos\alpha \cos\beta -\sin\alpha \sin\beta \\ \cos\left( \alpha -\beta \right) =\cos\alpha \cos\beta +\sin\alpha \sin\beta \\ \tan\left( \alpha +\beta \right) =\frac { \tan\alpha +\tan\beta }{ 1-\tan\alpha \tan\beta } \\ \tan\left( \alpha -\beta \right) =\frac { \tan\alpha -\tan\beta }{ 1+\tan\alpha \tan\beta } $$ Proof Proof using the Law of</description></item><item><title>Odd Functions and Even Functions</title><link>https://freshrimpsushi.github.io/en/posts/40/</link><pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/40/</guid><description>Definitions A function $f(x)$ that satisfies $f(-x) = f(x)$ is called an Even function. A function $f(x)$ that satisfies $f(-x) = -f(x)$ is called an Odd function. Description Even functions are symmetric about the $y$ axis in the coordinate plane, while Odd functions are symmetric about the origin $O$. For example, among the trigonometric functions, $\sin$ is Odd and $\cos$ is Even. Differentiating $\sin$ yields $\cos$, and differentiating $\cos$ yields</description></item><item><title>Integration Techniques for Various Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/31/</link><pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/31/</guid><description>Overview When solving integration problems, you often have to integrate trigonometric functions. And, getting familiar with these integration methods, trigonometric functions can be integrated as quickly as polynomial functions. Integration of Secant Functions, Integration of Cosecant Functions $$ \begin{align*} \int \sec x dx =&amp;amp; \int \frac { \sec x (\sec x +\tan x ) }{ (\sec x +\tan x ) }dx \\ =&amp;amp; \int \frac { \sec^{ 2 }x+\sec x</description></item><item><title>The Relationship between the Translation of Trigonometric Functions and Their Derivatives</title><link>https://freshrimpsushi.github.io/en/posts/11/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/11/</guid><description>Formula [1] Sine: $$\sin{\left( \theta +\frac { n }{ 2 }\pi \right)}={ \sin }^{ (n) }\theta$$ [2] Cosine: $$\cos{\left( \theta +\frac { n }{ 2 }\pi \right)}={ \cos }^{ (n) }\theta$$ $(n)$ means differentiating $n$ times. Explanation Simply put, you differentiate once every time you move by 90˚. Let&amp;rsquo;s actually calculate for $n=3$. Method Using Addition Theorem $$ \begin{align*} \cos \left( \theta +{3 \over 2}\pi \right) =&amp;amp;</description></item></channel></rss>