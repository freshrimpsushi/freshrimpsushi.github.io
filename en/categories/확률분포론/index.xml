<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Probability Distribution on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC%EB%A1%A0/</link><description>Recent content in Probability Distribution on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 03 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC%EB%A1%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>Mean and Variance of the Hypergeometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2689/</link><pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2689/</guid><description>Formula Random variable $X$ follows the hypergeometric distribution $X \sim \operatorname{HG}(N, D, n)$. Then its mean and variance for $p := D / N$ are given by: $$ \begin{align*} E (X) =&amp;amp; n \frac{D}{N} = n p \\ \Var (X) =&amp;amp; n {\frac{ D }{ N }} {\frac{ N - D }{ N }} {\frac{ N - n }{ N - 1 }} = np(1 - p) \frac{N - n}{N</description></item><item><title>Relative Entropy (Kullback-Leibler Divergence) between Two Normal Distributions</title><link>https://freshrimpsushi.github.io/en/posts/3681/</link><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3681/</guid><description>Formula The relative entropy (KLD) between two normal distributions $N(\mu, \sigma^{2})$ and $N(\mu_{1}, \sigma_{1}^{2})$ is given by the following expression. $$ D_{\text{KL}}\big( N(\mu, \sigma^{2}) \| N(\mu_{1}, \sigma_{1}^{2}) \big) = \log \left( \dfrac{\sigma_{1}}{\sigma} \right) + \dfrac{\sigma^{2} + (\mu - \mu_{1})^{2}}{2\sigma_{1}^{2}} - \dfrac{1}{2} $$ The relative entropy between two multivariate normal distributions $N(\boldsymbol{\mu}, \Sigma)$ and $N(\boldsymbol{\mu_{1}}, \Sigma_{1})$ is given by the following. $$ \begin{array}{l} D_{\text{KL}}\big( N(\boldsymbol{\mu}, \Sigma) \| N(\boldsymbol{\mu_{1}}, \Sigma_{1}) \big) \\[1em]</description></item><item><title>Hypergeometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2687/</link><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2687/</guid><description>Definition 1 For natural numbers $n, N, D \in \mathbb{N}$, the discrete probability distribution with the following probability mass function is called the hypergeometric distribution. $$ p(x) = {\frac{ \binom{D}{x} \binom{N - D}{n - x} }{ \binom{N}{n} }} \qquad , x \in 0, 1, \cdots , n $$ Here $\binom{N}{n} = _{N} C _{n}$ denotes the binomial coefficient. Description Commonly in the hypergeometric distribution, $N$ denotes the size of the</description></item><item><title>The Mean and Variance of the Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3654/</link><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3654/</guid><description>Formula Given $X \sim$ when $\operatorname{Bin}(1, p)$, the mean and variance of $X$ are as follows. $$ E(X) = p $$ $$ \Var(X) = p(1-p) = pq, \qquad q = 1 - p $$ Proof For $p \in [0, 1]$, a discrete probability distribution with the following probability mass function is called a Bernoulli distribution. $$ f(x) = p^{x}(1-p)^{1-x}, \qquad x = 0, 1 $$ Direct Calculation By the definition</description></item><item><title>Moment Generating Function of Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3653/</link><pubDate>Sat, 07 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3653/</guid><description>Formula $X \sim$ When $\operatorname{Bin}(1, p)$, the moment generating function of $X$ is given below. $$ m(t) = 1 - p + pe^{t} = q + pe^{t}, \qquad q = 1 - p $$ Proof For $p \in [0, 1]$, a discrete probability distribution with the following probability mass function is referred to as a Bernoulli distribution. $$ f(x) = p^{x}(1-p)^{1-x}, \qquad x = 0, 1 $$ From the Definition</description></item><item><title>Categorical Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3652/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3652/</guid><description>Definition1 Given a sample space with $k (\ge 2)$ categories, $\Omega = \left\{ 1, 2, \dots, k \right\}$, and a probability vector $\mathbf{p} = (p_{1}, \dots, p_{k})$, the discrete probability distribution with the following probability mass function is called the Categorical distribution. $$ p(x = i) = p_{i}, \qquad x \in \left\{ 1, 2, \dots, k \right\} $$ Description The probability of each of the $k$ categories occurring is represented</description></item><item><title>Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3651/</link><pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3651/</guid><description>Definition1 For $p \in [0, 1]$, a discrete probability distribution with the following probability mass function is referred to as a Bernoulli distribution. $$ f(x) = p^{x}(1-p)^{1-x}, \qquad x = 0, 1 $$ Description This distribution is used when describing an experiment with only two possible outcomes, such as a coin toss. Because there are two possible outcomes, $x = 1$ is commonly referred to as a success and $x</description></item><item><title>Maximum Likelihood Estimator of the Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3647/</link><pubDate>Thu, 22 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3647/</guid><description>Theorem Suppose we are given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \operatorname{Laplace}(\mu, b)$ that follows a Laplace distribution. The maximum likelihood estimator $(\hat{\mu}, \hat{b})$ for $(\mu, b)$ is as follows. $$ \hat{\mu} = \text{median}(\mathbf{x}_{1}, \cdots, \mathbf{x}_{n}) $$ $$ \hat{b} = \dfrac{1}{n} \sum\limits_{k=1}^{n} |x_{k} - \mu| $$ Proof Laplace Distribution: The Laplace distribution with parameters $\mu \in \mathbb{R}$ and $b &amp;gt; 0$ is a</description></item><item><title>Moment Generating Function of the Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3646/</link><pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3646/</guid><description>Formula $X \sim$ When $\operatorname{Laplace}(\mu, b)$, the moment-generating function of $X$ is as follows. $$ m(t) = \dfrac{1}{1 - b^{2}t^{2}} e^{\mu t} \qquad \text{for } |t| &amp;lt; \dfrac{1}{b} $$ Proof By the definition of the moment-generating function, $$ \begin{align*} E(e^{tX}) &amp;amp;= \int\limits_{-\infty}^{\infty} e^{tx} f(x) dx \\ &amp;amp;= \int\limits_{-\infty}^{\infty} e^{tx} \dfrac{1}{2b} e^{-|x - \mu|/b} dx \\ &amp;amp;= \dfrac{a}{2}e^{\mu t} \int\limits_{-\infty}^{\infty} e^{ty} e^{-a|y|} dx \qquad (y \equiv x - \mu, \quad a</description></item><item><title>Mean and Variance of Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3645/</link><pubDate>Sun, 18 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3645/</guid><description>Formula $X \sim$ When $\operatorname{Laplace}(\mu, b)$, the mean and variance of $X$ are as follows. $$ E(X) = \mu $$ $$ \Var(X) = 2b^{2} $$ Proof Direct Calculation By the definition of expectation and integration by parts, $$ \begin{align*} E(X) &amp;amp;= \int\limits_{-\infty}^{\infty} x \dfrac{1}{2b} e^{-|x - \mu|/b} dx \\ &amp;amp;= \int\limits_{-\infty}^{\mu} \dfrac{x}{2b} e^{(x - \mu)/b} dx + \int\limits_{\mu}^{\infty} \dfrac{x}{2b} e^{-(x - \mu)/b} dx \\ &amp;amp;= \left( \left[\dfrac{x}{2b}\left(b e^{(x - \mu)/b}\right)\right]_{-\infty}^{\mu}</description></item><item><title>Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3644/</link><pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3644/</guid><description>Definition1 For $\mu \in \mathbb{R}$ and $b &amp;gt; 0$, a continuous probability distribution $\operatorname{Laplace}(\mu, b)$ with the following probability density function is called the Laplace distribution. $$ f(x) = \dfrac{1}{2b} \exp \left( -\dfrac{|x - \mu|}{b} \right) $$ Explanation Relationship with the Normal Distribution Although it looks similar to the normal distribution, it has an absolute value $| x - \mu |$ instead of a square, giving it a sharper shape.</description></item><item><title>Expectation of the Power of Normally Distributed Random Variables with Mean Zero</title><link>https://freshrimpsushi.github.io/en/posts/300/</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/300/</guid><description>Official Random Variable $X$ follows a Normal Distribution $N \left( 0 , \sigma^{2} \right)$, then the expectation of its powers $X^{n}$ can be recursively expressed as follows1. $$ E \left( X^{n} \right) = (n - 1) \sigma^{2} E \left( X^{n-2} \right) $$ $E \left( X^{n} \right)$ is $0$ when $n$ is odd, and for even it is given by the following2. $$ E \left( X^{2n} \right) = \left( 2n -</description></item><item><title>Conditional Mean and Variance of the Multivariate Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2529/</link><pubDate>Mon, 26 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2529/</guid><description>Formulas Bivariate Normal Distribution $$ \left( X, Y \right) \sim N_{2} \left( \begin{bmatrix} \mu_{1} \\ \mu_{n} \end{bmatrix} , \begin{bmatrix} \sigma_{X}^{2} &amp;amp; \rho \sigma_{X} \sigma_{Y} \\ \rho \sigma_{X} \sigma_{Y} &amp;amp; \sigma_{Y}^{2} \end{bmatrix} \right) $$ When a random vector $\left( X,Y \right)$ follows a bivariate normal distribution as above, $X | Y$ follows a univariate normal distribution and the conditional mean and variance are as follows. $$ \begin{align*} E \left( X |</description></item><item><title>Independence and Zero Correlation are Equivalent in Multivariate Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2527/</link><pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2527/</guid><description>Theorem 1 $$ \begin{align*} \mathbf{X} =&amp;amp; \begin{bmatrix} \mathbf{X}_{1} \\ \mathbf{X}_{2} \end{bmatrix} &amp;amp; : \Omega \to \mathbb{R}^{n} \\ \mu =&amp;amp; \begin{bmatrix} \mu_{1} \\ \mu_{2} \end{bmatrix} &amp;amp; \in \mathbb{R}^{n} \\ \Sigma =&amp;amp; \begin{bmatrix} \Sigma_{11} &amp;amp; \Sigma_{12} \\ \Sigma_{21} &amp;amp; \Sigma_{22} \end{bmatrix} &amp;amp; \in \mathbb{R}^{n \times n} \end{align*} $$ Let&amp;rsquo;s assume that a random vector $\mathbf{X} \sim N_{n} \left( \mu , \Sigma \right)$, which follows a multivariate normal distribution, is given as shown</description></item><item><title>Linear Transformations of Multivariate Normal Distributions</title><link>https://freshrimpsushi.github.io/en/posts/2525/</link><pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2525/</guid><description>정리 1 Linear Transformations&amp;rsquo; Normality Regarding matrix $A \in \mathbb{R}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{R}^{m}$, a random vector $\mathbf{X} \sim N_{n} \left( \mu , \Sigma \right)$ following a multivariate normal distribution undergoes a linear transformation $\mathbf{Y} = A \mathbf{X} + \mathbf{b}$ will still follow a multivariate normal distribution $N_{m} \left( A \mu + \mathbf{b} , A \Sigma A^{T} \right)$. Normality of Marginal Distributions $$ \begin{align*} \mathbf{X} =&amp;amp;</description></item><item><title>Nonsymmetric F-distribution</title><link>https://freshrimpsushi.github.io/en/posts/2518/</link><pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2518/</guid><description>Definition Singly Non-central F-distribution 1 The degrees of freedom $r_{1} , r_{2} &amp;gt; 0$ and non-centrality $\lambda_{1} \ge 0$ define the probability density function of a continuous probability distribution $F \left( r_{1} , r_{2} , \lambda_{1} \right)$, known as the Singly Non-central F-distribution. $$ f(x) = \sum_{k=0}^{\infty} {{ e^{ - \lambda / 2 } \left( \lambda / 2 \right)^{k} } \over { B \left( {{ r_{2} } \over { 2</description></item><item><title>Noncentral Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2516/</link><pubDate>Wed, 31 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2516/</guid><description>Definition The continuous probability distribution that has the probability density function as follows with respect to degrees of freedom $r &amp;gt; 0$ and non-centrality $\lambda \ge 0$ is called the Noncentral Chi-squared Distribution. $$ f(x) = {{ 1 } \over { 2 }} e^{- \left( x + \lambda \right) / 2 } \left( {{ x } \over { \lambda }} \right)^{k/4 - 1/2} I_{r/2 - 1} \left( \sqrt{\lambda x} \right)</description></item><item><title>Kent Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2512/</link><pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2512/</guid><description>Definition 1 Concentration $\kappa &amp;gt; 0$ and $\beta \in \mathbb{R}$, Mean $\gamma_{1} \in S^{p-1}$, Major Axis $\gamma_{2} \in S^{p-1}$, Minor Axis $\gamma_{3} \in S^{p-1}$ are characterized by the following probability density function for the multivariate distribution $\text{FB}_{5} \left( \left( \gamma_{1} , \gamma_{2} , \gamma_{3} \right) , \kappa , \beta \right)$, known as the Kent Distribution. $$ f \left( \mathbf{x} \right) = {{ 1 } \over { c \left( \kappa ,</description></item><item><title>Bingham-Mardia Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2510/</link><pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2510/</guid><description>Definition 1 The Bingham-Mardia Distribution is a multivariate distribution $\text{BM}_{p} \left( \mu , \kappa, \nu \right)$ that has a probability density function for Unique Mode $\mu \in S^{p-1}$, Concentration $\kappa &amp;gt; 0$, and radius $\nu &amp;gt; 0$ as follows. $$ f \left( \mathbf{x} \right) = {{ 1 } \over { \alpha \left( \kappa , \nu \right) }} \exp \left( - \kappa \left( \mu^{T} \mathbf{x} - \nu \right)^{2} \right) \qquad ,</description></item><item><title>Why Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2508/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2508/</guid><description>Definition 1 The probability distribution which has the following probability density function in terms of location $\xi \in \mathbb{R}$, scale $\omega &amp;gt; 0$, and shape $\alpha \in \mathbb{R}$ parameters is called the Skew Normal Distribution. $$ \begin{align*} f(x) =&amp;amp; {{ 2 } \over { \omega }} \phi \left( {{ x - \xi } \over { \omega }} \right) \Phi \left( \alpha {{ x - \xi } \over { \omega }}</description></item><item><title>Von Mises-Fisher Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2504/</link><pubDate>Sun, 07 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2504/</guid><description>Definition 1 The Von Mises-Fisher Distribution is defined as the multivariate distribution $\text{vMF}_{p} \left( \mu , \kappa \right)$ with the following probability density function for Unique Mode $\mu \in S^{p-1}$ and Concentration $\kappa &amp;gt; 0$. $$ f \left( \mathbf{x} \right) = \left( {{ \kappa } \over { 2 }} \right)^{p/2-1} {{ 1 } \over { \Gamma \left( p/2 \right) I_{p/2-1} \left( \kappa \right) }} \exp \left( \kappa \mu^{T} \mathbf{x} \right)</description></item><item><title>Bivariate von Mises Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2490/</link><pubDate>Sun, 10 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2490/</guid><description>Definition 1 The mean direction $\mu, \nu \in \mathbb{R}$ and concentration $\kappa_{1}, \kappa_{2} &amp;gt; 0$ with respect to a certain matrix $A \in \mathbb{R}^{2 \times 2}$ have a continuous probability distribution $\text{vM}^{2} \left( \mu , \nu , \kappa_{1} , \kappa_{2} \right)$ with a probability density function that is proportional to the following, which is called the Bivariate von Mises Distribution: $$ \exp \left[ \kappa_{1} \cos \left( \theta - \mu \right)</description></item><item><title>Von Mises Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2488/</link><pubDate>Wed, 06 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2488/</guid><description>Definition 1 2 The von Mises Distribution is a continuous probability distribution with the probability density function given for Mean Direction $\mu \in \mathbb{R}$ and Concentration $\kappa &amp;gt; 0$ as follows: $$ f(x) = {{ 1 } \over {2 \pi I_{0} \left( \kappa \right) }} \exp \left( \kappa \cos \left( x - \mu \right) \right) \qquad , x \in \mathbb{R} \pmod{2 \pi} $$ $I_{\nu}$ is a modified Bessel function of</description></item><item><title>Proof of Pearson's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2486/</link><pubDate>Sat, 02 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2486/</guid><description>Theorem Let&amp;rsquo;s say that the random vector $\left( N_{1} , \cdots , N_{k} \right)$ follows the multinomial distribution $M_{k} \left( n ; \mathbf{p} \right)$ for $\mathbf{p} = \left( p_{1} , \cdots , p_{k} \right) \in [0,1]^{k}$ that satisfies $$ \sum_{i=1}^{k} N_{i} = n \qquad \&amp;amp; \qquad \sum_{i=1}^{k} p_{i} = 1 $$, and the sample sizes $n \in \mathbb{N}$ and $k \in \mathbb{N}$ categories. Then, when $n \to \infty$, the statistic</description></item><item><title>Derivation of the Covariance Matrix of the Multinomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2484/</link><pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2484/</guid><description>Formula If a random vector $\mathbf{X} := \left( X_{1} , \cdots , X_{k} \right)$ follows a multinomial distribution $M_{k} \left( n, \mathbf{p} \right)$, then its covariance matrix is as follows. $$ \operatorname{Cov} \left( \mathbf{X} \right) = n \begin{bmatrix} p_{1} \left( 1 - p_{1} \right) &amp;amp; - p_{1} p_{2} &amp;amp; \cdots &amp;amp; - p_{1} p_{k} \\ - p_{2} p_{1} &amp;amp; p_{2} \left( 1 - p_{2} \right) &amp;amp; \cdots &amp;amp; - p_{2}</description></item><item><title>Entropy of Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3510/</link><pubDate>Tue, 21 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3510/</guid><description>Theorem The entropy of the normal distribution $N(\mu, \sigma^{2})$ (when using natural logarithms) is as follows. $$ H = \dfrac{1}{2} \ln (2\pi e \sigma^{2}) = \ln \sqrt{2\pi e \sigma^{2}} $$ The entropy of the multivariate normal distribution $N_{p}(\boldsymbol{\mu}, \Sigma)$ is as follows. $$ H = \dfrac{1}{2}\ln \left[ (2 \pi e)^{p} \left| \Sigma \right| \right] = \dfrac{1}{2}\ln (\det (2\pi e \Sigma)) $$ $\left| \Sigma \right|$ is the determinant of the covariance</description></item><item><title>Polynomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2480/</link><pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2480/</guid><description>Definition Let a random vector composed of $n \in \mathbb{N}$ and $k \in \mathbb{N}$ counts of random variables be denoted as $\left( X_{1} , \cdots , X_{k} \right)$. $$ \sum_{i=1}^{k} X_{i} = n \qquad \&amp;amp; \qquad \sum_{i=1}^{k} p_{i} = 1 $$ For $\mathbf{p} = \left( p_{1} , \cdots , p_{k} \right) \in [0,1]^{k}$ that satisfies this, a multivariate probability distribution $M_{k} \left( n, \mathbf{p} \right)$ with the following probability mass</description></item><item><title>Reasons Why the Modified Bessel Function of the First Kind Appears in Directional Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2478/</link><pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2478/</guid><description>Buildup Modified Bessel Functions $$ J_{\nu}(x) = \sum \limits_{n=0}^{\infty} \frac{(-1)^{n} }{\Gamma (n+1) \Gamma (n+\nu+1)} \left(\frac{x}{2} \right)^{2n+\nu} $$ The $I_{\nu}$ defined as follows for the Bessel function of the first kind $J_{\nu}$ is called the modified Bessel function of the first kind1. $$ \begin{align*} I_{\nu} (z) :=&amp;amp; i^{-\nu} J_{\nu} \left( iz \right) \\ =&amp;amp; \left( {{ z } \over { 2 }} \right)^{\nu} \sum_{k=0}^{\infty} {{ {{ z } \over { 2</description></item><item><title>Proof of Normality of Regression Coefficients</title><link>https://freshrimpsushi.github.io/en/posts/2460/</link><pubDate>Wed, 11 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2460/</guid><description>Theorem $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ Given $p$ independent variables and</description></item><item><title>Chi-Square Distribution's Sufficient Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2359/</link><pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2359/</guid><description>Theorem Let&amp;rsquo;s assume we have a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \chi^{2} (r)$ that follows a chi-squared distribution. The sufficient statistic $T$ for $r$ is as follows. $$ T = \left( \prod_{i} X_{i} \right) $$ Proof Relationship between gamma distribution and chi-squared distribution: $$ \Gamma \left( { r \over 2 } , 2 \right) \iff \chi ^2 (r) $$ Sufficient statistic for the</description></item><item><title>Sufficient Statistics for the Beta Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2357/</link><pubDate>Sun, 19 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2357/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \text{Beta} \left( \alpha, \beta \right)$ that follows a beta distribution, the sufficient statistic $T$ for $\left( \alpha, \beta \right)$ is as follows. $$ T = \left( \prod_{i} X_{i}, \prod_{i} \left( 1 - X_{i} \right) \right) $$ Proof $$ \begin{align*} f \left( \mathbf{x} ; \alpha, \beta \right) =&amp;amp; \prod_{k=1}^{n} f \left( x_{k} ; \alpha, \beta \right)</description></item><item><title>Sufficient Statistics of the Gamma Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2355/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2355/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \Gamma \left( k, \theta \right)$ that follows the Gamma distribution. The sufficient statistic $T$ for $\left( k, \theta \right)$ is as follows. $$ T = \left( \prod_{i} X_{i}, \sum_{i} X_{i} \right) $$ Proof $$ \begin{align*} f \left( \mathbf{x} ; k, \theta \right) =&amp;amp; \prod_{k=1}^{n} f \left( x_{k} ; k, \theta \right) \\ =&amp;amp; \prod_{i=1}^{n} {{</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators of a Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2353/</link><pubDate>Sat, 11 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2353/</guid><description>Theorem A given random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim N \left( \mu , \sigma^{2} \right)$ follows a normal distribution. The sufficient statistic $T$ and maximum likelihood estimator $\left( \hat{\mu}, \widehat{\sigma^{2}} \right)$ for $\left( \mu, \sigma^{2} \right)$ are as follows: $$ \begin{align*} T =&amp;amp; \left( \sum_{k} X_{k}, \sum_{k} X_{k}^{2} \right) \\ \left( \hat{\mu}, \widehat{\sigma^{2}} \right) =&amp;amp; \left( {{ 1 } \over { n }} \sum_{k}</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators of Exponential Distributions</title><link>https://freshrimpsushi.github.io/en/posts/2349/</link><pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2349/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \exp \left( \lambda \right)$ that follows an exponential distribution. The sufficient statistic $T$ and maximum likelihood estimator $\hat{\lambda}$ for $\lambda$ are as follows. $$ \begin{align*} T =&amp;amp; \sum_{k=1}^{n} X_{k} \\ \hat{\lambda} =&amp;amp; {{ n } \over { \sum_{k=1}^{n} X_{k} }} \end{align*} $$ Proof Sufficient Statistic $$ \begin{align*} f \left( \mathbf{x} ; \lambda \right) =&amp;amp; \prod_{k=1}^{n}</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators of the Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2347/</link><pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2347/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \text{Poi} \left( \lambda \right)$ following a Poisson distribution, the sufficient statistic $T$ and the maximum likelihood estimator $\hat{\lambda}$ for $\lambda$ are as follows: $$ \begin{align*} T =&amp;amp; \sum_{k=1}^{n} X_{k} \\ \hat{\lambda} =&amp;amp; {{ 1 } \over { n }} \sum_{k=1}^{n} X_{k} \end{align*} $$ Proof Sufficient Statistic $$ \begin{align*} f \left( \mathbf{x} ; \lambda \right) =&amp;amp;</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators for the Geometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2345/</link><pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2345/</guid><description>Theorem Given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \text{Geo} \left( p \right)$ that follows a geometric distribution, the sufficient statistic $T$ and the maximum likelihood estimator $\hat{p}$ for $p$ are as follows. $$ \begin{align*} T =&amp;amp; \sum_{k=1}^{n} X_{k} \\ \hat{p} =&amp;amp; {{ n } \over { \sum_{k=1}^{n} X_{k} }} \end{align*} $$ Proof Sufficient Statistic $$ \begin{align*} f \left( \mathbf{x} ; p \right) =&amp;amp;</description></item><item><title>Sufficient Statistics and Maximum Likelihood Estimators for the Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2343/</link><pubDate>Sun, 19 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2343/</guid><description>Theorem Let&amp;rsquo;s assume we have a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim U \left( 0 , \theta \right)$ following a uniform distribution. The sufficient statistic $T$ and maximum likelihood estimator $\hat{\theta}$ for $\theta$ are as follows: $$ \begin{align*} T =&amp;amp; \max_{k=1 , \cdots , n} X_{k} \\ \hat{\theta} =&amp;amp; \max_{k=1 , \cdots , n} X_{k} \end{align*} $$ Proof Strategy: The sufficient statistic and maximum</description></item><item><title>Derivation of the F-distribution from the t-distribution</title><link>https://freshrimpsushi.github.io/en/posts/2233/</link><pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2233/</guid><description>Theorem 1 A random variable $X \sim t(\nu)$ that follows a t-distribution with degrees of freedom $\nu &amp;gt; 0$ is defined as $Y$, which follows an F-distribution $F (1,\nu)$. $$ Y := X^{2} \sim F (1,\nu) $$ Proof Via Chi-Square Distribution $X \sim t(\nu)$, which follows a standard normal distribution, and $W$, which follows a chi-square distribution with degrees of freedom $\nu$, are related as $$ X^{2} = \left( {{</description></item><item><title>Derivation of Beta Distribution from F-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2231/</link><pubDate>Sun, 10 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2231/</guid><description>Theorem 1 A random variable $X \sim F \left( r_{1}, r_{2} \right)$ following an F-distribution with degrees of freedom $r_{1} , r_{2}$ is defined as follows $Y$ and follows a beta distribution $\text{Best} \left( {{ r_{1} } \over { 2 }} , {{ r_{2} } \over { 2 }} \right)$. $$ Y := {{ \left( r_{1} / r_{2} \right) X } \over { 1 + \left( r_{1} / r_{2} \right)</description></item><item><title>Weibull Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2219/</link><pubDate>Thu, 16 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2219/</guid><description>Definition A Weibull Distribution is a probability distribution with the following probability density function, given scale parameter $\lambda &amp;gt; 0$ and shape parameter $k &amp;gt; 0$. $$ f(x) = {{ k } \over { \lambda }} \left( {{ x } \over { \lambda }} \right)^{k-1} e^{-(x/\lambda)^{k}} \qquad , x \ge 0 $$ Theorems [1] A Generalization of the Exponential Distribution: The Weibull Distribution becomes the Exponential Distribution when $k=1$. [2]</description></item><item><title>Rayleigh Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2210/</link><pubDate>Sun, 29 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2210/</guid><description>Definition 1 A continuous probability distribution with the probability density function given for the scale parameter $\sigma &amp;gt; 0$ is called the Rayleigh Distribution. $$ f(x) = {{ x } \over { \sigma^{2} }} e^{ - x^{2} / (2 \sigma^{2})} \qquad , x \ge 0 $$ Theorem [1]: If $X, Y \sim N \left( 0, \sigma^{2} \right)$ then $\sqrt{X^{2} + Y^{2}}$ follows a Rayleigh Distribution with $\sigma &amp;gt; 0$. Explanation</description></item><item><title>Pareto Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2181/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2181/</guid><description>Definition 1 For the scale parameter $x_{0} &amp;gt; 0$ and the shape parameter $\alpha &amp;gt; 0$, the following probability function is referred to as the Pareto Distribution, Power Law, or Scale-free Distribution: Continuous: For a constant $C$ that satisfies constant $\displaystyle \int_{x_{0}}^{\infty} p(x) dx = 1$ $$ p(x) = C x^{-\alpha} \qquad , x &amp;gt; x_{0} $$ Discrete: For the Riemann zeta function $\zeta$ $$ p_{k} = {{ 1 }</description></item><item><title>Log-Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2153/</link><pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2153/</guid><description>Definition 1 The continuous probability distribution $\log N \left( \mu,\sigma^{2} \right)$, which has the probability density function given for $\mu \in \mathbb{R}$ and $\sigma^{2} &amp;gt; 0$, is known as the log-normal distribution. $$ f(x) = {{ 1 } \over { x \sigma \sqrt{2 \pi}}} \exp \left[ - {{ \left( \log x - \mu \right)^{2} } \over { 2 \sigma^{2} }} \right] \qquad, x &amp;gt; 0 $$ Description In fact, the</description></item><item><title>Approximation of Normal Distribution Variance Stabilization from a Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2015/</link><pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2015/</guid><description>Example 1 If $Y = Y_{n}$ follows a binomial distribution $\text{Bin} (n,p)$, $$ \arcsin \sqrt{ {{ Y } \over { n }} } \overset{D}{\to} N \left( \arcsin \sqrt{p} , n/4 \right) $$ $N \left( \mu , \sigma^{2} \right)$ refers to the normal distribution. $\overset{D}{\to}$ refers to convergence in distribution. Explanation The binomial distribution $\text{Bin} (n, p )$ converges to the normal distribution $N \left( np, np(1-p) \right)$ when $n \to</description></item><item><title>Multivariate t-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/2003/</link><pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2003/</guid><description>Definition Given a location vector $\mathbf{\mu} \in \mathbb{R}^{p}$ and a scale matrix $\Sigma \in \mathbb{R}^{p \times p}$ that is positive definite, the multivariate distribution $t_{p} \left(\nu; \mu , \Sigma \right)$ with the following probability density function is referred to as the Multivariate t-distribution. $$ f (\textbf{x}) = {{ \Gamma \left[ (\nu + p) / 2 \right] } \over { \Gamma ( \nu / 2) \sqrt{ \nu^{p} \pi^{p} \det \Sigma }</description></item><item><title>Multivariate Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1954/</link><pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1954/</guid><description>Definition The multivariate distribution $N_{p} \left( \mu , \Sigma \right)$ with the following probability density function, given the population mean vector $\mathbf{\mu} \in \mathbb{R}^{p}$ and the covariance matrix $\Sigma \in \mathbb{R}^{p \times p}$, is called the multivariate normal distribution. $$ f (\textbf{x}) = \left( (2\pi)^{p} \det \Sigma \right)^{-1/2} \exp \left[ - {{ 1 } \over { 2 }} \left( \textbf{x} - \mathbf{\mu} \right)^{T} \Sigma^{-1} \left( \textbf{x} - \mathbf{\mu} \right) \right]</description></item><item><title>Deriving Standard Normal Distribution as a Limiting Distribution of Student's t-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/195/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/195/</guid><description>Theorem If $T_n \sim t(n)$ then $$ T_n \ \overset{D}{\to} N(0,1) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with mean $\mu$ and variance $\sigma^{2}$. $t(r)$ is a t-distribution with degrees of freedom $r$. $\overset{D}{\to}$ respectively imply distribution convergence. Originally, the Student t-distribution was created for statistical analysis when the sample size is small. As the sample size increases, it becomes similar to the standard normal distribution,</description></item><item><title>Derivation of the Standard Normal Distribution as the Limiting Distribution of the Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/197/</link><pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/197/</guid><description>Theorem If $X_{n} \sim \text{Poi} \left( n \right)$ and $\displaystyle Y_{n} := {{ X_{n} - n } \over { \sqrt{n} }}$ are given $$ Y_{n} \overset{D}{\to} N(0,1) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with a mean of $\mu$ and a variance of $\sigma^{2}$. $\text{Poi} (\lambda)$ is a Poisson distribution with mean and variance of $\lambda$. Explanation Considering the approximation of the binomial distribution to the</description></item><item><title>Derivation of the Standard Normal Distribution as a Limiting Distribution of the Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/196/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/196/</guid><description>Theorem De Moivre-Laplace Theorem If $X_i \sim B(1,p)$ and $Y_n = X_1 + X_2 + \cdots + X_n$, then $Y_n \sim B(n,p)$ and $$ { { Y_n - np } \over {\sqrt{ np(1-p) } } }\overset{D}{\to} N(0,1) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with mean $\mu$ and variance $\sigma^{2}$. $B(n,p)$ is a binomial distribution with $n$ trials and probability $p$. $\overset{D}{\to}$ denotes convergence in distribution.</description></item><item><title>The Poisson Distribution as a Limiting Distribution of the Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/198/</link><pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/198/</guid><description>Theorem Let&amp;rsquo;s say $X_{n} \sim B(n,p)$. If $\mu \approx np$ then $$ X_{n} \overset{D}{\to} \text{Poi} (\mu) $$ $B(n,p)$ is a binomial distribution with trials $n$ and probability $p$. $\text{Poi} (\lambda)$ is a Poisson distribution with mean and variance $\lambda$. $\overset{D}{\to}$ means distribution convergence. Description Note that the condition $\mu \approx np$ is necessary here. Since $ np \approx npq$, it implies $q = (1-p) \approx 1$, i.e., $p \approx 0$.</description></item><item><title>Cauchy Distribution: A Distribution Without a Mean</title><link>https://freshrimpsushi.github.io/en/posts/147/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/147/</guid><description>Definition The continuous probability distribution with the following probability density function is called a Cauchy distribution. $C$ $$ f(x) = {1 \over \pi} {1 \over {x^2 + 1}} \qquad , x \in \mathbb{R} $$ Explanation It may seem like all probability distributions would have a mean and variance, but in reality, that&amp;rsquo;s not always the case. A prime example of this is the Cauchy distribution, which at a glance resembles</description></item><item><title>Mean and Variance of the t-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1669/</link><pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1669/</guid><description>Formulas $X \sim t (\nu)$ if $$ E(X) = 0 \qquad , \nu &amp;gt;1 \\ \Var(X) = {{ \nu } \over { \nu - 2 }} \qquad , \nu &amp;gt; 2 $$ Derivation Strategy: Similar to the chi-squared distribution, the t-distribution also has known moment-generating functions, which we utilize. Moment of the t-distribution: Assume two random variables $W,V$ are independent and $W \sim N(0,1)$, $V \sim \chi^{2} (r)$. If $k</description></item><item><title>t-Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1667/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1667/</guid><description>Definition 1 A continuous probability distribution $t \left( \nu \right)$, known as the t-distribution, is defined for degrees of freedom $\nu &amp;gt; 0$ as having the following probability density function: $$ f(x) = {{ \Gamma \left( {{ \nu + 1 } \over { 2 }} \right) } \over { \sqrt{\nu \pi} \Gamma \left( {{ \nu } \over { 2 }} \right) }} \left( 1 + {{ x^{2} } \over {</description></item><item><title>Derivation of the Student's t-Distribution from Independent Normal Distributions and the Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/204/</link><pubDate>Sun, 13 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/204/</guid><description>Theorem Two independent random variables $W,V$ where $W \sim N(0,1)$ and $V \sim \chi^{2} (r)$, then $$ T = { {W} \over {\sqrt{V/r} } } \sim t(r) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with mean $\mu$ and variance $\sigma^{2}$. $\chi^{2} \left( r \right)$ is a chi-squared distribution with degrees of freedom $r$. $t(r)$ is a t-distribution with degrees of freedom $r$. Description If this theorem</description></item><item><title>The Square of a Standard Normal Distribution Follows a Chi-Square Distribution with One Degree of Freedom</title><link>https://freshrimpsushi.github.io/en/posts/148/</link><pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/148/</guid><description>Theorem If $X \sim N(\mu,\sigma ^2)$ then $$ V=\left( { X - \mu \over \sigma} \right) ^2 \sim \chi ^2 (1) $$ $N \left( \mu , \sigma^{2} \right)$ is a normal distribution with mean $\mu$ and variance $\sigma^{2}$. $\chi^{2} \left( 1 \right)$ is a chi-squared distribution with degrees of freedom $1$. Description In general, Student&amp;rsquo;s theorem is widely used to generalize this. Anyone studying statistics must always know as a</description></item><item><title>Normal Distribution: Mean and Variance</title><link>https://freshrimpsushi.github.io/en/posts/1661/</link><pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1661/</guid><description>Formula $X \sim N\left( \mu , \sigma^{2} \right)$ Plane $$ E(X) = \mu \\ \Var (X) = \sigma^{2} $$ Derivation Strategy: The normal distribution has a moment-generating function that is easy to differentiate, so we just directly derive it. Moment-generating function of normal distribution: $$ m(t) = \exp \left( \mu t + {{ \sigma^{2} t^{2} } \over { 2 }} \right) \qquad , t \in \mathbb{R} $$ $$ m '</description></item><item><title>Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1645/</link><pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1645/</guid><description>Definition A continuous probability distribution $N \left( \mu,\sigma^{2} \right)$ with a probability density function having a mean of $\mu \in \mathbb{R}$ and a variance of $\sigma^{2} &amp;gt; 0$ is called a normal distribution. $$ f(x) = {{ 1 } \over { \sqrt{2 \pi} \sigma }} \exp \left[ - {{ 1 } \over { 2 }} \left( {{ x - \mu } \over { \sigma }} \right)^{2} \right] \qquad, x \in</description></item><item><title>Derivation of F-distribution from Two Independent Chi-squared Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1643/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1643/</guid><description>Theorem If two random variables $U,V$ are independent and it is assumed that $U \sim \chi^{2} ( r_{1})$, $V \sim \chi^{2} ( r_{2})$ then $$ {{ U / r_{1} } \over { V / r_{2} }} \sim F \left( r_{1} , r_{2} \right) $$ Explanation If two data follow the Chi-squared distribution and are independent, it might be possible to explain their ratio using distribution theory. In statistics in general,</description></item><item><title>Mean and Variance of the F-distribution</title><link>https://freshrimpsushi.github.io/en/posts/1608/</link><pubDate>Sat, 15 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1608/</guid><description>Formulas $X \sim F ( r_{1} , r_{2})$ Surface Area $$ E(X) = {{ r_{2} } \over { r_{2} - 2 }} \qquad , r_{2} &amp;gt; 2 \\ \Var(X) = {{ 2 d_{2}^{2} (d_{1} + d_{2} - 2) } \over { d_{1} (d_{2} -2)^{2} (d_{2} - 4) }} \qquad , r_{2} &amp;gt; 4 $$ Derivation Strategy: Like the chi-squared distribution, the F-distribution also has known moment-generating functions, which we will</description></item><item><title>F-distribution</title><link>https://freshrimpsushi.github.io/en/posts/1606/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1606/</guid><description>Definition 1 The continuous probability distribution $F \left( r_{1} , r_{2} \right)$, which has the following probability density function for degrees of freedom $r_{1}, r_{2} &amp;gt; 0$, is called the F-distribution. $$ f(x) = {{ 1 } \over { B \left( r_{1}/2 , r_{2} / 2 \right) }} \left( {{ r_{1} } \over { r_{2} }} \right)^{r_{1} / 2} x^{r_{1} / 2 - 1} \left( 1 + {{ r_{1} }</description></item><item><title>The Mean and Variance of the Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1601/</link><pubDate>Wed, 05 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1601/</guid><description>Formula If $X \sim \chi^{2} (r)$ then $$ E(X) = r \\ \Var (X) = 2r $$ Derivation Strategy: Fortunately, the moment generating function of the chi-squared distribution is known. Moment of the chi-squared distribution: Let&amp;rsquo;s say $X \sim \chi^{2} (r)$. If $k &amp;gt; - r/ 2$, then there exists the $k$th moment $$ E X^{k} = {{ 2^{k} \Gamma (r/2 + k) } \over { \Gamma (r/2) }} $$</description></item><item><title>Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1600/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1600/</guid><description>Definition 1 The chi-square distribution refers to a continuous probability distribution $\chi^{2} (r)$ with the following probability density function, defined over the degrees of freedom $r &amp;gt; 0$. $$ f(x) = {{ 1 } \over { \Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} \qquad , x \in (0, \infty) $$ $\Gamma$ represents the gamma function. Basic Properties Moment Generating Function [1]: $$m(t) = (1-2t)^{-r/2} \qquad , t &amp;lt; {{ 1 }</description></item><item><title>Derivation of the Beta Distribution from Two Independent Gamma Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1596/</link><pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1596/</guid><description>Theorem If two random variables $X_{1},X_{2}$ are independent and $X_{1} \sim \Gamma ( \alpha_{1} , 1)$, $X_{2} \sim \Gamma ( \alpha_{2} , 1)$, then $$ {{ X_{1} } \over { X_{1} + X_{2} }} \sim \text{beta} \left( \alpha_{1} , \alpha_{2} \right) $$ Explanation If two data points follow the gamma distribution and are independent, it could be possible to explain the ratio of their sum using probability distribution theory. Specifically,</description></item><item><title>Mean and Variance of the Beta Distribution</title><link>https://freshrimpsushi.github.io/en/posts/97/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/97/</guid><description>Formula $X \sim \text{Beta}(\alpha,\beta)$ Surface $$ E(X)={\alpha \over {\alpha + \beta} } \\ \Var (X)={ { \alpha \beta } \over {(\alpha + \beta + 1) { ( \alpha + \beta ) }^2 } } $$ Derivation Strategy: Direct deduction using the definition of the beta distribution and the basic properties of the gamma function. Definition of the Beta Distribution: $\alpha , \beta &amp;gt; 0$ A continuous probability distribution $\text{Beta}(\alpha,\beta)$ with</description></item><item><title>Beta Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1540/</link><pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1540/</guid><description>Definition 1 For $\alpha , \beta &amp;gt; 0$, the continuous probability distribution $\text{Beta}(\alpha,\beta)$, called the beta Distribution, has the following probability density function: $$ f(x) = {{ 1 } \over { B(\alpha,\beta) }} x^{\alpha - 1} (1-x)^{\beta - 1} \qquad , x \in [0,1] $$ $B$ represents the beta function. Basic Properties Moment Generating Function [1]: $$m(t) = 1 + \sum_{k=1}^{\infty} \left( \prod_{r=0}^{k-1} {{ \alpha + r } \over {</description></item><item><title>The Relationship Between the Gamma Distribution and the Chi-Squared Distribution</title><link>https://freshrimpsushi.github.io/en/posts/135/</link><pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/135/</guid><description>Theorem $$ \Gamma \left( { r \over 2 } , 2 \right) \iff \chi ^2 (r) $$ Description The gamma distribution and the chi-square distribution have the following properties. Proof Strategy: It is shown that the moment-generating functions of the two distributions can be represented in the same form. The moment-generating function of the chi-square distribution $\chi ^2 (r)$ is $\displaystyle m_{1}(t) = (1- 2t)^{- {r \over 2} }$, and</description></item><item><title>Relationship between Gamma Distribution and Exponential Distribution</title><link>https://freshrimpsushi.github.io/en/posts/133/</link><pubDate>Sun, 05 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/133/</guid><description>Theorem $$ \Gamma \left(1, { 1 \over \lambda } \right) \iff \text{exp} (\lambda) $$ Description If we think about the intuitive definition of the exponential distribution, it&amp;rsquo;s about the interest in the amount of time it takes for a certain event to occur. If we were to relate this to a discrete probability distribution, the geometric distribution would correspond to this. In this sense, the generalization of the geometric distribution</description></item><item><title>The Relationship between Gamma Distribution and Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/98/</link><pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/98/</guid><description>Theorem For all natural numbers $k$, the following holds. $$ \int_{\mu}^{\infty} { { z^{k-1} e^{-z} } \over { \Gamma (k) } } dz = \sum_{x=0}^{k-1} { { {\mu}^{x} e^{-\mu} } \over {x!} } $$ $\Gamma$ is the gamma function. Description $k, \theta &amp;gt; 0$ For the following probability density function for continuous probability distribution $\Gamma ( k , \theta )$ is called Gamma Distribution. $$ f(x) = {{ 1 }</description></item><item><title>Mean and Variance of Gamma Distribution</title><link>https://freshrimpsushi.github.io/en/posts/132/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/132/</guid><description>Formula Random Variable $X$ is said to be $X \sim \Gamma \left( k , \theta \right)$ with respect to the Gamma Distribution $\Gamma \left( k , \theta \right)$. $$ E(X) = k \theta \\ \Var (X) = k \theta^{2} $$ Derivation Strategy: Directly infer using the definition of the gamma distribution and the basic properties of the gamma function. Use a trick to balance the numerator and denominator of the</description></item><item><title>Gamma Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1517/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1517/</guid><description>Definition 1 For $k, \theta &amp;gt; 0$, it is called the Gamma Distribution which has the following probability density function $\Gamma ( k , \theta )$. $$ f(x) = {{ 1 } \over { \Gamma ( k ) \theta^{k} }} x^{k - 1} e^{ - x / \theta} \qquad , x &amp;gt; 0 $$ $\Gamma$ represents the Gamma function. The probability density function of the Gamma distribution can also be</description></item><item><title>Geometric Distribution's Memorylessness</title><link>https://freshrimpsushi.github.io/en/posts/217/</link><pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/217/</guid><description>Theorem $X \sim \text{Geo} ( m )$ implies $P(X \ge s+ t ,|, X \ge s) = P(X \ge t)$ Explanation The geometric distribution concerns the number of trials needed for an event to occur and is a discrete probability distribution. Thinking of it as a discretization of the exponential distribution, it&amp;rsquo;s natural to consider the memorylessness of the geometric distribution. Here, the memoryless property refers to the characteristic where</description></item><item><title>Exponential Distribution's Memorylessness</title><link>https://freshrimpsushi.github.io/en/posts/216/</link><pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/216/</guid><description>Properties If $X \sim \exp{ ( \lambda ) }$ then $P(X \ge s+ t ,|, X \ge s) = P(X \ge t)$ Explanation The exponential distribution is a continuous probability distribution that focuses on the timeframe within which a certain event occurs. It&amp;rsquo;s easy to assume that it can be applied in predicting lifespans or in insurance. The Memoryless Property means that future events are not influenced by the amount</description></item><item><title>The Relationship between Exponential Distribution and Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/296/</link><pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/296/</guid><description>Theorem When the time it takes for an event to occur is given by $X_{k}$, and if $X_{k} \sim \exp (\lambda)$, then the number of occurrences of an event per unit time is given by $N$, and $\displaystyle N \sim \text{Poi} (\lambda)$</description></item><item><title>Mean and Variance of Exponential Distribution</title><link>https://freshrimpsushi.github.io/en/posts/62/</link><pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/62/</guid><description>Formula $X \sim \exp ( \lambda)$ Surface $$ E(X) = {{ 1 } \over { \lambda }} \\ \Var (X) = {{ 1 } \over { \lambda^{2} }} $$ Proof Strategy: Deduce directly from the definition of the exponential distribution. Definition of the Exponential Distribution: For $\lambda &amp;gt; 0$, continuous probability distribution $\exp ( \lambda)$ with the following probability density function is called the exponential distribution. $$ f(x) = \lambda</description></item><item><title>Exponential Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1510/</link><pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1510/</guid><description>Definition 1 The continuous probability distribution $\exp ( \lambda)$ with the following probability density function, for $\lambda &amp;gt; 0$, is called an Exponential Distribution. $$ f(x) = \lambda e^{-\lambda x} \qquad , x \ge 0 $$ Depending on the book, the parameter might be its reciprocal, $\displaystyle \theta = {{ 1 } \over { \lambda }}$. Basic Properties Moment Generating Function [1]: $$m(t) = {{ \lambda } \over { \lambda</description></item><item><title>Mean and Variance of the Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/61/</link><pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/61/</guid><description>Formulas $X \sim \text{Poi}(\lambda)$ Surface $$ E(X) = \lambda \\ \Var(X) = \lambda $$ Derivation Strategy: Directly deduce from the definition of the Poisson distribution. The trick of splitting factorials and series is important. Definition of Poisson Distribution: For $\lambda &amp;gt; 0$, an discrete probability distribution that has the following probability mass function $\text{Poi} ( \lambda )$ is called a Poisson distribution. $$ p(x) = {{ e^{-\lambda} \lambda^{x} } \over</description></item><item><title>Poisson Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1491/</link><pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1491/</guid><description>Definitions 1 For $\lambda &amp;gt; 0$, we refer to the following probability mass function as the Poisson Distribution that has a discrete probability distribution $\text{Poi} ( \lambda )$. $$ p(x) = {{ e^{-\lambda} \lambda^{x} } \over { x! }} \qquad , x = 0 , 1 , 2, \cdots $$ Basic Properties Moment Generating Function [1]: $$m(t) = \exp \left[ \lambda \left( e^{t} - 1 \right) \right] \qquad , t</description></item><item><title>Mean and Variance of the Negative Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/94/</link><pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/94/</guid><description>Formula $X \sim \text{NB}(r, p)$ Plane $$ E(X) = {{ r (1-p) } \over { p }} \\ \Var(X) = {{ r (1-p) } \over { p^{2} }} $$ Proof Strategy: Use the fact that the negative binomial distribution is a generalization of the geometric distribution. [b] Generalization of Geometric Distribution: If $Y = X_{1} + \cdots + X_{r}$ and $X_{i} \overset{\text{iid}}{\sim} \text{Geo}(p)$ then $Y \sim \text{NB}(r,p)$ Here, the definition</description></item><item><title>Negative Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1489/</link><pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1489/</guid><description>Definition 1 Given $r \in \mathbb{N}$ and $p \in (0,1]$, a discrete probability distribution $\text{NB}(r,p)$ with the following probability mass function is called the Negative Binomial Distribution. $$ p(x) = \binom{r+x-1}{x-1} p^{r}(1-p)^{x} \qquad, x = 0,1,2,\cdots $$ Basic Properties Moment Generating Function [1]: $$m(t) = \left[ {{ p } \over { 1 - (1-p) e^{t} }} \right]^{r} \qquad , t &amp;lt; -\log (1-P)$$ Mean and Variance [2]: If $X \sim</description></item><item><title>Differences Between the Two Definitions of the Geometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/295/</link><pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/295/</guid><description>Description While studying the geometric distribution, the most perplexing and confusing aspect is the differing explanations across textbooks, blogs, and wikis. Some sources mention the mean as $\displaystyle {{1} \over {p}} $, while others use $\displaystyle {{1-p} \over {p}}$. This discrepancy arises because there are two ways to define the geometric distribution. The probability mass function of the geometric distribution $\text{Geo}(p)$ is defined either as $$ p_{1}(x) = p(1-p)^{x-1} ,</description></item><item><title>Mean and Variance of the Geometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/63/</link><pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/63/</guid><description>Formula $X \sim \text{Geo} (p)$ Area $$ E(X) = {{ 1 } \over { p }} \\ \Var(X) = {{ 1-p } \over { p^{2} }} $$ Derivation The mean and variance of the Geometric Distribution are not as easily calculated as one might think. This post introduces two interesting and useful proofs. The discrete probability distribution that has the following probability mass function is called a Geometric Distribution, according</description></item><item><title>Geometric Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1486/</link><pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1486/</guid><description>Definition 1 For $p \in (0,1]$, the discrete probability distribution $\text{Geo}(p)$ that follows the probability mass function as shown above, is called the Geometric Distribution. $$ p(x) = p (1 - p)^{x-1} \qquad , x = 1 , 2, 3, \cdots $$ Take special care with the domain and the formula as there are two definitions used. Basic Properties Moment Generating Function [1]: $$m(t) = {{ p e^{t} } \over</description></item><item><title>Mean and Variance of the Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/60/</link><pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/60/</guid><description>Formulas $\displaystyle X \sim \text{Bin} (n,p)$ Surface $$ E(X)=np \\ \Var(X)=npq $$ Where $q : = 1-p$ stands. Derivation Strategy: Directly unravel the combinations. The expression might be quite messy, but it&amp;rsquo;s entirely digestible at the high school level. It&amp;rsquo;s worth trying out at least once. Upon encountering mathematical statistics, one can prove it using a much shorter and simpler method. Be it the mean or variance, start with the</description></item><item><title>Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1480/</link><pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1480/</guid><description>Definition 1 The discrete probability distribution $\text{Bin}(n,p)$ with the following probability mass function for $n \in \mathbb{N}$ and $p \in [0,1]$ is called the Binomial Distribution. $$ p(x) = \binom{n}{x} p^{x} (1-p)^{n-x} \qquad , x = 0 , 1, \cdots n $$ Basic Properties Moment Generating Function [1]: $$m(t) = \left[ (1-p) + pe^{t} \right]^{n} \qquad , t \in \mathbb{R}$$ Mean and Variance [2]: If $X \sim \text{Bin}(n,p)$ then $$</description></item><item><title>Mean and Variance of the Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/444/</link><pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/444/</guid><description>Formulas $X \sim U[a,b]$ Surface $$ E(X) = {{ a+b } \over { 2 }} \\ \Var(X) = {{ (b-a)^{2} } \over { 12 }} $$ Derivation Strategy: Directly deduce from the definition of the uniform distribution. Definition of Uniform Distribution: For $[a,b] \subset \mathbb{R}$, a continuous probability distribution with the following probability density function]] $U[a,b]$ is called a uniform distribution. $$ f(x) = {{ 1 } \over { b</description></item><item><title>Binomial Distribution</title><link>https://freshrimpsushi.github.io/en/posts/443/</link><pubDate>Sat, 24 Feb 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/443/</guid><description>Definition 1 Continuous For $[a,b] \subset \mathbb{R}$, a continuous probability distribution $U(a,b)$ with the following probability density function is called the Uniform Distribution. $$ f(x) = {{ 1 } \over { b - a }} \qquad , x \in [a,b] $$ Discrete For a finite set $\left\{ x_{k} \right\}_{k=1}^{n}$, a discrete probability distribution with the following probability mass function is called the Uniform Distribution. $$ p \left( x_{k} \right) =</description></item></channel></rss>