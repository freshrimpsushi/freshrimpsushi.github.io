<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistical Test on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%ED%86%B5%EA%B3%84%EC%A0%81%EA%B2%80%EC%A0%95/</link><description>Recent content in Statistical Test on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 12 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%86%B5%EA%B3%84%EC%A0%81%EA%B2%80%EC%A0%95/index.xml" rel="self" type="application/rss+xml"/><item><title>Fitness Test of a group</title><link>https://freshrimpsushi.github.io/en/posts/2491/</link><pubDate>Tue, 12 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2491/</guid><description>Hypothesis Testing 1 Assuming that in a multinomial experiment, where $k$ categories are each theoretically sampled with a probability of $p_{j} &amp;gt; 0$, categorical data obtained from $n$ independent trials are given. The following hypothesis test using the Pearson Chi-square test statistic is called a Goodness of Fit Test. $H_{0}$: The given data has been sampled in accordance with theoretical probabilities. $H_{1}$: The given data has not been sampled in</description></item><item><title>Polynomial Experiments and Contingency Tables</title><link>https://freshrimpsushi.github.io/en/posts/2489/</link><pubDate>Fri, 08 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2489/</guid><description>Definition 1 Multinomial Experiment An experiment that has the following characteristics and has three or more possible outcomes or categories is called a Multinomial Experiment. It consists of $n$ identical trials. Each trial&amp;rsquo;s outcome is one of $k&amp;gt;2$ possible outcomes or categories. Each trial is independent. The probabilities of various outcomes remain constant throughout the trials. Contingency Table When there is information about more than one variable for an element,</description></item><item><title>Pearson Chi-Square Test Statistic</title><link>https://freshrimpsushi.github.io/en/posts/2487/</link><pubDate>Mon, 04 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2487/</guid><description>Definition 1 Let&amp;rsquo;s assume that in a multinomial experiment where $k$ categories are drawn with a probability of $p_{j} &amp;gt; 0$ each, categorical data obtained from $n$ independent trials is given. The frequency $O_{j}$ of data belonging to the $j$th category is referred to as the Observed Cell Count, while the expected value under the null hypothesis of the hypothesis test $E_{j}$ is called the Expected Cell Count. The test</description></item><item><title>Small-Sample Hypothesis Testing for the Difference Between Two Population Means</title><link>https://freshrimpsushi.github.io/en/posts/2476/</link><pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2476/</guid><description>Hypothesis Testing 1 Assume that two independent populations, each following a normal distribution $N \left( \mu_{1} , \sigma_{1}^{2} \right)$ and $N \left( \mu_{2} , \sigma_{2}^{2} \right)$ with $\sigma_{1}^{2} = \sigma^{2} = \sigma_{2}^{2}$, i.e., the population variances are unknown but assumed to be equal. When the samples are small, meaning the number of samples is $n_{1} , n_{2} &amp;lt; 30$, the hypothesis testing for the difference between two population means $D_{0}$</description></item><item><title>Hypothesis Testing for the Population Mean with a Small Sample</title><link>https://freshrimpsushi.github.io/en/posts/2474/</link><pubDate>Wed, 08 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2474/</guid><description>Hypothesis Testing 1 Assuming the population distribution follows a normal distribution $N \left( \mu , \sigma^{2} \right)$ but the population variance $\sigma^{2}$ is unknown. When the sample size is $n &amp;lt; 30$, a small sample, the hypothesis test about the candidate $\mu_{0}$ for the population mean proceeds as follows. $H_{0}$: $\mu = \mu_{0}$. That is, the population mean is $\mu_{0}$. $H_{1}$: $\mu \ne \mu_{0}$. That is, the population mean is</description></item><item><title>Large Sample Hypothesis Testing for the Difference Between Two Population Means</title><link>https://freshrimpsushi.github.io/en/posts/2468/</link><pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2468/</guid><description>Hypothesis Testing 1 Let&amp;rsquo;s say two independent populations follow distributions $\left( \mu_{1} , \sigma_{1}^{2} \right)$ and $\left( \mu_{2} , \sigma_{2}^{2} \right)$, respectively. In the case of a large sample, meaning the sample size is $n_{1} , n_{2} &amp;gt; 30$, the hypothesis test about the difference between the two population means against candidate $D_{0}$ is as follows: $H_{0}$: $\mu_{1} - \mu_{2} = D_{0}$. That is, the difference in population means is</description></item><item><title>Hypothesis Testing for Population Mean</title><link>https://freshrimpsushi.github.io/en/posts/2466/</link><pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2466/</guid><description>Hypothesis Testing 1 Suppose the population distribution follows $\left( \mu , \sigma^{2} \right)$. When the sample is a large sample, i.e., when the number of samples is $n &amp;gt; 30$, the hypothesis testing for the candidate of population mean $\mu_{0}$ is as follows: $H_{0}$: $\mu = \mu_{0}$. That is, the population mean is $\mu_{0}$. $H_{1}$: $\mu \ne \mu_{0}$. That is, the population mean is not $\mu_{0}$. Test Statistic The test</description></item><item><title>Simplified Definition of Hypothesis Testing</title><link>https://freshrimpsushi.github.io/en/posts/2442/</link><pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2442/</guid><description>Definition 1 2 In science, a statistical hypothesis refers to an assumption about a population, and the statistical decision-making process of accepting or rejecting this hypothesis is called statistical hypothesis testing. This process involves two competing hypotheses, where the hypothesis that the researcher wishes to support is called the alternative hypothesis $H_{1}$, and the hypothesis accepted when there is no substantial evidence to claim that the alternative hypothesis is true</description></item><item><title>McLeod-Li Test</title><link>https://freshrimpsushi.github.io/en/posts/1279/</link><pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1279/</guid><description>Hypothesis Testing Given the time series data returns $\left\{ r_{t} \right\}$, let&amp;rsquo;s assume: $H_{0}$: The data does not exhibit autoregressive conditional heteroscedasticity (ARCH) effect at lag $k$. $H_{1}$: The data exhibits ARCH effect at lag $k$. Explanation McLeod-Li Test is used to check for ARCH effects in the given returns. Code Practice Fortunately, in R, the TSA packageâ€™s McLeod.Li.test() function allows for</description></item><item><title>Run-Test</title><link>https://freshrimpsushi.github.io/en/posts/1219/</link><pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1219/</guid><description>Hypothesis Testing Let us denote the ARMA model obtained from time series analysis as $ARMA(p,q)$ to be $M$. $H_{0}$: $M$ is fit. $H_{1}$: $M$ is not fit. Explanation The Ljung-Box Test, also abbreviated as LBQ, is a test for determining the goodness-of-fit of an ARIMA model. In 1970, Box and Pierce proposed the following test statistic $Q$, through the sACF $\hat{r}_{1} , \cdots , \widehat{r}_{k}$ of residuals obtained from ARIMA</description></item><item><title>Durbin-Watson Test</title><link>https://freshrimpsushi.github.io/en/posts/1217/</link><pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1217/</guid><description>Hypothesis Testing After performing a regression analysis, let&amp;rsquo;s assume we are given a residual $\left\{ e_{t} \right\}_{t=1}^{n}$ and express it as $e_{t} := \rho e_{t-1} + \nu_{t}$. $H_{0}$: $\rho = 0$ i.e., the residuals do not exhibit autocorrelation. $H_{1}$: $\rho \ne 0$ i.e., the residuals exhibit autocorrelation. Explanation Empirical Interpretation The Durbin-Watson test is used to check the independence of residuals after regression analysis, determining whether or not the residuals</description></item><item><title>Box-Cox Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1065/</link><pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1065/</guid><description>Buildup $x &amp;gt; 0$ is referred to as a Box-Cox transformation on $g(x) := \begin{cases} \displaystyle {{ x^{\lambda} - 1 } \over { \lambda }} &amp;amp; , \lambda \ne 0 \\ \log x &amp;amp; , \lambda = 0 \end{cases}$. $g$, originally known as Power Transformation, was introduced by Box and Cox, hence it is also called the Box-Cox transformation. The main uses of the Box-Cox transformation are to make data</description></item><item><title>Dickey-Fuller Test</title><link>https://freshrimpsushi.github.io/en/posts/921/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/921/</guid><description>Hypothesis Testing Given that we have time series data eq1. eq2: The data eq1 does not have stationarity. eq4: The data eq1 has stationarity. Explanation The Dickey-Fuller test is a hypothesis test used to determine whether time series data has stationarity or not. If it does not have stationarity, differencing must be used to make the mean constant. It is important to note that this diagnosis only occurs for the</description></item><item><title>Harke-Bera Test</title><link>https://freshrimpsushi.github.io/en/posts/949/</link><pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/949/</guid><description>Hypothesis Testing Given that we have quantitative data $\left\{ x_{i} \right\}_{i = 1}^{n}$. $H_{0}$: Data $\left\{ x_{i} \right\}_{i = 1}^{n}$ follows a normal distribution. $H_{1}$: Data $\left\{ x_{i} \right\}_{i = 1}^{n}$ does not follow a normal distribution. Explanation The Jarque-Bera test is used to test for normality as a hypothesis test, typically to demonstrate the presence of normality. This is one of the rare cases where the acceptance of the</description></item><item><title>Shapiro-Wilk Test</title><link>https://freshrimpsushi.github.io/en/posts/939/</link><pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/939/</guid><description>Hypothesis Testing Given quantitative data $\left\{ x_{i} \right\}_{i = 1}^{n}$. $H_{0}$: Data $\left\{ x_{i} \right\}_{i = 1}^{n}$ follows a normal distribution. $H_{1}$: Data $\left\{ x_{i} \right\}_{i = 1}^{n}$ does not follow a normal distribution. Description The Shapiro-Wilk test is a hypothesis test used to assess the normality of data, usually to demonstrate that normality is present. It&amp;rsquo;s one of the rare occasions where having the null hypothesis accepted matches &amp;rsquo;the</description></item><item><title>Hosmer-Lemeshow Goodness of Fit Test</title><link>https://freshrimpsushi.github.io/en/posts/852/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/852/</guid><description>Hypothesis Testing Let&amp;rsquo;s refer to the model obtained through logistic regression as $M$. $H_{0}$: $M$ is appropriate. $H_{1}$: $M$ is not appropriate. Description The Hosmer-Lemeshow goodness of fit test is a representative hypothesis test used to determine the adequacy of logistic regression models. Although it&amp;rsquo;s a very simple test, the null hypothesis and the alternative hypothesis can be confusing. While itâ€™s true that there is no good</description></item><item><title>F-test for Regression Coefficients</title><link>https://freshrimpsushi.github.io/en/posts/672/</link><pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/672/</guid><description>Hypothesis Testing Assuming in the model diagnostics of the linear multiple regression model, the residuals satisfy linearity, homoscedasticity, independence, and normality. The hypothesis testing for the multiple regression analysis with $n$ observations and $p$ independent variables is as follows: $H_{0}$: $\beta_{1} = \beta_{2} = \cdots = \beta_{p} = 0$ i.e., all independent variables do not have a correlation with the dependent variable. $H_{1}$: At least one among $\beta_{1} , \beta_{2}</description></item><item><title>Regression Coefficient's t-test</title><link>https://freshrimpsushi.github.io/en/posts/654/</link><pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/654/</guid><description>Hypothesis Testing $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ When independent variables of</description></item><item><title>Easy Definition of P-Value or Significance Probability</title><link>https://freshrimpsushi.github.io/en/posts/537/</link><pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/537/</guid><description>Definition 1 The probability of rejecting the null hypothesis in hypothesis testing is called the p-value. Explanation If the p-value is lower than the significance level, it is considered that the null hypothesis has been rejected. A small p-value under the null hypothesis can be understood as &amp;rsquo;the evidence against the null hypothesis is too strong to be attributed to chance.' When terms like power curve or rejection region are</description></item><item><title>Rejection Region and Significance Level</title><link>https://freshrimpsushi.github.io/en/posts/509/</link><pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/509/</guid><description>Definition 1 The error of rejecting the null hypothesis when it is actually true is called a Type I error. The error of failing to reject the null hypothesis when the alternative hypothesis is true is called a Type II error. The maximum probability of committing a Type I error is called the Significance Level. The statistic used for hypothesis testing is called the Test Statistic. The region of the</description></item><item><title>Type I and Type II Errors Difference</title><link>https://freshrimpsushi.github.io/en/posts/508/</link><pubDate>Sat, 07 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/508/</guid><description>Definition In hypothesis testing, an error of not accepting $H_{0}$ when $H_{0}$ is true is called a Type 1 Error, and an error of accepting $H_{0}$ when $H_{0}$ is false is called a Type 2 Error. Explanation We use the term &amp;lsquo;accept&amp;rsquo; for the null hypothesis and &amp;lsquo;reject&amp;rsquo; for the alternative hypothesis. If there is sufficient evidence supporting the null hypothesis, it means &amp;lsquo;rejecting the alternative hypothesis and accepting the</description></item><item><title>How to Set the Null Hypothesis and Alternative Hypothesis</title><link>https://freshrimpsushi.github.io/en/posts/500/</link><pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/500/</guid><description>Description Hypothesis Testing: Null Hypothesis $H_{0}$ vs Alternative Hypothesis $H_{1}$ As of April 2018, some textbooks and Wikipedia describe the null hypothesis as &amp;rsquo;the hypothesis that is initially assumed to be rejected in statistics&amp;rsquo;, and the alternative hypothesis as &amp;rsquo;the hypothesis that one hopes or expects to prove through research&amp;rsquo;. However, if you only study the necessary parts of statistics or lightly know about it, that might be fine, but</description></item></channel></rss>