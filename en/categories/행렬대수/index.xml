<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Matrix Algebra on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%ED%96%89%EB%A0%AC%EB%8C%80%EC%88%98/</link><description>Recent content in Matrix Algebra on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 18 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%96%89%EB%A0%AC%EB%8C%80%EC%88%98/index.xml" rel="self" type="application/rss+xml"/><item><title>Orthogonal Similarity and Orthogonal Diagonalization of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3690/</link><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3690/</guid><description>Definition1 Orthogonal similarity For two square matrices $A$ and $B$, if there exists an orthogonal matrix $P$ that satisfies the following, then $A$ and $B$ are said to be orthogonally similar. $$ A = P^{\mathsf{T}}BP $$ Orthogonal diagonalization A square matrix $A$ is said to be orthogonally diagonalizable (or $P$ is said to orthogonally diagonalize $A$) if it is orthogonally similar to some diagonal matrix $D$. In other words, $A$</description></item><item><title>Diagonalization of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3689/</link><pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3689/</guid><description>Definition1 If a square matrix $A$ is similar to some diagonal matrix $D$ via similarity, we say $A$ is diagonalizable, or that $P$ diagonalizes $A$. In other words, $A$ is called a diagonalizable matrix if there exists an invertible matrix $P$ satisfying the following. $$ A = P^{-1}DP $$ Explanation If the matrix $A$ is diagonalizable, computing its powers becomes very easy. If one computes $A^{k}$ directly, one must perform</description></item><item><title>Inner Product of Matrices (Frobenius Inner Product)</title><link>https://freshrimpsushi.github.io/en/posts/3671/</link><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3671/</guid><description>Definition The inner product or dot product of two matrices $m \times n$, $X = [x_{ij}]$, $Y=[Y]_{ij}$ is defined as follows. $$ X \cdot Y = \braket{X, Y} = \sum_{i=1}^{m}\sum\limits_{j=1}^{n} x_{ij}y_{ij} $$ In the case where the components are complex numbers in a complex matrix, it is defined as follows. $$ X \cdot Y = \braket{X, Y} = \sum_{i=1}^{m}\sum\limits_{j=1}^{n} \overline{x}_{ij}y_{ij} $$ Here, $\overline{x}$ denotes the complex conjugate. Explanation Since the</description></item><item><title>Outer Product of Two Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3669/</link><pubDate>Mon, 07 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3669/</guid><description>Definition The outer product of two column vectors $\mathbf{u} = \begin{bmatrix} u_{1} \\ \vdots \\ u_{n} \end{bmatrix}$ and $\mathbf{v} = \begin{bmatrix} v_{1} \\ \vdots \\ v_{n} \end{bmatrix}$ is defined as follows. $$ \mathbf{u} \otimes \mathbf{v} = \mathbf{u}\mathbf{v}^{\mathsf{T}} = \begin{bmatrix} u_{1} \\ u_{2} \\ \vdots \\ u_{n} \end{bmatrix} \begin{bmatrix} v_{1} &amp;amp; v_{2} &amp;amp; \cdots &amp;amp; v_{n} \end{bmatrix}= \begin{bmatrix} u_{1}v_{1} &amp;amp; u_{1}v_{2} &amp;amp; \cdots &amp;amp; u_{1}v_{n} \\ u_{2}v_{1} &amp;amp; u_{2}v_{2} &amp;amp; \cdots</description></item><item><title>Formula for Matrix Power Form</title><link>https://freshrimpsushi.github.io/en/posts/3668/</link><pubDate>Sat, 05 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3668/</guid><description>Formula For the matrix $X = [x_{ij}] \in \mathbb{R}^{n \times n}$, the following holds. $$ [XX]_{ij} = \sum_{k=1}^{n} x_{ik} x_{kj} $$ $$ XX = X^{2} = \begin{bmatrix} \sum\limits_{k=1}^{n} x_{1k} x_{k1} &amp;amp; \cdots &amp;amp; \sum\limits_{k=1}^{n} x_{1k} x_{kn} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \sum\limits_{k=1}^{n} x_{nk} x_{k1} &amp;amp; \cdots &amp;amp; \sum\limits_{k=1}^{n} x_{nk} x_{kn} \end{bmatrix} $$ If $X$ is a symmetric matrix, $$ X^{2} = \begin{bmatrix} \sum\limits_{k=1}^{n} (x_{1k})^{2} &amp;amp; \cdots &amp;amp; \sum\limits_{k=1}^{n}</description></item><item><title>Proof of the Subadditivity of Matrix Rank: rank(A+B) ≤ rankA + rankB</title><link>https://freshrimpsushi.github.io/en/posts/2653/</link><pubDate>Fri, 23 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2653/</guid><description>Theorem The rank of a matrix possesses a quasi-additive property. In other words, for two matrices $A, B$, the following holds. $$ \rank \left( A + B \right) \le \rank A + \rank B $$ Explanation This theorem is used in the proof of Cochran&amp;rsquo;s theorem. Proof 1 Bases for Row Space, Column Space, and Null Space: (a1) Two row equivalent matrices have the same row space, meaning that elementary</description></item><item><title>Properties of the Main Diagonal Elements of Positive Definite Matrices</title><link>https://freshrimpsushi.github.io/en/posts/2649/</link><pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2649/</guid><description>Theorem Suppose a positive definite matrix is given as $A = \left( a_{ij} \right) \in \mathbb{C}^{n \times n}$. Sign of the Main Diagonal Elements The sign of the main diagonal elements in $A$, denoted as $a_{ii}$, is the same as the sign of $A$. If $A$ is positive definite, then $a_{ii} &amp;gt; 0$ If $A$ is positive semidefinite, then $a_{ii} \ge 0$ If $A$ is negative definite, then $a_{ii} &amp;lt;</description></item><item><title>Prove that the trace of powers of a diagonalizable matrix is equal to the sum of powers of its eigenvalues</title><link>https://freshrimpsushi.github.io/en/posts/2645/</link><pubDate>Wed, 07 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2645/</guid><description>Theorem Suppose a diagonalizable matrix $A \in \mathbb{C}^{n \times n}$ and a natural number $k \in \mathbb{N}$ are given. Let the eigenvalue of $A$ be $\lambda_{1} , \cdots , \lambda_{n}$, then the following holds. $$ \operatorname{tr} A^{k} = \sum_{i=1}^{n} \lambda_{i}^{k} $$ Here, $\operatorname{tr}$ is the trace. Explanation Although it is not quite a corollary, it is useful to note that when $A \in \mathbb{R}^{n \times n}$ is a symmetric matrix,</description></item><item><title>Proof that if all eigenvalues of a symmetric real matrix are either 0 or 1, it is an idempotent matrix</title><link>https://freshrimpsushi.github.io/en/posts/2643/</link><pubDate>Sat, 03 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2643/</guid><description>Theorem If all the eigenvalues of the symmetric matrix $A \in \mathbb{R}^{n \times n}$ are either $0$ or $1$, then $A$ is an idempotent matrix. Explanation This lemma is used in the proof of the equivalence conditions for the chi-square-ness of quadratic forms of normally distributed random vectors and the proof of Cochran’s theorem. The converse does not hold. Proof Spectral Theory:</description></item><item><title>Proof that the eigenvalues of an idempotent matrix are either 0 or 1</title><link>https://freshrimpsushi.github.io/en/posts/2641/</link><pubDate>Tue, 29 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2641/</guid><description>Theorem The eigenvalues of an idempotent matrix are only $0$ or $1$. Explanation This lemma is used in the proof of the equivalence condition for the chi-squared property of a quadratic form of a normally distributed random vector. For the converse of this theorem to hold, the given idempotent matrix must be a real symmetric matrix. Proof 1 Let $A$ be an idempotent matrix, in other words, $A^{2} = A$.</description></item><item><title>Toeplitz Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2634/</link><pubDate>Tue, 15 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2634/</guid><description>Definition A component $\left( A \right)_{ij}$ of a matrix $A \in \mathbb{R}^{m \times n}$ is said to satisfy $\left( A \right)_{i, j} = \left( A \right)_{i+1, j+1}$ for all $i, j$ if it is called a Toeplitz matrix. In other words, a Toeplitz matrix is a matrix where all elements along a specific diagonal are the same. $$ A = \begin{bmatrix} a_{0} &amp;amp; a_{-1} &amp;amp; a_{-2} &amp;amp; \cdots &amp;amp; a_{-n+1}</description></item><item><title>Affine Transformation</title><link>https://freshrimpsushi.github.io/en/posts/365/</link><pubDate>Sun, 02 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/365/</guid><description>Definition Simple Definition Suppose a matrix $A$ and a vector $\mathbf{b}$ are given. The transformation that multiplies the matrix $A$ to the vector $\mathbf{x}$ and adds $\mathbf{b}$ is called an affine transformation. $$ \mathbf{x} \mapsto A \mathbf{x} + \mathbf{b} $$ Complex Definition 1 If $f : V \to V$ defined on a vector space $V$ satisfies the following condition for any scalar $\lambda$, then $f$ is called an affine transformation.</description></item><item><title>Definition of a Normal Matrix</title><link>https://freshrimpsushi.github.io/en/posts/286/</link><pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/286/</guid><description>Definition A square matrix $A \in \mathbb{C}^{n \times n}$ is called a normal matrix ▷normal matrix◁ if it satisfies the following condition: $$ A A^{\ast} = A^{\ast} A $$ Here, $X^{\ast}$ is the conjugate transpose of the matrix $X$. Properties Assume $A$ is a square matrix. The necessary and sufficient condition for the triangular matrix $A$ to be</description></item><item><title>Positive Semidefinite Matrices and Their Real Powers</title><link>https://freshrimpsushi.github.io/en/posts/69/</link><pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/69/</guid><description>Definition For a positive-definite matrix $A \ge 0$ and a real number $t \in \mathbb{R}$, the $t$-power of $A$ is defined as follows. $$ A^{t} := \exp \left( t \log A \right) $$ Here, $\exp$ and $\log$ are respectively the matrix exponential and matrix logarithm. Explanation Generally, the power of a matrix $A^{t}$ is defined as taking the product of the matrix $t$ times for a natural number $t \in</description></item><item><title>Matrix Logarithm</title><link>https://freshrimpsushi.github.io/en/posts/67/</link><pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/67/</guid><description>Definition Aiming to generalize the exponential function $\exp$ and the logarithmic function $\log$ to matrices. Matrix Exponential The generalization of the exponential function to matrices $\exp : \mathbb{C}^{n \times n} \to \mathbb{C}^{n \times n}$ is defined for a matrix $A \in \mathbb{C}^{n \times n}$ as follows: $$ \exp A := \sum_{k=0}^{\infty} \frac{A^{k}}{k!} $$ The expression $\exp A$ can also be simply written as $e^{A}$, and it is called the matrix</description></item><item><title>Hermitian Matrix's Loewner Order</title><link>https://freshrimpsushi.github.io/en/posts/26/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/26/</guid><description>Definition Loewner Order Let&amp;rsquo;s assume two matrices $A, B \in \mathbb{C}^{n \times n}$ are Hermitian matrices. If $\left( A - B \right)$ is positive semidefinite, it is denoted as $A \ge B$, and if $\left( A - B \right)$ is positive definite, it is denoted as $A &amp;gt; B$. This kind of partial order $\ge$, $&amp;gt;$ are called the Loewner order. Explanation Unlike commonly considered scalars, and indeed even with</description></item><item><title>Hermitian Matrix Spaces and Convex Cones of Positive Semidefinite Matrices</title><link>https://freshrimpsushi.github.io/en/posts/19/</link><pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/19/</guid><description>Definition Let&amp;rsquo;s denote as $n \in \mathbb{N}$. Hermitian Matrix Space The set of Hermitian matrices of size $n \times n$ is denoted as follows. $$ \mathbb{H}_{n} := \left\{ A \in \mathbb{C}^{n \times n} : A = A^{\ast} \right\} $$ Positive Definite Matrix Set The set of positive definite matrices of size $n \times n$ is denoted as $\mathbb{P}_{n}$. Theorem $\mathbb{H}_{n}$ is a Vector Space [1]: With respect to the scalar</description></item><item><title>Positive Semidefinite Matrices and the Proof of the Extended Cauchy-Schwarz Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2602/</link><pubDate>Thu, 25 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2602/</guid><description>Theorem 1 For any two vectors $\mathbf{b}, \mathbf{d} \in \mathbf{R}^{p}$ and a positive definite matrix $A \in \mathbf{R}^{p \times p}$, the following inequality holds. $$ \left( \mathbf{b}^{T} \mathbf{d} \right)^{2} \le \left( \mathbf{b}^{T} A \mathbf{b} \right) \left( \mathbf{d}^{T} A^{-1} \mathbf{d} \right) $$ The equivalence conditions for this to be an equality are represented as either $\mathbf{b} = c A^{-1} \mathbf{d}$ or $\mathbf{d} = c A \mathbf{b}$ for some constant $c \in</description></item><item><title>PLU Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/2/</link><pubDate>Thu, 11 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2/</guid><description>Definition 1 2 For permutation matrices $P^{T}$ and invertible matrices $A \in \mathbb{R}^{n \times n}$, their matrix product $P^{T} A$&amp;rsquo;s LU decomposition is referred to as the PLU Decomposition $A$. Since $P$ is a permutation matrix, it&amp;rsquo;s an orthogonal matrix, i.e., $P^{-1} = P^{T}$, and thus can be represented as follows. $$ P^{T} A = LU \iff A = PLU $$ Description Algorithm of LU decomposition: Let&amp;rsquo;s assume $(a_{ij}) \in</description></item><item><title>Permutation Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1/</link><pubDate>Tue, 09 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1/</guid><description>Definition 1 $P \in \mathbb{R}^{n \times n}$ in which only one component in each row is $1$ and the rest are $0$ is called a Permutation Matrix. Basic Properties Orthogonality All permutation matrices are orthogonal matrices: $$P^{-1} = P^{T}$$ Sparseness For sufficiently large $n$, $P \in \mathbb{R}^{n \times n}$ is a sparse matrix. Explanation The Permutation Matrix gives a permutation of rows and columns through matrix multiplication. The following example</description></item><item><title>Inverse and Square Root of Positive Definite Matrices</title><link>https://freshrimpsushi.github.io/en/posts/2592/</link><pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2592/</guid><description>Formulas 1 Let&amp;rsquo;s say the eigenpairs $\left\{ \left( \lambda_{k} , e_{k} \right) \right\}_{k=1}^{n}$ of a positive definite matrix $A$ are arranged in order $\lambda_{1} &amp;gt; \cdots &amp;gt; \lambda_{n} &amp;gt; 0$. Regarding the orthogonal matrix $P = \begin{bmatrix} e_{1} &amp;amp; \cdots &amp;amp; e_{n} \end{bmatrix} \in \mathbb{R}^{n \times n}$ and the diagonal matrix $\Lambda = \diag \left( \lambda_{1} , \cdots , \lambda_{n} \right)$, the inverse matrix $A^{-1}$ and the square root matrix</description></item><item><title>Spectral Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/2590/</link><pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2590/</guid><description>Definition 1 In spectral theory, the statement that $A$ is a Hermitian matrix is equivalent to it being unitarily diagonalizable: $$ A = A^{\ast} \iff A = Q \Lambda Q^{\ast} $$ The term $A = Q \Lambda Q^{\ast}$ as mentioned in spectral theory is referred to as Spectral Decomposition, and is expressed as a series of eigenpairs $\left\{ \left( \lambda_{k} , e_{k} \right) \right\}_{k=1}^{n}$. $$ A = \sum_{k=1}^{n} \lambda_{k} e_{k}</description></item><item><title>Toeplitz Matrices are Hermitian Matrices</title><link>https://freshrimpsushi.github.io/en/posts/2588/</link><pubDate>Sun, 23 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2588/</guid><description>Proof A positive-definite matrix $A \in \mathbb{C}^{n \times n}$ is a Hermitian matrix. Naturally, a positive semi-definite matrix is also a Hermitian matrix. Proof 1 $$ \mathbf{x}^{\ast} A \mathbf{x} = \lambda $$ If $A$ is a positive-definite matrix, for all $\mathbf{x} \in \mathbb{C}^{n}$, the quadratic form $\mathbf{x}^{\ast} A \mathbf{x}$ is expressed as some real number $\lambda \in \mathbb{R}$ as above. Taking the conjugate transpose on both sides, the complex conjugate</description></item><item><title>The Spectrum and Decomposition Set of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3614/</link><pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3614/</guid><description>Definition1 The set $\sigma (A)$ of all eigenvalues of a square matrix $A$ is called the spectrum of $A$. The complement set $\rho (A) = \mathbb{C} \setminus \sigma (A)$ of the spectrum is called the resolvent set of $A$. Explanation $$ Ax = \lambda x $$ Consider the eigenvalue equation for matrix $A$. A vector $x$ that satisfies the above equation is called an eigenvector, and the constant $\lambda$ is</description></item><item><title>Hankel Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2561/</link><pubDate>Tue, 30 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2561/</guid><description>Definition $$ H = \begin{bmatrix} h_{11} &amp;amp; h_{12} &amp;amp; h_{13} &amp;amp; \cdots &amp;amp; h_{1n} \\ h_{21} &amp;amp; h_{22} &amp;amp; h_{23} &amp;amp; \cdots &amp;amp; h_{2n} \\ h_{31} &amp;amp; h_{32} &amp;amp; h_{33} &amp;amp; \cdots &amp;amp; h_{3n} \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ h_{m1} &amp;amp; h_{m2} &amp;amp; h_{m3} &amp;amp; \cdots &amp;amp; h_{mn} \end{bmatrix} $$ A given matrix $H = \left( h_{ij} \right) \in \mathbb{R}^{m \times n}$ is called</description></item><item><title>3D Rotation Transformation Matrix: Roll, Pitch, Yaw</title><link>https://freshrimpsushi.github.io/en/posts/2557/</link><pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2557/</guid><description>Definition 1 In a 3-dimensional space $\mathbb{R}^{3}$, a matrix $R_{x}$, $R_{y}$, $R_{z}$ that rotates a vector around the $x$ axis, $y$ axis, and $z$ axis in a counterclockwise direction by $\theta$ is as follows. $$ \begin{align*} R_{x} =&amp;amp; \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; \cos \theta &amp;amp; - \sin \theta \\ 0 &amp;amp; \sin \theta &amp;amp; \cos \theta \end{bmatrix} \\ R_{y} =&amp;amp; \begin{bmatrix} \cos \theta &amp;amp; 0</description></item><item><title>Inverse Matrix of X^T X: Necessary and Sufficient Conditions</title><link>https://freshrimpsushi.github.io/en/posts/2533/</link><pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2533/</guid><description>Theorem Suppose that the matrix $X \in \mathbb{R}^{m \times n}$ is given and $m \ge n$, then the following holds: $$ \exists \left( X^{T} X \right)^{-1} \iff \text{rank} X = n $$ In other words, the necessary and sufficient condition for the inverse matrix of $X^{T} X$ to exist is that $X$ has full rank. $X^{T}$ is the transpose of $X$. Explanation The reason this fact is important is that</description></item><item><title>Finding the Inverse Matrix Using Gaussian Elimination Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/2517/</link><pubDate>Fri, 02 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2517/</guid><description>Algorithm Input A invertible matrix $M \in \mathbb{R}^{n \times n}$ is given. Step 1. Initialization Create an identity matrix $X$ of the same size as $M$. Step 2. Echelon Form Through Gaussian elimination, transform $M$ into echelon form. Normally, it would be sufficient for it to be in the form of an upper triangular matrix, but in the algorithm for calculating the inverse matrix, we go as far as calculating</description></item><item><title>Square Root Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2515/</link><pubDate>Mon, 29 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2515/</guid><description>Definition 1 A matrix $A$ is called the Square Root Matrix of $B$ if it satisfies the following condition and is denoted by $\sqrt{A} := B$. $$ B^{2} = A $$ Description The concept of square roots becomes more interesting in the context of matrices. For instance, $$ A = \begin{bmatrix} 2 &amp;amp; 2 \\ 2 &amp;amp; 2 \end{bmatrix} $$ if there is a matrix like this, its square root</description></item><item><title>Sparse Matrices</title><link>https://freshrimpsushi.github.io/en/posts/2513/</link><pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2513/</guid><description>Definition In natural language, sparse refers to being thin or scarce in a way that a value is considered virtually non-existent if it is $0$. Sparsity indicates the degree to which something is made up of such $0$ values. Sparse Matrix A matrix whose elements are mostly $0$ is referred to as a Sparse Matrix. $S$-Sparse 1 Even if there are many values, when there are only $S \ll d$</description></item><item><title>Determinant of a Triangular Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2485/</link><pubDate>Thu, 30 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2485/</guid><description>English Translation Theorem The determinant of a triangular matrix is expressed as the product of its diagonal elements. Proof 1 Without loss of generality, assume that $A$ is an upper triangular matrix. $$ A := \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} &amp;amp; \cdots &amp;amp; a_{1n} \\ 0 &amp;amp; a_{22} &amp;amp; a_{23} &amp;amp; \cdots &amp;amp; a_{2n} \\ 0 &amp;amp; 0 &amp;amp; a_{33} &amp;amp; \cdots &amp;amp; a_{3n} \\ \vdots &amp;amp; \vdots &amp;amp;</description></item><item><title>Eigenvalues and Eigenvectors</title><link>https://freshrimpsushi.github.io/en/posts/2483/</link><pubDate>Sun, 26 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2483/</guid><description>Definition Let&amp;rsquo;s assume that a square matrix $A_{n \times n} = (a_{ij})$ is given. The determinant $M_{ij}$ of the matrix obtained by removing the $i$-th row and the $j$-th row from $A$ is called the minor. $C_{ij} := (-1)^{i + j} M_{ij}$ is referred to as the cofactor. The matrix of cofactors $C = \left( C_{ij} \right)$ and its transpose $C^{T}$ is called the classical adjugate matrix, represented by $\operatorname{adj}</description></item><item><title>Derivation of the Sherman-Morrison Formula</title><link>https://freshrimpsushi.github.io/en/posts/2481/</link><pubDate>Wed, 22 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2481/</guid><description>Theorem Invertible matrix for $A \in \mathbb{R}^{n \times n}$ and $\mathbf{u} , \mathbf{v} \in \mathbb{R}^{n}$, the following holds true. $$ 1 + \mathbf{v}^{T} A^{-1} \mathbf{u} \ne 0 \iff \exists : \left( A + \mathbf{u} \mathbf{v}^{T} \right)^{-1} $$ Sherman-Morrison Formula When $\left( A + \mathbf{u} \mathbf{v}^{T} \right)^{-1}$ exists, the concrete formula is as follows. $$ \left( A + \mathbf{u} \mathbf{v}^{T} \right)^{-1} = A^{-1} - {{ A^{-1} \mathbf{u} \mathbf{v}^{T} A^{-1} } \over</description></item><item><title>Proof of the Matrix Determinant Lemma</title><link>https://freshrimpsushi.github.io/en/posts/2479/</link><pubDate>Sat, 18 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2479/</guid><description>Theorem An invertible matrix satisfies the following for $A \in \mathbb{R}^{n \times n}$ and $\mathbf{u} , \mathbf{v} \in \mathbb{R}^{n}$. $$ \det \left( A + \mathbf{u} \mathbf{v}^{T} \right) = \left( 1 + \mathbf{v}^{T} A^{-1} \mathbf{u} \right) \det A $$ Particularly, for the classical adjoint matrix $\operatorname{adj} (A) = A^{-1} \det A$, it can be represented as follows. $$ \det \left( A + \mathbf{u} \mathbf{v}^{T} \right) = \det A + \mathbf{v}^{T} \operatorname{adj}</description></item><item><title>Hadamard Product of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3436/</link><pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3436/</guid><description>Definition The Hadamard product $A \odot B$ of two matrices $A, B \in M_{m \times n}$ is defined as follows. $$ A \odot B = \begin{bmatrix} a_{11} &amp;amp; \cdots &amp;amp; a_{1n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} \odot\begin{bmatrix} b_{11} &amp;amp; \cdots &amp;amp; b_{1n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ b_{m1} &amp;amp; \cdots &amp;amp; b_{mn} \end{bmatrix} := \begin{bmatrix} a_{11}b_{11} &amp;amp; \cdots &amp;amp; a_{1n}b_{1n}</description></item><item><title>Kronecker Product of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3418/</link><pubDate>Fri, 19 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3418/</guid><description>Definition1 The Kronecker product of two matrices $A = [a_{ij}] \in M_{m \times n}$, $B \in M_{p \times q}$ is defined as follows. $$ A \otimes B := \begin{bmatrix} a_{11} B &amp;amp; \cdots &amp;amp; a_{1n} B \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} B &amp;amp; \cdots &amp;amp; a_{mn} B \end{bmatrix} \in M_{mp \times nq} $$ Explanation The matrix representation of the tensor product of two linear transformations is defined</description></item><item><title>Direct Sum of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3395/</link><pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3395/</guid><description>Definition1 The direct sum of two matrices $B \in M_{m\times n}$, $C \in M_{p\times q}$ is defined as matrix $A$ of the following $(m+p) \times (n+q)$, and is denoted by $B \oplus C$. $$ A = B \oplus C := \begin{bmatrix} b_{11} &amp;amp; \cdots &amp;amp; b_{1n} &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ b_{m1} &amp;amp; \cdots &amp;amp; b_{mn}</description></item><item><title>Zero Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3394/</link><pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3394/</guid><description>Definition The matrix of size $m\times n$ with all elements being $0$ is called a zero matrix, and is denoted as $O_{m\times n}$ or simply as $O$. Description Other notations include $Z_{m \times n}$, $Z$, $\mathbf{0}_{m\times n}$, or $\mathbf{0}$. It is better to write the number $0$ in bold to avoid confusion (actually, it is better just to use $O$). A zero matrix is the identity element for matrix addition.</description></item><item><title>Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3388/</link><pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3388/</guid><description>Definition1 A matrix whose elements are all $1$ is called an $1$ matrix. Explanation The sushi restaurant owner majored in physics, so he preferred to call it an $1$ matrix because &amp;ldquo;work matrix&amp;rdquo; sounded technical. There doesn&amp;rsquo;t seem to be a rigid convention, and absolutely $I$, $E$ or $O$ should be avoided. If notation is necessary, perhaps $\mathbf{1}_{m\times n}$ would be better. Conceptually, it is almost never needed when studying</description></item><item><title>Orthogonal Triangular Matrices are Nilpotent</title><link>https://freshrimpsushi.github.io/en/posts/3385/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3385/</guid><description>Theorem1 $n \times n$ Upper triangular matrix $A$ is a nilpotent matrix. Explanation The converse is not true. A simple counterexample is when $A = \begin{bmatrix} 1 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix}$, $$ A^{2} = \begin{bmatrix} 1 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix}\begin{bmatrix} 1 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix} = \begin{bmatrix} 0 &amp;amp; 0 \\ 0 &amp;amp; 0 \end{bmatrix} $$ The method of proof is</description></item><item><title>Row-wise and Column-wise Scalar Multiplication of Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2319/</link><pubDate>Mon, 02 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2319/</guid><description>Theorem Given a diagonal matrix $D := \text{diag} \left( d_{1} , \cdots , d_{n} \right)$ and a matrix $D := \text{diag} \left( d_{1} , \cdots , d_{n} \right)$, the following holds. $$ \begin{align*} D A =&amp;amp; \begin{bmatrix} d_{1} a_{11} &amp;amp; d_{1} a_{12} &amp;amp; \cdots &amp;amp; d_{1} a_{1n} \\ d_{2} a_{21} &amp;amp; d_{2} a_{22} &amp;amp; \cdots &amp;amp; d_{2} a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ d_{n} a_{n1} &amp;amp;</description></item><item><title>Block Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3323/</link><pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3323/</guid><description>Definition Let&amp;rsquo;s say $A$ is a matrix $m \times n$. $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \\ \end{bmatrix} $$ Consider any vertical and horizontal lines that cut the matrix as follows. $$ A = \left[ \begin{array}{cc|ccc|c|} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}</description></item><item><title>Definition of Spectral Radius</title><link>https://freshrimpsushi.github.io/en/posts/2292/</link><pubDate>Wed, 09 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2292/</guid><description>Definition The eigenvalue $\lambda_{1} , \cdots , \lambda_{n}$ of the matrix $A \in \mathbb{C}^{n \times n}$ with the largest modulus $\rho (A) = \argmax_{\lambda} \left| \lambda \right|$ is called the Spectral Radius. Explanation Usually, the spectra of a matrix refers to both its eigenvalues and eigenvectors collectively, and in the absence of a matrix, it denotes the matrix or the abstractly defined eigenvalues and eigenvectors algebraically1. Gantmacher. (2000). The Theory</description></item><item><title>Nilpotent Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3307/</link><pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3307/</guid><description>Definition1 $n \times n$ For a matrix $A$, if there exists a positive integer $k$ that satisfies $A^{k} = O$, then we call $A$ a nilpotent matrix. In this case, $O$ is the zero matrix of $n \times n$. Explanation &amp;ldquo;Nil&amp;rdquo; means &amp;lsquo;zero&amp;rsquo; or &amp;rsquo;none.&amp;rsquo; &amp;ldquo;Potent&amp;rdquo; means &amp;lsquo;powerful,&amp;rsquo; and is the root of the word &amp;ldquo;potential.&amp;rdquo; Therefore, the term &amp;ldquo;nilpotent&amp;rdquo; can be understood as &amp;lsquo;having the potential/power to become $0$.&amp;rsquo;</description></item><item><title>Triangular Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3305/</link><pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3305/</guid><description>Definition1 A matrix $A = [a_{ij}]$ with all elements above the main diagonal being $0$ is called a lower triangular matrix. $$ A \text{ is lower triangluar matrix if } a_{ij} = 0 \text{ whenever } i \lt j $$ A matrix $A = [a_{ij}]$ with all elements below the main diagonal being $0$ is called an upper triangular matrix. $$ A \text{ is upper triangluar matrix if } a_{ij}</description></item><item><title>Perron-Frobenius Theorem</title><link>https://freshrimpsushi.github.io/en/posts/2245/</link><pubDate>Sun, 07 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2245/</guid><description>Theorem Auxiliary Definitions To apply a permutation on a matrix literally means to change the order of rows and columns. A matrix $A$ is reducible if, after applying a permutation to $\tilde{A}$, it can be represented in the form of a Jordan block with respect to some square matrices $B, D$, a zero matrix $O$, and matrix $C$1. If not, it is said to be irreducible. $$ \tilde{A} = \begin{bmatrix}</description></item><item><title>Properties of Full Rank Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3271/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3271/</guid><description>Theorem1 Let&amp;rsquo;s refer to $A$ as matrix $m \times n$. Then, the necessary and sufficient condition for $A$ to have a full rank is for $A^{T}A$ to be an invertible matrix. Proof $(\Longrightarrow)$ Assume that $A$ has a full rank. Since $A^{T}A$ is a square matrix $n \times n$, showing that the linear system $A^{T}A \mathbf{x} = \mathbf{0}$ only has the trivial solution, according to the equivocal condition of being</description></item><item><title>Fundamental Spaces of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3269/</link><pubDate>Sat, 23 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3269/</guid><description>Explanation1 Let&amp;rsquo;s assume that the matrix $A$ is given. Then, we can think of the following 6 spaces for $A$. Row space of $A$, Row space of $A^{T}$ Column space of $A$, Column space of $A^{T}$ Null space of $A$, Null space of $A^{T}$ However, since the row vectors of $A$ are the column vectors of $A^{T}$, and the column vectors of $A$ are the row vectors of $A^{T}$, the</description></item><item><title>Supersaturated and Undersaturated Systems</title><link>https://freshrimpsushi.github.io/en/posts/3265/</link><pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3265/</guid><description>Definition1 Consider the following linear system for matrix $A$: $$ A \mathbf{x} = \mathbf{b} $$ If $m \gt n$, then there are more constraints than unknowns, and such a linear system is called an overdetermined system. If $m \lt n$, then there are less constraints than unknowns, and such a linear system is called an underdetermined system. Theorem 1 Consider the linear system for matrix $A$ with rank $r$ of</description></item><item><title>Frobenius Norm</title><link>https://freshrimpsushi.github.io/en/posts/2129/</link><pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2129/</guid><description>Definition 1 For a matrix $A = \left( a_{ij} \right) \in \mathbb{C}^{m \times n}$, the matrix norm $\left\| \cdot \right\|_{F}$ is defined as follows and is called the Frobenius norm. $$ \left\| A \right\|_{F} = \sqrt{ \sum_{ij} \left| a_{ij} \right|^{2} } = \sqrt{ \text{Tr} \left( A^{\ast} A \right) } $$ Explanation The Frobenius norm is also called the Hilbert–Schm</description></item><item><title>Rotation Transformation, Rotation Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3084/</link><pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3084/</guid><description>Definition In the two-dimensional plane $\mathbb{R}^{2}$, the transformation that rotates an arbitrary vector counterclockwise by $\theta$ is given by $$ \begin{bmatrix} x^{\prime} \\ y^{\prime} \end{bmatrix} = \begin{bmatrix} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} $$ Explanation The matrix $\begin{bmatrix} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \end{bmatrix}$ is called the rotation matrix or the rotation transformation.</description></item><item><title>Pseudo Inverse Matrix</title><link>https://freshrimpsushi.github.io/en/posts/2011/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2011/</guid><description>Overview The Pseudoinverse Matrix generalizes the concept of an inverse matrix to matrices $A \in \mathbb{R}^{m \times n}$ that are not square matrices because their rows and columns are not of the same size. It refers to a matrix that acts as an &amp;rsquo;effective&amp;rsquo; inverse matrix for them. If a matrix transformation $T_{A} : \mathcal{N} (A) \to \mathcal{C} (A)$ satisfies $$ T_{A} \mathbf{x} = A \mathbf{x} $$ for all $\mathbf{x}</description></item><item><title>Basis of Row Space, Column Space, and Null Space</title><link>https://freshrimpsushi.github.io/en/posts/3027/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3027/</guid><description>Overview1 The concepts such as row space, column space, null space were created to solve linear systems $A \mathbf{x} = \mathbf{b}$. Linear systems can be solved through basic row operations, and indeed, the row space and null space are invariant under basic row operations, indicating their relationship with linear systems. It is important to note here that the column space is not invariant under basic row operations. Theorem 1 (a1)</description></item><item><title>Inverse Matrices and Systems of Linear Equations</title><link>https://freshrimpsushi.github.io/en/posts/3024/</link><pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3024/</guid><description>Theorem: Equivalent Conditions for an Invertible Matrix1 Let $A$ be a square matrix of size $n\times n$. Then the following statements are equivalent. (a) $A$ is an invertible matrix. (e) $A\mathbf{x}=\mathbf{b}$ has a solution for all matrices $n\times 1$ of size $\mathbf{b}$. (f) $A\mathbf{x}=\mathbf{b}$ has exactly one solution for all matrices $n\times 1$ of size $\mathbf{b}$. That is, $\mathbf{x}=A^{-1}\mathbf{b}$ holds. Description (e) and (f) being equivalent means that if the</description></item><item><title>Basic Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3022/</link><pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3022/</guid><description>Definition[^1] If two matrices $A$ and $B$ can be derived from each other through basic row operations, these matrices are said to be row equivalent. A matrix that can be obtained by performing a single basic row operation on an identity matrix is called an elementary matrix, generally denoted by $E$.</description></item><item><title>Simultaneous Homogeneous Linear Equations</title><link>https://freshrimpsushi.github.io/en/posts/3020/</link><pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3020/</guid><description>Definition1 In a linear system, if the constant terms are all $0$, it is called homogeneous. $$ \begin{align*} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= 0 \\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= 0 \\ &amp;amp;\vdots \\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= 0 \end{align*} $$ Unlike general linear systems, every homogeneous linear system always has a solution because if the constant terms are $0$,</description></item><item><title>Gaussian-Jordan Elimination</title><link>https://freshrimpsushi.github.io/en/posts/3019/</link><pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3019/</guid><description>Definition1 An augmented matrix is said to be in echelon form if it satisfies the following conditions: In rows that have a non-zero element, the first non-zero number is a 1, referred to as the leading 1. Rows where all elements are zero are placed at the bottom. For consecutive rows that contain non-zero elements, the leading 1 in the upper row must be to the left of the leading</description></item><item><title>Matrix Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3025/</link><pubDate>Wed, 17 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3025/</guid><description>Definition A function from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$ is called a matrix transformation with respect to the matrix $m \times n$ $A$ if it maps as follows, and is denoted as $T_{A} : \mathbb{R}^{n} \to \mathbb{R}^{m}$. $$ \mathbf{w} = T_{A} (\mathbf{x}) = A\mathbf{x}\quad \left( \mathbf{x} \in \mathbb{R}^{n}, \mathbf{w} \in \mathbb{R}^{m} \right) $$ It can also be represented as $\mathbf{x} \overset{T_{A}}{\to} \mathbf{w}$. This mapping can be represented in matrix form as follows.</description></item><item><title>Matrix Similarity</title><link>https://freshrimpsushi.github.io/en/posts/3016/</link><pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3016/</guid><description>Definition1 A square matrix $A$, $B$ and an invertible matrix $P$ are said to be $B$ is similar to $A$ if the following equation holds. $$ B = P^{-1} A P $$ Description The reason why it is called similar is because similar matrices share many important properties. This is called similarity invariant or invariant under similarity. Conjugate When the given equation is expressed for $B$, $$ B = P^{-1}</description></item><item><title>Eigenvalues and Eigenvectors</title><link>https://freshrimpsushi.github.io/en/posts/319/</link><pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/319/</guid><description>Definition1 Given a matrix $n\times n$ $A$, for a non-zero column vector $\mathbf{0}$ $n\times 1$ and a constant $\mathbf{x}$, the following equation is referred to as the eigenvalue equation or the eigenvalue problem. $$ \begin{equation} A \mathbf{x} = \lambda \mathbf{x} \end{equation} $$ For a given $A$, a $\mathbf{x}$ that satisfies the eigenvalue equation above is called the eigenvalue of $A$, and $n\times 1$ is called the eigenvector corresponding to the</description></item><item><title>Definite matrix</title><link>https://freshrimpsushi.github.io/en/posts/336/</link><pubDate>Sat, 06 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/336/</guid><description>Definition1 Positive Definite Matrix A quadratic form $\mathbf{x}^{\ast} A \mathbf{x}$ is called a positive definite matrix or quadratic form if it satisfies $\mathbf{x}^{\ast} A \mathbf{x} &amp;gt; 0$ for all $\mathbf{x} \ne \mathbf{0}$. called a negative definite matrix or quadratic form if it satisfies $\mathbf{x}^{\ast} A \mathbf{x} &amp;lt; 0$ for all $\mathbf{x} \ne \mathbf{0}$. called indefinite if it sometimes satisfies $\mathbf{x}$ for the same quadratic form or matrix $A$. For real</description></item><item><title>Properties of Determinants</title><link>https://freshrimpsushi.github.io/en/posts/3015/</link><pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3015/</guid><description>Properties Let $A,B$ be a $n\times n$ matrix and $k$ be a constant. The determinant satisfies the following properties: (a) $\det(kA) = k^{n}\det(A)$ (b) $\det(AB) = \det(A)\det(B)$ (c) $\det(AB)=\det(BA)$ (d) If $A$ is an invertible matrix, then $\det(A^{-1}) = \dfrac{1}{\det(A)}$ (e) $\det(A^{T}) = \det(A)$. Here, $A^{T}$ is the transpose of $A$.</description></item><item><title>Determinants</title><link>https://freshrimpsushi.github.io/en/posts/252/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/252/</guid><description>Definition Let $A$ be the following $2 \times 2$ matrix. $$ A = \begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \end{bmatrix} $$ The determinant of $A$ is defined as follows and is denoted by $\det(A)$. $$ \det(A) := ad - bc $$ Explanation To discuss determinants, we cannot ignore the very purpose of linear algebra. Most problems in mathematics can be summarized as &amp;lsquo;Can we solve the equation?&amp;rsquo; A</description></item><item><title>Augmented Matrices and Elementary Row Operations</title><link>https://freshrimpsushi.github.io/en/posts/3014/</link><pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3014/</guid><description>Definition1 Let&amp;rsquo;s assume a linear system is given as follows. $$ \begin{equation} \begin{aligned} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= b_{1}\\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= b_{2}\\ &amp;amp;\vdots\\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= b_{m} \end{aligned} \end{equation} $$ The representation of constants of a linear system in a matrix is called an augmented matrix. $$ \begin{equation} \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}</description></item><item><title>Simultaneous Linear Equations</title><link>https://freshrimpsushi.github.io/en/posts/3013/</link><pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3013/</guid><description>Definition1 For constants $a_{1}$, $a_{2}$, $\dots$, $a_{n}$, $b$, we define a linear equation for variables $x_{1}$, $x_{2}$, $\dots$, $x_{n}$ as follows. $$ \begin{equation} a_{1}x_{1} + a_{2}x_{2} + \cdots + a_{n}x_{n} = b \label{lineq} \end{equation} $$ At least one of $a$ is not $0$. In other words, not &amp;ldquo;all $a$ are $0$&amp;rdquo;. A finite set of linear equations is called a system of linear equations or simply a linear system, and</description></item><item><title>The eigenvectors of distinct eigenvalues of Hermitian matrices are orthogonal.</title><link>https://freshrimpsushi.github.io/en/posts/330/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/330/</guid><description>Theorem Let $A$ be a Hermitian matrix of size $n \times n$. Let the eigenvectors corresponding to two distinct eigenvalues $\lambda , \mu$ of $A$ be $\mathbf{x}$ and $\mathbf{y}$, that is, $$ \begin{align*} A \mathbf{x} =&amp;amp; \lambda \mathbf{x} \quad \\ A \mathbf{y} =&amp;amp; \mu \mathbf{y} \end{align*} $$ Then, the two eigenvectors are orthogonal to each other. $$ \mathbf{x} \perp \mathbf{y} $$ Explanation Hermitian matrices have the property that not only</description></item><item><title>Unitary Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3008/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3008/</guid><description>Definition Unitary Matrix Let $A$ be a square complex matrix. $A$ is called a unitary matrix if it satisfies the following equation: $$ A^{-1}=A^{\ast} $$ Here, $A^{-1}$ is the inverse of $A$, $A^{\ast}$ is the conjugate transpose of $A$. Unitary Diagonalization1 Consider a square matrix $A$ of size $n \times n$. $A$ is said to be unitarily diagonalizable if it satisfies the following equation for some diagonal matrix $D$ and</description></item><item><title>The eigenvalues of a Hermitian matrix are always real</title><link>https://freshrimpsushi.github.io/en/posts/310/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/310/</guid><description>Theorem Let $A$ be a Hermitian matrix of size $n \times n$. Then, the eigenvalues of $A$ are all real. Explanation In general, there is no guarantee that the eigenvalues of a matrix are real, but for Hermitian matrices, this can be verified through proof. It may not be intuitively obvious, but the proof itself is relatively simple, and it is quite useful as a fact. It yields various good</description></item><item><title>Hermitian Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3007/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3007/</guid><description>Definition Let $A$ be a square complex matrix. If $A$ satisfies the following equation, it is called a Hermitian matrix or self-adjoint matrix. $$ A^{\ast}=A $$ Here, $A^{\ast}$ is the conjugate transpose of $A$. If $A$ satisfies the following equation, it is called a skew-Hermitian matrix . $$ A^{\ast}=-A $$ Explanation If it is a real matrix, since $A^{\ast}=A^{T}$, if it is a symmetric matrix, it is a Hermitian matrix.</description></item><item><title>Equivalence Conditions for Orthogonal Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3012/</link><pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3012/</guid><description>Theorem For a real matrix $A$, the following propositions are all equivalent. (a) $A$ is an orthogonal matrix. (b) The set of row vectors of $A$ forms a normal orthogonal set in $\mathbb{R}^n$. (c) The set of column vectors of $A$ forms a normal orthogonal set in $\mathbb{R}^n$. (d) $A$ preserves inner product, i.e., for all $\mathbf{x},\mathbf{y}\in \mathbb{R}^{n}$, the following holds: $$ (A \mathbf{x}) \cdot (A\mathbf{y}) = \mathbf{x} \cdot \mathbf{y}</description></item><item><title>대각합</title><link>https://freshrimpsushi.github.io/en/posts/1924/</link><pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1924/</guid><description>Definition Let the $n\times n$ matrix $A$ be given as follows. $$ A= \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{n1} &amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{bmatrix} $$ The sum of the diagonal entries of $A$ is defined to be the trace of $A$ and is denoted as follows. $$ \text{tr}(A)=\text{Tr}(A)=a_{11}+a_{22}+\cdots</description></item><item><title>Properties of Orthogonal Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3010/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3010/</guid><description>Properties1 An orthogonal matrix has the following properties: (a) The transpose of an orthogonal matrix is also an orthogonal matrix. (b) The inverse of an orthogonal matrix is an orthogonal matrix. (c) The product of two orthogonal matrices is an orthogonal matrix. (d) The determinant of an orthogonal matrix is either $1$ or $-1$. $$ \det(A)=\pm 1 $$ Proof (a) Let&amp;rsquo;s say $A$ is an orthogonal matrix. Let&amp;rsquo;s say $B$</description></item><item><title>Orthogonal Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3009/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3009/</guid><description>Definition Let $A$ be a square real matrix. $A$ is called an orthogonal matrix if it satisfies the following equation: $$ A^{-1} = A^{\mathsf{T}} $$ Another way to express this condition is as follows: $$ AA^{\mathsf{T}} = A^{\mathsf{T}}A =I $$ Explanation To put the definition in words, an orthogonal matrix is a matrix whose row vectors or column vectors are orthogonal unit vectors to each other. When extended to complex</description></item><item><title>Inner Product with Vector</title><link>https://freshrimpsushi.github.io/en/posts/3011/</link><pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3011/</guid><description>Definition: Inner Product of Two Column Vectors1 The inner product of two column vectors of size $n \times 1$, $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{R}^{n}$ is defined as follows. $$ \begin{equation} \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{T}\mathbf{v}=u_{1}v_{1} + u_{2}v_{2} + \cdots + u_{n}v_{n} \label{EuclideanIP} \end{equation} $$ In the case where $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{C}^{n}$, it is as follows. $$ \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{\ast}\mathbf{v}=u^{\ast}_{1}v_{1}^{\ } + u_{2}^{\ast}v_{2}^{\ } + \cdots + u_{n}^{\ast}v_{n}^{\ }</description></item><item><title>Conjugate Transpose Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3006/</link><pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3006/</guid><description>Definition Let $A$ be a complex matrix of size $m \times n $. Define $\overline{A}$ as follows, and call it the conjugate matrix of $A$. $$ \overline{A} :=\begin{bmatrix} \overline{a_{11}} &amp;amp; \overline{a_{12}} &amp;amp; \cdots &amp;amp; \overline{a_{1n}} \\ \overline{a_{21}} &amp;amp; \overline{a_{22}} &amp;amp; \cdots &amp;amp; \overline{a_{2n}} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \overline{a_{m1}} &amp;amp; \overline{a_{m2}} &amp;amp; \cdots &amp;amp; \overline{a_{mn}} \end{bmatrix} = \left[ \overline{a_{ij}} \right] $$ Here, $\overline{a}$ is the conjugate</description></item><item><title>Symmetric Matrices, Skew-Symmetric Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3005/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3005/</guid><description>Definition1 A square matrix $A$ is called a symmetric matrix if it satisfies the following equation: $$ A=A^{T} $$ Here, $A^{T}$ is the transpose of $A$. $A$ is called an anti-symmetric matrix if it satisfies the following equation: $$ A =-A^{T} $$ Explanation By the definition of the transpose, matrices that are not square cannot be symmetric or anti-symmetric. If $A$ is an anti-symmetric matrix, it follows from the definition</description></item><item><title>Conditions for a Matrix Being Invertible</title><link>https://freshrimpsushi.github.io/en/posts/3004/</link><pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3004/</guid><description>Theorem1 Let $A$ be a square matrix of size $n\times n$. Then the following propositions are all equivalent. (a) $A$ is an invertible matrix. (b) The homogeneous linear system $A\mathbf{x}=\mathbf{0}$ has only the trivial solution. (c) The reduced row echelon form of $A$ is $I_{n}$. (d) $A$ can be expressed as a product of elementary matrices. (e) $A\mathbf{x}=\mathbf{b}$ has a solution for all $n\times 1$ matrices $\mathbf{b}$. (f) $A\mathbf{x}=\mathbf{b}$ has</description></item><item><title>Inverse Matrix, Reversible Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3003/</link><pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3003/</guid><description>Definition Let $A$ be an arbitrary square matrix of size $n\times n$. A matrix $L$ is called the left inverse matrix of $A$ if it satisfies the following equation with $A$ in a matrix multiplication. $$ LA=I_{n} $$ Here, $I_{n}$ is the identity matrix of size $n\times n$. A matrix $R$ that is capable of matrix multiplication with $A$ and satisfies the following equation is called the right inverse matrix</description></item><item><title>Transpose Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3002/</link><pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3002/</guid><description>Definition1 Let&amp;rsquo;s consider a matrix of size $m\times n$ as $A$. The matrix obtained by swapping the rows and columns of $A$ is called the transpose of $A$ and is denoted by $A^{\mathsf{T}}$ or $A^{T}$, $A^{t}$. Description Following the definition, if $A$ is a $m \times n$ matrix then $A^{\mathsf{T}}$ will be a $n \times m$ matrix. Also, the $i$th row of $A$ is the same as the $i$th column</description></item><item><title>Identity Matrix, Unit Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3001/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3001/</guid><description>Definition A diagonal matrix of size $n\times n$ with all diagonal elements being $1$ is called an identity matrix or unit matrix, denoted as $I_{n}$ or $I_{n\times n}$. $$ I_{n\times n}= \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \end{bmatrix} $$ Description The identity matrix is</description></item><item><title>Diagonal Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1958/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1958/</guid><description>Diagonal Matrix1 Let&amp;rsquo;s consider a matrix $A$ of size $n\times m$. The elements whose row and column numbers are the same, that is, $a_{ii} (1 \le i \le \min(n,m))$, are called the main diagonal elements. The imaginary line connecting the main diagonal elements is referred to as the main diagonal, or principal diagonal. A matrix $A$, in which all elements except for the main diagonal elements are $0$, is called</description></item><item><title>Square Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1956/</link><pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1956/</guid><description>Definition A matrix $A$ is called a square matrix if the number of rows and columns of the matrix $A$ are equal. Explanation It is also referred to as a regular square matrix. Square matrices are easy to handle and possess various favourable properties. Examples Identity matrix Invertible matrix Elementary matrix Symmetric matrix Orthogonal matrix Hermitian matrix Unitary matrix</description></item><item><title>Matrix Operations: Scalar Multiplication, Addition, and Multiplication</title><link>https://freshrimpsushi.github.io/en/posts/1957/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1957/</guid><description>Scalar Multiplication The multiplication of an arbitrary matrix $A$ of size $m \times n$ by a scalar $k$ is defined as multiplying each element of $A$ by $k$ and is denoted as follows: $$ kA = k\begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} :=</description></item><item><title>Matrix Definitions</title><link>https://freshrimpsushi.github.io/en/posts/1955/</link><pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1955/</guid><description>Definition1 A matrix is an arrangement of numbers in the shape of a rectangle as follows: $$ A=\begin{bmatrix} 10 &amp;amp; 0 &amp;amp; 3 \\ 0 &amp;amp; 8 &amp;amp; 22 \end{bmatrix} $$ Each of the arranged numbers is called an entry or element. A horizontal line is called a row, and a vertical line is called a column. Moreover, if a certain matrix has $m$ rows and $n$ columns, its size</description></item><item><title>Definition of Vectors</title><link>https://freshrimpsushi.github.io/en/posts/1947/</link><pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1947/</guid><description>Definition A sequence of numbers is called a vector. Description In the general curriculum, a vector is learned as a &amp;lsquo;geometric object with magnitude and direction&amp;rsquo;. Since it&amp;rsquo;s the concept you first come across in physics, you inevitably become familiar with vectors of $3$ dimensions or less. $$ (3,4) = \begin{bmatrix} 3 \\ 4 \end{bmatrix} $$ $$ (x,y,z) = \begin{bmatrix} x \\ y \\ z \end{bmatrix} $$ However, vectors can</description></item><item><title>Operations and Notation Table of Vectors and Matrices</title><link>https://freshrimpsushi.github.io/en/posts/1886/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1886/</guid><description>Overview This document summarizes various notations and operations for vectors and matrices. Vector Vectors are usually denoted in lower case bold type, and unless otherwise stated, they refer to a $n\times 1$ matrix, that is, a column vector. The $i$rd component of vector $\mathbf{x}$ is represented as $x_{i}$. $$ \mathbf{x}=\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix},\quad\mathbf{y}=\begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} $$ Row vectors are</description></item><item><title>Generalization of the Ellipse: Ellipsoid</title><link>https://freshrimpsushi.github.io/en/posts/1471/</link><pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1471/</guid><description>Definition For a linear transformation $A \in \mathbb{R}^{m \times m}$, the image $AN$ of a $m$-dimensional unit sphere $N := \left\{ \mathbf{x} \in \mathbb{R}^{m} : \left\| \mathbf{x} \right\|_{2} = 1 \right\}$ is called an ellipsoid. The eigenvalues $\sigma_{1}^{2} &amp;gt; \cdots \ge \sigma_{m}^{2} \ge 0$ of $A$ and the corresponding unit eigenvectors $u_{1} , \cdots , u_{m}$ are referred to as the axes of the ellipsoid for $\sigma_{i} u_{i}$. Explanation A</description></item><item><title>Strassen Algorithm Proof</title><link>https://freshrimpsushi.github.io/en/posts/1284/</link><pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1284/</guid><description>Algorithm Let&amp;rsquo;s say for $k \in \mathbb{N}$ there is $n=2^{k}$. For $A, B \in \mathbb{R}^{n \times n}$, using the Jordan block matrix representation, let&amp;rsquo;s consider the following 8 matrices ${{n} \over {2}} \times {{n} \over {2}}$, $A_{i}$, and $B_{i}$. $$ AB= \begin{bmatrix} A_{1} &amp;amp; A_{2} \\ A_{3} &amp;amp; A_{4} \end{bmatrix} \begin{bmatrix} B_{1} &amp;amp; B_{2} \\ B_{3} &amp;amp; B_{4} \end{bmatrix} = \begin{bmatrix} C_{1} &amp;amp; C_{2} \\ C_{3} &amp;amp; C_{4} \end{bmatrix} =</description></item><item><title>Tri-diagonal Matrix Determinant Derivation</title><link>https://freshrimpsushi.github.io/en/posts/787/</link><pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/787/</guid><description>Formula $$ X_{n} := \begin{bmatrix} x &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\ 1 &amp;amp; x &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; x &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; x &amp;amp; 1 \\ 0 &amp;amp; 0 &amp;amp; 0</description></item><item><title>Derivation of the Determinant of the Vandermonde Matrix</title><link>https://freshrimpsushi.github.io/en/posts/736/</link><pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/736/</guid><description>Definition For distinct $1, x_{1} , x_{2 } , \cdots , x_{n}$, the matrix $V_{n}$ defined as follows is called a Vandermonde Matrix. $$ V_{n} := \begin{bmatrix} 1 &amp;amp; x_{1} &amp;amp; x_{1}^{2} &amp;amp; \cdots &amp;amp; x_{1}^{n-1} \\ 1 &amp;amp; x_{2} &amp;amp; x_{2}^{2} &amp;amp; \cdots &amp;amp; x_{2}^{n-1} \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{n} &amp;amp; x_{n}^{2} &amp;amp; \cdots &amp;amp; x_{n}^{n-1} \end{bmatrix} $$ Formula The</description></item><item><title>Proof of Cramer's Rule</title><link>https://freshrimpsushi.github.io/en/posts/783/</link><pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/783/</guid><description>Overview Cramer&amp;rsquo;s Rule is not efficient for solving systems of equations, but if $A_{j}$ is an invertible matrix or $A$ itself is given under conditions that make it convenient to calculate determinants, it can be sufficiently useful to directly find the necessary answers. Theorem Let&amp;rsquo;s assume the system of equations $A \mathbf{x} = \mathbf{b}$ consists of an invertible matrix $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}</description></item><item><title>Laplace Expansion</title><link>https://freshrimpsushi.github.io/en/posts/781/</link><pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/781/</guid><description>정리 A square matrix $A_{n \times n} = (a_{ij})$ is given. [1]: For the selected $i$ row $$ \det A = \sum_{j=1}^{n} a_{ij} C_{ij} $$ [2]: For the selected $j$ column $$ \det A = \sum_{i=1}^{n} a_{ij} C_{ij} $$ The determinant $M_{ij}$ of the matrix obtained by removing the $i$th row and $j$th column from the square matrix $A_{n \times n} = (a_{ij})$ is called a minor, and $C_{ij}</description></item><item><title>Singular Value Decomposition for Least Squares</title><link>https://freshrimpsushi.github.io/en/posts/359/</link><pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/359/</guid><description>Algorithm $A \in \mathbb{C}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{C}^{m}$, let&amp;rsquo;s say $\text{rank} A = n$ and the least squares solution of $A \mathbf{x} = \mathbf{b}$ is $\mathbf{x}_{\ast}$. Step 1. Singular Value Decomposition Find orthogonal matrix $\widehat{U}$, diagonal matrix $\widehat{\Sigma}$, and unitary matrix $V$ that satisfy $A = \widehat{U} \widehat{\Sigma} V^{\ast}$. Step 2. Using $\widehat{U}$ obtained from the singular value decomposition, compute the projection $P : = \widehat{U} \widehat{U}^{\ast}$.</description></item><item><title>Cholesky Decomposition for Least Squares Method</title><link>https://freshrimpsushi.github.io/en/posts/357/</link><pubDate>Fri, 05 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/357/</guid><description>Algorithm Given $A \in \mathbb{C}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{C}^{m}$ such that $\text{rank} A = n$ and the least-squares solution of $A \mathbf{x} = \mathbf{b}$ is denoted as $\mathbf{x}_{\ast}$. Step 1. Multiply both sides of the given equation by $A^{\ast}$ to form the normal equation $A^{\ast} A \mathbf{x} = A^{\ast} \mathbf{b}$. The solution of the normal equation is the least-squares solution of the original equation, hence, solving for</description></item><item><title>QR Decomposition for Least Squares Method</title><link>https://freshrimpsushi.github.io/en/posts/358/</link><pubDate>Fri, 05 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/358/</guid><description>Algorithm $A \in \mathbb{C}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{C}^{m}$, let $\text{rank} A = n$ and the least squares solution of $A \mathbf{x} = \mathbf{b}$ be $\mathbf{x}_{\ast}$. Step 1. QR decomposition Find the orthogonal matrix $\widehat{Q}$ and upper triangular matrix $\widehat{R}$ that satisfy $A = \widehat{Q} \widehat{R}$. Step 2. Using the obtained $\widehat{Q}$ from QR decomposition to compute the projection $P : = \widehat{Q} \widehat{Q}^{\ast}$. Since $A \mathbf{x}_{\ast} =</description></item><item><title>Least Squares Method</title><link>https://freshrimpsushi.github.io/en/posts/356/</link><pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/356/</guid><description>Definition1 Let&amp;rsquo;s say that a linear system $A\mathbf{x} = \mathbf{b}$ with a matrix $A \in \mathbb{C}^{m \times n}$ and a vector $\mathbf{b} \in \mathbb{C}^{m}$ is either overdetermined or underdetermined. In this case, the system either does not have a solution or has infinitely many. Here, consider the problem of minimizing the value of $$ \left\| A \mathbf{x} - \mathbf{b} \right\|_{2} $$ This is called the Least Squares Problem (LSP). The</description></item><item><title>Matrix QR Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/355/</link><pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/355/</guid><description>Overview Efficient matrix decomposition requires several conditions, but before efficiency, whether the decomposition itself is possible or not can be important. QR decomposition is a matrix decomposition method that does not require the condition of being a square matrix. Definition For a matrix $A := \begin{bmatrix} \mathbf{a}_{1} &amp;amp; \cdots &amp;amp; \mathbf{a}_{n} \end{bmatrix} \in \mathbb{C}^{m \times n}_{n}$ with coefficients $n$, let&amp;rsquo;s define the subspace generated by the column vectors up to</description></item><item><title>Matrix Algebra: Projections</title><link>https://freshrimpsushi.github.io/en/posts/354/</link><pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/354/</guid><description>Definition The projection $P \in \mathbb{C}^{m \times m}$ is called an orthogonal projection if it satisfies $\mathcal{C} (P) ^{\perp} = \mathcal{N} (P)$ and $P$. Explanation According to the property of projection $\mathbb{C}^{m } = \mathcal{C} (P) \oplus \mathcal{N} (P)$, it can be seen that $P$ divides $\mathbb{C}^{m}$ into exactly two subspaces, $\mathcal{C} (P)$ and $\mathcal{N} (P)$. The fact that this division satisfies the condition $\mathcal{N} (P) = \mathcal{C} (P) ^{\perp}$</description></item><item><title>Matrix Projection in Linear Algebra</title><link>https://freshrimpsushi.github.io/en/posts/352/</link><pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/352/</guid><description>Definition A square matrix $P \in \mathbb{C}^{m \times m}$ is a projector if $P^2 = P$. Explanation In algebraic terms, this is referred to as an idempotent, which similarly refers to an element like $a^2 = a$. If $P$ is a projection, then $(I-P)^2 = I - 2P + P^2 = I - 2P + P = (I-P)$, so it can be known that $(I-P)$ is also a projection. Such</description></item><item><title>Uniqueness Proof of Cholesky Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/351/</link><pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/351/</guid><description>Theorem $A&amp;gt;0$ has a unique Cholesky decomposition. Explanation Eigenvalue diagonalization, Singular value decomposition, Schur decomposition, LU decomposition, LDU decomposition all share the commonality of not being unique. This is because these methods either utilize the relationship between eigenvalues and eigenvectors, or because $1 = a \dfrac{1}{a}$ thus $L$ or $U$ can be divided into parts. However, Cholesky decomposition doesn&amp;rsquo;t use the concept of eigenvalues and is expressed as $A=LL^{T}$, so</description></item><item><title>Cholesky Decomposition of Positive Definite Matrices</title><link>https://freshrimpsushi.github.io/en/posts/350/</link><pubDate>Sat, 30 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/350/</guid><description>Overview Given a reversible matrix to a diagonal matrix, just as we could perform LDU decomposition, when the condition is strengthened to a positive definite matrix, an even more efficient matrix decomposition called Cholesky Decomposition can be done. Build-up Consider a $m \times m$ positive definite matrix $A : = \begin{bmatrix} a_{11} &amp;amp; \mathbf{w}^{T} \\ \mathbf{w} &amp;amp; K \end{bmatrix} &amp;gt; 0$. If $a_{11}$ is positive, $\mathbf{w} \in \mathbb{R}^{m-1}$ and $K</description></item><item><title>Decomposition of Symmetric Matrices into LDU</title><link>https://freshrimpsushi.github.io/en/posts/349/</link><pubDate>Fri, 29 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/349/</guid><description>Overview $L^{T}$ is an upper triangular matrix, so we can consider replacing $U$ in $A = LU$ with $U:= DL^{T}$. As general conditions become stricter than for the LU decomposition, the amount of computation significantly decreases. Theorem For the invertible symmetric matrix $A$, there exist a lower triangular matrix $L$ and a diagonal matrix $D$ satisfying $A = LDL^{T}$. Proof Since $A$ is an invertible matrix, there exist a lower</description></item><item><title>LU Decomposition of Invertible Matrices</title><link>https://freshrimpsushi.github.io/en/posts/345/</link><pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/345/</guid><description>Buildup When the matrix Matrix $A \in \mathbb{R}^{m \times m}$ is multiplied on the left side such that the component $(i, j)$ becomes $0$, the matrix $E_{ij}$ is defined as the $A$ Elimination Operator for $ij$. Specifically, for the square matrix Square Matrix $(a_{ij}) \in \mathbb{R}^{m \times m}$, $E_{ij}$ has the diagonal components as $1$, the $(i,j)$ component as $\displaystyle -m_{ij} = -{{a_{ij}} \over {a_{jj}}}$, and the remaining components as</description></item><item><title>Eigenvalue Diagonalization of Hermitian Matrices: Proof of Spectral Theory</title><link>https://freshrimpsushi.github.io/en/posts/346/</link><pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/346/</guid><description>Summary Let&amp;rsquo;s define the invertible matrix $A \in \mathbb{C}^{m \times m}$, the diagonal matrix composed of its eigenvalues $\lambda_{k}$ as $\Lambda : = \text{diag} ( \lambda_{1} , \cdots , \lambda_{m} )$, and the orthogonal matrix composed of the corresponding orthonormal eigenvectors $\mathbf{q}_{k}$ as $Q$. [1] Spectral Theory The necessary and sufficient condition for $A$ to be a normal matrix is that $A$ is unitarily diagonalizable. $$ A A^{\ast} = A^{\ast}</description></item><item><title>Schur Decomposition of Square Matrices</title><link>https://freshrimpsushi.github.io/en/posts/342/</link><pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/342/</guid><description>Definition For a given unitary matrix $Q$ and upper triangular matrix $T$, if $A = Q T Q^{\ast}$, then $A$ is said to have a Schur Factorization. Theorem Every square matrix $A \in \mathbb{C}^{ m \times m}$ has a Schur Factorization. Explanation The downside of eigenvalue diagonalization is that when it is decomposed into $A = S \Lambda S^{-1}$, it still requires the effort to find $S^{-1}$. Although it is</description></item><item><title>Proof of the Existence of a Complete Singular Value Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/341/</link><pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/341/</guid><description>Overview Eigenvalue diagonalization was limited to square matrices in application, but Singular Value Decomposition (SVD) had no such constraint. Determining whether such useful decomposition methods universally apply to all matrices, i.e., the existence of decomposition, is considered a significantly important issue. Theorem For three natural numbers $m \ge n \ge r = \text{rank} A$, the matrix $A \in \mathbb{R}^{m \times n}$ possesses an SVD. Proof For any vector $\mathbf{x} \ne</description></item><item><title>Singular Value Decomposition of a Matrix</title><link>https://freshrimpsushi.github.io/en/posts/340/</link><pubDate>Sat, 23 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/340/</guid><description>Overview While it would be great if every matrix could be decomposed through eigenvalue diagonalization, unfortunately, this method is limited by the requirement that the given matrix must be a square matrix. We aim to extend decomposability to matrices that are not square. Buildup Let us consider two natural numbers $m &amp;gt; n$ for a matrix $A \in \mathbb{C}^{ m \times n}$ whose coefficients are given by $\text{rank} A =</description></item><item><title>Eigenvalue Diagonalization of Invertible Matrices</title><link>https://freshrimpsushi.github.io/en/posts/339/</link><pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/339/</guid><description>Definition If there exists a unitary matrix $Q$ and a diagonal matrix $\Lambda$ that satisfy $A = Q^{ \ast } \Lambda Q$ for $A \in \mathbb{C}^{ m \times m }$, then the matrix $A$ is said to be unitarily diagonalizable.</description></item><item><title>The Algebraic Multiplicity of Eigenvalues is Greater Than or Equal to Their Geometric Multiplicity</title><link>https://freshrimpsushi.github.io/en/posts/328/</link><pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/328/</guid><description>Theorem A matrix matrix $A \in \mathbb{C}^{ m \times m}$ having an eigenvalue $\lambda$ with an algebraic multiplicity $a$ and a geometric multiplicity $g$ implies $a \ge g$. Explanation The algebraic and geometric multiplicities of an eigenvalue are not guaranteed to be the same. If they were, they wouldn&amp;rsquo;t have been defined differently in the first place. However, one certain thing is that regardless of how small the algebraic multiplicity</description></item><item><title>Similar Matrices Have the Same Eigenvalues</title><link>https://freshrimpsushi.github.io/en/posts/329/</link><pubDate>Mon, 20 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/329/</guid><description>Theorem If two matrices $A,B$ are similar, they have the same eigenvalues. $$ \det (A - \lambda I) = \det (B - \lambda I) $$ In this case, $\lambda$ is an eigenvalue of $A, B$. Description Having the same eigenvalues means that the characteristic equations are the same. Proof To show that the eigenvalues are the same, it is sufficient to show that the characteristic equations are the same. $$</description></item><item><title>Algebraic and Geometric Multiplicities of Eigenvalues</title><link>https://freshrimpsushi.github.io/en/posts/311/</link><pubDate>Sun, 19 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/311/</guid><description>Algebraic Multiplicity For a matrix $A \in \mathbb{R}^{m \times m}$, the eigenvalue is defined as $\lambda$ that satisfies $\det (A - \lambda I ) =0$. The characteristic equation is a polynomial equation of degree $m$ with respect to $\lambda$, which can be expressed as $$ \det (A - \lambda I ) = (-1)^m \lambda ^m + c_{m-1} \lambda ^{m-1} + \cdots + c_{1} \lambda + c_{0} = 0 $$ According</description></item><item><title>Understanding Ranks and Nullity through Systems of Equations</title><link>https://freshrimpsushi.github.io/en/posts/279/</link><pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/279/</guid><description>Historical Background Historically, the invention of matrices was primarily aimed at simplifying and conveniently notating systems of linear equations. For instance, observing the system of linear equations $$ \begin{cases} 2x_{1} &amp;amp; + &amp;amp; x_{2} &amp;amp; + &amp;amp; x_{3} =&amp;amp; 0 \\ &amp;amp; x_{2} &amp;amp; =&amp;amp; 0 \end{cases} $$ reveals the inconvenience of having to use the same variables multiple times. Representing this as a matrix leads to $$ \begin{bmatrix} 2</description></item><item><title>Matrix Rank, Nullity</title><link>https://freshrimpsushi.github.io/en/posts/3021/</link><pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3021/</guid><description>Theorem1 The dimensions of the row space and column space of matrix $A$ are the same. Proof Let $R$ be the row echelon form matrix of $A$. Since basic row operations do not change the dimensions of the row space and column space of $A$, the following equation holds: $$ \begin{align*} \dim \big( \mathcal{R}(A) \big) &amp;amp;= \dim \big( \mathcal{R}(R) \big) \\ \dim \big( \mathcal{C}(A) \big) &amp;amp;= \dim \big( \mathcal{C}(R) \big)</description></item><item><title>Row Space, Column Space, Null Space</title><link>https://freshrimpsushi.github.io/en/posts/254/</link><pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/254/</guid><description>Definition1 $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} $$ For a matrix $A$, the $m$ number of $\mathbb{R}^{n}$ vectors made from the rows of $A$ $$ \begin{align*} \mathbf{r}_{1} =&amp;amp; \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \end{bmatrix} \\ \mathbf{r}_{2} =&amp;amp;</description></item><item><title>A Simple Formula to Calculate the Sum of Elements of the Product of Second-Order Matrices</title><link>https://freshrimpsushi.github.io/en/posts/70/</link><pubDate>Sat, 22 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/70/</guid><description>Formulas The sum of the elements of the quadratic matrix $\begin{bmatrix} { a }&amp;amp;{ b } \\ { c }&amp;amp;{ d } \end{bmatrix} \begin{bmatrix} { p }&amp;amp;{ q } \\ { r }&amp;amp;{ s } \end{bmatrix}$ is as follows. $$ {(a+c)(p+q)}+{(b+d)(r+s)} $$ Description You may have encountered many problems asking to find the sum of the elements of the product of two quadratic matrices. As everyone knows, though multiplying matrices</description></item><item><title>Proof of the Power Formula for Rotation Transformation Matrices</title><link>https://freshrimpsushi.github.io/en/posts/55/</link><pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/55/</guid><description>Theorem For every natural number $n$, the following holds. $$ \begin{bmatrix} { \cos \theta }&amp;amp;{ -\sin \theta } \\ { \sin \theta }&amp;amp;{ \cos \theta } \end{bmatrix} ^{n} = \begin{bmatrix} { \cos n\theta }&amp;amp;{ -\sin n\theta } \\ { \sin n\theta }&amp;amp;{ \cos n\theta } \end{bmatrix} $$ Explanation A linear transformation matrix that rotates by $\theta$ around the origin, when squared, results in a linear transformation that rotates by $n\theta$.</description></item><item><title>Inverse Functions of Fractional Functions and the Shape of the Inverse Matrix of a Quadratic Square Matrix</title><link>https://freshrimpsushi.github.io/en/posts/53/</link><pubDate>Wed, 05 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/53/</guid><description>Theorem The inverse function of the fractional function $\displaystyle f(x)=\frac { ax+b }{ cx+d }$ is $$ f^{ -1 }(x)=\frac { dx-b }{ -cx+a } $$ The inverse matrix of the 2x2 square matrix $\begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \end{bmatrix}$ is $$ \frac { 1 }{ ad-bc } \begin{bmatrix} d &amp;amp; -b \\ -c &amp;amp; a \end{bmatrix} $$ Explanation It might just be a coincidence, but finding</description></item></channel></rss>