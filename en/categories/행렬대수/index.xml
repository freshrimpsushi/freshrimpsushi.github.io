<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>행렬대수 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%ED%96%89%EB%A0%AC%EB%8C%80%EC%88%98/</link>
    <description>Recent content in 행렬대수 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 25 Jan 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%96%89%EB%A0%AC%EB%8C%80%EC%88%98/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sparse Matrices</title>
      <link>https://freshrimpsushi.github.io/en/posts/2513/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2513/</guid>
      <description>Definition In natural language, sparse refers to being thin or scarce in a way that a value is considered virtually non-existent if it is $0$. Sparsity indicates the degree to which something is made up of such $0$ values. Sparse Matrix A matrix whose elements are mostly $0$ is referred to as a Sparse Matrix. $S$-Sparse 1 Even if there are many values, when there are only $S \ll d$</description>
    </item>
    <item>
      <title>Zero Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3394/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3394/</guid>
      <description>Definition The matrix of size $m\times n$ with all elements being $0$ is called a zero matrix, and is denoted as $O_{m\times n}$ or simply as $O$. Description Other notations include $Z_{m \times n}$, $Z$, $\mathbf{0}_{m\times n}$, or $\mathbf{0}$. It is better to write the number $0$ in bold to avoid confusion (actually, it is better just to use $O$). A zero matrix is the identity element for matrix addition.</description>
    </item>
    <item>
      <title>Block Matrices</title>
      <link>https://freshrimpsushi.github.io/en/posts/3323/</link>
      <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3323/</guid>
      <description>Definition Let&amp;rsquo;s say $A$ is a matrix $m \times n$. $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \\ \end{bmatrix} $$ Consider any vertical and horizontal lines that cut the matrix as follows. $$ A = \left[ \begin{array}{cc|ccc|c|} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}</description>
    </item>
    <item>
      <title>Power Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3307/</link>
      <pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3307/</guid>
      <description>Definition1 For a given matrix ▷ eq01◁ ▷ eq02◁, if there exists a positive number ▷ eq04◁ that satisfies ▷ eq03◁, then ▷ eq02◁ is referred to as nilpotent. In this case, ▷ eq06◁ is the zero matrix ▷ eq01◁</description>
    </item>
    <item>
      <title>Triangular Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3305/</link>
      <pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3305/</guid>
      <description>Definition1 A matrix $A = [a_{ij}]$ with all elements above the main diagonal being $0$ is called a lower triangular matrix. $$ A \text{ is lower triangluar matrix if } a_{ij} = 0 \text{ whenever } i \lt j $$ A matrix $A = [a_{ij}]$ with all elements below the main diagonal being $0$ is called an upper triangular matrix. $$ A \text{ is upper triangluar matrix if } a_{ij}</description>
    </item>
    <item>
      <title>Supersaturated and Undersaturated Systems</title>
      <link>https://freshrimpsushi.github.io/en/posts/3265/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3265/</guid>
      <description>Definition1 Consider the following linear system for matrix $A$: $$ A \mathbf{x} = \mathbf{b} $$ If $m \gt n$, then there are more constraints than unknowns, and such a linear system is called an overdetermined system. If $m \lt n$, then there are less constraints than unknowns, and such a linear system is called an underdetermined system. Theorem 1 Consider the linear system for matrix $A$ with rank $r$ of</description>
    </item>
    <item>
      <title>Simultaneous Homogeneous Linear Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3020/</link>
      <pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3020/</guid>
      <description>Definition1 In a linear system, if the constant terms are all $0$, it is called homogeneous. $$ \begin{align*} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= 0 \\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= 0 \\ &amp;amp;\vdots \\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= 0 \end{align*} $$ Unlike general linear systems, every homogeneous linear system always has a solution because if the constant terms are $0$,</description>
    </item>
    <item>
      <title>Gaussian-Jordan Elimination</title>
      <link>https://freshrimpsushi.github.io/en/posts/3019/</link>
      <pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3019/</guid>
      <description>Definition1 An augmented matrix is said to be in echelon form if it satisfies the following conditions: In rows that have a non-zero element, the first non-zero number is a 1, referred to as the leading 1. Rows where all elements are zero are placed at the bottom. For consecutive rows that contain non-zero elements, the leading 1 in the upper row must be to the left of the leading</description>
    </item>
    <item>
      <title>Eigenvalues and Eigenvectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/319/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/319/</guid>
      <description>Definition1 Given a matrix $n\times n$ $A$, for a non-zero column vector $\mathbf{0}$ $n\times 1$ and a constant $\mathbf{x}$, the following equation is referred to as the eigenvalue equation or the eigenvalue problem. $$ \begin{equation} A \mathbf{x} = \lambda \mathbf{x} \end{equation} $$ For a given $A$, a $\mathbf{x}$ that satisfies the eigenvalue equation above is called the eigenvalue of $A$, and $n\times 1$ is called the eigenvector corresponding to the</description>
    </item>
    <item>
      <title>Definite matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/336/</link>
      <pubDate>Sat, 06 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/336/</guid>
      <description>Definition1 Positive Definite Matrix A quadratic form $\mathbf{x}^{\ast} A \mathbf{x}$ is called a positive definite matrix or quadratic form if it satisfies $\mathbf{x}^{\ast} A \mathbf{x} &amp;gt; 0$ for all $\mathbf{x} \ne \mathbf{0}$. called a negative definite matrix or quadratic form if it satisfies $\mathbf{x}^{\ast} A \mathbf{x} &amp;lt; 0$ for all $\mathbf{x} \ne \mathbf{0}$. called indefinite if it sometimes satisfies $\mathbf{x}$ for the same quadratic form or matrix $A$. For real</description>
    </item>
    <item>
      <title>Properties of Determinants</title>
      <link>https://freshrimpsushi.github.io/en/posts/3015/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3015/</guid>
      <description>Properties Let $A,B$ be a $n\times n$ matrix and $k$ be a constant. The determinant satisfies the following properties: (a) $\det(kA) = k^{n}\det(A)$ (b) $\det(AB) = \det(A)\det(B)$ (c) $\det(AB)=\det(BA)$ (d) If $A$ is an invertible matrix, then $\det(A^{-1}) = \dfrac{1}{\det(A)}$ (e) $\det(A^{T}) = \det(A)$. Here, $A^{T}$ is the transpose of $A$.</description>
    </item>
    <item>
      <title>Determinants</title>
      <link>https://freshrimpsushi.github.io/en/posts/252/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/252/</guid>
      <description>Definitions Let&amp;rsquo;s denote $A$ as the following $2 \times 2$ matrix. $$ A = \begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \end{bmatrix} $$ The determinant of $A$ is defined as follows and is denoted by $\det(A)$. $$ \det(A) := ad - bc $$ Explanation To talk about determinants, we cannot skip discussing the very purpose of linear algebra. It wouldn&amp;rsquo;t be an exaggeration to say that most problems in</description>
    </item>
    <item>
      <title>Augmented Matrices and Elementary Row Operations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3014/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3014/</guid>
      <description>Definition1 Let&amp;rsquo;s assume a linear system is given as follows. $$ \begin{equation} \begin{aligned} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= b_{1}\\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= b_{2}\\ &amp;amp;\vdots\\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= b_{m} \end{aligned} \label{linsys2} \end{equation} $$ The representation of constants of a linear system in a matrix is called an augmented matrix. $$ \begin{equation} \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp;</description>
    </item>
    <item>
      <title>Simultaneous Linear Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3013/</link>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3013/</guid>
      <description>Definition1 For constants $a_{1}$, $a_{2}$, $\dots$, $a_{n}$, $b$, we define a linear equation for variables $x_{1}$, $x_{2}$, $\dots$, $x_{n}$ as follows. $$ \begin{equation} a_{1}x_{1} + a_{2}x_{2} + \cdots + a_{n}x_{n} = b \label{lineq} \end{equation} $$ At least one of $a$ is not $0$. In other words, not &amp;ldquo;all $a$ are $0$&amp;rdquo;. A finite set of linear equations is called a system of linear equations or simply a linear system, and</description>
    </item>
    <item>
      <title>Unitary Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3008/</link>
      <pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3008/</guid>
      <description>Definition Unitary Matrix Let $A$ be a square complex matrix. $A$ is called a unitary matrix if it satisfies the following equation: $$ A^{-1}=A^{\ast} $$ Here, $A^{-1}$ is the inverse of $A$, $A^{\ast}$ is the conjugate transpose of $A$. Unitary Diagonalization1 Consider a square matrix $A$ of size $n \times n$. $A$ is said to be unitarily diagonalizable if it satisfies the following equation for some diagonal matrix $D$ and</description>
    </item>
    <item>
      <title>The eigenvalues of a Hermitian matrix are always real</title>
      <link>https://freshrimpsushi.github.io/en/posts/310/</link>
      <pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/310/</guid>
      <description>Summary Let $A$ be a Hermitian matrix of size $n \times n$. Then, the eigenvalues of $A$ are all real. Explanation In general, there is no guarantee that the eigenvalues of a matrix are real, but for Hermitian matrices, this can be verified through proof. It may not be intuitively obvious, but the proof itself is relatively simple, and it is quite useful as a fact. It yields various good</description>
    </item>
    <item>
      <title>Hermitian Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3007/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3007/</guid>
      <description>Definition Let $A$ be a square complex matrix. If $A$ satisfies the following equation, it is called a Hermitian matrix or self-adjoint matrix. $$ A^{\ast}=A $$ Here, $A^{\ast}$ is the conjugate transpose of $A$. If $A$ satisfies the following equation, it is called a skew-Hermitian matrix . $$ A^{\ast}=-A $$ Explanation If it is a real matrix, since $A^{\ast}=A^{T}$, if it is a symmetric matrix, it is a Hermitian matrix.</description>
    </item>
    <item>
      <title>Trace</title>
      <link>https://freshrimpsushi.github.io/en/posts/1924/</link>
      <pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1924/</guid>
      <description>Definition Let matrix $n\times n$ be given as follows. $$ A= \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{n1} &amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{bmatrix} $$ The sum of the diagonal elements of $A$ is defined as the trace of $A$ and is denoted as follows. $$ \text{tr}(A)=\text{Tr}(A)=a_{11}+a_{22}+\cdots + a_{nn}=\sum \limits_{i=1}^{n}</description>
    </item>
    <item>
      <title>Orthogonal Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3009/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3009/</guid>
      <description>Definition Let $A$ be a square real matrix. $A$ is called an orthogonal matrix if it satisfies the following equation: $$ A^{-1} = A^{T} $$ Another way to express this condition is as follows: $$ AA^{T} = A^{T}A =I $$ Explanation To put the definition in words, an orthogonal matrix is a matrix whose row vectors or column vectors are orthogonal unit vectors to each other. When extended to complex</description>
    </item>
    <item>
      <title>Matrix Inner Product</title>
      <link>https://freshrimpsushi.github.io/en/posts/3011/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3011/</guid>
      <description>Definition: Inner Product of Two Column Vectors1 The inner product of two column vectors of size $n \times 1$, $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{R}^{n}$ is defined as follows. $$ \begin{equation} \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{T}\mathbf{v}=u_{1}v_{1} + u_{2}v_{2} + \cdots + u_{n}v_{n} \label{EuclideanIP} \end{equation} $$ In the case where $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{C}^{n}$, it is as follows. $$ \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{\ast}\mathbf{v}=u^{\ast}_{1}v_{1}^{\ } + u_{2}^{\ast}v_{2}^{\ } + \cdots + u_{n}^{\ast}v_{n}^{\ }</description>
    </item>
    <item>
      <title>Conjugate Transpose Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3006/</link>
      <pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3006/</guid>
      <description>Definition Let $A$ be a complex matrix of size $m \times n $. Define $\overline{A}$ as follows, and call it the conjugate matrix of $A$. $$ \overline{A} :=\begin{bmatrix} \overline{a_{11}} &amp;amp; \overline{a_{12}} &amp;amp; \cdots &amp;amp; \overline{a_{1n}} \\ \overline{a_{21}} &amp;amp; \overline{a_{22}} &amp;amp; \cdots &amp;amp; \overline{a_{2n}} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \overline{a_{m1}} &amp;amp; \overline{a_{m2}} &amp;amp; \cdots &amp;amp; \overline{a_{mn}} \end{bmatrix} = \left[ \overline{a_{ij}} \right] $$ Here, $\overline{a}$ is the conjugate</description>
    </item>
    <item>
      <title>Symmetric Matrices, Skew-Symmetric Matrices</title>
      <link>https://freshrimpsushi.github.io/en/posts/3005/</link>
      <pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3005/</guid>
      <description>Definition1 A square matrix $A$ is called a symmetric matrix if it satisfies the following equation: $$ A=A^{T} $$ Here, $A^{T}$ is the transpose of $A$. $A$ is called an anti-symmetric matrix if it satisfies the following equation: $$ A =-A^{T} $$ Explanation By the definition of the transpose, matrices that are not square cannot be symmetric or anti-symmetric. If $A$ is an anti-symmetric matrix, it follows from the definition</description>
    </item>
    <item>
      <title>Conditions for a Matrix Being Invertible</title>
      <link>https://freshrimpsushi.github.io/en/posts/3004/</link>
      <pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3004/</guid>
      <description>Theorem1 Let $A$ be a square matrix of size $n\times n$. Then the following propositions are all equivalent. (a) $A$ is an invertible matrix. (b) The homogeneous linear system $A\mathbf{x}=\mathbf{0}$ has only the trivial solution. (c) The reduced row echelon form of $A$ is $I_{n}$. (d) $A$ can be expressed as a product of elementary matrices. (e) $A\mathbf{x}=\mathbf{b}$ has a solution for all $n\times 1$ matrices $\mathbf{b}$. (f) $A\mathbf{x}=\mathbf{b}$ has</description>
    </item>
    <item>
      <title>Inverse Matrix, Reversible Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3003/</link>
      <pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3003/</guid>
      <description>Definition Let $A$ be an arbitrary square matrix of size $n\times n$. A matrix $L$ is called the left inverse matrix of $A$ if it satisfies the following equation with $A$ in a matrix multiplication. $$ LA=I_{n} $$ Here, $I_{n}$ is the identity matrix of size $n\times n$. A matrix $R$ that is capable of matrix multiplication with $A$ and satisfies the following equation is called the right inverse matrix</description>
    </item>
    <item>
      <title>Transpose Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3002/</link>
      <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3002/</guid>
      <description>Definition1 Let&amp;rsquo;s consider a matrix of size $m\times n$ as $A$. The matrix obtained by swapping the rows and columns of $A$ is called the transpose of $A$ and is denoted by $A^{T}$ or $A^{t}$. Description Following the definition, if $A$ is a $m \times n$ matrix then $A^{T}$ will be a $n \times m$ matrix. Also, the $i$th row of $A$ is the same as the $i$th column of</description>
    </item>
    <item>
      <title>Identity Matrix, Unit Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3001/</link>
      <pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3001/</guid>
      <description>Definition A diagonal matrix of size $n\times n$ with all diagonal elements being $1$ is called an identity matrix or unit matrix, denoted as $I_{n}$ or $I_{n\times n}$. $$ I_{n\times n}= \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \end{bmatrix} $$ Description The identity matrix is</description>
    </item>
    <item>
      <title>Diagonal Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/1958/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1958/</guid>
      <description>Diagonal Matrix1 Let&amp;rsquo;s consider a matrix $A$ of size $n\times m$. The elements whose row and column numbers are the same, that is, $a_{ii} (1 \le i \le \min(n,m))$, are called the main diagonal elements. The imaginary line connecting the main diagonal elements is referred to as the main diagonal, or principal diagonal. A matrix $A$, in which all elements except for the main diagonal elements are $0$, is called</description>
    </item>
    <item>
      <title>Square Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/1956/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1956/</guid>
      <description>Definition A matrix $A$ is called a square matrix if the number of rows and columns of the matrix $A$ are equal. Explanation It is also referred to as a regular square matrix. Square matrices are easy to handle and possess various favourable properties. Examples Identity matrix Invertible matrix Elementary matrix Symmetric matrix Orthogonal matrix Hermitian matrix Unitary matrix</description>
    </item>
    <item>
      <title>Matrix Operations: Scalar Multiplication, Addition, and Multiplication</title>
      <link>https://freshrimpsushi.github.io/en/posts/1957/</link>
      <pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1957/</guid>
      <description>Scalar Multiplication The multiplication of an arbitrary matrix $A$ of size $m \times n$ by a scalar $k$ is defined as multiplying each element of $A$ by $k$ and is denoted as follows: $$ kA = k\begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} :=</description>
    </item>
    <item>
      <title>Matrix Definitions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1955/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1955/</guid>
      <description>Definition1 A matrix is an arrangement of numbers in the shape of a rectangle as follows: $$ A=\begin{bmatrix} 10 &amp;amp; 0 &amp;amp; 3 \\ 0 &amp;amp; 8 &amp;amp; 22 \end{bmatrix} $$ Each of the arranged numbers is called an entry or element. A horizontal line is called a row, and a vertical line is called a column. Moreover, if a certain matrix has $m$ rows and $n$ columns, its size</description>
    </item>
    <item>
      <title>Definition of Vectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/1947/</link>
      <pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1947/</guid>
      <description>Definition A sequence of numbers is called a vector. Description In the general curriculum, a vector is learned as a &amp;lsquo;geometric object with magnitude and direction&amp;rsquo;. Since it&amp;rsquo;s the concept you first come across in physics, you inevitably become familiar with vectors of $3$ dimensions or less. $$ (3,4) = \begin{bmatrix} 3 \\ 4 \end{bmatrix} $$ $$ (x,y,z) = \begin{bmatrix} x \\ y \\ z \end{bmatrix} $$ However, vectors can</description>
    </item>
    <item>
      <title>Generalization of the Ellipse: Ellipsoid</title>
      <link>https://freshrimpsushi.github.io/en/posts/1471/</link>
      <pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1471/</guid>
      <description>Definition For a linear transformation $A \in \mathbb{R}^{m \times m}$, the image $AN$ of a $m$-dimensional unit sphere $N := \left\{ \mathbb{x} \in \mathbb{R}^{m} : \left\| \mathbb{x} \right\|_{2} = 1 \right\}$ is called an ellipsoid. The eigenvalues $\sigma_{1}^{2} &amp;gt; \cdots \ge \sigma_{m}^{2} \ge 0$ of $A$ and the corresponding unit eigenvectors $u_{1} , \cdots , u_{m}$ are referred to as the axes of the ellipsoid for $\sigma_{i} u_{i}$. Explanation A</description>
    </item>
    <item>
      <title>Laplace Expansion</title>
      <link>https://freshrimpsushi.github.io/en/posts/781/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/781/</guid>
      <description>정리 A square matrix $A_{n \times n} = (a_{ij})$ is given. [1]: For the selected $i$ row $$ \det A = \sum_{j=1}^{n} a_{ij} C_{ij} $$ [2]: For the selected $j$ column $$ \det A = \sum_{i=1}^{n} a_{ij} C_{ij} $$ The determinant $M_{ij}$ of the matrix obtained by removing the $i$th row and $j$th column from the square matrix $A_{n \times n} = (a_{ij})$ is called a minor, and $C_{ij}</description>
    </item>
    <item>
      <title>Least Squares Method</title>
      <link>https://freshrimpsushi.github.io/en/posts/356/</link>
      <pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/356/</guid>
      <description>Definition1 Let&amp;rsquo;s say that a linear system $A\mathbf{x} = \mathbf{b}$ with a matrix $A \in \mathbb{C}^{m \times n}$ and a vector $\mathbf{b} \in \mathbb{C}^{m}$ is either overdetermined or underdetermined. In this case, the system either does not have a solution or has infinitely many. Here, consider the problem of minimizing the value of $$ \left\| A \mathbf{x} - \mathbf{b} \right\|_{2} $$ This is called the Least Squares Problem (LSP). The</description>
    </item>
    <item>
      <title>Matrix Projection in Linear Algebra</title>
      <link>https://freshrimpsushi.github.io/en/posts/352/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/352/</guid>
      <description>Definition A square matrix $P \in \mathbb{C}^{m \times m}$ is a projector if $P^2 = P$. Explanation In algebraic terms, this is referred to as an idempotent, which similarly refers to an element like $a^2 = a$. If $P$ is a projection, then $(I-P)^2 = I - 2P + P^2 = I - 2P + P = (I-P)$, so it can be known that $(I-P)$ is also a projection. Such</description>
    </item>
    <item>
      <title>Singular Value Decomposition of a Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/340/</link>
      <pubDate>Sat, 23 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/340/</guid>
      <description>Overview While it would be great if every matrix could be decomposed through eigenvalue diagonalization, unfortunately, this method is limited by the requirement that the given matrix must be a square matrix. We aim to extend decomposability to matrices that are not square. Buildup Let us consider two natural numbers $m &amp;gt; n$ for a matrix $A \in \mathbb{C}^{ m \times n}$ whose coefficients are given by $\text{rank} A =</description>
    </item>
    <item>
      <title>Eigenvalue Diagonalization of Invertible Matrices</title>
      <link>https://freshrimpsushi.github.io/en/posts/339/</link>
      <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/339/</guid>
      <description>Definition If there exists a unitary matrix $Q$ and a diagonal matrix $\Lambda$ that satisfy $A = Q^{ \ast } \Lambda Q$ for $A \in \mathbb{C}^{ m \times m }$, then the matrix $A$ is said to be unitarily diagonalizable.</description>
    </item>
    <item>
      <title>Matrix Rank, Nullity</title>
      <link>https://freshrimpsushi.github.io/en/posts/3021/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3021/</guid>
      <description>Theorem1 The dimensions of the row space and column space of matrix $A$ are the same. Proof Let $R$ be the row echelon form matrix of $A$. Since basic row operations do not change the dimensions of the row space and column space of $A$, the following equation holds: $$ \begin{align*} \dim \big( \mathcal{R}(A) \big) &amp;amp;= \dim \big( \mathcal{R}(R) \big) \\ \dim \big( \mathcal{C}(A) \big) &amp;amp;= \dim \big( \mathcal{C}(R) \big)</description>
    </item>
    <item>
      <title>Row Space, Column Space, Null Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/254/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/254/</guid>
      <description>Definition1 $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} $$ For a matrix $A$, the $m$ number of $\mathbb{R}^{n}$ vectors made from the rows of $A$ $$ \begin{align*} \mathbf{r}_{1} =&amp;amp; \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \end{bmatrix} \\ \mathbf{r}_{2} =&amp;amp;</description>
    </item>
    <item>
      <title>PLU Decomposition</title>
      <link>https://freshrimpsushi.github.io/en/posts/2/</link>
      <pubDate>Fri, 11 Jul 1924 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2/</guid>
      <description>Definitions 1 2 For a permutation matrix $P^{T}$ and an invertible matrix $A \in \mathbb{R}^{n \times n}$, the matrix multiplication $P^{T} A$ gives us the product $LU$. This decomposition is referred to as the PLU decompositionPermutation LU Decomposition of $A$. Since $P$ is a permutation matrix, it is also an orthogonal matrix, that is $P^{-1} = P^{T}$, hence it can be written like this: $$ P^{T} A = LU \iff</description>
    </item>
    <item>
      <title>Permutation Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/1/</link>
      <pubDate>Wed, 09 Jul 1924 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1/</guid>
      <description>Definition 1 $P \in \mathbb{R}^{n \times n}$ in which only one component in each row is $1$ and the rest are $0$ is called a Permutation Matrix. Basic Properties Orthogonality All permutation matrices are orthogonal matrices: $$P^{-1} = P^{T}$$ Sparseness For sufficiently large $n$, $P \in \mathbb{R}^{n \times n}$ is a sparse matrix. Explanation The Permutation Matrix gives a permutation of rows and columns through matrix multiplication. The following example</description>
    </item>
  </channel>
</rss>
