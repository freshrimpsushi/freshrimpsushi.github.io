<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Numerical Analysis on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/</link><description>Recent content in Numerical Analysis on FreshrimpRestaurant</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sun, 14 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/index.xml" rel="self" type="application/rss+xml"/><item><title>Derivation of Finite Difference Using Multiple Points</title><link>https://freshrimpsushi.github.io/en/posts/2553/</link><pubDate>Sun, 14 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2553/</guid><description>Theorem Given a differentiable function eq01, at a point eq03 where the n-th derivative&amp;rsquo;s value is eq06, can be approximated by using the same number of points eq10 from a finite set eq09 with cardinality eq08 for sufficiently small eq07 as follows: eq01 Here, eq11 is determined as follows: eq02 The points eq10 are also referred to as Stencil Points. eq14 is the inverse matrix. eq15 is the factorial. eq16</description></item><item><title>Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/</link><pubDate>Sun, 14 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/</guid><description>Although mathematics is profound, not all problems in the world can be solved with just paper and pen. Numerical Analysis employs machinery to solve problems approximately and is an indispensable tool in applied mathematics research. Functions and Integration Differentiation Difference $\Delta$ Finite Difference Method (FDM) 🔒(24/04/14) Finite Difference Derivation Using Multiple Points Interpolation Divided Differences $f[x_0, \ldots, x_n]$ Hermite-Genocchi Formula</description></item><item><title>Numerical Solutions of Heat Equations: Finite Difference Method</title><link>https://freshrimpsushi.github.io/en/posts/3495/</link><pubDate>Sun, 22 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3495/</guid><description>Numerical Solution of Heat Equation1 Let&amp;rsquo;s assume we are given a one-dimensional heat equation as follows. $$ \dfrac{\partial u}{\partial t} = \dfrac{\partial^{2} u}{\partial x^{2}},\qquad 0\le x \le 1,\quad t \ge 0 \tag{1} $$ Our goal is to approximate the solution using finite points. Let’s divide the space-time domain as follows. $$ \left\{ (\ell \Delta x, n\Delta t) : \ell=0,1,\dots,d+1,\ n\ge 0 \right\}\quad \text{ where } \Delta</description></item><item><title>Finite Difference Method</title><link>https://freshrimpsushi.github.io/en/posts/3494/</link><pubDate>Fri, 20 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3494/</guid><description>Definition1 2 The finite difference method is a numerical method for computing derivatives, approximating the derivative as the average rate of change over a short interval. Explanation The key to deriving the formula is the Taylor expansion. $$ f(x+h) = f(x) + f^{\prime}(x)h + \dfrac{f^{\prime \prime}(x)}{2!}h^{2} + \dfrac{f^{\prime \prime \prime}}{3!}h^{3} + \cdots \tag{1} $$ Rearranging to have only the derivative on the left side, $$ \begin{align*} f^{\prime}(x) &amp;amp;= \dfrac{f(x+h) -</description></item><item><title>Implicit Runge-Kutta Methods</title><link>https://freshrimpsushi.github.io/en/posts/3319/</link><pubDate>Wed, 02 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3319/</guid><description>Overview This document introduces the Runge-Kutta method, an Ordinary Differential Equation (ODE) solver. The commonly used fourth-order Runge-Kutta method, RK4, is a type of explicit Runge-Kutta method. This document explains the implicit Runge-Kutta method. Buildup1 $$ y^{\prime} = f(t,y),\quad t \ge t_{0},\quad y(t_{0}) = y_{0} $$ Consider the given ordinary differential equation as above. $y$ is a function of $t$, and $^{\prime}$ represents the derivative with respect to $t$. When</description></item><item><title>Explicit Runge-Kutta Methods</title><link>https://freshrimpsushi.github.io/en/posts/3317/</link><pubDate>Sat, 29 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3317/</guid><description>Overview Introducing the Runge-Kutta method, an Ordinary Differential Equation (ODE) solver. A separate article is published for a detailed explanation of the commonly used 4th Order Runge-Kutta method RK4. Buildup Consider the following ordinary differential equation. $y$ is a function of $t$, and $^{\prime}$ means the derivative with respect to $t$. $$ y^{\prime} = f(t,y),\quad t \ge t_{0},\quad y(t_{0}) = y_{0} $$ Integrating this from $t_{n}$ to $t_{n+1} = t_{n}</description></item><item><title>Fourth-order Runge-Kutta method</title><link>https://freshrimpsushi.github.io/en/posts/796/</link><pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/796/</guid><description>Method 1 Given the initial value problem $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ for the continuous function $f$ defined in $D \subset \mathbb{R}^2$, let&amp;rsquo;s assume the interval $(a,b)$ is divided into nodes as in $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Particularly, for sufficiently small $h &amp;gt; 0$, if $x_{j} = x_{0} + j h$, then for</description></item><item><title>Numerical Solution to the Initial Value Problem for the Heat Equation Given Dirichlet Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/790/</link><pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/790/</guid><description>Example 1 $$ \begin{cases} u_{t} = \gamma u_{xx} \\ u(t,0) = u(t,l) = 0 \\ u(0,x) = f(x) \end{cases} $$ The given problem has an algebraic solution simple enough to solve, yet it serves as a clear example of why we learn numerical methods to solve differential equations. It shows how solving a simple differential equation of the form $y ' = f(x,y)$ leads to the solution of partial differential</description></item><item><title>A-Stable</title><link>https://freshrimpsushi.github.io/en/posts/774/</link><pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/774/</guid><description>Buildup Multistep methods including the midpoint method might have parasitic solutions when $h$ is not sufficiently small. Being not sufficiently small refers to situations such as when there is a problem like $ y ' = \lambda y$ and it fails to meet conditions like $| 1 + h \lambda| &amp;lt;1$. When we say $z : = h \lambda \in \mathbb{C}$ and represent the condition on the complex plane, it</description></item><item><title>Convergence and Root Condition of Consistent Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/754/</link><pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/754/</guid><description>Theorem If a multistep method is consistent, then the method is convergent $\iff$ The method satisfies the root condition Explanation For a closed interval $[x_{0} , b]$, when cutting into units of $h$ to create node points, let&amp;rsquo;s call it $x_{0} \le x_{1} \le \cdots \le x_{N(h) -1} \le x_{N(h) } \le b$. Where $N(h)$ represents the index of the last node point that varies according to $h$. The method</description></item><item><title>Stability and Root Conditions of Consistent Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/734/</link><pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/734/</guid><description>Theorem If a multistep method is consistent, the method is stable $\iff$ the method satisfies the root condition. Explanation When creating node points by cutting the closed interval $[x_{0} , b]$ at intervals of $h$, let&amp;rsquo;s call it $x_{0} \le x_{1} \le \cdots \le x_{N(h) -1} \le x_{N(h) } \le b$. Here, $N(h)$ represents the index of the last node point that changes according to $h$. Consider $z_{0} , \cdots</description></item><item><title>Multistep Methods' Root Condition</title><link>https://freshrimpsushi.github.io/en/posts/732/</link><pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/732/</guid><description>Definition 1 Multi-step method: Given the continuous function defined by $D \subset \mathbb{R}^2$ for $f$ and the initial value problem $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$ on the interval $(a,b)$. Let&amp;rsquo;s divide the interval into nodes as $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Especially, for a</description></item><item><title>Adams Method</title><link>https://freshrimpsushi.github.io/en/posts/724/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/724/</guid><description>Definition 1 Multistep methods: Given a continuous function $D \subset \mathbb{R}^2$ for $f$ and an initial value problem $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$. Let&amp;rsquo;s divide interval $(a,b)$ into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Especially, for a sufficiently small $h &amp;gt; 0$, if</description></item><item><title>Richardson Error Estimation</title><link>https://freshrimpsushi.github.io/en/posts/706/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/706/</guid><description>Buildup Comparing with the true value is the best method to check the performance of a method for solving differential equations, but there are cases where it is bothersome to immediately find the true value, and other cases where it is difficult to find the true solution at all. In such cases, one can estimate the error by comparing $y_{h} (x_{n} )$ and the result of doubling the step size</description></item><item><title>Trapezoidal Method</title><link>https://freshrimpsushi.github.io/en/posts/704/</link><pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/704/</guid><description>Definition 1 Given an initial value problem $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ for a continuous function defined in $D \subset \mathbb{R}^2$ on the interval $(a,b)$, let&amp;rsquo;s say this interval is divided into nodes as described in $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Particularly, for sufficiently small $h &amp;gt; 0$, if we assume $x_{j} = x_{0}</description></item><item><title>Parasitic Solution</title><link>https://freshrimpsushi.github.io/en/posts/701/</link><pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/701/</guid><description>Definition 1 A Parasitic Solution refers to a term whose magnitude grows and whose sign changes as the method progresses. It&amp;rsquo;s useful to imagine a sequence $a_{n} = 2^{-n} + (-2)^{n}$ that does not converge due to $ (-2)^{n}$. The term &amp;lsquo;parasitic&amp;rsquo; is quite intuitive and appropriate in describing these terms as they hinder convergence. Example: Dahlquist Problem Consider, for example, $\begin{cases} y ' = \lambda y \\ y(0) =</description></item><item><title>Midpoint Method</title><link>https://freshrimpsushi.github.io/en/posts/700/</link><pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/700/</guid><description>Methods In the continuous function defined by $D \subset \mathbb{R}^2$ for the initial value problem $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ), y (x_{1}) ) = ( Y_{0} , Y_{1} ) \end{cases}$ given for $f$, assume the interval $(a,b)$ is divided into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Particularly, for sufficiently small $h &amp;gt; 0$ if $x_{j}</description></item><item><title>Convergence and Error of Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/698/</link><pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/698/</guid><description>Theorem Given the initial value problem $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) )= ( Y_{0} , \cdots , Y_{p} ) \end{cases}$, a multistep method $$ \displaystyle y_{n+1} = \sum_{j=0}^{p} a_{j} y_{n-j} + h \sum_{j = -1}^{p} b_{j} f (x_{n-j} , y_{n-j} ) $$ is consistent, and if the initial error $\displaystyle \eta (h) : = \max_{ 0 \le i \le p} |</description></item><item><title>Consistency and Order of Convergence of Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/694/</link><pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/694/</guid><description>Theorem The necessary and sufficient condition for a multistep method $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) )= ( Y_{0} , \cdots , Y_{p} ) \end{cases}$ to be consistent is (i), and the necessary and sufficient condition to have convergence order $m \in \mathbb{N}$ is (ii). (i): $$\begin{cases} \displaystyle \sum_{j = 0}^{p} a_{j} = 1 \\ \displaystyle - \sum_{j = 0}^{p} j a_{j}</description></item><item><title>Multistep Methods</title><link>https://freshrimpsushi.github.io/en/posts/693/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/693/</guid><description>Definition 1 Given a continuous function defined in $D \subset \mathbb{R}^2$ for the initial value problem given in $\begin{cases} y ' = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$, let&amp;rsquo;s say we have broken down interval $(a,b)$ into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Especially for a sufficiently small</description></item><item><title>Initial Value Variation and the Error in the Euler Method</title><link>https://freshrimpsushi.github.io/en/posts/692/</link><pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/692/</guid><description>Theorem The solution $Y(x)$ to the initial value problem $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$ defined in $[x_{0} , b] \times \mathbb{R}$ with respect to $f$, is $Y \in C^{3} [ x_{0} , b ]$ if $\displaystyle f_{y} (x,y) = {{ \partial f (x,y) } \over { \partial y }}$ and $\displaystyle f_{yy} (x,y) = {{ \partial^{2} f (x,y) } \over { \partial y^{2}</description></item><item><title>Strong Lipschitz Condition and Error of the Euler Method</title><link>https://freshrimpsushi.github.io/en/posts/689/</link><pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/689/</guid><description>Theorem Let&amp;rsquo;s assume that the solution $Y(x)$ to the initial value problem $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$, defined in $[x_{0} , b] \times \mathbb{R}$ for $f$, is twice differentiable in $[x_{0} , b]$. If $f$ satisfies strong Lipschitz condition $$ |f(x,y_{1} ) - f(x,y_{2}) | \le K | y_{1} - y_{2} | $$ for all $x_{0} \le x \le b$, $ y_{1} ,</description></item><item><title>Euler Method in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/687/</link><pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/687/</guid><description>Method 1 $D \subset \mathbb{R}^2$ defines a continuous function $f$ for the initial value problem given by $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$. Suppose the interval $(a,b)$ is divided into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Particularly for a sufficiently small $h &amp;gt; 0$, if it is assumed that $x_{j} = x_{0} + j</description></item><item><title>Lipschitz Condition</title><link>https://freshrimpsushi.github.io/en/posts/684/</link><pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/684/</guid><description>Definition We can find the Lipschitz condition in the statement of Existence-Uniqueness Theorem for First Order Differential Equations. For a continuous function defined in $D \subset \mathbb{R}^2$ with an initial value problem given by $\begin{cases} y ' = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$, if $f$ satisfies the Lipschitz condition for all $(x,y_{1}) , (x , y_{2} ) \in D$ and $K &amp;gt; 0$, $$ |f(x,y_{1} ) -</description></item><item><title>Gaussian Quadrature for Numerically Computing Improper Integrals</title><link>https://freshrimpsushi.github.io/en/posts/1161/</link><pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1161/</guid><description>Definition 1 Gauss-Chebyshev Quadrature $$ \int_{-1}^{1} {{ 1 } \over { \sqrt{1 - x^2 } }} f(x) dx \approx \sum_{i=1}^{n} w_{i} f( x_{i} ) $$ $$ w_{i} = {{ \pi } \over { n }} $$ Here, $x_{i}$s are the Chebyshev nodes that satisfy $T_{n}(x) = 0$. Gauss-Laguerre Quadrature $$ \int_{0}^{\infty} e^{-x} f(x) dx \approx \sum_{i=1}^{n} w_{i} f( x_{i} ) $$ $$ w_{i} = {{ x_{i} } \over { (n+1)^2</description></item><item><title>Hermite Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1159/</link><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1159/</guid><description>Definition Probabilist&amp;rsquo;s Hermite Polynomial $$ H_{e_{n}} := (-1)^{n} e^{{x^2} \over {2}} {{d^{n}} \over {dx^{n}}} e^{- {{x^2} \over {2}}} $$ Physicist&amp;rsquo;s Hermite Polynomial $$ H_{n} := (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2} $$ Basic Properties Hermite polynomials are used in two forms, having a relationship as shown in $H_{n} (x) = 2^{{n} \over {2}} H_{e_{n}} \left( \sqrt{2} x \right)$. Recurrence Relation [0]: $$H_{n+1} (x) = 2x H_{n} (x) - H_{n} '</description></item><item><title>Laguerre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1156/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1156/</guid><description>Definition $\displaystyle L_{n} := {{ e^{x} } \over { n! }} {{ d^{n} } \over { dx^{n} }} \left( e^{-x} x^{n} \right)$ is called Laguerre Polynomial. Basic Properties Recursion Formula [0]: $$L_{n+1} (x) = {{ 1 } \over { n+1 }} \left[ \left( 2n + 1 - x \right) L_{n} (x) - n L_{n-1} (x) \right]$$ Orthogonal Set [1] Inner Product of Functions: Given the weight $w$ as $\displaystyle w(x)</description></item><item><title>Tricks for Variable Substitution to Compute Improper Integrals Numerically</title><link>https://freshrimpsushi.github.io/en/posts/1147/</link><pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1147/</guid><description>Theorem 1 Let&amp;rsquo;s say $0 &amp;lt; a &amp;lt; b &amp;lt; \infty$. [1]: If $ 0 &amp;lt; p &amp;lt; 1$ then $$\int_{0}^{b} {{ f(x) } \over {x^{p} }} dx = \int_{0}^{{{ 1 } \over { 1-p }} b^{1-p} } f \left( \left[ ( 1- p ) m \right]^{{{ 1 } \over { 1-p }}} \right) dm$$ [2]: If $ 1 &amp;lt; p$ then $$\int_{a}^{ \infty } {{ f(x) } \over {x^{p}</description></item><item><title>Gaussian Quadrature</title><link>https://freshrimpsushi.github.io/en/posts/1144/</link><pubDate>Thu, 04 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1144/</guid><description>Definition 1 Let $f : [a,b] \to \mathbb{R}$ be integrable over $[a,b]$ and divide $[a,b]$ into nodes such as $a = x_{1} &amp;lt; \cdots &amp;lt; x_{n} = b$. $$ I_{n} (f) := \sum_{j=1}^{n} w_{j} f ( x_{j} ) \approx \int_{a}^{b} f(x) dx = I ( f ) $$ Calculating the weights $w_{j}$ of the defined $I_{n}$ and computing the numerical integration this way is called Gaussian quadrature. Explanation As there</description></item><item><title>Newton-Cotes Integration Formulas</title><link>https://freshrimpsushi.github.io/en/posts/1138/</link><pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1138/</guid><description>Definition 1 Assume that $f : [a,b] \to \mathbb{R}$ is integrable over $[a,b]$ and $[a,b]$ is divided into nodes with a constant interval of $\displaystyle h:= {{b-a} \over {n}}$, like in $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. The numerical integration operator $I_{n}^{p}$ defined as follows is called the Newton-Cotes formula. $$ I_{n}^{p} (f) := \sum_{i=0}^{n} w_{i} f ( x_{i} ) $$ For $i=0,1,\cdots , n$, $x_{i} :=</description></item><item><title>Simpson's Rule</title><link>https://freshrimpsushi.github.io/en/posts/1132/</link><pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1132/</guid><description>Definition Let $f : [a,b] \to \mathbb{R}$ be integrable on $[a,b]$ and divide $[a,b]$ into nodes with equal intervals of $\displaystyle h:= {{b-a} \over {n}}$ like $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. Then, the numerical integration operator $I_{n}^{2}$ defined as follows is called the Simpson&amp;rsquo;s Rule. $$ I_{n}^{2} (f) := \sum_{k=1}^{n/2} {{h} \over {3}} \left[ f(x_{2k-2}) + 4 f( x_{2k-1} ) + f(x_{2k} ) \right] $$ Theorem</description></item><item><title>Trapezoidal Rule</title><link>https://freshrimpsushi.github.io/en/posts/1130/</link><pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1130/</guid><description>Definition Let&amp;rsquo;s assume $f : [a,b] \to \mathbb{R}$ is integrable over $[a,b]$ and $[a,b]$ is divided into nodes at intervals of $\displaystyle h:= {{b-a} \over {n}}$, like $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. The numerical integration operator $I_{n}^{1}$, defined as follows, is called the trapezoidal rule. $$ I_{n}^{1} (f) := \displaystyle \sum_{k=1}^{n} {{h} \over {2}} \left( f(x_{k-1}) + f(x_{k} ) \right) $$ Theorem Let&amp;rsquo;s say $f \in</description></item><item><title>Numerical Integration</title><link>https://freshrimpsushi.github.io/en/posts/1128/</link><pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1128/</guid><description>Definition 1 Let&amp;rsquo;s assume that $f : [a,b] \to \mathbb{R}$ is integrable over $[a,b]$, and $[a,b]$ is divided into nodes like $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. The integral operator $I$ is defined as $\displaystyle I(f) := \int_{a}^{b} f(x) dx$. The integral operator $I_{n}$ is defined as $\displaystyle I_{n} (f) := \sum_{k=1}^{n} \int_{x_{k-1}}^{x_{k}} f(x) dx$. The error $E_{n}$ is defined as $E_{n} (f) := I (f) -</description></item><item><title>Chebyshev Nodes</title><link>https://freshrimpsushi.github.io/en/posts/1124/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1124/</guid><description>Definition From $[-1,1]$ to $\displaystyle x_{k} = \cos \left( {{2k-1} \over {2n}} \pi \right)$, $k=1, \cdots , n$ are called Chebyshev nodes. Explanation Unlike the equidistant node points typically used, Chebyshev nodes refer to the node points obtained by dividing the arc of a semicircle into equal sizes and projecting these points onto the $x$ axis. The distribution of points tends to gather a bit more at the extremes than</description></item><item><title>Chebyshev Expansion</title><link>https://freshrimpsushi.github.io/en/posts/1122/</link><pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1122/</guid><description>Buildup 1 To understand Chebyshev expansions, it&amp;rsquo;s essential first to grasp how Chebyshev expansions arise. Consider instead solving a least squares problem as opposed to solving a minimax problem. $$ M_{n} (f) := \inf_{\deg(r) \le n } \left\| f - r \right\|_{2} $$ Given the least squares problem as described above in $f : [a,b] \to \mathbb{R}$, the goal is to find a polynomial of degree $n$ or less $r_{n}^{</description></item><item><title>Minimization and Maximization Approximations and Least Squares Approximations in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1116/</link><pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1116/</guid><description>Buildup 1 Let&amp;rsquo;s assume we are given the problem of approximating a given function $f : [a,b] \to \mathbb{R}$. Since computation is the responsibility of computers, our goal is to approximate it with a polynomial function $f$. Approximating a function means that we want to use a function similar to $f$ not just at a single point but across its entire domain $[a,b]$, so the goal is to reduce the</description></item><item><title>Function Approximation in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1107/</link><pubDate>Mon, 10 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1107/</guid><description>Buildup Although it is true that computers are overwhelmingly faster at numerical calculations than humans, it isn&amp;rsquo;t because they understand transcendental functions or irrational numbers. For instance, when asked to calculate $\displaystyle \sin {{ \pi } \over {6}} = {{1} \over { 2 }}$, instead of drawing a right triangle and finding the ratio of the hypotenuse to the height using the geometric definition of trigonometric functions, it uses polynomial</description></item><item><title>B-Splines in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1045/</link><pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1045/</guid><description>If you dislike reading texts and formulas, it&amp;rsquo;s perfectly fine to just look at the images to understand. Definition 1 Let&amp;rsquo;s divide the interval $[a,b]$ into node points such as $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Considering additional nodes $x_{-K} &amp;lt; x_{-K + 1} &amp;lt; \cdots &amp;lt; x_{-1} &amp;lt; x_{0}$ and $x_{N} &amp;lt; x_{N + 1} &amp;lt; \cdots &amp;lt; x_{N+K-1} &amp;lt; x_{N+K}$</description></item><item><title>Numerical Analysis in Splines</title><link>https://freshrimpsushi.github.io/en/posts/1036/</link><pubDate>Thu, 16 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1036/</guid><description>Buildup Interpolation is not about restoring the exact function but finding a similar yet more manageable function. Of course, it would be ideal if we could find one that is explicit and easy to calculate, but the universe is not so simple. Depending on the problem, it might be necessary to quickly solve simple parts and meticulously solve complex parts, and continuity might not even be guaranteed. In this way,</description></item><item><title>Hermite Interpolation</title><link>https://freshrimpsushi.github.io/en/posts/1034/</link><pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1034/</guid><description>Definition 1 For data $(x_{1}, y_{1} , y&amp;rsquo;_{1}) , \cdots , (x_{n} , y_{n}, y&amp;rsquo;_{n})$ of different $x_{1} , \cdots , x_{n}$ that satisfies $\begin{cases} p (x_{i} ) = y_{i} \\ p '(x_{i} ) = y&amp;rsquo;_{i} \end{cases}$ and $\deg H \le 2n-1$, the polynomial function $H$ is referred to as Hermite Interpolation. Theorem Existence and Uniqueness [1]: For the given data, $H$ uniquely exists. Lagrange Form [2]: $$H_{n} (x) =</description></item><item><title>Hermite-Genocchi Formula</title><link>https://freshrimpsushi.github.io/en/posts/1031/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1031/</guid><description>Formulas Let&amp;rsquo;s say we have different $x_{0}, \cdots , x_{n}$ for $f \in C^{n} \left( \mathscr{H} \left\{ x_{0}, \cdots , x_{n} \right\} \right)$. Then, for the standard simplex $$ \tau_{n} := \left\{ ( t_{1} , \cdots , t_{n} ) : t_{i} \ge 0 \land \sum_{i=1}^{t} t_{i} \le 1 \right\} $$ and $\displaystyle t_{0} = 1 - \sum_{i=1}^{n} t_{i}$, the following is true. $$ f [ x_{0}, \cdots , x_{n} ]</description></item><item><title>Derivation of Newton's Forward Difference Formula</title><link>https://freshrimpsushi.github.io/en/posts/1025/</link><pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1025/</guid><description>Formulas For different data $x_{0} , \cdots , x_{n}$ of $(x_{0}, f(x_{0} )) , \cdots , (x_{n} , f( x_{n} ) )$, $$ p_{n} (x) =\sum_{i=0}^{n} f [ x_{0} , \cdots , x_{i} ] \prod_{j=0}^{i-1} (x - x_{j} ) $$ Description Though it seems complicated, when actually expanding for $n=0,1,2$, it simplifies as follows. $$ \begin{align*} p_{0} (x) =&amp;amp; f(x_{0}) \\ p_{1} (x) =&amp;amp; f( x_{0} ) + (x -</description></item><item><title>Lagrange's Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/1023/</link><pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1023/</guid><description>Formula 1 Given different $x_{0} , \cdots , x_{n}$ data $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$, let&amp;rsquo;s say $\displaystyle l_{i} (x) := \prod_{i \ne j} \left( {{ x - x_{j} } \over { x_{i} - x_{j} }} \right)$, then $$ p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X) $$ Description Lagrange&amp;rsquo;s formula is the simplest method among those to find polynomial interpolation. Derivation Strategy: Prove that $l_{i}$ is the</description></item><item><title>Polynomial Interpolation</title><link>https://freshrimpsushi.github.io/en/posts/1021/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1021/</guid><description>Definition 1 For different $x_{0} , \cdots , x_{n}$ data $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$, a polynomial function $p$ that satisfies $p (x_{i} ) = y_{i}$ and $\deg p \le n$ is called Polynomial Interpolation. Theorem Existence and Uniqueness [1]: For the given data, there exists a unique $p$. Lagrange&amp;rsquo;s Formula [2]: $$p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X)$$ Newton&amp;rsquo;s Divided Difference Formula [3]: $$p_{n} (x) =</description></item><item><title>Interpolation in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1016/</link><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1016/</guid><description>Definition 1 For a given pair of data $(n+1)$ and $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$, the method or the function itself that satisfies $f (x_{i} ) = y_{i}$ while possessing some specific property is called interpolation. Description For example, consider the situation where there&amp;rsquo;s data available as shown above, but the middle part is missing. Of course, it&amp;rsquo;s best to have actual data, but if not, there</description></item><item><title>Newton's Method for Solving Nonlinear Systems</title><link>https://freshrimpsushi.github.io/en/posts/1005/</link><pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1005/</guid><description>Methods 1 Let&amp;rsquo;s assume that a multivariable function $\mathbb{f} : \mathbb{R}^{N} \to \mathbb{R}^{N}$ is defined as $\mathbb{f} \in C^{2} \left( N ( \alpha ) \right)$ with $\mathbb{f} ( \alpha ) = \mathbb{0}$, $\left[ D \mathbb{f} ( \alpha ) \right]^{-1}$ existing. For an initial value $\mathbb{x}_{0}$ sufficiently close to $\alpha$, the sequence $\left\{ \mathbb{x}_{n} \right\}$ defined by $$ \mathbb{x}_{n+1} := \mathbb{x}_{n} - \left[ D \mathbb{f} ( \mathbb{x}_{n} ) \right]^{-1} f (</description></item><item><title>Mueller Method</title><link>https://freshrimpsushi.github.io/en/posts/976/</link><pubDate>Sat, 13 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/976/</guid><description>Method Let $f (\alpha) = 0$. With the initial value $x_{0} , x_{1} , x_{2}$ and $$ w_{n} := f [x_{n} , x_{n-1} ] + f [ x_{n} , x_{n-2} ] - f [ x_{n-2} , x_{n-1} ] $$ , define the sequence $\left\{ x_{n} \right\}$ as $$ x_{n+1} : = x_{n} - {{ 2 f ( x_{n} ) } \over { w_{n} \pm \sqrt{ w_{n}^{2} - 4 f (x_{n}</description></item><item><title>Secant Method</title><link>https://freshrimpsushi.github.io/en/posts/682/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/682/</guid><description>Methods Let $f,f&amp;rsquo;,f&amp;rsquo;&amp;rsquo;$ be continuous near $\alpha$ and suppose $f(\alpha) = 0, f '(\alpha) \ne 0$. For a sufficiently close initial value $x_{0} , x_{1}$ to $\alpha$, the sequence $\left\{ x_{n} \right\}$ defined by $$ x_{n+1} := x_{n} - f ( x_{n} ) {{ x_{n} - x_{n-1} } \over { f ( x_{n} ) - f ( x_{n-1} ) }} $$ converges to $\alpha$ with order $\displaystyle {{1 + \sqrt{5}</description></item><item><title>Differential Stages in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/969/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/969/</guid><description>Definition A Divided Difference of a function $f : \mathbb{R} \to \mathbb{R}$ for distinct $x_{1} , \cdots , x_{n}$ is defined as follows: $$ \begin{align*} f[x_{0}] :=&amp;amp; f( x_{0} ) \\ f [ x_{0} , x_{1} ] :=&amp;amp; {{ f ( x_{1} ) - f ( x_{0} ) } \over { x_{1} - x_{0} }} \\ f [ x_{0} , x_{1} , x_{2} ] :=&amp;amp; {{ f [ x_{1} ,</description></item><item><title>Newton-Raphson Method</title><link>https://freshrimpsushi.github.io/en/posts/678/</link><pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/678/</guid><description>Methods 1 Let&amp;rsquo;s suppose that $f,f&amp;rsquo;,f&amp;rsquo;&amp;rsquo;$ is continuous near $\alpha$ and define it as $f(\alpha) = 0, f '(\alpha) \ne 0$. For an initial value $x_{0}$, close enough to $\alpha$, the sequence $\left\{ x_{n} \right\}$ defined by $$ x_{n+1} := x_{n} - {{ f ( x_{n} ) } \over { f ' ( x_{n} ) }} $$ converges quadratically to $\alpha$ when $n \to \infty$. Explanation The Newton-Raphson method, also</description></item><item><title>Bisection Method</title><link>https://freshrimpsushi.github.io/en/posts/676/</link><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/676/</guid><description>Method 1 Let&amp;rsquo;s assume that a continuous function $f$ is defined on the closed interval $[a,b]$ and is equal to $f(a) f(b) &amp;lt; 0$. The tolerance is $\varepsilon$. A $c \in [a,b]$ satisfying $f(c) = 0$ can be found as follows. Step 1. $$c:= {{a+b} \over {2}}$$ Step 2. If $b-c \le \varepsilon$, return $c$. Step 3. If $f(b) f(c) &amp;lt; 0$, then $a:=c$, else $b:=c$. Then return to Step</description></item><item><title>Rate of Convergence in Numerical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/674/</link><pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/674/</guid><description>Definition 1 If there exists a constant $c \ge 0$ such that the sequence $\left\{ x_{n} \right\}$ converging to $\alpha$ satisfies $$ | \alpha - x_{n+1} | \le c | \alpha - x_{n} | ^{p} $$ for the order of convergence $p \ge 1$, then $\left\{ x_{n} \right\}$ is said to converge to $\alpha$ at the rate of $c$ of order $p$. Explanation In particular, together with the condition $c</description></item><item><title>Relationship between the First and Second Kind Chebyshev Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/780/</link><pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/780/</guid><description>Theorem The first kind Chebyshev polynomials $T_{n} (x) = \cos \left( n \cos^{-1} x \right)$ and second kind Chebyshev polynomials $\displaystyle U_{n} (x) = {{1} \over {n+1} } T_{n+1} &amp;rsquo; (X)$ have the following relationship: [1]: $$U_{n} (x) - U_{n-2} (x) = 2 T_{n} (X)$$ [2]: $$T_{n} (x) - T_{n-2} (x) = 2( x^2 - 1 ) U_{n-2} (x)$$ Typically, for $0 \le \theta \le \pi$, it is set to</description></item><item><title>Second Kind Chebyshev Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/779/</link><pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/779/</guid><description>Definition $$U_{n} (x) := {{1} \over {n+1} } T_{n+1} &amp;rsquo; (x) = {{\sin \left( ( n +1 ) \theta \right)} \over { \sin \theta }} $$ is called the second kind Chebyshev polynomial. Basic Properties Recursive Formula [0]: $$U_{n+1} (x) = 2x U_{n} (x) - U_{n-1} (X)$$ Orthogonal Set [1] Inner product of functions: Given the weight $w$ for $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx$ as $\displaystyle w(x) :=</description></item><item><title>First kind Chebyshev polynomials</title><link>https://freshrimpsushi.github.io/en/posts/777/</link><pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/777/</guid><description>Definition 1 $T_{n} (x) = \cos \left( n \cos^{-1} x \right)$ is called the first kind Chebyshev polynomial. Basic Properties Recurrence Formula [0]: $$T_{n+1} (x) = 2x T_{n} (x) - T_{n-1} (X)$$ Orthogonal Set [1] Inner product of functions: Given the weight $w$ as $\displaystyle w(x) := {{1} \over { \sqrt{1 - x^2} }}$, $\left\{ T_{0} , T_{1}, T_{2}, \cdots \right\}$ forms an orthogonal set. Chebyshev Nodes [2]: The roots</description></item><item><title>Numerical Analysis in Differences</title><link>https://freshrimpsushi.github.io/en/posts/722/</link><pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/722/</guid><description>Definition 1 Forward Difference: $$ \begin{align*} \Delta f(x) =&amp;amp; f(x+h) - f(x) \\ \Delta^{r+1} f(x) =&amp;amp; \Delta^{r} f(x+h) - \Delta^{r} f(x) \end{align*} $$ Backward Difference: $$ \begin{align*} \nabla f(x) =&amp;amp; f(x) - f(x- h) \\ \nabla^{r+1} f(x) =&amp;amp; \nabla^{r} f(x) - \nabla^{r} f(x- h) \end{align*} $$ Description Generally, the term Difference is used throughout sequences, but in numerical analysis, it specifically refers to the difference between the function values of</description></item><item><title>파동 방정식의 수치적 풀이: k-space method</title><link>https://freshrimpsushi.github.io/en/posts/1627/</link><pubDate>Thu, 11 Dec 2014 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1627/</guid><description>Method Assume we are given the following wave equation. $$ \partial_{t}^{2} u(\mathbf{x}, t) = \Delta_{\mathbf{x}} u (\mathbf{x}, t), \qquad (\mathbf{x}, t) \in \mathbb{R}^{n} \times \mathbb{R} $$ By taking the Fourier transform of both sides with respect to the variable $\mathbf{x}$, we obtain the following. $$ \partial_{t}^{2} \mathcal{F}_{\mathbf{x}}u(\mathbf{k}, t) = \mathcal{F}_{\mathbf{x}}[\Delta u (\cdot, t)]u(\mathbf{k}, t) = -|\mathbf{k}|^{2} \mathcal{F}_{\mathbf{x}}u(\mathbf{k}, t) $$ Additionally, the left-hand side of the above equation can be approximated</description></item><item><title>파동 방정식의 수치적 풀이: 유한차분법(FDM)</title><link>https://freshrimpsushi.github.io/en/posts/1628/</link><pubDate>Sun, 07 Dec 2014 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1628/</guid><description>Method Assume we are given the following one-dimensional wave equation. $$ \dfrac{\partial^{2} u}{\partial t^{2}} = c^{2} \dfrac{\partial^{2} u}{\partial x^{2}}, \qquad 0 \le x \le 1, \quad t \ge 0 \tag{1} $$ Our goal is to approximate the above solution using a finite number of points. Let&amp;rsquo;s discretize the spacetime domain as follows. $$ \left\{ (\ell \Delta x, n\Delta t) : \ell=0,1,\dots,d+1,\ n\ge 0 \right\}\quad \text{ where } \Delta x =</description></item></channel></rss>