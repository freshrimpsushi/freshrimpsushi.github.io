<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>수치해석 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/</link>
    <description>Recent content in 수치해석 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 16 Aug 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%EC%88%98%EC%B9%98%ED%95%B4%EC%84%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fourth-order Runge-Kutta method</title>
      <link>https://freshrimpsushi.github.io/en/posts/796/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/796/</guid>
      <description>Method 1 Explanation The Runge-Kutta method, like the Adams method, comes in various forms and determines $\gamma_{j}$ and $V_{j}$ through complex algebraic operations. Among them, the 4th-order Runge-Kutta method, often abbreviated as RK4, is the most popularly used. 4th-order Runge-Kutta method: $$ \begin{align*} y_{n+1} =&amp;amp; y_{n} + {{h} \over {6}} \left[ V_{1} + 2 V_{2} + 2 V_{3} + V_{4} \right] \\ V_{1} =&amp;amp; f(x_{n} , y_{n}) \\ V_{2} =&amp;amp;</description>
    </item>
    <item>
      <title>Multistep Methods</title>
      <link>https://freshrimpsushi.github.io/en/posts/693/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/693/</guid>
      <description>Definition 1 Given a continuous function defined in $D \subset \mathbb{R}^2$ for the initial value problem given in $\begin{cases} y &#39; = f(x,y) \\ ( y( x_{0} ) , \cdots , y(x_{p}) ) = (Y_{0}, \cdots , Y_{p} ) \end{cases}$, let&amp;rsquo;s say we have broken down interval $(a,b)$ into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Especially for a sufficiently small</description>
    </item>
    <item>
      <title>Strong Lipschitz Condition and Error of the Euler Method</title>
      <link>https://freshrimpsushi.github.io/en/posts/689/</link>
      <pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/689/</guid>
      <description>Summary Let&amp;rsquo;s assume that the solution $Y(x)$ to the initial value problem $\begin{cases} y &#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$, defined in $[x_{0} , b] \times \mathbb{R}$ for $f$, is twice differentiable in $[x_{0} , b]$. If $f$ satisfies strong Lipschitz condition $$ |f(x,y_{1} ) - f(x,y_{2}) | \le K | y_{1} - y_{2} | $$ for all $x_{0} \le x \le b$, $ y_{1} ,</description>
    </item>
    <item>
      <title>Euler Method in Numerical Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/687/</link>
      <pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/687/</guid>
      <description>Method 1 $D \subset \mathbb{R}^2$ defines a continuous function $f$ for the initial value problem given by $\begin{cases} y &#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$. Suppose the interval $(a,b)$ is divided into nodes like $a \le x_{0} &amp;lt; x_{1} &amp;lt; \cdots &amp;lt; x_{n} &amp;lt; \cdots x_{N} \le b$. Particularly for a sufficiently small $h &amp;gt; 0$, if it is assumed that $x_{j} = x_{0} + j</description>
    </item>
    <item>
      <title>Lipschitz Condition</title>
      <link>https://freshrimpsushi.github.io/en/posts/684/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/684/</guid>
      <description>Definition We can find the Lipschitz condition in the statement of Existence-Uniqueness Theorem for First Order Differential Equations. For a continuous function defined in $D \subset \mathbb{R}^2$ with an initial value problem given by $\begin{cases} y &#39; = f(x,y) \\ y( x_{0} ) = Y_{0} \end{cases}$, if $f$ satisfies the Lipschitz condition for all $(x,y_{1}) , (x , y_{2} ) \in D$ and $K &amp;gt; 0$, $$ |f(x,y_{1} ) -</description>
    </item>
    <item>
      <title>Trapezoidal Rule</title>
      <link>https://freshrimpsushi.github.io/en/posts/1130/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1130/</guid>
      <description>Definition Let&amp;rsquo;s assume $f : [a,b] \to \mathbb{R}$ is integrable over $[a,b]$ and $[a,b]$ is divided into nodes at intervals of $\displaystyle h:= {{b-a} \over {n}}$, like $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. The numerical integration operator $I_{n}^{1}$, defined as follows, is called the trapezoidal rule. $$ I_{n}^{1} (f) := \displaystyle \sum_{k=1}^{n} {{h} \over {2}} \left( f(x_{k-1}) + f(x_{k} ) \right) $$ Theorem Let&amp;rsquo;s say $f \in</description>
    </item>
    <item>
      <title>Numerical Integration</title>
      <link>https://freshrimpsushi.github.io/en/posts/1128/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1128/</guid>
      <description>Definition 1 Let&amp;rsquo;s assume that $f : [a,b] \to \mathbb{R}$ is integrable over $[a,b]$, and $[a,b]$ is divided into nodes like $a = x_{0} &amp;lt; \cdots &amp;lt; x_{n} = b$. The integral operator $I$ is defined as $\displaystyle I(f) := \int_{a}^{b} f(x) dx$. The integral operator $I_{n}$ is defined as $\displaystyle I_{n} (f) := \sum_{k=1}^{n} \int_{x_{k-1}}^{x_{k}} f(x) dx$. The error $E_{n}$ is defined as $E_{n} (f) := I (f) -</description>
    </item>
    <item>
      <title>Function Approximation in Numerical Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1107/</link>
      <pubDate>Mon, 10 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1107/</guid>
      <description>Buildup Although it is true that computers are overwhelmingly faster at numerical calculations than humans, it isn&amp;rsquo;t because they understand transcendental functions or irrational numbers. For instance, when asked to calculate $\displaystyle \sin {{ \pi } \over {6}} = {{1} \over { 2 }}$, instead of drawing a right triangle and finding the ratio of the hypotenuse to the height using the geometric definition of trigonometric functions, it uses polynomial</description>
    </item>
    <item>
      <title>Derivation of Newton&#39;s Forward Difference Formula</title>
      <link>https://freshrimpsushi.github.io/en/posts/1025/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1025/</guid>
      <description>Formulas For different data $x_{0} , \cdots , x_{n}$ of $(x_{0}, f(x_{0} )) , \cdots , (x_{n} , f( x_{n} ) )$, $$ p_{n} (x) =\sum_{i=0}^{n} f [ x_{0} , \cdots , x_{i} ] \prod_{j=0}^{i-1} (x - x_{j} ) $$ Description Though it seems complicated, when actually expanding for $n=0,1,2$, it simplifies as follows. $$ \begin{align*} p_{0} (x) =&amp;amp; f(x_{0}) \\ p_{1} (x) =&amp;amp; f( x_{0} ) + (x -</description>
    </item>
    <item>
      <title>Lagrange&#39;s Formula Derivation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1023/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1023/</guid>
      <description>Formula 1 Given different $x_{0} , \cdots , x_{n}$ data $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$, let&amp;rsquo;s say $\displaystyle l_{i} (x) := \prod_{i \ne j} \left( {{ x - x_{j} } \over { x_{i} - x_{j} }} \right)$, then $$ p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X) $$ Description Lagrange&amp;rsquo;s formula is the simplest method among those to find polynomial interpolation. Derivation Strategy: Prove that $l_{i}$ is the</description>
    </item>
    <item>
      <title>Polynomial Interpolation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1021/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1021/</guid>
      <description>Definition 1 For different $x_{0} , \cdots , x_{n}$ data $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$, a polynomial function $p$ that satisfies $p (x_{i} ) = y_{i}$ and $\deg p \le n$ is called Polynomial Interpolation. Theorem Existence and Uniqueness [1]: For the given data, there exists a unique $p$. Lagrange&amp;rsquo;s Formula [2]: $$p_{n} (x) = \sum_{i=0}^{n} y_{i} l_{i} (X)$$ Newton&amp;rsquo;s Divided Difference Formula [3]: $$p_{n} (x) =</description>
    </item>
    <item>
      <title>Interpolation in Numerical Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1016/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1016/</guid>
      <description>Definition 1 For a given pair of data $(n+1)$ and $(x_{0}, y_{0}) , \cdots , (x_{n} , y_{n})$, the method or the function itself that satisfies $f (x_{i} ) = y_{i}$ while possessing some specific property is called interpolation. Description For example, consider the situation where there&amp;rsquo;s data available as shown above, but the middle part is missing. Of course, it&amp;rsquo;s best to have actual data, but if not, there</description>
    </item>
    <item>
      <title>Differential Stages in Numerical Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/969/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/969/</guid>
      <description>Definition A Divided Difference of a function $f : \mathbb{R} \to \mathbb{R}$ for distinct $x_{1} , \cdots , x_{n}$ is defined as follows: $$ \begin{align*} f[x_{0}] :=&amp;amp; f( x_{0} ) \\ f [ x_{0} , x_{1} ] :=&amp;amp; {{ f ( x_{1} ) - f ( x_{0} ) } \over { x_{1} - x_{0} }} \\ f [ x_{0} , x_{1} , x_{2} ] :=&amp;amp; {{ f [ x_{1} ,</description>
    </item>
    <item>
      <title>Second Kind Chebyshev Polynomials</title>
      <link>https://freshrimpsushi.github.io/en/posts/779/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/779/</guid>
      <description>Definition $$U_{n} (x) := {{1} \over {n+1} } T_{n+1} &amp;rsquo; (x) = {{\sin \left( ( n +1 ) \theta \right)} \over { \sin \theta }} $$ is called the second kind Chebyshev polynomial. Basic Properties Recursive Formula [0]: $$U_{n+1} (x) = 2x U_{n} (x) - U_{n-1} (X)$$ Orthogonal Set [1] Inner product of functions: Given the weight $w$ for $\displaystyle \left&amp;lt;f, g\right&amp;gt;:=\int_a^b f(x) g(x) w(x) dx$ as $\displaystyle w(x) :=</description>
    </item>
    <item>
      <title>First kind Chebyshev polynomials</title>
      <link>https://freshrimpsushi.github.io/en/posts/777/</link>
      <pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/777/</guid>
      <description>Definition 1 $T_{n} (x) = \cos \left( n \cos^{-1} x \right)$ is called the first kind Chebyshev polynomial. Basic Properties Recurrence Formula [0]: $$T_{n+1} (x) = 2x T_{n} (x) - T_{n-1} (X)$$ Orthogonal Set [1] Inner product of functions: Given the weight $w$ as $\displaystyle w(x) := {{1} \over { \sqrt{1 - x^2} }}$, $\left\{ T_{0} , T_{1}, T_{2}, \cdots \right\}$ forms an orthogonal set. Chebyshev Nodes [2]: The roots</description>
    </item>
  </channel>
</rss>
