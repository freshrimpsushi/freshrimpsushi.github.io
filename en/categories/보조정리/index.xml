<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Lemmas on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%EB%B3%B4%EC%A1%B0%EC%A0%95%EB%A6%AC/</link><description>Recent content in Lemmas on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 28 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%EB%B3%B4%EC%A1%B0%EC%A0%95%EB%A6%AC/index.xml" rel="self" type="application/rss+xml"/><item><title>Harmonic Mean</title><link>https://freshrimpsushi.github.io/en/posts/2530/</link><pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2530/</guid><description>Definition For a positive number $a,b &amp;gt; 0$, the following is called the Harmonic Mean. $$ H (a,b) := 2 \left( {{ 1 } \over { a }} + {{ 1 } \over { b }} \right)^{-1} = {{ 2 ab } \over { a + b }} $$ The generalized form with $n$ terms $x_{1} , \cdots , x_{n}$ is as follows. $$ H \left( x_{1} , \cdots ,</description></item><item><title>Fixed Points in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/2477/</link><pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2477/</guid><description>Definition A Fixed Point is said to be a $x_{0} \in X$ that satisfies the following condition for the function $f : X \to X$. $$ f \left( x_{0} \right) = x_{0} $$ When the derivative $f '$ of $f$ is given, the following is also referred to as a fixed point. $$ f ' \left( x_{0} \right) = 0 $$ Explanation In universal mathematics, the concept of fixed points</description></item><item><title>Difference Between Root and Solution</title><link>https://freshrimpsushi.github.io/en/posts/2475/</link><pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2475/</guid><description>Definitions A point of the domain that makes the function value of a given function become $0$ is called a root. Something that satisfies the conditions of a given problem is called a solution. Explanation In short, a root is something formal, and a solution is something conceptual. Many people not interested in mathematics get confused by these terms because in many cases, they are used interchangeably. $$ f(x) =</description></item><item><title>Meaning of Weak and Strong in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/3491/</link><pubDate>Sat, 14 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3491/</guid><description>Description In mathematics, &amp;ldquo;weak&amp;rdquo; means &amp;ldquo;(logically) loose, less strict, less rigorous&amp;rdquo;. Being &amp;ldquo;less&amp;rdquo; something is to be understood in a relative sense. Conversely, &amp;ldquo;strong&amp;rdquo; means that the condition is (relatively) strict. In simple terms, &amp;ldquo;weak&amp;rdquo; can be translated as &amp;ldquo;in effect, frankly&amp;rdquo;. For example, if we talk about college entrance examination scores, the top cumulative 4% of scores are awarded the 1st grade. Strictly, accurately speaking, it&amp;rsquo;s correct that only</description></item><item><title>Cumulative Average Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2455/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2455/</guid><description>Formula Given a sample $x_{1} , \cdots , x_{n}$ with a sample mean of $y_{n}$, when a new sample $x_{n+1}$ is provided, the overall sample mean $y_{n+1}$ is as follows. $$ y_{n+1} := {{ n } \over {n + 1}} y_{n} + {{1} \over {n+1}} x_{n+1} $$ Description Cumulative Average is also called Moving Average or Running Average. Itâ€™s a mistake anyone can make at least once</description></item><item><title>Definition of Intervals in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/2453/</link><pubDate>Wed, 27 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2453/</guid><description>Definition $$ [a,b] := \left\{ x \in \mathbb{R} : a \le x \le b \right\} \subset \mathbb{R} $$ For two real numbers $a \le b$, the set as described above is called an Interval. In particular, if both endpoints $a,b$ are included, it is notated as $\left[ a,b \right]$ using square brackets [] and is said to be Closed. If both endpoints $a,b$ are not included, it is notated as</description></item><item><title>Definition of Pi</title><link>https://freshrimpsushi.github.io/en/posts/2451/</link><pubDate>Sat, 23 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2451/</guid><description>Definitions Geometric Definition A circle is defined as the set of points in a plane that are at a given distance $r &amp;gt; 0$ from a given point. The ratio of a circle&amp;rsquo;s circumference $l$ to its diameter $2r$ is defined as the Pi $\pi$. $$ \pi := {{ l } \over { 2r }} $$ Analytical Definition 1 $$ E (z) := \sum_{k=0}^{\infty} {{ z^{k} } \over { k!</description></item><item><title>Inequalities for the Logarithmic Function 1-1/x &lt; log x &lt; x-1</title><link>https://freshrimpsushi.github.io/en/posts/3468/</link><pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3468/</guid><description>Theorem For a logarithmic function with base $e$, the following inequality holds: $$ 1 - \dfrac{1}{x} \le \ln x \le x - 1\qquad \text{ for } x \gt 0 $$ Proof1 Part 1. $\ln x \le x - 1$ Let&amp;rsquo;s set it as $f(x) = x - 1 - \ln x$. Differentiating it gives, $f^{\prime}(x) = 1 - \dfrac{1}{x}$ $(x&amp;gt;0)$. At $0 \lt x \lt 1$, it is $f^{\prime} \lt</description></item><item><title>Proof of the Cauchy-Schwarz Inequality in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2274/</link><pubDate>Tue, 04 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2274/</guid><description>Theorem For a random variable $X, Y$, the following holds. $$ \text{Cov} (X,Y) \le \text{Var} X \text{Var} Y $$ The necessary and sufficient condition for the equality to hold is as follows1. $$ \exist a \ne 0 , b \in \mathbb{R} : a X + b = Y $$ Proof Let&amp;rsquo;s denote the population means of $X,Y$ as $\mu_{X}$ and $\mu_{Y}$, respectively. $$ \begin{align*} h(t) :=&amp;amp; E \left( \left[ \left(</description></item><item><title>Product of Indicator Functions</title><link>https://freshrimpsushi.github.io/en/posts/2243/</link><pubDate>Wed, 03 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2243/</guid><description>Theorem For $x_{1} , \cdots , x_{n} \in \mathbb{R}$ and constant $\theta \in \mathbb{R}$, the product of $I_{\cdot} \left( x_{i} \right)$ is as follows: $$ \prod_{i=1}^{n} I_{[\theta,\infty)} \left( x_{i} \right) = I_{[\theta,\infty)} \left( \min_{i \in [n]} x_{i} \right) $$ $I_{A}$ is the indicator function for the set $A$. $$ I_{A} (x) = \begin{cases} 1 &amp;amp; , x \in A \\ 0 &amp;amp; , x \notin A \end{cases} $$ Proof Regardless</description></item><item><title>Summarizing Inequalities in the Form of an Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2241/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2241/</guid><description>Theorem Given positive numbers $x_{1} , \cdots , x_{n}$ and $a_{1} , \cdots , a_{n} &amp;gt; 0$, and constant $\theta \in \mathbb{R}$. $$ \forall i \in [n] : x_{i} &amp;lt; a_{i} \theta \iff \max_{i \in [n]} {{ x_{i} } \over { a_{i} }} &amp;lt; \theta $$ Theorem For all $(\implies)$, that $i \in [n]$ satisfies $x_{i} / a_{i} &amp;lt; \theta$ implies that even the greatest $x_{i} / a_{i}$ is less</description></item><item><title>Proof of Gronwall's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2131/</link><pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2131/</guid><description>Theorem Let&amp;rsquo;s assume there are two continuous functions $f,w : I \to \mathbb{R}$ defined in an interval $I \subset \mathbb{R}$ that contains the minimum value $a \in \mathbb{R}$. If $w$ is $\forall t \in I$ in $w(t) \ge 0$ and for some constant $C \in \mathbb{R}$, $$ f(t) \le C + \int_{a}^{t} w(s) f(s) ds \qquad , \forall t \in I $$ then the following holds. $$ f(t) \le C</description></item><item><title>Integrals of Trigonometric Functions Table</title><link>https://freshrimpsushi.github.io/en/posts/3107/</link><pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3107/</guid><description>Formulas $$ \begin{equation} \int_{0}^{\pi / 2} \sin \theta \cos \theta d \theta = \dfrac{1}{2} \end{equation} $$ $$ \begin{equation} \begin{aligned} \int \cos^{2}\theta d\theta &amp;amp;= \dfrac{1}{2}\theta + \dfrac{1}{4}\sin 2\theta + C \\ \int_{0}^{2\pi} \cos^{2}\theta d\theta &amp;amp;= \pi \\ \int_{0}^{\pi} \cos^{2}\theta d\theta &amp;amp;= \dfrac{\pi}{2} \end{aligned} \end{equation} $$ $$ \begin{equation} \begin{aligned} \int \sin^{2}\theta d\theta &amp;amp;= \dfrac{1}{2}\theta - \dfrac{1}{4}\sin 2\theta + C \\ \int_{0}^{2\pi} \sin^{2}\theta d\theta &amp;amp;= \pi \\ \int_{0}^{\pi} \sin^{2}\theta d\theta &amp;amp;= \dfrac{\pi}{2} \end{aligned}</description></item><item><title>Generalization of Gaussian Integrals</title><link>https://freshrimpsushi.github.io/en/posts/3105/</link><pubDate>Sun, 29 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3105/</guid><description>Formulas1 For an integer $n \ge 0$, the following expressions are true. When multiplied by an even degree polynomial $$ \int_{-\infty}^{\infty} x^{2n} e^{-\alpha x^{2}}dx = \dfrac{(2n)!}{n! 2^{2n}}\sqrt{\dfrac{\pi}{\alpha^{2n+1}}} $$ $$ \int_{0}^{\infty} x^{2n} e^{-\alpha x^{2}}dx = \dfrac{(2n)!}{n! 2^{2n+1}}\sqrt{\dfrac{\pi}{\alpha^{2n+1}}} $$ When multiplied by an odd degree polynomial $$ \int_{-\infty}^{\infty} x^{2n+1} e^{-\alpha x^{2}}dx = 0 $$ $$ \int_{0}^{\infty} x^{2n+1} e^{-\alpha x^{2}}dx = \dfrac{n!}{2 \alpha^{n+1}} $$ Explanation Gaussian Integral $$ \int_{-\infty}^{\infty} e^{-\alpha x^2} dx= \sqrt{\dfrac{\pi}{\alpha}}</description></item><item><title>Proof of the Expected Form of Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/266/</link><pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/266/</guid><description>Theorem 1 In the open interval $I$, if the function $\phi$ is convex and twice differentiable, the expected value $X$ of the random variable exists, and $X \subset I $ then $$ \phi [ E(X) ] \le E [ \phi (X)] $$ Other Forms Jensen&amp;rsquo;s Inequality in Finite Form Jensen&amp;rsquo;s Inequality in Integral Form Conditional Jensen&amp;rsquo;s Inequality It has a form quite similar to the integral form. Upon closer inspection,</description></item><item><title>Chebyshev's Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/34/</link><pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/34/</guid><description>Theorem 1 If the variance $\sigma^2 &amp;lt; \infty$ of a random variable $X$ exists for some positive number $k&amp;gt;0$, then $$ P(|X-\mu| \ge k\sigma) \le {1 \over k^2} $$ Explanation It is relatively simple in form and easy to manipulate, and the results are immediately apparent, making it widely used as a lemma. However, compared to Markov&amp;rsquo;s inequality, there is one more condition that the variance must exist. One might</description></item><item><title>Markov Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/33/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/33/</guid><description>Theorem 1 Let&amp;rsquo;s define a function $u(X) \ge 0$ for the random variable $X$. If $E \left( u(X) \right)$ exists, then for $c &amp;gt; 0$ $$ P(u(X) \ge c) \le {E \left( u(X) \right) \over c} $$ Explanation There is the Chebyshev&amp;rsquo;s inequality, which makes it more convenient to use as an auxiliary lemma in numerous proofs. You might consider the condition that the $1$th moment must exist as too</description></item><item><title>Proof of Bernoulli's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1126/</link><pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1126/</guid><description>Theorem If we call it $\alpha &amp;gt; 0$, for all $x \in [ - 1, \infty )$, the following two inequalities hold: [1]: $\alpha \in (0, 1] \implies (1 + x )^{\alpha } \le 1 + \alpha x $ [2] $\alpha \in (1, \infty] \implies (1 + x )^{\alpha } \ge 1 + \alpha x $ Explanation Looking closely at the shape of the inequalities, although it depends on the</description></item><item><title>How to Move the Big O Notation from Denominator to Numerator</title><link>https://freshrimpsushi.github.io/en/posts/727/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/727/</guid><description>Theorem The following holds for $a \ne 0$, $p&amp;gt;0$, and $n \in \mathbb{N}$. $$ {{1} \over { \sqrt[p]{a + O ( h^n ) } }} = {{1} \over { \sqrt[p]{a } }}+ O(h^n) $$ Explanation It serves as a handy lemma for converting complex denominators into cleaner forms. If there were no constant term $a$, it could neatly rise to $\displaystyle {{1} \over { \sqrt[p]{ O ( h^n ) }</description></item><item><title>Proof of Young's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/267/</link><pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/267/</guid><description>Theorem Given constants $\displaystyle {{1} \over {p}} + {{1} \over {q}} = 1$ that satisfy the condition and two positive numbers $p,q$ and $a,b$ which are greater than 1, $$ ab \le { {a^{p}} \over {p} } + {{b^{q}} \over {q}} $$ Description Apart from the aesthetically pleasing algebraic aspect, this inequality is not often mentioned except when proving the HÃ¶lder inequality. Proof Since both</description></item><item><title>Proof of the Finite Form of Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/264/</link><pubDate>Mon, 08 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/264/</guid><description>Theorem In $I \subset \mathbb{R}$, for the convex functions $f : I \to \mathbb{R}$ and $\displaystyle \sum_{k=1}^{n} \lambda_{k} = 1, \lambda_{k}&amp;gt;0$ $$ \begin{align*} f( \lambda_{1} x_{1} + \lambda_{2} x_{2} + \cdots + \lambda_{n} x_{n} ) &amp;amp; \le \lambda_{1} f( x_{1}) + \lambda_{2} f( x_{2}) + \cdots + \lambda_{n} f( x_{n} ) \\ f\left( \sum\limits_{k=1}^{n}\lambda_{k}x_{k} \right) &amp;amp;\le \sum\limits_{k=1}^{n} \lambda_{k} f(x_{k}) \end{align*} $$ If $f$ is a concave function, then the opposite</description></item><item><title>Indefinite Integral of the Form e^{x^2}</title><link>https://freshrimpsushi.github.io/en/posts/487/</link><pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/487/</guid><description>Theorem $$ \int e^{x^2}dx = \sum\limits_{n=0}^\infty \dfrac{x^{2n+1}}{(2n+1)n!}+C $$ Explanation Just like the form $e^{-x^{2}}$, it is difficult to integrate using general methods. There is a method to integrate by defining the error function, imaginary error function, erfi, but this article introduces solving it using Taylor series expansion. Proof By the method of Taylor series expansion, $$ e^{x} = \sum\limits_{n=0}^{\infty} \dfrac{x^{n}}{n!} = 1 + x + \dfrac{x^{2}}{2!} + \cdots + \dfrac{x^{n}}{n!}</description></item><item><title>Proof of the Integral Form of Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/265/</link><pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/265/</guid><description>Theorem Given a convex function $ \phi : [a,b] \to \mathbb{R}$ and $f: [0,1] \to [a,b]$, if $\phi \circ f$ is integrable over $[0,1]$, then $$ \phi \left( \int_{0}^{1} f(x) dx \right) \le \int_{0}^{1} (\phi \circ f ) (x) dx $$ Explanation Of course, given the conditions, the integration interval can also be changed through substitution, etc. Unlike finite form, which generalizes the number of terms using definitions, integration form</description></item><item><title>Definite Integration of the form e^-x^2, Gaussian Integral, Euler-Poisson Integral</title><link>https://freshrimpsushi.github.io/en/posts/219/</link><pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/219/</guid><description>Theorem The Gaussian function $f(x) := e^{-x^2}$&amp;rsquo;s integral over the entire domain is as follows. $$ \int_{-\infty}^{\infty} e^{-x^2} dx= \sqrt{\pi} $$ Description Physicist Kelvin is said to have left the remark that &amp;ldquo;one who finds this integral obvious is a mathematician&amp;rdquo;. It is also known by other names such as Gaussian integral, or Euler-Poisson integral. It&amp;rsquo;s a shocking integration for high school students and especially crucial for statistics. That&amp;rsquo;s because,</description></item><item><title>Cauchy-Schwarz Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/51/</link><pubDate>Mon, 03 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/51/</guid><description>Theorem $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\ge { (ax+by) }^{ 2 } $$ Proof $$ \begin{align*} &amp;amp; ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})-{ (ax+by) }^{ 2 } \\ =&amp;amp; {a}^{2}{x}^{2}+{b}^{2}{x}^{2}+{a}^{2}{y}^{2}+{b}^{2}{y}^{2}-{ (ax+by) }^{ 2 } \\ =&amp;amp; {b}^{2}{x}^{2}+{a}^{2}{y}^{2}-2axby \\ =&amp;amp; { (ay-bx) }^{ 2 } \\ \ge&amp;amp; 0 \end{align*} $$ Thus, we can summarize as follows. $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\ge { (ax+by) }^{ 2 } $$ â–  Explanation This inequality, which can be encountered as early as high school, is used widely</description></item><item><title>Arithmetic, Geometric, and Harmonic Means Inequality</title><link>https://freshrimpsushi.github.io/en/posts/3/</link><pubDate>Tue, 14 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3/</guid><description>Definitions For $n$ positive numbers ${x}_1,{x}_2,\cdots,{x}_n$, the arithmetic mean, geometric mean, and harmonic mean are defined as: Arithmetic Mean : $$ \sum_{ k=1 }^{ n }{ \frac { {x}_k }{ n } }=\frac { {x}_1+{x}_2+\cdots+{x}_n }{ n } $$ Geometric Mean : $$ \prod_{ k=1 }^{ n }{ { {x}_k }^{ \frac { 1 }{ n } } }=\sqrt [ n ]{ {x}_1{x}_2\cdots{x}_n } $$ Harmonic Mean : $$ \left(</description></item><item><title>Integration of 1/(1+x^2)</title><link>https://freshrimpsushi.github.io/en/posts/3570/</link><pubDate>Thu, 20 Mar 2014 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3570/</guid><description>Formulas Definite Integral $$ \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx = \pi $$ Indefinite Integral $$ \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx = \pi $$ $C$ is the integration constant. Proofs Definite Integral Let&amp;rsquo;s substitute with $x = \tan \theta$. Then, the range of integration becomes $\displaystyle \int_{-\infty}^{\infty} \to \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}$, and since $\tan ^{\prime} = \sec^{2}$, it results in $dx = \sec^{2} d\theta$. $$ \begin{align*} \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx &amp;amp;= \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \dfrac{1}{1 + \tan^{2}\theta} \sec^{2} \theta d\theta \\ &amp;amp;= \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}</description></item><item><title>The Limit of 1-cos(x)/x</title><link>https://freshrimpsushi.github.io/en/posts/3568/</link><pubDate>Sun, 16 Mar 2014 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3568/</guid><description>Formula $$ \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} = 0 $$ Proof $$ \begin{align*} \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} &amp;amp;= \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} \dfrac{1 + \cos x}{1 + \cos x} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{1 - \cos^{2} x}{x(1+\cos x)} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{\sin^{2}x}{x(1+\cos x)} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{\sin x}{x} \dfrac{\sin x}{1+\cos x}</description></item><item><title>Limit of sinx/x</title><link>https://freshrimpsushi.github.io/en/posts/3567/</link><pubDate>Fri, 14 Mar 2014 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3567/</guid><description>Formulas $$ \lim \limits_{x \to 0} \dfrac{\sin x}{x} = 1 $$ Proof Using L&amp;rsquo;HÃ´pital&amp;rsquo;s Rule Since $\lim\limits_{x \to 0} \sin x = 0 = \lim\limits_{x \to 0} x$, by L&amp;rsquo;HÃ´pital&amp;rsq</description></item><item><title>Integrability of 1/x^p</title><link>https://freshrimpsushi.github.io/en/posts/3564/</link><pubDate>Sat, 08 Mar 2014 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3564/</guid><description>Theorem The integrability of function $f(x) = \dfrac{1}{x^{p}}$ is as follows: when $x \in (0,1]$, if $p \lt 1$, then $f$ is integrable. when $x \in [1, \infty)$, if $p \gt 1$, then $f$ is integrable. Explanation If $x$ is less than $1$, then $p$ must also be less than $1$, and if $x$ is greater than $1$, then $p$ must also be greater than $1$. Just remember this. Proof</description></item></channel></rss>