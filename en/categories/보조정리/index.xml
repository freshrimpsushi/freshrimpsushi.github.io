<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Lemmas on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%EB%B3%B4%EC%A1%B0%EC%A0%95%EB%A6%AC/</link><description>Recent content in Lemmas on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 04 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%EB%B3%B4%EC%A1%B0%EC%A0%95%EB%A6%AC/index.xml" rel="self" type="application/rss+xml"/><item><title>Maintaining Generality in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/2720/</link><pubDate>Sat, 04 Oct 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2720/</guid><description>Terminology 1 The expression &amp;ldquo;&amp;quot;Without loss of generality assume $P$&amp;quot;&amp;rdquo; is mainly used in mathematical proofs to mean that, even if one specifically assumes $P$, it does not affect the overall argument. Examples Simple example As a very simple example, consider proving the proposition &amp;ldquo;For two real numbers $x, y$, if $xy = 0$ then $x = 0$ or $y = 0$&amp;rdquo;. Since this is only an example to understand</description></item><item><title>Geometric Mean</title><link>https://freshrimpsushi.github.io/en/posts/3677/</link><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3677/</guid><description>Definition The value for two positive numbers $a, b$ is called the geometric mean of $a$ and $b$. $$ \sqrt{ab} $$ Generalization For $n$ positive numbers $a_{1}, \dots, a_{n}$, the following value is called the geometric mean of $a_{1}, \dots, a_{n}$. $$ \sqrt[n]{a_{1}a_{2}\cdots a_{n}} $$ Explanation If one considers an extension to complex numbers, then $a_{i}$ do not necessarily have to be positive. One who encounters the geometric mean for</description></item><item><title>Professor Choi Byung-sun's Freely Distributed Textbooks on Mathematics, Statistics, and Economics</title><link>https://freshrimpsushi.github.io/en/posts/3659/</link><pubDate>Thu, 19 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3659/</guid><description>Description Professor Byung-Sun Choi, a former professor at Yonsei University in the Department of Applied Statistics and a current professor in the Department of Economics at Seoul National University, has published 12 textbooks on mathematics, statistics, and economics for free online. Each can be downloaded as a PDF from the links below. List 1995 Multivariate Time Series Analysis (Link) 1995 Introduction to SAS/IML (Link) 1997 Regression Analysis Volume I (Link)</description></item><item><title>Birthday Problem: Probability of Sharing the Same Birthday</title><link>https://freshrimpsushi.github.io/en/posts/998/</link><pubDate>Sat, 12 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/998/</guid><description>Formula Disregarding leap years, let $1$ be the number of years and $365$ be the number of days in a year. The probability that there is at least one pair of people with the same birthday among $n$ people is given by the following formula. $$ p(n) = 1 - \dfrac{365!}{365^n(365-n)!} $$ Explanation I recall that during my middle school years, this formula was introduced in a particular chapter of</description></item><item><title>The limit of n^(1/n)</title><link>https://freshrimpsushi.github.io/en/posts/1649/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1649/</guid><description>공식 $$ \lim \limits_{n \to \infty} \sqrt[n]{n} = 1 $$ $$ \lim \limits_{n \to \infty} \sqrt[n]{\dfrac{1}{n}} = 1 $$ 증명 $\sqrt[n]{n}$ 대신 $\ln \sqrt[n]{n}$의 극한을 구하면 쉽다. $$ \lim\limits_{n \to \infty} \ln \sqrt[n]{n} = \lim\limits_{n \to \infty} \dfrac{\ln n}{n} $$ $\dfrac{\infty}{\infty}$ 꼴이므로 로피탈 정</description></item><item><title>Recurrence Relation</title><link>https://freshrimpsushi.github.io/en/posts/1865/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1865/</guid><description>Definition Let&amp;rsquo;s consider a sequence $\left\{ a_{n} \right\}$. At this time, expressing $a_{n}$ as a function of $a_{n-1}$, $a_{n-2}$, $\cdots$, and $a_{1}$ is called a recurrence relation. Explanation For instance, the sequence of natural numbers $\left\{ 1, 2, 3, 4, \dots \right\}$ can be expressed by the following recurrence relation. $$ a_{n} = a_{n-1} + 1, \qquad a_{1} = 1 $$ The coefficients of the Legendre polynomial are expressed by</description></item><item><title>Continued Fraction</title><link>https://freshrimpsushi.github.io/en/posts/1876/</link><pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1876/</guid><description>Definition A fraction of the form as shown below is called a continued fraction. $$ a_{0} + \dfrac{1}{a_{1} + \dfrac{1}{a_{2} + \dfrac{1}{a_{3} + \dfrac{1}{\ddots + \dfrac{1}{a_{n}}}}}} \tag{1} $$ Explanation1 2 $(1)$ is denoted as $[a_{1}, a_{2}, \dots, a_{n}]$. Naturally, one can also consider taking the limit of it. For example, let&amp;rsquo;s consider a sequence with the recurrence relation given by $a_{n+1} = 1 + \dfrac{1}{1 + a_{n}}$ and $a_{1} =</description></item><item><title>Hamming Distance</title><link>https://freshrimpsushi.github.io/en/posts/78/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/78/</guid><description>Definition For a natural number $n \in \mathbb{N}$, the Hamming distance is defined in the set $\left\{ 0, 1 \right\}^{n}$ of code points of length $n$ as the distance function $H$ given below1. $$ H \left( \mathbf{x} , \mathbf{y} \right) = \operatorname{card} \left\{ k : x_{k} \ne y_{k} \right\} $$ Here, code points are represented by $\mathbf{x} = \left( x_{1} , \cdots , x_{n} \right)$ and $\mathbf{y} = \left( y_{1}</description></item><item><title>What is a Closed Form in Mathematics?</title><link>https://freshrimpsushi.github.io/en/posts/2576/</link><pubDate>Thu, 30 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2576/</guid><description>Glossary 1 In mathematics, a closed form expression of a function refers to an expression constructed from a finite number of symbols and arithmetic operations $+, -, \cdot, \div$, as well as several well-known functions. These well-known functions include radicals $\sqrt[n]{\cdot}$, exponential functions $\exp$, logarithmic functions $\log$, trigonometric functions $\sin , \cos$, factorials $\cdot !$, etc. Typically, sums $\sum$, products $\prod$, integrals $\int$, and limits $\lim$ are not included. Examples</description></item><item><title>Integration of 1/(1+x^2)</title><link>https://freshrimpsushi.github.io/en/posts/3570/</link><pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3570/</guid><description>Formulas Definite Integral $$ \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx = \pi $$ Indefinite Integral $$ \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx = \pi $$ $C$ is the integration constant. Proofs Definite Integral Let&amp;rsquo;s substitute with $x = \tan \theta$. Then, the range of integration becomes $\displaystyle \int_{-\infty}^{\infty} \to \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}$, and since $\tan ^{\prime} = \sec^{2}$, it results in $dx = \sec^{2} d\theta$. $$ \begin{align*} \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx &amp;amp;= \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \dfrac{1}{1 + \tan^{2}\theta} \sec^{2} \theta d\theta \\ &amp;amp;= \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}</description></item><item><title>The Limit of 1-cos(x)/x</title><link>https://freshrimpsushi.github.io/en/posts/3568/</link><pubDate>Sat, 16 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3568/</guid><description>Formula $$ \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} = 0 $$ Proof $$ \begin{align*} \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} &amp;amp;= \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} \dfrac{1 + \cos x}{1 + \cos x} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{1 - \cos^{2} x}{x(1+\cos x)} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{\sin^{2}x}{x(1+\cos x)} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{\sin x}{x} \dfrac{\sin x}{1+\cos x}</description></item><item><title>The limit of sin(x)/x</title><link>https://freshrimpsushi.github.io/en/posts/3567/</link><pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3567/</guid><description>공식 $$ \lim \limits_{x \to 0} \dfrac{\sin x}{x} = 1 $$ 증명 반지름이 $1$인 부채꼴 $OAB$가 주어졌다고 하자. 점 $B$에서 선분 $\overline{OA}$로 내린 수선의 발을</description></item><item><title>Integrability of 1/x^p</title><link>https://freshrimpsushi.github.io/en/posts/3564/</link><pubDate>Fri, 08 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3564/</guid><description>Theorem The integrability of function $f(x) = \dfrac{1}{x^{p}}$ is as follows: when $x \in (0,1]$, if $p \lt 1$, then $f$ is integrable. when $x \in [1, \infty)$, if $p \gt 1$, then $f$ is integrable. Explanation If $x$ is less than $1$, then $p$ must also be less than $1$, and if $x$ is greater than $1$, then $p$ must also be greater than $1$. Just remember this. Proof</description></item><item><title>Harmonic Mean</title><link>https://freshrimpsushi.github.io/en/posts/2530/</link><pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2530/</guid><description>Definition For a positive number $a,b &amp;gt; 0$, the following is called the Harmonic Mean. $$ H (a,b) := 2 \left( {{ 1 } \over { a }} + {{ 1 } \over { b }} \right)^{-1} = {{ 2 ab } \over { a + b }} $$ The generalized form with $n$ terms $x_{1} , \cdots , x_{n}$ is as follows. $$ H \left( x_{1} , \cdots ,</description></item><item><title>Square Root Expansion Formula</title><link>https://freshrimpsushi.github.io/en/posts/3519/</link><pubDate>Sat, 09 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3519/</guid><description>Formula When $a \gt b$, $$ \sqrt{a + b \pm 2\sqrt{a b}} = \sqrt{a} \pm \sqrt{b} $$ Explanation It might seem incredibly difficult to solve with two roots, but it can be directly solved in a perfect square form. Example $$ \begin{align*} \sqrt{13 - 2\sqrt{12}} &amp;amp;= \sqrt{12 + 1 - 2\sqrt{12 \cdot 1}} \\ &amp;amp;= \sqrt{(\sqrt{12})^{2} + (\sqrt{1})^{2} - 2\sqrt{12}\sqrt{1}} \\ &amp;amp;= \sqrt{(\sqrt{12} - \sqrt{1})^{2}} \\ &amp;amp;= \sqrt{12} - \sqrt{1}</description></item><item><title>Fixed Points in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/2477/</link><pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2477/</guid><description>Definition A Fixed Point is said to be a $x_{0} \in X$ that satisfies the following condition for the function $f : X \to X$. $$ f \left( x_{0} \right) = x_{0} $$ When the derivative $f '$ of $f$ is given, the following is also referred to as a fixed point. $$ f ' \left( x_{0} \right) = 0 $$ Explanation In universal mathematics, the concept of fixed points</description></item><item><title>Difference Between Root and Solution</title><link>https://freshrimpsushi.github.io/en/posts/2475/</link><pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2475/</guid><description>Definitions A point of the domain that makes the function value of a given function become $0$ is called a root. Something that satisfies the conditions of a given problem is called a solution. Explanation In short, a root is something formal, and a solution is something conceptual. Many people not interested in mathematics get confused by these terms because in many cases, they are used interchangeably. $$ f(x) =</description></item><item><title>Meaning of Weak and Strong in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/3491/</link><pubDate>Sat, 14 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3491/</guid><description>Description In mathematics, &amp;quot;weak&amp;quot; means &amp;quot;(logically) loose, less strict, less rigorous&amp;quot;. Being &amp;quot;less&amp;quot; something is to be understood in a relative sense. Conversely, &amp;quot;strong&amp;quot; means that the condition is (relatively) strict. In simple terms, &amp;quot;weak&amp;quot; can be translated as &amp;quot;in effect, frankly&amp;quot;. For example, if we talk about college entrance examination scores, the top cumulative 4% of scores are awarded the 1st grade. Strictly, accurately speaking, it&amp;rsquo;s correct that only</description></item><item><title>Cumulative Average Formula Derivation</title><link>https://freshrimpsushi.github.io/en/posts/2455/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2455/</guid><description>Formula Given a sample $x_{1} , \cdots , x_{n}$ with a sample mean of $y_{n}$, when a new sample $x_{n+1}$ is provided, the overall sample mean $y_{n+1}$ is as follows. $$ y_{n+1} := {{ n } \over {n + 1}} y_{n} + {{1} \over {n+1}} x_{n+1} $$ Description Cumulative Average is also called Moving Average or Running Average. It’s a mistake anyone can make at least once</description></item><item><title>Definition of Intervals in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/2453/</link><pubDate>Wed, 27 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2453/</guid><description>Definition $$ [a,b] := \left\{ x \in \mathbb{R} : a \le x \le b \right\} \subset \mathbb{R} $$ For two real numbers $a \le b$, the set as described above is called an Interval. In particular, if both endpoints $a,b$ are included, it is notated as $\left[ a,b \right]$ using square brackets [] and is said to be Closed. If both endpoints $a,b$ are not included, it is notated as</description></item><item><title>Definition of Pi</title><link>https://freshrimpsushi.github.io/en/posts/2451/</link><pubDate>Sat, 23 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2451/</guid><description>Definitions Geometric Definition A circle is defined as the set of points in a plane that are at a given distance $r &amp;gt; 0$ from a given point. The ratio of a circle&amp;rsquo;s circumference $l$ to its diameter $2r$ is defined as the Pi $\pi$. $$ \pi := {{ l } \over { 2r }} $$ Analytical Definition 1 $$ E (z) := \sum_{k=0}^{\infty} {{ z^{k} } \over { k!</description></item><item><title>Inequalities for the Logarithmic Function 1-1/x &lt; log x &lt; x-1</title><link>https://freshrimpsushi.github.io/en/posts/3468/</link><pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3468/</guid><description>Theorem For a logarithmic function with base $e$, the following inequality holds: $$ 1 - \dfrac{1}{x} \le \ln x \le x - 1\qquad \text{ for } x \gt 0 $$ Proof1 Part 1. $\ln x \le x - 1$ Let&amp;rsquo;s set it as $f(x) = x - 1 - \ln x$. Differentiating it gives, $f^{\prime}(x) = 1 - \dfrac{1}{x}$ $(x&amp;gt;0)$. At $0 \lt x \lt 1$, it is $f^{\prime} \lt</description></item><item><title>Proof of the Cauchy-Schwarz Inequality in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2274/</link><pubDate>Tue, 04 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2274/</guid><description>Theorem For a random variable $X, Y$, the following holds. $$ \operatorname{Cov} (X,Y) \le \Var X \Var Y $$ The necessary and sufficient condition for the equality to hold is as follows1. $$ \exist a \ne 0 , b \in \mathbb{R} : a X + b = Y $$ Proof Let&amp;rsquo;s denote the population means of $X,Y$ as $\mu_{X}$ and $\mu_{Y}$, respectively. $$ \begin{align*} h(t) :=&amp;amp; E \left( \left[ \left(</description></item><item><title>Product of Indicator Functions</title><link>https://freshrimpsushi.github.io/en/posts/2243/</link><pubDate>Wed, 03 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2243/</guid><description>Theorem For $x_{1} , \cdots , x_{n} \in \mathbb{R}$ and constant $\theta \in \mathbb{R}$, the product of $I_{\cdot} \left( x_{i} \right)$ is as follows: $$ \prod_{i=1}^{n} I_{[\theta,\infty)} \left( x_{i} \right) = I_{[\theta,\infty)} \left( \min_{i \in [n]} x_{i} \right) $$ $I_{A}$ is the indicator function for the set $A$. $$ I_{A} (x) = \begin{cases} 1 &amp;amp; , x \in A \\ 0 &amp;amp; , x \notin A \end{cases} $$ Proof Regardless</description></item><item><title>Summarizing Inequalities in the Form of an Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2241/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2241/</guid><description>Theorem Let $x_{1} , \cdots , x_{n}$ and positive $a_{1} , \cdots , a_{n} &amp;gt; 0$ along with constant $\theta \in \mathbb{R}$ be given. $$ \forall i \in [n] : x_{i} &amp;lt; a_{i} \theta \iff \max_{i \in [n]} {{ x_{i} } \over { a_{i} }} &amp;lt; \theta $$ Proof The fact that $(\implies)$ holds for all $i \in [n]$ implies that even the largest $x_{i} / a_{i}$ is smaller than</description></item><item><title>Proof of Gronwall's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/2131/</link><pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2131/</guid><description>Theorem Let&amp;rsquo;s assume there are two continuous functions $f,w : I \to \mathbb{R}$ defined in an interval $I \subset \mathbb{R}$ that contains the minimum value $a \in \mathbb{R}$. If $w$ is $\forall t \in I$ in $w(t) \ge 0$ and for some constant $C \in \mathbb{R}$, $$ f(t) \le C + \int_{a}^{t} w(s) f(s) ds \qquad , \forall t \in I $$ then the following holds. $$ f(t) \le C</description></item><item><title>Integrals of Trigonometric Functions Table</title><link>https://freshrimpsushi.github.io/en/posts/3107/</link><pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3107/</guid><description>Formulas $$ \begin{equation} \int_{0}^{\pi / 2} \sin \theta \cos \theta d \theta = \dfrac{1}{2} \end{equation} $$ $$ \begin{equation} \begin{aligned} \int \cos^{2}\theta d\theta &amp;amp;= \dfrac{1}{2}\theta + \dfrac{1}{4}\sin 2\theta + C \\ \int_{0}^{2\pi} \cos^{2}\theta d\theta &amp;amp;= \pi \\ \int_{0}^{\pi} \cos^{2}\theta d\theta &amp;amp;= \dfrac{\pi}{2} \end{aligned} \end{equation} $$ $$ \begin{equation} \begin{aligned} \int \sin^{2}\theta d\theta &amp;amp;= \dfrac{1}{2}\theta - \dfrac{1}{4}\sin 2\theta + C \\ \int_{0}^{2\pi} \sin^{2}\theta d\theta &amp;amp;= \pi \\ \int_{0}^{\pi} \sin^{2}\theta d\theta &amp;amp;= \dfrac{\pi}{2} \end{aligned}</description></item><item><title>Generalization of Gaussian Integrals</title><link>https://freshrimpsushi.github.io/en/posts/3105/</link><pubDate>Sun, 29 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3105/</guid><description>Formulas1 For an integer $n \ge 0$, the following expressions are true. When multiplied by an even degree polynomial $$ \int_{-\infty}^{\infty} x^{2n} e^{-\alpha x^{2}}dx = \dfrac{(2n)!}{n! 2^{2n}}\sqrt{\dfrac{\pi}{\alpha^{2n+1}}} $$ $$ \int_{0}^{\infty} x^{2n} e^{-\alpha x^{2}}dx = \dfrac{(2n)!}{n! 2^{2n+1}}\sqrt{\dfrac{\pi}{\alpha^{2n+1}}} $$ When multiplied by an odd degree polynomial $$ \int_{-\infty}^{\infty} x^{2n+1} e^{-\alpha x^{2}}dx = 0 $$ $$ \int_{0}^{\infty} x^{2n+1} e^{-\alpha x^{2}}dx = \dfrac{n!}{2 \alpha^{n+1}} $$ Explanation Gaussian Integral $$ \int_{-\infty}^{\infty} e^{-\alpha x^2} dx= \sqrt{\dfrac{\pi}{\alpha}}</description></item><item><title>Proof of the Expected Form of Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/266/</link><pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/266/</guid><description>Theorem 1 In the open interval $I$, if the function $\phi$ is convex and twice differentiable, the expected value $X$ of the random variable exists, and $X \subset I $ then $$ \phi [ E(X) ] \le E [ \phi (X)] $$ Other Forms Jensen&amp;rsquo;s Inequality in Finite Form Jensen&amp;rsquo;s Inequality in Integral Form Conditional Jensen&amp;rsquo;s Inequality It has a form quite similar to the integral form. Upon closer inspection,</description></item><item><title>Chebyshev's Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/34/</link><pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/34/</guid><description>Theorem 1 If the variance $\sigma^2 &amp;lt; \infty$ of a random variable $X$ exists for some positive number $k&amp;gt;0$, then $$ P(|X-\mu| \ge k\sigma) \le {1 \over k^2} $$ Explanation It is relatively simple in form and easy to manipulate, and the results are immediately apparent, making it widely used as a lemma. However, compared to Markov&amp;rsquo;s inequality, there is one more condition that the variance must exist. One might</description></item><item><title>Markov Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/33/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/33/</guid><description>Theorem 1 Let&amp;rsquo;s define a function $u(X) \ge 0$ for the random variable $X$. If $E \left( u(X) \right)$ exists, then for $c &amp;gt; 0$ $$ P(u(X) \ge c) \le {E \left( u(X) \right) \over c} $$ Explanation There is the Chebyshev&amp;rsquo;s inequality, which makes it more convenient to use as an auxiliary lemma in numerous proofs. You might consider the condition that the $1$th moment must exist as too</description></item><item><title>Proof of Bernoulli's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1126/</link><pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1126/</guid><description>Theorem If we call it $\alpha &amp;gt; 0$, for all $x \in [ - 1, \infty )$, the following two inequalities hold: [1]: $\alpha \in (0, 1] \implies (1 + x )^{\alpha } \le 1 + \alpha x $ [2] $\alpha \in (1, \infty] \implies (1 + x )^{\alpha } \ge 1 + \alpha x $ Explanation Looking closely at the shape of the inequalities, although it depends on the</description></item><item><title>Rationalizing Fractions Containing Roots Quickly</title><link>https://freshrimpsushi.github.io/en/posts/874/</link><pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/874/</guid><description>Formulas $$ {{ x } \over { \sqrt{a} \pm \sqrt{b} }} = {{ x \left( \sqrt{a} \mp \sqrt{b} \right) } \over { a - b }} $$ Explanation Rationalizing fractions may conceptually be simple, but it becomes difficult due to the extensive calculations involved in multiplying complex terms to both the numerator and denominator and then simplifying them. However, by using the formula above, one can rationalize quickly and easily,</description></item><item><title>Bounded Linear Operators Squared Norm</title><link>https://freshrimpsushi.github.io/en/posts/738/</link><pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/738/</guid><description>Terminology 1 Describing platform-independent and deeply technical code using natural language is referred to as pseudo code. Description Pseudo code is a representation that appears like code but is not actual code. It is used to illustrate algorithms without being constrained by any specific programming language. Pseudo code can utilize natural language as well as mathematical expressions, and as long as its definition is clear and not confusing, it can</description></item><item><title>How to Move the Big O Notation from Denominator to Numerator</title><link>https://freshrimpsushi.github.io/en/posts/727/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/727/</guid><description>Theorem The following holds for $a \ne 0$, $p&amp;gt;0$, and $n \in \mathbb{N}$. $$ {{1} \over { \sqrt[p]{a + O ( h^n ) } }} = {{1} \over { \sqrt[p]{a } }}+ O(h^n) $$ Explanation It serves as a handy lemma for converting complex denominators into cleaner forms. If there were no constant term $a$, it could neatly rise to $\displaystyle {{1} \over { \sqrt[p]{ O ( h^n ) }</description></item><item><title>Proof of Young's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/267/</link><pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/267/</guid><description>Theorem Given constants $\displaystyle {{1} \over {p}} + {{1} \over {q}} = 1$ that satisfy the condition and two positive numbers $p,q$ and $a,b$ which are greater than 1, $$ ab \le { {a^{p}} \over {p} } + {{b^{q}} \over {q}} $$ Description Apart from the aesthetically pleasing algebraic aspect, this inequality is not often mentioned except when proving the Hölder inequality. Proof Since both</description></item><item><title>Proof of the Finite Form of Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/264/</link><pubDate>Mon, 08 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/264/</guid><description>Theorem In $I \subset \mathbb{R}$, for the convex functions $f : I \to \mathbb{R}$ and $\displaystyle \sum_{k=1}^{n} \lambda_{k} = 1, \lambda_{k}&amp;gt;0$ $$ \begin{align*} f( \lambda_{1} x_{1} + \lambda_{2} x_{2} + \cdots + \lambda_{n} x_{n} ) &amp;amp; \le \lambda_{1} f( x_{1}) + \lambda_{2} f( x_{2}) + \cdots + \lambda_{n} f( x_{n} ) \\ f\left( \sum\limits_{k=1}^{n}\lambda_{k}x_{k} \right) &amp;amp;\le \sum\limits_{k=1}^{n} \lambda_{k} f(x_{k}) \end{align*} $$ If $f$ is a concave function, then the opposite</description></item><item><title>Derivation of the General Term of the Fibonacci Sequence</title><link>https://freshrimpsushi.github.io/en/posts/680/</link><pubDate>Sat, 15 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/680/</guid><description>Theorem Let&amp;rsquo;s say that the sequence sequence $\left\{ F_{n} \right\}_{n=0}^{\infty}$ is defined as $F_{n+1} := F_{n} + F_{n-1}$. If $F_{0} = F_{1} = 1$, then for $\displaystyle r_{0} : = {{1 + \sqrt{5} } \over {2}}$ and $\displaystyle r_{1} : = {{1 - \sqrt{5} } \over {2}}$, $$ F_{n} = {{ {r_{0}}^{n+1} - {r_{1}}^{n+1} } \over { r_{0} - r_{1} }} $$ Description Note that the Fibonacci sequence introduced above</description></item><item><title>Simplifying the Exponentiation of Two-digit Numbers Ending in 5</title><link>https://freshrimpsushi.github.io/en/posts/661/</link><pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/661/</guid><description>Formulas The square of a two-digit number whose ones place is 5 can be computed quickly and easily as shown in the photo above. It&amp;rsquo;s okay to just know and use the result, but some might be curious about why it works this way. Proof Let&amp;rsquo;s assume any two-digit number whose ones place is $5$ is $10a+5$. Then, the square can be calculated as follows. $$ \begin{align*} (10a+5)(10a+5) =&amp;amp;\ 100a^2+100a+25</description></item><item><title>Indefinite Integral of the Form e^{x^2}</title><link>https://freshrimpsushi.github.io/en/posts/487/</link><pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/487/</guid><description>Theorem $$ \int e^{x^2}dx = \sum\limits_{n=0}^\infty \dfrac{x^{2n+1}}{(2n+1)n!}+C $$ Explanation Just like the form $e^{-x^{2}}$, it is difficult to integrate using general methods. There is a method to integrate by defining the error function, imaginary error function, erfi, but this article introduces solving it using Taylor series expansion. Proof By the method of Taylor series expansion, $$ e^{x} = \sum\limits_{n=0}^{\infty} \dfrac{x^{n}}{n!} = 1 + x + \dfrac{x^{2}}{2!} + \cdots + \dfrac{x^{n}}{n!}</description></item><item><title>Proof of the Integral Form of Jensen's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/265/</link><pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/265/</guid><description>Theorem Given a convex function $ \phi : [a,b] \to \mathbb{R}$ and $f: [0,1] \to [a,b]$, if $\phi \circ f$ is integrable over $[0,1]$, then $$ \phi \left( \int_{0}^{1} f(x) dx \right) \le \int_{0}^{1} (\phi \circ f ) (x) dx $$ Explanation Of course, given the conditions, the integration interval can also be changed through substitution, etc. Unlike finite form, which generalizes the number of terms using definitions, integration form</description></item><item><title>Binomial Theorem Proof</title><link>https://freshrimpsushi.github.io/en/posts/218/</link><pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/218/</guid><description>Definition A combination is a subset of a finite set. The number of subsets with cardinality $k$ from a set with cardinality $n$ is denoted by $\binom{n}{k}$ or $_{n}C_{k}$, and is called the binomial coefficient. $$ \binom{n}{k} = _{n}C_{k} = \frac{ n! }{ k! (n-k)! } $$ Theorem Binomial Theorem $$ (x+y)^{n} = \sum_{k=0}^{n} \binom{n}{k} x^{k} y^{n-k} $$ Pascal&amp;rsquo;s Identity $$ \binom{n}{k} + \binom{n}{k+1} = \binom{n+1}{k+1} $$ Binomial Coefficient Sum</description></item><item><title>Definite Integration of the form e^-x^2, Gaussian Integral, Euler-Poisson Integral</title><link>https://freshrimpsushi.github.io/en/posts/219/</link><pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/219/</guid><description>Theorem The Gaussian function $f(x) := e^{-x^2}$&amp;rsquo;s integral over the entire domain is as follows. $$ \int_{-\infty}^{\infty} e^{-x^2} dx= \sqrt{\pi} $$ Description Physicist Kelvin is said to have left the remark that &amp;ldquo;one who finds this integral obvious is a mathematician&amp;rdquo;. It is also known by other names such as Gaussian integral, or Euler-Poisson integral. It&amp;rsquo;s a shocking integration for high school students and especially crucial for statistics. That&amp;rsquo;s because,</description></item><item><title>Proof that the Partial Sums of a Geometric Sequence are Also Geometric</title><link>https://freshrimpsushi.github.io/en/posts/194/</link><pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/194/</guid><description>Theorem The geometric sequence $a_n = a r^{n-1}$, its partial sum $\displaystyle S_n = \sum_{k=1}^{n} a_k$, and some natural number $m$, such that $A_n = S_{mn} - S_{m(n-1)}$ is a geometric sequence. Explanation It&amp;rsquo;s really difficult if you don&amp;rsquo;t know. For example, consider the sequence obtained by summing sets of three powers of 2, $(1 + 2+ 4)= 7 $, $(8 + 16 + 32)=56$, $(64+128+256)=448 \cdots$ is a geometric</description></item><item><title>Proof that the Partial Sums of an Arithmetic Sequence Also Form an Arithmetic Sequence</title><link>https://freshrimpsushi.github.io/en/posts/193/</link><pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/193/</guid><description>Theorem An arithmetic sequence $a_n = a + (n-1)d$, its partial sum $\displaystyle S_n = \sum_{k=1}^{n} a_k $, and a certain natural number $m$ for $A_n = S_{mn} - S_{m(n-1)} $ form an arithmetic sequence. Explanation It&amp;rsquo;s really tough if you don&amp;rsquo;t know. For example, consider the sequence formed by summing every three natural numbers: $(1 + 2+ 3)= 6 $, $(4+5+6)=15$, $(7+8+9)=24 \cdots$ form an arithmetic sequence with the</description></item><item><title>Finding the Sum of Squares</title><link>https://freshrimpsushi.github.io/en/posts/189/</link><pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/189/</guid><description>Formula $$ \sum_{k=1}^{n} { k^2} = {{n(n+1)(2n+1)} \over {6}} $$ Derivation Let&amp;rsquo;s consider the difference between $k^3$ and $(k-1)^3$ which are of one higher order. $$ 1^3 - 0^3 = 3 \cdot 1^2 - 3 \cdot 1 + 1 \\ 2^3 - 1^3 = 3 \cdot 2^2 - 3 \cdot 2 + 1 \\ 3^3 - 2^3 = 3 \cdot 3^2 - 3 \cdot 3 + 1 \\ \vdots \\</description></item><item><title>Finding the Sum of a Geometric Sequence</title><link>https://freshrimpsushi.github.io/en/posts/170/</link><pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/170/</guid><description>Formula Given a geometric sequence $a_{n} = a r^{n-1}$ with the first term $a$ and common ratio $r$, $$ \sum_{k=1}^{n} a_{k}= {{a (1- r^{n} ) } \over {1-r}} $$ Proof Let&amp;rsquo;s denote it as $\displaystyle S= \sum_{k=1}^{n} a_{k}$. Then, $$ S= a + ar + \cdots + ar^{n-2} + ar^{n-1} $$ Multiplying both sides by $r$ gives $$ rS= ar + a r^2 + \cdots + ar^{n-1} + ar^{n} $$</description></item><item><title>Finding the Sum of an Arithmetic Sequence</title><link>https://freshrimpsushi.github.io/en/posts/169/</link><pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/169/</guid><description>Formula The arithmetic sequence $a_{n} = a+(n-1)d$ with the first term $a$ and the common difference $d$ $$ \sum_{k=1}^{n} a_{k}= {{n \left\{ 2a + (n-1)d \right\} } \over {2}} $$ Explanation Although this is a series that you might look at once and never write down again in this form, never forget its proof. Even if the proof is simple and straightforward, it&amp;rsquo;s crucial to write it down by hand</description></item><item><title>Multiplication Formula Table</title><link>https://freshrimpsushi.github.io/en/posts/171/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/171/</guid><description>Overview Introducing commonly used multiplication formulas. Formulas $$ \begin{align} (a+b)^2 &amp;amp;=a^2+2ab+b^2 \\ (a-b)^2 &amp;amp;= a^2-2ab+b^2 \end{align} $$ $$ \begin{equation} (a+b)(a-b)=a^2-b^2 \end{equation} $$ $$ \begin{align} (a+b)^3 &amp;amp;= a^3+3a^2b+3ab^2+b^3 \\ (a-b)^3 &amp;amp;= a^3-3a^2b+3ab^2-b^3 \end{align} $$ $$ \begin{equation} (a+b+c)^2=a^2+b^2+c^2+2ab+2bc+2ca \end{equation} $$ $$ \begin{align} (a+b)(a^2-ab+b^2) &amp;amp;= a^3+b^3 \\ (a-b)(a^2+ab+b^2) &amp;amp;= a^3-b^3 \end{align} $$ Proof (1), (2) $$ \begin{align*} (a \pm b)^{2} &amp;amp;= (a \pm b)(a \pm b) \\ &amp;amp;= a^{2} \pm ab \pm ba</description></item><item><title>Cauchy-Schwarz Inequality Proof</title><link>https://freshrimpsushi.github.io/en/posts/51/</link><pubDate>Mon, 03 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/51/</guid><description>Theorem $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\ge { (ax+by) }^{ 2 } $$ Proof $$ \begin{align*} &amp;amp; ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})-{ (ax+by) }^{ 2 } \\ =&amp;amp; {a}^{2}{x}^{2}+{b}^{2}{x}^{2}+{a}^{2}{y}^{2}+{b}^{2}{y}^{2}-{ (ax+by) }^{ 2 } \\ =&amp;amp; {b}^{2}{x}^{2}+{a}^{2}{y}^{2}-2axby \\ =&amp;amp; { (ay-bx) }^{ 2 } \\ \ge&amp;amp; 0 \end{align*} $$ Thus, we can summarize as follows. $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\ge { (ax+by) }^{ 2 } $$ ■ Explanation This inequality, which can be encountered as early as high school, is used widely</description></item><item><title>Integration of the Natural Logarithm Raised to a Power</title><link>https://freshrimpsushi.github.io/en/posts/45/</link><pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/45/</guid><description>Formulas $$ \int {{(\ln x)}^{ n }} dx=x{{(\ln x)}^{ n }}-\int n{{(\ln x)}^{ n-1 }}dx $$ Explanation When solving integral problems, it&amp;rsquo;s not uncommon to encounter this type of problem. Solving these problems directly through integration by parts can be extremely time-consuming. First, let&amp;rsquo;s try to find a rule. Given $f(n)=\int {{(\ln x)}^{ n }} dx$ (where $n=1,2,3&amp;hellip;$), $$ \begin{align*} f(1) =&amp;amp; x(\ln|x|-1)+C \\ f(2) =&amp;amp; x{(\ln|x|)^{ 2 }-2\ln|x|+2}+C \\</description></item><item><title>Finding the Extremum of a Quadratic Function Quickly</title><link>https://freshrimpsushi.github.io/en/posts/30/</link><pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/30/</guid><description>Formula The vertex of the quadratic function $f(x)=c(x-a)(x-b)$ is $\frac { a+b }{ 2 }$ (where $c\neq 0$) For factorable quadratic functions, the vertex can be found without bothering with various calculations. It seems obvious, but knowing or not knowing this fact can make a difference in whether or not you can reduce one step in the calculation process. Derivation $$ \begin{align*} &amp;amp; f(x) = c(x-a)(x-b) = c x^2 -c(a+b)x+cab</description></item><item><title>The Number of Subsets of a Finite Set with n Elements</title><link>https://freshrimpsushi.github.io/en/posts/25/</link><pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/25/</guid><description>Formula Sum of Binomial Coefficients 1 The sum of binomial coefficients is as follows. $$ 2^{n} = \sum_{k=0}^{n} \binom{n}{k} $$ Corollary: Cardinality of the Power Set If the cardinality of a finite set $S$ is $n = |S|$, then the cardinality of its power set $2^{S}$ is $2^{n}$. Derivation Binomial Theorem: $$ (x+y)^{n} = \sum_{k=0}^{n} \binom{n}{k} x^{k} y^{n-k} $$ Substituting $x = y = 1$ gives $2^{n} = \sum_{k=0}^{n} \binom{n}{k}</description></item><item><title>Frequently Used Definite Integrals of Quadratic Functions</title><link>https://freshrimpsushi.github.io/en/posts/15/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/15/</guid><description>Formula $$ \int _{ \alpha }^{ \beta }{ (x-\alpha )(x-\beta )dx }=-\frac { { (\beta -\alpha ) } ^ { 3 } }{ 6 } $$ Description As you solve problems, you often find yourself calculating definite integrals of this sort more than you&amp;rsquo;d expect. This formula is entirely useless aside from making solutions quicker, and its derivation is just calculation. Just memorize the form so you can use it</description></item><item><title>Gabi's Proof of Li</title><link>https://freshrimpsushi.github.io/en/posts/16/</link><pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/16/</guid><description>Theorem If $bdf(b+d)\neq 0$ then $$ \frac { a }{ b }=\frac { c }{ d }=\frac { e }{ f } \implies \frac { a+c }{ b+d }=\frac { e }{ f } $$ Description &amp;ldquo;Gabi&amp;rdquo; is nothing else but a word made from two Hanja characters: add 加 and compare 比. Here, the compare 比 is the same as the &amp;lsquo;ratio&amp;rsquo; in ratios, making it a theorem</description></item><item><title>Arithmetic, Geometric, and Harmonic Means Inequality</title><link>https://freshrimpsushi.github.io/en/posts/3/</link><pubDate>Tue, 14 Mar 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3/</guid><description>Definitions For $n$ positive numbers ${x}_1,{x}_2,\cdots,{x}_n$, the arithmetic mean, geometric mean, and harmonic mean are defined as: Arithmetic Mean : $$ \sum_{ k=1 }^{ n }{ \frac { {x}_k }{ n } }=\frac { {x}_1+{x}_2+\cdots+{x}_n }{ n } $$ Geometric Mean : $$ \prod_{ k=1 }^{ n }{ { {x}_k }^{ \frac { 1 }{ n } } }=\sqrt [ n ]{ {x}_1{x}_2\cdots{x}_n } $$ Harmonic Mean : $$ \left(</description></item></channel></rss>