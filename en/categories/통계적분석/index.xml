<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistical Analysis on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/categories/%ED%86%B5%EA%B3%84%EC%A0%81%EB%B6%84%EC%84%9D/</link><description>Recent content in Statistical Analysis on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 20 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%86%B5%EA%B3%84%EC%A0%81%EB%B6%84%EC%84%9D/index.xml" rel="self" type="application/rss+xml"/><item><title>What is LASSO Regression?</title><link>https://freshrimpsushi.github.io/en/posts/2571/</link><pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2571/</guid><description>Definition $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ Given $n$ data points and</description></item><item><title>What is Ridge Regression?</title><link>https://freshrimpsushi.github.io/en/posts/2567/</link><pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2567/</guid><description>Definition $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ Given a set of data</description></item><item><title>What is Sparse Regression?</title><link>https://freshrimpsushi.github.io/en/posts/2563/</link><pubDate>Sat, 04 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2563/</guid><description>Definition Let&amp;rsquo;s consider a matrix equation given by matrix $A \in \mathbb{R}^{m \times n}$ and vector $\mathbf{b} \in \mathbb{R}^{m}$ as follows: $$ A \mathbf{x} = \mathbf{b} $$ Sparse Regression refers to methods that aim to find a solution or least squares solution to such matrix equations, maximizing the sparsity of $\mathbf{x}$, in other words, minimizing the number of non-zero elements in $\mathbf{x}$ as much as possible. When the solution $\mathbf{x}$</description></item><item><title>Introduction to PROJ in Earth Statistics</title><link>https://freshrimpsushi.github.io/en/posts/2541/</link><pubDate>Thu, 21 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2541/</guid><description>Build-up The Earth is round, and more precisely, it is considered an ellipsoid. A &amp;lsquo;globe&amp;rsquo;, which is a scaled-down model of the Earth, is an accurate model but not very widely used since, after all, humanity is still more comfortable with flat pictures. Thus, various coordinate systems have been devised, and there are&amp;hellip; quite a variety of them. 1 For example, the world map we commonly see uses the Mercator</description></item><item><title>Resolving Not Defined Because of Singularities in R Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/2534/</link><pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2534/</guid><description>If you are a major in statistics or mathematics, it is strongly recommended that you not only roughly grasp the cause and solve the problem you are facing, but also understand the mathematical proof. Error Diagnosis Coefficients: (1 not defined because of singularities) Estimate Std. Error t value Pr(&amp;gt;|t|) (Intercept) 0.5723 0.1064 5.381 4.98e-05 *** 최고기온 -0.3528 0.1490 -2.368 0.030 * 최저기온 0.2982 0.1955</description></item><item><title>Universal Kriging</title><link>https://freshrimpsushi.github.io/en/posts/2523/</link><pubDate>Wed, 14 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2523/</guid><description>Model Ordinary Kriging In Spatial Data Analysis, for a Random Field $\mathbf{Y} = \left( Y \left( s_{1} \right) , \cdots , Y \left( s_{n} \right) \right)$ that follows a Multivariate Normal Distribution $\varepsilon \sim N_{n} \left( \mathbf{0} , \Sigma \right)$ with Mean $\mu \in \mathbb{R}$ and Covariance Matrix $\Sigma \in \mathbb{R}^{n \times n}$, the following model $$ \mathbf{Y} = \mu \mathbf{1} + \varepsilon $$ is used to estimate ■c</description></item><item><title>Kringing in Spatial Data Analysis</title><link>https://freshrimpsushi.github.io/en/posts/2521/</link><pubDate>Sat, 10 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2521/</guid><description>Model Ordinary Kriging In Spatial Data Analysis, for a Random Field $\mathbf{Y} = \left( Y \left( s_{1} \right) , \cdots , Y \left( s_{n} \right) \right)$ following a Multivariate Normal Distribution with Mean $\mu \in \mathbb{R}$ and Covariance Matrix $\Sigma \in \mathbb{R}^{n \times n}$, the value estimated for a new site $s_{0}$ using the model $$ \mathbf{Y} = \mu \mathbf{1} + \varepsilon $$ is called the Ordinary Kriging Estimate. The</description></item><item><title>Empirical Variogram</title><link>https://freshrimpsushi.github.io/en/posts/2519/</link><pubDate>Tue, 06 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2519/</guid><description>Buildup Definition of the Variogram: Consider a spatial process $\left\{ Y(s) \right\}_{s \in D}$ which is a set of random variables $Y(s) : \Omega \to \mathbb{R}^{1}$ in a fixed subset $D \subset \mathbb{R}^{r}$ of the Euclidean space with direction vectors $\mathbf{h} \in \mathbb{R}^{r}$. Specifically, represent $n \in \mathbb{N}$ sites as follows $\left\{ s_{1} , \cdots , s_{n} \right\} \subset D$ and assume that $Y(s)$ has a variance for all $s</description></item><item><title>Models of Semivariograms</title><link>https://freshrimpsushi.github.io/en/posts/2502/</link><pubDate>Wed, 03 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2502/</guid><description>Overview In Spatial Statistics Analysis, if a Spatial Process is Isotropic and the Semivariogram satisfies $\gamma \left( \left\| \mathbf{h} \right\| \right) = \gamma (d)$, then $\gamma$ can be expressed not as a complex matrix form but as a one-dimensional scalar function, that is, $\gamma : \mathbb{R} \to \mathbb{R}$. This means that the correlation between point reference data $Y(s), Y(s + d)$ can be plotted as a line graph. Models 1</description></item><item><title>Isotropy of Variogram</title><link>https://freshrimpsushi.github.io/en/posts/2500/</link><pubDate>Sat, 30 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2500/</guid><description>Definition 1 The semivariogram $\gamma \left( \mathbf{h} \right)$ of a spatial process can be expressed as depending solely on the magnitude $d := \left\| \mathbf{h} \right\|$ rather than on the direction vector $\mathbf{h} \in \mathbb{R}^{r}$, in which case the variogram $2 \gamma$ is said to be isotropic. If it is not isotropic, it is referred to as anisotropic. $$ \gamma \left( \left\| \mathbf{h} \right\| \right) = \gamma (d) $$ Especially</description></item><item><title>Definition of Variogram</title><link>https://freshrimpsushi.github.io/en/posts/2498/</link><pubDate>Tue, 26 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2498/</guid><description>Definition 1 In a fixed subset $D \subset \mathbb{R}^{r}$ of Euclidean space , consider a space process $\left\{ Y(s) \right\}_{s \in D}$ which is a set of random variables $Y(s) : \Omega \to \mathbb{R}^{1}$ and a direction vector $\mathbf{h} \in \mathbb{R}^{r}$. Specifically, represent $n \in \mathbb{N}$ sites as $\left\{ s_{1} , \cdots , s_{n} \right\} \subset D$, and assume that $Y(s)$ has variance existing for all $s \in D$. The</description></item><item><title>Stationarity of Spatial Processes</title><link>https://freshrimpsushi.github.io/en/posts/2496/</link><pubDate>Fri, 22 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2496/</guid><description>Definitions 1 Consider a spatial process $\left\{ Y(s) \right\}_{s \in D}$ and direction vector $\mathbf{h} \in \mathbb{R}^{r}$, which is a set of random variables $Y(s) : \Omega \to \mathbb{R}^{1}$ in a fixed subset $D \subset \mathbb{R}^{r}$ of Euclidean space. Specifically, represent $n \in \mathbb{N}$ number of sites as $\left\{ s_{1} , \cdots , s_{n} \right\} \subset D$, and assume that $Y(s)$ has a variance for all $s \in D$. $\left\{</description></item><item><title>Spatial Processes</title><link>https://freshrimpsushi.github.io/en/posts/2494/</link><pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2494/</guid><description>Definition 1 Especially when it is $r &amp;gt; 1$, for a fixed subset $D \in \mathbb{R}^{r}$ of the Euclidean space, the following set of $p$-variate random vectors $Y(s) : \Omega \to \mathbb{R}^{p}$ is also referred to as a Spatial Process. $$ \left\{ Y(s) : s \in D \right\} $$ Especially when the spatial process is a finite set and represented as a vector like the following, it is also referred</description></item><item><title>What is Spatial Data Analysis?</title><link>https://freshrimpsushi.github.io/en/posts/2492/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2492/</guid><description>Explanation 1 Spatial Data refers to data that includes information about space, and Spatial Statistics is a branch of statistics that analyzes Euclidean space $\mathbb{R}^{r}$ as &amp;lsquo;space&amp;rsquo; in the true dictionary sense. While time series analysis analyses data that changes over the time axis $t$, spatial data analysis analyses data that changes depending on the given $D \subset \mathbb{R}^{r}$, (usually when $r = 2$) location. Even at first thought, the</description></item><item><title>Estimation of the Variance of Residuals and Standard Errors of Regression Coefficients in Multiple Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/2464/</link><pubDate>Thu, 19 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2464/</guid><description>Theorem $$ \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_{11} &amp;amp; \cdots &amp;amp; x_{p1} \\ 1 &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{p2} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 1 &amp;amp; x_{1n} &amp;amp; \cdots &amp;amp; x_{pn} \end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p} \end{bmatrix} + \begin{bmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{bmatrix} $$ When there are $p$ independent</description></item><item><title>The Definition of Regression Coefficients and Derivation of Estimator Formulas</title><link>https://freshrimpsushi.github.io/en/posts/2458/</link><pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2458/</guid><description>Definition 1 $$ Y = \beta_{0} + \beta_{1} X_{1} + \cdots + \beta_{p} X_{p} + \varepsilon $$ In multiple regression analysis, for the given $p$ independent variables $X_{1} , \cdots , X_{p}$, when setting up a linear model as above, $\beta_{0} , \beta_{1} , \cdots , \beta_{p}$ is called the regression coefficient. $Y$ represents the dependent variable, and $\varepsilon$ represents the randomly distributed error. Formula $$ \begin{bmatrix} y_{1} \\ y_{2}</description></item><item><title>How to Open a shp File with QGIS</title><link>https://freshrimpsushi.github.io/en/posts/2089/</link><pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/2089/</guid><description>Overview The shp extension represents a Shapefile. Many geographic information data are managed in formats accompanied by *.shp files and others such as *.dbf, *.sbn, *.sbx, *.shx. The most baffling thing upon receiving data is not knowing how to view it. Guide Step 1. Install QGIS https://qgis.org/en/site/forusers/download.html QGIS is an application software equipped with the ability to view and edit GIS (Geographic Information System) data. Download the installer appropriate for</description></item><item><title>Analyzing Time Series with Valuation Models in R</title><link>https://freshrimpsushi.github.io/en/posts/1282/</link><pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1282/</guid><description>Practice Value model serves as a useful tool for explaining archetypes, and the analytical procedure itself is similar to that of the ARMA model. The graph above shows the German DAX index from 1991 to 1999, drawn by extracting only DAX from EuStockMarkets. When we look at the square of returns, there seems to be an ARCH effect almost certainly. To check if the squared returns follow the ARMA model,</description></item><item><title>Time Series Analysis in Valuation Models</title><link>https://freshrimpsushi.github.io/en/posts/1280/</link><pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1280/</guid><description>Model 1 The Value-at-Risk (VaR) Model generalizes the ARCH model and is used for time series analysis to detect heteroskedasticity. $$ (1 - \beta{1} B - \cdots - \beta_{p} B^p) \sigma_{t | t-1}^2 = \omega + (\alpha_{1} B + \cdots + \alpha_{q} B^q) r_{t}^{2} $$ Derivation Let&amp;rsquo;s start with the simplest $ARCH(1)$ model. 2 Given time series data $\left\{ p_{t} \right\}$ and its returns $\left\{ r_{t} \right\}$, the data exhibiting</description></item><item><title>Arch Effect</title><link>https://freshrimpsushi.github.io/en/posts/1278/</link><pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1278/</guid><description>Definition 1 The term ARCH effect refers to the &amp;lsquo;AutoRegressive Conditional Heteroscedasticity,&amp;rsquo; which literally translates to &amp;lsquo;autoregressive conditional heteroscedastic effect.&amp;rsquo; Therefore, it is not neutralized because it is interpreted as such. Description In simpler terms, if the volatility of data changes and can be explained by previous data, it is said that the data exhibits the ARCH effect. The model that statistically explains this ARCH effect is called the ARCH</description></item><item><title>Heteroskedasticity and Volatility Clustering in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1272/</link><pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1272/</guid><description>Definition 1 Given a time series data $\left\{ p_{t} \right\}$. When the variance of $\left\{ p_{t} \right\}$ depends on $t$, $\left\{ p_{t} \right\}$ is said to have Heteroscedasticity. The phenomenon of the variance of $\left\{ p_{t} \right\}$, which has Heteroscedasticity, increasing and decreasing repeatedly is referred to as Volatility Clustering. The following defined $r_{t}$ is referred to as (Log) Return in $t$. $$ r_{t} := \nabla \log p_{t} = \log</description></item><item><title>Dynamic Regression Models</title><link>https://freshrimpsushi.github.io/en/posts/1265/</link><pubDate>Thu, 29 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1265/</guid><description>Model Dynamic regression models are, simply put, models that combine ARIMA models with regression models. Description It is also called ARIMAX $ARIMAX$, which means adding independent variables other than ARIMA $X$. In programming, especially as explained below in the practice section, in R, $X$ is emphasized with xreg. In fact, at this point, formulas are more comfortable than words. Let&amp;rsquo;s say the time series data to be analyzed as the</description></item><item><title>Time Series Analysis and Innovative Outliers</title><link>https://freshrimpsushi.github.io/en/posts/1260/</link><pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1260/</guid><description>Build-up In the graph above, a significant outlier can be found in September 2001. However, unlike additive outliers, it continues to have an effect thereafter. The number of air passengers was steadily increasing, showing seasonality, but the fear from the 9/11 terrorist attacks can be interpreted as sharply reducing the number of users itself. Definition 1 Outliers that change the landscape of analysis itself are called Innovative Outliers. Practice Such</description></item><item><title>Time Series Analysis of Additive Outliers</title><link>https://freshrimpsushi.github.io/en/posts/1258/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1258/</guid><description>Buildup The most noticeable point in the graph above is the huge outlier near February 2015. Such extreme values can have a detrimental effect on analysis. Luckily, it was just a brief, momentary outlier. Definition 1 Outliers that do not change the fluctuation of data itself are called additive outliers. Practice Such additive outliers can be intuitively found, or you can use the detectAO() function of the TSA package. The</description></item><item><title>Step Function and Pulse Function</title><link>https://freshrimpsushi.github.io/en/posts/1248/</link><pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1248/</guid><description>Definition 1 The function defined as follows $S_{t}^{(T)}$ is called a step function. $$ S_{t}^{(T)} := \begin{cases} 1 &amp;amp; , t \le T \\ 0 &amp;amp; , \text{otherwise} \end{cases} $$ The function defined as follows $P_{t}^{(T)}$ is called a pulse function. $$ \begin{align*} P_{t}^{(T)} :=&amp;amp; \nabla S_{t}^{(T)} \\ =&amp;amp; S_{t}^{(T)} - S_{t-1}^{(T)} \end{align*} $$ Description Step functions and pulse functions are useful for representing equations used in intervention analysis, and</description></item><item><title>Intervention Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1243/</link><pubDate>Wed, 21 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1243/</guid><description>Buildup The graph above represents the actual time series data of the fine dust concentration in Seoul in the year 2015. What immediately stands out to anyone looking at it is that there was a day, around the 50th marker or so, which means the end of February, when the fine dust concentration exceeded 500. Anyone somewhat familiar with handling data might first suspect this to be a measurement error,</description></item><item><title>Time Series Regression and Spurious Correlation</title><link>https://freshrimpsushi.github.io/en/posts/1238/</link><pubDate>Mon, 19 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1238/</guid><description>Definition 1 Spurious correlation refers to a relationship where two datasets appear to have a plausible correlation but actually do not. Cryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p260.&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Encyclopedia</title><link>https://freshrimpsushi.github.io/en/posts/1236/</link><pubDate>Sat, 17 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1236/</guid><description>Definition Prewhitening is a method that transforms time series data into white noise when calculating the Cross-Correlation Function (CCF) to more accurately identify the correlation between two datasets. Practical Exercise 1 If possible, it is recommended to fully understand mathematically how this is achievable. As an example, let&amp;rsquo;s look at the following data. bluebird consists of two time series data including the average price and sales volume of potato chips</description></item><item><title>Cross-Correlation Function</title><link>https://freshrimpsushi.github.io/en/posts/1227/</link><pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1227/</guid><description>Definition 1 Let&amp;rsquo;s say $\left\{ X_{t} \right\}_{t=1}^{n}$, $\left\{ Y_{t} \right\}_{t=1}^{n}$ are stochastic processes. The following defined $\rho_{k}$ is called the cross-correlation function at lag $k$. $$ \rho_{k} (X,Y) := \text{cor} \left( X_{t} , Y_{t-k} \right) = \text{cor} \left( X_{t+k} , Y_{t} \right) $$ The following defined $r_{k}$ is called the sample cross-correlation function at lag $k$. $$ r_{k} := {{ \sum \left( X_{t} - \overline{X} \right) \left( Y_{t-k} - \overline{Y}</description></item><item><title>Time Series Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1223/</link><pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1223/</guid><description>Definition Time series regression analysis refers precisely to the technique of performing regression analysis using time series data. It&amp;rsquo;s true that regression analysis is not inherently well-suited to handling time series data, yet, when dealing with multiple series of time-series data, it can be beneficial to borrow the ideas and tools of regression analysis. Practice Suppose we are given two types of data, x and y, as shown above. Of</description></item><item><title>Residual Analysis of ARIMA Models</title><link>https://freshrimpsushi.github.io/en/posts/1218/</link><pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1218/</guid><description>Explanation Like regression analysis, time series analysis also involves residual analysis. According to the assumptions of the ARIMA model, residuals are all white noise, thus they should follow linearity, homoscedasticity, independence, and normality. Compared to regression analysis, it&amp;rsquo;s generally not as strict, but independence is rigorously checked. After all, the purpose of time series analysis is to understand autocorrelation; if residuals still lack independence, it means the analysis is incomplete.</description></item><item><title>Selecting an ARMA Model Using EACF in R</title><link>https://freshrimpsushi.github.io/en/posts/1216/</link><pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1216/</guid><description>Practice 1 PACF is very helpful in determining the order of $AR(p)$, while ACF aids in determining the order of $MA(q)$. Let&amp;rsquo;s directly observe an example. ma1.2.s data comes from a $MA(1)$ model, and ar1.s data comes from a $AR(1)$ model, both from the TSA package. By using the acf() and pacf() functions from the TSA package, it generates a Correlogram for various lags $k$ as follows. Merely looking at</description></item><item><title>Extended Autocorrelation Function</title><link>https://freshrimpsushi.github.io/en/posts/1213/</link><pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1213/</guid><description>Buildup PACF helps in determining the order of $AR(p)$, while ACF is helpful in setting the order for $MA(q)$. However, when applying to $ARMA(p,q)$ model, due to the invertibility of ARMA models, $AR(p)$ may appear as $MA(\infty)$, and $MA(q)$ may appear as $AR(\infty)$. Therefore, various methods have been devised to circumvent these issues and find the ARMA model. Definition The Extended Autocorrelation Function is one such method, defined as EACF</description></item><item><title>Autocorrelation Function</title><link>https://freshrimpsushi.github.io/en/posts/1211/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1211/</guid><description>Definition 1 Let $\left\{ Y_{t} \right\}_{t=1}^{n}$ be a stochastic process, and for lag $k$, let the residuals obtained by regressing $Y_{t}$ on $Y_{t-1}, \cdots , Y_{t-(k-1)}$ be $\widehat{e_{t}}$, and the residuals obtained by regressing $Y_{t-k}$ on $Y_{t-1}, \cdots , Y_{t-(k-1)}$ be $\widehat{e_{t-k}}$. The following defined $\phi_{kk}$ is referred to as the partial autocovariance function at lag $k$. $$ \phi_{kk} := \text{cor} ( \widehat{e_{t}} , \widehat{e_{t-k}} ) $$ The following defined</description></item><item><title>Autocorrelation Function</title><link>https://freshrimpsushi.github.io/en/posts/1209/</link><pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1209/</guid><description>Definition 1 Let&amp;rsquo;s say $\left\{ Y_{t} \right\}_{t=1}^{n}$ is a stochastic process. $\mu_{t} := E ( Y_{t} )$ is called the mean function. The following defined $\gamma_{ t , s }$ is called the autocovariance function. $$ \gamma_{t , s} : = \text{cov} ( Y_{t} , Y_{s} ) = E ( Y_{t} - \mu_{t} ) E ( Y_{s} - \mu_{s} ) $$ The following defined $\rho_{ t , s }$ is</description></item><item><title>Reversibility of ARMA Models</title><link>https://freshrimpsushi.github.io/en/posts/1208/</link><pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1208/</guid><description>Definition 1 In the ARMA model, having invertibility means that $AR(p)$ and $MA(q)$ can represent each other. Examples Although it is not a formula development for the general $ARMA ( p , q)$, let&amp;rsquo;s examine the examples of $AR(1)$ and $MA(1)$. Autoregressive Model $AR(1) \implies MA( \infty )$ Considering the following autoregressive model $AR(1)$ for $| \phi | &amp;lt; 1$: $$ Y_{t} = \phi Y_{t-1} + e_{t} $$ $Y_{t-1}$ can</description></item><item><title>Predicting with ARIMA Models in R</title><link>https://freshrimpsushi.github.io/en/posts/1205/</link><pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1205/</guid><description>Practice The R built-in data UKDriverDeaths contains monthly data on road casualties in the UK from 1969 to 1984. It obviously follows a seasonal ARIMA model, and finding the actual model is not very difficult. However, performing calculations directly using the formula from the final model is quite laborious and complex. Therefore, we use the predict() function. You can set how far into the future you would like to predict</description></item><item><title>How to View Time Series Analysis Results Obtained with ARIMA Model in R</title><link>https://freshrimpsushi.github.io/en/posts/1200/</link><pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1200/</guid><description>Practice The built-in R dataset AirPassenger consists of monthly airline passenger numbers from 1949 to 1960. (1) Model: In fact, if the coefficients can be precisely identified, that&amp;rsquo;s not the most critical aspect. Represents the Seasonal ARIMA model $ARIMA(p,d,q)\times(P,D,Q)_{s}$. For instance, the result of the above analysis ARIMA(0,1,1)(0,1,1)[12] means $ARIMA(0,1,1)\times(0,1,1)_{12}$. (2) Coefficients: Represents the coefficients that fit the model. ma1 is the moving average process coefficient $\theta_{1}$, and sma1 is</description></item><item><title>How to Analyze Time Series with ARIMA Models in R</title><link>https://freshrimpsushi.github.io/en/posts/1197/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1197/</guid><description>Practice Let&amp;rsquo;s load the built-in data WWWusage in R and draw a graph to check it. WWWusage represents a time series data indicating the number of internet users a long time ago. To understand its trend, time series analysis is necessary. Among the time series analysis models, the most representative one is the ARIMA model. However, even within the ARIMA models, there are various methods to find the appropriate model.</description></item><item><title>Drift in the ARIMA Model</title><link>https://freshrimpsushi.github.io/en/posts/1115/</link><pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1115/</guid><description>Explanation When analyzing time series, one often comes across a coefficient called Drift. Of course, in the case above, the coefficient is too small compared to the standard error to matter. However, if you actually have a significant coefficient and need to write it down in a formula, it&amp;rsquo;s necessary to understand what a drift is. Unfortunately, there are no good explanations about what drift actually is domestically, and without</description></item><item><title>Seasonal ARIMA Model</title><link>https://freshrimpsushi.github.io/en/posts/1067/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1067/</guid><description>Model 1 An operator defined as $\nabla_{s} Y_{t} := Y_{t} - Y_{t-s}$ is called the Seasonal Difference. If $W_{t} := \nabla^{d} \nabla_{s}^{D} Y_{t}$ is defined as $\left\{ W_{t} \right\}_{t \in \mathbb{N}}$, and if $ARMA(P,Q)$ and $\left\{ Y_{t} \right\}_{t \in \mathbb{N}}$ is $ARMA(p,q)$, then $\left\{ Y_{t} \right\}_{t \in \mathbb{N}}$ is called a Seasonal ARIMA process $ARIMA(p,d,q)\times(P,D,Q)_{s}$. This form is called the Seasonal ARIMA model. Explanation Today&amp;rsquo;s temperature is, of course, mostly</description></item><item><title>Arima Model</title><link>https://freshrimpsushi.github.io/en/posts/941/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/941/</guid><description>Model 1 For the given white noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$, it is defined as $$ \nabla^{d} Y_{t} := \sum_{i = 1}^{p} \phi_{i} \nabla^{d} Y_{t-i} + e_{t} - \sum_{i = 1}^{q} \theta_{i} e_{t-i} $$ and this form is referred to as the $(p,d,q)$th ARIMA process $ARIMA (p,d,q)$. Such a form of time series analysis model is called ARIMA model. Explanation $ARI(p,d) \iff ARIMA(p,d,0)$ is referred to as AR model,</description></item><item><title>Transformation in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/938/</link><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/938/</guid><description>Buildup The reason why transformations are necessary in time series is to give a &amp;ldquo;penalty&amp;rdquo; for increasing variance over time, to keep the variance constant, and to achieve stationarity. The square root $\sqrt{}$ and log $\log$ are often used because the amount reduced is greater for larger values. Of course, when variance decreases, it means that the trend of data converges to some point, thus no time series analysis is</description></item><item><title>Differencing in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/916/</link><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/916/</guid><description>Definition 1 Define operator $B$ as $B Y_{t} = Y_{t-1}$, referred to as Backshift. Define operator $\nabla$ as $\nabla := 1 - B$ and $\nabla^{r+1} = \nabla \left( \nabla^{r} Y_{t} \right)$, referred to as Differencing. Explanation According to the definition of differencing, the $1$th difference is calculated as $$ \nabla Y_{t} = Y_{t} - Y_{t-1} $$, and the $2$th difference is calculated as $$ \begin{align*} \nabla^2 Y_{t} =&amp;amp; \nabla \left(</description></item><item><title>Autoregressive Moving Average Model</title><link>https://freshrimpsushi.github.io/en/posts/914/</link><pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/914/</guid><description>Model 1 White noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ is defined as $$ Y_{t} := \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots + \phi_{p} Y_{t-p} +e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2} - \cdots - \theta_{q} e_{t-q} $$ such a process is called a $(p,q)$th order autoregressive moving average process $ARMA(p,q)$. Explanation The ARMA model is simply a combination of the Moving Average Process and the Autoregressive Process. For instance,</description></item><item><title>Autoregressive Process</title><link>https://freshrimpsushi.github.io/en/posts/910/</link><pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/910/</guid><description>Model 1 White noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ is defined as in $Y_{t} := \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots + \phi_{p} Y_{t-p} + e_{t}$ and is called an $p$th order autoregressive process $AR(p)$. (1): $AR(1) : Y_{t} = \phi Y_{t-1} + e_{t}$ (2): $AR(2) : Y_{t} = \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + e_{t}$ (p): $AR(p) : Y_{t} = \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots +</description></item><item><title>Moving Average Process</title><link>https://freshrimpsushi.github.io/en/posts/909/</link><pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/909/</guid><description>Model 1 The process defined as follows for white noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ and according to $Y_{t} := e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2} - \cdots - \theta_{q} e_{t-q}$, is called the $q$th order moving average process $MA(q)$. (1): $MA(1) : Y_{t} = e_{t} - \theta e_{t-1}$ (2): $MA(2) : Y_{t} = e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2}$ (q): $MA(q) : Y_{t} = e_{t} - \theta_{1}</description></item><item><title>Stability in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/907/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/907/</guid><description>Definition 1 Time series data is said to have stationarity when its mean and variance are constant over time. Description It&amp;rsquo;s not normal正常 as in standard, but stationarity定常. The fact that data is stationary means that its mean and variance are stabilized, making it easier to analyze. If the</description></item><item><title>Time Series Analysis: White Noise</title><link>https://freshrimpsushi.github.io/en/posts/904/</link><pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/904/</guid><description>Definition 1 A sequence $\left\{ e_{t} \right\}_{t = 1}^{\infty}$ of independent identically distributed (iid) random variables $e_{t}$ is called White Noise. iid stands for independent identically distributed, meaning that they are independent from each other and share the same distribution. Description Following the definition of a sequence of random variables, it is naturally a stochastic process. Particularly, if $E ( e_{t} ) = 0$, then the stochastic process $\left\{ Y_{t}</description></item><item><title>Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/900/</link><pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/900/</guid><description>Description Time Series can be simply seen as a stochastic process obtained from real data. The stock index is a good example of a time series as its value changes with uncertainty over time. Time series analysis aims to understand and predict the movement of a dependent variable over time. The biggest difference between time series analysis and regression analysis is that, while regression analysis assumes that the independent variables</description></item><item><title>How to Read Logistic Regression Results in R</title><link>https://freshrimpsushi.github.io/en/posts/850/</link><pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/850/</guid><description>Practice Internal Data Let&amp;rsquo;s load turnout data. turnout is data pertaining to the 1992 U.S. general elections, which can identify the vote (whether voted) based on race, age, education level (educate), and income. Since this data is interested in the dependent variable of whether or not someone voted, logistic regression can be used. Logistic regression, unlike general regression analysis, builds a model through the glm() function. By inserting family=binomial() as</description></item><item><title>Logistic Regression</title><link>https://freshrimpsushi.github.io/en/posts/832/</link><pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/832/</guid><description>Buildup Let&amp;rsquo;s think about performing $Y \gets X_{1} , \cdots, X_{p}$. Here, $Y$ can be a categorical variable, particularly one with only two classes, such as male and female, success and failure, positive and negative, $0$ and $1$, etc. For convenience, let&amp;rsquo;s just call it $Y=0$ or $Y=1$. In cases where the dependent variable is binary, the interest is &amp;lsquo;what is $Y$ when we look at independent variables $ X_{1}</description></item><item><title>Variable Selection Criteria in Statistical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/826/</link><pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/826/</guid><description>Overview The problem of variable selection inevitably involves the subjectivity of the analyst, but a numerical indicator that helps draw as objective a conclusion as possible was needed. If such values could be calculated, it would provide a clear answer to when to stop the variable selection procedure. However, there are various types of criteria, and applying different criteria can lead to different results. Indicators [^1] Explained Variance $R^2$ The</description></item><item><title>Variable Selection Procedures in Statistical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/821/</link><pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/821/</guid><description>Buildup Let&amp;rsquo;s consider doing a multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$. Here, if we have $p$ independent variables, it would be ideal if they satisfy the various assumptions of regression analysis well, there is no multicollinearity, and the explanatory power is high. Of course, the more information, the better, but a regression model obtained from too much data also requires a lot of data to use. Therefore,</description></item><item><title>How to Perform Principal Component Regression in R</title><link>https://freshrimpsushi.github.io/en/posts/814/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/814/</guid><description>Overview Principal Components Regression (PCR) combines Principal Component Analysis and Multiple Regression Analysis. It involves using the principal components derived from PCA as new independent variables for regression analysis. From a statistical perspective, PCA itself might not be necessary, and its relevance usually comes into play for regression analysis. Practice (Following the method to detect multicollinearity) Although generating principal components involves complex computations including matrix decomposition, in R, this can</description></item><item><title>Principal Component Analysis in Statistics</title><link>https://freshrimpsushi.github.io/en/posts/812/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/812/</guid><description>Overview Think about performing Multiple Regression Analysis $Y \gets X_{1} , \cdots, X_{p}$. Principal Component Analysis, abbreviated as PCA in English, is, in simple terms, a method of &amp;lsquo;restructuring&amp;rsquo; quantitative variables so that they are properly independent for analysis. From the perspective of multivariate data analysis, it has the significance of &amp;lsquo;dimension reduction&amp;rsquo; as a means to explain phenomena with fewer variables. To properly understand the theoretical derivation of principal</description></item><item><title>Variance Inflation Factor VIF</title><link>https://freshrimpsushi.github.io/en/posts/810/</link><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/810/</guid><description>Definition 1 When performing multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$, let&amp;rsquo;s define the multiple regression coefficient for the $i$th independent variable as $R_{i}^2$. The following is called the Variance Inflation Factor for $X_{i}$. $$\displaystyle \text{VIF}_{i}: = {{1} \over {1 - R_{i}^{2} }}$$ Explanation First, it is recommended to read about multicollinearity. VIF is sometimes translated as the variance expansion index, but it is usually too long, so</description></item><item><title>Multicollinearity</title><link>https://freshrimpsushi.github.io/en/posts/808/</link><pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/808/</guid><description>Definition 1 Consider performing multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$. If among the independent variables $ X_{1} , \cdots, X_{p}$ there is a strong correlation between the independent variables, then it is said that there is multicollinearity. Practice Initially, the very idea that independent variables are dependent violates the assumptions of regression analysis and indeed leads to numerical problems that make the analysis results unreliable. It can</description></item><item><title>Nonlinear Regression Analysis: Variable Transformation in Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/805/</link><pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/805/</guid><description>Overview 1 Regression analysis is essentially a method to elucidate the linear relationships among variables; however, if necessary, the data can be &amp;lsquo;flattened&amp;rsquo; to analyze them linearly. This inherently involves explaining the dependent variable through a nonlinear combination of independent variables. Practice Let&amp;rsquo;s load the built-in data Pressure. Statistical analysis of the Pressure data is in fact unnecessary. This is merely a natural phenomenon which requires mathematical proof just as</description></item><item><title>Influence of Interaction in Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/696/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/696/</guid><description>Buildup It is recommended to read about regression analysis including qualitative variables first. Imagine guessing this year&amp;rsquo;s graduates&amp;rsquo; starting salaries based on their college entrance exam scores $X_{1}$, age $X_{2}$, gender $S$, and final educational attainment $E$. Firstly, with the presence of qualitative variables, gender is defined as $$ S = \begin{cases} 1 &amp;amp; ,\text{여성} \\ 0 &amp;amp; ,\t</description></item><item><title>Regression Analysis Including Qualitative Variables</title><link>https://freshrimpsushi.github.io/en/posts/686/</link><pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/686/</guid><description>Overview Regression analysis does not always guarantee that quantitative variables are used as independent variables. There is also a need to reflect categorical data in the analysis, such as what gender someone is, which company they belong to, what color something is, whether it&amp;rsquo;s a metal, etc. Build-up 1 Imagine guessing the starting salary $Y$ with the nationwide exam score $X_{1}$, age $X_{2}$, gender $S$, and the highest education level</description></item><item><title>Checking the Normality of Residuals through Model Diagnostics</title><link>https://freshrimpsushi.github.io/en/posts/683/</link><pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/683/</guid><description>Diagnostics To determine if regression analysis was performed correctly, you can check using the standardized residual plot. Normality is better assessed using histograms rather than looking at the scatter of residuals, or by conducting a normality test. The left side shows a density that decreases towards the top and bottom from the center, whereas the right side is evenly distributed regardless of the direction. However, cases where the residuals actually</description></item><item><title>Residual Independence Verified through Model Diagnosis</title><link>https://freshrimpsushi.github.io/en/posts/679/</link><pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/679/</guid><description>Diagnostic Methods Intuitive Pattern Recognition Using standardized residual plots, we can check if the regression analysis was performed correctly. To confirm independence, there should be no distinct patterns appearing in the residual plots. Unfortunately, diagnosing independence can be very subjective compared to other assumptions of regression analysis. A common example of lacking independence is seeing an unidentified straight line as shown above. It could be by chance, but usually, it</description></item><item><title>Homoscedasticity of Residuals Verified through Model Diagnostics</title><link>https://freshrimpsushi.github.io/en/posts/681/</link><pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/681/</guid><description>Diagnostic Methods 1 Standardized residual plots can be used to check if a regression analysis was conducted properly. To verify homoscedasticity, one should check if the scatter of the residuals is uniformly distributed overall. Common examples of lack of homoscedasticity include the following two cases. The variance increases towards the end, a situation that often requires a transformation or the introduction of weights to resolve. Regardless of how easy it</description></item><item><title>Residual Linearity Verified through Model Diagnostics</title><link>https://freshrimpsushi.github.io/en/posts/677/</link><pubDate>Wed, 12 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/677/</guid><description>Diagnostic Techniques 1 Standardized residual plots can be used to check if the regression analysis was performed correctly. To check for linearity, see if the residuals are symmetrically distributed around $0$. Looking at the figure on the right, it is evident that there is a lack of linearity. If it were a simple regression analysis, it would result in an inability to explain the trend of the data at all.</description></item><item><title>Regression Model Diagnostics</title><link>https://freshrimpsushi.github.io/en/posts/675/</link><pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/675/</guid><description>Necessity In the case of simple regression analysis, since it involves only one independent variable and one dependent variable, making it a $2$ dimensional analysis, it is easy to visually confirm if the analysis was conducted properly. However, for multiple regression analyses that exceed $3$ dimensions, it becomes difficult to represent the data graphically, making it hard to verify the accuracy of the analysis. There are instances where the analysis</description></item><item><title>How to Interpret Multiple Regression Analysis Results in R</title><link>https://freshrimpsushi.github.io/en/posts/670/</link><pubDate>Wed, 05 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/670/</guid><description>Data Exploration tail(attitude) In R, let&amp;rsquo;s load the built-in data attitude and check it using the tail() function. We are interested in performing multiple regression analysis on this data. We are interested in how the other independent variables affect the rating, which is our dependent variable. It&amp;rsquo;s difficult to see if there is a linear relationship between rating and the other variables just by looking at the data, so let&amp;rsquo;s</description></item><item><title>Multiple Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/666/</link><pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/666/</guid><description>Overview Regression analysis is a method used to discover the relationships between variables, particularly useful for identifying linear relationships. Multiple Linear Regression refers to the regression analysis that determines the effects of multiple independent variables (explanatory variables) on a single dependent variable (response variable). Model 1 $$Y = \beta_{0} + \beta_{1} X_{1} + \cdots + \beta_{p} X_{p} + \varepsilon $$ We are interested in whether variables have a linear relationship</description></item><item><title>How to View Simple Regression Analysis Results in R</title><link>https://freshrimpsushi.github.io/en/posts/652/</link><pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/652/</guid><description>Practice How to Do Regression Analysis head(faithful) In R, load the built-in data faithful using head() function and check. It&amp;rsquo;s difficult to confirm if there&amp;rsquo;s a linear relationship between the two variables just by looking at the data, so let&amp;rsquo;s plot it to see. win.graph(6,3) par(mfrow=c(1,2)) plot(faithful, main =&amp;#34;faithful&amp;#34;,asp=T) plot(faithful, main =&amp;#34;faithful&amp;#34;) points(head(faithful),col=&amp;#39;red&amp;#39;,pch=19) The one on the left is with the aspect ratio kept constant, which is accurate but hard</description></item><item><title>Simple Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/648/</link><pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/648/</guid><description>Overview Regression Analysis is a method for identifying relationships between variables, especially useful for elucidating linear relationships. Simple Linear Regression is the simplest among them, referring to regression analysis on one dependent (response) variable and one independent (explanatory) variable. Model 1 The statement that independent variable $x_{i}$ and dependent variable $y_{i}$ have a linear relationship means that for some $a,b$, it can be expressed as $y_{i} = ax_{i} + b$.</description></item><item><title>Fitted Values, Predicted Values, Residuals, Errors</title><link>https://freshrimpsushi.github.io/en/posts/650/</link><pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/650/</guid><description>Definition 1 The regression equation obtained through regression analysis $Y \gets X_{1} + X_{2} + \cdots + X_{n}$ is denoted as $y = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \cdots + \beta_{n} x_{n}$, and let&amp;rsquo;s indicate the n-th data as $(y_{i} , x_{i1} , x_{i2} , \cdots , x_{in})$. Mean: $$ \displaystyle \overline{y} := {{1} \over {n}} \sum_{i=1}^{n} y_{i} $$ Fitted Value: For the n-th data $y_{i}$ $$</description></item><item><title>Design Matrix</title><link>https://freshrimpsushi.github.io/en/posts/550/</link><pubDate>Sun, 13 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/550/</guid><description>Buildup Let&amp;rsquo;s load the built-in data faithful in R and check it with the head() function. Though only six, at a glance, eruptions and waiting seem to have a positive correlation. It would be nice if their relationship could be represented by some two constants $\beta_{0}, \beta_{1}$ such that $$\text{(eruptions)} = \beta_{0} + \beta_{1} \cdot \text{(waiting) }$$ The above equation represents the linear relationship between the two variables as the</description></item><item><title>What is Regression Analysis?</title><link>https://freshrimpsushi.github.io/en/posts/548/</link><pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/548/</guid><description>Description Regression analysis is so ubiquitous a foundation of nearly all statistical techniques that it is often described either too generally or too specifically. If one were to explain what regression analysis is in a sentence for someone curious, it could be described as a method for discovering the relationships between variables. This useful and astonishing method of analysis was born from the ideas of Francis Galton, the father of</description></item><item><title>Independence Does Not Imply No Correlation</title><link>https://freshrimpsushi.github.io/en/posts/536/</link><pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/536/</guid><description>Description If variables are independent, it means there is no correlation, but lack of correlation does not necessarily imply independence. The case when variables are independent if there is no correlation, that is when it is a necessary and sufficient condition, is when the random variables follow a normal distribution. In the case on the left, there is positive correlation, and in the case on the right, there is negative</description></item></channel></rss>