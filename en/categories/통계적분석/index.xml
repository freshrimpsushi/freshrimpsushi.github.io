<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>통계적분석 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%ED%86%B5%EA%B3%84%EC%A0%81%EB%B6%84%EC%84%9D/</link>
    <description>Recent content in 통계적분석 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 18 Dec 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%86%B5%EA%B3%84%EC%A0%81%EB%B6%84%EC%84%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spatial Processes</title>
      <link>https://freshrimpsushi.github.io/en/posts/2494/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2494/</guid>
      <description>Definition 1 Especially when it is $r &amp;gt; 1$, for a fixed subset $D \in \mathbb{R}^{r}$ of the Euclidean space, the following set of $p$-variate random vectors $Y(s) : \Omega \to \mathbb{R}^{p}$ is also referred to as a Spatial Process. $$ \left\{ Y(s) : s \in D \right\} $$ Especially when the spatial process is a finite set and represented as a vector like the following, it is also referred</description>
    </item>
    <item>
      <title>The Definition of Regression Coefficients and Derivation of Estimator Formulas</title>
      <link>https://freshrimpsushi.github.io/en/posts/2458/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2458/</guid>
      <description>Definition 1 $$ Y = \beta_{0} + \beta_{1} X_{1} + \cdots + \beta_{p} X_{p} + \varepsilon $$ In multiple regression analysis, for the given $p$ independent variables $X_{1} , \cdots , X_{p}$, when setting up a linear model as above, $\beta_{0} , \beta_{1} , \cdots , \beta_{p}$ is called the regression coefficient. $Y$ represents the dependent variable, and $\varepsilon$ represents the randomly distributed error. Formula $$ \begin{bmatrix} y_{1} \\ y_{2}</description>
    </item>
    <item>
      <title>Cross-Correlation Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1227/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1227/</guid>
      <description>Definition 1 Let&amp;rsquo;s say $\left\{ X_{t} \right\}_{t=1}^{n}$, $\left\{ Y_{t} \right\}_{t=1}^{n}$ are stochastic processes. The following defined $\rho_{k}$ is called the cross-correlation function at lag $k$. $$ \rho_{k} (X,Y) := \text{cor} \left( X_{t} , Y_{t-k} \right) = \text{cor} \left( X_{t+k} , Y_{t} \right) $$ The following defined $r_{k}$ is called the sample cross-correlation function at lag $k$. $$ r_{k} := {{ \sum \left( X_{t} - \overline{X} \right) \left( Y_{t-k} - \overline{Y}</description>
    </item>
    <item>
      <title>Autocorrelation Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1209/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1209/</guid>
      <description>Definition 1 Let&amp;rsquo;s say $\left\{ Y_{t} \right\}_{t=1}^{n}$ is a stochastic process. $\mu_{t} := E ( Y_{t} )$ is called the mean function. The following defined $\gamma_{ t , s }$ is called the autocovariance function. $$ \gamma_{t , s} : = \text{cov} ( Y_{t} , Y_{s} ) = E ( Y_{t} - \mu_{t} ) E ( Y_{s} - \mu_{s} ) $$ The following defined $\rho_{ t , s }$ is</description>
    </item>
    <item>
      <title>Arima Model</title>
      <link>https://freshrimpsushi.github.io/en/posts/941/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/941/</guid>
      <description>Model 1 For the given white noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$, it is defined as $$ \nabla^{d} Y_{t} := \sum_{i = 1}^{p} \phi_{i} \nabla^{d} Y_{t-i} + e_{t} - \sum_{i = 1}^{q} \theta_{i} e_{t-i} $$ and this form is referred to as the $(p,d,q)$th ARIMA process $ARIMA (p,d,q)$. Such a form of time series analysis model is called ARIMA model. Explanation $ARI(p,d) \iff ARIMA(p,d,0)$ is referred to as AR model,</description>
    </item>
    <item>
      <title>Differencing in Time Series Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/916/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/916/</guid>
      <description>Definition 1 Define operator $B$ as $B Y_{t} = Y_{t-1}$, referred to as Backshift. Define operator $\nabla$ as $\nabla := 1 - B$ and $\nabla^{r+1} = \nabla \left( \nabla^{r} Y_{t} \right)$, referred to as Differencing. Explanation According to the definition of differencing, the $1$th difference is calculated as $$ \nabla Y_{t} = Y_{t} - Y_{t-1} $$, and the $2$th difference is calculated as $$ \begin{align*} \nabla^2 Y_{t} =&amp;amp; \nabla \left(</description>
    </item>
    <item>
      <title>Autoregressive Moving Average Model</title>
      <link>https://freshrimpsushi.github.io/en/posts/914/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/914/</guid>
      <description>Model 1 White noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ is defined as $$ Y_{t} := \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots + \phi_{p} Y_{t-p} +e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2} - \cdots - \theta_{q} e_{t-q} $$ such a process is called a $(p,q)$th order autoregressive moving average process $ARMA(p,q)$. Explanation The ARMA model is simply a combination of the Moving Average Process and the Autoregressive Process. For instance,</description>
    </item>
    <item>
      <title>Time Series Analysis: White Noise</title>
      <link>https://freshrimpsushi.github.io/en/posts/904/</link>
      <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/904/</guid>
      <description>Definition 1 A sequence $\left\{ e_{t} \right\}_{t = 1}^{\infty}$ of independent identically distributed (iid) random variables $e_{t}$ is called White Noise. iid stands for independent identically distributed, meaning that they are independent from each other and share the same distribution. Description Following the definition of a sequence of random variables, it is naturally a stochastic process. Particularly, if $E ( e_{t} ) = 0$, then the stochastic process $\left\{ Y_{t}</description>
    </item>
    <item>
      <title>Time Series Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/900/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/900/</guid>
      <description>Description Time Series can be simply seen as a stochastic process obtained from real data. The stock index is a good example of a time series as its value changes with uncertainty over time. Time series analysis aims to understand and predict the movement of a dependent variable over time. The biggest difference between time series analysis and regression analysis is that, while regression analysis assumes that the independent variables</description>
    </item>
    <item>
      <title>Multicollinearity</title>
      <link>https://freshrimpsushi.github.io/en/posts/808/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/808/</guid>
      <description>Definition 1 Consider performing multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$. If among the independent variables $ X_{1} , \cdots, X_{p}$ there is a strong correlation between the independent variables, then it is said that there is multicollinearity. Practice Initially, the very idea that independent variables are dependent violates the assumptions of regression analysis and indeed leads to numerical problems that make the analysis results unreliable. It can</description>
    </item>
    <item>
      <title>Residual Independence Verified through Model Diagnosis</title>
      <link>https://freshrimpsushi.github.io/en/posts/679/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/679/</guid>
      <description>Diagnostic Methods Intuitive Pattern Recognition Using standardized residual plots, we can check if the regression analysis was performed correctly. To confirm independence, there should be no distinct patterns appearing in the residual plots. Unfortunately, diagnosing independence can be very subjective compared to other assumptions of regression analysis. A common example of lacking independence is seeing an unidentified straight line as shown above. It could be by chance, but usually, it</description>
    </item>
    <item>
      <title>Homoscedasticity of Residuals Verified through Model Diagnostics</title>
      <link>https://freshrimpsushi.github.io/en/posts/681/</link>
      <pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/681/</guid>
      <description>Diagnostic Methods 1 Standardized residual plots can be used to check if a regression analysis was conducted properly. To verify homoscedasticity, one should check if the scatter of the residuals is uniformly distributed overall. Common examples of lack of homoscedasticity include the following two cases. The variance increases towards the end, a situation that often requires a transformation or the introduction of weights to resolve. Regardless of how easy it</description>
    </item>
    <item>
      <title>Regression Model Diagnostics</title>
      <link>https://freshrimpsushi.github.io/en/posts/675/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/675/</guid>
      <description>Necessity In the case of simple regression analysis, since it involves only one independent variable and one dependent variable, making it a $2$ dimensional analysis, it is easy to visually confirm if the analysis was conducted properly. However, for multiple regression analyses that exceed $3$ dimensions, it becomes difficult to represent the data graphically, making it hard to verify the accuracy of the analysis. There are instances where the analysis</description>
    </item>
    <item>
      <title>Multiple Regression Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/666/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/666/</guid>
      <description>Overview Regression analysis is a method used to discover the relationships between variables, particularly useful for identifying linear relationships. Multiple Linear Regression refers to the regression analysis that determines the effects of multiple independent variables (explanatory variables) on a single dependent variable (response variable). Model 1 $$Y = \beta_{0} + \beta_{1} X_{1} + \cdots + \beta_{p} X_{p} + \varepsilon $$ We are interested in whether variables have a linear relationship</description>
    </item>
    <item>
      <title>Fitted Values, Predicted Values, Residuals, Errors</title>
      <link>https://freshrimpsushi.github.io/en/posts/650/</link>
      <pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/650/</guid>
      <description>Definition 1 The regression equation obtained through regression analysis $Y \gets X_{1} + X_{2} + \cdots + X_{n}$ is denoted as $y = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \cdots + \beta_{n} x_{n}$, and let&amp;rsquo;s indicate the n-th data as $(y_{i} , x_{i1} , x_{i2} , \cdots , x_{in})$. Mean: $$ \displaystyle \overline{y} := {{1} \over {n}} \sum_{i=1}^{n} y_{i} $$ Fitted Value: For the n-th data $y_{i}$ $$</description>
    </item>
    <item>
      <title>Design Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/550/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/550/</guid>
      <description>Buildup Let&amp;rsquo;s load the built-in data faithful in R and check it with the head() function. Though only six, at a glance, eruptions and waiting seem to have a positive correlation. It would be nice if their relationship could be represented by some two constants $\beta_{0}, \beta_{1}$ such that $$\text{(eruptions)} = \beta_{0} + \beta_{1} \cdot \text{(waiting) }$$ The above equation represents the linear relationship between the two variables as the</description>
    </item>
    <item>
      <title>What is Regression Analysis?</title>
      <link>https://freshrimpsushi.github.io/en/posts/548/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/548/</guid>
      <description>Description Regression analysis is so ubiquitous a foundation of nearly all statistical techniques that it is often described either too generally or too specifically. If one were to explain what regression analysis is in a sentence for someone curious, it could be described as a method for discovering the relationships between variables. This useful and astonishing method of analysis was born from the ideas of Francis Galton, the father of</description>
    </item>
    <item>
      <title>Kringing in Spatial Data Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/2521/</link>
      <pubDate>Sun, 10 Feb 1924 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2521/</guid>
      <description>Model Ordinary Kriging In Spatial Data Analysis, for a Random Field $\mathbf{Y} = \left( Y \left( s_{1} \right) , \cdots , Y \left( s_{n} \right) \right)$ following a Multivariate Normal Distribution with Mean $\mu \in \mathbb{R}$ and Covariance Matrix $\Sigma \in \mathbb{R}^{n \times n}$, the value estimated for a new site $s_{0}$ using the model $$ \mathbf{Y} = \mu \mathbf{1} + \varepsilon $$ is called the Ordinary Kriging Estimate. The</description>
    </item>
  </channel>
</rss>
