<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>통계적분석 on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/categories/%ED%86%B5%EA%B3%84%EC%A0%81%EB%B6%84%EC%84%9D/</link>
    <description>Recent content in 통계적분석 on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 03 Jan 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/categories/%ED%86%B5%EA%B3%84%EC%A0%81%EB%B6%84%EC%84%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Models of Semivariograms</title>
      <link>https://freshrimpsushi.github.io/en/posts/2502/</link>
      <pubDate>Wed, 03 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2502/</guid>
      <description>Overview In Spatial Statistics Analysis, if a Spatial Process is Isotropic and the Semivariogram satisfies $\gamma \left( \left\| \mathbf{h} \right\| \right) = \gamma (d)$, then $\gamma$ can be expressed not as a complex matrix form but as a one-dimensional scalar function, that is, $\gamma : \mathbb{R} \to \mathbb{R}$. This means that the correlation between point reference data $Y(s), Y(s + d)$ can be plotted as a line graph. Models 1</description>
    </item>
    <item>
      <title>Definition of Variogram</title>
      <link>https://freshrimpsushi.github.io/en/posts/2498/</link>
      <pubDate>Tue, 26 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2498/</guid>
      <description>Definition 1 In a fixed subset $D \subset \mathbb{R}^{r}$ of Euclidean space , consider a space process $\left\{ Y(s) \right\}_{s \in D}$ which is a set of random variables $Y(s) : \Omega \to \mathbb{R}^{1}$ and a direction vector $\mathbf{h} \in \mathbb{R}^{r}$. Specifically, represent $n \in \mathbb{N}$ sites as $\left\{ s_{1} , \cdots , s_{n} \right\} \subset D$, and assume that $Y(s)$ has variance existing for all $s \in D$. The</description>
    </item>
    <item>
      <title>Stationarity of Spatial Processes</title>
      <link>https://freshrimpsushi.github.io/en/posts/2496/</link>
      <pubDate>Fri, 22 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2496/</guid>
      <description>Definitions 1 Consider a spatial process $\left\{ Y(s) \right\}_{s \in D}$ and direction vector $\mathbf{h} \in \mathbb{R}^{r}$, which is a set of random variables $Y(s) : \Omega \to \mathbb{R}^{1}$ in a fixed subset $D \subset \mathbb{R}^{r}$ of Euclidean space. Specifically, represent $n \in \mathbb{N}$ number of sites as $\left\{ s_{1} , \cdots , s_{n} \right\} \subset D$, and assume that $Y(s)$ has a variance for all $s \in D$. $\left\{</description>
    </item>
    <item>
      <title>Spatial Processes</title>
      <link>https://freshrimpsushi.github.io/en/posts/2494/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2494/</guid>
      <description>Definition 1 Especially when it is $r &amp;gt; 1$, for a fixed subset $D \in \mathbb{R}^{r}$ of the Euclidean space, the following set of $p$-variate random vectors $Y(s) : \Omega \to \mathbb{R}^{p}$ is also referred to as a Spatial Process. $$ \left\{ Y(s) : s \in D \right\} $$ Especially when the spatial process is a finite set and represented as a vector like the following, it is also referred</description>
    </item>
    <item>
      <title>What is Spatial Data Analysis?</title>
      <link>https://freshrimpsushi.github.io/en/posts/2492/</link>
      <pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2492/</guid>
      <description>Explanation 1 Spatial Data refers to data that includes information about space, and Spatial Statistics is a branch of statistics that analyzes Euclidean space $\mathbb{R}^{r}$ as &amp;lsquo;space&amp;rsquo; in the true dictionary sense. While time series analysis analyses data that changes over the time axis $t$, spatial data analysis analyses data that changes depending on the given $D \subset \mathbb{R}^{r}$, (usually when $r = 2$) location. Even at first thought, the</description>
    </item>
    <item>
      <title>The Definition of Regression Coefficients and Derivation of Estimator Formulas</title>
      <link>https://freshrimpsushi.github.io/en/posts/2458/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2458/</guid>
      <description>Definition 1 $$ Y = \beta_{0} + \beta_{1} X_{1} + \cdots + \beta_{p} X_{p} + \varepsilon $$ In multiple regression analysis, for the given $p$ independent variables $X_{1} , \cdots , X_{p}$, when setting up a linear model as above, $\beta_{0} , \beta_{1} , \cdots , \beta_{p}$ is called the regression coefficient. $Y$ represents the dependent variable, and $\varepsilon$ represents the randomly distributed error. Formula $$ \begin{bmatrix} y_{1} \\ y_{2}</description>
    </item>
    <item>
      <title>Heteroskedasticity and Volatility Clustering in Time Series Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1272/</link>
      <pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1272/</guid>
      <description>Definition 1 Given a time series data $\left\{ p_{t} \right\}$. When the variance of $\left\{ p_{t} \right\}$ depends on $t$, $\left\{ p_{t} \right\}$ is said to have Heteroscedasticity. The phenomenon of the variance of $\left\{ p_{t} \right\}$, which has Heteroscedasticity, increasing and decreasing repeatedly is referred to as Volatility Clustering. The following defined $r_{t}$ is referred to as (Log) Return in $t$. $$ r_{t} := \nabla \log p_{t} = \log</description>
    </item>
    <item>
      <title>Cross-Correlation Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1227/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1227/</guid>
      <description>Definition 1 Let&amp;rsquo;s say $\left\{ X_{t} \right\}_{t=1}^{n}$, $\left\{ Y_{t} \right\}_{t=1}^{n}$ are stochastic processes. The following defined $\rho_{k}$ is called the cross-correlation function at lag $k$. $$ \rho_{k} (X,Y) := \text{cor} \left( X_{t} , Y_{t-k} \right) = \text{cor} \left( X_{t+k} , Y_{t} \right) $$ The following defined $r_{k}$ is called the sample cross-correlation function at lag $k$. $$ r_{k} := {{ \sum \left( X_{t} - \overline{X} \right) \left( Y_{t-k} - \overline{Y}</description>
    </item>
    <item>
      <title>Extended Autocorrelation Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1213/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1213/</guid>
      <description>Buildup PACF helps in determining the order of $AR(p)$, while ACF is helpful in setting the order for $MA(q)$. However, when applying to $ARMA(p,q)$ model, due to the invertibility of ARMA models, $AR(p)$ may appear as $MA(\infty)$, and $MA(q)$ may appear as $AR(\infty)$. Therefore, various methods have been devised to circumvent these issues and find the ARMA model. Definition The Extended Autocorrelation Function is one such method, defined as EACF</description>
    </item>
    <item>
      <title>Autocorrelation Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1211/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1211/</guid>
      <description>Definition 1 Let $\left\{ Y_{t} \right\}_{t=1}^{n}$ be a stochastic process, and for lag $k$, let the residuals obtained by regressing $Y_{t}$ on $Y_{t-1}, \cdots , Y_{t-(k-1)}$ be $\widehat{e_{t}}$, and the residuals obtained by regressing $Y_{t-k}$ on $Y_{t-1}, \cdots , Y_{t-(k-1)}$ be $\widehat{e_{t-k}}$. The following defined $\phi_{kk}$ is referred to as the partial autocovariance function at lag $k$. $$ \phi_{kk} := \text{cor} ( \widehat{e_{t}} , \widehat{e_{t-k}} ) $$ The following defined</description>
    </item>
    <item>
      <title>Autocorrelation Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1209/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1209/</guid>
      <description>Definition 1 Let&amp;rsquo;s say $\left\{ Y_{t} \right\}_{t=1}^{n}$ is a stochastic process. $\mu_{t} := E ( Y_{t} )$ is called the mean function. The following defined $\gamma_{ t , s }$ is called the autocovariance function. $$ \gamma_{t , s} : = \text{cov} ( Y_{t} , Y_{s} ) = E ( Y_{t} - \mu_{t} ) E ( Y_{s} - \mu_{s} ) $$ The following defined $\rho_{ t , s }$ is</description>
    </item>
    <item>
      <title>Seasonal ARIMA Model</title>
      <link>https://freshrimpsushi.github.io/en/posts/1067/</link>
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1067/</guid>
      <description>Model 1 An operator defined as $\nabla_{s} Y_{t} := Y_{t} - Y_{t-s}$ is called the Seasonal Difference. If $W_{t} := \nabla^{d} \nabla_{s}^{D} Y_{t}$ is defined as $\left\{ W_{t} \right\}_{t \in \mathbb{N}}$, and if $ARMA(P,Q)$ and $\left\{ Y_{t} \right\}_{t \in \mathbb{N}}$ is $ARMA(p,q)$, then $\left\{ Y_{t} \right\}_{t \in \mathbb{N}}$ is called a Seasonal ARIMA process $ARIMA(p,d,q)\times(P,D,Q)_{s}$. This form is called the Seasonal ARIMA model. Explanation Today&amp;rsquo;s temperature is, of course, mostly</description>
    </item>
    <item>
      <title>Arima Model</title>
      <link>https://freshrimpsushi.github.io/en/posts/941/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/941/</guid>
      <description>Model 1 For the given white noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$, it is defined as $$ \nabla^{d} Y_{t} := \sum_{i = 1}^{p} \phi_{i} \nabla^{d} Y_{t-i} + e_{t} - \sum_{i = 1}^{q} \theta_{i} e_{t-i} $$ and this form is referred to as the $(p,d,q)$th ARIMA process $ARIMA (p,d,q)$. Such a form of time series analysis model is called ARIMA model. Explanation $ARI(p,d) \iff ARIMA(p,d,0)$ is referred to as AR model,</description>
    </item>
    <item>
      <title>Transformation in Time Series Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/938/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/938/</guid>
      <description>Buildup The reason why transformations are necessary in time series is to give a &amp;ldquo;penalty&amp;rdquo; for increasing variance over time, to keep the variance constant, and to achieve stationarity. The square root $\sqrt{}$ and log $\log$ are often used because the amount reduced is greater for larger values. Of course, when variance decreases, it means that the trend of data converges to some point, thus no time series analysis is</description>
    </item>
    <item>
      <title>Differencing in Time Series Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/916/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/916/</guid>
      <description>Definition 1 Define operator $B$ as $B Y_{t} = Y_{t-1}$, referred to as Backshift. Define operator $\nabla$ as $\nabla := 1 - B$ and $\nabla^{r+1} = \nabla \left( \nabla^{r} Y_{t} \right)$, referred to as Differencing. Explanation According to the definition of differencing, the $1$th difference is calculated as $$ \nabla Y_{t} = Y_{t} - Y_{t-1} $$, and the $2$th difference is calculated as $$ \begin{align*} \nabla^2 Y_{t} =&amp;amp; \nabla \left(</description>
    </item>
    <item>
      <title>Autoregressive Moving Average Model</title>
      <link>https://freshrimpsushi.github.io/en/posts/914/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/914/</guid>
      <description>Model 1 White noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ is defined as $$ Y_{t} := \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots + \phi_{p} Y_{t-p} +e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2} - \cdots - \theta_{q} e_{t-q} $$ such a process is called a $(p,q)$th order autoregressive moving average process $ARMA(p,q)$. Explanation The ARMA model is simply a combination of the Moving Average Process and the Autoregressive Process. For instance,</description>
    </item>
    <item>
      <title>Autoregressive Process</title>
      <link>https://freshrimpsushi.github.io/en/posts/910/</link>
      <pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/910/</guid>
      <description>Model 1 White noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ is defined as in $Y_{t} := \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots + \phi_{p} Y_{t-p} + e_{t}$ and is called an $p$th order autoregressive process $AR(p)$. (1): $AR(1) : Y_{t} = \phi Y_{t-1} + e_{t}$ (2): $AR(2) : Y_{t} = \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + e_{t}$ (p): $AR(p) : Y_{t} = \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + \cdots +</description>
    </item>
    <item>
      <title>Moving Average Process</title>
      <link>https://freshrimpsushi.github.io/en/posts/909/</link>
      <pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/909/</guid>
      <description>Model 1 The process defined as follows for white noise $\left\{ e_{t} \right\}_{t \in \mathbb{N}}$ and according to $Y_{t} := e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2} - \cdots - \theta_{q} e_{t-q}$, is called the $q$th order moving average process $MA(q)$. (1): $MA(1) : Y_{t} = e_{t} - \theta e_{t-1}$ (2): $MA(2) : Y_{t} = e_{t} - \theta_{1} e_{t-1} - \theta_{2} e_{t-2}$ (q): $MA(q) : Y_{t} = e_{t} - \theta_{1}</description>
    </item>
    <item>
      <title>Stability in Time Series Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/907/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/907/</guid>
      <description>Definition 1 Time series data is said to have stationarity when its mean and variance are constant over time. Description It&amp;rsquo;s not normal正常 as in standard, but stationarity定常. The fact that data is stationary means that its mean and variance are stabilized, making it easier to analyze. If the</description>
    </item>
    <item>
      <title>Time Series Analysis: White Noise</title>
      <link>https://freshrimpsushi.github.io/en/posts/904/</link>
      <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/904/</guid>
      <description>Definition 1 A sequence $\left\{ e_{t} \right\}_{t = 1}^{\infty}$ of independent identically distributed (iid) random variables $e_{t}$ is called White Noise. iid stands for independent identically distributed, meaning that they are independent from each other and share the same distribution. Description Following the definition of a sequence of random variables, it is naturally a stochastic process. Particularly, if $E ( e_{t} ) = 0$, then the stochastic process $\left\{ Y_{t}</description>
    </item>
    <item>
      <title>Time Series Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/900/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/900/</guid>
      <description>Description Time Series can be simply seen as a stochastic process obtained from real data. The stock index is a good example of a time series as its value changes with uncertainty over time. Time series analysis aims to understand and predict the movement of a dependent variable over time. The biggest difference between time series analysis and regression analysis is that, while regression analysis assumes that the independent variables</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>https://freshrimpsushi.github.io/en/posts/832/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/832/</guid>
      <description>Buildup Let&amp;rsquo;s think about performing $Y \gets X_{1} , \cdots, X_{p}$. Here, $Y$ can be a categorical variable, particularly one with only two classes, such as male and female, success and failure, positive and negative, $0$ and $1$, etc. For convenience, let&amp;rsquo;s just call it $Y=0$ or $Y=1$. In cases where the dependent variable is binary, the interest is &amp;lsquo;what is $Y$ when we look at independent variables $ X_{1}</description>
    </item>
    <item>
      <title>Variable Selection Criteria in Statistical Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/826/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/826/</guid>
      <description>Overview The problem of variable selection inevitably involves the subjectivity of the analyst, but a numerical indicator that helps draw as objective a conclusion as possible was needed. If such values could be calculated, it would provide a clear answer to when to stop the variable selection procedure. However, there are various types of criteria, and applying different criteria can lead to different results. Indicators [^1] Explained Variance $R^2$ The</description>
    </item>
    <item>
      <title>Principal Component Analysis in Statistics</title>
      <link>https://freshrimpsushi.github.io/en/posts/812/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/812/</guid>
      <description>Overview Think about performing Multiple Regression Analysis $Y \gets X_{1} , \cdots, X_{p}$. Principal Component Analysis, abbreviated as PCA in English, is, in simple terms, a method of &amp;lsquo;restructuring&amp;rsquo; quantitative variables so that they are properly independent for analysis. From the perspective of multivariate data analysis, it has the significance of &amp;lsquo;dimension reduction&amp;rsquo; as a means to explain phenomena with fewer variables. To properly understand the theoretical derivation of principal</description>
    </item>
    <item>
      <title>Multicollinearity</title>
      <link>https://freshrimpsushi.github.io/en/posts/808/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/808/</guid>
      <description>Definition 1 Consider performing multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$. If among the independent variables $ X_{1} , \cdots, X_{p}$ there is a strong correlation between the independent variables, then it is said that there is multicollinearity. Practice Initially, the very idea that independent variables are dependent violates the assumptions of regression analysis and indeed leads to numerical problems that make the analysis results unreliable. It can</description>
    </item>
    <item>
      <title>Checking the Normality of Residuals through Model Diagnostics</title>
      <link>https://freshrimpsushi.github.io/en/posts/683/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/683/</guid>
      <description>Diagnostics To determine if regression analysis was performed correctly, you can check using the standardized residual plot. Normality is better assessed using histograms rather than looking at the scatter of residuals, or by conducting a normality test. The left side shows a density that decreases towards the top and bottom from the center, whereas the right side is evenly distributed regardless of the direction. However, cases where the residuals actually</description>
    </item>
    <item>
      <title>Residual Independence Verified through Model Diagnosis</title>
      <link>https://freshrimpsushi.github.io/en/posts/679/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/679/</guid>
      <description>Diagnostic Methods Intuitive Pattern Recognition Using standardized residual plots, we can check if the regression analysis was performed correctly. To confirm independence, there should be no distinct patterns appearing in the residual plots. Unfortunately, diagnosing independence can be very subjective compared to other assumptions of regression analysis. A common example of lacking independence is seeing an unidentified straight line as shown above. It could be by chance, but usually, it</description>
    </item>
    <item>
      <title>Homoscedasticity of Residuals Verified through Model Diagnostics</title>
      <link>https://freshrimpsushi.github.io/en/posts/681/</link>
      <pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/681/</guid>
      <description>Diagnostic Methods 1 Standardized residual plots can be used to check if a regression analysis was conducted properly. To verify homoscedasticity, one should check if the scatter of the residuals is uniformly distributed overall. Common examples of lack of homoscedasticity include the following two cases. The variance increases towards the end, a situation that often requires a transformation or the introduction of weights to resolve. Regardless of how easy it</description>
    </item>
    <item>
      <title>Residual Linearity Verified through Model Diagnostics</title>
      <link>https://freshrimpsushi.github.io/en/posts/677/</link>
      <pubDate>Wed, 12 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/677/</guid>
      <description>Diagnostic Techniques 1 Standardized residual plots can be used to check if the regression analysis was performed correctly. To check for linearity, see if the residuals are symmetrically distributed around $0$. Looking at the figure on the right, it is evident that there is a lack of linearity. If it were a simple regression analysis, it would result in an inability to explain the trend of the data at all.</description>
    </item>
    <item>
      <title>Regression Model Diagnostics</title>
      <link>https://freshrimpsushi.github.io/en/posts/675/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/675/</guid>
      <description>Necessity In the case of simple regression analysis, since it involves only one independent variable and one dependent variable, making it a $2$ dimensional analysis, it is easy to visually confirm if the analysis was conducted properly. However, for multiple regression analyses that exceed $3$ dimensions, it becomes difficult to represent the data graphically, making it hard to verify the accuracy of the analysis. There are instances where the analysis</description>
    </item>
    <item>
      <title>How to Interpret Multiple Regression Analysis Results in R</title>
      <link>https://freshrimpsushi.github.io/en/posts/670/</link>
      <pubDate>Wed, 05 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/670/</guid>
      <description>Data Exploration tail(attitude) In R, let&amp;rsquo;s load the built-in data attitude and check it using the tail() function. We are interested in performing multiple regression analysis on this data. We are interested in how the other independent variables affect the rating, which is our dependent variable. It&amp;rsquo;s difficult to see if there is a linear relationship between rating and the other variables just by looking at the data, so let&amp;rsquo;s</description>
    </item>
    <item>
      <title>Multiple Regression Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/666/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/666/</guid>
      <description>Overview Regression analysis is a method used to discover the relationships between variables, particularly useful for identifying linear relationships. Multiple Linear Regression refers to the regression analysis that determines the effects of multiple independent variables (explanatory variables) on a single dependent variable (response variable). Model 1 $$Y = \beta_{0} + \beta_{1} X_{1} + \cdots + \beta_{p} X_{p} + \varepsilon $$ We are interested in whether variables have a linear relationship</description>
    </item>
    <item>
      <title>Simple Regression Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/648/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/648/</guid>
      <description>Overview Regression Analysis is a method for identifying relationships between variables, especially useful for elucidating linear relationships. Simple Linear Regression is the simplest among them, referring to regression analysis on one dependent (response) variable and one independent (explanatory) variable. Model 1 The statement that independent variable $x_{i}$ and dependent variable $y_{i}$ have a linear relationship means that for some $a,b$, it can be expressed as $y_{i} = ax_{i} + b$.</description>
    </item>
    <item>
      <title>Fitted Values, Predicted Values, Residuals, Errors</title>
      <link>https://freshrimpsushi.github.io/en/posts/650/</link>
      <pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/650/</guid>
      <description>Definition 1 The regression equation obtained through regression analysis $Y \gets X_{1} + X_{2} + \cdots + X_{n}$ is denoted as $y = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \cdots + \beta_{n} x_{n}$, and let&amp;rsquo;s indicate the n-th data as $(y_{i} , x_{i1} , x_{i2} , \cdots , x_{in})$. Mean: $$ \displaystyle \overline{y} := {{1} \over {n}} \sum_{i=1}^{n} y_{i} $$ Fitted Value: For the n-th data $y_{i}$ $$</description>
    </item>
    <item>
      <title>Design Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/550/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/550/</guid>
      <description>Buildup Let&amp;rsquo;s load the built-in data faithful in R and check it with the head() function. Though only six, at a glance, eruptions and waiting seem to have a positive correlation. It would be nice if their relationship could be represented by some two constants $\beta_{0}, \beta_{1}$ such that $$\text{(eruptions)} = \beta_{0} + \beta_{1} \cdot \text{(waiting) }$$ The above equation represents the linear relationship between the two variables as the</description>
    </item>
    <item>
      <title>What is Regression Analysis?</title>
      <link>https://freshrimpsushi.github.io/en/posts/548/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/548/</guid>
      <description>Description Regression analysis is so ubiquitous a foundation of nearly all statistical techniques that it is often described either too generally or too specifically. If one were to explain what regression analysis is in a sentence for someone curious, it could be described as a method for discovering the relationships between variables. This useful and astonishing method of analysis was born from the ideas of Francis Galton, the father of</description>
    </item>
    <item>
      <title>Kringing in Spatial Data Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/2521/</link>
      <pubDate>Sun, 10 Feb 1924 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/2521/</guid>
      <description>Model Ordinary Kriging In Spatial Data Analysis, for a Random Field $\mathbf{Y} = \left( Y \left( s_{1} \right) , \cdots , Y \left( s_{n} \right) \right)$ following a Multivariate Normal Distribution with Mean $\mu \in \mathbb{R}$ and Covariance Matrix $\Sigma \in \mathbb{R}^{n \times n}$, the value estimated for a new site $s_{0}$ using the model $$ \mathbf{Y} = \mu \mathbf{1} + \varepsilon $$ is called the Ordinary Kriging Estimate. The</description>
    </item>
  </channel>
</rss>
