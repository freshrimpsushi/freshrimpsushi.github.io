[{"categories":"측도론","contents":"Definition1 Let\u0026rsquo;s say a function $f : \\mathbb{R} \\to \\mathbb{R}( \\text{or } \\mathbb{C})$ is given. If for any finite number of mutually disjoint intervals $(a_{i}, b_{i}) \\sub [a,b]$, the following condition is satisfied, then it is said to be absolutely continuousabsolutely continuous on $[a, b]$.\n$$ \\forall \\epsilon \\gt 0 \\quad \\exist \\delta \\gt 0 \\text{ such that } \\sum\\limits_{i=1}^{N} (b_{i} - a_{i}) \\lt \\delta \\implies \\sum\\limits_{i=1}^{N} \\left| f(b_{j}) - f(a_{j}) \\right| \\lt \\epsilon $$\nExplanation According to the definition, if it is absolutely continuous, it is also uniformly continuous.\nProperties If $f$ is differentiable and its derivative $f^{\\prime}$ is bounded, then $f$ is absolutely continuous.\nSee Also Absolute Continuity of Real Functions Absolute Continuity of Measures Absolute Continuity of Signed Measures Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p105\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3542,"permalink":"https://freshrimpsushi.github.io/en/posts/3542/","tags":null,"title":"Absolutely Continuous Real Function"},{"categories":"줄리아","contents":"Overview InfiniteArrays.jl is a package that enables the use of arrays of infinite size1, and is, in fact, closely related to Lazy Arrays. Lazy Evaluation refers to the method where the computer knows what needs to be computed but postpones the calculation until it is absolutely necessary. Obviously, computers cannot understand infinity, but this method allows for the implementation of infinite arrays on computers.\nCode ∞ julia\u0026gt; using InfiniteArrays\rjulia\u0026gt; 3141592 \u0026lt; ∞\rtrue\rjulia\u0026gt; Inf == ∞\rtrue\rjulia\u0026gt; Inf === ∞\rfalse Loading InfiniteArrays.jl, you first become able to express infinity with the ∞ symbol. Though you can express infinity as Inf without this package, using this symbol allows for a more intuitive use. While they hold the same magnitude of infinity in comparison, they can be differentiated as pointers.\nℵ₀ julia\u0026gt; x = zeros(Int64, ∞);\rjulia\u0026gt; length(x)\rℵ₀ Creating an infinite array filled with zeros results in a [countably infinite set], hence its size is aleph-zero $\\aleph_{0}$.\nCan be used ordinarily julia\u0026gt; x[2] = 3; x[94124843] = 7; x\rℵ₀-element LazyArrays.CachedArray{Int64, 1, Vector{Int64}, Zeros{Int64, 1, Tuple{InfiniteArrays.OneToInf{Int64}}}} with indices OneToInf():\r0\r3\r0\r0\r0\r0\r0\r0\r0\r⋮\rjulia\u0026gt; sum(x)\r10 Having an infinite array doesn\u0026rsquo;t mean the interface changes significantly. You can handle it just like any regular array, and it will work as expected.\nComplete code using InfiniteArrays\r3141592 \u0026lt; ∞\rInf == ∞\rInf === ∞\rx = zeros(Int64, ∞);\rlength(x)\rx[2] = 3; x[94124843] = 7; x\rsum(x) Environment OS: Windows julia: v1.7.3 https://github.com/JuliaArrays/InfiniteArrays.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2511,"permalink":"https://freshrimpsushi.github.io/en/posts/2511/","tags":null,"title":"How to Use Infinite Arrays in Julia"},{"categories":"줄리아","contents":"Overview MAT.jl is a library for reading and writing *.mat files, which are the data storage format used in MATLAB1. As is typical of Julia, this package does not force users to abandon their existing programming languages and habits; instead, it aims to secure users by providing an environment that is as familiar as possible.\nWhile the speed and convenience of Julia are significant advantages, MATLAB offers unique benefits for visualization for research purposes. If you are already proficient at drawing with MATLAB, a \u0026lsquo;complete transition to Julia\u0026rsquo; may seem less appealing due to the significant sacrifices it entails. The existence of MAT.jl presents a tempting proposition: \u0026ldquo;Don\u0026rsquo;t worry about that, just perform your calculations quickly with Julia and then go back to MATLAB for drawing.\u0026rdquo; Conversely, it is also helpful in situations where \u0026lsquo;you have already done much work and built structures with MATLAB but feel some limitations and wish to switch to Julia\u0026rsquo;. To learn about Julia\u0026rsquo;s advanced native storage format, refer to the JLD2.jl package.\nCode X = rand(0:9, 8, 3)\rusing MAT\rmatwrite(\u0026#34;example.mat\u0026#34;, Dict(\u0026#34;Y\u0026#34; =\u0026gt; X))\rmatfile = matopen(\u0026#34;elpmaxe.mat\u0026#34;)\rA = read(matfile, \u0026#34;A\u0026#34;)\rclose(matfile) Julia → MATLAB MATLAB → Julia Environment OS: Windows julia: v1.7.3 MAT v0.10.3 MATLAB: R2022b https://github.com/JuliaIO/MAT.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2509,"permalink":"https://freshrimpsushi.github.io/en/posts/2509/","tags":null,"title":"Reading and Writing mat Files in Julia"},{"categories":"줄리아","contents":"Overview UnicodePlots.jl is a library that uses Unicode characters to print graphics in the Julia REPL1, enabling lightweight yet high-quality visualization as the program runs.\nCode using UnicodePlots\rp1 = lineplot(100 |\u0026gt; randn |\u0026gt; cumsum)\rp1 = lineplot!(p1, 100 |\u0026gt; randn |\u0026gt; cumsum); p1\rUnicodePlots.heatmap(cumsum(abs.(randn(100,100)), dims=2)) The result of running the above example code is as follows.\nEnvironment OS: Windows julia: v1.7.3 UnicodePlots v3.0.4 https://github.com/JuliaPlots/UnicodePlots.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2507,"permalink":"https://freshrimpsushi.github.io/en/posts/2507/","tags":null,"title":"How to Output Simple Graphics in the Julia Console"},{"categories":"줄리아","contents":"Method In the console, pressing Ctrl + L appears to clear the console completely, but in some environments, it does not actually reset but rather scrolls the window as if it were pushed up. To cleanly remove or not rely on keyboard input, printing ASCII character \\033c can be used12.\nprint(\u0026#34;\\033c\u0026#34;) Also, printing \\007 will play a notification sound. 3 It\u0026rsquo;s surprisingly useful when you want to hear the end of a simple simulation or similar event through sound.\nprintln(\u0026#34;\\007\u0026#34;) Environment OS: Windows julia: v1.7.3 https://stackoverflow.com/questions/26548687/julia-how-to-clear-console\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/questions/47503734/what-does-printf-033c-mean\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://discourse.julialang.org/t/how-to-play-a-sound-or-tone-when-a-program-ends/41239/13\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2505,"permalink":"https://freshrimpsushi.github.io/en/posts/2505/","tags":null,"title":"How to Initialize the Console in Julia"},{"categories":"줄리아","contents":"Overview 1 In Julia, you can easily remove missing values using the dropmissing() function.\nCode julia\u0026gt; df = DataFrame(x = [\u0026#34;i\u0026#34;, missing, \u0026#34;k\u0026#34;, \u0026#34;j\u0026#34;], y = [1, 2, 3, missing])\r4×2 DataFrame\rRow │ x y │ String? Int64? ─────┼──────────────────\r1 │ i 1\r2 │ missing 2\r3 │ k 3\r4 │ j missing Suppose we have a dataframe with missing values missing as shown above.\njulia\u0026gt; dropmissing(df, :x)\r3×2 DataFrame\rRow │ x y │ String Int64? ─────┼─────────────────\r1 │ i 1\r2 │ k 3\r3 │ j missing julia\u0026gt; dropmissing(df, :y)\r3×2 DataFrame\rRow │ x y │ String? Int64 ─────┼────────────────\r1 │ i 1\r2 │ missing 2\r3 │ k 3 To remove missing values from the column you want, just put the symbol of the column as an argument.\njulia\u0026gt; dropmissing(df)\r2×2 DataFrame\rRow │ x y │ String Int64\r─────┼───────────────\r1 │ i 1\r2 │ k 3 If you want to remove all missing values from the entire dataframe, you don\u0026rsquo;t need to input any column.\nFull Code using DataFrames\rdf = DataFrame(x = [\u0026#34;i\u0026#34;, missing, \u0026#34;k\u0026#34;, \u0026#34;j\u0026#34;], y = [1, 2, 3, missing])\rdropmissing(df, :x)\rdropmissing(df, :y)\rdropmissing(df) Environment OS: Windows julia: v1.7.3 https://discourse.julialang.org/t/how-to-remove-rows-containing-missing-from-dataframe/12234/7\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2503,"permalink":"https://freshrimpsushi.github.io/en/posts/2503/","tags":null,"title":"Removing Missing Values in DataFrames in Julia"},{"categories":"머신러닝","contents":"Overview1 2 Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam.\nExplanation In gradient descent, the learning rate learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\\alpha$, $\\eta$, and determines how much of the gradient is taken into account when updating the parameter.\nOptimization Patterns with Learning Rate: Large Learning Rate (Left), Small Learning Rate (Right)\"\r$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L (\\boldsymbol{\\theta}_{i}) $$\nIn basic gradient descent, $\\alpha$ is described as a constant, and in this case, gradient is a vector, so the same learning rate applies to all variables (parameters) as follows\n$$ \\alpha \\nabla L (\\boldsymbol{\\theta}) = \\alpha \\begin{bmatrix} \\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} = \\begin{bmatrix} \\alpha\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nSo, by thinking of the learning rate as a vector like $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{k})$, we can generalize the gradient term to the expression below.\n$$ \\boldsymbol{\\alpha} \\odot \\nabla L (\\boldsymbol{\\theta}) = \\begin{bmatrix} \\alpha_{1}\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha_{2}\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha_{k}\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nwhere $\\odot$ is the matrix\u0026rsquo;s [Adamar product (componentwise product)] (../3436). The learning rate $\\boldsymbol{\\alpha}$, which varies from parameter to parameter in this way, is called the adaptive learning rateadaptive learning rate. Since the following techniques rely on the gradient to determine the adaptive learning rate, $\\boldsymbol{\\alpha}$ can be viewed as a function of $\\boldsymbol{\\alpha}$.\n$$ \\boldsymbol{\\alpha} (\\nabla L(\\boldsymbol{\\theta})) = \\begin{bmatrix} \\alpha_{1}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\alpha_{2}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\dots \u0026amp; \\alpha_{k}(\\nabla L(\\boldsymbol{\\theta})) \\end{bmatrix} $$\nBelow we introduce AdaGrad, RMSProp, and Adam. It is important to note that there is no absolute winner among these optimizers, including the [momentum] (../3528) technique. Different fields and different tasks require different optimizers, so it\u0026rsquo;s not a good idea to make a judgment or question about \u0026ldquo;what\u0026rsquo;s best\u0026rdquo;. It\u0026rsquo;s helpful to find out what\u0026rsquo;s commonly used in your field, and if you don\u0026rsquo;t have that or aren\u0026rsquo;t sure, you can try SGD+Momentum or Adam.\nAdaGrad AdaGrad is an adaptive learning rate technique introduced in the paper \u0026ldquo;Adaptive subgradient methods for online learning and stochastic optimization\u0026rdquo; by Duchi et al. (2011). The name is short for adaptive gradient and is pronounced as [AY-duh-grad] or [AH-duh-grad]. In AdaGrad, the learning rate for each parameter is set inversely proportional to the gradient. Consider the vector $\\mathbf{r}$ as follows.\n$$ \\mathbf{r} = (\\nabla L) \\odot (\\nabla L) = \\begin{bmatrix} \\left( \\dfrac{\\partial L}{\\partial \\theta_{1}} \\right)^{2} \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{2}} \\right)^{2} \u0026amp; \\cdots \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{k}} \\right)^{2} \\end{bmatrix} $$\nFor a global learning rate global learning rate $\\epsilon$, an arbitrary small number $\\delta$, the adaptive learning rate $\\boldsymbol{\\alpha}$ is given by\n$$ \\boldsymbol{\\alpha} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} $$\nAs you can see from the formula, we apply a smaller learning rate for variables with larger components of the gradient and a larger learning rate for variables with smaller components of the gradient. The $\\delta$, as you might guess, prevents the denominator from being $0$ or too small a number, and is often used between $10^{-5} \\sim 10^{-7}$, and is often used for values between $10^{-5} and $10^{-7}. Also, the learning rate is cumulative from iteration to iteration. The gradient at the $i$th iteration is called $\\nabla L _{i} = \\nabla L (\\boldsymbol{\\theta}_{i})$,\n$$ \\begin{align} \\mathbf{r}_{i} \u0026amp;= (\\nabla L_{i}) \\odot (\\nabla L_{i}) \\nonumber \\\\ \\boldsymbol{\\alpha}_{i} \u0026amp;= \\boldsymbol{\\alpha}_{i-1} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = \\sum_{j=1}^{i} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} \\\\ \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i} \\nonumber \\end{align} $$\nAlgorithm: AdaGrad Input Global learning rate $\\epsilon$, small positive value $\\delta$, epochs $N$ 1. Initialize parameters $\\boldsymbol{\\theta}$ with random values. 2. Initialize learning rates $\\boldsymbol{\\alpha} = \\mathbf{0}$. 3. for $i = 1, \\cdots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # Compute gradients and square each component 5. $\\boldsymbol{\\alpha} \\leftarrow \\boldsymbol{\\alpha} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # Update adaptive learning rates 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # Update parameters 7. end for RMSProp RMSProp, short for Root Mean Square Propagation, is an adaptive learning rate technique proposed in Geoffrey Hinton\u0026rsquo;s lecture Neural networks for machine learning. It is basically a variant of AdaGrad, only with the addition of $(1)$ replaced by a [weighted sum] (../2470/#exponentially weighted average) so that the added term decreases exponentially. For $\\rho \\in (0,1)$,\n$$ \\boldsymbol{\\alpha}_{i} = \\rho \\boldsymbol{\\alpha}_{i-1} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = (1-\\rho) \\sum_{j=1}^{i} \\rho^{i-j} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} $$\nLarge values such as $\\rho = 0.9, 0.99$ are commonly used.\nAlgorithm: RMSProp Input Global learning rate $\\epsilon$, small positive value $\\delta$, decay rate $\\rho$, epochs $N$ 1. Initialize parameters $\\boldsymbol{\\theta}$ with random values. 2. Initialize learning rates $\\boldsymbol{\\alpha} = \\mathbf{0}$. 3. for $i = 1, \\dots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # Compute gradients and square each component 5. $\\boldsymbol{\\alpha} \\leftarrow \\rho \\boldsymbol{\\alpha} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # Update adaptive learning rates 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # Update parameters 7. end for Adam Adamderives from \u0026quot;apative moments\u0026quot; is an optimizer introduced in the paper \u0026quot;Adam: A method for stochastic optimization\u0026quot; (Kingma and Ba, 2014). It combines the adaptive learning rate and the momentum technique and can be thought of as RMSProp + momentum. Once you understand RMSProp and momentum, it\u0026rsquo;s not hard to understand Adam. Here\u0026rsquo;s how RMSProp, momentum, and Adam compare to each other If $\\nabla L_{i} = \\nabla L(\\boldsymbol{\\theta}_{i})$,\nMomentum\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + \\nabla L_{i} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\mathbf{p}_{i}$\rRMSProp\r$\\mathbf{r}_{i} = \\nabla L_{i} \\odot \\nabla L_{i} \\\\\r\\boldsymbol{\\alpha}_{i} = \\beta_{2} \\boldsymbol{\\alpha}_{i-1} + (1-\\beta_{2})\\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} \\quad (\\boldsymbol{\\alpha}_{0}=\\mathbf{0})\\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i}$\rAdam\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + (1-\\beta_{1}) \\nabla L_{i-1} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\\\\[0.5em]\r\\hat{\\mathbf{p}}_{i} = \\dfrac{\\mathbf{p}_{i}}{1-(\\beta_{1})^{i}} \\\\\\\\[0.5em]\r\\mathbf{r}_{i} = \\beta_{2} \\mathbf{r}_{i-1} + (1-\\beta_{2}) \\nabla L_{i} \\odot \\nabla L_{i} \\\\\\\\[0.5em]\r\\hat{\\mathbf{r}}_{i} = \\dfrac{\\mathbf{r}}{1-(\\beta_{2})^{i}} \\\\\\\\[0.5em]\r\\hat{\\boldsymbol{\\alpha}}_{i} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}_{i}}} \\\\\\\\[0.5em]\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\hat{\\boldsymbol{\\alpha}_{i}} \\odot \\hat{\\mathbf{p}_{i}}\r$\rThe reason we divide $1 - \\beta^{i}$ when calculating $\\hat{\\mathbf{p}}_{i}$ and $\\hat{\\mathbf{r}}_{i}$ is to make them weighted averages since $\\mathbf{p}_{i}$ and $\\mathbf{r}_{i}$ are weighted sums.\nAlgorithm: Adam\rInput Global learning rate $\\epsilon$ (recommended value is $0.001$), epochs $N$ Small constant $\\delta$ (recommended value is $10^{-8}$) Decay rates $\\beta_{1}, \\beta_{2}$ (recommended values are $0.9$ and $0.999$ respectively) 1. Initialize parameters $\\boldsymbol{\\theta}$ with random values. 2. Initialize learning rates $\\boldsymbol{\\alpha} = \\mathbf{0}$. 3. Initialize momentum $\\mathbf{p} = \\mathbf{0}$. 4. for $i = 1, \\dots, N$ do 5. $\\mathbf{p} \\leftarrow \\beta_{1}\\mathbf{p} + (1-\\beta_{1}) \\nabla L$ # Update momentum using weighted sum 6. $\\hat{\\mathbf{p}} \\leftarrow \\dfrac{\\mathbf{p}}{1-(\\beta_{1})^{i}}$ # Correct with weighted average 7. $\\mathbf{r} \\leftarrow \\beta_{2} \\mathbf{r} + (1-\\beta_{2}) \\nabla L \\odot \\nabla L$ # Update gradient square vector with weighted sum 8. $\\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1-(\\beta_{2})^{i}}$ # Correct with weighted average 9. $\\hat{\\boldsymbol{\\alpha}} \\leftarrow \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}}}$ # Update adaptive learning rates 10. $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\alpha}} \\odot \\hat{\\mathbf{p}}$ # Update parameters 11. end for Ian Goodfellow, Deep Learning, 8.5 Algorithms with Adaptive Learning Rates\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n오일석, 기계 학습(MACHINE LEARNING) (2017), ch 5.4 적응적 학습률\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3529,"permalink":"https://freshrimpsushi.github.io/en/posts/3529/","tags":null,"title":"Adaptive Learning Rates: AdaGrad, RMSProp, Adam"},{"categories":"줄리아","contents":"Overview This document explains how to reference environment variables in Julia1.\nCode Base.ENV\rBase.ENV[\u0026#34;JULIA_NUM_THREADS\u0026#34;] As you can see, accessing environment variables does not require loading any separate package; you can directly access them through Base.ENV. Since they are read as a dictionary, using the name of the desired environment variable as a key will return the environment variable as a string. The results of executing the above two lines of code are as follows.\njulia\u0026gt; Base.ENV\rBase.EnvDict with 62 entries:\r\u0026#34;ALLUSERSPROFILE\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\u0026#34;\r\u0026#34;APPDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\AppData\\\\Roaming\u0026#34;\r\u0026#34;CHROME_CRASHPAD_PIPE_NAME\u0026#34; =\u0026gt; \u0026#34;\\\\\\\\.\\\\pipe\\\\crashpad_14984_WLSYYXMTXMJWXZQG\u0026#34;\r\u0026#34;COMMONPROGRAMFILES\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\Common Files\u0026#34;\r\u0026#34;COMMONPROGRAMFILES(X86)\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files (x86)\\\\Common Files\u0026#34;\r\u0026#34;COMMONPROGRAMW6432\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\Common Files\u0026#34;\r\u0026#34;COMPUTERNAME\u0026#34; =\u0026gt; \u0026#34;SICKRIGHT\u0026#34;\r\u0026#34;COMSPEC\u0026#34; =\u0026gt; \u0026#34;C:\\\\WINDOWS\\\\system32\\\\cmd.exe\u0026#34;\r\u0026#34;CUDA_PATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.5\u0026#34;\r\u0026#34;CUDA_PATH_V11_5\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.5\u0026#34;\r\u0026#34;DRIVERDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData\u0026#34;\r\u0026#34;GOPATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\go\u0026#34;\r\u0026#34;HOMEDRIVE\u0026#34; =\u0026gt; \u0026#34;C:\u0026#34;\r\u0026#34;HOMEPATH\u0026#34; =\u0026gt; \u0026#34;\\\\Users\\\\rmsms\u0026#34;\r\u0026#34;JULIA_NUM_THREADS\u0026#34; =\u0026gt; \u0026#34;16\u0026#34;\r\u0026#34;LOCALAPPDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\AppData\\\\Local\u0026#34;\r\u0026#34;LOGONSERVER\u0026#34; =\u0026gt; \u0026#34;\\\\\\\\SICKRIGHT\u0026#34;\r\u0026#34;NAVER\u0026#34; =\u0026gt; \u0026#34;e=2.718281\u0026#34;\r\u0026#34;NUMBER_OF_PROCESSORS\u0026#34; =\u0026gt; \u0026#34;16\u0026#34;\r\u0026#34;NVCUDASAMPLES11_5_ROOT\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v11.5\u0026#34;\r\u0026#34;NVCUDASAMPLES_ROOT\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v11.5\u0026#34;\r\u0026#34;NVTOOLSEXT_PATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NvToolsExt\\\\\u0026#34;\r\u0026#34;ONEDRIVE\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive - knu.ac.kr\u0026#34;\r\u0026#34;ONEDRIVECOMMERCIAL\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive - knu.ac.kr\u0026#34;\r\u0026#34;ONEDRIVECONSUMER\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive\u0026#34;\r\u0026#34;OPENBLAS_MAIN_FREE\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;\r\u0026#34;OPENBLAS_NUM_THREADS\u0026#34; =\u0026gt; \u0026#34;8\u0026#34;\r⋮ =\u0026gt; ⋮\rjulia\u0026gt; Base.ENV[\u0026#34;JULIA_NUM_THREADS\u0026#34;]\r\u0026#34;16\u0026#34; Environment OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/base/#Base.ENV\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2499,"permalink":"https://freshrimpsushi.github.io/en/posts/2499/","tags":null,"title":"How to Reference Environment Variables in Julia"},{"categories":"머신러닝","contents":"Overview1 2 In gradient descent, the momentum technique involves using all previous gradients when updating parameters. This is its essence and the end of the explanation. However, explanations involving strange update equations, the motivation from physics\u0026rsquo; momentum, setting the mass to $1$ and the initial velocity to $0$, only complicate understanding. This article explains the momentum technique as plainly as possible.\nBuild-Up Let\u0026rsquo;s denote the parameters as $\\boldsymbol{\\theta}$ and the loss function as $L$. The standard gradient descent method updates the parameters iteratively as follows:\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} $$\nHere, $L_{i} = L(\\boldsymbol{\\theta}_{i})$ represents the loss function calculated in the $i$th iteration. The momentum technique simply adds $\\nabla L_{i-1}$, the gradient of the loss function calculated in the previous iteration.\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\alpha \\nabla L_{i-1} - \\cdots - \\alpha \\nabla L_{0} $$\nAs iterations progress, to reduce the impact of the gradient and prevent the sum of gradients from diverging, a coefficient $\\beta \\in (0,1)$ is added as follows:\n$$ \\begin{align} \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\beta \\alpha \\nabla L_{i-1} - \\cdots - \\beta^{i}\\alpha \\nabla L_{0} \\nonumber \\\\ \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha\\sum_{j=0}^{i} \\beta^{j} \\nabla L_{i-j} \\end{align} $$\nDefinition Updating the parameters as in $(1)$ is called the momentum method, and the added term $\\alpha\\sum\\limits_{j=0}^{i} \\beta^{j} \\nabla L_{i-j}$ is referred to as momentum.\nExplanation According to the definition, the momentum method is a generalized form of gradient descent. In fact, gradient descent can be seen as a special case of the momentum method where $\\beta = 0$. The closer $\\beta$ is to $1$, the more it reflects previous gradients, and vice versa when it\u0026rsquo;s closer to $0$.\nGradient descent updates parameters in the direction of the steepest current gradient and is thus a greedy algorithm. The momentum method mitigates this greediness, enabling choices that may not be the best in the short term but are more beneficial in the long run. It also helps prevent sudden and excessive changes in gradient direction.\nObviously, since the magnitude of the gradient used to update parameters is larger compared to standard gradient descent, the momentum method has the advantage of faster convergence. Empirically, it is also known to escape local minima more effectively, similar to how a ball rolling down a hill can overcome small bumps if it has enough speed.\nIt is important to note that there is no absolute superiority among these optimizers, including adaptive learning rate techniques. The optimal optimizer varies by field and task, so it\u0026rsquo;s unwise to ask or judge which is the best. It\u0026rsquo;s helpful to learn what is commonly used in your field, or if unsure, to try SGD+momentum or Adam.\nNesterov Momentum To summarize the momentum method: to obtain the next parameter $\\boldsymbol{\\theta}_{i+1}$, the current gradient $\\alpha \\nabla L(\\boldsymbol{\\theta}_{i})$ is cumulatively added to the current parameter $\\boldsymbol{\\theta}_{i}$.\nNesterov momentum, or Nesterov accelerated gradient (NAG), computes the gradient at \u0026ldquo;the current parameter plus the previous gradient\u0026rdquo; and adds this to the current parameter to obtain the next one. This might sound complicated, but if you understand the momentum method, the following algorithm may make Nesterov momentum easier to grasp.\nAlgorithm Let\u0026rsquo;s denote the momentum term as $\\mathbf{p}$.\nAlgorithm: Momentum Method Input Learning rate $\\alpha$, momentum parameter $\\beta$, epochs $N$ 1. Initialize parameters $\\boldsymbol{\\theta}$ to random values. 2. Initialize momentum $\\mathbf{p} = \\mathbf{0}$. 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta})$ # Update momentum with calculated gradient 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # Update parameters 6. end for Algorithm: Nesterov Momentum Input Learning rate $\\alpha$, momentum parameter $\\beta$, epochs $N$ 1. Initialize parameters $\\boldsymbol{\\theta}$ to random values. 2. Initialize momentum $\\mathbf{p} = \\mathbf{0}$. 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta} + \\beta \\mathbf{p})$ # Update momentum with calculated gradient 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # Update parameters 6. end for Looking at the first few calculations of both methods, we can represent it as follows. If we simply denote $\\mathbf{p}_{i} = \\alpha \\nabla L_{i}$, and $\\mathbf{p}^{i} = \\alpha \\nabla L(\\boldsymbol{\\theta}_{i} - \\beta^{1}\\mathbf{p}^{i-1} - \\beta^{2}\\mathbf{p}^{i-2} - \\cdots - \\beta^{i}\\mathbf{p}^{0})$ (here $\\mathbf{p}^{0} = \\mathbf{p}_{0}$), then\nMomentum Nesterov Momentum $\\boldsymbol{\\theta}\\_{0} =$ initial\r$\\boldsymbol{\\theta}\\_{0} =$ initial\r$\\boldsymbol{\\theta}\\_{1} = \\boldsymbol{\\theta}\\_{0} - \\alpha \\nabla L\\_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}\\_{0} - \\mathbf{p}\\_{0}$\r$\\boldsymbol{\\theta}\\_{1} = \\boldsymbol{\\theta}\\_{0} - \\alpha \\nabla L\\_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}\\_{0} - \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}\\_{2} = \\boldsymbol{\\theta}\\_{1} - \\alpha\\nabla L\\_{1} - \\beta \\mathbf{p}\\_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}\\_{1} - \\mathbf{p}\\_{1} - \\beta \\mathbf{p}\\_{0}$\r$\\boldsymbol{\\theta}\\_{2} = \\boldsymbol{\\theta}\\_{1} - \\alpha \\nabla L(\\boldsymbol{\\theta}\\_{1} - \\beta \\mathbf{p}^{0}) - \\beta \\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}\\_{1} - \\mathbf{p}^{1} - \\beta \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}\\_{3} = \\boldsymbol{\\theta}\\_{2} - \\mathbf{p}\\_{2} - \\beta \\mathbf{p}\\_{1} - \\beta^{2} \\mathbf{p}\\_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}\\_{2} - \\sum\\limits\\_{j=0}^{2}\\beta^{j}\\mathbf{p}\\_{2-j}$\r$\\boldsymbol{\\theta}\\_{3} = \\boldsymbol{\\theta}\\_{2} - \\alpha \\nabla L(\\boldsymbol{\\theta}\\_{2} - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0}) - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}\\_{2} - \\mathbf{p}^{2} - \\beta \\mathbf{p}^{1} - \\beta^{2} \\mathbf{p}^{0}$\r$$\\vdots$$\r$$\\vdots$$\r$\\boldsymbol{\\theta}\\_{i+1} = \\boldsymbol{\\theta}\\_{i} - \\sum\\limits\\_{j=0}^{i}\\beta^{j}\\mathbf{p}\\_{i-j}$\r$\\boldsymbol{\\theta}\\_{i+1} = \\boldsymbol{\\theta}\\_{i} - \\sum\\limits\\_{j=0}^{i}\\beta^{j}\\mathbf{p}^{i-j}$\rIan Goodfellow, Deep Learning, Ch 8.3.2 Momentum, 8.3.3 Nesterov Momentum\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Ilseok, Machine Learning (2017), Ch 5.3 Momentum\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3528,"permalink":"https://freshrimpsushi.github.io/en/posts/3528/","tags":null,"title":"Momentum Method in Gradient Descent"},{"categories":"줄리아","contents":"Overview In Julia, you can easily use a progress bar to indicate the progress of a program.\nCode ProgressMeter.jl By placing the @showprogress macro from the ProgressMeter.jl package in a for loop, you can display the progress1.\nusing ProgressMeter chi2 = [] @showprogress for n in 1:20000 push!(chi2, sum(randn(n) .^ 2)) end Compared to ProgressBars.jl below, the use of a macro makes the code more concise.\nProgressBars.jl You can wrap the iterator of a for loop with the ProgressBar() function from the ProgressBars.jl package2.\nusing ProgressBars chi2 = [] for n in ProgressBar(1:20000) push!(chi2, sum(randn(n) .^ 2)) end Regardless of the actual task, the progress of the program is beautifully displayed. Naturally, since it only indicates which iteration the for loop is currently on, it can only provide the average execution time per iteration, not an accurate prediction of the total time required.\nEnvironment OS: Windows julia: v1.7.3 https://github.com/timholy/ProgressMeter.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/cloud-oak/ProgressBars.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2497,"permalink":"https://freshrimpsushi.github.io/en/posts/2497/","tags":null,"title":"How to use progress bars in Julia"},{"categories":"줄리아","contents":"Overview R language has options within functions like sum() or mean() to ignore missing values directly, whereas Julia lacks such options but actively employs Functional Programming approaches instead.\nCode julia\u0026gt; data = [0,1,2,3,0]\r5-element Vector{Int64}:\r0\r1\r2\r3\r0\rjulia\u0026gt; sum(data) / length(data)\r1.2\rjulia\u0026gt; sum(data) / sum(!iszero, data)\r2.0 The top portion results in 1.2, dividing by the total number of samples including up to $0$, while the bottom portion achieves 2.0 by dividing by the number of samples counted excluding $0$ values using the !iszero function as an argument. What could be considered more powerful than R is that numerous exception handling functions like isnan(), isinf(), ismissing(), etc., can be used in the same manner without relying on the function\u0026rsquo;s own options, and customization is flexible.\nWhile there isn\u0026rsquo;t a significant performance difference, if the return of is~ series functions is strictly Boolean, then it wouldn\u0026rsquo;t matter if the denominator\u0026rsquo;s sum() is changed to count().\nEnvironment OS: Windows julia: v1.7.0 ","id":2495,"permalink":"https://freshrimpsushi.github.io/en/posts/2495/","tags":null,"title":"Calculating the Mean Excluding 0 or Missing Values in Julia"},{"categories":"전자기학","contents":"Question Electromagnetism is literally the study of electric fields $\\mathbf{E}$ and magnetic fields $\\mathbf{B}$. While studying electromagnetism, one might have wondered the following at least once.\nWhy is the symbol for magnetic fields $\\mathbf{B}$ used?\nIt's understandable that the electric field is $\\mathbf{E}$, derived from the Electric field, but why is the magnetic field $\\mathbf{B}$ when it should be from Magnetic field? This notation might feel oddly placed, and it's because it was decided without any significant reason.\rAnswer Maxwell\u0026rsquo;s Notation1 2 Maxwell, often called the father of electromagnetism, completed classical electromagnetism3 with Maxwell\u0026rsquo;s equations. Like Newton, Leibniz, Euler, and other individuals who made significant achievements in mathematics/science, not only their names and accomplishments but also their notations have been passed down to future generations. This is true for Maxwell as well; the notation of using $\\mathbf{B}$ for magnetic fields and $\\mathbf{H}$ for auxiliary fields has naturally continued because Maxwell noted them this way.\nThe symbols for various vectors used by Maxwell in electromagnetism4 are as follows.5 Maxwell denoted these vectors with alphabets from A to J, and in cases where there were appropriate symbols like C, D, F, he assigned them, while the rest were seemingly chosen at his discretion.\nNotation\rMeaning\rMaxwell\rToday\r$\\frak{A}$$(A)$\r$\\mathbf{A}$\rThe electromagnetic momentum at a point\rCurrently known as vector potential.\r$\\frak{B}$$(B)$\r$\\mathbf{B}$\rThe magnetic induction\rNowadays referred to as the magnetic field.\r$\\frak{C}$$(C)$\r$I$\rThe (total) electric 'C'urrent, current\r$\\frak{D}$$(D)$\r$\\mathbf{D}$\rThe electric 'D'isplacement, displacement field\r$\\frak{E}$$(E)$\r$\\mathcal{E}$\rThe 'E'lectromotive intensity\rToday called electromotive force, emf.\r$\\frak{F}$$(F)$\r$\\mathbf{F}$\rThe mechanical 'F'orce\rCurrently known as Lorentz force.\r$\\frak{G}$$(G)$\rThe velocity of a point\r$\\frak{H}$$(H)$\r$\\mathbf{H}$\rThe magnetic force\rToday, this is referred to as the H-field, auxiliary field, magnetic field intensity, etc.\r$\\frak{I}$$(I)$\r$\\mathbf{M}$\rThe 'I'ntensity of magnetization\rSeems to be what is called magnetization density today.\r$\\frak{J}$$(J)$\r$\\mathbf{J}$\rThe current of conduction, conduction current\rMost of these notations are still used today, with the current being denoted as $I$, following the initial of the intensity of current.\nFurthermore, upon researching, one may find the argument that \u0026rsquo;the symbol for magnetic fields comes from Biot in the Biot-Savart law\u0026rsquo; but, in my opinion, that\u0026rsquo;s debatable. Firstly, the question \u0026lsquo;Why is the symbol for magnetic fields $\\mathbf{B}$?\u0026rsquo; is equivalent to asking \u0026lsquo;Why do we use $\\mathbf{B}$ as the symbol for magnetic fields today?\u0026rsquo;, so the answer \u0026lsquo;Because Maxwell used it that way\u0026rsquo; seems valid. Then that aside, one might wonder, \u0026lsquo;Did Maxwell choose $\\mathbf{B}$ for the magnetic field symbol because it was named after Biot?\u0026rsquo;. There doesn\u0026rsquo;t seem to be a concrete basis for this answer either. Even if it were, it wouldn\u0026rsquo;t be because it was named after Biot\u0026rsquo;s name but rather \u0026lsquo;Among the vectors listed above, the one that best fits the symbol $\\mathbf{B}$ is magnetic induction, related to the Biot-Savart law\u0026rsquo; might be a more fitting explanation. (Honestly, the claims that it was derived from Biot, bi-polar field, boreal, etc., seem like a stretch to me)\nBetween B and H, What is the Magnetic Field? Meanwhile, there\u0026rsquo;s also discussion on whether $\\mathbf{B}$ or $\\mathbf{H}$ should be called the magnetic field. $\\mathbf{H}$ is a value that can be adjusted in experiments regardless of the medium. Hence, in engineering-related areas (for example, electrical engineering textbooks), $\\mathbf{H}$ is commonly referred to as the magnetic field, whereas in physics-related areas, $\\mathbf{B}$ is typically called the magnetic field. However, as explained in the Wikipedia article on magnetic fields, since $\\mathbf{B}$ mediates the Lorentz force, it appears consistent and logical to call $\\mathbf{E}$ the electric field, similarly $\\mathbf{B}$ should be referred to as the magnetic field.\n$$ \\text{Lorentz force}: \\mathbf{F}=Q\\left[ \\mathbf{E} + (\\mathbf{v}\\times\\mathbf{B}) \\right] $$\nhttps://www.johndcook.com/blog/2012/02/12/why-magnetic-field-b/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.cantorsparadise.com/why-the-symbol-for-magnetic-field-is-b-e40658e17ece\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNot considering quantum mechanical phenomena\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWritten in FrakturFraktur typeface.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMaxwell, James Clerk. A treatise on electricity and magnetism. Vol. 2. Oxford: Clarendon Press, 1873. page 257\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3523,"permalink":"https://freshrimpsushi.github.io/en/posts/3523/","tags":null,"title":"자기장의 기호로 B를 사용하는 이유"},{"categories":"줄리아","contents":"Overview This brief introduction presents the GLM.jl package for conducting regression analysis in Julia, emphasizing its similarity to the interface in R and thus, skipping detailed explanations1.\nCode Julia using GLM, RDatasets\rfaithful = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;faithful\u0026#34;)\rout1 = lm(@formula(Waiting ~ Eruptions), faithful) The result of running the above code is as follows:\njulia\u0026gt; out1 = lm(@formula(Waiting ~ Eruptions), faithful)\rStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\rWaiting ~ 1 + Eruptions\rCoefficients:\r───────────────────────────────────────────────────────────────────────\rCoef. Std. Error t Pr(\u0026gt;|t|) Lower 95% Upper 95%\r───────────────────────────────────────────────────────────────────────\r(Intercept) 33.4744 1.15487 28.99 \u0026lt;1e-84 31.2007 35.7481\rEruptions 10.7296 0.314753 34.09 \u0026lt;1e-99 10.11 11.3493\r─────────────────────────────────────────────────────────────────────── Compare this with the results from regression analysis in R.\nComparison with R out1\u0026lt;-lm(waiting~eruptions,data=faithful); summary(out1) out1 = lm(@formula(Waiting ~ Eruptions), faithful) The above is the code in R, and below is the code in Julia. The @formula macro was used to input variables, almost perfectly replicating the convention in R.\nEnvironment OS: Windows julia: v1.7.0 GLM v1.8.0 See Also How to perform regression analysis in R https://juliastats.org/GLM.jl/v0.11/index.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2493,"permalink":"https://freshrimpsushi.github.io/en/posts/2493/","tags":null,"title":"How to Perform Regression Analysis in Julia"},{"categories":"머신러닝","contents":"Overview Monte Carlo integration is a numerical approximation method used when calculating the integral of a given function is difficult. Consider the following scenario: For an integrable function $f$ in the given $[0, 1]$or generally $[0, 1]^{n}$, we know the formula of $f(x)$ but calculating its integral is not straightforward. However, we want to calculate the integral $I[f]$ of $f$.\n$$ \\begin{equation} I[f] = \\int_{[0,1]} f(x) dx \\end{equation} $$\nDefinition Monte Carlo integrationMonte Carlo integration is a method to estimate the integral of $f$ by drawing samples $\\left\\{ x_{i} \\right\\}$ from a distribution on $[0, 1]$ as follows:\n$$ I[f] \\approx I_{n}[f] := \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\nDifference from Riemann Sum The idea of Riemann Sum is to divide the interval $[0,1]$ into $n$ equal parts and get points $\\left\\{ x_{i} = \\frac{i-1}{n} \\right\\}_{i=1}^{n}$, then sum up the function values at these points.\n$$ \\text{mensuration by parts}[f] = \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\nAt first glance, Monte Carlo integration and Riemann Sum may seem similar, but their meanings are completely different. In Riemann Sum, $\\left\\{ x_{i} \\right\\}$ are points obtained by dividing the interval $[0, 1]$ into $n$ equal parts, whereas in Monte Carlo integration, $x$ represents $n$ samples drawn from the distribution $p(x)$. Thus, the value obtained by Riemann Sum simply represents the area under the graph drawn by $f$, while the value obtained by Monte Carlo integration represents the expectation of $f$.\nProperties The statistical meaning of equation $(1)$ is that \u0026rsquo;the $I[f]$ is equal to the expectation of $f(X)$ when $X$ follows a uniform distribution'.\n$$ X \\sim U(0,1) \\implies I[f] = \\int_{[0,1]} f(x) dx = E\\left[ f(X) \\right] $$\nExpectation Let the random variable $X$ follow a uniform distribution. $I_{n}[f]$ is an unbiased estimator of $I[f]$.\n$$ E\\left[ I_{n}[f] \\right] = I[f] $$\nProof $$ \\begin{align*} E\\left[ I_{n}[f] \\right] \u0026amp;= E\\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} E\\left[ f(X_{i}) \\right] \\qquad \\text{by linearity of $E$} \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} I\\left[ f \\right] \\\\ \u0026amp;= I\\left[ f \\right] \\end{align*} $$\n■\nVariance Proof Properties of Variance\n[a] $\\Var (aX) = a^{2} \\Var (X)$\n[b] If $X, Y$ are independent, $\\Var (X + Y) = \\Var(X) + \\Var(Y)$\nLet\u0026rsquo;s denote the variance of $f(X)$ as $\\sigma^{2}$. Then, by the properties of variance:\n$$ \\begin{align*} \\Var \\left[ I_{n}[f] \\right] \u0026amp;= \\Var \\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\Var \\left[ \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\Var \\left[ f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\sigma^{2} \\\\ \u0026amp;= \\dfrac{\\sigma^{2}}{n} \\end{align*} $$\n■\nGeneralization Now consider a function $p(x) \\ge 0$ and $\\int_{[0,1]} p = 1$. Think about the integral $I[fp]$.\n$$ I[fp] = \\int_{[0, 1]}f(x)p(x) dx $$\nThis is the same as the expectation of $f(X)$ for a random variable $X$ with a probability density function $p$. To approximate this value, we can consider the following two methods:\nDraw samples $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$ uniformly and approximate $I[fp]$ as follows: $$ X_{i} \\sim U(0,1) \\qquad I[fp] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i})p(x_{i}) $$\nDraw samples $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$ from $p(x)$ and approximate $I[fp]$ as follows: $$ X_{i} \\sim p(x) \\qquad I[fp] = I_{p}[f] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i}) $$\nIn other words, 1. is averaging $f(x)p(x)$ by uniform sampling, and 2. is averaging $f(x)$ by sampling from $p(x)$. Among these, the one with smaller variance is 1. Let\u0026rsquo;s simplify the notation as $I = I[fp] = I[fp]$.\nIn case of 1. $$ \\begin{align*} \\sigma_{1}^{2} = \\Var [fp] \u0026amp;= E \\left[ (fp - I)^{2} \\right] \\\\ \u0026amp;= \\int (fp - I)^{2} dx \\\\ \u0026amp;= \\int (fp)^{2} dx - 2I\\int fp dx + I^{2}\\int dx\\\\ \u0026amp;= \\int (fp)^{2} dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int (fp)^{2} dx - I^{2}\\\\ \\end{align*} $$\nIn case of 2. $$ \\begin{align*} \\sigma_{2}^{2} = \\Var [f] \u0026amp;= E_{p} \\left[ (f - I)^{2} \\right] \\\\ \u0026amp;= \\int (f - I)^{2}p dx \\\\ \u0026amp;= \\int f^{2}p dx - 2I\\int fp dx + I^{2}\\int pdx\\\\ \u0026amp;= \\int f^{2}p dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int f^{2}p dx - I^{2}\\\\ \\end{align*} $$\nHowever, since $0 \\le p \\le 1$, it follows that $f^{2}p \\ge f^{2}p^{2}$. Therefore,\n$$ \\sigma_{1}^{2} \\le \\sigma_{2}^{2} $$\n","id":3515,"permalink":"https://freshrimpsushi.github.io/en/posts/3515/","tags":null,"title":"Monte Carlo Integration"},{"categories":"줄리아","contents":"Code Plots.jl essentially outputs everything including grids, ticks, axes, and color bars by default, but if you want to make it clean without these, you can add the following options.\ncolorbar=:none: Removes the color bar. showaxis = false: Removes the axes and ticks. grid=false: Removes the background grid. ticks=false: Removes both background grid and ticks. framestyle=:none: Removes both background grid and axes. using Plots\rsurface(L, title=\u0026#34;default\u0026#34;)\rsurface(L, title=\u0026#34;colorbar=:none\u0026#34;, colorbar=:none)\rsurface(L, title=\u0026#34;showaxis=false\u0026#34;, showaxis=false)\rsurface(L, title=\u0026#34;grid=false\u0026#34;, grid=false)\rsurface(L, title=\u0026#34;ticks=false\u0026#34;, ticks=false)\rsurface(L, title=\u0026#34;framestyle=:none\u0026#34;, framestyle=:none)\rsurface(L, title=\u0026#34;all off\u0026#34;, ticks=false, framestyle=:none, colorbar=:none) Environment OS: Windows11 Version: Julia v1.8.3, Plots v1.38.6 ","id":3501,"permalink":"https://freshrimpsushi.github.io/en/posts/3501/","tags":null,"title":"How to Neatly Print without Axes, Scales, etc. in Julia"},{"categories":"줄리아","contents":"Summary There is no direct equivalent to the meshgrid() function used in Python and MATLAB. If you only want to obtain the function values on a grid, there is a simpler method that does not require creating a grid.\nCode 2D Multiplying a column vector by a row vector gives the same result as taking the Kronecker product of a column vector and a row vector.\nU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\r# Fig. 1\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru1 = U.(T,X)\rheatmap(t\u0026#39;, x, u1, title=\u0026#34;Fig 1\u0026#34;) When using the Kronecker product,\nusing LinearAlgebra\rX = kron(x, ones(size(t)))\rT = kron(ones(size(x)), t)\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, title=\u0026#34;Fig 2\u0026#34;)\rjulia\u0026gt; u1 == u2\rtrue 3D1 U(x,y,t) = exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\ry = LinRange(-1., 1, 100)\rt = LinRange(0.,0.35, 50)\rX = getindex.(Iterators.product(x, y, t), 1)\rY = getindex.(Iterators.product(x, y, t), 2)\rT = getindex.(Iterators.product(x, y, t), 3)\ru3 = U.(X,Y,T)\ranim = @animate for i ∈ 1:50\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1), title=\u0026#34;Anim. 1\u0026#34;)\rend Full Code using Plots\rcd = @__DIR__\rU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\r# Fig. 1\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru1 = U.(T,X)\rheatmap(t\u0026#39;, x, u1, title=\u0026#34;Fig 1\u0026#34;)\rsavefig(cd*\u0026#34;/fig1.png\u0026#34;)\r# kron\rusing LinearAlgebra\rX = kron(x, ones(size(t)))\rT = kron(ones(size(x)), t)\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, title=\u0026#34;Fig 2\u0026#34;)\rsavefig(cd*\u0026#34;/fig2.png\u0026#34;)\ru1 == u2\r# 3d\rU(x,y,t) = (1/4) * exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\rx = LinRange(-2., 2, 100)\ry = LinRange(-2., 2, 100)\rt = LinRange(0.,0.35, 50)\rX = getindex.(Iterators.product(x, y, t), 1)\rY = getindex.(Iterators.product(x, y, t), 2)\rT = getindex.(Iterators.product(x, y, t), 3)\ru3 = U.(X,Y,T)\ranim = @animate for i ∈ 1:50\rsurface(u3[:,:,i], zlims=(0,0.5), clim=(0,0.3), title=\u0026#34;Anim. 1\u0026#34;)\rend\rgif(anim, cd*\u0026#34;/anim1.gif\u0026#34;, fps=10) Environment OS: Windows11 Version: Julia v1.8.3, Plots v1.38.6 https://discourse.julialang.org/t/meshgrid-function-in-julia/48679/26\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3500,"permalink":"https://freshrimpsushi.github.io/en/posts/3500/","tags":null,"title":"How to Create a Meshgrid in Julia"},{"categories":"줄리아","contents":"Overview Introducing how to broadcast multivariable functions in Julia. Like in Python, you can create a meshgrid, or you can easily calculate by creating vectors for each dimension.\nBivariate Functions $$ u(t,x) = \\sin(\\pi x) e^{-\\pi^{2}t} $$\nTo plot the function $(t,x) \\in [0, 0.35] \\times [-1,1]$ as above, the function values can be calculated like this:\nx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\ru1 = @. sin(π*x)*exp(- π^2 * t)\rheatmap(t\u0026#39;, x, u1, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 1\u0026#34;) After defining the function itself, the same results can be obtained by creating a 2D grid as follows:\nU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 2\u0026#34;) Trivariate Functions $$ u(x,y,t) = e^{-x^{2} - 2y^{2}}e^{-\\pi^{2}t} $$\nIf you want to get the function values of $u$ over the space-time domain $(x,y,t) \\in [-1,1] \\times [-1,1] \\times [0, 0.35]$, you can just create vectors that have dimensions only for each variable and broadcast.\nIf you want to create a 3D mesh and broadcast, see here.\njulia\u0026gt; x = reshape(LinRange(-1., 1, 100), (100,1,1))\r100×1×1 reshape(::LinRange{Float64, Int64}, 100, 1, 1) with eltype Float64:\rjulia\u0026gt; y = reshape(LinRange(-1., 1, 100), (1,100,1))\r1×100×1 reshape(::LinRange{Float64, Int64}, 1, 100, 1) with eltype Float64:\rjulia\u0026gt; t = reshape(LinRange(0.,0.35, 200), (1,1,200))\r1×1×200 reshape(::LinRange{Float64, Int64}, 1, 1, 200) with eltype Float64:\rjulia\u0026gt; u3 = @. exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\r100×100×200 Array{Float64, 3}:\ranim = @animate for i ∈ 1:200\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1))\rend Code Details using Plots\rcd = @__DIR__\r# Fig. 1\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\ru1 = @. sin(π*x)*exp(- π^2 * t)\rheatmap(t\u0026#39;, x, u1, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 1\u0026#34;)\rsavefig(cd*\u0026#34;/fig1.png\u0026#34;)\r# Fig. 2\rU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 2\u0026#34;)\rsavefig(cd*\u0026#34;/fig2.png\u0026#34;)\r# gif 1\rx = reshape(LinRange(-1., 1, 100), (100,1,1))\ry = reshape(LinRange(-1., 1, 100), (1,100,1))\rt = reshape(LinRange(0.,0.35, 200), (1,1,200))\ru3 = @. exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\ranim = @animate for i ∈ 1:200\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1), title=\u0026#34;Anim. 1\u0026#34;)\rend\rgif(anim, cd*\u0026#34;/anim1.gif\u0026#34;, fps=30) Environment OS: Windows11 Version: Julia v1.8.3, Plots v1.38.6 ","id":3499,"permalink":"https://freshrimpsushi.github.io/en/posts/3499/","tags":null,"title":"Broadcasting of Multivariable Functions in Julia"},{"categories":"푸리에해석","contents":"Overview1 The Discrete Fourier Transform (DFT), when computed naively following its mathematical definition, has a time complexity of $\\mathcal{O}(N^{2})$. However, by using the algorithm described below, the time complexity can be reduced to $\\mathcal{O}(N\\log_{2}N)$. This efficient computation method of the Discrete Fourier Transform is known as the Fast Fourier Transform (FFT).\nBuildup Let\u0026rsquo;s define multiplying two numbers and then adding them to another number as one operation. To compute the value of $\\sum\\limits_{i=0}^{n-1}a_{n}b_{n}$, $n$ operations are needed.\n$$ \\begin{align*} \\sum\\limits_{n=0}^{0} a_{n}b_{b} \u0026amp;= a_{0}b_{0} = \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\\\ \\sum\\limits_{n=0}^{1} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} = \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\\\ \\sum\\limits_{n=0}^{2} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} + a_{2}b_{2} = \\overbrace{\\bigg( \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\bigg) {\\color{#FE9A2E} + } a_{2} {\\color{#FE9A2E} \\times} b_{2}}^{\\color{#FE9A2E}3 \\text{ operations}} \\\\ \\end{align*} $$\nNow, recall the definition of the Discrete Fourier Transform.\nThe linear transformation $\\mathcal{F}_{N} : \\mathbb{C}^{N} \\to \\mathbb{C}^{N}$ is called the Discrete Fourier Transform.\n$$ \\mathcal{F}_{N}(\\mathbf{a}) = \\hat{\\mathbf{a}} = \\begin{bmatrix} \\hat{a}_{0} \\\\ \\hat{a}_{1} \\\\ \\dots \\\\ \\hat{a}_{N-1} \\end{bmatrix} ,\\quad \\hat{a}_{m} = \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n}\\quad (0\\le m \u0026lt; N) \\tag{1} $$\nWhere $\\mathbf{a} = \\begin{bmatrix} a_{0}\u0026amp; a_{1}\u0026amp; \\dots\u0026amp; a_{N-1} \\end{bmatrix}^{T}$.\nTo compute $\\hat{a}_{m}$, $N$ operations are needed, and to compute $\\hat{\\mathbf{a}}$, this must be performed $N$ times. Therefore, the total number of operations needed for the Discrete Fourier Transform is $N^{2}$, which means it has a time complexity of $\\mathcal{O}(N^{2})$. This implies a significant computational cost for Fourier Transforms from a computer calculation perspective.\nAlgorithm Let\u0026rsquo;s assume the length of the data, $N$, is a composite number $N = N_{1}N_{2}$. Now, define the indices $m, n$ as follows.\n$$ m = m^{\\prime}N_{1} + m^{\\prime \\prime},\\quad n = n^{\\prime}N_{2} + n^{\\prime \\prime} $$\nThen, $0 \\le m^{\\prime}, n^{\\prime \\prime} \\le N_{2}-1$, and $0 \\le m^{\\prime \\prime}, n^{\\prime} \\le N_{1}-1$. Let\u0026rsquo;s express the exponent part of $(1)$ as follows.\n$$ \\begin{align*} e^{-i2\\pi mn /N} \u0026amp;= e^{-i2\\pi (m^{\\prime}N_{1} + m^{\\prime \\prime})(n^{\\prime}N_{2} + n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi (m^{\\prime}n^{\\prime}N_{1}N_{2} + m^{\\prime}n^{\\prime \\prime}N_{1} + m^{\\prime \\prime}n^{\\prime}N_{2} + m^{\\prime \\prime}n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi m^{\\prime}n^{\\prime}} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \u0026amp;= e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \\end{align*} $$\nSubstituting this into $(1)$ gives,\n$$ \\begin{align*} \\hat{a}_{m} \u0026amp;= \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi m^{\\prime \\prime}n^{\\prime}/N_{1}}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\right] e^{-i2\\pi [ (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) ] } \\end{align*} $$\nAccording to the equation above, calculating $\\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} \\right]$ inside each bracket requires $N_{1}$ operations, and computing $\\sum_{n^{\\prime \\prime}=0}^{N_{2}-1}$ outside each bracket requires $N_{2}$ operations. Therefore, to calculate $\\hat{a}_{m}$, a total of $(N_{1} + N_{2})$ operations are needed. To obtain $\\hat{\\mathbf{a}}$, this must be repeated $N$ times, resulting in a total cost of $N(N_{1} + N_{2})$, which is a reduction from $N^{2}$.\nUpon closer inspection of the brackets, it is apparent that if $N_{1}$ is again a composite number, the same logic can be applied. Generally, when the length of the data is a composite number $N = N_{1} N_{2} \\cdots N_{k}$, the time complexity is reduced as follows.\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}\\big( N(N_{1} + N_{2} + \\cdots + N_{k}) \\big) $$\nNow, let\u0026rsquo;s assume $N$ is a power of $2$, $N = 2^{k}$. Then $\\log_{2}N = k$, and from $N^{2} = 2^{k}$, it is reduced to $2^{k}(2k)$, hence the time complexity is reduced as follows.\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}(2N \\log_{2}N) $$\nAside This method was proposed in 1965 by Cooley and Tukey2, hence it is also called the Cooley-Tukey algorithm. However, they were not the first to invent it. Gauss also researched a similar algorithm but did not publish it properly, so this fact became known later3.\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p252-253\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. W. Cooley and J. W. Tukey, An algorithm for the machine calculation of complex Fourier series, Mathematics of Computation 19 (1965), 297-301.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. T. Heideman, D. H. Johnson, and C. S. Burms, Gauss and the history of the fast Fourier transform, Archive for the History of the Exact Sciences 34 (1985), 264-277.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3492,"permalink":"https://freshrimpsushi.github.io/en/posts/3492/","tags":null,"title":"The Fast Fourier Transform Algorithm"},{"categories":"줄리아","contents":"Overview To achieve this, one can use the canonicalize() function of the Dates module1.\nCode using Dates\rtic = DateTime(2022,3,7,7,1,11)\rtoc = now()\rDates.canonicalize(toc-tic) The result of executing the above code is as follows.\njulia\u0026gt; using Dates\rjulia\u0026gt; tic = DateTime(2022,3,7,7,1,11)\r2022-03-07T07:01:11\rjulia\u0026gt; toc = now()\r2022-07-19T22:26:22.070\rjulia\u0026gt; Dates.canonicalize(toc-tic)\r19 weeks, 1 day, 15 hours, 25 minutes, 11 seconds, 70 milliseconds It automatically calculates and outputs up to weeks, precisely as multiples of smaller units.\nEnvironment OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/get-difference-between-two-dates-in-seconds/11641/4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2461,"permalink":"https://freshrimpsushi.github.io/en/posts/2461/","tags":null,"title":"How to Calculate the Difference Between Two Times in Seconds in Julia"},{"categories":"줄리아","contents":"Summary The keywords related to specifying the color of axes and ticks in Plots.jl are as follows.\nKeyword Name Function guidefontcolor Specify axis name color foreground_color_border, fgcolor_border Specify axis color foreground_color_axis, fgcolor_axis Specify tick color foreground_color_text, fgcolor_text Specify tick value color Adding x_ or y_ in front of the keyword name applies it to the respective axis only.\nCode1 Axis Names The keyword to specify the color of axis names is guidefontcolor. Axis names can be specified with xlabel, ylabel.\nx = randn(10, 3)\rplot(plot(x, guidefontcolor = :red),\rplot(x, x_guidefontcolor = :red),\rplot(x, y_guidefontcolor = :red),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) Axes The keyword to specify the color of axes is foreground_color_border.\nplot(plot(x, foreground_color_border = :red),\rplot(x, x_foreground_color_border = :red),\rplot(x, y_foreground_color_border = :red),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) Ticks The keyword to specify the color of ticks is foreground_color_axis. Setting it to false only removes the ticks.\nplot(plot(x, foreground_color_axis = :red),\rplot(x, x_foreground_color_axis = :red),\rplot(x, y_foreground_color_axis = false),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) Tick Values The keyword to specify the color of tick values is foreground_color_text. Setting it to false only removes the tick values.\nplot(plot(x, foreground_color_text = :red),\rplot(x, x_foreground_color_text = :red),\rplot(x, y_foreground_color_text = flase),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 See Also How to use colors How to use palettes How to use color gradients Package for color processing Colors.jl How to use RGB codes RGB(1, 0, 0) How to use HEX codes \u0026quot;#000000\u0026quot; How to specify the color of graph elements How to specify colors for each subplot How to specify the color of axes, axis names, ticks, and tick values How to specify background color https://docs.juliaplots.org/stable/generated/attributes_axis/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3490,"permalink":"https://freshrimpsushi.github.io/en/posts/3490/","tags":null,"title":"Specifying the Color of Axes, Axis Names, Ticks, and Tick Values in Julia Plots"},{"categories":"머신러닝","contents":"Overview This document organizes code that performs the same functions in Flux, PyTorch, and TensorFlow.\nJulia-Matlab-Python-R Cheat Sheet Let\u0026rsquo;s assume the following environment for Flux.\nusing Flux Let\u0026rsquo;s assume the following environment for PyTorch.\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F Let\u0026rsquo;s assume the following environment for TensorFlow.\nimport tensorflow as tf\rfrom tensorflow import keras 1-Dimensional Tensor 줄리아Julia\r파이토치PyTorch\r텐서플로우TensorFlow\rColumn Vectorcolumn vector\r[1 4 -1 2] [1;4;-1;2] ","id":3489,"permalink":"https://freshrimpsushi.github.io/en/posts/3489/","tags":null,"title":"Flux-PyTorch-TensorFlow Cheat Sheet"},{"categories":"줄리아","contents":"Overview Introducing tips for switching between 2D arrays and matrices in Julia, which may be the simplest, fastest, and most beautiful way to do it, especially in environments of Julia 1.7 or lower1.\nCode There are countless ways to switch between matrices and 2D arrays, not just the method introduced here. Since the goal itself is not difficult whether you code haphazardly or not, it\u0026rsquo;s better to consider not only the goal but also how Julia\u0026rsquo;s unique syntax was used when reading this carefully.\nFrom matrices to 2D arrays julia\u0026gt; M = rand(0:9, 3, 10)\r3×10 Matrix{Int64}:\r2 4 0 1 8 0 9 2 5 7\r5 2 1 5 4 3 7 2 7 3\r7 8 1 9 0 3 2 4 1 3 Let\u0026rsquo;s convert the matrix above to a 2D array.\njulia\u0026gt; [eachrow(M)...]\r3-element Vector{SubArray{Int64, 1, Matrix{Int64}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}}:\r[2, 4, 0, 1, 8, 0, 9, 2, 5, 7]\r[5, 2, 1, 5, 4, 3, 7, 2, 7, 3]\r[7, 8, 1, 9, 0, 3, 2, 4, 1, 3]\rjulia\u0026gt; [eachcol(M)...]\r10-element Vector{SubArray{Int64, 1, Matrix{Int64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}}:\r[2, 5, 7]\r[4, 2, 8]\r[0, 1, 1]\r[1, 5, 9]\r[8, 4, 0]\r[0, 3, 3]\r[9, 7, 2]\r[2, 2, 4]\r[5, 7, 1]\r[7, 3, 3] eachrow() and eachcol() return generators that extract each row and column of the matrix2, and through the splat operator3, handling them as variable arrays and putting them inside square brackets [] naturally turns them into an array.\nFrom 2D arrays to matrices julia\u0026gt; A = [rand(0:9,3) for _ in 1:10]\r10-element Vector{Vector{Int64}}:\r[5, 4, 9]\r[9, 7, 6]\r[9, 9, 6]\r[5, 9, 0]\r[0, 2, 8]\r[3, 9, 5]\r[1, 6, 0]\r[5, 7, 7]\r[1, 3, 5]\r[5, 4, 1] Let\u0026rsquo;s covert the above 2D array into a matrix.\njulia\u0026gt; hcat(A...)\r3×10 Matrix{Int64}:\r5 9 9 5 0 3 1 5 1 5\r4 7 9 9 2 9 6 7 3 4\r9 6 6 0 8 5 0 7 5 1\rjulia\u0026gt; hcat(A...)\u0026#39;\r10×3 adjoint(::Matrix{Int64}) with eltype Int64:\r5 4 9\r9 7 6\r9 9 6\r5 9 0\r0 2 8\r3 9 5\r1 6 0\r5 7 7\r1 3 5\r5 4 1\rjulia\u0026gt; vcat(A...)\r30-element Vector{Int64}:\r5\r4\r9\r9\r7\r6\r9\r9\r⋮\r7\r1\r3\r5\r5\r4\r1 You can use the hcat() function for merging arrays4. Fundamentally, hcat() and vcat() are fold functions as well as variadic functions, so the one-dimensional arrays, which are elements of the 2D array, have to be passed directly as arguments through the splat operator.\nComplete Code # matrix to 2d array\rM = rand(0:9, 3, 10)\r[eachrow(M)...]\r[eachcol(M)...]\r# 2d array to matrix\rA = [rand(0:9,3) for _ in 1:10]\rhcat(A...)\rhcat(A...)\u0026#39;\rvcat(A...) Environment OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/plots-efficient-scatter-plot-of-a-2xn-array/31803/6\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/arrays/#Base.eachcol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/arrays/#Base.cat\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2459,"permalink":"https://freshrimpsushi.github.io/en/posts/2459/","tags":null,"title":"How to Convert between 2D Arrays and Matrices in Julia"},{"categories":"줄리아","contents":"Overview This document introduces how to send emails from Naver using the SMTPClient.jl package with SMTP 1. I use it to send reports to Kakao Mail when long-running simulations are finished, which helps to speed up my research.\nThis way, knowing when simulations are finished without having to check the server myself, as Jordy notifies me via personal chat.\nCode Regardless of the programming language, the first thing to do is to enable SMTP as \u0026lsquo;Enabled\u0026rsquo; in Naver Mail as shown below.\nJulia using Dates\rtic = now()\rfor t in 1:1000\rprintln(t)\rend\rtoc = now()\rusing SMTPClient\ropt = SendOptions(\risSSL = true,\rusername = \u0026#34;네이버아이디\u0026#34;,\rpasswd = \u0026#34;비밀번호\u0026#34;)\r#Provide the message body as RFC5322 within an IO\rbody = IOBuffer(\r\u0026#34;Date: ▷eq1◁tic\\r\\n\u0026#34; *\r\u0026#34;▷eq2◁(Dates.canonicalize(toc - tic))\u0026#34; *\r\u0026#34;\\r\\n\u0026#34;)\rurl = \u0026#34;smtps://smtp.naver.com:465\u0026#34;\rrcpt = [\u0026#34;\u0026lt;수신자@kakao.com\u0026gt;\u0026#34;]\rfrom = \u0026#34;\u0026lt;발신자@naver.com\u0026gt;\u0026#34;\rresp = send(url, rcpt, from, body, opt) In the example above, the most critical part is url = \u0026quot;smtps://smtp.naver.com:465\u0026quot;. This needs to be appropriately changed to whichever server you\u0026rsquo;re using, not just Naver. For the sending time, I fixed it to the moment of sending the mail using the now() from the Dates module, but if this does not match the actual clock, there might be a delay of about 10 minutes before the mail is sent.\nPython Before trying with Julia, I first attempted with Python. Strangely, even using SSL and setting the port to 456 did not work, but disabling SSL and switching to port 587 did the job. The following code, based on a blog2 that explained Google\u0026rsquo;s case, works well for Naver.\nimport smtplib\rfrom email.mime.text import MIMEText\rsendEmail = \u0026#34;발신자@naver.com\u0026#34;\rrecvEmail = \u0026#34;수신자@kakao.com\u0026#34;\rpassword = \u0026#34;비밀번호\u0026#34;\rsmtpName = \u0026#34;smtp.naver.com\u0026#34; #smtp 서버 주소\rsmtpPort = 587 #smtp 포트 번호\rtext = \u0026#34;매일 내용\u0026#34;\rmsg = MIMEText(text) #MIMEText(text , _charset = \u0026#34;utf8\u0026#34;)\rmsg[\u0026#39;Subject\u0026#39;] = \u0026#34;시뮬레이션 종료\u0026#34;\rmsg[\u0026#39;From\u0026#39;] = sendEmail\rmsg[\u0026#39;To\u0026#39;] = recvEmail\rprint(msg.as_string())\rs=smtplib.SMTP( smtpName , smtpPort ) #메일 서버 연결\rs.starttls() #TLS 보안 처리\rs.login( sendEmail , password ) #로그인\rs.sendmail( sendEmail, recvEmail, msg.as_string() ) #메일 전송, 문자열로 변환하여 보냅니다.\rs.close() #smtp 서버 연결을 종료합니다. Environment OS: Windows julia: v1.7.0 https://github.com/aviks/SMTPClient.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gosmcom.tistory.com/72\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2457,"permalink":"https://freshrimpsushi.github.io/en/posts/2457/","tags":null,"title":"How to Send an Email via Naver in Julia"},{"categories":"위상데이터분석","contents":"Overview This section explains and implements the pseudocode of an algorithm introduced in the paper \u0026ldquo;Computing Persistent Homology\u0026rdquo; by Zomorodian and Carlsson1. It takes a filtered complex constructed from an abstract simplicial complex and returns $\\mathcal{P}$-intervals, omitting the construction of computationally challenging persistent modules and calculating persistent homology through matrix reduction. Furthermore, the actual implementation does not even use matrix operations.\nDerivation Derivation of Zomorodian\u0026rsquo;s algorithm: It\u0026rsquo;s certain that without understanding the theoretical aspects of the algorithm, one could not comprehend the algorithm just by looking at the pseudocode. Even if one does not understand it perfectly, one should study enough to grasp why matrices suddenly disappear and why marking is necessary before moving on to implementation. Algorithm Let\u0026rsquo;s assume we have received a filtered complex as data before the algorithm operates.\nLet\u0026rsquo;s create a dictionary or table $T$ like the one above to store data and record information from the algorithm. For example, in a Julia DataFrame, you would copy the numbers in the data to epsilon, the alphabetic part to simplex, and add a boolean column marked to store the marking status, a slot to store the chains of the chain complex, and a column J to store integers that come out during the calculation.\nJulia arrays start at $1$, not $0$, and there are parts where indices are not considered for convenience of implementation, so there are cases where numbers are all wrong by $1~2$ like the screenshot above, but it\u0026rsquo;s not important at all, so don\u0026rsquo;t worry about it. Note that the slot for storing chains can freely switch between representations as a set and as a chain. For example, the calculation is on $\\mathbb{Z}_{2}$ so $$ a + (a - b) = 2 a - b = b \\pmod{2} $$ such calculation takes place, which is also the same as the following set operation $$ \\left\\{ a \\right\\} \\cup \\left\\{ a, -b \\right\\} = \\left\\{ b \\right\\} $$ In algebraic operations, the element $0$ is treated as \u0026rsquo;nonexistent\u0026rsquo; in the set and should be accepted as such. It might be unpleasant for those who like precise and meticulous notation, but it\u0026rsquo;s not entirely unreasonable, so let\u0026rsquo;s just move on. Also, the original paper derives the algorithm over a general field $F$, meaning every $q \\in F$ has an inverse $q^{-1} \\in F$, so calculations like $$ d = d - q^{-1} T[i] $$ are performed, but in this implementation, the binary field $\\mathbb{Z}_{2}$ is sufficient, so $q^{-1}$ is not calculated separately and is replaced for $\\Delta$ defined as $A \\Delta B := \\left( A \\cup B \\right) \\setminus \\left( A \\cap B \\right)$ like the following. $$ d = d \\Delta T[i] $$ The expression $\\deg$ appears too frequently as a (program) function, an index, and the degree of a polynomial function, causing confusion. Therefore, in $T$, it\u0026rsquo;s denoted as epsilon instead of deg. In fact, in simple level topological data analysis, the value of that column, the radius $\\varepsilon \u0026gt; 0$, usually increases to construct a filtered complex. $T$ assumes a perfect alignment according to the dimensions of the simplices, and accordingly, epsilon is expected to have a partial order. Pseudocode $\\left\\{ L_{k} \\right\\}$ = COMPUTEINTERVALS$(K)$\nInput: Receives a filtered complex $K$. The filtered complex must have at least two pieces of information: at what timing $\\deg \\in \\mathbb{Z}$ a certain simplex $\\sigma$ was added. Output: Obtains a set $\\left\\{ L_{k} \\right\\}_{k=0}^{\\dim K}$ of sets $L_{k}$ of $\\mathcal{P}$-intervals for $k = 0, \\cdots , \\dim K$. Side Effect: Modifies marked in table $T$ where data is recorded. $d$ = REMOVEPIVOTROWS$(\\sigma)$\nInput: Receives a $k$-dimensional simplex $\\sigma$. Output: Obtains an element of some $\\mathsf{C}_{k-1}$, which is a $(k-1)$-dimensional chain, meaning it\u0026rsquo;s operated on by $k$-dimensional simplices. $i$ = maxindex$d$\nInput: Receives the chain $d$. Output: Returns the largest index $i$ among all simplex included in chain $d$ in table $T$. For example, for maxindex(abc), it should return the largest $9$ among $5$ of ab, $6$ of bc, and $9$ of ac. $k$ = dim$d$\nInput: Receives a $k$-dimensional chain $d$. Output: Returns the integer $k$. $k$ = dim$\\sigma$\nInput: Receives a $k$-dimensional simplex $\\sigma$. Output: Returns the integer $k$. $k$ = deg$(\\sigma)$\nInput: Receives the simplex $\\sigma$. Output: Returns the integer epsilon corresponding to simplex $\\sigma$ in table $T$. For example, if deg(cd), it should return $2$ since epsilon of cd is $2$. Keywords\nMark is used in the form Mark $\\sigma$ to change the marked of the corresponding simplex $\\sigma$ to true. Store is used in the form Store $j$ and $d$ in $T[i]$ to store the integer $j$ in J of $T[i]$ and the chain $d$ in slot. Remove is used in the form Remove $x$ in $d$ to remove the term $x$ in the chain $d$. $\\sigma^{i}$ is the simplex at the $i$th position in table $T$, and $m$ is the length of $T$.\nfunction COMPUTEINTERVALS$(K)$\n# Initialization\nfor $k \\in 0:\\dim K$\n$L_{k} := \\emptyset$\nend for\nfor $j \\in 0:(m-1)$\n$d$ = REMOVEPIVOTROWS$\\left( \\sigma^{j} \\right)$\nif $d = \\emptyset$\n# $d$ being empty means it\u0026rsquo;s a candidate for a (non-pivot) zero column\nmark $\\sigma^{j}$\nelse\n# Must calculate the degree of $d$, so it must be the max of all terms\n# $d$ is a chain one dimension lower than $\\sigma^{j}$ and must be $i \u0026lt; j$\n$i$ = maxindex$d$\n$k$ = dim$d$\n$L_{k}$ = $L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\deg \\sigma^{j} \\right) \\right\\}$\nend if\nend for\nfor $j \\in 0:(m-1)$\n# If it\u0026rsquo;s still not marked, it\u0026rsquo;s definitely a zero column\nif $\\sigma^{j}$ ismarked and $T[j]$ isempty\n$k$ = dim$d$\n# Corresponds to $H_{k-1}$ to $\\sum^{\\hat{e}_{i}} F[t]$, handling infinity\n$L_{k}$ = $L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\infty \\right) \\right\\}$\nend if\nend for\nreturn $\\left\\{ L_{k} \\right\\}$\nend function\nfunction REMOVEPIVOTROWS$(\\sigma)$\n$k$ = $\\dim \\sigma$\n# Assuming $\\partial abc = ab - bc + ca$, ∂(\u0026quot;abc\u0026quot;) = [\u0026quot;ab\u0026quot;, \u0026quot;bc\u0026quot;, \u0026quot;ca\u0026quot;]\n$d$ = $\\partial_{k} \\sigma$\nRemove not marked $(k-1)$-dimensional simplex in $d$\nwhile $d \\ne \\emptyset$\n$i$ = maxindex$d$\nif $T[i]$ isempty\nbreak\nend if\n# Since it\u0026rsquo;s $\\mathbb{Z}_{2}$, replace with symdiff (symmetric difference)\n$d$ = $d \\Delta T[i]$\nend while\nreturn $d$\nend function\nImplementation The result of the algorithm for the given example should be as follows: $$ \\begin{align*} L_{0} =\u0026amp; \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} \\\\ L_{1} =\u0026amp; \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} \\end{align*} $$\nThe result of the implementation in Julia is as follows. Except for the parts where the indices are precisely incorrect, it can be seen that it has been implemented correctly.\nFull Code As you can see, the code is written almost exactly following the notation of the original paper. For instance, the Julia-like code for $$ L_{k} = L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\deg \\sigma^{j} \\right) \\right\\} $$ is push!(L_[k], (deg(σⁱ), deg(σʲ))), but to make it look almost identical to the paper, it\u0026rsquo;s implemented as L_[k] = L_[k] ∪ [(deg(σⁱ), deg(σʲ))].\nusing DataFrames\rdata = DataFrame([\r0 \u0026#34;a\u0026#34;\r0 \u0026#34;b\u0026#34;\r1 \u0026#34;c\u0026#34;\r1 \u0026#34;d\u0026#34;\r1 \u0026#34;ab\u0026#34;\r1 \u0026#34;bc\u0026#34;\r2 \u0026#34;cd\u0026#34;\r2 \u0026#34;ad\u0026#34;\r3 \u0026#34;ac\u0026#34;\r4 \u0026#34;abc\u0026#34;\r5 \u0026#34;acd\u0026#34;\r], [\u0026#34;epsilon\u0026#34;, \u0026#34;simplex\u0026#34;])\rT = copy(data)\rT[!, :\u0026#34;marked\u0026#34;] .= false\rT[!, :\u0026#34;slot\u0026#34;] .= [[]]\rT[!, :\u0026#34;J\u0026#34;] .= 0\rdimK = 2\rm = nrow(T)\rtest_σ = \u0026#34;abc\u0026#34;\rdim(σ) = length(σ)\rfunction deg(σ)\rreturn T.epsilon[findfirst(T.simplex .== σ)]\rend\rdeg(test_σ)\rfunction ∂(σ)\rk = dim(σ)\rreturn [σ[(1:k)[Not(t)]] for t = 1:k]\rend\r∂(test_σ)\rfunction maxindex(chain)\rreturn (T.simplex .∈ Ref(chain)) |\u0026gt; findall |\u0026gt; maximum\rend\rmaxindex(∂(test_σ))\rfunction REMOVEPIVOTROWS(σ)\rk = dim(σ); d = ∂(σ)\rd = d[d .∈ Ref(T[T.marked,:simplex])] # Remove unmarked terms in ▷eq029◁\rwhile !(d |\u0026gt; isempty)\ri = maxindex(d)\rif T[i,:slot] |\u0026gt; isempty break end\rd = symdiff(d, T[i,:slot])\r# print(\u0026#34;d in empty\u0026#34;)\rend\rreturn d\rend\rREMOVEPIVOTROWS(test_σ)\rL_ = [[] for k = 0:dimK]\rfor j0 = 0:(m-1)\rj = j0+1\rσʲ = T[j,:simplex]\rd = REMOVEPIVOTROWS(σʲ)\rif d |\u0026gt; isempty\rT[j,:marked] = true\relse\ri = maxindex(d); k = dim(σʲ)\rσⁱ = T[i,:simplex]\rT[i,[:J,:slot]] = j0,d\rL_[k] = L_[k] ∪ [(deg(σⁱ), deg(σʲ))]\rend\rend\rfor j0 = 0:(m-1)\rj = j0+1\rσʲ = T[j,:simplex]\rif (T[j,:marked]) \u0026amp;\u0026amp; (T[j,:slot] |\u0026gt; isempty) \u0026amp;\u0026amp; (T[j,:J] |\u0026gt; iszero)\rk = dim(σʲ); L_[k] = L_[k] ∪ [(deg(σʲ), Inf)]\rprint(\u0026#34;j: $j\u0026#34;)\rend\rend Zomorodian. (2005). Computing Persistent Homology: ch4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2449,"permalink":"https://freshrimpsushi.github.io/en/posts/2449/","tags":null,"title":"Implementation of Zomorodian's Algorithm"},{"categories":"위상데이터분석","contents":"Overview The paper \u0026ldquo;Computing Persistent Homology\u0026rdquo; by Zomorodian and Carlsson introduces an algorithm for deriving $\\mathcal{P}$-intervals from a Filtered Complex created by an Abstract Simplicial Complex, bypassing the construction of Persistent Modules that are challenging to handle computationally, and computing persistent homology through matrix reduction1.\nDerivation Part 0. Preliminary Investigation\nThe derivation of the algorithm starts by examining the form of a Persistence Complex depicted as above, crucial for understanding the subsequent mathematical expressions.\nThe numbers at the bottom are denoted as $\\deg$, forming a Filtered Complex as they increase from $0$ to $5$. $$ \\left\\{ a,b \\right\\} = K_{0} \\subset K_{1} \\subset K_{2} \\subset K_{3} \\subset K_{4} \\subset \\left( K_{4} \\cup \\left\\{ acd \\right\\} \\right) = K_{5} $$ Regardless of $\\deg$, $K$, as a $2$-Simplex, forms a Chain Complex in the context of Homology. $$ \\mathsf{C}_{2} \\overset{\\partial_{2}}{\\longrightarrow} \\mathsf{C}_{1} \\overset{\\partial_{1}}{\\longrightarrow} \\mathsf{C}_{0} $$ The algorithm aims to compute how algebraic topological information provided by data, such as Betti Numbers $\\beta_{k}$, emerges and disappears over $\\deg$. $$ \\begin{align*} L_{0} =\u0026amp; \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} \\\\ L_{1} =\u0026amp; \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} \\end{align*} $$\n$L_{0}$ consists of $\\mathcal{P}$-intervals showing when components appear and vanish, while $L_{1}$ comprises $\\mathcal{P}$-intervals indicating the emergence and disappearance of \u0026lsquo;holes\u0026rsquo; in space.\nPart 1. $\\partial_{1}$\nThe paper suggests that the computation is possible over any Field, focusing on calculations within a $\\mathbb{Z}_{2} [t]$-module, an Graded Module. The homogeneous basis for $\\mathsf{C}_{k}$ is denoted as $\\left\\{ e_{j} \\right\\}$ and for $\\mathsf{C}_{k-1}$ as $\\left\\{ \\hat{e}_{i} \\right\\}$, where \u0026lsquo;homogeneous\u0026rsquo; implies single-term expressions, not polynomials like $t^{2} + t$ but monomials like $t^{4}$.\n$$ \\deg M_{k} (i,j) = \\deg e_{j} - \\deg \\hat{e}_{i} $$ Familiarity with Homological Algebra implies constructing the Boundary Matrix $M_{k}$ corresponding to $\\partial_{k}$ and finding its Smith Normal Form $\\tilde{M}_{1}$. For $k=1$, the unique $M_{1}$ can be obtained, considering the matrix bases are homogeneous.\nConstructing a matrix using these bases can be seen as the inverse operation of applying $t^{n}$, a Group Action raising the degree in the graded module. Let\u0026rsquo;s perform a few direct calculations to grasp the concept. $$ \\begin{align*} \\deg M_{1} (2,5) =\u0026amp; \\deg ac - \\deg c = 3 - 1 = 2 = \\deg t^{2} \\\\ \\deg M_{1} (4,5) =\u0026amp; \\deg ac - \\deg a = 3 - 0 = 3 = \\deg t^{3} \\\\ \\deg M_{1} (2,2) =\u0026amp; \\deg bc - \\deg c = 1 - 1 = 0 = \\deg t^{0} = \\deg 1 \\end{align*} $$\nAs mentioned before, once we form the Echelon Form, specifically the Column-Echelon Form, it appears as shown below:\nRecalling linear algebra from undergraduate studies, the elements at the top of each column that are non-zero, marked with rectangles in the image, are known as pivots. Here, we introduce two auxiliary lemmas:\n(1): The diagonal elements of the Column-Echelon Form are the same as those in the Smith Normal Form. (2): If the pivot of the $i$th row in $\\tilde{M}_{k}$ is $\\tilde{M}_{k} (i,j) = t^{n}$, it corresponds to $\\sum^{\\deg \\hat{e}_{i}} F[t] / t^{n}$ in the Homology Group $H_{k-1}$, otherwise it corresponds to $\\sum^{\\deg \\hat{e}_{i}} F[t]$ in $H_{k-1}$. This equates to $L_{k-1}$ being composed of intervals $( \\deg \\hat{e}_{i} , \\deg \\hat{e}_{i} + n )$ and $( \\deg \\hat{e}_{i} , \\infty )$. In other words,\nBy auxiliary lemma (1), only column operations are needed to compute persistent homology, eliminating the need for row operations. According to auxiliary lemma (2), $L_{k-1}$ consists of intervals $( \\deg \\hat{e}_{i} , \\deg \\hat{e}_{i} + n )$ and $( \\deg \\hat{e}_{i} , \\infty )$. Since the pivot of the first row is $t^{1}$ and $\\deg d = 1$, we obtain $(1,1+1)$. Since the pivot of the second row is $t^{0}$ and $\\deg c = 1$, we obtain $(1,1+0)$. Since the pivot of the third row is $t^{1}$ and $\\deg b = 0$, we obtain $(0,0+1)$. Since there is no pivot for the fourth row and $\\deg a = 0$, we obtain $(0,\\infty)$. This matches precisely with the previously mentioned $L_{0}$, expressed as: $$ L_{0} = \\left\\{ (0, \\infty) , (0,1) , (1,1) , (1,2) \\right\\} $$\nPart 2. $\\partial_{2}$\nTo find $L_{1}$, the matrix form $M_{2}$ of $\\partial_{2}$ is as shown above. However, with the following auxiliary lemma, we can simplify and ease the calculation process:\n(3): To represent the standard basis of $\\mathsf{C}_{k+1}$ and $\\mathsf{Z}_{k}$ for $\\partial_{k+1}$, rows corresponding to $\\tilde{M}_{k}$ can simply be removed from $M_{k+1}$. In our specific context, this means that from the $1$-simplices $ab, bc, cd, ad, ac$ in $\\tilde{M}_{1}$, only the pivots $cd, bc, ab$ remain, so we can directly delete these from $M_{2}$. Intuitively, this implies that since these elements have already been used in dimension $k$, they are not needed for dimension $k+1$. By omitting the direct construction of column-echelon form $\\tilde{M}_{2}$ and removing those three rows, we obtain the truncated form $\\check{M}_{2}$ as follows:\n$$ \\begin{align*} z_{2} =\u0026amp; ac - bc - ab \\ z_{1} =\u0026amp; ad - bc - cd - ab \\end{align*} $$\nFollowing auxiliary lemma (2) again for the computation:\nThe pivot of the first row is $t^{1}$, and since $$ \\deg z_{2} = \\deg ( ac - bc - ab ) = \\max \\deg { ac , bc , ab } = 3, $$ we obtain the interval $(3,3+1)$. The pivot of the second row is $t^{3}$, and since $$ \\deg z_{1} = \\deg ( ad - bc - cd - ab ) = \\max \\deg { ad , bc , cd , ab } = 2, $$ we obtain the interval $(2,2+3)$. This exactly matches the previously mentioned $L_{1}$, expressed as: $$ L_{1} = { (2,5), (3,4) } $$\nRepeating this process for each dimension $\\dim K$ of the complex $K$ yields the desired algorithm. The dimensions of the matrix follow $\\partial_{k}$, and its elements are filled according to $\\deg$, which should help clarify the procedure.\n■\nLemma (1) indicates that only column operations are necessary, suggesting no strict need for matrix representation. Moreover, lemma (3) allows for efficient procedure by boldly removing rows already computed, requiring capabilities like \u0026lsquo;marking\u0026rsquo; non-pivot columns. Consequently, the actual algorithm\u0026rsquo;s pseudocode is likely described using higher-level data structures, such as dictionaries or dataframes, rather than matrices directly, which can be quite challenging and confusing in practice.\nImplementation Zomorodian\u0026rsquo;s Algorithm Implementation: Introduces an almost literary translation of the paper\u0026rsquo;s pseudocode into the Julia language, accessible to anyone in the scientific community. Zomorodian. (2005). Computing Persistent Homology: ch4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2447,"permalink":"https://freshrimpsushi.github.io/en/posts/2447/","tags":null,"title":"Derivation of Zomorodian's Algorithm"},{"categories":"확률론","contents":"Overview This is a summary of definitions and concepts for those who have already studied measure theory and probability. It is intended to be viewed when definitions are confusing or unrecognizable, and when a general review is needed.\nMeasure Theory Algebras An algebra of sets on nonempty set $X$ is a nonempty collection $\\mathcal{A}$ of subsets of $X$ is colsed under finite unions ans complements.\n$\\sigma$-algebra is an algebra that is closed under countable unions.\nNote:\n$\\mathcal{A}$ is also closed under intersections, because $E_{1} \\cap E_{2} = \\left( E_{1} \\cup E_{2} \\right)^{c} \\in \\mathcal{A}$ for $E_{1}, E_{2} \\in \\mathcal{A}$. $\\varnothing$, $X$ $\\in \\mathcal{A}$, since if $E \\in \\mathcal{A}$ we have $\\varnothing = E \\cap E^{c} \\in \\mathcal{A}$ and $X = E \\cup E^{c} \\in \\mathcal{A}$. If $X$ any topological space, the $\\sigma$-algebra generated by the family of open sets in $X$ is called the Borel $\\sigma$-algebra on $X$ and is denoted by $\\mathcal{B}_{X}$.\nBorel $\\sigma$-algebra is unique smallest $\\sigma$-algebra containing all open sets. Let $\\mathcal{E}$ be a $\\sigma$-algebra on $X$, then $(X, \\mathcal{E})$ is called a measurable space and $E \\in \\mathcal{E}$ is called ($\\mathcal{E}$-)measurable set.\nIn the following, we shall consider a fixed measurable space $(X, \\mathcal{E})$.\nMeasurable Functions A function $f : X \\to \\mathbb{R}$ is said to be ($\\mathcal{E}$-)measurable, if for every real number $\\alpha \\in \\mathbb{R}$ the set $\\left\\{ x \\in X : f(x) \\gt \\alpha \\right\\}$ belongs to $\\mathcal{E}$.\nGeneralization Let $(X, \\mathcal{E})$ and $(Y, \\mathcal{F})$ be a measurable spaces. A function $f : X \\to Y$ is called $(\\mathcal{E}, \\mathcal{F})$-measurable, if $f^{-1}(F) = \\left\\{ x \\in X : f(x) \\in F \\right\\}$ belongs to $\\mathcal{E}$ for all $F \\in \\mathcal{F}$.\nNote: A $\\mathcal{E}$-measurable function is equivalent to this definition in the case $(Y, \\mathcal{F}) = (\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$.\nMeasures A measure on $\\mathcal{E}$ (or on $(X, \\mathcal{E})$, or simply on $X$ if $\\mathcal{E}$ is understood) is a function $\\mu : \\mathcal{E} \\to [0, \\infty]$ such that\nNull empty set: $\\mu (\\varnothing) = 0$. Countable additivity: If $\\left\\{ E_{j} \\right\\}$ is a sequence of disjoint sets in $\\mathcal{E}$, then $\\displaystyle \\mu \\left( \\bigcup\\limits_{j} E_{j} \\right) = \\sum\\limits_{j} \\mu (E_{j})$. A triple $(X, \\mathcal{E}, \\mu)$ is called a measure space and we shall be working on a fixed measure space $(X, \\mathcal{E}, \\mu)$.\nA Borel measure on $\\mathbb{R}$ is a measure whose domain is the Borel $\\sigma$-algebra $\\mathcal{B}_{\\mathbb{R}}$: $$ \\mu : \\mathcal{B}_{\\mathbb{R}} \\to [0, \\infty] $$\nFor two measures $\\mu$ and $\\nu$ on each $(X, \\mathcal{E})$ and $(Y, \\mathcal{F})$, measure $\\mu \\times \\nu$ is the product of $\\mu$ and $\\nu$ which is the unique measure on $\\mathcal{E} \\times \\mathcal{F}$ such that $\\mu \\times \\nu (E \\times F) = \\mu (E) \\nu (F)$ for all rectangles $E \\times F$.\nThe Integral A real-valued function $f$ is simple if it has only a finite number of values.\nA simple measurable function $\\varphi$ can be represented in the form $$ \\begin{equation} \\varphi = \\sum\\limits_{j=1}^{n} a_{j}\\chi_{E_{j}}, \\text{ where } E_{j} = \\varphi^{-1}(\\left\\{ a_{j} \\right\\}) \\text{ and } \\operatorname{range} (\\varphi) = \\left\\{ a_{1}, \\dots, a_{n} \\right\\}. \\end{equation} $$ where $\\chi_{E_{j}}$ is the characteristic function of $E_{j}$. We call this standard representation of $\\varphi$.\nIf $\\varphi$ simple measurable function with standard representation $(1)$, we define the integral of $\\varphi$ with respect to measure $\\mu$ by $$ \\int \\varphi d\\mu := \\sum\\limits_{j=1}^{n} a_{j}\\mu (E_{j}). $$ Notation: $$ \\int \\varphi d\\mu = \\int \\varphi = \\int \\varphi(x) d\\mu (x), \\qquad \\int = \\int_{X}. $$\nIf $f$ is measurable function on $(X, \\mathcal{E})$, we define the integral of $f$ with respect to $\\mu$ by $$ \\int f d\\mu := \\sup \\left\\{ \\int \\varphi d\\mu : 0 \\le \\varphi \\le f, \\varphi \\text{ is simple and measurable} \\right\\}. $$\nThe positive and negative parts of $f : X \\to \\mathbb{R}$ are defined repectively as $$ f^{+}(x) := \\max \\left( f(x), 0 \\right)),\\qquad f^{-1}(x) := \\min \\left(-f(x), 0 \\right)). $$ If $\\displaystyle \\int f^{+}$ and $\\displaystyle \\int f^{-}$ are both finite, then we say that $f$ is integrable. Also $\\left| f \\right| = f^{+} - f^{-}$.\nThe set of real-valued integrable functions is a vector space and the integral is a linear functional on it. This vector space is denoted as: $$ L = L(X, \\mathcal{E}, \\mu) = L(X, \\mu) = L(X) = L(\\mu), \\qquad L = L^{1} $$\n$L^{p}$ space For measure space $(X, \\mathcal{E}, \\mu)$ and $0 \\lt p \\lt \\infty$, we define $$ L^{p}(X, \\mathcal{E}, \\mu) := \\left\\{ f : X \\to \\mathbb{R} \\left| f \\text{ is measurable and } \\left( \\int \\left| f \\right|^{p} d\\mu \\right)^{1/p} \\lt \\infty \\right. \\right\\}. $$ $$ {} \\\\ {} \\\\ {} \\\\ $$\nProbability Theory Notation and Terminology $$ \\begin{array}{lll} \\text{Analysts\u0026rsquo; Term} \u0026amp;\u0026amp; \\text{Probabilists\u0026rsquo; Term} \\\\ \\hline \\text{Measure space } (X, \\mathcal{E}, \\mu) \\text{ such that } \\mu (X) = 1 \u0026amp;\u0026amp; \\text{Probability space } (\\Omega, \\mathcal{F}, P) \\\\ \\text{Measure } \\mu : \\mathcal{E} \\to \\mathbb{R} \\text{ such that } \\mu (X) = 1 \u0026amp;\u0026amp; \\text{Probability } P : \\mathcal{F} \\to \\mathbb{R} \\\\ (\\sigma\\text{-)algebra $\\mathcal{E}$ on $X$} \u0026amp;\u0026amp; (\\sigma\\text{-)field $\\mathcal{F}$ on $\\Omega$} \\\\ \\text{Mesurable set } E \\in \\mathcal{E} \u0026amp;\u0026amp; \\text{Event } E \\in \\mathcal{F} \\\\ \\text{Measurable real-valued function } f : X \\to \\mathbb{R} \u0026amp;\u0026amp; \\text{Random variable } X : \\Omega \\to \\mathbb{R} \\\\ \\text{Integral of } f, {\\displaystyle \\int f d\\mu} \u0026amp;\u0026amp; \\text{Expextation of } f, E(X) \\\\ f \\text{ is } L^{p} \u0026amp;\u0026amp; X \\text{ has finite $p$th moment} \\\\ \\text{Almost everywhere, a.e.} \u0026amp;\u0026amp; \\text{Almost surely, a.s.} \\end{array} $$\n$$ \\begin{align*} \\left\\{ X \\gt a \\right\\} \u0026amp;:= \\left\\{ w : X(w) \\gt a \\right\\} \\\\ P\\left( X \\gt a \\right) \u0026amp;:= P\\left( \\left\\{ w : X(w) \\gt a \\right\\} \\right) \\end{align*} $$\nBasic Definitions For measurable spacse $(\\Omega, \\mathcal{F})$ and $(\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$, $(\\mathcal{F}, \\mathcal{B}_{\\mathbb{R}})$-mearsuable function $X : \\Omega \\to \\mathbb{R}$ is called random variable. Namely, $$ X^{-1}(B) \\in \\mathcal{F}\\qquad \\forall B \\in \\mathcal{B}_{\\mathbb{R}}. $$\nA probability (or probability measure) on $(\\Omega, \\mathcal{F})$ is measure $P : \\mathcal{F} \\to \\mathbb{R}$ such that $P(\\Omega) = 1$.\nIf $X$ is a random variable,\nexpectation: $\\displaystyle E(X) := \\int X dP$ variance: $\\sigma^{2}(X) := E\\left[ (X - E(X))^{2} \\right] = E(X^{2}) - E(X)^{2}$ The (probability) distribution of $X$ is a probability on $\\mathbb{R}$, $P_{X} : \\mathcal{B}_{\\mathbb{R}} \\to \\mathbb{R}$ such that $$ P_{X}(B) := P(X^{-1}(B)). $$\nThe distribution fuction of $X$ is defined as $$ F_{X}(a) := P_{X}\\left( (-\\infty, a] \\right) = P(X \\le a). $$\nFor any finite sequence of random variables $\\left\\{ X_{i} \\right\\}_{i=1}^{n}$, random vector $(X_{1}, \\dots, X_{n})$ is defined as a map from $\\Omega \\to \\mathbb{R}^{n}$: $$ (X_{1}, \\dots, X_{n})(x) := (X_{1}(x), \\dots, X_{n}(x)). $$ Note: $(X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n})= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})$.\nFor $(X, Y) : \\Omega \\to \\mathbb{R}^{2}$, $$ (X, Y)^{-1} (a, b) = \\left\\{ x \\in \\Omega : X(x) = a \\right\\} \\cap \\left\\{ x \\in \\Omega : Y(x) = b \\right\\}. $$ Thus, for all Borel sets $B_{1}$ and $B_{2} \\in \\mathcal{B}_{\\mathbb{R}}$ we have $$ (X, Y)^{-1}(B_{1} \\times B_{2}) = (X, Y)^{-1}(B_{1}, B_{2}) = X^{-1}(B_{1}) \\cap Y^{-1}(B_{2}) $$ and extending to $\\mathbb{R}^{n}$ we obtain $$ \\begin{equation} \\begin{aligned} (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \u0026amp;= (X_{1}, \\dots, X_{n})^{-1}(B_{1}, \\dots, B_{n}) \\\\ \u0026amp;= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n}). \\end{aligned} \\end{equation} $$\nThe joint distribution of $X_{1}, \\dots, X_{n}$ is a probability distribution of $(X_{1}, \\dots, X_{n})$: $$ P_{(X_{1}, \\dots, X_{n})} : \\mathcal{B}_{\\mathbb{R}^{n}} \\to \\mathbb{R}, $$ $$ P_{(X_{1}, \\dots, X_{n})}(B_{1} \\times \\cdots \\times B_{n}) := P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right). $$\nIndependency For an event $E$ such that $P(E) \\gt 0$, a probability on $\\Omega$ $$ P_{E}(F) = P(E|F) := P(E \\cap F)/P(E) $$ is called conditional probability on $E$.\nIf $P_{E}(F) = P(F)$, then $F$ is said to be independent of $E$: $$ \\text{$F$ is independent of $E$} \\iff P(E \\cap F) = P(E)P(F). $$ A collection $\\left\\{ E_{j} \\right\\}$ of events in $\\Omega$ is indepencent if $$ P(E_{1} \\cap \\cdots \\cap E_{n}) = P(E_{1}) P(E_{2}) \\cdots P(E_{n}) = \\prod \\limits_{i=1}^{n} P(E_{j}) $$\nA collection $\\left\\{ X_{j} \\right\\}$ of random variables on $\\Omega$ is independent if the events $\\left\\{ X_{j}^{-1}(B_{j}) \\right\\}$ are independent for all Borel sets $B_{j} \\in \\mathcal{B}_{\\mathbb{R}}$, namely $$ P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) = \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})). $$\nWe have from LHS by definition of distribution and $(2)$ $$ \\begin{align*} P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) \u0026amp;= P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right) \\\\ \u0026amp;= P_{(X_{1}, \\dots, X_{n})} \\left( B_{1} \\times \\cdots \\times B_{n} \\right). \\end{align*} $$ By the way, we have from RHS by definition of product measure and distribution $$ \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})) = \\prod \\limits_{j=1}^{n} P_{X_{j}}(B_{j}) = \\left( \\prod \\limits_{j=1}^{n} P_{X_{j}} \\right) \\left( B_{1} \\times \\cdots \\times B_{n} \\right). $$ Therefore, if $\\left\\{ X_{j} \\right\\}$ are independent, then $$ P_{(X_{1}, \\dots, X_{n})} = \\prod\\limits_{j=1}^{n}P_{X_{j}}. $$\n$\\left\\{ X_{j} \\right\\}$ is an independent set of random variables if and only if the joint distribution of $\\left\\{ X_{j} \\right\\}$ is the product of their individual distributions.\nReferences Robert G. Bartle, The Elements of Integration and Lebesgue Measure (1995) Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (1999) ","id":3473,"permalink":"https://freshrimpsushi.github.io/en/posts/3473/","tags":null,"title":"Summary of Measure Theory and Probability Theory"},{"categories":"줄리아","contents":"설명 Using the Distributions.jl package, you can randomly sample from a given distribution.\n","id":3463,"permalink":"https://freshrimpsushi.github.io/en/posts/3463/","tags":null,"title":"Sampling Randomly from a Given Distribution in Julia"},{"categories":"줄리아","contents":"Description1 In Julia, the function for random sampling is as follows:\nrand([rng=default_rng()], [S], [dims...]) rng stands for Random Number Generator, which specifies the random number generation algorithm. If you don\u0026rsquo;t understand what this means, it\u0026rsquo;s okay to leave it untouched.\nS likely stands for Set, and it is a variable that specifies the set from which the random sampling will occur. The variables that can be input for S include the following:\nObjects with indices AbstractDict or AbstractSet Strings Types (only integers and floating points are possible. Rational and irrational numbers are not allowed.) When the extraction set is specified as a type, if it\u0026rsquo;s an integer type, random numbers are drawn from the range of typemin(S):type(S) (does not support BigInt).\njulia\u0026gt; typemin(Int16), typemax(Int16)\r(-32768, 32767)\rjulia\u0026gt; typemin(Int32), typemax(Int32)\r(-2147483648, 2147483647)\rjulia\u0026gt; typemin(Int64), typemax(Int64)\r(-9223372036854775808, 9223372036854775807) If it\u0026rsquo;s a floating-point, random numbers are drawn from the range of $[0, 1)$.\njulia\u0026gt; rand(Float64)\r0.4949745522302659\rjulia\u0026gt; rand(ComplexF64)\r0.8560168003603014 + 0.16478582700545064im [dims...] denotes the dimensions of the array to be drawn. If it\u0026rsquo;s rand(S, m, n), it draws $m \\times n$ elements (including duplicates) from the set S and returns an array of shape $m \\times n$. If dimensions are not specified, a real number is returned. Be careful because real numbers and 1-dimensional vectors are distinctly differentiated. Also, be careful when you wish to obtain an array of shape $2\\times 3$ and input the dimensions as a tuple like (2,3), as it gets treated as a variable for S, yielding a completely different result.\njulia\u0026gt; rand(Float64) # 실수 추출\r0.42226201756172266\rjulia\u0026gt; rand(Float64, 1) # 성분이 실수인 1x1 배열로 추출\r1-element Vector{Float64}:\r0.7361136057571305\rjulia\u0026gt; rand(2,3) # 성분이 실수인 2x3 배열로 추출 2×3 Matrix{Float64}:\r0.648742 0.364548 0.0550352\r0.0350098 0.56055 0.83297\rjulia\u0026gt; rand((2,3)) # 2와 3중에서 추출\r3 For more advanced content, refer to the following:\nHow to fix random seed How to randomly sample with weights How to randomly sample with a distribution Code Objects with Indices julia\u0026gt; rand((2,5))\r5\rjulia\u0026gt; rand(2:5)\r3\rjulia\u0026gt; rand([2,3,4,5])\r4\rjulia\u0026gt; rand([\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, 4])\r\u0026#34;x\u0026#34; Dictionary If the sampling set is a dictionary, the key-value pair itself is extracted.\njulia\u0026gt; d = Dict(2=\u0026gt;4, 3=\u0026gt;5, 4=\u0026gt;\u0026#34;6\u0026#34;)\rDict{Int64, Any} with 3 entries:\r4 =\u0026gt; \u0026#34;6\u0026#34;\r2 =\u0026gt; 4\r3 =\u0026gt; 5\rjulia\u0026gt; rand(d)\r4 =\u0026gt; \u0026#34;6\u0026#34;\rjulia\u0026gt; rand(d)\r2 =\u0026gt; 4 String If the sampling set is a string, a random character from the string is extracted.\njulia\u0026gt; str = \u0026#34;freshrimpsushi\u0026#34;\r\u0026#34;freshrimpsushi\u0026#34;\rjulia\u0026gt; rand(str)\r\u0026#39;e\u0026#39;: ASCII/Unicode U+0065 (category Ll: Letter, lowercase)\rjulia\u0026gt; rand(str)\r\u0026#39;h\u0026#39;: ASCII/Unicode U+0068 (category Ll: Letter, lowercase) Type julia\u0026gt; rand(Int32, 3)\r3-element Vector{Int32}:\r1552806175\r-384901411\r-1580189675\rjulia\u0026gt; rand(UInt32, 3)\r3-element Vector{UInt32}:\r0xd2f44f99\r0x166a8b9e\r0x92fe22dc\rjulia\u0026gt; rand(Float32, 3)\r3-element Vector{Float32}:\r0.59852564\r0.6247238\r0.23303497\rjulia\u0026gt; rand(ComplexF32, 3)\r3-element Vector{ComplexF32}:\r0.10872495f0 + 0.6622572f0im\r0.6408408f0 + 0.46815878f0im\r0.7766515f0 + 0.73314756f0im Environment OS: Windows11 Version: Julia 1.9.0 https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3462,"permalink":"https://freshrimpsushi.github.io/en/posts/3462/","tags":null,"title":"Sampling Randomly in Julia"},{"categories":"프로그래밍","contents":"Overview1 140+ CSS color palettes with names.\nCode ","id":3459,"permalink":"https://freshrimpsushi.github.io/en/posts/3459/","tags":null,"title":"CSS color name tags"},{"categories":"데이터과학","contents":"Definition 1 Qualitative Variables Variables that measure qualitative characteristics are called qualitative variables.\nThe food is\u0026hellip; delicious / so-so / tasteless The color is\u0026hellip; red / blue / yellow The major is\u0026hellip; mathematics / statistics / physics Such qualitative variables are often referred to as categorical data.\nQuantitative Variables Variables that measure quantitative characteristics are called quantitative variables.\nAge is\u0026hellip; 20 years old / 31 years old / 11 years old Height is\u0026hellip; 170.0 cm / 170.5 cm / 162.1 cm Quantitative variables that take specific values, like age or vision, are called discrete variables, while those that take continuous values, like height or weight, are called continuous variables.\nExplanation The definitions may seem odd, but \u0026lsquo;qualitative\u0026rsquo; and \u0026lsquo;quantitative\u0026rsquo; are not terms we\u0026rsquo;re born knowing; we learn their academic meanings and apply them to everyday language. For instance, when assessing the quality of an item, we might say it has \u0026lsquo;high quality\u0026rsquo; without quantifying it with a specific number like \u0026lsquo;1432 units of good\u0026rsquo; or \u0026lsquo;17% better\u0026rsquo;.\nQualitative refers to characteristics that might have an order (good-bad-terrible) but are usually not numerically expressed. It\u0026rsquo;s fine if they\u0026rsquo;re categorized without order (German-French-Japanese). Quantitative refers to measurable amounts. The distinction between discrete and continuous variables here can be a bit tricky. Discrete Values? The term discrete values describes values that are distinct and separated, like natural numbers or marked scales. It\u0026rsquo;s an expression not commonly found in textbooks, and even I acknowledge it\u0026rsquo;s not ideal. A more fitting description might be:\nVariables that take countable values are called discrete variables. They are assumed to take only finite or countable values.\nHowever, such a mathematically precise description might not be immediately helpful in understanding what discrete variables are.\nCountable in this context means something can be counted in a way familiar to Indo-European languages, like English, French, or Spanish, where we can say \u0026lsquo;one, two, \u0026hellip;\u0026rsquo; and count \u0026lsquo;how many\u0026rsquo;. In English, nouns that can be counted this way are called countable nouns. Mathematically, it means there\u0026rsquo;s a one-to-one correspondence with the set of natural numbers.\nExamples might help clarify. These are typically discrete variables:\nThe number of pigs on a farm The number of traffic accident fatalities per year The number of pages in a textbook The age of infants\u0026hellip; \u0026lsquo;24-month-old boy\u0026rsquo;, \u0026lsquo;1 year 2 months girl\u0026rsquo;, etc. The number of 1L water bottles Some examples might be ambiguous:\nThe amount of water in 3 1L bottles\u0026hellip; If we\u0026rsquo;re talking about the amount of water, it\u0026rsquo;s continuous. Vision\u0026hellip; Commonly measured in increments of 0.1, but if only divided into groups like 0.5, 1.0, 1.5, it could be considered discrete, and depending on the data, even qualitative. Classification and Regression Problems In data science, problems are often classified as classification or regression problems based on whether the dependent variable is qualitative or quantitative.\nPrecautions Beginners working with data can make mistakes not because they don\u0026rsquo;t understand qualitative and quantitative variables, but because they\u0026rsquo;re not yet familiar with them. These are common mistakes, often encountered when studying complex topics like regression analysis, and there\u0026rsquo;s almost no opportunity to artificially develop intuition for these pitfalls. The following post might not explain everything but can give an idea of what to watch out for:\nRegression Analysis Involving Qualitative Variables Encoding It\u0026rsquo;s common to see encoding like men as $0$ and women as $1$, but just because there are numbers, it doesn\u0026rsquo;t make it a quantitative (discrete) variable.\nSuch encoding can also be used for privacy. Imagine medical data, which can be highly personal and specific enough to identify individuals just by the data. In such cases, data might be published with sensitive information simply represented by numbers, like psychiatric history or abortion status for women.\nRatings Similarly to encoding, ratings might seem quantitative but are still qualitative. For example, if high school graduates are rated as $0$, bachelor\u0026rsquo;s degree holders as $1$, and PhDs as $2$, it might seem quantitative but is qualitative. Terms like \u0026rsquo;low-educated\u0026rsquo; or \u0026lsquo;high-educated\u0026rsquo; are societal constructs and don\u0026rsquo;t necessarily imply a numerical order in the data.\nHex Codes Distinguishing between red and blue is qualitative, but what about different shades like pink, fuchsia, or crimson? If it\u0026rsquo;s about lipstick, it might still be qualitative, but for fabric colors with thousands of shades, they might be represented by RGB hex codes. While it\u0026rsquo;s rare to encounter such data, it\u0026rsquo;s important to remember that what intuitively seems qualitative can be expressed quantitatively.\nGender Data might categorize gender, and whether or not one agrees with the political correctness behind it, if the data presents it that way, it must be accepted as is.\nTrue story. For instance, there have been cases where gender was encoded with numbers, and someone unfamiliar with gender issues was confused by values like 2 or 3 in a dataset. The point is not to become an expert on gender issues but to avoid relying solely on intuition when analyzing data from unfamiliar domains.\nWhy Do We Need to Know This? These concepts are straightforward, yet critical to understand and differentiate accurately. We here includes researchers applying statistics, statistics majors, and anyone potentially working in data science, even with a different background.\nWhile we study and gain experience, our peers may be engaging in society in other ways. Unfortunately, they might not be as data-savvy, possibly ignoring the precautions mentioned here and making these common mistakes. Consider the general public, who might not question such errors, and even your boss might not be an exception.\nIt\u0026rsquo;s our responsibility to prevent these misunderstandings.\nMendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2420,"permalink":"https://freshrimpsushi.github.io/en/posts/2420/","tags":null,"title":"Qualitative Variable and Quantitative Variable"},{"categories":"추상대수","contents":"Definition The set of real invertible $n \\times n$ matrices is denoted by $\\mathrm{GL}(n, \\mathbb{R})$ or $\\mathrm{GL}_{n}(\\mathbb{R})$ and is called the general linear group of degree $n$general linear group of degree $n$.\n$$ \\mathrm{GL}(n, \\mathbb{R}) := \\left\\{ n \\times n \\text{ invertible matrix} \\right\\} = M_{n \\times n}(\\mathbb{R}) \\setminus {\\left\\{ A \\in M_{n \\times n}(\\mathbb{R}) : \\det{A} = 0 \\right\\}} $$\nExplanation Since it consists only of invertible matrices, it forms a group with respect to matrix multiplication. Moreover, it has a differentiable structure, making it a Lie group.\n","id":3450,"permalink":"https://freshrimpsushi.github.io/en/posts/3450/","tags":null,"title":"General Linear Group"},{"categories":"위상데이터분석","contents":"Theorem 1 2 Definitions of Covering and Lifting: Let\u0026rsquo;s denote the unit interval as $I = [0,1]$.\nAn open set $U \\subset X$ of $X$ is evenly covered by $p$ if for every $\\alpha \\in \\forall$, all corresponding restricted functions $p |_{\\widetilde{U}_{\\alpha}}$ are homeomorphisms, and $$ \\alpha_{1} \\ne \\alpha_{2} \\implies \\widetilde{U}_{\\alpha_{1}} \\cap \\widetilde{U}_{\\alpha_{2}} = \\emptyset $$ holds, meaning there exist disjoint open sets $\\widetilde{U}_{\\alpha} \\subset \\widetilde{X}$ in $\\widetilde{X}$ such that $$ p^{-1} \\left( U \\right) = \\bigsqcup_{\\alpha \\in \\forall} \\widetilde{U}_{\\alpha} $$ is satisfied. If $p : \\widetilde{X} \\to X$ is a surjective function and for every $x \\in X$, there exists an open neighborhood $U_{x} \\subset X$ of $x$ that is evenly covered by $p$, then $p : \\widetilde{X} \\to X$ is called a covering. The domain $\\widetilde{X}$ of the covering $p$ is called the covering space, and the codomain $X$ is called the base space. Let $n \\in \\mathbb{N}$. If $f : I^{n} \\to X$ and $\\widetilde{f} : I^{n} \\to \\widetilde{X}$ satisfy the following, $\\widetilde{f}$ is called a lift of $f$. $$ f = p \\circ \\widetilde{f} $$ Let\u0026rsquo;s denote a covering with sphere $S^{1}$ as its codomain by $p : \\mathbb{R} \\to S^{1}$.\nPath Lifting Theorem A continuous function $f : I \\to S^{1}$ has a lift $\\widetilde{f} : I \\to \\mathbb{R}$. Specifically, for given $x_{0} \\in S^{1}$ and $\\widetilde{x}_{0} \\in p^{-1} \\left( x_{0} \\right)$, there exists a unique $\\widetilde{f}$ satisfying $\\widetilde{f} \\left( 0 \\right) = \\widetilde{x}_{0}$.\nHomotopy Lifting Theorem A continuous function $F : I^{2} \\to S^{1}$ has a lift $\\widetilde{F} : I^{2} \\to \\mathbb{R}$. Specifically, for given $x_{0} \\in S^{1}$ and $\\widetilde{x}_{0} \\in p^{-1} \\left( x_{0} \\right)$, there exists a unique $\\widetilde{F}$ satisfying $\\widetilde{F} \\left( 0 , 0 \\right) = \\widetilde{x}_{0}$.\nExplanation Lifting Theorems are often mentioned as auxiliary theorems for studying the properties of the unit circle $S^{1}$. Formally, the distinction between path lifting and homotopy lifting is not significant.\nThe question most mathematicians should be curious about is whether a generalization for $f: I^{m} \\to X$, when $X \\ne S^{1}$, is possible. It\u0026rsquo;s a fact that lifting theorems can be discussed even for continuous functions $f: Y \\times I^{m} \\to X$ over compact spaces $Y$. However, such extensions are said to be overly complicated and practically useless for direct study.\nProof Strategy: We\u0026rsquo;ll only prove the path lifting theorem. Essentially, the proof of the homotopy lifting theorem is similar to that of the path lifting theorem. Like how the path lifting theorem is proved by subdividing the compact space $I$ into finite intervals, the homotopy lifting theorem involves subdividing the compact space $I^{2}$ into finite parts and repeating the same discussion.\nPart 1. Setting\nIf $p : \\widetilde{X} \\to X$ is a surjective function and for every $x \\in X$, there exists an open neighborhood $U_{x} \\subset X$ of $x$ that is evenly covered by $p$, then $p : \\widetilde{X} \\to X$ is called a covering. Since $p : \\mathbb{R} \\to S^{1}$ is a covering, for every $x \\in S^{1}$, there exists a neighborhood $U_{x} \\subset S^{1}$ of $x$ that is evenly covered by $p$.\nSince $I = [0,1]$ is compact, there exists a finite set of points $\\left\\{ a_{k} \\right\\}_{k=0}^{n} \\subset I$ satisfying $$ 0 = a_{0} \u0026lt; a_{1} \u0026lt; \\cdots \u0026lt; a_{n-1} \u0026lt; a_{n} = 1 $$ and whose intervals $\\left[ a_{k-1} , a_{k} \\right] \\subset I$ have images in $S^{1}$, especially satisfying the inclusion relation for some open set $U \\subset S^{1}$ as follows: $$ f \\left( \\left[ a_{k-1} , a_{k} \\right] \\right) \\subset U \\subset S^{1} $$ If we denote the disjoint preimages of $U$ under covering $p$ by $\\widetilde{U}_{t} := p^{-1} \\left( U_{t} \\right)$, each of them is homeomorphic to $U$ for every $t \\in \\mathbb{Z}$.\nPart 2. Inductive Construction\nLet\u0026rsquo;s specifically fix $x_{0} \\in S^{1}$ instead of any $x \\in S^{1}$, and denote one of the preimages under $p$ as $\\widetilde{x}_{0} := p^{-1} \\left( x_{0} \\right) \\in \\mathbb{R}$. According to the initial setting, there exists a bijection between these elements and $\\mathbb{Z}$, and it doesn\u0026rsquo;t matter which one is chosen.\nWe aim to define the lifting $\\widetilde{f}_{k}$ inductively for $\\left[ 0, a_{k} \\right]$, satisfying $\\widetilde{f}_{k} (0) = \\widetilde{x}_{0}$, instead of the entire $I$ at once, to eventually find $\\widetilde{f}$.\nFor $k = 0$, we simply set it as $\\widetilde{f}_{0} (0) = \\widetilde{x}_{0}$, with no other choice. Assume a unique continuous function $\\widetilde{f}_{k} : \\left[ 0 , a_{k} \\right] \\to \\mathbb{R}$ is defined for $k \\ne 0$. For some unique $\\widetilde{U} \\in \\left\\{ \\widetilde{U}_{t} \\right\\}_{t \\in \\mathbb{Z}}$, $\\widetilde{f} \\left( a_{k} \\right) \\in \\widetilde{U}$ holds. Since $\\widetilde{f}_{k}$ is continuous and interval $\\left[ a_{k} , a_{k+1} \\right]$ is path-connected, the extension function $\\widetilde{f}_{k+1}$, however defined, must map $\\left[ a_{k} , a_{k+1} \\right]$ strictly inside $\\widetilde{U}$. Since $p$ is a covering, a homeomorphism $p | \\widetilde{U}_{t} : \\widetilde{U}_{t} \\to U$ exists for all $t \\in \\mathbb{Z}$, leading to a unique function $\\rho_{k} : \\left[ a_{k} , a_{k+1} \\right] \\to \\widetilde{U}$ satisfying $$ p \\circ \\rho_{k} = f | \\left[ a_{k} , a_{k+1} \\right] $$ The existence of such function $\\rho_{k}$ is based on the existence of a homeomorphism as a restriction of $p$—in other words, due to its injectivity, ensuring $\\rho_{k} \\left( a_{k} \\right) = \\widetilde{f}_{k} \\left( a_{k} \\right)$ and the continuity of $\\rho_{k}$. Gluing Lemma: For a topological space $X,Y$ and two closed sets $A,B \\subset X$ satisfying $A \\cup B = X$, and two continuous functions $f : A \\to Y$ and $g : B \\to Y$ agree on all $x \\in A \\cap B$ as $f(x) = g(x)$, then the function $h$ defined as follows is continuous. $$ h(x) : = \\begin{cases} f(x), \u0026amp; x \\in A \\\\ g(x), \u0026amp; x \\in B \\end{cases} $$\nBy the Gluing Lemma, a unique continuous function $\\widetilde{f}_{k+1} : \\left[ 0 , a_{k+1} \\right] \\to \\mathbb{R}$ can be defined as follows: $$ \\widetilde{f}_{k+1} := \\begin{cases} \\widetilde{f}_{k} (s) \u0026amp; , \\text{if } s \\in \\left[ 0, a_{k} \\right] \\\\ \\rho_{k} (s) \u0026amp; , \\text{if } s \\in \\left[ a_{k} , a_{k+1} \\right] \\end{cases} $$ According to mathematical induction, there specifically exists a lifting to the spiral winding around $S^{1}$ as many times as the wheel of $t \\in \\mathbb{Z}$. Here, $k = 0, 1, \\cdots , n$ is not an index moving up and down in $\\mathbb{R}$ but an index that slices $S^{1}$ into finite parts while rotating.\nPart 3. Homotopy Lifting Theorem\nLet\u0026rsquo;s denote the set of integers from $0$ to $n-1$ simply as $0:n$. Just as we could choose $0 = a_{0} \u0026lt; \\cdots \u0026lt; a_{n} = 1$ based on the compactness of $I$, $I^{2}$ is also compact, allowing for the existence of finite natural numbers $n , m \\in \\mathbb{N}$ that subdivide the square into a grid. $$ \\begin{align*} 0 = a_{0} \u0026lt; \\cdots \u0026lt; a_{n} = 1 \\\\ 0 = b_{0} \u0026lt; \\cdots \u0026lt; b_{m} = 1 \\end{align*} $$ Defining each small square for $i = 0:n$ and $j = 0:m$ as $$ R_{i,j} := \\left[ a_{i-1}, a_{i} \\right] \\times \\left[ b_{j-1} , b_{j} \\right] \\subset I^{2} $$ yields a sequence of small rectangles as follows: $$ R_{0,0} , R_{0,1} , \\cdots , R_{0,m} , R_{1,0} \\cdots, R_{n,m} $$ Repeating the discussion from the path lifting theorem proves the homotopy lifting theorem.\n■\nKosniowski. (1980). A First Course in Algebraic Topology: p137~138.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHatcher. (2002). Algebraic Topology: p29~31.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2419,"permalink":"https://freshrimpsushi.github.io/en/posts/2419/","tags":null,"title":"Proof of the Lifting Theorem in Algebraic Topology"},{"categories":"줄리아","contents":"Overview Originally, Julia formats the data output to fit the size of the REPL beautifully, but sometimes we want to see the entire data comfortably. If the data is foo, you can print the entire data using show(stdout, \u0026quot;text/plain\u0026quot;, foo)1.\nCode julia\u0026gt; foo = rand(100,2)\r100×2 Matrix{Float64}:\r0.956438 0.663427\r0.790117 0.472821\r0.976134 0.198475\r0.727601 0.472336\r0.0469046 0.991999\r0.625807 0.26634\r0.490773 0.588481\r0.352966 0.426474\r0.585632 0.00185974\r⋮\r0.615713 0.707071\r0.891683 0.622176\r0.370576 0.937107\r0.430644 0.0439135\r0.535987 0.551992\r0.43273 0.217023\r0.345366 0.500033\r0.551719 0.0761454 Originally, ⋮ is printed like above, but if it is printed in plain text, the entire data prints out as follows.\njulia\u0026gt; show(stdout, \u0026#34;text/plain\u0026#34;, foo)\r100×2 Matrix{Float64}:\r0.956438 0.663427\r0.790117 0.472821\r0.976134 0.198475\r0.727601 0.472336\r0.0469046 0.991999\r0.625807 0.26634\r0.490773 0.588481\r0.352966 0.426474\r0.585632 0.00185974\r0.13357 0.90977\r0.789999 0.137833\r0.11626 0.385958\r0.629265 0.40623\r0.111327 0.483414\r0.22717 0.0960839\r0.854027 0.690618\r0.00862816 0.426555\r0.292845 0.588308\r0.475157 0.935968\r0.936422 0.116917\r0.421748 0.335614\r0.354324 0.444122\r0.52423 0.311464\r0.306786 0.873037\r0.308008 0.70787\r0.0885757 0.558464\r0.0510476 0.840701\r0.320569 0.28571\r0.89837 0.517027\r0.218359 0.622536\r0.563148 0.488849\r0.508919 0.818068\r0.880726 0.550501\r0.555517 0.953056\r0.466298 0.29687\r0.816757 0.528656\r0.789289 0.294199\r0.51256 0.173814\r0.972556 0.11602\r0.438784 0.815105\r0.218237 0.257226\r0.0838205 0.535666\r0.287095 0.877342\r0.176927 0.942882\r0.855193 0.577759\r0.813356 0.488643\r0.407358 0.970933\r0.224252 0.455783\r0.430215 0.727\r0.0585314 0.727251\r0.77538 0.777196\r0.114963 0.610359\r0.445436 0.472755\r0.0565616 0.153393\r0.695217 0.00669471\r0.673818 0.284351\r0.308611 0.386984\r0.761394 0.32279\r0.017963 0.114759\r0.465956 0.788791\r0.970691 0.264864\r0.0953205 0.359958\r0.437556 0.283858\r0.323666 0.893141\r0.971015 0.109052\r0.117792 0.919322\r0.898883 0.947123\r0.248386 0.462831\r0.895525 0.434108\r0.526593 0.288652\r0.891208 0.848443\r0.344758 0.412774\r0.697527 0.592066\r0.531953 0.50251\r0.0565245 0.449993\r0.168528 0.783811\r0.129681 0.22014\r0.489568 0.232417\r0.875734 0.380527\r0.0207026 0.915546\r0.210948 0.476037\r0.822661 0.517793\r0.579839 0.0221691\r0.455027 0.920253\r0.932968 0.771582\r0.960643 0.841065\r0.0835567 0.943408\r0.578494 0.502968\r0.0655954 0.528926\r0.590831 0.41364\r0.840604 0.790515\r0.327964 0.269113\r0.615713 0.707071\r0.891683 0.622176\r0.370576 0.937107\r0.430644 0.0439135\r0.535987 0.551992\r0.43273 0.217023\r0.345366 0.500033\r0.551719 0.0761454 Environment OS: Windows julia: v1.7.0 https://stackoverflow.com/questions/49304329/how-to-show-all-elements-of-vectors-and-matrices-in-julia/67090474#67090474\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2416,"permalink":"https://freshrimpsushi.github.io/en/posts/2416/","tags":null,"title":"How to Print Without Omitting Data in Julia"},{"categories":"머신러닝","contents":"Overview1 $$ \\includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$\nThe MNISTmodified national institute of standards and technology database refers to a dataset of digit handwriting from American high school students and Census Bureau employees. It is commonly known as [MNIST].\nOfficial Website Description This dataset is frequently used as an example for beginners in machine learning/deep learning. NIST originally collected handwritten data in the following format for the evaluation of character recognition technology for automated sorting of handwritten postal codes. Yann LeCun took this handwritten data from high school students and Census Bureau employees, processed it, and created the MNIST. The image size is 28 x 28, and it consists of 60,000 training sets and 10,000 test sets.\n$$ \\includegraphics[height=30em]{https://www.nist.gov/sites/default/files/styles/960_x_960_limit/public/images/2019/04/27/sd19.jpg?itok=oETq77cZ} $$\nHow to Use Julia In Julia, the MNIST dataset can be used with the machine learning dataset package MLDatasets.jl. By default, it loads the training set in Float32 type. There are options to change this. The available methods are as follows:\ndataset[i]: Returns a tuple of the i-th features and target. dataset[:]: Returns a tuple of all features and target. length(dataset): Returns the number of data. convert2image(dataset, i): Converts the i-th data into a grayscale image. The ImageShow.jl package is required. julia\u0026gt; using MLDatasets\rjulia\u0026gt; train = MNIST()\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :train\rfeatures =\u0026gt; 28×28×60000 Array{Float32, 3}\rtargets =\u0026gt; 60000-element Vector{Int64}\rjulia\u0026gt; test = MNIST(Float64, :test)\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :test\rfeatures =\u0026gt; 28×28×10000 Array{Float64, 3}\rtargets =\u0026gt; 10000-element Vector{Int64}\rjulia\u0026gt; length(train), length(test)\r(60000, 10000)\rjulia\u0026gt; using Plots\rjulia\u0026gt; using ImageShow\rjulia\u0026gt; train.targets[1]\r5\rjulia\u0026gt; heatmap(convert2image(train, 1)) Since the labels are given as integers, one-hot encoding needs to be done separately.\njulia\u0026gt; train.targets[1:5]\r5-element Vector{Int64}:\r5\r0\r4\r1\r9\rjulia\u0026gt; using Flux\rjulia\u0026gt; Flux.onehotbatch(train.targets[1:5], 0:9)\r10×5 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\r⋅ 1 ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ 1 ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ 1 ⋅ ⋅\r1 ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ 1 How to do one-hot encoding in Julia Flux How to implement MLP and train MNIST in Julia Flux Environment OS: Windows11 Version: Julia v1.8.2, MLDatasets v0.7.6, Plots v1.36.1, ImageShow v0.3.6, Flux v0.13.7 Gun-Woo Kwon and Ryeong Heo, Learning AI through Night History and Comics 2, p68\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3444,"permalink":"https://freshrimpsushi.github.io/en/posts/3444/","tags":null,"title":"MNIST Database"},{"categories":"줄리아","contents":"Overview Julia supports [linear algebra](../../categories/Linear Algebra) as well as MATLAB does, if not better. The intuitive and elegant syntax of Julia gives a feeling that it has been well-designed since its inception1.\nCode julia\u0026gt; A = [ 1 0 3\r0 5 1\r3 1 9\r] 3×3 Matrix{Int64}:\r1 0 3\r0 5 1\r3 1 9 As you can see, defining matrices is intuitive and easy from the get-go. Now let\u0026rsquo;s look at some common functions that should exist. Links to related posts are provided in the subsection titles, and no further explanation is given.\nTrace tr() julia\u0026gt; tr(A)\r15 Determinant det() julia\u0026gt; det(A)\r-1.000000000000003 Inverse inv() julia\u0026gt; inv(A)\r3×3 Matrix{Float64}:\r-44.0 -3.0 15.0\r-3.0 6.10623e-16 1.0\r15.0 1.0 -5.0\rjulia\u0026gt; round.(Int64, inv(A))\r3×3 Matrix{Int64}:\r-44 -3 15\r-3 0 1\r15 1 -5 Diagonal matrix and diagonal elements diag(), diagm() julia\u0026gt; diag(A)\r3-element Vector{Int64}:\r1\r5\r9\rjulia\u0026gt; diagm([1,5,9])\r3×3 Matrix{Int64}:\r1 0 0\r0 5 0\r0 0 9 Norm norm() julia\u0026gt; norm(A, 1)\r23.0 Eigenvalues eigvals() julia\u0026gt; eigvals(A)\r3-element Vector{Float64}:\r-0.020282065792505244\r4.846013411157458\r10.174268654635046\rjulia\u0026gt; eigvecs(A)\r3×3 Matrix{Float64}:\r-0.944804 0.117887 0.305692\r-0.0640048 -0.981459 0.180669\r0.321322 0.151132 0.934832\rjulia\u0026gt; eigmax(A)\r10.174268654635046 Matrix Decomposition factorize() julia\u0026gt; factorize(A)\rBunchKaufman{Float64, Matrix{Float64}}\rD factor:\r3×3 Tridiagonal{Float64, Vector{Float64}}:\r-0.0227273 0.0 ⋅ 0.0 4.88889 0.0\r⋅ 0.0 9.0\rU factor:\r3×3 UnitUpperTriangular{Float64, Matrix{Float64}}:\r1.0 -0.0681818 0.333333\r⋅ 1.0 0.111111\r⋅ ⋅ 1.0\rpermutation:\r3-element Vector{Int64}:\r1\r2\r3\rjulia\u0026gt; svd(A)\rSVD{Float64, Float64, Matrix{Float64}}\rU factor:\r3×3 Matrix{Float64}:\r-0.305692 0.117887 -0.944804\r-0.180669 -0.981459 -0.0640048\r-0.934832 0.151132 0.321322\rsingular values:\r3-element Vector{Float64}:\r10.174268654635044\r4.846013411157461\r0.02028206579250516\rVt factor:\r3×3 Matrix{Float64}:\r-0.305692 -0.180669 -0.934832\r0.117887 -0.981459 0.151132\r0.944804 0.0640048 -0.321322 Refer to the [Matrix Algebra](../../categories/Matrix Algebra) category\u0026rsquo;s [Matrix Decomposition](../../categories/Matrix Algebra#Matrix-Decomposition-and-Least-Squares) section. Depending on the form of the matrix, it automatically chooses the appropriate decomposition. Of course, if desired and the conditions are met, one can directly use a specific decomposition function.\nMatrix Operations julia\u0026gt; B = [\r1 0 1\r1 1 0\r2 1 1\r]\r3×3 Matrix{Int64}:\r1 0 1\r1 1 0\r2 1 1\rjulia\u0026gt; A + B\r3×3 Matrix{Int64}:\r2 0 4\r1 6 1\r5 2 10\rjulia\u0026gt; A - B\r3×3 Matrix{Int64}:\r0 0 2\r-1 4 1\r1 0 8\rjulia\u0026gt; A * B\r3×3 Matrix{Int64}:\r7 3 4\r7 6 1\r22 10 12\rjulia\u0026gt; A .* B\r3×3 Matrix{Int64}:\r1 0 3\r0 5 0\r6 1 9\rjulia\u0026gt; B / A\r3×3 Matrix{Float64}:\r-29.0 -2.0 10.0\r-47.0 -3.0 16.0\r-76.0 -5.0 26.0\rjulia\u0026gt; B * inv(A)\r3×3 Matrix{Float64}:\r-29.0 -2.0 10.0\r-47.0 -3.0 16.0\r-76.0 -5.0 26.0\rjulia\u0026gt; A / B\rERROR: SingularException(3) All the operations we consider common sense apply. Division is naturally the same as multiplying by the inverse of multiplication, and if the inverse matrix doesn\u0026rsquo;t exist like B, it raises a singular exception.\nBlock Matrix [] Compared to other languages, creating block matrices is incredibly convenient.\njulia\u0026gt; [A B]\r3×6 Matrix{Int64}:\r1 0 3 1 0 1\r0 5 1 1 1 0\r3 1 9 2 1 1\rjulia\u0026gt; [A;B]\r6×3 Matrix{Int64}:\r1 0 3\r0 5 1\r3 1 9\r1 0 1\r1 1 0\r2 1 1\rjulia\u0026gt; [A,B]\r2-element Vector{Matrix{Int64}}:\r[1 0 3; 0 5 1; 3 1 9]\r[1 0 1; 1 1 0; 2 1 1] Placing a space between two matrices stacks them horizontally, while a semicolon stacks them vertically. The comma doesn\u0026rsquo;t stack matrices but arranges them as an array, following the same syntax used for arrays in general.\nFull Code Content related to complex matrices and inner products is omitted, but included in the full code.\nusing LinearAlgebra\rA = [\r1 0 3\r0 5 1\r3 1 9\r]\rtr(A)\rdet(A)\rinv(A)\rround.(Int64, inv(A))\rdiag(A)\rdiagm([1,5,9])\rnorm(A, 1)\reigvals(A)\reigvecs(A)\reigmax(A)\rfactorize(A)\rsvd(A)\rB = [\r1 0 1\r1 1 0\r2 1 1\r]\rdet(B)\rrank(B)\reigvals(B)\rSymmetric(B) # |\u0026gt; issymmetric\rtranspose(B)\rB\u0026#39;\rC = [\rim im 1\r2 im 0\rim 1 2\r]\rC\u0026#39;\rB\u0026#39;B\rx = [1,2,3]\ry = [0,1,2]\rx\u0026#39;y\rA + B\rA - B\rA * B\rA .* B\rB / A\rB * inv(A)\r[A B]\r[A;B]\r[A,B]\rx\u0026#39; * y\ry * x\u0026#39; Environment OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2412,"permalink":"https://freshrimpsushi.github.io/en/posts/2412/","tags":null,"title":"How to Use the Linear Algebra Package in Julia"},{"categories":"줄리아","contents":"Overview 1 Dates is a module that collects functions related to dates and times. It is inevitably useful not only for general programming but also for handling a lot of data, whether it\u0026rsquo;s related to time series or not1.\nCode Full Code using Dates\r오늘 = DateTime(2022,3,10)\rtypeof(오늘)\rpropertynames(오늘)\r오늘.instant\rmyformat = DateFormat(\u0026#34;d-m-y\u0026#34;)\r내일 = Date(\u0026#34;11-3-2022\u0026#34;, myformat)\rDates.dayname(내일)\r일주일뒤까지 = 오늘:Day(1):DateTime(2022,3,17)\rcollect(일주일뒤까지)\rDates.Day(일주일뒤까지[end]) - Dates.Day(오늘) DateTime Type julia\u0026gt; 오늘 = DateTime(2022,3,10)\r2022-03-10T00:00:00\rjulia\u0026gt; typeof(오늘)\rDateTime For instance, if you assigned the date of March 10th of the year 22 to today using the DateTime() function, then today would have the type DateTime. DateTime has a property called instant, which records time in milliseconds.\njulia\u0026gt; propertynames(오늘)\r(:instant,)\rjulia\u0026gt; 오늘.instant\rDates.UTInstant{Millisecond}(Millisecond(63782553600000)) Format DateFormat() julia\u0026gt; myformat = DateFormat(\u0026#34;d-m-y\u0026#34;)\rdateformat\u0026#34;d-m-y\u0026#34;\rjulia\u0026gt; 내일 = Date(\u0026#34;11-3-2022\u0026#34;, myformat)\r2022-03-11 This is often used when dates are written differently due to the difference between the East and the West.\nDay of the Week Dates.dayname() julia\u0026gt; Dates.dayname(내일)\r\u0026#34;Friday\u0026#34; Returns the day of the week for the given date. Due to the absurdity of the Gregorian calendar, creating such a function myself would be surprisingly difficult.\nVector of Dates julia\u0026gt; 일주일뒤까지 = 오늘:Day(1):DateTime(2022,3,17)\rDateTime(\u0026#34;2022-03-10T00:00:00\u0026#34;):Day(1):DateTime(\u0026#34;2022-03-17T00:00:00\u0026#34;)\rjulia\u0026gt; collect(일주일뒤까지)\r8-element Vector{DateTime}:\r2022-03-10T00:00:00\r2022-03-11T00:00:00\r2022-03-12T00:00:00\r2022-03-13T00:00:00\r2022-03-14T00:00:00\r2022-03-15T00:00:00\r2022-03-16T00:00:00\r2022-03-17T00:00:00 This is arguably the most useful part of the Julia date package. Vectorizing the span between specific points in time as above results in exactly what you would imagine. It\u0026rsquo;s challenging to create, but it\u0026rsquo;s rare to find syntax in other languages that integrates so well and provides such outstanding intuition.\nDate Subtraction - julia\u0026gt; Dates.Day(일주일뒤까지[end]) - Dates.Day(오늘)\r7 days Apparently, you can calculate the interval between two points in time with subtraction. Using Dates.canonicalize() allows for pretty output in hours, minutes, and seconds.\nEnvironment OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/stdlib/Dates/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2410,"permalink":"https://freshrimpsushi.github.io/en/posts/2410/","tags":null,"title":"Using Date and Time Functions in Julia"},{"categories":"줄리아","contents":"Overview 1 2 The Fastest Fourier Transform in the West (FFTW) is a software library developed by Matteo Frigo and Steven G. Johnson at the Massachusetts Institute of Technology (MIT) for computing the Discrete Fourier Transform. While there exists a Julia package named AbstractFFTs.jl for FFT implementation, it is not intended to be used on its own but rather to aid in the implementation of fast Fourier transforms, such as with FFTW.jl.\nThis package is mainly not intended to be used directly. Instead, developers of packages that implement FFTs (such as FFTW.jl or FastTransforms.jl) extend the types/functions defined in AbstractFFTs. This allows multiple FFT packages to co-exist with the same underlying fft(x) and plan_fft(x) interface.3\nSummary Fourier Transform: fft() Transform by column in a $2$-dimensional array: fft( ,[1]) Transform by row in a $2$-dimensional array: fft( ,[2]) Transform specific dimensions of an array: fft( ,[n₁, n₂, ...]) Inverse Fourier Transform: ifft() Frequency centering $0$: fftshift() Inverse: ifftshift() Frequency Sampling: fftfreq(n, fs=1) Code Fourier Transform In Julia, the notations for the Fourier Transform such as $\\mathcal{F}[f]$, $\\hat{f}$ are used directly in code. Let\u0026rsquo;s sample sine waves of frequencies $100$, $200$, $350$ at intervals of $1/1000$ and add them together.\nusing FFTW\rusing Plots\rusing LaTeXStrings\rFs = 1000 #진동수\rT = 1/1000 #샘플링 간격\rL = 1000 #신호의 길이\rx = [i for i in 0:L-1].*T #신호의 도메인\rf₁ = sin.(2π*100*x) #진동수가 100인 사인파\rf₂ = 0.5sin.(2π*200*x) #진동수가 100인 사인파\rf₃ = 2sin.(2π*350*x) #진동수가 100인 사인파\rf = f₁ + f₂ + f₃ Fourier Transform: fft() Inverse Fourier Transform: ifft() According to the definition, the Fourier Transform $f$ of $\\mathcal{F}f$ is only nonzero at $50$, $100$, $200$. By the definition of the Discrete Fourier Transform, we obtain symmetric values around the $y$ axis, but the frequency at $0$ is the first value by default. Therefore, if the aim is to check the frequency and amplitude of the signal, plotting only the first half is sufficient.\nFs = 1000 # 샘플링 주파수\rℱf = fft(f) # 푸리에 변환\rξ = Fs*[i for i in 0:L/2-1]/L #주파수 도메인(절반)\rplot(ξ, abs.(ℱf[1:Int(L/2)])*2/L, title=L\u0026#34;Fourier transform of ▷eq10◁\u0026#34;, label=\u0026#34;\u0026#34;) xlabel!(\u0026#34;frequency\u0026#34;)\rylabel!(\u0026#34;amplitude\u0026#34;)\rsavefig(\u0026#34;fft.png\u0026#34;) Frequency Centering $0$ The output of a Fourier Transform inherently places the value at frequency $0$ first. To center the value at frequency $0$, use fftshift(). To reverse this, use ifftshift(), which is not ifft + shift, but rather the inversion + fftshift, meaning it is the inverse operation of fftshift().\np1 = plot(ξ, abs.(ℱf), title=L\u0026#34;▷eq11◁\u0026#34;, label=\u0026#34;\u0026#34;, yticks=[], xticks=[0, 100, 200, 350, 500, 1000]) p2 = plot(ξ.-500, abs.(fftshift(ℱf)), title=L\u0026#34;▷eq22◁\u0026#34;, label=\u0026#34;\u0026#34;, yticks=[], xticks=[-500,-350,-200,-100,0,100,200,350,500]) plot(p1, p2, size=(800,400))\rsavefig(\u0026#34;fftshift.png\u0026#34;) Multi-dimensional Fourier Transform To compare with the values of a 2-dimensional Fourier Transform, let\u0026rsquo;s first calculate the Fourier Transform values of $x = [1\\ 2\\ 3\\ 4]^{T}$.\njulia\u0026gt; x = [1.0; 2; 3; 4]\r4-element Vector{Float64}:\r1.0\r2.0\r3.0\r4.0\rjulia\u0026gt; fft(x)\r4-element Vector{ComplexF64}:\r10.0 + 0.0im\r-2.0 + 2.0im\r-2.0 + 0.0im\r-2.0 - 2.0im fft() automatically returns a 2-dimensional Fourier Transform when given a 2-dimensional array as input. Alternatively, fft(, [1,2]) means to compute the transform along the first and second dimensions, and it returns the same result.\njulia\u0026gt; y = [x x x x]\r4×4 Matrix{Float64}:\r1.0 1.0 1.0 1.0\r2.0 2.0 2.0 2.0\r3.0 3.0 3.0 3.0\r4.0 4.0 4.0 4.0\rjulia\u0026gt; fft(y)\r4×4 Matrix{ComplexF64}:\r40.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0-8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\rjulia\u0026gt; fft(y, [1,2])\r4×4 Matrix{ComplexF64}:\r40.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0-8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im Therefore, to transform each column, use fft(, [1]), and to transform each row, use fft(, [2]).\njulia\u0026gt; fft(y, [1])\r4×4 Matrix{ComplexF64}:\r10.0+0.0im 10.0+0.0im 10.0+0.0im 10.0+0.0im\r-2.0+2.0im -2.0+2.0im -2.0+2.0im -2.0+2.0im\r-2.0+0.0im -2.0+0.0im -2.0+0.0im -2.0+0.0im\r-2.0-2.0im -2.0-2.0im -2.0-2.0im -2.0-2.0im\rjulia\u0026gt; fft(y, [2])\r4×4 Matrix{ComplexF64}:\r4.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r12.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r16.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im Frequency Sampling fftfreq(n, fs=1) Returns the frequency domain of length $n$ with interval $fs/n$. As mentioned earlier, since the Fourier Transform positions the frequency at $0$ at the forefront, the first value sampled by fftfreq() is $0$. The first half is positive frequencies, and the latter half is negative. Hence, using fftshift() arranges them in ascending order according to the index.\njulia\u0026gt; fftfreq(4, 1)\r4-element Frequencies{Float64}:\r0.0\r0.25\r-0.5\r-0.25\rjulia\u0026gt; fftfreq(5, 1)\r5-element Frequencies{Float64}:\r0.0\r0.2\r0.4\r-0.4\r-0.2\rjulia\u0026gt; fftshift(fftfreq(4, 1))\r-0.5:0.25:0.25 Environment OS: Windows11 Version: Julia 1.8.2, FFTW 1.5.0 http://www.fftw.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaMath/FFTW.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaMath/AbstractFFTs.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3440,"permalink":"https://freshrimpsushi.github.io/en/posts/3440/","tags":null,"title":"How to Use Fast Fourier Transform (FFT) in Julia"},{"categories":"줄리아","contents":"Overview In fields like machine learning, 32-bit floating point numbers are used instead of 64-bit ones for improving computation speed and saving memory. Therefore, in PyTorch, when tensors are created, their data type is fundamentally 32-bit floating point numbers by default. In Julia, there\u0026rsquo;s a machine learning package called Flux.jl, which takes Julia\u0026rsquo;s standard arrays as input for the neural networks it implements. The fact that it does not use separate data structures like tensors can be seen as an advantage, but the hassle of having to set the data type to Float32 manually can also be seen as a drawback. Below, we introduce a way to change the default data type.\nCode1 ChangePrecision.jl By using the @changeprecision macro, the default data type can be changed within a begin ... end block.\njulia\u0026gt; Pkg.add(\u0026#34;ChangePrecision\u0026#34;)\rjulia\u0026gt; using ChangePrecision\rjulia\u0026gt; rand(3)\r3-element Vector{Float64}:\r0.580516564576538\r0.33915094423556424\r0.3612907828959878\rjulia\u0026gt; @changeprecision Float32 begin\rrand(3)\rend\r3-element Vector{Float32}:\r0.0459705\r0.0033969283\r0.579983 Environment OS: Windows10 Version: Julia 1.8.2, ChangePrecision 1.0.0 https://stackoverflow.com/questions/68068823/how-to-change-default-float-to-float32-in-a-local-julia-environment\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3439,"permalink":"https://freshrimpsushi.github.io/en/posts/3439/","tags":null,"title":"How to Change Basic Data Types in Julia"},{"categories":"머신러닝","contents":"Theorem Let\u0026rsquo;s assume we are given an input set $X \\ne \\emptyset$ and a positive definite kernel $k: X \\times X \\to \\mathbb{R}$. Define the Training Dataset as $$ D := \\left\\{ \\left( x_{i} , y_{i} \\right) \\right\\}_{i=1}^{m} \\subset X \\times \\mathbb{R} $$ and a class in the Reproducing Kernel Hilbert Space $H_{k}$ as $$ \\mathcal{F} := \\left\\{ f \\in \\mathbb{R}^{X} : f \\left( \\cdot \\right) = \\sum_{i=1}^{\\infty} \\beta_{i} k \\left( \\cdot , z_{i} \\right) \\land \\beta_{i} \\in \\mathbb{R} \\land z_{i} \\in X \\land \\left\\| f \\right\\| \u0026lt; \\infty \\right\\} \\subset H_{k} $$ Consider an arbitrary objective function $c : \\left( D \\times \\mathbb{R} \\right) ^{m} \\to \\overline{\\mathbb{R}}$ and a monotonically increasing regularizer $g : \\mathbb{R} \\to [0,\\infty)$, defining the Regularized Objective Functional $L : \\mathcal{F} \\to \\overline{\\mathbb{R}}$ as follows: $$ L (f) := c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) $$ Here, the norm $\\left\\| \\cdot \\right\\|$ in $H_{k}$ is given according to the positive definiteness of $k$ as follows: $$ \\left\\| \\sum_{i=1}^{\\infty} \\beta_{i} k \\left( \\cdot , z_{i} \\right) \\right\\|^{2} := \\sum_{i=1}^{\\infty} \\sum_{j=1}^{\\infty} \\beta_{i} \\beta_{j} k \\left( z_{i} , z_{j} \\right) \\ge 0 $$\n$\\mathbb{R}$ is the set of real numbers, and $\\overline{\\mathbb{R}}$ includes infinity $\\infty$ as an extended real number. $\\mathbb{R}^{X}$ is a function space collecting functions with domain $X$ and codomain $\\mathbb{R}$. A regularizer is a penalty function to prevent overfitting on data. A functional is, loosely speaking, a function that takes another function as input. Nonparametric The function $f \\in \\mathcal{F}$ that minimizes $L (f)$ for some $\\left\\{ \\alpha_{i} \\right\\}_{i=1}^{m} \\subset \\mathbb{R}$ is expressed in the following form: $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$\nSemiparametric Assuming a set of $M$ real functions defined in $X$ such that the rank of matrix $\\left( \\psi_{p} \\left( x_{i} \\right) \\right)_{ip}$ is $M$ for $\\left\\{ \\psi_{p} : X \\to \\mathbb{R} \\right\\}_{p=1}^{M}$, then for $f \\in \\mathcal{F}$ and $h \\in \\span \\left\\{ \\psi_{p} \\right\\}$, the $\\tilde{f} = f + h$ that minimizes $$ c \\left( \\left( x_{1}, y_{1}, \\tilde{f} \\left( x_{1} \\right) \\right) , \\cdots , \\left( x_{m}, y_{m}, \\tilde{f} \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) $$ for some $\\left\\{ \\alpha_{i} \\right\\}_{i=1}^{m}, \\left\\{ \\beta_{p} \\right\\}_{p=1}^{M} \\subset \\mathbb{R}$ is expressed in the following form: $$ \\tilde{f} (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) + \\sum_{p=1}^{M} \\beta_{p} \\psi_{p} (\\cdot) $$\nExplanation It is recommended to read the following two posts first if possible:\nReproducing Kernel Hilbert Space Support Vector Machines Representers The Representer Theorem is one of the most important theorems in the context of classical machine learning, especially Support Vector Machines, stating that the objective function $f$ we wish to approximate for given data can be represented in the form of $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$ with respect to a suitable kernel $k$. Here, functions $$ k \\left( \\cdot , x_{i} \\right) = \\phi \\left( x_{i} \\right) (\\cdot)\\in H_{k} $$ with one of the kernel\u0026rsquo;s inputs fixed at $x_{i}$ are called representers. According to this theorem, any function fitted to the training data in the Reproducing Kernel Hilbert Space can be represented as a finite linear combination of representers. This aligns perfectly with the kernel trick for support vector machines in nonlinear regression.\nThis is analogous to the relationship between Deep Learning and the Cybenko Theorem. In the context of data science, representers $\\phi \\left( x_{i} \\right) (\\cdot)$ are also referred to as feature maps, implying that any data $X$ can be mapped into a Hilbert space where its features are known and represented by a finite sum of these features, justifying why many machine learning techniques we\u0026rsquo;ve learned so far work. While these techniques are valid in their own right even without mathematical guarantees, the Representer Theorem is crucial as it provides a theoretical foundation for them.\nObjective Function and Regularizer Although the objective function $c$ and regularizer $g$ are defined very generally in the theorem\u0026rsquo;s statement, in many cases, $c$ can be seen as the mean squared residual, $$ c = {{ 1 } \\over { m }} \\sum_{i=1}^{n} \\left( y_{i} - f \\left( x_{i} \\right) \\right)^{2} $$ measuring the fit between data and $f$, and $g$ can be a squared semi-norm penalty $g \\left( \\left\\| f \\right\\| \\right) = \\lambda \\left\\| f \\right\\|^{2}$1.\nIt\u0026rsquo;s important not to distinguish between the objective function and regularizer merely based on the appearance of the equations or the meanings of the words. The most emblematic application of the Representer Theorem is the Support Vector Machine, where the minimization problem handled in Soft Margin SVM is as follows: $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\lambda \\sum_{k=1}^{n} \\xi_{k} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k} \\\\ \u0026amp; \\xi_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nHere, considering the optimization itself, the objective function for a constant $\\lambda \\ne 0$ is essentially $$ {{ 1 } \\over { 2 \\lambda }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\sum_{k=1}^{n} \\xi_{k} $$ where $\\sum_{k=1}^{n} \\xi_{k}$, representing the discrepancy with data, becomes $c$, and ${{ 1 } \\over { 2 \\lambda }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2}$ derived from the hyperplane of the Support Vector Machine $f (\\mathbf{x}) = \\mathbf{w}^{T} + b$ becomes $g$. This can be confusing depending on whether the focus is on mathematics or machine learning because\nThose with a stronger mathematical inclination see SVM as \u0026rsquo;linear regression first, then considering exceptions\u0026rsquo; and tend to minimize $\\left\\| \\mathbf{w} \\right\\|_{2}^{2}$ first, Those with a stronger data science inclination view SVM as \u0026lsquo;first succeeding in classifying data well, then finding the hyperplane with the largest margin\u0026rsquo; and tend to minimize $\\sum_{k=1}^{n} \\xi_{k}$ first. Both perspectives are understandable, and since the application of the Representer Theorem isn\u0026rsquo;t limited to just SVM, one should think and adapt actively to the problem at hand rather than seeking a mnemonic technique.\nProof 2 For simplicity, only the nonparametric representer theorem is proved as per the reference.\nPart 1. $f = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v$\nDefinition of Reproducing Kernel: A function $k : X \\times X \\to \\mathbb{C}$ is called the reproducing kernel of $H$ if it satisfies the following two conditions:\n(i): Representer: For all $x \\in X$, $$ k \\left( \\cdot , x \\right) \\in H $$ (ii) Reproducing Property: For all $x \\in X$ and all $f \\in H$, $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) = \\delta_{x} (f) $$ Particularly, for all $x_{1} , x_{2} \\in X$, $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; k \\left( \\cdot , x_{2} \\right), k \\left( \\cdot , x_{1} \\right) \\right\u0026gt; $$ Define the (representer) function $\\phi : X \\to \\mathbb{R}^{X}$ in the reproducing kernel $k : X \\times X \\to \\mathbb{R}$ as $x \\mapsto k (\\cdot ,x)$. Since $k$ is a reproducing kernel, the function value of function $\\left( \\phi (x) \\right) (\\cdot)$ at any $x, x' \\in X$ is $$ \\left( \\phi (x) \\right) (x ') = k \\left( x' , x \\right) = \\left\u0026lt; \\phi \\left( x ' \\right) , \\phi (x) \\right\u0026gt; $$ where $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ is the inner product in $H_{k}$. For a given $\\left\\{ x_{i} \\right\\}_{i=1}^{m}$, any function $f \\in \\mathcal{F}$ can be expressed as a sum of a part in $\\span \\left\\{ \\phi \\left( x_{i} \\right) \\right\\}_{i=1}^{m}$ and a part orthogonal to it, satisfying $$ \\left\u0026lt; v , \\phi \\left( x_{j} \\right) \\right\u0026gt; = 0 $$ for all $j$, and for some $\\left( \\alpha_{1} , \\cdots , \\alpha_{m} \\right) \\subset \\mathbb{R}^{m}$ as follows: $$ f = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v $$ We will now argue that $c$ is independent of $v$ and when $v = 0$, $f$ minimizes $L(f)$ in $$ \\begin{align*} L (f) :=\u0026amp; c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) \\\\ =\u0026amp; c + g \\end{align*} $$.\nPart 2. $c$ and $v$ are independent\nThe inner product of function $f = f(\\cdot)$ and the reproducing kernel $k \\left( \\cdot , x_{j} \\right)$, according to the reproducing property, is $$ \\begin{align*} =\u0026amp; \\left\u0026lt; f , k \\left( \\cdot , x_{j} \\right) \\right\u0026gt; \\\\ f \\left( x_{j} \\right) =\u0026amp; \\left\u0026lt; \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v , \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\sum_{i=1}^{m} \\alpha_{i} \\left\u0026lt; \\phi \\left( x_{i} \\right) , \\phi \\left( x_{j} \\right) \\right\u0026gt; + 0 \\end{align*} $$ As this is independent of $v$, in $L (f) = c + g$, the expression dependent only on the training data $D$ and $f$, $$ c = c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) $$ is also independent of $v$.\nPart 3. $g$ is minimized when $v = 0$\n(1): $v$ is orthogonal to $\\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right)$, and (2): Since $g$ is assumed to be a monotonic function, we obtain $$ \\begin{align*} \u0026amp; g \\left( \\left\\| f \\right\\| \\right) \\\\ =\u0026amp; g \\left( \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v \\right\\| \\right) \\\\ =\u0026amp; g \\left( \\sqrt{\\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v \\right\\|^{2} + \\left\\| v \\right\\|^{2}} \\right) \u0026amp; \\because (1) \\\\ \\ge\u0026amp; g \\left( \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) \\right\\| \\right) \u0026amp; \\because (2) \\end{align*} $$ As seen, the equality holds when $v = 0$, and for $g$ to be minimized, $v=0$ must hold. Meanwhile, from Part 2, we confirmed that $v$ cannot influence $c$, so setting it to $v = 0$ is acceptable, and the function $f$ that minimizes $L = c + g$ can be represented in the form of $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$\n■\nWahba. (2019). Representer Theorem. https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchölkopf. (2001). A Generalized Representer Theorem. https://link.springer.com/chapter/10.1007/3-540-44581-1_27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2408,"permalink":"https://freshrimpsushi.github.io/en/posts/2408/","tags":null,"title":"Proof of the Representation Theorem"},{"categories":"머신러닝","contents":"Definition 1 2 Input Space $X \\ne \\emptyset$ is the domain and the codomain is the set of complex numbers $\\mathbb{C}$, and let\u0026rsquo;s denote the space of functions $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right) \\subset \\mathbb{C}^{X}$ composed of mappings $f: X \\to \\mathbb{C}$ as a Hilbert space.\nReproducing Kernel Hilbert Space For a fixed datum $x \\in X$, the functional $\\delta_{x} : H \\to \\mathbb{C}$, which takes a function $f \\in H$ and returns its value at $x$, is called the (Dirac) Evaluation Functional at $x$. $$ \\delta_{x} (f) := f (x) $$ If the evaluation functional $\\delta_{x}$ is continuous for all $x \\in X$, then $H$ is called a Reproducing Kernel Hilbert Space (RKHS) and is sometimes denoted as $H_{k}$. A function $k : X \\times X \\to \\mathbb{C}$ is called the reproducing kernel of $H$ if it satisfies the following two conditions: (i) Representer: For all $x \\in X$, $$ k \\left( \\cdot , x \\right) \\in H $$ (ii) Reproducing Property: For all $x \\in X$ and all $f \\ in H$, $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) = \\delta_{x} (f) $$ Especially, for all $x_{1} , x_{2} \\in X$, the following holds: $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; k \\left( \\cdot , x_{2} \\right), k \\left( \\cdot , x_{1} \\right) \\right\u0026gt; $$ Positive Definite Kernel Let\u0026rsquo;s call a mapping $\\phi : X \\to H$ from the input space $X \\ne \\emptyset$ to the Hilbert space $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ a feature map. In this context, $H$ is also referred to as the feature space. A function $k : X \\times X \\to \\mathbb{C}$ defined by the inner product $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; : H \\times H \\to \\mathbb{C}$ in $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ is called a kernel. $$ k \\left( x_{1} , x_{2} \\right) := \\left\u0026lt; \\phi \\left( x_{1} \\right) , \\phi \\left( x_{2} \\right) \\right\u0026gt; $$ For $m$ data points $\\left\\{ x_{1} , \\cdots , x_{m} \\right\\} \\subset X$, the matrix $K \\in \\mathbb{C}^{m \\times m}$ formed as follows is called the Gram Matrix of the kernel $k$. $$ K := \\left( k \\left( x_{i} , x_{j} \\right) \\right)_{ij} $$ If the Gram Matrix of $k$ is a positive definite matrix, then $k$ is called a positive definite kernel. In other words, a kernel $k$ whose Gram Matrix satisfies the following for all $\\left\\{ c_{1} , \\cdots , c_{m} \\right\\} \\subset \\mathbb{C}$ is a positive definite kernel. $$ \\sum_{i=1}^{m} \\sum_{j=1}^{m} c_{i} \\bar{c_{j}} K_{ij} \\ge 0 $$ Explanation Although the content is complex, let\u0026rsquo;s read it carefully as it\u0026rsquo;s been simplified as much as possible.\nThe Meaning of Hilbert Space in Data Science A Hilbert space is a complete space where an inner product is defined. In mathematics, an inner product is simply a bi-variable scalar function that satisfies certain conditions, but in machine learning, it can be thought of as a measure of similarity. In fact, the cosine similarity used to compare word frequencies between two documents also uses an inner product, and another naive example is when we have three vectors $$ A := \\left( 3, 0, 1 \\right) \\\\ B := \\left( 4, 1, 0 \\right) \\\\ C := \\left( 0, 2, 5 \\right) $$ it\u0026rsquo;s evident that $A$ and $B$ are similar, and both are different from $C$. Although this is still an intuitive inference, quantifying it through inner product yields: $$ A \\cdot B = 12 + 0 + 0 = 12 \\\\ A \\cdot C = 0 + 0 + 5 = 5 \\\\ B \\cdot C = 0 + 2 + 0 = 2 $$ This simple comparison of the absolute values of inner products explains the data better than just \u0026lsquo;it\u0026rsquo;s obvious\u0026rsquo;.\nNote that there are no specific assumptions about the input space $X$. In real applications, we cannot guarantee what kind of bad data we will handle. For example, if $X$ represents photos or document data, it does not make sense to take inner products of photos or documents.\nQ. If $X$ is a set of black and white photos, can\u0026rsquo;t we just consider the photos as matrices and take inner products based on pixel values? A. That would work, and that\u0026rsquo;s exactly what a feature map $\\phi : X \\to H$ does. In this case, $H$ becomes a space of functions defined on a rectangle $[a,b] \\times [c,d]$. Thinking in this way, the existence of a kernel itself is almost like bringing \u0026lsquo;difficult to handle data\u0026rsquo; into a space we are familiar with. Even if the meaning of inner products mentioned above doesn\u0026rsquo;t make sense, since an inner product space is a norm space and a metric space, most assumptions we consider necessary logically hold. An inner product $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ induces a norm $$ \\left\\| f \\right\\| := \\sqrt{ \\left\u0026lt; f , f \\right\u0026gt; } $$ and a norm $\\left\\| f \\right\\|$ induces a metric $$ d (f,g) = \\left\\| f -g \\right\\| $$\nFrom a data science perspective, a norm itself quantifies data. For example, if we define the norm of a black and white photo as the sum of all pixel values, this alone can roughly evaluate how bright or dark the photo is. From a data science perspective, a distance tells us how different two data points are. Distinguishing between right and wrong, similar and different, is undoubtedly important. Apart from these reasons, sometimes in mathematical derivations, inner products become necessary. This post won\u0026rsquo;t cover all related examples as it would become too cluttered. Refer to the \u0026lsquo;Kernel Trick\u0026rsquo; section of the \u0026lsquo;Support Vector Machine\u0026rsquo; post.\nWhy a Function Space? Does it Have to be This Complicated? Most applications of mathematics involve finding \u0026rsquo;the function we want\u0026rsquo;.\nInterpolation finds a polynomial function that fills in between given data points. Statistical regression analysis is a technique for finding a line that best explains the data, which is a linear function. Deep learning approximates non-linear functions by incorporating activation functions because linear functions alone are insufficient. Fourier transform represents a function as a linear combination of trigonometric functions. There are countless examples like these. Returning to machine learning, the reason we consider function spaces is that ultimately, what we are looking for is a function. Even if it\u0026rsquo;s not explicitly stated, we want a function that returns the desired result for a given input. For example, a function that returns the number on a photo of a digit, or one that calculates the probability of loan repayment based on personal information. Such useful functions are unlikely to be simple, and we hope they can be represented as a sum of finitely many $\\phi_{k} (x) (\\cdot)$, which serve as bases. In particular, for $\\phi (x) = k (\\cdot , x)$, the proposition that some function $f$ can be found is precisely the Representer Theorem.\nRepresenter Theorem: In a Reproducing Kernel Hilbert Space, any function fitted to the training data can be represented as a finite linear combination of representers.\nIn summary, in machine learning (especially in the context of Support Vector Machines), what we seek is ultimately a function, so exploring the function space where they reside is inevitable.\nWhy is Dirac\u0026rsquo;s Name in Front of the Evaluation Functional? $$ \\delta_{x_{0}} (x) = \\begin{cases} 1 \u0026amp; , \\text{if } x = x_{0} \\\\ 0 \u0026amp; , \\text{if } x \\ne x_{0} \\end{cases} $$ Originally, the Dirac delta function is known as a function that only has a value at one point. Regardless of its precise definition and usage, its variations are typically named after Dirac if they have a non-zero value at only one point. To aid understanding, imagine two functions $f : \\mathbb{R} \\to \\mathbb{R}$, $\\delta_{x_{0}} : \\mathbb{R} \\to \\mathbb{R}$, and their inner product as $$ \\left\u0026lt; f, \\delta_{x_{0}} \\right\u0026gt; = \\sum_{x \\in \\mathbb{R}} f (x) \\delta_{x_{0}} (x) = f \\left( x_{0} \\right) $$ Though we normally use integration instead of summation for the inner product of functions, and summing over all $x \\in \\mathbb{R}$ is risky, the concept aligns with the idea.\nIn this sense, $\\delta_{x_{0}} (f)$ straightforwardly yields $f \\left( x_{0} \\right)$, \u0026lsquo;obtaining a single point\u0026rsquo; at $x_{0}$, hiding the aforementioned discussion.\nWhy it\u0026rsquo;s Called Reproducing Property The definition of RKHS is quite interesting. Usually, when we say \u0026lsquo;some space\u0026rsquo; in mathematics, we define it as the space where \u0026lsquo;some\u0026rsquo; exists, but RKHS is defined as a Hilbert space where \u0026rsquo;the evaluation functional is continuous at every point\u0026rsquo;, which seems out of the blue.\nRiesz Representation Theorem: Let $\\left( H, \\left\\langle \\cdot,\\cdot \\right\\rangle \\right)$ be a Hilbert space. For a linear functional $f \\in H^{ \\ast }$ and $\\mathbf{x} \\in H$, there exists a unique $\\mathbf{w} \\in H$ such that $f ( \\mathbf{x} ) = \\left\\langle \\mathbf{x} , \\mathbf{w} \\right\\rangle$ and $\\| f \\|_{H^{\\ast}} = \\| \\mathbf{w} \\|_{H}$.\nMoore-Aronszajn Theorem: If a positive definite kernel exists, then a unique RKHS corresponding to it also exists.\nAccording to this definition, the existence of a reproducing kernel in RKHS is not self-evident and requires proof. In fact, the Riesz Representation Theorem guarantees the unique existence of a reproducing kernel in RKHS. Interestingly, conversely, an RKHS corresponding to a reproducing kernel also uniquely exists.\nLet\u0026rsquo;s delve into the formulas in the definition.\nOriginally, the function $k : X \\times X \\to \\mathbb{C}$ could take $x_{1}, x_{2} \\in X$, but if we fix $x$ like in the definition, $k$ essentially becomes $k : y \\mapsto k (y,x)$, a function $k : X \\to \\mathbb{C}$. By blocking one input, it\u0026rsquo;s like $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) $$ This expression is simply the inner product of two functions $f (\\cdot) : X \\to \\mathbb{C}$ and $k \\left( \\cdot , x \\right): X \\to \\mathbb{C}$. There\u0026rsquo;s no need to overcomplicate thinking, \u0026ldquo;How does $f$ come out and how does the inner product with $x$\u0026hellip;\u0026rdquo; It\u0026rsquo;s unnecessary. Since $f(x) \\in \\mathbb{C}$ is also just a result of the inner product, it\u0026rsquo;s just some complex number, the codomain being the set of complex numbers.\nHere, let\u0026rsquo;s discuss the naming of the reproducing property. The word Reproduction inherently carries the meaning of Re-(again, 再) -produce (create, 生), with its first translation being reproduction/generation, second being copying/duplication, and third being reproduction. Reproduction in the sense of breeding doesn\u0026rsquo;t fit, and copying doesn\u0026rsquo;t seem right as there\u0026rsquo;s no original to speak of.\nHowever, if we consider that inner-producting $f(\\cdot)$ and $k (\\cdot, x)$ to get $f(x)$ is \u0026lsquo;reproducing\u0026rsquo; the information contained in $f$ through the kernel, then wouldn\u0026rsquo;t it make sense? Imagine we have a function $y(t)$ dependent on time $t$, representing a YouTube video. We don\u0026rsquo;t see $y$ itself but the reproduction of $\\left\\{ y(t) : t \\in [0,T] \\right\\}$. In this analogy, the kernel $k$ \u0026lsquo;reproduces\u0026rsquo; the function values from $f$, not as the function itself but as its values, justifying the term \u0026lsquo;reproducing kernel\u0026rsquo;.\nFeature Map and Uncomfortable Notation Looking closely at the definitions of kernel and reproducing kernel, we notice that they don\u0026rsquo;t necessarily need each other for their definitions. A kernel is a kernel, and a reproducing kernel is a reproducing kernel, and they become the same when the feature map is also the representer, i.e., $$ \\phi (x) = k \\left( \\cdot , x \\right) $$ A feature map transforms original data into a form that\u0026rsquo;s easier for us to handle,\nand saying that a function is represented by such functions means it\u0026rsquo;s explained by certain features derived from the data. One issue is that even if one intuitively understands up to this point, the notation like $k \\left( \\cdot , x \\right)$ remains uncomfortable, and it\u0026rsquo;s hard to empathize with the motive behind defining kernels separately from their inner products and feature maps.\nSince a feature map is $\\phi : X \\to H$, its function value for $x \\in X$ is some function $\\lambda : X \\to \\mathbb{C}$, which usually isn\u0026rsquo;t confusing. More precisely, $\\phi (x)$ can be written as $$ \\left( \\phi (x) \\right) (\\cdot) = k \\left( \\cdot , x \\right) $$ Then why use such inconvenient notation with the dot $\\cdot$? Most people find it easier to understand with examples where not using such notation would cause more trouble. As mentioned earlier, whether it\u0026rsquo;s a kernel or a reproducing kernel, the space we consistently care about is the function space $H$, and the inner product in $H$ is between functions. First, let\u0026rsquo;s assume a function $f$ is represented by a linear combination of data representers $\\phi \\left( x_{i} \\right)$: $$ f (y) = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) (y) = \\sum_{i=1}^{m} \\alpha_{i} \\left( \\phi \\left( x_{i} \\right) \\right) (y) $$ This already looks messy. Considering another function $g$ and different data $\\left\\{ x'_{j} \\right\\}_{j=1}^{n}$, we get $$ g (y) = \\sum_{j=1}^{n} \\beta_{j} \\left( \\phi \\left( x'_{j} \\right) \\right) (y) $$ Moreover, if we\u0026rsquo;re not using inner products, there\u0026rsquo;s no point in considering an inner product space. Writing down $\\left\u0026lt; f,g \\right\u0026gt;$ gives us $$ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} \\left\u0026lt; \\phi \\left( x_{i} \\right) , \\phi \\left( x'_{j} \\right) \\right\u0026gt; $$ This is unnecessarily complicated. Before taking the inner product, we hardly need to deal with actual $y \\in X$ in the function space, and after taking the inner product, there\u0026rsquo;s no need to keep writing $\\phi$ and the inner product. Seeing this, the notation $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) \\\\ g (\\cdot) = \\sum_{j=1}^{n} \\beta_{j} k \\left( \\cdot , x'_{j} \\right) \\\\ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} k \\left( x_{i} , x'_{j} \\right) $$ might not seem as cumbersome.\nReproducing Kernels are Positive Definite Given data $\\left\\{ x_{k} \\right\\}_{k=1}^{m}$, if $k$ is a kernel, then the following holds: $$ \\begin{align*} \u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\bar{\\alpha_{i}} \\alpha_{j} k \\left( x_{i} , x_{j} \\right) \\\\ =\u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\left\u0026lt; \\alpha_{i} \\phi \\left( x_{i} \\right) , \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\u0026lt; \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) , \\sum_{j=1}^{m} \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) \\right\\|^{2} \\\\ \\ge \u0026amp; 0 \\end{align*} $$ As mentioned earlier, if we consider $\\phi : x \\mapsto k (\\cdot , x)$, the reproducing kernel $k$ is also a kernel and thus positive definite. This positive definiteness of kernels naturally appears in various properties related to kernels.\nKernels Outside of Functional Analysis (1) In general mathematics, a kernel often refers to the abstract algebra kernel $\\ker$. For a structure $Y$ where $0$ is defined, the kernel $\\ker f$ of a function $f : X \\to Y$ is defined as $\\ker f := f^{-1} \\left( \\left\\{ 0 \\right\\} \\right)$. (2) This concept is specialized in linear algebra as the kernel of a linear transformation. If you ask someone who isn\u0026rsquo;t specialized in functional analysis about kernels, nine out of ten times, they\u0026rsquo;ll think of meaning (1). If your background is in mathematics, you should at least know about (1), and even if not, you should be familiar with (2).\nNamed Kernels In the context of machine learning, the following kernels are known. 3 These might not seem like kernels at first glance, but they can be derived from the fact that the sum and product of kernels remain kernels.\nLinear Kernel: $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; $$ Polynomial Kernel: For $c \\ge 0$ and $d \\in \\mathbb{N}$, $$ k \\left( x_{1} , x_{2} \\right) = \\left( \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + c \\right) ^{d} $$ Gaussian Kernel: For $\\sigma^{2} \u0026gt; 0$, $$ k \\left( x_{1} , x_{2} \\right) = \\exp \\left( - {{ \\left\\| x_{1} - x_{2} \\right\\| } \\over { 2 \\sigma^{2} }} \\right) $$ Sigmoid Kernel: For $w, b \\in \\mathbb{C}$, $$ k \\left( x_{1} , x_{2} \\right) = \\tanh \\left( w \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + b \\right) $$ Sejdinovic, Gretton. (2014). What is an RKHS?: p7~11. http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchölkopf. (2001). A Generalized Representer Theorem. https://link.springer.com/chapter/10.1007/3-540-44581-1_27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJakkula. (2006). Tutorial on Support Vector Machine (SVM). https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2406,"permalink":"https://freshrimpsushi.github.io/en/posts/2406/","tags":null,"title":"Definite Kernel and Reproducing Kernel Hilbert Space in Machine Learning"},{"categories":"머신러닝","contents":"Model 1 Simple Definition The method of finding a Support Vector Machine is to find a line or plane that best separates binary classifiable data.\nComplex Definition For an inner product space $X = \\mathbb{R}^{p}$ and labeling $Y = \\left\\{ -1, +1 \\right\\}$, let\u0026rsquo;s denote the Training Dataset composed of $n$ pieces of data as $D = \\left\\{ \\left( \\mathbf{x}_{k} , y_{k} \\right) \\right\\}_{k=1}^{n} \\subset X \\times Y$, and $$ \\begin{align*} X^{+} :=\u0026amp; \\left\\{ \\mathbf{x}_{k} \\in X : y_{k} = +1 \\right\\} \\\\ X^{-} :=\u0026amp; \\left\\{ \\mathbf{x}_{k} \\in X : y_{k} = -1 \\right\\} \\end{align*} $$\nSuppose a hyperplane created by a linear function $f \\left( \\mathbf{x} \\right) = \\mathbf{w}^{T} \\mathbf{x} + b$ with some weight $\\mathbf{w} \\in \\mathbb{R}^{p}$ and bias $b \\in \\mathbb{R}$ is $H : \\mathbf{w}^{T} \\mathbf{x} + b = 0$. The $\\mathbf{x}^{+} \\in X^{+}$s and $\\mathbf{x}^{-} \\in X^{-}$s closest to $H$ are called Support Vectors, and the distance $\\delta$ between them is called the Margin. The [machine learning](../../categories/Machine Learning) technique that finds $\\mathbf{w} , b$ that maximizes the margin while satisfying $$ \\begin{align*} f \\left( \\mathbf{x}^{+} \\right) =\u0026amp; +1 \\\\ f \\left( \\mathbf{x}^{-} \\right) =\u0026amp; -1 \\end{align*} $$\nis called Support Vector Machine (SVM).\n$\\mathbb{R}$ is a set of real numbers, and $\\mathbb{R}^{p}$ is the $p$-dimensional Euclidean space. $X \\times Y$ denotes the Cartesian product of two sets. $\\mathbf{w}^{T}$ is the transpose matrix of $\\mathbf{w}$, and $\\mathbf{w}^{T} \\mathbf{x}$ is the inner product $\\left\u0026lt; \\mathbf{w} , \\mathbf{x} \\right\u0026gt;$ of two vectors $\\mathbf{w}, \\mathbf{x}$. Explanation Simply put, it\u0026rsquo;s like finding a line or plane that divides orange and sky-blue data as shown in the next picture. The red arrows in the picture correspond to the support vectors.\nIn the picture, we found a line for $2$ dimensions and a plane for $3$ dimensions, but for larger $p$ dimensions, we need to find a hyperplane, which becomes harder to represent in a picture. However, the concept of dividing the space into two remains the same. Once binary classification is done with the training dataset, new data can be classified using $f$ as a linear classifier.\nObviously, even with the same binary classification data, the left side is better than the right, as the margin for the sky-blue data on the right is excessive. Specifically, how this is calculated is not necessary to know since packages handle it.\nFor undergraduate students, just understanding up to the simple definition and grasping the concept through pictures is sufficient for future use or comprehension of the terminology. For slightly more complex content, practical summary points, and Python example codes, there are plenty of well-organized documents available on domestic web platforms. 2 3 4\nInner Product Space As evident, SVM itself is not particularly complex conceptually, but the reason for introducing mathematical definitions and equations is due to the extensive theoretical discussions to follow.\nThe Euclidean space $\\mathbb{R}^{p}$ is naturally a vector space, an inner product space, and since an inner product space is a metric space, it is also a metric space. Emphasizing this is important because in the real world of data, assuming an inner product space is quite a good assumption. For example, images, documents, or molecular structures immediately raise concerns about whether they can be directly input into SVM. The definition implicitly uses terms like \u0026lsquo;close in distance\u0026rsquo; and linear functions $f$ involving inner products of vectors, which should not be taken for granted as theory approaches reality.\nSupport Vectors In geometric problems, those on the edge (Boundary) are typically called supports. For example, in the Minimum Enclosing Disk problem, the points on the circumference of the circle determining the circle are called supports. Similarly, the support vectors in SVM are also on the boundary, positioned $\\delta/2$ away from the sets $X^{+}, X^{-}$ and $\\mathbf{x}^{+}, \\mathbf{x}^{-}$.\nThere\u0026rsquo;s no guarantee that support vectors are unique for each $X^{+}, X^{-}$, but uniqueness is not important for the upcoming discussions, so let\u0026rsquo;s assume they are unique without losing generality. No data exists on the margin $H$, and $$ f \\left( \\mathbf{x} \\right) \\begin{cases} \\ge +1 \u0026amp; , \\text{if } \\mathbf{x} \\in X^{+} \\\\ \\le -1 \u0026amp; , \\text{if } \\mathbf{x} \\in X^{-} \\end{cases} $$ thus, for all $\\left\\{ \\mathbf{x}_{k} \\right\\}_{k=1}^{n}$s, $\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ must hold.\nMaximizing the Margin Since support vectors are the closest points to $H$, the distance $\\delta/2$ to $H$ will be the distance when the support vectors fall in the direction $\\mathbf{w}$ perpendicular to $H$. This margin is the same whether for $\\mathbf{x}^{+}$ or $\\mathbf{x}^{-}$, and both being at a distance $\\delta/2$ from the hyperplane $H$ means the distance between the two support vectors is $$ \\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-} $$ where operations like $\\mathbf{x}^{+} - \\mathbf{x}^{-}$ are permitted under the assumption that $X$ is a vector space. Taking the inner product of both sides of $\\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-}$ with $\\mathbf{w}$, in other words, multiplying $\\mathbf{w}^{T}$ on the left side, gives $$ \\begin{align*} \u0026amp; \\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-} \\\\ \\implies \u0026amp; \\delta \\mathbf{w}^{T} \\mathbf{w} = \\mathbf{w}^{T} \\mathbf{x}^{+} - \\mathbf{w}^{T} \\mathbf{x}^{-} \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = \\left( \\mathbf{w}^{T} \\mathbf{x}^{+} + b \\right) - \\left( \\mathbf{w}^{T} \\mathbf{x}^{-} + b \\right) \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = +1 - (-1) \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = 2 \\\\ \\implies \u0026amp; \\delta = {{ 2 } \\over { \\left\\| \\mathbf{w} \\right\\|_{2}^{2} }} \\end{align*} $$ by definition of $f$. Therefore, maximizing the margin is equivalent to minimizing the objective function $\\left\\| \\mathbf{w} \\right\\|_{2}^{2} / 2$, and in summary, SVM is an optimizer that solves the following optimization problem. $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nDerived Models As per the complex definition, SVM finds a linear function, whether it\u0026rsquo;s a line or hyperplane, and naturally, that wouldn\u0026rsquo;t be satisfactory.\nSoft Margin SVM Consider data like the following. SVM cannot perfectly binary classify it due to the mixed data in the middle.\nNote that we had to satisfy the condition $\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ under the constraint that data couldn\u0026rsquo;t exist in the margin of support vectors. If we allow this inequality to be smaller than $1$, it would yield better results than giving up on binary classification altogether, even if it\u0026rsquo;s not perfect. If we denote this allowance for each data as $\\xi_{k} \\ge 0$, we get a new constraint $\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k}$. This weakened margin is called a Soft Margin.\nAlthough the constraint has been relaxed, completely loosening it to $\\xi_{l} = \\cdots = \\xi_{n} = 1$ would negate SVM altogether. To prevent this, a term like $\\sum_{k} \\xi_{k}$ can be added to the objective function as a penalty for making impossible binary classification possible. Of course, such a simple penalty could be meaningless or too sensitive depending on the data scale, so instead of using $0 \\le \\sum_{k} \\xi_{k} \\le n$ as is, it\u0026rsquo;s better to multiply it by a suitable positive number $\\lambda \u0026gt; 0$ and add it. $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\lambda \\sum_{k=1}^{n} \\xi_{k} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k} \\\\ \u0026amp; \\xi_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nKernel Trick Given data like the above, it seems impossible to binary classify it with SVM, whether with a soft margin or not. However, it\u0026rsquo;s clear that sky-blue points are clustered closer to $0$, and orange points appear on the outside. To utilize this information, let\u0026rsquo;s create a new $z$-axis as follows. $$ \\phi (x,y) := (x,y, x^{2} + y^{2}) $$\nThe above picture is a capture of the one below. The below is a 3D space interactive with the mouse, so take a look around.\nWhile it was difficult to find a line that separates data in the original $\\mathbb{R}^{2}$, in the expanded $\\mathbb{R}^{3}$, we can now use SVM to classify data with a suitable plane. Naturally, one might ask, \u0026lsquo;So, is this good transformation $\\phi$ called a Kernel, and is using a kernel called the Kernel Trick?\u0026rsquo; The answer is half right and half wrong. $\\phi$ with an extra step, including the inner product, is what constitutes a kernel.\nReturning to Maximizing the Margin, let\u0026rsquo;s revisit the optimization problem given to us. $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nAlthough constraint $\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ looks neat, it doesn\u0026rsquo;t help much when solving this problem. If we revert to the form in the original training dataset, for $k = 1 , \\cdots , n$, $$ \\begin{cases} f \\left( \\mathbf{x}_{k} \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = 1 \\\\ f \\left( \\mathbf{x}_{k} \\right) \\le -1 \u0026amp; , \\text{if } y_{k} = -1 \\end{cases} \\\\ \\implies \\begin{cases} y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = 1 \\\\ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = -1 \\end{cases} \\\\ \\implies y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 $$ must hold. The method of incorporating this constraint directly into the objective function, treating it as if there were no constraints, is the Lagrange Multiplier Method. For the objective function minus the terms multiplied by $y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\ge 0$ with $\\alpha_{k} \\ge 0$, we get the following optimization problem for $L(\\mathbf{w}, b)$: $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} - \\sum_{k=1}^{n} \\alpha_{k} \\left[ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\right] \\\\ \\text{subject to} \u0026amp; \\alpha_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nTo reiterate, our goal was to find $\\mathbf{w}, b$ that minimizes this objective function. The condition that makes the partial derivative of the objective function with respect to $\\mathbf{w}, b$ equal to $0$ is as follows: $$ \\begin{align*} {{ \\partial L } \\over { \\partial \\mathbf{w} }} = 0 \\implies \u0026amp; \\mathbf{w} = \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{x}_{k} \\\\ {{ \\partial L } \\over { \\partial b }} = 0 \\implies \u0026amp; 0 = \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\end{align*} $$\nSubstituting this directly into $L$ yields $$ \\begin{align*} \u0026amp; L(\\mathbf{w},b) \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} - \\sum_{k=1}^{n} \\alpha_{k} \\left[ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\right] \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\mathbf{w}^{T} \\mathbf{w} - \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) + \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\mathbf{w}^{T} \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{x}_{k} - \\sum_{k=1}^{n} \\alpha_{k} y_{k}\\mathbf{w}^{T} \\mathbf{x}_{k} - b \\sum_{k=1}^{n} \\alpha_{k} y_{k} - \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; - {{ 1 } \\over { 2 }} \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{w}^{T} \\mathbf{x}_{k} - b \\cdot 0 + \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} y_{i} a_{j} y_{j} \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} \\\\ =\u0026amp; \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} a_{j} y_{i} y_{j} \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} \\\\ =\u0026amp; L \\left( \\alpha_{1} , \\cdots , \\alpha_{n} \\right) \\end{align*} $$\nAs expected, to specifically calculate $\\mathbf{w}$ and $b$, the training data $\\left\\{ \\left( \\mathbf{x}_{k}, y_{k} \\right) \\right\\}_{k=1}^{n}$ is required.\nThe point to note here is that the inner product of $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ was used in the equation. Ultimately, we must perform an inner product, and if $X$ is not an inner product space, there\u0026rsquo;s no guarantee we can take this smooth path. Conversely, even if $X$ isn\u0026rsquo;t an inner product space, if the transformation $\\phi$ can send $X$ to an inner product space, considering SVM with the objective function $$ \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} a_{j} y_{i} y_{j} \\phi \\left( \\mathbf{x}_{i} \\right) ^{T} \\phi \\left( \\mathbf{x}_{j} \\right) $$ becomes plausible. In [Machine Learning](../../categories/Machine Learning), functions involving a transformation and inner product of two vectors $$ K \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right) := \\left\u0026lt; \\phi \\left( \\mathbf{x}_{i} \\right) , \\phi \\left( \\mathbf{x}_{j} \\right) \\right\u0026gt; $$ are sometimes referred to as a Kernel. [ NOTE: Even within data science, there are other kernels that could be confused with this. The original mathematical kernel is a function with the same name but entirely different functionality. ]\nIf you can accept the content up to this point mathematically, you should understand why introducing a transformation $\\phi$, not even a kernel, is called the Kernel Trick, and why it\u0026rsquo;s important that it guarantees an inner product space afterwards.\nSeveral kernels that satisfy certain conditions can be considered, especially the original SVM can also be seen as using a Linear Kernel $$ K \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right) = \\left\u0026lt; \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right\u0026gt;^{1} = \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} $$\nSee Also The Kernel Trick section dealt with mathematically simple content, but if you\u0026rsquo;re interested in deeper theories, go beyond SVM and study the following topics:\nKernels in Machine Learning and Reproducing Kernel Hilbert Spaces Proof of the Representation Theorem Code The following is Julia code implementing the kernel trick.\nstruct Sphere\rd::Int64\rend\rSphere(d) = Sphere(d)\rimport Base.rand\rfunction rand(Topology::Sphere, n::Int64)\rdirection = randn(Topology.d, n)\rboundary = direction ./ sqrt.(sum(abs2, direction, dims = 1))\rreturn boundary\rend\rusing Plots\rA = 0.3rand(Sphere(2), 200) + 0.1randn(2, 200)\rB = rand(Sphere(2), 200) + 0.1randn(2, 200)\rscatter(A[1,:],A[2,:], ratio = :equal, label = \u0026#34;+1\u0026#34;)\rscatter!(B[1,:],B[2,:], ratio = :equal, label = \u0026#34;-1\u0026#34;)\rpng(\u0026#34;raw.png\u0026#34;)\rPlots.plotly()\rϕ(z) = z[1]^2 + z[2]^2\rscatter(A[1,:],A[2,:],ϕ.(eachcol(A)), ms = 1, label = \u0026#34;+1\u0026#34;)\rscatter!(B[1,:],B[2,:],ϕ.(eachcol(B)), ms = 1, label = \u0026#34;-1\u0026#34;)\rsavefig(\u0026#34;kernel.html\u0026#34;) Jakkula. (2006). Tutorial on Support Vector Machine (SVM). https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ratsgo.github.io/machine%20learning/2017/05/23/SVM/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://bkshin.tistory.com/entry/Machine-Learning-2-Support-Vector-Machine-SVM\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://hleecaster.com/ml-svm-concept/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2402,"permalink":"https://freshrimpsushi.github.io/en/posts/2402/","tags":null,"title":"Support Vector Machine"},{"categories":"위상데이터분석","contents":"Overview Without considering the geometric meaning, if we just define it plainly, in Algebraic Topology, the Betti Number is merely the rank of the homology group in a chain complex. The problem is that such an explanation does not help those curious about the meaning of Betti numbers, and it\u0026rsquo;s also difficult to learn through examples because the specific calculation is daunting.\nIn this post, we introduce a theorem that answers at least the second question—how to calculate Betti numbers—with its detailed proof. According to the theorem introduced below, a certain matrix can be found according to the given chain complex, and through a series of calculations, the following explicit formula can be derived. $$ \\beta_{p} = \\rank ?_{1} - \\rank ?_{2} $$\nAlthough the best explanation for mathematical content is to convey it without using mathematics, in the case of Betti numbers, one can realize its fundamental principles through the process of deriving the formula. The proof may be quite difficult for undergraduates to follow, but it is written in detail without omissions, so it is recommended to at least give it a try.\nTheorem Definition of Homology Group:\nLet\u0026rsquo;s say $n \\in \\mathbb{N}_{0}$. Chain $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ If it satisfies $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ for all $n$, then $\\mathcal{C} := \\left\\{ \\left( C_{n}, \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ is called a Chain Complex. The Quotient Group $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ is called the $n$-th Homology Group of $\\mathcal{C}$. Homomorphism $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ is called a Boundary or Differential Operator. Elements of $Z_{n} := \\ker \\partial_{n}$ are called $n$-Cycles, and elements of $B_{n} := \\text{Im} \\partial_{n+1}$ are called $n$-Boundaries. Standard Basis Decomposition of Free Chain Complexes Assuming that all $C_{p}$ of the Chain Complex $\\mathcal{C} := \\left\\{ \\left( C_{p}, \\partial_{p} \\right) \\right\\}$ are Free Groups with Finite Rank, there exist Subgroups $U_{p}, V_{p}, W_{p} \\subset C_{p}$ and for all $p$ and $Z_{p} := \\ker \\partial_{p}$ that satisfy the following. $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ =\u0026amp; U_{p} \\oplus Z_{p} \\end{align*} $$ $$ \\begin{align*} \\partial_{p} \\left( U_{p} \\right) \\subset \u0026amp; W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$ Of course, $Z_{p}$ is the Kernel of $\\partial_{p}$, so $\\partial_{p} \\left( V_{p} \\right) = 0$ and $\\partial_{p} \\left( W_{p} \\right) = 0$. Furthermore, the Restriction Function ${\\partial_{p}}_{| U_{p}} : U_{p} \\to W_{p-1}$ from $U_{p}$ to $\\partial_{p}$ has the following Smith Normal Form. $$ \\begin{bmatrix} b_{1} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; \\cdots \u0026amp; b_{l} \\end{bmatrix} $$ Here, $b_{i} \\in \\mathbb{N}$ and $b_{1} \\mid \\cdots \\mid b_{l}$.\nEfficient Computability of Homology Groups 1 The Betti Number of $H_{p} \\left( \\mathcal{C} \\right)$ is called the $p$-th Betti Number of $\\mathcal{C}$. The $\\beta_{p}$ of a finite Complex $K$ is as follows. $$ \\beta_{p} = \\rank Z_{p} - \\rank B_{p} $$ Its specific values can be calculated by the Smith Normal Form of $\\partial_{p}$ as follows. In the diagram, the blue dotted line represents the diagonal components where $1$, the orange solid line represents the diagonal components where $1$ is not $0$, and all other components are $0$2.\nWhat is important here is the number $\\rank B_{p-1}$ of $1$ in the Smith Normal Form, and the number of zero columns $\\rank Z_{p}$.\nProof 3 Part 1. $B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$\nLet\u0026rsquo;s say $$ \\begin{align*} Z_{p} :=\u0026amp; \\ker \\partial_{p} \\\\ B_{p} :=\u0026amp; \\text{Im} \\partial_{p+1} \\\\ W_{p} :=\u0026amp; \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\} \\end{align*} $$. In particular, $W_{p}$ becomes a Subgroup of $C_{p}$, and considering only $\\lambda = 1$, it weakens the condition of the Boundary $B_{p}$, so it is called a Weak Boundary.\nFrom the definition of $W_{p}$, if we consider $\\lambda \\ne 1$ $$ B_{p} \\subset W_{p} $$ From the definition of $Z_{p}$, since $\\forall z_{p} \\in Z_{p}$ is $\\partial_{p} z_{p} = 0$ and $Z_{p} = \\ker \\partial_{p}$ is $\\partial_{p} : C_{p} \\to C_{p-1}$ $$ Z_{p} \\subset C_{p} $$ Since $C_{p}$ is assumed to be a Free Group, it is Torsion-Free, meaning there does not exist $\\lambda \\ne 0$ that satisfies $\\lambda z_{p} = 0$ for any $\\forall z_{p} \\in Z_{p} \\subset C_{p}$. Meanwhile, for all $c_{p+1} \\in C_{p+1}$ $$ \\partial_{p+1} c_{p+1} = \\lambda z_{p} \\in W_{p} $$ If we apply $\\partial_{p}$ to both sides, $$ 0 = \\partial_{p} \\partial_{p+1} c_{p+1} = \\partial_{p} \\lambda z_{p} = \\lambda \\partial_{p} z_{p} $$ so $\\partial_{p} z_{p} = 0$ must hold. This means if $\\lambda z_{p} \\in W_{p}$ then $\\lambda z_{p} \\in Z_{p}$, $$ W_{p} \\subset Z_{p} $$ From such considerations, we obtain the following inclusion relationships. $$ B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p} $$\nPart 2. $W_{p} \\subset Z_{p}$ is a Direct Summand of $Z_{p}$\nFrom the definition of the $p$-th Homology Group $H_{p} \\left( \\mathcal{C} \\right) = Z_{p} / B_{p}$ $$ \\text{proj}_{1} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) $$ is a Projection with a rank reduced by as much as the Coset $B_{p}$, and For the Torsion Subgroup $T_{p} \\left( \\mathcal{C} \\right) \\subset H_{p} \\left( \\mathcal{C} \\right)$ of $H_{p} \\left( \\mathcal{C} \\right)$ $$ \\text{proj}_{2} : H_{p} \\left( \\mathcal{C} \\right) \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ is also a projection. First Isomorphism Theorem: If a Homomorphism $\\phi : G \\to G'$ exists, then $$G / \\ker ( \\phi ) \\simeq \\phi (G)$$\nTherefore, defined as $\\text{proj} := \\text{proj}_{1} \\circ \\text{proj}_{2}$ $$ \\text{proj} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ is also a projection. Elements of $W_{p}$ are expressed as $\\partial_{p+1} d_{p+1}$, so the kernel of this projection $\\text{proj}$ is $W_{p}$, and since every projection is a Surjection, according to the First Isomorphism Theorem, $$ Z_{p} / W_{p} \\simeq H_{p} / T_{p} $$ holds. Here, regardless of how the right side of $H_{p}$ is formed, it\u0026rsquo;s reduced by the Torsion Subgroup $T_{p}$, so it\u0026rsquo;s torsion-free, thereby ensuring that the left side $Z_{p} / W_{p}$ is also torsion-free. Therefore, if $\\alpha_{1} , \\cdots , \\alpha_{k}$ is the basis of $Z_{p} / W_{p}$, and $\\alpha'_{1} , \\cdots , \\alpha'_{l} \\in W_{p}$ is the basis of $W_{p}$, then $\\alpha_{1} , \\cdots , \\alpha_{k}, \\alpha'_{1} , \\cdots , \\alpha'_{l}$ becomes the basis of $Z_{p}$. Thus, $Z_{p}$ can be expressed as a direct sum of the Subgroup $V_{p}$ with the basis $\\alpha_{1} , \\cdots , \\alpha_{k}$ and $W_{p}$.\nPart 3. Basis of $Z_{p}, B_{p-1}, W_{p-1}$\nHomomorphism\u0026rsquo;s Smith Normal Form: If the ranks of free Abelian groups $G$ and $G'$ are $n,m$ and $f : G \\to G'$, respectively, and $g$ is a homomorphism, then there exists a homomorphism $g$ with the following matrix. $$ \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\in \\mathbb{Z}^{m \\times n} $$ Here, $d_{1} , \\cdots, d_{r} \\in \\mathbb{N}$ and $d_{1} \\mid \\cdots \\mid d_{r}$, meaning $d_{k}$ must be a divisor of $d_{k+1}$.\n$\\partial_{p} : C_{p} \\to C_{p-1}$ has the following Smith Normal Form of the matrix $m \\times n$.\n$$ \\begin{matrix} \u0026amp; \\begin{matrix} e_{1} \u0026amp; \\cdots \u0026amp; e_{l} \u0026amp; e_{l} \u0026amp; \\cdots \u0026amp; e_{n} \\end{matrix} \\\\ \\begin{matrix} e'_{1} \\\\ \\vdots \\\\ e'_{l} \\\\ e'_{l} \\\\ \\vdots \\\\ e'_{m} \\end{matrix} \u0026amp; \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\end{matrix} $$\nHere, we will directly show the following three things:\n(1): $e_{l+1} , \\cdots , e_{n}$ is the basis of $Z_{p}$. (2): $b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ is the basis of $B_{p-1}$. (3): $e'_{1} , \\cdots , e'_{l}$ is the basis of $W_{p-1}$. Sub-proof\nAccording to the definition of $\\partial_{p}$, for a general $c_{p} \\in C_{p}$, the following holds. $$ c_{p} = \\sum_{i=1}^{n} a_{i} e_{i} \\implies \\partial_{p} c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ (1): Since $b_{i} \\ne 0$, the necessary and sufficient condition for $Z_{p} = \\ker \\partial_{p}$ is for $a_{i} = 0$ for any $i = 1 \\cdots , l$. Therefore, $e_{l+1} , \\cdots , e_{n}$ is the basis of $Z_{p}$. (2): Every $\\partial_{p} c_{p} \\in B_{p-1}$ can be expressed as a Linear Combination of $b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$, and since $b_{i} \\ne 0$, $b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ is the basis of $B_{p-1}$. (3): Since $b_{i} e'_{i} = \\partial e_{i}$, first of all, $e'_{1}, \\cdots, e'_{l} \\in W_{p-1}$. Conversely, if we set $c_{p-1} \\in C_{p-1}$ as $$ c_{p-1} = \\sum_{i=1}^{m} d_{i} e'_{i} $$ and assume $c_{p-1} \\in W_{p-1}$, then since $W_{p-1}$ was defined as $W_{p-1} = \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\}$, $c_{p-1}$ can be expressed in the form of $$ \\lambda c_{p-1} = \\partial c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ for some $\\lambda \\ne 0$. Comparing coefficients, for $i \u0026gt; l$, we obtain $$ \\lambda d_{i} = 0 \\implies d_{i} = 0 $$ Therefore, $e'_{1} , \\cdots , e'_{l}$ is the basis of $W_{p-1}$. Part 4. Proof of \u0026lsquo;Standard Basis Decomposition of Free Chain Complexes\u0026rsquo;\nFor $C_{p}$ and $C_{p-1}$, if we consider the Free Group generated by $e_{1} , \\cdots , e_{l}$ appearing in the discussion so far as $U_{p}$, then since $Z_{p} = V_{p} \\oplus W_{p}$, we obtain $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus Z_{p} \\\\ =\u0026amp; U_{p} \\oplus \\left( V_{p} \\oplus W_{p} \\right) \\end{align*} $$ as $\\partial V_{p} = \\partial W_{p} = 0$. Here, note that $W_{p}$ and $Z_{p}$ are unique according to $C_{p}$, but $U_{p}$ and $V_{p}$ do not necessarily have to be unique.\nPart 5. Proof of \u0026lsquo;Efficient Computability of Homology Groups\u0026rsquo;\nAccording to Part 4, for the Complex $K$, the following decomposition is guaranteed to exist. $$ \\begin{align*} C_{p} \\left( K \\right) =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$\nProperties of Direct Sum: Let\u0026rsquo;s say $G = G_{1} \\oplus G_{2}$. If $H_{1}$ is a subgroup of $G_{1}$ and $H_{2}$ is a subgroup of $G_{2}$, then $H_{1}$ and $H_{2}$ can also be expressed as a direct sum, especially the following holds. $${{ G } \\over { H_{1} \\oplus H_{2} }} \\simeq {{ G_{1} } \\over { H_{1} }} \\oplus {{ G_{2} } \\over { H_{2} }}$$\n[1]: If we say $H_{1} \\simeq G_{1}$ and $H_{2} \\simeq \\left\\{ 0 \\right\\}$, then $$ G / G_{1} \\simeq G_{2} $$ [2]: If we say $H_{1} \\simeq \\left\\{ 0 \\right\\}$, then $$ {{ G } \\over { H_{2} }} \\simeq G_{1} \\oplus {{ G_{2} } \\over { H_{2} }}$$ Since it was $B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$ in Part 1, according to the properties of the Direct Sum, $$ \\begin{align*} H_{p} \\left( K \\right) =\u0026amp; Z_{p} / B_{p} \\\\ =\u0026amp; \\left( {{ V_{p} \\oplus W_{p} } \\over { B_{p} }} \\right) \\\\ =\u0026amp; V_{p} \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [2] \\\\ =\u0026amp; \\left( {{ Z_{p} } \\over { W_{p} }} \\right) \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [1] \\end{align*} $$ is obtained. Here, in $H_{p} \\left( K \\right) = \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right)$,\n$Z_{p} / W_{p}$ is the Free part, and $W_{p} / B_{p}$ is the Torsion part. Therefore, the $p$-th Betti Number $\\beta_{p}$ of $K$ is calculated as follows. $$ \\begin{align*} \\beta_{p} =\u0026amp; \\rank H_{p} \\left( K \\right) \\\\ =\u0026amp; \\rank \\left[ \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right) \\right] \\\\ =\u0026amp; \\rank \\left( Z_{p} / W_{p} \\right) + \\rank \\left( W_{p} / B_{p} \\right) \\\\ =\u0026amp; \\left[ \\rank Z_{p} - \\rank W_{p} \\right] + \\left[ \\rank W_{p} - \\rank B_{p} \\right] \\\\ =\u0026amp; \\rank Z_{p} - \\rank B_{p} \\end{align*} $$\nMeanwhile, for the torsion part of $H_{p-1}(K)$ and $b_{1} | \\cdots | b_{l} \\in \\mathbb{N}$, the following Isomorphism can be known to exist. $$ W_{p-1} / B_{p-1} \\simeq \\left( {{ \\mathbb{Z} } \\over { b_{1} \\mathbb{Z} }} \\right) \\oplus \\cdots \\oplus \\left( {{ \\mathbb{Z} } \\over { b_{l} \\mathbb{Z} }} \\right) $$ Here, the fact that $b_{i} = 1$ for $i \\le l$, in other words, the rank of $B_{p-1}$ is $l$, is because $$ \\mathbb{Z} / b_{i} \\mathbb{Z} = \\mathbb{Z} / \\mathbb{Z} = \\left\\{ 0 \\right\\} $$ so the rank of $W_{p-1}$ is reduced by $l$.\n■\nExample Torus $$ \\begin{align*} \\beta_{0} =\u0026amp; 1 \\\\ \\beta_{1} =\u0026amp; 2 \\\\ \\beta_{2} =\u0026amp; 1 \\end{align*} $$\nThe Betti numbers of a torus are known as above. Assuming the chain complex of this torus is defined as in the above figure, let\u0026rsquo;s just calculate $\\beta_{1} = 2$ as an example. There is also a way to calculate it by just mathematically pondering without using the formula derived above, but as you can read, it\u0026rsquo;s headache-inducingly difficult. In contrast, let\u0026rsquo;s see how convenient it is to \u0026rsquo;efficiently calculate homology\u0026rsquo;.\nHomomorphism\u0026rsquo;s Smith Normal Form: For Free Abelian Groups $G$ and $G'$, if $a_{1} , \\cdots , a_{n}$ is the basis of $G$, and $a_{1}' , \\cdots , a_{m}'$ is the basis of $G'$, and if the function $f : G \\to G'$ is a Homomorphism, then there exists a unique set of integers $\\left\\{ \\lambda_{ij} \\right\\} \\subset \\mathbb{Z}$ that satisfies the following. $$ f \\left( a_{j} \\right) = \\sum_{i=1}^{m} \\lambda_{ij} a_{i}' $$ Here, the matrix $\\left( \\lambda_{ij} \\right) \\in \\mathbb{Z}^{m \\times n}$ is called the Matrix of $f$ with respect to the bases of $G$ and $G'$.\nSince $\\beta_{1} = \\rank Z_{1} - \\rank B_{1}$, at least the Boundary Matrices $\\left( \\partial_{1} \\right)$ and $\\left( \\partial_{2} \\right)$ need to be found. For all $a , b, c \\in C_{1} (T)$, $$ \\begin{align*} \\partial_{1} (a) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (b) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (c) =\u0026amp; v - v = 0 = 0v \\end{align*} $$ is obtained. $Z_{p}$ is the number of zero vectors on the right side of the matrix, and $B_{p-1}$ is the number of $1$ in the matrix. Considering $\\partial_{2}$ next, $$ \\begin{align*} \\partial_{2} (U) =\u0026amp; -a -b +c \\\\ \\partial_{2} (L) =\u0026amp; a + b - c \\end{align*} $$ is obtained. Combining these, the $1$-th Betti Number $\\beta_{1}$ of the torus is calculated as follows. $$ \\beta_{1} = \\rank Z_{1} - \\rank B_{1} = 3 - 1 = 2 $$ Of course, this result is guaranteed to match the value obtained by using all sorts of mathematical knowledge, discussing what the Free Group is, what an Isomorphism is, and so on, according to the theorems introduced in this post. To speak a bit recklessly, one can \u0026lsquo;calculate\u0026rsquo; Betti numbers, that is, \u0026lsquo;homology\u0026rsquo;, just by following instructions without using the brain. On the other hand, to put it more positively, it opens the way to studying topology through computers.\nMunkres. (1984). Elements of Algebraic Topology: p58.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p104.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (1984). Elements of Algebraic Topology: p58~61.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2399,"permalink":"https://freshrimpsushi.github.io/en/posts/2399/","tags":null,"title":"Betti Number of Homology Group"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 $2$Qubit $\\ket{a, b} = \\ket{a} \\otimes \\ket{b}$ The exchange gateexchange gate $\\text{ex}$ is defined as follows.\n$$ \\begin{align*} \\text{ex} : (\\mathbb{C}^{2})^{\\otimes 2} \u0026amp;\\to (\\mathbb{C}^{2})^{\\otimes 2} \\\\ \\ket{a, b} \u0026amp;\\mapsto \\ket{b, a},\\quad \\forall a,b \\in \\left\\{ 0, 1 \\right\\} \\end{align*} $$\n$$ \\text{ex} (\\ket{a} \\otimes \\ket{b}) = \\ket{b} \\otimes \\ket{a} $$\nExplanation The exchange gate swaps the states of two qubits. The specific input and output are as follows.\n$$ \\text{ex} (\\ket{00}) = \\ket{00} \\\\[0.5em] \\text{ex} (\\ket{01}) = \\ket{10} \\\\[0.5em] \\text{ex} (\\ket{10}) = \\ket{01} \\\\[0.5em] \\text{ex} (\\ket{11}) = \\ket{11} $$\nMatrix representation is as follows.\n$$ \\text{ex} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nKim Young-hoon \u0026amp; Heo Jae-seong, Quantum Information Theory (2020), p97\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3429,"permalink":"https://freshrimpsushi.github.io/en/posts/3429/","tags":null,"title":"Exchange Gate"},{"categories":"줄리아","contents":"Overview In Julia, this introduces how to schedule computations across multiple devices1. Honestly, I\u0026rsquo;m not quite sure myself.\nCode using Distributed\rip_ = []\rfor last in [160,161,162,163,164,32,33,34,35,36,43,44,45,46,47]\rpush!(ip_, join([155,230,211,last],\u0026#39;.\u0026#39;))\rend\rsort!(ip_)\rfor ip in ip_\raddprocs([(\u0026#34;chaos@\u0026#34; * ip, 8)]; dir =\u0026#34;/home/chaos\u0026#34;, exename = \u0026#34;julia\u0026#34;) #add slave node\\\u0026#39;s workers\rprintln(\u0026#34;ip $ip\u0026#34; * \u0026#34; passed\u0026#34;)\rend\rnworkers()\r@everywhere function f(n)\rreturn n^2 - n end\rA = pmap(f,1:20000)\rX = []\r@async @distributed for i in 1:200\rprint(f(i))\rpush!(X, f(i))\rend pmap works well, but @distributed does not.\nEnvironment OS: Windows julia: v1.7.0 https://thomaswiemann.com/assets/teaching/Fall2021-Econ-31720/Econ_31720_discussion_6.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2398,"permalink":"https://freshrimpsushi.github.io/en/posts/2398/","tags":null,"title":"Distributed Computing in Julia"},{"categories":"줄리아","contents":"Overview Julia provides a type of index that can reference multi-dimensional arrays, known as CatesianIndex1. Naturally, the naming Catesian comes from the Cartesian product, which is the product of sets.\nCode julia\u0026gt; M = rand(0:9, 4,4)\r4×4 Matrix{Int64}:\r9 3 7 0\r8 6 2 1\r3 8 4 9\r5 6 8 2 For example, let\u0026rsquo;s assume you want to access the element 9, which is in the 3rd row and 4th column of the matrix M.\njulia\u0026gt; pt = (3,4)\r(3, 4)\rjulia\u0026gt; M[pt]\rERROR: LoadError: ArgumentError: invalid index: (3, 4) of type Tuple{Int64, Int64}\rjulia\u0026gt; M[pt[1],pt[2]]\r9 Intuitively, it seems like you could simply use the tuple pt = (3,4), but people familiar with programming will recognize that this method has its flaws. Typically, when referencing a two-dimensional array, especially a matrix, you need to explicitly separate the two integers like pt[1],pt[2].\njulia\u0026gt; pt = CartesianIndex(3,4)\rCartesianIndex(3, 4)\rjulia\u0026gt; M[pt]\r9 Thankfully, Julia provides the CatesianIndex, which allows you to pass the index as a whole. By converting the tuple directly into a CatesianIndex, you get the desired result.\nFull Code M = rand(0:9, 4,4)\rpt = (3,4)\rM[pt]\rM[pt[1],pt[2]]\rpt = CartesianIndex(3,4)\rM[pt] Environment OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/arrays/#Base.IteratorsMD.CartesianIndex\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2394,"permalink":"https://freshrimpsushi.github.io/en/posts/2394/","tags":null,"title":"Julia's Multidimensional Indices"},{"categories":"줄리아","contents":"Overview In Julia, \u0026amp;\u0026amp; and || not only perform logical AND and OR operations but also execute short-circuit evaluation1. For instance, A \u0026amp;\u0026amp; B returns true only if both A and B are true, but in reality, if A is false, there is no need to check whether B is true or false; A \u0026amp;\u0026amp; B is false. Short-circuit evaluation essentially skips checking B. Skipping the calculation for B can lead to performance improvements in some cases.\nSee Also How to write conditional statements succinctly Speed Comparison M = rand(0:2, 10^4, 10^4);\rprint(first(M))\r@time if first(M) == 2 \u0026amp;\u0026amp; sum(M) \u0026lt; 1000\rprint(\u0026#34;!!\u0026#34;)\rend\r@time if sum(M) \u0026lt; 1000 \u0026amp;\u0026amp; first(M) == 2\rprint(\u0026#34;!!\u0026#34;)\rend The only difference between the two conditionals is the order of sum(M) \u0026lt; 1000 and first(M) == 2, yet they perform exactly the same task. However, since first(M) == 2 merely checks if the first element of matrix M is 2, and sum(M) sums up all elements, traversing through them, the latter relatively takes longer to compute.\njulia\u0026gt; M = rand(0:2, 10^4, 10^4);\rjulia\u0026gt; print(first(M))\r0 If the first element of M is 0 as shown above, there is no need to calculate sum(M) to see if sum(M) \u0026lt; 1000. The speeds can vary significantly just by changing the order.\njulia\u0026gt; @time if first(M) == 2 \u0026amp;\u0026amp; sum(M) \u0026lt; 1000\rprint(\u0026#34;!!\u0026#34;)\rend\r0.000009 seconds\rjulia\u0026gt; @time if sum(M) \u0026lt; 1000 \u0026amp;\u0026amp; first(M) == 2\rprint(\u0026#34;!!\u0026#34;)\rend\r0.040485 seconds (1 allocation: 16 bytes) Environment OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/control-flow/#Short-Circuit-Evaluation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2392,"permalink":"https://freshrimpsushi.github.io/en/posts/2392/","tags":null,"title":"Julia's Short Circuit"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition An element of the set $\\left\\{ 0, 1 \\right\\}$ is called a bitbit. An element of the set $\\left\\{ 0, 1 \\right\\}^{n}$ is called a $n$bit$n$bit.\nDescription The term bit is an abbreviation for binary digit. It is commonly described as \u0026ldquo;something that can have the value of either $0$ or $1$.\u0026rdquo; It is the smallest unit of information that a classical computer processes. In computer circuits, $1$ signifies the presence of an electrical signal, while $0$ signifies the absence of an electrical signal.\nThe smallest unit of information processed by a quantum computer is called a quantum bitqubit, adding quantum to bit.\nSee Also Boolean function Qubit ","id":3422,"permalink":"https://freshrimpsushi.github.io/en/posts/3422/","tags":null,"title":"Bit: Unit of Information Classical Computer"},{"categories":"줄리아","contents":"Overview Julia\u0026rsquo;s basic built-in functions are increasingly useful the more you know them. Without further ado, let\u0026rsquo;s learn through examples.\nCode x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rargmin(x)\rargmax(x)\rfindmin(x)\rfindmax(x)\rextrema(x)\rfindfirst(x .== 3)\rfindlast(x .== 3)\rfindall(x .== 3)\rfindnext(x .== 3, 5)\rfindprev(x .== 3, 5) Optimal solutions argmin(),argmax(),findmin(),findmax(),extrema() Finding the optimal solutions.\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; argmin(x)\r9\rjulia\u0026gt; argmax(x)\r7\ry = [7, 8, 9, 9, 7, 9]\rjulia\u0026gt; argmax(y)\r3\rjulia\u0026gt; findall(y.==maximum(y))\r3-element Vector{Int64}:\r3\r4\r6 argmin(),argmax() simply return the index of the optimal solution, i.e., the index where the value is the largest or smallest. If there are multiple indices, it returns the smallest one. So, in fact, argmax(x) $= \\min(\\argmax(x))$. The real $\\argmax$ is done using maximum() along with the findall() function.\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findmin(x)\r(2, 9)\rjulia\u0026gt; findmax(x)\r(12, 7) findmin(),findmax() return the optimal solution along with its value.\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; extrema(x)\r(2, 12) Note that extrema() returns not the indices but only the values. This is similar to the range() function in R1.\nFirst/Last index meeting a condition findfirst(), findlast() x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findfirst(x .== 3)\r1\rjulia\u0026gt; findlast(x .== 3)\r8 Found the first and last index where 3 is located. If the shape of the array can be roughly anticipated, these can be used to improve the speed of the code.\nAll indices that meet a condition findall() x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findall(x .== 3)\r3-element Vector{Int64}:\r1\r6\r8 This is the most convenient to use and the most useful function in general programming. Used with maximum(), minimum(), it finds all $\\text{argmin}\nSpecific range of indices meeting a condition findnext(), findprev() julia\u0026gt; findnext(x .== 3, 5)\r6\rjulia\u0026gt; findprev(x .== 3, 5)\r1 Sometimes, we may need to search beyond exceptions. For instance, using findall() to find an element identical to the first element of an array would also find that first element itself, which could be cumbersome.\nEnvironment OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/collections/#Base.extrema\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2390,"permalink":"https://freshrimpsushi.github.io/en/posts/2390/","tags":null,"title":"Julia's find functions"},{"categories":"줄리아","contents":"Overview 1 In Julia, appending an exclamation mark ! at the very end of a function name is referred to as the bang convention. Such functions are characterized by modifying the arguments they are given.\nCode function add_1!(x)\rx .+= 1\rreturn x\rend\rfoo = [2,5,-1]\radd_1!(foo)\rfoo For example, executing the code above yields the following result.\njulia\u0026gt; foo = [2,5,-1]\r3-element Vector{Int64}:\r2\r5\r-1\rjulia\u0026gt; add_1!(foo)\r3-element Vector{Int64}:\r3\r6\r0\rjulia\u0026gt; foo\r3-element Vector{Int64}:\r3\r6\r0 The array foo was defined outside the function and was not only returned with each element increased by $1$ through add_1!(), but the argument itself was modified.\nDescription A representative method, pop!(), deletes the last element of an array while also returning it. If this function couldn\u0026rsquo;t modify the original array, users familiar with general programming would have found it difficult to use widely recognized data structures as is the case with Matlab or R, making it cumbersome.\nIt can be perceived similarly to how in Python, using a method instead of a function can change the data of a class. While not an exact explanation due to Julia\u0026rsquo;s design not supporting classes, it can still be conveniently used like the methods in Python when that feels more intuitive.\nEnvironment OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/style-guide/#bang-convention\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2388,"permalink":"https://freshrimpsushi.github.io/en/posts/2388/","tags":null,"title":"Julia's Exclamation Point Convention"},{"categories":"위상데이터분석","contents":"Problem Smallest Enclosing Disk Let\u0026rsquo;s denote it as $n \u0026gt; d$. In a $d$-dimensional Euclidean space, given a finite set $P = \\left\\{ p_{k} \\right\\}_{k=1}^{n} \\subset \\mathbb{R}^{d}$, the following optimization problem is referred to as the Smallest Enclosing Disk Problem: $$ \\begin{matrix} \\text{Minimize} \u0026amp; r \\ge 0 \\\\ \\text{subject to} \u0026amp; \\left\\| c - p_{k} \\right\\|_{2} \\le r \\end{matrix} \\\\ c \\in \\mathbb{R}^{d} , k = 1, \\cdots , n $$\nTips Here are some useful facts about this problem, though they may not be grand enough to be called theorems:\nIf $P$ are affinely independent, the disk\u0026rsquo;s boundary will contain between $2$ and $d+1$ points from $P$. Simply put, unless the points overlap or more than two lie on the same line, exactly $2 \\le m \\le d+1$ points uniquely determine the smallest enclosing disk. For example, in a $d = 2$-dimensional plane, a circle is uniquely determined by exactly three points. Generally, when $n \u0026gt; d+1$ points are given, it is believed to be impossible to find an explicit formula, not an algorithm, to solve this problem. The points that uniquely determine the smallest enclosing disk on the boundary are called supports, and it\u0026rsquo;s impossible to identify the supports just by looking at the points. The concept of points on the boundary being called supports is common in geometric problems. In Support Vector Machines, the supports refer to the points on the boundary. Therefore, developing an algorithm to solve this problem essentially means ensuring or finding a quick way to identify the boundary supports. However, the fundamental idea of the current algorithms is almost based on Welzl\u0026rsquo;s concept, and they are collectively referred to as the Welzl Algorithm1, with subsequent studies following this lineage. Solution Welzl\u0026rsquo;s Algorithm 2 Welzl\u0026rsquo;s Algorithm is a recursive solution to the smallest enclosing disk problem. It fundamentally involves adding and removing points one by one to find supports, and repeating this process to ensure the disk obtained encompasses all given points.\nWhen $n \\le d+1$ points are given, finding a disk that exactly encloses them is relatively simple, so we assume such a function exists. Though in actual implementation, this may not be as straightforward, the crux of the smallest enclosing disk problem lies elsewhere.\nPseudocode 3 $(c,r)$ = welzl$\\left( P, S \\right)$\nInput: Receives a set of given points $P = \\left\\{ p_{k} \\right\\}_{k=1}^{n} \\subset \\mathbb{R}^{d}$ and a set of candidate supporters $S \\subset P$. As mentioned in the tips, $\\left| S \\right| \\le d+1$ holds. Output: Obtains a tuple $(c,r)$ of the center $c$ and radius $r$ of the smallest disk enclosing all points of $P$. Let\u0026rsquo;s denote the closed ball with center $c$ and radius $r$ as $D = B \\left[ c,r \\right]$. $(c,r)$ = trivial$\\left( S \\right)$\nInput: Receives a set of points $S \\subset P$ where $\\left| S \\right| \\le d+1$ holds. Output: Obtains a tuple $(c,r)$ of the center $c$ and radius $r$ of the smallest disk enclosing all points of $S$. Assuming trivial is simpler than welzl, its pseudocode is not provided separately. function welzl$\\left( P, S \\right)$\n$S := \\emptyset$\nif $P = \\emptyset$ or $\\left| S \\right| = d+1$ then\nreturn trivial$\\left( S \\right)$\nelse\nchoose $p \\in P$\n$D := $ welzl$\\left( P \\setminus \\left\\{ p \\right\\}, S \\right)$\nif $p \\in D$ then\nreturn $D$\nend if\nend if\nreturn welzl$\\left( P \\setminus \\left\\{ p \\right\\} , S \\cup \\left\\{ p \\right\\} \\right)$\nend function\nwelzl is written as a recursive function, meaning the calculation begins with trivial. The appearance of choose in the pseudocode of welzl as choose $x \\in X$ indicates randomly selecting an element $x$ from the set $X$.\nIf you understand that returning welzl$\\left( P \\setminus \\left\\{ p \\right\\} , S \\cup \\left\\{ p \\right\\} \\right)$ at the end signifies the process of finding supports by sequentially removing points from $P$ and adding them to $S$, then you have essentially grasped the algorithm.\nExplanation Let\u0026rsquo;s unpack the constraints of the smallest enclosing disk. Finding $c$ and $r$ that satisfy $\\left\\| c - p_{k} \\right\\|_{2} \\le r$ means finding a center $c$ and radius $r$ that can at least enclose all given points. However, since the Euclidean space is vast, these constraints can always be met by arbitrarily choosing $r$ as long as $c$ is fixed. Naturally, our interest lies in minimizing $r$, i.e., finding the smallest ball that encloses the given points.\nThe Issue with Simplification Enclosing is not a widely used term across mathematics, but using enclosing directly in translation felt too lengthy and abstract, hence the chosen term. Initially, enclosing itself isn\u0026rsquo;t overwhelmingly more common than bounding. Moreover, the term disk is used here to mean a closed ball, but in English, terms like ball or sphere are also frequently used. Let\u0026rsquo;s not get too hung up on each word and its translation.\nHistory An early solution based on linear programming was known through Raimund Seidel.\nIn 1991, Emo Welzl proposed a recursive algorithm in his paper, setting a new standard, the so-called SOTA (State Of The Art), for the smallest enclosing disk problem. As of 2022, Welzl\u0026rsquo;s algorithm is still known to be the best for the general smallest enclosing disk problem, with many subsequent studies improving upon it.\nIn 1999, Bernd Gärtner improved upon Welzl\u0026rsquo;s algorithm by incorporating applications of quadratic programming, making his approach more efficient.4 His implementation, written in C++, can be seen on the ETH Zurich website5.\nIn 2003, Kaspar Fischer introduced the use of the simplex method from linear programming and the Bland\u0026rsquo;s rule to write faster code,6 and in 2013, Thomas Larsson proposed a method that, while approximate, boasted speed and robustness.7\nThe research introduced thus far can be seen to follow a major lineage from Welzl to Gärtner to Fischer upon reviewing their references.\nApplications A notable application of Welzl\u0026rsquo;s algorithm is the construction of Čech complexes.\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p73~75.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWelzl. (1991). Smallest enclosing disks (balls and ellipsoids). https://doi.org/10.1007/BFb0038202\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Smallest-circle_problem#Welzl's_algorithm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGärtner. (1999). Fast and robust smallest enclosing balls. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.5783\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://people.inf.ethz.ch/gaertner/subdir/software/miniball.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKaspar Fischer. (2003). Fast Smallest-Enclosing-Ball Computation in High Dimensions. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.5783\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThomas Larsson. (2013). Fast and Robust Approximation of Smallest Enclosing Balls in Arbitrary Dimensions. https://doi.org/10.1111/cgf.12176\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2385,"permalink":"https://freshrimpsushi.github.io/en/posts/2385/","tags":null,"title":"Welzl Algorithm: Solution to Smallest Enclosing Disk Problem"},{"categories":"줄리아","contents":"Overview In Julia, view is a data structure that quickly refers to a subarray of an array, making it seem cumbersome from a user\u0026rsquo;s perspective, although there might seem to be no difference. However, it returns a lighter array as it is lazily referenced. Therefore, in Julia code that is optimized even at a very basic level, it is easy to find the macro @views.\nCode Let\u0026rsquo;s refer to a submatrix of the matrix M.\nFunctional form: view() view(A, inds...)\nReturns a view according to inds... of A. However, this form is generally not preferred as it makes the code harder to read. By using the following macro, view can be used without much difference from the basic Julia syntax.\nMacro: @view The @view macro changes the context of the code referring to subarrays as if view has been applied.\nApply to the entire block: @views The @views macro applies @view to the entire following block. Thanks to this, simply adding @views like @views f(x) ... end in front of a function written comfortably without view automatically applies view.\nFull Code Speed Comparison fcopy() and fview() are functions that perform exactly the same function but differ in speed. At a glance, the speeds seem similar, but most of it is compilation time. Excluding this and comparing only the simple execution time, there is about a 4-times difference.\nEnvironment OS: Windows julia: v1.7.0 ","id":2384,"permalink":"https://freshrimpsushi.github.io/en/posts/2384/","tags":null,"title":"How to Quickly Reference Subarrays in Julia"},{"categories":"위상데이터분석","contents":"Buildup Despite the complexity of the content, I made sure to leave detailed calculations and explanations to make it as understandable as possible. If you\u0026rsquo;re interested in homology, I highly recommend reading this.\nConsider a topological space $X$ of interest, represented through a $\\Delta$-complex structure according to a specific simplicial complex. As a small example, in the image on the right, the torus represents $X$, and the left side corresponds to the simplicial complex.\nDefinition of a simplex:\nThe convex hull of $v_{0}, v_{1} , \\cdots , v_{n} \\in \\mathbb{R}^{n+1}$, which are affinely independent, is called an $n$-simplex $\\Delta^{n}$, and the vectors $v_{k}$ are called vertices. Mathematically, it is expressed as follows. $$ \\Delta^{n} := \\left\\{ \\sum_{k} t_{k} v_{k} : v_{k} \\in \\mathbb{R}^{n+1} , t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$ An $n-1$-simplex $\\Delta^{n-1}$ created by removing a vertex from $\\Delta^{n}$ is called a face of $\\Delta^{n}$. The union of all faces of $\\Delta^{n}$ is called the boundary of $\\Delta^{n}$ and is denoted as $\\partial \\Delta^{n}$. The interior of a simplex $\\left( \\Delta^{n} \\right)^{\\circ} := \\Delta^{n} \\setminus \\partial \\Delta^{n}$ is called an open simplex. Let\u0026rsquo;s say a simplicial complex is a complex made up of simplices, specifically forming a CW complex as follows:\nThe definition of $n$:\n$D^{n} \\subset \\mathbb{R}^{n}$ defined as follows is called an $n$-unit disk. $$ D^{n} := \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{n} : \\left\\| \\mathbf{x} \\right\\| \\le 1 \\right\\} $$ A subset $e^{n}$ that is homeomorphic to $D^{n} \\setminus \\partial D^{n}$ is called an $n$-cell. Definition of CW Complex:\nA discrete set $X^{0} \\ne \\emptyset$ is considered as 0-cells. An $n$-skeleton $X^{n}$ is made by attaching $n$-cells $e_{\\alpha}^{n}$ to $X^{n-1}$ using the maps $\\phi_{\\alpha} : S^{n-1} \\to X^{n-1}$. $X := \\bigcup_{n \\in \\mathbb{N}} X^{n}$ becomes a topological space with a weak topology, then $X$ is called a cell complex. Definition Consider a topological space $X$ with a $\\Delta$-complex structure.\nLet\u0026rsquo;s denote the free abelian group with a basis of open $n$-simplices, or $n$-cells $e_{\\alpha}^{n}$ in $X$, as $\\Delta_{n} (X)$. Elements of $\\Delta_{n} (X)$ are called $n$-chains and are represented as formal sums with coefficients $k_{\\alpha} \\in \\mathbb{Z}$ as follows. $$ \\sum_{\\alpha} k_{\\alpha} e_{\\alpha}^{n} $$ Each $n$-cell $e_{\\alpha}^{n}$ corresponds to a characteristic map $\\sigma_{\\alpha} : \\Delta^{n} \\to X$, allowing representation as follows. $$ \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} $$ The boundary homomorphism $\\partial_{n} : \\Delta_{n} (X) \\to \\Delta_{n-1} (X)$ is defined as follows, where $\\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , v_{n} \\right]$ indicates the restriction of $\\sigma_{\\alpha}$ to an $n-1$-simplex in $X$. $$ \\partial _{n} \\left( \\sigma_{\\alpha} \\right) := \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right] $$ The quotient group $\\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ is denoted as $H_{n}^{\\Delta}$, and since $H_{n}^{\\Delta}$ is a homology group, it is called the $n$th simplicial homology group of $X$. The group $0$ is a magma defined on ${ 0 }$, essentially an empty algebraic structure. The homomorphism $\\partial^{2} = 0$ is a zero morphism. $\\text{Im}$ refers to the image. $\\ker$ refers to the kernel. In a set, the notation $\\hat{v}_{i}$ means excluding $v_{i}$, as follows: $$ { v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , v_{n} } := { v_{1} , \\cdots , v_{n} } \\setminus { v_{i} } $$ Explanation The definition section might be overwhelming with its dense text. It\u0026rsquo;s normal if it\u0026rsquo;s not immediately clear. The explanation aims to be thorough and accessible, addressing points that were confusing during my study.\nWhy are elements of $\\Delta_{n} (X)$ called chains? Considering the notation $\\sigma_{\\alpha} : \\Delta^{n} \\to X$, we can abstract away whether $e_{\\alpha}^{n}$ is an element of $\\Delta^{n}$ or $X$. For $n=2$ and all coefficients $k_{\\alpha} = 1$, the geometric representation can be imagined as the figure on the right, denoted as $\\sum_{i=1}^{7} \\sigma_{i}$.\nThe term \u0026ldquo;chain\u0026rdquo; might make sense now, but it\u0026rsquo;s not crucial for understanding. What\u0026rsquo;s important is that the collection of $n$-chains in $\\Delta_{n} (X)$ forms a chain complex.\nIs $\\Delta_{n} (X)$ really a group? It\u0026rsquo;s crucial to note that the \u0026ldquo;formal sum\u0026rdquo; used to describe chains is not an algebraic operation within $\\Delta_{n} (X)$. This notation is merely symbolic. For example, the expression\n2😀 + 💎 - 3🍌\rhas no mathematical meaning as it's unclear what \"twice 😀 plus 💎 minus three 🍌\" would entail. This confusion is similar to the uncertainty in $\\sum\\_{\\alpha} k\\_{\\alpha} e\\_{\\alpha} \\simeq \\sum\\_{\\alpha} k\\_{\\alpha} \\sigma\\_{\\alpha}$ regarding\r- The addition of open simplices $e\\_{\\alpha}^{n}$, which is undefined\r- The interpretation of $\\sigma\\_{\\alpha}$, which is a function\r- The meaning of operations like $-3 e\\_{1}^{n} + 7 e\\_{2}^{n} \\simeq -3 \\sigma\\_{1} + 7 \\sigma\\_{2}$\rThankfully, these concerns are irrelevant to $\\Delta_{n} (X)$. If we define\n$\\sigma=$2😀 + 💎 - 3🍌\ras an $n$-chain in $\\Delta\\_{n} (X)$, its inverse can be defined using the inverses of coefficients $k\\_{\\alpha} \\in (\\mathbb{Z}, +)$, resulting in\r$-\\sigma=$ (-2)😀 + (-1)💎 + 3🍌\rThis definition is sufficient regardless of the specific structure of $\\Delta_{n} (X)$. The identity element of $\\Delta_{n} (X)$ can be defined as $0 := \\sigma + (-\\sigma)$, and since $\\mathbb{Z}$ is an abelian group, so is $\\Delta_{n} (X)$. The operation $+$ in $(\\Delta_{n} (X), +)$ is induced from $(\\mathbb{Z}, +)$ but is distinct, and $\\Delta_{n} (X)$ is a free abelian group, with $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ now being an algebraic sum.\nIn summary:\nThe initial definition\u0026rsquo;s appearance of addition in $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ was merely notational, not an operation. The $+$ in $(\\Delta_{n} (X), +)$ is derived from $(\\mathbb{Z}, +)$ but is not the same. $(\\Delta_{n} (X), +)$ is a free abelian group, and $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ is now an algebraic sum. Why is $\\partial$ called the boundary? The definition of $\\partial_{n}$ may seem abstract, but the following illustration clarifies its meaning.\nFor example, for $\\partial_{2}$, we can perform the following calculation. $$ \\begin{align*} \u0026amp; \\partial _{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\\\ =\u0026amp; \\sum_{i=0}^{2} (-1)^{i} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\setminus \\left[ v_{i} \\right] \\\\ =\u0026amp; (-1)^{0} \\left[ v_{1}, v_{2} \\right] + (-1)^{1} \\left[ v_{0}, v_{2} \\right] + (-1)^{2} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\end{align*} $$\nIf you\u0026rsquo;re studying homology, it\u0026rsquo;s generally accepted that the boundary of a triangle $\\left[ v_{0} ,v_{1}, v_{2} \\right]$ consists of the segments $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$. The real challenge is understanding what $\\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right]$ means. How can segments be subtracted? And how about operations on 2-simplices like triangles?\nThese questions miss the point. Refocusing, $\\partial_{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\in \\Delta_{1} (X)$ is simply a formal sum of the three elements $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$. $$ (+1) \\left[ v_{1}, v_{2} \\right] + (-1) \\left[ v_{0}, v_{2} \\right] + (+1) \\left[ v_{0}, v_{1} \\right] $$\nDenoting these as $$ \\begin{align*} a := \\left[ v_{1}, v_{2} \\right] \\ b:= \\left[ v_{0}, v_{2} \\right] \\ c:= \\left[ v_{0} , v_{1} \\right] \\end{align*} $$ reveals the nature of $\\Delta_{1} (X)$. For example, a $1$-chain $x \\in \\Delta_{1} (X)$ can be represented with coefficients $k_{a} , k_{b} , k_{c} \\in \\mathbb{Z}$ as $$ x = k_{a} a + k_{b} b + k_{c} c $$\nViewing from the perspective of $a,b,c$, the free group $\\Delta_{1} (X) := F[{ a,b,c }]$ is constructed, essentially equivalent to $\\mathbb{Z}^{3} \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} \\oplus \\mathbb{Z}$.\nThis shift in perspective is crucial for understanding subsequent examples. We must think algebraically rather than geometrically.\nExamples Consider the following scenario: $$ \\begin{align*} \\\\ \\partial_{n} :\u0026amp; \\Delta_{n} (X) \\to \\Delta_{n-1} (X) \\\\ H_{n}^{\\Delta} (X) =\u0026amp; \\ker \\partial_{n} / \\text{Im} \\partial_{n+1} \\end{align*} $$\nFor $n = 0$, $\\partial_{0} : \\Delta_{0} (X) \\to 0$ implies $\\ker \\partial_{0} = \\Delta_{0} (X)$.\nCircle $S^{1}$ For a circle $X = S^{1}$, there\u0026rsquo;s one 0-simplex (vertex $v$), one 1-simplex (edge $e$), and no $n$-simplices for $n \\ge 2$. The chain complex is structured as follows: $$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{1}\\left( S^{1} \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( S^{1} \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\n$\\Delta_{1}(S^{1})$, being generated solely by $e$, is isomorphic to $\\mathbb{Z}$, and similarly, $\\Delta_{0}(S^{1})$ is isomorphic to $\\mathbb{Z}$ due to being generated by $v$ alone. Since $\\partial_{1}$ is a zero morphism: $$ \\partial e = v - v = 0 $$\nFor $n = 0$, $\\ker \\partial_{0} = \\Delta_{0} (S^{1})$, and since $\\partial_{1}$ is a zero morphism, its image is ${ 0 }$, leading to: $$ \\begin{align*} H_{0}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\nFor $n = 1$, $\\text{Im} \\partial_{2} = { 0 }$ since $\\partial_{1}$ is a zero morphism, and $\\ker \\partial_{1} = \\Delta_{1} (S^{1})$, resulting in: $$ \\begin{align*} H_{1}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{1} / \\text{Im} \\partial_{2} \\\\ \\simeq\u0026amp; \\Delta_{1} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\nFor $n \\ge 2$, $H_{n}^{\\Delta} (S_{1}) \\simeq 0$, summarizing as: $$ H_{n}^{\\Delta} \\left( S_{1} \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0, 1 \\\\ 0 \u0026amp; , \\text{if } n \\ge 2 \\end{cases} $$\nTorus $T^{2}$ Considering a torus $T^{2}$ as in the image, there\u0026rsquo;s one 0-simplex (vertex $v$), three 1-simplices (edges $a$, $b$, $c$), two 2-simplices ($U$, $L$), and no $n$-simplices for $n \\ge 3$. The chain complex is organized as follows: $$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{2}\\left( T \\right) \\overset{\\partial_{2}}{\\longrightarrow} \\Delta_{1}\\left( T \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( T \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\nHence, the free groups $\\Delta_{n} (T)$ are: $$ \\Delta_{n} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z}^{1} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z}^{3} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z}^{2} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\nSince the edges $a$, $b$, $c$ connect to vertex $v$ at both ends: $$ \\begin{align*} \\partial a =\u0026amp; v - v = 0 \\\\ \\partial b =\u0026amp; v - v = 0 \\\\ \\partial c =\u0026amp; v - v = 0 \\end{align*} $$ and $\\partial_{1}$ is a zero morphism, similar to the circle case.\nFor $n = 0$, the situation mirrors that of the circle: $$ \\begin{align*} H_{0}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( T \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\nFor $n = 1$, since $\\partial_{1}$ is a zero morphism, $\\ker \\partial_{1} = \\Delta_{1} (T)$. The boundary homomorphism $\\partial_{2} : \\Delta_{2}(T) \\to \\Delta_{1}(T)$ yields: $$ \\partial_{2} U = a + b - c = \\partial_{2} L $$ and since ${ a, b, a + b - c }$ is a basis for $\\Delta_{1}(T)$, $H_{1}^{\\Delta}$ is isomorphic to the free group generated by $a$ and $b$, resulting in: $$ H_{1}^{\\Delta} \\left( T \\right) \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} $$\nFor $n = 2$, $\\text{Im} \\partial_{3} = { 0 }$ and considering the dimensions of $\\Delta_{2}(T)$ and $\\Delta_{1}(T)$, we get: $$ \\begin{align*} H_{2}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{2} / \\text{Im} \\partial_{3} \\\\ \\simeq\u0026amp; \\mathbb{Z}^{3-2} / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\nFor $n \\ge 3$, $H_{n}^{\\Delta} (T) \\simeq 0$, summarizing as: $$ H_{n}^{\\Delta} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z} \\oplus \\mathbb{Z} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\nTheorem $H_{n}^{\\Delta}$ is a homology group Definition of a homology group:\nLet $n \\in \\mathbb{N}_{0}$. A sequence of abelian groups $C_{n}$ and homomorphisms $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ forming a chain $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ that satisfies $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ for all $n$ is called a chain complex. The quotient group $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ is called the $n$th homology group of the complex. The homomorphism $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ is called the boundary or differential operator. For the chain complex ${ (\\Delta_{n} (X), \\partial_{n}) }_{n=0}^{\\infty}$, $H_{n}^{\\Delta} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ is a homology group. That is, $\\partial_{n} \\circ \\partial_{n+1}$ is a zero morphism for all $n \\in \\mathbb{N}$.\nProof Applying $\\partial_{n-1} \\circ \\partial_{n}$ to $\\sigma \\in \\Delta_{n}$ yields: $$ \\begin{align*} \u0026amp; \\left( \\partial_{n-1} \\circ \\partial_{n} \\right) \\left( \\sigma \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\partial_{n} \\left( \\sigma \\right) \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , v_{n} \\right] \\right) \\\\ =\u0026amp; \\sum_{j \u0026lt; i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ \u0026amp; + \\left( -1 \\right) \\sum_{j \u0026gt;i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\nSuch proofs are often more illuminating with specific examples rather than generalizations. $$ \\begin{align*} \u0026amp; \\partial_{1} \\left( \\partial_{2} \\left[ v_{0}, v_{1} , v_{2} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left( \\left[ v_{1} , v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left[ v_{1} , v_{2} \\right] - \\partial_{1} \\left[ v_{0}, v_{2} \\right] + \\partial_{1} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{2} \\right] - \\left[ v_{1} \\right] - \\left( \\left[ v_{2} \\right] - \\left[ v_{0} \\right] \\right) + \\left[ v_{1} \\right] - \\left[ v_{0} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\n■\n","id":2383,"permalink":"https://freshrimpsushi.github.io/en/posts/2383/","tags":null,"title":"Definition of Simplicial Homology Group"},{"categories":"줄리아","contents":"Overview Broadcasting is one of the most important concepts in Julia, offering a convenient syntax for writing vectorized code1. It is used by placing a dot . before a binary operation or after a function. This represents the application of a function in a pointwise manner, which is a perfect expression of its purpose.\nFrom a programming perspective, broadcasting can be viewed as a simplification of using Map in Map and Reduce.\nCode Binary Operations For binary operations, we place a . before them. For example, to add a scalar $a \\in \\mathbb{R}$ to every element of a matrix $A \\in \\mathbb{Z}_{9}^{3 \\times 4}$, we use the following code.\njulia\u0026gt; A = rand(0:9, 3,4)\r3×4 Matrix{Int64}:\r5 6 3 3\r7 4 8 8\r0 2 2 7\rjulia\u0026gt; a = rand()\r0.23234165065465284\rjulia\u0026gt; A .+ a\r3×4 Matrix{Float64}:\r5.23234 6.23234 3.23234 3.23234\r7.23234 4.23234 8.23234 8.23234\r0.232342 2.23234 2.23234 7.23234 General Functions julia\u0026gt; f(x) = x^2 - 1\rf (generic function with 3 methods)\rjulia\u0026gt; f(a)\r-0.9460173573710713 Consider the function $f : \\mathbb{R} \\to \\mathbb{R}$, for example. Since this is a scalar function, it calculates well for $a \\in \\mathbb{R}$ as shown above.\njulia\u0026gt; f(A)\rERROR: LoadError: DimensionMismatch However, if we attempt to input a matrix $A$, we encounter a LoadError. Upon reflection, the very concept of squaring a matrix, especially a rectangular matrix like $A \\in \\mathbb{Z}_{9}^{3 \\times 4}$, is ambiguous, thus we cannot simply input it into a function like $f(x) = x^{2} - 1$. Yet, if what we desire is a matrix derived from taking the square of each value in matrix $A$ and then subtracting $1$, by placing a dot . as in f., we can apply the function $f : \\mathbb{R} \\to \\mathbb{R}$ to every element of the matrix.\njulia\u0026gt; f.(A)\r3×4 Matrix{Int64}:\r24 35 8 8\r48 15 63 63\r-1 3 3 48 Speed Comparison In many cases, broadcasting is also superior in terms of performance. However, when evaluating performance based on speed, there are some nuances to be aware of, which are crucial to understand as indicated below.\nFor instance, the following code squares the numbers from 1 to 100,000.\njulia\u0026gt; @time for x in 1:100000\rsqrt(x)\rend\r0.000001 seconds\rjulia\u0026gt; @time sqrt.(1:100000);\r0.000583 seconds (2 allocations: 781.297 KiB) Comparing simple speed, broadcasting is about 500 times slower than a for loop. However, this benchmark, derived from simple calculations, changes if we include processes such as storage ― then the story differs.\njulia\u0026gt; z = []\rAny[]\rjulia\u0026gt; @time for x in 1:100000\rpush!(z, sqrt(x))\rend\r0.005155 seconds (100.01 k allocations: 3.353 MiB)\rjulia\u0026gt; @time y = sqrt.(1:100000);\r0.000448 seconds (2 allocations: 781.297 KiB) Whether or not the process includes storage, the code with broadcasting remains unchanged. However, for a loop that needs to add values to an empty array, it\u0026rsquo;s about 10 times slower than vectorized code. This could be attributed to the cost of handling a dynamic array with push!() rather than sqrt() itself, but in any case, broadcasting turns out to be faster. Naturally, there are ways to make loops faster (for example, changing Any[] to Float64[] would help), but in most coding scenarios encountered in reality, using broadcasting is not only more convenient but also superior in speed.\nThis goes beyond merely conceptual issues, also relating to Julia being closer to a compiled language rather than an interpreted one1. If you were a compiler, wouldn\u0026rsquo;t you prefer compiling for a vector, whose type and size are specifically defined, over a for loop, where what happens next is uncertain?\nIt\u0026rsquo;s safe to say that in about 99% of functions, using the Julia developers\u0026rsquo; designed method is faster than crafting loops ourselves. There\u0026rsquo;s no need to force vectorization of code, but when it can be vectorized, it\u0026rsquo;s overwhelmingly\u0026hellip; indeed overwhelmingly faster. This is not unique to Julia, but is a characteristic of languages specialized in vector operations like Matlab, R. However, Julia sets itself apart by embracing the paradigm of functional programming while confidently asserting its speed advantage.\nEnvironment OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/arrays/#Broadcasting\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2382,"permalink":"https://freshrimpsushi.github.io/en/posts/2382/","tags":null,"title":"Julia's Broadcasting Syntax"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 The following vector-valued Boolean function is referred to as a Fredkin GateFredkin Gate.\n$$ F : \\left\\{ 0, 1 \\right\\}^{3} \\to \\left\\{ 0, 1 \\right\\}^{3} $$\n$$ F (a, b, c) = \\Big(a, (\\lnot a \\land b) \\lor (a \\land c), (\\lnot a \\land c) \\lor (a \\land b) \\Big) $$\n$\\text{CSWAP}$ GateControlled SWAP (CSWAP) Gate is also known as. Description Introduced by Edward FredkinEdward Fredkin, the Fredkin Gate swaps the remaining two values without altering the first input, if the first input is $1$. The specific computation is as follows.\n$$ \\begin{align*} F (0,0,0) \u0026amp;= (0, (\\lnot 0 \\land 0) \\lor (0 \\land 0), (\\lnot 0 \\land 0) \\lor (0 \\land 0)) = (0, 0 \\lor 0, 0 \\lor 0) = (0, 0, 0) \\\\ F (0,0,1) \u0026amp;= (0, (\\lnot 0 \\land 0) \\lor (0 \\land 1), (\\lnot 0 \\land 1) \\lor (0 \\land 0)) = (0, 0 \\lor 0, 1 \\lor 0) = (0, 0, 1) \\\\ F (0,1,0) \u0026amp;= (0, (\\lnot 0 \\land 1) \\lor (0 \\land 0), (\\lnot 0 \\land 0) \\lor (0 \\land 1)) = (0, 1 \\lor 0, 0 \\lor 0) = (0, 1, 0) \\\\ F (0,1,1) \u0026amp;= (0, (\\lnot 0 \\land 1) \\lor (0 \\land 1), (\\lnot 0 \\land 1) \\lor (0 \\land 1)) = (0, 1 \\lor 0, 1 \\lor 0) = (0, 1, 1) \\\\ F (1,0,0) \u0026amp;= (1, (\\lnot 1 \\land 0) \\lor (1 \\land 0), (\\lnot 1 \\land 0) \\lor (1 \\land 0)) = (1, 0 \\lor 0, 0 \\lor 0) = (1, 0, 0) \\\\ F (1,0,1) \u0026amp;= (1, (\\lnot 1 \\land 0) \\lor (1 \\land 1), (\\lnot 1 \\land 1) \\lor (1 \\land 0)) = (1, 0 \\lor 1, 0 \\lor 0) = (1, 1, 0) \\\\ F (1,1,0) \u0026amp;= (1, (\\lnot 1 \\land 1) \\lor (1 \\land 0), (\\lnot 1 \\land 0) \\lor (1 \\land 1)) = (1, 0 \\lor 0, 0 \\lor 1) = (1, 0, 1) \\\\ F (1,1,1) \u0026amp;= (1, (\\lnot 1 \\land 1) \\lor (1 \\land 1), (\\lnot 1 \\land 1) \\lor (1 \\land 1)) = (1, 0 \\lor 1, 0 \\lor 1) = (1, 1, 1) \\\\ \\end{align*} $$\nFrom the table, it is evident that $F$ is a reversible function and that composing $F$ twice results in an identity function.\n$$ \\operatorname{Id} = F \\circ F $$\nFurthermore, since $\\left\\{ F \\right\\}$ is functionally complete, $F$ is a universal gate.\nBoolean Function\rSymbol\r$F$\rTruth Table\rInput\rOutput\r$a$\r$b$\r$c$\r$a$\r$ (\\lnot a \\land b) \\lor (a \\land c)$\r$ (\\lnot a \\land c) \\lor (a \\land b)$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\rSummary Assuming unrestricted use of projection and injection, the Fredkin Gate $F$ is a universal gate. In other words, $\\left\\{ F \\right\\}$ is functionally complete.\nProof Theorem\nThe set of $\\text{NOT}$ Gate and $\\text{AND}$ Gate, $\\left\\{ \\lnot, \\land \\right\\}$, is functionally complete.\nFollowing the theorem, demonstrating that projection, injection, and $F$ can appropriately represent $\\text{NOT}$ and $\\text{AND}$ concludes the proof.\n$\\text{NOT}$ Gate\n$$ \\lnot = p_{0} \\circ p_{1} \\circ F \\circ \\jmath_{2} \\circ \\imath_{1} $$\nHolds true. Since $\\jmath_{2} \\circ \\imath_{1} (a) = (a, 0, 1)$, the following is obtained:\n$$ \\begin{equation} F \\circ \\jmath_{2} \\circ \\imath_{1}(a) = F(a, 0, 1) = (a, a, \\lnot a) \\end{equation} $$\nTo eliminate the first two values, take $p_{0} \\circ p_{1}$, resulting in:\n$$ p_{0} \\circ p_{1} \\circ F \\circ \\jmath_{2} \\circ \\imath_{1} (a) = p_{0} \\circ p_{1} (a, a, \\lnot a) = \\lnot a $$\n※ Additionally, applying $p_{2}$ to $(1)$ yields a duplication function.\n$\\text{AND}$ Gate\n$$ \\land = p_{0} \\circ p_{1} \\circ F \\circ \\imath_{2} $$\nHolds true. Initially, $F \\circ \\jmath_{2} (a, b)$ is as follows:\n$$ \\begin{align*} F \\circ \\jmath_{2} (a, b) = F(a, b, 0) \u0026amp;= (a, (\\lnot a \\land b) \\lor (a \\land 0), (\\lnot a \\land 0) \\lor (a \\land b)) \\\\ \u0026amp;= (a, (\\lnot a \\land b) \\lor 0, 0 \\lor (a \\land b)) \\\\ \u0026amp;= (a, \\lnot a \\land b, a \\land b) \\end{align*} $$\nTherefore, taking $p_{0} \\circ p_{1}$ results in:\n$$ p_{0} \\circ p_{1} \\circ F \\circ \\imath_{2} (a, b) = p_{0} \\circ p_{1} (a, \\lnot a \\land b, a \\land b) = a \\land b $$\n■\nSee Also $\\text{AND}$ GateAND $\\text{OR}$ GateOR $\\text{NOT}$ GateNOT $\\text{XOR}$ GateXOR $\\text{NAND}$ GateNAND $\\text{NOR}$ GateNOR $\\operatorname{CNOT}$ Gate Toffoli Gate$\\text{CCNOT}$ Gate Kim Young-Hoon \u0026amp; Heo Jae-Sung, Quantum Information Theory (2020), p90-93\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3412,"permalink":"https://freshrimpsushi.github.io/en/posts/3412/","tags":null,"title":"Fredkin/CSWAP Gate"},{"categories":"양자정보이론","contents":"Definition1 The following vector-valued Boolean function is called a Toffoli gateToffoli gate.\n$$ T : \\left\\{ 0, 1 \\right\\}^{3} \\to \\left\\{ 0, 1 \\right\\}^{3} $$\n$$ T (a, b, c) = (a, b, (a \\land b) \\oplus c) $$\nThe $\\text{CCNOT}$ gateControlled Controlled NOT(CCNOT) gate is also known as. Description In the Toffoli gate, if the first two inputs are both $1$, the third input is inverted. In all other cases, the input and output are the same. The specific calculation is as follows.\n$$ \\begin{align*} T (0,0,0) \u0026amp;= (0, 0, (0 \\land 0) \\oplus 0) = (0, 0, 0 \\oplus 0) = (0, 0, 0) \\\\ T (0,0,1) \u0026amp;= (0, 0, (0 \\land 0) \\oplus 1) = (0, 0, 0 \\oplus 1) = (0, 0, 1) \\\\ T (0,1,0) \u0026amp;= (0, 1, (0 \\land 1) \\oplus 0) = (0, 1, 0 \\oplus 0) = (0, 1, 0) \\\\ T (0,1,1) \u0026amp;= (0, 1, (0 \\land 1) \\oplus 1) = (0, 1, 0 \\oplus 1) = (0, 1, 1) \\\\ T (1,0,0) \u0026amp;= (1, 0, (1 \\land 0) \\oplus 0) = (1, 0, 0 \\oplus 0) = (1, 0, 0) \\\\ T (1,0,1) \u0026amp;= (1, 0, (1 \\land 0) \\oplus 1) = (1, 0, 0 \\oplus 1) = (1, 0, 1) \\\\ T (1,1,0) \u0026amp;= (1, 1, (1 \\land 1) \\oplus 0) = (1, 1, 1 \\oplus 0) = (1, 1, 1) \\\\ T (1,1,1) \u0026amp;= (1, 1, (1 \\land 1) \\oplus 1) = (1, 1, 1 \\oplus 1) = (1, 1, 0) \\\\ \\end{align*} $$\nFrom the table above, it\u0026rsquo;s easy to see that $T$ is a reversible function and that applying $T$ twice composes to an identity function.\n$$ \\operatorname{Id} = T \\circ T $$\nFurthermore, since $\\left\\{ T \\right\\}$ is functionally complete, $T$ is a universal gate.\n부울 함수\r기호\r진리표\r$T$\r입력\r출력\r$a$\r$b$\r$c$\r$a$\r$b$\r$(a \\land b) \\oplus c$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$0$\rSummary (Assuming unrestricted use of projection and injection), the Toffoli gate $T$ is a universal gate. In other words, $\\left\\{ T \\right\\}$ is functionally complete.\nProof Theorem\nIf duplication functions are allowed, $\\left\\{ \\uparrow \\right\\}$ is functionally complete. In other words, the $\\text{NAND}$ gate $\\uparrow$ is a universal gate.\nAccording to the theorem above, showing that projection, injection, and $T$ can be used appropriately to express the duplication function $\\text{cl}$ and the $\\text{NAND}$ gate completes the proof.\nDuplication Function\n$$ \\operatorname{cl} = p_{1} \\circ T \\circ \\imath_{2} \\circ \\jmath_{1} $$\nHolds. First, calculating $T \\circ \\imath_{2} \\circ \\jmath_{1} (a)$ gives the following.\n$$ T \\circ \\imath_{2} \\circ \\jmath_{1} (a) = T \\circ \\imath_{2} (a, 1) = T (a, 1, 0) $$\nHere, if $a = 1$, then $T(1, 1, 0) = (1, 1, 1)$; if $a = 0$, then $T(0, 1, 0) = (0, 1, 0)$, so the following holds.\n$$ T(a, 1, 0) = (a, 1, a) $$\nTherefore, by taking $p_{1}$,\n$$ p_{1} \\circ T \\circ \\imath_{2} \\circ \\jmath_{1} (a) = p_{1} (a, 1, a) = (a, a) = \\operatorname{cl}(a) $$\n$\\text{NAND}$ Gate\n$$ p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} = \\uparrow \\\\ p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} (a, b) = a \\uparrow b $$\nHolds. Calculating in order gives the following.\n$$ \\begin{align*} p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} (a, b) \u0026amp;= p_{0} \\circ p_{1} \\circ T (a, b, 1) \\\\ \u0026amp;= p_{0} \\circ p_{1} (a, b, (a \\land b) \\oplus 1) \\\\ \u0026amp;= p_{0} (a, (a \\land b) \\oplus 1) \\\\ \u0026amp;= (a \\land b) \\oplus 1 \\\\ \u0026amp;= \\lnot(a \\land b) = a \\uparrow b \\end{align*} $$\nThe last line holds due to the properties of the $\\text{XOR}$ gate.\n■\nKim Young-hoon and Heo Jae-seong, Quantum Information Theory (2020), pp. 89-93\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3411,"permalink":"https://freshrimpsushi.github.io/en/posts/3411/","tags":null,"title":"Toffoli/CCNOT Gate"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 The following vector-valued Boolean function is called the $\\operatorname{CNOT}$ gateControlled NOT (CNOT) gate.\n$$ \\operatorname{CNOT} : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\}^{2} $$\n$$ \\operatorname{CNOT} (a,b) = (a, a \\oplus b) $$\nIt is also known as the Feynman gate.Feynman gate 2 Description The specific calculations of the input and output of the $\\operatorname{CNOT}$ gate are as follows.\n$$ \\begin{align*} \\operatorname{CNOT} (0,0) \u0026amp;= (0, 0 \\oplus 0) = (0, 0) \\\\ \\operatorname{CNOT} (0,1) \u0026amp;= (0, 0 \\oplus 1) = (0, 1) \\\\ \\operatorname{CNOT} (1,0) \u0026amp;= (1, 1 \\oplus 0) = (1, 1) \\\\ \\operatorname{CNOT} (1,1) \u0026amp;= (1, 1 \\oplus 1) = (1, 0) \\end{align*} $$\nLooking at the table above, it is easy to see that $\\operatorname{CNOT}$ is a reversible function and that composing $\\operatorname{CNOT}$ twice results in an identity function.\n$$ \\operatorname{Id} = \\operatorname{CNOT} \\circ \\operatorname{CNOT} $$\nIf only the second value of the output is considered, it is similar to the $\\text{XOR}$ gate, hence it is also referred to as the reversible $\\text{XOR}$ gate..\n부울 함수\r기호\r진리표\r$\\operatorname{CNOT}$\r입력\r출력\r$a$\r$b$\r$a$\r$a \\oplus b$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$0$\rKim Young-hoon and Heo Jae-seong, Quantum Information Theory (2020), pp. 88-89\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Controlled_NOT_gate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3410,"permalink":"https://freshrimpsushi.github.io/en/posts/3410/","tags":null,"title":"Controlled NOT(CNOT) Gate"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 The following Boolean function is called the $\\text{NOR}$ gateNOR gate or negated logical sum and is denoted as follows.\n$$ \\downarrow : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\downarrow 0 = 1,\\quad 0\\downarrow 1 = 0,\\quad 1\\downarrow 0 = 0,\\quad 1\\downarrow 1 = 0 $$\nDescription It is a composition of the $\\text{NOT}$ gate and the $\\text{OR}$ gate, and it is named $\\text{NOR}$ by borrowing $\\text{N(OT)}$ and $\\text{OR}$.\n$$ \\begin{equation} \\downarrow = \\lnot \\circ \\lor \\end{equation} $$\n$$ a \\downarrow b = \\lnot (a \\lor b) $$\nIt operates opposite to the $\\text{OR}$ gate and produces a true output only when all inputs are false. Furthermore, $\\left\\{ \\downarrow \\right\\}$ is functionally complete, which can be seen as obvious due to $(1)$.\n부울 함수\r기호\r진리표\r$\\text{NOR}$\r$a$\r$b$\r$a \\downarrow b$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\rSummary If the replication function is allowed, then $\\left\\{ \\downarrow \\right\\}$ is functionally complete. In other words, $\\downarrow$ is a universal gate.\nProof Theorem\nThe set of $\\text{NOT}$ and $\\text{OR}$ gates, $\\left\\{ \\lnot, \\lor \\right\\}$, is functionally complete.\nAccording to the above theorem, it suffices to show that the $\\text{NOT}$ gate and the $\\text{OR}$ gate can be made only with the replication function $\\text{cl}$ and $\\downarrow$.\n$\\text{NOT}$ gate\n$$ \\lnot = \\downarrow \\circ \\operatorname{cl} \\\\ \\lnot a = a \\downarrow a $$\nholds.\n$$ \\begin{align*} \\downarrow \\circ \\operatorname{cl}(0) = 0 \\downarrow 0 = 1 = \\lnot 0 \\\\ \\downarrow \\circ \\operatorname{cl}(1) = 1 \\downarrow 1 = 0 = \\lnot 1 \\\\ \\end{align*} $$\n$\\text{OR}$ gate\n$$ \\lor = \\downarrow \\circ \\operatorname{cl} \\circ \\downarrow \\\\ a \\lor b = (a \\downarrow b) \\downarrow (a \\downarrow b) $$\nholds.\n$$ \\begin{align*} (0 \\downarrow 0) \\downarrow (0 \\downarrow 0) = (1 \\downarrow 1) = 0 = 0 \\lor 0 \\\\ (0 \\downarrow 1) \\downarrow (0 \\downarrow 1) = (0 \\downarrow 0) = 1 = 0 \\lor 1 \\\\ (1 \\downarrow 0) \\downarrow (1 \\downarrow 0) = (0 \\downarrow 0) = 1 = 1 \\lor 0 \\\\ (1 \\downarrow 1) \\downarrow (1 \\downarrow 1) = (1 \\downarrow 1) = 1 = 1 \\lor 1 \\\\ \\end{align*} $$\n■\nKim Young-hoon and Heo Jae-seong, Quantum Information Theory (2020), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3407,"permalink":"https://freshrimpsushi.github.io/en/posts/3407/","tags":null,"title":"NOR Gate"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 The following Boolean function is called a $\\text{NAND}$ GateNAND gate or Negated Logical Product and is denoted as follows.\n$$ \\uparrow : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\uparrow 0 = 1,\\quad 0\\uparrow 1 = 1,\\quad 1\\uparrow 0 = 1,\\quad 1\\uparrow 1 = 0 $$\nDescription It is a composition of the $\\text{NOT}$ Gate and $\\text{AND}$ Gate, and is named $\\text{NAND}$ by adopting $\\text{N(OT)}$ and $\\text{AND}$.\n$$ \\begin{equation} \\uparrow = \\lnot \\circ \\land \\end{equation} $$\n$$ a \\uparrow b = \\lnot (a \\land b) $$\nIt operates opposite to the $\\text{AND}$ Gate, outputting false only when all inputs are true. Additionally, $\\left\\{ \\uparrow \\right\\}$ is functionally complete, which is considered obvious due to $(1)$.\n부울 함수\r기호\r진리표\r$\\text{NAND}$\r$a$\r$b$\r$a \\uparrow b$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\rSummary If the replication function is allowed, $\\left\\{ \\uparrow \\right\\}$ is functionally complete. In other words, $\\uparrow$ is a universal gate.\nProof Theorem\nThe set of $\\text{NOT}$ and $\\text{AND}$ Gates, $\\left\\{ \\lnot, \\land \\right\\}$, is functionally complete.\nAccording to the above theorem, it is sufficient to show that the $\\text{NOT}$ Gate and the $\\text{AND}$ Gate can be created solely using the replication function $\\text{cl}$ and $\\uparrow$.\n$\\text{NOT}$ Gate\n$$ \\lnot = \\uparrow \\circ \\operatorname{cl} \\\\ \\lnot a = a \\uparrow a $$\nholds.\n$$ \\begin{align*} \\uparrow \\circ \\operatorname{cl}(0) = 0 \\uparrow 0 = 1 = \\lnot 0 \\\\ \\uparrow \\circ \\operatorname{cl}(1) = 1 \\uparrow 1 = 0 = \\lnot 1 \\\\ \\end{align*} $$\n$\\text{AND}$ Gate\n$$ \\land = \\uparrow \\circ \\operatorname{cl} \\circ \\uparrow \\\\ a \\land b = (a \\uparrow b) \\uparrow (a \\uparrow b) $$\nholds.\n$$ \\begin{align*} (0 \\uparrow 0) \\uparrow (0 \\uparrow 0) = (1 \\uparrow 1) = 0 = 0 \\land 0 \\\\ (0 \\uparrow 1) \\uparrow (0 \\uparrow 1) = (0 \\uparrow 0) = 0 = 0 \\land 1 \\\\ (1 \\uparrow 0) \\uparrow (1 \\uparrow 0) = (0 \\uparrow 0) = 0 = 1 \\land 0 \\\\ (1 \\uparrow 1) \\uparrow (1 \\uparrow 1) = (1 \\uparrow 1) = 1 = 1 \\land 1 \\\\ \\end{align*} $$\n■\nKim Young-Hoon \u0026amp; Heo Jae-Seong, Quantum Information Theory (2020), pp. 86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3406,"permalink":"https://freshrimpsushi.github.io/en/posts/3406/","tags":null,"title":"NAND Gate"},{"categories":"줄리아","contents":"Code 1 julia\u0026gt; Dict([\u0026#34;a\u0026#34;, \u0026#34;bc\u0026#34;] .=\u0026gt; [2,8])\rDict{String, Int64} with 2 entries:\r\u0026#34;a\u0026#34; =\u0026gt; 2\r\u0026#34;bc\u0026#34; =\u0026gt; 8 Given two arrays you want to use as keys and values, you can create a dictionary using Dict(Key .=\u0026gt; Value). Essentially, it\u0026rsquo;s nothing more than broadcasting the =\u0026gt; operator to create pairs.\nEnvironment OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/create-a-dictionary-from-arrays-of-keys-and-values/13908/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2375,"permalink":"https://freshrimpsushi.github.io/en/posts/2375/","tags":null,"title":"Creating Dictionaries from Arrays in Julia"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 The following Boolean function is referred to as a $\\text{XOR}$ gateXOR gate or exclusive disjunctionexclusive disjunction/or and is denoted as follows:\n$$ \\oplus : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\oplus 0 = 0,\\quad 0\\oplus 1 = 1,\\quad 1\\oplus 0 = 1,\\quad 1\\oplus 1 = 0 $$\nExplanation The $\\text{XOR}$ gate returns true when only one of the two truth values is true, i.e., when the number of true values is odd. In other words, it returns $0$ if the two values are the same and $1$ if they are different, making it useful for implementing a function to compare if two values are the same.\nThe period between 1974 and 1980, marked by the critique that \u0026ldquo;Perceptrons cannot solve the $\\text{XOR}$ problem,\u0026rdquo; leading to a stagnation in AI development, is referred to as the AI winterAI winter.\n부울 함수\r기호\r진리표\r$\\text{XOR}$\r$a$\r$b$\r$a \\oplus b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\rProperties This can be expressed with $\\text{NOT}$ gate, $\\text{AND}$ gate, and $\\text{OR}$ gate.\n$$ \\begin{align*} a \\oplus b \u0026amp;= (a \\land \\lnot b) \\lor (\\lnot a \\land b) \\\\ \u0026amp;= (a \\lor b) \\land (\\lnot a \\lor \\lnot b) \\\\ \u0026amp;= (a \\lor b) \\land \\lnot (a \\land b) \\end{align*} $$\n$a \\oplus 1 = \\lnot a$ is valid.\n$a \\oplus 0 = a$ is valid.\nKim Young-hoon·Heo Jae-seong, Quantum Information Theory (2020), p85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3405,"permalink":"https://freshrimpsushi.github.io/en/posts/3405/","tags":null,"title":"Exclusive Disjuction, XOR Gate"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 A Boolean function like the following is called a $\\text{NOT}$ gateNOT gate or logical negationnegation and is denoted as follows.\n$$ \\lnot : \\left\\{ 0, 1 \\right\\} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ \\lnot 0 = 1,\\quad \\lnot 1 = 0 $$\nDescription The $\\text{NOT}$ gate returns the opposite of the input.\n부울 함수\r기호\r진리표\r$\\text{NOT}$\r$a$\r$\\lnot a$\r$0$\r$1$\r$1$\r$0$\rKim Young-hoon and Heo Jae-seong, Quantum Information Theory (2020), p84-85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3404,"permalink":"https://freshrimpsushi.github.io/en/posts/3404/","tags":null,"title":"Negation, NOT Gate"},{"categories":"줄리아","contents":"Overview In Julia, complex numbers are natively supported, similar to R.\nCode Imaginary unit im julia\u0026gt; z = 3 + 4im\r3 + 4im im represents the pure imaginary unit $i = \\sqrt{-1}$. All the common arithmetic operations that we are familiar with can be used.\njulia\u0026gt; typeof(z)\rComplex{Int64}\rjulia\u0026gt; typeof(3.0 + 4.0im)\rComplexF64 (alias for Complex{Float64}) When checking the type, even though it\u0026rsquo;s the same complex number, the type of complex number it consists of differs. Similar to how in abstract algebra, it is distinguished whether it is an integer, in the case of $\\mathbb{Z} [i]$, or a real number, in the case of $\\mathbb{R} [i]$.\nReal and imaginary parts real(), imag() julia\u0026gt; real(z)\r3\rjulia\u0026gt; imag(z)\r4 Conjugate and modulus conj(), abs() julia\u0026gt; conj(z)\r3 - 4im\rjulia\u0026gt; abs(z)\r5.0 Note that here, the modulus abs() is not specifically redefined for complex numbers but is used in its general sense of absolute value. Julia has polymorphism, which allows for this design to be naturally well-implemented.\nGeneral complex functions julia\u0026gt; cos(z)\r-27.034945603074224 - 3.851153334811777im\rjulia\u0026gt; log(z)\r1.6094379124341003 + 0.9272952180016122im As expected, just like with absolute values, trigonometric functions and logarithmic functions are well defined for complex numbers $\\mathbb{C}$ and can be used directly in Julia without any special manipulation.\nComplete Code z = 3 + 4im\rreal(z)\rimag(z)\rconj(z)\rabs(z)\rcos(z)\rlog(z) Environment OS: Windows julia: v1.7.0 ","id":2373,"permalink":"https://freshrimpsushi.github.io/en/posts/2373/","tags":null,"title":"How to Use Complex Numbers in Julia"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 The following Boolean function is called $\\text{OR}$ GateOR gate or Disjunctiondisjunction, and it is denoted as follows.\n$$ \\lor : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\lor 0 = 0,\\quad 0\\lor 1 = 1,\\quad 1\\lor 0 = 1,\\quad 1\\lor 1 = 1 $$\nDescription $\\text{OR}$ Gate sends two truth values to one truth value, and it returns true if there is at least one true value among the two.\n부울 함수\r기호\r진리표\r$\\text{OR}$\r$a$\r$b$\r$a \\lor b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\rThis can be expressed with $\\text{NOT}$ Gate and $\\text{AND}$ Gate.\n$$ a \\lor b = \\lnot(\\lnot a \\land \\lnot b) $$\nKim Young-hoon·Heo Jae-seong, Quantum Information Theory (2020), p84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3403,"permalink":"https://freshrimpsushi.github.io/en/posts/3403/","tags":null,"title":"Disjunction, OR Gate"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 The following Boolean function is called the $\\text{AND}$ gateAND gate or logical conjunctionconjunction and is denoted as follows.\n$$ \\land : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\land 0 = 0,\\quad 0\\land 1 = 0,\\quad 1\\land 0 = 0,\\quad 1\\land 1 = 1 $$\nExplanation The $\\text{AND}$ gate sends two truth values to one truth value, and returns true only when both truth values are true.\n부울 함수\r기호\r진리표\r$\\text{AND}$\r$a$\r$b$\r$a \\land b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$1$\rThis can be expressed with $\\text{NOT}$ gate and $\\text{OR}$ gate.\n$$ a \\land b = \\lnot(\\lnot a \\lor \\lnot b) $$\nKim Young-hoon and Heo Jae-sung, Quantum Information Theory (2020), p84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3402,"permalink":"https://freshrimpsushi.github.io/en/posts/3402/","tags":null,"title":"Conjunction, AND Gate"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\rDefinition1 2 Let\u0026rsquo;s assume the discrete random variable $X$ can take values $n$. Let\u0026rsquo;s denote the probability mass function of $x_{1}, x_{2}, \\dots, x_{n}$ as $X$. Then, the entropyShannon entropy $p$ or $X$ is defined as follows:\n$$ \\begin{equation} H(X) = H(p) := E\\left[ I(x_{i}) \\right] = \\sum_{i=1}^{n} p(x_{i}) I(x_{i}) = -\\sum_{i=1}^{n} p(x_{i}) \\log_{2}p(x_{i}) \\end{equation} $$\nHere, $p$ represents information content, and $H$ represents the expected value.\nIf $I$ is a continuous random variable,\n$$ H(X) = H(p) = - \\int_{-\\infty}^{\\infty} p(x)\\log_{2}p(x) dx $$\nExplanation Simply put, entropy is the expected value (average) of information. Entropy allows us to mathematically handle the efficiency of coding and the limits of communication.\nEntropy is often described as disorder. Here, order refers to rules, trends, patterns, etc. Therefore, high entropy means high disorder, indicating that it is difficult to discern patterns or rules for the random variable $E$.\nLet\u0026rsquo;s consider a biased coin flip. If the probability of getting heads is $X$, then the probability of tails is $X$, and the entropy is as follows:\n$$ H = -p\\log_{2}p - (1-p)\\log_{2}(1-p) $$\nIf we plot $p$ against $1-p$, it looks like this:\nWhen the probability of heads is $p$, the entropy is $H$ and is at its maximum. This means it\u0026rsquo;s most challenging to discern any pattern or rule in the coin flip. In fact, we can\u0026rsquo;t be sure which side of the coin will show up in a coin flip. If the probability of heads changes slightly, the entropy decreases. For example, if the probability of heads is $\\dfrac{1}{2}$, the entropy is about $H = -\\dfrac{1}{2}\\log_{2}\\dfrac{1}{2}-\\dfrac{1}{2}\\log_{2}\\dfrac{1}{2} = 1$, indicating lower disorder, meaning there is some rule or pattern (in this case, heads come up most of the time). This can be summarized as follows:\nHigh entropy = high disorder = no regularity or pattern = hard to predict the result Low entropy = low disorder = presence of regularity or pattern = easier to predict the result\rAs you can guess from the above example, generally, when there are $\\dfrac{95}{100}$ possible outcomes, the highest entropy occurs when all probabilities are equal to $0.28$.\nProperties Let\u0026rsquo;s assume the random variable $n$ can take values $\\dfrac{1}{n}$. Entropy $X$ has the following properties:\n$n$ is a concaveconcave function. For any $x_{1}, x_{2}, \\dots, x_{n}$, if $H$, then $H$. When all probabilities are equal to $x_{i}$, entropy is maximum, and its value is $p(x_{i}) = 1$. For a random vector $H(X) = 0$ with a mean of $p(x_{i}) = \\dfrac{1}{n}$ and a covariance matrix of $\\log_{2}n$, the following holds for its entropy: $$ \\begin{equation} H(X) \\le \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{p} \\left| K \\right| \\right] \\end{equation} $$ $\\mathbf{0}$ is the determinant of the covariance matrix. If $K$ is normally distributed, equality holds. Given the mean $X \\in \\mathbb{R}^{n}$ and variance $\\left| K \\right|$, the distribution with the maximum entropy is the normal distribution. For the random variable $X$ and estimator $\\mu$, the following holds: $$ E\\left[ (X - \\hat{X})^{2} \\right] \\ge \\dfrac{1}{2\\pi e} e^{2H(X)} $$ Proof 4 For convenience, let\u0026rsquo;s denote $\\sigma^{2}$. Let\u0026rsquo;s assume $X$ is any probability density function that satisfies $\\hat{X}$. Let\u0026rsquo;s denote $\\mathbf{x} = X$ as the probability density function of the normal distribution $g$. $$ \\phi (\\mathbf{x}) = \\dfrac{1}{\\sqrt{(2\\pi)^{p} \\left| K \\right|}} \\exp \\left( -\\dfrac{1}{2}\\mathbf{x}^{T} K^{-1} \\mathbf{x} \\right) $$ First, we\u0026rsquo;ll show that formula $\\displaystyle \\int g(\\mathbf{x})x_{i}x_{j} d \\mathbf{x} = K_{ij}$ holds. Calculating $\\phi$ first,\n$$ \\begin{align*} \\ln \\phi (\\mathbf{x}) \u0026amp;= \\ln\\dfrac{1}{\\sqrt{(2\\pi)^{p} \\left| K \\right|}} - \\dfrac{1}{2}\\mathbf{x}^{T} K^{-1} \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} x_{i} x_{j} \\end{align*} $$\nThe first term can be expressed as some constant $N(\\mathbf{0}, K)$, and the second term can also be expressed as a quadratic form dependent only on $\\displaystyle \\int g(\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} = \\int \\phi (\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x}$. Therefore,\n$$ \\begin{align*} \\int g(\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} \u0026amp;= C \\int g(\\mathbf{x}) d \\mathbf{x} + \\int g(\\mathbf{x})\\sum a_{ij}x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} \\int g(\\mathbf{x}) x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij}K_{ij} \\qquad \\text{by assumption for $g$} \\end{align*} $$\nAlso,\n$$ \\begin{align*} \\int \\phi (\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} \u0026amp;= C \\int \\phi (\\mathbf{x}) d \\mathbf{x} + \\int \\phi (\\mathbf{x})\\sum a_{ij}x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} \\int \\phi (\\mathbf{x}) x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij}K_{ij} \\qquad \\text{by definition of covariance} \\end{align*} $$\nSince the relative entropy is always greater than or equal to $\\ln \\phi (\\mathbf{x})$,\n$$ \\begin{align*} 0 \u0026amp;\\le D(g \\| \\phi) \\\\ \u0026amp;= \\int g \\ln \\dfrac{g}{\\phi} \\\\ \u0026amp;= \\int g \\ln g - \\int g \\ln \\phi \\\\ \u0026amp;= - H(g) - \\int \\phi \\ln \\phi \\\\ \u0026amp;= - H(g) + H(\\phi) \\end{align*} $$\nThe entropy of the normal distribution is $C$, so,\n$$ H(X) = H(g) \\le H(\\phi) = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right] $$\nNow, let\u0026rsquo;s assume $K^{-1}$ is a one-dimensional random variable.\n$$ \\begin{align*} E\\left[ (X - \\hat{X})^{2} \\right] \u0026amp;\\ge \\min_{X} E\\left[ (X - \\hat{X})^{2} \\right] \\\\ \u0026amp;= E\\left[ (X - E(X))^{2} \\right] \\\\ \u0026amp;= \\Var(X) \\end{align*} $$\nFor a one-dimensional $a_{ji}$, we get the following equation:\n$$ \\begin{align*} \u0026amp;\u0026amp; H(X) \u0026amp;\\le \\dfrac{1}{2} \\ln(2\\pi e \\sigma^{2}) \\\\ \\implies \u0026amp;\u0026amp; 2H(X) \u0026amp;\\le \\ln(2\\pi e \\sigma^{2}) \\\\ \\implies \u0026amp;\u0026amp; e^{2H(X)} \u0026amp;\\le 2\\pi e \\sigma^{2} \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{2\\pi e}e^{2H(X)} \u0026amp;\\le \\sigma^{2} = \\Var(X) \\\\ \\end{align*} $$\nSubstituting into the above equation,\n$$ E\\left[ (X - \\hat{X})^{2} \\right] \\ge \\dfrac{1}{2\\pi e} e^{2H(X)} $$\nEntropy of the Normal Distribution The entropy of the normal distribution $0$ (when natural log is used) is as follows:\n$$ H = \\dfrac{1}{2} \\ln (2\\pi e \\sigma^{2}) = \\ln \\sqrt{2\\pi e \\sigma^{2}} $$\nThe entropy of the multivariate normal distribution $\\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right]$ is as follows:\n$$ H = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right] = \\dfrac{1}{2}\\ln (\\det (2\\pi e K)) $$\nSee Also Shannon entropy defined in probability information theory Entropy defined in thermodynamics Gibbs\u0026rsquo; entropy representation Kim Young-hoon·Heo Jae-seong, Quantum Information Theory (2020), p246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen M. Barnett, Quantum Information (2009), p7-10\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3400,"permalink":"https://freshrimpsushi.github.io/en/posts/3400/","tags":null,"title":"Shannon Entropy in Classical Information Theory"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r정의1 이산확률변수 $X$에 대해서, $X=x$인 사건의 정보(량)information $I$를 다음과 같이 정의한다.\n$$ \\begin{equation} I(x) = -\\log_{2} p(x) \\end{equation} $$\n$p$는 $X$의 확률질량함수이다.\n설명 추상적 개념인 정보에 대한 정량적인 정의를 제시한 사람은 디지털 논리회로 이론과 정보이론을 창시한 클래드 섀넌Claude Shannon이다. 정보량를 '확률의 마이너스 로그'로 정의한 것을 처음 볼 때는 이해가 안되겠지만, 설명을 듣고 나면 이보다 자연스러울 수 없다는 생각이 들 것이다.\n정보의 가치는 일어나기 힘든 일일수록, 그러니까 일어날 확률이 희박할수록 크다. 가령 \u0026quot;내일 물리과 건물에 물리과 학과장님이 오신다\u0026quot;는 문장이 갖고 있는 정보량은 거의 없다고 볼 수 있다. 당연히 내일 학과장이 출근할 것이기 때문이다. 반면에 \u0026quot;내일 물리과 건물에 아이브가 온다\u0026quot;는 문장은 완전히 특급 정보이다. 아이브가 뜬금없이 물리과 건물에 등장할 확률은 거의 없다시피하므로, 이런 정보는 가치가 아주 높은 정보라고 할 수 있다. 다른 예로 \u0026quot;내일 삼성전자의 주식 상승폭이 $1 \\%$ 포인트 이내이다\u0026quot;는 거의 가치가 없는 정보이겠지만, \u0026quot;내일 삼성전자의 주식이 상한가를 친다\u0026quot;는 엄청난 정보이다. 따라서 일어날 확률이 적은 사건이 많은 정보를 갖고있다고 볼 수 있다.\n확률의 함숫값은 $0 \\le p \\le 1$이므로, $p$가 작을수록 정보의 함숫값이 커지도록 하려면 마이너스 로그를 취하면 된다. 따라서 자연스럽게 정보를 $(1)$과 같이 정의할 수 있다.\n$-\\log_{2}(x)$의 치역이 $[0, \\infty)$이므로 확률인 $1$인 사건, 그러니까 반드시 일어나는 일은 정보량이 $0$이다. 또한 일어날 확률이 낮아질수록 정보의 가치는 계속 커진다.\n확률변수 $X$ 자체에 대한 정보량은 엔트로피라 부른다.\n같이보기 확률정보이론에서 정의되는 정보 김영훈·허재성, 양자 정보 이론 (2020), p246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3398,"permalink":"https://freshrimpsushi.github.io/en/posts/3398/","tags":null,"title":"고전정보이론에서 정보량이란?"},{"categories":"줄리아","contents":"Overview1 The framestyle attribute allows changing the style of the plot\u0026rsquo;s axes and border. The possible options are as follows:\n:box :semi :axes :origin :zerolines :grid :none Code The default setting is :axes.\n▷code1◁\nThe styles for each attribute are as follows.\n▷code2◁\nhttps://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3376,"permalink":"https://freshrimpsushi.github.io/en/posts/3376/","tags":null,"title":"How to Change Axis Style in Julia Plots `framestyle`e`"},{"categories":"줄리아","contents":"Overview In Julia, \u0026lt;condition\u0026gt; \u0026amp;\u0026amp; \u0026lt;statement\u0026gt; executes \u0026lt;statement\u0026gt; when \u0026lt;condition\u0026gt; is true. As a function, it returns the result of \u0026lt;statement\u0026gt; if true, and if false, \u0026lt;statement\u0026gt; is not even evaluated.\nWhile it allows writing code efficiently and concisely, it may reduce readability. Moreover, even if you don\u0026rsquo;t use it frequently, you should understand it to read the code written by others. Without any context, encountering such syntax can be utterly incomprehensible.\nSee Also Short Circuit Code Basic Example ▷code1◁\nSince 2 is even, push!(num, 2) is evaluated, and 2 is added to the empty array num.\nReturn ▷code2◁\n\u0026amp;\u0026amp; acts as a function too, thus can return some value. Here, check is considered to have received check = push!(num, 4) as a return.\n▷code3◁\nOn the other hand, if \u0026lt;statement\u0026gt; is false, it is not evaluated and \u0026amp;\u0026amp; itself returns false.\nNegation ▷code4◁\nUse || instead of \u0026amp;\u0026amp;.\nComplete Code ▷code5◁\nEnvironment OS: Windows julia: v1.6.3 ","id":2341,"permalink":"https://freshrimpsushi.github.io/en/posts/2341/","tags":null,"title":"How to Write Conditional Statements Concisely in Julia"},{"categories":"줄리아","contents":"Overview The method of replacing with a specific value is inconvenient because it changes one column at a time, and when dealing with NaN throughout the dataframe, it seems more practical to use a better trick.\nCode julia\u0026gt; df = DataFrame(rand(1:9,3,3), :auto) ./ DataFrame(rand(0:1,3,3), :auto)\r3×3 DataFrame\rRow │ x1 x2 x3 │ Float64 Float64 Float64\r─────┼───────────────────────────\r1 │ 5.0 Inf 7.0\r2 │ Inf 8.0 Inf\r3 │ 4.0 1.0 4.0 For instance, if you want to replace Inf with 0 in the dataframe above, you can do it in just one line as follows.\njulia\u0026gt; ifelse.(isinf.(df), 0, df)\r3×3 DataFrame\rRow │ x1 x2 x3 │ Float64 Float64 Float64\r─────┼───────────────────────────\r1 │ 5.0 0.0 7.0\r2 │ 0.0 8.0 0.0\r3 │ 4.0 1.0 4.0 Of course, if you replace isinf with isnan in ifelse.(isinf.(df), 0, df), you can handle NaN, and you can change 0 to send to any value you prefer.\nFull Code using DataFrames, Random\rRandom.seed!(0)\rdf = DataFrame(rand(1:9,3,3), :auto) ./ DataFrame(rand(0:1,3,3), :auto)\rifelse.(isinf.(df), 0, df) See Also How to change specific values in a dataframe Environment OS: Windows julia: v1.6.3 ","id":2330,"permalink":"https://freshrimpsushi.github.io/en/posts/2330/","tags":null,"title":"How to Replace NaN with 0 in Julia DataFrames"},{"categories":"줄리아","contents":"Overview In Julia, A ? B : C is known as the Ternary Operator, which returns B if A is true and C otherwise. Just like binary operations are defined as functions in mathematics, the ternary operation is also a function. It\u0026rsquo;s similar to an if statement but has this fundamental difference, making it very useful once you\u0026rsquo;re accustomed to it. However, it can make the code less readable, so it\u0026rsquo;s not necessary to overuse it, but since others may use it, it\u0026rsquo;s good to get familiar with it to some extent.\nCode julia\u0026gt; x = iseven(2) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; x\r\u0026#34;even\u0026#34;\rjulia\u0026gt; y = iseven(3) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; y\r\u0026#34;odd\u0026#34; As seen above, these commands assign the string \u0026quot;even\u0026quot; or \u0026quot;odd\u0026quot; to variables x, y, depending on whether the given number is even or odd.\njulia\u0026gt; x * y\r\u0026#34;evenodd\u0026#34; Since it\u0026rsquo;s a function rather than a conditional statement, it allows for such convenient code. Trying to accomplish the same functionality using only if statements could unnecessarily lengthen the code due to issues like scope.\nComplete Code x = iseven(2) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; x\ry = iseven(3) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; y\rx * y Environment OS: Windows julia: v1.6.3 ","id":2328,"permalink":"https://freshrimpsushi.github.io/en/posts/2328/","tags":null,"title":"Julia's Ternary Operator ? :"},{"categories":"줄리아","contents":"Overview To make replacements, use the replace!() method1. The first argument should be the column of the dataframe you want to change, and the second argument takes a pair A =\u0026gt; B. It\u0026rsquo;s important that it\u0026rsquo;s the dataframe\u0026rsquo;s column being specified here.\nCode julia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 보스즈\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 보스즈\r8 │ 지연 95 163 보스즈\r9 │ 진숙 99 162 쪼꼬미\r10 │ 현정 94 165 보스즈 The example uses the WJSN dataframe as shown above.\njulia\u0026gt; replace!(WJSN.member, \u0026#34;진숙\u0026#34; =\u0026gt; \u0026#34;여름\u0026#34;); WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 보스즈\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 보스즈\r8 │ 지연 95 163 보스즈\r9 │ 여름 99 162 쪼꼬미\r10 │ 현정 94 165 보스즈 The \u0026quot;Jinsuk\u0026quot; in the :member column was changed to \u0026quot;Yeoreum\u0026quot;. Note that replace!() was used here, not replace(), and that it was a specific column of the dataframe that was specified.\njulia\u0026gt; replace!(WJSN.unit, \u0026#34;보스즈\u0026#34; =\u0026gt; \u0026#34;더블랙\u0026#34;); WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 더블랙\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 더블랙\r8 │ 지연 95 163 더블랙\r9 │ 여름 99 162 쪼꼬미\r10 │ 현정 94 165 더블랙 The \u0026quot;Bozuz\u0026quot; in the :unit column was uniformly changed to \u0026quot;The Black\u0026quot;.\nComplete Code using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;보스즈\u0026#34;]\r)\rWJSN\rreplace!(WJSN.member, \u0026#34;진숙\u0026#34; =\u0026gt; \u0026#34;여름\u0026#34;); WJSN\rreplace!(WJSN.unit, \u0026#34;보스즈\u0026#34; =\u0026gt; \u0026#34;더블랙\u0026#34;); WJSN See Also How to replace all NaNs in a dataframe with 0 at once Environment OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Replacing-Data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2326,"permalink":"https://freshrimpsushi.github.io/en/posts/2326/","tags":null,"title":"How to Change a Specific Value in a DataFrame in Julia"},{"categories":"줄리아","contents":"Overview 1 Use the freqtable() function from the FreqTables.jl package. It provides a similar functionality to the freq() function in R.\nCode Arrays julia\u0026gt; compartment = rand([\u0026#39;S\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;], 1000);\rjulia\u0026gt; freqtable(compartment)\r3-element Named Vector{Int64}\rDim1 │\r──────┼────\r\u0026#39;I\u0026#39; │ 316\r\u0026#39;R\u0026#39; │ 342\r\u0026#39;S\u0026#39; │ 342 By inserting an array like shown above, it will count the frequency for each class.\nDataFrames freqtable() is particularly useful for dataframes. Let\u0026rsquo;s load the built-in data ToothGrowth, just like in the example of regression analysis with qualitative variables in R.\njulia\u0026gt; ToothGrowth = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;ToothGrowth\u0026#34;)\r60×3 DataFrame\rRow │ Len Supp Dose │ Float64 Cat… Float64\r─────┼────────────────────────\r1 │ 4.2 VC 0.5\r2 │ 11.5 VC 0.5\r3 │ 7.3 VC 0.5\r4 │ 5.8 VC 0.5\r⋮ │ ⋮ ⋮ ⋮\r58 │ 27.3 OJ 2.0\r59 │ 29.4 OJ 2.0\r60 │ 23.0 OJ 2.0\r53 rows omitted\rjulia\u0026gt; freqtable(ToothGrowth, :Len)\r43-element Named Vector{Int64}\rLen │\r─────┼──\r4.2 │ 1\r5.2 │ 1\r5.8 │ 1\r6.4 │ 1\r⋮ ⋮\r29.5 │ 1\r30.9 │ 1\r32.5 │ 1\r33.9 │ 1\rjulia\u0026gt; freqtable(ToothGrowth, :Supp)\r2-element Named Vector{Int64}\rSupp │\r──────┼───\r\u0026#34;OJ\u0026#34; │ 30\r\u0026#34;VC\u0026#34; │ 30\rjulia\u0026gt; freqtable(ToothGrowth, :Dose)\r3-element Named Vector{Int64}\rDose │\r──────┼───\r0.5 │ 20\r1.0 │ 20\r2.0 │ 20 ToothGrowth contains data on the length of teeth (:Len) in guinea pigs fed different amounts of vitamin C or orange juice (:Supp). Calculating the frequencies for each column as shown tidies up the data nicely. It demonstrates that the data doesn’t necessarily have to be categorical.\njulia\u0026gt; freqtable(ToothGrowth, :Supp, :Dose)\r2×3 Named Matrix{Int64}\rSupp ╲ Dose │ 0.5 1.0 2.0\r────────────┼──────────────\r\u0026#34;OJ\u0026#34; │ 10 10 10\r\u0026#34;VC\u0026#34; │ 10 10 10 Of course, this type of table is most effective for categorical data. Calculating the frequencies for :Supp, and :Dose automatically divides it into 2D categories for us.\njulia\u0026gt; freqtable(ToothGrowth, :Len, :Dose, :Supp)\r43×3×2 Named Array{Int64, 3}\r[:, :, Supp=\u0026#34;OJ\u0026#34;] =\rLen ╲ Dose │ 0.5 1.0 2.0\r───────────┼──────────────\r4.2 │ 0 0 0\r⋮ ⋮ ⋮ ⋮\r33.9 │ 0 0 0\r[:, :, Supp=\u0026#34;VC\u0026#34;] =\rLen ╲ Dose │ 0.5 1.0 2.0\r───────────┼──────────────\r4.2 │ 1 0 0\r⋮ ⋮ ⋮ ⋮\r33.9 │ 0 0 1 Calculating across more than 3 columns simply returns the class counts in a 2D table. At this point, it almost loses any meaning in terms of exploring or summarizing data.\nPerformance Comparison julia\u0026gt; @time for t in 1:10^4\rfreqtable(compartment)\rend\r@time for t in 1:10^4\rcount(compartment .== \u0026#39;S\u0026#39;)\rcount(compartment .== \u0026#39;I\u0026#39;)\rcount(compartment .== \u0026#39;R\u0026#39;)\rend\r0.068229 seconds (340.00 k allocations: 27.466 MiB)\r0.059198 seconds (180.00 k allocations: 134.125 MiB, 36.71% gc time) Leaving the table aside, the notion of counting frequencies itself seems useful. Several tests were conducted measuring the speed of manual counting versus using freqtable() to compute the frequencies all at once. Neither was consistently faster, fluctuating depending on the amount of data or the number of classes. Overall, freqtable() tended to be slower, but not by a huge margin. So, regardless of speed, it\u0026rsquo;s worth using thoughtfully when needed.\nFull Code using FreqTables\rcompartment = rand([\u0026#39;S\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;], 1000);\rfreqtable(compartment)\rusing RDatasets\rToothGrowth = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;ToothGrowth\u0026#34;)\rfreqtable(ToothGrowth, :Len)\rfreqtable(ToothGrowth, :Supp)\rfreqtable(ToothGrowth, :Dose)\rfreqtable(ToothGrowth, :Supp, :Dose)\rfreqtable(ToothGrowth, :Len, :Dose, :Supp)\r@time for t in 1:10^4\rfreqtable(compartment)\rend\r@time for t in 1:10^4\rcount(compartment .== \u0026#39;S\u0026#39;)\rcount(compartment .== \u0026#39;I\u0026#39;)\rcount(compartment .== \u0026#39;R\u0026#39;)\rend Environment OS: Windows julia: v1.6.3 FreqTables v0.4.5 https://github.com/nalimilan/FreqTables.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2324,"permalink":"https://freshrimpsushi.github.io/en/posts/2324/","tags":null,"title":"How to Calculate Frequency in Julia"},{"categories":"줄리아","contents":"Guide Let\u0026rsquo;s say we have an example.csv file like the one above. When loading it into a dataframe, sometimes we want to create an entirely empty dataframe that only retains the column names, without importing all the data. This is necessary in cases where an empty dataframe is needed.\nusing CSV # Loading the dataframe with no rows df_empty = CSV.read(\u0026#34;example.csv\u0026#34;, DataFrame; limit = 0) In the resulting dataframe created above, it has all three columns intact but hasn\u0026rsquo;t imported any of their contents.\nCSV.read(limit = 1)\nBy using the limit = 1 option, the dataframe is loaded with only one line. CSV.read()[[false],:]\nBy not referring to any rows with a bit array of length $1$ [false], an empty dataframe remained. # Verifying column names remain intact column_names = names(df_empty) By checking the column names with the names() function, we can confirm that the column names are appropriately retained.\n# Inserting new data into the empty dataframe push!(df_empty, [1, \u0026#34;new_data\u0026#34;, 3.14]) When inserting new data with push!(), it works properly just like the original dataframe.\nWhat if just doing limit = 0? # Attempting to load dataframe with limit 0 to get empty dataframe df_empty_error = CSV.read(\u0026#34;example.csv\u0026#34;, DataFrame; limit = 0) A StackOverflowError occurs.\nEnvironment OS: Windows julia: v1.6.3 ","id":2322,"permalink":"https://freshrimpsushi.github.io/en/posts/2322/","tags":null,"title":"Reading Only Columns from a CSV File in Julia"},{"categories":"줄리아","contents":"Guide 1 using RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rdescribe(iris) describe() function can be used. Let\u0026rsquo;s summarize the iris data.\njulia\u0026gt; describe(iris)\r5×7 DataFrame\rRow │ variable mean min median max nmissing eltype\r│ Symbol Union… Any Union… Any Int64 DataType\r─────┼────────────────────────────────────────────────────────────────────────────────────────────\r1 │ SepalLength 5.84333 4.3 5.8 7.9 0 Float64\r2 │ SepalWidth 3.05733 2.0 3.0 4.4 0 Float64\r3 │ PetalLength 3.758 1.0 4.35 6.9 0 Float64\r4 │ PetalWidth 1.19933 0.1 1.3 2.5 0 Float64\r5 │ Species setosa virginica 0 CategoricalValue{String, UInt8} Environment OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Summarizing-Data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2320,"permalink":"https://freshrimpsushi.github.io/en/posts/2320/","tags":null,"title":"How to View Data Frame Summaries in Julia"},{"categories":"줄리아","contents":"Overview The CategoricalArrays.jl package in Julia serves a similar function to factor in R.\nCode julia\u0026gt; A = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;]\r4-element Vector{String}:\r\u0026#34;red\u0026#34;\r\u0026#34;blue\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; B = categorical(A)\r4-element CategoricalArray{String,1,UInt32}:\r\u0026#34;red\u0026#34;\r\u0026#34;blue\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; levels(B)\r3-element Vector{String}:\r\u0026#34;blue\u0026#34;\r\u0026#34;green\u0026#34;\r\u0026#34;red\u0026#34; categorical() The categorical() function allows for casting a regular array to a categorical array.\nlevels() With the levels() function, one can view the categories. Naturally, there are no duplicates in categories, and even if an element corresponding to a category is missing from the array, the category itself remains.\njulia\u0026gt; B[2] = \u0026#34;red\u0026#34;; B\r4-element CategoricalArray{String,1,UInt32}:\r\u0026#34;red\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; levels(B)\r3-element Vector{String}:\r\u0026#34;blue\u0026#34;\r\u0026#34;green\u0026#34;\r\u0026#34;red\u0026#34; This characteristic of maintaining categories regardless of the array\u0026rsquo;s state is very useful in certain coding contexts. It\u0026rsquo;s particularly beneficial in data analysis tasks, where subsets of the dataset are frequently handled. Knowing the categorical array in such cases can be a great help.\nOptimization Technically, instead of using levels(), using unique() on a regular array could achieve a similar implementation.\njulia\u0026gt; @time for t in 1:10^6\runique(A)\rend\r0.543157 seconds (6.00 M allocations: 579.834 MiB, 17.33% gc time)\rjulia\u0026gt; @time for t in 1:10^6\rlevels(B)\rend\r0.013324 seconds However, the speed difference is about 40 times. Since the categories get updated every time the array changes, there\u0026rsquo;s no need to undergo any separate computation process, allowing for immediate referencing.\nFull Code using CategoricalArrays\rA = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;]\rB = categorical(A)\rlevels(B)\rB[2] = \u0026#34;red\u0026#34;; B\rlevels(B)\r@time for t in 1:10^6\runique(A)\rend\r@time for t in 1:10^6\rlevels(B)\rend Environment OS: Windows julia: v1.6.3 CategoricalArrays v0.10.2 ","id":2318,"permalink":"https://freshrimpsushi.github.io/en/posts/2318/","tags":null,"title":"Julia's Categorical Array"},{"categories":"줄리아","contents":"Guide Using the RDatasets.jl package should do the trick. The following is an example of how to load the simplest iris dataset. It includes a variety of datasets beyond the basic built-in ones, so make sure to check out GitHub1.\njulia\u0026gt; using RDatasets\rjulia\u0026gt; iris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\r150×5 DataFrame\rRow │ SepalLength SepalWidth PetalLength PetalWidth Species\r│ Float64 Float64 Float64 Float64 Cat…\r─────┼─────────────────────────────────────────────────────────────\r1 │ 5.1 3.5 1.4 0.2 setosa\r2 │ 4.9 3.0 1.4 0.2 setosa\r3 │ 4.7 3.2 1.3 0.2 setosa\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮\r149 │ 6.2 3.4 5.4 2.3 virginica\r150 │ 5.9 3.0 5.1 1.8 virginica\r145 rows omitted See Also How to Load Built-in Datasets in R Environment OS: Windows julia: v1.6.3 https://github.com/JuliaStats/RDatasets.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2316,"permalink":"https://freshrimpsushi.github.io/en/posts/2316/","tags":null,"title":"How to Load a Built-in Dataset in R Used in Julia"},{"categories":"줄리아","contents":"## Guide For example, let\u0026#39;s check the version of the `Plots.jl` package. Press the `]` key in the REPL to enter the package mode. Here, if you type `status foo`, you can check the version of the `foo` package as follows. ![20211204_193048.png](20211204_193048.png#center) ## Environment - OS: Windows - julia: v1.6.3 ","id":2313,"permalink":"https://freshrimpsushi.github.io/en/posts/2313/","tags":null,"title":"How to Check Package Versions in Julia"},{"categories":"줄리아","contents":"Overview Use the isempty() function.\nCode julia\u0026gt; isempty([])\rtrue\rjulia\u0026gt; isempty(Set())\rtrue\rjulia\u0026gt; isempty(\u0026#34;\u0026#34;)\rtrue Though it\u0026rsquo;s mentioned as an array in the title, it actually could be a set or a string.\nOptimization Of course, checking if an array is empty by seeing if length() is $0$ might be fine too.\njulia\u0026gt; @time for t in 1:10^6\risempty([])\rend\r0.039721 seconds (1000.00 k allocations: 76.294 MiB, 27.85% gc time)\rjulia\u0026gt; @time for t in 1:10^6\rlength([]) == 0\rend\r0.041762 seconds (1000.00 k allocations: 76.294 MiB, 19.18% gc time) As you can see, for an empty array, both methods show no performance difference.\njulia\u0026gt; x = 1:10^6;\rjulia\u0026gt; @time for t in 1:10^6\risempty(x)\rend\r0.017158 seconds\rjulia\u0026gt; @time for t in 1:10^6\rlength(x) == 0\rend\r0.043243 seconds (1000.00 k allocations: 15.259 MiB) However, when the array is not empty, like above, you can see more than twice the speed difference. It\u0026rsquo;s natural that length() has to return the actual length while isempty() only needs to check if the first element exists. Considering aspects like readability or when using in conditional statements, using isempty() is more advisable.\nFull Code isempty([])\risempty(Set())\risempty(\u0026#34;\u0026#34;)\r@time for t in 1:10^6\risempty([])\rend\r@time for t in 1:10^6\rlength([]) == 0\rend\rx = 1:10^6\r@time for t in 1:10^6\risempty(x)\rend\r@time for t in 1:10^6\rlength(x) == 0\rend Environment OS: Windows julia: v1.6.3 ","id":2311,"permalink":"https://freshrimpsushi.github.io/en/posts/2311/","tags":null,"title":"How to Check if an Array is Empty in Julia"},{"categories":"줄리아","contents":"Overview People who have struggled with severe loneliness know, oh they know\nAnyone who has struggled with unknown errors while coding understands the critical importance of errors in programming\u0026hellip;\nIn Julia, errors can be thrown using the error() function or the @error macro. As of Julia v1.63, 25 types of built-in exceptions are defined1.\nCode julia\u0026gt; log(1 + 2im)\r0.8047189562170501 + 1.1071487177940904im Consider, for instance, when using the logarithmic function $\\log$ in a program, only real numbers should be allowed as input. However, Julia essentially provides an extension to complex numbers $\\log_{\\mathbb{C}}$ by default. The program running without errors is not an accomplishment. Unintended calculations can lead to unexpected problems, so if a calculation we do not want occurs, we should error out and not proceed.\nLet\u0026rsquo;s create a code that restricts the domain of the original log to real numbers $\\mathbb{R}$.\nerror() Function julia\u0026gt; function Rlog(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\rerror(DomainError, \u0026#34;: Rlog allow real number only\u0026#34;)\rend\rend\rRlog(1 + 2im)\rERROR: LoadError: DomainError: Rlog allow real number only\rStacktrace:\r[1] error(::Type, ::String)\r@ Base .\\error.jl:42\r[2] Rlog(x::Complex{Int64})\r@ Main c:\\admin\\REPL.jl:7\r[3] top-level scope\r@ c:\\admin\\REPL.jl:11\rin expression starting at c:\\admin\\REPL.jl:11 In the above Rlog, if the input is not a real number, it is restricted to raise a DomainError.\n@error Macro julia\u0026gt; function Rlog2(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\r@error \u0026#34;Rlog2 also allow Real number only\u0026#34;\rend\rend\rRlog2(1 + 2im)\r┌ Error: Rlog2 also allow Real number only\r└ @ Main c:\\admin\\REPL.jl:17 In the above Rlog2, if the input is not a real number, it is limited to throw an error immediately.\nRaise and Throw both mean to cause an error to happen, and there\u0026rsquo;s no significant difference in the big picture. Raise is a term used in Python, etc., while Throw is used in Java, etc.\nFull Code log(1 + 2im)\rfunction Rlog(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\rerror(DomainError, \u0026#34;: Rlog allow real number only\u0026#34;)\rend\rend\rRlog(1 + 2im)\rfunction Rlog2(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\r@error \u0026#34;Rlog2 also allow Real number only\u0026#34;\rend\rend\rRlog2(1 + 2im) Environment OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/control-flow/#Built-in-Exceptions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2309,"permalink":"https://freshrimpsushi.github.io/en/posts/2309/","tags":null,"title":"How to Handle Exceptions in Julia"},{"categories":"줄리아","contents":"Overview nrow(), ncol(), and size() can be used. Unlike with R, length() results in an error.\nCode julia\u0026gt; df = DataFrame(rand(100000,5), :auto)\r100000×5 DataFrame\rRow │ x1 x2 x3 x4 x5 │ Float64 Float64 Float64 Float64 Float64\r────────┼─────────────────────────────────────────────────────\r1 │ 0.474921 0.942137 0.0523668 0.588696 0.0176242\r2 │ 0.842828 0.910385 0.216194 0.794668 0.664883\r3 │ 0.0350312 0.96542 0.837923 0.920311 0.748409\r4 │ 0.613249 0.731643 0.941826 0.688649 0.161736\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮\r99998 │ 0.767794 0.242687 0.965885 0.557483 0.723849\r99999 │ 0.743936 0.67815 0.529923 0.247698 0.861302\r100000 │ 0.628269 0.252583 0.985485 0.24541 0.942741\r99993 rows omitted df is a dataframe with 100,000 rows and 5 columns.\njulia\u0026gt; nrow(df)\r100000\rjulia\u0026gt; ncol(df)\r5\rjulia\u0026gt; size(df)\r(100000, 5) nrow() and ncol() return the number of rows and columns, respectively, and size() returns the size of the rows and columns as a tuple. By referencing them in the order of rows, columns, you can know the size of the rows and columns separately. At first glance, size() seems much more useful, but let\u0026rsquo;s compare their performances.\nOptimization julia\u0026gt; @time for i in 1:10^6\rnrow(df)\rend\r0.051730 seconds (1000.00 k allocations: 15.259 MiB)\rjulia\u0026gt; @time for i in 1:10^6\rsize(df)[1]\rend\r0.536297 seconds (3.00 M allocations: 61.035 MiB, 5.44% gc time) Above is a comparison of the speed of nrow() and size(). As expected, the single-function nrow() is faster. The test may seem forced by running it excessively, but in cases where the dataframe is significantly larger—handling big data or using size() thinking it won’t make much difference can result in a waste of unnecessary time.\nAlso, there is a big difference in terms of code readability. nrow() and ncol() are function names commonly used in other languages and are undoubtedly the number of rows and columns, but size() greatly reduces readability due to the suffixing index. If possible, it is advised to use nrow() and ncol().\nFull Code using DataFrames\rdf = DataFrame(rand(100000,5), :auto)\rnrow(df)\rncol(df)\rsize(df)\r@time for i in 1:10^6\rnrow(df)\rend\r@time for i in 1:10^6\rsize(df)[1]\rend Environment OS: Windows julia: v1.6.3 ","id":2307,"permalink":"https://freshrimpsushi.github.io/en/posts/2307/","tags":null,"title":"How to Check DataFrame Size in Julia"},{"categories":"줄리아","contents":"Overview Named tuples can be used. The way to create a named tuple is by attaching a semicolon ; right after the left parenthesis. For example, if you say DataFrame(; x, y), a DataFrame is created with column names :x and :y, and the contents are x and y respectively.\nCode julia\u0026gt; MyCol7 = rand(5); B = 1:5;\rjulia\u0026gt; DataFrame(; MyCol7, B)\r5×2 DataFrame\rRow │ MyCol7 B │ Float64 Int64\r─────┼─────────────────\r1 │ 0.911763 1\r2 │ 0.93374 2\r3 │ 0.116779 3\r4 │ 0.467364 4\r5 │ 0.473437 5 Environment OS: Windows julia: v1.6.3 ","id":2305,"permalink":"https://freshrimpsushi.github.io/en/posts/2305/","tags":null,"title":"How to Create a DataFrame with Variable Names as Column Names in Julia"},{"categories":"줄리아","contents":"Overview Named tuples are tuples that, unlike regular tuples, can be used like dictionaries or structures. They have an array of symbols as keys and allow access to values via those keys, all the while retaining their tuple-like usage.\nCode x = rand(Bool, 5); y = rand(Bool, 5);\rz = (; x, y)\rtypeof(z)\rz.x Let\u0026rsquo;s run the above code to see how named tuples are used.\njulia\u0026gt; z = (; x, y)\r(x = Bool[0, 0, 1, 1, 0], y = Bool[1, 1, 0, 0, 0])\rjulia\u0026gt; typeof(z)\rNamedTuple{(:x, :y), Tuple{Vector{Bool}, Vector{Bool}}} A simple way to create a named tuple is to just make a tuple and put a semicolon ; right after the opening parenthesis. For instance, (; x) is equivalent to (; x=x).\njulia\u0026gt; z.x\r5-element Vector{Bool}:\r0\r0\r1\r1\r0\rjulia\u0026gt; z[2]\r5-element Vector{Bool}:\r1\r1\r0\r0\r0 Named tuples can be accessed by the name of their symbols, as well as by their index.\nEnvironment OS: Windows julia: v1.6.3 ","id":2303,"permalink":"https://freshrimpsushi.github.io/en/posts/2303/","tags":null,"title":"Julia's Named Tuples"},{"categories":"줄리아","contents":"Overview When dealing with high-dimensional arrays in Julia and NumPy, PyTorch (hereinafter referred to collectively as Python for simplicity), it is important to pay attention to what each dimension signifies as they differ. This distinction arises because Julia\u0026rsquo;s arrays are column-major, whereas Python\u0026rsquo;s arrays are row-major. Note that Matlab, being column-major like Julia, does not have this discrepancy, so those familiar with Matlab need not be overly cautious, but those accustomed to Python should be careful not to make indexing errors.\nBe sure to distinguish between the dimensions of arrays and vectors as they are used interchangeably. Explanation 1-Dimensional Arrays In Julia, an array of size $n$ represents a $n$-dimensional column vector.\njulia\u0026gt; ones(3)\r3-element Vector{Float64}:\r1.0\r1.0\r1.0 In Python, an array of size $n$ represents a $n$-dimensional row vector.\n\u0026gt;\u0026gt;\u0026gt; import numpy as np\r\u0026gt;\u0026gt;\u0026gt; np.ones(3)\rarray([1., 1., 1.]) Although there is a difference between columns and rows, as it is a 1-dimensional array, there isn\u0026rsquo;t much to be cautious about in terms of indexing.\n2-Dimensional Arrays At first glance, up to 2 dimensions, they might not appear different. However, their significances are different thus requiring caution. In Julia, the dimensions of an array extend backwards. This means, for a $(m,n)$ array, there are $n$ 1-dimensional arrays (column vectors) of size $m$. Specifically, a $(3,2)$ array signifies having 2 3-dimensional column vectors.\njulia\u0026gt; ones(3,2)\r3×2 Matrix{Float64}:\r1.0 1.0\r1.0 1.0\r1.0 1.0 Additionally, as Julia is \u0026lsquo;column-major\u0026rsquo;, the index of elements increases from top to bottom first, and then left to right.\njulia\u0026gt; A = reshape(range(1,6), (3,2))\r3×2 reshape(::UnitRange{Int64}, 3, 2) with eltype Int64:\r1 4\r2 5\r3 6\rjulia\u0026gt; for i ∈ 1:6\rprintln(A[i])\rend\r1\r2\r3\r4\r5\r6 On the other hand, new dimensions in Python arrays extend forwards. Meaning, for a $(m,n)$ array, there are $m$ 1-dimensional arrays (row vectors) of size $n$. The result below shows that the arrays are divided on a row basis.\n\u0026gt;\u0026gt;\u0026gt; np.ones([3,2])\rarray([[1., 1.],\r[1., 1.],\r[1., 1.]]) Therefore, at a glance, in both Julia and Python, a $(m,n)$ array is a $m \\times n$ matrix, but due to the differences between column-major and row-major, the order of indices change. The direction of indexing is up-down left-right in Julia, and left-right up-down in Python.\n# julia에서 2차원 배열의 인덱싱은 위에서 아래로, 그 다음 좌에서 우로\rjulia\u0026gt; A = reshape(range(1,6), (3,2))\r3×2 reshape(::UnitRange{Int64}, 3, 2) with eltype Int64:\r1 4\r2 5\r3 6\r# python에서 2차원 배열의 인덱싱은 좌에서 우로, 그 다음 위에서 아래로 \u0026gt;\u0026gt;\u0026gt; np.arange(6).reshape(3,2)\rarray([[0, 1],\r[2, 3],\r[4, 5]]) 3-Dimensional Arrays In Julia, new dimensions to the array are added backwards. Thus, a $(m,n,k)$ array consists of $k$ $(m,n)$ arrays.\njulia\u0026gt; ones(3,2,4)\r3×2×4 Array{Float64, 3}:\r[:, :, 1] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 2] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 3] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 4] =\r1.0 1.0\r1.0 1.0\r1.0 1.0 Conversely, in Python, a $(m,n,k)$ array consists of $m$ $(n,k)$ arrays.\n\u0026gt;\u0026gt;\u0026gt; np.ones([3,2,4])\rarray([[[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.]]]) In Machine Learning Considering image data, where $H=\\text{hieht}$ is height, $W=\\text{width}$ is width, $C=\\text{channel}$ is the number of channels, and $B=\\text{batch size}$ is batch size, in PyTorch, it is a $(B,C,H,W)$ array, and in Julia, it is a $(H,W,C,B)$ array.\nEnvironment OS: Windows11 Version: Julia 1.7.1, Python 3.9.2, numpy 1.19.5 ","id":3315,"permalink":"https://freshrimpsushi.github.io/en/posts/3315/","tags":null,"title":"Differences in Array Dimensions in Julia, Python (NumPy, PyTorch)"},{"categories":"머신러닝","contents":"Overview The notation and numbering of references and formulas follow the conventions of the original paper. Physics-informed neural networks (referred to as PINN[pronounced \u0026lsquo;pin\u0026rsquo;]) are artificial neural networks designed to numerically solve differential equations, introduced in the 2018 Journal of Computational Physics paper Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. The authors of the paper are M. Raissi, P. Perdikaris, and G.E. Karniadakis from the departments of Applied Mathematics and Mechanical Engineering.\nThe physics information mentioned in this paper, although it may sound grandiose, simply refers to the given differential equations themselves. In other words, using the given differential equations when solving them with artificial neural networks is essentially the same as saying \u0026lsquo;using physics information\u0026rsquo; in this context. When reading machine learning papers, one should be cautious not to be swayed by such seemingly impressive terminology.\nThe reason PINN is receiving significant attention in the numerical solution of differential equations is likely due to the simplicity and ease of understanding of the idea behind the loss function, as well as its straightforward implementation. In fact, the paper introduces a very simple DNN as an example.\nCommonly, the model introduced in Section 3.1 is referred to as PINN.\n0. Abstract The authors describe PINN as \u0026lsquo;an artificial neural network trained to solve supervised learning problems while satisfying a given nonlinear partial differential equation\u0026rsquo;. The two main issues addressed in this paper are the \u0026lsquo;data-driven solution and data-driven discovery of partial differential equations\u0026rsquo;. To evaluate performance, problems in fluid mechanics, quantum mechanics, and diffusion equations were solved.\n1. Introduction Although recent advances in machine learning and data analysis have led to innovative results in scientific fields such as image recognition, cognitive science, and genomics, there is a challenge in complex physical, biological, and engineering systems to yield desired results with limited information (due to the high cost of data collection). In such a small data regime, the convergence of advanced technologies like DNNs, CNNs, and RNNs is not guaranteed.\nStudies on methods to learn physics information efficiently (i.e., solve differential equations with minimal data) were conducted in [4-6]. The extension to nonlinear problems was proposed in subsequent studies by Raissi, one of the authors of this paper, in [8,9].\n2. Problem setup The function represented by an artificial neural network is determined by its input values (coordinates $x, t$ of the solution $u$ in a partial differential equation) and parameters. Automatic differentiation is utilized to differentiate these two types of variables.\nSuch neural networks are constrained to respect any symmetries, invariances, or conservation principles originating from the physical laws that govern the observed data, as modeled by general time-dependent and nonlinear partial differential equations.\nThis sentence from the paper might seem complex, but simply put, it means that the proposed artificial neural network, PINN, must satisfy the given differential equations. This is because the condition of satisfying the differential equations is used as a loss function, as will be discussed later.\nThe aim of this paper is to present a new modeling and computational paradigm to advance deep learning in mathematical physics. To this end, as mentioned earlier, this paper mainly addresses two issues. One is the data-driven solution of partial differential equations, and the other is the data-driven discovery of partial differential equations. All the codes and datasets used can be found at https://github.com/maziarraissi/PINNs. In this paper, a simple MLP using hyperbolic tangent as the activation function is used without any regularization such as $L1$, $L2$, or dropout, as introduced in the regularization section. The structure of the neural network, optimizer, learning rate, etc., are specifically introduced in each example.\nThis paper deals with the general form of parameterized and nonlinear partial differential equations as follows:\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u; \\lambda] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nHere, $u=u(t,x)$ is the hidden (i.e., not given or unknown) function, the solution of $(1)$ that we seek, and $\\mathcal{N}[\\cdot; \\lambda]$ is a nonlinear operator parameterized by $\\lambda$, with $\\Omega \\subset \\mathbb{R}^{D}$. Many problems in mathematical physics can be represented in this form. For instance, consider the one-dimensional viscous Burgers\u0026rsquo; equation:\n$$ u_{t} + uu_{x} = \\nu u_{xx} $$\nThis corresponds to the case in $(1)$ where $\\mathcal{N}[u; \\lambda] = \\lambda_{1} uu_{x} - \\lambda_{2}u_{xx}$ and $\\lambda = (\\lambda_{1}, \\lambda_{2})$. The two problems addressed for the given equation $(1)$ are as follows:\ndata-driven solution of PDEs: For a fixed $\\lambda$, what is the solution $u(t,x)$ of the system? data-driven discovery of PDEs: What are the parameters $\\lambda$ that best describe the observed data? 3. Data-driven solutions of partial differential equations Section 3 discusses the problem of finding data-driven solutions for partial differential equations of the following form:\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nThis corresponds to the situation in $(1)$ where the parameter $\\lambda$ is fixed. Section 3.1 and Section 3.2 will cover continuous time models and discrete time models respectively. The problem of finding the equations will be addressed in Section 4. The meaning of \u0026lsquo;data\u0026rsquo; mentioned here will be explained in detail below.\n3.1. Continuous time models Assuming $(t,x) \\in \\mathbb{R} \\times \\mathbb{R}$, then $u : \\mathbb{R}^{2} \\to \\mathbb{R}$. This will be approximated using an artificial neural network, employing a simple MLP implemented as follows. In Julia, it would be:\nusing Flux u = Chain( Dense(2, 10, relu), Dense(10, 10, relu), Dense(10, 1) ) In PyTorch, it would be:\nimport torch import torch.nn as nn import torch.nn.functional as F layers = [2, 10, 10, 1] class network(nn.Module): def __init__(self): super(network, self).__init__() layer_list = [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)] self.linears = nn.ModuleList(layer_list) def forward(self, tx): u = tx for i in range(len(layers)-2): u = self.linears[i](u) u = F.relu(u) u = self.linears[-1](u) return u u = network() Now, $u$ represents the artificial neural network we\u0026rsquo;ve defined, with $2$ input nodes and $1$ output node. Let\u0026rsquo;s define the left-hand side of $(2)$ as a function $f = f(t,x; u)$ as follows:\n$$ \\begin{equation} f := u_{t} + \\mathcal{N}[u] \\end{equation} $$\nSince $u$ is an artificial neural network, $f$ also becomes a sort of artificial neural network with hidden layer parameters. The $f$ defined in this way is called a physics-informed neural network (PINN), which is, in essence, the given partial differential equation itself. The differentiation included in $f$ is implemented through automatic differentiation and shares the same parameters as $u$. If the artificial neural network $u$ accurately approximates the solution to $(2)$, the function values of $f$ should be zero everywhere. We can infer that we will train the artificial neural network in a direction where $ f \\to 0$.\nLet\u0026rsquo;s say $(t_{u}^{i}, x_{u}^{i})$ are points in the domain where the initial and boundary conditions are defined: $$ (t_{u}^{i}, x_{u}^{i}) \\in( \\Omega \\times \\left\\{ 0 \\right\\}) \\cup (\\partial \\Omega \\times [0, T]) $$ If $u_{\\ast}$ is the actual solution, having initial and boundary conditions means that the following values are given:\n$$ \\left\\{ t_{u}^{i}, x_{u}^{i}, u^{i} \\right\\}_{i=1}^{N_{u}},\\quad u^{i} = u_{\\ast} (t_{u}^{i}, x_{u}^{i}) $$\nTheoretically, we would have an infinite number of such values, but in numerical problems, we can only handle a finite number of points, so let\u0026rsquo;s say we have $N_{u}$ points. The artificial neural network $u$ should output $u^{i}$ when given $(t_{u}^{i}, x_{u}^{i})$ as input, making these pairs the inputs and corresponding labels:\n$$ \\text{input} = (t_{u}^{i}, x_{u}^{i}),\\qquad \\text{label} = u^{i} $$\nThis is precisely the \u0026lsquo;data\u0026rsquo; to be learned in PINN. We can now consider the following as the loss function:\n$$ MSE_{u} = \\dfrac{1}{N_{u}} \\sum\\limits_{i=1}^{N_{u}} \\left| u(t_{u}^{i},x_{u}^{i}) - u^{i} \\right|^{2} $$\nAdditionally, $f$ should satisfy $(2)$ at appropriate points (ideally at all points where the solution $u_{\\ast}$ is defined, but numerically we can only handle a finite number of points) $\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$. In the paper, these points are referred to as collocation points. We set the following as the loss function for the collocation points:\n$$ MSE_{f} = \\dfrac{1}{N_{f}}\\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} $$\nIn other words, $MSE_{f}$ getting closer to $0$ means satisfying the physical information (the partial differential equation). Therefore, the final loss function for training the artificial neural network $u$ is as follows:\n$$ MSE = MSE_{u} + MSE_{f} $$\nThe paper explains that using $MSE_{f}$ as a constraint for physical information, as done here, was first researched in [15, 16]. However, in the PINN paper, it was reviewed using modern computational tools and applied to more challenging dynamic systems.\nThe term physics-informed machine learning was first used in Wang\u0026rsquo;s study [17] on turbulence modeling. However, prior to PINN, studies simply employed machine learning algorithms like support vector machines, random forests, and FNNs. PINN is distinguished from these previous approaches by considering not only the derivatives with respect to the parameters commonly used in machine learning\nbut also the derivatives with respect to the coordinates $x, t$ of the solution. That is, if the solution approximated by an artificial neural network with parameter $w$ is denoted as $u(t,x; w)$, while previously proposed methods only utilized the partial derivatives $u_{w}$, PINN also uses $u_{t}$, $u_{x}$, etc., to find the solution. It explains that this approach allows for finding the solution well even with a small amount of data.\nDespite the fact that there is no theoretical guarantee that this procedure converges to a global minimum, our empirical evidence indicates that, if the given partial differential equation is well-posed and its solution is unique, our method is capable of achieving good prediction accuracy given a sufficiently expressive neural network architecture and a sufficient number of collocation points $N_{f}$.\nThe paper notes that although there is no theoretical guarantee for the convergence of the proposed method, empirical evidence suggests that if the given partial differential equation is well-posed and has a unique solution, and if there are a sufficient number of points, then high prediction accuracy can be achieved.\n3.1.1. Example (Schrodinger Equation) This example focuses on verifying the effectiveness of the proposed method for solutions with periodic boundary conditions and complex values. As an example, the Schrodinger Equation with the following initial and boundary conditions is considered:\n$$ \\begin{align*} ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2}h \u0026amp;= 0,\\quad x\\in [-5, 5], t\\in[0, \\pi/2], \\\\ h(0,x) \u0026amp;= 2\\operatorname{sech} (x), \\\\ h(t,-5) \u0026amp;= h(t,5), \\\\ h_{x}(t,-5) \u0026amp;= h_{x}(t,5) \\end{align*} $$\nThe solution to the problem, $h_{\\ast}(t,x)$, is a function with complex-valued function outputs, namely $h_{\\ast} : [0, \\pi/2] \\times [-5, 5] \\to \\mathbb{C}$. However, instead of defining an artificial neural network that outputs complex numbers, we define it to output a 2-dimensional vector consisting of $u(t,x)$ representing the real part and $v(t,x)$ representing the imaginary part. In simple terms, it is defined as an MLP with 2 input nodes and 2 output nodes:\n$$ h(t,x) = \\begin{bmatrix} u(t,x) \\\\[0.5em] v(t,x) \\end{bmatrix} $$\nIn this problem, the PINN $f$ is defined as:\n$$ f := ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2} h $$\nThe parameters of $h(t,x)$ and $f(t,x)$ are trained to minimize the loss for initial values $MSE_{0}$, the loss for boundary values $MSE_{b}$, and the loss for physical information $MSE_{f}$.\n$$ MSE = MSE_{0} + MSE_{b} + MSE_{f} $$\n$$ \\begin{align*} \\text{where } MSE_{0} \u0026amp;= \\dfrac{1}{N_{0}}\\sum_{i=1}^{N_{0}} \\left| h(0, x_{0}^{i}) - h_{0}^{i} \\right|^{2} \\qquad (h_{0}^{i} = 2\\operatorname{sech} (x_{0}^{i})) \\\\ MSE_{b} \u0026amp;= \\dfrac{1}{N_{b}}\\sum_{i=1}^{N_{b}} \\left( \\left| h(t_{b}^{i}, -5) - h(t_{b}^{i}, 5) \\right|^{2} + \\left| h_{x}(t_{b}^{i},-5) - h_{x}(t_{b}^{i},5) \\right|^{2} \\right) \\\\ MSE_{f} \u0026amp;= \\dfrac{1}{N_{f}} \\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} \\end{align*} $$\nBe aware that there is a typo in the formula for $MSE_{b}$ in the paper. Here, $\\left\\{ x_{0}^{i}, h_{0}^{i} \\right\\}_{i=1}^{N_{0}}$ are the initial value data, $\\left\\{ t_{b}^{i} \\right\\}_{i=1}^{N_{b}}$ are the collocation points at the boundary, and $\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$ are the collocation points for $f$.\nFor data generation, traditional spectral methods were used. The number of initial value data $N_{0} = 50$ and the number of boundary value data $N_{b} = 50$ were chosen randomly. Additionally, the number of collocation points for $f$ is $N_{f} = 20,000$. The artificial neural network was constructed by stacking\n5 linear layers each with 100 nodes, and hyperbolic tangent $\\tanh$ was used as the activation function between layers.\nFigure 1.\nIn Figure 1, the upper image shows the heatmap of the predicted solution $\\left| h(t, x) \\right|$. The lower images show how well the predicted solution matches the actual solution at times $t = 0.59, 0.79, 0.98$, respectively. The relative $L_{2}$-norm is $0.00197 = 1.97 \\cdot 10^{-3}$, which means the predicted solution differs by about $0.02\\%$ when compared to the accurate solution. Therefore, PINN can accurately capture the nonlinear behavior of the Schrodinger equation even with a small amount of initial data.\nThe continuous time model being discussed works well even with a few initial values but has a potential limitation in that a large number of collocation points $N_{f}$ are needed. This is not a significant issue when the spatial dimension is 2 or less, but in higher dimensions, the required number of collocation points can increase exponentially, which can be problematic. Therefore, in the next section, a more structured neural network that does not require many collocation points is presented, utilizing the classical Runge–Kutta time-stepping schemes.\n3.2. Discrete time models In Section 3.1, we approximated the solution over continuous time. In that case, the artificial neural network is trained simultaneously over the entire domain, providing an output for any arbitrary point $(x,t)$. In this section, unlike Section 3.1, we deal with discrete time. In other words, we will describe how to approximate the value at $t_{n+1}$ using an artificial neural network, given the value at $t_{n}$. Applying a $q$-stage Runge-Kutta method to $(2)$ yields the following: $$ u(t_{n+1}, x) = u(t_{n}, x) - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u(t_{n}+c_{j} \\Delta t, x) \\right] $$\nIf we denote $u^{n}(x) = u(t_{n}, x)$ and $u^{n+c_{j}} = u(t_{n} + c_{j}\\Delta t, x)$, then:\n$$ \\begin{equation} \\begin{aligned} u^{n+1} \u0026amp;= u^{n} - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] \\\\ \\text{where } u^{n+c_{j}} \u0026amp;= u^{n} - \\Delta t \\sum_{i=1}^{q} a_{j,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] \\quad j=1,\\dots,q \\end{aligned}\\tag{7} \\end{equation} $$\nIn the $q+1$ equations above, let\u0026rsquo;s move all the $\\sum$ terms on the right-hand side to the left-hand side. Then, denote the left-hand side as $u_{i}^{n}$.\n$$ \\begin{equation} \\begin{aligned} u_{q+1}^{n} \u0026amp;:= u^{n+1} + \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] = u^{n} \\\\ \\\\ u_{1}^{n} \u0026amp;:= u^{n+c_{1}} + \\Delta t \\sum_{i=1}^{q} a_{1,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ u_{2}^{n} \u0026amp;:= u^{n+c_{2}} + \\Delta t \\sum_{i=1}^{q} a_{2,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ \u0026amp;\\vdots \\\\ u_{q}^{n} \u0026amp;:= u^{n+c_{q}} + \\Delta t \\sum_{i=1}^{q} a_{q,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\end{aligned}\\tag{9} \\end{equation} $$\nFrom this, we can see that all these values should be equal to $u^{n}$.\n$$ u^{n} = u_{1}^{n} = u_{2}^{n} = \\cdots = u_{q+1}^{n} \\tag{8} $$\nTherefore, the physics information mentioned in Section 3.2 refers to the given initial \u0026amp; boundary conditions and $(8)$. Now, to compute $u(t_{n+1}, x)$, we define two artificial neural networks. The artificial neural network used in Section 3.1 was $u$ which is expected to converge to the exact solution $u_{\\ast}$ and the differential equation $f$ that $u$ must satisfy, but here it\u0026rsquo;s slightly different. First, let\u0026rsquo;s define the artificial neural network $U$ as the following function:\n$$ U : \\mathbb{R} \\to \\mathbb{R}^{q+1} $$\nThat is, it\u0026rsquo;s a neural network with $1$ input node and $q+1$ output nodes. Let\u0026rsquo;s assume the output of this network is as follows:\n$$ U(x) = \\begin{bmatrix} u^{n+c_{1}}(x) \\\\[0.5em] u^{n+c_{2}}(x) \\\\ \\vdots \\\\[0.5em] u^{n+c_{q}}(x) \\\\[0.5em] u^{n+1}(x) \\end{bmatrix} \\tag{10} $$\nThis network corresponds to the neural_net defined within the PhysicsInformedNN class in the attached code.\nIn the learning process below, the last component of the output of $U$ is expected to converge to $u(t_{n+1}, x)$. The second neural network is defined using the output of $U$ and the definition in $(7)$ as follows.\n3.2.1. Example (Allen–Cahn equation) The example for the discrete time model deals with the Allen-Cahn equation, given the following initial condition and periodic boundary conditions:\n$$ \\begin{equation} \\begin{aligned} \u0026amp;u_{t} - 0.0001u_{xx} + 5 u^{3} - 5u = 0,\\qquad x\\in [-1, 1], t\\in[0, 1], \\\\ \u0026amp;u(0,x) = x^{2} \\cos (\\pi x), \\\\ \u0026amp;u(t,-1) = u(t,1), \\\\ \u0026amp;u_{x}(t,-1) = u_{x}(t,1) \\end{aligned}\\tag{12} \\end{equation} $$\nIn this example, the nonlinear operator included in $(9)$ is as follows:\n$$ \\mathcal{N}[u^{n+c_{j}}] = -0.0001u_{xx}^{n+c_{j}} + 5(u^{n+c_{j}})^{3} - 5u^{n+c_{j}} $$\nLet\u0026rsquo;s denote the value of $u$ at time step $t^{n}$ as $u^{n,i}$:\n$$ u^{n,i} = u^{n}(x^{n,i}) = u(t^{n}, x^{n,i}),\\qquad i=1,\\dots,N_{n} $$\nSince our problem is to compute $u^{n+1}$ given $u^{n}$, $\\left\\{ x^{n,i}, u^{n,i} \\right\\}_{i=1}^{N_{n}}$ is our given dataset. According to $(8)$, the following must hold for this dataset:\n$$ u^{n,i} = u_{1}^{n}(x^{n,i}) = \\cdots = u_{q+1}^{n}(x^{n,i}) $$\nSo, let\u0026rsquo;s set the following loss function, the sum of squared error (SSE), for this:\nIt\u0026rsquo;s unclear why $MSE$ is not used here, but $SSE$ is used for the discrete time model. The paper uses $MSE$ for continuous time models and $SSE$ for discrete time models, which suggests there might be a reason (even if experimental). $$ SSE_{n} = \\sum\\limits_{j=1}^{q+1} \\sum\\limits_{i=1}^{N_{n}} \\left| u_{j}^{n} (x^{n,i}) - u^{n,i} \\right|^{2} $$\nEach $u_{j}^{n}$ is computed according to $(9)$, with the calculations involving $u^{n+1}$ and $u^{n+c_{j}}$ being the output of the neural network $U$. This loss corresponds to net_U0 defined within the PhysicsInformedNN class in the attached code. Since the output of $U$ must satisfy the boundary conditions of $(12)$, we set the following loss function:\n$$ \\begin{align*} SSE_{b} \u0026amp;= \\sum\\limits_{i=1}^{q} \\left| u^{n+c_{i}}(-1) - u^{n+c_{i}}(1) \\right|^{2} + \\left| u^{n+1}(-1) - u^{n+1}(1) \\right|^{2} \\\\ \u0026amp;\\quad+ \\sum\\limits_{i=1}^{q} \\left| u_{x}^{n+c_{i}}(-1) - u_{x}^{n+c_{i}}(1) \\right|^{2} + \\left| u_{x}^{n+1}(-1) - u_{x}^{n+1}(1) \\right|^{2} \\\\ \\end{align*} $$\nThe final loss is the sum of these two:\n$$ SSE = SSE_{n} + SSE_{b} $$\nFigure 2.\nIn Fig. 2, the upper image shows the heatmap of the exact solution. The lower image shows the predicted values at $t=0.9$, given the $u$ at $t=0.1$. In the lower left image, the blue line represents the exact solution, and $\\color{red}\\mathsf{X\n}$ marks the points used as data. In the lower right image, the blue line is the exact solution, and the red line is the predicted solution.\nIn Implicit Runge-Kutta methods (IRK), solving simultaneous equations for all $j$ is required to compute $u^{n+c_{j}}$, meaning that the computational cost increases significantly as $q$ increases. However, the paper explains that the proposed method does not incur much additional cost even if $q$ increases. It also explains that while IRK may not be able to make accurate predictions with large time steps $\\Delta t$ when $q$ is small, PINN can still make accurate predictions even with large $\\Delta t$.\n4. Data-driven discovery of partial differential equations This chapter deals with the problem of finding the parameters $\\lambda$ of the partial differential equation $(1)$ when observational data is available. The details are explained below with examples.\n4.1. Continuous time models Let\u0026rsquo;s define $f$ as the left-hand side of $(1)$:\n$$ f = u_{t} + \\mathcal{N}[u; \\lambda] $$\nThe difference from $(3)$ in Section 3 is that $\\lambda$ is no longer a fixed constant but an unknown parameter that needs to be learned.\n4.1.1. Example (Navier–Stokes equation) Section 4.1.1 introduces an example related to real data of an incompressible fluid described by the Navier-Stokes equation. Consider the following 2-dimensional Navier-Stokes equation:\n$$ \\begin{equation} \\begin{aligned} u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) \u0026amp;= -p_{x} + \\lambda_{2}(u_{xx} + u_{yy}) \\\\ v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) \u0026amp;= -p_{y} + \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned} \\tag{15} \\end{equation} $$\nHere, $u(t,x,y)$ is the $x$ component of the fluid\u0026rsquo;s velocity vector, $v(t,x,y)$ is the $y$ component. And $p(t,x,y)$ is the pressure, $\\lambda = (\\lambda_{1}, \\lambda_{2})$ are unknown parameters. The solution to the Navier-Stokes equation satisfies the condition that the divergence is $0$, hence the following holds:\n$$ \\begin{equation} u_{x} + v_{y} = 0 \\tag{17} \\end{equation} $$\nLet\u0026rsquo;s assume some latent function $\\psi (t, x, y)$ such that:\n$$ u = \\psi_{y},\\quad v = -\\psi_{x} $$\nIn other words, the fluid\u0026rsquo;s velocity vector is set as $\\begin{bmatrix} \\psi_{y} \u0026amp; -\\psi_{x}\\end{bmatrix}$. This naturally satisfies $(17)$ since $u_{x} + v_{y} = \\psi_{yx} - \\psi_{xy} = 0$. Instead of obtaining $u$ and $v$ individually, we approximate $\\psi$ with an artificial neural network and derive $u, v$ as its partial derivatives. Let\u0026rsquo;s assume that the following measured information is available for the actual velocity vector field:\n$$ \\left\\{ t^{i}, x^{i}, y^{i}, u^{i}, v^{i} \\right\\}_{i=1}^{N} $$\nFrom this, we set the loss function as follows, remembering that $u = \\psi_{y}$ and $v = -\\psi_{x}$:\n$$ \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) $$\nAnd let\u0026rsquo;s rearrange the right-hand side of $(15)$ to the left-hand side and define them as $f$ and $g$, respectively.\n$$ \\begin{equation} \\begin{aligned} f \u0026amp;:= u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) + p_{x} - \\lambda_{2}(u_{xx} + u_{yy}) \\\\ g \u0026amp;:= v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) + p_{y} - \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned}\\tag{18} \\end{equation} $$\nThen the values of $f, g$ are expressed with $\\psi$ as follows. (Note that $p$ will also be approximated by a neural network)\n$$ \\begin{align*} f \u0026amp;= \\psi_{yt} + \\lambda_{1}(\\psi_{y} \\psi_{yx} - \\psi_{x}\\psi_{yy}) + p_{x} -\\lambda_{2}(\\psi_{yxx} + \\psi_{yyy}) \\\\ g \u0026amp;= -\\psi_{xt} + \\lambda_{1}(-\\psi_{y} \\psi_{xx} + \\psi_{x}\\psi_{xy}) + p_{y} + \\lambda_{2}(\\psi_{xxx} + \\psi_{xyy}) \\\\ \\end{align*} $$\nAdd the information that $f(t^{i}, x^{i}, y^{i}) = 0 = g(t^{i}, x^{i}, y^{i})$ to the loss function, and finally set it as follows:\n$$ \\begin{aligned} MSE \u0026amp;:= \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) \\\\ \u0026amp;\\qquad + \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| f(t^{i}, x^{i}, y^{i}) \\right|^{2} + \\left| g(t^{i}, x^{i}, y^{i}) \\right|^{2} \\right) \\end{aligned} \\tag{19} $$\nNow let\u0026rsquo;s define an artificial neural network with $3$ input nodes and $2$ output nodes. Let\u0026rsquo;s assume its output to be $\\begin{bmatrix} \\psi (t, x, y) \u0026amp; p(t, x, y) \\end{bmatrix}$. Then, the above loss function can be computed.\nExperiments were conducted for cases with and without noise in the data, and in both cases, it was reported that $\\lambda_{1}, \\lambda_{2}$ could be predicted with high accuracy. It was also demonstrated that even if data for the pressure $p$ was not provided, the neural network could accurately approximate the parameters and $p$. The specific experimental settings, results, and how the reference solutions were obtained are detailed in the paper.\n5. Conclusions In this paper, we introduced the physics-informed neural network, a new structure of neural networks that is capable of encoding the physical laws satisfied by given data and can be described by partial differential equations. This result has revealed that deep learning can learn about physical models, which could be applied to various physical simulations.\nHowever, the authors note that the proposed method should not be considered as a replacement for traditional methods of solving partial differential equations, such as the finite element method or spectral methods. In fact, Runge-Kutta methods were utilized in conjunction with PINN in Section 3.2..\nThe authors also attempted to address questions about the hyperparameters required to implement PINN, such as how deep the neural network should be and how much data is needed. However, they observed that what is effective for one equation might not be effective for another.\n","id":3313,"permalink":"https://freshrimpsushi.github.io/en/posts/3313/","tags":null,"title":"Paper Review: Physics-Informed Neural Networks"},{"categories":"줄리아","contents":"English Translation Code println(ARGS[1] * \u0026#34; + \u0026#34; * ARGS[2] * \u0026#34; = \u0026#34; * string(parse(Float64, ARGS[1]) + parse(Float64, ARGS[2]))) Let\u0026rsquo;s say we have a file named example.jl that consists of a single line as shown above. In Julia, we can receive command line arguments as an array through ARGS, similar to how sys.argv works with command line arguments in Python. The code written is a program that takes two numbers and prints their sum. The execution result is as follows.\nEnvironment OS: Windows julia: v1.6.3 ","id":2280,"permalink":"https://freshrimpsushi.github.io/en/posts/2280/","tags":null,"title":"How to Insert Command Line Arguments in Julia"},{"categories":"줄리아","contents":"## Overview Symbolic operations in Julia can be used through the `SymEngine.jl`[^1] package. [^1]: https://symengine.org/SymEngine.jl/ ## Code ### Defining Symbols Symbols can be defined in the following way. julia\u0026gt; using SymEngine\njulia\u0026gt; x = symbols(:x) x\njulia\u0026gt; x, y = symbols(\u0026ldquo;x y\u0026rdquo;) (x, y)\njulia\u0026gt; @vars x, y (x, y)\njulia\u0026gt; x = symbols(:x) x\njulia\u0026gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x)\n### Vectors and Matrices julia\u0026gt; v = [symbols(\u0026ldquo;v_▷eq1◁i$j\u0026rdquo;) for i in 1:2, j in 1:3] 2×3 Matrix{Basic}: a_11 a_12 a_13 a_21 a_22 a_23\njulia\u0026gt; Av 2-element Vector{Basic}: v_1a_11 + v_2a_12 + v_3a_13 v_1a_21 + v_2a_22 + v_3*a_23\njulia\u0026gt; @vars a, b, c, d, x, y (a, b, c, d, x, y)\njulia\u0026gt; [a b; c d] * [a x; b y] 2×2 Matrix{Basic}: a^2 + b^2 ax + by ac + bd cx + dy\n### Differentiation Symbolic differentiation can also be used with the [`Calculus.jl`](../3135) package. julia\u0026gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x)\njulia\u0026gt; diff(f, x) 2 + 2*x - sin(x)\n## Environment - OS: Windows10 - Version: Julia v1.7.1, SymEngine v0.8.7 ","id":3311,"permalink":"https://freshrimpsushi.github.io/en/posts/3311/","tags":null,"title":"Methods for Symbolic Computation in Julia"},{"categories":"줄리아","contents":"Code In Julia, the run() function is used to execute a string wrapped in backticks `. This is similar to using the os.system() from the os module in Python.\njulia\u0026gt; txt = \u0026#34;helloworld\u0026#34; \u0026#34;helloworld\u0026#34; julia\u0026gt; typeof(`echo $txt`)\rCmd 위와 같이 백틱으로 감싸진 문자열은 Cmd라는 타입을 가지고, run() 함수로써 실행할 수 있다.\njulia\u0026gt; run(`cmd /C echo $txt`) helloworld Process(`cmd /C echo helloworld`, ProcessExited(0)) Limited to this example, on Windows, it becomes a bit complicated as you have to execute echo within cmd, but on Linux, you can simply use echo $txt. If you frequently use such commands on Windows, consider modifying the environment variables1.\nEnvironment OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/running-external-programs/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2278,"permalink":"https://freshrimpsushi.github.io/en/posts/2278/","tags":null,"title":"Executing External Programs in Julia"},{"categories":"줄리아","contents":"Code To convert a string str to a number of type type, use parse(type, str).\njulia\u0026gt; parse(Int, \u0026#34;21\u0026#34;)\r21\rjulia\u0026gt; parse(Float64, \u0026#34;3.14\u0026#34;)\r3.14 You might wonder why we can\u0026rsquo;t do something like Int64(\u0026quot;21\u0026quot;) as in Python\u0026hellip; That\u0026rsquo;s because changing \u0026lsquo;\u0026ldquo;21\u0026rdquo;\u0026rsquo; into 21 is not about changing types but interpreting the string \u0026quot;21\u0026quot; as a number, which justifies the use of parse1.\nEnvironment OS: Windows julia: v1.6.3 https://discourse.julialang.org/t/why-are-there-all-these-strange-stumbling-blocks-in-julia/92644/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2276,"permalink":"https://freshrimpsushi.github.io/en/posts/2276/","tags":null,"title":"Converting String to Number in Julia"},{"categories":"줄리아","contents":"Overview 1 A Varargs Function, commonly mentioned in programming, is a function that can receive an unlimited number of arguments. In Julia, you can simply set a variable to accept variadic arguments by appending ... after it. Let\u0026rsquo;s understand this with an example code.\nAdditionally, this ... is called splat operator.2\nCode Isaac Newton famously discovered that adding the reciprocals of factorials simply converges to $e$ with the following theorem. $$ e = {{ 1 } \\over { 0! }} + {{ 1 } \\over { 1! }} + {{ 1 } \\over { 2! }} + \\cdots = \\sum_{k=0}^{\\infty} {{ 1 } \\over { k! }} $$ In this example, we will look at a sequence converging to Euler\u0026rsquo;s constant $e = 2.71828182 \\cdots$.\nfunction f(x...)\rzeta = 0\rfor x_ in x\rzeta += 1/prod(1:x_)\rend\rreturn zeta\rend As shown above, appending a dot after x to write x... automatically considers the given arguments as an array. The content of the function is as seen in the equation above, simply taking the reciprocal of factorials in order and adding them to return.\njulia\u0026gt; f(0)\r1.0\rjulia\u0026gt; f(0,1)\r2.0\rjulia\u0026gt; f(0,1,2)\r2.5\rjulia\u0026gt; f(0,1,2,3,4,5,6,7,8,9,10)\r2.7182818011463845 The execution result shows that as natural numbers are given longer, it gets closer to Euler\u0026rsquo;s constant. It\u0026rsquo;s noteworthy here that the variably entered arguments are automatically bundled into an array called x. For example, putting an array conceptually like the following could result in an error.\njulia\u0026gt; f(0:10)\rERROR: MethodError: no method matching (::Colon)(::Int64, ::UnitRange{Int64}) Environment OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/functions/#Varargs-Functions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2266,"permalink":"https://freshrimpsushi.github.io/en/posts/2266/","tags":null,"title":"How to Define Variadic Functions in Julia"},{"categories":"줄리아","contents":"Overview To achieve this, use the eltype() function. It likely gets its name from element type.\nCode julia\u0026gt; set_primes = Set([2,3,5,7,11,13])\rSet{Int64} with 6 elements:\r5\r13\r7\r2\r11\r3\rjulia\u0026gt; arr_primes = Array([2,3,5,7,11,13])\r6-element Vector{Int64}:\r2\r3\r5\r7\r11\r13 Consider two types of containers that hold prime numbers up to $13$. Honestly, they contain the same data, but one is a set while the other is an array.\njulia\u0026gt; typeof(set_primes)\rSet{Int64}\rjulia\u0026gt; eltype(set_primes)\rInt64\rjulia\u0026gt; typeof(arr_primes)\rVector{Int64} (alias for Array{Int64, 1})\rjulia\u0026gt; eltype(arr_primes)\rInt64 Applying typeof() to these distinguishes whether one is a set or an array, whereas eltype() returns the type of elements inside the container, regardless of what the container itself is.\njulia\u0026gt; typeof(1:10)\rUnitRange{Int64}\rjulia\u0026gt; eltype(1:10)\rInt64\rjulia\u0026gt; typeof(1:2:10)\rStepRange{Int64, Int64}\rjulia\u0026gt; eltype(1:2:10)\rInt64 The difference between 1:10 and 1:2:10, as seen above, demonstrates how eltype() can be usefully applied in the world of Julia programming, which may seem excessively obsessed with types.\nEnvironment OS: Windows julia: v1.6.3 ","id":2264,"permalink":"https://freshrimpsushi.github.io/en/posts/2264/","tags":null,"title":"How to Check the Element Type Inside a Julia Container"},{"categories":"줄리아","contents":"Code You can use the default() function.\ncode1\nThere is a way to set it up like the ordinary plot() function, and there is a way to change them one by one by giving key and value. Usually, the former is more convenient, but in the case of very complicated settings, the latter method can also be used by utilizing loops.\nInitialization If you want to reset all default settings, you can use default().\nEnvironment OS: Windows julia: v1.6.3 ","id":2262,"permalink":"https://freshrimpsushi.github.io/en/posts/2262/","tags":null,"title":"How to Change the Basic Settings of a Julia Plot"},{"categories":"줄리아","contents":"Overview When indexing, you can use the Not() function1. If you input the symbol of the column name as is, or an array of symbols, those columns are excluded from the indexing.\nCode using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r) Let\u0026rsquo;s run the example code above and check the results.\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 The WJSN dataframe is as shown above.\njulia\u0026gt; WJSN[:,Not(:height)]\r10×3 DataFrame\rRow │ member birth unit │ String Int64 String ─────┼───────────────────────\r1 │ 다영 99 쪼꼬미\r2 │ 다원 97 메보즈\r3 │ 루다 97 쪼꼬미\r4 │ 소정 95 더블랙\r5 │ 수빈 96 쪼꼬미\r6 │ 연정 99 메보즈\r7 │ 주연 98 더블랙\r8 │ 지연 95 더블랙\r9 │ 진숙 99 쪼꼬미\r10 │ 현정 94 더블랙 Only :height went in, so the :height column was removed.\njulia\u0026gt; WJSN[:,Not([:birth, :unit])]\r10×2 DataFrame\rRow │ member height │ String Int64 ─────┼────────────────\r1 │ 다영 161\r2 │ 다원 167\r3 │ 루다 157\r4 │ 소정 166\r5 │ 수빈 159\r6 │ 연정 165\r7 │ 주연 172\r8 │ 지연 163\r9 │ 진숙 162\r10 │ 현정 165 The array of symbols [:birth, :unit] went in, so the :birth and :unit columns were removed.\nEnvironment OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Taking-a-Subset\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2260,"permalink":"https://freshrimpsushi.github.io/en/posts/2260/","tags":null,"title":"How to Remove Specific Rows from a DataFrame in Julia"},{"categories":"줄리아","contents":"Overview To draw vertical and horizontal lines, use the vline!() and hline!() functions respectively.\nCode @time using Plots\rplot(rand(100))\rhline!([0.5], linewidth = 2)\rvline!([25, 75], linewidth = 2)\rpng(\u0026#34;result\u0026#34;) The positions where lines are drawn should be passed as an array. If the array contains multiple elements, multiple lines will be drawn at once.\nEnvironment OS: Windows julia: v1.6.3 ","id":2258,"permalink":"https://freshrimpsushi.github.io/en/posts/2258/","tags":null,"title":"How to Insert Vertical and Horizontal Lines in Figures in Julia"},{"categories":"줄리아","contents":"Overview RecipesBase.jl is a package that allows users to create their own styles for new plots, similar to how ggplot works in the R programming language, with its own unique syntax1 separate from the base Julia. Let\u0026rsquo;s learn through examples.\nCode using Plots\rusing DataFrames\rdf = DataFrame(x = 1:10, y = rand(10))\rplot(df)\r@userplot TimeEvolution\r@recipe function f(te::TimeEvolution)\rdf = te.args[1]\rlinealpha --\u0026gt; 0.5\rcolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend\rend\rtimeevolution(df); png(\u0026#34;1\u0026#34;)\rtimeevolution(df, legend = :left); png(\u0026#34;2\u0026#34;) First, running the above code results in the following error:\njulia\u0026gt; df = DataFrame(x = 1:10, y = rand(10))\r10×2 DataFrame\rRow │ x y\r│ Int64 Float64 ─────┼──────────────────\r1 │ 1 0.636532\r2 │ 2 0.463764\r3 │ 3 0.604559\r4 │ 4 0.654089\r5 │ 5 0.883409\r6 │ 6 0.91667\r7 │ 7 0.0609783\r8 │ 8 0.602259\r9 │ 9 0.460372\r10 │ 10 0.479944\rjulia\u0026gt; plot(df)\rERROR: Cannot convert DataFrame to series data for plotting This is because plot() is not defined by default to draw pictures by taking a dataframe.\n@userplot and @recipe @userplot TimeEvolution\r@recipe function f(te::TimeEvolution)\r...\rend In the example, we will draw a line chart directly with the time series data.\n@userplot: Specifies the name of the function that will inherit the properties of the plot() function. Note that although case-sensitive here, the resulting function can only use lowercase. @recipe: Specifies the style of the plot explicitly. The name of the function that follows is usually conventionally f. f(te::TimeEvolution) df = te.args[1]\rlinealpha --\u0026gt; 0.5\rcolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend Let\u0026rsquo;s understand the above code line by line.\ndf = te.args[1] It considers the first argument received as df. In this example, since it\u0026rsquo;s assumed that the argument provided is a dataframe, the abbreviation df is used. Keep in mind that f is not the function we will be using, and its argument te is also not directly utilized.\nlinealpha --\u0026gt; 0.5 The transparency of the lines to be drawn in this plot is set to 0.5. Note that this value is given with --\u0026gt;, not linealpha = 0.5, which is completely different from the common Julia syntax.\ncolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r...\rend Even if plot() direct maps a 2D array to a plot, the labels are automatically given as y1, y2, etc. To prevent this, labels will be given separately by fetching the column names of the dataframe as shown above. The enumerate() function is used to iterate through both the index and the column names simultaneously. For more details, see the explanation on enumerate().\n@series for ...\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend The @series macro specifies the data and its style that will be repeatedly drawn. The label of the column name is given with label --\u0026gt; column_name, and by writing df[:,column_index] in the last row, the data from that column name will be drawn. Note that the data to be drawn must be in the last row.\nResults As a result, we can now draw pictures in the style we specified with timeevolution() or timeevolution!().\ntimeevolution(df) It can be confirmed that even with a dataframe inserted, the picture is drawn successfully without error. The transparency of the line is 0.5, and the label is also exactly carried over from the column names in the dataframe.\ntimeevolution(df, legend = :left) We have arbitrarily adjusted the position of the legend. Despite no mention of legend when defining f, it can be confirmed to apply well. This is because timeevolution() inherits the rest of the elements of plot().\nEnvironment OS: Windows julia: v1.6.3 https://docs.juliahub.com/RecipesBase/8e2Mm/1.0.1/syntax/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2256,"permalink":"https://freshrimpsushi.github.io/en/posts/2256/","tags":null,"title":"How to Create Art Styles in Julia"},{"categories":"줄리아","contents":"Overview Using groupby() to divide by group and combine() for calculation is the way to go1.\ngroupby(df, :colname)\nReturns a GroupedDataFrame based on :colname. combine(gdf, :colname =\u0026gt; fun)\ngdf is a GroupedDataFrame divided by groups. :colname =\u0026gt; fun represents a pair of the symbol :colname, which is the name of the column containing the values to be calculated, and the calculation function fun. Code using DataFrames\rusing StatsBase\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r)\rsort!(WJSN, :birth)\runique(WJSN, :unit)\runits = groupby(WJSN, :unit)\runits[1]\runits[2]\runits[3]\rcombine(units, :height =\u0026gt; mean) Let\u0026rsquo;s run the example code above and check the result.\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 The WJSN dataframe is as shown above.\nDividing by Group groupby() julia\u0026gt; units = groupby(WJSN, :unit)\rGroupedDataFrame with 3 groups based on key: unit\rFirst Group (4 rows): unit = \u0026#34;더블랙\u0026#34;\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 주연 98 172 더블랙\r⋮\rLast Group (2 rows): unit = \u0026#34;메보즈\u0026#34;\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다원 97 167 메보즈\r2 │ 연정 99 165 메보즈 The dataframe was divided into three groups based on the :unit column.\njulia\u0026gt; units[1]\r4×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 주연 98 172 더블랙\rjulia\u0026gt; units[2]\r4×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 수빈 96 159 쪼꼬미\r2 │ 루다 97 157 쪼꼬미\r3 │ 다영 99 161 쪼꼬미\r4 │ 진숙 99 162 쪼꼬미\rjulia\u0026gt; units[3]\r2×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다원 97 167 메보즈\r2 │ 연정 99 165 메보즈 By indexing into the GroupedDataFrame like above, we can access the divided dataframes.\nCalculating by Group combine() julia\u0026gt; combine(units, :height =\u0026gt; mean)\r3×2 DataFrame\rRow │ unit height_mean │ String Float64 ─────┼─────────────────────\r1 │ 더블랙 166.5\r2 │ 쪼꼬미 159.75\r3 │ 메보즈 166.0 The code above calculates the average mean of :height in the WJSN dataframe, which is grouped by :unit into the dataframe units. As mentioned in the overview, this StatBase.mean() is the function for calculating the average. Changing this to sum() calculates the sum, and to min() calculates the minimum value for each group. In this example, the average of :height by :unit was calculated, and the 쪼꼬미 group was found to have the lowest average at 159.75.\nhttps://stackoverflow.com/questions/64226866/groupby-with-sum-on-julia-dataframe\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2254,"permalink":"https://freshrimpsushi.github.io/en/posts/2254/","tags":null,"title":"Grouping and Calculating DataFrames in Julia"},{"categories":"줄리아","contents":"Overview To achieve this, we can use unique(). More precisely, it leaves only one of the duplicates rather than deleting duplicated rows.\nCode using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r)\rsort!(WJSN, :birth)\runique(WJSN, :unit) Let\u0026rsquo;s run the example code above and check its result.\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 The WJSN dataframe looks like the above.\nRemoving duplicated rows in a single column with unique() julia\u0026gt; unique(WJSN, :unit)\r3×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 수빈 96 159 쪼꼬미\r3 │ 다원 97 167 메보즈 You can see that only one row remains for each :unit symbol.\nEnvironment OS: Windows julia: v1.6.3 ","id":2252,"permalink":"https://freshrimpsushi.github.io/en/posts/2252/","tags":null,"title":"How to Delete Duplicate Rows in DataFrames in Julia"},{"categories":"줄리아","contents":"Overview In Julia, options related to subplots can be controlled through the layout option.\nEntering an integer automatically creates a grid of that many plots. Entering a 2-tuple of integers creates a grid exactly as specified. The @layout macro is used to create complex layouts of the Plots.GridLayout type. Code using Plots\rleft = plot(randn(100), color = :red)\rright = plot(randn(100), color = :blue)\rplot(left, right)\rpng(\u0026#34;easyone\u0026#34;)\rdata = rand(10, 6)\rplot(data, layout = 6)\rpng(\u0026#34;easytwo\u0026#34;)\rplot(data, layout = (3,2))\rpng(\u0026#34;easygrid\u0026#34;)\rl = @layout [p1 ; p2 p2]\rp = plot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l)\rpng(\u0026#34;hardgrid\u0026#34;) Simple Enumeration left = plot(randn(100), color = :red)\rright = plot(randn(100), color = :blue)\rplot(left, right) Just by grouping several plots together and plotting again, it becomes a subplot.\nSimple Layout layout plot(data, layout = 6) plot(data, layout = (3,2)) You can simply use an integer or provide a tuple to create the desired grid. Since it works the same way without providing an integer, it’s only necessary to remember the case where a tuple is given.\nComplex Layout @layout l = @layout [p1 ; p2 p2] Defined a Plots.GridLayout type l that directs the layout such that there\u0026rsquo;s one picture in the first row and two columns of pictures in the second row. By providing this as an input to layout, a much more complex layout is represented like this.\np = plot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l) Creating Empty Spaces _ In the example above, if you want to center the picture in the first row to be the same size as those below, you can create empty spaces on both sides. You can indicate an empty space with _. If you adjust the width to half of the total with {0.5w},\nl = @layout([_ p{0.5w} _; p p])\rplot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l) Environment OS: Windows julia: v1.6.3 ","id":2250,"permalink":"https://freshrimpsushi.github.io/en/posts/2250/","tags":null,"title":"Drawing Subplots with a Layout in Julia"},{"categories":"줄리아","contents":"Overview 1 The position of the legend can be freely adjusted with the legend option of the plot() function. Giving a 2-tuple comprised of values between $0$ and $1$ will exactly place it at that location, otherwise, it can be controlled by symbols.\nSymbols combine top/bottom and left/right in order. Adding an outer at the very beginning places the legend outside of the plot. Examples of symbols that can be created through combinations include:\n:bottom :left :bottomleft :outertopright Since the order must be connected, symbols like :leftbottom or :toprightouter are not allowed.\nCode data = randn(100, 2)\rplot(data)\rplot(data, legend = (0.5, 0.7)); png(\u0026#34;tuple\u0026#34;)\rSymbols = [:none, :bottom, :left, :bottomleft, :outertopright, :inline]\rfor symbol ∈ Symbols\rplot(data, legend = symbol)\rpng(string(symbol))\rend Specifying exact location legend = (0.5, 0.7) The tuple (0.5, 0.7) places the legend at about 50% of the horizontal axis and 70% of the vertical height.\nRemoving the legend :none Top, Bottom, Left, Right :bottom, :left Combination :bottomleft Outside :outertopright The legend is moved outside of the plot. Note that this may distort the figure.\nEnd-of-line :inline Useful when there are many lines that are hard to distinguish by color, or when the last value is particularly important.\nEnvironment OS: Windows julia: v1.6.3 https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2248,"permalink":"https://freshrimpsushi.github.io/en/posts/2248/","tags":null,"title":"Adjusting the Position of the Legend in Julia Plots"},{"categories":"줄리아","contents":"Overview 1 To adjust the width and height of the figure, you can include the ratio option. Other recommended aliases include aspect_ratios, axis_ratio.\nratio = :none: The default value, where the figure\u0026rsquo;s size is adjusted to fit the ratio. ratio = :equal: Regardless of the figure\u0026rsquo;s size, the x and y axes are adjusted to a one-to-one ratio. ratio = Number: The ratio is adjusted according to Number. Number is given as the ratio in ${{세로} \\over {가로}} = {{\\Delta y} \\over {\\Delta x}}$. Code using Plots\rx = rand(100)\ry = randn(100)\rplot(x,y,seriestype = :scatter, ratio = :none)\rpng(\u0026#34;none\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = :equal)\rpng(\u0026#34;equal\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 0.5)\rpng(\u0026#34;0.5\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 1)\rpng(\u0026#34;1\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 2)\rpng(\u0026#34;2\u0026#34;) Default :none plot(x,y,seriestype = :scatter, ratio = :none) One-to-One :equal plot(x,y,seriestype = :scatter, ratio = :equal) Specific Ratio Number Number is given as the ratio in ${{세로} \\over {가로}}$.\nplot(x,y,seriestype = :scatter, ratio = 0.5) plot(x,y,seriestype = :scatter, ratio = 1) plot(x,y,seriestype = :scatter, ratio = 2) Environment OS: Windows julia: v1.6.3 https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2246,"permalink":"https://freshrimpsushi.github.io/en/posts/2246/","tags":null,"title":"How to Adjust the Aspect Ratio of a Julia Set Picture"},{"categories":"줄리아","contents":"Error using DataFrames, CSV\rexample = DataFrame(x = 1:10, 가 = \u0026#34;나다\u0026#34;)\rCSV.write(\u0026#34;example.csv\u0026#34;, example) When outputting to a CSV file in Julia, you can see a phenomenon where the Korean text becomes garbled, as shown above.\nCause The garbling isn\u0026rsquo;t actually due to the Korean text itself but a Unicode encoding issue, especially due to the UTF-8 encoding\u0026rsquo;s BOM. This can be solved by setting the encoding to UTF-8-sig in Python, among others.\nSolution 1 CSV.write(\u0026#34;example.csv\u0026#34;, example, bom = true) In CSV.jl, simply setting the option bom = true will output the text without it becoming garbled, as seen below.\nEnvironment OS: Windows julia: v1.6.3 https://csv.juliadata.org/stable/writing.html#CSV.write\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2244,"permalink":"https://freshrimpsushi.github.io/en/posts/2244/","tags":null,"title":"Solving Broken Characters when Exporting CSV in Julia"},{"categories":"줄리아","contents":"Overview The Crayons.jl package is known for decorating text output in Julia1.\nIf you want to decorate using only built-in functions, you can use printstyled().\nCode using Crayons\rprint(Crayon(background = :red), \u0026#34;빨강\u0026#34;)\rprint(Crayon(foreground = :blue), \u0026#34;파랑\u0026#34;)\rprint(Crayon(bold = true), \u0026#34;볼드\u0026#34;)\rprint(Crayon(italics = true), \u0026#34;이탤릭\u0026#34;)\rprint(Crayon(bold = true, italics = true), \u0026#34;볼드 이탤릭\u0026#34;) Running the above console will give the following decorated result.\nCrayon(...)\nforeground: Changes the color of the text itself. It can be given as a symbol or an integer triple (r,g,b) or as an integer between 0 and 255. background: Changes the background color of the text. The method of passing arguments is the same as for foreground. The options that can be set as booleans are as follows. In the example above, both individual and simultaneous execution results are shown.\nbold: Bold text. italics: Italic text. underline: Underlined text. Environment OS: Windows julia: v1.6.3 https://github.com/KristofferC/Crayons.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2242,"permalink":"https://freshrimpsushi.github.io/en/posts/2242/","tags":null,"title":"- Text Formatting Package in Julia"},{"categories":"줄리아","contents":"Code Let\u0026rsquo;s say we are given the Cosmic Girls dataframe as follows.\nWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\r) julia\u0026gt; WJSN\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 다원 97 167\r3 │ 루다 97 157\r4 │ 소정 95 166\r5 │ 수빈 96 159\r6 │ 연정 99 165\r7 │ 주연 98 172\r8 │ 지연 95 163\r9 │ 진숙 99 162\r10 │ 현정 94 165 The code to add a new column to the dataframe is as follows.\ndataframe[!, :\u0026quot;column_name\u0026quot;] = values To add a column for the unit, we get,\nWJSN[!, :\u0026#34;unit\u0026#34;] = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\rjulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 더블랙\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 더블랙\r8 │ 지연 95 163 더블랙\r9 │ 진숙 99 162 쪼꼬미\r10 │ 현정 94 165 더블랙 Environment OS: Windows10 Version: Julia 1.7.1, DataFrames 1.3.2 ","id":3273,"permalink":"https://freshrimpsushi.github.io/en/posts/3273/","tags":null,"title":"Adding a New Column to a DataFrame in Julia"},{"categories":"줄리아","contents":"Code using Plots\rscatter(rand(100), randn(100))\rplot!([0,1],[0,1])\rpng(\u0026#34;example1\u0026#34;)\rplot!([.00,.25,.50],[-2,0,-2])\rpng(\u0026#34;example2\u0026#34;)\rθ = 0:0.01:2π\rplot!(.5 .+ cos.(θ)/3, 1.5sin.(θ))\rpng(\u0026#34;example3\u0026#34;) Let\u0026rsquo;s learn how to insert line segments into the diagram by executing the code above.\nLine Segment plot!([0,1],[0,1]) Whether you draw just one line segment or something else, the method is the same. For a line segment, you need two points, so you just need to give an array of x coordinates and an array of y coordinates.\nMultiple Line Segments plot!([.00,.25,.50],[-2,0,-2]) y3 draws two line segments at once. The start and end points of the segments are connected.\nEllipse plot!(.5 .+ cos.(θ)/3, 1.5sin.(θ)) By applying the method used to draw multiple line segments, you can also draw an ellipse. Both grammatically and computationally, it\u0026rsquo;s relatively easier to draw compared to other languages.\nEnvironment OS: Windows julia: v1.6.3 ","id":2240,"permalink":"https://freshrimpsushi.github.io/en/posts/2240/","tags":null,"title":"Inserting a Line into a Julia Set Image"},{"categories":"기하학","contents":"Buildup1 Consider the easy definition of a vector field. In 3-dimensional space, a vector fieldvector function, vector field is a function $X : \\mathbb{R}^{3} \\to \\mathbb{R}^{3}$ that maps a 3-dimensional vector to another 3-dimensional vector. When considering this in the context of manifolds, $X$ maps a point $\\mathbb{R}^{3}$ on the differential manifold $p$ to a vector $\\mathbb{R}^{3}$ in $\\mathbf{v}$, treating this vector $\\mathbf{v}$ as an operator to consider as a directional derivative (= tangent vector). Therefore, a vector field is a function that maps a point $\\mathbb{R}^{3}$ on a manifold $p$ to a tangent vector $p$ at $\\mathbf{v}_{p} \\in T_{p}\\mathbb{R}^{3}$.\nThe codomain of a vector field is then the set of all tangent vectors at every point. Thus, a vector field $X$ is defined as the following function.\n$$ X : \\mathbb{R}^{3} \\to \\bigcup \\limits_{p\\in \\mathbb{R}^{3}} T_{p}\\mathbb{R}^{3} $$\nTo generalize this concept to manifolds, let\u0026rsquo;s define the tangent bundletangent bundle $M$ of a differential manifold $TM$ as follows.\n$$ TM := \\bigsqcup \\limits_{p\\in M} T_{p}M $$\nHere, $\\bigsqcup$ is a disjoint union.\nDefinition A vector fieldvector field $M$ on a differential manifold $X$ is a function that maps each point $p \\in M$ to a tangent vector $p$ at $X_{p} \\in T_{p}M$.\n$$ \\begin{align*} X : M \u0026amp;\\to TM \\\\ p \u0026amp;\\mapsto X_{p} \\end{align*} $$\nExplanation Values of a Vector Field Considering the definition of the tangent bundle, the element of $TM$ is $(p, X_{p})$, but it is mentioned in the definition that it maps $X_{p}$, which can raise questions.\n$$ \\begin{equation} TM := \\bigsqcup \\limits_{p \\in M } T_{p}M = \\bigcup_{p \\in M} \\left\\{ p \\right\\} \\times T_{p}M = \\left\\{ (p, X_{p}) : p \\in M, X_{p} \\in T_{p}M \\right\\} \\end{equation} $$\nSo, to be precise, according to the definition of the disjoint union, an element of $TM$ is indeed the ordered pair $(p, X_{p})$, but it is essentially treated as if it were $X_{p}$.\nThinking again about the definition of the tangent bundle, what we really want to do is not just collect ordered pairs $(p, X_{p})$ but to collect all tangent vectors at each point $p$. However, since each of $T_{p}M$ is isomorphic to $\\mathbb{R}^{n}$, there can be ambiguity when doing the union.\n$$ T_{p}M \\approxeq \\mathbb{R}^{n} \\approxeq T_{q}M $$\nFor example, if $M$ is a 3-dimensional manifold, there is ambiguity in treating the vector $T_{p}M \\approxeq \\mathbb{R}^{3}$ represented from $\\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1\\end{bmatrix}^{T}$ and the vector $X_{p}$ represented from $T_{q}M \\approxeq \\mathbb{R}^{3}$ as the same. Therefore, defining $\\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1\\end{bmatrix}^{T}$ as a set of ordered pairs is to make clear that $X_{q}$ and $TM$ are not the same and are distinctly different. From this, it naturally leads to considering a bijective function like $X_{p}$, treating it as $X_{q}$.\nIn some textbooks, to avoid this detailed explanation or assuming that readers adequately understand, the tangent bundle $\\iota_{p} : (p, X_{p}) \\mapsto X_{p}$ is sometimes defined as follows.\n$$ TM := \\bigcup\\limits_{p\\in M} T_{p}M = \\left\\{ X_{p} \\in T_{p}M : \\forall p \\in M \\right\\} $$\nOf course, as reiterated, the above definition and $(p, X_{p}) \\approx X_{p}$ are essentially the same. Also, note that the function value of $TM$ according to the above definition is a function $(1)$.\n$$ X_{p} : \\mathcal{D} \\to \\mathbb{R} $$\nVector Field as an Operator Let\u0026rsquo;s say $X$ is an $X_{p}$-dimensional differential manifold. Let the set of differentiable functions on $M$ be called $n$.\n$$ \\mathcal{D} = \\mathcal{D}(M) := \\left\\{ \\text{all real-valued functions of class } C^{\\infty} \\text{ defined on } M \\right\\} $$\nSee Also Set of differentiable functions on a differential manifold $M$ Set of differentiable vector fields on a differential manifold $\\mathcal{D} = \\mathcal{D}(M)$ Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p25-27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3270,"permalink":"https://freshrimpsushi.github.io/en/posts/3270/","tags":null,"title":"Vector Field on Differentiable Manifold"},{"categories":"줄리아","contents":"Code Let\u0026#39;s run the example code above and check the results. ```code2 The `WJSN` dataframe is as shown above. ### Sort by Column Number `sort(df, cols::integer)` Sorts based on the column numbered `cols`. ```code3 It can be seen that it is sorted based on the `height` which is the 3rd column. ### Sort by Column Name `sort(df, cols::Symbol)` Sorts based on the column named by the symbol `cols`. ```code4 Sorted based on `:birth`, hence, on `birth`. ### Sorting Priority `sort(df, cols::Array)` Sorts based on the order of `cols`. ```code5 Sorted by `birth` but `height` is also sorted. Compared to sorting just by `birth`, rows `2` and `3` have been reversed. ### Reverse Sorting `sort(df, rev::Bool=false)` Set `rev = true` to sort in reverse. The default is `false`. ```code6 ## Environment - OS: Windows - julia: v1.6.3 ","id":2238,"permalink":"https://freshrimpsushi.github.io/en/posts/2238/","tags":null,"title":"Sorting DataFrames in Julia"},{"categories":"줄리아","contents":"Code using DataFrames\rUnit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\rUnit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\rWJSN = vcat(Unit1, Unit2)\rpush!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\rpush!(WJSN, [\u0026#34;연정\u0026#34;,99,165]) Let\u0026rsquo;s run the example code above and check the result.\nConcatenating Rows of Two Data Frames with vcat() julia\u0026gt; Unit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\r4×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161 2 │ 루다 97 157 3 │ 수빈 96 159 4 │ 진숙 99 162 julia\u0026gt; Unit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\r4×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 소정 95 166\r2 │ 주연 98 172\r3 │ 지연 95 163\r4 │ 현정 94 165\rjulia\u0026gt; WJSN = vcat(Unit1, Unit2)\r8×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165 Of course, the columns of the two data frames must be the same.\nInserting a Row with push!() julia\u0026gt; push!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\r9×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\rjulia\u0026gt; push!(WJSN, [\u0026#34;연정\u0026#34;,99,165])\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\r10 │ 연정 99 165 When adding data with push!(), you must provide an array that matches the number of columns.\nEnvironment OS: Windows julia: v1.6.2 ","id":2236,"permalink":"https://freshrimpsushi.github.io/en/posts/2236/","tags":null,"title":"Inserting a New Row into a DataFrame in Julia"},{"categories":"줄리아","contents":"Overview Infinities.jl is a package that aids in using infinity symbols in Julia1. Surprisingly, infinity is quite useful in scientific computing coding.\nCode julia\u0026gt; 8 \u0026lt; Inf\rtrue The reason it\u0026rsquo;s mentioned that it helps in using infinity symbols, not just infinity, in the introduction is that you can actually use them without the package.\njulia\u0026gt; using Infinities\rjulia\u0026gt; 8 \u0026lt; ∞\rtrue\rjulia\u0026gt; -∞ \u0026lt; 8\rtrue\rjulia\u0026gt; max(∞, 10, 11)\r∞\rjulia\u0026gt; sin(∞)\rERROR: MethodError: no method matching AbstractFloat(::Infinities.Infinity) As you can see, it possesses all the expected properties of infinity.\njulia\u0026gt; ℵ₀ \u0026lt; ℵ₁\rtrue\rjulia\u0026gt; ℵ₀ \u0026gt; ℵ₁\rfalse\rjulia\u0026gt; ∞ == ℵ₁\rfalse\rjulia\u0026gt; ∞ == ℵ₀\rtrue\rjulia\u0026gt; ∞ === ℵ₀\rfalse Moreover, it allows the use of the cardinality of infinite sets with $\\aleph_{0}$ and $\\aleph_{1}$, which makes it possible to have the same infinity in terms of computation but still give them an order for sorting or comparison.\nEnvironment OS: Windows julia: v1.6.2 https://github.com/JuliaMath/Infinities.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2234,"permalink":"https://freshrimpsushi.github.io/en/posts/2234/","tags":null,"title":"Using Infinity in Julia"},{"categories":"줄리아","contents":"Guide 1 (@v1.6) pkg\u0026gt; status JuMP Status `C:\\Users\\rmsms\\.julia\\environments\\v1.6\\Project.toml` [4076af6c] JuMP v0.20.0 By pressing the ] key in the REPL, you enter the package mode. For example, if you want to upgrade a package version from v0.20.0 to v0.21, you can do so by appending @x.yy to the package as follows.\n(@v1.6) pkg\u0026gt; add JuMP@0.21 Resolving package versions... ... (@v1.6) pkg\u0026gt; status JuMP Status `C:\\Users\\rmsms\\.julia\\environments\\v1.6\\Project.toml` [4076af6c] JuMP v0.21.4 If you check the version again, you can verify that the package version has been successfully updated.\nEnvironment OS: Windows julia: v1.6.2 https://pkgdocs.julialang.org/dev/api/#General-API-Reference\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2232,"permalink":"https://freshrimpsushi.github.io/en/posts/2232/","tags":null,"title":"How to Install a Specific Version of a Package in Julia"},{"categories":"기하학","contents":"Overview We define the pullback on a differential manifold. If differential manifolds are complex, one can think of $M = \\mathbb{R}^{m}$ and $N = \\mathbb{R}^{n}$.\nDefinition1 Given two differential manifolds $M, N$ and a differentiable function $f : M \\to N$, we can consider a function $f^{\\ast}$ that maps $N$\u0026rsquo;s $k$-forms to $M$\u0026rsquo;s $k$-forms. Let $\\omega$ be a $k$-form on the manifold $N$, then a $k$-form $f^{\\ast}\\omega$ on the manifold $M$ is defined as the pullback of $\\omega$ as follows.\n$$ \\begin{equation} (f^{\\ast}\\omega)(p) (v_{1}, \\dots, v_{k}) := \\omega (f(p))\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M \\end{equation} $$\nExplanation The name pullback implies that, contrary to $f$ mapping from $M$ to $N$, $f^{\\ast}$ maps from $N$ to $M$. The definition and notation are quite complex, so let\u0026rsquo;s understand them step by step.\n$f^{\\ast}$ $f^{\\ast}$ is a map that sends $k$-forms of $N$ to $k$-forms of $M. Therefore, if $\\omega$ is a $k$-form of $N$, then $f^{\\ast}\\omega = f^{\\ast}(\\omega)$ is a $k$-form of $M.\n$f^{\\ast}\\omega (p)$ A $k$-form on the manifold $M$ maps $p \\in M$ to an element of $\\Lambda^{k}(T_{p}^{\\ast}M)$.\n$$ f^{\\ast}\\omega : M \\to \\Lambda^{k}(T_{p}^{\\ast}M) $$\n$$ \\Lambda^{k} (T_{p}^{\\ast}M) := \\left\\{ \\varphi : \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\nIn other words, $f^{\\ast}\\omega (p) \\in \\Lambda^{k} (T_{p}^{\\ast}M)$ is also a function. By the definition of $\\Lambda^{k} (T_{p}^{\\ast}M)$, $f^{\\ast}\\omega (p)$ takes \u0026ldquo;$k$ tangent vectors at $p$\u0026rdquo; as variables. Thus, $(1)$ is the expression that specifically defines this function\u0026rsquo;s value. To emphasize that $f^{\\ast}(p)$ itself is a function, let\u0026rsquo;s use the following notation.\n$$ (f^{\\ast}\\omega)_{p} = f^{\\ast}\\omega (p) $$\n$\\omega (f(p))$ Since $\\omega$ is a $k$-form of $N$, it maps the point $f(p)$ of $N$ to an element of $\\Lambda^{k}(T_{f(p)}^{\\ast}N)$.\n$$ \\Lambda^{k} (T_{f(p)}^{\\ast}N) := \\left\\{ \\varphi : \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\nBy the definition of $\\Lambda^{k} (T_{f(p)}^{\\ast}N)$, $\\omega (f(p))$ is also a function. $\\omega (f(p))$ takes \u0026ldquo;$k$ tangent vectors at $f(p)$\u0026rdquo; as variables. Here too, to emphasize that $\\omega (f(p))$ itself is a function, let\u0026rsquo;s use the following notation.\n$$ \\omega_{f(p)} = \\omega (f(p)) $$\n$df_{p}v_{i}$ $$ df_{p} : T_{p}M \\to T_{f(p)}N $$\nFor $f : M \\to N$, the differential $df_{p}$ of $f$ is defined as above. Therefore, if $v_{i} \\in T_{p}M$, then $df_{p}v_{i} = df_{p}(v_{i})$ is an element of $T_{f(p)}N$.\nNow, combining these, we obtain $(1)$.\n$$ (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) := \\omega_{f(p)}\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M $$\nThe domains of these two functions show the following difference.\n$$ \\begin{align*} (f^{\\ast}\\omega)_{p} : \u0026amp;\u0026amp; \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\\\ \\omega_{f(p)} : \u0026amp;\u0026amp; \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\end{align*} $$\nThink of the differential $df_{p} : T_{p}M \\to T_{f(p)}N$ as bridging this difference. Hence, $df_{p}$ is also called push forward. For a $1$-form $\\varphi$, the following holds true.\n$$ \\begin{equation} \\varphi( dfv) = f^{\\ast}\\varphi(v) \\end{equation} $$\nPullback of $0$-forms Let\u0026rsquo;s consider $f : M \\to N$ as a function defined between two differential manifolds. Let $g : N \\to \\mathbb{R}$ be a function (a $0$-form of $N$). The pullback $f^{\\ast}g : M \\to \\mathbb{R}$ is defined as the following function (a $0$-form of $M$).\n$$ f^{\\ast}g := g \\circ f $$\nCoordinate Transformation Let\u0026rsquo;s assume a function $f : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$ is given. Let $\\mathbf{x} = (x_{1}, \\dots ,x_{n}) \\in \\mathbb{R}^{n}$, and $\\mathbf{y} = (y_{1}, \\dots ,y_{m}) \\in \\mathbb{R}^{m}$.\n$$ f(x_{1}, \\dots, x_{n}) = (f_{1}(\\mathbf{x}), \\dots, f_{m}(\\mathbf{x}) )= (y_{1}, \\dots ,y_{m}) $$\nAnd let $\\omega = \\sum\\limits_{I} a_{I} dy_{I}$ be a $k$-form on $\\mathbb{R}^{m}$. Then the pullback $f^{\\ast}\\omega$ is as follows, based on these properties.\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= f^{\\ast} \\left( \\sum a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast} \\left( a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}dy_{I} \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}(dy_{i1} \\wedge \\cdots \\wedge dy_{ik}) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} (f^{\\ast}dy_{i1} \\wedge \\cdots \\wedge f^{\\ast}dy_{ik}) \\end{align*} $$\nHere, due to $(2)$, $f^{\\ast}dy_{i1}(v) = dy_{i1}(df(v)) = d(y_{i1}\\circ f)(v) = df_{i1}(v)$, and $f^{\\ast}a_{I} = a_{I} \\circ f$, so,\n$$ \\begin{equation} f^{\\ast} \\omega = \\sum a_{I}(f_{1}, \\dots f_{m}) df_{i1} \\wedge \\cdots \\wedge df_{ik} \\end{equation} $$\nThis formula signifies coordinate transformation. Let\u0026rsquo;s see how it specifically works in the following example.\nExample Let\u0026rsquo;s assume a $1$-form $\\omega$ on $\\mathbb{R}^{2} \\setminus \\left\\{ 0, 0 \\right\\}$ is as follows.\n$$ \\omega = - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = a_{1}dx + a_{2}dy $$\nLet\u0026rsquo;s transform this $1$-form in Cartesian coordinates to polar coordinates. Let $U = \\left\\{ (r,\\theta) : 0 \\lt r, 0 \\le \\theta \\lt 2\\pi \\right\\}$. And let $f : U \\to \\mathbb{R}^{2}$ be as follows.\n$$ f(r,\\theta) = (r\\cos\\theta, r\\sin\\theta) = (f_{1}, f_{2}) $$\nNow, let\u0026rsquo;s calculate $df_{1}, df_{2}$. Since $f_{1} = r\\cos\\theta, f_{2}=r\\sin\\theta$,\n$$ \\begin{align*} df_{1} \u0026amp;= \\dfrac{\\partial f_{1}}{\\partial r}dr + \\dfrac{\\partial f_{1}}{\\partial \\theta}d\\theta = \\cos\\theta dr - r \\sin \\theta d\\theta \\\\ df_{2} \u0026amp;= \\dfrac{\\partial f_{2}}{\\partial r}dr + \\dfrac{\\partial f_{2}}{\\partial \\theta}d\\theta = \\sin\\theta dr + r \\cos \\theta d\\theta \\\\ \\end{align*} $$\nThen, by $(3)$,\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= a_{1}(f_{1}, f_{2})df_{1} + a_{2}(f_{1}, f_{2})df_{2} \\\\ \u0026amp;= - \\dfrac{f_{2}}{f_{1}^{2} + f_{2}^{2}}(\\cos\\theta dr - r \\sin \\theta d\\theta) + \\dfrac{f_{1}}{f_{1}^{2} + f_{2}^{2}}df_{2}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= - \\dfrac{r\\sin\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\cos\\theta dr - r \\sin \\theta d\\theta) \\\\ \u0026amp;\\quad + \\dfrac{r\\cos\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= -\\dfrac{\\sin\\theta \\cos\\theta}{r}dr + \\sin^{2}\\theta d\\theta + \\dfrac{\\cos\\theta \\sin\\theta}{r}dr + \\cos^{2}\\theta d\\theta \\\\ \u0026amp;= d\\theta \\end{align*} $$\nTherefore,\n$$ \\int - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = \\int d\\theta $$\n■\nProperties Let $M, N$ be differential manifolds of dimensions $m, n$ respectively, and let $f : M \\to N$. Let $\\omega, \\varphi$ be $k$-forms on $N$. Let $g$ be a $0$-form on $N$. Let $\\varphi_{i}$s be $1$-forms on $N. Then, the following hold true.\n$$ \\begin{align} f^{\\ast} (\\omega + \\varphi) =\u0026amp;\\ f^{\\ast}\\omega + f^{\\ast}\\varphi \\tag{a} \\\\ f^{\\ast} (g \\omega) =\u0026amp;\\ (f^{\\ast}g) (f^{\\ast}\\omega) \\tag{b} \\\\ f^{\\ast} (\\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k}) =\u0026amp;\\ f^{\\ast}(\\varphi_{1}) \\wedge \\cdots \\wedge f^{\\ast}(\\varphi_{k}) \\tag{c} \\end{align} $$\nHere, $+$ and $\\wedge$ represent the sum and wedge product of $k$-forms, respectively.\nLet $\\omega, \\varphi$ be arbitrary forms on $N. Let $L$ be a $l$-dimensional differential manifold, and let $g : L \\to N$.\n$$ \\begin{align*} f^{\\ast}(\\omega \\wedge \\varphi) \u0026amp;= (f^{\\ast}\\omega) \\wedge (f^{\\ast}\\varphi) \\tag{d} \\\\ (f \\circ g)^{\\ast} \\omega \u0026amp;= g^{\\ast}(f^{\\ast}\\omega) \\tag{e} \\end{align*} $$\nProof Proof $(a)$ $$ \\begin{align*} (f^{\\ast}(\\omega + \\varphi))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\omega + \\varphi)_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ \\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) + \\varphi_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ (f^{\\ast} \\omega)_{p}(v_{1}, \\dots, v_{k}) + (f^{\\ast} \\varphi)_{p}(v_{1}, \\dots, v_{k}) \\\\ =\u0026amp;\\ \\left( f^{\\ast}\\omega + f^{\\ast}\\varphi \\right)_{p}(v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\nProof $(b)$ Let\u0026rsquo;s define the product of a $0$-form $g$ and a $k$-form $\\omega$ as follows.\n$$ (g\\omega)(p) = g(p) \\omega (p) $$\nNote that $g(p) = g_{p}$ is a scalar, and $\\omega (p) = \\omega_{p}$ is a function. Then,\n$$ \\begin{align*} (f^{\\ast} (g\\omega))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ g\\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ g_{f(p)} \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ g\\circ f(p) \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ (f^{\\ast}g)_{p} (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\nProof $(c)$ $$ \\begin{align*} (f^{\\ast}\\left( \\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k} \\right))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\varphi_{1} \\wedge \\dots \\wedge \\varphi_{k})_{f(p)} \\left( df_{1}, \\dots, df_{k} \\right) \\\\ =\u0026amp;\\ \\det [\\varphi_{i}df(v_{j})] \\\\ =\u0026amp;\\ \\det [ f^{\\ast} \\varphi_{i}(v_{j})] \\\\ \\end{align*} $$\n■\nManfredo P. Do Carmo, Differential Forms and Applications, p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3262,"permalink":"https://freshrimpsushi.github.io/en/posts/3262/","tags":null,"title":"Pull Back in Differential Geometry"},{"categories":"줄리아","contents":"Overview Despite many languages supporting data frames, surprisingly creating empty arrays is always a new and annoying task.\nCode Specifying Types In fact, you just have to put an empty array as data. At this time, the type is specified, but if there is no data at all, neither column names nor types are shown. ```code2 If you insert data, the column names and types are displayed correctly. Be careful as data will not be added if the types do not match. ### Without Specifying Types ```code3 If you don\u0026#39;t want to stress about the type of the data frame, simply create an empty array of `Any` like the above. Unlike specifying types, you can confirm that the data has been properly added. ## Environment - OS: Windows - julia: v1.6.2 ","id":2230,"permalink":"https://freshrimpsushi.github.io/en/posts/2230/","tags":null,"title":"Creating an Empty DataFrame in Julia"},{"categories":"줄리아","contents":"Explanation Use the hclust() function from the Clustering.jl package.\nhclust(d::AbstractMatrix; [linkage], [uplo], [branchorder]) It takes a distance matrix as input and returns the result of hierarchical clustering. The default method for calculating distances between clusters is single linkage.\nTo plot a dendrogram, use StatsPlots.jl instead of Plots.jl.\nCode using StatsPlots using Clustering using Distances using Distributions\ra = rand(Uniform(-1,1), 2, 25)\rscatt = scatter(a[1,:], a[2,:], label=false)\rsavefig(scatt, \u0026#34;julia_hclust_scatter.png\u0026#34;) D_a = pairwise(Euclidean(), a, a)\rSL = hclust(D_a, linkage=:single)\rplot_SL = plot(SL)\rp = plot(scatt, plot_SL, size=(800,400))\rsavefig(p, \u0026#34;julia_hclust.png\u0026#34;) ","id":3259,"permalink":"https://freshrimpsushi.github.io/en/posts/3259/","tags":null,"title":"How to Perform Hierarchical Clustering in Julia"},{"categories":"줄리아","contents":"Code julia\u0026gt; findfirst(\u0026#34;li\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r8:9\rjulia\u0026gt; findlast(\u0026#34;li\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r14:15\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 1)\r3:3\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 4)\r8:8\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 9)\r14:14\rjulia\u0026gt; findfirst(r\u0026#34;t.+t\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r4:16 findfirst(pattern, A)\nReturns a Range representing the interval matching pattern in the string A. The pattern can include regular expressions. In the last example, it finds and returns the interval from the first t to the last t. ","id":2226,"permalink":"https://freshrimpsushi.github.io/en/posts/2226/","tags":null,"title":"How to Find a Specific Pattern Location in Julia Strings"},{"categories":"줄리아","contents":"Explanation When attempting to draw a dendrogram by using the plot() function after performing hierarchical clustering with hclust() on the given data, the following error occurs.\nusing Clustering using Distances using Plots\ra = rand(2, 10)\rD_a = pairwise(Euclidean(), a, a)\rSL = hclust(D_a, linkage=:single)\rdendrogram = plot(SL)\rERROR: LoadError: Cannot convert Hclust{Float64} to series data for plotting To draw a dendrogram, one should use StatsPlots.jl instead of Plots.jl.\nusing StatsPlots\rdendrogram = plot(SL)\rsavefig(dendrogram, \u0026#34;julia_dendrogram.png\u0026#34;) ","id":3257,"permalink":"https://freshrimpsushi.github.io/en/posts/3257/","tags":null,"title":"How to Draw a Dendrogram in Julia"},{"categories":"줄리아","contents":"Code julia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, \u0026#34;er\u0026#34;)\rtrue\rjulia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, \u0026#34;et\u0026#34;)\rfalse\rjulia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, r\u0026#34;q?\u0026#34;)\rtrue contains(haystack::AbstractString, needle)\nReturns a boolean indicating whether needle is contained in haystack. needle can include regular expressions, such as r\u0026quot;...\u0026quot;. Note that \u0026lsquo;haystack\u0026rsquo; means a hay pile, referring to the idiom \u0026ldquo;a needle in a haystack\u0026rdquo;.\nEnvironment OS: Windows julia: v1.6.2 ","id":2224,"permalink":"https://freshrimpsushi.github.io/en/posts/2224/","tags":null,"title":"How to Check if a Specific String is Contained in Julia"},{"categories":"줄리아","contents":"Overview Primes.jl is a package that deals with functions related to primes and prime factorization. The implementation of functions related to analytic number theory is still lacking.\nThis is not a comprehensive list of all the features of the package, but rather a selection of the useful ones. For more details, check the repository1.\nTypes Prime Factorization Primes.Factorization julia\u0026gt; factor(12)\r2^2 * 3\rjulia\u0026gt; factor(12)[1]\r0\rjulia\u0026gt; factor(12)[2]\r2\rjulia\u0026gt; factor(12)[3]\r1\rjulia\u0026gt; factor(12)[4]\r0 Prime factorization uses a unique data type that differentiates between base and exponent. Accessing a base as an index allows you to reference its exponent.\nFunctions Generating Primes prime(), primes() julia\u0026gt; using Primes\rjulia\u0026gt; prime(4)\r7\rjulia\u0026gt; primes(10)\r4-element Vector{Int64}:\r2\r3\r5\r7 prime(::Type{\u0026lt;:Integer}=Int, i::Integer)\nReturns the ith prime. primes([lo,] hi)\nReturns an array of primes up to hi. Prime Testing isprime() julia\u0026gt; isprime(7)\rtrue\rjulia\u0026gt; isprime(8)\rfalse isprime(n::Integer)\nReturns a boolean indicating whether n is a prime. This function\u0026rsquo;s implementation uses the Miller-Rabin primality test among others. Prime Factorization factor() julia\u0026gt; factor(24)\r2^3 * 3\rjulia\u0026gt; factor(Vector, 24)\r4-element Vector{Int64}:\r2\r2\r2\r3\rjulia\u0026gt; factor(Array, 24)\r4-element Vector{Int64}:\r2\r2\r2\r3\rjulia\u0026gt; factor(Set, 24)\rSet{Int64} with 2 elements:\r2\r3 factor(n::Integer) -\u0026gt; Primes.Factorization factor(ContainerType, n::Integer) -\u0026gt; ContainerType\nReturns the prime factorization of n. The implementation of this function uses the Pollard\u0026rsquo;s $p-1$ rho algorithm among others. If ContainerType is provided, it returns the result in the specified container, otherwise, it returns in its own data type, Primes.Factorization. Euler\u0026rsquo;s Totient Function julia\u0026gt; totient(12)\r4 totient(n::Integer)\nFor n=$n$, returns $\\displaystyle n \\prod_{p \\mid n} \\left( 1 - {{ 1 } \\over { p }} \\right)$ using Euler\u0026rsquo;s totient function $\\phi$. https://github.com/JuliaMath/Primes.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2222,"permalink":"https://freshrimpsushi.github.io/en/posts/2222/","tags":null,"title":"How to Use Factorization and Prime Number Functions in Julia"},{"categories":"줄리아","contents":"Overview Polynomials.jl is a package that includes the representation and computation of polynomial functions. Since polynomials are mathematically simple, there\u0026rsquo;s a tendency to think their coding should be straightforward, too. However, once you start implementing the necessary features, it can become quite cumbersome. Of course, it\u0026rsquo;s not extremely difficult, but it\u0026rsquo;s generally better to use a package whenever possible.\nThis is not an exhaustive list of all features of the package, but rather a selection of potentially useful ones, so check the repository for more details1.\nGeneral Polynomial Functions Define a Polynomial Function by Coefficients Polynomial() julia\u0026gt; using Polynomials\rjulia\u0026gt; p = Polynomial([1,0,0,1])\rPolynomial(1 + x^3)\rjulia\u0026gt; q = Polynomial([1,1])\rPolynomial(1 + x)\rjulia\u0026gt; r = Polynomial([1,1], :t)\rPolynomial(1 + t)\rjulia\u0026gt; p(0)\r1\rjulia\u0026gt; p(2)\r9 Polynomial{T, X}(coeffs::AbstractVector{T}, [var = :x])\nIt returns the polynomial function itself. coeffs is an array of coefficients, with the constant term at the beginning and higher-order terms towards the end. var specifies the polynomial\u0026rsquo;s variable. As shown in the example, entering the symbol :t makes it a polynomial in t. Define a Polynomial Function by Data fit() julia\u0026gt; fit([-1,0,1], [2,0,2])\rPolynomial(2.0*x^2) fit(::Type{RationalFunction}, xs::AbstractVector{S}, ys::AbstractVector{T}, m, n; var=:x)\nIt returns the polynomial function that passes through points with coordinates $x$ at xs and $y$ at ys. Define a Polynomial Function by Roots roots() julia\u0026gt; fromroots([-2,2])\rPolynomial(-4 + x^2) fromroots(::AbstractVector{\u0026lt;:Number}; var=:x)\nIt returns the polynomial function with the provided array\u0026rsquo;s elements as roots. Operations +, -, *, ÷ julia\u0026gt; p + 1\rPolynomial(2 + x^3)\rjulia\u0026gt; 2p\rPolynomial(2 + 2*x^3) Adding or multiplying by a scalar results in the intuitive operation as above.\njulia\u0026gt; p + q\rPolynomial(2 + x + x^3)\rjulia\u0026gt; p - q\rPolynomial(-x + x^3)\rjulia\u0026gt; p * q\rPolynomial(1 + x + x^3 + x^4)\rjulia\u0026gt; p ÷ q\rPolynomial(1.0 - 1.0*x + 1.0*x^2) Arithmetic operations between polynomial functions are overridden with +, -, *, ÷.\nFinding Roots roots() julia\u0026gt; roots(p)\r3-element Vector{ComplexF64}:\r-1.0 + 0.0im\r0.4999999999999998 - 0.8660254037844383im\r0.4999999999999998 + 0.8660254037844383im roots(f::AbstractPolynomial)\nIt returns the roots of f as a vector. Differentiation derivative() julia\u0026gt; derivative(p, 3)\rPolynomial(6) derivative(f::AbstractPolynomial, order::Int = 1)\nIt returns the orderth derivative of f. Integration integrate() julia\u0026gt; integrate(p, 7)\rPolynomial(7.0 + 1.0*x + 0.25*x^4) integrate(f::AbstractPolynomial, C = 0)\nIt returns the indefinite integral of f with an integration constant of C. Special Polynomial Functions Laurent Polynomial Functions LaurentPolynomial() julia\u0026gt; LaurentPolynomial([4,3,2,1], -1)\rLaurentPolynomial(4*x⁻¹ + 3 + 2*x + x²) LaurentPolynomial{T,X}(coeffs::AbstractVector, [m::Integer = 0], [var = :x])\nIt returns the Laurent polynomial function extended to integers for degree. The degree of the smallest term is given as m. Chebyshev Polynomial Functions ChebyshevT() julia\u0026gt; ChebyshevT([3,2,1])\rChebyshevT(3⋅T_0(x) + 2⋅T_1(x) + 1⋅T_2(x)) ChebyshevT{T, X}(coeffs::AbstractVector)\nIt returns the Type I Chebyshev polynomial function. The formula\u0026rsquo;s T_n(x) is represented by $T_{n}(x) = \\cos \\left( n \\cos^{-1} x \\right)$. Environment OS: Windows julia: v1.6.2 https://juliamath.github.io/Polynomials.jl/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2220,"permalink":"https://freshrimpsushi.github.io/en/posts/2220/","tags":null,"title":"How to Use Polynomials in Julia"},{"categories":"줄리아","contents":"Code Concatenate Strings * julia\u0026gt; \u0026#34;oh\u0026#34; * \u0026#34;my\u0026#34; * \u0026#34;girl\u0026#34;\r\u0026#34;ohmygirl\u0026#34; Corresponds to the + in Python.\nConcatenate Multiple Strings string() julia\u0026gt; string(\u0026#34;oh\u0026#34;,\u0026#34;my\u0026#34;, \u0026#34;girl\u0026#34;)\r\u0026#34;ohmygirl\u0026#34; Corresponds to paste0() in R.\nJoining Items of a List of Strings join() julia\u0026gt; OMG = [\u0026#34;oh\u0026#34;,\u0026#34;my\u0026#34;, \u0026#34;girl\u0026#34;]\r3-element Vector{String}:\r\u0026#34;oh\u0026#34;\r\u0026#34;my\u0026#34;\r\u0026#34;girl\u0026#34;\rjulia\u0026gt; join(OMG)\r\u0026#34;ohmygirl\u0026#34; Corresponds to join() in Python.\nRepeat the Same String ^ julia\u0026gt; \u0026#34;=-\u0026#34; ^ 10\r\u0026#34;=-=-=-=-=-=-=-=-=-=-\u0026#34; Corresponds to * in Python. The expression of repetition as exponentiation is no coincidence, as in Python, the binary operation for connecting strings is +(sum), and repeating it is *(product). Similarly, in Julia, the operation for connecting strings is *(product), and repeating it becomes ^(exponent).\nWhy? Why not use +, which is more intuitive and easy to understand as in other languages? It\u0026rsquo;s because, from an algebraic, mathematical perspective, merging strings is closer to multiplication than addition and is more natural1. It would be best if you could understand what a Free Group in Algebra means, but even without such background knowledge, the representation of the product of x and y as x * y = xy in mathematics is acceptable. $$ x \\ast y = xy $$ Now consider appending the string \u0026quot;litol\u0026quot; to \u0026quot;xy\u0026quot;, creating \u0026quot;xylitol\u0026quot;. $$ xy \\ast litol = xylitol $$ It makes sense. Thinking about \u0026quot;xy\u0026quot; + \u0026quot;litol\u0026quot; now might feel a bit strange. The people who made Julia are seriously committed to such mathematical intuition.\nEnvironment OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/why-are-there-all-these-strange-stumbling-blocks-in-julia/92644/40\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2218,"permalink":"https://freshrimpsushi.github.io/en/posts/2218/","tags":null,"title":"Concatenating Strings in Julia"},{"categories":"줄리아","contents":"Code 1 using Plots\rx = rand(30)\ry = rand(30)\rz = rand(30)\rplot(x)\rplot!(y)\rplot!(z)\rpng(\u0026#34;result1\u0026#34;) In some cases, you might want to make only certain data labels appear in the legend, as shown above.\nlabel = \u0026quot;\u0026quot; plot(x, label = \u0026#34;\u0026#34;)\rplot!(y)\rpng(\u0026#34;result2\u0026#34;) In such cases, you can set the option label = \u0026quot;\u0026quot;. As you can see, the first data is displayed in the figure, but it does not appear in the legend.\nprimary = false plot!(z, primary = false)\rpng(\u0026#34;result3\u0026#34;) Another way is to set the primary = false option, as shown. The last data is plotted in orange and hidden in the legend. Since this is a side effect of turning off primary, it\u0026rsquo;s better to only tweak the label option if possible.\nEnvironment OS: Windows julia: v1.6.2 https://github.com/JuliaPlots/Plots.jl/issues/1388#issuecomment-363940741\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2216,"permalink":"https://freshrimpsushi.github.io/en/posts/2216/","tags":null,"title":"Hiding Specific Data Labels in Julia Plots"},{"categories":"줄리아","contents":"Code 1 You can use annotate!(). The following code is for drawing a picture that marks the maximum and minimum points in Brownian motion.\nusing Plots\rcd(@__DIR__)\rdata = cumsum(randn(100))\rplot(data, color = :black, legend = :none)\rannotate!(argmax(data), maximum(data), \u0026#34;max\\n\u0026#34;)\rannotate!(argmin(data), minimum(data), \u0026#34;\\nmin\u0026#34;)\rpng(\u0026#34;result\u0026#34;) Environment OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/plots-annotate/37784\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2214,"permalink":"https://freshrimpsushi.github.io/en/posts/2214/","tags":null,"title":"How to Insert Text into a Julia Plot"},{"categories":"줄리아","contents":"Environment OS: Windows julia: v1.6.2 Error julia\u0026gt; plot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rGKS: glyph missing from current font: 48652\rGKS: glyph missing from current font: 46972\rGKS: glyph missing from current font: 50868\rGKS: glyph missing from current font: 47784\rGKS: glyph missing from current font: 49496 Cause The issue is due to the inability to find Korean fonts.\nSolution Both methods are not ideal, and alternative suggestions are always welcome. It is a fact that support for Korean is limited given that using Julia often does not require much Korean.\ndefault(fontfamily = \u0026quot;\u0026quot;) 1 plot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;, fontfamily = \u0026#34;\u0026#34;)\rdefault(fontfamily = \u0026#34;\u0026#34;)\rplot(data, color = :red,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;) Korean can be rendered by setting the plot() option fontfamily = \u0026quot;\u0026quot; or using default(fontfamily = \u0026quot;\u0026quot;). However, even if the specific font name is changed, it is confirmed that it does not recognize them properly, and when saving, it ends up not finding the font, resulting in the text being displayed as a blank space instead of being garbled as shown in the first image.\nPlots.plotly() 2 Plots.plotly()\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.html\u0026#34;) Using the plotly backend does render the Korean characters. However, it cannot save directly to *.png. Instead, it requires outputting to *.html first and then saving separately.\nCode using Plots\r#Plots.gr()\rdata = cumsum(randn(100))\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;, fontfamily = \u0026#34;\u0026#34;)\rdefault(fontfamily = \u0026#34;\u0026#34;)\rplot(data, color = :red,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.png\u0026#34;)\rPlots.plotly()\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.html\u0026#34;) https://discourse.julialang.org/t/nice-fonts-with-plots-gr-and-latexstrings/60037\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://besixdouze.net/16\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2212,"permalink":"https://freshrimpsushi.github.io/en/posts/2212/","tags":null,"title":"Inserting Korean Text into Julia Plots"},{"categories":"기하학","contents":"Gauss-Bonnet Theorem Let\u0026rsquo;s consider $\\mathbf{x} : U \\to \\mathbb{R}^{3}$ as a simple connected geodesic coordinate chart, and $\\boldsymbol{\\gamma}(I) \\subset \\mathbf{x}(U)$, which is $\\boldsymbol{\\gamma}$, as piecewise regular curves. Also, let\u0026rsquo;s say that $\\boldsymbol{\\gamma}$ surrounds some region $\\mathscr{R}$. Then, the following holds true.\n$$ \\iint_{\\mathscr{R}} K dA + \\int_{\\boldsymbol{\\gamma}} \\kappa_{g} ds + \\sum \\alpha_{i} = 2\\pi $$\nHere, $K$ denotes the Gaussian curvature, $\\kappa_{g}$ denotes the geodesic curvature, and $\\alpha_{i}$ denotes the difference in angles at the junction point between intervals of $\\boldsymbol{\\gamma}$, referred to as jump angles.\nExplanation Since $\\boldsymbol{\\gamma}$ is assumed to be a curve that is regular piecewise, there will be points where the direction of the tangent suddenly changes, and the difference in angles at those points is denoted as $\\alpha_{i}$. If $\\boldsymbol{\\gamma}$ is a curve that smoothly connects overall, there will be no points where the angle jumps, hence, such $\\alpha_{i}$s are $0$. (See figure (a))\nThe theorem above is a result when the strong condition of being a geodesic coordinate chart is applied. In more general results, the Euler characteristic appears in the formula, as follows.\n$$ \\iint_{\\mathscr{R}} K dA + \\int_{C_{i}}\\kappa_{g}ds + \\sum\\alpha_{i} = 2\\pi \\chi(\\mathscr{R}) $$\nProof Since $\\mathbf{x}$ is a geodesic coordinate chart, let\u0026rsquo;s set the coefficients of the first fundamental form as follows.\n$$ \\left[ g_{ij} \\right] = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; h^{2} \\end{bmatrix} $$\nAnd let\u0026rsquo;s denote $\\boldsymbol{\\gamma}(t) = \\mathbf{x}\\left( \\gamma^{1}(t), \\gamma^{2}(t) \\right)$. Now, let\u0026rsquo;s denote the angle between the tangents $\\mathbf{x}_{1}$ and $\\boldsymbol{\\gamma}$ as $T = \\boldsymbol{\\gamma}^{\\prime}$.\n$$ \\alpha (t) := \\angle ( \\mathbf{x}_{1}, T) $$\nWe will prove the theorem using the fact that the angle change of $\\alpha$ around the path, based on $\\boldsymbol{\\gamma}$, when making a full circle, is $\\mathbf{x}_{1}$. First, assume $T$ as a unit speed curve. And let\u0026rsquo;s denote $2 \\pi$ as a parallel vector field along $\\boldsymbol{\\gamma}$ satisfying the following. (Refer to the above figure (b))\n$$ P(t) = \\text{parallel vector field starting from a juction point s.t. } \\dfrac{P \\times T}{\\left\\| P \\times T \\right\\|} = \\mathbf{n} $$\nAnd let\u0026rsquo;s denote $P$ and $\\boldsymbol{\\gamma}$ respectively as the angles between $\\phi$ and $\\theta$, and $\\mathbf{x_{1}}$ and $P$.\n$$ \\phi (t) = \\angle(\\mathbf{x}_{1}, P),\\quad \\theta (t) = \\angle(P, T) $$\nIn other words, $P$, and differentiating this,\n$$ -\\sin \\phi (t) \\dfrac{d \\phi}{d t}(t) = \\left\\langle \\dfrac{d \\mathbf{x}_{1}(\\gamma^{1}(t), \\gamma^{2}(t))}{d t}, P(t) \\right\\rangle + \\left\\langle \\mathbf{x}_{1}, \\dfrac{d P}{d t}(t) \\right\\rangle $$\nSince $T$ is a parallel vector field along $\\left\\langle \\mathbf{x}_{1}, P(t) \\right\\rangle = \\cos\\phi (t)$, by definition, $P$ is perpendicular to $\\gamma$. $\\dfrac{dP}{dt}$ is tangent to $M$, so the latter term is $\\mathbf{x}_{1}$. Further calculations reveal,\n$$ \\begin{align*} \u0026amp;\\quad -\\sin \\phi (t) \\dfrac{d \\phi}{d t}(t) \\\\ \u0026amp;= \\left\\langle \\dfrac{d \\mathbf{x}_{1}(\\gamma^{1}(t), \\gamma^{2}(t))}{d t}, P(t) \\right\\rangle \\\\ \u0026amp;= \\Big[ \\mathbf{x}_{11}(\\gamma^{1}(t), \\gamma^{2}(t)) (\\gamma^{1})^{\\prime}(t) + \\mathbf{x}_{12}(\\gamma^{1}(t), \\gamma^{2}(t)) (\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\\\ \u0026amp;= \\Big[ \\left( L_{11}\\mathbf{n} + \\Gamma_{11}^{1}\\mathbf{x}_{1} + \\Gamma_{11}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{1})^{\\prime}(t) + \\left( L_{12}\\mathbf{n} + \\Gamma_{12}^{1}\\mathbf{x}_{1} + \\Gamma_{12}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\\\ \u0026amp;= \\Big[ \\left(\\Gamma_{11}^{1}\\mathbf{x}_{1} + \\Gamma_{11}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{1})^{\\prime}(t) + \\left(\\Gamma_{12}^{1}\\mathbf{x}_{1} + \\Gamma_{12}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\end{align*} $$\nThe second equality is due to the chain rule, the third equality is due to the definitions of the second fundamental form and the Christoffel symbols, and the fourth equality holds because $M$ and $0$ are perpendicular to each other.\nChristoffel symbols of the geodesic coordinate chart\nExcept for the below, everything is $P$.\n$$ \\Gamma_{22}^{1} = -hh_{1},\\quad \\Gamma_{12}^{2} = \\Gamma_{21}^{2} = \\dfrac{h_{1}}{h},\\quad \\Gamma_{22}^{2} = \\dfrac{h_{2}}{h} $$\nNow, organizing the terms that become $\\mathbf{n}$ yields the following.\n$$ -\\sin\\phi (t) \\phi^{\\prime}(t) = \\left\\langle \\dfrac{h_{1}}{h}(\\gamma^{2})^{\\prime}(t)\\mathbf{x}_{2}, P(t) \\right\\rangle = \\dfrac{h_{1}}{h}(\\gamma^{2})^{\\prime}(t) \\left\\langle \\mathbf{x}_{2}, P(t) \\right\\rangle\\tag{1} $$\nSince $0$, $0$ is a unit vector, and because $g_{11} = \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{1} \\right\\rangle = 1$, $\\mathbf{x}_{1}$ holds. Therefore, $g_{12} = \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\rangle = 0$ becomes a normal orthogonal basis of the tangent plane. Hence, any element $\\mathbf{x}_{1} \\perp \\mathbf{x}_{2}$ of the tangent plane is expressed as below.\n$$ P = \\left\\langle \\mathbf{x}_{1}, P \\right\\rangle\\mathbf{x}_{1} + \\left\\langle \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|}, P \\right\\rangle \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|} = \\cos\\phi \\mathbf{x}_{1} + \\sin\\phi \\dfrac{\\mathbf{x}_{2}}{h} $$\nAlso, substituting $\\left\\{ \\mathbf{x}_{1}, \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|} \\right\\}$ into $P$,\n$$ \\phi^{\\prime}(t) = -h_{1}(\\gamma^{2})^{\\prime}(t) $$\nTherefore, the total angle variation of $\\left\\langle \\mathbf{x}_{2}, P \\right\\rangle = \\left\\| x_{2} \\right\\|^{2} \\dfrac{\\sin \\phi}{h} = h\\sin \\phi$ is\n$$ \\delta \\phi = \\int_{\\boldsymbol{\\gamma}} \\phi^{\\prime} dt = - \\int_{\\boldsymbol{\\gamma}}h_{1}(\\gamma^{2})^{\\prime}(t)dt = - \\int_{\\boldsymbol{\\gamma}}h_{1} d\\gamma^{2} = - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} \\tag{2} $$\nMoreover, the following equation will now be shown to hold.\n$$ \\text{Claim: } \\theta^{\\prime} = k_{g} $$\nSince $(1)$ is assumed, $\\phi$ holds, and differentiating this,\n$$ -\\sin\\theta (t)\\theta^{\\prime}(t) = \\left\\langle \\dfrac{d P}{d t}, T \\right\\rangle + \\left\\langle P, \\dfrac{d T}{d t} \\right\\rangle = \\left\\langle P, T^{\\prime} \\right\\rangle $$\nThe second equality holds because $\\boldsymbol{\\gamma}$ is parallel to $\\mathbf{x}$. By the definition of geodesic curvature, we obtain what we intend to show as follows.\n$$ \\begin{align*} \\kappa_{g} = \\left\\langle \\mathbf{S}, T^{\\prime} \\right\\rangle \u0026amp;= \\left\\langle (\\mathbf{n} \\times T), T^{\\prime} \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\mathbf{n}, (T \\times T^{\\prime}) \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\dfrac{P \\times T}{\\sin \\theta}, (T\\times T^{\\prime}) \\right\\rangle \u0026amp; \\because \\dfrac{P \\times T}{\\left\\| P \\times T \\right\\|} = \\mathbf{n} \\\\ \u0026amp;= \\left\\langle \\dfrac{1}{\\sin\\theta} P, (T\\times (T\\times T^{\\prime})) \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\dfrac{1}{\\sin\\theta} P, -T^{\\prime} \\right\\rangle \\\\ \u0026amp;= \\theta^{\\prime}(t) \\end{align*} $$\nThe third and fifth equalities hold because the scalar triple product is commutative. Thus, we obtain the following.\n$$ \\delta \\theta = \\int_{\\boldsymbol{\\gamma}} \\theta^{\\prime} dt = \\int_{\\boldsymbol{\\gamma}} k_{g}dt \\tag{3} $$\nSince $u^{2}-$,\n$$ \\int_{\\boldsymbol{\\gamma}} \\alpha^{\\prime}dt = \\int_{\\boldsymbol{\\gamma}} \\phi^{\\prime}dt + \\int_{\\boldsymbol{\\gamma}} \\theta^{\\prime}dt $$\nDue to $\\theta (t) = \\angle(P, T)$ and $\\cos\\theta (t) = \\left\\langle P, T \\right\\rangle$, we obtain the following.\n$$ \\int_{\\boldsymbol{\\gamma}} \\alpha^{\\prime}dt + \\sum_{i}\\alpha_{i} = - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} + \\int_{\\boldsymbol{\\gamma}} k_{g}dt + \\sum_{i}\\alpha_{i} $$\n$dP/dt$ surrounds $\\mathbf{n}$, so the left-hand side of the above equation is clearly the angle change of one full rotation, which is $\\alpha = \\phi + \\theta$.\n$$ {} - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} + \\int_{\\boldsymbol{\\gamma}} k_{g}dt + \\sum_{i}\\alpha_{i} = 2 \\pi $$\nGreen\u0026rsquo;s Theorem\n$$ \\oint_{\\partial \\mathscr{R}} Pdx = - \\iint_{\\mathscr{R}} P_{y} dy dx $$\nGaussian curvature of the geodesic coordinate chart\n$$ K = -\\dfrac{h_{11}}{h} $$\nArea element of the surface\n$$ dA = \\sqrt{g} du^{1} du^{2} $$\nThe first term on the left-hand side can be rewritten using Green\u0026rsquo;s Theorem as follows.\n$$ \\begin{align*} {} - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} \u0026amp;= - \\iint_{\\mathscr{R}}h_{11} du^{1}du^{2} \\\\ \u0026amp;= - \\iint_{\\mathscr{R}}\\dfrac{h_{11}}{h} h du^{1}du^{2} \\\\ \u0026amp;= - \\iint_{\\mathscr{R}}\\dfrac{h_{11}}{h} \\sqrt{g} du^{1}du^{2} \\\\ \u0026amp;= \\iint_{\\mathscr{R}} K dA \\end{align*} $$\nFinally, we arrive at the following conclusion.\n$$ \\iint_{R} K dA + \\int_{\\gamma} \\kappa_{g} ds + \\sum \\alpha_{i} = 2\\pi $$\n","id":3238,"permalink":"https://freshrimpsushi.github.io/en/posts/3238/","tags":null,"title":"Gauss-Bonnet Theorem"},{"categories":"줄리아","contents":"Code 1 2 3 julia\u0026gt; replace(\u0026#34;qwerty\u0026#34;, \u0026#34;q\u0026#34;=\u0026gt;\u0026#34;Q\u0026#34;)\r\u0026#34;Qwerty\u0026#34;\rjulia\u0026gt; join(\u0026#34;qwerty\u0026#34;, \u0026#34;,\u0026#34;)\r\u0026#34;q,w,e,r,t,y\u0026#34;\rjulia\u0026gt; split(\u0026#34;qwerty\u0026#34;, \u0026#34;\u0026#34;)\r6-element Vector{SubString{String}}:\r\u0026#34;q\u0026#34;\r\u0026#34;w\u0026#34;\r\u0026#34;e\u0026#34;\r\u0026#34;r\u0026#34;\r\u0026#34;t\u0026#34;\r\u0026#34;y\u0026#34; Julia is not particularly outstanding in string processing, but maybe because of that, it has followed Python closely making it easy and quick to learn. Most of the already known functionalities are implemented, so apart from whether it\u0026rsquo;s a module or not, the usage is almost similar. Notably, when using replace(), \u0026quot;q\u0026quot;=\u0026gt;\u0026quot;Q\u0026quot; is not some unique syntax but directly using a pair.\nhttps://docs.julialang.org/en/v1/base/collections/#Base.replace-Tuple{Any,%20Vararg{Pair,%20N}%20where%20N}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/strings/#Base.join\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/strings/#Base.split\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2205,"permalink":"https://freshrimpsushi.github.io/en/posts/2205/","tags":null,"title":"Handling Strings in Julia like in Python"},{"categories":"줄리아","contents":"Code Using the comparison operator $\\approx$, it only returns true when two values are sufficiently similar. ≈ can be used by entering \\approx and then pressing Tab, just as in $\\TeX$.\njulia\u0026gt; π ≈ 3.141592653\rtrue\rjulia\u0026gt; π ≈ 3.14159265\rtrue\rjulia\u0026gt; π ≈ 3.1415926\rfalse\rjulia\u0026gt; π ≈ 3.141592\rfalse Environment OS: Windows julia: v1.7.0 ","id":2203,"permalink":"https://freshrimpsushi.github.io/en/posts/2203/","tags":null,"title":"How to Check Approximate Values in Julia"},{"categories":"줄리아","contents":"Code 1 julia\u0026gt; d = Dict(\u0026#34;A\u0026#34;=\u0026gt;1, \u0026#34;B\u0026#34;=\u0026gt;2)\rDict{String, Int64} with 2 entries:\r\u0026#34;B\u0026#34; =\u0026gt; 2\r\u0026#34;A\u0026#34; =\u0026gt; 1\rjulia\u0026gt; push!(d,(\u0026#34;C\u0026#34;,3))\rERROR: MethodError: no method matching push!(::Dict{String, Int64}, ::Tuple{String, Int64})\rjulia\u0026gt; push!(d,\u0026#34;C\u0026#34; =\u0026gt; 3)\rDict{String, Int64} with 3 entries:\r\u0026#34;B\u0026#34; =\u0026gt; 2\r\u0026#34;A\u0026#34; =\u0026gt; 1\r\u0026#34;C\u0026#34; =\u0026gt; 3\rjulia\u0026gt; typeof(\u0026#34;C\u0026#34; =\u0026gt; 3)\rPair{String, Int64} Dictionaries in Julia are data types that pair Keys and Values, much like in other programming languages. A slight difference in Julia is that dictionaries are seen as a collection of Pairs. As can be seen in the provided execution example, a pair is an element that constitutes the dictionary. Keys and Values are linked through the right arrow =\u0026gt;, and they themselves take the form of a Pair data type.\nThe following example shows how to replace parts of a string in Julia.\njulia\u0026gt; replace(\u0026#34;qwerty\u0026#34;, \u0026#34;q\u0026#34;=\u0026gt;\u0026#34;Q\u0026#34;)\r\u0026#34;Qwerty\u0026#34; If there\u0026rsquo;s something that sets it apart from Python, it\u0026rsquo;s that pairs can exist independently of a dictionary. Instead of viewing the dictionary as a collection containing just one pair, pairs themselves can be viewed as data, which is why Julia code may utilize pairs in a way that seems like a new syntax. Regardless of whether you use it or not, it\u0026rsquo;s something you should be able to read.\nEnvironment OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/base/collections/#Base.Dict\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2201,"permalink":"https://freshrimpsushi.github.io/en/posts/2201/","tags":null,"title":"From Julia: Dictionaries and Pairs"},{"categories":"줄리아","contents":"Overview JLD.jl is a package that allows the storage of temporary data created while using Julia1. It is useful for managing the input and output of data in pure Julia projects. On the other hand, JLD2.jl, which further improves the intuitiveness of the JLD.jl interface, is also available. 2 The content introduced in this post should be taken as a rough understanding of these functionalities, and it is recommended to use JLD2.jl whenever possible as it supports backward compatibility without issues.\nIf you specifically need to read and write MATLAB\u0026rsquo;s mat files, not just files similar to mat files, then the MAT.jl package can be referred to.\nCode using JLD\rcd(@__DIR__); pwd()\rnumpad = reshape(1:9, 3,3)\rcube = zeros(Int64, 3,3,3)\rsave(\u0026#34;mydata.jld\u0026#34;, \u0026#34;numpad\u0026#34;,numpad, \u0026#34;cube\u0026#34;,cube)\rmydata = load(\u0026#34;mydata.jld\u0026#34;)\rmydata[\u0026#34;numpad\u0026#34;]\rmydata[\u0026#34;cube\u0026#34;] Execution Result julia\u0026gt; numpad = reshape(1:9, 3,3)\r3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\r1 4 7\r2 5 8\r3 6 9\rjulia\u0026gt; cube = zeros(Int64, 3,3,3)\r3×3×3 Array{Int64, 3}:\r[:, :, 1] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 2] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 3] =\r0 0 0\r0 0 0\r0 0 0\rjulia\u0026gt; save(\u0026#34;mydata.jld\u0026#34;, \u0026#34;numpad\u0026#34;,numpad, \u0026#34;cube\u0026#34;,cube)\r┌ Warning: JLD incorrectly extends FileIO functions (see FileIO documentation)\r└ @ FileIO C:\\Users\\rmsms\\.julia\\packages\\FileIO\\5JdlO\\src\\loadsave.jl:215 The extension of the files saved should be *.jld. The names of each data to be stored are given as strings and assigned variables are sequentially appended, bundling the data together for storage.\njulia\u0026gt; mydata = load(\u0026#34;mydata.jld\u0026#34;)\r┌ Warning: JLD incorrectly extends FileIO functions (see FileIO documentation)\r└ @ FileIO C:\\Users\\rmsms\\.julia\\packages\\FileIO\\5JdlO\\src\\loadsave.jl:215 Dict{String, Any} with 7 entries:\r\u0026#34;_creator\\\\JULIA_PATCH\u0026#34; =\u0026gt; 0x00000001\r\u0026#34;cube\u0026#34; =\u0026gt; [0 0 0; 0 0 0; 0 0 0]…\r\u0026#34;_creator\\\\WORD_SIZE\u0026#34; =\u0026gt; 64\r\u0026#34;numpad\u0026#34; =\u0026gt; [1 4 7; 2 5 8; 3 6 9]\r\u0026#34;_creator\\\\JULIA_MINOR\u0026#34; =\u0026gt; 0x00000006\r\u0026#34;_creator\\\\ENDIAN_BOM\u0026#34; =\u0026gt; 0x04030201\r\u0026#34;_creator\\\\JULIA_MAJOR\u0026#34; =\u0026gt; 0x00000001 As you can see, a dictionary is returned upon loading. The names given as strings during the saving process enter as keys, and the actual data is in the values. It can be referenced as a dictionary as follows.\njulia\u0026gt; mydata[\u0026#34;numpad\u0026#34;]\r3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\r1 4 7\r2 5 8\r3 6 9\rjulia\u0026gt; mydata[\u0026#34;cube\u0026#34;]\r3×3×3 Array{Int64, 3}:\r[:, :, 1] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 2] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 3] =\r0 0 0\r0 0 0\r0 0 0 JLD2 In the example, the process of creating a dictionary with strings was cumbersome, but with JLD2.jl, the same functionality can be conveniently used with named tuples.\nhttps://github.com/JuliaIO/JLD.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaIO/JLD2.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2199,"permalink":"https://freshrimpsushi.github.io/en/posts/2199/","tags":null,"title":"How to Store Data like .mat in Julia"},{"categories":"줄리아","contents":"Code 1 Base.Iterators.enumerate() returns an iterator that allows referencing both the index and value of an array, similar to Python.\njulia\u0026gt; x = [3,5,4,1,2]\r5-element Vector{Int64}:\r3\r5\r4\r1\r2\rjulia\u0026gt; for (idx, value) in enumerate(x)\rprintln(\u0026#34;x[▷eq1◁value\u0026#34;)\rend\rx[1]: 3\rx[2]: 5\rx[3]: 4\rx[4]: 1\rx[5]: 2\rjulia\u0026gt; typeof(enumerate(x))\rBase.Iterators.Enumerate{Vector{Int64}} https://docs.julialang.org/en/v1/base/iterators/#Base.Iterators.enumerate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2197,"permalink":"https://freshrimpsushi.github.io/en/posts/2197/","tags":null,"title":"How to Refer to Both Index and Value in Julia's Loops"},{"categories":"줄리아","contents":"Overview When first encountering Julia, one might be perplexed by the symbol data type. Symbols are used with a preceding :, functioning simply by their name without any internal data. They are commonly used as names, labels, or dictionary keys1.\nExplanation In other programming languages, when giving options to a function, they are often provided as numbers or strings to clarify the meaning. For example, the following two functions illustrate this.\njulia\u0026gt; function foo0(x, option = 0)\rif option == 0\rreturn string(x)\relseif option == 1\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo0 (generic function with 2 methods)\rjulia\u0026gt; foo0(3.0, 0)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo0(3.0, 1)\r3\rjulia\u0026gt; function foo1(x, option = \u0026#34;string\u0026#34;)\rif option == \u0026#34;string\u0026#34;\rreturn string(x)\relseif option == \u0026#34;Int\u0026#34;\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo1 (generic function with 2 methods)\rjulia\u0026gt; foo1(3.0, \u0026#34;string\u0026#34;)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo1(3.0, \u0026#34;Int\u0026#34;)\r3 In contrast, the definition using a symbol is shown below. At first glance, it might seem no different from the two functions above.\njulia\u0026gt; function foo2(x, option = :string)\rif option == :string\rreturn string(x)\relseif option == :Int\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo2 (generic function with 2 methods)\rjulia\u0026gt; foo2(3.0, :string)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo2(3.0, :Int)\r3 The reason for using symbols can be explained simply: they are not meant to change mid-program. Sometimes, this might be inconvenient, but unlike integers or strings, there is no chance of them unexpectedly changing.\nMoreover, symbols are true embodiments of assignment and command. From an interface perspective, there\u0026rsquo;s little difference between strings and symbols, but taking the example of receiving the string \u0026quot;Int\u0026quot; and interpreting it as a command to return an integer, versus directly receiving the symbol :Int and returning an integer without question, there is a subtle difference. Even if this difference doesn\u0026rsquo;t resonate, there\u0026rsquo;s no need to force understanding.\nOther instances of using symbols include column names in data frames, where it\u0026rsquo;s difficult or undesirable to distinguish variables from strings. While the notation might seem daunting due to its unfamiliarity, understanding its purpose and differences alleviates the concern.\nhttps://docs.julialang.org/en/v1/base/base/#Core.Symbol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2195,"permalink":"https://freshrimpsushi.github.io/en/posts/2195/","tags":null,"title":"Symbols in Julia"},{"categories":"줄리아","contents":"Guide 1 julia\u0026gt; x = rand(\u0026#39;a\u0026#39;:\u0026#39;c\u0026#39;, 10)\r10-element Vector{Char}:\r\u0026#39;a\u0026#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\r\u0026#39;a\u0026#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase) Let\u0026rsquo;s say we have an array as shown above. In the example, our goal is to select both 'a' and 'b'. Logically, one might think to broadcast with inclusion operator $\\in$, but the result is as follows.\njulia\u0026gt; x .∈ [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]\rERROR: DimensionMismatch(\u0026#34;arrays could not be broadcast to a common size; got a dimension with lengths 10 and 2\u0026#34;) A DimensionMismatch error was raised. This error occurred because broadcasting happened simultaneously to both the array x and the category ['a', 'b']. To interpret the error message, it\u0026rsquo;s confusing because the length 10 of x and the length 2 of ['a', 'b'] came in at the same time.\njulia\u0026gt; x .∈ Ref([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;])\r10-element BitVector:\r1\r1\r1\r0\r1\r0\r0\r0\r1\r1 In this case, you can solve the broadcasting problem with the Ref() function. This allows 'a' and 'b' in ['a', 'b'] to be treated as scalars and find only the places with these two characters.\nPrecautions julia\u0026gt; y = rand(\u0026#39;a\u0026#39;:\u0026#39;c\u0026#39;, 1, 10)\r1×10 Matrix{Char}:\r\u0026#39;b\u0026#39; \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026#39;b\u0026#39; \u0026#39;a\u0026#39; \u0026#39;c\u0026#39; \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026#39;b\u0026#39; \u0026#39;c\u0026#39; Consider the case of a $1 \\times 10$ matrix as shown above. At first glance, it might seem no different from the case seen in the guide above, but .∈ is used in a completely different way.\njulia\u0026gt; y .∈ [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]\r2×10 BitMatrix:\r0 1 0 0 1 0 1 0 0 0\r1 0 1 1 0 0 0 1 1 0 As you can see, the first row indicates the position of 'a', and the second row indicates the position of 'b'. This is due to the difference between a vector and a matrix.\njulia\u0026gt; y .∈ Ref([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;])\r1×10 BitMatrix:\r1 1 1 1 1 0 1 1 1 0 Consistent results can be obtained when using Ref().\nEnvironment OS: Windows julia: v1.7.0 https://stackoverflow.com/a/59978386/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2193,"permalink":"https://freshrimpsushi.github.io/en/posts/2193/","tags":null,"title":"How to Check if Elements of an Array Belong to a List in Julia"},{"categories":"줄리아","contents":"Explanation 1D arrays (vectors) are defined as follows.\njulia\u0026gt; A = [1; 2; 3]\r3-element Vector{Int64}:\r1\r2\r3 Here, ; signifies moving to the next element based on the first dimension. By generalizing this, ;; signifies moving to the next element based on the second dimension.\njulia\u0026gt; A = [1; 2; 3;; 4; 5; 6]\r3×2 Matrix{Int64}:\r1 4\r2 5\r3 6 Similarly, arrays of three dimensions and above can be defined. Note that this code is possible from Julia version 1.7 onwards.\njulia\u0026gt; A = [1 2; 3 4;;; 5 6; 7 8]\r2×2×2 Array{Int64, 3}:\r[:, :, 1] =\r1 2\r3 4\r[:, :, 2] =\r5 6\r7 8\rjulia\u0026gt; A = [1 2; 3 4;;; 5 6; 7 8 ;;;; 9 10; 11 12;;; 13 14; 15 16]\r2×2×2×2 Array{Int64, 4}:\r[:, :, 1, 1] =\r1 2\r3 4\r[:, :, 2, 1] =\r5 6\r7 8\r[:, :, 1, 2] =\r9 10\r11 12\r[:, :, 2, 2] =\r13 14 Environment OS: Windows10 Version: Julia 1.7.1 ","id":3223,"permalink":"https://freshrimpsushi.github.io/en/posts/3223/","tags":null,"title":"How to Directly Define Multidimensional Arrays in Julia"},{"categories":"줄리아","contents":"Guide while The while loop is no different from other languages.\njulia\u0026gt; while x \u0026lt; 10\rx += 1\rprint(\u0026#34;▷eq1◁i - \u0026#34;)\rend\r1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 -\rjulia\u0026gt; for i = 1:10\rprint(\u0026#34;▷eq2◁i - \u0026#34;)\rend\r1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 - The three major looping styles used in Julia are as above. The top one is similar to the method used in R and Python, the second is similar to Matlab, and the most elegant expression is the third, which uses the Set comprehension method.\nNested Loops The following two loops function identically.\njulia\u0026gt; X = 1:4; Y = 8:(-1):5;\rjulia\u0026gt; for x ∈ X\rfor y ∈ Y\rprint(\u0026#34; (▷eq3◁y) = ▷eq4◁x + ▷eq5◁(x + y)\u0026#34;)\rif y == 5 println() end\rend\r(1 + 8) = 9 (1 + 7) = 8 (1 + 6) = 7 (1 + 5) = 6\r(2 + 8) = 10 (2 + 7) = 9 (2 + 6) = 8 (2 + 5) = 7\r(3 + 8) = 11 (3 + 7) = 10 (3 + 6) = 9 (3 + 5) = 8\r(4 + 8) = 12 (4 + 7) = 11 (4 + 6) = 10 (4 + 5) = 9 It\u0026rsquo;s like writing code as if it were Pseudo Code. A note of caution is the following case where a tuple is given as an Iterator.\njulia\u0026gt; for (x,y) ∈ (X, Y)\rprint(\u0026#34; (▷eq3◁y) = ▷eq7◁x + ▷eq5◁(x + y)\u0026#34;)\rif y == 5 println() end\rend\r(1 + 8) = 9 (2 + 7) = 9 (3 + 6) = 9 (4 + 5) = 9 ","id":2191,"permalink":"https://freshrimpsushi.github.io/en/posts/2191/","tags":null,"title":"How to Use Elegant Loops in Julia"},{"categories":"줄리아","contents":"Description By typing the right bracket ] in the Julia REPL, you can switch to package management mode. The available commands in package management mode are as follows.\nCommand Function add foo Adds the package foo. free foo Unpins the package version. help, ? Shows these commands. pin foo Pins the version of the package foo. remove foo, rm foo Removes the package foo. test foo Test-runs the package foo. status, st Shows all installed packages and their versions. Typing a package name shows only its version. undo Reverses the most recent action. update, up Updates all packages to their latest versions. Typing a package name updates only that specific package. Environment OS: Windows10 Version: Julia 1.7.1 ","id":3217,"permalink":"https://freshrimpsushi.github.io/en/posts/3217/","tags":null,"title":"List of Available Commands in Julia Package Management Mode"},{"categories":"줄리아","contents":"설명 This document outlines the process of calculating the Radon transform $\\mathcal{R}f$ of a phantom $f$ in Python and saving the results as a *.npy file. To load this file in Julia, one can use the PyCall.jl package.\nusing PyCall\rnp = pyimport(\u0026#34;numpy\u0026#34;) The above code is equivalent to executing import numpy as np in Python. This allows one to directly use the code written for numpy in Python to load $f$ and $\\mathcal{R}f$.\nf = np.load(\u0026#34;f.npy\u0026#34;)\rRf = np.load(\u0026#34;Rf.npy\u0026#34;) To check if it has been correctly loaded, let\u0026rsquo;s visualize it using a heatmap.\np1 = heatmap(reverse(f, dims=1), color=:viridis)\rp2 = heatmap(reverse(Rf, dims=1), color=:viridis)\rplot(p1, p2, size=(728,250)) Environment OS: Windows10 Version: Julia 1.6.2, PyCall 1.93.0 ","id":3215,"permalink":"https://freshrimpsushi.github.io/en/posts/3215/","tags":null,"title":"How to load a npy file in Julia"},{"categories":"줄리아","contents":"Code Let\u0026rsquo;s say we want to draw a sine curve from $0$ to $2\\pi$ on the heatmap of the array $(5,5)$. You might want to write the code like this, but as you can see in the figure, it doesn\u0026rsquo;t output as desired.\nusing Plots\rA = rand(Bool, 5,5)\rheatmap(A, color=:greens)\rx = range(0, 2pi, length=100)\ry = sin.(x)\rplot!(x, y, color=:red, width=3) This is because the horizontal and vertical range of array $A$ is recognized as from $1$ to $5$. To solve this, you can specify the horizontal and vertical range of $A$ as below. Note that if you do not specify the range of array $A$ and only specify the range shown in the heatmap, it will be displayed as below.\nxₐ = range(0,2pi, length=5)\ryₐ = range(-1.5,1.5, length=5)\rp1 = heatmap(xₐ, yₐ, A, color=:greens, xlims=(0,2pi), ylims=(-1.5,1.5)) #그림 (가)\rp2 = heatmap(A, color=:greens, xlims=(0,2pi), ylims=(-1.5,1.5)) #그림 (나) Now, if we redraw the sine curve on p1, it is drawn correctly as desired.\nplot!(x, y, color=:red, width=3) Environment OS: Windows10 Version: Julia 1.7.1, Plots 1.25.3 ","id":3213,"permalink":"https://freshrimpsushi.github.io/en/posts/3213/","tags":null,"title":"Overlaying Plots on Heatmaps in Julia"},{"categories":"줄리아","contents":"Code 1 To use the LaTeXStrings library, prefix strings with L, like so L\u0026quot;...\u0026quot;.\n@time using Plots\r@time using LaTeXStrings\rplot(0:0.1:2π, sin.(0:0.1:2π), xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;)\rtitle!(L\u0026#34;\\mathrm{TeX\\,representation:\\,} y = \\sin x , x \\in [0, 2 \\pi]\u0026#34;) Note that the package name is precisely LaTeXStrings and regular text does not work with \\text{}; instead, use \\mathrm{}. For spaces, use \\,.\nEnvironment OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/latex-code-for-titles-labels-with-plots-jl/1967/18\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2180,"permalink":"https://freshrimpsushi.github.io/en/posts/2180/","tags":null,"title":"Using TeX in Plots in Julia"},{"categories":"줄리아","contents":"Description julia\u0026gt; x = [1 2 3]\r1×3 Matrix{Int64}:\r1 2 3\rjulia\u0026gt; y = [1 2 3 4]\r1×4 Matrix{Int64}:\r1 2 3 4\rjulia\u0026gt; x .+ y\rERROR: DimensionMismatch Two vectors of different sizes cannot perform element-wise operations by default. To implement this manually, one would have to use a double for loop, but fortunately, it can be easily calculated by treating one as a row vector and the other as a column vector, returning a 2D array with element-wise operations. This is also possible in MATLAB or Python NumPy.\nDespite differing sizes, it does not result in an error, caution is needed to ensure that it does not lead to unintended calculations.\njulia\u0026gt; x\u0026#39; .+ y\r3×4 Matrix{Int64}:\r2 3 4 5\r3 4 5 6\r4 5 6 7\rjulia\u0026gt; x\u0026#39; .* y\r3×4 Matrix{Int64}:\r1 2 3 4\r2 4 6 8\r3 6 9 12\rjulia\u0026gt; x\u0026#39; ./ y\r3×4 Matrix{Float64}:\r1.0 0.5 0.333333 0.25\r2.0 1.0 0.666667 0.5\r3.0 1.5 1.0 0.75 It is also possible to use transpose(x) instead of x'.\nEnvironment OS: Windows10 Version: Julia 1.6.2 ","id":3207,"permalink":"https://freshrimpsushi.github.io/en/posts/3207/","tags":null,"title":"Performing Operations on Vectors of Different Sizes Component-wise in Julia"},{"categories":"줄리아","contents":"Code 1 If the browser is in dark mode, you can clearly see that the background has been rendered transparent.\nYou just need to insert the :transparent symbol into the background_color option. It saves well as *.png, but it is said that it doesn\u0026rsquo;t save well as *.pdf.\nusing Plots\rplot(rand(10), background_color = :transparent)\rpng(\u0026#34;example\u0026#34;) As you can guess from the option name, if you put in a color symbol, it will be output in that color. For example, a picture drawn with :yellow looks like this.\nEnvironment OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/save-figure-with-transparent-background-color-in-plots-jl/18808/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2173,"permalink":"https://freshrimpsushi.github.io/en/posts/2173/","tags":null,"title":"How to Make a Transparent Background in Graphics with Julia"},{"categories":"줄리아","contents":"Fill up to a Specific Value1 Using attributes fillrange=a, fillalpha=b, fillcolor=:color in plot(), it colors with :color to the value a from the plotted curve with the transparency b. It works the same by writing fill=(a,b,:color). That is, the following two codes are the same.\nplot(x,y, fillrange=a, fillalpha=b, fillcolor=:color)\rplot(x,y, fill=(a,b,:color)) It seems to be a bug, but selecting the value of fillrange as $(0,1)$ does not get colored.\nusing Plots\rrandom_walk = cumsum(rand(20).-.5)\rp1 = plot(random_walk,fill=(1,0.2,:lime), lw=3, legend=:bottomright)\rp2 = plot(random_walk,fill=(2,0.2,:tomato), lw=3, legend=:bottomright)\rplot(p1, p2) Filling Between Two Curves By putting one of the curve\u0026rsquo;s function values into fillalpha, the area between two curves gets colored.\nrw = random_walk\rplot([rw rw.+1],fill=(rw.+1,0.2,:lime), lw=3, legend=:bottomright) Filling Inside a Closed Curve The inside gets colored if fillalpha\u0026rsquo;s value is chosen not only from $(0,1)$.\ntheta = range(0,2pi, length=40)\rx = cos.(theta)\ry = sin.(theta)\rplot(x, y, fill=(1,0.2,:lime), xlim=(-3,3), ylim=(-1.5,1.5), size=(800,400), lw=3) Environment OS: Windows10 Version: Julia 1.6.2, Plots 1.23.6 http://docs.juliaplots.org/latest/attributes/#fill\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3203,"permalink":"https://freshrimpsushi.github.io/en/posts/3203/","tags":null,"title":"Methods for Coloring Up to a Certain Value from Curves in Julia / Between Two Curves / Inside a Closed Curve"},{"categories":"프로그래밍","contents":"Code import pandas as pd\rdata = { \u0026#39;나이\u0026#39; : [26,23,22,22,21,21,20,20,20,20,18,17], \u0026#39;키\u0026#39; : [160, 163, 163, 162, 164, 163, 164, 150, 158, 162, 172, 173], \u0026#39;별명\u0026#39; : [\u0026#39;땡모\u0026#39;, \u0026#39;김쿠라\u0026#39;, \u0026#39;광배\u0026#39;, \u0026#39;오리\u0026#39;, \u0026#39;깃털\u0026#39;, \u0026#39;쌈무\u0026#39;, \u0026#39;밍구리\u0026#39;, \u0026#39;나부키 야코\u0026#39;, \u0026#39;월클토미\u0026#39;, \u0026#39;쪼율\u0026#39;, \u0026#39;안댕댕\u0026#39;, \u0026#39;워뇨\u0026#39;], \u0026#39;국적\u0026#39; : [\u0026#39;한국\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;] }\rindexName = [\u0026#39;권은비\u0026#39;,\u0026#39;미야와키 사쿠라\u0026#39;,\u0026#39;강혜원\u0026#39;, \u0026#39;최예나\u0026#39;, \u0026#39;이채연\u0026#39;, \u0026#39;김채원\u0026#39;, \u0026#39;김민주\u0026#39;, \u0026#39;야부키 나코\u0026#39;, \u0026#39;혼다 히토미\u0026#39;, \u0026#39;조유리\u0026#39;, \u0026#39;안유진\u0026#39;, \u0026#39;장원영\u0026#39;]\rdf = pd.DataFrame(data, index = indexName) The labels of the columns are obtained as .columns.\n\u0026gt;\u0026gt;\u0026gt; df.columns Index([\u0026#39;나이\u0026#39;, \u0026#39;키\u0026#39;, \u0026#39;별명\u0026#39;, \u0026#39;국적\u0026#39;], dtype=\u0026#39;object\u0026#39;) \u0026gt;\u0026gt;\u0026gt; df.columns[2] \u0026#39;별명\u0026#39; The label of the row is obtained as .index. Note that it is not `.rows``.\n\u0026gt;\u0026gt;\u0026gt; df.index Index([\u0026#39;권은비\u0026#39;, \u0026#39;미야와키 사쿠라\u0026#39;, \u0026#39;강혜원\u0026#39;, \u0026#39;최예나\u0026#39;, \u0026#39;이채연\u0026#39;, \u0026#39;김채원\u0026#39;, \u0026#39;김민주\u0026#39;, \u0026#39;야부키 나코\u0026#39;, \u0026#39;혼다 히토미\u0026#39;, \u0026#39;조유리\u0026#39;, \u0026#39;안유진\u0026#39;, \u0026#39;장원영\u0026#39;], dtype=\u0026#39;object\u0026#39;) \u0026gt;\u0026gt;\u0026gt; df.index[10] \u0026#39;안유진\u0026#39; ","id":3189,"permalink":"https://freshrimpsushi.github.io/en/posts/3189/","tags":null,"title":"How to Get Column and Row Labels of Data Frame in Python Pandas"},{"categories":"줄리아","contents":"Code 1 == compares whether values are the same, and === operates differently depending on whether the values to be compared are Mutable or not.\nMutable: Checks if both terms refer to the same object, in other words, it returns whether the two variables can be programmatically distinguished or not. Immutable: Checks if both terms are of the same type, Checks if both terms have the same structure, And recursively checks if each element is == the same. julia\u0026gt; X = 1; Y = 1;\rjulia\u0026gt; X == Y\rtrue\rjulia\u0026gt; X === Y\rtrue\rjulia\u0026gt; X = [1]; Y = [1];\rjulia\u0026gt; X == Y\rtrue\rjulia\u0026gt; X === Y\rfalse For example, looking at the above execution result commonly seen in Python, the integer 1 is Immutable and programmatically indistinguishable, so both == and === return true. However, when looking at an array containing only 1, if a new element is added to X, X and Y can become different because they are Mutable, so == returns true for simply comparing values, while === returns false for comparing the objects themselves.\nYou can understand enough just by grasping this usage even if you don\u0026rsquo;t really know what objects mean.\nOptimization In Julia, which has weak object orientation, such differences are not significantly impactful. From the perspective of code optimization, the comparison of == and === shows the following level of performance difference in the comparison of Singletons.\nN = 10^7\rx = rand(0:9, N)\rjulia\u0026gt; @time for t ∈ 1:N\rx[t] == 0\rend\r1.292501 seconds (30.00 M allocations: 610.336 MiB, 2.63% gc time)\rjulia\u0026gt; @time for t ∈ 1:N\rx[t] === 0\rend\r1.016211 seconds (30.00 M allocations: 610.336 MiB, 2.77% gc time) A value is usually Immutable, so it\u0026rsquo;s understood that === is faster than ==.\nSuch differences might not be significant in most cases. Needless to say, non-Iterative tasks are much faster, like the following vector operations, where speed differences are negligible or none.\njulia\u0026gt; @time x .== 0;\r0.009509 seconds (6 allocations: 1.196 MiB)\rjulia\u0026gt; @time x .=== 0;\r0.009478 seconds (6 allocations: 1.196 MiB) Environment OS: Windows julia: v1.6.1 https://stackoverflow.com/a/38638838/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2157,"permalink":"https://freshrimpsushi.github.io/en/posts/2157/","tags":null,"title":"The Difference Between == and === in Julia"},{"categories":"확률미분방정식","contents":"Model 1 At time point $t$, let\u0026rsquo;s say the price of $S_{t}$ units of the underlying asset $1$, and assume that $S_{t}$ undergoes Geometric Brownian Motion. That is, for Standard Brownian Motion $W_{t}$, drift $\\mu \\in \\mathbb{R}$, and diffusion $\\sigma^{2} \u0026gt; 0$, $S_{t}$ is the solution to the following Stochastic Differential Equation. $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ When a risk-free rate $r \\in \\mathbb{R}$ is given, the price $F = F \\left( t, S_{t} \\right)$ of $1$ units of the derivative at time $t$ follows the following [Partial Differential Equation](../../categories/Partial Differential Equations). $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\nVariables $F \\left( t, S_{t} \\right)$: Derivatives refer to financial instruments such as futures and options. $S_{t}$: Underlying Assets are the commodities traded in derivatives, such as currency, bonds, and stocks. Parameters $r \\in \\mathbb{R}$: Represents the interest rate of Risk-free Assets. A typical example of a risk-free asset is a deposit. $\\sigma^{2} \u0026gt; 0$: Represents the market\u0026rsquo;s Volatility. Explanation Contrary to common misconceptions about derivatives, futures and options were created as means of hedging against uncertain futures. It was a way to reduce risk by paying a certain premium, even if it cost a bit. The problem was the lack of an appropriate method to price them, and traders traded derivatives based on experience. The Black-Scholes model is an equation that made it possible to mathematically explain the price of such derivatives.\nCommonly, the contributors to the Black-Scholes model (1973) are cited as Fischer Black, Myron Scholes, and Robert K. Merton, who introduced the \u0026lsquo;hedge-based derivation\u0026rsquo; in this post. Unfortunately, Black passed away in 1995, and Scholes and Merton were awarded the Nobel Prize in Economics in 1997. After the discovery of the Black-Scholes-Merton equation, the options market developed dazzlingly, and a new sub-discipline called financial engineering emerged in academia.\nThe Comedy of Black According to Wikipedia2, Black frequently changed majors during his PhD studies and had trouble settling down in one field. He switched from physics to mathematics, to computer science, and to artificial intelligence, but eventually made a significant contribution in economics.\nAlthough no reliable reference was found, it is said that Black, during his physics major, realized he couldn\u0026rsquo;t survive among the surrounding geniuses and became a pioneer in economics/finance, where there were no pioneers actively using mathematics, thus making a name for himself in a wasteland without science monsters.\nThe Tragedy of Scholes According to Namuwiki3, Scholes caused a sensation at the 1997 Nobel Economics Prize press conference by saying he would invest the prize money in stocks. The hedge fund Scholes was managing went bankrupt in 1998 due to overconfidence and excessive leverage, following Russia\u0026rsquo;s default. After the crisis, Scholes managed to return profits to investors and continued as a fund manager until retiring just before the subprime mortgage crisis erupted.\nAssumptions Before delving into the derivation, let\u0026rsquo;s check some assumptions.\nFactors such as commissions, taxes, and dividends are not considered Think of it as not considering resistance, temperature, or atmospheric pressure in a physics model, which are not the focus of the study. Additionally, it\u0026rsquo;s assumed that the trend $\\mu$ and $\\sigma$ are simply constants.\nDerivatives depend on the underlying assets and timing If the price of a derivative is independent of the underlying asset, there\u0026rsquo;s no need to use the terms \u0026lsquo;derivative\u0026rsquo; and \u0026lsquo;underlying\u0026rsquo;. It\u0026rsquo;s reasonable for the price of a derivative to change as the price of the underlying asset changes. If it doesn\u0026rsquo;t change over time (if it\u0026rsquo;s constant), then there\u0026rsquo;s no point in pondering the price of a derivative. Thus, it\u0026rsquo;s assumed that $F$ is a function of at least two factors $t$ and $S_{t}$, even if we can\u0026rsquo;t specify the exact form. $$ F = F \\left( t, S_{t} \\right) $$\nThe underlying asset undergoes Geometric Brownian Motion The primary application of Geometric Brownian Motion GBM is to explain the price fluctuations of underlying assets like stock prices. It assumes that the change in asset prices is proportional to the asset\u0026rsquo;s price, and that the price cannot become negative unless the asset is delisted, among other favorable assumptions. $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$\nLet\u0026rsquo;s assume the price $p_{t}$ of some stock follows GBM. The return, defined as taking the log of dividing the closing price on day $t$ by the closing price on day $t-1$, $$ r_{t} = \\nabla \\log p_{t} = \\log {{ p_{t} } \\over { p_{t-1} }} $$ aligns with our intuition that the return should be positive if the price increases and negative if it decreases, regardless of the stock\u0026rsquo;s size. As explained in the \u0026ldquo;Log-normal Distribution\u0026rdquo; section, this return follows a normal distribution, focusing on the essence of growth and decay rather than simple fluctuations.\nRisk-free assets grow according to Malthusian growth The Malthusian growth model is the simplest model describing the growth of a population without any limitations or interventions and can be used as an assumption to explain the proliferation of risk-free assets in economics/finance. The risk-free rate is assumed to be a constant $r$, and the financial income is proportional to the size of asset $N_{t}$, so it can be expressed by the following [Ordinary Differential Equation](../../categories/Ordinary Differential Equations). $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$\nArbitrage-free pricing: There\u0026rsquo;s no value difference between portfolios A mathematical explanation of the portfolio will be further detailed in this proof. The assumption of arbitrage-free pricing means that all portfolios we consider are balanced at the same value. For instance, if the value of portfolio $A$ is higher than $B$, a rational market participant would increase the proportion of the more valuable $A$ to make a profit, so there\u0026rsquo;s no reason to consider $B$. Therefore, it\u0026rsquo;s assumed that the portfolios we consider are already in a state where no further profits can be made through such arbitrage.\nFrictionless market: There are no restrictions on division and short selling Anyone who has traded stocks knows that there are minimum bidding units, so you can\u0026rsquo;t trade exactly the amount you want, and there are restrictions on short selling in the Korean stock market, where borrowed short selling (borrowing stocks) is the principle. Being able to divide trading units as desired and short sell without any restrictions can be considered as having no friction opposing actions.\nDerivation Part 1. Portfolio Composition\nLet\u0026rsquo;s assume we can only hold three types of assets:\nUnderlying Asset: Let\u0026rsquo;s say we hold $s$ units. Derivative: Let\u0026rsquo;s say we hold $f$ units. Risk-free Asset: An asset that is neither an underlying asset nor a derivative, which can be considered as cash. If we denote the value of all assets we hold at time $t$ as $V_{t}$, and if $S_{t}$ was the price of $1$ units of the underlying asset and $F \\left( t , S_{t} \\right)$ was the price of $1$ units of the derivative, it can be represented as follows. $$ V_{t} = f F \\left( t, S_{t} \\right) + s S_{t} $$ Composing a portfolio means adjusting the amounts of $f$ and $s$, in other words, strategizing on how to invest. Assuming that the trading volume generated by such portfolio composition significantly affects the market is irrational, so the prices of the underlying asset and derivative are independent of the choices of $f$ and $s$. In other words, the mathematical discussions that follow do not change regardless of how $f$ and $s$ are determined.\nIt\u0026rsquo;s important to note that $V_{t}$ is not the sum of the total assets. It\u0026rsquo;s easier to understand if you think of it as looking only at the stock balance, not the cash account. Let\u0026rsquo;s think about what portfolios could be:\nSavings $V_{t} = 0$: Clear the stock account and put everything into savings to receive interest. It might seem trivial to a scholar who knows nothing but mathematics, but it\u0026rsquo;s a legitimate strategy for dealing with market crashes or recessions. Individual Investor $V_{t} = 5 S_{t}$: Individuals should not touch derivatives. Most individual investors in countries where short selling is banned have this type of portfolio. For example, if $S_{t} = 81,200$ is the stock price of Samsung Electronics, this portfolio is my friend \u0026lsquo;Kim Soo-hyung\u0026rsquo;s account holding $5$ shares of Samsung Electronics. Hedge $\\displaystyle V_{t} = 1 \\cdot F- {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t}$: Consider buying a call option $1$ and short selling the underlying asset ${{ \\partial F } \\over { \\partial S_{t} }}$. If the price of the underlying asset increases significantly on the expiration date of the option, the call option will bring in a large profit, and if the price of the underlying asset falls, profit will have been made from the short sale early on. The options mentioned in the explanation of hedging and to be covered later are European Options, which are usually known as \u0026lsquo;options that can only be exercised on the expiration date\u0026rsquo;. American Options can be exercised at any time before maturity, but that\u0026rsquo;s not our concern here, so don\u0026rsquo;t be intimidated by the terms European and American.\nWe will derive the Black-Scholes equation from the last example, a portfolio that hedges a derivative with a spot short sale $$ V_{t} = 1 \\cdot F \\left( t, S_{t} \\right) - {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t} $$ Since it\u0026rsquo;s perfectly hedged, this portfolio is a risk-free asset, and the increment over time $t$ is $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} d S_{t} $$ Here, since $S_{t}$ is assumed to follow Geometric Brownian Motion, substituting $d S_{t}$ for $\\displaystyle S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right)$ yields $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ Considering the assumption of arbitrage-free pricing, the increment of this portfolio should be the same as that of a risk-free asset portfolio. If we assume there\u0026rsquo;s a price difference between the portfolios, profit could be made by liquidating one portfolio and investing in the other. Since we assumed risk-free assets grow according to Malthusian growth, the risk-free rate $r$ can be expressed by the following [Ordinary Differential Equation](../../categories/Ordinary Differential Equations). $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$ Organizing these, we get $$ \\begin{align*} d V_{t} =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\\\ d V_{t} =\u0026amp; r V_{t} dt \\end{align*} $$ thus, $$ \\begin{equation} r V_{t} dt = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\label{1} \\end{equation} $$ is obtained. Now, let\u0026rsquo;s use Itô calculus to find $dF$.\nPart 2. Itô Calculus\nItô\u0026rsquo;s Lemma: Given an Itô Process $\\left\\{ X_{t} \\right\\}_{t \\ge 0}$, $$ d X_{t} = u dt + v d W_{t} $$ for function $V \\left( t, X_{t} \\right) = V \\in C^{2} \\left( [0,\\infty) \\times \\mathbb{R} \\right)$, if we set $Y_{t} := V \\left( t, X_{t} \\right)$, then $\\left\\{ Y_{t} \\right\\}$ is also an Itô Process, and the following holds. $$ \\begin{align*} d Y_{t} =\u0026amp; V_{t} dt + V_{x} d X_{t} + {{ 1 } \\over { 2 }} V_{xx} \\left( d X_{t} \\right)^{2} \\\\ =\u0026amp; \\left( V_{t} + V_{x} u + {{ 1 } \\over { 2 }} V_{xx} v^{2} \\right) dt + V_{x} v d W_{t} \\end{align*} $$\nIn Geometric Brownian Motion, distributing $S_{t}$ according to the distributive law gives $$ d S_{t} = \\mu S_{t} dt + \\sigma S_{t} d W_{t} $$ and, from Itô\u0026rsquo;s Lemma, since $u = \\mu S_{t}$ and $v = \\sigma S_{t}$, we obtain $$ d F = \\left( {{ \\partial F } \\over { \\partial t }} + {{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} $$ Substituting this into $\\eqref{1}$ for $d F$ gives $$ \\begin{align*} r V_{t} dt =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt - {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} \\\\ =\u0026amp; \\left( {{ \\partial F } \\over { \\partial t }} + {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t}} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ \u0026amp; - {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt} - {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ =\u0026amp; {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt \\end{align*} $$ Since the portfolio\u0026rsquo;s value $V_{t}$ was defined as $\\displaystyle V_{t} = F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t}$, substituting this yields $$ r \\left( F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\right) dt = {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt $$ Rearranging the equation for $rF$, we obtain the desired equation. $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\n■\nByung-Seon Choi. (2012). Various Derivations of the Black-Scholes Formula\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Fischer_Black\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://namu.wiki/w/Black-Scholes%20model#s-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2156,"permalink":"https://freshrimpsushi.github.io/en/posts/2156/","tags":null,"title":"Derivation of Black-Scholes Model"},{"categories":"줄리아","contents":"Code 1 It\u0026rsquo;s quite simple, but a common mistake is to treat the negation operators ! and ~ not as unary operators but as functions, and use !. or ~.. They should be written as .! or .~ instead.\njulia\u0026gt; a = rand(1,10) .\u0026lt; 0.5\r1×10 BitMatrix:\r1 1 0 0 1 0 1 0 0 0\rjulia\u0026gt; .!(a)\r1×10 BitMatrix:\r0 0 1 1 0 1 0 1 1 1\rjulia\u0026gt; .~(a)\r1×10 BitMatrix:\r0 0 1 1 0 1 0 1 1 1 Environment OS: Windows julia: v1.6.0 https://discourse.julialang.org/t/negation-of-boolean-array/16159/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2149,"permalink":"https://freshrimpsushi.github.io/en/posts/2149/","tags":null,"title":"How to Invert a Bit Array in Julia"},{"categories":"줄리아","contents":"Code 1 using Gtk\rfile_name = open_dialog(\u0026#34;파일 열기\u0026#34;) The string given as the first argument is the title of the dialog. When executed, you can see a \u0026lsquo;Open File\u0026rsquo; dialog popping up like this.\nEnvironment OS: Windows julia: v1.6.0 https://discourse.julialang.org/t/choose-a-file-interactively/10910/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2143,"permalink":"https://freshrimpsushi.github.io/en/posts/2143/","tags":null,"title":"How to Open a Dialog Box and Select a File Like file.choose() in Julia"},{"categories":"기하학","contents":"Vector Field Along a Curve1 Definition Given a surface $M$ and a curve $\\alpha : \\left[ a, b \\right] \\to M$, let us consider a function $\\mathbf{X}$ that maps each $t \\in \\left[ a,b \\right]$ to a tangent vector at point $\\alpha (t)$ on surface $M$. This function $\\mathbf{X}$ is called a vector field along curve $\\alpha$vector field along a curve $\\alpha$.\n$$ \\mathbf{X} : \\left[ a, b \\right] \\to \\mathbb{R}^{3} \\\\ \\mathbf{X}(t) \\in T_{\\alpha (t)}M $$\nExplanation It\u0026rsquo;s important to note that the tangent vector mentioned in the definition is not the tangent vector of the curve $\\alpha$, but rather the tangent vector at point $\\alpha (t)$, which is an element of $T_{\\alpha (t)}M$. Since the tangent vector at each point $\\alpha (t)$ on the surface is not unique, the vector field along curve $\\alpha$ is not unique either. There are infinitely many vectors in the tangent plane, hence there are infinitely many vector fields as well.\nFor a simple example, given a curve $\\alpha (t)$ on surface $M$, the tangent vector field $\\mathbf{T}(t)$ of $\\alpha (t)$ becomes a vector field along $\\alpha$. $\\mathbf{S} = \\mathbf{n} \\times \\mathbf{T}$ is also a vector field along $\\alpha$.\n$\\mathbf{S}$ and $\\mathbf{T}$ form a basis of the tangent space, so every vector field $\\mathbf{X}$ along $\\alpha$ can be expressed as a linear combination of these.\n$$ \\mathbf{X}(t) = A(t)\\mathbf{T}(t) + B(t)\\mathbf{S}(t)\\quad \\text{for some } A,B:[a,b]\\to \\mathbb{R} $$\nDifferentiable Vector Field Definition A vector field $\\mathbf{X}(t)$ along $\\alpha (t)$ is differentiabledifferentiable if the function $\\mathbf{X} : \\left[ a, b \\right] \\to \\mathbb{R}^{3}$ is differentiable.\nExplanation Strictly speaking, it\u0026rsquo;s correct to say \u0026lsquo;$\\mathbf{X}$ is differentiable\u0026rsquo;, but it\u0026rsquo;s also convenient to say \u0026lsquo;$\\mathbf{X}(t)$ is differentiable\u0026rsquo;.\nParallel Vector Field Definition Given a differentiable vector field $\\mathbf{X}(t)$ along $\\alpha$, if $\\dfrac{d \\mathbf{X}}{dt}$ is perpendicular to surface $M$, then $\\mathbf{X}(t)$ is defined to be parallel along $\\alpha (t)$.\nExplanation As explained earlier, a vector field $\\alpha$ can be chosen arbitrarily, but the condition \u0026lsquo;a differentiable vector field $\\alpha$\u0026rsquo; sets a restriction for defining the concept of \u0026lsquo;parallel lines\u0026rsquo;.\nBeing perpendicular to surface $M$ means that $\\dfrac{d \\mathbf{X}}{dt}$ should have no component in the tangent direction, and only in the normal direction. The definition might not make much sense at first glance, so let\u0026rsquo;s look at the following examples.\nExample In a 2D Plane Consider a curve $\\boldsymbol{\\gamma}(t) = \\left( a(t), b(t), 0 \\right)$ on the $xy-$ plane. And let\u0026rsquo;s say $\\mathbf{X}(t) = \\left( A(t), B(t), 0 \\right)$ is a vector field along $\\boldsymbol{\\gamma}$. Then,\n$$ \\dfrac{d \\mathbf{X}}{dt} = \\left( \\dfrac{d A}{dt}, \\dfrac{d B}{dt}, 0 \\right) $$\nFor this vector to be perpendicular to the $xy-$ plane, the dot product with any vector $(x,y,0)$ must be $0$, leading to the following result.\n$$ \\dfrac{d A}{dt} = 0 = \\dfrac{d B}{dt} $$\nTherefore, $A(t), B(t)$ is a constant. If we illustrate this, it matches well with our intuitive understanding of \u0026lsquo;vectors that are parallel along curve $\\boldsymbol{\\gamma}$\u0026rsquo;.\nOn a Sphere Let $M$ be the unit sphere. Let ${\\color{6699CC}\\boldsymbol{\\gamma}(t)}$ be the equator. Consider a vector field ${\\color{295F2E}\\mathbf{X}_{\\boldsymbol{\\gamma}}(t) = (0, 0, 1)}$ along $\\boldsymbol{\\gamma}$. Then, since $\\dfrac{d \\mathbf{X}_{\\boldsymbol{\\gamma}}}{dt} = (0,0,0)$, it\u0026rsquo;s always perpendicular to $M$. Hence, $\\mathbf{X}_{\\boldsymbol{\\gamma}}$ is a vector field parallel along $\\boldsymbol{\\gamma}$. As seen in the 2D example, a constant vector field naturally becomes a parallel vector field.\nNow, consider a vector field ${\\color{295F2E}\\mathbf{X}_{\\boldsymbol{\\beta}} = \\left( -\\dfrac{\\sqrt{2}}{2}\\cos t, \\dfrac{\\sqrt{2}}{2}\\sin t, \\dfrac{\\sqrt{2}}{2} \\right)}$ along curves ${\\color{6699CC}\\boldsymbol{\\beta}(t) = \\left( \\dfrac{\\sqrt{2}}{2}\\cos t, \\dfrac{\\sqrt{2}}{2}\\sin t, \\dfrac{\\sqrt{2}}{2} \\right)}$ and $\\boldsymbol{\\beta}$. Then, ${\\color{f8c512}\\dfrac{d\\mathbf{X}_{\\boldsymbol{\\beta}}}{dt} = \\left( \\dfrac{\\sqrt{2}}{2}\\sin t, \\dfrac{\\sqrt{2}}{2}\\cos t, 0 \\right)}$, and if we plot this in 3D, it appears as follows.\nSince ${\\color{f8c512}\\dfrac{d\\mathbf{X}_{\\boldsymbol{\\beta}}}{dt}}$ is not perpendicular to the sphere, ${\\color{295F2E}\\mathbf{X}_{\\boldsymbol{\\beta}}}$ is not a vector field parallel along $\\boldsymbol{\\beta}$.\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p116-121\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3174,"permalink":"https://freshrimpsushi.github.io/en/posts/3174/","tags":null,"title":"Definition of Parallel Vector Field along a Curve on Surface"},{"categories":"줄리아","contents":"Code In fact, Julia is not a very convenient language for things like string formatting. While there are methods that utilize the intrinsic capabilities of strings for printing to the console, often it\u0026rsquo;s more convenient to use the round() function\u0026rsquo;s default option, digits.\njulia\u0026gt; for k in 0:8\rprintln(round(π, digits = k))\rend\r3.0\r3.1\r3.14\r3.142\r3.1416\r3.14159\r3.141593\r3.1415927\r3.14159265 Environment OS: Windows julia: v1.6.0 ","id":2133,"permalink":"https://freshrimpsushi.github.io/en/posts/2133/","tags":null,"title":"How to Round to a Specific Decimal Place in Julia"},{"categories":"줄리아","contents":"Code 1 When drawing a heatmap, sometimes it\u0026rsquo;s essential to fix the scale of values, so they don\u0026rsquo;t adjust with the numerical values. You can fix the color range using the clim option in the basic heatmap function.\nusing Plots\rcd(@__DIR__)\rheatmap(rand(4,4)); png(\u0026#34;1.png\u0026#34;)\rheatmap(rand(4,4), clim = (0,1)); png(\u0026#34;2.png\u0026#34;) The results are as follows. The first heatmap has no fixed range, but the second heatmap\u0026rsquo;s range is fixed between 0 and 1, as you can see.\nLimiting Only One Side heatmap(rand(4,4), clim = (0,Inf))\rheatmap(rand(4,4), clim = (-Inf,1)) If you want to set only an upper or a lower limit, you can do so by using Inf as shown above to open the bounds.\nEnvironment OS: Windows julia: v1.6.0 See Also In matplotlib.pyplot of Python https://discourse.julialang.org/t/setting-min-and-max-values-in-a-heatmap-plots-jl/36496\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2126,"permalink":"https://freshrimpsushi.github.io/en/posts/2126/","tags":null,"title":"How to Specify Heatmap Color Ranges in Julia"},{"categories":"줄리아","contents":"Overview 1 In Python, zfill() actually serves as a method of the string class, filling the left side with zeros. Julia, on the other hand, offers the lpad() as a more versatile and widely applicable built-in function. While zfill() means to fill with zeros, lpad() signifies padding to the left.\nCode julia\u0026gt; lpad(\u0026#34;12\u0026#34;, 4, \u0026#34;0\u0026#34;)\r\u0026#34;0012\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;0\u0026#34;)\r\u0026#34;0012\u0026#34; Continuing from the overview, lpad() in Julia is more generic compared to zfill(). It\u0026rsquo;s not a method of the string, so it returns a string whether the argument is a string or a number.\njulia\u0026gt; lpad(12, 4)\r\u0026#34; 12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_\u0026#34;)\r\u0026#34;__12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_!\u0026#34;)\r\u0026#34;_!12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_?!\u0026#34;)\r\u0026#34;_?12\u0026#34;\rjulia\u0026gt; lpad(12, 7, \u0026#34;_?!\u0026#34;)\r\u0026#34;_?!_?12\u0026#34; The common reason for using such a function is to tidy up the output aligning spaces, not necessarily because zeros are needed. If no filling character is provided, it uses a space, and if a character or string is given, it intelligently fills it as seen above.\njulia\u0026gt; rpad(\u0026#34;left\u0026#34;, 6, \u0026#39;0\u0026#39;)\r\u0026#34;left00\u0026#34; Of course, there\u0026rsquo;s also the rpad() function. It has the same basic functionality, only differing in that it pads to the right.\nEnvironment OS: Windows julia: v1.6.0 https://docs.julialang.org/en/v1/base/strings/#Base.lpad\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2124,"permalink":"https://freshrimpsushi.github.io/en/posts/2124/","tags":null,"title":"How to Use zfill() in Julia"},{"categories":"줄리아","contents":"Code propertynames() You can check with the propertynames() function1. Since Julia has no classes, only structs2, all symbols returned by this function are precisely the names of properties only.\nThe following is code for creating an Erdős–Rényi network in the Graphs package, checking the number of nodes, and each node\u0026rsquo;s neighborhood. The propertynames() function was used on this network, and :ne and fadjlist properties were returned as symbols.\n▣code1▣\nfieldnames() This is a bit of a more complicated story, but not knowing it doesn\u0026rsquo;t really impact Julia programming.\nFundamentally, propertynames(x) is the same as fieldnames(typeof(x))3. Although not very significant as a function in practical use, the fact we can learn from this is that in Julia, the instance of a structure is called an object, the attributes a structure itself possesses are called fields, and the properties of an object existing as an instance are called properties.\nEnvironment OS: Windows julia: v1.6.0 https://docs.julialang.org/en/v1/base/base/#Base.propertynames\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/a/56352954\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#Base.fieldnames\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2120,"permalink":"https://freshrimpsushi.github.io/en/posts/2120/","tags":null,"title":"Checking Struct Properties in Julia"},{"categories":"논문작성","contents":"Alpha $\\Alpha, \\alpha$ Alpha is read as \u0026ldquo;alpha\u0026rdquo;. The TeX codes are \\Alpha, \\alpha respectively.\nIt is the first letter of the Greek alphabet, and the phrase \u0026ldquo;alpha and omega\u0026rdquo; means \u0026ldquo;the beginning and the end.\u0026rdquo;\nIndex of an index set $\\alpha$ In differential geometry, a curve $\\alpha$ Curve used to define tangent vectors on a differential manifold $\\alpha$ Beta $\\Beta, \\beta$ Beta is read as \u0026ldquo;beta\u0026rdquo;. The TeX codes are \\Beta, \\beta respectively.\nBeta function $B$ Gamma $\\Gamma, \\gamma$ Gamma is read as \u0026ldquo;gamma\u0026rdquo;. The TeX codes are \\Gamma, \\gamma respectively.\nGamma function $\\Gamma$ In differential geometry, a curve $\\gamma$ Christoffel symbols $\\Gamma_{ij}^{k}$ Delta $\\Delta, \\delta$ Delta is read as \u0026ldquo;delta\u0026rdquo;. The TeX codes are \\Delta, \\delta respectively.\nIn calculus, a very small change $x$ $\\Delta x$ In physics, the Laplacian $\\Delta$ In partial differential equations, the Laplacian $\\Delta$ In mathematics, a very small positive number $\\delta$: $\\epsilon - \\delta$ argument Dirac delta function $\\delta$ Kronecker delta $\\delta_{ij}$ Epsilon $\\Epsilon, \\epsilon, \\varepsilon$ Epsilon is read as \u0026ldquo;epsilon\u0026rdquo;. The TeX codes are \\Epsilon, \\epsilon, \\varepsilon respectively.\nAlthough the correct pronunciation is epsilon, it is often pronounced as upsilon.\nIn mathematics, a very small positive number $\\epsilon$: $\\epsilon - \\delta$ argument In electromagnetism, the permittivity $\\epsilon$ Levi-Civita symbol $\\epsilon_{ijk}$ Zeta $\\Zeta, \\zeta$ Zeta is read as \u0026ldquo;zeta\u0026rdquo;. The TeX codes are \\Zeta, \\zeta respectively.\nIt is not widely used. Often used when running out of variables.\nRiemann zeta function $\\zeta$ Eta $\\Eta, \\eta$ Eta is read as \u0026ldquo;eta\u0026rdquo;. The TeX codes are \\Eta, \\eta respectively.\nLike zeta, it is used when there are no suitable variables available.\nIn particle physics, a baryon $\\eta$ Theta $\\Theta, \\theta, \\vartheta$ Theta is read as \u0026ldquo;theta\u0026rdquo;. The TeX codes are \\Theta, \\theta, \\vartheta respectively.\nIt is mostly considered to represent an angle.\nAngle $\\theta$ In variable separation, a function that has $\\theta$ as a variable $\\Theta (\\theta)$ Iota $\\Iota, \\iota$ Iota is read as \u0026ldquo;iota\u0026rdquo;. The TeX codes are \\Iota, \\iota respectively.\nIt is not commonly used.\nKappa $\\Kappa, \\kappa$ Kappa is read as \u0026ldquo;kappa\u0026rdquo;. The TeX codes are \\Kappa, \\kappa respectively.\nIn differential geometry, curvature $\\kappa$:\nNormal curvature $\\kappa _{n}$, Geodesic curvature $\\kappa_{g}$, [Principal curvature $\\kappa$] In quantum mechanics, a substitution constant for negative energy $\\kappa$](../1261) Lambda $\\Lambda, \\lambda$ Lambda is read as \u0026ldquo;lambda\u0026rdquo;. The TeX codes are \\Lambda, \\lambda respectively.\nEigenvalue $\\lambda$ In physics, wavelength $\\lambda$ Mu $\\Mu, \\mu$ Mu is read as \u0026ldquo;mu\u0026rdquo;. The TeX codes are \\Mu, \\mu respectively.\nMeasure $\\mu$ In electromagnetism, permeability $\\mu$ In particle physics, a muon $\\mu$ In statistics, mean $\\mu$ Nu $\\Nu, \\nu$ Nu is read as \u0026ldquo;nu\u0026rdquo;. The TeX codes are \\Nu, \\nu respectively.\nIn physics, frequency $\\nu$ In particle physics, neutrino $\\nu$ Xi $\\Xi, \\xi$ Xi is read as \u0026ldquo;xi\u0026rdquo;, also pronounced as \u0026ldquo;ksi\u0026rdquo; or \u0026ldquo;zai\u0026rdquo;. It\u0026rsquo;s the Xi apartment\u0026rsquo;s \u0026ldquo;Xi\u0026rdquo;. The TeX codes are \\Xi, \\xi respectively.\nIt is often used when running out of variables.\nVariable for the Fourier transform of $x$ $\\xi$ Riemann Xi function $\\xi$This is pronounced as \u0026ldquo;zai\u0026rdquo;. Omicron $\\Omicron, \\omicron$ Omicron is pronounced as \u0026ldquo;Omicron\u0026rdquo; and is almost the same shape as the alphabet $o$, so it is not used. The TeX codes are \\Omicron, \\omicron respectively.\nPi $\\Pi, \\pi$ Pi is pronounced as \u0026ldquo;pi\u0026rdquo; and represents the mathematical constant pi. The TeX codes are \\Pi, \\pi respectively.\nProduct symbol $\\Pi$ Pi, the mathematical constant $\\pi$ In particle physics, a pion meson $\\pi$ Rho $\\Rho, \\rho$ Rho is pronounced as \u0026ldquo;rho\u0026rdquo;. The TeX codes are \\Rho, \\rho respectively.\nRadius variable in cylindrical coordinates $\\rho$](../1795) In physics, density $\\rho$\nVolume charge density $\\rho$ Sigma $\\Sigma, \\sigma$ Sigma is pronounced as \u0026ldquo;sigma\u0026rdquo;. The TeX codes are \\Sigma, \\Sigma respectively.\nSummation symbol $\\Sigma$ Variance in statistics $\\sigma^{2}$ In electromagnetism, surface charge density $\\sigma$ In thermodynamics, [collision cross-section $\\sigma$] Tau $\\Tau, \\tau$ In mechanics, [torque $\\tau$](../167 4): It is also commonly written as $N$.\nUsed as a variable for time, instead of $t$. Twice the number pi $\\tau = 2\\pi$ Upsilon $\\Upsilon, \\upsilon, \\varUpsilon$ Upsilon is pronounced as \u0026ldquo;upsilon\u0026rdquo;. The TeX codes are \\Upsilon, \\upsilon, \\varUpsilon respectively.\nIn particle physics, an upsilon meson $\\varUpsilon$ Chi $\\Chi, \\chi$ Chi is pronounced as \u0026ldquo;chi\u0026rdquo;. Teachers advise not to write $x$ as $\\chi$ because it is actually chi $x$, not \u0026ldquo;x\u0026rdquo; $\\chi$. Please do not write it like this. The TeX codes are \\Chi, \\chi respectively.\nCharacteristic function $\\chi$ Psi $\\Psi, \\psi$ Psi is pronounced as \u0026ldquo;psi\u0026rdquo;. The TeX codes are \\Psi, \\psi respectively.\nAlong with $\\phi$, it is often used to represent an arbitrary function.\nIn quantum mechanics, wave function $\\psi$ Phi $\\Phi, \\phi ,\\varphi$ Phi is pronounced as \u0026ldquo;phi\u0026rdquo; or \u0026ldquo;fi\u0026rdquo;. It seems that in physics, it is often pronounced as \u0026ldquo;fi\u0026rdquo;, while in mathematics, it is pronounced as \u0026ldquo;phi\u0026rdquo;. The TeX codes are \\Phi, \\phi, \\varphi respectively.\nAlong with $\\psi$, it is often used to represent an arbitrary function.\nVariable in cylindrical coordinates $\\phi$](../1795) Variable in spherical coordinates $\\phi$](../152) In quantum mechanics, wave function $\\phi$ Omega $\\Omega, \\omega$ Omega is pronounced as \u0026ldquo;omega\u0026rdquo;. The TeX codes are Omega, omega respectively.\nIt is the last letter of the Greek alphabet, and the phrase \u0026ldquo;alpha and omega\u0026rdquo; means \u0026ldquo;the beginning and the end\u0026rdquo;, \u0026ldquo;everything\u0026rdquo;.\nIn partial differential equations, functional analysis, an open set $\\Omega$: It is commonly used notation along with $U$. In physics, the unit of resistance $\\Omega$ Solid angle of a sphere $\\Omega$ In physics, angular frequency $\\omega$ Variable for the Fourier transform of $t$ $\\omega$ Complex roots of a polynomial $\\omega$ ","id":3145,"permalink":"https://freshrimpsushi.github.io/en/posts/3145/","tags":null,"title":"How to Read and Write Greek Characters and Their Meaning in Mathematics and Science"},{"categories":"줄리아","contents":"Overview1 The package is named Calculus.jl, but it does not support integration.\nIf automatic differentiation, as discussed in machine learning, is needed, refer to the Zygote.jl package.\nDifferentiation of Single Variable Function Derivative function derivative() It calculates the derivative of $f : \\R \\to \\R$.\nderivative(f) or derivative(f, :x): Returns the derivative $f^{\\prime}$. derivative(f, a): Returns the differential coefficient $f^{\\prime}(a)$. julia\u0026gt; f(x) = 1 + 2x + 3x^2\rf (generic function with 1 method)\rjulia\u0026gt; g(x) = sin(x)\rg (generic function with 1 method)\rjulia\u0026gt; derivative(f)\r#1 (generic function with 1 method)\rjulia\u0026gt; derivative(f, 1)\r7.99999999996842\rjulia\u0026gt; Df = derivative(f)\r#1 (generic function with 1 method)\rjulia\u0026gt; Dg = derivative(g)\r#1 (generic function with 1 method)\r#f\u0026#39;(x) = 2 + 6x\rjulia\u0026gt; Df(1)\r7.99999999996842\r#g\u0026#39;(x) = cos x\rjulia\u0026gt; Dg(pi)\r-0.9999999999441258 Composite functions can also be differentiated.\n#f∘g(x) = (2 + 6 sin x)cos x\rjulia\u0026gt; derivative(f∘g)\r#1 (generic function with 1 method)\rjulia\u0026gt; derivative(f∘g, pi/4)\r4.414213562300037\rjulia\u0026gt; (2+6sin(pi/4))cos(pi/4)\r4.414213562373095 Second derivative function second_derivative() It calculates the second derivative of $f : \\R \\to \\R$.\nThe function returned by derivative() can take integers as input, but second_derivative() cannot use integer types. Irrational number types are also not allowed.\njulia\u0026gt; derivative(f, 1)\r7.99999999996842\rjulia\u0026gt; second_derivative(f, 1)\rERROR: MethodError: no method matching eps(::Type{Int64})\rClosest candidates are:\reps() at float.jl:764\reps(::AbstractFloat) at float.jl:760\reps(::Type{Float16}) at float.jl:761\rjulia\u0026gt; second_derivative(f, 1.)\r5.9999956492003905\rjulia\u0026gt; second_derivative(g, pi)\rERROR: MethodError: no method matching eps(::Type{Irrational{:π}})\rClosest candidates are:\reps() at float.jl:764\reps(::AbstractFloat) at float.jl:760\reps(::Type{Float16}) at float.jl:761\rjulia\u0026gt; second_derivative(g, convert(Float64, pi))\r-1.3553766145945872e-7\rjulia\u0026gt; second_derivative(g, 1pi)\r-1.3553766145945872e-7 Differentiation of Multivariable Function Gradient gradient() Returns the gradient of $f : \\mathbb{R}^{n} \\to \\mathbb{R}$.\nNote that when defining a multivariable function, it should not be defined as a function with multiple real variables. It must be defined as a single-variable function that takes a vector as input. If it\u0026rsquo;s not a function accepting a vector as input, differentiation can be performed, but the value cannot be calculated. For example, it should not be defined as $f_{1}$, but rather as $f_{2}$.\njulia\u0026gt; f₁(x,y,z) = x*y + z^2\rf₁ (generic function with 1 method)\rjulia\u0026gt; Calculus.gradient(f₁)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₁ = Calculus.gradient(f₁)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₁(1,1,1)\rERROR: MethodError: no method matching (::Calculus.var\u0026#34;#2#4\u0026#34;{typeof(f₁), Symbol})(::Int64, ::Int64, ::Int64)\rjulia\u0026gt; ∇f₁([1,1,1])\rERROR: MethodError: no method matching f₁(::Vector{Float64})\rjulia\u0026gt; Calculus.gradient(f₁, 1,1,1)\rERROR: MethodError: no method matching gradient(::typeof(f₁), ::Int64, ::Int64, ::Int64)\rjulia\u0026gt; Calculus.gradient(f₁, [1,1,1])\rERROR: MethodError: no method matching f₁(::Vector{Float64})\rjulia\u0026gt; f₂(x) = x[1]*x[2] + x[3]^2\rf₂ (generic function with 1 method)\rjulia\u0026gt; Calculus.gradient(f₂, [1,1,1])\r3-element Vector{Float64}:\r1.0000000000235538\r1.0000000000235538\r1.9999999999737708\rjulia\u0026gt; ∇f₂ = Calculus.gradient(f₂)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₂(1,1,1)\rERROR: MethodError: no method matching (::Calculus.var\u0026#34;#2#4\u0026#34;{typeof(f₂), Symbol})(::Int64, ::Int64, ::Int64)\rjulia\u0026gt; ∇f₂([1,1,1])\r3-element Vector{Float64}:\r1.0000000000235538\r1.0000000000235538\r1.9999999999737708 Hessian hessian() Returns the Hessian of $f : \\mathbb{R}^{n} \\to \\mathbb{R}$.\nSimilar to second_derivative(), it only accepts Float data types as input. Like gradient(), it can only return values for functions that take vectors as input.\njulia\u0026gt; hessian(f₂, [1.,1.,1.])\r3×3 Matrix{Float64}:\r3.88008e-7 1.0 0.0\r1.0 3.88008e-7 0.0\r0.0 0.0 2.0\rjulia\u0026gt; H = hessian(f₂)\r#7 (generic function with 1 method)\rjulia\u0026gt; H([1.,1.,1.])\r3×3 Matrix{Float64}:\r3.88008e-7 1.0 0.0\r1.0 3.88008e-7 0.0\r0.0 0.0 2.0 Jacobian jacobian() Returns the Jacobian of $f : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$.\nSimilar to second_derivative(), it only accepts Float data types as input. Like gradient(), it can only return values for functions that take vectors as input.\nUnlike other functions, using it like jacobian(f, [x, y, z]) is not possible.\njulia\u0026gt; h(x) = [x[1], x[1]*x[2], x[1]*x[3]^2]\rh (generic function with 2 methods)\rjulia\u0026gt; jacobian(h, [1.,1.,1.])\rERROR: MethodError: no method matching jacobian(::typeof(h), ::Vector{Float64})\rjulia\u0026gt; Jh = jacobian(h)\r(::Calculus.var\u0026#34;#g#5\u0026#34;{typeof(h), Symbol}) (generic function with 1 method)\rjulia\u0026gt; Jh([1.,1.,1.])\r3×3 Matrix{Float64}:\r1.0 0.0 0.0\r1.0 1.0 0.0\r1.0 0.0 2.0 Symbolic Differentiation Symbolic differentiation is also available in the SymEngine.jl package.\ndifferentiate() Performs symbolic differentiation.\nWhile it returns constant terms and $x$ neatly, for cases like $ax$ or $x^{n}$, it returns in the form of the product rule. For example, differentiating $3x^{2}$ would return it as the product of $3$ and $x^{2}$, seen as $\\dfrac{d 3}{dx} x^{2} + 3\\dfrac{d x^{2}}{dx}$. Even $x^{2}$ is seen as the product of $1$ and $x^{2}$.\njulia\u0026gt; differentiate(\u0026#34;1\u0026#34;, :x)\r0\rjulia\u0026gt; differentiate(\u0026#34;1 + x\u0026#34;, :x)\r1\rjulia\u0026gt; differentiate(\u0026#34;x^2\u0026#34;, :x)\r:(2 * 1 * x ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;x^3\u0026#34;, :x)\r:(3 * 1 * x ^ (3 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + x^2\u0026#34;, :x)\r:(1 + 2 * 1 * x ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + 2x + 3x^2 + 4x^3\u0026#34;, :x)\r:((0 * x + 2 * 1) + (0 * x ^ 2 + 3 * (2 * 1 * x ^ (2 - 1))) + (0 * x ^ 3 + 4 * (3 * 1 * x ^ (3 - 1))))\rjulia\u0026gt; differentiate(\u0026#34;x^2 * sin(x) + exp(x) * cos(x)\u0026#34;, :x)\r:(((2 * 1 * x ^ (2 - 1)) * sin(x) + x ^ 2 * (1 * cos(x))) + ((1 * exp(x)) * cos(x) + exp(x) * (1 * -(sin(x))))) Characters not specified as symbols are treated as constants, and if more than one symbol is input, it returns the differentiation for each symbol. However, writing \u0026quot;3yx\u0026quot; treats xy as a single variable, so it\u0026rsquo;s important to explicitly denote multiplication by writing it as 3x*y.\njulia\u0026gt; differentiate(\u0026#34;1 + x + 3yx + y^2\u0026#34;, :x)\r:(1 + (0yx + 3 * 0))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3y*x + y^2\u0026#34;, :x)\r:(1 + ((0y + 3 * 0) * x + (3y) * 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3yx + y^2\u0026#34;, [:x, :y])\r2-element Vector{Any}:\r:(1 + (0yx + 3 * 0))\r:((0yx + 3 * 0) + 2 * 1 * y ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, [:x, :y])\r2-element Vector{Any}:\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\r:(((0 * x + 3 * 0) * y + (3x) * 1) + 2 * 1 * y ^ (2 - 1)) simplify() The return of differentiate() lacks readability, but simplify() neatly organizes it. However, it doesn\u0026rsquo;t perform properly if vectors are used as input.\njulia\u0026gt; simplify(differentiate(\u0026#34;1 + 2x + 3x^2 + 4x^3\u0026#34;, :x))\r:(2 + 3 * (2x) + 4 * (3 * x ^ 2))\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :x))\r:(1 + 3y)\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :y))\r:(3x + 2y)\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, [:x, :y]))\r2-element Vector{Any}:\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\r:(((0 * x + 3 * 0) * y + (3x) * 1) + 2 * 1 * y ^ (2 - 1)) deparse() Turns the return of symbolic differentiation into a string.\njulia\u0026gt; a = differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :x)\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\rjulia\u0026gt; deparse(a)\r\u0026#34;1 + ((0 * x + 3 * 1) * y + (3 * x) * 0)\u0026#34;\rjulia\u0026gt; deparse(simplify(a))\r\u0026#34;1 + 3 * y\u0026#34; Verification check_derivative() One can check how much the derivative obtained by derivative() differs from the actual derivative. It\u0026rsquo;s implemented for the four types of derivatives excluding jacobian().\ncheck_derivative(f, Df, a): Returns the absolute value of derivative(f, a)-Df(a). julia\u0026gt; f(x) = 1 + x^2\rf (generic function with 1 method)\rjulia\u0026gt; Df(x) = 2x\rDf (generic function with 1 method)\rjulia\u0026gt; Calculus.check_derivative(f, Df, 1)\r2.6229241001374248e-11 Application Polynomials.jl Although derivative is implemented in Polynomials.jl itself, the derivatives can also be calculated with Calculus.derivative().\njulia\u0026gt; p = Polynomial([1,2,4,1])\rPolynomial(1 + 2*x + 4*x^2 + x^3)\rjulia\u0026gt; Polynomials.derivative(p)\rPolynomial(2 + 8*x + 3*x^2)\rjulia\u0026gt; Calculus.derivative(p)\r#1 (generic function with 1 method)\rjulia\u0026gt; Polynomials.derivative(p,2)\rPolynomial(8 + 6*x)\rjulia\u0026gt; Calculus.second_derivative(p)\r#6 (generic function with 1 method) Environment OS: Windows10 Version: Julia 1.6.2, Calculus 0.5.1 https://github.com/JuliaMath/Calculus.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3135,"permalink":"https://freshrimpsushi.github.io/en/posts/3135/","tags":null,"title":"How to Find Derivatives in Julia"},{"categories":"줄리아","contents":"Code fill() function can be used. It serves a similar purpose to the rep() function in R.\n","id":2101,"permalink":"https://freshrimpsushi.github.io/en/posts/2101/","tags":null,"title":"How to Create an Array Filled with a Specific Value in Julia"},{"categories":"기하학","contents":"Buildup1 To define a tangent vector at each point on a differentiable manifold $M$, let\u0026rsquo;s assume a differentiable curve $\\alpha : (-\\epsilon , \\epsilon) \\to M$ is given. We would like to define the derivative $\\dfrac{d \\alpha}{dt}(0)$ at $t=0$ in $\\alpha$ as a tangent vector, like in differential geometry, but since the range of $\\alpha$ is $M$ (since it\u0026rsquo;s not guaranteed to be a metric space), we cannot speak of the derivative of $\\alpha$. For this reason, tangent vectors on a manifold are defined as functions, namely operators. If you\u0026rsquo;ve studied differential geometry, treating vectors as operators should be familiar. See the following explanation.\nDirectional derivative\nLet $\\mathbf{X} \\in T_{p}M$ be a tangent vector at point $p$ of surface $M$, and let $\\alpha (t)$ be a curve on $M$. Then $\\alpha : (-\\epsilon, \\epsilon) \\to M$ and $\\alpha (0) = p$ are satisfied, meaning $\\mathbf{X} = \\dfrac{d \\alpha}{d t} (0)$. Now, let function $f$ be a differentiable function defined in some neighborhood of point $p \\in M$ on surface $M$. Then, the directional derivativedirectional derivative $\\mathbf{X}f$ in the direction of $\\mathbf{X}$ is defined as follows:\n$$ \\mathbf{X} : \\mathcal{D} \\to \\mathbb{R}, \\quad \\text{where } \\mathcal{D} \\text{ is set of all differentiable functions near } p $$\n$$ \\mathbf{X} f := \\dfrac{d}{dt_{}} (f \\circ \\alpha) (0) $$\nAs shown in the definition above, if there is a fixed tangent vector $\\mathbf{X}$, then every time $f$ is given, $\\mathbf{X}f$ is determined. Therefore, a tangent vector is treated as an operator itself. The notation like $\\mathbf{X}f$ is used because it is viewed from the perspective of an operator. Tangent vectors on a differential manifold are similarly defined as functions that map real space through the composition with some curve $\\alpha$ every time a differentiable function $f$ is given on $M$.\nDefinition Let\u0026rsquo;s say $M$ is a $n$-dimensional differentiable manifold. A differentiable function $\\alpha : (-\\epsilon , \\epsilon) \\to M$ is called a differentiable curve at $M$. Assuming $\\alpha (0)=p\\in M$, let\u0026rsquo;s define the set $\\mathcal{D}$ as the set of differentiable functions at $p$.\n$$ \\mathcal{D} := \\left\\{ f : M \\to \\mathbb{R} | \\text{functions on } M \\text{that are differentiable at } p \\right\\} $$\nThen, the tangent vector $\\alpha^{\\prime}(0) : \\mathcal{D} \\to \\mathbb{R}$ at $\\alpha (0) = p$ is defined as the following function.\n$$ \\alpha^{\\prime} (0) f = \\dfrac{d}{dt} (f\\circ \\alpha)(0),\\quad f\\in \\mathcal{D} $$\nThe set of all tangent vectors at point $p\\in M$ is called the tangent spacetangent space and is denoted as $T_{p}M$.\nExplanation $f : M \\to \\mathbb{R}$ and $\\alpha : (-\\epsilon, \\epsilon) \\to M$ cannot be differentiated in the classical sense because their domains and ranges are not guaranteed to be metric spaces, but their composition $f \\circ \\alpha : (-\\epsilon, \\epsilon) \\to \\mathbb{R}$ can be differentiated.\nSince a tangent vector is determined whenever a differentiable curve $\\alpha$ is given, it can be thought that there are as many tangent vectors as there are differentiable curves. Moreover, even if two tangent vectors $\\mathbf{X}, \\mathbf{Y}$ are determined by two different curves $\\alpha$ and $\\beta$, if $\\mathbf{X}f = \\mathbf{Y}f$ holds for all $f \\in \\mathcal{D}$, then $\\mathbf{X}$ and $\\mathbf{Y}$ are considered the same tangent vector.\nThe reason the set of tangent vectors $T_{p}M$ is called a tangent space is that it is actually a $n$-dimensional vector space.\nFrom the theorem introduced below, it is possible to express the function value $\\alpha^{\\prime}(0)f$ of the tangent vector at point $p$ in terms of any coordinate system $\\mathbf{x} : U \\to M$ concerning $p$, and this value does not depend on the choice of $\\mathbf{x}$.\nExample Consider $T_{p}\\mathbb{R}^{3}$. When a differentiable curve $\\alpha : (-\\epsilon, \\epsilon) \\to \\mathbb{R}^{3}$ is determined, a 3-dimensional vector $\\alpha^{\\prime}(0) = \\mathbf{v} = (v_{1}, v_{2}, v_{3}) \\in \\mathbb{R}^{3}$ is determined. Therefore, according to the definition, the tangent vector for $f : \\mathbb{R}^{3} \\to \\mathbb{R}$ is as follows:\n$$ \\mathbf{X}f = \\dfrac{d (f\\circ \\alpha)}{d t}(0) = \\sum \\limits_{i} \\dfrac{\\partial f}{\\partial x_{i}}\\dfrac{d \\alpha_{i}}{d t}(0) = \\sum\\limits_{i} v_{i} \\dfrac{\\partial f}{\\partial x_{i}} $$\nThis is the same as the directional derivative in Euclidean space.\n$$ \\mathbf{v}[f] = \\nabla _{\\mathbf{v}}f = \\mathbf{v} \\cdot \\nabla f = \\sum \\limits_{i} v_{i} \\dfrac{\\partial f}{\\partial v_{i}} $$\nThe directional derivative is essentially the same as treating the vector as an operator. Therefore, $\\mathbf{X}$ can be considered an element of $\\mathbb{R}^{3}$, and the following holds:\n$$ T_{p}\\mathbb{R}^{3} \\approxeq \\mathbb{R}^{3} $$\nTheorem Let\u0026rsquo;s say a differentiable curve $\\alpha (0) = p$ and a coordinate system $\\mathbf{x} : U \\to M$ at point $p$ are given. $(u_{1}, \\dots, u_{n})$ are the coordinates of $\\mathbb{R}^{n}$,\n$$ (x_{1}(p), \\dots, x_{n}(p)) = \\mathbf{x}^{-1}(p) $$\nThen, the following formula holds:\n$$ \\begin{align*} \\alpha ^{\\prime} (0) f =\u0026amp;\\ \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(p) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{p} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(\\alpha (0)) \\left.\\dfrac{\\partial f}{\\partial x_{i}}\\right|_{t=0} \\end{align*} $$\nHere, we simply denote it as $x_{i}^{\\prime}(0) = x_{i}^{\\prime}(\\alpha (0))$. Therefore, $\\alpha^{\\prime}(0)$ is defined as the following differential operator:\n$$ \\begin{equation} \\alpha ^{\\prime} (0) = \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} \\end{equation} $$\nIf we express it as coordinate vectors for basis $\\left\\{ \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} \\right\\}$, it is as follows:\n$$ \\alpha ^{\\prime} (0) = \\begin{bmatrix} x_{1}^{\\prime}(0) \\\\ \\vdots \\\\ x_{n}^{\\prime}(0) \\end{bmatrix} $$\nProof Choose a coordinate system $\\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to M$ such that $p = \\mathbf{x}(0)$ is satisfied. Consider $f\\circ \\alpha = f \\circ \\mathbf{x} \\circ \\mathbf{x}^{-1} \\circ \\alpha$ to express the tangent vector in terms of the coordinate system. Then, since $\\mathbf{x} \\circ \\mathbf{x}^{-1} = I$ is an identity function, any choice of coordinate system is irrelevant. Now, think of $f \\circ \\mathbf{x}$ and $\\mathbf{x}^{-1} \\circ \\alpha$ as one function each, and consider $f \\circ \\alpha$ as their composite function.\n$$ f \\circ \\alpha = (f \\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha) $$\nFirst, consider $f \\circ \\mathbf{x}$. Since $f \\circ \\mathbf{x} : \\mathbb{R}^{n} \\to \\mathbb{R}$, it can be expressed as follows and differentiated in the classical sense:\n$$ f \\circ \\mathbf{x} = f \\circ \\mathbf{x} (u) = f \\circ \\mathbf{x} (u_{1}, u_{2}, \\dots, u_{n}),\\quad u=(u_{1},\\dots,u_{n}) \\in \\mathbb{R}^{n} $$\n$\\mathbf{x}^{-1} \\circ \\alpha$ can also be expressed as follows since $\\mathbf{x}^{-1} \\circ \\alpha : \\mathbb{R} \\to \\mathbb{R}^{n}$, and it can be differentiated in the classical sense:\n$$ \\begin{align*} \\mathbf{x}^{-1} \\circ \\alpha (t) =\u0026amp;\\ (x_{1}(\\alpha (t)), x_{2}(\\alpha (t)), \\dots, x_{n}(\\alpha (t))) \\\\ =\u0026amp;\\ (x_{1}(t), x_{2}(t), \\dots, x_{n}(t)) \\end{align*} $$\nNote that $x_{i}$ is a function of $x_{i} : M \\to \\mathbb{R}$, and $x_{i}(t)$ is a simplified notation of $x_{i}(\\alpha (t))$.\nThinking this way, $f \\circ \\alpha$ is a composition of two functions, mapped as $\\mathbb{R} \\overset{\\mathbf{x}^{-1} \\circ \\alpha}{\\longrightarrow} \\mathbb{R}^{n} \\overset{f\\circ \\mathbf{x}}{\\longrightarrow} \\mathbb{R}$. Therefore, by the chain rule, the following holds:\n$$ \\dfrac{d}{d t}(f \\circ \\alpha) = \\dfrac{d}{dt} \\left( (f\\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha) \\right) = \\sum \\limits_{i=1}^{n}\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}} \\dfrac{d (\\mathbf{x}^{-1} \\circ \\alpha )_{i}}{d t} = \\sum \\limits_{i=1}^{n}\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}} \\dfrac{d x_{i}}{d t} $$\nThus, we obtain the following:\n$$ \\begin{align*} \\alpha^{\\prime}(0) f :=\u0026amp;\\ \\dfrac{d}{dt} (f\\circ \\alpha)(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\dfrac{d x_{i}}{d t}(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} x_{i}^{\\prime}(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\end{align*} $$\nHere, let\u0026rsquo;s define $\\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0}$ as the following operator:\n$$ \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} f := \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} $$\nSummarizing the meaning of $\\dfrac{\\partial f}{\\partial x_{i}}$:\n$f$ cannot be differentiated since its domain is $M$. Therefore, consider the composition with coordinate system $\\mathbf{x} : \\mathbb{R}^{n} \\to M$. This maps $\\mathbb{R}^{n}$ to $\\mathbb{R}$, thus can be differentiated in the classical sense. Therefore, $\\dfrac{\\partial f}{\\partial x_{i}}$ is defined as differentiating after composing $f$ with $\\mathbf{x}$ in Euclidean space $\\mathbb{R}^{n}$ at the $u_{i}$-th variable.\nFinally, we obtain the following:\n$$ \\begin{align*} \\alpha^{\\prime}(0) f =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0}f = \\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial f}{\\partial x_{i}}\\right|_{t=0} \\end{align*} $$\n$$ \\implies \\alpha^{\\prime}(0) = \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} $$\n■\nSee Also Directional derivative in analysis Directional derivative in differential geometry Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3132,"permalink":"https://freshrimpsushi.github.io/en/posts/3132/","tags":null,"title":"Tangent Vector on Differentiable Manifold"},{"categories":"줄리아","contents":"Code The code to read a shp file named XsDB_주거인구_100M_TM.shp is as follows.\nusing Shapefile\rcd(@__DIR__)\rpath = \u0026#34;XsDB_주거인구_100M_TM.shp\u0026#34;\rtable = Shapefile.Table(path)\rusing DataFrames\rdf = DataFrame(table) Of course, just reading the file is limited in what can be done, and it is necessary to convert it to a dataframe to examine the data.\nExecution result 959660×16 DataFrame\rRow │ geometry MEGA_NM MEGA_CD CTY_NM CTY_CD X_AXIS Y_AXIS HOUS POP POP_10 POP_20 POP_30 POP_40 POP_50 POP_60_O \\xb9\\xe8\\xc6\\xf7ó │ Point…? String String String String Int64 Int64 Float64 Float64 Float64 Float64 Float64 Float64 Float64 Float64 String\r────────┼─────────────────────────────────────────────────────────────────────────────────────────────────\r1 │ Point(254298.0, 4.26549e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 365950 526450 10.08 24.56 4.64 1.92 1.84 4.72 4.32 7.12 biz-gis.com\r2 │ Point(2.59622e5, 4.24405e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 371250 524250 1.42 3.47 0.81 0.29 0.52 0.52 0.59 0.74 biz-gis.com\r3 │ Point(2.61134e5, 423221.0) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 372750 523050 1.26 3.08 0.68 0.28 0.35 0.49 0.45 0.83 biz-gis.com\r4 │ Point(2.50311e5, 4.15806e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 361850 515750 10.08 25.2 3.68 2.96 1.68 4.4 6.0 6.48 biz-gis.com\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮\r959658 │ Point(2.09955e5, 2.46768e5) \\xc0\\xfc\\xb6\\xf3\\xbaϵ\\xb5 45 \\xbf\\xcf\\xc1ֱ\\xba 45710 319750 347150 1.83 4.53 1.92 0.24 0.45 0.72 0.69 0.51 biz-gis.com\r959659 │ Point(215588.0, 4.55344e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xb3\\xb2\\xbe\\xe7\\xc1ֽ\\xc3 41360 327550 555650 2.38 5.31 0.91 0.74 0.57 0.97 1.05 1.07 biz-gis.com\r959660 │ Point(2.54717e5, 4.24754e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 366350 524650 1.26 3.07 0.58 0.24 0.23 0.59 0.54 0.89 biz-gis.com\r959653 rows omitted Environment OS: Windows julia: v1.5.0 ","id":2097,"permalink":"https://freshrimpsushi.github.io/en/posts/2097/","tags":null,"title":"How to Read SHP Files in Julia"},{"categories":"줄리아","contents":"## Summary To use the `trunc` function, simply pass `Int` as the first argument. ## Code julia\u0026gt; @time for t in 1:10^8 Int64(ceil(t/1000)) end 0.189653 seconds\njulia\u0026gt; @time for t in 1:10^8 trunc(Int64, ceil(t/1000)) end 0.128472 seconds\nThe two loops perform the identical task but with about a 1.5 times speed difference. The former drops the decimal points using `ceil` and type casts to `Int64`, whereas the latter returns an integer natively using the built-in capability of the `trunc` function, which is faster. People who are used to programming in other languages might find the straightforward commands like the upper loop more intuitive, but in Julia, there are many built-in functions that take a data type as the first argument to return a result, so the usage like in the lower loop will become more familiar. ## Environment - OS: Windows - julia: v1.5.0 ","id":2095,"permalink":"https://freshrimpsushi.github.io/en/posts/2095/","tags":null,"title":"How to truncate decimal points and convert to an integer in Julia"},{"categories":"줄리아","contents":"Overview To rename columns, you can use the rename!() function1.\nYou can rename them all at once by providing a list of strings, or individually.\nCode using DataFrames\rdf = DataFrame(rand(1:9, 10, 3), :auto)\rrename!(df, [\u0026#34;X\u0026#34;, \u0026#34;Y\u0026#34;, \u0026#34;Z\u0026#34;])\rrename!(df, :X =\u0026gt; :A) When executed, it first creates the following dataframe:\njulia\u0026gt; df = DataFrame(rand(1:9, 10, 3), :auto)\r10×3 DataFrame\rRow │ x1 x2 x3 │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 Renaming All at Once julia\u0026gt; rename!(df, [\u0026#34;X\u0026#34;, \u0026#34;Y\u0026#34;, \u0026#34;Z\u0026#34;])\r10×3 DataFrame\rRow │ X Y Z │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 Just provide a list of strings.\nRenaming One by One julia\u0026gt; rename!(df, :X =\u0026gt; :A)\r10×3 DataFrame\rRow │ A Y Z │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 This method, which is rarely seen in other languages, involves prefixing column names with : and mapping them with =\u0026gt;. In Julia, a variable that starts with : is a Symbol.\nEnvironment OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/change-column-names-of-a-dataframe-previous-methods-dont-work/48026/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2093,"permalink":"https://freshrimpsushi.github.io/en/posts/2093/","tags":null,"title":"Renaming Column Names of a DataFrame in Julia"},{"categories":"줄리아","contents":"Overview Using a k-d tree, a data structure favorable for multi-dimensional search, can increase speed when calculating distances between points without the need for creating a matrix; it\u0026rsquo;s sufficient just to calculate the distance. All related algorithms are implemented in NearestNeighbors.jl, so it\u0026rsquo;s recommended to check the official GitHub page.\nSpeed Comparison Let\u0026rsquo;s compare with a technique optimized for calculating distance matrices using the pairwise() function.\nThe result of running the above code is as follows. The last two command lines perform the same task, but the speed difference is about 500 times. In fact, the [time complexity](../1283) of searching in a k-d tree is very efficient, being $\\log n$. ```code2 Aside from just the issue of speed, the method using the k-d tree is also more convenient from the user\u0026#39;s perspective as it returns a 1-dimensional array. ## Environment - OS: Windows - julia: v1.6.2 ","id":2088,"permalink":"https://freshrimpsushi.github.io/en/posts/2088/","tags":null,"title":"How to Quickly Calculate Distances in Julia Using NearstNeighbors.jl"},{"categories":"줄리아","contents":"Code using CSV, DataFrames\rA = rand(1:10, 10)\rB = zeros(10)\rAB = DataFrame(hcat(A,B), [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;])\rCSV.write(\u0026#34;AB.csv\u0026#34;, AB) The write function of the CSV package allows you to easily output a two-dimensional array. A, B are one-dimensional arrays, which are combined using the hcat function to transform into a dataframe.\nExecution Result julia\u0026gt; using CSV, DataFrames\rjulia\u0026gt; A = rand(1:10, 10)\r10-element Array{Int64,1}:\r8\r5\r4\r3\r6\r4\r10\r6\r2\r9\rjulia\u0026gt; B = zeros(10)\r10-element Array{Float64,1}:\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\rjulia\u0026gt; AB = DataFrame(hcat(A,B), [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;])\r10×2 DataFrame\rRow │ A B │ Float64 Float64\r─────┼──────────────────\r1 │ 8.0 0.0\r2 │ 5.0 0.0\r3 │ 4.0 0.0\r⋮ │ ⋮ ⋮\r9 │ 2.0 0.0\r10 │ 9.0 0.0\r5 rows omitted\rjulia\u0026gt; CSV.write(\u0026#34;AB.csv\u0026#34;, AB)\r\u0026#34;AB.csv\u0026#34; Here is the actual csv file that was output.\nSee Also How to fix broken characters when outputting CSV CSV.write(bom = true) Environment OS: Windows julia: v1.6.3 ","id":2073,"permalink":"https://freshrimpsushi.github.io/en/posts/2073/","tags":null,"title":"How to Output a 2D Array to a CSV File in Julia"},{"categories":"줄리아","contents":"Overview In Julia, Unicode (UTF-8) is allowed for variable names. Therefore, you can use not only Greek letters but also superscripts, subscripts, and even Korean or emojis. There\u0026rsquo;s no particular need to use them, but odd codes like the following work fine.\njulia\u0026gt; α₁ = 2\r2\rjulia\u0026gt; α₂ = 1\r1\rjulia\u0026gt; println(α₁ \\ast\\ α₂)\r2\rjulia\u0026gt; 사인(t) = sin(t)\r사인 (generic function with 1 method)\rjulia\u0026gt; 😂 = 1:20\r1:20\rjulia\u0026gt; 사인.(😂)\r20-element Array{Float64,1}:\r0.8414709848078965\r0.9092974268256817\r⋮\r0.14987720966295234\r0.9129452507276277 Greek Letters You can use all the Greek letters used in tex like the example above.\nSuperscripts, Subscripts Superscripts and subscripts can be used by typing numbers after \\_, \\^. They’re too small to use frequently, but Greek letters, English, or parentheses can also be used.\nEmoji Emojis can be entered like in other editors by pressing window + . (cmd + comma).\n","id":2065,"permalink":"https://freshrimpsushi.github.io/en/posts/2065/","tags":null,"title":"Writing Greek Characters and Subscripts in Julia Variable Names"},{"categories":"머신러닝","contents":"This article is written for math majors to understand the principles of the backpropagation algorithm.\nNotation Given an artificial neural network like the one shown above. Let $\\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n_{0}})$ be the inputinput, $y_{j}^{l}$ be the $j$th node of the $l$th layer, $\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}})$ is the output output.\nLet $L \\in \\mathbb{N}$ be the number of hidden layers, and the components of $\\mathbf{n}=(n_{0}, n_{1}, \\dots, n_{L}, \\hat{n}) \\in \\mathbb{N}^{L+2}$ be the number of nodes in the input layer, $L$ hidden layers, and output layer, in that order. Also, for convenience, let the $0$th hidden layer be the input layer and the $L+1$th hidden layer be the output layer.\nLet $w_{ji}^{l}$ denote the weight connecting the $i$th node in the $l$th layer to the $j$th node in the next layer. Propagation from each layer to the next then occurs as shown in the image below.\nwhere $\\phi$ is an arbitrary activation function. Let us denote by $v_{i}^{l}$ the linear combination passed from the $l$th layer to the $j$th node of the next layer.\n$$ \\begin{align*} v_{j}^{l} \u0026amp;= \\sum _{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\\\ y_{j}^{l+1} \u0026amp;= \\phi ( v_{j}^{l} ) = \\phi \\left( \\sum \\nolimits_{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\right) \\end{align*} $$\nTo summarize, this looks like this\nSymbols Meaning $\\mathbf{x}=(x_{1}, x_{2}, \\dots, x_{n_{0}})$ input $y^{l}_{j}$ The $j$th node in the $l$th layer $\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}} )$ output $n_{l}$ Number of nodes in the $l$th layer $w_{ji}^{l}$ The weight connecting the $i$th node in the $l$th layer to the $j$th node in the next layer. $\\phi$ Activation Functions $v_{j}^{l} = \\sum \\limits _{i=1} ^{n_{l}} w_{ji}^{l}y_{i}^{l}$ Linear Combination $y^{l+1}_{j} = \\phi (v_{j}^{l})$ Propagation from $l$th layer to next layer Theorem Let $E = E(\\hat{\\mathbf{y}})$ be a proper differentiable loss function, then the way to optimize $E$ is to update the weights $w_{ji}^{l}$ at each layer as follows. $$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} \\label{thm} \\end{equation} $$\nWhere $\\alpha$ is the learning rate and $\\delta_{j}^{l}$ is as follows when $l=L$,\n$$ -\\delta_{j}^{L} = \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\nFor $l \\in \\left\\{ 0,\\dots, L-1 \\right\\}$,\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i=1}^{n_{l}} \\delta_{i}^{l+1} w_{i j}^{l+1} $$\nExplanation Let\u0026rsquo;s look at $(1)$. It says that we rely on the $l$th nodes $y_{j}^{l}$ to update the weights between the $l$th and $l+1$th layers, which makes sense since the output of each layer ultimately determines the output $\\hat{\\mathbf{y}}$. Also, $y_{j}^{l}$ can be viewed as inputs as they propagate from the $l$th to the $l+1$th layer, which is similar to how a linear regression model is trained with LMSLeast Mean Squares.\n$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha (\\mathbf{w}^{T}\\mathbf{x} - \\mathbf{y}) \\mathbf{x} $$\nThis optimization technique is called a back propagation algorithm because the outputs $y_{j}^{l}$ at each layer are computed from the input layer to the output layer, while the $\\delta_{j}^{l}$ for optimization are computed backwards from the output layer to the input layer as follows.\n$$ \\begin{align*} \\delta_{j}^{L} \u0026amp;= - \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} \\\\ \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{j}^{L} w_{ij}^{L} \\\\ \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\\\ \\delta_{j}^{L-3} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-3}) \\sum _{i} \\delta_{i}^{L-2} w_{ij}^{L-2} \\\\ \u0026amp;\\vdots \\\\ \\delta_{j}^{1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{1}) \\sum _{i} \\delta_{i}^{2} w_{ij}^{2} \\\\ \\delta_{j}^{0} \u0026amp;= \\phi ^{\\prime} (v_{j}^{0}) \\sum _{i} \\delta_{i}^{1} w_{ij}^{1} \\end{align*} $$\nProof Let\u0026rsquo;s say we\u0026rsquo;re done computing from the input layer to the output layer. We can modify the weights in such a way that the loss function $E$ decreases, using the gradient descent method.\n$$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} - \\alpha \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l} } \\label{gradesent} \\end{equation} $$\nSince each $y_{i}^{l}$ is a given value, we can solve for the partial derivative in a computable form. The partial differential on the right hand side is given by the chain rule.\n$$ \\begin{equation} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}}) }{\\partial v_{j}^{l}} \\dfrac{\\partial v_{j}^{l}}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial v_{j}^{l}} y_{i}^{l} \\label{chainrule} \\end{equation} $$\nLetting $-\\delta_{j}^{l}$ be the partial derivative of the right-hand side of $(3)$, we obtain $(1)$ from $(2)$.\n$$ w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} $$\nFind $\\delta_{j}^{l}$ at each floor as follows.\nCase $l = L$\nFor $j \\in \\left\\{ 1, \\dots, \\hat{n} \\right\\}$, the following holds.\n$$ \\begin{equation} -\\delta_{j}^{L} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial \\hat{y}_{j}} \\dfrac{d \\hat{y}_{j}}{d v_{j}^{L}} \\label{deltamL} \\end{equation} $$\nSince $\\hat{y}_{j} =\\phi (v_{j}^{L})$, we get\n$$ -\\delta_{j}^{L} (t) =\\phi ^{\\prime} (v_{j}^{L}(t)) \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\n■\nCase $l = L-1$\nFor $j \\in \\left\\{ 1, \\dots, n_{L-1} \\right\\}$, we have\n$$ -\\delta_{j}^{L-1} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-1}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L}} \\dfrac{d y_{j}^{L}}{d v_{j}^{L-1}} $$\nSince $y_{j}^{L} =\\phi (v_{j}^{L-1})$, we get\n$$ -\\delta_{j}^{L-1} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\dfrac{\\partial y_{j}^{L}}{\\partial v_{j}^{L-1}} = \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} $$\nThe partial derivative on the right-hand side is computed by the chain rule as follows.\n$$ \\begin{align*} -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L}} \\end{align*} $$\nHere, by $(4)$ and ${\\color{green}v_{i}^{L}=\\sum_{j}w_{ij}^{L}y_{j}^{L}}$, we get the following.\n$$ \\begin{align} \u0026amp;\u0026amp; -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i=1} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial v_{i}^{L}}} {\\color{green} \\dfrac{d v_{i}^{L}}{d y_{j^{L}}} } \\nonumber \\\\ \u0026amp;\u0026amp; \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{green} w_{ij}^{L} }\\nonumber \\\\ {}\\nonumber \\\\ \\implies \u0026amp;\u0026amp; \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ij}^{L} \\label{deltajL-1} \\end{align} $$\n■\nCase $l = L-2$\nFor $j \\in \\left\\{ 1, \\dots, n_{L-2} \\right\\}$\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-2}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} $$\nSince $y_{j}^{L-1} =\\phi (v_{j}^{L-2})$, we get\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} = \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} $$\nThe partial derivative on the right-hand side is computed by the chain rule as follows.\n$$ \\begin{align*} -\\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{\\partial y_{k}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}} \\dfrac{\\partial v_{k}^{L-1}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}}} {\\color{red}\\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} } {\\color{green}\\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}}} {\\color{purple}\\dfrac{d v_{k}^{L-1}}{\\partial y_{j}^{L-1}}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{red} w_{ik}^{L}} {\\color{green} \\phi^{\\prime}(v_{k}^{L-1})} {\\color{purple} w_{kj}^{L-1}} \\end{align*} $$\nSo we get the following\n$$ \\delta_{j}^{L-2} = -\\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) w_{kj}^{L-1} $$\nThen, by $(5)$, the following holds.\n$$ \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) = \\phi^{\\prime}(v_{k}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} = \\delta_{k}^{L-1} $$\nTHerefore we get the following\n$$ \\begin{align*} \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\delta_{k}^{L-1} w_{kj}^{L-1} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\end{align*} $$\n■\nGeneralization: $l \\in \\left\\{ 1, \\dots, L-1 \\right\\}$\nBased on the above results, we can generalize as follows for $j \\in \\left\\{ 1, \\dots, n_{l} \\right\\}$,\n$$ -\\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} $$\nSolving the partial derivative on the right-hand side by the chain rule is as follows.\n$$ \\begin{align*} \u0026amp;\\quad \\delta_{j}^{l} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{\\partial \\hat{y}_{i_{(1)}}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{\\partial y_{i_{(2)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{\\partial y_{i_{(3)}}^{L-1} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{j}^{l}} \\\\ \u0026amp; \\quad \\vdots \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{i_{(4)}}^{L-2}} \\cdots \\frac{d y_{i_{(L-l+1)}}^{l+1} }{d v_{i_{(L-l+1)}}^{l} } \\frac{\\partial v_{i_{(L-l+1)}}^{l} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} -\\delta_{i_{(1)}}^{L} w_{i_{(1)}i_{(2)}}^{L} \\phi^{\\prime}(v_{i_{(2)}}^{L-1}) w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\delta_{i_{(2)}}^{L-1}w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\delta_{i_{(3)}}^{L-2} w_{i_{(3)} i_{(4)}}^{L-2} \\cdots w_{i_{(L-l)} j}^{L} \\\\ \u0026amp;\\quad \\vdots \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\delta_{i_{(L-l)}}^{l+1} w_{i_{(l-l)} j}^{l} \\end{align*} $$\nTherefore to summarize\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i} \\delta_{i}^{l+1} w_{ij}^{l+1} $$\n■\n","id":3077,"permalink":"https://freshrimpsushi.github.io/en/posts/3077/","tags":null,"title":"Back Propagation Algorithm"},{"categories":"줄리아","contents":"Overview This section explains the interpolation convenience feature in Julia. Interpolation can be very handy as it allows for writing output statements in an easy and clean manner. Although it is not directly related to numerical analysis interpolation, the term intersects in meaning. For functionalities related to numerical analysis interpolation, refer to the usage of Interpolations.jl.\nCode The usage is quite straightforward. Simply write the variable inside the string with a dollar sign $를 붙이면 변수가 알아서 문자열처럼 읽힌다. 변수 그대로가 아닌 계산이 필요하면 굳이 밖에서 계산할 필요 없이 $() in front of it.\njulia\u0026gt; x = 12\r12\rjulia\u0026gt; y = -2\r-2\rjulia\u0026gt; println(\u0026#34;value of x, y: ▷eq2◁y\u0026#34;)\rvalue of x, y: 12, -2\rjulia\u0026gt; println(\u0026#34;value of x+y: $(x+y)\u0026#34;)\rvalue of x+y: 10 Environment OS: Windows julia: v1.5.0 ","id":2041,"permalink":"https://freshrimpsushi.github.io/en/posts/2041/","tags":null,"title":"How to Conveniently Print Variable Values in Julia, Interpolation"},{"categories":"줄리아","contents":"Guide Step 0. Install julia 1.6 or higher\nFrom version 1.6, you can add it to the environment variables during the installation process. Just check the indicated option and install. If using an older version, either install version 1.6 or higher, or follow the instructions below.\nStep 1. Check the Julia installation path\nCheck the installation path of Julia. If you haven\u0026rsquo;t altered anything, it should be stored in the following path.\nC:\\Users\\사용자명\\AppData\\Local\\Programs\\Julia x.x.x\\bin Usually, C:\\Users\\Username\\AppData is a hidden folder, so don\u0026rsquo;t panic if it\u0026rsquo;s not visible.\nCheck if the julia.exe file is present in the said path. Copy the path for use in Step 3.\nStep 2. Edit environment variables\nPress Windows+s or search for \u0026lsquo;Edit the system environment variables\u0026rsquo; in the control panel.\nClick on Environment Variables(N).\nIn the User variables window, find Path and click Edit(E).\nStep 3. Add Julia path\nClick New(N) or the last row, enter the copied path from Step 1 as shown above, and click OK to finish editing the environmental variables.\nStep 4. Reboot\nAfter rebooting, you can confirm that Julia runs by executing the julia command in powershell, etc.\nEnvironment OS: Windows julia: v1.5.2 ","id":2036,"permalink":"https://freshrimpsushi.github.io/en/posts/2036/","tags":null,"title":"Using Julia in Windows CMD and PowerShell"},{"categories":"줄리아","contents":"## Code [^1] [^1]: https://docs.julialang.org/en/v1/manual/metaprogramming/ Julia supports [metaprogramming](../1457) at the language level. Here is the result of reading and executing a string as code itself. julia\u0026gt; text = \u0026ldquo;f(x) = 2x + 1; f(2)\u0026rdquo; \u0026ldquo;f(x) = 2x + 1; f(2)\u0026rdquo;\njulia\u0026gt; code = Meta.parse(text) :($(Expr(:toplevel, :(f(x) = begin #= none:1 =# 2x + 1 end), :(f(2)))))\njulia\u0026gt; eval(code) 5\n- `Meta.Parse()`: 이 함수를 통해 입력된 문자열을 **표현식**으로 바꿔 반환한다.\r- `eval()`: 표현식을 **평가**한다. 위 예제코드에서는 $f(2)$ 가 실제로 평가되어 함숫값인 $5) ## Environment - OS: Windows - julia: v1.5.0 ","id":2024,"permalink":"https://freshrimpsushi.github.io/en/posts/2024/","tags":null,"title":"Metaprogramming in Julia"},{"categories":"줄리아","contents":"Code Use the vec() function.\njulia\u0026gt; A = rand(0:9, 3,4)\r3×4 Array{Int64,2}:\r6 8 7 3\r2 9 3 2\r5 0 6 7\rjulia\u0026gt; vec(A)\r12-element Array{Int64,1}:\r6\r2\r5\r8\r9\r0\r7\r3\r6\r3\r2\r7 To the human eye, it appears the same as a 1-dimensional array, but it\u0026rsquo;s actually a 2-dimensional array by type, which can cause errors. This method can solve those cases. The following two commands look exactly the same, but there\u0026rsquo;s a difference between being a $\\mathbb{N}^{10 \\times 1}$ matrix or $\\mathbb{N}^{10 }$ vector.\njulia\u0026gt; b = rand(0:9, 10,1)\r10×1 Array{Int64,2}:\r4\r8\r0\r4\r7\r4\r4\r2\r4\r7\rjulia\u0026gt; vec(b)\r10-element Array{Int64,1}:\r4\r8\r0\r4\r7\r4\r4\r2\r4\r7 The Actual flatten() Function In fact, the real flatten() function is implemented in Base.Iterators. The result of its operation is as follows, and honestly, you might not want to use it. To be more precise, it\u0026rsquo;s not so much changing the array, but it might be necessary when using it as an iterator in loops or similar contexts. Honestly, it\u0026rsquo;s unnecessary.\njulia\u0026gt; c = rand(0:9, 3,3)\r3×3 Matrix{Int64}:\r7 7 4\r9 3 8\r4 4 5\rjulia\u0026gt; Iterators.flatten(c)\rBase.Iterators.Flatten{Matrix{Int64}}([7 7 4; 9 3 8; 4 4 5])\rjulia\u0026gt; vec(c)\r9-element Vector{Int64}:\r7\r9\r4\r7\r3\r4\r4\r8\r5 Environment OS: Windows julia: v1.5.0 ","id":2022,"permalink":"https://freshrimpsushi.github.io/en/posts/2022/","tags":null,"title":"How to Flatten an Array in Julia"},{"categories":"줄리아","contents":"Conclusion Let\u0026rsquo;s calculate the distance between $n$ coordinates.\nIf it\u0026rsquo;s not necessary to calculate the distance between all coordinates, divide them into groups and create a rectangular distance matrix. The rectangular distance matrix can be calculated quickly and easily with the pairwise() function. Speed Comparison Let\u0026rsquo;s imagine doing a moving agent-based simulation for the SIR model. The original time complexity is $O \\left( n^{2} \\right)$, but if you divide it into $S$ and $I$ groups for calculation, the time complexity drops to $O \\left( n_{S} n_{I} \\right)$. Typically, the spread of diseases is implemented by calculating the distance matrix between $S$ and $I$ to judge whether they fall within a certain radius $\\varepsilon$, and by how much they have made contact. Let\u0026rsquo;s compare the speeds with this in mind.\nusing Distances\rusing StatsBase\rusing Random\rRandom.seed!(0);\rN = 10^4\rlocation = rand(2, N);\rstate = sample([\u0026#39;S\u0026#39;, \u0026#39;I\u0026#39;], Weights([0.1, 0.9]), N);\rS = location[:, state .== \u0026#39;S\u0026#39;]\rI = location[:, state .== \u0026#39;I\u0026#39;]\rfunction foo(S, I)\rcontact = Int64[]\rfor s in 1:996\rpush!(contact, sum(sum((I .- S[:,s]).^2, dims = 1) .\u0026lt; 0.1))\rend\rreturn contact\rend\r@time foo(S, I) Of course, the idea of dividing into groups to calculate improves performance significantly, not just in Julia but with any method used. The point is that there\u0026rsquo;s no need to loop through brute force when you can make good use of the Distance package\u0026rsquo;s pairwise() function.\njulia\u0026gt; @time foo(S, I);\r0.170835 seconds (7.98 k allocations: 210.854 MiB, 12.56% gc Time)\rjulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; 0.1, dims = 1);\r0.087875 seconds (14 allocations: 69.639 MiB, 4.15% gc Time) These two commands do exactly the same thing, but in terms of speed, one is about twice as fast as the other, and in terms of allocation, the difference is so substantial that one might not even want to calculate it, not to mention that coding difficulty is much easier compared to looping.\nFurther Research 1 When the Euclidean distance is the distance function, using SqEuclidean() instead of Euclidean() omits the root taking calculation, thus speeding up the process. The following yields exactly the same results, but the speed difference is about 1.5 times.\njulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; 0.1, dims = 1);\r0.091917 seconds (14 allocations: 69.639 MiB, 7.60% gc Time)\rjulia\u0026gt; @time sum(pairwise(SqEuclidean(),S,I) .\u0026lt; 0.01, dims = 1);\r0.061776 seconds (14 allocations: 69.639 MiB, 11.37% gc Time) Moreover, it can get even faster. At this point, simple code optimization will not be sufficient to increase speed, and a k-d tree2, a data structure favorable for multi-dimensional search, must be utilized. See how to quickly calculate distances with NearstNeighbors.jl.\nEnvironment OS: Windows julia: v1.5.0 https://github.com/JuliaStats/Distances.jl#pairwise-benchmark\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/K-d_tree\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2020,"permalink":"https://freshrimpsushi.github.io/en/posts/2020/","tags":null,"title":"Optimizing Distance Matrix Calculations in Julia"},{"categories":"줄리아","contents":"Overview The usage of the function sample() which serves a similar role to R\u0026rsquo;s sample() or Python package numpy\u0026rsquo;s random.choice(), and the Weights function in Julia.\nCode 1 using StatsBase\ritems = 0:5\rweights = 0:5\rsample(items, Weights(weights))\r# With replacement\rmy_samps = sample(items, Weights(weights), 10)\r# Without replacement\rmy_samps = sample(items, Weights(weights), 2, replace=false) Execution Result julia\u0026gt; using StatsBase\rjulia\u0026gt; items = 0:5\r0:5\rjulia\u0026gt; weights = 0:5\r0:5\rjulia\u0026gt; sample(items, Weights(weights))\r5\rjulia\u0026gt; # With replacement\rjulia\u0026gt; my_samps = sample(items, Weights(weights), 10)\r10-element Array{Int64,1}:\r4\r3\r2\r1\r3\r3\r5\r5\r2\r2\rjulia\u0026gt; # Without replacement\rjulia\u0026gt; my_samps = sample(items, Weights(weights), 2, replace=false)\r2-element Array{Int64,1}:\r4\r5 Environment OS: Windows julia: v1.5.0 https://stackoverflow.com/a/27560273/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2018,"permalink":"https://freshrimpsushi.github.io/en/posts/2018/","tags":null,"title":"How to Weight and Random Sample in Julia"},{"categories":"줄리아","contents":"Conclusion Comparing each element in an array using the Equal Operator == shows that Char is faster than integers.\nSpeed Comparison julia\u0026gt; integer = rand(1:5, N); print(typeof(integer))\rArray{Int64,1}\rjulia\u0026gt; character = rand([\u0026#39;S\u0026#39;,\u0026#39;E\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;,\u0026#39;D\u0026#39;], N); print(typeof(character))\rArray{Char,1}\rjulia\u0026gt; @time integer .== 1;\r0.009222 seconds (6 allocations: 1.196 MiB)\rjulia\u0026gt; @time character .== \u0026#39;S\u0026#39;;\r0.005266 seconds (7 allocations: 1.196 MiB) The above code identifies where 1 and S are located in an array made of integers and characters, respectively. Except for the difference between being an integer or a character, everything else is exactly the same, yet there\u0026rsquo;s a significant difference in time consumption, almost double. Therefore, using characters is recommended in the code optimization process as it\u0026rsquo;s a commonly used method.\nFurther Research julia\u0026gt; string = rand([\u0026#34;S\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;I\u0026#34;,\u0026#34;R\u0026#34;,\u0026#34;D\u0026#34;], N); print(typeof(string))\rArray{String,1}\rjulia\u0026gt; @time string .== \u0026#34;S\u0026#34;;\r0.072692 seconds (7 allocations: 1.196 MiB) As one would expect, using a String instead of a Character causes more than a tenfold increase in speed degradation.\nEnvironment OS: Windows julia: v1.5.0 ","id":2016,"permalink":"https://freshrimpsushi.github.io/en/posts/2016/","tags":null,"title":"Comparison of the Speed of the Equality Operator == for Characters and Integers in Julia"},{"categories":"프로그래밍","contents":"Overview A commonly used RGB color palette.\nCode ","id":2013,"permalink":"https://freshrimpsushi.github.io/en/posts/2013/","tags":null,"title":"RGB Color Cheat Sheet"},{"categories":"줄리아","contents":"Error ERROR: SystemError: opening file \u0026quot;C:\\\\Users\\\\rmsms\\\\.julia\\\\registries\\\\General\\\\Registry.toml\u0026quot;: No such file or directory\rCause It\u0026rsquo;s a really frustrating error, which, as the message indicates, occurs because the Registry.toml file does not exist at the specified path.\nSolution Delete the C:\\Users\\사용자이름\\.julia\\registries\\General folder and try again.\nThen, as shown above, the Registry.toml file will be created, and the installation will proceed normally.\n","id":2069,"permalink":"https://freshrimpsushi.github.io/en/posts/2069/","tags":null,"title":"Solving \\General\\Registry.toml: No such file or directory when Installing Julia Packages"},{"categories":"줄리아","contents":"Guide Step 1. Install Julia\nDownload the installation file from the Julia download page and run it.\nStep 2. Install VS Code\nDownload the installation file from the Visual Studio Code download page and run it.\nStep 3. Install Julia Extension\nOpen Extensions by clicking on the fifth icon from the left or pressing Ctrl + Shift + X. Search for \u0026lsquo;julia\u0026rsquo; and Julia Language Support will appear at the top.\nClick Install to install it.\nCreate a file with the .jl extension in the editor, write Julia code, and try running it in full by pressing Shift + Enter. The screenshot above has only one line, println(helloworld), to print out \u0026ldquo;helloworld\u0026rdquo;.\nEnvironment OS: Windows julia: v1.5.4 ","id":2067,"permalink":"https://freshrimpsushi.github.io/en/posts/2067/","tags":null,"title":"How to Install the Latest Version of Julia on Windows"},{"categories":"프로그래밍","contents":"개요 In this post, we\u0026rsquo;ll organize code that does the same thing in Julia, Matlab, Python, and R.\nCheat Sheet: Flux-PyTorch-TensorFlow Let\u0026rsquo;s say we have the following environment for Python.\nimport numpy as np Common Julia\rMatlab\rPython\rR\rcomment\r#comment %comment #comment #comment 2d grid\rX = kron(x, ones(size(y)))Y = kron(ones(size(x)), y) [X,Y] = meshgrid(x,y) np.meshgrid(x,y) How to create an n-dimensional meshgrid in Julia Type Julia\rMatlab\rPython\rR\rtype of elements\reltype(x) x.dtype changing type of elements\rconvert(Array{Float64},x) x.astype(\"float64\") type of x\rtypeof(x) type(x)#class of x Vector Julia\rMatlab\rPython\rR\rColumn vector\r[1 4 -1 2] [1;4;-1;2] np.array([1,4,-1,2]).reshape(-1,1) Row vector\r[1;; 4;; -1;; 2]or[1 4 -1 2]' [1 4 -1 2]or[1,4,-1,2] np.array([1,4,-1,2]) Zero vector\rzeros(n)#column vector zeros(n,1)#not zeros(n)#zeros(n)=zeros(n,n) np.zeros(n)#row vector matrix(0,n) Vector with only ones\rones(n)#column vector ones(n,1)#not ones(n)#ones(n)=ones(n,n) np.ones(n)#row vector n equally spaced sample(length)\rrange(0,stop=1,length=10)orLinRange(0,1,10) linspace(0,1,10)#row vector np.linspace(0,stop=1,num=10)#row vector n equally spaced sample(steps)\rrange(0, stop=1, step=0.1) 0 : 0.1 : 1 np.arange(0,1.001,0.1) n logarithmically spaced sample\r10 .^range(0,1,10) logspace(0,1,10)#row vector np.logspace(0,stop=1,num=10,base=10)#row vector Matrix Julia\rMatlab\rPython\rR\rzero matrix\rzeros(m,n) zeros(m,n) np.zeros([m,n]) matrix(0,m,n) flatten\rvec(x) x(:) np.ravel(x) return zeros with the same shape and dtype as a given x\rzero(x) np.zeros_like(x) return ones with the same shape and dtype as a given x\rfill!(similar(x), 1) np.ones_like(x) 차원dimensions\rndims(x) ndims(x) x.ndim 크기size of matrix\rsize(x) size(x) x.shape 성분의 수number of elements\rlength(x) numel(x) x.size 가장 큰 차원의 길이length of largest dimension\rmaximum(size(x)) length(x) max(x.shape) 무작위 추출Random sampling Julia\rMatlab\rPython\rR\r균등분포 랜덤 벡터Uniformly distributed random vector rand(n)#column vector rand(n,1)#not rand(n)#rand(n)=rand(n,n) np.random.rand(n)#row vector 균등분포 랜덤 행렬Uniformly distributed random matrix rand(m,n) rand(m,n) np.random.rand(m,n) 표준정규분포Normally distributed random numbers randn(m,n) randn(m,n) np.random.randn(m,n) 푸리에 변환Fourier Transform 줄리아에 대해서 다음과 같은 환경이라고 하자.\nusing FFTW Julia\rMatlab\rPython\rR\r푸리에 변환 Fourier transform\rfft(x) fft(x) np.fft.fft(x) fft(x) 푸리에 역변환 Inverse Fourier transform\rifft(x) ifft(x) np.fft.ifft(x) fft(y,inverse=TRUE)/length(y) 보간법Interpolation 파이썬에 대해서 다음과 같은 환경이라고 하자.\nfrom scipy.interpolate import griddata $X, Y, P, Z$를 2차원 배열, $x, y$를 1차월 배열이라고 하자.\nJulia\rMatlab\rPython\rR\r2차원 보간 2D interpolation\rZ=interp2(X,Y,P,x,y) Z=griddata((np.ravel(X),np.ravel(Y)),np.ravel(P),(x,y)) 시각화Visualization 줄리아에 대해서 다음과 같은 환경이라고 하자.\nusing Plots 파이썬에 대해서 다음과 같은 환경이라고 하자.\nimport matplotlib.pyplot as plt 2차원 이미지로 출력하고자 하는 배열을 $A$라고 하자.\nJulia\rMatlab\rPython\rR\r스케일 범위 지정 Set scale range\rheatmap(A,clim=(a,b)) plt.imshow(A)plt.colorbar()plt.clim(a,b) 수평선 Horizon line\rhline!(a) plt.axhline(a) 수직선 Vertical line\rvline!(a) plt.axvline(a) ","id":3031,"permalink":"https://freshrimpsushi.github.io/en/posts/3031/","tags":["줄리아"],"title":"Cheat Sheet : Equivalent Codes in Julia, Matlab, Python, R"},{"categories":"머신러닝","contents":"Definition Reinforcement learning is the process where an agent interacts with the environment to find a policy that maximizes the cumulative reward.\nDescription1 The elements comprising reinforcement learning are as follows:\nAgentagent: Decides actions based on a policy, given a state. Statestate: Refers to the situation in which the agent is placed. Actionaction: Refers to the choices available to the agent in a given state. Policypolicy: Refers to the strategy according to which the agent decides its actions in a given state. Rewardreward: Refers to the score the agent receives based on the actions it chooses in a given state. It can be considered the goal the agent aims to achieve. Environmentenvironment: When the agent decides an action in a given state, according to the MDP (Markov Decision Process), the next state and the corresponding reward are determined. Episodeepisode: Refers to the period from when the interaction between the agent and the environment begins to when it ends. This can be analogized in various scenarios as follows:\nReinforcement Learning Studying for Exams Go Game Agent Student Go Player State Days Left Until Exam Go Board Action Study, Drinking, Gaming, etc. Placing a Stone Policy Study Plan per Day Strategy Reward Exam Score Win/Loss Episode Exam Period One Game Problem of Reinforcement Learning: Grid Model A typical example to explain reinforcement learning is the grid world. Let\u0026rsquo;s consider a robot that can move one square at a time in four directions: up, down, left, and right, in the following $4 \\times 4$ grid. The starting square is randomly chosen from $\\boxed{\\ 2\\ }$ to $\\boxed{15}$, and the goal is to reach $\\fcolorbox{black}{darkgray}{\\ 1\\ }$ or $\\fcolorbox{black}{darkgray}{16}$ in the shortest distance possible.\nAgent In reinforcement learning, the agent is the subject of learning, although it does not exist in reality. Unlike other concepts defined later, such as random variables, the agent does not have a clear mathematical definition. Therefore, it\u0026rsquo;s possible to study the theory of reinforcement learning without the concept of an agent, and indeed, this is often the case. Essentially, in the theory of reinforcement learning, the policy is what represents the agent. However, for convenience, it\u0026rsquo;s common to think of the learning subject as an entity and express it as 'the agent acts', 'the state of the agent has changed', etc. The agent is merely something that appears to be learning in computer simulations (especially games). For instance, the movement of an agent in the grid model can be expressed simply as a sequence of states. $$ \\boxed{\\ 3\\ } \\to \\boxed{\\ 2\\ } \\to \\fcolorbox{black}{darkgray}{\\ 1\\ } $$ You only need to print the sequence $3, 2, 1$. Ultimately, what we want to obtain from reinforcement learning is essentially a policy, so learning can occur even without defining an agent. In short, an agent can be considered the visualization (materialization) of a policy.\nOf course, the above is true in theory and computer simulations. In actual applications such as autonomous driving, there is a need for drones or cars that move according to policies. In this case, robots or machines like drones and cars become agents, and without them, learning the policy would be impossible.\nState The statestate is a random variable, denoted by $S$. As the episode progresses sequentially over time, we use the index $t$ for time. Hence, the state function at time $t$ is denoted as $S_{t}$. The initial state is usually represented as $t=0$. Summarizing, $S_{t}$ is a function that gives function values for each of the squares at time $t$.\n$$ S_{t} \\left( \\boxed{ N } \\right) = n,\\quad 1\\le n \\le 16 $$\nIn this case, the set of all possible state values (the function values of the state function) is denoted as $\\mathcal{S}\\subset \\mathbb{R}$, and its elements are denoted as $s$.\n$$ \\mathcal{S} = \\left\\{ s_{1}, s_{2},\\dots \\right\\} $$\nTherefore, the state function for the above grid model is as follows.\n$$ S_{t} : \\left\\{ \\fcolorbox{black}{darkgray}{\\ 1\\ } , \\boxed{\\ 2\\ }, \\dots, \\boxed{15}, \\fcolorbox{black}{darkgray}{16} \\right\\} \\to \\mathcal{S} \\\\ S_{t} \\left( \\boxed{\\ n\\ } \\right) = s_{n} = n,\\quad 1\\le n \\le 16 $$\nThen, the probability that the state value at time $t$ was $s_6$ and changes to $s_{10}$ in the next time step is as follows.\n$$ P \\left( S_{t+1} = s_{10} | S_{t} = s_{6} \\right) $$\nA state where the episode ends once reached is called a terminal state. In the above grid model, the terminal states are $\\fcolorbox{black}{darkgray}{1}, \\fcolorbox{black}{darkgray}{16}$.\nAction The actionaction refers to the choices the agent can make in the current state, and it is also a random variable. Denoted as $A_{t}$, it represents the action at time $t$. In the grid model example above, one can choose up, down, left, or right in each of the squares from $\\boxed{2}$ to $\\boxed{15}$. The set of all possible action values (the function values of the action function) is denoted as $\\mathcal{A}\\subset \\mathbb{R}$, and its elements are denoted as $a$.\n$$ \\mathcal{A} = \\left\\{ a_{1}, a_{2}, \\dots \\right\\} $$\nThen, the action function at time $t$ is as follows.\n$$ A_{t} : \\left\\{ \\uparrow, \\rightarrow, \\downarrow, \\leftarrow \\right\\} \\to \\mathcal{A} \\\\ \\begin{cases} A_{t}(\\uparrow) = a_{1} \\\\ A_{t}(\\rightarrow) = a_{2} \\\\ A_{t}(\\downarrow) = a_{3} \\\\ A_{t}(\\leftarrow) = a_{4} \\end{cases} $$\nThe agent decides its actions based on probabilities given the current state. For example, the probability of choosing action $a_1$ in state $s_6$ at time $t$ is as follows.\n$$ P(A_{t} = a_{1} | S_{t} = s_{6}) $$\nPolicy The policypolicy specifies the probability of deciding action $a$ in state $s$ for all $s$ and $a$, denoted as $\\pi$. It\u0026rsquo;s analogous to a strategy in a game or war. In the grid model example, if the probability of deciding each action is $\\dfrac{1}{4}$, the policy $\\pi$ is as follows.\n$$ \\pi \\begin{cases} P(a_{1} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{2} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{2}) = \\dfrac{1}{4} \\\\ \\vdots \\\\ P(a_{2} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{4} | s_{15}) = \\dfrac{1}{4} \\end{cases} \\quad \\text{or} \\quad \\pi : \\mathcal{S} \\times \\mathcal{A} \\to [0,1] $$\nOf course, this is not an optimized policy. Considering just the case of $\\boxed{2}$, it\u0026rsquo;s better not to have any probability of going upwards as it would go out of the grid. Therefore, $\\pi_2$ can be said to be a better policy than $\\pi_1$ as shown in the illustration below.\nThe goal of reinforcement learning algorithms is to find the optimal policy. The question then arises, how do we find the optimal policy? It can be found through the value functionvalue function, which evaluates the quality of a policy.\nReward The rewardreward is a function that maps a real number based on the action chosen by the agent in a given state, denoted as $R_t$. The set of all reward values (the function values of the reward function) is denoted as $\\mathcal{R} \\subset \\mathbb{R}$, and its elements are denoted as $r$.\n$$ \\mathcal{R} = \\left\\{ r_{1}, r_{2}, \\dots \\right\\} \\\\ R_{t} = \\mathcal{S} \\times \\mathcal{A} \\to \\mathcal{R} $$\nA reward is received once at each time step, and the ultimate goal of reinforcement learning is to find a policy that maximizes the cumulative reward, the total reward received in an episode.\nOne might wonder why we focus on maximizing cumulative rewards rather than rewards at each time step. This can be easily understood through the analogy of studying for exams. During the exam period, spending every evening drinking and partying or playing games might be more enjoyable than studying. However, the cumulative reward, i.e., the exam score, would be terrible. Hence, even if studying is tiring and challenging in the moment, it\u0026rsquo;s considered better for the sake of future greater rewards.\nThe reward is a hyperparameter set by a person and should be appropriately decided based on the task the agent must perform. For instance, in the grid model example, if the grid is a maze and the agent is a robot escaping the maze, a reward of $-1$ for moving one square and a reward of $+10$ for reaching a terminal state can be set. If the grid is a park and the agent is a robot walking a pet, a reward of $0$ for moving one square and a reward of $+10$ for reaching a terminal state can be set.\nEnvironment The environmentenvironment is a function that decides the next state and reward based on the action chosen by the agent in a given state, i.e., $f : (s,a) \\mapsto (s^{\\prime},r)$. Therefore, it\u0026rsquo;s always challenging to find a perfect analogy in reality.\nLet\u0026rsquo;s say the state at time $t$ is $s_{t}$, and the action chosen at $s_{t}$ is $a_t$. Then, the next state\ndecided by the environment is $s_{t+1}$, and the reward is $r_{t+1}$. It can be represented as follows.\n$$ f(s_{t}, a_{t}) = (s_{t+1}, r_{t+1}) $$\nIf in the grid model example, the agent chose $\\uparrow$ at $\\boxed{7}$ and the environment decided the next state to be $\\boxed{3}$ and the reward to be $-1$, it can be expressed with the following formula.\n$$ f(s_{7}, a_{1}) = (s_{3}, -1) $$\nIf the strategy by which the agent decides actions is called a policy, the environment\u0026rsquo;s decision of the next state and reward is called the MDPmarkov decision process. The interaction between the agent and the environment can be illustrated as follows.\nEpisode The sequence of states, actions, and rewards determined by the interaction between the agent and the environment is called a trajectory or history. When the trajectory is finite, it\u0026rsquo;s referred to as an episode task. The exam period, Go game, and grid model examples mentioned earlier fall into this category.\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{T-1}, s_{T}, r_{T} \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{T-1}}{\\to} (s_{T}, r_{T}) $$\nWhen the trajectory is infinite, it\u0026rsquo;s referred to as a continuing task. However, very long episodes are sometimes considered infinite.\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{t-1}, s_{t}, r_{t}, a_{t}, s_{t+1}, r_{t+1},\\dots \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{t-1}}{\\to} (s_{t}, r_{t}) \\overset{a_{t}}{\\to} (s_{t+1}, r_{t+1}) \\overset{a_{t+1}}{\\to} \\cdots $$\nO Il-Seok, Machine Learning (MACHINE LEARNING). 2017, p466-480\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3029,"permalink":"https://freshrimpsushi.github.io/en/posts/3029/","tags":null,"title":"What is Reinforcement Learning in Machine Learning"},{"categories":"줄리아","contents":"Code The package provided for handling colors in Julia is Colors.jl. By loading the visualization package Plots.jl, one can also use the functionality within Colors.jl. Color codes representing the RGB space, such as RGB, BGR, RGB24, RGBX, XRGB, are supported and are subtypes of AbstractRGB. RGBA is RGB with added transparency.\njulia\u0026gt; using Plots\rjulia\u0026gt; subtypes(AbstractRGB)\r5-element Vector{Any}:\rBGR\rRGB\rRGB24\rRGBX\rXRGB\rjulia\u0026gt; subtypes(AbstractRGBA)\r2-element Vector{Any}:\rBGRA\rRGBA Strings By inputting a string such as \u0026quot;#FF0000\u0026quot; as a color keyword in the plot() function, one can use the HEX code, a hexadecimal RGB code. As one can see below, the reason why string input is also feasible seems to be because plot() automatically parses the string.\nusing Plots\rr = \u0026#34;#FF0000\u0026#34; # R빨간색 RGB(255, 0, 0)의 6자리 HEX 코드\rg = \u0026#34;#00FF0033\u0026#34; # 투명도가 0.2인 초록색 RGBA(0, 255, 0, 0.2)의 8자리 HEX 코드\rp = \u0026#34;#80F\u0026#34; # 보라색 RGB(255, 0, 136)의 3자리 HEX 코드\rplot([1 2 3; 2 3 4], ones(2, 3), fillrange = 2,\rfillcolor = [r g p],\rlabel = [r g p]) Parsing HEX codes can be parsed in the form of colorant\u0026quot;#FF0000\u0026quot;.\njulia\u0026gt; r = colorant\u0026#34;#FF0000\u0026#34; # R빨간색 RGB(255, 0, 0)의 6자리 HEX 코드\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; g = colorant\u0026#34;#00FF0033\u0026#34; # 투명도가 0.2인 초록색 RGBA(0, 255, 0, 0.2)의 8자리 HEX 코드\rRGBA{N0f8}(0.0,1.0,0.0,0.2)\rjulia\u0026gt; p = colorant\u0026#34;#80F\u0026#34; # 보라색 RGB(255, 0, 136)의 3자리 HEX 코드\rRGB{N0f8}(0.533,0.0,1.0)\rjulia\u0026gt; plot([1 2 3; 2 3 4], ones(2, 3), fillrange = 2,\rfillcolor = [r g p]) It can also be parsed as parse(RGB, \u0026quot;#FF0000\u0026quot;).\njulia\u0026gt; parse(RGB, \u0026#34;#FF0000\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(RGBA, \u0026#34;#FF000080\u0026#34;)\rRGBA{N0f8}(1.0,0.0,0.0,0.502) Getting Color Names The hex() function returns the color\u0026rsquo;s HEX code as a string.\njulia\u0026gt; hex(colorant\u0026#34;red\u0026#34;)\r\u0026#34;FF0000\u0026#34;\rjulia\u0026gt; hex(colorant\u0026#34;rgb(0, 255, 128)\u0026#34;)\r\u0026#34;00FF80\u0026#34;\rjulia\u0026gt; hex(RGBA(1, 0.5, 0, 0.5), :RRGGBB)\r\u0026#34;FF8000\u0026#34;\rjulia\u0026gt; hex(RGBA(1, 0.5, 0, 0.5), :RRGGBBAA)\r\u0026#34;FF800080\u0026#34;\rjulia\u0026gt; hex(HSV(30,1.0,1.0), :AARRGGBB)\r\u0026#34;FFFF8000\u0026#34; See Also How to use colors in Plots How to use RGB color codes How to use HEX color codes Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10, FixedPointNumbers v0.8.4 See Also How to use colors How to use palettes How to use color gradients Packages for color processing Colors.jl How to use RGB codes RGB(1, 0, 0) How to use HEX codes \u0026quot;#000000\u0026quot; How to specify colors of graph elements How to specify colors of graphs in sub plots How to specify colors of axes, axis names, ticks, and tick values How to set background color ","id":1921,"permalink":"https://freshrimpsushi.github.io/en/posts/1921/","tags":null,"title":"How to Use Hexadecimal RGB Codes (HEX) in Julia"},{"categories":"줄리아","contents":"Code Matrix(df) or Array(df) functions can be used to convert a DataFrame into an array of the same size. To create a DataFrame from an array, use DataFrame(array, :auto). In the past, the convert function was used, but it\u0026rsquo;s not applicable anymore, so be careful.\nusing DataFrames\rjulia\u0026gt; A = rand(5,3)\r5×3 Matrix{Float64}:\r0.678876 0.10431 0.827079\r0.621647 0.372007 0.29346\r0.756844 0.171237 0.0732631\r0.922519 0.0535938 0.121689\r0.164058 0.0684278 0.68446\rjulia\u0026gt; df = DataFrame(A, :auto)\r5×3 DataFrame\rRow │ x1 x2 x3 │ Float64 Float64 Float64\r─────┼────────────────────────────────\r1 │ 0.678876 0.10431 0.827079\r2 │ 0.621647 0.372007 0.29346\r3 │ 0.756844 0.171237 0.0732631\r4 │ 0.922519 0.0535938 0.121689\r5 │ 0.164058 0.0684278 0.68446\rjulia\u0026gt; Matrix(df)\r5×3 Matrix{Float64}:\r0.678876 0.10431 0.827079\r0.621647 0.372007 0.29346\r0.756844 0.171237 0.0732631\r0.922519 0.0535938 0.121689\r0.164058 0.0684278 0.68446\rjulia\u0026gt; Array(df)\r5×3 Matrix{Float64}:\r0.678876 0.10431 0.827079\r0.621647 0.372007 0.29346\r0.756844 0.171237 0.0732631\r0.922519 0.0535938 0.121689\r0.164058 0.0684278 0.68446\rjulia\u0026gt; Array(df) == Matrix(df)\rtrue ","id":1930,"permalink":"https://freshrimpsushi.github.io/en/posts/1930/","tags":null,"title":"Converting Between DataFrames and 2D Arrays in Julia"},{"categories":"줄리아","contents":"Guide Old Version In julia v1.5.0, *.csv files were read as follows: In fact, Julia is not yet a language notably convenient for data input. However, if one desires speed, there may come a time when Julia should be chosen over Python, R, or Matlab. For instance, if one wants to load a *.csv file located just under the E drive, it can be entered as follows.\nusing CSV\rdata = CSV.read(\u0026#34;E:/example.csv\u0026#34;) The execution result shows that the *.csv file has been successfully read into a dataframe.\nNew Version Although the exact time is unknown, after julia v1.7.0, dataframes must be loaded separately as follows. using CSV, DataFrames\rdata = CSV.read(\u0026#34;E:/example.csv\u0026#34;, DataFrame) Environment OS: Windows ","id":1923,"permalink":"https://freshrimpsushi.github.io/en/posts/1923/","tags":null,"title":"How to Read *.csv Files in Julia"},{"categories":"줄리아","contents":"Guide In Julia, parallel computing is commonly used, so it may be necessary to focus all of a computer\u0026rsquo;s resources on computation depending on the situation. Although there are various ways to change the number of threads, the most static and convenient method is to edit environmental variables.\nStep 1. Edit System Environmental Variables\nPress the Windows key or Windows+S to search for \u0026lsquo;Edit system environment variables\u0026rsquo;.\nWhen the System Properties window appears, click on \u0026lsquo;Environment variables\u0026rsquo;.\nStep 2. Find JULIA_NUM_THREADS\nFind the variable as shown above in user variables. This value is the number of threads. If it exists, choose \u0026lsquo;Edit\u0026rsquo;, if not, select \u0026lsquo;New\u0026rsquo; and proceed to Step 3..\nStep 3. Modify Variable Value\nIn the area indicated in the screenshot above, type the desired number of threads. The appropriate number of threads varies depending on the specs of the computer, but since we are dealing with an example, let\u0026rsquo;s modify it to $5$ threads.\nStep 4. Confirm\nusing Base.Threads\rnthreads() Let\u0026rsquo;s run the above code in the Julia console to check.\nIf it doesn\u0026rsquo;t reflect, try rebooting first. If that doesn\u0026rsquo;t work, then try modifying it in the system variables.\nEnvironment OS: Windows julia: v1.5.0 ","id":1933,"permalink":"https://freshrimpsushi.github.io/en/posts/1933/","tags":null,"title":"Changing the Number of Threads for Parallel Computing in Julia on Windows"},{"categories":"줄리아","contents":"Guide If you are someone who uses Julia, it\u0026rsquo;s likely that you\u0026rsquo;re comfortable with using multiple operating systems or computers, including servers. If there is file input/output involved, having to adjust the path each time the development environment changes can be quite bothersome. This is where the @__DIR__ macro comes in handy. Suppose you have a Julia code file like the following.\nTypically, when executed from the terminal, pwd() and @__DIR__ may appear to not be differentiated as seen below.\nThe difference between these two arises when using an IDE (Integrated Development Environment) such as Atom. Unlike pwd() which simply returns the current working directory, @__DIR__ informs you of the actual location of the code file. This can be extremely useful for complex and repetitive tasks where the working directory may be changed frequently, but the location of the executing code file is not as likely to change.\n","id":1935,"permalink":"https://freshrimpsushi.github.io/en/posts/1935/","tags":null,"title":"How to Determine the Location of Code Files Executed in Julia"},{"categories":"줄리아","contents":"Guide In Julia, parallel computing is routinely used, so sometimes it\u0026rsquo;s necessary to focus all of a computer\u0026rsquo;s resources on the computation. There are several ways to change the number of threads, but the most static and convenient method is to edit the environment variables.\nStep 1. Edit System Environment Variables\nPress Ctrl + Alt + T to open the terminal and type gedit ~/.bashrc. A window for editing environment variables will pop up like this.\nStep 2. Modification\nAdd export JULIA_NUM_THREADS=5 at the very bottom. You can modify it with the desired number of threads where it\u0026rsquo;s indicated in the screenshot. The appropriate number of threads varies with the computer\u0026rsquo;s specifications, but since we\u0026rsquo;re dealing with an example, let\u0026rsquo;s modify it to $5$ arbitrarily.\nStep 3. Verification\nusing Base.Threads\rnthreads() Execute the above code in the Julia console to verify.\n","id":1937,"permalink":"https://freshrimpsushi.github.io/en/posts/1937/","tags":null,"title":"Changing the Number of Threads for Parallel Computing in Julia on Linux"},{"categories":"줄리아","contents":"Code julia\u0026gt; f(x) = 2x + 1\rf (generic function with 1 method)\rjulia\u0026gt; g(x) = x^2\rg (generic function with 1 method)\rjulia\u0026gt; (g ∘ f)(3)\r49 Description In Julia, function composition is similar to the pipe operator in programming. The main advantage of this composition is that it makes it easier for mathematicians to express formulas as code. The example above is simply a translation of the following formula into code.\n$$ f(x) := 2x + 1 \\\\ g(x) := x^2 \\\\ (g \\circ f) (3) $$\nLike many languages these days, treating functions as first-class citizens is the same, but dealing with function spaces as in pure mathematics might be considered a more extreme philosophy that has even influenced the syntax. Note that the function composition operator can be used by entering \\circ in tex syntax.\n","id":1942,"permalink":"https://freshrimpsushi.github.io/en/posts/1942/","tags":null,"title":"How to Use Composite Functions in Julia"},{"categories":"줄리아","contents":"Code Although the original Sakura Sushi restaurant tends to add much more detailed explanations, to emphasize how easy it is to create animated GIFs in Julia, we will keep this explanation as brief as possible.\nEven setting aside simulating a random walk, creating an animated GIF like the one above can be very difficult and demanding, depending on the language. However, Julia makes this incredibly easy with the @animate macro and gif() function. The principle is simple. Attach the macro in front of a loop and draw the frames one by one as the loop executes. Once the frames are gathered into a variable, simply putting them into the gif() function is all it takes. The fps option allows you to set the speed of the animated GIF by specifying frames per second.\nusing Plots\rrandom\\_walk = cumsum(rand(100).-.5)\ranim = @animate for t in 1:100\rplot(random\\_walk[1:t], legend = :none)\rend; gif(anim, \u0026#34;example.gif\u0026#34;, fps = 10) Be mindful that if you don\u0026rsquo;t specify a different path, it will be saved in your documents. By making good use of this, one can create amazing animated GIFs like the one shown below.\n","id":1863,"permalink":"https://freshrimpsushi.github.io/en/posts/1863/","tags":null,"title":"Making GIFs in Julia"},{"categories":"줄리아","contents":"Overview The Distance Matrix is commonly used in simulations based on Particle Dynamics and Moving Agents, but it is often difficult to find a ready-made function for this purpose, and coding it from scratch can be daunting. In Julia, you can easily calculate a distance matrix using the pairwise() function and the Euclidean() function from the Distances package1.\nThe dims option allows you to specify the direction of rows and columns. As you can see, given a $\\mathbb{R}^{5 \\times 3}$ matrix, it is possible to calculate the distances of $5$dimensional vectors for $3$ instances or the distances of $3$dimensional vectors for $5$ instances.\nCode using Distances\rcoordinate = [2 3 4; 5 1 3; 1 7 5; 1 7 6; 2 4 3]\rpairwise(Euclidean(), coordinate; dims=1)\rpairwise(Euclidean(), coordinate; dims=2) The result of running the above code is as follows.\njulia\u0026gt; using Distances\rjulia\u0026gt; coordinate = [2 3 4; 5 1 3; 1 7 5; 1 7 6; 2 4 3]\r5×3 Array{Int64,2}:\r2 3 4\r5 1 3\r1 7 5\r1 7 6\r2 4 3\rjulia\u0026gt; pairwise(Euclidean(), coordinate; dims=1)\r5×5 Array{Float64,2}:\r0.0 3.74166 4.24264 4.58258 1.41421\r3.74166 0.0 7.48331 7.81025 4.24264\r4.24264 7.48331 0.0 1.0 3.74166\r4.58258 7.81025 1.0 0.0 4.3589\r1.41421 4.24264 3.74166 4.3589 0.0\rjulia\u0026gt; pairwise(Euclidean(), coordinate; dims=2)\r3×3 Array{Float64,2}:\r0.0 9.64365 7.07107\r9.64365 0.0 3.31662\r7.07107 3.31662 0.0 Optimization How to optimize distance matrix calculation https://discourse.julialang.org/t/pairwise-distances-from-a-single-column-or-vector/29415/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1799,"permalink":"https://freshrimpsushi.github.io/en/posts/1799/","tags":null,"title":"How to Compute Distance Matrices in Julia"},{"categories":"줄리아","contents":"Code Size Specification julia\u0026gt; empty = Array{Float64, 2}(undef, 3, 4)\r3×4 Array{Float64,2}:\r3.39519e-313 3.18299e-313 4.66839e-313 1.061e-313\r4.03179e-313 5.51719e-313 1.6976e-313 4.24399e-314\r2.97079e-313 4.66839e-313 7.00259e-313 5.0e-324 Executing the code above results in an empty array being created. Occasionally, it may seem like a strange value such as 1.76297e-315 is entered, but this is a value very close to 0, so it\u0026rsquo;s not a major issue for initialization.\nArray{X, Y}(undef, ...) creates an array of size ... filled with uninitialized values of data type X and dimension Y. The key here is undef.\nResizable Arrays In the case of a one-dimensional array, an empty array can be simply made by not putting anything inside the parentheses.\njulia\u0026gt; empty = Array{Float64, 1}()\rFloat64[]\rjulia\u0026gt; empty = Array{Float64, 2}()\rERROR: MethodError: no method matching Array{Float64,2}()\rClosest candidates are:\rArray{Float64,2}(::UndefInitializer, ::Int64, ::Int64) where T at boot.jl:408\rArray{Float64,2}(::UndefInitializer, ::Int64...) where {T, N} at boot.jl:412\rArray{Float64,2}(::UndefInitializer, ::Tuple{Int64,Int64}) where T at boot.jl:416\r...\rStacktrace:\r[1] top-level scope at REPL[85]:1 However, trying to do the same for a two-dimensional array will result in a MethodError as shown above. Of course, it\u0026rsquo;s possible to create an empty array by making an array of one-dimensional arrays, but from a speed perspective, it is recommended to use the native syntax as it is.\njulia\u0026gt; empty = Array{Array{Float64, 1}, 1}()\rArray{Float64,1}[] A Simpler Method Using curly braces like below makes creating arrays even more straightforward.\njulia\u0026gt; empty = Float64[]\rFloat64[]\rjulia\u0026gt; empty = Array{Float64, 1}[]\rArray{Float64,1}[]\rjulia\u0026gt; empty = Array{Float64, 2}[]\rArray{Float64,2}[] Environment OS: Windows julia: v1.5.0 ","id":1797,"permalink":"https://freshrimpsushi.github.io/en/posts/1797/","tags":null,"title":"How to Create an Empty Array in Julia"},{"categories":"동역학","contents":"Summary $2$ Consider a manifold $\\mathcal{P}$ and a function $f,g \\in C^{r} \\left( \\mathcal{P} \\right)$ such that the following vector field is given as a differential equation: $$ \\dot{x} = f(x,y) \\\\ \\dot{y} = g(x,y) $$ If $\\mathcal{M}$ represents an invariant set with a finite number of fixed points, then the omega limit set $\\omega (p)$ of $p \\in \\mathcal{M}$ satisfies one of the following three conditions:\n(1): $\\omega (p)$ is a singleton set, meaning it contains only one fixed point. (2): $\\omega (p)$ is a closed orbit. (3): $\\omega (p)$ consists of orbits $\\gamma$ that satisfy the following for some fixed points $p_{1} , \\cdots , p_{n}$ of $i,j \\in [1,n]$: $$ \\alpha ( \\gamma ) = \\left\\{ p_{i} \\right\\} \\\\ \\omega ( \\gamma ) = \\left\\{ p_{j} \\right\\} $$ Explanation A metric space is naturally a $T_{1}$ space, and in $T_{1}$, singleton sets are closed sets, so it can be said that $\\omega (p) = \\left\\{ p \\right\\}$ is naturally a closed orbit. However, in the context of the statement, let\u0026rsquo;s differentiate the case that includes only one fixed point as something else.\nIn fact, in the Poincaré-Bendixson theorem, chaos doesn\u0026rsquo;t even need to be defined, and the statement that chaos does not occur is close to a corollary. What the theorem tells us is just a classification of omega limit sets, and it is deduced that chaos cannot occur because it is precisely made up of what we know. However, due to such theorems, the interest in chaos theory is able to definitively move beyond the dimension $2$.\nThe intuitive understanding of the theorem is not very difficult. If $\\mathcal{M}$ is not bounded, it cannot be chaotic in the first place, and if it is bounded, it cannot extend indefinitely, so the flow must either narrow down and rotate or spread out and rotate. However, unlike in the dimension $3$, in the dimension $2$, a line divides the plane into two regions, so one of the regions must always be abandoned. This is metaphorically like abandoning the remaining part of the bounded space $\\mathcal{M}$ as time passes. If you try to pass through the flow you have already passed to use an area you have not passed yet, at that moment, it becomes a closed orbit, and eventually, it converges to a fixed point or a closed orbit, making it impossible to cause chaos.\nProof 1 Strategy: As befitting a theorem named after Poincaré, it\u0026rsquo;s topologically oriented. Let $\\Sigma$ be a continuous, connected arc in the interior $\\mathcal{P}$.\nIf the normal vector at all points of $\\Sigma$ and the vector field do not have an inner product of $0$ and do not change sign, then $\\Sigma$ is said to transverse the vector field of $\\mathcal{P}$. This concept can also be thought of for just one point, where the vector field and $\\Sigma$ would not be tangent at that point. In terms of the flow\u0026rsquo;s sense, it not only meets at a point but also penetrates $\\Sigma$.\nLet\u0026rsquo;s represent the flow created by the given vector field as $\\phi_{t}$, and represent the orbit of a point $p \\in \\mathcal{P}$ under the flow $\\phi_{t}$ for positive time as $O_{+}(p)$. Let\u0026rsquo;s represent the orbit that a point $p_{i}$ reaches $p_{j}$ following the flow of time $t$ under the flow $\\phi_{t}$ as $\\widehat{p_{i} p_{j}} \\subset O_{+} (p)$. The omega limit set denoted as $\\omega ( \\cdot )$ was originally defined for a given point, but the $\\omega \\left( X \\right)$ for some set $X$ can be thought of as follows: $$ \\omega (X) := \\bigcup_{x \\in X} \\omega (x) $$ The same applies to the alpha limit set denoted as $\\alpha ( \\cdot )$.\nIn addition, the following auxiliary theorems will be used continuously.\nAuxiliary Theorem (Properties of Omega Limit Sets): Let the whole space be the Euclidean space $X = \\mathbb{R}^{n}$ and a point $p \\in \\mathcal{M}$ of the compact invariant set $\\mathcal{M}$ in the flow $\\phi_{t} ( \\cdot )$ is given:\n[1]: $\\omega (p) \\ne \\emptyset$ [2]: $\\omega (p)$ is a closed set. [3]: $\\omega (p)$ is invariant under the flow, i.e., $\\omega (p)$ is a union of orbits. [4]: $\\omega (p)$ is a connected space. First, the omega limit sets that occur in dimension $2$ will not be in the form of having an area, so the omega limit sets mentioned below can be thought of in the form of some curve.\nPart 1.\nIf $\\Sigma \\subset \\mathcal{M}$ is an arc that crosses the vector field, and since $\\mathcal{M}$ is an invariant set in the dimension $2$ vector field, $\\Sigma$ cannot go out of $\\mathcal{M}$ against the flow of the vector field. Therefore, for any $p \\in \\mathcal{M}$, if we call the $k$th point where $O_{+} (p)$ and $\\Sigma$ meet as $p_{k}$, then it must be $p_{k}\\subset \\widehat{p_{k-1} p_{k+1}} \\subset O_{+} (p)$. In other words, the flow is converging to some inner core while meeting $\\Sigma$, and it doesn\u0026rsquo;t happen that the intersection points get closer and then move away again.\nPart 2. The omega limit set $\\omega (p)$ of $p \\in \\mathcal{M}$ intersects $\\Sigma$ at most at one point.\nThis will be shown by contradiction. Assume that $\\omega (p)$ and $\\Sigma$ intersect at two different points $q , \\overline{q}$.\nThen, by the definition of the omega limit set, when $n \\to \\infty$, a sequence $\\left\\{ q_n \\right\\}_{n \\in \\mathbb{N}} , \\left\\{ \\overline{q}_n \\right\\}_{n \\in \\mathbb{N}} \\subset O_{+} (p)$ that satisfies $$ q_{n} \\to q \\\\ \\overline{q}_{n} \\to \\overline{q} $$ exists. However, according to Part 1, these intersection points are arranged in a sequence $p_{1} , p_{2} , \\cdots$, so there is a contradiction to the assumption. Therefore, $\\omega (p)$ and $\\Sigma$ either do not meet at all or meet at only one point if they do. [ NOTE: In the case of a torus, this logic cannot be applied directly, but the same conclusion can be reached by dividing it into pieces so that it becomes a shape like $\\mathcal{M}$. ]\nPart 3. If $\\omega (p)$ does not include fixed points, it is a closed orbit.\nAfter showing that the orbit $O_{+}(q)$ of $q \\in \\omega (p)$ is a closed orbit, it needs to be shown that it is $\\omega (p) = O_{+} (q)$.\nPart 3-1. Orbit $O_{+}(q)$ is closed. If we pick a point $x \\in \\omega (q)$, according to auxiliary theorem [2], since $\\omega (p)$ is closed and a union of orbits without fixed points, $x$ must also not be a fixed point. Be careful not to get confused with $p,q$, the assumption is that $\\omega (p)$ does not have fixed points, and since $x$ is said to be $x \\in \\omega (q)$, there is no guarantee that it is necessarily $x \\in \\omega (p)$, but it can be concluded that it is not a fixed point anyway. Now, let\u0026rsquo;s take one arc $\\Sigma_{x}$ that crosses the vector field of this non-fixed point $x$. According to Part 1., the sequence of intersections $\\left\\{ q_{n} \\right\\}_{n \\in \\mathbb{N}}$ of $\\Sigma_{x}$ and $O_{+} (q)$ is $q_{n} \\to x$ when $n \\to \\infty$, and since $x \\in \\mathcal{M}$, according to Part 2., for $\\forall n \\in \\mathbb{N}$, it must be $q_{n} = x$. Since $x$ is not a fixed point, if $O_{+} (q)$ intersects with $x$, it must cross and then return to intersect again repeatedly. Since it is said here that $x \\in \\omega (q)$, $O_{+}(q)$ actually intersects with $x$ without stopping or pausing as it approaches $x$, and therefore, $O_{+}(q)$ becomes a closed orbit. Part 3-2. $O_{+}(q) = \\omega (p)$ If we take one arc $\\Sigma_{q}$ that crosses the vector field from point $q \\in \\omega (p)$, according to Part 2, $\\omega (p)$ and $\\Sigma_{q}$ meet only at $q$. According to auxiliary theorem [3], since $\\omega (p)$ is a union of orbits, if $q \\in \\omega (p)$ then $O_{+} (q) \\subset \\omega (p)$, but since $\\omega (p)$ does not contain fixed points and is a connected space, it must be exactly $O_{+}(q) = \\omega (p)$. Part 4. For $p \\in \\mathcal{M}$, if $p_{1} , p_{2} \\in \\omega (p)$ are different fixed points of the vector field, there exists at most one orbit $\\gamma \\subset \\omega (p)$ that satisfies $\\alpha (\\gamma) = \\left\\{ p_{1} \\right\\}$ and $\\omega (\\gamma) = \\left\\{ p_{2} \\right\\}$.\nThis will be shown by contradiction. If there are two different orbits connecting two points, there would be some area $\\mathcal{K}$ with an area between the two orbits, and a contradiction will be derived from there. Assume that there exist two different orbits $\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$ satisfying the following conditions. $$ \\alpha \\left( \\gamma_{i} \\right) = \\left\\{ p_{1} \\right\\} \\\\ \\omega \\left( \\gamma_{i} \\right) = \\left\\{ p_{2} \\right\\} $$ Let\u0026rsquo;s take one point each from these orbits, $q_{1} \\in \\gamma_{1}$, $q_{2} \\in \\gamma_{2}$, and take arcs that cross the vector field from $q_{1}$ and $q_{2}$ as $\\Sigma_{1}, \\Sigma_{2}$.\nSince $\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$, according to Part 2, let\u0026rsquo;s say $O_{+} (p)$ intersects with $\\Sigma_{1}$ at one point $a$ and then $\\Sigma_{2}$ intersects at one point $b$. Then, in the dimension $2$ manifold, there will be a subregion $\\color{red}{\\mathcal{K}}$ surrounded by the following path.\n$$ q_{1} \\overset{\\Sigma_{1}}{\\to} a \\overset{ O_{+} (p) }{ \\to } b \\overset{\\Sigma_{2}}{\\to} q_{2} \\overset{ \\omega (\\gamma) }{ \\to } p_{2} \\overset{ \\gamma_{1} }{ \\gets } q_{1} $$ The notation $\\displaystyle x \\overset{\\mathcal{C}}{\\to} y$ was used to mean that point $x,y$ was connected to curve $\\mathcal{C}$. The flow starting from $\\color{red}{\\mathcal{K}}$ cannot go over $\\gamma_{1} , \\gamma_{2}$, so $\\color{red}{\\mathcal{K}}$ becomes an invariant set. However, the fact that the orbit $O_{+}(p)$ starting from $p$ cannot exit once it enters $\\color{red}{\\mathcal{K}}$ means that neither $\\gamma_{1}$ nor $\\gamma_{2}$ can belong to $\\omega (p)$. For example, if you think about $\\gamma_{2}$, $q_{2} \\overset{\\gamma_{2}}{\\to} p_{2}$ may belong to $\\omega (p)$, but it cannot go to the front part, $p_{1} \\overset{\\gamma_{2}}{\\to} q_{2}$. Therefore, the claim that the entire $\\gamma_{2}$ belongs to $\\omega (p)$ cannot be made, and it contradicts $\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$.\nPart 5.\nIn this part only, let\u0026rsquo;s call a point that is not a fixed point a Regular Point. There\u0026rsquo;s no need to limit this to just this part, but since the expression Regular is often used in academia regardless of the field, using it without caution or warning can cause great confusion.\nCase 1. If $\\omega (p)$ only has fixed points Since $\\mathcal{M}$ has a finite number of fixed points and $\\omega (p)$ is a connected space, it must have only one fixed point. Case 2. If $\\omega (p)$ only has regular points According to Part 3, $\\omega (p)$ is a closed orbit. Case 3. If $\\omega (p)$ has both fixed and regular points Consider the orbit $\\gamma \\subset \\omega (p)$ consisting only of regular points. Since $\\gamma$ consists only of regular points, according to Part 3, $\\omega ( \\gamma )$ and $\\alpha (\\gamma)$ are closed orbits, but they must also have fixed points. However, according to auxiliary theorem [4], since $\\omega ( \\gamma )$ is a connected space, the closed orbit and the fixed point cannot be separated, and the fixed point must be located at one point on the closed orbit, which means $\\omega ( \\gamma )$ is a singleton set containing only fixed points. The same discussion can be repeated for $\\alpha ( \\gamma )$, so all regular points of $\\omega (p)$ have fixed points as their omega and alpha limit points.\n$\\omega (p)$ must fall into one of these three cases. This concludes the proof.\n■\nWiggins. (2003). Introduction to Applied Nonlinear Dynamical Systems and Chaos Second Edition(2nd Edition): 118~120.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1788,"permalink":"https://freshrimpsushi.github.io/en/posts/1788/","tags":null,"title":"Proof of Poincaré bendixson Theorem"},{"categories":"수리물리","contents":"Definition For a vector function $\\mathbf{F}(x,y,z)=F_{x}\\hat{\\mathbf{x}}+F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$, the following scalar function is defined as the divergence $\\mathbf{F}$ of $\\mathbf{F}(x,y,z)=F_{x}\\hat{\\mathbf{x}}+F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$ and is denoted by $\\nabla \\cdot \\mathbf{F}$.\n$$ \\begin{equation} \\nabla \\cdot \\mathbf{F} := \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} \\label{divergence} \\end{equation} $$\nExplanation Geometrically, if $\\nabla \\cdot \\mathbf{F}\u0026gt;0$, it means that $\\mathbf{F}$ is spreading out or diverging. If $\\nabla \\cdot \\mathbf{F}\u0026lt;0$, it means that $\\mathbf{F}$ is converging or moving inward. If $\\nabla \\cdot \\mathbf{F}=0$, it means that $\\mathbf{F}$ is neither diverging nor converging, indicating an equilibrium where the amount going out equals the amount coming in.\nDivergence is translated as \u0026ldquo;divergence\u0026rdquo;. To maintain consistency with using gradient for slope and curl for rotation, it is denoted as divergence instead of using the literal translation.\nIt is important to note that the value defined as $\\dfrac{ \\partial F_{x}}{ \\partial x} + \\dfrac{ \\partial F_{y}}{ \\partial y }+ \\dfrac{ \\partial F_{z}}{ \\partial z}$ is denoted by $\\nabla \\cdot \\mathbf{F}$. While $\\nabla$ is referred to as the del operator, considering it as inherently meaningful can lead to confusion with $\\nabla \\cdot \\mathbf{F}$ or $\\nabla \\times \\mathbf{F}$ being mistaken for dot and cross products. Therefore, $\\nabla$ should be understood merely as a convenient notation, and it might be better to think of the del operator as equivalent to gradient. The details continue below.\nImportant Points $\\nabla \\cdot \\mathbf{F}$ is not the dot product of $\\nabla$ and $\\mathbf{F}$ $\\nabla \\cdot \\mathbf{F}$ is absolutely not the dot product of $\\nabla$ and $\\mathbf{F}$.\rA dot product is fundamentally an operation between two vectors. Thinking of $\\nabla \\cdot \\mathbf{F}$ as a dot product implies considering $\\nabla$ as a vector.\n$$ \\nabla \\overset{?}{=}\\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} +\\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}}+\\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{ \\partial }{ \\partial x},\\ \\dfrac{ \\partial }{ \\partial y},\\ \\dfrac{ \\partial }{ \\partial z} \\right) $$\nIndeed, thinking in this way conveniently matches the definition of divergence $(1)$.\n$$ \\nabla \\cdot \\mathbf{F} = \\left( \\dfrac{ \\partial }{ \\partial x},\\ \\dfrac{ \\partial }{ \\partial y},\\ \\dfrac{ \\partial }{ \\partial z} \\right) \\cdot \\left( F_{x}, F_{y}, F_{z} \\right) = \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} $$\nHowever, if this were truly a dot product, the commutative property would imply an odd conclusion as follows:\n$$ \\mathbf{F} \\cdot \\nabla = F_{x} \\dfrac{\\partial }{\\partial x} + F_{y} \\dfrac{\\partial }{\\partial y} + F_{z} \\dfrac{\\partial }{\\partial z} \\overset{?}{=} \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} = \\nabla \\cdot \\mathbf{F} $$\nIn reality, $\\nabla \\cdot$ represents an operator that maps a vector function $\\mathbf{F}(x,y,z)$ to a scalar function $\\frac{ \\partial F_{x}(x,y,z)}{ \\partial x} + \\frac{ \\partial F_{y}(x,y,z)}{ \\partial y }+ \\frac{ \\partial F_{z}(x,y,z)}{ \\partial z}$. This means that $\\operatorname{div}$, when defined as follows, simply yields $\\dfrac{ \\partial F_{x}}{ \\partial x} + \\dfrac{ \\partial F_{y}}{ \\partial y }+ \\dfrac{ \\partial F_{z}}{ \\partial z}$ for every substitution of $\\mathbf{F}$. This intuitive understanding explains why $\\operatorname{div}(\\mathbf{F})$ is denoted as $\\nabla \\cdot \\mathbf{F}$ instead of a direct interpretation. In advanced mathematics, divergence is often denoted differently, reflecting a less intuitive handling of 3D vectors than in physics.\nThen, why can\u0026rsquo;t $\\nabla \\cdot \\mathbf{F}$ be considered a non-commutative dot product? It\u0026rsquo;s because in $\\nabla \\cdot \\mathbf{F}$, $\\nabla \\cdot$ itself is a function (operator), and $\\mathbf{F}$ is a variable. In contrast, $\\mathbf{F} \\cdot \\nabla$ is an operator by itself. Therefore, $\\nabla \\cdot \\mathbf{F}$ represents the function value of $\\nabla \\cdot$, while $\\mathbf{F} \\cdot \\nabla$ is a function awaiting variable substitution. The notation $\\mathbf{F} \\cdot \\nabla$ intuitively represents function $f$. $f$ is an operator that takes the vector function $\\mathbf{A}$ as a variable and applies $\\left( F_{x}\\dfrac{\\partial }{\\partial x} + F_{y}\\dfrac{\\partial }{\\partial y} + F_{z}\\dfrac{\\partial }{\\partial z} \\right)$ to each component.\n$$ \\begin{align*} f (\\mathbf{A}) \\overset{\\text{definition}}{=}\u0026amp; \\left( F_{x}\\dfrac{\\partial A_{x}}{\\partial x} + F_{y}\\dfrac{\\partial A_{x}}{\\partial y} + F_{z}\\dfrac{\\partial A_{x}}{\\partial z} \\right)\\hat{\\mathbf{x}} \\\\ \u0026amp;\\quad+ \\left( F_{x}\\dfrac{\\partial A_{y}}{\\partial x} + F_{y}\\dfrac{\\partial A_{y}}{\\partial y} + F_{z}\\dfrac{\\partial A_{y}}{\\partial z} \\right)\\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad+ \\left( F_{x}\\dfrac{\\partial A_{z}}{\\partial x} + F_{y}\\dfrac{\\partial A_{z}}{\\partial y} + F_{z}\\dfrac{\\partial A_{z}}{\\partial z} \\right)\\hat{\\mathbf{z}} \\\\ \\overset{\\text{notation}}{=}\u0026amp; (\\mathbf{F}\\cdot \\nabla) (\\mathbf{A}) \\end{align*} $$\nWhen a scalar function $\\phi$ exists as a variable, it is considered as follows:\n$$ \\begin{align*} f (\\phi) \\overset{\\text{definition}}{=}\u0026amp; F_{x}\\dfrac{\\partial \\phi}{\\partial x} + F_{y}\\dfrac{\\partial \\phi}{\\partial y} + F_{z}\\dfrac{\\partial \\phi}{\\partial z} \\\\ \\overset{\\text{notation}}{=}\u0026amp; (\\mathbf{F}\\cdot \\nabla) (\\phi) \\end{align*} $$\nThus, $\\nabla \\cdot \\mathbf{F}$ and $\\mathbf{F}\\cdot \\nabla$ should not be understood as the dot product of $\\nabla$ and $\\mathbf{F}$, but as functions in themselves. This explanation applies not only to divergence but also to gradient $\\nabla f$ and curl $\\nabla \\times \\mathbf{F}$.\nDerivation First, consider a small volume in 3D space as shown below.\nOur goal is to understand how $\\mathbf{F}$ appears at each point within this small volume. Analogously, if $\\mathbf{F}$ represents heat, we aim to determine the direction and speed of flow, or if $\\mathbf{F}$ represents water, whether it is entering or exiting through a tap or drain. Let\u0026rsquo;s calculate for the $x$ axis direction first. The amount of $\\mathbf{F}$ passing through $d\\mathbf{a}_{1}$ can be calculated by the dot product.\n$$ \\begin{align} \\mathbf{F}(x+dx) \\cdot d\\mathbf{a}_{1} \u0026amp;= \\left( F_{x}(x+dx)\\hat{\\mathbf{x}}+F_{y}(x+dx)\\hat{\\mathbf{y}}+F_{z}(x+dx)\\hat{\\mathbf{z}} \\right) \\cdot dydz\\hat{\\mathbf{x}} \\nonumber \\\\ \u0026amp;= F_{x}(x+dx)dydz \\end{align} $$\nIf $F_{x}(x+dx)dydz \u0026gt;0$, it indicates the amount of $\\mathbf{F}$ exiting the small volume, and if $F_{x}(x+dx)dydz\u0026lt;0$, the amount entering. Similarly, the amount of $\\mathbf{F}$ exiting through $d\\mathbf{a}_{2}$ is as follows.\n$$ \\begin{equation} \\mathbf{F}(x) \\cdot d \\mathbf{a}_{2} = F_{x}(x)\\hat{\\mathbf{x}} \\cdot(-dydz\\hat{\\mathbf{x}})=-F_{x}(x)dydz \\end{equation} $$\nTherefore, $(2) + (3)$ represents the net flux of $\\mathbf{F}$ in the $x$ direction.\n$$ \\begin{align*} (2) + (3) \u0026amp;=\\left[ F_{x}(x+dx) -F_{x}(x)\\right]dydz \\\\ \u0026amp;= \\frac{F_{x}(x+dx) -F_{x}(x) }{dx}dxdydz \\end{align*} $$\nSince $dx$ is a small length, it can be approximated as follows. Thus, the amount of $\\mathbf{F}$ entering or exiting the small volume in the $x$ direction is expressed as:\n$$ \\frac{ \\partial F_{x}}{ \\partial x}dxdydz $$ Calculating similarly for the $y$ and $z$ directions yields:\n$$ \\frac{ \\partial F_{y}}{ \\partial y}dxdydz \\quad \\text{and} \\quad \\frac{ \\partial F_{z}}{ \\partial z}dxdydz $$\nSumming these gives the total flux of $\\mathbf{F}$ entering or exiting the small volume, and dividing by $dxdydz$ gives the flux per unit volume.\n$$ \\frac{ \\partial F_{x}}{ \\partial x}+\\frac{ \\partial F_{y}}{ \\partial y}+\\frac{ \\partial F_{z}}{ \\partial z} $$\nFrom now on, this will be referred to as the divergence of $\\mathbf{F}$ and denoted by $\\nabla \\cdot \\mathbf{F}$.\n$$ \\nabla \\cdot \\mathbf{F} := \\frac{ \\partial F_{x}}{ \\partial x}+\\frac{ \\partial F_{y}}{ \\partial y}+\\frac{ \\partial F_{z}}{ \\partial z} $$\n■\nAs derived, it\u0026rsquo;s important to remember that $\\nabla \\cdot \\mathbf{F}$ is not the dot product of $\\nabla$ and $\\mathbf{F}$.\nRelated Formulas Linearity:\nProduct Rule:\n$$ \\nabla \\cdot (f\\mathbf{A}) = f(\\nabla \\cdot \\mathbf{A}) + \\mathbf{A} \\cdot (\\nabla f) $$ $$ \\nabla \\cdot (\\mathbf{A} \\times \\mathbf{B}) = \\mathbf{B} \\cdot (\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\cdot (\\nabla \\times \\mathbf{B}) $$\nSecond Derivative:\n$$ \\nabla \\cdot (\\nabla T) = \\dfrac{\\partial^{2} T}{\\partial x^{2}} + \\dfrac{\\partial ^{2} T} {\\partial y^{2}} + \\dfrac{\\partial ^{2} T}{\\partial z^{2}} $$ $$ \\nabla (\\nabla \\cdot \\mathbf{A} ) $$ $$ \\nabla \\cdot (\\nabla \\times \\mathbf{A})=0 $$\nGauss\u0026rsquo;s Theorem (Divergence Theorem)\n$$ \\int_\\mathcal{V} \\nabla \\cdot \\mathbf{ F} dV = \\oint _\\mathcal{S} \\mathbf{F} \\cdot d \\mathbf{S} $$\nIntegration Formulas\n$$ \\int_{\\mathcal{V}} \\left[ T \\nabla^{2} U + (\\nabla T) \\cdot (\\nabla U) \\right] d \\tau = \\oint_{\\mathcal{S}} (T \\nabla U) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left( T \\nabla^{2} U - U \\nabla^{2} T \\right) d \\tau = \\oint_{\\mathcal{S}} \\left( T \\nabla U - U \\nabla T \\right) \\cdot d \\mathbf{a} $$\nPartial Integration\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$\nSee Also Del Operator $\\nabla$ ","id":1796,"permalink":"https://freshrimpsushi.github.io/en/posts/1796/","tags":null,"title":"Divergence of Vector Function in Cartesian Cooridenates System"},{"categories":"바나흐공간","contents":"The term \u0026ldquo;functional analysis\u0026rdquo; is indeed intriguing, especially when considering the word \u0026ldquo;functional\u0026rdquo; instead of merely \u0026ldquo;function analysis.\u0026rdquo; At first glance, \u0026ldquo;functional\u0026rdquo; appears to be an adjective form of \u0026ldquo;function,\u0026rdquo; suggesting meanings like \u0026ldquo;function-like\u0026rdquo; or \u0026ldquo;pertaining to functions.\u0026rdquo; This notion can also be found in another name for functionals, \u0026ldquo;generalized functions.\u0026rdquo; The question arises as to why these are not simply called functions. To understand this, let\u0026rsquo;s look at the general definition of a functional:\nFor a vector space $X$, a function $f$ called a functional is defined as follows:\n$$ f : X \\to \\mathbb{C} $$\nThis definition might prompt the question, \u0026ldquo;If $f$ is a function according to the above definition, why is it named a functional?\u0026rdquo; Although it\u0026rsquo;s understandable to assign a special name to functions satisfying certain conditions, the reason behind the specific choice of \u0026ldquo;functional\u0026rdquo; (implying something function-like) may not be immediately clear.\nTo grasp why something would be labeled as \u0026ldquo;function-like\u0026rdquo; rather than a function, it\u0026rsquo;s essential to consider the context in which functional analysis emerged. People learning mathematics today understand functions as follows:\nA correspondence $f$ is said to exist from $X$ to $Y$ if, for every $x_{1}, x_{2} \\in X$, there exist $f(x_{1})$ and $f(x_{2})$ that satisfy $x_{1} = x_{2} \\implies f(x_{1}) = f(x_{2})$.\n$$ f : X \\to Y $$\nWhen rigorously defined using set theory, it becomes:\nGiven two non-empty sets $X$ and $Y$, a binary relation $f \\subset (X,Y)$ is called a function if it satisfies the following, represented as $f : X \\to Y$:\n$$ (x ,y_{1}) \\in f \\land (x,y_{2}) \\in f \\implies y_{1} = y_{2} $$\nAs seen in these definitions, there are no specific conditions on sets $X$ and $Y$; whether they are sets of numbers or function spaces does not matter. However, to mathematicians in the late 19th century, a function was not perceived this way. They thought of functions primarily as mappings from values to values, essentially as formulas that provide one value from another, similar to how functions are introduced in middle school.\nThis view was somewhat natural because the rigorous definition of functions, as shown above, was developed through set theory, which was founded by Cantor, born in 1845. Thus, it\u0026rsquo;s not surprising that mathematicians up to the early 20th century considered functions mainly in terms of numerical formulas. The term \u0026ldquo;function\u0026rdquo; itself suggests a certain functionality or operation.\nConsider the following function:\nFor a differentiable function $f$ over a closed interval $[a,b]$, the length of the curve $y=f(x)$ is defined as follows:\n$$ L(f)=\\int_{a}^{b} \\sqrt{1+ f^{\\prime}(x)^{2}}dx $$\nTo the mathematicians of the time, $L$ was not considered a function because it mapped functions to values, not values to values. Hence, $L$ could be termed a \u0026ldquo;function of functions,\u0026rdquo; but since it was not strictly a function, there was ambiguity in terminology. Volterra referred to them as \u0026ldquo;functions of lines,\u0026rdquo; and the French mathematician Hadamard proposed calling these \u0026ldquo;function-like functions of functions\u0026rdquo; fonctionnelles. This term later became \u0026ldquo;functional\u0026rdquo; in English.\nAfter functions were rigorously defined using set theory, functionals became considered functions too. However, the term \u0026ldquo;functional\u0026rdquo; continues to be used, particularly because it clearly indicates that the domain is a space of functions, much like how a \u0026ldquo;collection\u0026rdquo; or \u0026ldquo;family\u0026rdquo; refers to a set of sets. Despite the conceptual overlap between functionals and functions, the term \u0026ldquo;functional\u0026rdquo; has persisted, likely to avoid confusion. The field of study being named functional analysis likely also played a role. Over time, the term \u0026ldquo;functional\u0026rdquo; has evolved to refer to mappings from vector spaces to complex number spaces.\nDistribution Theory As discussed, the term \u0026ldquo;functional\u0026rdquo; was originally coined to describe entities that were like functions but not exactly functions. However, once functions were defined through set theory, functionals became recognized as functions. Interestingly, functionals ended up being used to describe entities that truly were \u0026ldquo;not functions but function-like.\u0026rdquo; The Dirac delta function, first conceptualized by Poisson and Cauchy during their study of Fourier analysis and later popularized by the theoretical physicist Paul Dirac in quantum mechanics, is an example of such an entity. Its naive definition satisfies certain conditions that imply divergence, meaning the delta function is not strictly a function but rather a state or condition.\nIn 1950, after 15 years of research, the French mathematician Laurent-Moise Schwartz rigorously defined the delta function in the book \u0026ldquo;Theorie des distributions.\u0026rdquo; He introduced the concept of smooth functions called test functions and their space, denoted as $\\mathcal{D}$. Distributions are mappings from $\\mathcal{D}$ to $\\mathbb{C}$ and are considered functionals. Although initially functional referred to entities not traditionally regarded as functions, it eventually came to be used in developing a theory for entities that are not functions in the conventional sense but are treated as such, marking a fascinating turn of events.\n","id":1780,"permalink":"https://freshrimpsushi.github.io/en/posts/1780/","tags":null,"title":"Why Functional is Named Functional"},{"categories":"수리물리","contents":"Definition For a scalar function $f=f(x,y,z)$, the following vector function is defined as the gradient of $f$, denoted by $\\nabla f$:\n$$ \\nabla f := \\frac{ \\partial f}{ \\partial x }\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) $$\nExplanation The gradient is translated into English as gradient, slope, or incline. The terms \u0026lsquo;slope\u0026rsquo; and \u0026lsquo;incline\u0026rsquo; are old translations of the gradient and are not commonly used nowadays. Also, \u0026lsquo;slope\u0026rsquo; is a Sino-Korean word for gradient, so it\u0026rsquo;s essentially the same. The gradient is actually a vector, so the term \u0026lsquo;slope\u0026rsquo; seems insufficient to fully capture the meaning of the gradient. Here at Sashimi Sushi, we prefer to use the term \u0026lsquo;gradient\u0026rsquo; consistently.\nGeometrically, $\\nabla f$ represents the direction in which $f$ changes most rapidly. In other words, the direction in which the rate of increase of $f$ is highest at the point $(x,y,z)$ is the vector $\\left( \\dfrac{\\partial f(x,y,z)}{\\partial x}, \\dfrac{\\partial f(x,y,z)}{\\partial y}, \\dfrac{\\partial f(x,y,z)}{\\partial z} \\right)$. This is just an extension of the concept of differential coefficients to multiple dimensions. If $f$ is increasing, the differential coefficient is positive; if $f$ is decreasing, the coefficient is negative.\nMeanwhile, it\u0026rsquo;s important to note that $\\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right)$ is denoted as $\\nabla f$ in the definition. While $\\nabla$ is called the del operator, thinking of it as having its own meaning can lead to misunderstandings, such as misinterpreting $\\nabla \\cdot \\mathbf{F}$ or $\\nabla \\times \\mathbf{F}$ as dot products or cross products. Thus, $\\nabla$ should be understood merely as a convenient notation, and it\u0026rsquo;s better to think of the gradient, divergence, and curl collectively as del operators, or even to consider the del operator as equivalent to the gradient. More details will follow below.\nPoints of Attention $\\nabla f$ is not the product of $\\nabla$ and $f$ An important aspect of understanding the gradient is recognizing that $\\nabla f$ is not the product of the vector $\\nabla = \\left( \\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z} \\right)$ and the scalar $f$. It may seem intuitive and appealing to interpret it this way, but it\u0026rsquo;s actually the opposite. $\\nabla$ is presented as a vector like $\\left( \\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z} \\right)$ to make it appear like a product of a vector and a scalar. If $\\nabla f$ were really the product of the vector $\\nabla$ and the scalar $f$, then, since the product of a vector and a scalar is commutative, the following strange equation would hold:\n$$ \\nabla f = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) \\overset{?}{=} \\left( f\\dfrac{\\partial }{\\partial x}, f\\dfrac{\\partial }{\\partial y}, f\\dfrac{\\partial }{\\partial z} \\right) = f\\nabla $$\nThis odd result arises because $\\nabla$ is not actually a vector, and $\\nabla f$ is not a product of a vector and a scalar. $\\nabla$ is an operator that maps the scalar function $f(x,y,z)$ to the vector function $\\left( \\frac{\\partial f(x,y,z)}{\\partial x}, \\frac{\\partial f(x,y,z)}{\\partial y}, \\frac{\\partial f(x,y,z)}{\\partial z} \\right)$. Let\u0026rsquo;s define a function $\\operatorname{grad}$ that takes $f$ as a variable like this:\n$$ \\begin{equation} \\operatorname{grad} (f) = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right), \\quad f=f(x,y,z) \\end{equation} $$\nIn this definition, there is no need for explanations about the product of a vector and a scalar. $\\operatorname{grad}$ is just a function (operator) that, when given the variable $f$, follows the rule in $(1)$ to determine its function value. However, $\\operatorname{grad} (f)$\u0026rsquo;s function value, when denoted as $\\operatorname{grad} = \\nabla$, becomes a convenient and intuitive notation, and it\u0026rsquo;s helpful to explain it as a vector $\\nabla = \\left( \\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z} \\right)$.\nSimilarly to how Leibniz\u0026rsquo;s notation for differentiation isn\u0026rsquo;t an exact explanation of the underlying principle but is used for convenience and ease of understanding, $\\nabla f$ also appears as a product of a vector and a scalar for computational convenience, though that\u0026rsquo;s not its true nature.\nWhat about $f\\nabla$? Following the explanation above, since $\\nabla$ is a function, $\\nabla f = \\nabla(f)$ represents the function value obtained when the variable $f$ is substituted into the function $\\nabla$. On the other hand, $f \\nabla$ is a function in itself, which when another function $g$ is substituted as a variable, maps to a function value as follows:\n$$ (f\\nabla) (g) = f\\left( \\dfrac{\\partial g}{\\partial x}, \\dfrac{\\partial g}{\\partial y}, \\dfrac{\\partial g}{\\partial z} \\right) = \\left( f\\dfrac{\\partial g}{\\partial x}, f\\dfrac{\\partial g}{\\partial y}, f\\dfrac{\\partial g}{\\partial z} \\right) $$\nOf course, when looking at the function value $f \\nabla g$, it can be seen as substituting $g$ into $f \\nabla$, or as the product of the scalar function $f$ and the vector function $\\nabla g$.\nDerivation 1-Dimension Look at the above picture. The differential coefficient of $f_{1}$ at point $x=2$ is $4$. This value not only tells how much the function $f_{1}$ is inclined at $x=2$, but also indicates that the graph of $f_{1}$ increases in the direction where $x$ increases, as suggested by the $+$ sign in front of $4$. Therefore, the differential coefficient $4$ should be understood not merely as a scalar, but as a 1-dimensional vector $4\\hat{\\mathbf{x}}$.\nSimilarly, the differential coefficient of $f_{2}$ at $x=2$ is $-3$. This includes the meaning that the inclination is $3$ and also implies that as $x$ increases, the graph of $f_{2}$ decreases. In other words, if we think of the sign as indicating direction, the direction of the differential coefficient points towards where the graph of the function increases. Put differently, following the direction indicated by the differential coefficient leads to the peak of the graph.\nBefore extending to 3 dimensions, recall that the differential coefficient of $y$ at $x$, $\\dfrac{ d y}{ d x}=a$, can be written as if it were a fraction. Although this is not a mathematically rigorous way to handle differentiation, it helps understand the geometric meaning and has its advantages. Leibniz thought of $dy$ and $dx$ as very small changes in $y$ and $x$, respectively, and called the ratio between these changes the differential coefficient.\n$$ dy=adx $$\nAs an aside, this helps understand why $a$ is called a differential \u0026lsquo;coefficient\u0026rsquo;.\n3D Now, let\u0026rsquo;s assume a 3D scalar function $f=f(x,y,z)$ and a position vector $\\mathbf{r}=x\\hat{\\mathbf{x}}+y\\hat{\\mathbf{y}}+z\\hat{\\mathbf{z}}$ are given. The change in $f$ is expressed through total differentiation.\n$$ \\begin{equation} df=\\frac{ \\partial f}{ \\partial x }dx + \\frac{ \\partial f}{ \\partial y}dy+\\frac{ \\partial f}{ \\partial z}dz \\end{equation} $$\nThe change in $\\mathbf{r}$ is as follows:\n$$ d\\mathbf{r}=dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}} $$\nNow, as in the 1D case, let\u0026rsquo;s find something that represents the ratio between $df$ and $d\\mathbf{r}$. Since $df$ is a scalar and $d\\mathbf{r}$ is a vector, that \u0026lsquo;something\u0026rsquo; must be a vector, and $df$ can be imagined as the dot product of that \u0026lsquo;something\u0026rsquo; with $d\\mathbf{r}$. Therefore, let\u0026rsquo;s denote that \u0026lsquo;something\u0026rsquo; as $\\mathbf{a}=a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}}$ and express it as follows:\n$$ \\begin{align*} df=\\mathbf{a}\\cdot d\\mathbf{r}\u0026amp;=(a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}})\\cdot(dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}}) \\\\ \u0026amp;= a_{1}dx+a_{2}dy+a_{3}dz \\end{align*} $$\nComparing this with $(2)$ yields the following result:\n$$ \\mathbf{a}=\\frac{ \\partial f}{ \\partial x}\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} $$\nFrom now on, let\u0026rsquo;s denote this vector $\\mathbf{a}$ as $\\nabla f$ and call it the gradient of $f$. The direction of the gradient points to where the function $f$ increases most significantly, and its magnitude represents the extent of this increase.\nRelated Formulas Linearity:\n$$ \\nabla (f + g) = \\nabla f + \\nabla g $$\nProduct Rule:\n$$ \\nabla{(fg)}=f\\nabla{g}+g\\nabla{f} $$ $$ \\nabla(\\mathbf{A} \\cdot \\mathbf{B}) = \\mathbf{A} \\times (\\nabla \\times \\mathbf{B}) + \\mathbf{B} \\times (\\nabla \\times \\mathbf{A})+(\\mathbf{A} \\cdot \\nabla)\\mathbf{B}+(\\mathbf{B} \\cdot \\nabla) \\mathbf{A} $$\nSecond Derivative:\n$$ \\nabla \\cdot (\\nabla T) = \\dfrac{\\partial^{2} T}{\\partial x^{2}} + \\dfrac{\\partial ^{2} T} {\\partial y^{2}} + \\dfrac{\\partial ^{2} T}{\\partial z^{2}} $$ $$ \\nabla \\times (\\nabla T)= \\mathbf{0} $$ $$\\nabla (\\nabla \\cdot \\mathbf{A} ) $$\nFundamental Theorem of Gradient\n$$ T(b)-T(a) = \\int _{a}^{b} (\\nabla T) \\cdot d\\mathbf{l} $$\nIntegration Formulas\n$$ \\int_{\\mathcal{V}} (\\nabla T) d \\tau = \\oint_{\\mathcal{S}} T d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left[ T \\nabla^{2} U + (\\nabla T) \\cdot (\\nabla U) \\right] d \\tau = \\oint_{\\mathcal{S}} (T \\nabla U) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left( T \\nabla^{2} U - U \\nabla^{2} T \\right) d \\tau = \\oint_{\\mathcal{S}} \\left( T \\nabla U - U \\nabla T \\right) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\nPartial Integration\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$ $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\nSee Also Del Operator $\\nabla$ Gradient $\\nabla f$ Divergence $\\nabla \\cdot \\mathbf{F}$ Curl $\\nabla \\times \\mathbf{F}$ Laplacian $\\nabla^{2} f$ ","id":1778,"permalink":"https://freshrimpsushi.github.io/en/posts/1778/","tags":null,"title":"Gradient of Scalar Function in Cartesian Coordinate System"},{"categories":"수리물리","contents":"Definition For a vector function $\\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\\hat{\\mathbf{x}} + F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$, the following vector is defined as the curl of $\\mathbf{F}$, denoted as $\\nabla \\times \\mathbf{F}$.\n$$ \\begin{align} \\nabla \\times \\mathbf{F} \u0026amp;= \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} \\label{def1} \\\\ \u0026amp;=\\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z}\\end{vmatrix} \\label{def2} \\end{align} $$\n$(2)$ serves as an easy-to-remember formula for the curl of $\\mathbf{F}$. Think of it as a determinant and expand accordingly. Explanation Curl translates to rotation. However, since the term \u0026lsquo;rotation\u0026rsquo; is too common and may lead to confusion with rotation instead of curl, the term \u0026lsquo;curl\u0026rsquo; is preferred at the Fresh Shrimp Sushi Restaurant.\n$\\nabla \\times \\mathbf{F}$ is a vector that indicates in which direction the physical quantity $\\mathbf{F}$ is rotating. If you place the direction of $\\nabla \\times \\mathbf{F}$ as the axis (thumb) and apply the right-hand rule, the direction in which your right hand wraps around corresponds to the direction of rotation of $\\mathbf{F}$. The magnitude of the vector $\\nabla \\times \\mathbf{F}$ represents the extent of the rotation.\nUsing Einstein notation and Levi-Civita symbol, it can be expressed as follows. If $\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$ is denoted,\n$$ \\nabla \\times \\mathbf{F} = \\epsilon_{ijk}\\hat{\\mathbf{e}}_{i}\\nabla_{j}F_{k} $$\nMeanwhile, note that the value denoted as $(1)$ in the definition is expressed as $\\nabla \\times \\mathbf{F}$. Although $\\nabla$ is referred to as the del operator, thinking of it as having a meaning on its own could easily lead to confusion, mistaking $\\nabla \\cdot \\mathbf{F}$ or $\\nabla \\times \\mathbf{F}$ as the dot product and cross product, respectively. Hence, it\u0026rsquo;s best to understand $\\nabla$ merely as a convenient notation, and it might be even better to think of the del operator as synonymous with the gradient. The del operators, encompassing the gradient, divergence, and curl, will be discussed in more detail below.\nPoints to Note $\\nabla \\times \\mathbf{F}$ is not the cross product of $\\nabla$ and $\\mathbf{F}$ $\\nabla \\times \\mathbf{F}$ is definitely not the cross product of $\\nabla$ and $\\mathbf{F}$.\r$\\nabla \\times \\mathbf{F}$ is merely a vector containing some information about $\\mathbf{F}$. We think of $\\nabla$ as a vector like $\\nabla = \\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} + \\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}} + \\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}}$, and the result matches perfectly with $(1)$, so it\u0026rsquo;s denoted as $\\nabla \\times \\mathbf{F}$ for convenience. Assuming $\\nabla$ is an actual vector would lead to strange results.\nThe following equation holds for two vectors $\\mathbf{A}, \\mathbf{B}$:\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\nIf $\\nabla$ were truly a vector, we could substitute it into the above formula and obtain the following results.\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=(\\mathbf{F} \\cdot \\nabla)\\nabla - (\\nabla \\cdot \\nabla)\\mathbf{F} + \\nabla (\\nabla \\cdot \\mathbf{F}) - \\mathbf{F} (\\nabla \\cdot \\nabla) $$\nHowever, the correct result is as follows.\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=\\nabla(\\nabla \\cdot \\mathbf{F})-\\nabla ^{2} \\mathbf{F} $$\nAnother example exists. Since the cross product of vectors has the property of anticommutativity, if $\\nabla \\times \\mathbf{F}$ were a cross product, the following equation should hold:\n$$ \\nabla \\times \\mathbf{F} \\overset{?}{=} - \\mathbf{F} \\times \\nabla $$\nTherefore, $\\nabla$ is not a vector, and it can be understood that $\\nabla \\times \\mathbf{F}$ is not the cross product of $\\nabla$ and $\\mathbf{F}$. Instead of being a vector, $\\nabla \\times$ should be considered as a function in itself. In physics, functions that take other functions as variables are called operators.\nSo, what\u0026rsquo;s the difference between $\\nabla \\times \\mathbf{F}$ and $\\mathbf{F} \\times \\nabla$? $\\nabla \\times$ is an operator defined as follows, taking a vector function as its variable:\n$$ \\nabla \\times (\\mathbf{F}) = \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} $$\nIn other words, $\\nabla \\times \\mathbf{F}$ is the function value when the variable $\\mathbf{F}$ is substituted into the operator (function) $\\nabla \\times$. Of course, this in turn is a vector function of the variables $(x,y,z)$. While $\\nabla \\times \\mathbf{F}$ is the function value of $\\nabla \\times$, $\\mathbf{F} \\times \\nabla$ is an operator in itself. Although it\u0026rsquo;s not a commonly used expression, it can be defined as the following differential operator if we were to define it.\n$$ \\begin{align*} \\mathbf{F} \\times \\nabla \u0026amp;= \\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\end{vmatrix} \\\\ \u0026amp;= \\left( F_{y}\\dfrac{ \\partial }{ \\partial z} - F_{z}\\dfrac{ \\partial }{ \\partial y} \\right)\\hat{\\mathbf{x}} + \\left( F_{z}\\dfrac{ \\partial }{ \\partial x} - F_{x}\\dfrac{ \\partial }{ \\partial z} \\right)\\hat{\\mathbf{y}} + \\left( F_{x}\\dfrac{ \\partial }{ \\partial y} - F_{y}\\dfrac{ \\partial }{ \\partial x} \\right)\\hat{\\mathbf{z}} \\end{align*} $$\nDerivation Now, let\u0026rsquo;s consider a function that indicates the direction of rotation (clockwise or counterclockwise) of a rotating vector function. It\u0026rsquo;s important to note that no direction within the plane of rotation can specify the direction of rotation. Look at the diagram below.\nVector $-\\hat{\\mathbf{x}}$ can explain the movement at point $A$, but not at point $B$. Vector $\\hat{\\mathbf{y}}$ can explain the movement at point $C$, but not at point $D$. Vector $\\hat{\\mathbf{x}} + \\hat{\\mathbf{y}}$ can explain the path $F$, but not $G$. This is also true for clockwise rotation. Now, you should sense the need to move out of the plane of rotation to specify the direction of rotation. In fact, there\u0026rsquo;s already a good method to determine this: using the right-hand rule, which determines the axis of rotation in the direction of the thumb when the right hand wraps around. Therefore, in the $xy$-plane, the axis (direction) of counterclockwise rotation is $\\hat{\\mathbf{z}}$, and the axis (direction) of clockwise rotation is $-\\hat{\\mathbf{z}}$.\nNow, let\u0026rsquo;s find a value that indicates the $\\hat{\\mathbf{z}}$ direction when $\\mathbf{F}$ is rotating counterclockwise in the $xy$-plane, in other words, a positive value. Let\u0026rsquo;s represent the rotation simply with a rectangle as below.\nPath ① moves from point $a$ to point $b$, and let\u0026rsquo;s say $\\mathbf{F}(a) = (1,0,0)$ and $\\mathbf{F}(b) = (0,1,0)$. Then, as $x$ changes by +1 from point $a$ to $b$, and $F_{y}$ also changes by +1, we obtain the following.\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 $$\nIn the same manner, on the path where the point moves from $b$ to $c$, $y$ changes by +1, and $F_{x}$ changes by -1. Checking all four paths, we find that:\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 \\quad \\text{in path $\\textcircled{1}$, $\\textcircled{3}$} $$\n$$ \\dfrac{\\partial F_{x}}{\\partial y} \\lt 0 \\quad \\text{in path $\\textcircled{2}$, $\\textcircled{4}$} $$\nTherefore, for a vector $\\mathbf{F}$ that rotates counterclockwise as above, the value below is always positive:\n$$ \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\gt 0 $$\nConversely, if $\\mathbf{F}$ is rotating clockwise, the above value is always negative. Now, we can define the operator $\\operatorname{curl}_{xy}$, which indicates the direction and magnitude of rotation in the $xy$-plane when the vector function $\\mathbf{F}$ is substituted:\n$$ \\operatorname{curl}_{xy} (\\mathbf{F}) = \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right) \\hat{\\mathbf{z}} $$\nThe sign of the $\\hat{\\mathbf{z}}$ component of this function indicates the direction of rotation of $\\mathbf{F}$ in the $xy$-plane: If it\u0026rsquo;s positive (+), $\\mathbf{F}$ rotates counterclockwise in the $xy$-plane. If it\u0026rsquo;s negative (-), $\\mathbf{F}$ rotates clockwise in the $xy$-plane. If it\u0026rsquo;s zero (0), there is no rotation. The magnitude of the $\\hat{\\mathbf{z}}$ component of this function indicates how rapidly $\\mathbf{F}$ is rotating in the $xy$-plane. Applying this discussion similarly to the $yz$-plane and the $zx$-plane, we can define the vector $\\nabla \\times \\mathbf{F}$, which indicates the direction and magnitude of rotation of $\\mathbf{F}$ in 3-dimensional space, as follows.\n$$ \\nabla \\times \\mathbf{F} := \\left( \\dfrac{\\partial F_{z}}{\\partial y} - \\dfrac{\\partial F_{y}}{\\partial z} \\right)\\hat{\\mathbf{x}} + \\left( \\dfrac{\\partial F_{x}}{\\partial z} - \\dfrac{\\partial F_{z}}{\\partial x} \\right)\\hat{\\mathbf{y}} + \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right)\\hat{\\mathbf{z}} $$\n■\nRelated formulas Linearity: $$ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) = \\nabla \\times \\mathbf{A} + \\nabla \\times \\mathbf{B} $$\nMultiplication rule:\n$$ \\nabla \\times (f\\mathbf{A}) = f(\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\times (\\nabla f) $$\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\n2nd derivative:\n$$ \\nabla \\times (\\nabla f) = \\mathbf{0} $$\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F}) = \\nabla (\\nabla \\cdot \\mathbf{F}) - \\nabla^{2} \\mathbf{F} $$\nStokes\u0026rsquo; theorem $$ \\int_{\\mathcal{S}} (\\nabla \\times \\mathbf{v} )\\cdot d\\mathbf{a} = \\oint_{\\mathcal{P}} \\mathbf{v} \\cdot d\\mathbf{l} $$\nIntegral formulas $$ \\int_{\\mathcal{V}} (\\nabla \\times \\mathbf{v}) d \\tau = - \\oint_{\\mathcal{S}} \\mathbf{v} \\times d \\mathbf{a} $$\n$$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\nIntegration by parts $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n$$ \\int_{\\mathcal{V}} \\mathbf{B} \\cdot \\left( \\nabla \\times \\mathbf{A} \\right) d\\tau = \\int_{\\mathcal{V}} \\mathbf{A} \\cdot \\left( \\nabla \\times \\mathbf{B} \\right) d\\tau + \\oint_{\\mathcal{S}} \\left( \\mathbf{A} \\times \\mathbf{B} \\right) \\cdot d \\mathbf{a} $$\nProof Linearity Using Einstein notation and Levi-Civita symbol, if we denote $\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$, then:\n$$ \\begin{align*} \\left[ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) \\right]_{i} \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (\\mathbf{A} + \\mathbf{B})_{k} \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (A_{k} + B_{k}) \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j}A_{k} + \\epsilon_{ijk} \\nabla_{j}B_{k} \\\\ \u0026amp;= [\\nabla \\times \\mathbf{A}]_{i} + [\\nabla \\times \\mathbf{B}]_{i} \\\\ \\end{align*} $$\nThe third equality holds because $\\dfrac{\\partial (A_{k} + B_{k})}{\\partial x_{j}} = \\dfrac{\\partial A_{k}}{\\partial x_{j}} + \\dfrac{\\partial B_{k}}{\\partial x_{j}}$. ■\nFurther reading Del Operator $\\nabla$ Gradient $\\nabla f$ Divergence $\\nabla \\cdot \\mathbf{F}$ Curl $\\nabla \\times \\mathbf{F}$ Laplacian $\\nabla^{2} f$ ","id":1752,"permalink":"https://freshrimpsushi.github.io/en/posts/1752/","tags":null,"title":"Curl of Vector Functions in 3D Cartesian Coordinates"},{"categories":"거리공간","contents":"Definition For $a_{i},b_{i} \\in \\mathbb{R} (1\\le i \\le k)$, the set $I=[a_{1},b_{1}] \\times [a_{2},b_{2}]\\times \\cdots \\times [a_{k},b_{k}]$ is called a $k$-cellk-cell. Here, $\\times$ represents the Cartesian product of sets.\nTheorem 1 Let\u0026rsquo;s assume a sequence of closed intervals on $\\mathbb{R}$, $\\left\\{ I_{n} \\right\\}$, satisfies $I_{n}\\supset I_{n+1}\\ (n=1,2,\\cdots)$. Then, the following holds true.\n$$ \\bigcap_{i=1}^{\\infty}I_{n}\\ne \\varnothing $$\nProof Let\u0026rsquo;s denote $I_{n}=[a_{n},b_{n}]$. Also, let $E=\\left\\{ a_{n} : n=1,2,\\cdots \\right\\}$. Then, $E\\ne \\varnothing$ and is upper bounded by $b_{1}$1. Now, let\u0026rsquo;s define $x=\\sup E$. For any two positive numbers $m$ and $n$,\n$$ a_{n} \\le a_{m+n} \\le b_{m+n} \\le b_{m} $$\nholds true, thus for all $n$, $x\\le b_{n}$. Moreover, since $x$ is the upper bound of $E$, it\u0026rsquo;s trivial that for all $n$, $a_{n} \\le x$. Therefore, for all $n$, $a_{n}\\le x \\le b_{n}$, which implies $x\\in I_{n}\\ \\forall n$. Thus,\n$$ x\\in \\bigcap _{i=1}^{n}I_{n} $$\n■\nTheorem 2 Let $\\left\\{ I_{n} \\right\\}$ be a sequence of $k$-cells satisfying $I_{n}\\supset I_{n+1}(n=1,2,\\cdots)$. Then, $\\bigcap _{i=1}^{n}I_{n}\\ne\\varnothing$.\nTheorem 2 is an extension of Theorem 1 to $\\mathbb{R}^{k}$.\nProof Let\u0026rsquo;s represent $I_{n}$ as follows.\n$$ I_{n}=\\left\\{ \\mathbf{x}=(x_{1},\\cdots,x_{k}) : a_{n,j} \\le x_{j} \\le b_{nj},\\quad(1\\le j \\le k;\\ n=1,2,\\cdots) \\right\\} $$\nThat is, $I_{n}=I_{n,1}\\times \\cdots\\times I_{n,k}\\ (I_{n,j}=[a_{n,j},b_{n,j}])$. By Theorem 1, for each $I_{n,j}$, there exists $x_{j}^{\\ast}\\in I_{n,j} \\ (a_{n,j} \\le x_{j}^{\\ast} \\le b_{n,j})$. Therefore,\n$$ \\mathbf{x^{\\ast}} =(x_{1}^{\\ast},\\cdots ,x_{k}^{\\ast})\\in I_{n} ,\\quad (n=1,2,\\cdots) $$\n■\nTheorem 3 Every $k$-cell is compact.\nProof Let\u0026rsquo;s consider an arbitrary $k$-cell $I$ as follows.\n$$ I=I^{1}\\times \\cdots \\times I^{k}=[a_{1},b_{1}]\\times \\cdots \\times [a_{k},b_{k}] $$\nAnd let\u0026rsquo;s define as follows.\n$$ \\mathbf{x}=(x_{1},\\cdots,x_{k}) \\quad \\text{and} \\quad a_{j} \\le x_{j} \\le b_{j}(1\\le j \\le k) $$\nNow, let\u0026rsquo;s consider $\\delta$ as follows.\n$$ \\delta =\\left( \\sum \\limits_{j=1}^{k}(b_{j})-a_{j})^{2} \\right)^{{\\textstyle \\frac{1}{2}}}=|\\mathbf{b}-\\mathbf{a}| $$\nHere, $\\mathbf{a}=(a_{1},\\cdots,a_{n})$, $\\mathbf{b}=(b_{1},\\cdots,b_{n})$. Then, $\\delta$ is the same as the distance between $\\mathbf{b}$ and $\\mathbf{a}$. Therefore,\n$$ |\\mathbf{x}-\\mathbf{y}| \\le \\delta \\quad \\forall \\mathbf{x},\\mathbf{y}\\in I $$\nis valid. Now the proof begins in earnest, using a proof by contradiction. That is, assume that a $k$-cell is not compact. Then, by the definition of compactness, it\u0026rsquo;s the same as assuming that some open cover $\\left\\{ O_{\\alpha} \\right\\}$ of $I$ does not have a finite subcover. Let\u0026rsquo;s denote $c_{j}=(a_{j}+b_{j})/2$. Then, each $I^{j}$ can be divided into $[a_{j},c_{j}]$, $[c_{j},b_{j}]$ using $c_{j}$, creating $2^{k}$ 1-cells. Their union is naturally $I$, and by assumption, at least one of them cannot be covered by any finite subcover of $\\left\\{ O_{\\alpha} \\right\\}$. Let\u0026rsquo;s call this cell $I_{1}$. Then, by choosing intervals in the same way as $I_{1}$ was chosen from $I$, we can obtain a sequence $\\left\\{ I_{n} \\right\\}$ satisfying the following three rules.\n$(\\mathrm{i})$ $I\\supset I_{1} \\supset I_{2}\\supset \\cdots$\n$(\\mathrm{ii})$ Each $I_{n}$ cannot be covered by any finite subcover of $\\left\\{ O_{\\alpha} \\right\\}$.\n$(\\mathrm{iii})$ $|\\mathbf{x}-\\mathbf{y}|\\le 2^{-n}\\delta,\\quad \\forall \\mathbf{x},\\mathbf{y}\\in I_{n}$\nThen, by $(\\mathrm{i})$ and Theorem 2, there exists $\\mathbf{x}^{\\ast}\\in I_{n}$ for all $n$. Since $\\left\\{ O_{\\alpha} \\right\\}$ is an open cover of $I$, there is some $\\alpha$ for which $\\mathbf{x}^{\\ast}\\in O_{\\alpha}$. As $O_{\\alpha}$ is an open set, there exists $r\u0026gt;0$ such that $|\\mathbf{x}^{\\ast}-\\mathbf{y}|\u0026lt;r \\implies \\mathbf{y}\\in O_{\\alpha}$. On the other hand, $n$ can be sufficiently large so that $2^{-n}\\delta\u0026lt;r$. Then, by $(\\mathrm{iii})$, $I_{n}\\subset O_{\\alpha}$. However, this contradicts $(\\mathrm{ii})$, so the assumption is wrong. Therefore, every $k$-cell is compact.\n■\nFrom the above facts, we can prove the following useful theorems.\nEquivalent Conditions for Compactness in Euclidean Space For a subset $E\\subset \\mathbb{R}^{k}(\\mathrm{or}\\ \\mathbb{C}^{k})$ of the real (or complex) space, the following three propositions are equivalent.\n(a) $E$ is closed and bounded.\n(b) $E$ is compact.\n(c) Every infinite subset\nof $E$ has an accumulation point $p \\in E$.\nHere, the equivalence of (a) and (b) is known as the Heine-Borel theorem. An $E$ satisfying (c) is said to be \u0026lsquo;compact with respect to accumulation points\u0026rsquo; or \u0026lsquo;having the Bolzano-Weierstrass property\u0026rsquo;. The equivalence of (b) and (c) holds in metric spaces but is not generally true in topological spaces.\nProof (a) $\\implies$ (b)\nAssuming (a), there exists a $k$-cell $I$ such that $E \\subset I$. Since $I$ is compact, and a closed subset of a compact set is compact, $E$ is compact.\n(b) $\\implies$ (c)\nThis is proved by contradiction.\nLet $S$ be an infinite subset of a compact set $E$. Assume that $S$ has no accumulation point. Then, every $p\\in E$ has at most one point of $S$ in its neighborhood $N_{p}$. When $p \\in S$, that one point is $p$ itself. And this implies that the open cover $\\left\\{ N_{p} \\right\\}$ does not have a finite subcover for $S$. Since $S \\subset E$, similarly, there\u0026rsquo;s no finite subcover for $E$ either, contradicting the assumption that $E$ is compact. Hence, $S$ has an accumulation point $p \\in E$.\n(c) $\\implies$ (a)\nThis is proved by contradiction.\npart 1. $E$ is bounded\nLet\u0026rsquo;s assume $E$ is not bounded. Then, $E$ contains points $\\mathbf{x}_{n}$ satisfying the following inequality.\n$$ |\\mathbf{x}_{n}| \u0026gt;n\\quad (n=1,2,\\cdots) $$\nLet\u0026rsquo;s denote $S=\\left\\{ \\mathbf{x}_{n} :n=1,2,\\cdots\\right\\}$. $S$ is infinite and obviously does not have an accumulation point in $\\mathbb{R}^{k}, contradicting (c). Thus, $E$ is bounded.\npart 2. $E$ is closed\nLet\u0026rsquo;s assume $E$ is not closed. Then, by definition, there exists an accumulation point $\\mathbf{x}_{0}$ of $E$ that is not included in $E. Now, for $n=1,2,\\cdots$, let\u0026rsquo;s consider $\\mathbf{x}_{n} \\in E$ satisfying the following conditions.\n$$ \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \u0026lt; {\\textstyle \\frac{1}{n}} $$\nLet\u0026rsquo;s denote the set of such $\\mathbf{x}_{n}$ as $S$. $S$ is infinite and has $\\mathbf{x}_{0}$ as an accumulation point. If $\\mathbf{x}_{0}$ is the only accumulation point of $S$, then $\\mathbf{x}_{0}\\notin E$ contradicts (c), proving $E$ is closed. Now, consider $\\mathbf{y} \\ne\\mathbf{x}_{0}$ in $\\mathbb{R}^{k}$. Then,\n$$ \\begin{align*} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \u0026amp; \\ge \\left|\\mathbf{x}_{0} - \\mathbf{y} \\right| - \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \\\\ \u0026amp; \\ge \\left| \\mathbf{x}_{0} - \\mathbf{y} \\right| -\\frac{1}{n} \\end{align*} $$\nFor sufficiently large $n$, the following holds true.\n$$ \\begin{equation} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \\ge \\left| \\mathbf{x}_{0}- \\mathbf{y} \\right|-\\frac{1}{n} \\ge \\frac{1}{2}\\left|\\mathbf{x}_{0}-\\mathbf{y} \\right| \\label{eq1} \\end{equation} $$\nFurthermore, as $n$ increases, $\\mathbf{x}_{n}$ gets closer to $\\mathbf{x}_{0}$. This fact, along with $\\eqref{eq1}$, implies that we can find a neighborhood of $\\mathbf{y}$ that contains no point other than $\\mathbf{y}$ as $n$ increases. Thus, $\\mathbf{y}$ is not an accumulation point of $S$, proving $\\mathbf{x}_{0}$ is the only accumulation point of $S. This contradicts (c), proving $E$ is closed.\n■\nBolzano-Weierstrass Theorem Every bounded infinite subset of $\\mathbb{R}^{k}$ has an accumulation point $p \\in \\mathbb{R}^{k}$.\nProof Let $E$ be a bounded infinite subset of $\\mathbb{R}^{k}$. Since $E$ is bounded, there exists a $k$-cell $I$ such that $E \\subset I. As $k$-cells are compact, $I$ is compact. Then, by the equivalent condition for compactness in $\\mathbb{R}^{k}$ $(b)\\implies (c)$, $E$ has an accumulation point $p \\in I \\subset \\mathbb{R}^{k}$.\n■\nSee Also Specialization of Riesz\u0026rsquo;s Theorem Riesz\u0026rsquo;s Theorem in normed spaces indicates the compactness of the closed unit ball $\\overline{B (0;1)}$ as an equivalent condition of finite dimension. The $k$-cell $[0,1]^{k}$ in Euclidean space is compact, and since there is a homeomorphism with the closed unit ball, Riesz\u0026rsquo;s theorem can be seen as a generalization of the compactness of the $k$-cell.\nAny $b_{n}$ will suffice.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1711,"permalink":"https://freshrimpsushi.github.io/en/posts/1711/","tags":null,"title":"Every k Cell is Compact"},{"categories":"거리공간","contents":"Definitions Let\u0026rsquo;s say $(X,d)$ is a metric space. Suppose $p \\in X$ and $E \\subset X$.\nA set that contains all $q$ satisfying $d(q,p)\u0026lt;r$ is defined as the neighborhoodneighborhood of point $p$, denoted by $N_{r}(p)$. In this case, $r$ is called the radius of $N_{r}(p)$. If it\u0026rsquo;s permissible to omit the distance, it can also be denoted as $N_{p}$.\nIf every neighborhood of $p$ includes a $q$ that is $q\\ne p$ and $q\\in E$, then $p$ is called a limit pointlimit point of $E$.\nIf all limit points of $E$ are included in $E$, then $E$ is said to be closedclosed.\nIf there exists a neighborhood $N$ satisfying $N\\subset E$ for $p$, then $p$ is called an interior pointinterior point of $E$.\nIf every point of $E$ is an interior point of $E$, then $E$ is said to be openopen.\nThe set of all limit points of $E$ is called the derived setderived set of $E$, denoted by $E^{\\prime}$.\nThe union of $E$ and $E^{\\prime}$ is called the closureclosure, denoted by $\\overline{E}=E\\cup E^{\\prime}$.\nTheorem 1 For $A,B\\subset X$, the following equations hold:\n(1a) $A\\subset B \\implies A^{\\prime} \\subset B^{\\prime}$\n(1b) $(A\\cup B)^{\\prime}=A^{\\prime}\\cup B^{\\prime}$\n(1c) $(A \\cap B)^{\\prime} \\subset A^{\\prime}\\cap B^{\\prime}$\nProof (1a) Assume $A\\subset B$. And let $p\\in A^{\\prime}$. Then $p$ is a limit point of $A$, so by the definition of a limit point, every neighborhood $N$ of $p$ includes a $q$ that is $q\\ne p$ and $q\\in A$. Assuming $A\\subset B$, the statement means that every neighborhood $N$ of $p$ includes a $q$ that is $q\\ne p$ and $q\\in B$. Therefore, by the definition of a limit point, $p \\in B^{\\prime}$ is established.\n■\n(1b) part 1. $A^{\\prime} \\cup B^{\\prime} \\subset (A\\cup B)^{\\prime}$\nSince $A\\subset A\\cup B$ and $B \\subset A\\cup B$, by $(a1)$, it follows that:\n$$ A^{\\prime} \\subset (A\\cup B)^{\\prime} \\quad \\text{and} \\quad B^{\\prime} \\subset (A \\cup B)^{\\prime} $$\nTherefore,\n$$ A^{\\prime} \\cup B^{\\prime} \\subset (A\\cup B)^{\\prime} $$\npart 2. $(A\\cup B)^{\\prime} \\subset A^{\\prime}\\cup B^{\\prime}$\nLet\u0026rsquo;s say $p \\in (A\\cup B)^{\\prime}$. Then, by the definition of a limit point, every neighborhood $N$ of $p$ includes a $q$ that is $q\\ne p$ and $q\\in A\\cup B$. Rewriting $q\\in A\\cup B$ as $q\\in A \\text{ or } q\\in B$ means $p \\in A^{\\prime} \\text{ or } p\\in B^{\\prime}$. Therefore, $p\\in A^{\\prime}\\cup B^{\\prime}$, so it follows that:\n$$ (A\\cup B)^{\\prime} \\subset A^{\\prime}\\cup B^{\\prime} $$\npart 3.\nCombining the above results yields:\n$$ A^{\\prime}\\cup B^{\\prime} = (A\\cup B)^{\\prime} $$\n■\n(1c) Since $A\\cap B \\subset A$ and $A\\cap B \\subset B$, by (1a), it follows that:\n$$ (A\\cap B)^{\\prime} \\subset A^{\\prime} \\quad \\text{and} \\quad (A\\cap B)^{\\prime} \\subset B^{\\prime} $$\nTherefore,\n$$ (A\\cap B)^{\\prime} \\subset A^{\\prime}\\cap B^{\\prime} $$\n■\nTheorem 2 For $A,B \\subset X$, the following equations hold:\n(2a) $A\\subset B \\implies \\overline{A} \\subset \\overline{B}$\n(2b) $\\overline{A\\cup B} = \\overline{A}\\cup \\overline{B}$\n(2c) $\\overline{A\\cap B} \\subset \\overline{A}\\cap \\overline{B}$\nProof (2a) Assuming $A \\subset B$, by (1a), $A^{\\prime} \\subset B^{\\prime}$ is established. Therefore,\n$$ \\overline{A} = A\\cup A^{\\prime} \\subset B \\cup B^{\\prime} = \\overline{B} $$\n■\n(2b) part 1. $\\overline{A\\cup B}\\subset \\overline{A}\\cup \\overline{B}$\nLet\u0026rsquo;s say $p \\in \\overline{A\\cup B}$. This means $p\\in A\\cup B$ or $p \\in (A\\cup B)^{\\prime}$.\ncase 1-1. $p \\in A\\cup B$\nIn this case, $p \\in A$ or $p \\in B$. But since $A \\subset \\overline{A}$ and $B \\subset \\overline{B}$,\n$$ p\\in \\overline{A}\\ \\text{or} \\ p \\in \\overline{B}\\implies p \\in \\overline{A}\\cup \\overline{B} $$\ncase 1-2. $p\\in (A\\cup B)^{\\prime}$\nBy (1b), $p\\in (A\\cup B)^{\\prime}=A^{\\prime}\\cup B^{\\prime}$ is established. This means $p\\in A^{\\prime}$ or $p\\in B^{\\prime}$. But since $A^{\\prime} \\subset \\overline{A}$ and $B^{\\prime} \\subset \\overline{B}$, similarly to the previous case,\n$$ p\\in \\overline{A}\\ \\text{or} \\ p \\in \\overline{B}\\implies p \\in \\overline{A}\\cup\\overline{B} $$\nBy case 1-1, 1-2, the following is established:\n$$ \\overline{A\\cup B}\\subset \\overline{A}\\cup \\overline{B} $$\npart 2. $\\overline{A}\\cup \\overline{B} \\subset \\overline{A\\cup B}$\nSince $A \\subset A\\cup B$ and $B\\subset A\\cup B$, by $(b1)$, the following is established:\n$$ \\overline{A} \\subset \\overline{A\\cup B}\\quad \\text{and} \\quad \\overline{B}\\subset \\overline{A\\cup B} $$\nTherefore,\n$$ \\overline{A}\\cup \\overline{B} \\subset \\overline{A\\cup B} $$\n■\n(2c) Let\u0026rsquo;s say $p \\in \\overline{A\\cap B}$. Then, $p\\in A\\cap B$ or $p\\in (A \\cap B)^{\\prime}$.\ncase 1. $p\\in A\\cap B$\nIn this case, $p \\in A$ while $p \\in B$. But since $A\\subset \\overline{A}$ and $B\\subset \\overline{B}$,\n$$ p\\in A \\ \\text{and} \\ \\ p \\in B \\implies p\\in \\overline{A} \\ \\text{and} \\ p \\in \\overline{B} \\implies p\\in \\overline{A}\\cap \\overline{B} $$\ncase 2. $p \\in (A\\cap B)^{\\prime}$\nBy (1a), $(A\\cap B)^{\\prime}\\subset A^{\\prime}$ and $(A\\cap B)^{\\prime} \\subset B^{\\prime}$ are established. But since $A^{\\prime}\\subset \\overline{A}$ and $B^{\\prime} \\subset \\overline{B}$,\n$$ p\\in A^{\\prime} \\ \\text{and} \\ p\\in B^{\\prime} \\implies p\\in \\overline{A}\\quad \\text{and} \\quad p\\in \\overline{B}\\implies p\\in \\overline{A}\\cap \\overline{B} $$\n■\nTheorem 3 For metric spaces $(X,d)$ and $E \\subset X$, the following facts hold:\n(3a) $\\overline{E}$ is closed.\n(3b) $E=\\overline{E}$ being equivalent to $E$ being closed.\n(3c) For a closed set $F\\subset X$ satisfying $E\\subset F$, $\\overline{E} \\subset F$ holds.\n(3a) and (3c) imply that $\\overline{E}$ is the smallest closed subset of $X$ that includes $E$.\nProof (3a) Let\u0026rsquo;s say $p \\in X$ and $p \\notin \\overline{E}$. In other words, $p \\in (\\overline{E})^{c}$. Then $p$ is neither a point of $E$ nor of $E^{\\prime}$. Therefore, by the definition of an accumulation point, $p$ has at least one neighborhood $N$ where $N\\cap E=\\varnothing$ is true. Therefore, since $N\\subset (\\overline{E})^{c}$ and $p$ was any point of $(\\overline{E})^{c}$, by the definition of an interior point, every point of $(\\overline{E})^{c}$ is an interior point, which means $(\\overline{E})^{c}$ is an open set. Since $(\\overline{E})^{c}$ is an open set, $\\overline{E}$ is a closed set.1\n■\n(3b) $(\\implies)$\nSince $E=\\overline{E}=E \\cup E^{\\prime}$, all accumulation points of $E$ are elements of $E$. This is the definition of a closed set, so $E$ is closed. Or it can be immediately understood from the definitions of closure and being closed.\n$(\\impliedby)$\nBy the definition of a closed set, all accumulation points of $E$ are included in $E$. Therefore, $\\overline{E}=E\\cup E^{\\prime}=E$\n■\n(3c) Let $F$ be a closed set with $E\\subset F \\subset X$. Then, by (3b), $F^{\\prime} \\subset \\overline{F}=F$ is true. Also, by (2a), $E^{\\prime} \\subset F^{\\prime} \\subset F$ is true. Therefore, the following holds:\n$$ E \\subset F \\quad \\text{and} \\quad E^{\\prime}\\subset F $$\nTherefore,\n$$ E\\cup E^{\\prime} =\\overline{E} \\subset F $$\n■\nTheorem 4 Let $E$ be a non-empty set of real numbers and suppose it is bounded above. Let $y=\\sup E$. Then, $y \\in \\overline{E}$ is true. Moreover, if $E$ is closed, then $y \\in E$ is true.\nProof If $y \\in \\overline{E}$ holds, then the subsequent propositions are trivial by (3a), so we will only prove $y \\in \\overline{E}$. The proof is divided into two cases.\ncase 1. $y \\in E$\n$$ y \\in E \\subset \\overline{E} $$\nTherefore, it holds.\ncase 2. $y \\notin E$\nThen, for every positive number $h\u0026gt;0$, there exists $x\\in E$ satisfying $y-h\u0026lt;x\u0026lt;y$. This means that every neighborhood of $y$, which is $N_{h}(y)$, must contain an element of $E$. Therefore, by definition, $y$ is an accumulation point of $E$. Thus, $y\\in E\\cup E^{\\prime}=\\overline{E}$\n■\nRefer to Theorem 2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1701,"permalink":"https://freshrimpsushi.github.io/en/posts/1701/","tags":null,"title":"Closure and Derived Set in Metric Space"},{"categories":"거리공간","contents":"Definition Let\u0026rsquo;s say $(X,d)$ is a metric space. Suppose $p \\in X$ and $E \\subset X$.\nThe set that includes all $q$s satisfying $d(q,p)\u0026lt;r$ is defined as the neighborhoodneighborhood of point $p$ and is denoted as $N_{r}(p)$. Here, $r$ is called the radius of $N_{r}(p)$. If the distance can be omitted, it may also be denoted as $N_{p}$.\nIf all neighborhoods of $p$ contain $q$, which is $q\\ne p$ and $q\\in E$, then $p$ is called a limit pointlimit point of $E$.\nIf $p\\in E$ and $p$ is not a limit point of $E$, then $p$ is called an isolated pointisolated point of $E$.\nIf all limit points of $E$ are included in $E$, then $E$ is said to be closedclosed.\nIf there exists a neighborhood $N$ satisfying $N\\subset E$, then $p$ is called an interior pointinterior point of $E$.\nIf every point of $E$ is an interior point of $E$, then $E$ is said to be openopen.\nThe set that includes all $p$s that are $p \\in X$ and $p \\notin E$ is called the complementcomplement of $E$ and is denoted as $E^{c}$.\nIf $E$ is closed and every point of $E$ is a limit point of $E$, then $E$ is said to be perfectperfect.\nIf there exists a point $q\\in X$ and a real number $M$ satisfying $\\forall p\\in E,\\ d(p,q)\u0026lt;M$, then $E$ is said to be boundedbounded.\nIf every point of $X$ is either a limit point of $E$ or a point of $E$, then $E$ is said to be densedense in $X$.\nThe set of all limit points of $E$ is called the derived setderived set of $E$ and is denoted as $E^{\\prime}$.\nThe union of $E$ and $E^{\\prime}$ is called the closureclosure and is denoted as $\\overline{E}=E\\cup E^{\\prime}$.\nExplanation The concepts of openness, limit points, denseness, interior points, etc., mentioned above can be defined through different statements but are essentially the same. Why each concept is defined and named as such can be easily grasped by directly drawing them in one or two dimensions. An isolated point is defined as a point that is not a limit point, so it cannot be both an isolated and a limit point at the same time. Conversely, open and closed sets are defined based on separate conditions, so contrary to the intuitive feeling their names might convey, there can exist sets that are both open and closed, or neither open nor closed. An example of the former is $\\mathbb{R}^{2}$, and an example of the latter is $\\left\\{ {\\textstyle \\frac{1}{n}}\\ |\\ n\\in \\mathbb{N} \\right\\}$. Considering the definitions of interior points and neighborhoods, the condition for $x$ to be an interior point of $E$ is the same as the existence of some positive number $\\varepsilon\u0026gt;0$ such that\n$$ d(x,p) \u0026lt;\\varepsilon \\implies x \\in E $$\nis satisfied. Several theorems and proofs related to the above concepts are introduced, following the notation from the definitions.\nTheorem 1 All neighborhoods are open sets.\nProof Let\u0026rsquo;s say $E=N_{r}(p)$. Also, consider any $q \\in E$. Then, by the definition of a neighborhood, there must exist a positive real number $h$ that satisfies the following equation:\n$$ d(p,q)=r-h\u0026lt;r $$\nThen, by the definition of distance, for all $s$ that satisfy $d(q,s)\u0026lt;h$, the following equation holds:\n$$ d(p,s)\\le d(p,q)+d(q,s)\u0026lt;(r-h)+h=r $$\nTherefore, by the definition of a neighborhood, $s \\in E$ is true. This shows that any point $s$ within the neighborhood $N_{h}(q)$ of $q$ is an element of $E$. Hence, $N_{h}(q) \\subset E$, meaning $q$ is an interior point of $E$. Since we initially considered $q$ to be any point of $E$, all points of $E$ are interior points. Therefore, $E$ is an open set.\n■\nTheorem 2 A set $E$ being an open set is equivalent to $E^c$ being a closed set.\nProof $(\\impliedby)$\nAssume $E^c$ is closed. Now, for any $p\\in E$, since $p \\notin E^c$ and by the definition of being closed, $p$ is not a limit point of $E^c$. Thus, there exists a neighborhood $N$ satisfying $N \\cap E^c=\\varnothing$. This implies $N \\subset E$ and, by the definition of an interior point, $p$ is an interior point of $E$. Since any $p\\in E$ is an interior point of $E$, by definition, $E$ is an open set.\n$(\\implies)$\nAssume $E$ is open. And let $p$ be a limit point of $E^{c}$. Then, by the definition of a limit point, every neighborhood of $p$ contains at least one point of $E^{c}$. Thus, every neighborhood of $p$ does not include $E$, which means $p$ is not an interior point of $E$. Since we assumed $E$ is open, $p\\notin E$ is true. Therefore, since all limit points $p$ of $E^{c}$ are included in $E^{c}$, $E^{c}$ is closed.\n■\nTheorem 3 Let\u0026rsquo;s say $p$ is a limit point of $E$. Then, the neighborhood of $p$ contains infinitely many points of $E$.\nThis can be expressed differently as \u0026lsquo;A finite set does not have a limit point\u0026rsquo;, \u0026lsquo;A set with a limit point is an infinite set\u0026rsquo;.\nProof Assume the neighborhood $N$ of $p$ includes only a finite number of elements of $E$. And let $q_{1},q_{2},\\cdots,q_{n}$ be points of $N\\cap E$ that are not $p$. Also, let $r$ be the minimum of the distances between $p$ and $q_{i}$.\n$$ r= \\min \\limits _{1\\le i \\le n}d(p,q_{i}) $$\nSince each $q_{i}$ is different from $p$, all distances are positive, and the minimum of positive numbers is also positive, thus $r\u0026gt;0$ is true. Now, consider another neighborhood $N_{r}(p)$ of $p$. Then, by the definitions of neighborhood and distance, $N_{r}(p)$ contains no $q_{i}$. Thus, by the definition of a limit point, $p$ is not a limit point of $E$. This contradicts the fact that $p$ is a limit point of $E$. Therefore, by reductio ad absurdum, the assumption is incorrect, proving the theorem.\n■\nCorollary A set with only a finite number of points does not have a limit point.\nTheorem 4 For a metric space $(X,d)$ and $E \\subset X$, the following facts hold: $(a)$ $\\overline{E}$ is closed.$(b)$ Being $E=\\overline{E}$ is equivalent to $E$ being closed.$(c)$ For all closed sets $F\\subset X$ satisfying $E\\subset F$, $\\overline{E} \\subset F$ holds.\n$(a)$ and $(c)$ imply that $\\overline{E}$ is the smallest closed subset of $X$ that contains $E$.\n","id":1700,"permalink":"https://freshrimpsushi.github.io/en/posts/1700/","tags":null,"title":"Neighborhood, Limit Point, Open, Closed in Metric Space"},{"categories":"해석개론","contents":"Summary1 This article is based on the Riemann-Stieltjes integral. If set to $\\alpha=\\alpha (x)=x$, it is the same as the Riemann integral. Let\u0026rsquo;s say $f$ is integrable by Riemann(-Stieltjes) from $[a,b]$. Then, for a constant $c\\in \\mathbb{R}$, $cf$ is also integrable from $[a,b]$, and its value is as follows. $$ \\int_{a}^{b}cf d\\alpha = c\\int_{a}^{b}f d\\alpha $$\nLet two functions $f_{1}$, $f_{2}$ be integrable by Riemann(-Stieltjes) from $[a,b]$. Then, $f_{1}+f_{2}$ is also integrable, and its value is as follows. $$ \\int _{a} ^{b}(f_{1}+f_{2})d\\alpha = \\int _{a} ^{b} f_{1}d\\alpha + \\int_{a}^{b} f_{2} d\\alpha $$\nIt means that integration is linear.\n$$ \\int _{a} ^{b}(f_{1}+cf_{2})d\\alpha = \\int _{a} ^{b} f_{1}d\\alpha + c\\int_{a}^{b} f_{2} d\\alpha $$\nThe reason for specifically mentioning sums and constant multiples separately is because they are proven separately.\nAuxiliary Theorem For a function $f$ that is integrable by Riemann(-Stieltjes) from $[a,b]$ and any positive number $\\varepsilon\u0026gt; 0$, there exists a partition $P$ of $[a,b]$ that satisfies the following equation.\n$$ \\begin{align} U(P,f,\\alpha) \\lt \\int_{a}^{b}f d\\alpha +\\varepsilon \\tag{L1} \\\\ \\int_{a}^{b}f d\\alpha -\\varepsilon \\lt L(P,f,\\alpha) \\tag{L2} \\end{align} $$\n$U$, $L$ are respectively the Riemann(-Stieltjes) upper sum and lower sum.\nProof $\\eqref{L1}$ Let\u0026rsquo;s say an arbitrary positive number $\\varepsilon \\gt 0$ is given. Then, by the necessary and sufficient condition for integrability, there exists a partition $P$ that satisfies the following equation.\n$$ U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\nSince $L(P,f,\\alpha) \\le \\displaystyle \\int_{a}^{b}fd\\alpha$, the following holds.\n$$ U(P,f,\\alpha)-\\int_{a}^{b}f d\\alpha\\le U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\nTherefore, the summary is as follows.\n$$ U(P,f,\\alpha ) \\lt \\int_{a}^{b}f d\\alpha +\\varepsilon $$\n■\n$\\eqref{L2}$ As in the proof of $\\eqref{L1}$, there exists a partition $P$ that satisfies the following.\n$$ U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\nSince $\\displaystyle \\int_{a}^{b}fd\\alpha \\le U(P,f,\\alpha)$, the following holds.\n$$ \\int_{a}^{b}f d\\alpha-L(P,f,\\alpha)\\le U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\nTherefore, the summary is as follows.\n$$ \\int_{a}^{b}f d\\alpha -\\varepsilon \\lt L(P,f,\\alpha) $$\n■\nProof When $f_{1}, f_{2}, f$ is integrable, it will be shown that $f_{1}+f_{2}, cf$ is also integrable and that its actual value is equal to $\\displaystyle \\int f_{1} + \\int f_{2}, c\\int f$.\n1. Case 1. $c=0$\nIt is obvious that $cf=0$ is integrable. It is also obvious that the following equation holds.\n$$ \\int_{a}^{b}0fd\\alpha=0=0\\int_{a}^{b}fd\\alpha $$\nCase 2. $c\u0026gt;0$\nLet\u0026rsquo;s say an arbitrary positive number $\\varepsilon \u0026gt;0$ is given. Then, by the necessary and sufficient condition for integrability, there exists a partition $P=\\left\\{ a=x_{0} \\lt \\cdots \\lt x_{i} \\lt \\cdots \\lt x_{n}=b\\right\\}$ that satisfies the following.\n$$ \\begin{equation} U(P,f,\\alpha) - L(P,f,\\alpha)\u0026lt;\\frac{\\varepsilon}{c} \\end{equation} $$\nLet\u0026rsquo;s set it as follows.\n$$ \\begin{align*} M_{i} \u0026amp;= \\sup _{[x_{i-1}, x_{i}]} f(x) \\\\ m_{i} \u0026amp;= \\inf _{[x_{i-1}, x_{i}]} f(x) \\\\ M_{i}^{c} \u0026amp;= \\sup _{[x_{i-1}, x_{i}]} cf(x) \\\\ m_{i}^{c} \u0026amp;= \\inf _{[x_{i-1}, x_{i}]} cf(x) \\end{align*} $$\nSince $c\u0026gt;0$, $cM_{i} = M_{i}^{c}$ holds, and $cm_{i} = m_{i}^{c}$. Then, by the definition of Riemann-(Stieltjes) sum and $(1)$, the following holds.\n$$ \\begin{align} U(P,cf,\\alpha)- L(P,cf,\\alpha) \u0026amp;= \\sum \\limits_{i=1}^{n}M_{i}^{c}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}m_{i}^{c}\\Delta \\alpha_{i} \\nonumber\\\\ \u0026amp;= \\sum \\limits_{i=1}^{n}cM_{i}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}cm_{i}\\Delta \\alpha_{i} \\nonumber\\\\ \u0026amp;= c\\left( \\sum \\limits_{i=1}^{n}M_{i}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}m_{i}\\Delta \\alpha_{i} \\right) \\nonumber\\\\ \u0026amp;= c\\Big[ U(P,f,\\alpha)-L(P,f,\\alpha)\\Big] \\nonumber\\\\ \u0026amp;\\lt \\varepsilon \\end{align} $$\nTherefore, by the necessary and sufficient condition for integrability, $cf$ is integrable. Since the integral is less than the upper sum, the following holds.\n$$ c \\int_{a}^{b}fd \\alpha \\le cU(P,f,\\alpha) = U(P,cf,\\alpha) $$\nThen, by $(2)$ and the Auxiliary Theorem, the following holds.\n$$ c\\int _{a}^{b}f d\\alpha \\le U(P,cf,\\alpha) lt \\int _{a}^{b} cf d\\alpha +\\varepsilon $$\nSince $\\varepsilon$ is assumed to be any positive number, the following holds.\n$$ \\begin{equation} c\\int_{a}^{b}fd\\alpha \\le \\int_{a}^{b}cfd\\alpha \\end{equation} $$\nThe process of proving the opposite inequality is similar. By $(1)$ and the Auxiliary Theorem, the following holds.\n$$ cU(P,f,\\alpha) \\le c\\int_{a}^{b}fd\\alpha +\\varepsilon $$\nAlso, the following equation holds.\n$$ \\int_{a}^{b} cfd\\alpha \\le U(P,cf,\\alpha)=cU(P,f,\\alpha) $$\nFrom the above two equations, the following equation is obtained.\n$$ \\int_{a}^{b} cfd \\alpha \\le cU(P,f,\\alpha)\u0026lt; c\\int_{a}^{b}fd\\alpha +\\varepsilon $$\nSince $\\varepsilon$ is any positive number, the following holds.\n$$ \\begin{equation} \\int_{a}^{b} cf d\\alpha \\le c\\int_{a}^{b}fd\\alpha \\end{equation} $$\nBy $(3)$ and $(4)$, the following holds.\n$$ \\int_{a}^{b}cfd\\alpha = c\\int_{a}^{b}fd\\alpha $$\nCase 3. $c=-1$\nThe proof process is similar to Case 2. First, let\u0026rsquo;s say an arbitrary positive number $\\varepsilon$ is given. Since $f$ is integrable, by the necessary and sufficient condition for integrability, there exists a partition $P$ for the given $\\varepsilon$ that satisfies the following.\n$$ U(P,f,\\alpha) - L(P,f,\\alpha) \u0026lt;\\varepsilon $$\nNow, let\u0026rsquo;s set it as follows.\n$$ \\begin{align*} M_{i} \u0026amp;= \\sup _{[x_{i-1},x_{i}]}f \\\\ m_{i} \u0026amp;= \\inf_{[x_{i-1},x_{i}]}f \\\\ M_{i}^{\\ast} \u0026amp;= \\sup _{[x_{i-1},x_{i}]}(-f) \\\\ m_{i}^{\\ast} \u0026amp;= \\inf_{[x_{i-1},x_{i}]}(-f) \\end{align*} $$\nSince $M_{i}=-m_{i}^{\\ast}$ and $m_{i}=-M_{i}^{\\ast}$, $M_{i}-m_{i}=M_{i}^{\\ast}-m_{i}^{\\ast}$ holds. Therefore, the following holds.\n$$ \\begin{align*} U(P,-f,\\alpha)-L(P,-f,\\alpha) \u0026amp;= \\sum\\limits_{i=1}^{n}M_{i}^{\\ast}\\Delta \\alpha_{i}-\\sum\\limits_{i=1}^{n}m_{i}^{\\ast}\\Delta \\alpha_{i} \\\\ \u0026amp;= \\sum\\limits_{i=1}^{n}M_{i}\\Delta \\alpha_{i} - \\sum\\limits_{i=1}^{n}m_{i}\\Delta\\alpha_{i} \\\\ \u0026amp;= U(P,f,\\alpha) -L(P,f,\\alpha) \\\\ \u0026amp;\\lt \\varepsilon \\end{align*} $$\nTherefore, $-f$ is integrable.\nAs in the proof of Case 2., by the Auxiliary Theorem, the following holds.\n$$ U(P,-f,\\alpha) \\lt \\int_{a}^{b}(-f)d\\alpha +\\varepsilon $$\nAlso, the following equation holds.\n$$ -\\int_{a}^{b}fd\\alpha\\le -L(P,f,\\alpha)=U(P,-f,\\alpha) \\lt \\int_{a}^{b}(-f)d\\alpha + \\varepsilon $$\nSince $\\varepsilon$ is any positive number, the following holds. $$ -\\int_{a}^{b}fd\\alpha \\le \\int_{a}^{b}(-f)d\\alpha $$\nThen, by the Auxiliary Theorem, the following equation holds.\n$$ \\int_{a}^{b}(-f)d\\alpha -\\varepsilon \\lt L(P,-f,\\alpha)=-U(P,f,\\alpha)\\le-\\int_{a}^{b}fd\\alpha $$\nSince $\\varepsilon$ is any positive number, the following holds.\n$$ \\int_{a}^{b}(-f)d\\alpha \\le -\\int_{a}^{b}fd\\alpha $$\nTherefore, the following is obtained.\n$$ \\int_{a}^{b}(-f)d\\alpha =-\\int_{a}^{b}fd\\alpha $$\nCase 4. $c \\lt 0 \\quad \\text{and} \\quad c\\ne -1$\nIt holds by Case 2. and Case 3.\n■\n2. Let\u0026rsquo;s say $f=f_{1}+f_{2}$. Let $P$ be any partition of $[a,b]$. Then, by the definition of Riemann(-Stieltjes) upper and lower sums, the following holds.\n$$ \\begin{equation} \\begin{aligned} L(P,f_{1},\\alpha) + L(P,f_{2},\\alpha)\u0026amp; \\le L(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f_{1},\\alpha) +U(P,f_{2},\\alpha) \\end{aligned} \\end{equation} $$\nLet\u0026rsquo;s say an arbitrary positive number $\\varepsilon \u0026gt; 0$ is given. Then, by the necessary and sufficient condition for integrability, there exists a partition $P_{j}$ that satisfies the following.\n$$ U(P_{j},f_{j},\\alpha)-L(P_{j},f_{j},\\alpha)\u0026lt;\\varepsilon,\\quad (j=1,2) $$\nNow, let\u0026rsquo;s consider $P$ again as a common refinement of $P_{1}$ and $P_{2}$. Then, by $(5)$, the following holds.\n$$ \\begin{align*} U(P,f,\\alpha)-L(P,f,\\alpha) \u0026amp;\\le \\left[ U(P,f_{1},\\alpha)-L(P,f_{1},\\alpha) \\right] + \\left[ U(P,f_{2},\\alpha)-L(P,f_{2},\\alpha) \\right] \\\\ \u0026amp;\u0026lt; \\varepsilon \\end{align*} $$\nTherefore, by the necessary and sufficient condition for integrability, $f$ is integrable. Then, by the Auxiliary Theorem, the following equation holds.\n$$ U(P,f_{j},\\alpha)\u0026lt;\\int _{a}^{b}f_{j}d\\alpha+\\varepsilon,\\quad (j=1,2) $$\nFurthermore, by definition, since the upper sum is greater than the integral, the following holds.\n$$ \\int_{a}^{b}fd\\alpha \\le U(P,f,\\alpha) $$\nFrom the above equation and the third inequality of $(5)$, the following holds.\n$$ \\begin{align*} \\int_{a}^{b}fd\\alpha \u0026amp;\\le U(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f_{1},\\alpha)+U(P,f_{2},\\alpha) \\\\ \u0026amp;\u0026lt; \\int_{a}^{b}f_{1}d\\alpha +\\int_{a}^{b}f_{2}d\\alpha + 2\\varepsilon \\end{align*} $$\nSince $\\varepsilon$ is any positive number, the following holds.\n$$ \\begin{equation} \\int_{a}^{b} fd\\alpha \\le \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha \\label{6} \\end{equation} $$\nProving the opposite direction of the inequality completes the proof. Since it\u0026rsquo;s already shown above that a constant multiple of an integrable function is also integrable, $-f_{1}, -f_{2}$ is known to be integrable. Therefore, repeating the above process for these two functions yields the following equation.\n$$ \\int_{a}^{b}(-f)d\\alpha \\le \\int_{a}^{b}(-f_{1})d\\alpha + \\int_{a}^{b} (-f_{2})d\\alpha $$\nFurthermore, since $\\displaystyle \\int (-f)d\\alpha=-\\int fd\\alpha$, by multiplying both sides by $-1$, the following is obtained.\n$$ \\begin{equation} \\int_{a}^{b}fd\\alpha \\ge \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha \\label{7} \\end{equation} $$\nTherefore, by $(6)$ and $(7)$, the following is obtained.\n$$ \\int_{a}^{b}fd\\alpha = \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha $$\n■\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), p128-129\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1666,"permalink":"https://freshrimpsushi.github.io/en/posts/1666/","tags":null,"title":"Linearity of Riemann(-Stieltjes) Iintegral"},{"categories":"줄리아","contents":"Julia\u0026rsquo;s latest version as of this post is v1.3.1.\nGuide Step 1. Download Julia Download the file that matches your CPU\u0026rsquo;s bit from Generic Linux Binaries for x86.\nStep 2. Unzip and Move Unzip it.\nMove the folder to where Julia is to be stored. This can be any location of your preference, but this post has moved it to /home/[username]/julia-1.3.1.\nStep 3. Symbolic Link Use the following command to create a symbolic link.\nsudo ln -s /home/[유저이름]/julia-1.3.1/bin/julia /usr/bin/julia By running Julia through the command, you can confirm that version 1.3.1 is installed properly.\nIf You Don\u0026rsquo;t Need the Latest Version sudo apt-get install julia Using the above command, you can quickly install without setting up a symbolic link. However, this method does not install the latest stable version.\nEnvironment OS: Ubuntu 18.04 ","id":1511,"permalink":"https://freshrimpsushi.github.io/en/posts/1511/","tags":null,"title":"Installing the Latest Version of Julia on Linux"},{"categories":"정수론","contents":"Prime numbers A list of primes up to the 10,000th.\nDownload\r2 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97 101 103 107 109 113 127 131 137 139 149 151 157 163 167 173 179 181 191 193 197 199 211 223 227 229 233 239 241 251 257 263 269 271 277 281 283 293 307 311 313 317 331 337 347 349 353 359 367 373 379 383 389 397 401 409 419 421 431 433 439 443 449 457 461 463 467 479 487 491 499 503 509 521 523 541 547 557 563 569 571 577 587 593 599 601 607 613 617 619 631 641 643 647 653 659 661 673 677 683 691 701 709 719 727 733 739 743 751 757 761 769 773 787 797 809 811 821 823 827 829 839 853 857 859 863 877 881 883 887 907 911 919 929 937 941 947 953 967 971 977 983 991 997 1009 1013 1019 1021 1031 1033 1039 1049 1051 1061 1063 1069 1087 1091 1093 1097 1103 1109 1117 1123 1129 1151 1153 1163 1171 1181 1187 1193 1201 1213 1217 1223 1229 1231 1237 1249 1259 1277 1279 1283 1289 1291 1297 1301 1303 1307 1319 1321 1327 1361 1367 1373 1381 1399 1409 1423 1427 1429 1433 1439 1447 1451 1453 1459 1471 1481 1483 1487 1489 1493 1499 1511 1523 1531 1543 1549 1553 1559 1567 1571 1579 1583 1597 1601 1607 1609 1613 1619 1621 1627 1637 1657 1663 1667 1669 1693 1697 1699 1709 1721 1723 1733 1741 1747 1753 1759 1777 1783 1787 1789 1801 1811 1823 1831 1847 1861 1867 1871 1873 1877 1879 1889 1901 1907 1913 1931 1933 1949 1951 1973 1979 1987 1993 1997 1999 2003 2011 2017 2027 2029 2039 2053 2063 2069 2081 2083 2087 2089 2099 2111 2113 2129 2131 2137 2141 2143 2153 2161 2179 2203 2207 2213 2221 2237 2239 2243 2251 2267 2269 2273 2281 2287 2293 2297 2309 2311 2333 2339 2341 2347 2351 2357 2371 2377 2381 2383 2389 2393 2399 2411 2417 2423 2437 2441 2447 2459 2467 2473 2477 2503 2521 2531 2539 2543 2549 2551 2557 2579 2591 2593 2609 2617 2621 2633 2647 2657 2659 2663 2671 2677 2683 2687 2689 2693 2699 2707 2711 2713 2719 2729 2731 2741 2749 2753 2767 2777 2789 2791 2797 2801 2803 2819 2833 2837 2843 2851 2857 2861 2879 2887 2897 2903 2909 2917 2927 2939 2953 2957 2963 2969 2971 2999 3001 3011 3019 3023 3037 3041 3049 3061 3067 3079 3083 3089 3109 3119 3121 3137 3163 3167 3169 3181 3187 3191 3203 3209 3217 3221 3229 3251 3253 3257 3259 3271 3299 3301 3307 3313 3319 3323 3329 3331 3343 3347 3359 3361 3371 3373 3389 3391 3407 3413 3433 3449 3457 3461 3463 3467 3469 3491 3499 3511 3517 3527 3529 3533 3539 3541 3547 3557 3559 3571 3581 3583 3593 3607 3613 3617 3623 3631 3637 3643 3659 3671 3673 3677 3691 3697 3701 3709 3719 3727 3733 3739 3761 3767 3769 3779 3793 3797 3803 3821 3823 3833 3847 3851 3853 3863 3877 3881 3889 3907 3911 3917 3919 3923 3929 3931 3943 3947 3967 3989 4001 4003 4007 4013 4019 4021 4027 4049 4051 4057 4073 4079 4091 4093 4099 4111 4127 4129 4133 4139 4153 4157 4159 4177 4201 4211 4217 4219 4229 4231 4241 4243 4253 4259 4261 4271 4273 4283 4289 4297 4327 4337 4339 4349 4357 4363 4373 4391 4397 4409 4421 4423 4441 4447 4451 4457 4463 4481 4483 4493 4507 4513 4517 4519 4523 4547 4549 4561 4567 4583 4591 4597 4603 4621 4637 4639 4643 4649 4651 4657 4663 4673 4679 4691 4703 4721 4723 4729 4733 4751 4759 4783 4787 4789 4793 4799 4801 4813 4817 4831 4861 4871 4877 4889 4903 4909 4919 4931 4933 4937 4943 4951 4957 4967 4969 4973 4987 4993 4999 5003 5009 5011 5021 5023 5039 5051 5059 5077 5081 5087 5099 5101 5107 5113 5119 5147 5153 5167 5171 5179 5189 5197 5209 5227 5231 5233 5237 5261 5273 5279 5281 5297 5303 5309 5323 5333 5347 5351 5381 5387 5393 5399 5407 5413 5417 5419 5431 5437 5441 5443 5449 5471 5477 5479 5483 5501 5503 5507 5519 5521 5527 5531 5557 5563 5569 5573 5581 5591 5623 5639 5641 5647 5651 5653 5657 5659 5669 5683 5689 5693 5701 5711 5717 5737 5741 5743 5749 5779 5783 5791 5801 5807 5813 5821 5827 5839 5843 5849 5851 5857 5861 5867 5869 5879 5881 5897 5903 5923 5927 5939 5953 5981 5987 6007 6011 6029 6037 6043 6047 6053 6067 6073 6079 6089 6091 6101 6113 6121 6131 6133 6143 6151 6163 6173 6197 6199 6203 6211 6217 6221 6229 6247 6257 6263 6269 6271 6277 6287 6299 6301 6311 6317 6323 6329 6337 6343 6353 6359 6361 6367 6373 6379 6389 6397 6421 6427 6449 6451 6469 6473 6481 6491 6521 6529 6547 6551 6553 6563 6569 6571 6577 6581 6599 6607 6619 6637 6653 6659 6661 6673 6679 6689 6691 6701 6703 6709 6719 6733 6737 6761 6763 6779 6781 6791 6793 6803 6823 6827 6829 6833 6841 6857 6863 6869 6871 6883 6899 6907 6911 6917 6947 6949 6959 6961 6967 6971 6977 6983 6991 6997 7001 7013 7019 7027 7039 7043 7057 7069 7079 7103 7109 7121 7127 7129 7151 7159 7177 7187 7193 7207 7211 7213 7219 7229 7237 7243 7247 7253 7283 7297 7307 7309 7321 7331 7333 7349 7351 7369 7393 7411 7417 7433 7451 7457 7459 7477 7481 7487 7489 7499 7507 7517 7523 7529 7537 7541 7547 7549 7559 7561 7573 7577 7583 7589 7591 7603 7607 7621 7639 7643 7649 7669 7673 7681 7687 7691 7699 7703 7717 7723 7727 7741 7753 7757 7759 7789 7793 7817 7823 7829 7841 7853 7867 7873 7877 7879 7883 7901 7907 7919 7927 7933 7937 7949 7951 7963 7993 8009 8011 8017 8039 8053 8059 8069 8081 8087 8089 8093 8101 8111 8117 8123 8147 8161 8167 8171 8179 8191 8209 8219 8221 8231 8233 8237 8243 8263 8269 8273 8287 8291 8293 8297 8311 8317 8329 8353 8363 8369 8377 8387 8389 8419 8423 8429 8431 8443 8447 8461 8467 8501 8513 8521 8527 8537 8539 8543 8563 8573 8581 8597 8599 8609 8623 8627 8629 8641 8647 8663 8669 8677 8681 8689 8693 8699 8707 8713 8719 8731 8737 8741 8747 8753 8761 8779 8783 8803 8807 8819 8821 8831 8837 8839 8849 8861 8863 8867 8887 8893 8923 8929 8933 8941 8951 8963 8969 8971 8999 9001 9007 9011 9013 9029 9041 9043 9049 9059 9067 9091 9103 9109 9127 9133 9137 9151 9157 9161 9173 9181 9187 9199 9203 9209 9221 9227 9239 9241 9257 9277 9281 9283 9293 9311 9319 9323 9337 9341 9343 9349 9371 9377 9391 9397 9403 9413 9419 9421 9431 9433 9437 9439 9461 9463 9467 9473 9479 9491 9497 9511 9521 9533 9539 9547 9551 9587 9601 9613 9619 9623 9629 9631 9643 9649 9661 9677 9679 9689 9697 9719 9721 9733 9739 9743 9749 9767 9769 9781 9787 9791 9803 9811 9817 9829 9833 9839 9851 9857 9859 9871 9883 9887 9901 9907 9923 9929 9931 9941 9949 9967 9973 10007 10009 10037 10039 10061 10067 10069 10079 10091 10093 10099 10103 10111 10133 10139 10141 10151 10159 10163 10169 10177 10181 10193 10211 10223 10243 10247 10253 10259 10267 10271 10273 10289 10301 10303 10313 10321 10331 10333 10337 10343 10357 10369 10391 10399 10427 10429 10433 10453 10457 10459 10463 10477 10487 10499 10501 10513 10529 10531 10559 10567 10589 10597 10601 10607 10613 10627 10631 10639 10651 10657 10663 10667 10687 10691 10709 10711 10723 10729 10733 10739 10753 10771 10781 10789 10799 10831 10837 10847 10853 10859 10861 10867 10883 10889 10891 10903 10909 10937 10939 10949 10957 10973 10979 10987 10993 11003 11027 11047 11057 11059 11069 11071 11083 11087 11093 11113 11117 11119 11131 11149 11159 11161 11171 11173 11177 11197 11213 11239 11243 11251 11257 11261 11273 11279 11287 11299 11311 11317 11321 11329 11351 11353 11369 11383 11393 11399 11411 11423 11437 11443 11447 11467 11471 11483 11489 11491 11497 11503 11519 11527 11549 11551 11579 11587 11593 11597 11617 11621 11633 11657 11677 11681 11689 11699 11701 11717 11719 11731 11743 11777 11779 11783 11789 11801 11807 11813 11821 11827 11831 11833 11839 11863 11867 11887 11897 11903 11909 11923 11927 11933 11939 11941 11953 11959 11969 11971 11981 11987 12007 12011 12037 12041 12043 12049 12071 12073 12097 12101 12107 12109 12113 12119 12143 12149 12157 12161 12163 12197 12203 12211 12227 12239 12241 12251 12253 12263 12269 12277 12281 12289 12301 12323 12329 12343 12347 12373 12377 12379 12391 12401 12409 12413 12421 12433 12437 12451 12457 12473 12479 12487 12491 12497 12503 12511 12517 12527 12539 12541 12547 12553 12569 12577 12583 12589 12601 12611 12613 12619 12637 12641 12647 12653 12659 12671 12689 12697 12703 12713 12721 12739 12743 12757 12763 12781 12791 12799 12809 12821 12823 12829 12841 12853 12889 12893 12899 12907 12911 12917 12919 12923 12941 12953 12959 12967 12973 12979 12983 13001 13003 13007 13009 13033 13037 13043 13049 13063 13093 13099 13103 13109 13121 13127 13147 13151 13159 13163 13171 13177 13183 13187 13217 13219 13229 13241 13249 13259 13267 13291 13297 13309 13313 13327 13331 13337 13339 13367 13381 13397 13399 13411 13417 13421 13441 13451 13457 13463 13469 13477 13487 13499 13513 13523 13537 13553 13567 13577 13591 13597 13613 13619 13627 13633 13649 13669 13679 13681 13687 13691 13693 13697 13709 13711 13721 13723 13729 13751 13757 13759 13763 13781 13789 13799 13807 13829 13831 13841 13859 13873 13877 13879 13883 13901 13903 13907 13913 13921 13931 13933 13963 13967 13997 13999 14009 14011 14029 14033 14051 14057 14071 14081 14083 14087 14107 14143 14149 14153 14159 14173 14177 14197 14207 14221 14243 14249 14251 14281 14293 14303 14321 14323 14327 14341 14347 14369 14387 14389 14401 14407 14411 14419 14423 14431 14437 14447 14449 14461 14479 14489 14503 14519 14533 14537 14543 14549 14551 14557 14561 14563 14591 14593 14621 14627 14629 14633 14639 14653 14657 14669 14683 14699 14713 14717 14723 14731 14737 14741 14747 14753 14759 14767 14771 14779 14783 14797 14813 14821 14827 14831 14843 14851 14867 14869 14879 14887 14891 14897 14923 14929 14939 14947 14951 14957 14969 14983 15013 15017 15031 15053 15061 15073 15077 15083 15091 15101 15107 15121 15131 15137 15139 15149 15161 15173 15187 15193 15199 15217 15227 15233 15241 15259 15263 15269 15271 15277 15287 15289 15299 15307 15313 15319 15329 15331 15349 15359 15361 15373 15377 15383 15391 15401 15413 15427 15439 15443 15451 15461 15467 15473 15493 15497 15511 15527 15541 15551 15559 15569 15581 15583 15601 15607 15619 15629 15641 15643 15647 15649 15661 15667 15671 15679 15683 15727 15731 15733 15737 15739 15749 15761 15767 15773 15787 15791 15797 15803 15809 15817 15823 15859 15877 15881 15887 15889 15901 15907 15913 15919 15923 15937 15959 15971 15973 15991 16001 16007 16033 16057 16061 16063 16067 16069 16073 16087 16091 16097 16103 16111 16127 16139 16141 16183 16187 16189 16193 16217 16223 16229 16231 16249 16253 16267 16273 16301 16319 16333 16339 16349 16361 16363 16369 16381 16411 16417 16421 16427 16433 16447 16451 16453 16477 16481 16487 16493 16519 16529 16547 16553 16561 16567 16573 16603 16607 16619 16631 16633 16649 16651 16657 16661 16673 16691 16693 16699 16703 16729 16741 16747 16759 16763 16787 16811 16823 16829 16831 16843 16871 16879 16883 16889 16901 16903 16921 16927 16931 16937 16943 16963 16979 16981 16987 16993 17011 17021 17027 17029 17033 17041 17047 17053 17077 17093 17099 17107 17117 17123 17137 17159 17167 17183 17189 17191 17203 17207 17209 17231 17239 17257 17291 17293 17299 17317 17321 17327 17333 17341 17351 17359 17377 17383 17387 17389 17393 17401 17417 17419 17431 17443 17449 17467 17471 17477 17483 17489 17491 17497 17509 17519 17539 17551 17569 17573 17579 17581 17597 17599 17609 17623 17627 17657 17659 17669 17681 17683 17707 17713 17729 17737 17747 17749 17761 17783 17789 17791 17807 17827 17837 17839 17851 17863 17881 17891 17903 17909 17911 17921 17923 17929 17939 17957 17959 17971 17977 17981 17987 17989 18013 18041 18043 18047 18049 18059 18061 18077 18089 18097 18119 18121 18127 18131 18133 18143 18149 18169 18181 18191 18199 18211 18217 18223 18229 18233 18251 18253 18257 18269 18287 18289 18301 18307 18311 18313 18329 18341 18353 18367 18371 18379 18397 18401 18413 18427 18433 18439 18443 18451 18457 18461 18481 18493 18503 18517 18521 18523 18539 18541 18553 18583 18587 18593 18617 18637 18661 18671 18679 18691 18701 18713 18719 18731 18743 18749 18757 18773 18787 18793 18797 18803 18839 18859 18869 18899 18911 18913 18917 18919 18947 18959 18973 18979 19001 19009 19013 19031 19037 19051 19069 19073 19079 19081 19087 19121 19139 19141 19157 19163 19181 19183 19207 19211 19213 19219 19231 19237 19249 19259 19267 19273 19289 19301 19309 19319 19333 19373 19379 19381 19387 19391 19403 19417 19421 19423 19427 19429 19433 19441 19447 19457 19463 19469 19471 19477 19483 19489 19501 19507 19531 19541 19543 19553 19559 19571 19577 19583 19597 19603 19609 19661 19681 19687 19697 19699 19709 19717 19727 19739 19751 19753 19759 19763 19777 19793 19801 19813 19819 19841 19843 19853 19861 19867 19889 19891 19913 19919 19927 19937 19949 19961 19963 19973 19979 19991 19993 19997 20011 20021 20023 20029 20047 20051 20063 20071 20089 20101 20107 20113 20117 20123 20129 20143 20147 20149 20161 20173 20177 20183 20201 20219 20231 20233 20249 20261 20269 20287 20297 20323 20327 20333 20341 20347 20353 20357 20359 20369 20389 20393 20399 20407 20411 20431 20441 20443 20477 20479 20483 20507 20509 20521 20533 20543 20549 20551 20563 20593 20599 20611 20627 20639 20641 20663 20681 20693 20707 20717 20719 20731 20743 20747 20749 20753 20759 20771 20773 20789 20807 20809 20849 20857 20873 20879 20887 20897 20899 20903 20921 20929 20939 20947 20959 20963 20981 20983 21001 21011 21013 21017 21019 21023 21031 21059 21061 21067 21089 21101 21107 21121 21139 21143 21149 21157 21163 21169 21179 21187 21191 21193 21211 21221 21227 21247 21269 21277 21283 21313 21317 21319 21323 21341 21347 21377 21379 21383 21391 21397 21401 21407 21419 21433 21467 21481 21487 21491 21493 21499 21503 21517 21521 21523 21529 21557 21559 21563 21569 21577 21587 21589 21599 21601 21611 21613 21617 21647 21649 21661 21673 21683 21701 21713 21727 21737 21739 21751 21757 21767 21773 21787 21799 21803 21817 21821 21839 21841 21851 21859 21863 21871 21881 21893 21911 21929 21937 21943 21961 21977 21991 21997 22003 22013 22027 22031 22037 22039 22051 22063 22067 22073 22079 22091 22093 22109 22111 22123 22129 22133 22147 22153 22157 22159 22171 22189 22193 22229 22247 22259 22271 22273 22277 22279 22283 22291 22303 22307 22343 22349 22367 22369 22381 22391 22397 22409 22433 22441 22447 22453 22469 22481 22483 22501 22511 22531 22541 22543 22549 22567 22571 22573 22613 22619 22621 22637 22639 22643 22651 22669 22679 22691 22697 22699 22709 22717 22721 22727 22739 22741 22751 22769 22777 22783 22787 22807 22811 22817 22853 22859 22861 22871 22877 22901 22907 22921 22937 22943 22961 22963 22973 22993 23003 23011 23017 23021 23027 23029 23039 23041 23053 23057 23059 23063 23071 23081 23087 23099 23117 23131 23143 23159 23167 23173 23189 23197 23201 23203 23209 23227 23251 23269 23279 23291 23293 23297 23311 23321 23327 23333 23339 23357 23369 23371 23399 23417 23431 23447 23459 23473 23497 23509 23531 23537 23539 23549 23557 23561 23563 23567 23581 23593 23599 23603 23609 23623 23627 23629 23633 23663 23669 23671 23677 23687 23689 23719 23741 23743 23747 23753 23761 23767 23773 23789 23801 23813 23819 23827 23831 23833 23857 23869 23873 23879 23887 23893 23899 23909 23911 23917 23929 23957 23971 23977 23981 23993 24001 24007 24019 24023 24029 24043 24049 24061 24071 24077 24083 24091 24097 24103 24107 24109 24113 24121 24133 24137 24151 24169 24179 24181 24197 24203 24223 24229 24239 24247 24251 24281 24317 24329 24337 24359 24371 24373 24379 24391 24407 24413 24419 24421 24439 24443 24469 24473 24481 24499 24509 24517 24527 24533 24547 24551 24571 24593 24611 24623 24631 24659 24671 24677 24683 24691 24697 24709 24733 24749 24763 24767 24781 24793 24799 24809 24821 24841 24847 24851 24859 24877 24889 24907 24917 24919 24923 24943 24953 24967 24971 24977 24979 24989 25013 25031 25033 25037 25057 25073 25087 25097 25111 25117 25121 25127 25147 25153 25163 25169 25171 25183 25189 25219 25229 25237 25243 25247 25253 25261 25301 25303 25307 25309 25321 25339 25343 25349 25357 25367 25373 25391 25409 25411 25423 25439 25447 25453 25457 25463 25469 25471 25523 25537 25541 25561 25577 25579 25583 25589 25601 25603 25609 25621 25633 25639 25643 25657 25667 25673 25679 25693 25703 25717 25733 25741 25747 25759 25763 25771 25793 25799 25801 25819 25841 25847 25849 25867 25873 25889 25903 25913 25919 25931 25933 25939 25943 25951 25969 25981 25997 25999 26003 26017 26021 26029 26041 26053 26083 26099 26107 26111 26113 26119 26141 26153 26161 26171 26177 26183 26189 26203 26209 26227 26237 26249 26251 26261 26263 26267 26293 26297 26309 26317 26321 26339 26347 26357 26371 26387 26393 26399 26407 26417 26423 26431 26437 26449 26459 26479 26489 26497 26501 26513 26539 26557 26561 26573 26591 26597 26627 26633 26641 26647 26669 26681 26683 26687 26693 26699 26701 26711 26713 26717 26723 26729 26731 26737 26759 26777 26783 26801 26813 26821 26833 26839 26849 26861 26863 26879 26881 26891 26893 26903 26921 26927 26947 26951 26953 26959 26981 26987 26993 27011 27017 27031 27043 27059 27061 27067 27073 27077 27091 27103 27107 27109 27127 27143 27179 27191 27197 27211 27239 27241 27253 27259 27271 27277 27281 27283 27299 27329 27337 27361 27367 27397 27407 27409 27427 27431 27437 27449 27457 27479 27481 27487 27509 27527 27529 27539 27541 27551 27581 27583 27611 27617 27631 27647 27653 27673 27689 27691 27697 27701 27733 27737 27739 27743 27749 27751 27763 27767 27773 27779 27791 27793 27799 27803 27809 27817 27823 27827 27847 27851 27883 27893 27901 27917 27919 27941 27943 27947 27953 27961 27967 27983 27997 28001 28019 28027 28031 28051 28057 28069 28081 28087 28097 28099 28109 28111 28123 28151 28163 28181 28183 28201 28211 28219 28229 28277 28279 28283 28289 28297 28307 28309 28319 28349 28351 28387 28393 28403 28409 28411 28429 28433 28439 28447 28463 28477 28493 28499 28513 28517 28537 28541 28547 28549 28559 28571 28573 28579 28591 28597 28603 28607 28619 28621 28627 28631 28643 28649 28657 28661 28663 28669 28687 28697 28703 28711 28723 28729 28751 28753 28759 28771 28789 28793 28807 28813 28817 28837 28843 28859 28867 28871 28879 28901 28909 28921 28927 28933 28949 28961 28979 29009 29017 29021 29023 29027 29033 29059 29063 29077 29101 29123 29129 29131 29137 29147 29153 29167 29173 29179 29191 29201 29207 29209 29221 29231 29243 29251 29269 29287 29297 29303 29311 29327 29333 29339 29347 29363 29383 29387 29389 29399 29401 29411 29423 29429 29437 29443 29453 29473 29483 29501 29527 29531 29537 29567 29569 29573 29581 29587 29599 29611 29629 29633 29641 29663 29669 29671 29683 29717 29723 29741 29753 29759 29761 29789 29803 29819 29833 29837 29851 29863 29867 29873 29879 29881 29917 29921 29927 29947 29959 29983 29989 30011 30013 30029 30047 30059 30071 30089 30091 30097 30103 30109 30113 30119 30133 30137 30139 30161 30169 30181 30187 30197 30203 30211 30223 30241 30253 30259 30269 30271 30293 30307 30313 30319 30323 30341 30347 30367 30389 30391 30403 30427 30431 30449 30467 30469 30491 30493 30497 30509 30517 30529 30539 30553 30557 30559 30577 30593 30631 30637 30643 30649 30661 30671 30677 30689 30697 30703 30707 30713 30727 30757 30763 30773 30781 30803 30809 30817 30829 30839 30841 30851 30853 30859 30869 30871 30881 30893 30911 30931 30937 30941 30949 30971 30977 30983 31013 31019 31033 31039 31051 31063 31069 31079 31081 31091 31121 31123 31139 31147 31151 31153 31159 31177 31181 31183 31189 31193 31219 31223 31231 31237 31247 31249 31253 31259 31267 31271 31277 31307 31319 31321 31327 31333 31337 31357 31379 31387 31391 31393 31397 31469 31477 31481 31489 31511 31513 31517 31531 31541 31543 31547 31567 31573 31583 31601 31607 31627 31643 31649 31657 31663 31667 31687 31699 31721 31723 31727 31729 31741 31751 31769 31771 31793 31799 31817 31847 31849 31859 31873 31883 31891 31907 31957 31963 31973 31981 31991 32003 32009 32027 32029 32051 32057 32059 32063 32069 32077 32083 32089 32099 32117 32119 32141 32143 32159 32173 32183 32189 32191 32203 32213 32233 32237 32251 32257 32261 32297 32299 32303 32309 32321 32323 32327 32341 32353 32359 32363 32369 32371 32377 32381 32401 32411 32413 32423 32429 32441 32443 32467 32479 32491 32497 32503 32507 32531 32533 32537 32561 32563 32569 32573 32579 32587 32603 32609 32611 32621 32633 32647 32653 32687 32693 32707 32713 32717 32719 32749 32771 32779 32783 32789 32797 32801 32803 32831 32833 32839 32843 32869 32887 32909 32911 32917 32933 32939 32941 32957 32969 32971 32983 32987 32993 32999 33013 33023 33029 33037 33049 33053 33071 33073 33083 33091 33107 33113 33119 33149 33151 33161 33179 33181 33191 33199 33203 33211 33223 33247 33287 33289 33301 33311 33317 33329 33331 33343 33347 33349 33353 33359 33377 33391 33403 33409 33413 33427 33457 33461 33469 33479 33487 33493 33503 33521 33529 33533 33547 33563 33569 33577 33581 33587 33589 33599 33601 33613 33617 33619 33623 33629 33637 33641 33647 33679 33703 33713 33721 33739 33749 33751 33757 33767 33769 33773 33791 33797 33809 33811 33827 33829 33851 33857 33863 33871 33889 33893 33911 33923 33931 33937 33941 33961 33967 33997 34019 34031 34033 34039 34057 34061 34123 34127 34129 34141 34147 34157 34159 34171 34183 34211 34213 34217 34231 34253 34259 34261 34267 34273 34283 34297 34301 34303 34313 34319 34327 34337 34351 34361 34367 34369 34381 34403 34421 34429 34439 34457 34469 34471 34483 34487 34499 34501 34511 34513 34519 34537 34543 34549 34583 34589 34591 34603 34607 34613 34631 34649 34651 34667 34673 34679 34687 34693 34703 34721 34729 34739 34747 34757 34759 34763 34781 34807 34819 34841 34843 34847 34849 34871 34877 34883 34897 34913 34919 34939 34949 34961 34963 34981 35023 35027 35051 35053 35059 35069 35081 35083 35089 35099 35107 35111 35117 35129 35141 35149 35153 35159 35171 35201 35221 35227 35251 35257 35267 35279 35281 35291 35311 35317 35323 35327 35339 35353 35363 35381 35393 35401 35407 35419 35423 35437 35447 35449 35461 35491 35507 35509 35521 35527 35531 35533 35537 35543 35569 35573 35591 35593 35597 35603 35617 35671 35677 35729 35731 35747 35753 35759 35771 35797 35801 35803 35809 35831 35837 35839 35851 35863 35869 35879 35897 35899 35911 35923 35933 35951 35963 35969 35977 35983 35993 35999 36007 36011 36013 36017 36037 36061 36067 36073 36083 36097 36107 36109 36131 36137 36151 36161 36187 36191 36209 36217 36229 36241 36251 36263 36269 36277 36293 36299 36307 36313 36319 36341 36343 36353 36373 36383 36389 36433 36451 36457 36467 36469 36473 36479 36493 36497 36523 36527 36529 36541 36551 36559 36563 36571 36583 36587 36599 36607 36629 36637 36643 36653 36671 36677 36683 36691 36697 36709 36713 36721 36739 36749 36761 36767 36779 36781 36787 36791 36793 36809 36821 36833 36847 36857 36871 36877 36887 36899 36901 36913 36919 36923 36929 36931 36943 36947 36973 36979 36997 37003 37013 37019 37021 37039 37049 37057 37061 37087 37097 37117 37123 37139 37159 37171 37181 37189 37199 37201 37217 37223 37243 37253 37273 37277 37307 37309 37313 37321 37337 37339 37357 37361 37363 37369 37379 37397 37409 37423 37441 37447 37463 37483 37489 37493 37501 37507 37511 37517 37529 37537 37547 37549 37561 37567 37571 37573 37579 37589 37591 37607 37619 37633 37643 37649 37657 37663 37691 37693 37699 37717 37747 37781 37783 37799 37811 37813 37831 37847 37853 37861 37871 37879 37889 37897 37907 37951 37957 37963 37967 37987 37991 37993 37997 38011 38039 38047 38053 38069 38083 38113 38119 38149 38153 38167 38177 38183 38189 38197 38201 38219 38231 38237 38239 38261 38273 38281 38287 38299 38303 38317 38321 38327 38329 38333 38351 38371 38377 38393 38431 38447 38449 38453 38459 38461 38501 38543 38557 38561 38567 38569 38593 38603 38609 38611 38629 38639 38651 38653 38669 38671 38677 38693 38699 38707 38711 38713 38723 38729 38737 38747 38749 38767 38783 38791 38803 38821 38833 38839 38851 38861 38867 38873 38891 38903 38917 38921 38923 38933 38953 38959 38971 38977 38993 39019 39023 39041 39043 39047 39079 39089 39097 39103 39107 39113 39119 39133 39139 39157 39161 39163 39181 39191 39199 39209 39217 39227 39229 39233 39239 39241 39251 39293 39301 39313 39317 39323 39341 39343 39359 39367 39371 39373 39383 39397 39409 39419 39439 39443 39451 39461 39499 39503 39509 39511 39521 39541 39551 39563 39569 39581 39607 39619 39623 39631 39659 39667 39671 39679 39703 39709 39719 39727 39733 39749 39761 39769 39779 39791 39799 39821 39827 39829 39839 39841 39847 39857 39863 39869 39877 39883 39887 39901 39929 39937 39953 39971 39979 39983 39989 40009 40013 40031 40037 40039 40063 40087 40093 40099 40111 40123 40127 40129 40151 40153 40163 40169 40177 40189 40193 40213 40231 40237 40241 40253 40277 40283 40289 40343 40351 40357 40361 40387 40423 40427 40429 40433 40459 40471 40483 40487 40493 40499 40507 40519 40529 40531 40543 40559 40577 40583 40591 40597 40609 40627 40637 40639 40693 40697 40699 40709 40739 40751 40759 40763 40771 40787 40801 40813 40819 40823 40829 40841 40847 40849 40853 40867 40879 40883 40897 40903 40927 40933 40939 40949 40961 40973 40993 41011 41017 41023 41039 41047 41051 41057 41077 41081 41113 41117 41131 41141 41143 41149 41161 41177 41179 41183 41189 41201 41203 41213 41221 41227 41231 41233 41243 41257 41263 41269 41281 41299 41333 41341 41351 41357 41381 41387 41389 41399 41411 41413 41443 41453 41467 41479 41491 41507 41513 41519 41521 41539 41543 41549 41579 41593 41597 41603 41609 41611 41617 41621 41627 41641 41647 41651 41659 41669 41681 41687 41719 41729 41737 41759 41761 41771 41777 41801 41809 41813 41843 41849 41851 41863 41879 41887 41893 41897 41903 41911 41927 41941 41947 41953 41957 41959 41969 41981 41983 41999 42013 42017 42019 42023 42043 42061 42071 42073 42083 42089 42101 42131 42139 42157 42169 42179 42181 42187 42193 42197 42209 42221 42223 42227 42239 42257 42281 42283 42293 42299 42307 42323 42331 42337 42349 42359 42373 42379 42391 42397 42403 42407 42409 42433 42437 42443 42451 42457 42461 42463 42467 42473 42487 42491 42499 42509 42533 42557 42569 42571 42577 42589 42611 42641 42643 42649 42667 42677 42683 42689 42697 42701 42703 42709 42719 42727 42737 42743 42751 42767 42773 42787 42793 42797 42821 42829 42839 42841 42853 42859 42863 42899 42901 42923 42929 42937 42943 42953 42961 42967 42979 42989 43003 43013 43019 43037 43049 43051 43063 43067 43093 43103 43117 43133 43151 43159 43177 43189 43201 43207 43223 43237 43261 43271 43283 43291 43313 43319 43321 43331 43391 43397 43399 43403 43411 43427 43441 43451 43457 43481 43487 43499 43517 43541 43543 43573 43577 43579 43591 43597 43607 43609 43613 43627 43633 43649 43651 43661 43669 43691 43711 43717 43721 43753 43759 43777 43781 43783 43787 43789 43793 43801 43853 43867 43889 43891 43913 43933 43943 43951 43961 43963 43969 43973 43987 43991 43997 44017 44021 44027 44029 44041 44053 44059 44071 44087 44089 44101 44111 44119 44123 44129 44131 44159 44171 44179 44189 44201 44203 44207 44221 44249 44257 44263 44267 44269 44273 44279 44281 44293 44351 44357 44371 44381 44383 44389 44417 44449 44453 44483 44491 44497 44501 44507 44519 44531 44533 44537 44543 44549 44563 44579 44587 44617 44621 44623 44633 44641 44647 44651 44657 44683 44687 44699 44701 44711 44729 44741 44753 44771 44773 44777 44789 44797 44809 44819 44839 44843 44851 44867 44879 44887 44893 44909 44917 44927 44939 44953 44959 44963 44971 44983 44987 45007 45013 45053 45061 45077 45083 45119 45121 45127 45131 45137 45139 45161 45179 45181 45191 45197 45233 45247 45259 45263 45281 45289 45293 45307 45317 45319 45329 45337 45341 45343 45361 45377 45389 45403 45413 45427 45433 45439 45481 45491 45497 45503 45523 45533 45541 45553 45557 45569 45587 45589 45599 45613 45631 45641 45659 45667 45673 45677 45691 45697 45707 45737 45751 45757 45763 45767 45779 45817 45821 45823 45827 45833 45841 45853 45863 45869 45887 45893 45943 45949 45953 45959 45971 45979 45989 46021 46027 46049 46051 46061 46073 46091 46093 46099 46103 46133 46141 46147 46153 46171 46181 46183 46187 46199 46219 46229 46237 46261 46271 46273 46279 46301 46307 46309 46327 46337 46349 46351 46381 46399 46411 46439 46441 46447 46451 46457 46471 46477 46489 46499 46507 46511 46523 46549 46559 46567 46573 46589 46591 46601 46619 46633 46639 46643 46649 46663 46679 46681 46687 46691 46703 46723 46727 46747 46751 46757 46769 46771 46807 46811 46817 46819 46829 46831 46853 46861 46867 46877 46889 46901 46919 46933 46957 46993 46997 47017 47041 47051 47057 47059 47087 47093 47111 47119 47123 47129 47137 47143 47147 47149 47161 47189 47207 47221 47237 47251 47269 47279 47287 47293 47297 47303 47309 47317 47339 47351 47353 47363 47381 47387 47389 47407 47417 47419 47431 47441 47459 47491 47497 47501 47507 47513 47521 47527 47533 47543 47563 47569 47581 47591 47599 47609 47623 47629 47639 47653 47657 47659 47681 47699 47701 47711 47713 47717 47737 47741 47743 47777 47779 47791 47797 47807 47809 47819 47837 47843 47857 47869 47881 47903 47911 47917 47933 47939 47947 47951 47963 47969 47977 47981 48017 48023 48029 48049 48073 48079 48091 48109 48119 48121 48131 48157 48163 48179 48187 48193 48197 48221 48239 48247 48259 48271 48281 48299 48311 48313 48337 48341 48353 48371 48383 48397 48407 48409 48413 48437 48449 48463 48473 48479 48481 48487 48491 48497 48523 48527 48533 48539 48541 48563 48571 48589 48593 48611 48619 48623 48647 48649 48661 48673 48677 48679 48731 48733 48751 48757 48761 48767 48779 48781 48787 48799 48809 48817 48821 48823 48847 48857 48859 48869 48871 48883 48889 48907 48947 48953 48973 48989 48991 49003 49009 49019 49031 49033 49037 49043 49057 49069 49081 49103 49109 49117 49121 49123 49139 49157 49169 49171 49177 49193 49199 49201 49207 49211 49223 49253 49261 49277 49279 49297 49307 49331 49333 49339 49363 49367 49369 49391 49393 49409 49411 49417 49429 49433 49451 49459 49463 49477 49481 49499 49523 49529 49531 49537 49547 49549 49559 49597 49603 49613 49627 49633 49639 49663 49667 49669 49681 49697 49711 49727 49739 49741 49747 49757 49783 49787 49789 49801 49807 49811 49823 49831 49843 49853 49871 49877 49891 49919 49921 49927 49937 49939 49943 49957 49991 49993 49999 50021 50023 50033 50047 50051 50053 50069 50077 50087 50093 50101 50111 50119 50123 50129 50131 50147 50153 50159 50177 50207 50221 50227 50231 50261 50263 50273 50287 50291 50311 50321 50329 50333 50341 50359 50363 50377 50383 50387 50411 50417 50423 50441 50459 50461 50497 50503 50513 50527 50539 50543 50549 50551 50581 50587 50591 50593 50599 50627 50647 50651 50671 50683 50707 50723 50741 50753 50767 50773 50777 50789 50821 50833 50839 50849 50857 50867 50873 50891 50893 50909 50923 50929 50951 50957 50969 50971 50989 50993 51001 51031 51043 51047 51059 51061 51071 51109 51131 51133 51137 51151 51157 51169 51193 51197 51199 51203 51217 51229 51239 51241 51257 51263 51283 51287 51307 51329 51341 51343 51347 51349 51361 51383 51407 51413 51419 51421 51427 51431 51437 51439 51449 51461 51473 51479 51481 51487 51503 51511 51517 51521 51539 51551 51563 51577 51581 51593 51599 51607 51613 51631 51637 51647 51659 51673 51679 51683 51691 51713 51719 51721 51749 51767 51769 51787 51797 51803 51817 51827 51829 51839 51853 51859 51869 51871 51893 51899 51907 51913 51929 51941 51949 51971 51973 51977 51991 52009 52021 52027 52051 52057 52067 52069 52081 52103 52121 52127 52147 52153 52163 52177 52181 52183 52189 52201 52223 52237 52249 52253 52259 52267 52289 52291 52301 52313 52321 52361 52363 52369 52379 52387 52391 52433 52453 52457 52489 52501 52511 52517 52529 52541 52543 52553 52561 52567 52571 52579 52583 52609 52627 52631 52639 52667 52673 52691 52697 52709 52711 52721 52727 52733 52747 52757 52769 52783 52807 52813 52817 52837 52859 52861 52879 52883 52889 52901 52903 52919 52937 52951 52957 52963 52967 52973 52981 52999 53003 53017 53047 53051 53069 53077 53087 53089 53093 53101 53113 53117 53129 53147 53149 53161 53171 53173 53189 53197 53201 53231 53233 53239 53267 53269 53279 53281 53299 53309 53323 53327 53353 53359 53377 53381 53401 53407 53411 53419 53437 53441 53453 53479 53503 53507 53527 53549 53551 53569 53591 53593 53597 53609 53611 53617 53623 53629 53633 53639 53653 53657 53681 53693 53699 53717 53719 53731 53759 53773 53777 53783 53791 53813 53819 53831 53849 53857 53861 53881 53887 53891 53897 53899 53917 53923 53927 53939 53951 53959 53987 53993 54001 54011 54013 54037 54049 54059 54083 54091 54101 54121 54133 54139 54151 54163 54167 54181 54193 54217 54251 54269 54277 54287 54293 54311 54319 54323 54331 54347 54361 54367 54371 54377 54401 54403 54409 54413 54419 54421 54437 54443 54449 54469 54493 54497 54499 54503 54517 54521 54539 54541 54547 54559 54563 54577 54581 54583 54601 54617 54623 54629 54631 54647 54667 54673 54679 54709 54713 54721 54727 54751 54767 54773 54779 54787 54799 54829 54833 54851 54869 54877 54881 54907 54917 54919 54941 54949 54959 54973 54979 54983 55001 55009 55021 55049 55051 55057 55061 55073 55079 55103 55109 55117 55127 55147 55163 55171 55201 55207 55213 55217 55219 55229 55243 55249 55259 55291 55313 55331 55333 55337 55339 55343 55351 55373 55381 55399 55411 55439 55441 55457 55469 55487 55501 55511 55529 55541 55547 55579 55589 55603 55609 55619 55621 55631 55633 55639 55661 55663 55667 55673 55681 55691 55697 55711 55717 55721 55733 55763 55787 55793 55799 55807 55813 55817 55819 55823 55829 55837 55843 55849 55871 55889 55897 55901 55903 55921 55927 55931 55933 55949 55967 55987 55997 56003 56009 56039 56041 56053 56081 56087 56093 56099 56101 56113 56123 56131 56149 56167 56171 56179 56197 56207 56209 56237 56239 56249 56263 56267 56269 56299 56311 56333 56359 56369 56377 56383 56393 56401 56417 56431 56437 56443 56453 56467 56473 56477 56479 56489 56501 56503 56509 56519 56527 56531 56533 56543 56569 56591 56597 56599 56611 56629 56633 56659 56663 56671 56681 56687 56701 56711 56713 56731 56737 56747 56767 56773 56779 56783 56807 56809 56813 56821 56827 56843 56857 56873 56891 56893 56897 56909 56911 56921 56923 56929 56941 56951 56957 56963 56983 56989 56993 56999 57037 57041 57047 57059 57073 57077 57089 57097 57107 57119 57131 57139 57143 57149 57163 57173 57179 57191 57193 57203 57221 57223 57241 57251 57259 57269 57271 57283 57287 57301 57329 57331 57347 57349 57367 57373 57383 57389 57397 57413 57427 57457 57467 57487 57493 57503 57527 57529 57557 57559 57571 57587 57593 57601 57637 57641 57649 57653 57667 57679 57689 57697 57709 57713 57719 57727 57731 57737 57751 57773 57781 57787 57791 57793 57803 57809 57829 57839 57847 57853 57859 57881 57899 57901 57917 57923 57943 57947 57973 57977 57991 58013 58027 58031 58043 58049 58057 58061 58067 58073 58099 58109 58111 58129 58147 58151 58153 58169 58171 58189 58193 58199 58207 58211 58217 58229 58231 58237 58243 58271 58309 58313 58321 58337 58363 58367 58369 58379 58391 58393 58403 58411 58417 58427 58439 58441 58451 58453 58477 58481 58511 58537 58543 58549 58567 58573 58579 58601 58603 58613 58631 58657 58661 58679 58687 58693 58699 58711 58727 58733 58741 58757 58763 58771 58787 58789 58831 58889 58897 58901 58907 58909 58913 58921 58937 58943 58963 58967 58979 58991 58997 59009 59011 59021 59023 59029 59051 59053 59063 59069 59077 59083 59093 59107 59113 59119 59123 59141 59149 59159 59167 59183 59197 59207 59209 59219 59221 59233 59239 59243 59263 59273 59281 59333 59341 59351 59357 59359 59369 59377 59387 59393 59399 59407 59417 59419 59441 59443 59447 59453 59467 59471 59473 59497 59509 59513 59539 59557 59561 59567 59581 59611 59617 59621 59627 59629 59651 59659 59663 59669 59671 59693 59699 59707 59723 59729 59743 59747 59753 59771 59779 59791 59797 59809 59833 59863 59879 59887 59921 59929 59951 59957 59971 59981 59999 60013 60017 60029 60037 60041 60077 60083 60089 60091 60101 60103 60107 60127 60133 60139 60149 60161 60167 60169 60209 60217 60223 60251 60257 60259 60271 60289 60293 60317 60331 60337 60343 60353 60373 60383 60397 60413 60427 60443 60449 60457 60493 60497 60509 60521 60527 60539 60589 60601 60607 60611 60617 60623 60631 60637 60647 60649 60659 60661 60679 60689 60703 60719 60727 60733 60737 60757 60761 60763 60773 60779 60793 60811 60821 60859 60869 60887 60889 60899 60901 60913 60917 60919 60923 60937 60943 60953 60961 61001 61007 61027 61031 61043 61051 61057 61091 61099 61121 61129 61141 61151 61153 61169 61211 61223 61231 61253 61261 61283 61291 61297 61331 61333 61339 61343 61357 61363 61379 61381 61403 61409 61417 61441 61463 61469 61471 61483 61487 61493 61507 61511 61519 61543 61547 61553 61559 61561 61583 61603 61609 61613 61627 61631 61637 61643 61651 61657 61667 61673 61681 61687 61703 61717 61723 61729 61751 61757 61781 61813 61819 61837 61843 61861 61871 61879 61909 61927 61933 61949 61961 61967 61979 61981 61987 61991 62003 62011 62017 62039 62047 62053 62057 62071 62081 62099 62119 62129 62131 62137 62141 62143 62171 62189 62191 62201 62207 62213 62219 62233 62273 62297 62299 62303 62311 62323 62327 62347 62351 62383 62401 62417 62423 62459 62467 62473 62477 62483 62497 62501 62507 62533 62539 62549 62563 62581 62591 62597 62603 62617 62627 62633 62639 62653 62659 62683 62687 62701 62723 62731 62743 62753 62761 62773 62791 62801 62819 62827 62851 62861 62869 62873 62897 62903 62921 62927 62929 62939 62969 62971 62981 62983 62987 62989 63029 63031 63059 63067 63073 63079 63097 63103 63113 63127 63131 63149 63179 63197 63199 63211 63241 63247 63277 63281 63299 63311 63313 63317 63331 63337 63347 63353 63361 63367 63377 63389 63391 63397 63409 63419 63421 63439 63443 63463 63467 63473 63487 63493 63499 63521 63527 63533 63541 63559 63577 63587 63589 63599 63601 63607 63611 63617 63629 63647 63649 63659 63667 63671 63689 63691 63697 63703 63709 63719 63727 63737 63743 63761 63773 63781 63793 63799 63803 63809 63823 63839 63841 63853 63857 63863 63901 63907 63913 63929 63949 63977 63997 64007 64013 64019 64033 64037 64063 64067 64081 64091 64109 64123 64151 64153 64157 64171 64187 64189 64217 64223 64231 64237 64271 64279 64283 64301 64303 64319 64327 64333 64373 64381 64399 64403 64433 64439 64451 64453 64483 64489 64499 64513 64553 64567 64577 64579 64591 64601 64609 64613 64621 64627 64633 64661 64663 64667 64679 64693 64709 64717 64747 64763 64781 64783 64793 64811 64817 64849 64853 64871 64877 64879 64891 64901 64919 64921 64927 64937 64951 64969 64997 65003 65011 65027 65029 65033 65053 65063 65071 65089 65099 65101 65111 65119 65123 65129 65141 65147 65167 65171 65173 65179 65183 65203 65213 65239 65257 65267 65269 65287 65293 65309 65323 65327 65353 65357 65371 65381 65393 65407 65413 65419 65423 65437 65447 65449 65479 65497 65519 65521 65537 65539 65543 65551 65557 65563 65579 65581 65587 65599 65609 65617 65629 65633 65647 65651 65657 65677 65687 65699 65701 65707 65713 65717 65719 65729 65731 65761 65777 65789 65809 65827 65831 65837 65839 65843 65851 65867 65881 65899 65921 65927 65929 65951 65957 65963 65981 65983 65993 66029 66037 66041 66047 66067 66071 66083 66089 66103 66107 66109 66137 66161 66169 66173 66179 66191 66221 66239 66271 66293 66301 66337 66343 66347 66359 66361 66373 66377 66383 66403 66413 66431 66449 66457 66463 66467 66491 66499 66509 66523 66529 66533 66541 66553 66569 66571 66587 66593 66601 66617 66629 66643 66653 66683 66697 66701 66713 66721 66733 66739 66749 66751 66763 66791 66797 66809 66821 66841 66851 66853 66863 66877 66883 66889 66919 66923 66931 66943 66947 66949 66959 66973 66977 67003 67021 67033 67043 67049 67057 67061 67073 67079 67103 67121 67129 67139 67141 67153 67157 67169 67181 67187 67189 67211 67213 67217 67219 67231 67247 67261 67271 67273 67289 67307 67339 67343 67349 67369 67391 67399 67409 67411 67421 67427 67429 67433 67447 67453 67477 67481 67489 67493 67499 67511 67523 67531 67537 67547 67559 67567 67577 67579 67589 67601 67607 67619 67631 67651 67679 67699 67709 67723 67733 67741 67751 67757 67759 67763 67777 67783 67789 67801 67807 67819 67829 67843 67853 67867 67883 67891 67901 67927 67931 67933 67939 67943 67957 67961 67967 67979 67987 67993 68023 68041 68053 68059 68071 68087 68099 68111 68113 68141 68147 68161 68171 68207 68209 68213 68219 68227 68239 68261 68279 68281 68311 68329 68351 68371 68389 68399 68437 68443 68447 68449 68473 68477 68483 68489 68491 68501 68507 68521 68531 68539 68543 68567 68581 68597 68611 68633 68639 68659 68669 68683 68687 68699 68711 68713 68729 68737 68743 68749 68767 68771 68777 68791 68813 68819 68821 68863 68879 68881 68891 68897 68899 68903 68909 68917 68927 68947 68963 68993 69001 69011 69019 69029 69031 69061 69067 69073 69109 69119 69127 69143 69149 69151 69163 69191 69193 69197 69203 69221 69233 69239 69247 69257 69259 69263 69313 69317 69337 69341 69371 69379 69383 69389 69401 69403 69427 69431 69439 69457 69463 69467 69473 69481 69491 69493 69497 69499 69539 69557 69593 69623 69653 69661 69677 69691 69697 69709 69737 69739 69761 69763 69767 69779 69809 69821 69827 69829 69833 69847 69857 69859 69877 69899 69911 69929 69931 69941 69959 69991 69997 70001 70003 70009 70019 70039 70051 70061 70067 70079 70099 70111 70117 70121 70123 70139 70141 70157 70163 70177 70181 70183 70199 70201 70207 70223 70229 70237 70241 70249 70271 70289 70297 70309 70313 70321 70327 70351 70373 70379 70381 70393 70423 70429 70439 70451 70457 70459 70481 70487 70489 70501 70507 70529 70537 70549 70571 70573 70583 70589 70607 70619 70621 70627 70639 70657 70663 70667 70687 70709 70717 70729 70753 70769 70783 70793 70823 70841 70843 70849 70853 70867 70877 70879 70891 70901 70913 70919 70921 70937 70949 70951 70957 70969 70979 70981 70991 70997 70999 71011 71023 71039 71059 71069 71081 71089 71119 71129 71143 71147 71153 71161 71167 71171 71191 71209 71233 71237 71249 71257 71261 71263 71287 71293 71317 71327 71329 71333 71339 71341 71347 71353 71359 71363 71387 71389 71399 71411 71413 71419 71429 71437 71443 71453 71471 71473 71479 71483 71503 71527 71537 71549 71551 71563 71569 71593 71597 71633 71647 71663 71671 71693 71699 71707 71711 71713 71719 71741 71761 71777 71789 71807 71809 71821 71837 71843 71849 71861 71867 71879 71881 71887 71899 71909 71917 71933 71941 71947 71963 71971 71983 71987 71993 71999 72019 72031 72043 72047 72053 72073 72077 72089 72091 72101 72103 72109 72139 72161 72167 72169 72173 72211 72221 72223 72227 72229 72251 72253 72269 72271 72277 72287 72307 72313 72337 72341 72353 72367 72379 72383 72421 72431 72461 72467 72469 72481 72493 72497 72503 72533 72547 72551 72559 72577 72613 72617 72623 72643 72647 72649 72661 72671 72673 72679 72689 72701 72707 72719 72727 72733 72739 72763 72767 72797 72817 72823 72859 72869 72871 72883 72889 72893 72901 72907 72911 72923 72931 72937 72949 72953 72959 72973 72977 72997 73009 73013 73019 73037 73039 73043 73061 73063 73079 73091 73121 73127 73133 73141 73181 73189 73237 73243 73259 73277 73291 73303 73309 73327 73331 73351 73361 73363 73369 73379 73387 73417 73421 73433 73453 73459 73471 73477 73483 73517 73523 73529 73547 73553 73561 73571 73583 73589 73597 73607 73609 73613 73637 73643 73651 73673 73679 73681 73693 73699 73709 73721 73727 73751 73757 73771 73783 73819 73823 73847 73849 73859 73867 73877 73883 73897 73907 73939 73943 73951 73961 73973 73999 74017 74021 74027 74047 74051 74071 74077 74093 74099 74101 74131 74143 74149 74159 74161 74167 74177 74189 74197 74201 74203 74209 74219 74231 74257 74279 74287 74293 74297 74311 74317 74323 74353 74357 74363 74377 74381 74383 74411 74413 74419 74441 74449 74453 74471 74489 74507 74509 74521 74527 74531 74551 74561 74567 74573 74587 74597 74609 74611 74623 74653 74687 74699 74707 74713 74717 74719 74729 74731 74747 74759 74761 74771 74779 74797 74821 74827 74831 74843 74857 74861 74869 74873 74887 74891 74897 74903 74923 74929 74933 74941 74959 75011 75013 75017 75029 75037 75041 75079 75083 75109 75133 75149 75161 75167 75169 75181 75193 75209 75211 75217 75223 75227 75239 75253 75269 75277 75289 75307 75323 75329 75337 75347 75353 75367 75377 75389 75391 75401 75403 75407 75431 75437 75479 75503 75511 75521 75527 75533 75539 75541 75553 75557 75571 75577 75583 75611 75617 75619 75629 75641 75653 75659 75679 75683 75689 75703 75707 75709 75721 75731 75743 75767 75773 75781 75787 75793 75797 75821 75833 75853 75869 75883 75913 75931 75937 75941 75967 75979 75983 75989 75991 75997 76001 76003 76031 76039 76079 76081 76091 76099 76103 76123 76129 76147 76157 76159 76163 76207 76213 76231 76243 76249 76253 76259 76261 76283 76289 76303 76333 76343 76367 76369 76379 76387 76403 76421 76423 76441 76463 76471 76481 76487 76493 76507 76511 76519 76537 76541 76543 76561 76579 76597 76603 76607 76631 76649 76651 76667 76673 76679 76697 76717 76733 76753 76757 76771 76777 76781 76801 76819 76829 76831 76837 76847 76871 76873 76883 76907 76913 76919 76943 76949 76961 76963 76991 77003 77017 77023 77029 77041 77047 77069 77081 77093 77101 77137 77141 77153 77167 77171 77191 77201 77213 77237 77239 77243 77249 77261 77263 77267 77269 77279 77291 77317 77323 77339 77347 77351 77359 77369 77377 77383 77417 77419 77431 77447 77471 77477 77479 77489 77491 77509 77513 77521 77527 77543 77549 77551 77557 77563 77569 77573 77587 77591 77611 77617 77621 77641 77647 77659 77681 77687 77689 77699 77711 77713 77719 77723 77731 77743 77747 77761 77773 77783 77797 77801 77813 77839 77849 77863 77867 77893 77899 77929 77933 77951 77969 77977 77983 77999 78007 78017 78031 78041 78049 78059 78079 78101 78121 78137 78139 78157 78163 78167 78173 78179 78191 78193 78203 78229 78233 78241 78259 78277 78283 78301 78307 78311 78317 78341 78347 78367 78401 78427 78437 78439 78467 78479 78487 78497 78509 78511 78517 78539 78541 78553 78569 78571 78577 78583 78593 78607 78623 78643 78649 78653 78691 78697 78707 78713 78721 78737 78779 78781 78787 78791 78797 78803 78809 78823 78839 78853 78857 78877 78887 78889 78893 78901 78919 78929 78941 78977 78979 78989 79031 79039 79043 79063 79087 79103 79111 79133 79139 79147 79151 79153 79159 79181 79187 79193 79201 79229 79231 79241 79259 79273 79279 79283 79301 79309 79319 79333 79337 79349 79357 79367 79379 79393 79397 79399 79411 79423 79427 79433 79451 79481 79493 79531 79537 79549 79559 79561 79579 79589 79601 79609 79613 79621 79627 79631 79633 79657 79669 79687 79691 79693 79697 79699 79757 79769 79777 79801 79811 79813 79817 79823 79829 79841 79843 79847 79861 79867 79873 79889 79901 79903 79907 79939 79943 79967 79973 79979 79987 79997 79999 80021 80039 80051 80071 80077 80107 80111 80141 80147 80149 80153 80167 80173 80177 80191 80207 80209 80221 80231 80233 80239 80251 80263 80273 80279 80287 80309 80317 80329 80341 80347 80363 80369 80387 80407 80429 80447 80449 80471 80473 80489 80491 80513 80527 80537 80557 80567 80599 80603 80611 80621 80627 80629 80651 80657 80669 80671 80677 80681 80683 80687 80701 80713 80737 80747 80749 80761 80777 80779 80783 80789 80803 80809 80819 80831 80833 80849 80863 80897 80909 80911 80917 80923 80929 80933 80953 80963 80989 81001 81013 81017 81019 81023 81031 81041 81043 81047 81049 81071 81077 81083 81097 81101 81119 81131 81157 81163 81173 81181 81197 81199 81203 81223 81233 81239 81281 81283 81293 81299 81307 81331 81343 81349 81353 81359 81371 81373 81401 81409 81421 81439 81457 81463 81509 81517 81527 81533 81547 81551 81553 81559 81563 81569 81611 81619 81629 81637 81647 81649 81667 81671 81677 81689 81701 81703 81707 81727 81737 81749 81761 81769 81773 81799 81817 81839 81847 81853 81869 81883 81899 81901 81919 81929 81931 81937 81943 81953 81967 81971 81973 82003 82007 82009 82013 82021 82031 82037 82039 82051 82067 82073 82129 82139 82141 82153 82163 82171 82183 82189 82193 82207 82217 82219 82223 82231 82237 82241 82261 82267 82279 82301 82307 82339 82349 82351 82361 82373 82387 82393 82421 82457 82463 82469 82471 82483 82487 82493 82499 82507 82529 82531 82549 82559 82561 82567 82571 82591 82601 82609 82613 82619 82633 82651 82657 82699 82721 82723 82727 82729 82757 82759 82763 82781 82787 82793 82799 82811 82813 82837 82847 82883 82889 82891 82903 82913 82939 82963 82981 82997 83003 83009 83023 83047 83059 83063 83071 83077 83089 83093 83101 83117 83137 83177 83203 83207 83219 83221 83227 83231 83233 83243 83257 83267 83269 83273 83299 83311 83339 83341 83357 83383 83389 83399 83401 83407 83417 83423 83431 83437 83443 83449 83459 83471 83477 83497 83537 83557 83561 83563 83579 83591 83597 83609 83617 83621 83639 83641 83653 83663 83689 83701 83717 83719 83737 83761 83773 83777 83791 83813 83833 83843 83857 83869 83873 83891 83903 83911 83921 83933 83939 83969 83983 83987 84011 84017 84047 84053 84059 84061 84067 84089 84121 84127 84131 84137 84143 84163 84179 84181 84191 84199 84211 84221 84223 84229 84239 84247 84263 84299 84307 84313 84317 84319 84347 84349 84377 84389 84391 84401 84407 84421 84431 84437 84443 84449 84457 84463 84467 84481 84499 84503 84509 84521 84523 84533 84551 84559 84589 84629 84631 84649 84653 84659 84673 84691 84697 84701 84713 84719 84731 84737 84751 84761 84787 84793 84809 84811 84827 84857 84859 84869 84871 84913 84919 84947 84961 84967 84977 84979 84991 85009 85021 85027 85037 85049 85061 85081 85087 85091 85093 85103 85109 85121 85133 85147 85159 85193 85199 85201 85213 85223 85229 85237 85243 85247 85259 85297 85303 85313 85331 85333 85361 85363 85369 85381 85411 85427 85429 85439 85447 85451 85453 85469 85487 85513 85517 85523 85531 85549 85571 85577 85597 85601 85607 85619 85621 85627 85639 85643 85661 85667 85669 85691 85703 85711 85717 85733 85751 85781 85793 85817 85819 85829 85831 85837 85843 85847 85853 85889 85903 85909 85931 85933 85991 85999 86011 86017 86027 86029 86069 86077 86083 86111 86113 86117 86131 86137 86143 86161 86171 86179 86183 86197 86201 86209 86239 86243 86249 86257 86263 86269 86287 86291 86293 86297 86311 86323 86341 86351 86353 86357 86369 86371 86381 86389 86399 86413 86423 86441 86453 86461 86467 86477 86491 86501 86509 86531 86533 86539 86561 86573 86579 86587 86599 86627 86629 86677 86689 86693 86711 86719 86729 86743 86753 86767 86771 86783 86813 86837 86843 86851 86857 86861 86869 86923 86927 86929 86939 86951 86959 86969 86981 86993 87011 87013 87037 87041 87049 87071 87083 87103 87107 87119 87121 87133 87149 87151 87179 87181 87187 87211 87221 87223 87251 87253 87257 87277 87281 87293 87299 87313 87317 87323 87337 87359 87383 87403 87407 87421 87427 87433 87443 87473 87481 87491 87509 87511 87517 87523 87539 87541 87547 87553 87557 87559 87583 87587 87589 87613 87623 87629 87631 87641 87643 87649 87671 87679 87683 87691 87697 87701 87719 87721 87739 87743 87751 87767 87793 87797 87803 87811 87833 87853 87869 87877 87881 87887 87911 87917 87931 87943 87959 87961 87973 87977 87991 88001 88003 88007 88019 88037 88069 88079 88093 88117 88129 88169 88177 88211 88223 88237 88241 88259 88261 88289 88301 88321 88327 88337 88339 88379 88397 88411 88423 88427 88463 88469 88471 88493 88499 88513 88523 88547 88589 88591 88607 88609 88643 88651 88657 88661 88663 88667 88681 88721 88729 88741 88747 88771 88789 88793 88799 88801 88807 88811 88813 88817 88819 88843 88853 88861 88867 88873 88883 88897 88903 88919 88937 88951 88969 88993 88997 89003 89009 89017 89021 89041 89051 89057 89069 89071 89083 89087 89101 89107 89113 89119 89123 89137 89153 89189 89203 89209 89213 89227 89231 89237 89261 89269 89273 89293 89303 89317 89329 89363 89371 89381 89387 89393 89399 89413 89417 89431 89443 89449 89459 89477 89491 89501 89513 89519 89521 89527 89533 89561 89563 89567 89591 89597 89599 89603 89611 89627 89633 89653 89657 89659 89669 89671 89681 89689 89753 89759 89767 89779 89783 89797 89809 89819 89821 89833 89839 89849 89867 89891 89897 89899 89909 89917 89923 89939 89959 89963 89977 89983 89989 90001 90007 90011 90017 90019 90023 90031 90053 90059 90067 90071 90073 90089 90107 90121 90127 90149 90163 90173 90187 90191 90197 90199 90203 90217 90227 90239 90247 90263 90271 90281 90289 90313 90353 90359 90371 90373 90379 90397 90401 90403 90407 90437 90439 90469 90473 90481 90499 90511 90523 90527 90529 90533 90547 90583 90599 90617 90619 90631 90641 90647 90659 90677 90679 90697 90703 90709 90731 90749 90787 90793 90803 90821 90823 90833 90841 90847 90863 90887 90901 90907 90911 90917 90931 90947 90971 90977 90989 90997 91009 91019 91033 91079 91081 91097 91099 91121 91127 91129 91139 91141 91151 91153 91159 91163 91183 91193 91199 91229 91237 91243 91249 91253 91283 91291 91297 91303 91309 91331 91367 91369 91373 91381 91387 91393 91397 91411 91423 91433 91453 91457 91459 91463 91493 91499 91513 91529 91541 91571 91573 91577 91583 91591 91621 91631 91639 91673 91691 91703 91711 91733 91753 91757 91771 91781 91801 91807 91811 91813 91823 91837 91841 91867 91873 91909 91921 91939 91943 91951 91957 91961 91967 91969 91997 92003 92009 92033 92041 92051 92077 92083 92107 92111 92119 92143 92153 92173 92177 92179 92189 92203 92219 92221 92227 92233 92237 92243 92251 92269 92297 92311 92317 92333 92347 92353 92357 92363 92369 92377 92381 92383 92387 92399 92401 92413 92419 92431 92459 92461 92467 92479 92489 92503 92507 92551 92557 92567 92569 92581 92593 92623 92627 92639 92641 92647 92657 92669 92671 92681 92683 92693 92699 92707 92717 92723 92737 92753 92761 92767 92779 92789 92791 92801 92809 92821 92831 92849 92857 92861 92863 92867 92893 92899 92921 92927 92941 92951 92957 92959 92987 92993 93001 93047 93053 93059 93077 93083 93089 93097 93103 93113 93131 93133 93139 93151 93169 93179 93187 93199 93229 93239 93241 93251 93253 93257 93263 93281 93283 93287 93307 93319 93323 93329 93337 93371 93377 93383 93407 93419 93427 93463 93479 93481 93487 93491 93493 93497 93503 93523 93529 93553 93557 93559 93563 93581 93601 93607 93629 93637 93683 93701 93703 93719 93739 93761 93763 93787 93809 93811 93827 93851 93871 93887 93889 93893 93901 93911 93913 93923 93937 93941 93949 93967 93971 93979 93983 93997 94007 94009 94033 94049 94057 94063 94079 94099 94109 94111 94117 94121 94151 94153 94169 94201 94207 94219 94229 94253 94261 94273 94291 94307 94309 94321 94327 94331 94343 94349 94351 94379 94397 94399 94421 94427 94433 94439 94441 94447 94463 94477 94483 94513 94529 94531 94541 94543 94547 94559 94561 94573 94583 94597 94603 94613 94621 94649 94651 94687 94693 94709 94723 94727 94747 94771 94777 94781 94789 94793 94811 94819 94823 94837 94841 94847 94849 94873 94889 94903 94907 94933 94949 94951 94961 94993 94999 95003 95009 95021 95027 95063 95071 95083 95087 95089 95093 95101 95107 95111 95131 95143 95153 95177 95189 95191 95203 95213 95219 95231 95233 95239 95257 95261 95267 95273 95279 95287 95311 95317 95327 95339 95369 95383 95393 95401 95413 95419 95429 95441 95443 95461 95467 95471 95479 95483 95507 95527 95531 95539 95549 95561 95569 95581 95597 95603 95617 95621 95629 95633 95651 95701 95707 95713 95717 95723 95731 95737 95747 95773 95783 95789 95791 95801 95803 95813 95819 95857 95869 95873 95881 95891 95911 95917 95923 95929 95947 95957 95959 95971 95987 95989 96001 96013 96017 96043 96053 96059 96079 96097 96137 96149 96157 96167 96179 96181 96199 96211 96221 96223 96233 96259 96263 96269 96281 96289 96293 96323 96329 96331 96337 96353 96377 96401 96419 96431 96443 96451 96457 96461 96469 96479 96487 96493 96497 96517 96527 96553 96557 96581 96587 96589 96601 96643 96661 96667 96671 96697 96703 96731 96737 96739 96749 96757 96763 96769 96779 96787 96797 96799 96821 96823 96827 96847 96851 96857 96893 96907 96911 96931 96953 96959 96973 96979 96989 96997 97001 97003 97007 97021 97039 97073 97081 97103 97117 97127 97151 97157 97159 97169 97171 97177 97187 97213 97231 97241 97259 97283 97301 97303 97327 97367 97369 97373 97379 97381 97387 97397 97423 97429 97441 97453 97459 97463 97499 97501 97511 97523 97547 97549 97553 97561 97571 97577 97579 97583 97607 97609 97613 97649 97651 97673 97687 97711 97729 97771 97777 97787 97789 97813 97829 97841 97843 97847 97849 97859 97861 97871 97879 97883 97919 97927 97931 97943 97961 97967 97973 97987 98009 98011 98017 98041 98047 98057 98081 98101 98123 98129 98143 98179 98207 98213 98221 98227 98251 98257 98269 98297 98299 98317 98321 98323 98327 98347 98369 98377 98387 98389 98407 98411 98419 98429 98443 98453 98459 98467 98473 98479 98491 98507 98519 98533 98543 98561 98563 98573 98597 98621 98627 98639 98641 98663 98669 98689 98711 98713 98717 98729 98731 98737 98773 98779 98801 98807 98809 98837 98849 98867 98869 98873 98887 98893 98897 98899 98909 98911 98927 98929 98939 98947 98953 98963 98981 98993 98999 99013 99017 99023 99041 99053 99079 99083 99089 99103 99109 99119 99131 99133 99137 99139 99149 99173 99181 99191 99223 99233 99241 99251 99257 99259 99277 99289 99317 99347 99349 99367 99371 99377 99391 99397 99401 99409 99431 99439 99469 99487 99497 99523 99527 99529 99551 99559 99563 99571 99577 99581 99607 99611 99623 99643 99661 99667 99679 99689 99707 99709 99713 99719 99721 99733 99761 99767 99787 99793 99809 99817 99823 99829 99833 99839 99859 99871 99877 99881 99901 99907 99923 99929 99961 99971 99989 99991 100003 100019 100043 100049 100057 100069 100103 100109 100129 100151 100153 100169 100183 100189 100193 100207 100213 100237 100267 100271 100279 100291 100297 100313 100333 100343 100357 100361 100363 100379 100391 100393 100403 100411 100417 100447 100459 100469 100483 100493 100501 100511 100517 100519 100523 100537 100547 100549 100559 100591 100609 100613 100621 100649 100669 100673 100693 100699 100703 100733 100741 100747 100769 100787 100799 100801 100811 100823 100829 100847 100853 100907 100913 100927 100931 100937 100943 100957 100981 100987 100999 101009 101021 101027 101051 101063 101081 101089 101107 101111 101113 101117 101119 101141 101149 101159 101161 101173 101183 101197 101203 101207 101209 101221 101267 101273 101279 101281 101287 101293 101323 101333 101341 101347 101359 101363 101377 101383 101399 101411 101419 101429 101449 101467 101477 101483 101489 101501 101503 101513 101527 101531 101533 101537 101561 101573 101581 101599 101603 101611 101627 101641 101653 101663 101681 101693 101701 101719 101723 101737 101741 101747 101749 101771 101789 101797 101807 101833 101837 101839 101863 101869 101873 101879 101891 101917 101921 101929 101939 101957 101963 101977 101987 101999 102001 102013 102019 102023 102031 102043 102059 102061 102071 102077 102079 102101 102103 102107 102121 102139 102149 102161 102181 102191 102197 102199 102203 102217 102229 102233 102241 102251 102253 102259 102293 102299 102301 102317 102329 102337 102359 102367 102397 102407 102409 102433 102437 102451 102461 102481 102497 102499 102503 102523 102533 102539 102547 102551 102559 102563 102587 102593 102607 102611 102643 102647 102653 102667 102673 102677 102679 102701 102761 102763 102769 102793 102797 102811 102829 102841 102859 102871 102877 102881 102911 102913 102929 102931 102953 102967 102983 103001 103007 103043 103049 103067 103069 103079 103087 103091 103093 103099 103123 103141 103171 103177 103183 103217 103231 103237 103289 103291 103307 103319 103333 103349 103357 103387 103391 103393 103399 103409 103421 103423 103451 103457 103471 103483 103511 103529 103549 103553 103561 103567 103573 103577 103583 103591 103613 103619 103643 103651 103657 103669 103681 103687 103699 103703 103723 103769 103787 103801 103811 103813 103837 103841 103843 103867 103889 103903 103913 103919 103951 103963 103967 103969 103979 103981 103991 103993 103997 104003 104009 104021 104033 104047 104053 104059 104087 104089 104107 104113 104119 104123 104147 104149 104161 104173 104179 104183 104207 104231 104233 104239 104243 104281 104287 104297 104309 104311 104323 104327 104347 104369 104381 104383 104393 104399 104417 104459 104471 104473 104479 104491 104513 104527 104537 104543 104549 104551 104561 104579 104593 104597 104623 104639 104651 104659 104677 104681 104683 104693 104701 104707 104711 104717 104723 104729\n","id":2339,"permalink":"https://freshrimpsushi.github.io/en/posts/2339/","tags":null,"title":"List of decimals to the 10,000th"},{"categories":"줄리아","contents":"Code While the original sushi restaurant with fresh shrimp includes detailed explanations, Julia wants to omit explanations on purpose to emphasize how easy it is to do parallel processing.\nusing Base.Threads\rfor i in 1:10\rprintln(i^2)\rend If you want to parallelize the above loop, you just need to prepend @threads to the for loop.\n@threads for i in 1:10\rprintln(i^2)\rend However, if I must add one piece of advice, it is that not everything gets faster with parallel processing. Proper use of parallel processing can yield very high performance, but just because it\u0026rsquo;s easier to write code doesn\u0026rsquo;t mean optimization is easy as well. Take your time to measure and pay attention to the execution time.\nEnvironment OS: Windows julia: v1.5.0 ","id":1474,"permalink":"https://freshrimpsushi.github.io/en/posts/1474/","tags":null,"title":"How to Parallel Process in Julia"},{"categories":"양자역학","contents":"Generalization of Vectors Linear Algebra might be a new concept for science students who haven\u0026rsquo;t studied it yet. To them, a vector refers to a physical quantity with magnitude and direction, representing a point in 3-dimensional space, often denoted as $\\vec{x} = (x_{1}, x_{2}, x_{3})$. This definition is sufficient for studying classical mechanics and electromagnetism. However, in quantum mechanics, concepts like Fourier Analysis, Inner Product of Functions emerge, making it essential to understand the generalized definition of vectors to avoid significant difficulties in studying.\nIn linear algebra, a vector is an abstraction of the intuitive concept of vectors. Entities that have the same properties as 3-dimensional space vectors are collectively referred to as vectors, and the set of these vectors is called a vector space. These properties are the ones that we naturally associate with points in 3-dimensional space. For instance:\nThe sum of two vectors is also a vector. A vector multiplied by a scalar is also a vector. Therefore, a point in 3-dimensional space qualifies as a vector, and the 3-dimensional space itself becomes a vector space. Below are two critical examples in quantum mechanics. Both matrices and functions can be vectors.\nExamples Matrices Consider a set of matrices of size $m \\times n$. Adding two such matrices still results in an $m \\times n$ matrix, and multiplying a matrix by any scalar yields an $m \\times n$ matrix. Hence, this set forms a vector space, and each matrix is considered a vector.\nRealizing that there\u0026rsquo;s essentially no difference between representing a vector as an ordered pair $\\mathbf{x} = (x_{1}, x_{2}, x_{3})$ and representing it as a $1 \\times 3$ matrix $\\mathbf{x} = \\begin{bmatrix} x_{1} \u0026amp; x_{2} \u0026amp; x_{3} \\end{bmatrix}$ might make the concept of matrices being vectors more intuitive.\nFunctions Consider the set of continuous functions. If $f$ and $g$ are continuous functions, their sum $f+g$ is also a continuous function. Similarly, multiplying any continuous function by a scalar, $cf$, results in another continuous function. Therefore, the set of continuous functions forms a vector space, and each continuous function within is a vector.\nIn fact, recalling the notation for vector functions, where the function values are 3-dimensional vectors, can help in understanding this concept.\n$$ f(x,y,z) = (xy, yz, z^{2}) $$\nGeneralization of Inner Product The inner product is an operation that is extremely useful when dealing with vectors. Just as we generalized the concept of vectors, let\u0026rsquo;s generalize the concept of inner products. Instead of using the dot $\\cdot$ for traditional inner products, we use angle brackets $\\left\\langle \\ ,\\ \\right\\rangle$. If $\\mathbf{x} = \\left( x_{1}, x_{2}, x_{3} \\right)$ and $\\mathbf{y}=\\left( y_{1}, y_{2}, y_{3} \\right)$, we represent the inner product as follows:\n$$ \\mathbf{x} \\cdot \\mathbf{y} = x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} = \\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle $$\nIn quantum mechanics, instead of a comma, a bar $|$ is used in the middle.\n$$ \\mathbf{x} \\cdot \\mathbf{y} = \\braket{\\mathbf{x} \\vert \\mathbf{y} } $$\nThis is referred to as Dirac Notation. The essence of generalizing vectors lies in the concept that anything satisfying the \u0026lsquo;properties we attribute to vectors\u0026rsquo; can be called a vector, regardless of what it is. The same applies to the generalization of inner products; the concept of \u0026lsquo;adding up the product of each corresponding component\u0026rsquo; is maintained. Depending on the vector space being dealt with, the definition of inner product can vary as follows:\nExamples Matrices Consider two matrices $A = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \\ a_{21} \u0026amp; a_{22} \\end{pmatrix}, B = \\begin{pmatrix} b_{11} \u0026amp; b_{12} \\ b_{21} \u0026amp; b_{22} \\end{pmatrix}$. Their inner product is defined as the \u0026lsquo;sum of the products of corresponding components\u0026rsquo;, just like the inner product of 3-dimensional vectors.\n$$ \\braket{ A \\vert B } = a_{11}b_{11} + a_{12}b_{12} + a_{21}b_{21} + a_{22}b_{22} $$\nFunctions Since we have established that functions are also vectors, we can define the inner product of two functions. The inner product of functions is defined as follows, using definite integration:\n$$ \\braket{\\psi \\vert \\phi} = \\int \\psi^{\\ast}(x) \\phi (x) dx $$\nHere, $\\psi^{\\ast}$ represents the complex conjugate of $\\psi. Be mindful of the notation ambiguity. The reasons for defining the inner product of functions this way are well explained in 'Why the Inner Product of Functions is Defined Using Definite Integration'.\nWave Functions in Quantum Mechanics In quantum mechanics, a wave function represents the state of a particle with respect to position and time and is typically expressed using exponential functions.\n$$ \\psi (x,t) = e^{i(kx + \\omega t)} $$\nThey are commonly denoted by $\\psi$ and $\\phi$, pronounced as [psi] and [phi], respectively. $k$ is the wave number, satisfying the relationship with momentum, $p = \\hbar k$. Here, $\\hbar$ is a constant, so $k$ can be considered equivalent to momentum in quantum mechanics. $\\omega$ is the angular frequency, satisfying the energy relationship, $E = \\hbar \\omega$.\nHilbert Space The rigorous definition of a Hilbert Space is a 'complete inner product space'. While understanding its mathematical significance is beneficial, it\u0026rsquo;s not essential for undergraduate physics students studying quantum mechanics. The important thing to note is that a collection with very favorable properties is named Hilbert Space and that the set of wave functions constitutes a Hilbert Space. This means various excellent mathematical tools can be utilized to handle wave functions.\n","id":1509,"permalink":"https://freshrimpsushi.github.io/en/posts/1509/","tags":null,"title":"Vector, Inner Product, Wave Function, Hilbert Space in Quantum Mechanics"},{"categories":"줄리아","contents":"Overview Macros in Julia provide convenience features when coding, being executed in front of a scope. For example, if you want to know how much time your program is consuming, you can write it as follows.\n@time for t in 1:10\rfoo()\rbar()\rend Examples There are many types, but the following macros are especially widely used:\n@time: Measures the execution time of the function or scope that follows. When it\u0026rsquo;s unclear how to optimize in some situations, measuring the time first makes it easier to choose the better option. In some languages, writing code to measure the time can be cumbersome, but in Julia, a single macro tells you not only the execution time but also the amount of memory used. @.: Adds a dot (.) to the operations in the subsequent expression. @threads: A macro that makes it easier to implement parallel processing. @animate: A macro that easily bakes GIFs. Environment OS: Windows julia: v1.5.0 ","id":1454,"permalink":"https://freshrimpsushi.github.io/en/posts/1454/","tags":null,"title":"Julia's Powerful Convenience Features, Macros"},{"categories":"줄리아","contents":"Overview Julia supports the pipeline operator, highlighting its strength in handling data.\nCode julia\u0026gt; (1:5) .|\u0026gt; (x -\u0026gt; sqrt(x+2)) .|\u0026gt; sin |\u0026gt; minimum\r0.4757718381527513\rjulia\u0026gt; minimum(sin.((x -\u0026gt; sqrt(x+2)).(1:5)))\r0.4757718381527513 The example code above puts the array $[1,2,3,4,5]$ into $\\sqrt{x + 2}$, and then puts the result into $\\sin$ to obtain the smallest value. The code above and below produces exactly the same results. It goes without saying how useful the pipeline can be when writing complex code. Just remember to always use a dot when inserting arrays to treat each element individually, similar to the use of the pipeline in other languages.\nOther Languages Pipeline operator in R Environment OS: Windows julia: v1.5.0 ","id":1450,"permalink":"https://freshrimpsushi.github.io/en/posts/1450/","tags":null,"title":"How to Use Pipe Operators in Julia"},{"categories":"줄리아","contents":"Overview In Julia, lambdas are defined as follows:\n(x -\u0026gt; 3x^2 - 2x + 3)(1) This corresponds to defining the anonymous function $\\lambda : \\mathbb{Z} \\to \\mathbb{Z}$, substituting $1$ into it, and obtaining the function value $4$. $$ \\lambda : x \\mapsto ( 3 x^{2} - 2 x + 3 ) \\\\ \\lambda (1) = 4 $$ Indeed, lambda expressions themselves are not a Julia-specific feature but almost naturally supported, having been influenced by functional languages including MATLAB and Python. It is likely that learners interested in Julia have already had extensive experience using lambda expressions. However, for readers who may not know that \u0026lsquo;it\u0026rsquo; they have been using was a lambda expression, or who still do not know its true value, we introduce two immediately applicable examples especially for them.\nExample 1: Sorting lists by a different metric Sorting lists can be quite straightforward with built-in functions, but one might want to sort based on priority across categories in multi-dimensional arrays or to sort original data by different criteria. In such cases, you can easily write the code by inserting the corresponding function as a lambda expression into the by option of the sort() function.\njulia\u0026gt; # Example 1\rjulia\u0026gt; example = rand(-20:20,10)\r10-element Array{Int64,1}:\r3\r8\r19\r-12\r-20\r9\r-13\r19\r13\r2\rjulia\u0026gt; sort(example, by=(x -\u0026gt; abs(x)))\r10-element Array{Int64,1}:\r2\r3\r8\r9\r-12\r-13\r13\r19\r19\r-20 The above task represents sorting randomly selected integers from smallest to largest absolute values. It\u0026rsquo;s not impossible without lambda expressions, but it\u0026rsquo;s not as simple as it seems. By modifying the given lambda expression (x -\u0026gt; abs(x)), coders should be able to easily write the code they want.\nApplication of Example 1 Suppose a dictionary called value is created as follows. In this case, code to sort by the size of dictionary values can be simply written using a lambda expression as sort(value,by=(x -\u0026gt; value[x])), and the result is as follows. Example 2: Calculating frequency in a list Languages that prioritize data like R even have built-in functions for it, but this frequency calculation is not as straightforward as it might seem. While not complex enough to be called an algorithm, it can be quite involving when actually implemented. This can also be easily solved using a lambda function!\njulia\u0026gt; # Example 2\rjulia\u0026gt; example = rand(1:3,10); println(example)\r[3, 1, 2, 2, 3, 2, 3, 1, 3, 3]\rjulia\u0026gt; uexample = sort(unique(example))\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; counts = map(x-\u0026gt;count(y-\u0026gt;x==y,example),uexample)\r3-element Array{Int64,1}:\r2\r3\r5 The above task is about counting the frequencies of randomly chosen integers. The problem was solved simply by identifying the classes of data with unique() and then counting each element corresponding to those classes.\nEnvironment OS: Windows julia: v1.5.0 ","id":1448,"permalink":"https://freshrimpsushi.github.io/en/posts/1448/","tags":null,"title":"Lambda Expressions in Julia"},{"categories":"줄리아","contents":"Resizing Images To resize images, you can use the imresize function from the Images package. The function name is the same as in Matlab.\nimresize(X, ratio=a): Returns the image of array X scaled by a factor of a. Unlike Matlab, you must explicitly write ratio=a.\nimresize(X, m, n): Returns the image of array X resized to m rows and n columns. Below are example codes and their results.\nusing Images\rX=load(\u0026#34;example\\_{i}mage2.jpg\u0026#34;)\rY1=imresize(X, ratio=0.5)\rY2=imresize(X,500,500)\rY3=imresize(X,1500,1500)\rY4=imresize(X,700,1000)\rY5=imresize(X,1000,1300)\rY6=imresize(X,300,300)\rsave(\u0026#34;X.png\u0026#34;,colorview(RGB,X))\rsave(\u0026#34;Y1=imresize(0.5).png\u0026#34;,colorview(RGB,Y1))\rsave(\u0026#34;Y2=imresize(500,500).png\u0026#34;,colorview(RGB,Y2))\rsave(\u0026#34;Y3=imresize(1500,1500).png\u0026#34;,colorview(RGB,Y3))\rsave(\u0026#34;Y4=imresize(700,1000).png\u0026#34;,colorview(RGB,Y4))\rsave(\u0026#34;Y5=imresize(1000,1300).png\u0026#34;,colorview(RGB,Y5))\rsave(\u0026#34;Y6=imresize(300,300).png\u0026#34;,colorview(RGB,Y6)) See Also How to Resize Images in Matlab Environment OS: Windows10 Version: 1.5.3 (2020-11-09) ","id":1466,"permalink":"https://freshrimpsushi.github.io/en/posts/1466/","tags":null,"title":"How to Change Image Size in Julia"},{"categories":"줄리아","contents":"Code using Images\rcd(\u0026#34;C:/Users/rmsms/OneDrive/examples\u0026#34;)\rpwd()\rexample = load(\u0026#34;example.jpg\u0026#34;)\rtypeof(example)\rsize(example)\rgray1 = Gray.(example)\rtypeof(gray1)\rsize(gray1)\rM = convert(Array{Float64},gray1)\rtypeof(M)\rsize(M)\rcolorview(Gray, M.^(1/2))\rsave(\u0026#34;rgb.png\u0026#34;, colorview(RGB, example))\rsave(\u0026#34;gray1.png\u0026#34;, colorview(Gray, gray1))\rsave(\u0026#34;gray2.png\u0026#34;, colorview(Gray, transpose(gray1)))\rsave(\u0026#34;gray3.png\u0026#34;, colorview(Gray, M.^(1/2))) Let\u0026rsquo;s briefly understand the example code from top to bottom:\ncd() : Change Directory, changes the working directory to the desired location.\npwd() : Print Working Directory, prints the working directory. If you want to follow the example exactly, download the above file to your working directory and rename it to example.jpg.\nload() : Loads an image file in the working directory. The type of the loaded image is Array{RGB{Normed{UInt8,8}},2}. This is slightly different from other languages where a color image is represented by a tensor by combining three matrices. In fact, such an approach has caused a lot of confusion due to the lack of a unified standard across color spaces and libraries. Julia instead uses a single type and understands it as a two-dimensional array for width and height. The image is printed on the Plot panel the moment it is loaded. Gray() : Used to convert the image to grayscale. It is important to note that what is actually used is not Gray() but Gray.(). This indicates that the function itself converts a single pixel to grayscale, and putting a dot applies it to all pixels.\nsize() : Returns the size of the image. As mentioned, it does not treat color and grayscale as tensors of different shapes but as arrays of the same size with different data types. Convert() : Converted the gray1 image into Array{Float64}, i.e., a matrix. Thus, 0 represents black and 1 represents white in the matrix used to express the grayscale image.\ncolorview() : This function itself is used to print images or matrices. There’s no need to convert the matrix back into an image as one can directly check the image on the Plot panel. In the example code, the square root of each element of the matrix $M$ was taken. Since all elements of the matrix belong to $[0,1]$, this transformation corresponds to generally brightening the image. save() : Saves the images in the working directory:gray1.png : Saved in grayscale.gray2.png : Saved in grayscale and in the transpose matrix state.gray3.png : Saved in grayscale and in a brightened state.rgb.png : Saved with the original colors.\nEnvironment OS: Windows julia: v1.5.0 ","id":1446,"permalink":"https://freshrimpsushi.github.io/en/posts/1446/","tags":null,"title":"How to Load Images in Julia and Save Them as Matrices"},{"categories":"줄리아","contents":"Image Rotation imrotate(X, theta): Rotates array X by theta radians. Note that, unlike in MATLAB where the angle unit is degrees ($^{\\circ})$, the angle unit here is radians. Additionally, unlike MATLAB, it rotates clockwise. If no other variables are inputted, the interpolation method defaults to bilinear, and the rotated image is not cropped. Examples of rotating the original image X by $90^\\circ=\\pi/2$, $180^\\circ=\\pi$, and $270^\\circ=\\frac{3}{2}\\pi$, along with their results, are shown below.\nusing Images\rusing Interpolations\rX=load(\u0026#34;example\\_{i}mage.png\u0026#34;)\rY1=imrotate(X,pi/2)\rY2=imrotate(X,pi)\rY3=imrotate(X,pi*3/2) As shown, when rotated like this, the original array fits perfectly, so there is no change in the size of the image. However, when rotated by an angle that is not a multiple of $90$, it does not align with the original shape. Therefore, the image expands to represent all points of the original image. If one wishes to maintain the original size of the image, one can add the axes() variable.\nY4=imrotate(X,pi/6)\rY5=imrotate(X,pi/6,axes(X)) Additionally, using Constant() from the Interpolations package allows you to apply the nearest1 interpolation method. Since it only uses the nearest point for calculation, the accuracy is lower but the computation is faster. Conversely, bilinear2 involves the calculation of all four surrounding points, so it is relatively slower but more accurate. Being more accurate here means that there is less damage to the image upon rotation. Look at the pictures below. The images are large, so at first glance, there might not seem to be a difference, but upon zooming in, the differences between the two interpolation methods become clear.\nY6=imrotate(X,pi/6,Constant()) See Also How to Rotate Images in MATLAB Environment OS: Windows10 Version: 1.5.3 (2020-11-09) Nearest-neighbor interpolation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBilinear interpolation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1462,"permalink":"https://freshrimpsushi.github.io/en/posts/1462/","tags":null,"title":"How to Rotate Image Arrays in Julia"},{"categories":"줄리아","contents":"Let\u0026rsquo;s say $A = \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \\\\ 2 \u0026amp; 3 \u0026amp; 4\\end{pmatrix}$.\nTranspose Matrix julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; transpose(A)\r3×3 LinearAlgebra.Transpose{Int64,Array{Int64,2}}:\r1 0 2\r2 3 3\r1 0 4\rjulia\u0026gt; A\u0026#39;\r3×3 LinearAlgebra.Adjoint{Int64,Array{Int64,2}}:\r1 0 2\r2 3 3\r1 0 4 When the elements of a matrix are real numbers, transpose() and ' return the same matrix, but the data type is subtly different. This is because ' is not exactly transpose but conjugate transpose. Therefore, for real matrices, it effectively returns the same matrix, but for complex matrices, it returns a completely different result.\njulia\u0026gt; A_complex=[1+im 2 1+im;\r0 3 0+im;\r2 3+im 4]\r3×3 Array{Complex{Int64},2}:\r1+1im 2+0im 1+1im\r0+0im 3+0im 0+1im\r2+0im 3+1im 4+0im\rjulia\u0026gt; transpose(A_complex)\r3×3 LinearAlgebra.Transpose{Complex{Int64},Array{Complex{Int64},2}}:\r1+1im 0+0im 2+0im\r2+0im 3+0im 3+1im\r1+1im 0+1im 4+0im\rjulia\u0026gt; A_complex\u0026#39;\r3×3 LinearAlgebra.Adjoint{Complex{Int64},Array{Complex{Int64},2}}:\r1-1im 0+0im 2+0im\r2+0im 3+0im 3-1im\r1-1im 0-1im 4+0im Power julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; A^2\r3×3 Array{Int64,2}:\r3 11 5\r0 9 0\r10 25 18\rjulia\u0026gt; A*A\r3×3 Array{Int64,2}:\r3 11 5\r0 9 0\r10 25 18\rjulia\u0026gt; A^3\r3×3 Array{Int64,2}:\r13 54 23\r0 27 0\r46 149 82\rjulia\u0026gt; A*A*A\r3×3 Array{Int64,2}:\r13 54 23\r0 27 0\r46 149 82 A^2 and A*A return exactly the same result. Likewise, A^3 and A*A*A are the same.\nElement-wise Multiplication, Element-wise Division julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; A.*A\r3×3 Array{Int64,2}:\r1 4 1\r0 9 0\r4 9 16\rjulia\u0026gt; A./A\r3×3 Array{Float64,2}:\r1.0 1.0 1.0\rNaN 1.0 NaN\r1.0 1.0 1.0 Returns the result of multiplying or dividing each element.\nHorizontal Flip, Vertical Flip julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; reverse(A,dims=1)\r3×3 Array{Int64,2}:\r2 3 4\r0 3 0\r1 2 1\rjulia\u0026gt; reverse(A,dims=2)\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r4 3 2 reverse(A,dims=1) returns the vertically flipped matrix of $A$ and is equivalent to flipud(A) in MATLAB. reverse(A,dims=2) returns the horizontally flipped matrix of $A$ and is equivalent to fliplr(A) in MATLAB.\nInverse Matrix julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; inv(A)\r3×3 Array{Float64,2}:\r2.0 -0.833333 -0.5\r0.0 0.333333 0.0\r-1.0 0.166667 0.5 Returns the inverse matrix of $A$. If the inverse matrix cannot be found, it throws an error.\nEnvironment OS: Windows10 Version: 1.5.3 (2020-11-09) ","id":1460,"permalink":"https://freshrimpsushi.github.io/en/posts/1460/","tags":null,"title":"Functions for 2D Array Operations in Julia"},{"categories":"줄리아","contents":"Heatmap Using the heatmap function from the Plots package, you can output a 2D array as a heatmap image, and with the savefig function, you can save the resulting image. The @__DIR__ macro tells you the location of the Julia code file.\n# code1 However, if you compare array A with the heatmap image, you may notice that the top and bottom of the array are flipped in the heatmap image. The reason the output image is created this way is official crab talk says it\u0026rsquo;s because each element\u0026rsquo;s position is thought of not in terms of rows and columns but rather as coordinates in a Cartesian coordinate system. That is, for example, in the matrix $A$, the value 19 is not seen as an element in the 4th row and 4th column but as an element in Cartesian coordinates $(4,4)$. This explains why the matrix and the image are flipped upside down with respect to each other.\nTherefore, to make the output look the same as the array, you can add the yflip=true option1.\n# code2 Moreover, users familiar with MATLAB can use the color=:bgy option to get an output that closely matches MATLAB\u0026rsquo;s default colors.\n# code3 Color Themes The following are some of the available color themes.\nSee Also From MATLAB Environment OS: Windows10 Version: 1.5.3 (2020-11-09) https://github.com/JuliaPlots/Makie.jl/issues/46#issuecomment-357023505\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1459,"permalink":"https://freshrimpsushi.github.io/en/posts/1459/","tags":null,"title":"How to output and save arrays as heatmap images in Julia"},{"categories":"줄리아","contents":"Overview Julia, like Python, supports the set data type. As with any set data type, it is incredibly useful for those who use it and utterly ignored by those who don\u0026rsquo;t. Given that Julia\u0026rsquo;s design is closely aligned with mathematics, its implementation of set concepts and operations is robust, making it an important feature to understand. Perhaps the most distinct difference from other languages, especially Python, is the ability to use Unicode symbols as part of the code. If you are using Juno in the Atom editor, you can autocomplete such TeX codes as shown above. In this context, $\\in$ is not merely a symbol but actually describes whether an element belongs to a set.\nCode julia\u0026gt; X = Set([1,2,3,1]); print(X)\rSet([2, 3, 1])\rjulia\u0026gt; X[1]\rERROR: MethodError: no method matching getindex(::Set{Int64}, ::Int64)\rStacktrace:\r[1] top-level scope at REPL[23]:1\rjulia\u0026gt; for i in X print(i) end\r231 The code above means to define a set $X$ as $X := \\left\\{ 1, 2, 3, 1 \\right\\} = \\left\\{ 2,3,1 \\right\\}$. As in mathematics, the concept of duplicates and order does not exist. Therefore, referencing the first index will result in an error. However, the data type is still iterable like in Python, meaning it can be used in loops.\njulia\u0026gt; if 1∈X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if 0∈X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r?\rjulia\u0026gt; if 0∉X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if [1,2] ⊆ X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if [0,1,2] ⊆ X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r? If this feels like reading an equation, you are ready to use the set data type effectively. It is important to note that such operations are not exclusive to the set data type; they are equally applicable to lists. Even if the set data type feels unfamiliar, as long as you are comfortable with sets, using Julia\u0026rsquo;s set operators should pose no problem. Note that the relation of containment should be written as \\subseteq $\\subseteq$, not \\subset $\\subset$.\njulia\u0026gt; Y = [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;,3]\r3-element Array{Any,1}:\r\u0026#34;1\u0026#34;\r\u0026#34;2\u0026#34;\r3\rjulia\u0026gt; ∪(X,Y)\rSet{Any} with 5 elements:\r\u0026#34;1\u0026#34;\r2\r3\r\u0026#34;2\u0026#34;\r1\rjulia\u0026gt; ∩(X,Y)\rSet{Int64} with 1 element:\r3\rjulia\u0026gt; ∩(Y,X)\r1-element Array{Any,1}:\r3\rjulia\u0026gt; setdiff(X,Y); X\rSet{Int64} with 3 elements:\r2\r3\r1\rjulia\u0026gt; setdiff!(X,Y); X\rSet{Int64} with 2 elements:\r2\r1 Naturally, since it deals with sets, both union and intersection can be expressed similarly to mathematical equations. The difference between the second and third lines is the order of operations. $X$ is a set data type in Julia, while $Y$ is defined as an array, and the returned value follows the data type of the first argument. This distinction is crucial in a strongly typed language like Julia and must be thoroughly understood. The difference function is defined in two ways: setdiff() simply returns the set difference, while setdiff!() updates the set itself.\nEnvironment OS: Windows julia: v1.5.0 ","id":1442,"permalink":"https://freshrimpsushi.github.io/en/posts/1442/","tags":null,"title":"Sets and Operators in Julia"},{"categories":"줄리아","contents":"Overview Julia is a language that mixes the advantages of R, Python, and Matlab. Arrays are fundamental to programming, and their usage reveals traces of these languages.\nCode Matrix julia\u0026gt; M = [1. 2. ; 3. 4.]\r2×2 Array{Float64,2}:\r1.0 2.0\r3.0 4.0\rjulia\u0026gt; size(M)\r(2, 2)\rjulia\u0026gt; length(M)\r4 For matrices, the syntax is defined and used almost exactly like Matlab. The size() function is used just like in Matlab, and serves the same purpose as the .shape property in Python’s numpy package. length(), unlike in Matlab, returns the total number of elements.\n2D Arrays julia\u0026gt; x = [[1,2,3,4] for _ in 1:4]; x\r4-element Array{Array{Int64,1},1}:\r[1, 2, 3, 4]\r[1, 2, 3, 4]\r[1, 2, 3, 4]\r[1, 2, 3, 4] Placing loops inside arrays is a usage commonly seen in Python. This allows for a similar replication of the rep() function from R.\nSlicing julia\u0026gt; y = [3,2,5,1,4]\r5-element Array{Int64,1}:\r3\r2\r5\r1\r4\rjulia\u0026gt; y[[4,2,1,5,3]]\r5-element Array{Int64,1}:\r1\r2\r3\r4\r5\rjulia\u0026gt; y[3:end]\r3-element Array{Int64,1}:\r5\r1\r4\rjulia\u0026gt; y[3:4] .= -1; y\r5-element Array{Int64,1}:\r3\r2\r-1\r-1\r4 Indexing is similar to R, where providing an array of indexes will print the elements in that order. Seeing that the last index of an array is represented as end suggests that slicing is influenced by Matlab. Finally, using .= to directly assign -1 to the 3rd and 4th elements is also reminiscent of Matlab.\nIndexing julia\u0026gt; x = [1 2; 3 4]\r2×2 Array{Int64,2}:\r1 2\r3 4\rjulia\u0026gt; x[1,:]\r2-element Array{Int64,1}:\r1\r2\rjulia\u0026gt; x[[1],:]\r1×2 Array{Int64,2}:\r1 2\rjulia\u0026gt; x[1,1] = -1; x\r2×2 Array{Int64,2}:\r-1 2\r3 4 What is peculiar is that the result of indexing can vary depending on how it is performed. Conceptually, inserting the same thing should yield the same result; however, if elements are entered, the result is in elements, and if an array is entered, the result is in array form. This makes Julia hard to use while also providing significant help in implementing sophisticated features.\nEnvironment OS: Windows julia: v1.5.0 ","id":1437,"permalink":"https://freshrimpsushi.github.io/en/posts/1437/","tags":null,"title":"Slicing and Indexing of Arrays in Julia"},{"categories":"줄리아","contents":"Description Using circshifr(A, (n,m)), you can shift the rows of the array A $n$ positions down, and the columns $m$ positions to the right. (n,m) must be a tuple of integers, and negative numbers are also possible. If negative, it shifts in the opposite direction.\nFor arrays of 3 dimensions or more, it is applied to each smallest 2-dimensional array respectively.\nCode 2D array julia\u0026gt; A = transpose(reshape(1:25,5,5))\r5×5 LinearAlgebra.Transpose{Int64,Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}}}:\r1 2 3 4 5\r6 7 8 9 10\r11 12 13 14 15\r16 17 18 19 20\r21 22 23 24 25\rjulia\u0026gt; circshift(A, (-1,0))\r5×5 Array{Int64,2}:\r6 7 8 9 10\r11 12 13 14 15\r16 17 18 19 20\r21 22 23 24 25\r1 2 3 4 5\rjulia\u0026gt; circshift(A, (0,3))\r5×5 Array{Int64,2}:\r3 4 5 1 2\r8 9 10 6 7\r13 14 15 11 12\r18 19 20 16 17\r23 24 25 21 22\rjulia\u0026gt; circshift(A, (-1,3))\r5×5 Array{Int64,2}:\r8 9 10 6 7\r13 14 15 11 12\r18 19 20 16 17\r23 24 25 21 22\r3 4 5 1 2 Higher-dimensional array julia\u0026gt; B = reshape(1:4*4*3,4,4,3)\r4×4×3 reshape(::UnitRange{Int64}, 4, 4, 3) with eltype Int64:\r[:, :, 1] =\r1 5 9 13\r2 6 10 14\r3 7 11 15\r4 8 12 16\r[:, :, 2] =\r17 21 25 29\r18 22 26 30\r19 23 27 31\r20 24 28 32\r[:, :, 3] =\r33 37 41 45\r34 38 42 46\r35 39 43 47\r36 40 44 48\rjulia\u0026gt; circshift(B,(-1,0))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r2 6 10 14\r3 7 11 15\r4 8 12 16\r1 5 9 13\r[:, :, 2] =\r18 22 26 30\r19 23 27 31\r20 24 28 32\r17 21 25 29\r[:, :, 3] =\r34 38 42 46\r35 39 43 47\r36 40 44 48\r33 37 41 45\rjulia\u0026gt; circshift(B,(0,2))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r9 13 1 5\r10 14 2 6\r11 15 3 7\r12 16 4 8\r[:, :, 2] =\r25 29 17 21\r26 30 18 22\r27 31 19 23\r28 32 20 24\r[:, :, 3] =\r41 45 33 37\r42 46 34 38\r43 47 35 39\r44 48 36 40\rjulia\u0026gt; circshift(B,(-1,2))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r10 14 2 6\r11 15 3 7\r12 16 4 8\r9 13 1 5\r[:, :, 2] =\r26 30 18 22\r27 31 19 23\r28 32 20 24\r25 29 17 21\r[:, :, 3] =\r42 46 34 38\r43 47 35 39\r44 48 36 40\r41 45 33 37 julia\u0026gt; C = reshape(1:3*3*2*4,3,3,2,4)\r3×3×2×4 reshape(::UnitRange{Int64}, 3, 3, 2, 4) with eltype Int64:\r[:, :, 1, 1] =\r1 4 7\r2 5 8\r3 6 9\r[:, :, 2, 1] =\r10 13 16\r11 14 17\r12 15 18\r[:, :, 1, 2] =\r19 22 25\r20 23 26\r21 24 27\r[:, :, 2, 2] =\r28 31 34\r29 32 35\r30 33 36\r[:, :, 1, 3] =\r37 40 43\r38 41 44\r39 42 45\r[:, :, 2, 3] =\r46 49 52\r47 50 53\r48 51 54\r[:, :, 1, 4] =\r55 58 61\r56 59 62\r57 60 63\r[:, :, 2, 4] =\r64 67 70\r65 68 71\r66 69 72\rjulia\u0026gt; circshift(C,(1,1))\r3×3×2×4 Array{Int64,4}:\r[:, :, 1, 1] =\r9 3 6\r7 1 4\r8 2 5\r[:, :, 2, 1] =\r18 12 15\r16 10 13\r17 11 14\r[:, :, 1, 2] =\r27 21 24\r25 19 22\r26 20 23\r[:, :, 2, 2] =\r36 30 33\r34 28 31\r35 29 32\r[:, :, 1, 3] =\r45 39 42\r43 37 40\r44 38 41\r[:, :, 2, 3] =\r54 48 51\r52 46 49\r53 47 50\r[:, :, 1, 4] =\r63 57 60\r61 55 58\r62 56 59\r[:, :, 2, 4] =\r72 66 69\r70 64 67\r71 65 68 Environment OS: Windows10 Version: 1.5.3 (2020-11-09) ","id":1453,"permalink":"https://freshrimpsushi.github.io/en/posts/1453/","tags":null,"title":"Translating Arrays in Julia"},{"categories":"줄리아","contents":"코드 `x1` is a 2-dimensional array. Since it looks like a row vector, if you input only one coordinate component, it is recognized as a row vector. `x2`, `x3`, `x4`, `x5` are 1-dimensional arrays. - `x=[i for i in n:m]` returns an array with elements ranging from $n$ to $m$ with an interval of $1$. - `x=[i for i in n:k:m]` returns an array with elements ranging from $n$ to $m$ with an interval of $k$. The last element is the largest number that is less than or equal to $m$. This method of including a `for` loop inside a list to create a list is called **List Comprehension** in languages such as [Python](../../categories/programming#python). ```code2 Although it doesn\u0026#39;t actually happen, you can think of arrays like `x3`, `x4`, `x5` as being created as described above. As shown in the photo below, although the data types are different, they can be used in the same way as what was created above. - `range(n,stop=m,length=k)`: This is exactly the same as lispace(n,m,k) in [MATLAB](../1376). Let\u0026#39;s find out the specific differences through the example code and results below. ```code3 Similarly, you can think of the same vector being created, even though the actual data type is different. Unlike MATLAB, you can input only one of the second or third variables, and changing the order of input doesn\u0026#39;t matter. 1. **First line** returns a vector with the first element being $1$ and the last element $10$. Since nothing else is input, the interval between elements is $1$. 2. **Second line** returns a vector with the first element being $1$ and a total of $15$ elements. The interval automatically becomes $1$, similar to a vector created with `x=range(1,stop=15)` or `x=1:15`. 3. **Third line** returns a vector with the first element being $1$, the last element $10$, and a total of $15$ elements. Therefore, the interval automatically becomes $9/14=0.6428571428571429$. Since it is not an integer, it naturally returns a vector with real number elements. Also, it is the same as one created with `x=1.0:0.6428571428571429:10.0`. 4. **Fourth line** returns a vector exactly the same as the one returned in the third line. ## 타언어 - [How to create equally spaced row vectors in MATLAB](../1376) ## 환경 - OS: Windows10 - Version: 1.5.0 ","id":1452,"permalink":"https://freshrimpsushi.github.io/en/posts/1452/","tags":null,"title":"Various Methods of Creating Vectors in Julia"},{"categories":"줄리아","contents":"Method 1 using LinearAlgebra\rusing Pkg\rPkg.add(\u0026#34;Plots\u0026#34;)\rPkg.add(\u0026#34;Distributions\u0026#34;)\rusing Plots The above code demonstrates importing the LinearAlgebra and Pkg packages and installing the Plots, Distribution packages using the .add() function. The keyword using to import packages is somewhat reminiscent of the language used in mathematics when applying a theorem or argument. Installing packages is more akin to R than Python, and its usage closely resembles that of Python. Similar to R, package names must be enclosed in double quotes, and it\u0026rsquo;s common to write package names in Pascal Case1 and frequently append an -s to make them plural2, which can be confusing.\nMethod 2 By typing ] in the REPL, you switch to the Package Manager environment as shown above. Pressing backspace will take you back to the REPL environment. Entering add package_name in the package manager environment will install the specified package.\n(@v1.5) pkg\u0026gt; add Plots Resolving package versions... Updating `C:\\Users\\rydbr\\.julia\\environments\\v1.5\\Project.toml` [91a5bcdd] + Plots v1.0.14 No Changes to `C:\\Users\\rydbr\\.julia\\environments\\v1.5\\Manifest.toml A notation method that capitalizes the first letter of each word. As seen in the example code, linear algebra is written as LinearAlgebra, with the first letter of each word capitalized and spaces omitted.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAs seen in the example code, Plot and Distribution need to be referred to as Plots and Distributions, respectively.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1416,"permalink":"https://freshrimpsushi.github.io/en/posts/1416/","tags":null,"title":"Installing and Using Packages in Julia"},{"categories":"위상수학","contents":"Definition 1 Let\u0026rsquo;s assume that a topological space $(X,\\mathscr{T})$ and a subset $A \\subset X$ are given. Then, the following set\n$$ \\mathscr{T}_{A} =\\left\\{ A\\cap U\\ :\\ U\\in \\mathscr{T} \\right\\} $$\nis a topology on $A$. In this case, $\\mathscr{T}_{A}$ is referred to as the Subspace Topology or Relative Topology. Moreover, the topological space $(A, \\mathscr{T}_{A})$ is called the Subspace of $(X,\\mathscr{T})$.\nTheorem [0]: For a topological space $(X, \\mathscr{T}$) and a subset $A \\subset X$, $$ \\mathscr{T}_{A} = \\left\\{ A\\cap U\\ :\\ U \\in \\mathscr{T}\\right\\} $$\nbecomes a topology on $A$.\nGiven a topological space $X$ and a subspace $A$, the equivalence condition for a set in subspace $A$ to be open or closed is as follows:\n[a1]: The necessary and sufficient condition for $V\\subset A$ to be an open set in $A$ is the existence of an open set $U$ in $X$ that satisfies $V= A\\cap U$. [b1]: The necessary and sufficient condition for $F\\subset A$ to be a closed set in $A$ is the existence of a closed set $E$ in $X$ that satisfies $F=A\\cap E$. Being an open (closed) set in a subspace does not guarantee the same property in the entire space. If a subspace is an open (closed) set with respect to the entire space, then this property is preserved in the entire space. Let\u0026rsquo;s consider a topological space $(X,\\mathscr{T})$, a subspace $(A,\\mathscr{T}_{A})$, and a subset $B\\subset A\\subset X$:\n[a2]: If $B$ is an open set in the subspace $A$ and $A$ is an open set in $X$, then $B$ is an open set in $X$. [b2]: If $B$ is a closed set in the subspace $A$ and $A$ is a closed set in $X$, then $B$ is a closed set in $X$. [3]: Let\u0026rsquo;s say $\\mathscr{B}$ is a basis of topological space $(X,\\mathscr{T})$. Then, $$ \\mathscr{B}_{A} =\\left\\{ A\\cap B\\ :\\ B\\in \\mathscr{B} \\right\\} $$\nis a basis for the subspace $(A,\\mathscr{T}_{A})$.\nExplanation To avoid confusion, let\u0026rsquo;s clarify several notations. If $(X,\\mathscr{T})$ is the entire space,\n$$ \\mathscr{T}_{A}=\\left\\{A\\cap U\\ :\\ U \\in \\mathscr{T} \\right\\} $$\nis the topology of the subset $A$. Hence, it forms the subspace $(A,\\mathscr{T}_{A})$. $\\mathscr{B}$ is a basis of the entire set $X$. $\\mathscr{B}_{A}$ is a collection of intersections between each element of the entire set\u0026rsquo;s basis and $A$. This becomes the basis of the subset $A$, as per the theorem.\n$$ \\mathscr{T}_{\\mathscr{B}_{A}}=\\left\\{U_{A}\\subset A\\ :\\ \\forall\\ x \\in U_{A},\\ \\exists\\ (A\\cap B) \\in \\mathscr{B}_{A}\\ \\ \\text{s.t.}\\ x\\in (A\\cap B) \\subset U_{A}\\right\\} $$\nMoreover, the core is that $\\mathscr{T}_{\\mathscr{B}_{A}}$ is equivalent to $\\mathscr{T}_{A}$. The content may feel complex. To summarize, it goes as follows:\nIntersections between elements of the entire space\u0026rsquo;s basis and $A$ form the basis of $A$. The topology generated by this basis is $\\mathscr{T}_{\\mathscr{B}_{A}}$. The topology $\\mathscr{T}_{\\mathscr{B}_{A}}$ generated in 2 consists of sets that are intersections between open sets of $X$ and $A$, which is $\\mathscr{T}_{A}$. Proof [0] $(T1)$: Since $A \\cap \\varnothing =\\varnothing$ and $A \\cap X=A$, both the empty set and the entire set belong to $\\mathscr{T}_{A}$. $(T2)$: Let\u0026rsquo;s assume $V_\\alpha \\in \\mathscr{T}_{A}( \\alpha \\in \\Lambda)$. By the definition of $\\mathscr{T}_{A}$, for each $V_\\alpha$, there exists $U_\\alpha$ satisfying $V_\\alpha = A \\cap U_\\alpha$. By the definition of topology, $U=\\cup_{\\alpha \\in \\Lambda} U_\\alpha \\in \\mathscr{T}$. Thus, $$ \\bigcup_{\\alpha \\in \\Lambda} V_\\alpha = \\bigcup_{\\alpha \\in \\Lambda} (A \\cap U_\\alpha ) =A\\cap (\\cup_{\\alpha \\in \\Lambda} U_\\alpha ) =A\\cap U \\in \\mathscr{T}_{A} $$\nand hence $\\bigcup _{\\alpha \\in \\Lambda} V_\\alpha \\in \\mathscr{T}_{A}$ is true.\n$(T3)$: Let\u0026rsquo;s assume $V_{1},\\ \\cdots\\ ,V_{n} \\in \\mathscr{T}_{A}$. Similarly, for each $V_{i}$, there exists $U_{i}$ satisfying $V_{i} =A \\cap U_{i}$. And since $U=\\cap _{i} U_{i} \\in \\mathscr{T}$, $$ \\bigcap _{i=1}^n V_{i} = \\bigcap_{i=1}^n (A\\cap U_{i}) = A\\cap \\left( \\bigcap_{i=1}^n U_{i} \\right) =A\\cap U \\in \\mathscr{T}_{A} $$\nis true. Therefore, $\\bigcap_{i=1}^n V_{i} \\in \\mathscr{T}_{A}$ is true.\nBy satisfying three conditions of topology, $\\mathscr{T}_{A}$ is a topology on $A$.\n■\n[a1] Refer to the proof in metric spaces. It\u0026rsquo;s trivial by the definition of $\\mathscr{T}_{A}$.\n■\n[b1] $(\\implies)$ If $F$ is a closed set in $A$, then $A-F$ is an open set in $A$. Hence, by [a1], there exists an open set $U$ in $X$ satisfying $A-F=A\\cap U$. Since $U$ is an open set, $E=X-U$ is a closed set in $X$. Then,\n$$ A\\cap E=A\\cap (X-U)=A-(A\\cap U)=A-(A-F)=F $$\n$(\\Longleftarrow )$ If $E$ is a closed set in $X$, then $X-E$ is an open set in $X$. Thus, by [a1], $A \\cap (X-E)$ is an open set in $A$. Since $F^c=A-(A\\cap E)=A\\cap(X-E)$, $F ^c$ is an open set in $A$. Therefore, $F$ is a closed set in $A$.\n■\n[a2] If $B$ is an open set in $A$, by [a1], there exists an open set $U$ in $X$ satisfying $B=A\\cap U\\ (U\\in \\mathscr{T})$. By assumption, $A$ is an open set in $X$. Therefore, $B$ is the intersection of open sets in $X$, thus an open set in $X$.\n■\n[b2] If $B$ is a closed set in\n$A$, by [b1], there exists a closed set $E$ in $X$ satisfying $B=A\\cap E$. By assumption, $A$ is a closed set in $X$, and $B$ is the intersection of closed sets, thus $B$ is also a closed set in $X$.\n■\n[3] Part 1. $\\mathscr{B}_{A}$ is a basis for $A$.\n[b1]: For any $x\\in A$, since $A\\subset X$, $x\\in X$ is true. Since $\\mathscr{B}$ is a basis of $X$, by definition, there exists $B$ satisfying $x \\in B \\in \\mathscr{B}$. Hence, there exists $A\\cap B \\in \\mathscr{B}_{A}$ satisfying $x\\in (A\\cap B ) \\in \\mathscr{B}_{A}$. [b2]: For any $A\\cap B_{1}$, $A\\cap B_{2}$, and $x\\in \\Big( (A\\cap B_{1} ) \\cap (A \\cap B_{2}) \\Big)$, $$ (A\\cap B_{1})\\cap (A \\cap B_{2})=A\\cap B_{1}\\cap B_{2} $$ so, $x\\in (B_{1}\\cap B_{2})$ is true. Since $\\mathscr{B}$ is a basis of $X$, by definition, there exists $B_{3}$ satisfying $x\\in B_{3} \\subset ( B_{1}\\cap B_{2})$. Therefore, $$ x \\in (A\\cap B_{3})\\subset \\Big( A\\cap (B_{1}\\cap B_{2}) \\Big)=(A\\cap B_{1}) \\cap (A\\cap B_{2}) $$ As it satisfies the two conditions for being a basis according to conditions for a basis, $\\mathscr{B}_{A}$ is the basis for subset $A$.\nPart 2. $\\mathscr{T}_{\\mathscr{B}_{A}}=\\mathscr{T}_{A}$ is true.\n$(\\subset)$ Since $\\mathscr{B}$ is a basis of $(X,\\mathscr{T})$, $\\mathscr{T}_{\\mathscr{B}}=\\mathscr{T}$ and, therefore, $\\mathscr{B}\\subset \\mathscr{T_{\\mathscr{B}}}=\\mathscr{T}$ is true. Hence, for every $B \\in \\mathscr{B}$, $B\\in \\mathscr{T}$ is true. By the definition of $\\mathscr{T}_{A}$, $A\\cap B \\in \\mathscr{T}_{A}$ is true. Thus, $$ \\mathscr{B}_{A} \\subset \\mathscr{T}_{A} $$ $\\mathscr{T}_{\\mathscr{B}_{A}}$ is the smallest topology containing $\\mathscr{B}_{A}$, so $$ \\mathscr{T}_{\\mathscr{B}_{A}} \\subset \\mathscr{T}_{A} $$\n$(\\supset )$ Assume $V \\in \\mathscr{T}_{A}$. By [a1], there exists $U\\in \\mathscr{T}$ satisfying $V=A\\cap U$. For any point $x\\in V \\subset A$ of $V$, $x \\in U$ is true. Since $\\mathscr{B}$ is the basis generating $\\mathscr{T}$, there exists $B\\in \\mathscr{B}$ satisfying $x\\in B \\subset U$. Therefore, $A \\cap B \\in \\mathscr{B}_{A}$ satisfies $$ x\\in (A\\cap B) \\subset (A\\cap U) =V $$ This meets the condition for $V$ to belong to the topology $\\mathscr{T}_{\\mathscr{B}_{A}}$ generated by $\\mathscr{B}_{A}$, so $V \\in \\mathscr{T}_{\\mathscr{B}_{A}}$ is true. Therefore, $$ \\mathscr{T}_{\\mathscr{B}_{A}} \\supset \\mathscr{T}_{A} $$ ■\nMunkres. (2000). Topology(2nd Edition): p89.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1439,"permalink":"https://freshrimpsushi.github.io/en/posts/1439/","tags":null,"title":"Subspace Topology, Relative Topology"},{"categories":"줄리아","contents":"Code julia\u0026gt; typeof(0)\rInt64\rjulia\u0026gt; typeof(0.0)\rFloat64\rjulia\u0026gt; typeof(0 == 0.0)\rBool\rjulia\u0026gt; typeof(Bool)\rDataType\rjulia\u0026gt; typeof(NaN)\rFloat64\rjulia\u0026gt; typeof(Inf)\rFloat64\rjulia\u0026gt; typeof(\u0026#39;O\u0026#39;)\rChar\rjulia\u0026gt; typeof(\u0026#34;Ohmygirl\u0026#34;)\rString\rjulia\u0026gt; typeof(\u0026#34;O\u0026#34;)\rString Julia includes all sorts of types. $0$ and $0.0$ are the same $0$, but have different types, and as you can see, even a type like Bool has a type named DataType. Like in C, String is an array of Chars, differentiated by whether it uses double or single quotes.\n","id":1379,"permalink":"https://freshrimpsushi.github.io/en/posts/1379/","tags":null,"title":"Julia's Types and Annotations"},{"categories":"줄리아","contents":"Overview Julia has been developed at MIT and publicly released in 2012, aiming for a language that is both highly productive and fast. It achieves speeds comparable to C or Fortran while also providing a high-level syntax similar to Python or R, among absorbing benefits from various other languages. As of November 2019, it\u0026rsquo;s true that Julia is somewhat lagging due to the rapid advancement of GPUs and the prevalence of deep learning, but there is still no language that surpasses Julia in terms of convenience and speed in the academic field.\nIf you have any questions or comments about Julia, post them on the Freshrimp Sushi Community 💬.\nKey Features Let\u0026rsquo;s take a look at Julia\u0026rsquo;s features:\nIt\u0026rsquo;s fast. No other words are needed. The benchmarks show that it is on par with C and Fortran. It\u0026rsquo;s simple. If optimization isn\u0026rsquo;t a concern, programming in Julia is quite similar to Matlab, R, or Python. The unnecessary hassle of rewriting the same code in C for optimization after implementing a program in these languages does not exist. Instead, it only requires improving through coding techniques appropriate for Julia. Similarly, if the user is familiar with Matlab, R, or Python, picking up Julia can be quick. In reality, due to the high productivity of these languages, Julia feels easy regardless of the base programming language. It\u0026rsquo;s free. MATLAB supports powerful linear algebra but is expensive, and Julia can be 10 to 1,000 times faster depending on the optimization. Additionally, using the MATLAB.jl package allows for writing code in a similar style to MATLAB, making it convenient for users skilled in MATLAB to transition. Packages from other languages can be easily imported. The biggest weakness of a new language, and one of the main reasons languages like Fortran or Python are used, is the package ecosystem. For Python, the package PyCall.jl enables direct calling of Python functions. Although Julia is sufficiently high-performing on its own, it allows calling C or Fortran functions through the ccall() function. C++ is also supported through the Cpp.jl package. Specialized for parallel processing. Unlike other languages that have developed related packages after the fact, Julia was developed from the ground up with parallel processing in mind. Indeed, depending on the program, it could be not just convenient but central to optimization. ","id":1374,"permalink":"https://freshrimpsushi.github.io/en/posts/1374/","tags":null,"title":"Julia Programming Language"},{"categories":"확률론","contents":"Theorem Given a probability space $( \\Omega , \\mathcal{F} , P)$:\n[1] From measure theory: If measurable functions $f$, $g$ are $\\mathcal{F}$-measurable, then there exists a Borel function $h : \\mathbb{R} \\to \\mathbb{R}$ satisfying $g = h (f)$. [2] Application in probability theory: If random variables $X$, $Y$ are $\\sigma (X)$-measurable, then there exists a Borel function $h : \\mathbb{R} \\to \\mathbb{R}$ satisfying $E(Y | X) = h(X)$. [3]: If $X$ is $\\mathcal{F}$-measurable, then $$E(X|\\mathcal{F}) =X \\text{ a.s.}$$. [4]: For sigma field $\\mathcal{G} = \\left\\{ \\emptyset , \\Omega \\right\\}$, $$E(X|\\mathcal{G}) = E(X) \\text{ a.s.}$$. [5]: For constant $c$ and all sigma fields $\\mathcal{G}$, $$E(c|\\mathcal{F}) = c \\text{ a.s.}$$. [6]: For constant $c$, $$E(cX | \\mathcal{G}) = c E(X | \\mathcal{G}) \\text{ a.s.}$$. [7]: $$E(X+Y | \\mathcal{G}) = E(X | \\mathcal{G}) + E(Y| \\mathcal{G}) \\text{ a.s.}$$. [8]: If $X \\ge 0 \\text{ a.s.}$, then $$E(X | \\mathcal{G}) \\ge 0 \\text{ a.s.}$$. [9]: If $X \\ge Y \\text{ a.s.}$, then $$E(X | \\mathcal{G}) \\ge E(Y | \\mathcal{G}) \\text{ a.s.}$$. [10]: $$\\left| E( X | \\mathcal{G} ) \\right| \\le E ( | X | | \\mathcal{G} ) \\text{ a.s.}$$. [11]: For all sigma fields $\\mathcal{G}$, $$E \\left[ E ( X | \\mathcal{G} ) \\right] = E(X)$$. $\\sigma (X) = \\left\\{ X^{-1} (B) : B \\in \\mathcal{B}(\\mathbb{R}) \\right\\}$ represents the smallest sigma field generated by the random variable $X$, denoted as $\\Omega$. A function $Z$ being $\\mathcal{F}$-measurable means for all Borel sets $B \\in \\mathcal{B}(\\mathbb{R})$, $Z^{-1} (B) \\in \\mathcal{F}$ holds. A Borel function is a function $f : \\mathbb{R} \\to \\mathbb{R}$ for which $f^{-1} (B)$ is a Borel set for all Borel sets $B \\in \\mathcal{B}(\\mathbb{R})$. Explanation [1],[2]: These theorems indicate that the conditional expectation of $Y$ given $X$ can be represented by some function dependent on $X$. Specifically, given the value of $X$, it is represented as $E(Y | X = a) = h(a)$. [2] is a corollary of [1], ensuring that basic properties of expectation commonly used in elementary probability theory are almost surely guaranteed. Linearity [5]~[7]: The linearity of expectation, denoted as $E(aX + b | \\mathcal{G}) = aE(X | \\mathcal{G}) + b$, is preserved even conditionally. Sigma Fields as Information [3] $E(X | \\mathcal{F}) = X$: Considering the meaning of the formula, if the random variable $X$ is $\\mathcal{F}$-measurable, it means that the sigma field $\\mathcal{F}$ has all the information about $X$. Conversely, this is why it\u0026rsquo;s called measurable. Therefore, $E(X|\\mathcal{F})$ can be directly perceived without any interference. A $X$, fully known on $\\mathcal{F}$, doesn\u0026rsquo;t need to be calculated through $E$. For example, when playing a game where you earn 1 dollar for each face of a 6-sided die, the expected earnings are 3.5 dollars. This is calculated because we don\u0026rsquo;t know which face of the die will appear. However, if the sigma field $\\mathcal{F}$ is precisely given in my mind before throwing the die, the face of the die $X$ can be accurately measured, and thus how many dollars will be received is known. Even if one has to pay 3.5 dollars each game, by not playing the losing games and only playing the winning ones, one can avoid losses. In this sense, random number hacking corresponds to an attack technique that steals the sigma field (random number table) and makes what should have been random deterministic. If successful, cryptographic systems relying on randomness, such as bank security cards or OTPs, are compromised. Since $\\sigma (X)$ is defined as the smallest sigma field knowing all about $X$, it naturally follows $E(X| \\sigma (X)) = X$, denoted as $E(X|X) = X$ according to the introduced notation.\n[4] $E(X|\\mathcal{G}) = E(X)$: Considering the meaning of the formula, the trivial sigma field $\\mathcal{G} = \\left\\{ \\emptyset , \\Omega \\right\\}$ gives no information about $X$, thus necessitating a scan of the entire probability space $\\Omega$ to calculate $\\displaystyle \\int_{\\Omega} X d P$. [10] $\\left| E( X | \\mathcal{G} ) \\right| \\le E ( | X | | \\mathcal{G} )$: Following the properties of absolute value, $$ - E ( | X | | \\mathcal{G} ) \\le E( X | \\mathcal{G} ) \\le E ( | X | | \\mathcal{G} ) $$ [11] $E \\left[ E ( X | \\mathcal{G} ) \\right] = E(X)$: This equality is useful in various proofs in probability theory, mainly used as a trick when $E(X)$ is difficult to calculate directly, but given some $\\mathcal{G}$, $E(X|\\mathcal{G})$ becomes easier to compute. Proof [1] Define $h : \\mathbb{R} \\to \\mathbb{R}$ for $z \\in \\mathbb{R}$ as $h(z) := \\left( g \\circ f^{-1} ( \\left\\{ z \\right\\} ) \\right)$.\nIf $\\left\\{ z \\right\\} \\in \\mathcal{B}(\\mathbb{R})$, since $f$ is $\\mathcal{F}$-measurable, $f^{-1}(\\left\\{ z \\right\\}) \\in \\mathcal{F}$ holds, and since $g$ is also $\\mathcal{F}$-measurable, $h$ is well-defined and satisfies $g (\\omega) = ( h \\circ f ) ( \\omega )$.\nFor all Borel sets $B \\in \\mathcal{B}(\\mathbb{R})$, consider $$ h^{-1}(B) = (f \\circ g^{-1})(B) = f \\left( g^{-1} (B) \\right) $$, since $g^{-1} (B) \\in \\mathcal{F}$, we have $f(g^{-1} (B) ) \\in \\mathcal{B}(\\mathbb{R})$. As $h^{-1}(B) \\in \\mathcal{B}(\\mathbb{R})$ holds for all $B \\in \\mathcal{B}(\\mathbb{R})$, $h$ is a Borel function.\n■\n[2] $E ( Y | X ) = E ( Y | \\sigma (X) )$ is a $\\sigma (X)$-measurable random variable by the definition of conditional expectation, and $X$ is also a $\\sigma (X)$-measurable random variable by definition. Therefore, by [1], let\u0026rsquo;s denote $\\mathcal{F} = \\sigma (X)$ and set $$ f = X \\\\ g = E ( Y | X ) $$, then there exists a Borel function $h : \\mathbb{R} \\to \\mathbb{R}$ satisfying $E(Y|X) = h(X)$.\n■\nStrategies [3]~[7]: Convert to integral form, show that definite integrals are equal, and then apply the following theorem, named Lebesgue Integration Lemma for this post.\nProperties of Lebesgue Integration: $$ \\forall A \\in \\mathcal{F}, \\int_{A} f dm = 0 \\iff f = 0 \\text{ a.e.} $$\n[3] Since $X$ uniquely exists satisfying $\\displaystyle \\int_{A} X dP = \\int_{A} X dP$ for all $A \\in \\mathcal{F}$, by the definition of conditional expectation, $X = E(X| \\mathcal{F})$ is the conditional expectation of $X$ with respect to $\\mathcal{F}$. Therefore, for all $A \\in \\mathcal{F}$, $$ \\int_{A} E(X |\\mathcal{F}) dP = \\int_{A} X dP $$ and by the Lebesgue Integration Lemma\n, we have $X = E(X |\\mathcal{F}) \\text{ a.s.}$.\n■\n[4] By the definition of conditional expectation, we have $\\displaystyle \\int_{A} E(X |\\mathcal{G}) dP = \\int_{A} X dP$.\nCase 1. $A = \\emptyset$\n$$ 0 = \\int_{\\emptyset} E(X |\\mathcal{G}) dP = \\int_{\\emptyset} X dP = 0 $$\nCase 2. $A = \\Omega$\n$$ \\int_{\\Omega} E(X |\\mathcal{G}) dP = \\int_{\\Omega} X dP = E(X) = E(X) P(\\Omega) = E(X) \\int_{\\Omega} 1 dP = \\int_{\\Omega} E(X) dP $$\nIn either case, by the Lebesgue Integration Lemma, we have $X = E(X |\\mathcal{G}) \\text{ a.s.}$.\n■\n[5] Given $c \\in \\mathcal{G}$ and $E(c | \\mathcal{G}) \\in \\mathcal{G}$, by the definition of conditional expectation, for all $A \\in \\mathcal{G}$, $$ \\int_{A} E(c |\\mathcal{G}) dP = \\int_{A} X dP $$ and therefore, by the Lebesgue Integration Lemma, we have $c = E(c | \\mathcal{G}) \\text{ a.s.}$.\n■\n[6] By the definition of conditional expectation and the linearity of Lebesgue integration, for all $A \\in \\mathcal{G}$, $$ \\begin{align*} \\int_{A} E( cX |\\mathcal{G}) dP =\u0026amp; \\int_{A} cX dP \\\\ =\u0026amp; c \\int_{A} X dP \\\\ =\u0026amp; c \\int_{A} E(X|\\mathcal{G}) dP \\\\ =\u0026amp; \\int_{A} c E(X|\\mathcal{G}) dP \\end{align*} $$ and by the Lebesgue Integration Lemma, we have $E( cX |\\mathcal{G}) = c E(X|\\mathcal{G}) dP \\text{ a.s.}$.\n■\n[7] By the definition of conditional expectation and the linearity of Lebesgue integration, for all $A \\in \\mathcal{G}$, $$ \\begin{align*} \\int_{A} E( X+Y |\\mathcal{G}) dP =\u0026amp; \\int_{A} (X+Y) dP \\\\ =\u0026amp; \\int_{A} X dP +\\int_{A} Y dP \\\\ =\u0026amp; \\int_{A} E(X|\\mathcal{G}) dP + \\int_{A} E(Y|\\mathcal{G}) dP \\\\ =\u0026amp; \\int_{A} \\left[ E(X|\\mathcal{G}) + E(Y|\\mathcal{G}) \\right] dP \\end{align*} $$ and by the Lebesgue Integration Lemma, $$ E( X +Y |\\mathcal{G}) = E(X|\\mathcal{G}) + E(Y|\\mathcal{G}) dP \\text{ a.s.} $$\n■\n[8] Assuming $E( X |\\mathcal{G}) \u0026lt; 0$ leads to a contradiction, thus $E( X |\\mathcal{G}) \\ge 0 \\text{ a.s.}$ must hold.\n■\n[9] Assuming $Z := X - Y \\ge 0$ and by [8], $$ E(X-Y | \\mathcal{G}) \\ge 0 $$ and by the linearity of conditional expectation, $$ E(X| \\mathcal{G}) - E(Y | \\mathcal{G}) \\ge 0 \\text{ a.s.} $$\n■\n[10] Part 1. $X \\ge 0$\nIf $X \\ge 0$, then $|X| = X$ holds, leading to $$ E( |X| |\\mathcal{G}) = E(X|\\mathcal{G}) $$\nBy [8], we have $E(X|\\mathcal{G}) \\ge 0$, similarly leading to $E(X|\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right|$, $$ E( |X| |\\mathcal{G}) = E(X|\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right| $$\nPart 2. $X \u0026lt; 0$\nBy [6], $$ E( |X| |\\mathcal{G}) = E( -X |\\mathcal{G}) = - E(X |\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right| $$\nPart 3. $X = X^{+} - X^{-}$\nBy the triangle inequality, $$ \\left| E(X|\\mathcal{G}) \\right| \\le \\left| E( X^{+} |\\mathcal{G}) \\right| + \\left| E( X^{-} |\\mathcal{G}) \\right| $$ Given $X^{+} , X^{-} \\ge 0$, by Part 1, $$ \\left| E(X|\\mathcal{G}) \\right| \\le E( \\left| X^{+} \\right| |\\mathcal{G}) + E( \\left| X^{-} \\right| | \\mathcal{G}) $$\nBy [7] and the expression of absolute value $|f| = |f^{+}| + |f^{-}|$, $$ \\begin{align*} \\left| E(X|\\mathcal{G}) \\right| \\le \u0026amp; E( \\left| X^{+} \\right| + \\left| X^{-} \\right| | \\mathcal{G}) \\\\ =\u0026amp; E( \\left| X \\right| | \\mathcal{G}) \\text{ a.s.} \\end{align*} $$\n■\n[11] $$ \\begin{align*} E \\left[ E( X | \\mathcal{G} ) \\right] =\u0026amp; \\int_{\\Omega} E ( X | \\mathcal{G} ) d P \\\\ =\u0026amp; \\int_{\\Omega} X d P \\\\ =\u0026amp; E(X) \\end{align*} $$\n■\n","id":1322,"permalink":"https://freshrimpsushi.github.io/en/posts/1322/","tags":null,"title":"Properties of Conditional Expectation"},{"categories":"집합론","contents":"Definition 1 Set: A collection of distinct objects that serves as the subject of our intuition or thought is called a set. Element: An object belonging to a set is called an element. Propositional Function: For an element $x$ in the set $U$, a proposition $p(x)$ that is either true or false is referred to as a propositional function on $U$. Explanation In mathematics, the concept of sets is nearly as fundamental as a native language. Perhaps what makes it even better than natural language is its ability to eliminate inherent ambiguity and allow logical reasoning based solely on its definition and form. Typically, elements are represented in lowercase, and sets in uppercase. If $a$ belongs to $A$, it is written as $a \\in A$, and we say \u0026ldquo;$a$ is an element of $A$.\u0026rdquo; Of course, there is no strict requirement to represent elements and sets with alphabetic uppercase and lowercase, and it is common to denote the set of all natural numbers as $\\mathbb{N}$, where $N \\in \\mathbb{N}$ would also be acceptable. Enumeration: The set of natural numbers $\\mathbb{N}$ can be expressed as $\\left\\{1, 2, 3, \\cdots \\right\\}$. This representation, where the elements of the set are explicitly listed, is called enumeration. Set-Builder Notation: In contrast to enumeration, a set can also be represented as the collection of elements that satisfy certain conditions. For example, if we want to represent the set of natural numbers greater than $5$, we can express it as $\\left\\{ x \\in \\mathbb{N} : x \u0026gt; 5 \\right\\}$. This notation is known as set-builder notation or conditional specification. It is important to note that a propositional function is defined by the proposition function itself. Although it conforms to the definition of a function in set theory, it is crucial to recognize that it can be defined solely in terms of propositions, without depending on set theory. This clarity is essential to avoid difficulties in freely using set-builder notation, as circular definitions of functions could arise if this aspect is unclear. Additionally, propositional functions are also referred to as logical expressions. Heung-Chun Lee, You-Feng Lin. (2011). Set Theory: An Intuitive Approach, p.47, 73, 81, 85.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1316,"permalink":"https://freshrimpsushi.github.io/en/posts/1316/","tags":null,"title":"The Definitions of Sets and Propositional Functions"},{"categories":"집합론","contents":"Definition 1 A statement that is either true or false is called a proposition. Propositions have a single truth value – either true or false. If the truth values of two propositions $p$ and $q$ are the same, then $p$ and $q$ are said to be logically equivalent, denoted as $p \\equiv q$. The following symbols are used as connectives to form compound propositions:\nNegation: $\\lnot$ Conjunction: $\\land$ Disjunction: $\\lor$ Conditional: $\\to$ Biconditional: $\\leftrightarrow$ Truth Table Usually, true is represented by $T$, and false by $F$. The truth values of propositions formed by applying the connectives can be conveniently verified using a truth table:\nNegation If $p$ is true, then $\\lnot p$ is false, and if $p$ is false, then $\\lnot p$ is true.\n$\\text{NOT}$ gate Conjunction If both $p$ and $q$ are true, then $p \\land q$ is true; otherwise, it is false. In fields like computer engineering, $0$ is often considered false, and any non-zero number is considered true. Considering two non-zero numbers $a$ and $b$, $a \\times b = ab \\ne 0$ is true, but if either $a$ or $b$ is $0$, then $a \\times b = 0$ is false. In this sense, $\\land$ is called logical \u0026lsquo;AND\u0026rsquo;.\n$\\text{AND}$ gate Disjunction If at least one of $p$ and $q$ is true, then $p \\lor q$ is true; it is false only when both are false. Similar to conjunction, $\\lor$ is called logical \u0026lsquo;OR\u0026rsquo;. Of course, if $b = -a \\ne 0$, then $a$ and $b$ are both true, but $a+b = 0$ is false. Let\u0026rsquo;s not dwell on this exception; it\u0026rsquo;s why it\u0026rsquo;s called \u0026rsquo;logical OR\u0026rsquo; and not just \u0026lsquo;OR\u0026rsquo;.\n$\\text{OR}$ gate Conditional If $p$ is true and $q$ is true, then $p \\to q$ is true. It\u0026rsquo;s important to note that, contrary to natural language, if $p$ is false, $p \\to q$ is true regardless of the truth value of $q$. Additionally, $p \\to q \\equiv \\lnot p \\lor q$, which can be easily proven through a truth table. Refer to the bottom of the main text.\nBiconditional If both $p \\to q$ and $q \\to p$ are true, then $p \\leftrightarrow q$ is true. In mathematical notation, $(p \\to q) \\land (q \\to p) \\equiv p \\leftrightarrow q$. In terms of a truth table, $p \\leftrightarrow q$ is true only when $p$ and $q$ have the same truth value.\nProof of Conditional Based on the definitions of negation and disjunction:\n■\nHeung-Chun Lee, You-Feng Lin. (2011). Set Theory: An Intuitive Approach, pp. 3-21.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1299,"permalink":"https://freshrimpsushi.github.io/en/posts/1299/","tags":null,"title":"Propositions and Connectives, Truth Tables"},{"categories":"측도론","contents":"Theorem1 (a) Let $\\nu$ be a signed measure defined on a measurable space $(X, \\mathcal{E})$. Then there exist a positive set $P$ and a negative set $N$ for $\\nu$, satisfying the following:\n$$ P \\cup N=X \\quad \\text{and} \\quad P \\cap N =\\varnothing $$\nSuch a $X=P \\cup N$ is called a Hahn decomposition for $\\nu$.\n(b) Let $P^{\\prime}, N^{\\prime}$ be another pair of sets satisfying (a). Then the following sets are null sets for $\\nu$:\n$$ (P-P^{\\prime}) \\cup (P^{\\prime}-P)=(N-N^{\\prime}) \\cup (N^{\\prime}-N) $$\nThis can be denoted using the symmetric difference symbol as follows:\n$$ P\\Delta P^{\\prime}=N\\Delta N^{\\prime} $$\nExplanation (a) For any given measurable space, it is possible to separate the set $X$ into positive and negative sets with respect to $\\nu$ defined on the measurable space.\n(b) As stated above, even if there are multiple ways to divide the set $X$, essentially, there is no difference. $P$ and $P^{\\prime}$, $N$ and $N^{\\prime}$ always differ by only a null set, so they may be different from the set perspective but are the same from the measure perspective.\nProof Strategies: The** proof of this theorem itself is not very difficult, but the flow of the proof is not trivial, so I will explain it concretely before starting. First, define some positive set $P$. Then define $N$ as $N:=X-P$. If $N$ is a negative set, then the proof for (a) is complete. Before proving that $N$ is a negative set, we will verify that $N$ has two properties as defined above. Finally, we will use proof by contradiction. Assuming $N$ is not a negative set, we will complete the proof by showing that a contradiction arises using the two properties.\nWithout loss of generality, assume that $\\nu$ does not take the value $+\\infty$. For the other case, the same proof applies by considering $-\\nu$. Let $C$ be the collection of all positive sets in $\\mathcal{E}$. Then, by assumption, $\\nu$ does not take the value $+\\infty$, so there exists $M$ defined as below:\n$$ M:=\\sup \\limits_{P \\in C } \\nu (P) \u0026lt; \\infty $$\nNow, we can show that there exists a maximizer $P$ satisfying $\\nu (P)=M$. Consider the following maximizing sequence $\\left\\{ P_{j} \\right\\}$:\n$$ \\lim \\limits_{j \\rightarrow \\infty} \\nu (P_{j})=M $$\nSince there is no containment relationship between $P_{j}$\u0026rsquo;s, consider the following $\\tilde{P_{j}}$:\n$$ \\tilde{P_{j}} :=\\bigcup \\limits_{k=1}^j P_{k} $$\nThen, $\\nu (P_{j}) \\le \\nu (\\tilde{P_{j}}) \\le M$, so $\\left\\{ \\tilde{P_{j}} \\right\\}$ is a maximizing sequence. Also, it is obvious that $\\tilde{P_{1}} \\subset \\tilde{P_2}\\subset \\cdots $ by definition. Now, define $P$ as follows:\n$$ P := \\bigcup \\limits_{j=1}^\\infty \\tilde{P_{j}} $$\nThen, the following holds:\n$$ \\nu (P)=\\lim \\limits_{j\\rightarrow \\infty} \\nu (\\tilde{P_{j}})=M $$\nThus, we have shown the existence of a maximizer satisfying $\\nu (P)=M$. Moreover, since $P$ is the countable sum of positive sets, it is a positive set. In fact, such $P$ and $N:=X-P$ are the decomposition mentioned in the theorem. The process of proving that $N$ is such a negative set remains. Let\u0026rsquo;s now consider $N:=X \\setminus P$. As explained above, the proof ends if we show that $N$ is a negative set. First, let\u0026rsquo;s prove that such $N$ has the following two properties:\nClaim 1 $N$ does not contain any positive set with a measure value greater than $0$. In other words, it does not contain any non-null positive set. That is, if $\\nu (E)\u0026gt;0$ and $E$ is a positive set, then $E \\not \\subset N$.\nNote that it is possible for a set $E \\subset N$ to exist that is neither a positive nor a negative set. In other words, a subset of $N$ can be 1. a null set, 2. a negative set, or 3. a set that is neither positive nor negative.\nProof\nSuppose $E\\subset N$ is a positive set and $\\nu (E) \u0026gt;0$. Then, by the definition of $N$, $E$ and $P$ are disjoint sets. Therefore, the following holds:\n$$ \\nu (P \\cup E)=\\nu (P)+\\nu (E) $$\nHowever, since $\\nu (P)=M$, the following holds:\n$$ \\nu (P \\cup E)=\\nu (P)+\\nu (E)\u0026gt;M $$\nBut this contradicts the assumption that $M=\\sup \\nu (F)\\ \\forall F\\in \\mathcal{E}$. Therefore, there does not exist $E \\subset N$ such that $E$ is a positive set and $\\nu (E)\u0026gt;0$.\nClaim 2 If $A \\subset N$ and $\\nu (A)\u0026gt;0$, then there exists $B \\subset A$ such that $\\nu (B) \u0026gt; \\nu (A)$.\nProof\nLet\u0026rsquo;s say $A \\subset N$ and $\\nu (A)\u0026gt;0$. Then, by Claim 1, $A$ is not a positive set. Therefore, it is neither a null set nor a positive set. Hence, there exists $C$ satisfying the following conditions2:\n$$ C \\subset A,\\ \\nu (C) \u0026lt;0 $$\nNow, let\u0026rsquo;s define $B:=A-C$. Then, the following holds:\n$$ \\nu (A)=\\nu (B)+\\nu (C) \u0026lt; \\nu (B) $$\nNow, let\u0026rsquo;s assume $N$ is not a negative set. By showing that a contradiction arises using the above two properties, we prove that $N$ is a negative set.\nPart 1.\nLet $\\left\\{ A_{j} \\right\\}$ be a sequence of subsets of $N$. Let $\\left\\{ n_{j} \\right\\}$ be a sequence of natural numbers. Assuming $N$ is not a negative set, there exists some $B \\subset N$ with $\\nu (B) \u0026gt;0$. Let\u0026rsquo;s say the smallest $n_{j}$ satisfying $\\nu (B) \u0026gt; \\frac{1}{n_{j}}$ is $n_{1}$, and let\u0026rsquo;s call such $B$ as $A_{1}$. Since $\\nu (B)=\\nu (A_{1})\u0026gt;0$, the process done for $N$ can be applied to $A_{1}$ equally.\nPart 2\nAgain, there exists some $B\\subset A_{1}$ with $\\nu (B)\u0026gt;0$, and by Claim 2, $\\nu (B) \u0026gt; \\nu (A_{1})$. Therefore, there exists a natural number $n$ such that $\\nu (B) \u0026gt; \\nu (A_{1})+\\frac{1}{n}$. Let\u0026rsquo;s call the smallest such natural number $n_2$, and such $B$ as $A_2$.\nPart 3\nRepeating the same process, $n_{j}$ is the smallest natural number for which there exists some $B \\subset A_{j-1}$ satisfying $\\nu (B)\u0026gt;\\nu (A_{j-1}) + \\dfrac{1}{n_{j}}$, and such $B$ is called $A_{j}$. Now, let\u0026rsquo;s define $A=\\bigcap \\nolimits_{1}^\\infty A_{j}$. Since $\\nu$ is assumed not to take the value $+\\infty$ and by the property of signed measure $(B)$, the following holds:\n$$ \\begin{align*} +\\infty \\gt \\nu (A) \u0026amp;= \\nu \\left(\\bigcap \\nolimits_{1}^\\infty A_{j} \\right) \\\\ \u0026amp;= \\lim \\limits_{j \\rightarrow \\infty} \\nu (A_{j}) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{j-1}) +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{j-2}) + \\frac{1}{n_{j-1}} +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\vdots \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{1}) + \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\frac{1}{n_{1}}+ \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;= \\sum \\limits_{j=1}^\\infty \\frac{1}{n_{j}} \\end{align*} $$\nSince the series is finite, the limit is $0$.\n$$ \\lim \\limits_{j\\rightarrow \\infty} \\frac{1}{n_{j}} =0 $$\nTherefore, we obtain the following:\n$$ \\begin{equation} \\lim \\limits_{j\\rightarrow \\infty} n_{j} =\\infty \\label{eq1} \\end{equation} $$\nHowever, as seen in Part 1, by Claim 2, there exists some natural number $n$ for which there exists $B \\subset A$ satisfying $\\nu (B) \u0026gt; \\nu (A) +\\dfrac{1}{n}$. Then, by the definition of $A$, $A \\subset A_{j-1}$, and by Claim 2, the sequence $\\left\\{ \\nu (A_{j}) \\right\\}$ is increasing. Therefore, since $\\nu (A) =\\lim \\limits_{j \\rightarrow \\infty} \\nu (A_{j})$, $\\nu (A) \u0026gt; \\nu (A_{j-1})$.\nAdditionally, by $(1)$, for sufficiently large $j$, $n_{j} \u0026gt;n$. Therefore, the following holds:\n$$ \\nu (B) \u0026gt; \\nu (A) +\\frac{1}{n}\u0026gt;\\nu (A_{j-1}) +\\frac{1}{n} \u0026gt; \\nu (A_{j-1}) +\\frac{1}{n_{j}} $$\nBut this contradicts the definition of $n_{j}$ and $A_{j}$. Therefore, the assumption that $N$ is not a negative set is wrong. Hence, $N$ is a negative set.\nLet\u0026rsquo;s consider $P^{\\prime}$, $N^{\\prime}$ as another decomposition satisfying the theorem. Then, the following holds:\n$$ P^{\\prime} \\cup N^{\\prime} =X \\quad \\text{and} \\quad P^{\\prime}\\cap N^{\\prime} =\\varnothing $$\nTherefore, we can see that $P-P^{\\prime} \\subset P$, $P-P^{\\prime}\\subset N^{\\prime}$. Then, $P-P^{\\prime}$ is both a positive set and a negative set, which is only possible if it\u0026rsquo;s a null set, so $P-P^{\\prime}$ is $\\nu-\\mathrm{null}$. Similarly, $\\nu -\\mathrm{null}$ can be shown for $P^{\\prime}-P$, $N-N^{\\prime}$, and $N^{\\prime}-N$.\n■\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf not, A must be either a null set or a positive set by definition.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1308,"permalink":"https://freshrimpsushi.github.io/en/posts/1308/","tags":null,"title":"Hahn Decomposition Theorem"},{"categories":"바나흐공간","contents":"The Hahn-Banach Theorem for Real Numbers1 Let $X$ be a $\\mathbb{R}$-vector space and assume that $Y \\subset X$. Let us define $p : X \\to \\mathbb{ R}$ as a sublinear linear functional of $X$. Now, assume that $y^{\\ast} : Y \\to \\mathbb{ R}$ satisfies the following condition as a $\\mathbb{R}$-linear functional of $Y$.\n$$ y^{\\ast}(y) \\le p(y)\\quad \\forall y\\in Y $$\nThen, there exists a linear functional $x^{\\ast} : X \\to \\mathbb{R}$ of $X$ that satisfies the following conditions:\n(a) $x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$\n(b) $x^{\\ast}(x) \\le p(x),\\quad \\forall x \\in X$\nExplanation To say that $\\mathbb{R}-$ is a vector space means it is a vector space over the field $\\mathbb{R}$. In other words, it means that the conditions for scalar multiplication of a vector space, $(M1)$~$(M5)$, hold true for real numbers. Similarly, the term $\\mathbb{R}$-linear means that the two properties of linearity, specifically scalar multiplication, hold true for real numbers.\nSince $X, Y$ is a $\\mathbb{R}$-vector space, the terms linear and $\\mathbb{R}$-linear mean the same. If this concept is confusing, think of $\\mathbb{R}$-, $\\mathbb{C}$- as non-existent in this text for ease of understanding the proof. Later, when applying the Hahn-Banach theorem to normed spaces, the function $p$ corresponds to the norm. The proof of the theorem stated above is omitted and will be used as a lemma for proving the Hahn-Banach theorem for complex numbers.\nThe Hahn-Banach Theorem for Complex Numbers2 Let $X$ be a $\\mathbb{C}$-vector space and assume that $Y \\subset X$. Let us define $p : X \\to \\mathbb{ R}$ as the following sublinear functional.\n$$ p(\\lambda x)=|\\lambda| p(x),\\quad x\\in X, \\lambda \\in \\mathbb{C} $$\nAnd assume that $y^{\\ast} : Y \\to \\mathbb{ C}$ satisfies the following condition as a linear functional of $Y$.\n$$ \\begin{equation} \\text{Re}\\left( y^{\\ast}(y) \\right) \\le p(y),\\quad \\forall y\\in Y \\end{equation} $$\nThen, there exists a linear functional $x^{\\ast} : X \\to \\mathbb{C}$ of $X$ that satisfies the following conditions:\n$x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$ $\\text{Re}(x^{\\ast}(x)) \\le p(x),\\quad \\forall x \\in X$ Explanation Compared to the theorem for real numbers, the codomain of $p$ being $\\mathbb{R}$ remains unchanged because, as mentioned above, when $X$ is a normed space, $p$ corresponds to the norm. $X$, $Y$ are $\\mathbb{C}$-vector spaces and since $\\mathbb{R} \\subset \\mathbb{C}$, they also satisfy the conditions for being a $\\mathbb{R}$-vector space. This is because if all conditions for a vector space, $(M1)$~$(M5)$, hold true for all complex numbers, they automatically hold true for all real numbers as well. Similarly, $y^{\\ast}$, $x^{\\ast}$ being $\\mathbb{C}$-linear means they also satisfy the condition of being $\\mathbb{R}-$ linear.\nProof Define the function $\\psi : Y \\to \\mathbb{ R}$ as follows:\n$$ \\psi (y) = \\text{Re} ( y^{\\ast}(y) ) $$\nThen, it can be shown that $\\psi$ is also a $\\mathbb{C}$-linear functional of $Y$. This is a trivial result since $\\mathrm{ Re}$ and $y^{\\ast}$ are linear, and the demonstration is very straightforward, thus omitted. By the definition of $\\psi$ and $(1)$, the following equation holds:\n$$ \\psi(y)= \\text{Re} \\left( y^{\\ast}(y) \\right) \\le |y^{\\ast}(y)| \\le p(y) $$\nTherefore, by the Hahn-Banach theorem for real numbers, there exists a $\\mathbb{R}$-linear functional $\\Psi : X \\to \\mathbb{ R}$ of $X$ that satisfies:\n$$ \\Psi (y) = \\psi (y),\\quad \\forall y \\in Y $$\n$$ \\Psi (x) \\le p(x),\\quad \\forall x \\in X $$\nNext, define a new function $\\Phi : X \\to \\mathbb{ C}$ as follows. The final goal is to show that the defined $\\Phi$ is the $x^{\\ast}$ mentioned in the theorem.\n$$ \\Phi (x) := \\Psi (x) -i \\Psi(ix) $$\nThen, $\\Phi$ can be seen as a linear functional of $X$. Since $\\Psi$ is $\\mathbb{R}$-linear, linearity with respect to addition and multiplication by real numbers is trivial, so only $\\Phi(ix)=i\\Phi(x)$ needs to be verified.\n$$ \\begin{align*} \\Phi(ix) =\u0026amp;\\ \\Psi(ix) -i \\Psi( -x) \\\\ =\u0026amp;\\ \\Psi(ix)+i\\Psi(x) \\\\ =\u0026amp;\\ -i^2 \\Psi(ix)+i\\Psi(x) \\\\ =\u0026amp;\\ i \\big( \\Psi(x)-i\\Psi(ix) \\big) \\\\ =\u0026amp;\\ i\\Phi(x) \\end{align*} $$\n$\\Phi$ satisfying (a) can be shown as follows. If we assume $y \\in Y$,\n$$ \\begin{align*} \\Phi(y) =\u0026amp;\\ \\Psi (y) -i \\Psi(iy) \\\\ =\u0026amp;\\ \\psi(y) -i\\psi(iy) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right)-i\\text{Re} \\left( y^{\\ast}(iy) \\right) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right) +\\text{Im} \\left(-iy^{\\ast}(iy) \\right) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right) +\\text{Im} \\left( y^{\\ast}(y) \\right) \\\\ =\u0026amp;\\ y^{\\ast}(y) \\end{align*} $$\nProving that $\\Phi$ satisfies (b) is even simpler.\n$$ \\mathrm{Re }\\left( \\Phi(x) \\right) = \\Psi(x) \\le p(x) $$\nTherefore, since $\\Phi$ is a linear functional of $X$ and satisfies (a), (b), $x^{\\ast}=\\Phi$ exists.\n■\nThe Hahn-Banach Theorem for Seminorms Let $X$ be a $\\mathbb{C}$-vector space and assume that $Y \\subset X$. Let $p : X \\to \\mathbb{ R}$ be a seminorm of $X$. And assume that $y^{\\ast} : Y \\to \\mathbb{ C}$ satisfies the following condition as a linear functional of $Y$.\n$$ | y^{\\ast}(y) | \\le p(y),\\quad \\forall y\\in Y $$\nThen, there exists a linear functional $x^{\\ast} : X \\to \\mathbb{C}$ of $X$ that satisfies the following conditions:\n$x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$\n$| x^{\\ast}(x) | \\le p(x),\\quad \\forall x \\in X$\nProof From the definitions of seminorm and sublinear, if $p$ is a seminorm, it automatically satisfies the conditions of being sublinear.\nIt is trivial that the following equation is satisfied:\n$$ \\text{Re} \\left( y^{\\ast}(y) \\right) \\le |y^{\\ast}(y) | \\le p(y) $$\nTherefore, by the Hahn-Banach theorem for complex numbers, there exists a linear functional $x^{\\ast} : X \\to \\mathbb{C}$ of $X$ that satisfies the following two conditions:\n$$ x^{\\ast}(y)=y^{\\ast}(y) \\quad \\forall y \\in Y $$\n$$ \\text{Re} \\left( x^{\\ast}(x) \\right) \\le p(x) \\quad \\forall x \\in X $$\nLet\u0026rsquo;s assume $S = \\left\\{ \\lambda \\in \\mathbb{C} : | \\lambda | =1 \\right\\}$. Then,\n$$ \\begin{align*} \\text{Re} \\left( \\lambda x^{\\ast}(x) \\right) =\u0026amp;\\ \\text{Re} \\left( \\lambda x^{\\ast}(\\lambda x) \\right) \\\\ \\le \u0026amp; p(\\lambda x) \\\\ =\u0026amp;\\ |\\lambda| p(x)=p(x) \\quad \\forall x \\in X \\end{align*} $$\nFor a fixed $x \\in X$, a $\\lambda \\in S$ that satisfies $|x^{\\ast}(x)|=\\lambda x^{\\ast}(x)$ can always be found. Thus, for that particular $\\lambda$, the following equation holds:\n$$ | x^{\\ast}(x) | =\\lambda x^{\\ast}(x) = \\text{Re} \\left( \\lambda x^{\\ast}(x) \\right) \\le p(x), \\quad \\forall x \\in X $$\nSince the linear functional $x^{\\ast}$ of $X$ satisfies both conditions, the proof is complete.\n■\nAppendix For a fixed $x$, let\u0026rsquo;s say $x^{\\ast}(x)=a+ib$. If we assume $\\lambda=c+id$, then because of the condition on $\\lambda$, $c^2+d^2 =1$, thus $\\lambda=c+i\\sqrt{1-c^2}$ holds. Also, $|x^{\\ast}(x)|=\\sqrt{a^2+b^2}$ holds. Considering $\\lambda x^{\\ast}(x)=(ac-b\\sqrt{1-c^2})+i(a\\sqrt{1-c^2}+bc)$, and since $|x^{\\ast}(x)|$ is a non-negative real number,\n$$ \\begin{align*} \u0026amp;\u0026amp; a\\sqrt{1-c^2}+bc =\u0026amp;\\ 0 \\\\ \\implies\u0026amp;\u0026amp; a^2(1-c^2) =\u0026amp;\\ b^2c^2 \\\\ \\implies\u0026amp;\u0026amp; a^2 =\u0026amp;\\ (a^2+b^2)c^2 \\\\ \\implies\u0026amp;\u0026amp; c^2 =\u0026amp;\\ \\dfrac{a^2}{a^2+b^2} \\tag{2} \\end{align*} $$\nFor convenience, let\u0026rsquo;s denote $c=\\dfrac{a}{\\sqrt{a^2+b^2}}$. And let\u0026rsquo;s set $d=\\dfrac{-b}{\\sqrt{a^2+b^2}}$. Then, $(2)$ and $c^2+d^2=1$ hold true. Also, $|x^{\\ast}(x)|=ac-bd=\\sqrt{a^2+b^2}$ is true. Therefore, for a fixed $x$, if $x^{\\ast}(x)=a+ib$, then for $\\lambda=\\dfrac{a}{\\sqrt{a^2+b^2}}-i\\dfrac{b}{\\sqrt{a^2+b^2}}\\in S$, $|x^{\\ast}(x)|=\\lambda x^{\\ast}(x)$ holds true.\nhttp://mathonline.wikidot.com/the-hahn-banach-theorem-real-version\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://mathonline.wikidot.com/the-hahn-banach-theorem-complex-version\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1230,"permalink":"https://freshrimpsushi.github.io/en/posts/1230/","tags":null,"title":"Hahn Banach Theorem for Real, Complex, Seminorm"},{"categories":"고전역학","contents":"Overview Hamilton\u0026rsquo;s principle, functionals, action, and variation are explained here in a way that is as simple as possible. If you have not found a satisfactory explanation elsewhere, it is recommended to read through to the end. This has been written so that even freshmen and sophomores in college can understand it.\nLagrangian Mechanics1 When an object moves from time $t_{1}$ to $t_{2}$, the integral of the Lagrangian over the path of motion is called the actionaction and is denoted as $J$.\n$$ \\begin{equation} J=\\int_{t_{1}}^{t_{2}} L dt \\end{equation} $$\nAmong all possible paths of motion, the action of the actual path of motion is the minimum. The LagrangianLagrangian is defined as the difference between the kinetic energy and potential energy and is commonly denoted as $L$.\n$$ L = T-V $$\nThis is briefly referred to as Hamilton\u0026rsquo;s variational principle or the principle of least action. The reason it is called the principle of least action is because the integral in $(1)$ is called action. Although minimum and extremum are different concepts, let\u0026rsquo;s assume they mean the same thing here. Precisely, it would be correct to say extremum (either maximum or minimum). Based on the Marion textbook, this is probably the first content of Lagrangian mechanics you will learn in the first semester, and according to the Fowles textbook, in the second semester. However, it was very difficult to understand this content by sticking to the textbook alone. New concepts appear without kindly explaining what they are. For example, the Fowles textbook introduces the following equation:\n$$ \\begin{equation} \\delta J =\\delta \\int_{t_{1}}^{t_{2}} L dt = 0 \\end{equation} $$\nAnd the explanation for the newly introduced symbol $\\delta$ is as follows:\n\"$\\delta$ is the variation of the total integral for the extremum.\"\nHow can one understand what $\\delta$ means after reading this? They barely teach what variation is and then proceed with calculations recklessly. It\u0026rsquo;s too slow to read line by line without understanding why the equations hold, making it very difficult to grasp the content. Therefore, the author aims to explain Lagrangian mechanics as kindly as possible for students new to it. First, it is necessary to organize the terms used when describing Hamilton\u0026rsquo;s principle.\nFunctional Many sources refer to the integral in $(2)$ as a functional, but it is normal for physics students to not know what a functional is. You might commonly know a function as something where if you input a real number, you get a real number (or a complex number) as the output.\n$$ f(x)=x^2,\\quad g(x)=e^{2x} $$\nHowever, considering the mathematical definition of a function, there\u0026rsquo;s no need for the input to be a number and for the output to be a number. Since a function is something that gives a corresponding result when something is input, there\u0026rsquo;s no restriction on what can be input. If a function maps input functions to a number, that function is called a functionalfunctional. For example, the function $F$ defined below is a functional.\n$$ {\\color{blue}F\\big( {\\color{orange}f(x)} \\big)} := {\\color{red}\\int_{1}^{2} f(x) dx} $$\nThat is, the function $F$ takes any function and integrates it from $1$ to $2$ to get a function value. In reality, when you calculate\n$$ {\\color{blue}F( {\\color{orange} e^{x} })} = \\int_{1}^2 e^x dx = {\\color{red}e^2-e},\\quad {\\color{blue}F({\\color{orange}x^2})}=\\int_{1}^2 x^{2} dx = {\\color{red}\\frac{7}{3} } $$\nsuch a function that yields a real number (or a complex number) upon inserting a function is called a functional. The following content will discuss that action is precisely a functional because it produces some value when \u0026rsquo;the Lagrangian for each path of motion\u0026rsquo; is input, which is a function. There\u0026rsquo;s a post on the blog about the mathematical content of functionals, but it won\u0026rsquo;t be introduced here. It might be more confusing to read, so it\u0026rsquo;s recommended not to read it unless you\u0026rsquo;re curious. If you\u0026rsquo;re curious, search for functional in the top right search bar, read about it, and if you don\u0026rsquo;t understand, just forget it.\nAction and Lagrangian The difference between kinetic energy and potential energy is called the Lagrangian and is denoted as $L$.\n$$ L=T-V $$\nSince the Lagrangian depends on velocity, position, and time, if the position is denoted as $y$, it can also be denoted as follows:\n$$ L=L(y^{\\prime},\\ y,\\ t) $$\nThe name Lagrangian comes from the French mathematician Joseph Louis Lagrange. The integral of the Lagrangian over time is called action or act and is commonly denoted as $J$.\n$$ J = \\int_{t_{1}}^{t_{2}} L dt = \\int_{t_{1}}^{t_{2}} L(y^{\\prime},\\ y,\\ t) dt $$\nHamilton\u0026rsquo;s Principle Devised by the British mathematician William Rowan Hamilton in 1834, the principle states that the path actually taken by a body will make the action minimal. This is not a provable fact but one of the basic principles existing in nature, as if $F=ma$. For example, let\u0026rsquo;s say we want to know the path a body takes when thrown from a high place to the ground. There could be countless paths we might imagine, but there\u0026rsquo;s something special about the actual path the body takes. That is, when integrating the Lagrangian over each path, the integral of the Lagrangian over the actual path the body takes is the smallest. That is, the path that minimizes the action is the actual path the body takes. Hence, Hamilton\u0026rsquo;s principle is also known as the principle of least action. Based on this principle, dealing with the motion of a body is Lagrangian mechanicsLagrangian mechanics. The amazing thing is that Lagrangian mechanics, though appearing entirely different, yields the same results as Newtonian mechanics. That is, only the method of expression is different, but the essence is the same. Newtonian mechanics deals with the motion of bodies based on vector calculations, whereas Lagrangian mechanics describes mechanics by calculating scalars (energy).\nVariation Simply put, the content explained above is organized mathematically. As an easy example, consider the problem of finding the minimum value of a quadratic function.\nLet\u0026rsquo;s say we are given a quadratic function like the one in the picture above. The minimum value of the function is $1$, and the place where the function value is minimal is $x=3$. At the point where it has a minimum (extremum) value, the slope is $0$, so we know that when differentiated, $0$. Therefore,\n$$ \\dfrac{dy}{dx} \\bigg|_{x=3}=0 $$\nThis content will be applied directly to the principle of least action.\nIn the picture above, let\u0026rsquo;s denote the actual path of motion of the body as $y(0, t)$. Let\u0026rsquo;s call any possible path the body can take as $y(\\alpha, t)=y(0,t)+\\alpha \\eta (t)$, as shown in the picture above. Note that $\\eta$ is the Greek letter eta. $\\alpha \\eta (t)$ can be thought of as the error when compared to the actual path. As can be seen from the picture and formula, when there is no error, that is, $\\alpha=0$, the possible arbitrary path $y(\\alpha, t)$ becomes the actual path. Furthermore, the principle of least action is the content that among all possible paths, the action for the actual path is the minimum value. Combining the two pieces of content and applying the example mentioned above, when differentiating the action and substituting $\\alpha=0$, the result is that the value is $0$.\n$$ \\dfrac{\\partial J}{\\partial \\alpha}=\\dfrac{\\partial }{\\partial \\alpha} \\int_{t_{1}}^{t_{2}} L\\big( y^{\\prime}(\\alpha,t),\\ y(\\alpha,t),\\ t \\big) dt =0 $$\nThis can be simply denoted as follows, and $\\delta J$ is called the variation of $J$.\n$$ \\delta J = 0 $$\nThat is, it can be understood as $\\delta=\\dfrac{\\partial }{\\partial \\alpha}$. Therefore, the following equation holds:\n$$ \\delta \\dot{y}=\\dfrac{\\partial }{\\partial \\alpha}\\frac{dy}{dt}=\\dfrac{d}{dt}\\frac{\\partial y}{\\partial \\alpha}=\\dfrac{d}{dt}\\delta y $$\nSee Also Euler-Lagrange Equation Grant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p417-420\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1182,"permalink":"https://freshrimpsushi.github.io/en/posts/1182/","tags":null,"title":"Lagrangian Mechanics and Hamiltons Variational Principle"},{"categories":"편미분방정식","contents":"Buildup1 Let\u0026rsquo;s consider the initial value problem of the Hamilton-Jacobi equation that depends only on $H$ as $Du$ for the Hamilton-Jacobi equation.\n$$ \\begin{equation} \\left\\{ \\begin{aligned} u_{t} + H(Du)\u0026amp;=0 \u0026amp;\u0026amp; \\text{in } \\mathbb{R}^n \\times (0,\\infty) \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\mathbb{R}^n \\times \\left\\{ t=0 \\right\\} \\end{aligned} \\right. \\label{eq1} \\end{equation} $$\nGenerally, the Hamiltonian depends on the spatial variables as in the form of $H(Du, x)$, but let\u0026rsquo;s say here it is not affected by $x$. Also, let\u0026rsquo;s assume the following for the Hamiltonian $H\\in C^\\infty$.\n$$ \\begin{cases} H \\mathrm{\\ is\\ convex} \\\\ \\lim \\limits_{|p|\\to \\infty} \\dfrac{H(p)}{|p|}=\\infty \\end{cases} $$\nAnd if we say $L=H^{\\ast}$, the Lagrangian $L$ also satisfies the same characteristics. Lastly, let\u0026rsquo;s assume the initial value $g : \\mathbb{R}^n \\to \\mathbb{R}$ is Lipschitz continuous. That is,\n$$ \\mathrm{Lip}(g):=\\sup \\limits_{x,y\\in \\mathbb{R}^n \\\\ x \\ne y} \\dfrac{ |g(x)-g(y)| }{|x-y|} \u0026lt; \\infty $$\nFurthermore, the characteristic equation of the given Hamilton-Jacobi equation $\\eqref{eq1}$ is as follows.\n$$ \\begin{align*} \\dot{\\mathbf{p}}(s) \u0026amp;= -D_{x}H \\big( \\mathbf{p}(s), \\mathbf{x}(s) \\big) \\\\ \\dot{z}(s) \u0026amp;= D_{p} H\\big( \\mathbf{p}(s),\\ \\mathbf{x}(s)\\big)\\cdot \\mathbf{p}(s) -H\\big( \\mathbf{p}(s), \\mathbf{x}(s)\\big) \\\\ \\dot{\\mathbf{x}}(s) \u0026amp;= D_{p}H\\big( \\mathbf{p}(s), \\mathbf{x}(s) \\big) \\end{align*} $$\nHere, since $H$ is assumed to be independent of $x$, it can be rewritten as follows.\n$$ \\begin{align*} \\dot{\\mathbf{p}} \u0026amp;= 0 \\\\ \\dot{z} \u0026amp;= D H( \\mathbf{p} )\\cdot \\mathbf{p} -H ( \\mathbf{p} ) \\\\ \\dot{\\mathbf{x}} \u0026amp;= DH ( \\mathbf{p}) \\end{align*} $$\nAt this time, it is $t(s)=s, p(s)=Du(x(s), s), z(s)=u(x(s), s)$. Since there is no need to distinguish between the differentiation with respect to $p$ and $x$, the subscript of $D$ was omitted. Since the Euler-Lagrange equation holds for fixed start and end points, if there exists a solution to the given Hamilton-Jacobi equation $\\eqref{eq1}$, then it is a local in time solution as follows.\n$$ u= u(x,t) \\in C^2\\big( \\mathbb{R}^n \\times [0,T]\\big) $$\nIn the above characteristic equation, the first and third equations are Hamilton\u0026rsquo;s equations that satisfy the Euler-Lagrange equation from the minimization problem of action defined by Lagrangian $L=H*$.\nIf $H$ and $L$ are differentiable at $p$ and $v\\in \\mathbb{R}^n$, then all the following contents are equivalent.\n$$ \\begin{cases} p\\cdot v=L(v) + H(p) \\\\ p=DL(v) \\\\ v=DH(p) \\end{cases} $$\nAt this time, it is defined as $p=D_{v}L(v)$, so using the above lemma, we obtain the following.\n$$ \\begin{align*} \\dot{z}(s) \u0026amp;= DH(\\mathbf{p})\\cdot \\mathbf{p}-H(\\mathbf{p}) \\\\ \u0026amp;= \\mathbf{v} \\cdot \\mathbf{p}-H(\\mathbf{p}) \\\\ \u0026amp;= L(\\mathbf{v})+H(\\mathbf{p})-H(\\mathbf{p}) \\\\ \u0026amp;= L(\\mathbf{v}) = L\\big(\\dot{\\mathbf{x}}(s)\\big) \\end{align*} $$\nTherefore, if we find $z(t)$, it is as follows.\n$$ \\begin{align*} z(t) \u0026amp;= \\int_{0}^t \\dot{z}(s)dx +z(0) \\\\ \u0026amp;= \\int_{0}^tL \\big( \\dot{\\mathbf{x}}(s) \\big) + u\\big( \\mathbf{x}(0),\\ 0\\big) \\\\ \u0026amp;= \\int_{0}^t L\\big( \\dot{\\mathbf{x}}(s)\\big) ds +g\\big(\\mathbf{x}(0) \\big) \\end{align*} $$\nHowever, at this time, since it was $z(t)=u(x(t), t)$ in the above condition, we obtain the following.\n$$ u(x,t)=\\int_{0}^t L\\big( \\dot{\\mathbf{x}}(s)\\big) ds +g\\big(\\mathbf{x}(0) \\big) \\quad (0 \\le t \u0026lt;T) $$\nThis is a local in time smooth solution, so the question remains whether a global in time weak solution can be obtained. Returning to the minimization problem of action, the difference from when the Euler-Lagrange equation was derived is that only the endpoint is fixed.\nLet\u0026rsquo;s say a fixed $x \\in \\mathbb{R}^n, t\u0026gt;0$ is given. And let\u0026rsquo;s say the admissible class $\\mathcal{A}$ is as follows.\n$$ \\mathcal{A}=\\left\\{ \\mathbf{w}\\in C^1\\big( [0,t];\\mathbb{R}^n \\big)\\ :\\ \\mathbf{w}(t)=x \\right\\} $$\nAnd let\u0026rsquo;s consider the minimization problem of the following action.\n$$ \\mathbf{w}(\\cdot) \\in \\mathcal{A} \\mapsto \\int_{0}^t L\\big( \\dot{\\mathbf{w}}(s)\\big) ds + g(\\mathbf{w}(0)) $$\nIf a minimizer $\\mathbf{x}(\\cdot)$ exists, then it is $\\mathbf{p}(s):=DL(\\dot{\\mathbf{x}}(s))$, satisfies the Euler-Lagrange equation, and therefore also satisfies the Hamilton equation. Thus, as in the case of the local in time solution obtained earlier, the solution will be given as follows.\n$$ u(x,t)=\\int_{0}^tL\\big( \\dot{\\mathbf{x}}(s)\\big)ds +g \\big( \\mathbf{x}(0) \\big) $$\nBased on the above content, if a global in time weak solution exists, it can be defined as follows.\n$$ \\begin{equation} u(x,t):=\\inf \\limits_{\\mathbf{w} \\in \\mathcal{A}} \\left\\{ \\int_{0}^t L\\big( \\dot{\\mathbf{w}}(s) \\big)ds + g\\big( \\mathbf{w}(0) \\big) \\right\\} \\label{eq2} \\end{equation} $$\nTheorem Let\u0026rsquo;s say $x \\in \\mathbb{R}^n$ and $t\u0026gt;0$. Then, the solution to the minimization problem of $\\eqref{eq2}$ is given as follows.\n$$ u(x,t) = \\min \\limits_{y \\in \\mathbb{R}^n} \\left\\{ tL\\left( \\dfrac{x-y}{t} \\right) +g(y) \\right\\} $$\nThis is called the Hopf-Lax formula.\nProof First, we show that it holds for $\\inf$, and then actually show that it becomes $\\min$ in order.\nStep 1.\nThere exists an arbitrary fixed $y \\in \\mathbb{R}^n, t\\in \\mathbb{R}$. And let\u0026rsquo;s define $\\mathbf{w}$ as follows.\n$$ \\mathbf{w}(s) :=y+\\frac{s}{t}(x-y) \\quad (0 \\le s \\le t) $$\nThen $\\mathbf{w}(0)=y$ and $\\mathbf{w}(t)=x$. Then $\\mathbf{w}$ is an element of the admissible class $\\mathcal{A}$.\n$$ \\mathcal{A}= \\left\\{ \\mathbf{w}(\\cdot) \\ \\big| \\ \\mathbf{w}(0)=y,\\ \\mathbf{w}(t)=x\\right\\} $$\nThen, by the definition of ~, the following inequality holds.\n$$ \\begin{align*} u(x,t) \u0026amp; \\le\u0026amp; \\int_{0}^t L \\left( \\frac{x-y}{t}\\right)ds + g(y) \\\\ \u0026amp;= tL\\left( \\frac{x-y}{t}\\right)+g(y) \\end{align*} $$\nSince this inequality holds for all $y \\in \\mathbb{R}^n$, we obtain the following.\n$$ u(x,t) \\le \\inf \\limits_{y \\in \\mathbb{R}^n} \\left(t L\\left(\\frac{x-y}{t} \\right) +g(y)\\right) $$\nStep 2.\nLet\u0026rsquo;s say $\\mathbf{w}(\\cdot) \\in \\mathcal{A}$. Then $\\mathbf{w}(\\cdot) \\in C^1([0;t];\\mathbb{R}^n)$ and $\\mathbf{w}(t)=x$.\nJensen\u0026rsquo;s Inequality\nSuppose the function $f$ is convex. Then, the following formula holds. $$ f \\left( -\\!\\!\\!\\!\\!\\! \\int_{U} u dx \\right) \\le -\\!\\!\\!\\!\\!\\! \\int_{U} f(u) dx $$\nThen, by the above lemma, the following holds.\n$$ L \\left( \\frac{1}{t}\\int_{0}^t \\dot{\\mathbf{w}}(s) dx\\right) \\le \\dfrac{1}{t}\\int_{0}^t L \\big( \\dot{\\mathbf{w}(s)} \\big)ds $$\nAnd let\u0026rsquo;s say the starting point is $y$, $\\mathbf{w}(0)=y$. Then, the above inequality is as follows.\n$$ \\begin{align*} \u0026amp;\u0026amp; L\\left( \\dfrac{1}{t} \\big( \\mathbf{w}(t)-\\mathbf{w}(0) \\big) \\right) \u0026amp;\\le \\dfrac{1}{t}\\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds\n\\\\ \\implies\u0026amp;\u0026amp; L\\left( \\dfrac{x-y}{t} \\right) \u0026amp;\\le \\dfrac{1}{t}\\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds \\end{align*} $$\nMultiply both sides by $t$ and add $g(y)$ to get the following.\n$$ tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\le \\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds + g(y) $$\nSince the right-hand side\u0026rsquo;s $\\inf$ is $u(x,t)$, it is as follows.\n$$ tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\le u(x,t) $$\nFinally, taking $\\inf \\limits_{y\\in \\mathbb{R}^n}$ on both sides, we obtain the following.\n$$ \\inf \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\right) \\le u(x,t) $$\nTherefore, by Step 1. and Step 2., the following holds.\n$$ \\begin{equation} u(x,t) = \\inf \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\right) \\label{eq3} \\end{equation} $$\nStep 3.\nLet\u0026rsquo;s say $\\left\\{y_{k} \\right\\}_{k=1}^\\infty$ is a minimizing sequence for $\\eqref{eq3}$. Then, the following holds.\n$$ \\begin{equation} tL\\left( \\dfrac{x-y_{k}}{t} \\right) + g(y_{k}) \\to u(x,t)\\in [-\\infty, \\infty) \\quad \\mathrm{as}\\ k\\to \\infty \\label{eq4} \\end{equation} $$\nFirst, assume $\\left\\{y_{k} \\right\\}$ is not bounded. We will show this assumption leads to a contradiction, proving that $\\left\\{ y_{k} \\right\\}$ is bounded. By assumption, $|y_{k}| \\to \\infty$ and $y_{k}=0$, $k$ are at most finite. Therefore, let\u0026rsquo;s consider a subsequence that only satisfies $y_{k}\\ne 0$ again as $\\left\\{ y_{k} \\right\\}$. The following holds.\n$$ \\left| \\dfrac{x-y_{k}}{t} \\right| \\to \\infty $$\nThen, by the properties of the Lagrangian $L$, the following holds.\n$$ a_{k}:= \\dfrac{L\\left( \\dfrac{x-y_{k}}{t}\\right)}{\\left| \\dfrac{x-y_{k}}{t}\\right|} \\to \\infty $$\nTherefore, $L\\left( \\dfrac{x-y_{l}}{t}\\right) \\to \\infty$ and multiplying by a constant yields the same result.\n$$ \\begin{equation} tL\\left( \\dfrac{x-y_{k}}{t}\\right) \\to \\infty \\label{eq5} \\end{equation} $$\nRewriting the Lipschitz condition of $g$ is as follows.\n$$ \\dfrac{|g(x)-g(y_{k})|}{|x-y_{k}|} \\le \\mathrm{Lip}(g)=C \\quad \\forall \\ k \\in \\mathbb{N} $$\nTherefore, we obtain the following.\n▶eq32\n◀\nAdding $\\eqref{eq5}$ to both sides gives the following.\n$$ tL\\left( \\dfrac{x-y_{k}}{t}\\right)+ g(x) -g(y_{k}) \\le C|x-y_{k}|+ tL\\left( \\dfrac{x-y_{k}}{t}\\right) \\quad \\mathrm{for\\ large}\\ k $$\nAppropriately rearranging the above formula yields the following.\n$$ tL\\left( \\dfrac{x-y_{k}}{t}\\right)-C|x-y_{k}| + g(x) \\le tL\\left( \\dfrac{x-y_{k}}{t}\\right) + g(y_{k}) $$\nRewritten, it is as follows.\n$$ a_{k}|x-y_{k}| -C|x-y_{k}| + g(x) =|x-y_{k}|(a_{k}-C)+g(x) \\le tL\\left( \\dfrac{x-y_{k}}{t}\\right) + g(y_{k}) $$\nSince $a_{k}\\to \\infty$ and $|x-y_{k}| \\to \\infty$, the left side diverges to $\\infty$ and the right side also diverges. Therefore, by the definition of $u(x,t)$, $u(x,t)\\to \\infty$ is true. This is a contradiction to $\\eqref{eq4}$, so $\\left\\{ y_{k} \\right\\}$ is bounded.\nSince $\\left\\{ y_{k} \\right\\}$ is bounded, let\u0026rsquo;s assume $y_{k} \\to y_{0}$. Then, the following holds.\n$$ tL \\left( \\dfrac{x-y_{k}}{t} \\right)+g(y_{k}) \\to tL \\left( \\dfrac{x-y_{0}}{t}\\right)+g(y_{0}) =\\min\\limits_{y \\in \\mathbb{R}^n}\\left( tL \\left( \\dfrac{x-y}{t}\\right)+g(y) \\right) $$\nThen, by $\\eqref{eq4}$, the following holds.\n$$ tL\\left( \\dfrac{x-y_{k}}{t} \\right) + g(y_{k}) \\to u(x,t)\\in [-\\infty, \\infty) \\quad \\mathrm{as}\\ k\\to \\infty $$\nTherefore, we obtain the following.\n$$ u(x,t) = \\min \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) +g(y) \\right) $$\n■\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p122-124\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1174,"permalink":"https://freshrimpsushi.github.io/en/posts/1174/","tags":null,"title":"Hopf-Lax Formula"},{"categories":"편미분방정식","contents":"Explanation1 When emphasizing that x and p are variables of a partial differential equation, they are denoted in normal font as $x,p \\in \\mathbb{R}^{n}$, and when emphasizing them as functions of $s$, they are denoted in bold font as $\\mathbf{x}, \\mathbf{p} \\in \\mathbb{R}^{n}$. Characteristic Equations\n$$ \\begin{cases} \\dot{\\mathbf{p}} (s) = -D_{x}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)-D_{z}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)\\mathbf{p}(s) \\\\ \\dot{z}(s) = D_{p}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\cdot \\mathbf{p}(s) \\\\ \\dot{\\mathbf{x}}(s) = D_{p}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\end{cases} $$\nThe solution of nonlinear first-order partial differential equations using characteristic equations varies slightly depending on how the differential equation is given. This is distinguished by the linearity of the given differential equation and differs for cases of linear, quasi-linear, and fully nonlinear. The stronger the nonlinearity, the more challenging it is.\nSolution Homogeneous Linear If the given partial differential equation is completely linear, it can be solved most easily. The condition for $\\mathbf{p}(s)$ in the characteristic equation is so simple that it is unnecessary. Consider the following linear and homogeneous differential equation.\n$$ \\begin{equation} F(Du, u, x) = \\mathbf{b}(x)\\cdot Du(x)+c(x)u(x)=0 \\quad (x\\in \\Omega \\subset \\mathbb{R}^{n}) \\label{eq1} \\end{equation} $$\nHere, if we set the variables of $F$ as $p, z, x$, it would be as follows.\n$$ \\begin{equation} F(p,\\ z,\\ x)=\\mathbf{b}(x)\\cdot p +c(x)z=b_{1}p_{1}+\\cdots +b_{n}p_{n}+cz = 0 \\label{eq2} \\end{equation} $$\nIf we calculate $D_{p}F$, it would be as follows.\n$$ D_{p}F=(F_{p_{1}}, \\dots, F_{p_{n}})=(b_{1}, \\dots, b_{n})=\\mathbf{b}(x) $$\nThen, the characteristic equation is as follows.\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s))\\cdot \\mathbf{p}(s) \\end{align*} $$\nBy $(2)$, $\\dot{z}(s)$ is as follows.\n$$ \\dot{z}(s) = -c(\\mathbf{x}(s))z $$\nTherefore, the characteristic equation for a homogeneous linear first-order partial differential equation is as follows.\n$$ \\left\\{ \\begin{align*} \\dot{\\mathbf{x}}(s)\u0026amp;=\\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= -c(\\mathbf{x}(s))z \\end{align*} \\right. $$\nHere, it can be seen through an example that the characteristic equation for $\\mathbf{p}(s)$ is not necessary to solve the problem.\nExample Suppose the following differential equation is given.\n$$ \\left\\{ \\begin{align*} x_{1} u_{x_{2}} - x_{2} u_{x_{1}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0,\\ x_{2}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{ x_{1}\u0026gt;0,\\ x_{2}=0 \\right\\}$ Then in $(1)$, $\\mathbf{b}=(-x_{2},\\ x_{1}), c=-1$. Thus, the characteristic equation is as follows.\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;= -x^{2} \\\\ \\dot{x}^{2} \u0026amp;=x^{1} \\\\ \\dot{z}\u0026amp;=z \\end{align*} \\right. $$\nSince this is a simple ordinary differential equation, it can be easily solved as follows.\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;=x^{0}\\cos s \\\\ x^{2}(s)\u0026amp;=x^{0} \\sin s \\\\ z(s)\u0026amp;=z^{0}e^s=g(x^{0})e^s \\end{align*} \\right. $$\nHere, $x^{0}$ is a constant set to pass through the $x_{1}-$axis $(\\Gamma)$ when $s=0$. Then, by the boundary condition $z=u=g\\ \\mathrm{on}\\ \\Gamma$, when $s=0$, $z(0)=z^{0}=g(x^{0})$. Now, fix the point $(x_{1},\\ x_{2}) \\in \\Omega$.\n$$ (x_{1},\\ x_{2})=(x^{1}(s),\\ x^{2}(s)) = (x^{0} \\cos (s),\\ x^{0} \\sin (s)) $$\nThen, for $s\u0026gt;0, x^{0}\u0026gt;0$, we obtain the following.\n$$ x_{1}^{2} + x_{2}^{2} = (x^{0})^{2}\\cos^{2}(s) + (x^{0})^{2}\\sin^{2}(s) = (x^{0})^{2} \\implies x^{0}=({x_{1}}^{2}+{x_{2}}^{2})^{1/2} \\\\ \\dfrac{x_{2}}{x_{1}} = \\dfrac{x^{0}\\sin (s)}{x^{0} \\cos (s)} = \\tan (s) \\implies s=\\arctan \\left( \\frac{x_{2}}{x_{1}} \\right) $$\nTherefore, the solution of the equation is as follows.\n$$ \\begin{align*} u(x)\u0026amp;=u(x^{1}(s),\\ x^{2}(s)) \\\\ \u0026amp;= z(s) \\\\ \u0026amp;=g(x^{0})e^s \\\\ \u0026amp;= g(({x_{1}}^{2}+{x_{2}}^{2})^{1/2})e^{\\arctan \\left(\\frac{x_{2}}{x_{1}}\\right)} \\end{align*} $$\n■\nQuasi-Linear The following describes cases where the given differential equation is linear with respect to the highest order of differentiation. As we are dealing with first-order differential equations, it refers to cases where they are linear with respect to first-order derivatives.\n$$ F(Du,\\ u,\\ x)=\\mathbf{b}(x,\\ u(x))\\cdot Du(x)+c(x,\\ u(x))=0 $$\nHere, if we set the variables of $F$ as $p, z, x$, it would be as follows.\n$$ \\begin{equation} F(p, z, x)=\\mathbf{b}(x, z)\\cdot p + c(x, z)=b_{1}p_{1} + \\cdots + b_{n} p_{n} +c=0 \\label{eq3} \\end{equation} $$\nIf we calculate $D_{p}F$, it would be as follows.\n$$ D_{p}F=(F_{p_{1}},\\ \\cdots,\\ F_{p_{n}})=(b_{1},\\ \\cdots,\\ b_{n})=\\mathbf{b}(x,\\ z) $$\nThen, the characteristic equation is as follows.\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s)) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s))\\mathbf{p}(s)=-c(\\mathbf{x}(s),\\ z(s)) \\end{align*} $$\nThe second equality for $\\dot{z}$ is valid due to $(3)$. Even in this case, the condition for $\\mathbf{p}(s)$ is not necessary to solve the problem.\nExample Suppose the following differential equation is given.\n$$ \\left\\{ \\begin{align*} u_{x_{1}} + u_{x_{2}} \u0026amp;= u^{2} \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{2}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{x_{2}=0 \\right\\}$ Then in $(3)$, $\\mathbf{b}=(1,\\ 1)$, $c=-z^{2}$. Thus, the characteristic equation is as follows.\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;=1, \\dot{x}^{2}=1 \\\\ \\dot{z} \u0026amp;= z^{2} \\end{align*} \\right. $$\nSince these are simple ordinary differential equations, they can be solved as follows.\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;= x^{0}+s, x^{2}(s)=s \\\\ z(s)\u0026amp;=\\frac{z^{0}}{1-sz^{0}}=\\frac{g(x^{0})}{1-sg(x^{0})} \\end{align*} \\right. $$ $x^{0}$ is a constant set to pass through the $x_{2}-$axis $(\\Gamma)$ when $s=0$. Now, fix the point $(x_{1},\\ x_{2}) \\in \\Omega$.\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=(x^{0}+s, s) $$\nThen, for $s\u0026gt;0, x^{0} \\in \\mathbb{R}$, we obtain the following. $$ s=x_{2}, \\quad x^{0}=x_{1}-x_{2} $$\nTherefore, the solution of the equation is as follows.\n$$ \\begin{align*} u(x) \u0026amp;= u(x^{1}(s),\\ x^{2}(s)) \\\\ \u0026amp;= z(s) \\\\ \u0026amp;= \\frac{g(x^{0})}{1-sg(x^{0})} \\\\ \u0026amp;= \\frac{g(x_{1}-x_{2})}{1-x_{2}g(x_{1}-x_{2})} \\end{align*} $$\nOf course, this is only valid when $1-x^{2}g(x_{1}-x_{2})\\ne 0$.\n■\nFully Nonlinear Suppose the following differential equation is given.\n$$ \\begin{align*} u_{x_{1}}u_{x_{2}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u \u0026amp;= x_{2}^{2} \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{x_{1}=0 \\right\\}$ If we set the variables of $F$ as $P, z, x$, it would be as follows.\n$$ F(p, z, x)=p_{1}p_{2}-z $$\nThus, the characteristic equation is as follows.\n$$ \\begin{align*} \\dot{p}^{1} \u0026amp;= p^{1},\\quad \\dot{p}^{2}=p^{2} \\\\ \\dot{z} \u0026amp;= 2p^{1}p^{2} \\\\ \\dot{x}^{1} \u0026amp;= p^{2},\\quad \\dot{x}^{2}=p^{1} \\end{align*} $$\nFirst, if we solve the differential equation for $p$, it would be as follows.\n$$ p^{1}(s)=p_{1}^{0}e^s,\\ \\ p^{2}(s)=p_{2}^{0}e^s $$\nHere, $p_{1}^{0}=p(0)$, $p_{2}^{0}=p(0)$. Then, since $\\dot{z}(s)=2p_{1}^{0}p_{2}^{0}e^{2s}$, $z$ is as follows.\n$$ z(s)=p_{1}^{0}p_{2}^{0}e^{2s}+C $$\nSince $z(0)=z^{0}=p_{1}^{0}p_{2}^{0}+C$, $C=z^{0}-p_{1}^{0}p_{2}^{0}$. Therefore, it is as follows.\n$$ z(s)=z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) $$\nSimilarly, if we calculate $x^{1}$ and $x^{2}$, it would be as follows.\n$$ \\begin{equation} \\left\\{ \\begin{aligned} p^{1}(s) \u0026amp;= p_{1}^{0}e^s \\\\ p^{2}(s) \u0026amp;= p_{2}^{0}e^s \\\\ z(s) \u0026amp;= z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) \\\\ x^{1}(s) \u0026amp;= p_{2}^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+p_{1}^{0}(e^s-1) \\end{aligned} \\right. \\label{eq4} \\end{equation} $$\nHere, $x^{0}$ is a constant set to pass through the $x_{1}-$axis $(\\Gamma)$ when $s=0$. Since $u_{x_{2}}=p^{2}$ and by the boundary condition $u=x_{2}^{2}$ when $x_{1}=0$ ($s=0$), $p_{2}^{0}=u(0,\\ x^{0})=2x^{0}$. Also, as the given differential equation is $u_{x_{1}}u_{x_{2}}=u$, $p_{1}^{0}p_{2}^{0}=z^{0}=(x^{0})^{2}$ and $p_{1}^{0}=\\frac{x^{0}}{2}$. If we substitute all these into $(4)$, we obtain the following.\n$$ \\left\\{ \\begin{align*} p^{1}(s) \u0026amp;= \\frac{x^{0}}{2}e^s \\\\ p^{2}(s) \u0026amp;= 2x^{0}e^s \\\\ z(s) \u0026amp;= (x^{0})^{2}e^{2s} \\\\ x^{1}(s) \u0026amp;= 2x^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+\\frac{x^{0}}{2}(e^s-1) \\end{align*} \\right. $$\nNow, fix the point $(x_{1}, x_{2})\\in \\Omega$.\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=\\left( 2x^{0}(e^s -1), \\frac{x^{0}}{2}(e^s+1) \\right) $$\nThen, for $s,\\ x^{0}$, we obtain the following.\n$$ x^{0}=\\frac{4x_{2}-x_{1}}{4},\\ \\ e^s=\\frac{x_{1}+4x_{2}}{4x_{2}-x_{1}} $$\nTherefore, the solution of the equation is as follows.\n$$ u(x)=u(x^{1}(s),\\ x^{2}(s))=z(s)=(x^{0})^{2}e^{2s}=\\dfrac{(x_{1}+4x_{2})^{2}}{16} $$\n■\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p99-102\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1074,"permalink":"https://freshrimpsushi.github.io/en/posts/1074/","tags":null,"title":"Solution of Nonlinear First Order PDE Using Characteristic Equations"},{"categories":"상미분방정식","contents":"Definition The following differential equation is referred to as the Chebyshev Differential Equation:\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -x\\dfrac{dy}{dx}+n^2 y=0 $$\nDescription It\u0026rsquo;s a form that includes the independent variable $x$ in the coefficient, and assuming that the solution is in the form of a power series, it can be solved. The solution to the Chebyshev equation is called the Chebyshev polynomial, often denoted as $T_{n}(x)$.\nSolution $$ \\begin{equation} (1-x^2)y^{\\prime \\prime} -xy^{\\prime}+\\lambda^2 y=0 \\label{1} \\end{equation} $$\nLet\u0026rsquo;s assume the solution to the given Chebyshev differential equation is as follows:\n$$ y=a_{0}+a_{1}(x-x_{0})+a_2(x-x_{0})^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}(x-x_{0})^n $$\nWhen it is $x=0$, since the coefficient of $y^{\\prime \\prime}$ is $(1-x^2)|_{x=0}=1\\ne 0$, let\u0026rsquo;s set it to $x_{0}=0$. Then,\n$$ \\begin{equation} y=a_{0}+a_{1}x+a_2x^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}x^n \\label{2} \\end{equation} $$\nWe start solving by assuming a series solution, but at the end of the solution, we find that the terms of $y$ are finite. Now, to substitute into $\\eqref{1}$, let\u0026rsquo;s find $y^{\\prime}$ and $y^{\\prime \\prime}$.\n$$ y^{\\prime}=a_{1}+2a_2x+3a_{3}x^2+\\cdots=\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1} $$\n$$ y^{\\prime \\prime}=2a_2+3\\cdot 2a_{3}x+4\\cdot 3 a_{4}x^2 +\\cdots = \\sum \\limits_{n=2} n(n-1)a_{n}x^{n-2} $$\nSubstituting $y, y^{\\prime}, y^{\\prime \\prime}$ into $\\eqref{1}$ yields the following:\n$$ (1-x^2)\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\nExpanding and rearranging the first term\u0026rsquo;s coefficient $(1-x^2)$ gives:\n$$ \\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x^2\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n = 0 $$\n$$ \\implies \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n = 0 $$\nThe key here is to match the order of $x$. While the rest are all expressed as $x^n$, only the first series is expressed as $x^{n-2}$, so substituting $n+2$ instead of $n$ yields:\n$$ \\sum \\limits_{n=0} ^\\infty (n+2)(n+1)a_{n+2}x^{n} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\nSince the second series starts from the term $x^2$, we extract the term where $n=0,1$ from the rest of the series, and group constant terms with constant terms, and first-order terms with first-order terms:\n$$ \\left[ 2\\cdot 1 a_2+\\lambda^2 a_{0} \\right]+\\left[ 3\\cdot 2 a_{3}-a_{1}+\\lambda^2a_{1} \\right]x \\\\ + \\sum \\limits_{n=2}^\\infty \\left[ (n+2)(n+1)a_{n+2}-n(n-1)a_{n}-na_{n}+\\lambda^2a_{n} \\right] x^n=0 $$\nFor this equation to hold, all coefficients must be $0$.\n$$ 2\\cdot 1 a_2+\\lambda^2 a_{0} = 0 $$\n$$ 3\\cdot 2 a_{3}-a_{1}+\\lambda^2a_{1} =0 $$\n$$ (n+2)(n+1)a_{n+2}-n(n-1)a_{n}-na_{n}+\\lambda^2a_{n}=0 $$\nOrganizing each gives:\n$$ \\begin{align} a_2 \u0026amp;= -\\dfrac{\\lambda^2}{2 \\cdot 1}a_{0} \\label{3} \\\\ a_{3} \u0026amp;=-\\dfrac{\\lambda^2-1^2}{3\\cdot 2} a_{1} \\label{4} \\\\ a_{n+2} \u0026amp;= -\\dfrac{\\lambda^2-n^2}{(n+2)(n+1)}a_{n} \\label{5} \\end{align} $$\nHaving obtained the recurrence relation $\\eqref{5}$, we can determine all coefficients if we just know the values of $a_{0}$ and $a_{1}$. From $\\eqref{3}, \\eqref{5}$, obtaining the coefficients of even-order terms:\n$$ \\begin{align*} a_{4} \u0026amp;= -\\dfrac{\\lambda^2-2^2}{4\\cdot 3}a_2=\\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}a_{0} \\\\ a_{6} \u0026amp;= -\\dfrac{\\lambda^2-4^2}{6\\cdot 5}a_{4}= -\\dfrac{\\lambda^2(\\lambda^2-2^2)(\\lambda^2-4^2)}{6!}a_{0} \\\\ \u0026amp;\\vdots \\end{align*} $$\nIf we say $n=2m (m=1,2,3,\\cdots)$ then:\n$$ a_{n}=a_{2m}=(-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!}a_{0} $$\nSimilarly, obtaining the coefficients of odd-order terms from $\\eqref{4}, \\eqref{5}$:\n$$ \\begin{align*} a_{5} \u0026amp;= -\\dfrac{\\lambda^2-3^2}{5\\cdot 4}a_{3}=\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}a_{1} \\\\ a_{7} \u0026amp;= -\\dfrac{\\lambda^2-5^2}{7\\cdot 6 }a_{5}=-\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)(\\lambda^2-5^2)}{7!}a_{1} \\\\ \u0026amp;\\vdots \\end{align*} $$\nIf we say $n=2m+1 (m=1,2,3,\\cdots)$ then:\n$$ a_{n}=a_{2m+1}=(-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!}a_{1} $$\nSubstituting the obtained coefficients into $\\eqref{2}$ to find the solution gives:\n$$ \\begin{align*} y = \u0026amp;a_{0}+a_{1}x -\\dfrac{\\lambda^2}{2!}a_{0}x^2-\\dfrac{\\lambda^2-1^2}{3!} a_{1}x^3 + \\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}a_{0}x^4 \\\\ \u0026amp;+\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}a_{1}x^5+ \\cdots +(-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!}a_{0}x^{2m} \\\\ \u0026amp;+(-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!}a_{1}x^{2m+1}+\\cdots\\quad(m=1,2,3,\\cdots) \\end{align*} $$\nHere, grouping even-order terms as $a_{0}$ and odd-order terms as $a_{1}$ and organizing gives:\n$$ \\begin{align*} y\u0026amp;=a_{0}\\left[1-\\dfrac{\\lambda^2}{2!}x^2+\\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}x^4+\\sum \\limits_{m=3}^\\infty (-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!} x^{2m} + \\cdots \\right] \\\\ \u0026amp; + a_{1}\\left[x-\\dfrac{\\lambda^2-1^2}{3!}x^3+\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}x^5+\\sum \\limits_{m=3}^\\infty (-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!} x^{2m+1} + \\cdots\\right] \\end{align*} $$\nIf we call the first bracket $y_{0}$ and the second bracket $y_{1}$, then the general solution of the Chebyshev equation is as follows:\n$$ y=a_{0}y_{0}+a_{1}y_{1} $$\nThe two series $y_{0}$ and $y_{1}$ are known to converge within the range of $|x|\u0026lt;1$ according to the ratio test. Because of $\\eqref{5}$, $\\dfrac{a_{n+2}}{a_{n}}=\\dfrac{n^2-\\lambda^2}{(n+2)(n+1)}=\\dfrac{n^2-\\lambda^2}{n^2+3n+2}$, so if we use the ratio test:\n$$ \\lim \\limits_{n \\rightarrow \\infty} \\dfrac{n^2-\\lambda^2}{n^2+3n+2}x^2=x^2\u0026lt;1 $$\n$$ \\implies -1\u0026lt;x\u0026lt;1 $$\nHowever, in many problems, $x=\\cos \\theta$, $\\lambda$ appear in the form of non-negative integers, and we seek solutions that converge for all $\\theta$. In other words, the goal is to find solutions that also converge at $x=\\pm 1$. Fortunately, when $\\lambda$ is an integer, the desired solution exists, and depending on the value of $\\lambda$, only one of the two solutions $y_{0}, y_{1}$ must exist. When $\\lambda$ is $0$ or an even number, $y_{1}$ diverges, and $y_{0}$ becomes a finite-term polynomial with only even-order terms. When $\\lambda$ is odd, $y_{0}$ diverges, and $y_{1}$ becomes a finite-term polynomial with only odd-order terms. The summary is as follows:\nValue of $\\lambda$ $y_{0}$ $y_{1}$ Solution of the equation $0$ or even Finite-term polynomial Diverges $y=a_{0}y_{0}$ Odd Diverges Finite-term polynomial $y=a_{1}y_{1}$ Case 1. When $\\lambda$ is $0$ or even\nWhen $\\lambda=0$, from the second term, it takes $\\lambda^2$ as a factor, becoming all $0$, thus $y_{0}=1$\nWhen $\\lambda=2$, from the fourth term, it takes $(\\lambda^2-2^2)$ as a factor, becoming all $0$, thus $y_{0}=1-x^2$\nWhen $\\lambda=4$, from the sixth term, it takes $(\\lambda^2-4^2)$ as a factor, becoming all $0$, thus $y_{0}=1-8x^2+8x^4$\nAnd when $\\lambda=0$, $x=1$ diverges at $y_{1}=1+\\frac{1}{3!}+\\frac{1\\cdot3^2}{5!}+\\cdots$. The same applies to other even numbers. Therefore, when $\\lambda$ is $0$ or an even number, the solution becomes a finite-term polynomial with only even-order terms. That is, we obtain a form of the solution that only remains up to a specific term of the series $y_{0}$. The opposite result is obtained when $\\lambda$ is odd.\nCase 2. When $\\lambda$ is odd\nWhen $\\lambda=1$, from the third term, it takes $(\\lambda^2-1^2)$ as a factor, becoming all $0$, thus $y_{1}=x$\nWhen $\\lambda=3$, from the fifth term, it takes $(\\lambda^2-3^2)$ as a factor, becoming all $0$, thus $y_{1}=-3x+4x^3$\nWhen $\\lambda=5$, from the seventh term, it takes $(\\lambda^2-5^2)$ as a factor, becoming all $0$, thus $y_{1}=5x-20x^3+16x^5$\nWhen $\\lambda=1$, $x^2=1$ diverges at $y_{0}$, and the same applies to other odd numbers. Therefore, when $\\lambda$ is odd, the solution becomes a finite-term polynomial with only odd-order terms. That is, we obtain a form of the solution that only remains up to a specific term of the series $y_{1}$.\nIt\u0026rsquo;s also noted that when $\\lambda$ is negative, it\u0026rsquo;s the same as when $\\lambda$ is positive, which can be seen by examining $y_{0}$ and $y_{1}$. For instance, the case of $\\lambda=2$ is the same as the case of $\\lambda=-2$, and the case of $\\lambda=1$ is the same as the case of $\\lambda=-1$. Therefore, we only need to consider $\\lambda$ within the range of non-negative integers. Selecting $a_{0}$ and $a_{1}$ appropriately, when $x=1$, the solution becomes $y(x)=1$, this is called the Chebyshev polynomial and is commonly denoted as $T_{n}(x)$. The first few Chebyshev polynomials are as follows.\n$$ \\begin{align*} T_{0}(x) \u0026amp;= 1 \\\\ T_{1}(x) \u0026amp;= x \\\\ T_2(x) \u0026amp;= 2x^2-1 \\\\ T_{3}(x) \u0026amp;= 4x^3-3x \\\\ T_{4}(x) \u0026amp;= 8x^4-8x^2+1 \\\\ T_{5}(x) \u0026amp;= 16x^5-20x^3+5x \\\\ \\vdots \u0026amp; \\end{align*} $$\nSee Also Chebyshev Differential Equation and Chebyshev Polynomial ","id":955,"permalink":"https://freshrimpsushi.github.io/en/posts/955/","tags":null,"title":"Series Solution of Chebyshev Differential Equation"},{"categories":"푸리에해석","contents":"Definition The series for $2L$-periodic function $f$ is defined as the Fourier series of $f$ as follows:\n$$ \\begin{align*} \\lim \\limits_{N \\rightarrow \\infty} S^{f}_{N}(t) \u0026amp;= \\lim \\limits_{N \\to \\infty}\\left[ \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right] \\\\ \u0026amp;= \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\end{align*} $$\nHere, each coefficient $a_{0}, a_{n}, b_{n}$ is called the Fourier coefficient, and its value is as follows:\n$$ \\begin{align*} \\\\ a_{0} \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin \\dfrac{n\\pi t}{L}dt \\end{align*} $$\nDescription The Fourier series represents any function as a series expansion of trigonometric functions, famously developed by the French mathematician Joseph Fourier for solving heat equations. The term \u0026ldquo;any function\u0026rdquo; is used because if there is a function defined on a certain interval $(a,b)$, it can be replicated (Ctrl+C, Ctrl+V) to produce a $(b-a)$-periodic function.\nThe core principle is to express it as a linear combination of orthogonal trigonometric functions, analogous to decomposing $(4,-1,7)$ as follows in three-dimensional vectors:\n$$ (4,-1,7) = a_{1}\\hat{\\mathbf{e}}_{1} + a_{2}\\hat{\\mathbf{e}}_{1} + a_{3}\\hat{\\mathbf{e}}_{1} $$\nIndeed, the Fourier series of $f$ not only has minimal error with $f$ but also converges pointwise under well-defined conditions to $f$.\n$$ f(t) = \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L}t + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) $$\nDerivation Regression Analysis1 Part 1\nThe goal is to express the function $f(t)$ as a linear combination of $1, \\cos \\dfrac{\\pi t}{L}, \\cos\\dfrac{2\\pi t}{L}, \\cdots, \\sin \\dfrac{\\pi t}{L}, \\sin \\dfrac{2\\pi t}{L}, \\cdots $s. Thus, assuming $S^{f}_{N}(t)=\\dfrac{1}{2}{\\alpha_{0}}+\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right)$, $f(t)$ can be represented as follows:\n$$ f(t)=S^{f}_{N}(t)+e_{N}(t) $$\n$e_{N}(t)$ is the difference between $f(t)$ and the approximation $S_{N}^{f} (t)$. The smallest difference $S_{N}^{f}(t)$ leads to the closest series expansion to $f(t)$. Let\u0026rsquo;s define $e_{N}$ as the mean square error2.\n$$ e_{N}=\\dfrac{1}{2L}\\int_{-L}^{L} [e_{N}(t) ]^{2}dt=\\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N} (t) \\right]^{2} dt $$\nPart 2\n$$ \\begin{align*} e_{N} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N}(t) \\right]^{2} dt \\\\ \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\end{align*} $$\nLet the coefficients that minimize the mean square error $e_{N}$ be $\\alpha_{0},\\ \\alpha_{n},\\ \\beta_{n}$, $a_{0}$, $a_{n}$, respectively. The conditions that minimize $e_{N}$ are called the normal equations.\n$$ \\dfrac{\\partial e_{N}}{\\partial \\alpha_{0}}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\alpha_{n}}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\beta_{n}}=0\\quad (m=1,\\ 2,\\ \\cdots,\\ N) $$\nThen, $a_{0}$, $a_{n}$, $b_{n}$ can be calculated as follows:\nPart 2.1 $a_{0}$\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_{0}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_{0}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{-1}{2} \\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_ {N} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac {n\\pi t}{L} \\right) \\right] dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_{0} dt +\\dfrac{1}{2L}\\int_{-L}^{L} \\sum \\limits_ {n=1}^{N}\\left( \\alpha_{n}\\cos \\dfrac{n\\pi t}{L}+\\beta_{n} \\sin \\dfrac{n \\pi t}{L} \\right) dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_{0} dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt +\\dfrac{1}{2}\\alpha_{0} \\\\ \u0026amp;= 0 \\end{align*} $$\nThe fourth equality holds because the integral of a trigonometric function over one period is $0$.\n$$ a_{0} = \\dfrac{1}{L} \\int_{-L}^{L}f(t)dt $$\nPart 2.2 $a_{n}$\nChoose any $m \\in \\left\\{ 1,2,\\dots,N \\right\\}$.\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\cos \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_{0}\\cos\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad + \\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\cos\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\alpha_{m} \\int_{-L}^{L}\\cos\\dfrac{m\\pi t}{L}\\cos\\dfrac{m\\pi t} {L} dt\\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\alpha_{m} \\\\ \u0026amp;= 0 \\end{align*} $$\nThe fourth and fifth equalities hold due to the orthogonality of trigonometric functions.\n$$ a_{n}= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\quad (n=1, 2, \\cdots, N) $$\nPart 2.3 $b_{n}$\nChoose any $m \\in \\left\\{ 1,2,\\dots,N \\right\\}$.\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\beta_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\beta_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\sin \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_{0}\\sin\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad +\\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\sin\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\beta_{m} \\int_{-L}^{L}\\sin\\dfrac{m\\pi t}{L}\\sin\\dfrac{m\\pi t} {L} dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\beta_{m} \\\\ \u0026amp;=0 \\end{align*} $$\nThe fourth and fifth equalities hold due to the orthogonality of trigonometric functions.\n$$ b_{n}=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\quad (n=1, 2, \\cdots, N) $$\nPart 3 Using the obtained $a_{0}$, $a_{n}$, $b_{n}$ to express $f(t)$ results in the same.\n$$ \\begin{align*} f(t) \u0026amp;= S^{f}_{N}(t)+e_{N}(t) \\\\[1em] \\text{where } S^{f}_{N}(t) \u0026amp;= \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t} {L} \\right) \\\\ a_{0} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\end{align*} $$\nTaking the limit for $N$ yields\n$$ \\lim \\limits_{N \\rightarrow \\infty} S_{N}^{f} (t)=\\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n} \\sin\\dfrac{n\\pi t}{L} \\right) $$\nThe above series is called the Fourier series of $f$, and $a_{0}$, $a_{n}$, $b_{n}$ are called the Fourier coefficients of $f$.\n■\nByung Sun Choi, Introduction to Fourier Analysis (2002), pp. 51-53\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRSS is the mean square error.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":929,"permalink":"https://freshrimpsushi.github.io/en/posts/929/","tags":null,"title":"Derivation of Fourier Series"},{"categories":"추상대수","contents":"Summary 1 For a prime number $p$ and a natural number $n$, a finite field with cardinality $p^{n}$ is defined as the Galois Field of order $p^{n}$, denoted as $\\text{GF} \\left( p^{n} \\right)$. Finite fields are exclusively Galois Fields, and for a given $p$ and $n$, the Galois Field is uniquely determined.\nThe uniqueness implies that even if there are different fields, an isomorphism exists, making them essentially the same field. Description Even though no one believed in the existence of finite fields when Gauss first conceived the concept, it is now established that finite fields not only exist but are also unique and their specific structures have been identified. This eliminates the need for redundant research.\nFor instance, there is no need to ponder the existence of a field with $10$ elements, and since $\\text{GF} \\left( p \\right) = \\mathbb{Z}_{p}$ is an integer ring, we already know a lot about it. If more information is desired, one can approach through $\\mathbb{Z}_{p}$ without clinging to abstract definitions, and vice versa.\nProof 2 Part 1. All finite fields are Galois Fields.\nLet\u0026rsquo;s denote a finite extension field of field $F$ as $E$ and its degree over $F$ as $n := \\left[ E : F \\right]$.\nIf we set $| F | = q$, then $E$ is a $n$-dimensional vector space over $F$, hence $|E| = q^{n}$. A field has a multiplicative identity, and if the characteristic is $0$, an isomorphic subring exists, leading to an infinite field. Therefore, the characteristic of a finite field must be a finite natural number. If the characteristic of finite field $E$ is $p \\ne 0$, then since $E$ has a multiplicative identity $1$, it must be $p \\cdot 1 = 0$. Being a domain, $$ p \\cdot 1 = ( p_{1} \\cdot 1 ) ( p_{2} \\cdot 1 ) = 0 $$ cannot have a $p_{1}, p_{2} \\in \\mathbb{Z}$ satisfying it, thus $p$ must be a prime number. Consequently, $E$ has a subfield isomorphic to the prime field $\\mathbb{Z}_{p}$, and since $\\left| \\mathbb{Z}_{p} \\right| = p$, it follows that $|E| = p^{n}$.\nPart 2. Existence of Galois Fields\nPart 2-1. Zeros of $x^{p^{n}} - x$\nConsider the algebraic closure $\\overline{F}$ of a field $F$ with characteristic $p$, $\\left( x^{p^{n}} - x \\right)$.\nSince $\\overline{F}$ is algebraically closed, $\\left( x^{p^{n}} - x \\right) \\in \\overline{F} [ x ]$ factors into linear terms of $1$. A readily observable fact is $$ x^{p^{n}} - x = ( x - 0 ) \\left( x^{p^{n}-1} - 1 \\right) $$ thus $0$ becomes a zero in $\\left( x^{p^{n}} - x \\right)$. Considering another zero $\\alpha \\ne 0$ of $f(x) := x^{p^{n}-1} - 1$, $f \\left( \\alpha \\right) = 0$ hence $$ 0 = f \\left( \\alpha \\right) = \\alpha^{p^{n} - 1} - 1 \\implies \\alpha^{p^{n} - 1} = 1 $$ By this, expressing $f(x)$ as a product of $\\left( x - \\alpha \\right)$, $$ \\begin{align*} f(x) =\u0026amp; x^{p^{n}-1} - 1 \\\\ =\u0026amp; x^{p^{n}-1} - \\alpha^{p^{n}-1} \\\\ =\u0026amp; (x - \\alpha ) \\left( x^{p^{n} - 2 } + \\alpha x^{p^{n} - 3 } + \\cdots + \\alpha^{p^{n} - 3 } x + \\alpha^{p^{n} - 2} \\right) \\end{align*} $$ For convenience, let\u0026rsquo;s denote the second factor as $$ g(x) := \\left( x^{p^{n} - 2 } + \\alpha x^{p^{n} - 3 } + \\cdots + \\alpha^{p^{n} - 3 } x + \\alpha^{p^{n} - 2} \\right) $$ thus, $g(x)$ has $p^{n} - 1$ terms. Substituting $x = \\alpha$ gives $$ g ( \\alpha ) = \\alpha^{p^{n} - 2} \\cdot \\left( p^{n} - 1 \\right) = {{\\alpha^{p^{n} - 1}} \\over { \\alpha }} \\left( p^{n} - 1 \\right) $$ Previously, we stated that $\\alpha \\ne 0$ is a zero of $f(x)$, hence $\\alpha^{p^{n}-1} - 1 = 0$, and since we assumed the characteristic is the prime number $p$, $$ g ( \\alpha ) = {{1} \\over { \\alpha }} \\cdot (0 - 1) = - {{1} \\over { \\alpha }} \\ne 0 $$ Therefore, $\\alpha$ is not a repeated root of $f(x) = 0$, and the same applies to other non-$\\alpha$ zeros. Consequently, $\\left( x^{p^{n}} - x \\right)$ has precisely $p^{n}$ distinct zeros.\nPart 2-2. Freshman\u0026rsquo;s Dream\nConsidering $\\alpha , \\beta \\in F$, computing $\\left( \\alpha + \\beta \\right)^{p}$ by the binomial theorem yields $$ \\begin{align*} \\left( \\alpha + \\beta \\right)^{p} =\u0026amp; \\sum_{k=1}^{p} \\binom{p}{k} \\alpha^{k} \\beta^{p - k} \\\\ =\u0026amp; \\alpha^{p} + \\sum_{k=2}^{p-1} {{p!} \\over { ( p - k )! ( k )! }} \\alpha^{k} \\beta^{p - k} + \\beta^{p} \\\\ =\u0026amp; \\alpha^{p} + \\beta^{p} + p \\sum_{k=2}^{p-1} {{ ( p - 1 )! } \\over { ( p - k )! ( k )! }} \\alpha^{k} \\beta^{p - k} \\end{align*} $$ Since the characteristic of $F$ is $p$, the last term becomes $0$, thus $$ \\left( \\alpha + \\beta \\right)^{p} = \\alpha^{p} + \\beta^{p} $$ Taking $p$ to the power on both sides once more gives $$ \\left( \\left( \\alpha + \\beta \\right)^{p} \\right)^{p} = \\left( \\alpha^{p} \\right)^{p} + \\left( \\beta^{p} \\right)^{p} $$ Simplifying, we get $\\left( \\alpha + \\beta \\right)^{p^{2}} =\\alpha^{p^2} + \\beta^{p^2}$, and repeating this $n$ times results in $$ \\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n} $$\nNow, consider the algebraic closure $\\overline{ \\mathbb{Z}_{p} }$ of $\\mathbb{Z}_{p}$.\nLet\u0026rsquo;s denote the set of all zeros in $\\left( x^{p^{n}} - x \\right) \\in \\overline{ \\mathbb{Z}_{p} } [ x ]$ as $K \\subset \\overline{ \\mathbb{Z}_{p} } $, and its elements as $\\alpha , \\beta \\in K$.\nPart 2-3. $K$ is a Galois Field.\n(i) Closure under addition: $$ \\begin{cases} \\alpha^{p^{n}} - \\alpha = 0 \\\\ \\beta^{p^{n}} - \\beta = 0 \\end{cases} $$ Adding both sides, by Part 2-2 $\\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n}$, $$ \\left( \\alpha^{p^{n}} + \\beta^{p^{n}} \\right) - ( \\alpha + \\beta ) = \\left( \\alpha + \\beta \\right)^{p^{n}} - ( \\alpha + \\beta ) = 0 $$ thus, $( \\alpha + \\beta ) \\in K$. (ii) Additive identity: Since $0^{p^{n}} - 0 = 0$, $0 \\in K$. (iii) Additive inverse: If $\\left( - \\alpha \\right)^{p^{n}} = \\left( - 1 \\right)^{^{p^{n}}} \\left( \\alpha \\right)^{p^{n}} = \\left( - 1 \\right)^{^{p^{n}}} \\alpha$, If $p=2$, then $-1 = 1$, so $\\left( -\\alpha \\right) = \\alpha \\in K$. Since $p \\ne 2$ is an odd prime, $\\left( - \\alpha \\right)^{p^{n}} - ( - \\alpha ) = 0$, i.e., $( - \\alpha ) \\in K$. (iv) Closure under multiplication: Since $\\left( \\alpha \\beta \\right)^{p^{n}} = \\alpha^{p^{n}} \\beta^{p^{n}} = \\alpha \\beta$, $\\left( \\alpha \\beta \\right)^{p^{n}} - \\alpha \\beta = 0$, thus $\\alpha \\beta \\in K$. (v) Multiplicative identity: Since $1^{p^{n}} - 1 = 0$, $1 \\in K$. (vi) Multiplicative inverse: For $\\alpha \\ne 0$, taking the inverse of $\\displaystyle \\left( \\alpha \\right)^{p^{n}} = \\alpha$ gives $\\displaystyle {{1} \\over {\\left( \\alpha \\right)^{p^{n}} }} = {{1} \\over { \\alpha }}$, thus $$ \\left( {{1} \\over { \\alpha }} \\right)^{p^{n}} - {{1} \\over { \\alpha }} = 0 $$ so $\\alpha^{-1} \\in K$. (vii): $| K | = p^{n}$: The characteristic of $\\mathbb{Z}_{p}$ is $p$, so by Part 2-1, $\\left( x^{p^{n}} - x \\right)$ has precisely $p^{n}$ distinct zeros. Therefore, $K$ is a Galois Field of order\n$p^{n}$.\nPart 3. Uniqueness of Galois Fields\nFrom Part 1, the characteristic of $F$ is the prime number $p$, and from Part 2-1, operations in the algebraic closure $\\overline{F}$ of $F$, considering the multiplicative identity $1_{F}$ of $F$ as $1_{\\mathbb{Z}_{p}}$, are essentially the same as operations in the algebraic closure $\\overline{\\mathbb{Z}}_{p}$ of $\\mathbb{Z}_{p}$.\nPart 3-1. The nature of a field with cardinality $p^{n}$, $E \\subset \\overline{\\mathbb{Z}}_{p}$ 3\nLagrange\u0026rsquo;s Theorem: If $H$ is a subgroup of a finite group $G$, then $|H|$ is a divisor of $|G|$.\nConsidering the group $\\left( E^{\\ast} , \\times \\right)$ under multiplication $\\times$ for a field $\\left( E , + , \\times \\right)$ with cardinality $p^{n}$, $E^{\\ast}$ consists of $p^{n} - 1$ elements excluding the multiplicative identity $0 \\in E$ of $E$ and the identity $1 \\in E^{\\ast}$. The order of $\\alpha \\in E^{\\ast}$, the cardinality of the cyclic group generated by $\\alpha$, is a divisor of $p^{n} - 1$ by Lagrange\u0026rsquo;s Theorem, hence $$ \\alpha^{p^{n} - 1} = 1 \\implies a^{p^{n}} = \\alpha $$ In other words, all elements of $E$ are zeros of $x^{p^{n}} - x$ and, by the Fundamental Theorem of Algebra, the elements of the field $E$ with cardinality $p^{n}$ within the algebraic closure $\\overline{\\mathbb{Z}}_{p}$ of $\\mathbb{Z}_{p}$ are precisely the zeros of $\\left( x^{p^{n}} - x \\right) \\in \\mathbb{Z}_{p} [x]$.\nPart 3-2. Minimal Polynomials\nAccording to Part 2-1 and Part 3-1, for a given $p$ and $n$, there exists a field $E$ consisting entirely of zeros of $\\left( x^{p^{n}} - x \\right)$, and considering the characteristic of $F$ as $p$, the operations on its coefficients are the same as those in the prime field $\\mathbb{Z}_{p}$. By Part 2-3 and Part 1, $E$ is a Galois Field containing the prime field $\\mathbb{Z}_{p}$ as a subfield and satisfies $|E| = p^{n}$, and again by Part 2-1, $E$ is the minimal splitting field of $\\left( x^{p^{n}} - x \\right)$.\nProperties of Minimal Splitting Fields: All minimal splitting fields are isomorphic.\nHence, for a given $p$ and $n$, the Galois Field is unique.\n■\nCorollary: Freshman\u0026rsquo;s Dream As an interesting fact, the equation from Part 2-2 $$ \\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n} $$ is known as the Freshman\u0026rsquo;s Dream. It\u0026rsquo;s named because, from the perspective of a newcomer to the subject, the ability to distribute exponentiation inside a bracket would make solving complex problems straightforward without detailed expansions. Notably, in number theory, a similar result can be derived for congruences $\\left( \\alpha + \\beta \\right)^{p^{n}} \\equiv \\alpha^{p^n} + \\beta^{p^n} \\pmod{ p }$ without mentioning the characteristic.\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p300.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p302~304.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p301\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":820,"permalink":"https://freshrimpsushi.github.io/en/posts/820/","tags":null,"title":"Galois Field"},{"categories":"함수","contents":"Formula The explicit formula for the Legendre polynomials is as follows.\n$$ P_{l}(x)=\\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \\tag{1} $$\nDescription This formula is used to obtain the $l$th Legendre polynomial, known as the Rodrigues\u0026rsquo; formula. Originally, it referred to the explicit form of the Legendre polynomials, but later it became a universal name for formulas representing the explicit form of special functions expressed as polynomials.\nDerivation The Legendre polynomial $P_{l}$ refers to the solution of the following Legendre differential equation.\n$$ (1 - x^{2}) \\dfrac{d^{2} y}{d x^{2}} - 2x \\dfrac{d y}{d x} + l(l+1)y = 0 $$\nThus, the proof is complete if it is shown that $(1)$ is a solution to the above differential equation.\nFirst, when we set $v=(x^2-1)^l$, we will show that $\\dfrac{d^lv}{dx^l}$ is a solution to the Legendre equation. Later, normalization is performed to satisfy $P_{l}(1) = 1$, obtaining $(1)$.\n$$ \\dfrac{dv}{dx}=l(2x)(x^2-1)^{l-1} $$\nMultiplying both sides by $(x^2-1)$ yields the following equation.\n$$ (x^2-1)\\dfrac{dv}{dx}=2lx(x^2-1)^l=2lxv $$\nDifferentiating both sides $l+1$ times yields the following, according to the Leibniz rule.\n$$ \\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C}_{k} \\dfrac{ d^{l+1-k}}{dx^{l+1-k} } \\left( \\dfrac{dv}{dx} \\right) \\dfrac{d^k}{dx^k} (x^2-1) = 2l\\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C} _{k} \\dfrac{d^{l+1-k} v}{dx^{l+1-k}} \\dfrac{d^k x}{dx^k} $$\nAt this time, the left side remains only when $k \\ge 3$ is $\\dfrac{d^k}{dx^k}(x^2-1)=0$, so only the term $k=0,2,3$ remains. The right side remains only when $k \\ge 2$ is $\\dfrac{d^kx}{dx^k}=0$, so only the term $k=1,2$ remains. Thus, we obtain the following.\n$$ (x^2-1)\\dfrac{d^{l+2} v}{dx^{l+2}} + (l+1)(2x)\\dfrac{d^{l+1}v}{dx^{l+1}}+\\dfrac{l(l+1)}{2!}2\\dfrac{d^l v}{dx^l}=2lx\\dfrac{d^{l+1} v}{dx^{l+1}} + 2l(l+1)\\dfrac{d^lv}{dx^l} $$\nBy grouping like terms together and arranging them properly, the following is obtained.\n$$ (1-x^2)\\left( \\dfrac{d^l v}{dx^l} \\right)^{\\prime \\prime} -2x\\left( \\dfrac{d^lv}{dx^l} \\right)^{\\prime} + l(l+1)\\dfrac{d^lv}{dx^l}=0 $$\nThis has the same form as the Legendre equation. That is, $\\dfrac{d^l v}{dx^l}$ becomes a solution to the Legendre equation.\n$$ P_{l}(x)= \\dfrac{d^l}{dx^l}(x^2-1)^l $$\nLet\u0026rsquo;s find the coefficient that satisfies $P_{l}(1) = 1$. Factorizing $(x^2-1)^l$ into $(x-1)^l(x+1)^l$ and differentiating $l$ times using the Leibniz rule yields the following.\n$$ \\begin{align*} \u0026amp;\\quad \\ P_{l}(x) \\\\ \u0026amp;= \\dfrac{d^l}{dx^l} \\left[ (x-1)^l (x+1)^l \\right] \\\\ \u0026amp;= \\sum\\limits_{k=0}^l {}_{l}\\mathrm{C}_{k} \\dfrac{d^{l-k}}{dx^{l-k}}(x-1)^l \\dfrac{d^k}{dx^k}(x+1)^l \\\\ \u0026amp;= {}_{l}\\mathrm{C}_{0} l! (x+1)^l + {}_{l}\\mathrm{C}_{1} l!(x-1) l(x+1)^{l-1}+{}_{l}\\mathrm{C}_2\\dfrac{l!}{2}(x-1)^2l(l-1)(x+1)^{l-2}+\\cdots \\end{align*} $$\nFrom the second term onwards, because it includes the factor $(x-1)$, when $x=1$, $0$ is true. Therefore, $P_{l}(1)=l! 2^l$, and to make this value $1$, it must be divided by $\\dfrac{1}{2^l l!}$. Therefore, we finally obtain the following Rodrigues\u0026rsquo; formula.\n$$ P_{l}(x)=\\dfrac{1}{2^l l!}\\dfrac{d^l}{dx^l}(x^2-1)^l $$\n■\n","id":895,"permalink":"https://freshrimpsushi.github.io/en/posts/895/","tags":null,"title":"Rodrigues Formula for Legendre Polynomial"},{"categories":"상미분방정식","contents":"Definition1 The following differential equation is called the Legendre differential equation.\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -2x\\dfrac{dy}{dx}+l(l+1) y=0 $$\nThe solution to the Legendre differential equation is called the Legendre polynomial, commonly denoted as $P_{l}(x)$. The first few Legendre polynomials according to $l$ are as follows.\n$$ \\begin{align*} P_{0}(x) =\u0026amp;\\ 1 \\\\ P_{1}(x) =\u0026amp;\\ x \\\\ P_2(x) =\u0026amp;\\ \\dfrac{1}{2}(3x^2-1) \\\\ P_{3}(x) =\u0026amp;\\ \\dfrac{1}{2}(5x^3-3x) \\\\ P_{4}(x) =\u0026amp;\\ \\dfrac{1}{8}(35x^4-30x^2+3) \\\\ P_{5}(x) =\u0026amp;\\ \\dfrac{1}{8}(63x^5-70x^3+15x) \\\\ \\vdots\u0026amp; \\end{align*} $$\nDescription The Legendre differential equation is also introduced in the following form.\n$$ \\dfrac{d}{dx}\\left[ (1-x)^2 \\dfrac{dy}{dx} \\right] +l(l+1)y=0 $$\nThis is expressed in terms of Sturm-Liouville theory. Expanding and rearranging the first term yields the same equation. The generalized form of the Legendre differential equation as below is called the associated Legendre differential equation.\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -2x\\dfrac{dy}{dx}+\\left( \\dfrac{-m^2}{1-x^2} +l(l+1) \\right) y=0 $$\nHere, if $m=0$, it becomes the Legendre differential equation.\nThe Legendre equation appears in physics and engineering, especially when solving the Laplace equation in spherical coordinates. Physics majors may encounter it when calculating potential in spherical coordinates in electromagnetism, or when solving the Schrödinger equation in spherical coordinates in quantum mechanics. Because the solution process is lengthy, textbooks usually only write down the solution expressed by Rodrigues\u0026rsquo; formula. In fact, physics students do not necessarily need to be too curious about the solution.\nSolution Assuming the solution has the form of a power series with the independent variable $x$, it can be solved.\n$$ \\begin{equation} (1-x^2)y^{\\prime \\prime} -2xy^{\\prime}+l(l+1) y=0 \\label{1} \\end{equation} $$\nAssume the solution to the Legendre differential equation is as follows.\n$$ y=a_{0}+a_{1}(x-x_{0})+a_2(x-x_{0})^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}(x-x_{0})^n $$\nWhen $x=0$, the coefficient of $y^{\\prime \\prime}$ becomes $(1-x^2)|_{x=0}=1\\ne 0$, so we assume it as $x_{0}=0$. Then the series solution is\n$$ \\begin{equation} y=a_{0}+a_{1}x+a_2x^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}x^n \\label{2} \\end{equation} $$\nAlthough we assumed the solution as a series, at the end of the solution, we find that the terms of $y$ are finite. Now, to substitute for $\\eqref{1}$, let\u0026rsquo;s find $y^{\\prime}$ and $y^{\\prime \\prime}$.\n$$ y^{\\prime}=a_{1}+2a_2x+3a_{3}x^2+\\cdots=\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1} $$\n$$ y^{\\prime \\prime}=2a_2+3\\cdot 2a_{3}x+4\\cdot 3 a_{4}x^2 +\\cdots = \\sum \\limits_{n=2} n(n-1)a_{n}x^{n-2} $$\nNow, substituting $y, y^{\\prime}, y^{\\prime \\prime}$ into $\\eqref{1}$,\n$$ (1-x^2)\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -2x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\nExpanding the coefficient of the first term $(1-x^2)$ and rearranging gives\n$$ \\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x^2\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -2x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n$$ \\implies \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -2\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\nThe key here is matching the order of $x$. While the rest are expressed as $x^n$, only the first series is expressed as $x^{n-2}$, so substituting $n+2$ instead of $n$,\n$$ \\sum \\limits_{n=0} ^\\infty (n+2)(n+1)a_{n+2}x^{n} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -2\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\nSince the second series starts from the $x^2$ term, taking out the term where $n=0,1$ from the rest of the series and grouping constant terms with constant terms, and first-order terms with first-order terms,\n$$ \\left[ 2\\cdot 1 a_2+l(l+1)a_{0} \\right]+\\left[ 3\\cdot 2 a_{3}-2a_{1}+l(l+1)a_{1} \\right]x \\\\ + \\sum \\limits_{n=2}^\\infty \\left[ (n+2)(n+1)a_{n+2}-n(n+1)a_{n}-2na_{n}+l(l+1)a_{n} \\right] x^n=0 $$\nFor the above equation to hold, all coefficients must be $0$.\n$$ 2\\cdot 1 a_2+l(l+1)a_{0} =0 $$\n$$ 3\\cdot 2 a_{3}-2a_{1}+l(l+1)a_{1} =0 $$\n$$ (n+2)(n+1)a_{n+2}-n(n+1)a_{n}-2na_{n}+l(l+1)a_{n}=0 $$\nOrganizing each gives\n$$ \\begin{equation} a_2=-\\dfrac{l(l+1)}{2 \\cdot 1}a_{0} \\label{3} \\end{equation} $$\n$$ \\begin{equation} a_{3}=-\\dfrac{(l+2)(l-1)}{3\\cdot 2} a_{1} \\label{4} \\end{equation} $$\n$$ \\begin{equation} a_{n+2}=-\\dfrac{(l+n+1)(l-n)}{(n+2)(n+1)}a_{n} \\label{5} \\end{equation} $$\nUsing $\\eqref{3}, \\eqref{4}, \\eqref{5}$, knowing just the values of $a_{0}$ and $a_{1}$ allows us to determine all coefficients. Calculating the coefficients of even-order terms with $\\eqref{3}$ and $\\eqref{5}$,\n$$ \\begin{align*} a_{4} =\u0026amp;\\ - \\dfrac{(l+3)(l-2)}{ 4 \\cdots 3}a_2 = \\dfrac{l(l-2)(l+1)(l+3)}{4!}a_{0} \\\\ a_{6} =\u0026amp;\\ -\\dfrac{(l+5)(l-4)}{6\\cdot5} a_{4} = -\\dfrac{ l(l-2)(l-4)(l+1)(l+3)(l+5)}{6!} a_{0} \\\\ \\vdots\u0026amp; \\end{align*} $$\nIf we set $n=2m\\ (m=1,2,3,\\cdots)$,\n$$ a_{n}=a_{2m}=(-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!}a_{0} $$\nSimilarly, calculating the coefficients of odd-order terms with $\\eqref{4}$ and $\\eqref{5}$,\n$$ \\begin{align*} a_{5} =\u0026amp;\\ -\\dfrac{(l+4)(l-3)}{5\\cdot 4}a_{3} = \\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}a_{1} \\\\ a_{7} =\u0026amp;\\ -\\dfrac{(l+6)(l-5)}{7\\cdot 6}a_{5} = -\\dfrac{(l+2)(l+4)(l+6)(l-1)(l-3)(l-5)}{7!}a_{1} \\\\ \\vdots\u0026amp; \\end{align*} $$\nIf we set $n=2m+1\\ (m=1,2,3,\\cdots)$,\n$$ a_{n}=a_{2m+1}=(-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!}a_{1} $$\nSubstituting these coefficients into $\\eqref{2}$ to find the solution,\n$$ \\begin{align*} y =\u0026amp;\\a_{0}+a_{1}x -\\dfrac{l(l+1)}{2!}a_{0}x^2-\\dfrac{(l+2)(l-1)}{3!}a_{1}x^3 + \\dfrac{l(l-2)(l+1)(l+3)}{4!}a_{0}x^4+\\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}a_{1}x^5 \\\\ \u0026amp;+ \\cdots +(-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!}a_{0}x^{2m} \\\\ \u0026amp;+ (-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!}a_{1}x^{2m+1} +\\cdots \\end{align*} $$\nGrouping even-order terms as $a_{0}$ and odd-order terms as $a_{1}$,\n$$ \\begin{align*} y =\u0026amp;\\a_{0}\\left[1-\\dfrac{l(l+1)}{2!}x^2+\\dfrac{l(l-2)(l+1)(l+3)}{4!}x^4 \\right. \\\\ \u0026amp;\\left.+\\sum \\limits_{m=3}^\\infty (-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!} x^{2m} \\right] \\\\ \u0026amp;+ a_{1}\\left[x- \\dfrac{(l+2)(l-1)}{3!}x^3+\\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}x^5 \\right. \\\\ \u0026amp; \\left. +\\sum \\limits_{m=3}^\\infty (-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!} x^{2m+1} \\right] \\end{align*} $$\nSetting the first parenthesis as $y_{0}$ and the second as $y_{1}$, the general solution to the Legendre equation is as follows.\n$$ y=a_{0}y_{0}+a_{1}y_{1} $$\nThe two series $y_{0}$ and $y_{1}$ converge in the interval of $|x|\u0026lt;1$ according to the ratio test. By $\\eqref{5}$, since $\\dfrac{a_{n+2}}{a_{n}}=-\\dfrac{(l+n+1)(l-n)}{(n+2)(n+1)}=\\dfrac{(n+l+1)(n-l)}{(n+2)(n+1)}$, using the ratio test,\n$$ \\lim \\limits_{n \\rightarrow \\infty} \\dfrac{(n+l+1)(n-l)}{(n+2)(n+1)}x^2=x^2\u0026lt;1 $$\n$$ \\implies -1\u0026lt;x\u0026lt;1 $$\nHowever, in many problems, $x=\\cos \\theta$ and $l$ appear as non-negative integers, and the goal is to find solutions that converge for all $\\theta$. That is, to find solutions that also converge at $x=\\pm 1$. Fortunately, when $l$ is an integer, the desired solution exists, and depending on the value of $l$, only one of $y_{0}, y_{1}$ exists. If $l$ is $0$ or even, $y_{1}$ diverges, and $y_{0}$ becomes a finite-term polynomial with only even-order terms. If $l$ is odd, $y_{0}$ diverges, and $y_{1}$ becomes a finite-term polynomial with only odd-order terms. The summary is as follows.\nValue of $l$ $y_{0}$ $y_{1}$ Equation\u0026rsquo;s Solution $0$ or even Finite-term polynomial Diverge $y=a_{0}y_{0}$ Odd Diverge Finite-term polynomial $y=a_{1}y_{1}$ Case 1. If $l$ is $0$ or even\nFor $l=0$, from the 2nd term, taking $l$ as a factor, all become $0$, so $y_{0}=1$\nFor $l=2$, from the 4th term, taking $(l-2)$ as a factor, all become $0$, so $y_{0}=1-3x^2$\nFor $l=4$, from the 6th term, taking $(l-4)$ as a factor, all become $0$, so $y_{0}= 1-10x^2+\\dfrac{35}{3}x^4$\nWhen $l=0$, $x^2=1$ becomes $y_{1}=1+\\frac{1}{3}+\\frac{1}{5}+\\cdots$, which diverges by the integral test. The same applies to other even numbers. Thus, when $l$ is $0$ or even, the solution is a finite-term polynomial with only even-order terms. That is, we obtain a solution that only retains specific terms of the series $y_{0}$.\nCase 2. If $l$ is odd\nThe opposite result appears compared to even cases.\nFor $l=1$, from the 3rd term, taking $(l-1)$ as a factor, all become $0$, so $y_{1}=x$\nFor $l=3$, from the 5th term, taking $(l-3)$ as a factor, all become $0$, so $y_{1}=x-\\dfrac{5}{3}x^3$\nFor $l=5$, from the 7th term, taking $(l-5)$ as a factor, all become $0$, so $y_{1}=x-\\dfrac{14}{3}x^3+\\dfrac{21}{5}x^5$\nWhen $l=1$, $x^2=1$ diverges, and the same applies to other odd numbers. Thus, when $l$ is odd, the solution is a finite-term polynomial with only odd-order terms. That is, we obtain a solution that only retains specific terms of the series $y_{1}$.\nAnd if $l$ is negative, it is the same as when $l$ is a non-zero integer, as can be seen by examining $y_{0}$ and $y_{1}$. For example, the case of $l=2$ is the same as that of $l=-3$, and the case of $l=1$ is the same as that of $l=-2$. Therefore, it suffices to consider only when $l$ is a non-negative integer. Choosing the values of $a_{0}$ and $a_{1}$ wisely to make the solution $x=1$ when $y(x)=1$, this is called the Legendre polynomial, denoted as $P_{l}(x)$. The first few Legendre polynomials are as follows.\n$$ \\begin{align*} P_{0}(x) =\u0026amp;\\ 1 \\\\ P_{1}(x) =\u0026amp;\\ x \\\\ P_2(x) =\u0026amp;\\ \\dfrac{1}{2}(3x^2-1) \\\\ P_{3}(x) =\u0026amp;\\ \\dfrac{1}{2}(5x^3-3x) \\\\ P_{4}(x) =\u0026amp;\\ \\dfrac{1}{8}(35x^4-30x^2+3) \\\\ P_{5}(x) =\u0026amp;\\ \\dfrac{1}{8}(63x^5-70x^3+15x) \\end{align*} $$\nThe above result can also be obtained directly using Rodrigues\u0026rsquo; formula.\n■\nMary L. Boas, Mathematical Methods in the Physical Sciences, translated by Jun-gon Choi (3rd Edition, 2008), p577-580\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":889,"permalink":"https://freshrimpsushi.github.io/en/posts/889/","tags":null,"title":"Series Solution of Legendre Differential Equation: Legendre Polynomial"},{"categories":"추상대수","contents":"Definition 1 Let\u0026rsquo;s assume that the $p \\ne 0$ of the integral domain $D$ is not a field.\nPID An integral domain $D$ is called a Principal Ideal Domain (PID) if every ideal in $D$ is a principal ideal.\nConsequent Definitions Let\u0026rsquo;s say a commutative ring $R$ has a unity $1$. If for $a,b \\in R$ there exists $c \\in R$ that satisfies $b=ac$, then $a$ divides $b$ or $a$ is a factor of $b$, denoted as $a \\mid b$. If $a \\mid b$ and $b \\mid a$, then $a,b$ are called associates. For $\\forall a,b \\in D$ and $p=ab$, if one of $a$ or $b$ is a unit, then $p$ is called an irreducible element. For $\\forall a,b \\in D$, if $p \\mid ab$, then $p$ is called a prime element, which is either $p \\mid a$ or $p \\mid b$. A unity is the multiplicative identity $1$, a unit is an element with a multiplicative inverse. Theorem 2 Let\u0026rsquo;s assume $D$ is a principal ideal domain.\n[1]: $D$ is a Noetherian ring. [2]: Every element in $D$ that is neither $0$ nor a unit can be expressed as a product of irreducible elements of $D$. [3]: If $\\left\u0026lt; p \\right\u0026gt;$ is a maximal ideal of $D$ and $p$ is an irreducible element in $D$. [4]: Every irreducible element in $D$ is a prime element. Explanation The term \u0026ldquo;Principal Ideal Domain\u0026rdquo; is often abbreviated as PID due to its length.\nNote that associative in \u0026ldquo;associates\u0026rdquo; shares the same spelling with associative in \u0026ldquo;associative property\u0026rdquo; but is a noun, indicating a relation where two elements can be expressed as products of a unit.\nExamples Integer Ring $\\mathbb{Z}$ In the integer ring $\\mathbb{Z}$, all ideals can be expressed as principal ideals like $n \\mathbb{Z} = \\left\u0026lt; n \\right\u0026gt;$.\nAll Fields $\\mathbb{F}$ Gaussian Integer Ring $\\mathbb{Z} [i]$ and Eisenstein Integer Ring $\\mathbb{Z} [\\omega]$ The Gaussian integer ring and the Eisenstein integer ring are rings formed by adding the imaginary unit $i := \\sqrt{-1}$ or $\\omega := (-1)^{1/3}$ to the integer ring $\\mathbb{Z}$, respectively.\nProof [1] Definition of Noetherian Ring: Let\u0026rsquo;s consider a ring $N$.\nAn ascending chain $\\left\\{ S_{i} \\right\\}_{i \\in \\mathbb{N} }$ of ideals in $N$ that satisfies $S_{1} \\le S_{2} \\le \\cdots$ is called an Ascending Chain. If for an ascending chain $\\left\\{ S_{i} \\right\\}_{i \\in \\mathbb{N} }$, there exists $n \\in \\mathbb{n}$ satisfying $S_{n} = S_{n+1} = \\cdots$, it is called Stationary. Meaning, at some point, the ideals in the chain no longer increase. A ring in which every ascending chain of ideals is stationary is called a Noetherian Ring. Consider an ascending chain of ideals $N_{1} \\le N_{2} \\le \\cdots$ in $D$ and its union $\\displaystyle N := \\bigcup_{k=1}^{ \\infty } N_{k}$. For some $i, j \\in \\mathbb{N}$, if we assume $$ a \\in N_{i} \\\\ b \\in N_{j} \\\\ N_{i} \\le N_{j} $$, then $( N_{j} , + , \\cdot )$, defined as an ideal, is a subring, and there exists an additive inverse $(-b) \\in N_{j}$ for $b$. Since $ab \\in N_{j}$, we have $(a-b), ab \\in N$, and by the subring test, $N$ is a subring of $D$. Moreover, since $N_{i}$ is an ideal, for all $d \\in D$, $d a = a d$ holds, and because of $da \\in N$, $N$ is an ideal of $D$.\nSince $D$ is a PID, all ideals are principal ideals, and for some $c \\in N$, it can be expressed as $N = \\left\u0026lt; c \\right\u0026gt;$. Here, since $\\displaystyle N = \\bigcup_{k=1}^{ \\infty } N_{k}$, if $c \\in N$, then a natural number $r \\in \\mathbb{N}$ exists satisfying $c \\in N_{r}$. $c \\in N_{r}$ implies the existence of a principal ideal smaller than $c$ with $c$ as its generator. Mathematically, $$ \\left\u0026lt; c \\right\u0026gt; \\le N_{r} \\le N_{r+1} \\le \\cdots \\le N = \\left\u0026lt; c \\right\u0026gt; $$, leading to $N_{r} = N_{r+1} = \\cdots$. Hence, $D$ is a Noetherian ring.\n■\n[2] If $d$ is an irreducible element, there is nothing to prove. Assuming a non-unit element $d_{1}, c_{1} \\in D$, let\u0026rsquo;s express it as $d = d_{1} c_{1}$.\nThen $\\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; d_{1} \\right\u0026gt;$, and by continually defining $d_{i} := d_{i+1} c_{i+1}$, we get an ascending chain $$ \\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; d_{1} \\right\u0026gt; \\le \\left\u0026lt; d_{2} \\right\u0026gt; \\le \\cdots $$. However, according to theorem [1], there must exist an end $a_{r}$ to this chain, and $a_{r}$ simultaneously becomes an irreducible factor of $a$. By setting the irreducible element dividing $d$ as $p_{1}$ and considering a non-unit element $f_{1}$ such that $d = p_{1} f_{1}$, we have $\\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; f_{1} \\right\u0026gt;$, and by continually defining $f_{i} := p_{i+1} f_{i+1}$, we get an ascending chain $$ \\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; f_{1} \\right\u0026gt; \\le \\left\u0026lt; f_{2} \\right\u0026gt; \\le \\cdots $$. This chain must also end at some point $f_{s}$ according to theorem [1], making $f_{s}$ an irreducible factor of $f_{i}$.\nThis process, repeated a finite number of times, confirms that $d$ can be expressed as a product of irreducible elements.\n■\n[3] $( \\implies )$\nAssuming that $p$, an irreducible element, is expressed as $p=ab$ with respect to a maximal ideal $\\left\u0026lt; p \\right\u0026gt;$ of $D$ and a non-unit element $a,b$.\nThen $\\left\u0026lt; p \\right\u0026gt; \\le \\left\u0026lt; a \\right\u0026gt;$, but if $\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$, then $b$ must be a unit, leading to the actuality of $\\left\u0026lt; p \\right\u0026gt; \\lneq \\left\u0026lt; a \\right\u0026gt;$. Since $\\left\u0026lt; p \\right\u0026gt;$ is a maximal ideal, it must be $\\left\u0026lt; a \\right\u0026gt; = D = \\left\u0026lt; 1 \\right\u0026gt;$, making $a$ and $1$ associates. In summary:\nIf $\\left\u0026lt; p \\right\u0026gt; \\ne \\left\u0026lt; a \\right\u0026gt;$, then $a$ is a unit, and If $\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$, then $b$ is a unit, making $p$ an irreducible element by necessity.\n$( \\impliedby )$\nAssuming a prime element $p=ab$ and $\\left\u0026lt; p \\right\u0026gt; \\le \\left\u0026lt; a \\right\u0026gt;$,\nIf $a$ is a unit, then $\\left\u0026lt; a \\right\u0026gt; = D$, causing no issues, but if $a$ is not a unit, then $b$ must necessarily be a unit.\nIf $b$ is a unit, it means for some $u \\in D$, $bu =1$ holds, and since $$ pu = abu = a $$, we get $\\left\u0026lt; p \\right\u0026gt; \\ge \\left\u0026lt; a \\right\u0026gt;$, meaning $\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ must hold. In summary:\nEither $\\left\u0026lt; a \\right\u0026gt; = D$ must hold or $\\left\u0026lt; a \\right\u0026gt; = \\left\u0026lt; p \\right\u0026gt;$ must hold, making $\\left\u0026lt; p \\right\u0026gt;$ a maximal ideal.\n■\n[4] If $p$ is an irreducible element, then $\\left\u0026lt; p \\right\u0026gt;$, by theorem [3], is a maximal ideal and hence a prime ideal.\nIf $p$ divides $ab$, then $(ab) \\in \\left\u0026lt; p \\right\u0026gt;$ holds, and since $\\left\u0026lt; p \\right\u0026gt;$ is a prime ideal, either $a \\in \\left\u0026lt; p \\right\u0026gt;$ or $b \\in \\left\u0026lt; p \\right\u0026gt;$ holds. Rewritten differently, if $p \\mid ab$, then either $p \\mid a$ or $p \\mid b$, making $p$ a prime element.\n■\nSee Also Euclidean Domain $\\implies$ Principal Ideal Domain $\\implies$ Unique Factorization Domain $\\implies$ Integral Domain Euclidean Domain $\\implies$ Principal Ideal Domain $\\implies$ Noetherian Ring Fraleigh. (2003). A first course in abstract algebra (7th Edition): p389~391, 394.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra (7th Edition): p392~393.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":825,"permalink":"https://freshrimpsushi.github.io/en/posts/825/","tags":null,"title":"Principal Ideal Domain"},{"categories":"상미분방정식","contents":"Formula1 This is table of Laplace transform.\n$f(t)=\\mathcal{L^{-1}}$ $F(s)=\\mathcal{L} \\left\\{ f(t) \\right\\}$ Derivation $1$ $\\dfrac{1}{s}$ link $e^{at}$ $\\dfrac{1}{s-a}$ link $t^n$ $\\dfrac{n!}{s^{n+1}}$ link $t^{p}$ $\\dfrac{ \\Gamma (p+1) }{ s^{p+1}}$ link $t^{p}e^{at}$ $\\dfrac{ \\Gamma (p+1) }{ (s-a)^{p+1}}$ link $\\sin (at)$ $\\dfrac{a}{s^2+a^2}$ link $\\cos (at)$ $\\dfrac{s}{s^2+a^2}$ link $e^{at}\\sin(bt)$ $\\dfrac{b}{(s-a)^2 +b^2}$ link $e^{at}\\cos(bt)$ $\\dfrac{s-a}{(s-a)^2+b^2}$ link $\\sinh (at)$ $\\dfrac{a}{s^2-a^2}$ link $\\cosh (at)$ $\\dfrac{s}{s^2-a^2}$ link $e^{at} \\sinh (bt)$ $\\dfrac{b}{(s-a)^2-b^2}$ link $e^{at} \\cosh (bt)$ $\\dfrac{s-a}{(s-a)^2-b^2}$ link $u_{c}(t)= \\begin{cases} 0 \u0026amp; t\u0026lt;c \\\\ 1 \u0026amp; t\\ge c\\end{cases}$ $\\dfrac{e^{-cs}}{s}$ link $u_{c}(t)f(t-c)$ $e^{-cs}F(s)$ link $f^{\\prime}(t)$ $s\\mathcal{L} \\left\\{ f(t) \\right\\} -f(0)$ link $f^{(n)}$ ${s^n\\mathcal {L}\\left\\{ f(t) \\right\\} -s^{n-1}f(0) - \\cdots -f^{(n-1)}(0) }$ link $f(t)=f(t+T)$ $\\dfrac{\\displaystyle \\int_{0}^T e^{-st}f(t)dt}{1-e^{-st}}$ link $\\delta (t-t_{0})$ $e^{-st_{0}}$ link $f(ct)$ $\\frac{1}{c}F \\left( \\frac{s}{c} \\right)$ link $\\frac{1}{k}f (\\frac{t}{k} )$ $F(ks)$ link $\\frac{1}{a} e^{-\\frac{b} {a}t}f\\left(\\frac{t}{a}\\right)$ $F(as+b)$ link $t^{n}f(t)$ $(-1)^{n}F^{(n)}(s)$ link William E. Boyce , Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), Chapter6 The Laplace Transform\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":743,"permalink":"https://freshrimpsushi.github.io/en/posts/743/","tags":null,"title":"Laplace Transform Table"},{"categories":"위상수학","contents":"Definition 1 Let\u0026rsquo;s say we have a topological space $\\left( X, \\mathscr{T} \\right)$.\nA set $\\mathscr{O} \\subset \\mathscr{T}$ consisting of open sets of $X$ is called an open covering of $A$ if it satisfies the following: $$ A \\subset \\bigcup_{O \\in \\mathscr{O}} O $$ A subset $\\mathscr{O} ' $ of $\\mathscr{O}$ is called a subcover of $\\mathscr{O}$. If the cardinality of $\\mathscr{O} ' $ is a natural number, it is called a finite subcover. $X$ is said to be compact if every open cover of $X$ has a finite subcover. In other words, for every open cover $\\mathscr{O}$, if there exists a finite set $\\mathscr{O} ' = \\left\\{ O_{1} , \\cdots , O_{n} \\right\\} \\subset \\mathscr{O}$ satisfying the following, then $X$ is compact: $$ X = \\bigcup_{i=1}^{n} O_{i} $$ If a subspace $A$ of $X$ is compact as a subspace, then $A$ is said to be compact. Let\u0026rsquo;s consider $X$ as a topological space. A subset $K$ is said to be precompact or relatively compact if the closure $\\overline{K}$ of $K$ is compact. Explanation Compactness Considering how useful the condition of compactness was in introductory analysis, it\u0026rsquo;s natural to seek its generalization. While the generalization might have made the terminology more complex, the essence hasn\u0026rsquo;t changed.\nIn fact, compactness is crucially applied in various theories. Being compact implies that a set can be considered in finite pieces, making it an excellent condition for rigorous proofs. Conversely, demonstrating that a certain set $A$ appearing in a proof is indeed compact often becomes a key part of the argument.\nPrecompactness Precompactness suggests that even if $K$ itself is not compact, taking the closure of $K$ results in a compact set, indicating the potential for $K$ to become compact. In metric spaces, this is also known as a totally bounded space. The alternative term relatively compact comes from the relative nature of closure; if $K$ is considered not as a subspace but as the entire space itself, then $K$ is closed in $K$, making $\\overline{K}$ compact, which essentially means $K$ is (relatively) compact.\nPrecompactness can also be defined in terms of sequences:\n$K \\subset X$ is precompact if, for every sequence $\\left\\{ x_{n} \\right\\} \\subset K$ defined in $K$, there exists a subsequence $\\left\\{ x_{n '} \\right\\} \\subset \\left\\{ x_{n} \\right\\}$ that converges to $x \\in X$.\nExpressed mathematically as:\n$$ K : \\text{precompact} \\iff \\forall \\left\\{ x_{n} \\right\\} \\subset K, \\exists \\left\\{ x_{n '} \\right\\} \\subset \\left\\{ x_{n} \\right\\} : x_{n '} \\to x \\in X \\text{ as } n \\to \\infty $$\nEspecially, when the condition involves $x \\in K$ instead of $x \\in X$, $K$ is termed sequentially compact.\nTheorem [1]: $A$ being compact is equivalent to every open cover of $A$ having a finite subcover. [2]: If a subset $F$ of a compact set $K$ is closed, then $F$ is compact. [3]: $X$ being compact is equivalent to every family of closed sets in $X$ having the finite intersection property (f.i.p.), meaning taking intersections over any collection of these sets results in a non-empty set. Proof [1] $\\Gamma$ represents an index set.\n$( \\implies )$\nAssume $A \\subset X$ is compact and $\\mathscr{U} := \\left\\{ U_{\\alpha} : \\alpha \\in \\Gamma \\right\\}$ is an open cover of $A$. Then, $U_{\\alpha} \\cap A$ is an open set in the subspace $A$ of $X$, and $\\mathscr{O} := \\left\\{ U_{\\alpha} \\cap A : U_{\\alpha} \\in \\mathscr{U} \\right\\}$ becomes an open cover of $A$. Since $A$ is compact, there exists $\\alpha_{1} , \\cdots , \\alpha_{n} \\in \\Gamma$ satisfying $\\displaystyle A \\subset \\bigcup_{i=1}^{n} \\left( U_{\\alpha_{i}} \\cap A \\right)$. Hence, $\\left\\{ U_{\\alpha_{1}} , \\cdots , U_{\\alpha_{n}} \\right\\}$ exists as a finite subcover of $\\mathscr{U}$.\n$( \\impliedby )$\nConsider an open cover $\\mathscr{O} := \\left\\{ O_{\\alpha} : O_{\\alpha} \\text{ is open in } A, \\alpha \\in \\Gamma \\right\\}$ of $A$ consisting of open sets. For each $\\alpha \\in \\Gamma$, there exists an open set $U_{\\alpha}$ satisfying $U_{\\alpha} \\cap A = O_{\\alpha}$. The set of these $U_{\\alpha}$, denoted as $\\mathscr{U} := \\left\\{ U_{\\alpha} : \\alpha \\in \\Gamma \\right\\}$, forms an open cover of $A$. Since every open cover has a finite subcover $\\left\\{ U_{\\alpha_{1}} , \\cdots , U_{\\alpha_{n}} \\right\\}$, $\\left\\{ O_{\\alpha_{1}} , \\cdots , O_{\\alpha_{n}} \\right\\}$ becomes a finite subcover of $\\mathscr{O}$.\n■\n[2] $$ F \\subset K \\subset X $$ Assume $F$ is a closed set in $X$ and $K$ is compact. Consider an open cover $\\left\\{ U_{\\alpha} \\right\\}$ of $F$. Since $F$ is closed, $F^{c}$ is an open set in $X$ and hence $F^{c} \\cup \\left\\{ U_{\\alpha} \\right\\}$ becomes one of the open covers of $K$. Since $K$ is compact, there exists a finite subcover $\\Phi$ satisfying $F \\subset K \\subset \\Phi$.\nIf $F^{c}\\notin \\Phi$, then $\\Phi$ becomes a finite subcover of $\\left\\{ U_{\\alpha} \\right\\}$, making $F$ compact. If $F^{c}\\in \\Phi$, then $\\Phi \\setminus \\left\\{ F^{c} \\right\\}$ becomes a finite subcover of $\\left\\{ U_{\\alpha} \\right\\}$, making $F$ compact. In both possible cases, $F$ is compact.\n■\n[3] Strategy: The terminology here is quite complex, so understanding the terms is crucial. Having the finite intersection property (f.i.p.) for $\\mathscr{C}$ does not guarantee $\\displaystyle \\bigcap_{C \\in \\mathscr{C}} C \\ne \\emptyset$; the compact condition is necessary. The definition of compactness involves unions of open sets, while this theorem involves intersections of closed sets, highlighting the relationship between finite intersection properties and compactness.\n$\\Gamma$ is an index set.\nFinite Intersection Property: For a family $\\mathscr{A} \\subset \\mathscr{P}(X)$ of subsets of $X$, having the finite intersection property (f.i.p) means that for every finite subfamily $A \\subset \\mathscr{A}$, the intersection of its members is non-empty. This is expressed mathematically as: $$ \\forall A \\subset \\mathscr{A}, \\bigcap_{a \\in A} a \\ne \\emptyset $$\n$( \\implies )$\nAssuming $X$ is compact and $\\mathscr{C} := \\left\\{ C_{\\alpha} : C_{\\alpha} \\text{ is closed in } X, \\alpha \\in \\Gamma \\right\\}$ has the f.i.p., suppose $\\displaystyle \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} = \\emptyset$ and choose $\\mathscr{O} := \\left\\{ X \\setminus C_{\\alpha} : C_{\\alpha} \\in \\mathscr{C} \\right\\}$. Then, $$ \\begin{align*} \\bigcup_{\\alpha \\in \\Gamma} ( X \\setminus C_{\\alpha}) =\u0026amp; X \\setminus \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} \\\\ =\u0026amp; X \\setminus \\emptyset \\\\ =\u0026amp; X \\end{align*} $$ making $\\mathscr{O}$ an open cover of $X$. Since $X$ is compact, $\\mathscr{O}$ has a finite subcover $\\displaystyle \\left\\{ (X \\setminus C_{\\alpha_{1}}) , \\cdots ,(X \\setminus C_{\\alpha_{n}}) \\right\\}$, which implies $$ X = \\bigcup_{i=1}^{n} ( X \\setminus C_{\\alpha_{i}}) = X \\setminus \\bigcap_{i=1}^{n} C_{\\alpha_{i}} $$, leading to $\\displaystyle \\bigcap_{i=1}^{n} C_{\\alpha_{i}} = \\emptyset$. This contradicts the assumption that $\\mathscr{C}$ has the f.i.p., thus $\\displaystyle \\bigcap_{C \\in \\mathscr{C}} C \\ne \\emptyset$ must hold.\n$( \\impliedby )$\nConsider the open cover $\\mathscr{O} := \\left\\{ O_{\\alpha} : O_{\\alpha} \\text{ is open in } A, \\alpha \\in \\Gamma \\right\\}$ of $X$ and $\\mathscr{C} := \\left\\{ X \\setminus O_{\\alpha} : O_{\\alpha} \\in \\mathscr{O} \\right\\}$. Since $$ \\begin{align*} \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} \u0026amp;= \\bigcap_{\\alpha \\in \\Gamma} ( X \\setminus O_{\\alpha}) \\\\ =\u0026amp; X \\setminus \\bigcup_{\\alpha \\in \\Gamma} O_{\\alpha} \\\\ =\u0026amp; X \\setminus X \\\\ =\u0026amp; \\emptyset \\end{align*} $$, by contraposition, $\\mathscr{C}$ lacks the f.i.p., meaning there exists $C_{\\alpha_{1}} , \\cdots , C_{\\alpha_{n}} \\in \\mathscr{C}$ satisfying $\\displaystyle \\bigcap_{i=1}^{n} C_{\\alpha_{i}} = \\emptyset$. Then, $$ \\begin{align*} X \\setminus \\bigcup_{i=1}^{n} O_{i} =\u0026amp; X \\setminus \\bigcup_{i=1}^{n} (X \\setminus C_{i}) \\\\ =\u0026amp; X \\setminus \\left( X \\setminus \\bigcap_{i=1}^{n} C_{i} \\right) \\\\ =\u0026amp; \\bigcap_{i=1}^{n} C_{i} \\\\ =\u0026amp; \\emptyset \\end{align*} $$, making $\\displaystyle X = \\bigcup_{i=1}^{n} O_{i}$. Hence, having a finite subcover for every open cover implies compactness.\n■\nSee Also Compactness in Metric Spaces Munkres. (2000). Topology (2nd Edition): p164.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":489,"permalink":"https://freshrimpsushi.github.io/en/posts/489/","tags":null,"title":"What are Compact and Precompact in Topological Spaces?"},{"categories":"위상수학","contents":"정리 일반적으로, 위상공간에서 수열의 극한은 유일하지 않다.\n설명 도대체 이게 무슨 소린가 싶겠지만 놀랍게도 사실이다. 우리는 이제껏 해석학 등에서 수열을 포함하는 구간이 점점 좁아지면서 한 점으로 수렴하는 이미지를 떠올려왔다. 하지만 위상수학에서 정의하는 수렴의 개념에 따르면 위상공간에 따라선 한 점으로 수렴할 이유가 전혀 없다.\n극한의 유일성을 보장하기 위해서는 하우스도르프 공간이 주로 가정된다.\n반증 극한이 복수로 존재하는 반례를 제시하면 충분하다.\n여유한공간 $\\left( \\mathbb{R} , \\mathscr{T}_{f} \\right)$ 에서 서로 다른 점들로 이루어진 수열 $\\left\\{ x_{n} \\right\\}$ 을 생각해보자. 우선 $\\left\\{ x_{n} \\right\\}$ 이 수렴하는 점은 임의로 $x \\in \\mathbb{R}$ 라 두자. 여기서 $x$ 를 포함하는 열린 집합 $U \\in \\mathscr{T}_{f}$ 가 존재하고, $\\mathbb{R} \\setminus U$ 는 유한집합이다. $\\left\\{ x_{n} \\right\\}$ 은 서로 다른 점들로 이루어져있으므로 모든 $n \u0026gt; n_{0}$ 에 대해 $x_{n} \\notin \\mathbb{R} \\setminus U$ 를 만족하는 $n_{0} \\in \\mathbb{N}$ 이 존재할 수 없다. 그런데 $\\left\\{ x_{n} \\right\\}$ 이 수렴하긴 수렴하므로, 모든 $n \u0026gt; n_{0}$ 에 대해 $x_{n} \\in U$ 를 만족하는 $n_{0} \\in \\mathbb{N}$ 이 존재해야한다. 수렴의 정의에 따라 $\\left\\{ x_{n} \\right\\}$ 는 $x$ 로 수렴하긴 하지만 여기서 $x$ 는 무엇이 되든 딱히 상관이 없다.\n■\n이해가 잘 안 간다면 수렴의 정의가 어떻게 변했는지와 여유한공간의 열린 집합이 뭔지 생각해보는 게 좋다.\n기존의 거리공간에서 열린 집합이라고 하면 한 점을 중심으로 주어진 거리 이내의 점들의 집합을 말하는 것이었다. 따라서 \u0026lsquo;모든 열린 집합\u0026rsquo;이라고는 하지만 실제로는 그 점의 근방에서 조건을 만족시키는 것이 관건이었다. 아무리 작게 잡아도 계속해서 조건을 만족시킨다면 \u0026lsquo;모든 열린 집합\u0026rsquo;에 대한 체크가 끝나는 것이나 마찬가지인 것이다.\n하지만 여유한공간에서 $U$ 와 또다른 열린 공간을 생각해본다면, $U \\setminus \\left\\{ a \\right\\}$ 와 같은 것도 열린 집합이 된다. 전체공간이 실수든 뭐든 모든 점을 하나씩 빼기만해도 열린 집합은 열린 집합으로, 여기서 \u0026lsquo;거리\u0026rsquo;를 생각하는 건 무의미하다. 따라서 모든 열린 집합에 대해 체크한다고 하더라도 거리공간처럼 점점 작아질 이유가 없으며, 극한이 특정되지 않는 것이다.\n","id":407,"permalink":"https://freshrimpsushi.github.io/en/posts/407/","tags":null,"title":"일반적인 위상공간에서 수열의 극한은 유일하지 않다"},{"categories":"양자역학","contents":" 🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧\n조화진동자 문제를 연산자 방법으로 풀 때 아주 유용한 연산자가 있다.바로 조화진동자의 사다리연산자$\\mathrm{Ladder\\ Operator}$이다.에너지 연산자인 해밀토니안$H$과도 치환이 가능하고,사다리 연산자의 특징을 이용해 바닥상태부터의 고유함수도 구할 수 있다.조화 진동자의 고전적인 해밀토니안$H$을 인수분해 하는 것에서 새 연산자를 정의하는 힌트를 얻을 수 있다.$\\begin{align*} H =\u0026amp;\\ \\frac{1}{2m}p^2+\\frac{1}{2m}mw^2x^2 \\\\ =\u0026amp;\\ \\frac{1}{2m} \\left(p^2+m^2w^2x^2 \\right) \\\\ =\u0026amp;\\ \\frac{1}{2m} (ip+mwx)(-ip+mwx) \\end{align*}$여기서 힌트를 얻어 조화 진동자의 두 사다리 연산자를 다음과 같이 정의할 수 있다.하필 $i$가 $x$항이 아닌 $p$항에 붙는 이유나 앞의 상수가 $ \\dfrac{1}{\\sqrt{2\\hbar mw}}$인 이유는 최종적으로 나올 식의 형태를 생각해서 계산을 쉽게 할 수 있게 한 것이다.$\\displaystyle a_+=\\frac{1}{\\sqrt{2\\hbar mw}}(ip+mwx) $$ a_-=\\frac{1}{\\sqrt{2\\hbar mw}}(-ip+mwx)=(a_+)^{\\ast}$이제 두 연산자의 곱으로 해밀토니안$H$을 표현해보자.연산자이므로 곱셈의 순서에 따라 결과가 달라질 수 있음을 유의해야 한다.또한 표준교환관계식$\\mathrm{canonical\\ commutation\\ relation}$ $([x,p]=i\\hbar)$을 사용한다.$\\begin{align*} a_-a_+ =\u0026amp;\\ \\frac{1}{2\\hbar mw} (p^2 + m^2w^2x^2+mwipx-mwixp) \\\\ =\u0026amp;\\ \\frac{1}{2\\hbar mw} (p^2 + m^2w^2x^2-mwi[x,p]) \\\\ =\u0026amp;\\ \\frac{1}{2\\hbar mw} (p^2 + ^2w^2x^2+mw\\hbar) \\\\ =\u0026amp;\\ \\frac{1}{2\\hbar mw} (p^2 + m^2w^2x^2) + \\frac{1}{2\\hbar mw}(mw\\hbar) \\\\ =\u0026amp;\\ \\frac{1}{\\hbar w}\\frac{1}{2m} (p^2 + m^2w^2x^2) + \\frac{1}{2} \\\\ =\u0026amp;\\ \\frac{1}{\\hbar w}H+\\frac{1}{2} \\end{align*} $$ \\therefore H=\\hbar w(a_-a_+ - \\dfrac{1}{2})$곱의 순서를 바꾸고 같은 과정으로 계산하면$a_+a_-=\\dfrac{1}{\\hbar w}H-\\dfrac{1}{2} $$ \\implies H=\\hbar w(a_+a_- + \\dfrac{1}{2})$이로부터 두 연산자의 교환자도 계산할 수 있다.$[a_-,a_+]=1$이제 두 연산자$a_\\pm$를 이용하여 조화진동자의 슈뢰딩거 방정식을 새롭게 표현할 수 있다.$H\\psi=E\\psi $$ \\implies \\hbar w (a_\\pm a_\\mp \\pm \\dfrac{1}{2})\\psi=E\\psi$이 두 연산자 $a_+$, $a_-$의 이름은 각각 올림 연산자$\\mathrm{rasing\\ operator}$, 내림 연산자$\\mathrm{lowering\\ operator}$이다.이런 이름을 가지는 이유는 바로 고유함수에 적용시켰을 때 에너지(고유값)가 증가하거나 감소하기 때문이다.즉, 에너지를 올려주거나 내려주는 연산자라는 뜻이다.왜 그렇게 되는지는 다음 글을 참고하자다음 글 : 연산자 방법으로 조화진동자 문제 해결하기 : 사다리 연산자 적용\n🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧\n이전 글 : 연산자 방법으로 조화진동자 문제 풀기 : 사다리 연산자의 정의이제 사다리 연산자가 조화진동자의 고유함수에 어떻게 작용하는지 알아보자.참고로 임의의 상수와 임의의 연산자간의 교환자는 항상 $0$이다.아래의 수식 전개에서 사용한 관계식 $( [AB,C]=A[B,C]+[A,C]B,\\ \\ [a_-,a_+]=1 ) $$ H=(a_+a_-+\\dfrac{1}{2})\\hbar w$이므로, $ \\begin{align*} [H,a_+] =\u0026amp;\\ [(a_+a_-+\\dfrac{1}{2})\\hbar w,a_+] \\\\ =\u0026amp;\\ [a_+a_-\\hbar w,a_+]+[\\dfrac{1}{2}\\hbar w, a_+] \\\\ =\u0026amp;\\ \\hbar w[a_+a_-,a_+] \\\\ =\u0026amp;\\ \\hbar w(a_+[a_-,a_+] + [a_+,a_+]a_-) \\\\ =\u0026amp;\\ \\hbar w a_+ \\\\ =\u0026amp;\\ Ha_+ - a_+H \\end{align*} $같은 방법으로 $[H,a_-]$를 구하면$[H,a_-]=-\\hbar w a_-=Ha_- - a_-H$이제 위에서 구한 관계식을 써서 조화진동자의 고유함수에 적용시켜보자.$a_\\pm$는 고유함수$|\\psi\u0026gt;$에 대해서 고유값 방정식을 만족시키는 연산자가 아니므로$a_\\pm$대신 $H$가 고유함수에 적용될 수 있도록 모양을 바꿔준다.슈뢰딩거 방정식은 $H|\\psi\u0026gt;=E|\\psi\u0026gt; $$ \\begin{align*} Ha_+|\\psi\u0026gt; =\u0026amp;\\ (\\hbar wa_+ + a_+H)|\\psi\u0026gt; \\\\ =\u0026amp;\\ (\\hbar wa_+ + a_+E)|\\psi\u0026gt; \\\\ =\u0026amp;\\ (E+\\hbar w)a_+|\\psi\u0026gt; \\end{align*}$따라서 $|\\psi\u0026gt;$가 $H$에 대한 고유함수일 때 $a_+|\\psi\u0026gt;$역시 고유값 방 정식을 만족하는 고유함수이다.이 때 $a_+|\\psi\u0026gt;$의 고유값은 $(E+\\hbar w)$이다.일반식을 구하기 위해 두 번 적용시켜보자.한 번 적용했을 때의 결과를 여기서 사용한다.$\\begin{align*} H(a_+)^2|\\psi\u0026gt; =\u0026amp;\\ Ha_+a_+|\\psi\u0026gt; \\\\ =\u0026amp;\\ (a_+H+\\hbar w a_+)a_+|\\psi\u0026gt; \\\\ =\u0026amp;\\ a_+H+a_+|\\psi\u0026gt; + \\hbar w a_+a_+|\\psi\u0026gt; \\\\ =\u0026amp;\\ a_+(E+\\hbar w)a_+|\\psi\u0026gt; + \\hbar w a_+a_+|\\psi\u0026gt; \\\\ =\u0026amp;\\ (E+\\hbar w)(a_+)^2|\\psi\u0026gt; + \\hbar w (a_+)^2|\\psi\u0026gt; \\\\ =\u0026amp;\\ (E+2\\hbar w)(a_+)^2|\\psi\u0026gt; \\end{align*}$따라서 고유함수 $|\\psi\u0026gt;$에 $a_+$를 $n$번 적용하면 다음과 같은 결과를 얻는다.$\\implies H(a_+)^n|\\psi\u0026gt;=(E+n\\hbar w)(a_+)^n|\\psi\u0026gt;$마찬가지로 같은 방법으로 고유함수에 $a_-$를 적용시키면$Ha_-|\\psi\u0026gt;=(a_-H - \\hbar w a_-)|\\psi\u0026gt;=(E-\\hbar w)a_-|\\psi\u0026gt; $$ \\implies H(a_-)^n|\\psi\u0026gt;=(E-n\\hbar w)(a_-)^n|\\psi\u0026gt;$따라서 고유함수에 적용시킬수록 에너지가 커지는 $a_+$를 올림연산자$\\mathrm{rasing\\ operator}$라 한다.고유함수에 적용시킬수록 에너지가 작아지는 $a_-$를 내림연산자$\\mathrm{lowering\\ operator}$라 한다.다음글 : 연산자 방법으로 조화진동자 문제 풀기 : 에너지 준위와 바닥상태 🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧\n이전 글 : 연산자 방법으로 조화진동자 문제 풀기 : 사다리 연산자 적용계속해서 조화진동자의 에너지와 바닥상태의 고유함수를 구해보자.$E=\u0026lt;\\psi|H|\\psi\u0026gt;=\u0026lt;\\psi|(a_+a_-+\\dfrac{1}{2})\\hbar w|\\psi\u0026gt;$이 때 고유함수에 한 없이 $a_-$를 적용시킬 순 없다.에너지가 $0$보다 작아질 수는 없다는 말이다.에너지가 포텐셜 보다 작을 때는 해가 없기 때문에 $E\u0026gt;U$이어야 하기 때문이다.즉, 더 이상 에너지준위가 내려가지 않는 바닥상태$\\mathrm{ground\\ state}$가 있고바닥 상태에 내림연산자$\\mathrm{lowering\\ operator}$를 적용시키면 $0$이다.즉, 바닥상태를 $|\\psi_{0}\u0026gt;$라고 하면$a_-|\\psi_{0}\u0026gt;=0$이제 이 사실을 이용해서 바닥상태의 에너지를 구해보자.$\\begin{align*} H|\\psi_{0}\u0026gt; =\u0026amp;\\ (a_+a_- + \\frac{1}{2})\\hbar w|\\psi_{0}\u0026gt; \\\\ =\u0026amp;\\ \\hbar w a_+a_-|\\psi_{0}\u0026gt;+\\frac{1}{2}\\hbar w|\\psi_{0}\u0026gt; \\\\ =\u0026amp;\\ \\frac{1}{2}\\hbar w |\\psi_{0}\u0026gt; \\end{align*} $$ \\therefore H|\\psi_{0}\u0026gt;=\\dfrac{1}{2}\\hbar w |\\psi_{0}\u0026gt;$바닥상태의 에너지는 $E_{0}=\\dfrac{1}{2}\\hbar w$이다.$0$이 아니다!이전 글에서 사다리 연산자는 고유함수의 에너지를 $\\pm \\hbar w$만큼 변화시키는 것을 알았다.즉, 첫 번째 들뜬 상태의 에너지는$E_{1}=\\dfrac{1}{2}\\hbar w +\\hbar w$두 번째 들뜬 상태의 에너지는$E_2=\\dfrac{1}{2}\\hbar w+2\\hbar w$따라서 $n$번째 에너지에 대해서 일반적으로 표현할 수 있다.$E_{n}=(n+\\dfrac{1}{2})\\hbar w,\\ \\ (n=0,\\ 1,\\ 2,\\ \\cdot)$에너지 준위가 등간격($\\hbar w $)으로 이루어져있음을 알 수 있다.이제 바닥상태의 고유함수 $\\psi_{0}$를 구체적으로 구해보자.참고로 운동량 연산자 $p$는 $p={\\hbar \\over i}{\\partial \\over \\partial x}$$a_-\\psi_{0}=0 $$ \\implies {1 \\over {\\sqrt{2\\hbar mw}} }(-ip+mwx) \\psi_{0}=0 $$ \\implies (-ip+mwx) \\psi_{0}=0 $$ \\implies (\\hbar\\frac{\\partial}{\\partial x} +mwx) \\psi_{0}=0 $$ \\implies \\frac{\\partial}{\\partial x}\\psi_{0}=-\\frac{mwx}{\\hbar}\\psi_{0}$여기서 변수분리를 해주면$\\displaystyle \\frac{1}{\\psi_{0}} d\\psi_{0}=-\\frac{mwx}{\\hbar} dx $$ \\implies \\ln (\\psi_{0}) = -\\frac{mwx^2}{2\\hbar}+C $$ \\implies \\psi_{0}(x)=Ce^{-\\frac{mwx^2}{2\\hbar}}$이제 규격화상수 $C$만 구하면 바닥상태의 고유함수를 정확하게 알 수 있다.규격화 조건에 의해$\\displaystyle \\int_{-\\infty}^{\\infty} (\\psi_{0})^{\\ast}\\psi_{0}dx=1 $$ \\implies |C|^2\\int_{-\\infty}^{\\infty}e^{-\\frac{mwx^2}{\\hbar}} dx=1$이 적분을 계산하기 위해 가우스 적분 : $e^{-x^2}$꼴의 정적분을 참고하자.적분하기 편하게 치환을 해주면 $\\sqrt{\\frac{mw}{\\hbar}}x \\equiv y $$ dx=\\sqrt{\\frac{\\hbar}{mw}}dy$, 적분 범위는 변함 없다.이제 원래의 식에 대입해주면$\\displaystyle \\implies |C|^2\\sqrt{\\dfrac{\\hbar}{mw}}\\int_{-\\infty}^{\\infty}e^{-^y2} dy=1 $$ \\implies |C|^2\\sqrt{\\dfrac{\\hbar}{mw}}\\sqrt{\\pi}=1 $$ \\implies |C|^2\\sqrt{\\dfrac{\\hbar \\pi}{mw}}=1 $$ \\therefore |C|^2=\\sqrt{\\frac{mw}{\\hbar \\pi}},\\ \\ C=(\\frac{mw}{\\hbar \\pi})^{\\frac{1}{4}}$따라서 최종적으로 바닥상태의 고유함수는$\\psi_{0} (x)=(\\frac{mw}{\\hbar \\pi})^{\\frac{1}{4}} e^{-\\frac{mwx^2}{2\\hbar}}$다음 글에서는 $n$에 대해서 일반화된 고유함수를 구해보겠다.다음 글 : 연산자 방법으로 조화진동자 문제 풀기 : 일반화된 고유함수\n🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧\n이전 글 : 연산자 방법으로 조화진동자 문제 풀기 : 에너지 준위와 바닥상태의 고유함수 이제 조화진동자의 사다리 연산자와 바닥상태의 고유함수로부터 일반화된 고유함수를 구해보자.사다리 연산자 $a_\\pm$는 고유함수 $\\psi_{n}$의 상태를 한 단계 올려주거나 내려준다.따라서 다음과 같은 식을 세울 수 있다.$a_+|\\psi_{n}\u0026gt;=C_+|\\psi_{n+1}\u0026gt; $$ a_-|\\psi_{n}\u0026gt;=C_-|\\psi_{n-1}\u0026gt; $$ C_\\pm$는 각각 $n$번째와 $(n+1)$번째, $n$번째와 $(n-1)$번째 상태 사이의 비례계수이다. 이 비례계수를 정확하게 구해보자.각 고유함수들이 규격화된 고유함수라고 가정하고 규격화조건을 사용하자.규격화된 고유 함수는 자신과 내적하면 값이 $1$이다.사다리 연산자와 해밀토니안 사이의 관계식을 사용하면.$\\begin{align*} (a_+|\\psi_{n}\u0026gt;)^{\\ast}(a_+|\\psi_{n}\u0026gt;) =\u0026amp;\\ \u0026lt;\\psi_{n}|a_-a_+|\\psi_{n}\u0026gt; \\\\ =\u0026amp;\\ \u0026lt;\\psi_{n}| \\frac{1}{\\hbar w}H + \\frac{1}{2} |\\psi_{n}\u0026gt; \\\\ =\u0026amp;\\ \\frac{1}{\\hbar w} E_{n} \u0026lt;\\psi_{n}|\\psi_{n}\u0026gt; + \\frac{1}{2} \u0026lt;\\psi_{n}|\\psi_{n}\u0026gt; \\\\ =\u0026amp;\\ \\frac{1}{\\hbar w}(n+\\frac{1}{2})\\hbar w +\\frac{1}{2} \\\\ =\u0026amp;\\ n+1 \\end{align*}$ 반면 위의 비례식을 사용하면$\\begin{align*} (a_+|\\psi_{n}\u0026gt;)^{\\ast}(a_+|\\psi_{n}\u0026gt;) =\u0026amp;\\ (C_+|\\psi_{n+1}\u0026gt;)^{\\ast}(C_+|\\psi_{n+1}\u0026gt;) \\\\ =\u0026amp;\\ |C_+|^2\u0026lt;\\psi_{n+1}|\\psi_{n+1}\u0026gt; \\\\ =\u0026amp;\\ |C_+|^2 \\end{align*}$ 따라서 위의 두 결과를 종합하면 $C_+$값을 얻을 수 있다.$|C_+|^2=n+1 $$ \\implies C_+=\\sqrt{n+1} $$ \\therefore a_+|\\psi_{n}\u0026gt;=\\sqrt{n+1}|\\psi_{n+1}\u0026gt;$ 같은 방법으로 $C_-$도 구할 수 있다.과정은 생략하고 결과만 적을 테니 직접 해보길 바란다.$|C_-|^2=n $$ \\implies C_-=\\sqrt n $$ \\implies a_-|\\psi_{n}\u0026gt;=\\sqrt n |\\psi_{n-1}\u0026gt;$ 이제 이 결과와 바닥상태 $|\\psi_{0}\u0026gt;$를 이용해서 일반화된 $n$번째 상태를 구해보자.$ a_+|\\psi_{n}\u0026gt;=\\sqrt{n+1}|\\psi_{n+1}\u0026gt;$이므로 $ |\\psi_{n+1}\u0026gt;=\\frac{1}{\\sqrt{n+1}}a_+|\\psi_{n}\u0026gt;$이다. 1번째 들뜬 상태$|\\psi_{1}\u0026gt;=a_+|\\psi_{0}\u0026gt;$2번째 들뜬 상태$\\begin{align*} |\\psi_2\u0026gt; =\u0026amp;\\ \\frac{1}{\\sqrt{2}}a_+|\\psi_{1}\u0026gt; \\\\ =\u0026amp;\\ \\frac{1}{\\sqrt{2}}a_+a_+|\\psi_{0}\u0026gt; \\\\ =\u0026amp;\\ \\frac{1}{\\sqrt{2}}(a_+)^2|\\psi_{0}\u0026gt; \\end{align*}$3번째 들뜬 상태$\\begin{align*} |\\psi_{3}\u0026gt; =\u0026amp;\\ \\frac{1}{\\sqrt{3}}a_+|\\psi_2\u0026gt; \\\\ =\u0026amp;\\ \\frac{1}{\\sqrt{3}}a_+\\frac{1}{\\sqrt{2}}(a_+)^2|\\psi_{0}\u0026gt; \\\\ =\u0026amp;\\ \\frac{1}{\\sqrt{3!}}(a_+)^3|\\psi_{0}\u0026gt; \\end{align*}$4번째 들뜬 상태$\\begin{align*} |\\psi_{4}\u0026gt; =\u0026amp;\\ \\frac{1}{\\sqrt{4}}a_+|\\psi_{3}\u0026gt; \\\\ =\u0026amp;\\ \\frac{1}{\\sqrt{4}}a_+\\frac{1}{\\sqrt{3!}}(a_+)^3|\\psi_{0}\u0026gt; \\\\ =\u0026amp;\\ \\frac{1}{\\sqrt{4!}}(a_+)^4|\\psi_{0}\u0026gt; \\end{align*}$ 따라서 $|\\psi_{n}\u0026gt;=\\frac{1}{\\sqrt{n!}}(a_+)^n|\\psi_{0}\u0026gt; $ 여기에 이전에 구한 바닥상태의 고유함수 $\\psi_{0}(x)=(\\frac{mw}{\\hbar \\pi})^{\\frac{1}{4}} e^{-\\frac{mwx^2}{2\\hbar}}$와사다리 연산자 $a_+=\\frac{1}{\\sqrt{2\\hbar mw }}(mwx+ip)=\\sqrt{\\frac{mw}{2\\hbar}}x-\\sqrt{\\frac{\\hbar}{2mw}}\\frac{d}{dx}$를 대입하면 $\\psi_{n} (x) =\\frac{1}{\\sqrt{n!}} \\left( \\sqrt{\\frac{mw}{2\\hbar}}x-\\sqrt{\\frac{\\hbar}{2mw}}\\frac{d}{dx} \\right)^n (\\frac{mw}{\\hbar \\pi})^{\\frac{1}{4}} e^{-\\frac{mwx^2}{2\\hbar}}$이것이 바로 연산자를 이용해서 구한 조화진동자의 일반화된 $n$번째 상태($n$번째 고유함수)이다.\n","id":362,"permalink":"https://freshrimpsushi.github.io/en/posts/362/","tags":null,"title":"연산자 방법으로 조화진동자 문제 풀기  사다리 연산자의 정의"},{"categories":"선형대수","contents":"Definition1 Let $W$ be a non-empty subset of the vector space $V$. If $W$ satisfies the definition of a vector space with respect to the addition and scalar multiplication defined in $V$, then $W$ is called a subspacesubspace of the vector space $V$, and is denoted as follows:\n$$ W \\le V $$\nExplanation To determine whether a subset $W$ of a vector space $V$ is a subspace of $V$, it must satisfy all 10 rules required for being a vector space. It would be quite cumbersome and difficult to check all 10 rules every time we consider a subset of a vector space. Fortunately, due to being a subset of some vector space, some rules are trivially satisfied.\nFor instance, if $\\mathbf{u},\\mathbf{v}$ is an element of $W$, it is also an element of $V$, hence (A2), (A3), (M2)-(M5) are naturally satisfied. Therefore, it is enough to verify the closure under addition (A1), the existence of the zero vector (A4), the existence of additive inverses (A5), and the closure under scalar multiplication (M1) to conclude that $W$ is a subspace. However, in practice, it\u0026rsquo;s even simpler. Satisfying conditions (A1) and (M1) is a necessary and sufficient condition for being a subspace.\nExamples Examples of subspaces of the vector space $V$ include:\nItself $V$ Cosets $v + W$ For a linear transformation $T : V \\to W$,\nThe null space of $T$ $N(T) \\le V$ The range of $T$ $R(T) \\le W$ For a linear transformation $T : V \\to V$,\nEigenspace $E_{\\lambda}$ $T$-Invariant space $T$-Cyclic space Theorem: Subspace Test Let $W$ be a non-empty subset of the vector space $V$. It is a necessary and sufficient condition for $W$ to be a subspace of $V$ if $W$ satisfies the following two conditions:\n(A1) The subset $W$ is closed under addition as defined in $V$.\n(M1) The subset $W$ is closed under scalar multiplication as defined in $V$.\nProof $(\\implies)$\nAssume that $W$ is a subspace of $V$. If $W$ is a subspace, it is trivial that $W$ satisfies (A1) and (M1) by the definition of a vector space.\n$(\\impliedby)$\nAssume that $W$ satisfies $(A1)$ and $(M1)$. Let $\\mathbf{u} \\in W$. Then, $W$ is closed under scalar multiplication, and since $0\\mathbf{u}=\\mathbf{0}$, the following is true:\n$$ 0 \\mathbf{u} = \\mathbf{0} \\in W $$\nFor the same reason, the following is true by $(-1)\\mathbf{u}=-\\mathbf{u}$:\n$$ (-1)\\mathbf{u} = -\\mathbf{u} \\in W $$\nTherefore, $W$ satisfies (A1)-(M5), thus it is a subspace of $V$.\n■\nTheorem: The Intersection of Subspaces is a Subspace2 Let $W_{1}, W_2$ be subspaces of the vector space $V$. Then, $W_{1} \\cap W_2$ is also a subspace of $V$.\nProof By the Subspace Test, we need to check if $W_{1} \\cap W_2$ satisfies (A1) and (M1). Let $W= W_{1} \\cap W_2$.\n(A1)\nSince $W = W_{1} \\cap W_2$, any two vectors $\\mathbf u,\\mathbf v$ inside $W$ are also contained in $W_{1}$ and $W_2$. Since $W_{1}, W_2$ is a subspace, it is closed under addition. Therefore, the following is true:\n$$ \\mathbf u + \\mathbf v \\in W_{1}, \\quad \\mathbf u + \\mathbf v \\in W_2 $$\nHence, by the definition of intersection, the following is true:\n$$ \\mathbf u + \\mathbf v \\in W $$\nAny two vectors $\\mathbf u,\\ \\mathbf v$ inside $W$ implies that $\\mathbf u + \\mathbf v$ is also an element of $W$, so $W$ is closed under addition and satisfies (A1).\n(M1)\nThe proof follows similarly to the above case.\nSince $W = W_{1} \\cap W_2$, any vector $\\mathbf u$ inside $W$ is also contained in $W_{1}$ and $W_2$. Since $W_{1},\\ W_2$ is a subspace, it is closed under scalar multiplication. Hence, for any scalar $k$, the following is true:\n$$ k\\mathbf{u} \\in W_{1} \\quad k \\mathbf{u} \\in W_2 $$\nHence, by the definition of intersection, the following is true:\n$$ k\\mathbf u \\in W $$\nAny vector $\\mathbf u$ inside $W$ implies that $k\\mathbf u$ is also an element of $W$, so $W$ is closed under scalar multiplication and satisfies (M1).\nConclusion\nIf $W_{1}, W_{2}$ is a subspace, then $W = W_{1} \\cap W_2$ satisfies (A1) and (M1), thus $W$ is also a subspace.\n■\nCorollary If $W_{1}, W_{2}, \\dots W_{n}$ are subspaces of the vector space $V$, then $W = W_{1} \\cap \\cdots \\cap \\dots W_{n}$ is also a subspace of $V$.\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p211-212\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p216\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":285,"permalink":"https://freshrimpsushi.github.io/en/posts/285/","tags":null,"title":"Subspace of Vector Space"},{"categories":"선형대수","contents":"Definition1 When the elements of a non-empty set $V$ satisfy the following ten rules for two operations, additionaddition and scalar multiplicationscalar multiplication, $V$ is called a vector spacevector space over field2 $\\mathbb{F}$, and the elements of $V$ are called vectorsvector.\nFor $\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V$ and $k, l \\in \\mathbb{F}$,\n(A1) If $\\mathbf{u}, \\mathbf{v}$ is an element of $V$, then $\\mathbf{u}+\\mathbf{v}$ is also an element of $V$.\n(A2) $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$\n(A3) $(\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})$\n(A4) For all $\\mathbf{u}$ in $V$, there exists $\\mathbf{0}$ in $V$ that satisfies $\\mathbf{u} + \\mathbf{0} = \\mathbf{0} + \\mathbf{u} = \\mathbf{u}$. This $\\mathbf{0}$ is called the zero vectorzero vector.\n(A5) For all $\\mathbf{u}$ in $V$, there exists $\\mathbf{v}$ in $V$ that satisfies $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u} = \\mathbf{0}$. This $\\mathbf{v}$ is called the negative of $\\mathbf{u}$negative of $\\mathbf{u}$ and is denoted by $\\mathbf{v} = -\\mathbf{u}$.\n(M1) If $\\mathbf{u}$ is an element of $V$, then $k \\mathbf{u}$ is also an element of $V$.\n(M2) $k(\\mathbf{u} + \\mathbf{v})=k\\mathbf{u} + k\\mathbf{v}$\n(M3) $(k+l)\\mathbf{u}=k\\mathbf{u}+ l\\mathbf{u}$\n(M4) $k(l\\mathbf{u})=(kl)(\\mathbf{u})$\n(M5) For $1\\in \\mathbb{F}$, $1\\mathbf{u} = \\mathbf{u}$\nExplanation The term linear spacelinear space is also used. Naturally, the scalar (field) does not need to be real numbers. Specifically, when $\\mathbb{F} = \\mathbb{R}$, it is called a real vector spacereal vector space, and when $\\mathbb{F} = \\mathbb{C}$, it is called a complex vector spacecomplex vector space.\nIn undergraduate linear algebra, mainly $\\mathbb{R}^{n}$ or $\\mathbb{C}^{n}$ are discussed. $\\mathbb{R}^{n}$ refers to the vector space containing ordered pairs of real numbers $n$. In other words, it signifies the $n$-dimensional Euclidean space, specifically $\\mathbb{R}^{3}$ refers to the three-dimensional space extensively covered in high school mathematics and calculus.\nThere are various sets that can become vector spaces. A set of functions can also be a vector space, which is called a function space.\nIn physics, something with magnitude and direction is called a vector. This concept is generalized in linear algebra. For example, consider a set $M_{m\\times n}(\\mathbb{R})$ that collects real number matrices of size $m\\times n$. Then, it can be seen that $M_{m\\times n}(\\mathbb{R})$ satisfies all ten rules mentioned above. Therefore, a set of matrices of the same size becomes a vector space, and each matrix within it becomes a vector. If you are encountering this abstract vector space for the first time, the fact that matrices can be vectors might be surprising, but it makes sense when thinking about how vectors in coordinate spaces have been denoted.\nTo determine whether a set is a vector space, one must examine whether it satisfies the definition above. It might seem intuitively to be a vector space when it is not, and vice versa. Since there can be cases entirely different from intuition, it is advisable to carefully examine each one when solving problems. Also, the zero vector $\\mathbf{0}$ and the scalar $0$ are entirely different entities and should be distinguished. Typically, vectors are expressed in bold type in textbooks.\nTheorem 1 Let $V$ be a vector space and $\\mathbf{u}$ be an element of $V$.\n(1a) The zero vector of $V$ is unique.\n(1b) The negative of $\\mathbf{u}$ is unique.\nProof This is a proof using the definition of vector spaces.\n(1a) Let\u0026rsquo;s say $\\mathbf{0},\\mathbf{0}^{\\prime}$ is the zero vector of $V$. Then, by the definition of vector spaces, the following holds.\n$$ \\begin{align*} \\mathbf{0} \u0026amp;= \\mathbf{0} + \\mathbf{0}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\\\ \u0026amp;= \\mathbf{0}^{\\prime} + \\mathbf{0} \u0026amp;\u0026amp; \\text{by (A2)} \\\\ \u0026amp;= \\mathbf{0}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\end{align*} $$\nTherefore, the two zero vectors are the same.\n■\n(1b) Let\u0026rsquo;s say $\\mathbf{v}, \\mathbf{v}^{\\prime}$ is the negative of $\\mathbf{u}$. Then, by the definition of vector spaces, the following holds.\n$$ \\begin{align*} \\mathbf{v} \u0026amp;= \\mathbf{v} + \\mathbf{0} \u0026amp;\u0026amp; \\text{by (A4)} \\\\ \u0026amp;= \\mathbf{v} + \\left( \\mathbf{u} + \\mathbf{v}^{\\prime} \\right) \u0026amp;\u0026amp; \\text{by (A5)} \\\\ \u0026amp;= \\left( \\mathbf{v} + \\mathbf{u} \\right) + \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A3)} \\\\ \u0026amp;= \\mathbf{0} + \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A5)} \\\\ \u0026amp;= \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\end{align*} $$\nTherefore, the two negatives of $\\mathbf{u}$ are the same.\n■\nTheorem 2 Let $V$ be a vector space, $\\mathbf{u}$ be an element of $V$, and $k$ be a scalar.\n(2a) $0 \\mathbf{u} = \\mathbf{0}$\n(2b) $k \\mathbf{0} = \\mathbf{0}$\n(2c) $(-1) \\mathbf{u} = -\\mathbf{u}$\n(2d) If $k \\mathbf{u} = \\mathbf{0}$, then $k = 0$ or $\\mathbf{u} = \\mathbf{0}$.\nProof This is a proof using the definition of vector spaces.\n(2a) $$ \\begin{align*} \u0026amp;\u0026amp; 0\\mathbf{u} \u0026amp;= (0 + 0)\\mathbf{u} \\\\ \u0026amp;\u0026amp; \u0026amp;= 0\\mathbf{u} + 0\\mathbf{u} \u0026amp;\u0026amp;\\text{by (M3)} \\\\ \u0026amp; \u0026amp; \\\\ \\implies \u0026amp;\u0026amp; 0\\mathbf{u}+(-0\\mathbf{u}) \u0026amp;= 0\\mathbf{u} + 0\\mathbf{u} +(-0\\mathbf{u}) \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{0} \u0026amp;= 0\\mathbf{u} \u0026amp;\u0026amp;\\text{by (A5)} \\end{align*} $$\n■\n(2b) $$ \\begin{align*} \u0026amp;\u0026amp; k\\mathbf{0} \u0026amp;= k(\\mathbf{0} + \\mathbf{0}) \u0026amp;\u0026amp;\\text{by (A4)} \\\\ \u0026amp;\u0026amp; \u0026amp;= k\\mathbf{0} + k\\mathbf{0} \u0026amp;\u0026amp;\\text{by (M2)} \\\\ \u0026amp; \u0026amp; \\\\ \\implies \u0026amp;\u0026amp; k\\mathbf{0}+(-k\\mathbf{0}) \u0026amp;= k\\mathbf{0} + k\\mathbf{0} +(-k\\mathbf{0}) \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{0} \u0026amp;= k\\mathbf{0} \u0026amp;\u0026amp;\\text{by (A5)} \\end{align*} $$\n■\n(2c) $$ \\begin{align*} \\mathbf{u} + (-1)\\mathbf{u} \u0026amp;= 1 \\mathbf{u} + (-1)\\mathbf{u} \u0026amp;\u0026amp;\\text{by (M5)} \\\\ \u0026amp;= \\big( 1 + (-1) \\big) \\mathbf{u} \u0026amp;\u0026amp;\\text{by (M3)} \\\\ \u0026amp;= 0 \\mathbf{u} \\\\ \u0026amp;= \\mathbf{0} \u0026amp;\u0026amp;\\text{by (a2)} \\end{align*} $$\nThus, by (A5), $(-1)\\mathbf{u}$ is the negative of $\\mathbf{u}$, and since the negative of $\\mathbf{u}$ is unique by (1b),\n$$ (-1)\\mathbf{u} = -\\mathbf{u} $$\n■\n(2d) $k$ must either be $0$ or not $0$, so let\u0026rsquo;s consider both scenarios.\nIf $k=0$\nThis satisfies the conclusion.\nIf $k\\ne 0$\nSince $k$ is not $0$, it can be divided by $k$, hence\n$$ \\begin{align*} \u0026amp;\u0026amp; k \\mathbf{u} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{u} \u0026amp;= \\frac{1}{k}\\mathbf{0} \\\\ \u0026amp;\u0026amp; \u0026amp;= \\mathbf{0} \u0026amp;\u0026amp; \\text{by (2b)} \\end{align*} $$\n■\nSee Also A simple definition of vectors Abstract Algebra Vector spaces in linear algebra Vector spaces in abstract algebra The $F$-vector spaces discussed in the documents below are essentially no different from the vector spaces mentioned above, albeit from a slightly different perspective. Vector spaces in linear algebra are abstractions of intuitive Euclidean spaces, whereas in abstract\nalgebra, vector spaces are brought into the realm of \u0026lsquo;algebra\u0026rsquo; in the true sense.\nConversely, $R$-modules generalize the scalar field $F$ of $F$-vector spaces into scalar rings $R$, thereby revealing their identity in a naming that is indifferent to the history and meaning of $F$-vector fields. From the perspective of group $G$, it\u0026rsquo;s about adding a new operation $\\mu$ to ring $R$, thus also being a module加群.\nR-modules in abstract algebra $F$-vector spaces in abstract algebra Howard Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p202-203\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf you are unfamiliar with fields, you can simply think of them as $\\mathbb{F}=\\mathbb{R}$ or $\\mathbb{F}=\\mathbb{C}$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":282,"permalink":"https://freshrimpsushi.github.io/en/posts/282/","tags":null,"title":"Definition of Vector Space"},{"categories":"수리물리","contents":"Theorem The $\\epsilon_{ijk}$, defined as follows, is referred to as the Levi-Civita symbol.\n$$ \\epsilon_{ijk} = \\begin{cases} +1 \u0026amp; \\text{if} \\ \\epsilon_{123}, \\epsilon_{231}, \\epsilon_{312} \\\\ -1 \u0026amp; \\text{if} \\ \\epsilon_{132}, \\epsilon_{213}, \\epsilon_{321} \\\\ 0 \u0026amp; \\text{if} \\ i=j \\ \\text{or} \\ j=k \\ \\text{or} \\ k=i \\end{cases} $$\nThe $\\delta_{ij}$, defined as follows, is referred to as the Kronecker delta.\n$$ \\delta_{ij} := \\begin{cases} 1,\u0026amp;i=j \\\\ 0, \u0026amp; i\\ne j \\end{cases} $$\nBetween the product of two Levi-Civita symbols and the Kronecker delta, the following relationships hold:\n(a) When one index is the same: $\\epsilon_{ijk}\\epsilon_{ilm} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl}$\n(b) When two indices are the same: $\\epsilon_{ijk}\\epsilon_{ijm}=2\\delta_{km}$\n(c) When all three indices are the same: $\\epsilon_{ijk}\\epsilon_{ijk}=6$\nExplanation Note that the summation symbol $\\sum$ is omitted throughout this text, adhering to the Einstein notation. This applies to the formulas above as well. Memorizing (a) can be very useful as it\u0026rsquo;s frequently used. A simple way to remember it is as follows.\nProof (a) Let $\\mathbf{e}_{i}$ $(i=1,2,3)$ be the standard unit vectors in 3-dimensional space.\n$$ \\mathbf{e}_{1} = (1, 0, 0),\\quad \\mathbf{e}_{2} = (0, 1, 0),\\quad \\mathbf{e}_{3} = (0, 0, 1) $$\nLet $P_{ijk}$ be a $3 \\times 3$ matrix whose 1st, 2nd, and 3rd rows are $\\mathbf{e}_{i}$, $\\mathbf{e}_{j}$, and $\\mathbf{e}_{k}$, respectively.\n$$ P_{ijk} = \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} $$\nThen, by the properties of determinants, it\u0026rsquo;s easy to see that $\\det P_{ijk} = \\epsilon_{ijk}$. Initially, $P_{123}$ is the identity matrix, hence its determinant is $1$. Moreover, the value of the determinant remains unchanged when swapping different rows an even number of times, hence,\n$$ \\det P_{123} = \\det P_{231} = \\det P_{312} = 1 $$\nWhen different rows are swapped an odd number of times, the sign of the determinant changes, hence,\n$$ \\det P_{132} = \\det P_{213} = \\det P_{321} = -1 $$\nThe determinant of a matrix with two or more identical rows is $0$, hence the rest of the cases are all $0$. Therefore, $\\det P_{ijk} = \\epsilon_{ijk}$ holds true. The product of two Levi-Civita symbols with one identical index can be expressed as follows, using the properties of determinants.\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ilm} \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{l} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{m} \\text{ \u0026mdash;} \\end{bmatrix} \\\\ \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \u0026amp; (\\because \\det A = \\det A^{T}) \\\\ \u0026amp;= \\det \\left( \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \\right) \u0026amp; \\Big(\\because (\\det A) (\\det B) = \\det (AB) \\Big) \\\\ \u0026amp;= \\det \\begin{bmatrix} \\mathbf{e}_{i} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{j} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{k} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{m} \\end{bmatrix} \\end{align*} $$\nSince $\\mathbf{e}_{i}$ are standard unit vectors, $\\mathbf{e}_{i} \\cdot \\mathbf{e}_{j} = \\delta_{ij}$ holds true.\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} \\delta_{ii} \u0026amp; \\delta_{il} \u0026amp; \\delta_{im} \\\\ \\delta_{ji} \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ \\delta_{ki} \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} $$\nNote that we are only considering cases where $i$ is different from $j, k, l, m$. This is because if $j, k, l, m$ includes $i$, then $\\epsilon_{ijk}\\epsilon_{ilm} = 0$, rendering the result meaningless. Therefore, the result is\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ 0 \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl} $$\n■\n(b) This is the case where $l=j$ in (a). Thus, it can be expressed as follows.\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} $$\nHere, $\\delta_{jj}=3$ and $\\delta_{jm}\\delta_{kj}=\\delta_{mk}$ hold, leading to the following.\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} = 3\\delta_{km} - \\delta_{mk} = 2\\delta_{km} $$\n■\n(c) This is the case where $m=k$ in (b), hence,\n$$ \\epsilon_{ijk}\\epsilon_{ijk} = \\sum_{k=1}^{3}2\\delta_{kk} = 2\\delta_{11} + 2\\delta_{22} + 2\\delta_{33} = 2 + 2 + 2 = 6 $$\nAlternatively, by explicitly writing out all non-zero terms, the following can be obtained.\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ijk} \u0026amp;=\\sum \\limits _{i=1} ^{3}\\sum \\limits _{j=1} ^{3}\\sum \\limits _{k=1} ^{1} \\epsilon_{ijk}\\epsilon_{ijk} \\\\ \u0026amp;=\\epsilon_{123}\\epsilon_{123}+\\epsilon_{231}\\epsilon_{231}+\\epsilon_{312}\\epsilon_{312}+\\epsilon_{132}\\epsilon_{132}+\\epsilon_{213}\\epsilon_{213}+\\epsilon_{321}\\epsilon_{321} \\\\ \u0026amp;=6 \\end{align*} $$\n■\n","id":88,"permalink":"https://freshrimpsushi.github.io/en/posts/88/","tags":null,"title":"Product of Two Levi-Civita Symbols"},{"categories":"교과과정","contents":"Formulas $$ d=\\frac { |2k| }{ \\sqrt { m^{ 2 }+1 } } $$\nExplanation When solving problems involving the tangent to a conic section, one often needs to calculate the distance between two tangents. While it\u0026rsquo;s not particularly challenging, thanks to the formula for the distance from a given point to a line, having an easy and quick formula for this distance can help to reduce calculation time.\nDerivation Let\u0026rsquo;s assume two parallel lines have the equation $y=mx\\pm k$. The distance from any point $(x,y)$ to the line $y=mx+k$ is $$ \\frac { |mx-y+k| }{ \\sqrt { m^{ 2 }+1 } } $$ For a point $(x_1,y_1)$ on the line $y=mx-k$, we have $$ k=mx_1-y_1 $$ Substituting $mx_1-y_1=k$ into the distance formula, we get $$ {{ |mx_{1}-y_{1}+k| }\\over{ \\sqrt { m^{ 2 }+1 } }} = {{ |k+k| }\\over{\\sqrt { m^{ 2 }+1 }}} $$ Therefore, the distance between the two parallel lines $y=mx\\pm k$ is $$ \\frac { |2k| }{ \\sqrt { m^{ 2 }+1 } } $$\n■\n","id":4,"permalink":"https://freshrimpsushi.github.io/en/posts/4/","tags":null,"title":"Derivation of the Formula to Calculate the Distance Between Two Parallel Lines"},{"categories":"복소해석","contents":"Theorem 1 Let $\\left\\{ a_{i} \\right\\}_{i=0}^{n} \\subset \\mathbb{R}$ such that $a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$. Then for the polynomial function $$ P(z) := a_0 + a_1 z + \\cdots + a_{n-1} z^{n-1} + a_n z^n $$ all roots $z \\in \\mathbb{C}$ satisfy $|z| \\ge 1$.\nProof If there is a root of $P(z) = 0$ at $z=1$, then we have $\\displaystyle 0 = P(1) = \\sum_{i=0}^{n} a_{i} \u0026gt; 0$, so the root must be $z \\ne 1$. Multiply both sides of the equation $P(z) = 0$ by $z$ and subtract from the original equation to express $a_0$ as $$ a_0 = (1-z)P(z) + (a_0 - a_1) z + \\cdots + (a_{n-1} - a_n) z^n + a_n z^{n+1} $$ If we assume that a root $z \\ne 1$ of $P(z) = 0$ satisfies $|z| \u0026lt; 1$ given that $a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$, then we have $$ \\begin{align*} \u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + (a_0 - a_1) + \\cdots + (a_{n-1} - a_n) + a_n \\\\ \\implies\u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + a_0 + (- a_1 + a_1) + \\cdots + (- a_{n-1} + a_{n-1} )+ (- a_n + a_n ) \\\\ \\implies\u0026amp; a_0 = |a_0| \u0026lt; |(1-z)P(z)| + a_0 \\\\ \\implies\u0026amp; 0 \u0026lt; |(1-z)P(z)| \\end{align*} $$ Yet, since we assumed $z \\ne 1$ is a root of $P(z) = 0$, we find a contradiction $$ 0 \u0026lt; |(1-z)P(z)| = 0 $$ This indicates the wrongness of the assumption that $| z | \u0026lt; 1$, hence we must have $|z | \\ge 1$.\n■\nOsborne. (1999). Complex variables and their applications: p. 6.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":5,"permalink":"https://freshrimpsushi.github.io/en/posts/5/","tags":null,"title":"Ernestrom-Kakeya Theorem Proof"},{"categories":"보조정리","contents":"Definitions For $n$ positive numbers ${x}_1,{x}_2,\\cdots,{x}_n$, the arithmetic mean, geometric mean, and harmonic mean are defined as:\nArithmetic Mean : $$ \\sum_{ k=1 }^{ n }{ \\frac { {x}_k }{ n } }=\\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n } $$ Geometric Mean : $$ \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } }=\\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n } $$ Harmonic Mean : $$ \\left( \\frac { \\sum_{ k=1 }^{ n }{ \\frac { 1 }{ {x}_k } } }{ n } \\right)^{-1}=\\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ {x}_2 }+\\cdots+\\frac { 1 }{ {x}_n } } $$ Theorem The following inequality holds for these means:\n$$ \\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n }\\ge \\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ {x}_2 }+\\cdots+\\frac { 1 }{ {x}_n } } $$\nExplanation High school students might have heard about the arithmetic-geometric mean at some point. It is not typically defined by a specific name but is commonly passed down colloquially as \u0026ldquo;Arith-Geo.\u0026rdquo; For the case when $n=2$, its proof is simple and useful even for high school level problem solving. A general proof at the high school level requires the intervention of messy expressions using mathematical induction, but instead, a more sophisticated but challenging proof is introduced.\nProof Strategy: Utilizing the following lemma:\nJensen\u0026rsquo;s Inequality: If $f$ is a convex function and $E(X) \u0026lt; \\infty$, then the following inequality holds: $$ E{f(X)}\\ge f{E(X)} $$\nArithmetic-Geometric Let $f(x)=-\\ln x$, then $f$ is convex on the interval $(0,\\infty )$. Assume that a random variable $X$ has the probability mass function\n$$ p(X=x)=\\begin{cases}{1 \\over n} \u0026amp; , x={x}_1,{x}_2, \\cdots ,{x}_n \\\\ 0 \u0026amp; , \\text{otherwise}\\end{cases} $$\nThen $E(X)$ is\n$$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\u0026lt;\\infty $$\nhence finite. This satisfies all necessary conditions for Jensen\u0026rsquo;s inequality, yielding:\n$$ E(-\\ln X)\\ge –\\ln E(X) $$\nThe left-hand side is\n$$ \\begin{align*} E(-\\ln X)\u0026amp;=-E(\\ln X) \\\\ \u0026amp;=-\\frac { 1 }{ n } \\sum_{ k=1 }^{ n }{ \\ln{x}_k } \\\\ \u0026amp;=-\\frac { 1 }{ n }\\ln \\prod_{ k=1 }^{ n }{ {x}_k } \\\\ \u0026amp;=-\\ln { \\left( \\prod_{ k=1 }^{ n }{ {x}_k } \\right) }^{ \\frac { 1 }{ n } } \\\\ \u0026amp;=-\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\end{align*} $$\nThe right-hand side is\n$$ \\begin{align*} -\\ln E(X)=-\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\end{align*} $$\nUpon rearranging, we get\n$$ \\begin{align*} -\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\ge\u0026amp; -\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\\\ \\implies \\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n } \\ge\u0026amp; \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } \\end{align*} $$\n■\nThis proves the inequality between the arithmetic and geometric means. Using this, let\u0026rsquo;s prove the inequality between the geometric and harmonic means.\nGeometric-Harmonic $$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } $$\nBy setting $\\displaystyle {x}_k=\\frac { 1 }{ {y}_k }$, we get\n$$ \\begin{align*} \\frac { \\frac { 1 }{ {y}_1 }+\\frac { 1 }{ {y}_2 }+\u0026hellip;+\\frac { 1 }{ {y}_n } }{ n }\\ge \\sqrt [ n ]{ \\frac { 1 }{ {y}_1 }\\frac { 1 }{ {y}_2 }\u0026hellip;\\frac { 1 }{ {y}_n } } \\\\ \\implies \\frac { 1 }{ \\sqrt [ n ]{ \\frac { 1 }{ {y}_1 }\\frac { 1 }{ {y}_2 }\u0026hellip;\\frac { 1 }{ {y}_n } } }\\ge \\frac { n }{ \\frac { 1 }{ {y}_1 }+\\frac { 1 }{ {y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\\\ \\implies \\sqrt [ n ]{ {y}_1{y}_2\u0026hellip;{y}_n }\\ge \\frac { n }{ \\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\end{align*} $$ ■\n","id":3,"permalink":"https://freshrimpsushi.github.io/en/posts/3/","tags":null,"title":"Arithmetic, Geometric, and Harmonic Means Inequality"},{"categories":"편미분방정식","contents":"Tidy up Let\u0026rsquo;s say that we have the following wave equation. where $\\Delta_{\\mathbf{x}}$ is Laplacian for the variable $\\mathbf{x}$.\n$$ \\begin{align} \\partial_{t}^{2} p(\\mathbf{x}, t) \u0026amp;= \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;\\text{on } \\mathbb{R} \\times [0, \\infty) \\\\ p(\\mathbf{x}, 0) \u0026amp;= f(\\mathbf{x}) \u0026amp;\\text{on } \\mathbb{R} \\\\ \\partial_{t} p(\\mathbf{x}, 0) \u0026amp;= 0 \u0026amp;\\text{on } \\mathbb{R} \\end{align} $$\nThe solution of the above partial differential equation is as follows.\n$$ \\begin{equation} p(\\mathbf{x}, t) = \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\end{equation} $$\nHere, $\\hat{f}$ is the Fourier transform of $f$(\u0026hellip;/1086). Now let\u0026rsquo;s consider the wave equation with the initial conditions as follows.\n$$ \\begin{align*} \\partial_{t}^{2} p(\\mathbf{x}, t) \u0026amp;= \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;\\text{on } \\mathbb{R} \\times [0, \\infty) \\\\ p(\\mathbf{x}, 0) \u0026amp;= 0 \u0026amp;\\text{on } \\mathbb{R} \\\\ \\partial_{t} p(\\mathbf{x}, 0) \u0026amp;= g(\\mathbf{x}) \u0026amp;\\text{on } \\mathbb{R} \\end{align*} $$\nThe solution of the above partial differential equation is as follows.\n$$ p(\\mathbf{x}, t) = \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{g} (\\boldsymbol{\\xi}) \\dfrac{\\sin (t \\left| \\boldsymbol{\\xi} \\right|)}{\\left| \\boldsymbol{\\xi} \\right|} e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\nDescription Let\u0026rsquo;s put the definitions of Fourier transform and it\u0026rsquo;s Inverse transform as below.\n$$ \\hat{f}(\\boldsymbol{\\xi}) = \\int\\limits_{\\mathbb{R}^{n}} f(\\mathbf{x}) e^{\\mathrm{i} \\boldsymbol{\\xi} \\cdot \\mathbf{x}} \\mathrm{d} \\mathbf{x}, \\qquad f(\\mathbf{x}) = \\dfrac{1}{(2\\pi)^{n}}\\int\\limits_{\\mathbb{R}^{n}} f(\\mathbf{x}) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\nThe latter method of proof is the same as the former, so it is omitted.\nProof of proof Just check that $(4)$ satisfies $(1)$, $(2)$, and $(3)$. Let\u0026rsquo;s first calculate the second derivative of time,\n$$ \\partial_{t}^{2} p(\\mathbf{x}, t) = -\\left| \\boldsymbol{\\xi} \\right|^{2} \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\nThe Laplacian is calculated as follows.\n$$ \\begin{align*} \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) (\\Delta_{\\mathbf{x}} e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}}) \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= (- \\left| \\boldsymbol{\\xi} \\right|^{2}) \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \\end{align*} $$\nThus, $(1)$ is established. When $p(\\mathbf{x}, 0)$ is calculated, $(2)$ is established because it is as follows.\n$$ \\begin{align*} p(\\mathbf{x}, 0) \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos ( 0 \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= f(\\mathbf{x}) \\end{align*} $$\nIt is also easy to see that $(3)$ is established.\n$$ \\begin{align*} \\partial_{t}p(\\mathbf{x}, 0) \u0026amp;= - \\left| \\boldsymbol{\\xi} \\right| \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\sin ( 0 \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= 0 \\end{align*} $$\n■\n","id":3623,"permalink":"https://freshrimpsushi.github.io/en/posts/3623/","tags":null,"title":"Solution of Wave Equation with Zero Initial Condition"},{"categories":"줄리아","contents":"Description A color gradient is one of the two color schemes supported by Julia\u0026rsquo;s visualization package Plots.jl (the other is palette), which is what we commonly refer to as gradation. Simply put, a type that implements gradation is ColorGradient.\nGradients are used to draw charts such as heatmap(), surface(), contour(). If you want to differentiate the colors of various graphs, use a palette instead of a gradient.\nCode Symbol It can be used with cgrad(symbol). The default gradient is cgrad(:inferno), and the colors are as follows.\nusing Plots\rcgrad(:inferno) heatmap(reshape(1:25, (5, 5))) Pre-defined palettes and gradients in Plots.jl can be found in the official documentation (More diverse palettes and gradients can be found in the official documentation of the package ColorSchemes.jl here).\nIn Python\u0026rsquo;s matplotlib, the default colormap for imshow that resembles a gradient is :viridis.\nheatmap(reshape(1:25, (5, 5)), fillcolor = cgrad(:viridis)) Custom Definition A palette can be defined directly with cgrad([start color, end color]). To set the points of color transformation, input a vector containing values between $0$ and $1$ as an optional argument.\ncgrad([:blue, :orange]) cgrad([:blue, :orange], [0.1, 0.9]) cgrad([:blue, :orange], [0.5, 0.50001]) Keywords rev Entering the keyword argument rev = true will reverse the order.\ncgrad(:darktest) cgrad(:darktest, rev = true) scale The keyword scale specifies the scale of the gradient. You can input :log or :exp.\ncgrad(:rainbow) cgrad(:rainbow, scale = :log) cgrad(:rainbow, scale = :exp) Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 See Also How to use Color How to use palettes How to use Color Gradient (Gradation) Package for Color Processing Colors.jl Using RGB code RGB(1, 0, 0) Using HEX code \u0026quot;#000000\u0026quot; How to specify the color of graph elements Specifying the color of graphs for each subplot How to specify the color of axes, axis names, ticks, and tick values How to specify background color ","id":3608,"permalink":"https://freshrimpsushi.github.io/en/posts/3608/","tags":null,"title":"How to Use Color Gradients in Julia Plots"},{"categories":"줄리아","contents":"Explanation A palette refers to a board where paints are squeezed out in advance. Mathematically, it can be explained as a \u0026lsquo;set of colors\u0026rsquo; or a \u0026lsquo;sequence of colors\u0026rsquo;. When drawing multiple graphs in one picture, the most common way is to distinguish them by using different colors. For this purpose, Julia has implemented a type called ColorPalette that collects various colors. It can be comfortably understood as a vector of colors. Indeed, if we load the default palette :default, it may look incredibly complicated, but if we look inside, it\u0026rsquo;s just a vector of colors.\nWhen drawing heatmaps, gradients are used instead of palettes.\njulia\u0026gt; using Plots\rjulia\u0026gt; palette(:default)\rColorPalette(ColorSchemes.ColorScheme{Vector{RGB{Float64}}, String, String}(RGB{Float64}[RG\rB{Float64}(0.0,0.6056031611752245,0.9786801175696073), RGB{Float64}(0.8888735002725198,0.43\r564919034818994,0.2781229361419438), RGB{Float64}(0.2422242978521988,0.6432750931576305,0.3\r044486515341153), RGB{Float64}(0.7644401754934356,0.4441117794687767,0.8242975359232758), R\rGB{Float64}(0.6755439572114057,0.5556623322045815,0.09423433626639477), RGB{Float64}(4.8211\r81644776295e-7,0.6657589812923561,0.6809969518707945), RGB{Float64}(0.930767491919665,0.367\r4771896571412,0.5757699667547829), RGB{Float64}(0.7769816661712932,0.5097431319944513,0.146\r4252569555497), RGB{Float64}(3.8077343661790943e-7,0.6642678029460116,0.5529508754522481), RGB{Float64}(0.558464964115081,0.5934846564332882,0.11748125233232104), RGB{Float64}(5.9476\r23898072685e-7,0.6608785231434254,0.7981787608414297), RGB{Float64}(0.6096707676128648,0.49\r918492100827777,0.9117812665042642), RGB{Float64}(0.3800016049820351,0.5510532724353506,0.9\r665056985227146), RGB{Float64}(0.942181647954218,0.37516423354097583,0.4518168202944593), R\rGB{Float64}(0.8684020893043971,0.3959893639954845,0.7135147524811879), RGB{Float64}(0.42314\r674364630817,0.6224954944199981,0.19877060252130468)], \u0026#34;\u0026#34;, \u0026#34;\u0026#34;))\rjulia\u0026gt; palette(:default).colors.colors\r16-element Array{RGB{Float64},1} with eltype RGB{Float64}:\rRGB{Float64}(0.0,0.6056031611752245,0.9786801175696073)\rRGB{Float64}(0.8888735002725198,0.43564919034818994,0.2781229361419438)\rRGB{Float64}(0.2422242978521988,0.6432750931576305,0.3044486515341153)\rRGB{Float64}(0.7644401754934356,0.4441117794687767,0.8242975359232758)\rRGB{Float64}(0.6755439572114057,0.5556623322045815,0.09423433626639477)\rRGB{Float64}(4.821181644776295e-7,0.6657589812923561,0.6809969518707945)\rRGB{Float64}(0.930767491919665,0.3674771896571412,0.5757699667547829)\rRGB{Float64}(0.7769816661712932,0.5097431319944513,0.1464252569555497)\rRGB{Float64}(3.8077343661790943e-7,0.6642678029460116,0.5529508754522481)\rRGB{Float64}(0.558464964115081,0.5934846564332882,0.11748125233232104)\rRGB{Float64}(5.947623898072685e-7,0.6608785231434254,0.7981787608414297)\rRGB{Float64}(0.6096707676128648,0.49918492100827777,0.9117812665042642)\rRGB{Float64}(0.3800016049820351,0.5510532724353506,0.9665056985227146)\rRGB{Float64}(0.942181647954218,0.37516423354097583,0.4518168202944593)\rRGB{Float64}(0.8684020893043971,0.3959893639954845,0.7135147524811879)\rRGB{Float64}(0.42314674364630817,0.6224954944199981,0.19877060252130468) You can load or create a palette by entering the symbol of an already defined palette or by entering the color and length into the function palette(). When drawing a chart, you just need to assign it to the palette keyword of the plot() function.\nCode Symbols Use it like palette(symbol). The symbol for the default palette is :default, and its colors are as follows.\nWhen drawing multiple graphs in one picture, the above colors are applied in order. After using all the colors, it cycles back to the beginning.\nusing Plots x = 0:0.01:2π plot([x -\u0026gt; sin(x - a) for a in range(0, π, length = 5)], 0, 2π) Palettes and gradients predefined in Plots.jl can be checked in the official documentation. (More diverse palettes and gradients can be found in the official documentation of the package ColorSchemes.jl.)\nIf drawn with :rainbow,\npalette(:rainbow) plot([x -\u0026gt; sin(x - a) for a in range(0, π, length = 5)], 0, 2π,\rpalette = palette(:rainbow)) Custom Definition You can define your palette directly with palette([start color, end color], length). Alternatively, you can interpolate colors using range().\npalette([:blue, :orange], 10) palette([RGB(0.5, 0.6, 0.2), RGB(1.0, 0.2, 0.9)], 10) Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 See Also How to Use Colors How to Use Palettes How to Use Color Gradients Package for Color Processing Colors.jl How to Use RGB Code RGB(1, 0, 0) How to Use HEX Code \u0026quot;#000000\u0026quot; How to Specify Colors of Graph Elements [How to Specify Colors for Each Subplot] (../3602) How to Specify Colors of Axes, Axis Names, Ticks, and Tick Values How to Specify Background Color ","id":3607,"permalink":"https://freshrimpsushi.github.io/en/posts/3607/","tags":null,"title":"How to Use Palettes in Julia Plots"},{"categories":"줄리아","contents":"코드 When plotting two data sets that have a large scale difference on the same plot, the one with the smaller scale gets completely ignored as shown in the figure below.\nusing Plots\rx = 0:0.01:2π\rplot(x, sin.(x))\rplot!(x, exp.(x)) When plotting the second data set, if you input twinx() as the first argument, it shares the $x$ axis and the graph is drawn on the new $y$ axis.\nplot(x, sin.(x), ylabel = \u0026#34;sin x\u0026#34;)\rplot!(twinx(), x, exp.(x), ylabel = \u0026#34;exp x\u0026#34;) Conversely, to share the $y$ axis and plot, you can input twiny() as the first argument.\nEnvironment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3606,"permalink":"https://freshrimpsushi.github.io/en/posts/3606/","tags":null,"title":"How to Plot Two Data Axes of Different Scales in Julia Plots"},{"categories":"줄리아","contents":"Description In Julia\u0026rsquo;s Plots.jl, a plot is also an object. If you draw an empty plot to check its type, it looks like this.\njulia\u0026gt; using Plots\rjulia\u0026gt; p = plot()\rjulia\u0026gt; p |\u0026gt; typeof\rPlots.Plot{Plots.GRBackend} Removing Plots., it becomes Plot{GRBackend}, meaning the plot\u0026rsquo;s backend is GR, similar to how a vector with elements of type Float64 is denoted as Vector{Float64}. Checking the properties of Plot, we find the following.\njulia\u0026gt; p |\u0026gt; propertynames\r(:backend, :n, :attr, :series_list, :o, :subplots, :spmap, :layout, :inset_subplots, :init) Each property includes vectors or dictionaries containing attributes of the picture.\np.backend This is the plot\u0026rsquo;s backend.\njulia\u0026gt; p.backend\rPlots.GRBackend() p.attr This is a dictionary about the attributes of the picture. It contains 30 key-value pairs as follows.\njulia\u0026gt; plot(rand(10, 4), layout = 4).attr\rRecipesPipeline.DefaultsDict with 30 entries:\r:dpi =\u0026gt; 100\r:background_color_outside =\u0026gt; :match\r:plot_titlefontvalign =\u0026gt; :vcenter\r:warn_on_unsupported =\u0026gt; true\r:background_color =\u0026gt; RGBA{Float64}(1.0,1.0,1.0,1.0)\r:inset_subplots =\u0026gt; nothing\r:size =\u0026gt; (600, 400)\r:display_type =\u0026gt; :auto\r:overwrite_figure =\u0026gt; true\r:html_output_format =\u0026gt; :auto\r:plot_titlefontfamily =\u0026gt; :match\r:plot_titleindex =\u0026gt; 0\r:foreground_color =\u0026gt; RGB{N0f8}(0.0,0.0,0.0)\r:window_title =\u0026gt; \u0026#34;Plots.jl\u0026#34;\r:plot_titlefontrotation =\u0026gt; 0.0\r:extra_plot_kwargs =\u0026gt; Dict{Any, Any}()\r:pos =\u0026gt; (0, 0)\r:plot_titlefonthalign =\u0026gt; :hcenter\r:tex_output_standalone =\u0026gt; false\r:extra_kwargs =\u0026gt; :series\r:thickness_scaling =\u0026gt; 1\r:layout =\u0026gt; 4\r:plot_titlelocation =\u0026gt; :center\r:plot_titlefontsize =\u0026gt; 16\r:plot_title =\u0026gt; \u0026#34;\u0026#34;\r:show =\u0026gt; false\r:link =\u0026gt; :none\r:plot_titlefontcolor =\u0026gt; :match\r:plot_titlevspan =\u0026gt; 0.05\r:fontfamily =\u0026gt; \u0026#34;sans-serif\u0026#34;\rjulia\u0026gt; plot(rand(10, 4), layout = 4).attr[:size]\r(600, 400) p.series_list It is a vector whose elements are dictionaries of attributes for each data graph.\njulia\u0026gt; plot(rand(10,5)).series_list\r5-element Vector{Plots.Series}:\rjulia\u0026gt; plot(plot(rand(10, 4)), plot(rand(10, 3))).series_list\r7-element Vector{Plots.Series}: The included key-value pairs in each dictionary are as follows.\njulia\u0026gt; plot(rand(10, 2)).series_list[1].plotattributes\rRecipesPipeline.DefaultsDict with 62 entries:\r:plot_object =\u0026gt; Plot{Plots.GRBackend() n=2}\r:subplot =\u0026gt; Subplot{1}\r:label =\u0026gt; \u0026#34;y1\u0026#34;\r:fillalpha =\u0026gt; nothing\r:linealpha =\u0026gt; nothing\r:linecolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:x_extrema =\u0026gt; (NaN, NaN)\r:series_index =\u0026gt; 1\r:markerstrokealpha =\u0026gt; nothing\r:markeralpha =\u0026gt; nothing\r:seriestype =\u0026gt; :path\r:z_extrema =\u0026gt; (NaN, NaN)\r:x =\u0026gt; Base.OneTo(10)\r:markerstrokecolor =\u0026gt; RGBA{Float64}(0.0,0.0,0.0,1.0)\r:fillcolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:clims_calculated =\u0026gt; (NaN, NaN)\r:seriescolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:extra_kwargs =\u0026gt; Dict{Symbol, Any}()\r:z =\u0026gt; nothing\r:series_plotindex =\u0026gt; 1\r:y =\u0026gt; [0.477103, 0.00362131, 0.864524, 0.391488, 0.663659, 0.89787, 0.157973, 0.964416, 0.806635, 0.243531]\r:markercolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:y_extrema =\u0026gt; (0.00362131, 0.964416)\r:linewidth =\u0026gt; 1\r:group =\u0026gt; nothing\r:stride =\u0026gt; (1, 1)\r:permute =\u0026gt; :none\r:marker_z =\u0026gt; nothing\r:show_empty_bins =\u0026gt; false\r:seriesalpha =\u0026gt; nothing\r:smooth =\u0026gt; false\r:zerror =\u0026gt; nothing\r:arrow =\u0026gt; nothing\r:normalize =\u0026gt; false\r:linestyle =\u0026gt; :solid\r:contours =\u0026gt; false\r:bar_width =\u0026gt; nothing\r:bins =\u0026gt; :auto\r:markerstrokestyle =\u0026gt; :solid\r:weights =\u0026gt; nothing\r:z_order =\u0026gt; :front\r:fill_z =\u0026gt; nothing\r:markershape =\u0026gt; :none\r:markerstrokewidth =\u0026gt; 1\r:xerror =\u0026gt; nothing\r:bar_position =\u0026gt; :overlay\r:contour_labels =\u0026gt; false\r:hover =\u0026gt; nothing\r:primary =\u0026gt; true\r:yerror =\u0026gt; nothing\r:ribbon =\u0026gt; nothing\r:fillstyle =\u0026gt; nothing\r:line_z =\u0026gt; nothing\r:orientation =\u0026gt; :vertical\r:markersize =\u0026gt; 4\r:bar_edges =\u0026gt; false\r:quiver =\u0026gt; nothing\r:fillrange =\u0026gt; nothing\r:colorbar_entry =\u0026gt; true\r:series_annotations =\u0026gt; nothing\r:levels =\u0026gt; 15\r:connections =\u0026gt; nothing\rjulia\u0026gt; plot(rand(10, 2)).series_list[1][:label]\r\u0026#34;y1\u0026#34; Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3605,"permalink":"https://freshrimpsushi.github.io/en/posts/3605/","tags":null,"title":"List of Plot Properties in Julia Plots"},{"categories":"줄리아","contents":"Summary Keywords related to the grid background in Plots.jl are as follows:\nKeyword Name Function grid Display grid gridalpha, ga, gα Specify grid transparency foreground_color_grid, fgcolor_grid Specify grid color gridlinewidth, grid_lw Specify grid thickness gridstyle, grid_ls Specify grid line style minorgrid Display minor grid minorgridalpha Specify minor grid transparency foreground_color_minor_grid, fgcolor_minorgrid Specify minor grid color minorgridlinewidth, minorgrid_lw Specify minor grid thickness minorgridstyle, minorgrid_ls Specify minor grid line style Code Display Grid The keyword for displaying the grid is grid. Inputting :x or :y displays only the lines assisting the $x$ axis or the $y$ axis ticks, respectively. Inputting false does not display the grid.\nplot(plot(rand(10)), plot(rand(10), grid = :x), plot(rand(10), grid = :y), plot(rand(10), grid = false)) Transparency The grid in the background is drawn with a transparency of 0.1 by default. The keyword for adjusting the grid’s transparency is gridalpha(=ga)(=gα).\nplot(rand(10, 3), layout = (3, 1), gridalpha = [0.1 0.5 1]) Color The default grid color is black, and it can be changed with the keyword foreground_color_grid(=fgcolor_grid).\nplot(rand(10, 3), layout = (3, 1), gridalpha = 1, fgcolor_grid = [:red :green :orange]) Thickness The keyword for specifying the grid thickness is gridlinewidth(=grid_lw), with a default value of 0.5.\nplot(rand(10, 3), layout = (3, 1), grid_lw = [0.5 5 10]) Grid Style The grid line style can be specified with the keyword gridstyle(=grid_ls). Possible symbols are :auto, :solid, :dash, :dot, :dashdot, :dashdotdot.\nplot(rand(10, 2), layout = 2, ga = 1, gridstyle = [:solid :dash]) Minor Grid Inputting the keyword argument minorgrid = true draws the minor grid. Keywords for specifying the minor grid’s transparency, color, thickness, and line style are minorgridalpha, foreground_color_minor_grid minorgrid_lw, minorgrid_ls, respectively.\nplot(plot(rand(10)), plot(rand(10), minorgrid = true), gridalpha = 0.8, minorgridalpha = 0.2) Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3604,"permalink":"https://freshrimpsushi.github.io/en/posts/3604/","tags":null,"title":"Decorating the Background Grid in Julia Plots"},{"categories":"줄리아","contents":"Summary The keywords related to the background color of figures in Plots.jl are as follows.\nKeyword Name Function background_color, bg_color Specify the color of the overall background background_color_outside, bg_color_outside Specify the color of the area outside where the graph is drawn background_subplot, bg_subplot Specify the color of the area where the graph is drawn background_inside, bg_inside Specify the color of the area where the graph is drawn, excluding the legend Code The keyword to specify the background color is background_color(=bg_color). It sets the color of the legend, the area where the graph is drawn, and all the background to the entered value.\nplot(rand(10), bg_color = :tomato) The keyword to specify the color outside the area where the graph is drawn is background_color_outside(=bg_color_outside).\nplot(rand(10), bg_color_outside = :palegreen) The keyword to specify the color of the area where the graph is drawn is background_subplot(=bg_subplot).\nplot(rand(10), bg_subplot = :violet) The keyword to specify the color of the area where the graph is drawn, excluding the legend, is background_inside(=bg_inside).\nplot(rand(10), bg_inside = :brown4) Sub Plots When there are multiple sub plots, it is necessary to set the color using bg_subplot or bg_inside to maintain each background color when combined into an overall plot.\np₁ = plot(rand(10), bg_subplot = :tomato)\rp₂ = scatter(rand(10), bg_inside = :yellow)\rp = plot(p₁, p₂) Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 See Also How to Use Colors How to Use Palettes How to Use Color Gradients Package for Color Processing Colors.jl How to Use RGB Codes RGB(1, 0, 0) How to Use HEX Codes \u0026quot;#000000\u0026quot; How to Specify the Color of Graph Elements How to Specify Graph Colors for Each Sub Plot How to Specify the Color of Axes, Axis Names, Ticks, and Tick Values How to Specify Background Color ","id":3603,"permalink":"https://freshrimpsushi.github.io/en/posts/3603/","tags":null,"title":"Specifying Background Color in Julia Plots"},{"categories":"줄리아","contents":"Overview This section introduces three methods for specifying graph colors for each subplot. To learn how to specify colors for graph elements, refer here.\nMethod 1 The first way to specify the graph color for a subplot is to predefine the color when defining each subplot. In Julia, since a picture is an object itself, you can define multiple pictures with different attributes and then combine them into one plot.\np₁ = plot(rand(10), lc = :red)\rp₂ = scatter(rand(10), mc = :blue)\rp₃ = bar(rand(10), fc = :green)\rplot(p₁, p₂, p₃,\rlayout = (3, 1),\rtitle = [\u0026#34;p₁\u0026#34; \u0026#34;p₂\u0026#34; \u0026#34;p₃\u0026#34;],\r) Method 2 The second method involves entering the colors as a row vector when defining the subplots in the overall plot through keyword arguments. Note that it must be a row vector, not a column vector.\np₄ = plot(rand(10))\rp₅ = plot(rand(10))\rp₆ = plot(rand(10))\rplot(p₄, p₅, p₆,\rlayout = (3, 1),\rlinecolor = [:brown :purple :orange],\rtitle = [\u0026#34;p₄\u0026#34; \u0026#34;p₅\u0026#34; \u0026#34;p₆\u0026#34;],\r) Method 3 The third method changes the property value of each subplot after defining the overall plot. The property .series_list is a vector of dictionaries containing series attributes information of each subplot. That is, p.series_list[1] returns the series attributes dictionary of the first subplot. By entering the :linecolor key and changing its value in this dictionary, the line color of the first subplot changes.\np₇ = plot(rand(10))\rp₈ = scatter(rand(10))\rp₉ = bar(rand(10))\rp = plot(p₇, p₈, p₉,\rlayout = (3, 1),\rtitle = [\u0026#34;p₇\u0026#34; \u0026#34;p₈\u0026#34; \u0026#34;p₉\u0026#34;],\r)\rp.series_list[1][:linecolor] = :goldenrod1\rp.series_list[2][:markercolor] = :olivedrab3\rp.series_list[3][:fillcolor] = :hotpink3 Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 See Also How to Use Colors How to Use Palettes How to Use Color Gradients Package for Color Handling Colors.jl How to Use RGB Codes RGB(1, 0, 0) How to Use HEX Codes \u0026quot;#000000\u0026quot; How to Specify Colors for Graph Elements How to Specify Graph Colors for Each Subplot How to Specify Colors for Axes, Axis Names, Ticks, and Tick Labels How to Specify Background Color ","id":3602,"permalink":"https://freshrimpsushi.github.io/en/posts/3602/","tags":null,"title":"How to Specify Graph Colors for Each Subplot in Julia Plots"},{"categories":"줄리아","contents":"Summary In Plots.jl, the keywords for specifying the color of each graph component are as follows.\nKeyword Function markercolor, mc Specify the marker\u0026rsquo;s inside color markerstrokecolor, msc Specify the marker\u0026rsquo;s border color linecolor, lc Specify the line color fillcolor, fc Specify the fill color seriescolor, c Specify the color of all components Keyword Function markeralpha, ma, mα Specify the marker\u0026rsquo;s inside transparency markerstrokealpha, msa, msα Specify the marker\u0026rsquo;s border transparency linealpha, la, lα Specify the line transparency fillalpha, fa, fα Specify the fill transparency seriesalpha, a, α Specify the transparency of all components Colors In Plots.jl, the targets whose color can be changed are dots, lines, and areas. The keyword arguments for specifying each color are markercolor(=mc), linecolor(=lc), and fillcolor(=fc). The property specified by these keywords does not affect each other; hence, even if you input mc = :red after plotting a line graph, the line color will not apply as red. Indeed, checking the property of p = plot(rand(10), mc = :red) shows the following.\njulia\u0026gt; p = plot(rand(10), mc = :red)\rjulia\u0026gt; p.series_list[1][:linecolor]\rRGBA{Float64}(0.0,0.6056031611752245,0.9786801175696073,1.0)\rjulia\u0026gt; p.series_list[1][:markercolor]\rRGBA{Float64}(1.0,0.0,0.0,1.0) The color of the plotted line graph is still the default color, not red.\nThus, if you plot multiple subplots and specify colors with the three keywords above, each will be applied accordingly. If you color the dots (markers) in purple :purple, the line in dark green :darkgreen, and the area in sky blue :skyblue, it will look as follows.\nst = [:line :scatter :barhist :steppre :scatterhist :bar]\rx = rand(20)\ry = repeat(x, outer = (1, length(st)))\rplot(y, seriestype = st, layout = 6, mc = :purple,\rlc = :darkgreen,\rfc = :skyblue\r) Transparency The keyword for determining the transparency of a color is created by replacing color with alpha in the color specifying keyword name. It is also acceptable to use the Greek letter α directly.\nAlternatively, you can input a color code that includes transparency, such as RGBA, into the color specifying keyword.\nplot(rand(20, 3), layout = (3,1), seriestype = [:scatter :line :bar],\rmc = :red,\rlc = :green,\rfc = :blue\r) plot(rand(20, 3), layout = (3,1), seriestype = [:scatter :line :bar],\rmarkeralpha = 0.5, mc = :red,\rla = 0.5, lc = :green,\rfα = 0.5, fc = :blue\r) Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 See Also How to Use Colors How to Use Palettes How to Use Color Gradient (Gradation) Package for Color Processing Colors.jl How to Use RGB Codes RGB(1, 0, 0) How to Use HEX Codes \u0026quot;#000000\u0026quot; How to Specify Colors of Graph Components How to Specify Colors of Subplots How to Specify Colors of Axes, Axis Labels, Ticks, and Tick Values How to Specify Background Color ","id":3601,"permalink":"https://freshrimpsushi.github.io/en/posts/3601/","tags":null,"title":"Specifying the Color of Graph Elements in Julia Plots"},{"categories":"줄리아","contents":"Code The package provided in Julia for dealing with colors is Colors.jl. By importing the visualization package Plots.jl, the features within Colors.jl can also be used. The color codes representing the RGB space include RGB, BGR, RGB24, RGBX, XRGB, which are subtypes of AbstractRGB. RGBA adds transparency to RGB.\njulia\u0026gt; using Plots\rjulia\u0026gt; subtypes(AbstractRGB)\r5-element Vector{Any}:\rBGR\rRGB\rRGB24\rRGBX\rXRGB\rjulia\u0026gt; subtypes(AbstractRGBA)\r2-element Vector{Any}:\rBGRA\rRGBA Strings For the function plot(), entering a string like \u0026quot;rgb(255, 0, 0)\u0026quot; as a color specifying keyword allows the use of the color with RGB code (255, 0, 0). As seen below, the reason why entering a string also works is apparently because plot() parses it automatically. For named colors, they can be used as either strings or symbols like \u0026quot;red\u0026quot; or :red.\nusing Plots\rr = \u0026#34;rgb(255, 0, 0)\u0026#34; # 정수로 표현된 RGB 빨간색\rg = \u0026#34;rgba(0, 255, 0, 0.2)\u0026#34; # 투명도가 0.2인 RGB 초록색\rp = \u0026#34;rgb(50%, 0%, 100%)\u0026#34; # 퍼센테이지로 표현된 RGB 보라색\rplot(\rplot(rand(15), lc = r),\rbar(rand(15), fc = g),\rscatter(rand(15), mc = p),\rlayout = (3, 1)\r) Parsing RGB color codes can be parsed like colorant\u0026quot;rgb(0, 0, 0)\u0026quot;.\njulia\u0026gt; r = colorant\u0026#34;rgb(255, 0, 0)\u0026#34; # 정수로 표현된 RGB 빨간색\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; g = colorant\u0026#34;rgba(0, 255, 0, 0.2)\u0026#34; # 투명도가 0.2인 초록색\rRGBA{N0f8}(0.0,1.0,0.0,0.502)\rjulia\u0026gt; p = colorant\u0026#34;rgb(50%, 0%, 100%)\u0026#34; # 퍼센테이지로 표현된 RGB 보라색\rRGB{N0f8}(0.502,0.0,1.0)\rjulia\u0026gt; plot(\rplot(rand(15), lc = r),\rbar(rand(15), fc = g),\rscatter(rand(15), mc = p),\rlayout = (3, 1)) It is also possible to parse using parse(RGB, \u0026quot;rgb(0, 255, 255)\u0026quot;).\njulia\u0026gt; parse(RGB, \u0026#34;rgb(0, 255, 255)\u0026#34;)\rRGB{N0f8}(0.0,1.0,1.0)\rjulia\u0026gt; parse(RGBA, \u0026#34;rgba(0, 255, 0, 0.5)\u0026#34;)\rRGBA{N0f8}(0.0,1.0,0.0,0.502) Custom Definition Colors can be defined directly using functions like RGB(), RGBA().\njulia\u0026gt; RGB(1, 0, 0)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0, 0.0, 0.0)\rRGB{Float64}(1.0,0.0,0.0)\rjulia\u0026gt; RGBA(1, 0, 0.5, 0.5)\rRGBA{Float64}(1.0,0.0,0.5,0.5) To get exactly the same type of color as parsed by colorant, input of numbers in N0f8 type is needed. To use this, FixedPointNumbers.jl is required. Or, directly defining it as 1.0N0f8 is also an option. Below is the code returning the same color as colorant\u0026quot;rgb(255, 0, 0)\u0026quot;, which is red.\njulia\u0026gt; using FixedPointNumbers\r# RGB 빨간색 RGB\rjulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0, 0.0, 0.0)\rRGB{Float64}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0N0f8, 0N0f8, 0N0f8)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(reinterpret(N0f8, UInt8(255)), reinterpret(N0f8, UInt8(0)), reinterpret(N0f8, UInt8(0)))\rRGB{N0f8}(1.0,0.0,0.0) Conversion from Other Color Spaces The function convert() converts color codes from other color spaces to RGB code.\njulia\u0026gt; using Colors\rjulia\u0026gt; convert(RGB{N0f8}, HSL(270, 0.5, 0.5))\rERROR: UndefVarError: `N0f8` not defined\rjulia\u0026gt; using FixedPointNumbers\rjulia\u0026gt; convert(RGB{N0f8}, HSL(270, 0.5, 0.5))\rRGB{N0f8}(0.502,0.251,0.749) Getting Color Names Functions rgb_string() and rgba_string() return the RGB, RGBA codes of colors as strings, respectively.\njulia\u0026gt; rgb_string(colorant\u0026#34;rgba(255, 0, 0, 0.5)\u0026#34;)\r\u0026#34;rgb(255, 0, 0)\u0026#34;\rjulia\u0026gt; rgba_string(colorant\u0026#34;rgba(255, 0, 0, 0.5)\u0026#34;)\r\u0026#34;rgba(255, 0, 0, 0.502)\u0026#34;\rjulia\u0026gt; rgb_string(colorant\u0026#34;red\u0026#34;)\r\u0026#34;rgb(255, 0, 0)\u0026#34;\rjulia\u0026gt; rgb_string(parse(RGB, :blue))\r\u0026#34;rgb(0, 0, 255)\u0026#34;\rjulia\u0026gt; rgb_string(colorant\u0026#34;#00FF00\u0026#34;)\r\u0026#34;rgb(0, 255, 0)\u0026#34; See Also Using colors in Plots Using RGB color codes Using HEX color codes Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10, FixedPointNumbers v0.8.4 See Also How to use colors Using palettes Using color gradients Package for color processing Colors.jl Using RGB codes RGB(1, 0, 0) Using HEX codes \u0026quot;#000000\u0026quot; Determining the color of graph elements Specifying graph colors for each subplot Designating colors for axes, axis labels, ticks, and tick values Setting background colors ","id":3600,"permalink":"https://freshrimpsushi.github.io/en/posts/3600/","tags":null,"title":"How to Use RGB Color Codes in Julia"},{"categories":"줄리아","contents":"Introduction1 Introducing the capabilities of Colors.jl, a package for color processing in Julia. When using the visualization package Plots.jl, there\u0026rsquo;s no need to load Colors.jl separately. It provides the following functionalities:\nColor parsing and conversion Color maps Color scales Parsing and Conversion Assuming str is a string representing color information, you can parse the string into a color code of a specific color space using @colorant_str or parse(Colorant, str). Note that colorant means a dye or pigment.\nHow to use RGB codes How to use HEX codes julia\u0026gt; using Colors\rjulia\u0026gt; colorant\u0026#34;red\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(Colorant, \u0026#34;rgba(0, 255, 0, 0.5)\u0026#34;)\rRGBA{N0f8}(0.0,1.0,0.0,0.502)\rjulia\u0026gt; parse(Colorant, \u0026#34;#FF0000\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(Colorant, \u0026#34;hsl(120, 100%, 25%)\u0026#34;)\rHSL{Float32}(120.0f0,1.0f0,0.25f0) The convert() function can be used to convert to a color code of a different color space.\njulia\u0026gt; convert(RGB, HSL(270, 0.5, 0.5))\rRGB{Float64}(0.5,0.25,0.75) Color Interpolation Using the range() function, colors can be interpolated. This operation is quite logical and intuitive. For example, consider the RGB codes. Red is represented by $(255, 0, 0)$ or $(1, 0, 0)$, which is essentially a 3D vector. Therefore, there\u0026rsquo;s no reason not to use the color code as an argument for the range() function which interpolates between two vectors.\njulia\u0026gt; v1 = [1.0, 0.0, 0.0];\rjulia\u0026gt; v2 = [0.0, 0.5, 0.0];\rjulia\u0026gt; collect(range(v1, v2, length = 15))\r15-element Vector{Vector{Float64}}:\r[1.0, 0.0, 0.0]\r[0.9285714285714286, 0.03571428571428571, 0.0]\r[0.8571428571428572, 0.07142857142857142, 0.0]\r[0.7857142857142857, 0.10714285714285714, 0.0]\r[0.7142857142857143, 0.14285714285714285, 0.0]\r[0.6428571428571428, 0.17857142857142858, 0.0]\r[0.5714285714285714, 0.21428571428571427, 0.0]\r[0.5, 0.25, 0.0]\r[0.4285714285714286, 0.2857142857142857, 0.0]\r[0.3571428571428571, 0.32142857142857145, 0.0]\r[0.2857142857142857, 0.35714285714285715, 0.0]\r[0.2142857142857143, 0.39285714285714285, 0.0]\r[0.1428571428571429, 0.42857142857142855, 0.0]\r[0.0714285714285714, 0.4642857142857143, 0.0]\r[0.0, 0.5, 0.0]\rjulia\u0026gt; c1 = colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; c2 = colorant\u0026#34;rgb(0, 128, 0)\u0026#34;\rRGB{N0f8}(0.0,0.502,0.0)\rjulia\u0026gt; range(colorant\u0026#34;rgb(255,0,0)\u0026#34;, colorant\u0026#34;rgb(0,128,0)\u0026#34;, length=15)\r15-element Array{RGB{N0f8},1} with eltype RGB{FixedPointNumbers.N0f8}:\rRGB{N0f8}(1.0,0.0,0.0)\rRGB{N0f8}(0.929,0.035,0.0)\rRGB{N0f8}(0.859,0.071,0.0)\rRGB{N0f8}(0.784,0.11,0.0)\rRGB{N0f8}(0.714,0.145,0.0)\rRGB{N0f8}(0.643,0.18,0.0)\rRGB{N0f8}(0.573,0.216,0.0)\rRGB{N0f8}(0.502,0.251,0.0)\rRGB{N0f8}(0.427,0.286,0.0)\rRGB{N0f8}(0.357,0.322,0.0)\rRGB{N0f8}(0.286,0.357,0.0)\rRGB{N0f8}(0.216,0.392,0.0)\rRGB{N0f8}(0.141,0.431,0.0)\rRGB{N0f8}(0.071,0.467,0.0)\rRGB{N0f8}(0.0,0.502,0.0) Visualization of the above color range can be obtained in VS Code by installing and running the Julia extension as follows.\nAlso, the return of range() can be used as a palette.\nmy_palette = range(colorant\u0026#34;rgb(255,0,0)\u0026#34;, colorant\u0026#34;rgb(0,128,0)\u0026#34;, length=15)\rplot(rand(10, 15), palette = my_palette) Environment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10 See Also How to Use Colors How to Use Palettes How to Use Color Gradients Package for Color Processing Colors.jl How to Use RGB Codes RGB(1, 0, 0) How to Use HEX Codes \u0026quot;#000000\u0026quot; How to Specify Colors for Graph Elements How to Specify Graph Colors for Each Subplot How to Specify Colors for Axes, Axis Labels, Ticks, and Tick Labels How to Set Background Color https://juliagraphics.github.io/Colors.jl/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3599,"permalink":"https://freshrimpsushi.github.io/en/posts/3599/","tags":null,"title":"Package for Color Processing in Julia"},{"categories":"줄리아","contents":"Overview The package that facilitates the convenient use of colors in Julia is Colors.jl. It can be used together just by importing the visualization package Plots.jl.\nSymbols and Strings The way to check the list of named colors is by entering Colors.color_names in the console window or checking the official documentation.\njulia\u0026gt; using Plots\rjulia\u0026gt; Colors.color_names\rDict{String, Tuple{Int64, Int64, Int64}} with 666 entries:\r\u0026#34;darkorchid\u0026#34; =\u0026gt; (153, 50, 204)\r\u0026#34;chocolate\u0026#34; =\u0026gt; (210, 105, 30)\r\u0026#34;chocolate2\u0026#34; =\u0026gt; (238, 118, 33)\r\u0026#34;grey69\u0026#34; =\u0026gt; (176, 176, 176)\r\u0026#34;grey97\u0026#34; =\u0026gt; (247, 247, 247)\r\u0026#34;olivedrab3\u0026#34; =\u0026gt; (154, 205, 50)\r\u0026#34;deeppink2\u0026#34; =\u0026gt; (238, 18, 137)\r\u0026#34;mediumpurple2\u0026#34; =\u0026gt; (159, 121, 238)\r\u0026#34;ivory1\u0026#34; =\u0026gt; (255, 255, 240)\r⋮ =\u0026gt; ⋮ Keywords that can designate colors basically allow the use of symbols and strings. Whether you enter the color name as a symbol or a string, the respective color is applied. It doesn’t matter what is entered as it is passed to Colors.parse(Colorant, color name), so the result is the same whether a symbol or string.\njulia\u0026gt; Colors.parse(Colorant, :red)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; Colors.parse(Colorant, \u0026#34;red\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0) The results of designating colors in various graphs are as follows.\nplot(randn(50, 6),\rseriescolor = [:red :hotpink1 :purple3 \u0026#34;blue\u0026#34; \u0026#34;lime\u0026#34; \u0026#34;brown4\u0026#34;],\rseriestype = [:line :scatter :histogram :shape :sticks :steppre],\rlayout = (3,2)\r) RGB RGB color codes can be used like colorant\u0026quot;rgb(255, 0, 0)\u0026quot;. Only integers in $[0, 255]$ can be entered in rgb().\njulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34; # rgb() notation with integers in [0, 255]\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;rgba(0, 0, 255, 0.5)\u0026#34; # with alpha in [0, 1]\rRGBA{N0f8}(0.0,0.0,1.0,0.502)\rplot(rand(20, 2),\rseriescolor = [colorant\u0026#34;rgb(255, 0, 0)\u0026#34; colorant\u0026#34;rgba(0, 0, 255, 0.5)\u0026#34;],\rlayout = 2\r) For more details on handling RGB color codes, refer to here.\nHEX 6-digit HEX codes can be used like colorant\u0026quot;#FF0000\u0026quot;, and 3-digit HEX codes like colorant\u0026quot;#00f\u0026quot;.\njulia\u0026gt; colorant\u0026#34;#FF0000\u0026#34; # 6-digit hex notation\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;#00f\u0026#34; # 3-digit hex notation\rRGB{N0f8}(0.0,0.0,1.0)\rjulia\u0026gt; plot(rand(20, 2),\rseriescolor = [colorant\u0026#34;#FF0000\u0026#34; colorant\u0026#34;#00f\u0026#34;],\rlayout = 2\r) For more details on handling HEX color codes, refer to here.\nEnvironment OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10 See Also How to use colors How to use palettes How to use color gradients Package for color processing Colors.jl How to use RGB codes RGB(1, 0, 0) How to use HEX codes \u0026quot;#000000\u0026quot; How to designate colors for graph elements How to designate colors for each subplot How to designate colors for axes, axis names, ticks, and tick values How to designate background colors ","id":3598,"permalink":"https://freshrimpsushi.github.io/en/posts/3598/","tags":null,"title":"How to Use Colors in Julia Plots"},{"categories":"줄리아","contents":"Code Using the function printstyled(string; color = color) allows you to decorate the outputted function. As input for the keyword argument color, symbols and integers $(0 \\le n \\le 255)$ are possible. Note that strings are not allowed.\nThe available symbols include not only colors but also options like :blink, :reverse, etc. These can also be applied by entering them as keyword arguments like blink = true, bold = true.\n:normal :default :blink :bold :hidden :nothing :reverse :underline :white :light_white :black :light_black :blue :light_blue :cyan :light_cyan :green :light_green :magenta :light_magenta :red :light_red :yellow :light_yellow symbols = [:normal :default :blink :bold :hidden :nothing :reverse :underline :white :light_white :black :light_black :blue :light_blue :cyan :light_cyan :green :light_green :magenta :light_magena :red :light_red :yellow :light_yellow]\rfor i ∈ 1:length(symbols)\rprintstyled(\u0026#34;Hello ($(symbols[i]))\\n\u0026#34;, color = symbols[i])\rend Entering Base.text_colors returns all possible values for the keyword argument (including symbols).\njulia\u0026gt; Base.text_colors\rDict{Union{Int64, Symbol}, String} with 280 entries:\r56 =\u0026gt; \u0026#34;\\e[38;5;56m\u0026#34;\r35 =\u0026gt; \u0026#34;\\e[38;5;35m\u0026#34;\r60 =\u0026gt; \u0026#34;\\e[38;5;60m\u0026#34;\r220 =\u0026gt; \u0026#34;\\e[38;5;220m\u0026#34;\r:blink =\u0026gt; \u0026#34;\\e[5m\u0026#34;\r67 =\u0026gt; \u0026#34;\\e[38;5;67m\u0026#34;\r215 =\u0026gt; \u0026#34;\\e[38;5;215m\u0026#34;\r73 =\u0026gt; \u0026#34;\\e[38;5;73m\u0026#34;\r251 =\u0026gt; \u0026#34;\\e[38;5;251m\u0026#34;\r115 =\u0026gt; \u0026#34;\\e[38;5;115m\u0026#34;\r⋮ =\u0026gt; ⋮ See Also The package Crayons.jl can also be used.\nEnvironment OS: Windows11 Version: Julia 1.9.4 ","id":3597,"permalink":"https://freshrimpsushi.github.io/en/posts/3597/","tags":null,"title":"Decorating Text Output with Built-in Functions in Julia"},{"categories":"데이터과학","contents":"Definition Let\u0026rsquo;s assume a data set $X \\subset \\mathbb{R}^{n}$ is given. The following mapping for $m \\lt n$ is called dimension reductiondimension reduction.\n$$ r : X \\to \\mathbb{R}^{m} $$\nOr more commonly in machine learning, any method that reduces the number of input variables in a way that retains as much of the performance as possible is called a dimension reduction technique.\nExplanation Dimension reduction, as the name suggests, refers to reducing the dimensionality of vectors. It is often used to make data easier and more intuitive to understand. The method of reduction varies by algorithm. It might involve outright deleting certain components or creating new, lower-dimensional data from the existing data according to predefined rules. The following are some of the techniques:\nPrincipal Component AnalysisPCA Principal Component Analysis in Mathematical Statistics Purpose Visualization It is practically impossible to efficiently visualize data with more than four dimensions. Even with three-dimensional data, depending on its form, there can be difficulties in visualization. Difficulties in visualization mean that it is challenging to draw pictures that well represent the features of the data. For three-dimensional data, the shape may appear differently depending on the viewpoint. In such cases, reducing the dimensionality for drawing can make it easier to grasp the features of the data. The picture below shows an example where the same data looks significantly different depending on the viewing direction. The right picture is a projection of the left data onto the $xy$-plane.\nThe Iris dataset, which is four-dimensional data, is often introduced in many data science textbooks as being visualized by splitting it into several two-dimensional figures like the following.\nFocus and Selection Dimension reduction can be used to discard less important information to focus more on the important information. \u0026ldquo;Less important information\u0026rdquo; here refers to noise or redundant information. For example, looking at the left table below, one can see that the first column has the same value for all data. Also, the second and third columns have different values but are essentially the same. Thus, dimension reduction can be done by discarding the first column and either the second or third column. Furthermore, the right table summarizes weather information for Daegu. At first glance, it might seem like there is no unnecessary information, but since \u0026ldquo;daily temperature range = maximum temperature - minimum temperature,\u0026rdquo; these three are not linearly independent, and actually, errors can occur in regression analysis. Therefore, in this case, deleting the fourth column to remove multicollinearity is an example of dimension reduction.\nSchool\rGrade\rGroup\rName\rHive High School\r3rd grade\rfromis_9\rLEE NA GYUNG\rHive High School\r3rd grade\rfromis_9\rBAEK JI HEON\rHive High School\r2nd grade\rLE SSERAFIM\rKIM CHAEWON\rHive High School\r2nd grade\rLE SSERAFIM\rHUH YUNJIN\rHive High School\r1st grade\rNewJeans\rHAERIN\rHive High School\r1st grade\rNewJeans\rMINJI\rDate\rHigh Temp\rLow Temp\rDaily\nTemp Range\rPrecipitation\nProbability\r19th\r32º\r24º\r8º\r60%\r20th\r33º\r22º\r11º\r0%\r21st\r32º\r23º\r9º\r30%\r22nd\r30º\r21º\r9º\r60%\r23rd\r31º\r24º\r7º\r60%\r24th\r33º\r25º\r8º\r60%\rLightweighting Reducing the dimensions of data means there are fewer numbers to store, thereby reducing the data\u0026rsquo;s storage size. In the case of artificial neural networks, MLPs consist of linear layers where the dimension of input data influences the number of model parameters. Dimension reduction can be used here to reduce the number of model parameters. Even in models like CNNs, where the input data\u0026rsquo;s dimensions do not affect the number of model parameters, reducing dimensions can still offer computational speed advantages.\nPreventing Overfitting Appropriate dimension reduction is known to be able to prevent overfitting to some extent.\n","id":3563,"permalink":"https://freshrimpsushi.github.io/en/posts/3563/","tags":null,"title":"Dimensionality Reduction in Data Science"},{"categories":"머신러닝","contents":"Overview In TensorFlow, neural networks can be easily defined using Keras. Below, we introduce how to define and train a simple MLP using Sequential() and the functional API. However, Sequential() is only easy for defining models and can be challenging to use for designing complex structures. Similarly, if you plan to design complex structures using the functional API, it’s better to use the keras.Model class, and for even more complex and customizable designs, implementing at a lower level without Keras might be preferable. Depending on the deep learning task, these methods might not be the primary choice, especially for researchers in STEM fields looking to integrate deep learning into their domain. These methods are more about getting a feel for \u0026rsquo;this is how it\u0026rsquo;s used\u0026rsquo; when first learning and practicing deep learning.\nSequential Model Model Definition Let’s define an MLP with input and output dimensions of 1 to approximate the sine function $\\sin : \\mathbb{R} \\to \\mathbb{R}$ as follows.\nimport tensorflow as tf\rfrom tensorflow.keras import Sequential\rfrom tensorflow.keras.layers import Dense\r# model define\rmodel = Sequential([Dense(10, input_dim = 1, activation = \u0026#34;relu\u0026#34;),\rDense(10, input_dim = 10, activation = \u0026#34;relu\u0026#34;),\rDense(1, input_dim = 10)])\rmodel.summary() # output↓\r# Model: \u0026#34;sequential_3\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param # # =================================================================\r# dense_9 (Dense) (None, 10) 20 # # dense_10 (Dense) (None, 10) 110 # # dense_11 (Dense) (None, 1) 11 # # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ One feature of keras.layers.Dense() is that it\u0026rsquo;s not necessary to specify the input dimensions. The reason for this allowance is unclear, but for readability (especially in code that others might see), it\u0026rsquo;s better to explicitly state the input dimensions. This results in a characteristic where the output dimensions are on the left and input dimensions on the right. To read the structure of the model, one must read from right to left, which is not the standard in most languages. If we consider a linear layer as a matrix in terms of linear transformation, then it\u0026rsquo;s natural for the input to be on the right and the output on the left as in $\\mathbf{y} = A\\mathbf{x}$. However, TensorFlow wasn’t necessarily designed with such mathematical precision in mind. Even in Julia, known for its mathematical rigor, linear layers are implemented as Dense(in, out), which is naturally read from left to right. After all, it’s more comfortable and easier to understand. Moreover, the notation of a function $f$ from $X$ to $Y$ is $f : X \\to Y$, and there’s no function anywhere (apart from Keras) that is described as mapping from right to left.\nData Generation Since we are training a sine function, if we take the function values as data and compare the graph of the sine function with the model\u0026rsquo;s output, it will look like this:\n# generating data\rfrom math import pi\rx = tf.linspace(0., 2*pi, num=1000) # 入力データ\ry = tf.sin(x) # 出力データ(label)\r# check output of model\rimport matplotlib.pyplot as plt\rplt.plot(x, model(x), label=\u0026#34;model\u0026#34;)\rplt.plot(x, y, label=\u0026#34;sin\u0026#34;)\rplt.legend()\rplt.show() Training and Results from tensorflow.keras.optimizers import Adam\rmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\u0026#39;mse\u0026#39;) model.compile(optimizer, loss, metric) The .compile() method specifies the optimizer and loss function. Another key option is metric, which is a function for evaluating the model. It can be the same as or different from the loss. For example, if training an MLP on the MNIST dataset, the loss might be the MSE between the output and the label, while the metric could be the ratio of correctly predicted data out of the total.\n\u0026gt; model.fit(x, y, epochs=10000, batch_size=1000, verbose=\u0026#39;auto\u0026#39;)\r.\r.\r.\rEpoch 9998/10000\r1/1 [==============================] - 0s 8ms/step - loss: 6.2260e-06\rEpoch 9999/10000\r1/1 [==============================] - 0s 4ms/step - loss: 6.2394e-06\rEpoch 10000/10000\r1/1 [==============================] - 0s 3ms/step - loss: 6.2385e-06 The .fit() method takes inputs, labels, epochs, batch sizes, etc., and executes the training. verbose determines how the training progress is output. There are options 0, 1, 2, where 0 outputs nothing. The others output in the following format: # verbose=1\rEpoch (current epoch)/(total epochs)\r(current batch)/(total batchs) [==============================] - 0s 8ms/step - loss: 0.7884\r# verbose=2\rEpoch (current epoch)/(total epochs)\r(current batch)/(total batchs) - 0s - loss: 0.7335 - 16ms/epoch - 8ms/step After training, comparing the sine function with the model\u0026rsquo;s function values shows that the training was successful.\nFunctional API This method directly connects layers using the Input() and Model() functions. For simple models like MLPs, defining them using the Sequential model above is much more convenient. The method to define the same structure as the neural network defined in the Sequential model above is as follows:\nfrom tensorflow.keras import Model\rfrom tensorflow.keras.layers import Input, Dense\rinput = Input(shape=(10)) # \u0026#34;dim of output = dim of input in 1st layer\u0026#34;\rdense1 = Dense(10, activation = \u0026#34;relu\u0026#34;)(input)\rdense2 = Dense(10, activation = \u0026#34;relu\u0026#34;)(dense1)\routput = Dense(1)(dense2)\rmodel = Model(inputs=input, outputs=output)\rmodel.summary() # output↓\r# Model: \u0026#34;model_10\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param #\r# =================================================================\r# input_13 (InputLayer) [(None, 1)] 0\r# # dense_19 (Dense) (None, 10) 20\r# # dense_20 (Dense) (None, 10) 110\r# # dense_21 (Dense) (None, 1) 11\r# # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ Input is a function for defining the input layer. Strictly speaking, it’s not a layer but a tensor, but this is a minor detail. A confusing point is that the output dimension should be input as a variable. In other words, the input dimension of the first layer should be input. After defining this, connect each layer directly and explicitly as input to the Dense function. Finally, input the input and output as arguments to the Model function to define the model.\nThe subsequent process of compiling the model with the .compile() method and training it with the .fit() method is the same as introduced above.\nEnvironment OS: Windows11 Version: Python 3.9.13, tensorflow==2.12.0, keras==2.12.0 ","id":3562,"permalink":"https://freshrimpsushi.github.io/en/posts/3562/","tags":null,"title":"How to Define and Train MLP with the Sequence Model and Functional API in TensorFlow and Keras"},{"categories":"줄리아","contents":"Code Size plot(x, y, size=(600,400)) In Julia, the size of a plot is set using the size option. It must be input as a Tuple{Integer, Integer}, where each integer represents the width and height in pixels, respectively. The default value is (600,400).\nusing Plots\rx = rand(10)\rplot(x)\rsavefig(\u0026#34;size_default.png\u0026#34;)\rplot(x, size=(1200,800))\rsavefig(\u0026#34;size_(1200,800).png\u0026#34;) 1800x1200 image (left), 600x400 image (right) Resolution plot(x, y, dpi=100) The resolution of an image is set using the dpi option, with the default value being 100. It’s advisable to use about 300 for documents to be included in papers, reports, PowerPoint presentations, etc.\nusing Plots\rx = rand(10)\rplot(x)\rsavefig(\u0026#34;dpi_default.png\u0026#34;)\rplot(x, dpi=300) savefig(\u0026#34;dpi_300.png\u0026#34;) dpi=100 image (top), dpi=300 image (bottom) Also, when increasing the size, it’s necessary to increase the resolution as well to maintain the attractiveness of the image. If you save with the dpi set to 300 and the size increases to 1800x1200, but keep the dpi at the default value of 100 while increasing just the size, the image will become unattractive, so pay attention.\ndpi=300, size=1800x1200 image (left), dpi=100, size=1800x1200 image (right) Environment OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12 ","id":3559,"permalink":"https://freshrimpsushi.github.io/en/posts/3559/","tags":null,"title":"How to Adjust the Size and Resolution of an Image in Julia"},{"categories":"줄리아","contents":"Code plot!([x1, x2], [y1, y2], arrow=:true) This code plots an arrow from point $(x1, y1)$ to point $(x2, y2)$ on the plot. Naturally, the tip of the arrow is at the terminal point $(x2, y2)$. The maximum value of the sine function can be shown as follows.\nusing Plots\rx = range(0, 2π, 100)\rplot(x, sin.(x), label=\u0026#34;\u0026#34;, ylims=(-1.3,1.3))\rplot!([π/2, 3], [1, 1.1], arrow=:true, color=:black, label=\u0026#34;\u0026#34;)\rannotate!(3.7, 1.1, \u0026#34;maximum\u0026#34;) Arrow tip The style of the tip can be chosen as :open or :closed.\nNot specified or :true: a polyline $\\to$ plot!([3π/2, 3], [-1, -1.1], arrow=:open, color=:red, label=\u0026#34;\u0026#34;)\rannotate!(2.3, -1.1, \u0026#34;minimum\u0026#34;) Arrow direction The direction of the tip can be set to :head, :tail, :both, with :head being the default.\nplot!([π/2, π/2], [0, 1], arrow=(:closed, :both), color=:purple, label=\u0026#34;\u0026#34;)\rannotate!(0.75π, 0.5, \u0026#34;amplitude\u0026#34;) In the official documentation1, there are explanations about the headlength and headwidth options, but trying to use them only leads to errors and it is unclear how to use them.\nEnvironment OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12 https://docs.juliaplots.org/v1.38/api/#Plots.arrow-Tuple\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3558,"permalink":"https://freshrimpsushi.github.io/en/posts/3558/","tags":null,"title":"Drawing Arrows in Graphics with Julia"},{"categories":"줄리아","contents":"Explanation1 In Julia, the random seed can be fixed as follows:\nseed!([rng=default_rng()], seed) -\u0026gt; rng seed!([rng=default_rng()]) -\u0026gt; rng The input variable rng stands for Random Number Generator, which refers to the algorithm used for drawing random numbers. The Random package offers the following options:\nTaskLocalRNG: This is the default setting. Xoshiro RandomDevice MersenneTwister Code By fixing the seed to 0, drawing three times, and then fixing it again to 0 and drawing three times, it can be verified that the same values are obtained.\njulia\u0026gt; using Random\rjulia\u0026gt; Random.seed!(0)\rTaskLocalRNG()\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.4056994708920292\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.06854582438651502\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.8621408571954849\rjulia\u0026gt; Random.seed!(0)\rTaskLocalRNG()\rjulia\u0026gt; rand(3)\r3-element Vector{Float64}:\r0.4056994708920292\r0.06854582438651502\r0.8621408571954849 https://docs.julialang.org/en/v1/stdlib/Random/index.html#Random.seed!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3555,"permalink":"https://freshrimpsushi.github.io/en/posts/3555/","tags":null,"title":"How to Fix the Random Seed in Julia"},{"categories":"줄리아","contents":"English Translation Description To draw a box plot, the statistical visualization package StatsPlots.jl must be used.\nboxplot([data], labels=[label]) Code using StatsPlots\rx = rand(0:100, 100)\ry = rand(50:100, 100)\rz = cat(x,y, dims=1)\rboxplot(x, label=\u0026#34;x\u0026#34;)\rboxplot!(y, label=\u0026#34;y\u0026#34;)\rboxplot!(z, label=\u0026#34;z\u0026#34;) Or boxplot([x,y,z], label=[\u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot;]) will draw the same figure. Note that there should be no commas in lable. That is, it needs to be an array, not an $3 \\times 1$ vector as in $1 \\times 3$.\nx-axis tick If you want to represent the x-axis ticks with strings,\nboxplot([x, y, z], xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;]) Or the code below will also draw the same figure. The difference is how the actual coordinates are. In the code above, the actual x-coordinates where each box is drawn are 1, 2, 3, but it just appears with the tick values as x, y, z. The code below actually draws boxes on the \u0026ldquo;x\u0026rdquo;, \u0026ldquo;y\u0026rdquo;, \u0026ldquo;z\u0026rdquo; coordinates.\nDrawing with a 2D array a = rand(100, 3)\rboxplot(a, xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;]) Drawing with a data frame Data frames cannot be drawn on their own and need to be converted to an array.\nusing MLDatasets\rusing DataFrames\rdf = Iris().features\rboxplot(Array(df), xticks=(1:4, names(df)), label=reshape(names(df), (1,4))) Average There is no separate option to display the average. Use scatter to plot it.\nusing Statistics\rboxplot(fill(\u0026#34;x\u0026#34;, length(x)), x, labels=\u0026#34;x\u0026#34;)\rboxplot!(fill(\u0026#34;y\u0026#34;, length(y)), y, labels=\u0026#34;y\u0026#34;)\rboxplot!(fill(\u0026#34;z\u0026#34;, length(z)), z, labels=\u0026#34;z\u0026#34;)\rscatter!([\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;], [mean(x), mean(y), mean(z)], color=palette(:default)[1:3], label=\u0026#34;\u0026#34;) Or the following code also draws the same picture.\nboxplot([x, y, z], xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;])\rscatter!([1, 2, 3], [mean(x), mean(y), mean(z)], color=palette(:default)[1:3], label=\u0026#34;\u0026#34;) Environment OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12, StatsPlots v0.15.5, DataFrames v1.5.0, MLDatasets v0.7.11 See Also How to draw with Python matplotlib ","id":3553,"permalink":"https://freshrimpsushi.github.io/en/posts/3553/","tags":null,"title":"How to Draw a Box Plot in Julia"},{"categories":"줄리아","contents":"Overview Introducing the DecisionTree.jl package, which implements Decision Trees in Julia1.\nCode As an example, we use the iris dataset, a classic built-in dataset in R. Our goal is to create a decision tree that uses four variables SepalLength, SepalWidth, PetalLength, PetalWidth to predict Species and evaluate its performance.\njulia\u0026gt; iris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\r150×5 DataFrame\rRow │ SepalLength SepalWidth PetalLength PetalWidth Speci ⋯\r│ Float64 Float64 Float64 Float64 Cat… ⋯\r─────┼──────────────────────────────────────────────────────────\r1 │ 5.1 3.5 1.4 0.2 setos ⋯\r2 │ 4.9 3.0 1.4 0.2 setos 3 │ 4.7 3.2 1.3 0.2 setos 4 │ 4.6 3.1 1.5 0.2 setos 5 │ 5.0 3.6 1.4 0.2 setos ⋯\r6 │ 5.4 3.9 1.7 0.4 setos 7 │ 4.6 3.4 1.4 0.3 setos 8 │ 5.0 3.4 1.5 0.2 setos 9 │ 4.4 2.9 1.4 0.2 setos ⋯\r10 │ 4.9 3.1 1.5 0.1 setos 11 │ 5.4 3.7 1.5 0.2 setos 12 │ 4.8 3.4 1.6 0.2 setos 13 │ 4.8 3.0 1.4 0.1 setos ⋯\r14 │ 4.3 3.0 1.1 0.1 setos 15 │ 5.8 4.0 1.2 0.2 setos ⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋱\r136 │ 7.7 3.0 6.1 2.3 virgi 137 │ 6.3 3.4 5.6 2.4 virgi ⋯\r138 │ 6.4 3.1 5.5 1.8 virgi 139 │ 6.0 3.0 4.8 1.8 virgi 140 │ 6.9 3.1 5.4 2.1 virgi 141 │ 6.7 3.1 5.6 2.4 virgi ⋯\r142 │ 6.9 3.1 5.1 2.3 virgi 143 │ 5.8 2.7 5.1 1.9 virgi 144 │ 6.8 3.2 5.9 2.3 virgi 145 │ 6.7 3.3 5.7 2.5 virgi ⋯\r146 │ 6.7 3.0 5.2 2.3 virgi 147 │ 6.3 2.5 5.0 1.9 virgi 148 │ 6.5 3.0 5.2 2.0 virgi 149 │ 6.2 3.4 5.4 2.3 virgi ⋯\r150 │ 5.9 3.0 5.1 1.8 virgi 1 column and 120 rows omitted Model Creation julia\u0026gt; using DecisionTree\rjulia\u0026gt; model = DecisionTreeClassifier(max_depth=2)\rDecisionTreeClassifier\rmax_depth: 2\rmin_samples_leaf: 1\rmin_samples_split: 2\rmin_purity_increase: 0.0\rpruning_purity_threshold: 1.0\rn_subfeatures: 0\rclasses: nothing\rroot: nothing Create the model. Parameters for the decision tree can be given through DecisionTreeClassifier().\nModel Fitting julia\u0026gt; features = Matrix(iris[:, Not(:Species)]);\rjulia\u0026gt; labels = iris.Species;\rjulia\u0026gt; fit!(model, features, labels) DecisionTreeClassifier\rmax_depth: 2\rmin_samples_leaf: 1\rmin_samples_split: 2\rmin_purity_increase: 0.0\rpruning_purity_threshold: 1.0\rn_subfeatures: 0\rclasses: [\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;]\rroot: Decision Tree Leaves: 3\rDepth: 2 Divide the data into independent and dependent variables and train the model using the fit!() function.\nPerformance Check julia\u0026gt; print_tree(model)\rFeature 3 \u0026lt; 2.45 ?\r├─ setosa : 50/50\r└─ Feature 4 \u0026lt; 1.75 ?\r├─ versicolor : 49/54\r└─ virginica : 45/46 After training, the structure of the model can be checked with the print_tree() function.\njulia\u0026gt; sum(labels .== predict(model, features)) / length(labels)\r0.96 A quick check of the accuracy rate showed it was around 96%, which is quite decent.\nFull Code using RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rusing DecisionTree\rmodel = DecisionTreeClassifier(max_depth=2)\rfeatures = Matrix(iris[:, Not(:Species)]);\rlabels = iris.Species;\rfit!(model, features, labels)\rprint_tree(model)\rsum(labels .== predict(model, features)) / length(labels) Environment OS: Windows julia: v1.9.0 https://github.com/JuliaAI/DecisionTree.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2618,"permalink":"https://freshrimpsushi.github.io/en/posts/2618/","tags":null,"title":"How to Use Decision Trees in Julia"},{"categories":"줄리아","contents":"Overview This section introduces how to remove and check for duplicates in collections in Julia. The unique() function, which eliminates duplicates, is algorithmically straightforward, but can be bothersome to implement on your own, and may not be efficient. The allunique() function, which checks for the absence of duplicate elements, is easy enough to implement that one might not have sought it out, so it’s worth getting familiar with.\nCode unique() julia\u0026gt; x = [3, 1, 4, 1, 5, 9, 2, 6, 5];\rjulia\u0026gt; y = unique(x)\r7-element Vector{Int64}:\r3\r1\r4\r5\r9\r2\r6 allunique() 1 In fact, the purpose of this post is to introduce allunique(). One common sense way to check if a collection has duplicate elements is to see if length(unique(x)) == length(x) by applying unique() to see if the number of elements has decreased.\nThis method might seem too easy to be overconfident about its efficiency; however, the unique() function has to look at every element of an array of length $n$ at least once, which means its time complexity is $O (n)$. This could certainly be a noticeable burden when frequently checking for duplicates in code, and allunique() offers a clear advantage in performance as it may vary its implementation based on the array\u0026rsquo;s length and can stop its calculation upon finding a duplicate, thereby efficiently determining success.\njulia\u0026gt; allunique(x)\rfalse\rjulia\u0026gt; allunique(y)\rtrue Complete Code x = [3, 1, 4, 1, 5, 9, 2, 6, 5];\ry = unique(x)\rallunique(x)\rallunique(y) Environment OS: Windows julia: v1.9.0 https://docs.julialang.org/en/v1/base/collections/#Base.allunique\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2616,"permalink":"https://freshrimpsushi.github.io/en/posts/2616/","tags":null,"title":"How to Remove Duplicates from a Collection in Julia"},{"categories":"줄리아","contents":"Overview In Julia, the package for clustering offered is Clustering.jl1. The algorithms implemented include:\nK-means K-medoids Affinity Propagation Density-based spatial clustering of applications with noise (DBSCAN) Markov Clustering Algorithm (MCL) Fuzzy C-Means Clustering Hierarchical Clustering Single Linkage Average Linkage Complete Linkage Ward\u0026rsquo;s Linkage Code DBSCAN DBSCAN (Density-based spatial clustering of applications with noise) is implemented with the dbscan() function. If there are $n$ pieces of data in $p$ dimensions, a matrix of size $p \\times n$ and a radius should be given as arguments.\njulia\u0026gt; points = [iris.PetalLength iris.PetalWidth]\u0026#39;\r2×150 adjoint(::Matrix{Float64}) with eltype Float64:\r1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 … 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.0 5.2 5.4 5.1 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 0.2 2.1 2.4 2.3 1.9 2.3 2.5 2.3 1.9 2.0 2.3 1.8 julia\u0026gt; dbscaned = dbscan(points, 0.5)\rDbscanResult(DbscanCluster[DbscanCluster(50, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], Int64[]), DbscanCluster(100, [51, 52, 53, 54, 55, 56, 57, 58, 59, 60 … 141, 142, 143, 144, 145, 146, 147, 148, 149, 150], Int64[])], [1, 51], [50, 100], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1 … 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\rjulia\u0026gt; dbscaned |\u0026gt; propertynames\r(:clusters, :seeds, :counts, :assignments) The result of DBSCAN is returned as a structure called DbscanResult. .assignments and .cluster are important for us.\nHow each data point belongs to each cluster can be obtained through the getproperty() function as follows.\njulia\u0026gt; getproperty.(dbscaned.clusters, :core_indices)\r2-element Vector{Vector{Int64}}:\r[1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\r[51, 52, 53, 54, 55, 56, 57, 58, 59, 60 … 141, 142, 143, 144, 145, 146, 147, 148, 149, 150] Which cluster each data point belongs to can be known through the .assignments property as follows.\njulia\u0026gt; dbscaned.assignments\r150-element Vector{Int64}:\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r⋮\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2 As a tip for visualization, since clusters are assigned arbitrary integers, putting *.assignments directly into the color option when drawing a scatter plot assigns colors corresponding to each cluster as follows.\nscatter(iris.PetalLength, iris.PetalWidth,\rxlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;,\rcolor = dbscaned.assignments) It can be confirmed that the clustering has been performed well.\nFull Code using Clustering\rusing RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rscatter(iris.PetalLength, iris.PetalWidth, xlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;)\rpng(\u0026#34;iris\u0026#34;)\rpoints = [iris.PetalLength iris.PetalWidth]\u0026#39;\rdbscaned = dbscan(points, 0.5)\rdbscaned |\u0026gt; propertynames\rgetproperty.(dbscaned.clusters, :core_indices)\rdbscaned.assignments\rscatter(iris.PetalLength, iris.PetalWidth,\rxlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;,\rcolor = dbscaned.assignments) Environment OS: Windows julia: v1.9.0 Clustering v0.15.4 https://github.com/JuliaStats/Clustering.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2613,"permalink":"https://freshrimpsushi.github.io/en/posts/2613/","tags":null,"title":"How to Use Clustering Packages in Julia"},{"categories":"줄리아","contents":"Overview In Julia, the Zygote.jl package is used for automatic differentiation, especially in the field of machine learning, and particularly for deep learning. The developers promote this package as the next-generation automatic differentiation system that enables differentiable programming in Julia, and indeed, using it can be surprisingly intuitive.\nIf you are curious about packages related to the derivative itself, not automatic differentiation, check out the Calculus.jl package.\nCode Univariate Functions It\u0026rsquo;s incredibly simple. Just like when we differentiate normally, appending a prime ' to the function name computes the derivative as if we are working with an actual derivative.\njulia\u0026gt; using Zygote\rjulia\u0026gt; p(x) = 2x^2 + 3x + 1\rp (generic function with 1 method)\rjulia\u0026gt; p(2)\r15\rjulia\u0026gt; p\u0026#39;(2)\r11.0\rjulia\u0026gt; p\u0026#39;\u0026#39;(2)\r4.0 Multivariate Functions Use the gradient() function.\njulia\u0026gt; g(x,y) = 3x^2 + 2y + x*y\rg (generic function with 1 method)\rjulia\u0026gt; gradient(g, 2,-1)\r(11.0, 4.0) If you want to write code a bit more intuitively, you can redefine the function using \\nabla, or ∇, like below.\njulia\u0026gt; ∇(f, v...) = gradient(f, v...)\r∇ (generic function with 1 method)\rjulia\u0026gt; ∇(g, 2, -1)\r(11.0, 4.0) Full Code using Zygote\rp(x) = 2x^2 + 3x + 1\rp(2)\rp\u0026#39;(2)\rp\u0026#39;\u0026#39;(2)\rg(x,y) = 3x^2 + 2y + x*y\rgradient(g, 2,-1)\r∇(f, v...) = gradient(f, v...)\r∇(g, 2, -1) Environment OS: Windows julia: v1.9.0 Zygote: v0.6.62 ","id":2609,"permalink":"https://freshrimpsushi.github.io/en/posts/2609/","tags":null,"title":"Julia's Automatic Differentiation Package Zygote.jl"},{"categories":"줄리아","contents":"Overview In Julia, there are mainly two ways to reference the properties of a structure. They should be used appropriately according to grammatical convenience or actual use.\nCode For example, in Julia, the // operator creates a Rational type of number as follows. The names of the properties that a rational number has include :num meaning numerator and :den meaning denominator.\njulia\u0026gt; q = 7 // 12\r7//12\rjulia\u0026gt; q |\u0026gt; typeof\rRational{Int64}\rjulia\u0026gt; q |\u0026gt; propertynames\r(:num, :den) getproperty(x, :y) and x.y julia\u0026gt; getproperty(q, :den)\r12\rjulia\u0026gt; q.den\r12 Essentially, you just need to provide the name of the property as a symbol in the second argument of the getproperty() function. Or, as in many common programming languages, you can access the property by appending a dot after the object variable name.\nReference to Properties of Arrays Meanwhile, the above method can be used when needed just once, but if you need to access several elements in an array, you should use broadcasting as follows. Or, if performance is not important and quick coding is needed, using list comprehension like Python is also an option.\njulia\u0026gt; Q = [k // 12 for k in 1:12]\r12-element Vector{Rational{Int64}}:\r1//12\r1//6\r1//4\r1//3\r5//12\r1//2\r7//12\r2//3\r3//4\r5//6\r11//12\r1//1\rjulia\u0026gt; getproperty.(Q, :num)\r12-element Vector{Int64}:\r1\r1\r1\r1\r5\r1\r7\r2\r3\r5\r11\r1\rjulia\u0026gt; [q.num for q in Q]\r12-element Vector{Int64}:\r1\r1\r1\r1\r5\r1\r7\r2\r3\r5\r11\r1 Full Code q = 7 // 12 q |\u0026gt; typeof q |\u0026gt; propertynames getproperty(q, :den) q.den Q = [k // 12 for k in 1:12] getproperty.(Q, :num) [q.num for q in Q] Environment OS: Windows julia: v1.9.0 ","id":2607,"permalink":"https://freshrimpsushi.github.io/en/posts/2607/","tags":null,"title":"Referencing Struct Properties as Functions in Julia"},{"categories":"줄리아","contents":"Code quiver(, quiver=) In Julia, the quiver() function can be used to visualize a vector field.\nθ = 0:0.2:2π\rquiver(cos.(θ),sin.(θ), quiver = (-sin.(θ), cos.(θ)), size = (600,600), lims = (-2,2)); png(\u0026#34;1\u0026#34;) Changing the Length of Arrows There might be a better way to change the size of the arrows, but essentially, you can draw a better picture by increasing or decreasing the length of vectors provided by the quiver= option.\nquiver(cos.(θ),sin.(θ), quiver = (-sin.(θ), cos.(θ)) ./ 2, size = (600,600), lims = (-2,2)); png(\u0026#34;2\u0026#34;) Environment OS: Windows julia: v1.9.0 ","id":2605,"permalink":"https://freshrimpsushi.github.io/en/posts/2605/","tags":null,"title":"How to Draw Vector Fields in Julia"},{"categories":"줄리아","contents":"Overview When multiple arrays are given, there are often situations where one wants to access a specific element of these arrays, for example, the third element in each array. In Julia, this can be implemented through broadcasting the getindex() function.\nCode getindex.() julia\u0026gt; seq_ = [collect(1:k:100) for k in 1:10]\r10-element Vector{Vector{Int64}}:\r[1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\r[1, 3, 5, 7, 9, 11, 13, 15, 17, 19 … 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\r[1, 4, 7, 10, 13, 16, 19, 22, 25, 28 … 73, 76, 79, 82, 85, 88, 91, 94, 97, 100]\r[1, 5, 9, 13, 17, 21, 25, 29, 33, 37 … 61, 65, 69, 73, 77, 81, 85, 89, 93, 97]\r[1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81, 86, 91, 96]\r[1, 7, 13, 19, 25, 31, 37, 43, 49, 55, 61, 67, 73, 79, 85, 91, 97]\r[1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99]\r[1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97]\r[1, 10, 19, 28, 37, 46, 55, 64, 73, 82, 91, 100]\r[1, 11, 21, 31, 41, 51, 61, 71, 81, 91]\rjulia\u0026gt; getindex.(seq_, 3)\r10-element Vector{Int64}:\r3\r5\r7\r9\r11\r13\r15\r17\r19\r21 first(), last() first() is the same as getindex(, 1), but last() is special because there is no equivalent expression like getindex(, end). It\u0026rsquo;s often necessary to get the last result as the program iterates, and the index of that last element can vary greatly, so it\u0026rsquo;s good to know the last() function.\njulia\u0026gt; first.(seq_)\r10-element Vector{Int64}:\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\rjulia\u0026gt; last.(seq_)\r10-element Vector{Int64}:\r100\r99\r100\r97\r96\r97\r99\r97\r100\r91 Environment OS: Windows julia: v1.9.0 ","id":2603,"permalink":"https://freshrimpsushi.github.io/en/posts/2603/","tags":null,"title":"Referencing Specific Positions in an Array with Functions in Julia"},{"categories":"줄리아","contents":"Overview The way to load a package in Julia is to use using, but as the program grows, it becomes a task to individually write them each time. This introduces a method to load packages through a loop1.\nCode Metaprogramming packages = [:CSV, :DataFrames, :LinearAlgebra, :Plots]\rfor package in packages\r@eval using ▷eq1◁(package)\rend In actual use, it is better to load only the progress bar separately and visually check the loading of other packages.\nEnvironment OS: Windows julia: v1.9.0 https://discourse.julialang.org/t/programmatically-load-packages/52435/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2601,"permalink":"https://freshrimpsushi.github.io/en/posts/2601/","tags":null,"title":"How to Import Packages from Julia to R"},{"categories":"줄리아","contents":"Overview This document introduces a tip for easily normalizing matrices in Julia 1. At its core, it’s just mixing the method of scalar multiplying matrices by rows and columns, the eachcol() function, and the norm() function from the LinearAlgebra module, but it’s concise, ending in one line, and proving to be quite useful to memorize for frequent use.\nCode julia\u0026gt; using LinearAlgebra\rjulia\u0026gt; X = reshape(1:15, 5, :)\r5×3 reshape(::UnitRange{Int64}, 5, 3) with eltype Int64:\r1 6 11\r2 7 12\r3 8 13\r4 9 14\r5 10 15 Given a matrix X, normalizing it by columns can be succinctly done with just one line: X ./ norm.(eachcol(X))'. The execution and the results confirming that it was indeed properly normalized are as follows.\njulia\u0026gt; Z = X ./ norm.(eachcol(X))\u0026#39;\r5×3 Matrix{Float64}:\r0.13484 0.330289 0.376192\r0.26968 0.385337 0.410391\r0.40452 0.440386 0.444591\r0.53936 0.495434 0.47879\r0.6742 0.550482 0.512989\rjulia\u0026gt; norm.(eachcol(Z))\r3-element Vector{Float64}:\r1.0\r1.0\r1.0 Complete Code using LinearAlgebra\rX = reshape(1:15, 5, :)\rZ = X ./ norm.(eachcol(X))\u0026#39;\rnorm.(eachcol(Z)) Environment OS: Windows julia: v1.9.0 https://stackoverflow.com/a/72627341/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2599,"permalink":"https://freshrimpsushi.github.io/en/posts/2599/","tags":null,"title":"How to Normalize Matrices Column-wise in Julia"},{"categories":"행렬대수","contents":"Definitions 1 2 For a permutation matrix $P^{T}$ and an invertible matrix $A \\in \\mathbb{R}^{n \\times n}$, the matrix multiplication $P^{T} A$ gives us the product $LU$. This decomposition is referred to as the PLU decompositionPermutation LU Decomposition of $A$. Since $P$ is a permutation matrix, it is also an orthogonal matrix, that is $P^{-1} = P^{T}$, hence it can be written like this: $$ P^{T} A = LU \\iff A = PLU $$\nExplanation Algorithm for LU Decomposition: Suppose $(a_{ij}) \\in \\mathbb{R}^{n \\times n}$ is an invertible matrix.\nStep 1. $k = 1$\nInsert $u_{1j} = a_{1j}$ and compute $\\displaystyle l_{i1} = {{1} \\over {u_{11}}} a_{i1}$.\nStep 2. $k = 2, 3, \\cdots , n-1$\nStep 2-1. Compute the following: $$ u_{kk} = a_{kk} - \\sum_{s = 1}^{k-1} l_{ks} u_{sk} $$ Step 2-2. For $j = k+1, k+2, \\cdots , n-1$, compute the following: $$ u_{kj} = a_{kj} - \\sum_{s = 1}^{k-1} l_{ks} u_{sj} $$ Step 2-3. For $i = k+1, k+2, \\cdots , n-1$, compute the following: $$ l_{ik} = {{1} \\over {u_{kk}}} \\left{ a_{ik} - \\sum_{s = 1}^{k-1} l_{is} u_{sk} \\right} $$ Step 3. For $k = n$, compute the following: $$ u_{nn} = a_{nn} - \\sum_{s = 1}^{n-1} l_{ns} u_{sn} $$\nIn order to perform an LU decomposition of a matrix, one would need $u_{11} = a_{11}$ or to be able to take the reciprocal of $u_{kk}$. However, even simple matrices like $$ A = \\begin{bmatrix} 0 \u0026amp; 3\\\\ 2 \u0026amp; 1 \\end{bmatrix} $$ cannot be decomposed using this algorithm. To perform LU decomposition on such matrices, one multiplies by some permutation matrix $P^{T}$, expressing $A$ as $PLU$; this process is called PLU decomposition. Since it doesn\u0026rsquo;t really matter if it\u0026rsquo;s left or right, rows or columns, we can also write $$ A P^{T} = LU \\iff A = LUP $$ and refer to it as LUP decomposition.\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://math.unm.edu/~loring/links/linear_s08/LU\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2,"permalink":"https://freshrimpsushi.github.io/en/posts/2/","tags":null,"title":"PLU Decomposition"},{"categories":"행렬대수","contents":"Definition 1 $P \\in \\mathbb{R}^{n \\times n}$ in which only one component in each row is $1$ and the rest are $0$ is called a Permutation Matrix.\nBasic Properties Orthogonality All permutation matrices are orthogonal matrices: $$P^{-1} = P^{T}$$\nSparseness For sufficiently large $n$, $P \\in \\mathbb{R}^{n \\times n}$ is a sparse matrix.\nExplanation The Permutation Matrix gives a permutation of rows and columns through matrix multiplication. The following example shows that if it is multiplied on the left, it gives a row permutation, and if it is multiplied on the right, it gives a column permutation. $$ \\begin{align*} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\\\ \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{12} \u0026amp; a_{11} \u0026amp; a_{13} \\\\ a_{22} \u0026amp; a_{21} \u0026amp; a_{23} \\\\ a_{32} \u0026amp; a_{31} \u0026amp; a_{33} \\end{bmatrix} \\end{align*} $$\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1,"permalink":"https://freshrimpsushi.github.io/en/posts/1/","tags":null,"title":"Permutation Matrix"},{"categories":"줄리아","contents":"Error When using data frames in Julia, string data sometimes get read as String7, String15, String31, etc., causing various errors. Rather than specific errors occurring, the usual functions don\u0026rsquo;t work with these, causing all sorts of problems.\nCause For performance reasons, String was changed to faster versions like String7, etc. It\u0026rsquo;s designed this way on purpose, so nothing much can be done about it.\nSolution Passing the option stringtype = String to CSV.read() works.\nIt\u0026rsquo;s actually quite inconvenient, but you\u0026rsquo;ll have to deal with it.\nEnvironment OS: Windows julia: v1.8.5 ","id":2574,"permalink":"https://freshrimpsushi.github.io/en/posts/2574/","tags":null,"title":"How to Call a DataFrame without String7, String15 in Julia"},{"categories":"다변수벡터해석","contents":"Question In partial derivatives, unlike the usual derivatives, expressions like $\\displaystyle {{ \\partial f } \\over { \\partial t }}$ are used instead of $\\displaystyle {{ d f } \\over { d t }}$. $\\partial$ is read as \u0026ldquo;Round Dee\u0026rdquo; or \u0026ldquo;Partial,\u0026rdquo; and historically, it originated from \u0026ldquo;Curly Dee,\u0026rdquo; which is a cursive form of $d$1. In code, it\u0026rsquo;s \\partial, and in Korea, some people even shorten it to just \u0026ldquo;Round,\u0026rdquo; considering \u0026ldquo;Round Dee\u0026rdquo; too long.\nWhy is $d$ written as $\\partial$? The problem is that it\u0026rsquo;s hard to understand why a different symbol is used for partial differentiation, which is just differentiating with respect to another variable. At the undergraduate level, this question inevitably arises whenever partial differentiation is introduced, but the answer could be, if you\u0026rsquo;re not in a math department, \u0026ldquo;That\u0026rsquo;s something for mathematicians to worry about,\u0026rdquo; or even if you are, \u0026ldquo;It\u0026rsquo;s okay to accept it as just a notational difference.\u0026rdquo; This isn\u0026rsquo;t necessarily wrong, as whether it\u0026rsquo;s written as $d$ or $\\partial$, if you\u0026rsquo;re not in a math department, it\u0026rsquo;s not particularly important, and even if you are, it doesn\u0026rsquo;t change the meaning of the equation itself. For example, in the context of studying heat equations, $$ {{ \\partial u } \\over { \\partial t }} = {{ \\partial u } \\over { \\partial x^{2} }} $$ changing $\\partial$ to an ordinary differential expression $d$ and writing it as $$ {{ d u } \\over { d t }} = {{ d u } \\over { d x^{2} }} $$ could raise the question of whether these two equations are the same. Confusingly, the answer would be \u0026rsquo;they are indeed the same,\u0026rsquo; leading many students to feel that there\u0026rsquo;s no difference between $d$ and $\\partial$, or to just accept the definition and move on.\nAnswer Newton and Leibniz Before diving into partial differentiation, let\u0026rsquo;s start with an interesting read about the two fathers of differentiation, Newton and Leibniz. Nowadays, both are recognized for independently inventing the concept and notation of differentiation, with function $y = f(x)$ and its derivative, where Newton used notation like $$ y ' = f ' (x) $$ and Leibniz used $$ {{ dy } \\over { dx }} = {{ d f(x) } \\over { dx }} $$ The reason for the difference in expression, despite being the same differentiation, is due to their different thought processes and perspectives on calculus. It\u0026rsquo;s fortunate that there were two people independently inventing differentiation at the same time, as it could have been beneficial if there had been another person as well. Newton, as a master of [classical mechanics](../../categories/Classical Mechanics), often discussed \u0026lsquo;differentiating position once gives velocity, and twice gives acceleration,\u0026rsquo; making expressions like $$ \\begin{align*} v =\u0026amp; x ' \\\\ a =\u0026amp; v ' = x '' \\end{align*} $$ very neat and efficient. Leibniz, on the other hand, had a more logical approach from a geometric perspective, where the slope of a line is defined as the ratio of changes in horizontal and vertical directions, so for a curve, one could naturally approach the slope of the tangent by giving very small units as $$ {{ \\Delta y } \\over { \\Delta x }} \\approx {{ d y } \\over { d x }} $$ Interestingly, despite being about ordinary differentiation, the field allows for such divergence in notation, where Newton\u0026rsquo;s and Leibniz\u0026rsquo;s notations can coexist.\nIn differential geometry, the notation for differentiation with respect to $s$ and $t$: $$ {{ df } \\over { ds }} = f^{\\prime} \\quad \\text{and} \\quad {{ df } \\over { dt }} = \\dot{f} $$ Dot $\\dot{}$ or prime $'$, although both denote differentiation, can be distinguished in the context of differential geometry. Typically, $s$ represents the parameter of a unit speed curve, and $t = t(s)$ represents the parameter of the curve reparameterized by arc length.\nThis notation didn\u0026rsquo;t arise because the concept of differentiation was transformed. In differential geometry, differentiation is often performed with both $s$ and $t$, but Newton\u0026rsquo;s notation doesn\u0026rsquo;t allow distinguishing what is being differentiated, and Leibniz\u0026rsquo;s notation makes the expression too complicated, leading to the creation of an additional notation to take advantage of both.\nWhat\u0026rsquo;s fascinating is that, despite $s$ and $t$ being just variables used as parameters, in the context of ordinary differential equations involving time, the derivative with respect to $t$ began to be written not as $v '$ but as $\\dot{v}$, borrowing the first letter. As a result, in almost all systems describing changes over time, dynamics prefer to use expressions like $$ \\dot{v} = f(v) $$ instead of $v '$. The point is that the concern to clearly and neatly represent \u0026lsquo;what is being differentiated\u0026rsquo; can naturally arise even outside the context of partial differentiation.\nImplication of Multivariable Functions In the previous section, it was noted that $f '$ and $\\dot{f}$ could be distinguished just by their expressions, and especially in dynamical systems, even without the appearance of time $t$ in the expression, it could be implied as differentiation with respect to time due to universal conventions and context. Let\u0026rsquo;s discuss a bit more about the implicit information conveyed by expressions.\nReturning to partial differentiation, the reason why the notation $d$ and $\\partial$ doesn\u0026rsquo;t seem different is that there\u0026rsquo;s no difference in the partial derivatives they represent. For instance, if the derivative of $f$ with respect to $t$ is $g$, then that $g$ could be represented as $$ g = {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} $$ either way as $d$ or $\\partial$, and it wouldn\u0026rsquo;t matter much. Regardless of the notation, it\u0026rsquo;s differentiated with respect to $t$, and the \u0026lsquo;result\u0026rsquo; $g$ is the same. In fact, the implicit information given by $\\partial$ is not about $g$ but about $f$. When a function $h$ is said to be differentiated with respect to $H$ giving $x$, consider the following two expressions:\nWithout $\\partial$: It seems $\\displaystyle h = {{ d H } \\over { d x }} \\implies$ $H$ is differentiated to $h$.\rWith $\\partial$: Why is it only this one? There must be some $y$, so it should be $H = H (x , y)$.\rIn other words, the notation $\\partial$ itself implies that the given function is a multivariable function. Often, the first serious encounter with partial differentiation is usually through partial differential equations, and in equations like $$ {{ \\partial u } \\over { \\partial t }} = {{ \\partial u } \\over { \\partial x^{2} }} $$ we are not interested in the first-order partial derivative $u_{t}$ of $u$ with respect to $t$, nor the second-order partial derivative $u_{xx}$ of $u$ with respect to $x$, but what function $u = u (t,x)$ of $t$ and $x$ satisfies them being equal. From this perspective, using $\\partial$ in partial differential equations is justifiable and natural.\nOn the other hand, such conventions being widely accepted also changes the meaning of $d$ itself. If a function isn\u0026rsquo;t multivariable, it\u0026rsquo;s pointless to differentiate it with $\\partial$, so if the derivative expression uses $d$, it implies that it\u0026rsquo;s not a multivariable function. For instance, if we fix the location of a bivariate function $u = u (t,x)$ to a point and set $u = u \\left( t , x_{0} \\right)$, then $$ \\left. {{ \\partial u } \\over { \\partial t }} \\right|_{x = x_{0} } = {{ d u } \\over { d t }} = \\dot{u} $$ such an expression makes excellent use of the implicit information transmission of $\\partial$ and $d$. This goes beyond just a difference in expression and influences the way we handle equations, leading to ideas like transforming partial differential equation problems into relatively simple ordinary differential equations to solve them.\n✅ To Avoid Confusion in Total Differentiation $$ df = \\frac{ \\partial f}{ \\partial x_{1} }dx_{1} + \\frac{ \\partial f}{ \\partial x_{2} }dx_{2} + \\cdots + \\frac{ \\partial f}{ \\partial x_{n} }dx_{n} $$ For a multivariable function $f : \\mathbb{R}^{n} \\to \\mathbb{R}$, the total differentiation used in fields like mathematical physics is often represented as above, and to write it more intuitively when $n = 3$, we only write $t,x,y,z$ like this, assuming $x,y,z$ are independent. $$ df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz $$ At first glance, this expression might seem complicated with a mix of $d$ and $\\partial$, but by applying Leibniz\u0026rsquo;s legacy of \u0026lsquo;dividing both sides by $dt$ or $dx$,\u0026rsquo; we can see $$ \\begin{align*} df =\u0026amp; {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz \\\\ {{ d f } \\over { d t }} =\u0026amp; {{ \\partial f } \\over { \\partial x }} {{ d x } \\over { d t }} + {{ \\partial f } \\over { \\partial y }} {{ d y } \\over { d t }} + {{ \\partial f } \\over { \\partial z }} {{ d z } \\over { d t }} \\\\ {{ d f } \\over { d x }} =\u0026amp; {{ \\partial f } \\over { \\partial x }} {{ d x } \\over { d x }} + {{ \\partial f } \\over { \\partial y }} {{ d y } \\over { d x }} + {{ \\partial f } \\over { \\partial z }} {{ d z } \\over { d x }} = {{ \\partial f } \\over { \\partial x }} \\end{align*} $$ that it clearly represents both the meaning of differentiating $f$ with respect to $t$ and partial differentiating with respect to $x$. This shows how useful the form of total differentiation can be in handling equations, and if we eliminate $\\partial$ altogether and unify it with $d$ to rewrite it, we get $$ df = {{ d f } \\over { d x }} dx + {{ d f } \\over { d y }} dy + {{ d f } \\over { d z }} dz $$ Of course, Leibniz\u0026rsquo;s differential notation is incredibly intuitive when dealing with numerator and denominator like fractions, but if you\u0026rsquo;re reading this, you should know not to treat $dx$, $dy$, or $dz$ so carelessly. Despite this, your inner instincts will scream to simplify it like this. $$ \\begin{align*} df =\u0026amp; {{ d f } \\over { dx }} dx + {{ d f } \\over { dy }} dy + {{ d f } \\over { dz }} dz \\\\ \\overset{?}{=} \u0026amp; {{ d f } \\over { \\cancel{dx} }} \\cancel{dx} + {{ d f } \\over { \\cancel{dy} }} \\cancel{dy} + {{ d f } \\over { \\cancel{dz} }} \\cancel{dz} \\\\ =\u0026amp; df + df + df \\\\ \\overset{???}{=}\u0026amp; 3 df \\end{align*} $$ This disaster can be seen as a circular argument caused by overlooking what conditions make $d$ equal to $\\partial$. The progression that casually assumes \u0026rsquo;eliminate $\\partial$ and unify with $d$ to rewrite\u0026rsquo; is too bold, especially since the very thought of replacing $\\partial$ with $d$ came from $$ df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz \\implies {{ d f } \\over { d x }} = {{ \\partial f } \\over { \\partial x }} \\implies d \\equiv \\partial $$ assuming $x,y,z$ are independent. While tampering with $df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz$, which forms the basis for $d \\equiv \\partial$, it\u0026rsquo;s natural that some form of error would occur. For $d$ and $\\partial$ to be equal, as assumed in the example, either the variables of a multivariable function must be independent, or some special condition or remarkable theorem must truly equate $d$ and $\\partial$.\nFrom the considerations so far, we can summarize that the reason for using $d$ instead of $\\partial$ for partial differentiation is because they are indeed different. All the examples we\u0026rsquo;ve seen where $d$ and $\\partial$ were the same always implicitly assume certain conditions. Within those good assumptions, $\\partial$ might essentially be the same as $d$, but that doesn\u0026rsquo;t mean we necessarily have to rewrite $\\partial$ as $d$.\n❌ Treating Variables Other Than the Differentiated One as Constants? To put it bluntly, this is a wrong answer.\nMore precisely, the causal relationship explaining the phenomenon is reversed. For example, if $f(t,x) = \\left( t^{2} + x^{2} \\right)$, then $\\partial t$ is not formally treating variables other than $t$ as constants because of $$ {{ \\partial f } \\over { \\partial t }} = 2t + 0 = 2t = {{ d f } \\over { d t }} $$ but, as seen in the previous section, it\u0026rsquo;s because of the assumption $\\displaystyle {{ dx } \\over { dt }} = 0$ that $$ \\begin{align*} \u0026amp; df = {{ \\partial f } \\over { \\partial t }} dt + {{ \\partial f } \\over { \\partial x }} dx \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} {{ dt } \\over { dt }} + {{ \\partial f } \\over { \\partial x }} {{ dx } \\over { dt }} \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} \\cdot 1 + {{ \\partial f } \\over { \\partial x }} \\cdot 0 \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} \\end{align*} $$ holds. Partial differentiation $\\partial$ itself didn\u0026rsquo;t produce the result $\\displaystyle {{ dx } \\over { dt }} = 0$; it\u0026rsquo;s the cause $\\displaystyle {{ dx } \\over { dt }} = 0$ that led to the result $\\partial \\equiv d$. Thus, the explanation that \u0026lsquo;partial differentiation treats variables other than the differentiated one as constants\u0026rsquo; gives a misleading impression and misconception that partial differentiation $\\partial$ is somehow a more powerful operator than ordinary differentiation $d$. Moreover, if $x$ were treated as a constant, it should disappear after differentiation with respect to $t$, but as can be simply seen with $f(t,x) = t^{2} + x^{2} + 2tx$, $\\dfrac{\\partial f}{\\partial t}$ remains a bivariate function with variable $(t,x)$.\nThe persistence of this fallacy is due to its plausibility. Practically, when assuming relationships between variables like $x = x(t)$, there\u0026rsquo;s no need to use the expression \u0026lsquo;differentiate with respect to $t$\u0026rsquo; in the first place, as according to the chain rule, $$ \\begin{align*} {{ d f } \\over { d t }} =\u0026amp; {{ d } \\over { d t }} \\left( t^{2} + x^{2} \\right) \\\\ =\u0026amp; 2t + {{ d x^{2} } \\over { d x }} {{ dx } \\over { dt }} \\\\ =\u0026amp; 2t + 2x \\dot{x} \\end{align*} $$ the equation can be unfolded without ambiguity right from the start. At least in this example, $f = f(t,x)$ is essentially the same as a univariate function like $f = f(t)$, or it\u0026rsquo;s unnecessarily complex, and eventually, textbooks only include cases that are clean, independent, and yet still multivariable. Typically, people study with clean examples, time passes, familiarity with partial differentiation grows, incorrect intuitions settle, and everyone else does the same. However, what\u0026rsquo;s wrong is wrong. Merely changing the notation of differentiation can\u0026rsquo;t arbitrarily alter the dependencies of the given function.\nhttps://math.stackexchange.com/a/2000353/459895\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2573,"permalink":"https://freshrimpsushi.github.io/en/posts/2573/","tags":null,"title":"Why Notation of Partial Differential is Different?"},{"categories":"줄리아","contents":"Overview When drawing figures in Julia, to apply a title to all subplots, one should use plot_title instead of title1. This is because the arguments of the outermost plot() function, in the case of a plot with subplots like\nplot(\rplot1, plot2, ...\r) inherit properties to the inner subplots. To clearly distinguish between them, title and plot_title are used separately.\nCode plot(p1, p2, title = \u0026#34;Two Plots\u0026#34;) As you can see, using title = \u0026quot;Two Plots\u0026quot; applies the title to all subplots.\nplot(p1, p2, plot_title = \u0026#34;Two Plots\u0026#34;) plot_title = \u0026quot;Two Plots\u0026quot; applies just one title to the entire figure, as can be seen.\nEntire code using Plots\rp1 = scatter(rand(100))\rp2 = histogram(rand(100))\rplot(p1, p2, title = \u0026#34;Two Plots\u0026#34;)\rplot(p1, p2, plot_title = \u0026#34;Two Plots\u0026#34;) Environment OS: Windows julia: v1.8.5 https://stackoverflow.com/a/69713616/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2572,"permalink":"https://freshrimpsushi.github.io/en/posts/2572/","tags":null,"title":"How to Add a Main Title in Julia Subplots"},{"categories":"줄리아","contents":"Overview In Julia, there are ways to remove color bars, axes, ticks, grids, etc., but these involve graphic elements, so it\u0026rsquo;s not possible to cleanly remove numbers alone. You must use an option called formatter.\nformatter = (_...) -\u0026gt; \u0026quot;\u0026quot; By giving the option formatter = (_...) -\u0026gt; \u0026quot;\u0026quot; to the plot() function, it\u0026rsquo;s done.\nusing Plots\rx = rand(10)\ry = rand(10)\rplot(\rplot(x,y)\r,plot(x,y, formatter = (_...) -\u0026gt; \u0026#34;\u0026#34;)\r) In the image above, the left is a plain picture, and the right is a picture with all values removed. Originally, formatter is not just used this way; it has much more functionality. Briefly explained, it applies a given function to the values that should have been displayed in the original image. In the example above, a lambda expression (_...) -\u0026gt; \u0026quot;\u0026quot; is received, and returns an empty string for whatever numerical value comes in, thus removing the axis values1.\nxformatter, yformatter Of course, there are xformatter and yformatter which can be specified for each axis. If you want to remove only the x-axis, pass (_...) -\u0026gt; \u0026quot;\u0026quot; to yformatter, and vice versa for the y-axis.\nEnvironment OS: Windows julia: v1.8.5 foreground_color_text = false By inputting foreground_color_text = false as a keyword for the plot() function, it\u0026rsquo;s done. Although it\u0026rsquo;s a keyword for specifying the color of the tick values (names), entering false results in the values not being output at all.\nx_foreground_color_text and y_foreground_color_text can be specified for each axis respectively.\nusing Plots\rplot(\rplot(rand(10)),\rplot(rand(10), foreground_color_text = false)\r) Environment OS: Windows11 Version: Julia 1.9.3, Plots v1.39.0 https://stackoverflow.com/questions/74842089/remove-only-axis-values-in-plot-julia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2570,"permalink":"https://freshrimpsushi.github.io/en/posts/2570/","tags":null,"title":"How to Remove Axis Values in Julia Plots"},{"categories":"줄리아","contents":"Overview To use Finite Differences in Julia, more specifically to calculate the coefficients of finite differences, it is advisable to use FiniteDifferences.jl1. In cases where there is susceptibility to noise, it’s possible to use Total Variation Regularized Numerical Differentiation, known as TVDiff, implemented in NoiseRobustDifferentiation.jl.\nCode FiniteDifferenceMethod() julia\u0026gt; f′ = FiniteDifferenceMethod([-2, 0, 5], 1)\rFiniteDifferenceMethod:\rorder of method: 3\rorder of derivative: 1\rgrid: [-2, 0, 5]\rcoefficients: [-0.35714285714285715, 0.3, 0.05714285714285714]\rjulia\u0026gt; typeof(f′)\rFiniteDifferences.UnadaptedFiniteDifferenceMethod{3, 1} The example above calculates the first-order derivative using the middle point of the function, the second point from the left, and the fifth point from the right, providing the weights of these three points. The basic thing that FiniteDifferenceMethod() provides is these coefficients.\njulia\u0026gt; propertynames(f′)\r(:grid, :coefs, :coefs_neighbourhood, :condition, :factor, :max_range, :∇f_magnitude_mult, :f_error_mult)\rjulia\u0026gt; f′.grid\r3-element StaticArraysCore.SVector{3, Int64} with indices SOneTo(3):\r-2\r0\r5\rjulia\u0026gt; f′.coefs\r3-element StaticArraysCore.SVector{3, Float64} with indices SOneTo(3):\r-0.35714285714285715\r0.3\r0.05714285714285714 The properties to be used in the structure primarily include .grid and .coefs.\njulia\u0026gt; f′(sin, π/2)\r-1.2376571459669071e-11\rjulia\u0026gt; f′(cos, π/2)\r-1.0000000000076525 When the function itself is given instead of data, it\u0026rsquo;s perfectly fine to write it directly as a function form as seen above.\n_fdm() central_fdm(2, 1)\rcentral_fdm(3, 2)\rbackward_fdm(4, 1)\rforward_fdm(5, 1) If you need a well-known FDM without customizations, it\u0026rsquo;s more convenient to use the function already provided as seen above. The first argument depends on how many points will be used according to the type of the function, and the second argument $n$ determines that it will calculate the $n$ derivative. Obviously, when the first argument of central_fdm() is given as an odd number, the coefficient of the central point is definitely 0.\nFull Code using FiniteDifferences\rf′ = FiniteDifferenceMethod([-2, 0, 5], 1)\rtypeof(f′)\rpropertynames(f′)\rf′.grid\rf′.coefs\rf′(sin, π/2)\rf′(cos, π/2)\rcentral_fdm(2, 1)\rcentral_fdm(3, 2)\rbackward_fdm(4, 1)\rforward_fdm(5, 1) Environment OS: Windows julia: v1.8.5 https://github.com/JuliaDiff/FiniteDifferences.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2568,"permalink":"https://freshrimpsushi.github.io/en/posts/2568/","tags":null,"title":"How to Use Finite Difference in Julia"},{"categories":"줄리아","contents":"Overview In Julia, the Interpolations.jl package is used for numerical interpolation1. Be cautious not to confuse it with the interpolation method used when printing the value of a variable in Julia[../2041].\nCode Interpolate() julia\u0026gt; y = rand(10)\r10-element Vector{Float64}:\r0.8993801321974316\r0.12988982511901515\r0.49781160399025925\r0.22555299914088356\r0.4848674643768577\r0.6089318286915111\r0.10444895196527337\r0.5921775799940143\r0.2149546302906653\r0.32749334953170317\rjulia\u0026gt; f = interpolate(y, BSpline(Linear()));\rjulia\u0026gt; f(1.2)\r0.7454820707817483\rjulia\u0026gt; f(0.1)\rERROR: BoundsError: attempt to access 10-element interpolate(::Vector{Float64}, BSpline(Linear())) with element type Float64 at index [0.1] Basically, as shown above, you can receive the interpolation function f=$f$ itself by giving data and use it. The example shows how a value (0.745) around 1.2 between the 1st (0.899) and 2nd (0.129) given points is interpolated. Refer to the API section of the official documentation to determine the specific method to use2.\nlinear_interpolation() x = 1:10\rf_lin = linear_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_lin.(1:0.01:10), label = \u0026#34;Linear\u0026#34;) cubic_spline_interpolation() f_cub = cubic_spline_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_cub.(1:0.01:10), label = \u0026#34;Cubic\u0026#34;) constant_interpolation() f_con = constant_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_con.(1:0.01:10), label = \u0026#34;Constant\u0026#34;) Full Code using Interpolations, Plots\ry = rand(10)\rf = interpolate(y, BSpline(Linear()));\rf(1.2)\rf(0.1)\rx = 1:10\rf_lin = linear_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_lin.(1:0.01:10), label = \u0026#34;Linear\u0026#34;)\rf_cub = cubic_spline_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_cub.(1:0.01:10), label = \u0026#34;Cubic\u0026#34;)\rf_con = constant_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_con.(1:0.01:10), label = \u0026#34;Constant\u0026#34;) Environment OS: Windows julia: v1.8.5 https://github.com/JuliaMath/Interpolations.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://juliamath.github.io/Interpolations.jl/stable/api/#Public-API\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2566,"permalink":"https://freshrimpsushi.github.io/en/posts/2566/","tags":null,"title":"Numerical Interpolation in Julia"},{"categories":"줄리아","contents":"Overview In Julia, the diff() function is provided to calculate differences1. It\u0026rsquo;s also possible to use the circshift() function to easily create a similar effect, but dealing with end points and such can be somewhat inconvenient, so knowing how to use diff() can be much more comfortable. It can be used almost in the same way as the diff() functions in R and MATLAB, however, unlike these, Julia does not specifically implement second-order differences (calculating the difference twice).\nCode Basic Usage julia\u0026gt; x = rand(0:9, 12)\r12-element Vector{Int64}:\r3\r1\r9\r7\r1\r0\r6\r5\r3\r2\r9\r9 For example, with an array like the one above, simply applying diff() calculates the difference between preceding and following elements, resulting in the output below. Notice that the size of the array has decreased by exactly 1.\njulia\u0026gt; diff(x)\r11-element Vector{Int64}:\r-2\r8\r-2\r-6\r-1\r6\r-1\r-2\r-1\r7\r0 Multidimensional Arrays julia\u0026gt; X = reshape(x, 3, :)\r3×4 Matrix{Int64}:\r3 7 6 2\r1 1 5 9\r9 0 3 9\rjulia\u0026gt; diff(X)\rERROR: UndefKeywordError: keyword argument dims not assigned\rStacktrace:\r[1] diff(a::Matrix{Int64})\r@ Base .\\multidimensional.jl:997\r[2] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\DataDrivenModel\\REPL.jl:7 For instance, if you have a multidimensional array like the one above, applying diff() directly will result in an error. This is because the direction in which to compute the difference is not specified. As in the one-dimensional case, you must specify the dims argument to determine in which dimension to calculate the difference. Pay attention to the fact that the length decreases by one in the direction in which the difference was taken.\njulia\u0026gt; diff(X, dims = 1)\r2×4 Matrix{Int64}:\r-2 -6 -1 7\r8 -1 -2 0\rjulia\u0026gt; diff(X, dims = 2)\r3×3 Matrix{Int64}:\r4 -1 -4\r0 4 4\r-9 3 6 Complete Code x = rand(0:9, 12)\rdiff(x)\rX = reshape(x, 3, :)\rdiff(X)\rdiff(X, dims = 1)\rdiff(X, dims = 2) Environment OS: Windows julia: v1.8.5 https://docs.julialang.org/en/v1/base/arrays/#Base.diff\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2564,"permalink":"https://freshrimpsushi.github.io/en/posts/2564/","tags":null,"title":"Calculating the Difference of Arrays in Julia"},{"categories":"줄리아","contents":"Overview Though Julia does not natively support Circular Arrays, it essentially allows for such functionality by providing the circshift() function, which pushes or pulls elements circularly1. It\u0026rsquo;s not particularly difficult to write this function yourself, but knowing it obviates the need. This function can be used almost exactly like the circshift() in MATLAB.\nCode This function has been introduced in the post about how to translate arrays in parallel as well.\nBasic Usage julia\u0026gt; circshift(1:4, 1)\r4-element Vector{Int64}:\r4\r1\r2\r3\rjulia\u0026gt; circshift(1:4, -1)\r4-element Vector{Int64}:\r2\r3\r4\r1 circshift() basically takes an integer as the second argument to push or pull elements. In the example above, positive integers cause a backward (downward) shift, while negative integers cause a forward (upward) shift.\nMultidimensional Arrays julia\u0026gt; ca = reshape(1:20, 5, :)\r5×4 reshape(::UnitRange{Int64}, 5, 4) with eltype Int64:\r1 6 11 16\r2 7 12 17\r3 8 13 18\r4 9 14 19\r5 10 15 20 Given a multidimensional array like the one above, you specify a tuple of the same dimension to determine how much to push or pull each dimension.\njulia\u0026gt; circshift(ca, (0,1))\r5×4 Matrix{Int64}:\r16 1 6 11\r17 2 7 12\r18 3 8 13\r19 4 9 14\r20 5 10 15\rjulia\u0026gt; circshift(ca, (-1,0))\r5×4 Matrix{Int64}:\r2 7 12 17\r3 8 13 18\r4 9 14 19\r5 10 15 20\r1 6 11 16\rjulia\u0026gt; circshift(ca, (-1,1))\r5×4 Matrix{Int64}:\r17 2 7 12\r18 3 8 13\r19 4 9 14\r20 5 10 15\r16 1 6 11 Complete Code circshift(1:4, 1)\rcircshift(1:4, -1)\rca = reshape(1:20, 5, :)\rcircshift(ca, (0,1))\rcircshift(ca, (-1,0))\rcircshift(ca, (-1,1)) Environment OS: Windows julia: v1.8.5 https://docs.julialang.org/en/v1/base/arrays/#Base.circshift\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2562,"permalink":"https://freshrimpsushi.github.io/en/posts/2562/","tags":null,"title":"How to Use Circular Arrangements in Julia"},{"categories":"줄리아","contents":"Code 1 No need for a lengthy description, it literally shows what the marker and line styles look like in reality.\nlinesytle Choose one from [:auto, :solid, :dash, :dot, :dashdot, :dashdotdot].\nshape Choose one from [:none, :auto, :circle, :rect, :star5, :diamond, :hexagon, :cross, :xcross, :utriangle, :dtriangle, :rtriangle, :ltriangle, :pentagon, :heptagon, :octagon, :star4, :star6, :star7, :star8, :vline, :hline, :+, :x].\nFull Code ▷code1◁\nEnvironment OS: Windows julia: v1.8.5 Plots v1.38.5 https://docs.juliaplots.org/latest/generated/attributes_series/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2560,"permalink":"https://freshrimpsushi.github.io/en/posts/2560/","tags":null,"title":"List of Markers and Line Styles in Julia"},{"categories":"줄리아","contents":"Code To insert a regression line in the scatter plot of Julia, simply use the option smooth = true.\nusing Plots\rx = rand(100)\rscatter(x, 2x .+ 0.1randn(100), smooth = true)\rsavefig(\u0026#34;plot.svg\u0026#34;) Environment OS: Windows julia: v1.8.3 Plots v1.38.5 ","id":2558,"permalink":"https://freshrimpsushi.github.io/en/posts/2558/","tags":null,"title":"Drawing a Regression Line on a Julia Plot"},{"categories":"줄리아","contents":"Overview In Julia, the most frequently used purpose of the ... splat is explained as the method of passing optional arguments. Basically, it uses the method of applying the splat operator to the tuple after determining in advance what options to put into which arguments, in the form of a named tuple.\nCode Passing to Multiple Functions args1 = (; dims = 1)\nThe named tuple args1 above can be commonly used for all functions having an optional argument called dims. In the following example, sum() and minimum() are completely different functions, but they were applied regardless of the type of function as they both have dims.\njulia\u0026gt; sum(rand(100,100); args1...)\r1×100 Matrix{Float64}:\r47.0704 45.7637 44.4513 48.2325 50.5745 51.9176 … 49.9548 47.6825 50.7284 50.0861 50.0168 50.5116\rjulia\u0026gt; minimum(rand(100,100); args1...)\r1×100 Matrix{Float64}:\r0.00702003 0.0163299 0.00665818 0.0174564 0.00589048 … 0.002967 0.00460205 0.0116248 0.0114521 0.0698425 Passing Multiple Arguments args2 = (; dims = 2, rev = true)\nThe named tuple args2 above includes the optional argument rev in addition to dims. In the following example, the sort() function reflected both options well and returned the computation result regardless of the input data.\njulia\u0026gt; sort(rand(0:9, 3,3); args2...)\r3×3 Matrix{Int64}:\r9 4 4\r6 5 2\r8 0 0\rjulia\u0026gt; sort(rand(3,3); args2...)\r3×3 Matrix{Float64}:\r0.438682 0.211154 0.108741\r0.72113 0.445214 0.00910109\r0.971441 0.666732 0.0227372 Entire Code args1 = (; dims = 1)\rsum(rand(100,100); args1...)\rminimum(rand(100,100); args1...)\rargs2 = (; dims = 2, rev = true)\rsort(rand(0:9, 3,3); args2...)\rsort(rand(3,3); args2...) Environment OS: Windows julia: v1.8.3 ","id":2554,"permalink":"https://freshrimpsushi.github.io/en/posts/2554/","tags":null,"title":"Tips for Passing Optional Arguments through the Julia Splatt Operator"},{"categories":"줄리아","contents":"Overview In Julia, ... is called the splat operator. It is usefully employed when using functions or defining arrays1. This operator isn\u0026rsquo;t exclusive to Julia, but it\u0026rsquo;s defined in a more intuitive way compared to other languages, making it exceptionally easy to learn and understand. From personal experience, using ... seems to bring some sort of enlightenment regarding Julia programming.\nCode Function Input Primarily, ... is appended after an array or generator and unfolds every element from the preceding container/iterator as it appears.\njulia\u0026gt; min([1,2,3,4,5,6,7,8,9,10])\rERROR: MethodError: no method matching min(::Vector{Int64})\rjulia\u0026gt; min(1,2,3,4,5,6,7,8,9,10)\r1 For instance, Julia’s min() function acts as a reducer, so you cannot pass an array as a whole; instead, you must directly provide multiple numbers as arguments. Naturally, as arrays grow larger, it becomes cumbersome to manually unfold and list each element, and using ... allows for these elements to be conveniently incorporated.\njulia\u0026gt; min(1:10)\rERROR: MethodError: no method matching min(::UnitRange{Int64})\rjulia\u0026gt; min((1:10)...)\r1 Of course, there actually exists a minimum() function that can be used on arrays, making this approach somewhat unnecessary.\nArray Definition julia\u0026gt; [(1:10)]\r1-element Vector{UnitRange{Int64}}:\r1:10 The array defined above is an array of unit ranges, which makes directly accessing elements somewhat tedious. If time permits, one could manually insert numbers from 1 to 10, but employing the splat operator allows for an easy definition like so:\njulia\u0026gt; [(1:10)...]\r10-element Vector{Int64}:\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10 There are alternatives such as the collect() function, but this method is appealing for its neat and ingenious representation. However, it’s not highly recommended in terms of speed, so avoid overusing ....\njulia\u0026gt; [eachrow(rand(3,3))...]\r3-element Vector{SubArray{Float64, 1, Matrix{Float64}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}}:\r[0.6695368164913422, 0.69695509795356, 0.12084083239135612]\r[0.6833475867141307, 0.5368141950494666, 0.7877252857572066]\r[0.2810163716135018, 0.04317485597011517, 0.44214186775440534] ... becomes interesting when used with generators as seen above. eachrow() returns a generator of vectors, each corresponding to a row in the matrix. Through the splat operator, these vectors are inserted into array notation [], creating a vector of vectors.\nFull code min([1,2,3,4,5,6,7,8,9,10])\rmin(1,2,3,4,5,6,7,8,9,10)\rmin(1:10)\rmin((1:10)...)\r[(1:10)]\r[(1:10)...]\r[eachrow(rand(3,3))...] Environment OS: Windows julia: v1.8.3 https://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2552,"permalink":"https://freshrimpsushi.github.io/en/posts/2552/","tags":null,"title":"Julia's Splat Operator"},{"categories":"줄리아","contents":"Overview As with many programming languages, in Julia, English is written in ASCII code and characters like Chinese and Korean are written in Unicode. The trouble, unlike with other languages, is that dealing with these strings is quite tricky, which is intended for performance reasons1, so one has no choice but to bear with it and use them as they are.\nCode julia\u0026gt; str1 = \u0026#34;English\u0026#34;\r\u0026#34;English\u0026#34;\rjulia\u0026gt; str2 = \u0026#34;日本語\u0026#34;\r\u0026#34;日本語\u0026#34;\rjulia\u0026gt; str3 = \u0026#34;한국어\u0026#34;\r\u0026#34;한국어\u0026#34; For example, let\u0026rsquo;s say the strings are given as above.\njulia\u0026gt; str1[2:end]\r\u0026#34;nglish\u0026#34; str1 is a simple English string and, because it\u0026rsquo;s ASCII code, it can be sliced like accessing a regular array as above.\njulia\u0026gt; str2[2:end]\rERROR: StringIndexError: invalid index [2], valid nearby indices [1]=\u0026gt;\u0026#39;日\u0026#39;, [4]=\u0026gt;\u0026#39;本\u0026#39;\rStacktrace:\r[1] string_index_err(s::String, i::Int64)\r@ Base .\\strings\\string.jl:12\r[2] getindex(s::String, r::UnitRange{Int64})\r@ Base .\\strings\\string.jl:266\r[3] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\population_dynamics\\REPL.jl:6 However, str2 is written in Unicode because of the Chinese characters, and as shown, raises an index error. Judging by the error message, one can guess that the index for the second character is not 2 but 4, and indeed, starting indexing at 4 slices it as originally intended.\njulia\u0026gt; str2[4:end]\r\u0026#34;本語\u0026#34; This applies to Korean as well in the same way. There\u0026rsquo;s no reason for it to be different because it\u0026rsquo;s also Unicode.\njulia\u0026gt; str3[4:end]\r\u0026#34;국어\u0026#34;\rjulia\u0026gt; str3[6]\rERROR: StringIndexError: invalid index [6], valid nearby indices [4]=\u0026gt;\u0026#39;국\u0026#39;, [7]=\u0026gt;\u0026#39;어\u0026#39;\rStacktrace:\r[1] string_index_err(s::String, i::Int64)\r@ Base .\\strings\\string.jl:12\r[2] getindex_continued(s::String, i::Int64, u::UInt32)\r@ Base .\\strings\\string.jl:237\r[3] getindex(s::String, i::Int64)\r@ Base .\\strings\\string.jl:230\r[4] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\population_dynamics\\REPL.jl:9\rjulia\u0026gt; str3[7]\r\u0026#39;어\u0026#39;: Unicode U+C5B4 (category Lo: Letter, other) Trick julia\u0026gt; String(collect(str3)[2:3])\r\u0026#34;국어\u0026#34; A somewhat convenient way to use it is to unravel the string into an array of characters using collect(), slice it, and then reassemble it into a string like above.\nEnvironment OS: Windows julia: v1.8.3 https://discourse.julialang.org/t/weird-string-slicing-in-korean/92252/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2550,"permalink":"https://freshrimpsushi.github.io/en/posts/2550/","tags":null,"title":"Slicing Only a Part of Unicode Strings in Julia"},{"categories":"줄리아","contents":"Overview In the StatsPlots package of Julia, the @df macro allows omitting the repeatedly mentioned dataframe name when plotting1. The syntax for using the macro, when using column a of dataframe X, is to specify which dataframe to use with @df X, followed immediately by passing the argument a as a symbol :a in the scope that follows, writing it as plot (:a). In summary, the code is written as @df X plot(:a).\nCode Below is a scatter plot using SepalLength and SepalWidth from the Iris data.\nThe following code scatter(iris.SepalLength, iris.SepalWidth) and @df iris scatter(:SepalLength, :SepalWidth) are equivalent.\nusing RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rusing StatsPlots\rscatter(iris.SepalLength, iris.SepalWidth)\r@df iris scatter(:SepalLength, :SepalWidth) Environment OS: Windows julia: v1.8.3 StatsPlots v0.15.4 https://github.com/JuliaPlots/StatsPlots.jl#original-author-thomas-breloff-tbreloff-maintained-by-the-juliaplots-members\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2548,"permalink":"https://freshrimpsushi.github.io/en/posts/2548/","tags":null,"title":"Omitting DataFrame Names in Julia StatsPlots with Macro @df"},{"categories":"줄리아","contents":"Overview Introducing the function include() which executes Julia code itself to use functions from another file. In MATLAB, if it\u0026rsquo;s in the same directory, it tends to automatically find the function, so some people think this process is hard. There is a way to properly modularize and export, but1 it is not recommended for beginners who urgently need functionality because it is difficult and complicated. It\u0026rsquo;s not too late to learn about modularization after making a package yourself or when the scale of the program has become unmanageable.\nGuide Let\u0026rsquo;s say you want to run the baz() function in the foo/bar.jl file from main.jl as shown above. As you can see in the screenshot, you just write Julia code normally without using anything like modules.\nNow, the result of the execution by giving the path with include() is as follows.\nThe reason why 23 is printed as a result of the include() execution is that there was a value assignment like y = 23 at the bottom of the bar.jl file. As you can see, not only functions can be transferred, but also variables, and it\u0026rsquo;s possible to load data and print logs since it executes the file itself.\nEnvironment OS: Windows julia: v1.8.3 https://docs.julialang.org/en/v1/manual/modules/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2544,"permalink":"https://freshrimpsushi.github.io/en/posts/2544/","tags":null,"title":"How to Use Functions Defined in Other Files in Julia"},{"categories":"줄리아","contents":"Code using Plots\rx, y = rand(100), rand(100) Given the data above, depending on whether the data is continuous or categorical, the shape of the plot and the method of plotting differ.\nContinuous scatter(marker_z=) z = x + y\rscatter(x, y, marker_z = z) Categorical scatter(group=) 1 team = rand(\u0026#39;A\u0026#39;:\u0026#39;C\u0026#39;, 100)\rscatter(x, y, group = team) Environment OS: Windows julia: v1.8.3 https://stackoverflow.com/a/60846501/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2537,"permalink":"https://freshrimpsushi.github.io/en/posts/2537/","tags":null,"title":"How to Color Markers in a Julia Fractal"},{"categories":"통계적분석","contents":"Model Ordinary Kriging In Spatial Data Analysis, for a Random Field $\\mathbf{Y} = \\left( Y \\left( s_{1} \\right) , \\cdots , Y \\left( s_{n} \\right) \\right)$ following a Multivariate Normal Distribution with Mean $\\mu \\in \\mathbb{R}$ and Covariance Matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$, the value estimated for a new site $s_{0}$ using the model $$ \\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon $$ is called the Ordinary Kriging Estimate. The act of estimating using this model is also referred to as Kriging.\n$\\mathbf{1} = (1 , \\cdots , 1)$ is a 1-vector with all elements being $1$. $N_{n} \\left( \\mathbf{0} , \\Sigma \\right)$ denotes the Multivariate Normal Distribution. Explanation Etymology Kriging is named after the pioneer Daine G. Krige, and the term has become a general verb. In statistics, terms like prediction, forecasting, fitting, estimation, and the concept of filling in \u0026rsquo;empty spaces\u0026rsquo; similar to Interpolation are often used to describe it, summing up all these concepts into the verb \u0026ldquo;to Krig.\u0026rdquo;\nA distinguishing feature of Kriging, as opposed to applied mathematics, computer algorithms, and machine learning techniques, is its statistical approach of considering not just the mean (point estimate) but also the variance. For example, the Kriging estimate\u0026rsquo;s variance would be high in areas where each point has high variance, and low between points of low variance. This can be applied in choosing locations for data observation sites; for instance, in measuring concentrations of particulate matter, the interest may not be in how to measure but where to measure, focusing on where the variance of measurements is highest.\nDependence The formula $$ \\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon \\qquad , \\text{where } \\varepsilon \\sim N_{n} \\left( 0, \\Sigma \\right) $$ reveals that in the model, our main interest is in $\\varepsilon$, unlike in regression or time series analysis. If $\\Sigma$ is a Diagonal Matrix, implying no dependence between observations, it essentially means there\u0026rsquo;s no spatial structure to analyze, negating the need for Kriging. In practice, $\\Sigma$ is determined through the Semivariogram Model as follows. $$ \\Sigma = \\sigma^{2} H \\left( \\phi \\right) + \\tau^{2} I $$ Here, $\\tau^{2}$ represents the \u0026ldquo;Nugget Effect Variance\u0026rdquo; (the covariance seen regardless of distance in actual Data), and $I$ is the Identity Matrix.\nGeneralization The use of a generalized model for other independent variables in Kriging is called Universal Kriging. $$ \\mathbf{Y} = \\mathbf{X} \\beta + \\varepsilon $$\nFormula Given a Random Field $\\left\\{ Y (s_{k}) \\right\\}_{k=1}^{n}$ assumed to be an Intrinsic Stationary Spatial Process, and a new point to predict $s_{0}$, define the Matrix $\\Gamma \\in \\mathbb{R}^{n \\times n}$ with respect to the Variogram $2 \\gamma$ as $\\left( \\Gamma \\right)_{ij} := \\gamma \\left( s_{i} - s_{j} \\right)$, and the Vector $\\gamma_{0} \\in \\mathbb{R}^{n}$ as $\\left( \\gamma_{0} \\right)_{i} := \\left( \\gamma \\left( s_{0} - s_{i} \\right) \\right)$. The Best Linear Unbiased Predictor (BLUP) of $Y \\left( s_{0} \\right)$ is the Inner Product of $l$ and $\\mathbf{Y}$, given by $$ l^{T} \\mathbf{Y} = \\begin{bmatrix} l_{1} \u0026amp; \\cdots \u0026amp; l_{n} \\end{bmatrix} \\begin{bmatrix} Y \\left( s_{1} \\right)\\\\ Y \\left( s_{2} \\right)\\\\ \\vdots\\\\ Y \\left( s_{n} \\right) \\end{bmatrix} = \\sum_{k=1}^{n} l_{k} Y \\left( s_{k} \\right) $$ where the vector $l$ is specifically determined as follows. $$ l = \\Gamma^{-1} \\left( \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\right) $$\nDerivation 1 This section elaborates on the mathematical process of Kriging. Adding the assumption of a Gaussian Process to these formulas results in Ordinary Kriging.\nPart 1. Optimization Problem\nFor some constants $l_{1} , \\cdots , l_{n} , \\delta_{0} \\in \\mathbb{R}$, we aim to predict a new $Y \\left( s_{0} \\right)$ as a Linear Combination of existing data $$ \\hat{y} \\left( s_{0} \\right) = l_{1} y_{1} + \\cdots + l_{n} y_{n} + \\delta_{0} $$ which means finding the Optimal Solution $l_{1} , \\cdots , l_{n} , \\delta_{0}$ that Minimizes the Objective Function $$ E \\left[ Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) + \\delta_{0} \\right) \\right]^{2} $$\nDefinition of Intrinsic Stationarity: Consider a Spatial Process $\\left\\{ Y(s) \\right\\}_{s \\in D}$ in a fixed subset $D \\subset \\mathbb{R}^{r}$ of the Euclidean Space, comprising a set of Random Variables $Y(s) : \\Omega \\to \\mathbb{R}^{1}$. Specifically, let\u0026rsquo;s denote $n \\in \\mathbb{N}$ Sites as $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$, assuming that $Y(s)$ has a Variance for all $s \\in D$. If $\\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]$\u0026rsquo;s mean is $0$ and its variance depends only on $\\mathbf{h}$, then $\\left\\{ Y \\left( s_{k} \\right) \\right\\}$ is said to possess Intrinsic Stationarity. $$ \\begin{align*} E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 0 \\\\ \\text{Var} \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 2 \\gamma ( \\mathbf{h} ) \\end{align*} $$\nBy imposing the constraint $\\sum_{k} l_{k} = 1$, if $\\left\\{ Y \\left( s_{k} \\right) \\right\\}_{k=1}^{n}$ is intrinsically stationary, $$ \\begin{align*} \u0026amp; E \\left[ Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right) \\right] \\\\ =\u0026amp; E \\left[ \\sum_{k} l_{k} Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right) \\right] \\\\ =\u0026amp; \\sum_{k} l_{k} E \\left[ Y \\left( s_{0} \\right) - Y \\left( s_{k} \\right) \\right] \\\\ =\u0026amp; 0 \\end{align*} $$ we can ensure that. The Objective Function to be minimized becomes $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right]^{2} + \\delta_{0}^{2} $$ where $\\delta_{0}^{2}$ is irrelevant to the prediction. If the model is $\\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon$, $\\delta_{0}$ corresponds to $\\mu$, and it\u0026rsquo;s reasonable to set $\\delta_{0} = 0$ as $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right]^{2} $$ Setting $a_{0} = 1$ and $a_{k} = - l_{k}$, $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k=1}^{n} l_{k} Y \\left( s_{k} \\right) \\right]^{2} = E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2} $$ leads to solving the following optimization problem using the Lagrange Multiplier Method. $$ \\begin{matrix} \\text{Minimize} \u0026amp; \\displaystyle E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2} \\\\ \\text{subject to} \u0026amp; \\displaystyle \\sum_{k=0}^{n} a_{k} = 0 \\end{matrix} $$\nPart 2. Semivariogram $\\gamma$\nLet\u0026rsquo;s express $E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2}$ in terms of a semivariogram assumed to be computable from the data.\nDefinition of Semivariogram: Consider a Spatial Process $\\left\\{ Y(s) \\right\\}_{s \\in D}$ in a fixed subset $D \\subset \\mathbb{R}^{r}$ of the Euclidean Space, comprising a set of Random Variables $Y(s) : \\Omega \\to \\mathbb{R}^{1}$. Specifically, let\u0026rsquo;s denote $n \\in \\mathbb{N}$ sites as $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$, assuming that $Y(s)$ has a Variance for all $s \\in D$. The function defined as $2 \\gamma ( \\mathbf{h} )$ is called the Variogram. $$ 2 \\gamma ( \\mathbf{h} ) := E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]^{2} $$ Specifically, half of the Variogram $\\gamma ( \\mathbf{h} )$ is known as the Semivariogram.\nAssuming $\\gamma \\left( s_{i} - s_{j} \\right)$ as the semivariogram between two sites $s_{i}, s_{j}$, for any set $\\left\\{ a_{k} : k = 1 , \\cdots , n \\right\\} \\subset \\mathbb{R}$ satisfying $\\sum_{0=1}^{n} a_{k} = 0$, the following holds true. $$ \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) = - E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} $$ This is evident from the following derivation: $$ \\begin{align*} \u0026amp; \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} \\text{Var} \\left[ Y \\left( s_{i} \\right) - Y \\left( s_{j} \\right) \\right] \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left[ Y \\left( s_{i} \\right) - Y \\left( s_{j} \\right) \\right]^{2} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left( \\left[ Y \\left( s_{i} \\right) \\right]^{2} - 2 Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) + \\left[ Y \\left( s_{j} \\right) \\right]^{2} \\right) \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left( Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) \\right) \u0026amp; \\because \\text{cases of } i = j \\\\ =\u0026amp; - E \\sum_{i} \\sum_{j} a_{i} a_{j} Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) \\\\ =\u0026amp; - E \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\sum_{j} a_{j} Y \\left( s_{j} \\right) \\\\ =\u0026amp; - E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} \\end{align*} $$ Setting $\\gamma_{ij} = \\gamma \\left( s_{i} - s_{j} \\right)$ and $\\gamma_{0j} = \\gamma \\left( s_{0} - s_{j} \\right)$, our objective function becomes $$ \\begin{align*} \u0026amp; E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} l_{i} l_{j} \\gamma_{ij} + 2 \\sum_{i} l_{i} \\gamma_{0i} \\end{align*} $$ Applying the Lagrange Multiplier Method with the constraint $\\sum_{i} l_{i} = 1$ leads to $$ \\ - \\sum_{i} \\sum_{j} l_{i} l_{j} \\gamma_{ij} + 2 \\sum_{i} l_{i} \\gamma_{0i} - \\lambda \\sum_{i} l_{i} $$ Hence, differentiating with respect to $l_{i}$ yields $$ \\ - \\sum_{j} l_{j} \\gamma_{ij} + \\gamma_{0i} - \\lambda = 0 $$ which minimizes the objective function.\nPart 3. Optimal Solution\nNow, we derive the specific form of the optimal solution. Denote the element of matrix $\\Gamma \\in \\mathbb{R}^{n \\times n}$ as $\\gamma_{ij}$, meaning $\\left( \\Gamma \\right)_{ij} := \\gamma_{ij}$, and define vector $\\gamma_{0} \\in \\mathbb{R}^{n}$ as $\\left( \\gamma_{0} \\right)_{i} := \\gamma_{0i}$. If we set the vector of coefficients as $l := \\left( l_{1} , \\cdots , l_{n} \\right) \\in \\mathbb{R}^{n}$, then from Part 2, the derived equation can be expressed in the following matrix/vector form: $$ \\begin{align*} \u0026amp; - \\sum_{j} l_{j} \\gamma_{ij} + \\gamma_{0i} - \\lambda = 0 \\\\ \\implies \u0026amp; - \\Gamma l + \\gamma_{0} - \\lambda \\mathbf{1} = 0 \\\\ \\implies \u0026amp; \\Gamma l + \\lambda \\mathbf{1} = \\gamma_{0} \\end{align*} $$ Meanwhile, from the constraint, we have $$ \\sum_{i} l_{i} = 1 \\iff \\begin{bmatrix} 1 \u0026amp; \\cdots \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} l_{1} \\\\ \\vdots \\\\ l_{n} \\end{bmatrix} = 1 \\iff \\mathbf{1}^{T} l = 1 $$ which also gives us $\\mathbf{1}^{T} l = 1$. Here, $\\mathbf{x}^{T}$ represents the Transpose of $\\mathbf{x}$. First, considering $\\lambda$ alone, we have $$ \\begin{align*} 1 =\u0026amp; \\mathbf{1}^T l \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\Gamma l \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\left( \\gamma_{0} - \\lambda \\mathbf{1} \\right) \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} - \\lambda \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} \\end{align*} $$ which simplifies to $$ \\ - \\lambda = {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} $$ At this point, $l$ is essentially determined. $$ \\begin{align*} \u0026amp; \\Gamma l + \\lambda \\mathbf{1} = \\gamma_{0} \\\\ \\implies \u0026amp; \\Gamma l = \\gamma_{0} - \\lambda \\mathbf{1} \\\\ \\implies \u0026amp; \\Gamma l = \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\\\ \\implies \u0026amp; l = \\Gamma^{-1} \\left( \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\right) \\end{align*} $$\nThis concludes the derivation process for the optimal solution in Kriging, detailing how the estimates are calculated through the optimization of the objective function under given constraints, and how the semivariogram plays a central role in this estimation process.\n■\nBanerjee. (2015). Hierarchical Modeling and Analysis for Spatial Data(2nd Edition): p25, 40~41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2521,"permalink":"https://freshrimpsushi.github.io/en/posts/2521/","tags":null,"title":"Kringing in Spatial Data Analysis"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","id":null,"permalink":"https://freshrimpsushi.github.io/en/search/","tags":null,"title":"Search Results"}]