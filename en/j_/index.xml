<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>J_s on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/j_/</link><description>Recent content in J_s on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 11 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/j_/index.xml" rel="self" type="application/rss+xml"/><item><title>Meter</title><link>https://freshrimpsushi.github.io/en/posts/3702/</link><pubDate>Thu, 11 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3702/</guid><description>Definition1 The distance that light travels in $\dfrac{1}{299\ 792\ 458}$ seconds in a vacuum is defined as $1$ meters, and it is denoted by $\text{m}$. Explanation Conversely, the speed of light $c$ is $299\ 792\ 458$ meters per second. This is an exact figure, not an approximation. $$ c = 299\ 792\ 458\ \text{m/s} $$ It is the most popular unit of length and one of the SI base units.</description></item><item><title>Angstrom</title><link>https://freshrimpsushi.github.io/en/posts/3701/</link><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3701/</guid><description>Definition Ångström is one of the units of length and represents $10^{-10}$ meters. The symbol $\text{\AA}$ is used for it. $$ 1\ \text{\AA} = 10^{-10}\ \text{m} = 10^{-8}\ \text{cm} = 10^{-7}\ \text{mm} = 10^{-4}\ \mu\text{m} $$ Description It is named after the Swedish physicist Anders Ångström. It is mainly used to denote distances (lengths) in</description></item><item><title>Summary of PBS Job Commands</title><link>https://freshrimpsushi.github.io/en/posts/3699/</link><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3699/</guid><description>Overview PBS stands for Portable Batch System, which is software used to manage jobs on large computer clusters. PBS supports job scheduling, resource management, and job monitoring, enabling users to perform tasks efficiently on a cluster. Job Script A job script is a file defining the job to be submitted to PBS. Note that comments start with #, which is different from #! or #PBS. #!/bin/sh: Specifies the interpreter for</description></item><item><title>Volume Formula of a Parallelepiped</title><link>https://freshrimpsushi.github.io/en/posts/3698/</link><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3698/</guid><description>Formula The volume of a parallelepiped $V$ can be determined as follows: [1]: When the area of the base $A$ and the height $h$ are known, it is given by: $$ V = A \times h $$ [2]: When you know the three vectors $\mathbf{a}$, $\mathbf{b}$, $\mathbf{c}$ that form the parallelepiped in a coordinate space, it is given by: $$ V = |\mathbf{a} \cdot (\mathbf{b} \times \mathbf{c})| $$ Alternatively, it</description></item><item><title>Trapezoid</title><link>https://freshrimpsushi.github.io/en/posts/3697/</link><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3697/</guid><description>Definition A quadrilateral with one pair of opposite sides parallel to each other is called a trapezoid. Explanation According to the definition, if it&amp;rsquo;s a parallelogram, then it&amp;rsquo;s a trapezoid. Square $\implies$ Rectangle $\implies$ $\implies$ Rhombus $\implies$ Parallelogram $\implies$ Trapezoid</description></item><item><title>Parallelepiped</title><link>https://freshrimpsushi.github.io/en/posts/3696/</link><pubDate>Sat, 30 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3696/</guid><description>Definition Simple Definition A hexahedron in which all faces are parallelograms is called a parallelepiped. Linear Algebraic Definition For three distinct vectors $\mathbf{a}$, $\mathbf{b}$, $\mathbf{c}$ in a 3-dimensional coordinate space, the following set is called a parallelepiped. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} + \lambda_{3} \mathbf{c} : 0\leq \lambda_{1},\lambda_{2},\lambda_{3}\leq 1 \right\} $$ Explanation It is a 3-dimensional extension of a parallelogram. It is defined in the same way</description></item><item><title>Rhombus</title><link>https://freshrimpsushi.github.io/en/posts/3695/</link><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3695/</guid><description>Definition Simple Definition A quadrilateral with all sides of equal length is called a rhombus. Linear Algebraic Definition For two distinct vectors $\mathbf{a}$ and $\mathbf{b}$ of equal magnitude on a 2D Cartesian plane, the following set is called a rhombus. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} : 0\leq \lambda_{1},\lambda_{2}\leq 1, \quad |\mathbf{a}| = |\mathbf{b}|, \quad \mathbf{a} \ne \mathbf{b} \right\} $$ Explanation By definition, if it is a square,</description></item><item><title>Parallelogram</title><link>https://freshrimpsushi.github.io/en/posts/3694/</link><pubDate>Tue, 26 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3694/</guid><description>Definition Simple Definition A quadrilateral with two pairs of opposite sides that are each parallel to each other is called a parallelogram. Linear Algebraic Definition For two distinct vectors $\mathbf{a}$ and $\mathbf{b}$ on a 2-dimensional coordinate plane, the following set is called a parallelogram. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} : 0\leq \lambda_{1},\lambda_{2}\leq 1, \quad \mathbf{a} \ne \mathbf{b} \right\} $$ Explanation By definition, a parallelogram is also a</description></item><item><title>Square</title><link>https://freshrimpsushi.github.io/en/posts/3693/</link><pubDate>Sun, 24 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3693/</guid><description>Definition Simple Definition A quadrilateral with all sides of equal length and all angles of equal measure is called a square. Linear Algebraic Definition On a 2-dimensional coordinate plane, for two perpendicular and equal magnitude vectors $\mathbf{a}$ and $\mathbf{b}$, the following set is called a square. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} : 0\leq \lambda_{1},\lambda_{2}\leq 1, \quad \mathbf{a} \cdot \mathbf{b} = 0, \quad |\mathbf{a}| = |\mathbf{b}| \right\} $$</description></item><item><title>Rectangle</title><link>https://freshrimpsushi.github.io/en/posts/3692/</link><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3692/</guid><description>Definition Simple Definition A quadrilateral where all four angles are equal is called a rectangle. Linear Algebraic Definition On a 2D coordinate plane, for two perpendicular vectors $\mathbf{a}$ and $\mathbf{b}$, the set defined as follows is called a rectangle. $$ R = \left\{ \lambda_{1}\mathbf{a} + \lambda_{2} \mathbf{b} : 0\leq \lambda_{1},\lambda_{2}\leq 1, \quad \mathbf{a} \cdot \mathbf{b} = 0 \right\} $$ Explanation By definition, if it is a square, then it is</description></item><item><title>Comprehensive Summary of Julia String Syntax and Functions</title><link>https://freshrimpsushi.github.io/en/posts/3691/</link><pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3691/</guid><description>Overview This document provides a brief overview of the syntax and functions related to characters/strings used in Julia. The version reference is Julia v1.11.5. Official Documentation Manual&amp;gt;Strings: https://docs.julialang.org/en/v1/manual/strings/ Base&amp;gt;Strings: https://docs.julialang.org/en/v1/base/strings/ Summary Definition 'x' &amp;quot;x&amp;quot; repr(foo) Char(decimal) Char(hex) Operations &amp;quot;foo&amp;quot; &amp;lt; &amp;quot;bar&amp;quot; &amp;quot;foo&amp;quot; &amp;gt; &amp;quot;bar&amp;quot; &amp;quot;foo&amp;quot; == &amp;quot;bar&amp;quot; &amp;quot;foo&amp;quot; != &amp;quot;bar&amp;quot; isless(&amp;quot;foo&amp;quot;, &amp;quot;bar&amp;quot;) Predicates isspace(' ') isletter('x') isuppercase('X') islowercase('x') isdigit('1') isxdigit('x') isnumeric('1') ispunct('!') isascii(&amp;quot;foo&amp;quot;) iscntrl('x') isprint('x') General length(&amp;quot;foo bar&amp;quot;) sizeof(&amp;quot;foo bar&amp;quot;)</description></item><item><title>Orthogonal Similarity and Orthogonal Diagonalization of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3690/</link><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3690/</guid><description>Definition1 Orthogonal similarity For two square matrices $A$ and $B$, if there exists an orthogonal matrix $P$ that satisfies the following, then $A$ and $B$ are said to be orthogonally similar. $$ A = P^{\mathsf{T}}BP $$ Orthogonal diagonalization A square matrix $A$ is said to be orthogonally diagonalizable (or $P$ is said to orthogonally diagonalize $A$) if it is orthogonally similar to some diagonal matrix $D$. In other words, $A$</description></item><item><title>Diagonalization of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3689/</link><pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3689/</guid><description>Definition1 If a square matrix $A$ is similar to some diagonal matrix $D$ via similarity, we say $A$ is diagonalizable, or that $P$ diagonalizes $A$. In other words, $A$ is called a diagonalizable matrix if there exists an invertible matrix $P$ satisfying the following. $$ A = P^{-1}DP $$ Explanation If the matrix $A$ is diagonalizable, computing its powers becomes very easy. If one computes $A^{k}$ directly, one must perform</description></item><item><title>Topological Group</title><link>https://freshrimpsushi.github.io/en/posts/3687/</link><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3687/</guid><description>Definition1 A group $\braket{G, \cdot}$ that is a topological space and satisfies the following is called a topological group. The group multiplication $\cdot : G \times G \to G$, $\quad (g, h) \mapsto g \cdot h$ is continuous. The map to inverses $i : G \to G$, $\quad g \mapsto g^{-1}$ is continuous. From the topological viewpoint A Hausdorff space $G$ is called a topological group if it forms a</description></item><item><title>Differentiation of Polynomial Functions</title><link>https://freshrimpsushi.github.io/en/posts/3686/</link><pubDate>Sun, 10 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3686/</guid><description>Formula The derivative of a polynomial function is as follows. $$ \dfrac{d x^{n}}{dx} = n x^{n-1} $$ If $n \in \mathbb{N}$, it holds in $x \in \mathbb{R}$. If $n \in \mathbb{Z}$, it holds in $x \ne 0$. If $n \in \mathbb{R}$, it holds in $x \gt 0$. Proof $n \in \mathbb{N}$ By the definition of the derivative, $$ \dfrac{d x^{n}}{dx} = \lim_{h \to 0} \dfrac{(x+h)^{n} - x^{n}}{h} $$ Binomial theorem:</description></item><item><title>Representation of Groups</title><link>https://freshrimpsushi.github.io/en/posts/3684/</link><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3684/</guid><description>Definition1 2 Let a group $G$ and a finite-dimensional vector space $V$ be given. Call $\operatorname{GL}(V)$ the general linear group. A homomorphism $\rho$ as below is called a representation of $G$ on $V$. $$ \rho : G \to \operatorname{GL}(V) $$ By definition one may also say “the map $\rho$ endows the vector space $V$ with the structure of a $G$-modu</description></item><item><title>Equivariant Map of Group Action</title><link>https://freshrimpsushi.github.io/en/posts/3683/</link><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3683/</guid><description>Definition1 2 Suppose a group $G$, and two actions $\ast_{1} : G \times X \to X$, $\ast_{2} : G \times Y \to Y$ are given. A function $f : X \to Y$ between two $G$-sets is said to be equivariant concerning $G$ if it satisfies the following condition. $$ f(g \ast_{1} x) = g \ast_{2} f(x), \qquad \forall g \in G, x \in X $$ Explanation Simply speaking, the result</description></item><item><title>Kronecker Delta through Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3682/</link><pubDate>Sat, 02 Aug 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3682/</guid><description>Definition We define $\delta_{ij}$, as shown below, to be the Kronecker delta. $$ \delta_{ij} = \begin{cases} 1 &amp;amp; \text{if } i = j \\ 0 &amp;amp; \text{if } i \ne j \end{cases} $$ Explanation The Kronecker delta is typically encountered around the second year of undergraduate studies in science and engineering when vector calculus becomes prominent. While it is a useful tool for converting complex vector calculations into simpler scalar</description></item><item><title>Relative Entropy (Kullback-Leibler Divergence) between Two Normal Distributions</title><link>https://freshrimpsushi.github.io/en/posts/3681/</link><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3681/</guid><description>Formula The relative entropy (KLD) between two normal distributions $N(\mu, \sigma^{2})$ and $N(\mu_{1}, \sigma_{1}^{2})$ is given by the following expression. $$ D_{\text{KL}}\big( N(\mu, \sigma^{2}) \| N(\mu_{1}, \sigma_{1}^{2}) \big) = \log \left( \dfrac{\sigma_{1}}{\sigma} \right) + \dfrac{\sigma^{2} + (\mu - \mu_{1})^{2}}{2\sigma_{1}^{2}} - \dfrac{1}{2} $$ The relative entropy between two multivariate normal distributions $N(\boldsymbol{\mu}, \Sigma)$ and $N(\boldsymbol{\mu_{1}}, \Sigma_{1})$ is given by the following. $$ \begin{array}{l} D_{\text{KL}}\big( N(\boldsymbol{\mu}, \Sigma) \| N(\boldsymbol{\mu_{1}}, \Sigma_{1}) \big) \\[1em]</description></item><item><title>How to input physical units in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3680/</link><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3680/</guid><description>Code Unit Plain Text Inside an Equation Using siunitx Ångström Å \AA \mathrm{\AA} \si{\angstrom}</description></item><item><title>Inverse Function</title><link>https://freshrimpsushi.github.io/en/posts/3679/</link><pubDate>Sun, 27 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3679/</guid><description>Definition For a given surjective function $f: X \to Y$, the inverse function of $f$ is defined as follows. $$ f^{-1} : Y \to X, \quad f^{-1}(y) = x \iff f(x) = y $$ A function for which an inverse function exists is called an invertible function. Explanation By definition, $f$ is the inverse function of $f^{-1}$. $$ f = (f^{-1})^{-1} $$ $f \circ f^{-1}$ and $f^{-1} \circ f$ are</description></item><item><title>Radian</title><link>https://freshrimpsushi.github.io/en/posts/3678/</link><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3678/</guid><description>Definition The angle of a sector with a radius $r$ and an arc length $\ell$ is called $\theta$ $\text{rad}$. Here, $\text{rad}$ is read as radian. Explanation Since it is a value derived from dividing a length by a length, it is a dimensionless unit. Therefore, this unit is often omitted in usage. An angle without a unit is fundamentally assumed to be in radians. In the unit circle, where the</description></item><item><title>Geometric Mean</title><link>https://freshrimpsushi.github.io/en/posts/3677/</link><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3677/</guid><description>Definition The value for two positive numbers $a, b$ is called the geometric mean of $a$ and $b$. $$ \sqrt{ab} $$ Generalization For $n$ positive numbers $a_{1}, \dots, a_{n}$, the following value is called the geometric mean of $a_{1}, \dots, a_{n}$. $$ \sqrt[n]{a_{1}a_{2}\cdots a_{n}} $$ Explanation If one considers an extension to complex numbers, then $a_{i}$ do not necessarily have to be positive. One who encounters the geometric mean for</description></item><item><title>Paper Review: Score Matching</title><link>https://freshrimpsushi.github.io/en/posts/3676/</link><pubDate>Mon, 21 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3676/</guid><description>Overview Score Matching is a statistical technique introduced in the 2005 paper by Aapo Hyvarinen, Estimation of Non-Normalized Statistical Models by Score Matching, which provides a method for estimating non-normalized models without considering the normalization constant. 1. Introduction In many cases, probabilistic models are given as non-normalized models containing a normalization constant $Z$. For instance, a probability density function $p_{\boldsymbol{\theta}}$ with parameters $\boldsymbol{\theta}$ is defined as follows: $$ p(\boldsymbol{\xi}; \boldsymbol{\theta})</description></item><item><title>Matrix Differentiation Table for Scalar Functions</title><link>https://freshrimpsushi.github.io/en/posts/3675/</link><pubDate>Sat, 19 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3675/</guid><description>Explanation The formulas for matrix calculus have been summarized in the table below. The notation used throughout the document is as follows: $\mathbf{a}, \mathbf{b} \in \mathbb{R}^{n}$: A constant vector independent of $\mathbf{x}$ or $\mathbf{X}$ $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{n \times n}$: A constant matrix independent of $\mathbf{x}$ or $\mathbf{X}$ $\mathbf{x} \in \mathbb{R}^{n}$: A variable vector $\mathbf{X} \in \mathbb{R}^{n \times n}$: A variable matrix An interesting aspect of differentiation rules is</description></item><item><title>Trace Trick</title><link>https://freshrimpsushi.github.io/en/posts/3674/</link><pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3674/</guid><description>Equation Suppose a scalar function $f: \mathbb{R}^{n \times n} \to \mathbb{R}$ is defined over matrix space $M(\mathbb{R}^{n \times n})$. Let $\mathrm{d}f$ be the total differential of $f$. The following holds. $$ \mathrm{d}f = \Tr (\mathrm{d}f) = \mathrm{d}\Tr (f) \tag{1} $$ $\Tr$ is the trace. Proof The function value of $f$ is a scalar, so taking the trace yields the same result. Therefore, we obtain: $$ f = \Tr(f) \implies \mathrm{d}f</description></item><item><title>Matrix Differential Dissection</title><link>https://freshrimpsushi.github.io/en/posts/3673/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3673/</guid><description>Definition $n \times n$ The differential of a matrix $\mathbf{X} = [x_{ij}] a$ is defined as follows. $$ \mathrm{d} \mathbf{X} = \begin{bmatrix} \mathrm{d} x_{11} &amp;amp; \mathrm{d} x_{12} &amp;amp; \cdots &amp;amp; \mathrm{d} x_{1n} \\ \mathrm{d} x_{21} &amp;amp; \mathrm{d} x_{22} &amp;amp; \cdots &amp;amp; \mathrm{d} x_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \mathrm{d} x_{n1} &amp;amp; \mathrm{d} x_{n2} &amp;amp; \cdots &amp;amp; \mathrm{d} x_{nn} \end{bmatrix} $$ Explanation If the generalization of the</description></item><item><title>Total Differential of Function of a Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3672/</link><pubDate>Sun, 13 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3672/</guid><description>Background Scalar Differentiation Let&amp;rsquo;s consider a scalar function $f : \mathbb{R} \to \mathbb{R}$ and ordinary differentiation. $$ \dfrac{d f}{d x} \tag{1} $$ This notation resembles a fraction, and it&amp;rsquo;s assured that it can indeed be treated as such for calculations. For example, the chain rule, which is the differentiation rule for composite functions, can be intuitively calculated as if canceling fractions, as shown below. $$ \dfrac{d f}{d t} = \dfrac{d</description></item><item><title>Inner Product of Matrices (Frobenius Inner Product)</title><link>https://freshrimpsushi.github.io/en/posts/3671/</link><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3671/</guid><description>Definition The inner product or dot product of two matrices $m \times n$, $X = [x_{ij}]$, $Y=[Y]_{ij}$ is defined as follows. $$ X \cdot Y = \braket{X, Y} = \sum_{i=1}^{m}\sum\limits_{j=1}^{n} x_{ij}y_{ij} $$ In the case where the components are complex numbers in a complex matrix, it is defined as follows. $$ X \cdot Y = \braket{X, Y} = \sum_{i=1}^{m}\sum\limits_{j=1}^{n} \overline{x}_{ij}y_{ij} $$ Here, $\overline{x}$ denotes the complex conjugate. Explanation Since the</description></item><item><title>Matrix Calculus of Trace</title><link>https://freshrimpsushi.github.io/en/posts/3670/</link><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3670/</guid><description>Formula Let $\mathbf{X}$ be $n \times n$ matrix. Define $\dfrac{\partial }{\partial \mathbf{X}} = \nabla_{\mathbf{X}}$ as the matrix gradient. Then, the following formula holds: $$ \dfrac{\partial \Tr(\mathbf{X})}{\partial \mathbf{X}} = I, \qquad \dfrac{\partial \Tr(a\mathbf{X})}{\partial \mathbf{X}} = aI \tag{1} $$ Here, $a \in \mathbb{R}$ is a constant (scalar), and $I$ is an identity matrix. Suppose $\mathbf{A} \in \mathbb{R}^{n \times p}$ and $\mathbf{X} \in \mathbb{R}^{p \times n}$. Then, the following holds: $$ \dfrac{\partial \Tr(\mathbf{A}\mathbf{X})}{\partial</description></item><item><title>Outer Product of Two Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3669/</link><pubDate>Mon, 07 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3669/</guid><description>Definition The outer product of two column vectors $\mathbf{u} = \begin{bmatrix} u_{1} \\ \vdots \\ u_{n} \end{bmatrix}$ and $\mathbf{v} = \begin{bmatrix} v_{1} \\ \vdots \\ v_{n} \end{bmatrix}$ is defined as follows. $$ \mathbf{u} \otimes \mathbf{v} = \mathbf{u}\mathbf{v}^{\mathsf{T}} = \begin{bmatrix} u_{1} \\ u_{2} \\ \vdots \\ u_{n} \end{bmatrix} \begin{bmatrix} v_{1} &amp;amp; v_{2} &amp;amp; \cdots &amp;amp; v_{n} \end{bmatrix}= \begin{bmatrix} u_{1}v_{1} &amp;amp; u_{1}v_{2} &amp;amp; \cdots &amp;amp; u_{1}v_{n} \\ u_{2}v_{1} &amp;amp; u_{2}v_{2} &amp;amp; \cdots</description></item><item><title>Formula for Matrix Power Form</title><link>https://freshrimpsushi.github.io/en/posts/3668/</link><pubDate>Sat, 05 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3668/</guid><description>Formula For the matrix $X = [x_{ij}] \in \mathbb{R}^{n \times n}$, the following holds. $$ [XX]_{ij} = \sum_{k=1}^{n} x_{ik} x_{kj} $$ $$ XX = X^{2} = \begin{bmatrix} \sum\limits_{k=1}^{n} x_{1k} x_{k1} &amp;amp; \cdots &amp;amp; \sum\limits_{k=1}^{n} x_{1k} x_{kn} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \sum\limits_{k=1}^{n} x_{nk} x_{k1} &amp;amp; \cdots &amp;amp; \sum\limits_{k=1}^{n} x_{nk} x_{kn} \end{bmatrix} $$ If $X$ is a symmetric matrix, $$ X^{2} = \begin{bmatrix} \sum\limits_{k=1}^{n} (x_{1k})^{2} &amp;amp; \cdots &amp;amp; \sum\limits_{k=1}^{n}</description></item><item><title>Matrix Calculus of Quadratic and Bilinear Forms</title><link>https://freshrimpsushi.github.io/en/posts/3667/</link><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3667/</guid><description>Formula For two vectors $\mathbf{a} \in \mathbb{R}^{m}$, $\mathbf{b} \in \mathbb{R}^{n}$ and a matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$, the gradient matrix of the bilinear form $\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{b}$ is as follows. $$ \nabla_{\mathbf{X}} (\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{b}) = \dfrac{\partial (\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{b})}{\partial \mathbf{X}} = \mathbf{a}\mathbf{b}^{\mathsf{T}} \tag{1} $$ As a corollary, for quadratic form $\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{a}$, the following holds. $$ \dfrac{\partial (\mathbf{a}^{\mathsf{T}} \mathbf{X} \mathbf{a})}{\partial \mathbf{X}} = \mathbf{a}\mathbf{a}^{\mathsf{T}} $$ For two vectors</description></item><item><title>Probability Vector</title><link>https://freshrimpsushi.github.io/en/posts/3665/</link><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3665/</guid><description>Definition A vector that satisfies the following condition $\mathbf{p} = \begin{bmatrix}p_{1} &amp;amp; \cdots &amp;amp; p_{n} \end{bmatrix}^{\mathsf{T}}$ is called a probability vector. $$ 0 \le p_{i} \le 1 \quad (1 \le i \le n)\quad \text{and} \quad \sum_{i=1}^{n} p_{i} = 1 $$ Explanation A probability vector is a vector that represents the probabilities of each state when there are $n$ states. Conceptually, it is analogous to a probability mass function. If the</description></item><item><title>Energy-Based Model</title><link>https://freshrimpsushi.github.io/en/posts/3664/</link><pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3664/</guid><description>Overview1 2 3 Energy-based models are a methodology where a function of data referred to as energy is defined, with the notion that data with lower energy is considered more plausible (i.e., more probable). Unlike assuming previously known distributions, energy-based models directly model the distribution of the data, making them useful for handling complex datasets. Build-up In traditional probabilistic models, the approach to handling data involves assuming that the data</description></item><item><title>How to Use Dice Images in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3663/</link><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3663/</guid><description>\usepackage{epsdice} This package provides black and white images of dice with faces numbered 1 through 6. \documentclass{article} \usepackage{epsdice} \begin{document} \epsdice{1} \epsdice{2} \epsdice{3} \epsdice[white]{4} \epsdice[white]{5} \epsdice[white]{6} \epsdice[black]{1} \epsdice[black]{2} \epsdice[black]{3} \epsdice{[black]4} \epsdice[black]{5} \epsdice[black]{6} \end{document} \usepackage{customdice} This package provides the functionality to draw user-defined dice images. The number of faces, color, characters to be displayed, and size can all be customized according to user preference. For more details, refer to the official documentation.</description></item><item><title>How to Define and Use Your Colors in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3662/</link><pubDate>Wed, 25 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3662/</guid><description>Description The xcolor package allows users to employ predefined colors, but users can also define and use their own colors. By including \definecolor{color name}{color space}{values} in the preamble, you can define custom colors. The color spaces include rgb, cmyk, HTML, gray, among others. \definecolor{color name}{gray}{gray}: Definition of grayscale color $[0, 1]$ \definecolor{color name}{rgb}{r,g,b}: Definition of RGB color $[0, 1]^{3}$ \definecolor{color name}{cmyk}{c,m,y,k}: Definition of CMYK color $[0, 1]^{4}$ \definecolor{color name}{RGB}{R, G,</description></item><item><title>How to Use Colors in LaTeX: The xcolor Package</title><link>https://freshrimpsushi.github.io/en/posts/3661/</link><pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3661/</guid><description>Explanation \usepackage{color} In $\LaTeX$, the most basic package that allows the use of colors is color. You can add \usepackage{color} to the preamble and use it as {\color{color}content}. The available colors are as follows. $$ \begin{array}{lll} \color{black}\colorbox{black}{\phantom{colorbox}}black&amp;amp; \color{blue}\colorbox{blue}{\phantom{colorbox}}blue&amp;amp; \color{cyan}\colorbox{cyan}{\phantom{colorbox}}cyan\\ \color{green}\colorbox{green}{\phantom{colorbox}}green&amp;amp; \color{magenta}\colorbox{magenta}{\phantom{colorbox}}magenta&amp;amp; \color{red}\colorbox{red}{\phantom{colorbox}}red\\ \color{white}\colorbox{white}{\phantom{colorbox}} \color{black}{white}&amp;amp; \color{yellow}\colorbox{yellow}{\phantom{colorbox}}yellow \end{array} $$ \documentclass{article} \usepackage{color} \begin{document} This is {\color{red}red} text. \end{document} $$ \text{This is } \textcolor{red}{\text{red}} \text{ text.} $$ \usepackage{xcolor} By using xcolor, you can utilize</description></item><item><title>How to Automatically Number Figures in LaTeX Beamer</title><link>https://freshrimpsushi.github.io/en/posts/3660/</link><pubDate>Sat, 21 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3660/</guid><description>Code In the case of the article document style in $\LaTeX$, when inserting a photo and writing \caption{}, numbering is automatically applied as follows. However, in the case of beamer, numbering is not applied as follows. By adding \setbeamertemplate{caption}[numbered] to the preamble, numbering will be applied.</description></item><item><title>Professor Choi Byung-sun's Freely Distributed Textbooks on Mathematics, Statistics, and Economics</title><link>https://freshrimpsushi.github.io/en/posts/3659/</link><pubDate>Thu, 19 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3659/</guid><description>Description Professor Byung-Sun Choi, a former professor at Yonsei University in the Department of Applied Statistics and a current professor in the Department of Economics at Seoul National University, has published 12 textbooks on mathematics, statistics, and economics for free online. Each can be downloaded as a PDF from the links below. List 1995 Multivariate Time Series Analysis (Link) 1995 Introduction to SAS/IML (Link) 1997 Regression Analysis Volume I (Link)</description></item><item><title>Free Online Available R Textbook (Original Version)</title><link>https://freshrimpsushi.github.io/en/posts/3658/</link><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3658/</guid><description>Description Hadley Wickham hails from the University of Auckland in New Zealand and is the creator of most of the current tools for big data analysis in R, including the visualization package ggplot2, the preprocessing package dplyr, and the string package stringr.1 The four books introduced below are R texts authored by Hadley Wickham, which are available for free on the web. List R for Data Science (1e, 2e) ggplot2</description></item><item><title>List of Available Fonts in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3657/</link><pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3657/</guid><description>Basic Fonts These are the default fonts that can be used without installing any packages. Roman Used with \mathrm{words} or \rm{words}. \mathrm{ABCDEFGHIJKLMNOPQRSTUVWXYZ} $$\rm{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$$ \mathrm{abcdefghijklmnopqrstuvwxyz} $$\rm{abcdefghijklmnopqrstuvwxyz}$$ \mathrm{0123456789} $$\rm{0123456789}$$ Caligraphic Used with \mathcal{words} or \cal{words}. In mathematics, it is commonly used to denote operators such as Fourier transform and Laplace transform. Only uppercase letters are possible; lowercase letters are rendered as entirely different characters. \mathcal{ABCDEFGHIJKLMNOPQRSTUVWXYZ} $$\cal{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$$ Sans-serif Used with \mathsf{words} or \sf{words}.</description></item><item><title>How to Use Hyperlinks in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3656/</link><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3656/</guid><description>Code You can load the hyperref package and use the \href{URL}{text} command. It can be used to wrap text or equations and can be applied even in display mode. \documentclass{article} \usepackage{hyperref} \hypersetup{ colorlinks=true, urlcolor=blue, } \begin{document} unlinked Freshrimpsushi Blog linked \href{https://freshrimpsushi.github.io/ko/}{Freshrimpsushi Blog} The Fourier transform \href{https://freshrimpsushi.github.io/ko/posts/1086/}{$\hat{f}$} is defined as $$ \href{https://freshrimpsushi.github.io/ko/posts/1086/}{\hat{f}(\xi) = \frac{1}{2\pi}\int_{-\infty}^{\infty} f(x) e^{-i \xi x} dx} $$ \end{document} Through \hypersetup, you can also specify the colors for URLs,</description></item><item><title>Differentiation of the Absolute Value Function</title><link>https://freshrimpsushi.github.io/en/posts/3655/</link><pubDate>Wed, 11 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3655/</guid><description>Theorem The derivative of the absolute value function is as follows. $$ \frac{ d |x| } {d x} = \dfrac{1}{|x|}x = \begin{cases} 1 &amp;amp; x &amp;gt; 0 \\ -1 &amp;amp; x &amp;lt; 0 \end{cases}, \qquad x \neq 0 $$ Explanation In fact, the absolute value function is not differentiable over the entire set of real numbers because of its sharp point at $x = 0$. However, excluding just one point</description></item><item><title>The Mean and Variance of the Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3654/</link><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3654/</guid><description>Formula Given $X \sim$ when $\operatorname{Bin}(1, p)$, the mean and variance of $X$ are as follows. $$ E(X) = p $$ $$ \Var(X) = p(1-p) = pq, \qquad q = 1 - p $$ Proof For $p \in [0, 1]$, a discrete probability distribution with the following probability mass function is called a Bernoulli distribution. $$ f(x) = p^{x}(1-p)^{1-x}, \qquad x = 0, 1 $$ Direct Calculation By the definition</description></item><item><title>Moment Generating Function of Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3653/</link><pubDate>Sat, 07 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3653/</guid><description>Formula $X \sim$ When $\operatorname{Bin}(1, p)$, the moment generating function of $X$ is given below. $$ m(t) = 1 - p + pe^{t} = q + pe^{t}, \qquad q = 1 - p $$ Proof For $p \in [0, 1]$, a discrete probability distribution with the following probability mass function is referred to as a Bernoulli distribution. $$ f(x) = p^{x}(1-p)^{1-x}, \qquad x = 0, 1 $$ From the Definition</description></item><item><title>Categorical Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3652/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3652/</guid><description>Definition1 Given a sample space with $k (\ge 2)$ categories, $\Omega = \left\{ 1, 2, \dots, k \right\}$, and a probability vector $\mathbf{p} = (p_{1}, \dots, p_{k})$, the discrete probability distribution with the following probability mass function is called the Categorical distribution. $$ p(x = i) = p_{i}, \qquad x \in \left\{ 1, 2, \dots, k \right\} $$ Description The probability of each of the $k$ categories occurring is represented</description></item><item><title>Bernoulli Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3651/</link><pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3651/</guid><description>Definition1 For $p \in [0, 1]$, a discrete probability distribution with the following probability mass function is referred to as a Bernoulli distribution. $$ f(x) = p^{x}(1-p)^{1-x}, \qquad x = 0, 1 $$ Description This distribution is used when describing an experiment with only two possible outcomes, such as a coin toss. Because there are two possible outcomes, $x = 1$ is commonly referred to as a success and $x</description></item><item><title>Generalizing Differentiation: Gradient Matrices and Matrix Calculus</title><link>https://freshrimpsushi.github.io/en/posts/3666/</link><pubDate>Fri, 30 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3666/</guid><description>Definition We define the gradient matrix $\nabla_{\mathbf{X}} f$ for a scalar function $f : \mathbb{R}^{n \times n} \to \mathbb{R}$ and a matrix $\mathbf{X} = [x_{ij}] \in \mathbb{R}^{n \times n}$ as follows. $$ [\nabla_{\mathbf{X}} f]_{ij} = \dfrac{\partial f}{\partial x_{ij}} \quad (i,j=1,\dots,n) $$ $$ \nabla_{\mathbf{X}} f = \dfrac{\partial f}{\partial \mathbf{X}} = \begin{bmatrix} \dfrac{\partial f}{\partial x_{11}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f}{\partial x_{1n}} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \dfrac{\partial f}{\partial x_{n1}} &amp;amp;</description></item><item><title>Differences between Python special methods __str__ and __repr__</title><link>https://freshrimpsushi.github.io/en/posts/3650/</link><pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3650/</guid><description>Description __str__ and __repr__ are two special methods in Python used to define the string representation of an object. While these two methods serve similar roles, their purposes and usage contexts differ. Firstly, the name __str__ is derived from &amp;ldquo;string,&amp;rdquo; and it literally means returning the object or its value itself as a string. On the other hand, the name __repr__ comes from &amp;ldquo;representation,&amp;rdquo; and it returns a string that</description></item><item><title>How to Mute VSCode (Not Audio Cues)</title><link>https://freshrimpsushi.github.io/en/posts/3649/</link><pubDate>Mon, 26 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3649/</guid><description>Solution When using VSCode and you hear sounds like &amp;lsquo;boop&amp;rsquo;, &amp;lsquo;bop&amp;rsquo;, or &amp;lsquo;beep&amp;rsquo;, you can change the setting in the configuration (Ctrl + ,) by turning off the editor.accessibility support option. Upon searching, you might find suggestions to change the audio cues option, but it seems that this option is not available anymore. Environment Version: 1.98.2 (user setup) Commit: ddc367ed5c8936efe395cffeec279b04ffd7db78 Date: 2025-03-12T13:32:45.399Z Electron: 34.2.0 ElectronBuildId: 11161602 Chromium: 132.0.6834.196 Node.js: 20.18.2</description></item><item><title>Summary of Python Special Methods</title><link>https://freshrimpsushi.github.io/en/posts/3648/</link><pubDate>Sat, 24 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3648/</guid><description>Overview In Python, you can define operations on a class&amp;rsquo;s instance by defining methods with specific names. These methods are called special methods, also known as magic methods. By prefixing and suffixing these names with __, they are sometimes referred to as dunder methods (double underscore method). __init__ and __call__ __init__: A method for initializing an instance. Its contents are automatically executed when defining an instance. __call__: Executed when the</description></item><item><title>Maximum Likelihood Estimator of the Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3647/</link><pubDate>Thu, 22 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3647/</guid><description>Theorem Suppose we are given a random sample $\mathbf{X} := \left( X_{1} , \cdots , X_{n} \right) \sim \operatorname{Laplace}(\mu, b)$ that follows a Laplace distribution. The maximum likelihood estimator $(\hat{\mu}, \hat{b})$ for $(\mu, b)$ is as follows. $$ \hat{\mu} = \text{median}(\mathbf{x}_{1}, \cdots, \mathbf{x}_{n}) $$ $$ \hat{b} = \dfrac{1}{n} \sum\limits_{k=1}^{n} |x_{k} - \mu| $$ Proof Laplace Distribution: The Laplace distribution with parameters $\mu \in \mathbb{R}$ and $b &amp;gt; 0$ is a</description></item><item><title>Moment Generating Function of the Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3646/</link><pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3646/</guid><description>Formula $X \sim$ When $\operatorname{Laplace}(\mu, b)$, the moment-generating function of $X$ is as follows. $$ m(t) = \dfrac{1}{1 - b^{2}t^{2}} e^{\mu t} \qquad \text{for } |t| &amp;lt; \dfrac{1}{b} $$ Proof By the definition of the moment-generating function, $$ \begin{align*} E(e^{tX}) &amp;amp;= \int\limits_{-\infty}^{\infty} e^{tx} f(x) dx \\ &amp;amp;= \int\limits_{-\infty}^{\infty} e^{tx} \dfrac{1}{2b} e^{-|x - \mu|/b} dx \\ &amp;amp;= \dfrac{a}{2}e^{\mu t} \int\limits_{-\infty}^{\infty} e^{ty} e^{-a|y|} dx \qquad (y \equiv x - \mu, \quad a</description></item><item><title>Mean and Variance of Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3645/</link><pubDate>Sun, 18 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3645/</guid><description>Formula $X \sim$ When $\operatorname{Laplace}(\mu, b)$, the mean and variance of $X$ are as follows. $$ E(X) = \mu $$ $$ \Var(X) = 2b^{2} $$ Proof Direct Calculation By the definition of expectation and integration by parts, $$ \begin{align*} E(X) &amp;amp;= \int\limits_{-\infty}^{\infty} x \dfrac{1}{2b} e^{-|x - \mu|/b} dx \\ &amp;amp;= \int\limits_{-\infty}^{\mu} \dfrac{x}{2b} e^{(x - \mu)/b} dx + \int\limits_{\mu}^{\infty} \dfrac{x}{2b} e^{-(x - \mu)/b} dx \\ &amp;amp;= \left( \left[\dfrac{x}{2b}\left(b e^{(x - \mu)/b}\right)\right]_{-\infty}^{\mu}</description></item><item><title>Laplace Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3644/</link><pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3644/</guid><description>Definition1 For $\mu \in \mathbb{R}$ and $b &amp;gt; 0$, a continuous probability distribution $\operatorname{Laplace}(\mu, b)$ with the following probability density function is called the Laplace distribution. $$ f(x) = \dfrac{1}{2b} \exp \left( -\dfrac{|x - \mu|}{b} \right) $$ Explanation Relationship with the Normal Distribution Although it looks similar to the normal distribution, it has an absolute value $| x - \mu |$ instead of a square, giving it a sharper shape.</description></item><item><title>Maximum Likelihood Estimation for Linear Regression Model in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3643/</link><pubDate>Wed, 14 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3643/</guid><description>Summary Assume the relationship between data $\mathbf{x}_{i} \in \mathbb{R}^{n}$ and its labels $y_{i} \in \mathbb{R}$ is described by the following linear model. $$ y_{i} = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{i} + \epsilon_{i}, \qquad i = 1, \ldots, K \tag{1} $$ When $K &amp;gt; n$, the parameter $\mathbf{w}_{\text{ML}}$ that maximizes the likelihood is as follows. $$ \mathbf{w}_{\text{ML}} = (\mathbf{X}^{\mathsf{T}} \mathbf{X})^{-1} \mathbf{X}^{\mathsf{T}} \mathbf{y} $$ Here, $\mathbf{y} = \begin{bmatrix} y_{1} &amp;amp; \cdots &amp;amp; y_{K} \end{bmatrix}^{\mathsf{T}}$ and</description></item><item><title>Bayesian Inference in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3642/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3642/</guid><description>Overview Bayesian inference is a statistical method for estimating the distribution of parameters using prior knowledge and observed data based on Bayes&amp;rsquo; theorem. Explanation Assume that a random variable $\mathbf{x}$ follows a probability distribution with parameter $\theta$. The purpose of Bayesian inference is to estimate the distribution of $\theta$ by examining the samples drawn from $\mathbf{x}$. The key point is not the value of $\theta$, but estimating the &amp;ldquo;distribution&amp;rdquo; of</description></item><item><title>Maximum a Posteriori Estimation for Linear Regression Model in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3641/</link><pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3641/</guid><description>Theorem Assume that the relationship between data $\mathbf{x}_{i} \in \mathbb{R}^{n}$ and its labels $y_{i} \in \mathbb{R}$ can be expressed by the following linear model. $$ y_{i} = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{i} + \epsilon_{i}, \qquad i = 1, \ldots, K \tag{1} $$ The parameter $\mathbf{w}_{\text{MAP}}$ that maximizes the posterior probability is as follows. For $\mathbf{y} = \begin{bmatrix} y_{1} &amp;amp; \cdots &amp;amp; y_{K} \end{bmatrix}^{\mathsf{T}}$ and $\mathbf{X} = \begin{bmatrix} \mathbf{x}_{1} &amp;amp; \cdots &amp;amp; \mathbf{x}_{K} \end{bmatrix}^{T}</description></item><item><title>Mixture Distributions</title><link>https://freshrimpsushi.github.io/en/posts/3639/</link><pubDate>Tue, 06 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3639/</guid><description>Build-up1 Suppose we want to approximate a probability distribution with the probability density function as shown in the image below. One of the basic methods to approximate a probability distribution is to find a normal distribution that closely resembles the distribution we aim to approximate. However, as the following figures show, the distribution we want to approximate has three peaks, making it challenging to approximate using a normal distribution. Here,</description></item><item><title>Paper Review: Denoising Diffusion Probabilistic Models (DDPM)</title><link>https://freshrimpsushi.github.io/en/posts/3638/</link><pubDate>Sun, 04 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3638/</guid><description>Overview and Summary A generative model refers to a method to find a probability distribution $Y$ that a given random sample $\left\{ y_{i} \right\}$ follows. As it&amp;rsquo;s highly challenging to directly find this from scratch, it&amp;rsquo;s common to use well-known distributions to approximate the desired distribution. Thus, if a dataset $\left\{ x_{i} \right\}$ following a well-known distribution $X$ is given, the generative model can be regarded as a function $f$,</description></item><item><title>Generative Model</title><link>https://freshrimpsushi.github.io/en/posts/3637/</link><pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3637/</guid><description>Overview Determining the exact probability distribution that our data follows is a crucial yet very challenging problem in many application fields. For instance, if we precisely know the probability distribution of human face photographs and the method to sample from this distribution, we can obtain plausible human face images every time we sample data from this distribution. Obviously, this task is nearly impossible. Much like how many difficult problems begin</description></item><item><title>A Comprehensive Compilation of Mathematical and Scientific Terms (Korean, English, Japanese)</title><link>https://freshrimpsushi.github.io/en/posts/3636/</link><pubDate>Wed, 30 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3636/</guid><description>Overview For Korean terminology, we follow the Korean Mathematical Society(KMS). However, for expressions that are rarely used—for example, using “옹골찬” for “compact”—we instead transliterate the English term and write “컴팩트.” For English and Japanese, we consulted Wikipedia. Click the table&amp;rsquo;s header row (first</description></item><item><title>gravitational acceleration</title><link>https://freshrimpsushi.github.io/en/posts/3633/</link><pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3633/</guid><description>Definition The acceleration of an object moving under the influence of gravity is called gravitational acceleration. When an object moves solely under the effect of Earth&amp;rsquo;s gravity, without other external forces such as friction or air resistance, this motion is termed free-fall motion. Explanation When gravity or gravitational field is mentioned simply, it usually refers to Earth&amp;rsquo;s gravity. Earth&amp;rsquo;s gravitational acceleration is approximately $9.8 \mathrm{m/s^{2}}$, commonly denoted as $g$. This</description></item><item><title>Graph Laplacian</title><link>https://freshrimpsushi.github.io/en/posts/925/</link><pubDate>Wed, 16 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/925/</guid><description>Definition Let the degree matrix of a graph be $D$ and the adjacency matrix be $A$. Then, the graph Laplacian $L$ of $G$ is defined as follows: $$ L := D - A $$ Explanation Unlike the Laplacian $\Delta = \sum\limits_{i} \frac{\partial^{2}}{\partial x_{i}^{2}}$ defined for functions over Euclidean space, it depends on the given graph. For the graph $G = (V, E)$, denote $f : V \to \mathbb{R}$ as the</description></item><item><title>Graph Embedding, Node Embedding, Edge Embedding</title><link>https://freshrimpsushi.github.io/en/posts/975/</link><pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/975/</guid><description>Definition1 Let the graph $G(V, E)$ be given. The function $f: V \to \mathbb{R}^{n}$ is called node embedding, and the function $g: E \to \mathbb{R}^{m}$ is called edge embedding. For the set of graphs $\mathcal{G} = \left\{ G_{i} \right\}$, $h: \mathcal{G} \to \mathbb{R}^{k}$ is referred to as graph embedding. Explanation Graph/node/edge embedding is a function that maps a graph or its constituents to Euclidean space. To handle the abstract entity</description></item><item><title>Birthday Problem: Probability of Sharing the Same Birthday</title><link>https://freshrimpsushi.github.io/en/posts/998/</link><pubDate>Sat, 12 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/998/</guid><description>Formula Disregarding leap years, let $1$ be the number of years and $365$ be the number of days in a year. The probability that there is at least one pair of people with the same birthday among $n$ people is given by the following formula. $$ p(n) = 1 - \dfrac{365!}{365^n(365-n)!} $$ Explanation I recall that during my middle school years, this formula was introduced in a particular chapter of</description></item><item><title>Optimizer</title><link>https://freshrimpsushi.github.io/en/posts/1019/</link><pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1019/</guid><description>Definition An optimization problem refers to finding $x_{\ast}$ such that the value of function $f : \mathbb{R}^{n} \to \mathbb{R}$ is minimized. $$ x_{\ast} = \argmin\limits_{x} f(x) $$ A series of algorithms used to solve an optimization problem is called an optimizer. Explanation In machine learning and deep learning, the function $f$ is referred to as the loss function, and in this context, $x$ becomes the parameters of the neural network,</description></item><item><title>Parity Plot</title><link>https://freshrimpsushi.github.io/en/posts/1053/</link><pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1053/</guid><description>Definition A plot representing the ordered pairs of true values and predicted values as a scatter plot is called a parity plot. Explanation Phonetically, it would sound more like [parety plot], but in this article, it will be referred to as a parity plot in accordance with foreign word notation rules. The word &amp;ldquo;parity&amp;rdquo; implies &amp;lsquo;agreement&amp;rsquo;, so a parity plot can be described as a visual representation showing how well</description></item><item><title>Inner Product of Dual Space</title><link>https://freshrimpsushi.github.io/en/posts/1064/</link><pubDate>Mon, 31 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1064/</guid><description>Introduction For the vector space $V$, let&amp;rsquo;s denote $(V, \braket{\cdot, \cdot}_{V})$ as a Hilbert space. Let $V^{\ast}$ represent the dual space of $V$. By the Riesz Representation Theorem, any $f \in V^{\ast}$ can be expressed uniquely in terms of $\mathbf{v}_{f} \in V$ as follows: $$ f = \braket{\cdot, \mathbf{v}_{f}}_{V}; \quad f(\mathbf{x}) = \braket{\mathbf{x}, \mathbf{v}_{f}}_{V} \tag{1} $$ That is, $f \in V^{\ast}$ and $\mathbf{v}_{f} \in V$ are in a one-to-one correspondence.</description></item><item><title>How to Interpolate Strings in Python</title><link>https://freshrimpsushi.github.io/en/posts/1068/</link><pubDate>Sat, 29 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1068/</guid><description>f-string f-strings can be used from Python 3.6 and are the simplest and most convenient method of string interpolation. By prefixing the string with an f and using variables within the string as {variable}, you can achieve this. &amp;gt;&amp;gt;&amp;gt; name = &amp;#39;An, Yujin&amp;#39; &amp;gt;&amp;gt;&amp;gt; birthday = &amp;#39;2003&amp;#39; &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print(f&amp;#39;The leader of IVE is {name}, and she was born in {birthday}.&amp;#39;) The leader of IVE is An, Yujin, and she</description></item><item><title>How to Check the PC Name in Python</title><link>https://freshrimpsushi.github.io/en/posts/1069/</link><pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1069/</guid><description>Code Use socket.gethostname(). &amp;gt;&amp;gt;&amp;gt; import socket &amp;gt;&amp;gt;&amp;gt; socket.gethostname() &amp;#39;lab1&amp;#39; See Also Check Operating System platform.system() Check PC Username os.getlogin Check PC Name socket.gethostname() Environment OS: Windows11 Version: Python 3.11.5</description></item><item><title>Avoid Starting Sentences with And, But, and So When Writing English Papers</title><link>https://freshrimpsushi.github.io/en/posts/1543/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1543/</guid><description>Description When writing academic papers, it is not advisable to start sentences with &amp;ldquo;and,&amp;rdquo; &amp;ldquo;but,&amp;rdquo; or &amp;ldquo;so.&amp;rdquo; First of all, these three words are all conjunctions, so starting a sentence with them is grammatically incorrect. Conjunctions are used to connect two sentences into one, so if the previous sentence ends with a period, the next sentence should not start with and/but/so. While these words are frequently used in speech, colloquial,</description></item><item><title>Numerical solution of ordinary differential equations with external forces (Julia DifferentialEquations Package)</title><link>https://freshrimpsushi.github.io/en/posts/1083/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1083/</guid><description>Explanation DifferentialEquations.jl is a Julia package for numerical solutions of differential equations. In this article, we will introduce how to find a numerical solution of an ordinary differential equation with a forcing term using DifferentialEquations.jl. Reading the tutorial might be helpful. First-order ODE with Forcing Term Consider the following simple problem. $$ \begin{align*} \dfrac{du(t)}{dt} &amp;amp;= u(t) + \sin(t) \qquad t \in [0, 1] \\ u(0) &amp;amp;= 0.0 \end{align*} $$ Solving</description></item><item><title>What is a Differential Equation Solver?</title><link>https://freshrimpsushi.github.io/en/posts/1093/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1093/</guid><description>Definition An algorithm used to numerically solve differential equations is called a solver. Explanation For example, if a differential equation is solved using the 4th-order Runge-Kutta method (RK4), one might say, &amp;ldquo;RK4 was chosen as the solver for solving the differential equation.&amp;rdquo;</description></item><item><title>The Net, Moore-Smith Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1133/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1133/</guid><description>Definition1 Given a set $X$ and a directed set $A$, a function $f : A \to X$ from $A$ to $X$ is called a net. Notation For each $a \in A$, if we denote it by $x_{a} = f(a) \in X$, the net $f$ is represented as $(x_{a})_{a \in A}$ or $x_{\centerdot}$2. In other words, $$ x_{\centerdot} : A \to X \\ x_{\centerdot}(a) = x_{a} = f(a) $$ Explanation A</description></item><item><title>Directed Set</title><link>https://freshrimpsushi.github.io/en/posts/1134/</link><pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1134/</guid><description>Definition 1 Let $(A, \le)$ be a partially ordered set. If for any $a, b \in A$ there exists $c \in A$ satisfying $a \le c$ and $b \le c$, then $(A, \le)$ is called a directed set . Explanation A totally ordered set is a directed set. For the set $X$, the power set $P(X)$ with the subset relation $\subset$ as a partial order on $(P(X), \subset)$ is a</description></item><item><title>Approximation, Best Approximation</title><link>https://freshrimpsushi.github.io/en/posts/1152/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1152/</guid><description>Definition1 Let $(X, d)$ be a metric space. For a subset $U \subset X$, a mapping $X \to U$ is called an approximation (method). The best approximation for $f \in X$ is defined as follows: $$ u^{\ast} = \argmin_{u \in U} d(f, u) $$ Explanation The term &amp;ldquo;approximation&amp;rdquo; refers to something close, thus &amp;ldquo;approximation to $f$ (an approximation of $f$)&amp;rdquo; means something close to $f$. Mathematically, the concepts of being</description></item><item><title>Zero-function</title><link>https://freshrimpsushi.github.io/en/posts/1155/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1155/</guid><description>Definition In the Real Space $0 : \mathbb{R} \to \mathbb{R}$ defined as follows is called the zero function. $$ 0(x) = 0 \quad \text{for all } x \in \mathbb{R} $$ In the Vector Space Let the zero vector of the vector space $V$ be denoted by $\mathbf{0}_{V}$. The zero function $\mathbf{0} : V \to V$ defined on $V$ is as follows. $$ \mathbf{0}(\mathbf{v}) = \mathbf{0}_{V} \quad \text{for all } \mathbf{v}</description></item><item><title>Differentiation of Constant Functions</title><link>https://freshrimpsushi.github.io/en/posts/1175/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1175/</guid><description>Formula The derivative of the constant function $C$ is $0$. $$ \dfrac{d C}{dx} = 0 $$ Explanation To be precise, the derivative being a function means &amp;ldquo;the derivative of the constant function is the zero function.&amp;rdquo; Since the zero function is also a constant function, the derivative of the constant function is a constant function. Derivation For all $x \in \mathbb{R}$, let it be $C(x) = c$ ($c \in \mathbb{R}$</description></item><item><title>Antiderivatives and Indefinite Integrals</title><link>https://freshrimpsushi.github.io/en/posts/1177/</link><pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1177/</guid><description>Definition A function $F$ is called an antiderivative of another function $f$ if it satisfies $F^{\prime} = f$. Explanation An antiderivative is translated as 원시함수 (primitive function), 역도함수 (reverse derivative), etc. The process of finding a function $F$ that satisfies $F^{\prime} = f$ for a given $f$, or the function $F$ itself, is called an indefinite integral. The indefinite integral or antiderivative of $f$</description></item><item><title>Subgroup Test</title><link>https://freshrimpsushi.github.io/en/posts/3474/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3474/</guid><description>Theorem One-step subgroup test Let the group $G$ and let $H$ be a nonempty subset of $G$ (empty set가 아닌 부분집합). If whenever $a$ and $b$ are elements of $H$ then $ab^{-1}$ is also an element of $H$, then $H$ is a subgroup of $G$. Equivalently, if whenever $a$ and $b$ are in $H$ then $a-b$ is also in $H$, then</description></item><item><title>Autoencoder</title><link>https://freshrimpsushi.github.io/en/posts/1181/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1181/</guid><description>Definition For two natural numbers $m \ll n$, the function $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is called an encoder. The function $g : \mathbb{R}^{m} \to \mathbb{R}^{n}$ is called a decoder. If $h = g \circ f$ satisfies $h(x) = x$, it is called an autoencoder. Explanation Since the encoder&amp;rsquo;s output dimension is smaller than the input dimension, it can be considered as performing data compression and encryption. On the other</description></item><item><title>How to Assign Numbers Arbitrarily in LaTeX Enumerate</title><link>https://freshrimpsushi.github.io/en/posts/1188/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1188/</guid><description>Code Using \setcounter{enumi}{n}, the following list starts from $n+1$. \documentclass{article} \begin{document} \begin{enumerate} \setcounter{enumi}{2} \item IVE \item LE SSERAFIM \setcounter{enumi}{7} \item IZ*ONE \setcounter{enumi}{10} \item LOVELYZ \end{enumerate} \end{document}</description></item><item><title>Difference between Noise and Artifacts in Images (Signals, Data)</title><link>https://freshrimpsushi.github.io/en/posts/1192/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1192/</guid><description>Overview Noise and artifacts are common factors that degrade the original signal (data) and need to be removed. This document explains the characteristics of these two elements and how they differ from each other. Definition In an image, noise refers to any values that degrade the original image, typically caused by random, unpredictable, and unremovable factors. On the other hand, the degradation caused by removable, regular, or predictable factors is</description></item><item><title>How to Use Square Brackets in LaTeX Enumerate</title><link>https://freshrimpsushi.github.io/en/posts/1193/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1193/</guid><description>Code In an enumerate environment, round brackets (, ) are used by default. If you want to use square brackets [, ], you should load the package with \usepackage{enumitem} and use it with \begin{enumerate}[label={[}\arabic*{]}]...\end{enumerate}. \documentclass{article} \usepackage{enumitem} \begin{document} \begin{enumerate}[label={[}\arabic*{]}] \item IVE \item LE SSERAFIM \item IZ*ONE \item LOVELYZ \end{enumerate} \end{document}</description></item><item><title>Website for Easily Creating LaTeX Tables</title><link>https://freshrimpsushi.github.io/en/posts/1194/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1194/</guid><description>Description Creating tables in a LaTeX document is quite cumbersome. Unlike typical document editing programs, it&amp;rsquo;s not easy to construct them. Particularly, attempting to draw tables like the one above, where rows or columns are merged, can be quite frustrating. At times like these, you can use the following website to effortlessly create tables. Tables Generator By constructing tables as shown in the image below, similar to editing in Hangul</description></item><item><title>How to Make References Appear in Numerical Order When Using \cite{} in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1215/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1215/</guid><description>Code When writing a TeX document, if the bibliography is sorted by &amp;lsquo;order of citation,&amp;rsquo; using \cite{} will automatically arrange them in numerical order (as that is the order in which they are cited). However, if sorted by other criteria like last name, the bibliography numbers will appear in the order in which the code is written, not in numerical order. In this case, using \usepackage{cite} will automatically sort them</description></item><item><title>How to Add Hyperlinks to Bibliography Numbers in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1232/</link><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1232/</guid><description>Code Add \usepackage{hyperref} to the preamble of the document. This package automatically generates links within the document and provides various linking functionalities, including referencing. You can specify detailed options, for example, in the following code: colorlinks=true: Apply colors to links (the default is a box display), linkcolor=blue: Set the color of internal links to blue, citecolor=green: Set the color of bibliography (reference) links to green. \usepackage[colorlinks=true, linkcolor=blue, citecolor=green]{hyperref} (Left) Before</description></item><item><title>Jacobi's Formula</title><link>https://freshrimpsushi.github.io/en/posts/1234/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1234/</guid><description>Formula Let $A = A(t)$ be a differentiable matrix function. The derivative of the determinant $\det A(t)$ is given by: $$ \dfrac{\mathrm{d}}{\mathrm{d} t} \det A(t) = \Tr \Big( (\operatorname{adj}A(t)) \dfrac{\mathrm{d}A(t)}{\mathrm{d}t} \Big) = \det A(t) \cdot \Tr\left( A^{-1}(t) \dfrac{\mathrm{d}A(t)}{\mathrm{d}t} \right) $$ This is known as Jacobi&amp;rsquo;s formula. In the form of a total differential, it can be written as follows. The second equality holds when $A$ is an invertible matrix. $$</description></item><item><title>Implementing a Discrete Fourier Transform Matrix in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1249/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1249/</guid><description>Explanation The Discrete Fourier Transform of an $N$-dimensional vector $\mathbf{x}$ can be represented as a matrix multiplication as follows: $$ \widehat{\mathbf{x}} = F \mathbf{x} $$ Here, if we express $F$ as $\omega = e^{-i2\pi/N}$, $$ F = \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \\ 1 &amp;amp; \omega &amp;amp; \omega^2 &amp;amp; \cdots &amp;amp; \omega^{N-1} \\ 1 &amp;amp; \omega^2 &amp;amp; \omega^4 &amp;amp; \cdots &amp;amp; \omega^{2(N-1)} \\ \vdots &amp;amp;</description></item><item><title>Discrete Fourier Transform Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1250/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1250/</guid><description>Description The discrete Fourier transform is an adaptation of the Fourier transform for finite-dimensional vectors or digital signals. The definition of the discrete Fourier transform is as follows. The linear transformation defined as $\mathcal{F}_{N} : \mathbb{C}^{N} \to \mathbb{C}^{N}$ is called the discrete Fourier transform (DFT). $$ \mathcal{F}_{N}(\mathbf{a}) = \hat{\mathbf{a}},\quad \hat{a}_{m}=\sum_{n=0}^{N-1}e^{-i2\pi mn /N} a_{n}\quad (0\le m &amp;lt; N) $$ Here, $\mathbf{a} = (a_{0}, a_{1}, \dots, a_{N-1})$, $\hat{\mathbf{a}} = (\hat{a}_{0}, \hat{a}_{1}, \dots,</description></item><item><title>Free Online Resources for Artificial Intelligence, Machine Learning, and Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/1253/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1253/</guid><description>Description Textbooks in the fields of artificial intelligence, machine learning, and deep learning are often freely available on the internet. Even in the case of well-known textbooks, there are quite a few that are freely accessible. Here are a few introductions. List Pattern Recognition and Machine Learning (Link) Authored by Christopher Bishop, this book is also well-known as PRML. The Korean translation title is &amp;ldquo</description></item><item><title>Hiding Specific Fields in Biblatex</title><link>https://freshrimpsushi.github.io/en/posts/1275/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1275/</guid><description>Code To exclude specific fields from the information displayed in biblatex, you can set them using the \AtEveryBibitem command. Below are the BibTeX information and rendered result for Walter Rudin&amp;rsquo;s book &amp;ldquo;Principles of Mathematical Analysis.&amp;rdquo; @book{rudin1964principles, title={Principles of mathematical analysis}, author={Rudin, Walter and others}, volume={3}, year={1964}, publisher={McGraw-hill New York} } If you want to exclude the volume and year fields from the above reference, you can write it as follows.</description></item><item><title>Customizing the Order of References in BibLaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1276/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1276/</guid><description>Code In the preamble of the document (\begin{document} section), you can add the \DeclareSortingScheme command to arbitrarily set the sorting order of the references. To set the first sorting criterion as ascending by name, the second criterion as descending by year, and the third criterion as by title, write as follows. \documentclass{article} \usepackage[backend=biber,sorting=mycustomscheme]{biblatex} % BibLaTeX 설정 \DeclareSortingTemplate{mycustomscheme}{ \sort[direction=ascending]{\field{author}} \sort[direction=descending]{\field{year}} \sort[direction=descending]{\field{title}} } \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography[heading=none] \end{refsection} \end{document} This time,</description></item><item><title>How to Set a LaTeX Document Horizontally</title><link>https://freshrimpsushi.github.io/en/posts/1345/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1345/</guid><description>Code If you want to set the document in landscape orientation, you can add \usepackage[landscape]{geometry}. This is useful when creating presentation materials. \documentclass{article} \usepackage[a6paper, landscape]{geometry} \begin{document} with landscape \end{document} \documentclass{article} \usepackage[a6paper]{geometry} \begin{document} without landscape \end{document}</description></item><item><title>IguanaTeX: How to Input TeX in PowerPoint</title><link>https://freshrimpsushi.github.io/en/posts/1366/</link><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1366/</guid><description>Description The native equation editor in PowerPoint is quite crude. Although it somewhat follows TeX syntax, it&amp;rsquo;s virtually unusable, and the most significant issue is that the equations do not look good. For presentation materials, unattractive equations are a critical flaw. Consequently, for presentation materials requiring a lot of equations, TeX&amp;rsquo;s beamer format is often used. However, because TeX was not developed for creating presentation materials, using beamer is essentially</description></item><item><title>How to Automatically Fill Blank Spaces in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1372/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1372/</guid><description>Code If you want to place any equations or images at the bottom of the document, you can use \vfill. This will fill the empty space vertically. If you want to fill the empty space horizontally, use \hfill. \documentclass{article} \usepackage[a6paper]{geometry} \begin{document} aaaaaaaaaa \hfill bbbbbbbbbbbb \vfill cccccccccc \end{document}</description></item><item><title>Angle Between Two Vectors in an n-Dimensional Euclidean Space</title><link>https://freshrimpsushi.github.io/en/posts/1408/</link><pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1408/</guid><description>Definition1 $n$Vector space of dimension n, for two vectors $\mathbf{v}, \mathbf{u} \in \mathbb{R}^{n}$, the angle between them is defined as the $\theta$ satisfying the following condition. $$ \cos \theta = \dfrac{\mathbf{v} \cdot \mathbf{u}}{\|\mathbf{v}\| \|\mathbf{u}\|} \tag{1} $$ Here, $\cdot$ is the inner product. Explanation In 2D or 3D spaces, since vectors can be visualized as arrows, the concept of the &amp;ldquo;angle between two vectors&amp;rdquo; can be understood intuitively and geometrically represented.</description></item><item><title>A Constant-Magnitude Vector-Valued Function is Orthogonal to Its Derivative</title><link>https://freshrimpsushi.github.io/en/posts/1411/</link><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1411/</guid><description>Theorem1 For a vector function $\mathbf{r} : \mathbb{R} \to \mathbb{R}^{n}$, if $|\mathbf{r}(t)| = c$ then the following holds. ($c$ is a constant) $$ \mathbf{r}(t) \perp \mathbf{r}^{\prime}(t) \quad \forall t $$ Explanation An example can be given for uniform circular motion with a constant radius. In this case, the velocity vector and the acceleration vector are always perpendicular to each other. Proof By the property of the dot product, $$ \mathbf{r}</description></item><item><title>Limits and Continuity of Vector-Valued Functions</title><link>https://freshrimpsushi.github.io/en/posts/1418/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1418/</guid><description>Definition1 Let the vector function $\mathbf{r} : \mathbb{R} \to \mathbb{R}^{3}$ for three scalar functions $f, g, h: \mathbb{R} \to \mathbb{R}$ be given as follows. $$ \mathbf{r}(t) = \left( f(t), g(t), h(t) \right) $$ Define the limit of $\mathbf{r}$ at $a$ as follows. $$ \lim\limits_{t \to a} \mathbf{r}(t) = \left( \lim\limits_{t \to a} f(t), \lim\limits_{t \to a} g(t), \lim\limits_{t \to a} h(t) \right) $$ We say that $\mathbf{r}$ is continuous at</description></item><item><title>Derivative of a Vector-Valued Function</title><link>https://freshrimpsushi.github.io/en/posts/1419/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1419/</guid><description>Definition1 For the vector function $\mathbf{r} : I \subset \mathbb{R} \to \mathbb{R}^{3}$, if the following limit exists, then we say that $\mathbf{r}$ is differentiable at $t$ and its value is called the derivative of $\mathbf{r}$ at $t$. $$ \dfrac{d \mathbf{r}}{d t} = \mathbf{r}^{\prime}(t) := \lim_{h \to 0} \dfrac{\mathbf{r}(t+h) - \mathbf{r}(t)}{h} $$ If for all $t \in I$ there exists $\mathbf{r}^{\prime}(t)$, then we say that $\mathbf{r}$ is differentiable at $I$. When</description></item><item><title>The Relationship between the Dot Product of Two Vectors and the Angle Between Them</title><link>https://freshrimpsushi.github.io/en/posts/1435/</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1435/</guid><description>Theorem Let the angle between two vectors $\mathbf{a} = (a_{1}, a_{2}, a_{3})$ and $\mathbf{b} = (b_{1}, b_{2}, b_{3})$ be $\theta$. Then, the following holds. $$ \mathbf{a} \cdot \mathbf{b} = |\mathbf{a}| |\mathbf{b}| \cos \theta $$ Here, $\mathbf{a} \cdot \mathbf{b}$ is the dot product (inner product) of the two vectors. Corollary The necessary and sufficient condition for two non-zero vectors $\mathbf{a}$ and $\mathbf{b}$ to be orthogonal is as follows. $$ \mathbf{a} \cdot</description></item><item><title>How to Highlight (with a Marker) Text and Equations in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1438/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1438/</guid><description>Using the soul package and the \hl command, it is possible to highlight texts, equations, and more. In this case, the xcolor package must also be loaded for the highlight color to appear. Applying this to display mode can be somewhat complex. Refer to the following code: \documentclass{article} \usepackage{xcolor} \usepackage{soul} \begin{document} \hl{asdfasdf} \\ \begin{center} \hl{$\displaystyle \int\limits_{-\infty}^{\infty} f(x) e^{-i\xi x} dx$} \end{center} \end{document}</description></item><item><title>How to Merge Multiple TeX Files into One</title><link>https://freshrimpsushi.github.io/en/posts/1440/</link><pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1440/</guid><description>Code \include Use the \include command. Create a new tex file in the same folder as the two files, file1.tex and file2.tex, and write as follows. However, note that a page break will be forced between the two files. \documentclass{article} \usepackage[a5paper]{geometry} \usepackage{kotex} \begin{document} \include{file1} \include{file2} \end{document} \input If you want the contents of the two files to seamlessly continue on the same page, use the \input command. \documentclass{article} \usepackage[a5paper]{geometry} \usepackage{kotex}</description></item><item><title>Azimuth and Direction Cosines</title><link>https://freshrimpsushi.github.io/en/posts/1468/</link><pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1468/</guid><description>Definition1 Suppose we are given a three-dimensional vector $\mathbf{a} = (a_{1}, a_{2}, a_{3})$. The angles that $\mathbf{a}$ forms with the $x$-, $y$-, and $z$-axes are denoted as $\alpha$, $\beta$, and $\gamma$, respectively. These are called direction angles. The cosines of the direction angles $\cos \alpha$, $\cos \beta$, and $\cos \gamma$ are called direction cosines. Properties From the definition of direction angles and the properties of the dot product, the direction</description></item><item><title>Implementing Automatic Differentiation Forward Mode Using Dual Numbers in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1498/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1498/</guid><description>Overview The forward mode of automatic differentiation can be easily implemented using dual numbers. This document explains how to implement the forward mode in Julia. For background knowledge on dual numbers and automatic differentiation, the following articles are recommended: Dual Numbers Differentiable Real Functions Defined on Dual Number Fields Automatic Differentiation Automatic Differentiation and Dual Numbers Code 1 An example of calculating the automatic differentiation for function $y(x) = \ln</description></item><item><title>Differentiable Real Functions Defined on a Dual Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1500/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1500/</guid><description>Build-up1 Assume the given smooth function $f : \mathbb{R} \to \mathbb{R}$. The Taylor series of $f$ at $a$ is as follows: $$ \begin{align*} f(x) &amp;amp;= f(a) + f^{\prime}(a)(x - a) + \dfrac{f^{\prime \prime}(a)}{2!}(x - a)^{2} + \cdots \\ &amp;amp;= f(a) + f^{\prime}(a)(x - a) + \sum_{n=2}^{\infty} \dfrac{f^{(n)}(a)}{n!}(x - a)^{n} \end{align*} $$ Although the above equation is obtained for a function defined on the real space, let&amp;rsquo;s substitute the dual number</description></item><item><title>Automatic Differentiation and Dual Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1501/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1501/</guid><description>Overview Dual numbers are numbers that can be expressed in the following form for two real numbers $a, b \in \mathbb{R}$. $$ a + b\epsilon, \quad (\epsilon^{2} = 0,\ \epsilon \neq 0) $$ The addition and multiplication system of dual numbers is useful for implementing the forward mode of automatic differentiation. Description1 In automatic differentiation, especially the forward mode, when computing the function value $f$, the derivative is calculated simultaneously.</description></item><item><title>Dual Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1502/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1502/</guid><description>Definition The following form for $\epsilon$ satisfying $\epsilon^{2} = 0 (\epsilon \neq 0)$ is called dual numbers. $$ a + b\epsilon,\qquad a, b \in \mathbb{R} $$ Explanation As can be seen from the definition, $\epsilon$ plays a role similar to the $i$ of complex numbers in that it creates the second dimension in the ordered pair. Of course, its properties are entirely different. $$ x + yi = (x, y)</description></item><item><title>How to not omit author names in BibTeX</title><link>https://freshrimpsushi.github.io/en/posts/1519/</link><pubDate>Sun, 29 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1519/</guid><description>Code In biblatex, the authors are abbreviated as et al. when there are four or more authors. \documentclass[10pt]{article} \usepackage{biblatex} \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography \end{refsection} \end{document} By placing a sufficiently large number in the maxbibnames option, you can display all author names. \documentclass[10pt]{article} \usepackage[maxbibnames=99]{biblatex} \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography \end{refsection} \end{document}</description></item><item><title>How to Sort Bibliographies Chronologically or Reverse Chronologically in BibTeX</title><link>https://freshrimpsushi.github.io/en/posts/1535/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1535/</guid><description>Code By default, when no settings are applied, the output is sorted in ascending order by first name. \documentclass[10pt]{article} \usepackage{kotex} \usepackage{biblatex} \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography \end{refsection} \end{document} When the package is loaded with the option sorting=ynt, it is sorted in ascending order of year, name (title), and title. \documentclass[10pt]{article} \usepackage{kotex} \usepackage[sorting=ynt]{biblatex} \begin{document} \begin{refsection}[ref.bib] \nocite{*} \printbibliography \end{refsection} \end{document} If the option sorting=ydnt is added, it is sorted in descending order. \documentclass[10pt]{article}</description></item><item><title>How to Choose Coefficients in the Runge-Kutta Method</title><link>https://freshrimpsushi.github.io/en/posts/1536/</link><pubDate>Wed, 25 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1536/</guid><description>Overview1 Let’s consider the ordinary differential equation given as follows. $y$ is a function of $t$, and prime$(^{\prime})$ denotes the derivative with respect to $t$. $$ y^{\prime} = f(t,y),\quad t \ge t_{0},\quad y(t_{0}) = y_{0}, \quad t_{n+1} = t_{n} + h $$ The explicit Runge-Kutta method is a way to approximate $y_{n+1} = y(t_{n+1})$ for a given $y_{n} = y(t_{n})$ as follows: $$ y_{n+1} = y_{n}</description></item><item><title>How to Define Neural Networks Using Functional API in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/1539/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1539/</guid><description>Description A simple neural network structure can be defined using Flux.Chain, but it is difficult to define a neural network with a complex structure using Chain. In such cases, you can define the neural network using a functional API with the @functor macro. The @functor allows the parameters of a struct-defined neural network to be tracked for performing backpropagation. Code Let&amp;rsquo;s define a neural network consisting of four linear layers.</description></item><item><title>Additive Group of Integer Modulo n</title><link>https://freshrimpsushi.github.io/en/posts/1088/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1088/</guid><description>Definition For an arbitrary natural number $n$, let the set $\mathbb{Z}_{n}$ and the binary operation $+$ be defined as follows. $$ \mathbb{Z}_{n} = \{ 0, 1, 2, \cdots, n-1 \} \\ a + b = (a + b) \mod n $$ Here $\operatorname{mod}$ is the modulo operation. The binary operation structure $(\mathbb{Z}_{n}, +)$ is called the integer modulo (additive) group. It is denoted simply by $\mathbb{Z}_{n}$. Explanation The binary operation</description></item><item><title>Do not use articles for Figure, Table, and Appendice in English papers</title><link>https://freshrimpsushi.github.io/en/posts/1597/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1597/</guid><description>Description In English academic papers, the first letters of Figure, Table, Appendix, and Diagram are always capitalized. Additionally, articles such as a/an/the are not attached to them. Remember especially not to attach &amp;ldquo;the&amp;rdquo; to terms like Definition, Theorem, and Lemma in mathematics papers. Examples $\sout{\text{The figure}}\text{ 1 shows the relationship between A and B. (X)}$ $\text{Figure 1 shows the relationship between A and B. (O)}$ $\text{See } \sout{\text{the appendix}} \text{</description></item><item><title>Difference between Abbreviation and Acronym</title><link>https://freshrimpsushi.github.io/en/posts/1602/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1602/</guid><description>Description The act of shortening long English words or sentences into succinct forms is referred to as abbreviation or acronym. An abbreviation is a reduced form of a word or phrase by omitting some of its letters, and it is translated to Korean as 약어 or 축약어. In the case of abbreviations, a period (.) is placed to indicate the omission of letters. Doctor $\textbf{D}\text{octo}\textbf{r} \mapsto</description></item><item><title>Numerical Solution of the Wave Equation: K-Space Method</title><link>https://freshrimpsushi.github.io/en/posts/1627/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1627/</guid><description>Method Assume we are given the following wave equation. $$ \partial_{t}^{2} u(\mathbf{x}, t) = \Delta_{\mathbf{x}} u (\mathbf{x}, t), \qquad (\mathbf{x}, t) \in \mathbb{R}^{n} \times \mathbb{R} $$ By taking the Fourier transform of both sides with respect to the variable $\mathbf{x}$, we obtain the following. $$ \partial_{t}^{2} \mathcal{F}_{\mathbf{x}}u(\mathbf{k}, t) = \mathcal{F}_{\mathbf{x}}[\Delta u (\cdot, t)]u(\mathbf{k}, t) = -|\mathbf{k}|^{2} \mathcal{F}_{\mathbf{x}}u(\mathbf{k}, t) $$ Additionally, the left-hand side of the above equation can be approximated</description></item><item><title>Notation of Expectation Values in Physics</title><link>https://freshrimpsushi.github.io/en/posts/1992/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1992/</guid><description>Definition When the probability density function of a random variable $X$ is $f(x)$, the following value is referred to as the expectation. $$ \braket{X} = \braket{x} = \int_{-\infty}^{\infty} x f(x) dx \tag{1} $$ Explanation In statistics, the expectation is usually denoted as follows: $$ E(X) = \int_{-\infty}^{\infty} x f(x) dx $$ However, in physics, it is common to use single angle brackets to denote it as $(1)$. This is called</description></item><item><title>Numerical Solution of the Wave Equation: Finite Difference Method (FDM)</title><link>https://freshrimpsushi.github.io/en/posts/1628/</link><pubDate>Sat, 07 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1628/</guid><description>Method Assume we are given the following one-dimensional wave equation. $$ \dfrac{\partial^{2} u}{\partial t^{2}} = c^{2} \dfrac{\partial^{2} u}{\partial x^{2}}, \qquad 0 \le x \le 1, \quad t \ge 0 \tag{1} $$ Our goal is to approximate the above solution using a finite number of points. Let&amp;rsquo;s discretize the spacetime domain as follows. $$ \left\{ (\ell \Delta x, n\Delta t) : \ell=0,1,\dots,d+1,\ n\ge 0 \right\}\quad \text{ where } \Delta x =</description></item><item><title>Absorbing Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/1631/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1631/</guid><description>Overview Methods for numerically calculating the propagation of waves include the Finite Element Method (FEM) and the $k$-space method. These methods assume that waves propagate infinitely, but in actual simulations, waves propagate within a finite grid. This can cause reflections at the boundaries. To solve this, grids can be set much larger than the actual region of interest, or boundary conditions can be applied to prevent wave reflections. These boundary</description></item><item><title>Integration of Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1647/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1647/</guid><description>Summary Suppose the power series $\sum\limits_{n = 0}^{\infty} c_{n}x^{n}$ converges at $\left| x \right| \lt R$. We then define the function $f$ as follows: $$ f(x) = \sum\limits_{n = 0}^{\infty} c_{n}x^{n} \qquad \left| x \right| \lt R \tag{1} $$ Then the function $f$ is integrable at $(-R, R)$, and its indefinite integral is as follows: $$ \int f(x) dx = C + \sum\limits_{n = 0}^{\infty} \dfrac{c_{n}}{n + 1} x^{n+1} \qquad</description></item><item><title>The limit of n^(1/n)</title><link>https://freshrimpsushi.github.io/en/posts/1649/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1649/</guid><description>공식 $$ \lim \limits_{n \to \infty} \sqrt[n]{n} = 1 $$ $$ \lim \limits_{n \to \infty} \sqrt[n]{\dfrac{1}{n}} = 1 $$ 증명 $\sqrt[n]{n}$ 대신 $\ln \sqrt[n]{n}$의 극한을 구하면 쉽다. $$ \lim\limits_{n \to \infty} \ln \sqrt[n]{n} = \lim\limits_{n \to \infty} \dfrac{\ln n}{n} $$ $\dfrac{\infty}{\infty}$ 꼴이므로 로피탈 정</description></item><item><title>Convergence of Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1665/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1665/</guid><description>Theorem1 Power Series $\sum\limits_{n = 0}^{\infty} c_{n} (x - a)^{n}$&amp;rsquo;s radius of convergence is $R$. Then, The series converges absolutely within $x \in (a - R, a + R)$. The series converges uniformly within any closed interval $[b, d] \subset (a - R, a + R)$. Regarding $(R \lt \infty$, the series diverges beyond $x \notin [a - R, a + R]$. Explanation Refer to here for the proof of</description></item><item><title>Differentiation of Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1704/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1704/</guid><description>정리1 멱급수 $\sum\limits_{n = 0}^{\infty} c_{n}x^{n}$이 $\left| x \right| \lt R$에서 수렴한다고 하자. 그리고 함수 $f$를 다음과 같이 정의하자. $$ f(x) = \sum\limits_{n = 0}^{\infty} c_{n}x^{n} \qquad \left| x \right| \lt R \tag{1}</description></item><item><title>Convergence Properties of Series</title><link>https://freshrimpsushi.github.io/en/posts/1737/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1737/</guid><description>정리 두 급수 $\sum a_{n}$과 $\sum b_{n}$이 수렴하면, 급수 $\sum c a_{n}$ ($c$는 상수), $\sum (a_{n} \pm b_{n})$도 수렴하고 다음이 성립한다. $\sum\limits_{n = 1}^{\infty} c a_{n} = c</description></item><item><title>p-Series and the p-Series Test</title><link>https://freshrimpsushi.github.io/en/posts/1754/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1754/</guid><description>정의1 다음과 같은 급수를 $p$-급수라고 한다. $$ \sum\limits_{n=1}^{\infty} \dfrac{1}{n^{p}} $$ 설명 역제곱수의 무한합에 대한 일반화이다. 아래에서 소개할 판정법은 $p$-급수에 대해서만 쓸 수 있</description></item><item><title>Limit Comparison Test</title><link>https://freshrimpsushi.github.io/en/posts/1756/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1756/</guid><description>Summary1 Given two series $\sum a_{n}$ and $\sum b_{n}$, let us assume $a_{n}, b_{n} \gt 0$. If there exists a positive number $c \gt 0$ such that $$ \lim\limits_{n \to \infty} \dfrac{a_{n}}{b_{n}} = c $$ is satisfied, then either both series converge, or both diverge. Explanation This is called the limit comparison test. The comparison test is intuitive and useful, but it can only determine the convergence of a series</description></item><item><title>Comparison Test</title><link>https://freshrimpsushi.github.io/en/posts/1759/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1759/</guid><description>발췌한 문서: 정리1 두 급수 $\sum a_{n}$과 $\sum b_{n}$에 대해서 $a_{n}, b_{n} \gt 0$이라 하자. 그러면 다음이 성립한다. 만약 $\forall n \ a_{n} \le b_{n}$이고 $\sum b_</description></item><item><title>Absolute Convergence and Conditional Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1760/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1760/</guid><description>정의1 급수 $\sum\limits_{n=0}^{\infty} a_{n}$에 대해서, $\sum\limits_{n=0}^{\infty} |a_{n}|$이 수렴하면 $\sum\limits_{n=0}^{\infty} a_{n}$이 절대수렴한다고 말한다. 설명 주의할 점은 주어진 급수가 아니라 &amp;ldq</description></item><item><title>Ratio Test</title><link>https://freshrimpsushi.github.io/en/posts/1771/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1771/</guid><description>Theorem1 For the series $\sum\limits_{n=0}^{\infty} a_{n}$, let $\lim\limits_{n \to \infty} \left| \dfrac{a_{n+1}}{a_{n}} \right| = L$. (a) If $L &amp;lt; 1$, the series absolutely converges. (b) If $L &amp;gt; 1$ or $L = \infty$, the series diverges. (c) If $L = 1$, the convergence cannot be determined. Explanation If $L = 1$, the convergence cannot be determined, and another method must be used to judge whether the series converges or diverges.</description></item><item><title>Root Test</title><link>https://freshrimpsushi.github.io/en/posts/1779/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1779/</guid><description>Summary1 For the series Series $\sum\limits_{n=0}^{\infty} a_{n}$, let $\lim\limits_{n \to \infty} \sqrt[n]{\left| a_{n} \right|} = L$ be given. (a) If $L &amp;lt; 1$, the series absolutely converges. (b) If $L &amp;gt; 1$ or $L = \infty$, the series diverges. (c) If $L = 1$, it cannot be determined. Explanation If $L = 1$, it cannot be determined, so other methods must be used to judge whether the series converges or</description></item><item><title>Radius of Convergence of Power Series</title><link>https://freshrimpsushi.github.io/en/posts/1791/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1791/</guid><description>Summary1 For the given power series $\sum\limits_{n=0}^{\infty} c_{n}(x - a)^{n}$, let $\alpha$ and $R$ be defined as follows. $$ \alpha = \limsup\limits_{n \to \infty} \sqrt[n]{|c_{n}|}, \qquad R = \dfrac{1}{\alpha} $$ Then the series converges if $\left| x - a \right| \lt R$, and diverges if $\left| x - a \right| \gt R$. If $\alpha = 0$, let $R = \infty$, and if $\alpha = \infty$, let $R = 0$. Definition</description></item><item><title>Notation and Naming Conventions of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1825/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1825/</guid><description>Definition The hyperbolic sine function is defined as the linear combination of two exponential functions $\frac{1}{2}e^{x} - \frac{1}{2}e^{-x}$ and is denoted as follows: $$ \sinh x := \dfrac{e^{x} - e^{-x}}{2} $$ Similarly, the hyperbolic cosine function is defined as the linear combination of two exponential functions $\frac{1}{2}e^{x} + \frac{1}{2}e^{-x}$ and is denoted as follows: $$ \cosh x := \dfrac{e^{x} + e^{-x}}{2} $$ Explanation The names and notations of $\sinh$ and</description></item><item><title>Uniform Convergence and Integrability of Function Series</title><link>https://freshrimpsushi.github.io/en/posts/1829/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1829/</guid><description>정리1 구간 $[a, b]$에서 적분가능한 함수들의 수열 $\left\{ f_{n} : f_{n} \text{ is integrable on } [a, b] \right\}$이 $[a, b]$에서 $f$로 균등 수렴한다고 하자. $$ f_{n} \rightrightarrows f $$ 그러</description></item><item><title>Uniform Convergence and Differentiation</title><link>https://freshrimpsushi.github.io/en/posts/1836/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1836/</guid><description>Theorem1 $$ f^{\prime} (x) = \lim_{n \to \infty} f_{n}^{\prime} (x) \quad a \le x \le b $$ Explanation2 The result of the theorem can be summarized as &amp;ldquo;the limit of the derivatives is equal to the derivative of the limits&amp;rdquo;. In other words, it is possible to interchange the limit symbol and the differentiation symbol. $$ \dfrac{d}{dx} \lim\limits_{n \to \infty} f_{n} (x) = \lim\limits_{n \to \infty} \dfrac{d}{dx} f_{n} (x) \quad</description></item><item><title>Special Angles of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/1849/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1849/</guid><description>공식 몇몇의 특수한 각도에 대한 삼각함수의 함숫값은 다음과 같다. 라디안(각도) $0$ $\frac{\pi}{12} (15^{\circ})$ $\frac{\pi}{6} (30^{\circ})$ $\frac{\pi}{4} (45^{\circ})$ $\frac{\pi}{3} (60^{\circ})$ $\frac{\pi}{2} (90^{\circ})$ $\sin$ $0$ $\dfrac{\sqrt{6} - \sqrt{2}}{4}$ $\dfrac{1}{2}$ $\dfrac{\sqrt{2}}{2}$ $\dfrac{\sqrt{3}}{2}$ $1$ $\cos$ $1$ $ \dfrac{\sqrt{6} + \sqrt{2}}{4}$ $\dfrac{\sqrt{3}}{2}$ $\dfrac{\sqrt{2}}{2}$ $\dfrac{1}{2}$ $0$ $\tan$ $0$ $2 - \sqrt{3}$</description></item><item><title>Taylor Series and Maclaurin Series</title><link>https://freshrimpsushi.github.io/en/posts/1854/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1854/</guid><description>Build-Up1 Assume that the given function $f$ is expressed as a power series. $$ f(x) = c_{0} + c_{1}(x - a) + c_{2}(x - a)^{2} + c_{3}(x - a)^{3} + \cdots \qquad |x - a| \lt R \tag{1} $$ In this context, finding the power series representation of the function $f$ is equivalent to determining the coefficients of each term $c_{n}$. By substituting $x = a$ into both sides, we</description></item><item><title>How to Read Text (txt) Files as Strings in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3727/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3727/</guid><description>Description There are several ways to load a text file. A simple approach is to use the functions read(), readline(), and readlines(). Among these, read() reads the entire file and returns a string, readline() reads the first line from the file and returns a string, and readlines() reads all lines of the file and returns a vector whose elements are the strings of each line. If you do not specify</description></item><item><title>Applications of Taylor Series</title><link>https://freshrimpsushi.github.io/en/posts/1861/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1861/</guid><description>Explanation Taylor series (Maclaurin series) is an approximation of a given function as a power series, and the Taylor series of function $f$ is as follows. $$ \sum\limits_{n=0}^{\infty} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} = f(a) + f^{\prime}(a)(x-a) + \dfrac{f^{\prime \prime}(a)}{2!} (x-a)^{2} + \cdots $$ Under good conditions, $f$ and its Taylor series are indeed the same. $$ f(x) = \sum\limits_{n=0}^{\infty} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} = f(a) + f^{\prime}(a)(x-a) + \dfrac{f^{\prime \prime}(a)}{2!} (x-a)^{2} + \cdots $$</description></item><item><title>Recurrence Relation</title><link>https://freshrimpsushi.github.io/en/posts/1865/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1865/</guid><description>Definition Let&amp;rsquo;s consider a sequence $\left\{ a_{n} \right\}$. At this time, expressing $a_{n}$ as a function of $a_{n-1}$, $a_{n-2}$, $\cdots$, and $a_{1}$ is called a recurrence relation. Explanation For instance, the sequence of natural numbers $\left\{ 1, 2, 3, 4, \dots \right\}$ can be expressed by the following recurrence relation. $$ a_{n} = a_{n-1} + 1, \qquad a_{1} = 1 $$ The coefficients of the Legendre polynomial are expressed by</description></item><item><title>Continued Fraction</title><link>https://freshrimpsushi.github.io/en/posts/1876/</link><pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1876/</guid><description>Definition A fraction of the form as shown below is called a continued fraction. $$ a_{0} + \dfrac{1}{a_{1} + \dfrac{1}{a_{2} + \dfrac{1}{a_{3} + \dfrac{1}{\ddots + \dfrac{1}{a_{n}}}}}} \tag{1} $$ Explanation1 2 $(1)$ is denoted as $[a_{1}, a_{2}, \dots, a_{n}]$. Naturally, one can also consider taking the limit of it. For example, let&amp;rsquo;s consider a sequence with the recurrence relation given by $a_{n+1} = 1 + \dfrac{1}{1 + a_{n}}$ and $a_{1} =</description></item><item><title>Integral Test</title><link>https://freshrimpsushi.github.io/en/posts/1900/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1900/</guid><description>빌드업1 $$ \sum\limits_{n=1}^{\infty} \dfrac{1}{n^{2}} = 1 + \dfrac{1}{2^{2}} + \dfrac{1}{3^{2}} + \dfrac{1}{4^{2}} + \cdots $$ 위과 같은 급수가 수렴하는지 발산하는지 알고싶은 상황이라고 하자. 이를 위해 $\dfrac{1}{n^{2}} = f(n)$을 만족하는 함수를 생각하자</description></item><item><title>Subsequence</title><link>https://freshrimpsushi.github.io/en/posts/1905/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1905/</guid><description>정의 수열 $\left\{ a_{n} \right\}$이 주어졌다고 하자. 자연수의 수열 $\left\{ n_{k} : n_{i} \lt n_{i+1} \right\}_{ k \in \mathbb{N}}$ 에 대해 $\left\{ a_{n_{k}} \right\}_{ k \in \mathbb{N}}$ 를 $\left\{ a_{n} \right\}_{ n \in \mathbb{N}}$ 의 부분수열이라 한다. 만약 부분</description></item><item><title>Limits of Subsequences and Convergence of Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1911/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1911/</guid><description>정리 수열 $\left\{ a_{n} \right\}$이 주어졌다고 하자. 두 부분수열 $\left\{ a_{2n} \right\}$과 $\left\{ a_{2n+1} \right\}$에 대해서, $\lim\limits_{n \to \infty} a_{2n} = L$이고 $\lim\limits_{n \to \infty} a_{2n+1}</description></item><item><title>Alternating Series</title><link>https://freshrimpsushi.github.io/en/posts/1925/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1925/</guid><description>Definition A series in which the sign of each term alternates is called an alternating series. In other words, for $b_{n} \gt 0$, a series whose general term is expressed in the following form. $$ a_{n} = (-1)^{n-1}b_{n} \qquad \text{ or } \qquad a_{n} = (-1)^{n}b_{n} $$ Explanation One method to determine the convergence of an alternating series is the Alternating Series Test. Alternating Series Test An alternating series $\sum\limits_{n</description></item><item><title>Alternating Series Test</title><link>https://freshrimpsushi.github.io/en/posts/1927/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1927/</guid><description>Summary1 The alternating series satisfying the following conditions $\sum\limits_{n = 1}^{\infty} (-1)^{n-1}b_{n}$ $(b_{n} \gt 0)$ converges. $b_{n+1} \le b_{n} \quad \forall n$. $\lim\limits_{n \to \infty} b_{n} = 0$. Proof First, consider the partial sum up to the even terms. $$ \begin{align*} s_{2} &amp;amp;= b_{1} - b_{2} \ge 0 \\ s_{4} &amp;amp;= s_{2} + (b_{3} - b_{4}) \ge s_{2} \\ s_{6} &amp;amp;= s_{4} + (b_{5} - b_{6}) \ge s_{4} \\ &amp;amp;\quad</description></item><item><title>Alternating Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/1928/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1928/</guid><description>Definition The following series is called the alternating harmonic series. $$ \sum\limits_{n = 1}^{\infty} (-1)^{n-1}\dfrac{1}{n} = 1 - \dfrac{1}{2} + \dfrac{1}{3} - \dfrac{1}{4} + \cdots $$ Convergence The alternating harmonic series converges. $$ \sum\limits_{n = 1}^{\infty} (-1)^{n-1}\dfrac{1}{n} = \ln 2 $$ Explanation On the other hand, the harmonic series diverges. $$ \sum\limits_{n = 1}^{\infty} \dfrac{1}{n} = 1 + \dfrac{1}{2} + \dfrac{1}{3} + \dfrac{1}{4} + \cdots = \infty $$</description></item><item><title>Harmonic Series</title><link>https://freshrimpsushi.github.io/en/posts/1938/</link><pubDate>Sat, 12 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1938/</guid><description>Definition The following series is called the harmonic series. $$ \sum\limits_{n = 1}^{\infty} \dfrac{1}{n} = 1 + \dfrac{1}{2} + \dfrac{1}{3} + \dfrac{1}{4} + \cdots $$ Explanation It is a representative counterexample to the divergence test. That is, the harmonic sequence converges, but the harmonic series diverges. $$ \lim\limits_{n \to \infty} \dfrac{1}{n} = 0 \quad \text{ but } \quad \sum\limits_{n = 1}^{\infty} \dfrac{1}{n} = \infty $$ On the other hand, the</description></item><item><title>Divergence Test</title><link>https://freshrimpsushi.github.io/en/posts/1940/</link><pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1940/</guid><description>Summary If the series $\sum\limits_{n = 1}^{\infty} a_{n}$ converges, then the sequence $\{a_{n}\}$ converges to $0$. $$ \sum\limits_{n = 1}^{\infty} a_{n} \text{ is convergent } \implies \lim\limits_{n \to \infty} a_{n} = 0 $$ Proof Let the sum of the series be $\sum\limits_{n = 1}^{\infty} a_{n} = s$. That is, for the partial sum $s_{n}$, it is $\lim\limits_{n \to \infty} s_{n} = s$. Then, since $a_{n} = s_{n} - s_{n-1}$, $$</description></item><item><title>Limits of Geometric Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1949/</link><pubDate>Tue, 08 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1949/</guid><description>Summary The geometric sequence $\left\{ r^{n} \right\}$ converges to $-1 \lt r \le 1$, and its value is as follows: $$ \lim\limits_{n \to \infty} r^{n} = \begin{cases} 0 &amp;amp; \text{if } -1 \lt r \lt 1 \\ 1 &amp;amp; \text{if } r = 1 \end{cases} $$ Proof $r = 1$ If $r = 1$, $$ \lim\limits_{n \to \infty} 1^{n} = \lim\limits_{n \to \infty} 1 = 1 $$ ■ $-1 \lt</description></item><item><title>Geometric Series</title><link>https://freshrimpsushi.github.io/en/posts/1951/</link><pubDate>Sun, 06 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1951/</guid><description>Definition1 The following series is called a geometric series for $a \ne 0$. $$ a + ar + ar^{2} + ar^{3} + \cdots = \sum_{n=0}^{\infty} ar^{n} $$ Explanation It is the infinite sum of a geometric sequence with the first term $a$ and the common ratio $r$. The ▷eq04 Definition1 The following series is called a geometric series for $a \ne 0$. $$ a + ar</description></item><item><title>How to Remove Indentation in the Entire Document in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1953/</link><pubDate>Fri, 04 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1953/</guid><description>Code Add \setlength{\parindent}{0pt}. \documentclass{article} \begin{document} Me and my girlies. We gon party til its early. Got me feeling otherworldly tonight. Caught in some traffic. But the radio is blasting. Drop a red light and we&amp;#39;ll sing it goodbye. Ooh. By the morning, feel like magic. I got all I need you know nothing else can beat. The way that I feel when I&amp;#39;m dancing with my girls. Perfect energy yeah</description></item><item><title>ulem: Packages Related to Underlining in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1959/</link><pubDate>Wed, 02 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1959/</guid><description>Description1 The ulem package is related to underline functions $\LaTeX$. It allows you to use underline, double underline, wavy underline, strikethrough, diagonal strikethrough, dashed underline, and dotted underline. Code \documentclass[10pt]{article} \usepackage{ulem} \begin{document} underlined text like \uline{important} \\ % 밑줄 double-underlined text like \uuline{urgent} \\ % 겹밑줄 wavy underline like \uwave{boat} \\ % 물결 밑줄 line struck through word like \sout{wrong} \\ % 취소선</description></item><item><title>How to Use Strikethrough in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1960/</link><pubDate>Mon, 30 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1960/</guid><description>Description1 You can apply strikethrough using either the \sout from the ulem package or the st from the soul package. Code \sout It works well with equations, but it doesn&amp;rsquo;t seem to work properly in display mode. \documentclass[10pt]{article} \usepackage{amsmath, amssymb} \usepackage{ulem} \begin{document} \sout{The ulem package provides various types of underlining} that can stretch between words and be broken across lines. Use it with {\LaTeX} or plain \TeX. \\ \sout{$x^{2} +</description></item><item><title>How to Order Alphabetically with enumerate in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1961/</link><pubDate>Sat, 28 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1961/</guid><description>Code You need to add \usepackage{enumitem}. \begin{enumerate}[label=(\alph*)] \item Lorem Ipsum \item Lorem Ipsum is simply dummy text of the printing and typesetting industry. \item Lorem Ipsum has been the industry&amp;#39;s standard dummy text ever since the 1500s, \item when an unknown printer took a galley of type and scrambled it to make a type specimen book. \end{enumerate} \begin{enumerate}[label=(\Alph*)] \item Lorem Ipsum \item Lorem Ipsum is simply dummy text of the</description></item><item><title>How to Pass Multiple Keyword Arguments at Once Using a Dictionary in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1962/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1962/</guid><description>Explanation Using a dictionary and a splat operator, you can pass multiple keyword arguments to a function at once. This technique is useful when you need to apply the same options to multiple plots. If all the plots need the same options, using the function default is straightforward. However, if even one plot needs to be drawn with a different style, using default can become inconvenient. Code You can create</description></item><item><title>Parametric Equation</title><link>https://freshrimpsushi.github.io/en/posts/1963/</link><pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1963/</guid><description>Build-Up Let&amp;rsquo;s consider the situation of expressing the position of a particle on a two-dimensional plane in a formula. The path of the particle&amp;rsquo;s movement is shown in the following figure. It is impossible to represent the path in the above figure as a function of $x$, i.e., in the form of $y = f(x)$. This is because there are multiple $y$ values corresponding to points like $x_{0}$. (A function</description></item><item><title>Implementing DeepONet Paper Step by Step (PyTorch)</title><link>https://freshrimpsushi.github.io/en/posts/1153/</link><pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1153/</guid><description>Overview DeepONet is a neural network architecture for learning nonlinear operators and has been applied in various fields such as solving partial differential equations since its paper was published. In this article, we introduce how to implement DeepONet using PyTorch and follow the problems presented in the paper. Paper Review Implementation in Julia DeepONet Theory Let $X$, $X^{\prime}$ be function spaces, and operator $G : X \to X^{\prime}$ be as</description></item><item><title>Comprehensive Guide to File I/O in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3707/</link><pubDate>Sat, 21 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3707/</guid><description>Overview This document summarizes packages and functions related to file I/O in Julia. Summary CSV: CSV.jl Reading CSV.read(&amp;quot;file_name.csv&amp;quot;, DataFrame) CSV.File(&amp;quot;file_name.csv&amp;quot;) Writing: CSV.write(&amp;quot;file_name.csv&amp;quot;, df_data) JSON: JSON3.jl, JSON.jl Reading String: read(&amp;quot;file_name.json&amp;quot;, String) JSON3.Object: JSON3.read(cd*&amp;quot;/wonnyo.json&amp;quot;) Dictionary: JSON3.read(cd*&amp;quot;/wonnyo.json&amp;quot;) |&amp;gt; Dict Writing JSON3.write(&amp;quot;file_name.json&amp;quot;, json_data) open(&amp;quot;file_name.json&amp;quot;, &amp;quot;w&amp;quot;) do io; JSON3.pretty(io, json_data); end Pickle: PyCall.jl Reading: open(&amp;quot;file_name.pkl&amp;quot;) do f; pickle.load(f); end Writing: open(&amp;quot;file_name.pkl&amp;quot;, &amp;quot;w&amp;quot;) do f; pickle.dump(dict_data, f); end Numpy: PyCall.jl Reading: open(joinpath(cd, &amp;quot;file_name.npy&amp;quot;)) do f; np.load(f);</description></item><item><title>How to Use Automatic Differentiation in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/1966/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1966/</guid><description>Explanation We introduce how to perform automatic differentiation in PyTorch. In PyTorch, automatic differentiation is implemented through the torch.autograd.grad function. torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=None, is_grads_batched=False, materialize_grads=False) outputs: The function value for which the gradient (automatic differentiation) is calculated. inputs: The point at which the gradient is calculated. grad_outputs: This is the element to be multiplied by the calculated gradient. Usually, it is set to torch.ones_like(outputs). For instance,</description></item><item><title>파이토치로 PINN 논문 구현하기</title><link>https://freshrimpsushi.github.io/en/posts/1967/</link><pubDate>Mon, 16 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1967/</guid><description>Description PINN stands for Physics-Informed Neural Networks and refers to numerically solving differential equations using automatic differentiation and artificial neural networks. In the PINN paper, it has been implemented with TensorFlow. This article explains how to implement it using PyTorch. It proceeds under the assumption that you have read the following two articles. Review of the PINN paper How to use automatic differentiation in PyTorch Schrö</description></item><item><title>NamedArrays.jl Package in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1968/</link><pubDate>Sat, 14 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1968/</guid><description>Description Julia&amp;rsquo;s NamedArrays.jl package allows the use of 2D arrays with named rows and columns. Although it is also applicable to arrays with three or more dimensions, this article focuses on 2D arrays. Code Definition When inputting an array of size $3 \times 4$ into the NamedArray function, it outputs the array with names attached to rows and columns. julia&amp;gt; using NamedArrays julia&amp;gt; X = reshape(1:12, (3, 4)) 3×</description></item><item><title>How to Define an Array by Specifying Its Type in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1969/</link><pubDate>Thu, 12 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1969/</guid><description>Code To specify the type in front of the brackets, simply write the type name. julia&amp;gt; [1, 2, 3] 3-element Vector{Int64}: 1 2 3 julia&amp;gt; Float64[1, 2, 3] 3-element Vector{Float64}: 1.0 2.0 3.0 julia&amp;gt; Complex{Float64}[1, 2, 3] 3-element Vector{ComplexF64}: 1.0 + 0.0im 2.0 + 0.0im 3.0 + 0.0im julia&amp;gt; Char[1, 2, 3] 3-element Vector{Char}: &amp;#39;\x01&amp;#39;: ASCII/Unicode U+0001 (category Cc: Other, control) &amp;#39;\x02&amp;#39;: ASCII/Unicode U+0002 (category Cc: Other, control) &amp;#39;\x03&amp;#39;: ASCII/Unicode</description></item><item><title>Comprehensions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1970/</link><pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1970/</guid><description>Code In Julia, like in Python, comprehension is possible. Comprehension is a method of creating arrays that involves embedding conditional expressions directly into the array. For instance, if you want to define an array containing integers sequentially from $0$ to $9$, you can embed a for loop directly into the array. julia&amp;gt; [i for i ∈ 0:9] 10-element Vector{Int64}: 0 1 2 3 4 5 6 7 8 9 julia&amp;gt;</description></item><item><title>Methods for Calculating Arrays Column-wise in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1971/</link><pubDate>Sun, 08 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1971/</guid><description>Code There are methods like map, broadcast, and comprehension for calculating arrays by column or row. julia&amp;gt; using Statistics julia&amp;gt; X = stack([i*ones(8) for i ∈ 1:9], dims=2) 8×9 Matrix{Float64}: 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 1.0 2.0</description></item><item><title>Differences Between Vectors and Tuples in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1972/</link><pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1972/</guid><description>Description Vectors and tuples may appear similar at first glance, and they indeed share several common points, which might lead one to think, &amp;quot;Are they just the same concept with different names?&amp;quot; However, there are a few important differences between vectors and tuples. This article will primarily focus on the similarities and differences from the perspective of writing and using code. The mathematical basis for tuples in Julia and how</description></item><item><title>Comparing NaN in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1973/</link><pubDate>Wed, 04 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1973/</guid><description>Description In Julia, the standard for floating-point numbers follows the IEEE 754 standard. Under this rule, NaN always returns false when compared with all other values. It only returns true when using the != and !== operators. Code julia&amp;gt; NaN &amp;gt; 1 false julia&amp;gt; NaN ≥ 2 false julia&amp;gt; NaN == 3 false julia&amp;gt; NaN ≤ 4 false julia&amp;gt; NaN &amp;lt; 5 false julia&amp;gt; NaN != 6 true julia&amp;gt; NaN</description></item><item><title>Fall 2024 Omakase: Dual Numbers and Automatic Differentiation</title><link>https://freshrimpsushi.github.io/en/posts/1484/</link><pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1484/</guid><description>Introduction Hello, esteemed guests. For those of you disappointed that the magical moments of Cheoseo haven&amp;rsquo;t arrived, we have prepared a special omakase menu just for you. This course will introduce a variety of writings under the theme of &amp;ldquo;Dual Numbers and Automatic Differentiation.&amp;rdquo; Just like enjoying an omakase meal, savor each one in order to deepen your mathematical thinking. Menu Dual Numbers First, as a starter for this omakase,</description></item><item><title>How to Set Training and Testing Modes for Neural Networks in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/1975/</link><pubDate>Sat, 31 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1975/</guid><description>Description In the architecture of a neural network, there are components that must operate differently during training and testing phases. For instance, dropout should be applied during training but must not be applied during testing or actual use after training is complete. In such cases, it is necessary to distinguish between training mode and testing mode. Code The function to switch the neural network to training mode is trainmode!, and</description></item><item><title>Derivative of the Inverse Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1976/</link><pubDate>Thu, 29 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1976/</guid><description>Formulas1 The derivatives of the inverse hyperbolic functions are as follows: $$ \begin{align*} \dfrac{d}{dx} (\sinh^{-1} x) &amp;amp;= \dfrac{1}{\sqrt{x^{2} + 1}} \qquad &amp;amp; \dfrac{d}{dx} (\csch^{-1} x) &amp;amp;= - \dfrac{1}{|x|\sqrt{x^{2} + 1}} \\ \dfrac{d}{dx} (\cosh^{-1} x) &amp;amp;= \dfrac{1}{\sqrt{x^{2} - 1}} \qquad &amp;amp; \dfrac{d}{dx} (\sech^{-1} x) &amp;amp;= - \dfrac{1}{x\sqrt{1 - x^{2}}} \\ \dfrac{d}{dx} (\tanh^{-1} x) &amp;amp;= \dfrac{1}{1 - x^{2}} \qquad &amp;amp; \dfrac{d}{dx} (\coth^{-1} x) &amp;amp;= \dfrac{1}{1 - x^{2}} \end{align*} $$ Description The closed</description></item><item><title>Inverse Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1977/</link><pubDate>Tue, 27 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1977/</guid><description>Definition1 The inverse functions of hyperbolic functions are called inverse hyperbolic functions. $$ \begin{align*} y = \sinh^{-1} x &amp;amp;\iff \sinh y = x \\ y = \cosh^{-1} x &amp;amp;\iff \cosh y = x \\ y = \tanh^{-1} x &amp;amp;\iff \tanh y = x \\ \end{align*} $$ Closed Form The values of the inverse hyperbolic functions are concretely as follows. $$ \begin{align*} \sinh^{-1} x &amp;amp;= \ln \left( x + \sqrt{x^{2} +</description></item><item><title>Derivatives of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1978/</link><pubDate>Sun, 25 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1978/</guid><description>Formula1 The derivative of hyperbolic functions is as follows: $$ \begin{align*} \dfrac{d}{dx} (\sinh x) &amp;amp;= \cosh x \qquad &amp;amp; \dfrac{d}{dx} (\csch x) &amp;amp;= - \csch x \coth x \\ \dfrac{d}{dx} (\cosh x) &amp;amp;= \sinh x \qquad &amp;amp; \dfrac{d}{dx} (\sech x) &amp;amp;= - \sech x \tanh x \\ \dfrac{d}{dx} (\tanh x) &amp;amp;= \sech^{2} x \qquad &amp;amp; \dfrac{d}{dx} (\coth x) &amp;amp;= - \csch^{2} x \end{align*} $$ Proof Since hyperbolic functions are linear</description></item><item><title>Trigonometric Identities</title><link>https://freshrimpsushi.github.io/en/posts/1979/</link><pubDate>Fri, 23 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1979/</guid><description>Formula The following identity holds for trigonometric functions. $$ \begin{align} \cos^{2} x + \sin^{2} x &amp;amp;= 1 \\ 1 + \tan^{2} x &amp;amp;= \sec^{2} x \\ 1 + \cot^{2} x &amp;amp;= \csc^{2} x \end{align} $$ Proof $(1)$ From the addition formula of trigonometric functions, $$ \cos (x - y) = \cos x \cos y + \sin x \sin y $$ Substituting $y= x$, $$ \cos 0 = \cos^{2} x +</description></item><item><title>Derivatives of Inverse Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/1980/</link><pubDate>Wed, 21 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1980/</guid><description>Formulas Inverse trigonometric functions&amp;rsquo; derivatives are as follows. $$ \begin{align*} \dfrac{d}{dx} \sin ^{-1} x &amp;amp;= \dfrac{1}{\sqrt{1-x^2}} \qquad &amp;amp; \dfrac{d}{dx} \csc ^{-1} x &amp;amp;= -\dfrac{1}{x\sqrt{x^2-1}} \\ \dfrac{d}{dx} \cos ^{-1} x &amp;amp;= -\dfrac{1}{\sqrt{1-x^2}} \qquad &amp;amp; \dfrac{d}{dx} \sec ^{-1} x &amp;amp;= \dfrac{1}{x\sqrt{x^2-1}} \\ \dfrac{d}{dx} \tan ^{-1} x &amp;amp;= \dfrac{1}{1+x^2} \qquad &amp;amp; \dfrac{d}{dx} \cot ^{-1} x &amp;amp;= -\dfrac{1}{1+x^2} \end{align*} $$ Proof Differentiation of trigonometric functions $$ \begin{align*} \dfrac{d}{dx} \sin x &amp;amp;= \cos x \qquad</description></item><item><title>줄리아 미분방정식 패키지 DiffetentialEquations 튜토리얼</title><link>https://freshrimpsushi.github.io/en/posts/1098/</link><pubDate>Sun, 18 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1098/</guid><description>Description DifferentialEquations.jl is one of the packages under the SciML group developed for the numerical solution of differential equations. The equations that can be solved with this package are as follows: Discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations) Ordinary Differential Equations (ODEs) Split and Partitioned ODEs (Symplectic integrators, IMEX Methods) Stochastic Differential Equations (SDEs) Stochastic differential-algebraic equations (SDAEs) Random differential equations (RODEs or RDEs) Differential algebraic equations (DAEs) Delay</description></item><item><title>How to Use Quotation Marks in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1982/</link><pubDate>Sat, 17 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1982/</guid><description>Code In $\LaTeX$, opening and closing quotation marks exist separately. The opening quotation mark is `, located to the left of the number key 1 and above the tab key. The closing quotation mark is &amp;rsquo;, located to the left of the Enter key. &amp;#39;Example with only closing quotation marks 1&amp;#39; &amp;#34;Example with only closing quotation marks 2&amp;#34; `Example with both opening and closing quotation marks used separately&amp;#39; ``To type</description></item><item><title>How to render text, formulas, pictures, and more transparently in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/1983/</link><pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1983/</guid><description>Code The part wrapped in \phantom{} is rendered transparently. It remains transparent while still occupying its original place. If the original text is as follows, Using \phantom results in the following. The full code is as follows. \documentclass{article} \usepackage{graphicx} \usepackage{amsmath} \usepackage{kotex} \begin{document} Let $f$ be defined \phantom{(and real-valued) on $[a, b ]$}. For any $x \in [a, b]$ form the quotient $$ \phi(t) = \dfrac{f(t) - f(x)}{t - x} \qquad</description></item><item><title>양자 조화 진동자의 사다리 연산자</title><link>https://freshrimpsushi.github.io/en/posts/1984/</link><pubDate>Tue, 13 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1984/</guid><description>Definition Let&amp;rsquo;s define two operators $a_{+}$ and $a_{-}$ as follows. Then $a_{\pm}$ is the ladder operator of the Hamiltonian in a quantum harmonic oscillator. $$ \begin{align*} a_{+} &amp;amp;= \dfrac{1}{\sqrt{2\hbar m \omega}}(- \i P + m\omega X) \\ a_{-} &amp;amp;= \dfrac{1}{\sqrt{2\hbar m \omega}}(+ \i P + m\omega X) \end{align*} $$ Here, $P$ is the momentum operator, $X$ is the position operator, $\hbar$ is the Planck constant, $m$ is the mass of</description></item><item><title>Paper Review: DeepONet</title><link>https://freshrimpsushi.github.io/en/posts/1180/</link><pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1180/</guid><description>Overview and Summary Follow the references, equation numbers, and notation in the paper as closely as possible. For accessibility, this review is based on the version available on arXiv rather than the journal published version. Although the problems covered in the experimental section differ slightly, the core focus is not on the experimental results and performance but on the explanation of the DeepONet method itself. DeepONet is a deep learning</description></item><item><title>Variables Separable Function</title><link>https://freshrimpsushi.github.io/en/posts/1985/</link><pubDate>Fri, 09 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1985/</guid><description>Definition For a multivariable function, if there exists an $g_{i}$ that satisfies the following equation $f$, it is said to be separable in variables. $$ f(x_{1}, x_{2}, \dots, x_{n}) = g_{1}(x_{1}) g_{2}(x_{2}) \cdots g_{n}(x_{n}) $$ Description In simple terms, separating variables means expressing something as a product of functions that depend only on each variable. This assumption is often made when solving differential equations.</description></item><item><title>Cylindrical Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/1986/</link><pubDate>Wed, 07 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1986/</guid><description>Definition The coordinates of a point $p$ in coordinate space are represented as &amp;ldquo;the length $\rho$ of the line segment obtained by projecting the line between the origin and $p$ onto the $xy-$ plane&amp;rdquo; and &amp;ldquo;the angle $\phi$ formed by the line connecting the origin $O$ and the projection $p^{\prime}$ with the positive direction of the $x-$ axis&amp;rdquo; and &amp;ldquo;the value $z$ in the positive direction of the $z-$ axis</description></item><item><title>How to Change the Default Save Location for Excel, PowerPoint, and Word</title><link>https://freshrimpsushi.github.io/en/posts/1987/</link><pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1987/</guid><description>Description Click the [File] tab. Click the [Options] tab. Click the [Save] tab. Change the Default Local File Location (I) to your desired path. Changing it to the relative path ./ will directly open the file location when F12 is pressed, and the path remains unchanged even when saving multiple image files.</description></item><item><title>Cylindrical Coordinates에서의 Del 연산자</title><link>https://freshrimpsushi.github.io/en/posts/1988/</link><pubDate>Sat, 03 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1988/</guid><description>Formula The del operator in the cylindrical coordinate system is as follows. $$ \nabla = \dfrac{\partial}{\partial \rho} \widehat{\boldsymbol{\rho}} + \dfrac{1}{\rho}\dfrac{\partial}{\partial \phi} \widehat{\boldsymbol{\phi}} + \dfrac{\partial}{\partial z} \widehat{\mathbf{z}} $$ Description The del operator is not a vector, but for convenience, it is represented as above. Gradient: $$ \nabla f = \frac{\partial f}{\partial \rho}\boldsymbol{\hat \rho} + \frac{1}{\rho}\frac{\partial f}{\partial \phi}\boldsymbol{\hat \phi} + \frac{\partial f}{\partial z}\mathbf{\hat{\mathbf{z}}} $$ Divergence: $$ \begin{align*} \nabla \cdot \mathbf{F} &amp;amp;= \frac{1}{\rho}</description></item><item><title>Curvilinear Coordinate System and the Del Operator</title><link>https://freshrimpsushi.github.io/en/posts/1989/</link><pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1989/</guid><description>Definition In any curvilinear coordinate system where the coordinates are $(q_{1}, q_{2}, q_{3})$, the del operator is as follows. $$ \begin{align*} \nabla &amp;amp;= \dfrac{1}{h_{1}}\dfrac{\partial}{\partial q_{1}} \widehat{\mathbf{q}}_{1} + \dfrac{1}{h_{2}}\dfrac{\partial}{\partial q_{2}} \widehat{\mathbf{q}}_{2} + \dfrac{1}{h_{3}}\dfrac{\partial}{\partial q_{3}} \widehat{\mathbf{q}}_{3} \\ &amp;amp;= \sum\limits_{1}^{3} \dfrac{1}{h_{i}}\dfrac{\partial}{\partial q_{i}} \widehat{\mathbf{q}}_{i} \end{align*} $$ Here, $h_{i}$ represents the scale factor. Description The del operator is not a vector, but for convenience, it is represented as above. Cartesian Coordinate System In the Cartesian</description></item><item><title>Gradient, Divergence and Curl in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1990/</link><pubDate>Tue, 30 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1990/</guid><description>Formula The del operator in the spherical coordinates is as follows. $$ \nabla = \dfrac{\partial}{\partial r} \widehat{\mathbf{r}} + \dfrac{1}{r}\dfrac{\partial}{\partial \theta} \widehat{\boldsymbol{\theta}} + \dfrac{1}{r\sin\theta}\dfrac{\partial}{\partial \phi} \widehat{\boldsymbol{\phi}} $$ Description The del operator is not a vector, but for convenience, it is represented as above. Gradient: $$ \nabla f= \frac{\partial f}{\partial r} \mathbf{\hat r} + \frac{1}{r}\frac{\partial f}{\partial \theta} \boldsymbol{\hat \theta} + \frac{1}{r\sin\theta}\frac{\partial f}{\partial \phi}\boldsymbol{\hat \phi} $$ Divergence: $$ \begin{align*} \nabla \cdot \mathbf{F} &amp;amp;=</description></item><item><title>What is Flux in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/1991/</link><pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1991/</guid><description>Definition1 In physics, flux refers to the number of particles (or physical quantities such as energy, momentum, etc.) passing through (colliding with) a unit area per unit time. Flux is typically denoted by the capital letter pi $\Phi$. $$ \Phi = \dfrac{\text{physical quantity}}{\text{area} \times \text{time}} $$ Explanation Electric field flux: The flux of an electric field $\mathbf{E}$ passing through surface $\mathcal S$ is as follows: $$ \Phi_{E} = \int_{\mathcal S}</description></item><item><title>Hamiltonian Operator</title><link>https://freshrimpsushi.github.io/en/posts/1993/</link><pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1993/</guid><description>정의 양자역학에서 해밀토니안 연산자 $H$란 다음과 같다. $$ H = \dfrac{P^{2}}{2m} + V $$ 여기서 $P$는 운동량 연산자, $m$은 입자의 질량, $V$는 퍼텐셜이다. 설명 흔</description></item><item><title>What is a Ladder Operator in Quantum Mechanics?</title><link>https://freshrimpsushi.github.io/en/posts/1994/</link><pubDate>Wed, 24 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1994/</guid><description>Definition For an arbitrary operator $N$, let $n$ be its eigenvalue and $\ket{n}$ be the corresponding eigenfunction. $$ N \ket{n} = n \ket{n} $$ An operator $A$ that satisfies the following conditions is called the ladder operator corresponding to $N$. $$ \left[ N, A \right] = cA \tag{1} $$ Here, $c$ is a constant, and $[N, A]$ is the commutator. Explanation The reason why $A$ is called the ladder operator</description></item><item><title>Quantum Mechanics: The Position Operator</title><link>https://freshrimpsushi.github.io/en/posts/1995/</link><pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1995/</guid><description>Definition In quantum mechanics, the position operator $X$ is defined as follows for a wave function $\psi(x, t)$: $$ X \psi(x, t) = x \psi(x, t) $$ Description The equation where the position operator is applied to a wave function can be interpreted in quantum mechanics as the act of measuring (observing) the position of the wave function.</description></item><item><title>Rodrigues' Formula for Hermite Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/1996/</link><pubDate>Sat, 20 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1996/</guid><description>Official The explicit form of the Hermite polynomial is as follows. Physicist&amp;rsquo;s Hermite Polynomial $$ H_{n} = (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2} \tag{1} $$ Probabilist&amp;rsquo;s Hermite Polynomial $$ H_{e_{n}} = (-1)^{n} e^{{x^2} \over {2}} {{d^{n}} \over {dx^{n}}} e^{- {{x^2} \over {2}}} $$ Derivation The solution to the differential equation below $$ y_{n}^{\prime \prime} - x^{2}y_{n} = -(2n+1)y_{n} \tag{2} $$ is called the Hermite function, and is as follows. $$</description></item><item><title>Laguerre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/3630/</link><pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3630/</guid><description>정의 라게르 다항식은 다음과 같은 방법들로 정의된다. 미분방정식의 해로서 아래와 같은 라게르 미분방정식의 해를 라게르 다항함수라 한다. $$ xy^{\prime \prime} + (1-x)y^{\prime} + ny = 0, \quad n=0,1,2,\cdots</description></item><item><title>Customizing Chapter and Section Numbering in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3629/</link><pubDate>Tue, 16 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3629/</guid><description>Code The LaTeX code \setcounter{section}{n} sets the current section number to $n$. Since this counter increases by 1 with each added section, the order of the section created next becomes $n+1$. \documentclass{book} \begin{document} \chapter{Limits and Derivatives} \section{Continuity} \end{document} \documentclass{book} \begin{document} \setcounter{chapter}{1} \chapter{Limits and Derivatives} \setcounter{section}{4} \section{Continuity} \end{document}</description></item><item><title>프레드홀 적분 방정식</title><link>https://freshrimpsushi.github.io/en/posts/3628/</link><pubDate>Sun, 14 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3628/</guid><description>Definition1 The following integral equation is referred to as a Fredholm Integral Equation of the first kind. $$ g(s) = \int K(s, t) f(t) dt \tag{1} $$ Here, $K$ is called the kernel. The following form is referred to as the Fredholm integral equation of the second kind. $$ g(s) = f(s) + \int K(s, t) f(t) dt \tag{2} $$ Explanation Solving the integral equation $(1), (2)$ typically means finding</description></item><item><title>Creating a New Command in LaTeX That Acts Like a Function Using the begin...end Structure</title><link>https://freshrimpsushi.github.io/en/posts/3627/</link><pubDate>Fri, 12 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3627/</guid><description>Code Writing TeX code with \begin{}...\end{} statements can be quite bothersome. \documentclass{article} \usepackage{amsthm} \newtheorem*{theorem*}{Theorem} \begin{document} \begin{theorem*} Let $f$ and $g$ be continuous functions on a metric space $X$. Then $f + g$, $fg$, and $f/g$ are continuous on $X$. \end{theorem*} \end{document} By adding a new command like \newcommand{\thm}[1]{\begin{theorem*}#1\end{theorem*}}, it can be conveniently used. \documentclass{article} \usepackage{amsthm} \newtheorem*{theorem*}{Theorem} \newcommand{\thm}[1]{\begin{theorem*}#1\end{theorem*}} \begin{document} \thm{ Let $f$ and $g$ be continuous functions on a metric space</description></item><item><title>Difference Between torch.nn and torch.nn.functional in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3626/</link><pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3626/</guid><description>Description PyTorch contains many functions related to neural networks, which are included under the same names in torch.nn and torch.nn.functional. The functions in nn return a neural network as a function, while those in nn.functional are the neural network itself. For instance, nn.MaxPool2d takes the kernel size as input and returns a pooling layer. import torch import torch.nn as nn pool = nn.MaxPool2d(kernel_size = 2) # MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,</description></item><item><title>What are Weights in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/3625/</link><pubDate>Mon, 08 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3625/</guid><description>Definition In machine learning, the parameters to be optimized are called weights.</description></item><item><title>What is Skip Connection in Artificial Neural Networks?</title><link>https://freshrimpsushi.github.io/en/posts/3624/</link><pubDate>Sat, 06 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3624/</guid><description>Definition Let $\mathbf{W}$ be the weight, $\mathbf{x}$ the input, and $\sigma$ the nonlinear activation function. Let&amp;rsquo;s define layer $L_{\mathbf{w}}$ as follows. $$ L_{\mathbf{W}}(\mathbf{x}) := \sigma(\mathbf{W} \mathbf{x}) $$ A function in the form that adds an identity function to the layer like the following is called a skip connection. $$ L_{\mathbf{W}} + I : \mathbf{x} \mapsto \sigma(\mathbf{W} \mathbf{x}) + \mathbf{x} $$ Explanation Typically, the input $\mathbf{x}$ and the weight $\mathbf{W}$ are</description></item><item><title>Solution of Wave Equation with Zero Initial Condition</title><link>https://freshrimpsushi.github.io/en/posts/3623/</link><pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3623/</guid><description>Tidy up Let&amp;rsquo;s say that we have the following wave equation. where $\Delta_{\mathbf{x}}$ is Laplacian for the variable $\mathbf{x}$. $$ \begin{align} \partial_{t}^{2} p(\mathbf{x}, t) &amp;amp;= \Delta_{\mathbf{x}} p(\mathbf{x}, t) &amp;amp;\text{on } \mathbb{R} \times [0, \infty) \\ p(\mathbf{x}, 0) &amp;amp;= f(\mathbf{x}) &amp;amp;\text{on } \mathbb{R} \\ \partial_{t} p(\mathbf{x}, 0) &amp;amp;= 0 &amp;amp;\text{on } \mathbb{R} \end{align} $$ The solution of the above partial differential equation is as follows. $$ \begin{equation} p(\mathbf{x}, t) = \dfrac{1}{(2\pi)^{n}}</description></item><item><title>Rodrigues' Formula for Multiple Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/3622/</link><pubDate>Tue, 02 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3622/</guid><description>Description Rodrigues&amp;rsquo; formula originally referred to an explicit form of the Legendre polynomials, but later became the general term for formulae representing the explicit forms of special functions expressed in polynomials. Formulas Legendre Polynomials: $$ P_{l}(x)=\dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} $$ Laguerre Polynomials: $$ L_{n}(x) = \frac{1}{n!}e^{x}\frac{ d ^{n}}{ dx^{n} }(x^{n}e^{-x}) $$ Hermite Polynomials: $$ H_{n} = (-1)^{n} e^{x^2} {{d^{n}} \over {dx^{n}}} e^{-x^2} $$</description></item><item><title>What is a Special Function?</title><link>https://freshrimpsushi.github.io/en/posts/3621/</link><pubDate>Sun, 30 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3621/</guid><description>Description Special functions, as called in mathematics, are usually the solutions to certain differential equations, defined by complex integrals, can&amp;rsquo;t be expressed by elementary functions, or have mathematically interesting properties. They often bear names of people, letters of the alphabet, or Greek letters, and it can be said that almost all named functions, except polynomial, trigonometric, exponential, and logarithmic functions, are referred to as special functions. As you can see</description></item><item><title>Using AdaBelief Optimizer in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3620/</link><pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3620/</guid><description>Description AdaBelief, introduced by J. Zhuang et al. in 2020, is one of the variations of Adam1. Since PyTorch does not natively provide this optimizer, it must be installed separately. Code2 Installation The following command can be used to install it via cmd. pip install adabelief-pytorch==0.2.0 Usage The code below can be used to import and utilize it. from adabelief_pytorch import AdaBelief optimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple =</description></item><item><title>Uniform Continuity in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3618/</link><pubDate>Mon, 24 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3618/</guid><description>Definition1 Given two metric spaces $(X, d_{X})$, $(Y, d_{Y})$ and a sequence of functions $\left\{ f_{n} : X \to Y \right\}$. If for every $\varepsilon \gt$ there exists $\delta (\varepsilon) \gt 0$ satisfying the following condition, then the sequence $\left\{ f_{n} \right\}$ is called equicontinuous. $$ \forall x_{1}, x_{2} \in X \text{ and } f_{n}\quad d_{X}(x_{1}, x_{2}) \lt \delta (\varepsilon) \implies d_{Y} \big( f_{n}(x_{1}), f_{n}(x_{2}) \big) \lt \varepsilon $$ Explanation</description></item><item><title>Compact Integral Operators</title><link>https://freshrimpsushi.github.io/en/posts/3617/</link><pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3617/</guid><description>Theorem1 For a given compact interval $J = [a, b]$, let $K$ be a function continuous on $J \times J$. Let $X = C[a, b]$ be the space of continuous functions. Then, the integral operator $T : X \to X$ with kernel $K$ is a compact linear operator. $$ (Tx)(s) = \int\limits_{a}^{b} K(s, t) x(t) dt,\qquad \forall x \in X $$ Proof Since the integral operator is linear and bounded,</description></item><item><title>Integration Operator</title><link>https://freshrimpsushi.github.io/en/posts/3616/</link><pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3616/</guid><description>Definition1 For the space of continuous functions $C[0 ,1]$, the operator $T : C[0, 1] \to C[0, 1]$ defined as follows is called the integral operator. $$ y = Tx \qquad \text{where} \qquad y(s) = \int_{0}^{1} K(s, t) x(t) dt $$ Herein, $K$ is called the kernel of $T$. (It is assumed that the kernel $K$ is continuous on $[0, 1] \times [0, 1]$.) Explanation The integral operator is also</description></item><item><title>Compact Operator Equivalence Conditions</title><link>https://freshrimpsushi.github.io/en/posts/3615/</link><pubDate>Tue, 18 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3615/</guid><description>Theorem1 Let $X$ and $Y$ be normed spaces. Let $T : X \to Y$ be a linear operator. Then the following two propositions are equivalent. $T$ is a compact operator. $T$ maps &amp;ldquo;every bounded sequence in $X$&amp;rdquo; to &amp;ldquo;a sequence in $Y$ that has a convergent subsequence&amp;rdquo;. Proof $1. \Longrightarrow 2.$ Assume $T : X \to Y$ is compact. Let $\left\{ x_{n} \right\}$ be a bounded sequence. By the definition</description></item><item><title>The Spectrum and Decomposition Set of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3614/</link><pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3614/</guid><description>Definition1 The set $\sigma (A)$ of all eigenvalues of a square matrix $A$ is called the spectrum of $A$. The complement set $\rho (A) = \mathbb{C} \setminus \sigma (A)$ of the spectrum is called the resolvent set of $A$. Explanation $$ Ax = \lambda x $$ Consider the eigenvalue equation for matrix $A$. A vector $x$ that satisfies the above equation is called an eigenvector, and the constant $\lambda$ is</description></item><item><title>In a Metric Space, Compact Implies Closed and Bounded</title><link>https://freshrimpsushi.github.io/en/posts/3613/</link><pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3613/</guid><description>Theorem A compact subset $K$ of a metric space $(X, d)$ is bounded and a closed set. Description The converse does not generally hold. In Euclidean spaces, the converse holds. Proof Boundedness1 By contradiction, assume that $K$ is not bounded. Since being compact in a metric space is equivalent to being sequentially compact, $K$ is sequentially compact. Sequentially Compact A metric space $X$ being sequentially compact means that for every</description></item><item><title>Equivalence of Various Compactnesses in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3612/</link><pubDate>Wed, 12 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3612/</guid><description>Theorem1 Let $X$ be a metric space. The following propositions are equivalent. $X$ is compact. $X$ is countably compact. $X$ is limit point compact. $X$ is sequentially compact. Explanation This generally does not hold in topological spaces but holds in metric spaces. 박대희·안승호, 위상수학(하) (5/E, 2022), p503&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>What is Sequential Compactness in Topological Spaces?</title><link>https://freshrimpsushi.github.io/en/posts/3611/</link><pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3611/</guid><description>Definition1 A topological space $X$ is said to be sequentially compact if every sequence $\left\{ x_{n} \right\}$ in $X$ has a subsequence $\left\{ x_{n_{k}} \right\}$ that converges to a point in $X$. Explanation In general, in topological spaces, compactness and sequential compactness are independent. For example, there are spaces that are compact but not sequentially compact, and vice versa, spaces that are sequentially compact but not compact. In metric spaces,</description></item><item><title>Necessary and Sufficient Conditions for a Subset of a Normed Space to be Bounded</title><link>https://freshrimpsushi.github.io/en/posts/3610/</link><pubDate>Sat, 08 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3610/</guid><description>Definition Diameter Given a normed space $(X, \left\| \cdot \right\|)$, the diameter $\diam M$ of a non-empty subset $M \subset X$ is defined as follows. $$ \diam M =: \sup\limits_{x, y \in M} \left\| x - y \right\| $$ Bounded If $\diam M \lt \infty$ is satisfied, then $M$ is said to be bounded. Explanation In a normed space, since the metric can be naturally induced as $d (x, y)</description></item><item><title>Compact Action Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3609/</link><pubDate>Thu, 06 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3609/</guid><description>Definition1 Let $X$ and $Y$ be normed spaces, and let $T : X \to Y$ be an operator between these spaces. If for every bounded subset $M \subset X$, the image $T(M)$ of the operator $T$ is precompact, then $T$ is called a compact operator. Explanation That $T(M)$ is precompact means its closure $\overline{T(M)}$ is compact. In other words, a compact operator is an operator that maps bounded sets to</description></item><item><title>How to Use Color Gradients in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3608/</link><pubDate>Tue, 04 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3608/</guid><description>Description A color gradient is one of the two color schemes supported by Julia&amp;rsquo;s visualization package Plots.jl (the other is palette), which is what we commonly refer to as gradation. Simply put, a type that implements gradation is ColorGradient. Gradients are used to draw charts such as heatmap(), surface(), contour(). If you want to differentiate the colors of various graphs, use a palette instead of a gradient. Code Symbol It</description></item><item><title>How to Use Palettes in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3607/</link><pubDate>Sun, 02 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3607/</guid><description>Explanation A palette refers to a board where paints are squeezed out in advance. Mathematically, it can be explained as a &amp;lsquo;set of colors&amp;rsquo; or a &amp;lsquo;sequence of colors&amp;rsquo;. When drawing multiple graphs in one picture, the most common way is to distinguish them by using different colors. For this purpose, Julia has implemented a type called ColorPalette that collects various colors. It can be comfortably understood as a vector</description></item><item><title>How to Plot Two Data Axes of Different Scales in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3606/</link><pubDate>Fri, 31 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3606/</guid><description>Code When plotting two data sets that have a large scale difference on the same plot, the one with the smaller scale gets completely ignored as shown in the figure below. using Plots x = 0:0.01:2π plot(x, sin.(x)) plot!(x, exp.(x)) When plotting the second data set, if you input twinx() as the first argument, it shares the $x$ axis and the graph</description></item><item><title>List of Plot Properties in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3605/</link><pubDate>Wed, 29 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3605/</guid><description>Description In Julia&amp;rsquo;s Plots.jl, a plot is also an object. If you draw an empty plot to check its type, it looks like this. julia&amp;gt; using Plots julia&amp;gt; p = plot() julia&amp;gt; p |&amp;gt; typeof Plots.Plot{Plots.GRBackend} Removing Plots., it becomes Plot{GRBackend}, meaning the plot&amp;rsquo;s backend is GR, similar to how a vector with elements of type Float64 is denoted as Vector{Float64}. Checking the properties of Plot, we find the following.</description></item><item><title>Decorating the Background Grid in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3604/</link><pubDate>Mon, 27 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3604/</guid><description>Overview Keywords related to the grid background in Plots.jl are as follows: Keyword Name Function grid Display grid gridalpha, ga, gα Specify grid transparency foreground_color_grid, fgcolor_grid Specify grid color gridlinewidth, grid_lw Specify grid thickness gridstyle, grid_ls Specify grid line style minorgrid Display minor grid minorgridalpha Specify minor grid transparency foreground_color_minor_grid, fgcolor_minorgrid Specify minor grid color minorgridlinewidth, minorgrid_lw Specify minor grid thickness minorgridstyle, minorgrid_ls Specify minor grid line style Code</description></item><item><title>Specifying Background Color in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3603/</link><pubDate>Sat, 25 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3603/</guid><description>Overview The keywords related to the background color of figures in Plots.jl are as follows. Keyword Name Function background_color, bg_color Specify the color of the overall background background_color_outside, bg_color_outside Specify the color of the area outside where the graph is drawn background_subplot, bg_subplot Specify the color of the area where the graph is drawn background_inside, bg_inside Specify the color of the area where the graph is drawn, excluding the legend</description></item><item><title>How to Specify Graph Colors for Each Subplot in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3602/</link><pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3602/</guid><description>Overview This section introduces three methods for specifying graph colors for each subplot. To learn how to specify colors for graph elements, refer here. Method 1 The first way to specify the graph color for a subplot is to predefine the color when defining each subplot. In Julia, since a picture is an object itself, you can define multiple pictures with different attributes and then combine them into one plot.</description></item><item><title>Specifying the Color of Graph Elements in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3601/</link><pubDate>Tue, 21 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3601/</guid><description>Overview In Plots.jl, the keywords for specifying the color of each graph component are as follows. Keyword Function markercolor, mc Specify the marker&amp;rsquo;s inside color markerstrokecolor, msc Specify the marker&amp;rsquo;s border color linecolor, lc Specify the line color fillcolor, fc Specify the fill color seriescolor, c Specify the color of all components Keyword Function markeralpha, ma, mα Specify the marker&amp;rsquo;s inside transparency markerstrokealpha, msa, msα Specify the</description></item><item><title>How to Use RGB Color Codes in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3600/</link><pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3600/</guid><description>Code The package provided in Julia for dealing with colors is Colors.jl. By importing the visualization package Plots.jl, the features within Colors.jl can also be used. The color codes representing the RGB space include RGB, BGR, RGB24, RGBX, XRGB, which are subtypes of AbstractRGB. RGBA adds transparency to RGB. julia&amp;gt; using Plots julia&amp;gt; subtypes(AbstractRGB) 5-element Vector{Any}: BGR RGB RGB24 RGBX XRGB julia&amp;gt; subtypes(AbstractRGBA) 2-element Vector{Any}: BGRA RGBA Strings For the</description></item><item><title>Package for Color Processing in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3599/</link><pubDate>Fri, 17 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3599/</guid><description>Introduction1 Introducing the capabilities of Colors.jl, a package for color processing in Julia. When using the visualization package Plots.jl, there&amp;rsquo;s no need to load Colors.jl separately. It provides the following functionalities: Color parsing and conversion Color maps Color scales Parsing and Conversion Assuming str is a string representing color information, you can parse the string into a color code of a specific color space using @colorant_str or parse(Colorant, str). Note</description></item><item><title>How to Use Colors in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3598/</link><pubDate>Wed, 15 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3598/</guid><description>Overview The package that facilitates the convenient use of colors in Julia is Colors.jl. It can be used together just by importing the visualization package Plots.jl. Symbols and Strings The way to check the list of named colors is by entering Colors.color_names in the console window or checking the official documentation. julia&amp;gt; using Plots julia&amp;gt; Colors.color_names Dict{String, Tuple{Int64, Int64, Int64}} with 666 entries: &amp;#34;darkorchid&amp;#34; =&amp;gt; (153, 50, 204) &amp;#34;chocolate&amp;#34; =&amp;gt;</description></item><item><title>Decorating Text Output with Built-in Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3597/</link><pubDate>Mon, 13 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3597/</guid><description>Code Using the function printstyled(string; color = color) allows you to decorate the outputted function. As input for the keyword argument color, symbols and integers $(0 \le n \le 255)$ are possible. Note that strings are not allowed. The available symbols include not only colors but also options like :blink, :reverse, etc. These can also be applied by entering them as keyword arguments like blink = true, bold = true.</description></item><item><title>Proximal Alternating Linearized Minimization Algorithm (PALM)</title><link>https://freshrimpsushi.github.io/en/posts/3596/</link><pubDate>Sat, 11 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3596/</guid><description>Overview Jérôme Bolte, Shoham Sabach, and Marc Teboulle introduced an optimization technique called Proximal Alternating Linearized Minimization (PALM) algorithm in their paper Proximal alternating linearized minimization for nonconvex and nonsmooth problems. Algorithm The method to solve optimization problems like $(1)$ is called the Proximal Alternating Linearized Minimization (PALM) algorithm. This, in simple terms, means performing alternating optimization for two variables using the proximal gradient method.</description></item><item><title>Proximal Gradient Method</title><link>https://freshrimpsushi.github.io/en/posts/3595/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3595/</guid><description>Definition 1 Let&amp;rsquo;s say the non-differentiable objective function $H(\mathbf{x}) : \mathbb{R}^{n} \to \mathbb{R}$ is decomposed into a differentiable function $f$ and a non-differentiable function $g$. $$ H(\mathbf{x}) = f(\mathbf{x}) + g(\mathbf{x}) $$ The method of solving the optimization problem for $H$ using the following iterative algorithm is called the proximal gradient method. $$ \mathbf{x}^{(k+1)} = \operatorname{prox}_{\lambda g}(\mathbf{x}^{(k)} - \lambda \nabla f(\mathbf{x}^{(k)})) $$ Explanation It is called the proximal gradient method</description></item><item><title>Subgradient Method</title><link>https://freshrimpsushi.github.io/en/posts/3594/</link><pubDate>Tue, 07 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3594/</guid><description>Definition1 Let&amp;rsquo;s say the objective function $f : \mathbb{R}^{n} \to \mathbb{R}$ is a convex function. Let&amp;rsquo;s denote the subgradient of $f$ at point $\mathbf{x}^{(k)}$ as $\mathbf{g}^{(k)}$. The method of updating $\mathbf{x}^{(k)}$ in the following way to solve the optimization problem for $f$ is called the subgradient method. $$ \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \alpha \mathbf{g}^{(k)} $$ Description2 It&amp;rsquo;s a form where the gradient in the Gradient Descent is replaced with a</description></item><item><title>Subgradient</title><link>https://freshrimpsushi.github.io/en/posts/3593/</link><pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3593/</guid><description>Definition 1 2 For a function $f : \mathbb{R}^{n} \to \mathbb{R}$, the $\mathbf{g} \in \mathbb{R}^{n}$ that satisfies the following is called a subgradient of $f$ at point $\mathbf{x}$. $$ f(\mathbf{y}) \ge f(\mathbf{x}) + \mathbf{g}^{T}(\mathbf{y} - \mathbf{x}) \qquad \forall \mathbf{y} \in \mathbb{R}^{n} $$ Explanation If the convex function $f$ is differentiable at $\mathbf{x}$, then $\mathbf{g} =$ $\nabla f(\mathbf{x})$ is unique. Conversely, if $\partial f(\mathbf{x}) = \left\{ \mathbf{g} \right\}$, then $f$ is</description></item><item><title>Alternating Optimization</title><link>https://freshrimpsushi.github.io/en/posts/3592/</link><pubDate>Fri, 03 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3592/</guid><description>Definition When optimizing a multivariate objective function, the practice of optimizing over one variable at a time, alternating between variables, is known as alternating optimization. Description Consider the following optimization problem where the objective function is $H(x,y)$. $$ \argmin\limits_{x,y} H(x,y) $$ This can be divided into two subproblems by fixing one variable and optimizing over the other. $$ \begin{cases} \argmin\limits_{x} H(x,y) \\ \argmin\limits_{y} H(x,y) \end{cases} $$ Alternating optimization is the</description></item><item><title>Proximal Minimization Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/3591/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3591/</guid><description>Definition1 When solving the optimization problem for the objective function $f : \mathbb{R}^{n} \to \mathbb{R}^{n}$, the method of updating the optimal solution $\mathbf{x}^{(k)}$ by repeatedly applying the proximal operator is called the proximal minimization algorithm. $$ \mathbf{x}^{(k+1)} = \operatorname{prox}_{\lambda f}(\mathbf{x}^{(k)}) = \argmin\limits_{\mathbf{v}} \left\{ \lambda f(\mathbf{v}) + \dfrac{1}{2}\left\| \mathbf{v} - \mathbf{x}^{(k)} \right\|_{2}^{2} : \mathbf{v} \in \mathbb{R}^{n} \right\} $$ Description It is also known as the proximal point algorithm or proximal iteration.</description></item><item><title>Proximal Operator</title><link>https://freshrimpsushi.github.io/en/posts/3590/</link><pubDate>Mon, 29 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3590/</guid><description>Definition1 Let $\left\| \cdot \right\|_{X}$ be the norm of the vector space $X$. The proximal operator $\operatorname{prox}_{\lambda f} : X \to 2^{X}$ for a function $f : X \to \mathbb{R}$ is defined as follows, for $\lambda &amp;gt; 0$, $$ \begin{align} \operatorname{prox}_{\lambda f} (\mathbf{x}) &amp;amp;:= \argmin\limits_{\mathbf{v}} \left\{ \lambda f(\mathbf{v}) + \dfrac{1}{2}\left\| \mathbf{v} - \mathbf{x} \right\|_{X}^{2} : \mathbf{v} \in X \right\} \\ &amp;amp;= \argmin\limits_{\mathbf{v}} \left\{ f(\mathbf{v}) + \dfrac{1}{2\lambda}\left\| \mathbf{v} - \mathbf{x} \right\|_{X}^{2}</description></item><item><title>Latent Variable and Latent Space</title><link>https://freshrimpsushi.github.io/en/posts/3589/</link><pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3589/</guid><description>Definition Suppose a dataset $X \subset \mathbb{R}^{n}$ is given. A function with the dataset as its domain is called an encoder. $$ \begin{align*} f : X &amp;amp;\to Z \\ \mathbf{x} &amp;amp;\mapsto \mathbf{z} = f(\mathbf{x}) \end{align*} $$ The range of the encoder $Z \subset \mathbb{R}^{m}$ ($m \le n$) is called the latent space, and the elements of the latent space $\mathbf{z}$ are referred to as latent variables or feature vectors. For</description></item><item><title>What are Generalized Coordinates in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/3588/</link><pubDate>Thu, 25 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3588/</guid><description>Definition1 The coordinates of a system of particles with $n$ degrees of freedom, expressed in $n$ variables (1) that are independent of constraints and (2) mutually independent, are called generalized coordinates. Generalized Coordinates In a three-dimensional space where the degrees of freedom of a particle are $3$, the position of the particle can be expressed by the generalized coordinates $q_{1}, q_{2}, q_{3}$ as follows: $$ \begin{align*} x &amp;amp;= x(q_{1}, q_{2},</description></item><item><title>What is a Constraint in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/3587/</link><pubDate>Tue, 23 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3587/</guid><description>Definition A particle, or a system of particles, is said to undergo constrained motion when it moves only within a geometrically confined area (such as given curves or surfaces), and such restrictions themselves are called constraints. Explanation In Korean, it is commonly referred to as 구속조건, but in English, it is just called a constraint, not a constraint condition. Simple examples of constrained motion include circular</description></item><item><title>What is Degrees of Freedom in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/3586/</link><pubDate>Sun, 21 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3586/</guid><description>Definition The number of independent coordinates among the total coordinates of a particle system is called the degree of freedom. Explanation To put it simply, the degree of freedom is the minimum number of variables needed to describe a particle system. Consider a particle moving freely in three-dimensional space. The position of this particle can be expressed as $r = (x,y,z)$, and since the variables for each axis $x, y,</description></item><item><title>Differentiation Operators and Symbols</title><link>https://freshrimpsushi.github.io/en/posts/3585/</link><pubDate>Fri, 19 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3585/</guid><description>Definition1 For a natural number $m \in \mathbb{N}$, a differential operator refers to the following map $P$. $$ \begin{equation} P = \sum\limits_{\left| \alpha \right| \le m} a_{\alpha}(x) D^{\alpha},\qquad x = (x_{1}, \dots, x_{n}) \end{equation} $$ Here, $\alpha = (\alpha_{1}, \dots, \alpha_{n})$ is a multi-index. $D^{\alpha}$ is as follows. $$ \begin{align*} D^\alpha &amp;amp;:= \dfrac{\partial ^{|\alpha|} } {{\partial x_{1}}^{\alpha_{1}}\cdots {\partial x_{n}}^{\alpha_{n}}} \\ &amp;amp;= \left( \frac{ \partial }{ \partial x_{1}} \right)^{\alpha_{1}}\left( \frac{ \partial</description></item><item><title>Polyharmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3584/</link><pubDate>Wed, 17 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3584/</guid><description>Definition Let $\Delta = \nabla^{2}$ be called a Laplacian. For a natural number $k \in \mathbb{N}$, $\Delta ^{k}$ is referred to as a polyharmonic operator or a polylaplacian. The equation below is called the polyharmonic equation. $$ \Delta^{k} f = 0 $$ The solutions to the polyharmonic equation are referred to as polyharmonic functions. Description It is a generalization of harmonic functions. See Also Harmonic functions Biharmonic functions Polyharmonic functions</description></item><item><title>Biharmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3583/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3583/</guid><description>Definition1 Let&amp;rsquo;s call $\Delta = \nabla^{2}$ the Laplacian. $\Delta ^{2}$ is called the biharmonic operator or bilaplacian. The following equation is called the biharmonic equation. $$ \Delta^{2} f = 0 $$ The solutions to the biharmonic equation are called biharmonic functions. Explanation Let&amp;rsquo;s say $\partial_{i} = \dfrac{\partial}{\partial x_{i}}$. In the Cartesian coordinate system, since $\Delta = \sum\limits_{i} \partial_{i}\partial_{i}$, $$ \Delta^{2} f = \sum\limits_{j} \sum\limits_{i} \partial_{j}\partial_{j} \partial_{i}\partial_{i} f $$ Especially in</description></item><item><title>파이썬에서 운영체제 확인하는 방법</title><link>https://freshrimpsushi.github.io/en/posts/3582/</link><pubDate>Sat, 13 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3582/</guid><description>Code You can check it with platform.system(). &amp;gt;&amp;gt;&amp;gt; import platform &amp;gt;&amp;gt;&amp;gt; platform.system() &amp;#39;Windows&amp;#39; # Windows의 경우 &amp;gt;&amp;gt;&amp;gt; platform.system() &amp;#39;Linux&amp;#39; # Ubuntu의 경우 See Also Check the operating system with platform.system() Check PC username with os.getlogin Check PC name with socket.gethostname()</description></item><item><title>파이썬에서 PC 이름 얻는 방법</title><link>https://freshrimpsushi.github.io/en/posts/3581/</link><pubDate>Thu, 11 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3581/</guid><description>Code You can obtain it using os.getlogin. &amp;gt;&amp;gt;&amp;gt; import os &amp;gt;&amp;gt;&amp;gt; os.getlogin() &amp;#39;rydbr&amp;#39; See Also Check operating system using platform.system() Retrieve PC username using os.getlogin Check PC name using socket.gethostname() Environment OS: Windows 11 Version: Python 3.11.5</description></item><item><title>Sperical Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/3580/</link><pubDate>Tue, 09 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3580/</guid><description>Definition 1 The coordinates of a point $p$ in coordinate space are represented by &amp;ldquo;the distance $r$ between the origin and $p$,&amp;rdquo; &amp;ldquo;the angle $\theta$ between the line connecting the origin and $p$ and the positive direction of the $z-$axis,&amp;rdquo; and &amp;ldquo;the angle $\phi$ between the projection of the line connecting the origin and $p$ on the $xy-$plane and the positive direction of the $x-$axis.&amp;rdquo; This is called the spherical</description></item><item><title>Coordinates of a Möbius Strip in Three-Dimensional Space</title><link>https://freshrimpsushi.github.io/en/posts/3579/</link><pubDate>Sun, 07 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3579/</guid><description>Definition1 A surface represented by the coordinate chart mapping $\mathbf{x}$ is called a Möbius band. $$ \mathbf{x}(\theta, v) = (\cos \theta, \sin \theta, 0) + v(\textstyle \sin\frac{\theta}{2}\cos\theta, \sin\frac{\theta}{2}\sin\theta, \cos\frac{\theta}{2}) $$ $$ \textstyle -\pi \lt \theta \lt \pi, \quad -\frac{1}{2} \lt v \lt \frac{1}{2} $$ Properties The Möbius band is not an orientable surface Richard S. Millman and George D. Parker,</description></item><item><title>Coordinate Space, Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/3578/</link><pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3578/</guid><description>Definition When the coordinate plane and the number line meet at the origin of the coordinate plane and are drawn orthogonally, it is called a coordinate space. The vertical line that is orthogonal to the coordinate plane is called the $z-$axis. The point determined by the three axes as shown above is called point $(a,b,c)$. Point $(0,0,0)$ is called the origin. The coordinate plane made by the $x$ axis and</description></item><item><title>Polar Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/3577/</link><pubDate>Wed, 03 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3577/</guid><description>Definition The coordinates of a point on the coordinate plane are defined as &amp;ldquo;the distance $r$ from the origin&amp;rdquo; and &amp;ldquo;the angle $\theta$ made by the line connecting the point to the origin with the $x$ axis,&amp;rdquo; which is referred to as the polar coordinate system. Explanation It is useful for expressing functions that depend on the distance from the origin. For example, the position of an object performing circular</description></item><item><title>Definition of Coordinate Plane</title><link>https://freshrimpsushi.github.io/en/posts/3576/</link><pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3576/</guid><description>Definition The coordinate plane is created by drawing two perpendicular lines that intersect at $0$ in an orthogonal manner. These lines are referred to as axes. The horizontal line is called the $x$ axis, and the vertical line is called the $y$ axis. A line drawn orthogonally to the $x$ axis at the real number $a$ on the $x$ axis, and a line drawn orthogonally to the $y$ axis at</description></item><item><title>Definition of a Line</title><link>https://freshrimpsushi.github.io/en/posts/3575/</link><pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3575/</guid><description>Definition The assigning of each point on a straight line to a real number is called the number line. Explanation It is called a number line even though it is always drawn horizontally because it is line of numbers, not a perpendicular line. An arrow is usually drawn in the direction that the numbers increase. If you draw two number lines, it becomes a coordinate plane. Drawing three lines results</description></item><item><title>Geodesic Coordinate Mapping and Gaussian Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3574/</link><pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3574/</guid><description>Theorem1 The metric matrix of the geodesic coordinate mapping $\mathbf{x} : U \to \mathbb{R}^{3}$ is as follows. $$ \left[ g_{ij} \right] = \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; h^{2} \end{bmatrix} \quad (h \gt 0) $$ Then, the Gaussian curvature of $\mathbf{x}$ is as follows. $$ K = -\dfrac{h_{11}}{h} $$ At this time, $(u^{1}, u^{2})$ is the coordinate of $U$, and $h_{i} = \dfrac{\partial h}{\partial u^{i}}$. Proof Gauss&amp;rsquo; Theorema Egregium</description></item><item><title>Geodesic Coordinate Patch Mapping and Christoffel Symbols</title><link>https://freshrimpsushi.github.io/en/posts/3573/</link><pubDate>Tue, 26 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3573/</guid><description>Theorem1 $$ \left[ g_{ij} \right] = \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; h^{2} \end{bmatrix} \quad (h \gt 0) $$ Then, the Christoffel symbols of $\mathbf{x}$ are as follows, and except for the below, all are $0$. $$ \Gamma_{22}^{1} = -hh_{1},\quad \Gamma_{12}^{2} = \Gamma_{21}^{2} = \dfrac{h_{1}}{h},\quad \Gamma_{22}^{2} = \dfrac{h_{2}}{h} $$ At this time, $(u^{1}, u^{2})$ is the coordinate of $U$, and $h_{i} = \dfrac{\partial h}{\partial u^{i}}$ is valid. Proof Before</description></item><item><title>How to k-means cluster in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3572/</link><pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3572/</guid><description>Description k-means clustering is a clustering algorithm that divides the given $n$ data points into $k$ clusters. In Julia, it can be easily implemented using the Clustering.jl package. Code The following is a code to perform clustering with $k=3$ on the Iris dataset. Since data loaded from RDatasets.jl are by default data frames, they are converted into arrays, and transposed so that each column becomes a single data point. It</description></item><item><title>Integration of 1/(1+x^2)</title><link>https://freshrimpsushi.github.io/en/posts/3570/</link><pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3570/</guid><description>Formulas Definite Integral $$ \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx = \pi $$ Indefinite Integral $$ \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx = \pi $$ $C$ is the integration constant. Proofs Definite Integral Let&amp;rsquo;s substitute with $x = \tan \theta$. Then, the range of integration becomes $\displaystyle \int_{-\infty}^{\infty} \to \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}$, and since $\tan ^{\prime} = \sec^{2}$, it results in $dx = \sec^{2} d\theta$. $$ \begin{align*} \int_{-\infty}^{\infty} \dfrac{1}{1+x^{2}}dx &amp;amp;= \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \dfrac{1}{1 + \tan^{2}\theta} \sec^{2} \theta d\theta \\ &amp;amp;= \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}</description></item><item><title>Differentiation of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/3569/</link><pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3569/</guid><description>Formula1 The derivatives of trigonometric functions are as follows. $$ \begin{align*} \dfrac{d}{dx} \sin x &amp;amp;= \cos x \qquad &amp;amp; \dfrac{d}{dx} \csc x &amp;amp;= -\csc x \cot x \\[1em] \dfrac{d}{dx} \cos x &amp;amp;= - \sin x \qquad &amp;amp; \dfrac{d}{dx} \sec x &amp;amp;= \sec x \tan x \\[1em] \dfrac{d}{dx} \tan x &amp;amp;= \sec^{2} x \qquad &amp;amp; \dfrac{d}{dx} \cot x &amp;amp;= -\csc^{2} x \end{align*} $$ Proof Sum formulas for trigonometric functions $$ \sin\left(</description></item><item><title>The Limit of 1-cos(x)/x</title><link>https://freshrimpsushi.github.io/en/posts/3568/</link><pubDate>Sat, 16 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3568/</guid><description>Formula $$ \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} = 0 $$ Proof $$ \begin{align*} \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} &amp;amp;= \lim \limits_{x \to 0} \dfrac{1 - \cos x}{x} \dfrac{1 + \cos x}{1 + \cos x} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{1 - \cos^{2} x}{x(1+\cos x)} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{\sin^{2}x}{x(1+\cos x)} \\ &amp;amp;= \lim \limits_{x \to 0} \dfrac{\sin x}{x} \dfrac{\sin x}{1+\cos x}</description></item><item><title>The limit of sin(x)/x</title><link>https://freshrimpsushi.github.io/en/posts/3567/</link><pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3567/</guid><description>공식 $$ \lim \limits_{x \to 0} \dfrac{\sin x}{x} = 1 $$ 증명 반지름이 $1$인 부채꼴 $OAB$가 주어졌다고 하자. 점 $B$에서 선분 $\overline{OA}$로 내린 수선의 발을</description></item><item><title>What are Frequency and Oscillation?</title><link>https://freshrimpsushi.github.io/en/posts/3566/</link><pubDate>Tue, 12 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3566/</guid><description>Definition The number of oscillations per unit time is called the frequency. Description Frequency refers to the same concept as oscillation frequency. Its unit is $\mathrm{Hz}$[Hertz], and it is commonly denoted by $f$, $\nu$, $k$. When saying $k \mathrm{Hz}$ is &amp;ldquo;1 second with $k번 진동한다는 뜻이다. 주기 $T$는 한 번 진동하는데 걸리</description></item><item><title>Sampling with Replacement and without Replacement in Python</title><link>https://freshrimpsushi.github.io/en/posts/3565/</link><pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3565/</guid><description>Code To perform sampling with replacement/replacement in Python, one can use the np.random.choice() function of numpy. random.choice(a, size=None, replace=True, p=None) a: 1-dimensional array or integer Represents the set from which to sample. If a is an integer, sampling is done from np.arange(a). size: An integer or a tuple of integers Represents the size of the output sample. replace: Boolean T for sampling with replacement, F for sampling without replacement. p:</description></item><item><title>Integrability of 1/x^p</title><link>https://freshrimpsushi.github.io/en/posts/3564/</link><pubDate>Fri, 08 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3564/</guid><description>Theorem The integrability of function $f(x) = \dfrac{1}{x^{p}}$ is as follows: when $x \in (0,1]$, if $p \lt 1$, then $f$ is integrable. when $x \in [1, \infty)$, if $p \gt 1$, then $f$ is integrable. Explanation If $x$ is less than $1$, then $p$ must also be less than $1$, and if $x$ is greater than $1$, then $p$ must also be greater than $1$. Just remember this. Proof</description></item><item><title>Dimensionality Reduction in Data Science</title><link>https://freshrimpsushi.github.io/en/posts/3563/</link><pubDate>Wed, 06 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3563/</guid><description>Definition Let&amp;rsquo;s assume a data set $X \subset \mathbb{R}^{n}$ is given. The following mapping for $m \lt n$ is called dimension reduction. $$ r : X \to \mathbb{R}^{m} $$ Or more commonly in machine learning, any method that reduces the number of input variables in a way that retains as much of the performance as possible is called a dimension reduction technique. Explanation Dimension reduction, as the name suggests, refers</description></item><item><title>How to Define and Train MLP with the Sequence Model and Functional API in TensorFlow and Keras</title><link>https://freshrimpsushi.github.io/en/posts/3562/</link><pubDate>Mon, 04 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3562/</guid><description>Overview In TensorFlow, neural networks can be easily defined using Keras. Below, we introduce how to define and train a simple MLP using Sequential() and the functional API. However, Sequential() is only easy for defining models and can be challenging to use for designing complex structures. Similarly, if you plan to design complex structures using the functional API, it’s better to use the keras.Model class, and for</description></item><item><title>Python's Command Line Parsing Module argparse</title><link>https://freshrimpsushi.github.io/en/posts/3561/</link><pubDate>Sat, 02 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3561/</guid><description>Description argparse is one of the Python standard libraries that helps in handling arguments passed from the command-line. While it is possible to receive input arguments using sys.argv, using argparse allows for advanced features such as specifying the type and whether an argument is mandatory. Code argparse.ArgumentParser: Creates an object. add_argument(): Method to register an argument to be accepted. parse_args(): Method to parse the received arguments. # 테스트</description></item><item><title>How to Pass Arguments When Executing a Python File</title><link>https://freshrimpsushi.github.io/en/posts/3560/</link><pubDate>Thu, 29 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3560/</guid><description>Code To pass arguments to a Python file when executing, you just need to input the path to the Python file followed by the arguments separated by spaces. If you want to pass the variables 1, x, &amp;ldquo;3&amp;rdquo;, and 3 to a Python file named text.py, you would enter the following in the command line: python test.py 1 x &amp;#34;3&amp;#34; 3 To use the passed arguments inside the Python file,</description></item><item><title>How to Adjust the Size and Resolution of an Image in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3559/</link><pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3559/</guid><description>Code Size plot(x, y, size=(600,400)) In Julia, the size of a plot is set using the size option. It must be input as a Tuple{Integer, Integer}, where each integer represents the width and height in pixels, respectively. The default value is (600,400). using Plots x = rand(10) plot(x) savefig(&amp;#34;size_default.png&amp;#34;) plot(x, size=(1200,800)) savefig(&amp;#34;size_(1200,800).png&amp;#34;) 1800x1200 image (left), 600x400 image (right) Resolution plot(x, y, dpi=100) The resolution of an image is set using</description></item><item><title>Drawing Arrows in Graphics with Julia</title><link>https://freshrimpsushi.github.io/en/posts/3558/</link><pubDate>Sun, 25 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3558/</guid><description>Code plot!([x1, x2], [y1, y2], arrow=:true) This code plots an arrow from point $(x1, y1)$ to point $(x2, y2)$ on the plot. Naturally, the tip of the arrow is at the terminal point $(x2, y2)$. The maximum value of the sine function can be shown as follows. using Plots x = range(0, 2π, 100) plot(x, sin.(x), label=&amp;#34;&amp;#34;, ylims=(-1.3,1.3)) plot!([π/2</description></item><item><title>What is a Box Plot?</title><link>https://freshrimpsushi.github.io/en/posts/3557/</link><pubDate>Fri, 23 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3557/</guid><description>Definition1 A box plot is a diagram that represents the median, first quartile, third quartile, maximum, and minimum of data as shown below. Explanation The third quartile, median, and first quartile are denoted as $Q3$, $Q2$, and $Q1$, respectively. The difference between $Q3$ and $Q1$ is called the IQR. The maximum and minimum values are denoted as $Q4$ and $Q0$, respectively. The rectangle in the middle is called the box,</description></item><item><title>How to Adjust Camera Position for 3D Plots in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3556/</link><pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3556/</guid><description>Explanation Unlike 2D plots such as line graphs and heatmaps, 3D plots have different appearances depending on the viewing angle. The viewpoint of a 3D plot can be set using the camera=(azimuth, altitude) option. Azimuth represents the compass direction and corresponds to the angle measured from the $xz$-plane. Altitude represents the elevation and corresponds to the angle measured from the $xy$-plane. The default value is camera=(30, 30). Drawing a spiral</description></item><item><title>How to Fix the Random Seed in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3555/</link><pubDate>Mon, 19 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3555/</guid><description>Explanation1 In Julia, the random seed can be fixed as follows: seed!([rng=default_rng()], seed) -&amp;gt; rng seed!([rng=default_rng()]) -&amp;gt; rng The input variable rng stands for Random Number Generator, which refers to the algorithm used for drawing random numbers. The Random package offers the following options: TaskLocalRNG: This is the default setting. Xoshiro RandomDevice MersenneTwister Code By fixing the seed to 0, drawing three times, and then fixing it again to 0</description></item><item><title>How to Draw a Box Plot in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3554/</link><pubDate>Sat, 17 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3554/</guid><description>Description plt.boxplot() can draw a Box Plot. If you draw it with the default settings, it appears as a white box with black lines only, so if you want to make it prettier, you have to manually adjust the settings. Code Basic import numpy as np import matplotlib.pyplot as plt x = 100*np.random.random_sample(100) y = 50*np.random.random_sample(100) + 50 z = np.concatenate((x,y)) plt.boxplot([x,y,z]) plt.show() Legend fig, ax = plt.subplots() bp1 =</description></item><item><title>How to Draw a Box Plot in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3553/</link><pubDate>Thu, 15 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3553/</guid><description>English Translation Description To draw a box plot, the statistical visualization package StatsPlots.jl must be used. boxplot([data], labels=[label]) Code using StatsPlots x = rand(0:100, 100) y = rand(50:100, 100) z = cat(x,y, dims=1) boxplot(x, label=&amp;#34;x&amp;#34;) boxplot!(y, label=&amp;#34;y&amp;#34;) boxplot!(z, label=&amp;#34;z&amp;#34;) Or boxplot([x,y,z], label=[&amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;]) will draw the same figure. Note that there should be no commas in lable. That is, it needs to be an array, not an $3 \times</description></item><item><title>The Hyperfunctional Derivative of Brownian Motion is White Noise</title><link>https://freshrimpsushi.github.io/en/posts/3552/</link><pubDate>Tue, 13 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3552/</guid><description>Summary The distributional derivative of Brownian motion is white noise. Description Brownian motion $B_{t}$ does not have a derivative in the traditional sense. Therefore, it can be defined as a stochastic process that satisfies the following condition $\xi$, which is defined as white noise: $$ \begin{align} E[\xi_{t}] &amp;amp;= 0, &amp;amp; \forall t \\ \Cov(\xi_{t}, \xi_{s}) &amp;amp;= \delta_{0} \end{align} $$ Here, $\Cov$ is the covariance, and $\delta$ is the Dirac delta</description></item><item><title>Drawing Subplots at Desired Arbitrary Locations or Overlapping in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3551/</link><pubDate>Sun, 11 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3551/</guid><description>Overview Using plt.subplot or gridspec, you can create subplots in simple or complex grid-based layouts. This article introduces a method to draw subplots at completely arbitrary positions, independent of the grid system. Code fig.add_axes([left, bottom, width, height]) By using add_axes, you can create a new subplot at a desired location on the figure. The area of the subplot is determined by the input arguments [left, bottom, width, height]. Each item</description></item><item><title>Drawing Subplots with Complex Layouts in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3550/</link><pubDate>Fri, 09 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3550/</guid><description>Overview Essentially, grids of subplots can be drawn using plt.subplot(nrows, ncols, index) or plt.subplots(nrows, ncols). This article introduces methods to draw more complex subplot arrangements. For methods on overlaying or truly drawing as one pleases, independent of a grid, see here. Code plt.subplot(nrows, ncols, index) Using plt.subplot, placing a tuple instead of an integer in the index position allows for spanning the drawing over multiple grids. The frustrating part here</description></item><item><title>How to Draw Subplots in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3549/</link><pubDate>Wed, 07 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3549/</guid><description>Overview This document introduces several ways to draw multiple figures within one figure. The methods mentioned below are for creating simple layouts, and for more complex layouts, refer to the following. Creating Complex Layouts Drawing or Overlapping Subplots at Arbitrary Locations Code plt.subplot(nrows, ncols, index) Entering plt.subplot(r, c, n) divides the entire figure into a grid of $r$ rows and $c$ columns, allowing you to draw a picture in the</description></item><item><title>신호의 교차상관함수</title><link>https://freshrimpsushi.github.io/en/posts/3548/</link><pubDate>Mon, 05 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3548/</guid><description>Definition1 Analog Signal For an Energy Signal $f \in L^{2}(\mathbb{R})$, $f \star g$ defined as follows is called the cross-correlation function defined by $f$ and $g$. $$ (f \star g)(\tau) = R_{fg}(\tau) := \int_{-\infty}^{\infty} \overline{f(t)} g(t + \tau) dt $$ Here, $\overline{f(t)}$ is the conjugate complex number of $f(t)$. Digital Signal The cross-correlation function of energy signal $\left\{ x_{n} \right\} \in \ell^{2}$ is defined as follows. $$ (x\star y)[n] =</description></item><item><title>EasyDict: A Python Package for Convenient Dictionary Use</title><link>https://freshrimpsushi.github.io/en/posts/3547/</link><pubDate>Sat, 03 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3547/</guid><description>Overview1 Introducing the easydict package that makes it convenient to use dictionaries. By using EasyDict within the package, you can access the values of a dictionary as if they were attributes, and this works recursively. Code A standard Python dictionary is defined as follows. 김채원 = { &amp;#39;국적&amp;#39;: &amp;#39;대한민</description></item><item><title>L² Space</title><link>https://freshrimpsushi.github.io/en/posts/3546/</link><pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3546/</guid><description>Definition1 The set of sequences that are square-convergent is denoted as $\ell^{2}(\mathbb{N})$. $$ \ell^{2}(\mathbb{N}) := \left\{ \left\{ x_{k} \right\}_{k \in \mathbb{N}} : x \in \mathbb{C}(\text{or } \mathbb{R}),\quad \sum\limits_{k \in \mathbb{N}} \left| x_{k} \right|^{2} \lt \infty \right\} $$ It can also be simply denoted as follows. $$ \mathbf{x} = \left\{ x_{k} \right\}_{k \in \mathbb{N}} = (x_{1}, x_{2}, \dots, x_{n}, \dots) $$ Description $\ell^{2}$ space is a special case when $\ell^{p}$ space</description></item><item><title>신호의 자기상관함수</title><link>https://freshrimpsushi.github.io/en/posts/3545/</link><pubDate>Tue, 30 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3545/</guid><description>Definition1 Analog Signal For an energy signal $f \in L^{2}(\mathbb{R})$, $R_{f}$ defined as follows is called the auto-correlation function. $$ R_{f}(\tau) := \int_{-\infty}^{\infty} \overline{f(t)} f(t + \tau) dt $$ Here $\overline{f(t)}$ is the complex conjugate of $f(t)$. Digital Signal The auto-correlation function for the energy signal $\left\{ x_{n} \right\} \in \ell^{2}$ is defined as follows. $$ R_{x}(m) := \sum\limits_{n \in \mathbb{N}} \overline{x_{n}}x_{n+m} $$ Explanation Expressed in terms of inner product,</description></item><item><title>신호의 에너지와 평균 전력</title><link>https://freshrimpsushi.github.io/en/posts/3544/</link><pubDate>Sun, 28 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3544/</guid><description>Definition1 Analog Signal The energy of an analog signal $f \in L^{p}$ $E_{f}$ is defined as follows. $$ E_{f} := \int_{-\infty}^{\infty} \left| f(t) \right|^{2} dt = \left\| f \right\|_{2}^{2} $$ If $E_{f} \lt \infty$, then $f$ is called an energy signal. For $f$, which is not an energy signal, the mean power $P_{f}$ is defined as follows. $$ P_{f} := \lim\limits_{T \to \infty} \dfrac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} \left| f(t) \right|^{2}dt $$ If $P_{f}</description></item><item><title>아날로그신호와 디지털신호의 정의</title><link>https://freshrimpsushi.github.io/en/posts/3543/</link><pubDate>Fri, 26 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3543/</guid><description>Definition1 The elements of the $L^{p}$ space are referred to as analog signals or continuous-time signals. $$ f \in L^{p}(\mathbb{R}) $$ The elements of the $\ell^{p}$ space are referred to as digital signals or discrete-time signals. $$ x_{n} = \left\{ x_{n} \right\} \in \ell^{p}(\mathbb{N}) $$ Explanation By definition, analog signals and digital signals possess almost identical characteristics, with the primary difference being whether they are functions or sequences. It is</description></item><item><title>Absolutely Continuous Real Function</title><link>https://freshrimpsushi.github.io/en/posts/3542/</link><pubDate>Wed, 24 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3542/</guid><description>Definition1 Let&amp;rsquo;s say a function $f : \mathbb{R} \to \mathbb{R}( \text{or } \mathbb{C})$ is given. If for any finite number of mutually disjoint intervals $(a_{i}, b_{i}) \sub [a,b]$, the following condition is satisfied, then it is said to be absolutely continuous on $[a, b]$. $$ \forall \epsilon \gt 0 \quad \exist \delta \gt 0 \text{ such that } \sum\limits_{i=1}^{N} (b_{i} - a_{i}) \lt \delta \implies \sum\limits_{i=1}^{N} \left| f(b_{j}) - f(a_{j})</description></item><item><title>Lipschitz Continuity</title><link>https://freshrimpsushi.github.io/en/posts/3541/</link><pubDate>Mon, 22 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3541/</guid><description>Definition1 For two metric spaces $(X, d_{X})$ and $(Y, d_{Y})$, let&amp;rsquo;s assume that a function $f : X \to Y$ is given. If there exists a constant $K$ such that the following holds for all $x_{1}, x_{2} \in X$, then $f$ is called $K$-Lipschitz continuous. $$ d_{Y} \big( f(x_{1}), f(x_{2}) \big) \le K d_{X} \big( x_{1}, x_{2} \big) $$ Such a constant $K$ is called the Lipschitz constant. Explanation It</description></item><item><title>Homogeneous Function</title><link>https://freshrimpsushi.github.io/en/posts/3540/</link><pubDate>Sat, 20 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3540/</guid><description>Definition For a constant $a$ and a function $f$, if there exists a $k \in \mathbb{N}$ that satisfies the following conditions, then $f$ is called a $k$-th degree homogeneous function. $$ f(ax) = a^{k}f(x) $$ In the case of a multivariable function, $$ f(ax_{1}, ax_{2}, \dots, ax_{n}) = a^{k}f(x_{1}, x_{2}, \dots, x_{n}) $$ Explanation In the case of a univariate function, it is similar to a polynomial function that only</description></item><item><title>Origin of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/3538/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3538/</guid><description>Explanation In mathematics, there are functions called trigonometric functions. These include $\sin, \cos, \tan, \sinh, \sec, \dots$ and others. They are collectively referred to as trigonometric functions because they are fundamentally related to triangles. Each of their names also originates from geometric meanings related to triangles. $\sin$ Let&amp;rsquo;s assume a right triangle like the one shown in the figure above. If the angle between the base and the hypotenuse is</description></item><item><title>Origin of the arc notation for inverse trigonometric functions</title><link>https://freshrimpsushi.github.io/en/posts/3537/</link><pubDate>Sun, 14 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3537/</guid><description>Definition The inverse function of a trigonometric function is called the inverse trigonometric function. Explanation Inverse trigonometric functions are often denoted using standard inverse notation $\sin^{-1}$, $\cos^{-1}$, but they are also frequently denoted with the prefix arc-, as in $\arcsin$, $\arccos$. $$ \begin{align*} \arcsin x &amp;amp;= \sin^{-1} x \qquad &amp;amp; \operatorname{arccsc} x &amp;amp;= \csc^{-1} x \\ \arccos x &amp;amp;= \cos^{-1} x \qquad &amp;amp; \operatorname{arcsec} x &amp;amp;= \sec^{-1} x \\ \arctan</description></item><item><title>Notation for Vectors Entering and Exiting the Plane</title><link>https://freshrimpsushi.github.io/en/posts/3536/</link><pubDate>Fri, 12 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3536/</guid><description>Definition As depicted above, from the observer&amp;rsquo;s perspective, the direction that goes into the plane is denoted as $\otimes$. Conversely, the direction that comes out of the plane is denoted as $\odot$. Explanation Looking at the $xy$-plane as shown in the figure above, the $\hat{z}$ direction is $=\odot$ and the $-\hat{z}$ direction is $=\otimes$. This is primarily used in physics, engineering, etc. The notation is not based on circles and</description></item><item><title>Curl of a Vector Function in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/3535/</link><pubDate>Wed, 10 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3535/</guid><description>Theorem In the curvilinear coordinate system, the curl of the vector function $\mathbf{F}=\mathbf{F}(q_{1},q_{2},q_{3})=F_{1}\hat{\mathbf{q}}_{1}+F_{2}\hat{\mathbf{q}}_{2}+F_{3}\hat{\mathbf{q}}_{3}$ is as follows. $$ \begin{align*} \nabla \times \mathbf{F} &amp;amp;= \frac{\hat{\mathbf{q}}_{1}}{h_{2}h_{3}}\left( \dfrac{\partial (F_{3}h_{3})}{\partial q_{2}} - \dfrac{\partial (F_{2}h_{2})}{\partial q_{3}} \right) + \frac{\hat{\mathbf{q}}_{2}}{h_{1}h_{3}}\left( \dfrac{\partial (F_{1}h_{1})}{\partial q_{3}} - \dfrac{\partial (F_{3}h_{3})}{\partial q_{1}} \right) \\ &amp;amp;\quad+ \frac{\hat{\mathbf{q}}_{3}}{h_{1}h_{2}}\left( \dfrac{\partial (F_{2}h_{2})}{\partial q_{1}} - \dfrac{\partial (F_{1}h_{1})}{\partial q_{2}} \right) \\ &amp;amp;= \frac{1}{h_{1}h_{2}h_{3}} \begin{vmatrix} h_{1}\hat{\mathbf{q}}_{1} &amp;amp; h_{2}\hat{\mathbf{q}}_{2} &amp;amp; h_{3}\hat{\mathbf{q}}_{3} \\[0.5em] \dfrac{\partial }{\partial q_{1}} &amp;amp; \dfrac{\partial }{\partial q_{2}} &amp;amp;</description></item><item><title>First-Order Necessary Conditions for Extrema of Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/3534/</link><pubDate>Mon, 08 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3534/</guid><description>Theorem1 Let&amp;rsquo;s assume the function $f : \mathbb{R}^{n} \to \mathbb{R}$ is given. If $x^{\ast}$ is a local optimizer and $f \in C^{1}$ in the vicinity of $x^{\ast}$, then, $$ \nabla f(x^{\ast}) = 0 $$ $\nabla f$ is the gradient of $f$. Note here that $0$ is not the numeric zero, but a zero vector. Explanation The first-order necessary condition tells us about the property of the gradient, which is the</description></item><item><title>Second Order Necessary/Sufficient Conditions for the Extreme Values of Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/3533/</link><pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3533/</guid><description>Theorem1 Let&amp;rsquo;s say the function $f : \mathbb{R}^{n} \to \mathbb{R}$ is given. $\nabla f$, $\nabla^{2}f$ are the gradient and Hessian of $f$, respectively. Second-order necessary conditions If $x^{\ast}$ is a local optimizer and $\nabla^{2}f$ exists and is continuous in the neighborhood of $x^{\ast}$, $$ \nabla f(x^{\ast}) = 0 $$ and $\nabla^{2} f(x^{\ast})$ is positive semidefinite. Note that $0$ is not the number zero, but the zero vector. Second-order sufficient conditions</description></item><item><title>Taylor's Theorem Rest Term</title><link>https://freshrimpsushi.github.io/en/posts/3532/</link><pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3532/</guid><description>Definition1 2 For a differentiable function $f$, the $P_{k}$ defined below is called the Taylor polynomial of $f$ at point $a$. $$ P_{k} (x) := f(a) + f^{\prime}(a) (x-a) + \dfrac{f^{\prime \prime}(a)}{2!}(x-a)^{2} + \cdots + \dfrac{f^{(k)}(a)}{k!}(x-a)^{k} $$ The difference between $f$ and $P_{k}$ is called the remainder term. $$ R_{k}(x) = f(x) - P_{k}(x) $$ Explanation $$ f(x) = P_{k}(x) + R_{k}(x) = \sum \limits_{n=0}^{k} \dfrac{f^{(n)}(a)}{n!} (x-a)^{n} + R_{k}(x) $$</description></item><item><title>Secant Method: Newton's Method</title><link>https://freshrimpsushi.github.io/en/posts/3530/</link><pubDate>Sun, 31 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3530/</guid><description>Definition1 2 In the problem of optimizing the objective function $J : \mathbb{R}^{n} \to \mathbb{R}$, the following iterative algorithm is called Newton&amp;rsquo;s method. $$ \begin{equation} \mathbf{x}_{n+1} = \mathbf{x}_{n} - H^{-1}(\mathbf{x}_{n}) \nabla J(\mathbf{x}_{n}) \end{equation} $$ $\nabla J$ is the gradient, and $H$ is the Hessian of $J$. Derivation By approximating $J$ with up to the second derivative term using the Taylor expansion, it looks like this: $$ J(\mathbf{x}) \approx J(\mathbf{x}_{0}) +</description></item><item><title>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</title><link>https://freshrimpsushi.github.io/en/posts/3529/</link><pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3529/</guid><description>Overview1 2 Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam. Explanation In gradient descent, the learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\alpha$, $\eta$, and determines how much of the gradient is taken into account when updating the parameter. Optimization</description></item><item><title>Momentum Method in Gradient Descent</title><link>https://freshrimpsushi.github.io/en/posts/3528/</link><pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3528/</guid><description>Overview1 2 In gradient descent, the momentum technique involves using all previous gradients when updating parameters. This is its essence and the end of the explanation. However, explanations involving strange update equations, the motivation from physics&amp;rsquo; momentum, setting the mass to $1$ and the initial velocity to $0$, only complicate understanding. This article explains the momentum technique as plainly as possible. Build-Up Let&amp;rsquo;s denote the parameters as $\boldsymbol{\theta}$ and the</description></item><item><title>Modular Arithmetic in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3527/</link><pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3527/</guid><description>Explanation Modular arithmetic, also known as the remainder operation, is a function that returns the remainder when dividing $a$ by $b$. In PyTorch, there are two functions available: torch.remainder(a,b) torch.fmod(a,b) Both provide the remainder when $a$ is divided by $b$, but the outcomes are slightly different. If you&amp;rsquo;re curious about the specific formulas, refer to the official documents for remainder and fmod. Simply put, in remainder, the sign of the</description></item><item><title>Online Learning vs. Batch Learning in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3526/</link><pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3526/</guid><description>Overview This document explains online learning, batch learning, and mini-batch learning. It emphasizes that the names and differences among them are not actually crucial, but rather inconsequential. In fact, it can be assumed that only mini-batch learning is used in recent deep learning. Definition Let&amp;rsquo;s suppose we have a true value $\mathbf{y} = \left\{ y_{i} \right\}_{i=1}^{N}$ and an estimated value $\hat{\mathbf{y}} = \left\{ \hat{y}_{i} \right\}_{i=1}^{N}$. Assuming the loss function for</description></item><item><title>How to Prevent the Date from Appearing when Creating a Title with maketitle in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3525/</link><pubDate>Thu, 21 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3525/</guid><description>Explanation When creating a title with \maketitle in $\LaTeX$, how can one prevent the date from being displayed? Many would intuitively try to comment out \date{2023.12.21}. However, commenting out will not stop the date from being printed, and in fact, you just have to not enter a date at all (the frustrating part is that you can stop the display by commenting out \author{}). Code \title{Using maketitle in LaTeX...} \author{Jeon</description></item><item><title>How to Insert Code Blocks in LaTeX Documents</title><link>https://freshrimpsushi.github.io/en/posts/3524/</link><pubDate>Tue, 19 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3524/</guid><description>Description $\TeX$ There are two methods you can use when you want to add a code block to a document. $\text{\textbackslash begin}\{\text{verbatim}\} &amp;hellip; \text{\textbackslash end}\{\text{verbatim}\}$ $\text{\textbackslash begin}\{\text{lstlisting}\} &amp;hellip; \text{\textbackslash end}\{\text{lstlisting}\}$ Among these, verbatim is a basic feature that doesn&amp;rsquo;t require the use of a package, whereas lstlisting requires \usepackage{listings}. Code \documentclass{article} \usepackage[utf8]{inputenc} \usepackage{listings} \title{How to Insert a Code Block in \LaTeX?} \date{} \begin{document} \maketitle verbatim style: \begin{verbatim} def add(x, y)</description></item><item><title>자기장의 기호로 B를 사용하는 이유</title><link>https://freshrimpsushi.github.io/en/posts/3523/</link><pubDate>Sun, 17 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3523/</guid><description>Question Electromagnetism is literally the study of electric fields $\mathbf{E}$ and magnetic fields $\mathbf{B}$. While studying electromagnetism, one might have wondered the following at least once. Why is the symbol for magnetic fields $\mathbf{B}$ used? It's understandable that the electric field is $\mathbf{E}$, derived from the Electric field, but why is the magnetic field $\mathbf{B}$ when it should be from Magnetic field? This notation might feel oddly placed, and it's</description></item><item><title>What is Data Augmentation?</title><link>https://freshrimpsushi.github.io/en/posts/3522/</link><pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3522/</guid><description>Definition Suppose a set of data $X = \left\{ x \in \mathbb{R}^{n} \right\}$ is given. Using appropriate transformations $f_{i} : \mathbb{R}^{n} \to \mathbb{R}^{n}$ to obtain $X^{\prime}$ from $X$ is called data augmentation. $$ X^{\prime} = X \cup \left( \bigcup_{i} \left\{ f_{i}(x) : x \in X \right\} \right) $$ Description Simply put, for example with images, this means adding new images to the dataset, which are obtained by applying modifications like</description></item><item><title>Monte Carlo Method</title><link>https://freshrimpsushi.github.io/en/posts/3521/</link><pubDate>Wed, 13 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3521/</guid><description>Definition Simply put, the Monte Carlo method is about doing something many times at random. Description Although it&amp;rsquo;s a simple method, there are few methods as easy and certain as trying many times. Of course, &amp;ldquo;easy&amp;rdquo; refers to when utilizing a computer. Monte Carlo refers to the name of an area famous for its casinos and hotels in northern Monaco. The name &amp;ldquo;Monte Carlo method&amp;rdquo; also comes from the Monte</description></item><item><title>Square Root Expansion Formula</title><link>https://freshrimpsushi.github.io/en/posts/3519/</link><pubDate>Sat, 09 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3519/</guid><description>Formula When $a \gt b$, $$ \sqrt{a + b \pm 2\sqrt{a b}} = \sqrt{a} \pm \sqrt{b} $$ Explanation It might seem incredibly difficult to solve with two roots, but it can be directly solved in a perfect square form. Example $$ \begin{align*} \sqrt{13 - 2\sqrt{12}} &amp;amp;= \sqrt{12 + 1 - 2\sqrt{12 \cdot 1}} \\ &amp;amp;= \sqrt{(\sqrt{12})^{2} + (\sqrt{1})^{2} - 2\sqrt{12}\sqrt{1}} \\ &amp;amp;= \sqrt{(\sqrt{12} - \sqrt{1})^{2}} \\ &amp;amp;= \sqrt{12} - \sqrt{1}</description></item><item><title>Rejection Sampling</title><link>https://freshrimpsushi.github.io/en/posts/3518/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3518/</guid><description>Overview 1 Rejection sampling is one of the Monte Carlo methods, where a proposal distribution $q$, easy to sample from, is used to obtain samples following a given distribution $p$, especially when it&amp;rsquo;s difficult to sample from $p$ directly. Build-up Let&amp;rsquo;s assume we are given a random variable $X$ with a probability density function $p$. We want to sample from the distribution of $p$, but it&amp;rsquo;s challenging. (For instance, if</description></item><item><title>Bounded Function</title><link>https://freshrimpsushi.github.io/en/posts/3517/</link><pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3517/</guid><description>Definition1 For a function $f : X \to \mathbb{R}^{n}$, if there exists a constant $M \in \mathbb{R}$ that satisfies the following, $f$ is said to be bounded. $$ \left| f(x) \right| \le M \quad \text{for all $x \in X, $} $$ 설명 다시말해 $f(x)$의 치역이 유계 집합이면 $f$ is called a bounded function. Walter</description></item><item><title>중요도 샘플링</title><link>https://freshrimpsushi.github.io/en/posts/3516/</link><pubDate>Sun, 03 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3516/</guid><description>Overview1 Importance sampling is one of the Monte Carlo methods, and it is a sampling trick that can be used when approximating integrations (expectations) with finite sums. Build-up The probability that a value from a standard normal distribution exceeds $z$ is about $4$. $$ \begin{equation} \int_{4}^{\infty} \dfrac{1}{\sqrt{2\pi}} e^{-z^{2}/2}dz \approx 3.167 \times 10^{-5} \end{equation} $$ The Julia code to calculate this integral using Monte Carlo integration is as follows. using Distributions</description></item><item><title>Monte Carlo Integration</title><link>https://freshrimpsushi.github.io/en/posts/3515/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3515/</guid><description>Overview Monte Carlo integration is a numerical approximation method used when calculating the integral of a given function is difficult. Consider the following scenario: For an integrable function $f$ in the given $[0, 1]$, we know the formula of $f(x)$ but calculating its integral is not straightforward. However, we want to calculate the integral $I[f]$ of $f$. $$ \begin{equation} I[f] = \int_{[0,1]} f(x) dx \end{equation} $$ Definition Monte Carlo integration</description></item><item><title>Conditional Expectation Minimizes the Sum of Squared Deviations</title><link>https://freshrimpsushi.github.io/en/posts/3514/</link><pubDate>Wed, 29 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3514/</guid><description>Summary The following holds true: $$ \begin{equation} E\left[ Y | X \right] = \argmin_{f(X)} E\left[ (Y - f(X))^{2} | X \right] \end{equation} $$ $$ \begin{equation} E\left[ Y | X \right] = \argmin_{f(X)} E\left[ (Y - f(X))^{2} \right] \end{equation} $$ Proof (1) $$ \begin{align*} &amp;amp; \argmin_{f(X)} E\left[ (Y - f(X))^{2} | X \right] \\ &amp;amp;= \argmin_{f(X)} E\left[ Y^{2} - 2Yf(X) + f(X)^{2} | X \right] \\ &amp;amp;= \argmin_{f(X)} \left( E\left[ Y^{2}</description></item><item><title>Bilinear Forms and Hermitian Forms</title><link>https://freshrimpsushi.github.io/en/posts/3513/</link><pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3513/</guid><description>Definition1 Let’s say we have two vectors $\mathbf{x}, \mathbf{u} \in \mathbb{R}^{n}$ as follows. $$ \mathbf{x}=\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix},\quad \mathbf{u}^{T} = \begin{bmatrix} u_{1} &amp;amp; u_{2} &amp;amp; \cdots &amp;amp; u_{n} \end{bmatrix} $$ For a real constant $a_{ij} \in \mathbb{R} (1\le i,j \le n)$, the function $A : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$, defined as follows, is called the bilinear form. $$ A(\mathbf{u},\mathbf{x}):=\sum \limits_{i,k=1}^{n}</description></item><item><title>Quadratic Form</title><link>https://freshrimpsushi.github.io/en/posts/3512/</link><pubDate>Sat, 25 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3512/</guid><description>Definition $V$ is called a $n$dimensional vector space. For a given constant $a_{ij} \in \mathbb{R}(\text{or } \mathbb{C})$, the following second order homogeneous function $A : V \to \mathbb{R}(\text{or } \mathbb{C})$ is called a quadratic form. $$ A(\mathbf{x}) := \sum\limits_{i,j=1}^{n} a_{ij}x_{i}x_{j},\qquad (a_{ij} = a_{ji}) $$ Here, $\mathbf{x} = \begin{bmatrix} x_{1} &amp;amp; \cdots &amp;amp; x_{n} \end{bmatrix}^{T}$ holds. The term $i \ne j$ for $a_{ij}x_{i}x_{j}$ is called the cross product terms. Explanation According</description></item><item><title>Hyperbolic Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/3511/</link><pubDate>Thu, 23 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3511/</guid><description>Definition1 2 Consider the following 2nd order linear partial differential equation for $u(t,x)$. $$ Au_{tt} + Bu_{tx} + Cu_{xx} + Du_{t} + Eu_{x} + Fu + G = 0\qquad (ABC \ne 0) \tag{1} $$ Here, the coefficients $A, \dots, G$ are functions of $(t,x)$. $\Delta = B^{2} - 4AC$ is called the discriminant. A partial differential equation $(1)$ with a positive discriminant is called a hyperbolic PDE. $$ (1) \text{</description></item><item><title>Entropy of Normal Distribution</title><link>https://freshrimpsushi.github.io/en/posts/3510/</link><pubDate>Tue, 21 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3510/</guid><description>Theorem The entropy of the normal distribution $N(\mu, \sigma^{2})$ (when using natural logarithms) is as follows. $$ H = \dfrac{1}{2} \ln (2\pi e \sigma^{2}) = \ln \sqrt{2\pi e \sigma^{2}} $$ The entropy of the multivariate normal distribution $N_{p}(\boldsymbol{\mu}, \Sigma)$ is as follows. $$ H = \dfrac{1}{2}\ln \left[ (2 \pi e)^{p} \left| \Sigma \right| \right] = \dfrac{1}{2}\ln (\det (2\pi e \Sigma)) $$ $\left| \Sigma \right|$ is the determinant of the covariance</description></item><item><title>Drawing Subplots in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3509/</link><pubDate>Sun, 19 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3509/</guid><description>설명 Putting multiple photos in a single line can actually be done just by resizing them as shown above. ![3509_image1.png](3509_image1.png#center) To attach captions to each subplot and group them into a single Figure with numbering, the `subfig` package can be used. - `\subfloat[caption]{figure}` Note that it is `\subfloat` not `\subplot`. ```code2 ![3509_image3.png](3509_image3.png#center) ## Full Code ```code3</description></item><item><title>How to Insert GIF Animations into a PDF File Using LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3508/</link><pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3508/</guid><description>Description Using \animategraphics from the animate package allows you to compile a pdf file with GIF-like animations inserted. Such animations can be viewed on JavaScript-supporting pdf viewers like Adobe Acrobat or KDE Ocular. \animategraphics[options]{frame}{file name}{start}{end} options: Defines settings like size or autoplay. See the homepage for more details. autoplay: Autoplay loop: Loop playback frame: Frame rate (number of images played per second) file name: Enter the file name. The file</description></item><item><title>How to Replace Existing Output with New Output in Python and Display Progress</title><link>https://freshrimpsushi.github.io/en/posts/3507/</link><pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3507/</guid><description>English Translation When you want to find out the progress while the code is running, using the print() function, it will be printed like this: for i in range(1,6): print(f&amp;#34;진행 경과[{i}/5]&amp;#34;) At this time, you can add \r in front of the string</description></item><item><title>Existence of a Sequence of Simple Functions Converging to a Measurable Function</title><link>https://freshrimpsushi.github.io/en/posts/3506/</link><pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3506/</guid><description>Theorem1 Let $(X, \mathcal{E})$ be a measurable space. If $f : X \to [0, \infty]$ is a measurable function, then there exists a sequence of simple functions $\left\{ \phi_{n} \right\}$ satisfying the following: $$ 0 \le \phi_{1} \le \phi_{2} \le \cdots \le f \quad \text{and} \quad \phi \to f $$ If $f$ is bounded, $$ \phi \rightrightarrows f $$ Here, $\phi \to f$ denotes pointwise convergence, and $\phi \rightrightarrows f$</description></item><item><title>Converting Strings like 'False', 'True' to Bool Type in Python</title><link>https://freshrimpsushi.github.io/en/posts/3505/</link><pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3505/</guid><description>Code When you want to convert the string &amp;quot;False&amp;quot; into a boolean False in Python, the first code you might try looks like this. &amp;gt;&amp;gt;&amp;gt; bool(&amp;#34;False&amp;#34;) True &amp;gt;&amp;gt;&amp;gt; int(&amp;#34;False&amp;#34;) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; ValueError: invalid literal for int() with base 10: &amp;#39;False&amp;#39; However, in this case, since &amp;quot;False&amp;quot; is a non-empty string, bool(&amp;quot;False&amp;quot;) will return True. The function to return False from the</description></item><item><title>What is the Dot Product in Three-Dimensional Space?</title><link>https://freshrimpsushi.github.io/en/posts/3504/</link><pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3504/</guid><description>Definition The inner product of two 3-dimensional vectors $\mathbf{A} = (A_{x}, A_{y}, A_{z})$ and $\mathbf{B} = (B_{x}, B_{y}, B_{z})$ is defined as follows. $$ \mathbf{A} \cdot \mathbf{B} := A_{x}B_{x} + A_{y}B_{y} + A_{z}B_{z} $$ Description In fact, the above definition specifically refers to the dot product. The term inner product is a translation of inner product, which is often used to refer to a more general concept. However, in high</description></item><item><title>Python Matplotlib Basics &amp; Custom Line Styles</title><link>https://freshrimpsushi.github.io/en/posts/3503/</link><pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3503/</guid><description>Basic Configuration1 &amp;lsquo;-&amp;rsquo; or &amp;lsquo;solid&amp;rsquo;: Solid &amp;lsquo;&amp;ndash;&amp;rsquo; or &amp;lsquo;dashed&amp;rsquo;: Dashed &amp;lsquo;-.&amp;rsquo; or &amp;lsquo;dashdot&amp;rsquo;: Dash-Dot &amp;lsquo;:&amp;rsquo; or &amp;lsquo;dotted&amp;rsquo;: Dotted &amp;rsquo;none&amp;rsquo;, &amp;lsquo;None&amp;rsquo;, &amp;rsquo; &amp;lsquo;, or &amp;lsquo;&amp;rsquo;: No line linestyle or ls can be used to set the line style. import numpy as np import matplotlib.pyplot as plt solid = np.ones(10) dashed = 2*solid dashdot = 3*solid dotted = 4*solid none = 5*solid plt.plot(solid, ls=&amp;#34;-&amp;#34;, label=&amp;#34;-&amp;#34;) plt.plot(dashed, ls=&amp;#34;--&amp;#34;, label=&amp;#34;--&amp;#34;) plt.plot(dashdot, ls=&amp;#34;-.&amp;#34;, label=&amp;#34;-.&amp;#34;) plt.plot(dotted,</description></item><item><title>Solutions to 'RuntimeError: Parent directory does not exist' Error When Saving Models in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3502/</link><pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3502/</guid><description>Error When saving a model or weights in PyTorch, you may encounter the following error, even though the path definitely exists. &amp;gt;&amp;gt;&amp;gt; print(&amp;#34;Is exists path?: &amp;#34;, os.path.exists(directory)) Is exists path?: True &amp;gt;&amp;gt;&amp;gt; torch.save(model.state_dict(), directory + &amp;#39;weights.pt&amp;#39;) RuntimeError: Parent directory _____ does not exist. Solution In my case, the file path contained the special character deltaΔ, and removing it solved the problem. However, this is</description></item><item><title>How to Neatly Print without Axes, Scales, etc. in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3501/</link><pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3501/</guid><description>Code Plots.jl essentially outputs everything including grids, ticks, axes, and color bars by default, but if you want to make it clean without these, you can add the following options. colorbar=:none: Removes the color bar. showaxis = false: Removes the axes and ticks. grid=false: Removes the background grid. ticks=false: Removes both background grid and ticks. framestyle=:none: Removes both background grid and axes. using Plots surface(L, title=&amp;#34;default&amp;#34;) surface(L, title=&amp;#34;colorbar=:none&amp;#34;, colorbar=:none) surface(L,</description></item><item><title>How to Create a Meshgrid in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3500/</link><pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3500/</guid><description>Overview There is no direct equivalent to the meshgrid() function used in Python and MATLAB. If you only want to obtain the function values on a grid, there is a simpler method that does not require creating a grid. Code 2D Multiplying a column vector by a row vector gives the same result as taking the Kronecker product of a column vector and a row vector. U(t,x) = si</description></item><item><title>Broadcasting of Multivariable Functions in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3499/</link><pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3499/</guid><description>Overview Introducing how to broadcast multivariable functions in Julia. Like in Python, you can create a meshgrid, or you can easily calculate by creating vectors for each dimension. Bivariate Functions $$ u(t,x) = \sin(\pi x) e^{-\pi^{2}t} $$ To plot the function $(t,x) \in [0, 0.35] \times [-1,1]$ as above, the function values can be calculated like this: x = LinRange(-1., 1, 100) t = LinRange(0., 0.35, 200)&amp;#39; u1 = @.</description></item><item><title>Lusin's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3498/</link><pubDate>Sat, 28 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3498/</guid><description>Theorem 1 Let $f : E \to \mathbb{R}$ be a Lebesgue measurable function defined on the measurable set $E \subset \mathbb{R}$. Then, for any given positive number $\epsilon \gt 0$, there exists a measurable set $A \subset \mathbb{R}$ that satisfies the following. $$ m(A) \le \epsilon \quad \text{ and } \quad g = f|_{E\setminus A} \text{ is continuous.} $$ Here, $m$ is the Lebesgue measure. Generalization2 If $f$ is a</description></item><item><title>Egorov's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3497/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3497/</guid><description>Theorem1 2 Let a measure space $( X , \mathcal{E} , \mu)$ be given, and let $\mu$ be a finite measure. If a sequence of measurable functions $\left\{ f_{n} : X \to \mathbb{R} \right\}_{n \in \mathbb{N}}$ converges to a measurable function $f$ almost everywhere on $X$, then $f_{n}$ converges to $f$ almost uniformly and in measure. Explanation This theorem essentially states that for measurable functions, pointwise convergence and uniform convergence</description></item><item><title>Numerical Solutions of Heat Equations: Finite Difference Method</title><link>https://freshrimpsushi.github.io/en/posts/3495/</link><pubDate>Sun, 22 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3495/</guid><description>Numerical Solution of Heat Equation1 Let&amp;rsquo;s assume we are given a one-dimensional heat equation as follows. $$ \dfrac{\partial u}{\partial t} = \dfrac{\partial^{2} u}{\partial x^{2}},\qquad 0\le x \le 1,\quad t \ge 0 \tag{1} $$ Our goal is to approximate the solution using finite points. Let’s divide the space-time domain as follows. $$ \left\{ (\ell \Delta x, n\Delta t) : \ell=0,1,\dots,d+1,\ n\ge 0 \right\}\quad \text{ where } \Delta</description></item><item><title>Finite Difference Method</title><link>https://freshrimpsushi.github.io/en/posts/3494/</link><pubDate>Fri, 20 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3494/</guid><description>Definition1 2 The finite difference method is a numerical method for computing derivatives, approximating the derivative as the average rate of change over a short interval. Explanation The key to deriving the formula is the Taylor expansion. $$ f(x+h) = f(x) + f^{\prime}(x)h + \dfrac{f^{\prime \prime}(x)}{2!}h^{2} + \dfrac{f^{\prime \prime \prime}}{3!}h^{3} + \cdots \tag{1} $$ Rearranging to have only the derivative on the left side, $$ \begin{align*} f^{\prime}(x) &amp;amp;= \dfrac{f(x+h) -</description></item><item><title>Radial Functions</title><link>https://freshrimpsushi.github.io/en/posts/3493/</link><pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3493/</guid><description>Definition1 If the function defined above by $\mathbb{R}^{n}$ satisfies the following, it is called radial. $$ f(R\mathbf{x}) = f(\mathbf{x}) \text{ for all rotations } R $$ Explanation Directly translated as a radial function, but hardly anyone calls it that. The function value depends only on the distance $\left| x \right|$ from the origin. In physics, it is often referred to as spherical symmetry. Examples include gravity, the electric field created</description></item><item><title>The Fast Fourier Transform Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/3492/</link><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3492/</guid><description>Overview1 The Discrete Fourier Transform (DFT), when computed naively following its mathematical definition, has a time complexity of $\mathcal{O}(N^{2})$. However, by using the algorithm described below, the time complexity can be reduced to $\mathcal{O}(N\log_{2}N)$. This efficient computation method of the Discrete Fourier Transform is known as the Fast Fourier Transform (FFT). Buildup Let&amp;rsquo;s define multiplying two numbers and then adding them to another number as one operation. To compute the</description></item><item><title>Meaning of Weak and Strong in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/3491/</link><pubDate>Sat, 14 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3491/</guid><description>Description In mathematics, &amp;quot;weak&amp;quot; means &amp;quot;(logically) loose, less strict, less rigorous&amp;quot;. Being &amp;quot;less&amp;quot; something is to be understood in a relative sense. Conversely, &amp;quot;strong&amp;quot; means that the condition is (relatively) strict. In simple terms, &amp;quot;weak&amp;quot; can be translated as &amp;quot;in effect, frankly&amp;quot;. For example, if we talk about college entrance examination scores, the top cumulative 4% of scores are awarded the 1st grade. Strictly, accurately speaking, it&amp;rsquo;s correct that only</description></item><item><title>Specifying the Color of Axes, Axis Names, Ticks, and Tick Values in Julia Plots</title><link>https://freshrimpsushi.github.io/en/posts/3490/</link><pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3490/</guid><description>Overview The keywords related to specifying the color of axes and ticks in Plots.jl are as follows. Keyword Name Function guidefontcolor Specify axis name color foreground_color_border, fgcolor_border Specify axis color foreground_color_axis, fgcolor_axis Specify tick color foreground_color_text, fgcolor_text Specify tick value color Adding x_ or y_ in front of the keyword name applies it to the respective axis only. Code1 Axis Names The keyword to specify the color of axis names</description></item><item><title>Flux-PyTorch-TensorFlow Cheat Sheet</title><link>https://freshrimpsushi.github.io/en/posts/3489/</link><pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3489/</guid><description>Overview This document organizes code that performs the same functions in Flux, PyTorch, and TensorFlow. Julia-Matlab-Python-R Cheat Sheet Let&amp;rsquo;s assume the following environment for Flux. using Flux Let&amp;rsquo;s assume the following environment for PyTorch. import torch import torch.nn as nn import torch.nn.functional as F Let&amp;rsquo;s assume the following environment for TensorFlow. import tensorflow as tf from tensorflow import keras 1-Dimensional Tensor 줄리아Julia 파</description></item><item><title>Functions for Tensor Sorting in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3487/</link><pubDate>Fri, 06 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3487/</guid><description>torch.sort() torch.sort() takes a tensor as input and returns sorted values and indices. 1-dimensional tensor &amp;gt;&amp;gt;&amp;gt; x = torch.tensor([1, 3, -2, 5, -1, 7, 0]) &amp;gt;&amp;gt;&amp;gt; values, indices = torch.sort(x) &amp;gt;&amp;gt;&amp;gt; values tensor([-2, -1, 0, 1, 3, 5, 7]) &amp;gt;&amp;gt;&amp;gt; indices tensor([2, 4, 6, 0, 1, 3, 5]) Multi-dimensional tensor If only the tensor is inputted, it sorts each row. That is, torch.sort(x)=torch.sort(x, dim=1). If a dimension is specified, it</description></item><item><title>Solving 'RuntimeError: Boolean value of Tensor with more than one value is ambiguous' Error in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3486/</link><pubDate>Wed, 04 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3486/</guid><description>Error When using the loss function nn.MESLoss(), the following error occurred. RuntimeError Traceback (most recent call last) &amp;lt;ipython-input-75-8c6e9ea829d4&amp;gt; in &amp;lt;module&amp;gt; ----&amp;gt; 1 nn.MSELoss(y_pred, y) 2 frames /usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py in legacy_get_string(size_average, reduce, emit_warning) 33 reduce = True 34 ---&amp;gt; 35 if size_average and reduce: 36 ret = &amp;#39;mean&amp;#39; 37 elif reduce: RuntimeError: Boolean value of Tensor with more than one value is ambiguous Solution1 The code can be fixed by changing it</description></item><item><title>Sampling Randomly from a Given Distribution in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3485/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3485/</guid><description>Overview Introducing how to random sample from a given distribution in PyTorch. Various distributions such as Beta, Bernoulli, Cauchy, Gamma, Pareto, and Poisson are implemented. This article explains using the uniform distribution as an example. Code1 The code to random sample from the uniform distribution from $0$ to $5$ in PyTorch is as follows: &amp;gt;&amp;gt;&amp;gt; m = torch.distributions.Uniform(0.0, 5) &amp;gt;&amp;gt;&amp;gt; m.sample() tensor(1.6371) To sample a tensor of size $2 \times</description></item><item><title>딥러닝에서 레이어란?</title><link>https://freshrimpsushi.github.io/en/posts/3484/</link><pubDate>Sat, 30 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3484/</guid><description>Definition In deep learning, a linear transformation $L^{mn} : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is called a layer. Generalization In deep learning, for a fixed $\mathbf{b} \in \mathbb{R}^{m}$, an affine transformation $\mathbf{x} \mapsto L^{mn}(\mathbf{x}) + \mathbf{b}$ is also called a layer. Description In other words, a layer refers to a linear vector function. On the other hand, a non-linear scalar function is called an activation function. The reason it is called a</description></item><item><title>Quadratic Curve</title><link>https://freshrimpsushi.github.io/en/posts/3480/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3480/</guid><description>Definition Quadratic curves are planar curves represented by quadratic equations with two variables, such as: Parabola Ellipse Hyperbola Description A circle can be considered a special case of an ellipse, hence it is not specifically mentioned when generally speaking about quadratic curves.</description></item><item><title>Definition of Circle</title><link>https://freshrimpsushi.github.io/en/posts/3479/</link><pubDate>Wed, 20 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3479/</guid><description>Definition A set of points on a plane whose distance from a single point $O$ is constant is called a circle. The point $O$ is called the center of the circle. The distance $r$ from the center is called the radius. Description It can be considered as a special case of an ellipse. An ellipse where the two foci are the same point becomes a circle.</description></item><item><title>Hyperbola</title><link>https://freshrimpsushi.github.io/en/posts/3478/</link><pubDate>Mon, 18 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3478/</guid><description>Definition 1 The set of points on a plane whose difference in distances to two distinct points $F$, $F^{\prime}$ is constant is called a hyperbola. $\\ $ The points $F$, $F^{\prime}$ are called focus. The midpoint of segment $\overline{FF^{\prime}}$ is called center. The two points where the hyperbola intersects segment $\overline{FF^{\prime}}$, $A$, $A^{\prime}$, are called vertices. The segment $\overline{AA^{\prime}}$ is called major axis. Description Method of Discrimination For a given</description></item><item><title>Parabola</title><link>https://freshrimpsushi.github.io/en/posts/3477/</link><pubDate>Sat, 16 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3477/</guid><description>Definition 1 For a point $F$ on the plane and a line $l$ that does not pass through it, the set of points equidistant from $F$ and $l$ is called a parabola. $\\ $ $F$ is called the focus. $l$ is called the directrix. The line passing through $F$ and perpendicular to $l$ is called the axis of the parabola. The intersection of the axis and the parabola is called</description></item><item><title>Summary of Measure Theory and Probability Theory</title><link>https://freshrimpsushi.github.io/en/posts/3473/</link><pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3473/</guid><description>Overview This is a summary of definitions and concepts for those who have already studied measure theory and probability. It is intended to be viewed when definitions are confusing or unrecognizable, and when a general review is needed. Measure Theory Algebras An algebra of sets on nonempty set $X$ is a nonempty collection $\mathcal{A}$ of subsets of $X$ is colsed under finite unions ans complements. $\sigma$-algebra is an algebra that</description></item><item><title>Physics에서 Coordinate Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3472/</link><pubDate>Wed, 06 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3472/</guid><description>Definition Two-dimensional coordinate systems $(x_{1}, \dots, x_{n})$ and $(x_{1}^{\prime}, \dots, x_{n}^{\prime})$ are related by the following functions $f_{1}, \dots, f_{n}$ (or the act of changing coordinates itself) is referred to as a coordinate transformation from $(x_{1}, \dots, x_{n})$ coordinate system to $(x_{1}^{\prime}, \dots, x_{n}^{\prime})$ coordinate system. $$ \begin{align*} x_{1}^{\prime} &amp;amp;= f_{1}(x_{1}, \dots, x_{n}) \\ x_{2}^{\prime} &amp;amp;= f_{2}(x_{1}, \dots, x_{n}) \\ &amp;amp;\vdots \\ x_{n}^{\prime} &amp;amp;= f_{n}(x_{1}, \dots, x_{n}) \\ \end{align*} $$</description></item><item><title>Coordinate Systems and Coordinates in Physics</title><link>https://freshrimpsushi.github.io/en/posts/3471/</link><pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3471/</guid><description>Definition When each pair of $n$-ordered pairs $(a_{1}, a_{2}, \dots, a_{n})$ uniquely determines a point in a $n$-dimensional space, the set of these $n$-ordered pairs is called a ($n$-dimensional) coordinate system, and the element $(a_{1}, a_{2}, \dots, a_{n})$ of the coordinate system is called the coordinate of that point. Description In physics, it is generally $n \le 4$. The above definition is nothing more than a reorganization of concepts that</description></item><item><title>Parabolic Partial Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/3470/</link><pubDate>Sat, 02 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3470/</guid><description>Definition1 2 Consider the following second-order linear partial differential equation for $u(t,x)$. $$ Au_{tt} + Bu_{tx} + Cu_{xx} + Du_{t} + Eu_{x} + Fu + G = 0\qquad (ABC \ne 0) \tag{1} $$ Here, the coefficients $A, \dots, G$ are functions of $(t,x)$. $\Delta = B^{2} - 4AC$ is called the discriminant. A partial differential equation $(1)$ with a discriminant of $0$ is referred to as a parabolic PDE. $$</description></item><item><title>Elliptic Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/3469/</link><pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3469/</guid><description>Definition1 2 Consider the following second-order linear partial differential equation for $u(x,y)$. $$ Au_{xx} + Bu_{xy} + Cu_{yy} + Du_{x} + Eu_{y} + Fu + G = 0\qquad (ABC \ne 0) \tag{1} $$ Here, the coefficients $A, \dots, G$ are functions of $(x,y)$. $\Delta = B^{2} - 4AC$ is called the discriminant. A partial differential equation $(1)$ with a negative discriminant is called an elliptic PDE. $$ (1) \text{ is</description></item><item><title>Inequalities for the Logarithmic Function 1-1/x &lt; log x &lt; x-1</title><link>https://freshrimpsushi.github.io/en/posts/3468/</link><pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3468/</guid><description>Theorem For a logarithmic function with base $e$, the following inequality holds: $$ 1 - \dfrac{1}{x} \le \ln x \le x - 1\qquad \text{ for } x \gt 0 $$ Proof1 Part 1. $\ln x \le x - 1$ Let&amp;rsquo;s set it as $f(x) = x - 1 - \ln x$. Differentiating it gives, $f^{\prime}(x) = 1 - \dfrac{1}{x}$ $(x&amp;gt;0)$. At $0 \lt x \lt 1$, it is $f^{\prime} \lt</description></item><item><title>Mutual Information</title><link>https://freshrimpsushi.github.io/en/posts/3467/</link><pubDate>Sun, 27 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3467/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Relative Entropy (Kullback-Leibler Divergence) in Classical Information Theory</title><link>https://freshrimpsushi.github.io/en/posts/3466/</link><pubDate>Fri, 25 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3466/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>What is Conditional Entropy in Classical Information Theory?</title><link>https://freshrimpsushi.github.io/en/posts/3465/</link><pubDate>Wed, 23 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3465/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>What is Joint Entropy in Classical Information Theory?</title><link>https://freshrimpsushi.github.io/en/posts/3464/</link><pubDate>Mon, 21 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3464/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Sampling Randomly from a Given Distribution in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3463/</link><pubDate>Sat, 19 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3463/</guid><description>설명 Using the Distributions.jl package, you can randomly sample from a given distribution.</description></item><item><title>Sampling Randomly in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3462/</link><pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3462/</guid><description>Description1 In Julia, the function for random sampling is as follows: rand([rng=default_rng()], [S], [dims...]) rng stands for Random Number Generator, which specifies the random number generation algorithm. If you don&amp;rsquo;t understand what this means, it&amp;rsquo;s okay to leave it untouched. S likely stands for Set, and it is a variable that specifies the set from which the random sampling will occur. The variables that can be input for S include</description></item><item><title>Writing TeX Code in Gmail (Email)</title><link>https://freshrimpsushi.github.io/en/posts/3461/</link><pubDate>Tue, 15 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3461/</guid><description>Description Installing the Chrome extension TeX for Gmail allows you to use TeX code in Gmail. It&amp;rsquo;s also available for the Naver Whale browser. The usage is straightforward. Write your TeX code and press F8, then all the mathematical formulas in the email get rendered. You can copy and paste this into another email as well. As shown in the GIF below, you don&amp;rsquo;t necessarily have to highlight anything; simply</description></item><item><title>CSS color name tags</title><link>https://freshrimpsushi.github.io/en/posts/3459/</link><pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3459/</guid><description>Overview1 140+ CSS color palettes with names. Code</description></item><item><title>Drawing Diagrams of Universal Properties with TikZ</title><link>https://freshrimpsushi.github.io/en/posts/3458/</link><pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3458/</guid><description>Code \documentclass[12pt]{article} \usepackage{tikz} \begin{document} $$ \begin{tikzpicture}[node distance=3.5cm, auto] \node (Vtimes) {$V_{1} \times V_{2}$}; \node (Votimes) [right of = Vtimes] {$V_{1} \otimes V_{2}$}; \node (W) [node distance=2.0cm, below of = Votimes] {$W$}; \draw[-&amp;gt;] (Vtimes) to node [swap] {$\phi$} (W); \draw[-&amp;gt;] (Vtimes) to node {$f$} (Votimes); \draw[-&amp;gt;, dashed] (Votimes) to node {$\psi$} (W); \end{tikzpicture} $$ \end{document} 설명 $\KaTeX$ is still lacking the ability to draw diagrams. To draw diagrams like the</description></item><item><title>Matrix Representation of the Sum and Scalar Multiplication of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3457/</link><pubDate>Sat, 05 Aug 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3457/</guid><description>Theorem Let $V, W$ be a finite-dimensional vector space with a given ordered basis $\beta, \gamma$. Also, let $T, U : V \to W$. Then, the following hold:$\\[0.5em]$ $[T + U]_{\beta}^{\gamma} = [T]_{\beta}^{\gamma} + [U]_{\beta}^{\gamma}$ $[aT]_{\beta}^{\gamma} = a[T]_{\beta}^{\gamma}$ Here, $[T]_{\beta}^{\gamma}$ is the matrix representation of $T$. Proof Since the proofs are similar, we will only prove the first equation. Let $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ and $\gamma =</description></item><item><title>Special Unitary Group</title><link>https://freshrimpsushi.github.io/en/posts/3454/</link><pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3454/</guid><description>Definition The set of unitary matrices whose determinant is $1$ is denoted by $\mathrm{SU}(n)$ and called the special unitary group of degree $n$. $$ \mathrm{SU}(n) := \left\{ n \times n \text{ unitary matrix} \right\} = {\left\{ A \in M_{n \times n}(\mathbb{C}) : A A^{\ast} = I \right\}} $$ Here, $A^{\ast}$ is the conjugate transpose matrix. Explanation Since it consists only of unitary matrices, it forms a group with respect to</description></item><item><title>Orthogonal Group</title><link>https://freshrimpsushi.github.io/en/posts/3453/</link><pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3453/</guid><description>Definition $n \times n$ The set of orthogonal matrices is denoted by $\mathrm{O}(n)$ and is called the $n$-dimensional orthogonal group. $$ \mathrm{O}(n) := {\left\{ A \in M_{n \times n}(\mathbb{R}) : AA^{T} = I \right\}} $$ Description Since it is a set of orthogonal matrices, only invertible matrices exist. Hence, it forms a group with respect to matrix multiplication, and is a subgroup of the general linear group $\mathrm{GL}(n, \mathbb{R})$. It</description></item><item><title>Special Linear Group</title><link>https://freshrimpsushi.github.io/en/posts/3452/</link><pubDate>Wed, 26 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3452/</guid><description>Definition The set of matrices with determinant $1$ is denoted by $\mathrm{SL}(n, \mathbb{R})$ and called the special linear group of degree $n$. $$ \mathrm{SL}(n, \mathbb{R}) := {\left\{ A \in M_{n \times n}(\mathbb{R}) : \det{A} = 1 \right\}} $$ Description Since it is a set of matrices with determinant $1$, only invertible matrices exist. Thus, it forms a group under matrix multiplication and is a subgroup of the general linear group</description></item><item><title>Unitary Group</title><link>https://freshrimpsushi.github.io/en/posts/3451/</link><pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3451/</guid><description>Definition $n \times n$ The set of unitary matrices is denoted as $\mathrm{U}(n)$ and is called the unitary group of degree $n$. $$ \mathrm{U}(n) := \left\{ n \times n \text{ unitary matrix} \right\} = {\left\{ A \in M_{n \times n}(\mathbb{C}) : A A^{\ast} = I \right\}} $$ Here, $A^{\ast}$ is the conjugate transpose matrix. Explanation Since it only collects unitary matrices, it forms a group under matrix multiplication. It is</description></item><item><title>General Linear Group</title><link>https://freshrimpsushi.github.io/en/posts/3450/</link><pubDate>Sat, 22 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3450/</guid><description>Definition The set of real invertible $n \times n$ matrices is denoted by $\mathrm{GL}(n, \mathbb{R})$ or $\mathrm{GL}_{n}(\mathbb{R})$ and is called the general linear group of degree $n$. $$ \mathrm{GL}(n, \mathbb{R}) := \left\{ n \times n \text{ invertible matrix} \right\} = M_{n \times n}(\mathbb{R}) \setminus {\left\{ A \in M_{n \times n}(\mathbb{R}) : \det{A} = 0 \right\}} $$ Explanation Since it consists only of invertible matrices, it forms a group with respect</description></item><item><title>Convolutional Neural Network (CNN)</title><link>https://freshrimpsushi.github.io/en/posts/3449/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3449/</guid><description>Definition A composite function obtained by appropriately combining a convolutional layer, pooling layer, and activation function is called a convolutional neural network. Description A function composed of a convolutional layer and an activation function is called a convolutional neural network. CNNs demonstrate excellent performance predominantly in image-related tasks. In the case of MLP, when values pass through each layer, they are sent to a fully connected layer, which can lead</description></item><item><title>딥러닝에서 풀링층이란?</title><link>https://freshrimpsushi.github.io/en/posts/3448/</link><pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3448/</guid><description>Overview In artificial neural networks, a pooling layer is a function that reduces the dimension of input data into smaller local units. Keeping only the maximum value within a specified region is called max pooling, while retaining the average value of a specified region is referred to as average pooling. Definition For $m \times m$ matrix $\mathbf{X} = [X_{ij}]$, the following function is called max pooling. $$ \begin{align*} P_{\text{max}} :</description></item><item><title>Multilayer Perceptron (MLP), Fully Connected Neural Network (FCNN)</title><link>https://freshrimpsushi.github.io/en/posts/3447/</link><pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3447/</guid><description>Definition Let $L_{i} : \mathbb{R}^{n_{i}} \to \mathbb{R}^{n_{i+1}}$ be referred to as a fully connected layer. Let $\sigma : \mathbb{R} \to \mathbb{R}$ be referred to as an activation function. The composition of these is called a multilayer perceptron. $$ \operatorname{MLP}(\mathbf{x}) = T_{N} \circ \overline{\sigma} \circ T_{N-1} \circ \overline{\sigma} \circ \cdots \circ T_{1} (\mathbf{x}) $$ Here, $\overline{\sigma}$ is a function that applies $\sigma$ to each component. $$ \overline{\sigma}(\mathbf{x}) = \begin{bmatrix} \sigma(x_{1}) \\</description></item><item><title>Total Variation in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3232/</link><pubDate>Fri, 14 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3232/</guid><description>Definition1 A regular curve $\gamma$ is considered piecewise simple curve with a period of $L$ being a closed curve. Let $\mathbf{Z}(t)$ be a continuous vector field along $\gamma$. Suppose the vector field $\mathbf{V}$ satisfies $\left| \mathbf{V}(p) \right| = 1$ $\forall p \in U$. Let $\alpha$ be a function mapping the angle between $\mathbf{Z}$ and $\mathbf{V}$. $$ \alpha (t) = \angle \left( \mathbf{V}(\gamma (t)), \mathbf{Z}(t) \right) $$ Now, the total angular</description></item><item><title>Iris Dataset</title><link>https://freshrimpsushi.github.io/en/posts/3445/</link><pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3445/</guid><description>Overview1 The Iris dataset refers to a dataset about the observation records of iris flowers, created by the American botanist, Edgar Anderson, and introduced by the British statistician, Ronald Fisher2. Description It is the most commonly used dataset in machine learning and data analysis practice.3 It consists of data from observing 50 flowers each of the three species of iris: setosa, versicolor, and virginica. It measures each flo</description></item><item><title>MNIST Database</title><link>https://freshrimpsushi.github.io/en/posts/3444/</link><pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3444/</guid><description>Overview1 $$ \includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$ The MNIST database refers to a dataset of digit handwriting from American high school students and Census Bureau employees. It is commonly known as [MNIST]. Official Website Description This dataset is frequently used as an example for beginners in machine learning/deep learning. NIST originally collected handwritten data in the following format for the evaluation of character recognition technology for automated sorting of handwritten postal codes. Yann</description></item><item><title>Various Deep Learning Frameworks of Julia</title><link>https://freshrimpsushi.github.io/en/posts/3443/</link><pubDate>Sat, 08 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3443/</guid><description>Overview Last modified date: November 22, 2022 Among Julia&amp;rsquo;s representative deep learning frameworks, there is Flux.jl. Along with it, other frameworks such as Knet.jl and Lux.jl will be briefly introduced. Description Flux Flux is the official deep learning framework of Julia. Various packages of GraphNeuralNetworks.jl and SciML are implemented based on Flux. Although Flux&amp;rsquo;s functionality and features are still lacking compared to TensorFlow and PyTorch, it might eventually catch up</description></item><item><title>Automatic differentiation</title><link>https://freshrimpsushi.github.io/en/posts/3442/</link><pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3442/</guid><description>Definition1 2 Automatic differentiation refers to a method for obtaining the derivative of a function defined by computer programming code. It is also abbreviated as AD or autodiff. Explanation Automatic differentiation involves using the chain rule to compute the derivative of a composite function composed of functions whose derivatives are already known. In simple terms, it is the chain rule itself. Implementing the chain rule in programming code constitutes automatic</description></item><item><title>How to Use Fast Fourier Transform (FFT) in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3440/</link><pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3440/</guid><description>Overview 1 2 The Fastest Fourier Transform in the West (FFTW) is a software library developed by Matteo Frigo and Steven G. Johnson at the Massachusetts Institute of Technology (MIT) for computing the Discrete Fourier Transform. While there exists a Julia package named AbstractFFTs.jl for FFT implementation, it is not intended to be used on its own but rather to aid in the implementation of fast Fourier transforms, such as</description></item><item><title>How to Change Basic Data Types in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3439/</link><pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3439/</guid><description>Overview In fields like machine learning, 32-bit floating point numbers are used instead of 64-bit ones for improving computation speed and saving memory. Therefore, in PyTorch, when tensors are created, their data type is fundamentally 32-bit floating point numbers by default. In Julia, there&amp;rsquo;s a machine learning package called Flux.jl, which takes Julia&amp;rsquo;s standard arrays as input for the neural networks it implements. The fact that it does not use</description></item><item><title>What is One-Hot Encoding in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/3438/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3438/</guid><description>Definition Given a set $X \subset \mathbb{R}^{n}$, suppose its subsets $X_{i}$ satisfy the following. $$ X = X_{1} \cup \cdots \cup X_{N} \quad \text{and} \quad X_{i} \cap X_{j} = \varnothing \enspace (i \ne j) $$ Let&amp;rsquo;s call $\beta = \left\{ e_{1}, \dots, e_{N} \right\}$ the standard basis of $\mathbb{R}^{N}$. Then, the following function, or mapping $x \in X$ itself, is called one-hot encoding. $$ \begin{align*} f : X &amp;amp;\to \beta</description></item><item><title>Encoding and Decoding in Information Theory</title><link>https://freshrimpsushi.github.io/en/posts/3437/</link><pubDate>Mon, 26 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3437/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Hadamard Product of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3436/</link><pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3436/</guid><description>Definition The Hadamard product $A \odot B$ of two matrices $A, B \in M_{m \times n}$ is defined as follows. $$ A \odot B = \begin{bmatrix} a_{11} &amp;amp; \cdots &amp;amp; a_{1n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} \odot\begin{bmatrix} b_{11} &amp;amp; \cdots &amp;amp; b_{1n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ b_{m1} &amp;amp; \cdots &amp;amp; b_{mn} \end{bmatrix} := \begin{bmatrix} a_{11}b_{11} &amp;amp; \cdots &amp;amp; a_{1n}b_{1n}</description></item><item><title>줄리아에서 U-net 구현하기</title><link>https://freshrimpsushi.github.io/en/posts/3434/</link><pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3434/</guid><description>Overview This document introduces how to implement the U-Net presented in the paper &amp;ldquo;U-Net: Convolutional networks for Biomedical Image Segmentation&amp;rdquo; using Julia. Code The structure of U-Net is divided into two main parts: a contracting path where the encoder compresses the input data, and an expansive path where the compressed data is restored. In the image below, the left half is the contracting path, and the right is the expansive</description></item><item><title>Impossibility of Cloning Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3433/</link><pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3433/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Solvay-Kitaev Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3432/</link><pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3432/</guid><description>Theorem1 Let&amp;rsquo;s consider $\mathcal{G}$ as a finite subset of the special unitary group $\mathrm{SU}(2)$ that is closed under taking inverses. $$ \mathcal{G} \subset \mathrm{SU}(2), \qquad g \in \mathcal{G} \implies g^{-1} \in \mathcal{G} $$ Then, the free group $\braket{\mathcal{G}}$ generated by $\mathcal{G}$ is dense in $\mathrm{SU}(2)$. Specialization2 By composing the Hadamard gate $H$, phase gate $R_{\pi/4}$, and $\operatorname{CNOT}_{q}$gate, any quantum gate can be approximated as closely as desired. Description In classical</description></item><item><title>Quantum Fredkin/CSWAP Gate</title><link>https://freshrimpsushi.github.io/en/posts/3431/</link><pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3431/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Quantum Topology/CCNOT Gate</title><link>https://freshrimpsushi.github.io/en/posts/3430/</link><pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3430/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Exchange Gate</title><link>https://freshrimpsushi.github.io/en/posts/3429/</link><pubDate>Sat, 10 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3429/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Quantum CNOT Gate</title><link>https://freshrimpsushi.github.io/en/posts/3428/</link><pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3428/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Pauli Gates</title><link>https://freshrimpsushi.github.io/en/posts/3427/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3427/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Topological Gates</title><link>https://freshrimpsushi.github.io/en/posts/3426/</link><pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3426/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Hadamard Gate</title><link>https://freshrimpsushi.github.io/en/posts/3425/</link><pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3425/</guid><description>English Translation 양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양</description></item><item><title>Quantum Gates and Quantum Circuits</title><link>https://freshrimpsushi.github.io/en/posts/3424/</link><pubDate>Wed, 31 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3424/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Qubits: The Basic Unit of Information in Quantum Computers</title><link>https://freshrimpsushi.github.io/en/posts/3423/</link><pubDate>Mon, 29 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3423/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Bit: Unit of Information Classical Computer</title><link>https://freshrimpsushi.github.io/en/posts/3422/</link><pubDate>Sat, 27 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3422/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Matrix Representation of Tensor Product</title><link>https://freshrimpsushi.github.io/en/posts/3419/</link><pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3419/</guid><description>Buildup1 Choose bases $\mathcal{V}, {\mathcal{V}}^{\prime}$ respectively for the finite-dimensional vector spaces $V, V^{\prime}$. Then, there exists a matrix equivalent to the linear transformation $\phi : V \to V^{\prime}$, called its matrix representation $\phi$. Now assume we have the finite-dimensional vector space $V, V^{\prime}, W, W^{\prime}$ and its ordered basis $\mathcal{V}, {\mathcal{V}}^{\prime}, \mathcal{W}, {\mathcal{W}}^{\prime}$, as well as two linear transformations $\phi : V \to V^{\prime}$ and $\psi : W \to W^{\prime}$.</description></item><item><title>Kronecker Product of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3418/</link><pubDate>Fri, 19 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3418/</guid><description>Definition1 The Kronecker product of two matrices $A = [a_{ij}] \in M_{m \times n}$, $B \in M_{p \times q}$ is defined as follows. $$ A \otimes B := \begin{bmatrix} a_{11} B &amp;amp; \cdots &amp;amp; a_{1n} B \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} B &amp;amp; \cdots &amp;amp; a_{mn} B \end{bmatrix} \in M_{mp \times nq} $$ Explanation The matrix representation of the tensor product of two linear transformations is defined</description></item><item><title>Tensor Product of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3417/</link><pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3417/</guid><description>Buildup1 Given finite-dimensional vector spaces $V_{1}, V_{2}, W_{1}, W_{2}$ and linear transformations $\phi_{1} : V_{1} \to W_{1}$, $\phi_{2} : V_{2} \to W_{2}$, one can consider the following bilinear transformation. $$ V_{1} \times V_{2} \to W_{1} \otimes W_{2} $$ $$ (v_{1}, v_{2}) \mapsto \phi_{1}(v_{1}) \otimes \phi_{2}(v_{2}) $$ $\phi_{1}, \phi_{2}$ is a linear transformation, and it is easy to see that this function is bilinear due to the definition of product vectors.</description></item><item><title>Universal Properties of Tensor Products</title><link>https://freshrimpsushi.github.io/en/posts/3416/</link><pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3416/</guid><description>Buildup1 Given a finite-dimensional vector space $V_{1}, \dots, V_{r}$. If $n_{i} = \dim V_{i}$, and we select a basis for each vector space, we obtain the following coordinate vector as a bijective function $f_{i}$. $$ \begin{align*} f _{i}: &amp;amp; V_{i} \to \mathbb{C}^{n_{i}} \\ &amp;amp; v_{i} \mapsto (a_{i1}, \dots, a_{i n_{i}}) \end{align*} $$ From this, the following multilinear transformation $f$ is naturally defined. $$ \begin{align*} f : V_{1} \times \cdots \times</description></item><item><title>Tensor Product of Product Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3415/</link><pubDate>Sat, 13 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3415/</guid><description>Buildup For convenience, we will develop the concept in the complex number space $\mathbb{C}$, but $\mathbb{R}$ or any vector space is also applicable. Let&amp;rsquo;s denote the set of functions from the finite set $\Gamma$ to the complex number space as indicated by $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ When $\Gamma = \mathbf{n} = \left\{ 1, \dots, n \right\}$, it essentially becomes $\mathbb{C}^{\mathbf{n}} = \mathbb{C}^{n}$,</description></item><item><title>Tensor Product of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3414/</link><pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3414/</guid><description>Buildup1 For convenience, this exposition is developed for the complex number space $\mathbb{C}$, but it is applicable to $\mathbb{R}$ or any vector space as well. Let&amp;rsquo;s denote the set of functions from a finite set $\Gamma$ to the complex number space as described by $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ Let&amp;rsquo;s set $\Gamma$ as $\mathbf{n} = \left\{ 1, 2, \dots, n \right\}$. A function</description></item><item><title>Shadow and Injection</title><link>https://freshrimpsushi.github.io/en/posts/3413/</link><pubDate>Tue, 09 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3413/</guid><description>Definition1 For $n \in \mathbb{N}$ and $0 \le i \le n$, the following function $p_{i}$ $$ \begin{align*} p_{i} : &amp;amp;\left\{ 0, 1 \right\}^{n+1} \to \left\{ 0, 1 \right\}^{n} \\ &amp;amp; (a_{0}, \dots, a_{n}) \mapsto (a_{0}, \dots, a_{i-1}, a_{i+1}, \dots, a_{n}) \end{align*} $$ is called a projection. The following two functions $\imath_{i}$, $\jmath_{i}$ $$ \begin{align*} \imath : &amp;amp;\left\{ 0, 1 \right\}^{n} \to \left\{ 0, 1 \right\}^{n+1} \\ &amp;amp; (a_{0}, \dots, a_{n-1})</description></item><item><title>Fredkin/CSWAP Gate</title><link>https://freshrimpsushi.github.io/en/posts/3412/</link><pubDate>Sun, 07 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3412/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Toffoli/CCNOT Gate</title><link>https://freshrimpsushi.github.io/en/posts/3411/</link><pubDate>Fri, 05 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3411/</guid><description>Definition1 The following vector-valued Boolean function is called a Toffoli gate. $$ T : \left\{ 0, 1 \right\}^{3} \to \left\{ 0, 1 \right\}^{3} $$ $$ T (a, b, c) = (a, b, (a \land b) \oplus c) $$ The $\text{CCNOT}$ gate is also known as. Description In the Toffoli gate, if the first two inputs are both $1$, the third input is inverted. In all other cases, the input and</description></item><item><title>Controlled NOT(CNOT) Gate</title><link>https://freshrimpsushi.github.io/en/posts/3410/</link><pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3410/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Replication Function</title><link>https://freshrimpsushi.github.io/en/posts/3409/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3409/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>What is a Functionally Complete Set?</title><link>https://freshrimpsushi.github.io/en/posts/3408/</link><pubDate>Sat, 29 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3408/</guid><description>Definition1 Let&amp;rsquo;s assume that a set of Boolean functions $\left\{ f_{k} \right\} = \left\{ f_{k} : \left\{ 0, 1 \right\}^{n_{k}} \to \left\{ 0, 1 \right\} \right\}_{k\in \Gamma}$ is given. $\Gamma$ is a finite set. If any Boolean function $$ \left\{ 0, 1 \right\}^{n} \to \left\{ 0, 1 \right\}\quad (n \in \mathbb{N}) $$ can be expressed by the compositions of $\left\{ f_{k} \right\}$, then the set $\left\{ f_{k} \right\}$ is called</description></item><item><title>NOR Gate</title><link>https://freshrimpsushi.github.io/en/posts/3407/</link><pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3407/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>NAND Gate</title><link>https://freshrimpsushi.github.io/en/posts/3406/</link><pubDate>Tue, 25 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3406/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Exclusive Disjuction, XOR Gate</title><link>https://freshrimpsushi.github.io/en/posts/3405/</link><pubDate>Sun, 23 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3405/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Negation, NOT Gate</title><link>https://freshrimpsushi.github.io/en/posts/3404/</link><pubDate>Fri, 21 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3404/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Disjunction, OR Gate</title><link>https://freshrimpsushi.github.io/en/posts/3403/</link><pubDate>Wed, 19 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3403/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Conjunction, AND Gate</title><link>https://freshrimpsushi.github.io/en/posts/3402/</link><pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3402/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Boolean Functions</title><link>https://freshrimpsushi.github.io/en/posts/3401/</link><pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3401/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>How to Check the Device on which the PyTorch Model/Tensor is loaded</title><link>https://freshrimpsushi.github.io/en/posts/3364/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3364/</guid><description>Code1 2 It can be checked with get_device(). &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import torch.nn as nn &amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available() True &amp;gt;&amp;gt;&amp;gt; Device = torch.device(&amp;#34;cuda:0&amp;#34; if torch.cuda.is_available() else &amp;#34;cpu&amp;#34;) # Model &amp;gt;&amp;gt;&amp;gt; model = nn.Sequential(nn.Linear(5,10), nn.ReLU(), nn.Linear(10,10), nn.ReLU(), nn.Linear(10,1)) &amp;gt;&amp;gt;&amp;gt; next(model.parameters()).get_device() -1 &amp;gt;&amp;gt;&amp;gt; model.to(Device) Sequential( (0): Linear(in_features=5, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=10, bias=True) (3): ReLU() (4): Linear(in_features=10, out_features=1, bias=True) ) &amp;gt;&amp;gt;&amp;gt; next(model.parameters()).get_device() 0 # Tensor &amp;gt;&amp;gt;&amp;gt; A = torch.rand(5) &amp;gt;&amp;gt;&amp;gt;</description></item><item><title>고전정보이론에서 정보량이란?</title><link>https://freshrimpsushi.github.io/en/posts/3398/</link><pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3398/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>What is a Flag in Linear Algebra?</title><link>https://freshrimpsushi.github.io/en/posts/3397/</link><pubDate>Fri, 07 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3397/</guid><description>Definition1 2 $n$Dimension Vector space $V$Subspaces sequences $\left\{ W_{i} \right\}$ satisfying the following equations are termed flags. $$ \left\{ \mathbf{0} \right\} = W_{0} \lneq W_{1} \lneq W_{2} \lneq \cdots \lneq W_{k-1} \lneq W_{k} = V $$ By definition, the following holds. $$ 0 = \dim V_{0} \lt \dim V_{1} \lt \dim V_{2} \lt \cdots \lt \dim V_{k-1} \lt \dim V_{k} = n $$ Explanation The term flag is used because,</description></item><item><title>What is a Softplus Function?</title><link>https://freshrimpsushi.github.io/en/posts/3396/</link><pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3396/</guid><description>Definition1 The function defined below is called a softplus. $$ \zeta (x) = \ln (1 + e^{x}) $$ Ian Goodfellow, Deep Learning, p68&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Direct Sum of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3395/</link><pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3395/</guid><description>Definition1 The direct sum of two matrices $B \in M_{m\times n}$, $C \in M_{p\times q}$ is defined as matrix $A$ of the following $(m+p) \times (n+q)$, and is denoted by $B \oplus C$. $$ A = B \oplus C := \begin{bmatrix} b_{11} &amp;amp; \cdots &amp;amp; b_{1n} &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ b_{m1} &amp;amp; \cdots &amp;amp; b_{mn}</description></item><item><title>Zero Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3394/</link><pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3394/</guid><description>Definition The matrix of size $m\times n$ with all elements being $0$ is called a zero matrix, and is denoted as $O_{m\times n}$ or simply as $O$. Description Other notations include $Z_{m \times n}$, $Z$, $\mathbf{0}_{m\times n}$, or $\mathbf{0}$. It is better to write the number $0$ in bold to avoid confusion (actually, it is better just to use $O$). A zero matrix is the identity element for matrix addition.</description></item><item><title>Direct Sum of Invariant Subspaces and Its Characteristic Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/3393/</link><pubDate>Thu, 30 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3393/</guid><description>Theorem1 Let $T : V \to V$ be a linear transformation on a finite-dimensional vector space $V$ above. Assume that $V$ is the direct sum of the $T$-invariant subspaces $W_{i}$. $$ V = W_{1} \oplus W_{2} \oplus \cdots \oplus W_{k} $$ Let $f_{i}(t)$ be the characteristic polynomial of the restriction $T|_{W_{i}}$. Then, the characteristic polynomial of $T$, $f(t)$, is as follows. $$ f(t) = f_{1}(t) \cdot f_{2}(t) \cdot \cdots \cdot</description></item><item><title>Types of Cross Sections in Medical Imaging</title><link>https://freshrimpsushi.github.io/en/posts/3392/</link><pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3392/</guid><description>Overview The planes dividing the human body can be categorized into three types based on their orientation: coronal plane, sagittal plane, and axial plane. Distinguishing these planes is crucial, especially in medical imaging, as the information derived from an image can significantly vary depending on the plane it is associated with. Description In the descriptions below, it is assumed that the human body is standing on the ground. Coronal Plane</description></item><item><title>Cayley-Hamilton Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3391/</link><pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3391/</guid><description>Definition1 Let $T : V \to V$ be a linear transformation on a finite-dimensional vector space $V$. Let $f(t)$ be the characteristic polynomial of $T$. Then, the following holds: $$ f(T) = T_{0} $$ Here, $T_{0}$ is the zero transformation. In other words, a linear transformation satisfies its own characteristic polynomial. Rewriting this theorem from the perspective of matrices, Corollary Square matrices satisfy their own characteristic equations. $$ f(A) =</description></item><item><title>Structural Medical Imaging and Functional Medical Imaging</title><link>https://freshrimpsushi.github.io/en/posts/3390/</link><pubDate>Fri, 24 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3390/</guid><description>Structural Imaging Photos taken to obtain structural/anatomical information inside the human body are called structural imaging. They are taken to check for lesions such as fractures or tumors. Usually, when people mention taking photos at a hospital, it refers to this. Functional Imaging Photos taken to obtain functional information about the body are called functional imaging. Here, functional does not relate to functions or [functionals] as mentioned in mathematics, but</description></item><item><title>Cyclic Subspaces of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3389/</link><pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3389/</guid><description>Definition1 Let us consider $T : V \to V$ as a linear transformation on the vector space $V$. Suppose $\mathbf{v} \ne \mathbf{0} \in V$. The following subspace $$ W = \span\left( \left\{ \mathbf{v}, T\mathbf{v}, T^{2}\mathbf{v}, \dots \right\} \right) $$ is called the $V$ $T$-cyclic subspace generated by $\mathbf{v}$. Description The $T$-cyclic subspace is trivially a $T$-invariant subspace. Also, it is the smallest $T$-invariant subspace that includes $\mathbf{v}$. Theorem1 Let $T</description></item><item><title>Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3388/</link><pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3388/</guid><description>Definition1 A matrix whose elements are all $1$ is called an $1$ matrix. Explanation The sushi restaurant owner majored in physics, so he preferred to call it an $1$ matrix because &amp;ldquo;work matrix&amp;rdquo; sounded technical. There doesn&amp;rsquo;t seem to be a rigid convention, and absolutely $I$, $E$ or $O$ should be avoided. If notation is necessary, perhaps $\mathbf{1}_{m\times n}$ would be better. Conceptually, it is almost never needed when studying</description></item><item><title>Null Space of Power Maps</title><link>https://freshrimpsushi.github.io/en/posts/3387/</link><pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3387/</guid><description>Theorem1 $n$ Let&amp;rsquo;s say that the linear transformation $T : V \to V$ on the dimension vector space is nilpotent. $$ T^{p} = T_{0} $$ Here, $T_{0}$ is the zero transformation. Let&amp;rsquo;s call $N(T)$ the null space of $T$. Then, the following holds: For all $i \in \mathbb{N}$, it is $N(T^{i}) \subset N(T^{i+1})$. For $1 \le i \le p-1$, there exists a sequence basis $N(T^{i})$ of $\beta_{i}$ such that the</description></item><item><title>Orthogonal Triangular Matrices are Nilpotent</title><link>https://freshrimpsushi.github.io/en/posts/3385/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3385/</guid><description>Theorem1 $n \times n$ Upper triangular matrix $A$ is a nilpotent matrix. Explanation The converse is not true. A simple counterexample is when $A = \begin{bmatrix} 1 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix}$, $$ A^{2} = \begin{bmatrix} 1 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix}\begin{bmatrix} 1 &amp;amp; 1 \\ -1 &amp;amp; -1 \end{bmatrix} = \begin{bmatrix} 0 &amp;amp; 0 \\ 0 &amp;amp; 0 \end{bmatrix} $$ The method of proof is</description></item><item><title>Fully Connected Layer (Linear Layer, Dense Layer)</title><link>https://freshrimpsushi.github.io/en/posts/3384/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3384/</guid><description>Definition Let&amp;rsquo;s refer to $L: \mathbb{R}^{n} \to \mathbb{R}^{m}$ as a layer. Consider $\mathbf{W}$ as the matrix representation of $L$. When $\mathbf{W}$ consists only of components that are not $0$, $L$ is called a fully connected layer. Explanation A fully connected layer is the most fundamental layer in an artificial neural network. In deep learning, most layers are linear; however, the term linear layer often refers to a fully connected layer.</description></item><item><title>Training/Validation/Test Sets in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3382/</link><pubDate>Wed, 08 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3382/</guid><description>Definitions During training, data sets used are: The data set used to optimize the model&amp;rsquo;s parameters is called the training set. The data set used to optimize the model&amp;rsquo;s hyperparameters is called the validation set. After training, the data set used: The data set used to evaluate the model&amp;rsquo;s performance after training is called the test set. The function value of the loss function for the training/validation/test sets is called</description></item><item><title>The Eigenvalues of the Null Matrix are Only Zero</title><link>https://freshrimpsushi.github.io/en/posts/3381/</link><pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3381/</guid><description>Theorem1 English Translation: Let&amp;rsquo;s consider $V$ as a finite-dimensional vector space, and $T : V \to V$ as a nilpotent linear transformation. Then, the eigenvalues of $T$ are exclusively $0$. Japanese Translation: $V$を有限次元のベクトル空間、$T : V \to V$を冪零の線形変換としよう。する</description></item><item><title>Encoder and Decoder in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3380/</link><pubDate>Sat, 04 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3380/</guid><description>Definition Suppose a dataset $X \subset \mathbb{R}^{n}$ is given. In the context of machine learning, an encoder is defined as a function for an appropriate set $Z \subset \mathbb{R}^{m}$ ($m \le n$) such as the following: $$ \begin{align*} f : X &amp;amp;\to Z \\ \mathbf{x} &amp;amp;\mapsto \mathbf{z} = f(\mathbf{x}) \end{align*} $$ The following $g$ is called a decoder: $$ \begin{align*} g : Z &amp;amp;\to X \\ \mathbf{z} &amp;amp;\mapsto \mathbf{x} =</description></item><item><title>Power Series Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3379/</link><pubDate>Thu, 02 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3379/</guid><description>Definition1 A linear transformation $T : V \to V$ on a vector space $V$ is called nilpotent if there exists a positive number $k$ such that $T^{k} = T_{0}$, where $T_{0}$ is the zero transformation. Stephen H. Friedberg, Linear Algebra (4th Edition, 2002), p512&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Spring 2025 Omakase: Your Name</title><link>https://freshrimpsushi.github.io/en/posts/3378/</link><pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3378/</guid><description>Introduction In Korea, the new academic year starts in March, so around this time, you have many new names to learn. Therefore, we have prepared a menu related to names today. Menu Trigonometric Functions Trigonometric functions are some of the most encountered functions when studying STEM fields. However, not many may be well-versed with their names. As I recall, I first learned about them in middle school, but perhaps because</description></item><item><title>Diagonalizability of Linear Transformations, Multiplicity of Eigenvalues, and the Relationship with Eigenspaces</title><link>https://freshrimpsushi.github.io/en/posts/3377/</link><pubDate>Sun, 26 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3377/</guid><description>Theorem1 Let $T : V \to V$ be a linear transformation on a finite-dimensional vector space $V$. Suppose the characteristic polynomial of $T$ splits and $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$ are distinct eigenvalues of $T$. Then, $T$ is diagonalizable if and only if, for all $i$, the multiplicity of $\lambda_{i}$ and the dimension $\dim(E_{\lambda_{i}})$ of the eigenspace are the same. $$ T \text{ is diagobalizable. } \iff \text{multiplicity of } \lambda_{i}</description></item><item><title>How to Change Axis Style in Julia Plots `framestyle`e`</title><link>https://freshrimpsushi.github.io/en/posts/3376/</link><pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3376/</guid><description>Overview1 The framestyle attribute allows changing the style of the plot&amp;rsquo;s axes and border. The possible options are as follows: :box :semi :axes :origin :zerolines :grid :none Code The default setting is :axes. ▷code1◁ The styles for each attribute are as follows. ▷code2◁ https://docs.juliaplots.org/latest/generated/attributes_subplot/&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>The Creation of Unions is Equal to the Sum of Creations</title><link>https://freshrimpsushi.github.io/en/posts/3375/</link><pubDate>Wed, 22 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3375/</guid><description>Theorem1 Let $S_{1}, S_{2}$ be a subset of the vector space $V$. Then, the following holds. $$ \span(S_{1} \cup S_{2}) = \span(S_{1}) + \span(S_{2}) $$ Here, $\span$ means generation, and $+$ means the sum of sets. Proof $\span(S_{1} \cup S_{2}) \subset \span(S_{1}) + \span(S_{2})$ Let $v \in \span(S_{1} \cup S_{2})$. Then, $v$ can be expressed as follows: $$ v = \sum\limits_{i=1}^{n}a_{i}x_{i} + \sum\limits_{j=1}^{m}b_{j}y_{j},\quad x_{i}\in S_{1},\ y_{j} \in S_{2} $$ The</description></item><item><title>The Coordinate Patch Mapping of a Torus in Three-Dimensional Space</title><link>https://freshrimpsushi.github.io/en/posts/3374/</link><pubDate>Mon, 20 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3374/</guid><description>Formula1 The coordinate chart mapping of a torus with a distance from the center to the tube of $R$ and the diameter of the tube of $r$ is as follows. $$ \mathbf{x}(u_{1}, u_{2}) = \left( (R + r\cos u_{2})\cos u_{1}, (R + r\cos u_{2})\sin u_{1}, r\sin u_{1} \right) $$ Where $(u_{1}, u_{2}) \in [0, 2\pi) \times [0, 2\pi)$. Code The Julia code to draw the above figure is as follows.</description></item><item><title>직합의 성질</title><link>https://freshrimpsushi.github.io/en/posts/3373/</link><pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3373/</guid><description>Theorem1 Let $W_{1}, W_{2}, \dots, W_{k}$ be subspaces of a finite-dimensional vector space $V$. The following propositions are equivalent: $V = W_{1} \oplus W_{2} \oplus \cdots \oplus W_{k}$ It is $V = \sum\limits_{i=1}^{k}W_{i}$, and for any vectors $v_{i} \in W_{i}(1 \le i \le k)$, if $v_{1} + \cdots v_{k} = 0$, then for all $i$, $v_{i} = 0$ holds. All $v \in V$ can be uniquely expressed in the form</description></item><item><title>Hyperbolic Functions Composite Formula</title><link>https://freshrimpsushi.github.io/en/posts/3372/</link><pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3372/</guid><description>Formulas The following holds true. $$ c_{1} \cosh x + c_{2} \sinh x = \begin{cases} A\cosh(x + y_{1}) &amp;amp; \text{if } c_{1} \gt c_{2} \\ B e^{x} &amp;amp; \text{if } c_{1} = c_{2} = B \\ C\sinh(x + y_{2}) &amp;amp; \text{if } c_{1} \lt c_{2} \end{cases} $$ $A = \sqrt{c_{2}^{2} - c_{1}^{2}}$ $C = \sqrt{c_{2}^{2} - c_{1}^{2}}$ $y_{1} = \cosh^{-1}\left( \dfrac{c_{1}}{\sqrt{c_{1}^{2} - c_{2}^{2}}} \right) = \sinh^{-1} \left( \dfrac{c_{2}}{\sqrt{c_{1}^{2} - c_{2}^{2}}}</description></item><item><title>Sum of Subspaces in a Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3371/</link><pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3371/</guid><description>Definition1 Let&amp;rsquo;s refer to $W_{1}, W_{2}$ as a subspace of the vector space $V$. The sum of $W_{1}$ and $W_{2}$ is denoted as $W_{1} + W_{2}$ and defined as follows. $$ W_{1} + W_{2} := \left\{ x + y : x\in W_{1}, y \in W_{2} \right\} $$ Generalization2 Let $W_{1}, W_{2}, \dots, W_{k}$ be a subspace of the vector space $V$. The sum of these subspaces is denoted as $W_{1}</description></item><item><title>Sum and Difference Identities for Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/3370/</link><pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3370/</guid><description>Formulas Composition into sine $$ A \cos \theta + B \sin \theta = C\sin(\theta + \phi) $$ Here, $C = \sqrt{A^{2} + B^{2}}$, $\phi = \sin^{-1} \left( \dfrac{A}{\sqrt{A^{2} + B^{2}}} \right) = \cos^{-1} \left( \dfrac{B}{\sqrt{A^{2} + B^{2}}} \right)$ are given. Composition into cosine $$ A \cos \theta + B \sin \theta = C\cos(\theta - \phi) $$ Here, $C = \sqrt{A^{2} + B^{2}}$, $\phi = \sin^{-1} \left( \dfrac{B}{\sqrt{A^{2} + B^{2}}} \right)</description></item><item><title>Transformations on the Quotient Space of Diagonalizable Linear Transformations are also Diagonalizable</title><link>https://freshrimpsushi.github.io/en/posts/3369/</link><pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3369/</guid><description>Theorem1 Let $V$ be a dimension vector space, $T : V \to V$ be a linear transformation, and $W$ be an $T$-invariant subspace. If $T$ is diagonalizable, then $\overline{T} : V/W \to V/W$ is also diagonalizable. In this case, $V/W$ is the quotient space of $V$. Proof If $T$ is diagonalizable, so is $T|_{W}$, there exists a basis $\gamma = \left\{ v_{1}, v_{2}, \dots, v_{k} \right\}$ of $W$ such that</description></item><item><title>Gaussian Curvature with Negative Values on Rotational Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3368/</link><pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3368/</guid><description>Overview1 The document explains rotation surfaces with negative Gaussian curvature. Description The curvature of the rotation surface is $K = - \dfrac{r^{\prime \prime}}{r}$, therefore $r^{\prime \prime} - a^{2}r = 0$. The solution of this differential equation is as follows: $$ r(s) = c_{1} \cosh(as) + c_{2}\sinh(as) $$ This can be written for some appropriate constant $B, b, C, c \in \mathbb{R}$ as follows: $$ r(s) = c_{1} \cosh (as) +</description></item><item><title>Characteristics of the Mapping into the Quotient Space via Linear Transformations between Polynomial Relations</title><link>https://freshrimpsushi.github.io/en/posts/3367/</link><pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3367/</guid><description>Theorem1 Let $V$ be a $n$-dimensional vector space, $T : V \to V$ a linear transformation, $W \le V$ a $T$-invariant subspace, $T|_{W}$ a contraction mapping, and $\overline{T}$ a linear transformation on the quotient space. $$ T|_{W} : W \to W \\ \overline{T} : V/W \to V/W $$ Let $f(t), g(t), h(t)$ be the characteristic polynomial of $T, T|_{W}, \overline{T}$, respectively. Then, the following holds. $$ f(t) = g(t)h(t) $$</description></item><item><title>Various Properties of Convex Functions</title><link>https://freshrimpsushi.github.io/en/posts/3366/</link><pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3366/</guid><description>Theorem1 All convex functions are continuous. If $f$ is an increasing convex function, and $g$ is a convex function, then $f \circ g$ is also a convex function. If $f$ is convex in $(a, b)$, and if $a \lt s \lt t \lt u \lt b$, then $$ \dfrac{f(t) - f(s)}{t-s} \le \dfrac{ f(u) - f(s) }{ u - s } \le \dfrac{ f(u) - f(t) }{ u - t</description></item><item><title>Linear Transformations on the Quotient Space</title><link>https://freshrimpsushi.github.io/en/posts/3365/</link><pubDate>Thu, 02 Feb 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3365/</guid><description>Definition 1 Let $V$ be a vector space and $T : V \to V$ be a linear transformation. Let $W \le V$ be a $T$-invariant subspace. The linear transformation on quotient space $\overline{T}$ is defined as follows: $$ \begin{align*} \overline{T} : V/W &amp;amp;\to V/W \\ v + W &amp;amp;\mapsto T(v) + W \end{align*} $$ Here, $V/W$ is the quotient space. Theorem (a) $\overline{T}$ is well-defined. (b) $\overline{T}$ is indeed a</description></item><item><title>Shannon Entropy in Classical Information Theory</title><link>https://freshrimpsushi.github.io/en/posts/3400/</link><pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3400/</guid><description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description></item><item><title>Mapping to the Quotient Space</title><link>https://freshrimpsushi.github.io/en/posts/3363/</link><pubDate>Sun, 29 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3363/</guid><description>Theorem1 Let $V$ be a vector space, and $W \le V$ a subspace. Define the function $\eta$ as follows. $$ \begin{align*} \eta : V &amp;amp;\to V/W \\ v &amp;amp;\mapsto v + W \end{align*} $$ In this case, $V/W$ is the quotient space of $V$. Then $\eta$ is a linear transformation and its null space is $N(\eta) = W$. If $V$ is finite-dimensional, then $$ \begin{equation} \dim(W) + \dim(V/W) = \dim(V)</description></item><item><title>What is ReLU in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/3362/</link><pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3362/</guid><description>Definition In machine learning, the following function is referred to as a Rectified Linear Unit. $$ f(x) = x^{+} := \max \left\{ 0, x \right\} $$ Description In electrical engineering, this is known as a ramp function. For a description and properties of the function itself, not from the perspective of its use as an activation function in machine learning, refer to the ramp function article. See Also Dirac delta</description></item><item><title>몫공간의 기저와 차원</title><link>https://freshrimpsushi.github.io/en/posts/3361/</link><pubDate>Wed, 25 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3361/</guid><description>Theorem1 Let $V$ be a $n$-dimensional vector space, and let $W \le V$ be a $k (\lt n)$-dimensional subspace. Let $\left\{ u_{1}, \dots, u_{k} \right\}$ be a basis of $W$. And let $\left\{ u_{1}, \dots, u_{k}, u_{k+1}, \dots, u_{n} \right\}$ be a basis of $V$ obtained by extending this basis. Then $\left\{ u_{k+1} + W, \dots, u_{n} + W \right\}$ is a basis of the quotient space $V/W$. $\dim(V) =</description></item><item><title>Ramp Function</title><link>https://freshrimpsushi.github.io/en/posts/3360/</link><pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3360/</guid><description>Definition The following function is called a ramp function. $$ R(x) := \begin{cases} x &amp;amp; x \gt 0 \\ 0 &amp;amp; x \le 0 \end{cases} $$ Various Definitions1 It can be defined in several ways as follows. $$ \begin{align*} R(x) &amp;amp;:= \begin{cases} x &amp;amp; x \gt 0 \\ 0 &amp;amp; x \le 0 \end{cases} \\[1em] &amp;amp;= \max \left\{ 0, x \right\} \\[1em] &amp;amp;= x H(x) \\[1em] &amp;amp;= \dfrac{x + \left|</description></item><item><title>Residual Classes and Quotient Spaces in Linear Algebra</title><link>https://freshrimpsushi.github.io/en/posts/3359/</link><pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3359/</guid><description>Definition1 $V$ is a $F$-vector space and $W \le V$ is a subspace. For $v \in V$, the set $$ \left\{ v \right\} + W := \left\{ v + w : w \in W \right\} $$ is called the coset of $W$ containing $v$ . The left-hand $+$ is the sum of sets. Explanation Often $\left\{ v \right\} + W$ is simply denoted by $v + W$. Let $\left\{ v</description></item><item><title>Free Fall Motion</title><link>https://freshrimpsushi.github.io/en/posts/3358/</link><pubDate>Thu, 19 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3358/</guid><description>Definition When a body moves only under the influence of gravitational acceleration, without any other external force, this is called free fall motion.</description></item><item><title>대각화가능한 선형변환의 불변부분공간으로의 축소사상도 대각화가능하다</title><link>https://freshrimpsushi.github.io/en/posts/3357/</link><pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3357/</guid><description>Theorem1 Let $V$ be a vector space, and let $T : V \to V$ be a diagonalizable linear transformation. Suppose $\left\{ \mathbf{0} \right\} \ne W \le V$ denotes a non-trivial $T$-invariant subspace. Then the restriction map $T|_{W}$ is also diagonalizable. A trivial $T$-invariant subspace refers to the zero vector set $\left\{ \mathbf{0} \right\}$, the entire set $V$, the range $R(T)$, the null space $N(T)$, and the eigenspace $E_{\lambda}$. Proof Since</description></item><item><title>Position, Velocity, and Acceleration</title><link>https://freshrimpsushi.github.io/en/posts/3356/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3356/</guid><description>Position Definition The function representing an object&amp;rsquo;s position is referred to as the position function, or simply position. Description In physics, since we consider the change of position over time (referred to as motion), position is a function of time, as shown by $x = x(t)$. When assuming a one-dimensional space, it is commonly denoted as $x$. In the case of two-dimensional or three-dimensional spaces, it is usually denoted in</description></item><item><title>The Relationship between Invariant Subspaces and Eigenvectors</title><link>https://freshrimpsushi.github.io/en/posts/3355/</link><pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3355/</guid><description>Theorem1 Let $V$ be a $n$-dimension vector space, $T : V \to V$ a linear transformation, and $W \le V$ a $T$-invariant subspace. Let $v_{1}, \dots, v_{k}$ be $T$&amp;rsquo;s eigenvectors corresponding to distinct eigenvalues. If $v_{1} + \cdots + v_{k} \in W$, then for every $i$ we have $v_{i} \in W$. Explanation Since $W$ is a subspace, if $v_{i} \in W$ then $\sum_{i}v_{i} \in W$ holds. However, the converse does</description></item><item><title>Mass Dependent on Position: The Motion of Balls Connected by Chains</title><link>https://freshrimpsushi.github.io/en/posts/3354/</link><pubDate>Wed, 11 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3354/</guid><description>Overview1 Consider the case where a ball with a chain moves in the vertical direction. If the chain is long enough that the ball moves up and down, causing the length of the chain that is floating in the air to continuously change, then the total mass of the object changes according to the position of the ball. In other words, the mass of the object becomes dependent on its</description></item><item><title>Invariant Subspaces of Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3353/</link><pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3353/</guid><description>Overview Let $\beta = v_{1}, \dots, v_{k}$ be the set of eigenvectors of the linear transformation $T : V \to V$. Then, it can be understood that $T$ maps $\span{\beta}$ to $\span{\beta}$. A subspace that maps itself to itself in this manner is defined as an invariant subspace. Definition1 Let $V$ be a vector space, and $T : V \to V$ a linear transformation. A subspace $W$ is called an</description></item><item><title>Reading and Writing GEXF Files in NetworkX</title><link>https://freshrimpsushi.github.io/en/posts/3352/</link><pubDate>Sat, 07 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3352/</guid><description>Explanation1 2 GEXF stands for Graph Exchange XML Format, a language for describing graph structures. Considering the explanation that it started together with the Gephi project, it seems to be designed to be easily handled by Gephi. Code Writing Let&amp;rsquo;s create the following graph with NetworkX. import networkx as nx from itertools import combinations &amp;gt;&amp;gt;&amp;gt; G = nx.Graph() &amp;gt;&amp;gt;&amp;gt; IVE = [&amp;#34;가을&amp;</description></item><item><title>The Union of Linearly Independent Sets from Different Eigenspaces is Linearly Independent</title><link>https://freshrimpsushi.github.io/en/posts/3351/</link><pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3351/</guid><description>Theorem1 Let $V$ be a vector space, $T : V \to V$ a linear transformation, and $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$ and $T$ different eigenvalues of $T$. For each $i = 1, \dots, k$, let $S_{i}$ be a linearly independent subset of the eigenspace $E_{\lambda_{i}}$. Then, ▶Eq1◀ is a linearly independent subset of $V$. Proof Lemma Following the notation of the theorem, let $\lambda_{1}, \lambda_{2}, \dots, \lambda_{k}$</description></item><item><title>Julia's Graph Analysis Package Graphs.jl</title><link>https://freshrimpsushi.github.io/en/posts/3350/</link><pubDate>Tue, 03 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3350/</guid><description>Introduction Graphs.jl is a package for graph (network) analysis, similar to Python&amp;rsquo;s NetworkX. It was created by rebooting the LightGraphs.jl package. The goal of Graphs.jl is to offer a performant platform for network and graph analysis in Julia, following the example of libraries such as NetworkX in Python. The Graphs.jl project is a reboot of the LightGraphs.jl package (archived in October 2021), which remains available on GitHub at sbromberger/LightGraphs.jl. Code</description></item><item><title>Eigen Spaces of Linear Transformations and Geometric Multiplicity</title><link>https://freshrimpsushi.github.io/en/posts/3349/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3349/</guid><description>Definition1 Let&amp;rsquo;s define $V$ and $n$ as dimension vector spaces, $T : V \to V$ as linear transformation. Let&amp;rsquo;s also define $\lambda$ as the eigenvalue of $T$. The set defined as follows, $E_{\lambda}$, is called the eigenspace of $T$ corresponding to the eigenvalue $\lambda$. $$ E_{\lambda} = V_{\lambda} := \left\{ x \in V : Tx = \lambda x \right\} = N(T - \lambda I) $$ In this case, $N$ is</description></item><item><title>Graph (Network) Analysis Package NetworkX in Python</title><link>https://freshrimpsushi.github.io/en/posts/3348/</link><pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3348/</guid><description>Introduction NetworkX is a Python package for analyzing graphs (networks). Code Installation Enter the following in the terminal. #설치 &amp;gt; pip install networkx #버전 업데이트 &amp;gt; pip install --upgrade networkx Importing and Checking Version networkx is abbreviated as nx. &amp;gt;&amp;gt;&amp;gt; import networkx as nx &amp;gt;&amp;gt;&amp;gt; nx.__version__ &amp;#39;2.8.6&amp;#39; Graph Creation Create a null graph with the following code. &amp;gt;&amp;gt;&amp;gt; G = nx.Graph() &amp;gt;&amp;gt;&amp;gt; nx.info(G)</description></item><item><title>Multiplicity of Eigenvalues of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3347/</link><pubDate>Wed, 28 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3347/</guid><description>Definition1 Let $V$ be a finite-dimensional vector space, and let $T : V \to V$ be a linear transformation. Let $f(t)$ be the characteristic polynomial of $T$, and let $\lambda$ be an eigenvalue of $T$. The highest power $k$ of the factor $(t - \lambda)^{k}$ in $f(t)$ is called the (algebraic) multiplicity of $\lambda$. Explanation Simply put, the multiplicity of an eigenvalue refers to how many times $\lambda$ is a</description></item><item><title>Graph (Network) Visualization and Analysis Program Gephi</title><link>https://freshrimpsushi.github.io/en/posts/3346/</link><pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3346/</guid><description>Introduction Gephi is an open-source, free program designed for graph (network) analysis, with a particular strength in visualization. It supports Windows, Mac, and Linux. Basic Usage Let&amp;rsquo;s create a graph using NetworkX and save it as a gexf file. &amp;gt;&amp;gt;&amp;gt; import networkx as nx &amp;gt;&amp;gt;&amp;gt; G = nx.gnm_random_graph(100,200) &amp;gt;&amp;gt;&amp;gt; nx.info(G) &amp;#39;Graph with 100 nodes and 200 edges&amp;#39; &amp;gt;&amp;gt;&amp;gt; nx.write_gexf(G, &amp;#39;graph.gexf&amp;#39;) When you select a gexf file, information about the graph</description></item><item><title>The Characteristic Polynomial of a Diagonalizable Linear Transformation Is Factorizable</title><link>https://freshrimpsushi.github.io/en/posts/3345/</link><pubDate>Sat, 24 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3345/</guid><description>Definition1 The polynomial $P(F)$ being split over $F$ means that there exists a constant $c, a_{1}, \dots, a_{n} \in F$ that satisfies the following. $$ f(t) = c(t - a_{1})(t - a_{2})\cdots(t - a_{n}) $$ If the polynomial $f(t)$ that is decomposed is the characteristic polynomial of some linear transformation $T$ or matrix $A$, we say that $T$(or $A$) is decomposed. Explanation By definition, if the characteristic polynomial of $T:</description></item><item><title>Tsiolkovsky Rocket Equation</title><link>https://freshrimpsushi.github.io/en/posts/3344/</link><pubDate>Thu, 22 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3344/</guid><description>Formulas1 The equation depicting the velocity of a rocket ejecting fuel in one-dimensional space without external forces is known as the Tsiolkovsky rocket equation. $$ v = v_{0} + V \ln \dfrac{m_{0}}{m} $$ Here, $v$ represents the final velocity of the rocket, $v_{0}$ the initial velocity of the rocket, $V$ the relative ejection speed of the fuel to the rocket, $m$ the final mass of the rocket, and $m_{0}$ the</description></item><item><title>Polynomial Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3343/</link><pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3343/</guid><description>Definition1 Polynomial A polynomial with coefficients from a field $F$ is defined for a non-negative integer $n$ as the following form. $$ \begin{equation} f(x) = a_{n}x^{n} + a_{n-1}x^{n-1} + \cdots + a_{1}x + a_{0} \end{equation} $$ Here, each $a_{k} \in F$ is called the coefficient of $x^{k}$. If $a_{n}=a_{n-1}=\cdots=a_{0}=0$, then $f(x)$ is called a zero polynomial. The degree of a polynomial is the largest power of $x$ with a coefficient</description></item><item><title>Equations of Motion for a Variable Mass System</title><link>https://freshrimpsushi.github.io/en/posts/3342/</link><pubDate>Sun, 18 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3342/</guid><description>Overview1 In physics, mass is often treated as a fixed constant in many situations. However, there are many cases where this is not true. For example, a falling raindrop absorbs smaller droplets in the atmosphere, increasing its mass. A rocket accelerates by expelling gases from burning fuel, thereby reducing its mass as the fuel is consumed. Formulas An Object Moving with Increasing Mass Consider a case where an object moves</description></item><item><title>Eigenvectors Corresponding to Distinct Eigenvalues are Linearly Independent</title><link>https://freshrimpsushi.github.io/en/posts/3341/</link><pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3341/</guid><description>Theorem1 Let $V$ be a vector space, $T : V \to V$ be a linear transformation, and $\lambda_{1}, \dots, \lambda_{k}$ be distinct eigenvalues of $T$. If $\mathbf{v}_{1}, \dots, \mathbf{v}_{k}$ are eigenvectors of $T$ corresponding to the eigenvalue $\lambda_{1}, \dots, \lambda_{k}$, then $\left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{k} \right\}$ are linearly independent. Corollary Diagonalization T is diagonalizable if there exist $n$ linearly independent eigenvectors of $T$. If $T : V \to V$ is</description></item><item><title>Diagonalizable Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3335/</link><pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3335/</guid><description>Definition 1 Let $V$ be called a finite-dimensional vector space. Let $T : V \to V$ be called a linear transformation. If there exists an ordered basis $\beta$ for which the matrix representation $\begin{bmatrix} T \end{bmatrix}_{\beta}$ of $T$ becomes a diagonal matrix, $T$ is said to be diagonalizable. For a square matrix $A$, if the $L_{A}$ is diagonalizable, then the matrix $A$ is said to be diagonalizable. Explanation Suppose the</description></item><item><title>Characteristics Polynomial of Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3339/</link><pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3339/</guid><description>Overview The characteristic polynomial of linear transformation is defined. From the theorem below, it can be seen that solving equation $\det(A - \lambda I) = 0$ is equivalent to finding the eigenvalues. Therefore, it is quite natural to name $\det(A - \lambda I)$ the characteristic polynomial. Theorem1 Let&amp;rsquo;s say $F$ is any field, and $A \in M_{n\times n}(F)$. That $\lambda \in F$ is an eigenvalue of $A$ is equivalent to</description></item><item><title>딥러닝에서 인공신경망(ANN), 심층신경망(DNN), 순방향신경망(FNN)의 뜻과 차이점</title><link>https://freshrimpsushi.github.io/en/posts/3446/</link><pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3446/</guid><description>Overview This document summarizes terms used in deep learning, such as artificial neural networks, deep neural networks, and feedforward neural networks. These terms are often used interchangeably without clear definitions and can be confusing for beginners, but essentially, they can be considered the same. The origins and historical contexts of the terms explained below are not based on exhaustive research and are the author&amp;rsquo;s own hypotheses. Artificial Neural Networks and</description></item><item><title>Eigenvalues and Eigenvectors of Finite-Dimensional Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3337/</link><pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3337/</guid><description>Definition1 Let $V$ be a finite-dimensional $F$-vector space. Let $T : V \to V$ be a linear transformation. For $\lambda \in F$, $$ Tx = \lambda x $$ a non-zero vector $x \in V$ satisfying this is called an eigenvector of $T$. The scalar $\lambda \in F$ is called the eigenvalue corresponding to the eigenvector $x$. Explanation Although one might find the term eigenvector replaced by the terms characteristic vector</description></item><item><title>Sets of Differentiable Real-Valued Functions on a Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3340/</link><pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3340/</guid><description>Definition1 Let $M$ be a differentiable manifold. The set of differentiable functions $f : M \to \mathbb{R}$ at point $p \in M$ is denoted as $\mathcal{D}$. $$ \mathcal{D} := \left\{ \text{all real-valued functions on } M \text{ that are differentialable at } p \right\} $$ On $M$, the set of differentiable functions $f : M \to \mathbb{R}$ is denoted as $\mathcal{D}(M)$. $$ \mathcal{D}(M) := \left\{ \text{all real-valued functions of class</description></item><item><title>Differentiable Vector Fields on a Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3338/</link><pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3338/</guid><description>Definition1 Let&amp;rsquo;s call $M$ a differentiable manifold. The set of all differentiable vector fields on $M$ is denoted as $\frak{X}(M)$. $$ \frak{X}(M) := \left\{ \text{all vector fileds of calss } C^{\infty} \text{ on } M \right\} $$ Explanation $\frak{X}(M)$ is a module over the ring $\mathcal{D}(M)$ of $\mathcal{D}(M)$. In other words, for a differentiable function $f \in \mathcal{D}(M)$ and a vector field $X \in \frak{X}(M)$, $fX$ is (pointwise) well defined.</description></item><item><title>Tensors Defined on Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3334/</link><pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3334/</guid><description>Definition1 Let $M$ be a $n$-dimensional differentiable manifold, $\mathcal{D}(M)$ be the set of differentiable functions on $M$, $\mathfrak{X}(M)$ be the set of all vector fields on $M$. $$ \mathcal{D}(M) := \left\{ \text{all real-valued functions of class } C^{\infty} \text{ defined on } M \right\} $$ $$ \frak{X}(M) := \left\{ \text{all vector fileds of calss } C^{\infty} \text{ on } M \right\} $$ A multilinear function $T$ as follows is called</description></item><item><title>Basis Transformation (Coordinate Transformation) of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3333/</link><pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3333/</guid><description>Overview1 Let $V$ to $n$ dimensional vector space, and call it $\mathbf{v} \in V$. Let $\beta, \beta^{\prime}$ be the ordered basis of $V$. Then, the two coordinates $[\mathbf{v}]_{\beta}$ and $[\mathbf{v}]_{\beta^{\prime}}$ of $\mathbf{v}$ are transformed by the coordinate transformation matrix $Q$ as follows. $$ [\mathbf{v}]_{\beta} = Q [\mathbf{v}]_{\beta^{\prime}} $$ Now, suppose a linear transformation $T : V \to V$ is given. Then, for each ordered basis, there exist matrix representations $\begin{bmatrix}</description></item><item><title>The Relationship between Covariant Derivative and Riemann Curvature Tensor</title><link>https://freshrimpsushi.github.io/en/posts/3332/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3332/</guid><description>Theorem1 Let $f : A \subset \mathbb{R}^{2} \to M$ be a parametrized surface. Let $(s, t)$ be the standard coordinates of $\mathbb{R}^{2}$. Let $V = V(s,t)$ be a vector field following $f$. At each point $(s, t)$, the following holds: $$ \dfrac{D }{\partial t} \dfrac{D }{\partial s}V - \dfrac{D }{\partial s} \dfrac{D }{\partial t}V = R(\dfrac{\partial f}{\partial s}, \dfrac{\partial f}{\partial t})V $$ Description Einstein notation is used. Proof Choose a</description></item><item><title>Coordinate Transformation of Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3331/</link><pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3331/</guid><description>Overview1 2 Let $V$ be $n$dimensional vector space, and let us call $\mathbf{v} \in V$ as such. Let $\beta$ be some ordered basis of $V$. Then, $\mathbf{v}$ is expressed as the coordinate vector $[\mathbf{v}]_{\beta}$. Given another ordered basis $\beta ^{\prime}$, $\mathbf{v}$ can also be expressed as the coordinate vector $[\mathbf{v}]_{\beta^{\prime}}$ with respect to it. Coordinate transformation of vectors refers to the equation relating these two coordinate vectors. Build-up For convenience,</description></item><item><title>Convolutional Layer</title><link>https://freshrimpsushi.github.io/en/posts/3386/</link><pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3386/</guid><description>Definition Let $\mathbf{W}$ be $k \times k$ matrix. Define $M^{n\times n} = M^{n\times n}(\mathbb{R})$ to be the set of real matrices of size $n \times n$. A convolutional layer $C_{\mathbf{W}} : M^{nn} \to M^{(n-k+1) \times (n-k+1)}$ is a function defined as follows. For $\mathbf{X} \in M^{n\times n}$ and $\mathbf{Y} = C_{\mathbf{W}}(\mathbf{X})$, $$ \begin{align*} Y_{ij} &amp;amp;= \begin{bmatrix} w_{11} &amp;amp; w_{12} &amp;amp; \cdots &amp;amp; w_{1k} \\ w_{21} &amp;amp; w_{22} &amp;amp; \cdots &amp;amp;</description></item><item><title>Lie Groups</title><link>https://freshrimpsushi.github.io/en/posts/3330/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3330/</guid><description>Definition1 A group $G$ is called a Lie group if it satisfies the following conditions: It has a differentiable structure. The binary operation $\cdot : G \times G \to G$ defined in $G$ is differentiable. The inverse ${}^{-1} : G \to G$ is differentiable. Explanation Simply put, a Lie group is a differentiable group. Examples $(\mathbb{R}, +)$ Euclidean space has a differentiable structure. $f : (x,y) \mapsto x+y \in C^{\infty}$</description></item><item><title>Left Multiplication Transformation (Matrix Transformation)</title><link>https://freshrimpsushi.github.io/en/posts/3329/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3329/</guid><description>Definition1 Regarding the field $F$, let&amp;rsquo;s say $A \in M_{m \times n}(F)$. The following $L_{A}$ is defined as the left-multiplication transformation. $$ \begin{align*} L_{A} : F^{n} &amp;amp;\to F^{m} \\ x &amp;amp;\mapsto Ax \end{align*} $$ Here, $Ax$ is the matrix product of $A$ and $x$. Description This is a more abstract description of matrix transformation using the concept of a field. Theorems Let&amp;rsquo;s say $A \in M_{m \times n}(F)$. Then, $L_{A}$</description></item><item><title>Matrix Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3327/</link><pubDate>Fri, 18 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3327/</guid><description>Definitions For a field $F$, let us define the set of $m \times n$ matrices whose components are elements of $F$ as $M_{m \times n}(F)$. $$ M_{m \times n}(F) := \left\{ \begin{bmatrix} a_{11} &amp;amp; \cdots &amp;amp; a_{1n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} : a_{ij} \in F \right\} $$ Then, with respect to matrix addition and scalar multiplication, $M_{m \times n}(F)$ is a</description></item><item><title>Scalar Curvature of Differential Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3328/</link><pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3328/</guid><description>Definition1 Riesz Representation Theorem Let $\left( H, \left\langle \cdot,\cdot \right\rangle \right)$ be an inner product space. For linear functionals $f \in H^{ \ast }$ and $\mathbf{x} \in H$ on $H$, there exists a unique $\mathbf{w} \in H$ that satisfies $f ( \mathbf{x} ) = \left\langle \mathbf{w} , \mathbf{x} \right\rangle$. Let $M$ be a differentiable manifold. Let $T_{p}M$ be the tangent space at point $p\in M$. Now, for a fixed $X</description></item><item><title>Matrices of Linear Transformations from a Basis of a Subspace to an Extended Basis</title><link>https://freshrimpsushi.github.io/en/posts/3325/</link><pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3325/</guid><description>Theorem Let █eq01█ be a subspace in █eq02█ dimension vector space called █eq03█. Let █eq04█ be the ordered basis of █eq05█. Consider █eq06█ as an extension of the basis of █eq03█ from █</description></item><item><title>Ricci Curvature of Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3326/</link><pubDate>Sat, 12 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3326/</guid><description>Definition1 1 Given a differentiable manifold $M$ and the tangent space $T_{p}M$ at point $p \in M$, let&amp;rsquo;s have the function $f$ as follows. For a given $X, Y \in T_{p}M$, $$ \begin{align*} f : T_{p}M &amp;amp;\to T_{p}M \\ Z &amp;amp;\mapsto R(X,Z)Y \end{align*} $$ where $R$ is the Riemann curvature. Then, the Ricci curvature $\Ric : T_{p}M \times T_{p}M \to \mathbb{R}$ at point $p$ is defined as $$ \Ric (X,Y)</description></item><item><title>Block Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3323/</link><pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3323/</guid><description>Definition Let&amp;rsquo;s say $A$ is a matrix $m \times n$. $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \\ \end{bmatrix} $$ Consider any vertical and horizontal lines that cut the matrix as follows. $$ A = \left[ \begin{array}{cc|ccc|c|} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}</description></item><item><title>If the Sectional Curvature is the Same, the Riemannian Curvature is also the Same.</title><link>https://freshrimpsushi.github.io/en/posts/3324/</link><pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3324/</guid><description>Theorem1 Let $V$ be defined in a vector space of dimension $2$ or higher, and let $\left\langle \cdot, \cdot \right\rangle$ be the inner product defined on $V$. Let $R : V \times V \times V \times V \to V$ and $R^{\prime} : V \times V \times V \times V \to V$ be multilinear functions satisfying the conditions below. $$ R(x,y,z,w) = \left\langle R(x,y)z, w \right\rangle,\quad R^{\prime}(x,y,z,w) = \left\langle R^{\prime}(x,y)z, w</description></item><item><title>Expansion and Contraction of the Basis</title><link>https://freshrimpsushi.github.io/en/posts/3321/</link><pubDate>Sun, 06 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3321/</guid><description>Theorem1 Let $S$ be a finite subset of the finite-dimensional vector space $V$. (a) If $S$ generates $V$ but is not a basis of $V$, then elements of $S$ can be appropriately removed to reduce it to a basis of $V$. (b) If $S$ is linearly independent but not a basis of $V$, then elements can be suitably added to $S$ to extend it to a basis of $V$. Corollary</description></item><item><title>Coordinate Representation of the Riemann Curvature Tensor</title><link>https://freshrimpsushi.github.io/en/posts/3320/</link><pubDate>Fri, 04 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3320/</guid><description>Explanation1 Given a Riemannian manifold $(M, g)$, let&amp;rsquo;s say the coordinate system at $p$ is referred to as $(U, \mathbf{x})$. And let the tangent vector be denoted as follows. $$ \dfrac{\partial }{\partial x_{i}} \overset{\text{denote}}{=} X_{i} $$ Now, consider $R(X_{i}, X_{j})X_{k}$. By the definition of the Riemann curvature tensor $R$, it is also a vector field. Therefore, it can be expressed as follows. $$ R(X_{i}, X_{j})X_{k} = \sum_{l}R_{ijk}^{l}X_{l} $$ The coefficient</description></item><item><title>Implicit Runge-Kutta Methods</title><link>https://freshrimpsushi.github.io/en/posts/3319/</link><pubDate>Wed, 02 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3319/</guid><description>Overview This document introduces the Runge-Kutta method, an Ordinary Differential Equation (ODE) solver. The commonly used fourth-order Runge-Kutta method, RK4, is a type of explicit Runge-Kutta method. This document explains the implicit Runge-Kutta method. Buildup1 $$ y^{\prime} = f(t,y),\quad t \ge t_{0},\quad y(t_{0}) = y_{0} $$ Consider the given ordinary differential equation as above. $y$ is a function of $t$, and $^{\prime}$ represents the derivative with respect to $t$. When</description></item><item><title>Symmetry of the Riemann Curvature Tensor</title><link>https://freshrimpsushi.github.io/en/posts/3318/</link><pubDate>Mon, 31 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3318/</guid><description>Definition1 The Riemann curvature tensor $R$ is defined as follows for $R: \frak{X}(M) \times \frak{X}(M) \times \frak{X}(M) \times \frak{X}(M) \to \mathcal{D}(M)$. $$ R(X, Y, Z, W) := g(R(X, Y)Z, W) = \left\langle R(X, Y)Z, W \right\rangle $$ Here, $\frak{X}(M)$ is the set of all vector fields defined on $M$, $\mathcal{D}(M)$ is the set of differentiable functions defined on $M$, and $g$ is the Riemannian metric. Description Note that notation is</description></item><item><title>Explicit Runge-Kutta Methods</title><link>https://freshrimpsushi.github.io/en/posts/3317/</link><pubDate>Sat, 29 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3317/</guid><description>Overview Introducing the Runge-Kutta method, an Ordinary Differential Equation (ODE) solver. A separate article is published for a detailed explanation of the commonly used 4th Order Runge-Kutta method RK4. Buildup Consider the following ordinary differential equation. $y$ is a function of $t$, and $^{\prime}$ means the derivative with respect to $t$. $$ y^{\prime} = f(t,y),\quad t \ge t_{0},\quad y(t_{0}) = y_{0} $$ Integrating this from $t_{n}$ to $t_{n+1} = t_{n}</description></item><item><title>Bianchi Identity</title><link>https://freshrimpsushi.github.io/en/posts/3316/</link><pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3316/</guid><description>Theorem1 Let&amp;rsquo;s call $R$ the Riemann curvature. Then, the following holds. $$ R(X, Y) Z + R(Y, Z) X + R(Z, X) Y = 0 $$ Proof It is proven by a straightforward, though complex, calculation without any special techniques. By the definition of Riemann curvature, $$ \begin{align*} R(X, Y) Z + R(Y, Z) X + R(Z, X) Y &amp;amp;= \nabla_{Y} \nabla_{X} Z - \nabla_{X} \nabla_{Y} Z + \nabla_{[X,Y]}Z \\</description></item><item><title>Differences in Array Dimensions in Julia, Python (NumPy, PyTorch)</title><link>https://freshrimpsushi.github.io/en/posts/3315/</link><pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3315/</guid><description>Overview When dealing with high-dimensional arrays in Julia and NumPy, PyTorch (hereinafter referred to collectively as Python for simplicity), it is important to pay attention to what each dimension signifies as they differ. This distinction arises because Julia&amp;rsquo;s arrays are column-major, whereas Python&amp;rsquo;s arrays are row-major. Note that Matlab, being column-major like Julia, does not have this discrepancy, so those familiar with Matlab need not be overly cautious, but those</description></item><item><title>Sectional Curvature of Differential Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3322/</link><pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3322/</guid><description>Theorem1 Let $\sigma \subset T_{p}M$ be a 2-dimensional subspace of the tangent space $T_{p}M$. Suppose that $x, y \in \sigma$ are linearly independent. Then, the following $K$ does not depend on the choice of $x, y$. $$ K(x, y) = \dfrac{R(x,y,x,y)}{\left\| x \times y \right\|^{2}} $$ Here, $R$ is the Riemann curvature tensor. Explanation According to the above theorem, if $\sigma$ is given, the value of $K$ is the same</description></item><item><title>Differential Geometry of Curved Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3314/</link><pubDate>Fri, 21 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3314/</guid><description>Definition1 Let&amp;rsquo;s denote $M$ as a Riemannian manifold, and $\frak{X}(M)$ as the set of all vector fields on $M$. $$ \frak{X}(M) = \text{the set of all vector fileds of calss } C^{\infty} \text{ on } M $$ The curvature $R$ of $M$ is a function that maps $X, Y \in \frak{X}(M)$ to $R(X, Y) : \frak{X}(M) \to \frak{X}(M)$. In this context, $R(X, Y)$ is given as follows. $$ \begin{equation} R(X,</description></item><item><title>Paper Review: Physics-Informed Neural Networks</title><link>https://freshrimpsushi.github.io/en/posts/3313/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3313/</guid><description>Overview The notation and numbering of references and formulas follow the conventions of the original paper. Physics-informed neural networks (referred to as PINN) are artificial neural networks designed to numerically solve differential equations, introduced in the 2018 Journal of Computational Physics paper Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. The authors of the paper are M. Raissi, P. Perdikaris,</description></item><item><title>Poincaré Metric</title><link>https://freshrimpsushi.github.io/en/posts/3312/</link><pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3312/</guid><description>1 Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p73-74&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Methods for Symbolic Computation in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3311/</link><pubDate>Sat, 15 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3311/</guid><description>Overview Symbolic operations in Julia can be used through the SymEngine.jl1 package. Code Defining Symbols Symbols can be defined in the following way. julia&amp;gt; using SymEngine julia&amp;gt; x = symbols(:x) x julia&amp;gt; x, y = symbols(&amp;#34;x y&amp;#34;) (x, y) julia&amp;gt; @vars x, y (x, y) julia&amp;gt; x = symbols(:x) x julia&amp;gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x) Vectors and Matrices julia&amp;gt; v = [</description></item><item><title>Exponential Mapping and Normal Neighborhood</title><link>https://freshrimpsushi.github.io/en/posts/3310/</link><pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3310/</guid><description>1 Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p71-73&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Binomial operation's Jacobi Identity</title><link>https://freshrimpsushi.github.io/en/posts/3309/</link><pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3309/</guid><description>Definition Set $S$ and binary operations $\ast : S \times S \to S$, commutative binary operations $+ : S \times S \to S$, are considered. The following form of equation is called the Jacobi identity. $$ a \ast (b \ast c) + c \ast (a \ast b) + b \ast (c \ast a) = 0,\quad a,b,c \in S $$ If the above equation holds, $\ast$ satisfies the Jacobi identity. Description</description></item><item><title>Minimizing Geodesics</title><link>https://freshrimpsushi.github.io/en/posts/3308/</link><pubDate>Sun, 09 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3308/</guid><description>English Translation 1 Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p70-71&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Nilpotent Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3307/</link><pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3307/</guid><description>Definition1 $n \times n$ For a matrix $A$, if there exists a positive integer $k$ that satisfies $A^{k} = O$, then we call $A$ a nilpotent matrix. In this case, $O$ is the zero matrix of $n \times n$. Explanation &amp;ldquo;Nil&amp;rdquo; means &amp;lsquo;zero&amp;rsquo; or &amp;rsquo;none.&amp;rsquo; &amp;ldquo;Potent&amp;rdquo; means &amp;lsquo;powerful,&amp;rsquo; and is the root of the word &amp;ldquo;potential.&amp;rdquo; Therefore, the term &amp;ldquo;nilpotent&amp;rdquo; can be understood as &amp;lsquo;having the potential/power to become $0$.&amp;rsquo;</description></item><item><title>Gauss's Lemma in Riemannian Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3306/</link><pubDate>Wed, 05 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3306/</guid><description>1 Manfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p68-70&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Triangular Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3305/</link><pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3305/</guid><description>Definition1 A matrix $A = [a_{ij}]$ with all elements above the main diagonal being $0$ is called a lower triangular matrix. $$ A \text{ is lower triangluar matrix if } a_{ij} = 0 \text{ whenever } i \lt j $$ A matrix $A = [a_{ij}]$ with all elements below the main diagonal being $0$ is called an upper triangular matrix. $$ A \text{ is upper triangluar matrix if } a_{ij}</description></item><item><title>Parameterized Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3304/</link><pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3304/</guid><description>Definition1 For an open set $U \subset \mathbb{R}^{2}$, let&amp;rsquo;s say the connected set $A\subset \mathbb{R}^{2}$ satisfies the following: $$ U \subset A \subset \overline{U} \quad \text{and} \quad \partial A \text{ is piecewise differentiable} $$ $s : A \to M$ is called a parameterized surface within the manifold $M$. A vector field $V$ along $s$, maps each $q \in A$ to $V(q) \in T_{s(q)}M$ and is a differentiable function in the</description></item><item><title>What is a commutator in field theory?</title><link>https://freshrimpsushi.github.io/en/posts/3303/</link><pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3303/</guid><description>Definition For a ring $(R, +, \cdot)$, the commutator of two elements $a, b \in R$ is defined as follows. $$ [a, b] := a \cdot b - b \cdot a = ab - ba $$ If $[a, b] = 0$, then $a, b$ are said to commute. The anticommutator of $a, b$ is defined as follows. $$ \left\{a, b\right\} = ab + ba $$ Explanation While similar to the</description></item><item><title>Differentiable Curves and Minimization</title><link>https://freshrimpsushi.github.io/en/posts/3302/</link><pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3302/</guid><description>본 문서의 내용이 없어 번역 진행이 불가능합니다. 구체적인 내용을 제공해주세요.</description></item><item><title>What is a Commutator in Group Theory?</title><link>https://freshrimpsushi.github.io/en/posts/3301/</link><pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3301/</guid><description>Definition For two elements $g, h \in G$ of a group $G$, the commutator of $g, h$ is defined as follows. $$ [g,h] := ghg^{-1}h^{-1} $$ Explanation According to the definition, it is equivalent that the commutator of all elements is the identity element $e$ and that $G$ is an Abelian group. The commutator mentioned in group theory and the commutators that appear in quantum mechanics, differential geometry, etc., are</description></item><item><title>Using Tuples for Indexing in Python</title><link>https://freshrimpsushi.github.io/en/posts/3299/</link><pubDate>Wed, 21 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3299/</guid><description>2D Arrays Let&amp;rsquo;s call $A$ a 2D array. Then $A[(y,x)]$ and $A[y,x]$ perform the same function. &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; A = np.arange(16).reshape(4,4) &amp;gt;&amp;gt;&amp;gt; A array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) &amp;gt;&amp;gt;&amp;gt; A[2,3] 11 &amp;gt;&amp;gt;&amp;gt; A[(2,3)] 11 &amp;gt;&amp;gt;&amp;gt; Using a tuple of tuples allows you to reference multiple specific indexes. It&amp;rsquo;s important to note that</description></item><item><title>Homogeneity of Geodesics</title><link>https://freshrimpsushi.github.io/en/posts/3298/</link><pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3298/</guid><description>I&amp;rsquo;m sorry for any confusion, but it seems like the provided prompt does not contain any specific Korean document for translation into English and Japanese. Can you please provide the content you wish to have translated?</description></item><item><title>Geodesic Flow</title><link>https://freshrimpsushi.github.io/en/posts/3296/</link><pubDate>Sat, 17 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3296/</guid><description>죄송하지만, 제공된 한국어 문서의 내용이 보이지 않습니다. 정확한 내용을 제공해 주시면, 해당 내용을 영어와 일본어로 번역하는데 도움을 줄 수 있겠습니다.</description></item><item><title>How to Draw a Histogram in Excel</title><link>https://freshrimpsushi.github.io/en/posts/3297/</link><pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3297/</guid><description>Overview This introduces how to draw a histogram through Excel. Description Click on the [File] tab, then click on [Options] below to bring up the [Excel Options] window. In [Add-Ins], press [Go] at [Analysis ToolPak] and then press [OK]. Now you can confirm that the [Data Analysis] item has been added to the [Data] tab. Press it and select [Histogram]. Choose the range of values and the intervals of the</description></item><item><title>The Equivalence Condition When the Range of a Linear Transformation is Smaller than the Kernel</title><link>https://freshrimpsushi.github.io/en/posts/3295/</link><pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3295/</guid><description>Theorem1 Let $V$ be a vector space, and $T : V \to V$ be a linear transformation. Then, the following holds: $$ T^{2} = T_{0} \iff R(T) \subset N(T) $$ Here, $T_{0}$ is the zero transformation, and $R(T), N(T)$ are the respective range and null space of $T$. Generalization Let $U, V, W$ be a vector space, and $T_{1} : U \to V$, $T_{2} : V \to W$ be linear</description></item><item><title>Geodesics on a Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3294/</link><pubDate>Sun, 11 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3294/</guid><description>Definition1 On the manifold $M$, for a curve $\gamma : I \to M$ at point $t_{0} \in I$, if $\dfrac{D}{dt}\left( \dfrac{d \gamma}{d t} \right) = 0$, then $\gamma$ is a geodesic at $t_{0}$. If for all points $t \in I$, $\gamma$ is a geodesic at $t$, then $\gamma$ is called a geodesic. If $[a,b] \subset I$ and $\gamma : I \to M$ is a geodesic, a contraction mapping $\gamma|_{[a,b]}$ connecting</description></item><item><title>Dual Pair Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3293/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3293/</guid><description>Definition1 Let us call $X$ a vector space. Let $X^{\ast\ast}$ be the dual space of $X$&amp;rsquo;s dual space, $X^{\ast}$. $$ X^{\ast\ast} = (X^{\ast})^{\ast} $$ This is called the bidual space of $X$. Theorem If $X$ is a finite-dimensional vector space, then $X$ and $X^{\ast\ast}$ are isomorphic. $$ X \approx X^{\ast\ast} $$ Explanation Bidual, double dual, and second dual all mean the same thing. The above theorem holds only when $X$</description></item><item><title>Levi-Civita Connection, Riemannian Connection, Coefficients of Connection, Christoffel Symbols</title><link>https://freshrimpsushi.github.io/en/posts/3292/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3292/</guid><description>Theorem1 Let $(M,g)$ be a Riemannian manifold. Then, there uniquely exists an affine connection $\nabla$ on $M$ satisfying the following: $\nabla$ is symmetric. $\nabla$ is compatible with $g$. Such $\nabla$ specifically satisfies the following equation: $$ \begin{align*} g(Z, \nabla_{Y}X) =&amp;amp;\ \dfrac{1}{2}\Big( X g(Y, Z) + Y g(Z, X) - Z g(X, Y) \\ &amp;amp;\ - g([X, Z], Y) - g([Y, Z], X) - g([X, Y], Z) \Big) \tag{1} \end{align*} $$</description></item><item><title>Transpose of Linear Transformations Defined by Dual Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3291/</link><pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3291/</guid><description>Theorem1 Let&amp;rsquo;s denote the ordered bases of two finite-dimensional vector spaces $V, W$ as $\beta, \gamma$, respectively. For any linear transformation $T : V \to W$, the following defined function $U$ is a linear transformation and satisfies $[U]_{\gamma^{\ast}}^{\beta^{\ast}} = ([T]_{\beta}^{\gamma})^{t}$. $$ U : W^{\ast} \to V^{\ast} \quad \text{ by } \quad U(g) = gT \quad \forall g \in W^{\ast} $$ Here, $[T]_{\beta}^{\gamma}$ is the matrix representation of $T$, ${}^{t}$ is</description></item><item><title>Symmetry of Connection</title><link>https://freshrimpsushi.github.io/en/posts/3290/</link><pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3290/</guid><description>Definition1 An affine connection $\nabla$ on a differentiable manifold $M$ is said to be symmetric if it satisfies the following. $$ \nabla_{X}Y - \nabla_{Y} X = \left[ X, Y \right] \quad \forall X,Y \in \mathfrak{X}(M) $$ Here $\mathfrak{X}(M)$ is the set of vector fields on $M$, and $[ \cdot, \cdot]$ is the Lie bracket. Explanation Let&amp;rsquo;s take Euclidean space as an example. Consider a coordinate system $(U, \mathbf{x})$ of $\mathbb{R}^{n}$.</description></item><item><title>Linear Transformation Spaces and Their Matrix Representation Spaces are Isomorphic</title><link>https://freshrimpsushi.github.io/en/posts/3289/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3289/</guid><description>Theorem1 Let&amp;rsquo;s assume that two vector spaces $V, W$ have dimensions $n, m$, respectively. And let $\beta, \gamma$ be the ordered bases for each. Then the function defined as follows $\Phi$ is an isomorphism. $$ \Phi : L(V, W) \to M_{m\times n}(\mathbb{R}) \quad \text{ by } \quad \Phi (T) = [T]_{\beta}^{\gamma} $$ $[T]_{\beta}^{\gamma}$ is the matrix representation of $T$. Corollary A necessary and sufficient condition for a linear transformation to</description></item><item><title>Coexistence Compatible Connection</title><link>https://freshrimpsushi.github.io/en/posts/3288/</link><pubDate>Tue, 30 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3288/</guid><description>Definition1 Let&amp;rsquo;s assume that an affine connection $M$ and a Riemannian metric $\nabla$ are given on a differentiable manifold. For all differentiable curves $g$, if any parallel vector fields $c$ along any two $g$ satisfy $c$, then the connection $M$ is said to be compatible with the metric $\nabla$. Explanation The following corollary is sometimes stated as the definition of compatibility. The definition given above is easy to conceptualize because</description></item><item><title>Homomorphism</title><link>https://freshrimpsushi.github.io/en/posts/3287/</link><pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3287/</guid><description>Definition1 For two vector spaces $V, W$, if there exists an invertible linear transformation $T : V \to W$, then $V$ is said to be isomorphic to $W$, and is denoted as follows. $$ V \cong W $$ Furthermore, $T$ is called an isomorphism. Explanation By the equivalence condition of being invertible, saying $T$ is an isomorphism means that $T$ is a bijective function. Therefore, if there exists a bijective</description></item><item><title>Parallel Vector Fields on Differential Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3286/</link><pubDate>Fri, 26 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3286/</guid><description>Definition1 Let us consider a differentiable manifold with given affine connection $M$ and $\nabla$. A vector field $V$ following the curve $c : I \to M$ is said to be parallel if it satisfies the following condition. $$ \dfrac{DV}{dt} = 0,\quad \forall i \in I $$ Theorem Let us consider a differentiable manifold with given affine connection $M$ and $\nabla$. Let $c : T \to M (t\in I)$ be a</description></item><item><title>Inverse of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3285/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3285/</guid><description>Definition1 Let $V, W$ be a vector space, and $T : V \to W$ be a linear transformation. If the linear transformation $U : W \to V$ satisfies the following, then $U$ is called the inverse or inverse transformation of $T$. $$ TU = I_{W} \quad \text{and} \quad UT = I_{V} $$ $TU$ is the composition of $U$ and $T$, $I_{X} : X \to X$ is the identity transformation. If</description></item><item><title>Covariant Derivative of Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/3284/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3284/</guid><description>Theorem1 Let $M$ be a differentiable manifold, and let $\nabla$ be an affine connection on $M$. Then, there exists a unique function $\dfrac{D}{dt} : V \mapsto \dfrac{DV}{dt}$ that maps a vector field $V$ following a differentiable curve $c : I \to M(t\in I)$ to another vector field $\dfrac{D V}{dt}$ following $c$. Such a mapping is referred to as the covariant derivative of $V$ along $c$, and it has the following</description></item><item><title>Linear Transformation Space</title><link>https://freshrimpsushi.github.io/en/posts/3283/</link><pubDate>Sat, 20 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3283/</guid><description>Definition1 The set of all linear transformations from the vector space $V$ to $W$ is denoted as $L(V,W)$. $$ L(V, W) = \mathcal{L}(V, W) := \left\{ T : V \to W\enspace |\enspace T \text{ is linear } \right\} $$ This is also expressed as follows, referred to as the homomorphism space. $$ \operatorname{Hom}(V,W) = L(V, W) = \left\{ T : V \to W \text{ is linear} \right\} $$ Additionally, when</description></item><item><title>Affine Connection</title><link>https://freshrimpsushi.github.io/en/posts/3282/</link><pubDate>Thu, 18 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3282/</guid><description>Buildup Given a vector field $\mathbf{V}$ on a differentiable manifold, we can differentiate functions defined on the manifold using the vector field. Naturally, one might also want to differentiate the vector field itself. However, approaching the differentiation of the vector field $\mathbb{R}^{3}$ in the sense of differential geometry proves to be impossible as follows. First Case Let&amp;rsquo;s consider $S \subset \mathbb{R}^{3}$ as a surface and $c : I \to S$</description></item><item><title>Linear Functional</title><link>https://freshrimpsushi.github.io/en/posts/3281/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3281/</guid><description>Definitions1 Let&amp;rsquo;s call $V$ a vector space. A mapping $f$ from $V$ to $\mathbb{C}$ (or $\mathbb{R}$) is called a functional. $$ f : V \to \mathbb{C} $$ If $f$ is linear, it is called a linear functional. More Detailed Definitions2 Let&amp;rsquo;s call $V$ a vector space over the field $F$. Here, the field $F$ itself becomes a $1$-dimensional vector space over $F$. A linear transformation $f : V \to F$</description></item><item><title>Vector Fields Along Curves on Differential Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3280/</link><pubDate>Sun, 14 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3280/</guid><description>Definition1 Let $M$ be a differentiable manifold. A $c : I\subset \mathbb{R} \to M$ that is a (parameterized) curve is called a differentiable function. A differentiable $V$ that satisfies the following is called a vector field along the curve $c : I \to M$. Being differentiable means that for a differentiable function $f$ on $M$, the function $t \mapsto V(t)f$ is differentiable on $I$. $$ V : I \to T_{c(t)}M</description></item><item><title>Order Basis and Coordinate Vectors</title><link>https://freshrimpsushi.github.io/en/posts/3279/</link><pubDate>Fri, 12 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3279/</guid><description>Definition1 Let&amp;rsquo;s say $V$ is a finite-dimensional vector space. When a specific order is assigned to a basis of $V$, it is called an ordered basis. Let&amp;rsquo;s say $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ is an ordered basis of $V$. Then, due to the uniqueness of basis representation, for $\mathbf{v} \in V$, scalars $a_{i}$ uniquely exist as follows. $$ \mathbf{v} = a_{1}\mathbf{v}_{1} + \dots a_{n}\mathbf{v}_{n} $$ $a_{1},\dots,a_{n}$ is called</description></item><item><title>Isometries and Local Isometries on Riemann Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3278/</link><pubDate>Wed, 10 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3278/</guid><description>Isometry Given a Riemannian manifold $(M, g), (N, h)$, a diffeomorphism $f : M \to N$ is called an isometry if the following holds for $f$: $$ \begin{equation} g(u, v)_{p} = h\left( df_{p}(u), df_{p}(v) \right)_{f(p)},\quad \forall p\in M,\quad u,v\in T_{p}M \end{equation} $$ or $$ \left\langle u, v \right\rangle_{p} = \left\langle df_{p}(u), df_{p}(v) \right\rangle_{f(p)},\quad \forall p\in M,\quad u,v\in T_{p}M $$ Here $df_{p} : T_{p}M \to T_{f(p)}N$ is the derivative of $f$.</description></item><item><title>Linear Transformations Between Finite-Dimensional Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3277/</link><pubDate>Mon, 08 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3277/</guid><description>Theorem1 Let $V, W$ be a vector space. Let $\left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ and $\left\{ \mathbf{w}_{1}, \mathbf{w}_{2}, \dots, \mathbf{w}_{n} \right\}$ be bases of $V, W$, respectively. Then there exists a unique linear transformation $T : V \to W$ that satisfies $T(\mathbf{v}_{i}) = \mathbf{w}_{i}$. Corollary2 Let $V, W$ be a vector space. Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ be a basis of $V$. If $U, T</description></item><item><title>Riemann Metric and Riemann Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3276/</link><pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3276/</guid><description>Definition1 A Riemannian metric $g$ on a $n$-dimensional differentiable manifold $M$ is a function that maps each point $p \in M$ to $g_{p}$. Here, $g_{p}$ is an inner product defined in the tangent space $p$ over $T_{p}M$. $$ \begin{align*} g : M &amp;amp;\to \left\{ \text{all inner products on tangent space } T_{p}M \right\} \\ p &amp;amp;\mapsto g_{p}=\left\langle \cdot, \cdot \right\rangle_{p} \end{align*} $$ $$ \begin{align*} g_{p} : T_{p}M \times T_{p}M &amp;amp;\to</description></item><item><title>Linear Transformation Trace</title><link>https://freshrimpsushi.github.io/en/posts/3275/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3275/</guid><description>Definition Let $V$ be a $n$-dimensional vector space. Let $f : V \to V$ be a linear transformation. Let $B = \left\{ e_{i} \right\}$ be a basis of $V$. Let $n \times n$ matrix $A$ be the matrix representation of $f$ with respect to $B$. $$ A = [f]_{B} $$ Since $f(e_{i}) \in V$, we represent it as $f(e_{i}) = \sum f_{j}(e_{i})e_{j}$. Then, $$ A = \begin{bmatrix} f_{1}(e_{1}) &amp;amp; f_{2}(e_{1})</description></item><item><title>Adding a New Column to a DataFrame in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3273/</link><pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3273/</guid><description>Code Let&amp;rsquo;s say we are given the Cosmic Girls dataframe as follows. WJSN = DataFrame( member = [&amp;#34;다영&amp;#34;,&amp;#34;다원&amp;#34;,&amp;#34;루다&amp;#34;,&amp;#34;소정&amp;#34;,</description></item><item><title>Lie Brackets of Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/3272/</link><pubDate>Fri, 29 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3272/</guid><description>Definition1 On two differentiable vector fields $X, Y$ on a differentiable manifold $M$, $[X, Y]$ is defined as follows, and is called the (Lie-)bracket or Lie algebra. $$ \begin{equation} [X, Y] := XY - YX \end{equation} $$ Explanation Vector field $X, Y$ can be seen as an operator acting on $\mathcal{D}(M)$, and $XY$ although not a vector field, $[X, Y] = XY - YX$ becomes a vector field. $(1)$ satisfying</description></item><item><title>Properties of Full Rank Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3271/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3271/</guid><description>Theorem1 Let&amp;rsquo;s refer to $A$ as matrix $m \times n$. Then, the necessary and sufficient condition for $A$ to have a full rank is for $A^{T}A$ to be an invertible matrix. Proof $(\Longrightarrow)$ Assume that $A$ has a full rank. Since $A^{T}A$ is a square matrix $n \times n$, showing that the linear system $A^{T}A \mathbf{x} = \mathbf{0}$ only has the trivial solution, according to the equivocal condition of being</description></item><item><title>Vector Field on Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3270/</link><pubDate>Mon, 25 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3270/</guid><description>Buildup1 Consider the easy definition of a vector field. In 3-dimensional space, a vector field is a function $X : \mathbb{R}^{3} \to \mathbb{R}^{3}$ that maps a 3-dimensional vector to another 3-dimensional vector. When considering this in the context of manifolds, $X$ maps a point $\mathbb{R}^{3}$ on the differential manifold $p$ to a vector $\mathbb{R}^{3}$ in $\mathbf{v}$, treating this vector $\mathbf{v}$ as an operator to consider as a directional derivative (=</description></item><item><title>Fundamental Spaces of Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3269/</link><pubDate>Sat, 23 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3269/</guid><description>Explanation1 Let&amp;rsquo;s assume that the matrix $A$ is given. Then, we can think of the following 6 spaces for $A$. Row space of $A$, Row space of $A^{T}$ Column space of $A$, Column space of $A^{T}$ Null space of $A$, Null space of $A^{T}$ However, since the row vectors of $A$ are the column vectors of $A^{T}$, and the column vectors of $A$ are the row vectors of $A^{T}$, the</description></item><item><title>Tangent Bundles on Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3268/</link><pubDate>Thu, 21 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3268/</guid><description>Definition1 Let&amp;rsquo;s call a $M$ a $n$-dimensional differentiable manifold. Let&amp;rsquo;s denote the tangent space at point $p \in M$ as $T_{p}M$. The tangent bundle $TM$ of $M$ is defined as follows. $$ \begin{align*} TM &amp;amp;:= \bigsqcup \limits_{p \in M } T_{p}M \\ &amp;amp;= \bigcup_{p \in M} \left\{ p \right\} \times T_{p}M \\ &amp;amp;= \left\{ (p, v) : p \in M, v \in T_{p}M \right\} \end{align*} $$ Here, $\bigsqcup$ is a</description></item><item><title>Lagrange Multiplier Method</title><link>https://freshrimpsushi.github.io/en/posts/3267/</link><pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3267/</guid><description>Definition1 The optimal value of the multivariable function $f(x_{1}, \dots, x_{n})$ Description Let&amp;rsquo;s assume the graph of $y = 2 - x^{2}$ is given as shown in the figure above. Let&amp;rsquo;s denote the distance between the origin and the graph as $d$. $$ d(x,y) = \sqrt{x^{2} + y^{2}} $$ Then, the problem of finding the point that minimizes the distance $d$ is the same as finding the point where the</description></item><item><title>Emulsions are locally embedded.</title><link>https://freshrimpsushi.github.io/en/posts/3266/</link><pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3266/</guid><description>3173) An immersion is locally embedded. Theorem1 Proof Assuming that $\phi$ is an immersion, it follows that $d\phi{p}$ is injective. To demonstrate embedding, it is necessary for $\phi|_{V}$ and $(\phi|_{V})^{-1}$ to be bijections, hence only up to $\mathbb{R}^{m}$ coordinates are considered for $$ \begin{bmatrix} \dfrac{\partial y_{1}}{\partial x_{1}} &amp;amp; \dfrac{\partial y_{1}}{\partial x_{2}} &amp;amp; \dots &amp;amp; \dfrac{\partial y_{1}}{\partial x_{n}} \\[1em] \dfrac{\partial y_{2}}{\partial x_{1}} &amp;amp; \dfrac{\partial y_{2}}{\partial x_{2}} &amp;amp; \dots &amp;amp; \dfrac{\partial y_{2}}{\partial</description></item><item><title>Supersaturated and Undersaturated Systems</title><link>https://freshrimpsushi.github.io/en/posts/3265/</link><pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3265/</guid><description>Definition1 Consider the following linear system for matrix $A$: $$ A \mathbf{x} = \mathbf{b} $$ If $m \gt n$, then there are more constraints than unknowns, and such a linear system is called an overdetermined system. If $m \lt n$, then there are less constraints than unknowns, and such a linear system is called an underdetermined system. Theorem 1 Consider the linear system for matrix $A$ with rank $r$ of</description></item><item><title>Differential Forms of Type k</title><link>https://freshrimpsushi.github.io/en/posts/3264/</link><pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3264/</guid><description>Definition1 Let $\omega = \sum\limits_{I} a_{I} dx_{I}$ be a $k$-form on the $n$-dimensional differential manifold $M$. The exterior differential $d\omega$ of $\omega$ is defined as follows: $$ d\omega := \sum\limits_{I} da_{I} \wedge dx_{I} $$ Here, $\wedge$ is the wedge product. Description Since $da_{I}$ is a $1$-form and $dx_{I}$ is a $k$-form, $d\omega$ is a $(k+1)$-form. Example Let $\omega$ be a $1$-form given in $\mathbb{R}^{3}$ as follows: $$ \omega = xyz</description></item><item><title>머신러닝에서 선형회귀모델의 최소제곱법 학습</title><link>https://freshrimpsushi.github.io/en/posts/3263/</link><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3263/</guid><description>Overview1 We introduce a method using the least squares, a learning method for linear regression models. Description Let the dataset be $X = \left\{ \mathbf{x}_{i} \right\}_{i=1}^{N}$, and the label set be $Y = \left\{ y_{i} \right\}_{i=1}^{N}$. Assume the following linear regression model: $$ \hat{y} = \sum\limits_{j=0}^{n-1} w_{j}x_{j} = \mathbf{w}^{T} \mathbf{x} $$ Here, $\mathbf{x} = \begin{bmatrix} x_{0} &amp;amp; \dots &amp;amp; x_{n-1} \end{bmatrix}^{T}$ and $\mathbf{w} = \begin{bmatrix} w_{0} &amp;amp; \dots &amp;amp; w_{n-1}</description></item><item><title>Pull Back in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3262/</link><pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3262/</guid><description>Overview We define the pullback on a differential manifold. If differential manifolds are complex, one can think of $M = \mathbb{R}^{m}$ and $N = \mathbb{R}^{n}$. Definition1 Given two differential manifolds $M, N$ and a differentiable function $f : M \to N$, we can consider a function $f^{\ast}$ that maps $N$&amp;rsquo;s $k$-forms to $M$&amp;rsquo;s $k$-forms. Let $\omega$ be a $k$-form on the manifold $N$, then a $k$-form $f^{\ast}\omega$ on the manifold</description></item><item><title>Gradient Descent Learning of Linear Regression Models in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3261/</link><pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3261/</guid><description>Overview1 Introducing a method using gradient descent, one of the learning methods of the linear regression model. Description Let&amp;rsquo;s assume that the data set is $X = \left\{ \mathbf{x}_{i} \right\}_{i=1}^{N}$ and the label set is $Y = \left\{ y_{i} \right\}_{i=1}^{N}$. And let&amp;rsquo;s assume the following linear regression model. $$ \hat{y} = \sum\limits_{j=0}^{n} w_{j}x_{j} = \mathbf{w}^{T} \mathbf{x} $$ At this time, $\mathbf{x} = \begin{bmatrix} x_{0} &amp;amp; \dots &amp;amp; x_{n} \end{bmatrix}^{T}$ and</description></item><item><title>Operations on Differential Forms: Sum and Wedge Product</title><link>https://freshrimpsushi.github.io/en/posts/3260/</link><pubDate>Tue, 05 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3260/</guid><description>Definition1 Sum $+$ Suppose $\omega = \sum\limits_{I} a_{I} dx_{I}, \varphi = \sum\limits_{I} b_{I} dx_{I}$ is in $k$ notation. Then, the sum of these two is defined as follows. $$ \omega + \varphi := \sum\limits_{I}\left( a_{I} + b_{I} \right)dx_{I} $$ Wedge Product $\wedge$ Let $\omega = \sum\limits_{I} a_{I}dx_{I}$, $\varphi = \sum\limits_{J} b_{J}dx_{J}$ be in $k$ notation and $s$ notation, respectively. Then, the wedge product of these two is defined as: $$</description></item><item><title>How to Perform Hierarchical Clustering in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3259/</link><pubDate>Sun, 03 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3259/</guid><description>Explanation Use the hclust() function from the Clustering.jl package. hclust(d::AbstractMatrix; [linkage], [uplo], [branchorder]) It takes a distance matrix as input and returns the result of hierarchical clustering. The default method for calculating distances between clusters is single linkage. To plot a dendrogram, use StatsPlots.jl instead of Plots.jl. Code using StatsPlots using Clustering using Distances using Distributions a = rand(Uniform(-1,1), 2, 25) scatt = scatter(a[1,:], a[2,:], label=false) savefig(scatt, &amp;#34;julia_hclust_scatter.png&amp;#34;) D_a =</description></item><item><title>kth Order Differential Forms</title><link>https://freshrimpsushi.github.io/en/posts/3258/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3258/</guid><description>Overview Just as we defined the second-order differential form, we generalize to define the k-th order forms on the differential manifold $M$. If the concept of a differential manifold is challenging, it can be simply thought of as $M = \mathbb{R}^{n}$. Build-up Let&amp;rsquo;s say $M$ is a $n$-dimensional differential manifold. $p \in M$ is a point in $M$, and $T_{p}M$ is the tangent space at point $p$ in $M$. $T_{p}^{\ast}M$</description></item><item><title>How to Draw a Dendrogram in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3257/</link><pubDate>Wed, 29 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3257/</guid><description>Explanation When attempting to draw a dendrogram by using the plot() function after performing hierarchical clustering with hclust() on the given data, the following error occurs. using Clustering using Distances using Plots a = rand(2, 10) D_a = pairwise(Euclidean(), a, a) SL = hclust(D_a, linkage=:single) dendrogram = plot(SL) ERROR: LoadError: Cannot convert Hclust{Float64} to series data for plotting To draw a dendrogram, one should use StatsPlots.jl instead of Plots.jl. using</description></item><item><title>Second-Order Differential Form</title><link>https://freshrimpsushi.github.io/en/posts/3256/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3256/</guid><description>Overview We define the binary operation $\wedge$ and, in the sense that we defined the first-order differential form, we define a second-order form for the differential manifold $M$. If differential manifolds seem difficult, one can think of them as $M = \mathbb{R}^{n}$. Buildup1 Let’s consider the first-order form $\omega$. $$ \begin{align*} \omega : M &amp;amp;\to T^{\ast}M \\ p &amp;amp;\mapsto \omega_{p} \end{align*} $$ This maps a point</description></item><item><title>What is a Dendrogram?</title><link>https://freshrimpsushi.github.io/en/posts/3255/</link><pubDate>Sat, 25 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3255/</guid><description>Definition A dendrogram is a tree-like diagram that shows the hierarchical clustering of given data (left side) as a picture (right side).</description></item><item><title>Cotangent Space and First-Order Differential Forms</title><link>https://freshrimpsushi.github.io/en/posts/3254/</link><pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3254/</guid><description>Overview We define the cotangent space and the differential 1-form. If differential manifolds are challenging, one can think of it as $M = \mathbb{R}^{n}$. We use Einstein notation. Cotangent Space1 Let&amp;rsquo;s consider a $M$ as a $n$-dimensional differential manifold. Then, the tangent space $T_{p}M$ at point $p \in M$ becomes a $n$-dimensional vector space (function space), with the basis being $\left\{ \mathbf{e}_{i} = \left. \frac{\partial }{\partial x_{i}}\right|_{p} \right\}_{i}$. At this</description></item><item><title>Hierarchical Clustering</title><link>https://freshrimpsushi.github.io/en/posts/3253/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3253/</guid><description>Algorithm Input Given data of dimension $p$ and $N$ instances with a distance $d$. Step 1. Consider each point as a single cluster. Combine the two closest clusters into one. Step 2. Again, combine the two closest clusters into one. Step 3. Repeat until only one cluster remains. Output Returns which cluster each data belongs to and the distance between clusters. The tree structure obtained on the right is called</description></item><item><title>Angular Momentum Operator in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/3251/</link><pubDate>Fri, 17 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3251/</guid><description>Build-Up The angular momentum operator is naturally derived from the classical definition of angular momentum. $$ \mathbf{l} = \mathbf{r} \times \mathbf{p} \tag{1} $$ Let’s denote this as $\mathbf{r} = (x, y, z)$ and $\mathbf{p} = (p_{x}, p_{y}, p_{z})$. Then each component of the angular momentum $\mathbf{l} = (l_{x}, l_{y}, l_{z})$ is described as follows. $$ l_{x} = yp_{z} - zp_{y},\quad l_{y} = zp_{x} - xp_{z},\quad l_{z}</description></item><item><title>Convolution Support</title><link>https://freshrimpsushi.github.io/en/posts/3249/</link><pubDate>Mon, 13 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3249/</guid><description>Theorem Given two sets of real numbers $A, B$, we define $A + B$ as follows. $$A + B := \left\{ a + b : \forall a \in A, \forall b \in \supp B \right\}$$ For two functions $f, g$, the following holds. $$\supp f \ast g \subset \supp f + \supp g$$ Here, $\supp$ is the function&amp;rsquo;s support, and $\ast$ is the convolution. Proof1 Assume $x \notin \supp f</description></item><item><title>Radon Transform and Product Integration, Convolution</title><link>https://freshrimpsushi.github.io/en/posts/3245/</link><pubDate>Sun, 05 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3245/</guid><description>Summary1 Let&amp;rsquo;s call $\mathcal{R}$ the Radon Transform. $$ \mathcal{R} f (s, \boldsymbol{\theta}) = \int\limits_{\mathbf{x} \cdot \boldsymbol{\theta} = s} f(\mathbf{x}) d \mathbf{x} $$ Let&amp;rsquo;s say $\mathcal{R}_{\boldsymbol{\theta}}f(s) = \mathcal{R} f (s, \boldsymbol{\theta})$. The following formulas hold. Radon Transform and Product Integration $$ \int\limits_{-\infty}^{\infty} \mathcal{R}_{\boldsymbol{\theta}}f(s)g(s) ds = \int \limits_{\mathbb{R}^{n}} f(\mathbf{x}) g(\mathbf{x} \cdot \boldsymbol{\theta}) d \mathbf{x} $$ Corollary $$ \int\limits_{-\infty}^{\infty} \mathcal{R}_{\boldsymbol{\theta}}f(t - s)g(s) ds = \int \limits_{\mathbb{R}^{n}} f(\mathbf{x}) g(-\mathbf{x} \cdot \boldsymbol{\theta} + t) d</description></item><item><title>Writing Korean in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3243/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3243/</guid><description>Description Using the kotex package allows for the use of Korean. With kotex \documentclass{article} \usepackage[utf8]{inputenc} \usepackage{kotex} \title{TeX에서 한글 쓰는 방법} \author{전기현} \date{2022년 06월 01일} \begin{document} \maketitle TeX</description></item><item><title>Euler Characteristics in Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3242/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3242/</guid><description>Definition Simple Definition Let&amp;rsquo;s assume we are given a shape. Let&amp;rsquo;s call the number of vertices $V$, the number of edges $E$, and the number of faces $F$. The Euler characteristic $\chi$ of this shape is defined as follows. $\chi := V - E + F$ Complex Definition1 For a surface $M$ and its region $\mathscr{R}$, the term $\chi(\mathscr{R}) \in \mathbb{Z}$ that satisfies the Gauss-Bonnet theorem is called the Euler</description></item><item><title>How to Install TeX and Use It in VS Code</title><link>https://freshrimpsushi.github.io/en/posts/3241/</link><pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3241/</guid><description>Installation Download TeX Live from here. Windows users should click the link pointed to in the screenshot above. Follow the installation steps in order. A dog will pop up; just let it be. Then, the installation window shown below will appear. Click &amp;ldquo;Install.&amp;rdquo; Be aware that it might take a long time. It took me 2 hours and 11 seconds. VS Code Install TeX Workshop from extensions. If you modify</description></item><item><title>Principles of Photoacoustic Tomography</title><link>https://freshrimpsushi.github.io/en/posts/3239/</link><pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3239/</guid><description>Photoacoustic Effect The photoacoustic (or optoacoustic) effect is a physical phenomenon discovered1 by Alexander Graham Bell, who is erroneously known2 as the inventor of the telephone in 1880. When a material is exposed to light (electromagnetic waves), it absorbs the light, leading to an increase in temperature and thermal expansion. Once the radiation ceases, the material cools and contracts. This expansion and contraction of the material result in pressure changes,</description></item><item><title>Gauss-Bonnet Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3238/</link><pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3238/</guid><description>Gauss-Bonnet Theorem Let&amp;rsquo;s consider $\mathbf{x} : U \to \mathbb{R}^{3}$ as a simple connected geodesic coordinate chart, and $\boldsymbol{\gamma}(I) \subset \mathbf{x}(U)$, which is $\boldsymbol{\gamma}$, as piecewise regular curves. Also, let&amp;rsquo;s say that $\boldsymbol{\gamma}$ surrounds some region $\mathscr{R}$. Then, the following holds true. $$ \iint_{\mathscr{R}} K dA + \int_{\boldsymbol{\gamma}} \kappa_{g} ds + \sum \alpha_{i} = 2\pi $$ Here, $K$ denotes the Gaussian curvature, $\kappa_{g}$ denotes the geodesic curvature, and $\alpha_{i}$ denotes the</description></item><item><title>What is a Sinogram in Tomography?</title><link>https://freshrimpsushi.github.io/en/posts/3237/</link><pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3237/</guid><description>Definition The heat map of the Radon transform is called a sinogram. Description The name sinogram comes from the fact that, as can be seen in the image below, it appears similar to a sine graph for small phantoms.1 (Above) Phantom (Below) The sinogram of the phantom above Peter Kuchment, The Radon Transform and Medical Imaging (2014), p28&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Simple Connected Region</title><link>https://freshrimpsushi.github.io/en/posts/3236/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3236/</guid><description>Definitions Let $\mathscr{R}$ be a region of the surface $M$. If every closed curve within $\mathscr{R}$ is null-homotopic, then $\mathscr{R}$ is said to be simply connected. Description Easy examples such as $\mathbb{R}^{2}$, disk $\left\{ x^{2} + y^{2} = r^{2} \right\}$, and sphere $\mathbb{S}^{2}$ are immediately thought to be simply connected. However, as shown in the figure below, one can see that the torus $T^{2}$ is not simply connected. Unlike $\gamma$,</description></item><item><title>What is a Phantom in Tomography?</title><link>https://freshrimpsushi.github.io/en/posts/3235/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3235/</guid><description>Definition A hypothetical image used for the numerical simulation of tomography is called a phantom. Description The problem dealt with in tomography is, given a function $f$ and an operator $A$, finding $f$ when $Af$ is given. For example, in the case of CT imaging, $Af$ is the Radon transform $\mathcal{R}f$, which refers to the data obtained by a CT scanner passing radiation through our body. Specifically, the brain CT</description></item><item><title>Homotopy to Null in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3234/</link><pubDate>Sat, 14 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3234/</guid><description>Definition1 Let&amp;rsquo;s say that a closed curve $\gamma$ encloses a region $\mathscr{R}$ on a surface $M$. Suppose $\sigma$ is a closed curve or a loop with period $L$ placed on $\mathscr{R}$. And let $\sigma (0) = x_{0}$. If there exists a closed curve $\sigma_{s}$ on the surface $M$ that satisfies the following conditions for $s \in [0,1]$, then $\sigma$ is said to be null-homotopic. $\sigma_{s}(0) = x_{0}$ $\sigma_{0}(t) = \sigma</description></item><item><title>What is a Heatmap?</title><link>https://freshrimpsushi.github.io/en/posts/3233/</link><pubDate>Tue, 10 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3233/</guid><description>Definition The graph of a function $f : \mathbb{R}^{2} \to \mathbb{R}$, when projected onto the $xy-$ plane and its value represented as color, is called a heatmap. Explanation The image above shows both the graph of $z = -4x e^{-(x^2+y^2)}$ (top) and its heatmap (bottom) in one picture. A heatmap, like this, turns a 3D graph into a 2D format using colors for ease of understanding. It is easier to</description></item><item><title>n-Dimensional Radon Transform</title><link>https://freshrimpsushi.github.io/en/posts/3231/</link><pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3231/</guid><description>Definition1 For $s \in \mathbb{R}^{1}$, $\boldsymbol{\theta} \in S^{n-1}$, the Radon transform $\mathcal{R} : L^{2}(\mathbb{R}^{n}) \to L^{2}(Z_{n})$ is defined as follows. $$ \mathcal{R} f (s, \boldsymbol{\theta}) = \int\limits_{\mathbf{x} \cdot \boldsymbol{\theta} = s} f(\mathbf{x}) d \mathbf{x} $$ Here, $Z_{n} := \mathbb{R}^{1} \times S^{n-1}$ is a unit cylinder in $n+1$ dimensions. Description The geometric meaning of $\mathcal{R} f (s, \boldsymbol{\theta})$ is to integrate over all points that are $s$ away from the origin</description></item><item><title>Differentiable Surfaces and Boundaries of Regions in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3230/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3230/</guid><description>Region1 Consider a subset $\mathscr{R}$ of a surface $M$. If $\mathscr{R}$ is an open set, and for any two points in $\mathscr{R}$ there exists a curve on $\mathscr{R}$ containing both, then $\mathscr{R}$ is called a region of $M$. Boundary For a region $\mathscr{R}$ of a surface $M$, the following set $\partial \mathscr{R}$ is called the boundary of $\mathscr{R}$. $$ \partial \mathscr{R} = \left\{ p \notin \mathscr{R} : \exists \left\{ p_{j}</description></item><item><title>n-Dimensional Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/3229/</link><pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3229/</guid><description>Definition1 Let&amp;rsquo;s say the Cartesian coordinates of point $x \in \mathbb{R}^{n}$ are $x_{1}, \dots, x_{n}$. Then, the relationship with its polar coordinates $r, \varphi_{1}, \dots, \varphi_{n-1}$ is as follows. $$ \begin{align*} x_{n} &amp;amp;= r \cos \varphi_{1} \\ x_{n-1} &amp;amp;= r \sin \varphi_{1} \sin \varphi_{2} \\ x_{n-2} &amp;amp;= r \sin \varphi_{1} \cos \varphi_{2} \\ \vdots&amp;amp; \\ x_{4} &amp;amp;= r \sin \varphi_{1} \sin \varphi_{2} \cdots \sin \varphi_{n-3} \sin \varphi_{n-2} \\ x_{3} &amp;amp;=</description></item><item><title>Gaussian Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3228/</link><pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3228/</guid><description>Definition1 Mapping a point $M$ of a surface to the normal vector at $p$ $$ \nu : M \to S^{2} \text{ with } \nu (p) \text{ normal to } M \text{ at } p $$ If it is continuous at every point, $M$ is called an orientable surface. Description $\nu$ is called the Gauss map. Examples Sphere $S^{2}$ If $\nu (p)$ is called the outward normal vector at $p$, since</description></item><item><title>Implementing MLP in Julia Flux to Approximate Nonlinear Functions</title><link>https://freshrimpsushi.github.io/en/posts/3227/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3227/</guid><description>Start Import the necessary packages and define the nonlinear function we want to approximate. Creating the Training Set From the domain of the function $[-5, 5]$, 1024 random points were selected. These points are of type Float64, but deep learning typically handles the Float32 data type, so it was converted. Of course, the model can also automatically convert and run data types like Float64 or Int64 when used as input.</description></item><item><title>Resolving 'TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.' with Lists in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3225/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3225/</guid><description>Error TypeError: can&amp;#39;t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first. Despite dealing with a list, not a PyTorch tensor or a NumPy array, the above error can occur. If you follow the instruction and use the .cpu() or .numpy() methods, you will encounter the following error. AttributeError: &amp;#39;list&amp;#39; object has no attribute &amp;#39;cpu&amp;#39; Solution The reason this error occurs is</description></item><item><title>Differentiable Manifolds in Compact Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3224/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3224/</guid><description>Definition1 Let us say $\overline{B}_{r} = \left\{ p \in \mathbb{R}^{3} : \left| p \right| \le r \right\}$. If for a surface $M \subset \mathbb{R}^{3}$, there exists $r$ that satisfies the following, then $M$ is said to be bounded. $$ M \subset \overline{B}_{r} $$ If every sequence of points $\left\{ p_{n} \right\}$ on $M$ satisfies the following equation, in other words, converges to point $p$ on $M$, then $M$ is said</description></item><item><title>How to Directly Define Multidimensional Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3223/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3223/</guid><description>Explanation 1D arrays (vectors) are defined as follows. julia&amp;gt; A = [1; 2; 3] 3-element Vector{Int64}: 1 2 3 Here, ; signifies moving to the next element based on the first dimension. By generalizing this, ;; signifies moving to the next element based on the second dimension. julia&amp;gt; A = [1; 2; 3;; 4; 5; 6] 3×2 Matrix{Int64}: 1 4 2 5 3 6 Similarly, arrays of three</description></item><item><title>The Rotational Surface with Zero Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3222/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3222/</guid><description>Theorem1 Let $M$ be the unit speed curve of the rotational surface $\boldsymbol{\alpha}$ and let the Gaussian curvature be $K=0$. Then, $M$ satisfies one of the following conditions. It is a part of a cylinder. It is a part of a plane. It is a part of a cone. Moreover, these surfaces are locally isometric. Proof Let the Gaussian curvature of the rotational surface be $K = 0$. Since the</description></item><item><title>Implementing MLP in Julia Flux and Learning with MNIST</title><link>https://freshrimpsushi.github.io/en/posts/3221/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3221/</guid><description>Loading the MNIST Dataset In older examples, you might see code using Flux.Data, but this is no longer supported in Flux. julia&amp;gt; Flux.Data.MNIST.images() ┌ Warning: Flux&amp;#39;s datasets are deprecated, please use the package MLDatasets.jl The official documentation1 advises using the `MLDatasets.jl&amp;rsquo; package. julia&amp;gt; using Flux julia&amp;gt; using MLDatasets julia&amp;gt; imgs = MLDatasets.MNIST.traintensor() 28×28×60000 reinterpret(FixedPointNumbers.N0f8, ::Array{UInt8, 3}) julia&amp;gt; labs = MLDatasets.MNIST.trainlabels()</description></item><item><title>Two Rotational Surfaces with Positive Curvature are Locally Isometric</title><link>https://freshrimpsushi.github.io/en/posts/3220/</link><pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3220/</guid><description>Theorem1 Let $M_{1}$, $M_{2}$ be the unit speed curves $\boldsymbol{\alpha}_{1}$, $\boldsymbol{\alpha}_{2}$ of the surfaces of revolution respectively. If $M_{1}$, $M_{2}$ have a constant curvature $a^{2} \gt 0$, then $M_{1}$, $M_{2}$ are locally isometric. Proof Lemma The following two propositions are equivalent: The two surfaces $M$ and $N$ are locally isometric. For all $p \in M$, there exist two coordinate patch mappings $\mathbf{x} : U \to M$, $\mathbf{y} : U \to</description></item><item><title>How to Perform One-Hot Encoding in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/3219/</link><pubDate>Thu, 14 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3219/</guid><description>Overview One-hot encoding is the process of mapping data to standard basis vectors based on its classification. Flux provides functions for this. Code1 onehot() onehot(x, labels, [default]) Returns x .== labels. However, it does not return the exact same result, but returns a type called OneHotVector. For encoding multiple data points, use onehotbatch() below. julia&amp;gt; 3 .== [1,3,4] 3-element BitVector: 0 1 0 julia&amp;gt; Flux.onehot(3, [1,3,4]) 3-element OneHotVector(::UInt32) with eltype</description></item><item><title>Positive Gaussian Curvature Surfaces of Revolution</title><link>https://freshrimpsushi.github.io/en/posts/3218/</link><pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3218/</guid><description>Overview1 The surface of revolution created by the unit speed curve $\boldsymbol{\alpha}(s) = \left( r(s), z(s) \right)$ is called $M$. $$ M = \left\{ \left( r(s)\cos\theta, r(s)\sin\theta, z(s) \right) : 0 \le \theta \le 2\pi, s \in (s_{0}, s_{1}) \right\} $$ The coordinate patch mapping $\mathbf{x}$ of $M$ is as follows. $$ \mathbf{x}(s, \theta) = \left( r(s)\cos\theta, r(s)\sin\theta, z(s) \right) $$ At this time, the Gaussian curvature of this surface</description></item><item><title>List of Available Commands in Julia Package Management Mode</title><link>https://freshrimpsushi.github.io/en/posts/3217/</link><pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3217/</guid><description>Description By typing the right bracket ] in the Julia REPL, you can switch to package management mode. The available commands in package management mode are as follows. Command Function add foo Adds the package foo. free foo Unpins the package version. help, ? Shows these commands. pin foo Pins the version of the package foo. remove foo, rm foo Removes the package foo. test foo Test-runs the package foo.</description></item><item><title>Fundamental Theorem of Curved Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3216/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3216/</guid><description>Theorem1 For an open set $U \subset \mathbb{R}^{2}$, suppose any two points within $U$ are connected by a curve within $U$. Also, let the function $L_{ij}, g_{ij} : U \to \mathbb{R}\ (i,j = 1,2)$ be differentiable and have the following properties: $L_{12} = L_{21}$, $g_{12} = g_{21}$, $g_{11}, g_{22} &amp;gt; 0$, and $g_{11}g_{22} - (g_{12})^{2} &amp;gt; 0$ Assume that $L_{ij}, g_{ij}$ satisfies the Gauss equation and the Codazzi-Mainardi equation. $$</description></item><item><title>How to load a npy file in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3215/</link><pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3215/</guid><description>설명 This document outlines the process of calculating the Radon transform $\mathcal{R}f$ of a phantom $f$ in Python and saving the results as a *.npy file. To load this file in Julia, one can use the PyCall.jl package. using PyCall np = pyimport(&amp;#34;numpy&amp;#34;) The above code is equivalent to executing import numpy as np in Python. This allows one to directly use the code written for numpy in Python</description></item><item><title>Differentiable Geometry: Local Isometries</title><link>https://freshrimpsushi.github.io/en/posts/3214/</link><pubDate>Mon, 04 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3214/</guid><description>Definition1 Let&amp;rsquo;s suppose a function $f : M \to N$ defined between two surfaces is given. If there exists an open set $$ U, V \ \text{ such that }\ p \in U \subset M, V \subset N $$ such that for all points $p \in M$, the contraction mapping $f|_{U} : U \to V$ becomes an isometry, then $M$ and $N$ are said to be locally isometric. Furthermore, such</description></item><item><title>Overlaying Plots on Heatmaps in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3213/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3213/</guid><description>Code Let&amp;rsquo;s say we want to draw a sine curve from $0$ to $2\pi$ on the heatmap of the array $(5,5)$. You might want to write the code like this, but as you can see in the figure, it doesn&amp;rsquo;t output as desired. using Plots A = rand(Bool, 5,5) heatmap(A, color=:greens) x = range(0, 2pi, length=100) y = sin.(x) plot!(x, y, color=:red, width=3) This is because the horizontal and vertical</description></item><item><title>Differential Geometry: Isometric Mappings</title><link>https://freshrimpsushi.github.io/en/posts/3212/</link><pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3212/</guid><description>Definition1 Suppose a function $f : M \to N$ is given between two surfaces $M, N$. Then $f$ is called an isometry if it satisfies the following conditions: $f$ is differentiable. $f$ is bijective. For every curve $\boldsymbol{\gamma}:[c,d] \to M$, the length of $\boldsymbol{\gamma}$ and the length of $f \circ \boldsymbol{\gamma}$ are the same. If there exists an isometry $f$ between $M$ and $N$, then $M$ and $N$ are said</description></item><item><title>Implementing MLP in Julia Flux and Optimizing with Gradient Descent</title><link>https://freshrimpsushi.github.io/en/posts/3211/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3211/</guid><description>MLP Implementation First, let&amp;rsquo;s load the machine learning package in Julia, Flux.jl, and the optimizer update method update!. using Flux using Flux: update! We can use the Dense() function for linear layers. The Chain() function stacks these linear layers, similar to the Sequential() feature in Keras and PyTorch. julia&amp;gt; model = Chain( Dense(10, 5, relu), Dense(5, 5, relu), Dense(5, 2) ) Chain( Dense(10, 5, relu), # 55 parameters Dense(5, 5,</description></item><item><title>Differentiable Functions Between Two Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3210/</link><pubDate>Sun, 27 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3210/</guid><description>Definition1 Let&amp;rsquo;s say we are given a function $f : M \to N$ between two surfaces $M, N$. Suppose $\mathbf{x} : U \to M$ and $\mathbf{y} : V \to N$ are coordinate chart mappings that include the points $p \in M$, $f(p) \in N$, respectively. If $\mathbf{y}^{-1} \circ f \circ \mathbf{x} : U \to V$ is differentiable, then we say $f$ is differentiable at point $p$. Explanation Why don&amp;rsquo;t we</description></item><item><title>Handling Hidden Layers in Julia Flux</title><link>https://freshrimpsushi.github.io/en/posts/3209/</link><pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3209/</guid><description>Linear1 In Flux, a linear layer can be implemented as Dense(). Dense(in, out, σ=identity; bias=true, init=glorot_uniform) Dense(W::AbstractMatrix, [bias, σ] The default value for the activation function is the identity function. Well-known functions like relu, tanh, and sigmoid can be used. julia&amp;gt; Dense(5, 2) Dense(5, 2) # 12 parameters julia&amp;gt; Dense(5, 2, relu) Dense(5, 2, relu) # 12 parameters julia&amp;gt;</description></item><item><title>Gauss's Great Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3208/</link><pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3208/</guid><description>정리[^1] Gaussian curvature $K$ is intrinsic, and the following holds. $$ K = \dfrac{\sum\limits_{l} R_{121}^{l}g_{l2}}{g} $$ Here, $R_{ijk}^{l}$ are the coefficients of the Riemann curvature tensor, and $g$ and $g_{ij}$ are the coefficients of the Riemann metric. Corollary Since $R_{ijk}^{l} = \dfrac{\partial \Gamma_{ik}^{l}}{\partial u^{j}} - \dfrac{\partial \Gamma_{ij}^{l}}{\partial u^{k}} + \sum_{p} \left( \Gamma_{ik}^{p} \Gamma_{pj}^{l} - \Gamma_{ij}^{p}\Gamma_{pk}^{l}\right) \text{ for }$, the following holds. $$ K = \dfrac{1}{g}\left(</description></item><item><title>Performing Operations on Vectors of Different Sizes Component-wise in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3207/</link><pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3207/</guid><description>Description julia&amp;gt; x = [1 2 3] 1×3 Matrix{Int64}: 1 2 3 julia&amp;gt; y = [1 2 3 4] 1×4 Matrix{Int64}: 1 2 3 4 julia&amp;gt; x .+ y ERROR: DimensionMismatch Two vectors of different sizes cannot perform element-wise operations by default. To implement this manually, one would have to use a double for loop, but fortunately, it can be easily calculated by treating one as</description></item><item><title>Riemann Curvature Tensor, Gauss Equation, and Codazzi-Mainardi Equation in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3206/</link><pubDate>Sat, 19 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3206/</guid><description>Definition1 The coefficients of the Riemannian curvature tensor $R_{ijk}^{l}$ are defined as follows. $$ R_{ijk}^{l} = \dfrac{\partial \Gamma_{ik}^{l}}{\partial u^{j}} - \dfrac{\partial \Gamma_{ij}^{l}}{\partial u^{k}} + \sum_{p} \left( \Gamma_{ik}^{p} \Gamma_{pj}^{l} - \Gamma_{ij}^{p}\Gamma_{pk}^{l}\right) \text{ for } 1 \le i,j,k,l \le 2 $$ Here, $\Gamma_{ij}^{k}$ is the Christoffel symbol. Explanation Since Christoffel symbols are intrinsic, the Riemann curvature tensor is also intrinsic. The so-called coefficients that appear in differential geometry do not depend on</description></item><item><title>Handling the Dimensions and Sizes of PyTorch Tensors</title><link>https://freshrimpsushi.github.io/en/posts/3205/</link><pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3205/</guid><description>Definition Let&amp;rsquo;s call $A$ a PyTorch tensor. The following pair $(a_{0}, a_{1}, \dots, a_{n-1})$ is called the size of $A$. $$ \text{A.size() = torch.Size}([a_{0}, a_{1}, \dots, a_{n-1} ]) $$ Let&amp;rsquo;s refer to $\prod \limits_{i=0}^{n-1} a_{i} = a_{0} \times a_{1} \times \cdots a_{n-1}$ as the dimension of $A$. Call $A$ a $n$-dimensional tensor. $a_{i}$ are the sizes of the respective $i$th dimensions, which are integers greater than $1$. Since this is</description></item><item><title>Necessary and Sufficient Conditions for a Vector Field to be Parallel Along a Curve</title><link>https://freshrimpsushi.github.io/en/posts/3204/</link><pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3204/</guid><description>Theorem1 Let $\boldsymbol{\gamma}(t) = \mathbf{x}\left( \gamma^{1}(t), \gamma^{2}(t) \right)$ be a regular curve on $\mathbf{x}$, the coordinate patch. Let $\mathbf{X}(t)$ be a differentiable vector field along the curve $\boldsymbol{\gamma}$. $$ \mathbf{X} = X^{1}\mathbf{x}_{1} + X^{2}\mathbf{x}_{2} $$ Then, the necessary and sufficient condition for $\mathbf{X}(t)$ to be parallel along $\boldsymbol{\gamma}$ is as follows. $$ 0 = \dfrac{d X^{k}}{d t} + \sum_{i,j} \Gamma_{ij}^{k} X^{i}\dfrac{d \gamma^{j}}{d t},\quad k=1,2 $$ Proof The definition of $\mathbf{X}$</description></item><item><title>Methods for Coloring Up to a Certain Value from Curves in Julia / Between Two Curves / Inside a Closed Curve</title><link>https://freshrimpsushi.github.io/en/posts/3203/</link><pubDate>Sun, 13 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3203/</guid><description>Fill up to a Specific Value1 Using attributes fillrange=a, fillalpha=b, fillcolor=:color in plot(), it colors with :color to the value a from the plotted curve with the transparency b. It works the same by writing fill=(a,b,:color). That is, the following two codes are the same. plot(x,y, fillrange=a, fillalpha=b, fillcolor=:color) plot(x,y, fill=(a,b,:color)) It seems to be a bug, but selecting the value of fillrange as $(0,1)$ does not get colored. using</description></item><item><title>Definition and Relationship between the Gaussian Map and Gaussian Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3202/</link><pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3202/</guid><description>Definition1 A function $\nu$ that maps each point $p$ on a surface $M$ to a unit normal is called the Gaussian map. $$ \nu : M \to \mathbb{S}^{2} \quad \text{and} \quad \nu (p) = \mathbf{n}_{p} $$ Description The Gaussian map is also referred to as the normal spherical image. Theorem Let us call the area of any region $\mathscr{R}$ on the surface $A(\mathscr{R})$ as the area of $\mathscr{R}$. Then the</description></item><item><title>Gaussian Curvature and Mean Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3200/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3200/</guid><description>Definition1 Let&amp;rsquo;s consider the principal curvature at a point $p$ on a surface $M$ be denoted as $\kappa_{1}, \kappa_{2}$. Let $L$ be referred to as the Weingarten map. The Gaussian curvature $K$ is defined as follows: $$ K := \kappa_{1} \kappa_{2} = \det L = \det ([{L^{i}}_{j}]) $$ where ${L^{i}}_{j} = \sum \limits_{k} L_{kj}g^{ki}$ applies. Formula The product of the principal curvatures $$ K = \kappa_{1} \kappa_{2} $$ Gaussian curvature</description></item><item><title>How to Pad PyTorch Tensors</title><link>https://freshrimpsushi.github.io/en/posts/3199/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3199/</guid><description>Code 1 torch.nn.functional.pad(input, pad, mode='constant', value=0.0) input: The tensor to pad pad: Where to pad mode: Method of padding value: Value for padding Description pad When using a tensor of dimension $n$ as an input, you can pass up to $2n-$pairs of arguments. $$ ((n-1)_{\text{low}}, (n-1)_{\text{up}}, (n-2)_{\text{low}}, (n-2)_{\text{up}}, \dots, 1_{\text{low}}, 1_{\text{up}}, 0_{\text{low}}, 0_{\text{up}}) $$ $i_{\text{low}}$ indicates how many values to pad before the $i$th dimension&amp;rsquo;s lower index, i.e., before the</description></item><item><title>Properties of Vector Fields Parallel to a Curve</title><link>https://freshrimpsushi.github.io/en/posts/3198/</link><pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3198/</guid><description>Properties Let $\mathbf{X}(t)$ and $\mathbf{Y}(t)$ be vectors parallel to a regular curve $\alpha (t)$ on the surface $M$. Then the angle between $\mathbf{X}$ and $\mathbf{X}(t), \mathbf{Y}(t)$, and the magnitude of $\left\| \mathbf{X}(t) \right\|$ are constants. Description In other words, both the angle and magnitude are conserved. Proof Let $f(t) = \left\langle \mathbf{X}(t), \mathbf{Y}(t) \right\rangle$. Differentiating $f$, by the differentiation of inner products, we get: $$ \dfrac{d f}{d t} = \left\langle</description></item><item><title>How to Remove Axes in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3197/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3197/</guid><description>Code1 If you want to hide the axes on a graph, you can use plt.axis('off') (or False instead of &amp;lsquo;off&amp;rsquo;). This will make both the x and y axes disappear. If you wish to remove only one of the axes, you can apply a method like the example below to the current figure&amp;rsquo;s axis object, plt.gca(). import numpy as np import matplotlib.pyplot as plt plt.subplots(figsize=(15,3)) plt.subplot(1,4,1) plt.plot(x,y) plt.subplot(1,4,2) plt.plot(x,y) plt.gca().axes.xaxis.set_visible(False)</description></item><item><title>Euler's Theorem in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3196/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3196/</guid><description>Theorem1 Let&amp;rsquo;s define the unit tangent vector to the surface $M$ at point $p$ as $\mathbf{Y}$. $$ \mathbf{Y} \in T_{p}M \quad \text{and} \quad \left\| \mathbf{Y} \right\| = 1 $$ Let $\kappa_{1} \ge \kappa_{2}$ represent the principal curvature at $p$. Then, the following equation holds: $$ II(\mathbf{Y}, \mathbf{Y}) = \kappa_{1} \cos^{2} \theta + \kappa_{2} \sin^{2} \theta $$ In this context, $II$ represents the second fundamental form, $\mathbf{X}_{1}$ represents the principal direction</description></item><item><title>Drawing Vertical and Horizontal Lines in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3195/</link><pubDate>Fri, 25 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3195/</guid><description>CODE1 2 import numpy as np import matplotlib.pyplot as plt x = np.linspace(0,2*np.pi,num=1000) y = np.sin(x) plt.plot(x,y) plt.show() axhline(y=0, xmin=0, xmax=1, **kwargs) axvline(x=0, ymin=0, ymax=1, **kwargs) Horizontal lines can be added with plt.axhline(), and vertical lines with plt.axvline(). The min/max values for the range to draw these lines are not real values but should be entered as a ratio between 0 and 1. The major options include: color or c</description></item><item><title>Curvature of a Principal Curve</title><link>https://freshrimpsushi.github.io/en/posts/3194/</link><pubDate>Wed, 23 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3194/</guid><description>Buildup1 To know in which direction and how much a surface $M$ is curved, it is sufficient to know the normal curvatures $\kappa_{n}$ in each direction. In other words, knowing all $\kappa_{n}$ at point $p$ allows us to understand how $M$ is bent. The first step towards this is to think about the maximum and minimum values of $\kappa_{n}$. The following theorem applies to the unit tangent curve $\boldsymbol{\gamma}$: Lemma</description></item><item><title>The Relationship between the Fundamental Form and Coordinate Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3193/</link><pubDate>Mon, 21 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3193/</guid><description>Overview1 Given the coordinate transformation $f : V \to U$, this explains the relationship between the metric $g$ on $U$ and the metric $\overline{g}$ on $V$. Einstein notation is used. Formulas For the metric $g$ of coordinate patch mapping $\mathbf{x} : U \to \mathbb{R}^{3}$ and the metric $\overline{g}$ of $\mathbf{y} = \mathbf{x} \circ f : V \to \mathbb{R}^{3}$, and the tangent vector $\mathbf{X} = X^{i}\mathbf{x}_{i} = \overline{X}^{\alpha} \mathbf{y}_{\alpha}$, the following</description></item><item><title>The Relationship between the Second Normal Form and the Vingarten Map</title><link>https://freshrimpsushi.github.io/en/posts/3192/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3192/</guid><description>Theorem1 Let&amp;rsquo;s call the tangent vector for a point $p$ on a surface $M$ as $\mathbf{X}, \mathbf{Y} \in T_{p}M$. Then the following holds. $$ II(\mathbf{X}, \mathbf{Y}) = \left\langle L(\mathbf{X}), \mathbf{Y} \right\rangle = \left\langle \mathbf{X}, L(\mathbf{Y}) \right\rangle $$ Here, $L$ is the Weingarten map. Description In other words, the Weingarten map $L$ is a self-adjoint linear transformation. Proof Properties of the Weingarten Map If we define ${L^{l}}_{k} = \sum \limits_{i} L_{ik}g^{il}$,</description></item><item><title>Using Machine Learning Datasets in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3191/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3191/</guid><description>Description The MLDatasets.jl1 2 package allows for the use of the following datasets. Datasets with links have their usage explained in their respective documents. Vision CIFAR10 CIFAR100 EMNIST FashionMNIST MNIST Omniglot SVHN2 convert2image Mesh FAUST Miscellaneous BostonHousing Iris Mutagenesis Titanic Text PTBLM SMSSpamCollection UD_English Graphs CiteSeer Cora Graph HeteroGraph KarateClub MovieLens OGBDataset OrganicMaterialsDB PolBlogs PubMed Reddit TUDataset For one-hot encoding this data or training methods, refer to the following. How</description></item><item><title>Bingarten Equation</title><link>https://freshrimpsushi.github.io/en/posts/3190/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3190/</guid><description>Theorem1 On the surface $M$, the following equation holds. $$ \mathbf{n}_{j} = - \sum_{k} {L^{k}}_{j}\mathbf{x}_{k} $$ Here, $\mathbf{x} : U \to M$ is a coordinate chart mapping, $\mathbf{n}$ is the unit normal, and ${L^{k}}_{j} = \sum\limits_{i}L_{ij}g^{ik}$ is. Explanation Consider the Frenet-Serret frame $\left\{ \mathbf{T}, \mathbf{N}, \mathbf{B} \right\}$ of a curve. Since these are three mutually orthogonal vectors, they form a basis of $\mathbb{R}^{3}$. Moreover, the derivative of each is expressed</description></item><item><title>How to Get Column and Row Labels of Data Frame in Python Pandas</title><link>https://freshrimpsushi.github.io/en/posts/3189/</link><pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3189/</guid><description>Code import pandas as pd data = { &amp;#39;나이&amp;#39; : [26,23,22,22,21,21,20,20,20,20,18,17], &amp;#39;키&amp;#39; : [160, 163, 163, 162, 164, 163, 164, 150, 158, 162, 172, 173], &amp;#39;별명&amp;#39; : [&amp;#39;땡모&amp;#3</description></item><item><title>Bingarten Map</title><link>https://freshrimpsushi.github.io/en/posts/3188/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3188/</guid><description>Definition1 Let $M$ be a surface, and $p \in M$ be a point on the surface. The map $L : T_{p}M \to \mathbb{R}^{3}$, defined as follows, is called the Weingarten map. $$ L (\mathbf{X}) = - \mathbf{X}\mathbf{n} $$ Here, $\mathbf{X} \in T_{p}M$ is a tangent vector, $\mathbf{n}$ is a unit normal, and $\mathbf{X}\mathbf{n}$ is the directional derivative of $\mathbf{n}$. Properties $L$ is a linear transformation that is $L : T_{p}M</description></item><item><title>How to Concatenate or Stack Tensors in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3187/</link><pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3187/</guid><description>Concatenate Tensors cat()1 cat(tensors, dim=0) concatenates two or more tensors along a specified dimension. This means that the size of the specified dimension increases when the tensors are concatenated. Therefore, it is natural that the sizes of the other dimensions need to be the same. For example, if there are tensors $(2,2)$ and $(2,3)$, they cannot be concatenated along the 0th dimension but can be concatenated along the 1st dimension.</description></item><item><title>Definition of Normal Sections and Menelaus's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3186/</link><pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3186/</guid><description>Definition1 Let&amp;rsquo;s suppose a curve $\boldsymbol{\gamma}$ is given on a surface $M$. We denote by $\Pi$ the plane generated by the normal $\mathbf{n}(p)$ and $\boldsymbol{\gamma}^{\prime}(p) \in T_{p}M$ at $p \in M$. The normal section at $M \cap \Pi$ in the direction from $p$ to $\boldsymbol{\gamma}^{\prime}$ on $M$ is referred to as $M \cap \Pi$. Theorem2 Let&amp;rsquo;s denote by $\boldsymbol{\gamma}(s)$ the unit-speed curve on the surface $M$, which has the normal</description></item><item><title>How to Set Plot Scale Range in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3185/</link><pubDate>Sat, 05 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3185/</guid><description>Code1 You can fix the scale of the image using plt.clim(). However, plt.imshow() does not print the color bar by default, so you need to add plt.colorbar() to see it. import numpy as np import matplotlib.pyplot as plt A = np.random.rand(4,4) plt.imshow(A) plt.colorbar() plt.show() plt.imshow(A) plt.colorbar() plt.clim(0,1) plt.show() The results are as follows. Unlike the first image, in the second image, you can see that the range of the color</description></item><item><title>Properties of the Second Normal Form</title><link>https://freshrimpsushi.github.io/en/posts/3184/</link><pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3184/</guid><description>Definition The second fundamental form is defined as a bilinear form on the tangent space $T_{p}M$. For two tangent vectors $\mathbf{X}=\sum X^{i}\mathbf{x}_{i}$ and $\mathbf{Y} = \sum Y^{j}\mathbf{x}_{j}$, it is defined as: $$ II ( \mathbf{X}, \mathbf{Y}) = \sum _{i,j} L_{ij} X^{i} Y^{j} $$ where the coefficients $L_{ij}$ are as follows. $$ L_{ij} = \left\langle \mathbf{x}_{ij}, \mathbf{n} \right\rangle $$ Properties1 $II$ is symmetric. If $\mathbf{T}$ is the tangent field of a</description></item><item><title>How to Obtain the Weight Values of a Model in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3183/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3183/</guid><description>Explanation Let&amp;rsquo;s define a model as follows. import torch import torch.nn as nn class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.linear = nn.Linear(3, 3, bias=True) self.conv = nn.Conv2d(3, 5, 2) f = Model() Then, you can access the weights and biases of each layer with the .weight and .bias methods, respectively. Note that the values obtained through .weight (.bias) are not tensors but Parameter objects. So, if you want to get</description></item><item><title>Geodesic Coordinate Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3182/</link><pubDate>Sun, 30 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3182/</guid><description>Definition1 Let&amp;rsquo;s define $U \subset \mathbb{R}^{2}$ as an open set. Define $\mathbf{x} : U \to \mathbb{R}^{3}$ as a coordinate chart that satisfies the following: $$ g_{11} = 1 \quad \text{and} \quad g_{12} = g_{21} = 0 $$ $$ \left[ g_{ij} \right] = \begin{bmatrix} 1 &amp;amp; 0 \\ 0 &amp;amp; g_{22} \end{bmatrix} $$ In this case, $g_{ij}$ is the coefficient of the first fundamental form. Such $\mathbf{x}$ is called a geodesic</description></item><item><title>Solutions When Python npy Files Won't Open</title><link>https://freshrimpsushi.github.io/en/posts/3181/</link><pubDate>Fri, 28 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3181/</guid><description>Error # code1 Solution If you encounter the above error when opening a npy file, you can solve it by adding allow_pickle=True as follows. # code2</description></item><item><title>Directional Derivatives in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3180/</link><pubDate>Wed, 26 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3180/</guid><description>Definition1 Let $\mathbf{X} \in T_{p}M$ be a tangent vector, and let $\alpha (t)$ be a curve on the surface $M$. Then, we have $\alpha : (-\epsilon, \epsilon) \to M$ and it satisfies $\alpha (0) = p$. In other words, $\mathbf{X} = \dfrac{d \alpha}{d t} (0)$. Now, let&amp;rsquo;s say the function $f$ is a differentiable function defined in some neighborhood of point $p \in M$ on the surface $M$. Then, the</description></item><item><title>Writing Multiple for Loops in One Line Using Python</title><link>https://freshrimpsushi.github.io/en/posts/3179/</link><pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3179/</guid><description>Code To repeat for index i up to 2 and j up to 4, you can write the for loop as follows: &amp;gt;&amp;gt;&amp;gt; for i in range(3): ... for j in range(5): ... if j == 4: ... print((i,j)) ... else : ... print((i,j), end=&amp;#34;&amp;#34;) ... (0, 0)(0, 1)(0, 2)(0, 3)(0, 4) (1, 0)(1, 1)(1, 2)(1, 3)(1, 4) (2, 0)(2, 1)(2, 2)(2, 3)(2, 4) Using the product() from the standard</description></item><item><title>If It's the Shortest Curve, It's a Geodesic</title><link>https://freshrimpsushi.github.io/en/posts/3178/</link><pubDate>Sat, 22 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3178/</guid><description>Theorem1 Let&amp;rsquo;s say $\boldsymbol{\gamma}$ is a unit speed curve connecting two points $P = \boldsymbol{\gamma}(a), Q = \boldsymbol{\gamma}(b)$ on surface $M$. If $\boldsymbol{\gamma}$ is the shortest distance curve connecting $P$ and $Q$, then $\boldsymbol{\gamma}$ is a geodesic. Explanation The converse does not hold. In other words, a geodesic is not necessarily the shortest distance curve. Proof Strategy: Prove by contradiction. What needs to be shown is $\kappa_{g} = 0$, so</description></item><item><title>How to Deep Copy Tensors in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3177/</link><pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3177/</guid><description>Description PyTorch tensors, like other objects, can be deep-copied using copy.deepcopy(). &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import copy &amp;gt;&amp;gt;&amp;gt; a = torch.ones(2,2) &amp;gt;&amp;gt;&amp;gt; b = a &amp;gt;&amp;gt;&amp;gt; c = copy.deepcopy(a) &amp;gt;&amp;gt;&amp;gt; a += 1 &amp;gt;&amp;gt;&amp;gt; a tensor([[2., 2.], [2., 2.]]) &amp;gt;&amp;gt;&amp;gt; b tensor([[2., 2.], [2., 2.]]) &amp;gt;&amp;gt;&amp;gt; c tensor([[1., 1.], [1., 1.]]) However, this is only possible for tensors that are explicitly defined by the user. For instance, attempting to deep-copy</description></item><item><title>Uniqueness Theorem of Geodesics</title><link>https://freshrimpsushi.github.io/en/posts/3176/</link><pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3176/</guid><description>Theorem1 Let&amp;rsquo;s say point $p$ is on the surface $M$ and let $\mathbf{X} \in T_{p}M$ be the unit tangent vector at point $p$. Then, there exists a unique geodesic $\boldsymbol{\gamma} : (-\epsilon, \epsilon) \to M$ that satisfies the following initial value condition. $$ \boldsymbol{\gamma} (0) = p \quad \text{and} \quad \boldsymbol{\gamma}^{\prime}(0) = \mathbf{X} $$ Description This theorem states that, at least locally, there exists a straight line of shortest distance</description></item><item><title>Differences among \mathrm, \text, and \operatorname in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3175/</link><pubDate>Sun, 16 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3175/</guid><description>\mathrm, \text code1 $$ \begin{array}{cc} \mathrm{sin}(\mathrm{mathrm}) &amp;amp; \text{sin}(\text{text}) \end{array} \\[1em] \text{basic font: abcdefghijklmnopqrstuvwxyz} $$ When you look at the default setting, there seems to be no difference in rendering between \mathrm and \text. That is because the default font itself is exactly the same as \mathrm. \mathrm is precisely rendered in Roman type, while \text follows the font that has been set. Therefore, changing the font would result in the</description></item><item><title>Definition of Parallel Vector Field along a Curve on Surface</title><link>https://freshrimpsushi.github.io/en/posts/3174/</link><pubDate>Fri, 14 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3174/</guid><description>Vector Field Along a Curve1 Definition Given a surface $M$ and a curve $\alpha : \left[ a, b \right] \to M$, let us consider a function $\mathbf{X}$ that maps each $t \in \left[ a,b \right]$ to a tangent vector at point $\alpha (t)$ on surface $M$. This function $\mathbf{X}$ is called a vector field along curve $\alpha$. $$ \mathbf{X} : \left[ a, b \right] \to \mathbb{R}^{3} \\ \mathbf{X}(t) \in T_{\alpha</description></item><item><title>How to Define Artificial Neural Network Layers with Lists and Loops in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3173/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3173/</guid><description>Explanation If there are many layers to stack or if there is a need to frequently change the structure of the neural network, one might want to automate the definition of the artificial neural network. In such cases, you might think of defining it using the following for loop. class Model(nn.Module): def __init__(self): super(Model, self).__init__() fc_ = [nn.Linear(n,n) for i in range(m)] def forward(self, x): for i in range(m): x</description></item><item><title>Geodesic on a Surface of Revolution</title><link>https://freshrimpsushi.github.io/en/posts/3172/</link><pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3172/</guid><description>Theorem1 Let us assume that $M$ is a surface of revolution generated by the unit-speed curve $\alpha (t) = (r(t), z(t))$. Then (a) All meridians are geodesics. (b) The condition for parallels to be geodesics is that $\mathbf{x}_{t}$ is parallel to the axis of rotation at all points on the parallel. $$ \text{The circle of latitude is a geodesic.} \\ \iff \mathbf{x}_{t} \text{ is parallel to the axis of revolution</description></item><item><title>Emersion and Embedding on a Differential Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3171/</link><pubDate>Sat, 08 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3171/</guid><description>Definition1 Let us consider $M^{m}, N^{m}$ as an $m, n$-dimensional differentiable manifold, and $\phi : M \to N$ as a differentiable function. If the derivative $d\phi_{p}$ is a one-to-one function at every point $p \in M$, then $\phi$ is called an immersion. If $\phi$ is both an immersion and a homeomorphic, then $\phi$ is called an embedding. If the inclusion function $i : M \subset N$ is an embedding, then</description></item><item><title>Rotational Surfaces in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3170/</link><pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3170/</guid><description>Definition1 Let $z$ be the variable on the given axis, and $r&amp;gt;0$ be the distance from the $z-$ axis. Then, one can consider the curve $\alpha$ on the $rz-$ plane as shown in the figure below. As shown in the figure below, the surface obtained by rotating the curve $\alpha$ about the $z-$ axis is called a surface of revolution. The surface of revolution is expressed as follows. $$ \mathbf{x}(t,</description></item><item><title>Embedding in Topology</title><link>https://freshrimpsushi.github.io/en/posts/3169/</link><pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3169/</guid><description>Definition1 Let $X, Y$ be a topological space. If $f : X \to Y$ satisfies the following, $f$ is called an embedding and denoted as $f : X \hookrightarrow Y$. $X$ is a subspace of $Y$. $f : X \to f(X)$ is a homeomorphism. See also Embedding in Functional Analysis Embedding in Differential Manifolds Daehie Park &amp;amp; Seungho Ahn, Topology (4th Edition, 2018), p232&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Definition of a Straight Line (Geodesic) in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3168/</link><pubDate>Sun, 02 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3168/</guid><description>Buildup1 2 Let&amp;rsquo;s assume there is an object moving along a certain curve on the surface $M \subset \mathbb{R}^{3}$. Even if the line looks curved from the perspective of the entire space $\mathbb{R}^{3}$, an object moving on the surface can be thought of as moving straight ahead. Then, such a line can be defined as a straight line (geodesic) on the surface. First, let&amp;rsquo;s consider the properties of a straight</description></item><item><title>Identity Function</title><link>https://freshrimpsushi.github.io/en/posts/3167/</link><pubDate>Fri, 31 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3167/</guid><description>Definition1 Given a set $X$, the following function $I_{X} : X \to X$ is called the identity function. $$ I_{X}(x) = x,\quad \forall x \in X $$ Explanation The following notations are commonly used. $$ I,\quad \text{id},\quad \text{1} $$ Tangent vectors on a differentiable manifold are defined as follows in $\dfrac{d (f\circ \alpha)}{d t}$, where the function to be differentiated $$ f \circ \alpha = f \circ I \circ \alpha</description></item><item><title>Geodesic Curvature is Intrinsic</title><link>https://freshrimpsushi.github.io/en/posts/3166/</link><pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3166/</guid><description>Theorem1 The geodesic curvature $\kappa_{g}$ of a curve on a surface is intrinsic. Description In other words, $\kappa_{g}$ can be calculated solely using the coefficients of the Riemannian metric, without the unit normal $\mathbf{n}$. Of course, it can also be expressed using the extrinsic formula, as $\kappa \mathbf{N} = \mathbf{T}^{\prime} = \alpha^{\prime \prime} = \kappa_{n}\mathbf{n}+ \kappa_{g}\mathbf{S}$, so $$ \begin{align*} \kappa_{g} =&amp;amp;\ \left\langle \mathbf{T}^{\prime}, \mathbf{S} \right\rangle \\ =&amp;amp;\ \left\langle \mathbf{T}^{\prime}, \mathbf{n}</description></item><item><title>The Tangent Space on an n-Dimensional Differentiable Manifold is an n-Dimensional Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3165/</link><pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3165/</guid><description>Overview Let $M$ be a $n$-dimensional differential manifold, and let $T_{p}M$ be the tangent space at point $p\in M$. The tangent space becomes a vector space, specifically, a $n$-dimensional vector space. The following set becomes the basis of the tangent space, which is very useful in the study of differential manifolds. $$ \mathcal{B} = \left\{ \left. \dfrac{\partial }{\partial x_{i}} \right|_{p} : 1 \le i \le n \right\} $$ Theorem 11</description></item><item><title>The Christoffel Symbols are Intrinsic</title><link>https://freshrimpsushi.github.io/en/posts/3164/</link><pubDate>Sat, 25 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3164/</guid><description>Theorem1 The Christoffel symbols $\Gamma_{ij}^{k}$ satisfy the following equation. In other words, they are intrinsic. $$ \Gamma_{ij}^{k} = \dfrac{1}{2} \sum \limits_{l=1}^{2} g^{lk} \left( \dfrac{\partial g_{lj}}{\partial u_{i}} - \dfrac{\partial g_{ij}}{\partial u_{l}} + \dfrac{\partial g_{il}}{\partial u_{j}} \right) $$ Explanation Gauss proved it. The Christoffel symbols depend only on the Riemann metric and are independent of the normal vector. Therefore, by using Christoffel symbols, one can understand the structure of a surface without</description></item><item><title>Taylor's Theorem for Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/3163/</link><pubDate>Thu, 23 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3163/</guid><description>Theorem1 Let $f : \mathbb{R}^{n} \to \mathbb{R}$ be $C^{k}$ function, and call it $\mathbf{a} = (a_{1}, \dots, a_{n}) \in \mathbb{R}^{n}$. Then, there exists $C^{k-2}$ function $h_{ij}$ that satisfies the following. $$ f(\mathbf{x}) = f(\mathbf{a}) + \sum_{i} (x_{i} - a_{i})\dfrac{\partial f}{\partial x_{i}}(\mathbf{a}) + \sum_{i,j}h_{ij}(\mathbf{x})(x_{i} - a_{i}) (x_{j} - a_{j}) $$ Description It generalizes the Taylor theorem to functions of several variables. second-order $$ \begin{align*} f(\mathbf{x}) &amp;amp;= f(\mathbf{a}) + \sum\limits_{i=1}^{n} (x_{i} -</description></item><item><title>Definition of Intrinsic in Differntial Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3162/</link><pubDate>Tue, 21 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3162/</guid><description>Definition1 In differential geometry, a function that depends only on the coefficients of the first fundamental form $g_{ij}$, and not on the unit normal $\mathbf{n}$, is called intrinsic. Explanation2 3 If the coefficients of the Riemann metric $g_{ij}$ are known, then the length of curves on the surface and the area of the surface can be calculated without leaving the surface as follows can be calculated. $$ \text{length of }</description></item><item><title>Silver-Müller Radiation Condition</title><link>https://freshrimpsushi.github.io/en/posts/3161/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3161/</guid><description>Definition1 Let&amp;rsquo;s define $E$ as the electric field. The following condition is called the Silver-Mueller radiation condition. When $x\in \mathbb{R}^{3}, r = \left| x \right|$, $$ \lim \limits_{r \to \infty} r \left[ (\nabla \times E) \times x - r E \right] = 0 $$ Explanation The Silver-Mueller radiation condition is a necessary condition for the scattering problem of electromagnetic waves for electromagnetic waves to satisfy. Explanation The Sommerfeld radiation condition,</description></item><item><title>Gauss's Theorem in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3160/</link><pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3160/</guid><description>정리1 Let&amp;rsquo;s call $\mathbf{x} : U \to \R^{3}$ the coordinate patch. Let $(u_{1}, u_{2})$ be the coordinates of $U$. Let $\mathbf{n}$ be the unit normal, $L_{ij} = \left\langle \mathbf{x}_{ij}, \mathbf{n} \right\rangle$ the coefficients of the second fundamental form, and $\Gamma_{ij}^{k} = \sum \limits_{l=1}^{2} \left\langle \mathbf{x}_{ij}, \mathbf{x}_{l} \right\rangle g^{lk} = \left\langle \mathbf{x}_{ij}, \mathbf{x}_{l} \right\rangle g^{lk}$ the Christoffel symbols. Then, the following are true: (a) Gauss&amp;rsquo;s formulas: $$ \mathbf{x}_{ij} =</description></item><item><title>Paper Review: Neural Ordinary Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/3159/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3159/</guid><description>Overview and Summary &amp;ldquo;Neural Ordinary Differential Equations&amp;rdquo; is a paper published in 2018 by Ricky T. Q. Chen and three others, and it was selected for 2018 NeurIPS Best Papers. It proposes a method to approximate a simple first-order differential equation, which is a non-autonomous system, using neural networks. $$ \dfrac{\mathrm{d}y}{\mathrm{d}t} = f(y, t) $$ Notably, what the neural network approximates (predicts) is not the $y$ but the rate of</description></item><item><title>Christoffel Symbols in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3158/</link><pubDate>Mon, 13 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3158/</guid><description>Buildup Let $\mathbf{x} : U \to \mathbb{R}^{3}$ represent a coordinate mapping. In differential geometry, the characteristics and properties of geometric objects are described through differentiation. Therefore, the derivatives of the coordinate fragments $\mathbf{x}$ appear in various theorems and formulas. For instance, the first-order derivatives $\left\{ \mathbf{x}_{1}, \mathbf{x}_{2} \right\}$ become the basis of the tangent space $T_{p}M$. Hence, any tangent vector $\mathbf{X} \in T_{p}M$ can be expressed as follows: $$ \mathbf{X}</description></item><item><title>Second Fundamental Form in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3156/</link><pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3156/</guid><description>Build-up Let $\mathbf{x} : U \to \mathbb{R}^{3}$ be referred to as a chart. In differential geometry, the characteristics and properties of geometric objects are explained through differentiation. Hence, the derivatives of coordinate charts $\mathbf{x}$ appear in various theorems and formulas. For instance, the first-order derivatives $\left\{ \mathbf{x}_{1}, \mathbf{x}_{2} \right\}$ become the basis of the tangent space $T_{p}M$. Therefore, any tangent vector $\mathbf{X} \in T_{p}M$ can be expressed as follows. $$</description></item><item><title>Robin Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/3155/</link><pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3155/</guid><description>Definition1 Let&amp;rsquo;s assume that a partial differential equation is defined in an open set $\Omega$. The following boundary conditions are called Robin boundary conditions. $$ u + \dfrac{\partial u}{\partial \nu} = 0 \quad \text{on }\partial \Omega $$ Here, $\nu$ represents the outward unit normal vector. Description Example For instance, solving the Poisson&amp;rsquo;s equation with given Robin boundary conditions is to find $u$ that satisfies the following. $$ \left\{ \begin{align*} -\Delta</description></item><item><title>Gauss Curvature and Geodesic Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3154/</link><pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3154/</guid><description>Buildup1 $$ \left\{ T(s), N(s), B(s), \kappa (s), \tau (s) \right\} $$ Recall how we used the Frenet-Serret apparatus when analyzing curves. When studying surfaces, we will consider similar concepts. When $\boldsymbol{\alpha}$ is the unit speed curve, the curvature of the curve was defined as the magnitude of acceleration $\kappa = \left| T^{\prime} \right| = \left| \boldsymbol{\alpha}^{\prime \prime} \right|$. It is natural to think about how curved a surface is</description></item><item><title>Neumann Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/3153/</link><pubDate>Fri, 03 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3153/</guid><description>Definition1 Let&amp;rsquo;s assume a partial differential equation is given, defined on an open set $\Omega$. The following boundary condition is called the Neumann boundary condition. The problem of finding the solution to the partial differential equation with the Neumann boundary condition is referred to as the Neumann problem. $$ \dfrac{\partial u}{\partial \nu} = 0 \quad \text{on } \partial \Omega $$ Here, $\nu$ represents the outward unit normal vector. Description Nonhomogeneous</description></item><item><title>Parametric Curves on a Simple Surface</title><link>https://freshrimpsushi.github.io/en/posts/3152/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3152/</guid><description>Definition1 2 Let $\mathbf{x} : U \to \R^{3}$ be called a simple surface. Let the coordinates of $U$ be called $(u, v)$. For any point $(u_{0}, v_{0})$, the following curve is called the $u-$parameter curve at $v = v_{0}$ of $\mathbf{x}$. $$ u \mapsto \mathbf{x}(u, v_{0}) $$ The following curve is called the $v-$parameter curve at $u = u_{0}$ of $\mathbf{x}$. $$ v \mapsto \mathbf{x}(u_{0}, v) $$ The velocity vectors</description></item><item><title>Boundary Value Problems in Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/3151/</link><pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3151/</guid><description>Definition Given a partial differential equation defined in an open set $\Omega$, let&amp;rsquo;s assume the values of the unknown $u$ are given on the boundary $\partial \Omega$ of $\Omega$. This is called a boundary condition. The partial differential equation together with the boundary condition is referred to as a boundary value problem. Description The abbreviation BVP is commonly used. To solve a boundary value problem means to find a solution</description></item><item><title>Specific Examples of Calculations Using the Riemann Metric</title><link>https://freshrimpsushi.github.io/en/posts/3150/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3150/</guid><description>Notation Let&amp;rsquo;s say we have coordinates for $(u,v)$ as $U$ on a simple surface $\mathbf{x} : U \to \mathbb{R}^{3}$. $$ \mathbf{x}_{1} := \dfrac{\partial \mathbf{x}}{\partial u}\quad \text{and} \quad \mathbf{x}_{2} := \dfrac{\partial \mathbf{x}}{\partial v} $$ Let&amp;rsquo;s represent the coefficients of the Riemannian metric as follows. $$ \begin{align*} g_{ij} =&amp;amp;\ \left\langle \mathbf{x}_{i}, \mathbf{x}_{j} \right\rangle \\ g_{11} =&amp;amp;\ \left\langle \mathbf{x}_{1}, \mathbf{x}_{1} \right\rangle = E \\ g_{12} =&amp;amp;\ g_{21} = \left\langle \mathbf{x}_{1}, \mathbf{x}_{2} \right\rangle =</description></item><item><title>Potential, A General Definition of Potential Energy</title><link>https://freshrimpsushi.github.io/en/posts/3149/</link><pubDate>Thu, 25 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3149/</guid><description>Definition1 Scalar Potential Let&amp;rsquo;s assume that the vector field $\mathbf{V}$ is a conservative field. In other words, let&amp;rsquo;s say $\nabla \times \mathbf{V} = \mathbf{0}$. Then, there exists a scalar field $W$ that satisfies $\mathbf{V} = -\nabla W$, and this is called the scalar potential of $\mathbf{V}$. Vector Potential Assume the vector field $\mathbf{V}$ satisfies $\nabla \cdot \mathbf{V} = 0$. Then, there exists a vector field $\mathbf{A}$ that satisfies $\mathbf{V} =</description></item><item><title>First Basic Forms, Riemannian Metrics</title><link>https://freshrimpsushi.github.io/en/posts/3148/</link><pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3148/</guid><description>Buildup Riemannian metric is a concept that comes from the process of calculating the length of curves on a surface, and the process is as follows. Let&amp;rsquo;s say $\boldsymbol{\alpha}(t)$ is a regular curve moving on a simple surface $\mathbf{x} : U \to \mathbb{R}^{3}$. Let&amp;rsquo;s say $(u_{1}, u_{2})$ are the coordinates in $U$. Then, $\boldsymbol{\alpha}$ can be expressed as follows. $$ \boldsymbol{\alpha}(t) = \mathbf{x}(u_{1}(t), u_{2}(t)) $$ At this point, the length</description></item><item><title>Sign function</title><link>https://freshrimpsushi.github.io/en/posts/3147/</link><pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3147/</guid><description>Definition Sign function $\mathrm{sgn} : \mathbb{R} \to \mathbb{R}$ is defined as follows. $$ \mathrm{sgn}(x) :=\begin{cases} 1 &amp;amp; x&amp;gt;0 \\ 0 &amp;amp; x=0 \\ -1 &amp;amp; x&amp;lt;0 \end{cases} $$ Explanation It is mainly used to simplify the notation of equations or definitions. It is also written as $\mathrm{sign}$. See Also Sign of complex numbers</description></item><item><title>Definition of Surfaces in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3146/</link><pubDate>Fri, 19 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3146/</guid><description>Definition1 If for every point $M \subset \R^{3}$ of $P \in M$, there exists an $C^{k}$ diffeomorphism $\mathbf{x} : U \subset \R^{2} \to M$ such that the image $\mathbf{x}(U)$ contains some $\epsilon-$neighborhood $N_{p}$ of $P$, then $M$ is called a $\R^{3}$ surface. Moreover, for such two diffeomorphisms $\mathbf{x} : U \to \R^{3}$ and $\mathbf{y} : V \to \R^{3}$, $$ \mathbf{y}^{-1} \circ \mathbf{x} : \mathbf{x}^{-1}\left( \mathbf{x}(U) \cap \mathbf{y}(V) \right) \to \mathbf{y}^{-1}\left(</description></item><item><title>How to Read and Write Greek Characters and Their Meaning in Mathematics and Science</title><link>https://freshrimpsushi.github.io/en/posts/3145/</link><pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3145/</guid><description>Alpha $\Alpha, \alpha$ Alpha is read as &amp;ldquo;alpha&amp;rdquo;. The TeX codes are \Alpha, \alpha respectively. It is the first letter of the Greek alphabet, and the phrase &amp;ldquo;alpha and omega&amp;rdquo; means &amp;ldquo;the beginning and the end.&amp;rdquo; Index of an index set $\alpha$ In differential geometry, a curve $\alpha$ Curve used to define tangent vectors on a differential manifold $\alpha$ Beta $\Beta, \beta$ Beta is read as &amp;ldquo;beta&amp;rdquo;. The TeX codes</description></item><item><title>Eigen Decomposition</title><link>https://freshrimpsushi.github.io/en/posts/3144/</link><pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3144/</guid><description>Definition1 Let us assume that $M \subset \mathbb{R}^{3}$ and $\epsilon &amp;gt;0$ are given. Let&amp;rsquo;s call $d$ the Euclidean distance. The set defined as follows is called the $\epsilon -$ neighborhood of point $P \in M$. $$ N_{p} := \left\{ Q \in M : d(P,Q) &amp;lt; \epsilon \right\} $$ Let&amp;rsquo;s say $M \subset \mathbb{R}^{3}$. Given a function $g : M \to \R^{2}$. For all open sets $U \subset \R^{2}$ containing $g(P)$,</description></item><item><title>Points to Note When Slicing in Python</title><link>https://freshrimpsushi.github.io/en/posts/3143/</link><pubDate>Sat, 13 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3143/</guid><description>Explanation When indexing strings, lists, etc. in Python, entering an index that exceeds the last index results in the following error. &amp;gt;&amp;gt;&amp;gt; list = [0, 1, 2, 3] &amp;gt;&amp;gt;&amp;gt; list[4] Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; IndexError: list index out of range &amp;gt;&amp;gt;&amp;gt; string = &amp;#39;abcde&amp;#39; &amp;gt;&amp;gt;&amp;gt; string[5] Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; IndexError: string index out of range</description></item><item><title>Tangent Vectors on Simple Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3142/</link><pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3142/</guid><description>Definition1 Consider a point $p = \mathbf{x}(a,b)$ on a coordinate patch $\mathbf{x} : U \to \mathbb{R}^{3}$. If a vector $\mathbf{X}$ is the velocity vector at $p$ of some curve $\mathbf{x}(U)$ on the curve passing through $p$, then $\mathbf{X}$ is defined as the tangent vector to the simple surface $\mathbf{x}$. In other words, if for any arbitrary $\epsilon &amp;gt; 0$, there exists a suitably short curve $\boldsymbol{\alpha} : (-\epsilon, \epsilon) \to</description></item><item><title>Differentiable Homomorphism</title><link>https://freshrimpsushi.github.io/en/posts/3141/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3141/</guid><description>Definition1 Let&amp;rsquo;s call $M_{1}, M_{2}$ a differential manifold. A function $\varphi : M_{1} \to M_{2}$ is called a diffeomorphism if it satisfies the following conditions: $\varphi$ is differentiable. $\varphi$ is a bijective function. $\varphi ^{-1}$ is differentiable. If for the neighborhoods $U$ and $V$ of points $p \in M_{1}$ and $\varphi(p)$, the contraction mapping $\varphi|_{U} : U \to V$ is a diffeomorphism, then $\varphi$ is called a local diffeomorphism. Theorem</description></item><item><title>Creating Random Permutations and Shuffling Tensor Order in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3140/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3140/</guid><description>torch.randperm()1 torch.randperm(n): Returns a random permutation of integers from 0 to n-1. Of course, non-integer types cannot be used as input. &amp;gt;&amp;gt;&amp;gt; torch.randperm(4) tensor([2, 1, 0, 3]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(8) tensor([4, 0, 1, 3, 2, 5, 6, 7]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(16) tensor([12, 5, 6, 3, 15, 13, 2, 4, 7, 11, 1, 0, 9, 10, 14, 8]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(4.0) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; TypeError: randperm():</description></item><item><title>Inverse Function Theorem in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/3139/</link><pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3139/</guid><description>Theorem1 Let&amp;rsquo;s say a function $\mathbf{f} : E \subset \mathbb{R}^{n} \to \mathbb{R}^{n}$ defined in an open set $E$ is a $C^{1}$-function. For $\mathbf{a} \in E$, let&amp;rsquo;s assume that $\mathbf{f}^{\prime}(\mathbf{a})$ is invertible and $\mathbf{b} = \mathbf{f}(\mathbf{a})$. Then, the following holds. (a) There exists an open set $U, V \subset \mathbb{R}^{n}$ where $\mathbf{a} \in U, \mathbf{b} \in V$, and over $U$, $\mathbf{f}$ is one-to-one and $\mathbf{f}(U) = V$. (b) If $\mathbf{g}$ is</description></item><item><title>Jacobian of Composite Functions</title><link>https://freshrimpsushi.github.io/en/posts/3138/</link><pubDate>Wed, 03 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3138/</guid><description>Theorem Let&amp;rsquo;s assume we have two functions $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$ and $g : \mathbb{R}^{m} \to \mathbb{R}^{k}$. We denote the Jacobian of $f$ as $J(f)$. Then, the following holds. $$ J(g \circ f) = J(g) J(f) $$ Explanation Since the Jacobian is the most generalized derivative, the above theorem is a generalization of the chain rule. Proof By definition of the Jacobian, $$ J(g \circ f) = \begin{bmatrix} \dfrac{\partial</description></item><item><title>Alternating Function</title><link>https://freshrimpsushi.github.io/en/posts/3137/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3137/</guid><description>Definition Let set $X$ be given. A function that satisfies the following is called an alternating function. $$ \phi : \overbrace{X \times X \times \cdots \times X}^{n} \to \mathbb{R} \\ \phi (x_{1}, \dots, x_{i}, x_{i+1}, \dots, x_{n}) = - \phi (x_{1}, \dots, x_{i+1}, x_{i}, \dots, x_{n}) $$ Explanation It is a function whose sign changes when two adjacent variables are swapped. Of course, it can also be shown that this</description></item><item><title>Differentiation of Functions Defined on Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3136/</link><pubDate>Sat, 30 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3136/</guid><description>Theorem1 Let&amp;rsquo;s call $M_{1}^{n}, M_{2}^{m}$ and $m, n$, respectively, $m, n$-dimensional differentiable manifolds. Let&amp;rsquo;s say $\varphi : M_{1} \to M_{2}$ is a differentiable function. And for every point $p \in M_{1}$ and tangent vector $v \in T_{p}M$, choose a differentiable curve $$\alpha : (-\epsilon, \epsilon) \to M_{1} \text{ with } \alpha (0) = p,\ \alpha^{\prime}(0)=v$$ Let&amp;rsquo;s set it as $\beta = \varphi \circ \alpha$. Then, the following mapping $$ d\varphi_{p}</description></item><item><title>How to Find Derivatives in Julia</title><link>https://freshrimpsushi.github.io/en/posts/3135/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3135/</guid><description>Overview1 The package is named Calculus.jl, but it does not support integration. If automatic differentiation, as discussed in machine learning, is needed, refer to the Zygote.jl package. Differentiation of Single Variable Function Derivative function derivative() It calculates the derivative of $f : \R \to \R$. derivative(f) or derivative(f, :x): Returns the derivative $f^{\prime}$. derivative(f, a): Returns the differential coefficient $f^{\prime}(a)$. julia&amp;gt; f(x) = 1 + 2x + 3x^2 f (generic</description></item><item><title>Chain Rule for Multivariable Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/3134/</link><pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3134/</guid><description>Theorem Let&amp;rsquo;s assume that two functions $\mathbf{g} : D \subset \mathbb{R}^{m} \to \mathbb{R}^{k}$, $\mathbf{f} : \mathbf{g}(\mathbb{R}^{k}) \subset \mathbb{R}^{k} \to \mathbb{R}^{n}$ are differentiable. Then, the composition of these two functions $\mathbf{F} = \mathbf{f} \circ \mathbf{g} : \mathbb{R}^{m} \to \mathbb{R}^{n}$ is also differentiable, and the (total) derivative of $\mathbf{F}$ satisfies the following. $$ \mathbf{F}^{\prime}(\mathbf{x}) = \mathbf{f}^{\prime}\left( \mathbf{g}(\mathbf{x}) \right) \mathbf{g}^{\prime}(\mathbf{x}) $$ Explanation This is called the chain rule. If we denote $\mathbf{x} =</description></item><item><title>Tangent Vector on Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3132/</link><pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3132/</guid><description>Buildup1 To define a tangent vector at each point on a differentiable manifold $M$, let&amp;rsquo;s assume a differentiable curve $\alpha : (-\epsilon , \epsilon) \to M$ is given. We would like to define the derivative $\dfrac{d \alpha}{dt}(0)$ at $t=0$ in $\alpha$ as a tangent vector, like in differential geometry, but since the range of $\alpha$ is $M$ (since it&amp;rsquo;s not guaranteed to be a metric space), we cannot speak of</description></item><item><title>Scattering Problem of Sound Waves</title><link>https://freshrimpsushi.github.io/en/posts/3131/</link><pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3131/</guid><description>Explanation1 The canonical problem of scattering theory is, given the incident field $u^{i}$, to find the scattered field $u^{s}$ in the situation where the total field is as per $u = u^{i} + u^{s}$. For acoustic waves, it is assumed that the incident field is given as the following time-harmonic plane wave. $$ u^{i} (x,t) = e^{i(k x\cdot d - \omega t)},\quad x\in \mathbb{R}^{3} $$ Here, $k = \dfrac{\omega}{c_{0}}$ denotes</description></item><item><title>Differentiable Functions from a Differentiable Manifold to a Differentiable Manifold</title><link>https://freshrimpsushi.github.io/en/posts/3130/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3130/</guid><description>Definition1 Given that $M_{1}, M_{2}$ are each a $n, m$-dimensional differentiable manifold, a mapping $\varphi : M_{1} \to M_{2}$ is defined to be differentiable at $p \in M_{1}$ if it satisfies the following conditions: Whenever a coordinate system $\mathbf{y} : V \subset \mathbb{R}^{m} \to M_{2}$ is given in $\varphi(p)$, there exists a coordinate system $\mathbf{x} : U \subset \mathbb{R}^{n} \to M_{1}$ in $p$ such that $\varphi\left( \mathbf{x}(U) \right) \subset \mathbf{y}(V)$</description></item><item><title>Sommerfeld Radiation Condition</title><link>https://freshrimpsushi.github.io/en/posts/3129/</link><pubDate>Sat, 16 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3129/</guid><description>Definition1 Let&amp;rsquo;s refer to $u$ as a time-harmonic wave. The following condition is known as the Sommerfeld radiation condition. $$ \lim \limits_{r \to \infty} r \left( \dfrac{\partial u}{\partial r} - ik u \right) = 0 $$ Explanation The Sommerfeld radiation condition is a criterion that physically feasible solutions of the Helmholtz equation must satisfy. It was proposed by the German physicist Sommerfeld in his 1912 paper Die greensche Funktion der</description></item><item><title>Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3128/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3128/</guid><description>Definition1 In a Euclidean space $\mathbb{R}^{n+1}$, the set of all lines passing through the origin $\mathbf{0}$ is denoted by $\mathbb{P}^{n}$ and referred to as the projective space. $$ \mathbb{P}^{n} := \left\{ \text{all straight lines passing through in } \mathbb{R}^{n+1} \right\} $$ Explanation An easy example of a moduli space. $$ (x_{1}, \dots, x_{n+1}) \sim (\lambda x_{1}, \dots, \lambda x_{n+1}),\quad \lambda \in \mathbb{R}\setminus \left\{ 0 \right\} $$ Since the points on</description></item><item><title>What is Scattering Theory?</title><link>https://freshrimpsushi.github.io/en/posts/3127/</link><pubDate>Tue, 12 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3127/</guid><description>Explanation1 What is Scattering Theory? Scattering theory studies the effects that an inhomogeneous medium has on incoming particles or waves. It explains why the sky is blue and Rutherford&amp;rsquo;s alpha particle scattering experiment, and is also applied in medical imaging technologies such as tomography. Problems Addressed Scattering theory can broadly be divided into quantum scattering theory and classical scattering theory. A fundamental problem in classical scattering theory involves &amp;ldquo;boundaries with</description></item><item><title>Moduli Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3126/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3126/</guid><description>Definition Simple Definition A collection of geometric objects is called a moduli space. Description For example, there is the projective space, which is the set of all lines passing through the origin.</description></item><item><title>What is an Inverse Problem?</title><link>https://freshrimpsushi.github.io/en/posts/3125/</link><pubDate>Fri, 08 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3125/</guid><description>Definition When there is a physical phenomenon expressed by a formula, the process of solving the formula based on a cause to find information about the result is called a direct problem or forward problem. Conversely, the process of finding information about the cause from information about the result is called an inverse problem. Explanation Many problems are direct problems, for example, when an object with an initial velocity of</description></item><item><title>Including Functions</title><link>https://freshrimpsushi.github.io/en/posts/3124/</link><pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3124/</guid><description>Definition Let&amp;rsquo;s denote $X \subset Y$. A function that satisfies the following is called an inclusion function. $$ i : X \to Y, \quad \text{and} \quad i(x) = x,\quad \forall x\in X $$ Explanation Simply put, it&amp;rsquo;s an identity function whose codomain could be larger than its domain. $i : X \hookrightarrow Y$1 or $i : X \subset Y$2 notation is also used. 박대희·안승호</description></item><item><title>Expansion and Contraction of a Function</title><link>https://freshrimpsushi.github.io/en/posts/3123/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3123/</guid><description>Definition1 Let&amp;rsquo;s assume that function $f : X \to Y$ is given. Let&amp;rsquo;s also assume that $U \subset X \subset V$ holds. Contraction Mapping We call $f |_{U}$ a contraction mapping of $f$ if it satisfies the following. $$ f|_{U} : U \to Y \quad \text{and} \quad f|_{U}(x) = f (x),\quad \forall x \in U $$ Extension We call $\tilde{f}$ an extension of $f$ if it satisfies the following. $$</description></item><item><title>Helmholtz Equation</title><link>https://freshrimpsushi.github.io/en/posts/3122/</link><pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3122/</guid><description>Definition The following partial differential equation is called the Helmholtz equation. $$ \nabla^{2}u(x) + k^{2} u(x) = \Delta u(x) + k^{2} u(x) = (\Delta + k^{2} )u(x) = 0,\quad x \in \mathbb{R}^{n} $$ Here, $\nabla ^{2} = \Delta$ is the Laplacian. Explanation It can also be expressed in the form of $-\Delta u = \lambda u$. Hence it is sometimes called the eigenvalue equation for the Laplace operator. It can</description></item><item><title>Differential Equations: Fundamental Solutions and Green's Functions</title><link>https://freshrimpsushi.github.io/en/posts/3121/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3121/</guid><description>Definition A solution $u$ of a nonhomogeneous differential equation with a nonhomogeneous term of $f$, expressed as a function of $\Phi$ and $f$, is called the fundamental solution of the differential equation. $$ u = u\left( \Phi, f \right) $$ Description Note that this is not a strict definition. It is also called Green&amp;rsquo;s function. Both terms refer to the same concept, but Green&amp;rsquo;s function usually implies that boundary conditions</description></item><item><title>Coordinates of a Three-Dimensional Unit Sphere</title><link>https://freshrimpsushi.github.io/en/posts/3120/</link><pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3120/</guid><description>Formulas 3D space&amp;rsquo;s unit sphere can be represented by the following six coordinate patch mappings. For $(u,v) \in U = \left\{ (u,v) : u^{2} + v^{2} \lt 1 \right\}$, $$ \begin{align*} \mathbf{x} _{(0,0,1)}(u, v) &amp;amp;= \left( u, v , \sqrt{1- u^{2} -v^{2} } \right) \\ \mathbf{x}_{(0,0,-1)}(u, v) &amp;amp;= \left( u, v , -\sqrt{1- u^{2} -v^{2} } \right) \\ \mathbf{x}_{(0,1,0)}(u, v) &amp;amp;= \left( u, \sqrt{1- u^{2} -v^{2}}, v \right) \\ \mathbf{x}_{(0,-1,0)}(u,</description></item><item><title>Extensions of Bounded Linear Operators</title><link>https://freshrimpsushi.github.io/en/posts/3119/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3119/</guid><description>Theorem1 2 Let $V_{1}, V_{2}$ be a Banach space. Let $W \subset V_{1}$ be a dense subspace. And let $T : W \to V_{2}$ be a bounded linear operator. Then for all $\mathbf{v} \in W$, there exists a unique bounded linear operator that satisfies $$ \widetilde{T} : V_{1} \to V_{2} $$. Moreover, the following holds: $$ \| \widetilde{T} \| = \left\| T \right\| $$. Explanation $\widetilde{T}$ is called the extension</description></item><item><title>Properties of Bounded Linear Operators</title><link>https://freshrimpsushi.github.io/en/posts/3118/</link><pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3118/</guid><description>Theorem1 Let&amp;rsquo;s denote $V$ as normed space, $T$ as bounded linear operator, and $W \subset V$. Then, the following holds: (a) $$ T\left( \overline{W} \right) \subset \overline{T(W)} $$ Moreover, if $T$ is invertible, and $T^{-1}$ is also a bounded linear operator, then the following is true: $$ T\left( \overline{W} \right) = \overline{T(W)} $$ Here, $\overline{W}$ is the closure of $W$. (b) Let $\left\{ \mathbf{v}_{k} \right\}$ be a sequence in $V$,</description></item><item><title>Dense Subsets and Closures</title><link>https://freshrimpsushi.github.io/en/posts/3117/</link><pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3117/</guid><description>Dense Subsets Definition1 Let $W \subset V$ be a subset of the normed space $V$. For any $\mathbf{v} \in V$ and $\epsilon \gt 0$, if there always exists $\mathbf{w} \in W$ satisfying the following, then $W$ is called a dense subset in $V$. $$ \left\| \mathbf{v} - \mathbf{w} \right\| \le \epsilon $$ Explanation If $W$ is a dense subspace of $V$, it implies that any element within $V$ can be</description></item><item><title>Differentiable Manifolds</title><link>https://freshrimpsushi.github.io/en/posts/3116/</link><pubDate>Mon, 20 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3116/</guid><description>Definition1 Let $M$ be an arbitrary set and $U_{\alpha} \subset \mathbb{R}^{n}$ an open set. For a one-to-one function $\mathbf{x}_{\alpha} : U_{\alpha} \to M$, define the ordered pair $\left( M, \left\{ \mathbf{x}_{\alpha} \right\}_{\alpha\in \mathscr{A}} \right)$, or simply $M$, that satisfies the following conditions as a differentiable manifold of dimension $n$. $\bigcup \limits_{\alpha} \mathbf{x}_{\alpha} \left( U_{\alpha} \right) = M$ The map $\mathbf{x}_{\beta}^{-1} \circ \mathbf{x}_{\alpha} : \mathbf{x}_{\alpha}^{-1}(W) \to \mathbf{x}_{\beta}^{-1}(W)$ is differentiable for $\varnothing</description></item><item><title>Increasing Line Spacing in LaTeX</title><link>https://freshrimpsushi.github.io/en/posts/3115/</link><pubDate>Sat, 18 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3115/</guid><description>Code $$ \begin{vmatrix} \dfrac{ \partial x}{ \partial r} &amp;amp; \dfrac{ \partial x}{ \partial \theta} \\[1em] \dfrac{ \partial y}{ \partial r} &amp;amp; \dfrac{ \partial y}{ \partial \theta} \end{vmatrix} $$ $$ \begin{align*} \begin{vmatrix} \frac{ \partial x}{ \partial r} &amp;amp; \frac{ \partial x}{ \partial \theta} \\ \frac{ \partial y}{ \partial r} &amp;amp; \frac{ \partial y}{ \partial \theta} \end{vmatrix} &amp;amp;&amp;amp; \begin{vmatrix} \dfrac{ \partial x}{ \partial r} &amp;amp; \dfrac{ \partial x}{ \partial \theta} \\ \dfrac{</description></item><item><title>Saving and Loading Weights, Models, and Optimizers in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3114/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3114/</guid><description>Not Re-training1 2 3 Saving If you&amp;rsquo;re not planning to re-train, you can simply save the weights or the entire model. However, as mentioned below, if you&amp;rsquo;re planning to re-train, you also need to save the optimizer. Weights can be easily saved as follows: # 모델 정의 class CustomModel(nn.module): ...(이하생략) model = CustomModel() # 가중치 저장 torch.save(model.state_dict(),</description></item><item><title>Line Integrals of Vector Fields</title><link>https://freshrimpsushi.github.io/en/posts/3113/</link><pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3113/</guid><description>Definition1 Let a vector field $\mathbf{F} : \mathbb{R}^{3} \to \mathbb{R}^{3}$ and a curve $C$ in 3-dimensional space be given as $\mathbf{r}(t)$. Let $\mathbf{T}$ be called the tangent field of the vector field. Then, the $\mathbf{F}$ line integral along the curve $C$ is defined as follows. $$ \int_{C} \mathbf{F} \cdot d \mathbf{r} = \int_{a}^{b} \mathbf{F}\left( \mathbf{r}(t) \right) \cdot \mathbf{r}^{\prime}(t) dt = \int_{C} \mathbf{F} \cdot \mathbf{T} ds $$ Explanation The buildup to</description></item><item><title>Scalar Field Line Integral</title><link>https://freshrimpsushi.github.io/en/posts/3112/</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3112/</guid><description>Line Integral over a Plane Curve1 Buildup Given a function as in $y = f(x)$, its definite integral is defined by the idea of adding up all the function values $f(x)$ along the $x$ axis. Thus, the integral value is obtained along a straight line on the $x$ axis. Now, consider a two-variable function $z=f(x,y)$. Unlike in the case of single-variable functions, since the variable moves over the $xy-$ plane,</description></item><item><title>Length of a Curve</title><link>https://freshrimpsushi.github.io/en/posts/3111/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3111/</guid><description>Length of a Plane Curve1 Buildup Suppose we have a smooth function $y=f(x)$ given as in figure (a) above, with $n+1$ points on it. The total length $s$ of the curve can be obtained by summing up the lengths $s_{k}$ of each arc divided by points. Moreover, the length of each arc can be approximated by the length between two points as shown in figure (b). As the number of</description></item><item><title>Smooth Functions</title><link>https://freshrimpsushi.github.io/en/posts/3110/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3110/</guid><description>Definition If a function $f$ is infinitely differentiable, then $f$ is called a smooth function. If a function $f$ is differentiable and $f^{\prime}$ is continuous, then $f$ is called a smooth function. Explanation In analysis and functional analysis, the term smooth likely refers to the first definition. The phrase &amp;lsquo;infinitely differentiable&amp;rsquo; may seem ambiguous, but it can be understood as follows: $$ \text{For any natural number $n$, the $n$th derivative</description></item><item><title>Definition of Directional Derivative</title><link>https://freshrimpsushi.github.io/en/posts/3109/</link><pubDate>Mon, 06 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3109/</guid><description>Buildup Let&amp;rsquo;s say a multivariable function $f = \mathbb{R}^{n} \to \mathbb{R}$ is given. When trying to calculate the derivative of $f$, unlike the case with a univariable function, one must consider the rate of change in &amp;lsquo;which direction&amp;rsquo;. A familiar example is the partial derivative. The partial derivative considers the rate of change with respect to only one variable. For instance, the partial derivative $\dfrac{\partial f}{\partial y}$ of $f=f(x,y,z)$ with</description></item><item><title>Creating and Using Custom Datasets from Numpy Arrays in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3108/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3108/</guid><description>Description &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; from torch.utils.data import TensorDataset, DataLoader Assuming that a stack of 100 &amp;lsquo;black and white&amp;rsquo; photographs of size $32\times 32$ represented as a numpy array $X$, along with their labels $Y$, has been prepared. Let&amp;rsquo;s say it was imported with the following code. &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#34;X.npy&amp;#34;) &amp;gt;&amp;gt;&amp;gt; X.shape (100, 32, 32) &amp;gt;&amp;gt;&amp;gt; Y = np.load(&amp;#34;Y.npy&amp;#34;) &amp;gt;&amp;gt;&amp;gt; Y.shape (100) In order</description></item><item><title>Integrals of Trigonometric Functions Table</title><link>https://freshrimpsushi.github.io/en/posts/3107/</link><pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3107/</guid><description>Formulas $$ \begin{equation} \int_{0}^{\pi / 2} \sin \theta \cos \theta d \theta = \dfrac{1}{2} \end{equation} $$ $$ \begin{equation} \begin{aligned} \int \cos^{2}\theta d\theta &amp;amp;= \dfrac{1}{2}\theta + \dfrac{1}{4}\sin 2\theta + C \\ \int_{0}^{2\pi} \cos^{2}\theta d\theta &amp;amp;= \pi \\ \int_{0}^{\pi} \cos^{2}\theta d\theta &amp;amp;= \dfrac{\pi}{2} \end{aligned} \end{equation} $$ $$ \begin{equation} \begin{aligned} \int \sin^{2}\theta d\theta &amp;amp;= \dfrac{1}{2}\theta - \dfrac{1}{4}\sin 2\theta + C \\ \int_{0}^{2\pi} \sin^{2}\theta d\theta &amp;amp;= \pi \\ \int_{0}^{\pi} \sin^{2}\theta d\theta &amp;amp;= \dfrac{\pi}{2} \end{aligned}</description></item><item><title>Formulas Related to Factorials</title><link>https://freshrimpsushi.github.io/en/posts/3106/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3106/</guid><description>Product of Consecutive Odd Numbers For an integer $n \ge 0$, the following holds. $$ (2n-1) \cdot (2n-3) \cdots 5 \cdot 3 \cdot 1 = \dfrac{(2n)!}{2^{n} (n!)} = (2n-1)!! $$ Here, $n!!$ refers to the double factorial. Proof A detailed explanation is omitted. $$ \begin{align*} 3 \cdot 1 =&amp;amp;\ \dfrac{4 \cdot 3 \cdot 2 \cdot 1}{4 \cdot 2} = \dfrac{4!}{2^{2}(2 \cdot 1)} = \dfrac{(2 \cdot 2)!}{2^{2}(2!)} \\ 5 \cdot 3</description></item><item><title>Generalization of Gaussian Integrals</title><link>https://freshrimpsushi.github.io/en/posts/3105/</link><pubDate>Sun, 29 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3105/</guid><description>Formulas1 For an integer $n \ge 0$, the following expressions are true. When multiplied by an even degree polynomial $$ \int_{-\infty}^{\infty} x^{2n} e^{-\alpha x^{2}}dx = \dfrac{(2n)!}{n! 2^{2n}}\sqrt{\dfrac{\pi}{\alpha^{2n+1}}} $$ $$ \int_{0}^{\infty} x^{2n} e^{-\alpha x^{2}}dx = \dfrac{(2n)!}{n! 2^{2n+1}}\sqrt{\dfrac{\pi}{\alpha^{2n+1}}} $$ When multiplied by an odd degree polynomial $$ \int_{-\infty}^{\infty} x^{2n+1} e^{-\alpha x^{2}}dx = 0 $$ $$ \int_{0}^{\infty} x^{2n+1} e^{-\alpha x^{2}}dx = \dfrac{n!}{2 \alpha^{n+1}} $$ Explanation Gaussian Integral $$ \int_{-\infty}^{\infty} e^{-\alpha x^2} dx= \sqrt{\dfrac{\pi}{\alpha}}</description></item><item><title>Initializing Weights in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3104/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3104/</guid><description>Code1 Assuming we have defined a neural network as follows. The forward part is omitted. import torch import torch.nn as nn class Custom_Net(nn.Module): def __init__(self): super(Custom_Net, self).__init__() self.linear_1 = nn.Linear(1024, 1024, bias=False) self.linear_2 = nn.Linear(1024, 512, bias=False) self.linear_3 = nn.Linear(512, 10, bias=True) torch.nn.init.constant_(self.linear_1.weight.data, 0) torch.nn.init.unifiom_(self.linear_2.weight.data) torch.nn.init.xavier_normal_(self.linear_3.weight.data) torch.nn.init.xavier_normal_(self.linear_3.bias.data) def forward(self, x): ... Weight initialization can be set through nn.init. For layers with bias, this also needs to be specifically set. Basics</description></item><item><title>How to Implement MLP in PyTorch</title><link>https://freshrimpsushi.github.io/en/posts/3103/</link><pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3103/</guid><description>Library import torch import torch.nn as nn import torch.nn.functional as F nn and nn.functional include various layers, loss functions, activation functions, etc., for constructing neural networks.</description></item><item><title>Conformal Mapping</title><link>https://freshrimpsushi.github.io/en/posts/3102/</link><pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3102/</guid><description>Definition1 Assuming the mapping $\mathbf{f} : \mathbb{R}^{n} \to \mathbb{R}^{m}$ is given as follows. $$ \mathbf{f}(\mathbf{x}) = \left( f_{1}(\mathbf{x}), f_{2}(\mathbf{x}), \dots, f_{m}(\mathbf{x}) \right),\quad \mathbf{x}\in \R^{n} $$ The total derivative, or Jacobian matrix of $\mathbf{f}$ is as follows. $$ \mathbf{f}^{\prime} = J = \begin{bmatrix} \dfrac{\partial f_{1}}{\partial x_{1}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_{1}}{\partial x_{n}} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \dfrac{\partial f_{m}}{\partial x_{1}} &amp;amp; \cdots &amp;amp; \dfrac{\partial f_{m}}{\partial x_{n}} \end{bmatrix} $$ If</description></item><item><title>Derivatives of 3D Scalar/Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/3101/</link><pubDate>Sat, 21 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3101/</guid><description>Theorem When the 3D scalar function $f : \mathbb{R}^{3} \to \mathbb{R}^{1}$ is $f(x(t), y(t), z(t)) = f$, $\dfrac{df}{dt}$ is as follows. $$ \dfrac{d f}{d t} = \dfrac{\partial f}{\partial x}\dfrac{dx}{dt} + \dfrac{\partial f}{\partial y}\dfrac{dy}{dt} + \dfrac{\partial f}{\partial z}\dfrac{dz}{dt} $$ When the 3D vector function $\mathbf{f} : \mathbb{R}^{3} \to \mathbb{R}^{3}$ is $\mathbf{f}(x(t), y(t), z(t)) = (f_{1}, f_{2}, f_{3})$, $\dfrac{d \mathbf{f}}{dt}$ is as follows. $$ \begin{align*} \dfrac{d \mathbf{f}}{d t} =&amp;amp;\ \left( \dfrac{d f_{1}}{d</description></item><item><title>Embedding Theorems in Lp Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3100/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3100/</guid><description>Theorem1 If $\Omega \subset \mathbb{R}^{n}$ is an open set and let&amp;rsquo;s assume $\text{vol}(\Omega) = \int_{\Omega} 1 dx \lt \infty$. (a) For $1 \le p \le q \le \infty$, if $u \in L^{q}(\Omega)$ then, $u \in L^{p}(\Omega)$ and $$ \begin{equation} \left\| u \right\|_{p} \le \left( \text{vol}(\Omega) \right)^{\frac{1}{p} - \frac{1}{q}} \left\| u \right\|_{q} \end{equation} $$ And $L^{q}$ is embedded into $L^{p}$. $$ \begin{equation} L^{q}(\Omega) \to L^{p}(\Omega) \end{equation} $$ (b) For $1 \le</description></item><item><title>L infinity space</title><link>https://freshrimpsushi.github.io/en/posts/3099/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3099/</guid><description>Definitions1 Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. For a measurable function $u$ on $\Omega$, if there exists a constant $K$ that satisfies the following condition for $u$, then $u$ is said to be essentially bounded on $\Omega$. $$ \left| u(x) \right| \le K \text{ a.e. on } \Omega $$ Here, $\text{a.e.}$ means almost everywhere. The supremum of such $K$ is called the essential supremum of $\left| u</description></item><item><title>Inverse Holder's Inequality: A Sufficient Condition for Lp Functions</title><link>https://freshrimpsushi.github.io/en/posts/3098/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3098/</guid><description>Theorem1 Let&amp;rsquo;s consider $\Omega \subset \mathbb{R}^{n}$ as an open set. The necessary and sufficient condition for the measurable function $u$ to be included in the $L^{p}$ space is $$ \sup \left\{ \int_{\Omega} \left| u(x) \right| v(x) dx : v(x) \ge 0 \text{ on } \Omega, \left\| v \right\|_{p^{\prime}} \le 1 \right\} \lt \infty $$ Furthermore, the above supremum is as follows $\left\| u \right\|_{p}$. Here, $p^{\prime} = \dfrac{p}{p-1}$ is the</description></item><item><title>Definition and Properties of Vector Areas</title><link>https://freshrimpsushi.github.io/en/posts/3097/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3097/</guid><description>Definition For a given surface $S$, the following integral is called the vector area of $S$. $$ \mathbf{a} := \int_{\mathcal{S}} d \mathbf{a} $$ Description As an example, let&amp;rsquo;s calculate the vector area of a hemisphere with a radius of $R$. It is $d \mathbf{a} = R^{2}\sin\theta d\theta d\phi \hat{\mathbf{r}}$. Here, $$ \hat{\mathbf{r}} = \cos\phi \sin\theta \hat{\mathbf{x}} + \sin\phi \sin\theta\hat{\mathbf{y}} + \cos\theta\hat{\mathbf{z}} $$ when integrated over the region of the northern</description></item><item><title>Various Formulas of Vector Integration Involving the Del Operator</title><link>https://freshrimpsushi.github.io/en/posts/3096/</link><pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3096/</guid><description>Formulas1 Let&amp;rsquo;s designate $T, U$ as a scalar function, and $\mathbf{v}$ as a vector function. Then, the following equations hold: $$ \begin{equation} \int_{\mathcal{V}} (\nabla T) d \tau = \oint_{\mathcal{S}} T d \mathbf{a} \end{equation} $$ $$ \begin{equation} \int_{\mathcal{V}} (\nabla \times \mathbf{v}) d \tau = - \oint_{\mathcal{S}} \mathbf{v} \times d \mathbf{a} \end{equation} $$ $$ \begin{equation} \int_{\mathcal{V}} \left[ T \nabla^{2} U + (\nabla T) \cdot (\nabla U) \right] d \tau = \oint_{\mathcal{S}} (T</description></item><item><title>PyTorch RuntimeError: "grad can be implicitly created only for scalar outputs" Solution</title><link>https://freshrimpsushi.github.io/en/posts/3095/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3095/</guid><description>Example 1 If you have set the loss function as loss = sum(a,b), an error might occur during backpropagation when loss.backward() is called. Changing it to loss = torch.sum(a,b) will prevent the error.</description></item><item><title>Definition of Improper Integrals</title><link>https://freshrimpsushi.github.io/en/posts/3094/</link><pubDate>Sat, 07 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3094/</guid><description>Definition1 Assume that the function $f$ is integrable over every interval $[a,b]$ with fixed $a$ and $b&amp;gt;a$. If the following limit exists, then it is defined as the improper integral of $f$. $$ \int _{a}^{\infty} f(x) dx = \lim \limits_{b \to \infty} \int _{a}^{b} f(x)dx $$ In this case, if the integration on the left-hand side converges, and if replacing $f$ with $\left| f \right|$ the limit still exists, it</description></item><item><title>Cauchy Problem, Initial Value Problem</title><link>https://freshrimpsushi.github.io/en/posts/3093/</link><pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3093/</guid><description>Definition1 Let&amp;rsquo;s say we have a partial differential equation defined on an open set $\Omega=\mathbb{R}^{n}$. When the time is $t=0$, the value of the unknown $u$ at $\Omega$, that is, the initial value, is given. The problem of finding solutions to such partial differential equations is called the Cauchy problem or the initial value problem. Explanation The acronym IVP is commonly used. Example Solving the Cauchy problem for the heat</description></item><item><title>Wave Equation</title><link>https://freshrimpsushi.github.io/en/posts/3092/</link><pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3092/</guid><description>Definition1 The following partial differential equation is called the wave equation.wave equation. $$ u_{tt} - \Delta u =0 $$ This equation assumes the propagation speed of the wave as a constant $1$. If the propagation speed of the wave is denoted as $c$, then the wave equation becomes, $$ u_{tt} - c^{2}\Delta u =0 $$ In the case of being nonhomogeneousnonhomogeneous, $$ u_{tt} - \Delta u = f $$ $U</description></item><item><title>How to Solve the "Fail to create pixmap with TK_GetPixmap in TKImgPhotoInstanceSetSize" Error in Python matplotlib</title><link>https://freshrimpsushi.github.io/en/posts/3091/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3091/</guid><description>Problem import matplotlib.pyplot as plt import numpy as np for i in range(400): fig = plt.figure(figsize=(12, 12)) plt.plot(np.random.rand(10)) plt.savefig(&amp;#34;./plt_test/no_%d&amp;#34; %i) print(&amp;#34;test_%d&amp;#34; %i) Let&amp;rsquo;s say you are running a code in Python using matplotlib.pyplot to draw a graph and save it, like the one above. The moment you generate the 369th figure, the following error occurs in the output window. test_366 test_367 test_368 Fail to create pixmap with TK_GetPixmap in TKImgPhotoInstanceSetSize</description></item><item><title>What is a State Function in Thermophysics?</title><link>https://freshrimpsushi.github.io/en/posts/3090/</link><pubDate>Fri, 30 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3090/</guid><description>Definition1 A state function or state variable is a property that has a fixed value independent of the path taken and can be measured macroscopically. Explanation Let&amp;rsquo;s explain this more mathematically. Consider a function $f(\mathbf{x})$ that has a value in three dimensions. When $\mathbf{x}$ changes from $\mathbf{x}_{1}=a$ to $\mathbf{x}=b$, if the difference in the value of $f$ is independent of the path, then $f$ is called a state function. $$</description></item><item><title>How to Use the Shepp-Logan Phantom in Julia, MATLAB, and Python</title><link>https://freshrimpsushi.github.io/en/posts/3089/</link><pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3089/</guid><description>Julia1 To use the Tomography package Tomography.jl, you can use the phantom() function. phantom(m,n=1): Generates a Shepp-Logan phantom of size $m\times m$. When n=1, it generates a Shepp-Logan phantom, and when n=2, it generates a Modified Shepp-Logan phantom. using Tomography using Plots # 팬텀 생성 p = phantom(256,2) # 그림 출력 heatmap(reverse(p, dims=1)) Environment OS: Windows10 Version: Julia 1.7.1, Tomography 0.1.5 MATLAB2 You can create a</description></item><item><title>How to Use Radon Transform in Julia, MATLAB, Python</title><link>https://freshrimpsushi.github.io/en/posts/3088/</link><pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3088/</guid><description>Julia1 In the Tomography package Tomography.jl, one can use the radon() function. radon(f, θ): f stands for the image, θ for the projection angle. If not specified, it calculates from 0 degrees to $\pi$ by default. using Tomography using Plots # 팬텀 생성 f = phantom(256,2) # 라돈 변환 계산 ℛf = radon(f) # 그림 출력 h1 = heatmap(reverse(f, dims=1),</description></item><item><title>Dirichlet Boundary Conditions</title><link>https://freshrimpsushi.github.io/en/posts/3087/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3087/</guid><description>Definition1 Let us assume that a partial differential equation is given on an open set $\Omega$. The following boundary conditions are referred to as Dirichlet boundary conditions. The problem of finding solutions to partial differential equations with Dirichlet boundary conditions is called the Dirichlet problem. $$ u = 0 \quad \text{on } \partial \Omega $$ Explanation Nonhomogeneous Conditions The following boundary conditions are referred to as nonhomogeneous Dirichlet conditions, although,</description></item><item><title>Fundamental Solution of the Laplace Equation</title><link>https://freshrimpsushi.github.io/en/posts/3086/</link><pubDate>Thu, 22 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3086/</guid><description>Buildup1 Laplace&amp;rsquo;s equation is invariant under rotation transformations, so consider changing $u(x)$&amp;rsquo;s variables to radii. This allows simplifying the differential equation as follows. Let&amp;rsquo;s assume that $u=u(x)$ is a solution to Laplace&amp;rsquo;s equation. $$ \Delta u = 0 $$ And let&amp;rsquo;s set $r=|x|=(x_{1}^{2} + \cdots + x_{n}^{2})^{1/2}$ and assume $v\in C^2$ and $u(x) = v(|x|) = v(r) (x\in \mathbb{R}^{n} \setminus \left\{ 0 \right\})$. $$ \begin{align*} v(r) &amp;amp;= u(x) \\ \Delta</description></item><item><title>Smoothness of Boundaries</title><link>https://freshrimpsushi.github.io/en/posts/3085/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3085/</guid><description>Definition1 Let&amp;rsquo;s call $U \subset \mathbb{R}^{n}$ a bounded open set. Let $\partial U$ be the boundary of $U$. If there exists a $C^{k}$ function $\gamma = \mathbb{R}^{n-1} \to \mathbb{R}$ satisfying the following for each point $x = (x_{1}, \dots, x_{n}) \in \partial U$ on the boundary, then we say &amp;rsquo;the boundary $\partial U$ is $C^{k}$'. $$ \gamma (x_{1}, x_{2}, \dots, x_{n-1}) = x_{n} $$ Explanation To rephrase the condition in</description></item><item><title>Rotation Transformation, Rotation Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3084/</link><pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3084/</guid><description>Definition In the two-dimensional plane $\mathbb{R}^{2}$, the transformation that rotates an arbitrary vector counterclockwise by $\theta$ is given by $$ \begin{bmatrix} x^{\prime} \\ y^{\prime} \end{bmatrix} = \begin{bmatrix} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} $$ Explanation The matrix $\begin{bmatrix} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \end{bmatrix}$ is called the rotation matrix or the rotation transformation.</description></item><item><title>Absolute Value Function</title><link>https://freshrimpsushi.github.io/en/posts/3083/</link><pubDate>Fri, 16 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3083/</guid><description>Definition A function defined as $f$ is called the absolute value function, and its values are denoted as shown in $|x|$. $$ |x| := f(x) = \begin{cases} x &amp;amp;\text{if } x&amp;gt;0 \\ 0 &amp;amp;\text{if } x=0 \\ -x &amp;amp;\text{if } x&amp;lt;0 \end{cases},\quad x\in \mathbb{R} $$ Explanation Absolute value refers to the magnitude of a real number, and a generalization of this is the norm. The triangle inequality holds. $$ |x</description></item><item><title>Partial Derivatives: Derivatives of Multivariable Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/3082/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3082/</guid><description>Buildup[^1] Recall the definition of the derivative of a univariate function. $$ \lim \limits_{h\to 0} \dfrac{f(x+h) - f(x)}{h} = f^{\prime}(x) $$ By approximating the numerator on the left-hand side as a linear function of $h$, we get the following. $$ \begin{equation} f(x+h) - f(x) = a h + r(h) \label{1} \end{equation} $$ Let&amp;rsquo;s call $r(h)$ the remainder, satisfying the condition below. $$ \lim \limits_{h \to 0} \dfrac{r(h)}{h}=0 $$ Then, dividing</description></item><item><title>Laplacian of a Scalar Field</title><link>https://freshrimpsushi.github.io/en/posts/3081/</link><pubDate>Mon, 12 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3081/</guid><description>Definition The divergence of the gradient of the scalar function $u : \mathbb{R}^{n} \to \mathbb{R}$ is called the Laplacian and is denoted as follows. $$ \begin{align*} \Delta u :&amp;amp;= \mathrm{div}(\nabla (u)) \\ &amp;amp;= \mathrm{div} \left( \left( u_{x_{1}}, u_{x_{2}}, \dots, u_{x_{n}} \right) \right) \\ &amp;amp;= u_{x_{1}x_{1}} + u_{x_{2}x_{2}} + \cdots + u_{x_{n}x_{n}} \\ &amp;amp;= \sum _{i=1}^{n} u_{x_{i}x_{i}} \end{align*} $$ Here, $u_{x_{i}}=\dfrac{\partial u}{\partial x_{i}}$ is. Explanation In mathematics, the divergence is often</description></item><item><title>Transport Equation</title><link>https://freshrimpsushi.github.io/en/posts/3080/</link><pubDate>Sat, 10 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3080/</guid><description>Definition1 Below is referred to as a transport equation. $$ \begin{equation} u_{t} + b \cdot Du=0\quad \text{in }\mathbb{R}^n \times (0,\ \infty) \end{equation} $$ $b=(b_{1}, b_2, \cdot, b_{n}) \in \mathbb{R}^n$ is a fixed vector $u=u(x,t)$ is $u:\mathbb{R}^n \times [0,\infty) \rightarrow \mathbb{R}$ $x=(x_{1}, \cdots , x_{n})\in \mathbb{R}^n$ $t \ge 0$ is time $Du=D_{x}u=(u_{x_{1}}, \cdots ,u_{x_{n}})$ is the gradient of $u$ with respect to the spatial variable $x$ Explanation Assume $u \in C^1$</description></item><item><title>Every n-dimensional Real Vector Space is Isomorphic to R^n</title><link>https://freshrimpsushi.github.io/en/posts/3079/</link><pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3079/</guid><description>Definition1 Let $V$ and $W$ be called a vector space. If there exists an invertible (bijective) linear transformation $T : V \to W$, then $V$ and $W$ are said to be isomorphic. $T$ is called an isomorphism. Theorem Every $n$-dimentional real vector space is isomorphic to $\mathbb{R}^{n}$. Explanation Another way to express the theorem is as follows. &amp;ldquo;A $\mathbb{R}$-vector space $V$ being isomorphic to $\mathbb{R}^{n}$&amp;rdquo; is equivalent to &amp;ldquo;being $\dim{V}=n$&amp;rdquo;.</description></item><item><title>The Matrix Representation of a Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3078/</link><pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3078/</guid><description>Definition1 Let&amp;rsquo;s call $V, W$ a finite-dimensional vector space. Let&amp;rsquo;s call $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ and $\gamma = \left\{ \mathbf{w}_{1}, \dots, \mathbf{w}_{m} \right\}$ the ordered bases for $V$ and $W$, respectively. Let&amp;rsquo;s call $T : V \to W$ a linear transformation. Then, by the uniqueness of the basis representation, there exists a unique scalar $a_{ij}$ satisfying the following. $$ T(\mathbf{v}_{j}) = \sum_{i=1}^{m}a_{ij}\mathbf{w}_{i} = a_{1j}\mathbf{w}_{1} + \cdots +</description></item><item><title>Uniform Convergence and Continuity of Function Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1893/</link><pubDate>Sun, 04 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1893/</guid><description>정리1 거리공간 $E$위에서 함수열 $\left\{ f_{n} \right\}$이 $f$로 균등 수렴한다고 하자. $$ f_{n} \rightrightarrows f $$ $E$의 집적점 $x$에 수열 $A_{n}(x)$</description></item><item><title>Back Propagation Algorithm</title><link>https://freshrimpsushi.github.io/en/posts/3077/</link><pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3077/</guid><description>This article is written for math majors to understand the principles of the backpropagation algorithm. Notation Given an artificial neural network like the one shown above. Let $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n_{0}})$ be the input, $y_{j}^{l}$ be the $j$th node of the $l$th layer, $\hat{\mathbf{y}} = (\hat{y}_{1}, \hat{y}_{2}, \dots, \hat{y}_{\hat{n}})$ is the output . Let $L \in \mathbb{N}$ be the number of hidden layers, and the components of $\mathbf{n}=(n_{0},</description></item><item><title>Properties of the Space of Invertible Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3076/</link><pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3076/</guid><description>Theorem1 Let&amp;rsquo;s call the set of all invertible linear transformations on $\Omega$. $$ \Omega = \left\{ \text{all invertible linear operator on } \mathbb{R}^{n} \right\} $$ (a) If the following holds for $T_{1} \in \Omega$ and $T_{2} \in L(\mathbb{R}^{n})$, then $T_{2} \in \Omega$ is true. $$ \| T_{2} - T_{1} \| \| T_{1}^{-1} \| &amp;lt; 1 $$ Here, $\| T \|$ is the norm of the linear transformation. (b) The following</description></item><item><title>Norm of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3075/</link><pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3075/</guid><description>Definition1 Define the norm of the linear transformation $T \in L(\mathbb{R}^{n}, \mathbb{R}^{m})$ as follows. $$ \begin{equation} \| T \| := \sup \limits_{\| \mathbf{x} \| = 1} \| T(\mathbf{x}) \| \end{equation} $$ Explanation (a) From (a) we have the following equality, so $\| T \|$ can be interpreted as the ratio by which $T$ changes the magnitude of elements of $\mathbb{R}^{n}$ when mapping them into $\mathbb{R}^{m}$. In other words, no matter</description></item><item><title>Composition of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3074/</link><pubDate>Sat, 26 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3074/</guid><description>Definition1 Given linear transformations $T_{1} : V \to W$ and $T_{2} : W \to Z$, the transformation defined by $T_{2} T_{1}$ is called the composition of $T_{1}$ and $T_{2}$. $$ (T_{2} \circ T_{1})(\mathbf{x}) = T_{2}\left( T_{1}(\mathbf{x}) \right) \quad \mathbf{x} \in V $$ Explanation The composition of linear transformations is often denoted simply as follows: $$ T_{2}T_{1}\mathbf{x} = (T_{2} \circ T_{1}) (\mathbf{x}) $$ In finite dimensions, this is essentially the same</description></item><item><title>Necessary and Sufficient Conditions for Linear Transformations to be Surjective and Injective</title><link>https://freshrimpsushi.github.io/en/posts/3073/</link><pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3073/</guid><description>Theorem 11 The following two propositions are equivalent concerning a linear transformation $T: V \to W$. $T$ is one-to-one. $N(T) = \text{ker}(T) = \left\{ \mathbf{0} \right\}$ Explanation This means that understanding the kernel of $T$ is a method to determine whether $T$ is one-to-one or not. According to the theorem, a linear transformation being one-to-one is equivalent to the following condition. $$ \mathbf{x} \ne \mathbf{0} \implies T(\mathbf{x}) \ne \mathbf{0} $$</description></item><item><title>Rank, Nullity, and Dimension Theorems of Linear Transformations</title><link>https://freshrimpsushi.github.io/en/posts/3072/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3072/</guid><description>Definition1 Let $T : V \to W$ be a linear transformation. If the range $R(T)$ of $T$ is finite-dimensional, the dimension of $R(T)$ is called the rank of $T$, denoted by: $$ \mathrm{rank}(T) := \dim (R(T)) $$ If the null space $N(T)$ of $T$ is finite-dimensional, the dimension of $N(T)$ is called the nullity of $T$, denoted by: $$ \mathrm{nullity}(T) := \dim\left( N(T) \right) $$ Explanation This is a generalization</description></item><item><title>Kernel and Range of Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3071/</link><pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3071/</guid><description>Definition1 Let&amp;rsquo;s say $T : V \to W$ is a linear transformation. The set of elements of $V$ that are mapped to $\mathbf{0}$ by $T$ is called the kernel or null space, and is denoted as follows. $$ \text{ker}(T) = N(T) := \left\{ \mathbf{v} \in V : T( \mathbf{v} ) = \mathbf{0} \right\} $$ The set of images under $\mathbf{v} \in V$ by $T$ is called the range or image</description></item><item><title>The Basis of the Domain Generates the Image of the Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3070/</link><pubDate>Fri, 18 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3070/</guid><description>Theorem1 Let&amp;rsquo;s suppose we have a given linear transformation $T : V \to W$. Assume $V$ is finite-dimensional, and let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ be a basis of $V$. Then, the image of any $\mathbf{v} \in V$ can be represented as follows. $$ T(\mathbf{v}) = c_{1}T(\mathbf{v}_{1}) + c_{2}T(\mathbf{v}_{2}) + \cdots c_{n}T(\mathbf{v}_{n}) $$ Here, $c_{i}$ are coefficients that satisfy $\mathbf{v} = \sum c_{i}\mathbf{v}_{i}$. In other words, $\left\{</description></item><item><title>Distributional Convolution Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1895/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1895/</guid><description>Theorem1 Let&amp;rsquo;s say $\phi$ is a test function that satisfies $\int_{\mathbb{R}^{n}}\phi (\mathbf{x})d\mathbf{x}=1$. And let $\phi_{\epsilon}(\mathbf{x})=\epsilon^{-n}\phi (\epsilon^{-1}\mathbf{x})$ be given. Then, for any distribution $F$ and regular distribution $T_{F*\phi_{\epsilon}}$, when $\epsilon \to 0$, $T_{F*\phi_{\epsilon}}$ converges to $F$. $$ T_{F * \phi_{\epsilon}} \overset{\text{w}}{\to} F\quad \text{as } \epsilon \to 0 $$ Description The name &amp;lsquo;The Convolution Convergence Theorem for Distributions&amp;rsquo; is arbitrarily given as there was no specific name attached to the content above.</description></item><item><title>Discrete Fourier Inversion</title><link>https://freshrimpsushi.github.io/en/posts/3069/</link><pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3069/</guid><description>Formula1 Let&amp;rsquo;s denote the Discrete Fourier Transform of $\mathbf{a} = (a_{0}, a_{1}, \dots, a_{N-1}) \in \mathbb{C}^{N}$ as $\hat{\mathbf{a}} = (\hat{a}_{0}, \hat{a}_{1}, \dots, \hat{a}_{N-1}) \in \mathbb{C}^{N}$. $$ \mathcal{F}_{N}(\mathbf{a}) = \hat{\mathbf{a}},\quad \hat{a}_{m}=\sum_{n=0}^{N-1}e^{-i2\pi mn /N}a_{n} $$ Then, the following holds. $$ a_{n} = \dfrac{1}{N} \sum \limits_{m=0}^{N-1} e^{i 2 \pi m n / N} \hat{a}_{m} $$ Explanation This is known as the inverse formula for discrete Fourier transform. Proof Lemma For $m = 0,</description></item><item><title>Gas Flux</title><link>https://freshrimpsushi.github.io/en/posts/3063/</link><pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3063/</guid><description>Definition1 In physics, flux is the number of particles (or a physical quantity such as energy, momentum, etc.) that pass through (collide with) a unit area per unit time. Flux is often denoted by the capital letter Phi (Φ) $\Phi$. $$ \Phi = \dfrac{\text{physical quantity}}{\text{area} \times \text{time}} $$ Explanation According to the definition, the flux of a gas refers to the number of gas molecules passing through a</description></item><item><title>Dalton's Law</title><link>https://freshrimpsushi.github.io/en/posts/3062/</link><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3062/</guid><description>Law1 $n$ Let&amp;rsquo;s consider a mixture of gases composed of ▷eq.◁ different types of molecules. The total pressure of this gas mixture is equal to the sum of the pressures of each individual gas. $$ p = \sum \limits_{i=1}^{n}N_{i} k_{B} T = \sum \limits_{i=1}^{n}p_{i} $$ Explanation This law was named after the British chemist John Dalton (1766~1844). Stephen J. Blundell and Katherine M. Blundell, Concepts in</description></item><item><title>Solid Angle of a Sphere</title><link>https://freshrimpsushi.github.io/en/posts/3061/</link><pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3061/</guid><description>Definition1 A solid angle $\Omega$ of a 3-dimensional sector with a radius of $r$ and a surface area of $A$ is defined as follows: $$ \Omega := \dfrac{A}{r^{2}} $$ The unit is called steradian and is denoted as $\mathrm{sr}$. Explanation Considering how the radian angle in a circle is defined as the ratio of the arc length to the radius, this definition seems natural. $$ \theta := \dfrac{s}{r} $$ However,</description></item><item><title>Derivation of the Ideal Gas Equation through Kinetic Theory of Gases</title><link>https://freshrimpsushi.github.io/en/posts/3060/</link><pubDate>Sat, 29 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3060/</guid><description>Definition[^1] Pressure $p$ of a fluid acting on an area $M$ is defined as the ratio of the force $F$ the fluid exerts perpendicularly on area $M$ to the area $M$ itself. $$ p:=\frac{F}{M} \left[ \mathrm{N/m^{2}} \right] $$ Formula Let the volume of the gas be $V$, the temperature be $T$, and the number of molecules be $N$. Then, the pressure $p$ of the gas satisfies the following equation. $$</description></item><item><title>Expectation Value of Speed and Velocity of Gas Molecules</title><link>https://freshrimpsushi.github.io/en/posts/3059/</link><pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3059/</guid><description>Formulas1 Let&amp;rsquo;s denote the velocity of gas molecules as $\mathbf{v} = (v_{x}, v_{y}, v_{z})$ and their speed as $v = | \mathbf{v} |$. The expected values of velocity and speed of gas molecules are as follows. $$ \begin{align*} \left\langle v_{x} \right\rangle &amp;amp;= 0 \\ \left\langle |v_{x}| \right\rangle &amp;amp;= \sqrt{\dfrac{2 k_{B} T}{\pi m}} \\ \left\langle v_{x} ^{2} \right\rangle &amp;amp;= \dfrac{k_{B} T}{\pi m} \\ \left\langle v \right\rangle &amp;amp;= \sqrt{\dfrac{8 k_{B} T}{\pi m}}</description></item><item><title>Discrete Fourier Transform Properties</title><link>https://freshrimpsushi.github.io/en/posts/3058/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3058/</guid><description>Properties1 Let&amp;rsquo;s denote the Discrete Fourier Transform as $\mathscr{F}_{N}$ or $\hat{\mathbf{a}}$ given $\mathbf{a} \in \mathbb{C}^{N}$. Convolution $$ \mathscr{F}_{N}(\mathbf{a} \ast \mathbf{b}) = \hat{\mathbf{a}} \hat{\mathbf{b}} = (\hat{a}_{0}\hat{b}_{0}, \dots, \hat{a}_{N-1}\hat{b}_{N-1}) $$ In this case, $\ast$ is the discrete convolution. Explanation The Discrete Fourier Transform also satisfies the properties that the Fourier Transform does. Gerald B. Folland, Fourier Analysis and Its Applications (1992), p251&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Convergence in Schwartz Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1897/</link><pubDate>Sun, 23 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1897/</guid><description>Definition Let us assume that a sequence in the Schwartz space is denoted by $\left\{ \phi_{n} \right\}$. If for all multi-indexes $\alpha$, $\beta$, the sequence $\left\{ \mathbf{x}^{\beta}D^{\alpha}\phi_{n}(\mathbf{x}) \right\}$ converges uniformly to $0$, then we define that $\left\{ \phi_{n} \right\}$ converges to $0$ and denote it as follows. $$ \phi_{n} \overset{\mathcal{S}}{\to} 0 $$ Explanation By generalizing the above definition, if $\left\{ \phi_{n}-\phi \right\}$ converges to $0$, we can say that $\left\{</description></item><item><title>Picard's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3057/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3057/</guid><description>Buildup1 Consider the following ODE system. $$ \begin{equation} \begin{aligned} x_{1}^{\prime}(t) =&amp;amp;\ F_{1}(t,x_{1},x_{2},\cdots,x_{n}) \\ x_{2}^{\prime}(t) =&amp;amp;\ F_{2}(t,x_{1},x_{2},\cdots,x_{n}) \\ \vdots &amp;amp; \\ x_{n}^{\prime}(t) =&amp;amp;\ F_{n}(t,x_{1},x_{2},\cdots,x_{n}) \end{aligned} \end{equation} $$ Assume the values of $x_{i}$ are as follows when $t=t_{0}$. $$ \begin{equation} x_{1}(t_{0}) = x_{1}^{0}, x_{2}(t_{0}) = x_{2}^{0}, \dots, x_{n}(t_{0}) = x_{n}^{0} \end{equation} $$ Combining $(1)$ and $(2)$ into an initial value problem of a system of first-order differential equations, and finding the solution</description></item><item><title>Proof of Heisenberg's Uncertainty Principle</title><link>https://freshrimpsushi.github.io/en/posts/3056/</link><pubDate>Wed, 19 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3056/</guid><description>정리1 두 연산자 $A$와 $B$에 대하여 다음이 성립한다. $$ \sigma_{A}^{2}\sigma_{B}^{2} \ge \left( \dfrac{1}{2\i} \braket{[A, B]} \right)^{2} $$ 이때 $\sigma_{A}^{2}$는 $A$의 분산, $[A, B]$는 $A$와</description></item><item><title>Heisenberg Uncertainty Principle</title><link>https://freshrimpsushi.github.io/en/posts/3055/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3055/</guid><description>Buildup There exists a special relationship between $f$ and its Fourier transform $\hat{f}$. If for some constant $\Omega$, $\hat{f} (\omega) = 0\ for\ | \omega | \ge \Omega$ holds, then it is not possible for $f$ to exhibit the same property. In other words, it indicates that both $f$ and $\hat{f}$ cannot be concentrated in a narrow location simultaneously, mathematically speaking, this means $f$ and $\hat{f}$ cannot both have a</description></item><item><title>Properties of Divergent Real Sequences</title><link>https://freshrimpsushi.github.io/en/posts/3052/</link><pubDate>Sat, 15 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3052/</guid><description>Summary1 Let $\left\{ x_{n} \right\}$, $\left\{ y_{n} \right\}$ be real sequences and let $\lim \limits_{n\to\infty} x_{n}=\infty(-\infty)$. Then the following hold: (a) If $\left\{ y_{n} \right\}$ is bounded below (bounded above), then $\lim \limits_{n\to\infty}(x_{n}+y_{n}) = \infty(-\infty)$. (b) $\forall \alpha &amp;gt; 0,\quad \lim \limits_{n\to\infty} \alpha x_{n} = \infty (-\infty)$. (c) If for every $n\in \mathbb{N}$, there exists a $M_{0} &amp;gt;0$ such that $y_{n} &amp;gt; M_{0}$, then $\lim \limits_{n\to\infty} x_{n}y_{n} = \infty(-\infty)$. (d)</description></item><item><title>Sampling Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3054/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3054/</guid><description>Buildup[^1] Consider a physical signal $f$ being measured over time $t_{1} &amp;lt; t_{2} &amp;lt; t_{3} &amp;lt; \cdots$. Even if we know $f(t_{1}), f(t_{2}), \dots$, we generally cannot know the values for arbitrary $f(t)$. However, let&amp;rsquo;s assume that signal $f$ contains only frequencies within a certain range. That is, we consider a signal $f$ that contains only frequencies smaller than some constant $\Omega$, which is referred to as a band-limited signal.</description></item><item><title>Limits of Exponential and Logarithmic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3053/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3053/</guid><description>Formulas Exponential functions and logarithmic functions satisfy the following equations. $$ \begin{equation} \lim \limits_{x \to 0} \dfrac{\log (x + 1) }{x} = 1 \end{equation} $$ $$ \begin{equation} \lim \limits_{x \to 0} \dfrac{ e^{x} - 1}{x} = 1 \end{equation} $$ Proof $(1)$ $$ \begin{align*} \lim \limits_{x \to 0} \dfrac{\log (x + 1) }{x} &amp;amp;= \lim \limits_{x \to 0} \dfrac{1}{x} \log ( x + 1) \\ &amp;amp;= \lim \limits_{x \to 0} \log</description></item><item><title>Monotone Sequence and Monotone Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3051/</link><pubDate>Sun, 09 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3051/</guid><description>Definition1 For the sequence of real numbers $\left\{ s_{n} \right\}$, If $s_{n} \le s_{n+1}$ holds, it is called monotonically increasing. If $s_{n} \ge s_{n+1}$ holds, it is called monotonically decreasing. A sequence that is either monotonically increasing or monotonically decreasing is called monotonic. If $s_{n} \lt s_{n+1}$ holds, it is called an increasing sequence. If $s_{n} \gt s_{n+1}$ holds, it is called a decreasing sequence. Explanation Since a sequence is</description></item><item><title>Integration of Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/1902/</link><pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1902/</guid><description>Definition1 Let $I^{k}$ be a k-cell, and assume $\mathbf{x} \in I^{k}$. $$ \mathbf{x} = (x_{1},\dots,x_{k}),\quad a_{i} \le x_{i} \le b_{i} (i=1,\dots,k) $$ Suppose $f: I^{k} \to \mathbb{R}$ is continuous. Then, since it is integrable, let us set it as $f=f_{k}$, and define $f_{k-1} : I^{k-1} \to \mathbb{R}$ as follows: $$ f_{k-1} (x_{1}, \dots, x_{k-1}) = \int_{a_{k}}^{b_{k}} f_{k}(x_{1}, \dots, x_{k}) dx_{k} $$ Then, by the Leibniz Rule, $f_{k-1}$ is continuous in</description></item><item><title>Convolution of Multivariable Functions</title><link>https://freshrimpsushi.github.io/en/posts/1903/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1903/</guid><description>Definition Let&amp;rsquo;s say we have $f,g:\mathbb{R}^{n}\to \mathbb{C}$ and $\mathbf{x},\mathbf{y} \in \mathbb{R}^{n}$. Then the convolution of these two multivariable functions is as follows: $$ f \ast g(\mathbf{x})=\int f(\mathbf{y})g(\mathbf{x}-\mathbf{y})d\mathbf{y} $$ In this case, the integral mentioned above is the integral of a multivariable function. Properties The convolution of multivariable functions also satisfies the same desirable properties as the convolution of single-variable functions. (a) Commutative Law $$ f \ast g = g \ast</description></item><item><title>Weighted Lp Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1856/</link><pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1856/</guid><description>Definition1 A function space defined as follows is called a weighted $L^{p}$ space or specifically a $w$-weighted $L^{p}$ space. $$ L_{w}^{p}(a,b):= \left\{ f : \mathbb{R}\to \mathbb{C}\ \big|\ \int_{a}^{b} \left| f(x) \right|^{p}w(x)dx &amp;lt;\infty \right\} $$ Here, $w:\mathbb{R}\to[0,\infty)$ is called a weight function. Description It is one of the spaces that generalizes the $L^{p}$ space. When it is $w(x)=1$, $L_{w}^{p}=L^{p}$ holds. The norm of the weighted $L^{p}$ space is defined as follows</description></item><item><title>The Definition of Euler's Constant, the Natural Number e</title><link>https://freshrimpsushi.github.io/en/posts/3050/</link><pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3050/</guid><description>Definition1 The limit of the series below is defined as constant $e$. $$ e: = \sum \limits_{n=0}^{\infty} \dfrac{1}{n!} $$ Explanation Even if we do not know what that value is right away, it can be easily shown that the series mentioned above converges to some limit. The partial sum $s_{n}$, as it is bounded and increasing, converges. $$ \begin{align*} s_{n} &amp;amp;= 1 + 1 + \dfrac{1}{2} + \dfrac{1}{2 \cdot 3}</description></item><item><title>Orthogonal Basis and Its Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/3040/</link><pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3040/</guid><description>Definition1 An inner product space basis $V$ that is an orthogonal set is called an orthogonal basis. If $S$ is an orthonormal set, it is called an orthonormal basis. Theorem If $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{n} \right\}$ is an orthogonal basis of the inner product space $V$, and let $\mathbf{u} \in V$. Then, the following equation holds. $$ \begin{equation} \begin{aligned} \mathbf{u} &amp;amp;= \dfrac{\langle \mathbf{u}, \mathbf{v}_{1} \rangle}{\| \mathbf{v}_{1} \|^{2}}</description></item><item><title>Basis Addition/Subtraction Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3028/</link><pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3028/</guid><description>Theorem1 Let $S$ be a non-empty subset of vector space $V$. (a) If $S$ is linearly independent and if $\mathbf{v} \in V$ equals $\mathbf{v} \notin \text{span}(S)$, then $S \cup \left\{ \mathbf{v} \right\}$ remains linearly independent. (b) If $\mathbf{v} \in S$ can be represented as a linear combination of other vectors in $S$, then $S$ and $S \setminus \left\{ \mathbf{v} \right\}$ span the same space. That is, the following holds: $$</description></item><item><title>Differentiation of Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/3049/</link><pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3049/</guid><description>Formulas The derivative of the exponential function is as follows. $$ \begin{equation} \dfrac{d e^{x}}{dx} = e^{x} \label{fml1} \end{equation} $$ The derivative of the exponential composite function is as follows. $$ \begin{equation} \dfrac{d \left( e^{f(x)} \right)}{dx} = f^{\prime}(x)e^{f(x)} \label{fml2} \end{equation} $$ Description The exponential function is the only function that is equal to its own derivative. Derivation (1) Using the definition of the derivative, the calculation is as follows. $$ \begin{align*}</description></item><item><title>Relationship Between Orthogonality and Linear Independence</title><link>https://freshrimpsushi.github.io/en/posts/3045/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3045/</guid><description>Definition1 An inner product space $V$&amp;rsquo;s two vectors $\mathbf{u}, \mathbf{v}$ are said to be orthogonal if they satisfy $\langle \mathbf{u}, \mathbf{v} \rangle = 0$. A set made up of elements of $V$ where each element is orthogonal to every other element is called an orthogonal set. If the norm of every element in an orthogonal set is $1$, then it is called an orthonormal set. Theorem A subset $S =</description></item><item><title>Basis of Row Space, Column Space, and Null Space</title><link>https://freshrimpsushi.github.io/en/posts/3027/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3027/</guid><description>Overview1 The concepts such as row space, column space, null space were created to solve linear systems $A \mathbf{x} = \mathbf{b}$. Linear systems can be solved through basic row operations, and indeed, the row space and null space are invariant under basic row operations, indicating their relationship with linear systems. It is important to note here that the column space is not invariant under basic row operations. Theorem 1 (a1)</description></item><item><title>Composition of Functions</title><link>https://freshrimpsushi.github.io/en/posts/3048/</link><pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3048/</guid><description>Definitions A function $f: X \to Y$, $g: f(X) \to Z$ is defined as follows: the composition of $g$ with $f$ is called $h: X \to Z$, and it is denoted by $h=g \circ f$. $$ h(x) = (g\circ f) (x) := g\left( f(x) \right) $$</description></item><item><title>What is Inner Product in Real Vector Spaces?</title><link>https://freshrimpsushi.github.io/en/posts/3044/</link><pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3044/</guid><description>Definition1 Let $V$ be a real vector space. An inner product on $V$ is a function that maps two vectors in $V$ to a single real number $\langle \mathbf{u}, \mathbf{v} \rangle$, satisfying the following conditions: When $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k \in \mathbb{R}$, $\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle$ $\langle \mathbf{u} + \mathbf{v}, \mathbf{w} \rangle = \langle \mathbf{u}, \mathbf{w} \rangle + \langle \mathbf{v}, \mathbf{w} \rangle$</description></item><item><title>Linear Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3026/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3026/</guid><description>Definition1 A transformation is when a function $T : V \to W$ maps from one vector space to another, that is $V$, $W$ are both vector spaces, we call $T$ a transformation. If the transformation $T$ is a linear function, satisfying the following two conditions for any $\mathbf{v},\mathbf{u} \in V$ and scalar $k$, it is called a linear transformation: $T(k \mathbf{u}) = k T(\mathbf{u})$ $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) +</description></item><item><title>Derivatives of Logarithmic Functions</title><link>https://freshrimpsushi.github.io/en/posts/3047/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3047/</guid><description>Formulas The derivative of a logarithmic function with base $e$ is as follows. $$ \begin{equation} \dfrac{d \log x}{dx}=\dfrac{1}{x} \end{equation} $$ The derivative of a composite logarithmic function is as follows. $$ \begin{equation} \dfrac{d \left( \log f(x) \right)}{dx} = \dfrac{f^{\prime}(x)}{f(x)} \end{equation} $$ Explanation Especially, $(2)$ is used as a useful substitution trick. Derivation $(1)$ By the definition of logarithmic functions, the following equation holds. $$ x = e^{\log x} $$ Differentiating</description></item><item><title>Necessary and Sufficient Conditions for a Basis in Finite-Dimensional Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3043/</link><pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3043/</guid><description>Theorem1 Let $V$ be a $n$-dimensional vector space. Suppose a subset $S\subset V$ has $n$ elements. A necessary and sufficient condition for $S$ to be a basis of $V$ is that $V = \text{span}(S)$ or $S$ is linearly independent. Explanation Vector space, dimension, basis, span, independence - all these fundamental concepts of linear algebra appear here. For a set to be a basis of a vector space, it must be</description></item><item><title>Inverse Matrices and Systems of Linear Equations</title><link>https://freshrimpsushi.github.io/en/posts/3024/</link><pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3024/</guid><description>Theorem: Equivalent Conditions for an Invertible Matrix1 Let $A$ be a square matrix of size $n\times n$. Then the following statements are equivalent. (a) $A$ is an invertible matrix. (e) $A\mathbf{x}=\mathbf{b}$ has a solution for all matrices $n\times 1$ of size $\mathbf{b}$. (f) $A\mathbf{x}=\mathbf{b}$ has exactly one solution for all matrices $n\times 1$ of size $\mathbf{b}$. That is, $\mathbf{x}=A^{-1}\mathbf{b}$ holds. Description (e) and (f) being equivalent means that if the</description></item><item><title>Projection Theorem in Linear Algebra</title><link>https://freshrimpsushi.github.io/en/posts/3046/</link><pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3046/</guid><description>Theorem1 If $W$ is a subspace of a finite-dimensional inner product space $V$, then every $\mathbf{u} \in V$ is uniquely represented by the following formula. $$ \begin{equation} \mathbf{u} = \mathbf{w}_{1} + \mathbf{w}_{2} \end{equation} $$ Here, $\mathbf{w}_{1} \in W$ and $\mathbf{w}_{2} \in W^{\perp}$ apply. Explanation The notations $\mathbf{w}_{1}$ and $\mathbf{w}_{2}$ in the theorem are also marked as follows. $$ \mathbf{w}_{1} = \mathrm{proj}_{W} \mathbf{u} \quad \text{and} \quad \mathbf{w}_{2} = \mathrm{proj}_{W^{\perp}} \mathbf{u} $$</description></item><item><title>Perceptron Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/3023/</link><pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3023/</guid><description>Let&amp;rsquo;s say we have a training set that is linearly separable as represented by $X^{+}$, $X^{-}$. Consider $y$ as the following labels. $$ y_{i} = \pm 1\ (\mathbf{x}_{i} \in X^{\pm}) $$ Suppose the entire training set $X = X^{+} \cup X^{-}$ has $N$ data points. Then, let&amp;rsquo;s say we insert input values in the following order. $$ \mathbf{x}(1), \mathbf{x}(2), \cdots \mathbf{x}(N), \mathbf{x}(1), \mathbf{x}(2), \cdots \mathbf{x}(N),\mathbf{x}(1), \mathbf{x}(2), \cdots $$ That is,</description></item><item><title>Basic Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3022/</link><pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3022/</guid><description>Definition[^1] If two matrices $A$ and $B$ can be derived from each other through basic row operations, these matrices are said to be row equivalent. A matrix that can be obtained by performing a single basic row operation on an identity matrix is called an elementary matrix, generally denoted by $E$.</description></item><item><title>Various Meanings of the Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/3039/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3039/</guid><description>The Fourier transform is widely treated across various fields such as mathematics, physics, and engineering, and thus it comes to have different meanings depending on the perspective from which it is viewed. Here, its meanings in the context of mathematics, quantum mechanics, and signal processing are introduced. Let&amp;rsquo;s first define the Fourier transform and inverse transform, as they are defined in various forms in this document. $$ \hat{f}(\xi) := \int_{-\infty}^{\infty}</description></item><item><title>Definition of Heat in Physics</title><link>https://freshrimpsushi.github.io/en/posts/3038/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3038/</guid><description>Definition1 The energy transferred between two interacting systems is defined as heat. Description To understand the definition of heat, it is useful to recall the concept of work. Consider a situation where a person applies a force to move an object at rest. When the object moves, it is said that the force has done work on the object. From the object&amp;rsquo;s perspective, it has gained kinetic energy, which can</description></item><item><title>Simultaneous Homogeneous Linear Equations</title><link>https://freshrimpsushi.github.io/en/posts/3020/</link><pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3020/</guid><description>Definition1 In a linear system, if the constant terms are all $0$, it is called homogeneous. $$ \begin{align*} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= 0 \\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= 0 \\ &amp;amp;\vdots \\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= 0 \end{align*} $$ Unlike general linear systems, every homogeneous linear system always has a solution because if the constant terms are $0$,</description></item><item><title>Wirtinger Derivatives of Complex Functions</title><link>https://freshrimpsushi.github.io/en/posts/3035/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3035/</guid><description>Buildup Let&amp;rsquo;s assume the complex function $f : \mathbb{C} \to \mathbb{C}$ is given. The complex number $z=x+iy$, being a linear combination of two real numbers $x,y \in \mathbb{R}$, allows us to consider the function $f$ as a function of two real variables. Moreover, using two real functions $u,v : \mathbb{R}^{2} \to \mathbb{R}$, the value of function $f$ can be divided into real and imaginary parts as follows. $$ f(z) =</description></item><item><title>Translation of Vector Field Translation:</title><link>https://freshrimpsushi.github.io/en/posts/1632/</link><pubDate>Wed, 24 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1632/</guid><description>Theorem Let&amp;rsquo;s call a regular curve on the surface $C^{2}$ through $\boldsymbol{\alpha} (t)$. Let $\tilde{\mathbf{X}} = (\tilde{X}^{1}, \tilde{X}^{2})$ be a vector tangent to $M$ at point $\boldsymbol{\alpha}(t_{0})$. Then, there exists a unique vector field $\mathbf{X}(t)$ parallel to $\boldsymbol{\alpha}(t)$ that satisfies $\mathbf{X}(t_{0}) =\tilde{\mathbf{X}}$. Definition The unique vector field $X(t)$ is called the parallel translate of $\tilde{X}$ along $\alpha$. Proof Let $\mathbf{x}$ be the coordinate chart mapping for $\boldsymbol{\alpha}(t_{0})$. It can be</description></item><item><title>Gaussian-Jordan Elimination</title><link>https://freshrimpsushi.github.io/en/posts/3019/</link><pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3019/</guid><description>Definition1 An augmented matrix is said to be in echelon form if it satisfies the following conditions: In rows that have a non-zero element, the first non-zero number is a 1, referred to as the leading 1. Rows where all elements are zero are placed at the bottom. For consecutive rows that contain non-zero elements, the leading 1 in the upper row must be to the left of the leading</description></item><item><title>Basis of Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3017/</link><pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3017/</guid><description>Definition1 Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$ be a subset of vector space $V$. If $S$ satisfies the following two conditions, then $S$ is called a basis of $V$. $S$ spans $V$. $$ V = \text{span}(S) $$ $S$ is linearly independent. Explanation As the name suggests, the concept of a basis corresponds to &amp;rsquo;the smallest thing that can create a vector space&amp;rsquo;. The condition of spanning has</description></item><item><title>Matrix Transformation</title><link>https://freshrimpsushi.github.io/en/posts/3025/</link><pubDate>Wed, 17 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3025/</guid><description>Definition A function from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$ is called a matrix transformation with respect to the matrix $m \times n$ $A$ if it maps as follows, and is denoted as $T_{A} : \mathbb{R}^{n} \to \mathbb{R}^{m}$. $$ \mathbf{w} = T_{A} (\mathbf{x}) = A\mathbf{x}\quad \left( \mathbf{x} \in \mathbb{R}^{n}, \mathbf{w} \in \mathbb{R}^{m} \right) $$ It can also be represented as $\mathbf{x} \overset{T_{A}}{\to} \mathbf{w}$. This mapping can be represented in matrix form as follows.</description></item><item><title>Matrix Similarity</title><link>https://freshrimpsushi.github.io/en/posts/3016/</link><pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3016/</guid><description>Definition1 A square matrix $A$, $B$ and an invertible matrix $P$ are said to be $B$ is similar to $A$ if the following equation holds. $$ B = P^{-1} A P $$ Description The reason why it is called similar is because similar matrices share many important properties. This is called similarity invariant or invariant under similarity. Conjugate When the given equation is expressed for $B$, $$ B = P^{-1}</description></item><item><title>Various Function Spaces</title><link>https://freshrimpsushi.github.io/en/posts/3032/</link><pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3032/</guid><description>Definition A set of functions $X$ is called a function space if it forms a vector space. Explanation In the function space $X$, the inner product is defined by integration as follows. $$ \langle f, g \rangle = \int f(x) g(x) dx,\quad f,g\in X $$ The main function spaces considered include the following. Space of continuous functions $C^{m}$ $$ C^{m}(\mathbb{R}) : =\left\{ f \in C(\mathbb{R}) : f^{(n)} \text{ is continuous</description></item><item><title>Referencing Equations in TeX (Hyperlink)</title><link>https://freshrimpsushi.github.io/en/posts/3201/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3201/</guid><description>Code \label{} and \eqref{} can be used to reference equations. It is also possible with \ref{}, but the difference from \eqref{} is that it does not automatically put parentheses. $$ \begin{equation} (a+b)^{2} = a^{2} + 2ab + b^{2} \label{a} \end{equation} $$ 곱셈공식 $\eqref{a}$를 적용하면, The code like $\TeX$ above is displayed as</description></item><item><title>Partial Derivatives</title><link>https://freshrimpsushi.github.io/en/posts/3036/</link><pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3036/</guid><description>Definitions1 Let us define $E\subset \mathbb{R}^{n}$ as an open set, and $\mathbf{x}\in E$, and $\mathbf{f} : E \to \mathbb{R}^{m}$. Let $\left\{ \mathbf{e}_{1}, \mathbf{e}_{2}, \dots, \mathbf{e}_{n} \right\}$, and $\left\{ \mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{m} \right\}$ be the standard basis of $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$, respectively. Then, the components $f_{i} : \mathbb{R}^{n} \to \mathbb{R}$ of $\mathbf{f}$ are defined as follows. $$ \mathbf{f} (\mathbf{x}) = \sum_{i=1}^{m} f_{i}(\mathbf{x})\mathbf{u}_{i}, \quad \mathbf{x} \in E $$ or $$ f_{i}</description></item><item><title>Linear Forms</title><link>https://freshrimpsushi.github.io/en/posts/1734/</link><pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1734/</guid><description>Definition Let $V$ be a $n$dimensional vector space. For a given constant $a_{i} \in \mathbb{R}(\text{or } \mathbb{C})$, the following linear transformation $A : V \to \mathbb{R}(\text{or } \mathbb{C})$ is called a linear form. $$ A(\mathbf{x}) := \sum\limits_{i=1}^{n} a_{i}x_{i} $$ In this case, $\mathbf{x} = \begin{bmatrix} x_{1} &amp;amp; \cdots &amp;amp; x_{n} \end{bmatrix}^{T}$. Generalization For a given inner product space $(V, \left&amp;lt; \cdot, \cdot \right&amp;gt;)$ and $\mathbf{a} \in V$, the following linear</description></item><item><title>Properties of Rotational Surfaces</title><link>https://freshrimpsushi.github.io/en/posts/3042/</link><pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3042/</guid><description>Overview Consider a surface obtained by rotating the curve $\boldsymbol{\alpha}(t) = \left( r(t), z(t) \right)$ about the $z-$ axis, denoted $\mathbf{x}$. $$ \mathbf{x}(t, \theta) = \left( r(t)\cos \theta, r(t)\sin \theta, z(t) \right) $$ This document discusses various properties of the rotational surface. Properties For better readability, let&amp;rsquo;s denote $r = r(t)$, $z = z(t)$ as follows: Partial Derivations $$ \begin{align*} \mathbf{x}_{1} &amp;amp;= \mathbf{x}_{t} = \left( \dot{r}\cos\theta, \dot{r}\sin\theta, \dot{z} \right) \\</description></item><item><title>Properties of Determinants</title><link>https://freshrimpsushi.github.io/en/posts/3015/</link><pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3015/</guid><description>Properties Let $A,B$ be a $n\times n$ matrix and $k$ be a constant. The determinant satisfies the following properties: (a) $\det(kA) = k^{n}\det(A)$ (b) $\det(AB) = \det(A)\det(B)$ (c) $\det(AB)=\det(BA)$ (d) If $A$ is an invertible matrix, then $\det(A^{-1}) = \dfrac{1}{\det(A)}$ (e) $\det(A^{T}) = \det(A)$. Here, $A^{T}$ is the transpose of $A$.</description></item><item><title>Classification of Surfaces of Revolution According to Gaussian Curvature</title><link>https://freshrimpsushi.github.io/en/posts/3034/</link><pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3034/</guid><description>Overview1 Rotational surfaces are classified into three types according to the sign of the Gaussian curvature. Within each classification, surfaces with the same curvature share the same local intrinsic characteristics, even though they might have different global, extrinsic properties. In other words, they are locally isometric. Richard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p153-154&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Augmented Matrices and Elementary Row Operations</title><link>https://freshrimpsushi.github.io/en/posts/3014/</link><pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3014/</guid><description>Definition1 Let&amp;rsquo;s assume a linear system is given as follows. $$ \begin{equation} \begin{aligned} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= b_{1}\\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= b_{2}\\ &amp;amp;\vdots\\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= b_{m} \end{aligned} \end{equation} $$ The representation of constants of a linear system in a matrix is called an augmented matrix. $$ \begin{equation} \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}</description></item><item><title>Area of a Surface in Differential Geometry</title><link>https://freshrimpsushi.github.io/en/posts/3033/</link><pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3033/</guid><description>Definition1 Let&amp;rsquo;s say $\mathbf{x} : U \to \mathbb{R}^{3}$ is the coordinate chart mapping of a surface. The area of any region $\mathscr{R} \subset \mathbf{x}(U)$ on the surface is defined as follows. $$ \begin{align*} A(\mathscr{R}) &amp;amp;:= \int\int_{\mathbf{x}^{-1}(\mathscr{R})} [\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{n}] du^{1}du^{2} \\ &amp;amp;= \int\int_{\mathbf{x}^{-1}(\mathscr{R})} \sqrt{g} du^{1}du^{2} \end{align*} $$ Here, $(u^{1}, u^{2})$ are the coordinates of $U$, $\mathbf{x}_{i} = \dfrac{\partial \mathbf{x}}{\partial u^{i}}$ is the partial derivative of the $i$-th coordinate, $[\mathbf{x}_{1}, \mathbf{x}_{2},</description></item><item><title>Simultaneous Linear Equations</title><link>https://freshrimpsushi.github.io/en/posts/3013/</link><pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3013/</guid><description>Definition1 For constants $a_{1}$, $a_{2}$, $\dots$, $a_{n}$, $b$, we define a linear equation for variables $x_{1}$, $x_{2}$, $\dots$, $x_{n}$ as follows. $$ \begin{equation} a_{1}x_{1} + a_{2}x_{2} + \cdots + a_{n}x_{n} = b \label{lineq} \end{equation} $$ At least one of $a$ is not $0$. In other words, not &amp;ldquo;all $a$ are $0$&amp;rdquo;. A finite set of linear equations is called a system of linear equations or simply a linear system, and</description></item><item><title>Linear Function</title><link>https://freshrimpsushi.github.io/en/posts/3037/</link><pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3037/</guid><description>Definition A function $f : X \to Y$ is called linear if it satisfies the following two conditions for $x,x_{1},x_{2}\in X$ and $a \in \mathbb{R}$, $f(ax) = af(x)$ $f(x_{1} + x_{2}) = f(x_{1}) + f(x_{2})$ Explanation If it is not linear, it is called nonlinear. The two conditions are sometimes combined as follows $$ f(ax_{1} + x_{2}) = af(x_{1}) + f(x_{2}) $$ If in 2., instead of being equal, it</description></item><item><title>Equivalents Codes in Julia, MATLAB, Python, and R</title><link>https://freshrimpsushi.github.io/en/posts/3031/</link><pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3031/</guid><description>Overview Organizing code that performs the same function in Julia, Matlab, Python, and R. Flux-PyTorch-TensorFlow Cheat Sheet Let&amp;rsquo;s assume the following environment for Python. import numpy as np General Julia Matlab Python R comment #comment %comment #comment #comment 2d grid X = kron(x, ones(size(y)))Y = kron(ones(size(x)), y) [X,Y] = meshgrid(x,y) np.meshgrid(x,y) How to make an n-dimensional meshgrid in Julia Type 줄리아Julia 매트</description></item><item><title>Unitary Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3008/</link><pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3008/</guid><description>Definition Unitary Matrix Let $A$ be a square complex matrix. $A$ is called a unitary matrix if it satisfies the following equation: $$ A^{-1}=A^{\ast} $$ Here, $A^{-1}$ is the inverse of $A$, $A^{\ast}$ is the conjugate transpose of $A$. Unitary Diagonalization1 Consider a square matrix $A$ of size $n \times n$. $A$ is said to be unitarily diagonalizable if it satisfies the following equation for some diagonal matrix $D$ and</description></item><item><title>Hermitian Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3007/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3007/</guid><description>Definition Let $A$ be a square complex matrix. If $A$ satisfies the following equation, it is called a Hermitian matrix or self-adjoint matrix. $$ A^{\ast}=A $$ Here, $A^{\ast}$ is the conjugate transpose of $A$. If $A$ satisfies the following equation, it is called a skew-Hermitian matrix . $$ A^{\ast}=-A $$ Explanation If it is a real matrix, since $A^{\ast}=A^{T}$, if it is a symmetric matrix, it is a Hermitian matrix.</description></item><item><title>Equivalence Conditions for Orthogonal Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3012/</link><pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3012/</guid><description>Theorem For a real matrix $A$, the following propositions are all equivalent. (a) $A$ is an orthogonal matrix. (b) The set of row vectors of $A$ forms a normal orthogonal set in $\mathbb{R}^n$. (c) The set of column vectors of $A$ forms a normal orthogonal set in $\mathbb{R}^n$. (d) $A$ preserves inner product, i.e., for all $\mathbf{x},\mathbf{y}\in \mathbb{R}^{n}$, the following holds: $$ (A \mathbf{x}) \cdot (A\mathbf{y}) = \mathbf{x} \cdot \mathbf{y}</description></item><item><title>대각합</title><link>https://freshrimpsushi.github.io/en/posts/1924/</link><pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1924/</guid><description>Definition Let the $n\times n$ matrix $A$ be given as follows. $$ A= \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{n1} &amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{bmatrix} $$ The sum of the diagonal entries of $A$ is defined to be the trace of $A$ and is denoted as follows. $$ \text{tr}(A)=\text{Tr}(A)=a_{11}+a_{22}+\cdots</description></item><item><title>Properties of Orthogonal Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3010/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3010/</guid><description>Properties1 An orthogonal matrix has the following properties: (a) The transpose of an orthogonal matrix is also an orthogonal matrix. (b) The inverse of an orthogonal matrix is an orthogonal matrix. (c) The product of two orthogonal matrices is an orthogonal matrix. (d) The determinant of an orthogonal matrix is either $1$ or $-1$. $$ \det(A)=\pm 1 $$ Proof (a) Let&amp;rsquo;s say $A$ is an orthogonal matrix. Let&amp;rsquo;s say $B$</description></item><item><title>What is Reinforcement Learning in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/3029/</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3029/</guid><description>Definition Reinforcement learning is the process where an agent interacts with the environment to find a policy that maximizes the cumulative reward. Description1 The elements comprising reinforcement learning are as follows: Agent: Decides actions based on a policy, given a state. State: Refers to the situation in which the agent is placed. Action: Refers to the choices available to the agent in a given state. Policy: Refers to the strategy</description></item><item><title>Orthogonal Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3009/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3009/</guid><description>Definition Let $A$ be a square real matrix. $A$ is called an orthogonal matrix if it satisfies the following equation: $$ A^{-1} = A^{\mathsf{T}} $$ Another way to express this condition is as follows: $$ AA^{\mathsf{T}} = A^{\mathsf{T}}A =I $$ Explanation To put the definition in words, an orthogonal matrix is a matrix whose row vectors or column vectors are orthogonal unit vectors to each other. When extended to complex</description></item><item><title>Inner Product with Vector</title><link>https://freshrimpsushi.github.io/en/posts/3011/</link><pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3011/</guid><description>Definition: Inner Product of Two Column Vectors1 The inner product of two column vectors of size $n \times 1$, $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{R}^{n}$ is defined as follows. $$ \begin{equation} \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{T}\mathbf{v}=u_{1}v_{1} + u_{2}v_{2} + \cdots + u_{n}v_{n} \label{EuclideanIP} \end{equation} $$ In the case where $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{C}^{n}$, it is as follows. $$ \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{\ast}\mathbf{v}=u^{\ast}_{1}v_{1}^{\ } + u_{2}^{\ast}v_{2}^{\ } + \cdots + u_{n}^{\ast}v_{n}^{\ }</description></item><item><title>Conjugate Transpose Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3006/</link><pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3006/</guid><description>Definition Let $A$ be a complex matrix of size $m \times n $. Define $\overline{A}$ as follows, and call it the conjugate matrix of $A$. $$ \overline{A} :=\begin{bmatrix} \overline{a_{11}} &amp;amp; \overline{a_{12}} &amp;amp; \cdots &amp;amp; \overline{a_{1n}} \\ \overline{a_{21}} &amp;amp; \overline{a_{22}} &amp;amp; \cdots &amp;amp; \overline{a_{2n}} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \overline{a_{m1}} &amp;amp; \overline{a_{m2}} &amp;amp; \cdots &amp;amp; \overline{a_{mn}} \end{bmatrix} = \left[ \overline{a_{ij}} \right] $$ Here, $\overline{a}$ is the conjugate</description></item><item><title>Symmetric Matrices, Skew-Symmetric Matrices</title><link>https://freshrimpsushi.github.io/en/posts/3005/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3005/</guid><description>Definition1 A square matrix $A$ is called a symmetric matrix if it satisfies the following equation: $$ A=A^{T} $$ Here, $A^{T}$ is the transpose of $A$. $A$ is called an anti-symmetric matrix if it satisfies the following equation: $$ A =-A^{T} $$ Explanation By the definition of the transpose, matrices that are not square cannot be symmetric or anti-symmetric. If $A$ is an anti-symmetric matrix, it follows from the definition</description></item><item><title>Conditions for a Matrix Being Invertible</title><link>https://freshrimpsushi.github.io/en/posts/3004/</link><pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3004/</guid><description>Theorem1 Let $A$ be a square matrix of size $n\times n$. Then the following propositions are all equivalent. (a) $A$ is an invertible matrix. (b) The homogeneous linear system $A\mathbf{x}=\mathbf{0}$ has only the trivial solution. (c) The reduced row echelon form of $A$ is $I_{n}$. (d) $A$ can be expressed as a product of elementary matrices. (e) $A\mathbf{x}=\mathbf{b}$ has a solution for all $n\times 1$ matrices $\mathbf{b}$. (f) $A\mathbf{x}=\mathbf{b}$ has</description></item><item><title>Inverse Matrix, Reversible Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3003/</link><pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3003/</guid><description>Definition Let $A$ be an arbitrary square matrix of size $n\times n$. A matrix $L$ is called the left inverse matrix of $A$ if it satisfies the following equation with $A$ in a matrix multiplication. $$ LA=I_{n} $$ Here, $I_{n}$ is the identity matrix of size $n\times n$. A matrix $R$ that is capable of matrix multiplication with $A$ and satisfies the following equation is called the right inverse matrix</description></item><item><title>Transpose Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3002/</link><pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3002/</guid><description>Definition1 Let&amp;rsquo;s consider a matrix of size $m\times n$ as $A$. The matrix obtained by swapping the rows and columns of $A$ is called the transpose of $A$ and is denoted by $A^{\mathsf{T}}$ or $A^{T}$, $A^{t}$. Description Following the definition, if $A$ is a $m \times n$ matrix then $A^{\mathsf{T}}$ will be a $n \times m$ matrix. Also, the $i$th row of $A$ is the same as the $i$th column</description></item><item><title>Identity Matrix, Unit Matrix</title><link>https://freshrimpsushi.github.io/en/posts/3001/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3001/</guid><description>Definition A diagonal matrix of size $n\times n$ with all diagonal elements being $1$ is called an identity matrix or unit matrix, denoted as $I_{n}$ or $I_{n\times n}$. $$ I_{n\times n}= \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \end{bmatrix} $$ Description The identity matrix is</description></item><item><title>Diagonal Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1958/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1958/</guid><description>Diagonal Matrix1 Let&amp;rsquo;s consider a matrix $A$ of size $n\times m$. The elements whose row and column numbers are the same, that is, $a_{ii} (1 \le i \le \min(n,m))$, are called the main diagonal elements. The imaginary line connecting the main diagonal elements is referred to as the main diagonal, or principal diagonal. A matrix $A$, in which all elements except for the main diagonal elements are $0$, is called</description></item><item><title>Square Matrix</title><link>https://freshrimpsushi.github.io/en/posts/1956/</link><pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1956/</guid><description>Definition A matrix $A$ is called a square matrix if the number of rows and columns of the matrix $A$ are equal. Explanation It is also referred to as a regular square matrix. Square matrices are easy to handle and possess various favourable properties. Examples Identity matrix Invertible matrix Elementary matrix Symmetric matrix Orthogonal matrix Hermitian matrix Unitary matrix</description></item><item><title>Matrix Operations: Scalar Multiplication, Addition, and Multiplication</title><link>https://freshrimpsushi.github.io/en/posts/1957/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1957/</guid><description>Scalar Multiplication The multiplication of an arbitrary matrix $A$ of size $m \times n$ by a scalar $k$ is defined as multiplying each element of $A$ by $k$ and is denoted as follows: $$ kA = k\begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} :=</description></item><item><title>How to Print and Save a 2D Array as a Heatmap Image in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1948/</link><pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1948/</guid><description>Imagesc imagesc function allows you to display a 2D array as a heatmap. colorbar is a setting that outputs a color bar indicating the scale. N=2^8; p=phantom(&amp;#39;Modified Shepp-Logan&amp;#39;,N); figure() imagesc(p) colorbar Saving Method 1 You can use the saveas function to save the figure displayed above. The setting gcf refers to the current figure. Then, the picture below is saved. N=2^8; p=phantom(&amp;#39;Modified Shepp-Logan&amp;#39;,N); figure() imagesc(p) colorbar saveas(gcf,&amp;#39;phantom.png&amp;#39;) Method 2 You</description></item><item><title>code summary</title><link>https://freshrimpsushi.github.io/en/posts/10/</link><pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/10/</guid><description>matplotlib Image Size Create a figure with the specified size: plt.figure(figsize=(w,h)) Default size: plt.figure(figsize=(6.4,4.8)) Adjust default plot size: import matplotlib plt.rcParams[&amp;#39;figure.figsize&amp;#39;] = [width, height] tmux tmux attach -t 0: Attach to session 0 Ctrl + b + Arrow Keys: Switch to the session in the specified direction Ctrl + b + Number: Switch to the session with the specified number Ctrl + b + d: Return to shell while keeping</description></item><item><title>Matrix Definitions</title><link>https://freshrimpsushi.github.io/en/posts/1955/</link><pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1955/</guid><description>Definition1 A matrix is an arrangement of numbers in the shape of a rectangle as follows: $$ A=\begin{bmatrix} 10 &amp;amp; 0 &amp;amp; 3 \\ 0 &amp;amp; 8 &amp;amp; 22 \end{bmatrix} $$ Each of the arranged numbers is called an entry or element. A horizontal line is called a row, and a vertical line is called a column. Moreover, if a certain matrix has $m$ rows and $n$ columns, its size</description></item><item><title>Product Rule Involving the Del Operator</title><link>https://freshrimpsushi.github.io/en/posts/93/</link><pubDate>Thu, 07 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/93/</guid><description>Formulas Let&amp;rsquo;s call $f=f(x,y,z)$ a scalar function. Let&amp;rsquo;s call $\mathbf{A} = A_{x}\hat{\mathbf{x}} + A_{y}\hat{\mathbf{y}} + A_{z}\hat{\mathbf{z}}, \mathbf{B} = B_{x}\hat{\mathbf{x}} + B_{y}\hat{\mathbf{y}} + B_{z}\hat{\mathbf{z}}$ a vector function. Then, the following equations hold. Gradient (a) $\nabla{(fg)}=f\nabla{g}+g\nabla{f}$ (b) $\nabla(\mathbf{A} \cdot \mathbf{B}) = \mathbf{A} \times (\nabla \times \mathbf{B}) + \mathbf{B} \times (\nabla \times \mathbf{A})+(\mathbf{A} \cdot \nabla)\mathbf{B}+(\mathbf{B} \cdot \nabla) \mathbf{A}$ Divergence (c) $\nabla \cdot (f\mathbf{A}) = f(\nabla \cdot \mathbf{A}) + \mathbf{A} \cdot (\nabla f)$ (d)</description></item><item><title>Radon Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1945/</link><pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1945/</guid><description>Definition Let&amp;rsquo;s assume that a function $f :D \to \mathbb{R}$ is defined on some 2D domain $D\subset \mathbb{R}^{2}$. The Radon transform $\mathcal{R}f$ of $f$ is defined as follows, for $s \in \mathbb{R}$, $\boldsymbol{\theta} = (\cos \theta, \sin \theta) \in S^{1}$, $$ \begin{align*} \mathcal{R} f(s, \boldsymbol{\theta}):=&amp;amp;\ \int \limits_{t=-\infty}^{\infty} f ( s \boldsymbol{\theta} + t \boldsymbol{\theta}^{\perp} )dt \\ =&amp;amp;\ \int \limits_{t=-\infty} ^{\infty} f \left( s\cos\theta-t\sin\theta, s\sin\theta + t\cos\theta \right)dt \end{align*} $$ Explanation</description></item><item><title>Solution of Differential Equations Using Series Solutions</title><link>https://freshrimpsushi.github.io/en/posts/888/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/888/</guid><description>Description Differential equations with constant coefficients can be relatively easily solved using methods such as separation of variables or integrating factor method. However, differential equations with coefficients that include the independent variable, as shown below, cannot be easily solved. $$ \begin{equation} P(x)\dfrac{d^2 y}{dx^2} + Q(x)\dfrac{dy}{dx}+R(x)y=0 \label{1}\end{equation} $$ Here, $P$, $Q$, and $R$ are assumed to be polynomials without common factors. Equations of the above form include Bessel&amp;rsquo;s equation $$ x^2</description></item><item><title>Generalized Fourier Coefficients and Fourier Series in Hilbert Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1913/</link><pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1913/</guid><description>Definition[^1] Let $H$ be a Hilbert space, and let $\left\{ u_{\alpha} \right\}_{\alpha\in A}$ be a normal orthogonal system in $H$. Then, for a fixed $x\in H$, let&amp;rsquo;s define the complex function $\hat{x} :A\to \mathbb{C}$ as follows. $$ \hat{x}(\alpha)=\left\langle x,u_{\alpha} \right\rangle $$ The values above are referred to as the Fourier coefficients of $x$ with respect to $\left\{ u_{\alpha} \right\}$.</description></item><item><title>Inner Product is a Continuous Mapping</title><link>https://freshrimpsushi.github.io/en/posts/1916/</link><pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1916/</guid><description>Theorem1 Let&amp;rsquo;s assume $\left( X, \left\langle \cdot,\cdot \right\rangle \right)$ is an inner product space and $\left\{ \mathbf{x}_{n} \right\}$, $\left\{ \mathbf{y}_{n} \right\}$ are sequences in $X$ converging to $\mathbf{x}$ and $\mathbf{y}$, respectively. Then, the following holds. $$ \left\langle \mathbf{x}_{n},\mathbf{y}_{n} \right\rangle \to \left\langle \mathbf{x},\mathbf{y} \right\rangle \text{ as } n \to \infty $$ Since the limit can move inside and outside of the inner product, we obtain the following corollary. Corollary Assuming that</description></item><item><title>Derivatives of Vectors and Matrices</title><link>https://freshrimpsushi.github.io/en/posts/1926/</link><pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1926/</guid><description>Gradient of a Scalar Function Scalar function $f : \mathbb{R}^{n} \to \mathbb{R}$&amp;rsquo;s gradient is as follows. $$ \frac{ \partial f(\mathbf{x})}{ \partial \mathbf{x} } := \nabla f(\mathbf{x}) = \begin{bmatrix} \dfrac{ \partial f(\mathbf{x})}{ \partial x_{1} } &amp;amp; \dfrac{ \partial f(\mathbf{x})}{ \partial x_{2} } &amp;amp; \cdots &amp;amp; \dfrac{ \partial f(\mathbf{x})}{ \partial x_{n} } \end{bmatrix}^{T} $$ Here, $\dfrac{ \partial f(\mathbf{x})}{ \partial x_{i} }$ is the partial derivative of $f$ with respect to $x_{i}$. Inner</description></item><item><title>Properties of Adjoint Operators</title><link>https://freshrimpsushi.github.io/en/posts/1919/</link><pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1919/</guid><description>Theorem1 Let&amp;rsquo;s call $H,K$ a Hilbert space. For a bounded linear operator $T : K \to H$, $T^{\ast} : H \to K$ is called the adjoint operator of $T$ if it satisfies the following. $$ \left\langle T \textbf{v} , \textbf{w} \right\rangle_{H} = \left\langle \textbf{v} , T^{\ast} \textbf{w} \right\rangle_{K},\quad \forall \textbf{v} \in K $$ The adjoint operator has the following properties. (a) $T^{\ast}$ is linear and bounded. (b) $\left( T^{\ast} \right)^{\ast}</description></item><item><title>Convex Sets in Vector Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1914/</link><pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1914/</guid><description>Definition A subset $M$ of a vector space $V$ is called a convex set if the following equation holds: $$ \lambda x +(1-\lambda)y \in M,\quad \forall \lambda\in[0,1],\ \forall x,y \in M $$ Description Verbally, this equation means &amp;quot;$M$ is a convex set implies that every vector lying between any two vectors in $M$ also belongs to $M$&amp;quot;. Also, if $M$ is a subspace, it is closed under addition and scalar</description></item><item><title>Norm Space에서의 Infinite Series Span Total Sequence</title><link>https://freshrimpsushi.github.io/en/posts/1918/</link><pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1918/</guid><description>Infinite Series1 Definition Let $(X, \left\| \cdot \right\|)$ be a normed space. For a sequence $X$ of $\left\{ \mathbf{x}_{k}\right\}_{k\in \mathbb{N}}$, let&amp;rsquo;s define the partial sum as follows. $$ \mathbf{S}_{N} := \sum \limits_{k=1}^{N}\mathbf{x}_{k} $$ If the limit of the partial sum $\mathbf{S}_{N}$ is $\mathbf{x} \in X$, i.e., if it satisfies the following equation $$ \lim \limits_{N\to \infty}\left\| \mathbf{x}-\sum \limits_{k=1}^{N}\mathbf{x}_{k} \right\|=0 $$ then the infinite series $\sum_{k=1}^{\infty}\mathbf{x}_{k}$ is said to converge to</description></item><item><title>Properties of Zero in Inner Product Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1917/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1917/</guid><description>Theorem Let&amp;rsquo;s call $\left( X, \left\langle \cdot,\cdot \right\rangle \right)$ an inner space. (a) For all $\mathbf{x}\in X$, the following holds: $$ \left\langle \mathbf{0},\mathbf{x} \right\rangle = 0 $$ (b) For all $\mathbf{x}\in X$, there exists only one element in $X$ that satisfies the following equation: $$ \forall \mathbf{x}\in X,\ \left\langle \mathbf{x},\mathbf{y} \right\rangle = 0 \implies \mathbf{y}=\mathbf{0} $$ (c) Let&amp;rsquo;s call it $\mathbf{y}, \mathbf{\mathbf{z}} \in X$. And $$ \begin{equation} \left\langle \mathbf{x},\mathbf{y} \right\rangle</description></item><item><title>B-spline Scaling Equation</title><link>https://freshrimpsushi.github.io/en/posts/1910/</link><pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1910/</guid><description>Formulas1 (a) B-spline scaling equation: For a B-spline of order $m\in N$, the following equation holds: $$ \widehat{N_{m}}(2\gamma)=H_{0}(\gamma)\widehat{N_{m}}(\gamma),\quad \forall \gamma \in \mathbb{R} $$ Where $H_{0}$ is a function with period $1$, which is described as follows: $$ H_{0}(\gamma)=\left( \frac{1+e^{-2\pi i \gamma}}{2} \right)^{m} $$ Also, the definition of the Fourier transform of $f$, $\widehat{f}$, is as follows: $$ \widehat{f}(\gamma):=\int _{-\infty} ^{\infty} f(x)e^{-2\pi i x \gamma}dx $$ (b) Central B-spline scaling equation:</description></item><item><title>Central B-spline</title><link>https://freshrimpsushi.github.io/en/posts/1909/</link><pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1909/</guid><description>Definition1 For $m\in \mathbb{N}$, the centered B-spline $B_{m}$ is defined as follows: $$ B_{m}(x):= T_{-\frac{m}{2}}N_{m}(x)=N_{m}(x+{\textstyle \frac{1}{2}}) $$ Here, $T$ is the translation of the space $L^{2}$ space. Description It can also be defined as follows: $$ B_{1}:= \chi_{[-1/2,1/2]},\quad B_{m+1}:=B_{m}*B_{1},\ m\in\mathbb{N} $$ Both definitions actually mean the same function. The key point here is that $B_{m}$ is defined to be an even function. As with the B-spline, it is easy to</description></item><item><title>Regularity of B-Splines</title><link>https://freshrimpsushi.github.io/en/posts/1908/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1908/</guid><description>Theorem1 For $m=2,3,\dots$, the B-spline $N_{m}$ has the following properties. (a) $N_{m}\in C^{m-2}(\mathbb{R})$ (b) For $k\in \mathbb{Z}$, in each interval $[k,k+1]$, $N_{m}$ is at most a polynomial of degree $m-1$. Explicit formula of B-spline $$ N_{m}(x) = \frac{1}{(m-1)!}\sum \limits_{j=0}^{m} \left( -1 \right)^{j}\binom{m}{j}\left( x-j \right)_{+}^{m-1},\quad x\in \mathbb{R} $$ Where $$ f(x)_{+}:=\max \left( 0,f(x) \right) \quad \&amp;amp; \quad f(x)_{+}^{n}:=\left( f(x)_{+} \right)^{n} $$ Lemma For $m=2,3,\cdots$, $x_{+}^{m-1}$ is differentiable up to $m-2$ times,</description></item><item><title>Explicit Formulas of B-splines</title><link>https://freshrimpsushi.github.io/en/posts/1907/</link><pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1907/</guid><description>Formulas1 For the function $f: \mathbb{R}\to \mathbb{R}$, let&amp;rsquo;s say $$ f(x)_{+}:=\max \left( 0,f(x) \right) $$ That is, $f_{+}$ is a function that replaces all parts of $f$ where the function value is less than $0$, with $0$. Also, let&amp;rsquo;s define $$ f(x)_{+}^{n}:=\left( f(x)_{+} \right)^{n} $$ Then, for each $m=2,3,\dots$, the B-spline $N_{m}$ can be expressed as follows. $$ N_{m}(x) = \frac{1}{(m-1)!}\sum \limits_{j=0}^{m} \left( -1 \right)^{j}\binom{m}{j}\left( x-j \right)_{+}^{m-1},\quad x\in \mathbb{R} $$</description></item><item><title>Fourier Transform of B-Splines</title><link>https://freshrimpsushi.github.io/en/posts/1906/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1906/</guid><description>The Equation1 The Fourier transform of a B-spline of order $m \in \mathbb{N}$ is given as follows. $$ \widehat{N_{m}}(\gamma)=\left( \frac{1-e^{-2\pi i\gamma}}{2\pi i \gamma} \right)^{m} $$ Here, the definition of the Fourier transform of $f$ is as follows. $$ \widehat{f}(\gamma):=\int _{-\infty} ^{\infty} f(x)e^{-2\pi i x\gamma}dx $$ Explanation Using the properties of B-splines, Fourier transforms, and convolutions, the calculation can be done without much difficulty. Proof First, computing the Fourier transform of</description></item><item><title>Orthogonality, Orthogonal Sets, and Orthonormal Sets in Inner Product Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1912/</link><pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1912/</guid><description>Definition1 Let $\left( X, \left\langle \cdot, \cdot \right\rangle \right)$ be an inner product space. If two elements $\mathbf{x}, \mathbf{y}\in X$ satisfy $\left\langle \mathbf{x}, \mathbf{y} \right\rangle =0$, then $\mathbf{y}$ and $\mathbf{x}$ are said to be orthogonal and denoted as follows. $$ \mathbf{x} \perp \mathbf{y} $$ If the set of elements $X$, $\left\{ \mathbf{x}_{k} \right\}_{k\in \mathbb{N}}$, satisfies the following equation, it is called an orthogonal system or an orthogonal set. $$ \left\langle</description></item><item><title>Properties of B-Splines</title><link>https://freshrimpsushi.github.io/en/posts/1904/</link><pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1904/</guid><description>Properties1 The B-spline of order $m\in \mathbb{N}$, denoted $N_{m}$, satisfies the following properties. (a) $\mathrm{supp}N_{m}=[0,m] \quad \text{and} \quad N_{m}(x)&amp;gt;0 \text{ for } x\in(0,m)$ (b) $\displaystyle \int _{-\infty} ^{\infty} N_{m}(x)dx=1$ (c) For $m\ge 2$, the following equation holds. $$ \begin{equation} \sum \limits_{k \in \mathbb{Z}} N_{m}(x-k)=1,\quad \forall x\in \mathbb{R} \end{equation} $$ (c&amp;rsquo;) When $m=1$, the above equation holds for $x\in \mathbb{R}\setminus \mathbb{Z}$. Explanation (c) In other words, $\left\{ N_{m}(x-k) \right\}_{k}$ means that</description></item><item><title>Properties of Convolution</title><link>https://freshrimpsushi.github.io/en/posts/1901/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1901/</guid><description>Theorem Convolution satisfies the following properties. (a) Commutative Law $$ f \ast g = g \ast f $$ (b) Distributive Law $$ f \ast (g+h) = f \ast g + f \ast h $$ (c) Associative Law $$ f \ast (g \ast h) = (f \ast g) \ast h $$ (d) Scalar Multiplication Associative Law $$ a(f \ast g)=(af\ast g)=(f\ast ag) $$ (e) Differentiation $$ (f\ast g)^{\prime}=f^{\prime}\ast g=f\ast g^{\prime} $$</description></item><item><title>Several Definitions and Notations of Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/1898/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1898/</guid><description>Overview The definition and notation of the Fourier transform vary depending on the needs and preferences of the author. Therefore, before dealing with the Fourier transform in textbooks, lectures, research papers, etc., it is common to clarify the definition and notation. If you skip over the definition thinking it is a concept you know, you might find the equations odd, so it is necessary to check carefully. Of course, the</description></item><item><title>Plancherel's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1899/</link><pubDate>Mon, 30 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1899/</guid><description>Theorem For all $f,g \in L^{2}$, the following equation holds. $$ \begin{align} \langle \hat{f},\hat{g} \rangle &amp;amp;= 2\pi \left\langle f,g \right\rangle \\[1em] \| \hat{f} \|_{2}^{2} &amp;amp;= 2\pi \| f \|_{2}^{2} \end{align} $$ Here $\hat{f}$ is the Fourier transform of $f$. Explanation If expressed in integral form, it is as follows. $$ \begin{align} \int \overline{f(x)}g(x)dx &amp;amp;= \dfrac{1}{2\pi} \int \overline{\hat{f}(\xi)} \hat{g}(\xi) d\xi \tag{1} \\[1em] \int \left| f(x) \right|^{2} dx &amp;amp;= \dfrac{1}{2\pi} \int |</description></item><item><title>Proof That the Space of Test Functions is a Proper Subset of the Schwartz Space</title><link>https://freshrimpsushi.github.io/en/posts/1896/</link><pubDate>Sat, 28 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1896/</guid><description>Theorem1 Let $\mathcal{D}$ be the space of test functions, and let $\mathcal{S}$ be the Schwartz space. Then the following equation holds. $$ \mathcal{D} \subsetneq \mathcal{S} $$ Proof Strategy: First, we show that all test functions belong to the Schwartz space, and then by providing an example of a Schwartz function that is not a test function, we prove the theorem. Schwartz Functions Define $\phi$ as a Schwartz function if it</description></item><item><title>Distributional Convolution Lemma</title><link>https://freshrimpsushi.github.io/en/posts/1894/</link><pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1894/</guid><description>Theorem1 Let $F$ be a distribution, and $\phi,\psi$ be a test function. Then $F \ast \phi$ is a function defined in the real space and is locally integrable. Therefore, there exists a corresponding regular distribution $T$ as follows: $$ T_{F \ast \phi}(\psi)=F(\tilde{\phi} \ast \psi) $$ Here, $\tilde{\phi}(x)=\phi (-x)$. Description The name &amp;lsquo;distribution convolution lemma&amp;rsquo; is arbitrarily given as there&amp;rsquo;s no specific name attached to the content above. Proof Case 1.</description></item><item><title>Convolution of Distributions, Distributions as Functions Defined on Real Numbers</title><link>https://freshrimpsushi.github.io/en/posts/1892/</link><pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1892/</guid><description>Buildup1 The goal of distribution theory is to rigorously define entities like the naively defined Dirac delta function in mathematical terms. As such, it becomes necessary to treat distributions, defined in the function space, as functions defined over the real number space. Initially, let&amp;rsquo;s consider how the differentiation, translation, etc., of distributions have been defined. Since the domain of a distribution is a function space, it is thought that actions</description></item><item><title>Necessary and Sufficient Condition for Uniform Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1891/</link><pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1891/</guid><description>Theorem 1 Let us suppose that a sequence of functions $\left\{ f_{n} \right\}$ defined on the metric space $E$ is given. The following two conditions are equivalent. $\left\{ f_{n} \right\}$ converges uniformly on $E$. For all $\varepsilon&amp;gt;0$, there exists a natural number $N$ such that the following equation holds. $$ \begin{equation} \quad m,n\ge N,\ x\in E \implies \left| f_{n}(x)-f_{m}(x) \right| \le \varepsilon \end{equation} $$ Explanation In other words, for all</description></item><item><title>Differentiability of Distributions is Continuous with Respect to Weak Convergence</title><link>https://freshrimpsushi.github.io/en/posts/1890/</link><pubDate>Sun, 22 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1890/</guid><description>Theorem1 The differentiation of a distribution is continuous with respect to weak convergence. In other words, if $T_{k}$ weakly converges to $T$, then $\partial ^{\alpha} T_{k}$ weakly converges to $\partial ^{\alpha}T$. $$ T_{k} \to T \quad \text{weakly} \implies \partial ^{\alpha} T_{k}\to \partial ^{\alpha}T \quad \text{weakly} $$ Here, $\alpha$ is any multi-index. Explanation It means that the differential operator of a distribution satisfies the condition for being continuous with respect to</description></item><item><title>Convergence of Distributions to the Dirac Delta Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1889/</link><pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1889/</guid><description>Theorem1 Let&amp;rsquo;s assume that $f$ is a function satisfying $\int_{\mathbb{R}^{n}} f(\mathbf{x})d\mathbf{x}=1$. And let $f_{\epsilon}(\mathbf{x})=\dfrac{ 1 }{ \epsilon^{n} }f\left( \dfrac{\mathbf{x}}{\epsilon} \right)$. Then, the corresponding regular antifunction $T_{\epsilon}=T_{f_{\epsilon}}$ of $f$ weakly converges to the Dirac delta antifunction. That is, the following holds. $$ \lim \limits_{\epsilon \to 0} T_{\epsilon}=\delta $$ Proof Let $\tilde{f}(\mathbf{x})=f(-\mathbf{x})$. Then, the following holds. $$ \tilde{f_{\epsilon}}(\mathbf{x})=\frac{1}{\epsilon}f\left( -\frac{\mathbf{x}}{\epsilon} \right) \quad \text{and} \quad \int_{\mathbb{R}^{n}}\tilde{f_{\epsilon}}d\mathbf{x}=1 $$ Furthermore, the test function $\phi$ is a</description></item><item><title>Linear Models for Regression in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/1887/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1887/</guid><description>Definition1 Simple Model Let&amp;rsquo;s define the target function $f : X \to Y$ between the data set $X = \left\{ \mathbf{x}_{i} \right\}$ and the label set $Y = \left\{ y_{i} \right\}$ as follows. $$ y_{i} = f(\mathbf{x}_{i}) $$ In machine learning, linear regression refers to finding a linear function $\hat{f}$ that satisfies the following equation for $\mathbf{w}$. $$ y_{i} \approx \hat{y}_{i} = \hat{f}(\mathbf{x}_{i}, \mathbf{w}) = w_{0} + w_{1}x_{1} + \cdots</description></item><item><title>Operations and Notation Table of Vectors and Matrices</title><link>https://freshrimpsushi.github.io/en/posts/1886/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1886/</guid><description>Overview This document summarizes various notations and operations for vectors and matrices. Vector Vectors are usually denoted in lower case bold type, and unless otherwise stated, they refer to a $n\times 1$ matrix, that is, a column vector. The $i$rd component of vector $\mathbf{x}$ is represented as $x_{i}$. $$ \mathbf{x}=\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix},\quad\mathbf{y}=\begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix} $$ Row vectors are</description></item><item><title>Inverse Fourier Transform Theorem for Smooth Functions</title><link>https://freshrimpsushi.github.io/en/posts/1885/</link><pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1885/</guid><description>Theorem Assuming that $f$ is integrable on $\mathbb{R} $ and piecewise smooth, then the following equation holds: $$ \lim \limits_{r\to \infty} \frac{1}{2\pi} \int_{-r}^{r}e^{i\xi x} \hat{f}(\xi)d\xi= \frac{1}{2}\big[f(x-)+f(x+) \big],\quad \forall x\in \mathbb{R} $$ Here, $f(x+)$ and $f(x-)$ are respectively the right-hand limit and left-hand limit of $f$ at $x$. Description The inverse Fourier transform theorem used a cutoff function instead of requiring a relatively weak condition for $f$. The theorem above is</description></item><item><title>Convolution Norm Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1883/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1883/</guid><description>Theorem Let the function $g \in L^{1}$ be bounded and satisfy $\int_{\mathbb{R}}g(y)dy=1$. Assuming $f\in L^{2}$ and that the convolution $f \ast g$ of $f$ and $g$ is well-defined for all $x\in \mathbb{R}$, then $f \ast g_{\epsilon}$ converges in norm to $f$. $$ \begin{equation} \lim \limits_{\epsilon \to 0} \left\| f \ast g_{\epsilon} -f \right\| = 0 \end{equation} $$ In this case, $g_{\epsilon}(y)=\frac{1}{\epsilon}g \left( \frac{y}{\epsilon} \right)$. The name &amp;lsquo;Convolution Norm Convergence Theorem&amp;rsquo;</description></item><item><title>Laplacian of a Scalar Function in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1882/</link><pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1882/</guid><description>Theorem In the curvilinear coordinate system, the Laplacian of a scalar function $f=f(q_{1},q_{2},q_{3})$ is as follows. $$ \nabla ^{2}f= \frac{1}{h_{1}h_{2}h_{3}}\left[\frac{ \partial }{ \partial q_{1} } \left( \frac{h_{2}h_{3}}{h_{1}} \frac{ \partial f}{ \partial q_{1}}\right)+\frac{ \partial }{ \partial q_{2} } \left( \frac{h_{1}h_{3}}{h_{2}} \frac{ \partial f}{ \partial q_{2}}\right)+\frac{ \partial }{ \partial q_{3} } \left( \frac{h_{1}h_{2}}{h_{3}} \frac{ \partial f}{ \partial q_{3}}\right) \right] $$ Formulas Cartesian coordinates: $$ h_{1}=h_{2}=h_{3}=1 $$ $$ \nabla ^2 f= \frac{ \partial^2</description></item><item><title>Laplacian of a Scalar Function in the Three-Dimensional Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/1879/</link><pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1879/</guid><description>Definition The Laplacian of a 3D scalar function $f=f(x,y,z)$ is the divergence of its gradient $f$ and is denoted by $\nabla^{2}$. $$ \nabla ^{2} f := \nabla \cdot(\nabla f)= \frac{ \partial^{2} f}{ \partial x^{2} }+\frac{ \partial^{2} f}{ \partial y^{2}}+\frac{ \partial^{2} f}{ \partial z^{2}} $$ Explanation The name Laplacian comes from the French mathematician Laplace. The notation $\nabla^{2}$ is used for convenience. In mathematics (theory of partial differential equations), the notation</description></item><item><title>Convolution Convergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1877/</link><pubDate>Sat, 14 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1877/</guid><description>Theorem Let&amp;rsquo;s assume that a function $g \in L^{1}$ satisfies the following condition. $$ \begin{align*} \int_{\mathbb{R}}g(y)dy &amp;amp;= 1 \\ \int_{-\infty}^{0}g(y)dy &amp;amp;= \alpha \\ \int_{0}^{\infty}g(y)dy &amp;amp;=\beta \\ \alpha+\beta &amp;amp;= 1 \end{align*} $$ Furthermore, let&amp;rsquo;s say $f$ is piecewise continuous on $\mathbb{R}$. And either $f$ is bounded, or $g$ outside any interval $[-a,a]$ is $g=0$. That is, the convolution $f \ast g(x)$ is well-defined for all $x\in \mathbb{R}$. Now, let&amp;rsquo;s assume for</description></item><item><title>If the Derivative of a Curve is Continuous, the Curve Can Be Measured</title><link>https://freshrimpsushi.github.io/en/posts/1870/</link><pubDate>Thu, 12 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1870/</guid><description>Theorem1 If $\gamma ^{\prime}$ is continuous on the interval $[a,b]$, then $\gamma$ forms a rectifiable curve, and the following equation holds: $$ \Lambda (\gamma) = \int _{a} ^{b} \left| \gamma^{\prime}(t) \right| dt $$ Proof Part 1. Let $P=\left\{ a=x_{0},\dots,x_{n}=b \right\}$ be any partition of the interval $[a,b]$. If we state $a\le x_{i-1}&amp;lt;x_{i}\le b$, then the following is true: $$ \begin{align*} \left| \gamma (x_{i})-\gamma (x_{i-1}) \right| &amp;amp;= \left| \int_{x_{i-1}}^{x_{i}}\gamma^{\prime} (t)dt \right|</description></item><item><title>Measuring Curves: A Guide to Length</title><link>https://freshrimpsushi.github.io/en/posts/1869/</link><pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1869/</guid><description>Definition1 A curve $\gamma : [a,b] \to \mathbb{R}^{k}$ on $\mathbb{R}^{k}$ or simply on $[a,b]$ is called a continuous function. If the curve $\gamma$ is a one-to-one function, it is called an arc. If $\gamma (a)=\gamma (b)$, then $\gamma$ is called a closed curve. Explanation The important point is that the curve is defined as a mapping, not as a collection of points. Now let&amp;rsquo;s define $\Lambda$ for the partition $P=\left\{</description></item><item><title>Integration of Vector-Valued Functions</title><link>https://freshrimpsushi.github.io/en/posts/1868/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1868/</guid><description>Definition1 Let $f_{1}$, $f_{2}$, $\dots$, $f_{k}$ be functions taking real values on the interval $[a,b]$. And suppose $\mathbf{f} : [a,b] \to \mathbb{R}^{k}$ is defined as follows. $$ \mathbf{f}(x)=\left( f_{1}(x),\dots,f_{k}(x) \right),\quad x\in [a,b] $$ If each $f_{k}$ is integrable on the interval $[a,b]$, then the integral of $\mathbf{f}$ is defined as follows. $$ \int _{a} ^{b} \mathbf{f}dx = \left( \int _{a} ^{b}f_{1} dx, \dots, \int _{a} ^{b}f_{k} dx \right) $$ Theorem</description></item><item><title>Integration by Parts</title><link>https://freshrimpsushi.github.io/en/posts/1867/</link><pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1867/</guid><description>Theorem 1 Assuming $F$, $G$ are differentiable in the interval $[a,b]$, and $F^{\prime}=f$, $G^{\prime}=g$ are integrable. Then, the following equation holds: $$ \begin{align*} \int _{a} ^{b} F(x)g(x)dx &amp;amp;= F(b)G(b)-F(a)G(a)-\int _{a} ^{b}f(x)G(x)dx \\ &amp;amp;= \left[ F(x)G(x) \right]_{a}^{b} -\int _{a} ^{b}f(x)G(x)dx \end{align*} $$ Description This result is called the integration by parts. Memorizing it as Integration-Differential-Integration makes it easy. What to integrate is kept on both sides as is, and what to</description></item><item><title>The Fundamental Theorem of Calculus in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1866/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1866/</guid><description>Theorem1 Given that function $f$ is Riemann integrable on the interval $[a,b]$, and there exists a function $F$ that is differentiable on $[a,b]$, satisfying $F^{\prime}=f$. Then, the following holds true. $$ \int_{a}^{b} f(x) dx= F(b)-F(a) $$ Explanation This theorem is famously known as the Fundamental Theorem of Calculus Part 2, often abbreviated as FTC2[^Funcamental Theorem of Calculus1]. It implies that the definite integral of $f$ is represented by the difference</description></item><item><title>Orthogonality of Solutions to the Regular Sturm-Liouville Problem</title><link>https://freshrimpsushi.github.io/en/posts/1859/</link><pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1859/</guid><description>Theorem1 Assume that $\lambda_{n}, \lambda_{m}$ are distinct eigenvalues of the regular S-L problem, and $u_{n}, u_{m}$ are the eigenfunctions corresponding to each eigenvalue with real values. Then, $u_{n}, u_{m}$ are orthogonal to each other in the $L_{w}^{2}(a,b)$ space. That is, $$ \int _{a} ^{b} u_{n}(x)u_{m}(x)w(x)dx=0 $$ Explanation Regular Sturm-Liouville Problem The differential equation $(1)$ is defined on the interval $[a,b]$ and is called a regular Sturm-Liouville problem when it satisfies</description></item><item><title>Eigenvalues and Eigenfunctions in S-L Problems</title><link>https://freshrimpsushi.github.io/en/posts/1858/</link><pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1858/</guid><description>Definition1 If the Sturm-Liouville differential equation $$ \begin{equation} \left[ p(x)u^{\prime}(x) \right]^{\prime}+\left[ q(x) +\lambda w(x) \right]u(x)=0 \end{equation} $$ has a solution $u \in L_{r}^{2}(a,b)$ different from $0$, then $\lambda$ is called an eigenvalue, and the corresponding $u$ is referred to as the eigenfunction. Explanation Let&amp;rsquo;s assume the weighting function is $w(x)=1$. Then, $(1)$ can be written as follows. $$ \begin{equation} \begin{aligned} &amp;amp;&amp;amp; p(x)u^{\prime \prime}(x) +p^{\prime}(x)u^{\prime}(x)+q(x)u(x)+\lambda u(x) =&amp;amp;\ 0 \\ \implies &amp;amp;&amp;amp;</description></item><item><title>Sturm-Liouville Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1857/</link><pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1857/</guid><description>Definition1 Let $p\in$, $C^{1}(\mathbb{R})$(../1594) and assume $q,r\in C(\mathbb{R})$, $\lambda \in \mathbb{R}$. The differential equation of the following form is called a Sturm-Liouville differential equation. $$ \begin{equation} \left[ p(x)u^{\prime}(x) \right]^{\prime}+\left[ q(x) +\lambda w(x) \right]u(x)=0 \end{equation} $$ or $$ p(x)u^{\prime \prime}(x)+p^{\prime}(x)u^{\prime}(x)+\left[ q(x)+\lambda w(x) \right]u(x)=0 $$ Explanation It is also referred to as the S-L problem. Here, $w$ is called the weight function, because it becomes the weight for the inner product in</description></item><item><title>Convolution's General Definition</title><link>https://freshrimpsushi.github.io/en/posts/1848/</link><pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1848/</guid><description>Definition Given the integral transform $J$ and two functions $f$, $g$, a function $f \ast g$ fulfilling the conditions below is defined as the convolution of $f$ and $g$ with respect to $J$. $$ J(f \ast g)=(Jf)(Jg) $$ Explanation According to the definition, the convolution, being the integral transform of a product, can be divided into the product of integral transforms. This means that two functions, which were bound in</description></item><item><title>Integral Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1847/</link><pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1847/</guid><description>Definition If a map $J$ from a function space to a function space is defined as the following integral, then $J$ is called an integral transform. $$ (Jf) (x) = \int_{a}^{b} K(x,t)f(t)dt $$ $$ J : f(\cdot) \mapsto \int_{a}^{b} K(\cdot,t)f(t)dt $$ In this case, $K$ is referred to as the kernel of $J$. If a map from $Jf$ to $f$ exists, it is denoted as $J^{-1}$ and called the inverse</description></item><item><title>Perceptron Definition</title><link>https://freshrimpsushi.github.io/en/posts/1846/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1846/</guid><description>Definition A perceptron is defined as the composition of a linear function $f(x) = wx + b$ and a unit step function $H$. $$ \text{Perceptron} := H \circ f (x) = H(wx + b) $$ In the case of a multivariable function, $f(\mathbf{x}) = \mathbf{w}\cdot \mathbf{x} + b = w_{1}x_{1} + \cdots w_{n}x_{n} + b$ and, $$ \text{Perceptron} := H \circ f (\mathbf{x}) = H(\mathbf{w} \cdot \mathbf{x} + b) $$</description></item><item><title>Prove that Norm is a Continuous Mapping</title><link>https://freshrimpsushi.github.io/en/posts/1845/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1845/</guid><description>Theorem Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ a norm space. Then for a sequence $\left\{ x_{k} \right\}$ of $X$ which is $\lim \limits_{k\to\infty} x_{k} = x$, the following equation holds. $$ \lim \limits_{k \to\infty} \left\| x_{k} \right\| = \left\| x \right\| $$ Explanation $\left\| \cdot \right\|$ means that it is a continuous function. The limit symbol can freely enter and exit the continuous function, which is a very good property.</description></item><item><title>Properties of the Norm Associated with the Inner Product Defined in Inner Space</title><link>https://freshrimpsushi.github.io/en/posts/1844/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1844/</guid><description>Theorem1 Given an inner space $\left( X, \langle \cdot,\cdot \rangle \right)$, one can naturally define the [norm] as in $\left\| \cdot \right\|:=\sqrt{\left\langle \cdot,\cdot \right\rangle }$ and the following properties hold. (a) The Cauchy-Schwarz Inequality: For any $\mathbf{x}, \mathbf{y}\in X$, $$ \left| \langle \mathbf{x},\mathbf{y} \rangle \right| \le \left\| \mathbf{x} \right\| \left\| \mathbf{y} \right\| $$ (b) The Parallelogram Law: For any $\mathbf{x},\mathbf{y}\in X$, $$ \left\| \mathbf{x} + \mathbf{y} \right\|^{2} + \left\| \mathbf{x}</description></item><item><title>Inner Product Spaces and the Cauchy-Schwarz Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1843/</link><pubDate>Mon, 26 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1843/</guid><description>Theorem1 Let $(H, \langle \cdot ,\cdot \rangle)$ be an inner product space. Then, the following inequality holds, and it is called the Cauchy-Schwarz inequality. $$ \left| \langle x,y \rangle \right| \le \langle x,x \rangle^{1/2} \langle y,y \rangle ^{1/2},\quad \forall x,y \in H $$ Explanation Since a norm can be defined from the inner product, it can also be expressed as the following equation. $$ \left| \left\langle x, y \right\rangle \right|</description></item><item><title>Inner product spaces</title><link>https://freshrimpsushi.github.io/en/posts/1842/</link><pubDate>Sun, 25 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1842/</guid><description>Definition1 Let&amp;rsquo;s consider $X$ as a vector space. For $\mathbf{x}, \mathbf{y}, \mathbf{z} \in X$ and $\alpha, \beta \in \mathbb{C}$(or $\mathbb{R}$), the following conditions satisfied by a function $$ \langle \cdot , \cdot \rangle : X \times X \to \mathbb{C} $$ are defined as the inner product, and $\left( X, \langle \cdot ,\cdot \rangle \right)$ is called an inner product space. Linearity: $$\langle \alpha \mathbf{x} + \beta \mathbf{y} ,\mathbf{z} \rangle =\alpha</description></item><item><title>Semi-Linear (Conjugate Linear) Functions</title><link>https://freshrimpsushi.github.io/en/posts/1841/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1841/</guid><description>Definition Assuming that a function $f : X \to \mathbb{C}$ is given. If the following equation holds for $x,y\in X$, $a,b \in \mathbb{C}$, then $f$ is called antilinear or conjugate linear. $$ f(ax + by)=\overline{a}f(x)+\overline{b}f(y) $$ Explanation Unlike linear functions, where the multiplied constant is the same inside and outside the function, it refers to a function in which the constant is the conjugate complex number inside and outside the</description></item><item><title>Relations among Inner Product Spaces, Normed Spaces, and Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1840/</link><pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1840/</guid><description>Description Let&amp;rsquo;s say an inner space $\left( X, \langle\cdot, \cdot\rangle \right)$ is given. Then, one can naturally define a norm as follows from the inner product. $$ \begin{equation} \left\| x \right\| := \sqrt{ \langle x, x\rangle},\quad x\in X \end{equation} $$ Hence, if it is an inner space, then it&amp;rsquo;s a normed space. Subsequently, one can define a distance from the norm thus defined. $$ \begin{equation} d(x,y):=\left\| x -y \right\| =\sqrt{</description></item><item><title>What is Computer Vision</title><link>https://freshrimpsushi.github.io/en/posts/1839/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1839/</guid><description>Explanation Computer vision is the research area that allows computers to perform functions corresponding to human vision, mainly dealing with images and videos. The conferences specialized in computer vision include ICCV (International Conference on Computer Vision), ECCV (European Conference on Computer Vision), and CVPR (Conference on Computer Vision and Pattern Recognition). The problems primarily handled in computer vision can be classified into three major categories, as shown in the picture</description></item><item><title>Continuous Learning in Deep Learning</title><link>https://freshrimpsushi.github.io/en/posts/1837/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1837/</guid><description>Explanation Continual learning in deep learning refers to the sequential learning of multiple tasks by artificial neural networks, synonymous with lifelong learning or incremental learning. Unlike humans, who do not forget existing knowledge simply by learning something new – though they may forget over time, not due to the acquisition of new knowledge – artificial neural networks exhibit a decline in performance on previously learned tasks after sufficiently learning one</description></item><item><title>Mellin Transform Convolution</title><link>https://freshrimpsushi.github.io/en/posts/1835/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1835/</guid><description>Definition The convolution of the Mellin transform is as follows. $$ (f \times g) (y) = \int _{0}^{\infty} f(x)g \left(\frac{y}{x} \right)\frac{dx}{x} $$ Explanation It is also called multiplicative convolution1. Proof $$ \mathcal{M}(f \times g)=(\mathcal{M}f)(\mathcal{M}g) $$ It suffices to show that the above equation holds. $$ \begin{align*} \mathcal{M}(f\times g)(s) &amp;amp;= \int _{0} ^{\infty} x^{s-1} (f\times g)(x)dx \\ &amp;amp;= \int _{0} ^{\infty} x^{s-1} (f\times g)(x)dx \\ &amp;amp;= \int _{0} ^{\infty} x^{s-1} \left(</description></item><item><title>Classification of Discontinuities</title><link>https://freshrimpsushi.github.io/en/posts/1833/</link><pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1833/</guid><description>Definition1 Let&amp;rsquo;s assume the function $f :X \to \mathbb{R}$ is given in a metric space $X$. If $f$ is not continuous at $x\in X$, it is said that $f$ is discontinuous at $x$ or has a discontinuity at $x$. Let&amp;rsquo;s say $f: (a,b) \to \mathbb{R}$. If $f$ is discontinuous at $x\in (a,b)$ and the left/right limits $f(x-)$, $f(x+)$ at $x$ exist, it is said that $f$ has a discontinuity of</description></item><item><title>Limits from the Left and the Right Strictly Defined in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1830/</link><pubDate>Sat, 17 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1830/</guid><description>Definition Let&amp;rsquo;s assume a function $f :X \to \mathbb{R}$ is given in a metric space $X$. If $f$ is not continuous at $x\in X$, $f$ is said to be discontinuous at $x$ or to have a discontinuity at $x$. $f: (a,b) \to \mathbb{R}$ is assumed. For any point $x$, let $a \le x &amp;lt;b$. Consider a sequence of points $(x,b)$ that converges to $x$ and call it $\left\{ t_{n} \right\}$.</description></item><item><title>The Relationship between Derivatives and the Increasing/Decreasing of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1826/</link><pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1826/</guid><description>정리 Let the function $f$ be differentiable at $(a,b)$. If for all $x\in (a,b)$, $f^{\prime}(x) \ge 0$ holds, then $f$ is monotonically increasing. If for all $x\in (a,b)$, $f^{\prime}(x)=0$ holds, then $f$ is a constant function. If for all $x\in (a,b)$, $f^{\prime}(x) \le 0$ holds, then $f$ is monotonically decreasing. Proof From the Mean Value Theorem, it follows that for all $x_{1},x_{2}\in (a,b)$ and $x \in (x_{1},x_{2})$ the following</description></item><item><title>Mean Value Theorem in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1824/</link><pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1824/</guid><description>Theorem Let the functions $f$ and $g$ be continuous on the interval $[a,b]$ and differentiable on $(a,b)$. Then there exists $x \in (a,b)$ that satisfies the following equation. $$ [f(b)-f(a)]g^{\prime}(x)=[g(b)-g(a)]f^{\prime}(x) $$ Note that differentiability is not necessary at the endpoints $a$ and $b$. Explanation This is a generalization of the Mean Value Theorem learned in high school and in calculus. If we set it as $g(x)=x$, it becomes the familiar</description></item><item><title>Monotonic Functions, Increasing Functions, Decreasing Functions</title><link>https://freshrimpsushi.github.io/en/posts/848/</link><pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/848/</guid><description>Definition Let&amp;rsquo;s assume the function $f:[a,b] \rightarrow \mathbb{R}$ is given. For $x_{1}$, $x_{2}$, $\in [a,b]$ $$ x_{1} \lt x_{2} \ \implies f(x_{1}) \le f(x_{2}) $$ If it satisfies, then $f$ is said to be monotonically increasing or $f$ is called a monotone increasing function. Conversely, $$ x_{1} \lt x_{2} \ \implies f(x_{1}) \ge f(x_{2}) $$ If it satisfies, then $f$ is said to be monotonically decreasing or $f$ is called</description></item><item><title>Definition and Relationship of Extremum in Analysis and Differential Coefficients</title><link>https://freshrimpsushi.github.io/en/posts/1699/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1699/</guid><description>Definition Let $(X,d)$ be a metric space. If there exists a positive real number $\delta &amp;gt;0$ such that the function $f : X \rightarrow \mathbb{R}$ satisfies the condition below, then $f$ has a local maximum at point $p \in X$. $$ \forall q\in X,\quad f(q)\le f(p)\ \mathrm{with}\ d(p,q)&amp;lt;\delta $$ Explanation To put it in words: If $f(p)$ is the largest within a distance of $\delta$ from $p$, then $f(p)$ is</description></item><item><title>The Chain Rule of Differentiation in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1823/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1823/</guid><description>Theorem1 If $f :[a,b] \to \mathbb{R}$ is a continuous function and is differentiable at $x\in [a,b]$, and if $g : f([a,b])\to \mathbb{R}$ is differentiable at $f (x)\in f([a,b])$, and if we define $h : [a,b] \to \mathbb{R}$ as follows. $$ h(t)=g\left( f(t) \right)\quad (a\le t \le b) $$ Then, $h$ is differentiable at $x$ and its value is as follows. $$ h^{\prime}(x)=g^{\prime}(f(x))f^{\prime}(x) $$ Using the composite function symbol, it can</description></item><item><title>Differentiable Function Properties</title><link>https://freshrimpsushi.github.io/en/posts/1821/</link><pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1821/</guid><description>Theorem1 Let&amp;rsquo;s say $f, g : [a,b] \to \mathbb{R}$. If $f,g$ is differentiable at $x\in [a,b]$, then $f+g$, $fg$, and $f/g$ are also differentiable at $x$ and the following equation holds. $$ \begin{align} (f+g)^{\prime}(x) &amp;amp;=f^{\prime}(x)+g^{\prime}(x) \\ (fg)^{\prime}(x) &amp;amp;= f^{\prime}(x)g(x)+f(x)g^{\prime}(x) \\ \left( \frac{f}{g} \right)^{\prime}(x) &amp;amp;= \frac{f^{\prime}(x)g(x)-f(x)g^{\prime}(x)}{g^{2}(x)} \end{align} $$ However, $(3)$ holds when $g(x)\ne 0$. Description $(2)$ is commonly referred to as the product rule of differentiation. Proof $(1)$ By the definition</description></item><item><title>If Differentiable, Then Continuous</title><link>https://freshrimpsushi.github.io/en/posts/1820/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1820/</guid><description>Theorem1 Let&amp;rsquo;s say $f : [a,b] \to \mathbb{R}$. If $f$ is differentiable at $p \in [a,b]$, then $f$ is continuous at $p$. Explanation Note that the converse &amp;lsquo;if it is continuous, it is differentiable&amp;rsquo; does not hold. In the past, there was a pun called Simple Integration (simply put, if it&amp;rsquo;s differentiable, then it&amp;rsquo;s continuous) among older students, but I wonder if this pun has become unused as current students</description></item><item><title>Regulating Supersaturation</title><link>https://freshrimpsushi.github.io/en/posts/1819/</link><pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1819/</guid><description>Definition1 A continuous linear functional $T:\mathcal{S}(\mathbb{R}^{n}) \to \mathbb{C}$ on the Schwartz space is called a tempered distribution. In other words, a tempered distribution is an element of the dual space of the Schwartz space. Therefore, $$ T \in \mathcal{S}^{ \ast } $$ is denoted as such, and $\mathcal{S}^{ \ast }$ is called the space of tempered distributions. Description Since a tempered distribution $T$ is linear, the following holds true. $$</description></item><item><title>Fourier Series in Complex Notation</title><link>https://freshrimpsushi.github.io/en/posts/964/</link><pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/964/</guid><description>Formulas The complex Fourier series of a function $f$ defined over the interval $[-L,\ L)$ is given by: $$ f(t) = \sum \limits_{n=-\infty}^{\infty} c_{n} e^{i\frac{n\pi t}{L}} $$ Here, the complex Fourier coefficients are as follows: $$ c_{n} = \dfrac{1}{2L}\int_{-L}^{L}f(t)e^{-i\frac{n \pi t}{L} }dt $$ The Fourier coefficients satisfy the following equation: $$ \begin{align*} a_{0} &amp;amp; = 2 c_{0} \\ a_{n} &amp;amp;= c_{n}+c_{-n} \\ b_{n} &amp;amp;= i(c_{n}-c_{-n}) \\ c_{n} &amp;amp;= \frac{1}{2} (a_{n}-ib_{n})</description></item><item><title>Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1818/</link><pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1818/</guid><description>Definitions1 Partial Differential Equations For a natural number $k \in \mathbb{N}$ and an open set $U \subset \mathbb{R}^{n}$, the following expression is called a $k$-order partial differential equation. $$ \begin{equation} F(D^{k}u(x), D^{k-1}u(x),\cdots,Du(x),u(x),x)=0\quad (x\in U) \end{equation} $$ Here, $D^{k}u$ is the multi-index notation. $F$ is given as follows, and the unknown $u$ is as follows. $$ F : {\mathbb{R}}^{n^{k}}\times{\mathbb{R}}^{n^{k-1}}\times \cdots \times \mathbb{R}^{n}\times \mathbb{R}\times U \to \mathbb{R} \\ u : U \to</description></item><item><title>Divergence of Vector Functions in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1817/</link><pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1817/</guid><description>Theorem The divergence of the vector function $\mathbf{F}=\mathbf{F}(q_{1},q_{2},q_{3})=F_{1}\hat{\mathbf{q}}_{1}+F_{2}\hat{\mathbf{q}}_{2}+F_{3}\hat{\mathbf{q}}_{3}$ in curvilinear coordinates is as follows. $$ \nabla \cdot \mathbf{F}=\frac{1}{h_{1}h_{2}h_{3}}\left[ \frac{ \partial }{ \partial q_{1} }(h_{2}h_{3}F_{1})+\frac{ \partial }{ \partial q_{2} }(h_{1}h_{3}F_{2})+\frac{ \partial }{ \partial q_{3} }(h_{1}h_{2}F_{3}) \right] $$ $h_{i}$ is the scale factor. Formulas Cartesian coordinates: $$ h_{1}=h_{2}=h_{3}=1 $$ $$ \begin{align*} \nabla \cdot \mathbf{F} =\frac{\partial F_{x}}{\partial x}+\frac{\partial F_{y}}{\partial y}+\frac{\partial F_{z}}{\partial z} \end{align*} $$ Cylindrical coordinates: $$ h_{1}=1,\quad h_{2}=\rho,\quad h_{3}=1 $$ $$ \begin{align*}</description></item><item><title>Gradient of a Scalar Function in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1816/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1816/</guid><description>Theorem In a curvilinear coordinate system, the gradient of a scalar function $f=f(q_{1},q_{2},q_{3})$ is as follows. $$ \nabla f= \frac{1}{h_{1}}\frac{ \partial f }{ \partial q_{1} } \hat{\mathbf{q}}_{1} + \frac{1}{h_{2}}\frac{ \partial f }{ \partial q _{2}}\hat{\mathbf{q}}_{2}+\frac{1}{h_{3}}\frac{ \partial f }{ \partial q_{3} } \hat{\mathbf{q}}_{3}=\sum \limits _{i=1} ^{3}\frac{1}{h_{i}}\frac{ \partial f}{ \partial q_{i}}\hat{\mathbf{q}}_{i} $$ $h_{i}$ is the scale factor. Formulas Cartesian Coordinate System: $$ h_{1}=h_{2}=h_{3}=1 $$ $$ \nabla f= \frac{\partial f}{\partial x}\mathbf{\hat{\mathbf{x}} }+ \frac{\partial</description></item><item><title>Spline, B-Spline in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1815/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1815/</guid><description>Definition1 If the function $f:\mathbb{R} \to \mathbb{R}$ is a piecewise polynomial on interval $\mathbb{R}$, it is called a spline on $\mathbb{R}$. The points where the polynomial changes are called knots. Explanation As can be seen from the definition, a spline does not have to be a continuous function. The following function $f$ is an example of a spline. $$ f(x) = \begin{cases} 0 &amp;amp; x\in[\infty,0] \\ 2x^{2}&amp;amp;x\in(0,1] \\ 2-x &amp;amp;</description></item><item><title>Unit Partition in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1814/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1814/</guid><description>Definition A set $\left\{ u_{\alpha} \right\}_{\alpha \in \Lambda}$ of continuous functions $u_{\alpha} : X \to [0,1], \alpha \in \Lambda$ that satisfies the conditions below is called a Partition of Unity. $$ \sum _{\alpha \in \Lambda}u_{\alpha}(x)=1 $$</description></item><item><title>Zeeman Effect</title><link>https://freshrimpsushi.github.io/en/posts/1813/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1813/</guid><description>설명 1897년 네덜란드 물리학자 피테르 제이만이 발견한 현상으로 원자가 자기장 내에 있을 때 방출 스펙트럼 선이 갈라지는 것을 말한다. 패러데이가 1860년에 나</description></item><item><title>Spectrum and Fraunhofer Lines</title><link>https://freshrimpsushi.github.io/en/posts/1812/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1812/</guid><description>Explanation1 2 3 4 A spectrum refers to the object of study in spectroscopy, which is light decomposed into various colors. The first spectrum picture one often sees is a line spectrum like the one above, either long horizontally or vertically. However, the spectrum was not always in this form. The first person to discover that light could be decomposed into various colors through a prism was Newton. Because Newton</description></item><item><title>Spectroscopy</title><link>https://freshrimpsushi.github.io/en/posts/1811/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1811/</guid><description>Explanation1 2 As shown in the above gif, breaking light into various colors is called a spectrum, or spectroscopy in Korean. Spectroscopy is a branch of optics that focuses on observing and studying visible light that has been dispersed according to wavelength. However, its meaning has recently expanded to include the measurement and study of any physical quantity based on wavelength or frequency. The concept of the spectrum has also</description></item><item><title>Paper Review: Do We Need Zero Training Loss After Achieving Zero Training Error?</title><link>https://freshrimpsushi.github.io/en/posts/1809/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1809/</guid><description>Paper Review Flooding refers to a regularization technique introduced in Do We Need Zero Training Loss After Achieving Zero Training Error?, presented at ICML 2020. According to the authors of the paper, the root cause of overfitting is the excessively low training loss as illustrated below. Thus, the core idea of the paper is that controlling the training loss not to fall below a certain value during the learning process,</description></item><item><title>Commonly Used Datasets in Machine Learning</title><link>https://freshrimpsushi.github.io/en/posts/1808/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1808/</guid><description>Computer Vision MNIST This is the first dataset that one encounters when studying machine learning. It is pronounced as [em-nist] and consists of hand-written digit images of size $28\times 28$. The dataset includes 60,000 training images and 10,000 testing images[^1]. CIFAR-10, CIFAR-100 CIFAR-10, pronounced as [cypher-ten], includes 60,000 images in 10 different categories, with images of size $32\times 32$. It is composed of 50,000 training images and 10,000 testing images.</description></item><item><title>What is Overfitting and Regularization in Machine Learning?</title><link>https://freshrimpsushi.github.io/en/posts/1807/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1807/</guid><description>Overfitting The phenomenon where the training loss decreases, but the test loss (or validation loss) does not decrease or rather increases is called overfitting. Explanation There is also a term called underfitting, which basically means the opposite, but frankly, it&amp;rsquo;s a meaningless term and not often used in practice. A crucial point in machine learning is that the function trained with the available data must also work well with new</description></item><item><title>Curved Coordinate Systems: Coordinate Transformations and Jacobians</title><link>https://freshrimpsushi.github.io/en/posts/1806/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1806/</guid><description>Formulas The volume in a 3-dimensional Cartesian coordinate system is represented for any curvilinear coordinate system as follows. $$ dxdydz =\begin{vmatrix} \dfrac{ \partial x}{ \partial q_{1}} &amp;amp; \dfrac{ \partial y}{ \partial q_{1}} &amp;amp; \dfrac{ \partial z}{ \partial q_{1} } \\[1em] \dfrac{ \partial x}{ \partial q_{2}} &amp;amp; \dfrac{ \partial y}{ \partial q_{2}} &amp;amp; \dfrac{ \partial z}{ \partial q_{2} } \\[1em] \dfrac{ \partial x}{ \partial q_{3}} &amp;amp; \dfrac{ \partial y}{ \partial q_{3}}</description></item><item><title>Schwartz Space and Schwartz Functions</title><link>https://freshrimpsushi.github.io/en/posts/1805/</link><pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1805/</guid><description>Definition The set of functions $\phi : \mathbb{R}^{n} \to \mathbb{C}$ that satisfy the following two conditions is called the Schwartz space, denoted by $\mathcal{S}(\mathbb{R}^{n})$. An element $\phi$ of the Schwartz space is called a Schwartz function. (a) $\phi \in $ $C^{\infty}$ (b) For any multi-index $\alpha$, $\beta$, the following holds: $\left| \mathbf{x}^{\beta}D^{\alpha}\phi (\mathbf{x}) \right| &amp;lt;\infty$. Here, for $\beta=(\beta_{1}, \beta_{2},\dots,\beta_{n})$, $$ \mathbf{x}^{\beta}=x_{1}^{\beta_{1}}x_{2}^{\beta_{2}}\dots x_{n}^{\beta_{n}} $$ (b) can be rewritten as: $$ \mathbf{x}^{</description></item><item><title>Curvilinear Coordinates in Three-Dimensional Space</title><link>https://freshrimpsushi.github.io/en/posts/1774/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1774/</guid><description>Buildup The most common way to express a position in three-dimensional space is the Cartesian coordinate system. Named after Descartes, who devised it, it is also widely known as the orthogonal coordinate system. However, in specific situations, it might be difficult to represent the position using the Cartesian coordinate system. For instance, let&amp;rsquo;s consider an object performing rotational motion on a two-dimensional plane. Then, it would be much simpler to</description></item><item><title>Derivative Approximation</title><link>https://freshrimpsushi.github.io/en/posts/1085/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1085/</guid><description>Buildup Let&amp;rsquo;s recall the idea of defining the differentiation of distributions. There exists a regular distribution $T_{u}$ for $u \in {L}_{\mathrm{loc}}^1(\Omega)$. If $u$ is differentiable, by applying the integration by parts, the following equation holds, and the derivative of $T_{u}$ is defined as $T_{u^{\prime}}$, which corresponds to the derivative of $u$, $u^{\prime}$. $$ \begin{align*} T_{u}^{\prime}(\phi) &amp;amp;:= T_{u^{\prime}}(\phi) \\ &amp;amp;= \int u^{\prime}(x)\phi (x)dx \\ &amp;amp;= \left[ u(x) \phi (x) \right]_{-\infty}^{\infty} -\int</description></item><item><title>Differentiation of the Product of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1804/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1804/</guid><description>Theorem1 Let&amp;rsquo;s denote $T\in D^{\ast}$ as a distribution, and $f \in C^{\infty}$ as a smooth function. Then, the following equation holds. $$ (fT)^{\prime}= f^{\prime}T+fT^{\prime} $$ Explanation It fits perfectly with the existing product rule, so one can feel that the differentiation of a distribution and product of distributions have been plausibly defined. Proof By the definition of distribution differentiation and product, the following is true. $$ \begin{align*} D( fT (\phi)</description></item><item><title>Multiplication of a Distribution with a Smooth Function</title><link>https://freshrimpsushi.github.io/en/posts/1803/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1803/</guid><description>Buildup A distribution cannot be multiplied with a function defined on the real space since its domain is a function space. However, in the case of regular distributions, there exists a corresponding locally integrable function $u\in L_{\mathrm{loc}}^{1}$, so it is represented as follows. $$ T_{u}(\phi)=\int u(x)\phi (x) dx,\quad \phi \in \mathcal{D} $$ Therefore, considering some action $S$ applied to $u$ by which we can obtain $Su=u^{\prime}$, if $u^{\prime}$ remains a</description></item><item><title>Scaling Factors of Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1776/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1776/</guid><description>Buildup In the curvilinear coordinate system, the scale factor is an element that multiplies each component so that they have dimensions of length. For instance, the polar coordinate system is represented by $(r,\theta)$, where the distance the coordinates move as $\theta$ changes is the length of the arc, which is $l=r\theta$. Here, things like $r$ are called scale factors. Let&amp;rsquo;s say the variable of an arbitrary coordinate system is $(q_{1},q_{2},q_{3})$.</description></item><item><title>Convergence of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1802/</link><pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1802/</guid><description>Definition1 Let&amp;rsquo;s say $D^{\ast}$ is a distribution space, $\left\{ T_{n} \right\}$ a sequence of distributions in $D^{\ast}$. If for all test functions $\phi$, the following equation holds, then $\left\{ T_{n} \right\}$ is said to weakly converge to $T$. $$ T_{n}(\phi) \to T(\phi) ,\quad \forall \phi \in \mathcal{D} $$ Explanation The convergence of distributions is referred to as weak convergence because, in the case that $T$, $T_{n}$ are regular distributions, it</description></item><item><title>Convergence of Sequences in Normed Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1800/</link><pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1800/</guid><description>Definition Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ as a normed space. For a sequence $\left\{ x_{n} \right\}$ of $X$, $$ \lim \limits_{n \to \infty} \left\| x - x_{n} \right\| = 0,\quad x\in X $$ it is said to converge to $x$ if it satisfies the following condition, and it is represented as follows. $$ x_{n} \to x \text { as } n \to \infty \quad \text{or} \quad x=\lim \limits_{n\to\infty}x_{n} $$</description></item><item><title>Weak Convergence in Hilbert Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1801/</link><pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1801/</guid><description>Definition Let $(H,\langle \cdot \rangle)$ be a Hilbert space, and let $\left\{ x_{n} \right\}$ be a sequence in $H$. For all $y\in H$, if the following equation holds, $\left\{ x_{n} \right\}$ is said to converge weakly and is denoted as $x_{n} \rightharpoonup x$. $$ \langle x_{n}, y \rangle \to \langle x , y \rangle ,\quad \forall y\in H $$ Following the &amp;lsquo;w&amp;rsquo; in weak, it can also be denoted as:</description></item><item><title>Total Differentiation, Exact Differentiation</title><link>https://freshrimpsushi.github.io/en/posts/1773/</link><pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1773/</guid><description>Definition Let&amp;rsquo;s assume that a multivariable function $f : \mathbb{R}^{n} \to \mathbb{R}$ is given. The change of $f(\mathbf{x})$ according to the change of variable $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n})$ is denoted as $df$, and this is called the total differential or exact differential of $f$. $$ \begin{equation} df = \frac{ \partial f}{ \partial x_{1} }dx_{1} + \frac{ \partial f}{ \partial x_{2} }dx_{2} + \cdots + \frac{ \partial f}{ \partial</description></item><item><title>Differentiation of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1084/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1084/</guid><description>Buildup The distribution cannot be differentiated in the same manner as functions defined over real numbers since its domain is a function space. However, for regular distributions, there is a corresponding locally integrable function $u\in L_{\mathrm{loc}}^{1}$, expressed as follows. $$ T_{u}(\phi) =\int u(x)\phi (x) dx,\quad \phi \in \mathcal{D} $$ Hence, the action $S$ on $u$ could yield $Su=u^{\prime}$; if $u^{\prime}$ remains a locally integrable function, then there exists a corresponding</description></item><item><title>Divergence of Vector Function in Cartesian Cooridenates System</title><link>https://freshrimpsushi.github.io/en/posts/1796/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1796/</guid><description>Definition For a vector function $\mathbf{F}(x,y,z)=F_{x}\hat{\mathbf{x}}+F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$, the following scalar function is defined as the divergence $\mathbf{F}$ of $\mathbf{F}(x,y,z)=F_{x}\hat{\mathbf{x}}+F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$ and is denoted by $\nabla \cdot \mathbf{F}$. $$ \begin{equation} \nabla \cdot \mathbf{F} := \frac{ \partial F_{x}}{ \partial x} + \frac{ \partial F_{y}}{ \partial y }+ \frac{ \partial F_{z}}{ \partial z} \label{divergence} \end{equation} $$ Explanation Geometrically, if $\nabla \cdot \mathbf{F}&amp;gt;0$, it means that $\mathbf{F}$ is spreading out or diverging.</description></item><item><title>Reasons Not to Use r, Theta as Variables in Cylindrical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1795/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1795/</guid><description>Notation of Cylindrical Coordinates The cylindrical coordinate system represents points in 3D space as shown in $(\rho, \phi, z)$. Here, $\rho$: the magnitude of the vector projection of the position vector $\mathbf{r}$ onto the $xy$-plane $\phi$: the angle between the projected vector and the $x$ axis $z$: the magnitude of the vector projection of the position vector $\mathbf{r}$ onto the $z$ axis However, the notation $(r,\theta, z)$ is also often</description></item><item><title>Dilation of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1794/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1794/</guid><description>Buildup The distribution cannot be dilated in the same manner as functions defined in real space because its domain is a function space. However, for regular distributions, there exists a corresponding locally integrable function $u\in L_{\mathrm{loc}}^{1}$, allowing it to be expressed as follows. $$ T_{u}(\phi) =\int u(x)\phi (x) dx,\quad \phi \in \mathcal{D} $$ Therefore, through some action $S$ on $u$, we can obtain $Su=u^{\prime}$, and if $u^{\prime}$ remains a locally</description></item><item><title>Fourier Transform of the Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/1793/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1793/</guid><description>Formulas Let&amp;rsquo;s assume that the Fourier transform of the function $f(x)$ is $\hat{f}(\xi) = \mathcal{F}[f] (x) = \displaystyle \int_{-\infty}^{\infty} f(x)e^{-i \xi x}dx$. The Fourier transform of the Dirac delta function $\delta (x)$ is as follows. $$ \hat{\delta}(\xi) = \mathcal{F}[\delta] (\xi) = 1 $$ The Fourier transform of $\delta (x - y)$ is $$ \mathcal{F}[\delta (\cdot - y)] (\xi) = e^{-i\xi y} $$ Explanation Depending on how the Fourier transform is</description></item><item><title>Proof that the Dirac Delta Function is Not a Regularized Distribution</title><link>https://freshrimpsushi.github.io/en/posts/1785/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1785/</guid><description>Theorem1 $$ \delta (\phi) := \phi (0), \quad \phi \in \mathcal{D} $$ As defined above, the Dirac delta function is not a regular distribution. Distributions that are not regular are called singular distributions. Description A regular distribution refers to a distribution that is defined with the existence of a locally integrable function $u$ as follows: $$ T_{u}(\phi) := \int u(x) \phi (x) dx,\quad \phi \in \mathcal{D} $$ The statement that</description></item><item><title>Characteristic Function, Indicator Function</title><link>https://freshrimpsushi.github.io/en/posts/1790/</link><pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1790/</guid><description>Definition For $A \subset X$, the function defined as $\chi_{A} : X \to \mathbb{R}$ is referred to as the characteristic function or the indicator function. $$ \chi _{A}(x) := \begin{cases} 1, &amp;amp; x\in A \\ 0 ,&amp;amp; x \notin A \end{cases} $$ Explanation $\chi$ is the Greek letter chi. The reason our math teacher used to say you should not write the letter x as $\chi$ but should instead use</description></item><item><title>The Dirac Delta Function Rigorously Defined through Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1784/</link><pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1784/</guid><description>Definition1 Let&amp;rsquo;s define the functional of the space of test functions $\mathcal{D}(\mathbb{R}^{n})$ as follows and call it the Dirac delta function. $$ \delta_{a}(\phi):=\phi (a) $$ Then, the Dirac delta function becomes a distribution. It is briefly represented as follows if $a=0$. $$ \delta=\delta_{0} $$ Explanation The Dirac delta function, which was not strictly defined due to having divergent values and was roughly termed as a function, is rigorously defined by</description></item><item><title>Convergence in the Space of Test Functions</title><link>https://freshrimpsushi.github.io/en/posts/1077/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1077/</guid><description>In the test function space, &amp;lsquo;convergence&amp;rsquo; is defined in a special way. Normally, when a space $X$ is given, convergence is defined using the norm or distance defined in $X$. However, in the test function space, convergence is defined under stronger conditions to properly define and handle distributions. Definition Let $\Omega \subset \mathbb{R}^n$ be an open set, and $\left\{ \phi _{j} \right\}$ be a sequence of test functions. We say</description></item><item><title>Distributions, Generalized Functions</title><link>https://freshrimpsushi.github.io/en/posts/1009/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1009/</guid><description>Definition1 2 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. A continuous linear functional $T : \mathcal{D}(\Omega) \to \mathbb{C}$ on the space of test functions is defined as a distribution. That is, a distribution is an element of the dual space of the test function space. Thus $$ T \in \mathcal{D}^{\ast} $$ and we call $D^{\ast}$ the (Schwartz) distribution space. Explanation The name distribution seems to be influenced by the</description></item><item><title>Locally Integrable Function</title><link>https://freshrimpsushi.github.io/en/posts/1783/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1783/</guid><description>Definition Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. Definition 11 For every bounded measurable set $K \subset \Omega$, $$ \int_{K} \left| u(x) \right| dx \lt \infty $$ a function $u : \Omega \to \mathbb{C}$ satisfying this is said to be locally integrable with respect to (the Lebesgue measure). Definition 22 Let the function $u$ be defined almost everywhere on $\Omega$. For every open set $U \Subset \Omega$ when</description></item><item><title>Proving that All Locally Integrable Functions Can Be Extended to Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1078/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1078/</guid><description>Theorem1 For every $u \in L_{\mathrm{loc} }^1(\Omega) $, there exists a distribution $T_{u} \in D^{\ast}(\Omega)$ defined as follows: $$ T_{u} (\phi) := \int_{\Omega} u(x)\phi (x)dx, \quad \phi \in D(\Omega) $$ Description $\mathcal{D}(\Omega)$ is the space of test functions. The distribution defined as above is called a regular distribution. Moreover, the above expression can be regarded as the inner product of $u$ and $\phi$ from the viewpoint of inner product spaces,</description></item><item><title>Test Functions and Test Function Space</title><link>https://freshrimpsushi.github.io/en/posts/1782/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1782/</guid><description>Definition1 Let an open set $\Omega \subset \mathbb{R}^{n}$ and a function $\phi : \Omega \to \mathbb{C}$ be given. If $\phi$ is infinitely differentiable, and all its derivatives are continuous and have a compact support, it is called a test function. The function space of test functions is denoted by $C_{c}^{\infty}(\Omega)$ or simply as $\mathcal{D}(\Omega)$. Explanation It is also called a test function or testing function. The reason $\phi$ is named</description></item><item><title>Translation of Distributions</title><link>https://freshrimpsushi.github.io/en/posts/1786/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1786/</guid><description>Buildup Distribution cannot be translated in the same manner as functions defined on the real space because their domain is a function space. However, for regular distributions, there is a corresponding locally integrable function $u\in L_{\mathrm{loc}}^{1}$, which can be represented as follows. $$ T_{u}(\phi) =\int u(x)\phi (x) dx,\quad \phi \in \mathcal{D}(\mathbb{R}^{n}) $$ Thus, some action $S$ on $u$ would yield $Su=u^{\prime}$, and if $u^{\prime}$ is still a locally integrable function,</description></item><item><title>The History of the Delta Function and Why Dirac Used the Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/1781/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1781/</guid><description>The History of the Delta Function1 2 3 The delta function started to appear in the works of scholars such as Poisson (1815), Fourier (1822), and Cauchy (1823, 1827) who made significant contributions to mathematics and physics in the early 19th century. However, at that time, there was not a focus on rigorously defining the delta function as we do today. Later, Kirchhoff (1882, 1891) and Heaviside (1893, 1899) were</description></item><item><title>Why Functional is Named Functional</title><link>https://freshrimpsushi.github.io/en/posts/1780/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1780/</guid><description>The term &amp;ldquo;functional analysis&amp;rdquo; is indeed intriguing, especially when considering the word &amp;ldquo;functional&amp;rdquo; instead of merely &amp;ldquo;function analysis.&amp;rdquo; At first glance, &amp;ldquo;functional&amp;rdquo; appears to be an adjective form of &amp;ldquo;function,&amp;rdquo; suggesting meanings like &amp;ldquo;function-like&amp;rdquo; or &amp;ldquo;pertaining to functions.&amp;rdquo; This notion can also be found in another name for functionals, &amp;ldquo;generalized functions.&amp;rdquo; The question arises as to why these are not simply called functions. To understand this, let&amp;rsquo;s look at the</description></item><item><title>Gradient of Scalar Function in Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/1778/</link><pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1778/</guid><description>Definition For a scalar function $f=f(x,y,z)$, the following vector function is defined as the gradient of $f$, denoted by $\nabla f$: $$ \nabla f := \frac{ \partial f}{ \partial x }\hat{\mathbf{x}}+\frac{ \partial f}{ \partial y}\hat{\mathbf{y}}+\frac{ \partial f}{ \partial z}\hat{\mathbf{z}} = \left( \dfrac{\partial f}{\partial x}, \dfrac{\partial f}{\partial y}, \dfrac{\partial f}{\partial z} \right) $$ Explanation The gradient is translated into English as gradient, slope, or incline. The terms &amp;lsquo;slope&amp;rsquo; and &amp;lsquo;incline&amp;rsquo; are</description></item><item><title>Coupled Oscillations</title><link>https://freshrimpsushi.github.io/en/posts/1767/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1767/</guid><description>Simple Coupled Oscillations Let&amp;rsquo;s say we have two objects, $m_{1}$ and $m_{2}$, connected by two springs as shown in the above figure. Let the distance from the equilibrium point to object $m_{1}$ be $x_{1}$, and to object $m_{2}$ be $x_{2}$. The restoring force exerted by a spring on an object is the product of the spring constant and the stretch (or compression) of the spring, so the force exerted by</description></item><item><title>Fluids and the Definition of Fluid Dynamics</title><link>https://freshrimpsushi.github.io/en/posts/1768/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1768/</guid><description>Definition Fluid refers to: A term that combines both liquid and gas A collection of molecules that are randomly arranged and clustered together1 In a stationary state, it is subject to vertical stress, and in a flow state, it undergoes continuous deformation and flows when subjected to shear forces2 Description Although it&amp;rsquo;s called a definition, it&amp;rsquo;s not that strict. However, without pushing for a strict definition, we naturally understand what</description></item><item><title>Multiple Spring Oscillation</title><link>https://freshrimpsushi.github.io/en/posts/1766/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1766/</guid><description>When springs are connected on both sides of an object Let $x$ be the distance the object has moved. Since the restoring force of the spring is $-kx$, the object receives a force of $-k_{1}x$ from the left spring and $-k_{2}x$ from the right spring. Therefore, the equation of motion is as follows. $$ \begin{align*} &amp;amp;&amp;amp; m\ddot{x}&amp;amp;=-k_{1}x-k_{2}x \\ \implies &amp;amp;&amp;amp;m\ddot{x}+(k_{1}+k_{2})x&amp;amp;=0 \\ \implies &amp;amp;&amp;amp; \ddot{x}+\frac{k_{1}+k_{2}}{m}x &amp;amp;=0 \end{align*} $$ This is the</description></item><item><title>Frequently Used Symbols and Abbreviations in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1764/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1764/</guid><description>for all, exist, such that Example1 For every $\varepsilon \gt 0$, there is an integer N such that $n \ge N$ implies that $d(p_{n},p)&amp;lt;\varepsilon$. Every positive real number $\varepsilon$, there exists an integer $N$ such that whenever $n$ is greater than some integer $N$, $d(p_{n},p) \lt \varepsilon$ holds. $$ \forall \varepsilon \gt 0, \exists N \in \mathbb{N}\quad \text{s.t. } n\ge N \implies d(p_{n},p) \lt \varepsilon $$ Explanation $\forall$ It means</description></item><item><title>Physical Pendulum</title><link>https://freshrimpsushi.github.io/en/posts/1765/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1765/</guid><description>Definition1 A physical pendulum refers to a rigid body swinging about a fixed horizontal axis due to gravity. Physical Pendulum Pendulum motion is a type of harmonic oscillation. The magnitude of the torque acting on the center of mass is as follows: $$ \begin{align*} N &amp;amp;=\left| \mathbf{r} \times \mathbf{F} \right| \\ &amp;amp;= rF\sin\theta \\ &amp;amp;=lmg \sin\theta \end{align*} $$ Expressing the torque in terms of moment of inertia, we get the</description></item><item><title>Multi-Resolution Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1762/</link><pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1762/</guid><description>Definition If a sequence of closed subspaces $L^{2}(\mathbb{R})$ and a function $\phi \in V_{0}$ satisfy the following conditions, then $\left( \left\{ V_{j} \right\}, \phi \right)$ is called a multiresolution analysis. (a) For each $V_{j}$, $\cdots V_{-1} \subset V_{0} \subset V_{1}\cdots$ holds. (b) $\overline{\cup_{j\in\mathbb{Z}}V_{j}}=L^{2}(\mathbb{R})$ and $\cap_{j\in\mathbb{Z}}V_{j}=\left\{ 0\right\}$. (c) $\forall j\in \mathbb{Z}$, $V_{j+1}=D(V_{j})$. (d) If $\forall k \in \mathbb{Z}$, $f \in V_{0}$, then $T_{k}f \in V_{0}$. (e) $\left\{ T_{k} \phi\right\}_{k\in \mathbb{Z}}$ is</description></item><item><title>Multi-Resolution Analysis Scaling Equation</title><link>https://freshrimpsushi.github.io/en/posts/1763/</link><pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1763/</guid><description>정리 함수 $\phi \in L^{2}(\mathbb{R})$가 multiresolution analysis을 생성한다고 하자. 그러면 아래의 식을 만족하는 주기가 $1$인 함수 $H_{0}\in L^{2}</description></item><item><title>How to Initialize the Workspace and Remove All Variables in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1758/</link><pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1758/</guid><description>Method clear Command Typing clear in the command window will reset the workspace. Clearing Workspace (Alt+T+O) Right-clicking on the workspace window allows you to select &amp;lsquo;Clear Workspace (O)&amp;rsquo;. Pressing it resets the workspace. This can also be done using the shortcut Alt+T+O, but it does not work when an editor is opened. Deleting by Selection You can delete by dragging to select everything or by pressing Ctrl+a to select all</description></item><item><title>Differential Volume in Cylindrical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1753/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1753/</guid><description>Formulas The infinitesimal volume in spherical coordinates is given by: $$ dV=r^{2}\sin\theta dr d\theta d\phi $$ The infinitesimal surface area on a sphere can be obtained without multiplying $dr$ by: $$ da=\color{blue}{rd\theta} \cdot \color{red}{r\sin\theta d \phi}=r^{2}\sin\theta d\theta d\phi $$</description></item><item><title>Infinitesimal Area in Polar Coordinates, Infinitesimal Volume in Cylindrical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1755/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1755/</guid><description>Formula In polar coordinates, the infinitesimal area is as follows. $$ dA=rdrd\theta $$ In cylindrical coordinates, the infinitesimal volume and the infinitesimal surface area of a cylinder are as follows. $$ dV=\rho d\rho d\phi dz \\ dA=\rho d\phi dz $$ Description Polar Coordinates $\mathbf{r}=\mathbf{r}(r,\theta)$ The infinitesimal area, as shown in the figure, is (length of the green line)$\times$(length of the blue line). The green line represents the infinitesimal change in</description></item><item><title>Curl of Vector Functions in 3D Cartesian Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1752/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1752/</guid><description>Definition For a vector function $\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\hat{\mathbf{x}} + F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$, the following vector is defined as the curl of $\mathbf{F}$, denoted as $\nabla \times \mathbf{F}$. $$ \begin{align} \nabla \times \mathbf{F} &amp;amp;= \left( \dfrac{ \partial F_{z}}{ \partial y }-\dfrac{ \partial F_{y}}{ \partial z} \right)\hat{\mathbf{x}}+ \left( \dfrac{ \partial F_{x}}{ \partial z }-\dfrac{ \partial F_{z}}{ \partial x} \right)\hat{\mathbf{y}}+ \left( \dfrac{ \partial F_{y}}{ \partial x }-\dfrac{ \partial F_{x}}{ \partial y} \right)\hat{\mathbf{z}} \label{def1} \\ &amp;amp;=\begin{vmatrix}</description></item><item><title>Double and Half Angle Formulas for Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1748/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1748/</guid><description>Formulas Double Angle Formula: $$ \begin{align} \sinh (2x) =&amp;amp;\ 2\sinh x \cosh x \label{1} \\ \cosh (2x) =&amp;amp;\ \cosh^{2} x + \sinh^{2} x = 2\cosh ^{2 } x -1 = 2\sinh ^{2} x +1 \\ \tanh (2x) =&amp;amp;\ \frac{2\tanh x}{1+\tanh^{2}x} \end{align} $$ Half Angle Formula: $$ \begin{align} \sinh^{2} \frac{x}{2} =&amp;amp;\ \frac{\cosh x -1 }{2} \\ \cosh^{2} \frac{x}{2} =&amp;amp;\ \frac{\cosh x +1 }{2} \\ \tanh ^{2} \frac{x}{2} =&amp;amp;\ \frac{\cosh x -1}{\cosh</description></item><item><title>Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1749/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1749/</guid><description>Definition Regarding $z \in \mathbb{C}$, $$ \begin{align*} \sinh z &amp;amp;:= \frac{e^{z}-e^{-z}}{2} \\ \cosh z &amp;amp;:= \frac{e^{z}+e^{-z}}{2} \\ \tanh z &amp;amp;:= \frac{\sinh z}{\cosh z} \end{align*} $$ $$ \begin{align*} \mathrm{csch}x&amp;amp;=\frac{1}{\sinh x} \\ \mathrm{sech} x&amp;amp;=\frac{1}{\cosh x} \\ \coth x &amp;amp;=\frac{1}{\tanh x} \end{align*} $$ Relationship with Trigonometric Functions $$ \begin{align*} \sinh (iz) &amp;amp;= i\sin z \\ \sin (iz) &amp;amp;= i\sinh z \\ \cosh (iz) &amp;amp;= \cos z \\ \cos (iz) &amp;amp;= \cosh z \end{align*}</description></item><item><title>Sum and Difference Formulas and Multiplication Formulas for Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1750/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1750/</guid><description>Formulas Sum and Difference Formulas: $$ \begin{align} \sinh x +\sinh y =&amp;amp;\ 2\sinh \left(\frac{x+y}{2}\right) \cosh \left(\frac{x-y}{2}\right) \\[1em] \sinh x -\sinh y =&amp;amp;\ 2\sinh \left(\frac{x-y}{2}\right) \cosh \left( \frac{x+y}{2} \right) \\[1em] \cosh x + \cosh y =&amp;amp;\ 2 \cosh \left(\frac{x+y}{2}\right) \cosh \left(\frac{x-y}{2}\right) \\[1em] \cosh x -\cosh y =&amp;amp;\ 2 \sinh \left( \frac{x+y}{2} \right) \sinh \left(\frac{x-y}{2}\right) \end{align} $$ Product Formulas: $$ \begin{align} \sinh x \sinh y =&amp;amp;\ \frac{\cosh (x+y)-\cosh (x-y)}{2} \\ \sinh x</description></item><item><title>Forced Harmonic Vibration and Resonant Frequency</title><link>https://freshrimpsushi.github.io/en/posts/1742/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1742/</guid><description>Forced Harmonic Oscillation1 Harmonic motion, such as the movement of an object hanging from a spring, occurs when no external forces other than the spring constant $k$&amp;rsquo;s restoring force are acting. This is called simple harmonic oscillation when excluding other external forces like air resistance or any friction. If there is an external force proportional to the velocity, like friction, it&amp;rsquo;s referred to as damped harmonic oscillation. When an external</description></item><item><title>Hyperbolic Functions' Identities</title><link>https://freshrimpsushi.github.io/en/posts/1744/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1744/</guid><description>Formulas $$ \begin{align} \sinh(-x) =&amp;amp;\ -\sinh x \\ \cosh(-x) =&amp;amp;\ \cosh x \\ \tanh(-x) =&amp;amp;\ - \tanh x \\ \cosh x + \sinh x =&amp;amp;\ e^{x} \\ \cosh x - \sinh x =&amp;amp;\ e^{-x} \\ \cosh^{2}x -\sinh^{2}x =&amp;amp;\ 1 \end{align} $$ Explanation There&amp;rsquo;s really no proof needed. This can be directly known from the definition. Proof Proof of $(1)$ $$ \begin{align*} \sinh(-x) =&amp;amp;\ \frac{e^{-x}-e^{x}}{2} \\ =&amp;amp;-\frac{e^{x}-e^{-x}}{2} \\ =&amp;amp;-\sinh x \end{align*}</description></item><item><title>Proof of the Addition Formulas for Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1743/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1743/</guid><description>Formula $$ \begin{align} \sinh (x\pm y) =&amp;amp;\ \sinh x \cosh y \pm \sinh y \cosh x \\ \cosh (x \pm y) =&amp;amp;\ \cosh x \cosh y \pm \sinh x \sinh y \\ \tanh{x \pm y}&amp;amp;=\frac{\tanh x \pm \tanh y}{1 \pm \tanh x \tanh y} \end{align} $$ Description Thinking about the relationship between hyperbolic and trigonometric functions makes it natural that their forms are similar to the addition theorem of trigonometric</description></item><item><title>Principle of the Compton Camera</title><link>https://freshrimpsushi.github.io/en/posts/1740/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1740/</guid><description>Principles The Compton camera uses Compton scattering to locate materials emitting gamma rays. It is also called the Compton telescope or Compton imager. On the right side of the figure, the Compton camera is simply represented by two detectors. The detectors measure the energy of the gamma rays, with scattering of the gamma rays occurring at the first detector. The black square on the left side of the figure is</description></item><item><title>What is Tomography?</title><link>https://freshrimpsushi.github.io/en/posts/1741/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1741/</guid><description>What is Tomography?1 Tomography, derived from the Greek words τoμoσ and γραψετε, translates to slicing and recording in Korean. The images we see after a CT scan are essentially cross-sectional pictures2 of the body. In tomography, opaque objects are penetrated with signals in the form of waves or particles to gather information about their internal structure. The signals commonly</description></item><item><title>Damped Harmonic Oscillation</title><link>https://freshrimpsushi.github.io/en/posts/1736/</link><pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1736/</guid><description>Damped Harmonic Oscillation1 When the spring constant is denoted as $k$, the equation of motion for a simple harmonic oscillator is as follows. $$ m \ddot {x}+kx=0 $$ The simple harmonic motion only considers the restoring force by the spring. However, in reality, other external forces such as frictional forces also affect the motion of the object, so they cannot be ignored. So, let&amp;rsquo;s assume there is a frictional force</description></item><item><title>Connected Sets in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1729/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1729/</guid><description>Definition If two subsets $A$ and $B$ of a metric space $X$ satisfy $$ A \cap \overline{B}= \varnothing \quad \text{and} \quad \overline{A}\cap B= \varnothing $$ then $A$ and $B$ are said to be separated. In other words, there is no point of $A$ included in the closure of $B$, and there is no point of $B$ included in the closure of $A$. A subset $E \subset X$ that cannot be</description></item><item><title>Continuity and Compactness in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1724/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1724/</guid><description>Theorem Let $X$ be a compact metric space, $Y$ be a metric space, and $f:X\to Y$ be continuous. Then $f(X)$ is compact. The compactness condition cannot be omitted. Proof Let $\left\{ O_\alpha \right\}$ be an open cover of $f(X)$. Since $f$ is continuous, by the equivalence condition, each preimage $f^{-1}(O_{\alpha})$ is also an open set in $X$. Therefore, $\left\{ f^{-1}(O_{\alpha}) \right\}$ is an open cover of $X$, and since $X$</description></item><item><title>Limit of Functions in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1719/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1719/</guid><description>Definition Let $(X,d_{X})$, $(Y,d_{Y})$ be a metric space. Suppose $E\subset X$, $f: E\rightarrow Y$, and that $p$ is a limit point of $E$. Then, for every positive number $\varepsilon$, $$ x \in E \ \text{and} \ d_{X}(x,p)&amp;lt;\delta \implies d_{Y}(f(x),q) &amp;lt;\varepsilon $$ exists $\delta&amp;gt;0$ such that, $$ f(x)\rightarrow q\ \mathrm{as}\ x\to p $$ or $$ \lim \limits_{x\to p}f(x)=q $$ is denoted and $f$ is said to have the limit $q$ at</description></item><item><title>Maximum and Minimum Theorem in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1725/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1725/</guid><description>Theorem Let $X$ be a compact metric space, and let $f : X \to \mathbb{R}$ be continuous. Then, it is as follows. $$ M = \sup \limits_{x\in X} f(x),\quad m=\inf \limits_{x \in X}f(x) $$ Then, $$ M=f(p),\quad m=f(q) $$ there exists a $q,p\in X$ that satisfies this. In other words: for every $x$, $$ f(q)\le f(x) \le f(p) $$ there exists a $q,p \in X$ that satisfies this. This is</description></item><item><title>Proof that Continuous Functions on Compact Metric Spaces are Uniformly Continuous</title><link>https://freshrimpsushi.github.io/en/posts/1727/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1727/</guid><description>Theorem Let $(X,d_{X})$ be a compact metric space, $(Y,d_{Y})$ a metric space, and $f:X\to Y$ continuous. Then, $f$ is uniformly continuous on $X$. Explanation The condition of being compact cannot be omitted. Proof Suppose we are given any positive number $\varepsilon &amp;gt;0$. Since $f$ is assumed to be continuous, by definition, for each point $p\in X$, there exists a positive number $\delta_{p}$ satisfying the following equation: $$ \forall q\in X,\quad</description></item><item><title>Properties of Continuous Functions in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1723/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1723/</guid><description>Theorem: Real Functions Let two functions $f$, $g$ be functions from a metric space $X$ to the complex numbers. $$ f:X \to \mathbb{C},\quad g:X \to \mathbb{C} $$ If the two functions are continuous, then $f+g$, $fg$, $f/g$ are also continuous. However, in the last case, it only holds for $g(x)\ne 0$ being $x\in X$. Proof Lemma 1 Let $(X,d)$ be a metric space, $E\subset X$ a subset, and $p$ an</description></item><item><title>Properties of Limits of Functions in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1720/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1720/</guid><description>Theorem 1 Let $(X,d)$ be a metric space, $E\subset X$ a subset, and $p$ an accumulation point of $E$. Suppose two complex-valued functions defined on $E$, $f:E\to \mathbb{C}$ and $g: E\to \mathbb{C}$, are given. Furthermore, assume that these two functions have the following limits at $p$. $$ \begin{equation} \lim \limits_{x \to p}f(x)=A \quad \text{and} \quad \lim \limits_{x \to p}g(x)=B \tag{1} \label{thm1} \end{equation} $$ Then, the following holds. (a) $\lim \limits_{x</description></item><item><title>The Composition of Continuous Functions in Metric Spaces Preserves Continuity</title><link>https://freshrimpsushi.github.io/en/posts/1721/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1721/</guid><description>Theorem Let there be three metric spaces $(X,d_{X})$, $(Y,d_{Y})$, $(Z,d_{Z})$. Assume that $E\subset X$ and there are two functions $f:E\to Y$, $g:f(E) \to Z$. Also, let $h : E \to Z$ defined in $E$ be as follows. $$ h(x) = g(f(x))\quad \forall x \in E $$ If $f$ is continuous at $p\in E$ and $g$ is continuous at $f(p)\in f(E)$, then $h$ is also continuous at $p$. Here, $h$ is</description></item><item><title>The Importance of Compactness Conditions in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1728/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1728/</guid><description>Overview Many theorems in analysis require compactness as a necessary condition (Reference1, Reference2, Reference3, Reference4). While it&amp;rsquo;s taken for granted that &amp;lsquo;if it&amp;rsquo;s compact, then it&amp;rsquo;s fine&amp;rsquo; due to the assumption of compactness in the proof process, one might wonder why &amp;lsquo;if it&amp;rsquo;s not compact, then it&amp;rsquo;s not fine&amp;rsquo;. If the condition of being compact is absent, the following undesirable situations can occur. Theorem1 Let&amp;rsquo;s say $E\subset \mathbb{R}$ is a</description></item><item><title>The inverse of a continuous bijection on a compact metric space is continuous.</title><link>https://freshrimpsushi.github.io/en/posts/1726/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1726/</guid><description>Theorem Let $X$ be a compact metric space, and let $Y$ be a metric space. Assume that $f : X \to Y$ is a bijective continuous function. Then, the inverse $f^{-1}$ of $f$, defined as follows, is bijective and continuous. $$ f^{-1} (f(x))=x, \quad x\in X $$ The compactness condition is essential Proof Equivalent conditions for continuity in metric spaces For two metric spaces $(X,d_{X})$ and $(Y,d_{Y})$, let $f :</description></item><item><title>거리공간에서 연속함수일 동치 조건</title><link>https://freshrimpsushi.github.io/en/posts/1722/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1722/</guid><description>Theorem 1 For two metric spaces $(X,d_{X})$, $(Y,d_{Y})$, let $E\subset X$ and $p \in E$, $f : E \to Y$. Then, the following three statements are equivalent. (1a) $f$ is continuous at $p$. (1b) $ \lim \limits_{x \to p} f(x)=f(p)$. (1c) For $\lim \limits_{n\to\infty} p_{n}=p$, $\left\{ p_{n} \right\}$, it follows that $\lim \limits_{n\to\infty} f(p_{n})=f(p)$. Proof (1a) $\iff$ (1b) By the definition of a limit and continuity, it is trivial. ■</description></item><item><title>Convergence of Cauchy Sequences in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1718/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1718/</guid><description>Definition Let $\left\{ p_{n} \right\}$ be a sequence of points in a metric space $(X,d)$. If for every positive number $\varepsilon$, there exists a positive number $N$ such that $$ n\ge N,\ m\ge N \implies d(p_{n},p_{m})&amp;lt;\varepsilon $$ is satisfied, then $\left\{ p_{n} \right\}$ is called a Cauchy sequence. If every Cauchy sequence in a metric space $X$ converges to a point in $X$, then $X$ is called a complete space.</description></item><item><title>Convergence of Sequences in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1713/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1713/</guid><description>Definitions1 If there exists a point $p \in X$ such that the sequence $\left\{ p_{n} \right\}$ of points in a metric space $(X,d)$ satisfies the following condition, the sequence $\left\{ p_{n} \right\}$ is said to converge to $p$, and it is denoted by $p_{n} \rightarrow p$ or $\lim \limits_{n\to \infty}p_{n}=p$. $$ \forall \varepsilon &amp;gt;0,\ \exists N\in \mathbb{N}\ \mathrm{s.t}\ n\ge N \implies d(p_{n},p)&amp;lt;\varepsilon $$ If $\left\{ p_{n} \right\}$ does not converge,</description></item><item><title>Diameter of a Set in a Metric Space</title><link>https://freshrimpsushi.github.io/en/posts/1717/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1717/</guid><description>Definition1 Let $E$ be a subset of the metric space $(X,d)$. And suppose $S$ is as follows. $$ S=\left\{ d(p, q) : \forall p, q \in E\right\} $$ Then, the least upper bound $\sup S$ of $S$ is called the diameter of $E$ and is denoted by $\operatorname{diam} E$. Explanation Let $\left\{ p_{n} \right\}$ be a sequence in the metric space $X$, called $E_{N}=\left\{ p_{N},p_{N+1},p_{N+2},\cdots \right\}$. Then, by the definition</description></item><item><title>Every Non-Empty Perfect Set in Euclidean Space is Uncountable</title><link>https://freshrimpsushi.github.io/en/posts/1712/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1712/</guid><description>Definition Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. The set that includes all $q$s that satisfy $d(q,p)&amp;lt;r$ is defined as the neighborhood of point $p$ and denoted by $N_{r}(p)$. In this case, $r$ is called the radius of $N_{r}(p)$. It can also be denoted as $N_{p}$ when it&amp;rsquo;s okay to omit the distance. If every neighborhood of $p$ contains a $q$ that</description></item><item><title>Properties of Converging Real Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1714/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1714/</guid><description>Theorem 1[^1] Let $\left\{ s_{n} \right\}$, $\left\{ t_{n} \right\}$ be sequences of real (or complex) numbers and assume that $\lim \limits_{n\to\infty} s_{n}=s$, $\lim\limits_{n\to\infty}t_{n}=t$. Then (a) $\lim \limits_{n\to\infty}(s_{n}+t_{n})=s+t$ (b) $\forall c \in \mathbb{C},\quad\lim \limits_{n\to\infty} cs_{n}=cs \quad \text{and} \quad \lim \limits_{n\to\infty} (c+s_{n})=c+s$ (c) $\lim \limits_{n\to\infty} s_{n}t_{n}=st$ (d) $\forall s_{n}\ne 0,s\ne0,\quad \lim \limits_{n\to\infty}\frac{1}{s_{n}}=\frac{1}{s}$ Of course, this can be extended to $\mathbb{R}^{k}$ as well. See Theorem 2 for a deeper look. Proof (a) Given</description></item><item><title>Every k Cell is Compact</title><link>https://freshrimpsushi.github.io/en/posts/1711/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1711/</guid><description>Definition For $a_{i},b_{i} \in \mathbb{R} (1\le i \le k)$, the set $I=[a_{1},b_{1}] \times [a_{2},b_{2}]\times \cdots \times [a_{k},b_{k}]$ is called a $k$-cell. Here, $\times$ represents the Cartesian product of sets. Theorem 1 Let&amp;rsquo;s assume a sequence of closed intervals on $\mathbb{R}$, $\left\{ I_{n} \right\}$, satisfies $I_{n}\supset I_{n+1}\ (n=1,2,\cdots)$. Then, the following holds true. $$ \bigcap_{i=1}^{\infty}I_{n}\ne \varnothing $$ Proof Let&amp;rsquo;s denote $I_{n}=[a_{n},b_{n}]$. Also, let $E=\left\{ a_{n} : n=1,2,\cdots \right\}$. Then, $E\ne \varnothing$</description></item><item><title>Generalized Cantor's Intersection Theorem in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1710/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1710/</guid><description>Theorem 1 Let&amp;rsquo;s assume that $(X,d)$ is a metric space. $K_{n}\subset X (n=1,2,\cdots)$ is a non-empty compact subset. In this case, if $\left\{ K_{n} \right\}$ $$ K_{n}\supset K_{n+1}\ (n=1,2,\cdots) $$ is satisfied, then $\bigcap _{i=1}^{\infty} K_{n} \ne \varnothing$ is true. If we set $\left\{ K_{n} \right\}$ as above, it has the finite intersection property, and therefore it immediately applies as a corollary of the theorem shown below. When set to</description></item><item><title>Closed Subsets of Compact Sets in Metric Spaces are Compact</title><link>https://freshrimpsushi.github.io/en/posts/1706/</link><pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1706/</guid><description>Theorem1 In a metric space $X$, a closed (relative to $X$) subset of a compact set $K$ is compact. Proof Given a metric space $X$ where $F\subset K \subset X$ and assuming $F$ is a closed set in $X$ and $K$ is a compact set. Let $\left\{ V_{\alpha}\right\}$ be an arbitrary open cover of $F$. By adding $F^{c}$, let&amp;rsquo;s denote it as $\Omega=\left\{ V_\alpha \right\}\cup \left\{ F^{c} \right\}$. Then $\Omega$</description></item><item><title>Compactness in Metric Space</title><link>https://freshrimpsushi.github.io/en/posts/1705/</link><pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1705/</guid><description>Definition Open Cover Given a metric space $(X,d)$ and a subset $E\subset X$, a set of open sets $\left\{ O_{\alpha} \right\}$ that satisfies the following equation $X$ is called an open cover of $E$. $$ E\subset \bigcup _{\alpha} O_{\alpha} $$ A subset of an open cover is called a subcover. Specifically, a subcover with a finite number of elements is called a finite subcover. Compact Given a subset $K$ of</description></item><item><title>Relatively Open Sets in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1703/</link><pubDate>Sat, 15 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1703/</guid><description>Explanation Let&amp;rsquo;s say there are two metric spaces $Y\subset X$. And suppose a subset $E \subset Y \subset X$ is given. If $E$ is open with respect to the entire space $X$, then by the definition of being open and interior points, $E$ remains an open set even when $Y$ is considered the entire space. This is because the situation involves a reduction of the whole set hence there is</description></item><item><title>Properties of Open and Closed Sets in Metric Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1702/</link><pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1702/</guid><description>Let $(X,d)$ be a metric space. Suppose $p \in X$ and $E \subset X$. The set that contains all $q$s satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhood of point $p$ and is denoted by $N_{r}(p)$. Here $r$ is called the radius of $N_{r}(p)$. When it&amp;rsquo;s possible to omit the metric, it can also be denoted as $N_{p}$. If every neighborhood of $p$ contains $q$s with $q\ne p$ and $q\in E$,</description></item><item><title>Closure and Derived Set in Metric Space</title><link>https://freshrimpsushi.github.io/en/posts/1701/</link><pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1701/</guid><description>Definitions Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. A set that contains all $q$ satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhood of point $p$, denoted by $N_{r}(p)$. In this case, $r$ is called the radius of $N_{r}(p)$. If it&amp;rsquo;s permissible to omit the distance, it can also be denoted as $N_{p}$. If every neighborhood of $p$ includes a $q$ that is $q\ne</description></item><item><title>Neighborhood, Limit Point, Open, Closed in Metric Space</title><link>https://freshrimpsushi.github.io/en/posts/1700/</link><pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1700/</guid><description>Definition Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. The set that includes all $q$s satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhood of point $p$ and is denoted as $N_{r}(p)$. Here, $r$ is called the radius of $N_{r}(p)$. If the distance can be omitted, it may also be denoted as $N_{p}$. If all neighborhoods of $p$ contain $q$, which is $q\ne p$ and</description></item><item><title>Integrable Functions and Absolute Values</title><link>https://freshrimpsushi.github.io/en/posts/1697/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1697/</guid><description>This article is based on Riemann-Stieltjes integration. If set as $\alpha=\alpha (x)=x$, it equals the Riemann integration. Theorem1 Let function $f$ be Riemann(-Stieltjes) integrable over the interval $[a,b]$. Then (a) $\left|f\right|$ is also integrable over $[a,b]$. (b) Furthermore, the following inequality holds: $$ \left|\int_{a}^{b}fd\alpha \right| \le \int_{a}^{b}\left| f\right| d\alpha $$ Proof (a) Integrability is defined for bounded functions. Hence, assuming that $f$ is integrable implies that $f$ is bounded. Let&amp;rsquo;s</description></item><item><title>Riemann-Stieltjes Integrability is Preserved within an Interval</title><link>https://freshrimpsushi.github.io/en/posts/1695/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1695/</guid><description>The following document is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem 1 Let function $f$ be Riemann(-Stieltjes) integrable on $[a,b]$. Let us also say $a&amp;lt;c&amp;lt;b$. Then, $f$ is also integrable on $[a,c]$ and $[c,b]$, and the sum of the integration values is equal to the integral on $[a,b]$. $$ \int_{a}^{c}fd\alpha + \int_{c}^{b}fd\alpha=\int_{a}^{b}f d\alpha $$ Proof In the first</description></item><item><title>The Fundamental Theorem of Calculus in Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1698/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1698/</guid><description>Theorem1 Let&amp;rsquo;s say $f$ is a function that is Riemann integrable over the interval $[a,b]$. And let&amp;rsquo;s define $F$ for $a\le x \le b$ as follows. $$ F(x) = \int _{a} ^{x} f(t)dt $$ (a) Then, $F$ is continuous over $[a,b]$. (b) If $f$ is continuous over $x_{0}\in [a,b]$, then $F$ is differentiable over $x_{0}$ and satisfies $F^{\prime}(x_{0})=f(x_{0})$. Explanation This is known by the name Fundamental Theorem of Calculus 1,</description></item><item><title>The Relationship Between the Size of Integrals Based on the Order of Functions</title><link>https://freshrimpsushi.github.io/en/posts/1696/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1696/</guid><description>This article is based on Riemann-Stieltjes integration. If we set it as $\alpha=\alpha (x)=x$, it is the same as Riemann integration. Theorem1 Let us assume that two functions $f_{1}, f_{2}$ are Riemann(-Stieltjes) integrable over the interval $[a,b]$. Also, assume that $f_{1} \le f_{2}$ in $[a,b]$. Then, the following inequality holds. $$ \int_{a}^{b}f_{1}d\alpha \le \int_{a}^{b}f_{2}d\alpha $$ Proof Let there be a positive $\varepsilon &amp;gt;0$. Since $f_{2}$ is integrable, by the necessary</description></item><item><title>Day, Work-Energy Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1694/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1694/</guid><description>Definition A work is defined as the product of the magnitude of a force $\mathbf{F}$ and the distance $s$ by which an object has moved in the same direction as the force $W=Fs$, when the force $\mathbf{F}$ acts on the object. Description In the distance-force graph, the area under the graph is equal to the amount of work. Since the direction of movement and the direction of the force must</description></item><item><title>Formula for the Roots of a Cubic Equation</title><link>https://freshrimpsushi.github.io/en/posts/1692/</link><pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1692/</guid><description>Formulas The solution of the cubic equation $t^{3}+pt+q = 0$ is as follows. $$ \begin{cases} t_{1}=u_{1}+v_{1}=\sqrt[3]{-\frac{q}{2}+\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}+\sqrt[3]{-\frac{q}{2}-\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}} \\ t_{2}=u_{2}+v_{3}=\sqrt[3]{-\frac{q}{2}+\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}\omega+\sqrt[3]{-\frac{q}{2}-\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}\omega^{2} \\ t_{3}=u_{3}+v_{2}=\sqrt[3]{-\frac{q}{2}+\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}\omega^{2}+\sqrt[3]{-\frac{q}{2}-\sqrt{\frac{q^{2}}{4}+\frac{p^{3}}{27}}}\omega\end{cases} $$ Here $\omega = e^{i\frac{2}{3}\pi}$. Proof Cardano&amp;rsquo;s Method Let us consider a cubic equation $ax^{3}+bx^{2}+cx+d=0(a\ne0)$ is given. To simplify the solution, without loss of generality, it is shown as follows. $$ \begin{equation} x^{3}+ax^{2} +bx+c=0 \end{equation} $$ To eliminate the quadratic term, substitute $x=t-{\textstyle \frac{a}{3}}$. Then it becomes: $$ \begin{align*} &amp;amp;&amp;amp;\left(</description></item><item><title>Relationships between the Roots and Coefficients of Quadratic/Tertiary/nth Degree Equations</title><link>https://freshrimpsushi.github.io/en/posts/1691/</link><pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1691/</guid><description>Formulas Relationship Between Roots and Coefficients of a Quadratic Equation Let&amp;rsquo;s consider the roots of the quadratic equation $ax^{2}+bx+c=0$ to be $\alpha$ and $\beta$. Then the following relationship holds true. $$ \alpha+\beta=-\frac{b}{a}\quad \&amp;amp; \quad \alpha\beta= \frac{ c}{a} $$ Relationship Between Roots and Coefficients of a Cubic Equation Let&amp;rsquo;s consider the roots of the cubic equation $ax^{3}+bx^{2}+cx+d=0$ to be $\alpha$, $\beta$, and $\gamma$. Then the following relationship holds true. $$ \alpha</description></item><item><title>Kepler's First Law: The Law of Elliptical Orbits</title><link>https://freshrimpsushi.github.io/en/posts/1689/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1689/</guid><description>Kepler&amp;rsquo;s First Law: The Law of Elliptical Orbits Planets revolve in elliptical orbits with the Sun at one focus. This is the first law among Kepler&amp;rsquo;s laws of planetary motion. Proof1 The equation of orbit for a particle moving under a central force is as follows. $$ \frac{ d ^{2}u}{ d \theta^{2} } + u=-\frac{1}{ml^{2}u^{2}}F(u^{-1}) $$ Here, $u={\textstyle \frac{1}{r}}$. Since we want to solve the problem with respect to gravity,</description></item><item><title>Kepler's Third Law: The Harmony of the Worlds</title><link>https://freshrimpsushi.github.io/en/posts/1690/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1690/</guid><description>Kepler&amp;rsquo;s Third Law: The Law of Harmonies The square of the orbital period of a planet is proportional to the cube of the semi-major axis of its orbit. Kepler&amp;rsquo;s third law among Kepler&amp;rsquo;s laws of planetary motion. When approximating the orbit of a planet as a circle, it becomes &amp;lsquo;The square of the orbital period is proportional to the cube of the distance to the sun&amp;rsquo;. Proof1 Let the area</description></item><item><title>Elliptic Integral of the Second Kind</title><link>https://freshrimpsushi.github.io/en/posts/1687/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1687/</guid><description>Definition The integral below is referred to as the complete elliptic integral of the second kind. $$ E(k)=\int_{0}^{{\textstyle \frac{\pi}{2}}}\sqrt{1-k^{2} \sin ^{2} \theta} d\theta $$ The integral below is referred to as the incomplete elliptic integral of the second kind. $$ E(\phi, k)=\int_{0}^{\phi}\sqrt{1-k^{2} \sin ^{2} \theta}d\theta $$ Explanation The reason why the above two integrals are named elliptic integrals is that they emerge from the process of calculating the perimeter of</description></item><item><title>Perimeter of an Ellipse</title><link>https://freshrimpsushi.github.io/en/posts/1688/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1688/</guid><description>Most materials do not provide a detailed explanation of how the elliptic integral of the second kind is derived. Even if they do, many are incorrect1 so I wrote the &amp;lsquo;accurate&amp;rsquo; and &amp;lsquo;detailed&amp;rsquo; content myself. For reference, the content in Boas&amp;rsquo; Mathematical Methods in the Physical Sciences, 3rd edition, is also incorrect. Formula The perimeter of an ellipse with semi-major axis $a$, semi-minor axis $b$, and eccentricity $k^{2}$ is calculated</description></item><item><title>Ellipse</title><link>https://freshrimpsushi.github.io/en/posts/1685/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1685/</guid><description>Definition The set of points on a plane whose sum of distances to two fixed points $F$, $F^{\prime}$ is constant, are called an ellipse. The components of an ellipse are as follows. $F$, $F^{\prime}$ are called foci. $a$ is called the semimajor axis, and $b$ is called the semiminor axis. $b=\sqrt{1-\epsilon^{2}}a$ is satisfied. $\epsilon$ is called the eccentricity of the ellipse. It represents how ellipsed is compressed, and the foci</description></item><item><title>Equation of an Ellipse with the Focus at the Origin in Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1686/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1686/</guid><description>Theorem The equation of an ellipse in polar coordinates is given as follows. $$ r=\frac{\alpha}{1+\epsilon \cos \theta}\tag{a} $$ or $$ r=\frac{b^{2}/a}{1+\frac{\sqrt{a^{2}-b^{2}}}{a}\cos\theta} \tag{b} $$ Where $\alpha$ is the focal parameter, $\epsilon$ is the eccentricity, $a$ is the semi-major axis, and $b$ is the semi-minor axis. Explanation The two proofs below are essentially the same. Proof High School Level The definition of an ellipse is the set of points where the sum</description></item><item><title>Orbit Equation of a Particle under Central Force</title><link>https://freshrimpsushi.github.io/en/posts/1684/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1684/</guid><description>Orbit Equation of a Particle Subject to Central Force1 A particle of mass $m$ subject to a central force can be described by its motion equation in polar coordinates as follows. $$ \begin{equation} m\ddot{\mathbf{r}}=F(r)\hat{\mathbf{r}} \label{eq1} \end{equation} $$ $F(r)$ represents the central force acting on the particle. The acceleration in polar coordinates can be expressed as: $$ \ddot{\mathbf{r}}=\mathbf{a}=\left( \ddot{r}-r\dot{\theta}{}^{2} \right)\hat{\mathbf{r}} +\left(2\dot{r}\dot{\theta}+r\ddot{\theta} \right)\hat{\boldsymbol{\theta}} $$ Hence, separating the motion equation $\eqref{eq1}$ into components</description></item><item><title>Derivation of the Equation of an Ellipse</title><link>https://freshrimpsushi.github.io/en/posts/1683/</link><pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1683/</guid><description>Formulas The equation of an ellipse with the center at $(x_{0},y_{0})$, major axis $a$, and minor axis $b$ is as follows. $$ \frac{(x-x_{0})^{2}}{a^{2}}+\frac{(y-y_{0})^{2}}{b^{2}}=1 $$ Description An ellipse is a set of points where the sum of the distances to two foci is constant. Derivation Let&amp;rsquo;s consider an ellipse as shown in the figure above. Based on the definition of an ellipse, we can establish the following equation. $$ \begin{align*} \overline{F^{\prime}P}</description></item><item><title>Kepler's Second Law: The Law of Equal Areas</title><link>https://freshrimpsushi.github.io/en/posts/1682/</link><pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1682/</guid><description>Kepler&amp;rsquo;s Second Law: Law of Equal Areas1 A line segment joining a planet and the Sun sweeps out equal areas during equal intervals of time. The Law of Equal Areas is the second of Kepler&amp;rsquo;s laws of planetary motion. However, this is not a special law that occurs only between the Sun and the planets, but a general law that is valid for any object (particle) moving under a central</description></item><item><title>The Magnitude of the Cross Product of Two Vectors is Equal to the Area of the Parallelogram They Form</title><link>https://freshrimpsushi.github.io/en/posts/1681/</link><pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1681/</guid><description>Theorem The magnitude of the cross product of two vectors $\mathbf{A}$ and $\mathbf{B}$, when the angle between them is $\theta$, is as follows: $$ \left| \mathbf{A}\times \mathbf{B}\right| =\left|\mathbf{A}\right|\left| \mathbf{B} \right|\sin \theta $$ And this is equal to the area of the parallelogram that the two vectors form. Proof Let&amp;rsquo;s say the two vectors $\mathbf{A}=(A_{x},A_{y},A_{z})$ and $\mathbf{B}=(B_{x},B_{y},B_{z})$ are as shown in the figure above. Then part 1. Area of the parallelogram</description></item><item><title>Kepler's Laws of Planetary Motion</title><link>https://freshrimpsushi.github.io/en/posts/1680/</link><pubDate>Tue, 21 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1680/</guid><description>Kepler&amp;rsquo;s Laws of Planetary Motion1 The Kepler&amp;rsquo;s laws are empirical laws formulated by compiling and analyzing the observation records of astronomers such as Tycho Brahe and Hipparchus. They greatly contributed to the derivation of Newton&amp;rsquo;s Law of Universal Gravitation. Conversely, Kepler&amp;rsquo;s laws could be mathematically explained through the Law of Universal Gravitation and Newton&amp;rsquo;s Laws of Motion. First Law: The Law of Ellipses The orbit of a planet is an</description></item><item><title>Uniform Sphere Shell and the Gravity of a Separated Particle</title><link>https://freshrimpsushi.github.io/en/posts/1679/</link><pubDate>Tue, 21 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1679/</guid><description>Uniform Spherical Shell and a Displaced Particle&amp;rsquo;s Gravity1 Let&amp;rsquo;s assume there is a uniform spherical shell with a total mass of $M$ and a radius of $R$. And there exists a particle with a mass of $m$, displaced at a distance of $r$ from the center $O$ of the spherical shell. In this case, $R&amp;lt;r$ holds. Let&amp;rsquo;s first calculate the force exerted by a portion of the spherical shell on</description></item><item><title>Centrifugal Force</title><link>https://freshrimpsushi.github.io/en/posts/1677/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1677/</guid><description>Definition A force is called a central force if its direction is always towards the same point, regardless of the position of the object it acts upon. Gravity is a typical example. No matter where on Earth we are, the gravity acting on us is directed towards the center of the Earth. It can be mathematically demonstrated that a particle moving under a central force conserves angular momentum and maintains</description></item><item><title>Kinetic Energy of Particle Systems</title><link>https://freshrimpsushi.github.io/en/posts/1676/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1676/</guid><description>Particle System&amp;rsquo;s Kinetic Energy1 The kinetic energy of a particle system, like the linear momentum and angular momentum we defined before, can also be naturally defined as the sum of the kinetic energy of each particle. $$ \begin{equation} T=\sum \limits _{i=1} ^{n} \frac{ 1}{ 2 }m_{i}v_{i}^{2} \label{kinetic} \end{equation} $$ Now, we will do the same operation for the particle system&amp;rsquo;s linear and angular momentum, representing each particle&amp;rsquo;s position vector with</description></item><item><title>Law of Universal Gravitation: Gravity</title><link>https://freshrimpsushi.github.io/en/posts/1678/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1678/</guid><description>Law of Universal Gravity1 The law of universal gravity, announced by Newton through his Principia in 1687, is a physical law that simply states &amp;ldquo;every object attracts every other object&amp;rdquo;. To describe this concept in detail: Every particle of matter in the universe with mass attracts every other particle with a force that is directly proportional to the product of their masses and inversely proportional to the square of the</description></item><item><title>Angular Momentum of Particle Systems</title><link>https://freshrimpsushi.github.io/en/posts/1675/</link><pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1675/</guid><description>Formulas The torque of a particle system equals the sum of the torques of each particle. $$ \mathbf{N}=\frac{ d \mathbf{L}}{ d t }=\sum \limits _{i=1} ^{n} \mathbf{r}_{i}\times \mathbf{F}_{i} $$ Derivation1 The linear momentum of a particle system was defined as the sum of the linear momenta of each particle. Similarly, the angular momentum of a particle system is defined as the sum of the angular momenta of each particle. $$</description></item><item><title>Angular Momentum and Torque</title><link>https://freshrimpsushi.github.io/en/posts/1674/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1674/</guid><description>Angular Momentum1 Momentum is a physical quantity that represents the state of motion of a moving object. The larger the mass and the faster the speed, the greater the momentum. In physics, there is an interest in how the motion of an object changes. Therefore, the force, which is the cause of changing the state of motion of an object, is expressed as a change in momentum. $$ \mathbf{F}=\frac{d \mathbf{p}}{dt}</description></item><item><title>Newton's Laws of Motion</title><link>https://freshrimpsushi.github.io/en/posts/1671/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1671/</guid><description>Newton&amp;rsquo;s Laws of Motion 1 English mathematician and physicist Isaac Newton presented three laws about motion in 1687 in Principia as follows: An object not subjected to an external force does not change its state of motion. The change in motion is proportional to the applied force on the object. When object 1 applies a force to object 2, object 2 simultaneously applies a force equal in magnitude and opposite</description></item><item><title>Physics: The Definition of Mass, Force, and Momentum</title><link>https://freshrimpsushi.github.io/en/posts/1673/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1673/</guid><description>Mass1 In Newton&amp;rsquo;s laws of motion, inertia is described as the property that resists changes in motion. That is, the greater the inertia, the harder it is to move, and the smaller the inertia, the easier it is to move. This exactly aligns with our experience that it&amp;rsquo;s harder to push a heavier object than a lighter one. Hence, the magnitude of inertia can be expressed by the magnitude of</description></item><item><title>The Reason Why Momentum is Denoted by p</title><link>https://freshrimpsushi.github.io/en/posts/1672/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1672/</guid><description>Explanation Many of the symbols used in physics can be easily understood why they were chosen without much thought. For example, the symbols for force, mass, velocity, and acceleration denoted by $\mathbf{F}$, $m$, $\mathbf{v}$, and $\mathbf{a}$ respectively, can be easily guessed to originate from the first letter of the corresponding English words force, mass, velocity, acceleration. However, the symbol for momentum is $\mathbf{p}$, even though momentum is the term used</description></item><item><title>Center of Mass and Linear Momentum of a Particle System</title><link>https://freshrimpsushi.github.io/en/posts/1670/</link><pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1670/</guid><description>Definition A system of particles is referred to as a particle system. Description 1 When the position vectors of particles with masses $m_{1}$, $m_2$, $\cdots$, and $m_{n}$ are $\mathbf{r}_{1}$, $\mathbf{r}_{2}$, $\cdots$, and $\mathbf{r}_{n}$, respectively, the center of mass of this particle system is defined as follows. $$ \mathbf{r}_{cm}=\frac{m_{1}\mathbf{r}_{1}+m_{2}\mathbf{r}_{2}+\cdots + m_{n}\mathbf{r}_{n}}{m_{1}+ m_{2}+ \cdots+ m_{n}}=\frac{\sum m_{i}\mathbf{r}_{i}}{m} $$ Here, $m=\sum \limits_{i}m_{i}$ denotes the total mass of the particle system. The subscript $cm$ stands</description></item><item><title>Linearity of Riemann(-Stieltjes) Iintegral</title><link>https://freshrimpsushi.github.io/en/posts/1666/</link><pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1666/</guid><description>Theorem1 This article is based on the Riemann-Stieltjes integral. If set to $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Let&amp;rsquo;s say $f$ is integrable by Riemann(-Stieltjes) from $[a,b]$. Then, for a constant $c\in \mathbb{R}$, $cf$ is also integrable from $[a,b]$, and its value is as follows. $$ \int_{a}^{b}cf d\alpha = c\int_{a}^{b}f d\alpha $$ Let two functions $f_{1}$, $f_{2}$ be integrable by Riemann(-Stieltjes) from $[a,b]$. Then, $f_{1}+f_{2}$ is</description></item><item><title>Definition of Wavelets</title><link>https://freshrimpsushi.github.io/en/posts/1663/</link><pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1663/</guid><description>Definition Let&amp;rsquo;s denote as $\psi \in L^{2}(\mathbb{R})$. When $\psi$ satisfies the following two conditions, the function $\psi$ is called a wavelet. (a) For an integer $j,k \in \mathbb{Z}$, $\psi_{j,k}$ is defined as follows. $$ \psi_{j,k} (x):=2^{\frac{j}{2}}\psi (2^{j}x-k),\quad x\in \mathbb{R} $$ (b) $\left\{ \psi _{j,k}\right\}_{j,k\in \mathbb{Z}}$ is an orthonormal basis of $L^{2}(\mathbb{R})$ space. $\psi_{j,k}$ can also be represented by dilation D and translation $T_{k}$ as follows. $$ \psi_{j,k}=D^{j}T_{k}\psi,\quad j,k\in\mathbb{Z} $$ Explanation</description></item><item><title>지수성장방정식/상수 계수를 갖는 1계 선형 동차 미분 방정식</title><link>https://freshrimpsushi.github.io/en/posts/1660/</link><pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1660/</guid><description>Definition In a first-order ordinary differential equation where the independent variable $t$ is not explicitly included in $f$, it is called an autonomous system or autonomous differential equation. $$ \dfrac{\mathrm{d}y}{\mathrm{d}t} = f(y) $$ Conversely, an equation in the following form is called a non-autonomous system. $$ \dfrac{\mathrm{d}y}{\mathrm{d}t} = f(y, t) $$ Explanation The term autonomous system has a more dynamical sense, whereas autonomous differential equation feels more focused on the</description></item><item><title>First-Order Linear Differential Equation System</title><link>https://freshrimpsushi.github.io/en/posts/1659/</link><pubDate>Thu, 11 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1659/</guid><description>Buildup1 When the mass is $m$, the damping factor is $\gamma$, and the spring constant is $k$, the equation of motion representing the vibration of an object hung on a spring is as follows. $$ m x^{\prime \prime} + \gamma x^{\prime} + kx = F $$ Letting $x_{1}=x$, $x_{2}=x_{1}^{\prime}$, the above equation of motion can be expressed as the following system. $$ \begin{align*} x_{1}^{\prime}(t) =&amp;amp;\ x_{2}(t) \\ x_{2}^{\prime} (t) =&amp;amp;\</description></item><item><title>Laguerre Polynomials' Rodrigues' Formula</title><link>https://freshrimpsushi.github.io/en/posts/1658/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1658/</guid><description>Formulas The explicit formula for the Laguerre polynomials is as follows. $$ L_{n}(x) = \frac{1}{n!}e^{x}\frac{ d ^{n}}{ dx^{n} }(x^{n}e^{-x}) \tag{1} $$ Description The formula above is referred to as the Rodrigues&amp;rsquo; formula for Laguerre polynomials. Originally, the term Rodrigues&amp;rsquo; formula denoted the explicit form of the Legendre polynomials, but it later became a general term for formulas expressing the explicit form of special functions represented by polynomials. Writing down the</description></item><item><title>Hermite Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1655/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1655/</guid><description>Description Hermite Polynomials are defined in several ways as follows. As solutions to a differential equation Hermite polynomials are defined as the solutions to the following Hermite Differential Equation. $$ y^{\prime \prime} -2xy^{\prime} +2ny=0,\quad n=0,1,2,\cdots $$ Rodrigues&amp;rsquo; formula The following function $H_{n}$ is called the Hermite polynomial. $$ H_{n}(x)=(-1)^{n}e^{x^{2}}\frac{ d ^{n}}{ dx^{n} }e^{-x^{2}} $$ This is known as the Rodrigues&amp;rsquo; formula. Meanwhile, the above function is referred to as the</description></item><item><title>Hermite Polynomials' Generating Function</title><link>https://freshrimpsushi.github.io/en/posts/1654/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1654/</guid><description>Formulas The generating function of Hermite Polynomials is as follows. $$ \Phi (x,t)=\sum \limits _{n=0}^{\infty} \frac{H_{n}(x)}{n!}t^{n}= e^{2xt-t^{2}} $$ Explanation The generating function of Hermite Polynomials, simply put, is a polynomial that uses Hermite Polynomials as its coefficients. $H_{n}(x)$ is a Hermite Polynomial, and can be obtained by multiplying Hermite function $y_{n}=e^{\frac{x^{2}}{2}}\frac{ \d ^{n} }{ \d x^{n} }e^{-x^{2}}$ with $(-1)^{n}e^{\frac{x^{2}}{2}}$ or by solving the Hermite Differential Equation. $$ H_{n}(x)=(-1)^{n}e^{x^{2}}\frac{ \d ^{n}}{</description></item><item><title>Hermite Polynomials' Recursive Relations</title><link>https://freshrimpsushi.github.io/en/posts/1656/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1656/</guid><description>Theorem The Hermite polynomials satisfy the following recursive relation. $$ \begin{align} H_{n}^{\prime}(x) &amp;amp;= 2nH_{n-1}(x) \\ H_{n+1}(x) &amp;amp;= 2xH_{n}(x)-2nH_{n-1}(x) \\ &amp;amp;= 2xH_{n}(x)-H_{n}^{\prime}(x) \nonumber \end{align} $$ Proof $(1)$ Solving using the Generating Function Generating function of the Hermite polynomials $$ \Phi (x,t) = e^{2xt-t^{2}}=\sum \limits _{n=0}^{\infty} H_{n}(x)\frac{t^{n}}{n!} $$ Differentiating the generating function of the Hermite polynomials gives, $$ 2te^{2xt-t^{2}} = \sum \limits _{n=0}^{\infty}H_{n}^{\prime}(x)\frac{t^{n}}{n!} $$ Then, the left side, by the definition of</description></item><item><title>Orthogonality of Hermite Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1657/</link><pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1657/</guid><description>Theorem The Hermite polynomials $\left\{ H_{n} \right\}_{n=0}^{\infty}$ are orthogonal with respect to the weight function $w(x)=e^{-x^{2}}$ over the interval $(-\infty, \infty)$. $$ \braket{ H_{n} | H_{m} }_{e^{-x^{2}}} =\int_{-\infty}^{\infty}e^{-x^{2}}H_{n}(x)H_{m}(x)dx=\sqrt{\pi}2^{n}n!\delta_{nm} $$ Here, $\delta_{nm}$ is the Kronecker delta. Proof Case 1: $n=m$ Let&amp;rsquo;s denote the differential operator as $D = \dfrac{d}{dx}$. $$ \int_{-\infty}^{\infty} e^{-x^{2}}H_{n}(x)H_{n}(x)dx $$ Hermite polynomials $$ H_{n}(x) = (-1)^{n}e^{x^{2}}\frac{d^{n}}{dx^{n}}e^{-x^{2}} = (-1)^{n}e^{x^{2}}D^{n}e^{-x^{2}} $$ If we solve the front part $H_{n}(x)$ of the</description></item><item><title>Foehammer Symbol</title><link>https://freshrimpsushi.github.io/en/posts/1652/</link><pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1652/</guid><description>Definitions The Pochhammer symbol has two kinds of representations as follows. The following equation is defined as the falling factorial. $$ \begin{align*} x^{\underline{n}} := (x)_{n}&amp;amp;=x(x-1)(x-2)\cdots(x-n+1) \\ &amp;amp;=\frac{x!}{(x-n)!}=\frac{\Gamma (x+1) }{ \Gamma (x-n+1)} \\ &amp;amp;=\prod \limits_{k=0}^{n-1}(x-k) \end{align*} $$ The following equation is defined as the raising factorial. $$ \begin{align*} x^{\overline{n}} := x^{(n)}&amp;amp;=x(x+1)(x+2)\cdots(x+n-1) \\ &amp;amp;=\frac{(x+n-1)!}{(x-1)!}=\frac{\Gamma (x+n) }{ \Gamma (x)} \\ &amp;amp;=\prod \limits_{k=0}^{n-1}(x+k) \end{align*} $$ $x^{\overline{0}}$ and $x^{\underline{0}}$ are defined as $1$. $$ x^{\overline{0}}=x^{\underline{n}}=1</description></item><item><title>Series Solution of Laguerre Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1651/</link><pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1651/</guid><description>Definition The following differential equation is referred to as the Laguerre differential equation. $$ xy^{\prime \prime}+(1-x)y^{\prime}+ny=0,\quad n=0,1,2,\cdots $$ Description The solution to the Laguerre differential equation is called Laguerre polynomials, and the first few Laguerre polynomials are as follows. $$ \begin{align*} L_{0}(x) &amp;amp;= 1 \\ L_{1}(x) &amp;amp;= -x+1 \\ L_{2}(x) &amp;amp;= \frac{1}{2}\left( x^{2}-4x+2 \right) \\ L_{3}(x) &amp;amp;= \frac{1}{6}\left( -x^{3}+9x^{2}-18x+6 \right) \\ \vdots &amp;amp; \end{align*} $$ Examining the equation to solve</description></item><item><title>Hermite Differential Equations and Series Solutions</title><link>https://freshrimpsushi.github.io/en/posts/1650/</link><pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1650/</guid><description>Definition The differential equation given below is referred to as the Hermite Differential Equation. $$ y^{\prime \prime}-2xy^{\prime}+2ny=0,\quad n=0,1,2,\cdots $$ The solution to the Hermite Differential Equation is called the Hermite Polynomial, and it is commonly denoted as $H_{n}(x)$. $$ \begin{align*} H_{0}(x) &amp;amp;= 1 \\ H_{1}(x) &amp;amp;= 2x \\ H_{2}(x) &amp;amp;= 4x^{2} - 2 \\ H_{3}(x) &amp;amp;= 8x^{3} - 12x \\ H_{4}(x) &amp;amp;= 16x^{4} - 48x^{2} + 12 \\ H_{5}(x) &amp;amp;=</description></item><item><title>Hermite Functions</title><link>https://freshrimpsushi.github.io/en/posts/1646/</link><pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1646/</guid><description>Definitions Hermite functions are defined as follows: $$ \begin{align} y_{n} &amp;amp;= \left( D-x \right)^{n} e^{-\frac{x^{2}}{2}} \\ &amp;amp;=e^{\frac{x^{2}}{2}} D^{n} e^{-x^{2}} \end{align} $$ where $D=\frac{d}{dx}$ is the differential operator. Description Hermite functions are solutions to the differential equation $$ y_{n}^{\prime \prime}-x^{2}y_{n}=-(2n+1)y_{n},\quad n=0,1,2,\cdots $$ and represent the solution to the one-dimensional harmonic oscillator Schrödinger equation in physics, i.e., the wave function of a</description></item><item><title>Operator Solution of the Differential Equation Satisfied by Hermite Functions</title><link>https://freshrimpsushi.github.io/en/posts/1648/</link><pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1648/</guid><description>Theorem Given the differential equation $$ y_{n}^{\prime \prime}-x^{2}y_{n}=-(2n+1)y_{n},\quad n=0,1,2,\cdots \tag{1} \label{eq1} $$ The solution to $(1)$ is as follows, known as the Hermite function. $$ \begin{align*} y_{n} &amp;amp;= \left( D-x \right)^{n} e^{-\frac{x^{2}}{2}} \\ &amp;amp;= e^{\frac{x^{2}}{2}} D^{n} x^{-x^{2}} \end{align*} $$ Here, $D$ is the differential operator $D=\frac{ d }{ dx }$. Explanation The first equation of $y_{n}$ can be directly obtained by solving the differential equation. That the second equation is</description></item><item><title>What is a Differential Operator in Physics?</title><link>https://freshrimpsushi.github.io/en/posts/1638/</link><pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1638/</guid><description>Explanation One of the methods to solve differential equations is to use the differential operator. Let&amp;rsquo;s define the differential operator $D$ as follows. $$ D:= \frac{d}{dx} $$ When explicitly expressing the variable being differentiated, it is also denoted as $D_{x}$. For partial differentiation, it is represented as follows. $$ \partial _{x}:=\frac{ \partial }{ \partial x},\quad \partial_{y}=\frac{ \partial }{ \partial y} $$ Using the differential operator, the differential equation is expressed</description></item><item><title>Wave Function and Hilbert Space in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/1637/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1637/</guid><description>Build-Up In Classical Mechanics, the main concern is finding the position function $\mathbf{r}(t)$ that satisfies Newton&amp;rsquo;s Second Law $\mathbf{F} = m \mathbf{a}$ given specific conditions. For instance, the position of an object launched with an initial velocity $\mathbf{v}_{0} = (v_{0}\cos\theta, v_{0}\sin\theta)$ in 2-dimensional space over time can be found by solving the following system of equations, and this is called Projectile Motion. $$ \begin{align*} m \dfrac{d^{2}\mathbf{r}}{dt^{2}} &amp;amp;= -mg\hat{\mathbf{y}} \\ \mathbf{v}(0)</description></item><item><title>Angular Momentum and Position/Momentum Commutation Relations</title><link>https://freshrimpsushi.github.io/en/posts/1636/</link><pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1636/</guid><description>Formula The commutator of angular momentum and position is as follows. $$ \begin{align*} [L_{z}, x] &amp;amp;= \i \hbar y \\ [L_{z}, y] &amp;amp;= -\i \hbar x \\ [L_{z}, z] &amp;amp;= 0 \end{align*} $$ The commutator of angular momentum and momentum is as follows. $$ \begin{align*} [L_{z}, p_{x}] &amp;amp;= \i \hbar p_{y} \\ [L_{z}, p_{y}] &amp;amp;= -\i \hbar p_{x} \\ [L_{z}, p_{z}] &amp;amp;= 0 \end{align*} $$ The squares of angular momentum</description></item><item><title>Ladder Operators of Angular Momentum in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1634/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1634/</guid><description>공식 구 좌표계에서 각운동량의 사다리 연산자는 다음과 같다. $$ \begin{align*} L_{+} &amp;amp;= \hbar e^{\i\phi}\left( \dfrac{\partial }{\partial \theta} + \i\cot\theta \dfrac{\partial }{\partial \phi}\right) \\ L_{-} &amp;amp;= -\hbar e^{-\i\phi}\left( \dfrac{\partial }{\partial \theta} - \i\cot\theta \dfrac{\partial }{\partial \phi}\right) \\ L_{+}L_{-} &amp;amp;= -\hbar ^{2} \left( \frac{ \partial ^{2}}{ \partial \theta ^{2} } + \cot \theta \frac{</description></item><item><title>The eigenfunctions of the angular momentum operator are spherical harmonics.</title><link>https://freshrimpsushi.github.io/en/posts/1633/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1633/</guid><description>Summary The angular momentum operator $L^{2}$ and $L_{z}$ have simultaneous eigenfunctions determined by constants $l$, $m$ $\ket{\ell, m}$. $$ \begin{align*} L^{2}\ket{\ell, m} &amp;amp;= \hbar^{2}\ell(\ell+1)\ket{\ell, m} \\ L_{z}\ket{\ell, m} &amp;amp;= m\hbar\ket{\ell, m} \end{align*} $$ Here, the eigenfunction of the angular momentum operator $\ket{\ell, m}$ is actually spherical harmonics $Y_{l}^{m}$. $$ \ket{\ell, m} = Y_{l}^{m} $$ Proof In spherical coordinates, the angular momentum operator $L_{z}$ is as follows: $$ L_{z} = -\i\hbar\frac{\partial}{\partial</description></item><item><title>The Airy Function</title><link>https://freshrimpsushi.github.io/en/posts/1629/</link><pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1629/</guid><description>Definition The function below is referred to as the Airy function. $$ \begin{align*} \operatorname{Ai}(x) &amp;amp;= \frac{1}{\pi}\sqrt{\frac{x}{3}}K_{1/3}\left( \frac{2}{3}x^{2/3} \right) \\ \operatorname{Bi}(x) &amp;amp;= \sqrt{\frac{x}{3}}\left[ I_{-1/3}\left( \frac{2}{3}x^{3/2} \right) + I_{1/3} \left( \frac{2}{3}x^{2/3} \right) \right] \end{align*} $$ Here, $I_{\nu}$ and $K_{\nu}$ are modified Bessel functions. Description The Airy function represents the solution to the Airy differential equation using Bessel functions. Integral Form The Airy function has the following integral form: $$ \begin{align*} \operatorname{Ai}(x) &amp;amp;=</description></item><item><title>Series Solutions to the Airy Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1625/</link><pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1625/</guid><description>Definition The following differential equation is called the Airy differential equation. $$ y^{\prime \prime}-xy=0,\quad -\infty&amp;lt;x&amp;lt;\infty $$ Explanation The name originates from the British astronomer George Biddell Airy. It is also called the Stokes equation. Solution Since the coefficient of $y^{\prime \prime}$ is $1$, all points are ordinary points. Among them, let&amp;rsquo;s find the power series solution around $x=0$. Assume that the solution of the Airy equation is as follows and</description></item><item><title>Modified Bessel Equation and Modified Bessel Function</title><link>https://freshrimpsushi.github.io/en/posts/1624/</link><pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1624/</guid><description>Buildup The differential equation below is referred to as the modified Bessel equation. $$ x^2 y^{\prime \prime} + xy^{\prime}-(x^2-\nu^2)y=0 $$ It is a form of the Bessel equation where the sign of the term $y$ has been changed to $+ \rightarrow -$. The solution to this differential equation is given by the formula for differential equations that have Bessel equation solutions, as follows. $$ y=Z_{\nu}(ix)=AJ_{\nu}(ix)+BN_{\nu}(ix) $$ The two commonly used</description></item><item><title>Bessel Functions</title><link>https://freshrimpsushi.github.io/en/posts/1622/</link><pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1622/</guid><description>Definition Bessel Equation The differential equation below is called the $\nu$ order Bessel equation. $$ \begin{align*} x^2 y^{\prime \prime} +xy^{\prime} +(x^2-\nu^2)y&amp;amp;=0 \\ x(xy^{\prime})^{\prime}+(x^2- \nu ^2) y&amp;amp;=0 \\ y^{\prime \prime}+\frac{1}{x} y^{\prime} + \left( 1-\frac{\nu^{2}}{x^{2}} \right)y&amp;amp;=0 \end{align*} $$ Description Functions Related First Kind Bessel Function First Kind Bessel Function The first solution of the Bessel equation is written as $J_{\nu}(x)$ and is called the first kind Bessel function. $$ J_{\nu}(x)=\sum \limits_{n=0}^{\infty} \frac{(-1)^{n}</description></item><item><title>Hankel Functions, Bessel Functions of the Third Kind</title><link>https://freshrimpsushi.github.io/en/posts/1623/</link><pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1623/</guid><description>Definition A Hankel function, also known as a Bessel function of the third kind, is defined as the following two linear combinations of the Bessel function of the first kind $J_{\nu}$ and the Bessel function of the second kind $N_{\nu}$. $$ H_{\nu}^{(1)}(x) = J_{\nu}(x)+iN_{\nu}(x) $$ $$ H_{\nu}^{(2)}(x) = J_{\nu}(x)-iN_{\nu}(x) $$ Explanation It was introduced by the German mathematician Hermann Hankel in 1869. Specifically, $H_{\nu}^{(1)}$ is called the Hankel function of</description></item><item><title>Orthogonality of Bessel Functions</title><link>https://freshrimpsushi.github.io/en/posts/1621/</link><pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1621/</guid><description>Theorem Let&amp;rsquo;s assume that the roots of the first kind Bessel function $\alpha, \beta$ are $J_{\nu}(x)$. Then, in the interval $[0,1]$, $\sqrt{x}J_{\nu}(x)$ forms an orthogonal set. $$ \int_{0}^{1} x J_{\nu}(\alpha x) J_{\nu}(\beta x)dx = \begin{cases} 0 &amp;amp;\alpha\ne \beta \\ \frac{1}{2}J^{2}_{\nu+1}(\alpha)=\frac{1}{2}J_{\nu-1}^{2}(\alpha)=\frac{1}{2}J_{\nu^{\prime}}^{2}(\alpha) &amp;amp;\alpha=\beta \end{cases} $$ Description The above content can also be expressed as &amp;lsquo;Bessel function $J_{\nu}(x)$ is orthogonal in the interval $[0,1]$ with respect to the weight function $x$&amp;rsquo;. Proof $\alpha</description></item><item><title>Bessel Functions as Solutions to Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1620/</link><pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1620/</guid><description>Theorem1 Theorem 1 Given a differential equation slightly different from the Bessel equation as follows: $$ \begin{equation} \begin{aligned} &amp;amp;&amp;amp; y^{\prime \prime}+\frac{1-2a}{x}y^{\prime}+\left[ (bcx^{c-1})^{2}+\frac{a^{2}-\nu^{2}c^{2}}{x^{2}} \right]y =&amp;amp;\ 0 \\ \text{or} &amp;amp;&amp;amp; x^{2}y^{\prime \prime}+(1-2a)xy^{\prime}+\left[ b^{2}c^{2}x^{2c}+(a^{2}-\nu^{2}c^{2}) \right]y =&amp;amp;\ 0 \end{aligned} \label{1} \end{equation} $$ And let $Z_{\nu}(x)$ be any linear combination of $J_{\nu}(x)$ and $N_{\nu}(x)$. Then, the solution to the given differential equation is as follows: $$ y=x^{a}Z_{\nu}(bx^{c})=x^{a}[AJ_{\nu}(bx^{c})+BN_{\nu}(bx^{c})] $$ $\nu$, $a$, $b$, $c$, $A$, $B$ are</description></item><item><title>Bessel Function's Recursive Relations</title><link>https://freshrimpsushi.github.io/en/posts/1619/</link><pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1619/</guid><description>Theorem $$ J_{\nu}(x)=\sum \limits_{n=0}^{\infty} \frac{(-1)^{n} }{\Gamma (n+1) \Gamma (n+\nu+1)} \left(\frac{x}{2} \right)^{2n+\nu} \tag{1} $$ The function above is called the first kind Bessel function of order $\nu$. The first kind Bessel function $J_{\nu}(x)$ satisfies the following equation. $$ \begin{align*} \frac{d}{dx}[x^{\nu} J_{\nu}(x)] &amp;amp;= x^{\nu}J_{\nu-1}(x) \tag{a} \\ \frac{d}{dx}[x^{-\nu}J_{\nu}(x)] &amp;amp;= -x^{-\nu}J_{\nu+1}(x) \tag{b} \\ J_{\nu-1}(x)+J_{\nu+1}(x) &amp;amp;= \frac{2\nu}{x}J_{\nu}(x) \tag{c} \\ J_{\nu-1}(x)-J_{\nu+1}(x) &amp;amp;= 2J^{\prime}_{\nu}(x) \tag{d} \\ J_{\nu}^{\prime}(x) = -\frac{\nu}{x}J_{\nu}(x)+J_{\nu-1}(x) &amp;amp;= \frac{\nu}{x}J_{\nu}(x)-J_{\nu+1}(x) \tag{e} \end{align*} $$ Proof $(a)$ By</description></item><item><title>The Second Series Solution of the Bessel Equation: Bessel Functions of the Second Kind, Neumann Functions, Weber Functions</title><link>https://freshrimpsushi.github.io/en/posts/1618/</link><pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1618/</guid><description>Definition[^1] A second solution of the Bessel equation is called the Neumann function, denoted by $N_{\nu}(x)$ or $Y_{\nu}(x)$. For non-integer $\nu$, $$ N_{\nu}(x)=Y_{\nu}(x)=\frac{\cos (\nu \pi)J_{\nu}(x)-J_{-\nu}(x)}{\sin (\nu\pi)} $$ For integer $\nu$, it is defined by the limit. For $n\in \mathbb{Z}$, $\nu \in \mathbb{R}\setminus\mathbb{Z}$, $$ N_{n}(x)=\lim \limits_{\nu \rightarrow n}N_{\nu}(x) $$ Here, $J_{\pm \nu}(x)$ is the first kind Bessel function. Thus, the general solution of the Bessel equation is as follows. $$ y(x)=AJ_{\nu}(x)+BN_{\nu}(x)</description></item><item><title>Schrödinger Equation in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1617/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1617/</guid><description>Equation In the spherical coordinate system, the Schrödinger equation is as follows. $$ -\frac{\hbar^{2}}{2M}\left[\frac{1}{r^2}\frac{\partial}{\partial r} \left( r^2\frac{\partial \psi}{\partial r} \right) + \frac{1}{r^2\sin\theta}\frac{\partial}{\partial\theta}\left( \sin\theta \frac{\partial \psi}{\partial \theta} \right) + \frac{1}{r^2\sin^2\theta}\frac{\partial^2 \psi}{\partial^2 \phi} \right]+V\psi=E\psi \tag{1} $$ Explanation The time-independent Schrödinger equation in three dimensions is as follows. $$ -\frac{\hbar^{2}}{2M}\nabla^{2}\psi+V\psi=E\psi $$ Here, $M$</description></item><item><title>Associated Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1615/</link><pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1615/</guid><description>Definition Associated Legendre polynomials are defined in the following ways. As a Solution to a Differential Equation The solutions to the associated Legendre differential equation below are referred to as associated Legendre polynomials. $$ \begin{align*} &amp;amp;&amp;amp; (1-x^{2}) \frac{d^{2}y}{dx^{2}} - 2x \frac{dy}{dx} + \left[l(l+1) - \frac{m^{2}}{1-x^{2}}\right] y &amp;amp;= 0 \\ \text{or} &amp;amp;&amp;amp; \frac{d}{dx} \left[(1-x^{2})y^{\prime}\right] + \left[l(l+1) - \frac{m^{2}}{1-x^{2}}\right] y &amp;amp;= 0 \end{align*} $$ Rodrigues&amp;rsquo; Formula The polynomial function $P_{l}^{m}$ below is</description></item><item><title>Normalization of Spherical Harmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1614/</link><pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1614/</guid><description>Theorem The standardized spherical harmonics are as follows. $$ Y_{l}^{m}(\theta,\phi)=\sqrt{\frac{2l+1}{4\pi}\frac{(l-m)!}{(l+m)!}}P_{l}^{m}(\cos\theta)e^{im\phi} $$ $$ \nabla ^2 f = \frac{1}{r^2}\frac{\partial}{\partial r} \left( r^2\frac{\partial f}{\partial r} \right) + \frac{1}{r^2\sin\theta}\frac{\partial}{\partial\theta}\left( \sin\theta \frac{\partial f}{\partial \theta} \right) + \frac{1}{r^2\sin^2\theta}\frac{\partial^2 f}{\partial^2 \phi}=0 $$ $$ f(r,\theta,\phi)=R(r)\Theta (\theta)\Phi (\phi) $$ Description In the Laplace equation on spherical coordinates, solutions for the polar angle $\theta$ and the azimuthal angle $\phi$ are referred to as spherical harmonics. $$ \Theta (\theta)\Phi (\phi)=Y_{l}^{m}(\theta,\phi)=e^{im\phi}P_{l}^{m}(\cos \theta)</description></item><item><title>Orthogonality of Associated Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1613/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1613/</guid><description>Theorem The associated Legendre polynomials over the interval $[-1,1]$ for a fixed $m$ form an orthogonal set. $$ \int_{-1}^{1} P_{l}^{m}(x)P_{k}^{m}(x)dx =\frac{ 2}{ 2l+1 }\frac{(l+m)!}{(l-m)!}\delta_{lk} $$ In the case of $x=\cos \theta$, $$ \int_{0}^{\pi} P_{l}^{m}(\cos \theta)P_ {k}^{m}(\cos\theta)\sin \theta d\theta =\frac{ 2}{ 2l+1 }\frac{(l+m)!}{(l-m)!}\delta_{lk} $$ Associated Legendre Polynomials $$ P_{l}^{m}(x) = (1-x ^{2})^{\frac{m}{2}} \dfrac{1}{2^l l!} \dfrac{d^{l+m}}{dx^{l+m}}(x^2-1)^l $$ Proof For convenience, let&amp;rsquo;s briefly denote it as $P_{lm} = P_{l}^{m}(x)$. The associated Legendre differential</description></item><item><title>Associated Legendre Polynomials for Negative Index m</title><link>https://freshrimpsushi.github.io/en/posts/1612/</link><pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1612/</guid><description>Formulas The Associated Legendre Polynomials obey the proportionality relation below, depending on the sign of $m$. $$ P_{l}^{-m}(x)=(-1)^{m}\frac{(l-m)!}{(l+m)!}P_{l}^{m}(x) $$ $$ (1-x^{2})\frac{ d^{2}y }{ dx^{2} }-2x \frac{dy}{dx}+\left( \frac{-m^{2}}{1-x^{2}}+l(l+1) \right)y=0 $$ Explanation Looking at the associated Legendre differential equation, the section regarding $m$ is shown as $m^2$, so whether $m$ is positive or negative doesn&amp;rsquo;t affect the solution. Thus, the associated Legendre polynomials can be derived as follows. $$ \begin{align*} P_{l}^{m}(x)&amp;amp;= (1-x</description></item><item><title>Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1611/</link><pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1611/</guid><description>Definition Legendre polynomials are defined in various ways. As Solutions to a Differential Equation The solutions to the following Legendre differential equation are called Legendre polynomials. $$ (1-x^{2}) \dfrac{d^{2} y}{dx^{2}} -2x\dfrac{dy}{dx} + l(l+1) y = 0 $$ Rodrigues&amp;rsquo; Formula The following function $P_{l}$ is called a Legendre polynomial. $$ P_{l}(x) = \dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} $$ This is known as Rodrigues&amp;rsquo; formula. Explanation By definition, $P_{n}$ is technically a polynomial &amp;lsquo;function&amp;rsquo;,</description></item><item><title>Generating Functions of Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1610/</link><pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1610/</guid><description>Theorem The generating function of Legendre polynomials is as follows. $$ \Phi (x,t) = \frac{1}{\sqrt{1-2xt+t^{2}}} = \sum \limits_{l=0}^{\infty}P_{l}(x)t^{l},\quad |t|&amp;lt;1 $$ Description The generating function of Legendre polynomials is, put simply, a polynomial that has the Legendre polynomial $P_{l}(x)$ as its coefficients. Lemma The function $\Phi (x,t) = \dfrac{1}{\sqrt{1-2xt+t^{2}}}$ is a solution to the differential equation below. $$ \begin{equation} (1-x^{2})\frac{ \partial ^{2} \Phi}{ \partial x^{2} }-2x\frac{ \partial \Phi}{ \partial x }+t\frac{</description></item><item><title>Recursive Relations of Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1609/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1609/</guid><description>Theorem $$ P_{l}(x)=\dfrac{1}{2^l l!} \dfrac{d^l}{dx^l}(x^2-1)^l \tag{1} $$ Such a Legendre Polynomial $P_{l}$ satisfies the following recursive relation: $$ P^{\prime}_{l+1}(x)-P^{\prime}_{l-1}(x)=(2l+1)P_{l}(x) \tag{a} $$ $$ lP_{l}(x)=(2l-1)xP_{l-1}(x)-(l-1)P_{l-2}(x) \tag{b} $$ $$ xP^{\prime}_{l}(x)-P^{\prime}_{l-1}(x)=lP_{l}(x)\tag{c} $$ Proof $(a)$ First, if we calculate the derivative of $P_{l}(x)$, $$ \begin{align*} \frac{d}{dx}P_{l}(x) &amp;amp;= \frac{1}{2^l l!}\frac{d}{dx} \dfrac{d^l}{dx^l}(x^2-1)^l \\ &amp;amp;= \frac{1}{ 2^{l}l! }\frac{ d ^{l} }{ dx^{l} }\frac{d}{dx}(x^{2}-1)^{l} \\ &amp;amp;= \frac{2l}{ 2^{l}l! }\frac{ d ^{l} }{ dx^{l} }\left[ x(x^{2}-1)^{l-1} \right] \\ &amp;amp;= \frac{1}{</description></item><item><title>Associated Legendre Differential Equations and Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/1605/</link><pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1605/</guid><description>Definition1 The differential equation given below is called the associated Legendre differential equation. $$ \begin{equation} \begin{aligned} &amp;amp;&amp;amp;(1-x^{2})\frac{ d^{2}y }{ dx^{2} }-2x \frac{dy}{dx}+\left[ +l(l+1)-\frac{m^{2}}{1-x^{2}} \right]y =&amp;amp;\ 0 \\ \mathrm{or} &amp;amp;&amp;amp; \frac{ d }{ dx } \left[ (1-x^{2})y^{\prime} \right] +\left[ l(l+1)-\frac{m^{2}}{1-x^{2}} \right]y =&amp;amp;\ 0 \end{aligned} \label{1} \end{equation} $$ The solution to the associated Legendre differential equation is denoted as $P_{l}^{m}(x)$, and this is called the associated Legendre polynomial or the generalized Legendre</description></item><item><title>Derivation of the Schrödinger Equation</title><link>https://freshrimpsushi.github.io/en/posts/1598/</link><pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1598/</guid><description>Overview Time-independent Schrodinger equation $$ H\psi=\left(-\frac{\hbar^{2}}{2m}\frac{ d ^{2} }{ d x^{2} }+V\right)\psi=E\psi \\ H\psi=\left(-\frac{\hbar^{2}}{2m}\nabla^{2}+V\right)\psi=E\psi $$ Time-dependent Schrodinger equation $$ i\hbar\frac{ \partial \psi}{ \partial t}=\left(-\frac{\hbar^{2}}{2m}\frac{ \partial ^{2} }{\partial x^{2} }+V\right)\psi \\ i\hbar\frac{ \partial \psi}{ \partial t}=\left(-\frac{\hbar^{2}}{2m}\nabla^{2}+V\right)\psi $$ The Schrodinger equation is a partial differential equation related to the energy, position, and time of a complex wave function. In simpler terms, it&amp;rsquo;s like the following in classical mechanics: $$ F=ma $$ Using</description></item><item><title>Solutions to Euler's Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1599/</link><pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1599/</guid><description>Definition The differential equation of the following form is called the Euler differential equation or Euler-Cauchy equation. $$ \begin{equation} a_{2}x^{2}\frac{ d ^{2 }y}{ dx^{2} }+a_{1}x\frac{ d y}{ d x }+a_{0}y=0 \end{equation} $$ Explanation For a non-homogeneous equation where the right side is not $0$, it can be solved by substituting it with $x=e^{z}$. Solution For convenience of calculation, both sides of $(1)$ are divided by $a_{2}$, and let&amp;rsquo;s call the</description></item><item><title>General Solution to the Laplace Equation in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1588/</link><pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1588/</guid><description>Theorem In spherical coordinates, the Laplace equation is as follows: $$ \nabla ^2 f = \frac{1}{r^2}\frac{\partial}{\partial r} \left( r^2\frac{\partial f}{\partial r} \right) + \frac{1}{r^2\sin\theta}\frac{\partial}{\partial\theta}\left( \sin\theta \frac{\partial f}{\partial \theta} \right) + \frac{1}{r^2\sin^2\theta}\frac{\partial^2 f}{\partial^2 \phi}=0 $$ Explanation Assuming that $f$ can be separated into variables as $f(r,\theta,\phi)=R(r)\Theta (\theta)\Phi (\phi)$, the general solution for the radial component can be derived by solving the Euler differential equation as follows: $$ R(r)=\sum \limits_{l=0}^{\infty}R_{l}(r)=\sum \limits_{l=0}^{\infty}\left( A_{l}r^{l}+\frac{</description></item><item><title>General Solution to the Radial Component Equation in the Laplace's Equation in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1587/</link><pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1587/</guid><description>Theorem The general solution of the radial part equation in the spherical coordinate system Laplace&amp;rsquo;s equation is given below. $$ R(r)=\sum \limits_{l=0}^{\infty}R_{l}(r)=\sum \limits_{l=0}^{\infty}\left( A_{l}r^{l}+\frac{ B_{l}}{r^{l+1}} \right) $$ Here, $l$ is a non-negative integer, and $A_{l}$, $B_{l}$ are constants. Description The process of finding this is relatively simple compared to the solution for polar and azimuthal angles. Proof In the spherical coordinate system Laplace&amp;rsquo;s equation, the solutions for the polar angle</description></item><item><title>Spherical Harmonics: General Solutions for the Polar and Azimuthal Angles in the Spherical Coordinate Laplace's Equation</title><link>https://freshrimpsushi.github.io/en/posts/1580/</link><pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1580/</guid><description>Definition The general solution for the polar and azimuthal angles in the spherical coordinate system for the Laplace equation is as follows, and this is called Spherical harmonics. $$ Y_{l}^{m}(\theta,\phi)=e^{im\phi}P_{l}^{m}(\cos \theta) $$ Here, $l$ is $l=0,1,2\cdots$ and $m$ is an integer that satisfies $ -l \le m \le l$. Also, $P_{l}^{m}(\cos\theta)$ is as follows. $$ \begin{align*} P_{l}^{m}(\cos \theta)&amp;amp;= (1-\cos ^{2}\theta)^{\frac{|m|}{2}} \frac{ d^{|m|} }{ dx^{|m|} } P_{l}(x) \\ &amp;amp; =(1-\cos ^{2}\theta)^{\frac{|m|}{2}}</description></item><item><title>Principles of CT (Computed Tomography)</title><link>https://freshrimpsushi.github.io/en/posts/1579/</link><pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1579/</guid><description>Principle CT stands for Computerized Tomography, a technology that captures cross-sectional images of the body, similarly well-known as MRI among the public. Our bodies are composed of various materials such as bones, muscles, and water. CT uses the difference in how these materials absorb X-rays1 to obtain cross-sectional images. Let&amp;rsquo;s look at the diagram below. In (a), let&amp;rsquo;s say the white square represents a material (for example, bones) that absorbs</description></item><item><title>Magnetic Field Created by a Moving Point Charge</title><link>https://freshrimpsushi.github.io/en/posts/1577/</link><pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1577/</guid><description>Overview 1 The electromagnetic field created by a moving point charge is as follows. $$ \begin{align*} \mathbf{E}(\mathbf{r}, t) &amp;amp;= \frac{q}{4\pi\epsilon_{0}} \frac{\cR} {( \bcR\cdot \mathbf{u} )^3 } \left[(c^2-v^2)\mathbf{u} +\bcR\times (\mathbf{u} \times \mathbf{a} ) \right] \\ \mathbf{B} (\mathbf{ r}, t) &amp;amp;=\frac{1}{c} \crH\times \mathbf{ E } (\mathbf{ r}, t) \end{align*} $$ Description The formula for the magnetic field is specifically as follows. $$ \mathbf{B}=-\frac{1}{c}\frac{1}{4\pi \epsilon_{0}} \frac{q}{ (\mathbf{u}\cdot \bcR)^{3}} \bcR \times \left[ (c^{2}-v^{2})\mathbf{v}+(\bcR \cdot</description></item><item><title>Physics에서의 Del 연산자</title><link>https://freshrimpsushi.github.io/en/posts/1575/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1575/</guid><description>Explanation In physics, an operator refers to a function that maps a function to another function. Among these, the del operator refers to a function that, given a function, results in a function that has the derivative of the given function as its function value. If the term operator is unfamiliar, you can simply understand it as a rule that computes a target. For example, if you insert $f$ into</description></item><item><title>In Quantum Mechanics, what is a commutator?</title><link>https://freshrimpsushi.github.io/en/posts/1574/</link><pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1574/</guid><description>Definition For the given two operators $A, B$, $AB - BA$ is defined as the commutator of $A, B$ and is denoted as follows: $$ [A,B]=AB-BA $$ Explanation Upon first encountering the definition of a commutator, one might wonder if it is not $AB - BA = 0$. However, since operators are expressed as matrices and the product of two matrices does not satisfy the commutative law, different results can</description></item><item><title>de Broglie Equation and Matter Waves</title><link>https://freshrimpsushi.github.io/en/posts/1573/</link><pubDate>Sun, 19 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1573/</guid><description>Description The question of whether light is a wave or a particle has been a major interest in the history of physics. In the early 20th century, several experiments revealed that light possesses both particle and wave properties. $$ \begin{align} E=\sqrt{p^2c^2+m_{0}^{2}c^{4}} \\ E=h\nu= \frac{hc}{\lambda} \end{align} $$ From the equation $(1)$ that expresses the relativistic energy of a particle and the equation $(2)$ derived from the photoelectric effect, it is understood</description></item><item><title>Expectation Value in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/1572/</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1572/</guid><description>Definition The expectation value of a normalized wavefunction $\psi$ for an operator $A$ is defined as follows. $$ \braket{A} =\int_{-\infty}^{\infty} \psi^{\ast}A\psi dx $$ Explanation To put it simply, it is the same expectation value that you learned in high school statistics. When studying quantum mechanics, if you find it difficult to understand the expectation value, the difficulties can be broadly classified into two types. The first is the difficulty in</description></item><item><title>Moving Point Charges and the Electric Fields They Create</title><link>https://freshrimpsushi.github.io/en/posts/1273/</link><pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1273/</guid><description>Overview 1 The electromagnetic field created by a moving point charge is as follows. $$ \begin{align*} \mathbf{E}(\mathbf{r}, t) &amp;amp;= \frac{q}{4\pi\epsilon_{0}} \frac{\cR} {( \bcR\cdot \mathbf{u} )^3 } \left[(c^2-v^2)\mathbf{u} +\bcR\times (\mathbf{u} \times \mathbf{a} ) \right] \\ \mathbf{B} (\mathbf{ r}, t) &amp;amp;=\frac{1}{c} \crH\times \mathbf{ E } (\mathbf{ r}, t) \end{align*} $$ Description An introduction to the induction process for electric fields. Induction The electric and magnetic fields created by a moving point charge</description></item><item><title>Parity Operator</title><link>https://freshrimpsushi.github.io/en/posts/1571/</link><pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1571/</guid><description>Definition The operator $P$, defined as follows, is called the parity operator. $$ P\psi (x) = \psi (-x) $$ Description It is an operator that translates the position variable of a wave function symmetrically. The parity operator $P$ is used in quantum mechanics to distinguish between two degenerate eigenfunctions. Let&amp;rsquo;s suppose there are two degenerate wave functions as follows. $$ \psi_{1}(x)=e^{ikx},\quad \psi_{2}(x)=e^{-ikx} $$ Then, solving the eigenvalue equation for the</description></item><item><title>Time Derivatives of the Lienard-Wiechert Potentials</title><link>https://freshrimpsushi.github.io/en/posts/1544/</link><pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1544/</guid><description>Overview The time derivative of the Liénard-Wiechert potential is as follows. $$ \begin{align*} \frac{ \partial V}{ \partial t} &amp;amp;= \frac{qc}{4\pi \epsilon_{0}} \frac{1}{(\cR c -\bcR \cdot \mathbf{v})^{2}} \left( c^{2} -c^{2}\frac{ \partial t}{ \partial t_{r}}-v^{2}+\bcR\cdot \mathbf{a} \right)\frac{ \partial t_{r}}{ \partial t } \\ \frac{ \partial \mathbf{A}}{ \partial t } &amp;amp;= \frac{qc}{4\pi \epsilon_{0}} \frac{1}{(\cR c -\bcR \cdot \mathbf{v})^{3}}\left[</description></item><item><title>Differential Equations for Physics: Solutions to Commonly Encountered Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1538/</link><pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1538/</guid><description>Differential Equations This has been explained as intuitively as possible for those studying undergraduate physics. A differential equation is, simply put, an equation that involves derivatives. Without any complications, since acceleration is the second derivative of position, the most famous physics formula $F=ma$ is also a differential equation. The polynomial $x^{3}+3x+1=0$ is called a third-degree equation because its highest order is 3. Similarly, when the maximum number of times differentiated</description></item><item><title>Trigonometric Form of the Legendre Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1537/</link><pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1537/</guid><description>Definition The associated Legendre differential equation in the form of a trigonometric function is as follows. $$ \begin{align} \frac{ d^{2} y}{ d \theta^{2} }+\cot \theta \frac{ d y}{ d \theta}+ \left( l(l+1) -\frac{m^{2}}{\sin ^{2 }\theta} \right)y=0 \\ \mathrm{or} \quad\frac{1}{\sin \theta}\left(\sin \theta \frac{dy}{d\theta} \right)+ \left(l(l+1) -\frac{ m^{2}}{\sin ^{2} \theta} \right)y=0 \end{align} $$ Explanation Useful for solving spherical coordinate Laplace&amp;rsquo;s equation in electromagnetics, quantum mechanics, etc. The solutions are as follows. $$</description></item><item><title>Velocity and Acceleration of an Object Moving in a Rotating Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/906/</link><pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/906/</guid><description>Formulas The velocity and acceleration of an object in a rotating coordinate system are as follows. $$ \mathbf{v} = \mathbf{v}^{\prime} + \boldsymbol{\omega} \times \mathbf{r}^{\prime} +\mathbf{V}_{0} $$ $$ \mathbf{a} = \mathbf{a}^{\prime} + \dot{\boldsymbol{\omega}} \times \mathbf{r}^{\prime}+ 2\boldsymbol{\omega} \times \mathbf{v}^{\prime}+\boldsymbol{\omega} \times ( \boldsymbol{\omega} \times \mathbf{r}^{\prime}) + \mathbf{A}_{0} $$ Rotating Coordinate System1 Imagine a train in motion and a fly buzzing around inside it. For a person inside the train, considering only the movement</description></item><item><title>For an Angle Small Enough</title><link>https://freshrimpsushi.github.io/en/posts/1516/</link><pubDate>Sun, 09 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1516/</guid><description>Description Physics makes use of the approximation $\sin x\approx x$ in many places. The reason this approximation can be used is because the following equation holds: $$ \lim \limits_{x\rightarrow 0}\frac{\sin x}{x}=1 $$ Since this equation is first introduced in high school, college students might feel it is obvious enough to not question the validity of such approximation. However, how small should something be to be considered similar? For instance, when</description></item><item><title>Vectors and Inner Products in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/1509/</link><pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1509/</guid><description>Generalization of Vectors For science students who have not learned linear algebra, a vector is a physical quantity with magnitude and direction, meaning a point in 3-dimensional space, and it is usually represented as in $\vec{x} = (x_{1}, x_{2}, x_{3})$. This definition poses no significant problem in studying classical mechanics and electromagnetism. However, in quantum mechanics, concepts like Fourier analysis and inner product of functions arise, so if one does</description></item><item><title>Frobenius Method</title><link>https://freshrimpsushi.github.io/en/posts/1508/</link><pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1508/</guid><description>Explanation1 There are various methods to solve differential equations. One of them involves assuming the solution as a power series, as follows. $$ y=\sum \limits_{n=0}^{\infty} a_{n}x^{n} $$ However, some series cannot be represented in the form above. For example, as follows. $$ \frac{\cos x}{x^{2}}=\frac{1}{x^{2}}-\frac{1}{2!}+\frac{ x^{2}}{4!}-\cdots $$ $$ \sqrt{x} \sin x = x^{\frac{1}{2}}\left( x - \frac{x^{3}}{3!}+\cdots \right) $$ In such cases, the solution is assumed to be in the following form.</description></item><item><title>Series Solution of the Bessel Equation: Bessel Functions of the First Kind</title><link>https://freshrimpsushi.github.io/en/posts/1503/</link><pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1503/</guid><description>Definition1 For $\nu \in \mathbb{R}$, a differential equation of the following form is called a $\nu$ order Bessel equation. $$ \begin{align*} &amp;amp;&amp;amp; x^{2} y^{\prime \prime} +xy^{\prime}+(x^{2}-\nu^{2})y &amp;amp;= 0 \\ \text{or} &amp;amp;&amp;amp; y^{\prime \prime}+\frac{1}{x} y^{\prime} + \left( 1-\frac{\nu^{2}}{x^{2}} \right)y &amp;amp;= 0 \end{align*} $$ Explanation The Bessel equation emerges when solving the wave equation in spherical coordinates. The coefficients are not constant but depend on the independent variable $x$. Since, at $x=0$,</description></item><item><title>Angular Momentum Operator in Spherical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1495/</link><pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1495/</guid><description>Formula The angular momentum operator is expressed in spherical coordinates as follows. $$ \begin{align*} L_{x} &amp;amp;= \i\hbar \left(\sin\phi\dfrac{\partial }{\partial \theta} + \cos\phi \cot\theta \dfrac{\partial }{\partial \phi}\right) \\ L_{y} &amp;amp;= -\i\hbar \left( \cos\phi \dfrac{\partial }{\partial \theta} - \sin\phi \cot\theta \dfrac{\partial }{\partial \phi}\right) \\ L_{z} &amp;amp;= -\i\hbar \dfrac{\partial }{\partial \phi} \end{align*} $$ Derivation The definition of the angular momentum operator is as follows. $$ L = \mathbf{r} \times P = - \i\hbar</description></item><item><title>Second Derivative, Higher Order Derivative</title><link>https://freshrimpsushi.github.io/en/posts/1094/</link><pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1094/</guid><description>Definition1 Let the function $f$ have a derivative $f^{\prime}$ on the interval $I$. If $f^{\prime}$ itself also has a derivative, we call it the second derivative of $f$ and denote it by $f^{\prime\prime}$. If $f^{\prime\prime}$ also has a derivative, we denote it as $f^{\prime \prime \prime}$, or simply as $f^{(3)}$. In the same manner, the $n$th derivative of $f$ is denoted as $f^{(n)}$. $$ f,\ f^{\prime},\ f^{\prime\prime},\ f^{(3)},\ \dots,\ f^{(n)}</description></item><item><title>Ordinary Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/1097/</link><pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1097/</guid><description>Definition1 For the univariate function $u(t)$, the following form is called an ordinary differential equation (ODE). $$ F(t, u(t), u^{\prime}(t), \dots, u^{(n)}(t)) = 0 \tag{1} $$ Here, $u^{\prime}$ is the derivative of $u$, and $u^{(n)}$ is the $n$-th order derivative of $u$, or simply referred to as $y = u(t)$, $$ F(t, y, y^{\prime}, \dots, y^{(n)}) = 0 $$ Explanation In $(1)$, $n$ is referred to as the order of</description></item><item><title>Euler Integrals: Beta Function and Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/1483/</link><pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1483/</guid><description>Definition Euler Integrals The following two integrals are referred to as Euler integrals. $(a)$ Euler integral of the first kind: Beta function $$ B(p,q)=\int_{0}^1 t^{p-1}(1-t)^{q-1}dt,\quad p&amp;gt;0,\quad q&amp;gt;0 $$ $(b)$ Euler integral of the second kind: Gamma function $$ \Gamma (p) = \int_{0}^\infty t^{p-1}e^{-t}dt,\quad p&amp;gt;0 $$ Explanation Euler Integral of the First Kind 1-1. Beta Function: If the gamma function is considered a generalization of the factorial, then the beta function</description></item><item><title>Representation of the Beta Function in the Form of an Improper Integral</title><link>https://freshrimpsushi.github.io/en/posts/1482/</link><pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1482/</guid><description>Theorem Beta Function: $$ B(p,q)=\int_{0}^{1}t^{p-1}(1-t)^{q-1}dt\quad \cdots (1) $$ The beta function can be expressed as an improper integral as follows: $$ B(p,q)=\int_{0}^{\infty}\frac{ t^{p-1} }{ (1+t)^{p+q}}dt\quad \cdots (2) $$ Explanation Using the above formula makes it easier to obtain difficult integral values. The proof is not difficult. Proof Let&amp;rsquo;s substitute $(1)$ with $t=\frac{x}{1+x}$. Then, $1-t=\frac{1}{1+x}$, and the range of integration changes to $\int_{0}^{1}\rightarrow \int_{0}^{\infty}$. Also, since $ \displaystyle \frac{ d t</description></item><item><title>Relationship between Beta Function and Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/1481/</link><pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1481/</guid><description>Theorem $$ B(p,q) = {{\Gamma (p) \Gamma (q)} \over {\Gamma (p+q) }} $$ Explanation The Beta function is defined as $\displaystyle B(p,q) := \int_{0}^{1} t^{p-1} (1-t)^{q-1} dt $, and, like the Gamma function, it is an important function applied in many fields. Since the Gamma function can be easily calculated using the recursive relationship, the Beta function can also be calculated easily using the above relation. Intuitively, it can be</description></item><item><title>Various Important Formulas Involving the Gamma Function and Factorials</title><link>https://freshrimpsushi.github.io/en/posts/1478/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1478/</guid><description>Formulas $$ \Gamma (\frac{1}{2})=\sqrt{\pi} \tag{a} $$ - Euler&amp;rsquo;s Reflection Formula: $$ \Gamma (p)\Gamma (1-p)=\dfrac{\pi}{\sin(\pi p)} \tag{b} $$ $$ \Gamma (n+\frac{1}{2})=\frac{1\cdot 3\cdot \cdot5 \cdots (2n-1)}{2^{n}}\sqrt{\pi}=\frac{(2n-1)!!}{2^n}\sqrt{\pi}=\frac{(2n)!}{4^{n}n!}\sqrt{\pi},\quad n\in \mathbb{N} \tag{c} $$ $!!$ is the Double Factorial. - Binomial Coefficient: $$ \begin{pmatrix} n \\ k \end{pmatrix}=\frac{\Gamma (n+1)}{k! \Gamma (n-k+1)} \tag{d} $$ - Euler-Mascheroni Constant: $$ \gamma=-\Gamma^{\prime} (1) \tag{e} $$ - Beta Function: $$ B(p,q)=\frac{\Gamma (p) \Gamma (q)}{\Gamma (p+q)} \tag{f} $$ Proofs Gamma Function: $$</description></item><item><title>Factorial, Double Factorial, and Multifactorial</title><link>https://freshrimpsushi.github.io/en/posts/1477/</link><pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1477/</guid><description>Factorial For a natural number $n$, $n!$ is read as $n$factorial and is defined as follows. $$ n!=n\cdot(n-1)\cdot(n-2)\cdots 2\cdot 1 =\prod\limits_{k=1}^n k $$ Description It is used in many places to neatly express equations. The factorial of $0$ is defined as $0!:=1$. By generalizing the domain of definition of factorial, one can also define something called a Gamma function. Double Factorial For a natural number $n$, $n!!$ is read as</description></item><item><title>Derivation of the Gamma Function</title><link>https://freshrimpsushi.github.io/en/posts/1476/</link><pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1476/</guid><description>Non-negative Integers and the Gamma Function For $\alpha &amp;gt;0$, $$ \int_{0}^{\infty} e^{-\alpha x} dx=\left[-\frac{1}{\alpha}e^{-\alpha x}\right]_{0}^{\infty}=\frac{1}{\alpha} $$ Differentiating both sides with respect to $\alpha$, according to the Leibniz integral rule, allows the differentiation to move under the integration sign, thus giving $$ \begin{align*} &amp;amp;&amp;amp;\int_{0}^\infty -xe^{-\alpha x}dx&amp;amp;=-\frac{1}{\alpha^2} \\ \implies &amp;amp;&amp;amp; \int_{0}^\infty xe^{-\alpha x}dx &amp;amp;= \frac{1}{\alpha ^2} \end{align*} $$ Continuing to differentiate gives $$ \begin{align*} \int_{0}^\infty x^2e^{-\alpha x}dx&amp;amp;=\frac{2}{\alpha^3} \\ \int_{0}^\infty x^3e^{-\alpha x}dx&amp;amp;=\frac{3\cdot 2}{\alpha^4}</description></item><item><title>Leibniz Integral Rule</title><link>https://freshrimpsushi.github.io/en/posts/1475/</link><pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1475/</guid><description>Theorem Let&amp;rsquo;s assume that $f(x,t)$ and $\dfrac{\partial f}{\partial x}(x,t)$ are consecutive. Then, the following equation holds. $$ \frac{d}{dx} \int_{a}^b f(x,t)dt = \int_{a}^b\frac{\partial f}{\partial x}(x,t)dt $$ Description Being able to interchange the order of differentiation and integration is undoubtedly useful. Besides, there are many theorems or formulas related to differentiation and integration named after Leibniz. Proof Since if continuous, then integrable, let&amp;rsquo;s assume $u$ as follows. $$ u(x):=\int_{a}^b f(x,t)dt $$ Then,</description></item><item><title>Properties of the Interior in Topological Spaces and Subspaces</title><link>https://freshrimpsushi.github.io/en/posts/1473/</link><pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1473/</guid><description>Theorem Given a topological space $(X,\mathcal{T})$ and a subset $A,B,A_{\alpha}\subset X\ (\alpha \in \Lambda)$, then: $(a1)$: If $A\subset B$, then $A^{\circ} \subset B^{\circ}$. $(b1)$: $A^{\circ}\cup B^{\circ} \subset (A\cup B)^{\circ}$ $(c1)$: $A^{\circ} \cap B^{\circ} = (A\cap B)^{\circ}$ $(d1)$: $(\cap_{\alpha\in\Lambda}A_{\alpha})^{\circ} \subset \cap _{\alpha \in \Lambda} A_{\alpha}^{\circ}$ Interior of Subspace Even if it&amp;rsquo;s the same set, depending on how the whole space is given, it may or may not become an open set.</description></item><item><title>Generating Topology from a Basis</title><link>https://freshrimpsushi.github.io/en/posts/1470/</link><pubDate>Mon, 06 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1470/</guid><description>Buildup Topology For a set $X$, a collection $\mathscr{T}$ of subsets of $X$ that satisfies the following three conditions is called the topology on set $X$. $(T1)$ $\varnothing, X \in \mathscr{T}$ If $(T2)$ $U_{\alpha} \in \mathscr{T} (\alpha \in \Lambda)$, then $\bigcup_{\alpha \in \Lambda} U_{\alpha} \in \mathscr{T}$. If $(T3)$ $U_{1},\cdots,U_{n} \in \mathscr{T}$, then $\bigcap_{i=1}^{n}U_{i} \in \mathscr{T}$. In simple terms, a collection of subsets that contains the empty set and the whole</description></item><item><title>How to Change Image Size in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1466/</link><pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1466/</guid><description>Resizing Images To resize images, you can use the imresize function from the Images package. The function name is the same as in Matlab. imresize(X, ratio=a): Returns the image of array X scaled by a factor of a. Unlike Matlab, you must explicitly write ratio=a. imresize(X, m, n): Returns the image of array X resized to m rows and n columns. Below are example codes and their results. using Images</description></item><item><title>Measuring Code Execution Time in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1467/</link><pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1467/</guid><description>Methods tic X1=rand(2^7); X2=rand(2^8); X3=rand(2^9); X4=rand(2^10); X5=rand(2^11); toc Y1=imrotate(X1,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y2=imrotate(X2,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y3=imrotate(X3,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y4=imrotate(X4,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y5=imrotate(X5,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc tic: Starts a stopwatch for measuring execution time. toc: Returns the current time on the stopwatch. Note that it&amp;rsquo;s not measuring the time between toc and toc. To measure the computation time of calculating Y1~Y6 in the example code above, you should enter the code as follows. tic X1=rand(2^7); X2=rand(2^8); X3=rand(2^9); X4=rand(2^10);</description></item><item><title>How to Rotate Image Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1462/</link><pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1462/</guid><description>Image Rotation imrotate(X, theta): Rotates array X by theta radians. Note that, unlike in MATLAB where the angle unit is degrees ($^{\circ})$, the angle unit here is radians. Additionally, unlike MATLAB, it rotates clockwise. If no other variables are inputted, the interpolation method defaults to bilinear, and the rotated image is not cropped. Examples of rotating the original image X by $90^\circ=\pi/2$, $180^\circ=\pi$, and $270^\circ=\frac{3}{2}\pi$, along with their results, are</description></item><item><title>Resizing Images in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1465/</link><pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1465/</guid><description>Methods imresize(A, scale): Returns a new image by adjusting the size of A by a factor of scale. If A is a 10x10 image and a scale of 0.5 is input, it returns a 5x5 image. You can also adjust the size directly as follows. imresize(A, [m n]): Returns an image with m rows and n columns. Below are example codes and their results. X=imread(&amp;#39;test\_{i}mage.jpg&amp;#39;); figure() imshow(X) saveas(gcf,&amp;#39;X.png&amp;#39;) title(&amp;#39;X&amp;#39;) Y1=imresize(X,0.5);</description></item><item><title>Functions for 2D Array Operations in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1460/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1460/</guid><description>Let&amp;rsquo;s say $A = \begin{pmatrix} 1 &amp;amp; 2 &amp;amp; 1 \\ 0 &amp;amp; 3 &amp;amp; 0 \\ 2 &amp;amp; 3 &amp;amp; 4\end{pmatrix}$. Transpose Matrix julia&amp;gt; A =[1 2 1; 0 3 0; 2 3 4] 3×3 Array{Int64,2}: 1 2 1 0 3 0 2 3 4 julia&amp;gt; transpose(A) 3×3 LinearAlgebra.Transpose{Int64,Array{Int64,2}}: 1 0 2 2 3 3 1 0 4 julia&amp;gt; A&amp;#39; 3×3 LinearAlgebra.Adjoint{Int64,Array{Int64,2}}: 1</description></item><item><title>How to output and save arrays as heatmap images in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1459/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1459/</guid><description>Heatmap Using the heatmap function from the Plots package, you can output a 2D array as a heatmap image, and with the savefig function, you can save the resulting image. The @__DIR__ macro tells you the location of the Julia code file. # code1 However, if you compare array A with the heatmap image, you may notice that the top and bottom of the array are flipped in the heatmap</description></item><item><title>Translating Arrays in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1453/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1453/</guid><description>Description Using circshifr(A, (n,m)), you can shift the rows of the array A $n$ positions down, and the columns $m$ positions to the right. (n,m) must be a tuple of integers, and negative numbers are also possible. If negative, it shifts in the opposite direction. For arrays of 3 dimensions or more, it is applied to each smallest 2-dimensional array respectively. Code 2D array julia&amp;gt; A = transpose(reshape(1:25,5,5)) 5×</description></item><item><title>Various Methods of Creating Vectors in Julia</title><link>https://freshrimpsushi.github.io/en/posts/1452/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1452/</guid><description>Code julia&amp;gt; x1=[1 2 3] 1×3 Array{Int64,2}: 1 2 3 julia&amp;gt; x2=[1, 2, 3] 3-element Array{Int64,1}: 1 2 3 julia&amp;gt; x3=[i for i in 1:3] 3-element Array{Int64,1}: 1 2 3 julia&amp;gt; x4=[i for i in 1:3:10] 4-element Array{Int64,1}: 1 4 7 10 julia&amp;gt; x5=[i for i in 1:3:11] 4-element Array{Int64,1}: 1 4 7 10 x1 is a 2-dimensional array. Since it looks like a row vector, if you</description></item><item><title>Subspace Topology, Relative Topology</title><link>https://freshrimpsushi.github.io/en/posts/1439/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1439/</guid><description>Definition 1 Let&amp;rsquo;s assume that a topological space $(X,\mathscr{T})$ and a subset $A \subset X$ are given. Then, the following set $$ \mathscr{T}_{A} =\left\{ A\cap U\ :\ U\in \mathscr{T} \right\} $$ is a topology on $A$. In this case, $\mathscr{T}_{A}$ is referred to as the Subspace Topology or Relative Topology. Moreover, the topological space $(A, \mathscr{T}_{A})$ is called the Subspace of $(X,\mathscr{T})$. Theorem [0]: For a topological space $(X, \mathscr{T}$)</description></item><item><title>Several Equivalent Conditions for the Interior in a Topological Space</title><link>https://freshrimpsushi.github.io/en/posts/1424/</link><pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1424/</guid><description>Definition 1 Let us consider a topological space $(X,\mathcal{T})$ and a subspace $A$. The union of all open sets contained in $A$ is called the interior of $A$, denoted by $A^{\circ}$ or $\mathrm{int}(A)$. $$ A^{\circ} = \cup \left\{ U \in \mathcal{T} \ :\ U \subset A\right\} $$ Furthermore, if there exists an open set $U$ satisfying $x \in U \subset A$ with respect to $x \in X$, then $x$ is</description></item><item><title>Easy Ways to Memorize Surjections, Injections, Ranges, and Domains, Explained</title><link>https://freshrimpsushi.github.io/en/posts/690/</link><pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/690/</guid><description>Explanation I found it really hard to memorize the names when I first encountered injective, surjective, and codomain. It was easier to distinguish them in English, but in Korean, it didn&amp;rsquo;t stick well. &amp;lsquo;Is this injective or surjective?&amp;rsquo; &amp;lsquo;Was the codomain the larger one? What was it?&amp;rsquo;. Since the names sounded similar, I was always confused when I needed to use them. I don&amp;rsquo;t know how many people are like</description></item><item><title>Homomorphism Preserves Basis</title><link>https://freshrimpsushi.github.io/en/posts/1407/</link><pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1407/</guid><description>Theorem Let&amp;rsquo;s suppose two topological spaces $X$, $Y$ and a homeomorphism $f$ between them are given. $$ f\ :\ X \rightarrow Y $$ If $\mathcal{B}_{X}$ is called a basis of $X$, then $f(\mathcal{B}_{X})$ becomes a basis of $Y$. Description Simply put, a homeomorphic mapping preserves the basis. Proof A collection $\mathcal{B}$ of subsets of $X$ that satisfies the following two conditions is called a basis for the topology on $X$:</description></item><item><title>In Topology, What is a Coordinate System?</title><link>https://freshrimpsushi.github.io/en/posts/1404/</link><pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1404/</guid><description>Definition Let us consider $M$ to be a $n$-dimensional manifold. Suppose two open sets $U\subset M$, $\tilde{U} \subset \mathbb{R}^n$ and a homeomorphism $\phi\ :\ U \rightarrow \tilde{U}$ are given. Then, the ordered pair $(U, \phi)$ is called the coordinate system on $M$, or simply coordinates$(\mathrm{Chart})$. Explanation If $p \in U$, $\phi (p)=0$, then $(U,\phi)$ is called the center in $p$. Moreover, $U$ is referred to as the coordinate domain or</description></item><item><title>Maximal Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1390/</link><pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1390/</guid><description>Theorem1 For every $f \in L^1_{\mathrm{loc}}$ and every $\alpha &amp;gt;0$, there exists a constant $C&amp;gt;0$ that satisfies the following condition. $$ \mu \big( \left\{ x\ :\ Hf(x)&amp;gt;\alpha \right\}\big) \le \frac{C}{\alpha} \int |f(y)| dy $$ This inequality is called the Hardy-Littlewood maximal inequality. The Hardy-Littlewood maximal function $$ Hf (x) = \sup \limits_{r&amp;gt;0} A_{r} |f|(x) = \sup \limits_{r&amp;gt;0} \frac{1}{\mu \big( B(r,x) \big)}\int_{B(r,x)}|f(y)|dy $$ Proof Let&amp;rsquo;s say $E_\alpha =\left\{ x\ |\ Hf(x)</description></item><item><title>The Mean Value of Locally Integrable Functions Converges to the Value of the Function at the Center.</title><link>https://freshrimpsushi.github.io/en/posts/1391/</link><pubDate>Tue, 22 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1391/</guid><description>Theorem1 Let&amp;rsquo;s say $f \in L^1_{\mathrm{loc}}$. Then, the following is true. $$ \lim \limits_{r \rightarrow 0} A_{r} f(x)=f(x) \text{ a.e. } x\in \mathbb{R}^n $$ Here, $\text{ a.e. }$ is almost everywhere. Description The message here is that the limit of the average value of a locally integrable function&amp;rsquo;s value over the volume $B(r,x)$ as the radius goes to $0$ equals the function value at the center of the volume. Proof</description></item><item><title>Hardy-Littlewood Maximal Function</title><link>https://freshrimpsushi.github.io/en/posts/1389/</link><pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1389/</guid><description>Definition1 Let&amp;rsquo;s denote $ f \in L^1_{\mathrm{loc}}$. Then, the Hardy-Littlewood maximal function $Hf$ is defined as follows: $$ Hf (x) := \sup \limits_{r&amp;gt;0} A_{r} |f|(x) = \sup \limits_{r&amp;gt;0} \frac{1}{\mu \big( B(r,x) \big)}\int_{B(r,x)}|f(y)|dy $$ $A_{r}f(x)$ represents the average of the function values of $B_{r}(x)$ on the top of $f$. $H$ is called the maximal operator. Theorem $Hf$ is a Lebesgue measurable function. If $f \in L^1_{\mathrm{loc}}$, then $A_{r}f(x)$ is continuous with</description></item><item><title>Maximal Lemma</title><link>https://freshrimpsushi.github.io/en/posts/1388/</link><pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1388/</guid><description>Theorem1 Let&amp;rsquo;s call a collection of open balls at $\mathbb{R}^n$ given $\mathcal{B}$. Let&amp;rsquo;s say $U=\bigcup \limits_ { B\in \mathcal{B}} B$. Then, for some constant $c \lt m (U)$, there exist a finite number of mutually disjoint $B_{j} \in \mathcal{B}$ that satisfy the following condition. $$ \dfrac{c}{3^{n}} \lt \sum \limits_{j=1}^{k} m(B_{j}) $$ Here, $m$ is the $n$-dimensional Lebesgue measure. Description Actually, this theorem is not officially named the maximal lemma, but</description></item><item><title>Saturation and Definition of Fibers in Mathematics</title><link>https://freshrimpsushi.github.io/en/posts/1387/</link><pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1387/</guid><description>Definitions Given two sets $X$, $Y$ and a function $\pi\ :\ X\rightarrow Y$. If $\pi^{-1}\big( \pi (u) \big)=u$ holds, then $u\subset X$ is called saturation. The set $\pi^{-1}(y) \subset X$ is called the fiber or stalk over the point $y\in Y$ in $\pi$. Description $\pi^{-1}$ is a preimage. Let&amp;rsquo;s easily understand through the pictures below. Saturation $u$ is always less than or equal to $\pi^{-1} \big( \pi (u) \big)$. Thus,</description></item><item><title>Disjoint Union: Disjoint Unions</title><link>https://freshrimpsushi.github.io/en/posts/1385/</link><pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1385/</guid><description>Definition Let $\left\{ X_{\alpha} \right\} _{\alpha\in A}$ be an arbitrary index family. Then, the set of ordered pairs defined as follows is called the $\left\{ X_{\alpha}\right\}$ disjoint union. $$ \bigsqcup \limits_{\alpha \in A} X_{\alpha} := \left\{ (x,\alpha)\ |\ x\in X_{\alpha},\ \alpha \in A \right\} $$ Explanation Instead of $\bigsqcup$, $\amalg$, $\biguplus$, etc., are also used. Note that $\amalg$ is not the capital Pi $\Pi$. It is the mirrored version of</description></item><item><title>Separated Union Topological Space</title><link>https://freshrimpsushi.github.io/en/posts/1386/</link><pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1386/</guid><description>Definition Let $\left\{ X_\alpha \right\}_{\alpha \in A}$ be an arbitrary topological space index family. Let us say $u \subset \bigsqcup \limits_{\alpha \in A} X_\alpha$. Then, for all $\alpha \in A$, if $u \cap X_\alpha$ is an open set in $ X_\alpha$, then $u$ is said to be an open set $^{\ast}$ in $\bigsqcup \limits_{\alpha \in A} X_\alpha$. The so-called open$^{\ast}$ here is not exactly open in the sense of topology.</description></item><item><title>Complex Measures, Vector Measures</title><link>https://freshrimpsushi.github.io/en/posts/1378/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1378/</guid><description>Definition1 Let $(X,\mathcal{E})$ be a measurable space. A function $\nu : \mathcal{E} \to \mathbb{C}$ that satisfies the following conditions is called a complex measure or vector measure on $(X,\mathcal{E})$. (a) $\nu (\varnothing) = 0$ (b) For mutually disjoint $E_{j} \in \mathcal{E}$, $$ \nu \left( \bigcup \limits_{j=1}^\infty E_{j} \right) = \sum \limits_{1} ^\infty \nu (E_{j}) $$ Explanation (b) signifies countable additivity. Unlike measures and signed measures, complex measures are defined not</description></item><item><title>Algebra, Quasi-measure</title><link>https://freshrimpsushi.github.io/en/posts/1377/</link><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1377/</guid><description>Definition A collection $\mathcal{A}$ of subsets of a set $X \ne \varnothing$ is called the algebra of sets on $X$ if it satisfies the following three conditions: (a) If $E_{1}$, $\cdots$, and $E_{n}\in \mathcal{A}$, then $\bigcup \nolimits_{1}^n E_{n} \in \mathcal{A}$ is true. (b) If $E_{1}$, $\cdots$, and $E_{n}\in \mathcal{A}$, then $\bigcap \nolimits_{1}^n E_{n} \in \mathcal{A}$ is true. (c) If $E \in \mathcal{A}$, then $E^c\in \mathcal{A}$ is true. Let&amp;rsquo;s refer to</description></item><item><title>How to Create Equally Spaced Row Vectors in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1376/</link><pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1376/</guid><description>Method linspace(a,b,n): Returns a row vector of $[a,b]$ divided into $n$ equal intervals. If the number of elements is not specified, it returns a $1\times 100$ vector. It is used when the number of intervals is important, not the length of the intervals. a: m :b : Returns a row vector of $[a,b]$ divided by equal intervals of $m$. If the interval is not specified, the interval is set to</description></item><item><title>Relationship between Absolutely Continuous and Integrable Functions</title><link>https://freshrimpsushi.github.io/en/posts/1373/</link><pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1373/</guid><description>Buildup Consider the following proposition. Given a measure $\mu$ and $\mu-$ integrable function $f$ on a measurable space $(X,\mathcal{E})$, then there exists a $\nu \ll\mu$ $\nu$ depending on $f$. It&amp;rsquo;s hardly a proof to show it. If we define $\nu$ as follows, we know that such a $\nu$ exists because it satisfies the above conditions and thus is a $\nu \ll\mu$. $$ \nu (E):=\int_{E} f d\mu,\quad E \in \mathcal{E} $$</description></item><item><title>Lebesgue-Radon-Nikodym Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1371/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1371/</guid><description>Theorem1 A finite measure $\mu$, $\nu$ on a measurable space $(X, \mathcal{E})$ is given. Then, either $\mu \perp \nu$ or there exist $\epsilon&amp;gt;0$, $E \in \mathcal{E}$ satisfying the conditions below. $$ \mu (E) &amp;gt;0 \quad \text{and} \quad \nu (E) \ge \epsilon \mu (E) $$ Explanation Although this theorem does not have a specific name, it is used as an auxiliary lemma when proving the Lebesgue-Radon-Nikodym theorem. It contains quite a</description></item><item><title>Absolute Continuity of the Sign Measure</title><link>https://freshrimpsushi.github.io/en/posts/1369/</link><pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1369/</guid><description>Definitions1 Given a signed measure $\nu$ and a positive measure $\mu$ on a measurable space $(X, \mathcal{E})$, for all $E \in \mathcal{E}$, $$ \mu (E) = 0 \implies \nu (E) = 0 $$ then $\nu$ is absolutely continuous with respect to $\mu$, denoted as $\nu \ll \mu$. Explanation Absolute continuity This is a generalization of absolute continuity for measures. Like measures that are absolutely continuous, the following equivalent condition holds.</description></item><item><title>Total Variation</title><link>https://freshrimpsushi.github.io/en/posts/1367/</link><pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1367/</guid><description>Definition1 A total variation $| \nu |$ of a signed measure $\nu$ on a measurable space $(X, \mathcal{E})$ is defined as follows. $$ |\nu |= \nu^{+} +\nu^{-} $$ Here, $\nu=\nu^{+}-\nu^{-}$ is the Jordan decomposition of $\nu$. Explanation $\nu^{+}$ and $\nu^{-}$ are called the positive variation and negative variation of $\nu$, respectively. The Jordan decomposition and the total variation for a measure are exactly the same as the method of expressing</description></item><item><title>Selecting Specific Rows and Columns in a Matrix in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1362/</link><pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1362/</guid><description>Methods $m \times n$ Given a data in the form of a matrix, let&amp;rsquo;s call it $A$. If you want to use only a specific part of matrix $A$, you can use the following method. B=A(a:b, c:d) Running the code as above, $B$ becomes a $(b-a) \times (d-c)$ matrix containing the data from row $a$ to $b$, column $c$ to $d$ of matrix $A$. Below is the example code and</description></item><item><title>Matrix Functions, Definition of Matrix Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/1342/</link><pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1342/</guid><description>Definitions1 $$ \mathbf{x}(t) = \begin{pmatrix} x_{1}(t) \\ \vdots \\ x_{n}(t) \end{pmatrix},\quad \mathbf{A}(t) = \begin{pmatrix} a_{11}(t) &amp;amp; \cdots &amp;amp; a_{1m}(t) \\ \vdots &amp;amp; &amp;amp; \vdots \\ a_{n1}(t) &amp;amp; \cdots &amp;amp; a_{nm}(t) \end{pmatrix} $$ If each element of a matrix is a function of variable $t$, it is called a matrix function. All elements of $\mathbf{A}(t)$, i.e., all $a_{ij}$ being continuous at a given point (or interval) means $\mathbf{A}(t)$ is continuous. If</description></item><item><title>Chain Rule for Fréchet Derivatives</title><link>https://freshrimpsushi.github.io/en/posts/1334/</link><pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1334/</guid><description>Theorem Let&amp;rsquo;s assume $(X, \left\| \cdot \right\|_{X}), (Y, \left\| \cdot \right\|_{Y}), (Z, \left\| \cdot \right\|_{Z})$ is a Banach space. Let $\Omega \subset X$, $U \subset Y$ be open sets. And functions $F : \Omega \to Y$, $G : U \to Z$ are given. Then, $F(\Omega) \subset U$ is satisfied. Now, let&amp;rsquo;s assume $F$ is differentiable at $x\in\Omega$ in the sense of [Fréchet]</description></item><item><title>Fréchet Derivative</title><link>https://freshrimpsushi.github.io/en/posts/1332/</link><pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1332/</guid><description>Definition Given two Banach spaces $X, Y$ and an open set $\Omega \subset X$. Then a function $F : \Omega \to Y$ is said to be Frechet differentiable at $x\in \Omega$ if there exists a bounded linear operator $L : X \to Y$ that satisfies the following condition: $$ \lim \limits_{ \left\| y \right \| \to 0} \frac{\| F(x+y) -F(x)-Ly \|}{\|y\|}=0 $$ In this case, such a linear transformation $L$</description></item><item><title>How to Specify Colors, Line Styles, and Marker Types in MATLAB Graphs</title><link>https://freshrimpsushi.github.io/en/posts/1330/</link><pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1330/</guid><description>Properties The properties of a graph can be specified as follows. Graph Color Marker Line Style Red r Dot . Solid - Green g Star * Dotted : Blue b X x Dash-dot -. Black k Circle o (letter o) Dashed -- Yellow y Plus + Magenta m Square s White w Diamond d Cyan c Star p Triangle down v Triangle up ^ Triangle left &amp;lt; Triangle right &amp;gt;</description></item><item><title>Rotating an Image in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1328/</link><pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1328/</guid><description>Method imrotate(I,angle,method,bbox) I: Image to be rotated. angle: The angle of rotation in degrees. method: The interpolation method. Options are &amp;rsquo;nearest&amp;rsquo;, &amp;lsquo;bilinear&amp;rsquo;, &amp;lsquo;bicubic&amp;rsquo;. If nothing is specified, &amp;rsquo;nearet&amp;rsquo; is applied. X = phantom(&amp;#39;Modified Shepp-Logan&amp;#39;,64); figure() imagesc(X) title(&amp;#39;X&amp;#39;) Y1=imrotate(X,30,&amp;#39;nearest&amp;#39;,&amp;#39;crop&amp;#39;); Y2=imrotate(X,30,&amp;#39;bilinear&amp;#39;,&amp;#39;crop&amp;#39;); Y3=imrotate(X,30,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); figure() subplot(1,3,1) imagesc(Y1) title(&amp;#39;Y1 - nearest&amp;#39;) subplot(1,3,2) imagesc(Y2) title(&amp;#39;Y2 - bilinear&amp;#39;) subplot(1,3,3) imagesc(Y3) title(&amp;#39;Y3 - bicubic&amp;#39;) bbox: Specifies the size of the output image. &amp;rsquo;loose&amp;rsquo; enlarges the size of the</description></item><item><title>Creating Special Matrices in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1327/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1327/</guid><description>Zero Matrix zeros(): Returns a zero matrix. zeros(n): Returns a $n\times n$ zero matrix. zeros(m,n): Returns a $n\times m$ zero matrix. zeros(size(A)): Returns a zero matrix of the same size as matrix A. Matrix with All Elements as 1 ones(): Returns a matrix where all elements are 1. However, for operations between two matrices, it&amp;rsquo;s more convenient to just use 1. It&amp;rsquo;s obvious that the code below is much simpler</description></item><item><title>How to Perform Element-wise Operations on Two Matrices in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1326/</link><pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1326/</guid><description>Multiplication times(), .*: Returns the result of multiplying each element of two matrices. The operation can only proceed if the two matrices are of the exact same size, or one of them is a scalar, or if one is a row vector with the same row size, or a column vector with the same column size. If the sizes are different, the smaller matrix is treated as if it were</description></item><item><title>Uniform C^m-Regularity Condition</title><link>https://freshrimpsushi.github.io/en/posts/1324/</link><pubDate>Sun, 22 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1324/</guid><description>Definition1 If there exists a locally finite open cover $\left\{ U_{j} \right\}$ of $\mathrm{bdry}\Omega$, and a sequence $\left\{ \Phi_{j} \right\}$ of $m$-smooth transformations taking $U_{j}$ onto the ball $B=\left\{ y\in \mathbb{R}^n : |y| \lt 1 \right\}$, with an inverse transformation $\Psi _{j}=\Phi_{j}^{-1}$ existing and satisfying $\text{(i)}$ ~ $\text{(iv)}$, then the open set $\Omega \subset \mathbb{R}^n$ satisfies the uniform $C^{m}$-regularity condition. $\text{(i)}$ For any $\delta &amp;gt;0$, $\Omega_{&amp;lt;\delta}$$\subset \bigcup \nolimits_{j=1}^\infty \Psi \Big(</description></item><item><title>Matrix Size and Related Functions in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1323/</link><pubDate>Sat, 21 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1323/</guid><description>Functions size(): Returns a row vector that contains the lengths of the rows and columns of the matrix. It is useful for creating a zero matrix of the same size as the matrix being dealt with. zeros(size(A)): Returns a zero matrix of the same size as A. length(): Returns the larger number among the rows and columns. In the case of row vectors and column vectors, it is the same</description></item><item><title>Strong Local Lipschitz Condition</title><link>https://freshrimpsushi.github.io/en/posts/1319/</link><pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1319/</guid><description>Definition1 If there exists a locally finite open cover $\left\{ U_{j} \right\}$ of $\delta \gt 0$, $M \gt 0$, and $\mathrm{bdry}\Omega$, such that for each $j$, there is a real-valued function $f_{j}$ with $n-1$ variables satisfying $\text{(i)}$ ~ $\text{(iv)}$, then the open set $\Omega \subset \mathbb{R}^n$ satisfies the strong local Lipschitz condition. For all pairs $x,y\in$ $\Omega_{\lt \delta}$ that satisfy $|x-y| \lt \delta$, there exists $j$ that satisfies the condition</description></item><item><title>Uniform Cone Condition</title><link>https://freshrimpsushi.github.io/en/posts/1318/</link><pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1318/</guid><description>Definition1 If there exists a locally finite open cover $\left\{ U_{j} \right\}$ of the boundary of $\Omega$ and a corresponding sequence of finite cones $\left\{ C_{j} \right\}$ that satisfy $\text{(i)}$ ~ $\text{(iv)}$, then the open set $\Omega \subset \mathbb{R}^n$ is said to satisfy the uniform cone condition. $\text{(i)}$ There exists $M \lt \infty$ such that every $U_{j}$ has a diameter smaller than $M$. $\text{(ii)}$ For some $\delta \gt 0$ $\Omega_{\lt</description></item><item><title>Sets Outside/Inside a Certain Distance from the Boundary of a Set</title><link>https://freshrimpsushi.github.io/en/posts/1317/</link><pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1317/</guid><description>Definition Let us assume an open set $\Omega \subset \mathbb{R}^n$ is given. Then, $\Omega_{&amp;lt;\delta}$ and $\Omega_{&amp;gt;\delta}$ are defined as follows. $$ \begin{align*} \Omega_{&amp;lt;\delta} :=&amp;amp; \left\{ x\in\Omega : \mathrm{dist}(x, \mathrm{bdry}\Omega)&amp;lt;\delta \right\} \\ \Omega_{&amp;gt;\delta} :=&amp;amp; \left\{ x\in\Omega : \mathrm{dist}(x, \mathrm{bdry}\Omega)&amp;gt;\delta \right\} \end{align*} $$ Explanation Such sets are usefully employed in partial differential equations, functional analysis, etc. Depending on the textbook, there are cases where it&amp;rsquo;s $\Omega_\delta=\Omega_{&amp;lt;\delta}$1 and cases where it&amp;rsquo;s $\Omega_\delta=\Omega_{&amp;gt;\delta}$2. In</description></item><item><title>Jordan Decomposition Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1311/</link><pubDate>Tue, 17 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1311/</guid><description>Theorem Let&amp;rsquo;s assume a measurable space $(X,\mathcal{E})$ and a signed measure $\nu$ defined on it. Then, there uniquely exist two positive measures $\nu^{+}$, $\nu^{-}$ that satisfy the following conditions, and $\nu=\nu^{+}-\nu^{-}$ is called the Jordan decomposition of $\nu$. $$ \nu=\nu^{+}-\nu^{-} $$ $$ \nu^{+} \perp \nu^{-} $$ If $X=P \cup N$ is called a partition, then $\nu^{+}, \nu^{-}$ is as follows. $$ \begin{align*} \nu^{+} (E) &amp;amp;= \nu ( E \cap P)</description></item><item><title>Mutually Singular</title><link>https://freshrimpsushi.github.io/en/posts/1310/</link><pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1310/</guid><description>Definition1 Given two signed measures $\nu$, $\nu$. If there exists a $E,F\ \in \mathcal{E}$ that satisfies the following three conditions for $\nu$, $\mu$, we say that the two signed measures $\nu$, $\mu$ are and denote it as $\nu \perp \mu$ or $\mu \perp \nu$: $E \cup F=X$ $E \cap F=\varnothing$ $E$ is a null set with respect to $\nu$, and $F$ is a null set with respect to $\mu$. Also,</description></item><item><title>Hahn Decomposition Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1308/</link><pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1308/</guid><description>Theorem1 (a) Let $\nu$ be a signed measure defined on a measurable space $(X, \mathcal{E})$. Then there exist a positive set $P$ and a negative set $N$ for $\nu$, satisfying the following: $$ P \cup N=X \quad \text{and} \quad P \cap N =\varnothing $$ Such a $X=P \cup N$ is called a Hahn decomposition for $\nu$. (b) Let $P^{\prime}, N^{\prime}$ be another pair of sets satisfying (a). Then the following</description></item><item><title>Positive Set, Negative Set, Null Set</title><link>https://freshrimpsushi.github.io/en/posts/1303/</link><pubDate>Sat, 14 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1303/</guid><description>Definition1 Let us call $\nu$ on $(X,\mathcal{E})$ a sign measure. And let us denote $E,F \in \mathcal{E}$. Then When $\nu (F) \ge 0,\ \forall F\subset E$, we call $E$ regarding $\nu$ a positive set or simply positive. When $\nu (F) \le 0,\ \forall F\subset E$, we call $E$ regarding $\nu$ a negative set or simply negative. When $\nu (F)=0,\ \forall F\subset E$, we call $E$ regarding $\nu$ a null set</description></item><item><title>Signed Measures</title><link>https://freshrimpsushi.github.io/en/posts/1301/</link><pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1301/</guid><description>Definition1 Let $(X, \mathcal{E})$ be a measurable space. A function $\nu : \mathcal{E} \to \overline{\mathbb{R}}$ which takes extended real values and satisfies the conditions below is called a signed measure. $\nu ( \varnothing ) = 0$ At most one of $\pm \infty$ can have a function value of $\nu$. In other words, if $-\infty \in \nu (\mathcal{E})$ then $+\infty \notin \nu (\mathcal{E})$, and if $+\infty \in \nu (\mathcal{E})$ then $-\infty</description></item><item><title>General Definitions of Measure</title><link>https://freshrimpsushi.github.io/en/posts/1302/</link><pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1302/</guid><description>Definition Let $(X,\mathcal{E})$ be a measurable space. A function $\mu : \mathcal{E} \to \overline{\mathbb{R}}$ that takes extended real values is called a measure if it satisfies the following three conditions: (a) $\mu ( \varnothing ) = 0$ (b) $\mu (E) \ge 0,\quad \forall E\in \mathcal{E}$ (c) Suppose $\left\{E_{j}\right\}$ are sequences of mutually disjoint sets in $\mathcal{E}$. Then the following holds: $$ \mu \left( \bigcup _{j=1}^\infty E_{j} \right) =\sum \limits_{j=1}^\infty \mu</description></item><item><title>Line Segment Conditions</title><link>https://freshrimpsushi.github.io/en/posts/1298/</link><pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1298/</guid><description>Definition1 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. For all $x \in \mathrm{bdry}\Omega$, if there exists a neighborhood $x$ of $U_{x}$ and a nonzero vector $y_{x}$ such that the following condition is satisfied, then $\Omega$ satisfies the segment condition. $$ z\in \overline{\Omega}\cap U_{x} \quad \implies \quad z+ty_{x} \in \Omega, 0 \lt t \lt 1 $$ Robert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p82&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Weak Cone Condition</title><link>https://freshrimpsushi.github.io/en/posts/1297/</link><pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1297/</guid><description>Definition1 Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. Suppose a point $x \in \Omega$ is given. Let $R(x)$ be the set of all $y$ that ensure the line segment from $x$ to $y \in \Omega$ is included within $\Omega$ again. That is, $R(x)$ is the set of points on all lines starting from $x$ within $\Omega$. And let $\Gamma (x)$ be defined as follows. $$ \begin{align*} \Gamma (x)</description></item><item><title>Locally Finite Covers</title><link>https://freshrimpsushi.github.io/en/posts/1295/</link><pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1295/</guid><description>Definition1 English An open cover $\mathcal{O}$ of a set $S \subset \mathbb{R}^n$ is said to be locally finite if any compact set in $\mathbb{R}^n$ can intersect at most finitely many members of $\mathcal{O}$. Explanation Even an infinite cover can be locally finite. By definition, a locally finite cover is at most countable, and a finite set is naturally locally finite. Furthermore, if $S$ is closed, then any open cover of</description></item><item><title>Under Cone Conditions</title><link>https://freshrimpsushi.github.io/en/posts/1296/</link><pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1296/</guid><description>Definition1 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. If there exists some finite cone such that for each $x \in \Omega$, there exists a finite cone $C_{x} \subset \Omega$ having $x$ as its vertex, then $\Omega$ satisfies the cone condition. Explanation For all $x\in \Omega$, if there exists $C_{x} \in \Omega$ as shown in the figure above, then $\Omega$ satisfies the cone condition. If $\Omega$ includes a pointed part</description></item><item><title>Finite Cone</title><link>https://freshrimpsushi.github.io/en/posts/1287/</link><pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1287/</guid><description>Definition1 Let $v$ be a unit vector2 in $\mathbb{R}^n$. For each non-zero $x\in \mathbb{R}^n$, let $\angle(x,v)$ be the angle between two vectors $v,x$. Then, for given $v$, $\rho \gt 0$, $0 \lt \kappa \le \pi$, the set $C$ is called a finite cone of height $\rho$, axis direction $v$, and aperture angle $\kappa$ with the vertex at the origin. $$ C= \left\{ x \in \mathbb{R}^n \ \ \big| \ \</description></item><item><title>Definition of a General Parallelepiped</title><link>https://freshrimpsushi.github.io/en/posts/1286/</link><pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1286/</guid><description>Definition Given $n$ linearly independent vectors $y_{1},\ \cdots,\ y_{n} \in \mathbb{R}^n$, the set $P$ is known as a parallelepiped. $$ P = \left\{ \sum \limits_{j=1}^{n} \lambda_{j} y_{j} \ \ \Big| \quad 0\le \lambda_{j} \le 1 \right\} $$ Description As defined, it includes the origin as a vertex. Simply put, it is the set of all linear combinations with coefficients up to 1. For $n=3$, it forms a parallelepiped, and for</description></item><item><title>Gradient of Time Delay</title><link>https://freshrimpsushi.github.io/en/posts/1274/</link><pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1274/</guid><description>Overview The gradient of time delay is as follows. $$ \nabla t_{r}=-\frac{1}{c} \crH $$ Proof Given $\acR = c(t -t_{r})$ and since $t$ is independent of spatial variables, $$ \nabla \cR =\nabla(-c t_{r})=-c \nabla t_{r} $$ Therefore, the gradient of time delay can be computed by calculating $\nabla \cR$. $$ \begin{align} \nabla \cR &amp;amp;= \nabla \sqrt{\bcR \cdot \bcR} \nonumber \\ &amp;amp;= \frac{1}{2\sqrt{\bcR\cdot \bcR}} \nabla (\bcR \cdot \bcR ) \nonumber \\</description></item><item><title>Liénard-Wiechert Potentials</title><link>https://freshrimpsushi.github.io/en/posts/1269/</link><pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1269/</guid><description>Overview1 The Liénard-Wiechert potentials for a point charge $q$ moving at a speed $\mathbf{v}$ at the retarded time $t_{r}$ are as follows. $$ \begin{align*} V(\mathbf{r}, t) &amp;amp;= \frac{1}{4\pi \epsilon_{0}} \frac{qc}{ (\cR c -\bcR\cdot \mathbf{v})} \\ \mathbf{A}(\mathbf{r}, t) &amp;amp;= \frac{\mu_{0}}{4 \pi}\frac{qc \mathbf{v} }{(\cR c - \bcR\cdot \mathbf{v} )}=\frac{\mathbf{v}}{c^2}V(\mathbf{r}, t) \end{align*} $$ Here, $\bcR=\mathbf{r} -\mathbf{w}(t_{r})$ is the</description></item><item><title>The Zhemengko Equation</title><link>https://freshrimpsushi.github.io/en/posts/1264/</link><pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1264/</guid><description>Overview1 When the distribution of continuous charge changes over time, the electric field is as follows. $$ \mathbf{E} (\mathbf{r},t)=\frac{1}{4\pi \epsilon_{0}} \int \left[ \frac{ \rho (\mathbf{r}^{\prime}, t_{r}) }{\cR ^2} \crH + \frac{ \dot{\rho}(\mathbf{r}^{\prime}, t_{r})}{c\cR}\crH-\frac{\dot{\mathbf{J}}(\mathbf{r}^{\prime},t_{r}) }{c^2 \cR} \right]d\tau^{\prime} $$ When the distribution of continuous current changes over time, the magnetic field is as follows. $$ \mathbf{B}( \mathbf{r}, t) = \dfrac{\mu_{0}}{4\pi} \int \left[ \frac{\mathbf{J}(\mathbf{r}^{\prime},t_{r})}{\cR^2} + \dfrac{ \dot{\mathbf{J}}(\mathbf{r}^{\prime}, t_{r}) } {c\cR} \right]\times \crH d\tau^{\prime}</description></item><item><title>Electric Potential and Electromagnetic Fields</title><link>https://freshrimpsushi.github.io/en/posts/1263/</link><pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1263/</guid><description>Overview1 When the charge and current distribution change over time, the electric field and magnetic field are as follows. $$ \mathbf{E}= -\nabla V-\frac{\partial \mathbf{A}}{\partial t} $$ $$ \mathbf{B} = \nabla \times \mathbf{A} $$ $V$ is the scalar potential, and $\mathbf{A}$ is the vector potential. Description When the charge density $\rho (\mathbf{r}, t)$ and current density $\mathbf{J}(\mathbf{r},t)$ are constant1, knowing the Coulomb&amp;rsquo;s law and the Biot-Savart law allows us to find</description></item><item><title>Physics Appendix</title><link>https://freshrimpsushi.github.io/en/posts/1257/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1257/</guid><description>Fundamentals Appendix A-1 $$ \begin{align*} \frac{ d |x| } {d x} &amp;amp;= \frac{d \sqrt{x^2} }{d x} \\ &amp;amp;= \frac{d \sqrt{x^2}}{d x^2} \frac{d x^2}{dx} \\ &amp;amp;= \frac{1}{2}\frac{1}{\sqrt{x^2}} \cdot 2x \\ &amp;amp;= \dfrac{1}{|x|}x \end{align*} $$ Electromagnetism Appendix E-1 Go to the original article $$ \delta \big( f(x) \big) =\sum \limits_{x_{0}} \frac{\delta (x-x_{0})}{ \frac{\partial f}{\partial x}\Big|_{x=x_{0}} } $$ In this case, $x_{0}$ is a solution to $f(x)$. Using the fact above, $$ \delta</description></item><item><title>Solution of Schrödinger Equation for Finite Square Well Potential</title><link>https://freshrimpsushi.github.io/en/posts/1261/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1261/</guid><description>Overview Let’s examine how a particle moves when the potential takes the shape of a finite square well as shown in the figure above. The potential $U$ is $$ U(x) = \begin{cases} 0 &amp;amp; x&amp;lt;-a \\ U_{0} &amp;amp; -a &amp;lt; x &amp;lt;a \\ 0 &amp;amp;a&amp;lt;x \end{cases} $$ When the potential is $U(x)$, the time-independent Schrödinger equation</description></item><item><title>Solution of the Schrödinger Equation for a Potential Barrier</title><link>https://freshrimpsushi.github.io/en/posts/1256/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1256/</guid><description>Overview Let’s explore how a particle behaves when the potential is in the form of a wall as shown in the figure above. The potential $U$ is $$ U(x) = \begin{cases} 0 &amp;amp; x&amp;lt;-a \\ U_{0} &amp;amp; -a &amp;lt; x &amp;lt;a \\ 0 &amp;amp;a&amp;lt;x \end{cases} $$ The time-independent Schrödinger equation when the potential is $U(x)$ is</description></item><item><title>Conditions for a Function with Extended Real Values to be Measurable</title><link>https://freshrimpsushi.github.io/en/posts/1255/</link><pubDate>Sat, 24 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1255/</guid><description>Theorem1 The necessary and sufficient condition for a function $f : X\rightarrow \overline{\mathbb{R}}$ with values in the extended real numbers to be measurable is as follows, given that σ-algebra $(X,\mathcal{E})$. $$ f\text{ is measurable} \iff \begin{align} &amp;amp; \left\{ x \in X : f(x)=-\infty \right\} \in \mathcal{E} \\ &amp;amp; \left\{ x \in X : \alpha &amp;lt; f(x) &amp;lt; +\infty \right\} \in \mathcal{E}\quad (\forall</description></item><item><title>Extended Real Number System</title><link>https://freshrimpsushi.github.io/en/posts/1252/</link><pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1252/</guid><description>Definition The set defined as follows is called the extended real number system. $$ \overline{ \mathbb{R} } := \mathbb{R} \cup \left\{ -\infty, +\infty\right\} $$ Explanation In fields such as analysis, for convenience, the set $\mathbb{R}$ is often replaced with $\overline{ \mathbb{R} }$. $\pm \infty$ is not a number, but for convenience, it is treated as one and added to $\mathbb{R}$. Within the extended real number system, the rules for comparison</description></item><item><title>Borel Sigma-Algebra, Borel Measurable Space</title><link>https://freshrimpsushi.github.io/en/posts/1251/</link><pubDate>Wed, 21 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1251/</guid><description>Theorem Let $X$ be an arbitrary set. Given a non-empty set $A \subset \mathcal{P}(X)$, there exists the smallest $\sigma$-algebra, $\mathcal{E}_{A}$, that contains $A$. Proof We define $\mathcal{E}_{A}$ and show that it is an $\sigma$-algebra and then prove that it is the smallest1. Let $S$ be the set of all $\sigma$-algebras that contain $A$. $$ S:= \left\{ \mathcal{E} \subset \mathcal{P}(X)\ :\ \mathcal{E}\ \mathrm{is\ } \sigma \mathrm{-algebra, \ } A \subset \mathcal{E}</description></item><item><title>How to Output Multiple Figures on One Page in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1247/</link><pubDate>Sat, 17 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1247/</guid><description>Method The subplot() function can be used to print multiple figures on one page. The first and second parameters respectively indicate the rows and columns of the chessboard on which images will be displayed, deciding the layout of the figures. The third parameter determines the sequence in which the specific figure will be placed. Below is the code and the actual output. X1=Phantom(); X2=radon(X1); X3=fft(X2); X4=iradon(X2,0:179); subplot(2,2,1) imagesc(X1) title(&amp;#34;Phantom&amp;#34;); subplot(2,2,2)</description></item><item><title>Solution of the Schrödinger Equation for a Step Potential</title><link>https://freshrimpsushi.github.io/en/posts/1245/</link><pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1245/</guid><description>Overview Let&amp;rsquo;s examine how a particle moves when the potential is a step function as shown in the figure above. The potential $U$ is $$ U(x) = \begin{cases} 0 &amp;amp; x&amp;lt;0 \\ U_{0} &amp;amp; x&amp;gt;0 \end{cases} $$ The time-independent Schrödinger equation for this potential $U(x)$ is $$ \dfrac{d^2 u(x)}{dx^2}+\frac{2m}{\hbar ^2} \Big[ E-U(x) \Big]u(x)=0 $$ Solution1 $E&amp;lt;0$ No solution exists if</description></item><item><title>Proving that Lp Spaces are Uniformly Convex and Reflexive</title><link>https://freshrimpsushi.github.io/en/posts/1244/</link><pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1244/</guid><description>Theorem1 Let&amp;rsquo;s say $1 \lt p \lt \infty$. Then, the $L^{p}$ space is uniformly convex and reflexive. Explanation It can be proven using the definition of uniform convexity and the Clarkson inequality. Thanks to the Clarkson inequality, the proof ends easily and briefly. It feels like a finishing move. Uniformly convex On the norm space $X$, the norm $\left\| \cdot \right\|$ is said to be uniformly convex if for all</description></item><item><title>Reflection and Transmission of Wave Functions</title><link>https://freshrimpsushi.github.io/en/posts/1241/</link><pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1241/</guid><description>Definition The reflection coefficient $R$ and transmission coefficient $T$ of a wave function are defined as follows. $$ R = \left| \frac{j_{\text{ref}}}{j_{\text{inc}}} \right|,\quad T = \left| \frac{j_{\text{trans}}}{j_{\text{inc}}}\right| \tag{1} $$ Here, $j_{\text{inc}}$ represents the probability flux of the incident wave, $j_{\text{ref}}$ represents the probability flux of the reflection wave, and $j_{\text{trans}}$ represents the probability flux of the transmission wave. Explanation When a particle with energy $E$ encounters a potential barrier higher</description></item><item><title>Probability Flow</title><link>https://freshrimpsushi.github.io/en/posts/1240/</link><pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1240/</guid><description>정의1 2 Wave Function $\psi (x, t)$ of Probability Current is defined as follows. $$ j(x,t) := \frac{\hbar}{2m\i}\left( \psi^{\ast}\dfrac{\partial \psi}{\partial x} - \psi\frac{\partial \psi^{\ast}}{\partial x}\right) \tag{1} $$ Formula The rate of change of the probability current is equal to the time derivative of the Probability Density. That is, the following equation holds. $$ \dfrac{\partial \left| \psi(x, t) \right|^{2}}{\partial t} = - \dfrac{\partial j(x,t)}{\partial x} \tag{2} $$ Explanation In</description></item><item><title>Gram-Schmidt Orthogonalization Process in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/1239/</link><pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1239/</guid><description>Definition The Gram-Schmidt orthogonalization procedure is a method for creating an orthogonal set from vectors that are not orthogonal to each other. Formula Suppose there are two time-independent one-dimensional wave functions $u_{1}$, $u_{2}$. Let’s assume that $u_{1}$ and $u_{2}$ are normalized and not orthogonal to each other. Then, the following wave function $u$ is a normalized wave function orthogonal to $u_{1}$. $$ \begin{align*} u &amp;amp;= \dfrac</description></item><item><title>Natural Embeddings and Reflexive Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1233/</link><pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1233/</guid><description>Definition1 Let us call $\left( X, \left\| \cdot \right\|_{X} \right)$ a normed space. And let us define $X^{\ast \ast}=(X^{\ast})^{\ast}$ as the bidual of $X$. Define the function $J : X \to X^{\ast \ast}$ as follows. $$ J(x)=J_{x},\quad x\in X $$ Here, $J_{x} \in X^{\ast \ast}$ is specifically given as follows. $$ J_{x} : X^{\ast} \to \mathbb{C} \quad \text{and} \quad J_{x}(x^{\ast})=x^{\ast}(x) $$ In this case, $J$ becomes an embedding. Such $J$</description></item><item><title>Hahn-Banach Extension Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1231/</link><pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1231/</guid><description>Theorem1 Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ a normed space. Let $Y \subset X$. Also, given a linear functional $y^{\ast} \in Y^{\ast}$ of $Y$. Then there exists a linear functional $x^{\ast} \in X^{\ast}$ of $X$ that satisfies the following equation. $$ \begin{equation} x^{\ast}(y)=y^{\ast}(y),\quad \forall y \in Y \end{equation} $$ $$ \begin{equation} \| x^{\ast}\|_{X^{\ast}} = \| y^{\ast}\|_{Y^{\ast}} \end{equation} $$ Explanation In simple terms, this means that the dual of a subspace</description></item><item><title>Hahn Banach Theorem for Real, Complex, Seminorm</title><link>https://freshrimpsushi.github.io/en/posts/1230/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1230/</guid><description>The Hahn-Banach Theorem for Real Numbers1 Let $X$ be a $\mathbb{R}$-vector space and assume that $Y \subset X$. Let us define $p : X \to \mathbb{ R}$ as a sublinear linear functional of $X$. Now, assume that $y^{\ast} : Y \to \mathbb{ R}$ satisfies the following condition as a $\mathbb{R}$-linear functional of $Y$. $$ y^{\ast}(y) \le p(y)\quad \forall y\in Y $$ Then, there exists a linear functional $x^{\ast} : X</description></item><item><title>Semi Norm</title><link>https://freshrimpsushi.github.io/en/posts/1229/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1229/</guid><description>Definition1 Let $X$ be a vector space. A function $\left\| \cdot \right\| : X \to \mathbb{R}$ is called a semi norm of $X$ if it satisfies the following three conditions: (a) $\left\| x \right\| \ge 0,\quad \forall\ x \in X$ (b) $|cx|=|c|\left\| x \right\|,\quad \forall\ x\in X,\ \forall\ c \in\mathbb{C}$ (c) $\left\| x + y \right\| \le \left\| x \right\| + \left\| y \right\|,\quad \forall\ x,y\in X$ Explanation The definition</description></item><item><title>Semi-linear Function</title><link>https://freshrimpsushi.github.io/en/posts/1333/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1333/</guid><description>Definition1 If a function $f : X \to Y$ satisfies the following two conditions, it is called sublinear. For $x,x_{1},x_{2}\in X$ and $a \in \mathbb{R}$, $f(ax) = af(x)$ $f(x_{1} + x_{2}) \le f(x_{1}) + f(x_{2})$ Explanation If the second condition holds as an equality, it is linear, and if it holds as an inequality, it is sublinear. Robert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003),</description></item><item><title>The Riesz Representation Theorem for Lp Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1228/</link><pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1228/</guid><description>Theorem[^1] ${L}^{\ p}$ Representation Theorem for Spaces Let&amp;rsquo;s say $1&amp;lt;p&amp;lt;\infty$ and $L\in \big( {L}^{\ p} \big)^{\ast}$. Then, $({L}^{\ p})^{\ast}$ is the dual of the ${L}^{\ p}$ space. Therefore, for every $u\in {L}^{\ p}$, there exists a $v \in {L}^{\ p^{\prime}}$ that satisfies the following equation. $$ L(u)=L_{v}(u)=\int_{\Omega} u(x)v(x)dx $$ Explanation Note that the case where $p=1$ is not included. $\left\| v \right\|_{p^{\prime}} =\left\| L\ ; ({L}^{\ p})^{\ast}\right\|$ satisfies that $f\</description></item><item><title>Proof that All Isometric Mappings are Embeddings</title><link>https://freshrimpsushi.github.io/en/posts/1226/</link><pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1226/</guid><description>Theorem Let&amp;rsquo;s call $(X, \left\| \cdot \right\|_{X}), (Y, \left\| \cdot \right\|_{Y})$ a norm space. And let&amp;rsquo;s say $f : X \to Y$ is an isometry. Then, $f$ is an embedding. In other words, $f$ satisfies the following two conditions. (a) $f(X) \subset Y$ (b) $f : X \to f(X)$ is a homeomorphism. Proof Strategy: Prove $(b)$ first, then prove $(a)$. Although there isn&amp;rsquo;t anything particularly difficult in each proof process,</description></item><item><title>What is a Norm Space?</title><link>https://freshrimpsushi.github.io/en/posts/1225/</link><pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1225/</guid><description>Definition1 Let&amp;rsquo;s call $X$ a vector space. If there exists a function $\left\| \cdot \right\| : X \to \mathbb{R}$ that satisfies the following three conditions, then $\left\| \cdot \right\|$ is called the norm of $X$, and $(X,\left\| \cdot \right\| )$ is called a normed space. (a) $\left\| x \right\| \ge 0,\quad \forall\ x \in X$ and $\left\| x \right\|=0 \iff x = 0$ (b) $\|cx\|=|c|\left\| x \right\|,\quad \forall\ x\in X,\</description></item><item><title>Sobolev Spaces are Separable, Uniformly Convex, and Reflexive: A Proof</title><link>https://freshrimpsushi.github.io/en/posts/1222/</link><pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1222/</guid><description>Theorem1 When $1\le p &amp;lt;\infty$, the Sobolev space $W^{m, p}$ is separable. Moreover, when $1&amp;lt; p &amp;lt; \infty$, the Sobolev space is reflexive and uniformly convex. Description A vector space in which an inner product is defined is called an inner product space, and a complete inner product space is specially called a Hilbert space. Since $W^{m, p}$ is complete, if the inner product is defined as below, $W^{m,\ 2}$</description></item><item><title>Sobolev Spaces are Banach Spaces: A Proof</title><link>https://freshrimpsushi.github.io/en/posts/1221/</link><pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1221/</guid><description>Theorem1 The Sobolev space $W^{m, p}$ is a Banach space. Description A Banach space is defined as a space where a norm is defined and is complete. As the norm is also defined when defining the Sobolev space, we only need to verify that it is complete. Therefore, it suffices to show that the Cauchy sequence within $W^{m, p}$ converges within $W^{m, p}$. The proof is relatively straightforward. Proof Let</description></item><item><title>Sobolev Norm and Sobolev Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1220/</link><pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1220/</guid><description>Definition1 Sobolev Space Let $\Omega \subset \mathbb{R}^{n}$ be an open set. For positive integers $m$ and $1\le p \le \infty$, the function space defined as follows is called the Sobolev space. $$ W^{m, p}(\Omega):=\left\{ u \in L^{p}(\Omega) : D^\alpha u \in L^{p}(\Omega)\quad \forall 0\le |\alpha | \le m \right\} $$ Here, $\alpha$ is a multi-index, $D^\alpha u$ is a weak derivative, and $L^{p}$ is a Lebesgue space. Sobolev Norm For</description></item><item><title>Embeddings in Mathematics, Insertion Mappings</title><link>https://freshrimpsushi.github.io/en/posts/1214/</link><pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1214/</guid><description>imbedding and embedding mean the same thing. Embedding is translated as insertion, embedding, incorporating, burying, etc. Definition1 Let $(X, \left\| \cdot \right\|_{X}), (Y, \left\| \cdot \right\|_{Y})$ be a normed space. If the following two conditions are satisfied for $X$ and $Y$, then $X$ is said to be embedded into $Y$, and $I : X \to Y$ is called the embedding. $X$ is a subspace of $Y$. For all $x \in</description></item><item><title>Minkowski's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1212/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1212/</guid><description>Theorem1 Let&amp;rsquo;s denote $\Omega \subset \mathbb{R}^{n}$ as an open set, and $0 \lt p \lt 1$. If $u, v \in L^p(\Omega)$ then $u+v \in L^p(\Omega)$. Explanation This is called the reverse Minkowski&amp;rsquo;s inequality. It&amp;rsquo;s not the converse of the Minkowski&amp;rsquo;s inequality proposition, but the direction of the inequality is reversed. The Minkowski inequality shows that when $1 \le p \lt \infty$, the defined $\left\| \cdot \right\|_{p}$ satisfies the triangle inequality</description></item><item><title>Holder's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1203/</link><pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1203/</guid><description>Theorem1 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. Assume $0 &amp;lt; p &amp;lt; 1$ and $p^{\prime} = \dfrac{p}{p-1} &amp;lt; 0$. If $u \in$ $L^{p}(\Omega)$, $uv\in$ $L^{1}(\Omega)$, then $$ \begin{equation} 0 \lt \int_{\Omega} |v(x)|^{p^{\prime}}dx \lt \infty \end{equation} $$ the following inequality is established: $$ \int_{\Omega} |u(x)v(x)|dx \ge \left( \int_{\Omega} |u(x)|^{p} dx \right)^{1/p} \left( \int_{\Omega} |v(x)|^{p^{\prime}} dx \right) ^{1/p^{\prime}} $$ Explanation This is called the reverse Höeld</description></item><item><title>Proof of Clarkson's Inequalities</title><link>https://freshrimpsushi.github.io/en/posts/1202/</link><pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1202/</guid><description>Theorem1 Let&amp;rsquo;s say that $\Omega \subset \mathbb{R}^{n}$ is an open set. Let&amp;rsquo;s say that $u,v\in {L}^{p}(\Omega)$. Also, let&amp;rsquo;s say it satisfies $\frac{1}{p}+\frac{1}{p^{\prime}}=1$. If $2 \le p \lt \infty$, then the following two inequalities hold. $$ \begin{equation} \left\| \frac{u+v}{2}\right\|_{p}^{p}+ \left\| \frac{u-v}{2} \right\|_{p}^{p} \le \frac{1}{2}\left\| u \right\|_{p}^{p} + \frac{1}{2}\left\| v \right\|_{p}^{p} \end{equation} $$ $$ \begin{equation} \left\| \frac{u+v}{2}\right\|_{p}^{p^{\prime}}+ \left\| \frac{u-v}{2} \right\|_{p}^{p^{\prime}} \ge \left( \frac{1}{2}\left\| u \right\|_{p}^{p} + \frac{1}{2}\left\| v \right\|_{p}^{p}\right)^{p^{\prime}-1} \end{equation} $$ If</description></item><item><title>Interpolation Inequalities in Lebesgue Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1201/</link><pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1201/</guid><description>Theorem1 Let&amp;rsquo;s say $\Omega \subset \mathbb{R}^{n}$ is an open set. Suppose that for some $\theta$, $1 \le p \lt q\lt r \le \infty$ satisfies the following equation being $0 \lt \theta \lt 1$. $$ \dfrac{1}{q} = \frac{\theta}{p} + \frac{1-\theta}{r} $$ Assume that $u \in L^p(\Omega) \cap L^r(\Omega)$. Then, $u\in L^{q}(\Omega)$ holds, and the following inequality is established. $$ \left\| u \right\|_{q} \le \left\| u \right\|_{p}^{\theta} \left\| u \right\|_{r}^{1-\theta} $$ This</description></item><item><title>Linear Functionals on Lp Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1196/</link><pubDate>Mon, 22 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1196/</guid><description>Definition1 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. Suppose that $1 \le p \le \infty$ and $p^{\prime}=\frac{p}{p-1}$. For each $v \in L^{p^{\prime}}(\Omega)$, define the linear functional $L_{v}\ :\ L^p(\Omega) \rightarrow \mathbb{C}$ on the space $L^p(\Omega)$ as follows. $$ L_{v}(u) = \int_{\Omega} u(x)v(x)dx, \quad u\in L^p(\Omega) $$ Theorem Let the norm on the space $L^p$ be denoted by $\| \cdot \|_{p}$. Then, by the Hölder</description></item><item><title>Uniform Convexity</title><link>https://freshrimpsushi.github.io/en/posts/1199/</link><pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1199/</guid><description>Definitions1 Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ a normed space. We say that the norm $\left\| \cdot \right\|$ of $X$ is uniformly convex if it satisfies the following condition: For every $\epsilon$ where $0 \lt \epsilon \le 2$, there exists a positive number $\delta (\epsilon) \gt 0$ such that if $x,y \in X$ and $\| x \| = \|y\| = 1$, $\| x-y\| \ge \epsilon$ then it satisfies $\|( x+y)/2</description></item><item><title>Derivation of Bessel's Equation</title><link>https://freshrimpsushi.github.io/en/posts/1195/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1195/</guid><description>Definition The differential equation below is called the $\nu$th order Bessel&amp;rsquo;s equation. $$ \begin{align*} x^2 y^{\prime \prime} +xy^{\prime} +(x^2-\nu^2)y =&amp;amp;\ 0 \\ x(xy^{\prime})^{\prime} + (x^2- \nu ^2) y =&amp;amp;\ 0 \\ y^{\prime \prime}+\frac{1}{x} y^{\prime} + \left( 1-\frac{\nu^{2}}{x^{2}} \right)y =&amp;amp;\ 0 \end{align*} $$ Description The solution to the Bessel&amp;rsquo;s equation is called the Bessel function. Bessel functions are often seen in physics, engineering, and more, especially in problems involving cylindrical symmetry.</description></item><item><title>List of Special Symbols Available for Graphs in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1191/</link><pubDate>Wed, 17 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1191/</guid><description>Method When labeling graphs in MATLAB to indicate what each axis represents, you can use xlabel and ylabel. It&amp;rsquo;s also possible to use special symbols, bold, and italic styles. x=-3*pi:0.2:3* pi; y=sin(x-pi/6); plot(x,y); xlabel(&amp;#39;\beta&amp;#39;), ylabel(&amp;#39;\nabla f(x)&amp;#39;),; x=-3*pi:0.2:3* pi; y=sin(x-pi/6); plot(x,y); xlabel(&amp;#39;진폭{\bf Volt}&amp;#39;), ylabel(&amp;#39</description></item><item><title>Euler-Lagrange Equation in Physics</title><link>https://freshrimpsushi.github.io/en/posts/1183/</link><pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1183/</guid><description>Overview This article assumes that the reader has already gone through the category of classical mechanics, specifically Lagrangian Mechanics and Hamilton&amp;rsquo;s Principle of Least Action. While this article will attempt to re-explain notations and content, even if they have been covered before, it is recommended to refer to the linked document for any unexplained notation. The integral of the Lagrangian over a path of motion is called the action and</description></item><item><title>Lagrangian Mechanics and Hamiltons Variational Principle</title><link>https://freshrimpsushi.github.io/en/posts/1182/</link><pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1182/</guid><description>Overview Hamilton&amp;rsquo;s principle, functionals, action, and variation are explained here in a way that is as simple as possible. If you have not found a satisfactory explanation elsewhere, it is recommended to read through to the end. This has been written so that even freshmen and sophomores in college can understand it. Lagrangian Mechanics1 When an object moves from time $t_{1}$ to $t_{2}$, the integral of the Lagrangian over the</description></item><item><title>Proof that the Hopf-Lax Formula Satisfies the Hamilton-Jacobi Equation</title><link>https://freshrimpsushi.github.io/en/posts/1178/</link><pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1178/</guid><description>Theorem 1 Hopf-Lax Formula $$ u(x,t) = \min \limits_{y \in \mathbb{R}^n} \left\{ tL\left( \dfrac{x-y}{t} \right) +g(y) \right\} $$ Let&amp;rsquo;s denote $x \in \mathbb{R}^n$ and $t&amp;gt;0$. And suppose that $u$ defined by the Hopf-Lax formula is differentiable at point $(x,t)$. Then, $u$ satisfies the Hamilton-Jacobi Equation. $$ u_{t}(x, t) + H\big( Du(x, t) \big) =0 $$ Proof Lemma: Generalization of Hopf-Lax Formula Let&amp;rsquo;s denote $t&amp;gt;0$. Then, for any given $x \in</description></item><item><title>Fundamental Theorem of Algebra</title><link>https://freshrimpsushi.github.io/en/posts/1179/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1179/</guid><description>Theorem1 Suppose that $p, q, r \ge 1$ satisfies $\dfrac{1}{p} + \dfrac{1}{q} + \dfrac{1}{r} = 2$. Then, for all ${u \in L^{p}(\mathbb{R}^{n})}$, ${v \in L^{q}(\mathbb{R}^{n})}$, ${w \in L^{r}(\mathbb{R}^{n})}$, the following equation holds. $$ \begin{equation} \left| \int_{\mathbb{R}^{n}} (u \ast v)(x)w(x)dx \right| \le \left\| u \right\|_{p} \left\| v \right\|_{q} \left\| w \right\|_{r} \end{equation} $$ Here, $u \ast v$ is the convolution of $u$ and $v$. Description This is called Young&amp;rsquo;s theorem. The</description></item><item><title>Hopf-Lax Formula</title><link>https://freshrimpsushi.github.io/en/posts/1174/</link><pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1174/</guid><description>Buildup1 Let&amp;rsquo;s consider the initial value problem of the Hamilton-Jacobi equation that depends only on $H$ as $Du$ for the Hamilton-Jacobi equation. $$ \begin{equation} \left\{ \begin{aligned} u_{t} + H(Du)&amp;amp;=0 &amp;amp;&amp;amp; \text{in } \mathbb{R}^n \times (0,\infty) \\ u&amp;amp;=g &amp;amp;&amp;amp; \text{on } \mathbb{R}^n \times \left\{ t=0 \right\} \end{aligned} \right. \end{equation} $$ Generally, the Hamiltonian depends on the spatial variables as in the form of $H(Du, x)$, but let&amp;rsquo;s say here it is</description></item><item><title>Mellin Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1176/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1176/</guid><description>Definition For a given function $f : [0, \infty) \to \mathbb{C}$, if the following integral exists, it is called the Mellin transform of $f$, and such integral transformations are denoted by $\mathcal{M}$. $$ \mathcal{M}f (s) = \int_{0}^{\infty} x^{s-1}f(x)dx = \phi (s),\quad s \in \mathbb{C} $$ The inverse Mellin transform is as follows. $$ \mathcal{M}^{-1}\phi (x) = \dfrac{1}{2\pi i }\int_{c-i\infty}^{c+i\infty} x^{-s}\phi (s) ds $$ Explanation It is a kind of integral</description></item><item><title>Hamiltonian and Lagrangian Convex Duality</title><link>https://freshrimpsushi.github.io/en/posts/1172/</link><pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1172/</guid><description>Theorem1 Legendre Transform $L$ is a convex function. $\lim \limits_{ |v|\to \infty} \dfrac{ L(v) }{ |v| }=+\infty$ Given the conditions, the Legendre transform $L^{\ast} : \mathbb{R}^{n} \to \mathbb{R}$ of $L$ for the Lagrangian $L : \mathbb{R}^{n} \to \mathbb{R}$ is defined as follows: $$ L^{\ast} (p) := \sup \limits_{v \in \mathbb{R}^{n}} \big( p\cdot v -L(v) \big) \quad \forall \ p \in \mathbb{R}^{n} $$ Let&amp;rsquo;s assume the Lagrangian $L$ satisfies the conditions</description></item><item><title>Legendre Transformation</title><link>https://freshrimpsushi.github.io/en/posts/1171/</link><pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1171/</guid><description>Regarding x and p, when emphasizing that they are variables of a partial differential equation, it is indicated with regular font $x,p \in \mathbb{R}^{n}$, and when emphasizing that it is a function of $s$, it is indicated with bold font $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Definition 1 For simplicity, let&amp;rsquo;s say the Lagrangian is a function of variable $v\in \mathbb{R}^{n}$ only. $$ L(v) = L : \mathbb{R}^{n} \to \mathbb{R} $$ Let&amp;rsquo;s</description></item><item><title>Laplace Transform Convolution</title><link>https://freshrimpsushi.github.io/en/posts/1170/</link><pubDate>Thu, 04 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1170/</guid><description>Definitions1 Let&amp;rsquo;s call $\mathcal{L}$ the Laplace transform. The function $f*g$ that satisfies the following expression is called the convolution with respect to the Laplace transforms of $f$ and $g$. $$ \mathcal{L}(f*g) = \mathcal{L}(f) \cdot \mathcal{L}(g) $$ Theorem The convolution of $f$ and $g$ with respect to their Laplace transforms $h=f*g$ is as follows. $$ h(t) = f*g(t) = \int_{0}^t f(t-\tau)g(\tau)d\tau = \int_{0}^t f(\tau) g(t-\tau)d\tau $$ Proof $$ \begin{align*} \mathcal{L} \left\{</description></item><item><title>Hamiltonian Equations Derived from Variational Calculus and Euler-Lagrange Equation</title><link>https://freshrimpsushi.github.io/en/posts/1168/</link><pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1168/</guid><description>For variables x and p, when emphasizing that they are variables in partial differential equations, they are denoted in regular font $x,p \in \mathbb{R}^{n}$, and when emphasizing as a function of $s$, they are denoted in bold $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Similarly, v is denoted in regular font $v \in \mathbb{R}^{n}$ to emphasize it as a variable, and in bold $\mathbf{v} \in \mathbb{R}^{n}$ to emphasize it as a function. There</description></item><item><title>Hilbert Transform</title><link>https://freshrimpsushi.github.io/en/posts/1167/</link><pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1167/</guid><description>Buildup Radon Inversion $$ f(x,y)=\dfrac{1}{2} \mathcal{B} \left\{ \mathcal{F}^{-1} \Big[ |S|\mathcal{F} (\mathcal{R}f) (S,\ \theta) \Big]&amp;gt; \right\} (x,y) $$ This formula is for obtaining $f$ from Radon transform $\mathcal{R}f$ of $f$. First, recall the following property of the Fourier transform. $$ \mathcal{F} [f^{\prime} ] (\xi) = i\xi \mathcal{F}(\xi) $$ Here, if we substitute $\mathcal{R}f$ for $f$, we get the following. $$ \begin{equation} \mathcal{F} \left( \dfrac{\partial (\mathcal{R}f)(t,\ \theta) } {\partial t} \right) (S,\</description></item><item><title>Radon Inverse Transform: Filtered Back Projection (FBP)</title><link>https://freshrimpsushi.github.io/en/posts/1166/</link><pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1166/</guid><description>Theorem There is a formula that holds for $f : \mathbb{R}^{2} \to \mathbb{R}$. Description Also known as The filtered back projection formula. Given the Radon transform $\mathcal{R}f$ of $f$, it is said that $f$ can be obtained using the Fourier transform and back projection. This means, applying a Fourier transform to the Radon transform, multiplying by $|S|$, then applying the inverse Fourier transform, and finally, back projection, is known as</description></item><item><title>Back Projection: The Dual of the Radon Transform</title><link>https://freshrimpsushi.github.io/en/posts/1164/</link><pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1164/</guid><description>Definition1 2 The dual operator $\mathcal{R}^{\#} : L^{2}(Z_{n}) \to L^{2}(\mathbb{R}^{n})$ of the Radon transform $\mathcal{R} : L^{2}(\mathbb{R}^{n}) \to L^{2}(Z_{n})$ is referred to as back projection. $$ \left\langle \mathcal{R}f ,g \right\rangle_{L^{2}(Z_{n})} = \left\langle f , \mathcal{R}^{\#}g \right\rangle_{L^{2}(\mathbb{R}^{n})} $$ Here, $Z_{n} := \mathbb{R}^{1} \times S^{n-1}$ is the unit cylinder of $\mathbb{R}^{n+1}$. Theorem Formula Specifically, back projection is as follows. $$ \mathcal{R}^{\#} g (\mathbf{x}) = \int_{S^{n-1}} g (\mathbf{x} \cdot \boldsymbol{\theta}, \boldsymbol{\theta}) d\boldsymbol{\theta} $$</description></item><item><title>Fourier Slice Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1165/</link><pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1165/</guid><description>Theorem For $f : \mathbb{R}^{2} \to \mathbb{R}$, the following equation holds: $$ \begin{equation} \mathcal{F}_2 f(\xi \cos\theta,\ \xi \sin\theta)=\mathcal{F}(\mathcal{R}f)(\xi ,\ \theta) \label{thm1} \end{equation} $$ Here, $\mathcal{F}$ represents the 1-dimensional Fourier transform, $\mathcal{F}_2$ represents the 2-dimensional Fourier transform, and $\mathcal{R}$ is the Radon transform. $$ \begin{align*} \mathcal{F}f (y) &amp;amp;= \int f(x) e^{-i xy } dx \\ \mathcal{F}_{2} f (y_{1}, y_{2}) &amp;amp;= \int \int f(x_{1}, x_{2}) e^{-i (x_{1}, x_{2}) \cdot (y_{1}, y_{2})} dx_{1}</description></item><item><title>Hamilton-Jacobi Equation and Hamiltonian Equation</title><link>https://freshrimpsushi.github.io/en/posts/1162/</link><pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1162/</guid><description>There are two ways to derive the Hamilton equations. One is from the Euler-Lagrangian equations, and the other, which will be introduced in this article, is from the characteristic equations of the Hamilton-Jacobi equation. Definition1 The following partial differential equation is called the general Hamilton-Jacobi equation. $$ G(Du, u_{t}, u, x, t)=u_{t}+H(Du, x)=0 $$ $t &amp;gt;0 \in \mathbb{R}$ $x \in \mathbb{R}^{n}$ $u : \mathbb{R}^{n} \to \mathbb{R}$ Here, the differential operator</description></item><item><title>Importing Excel Data into MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1163/</link><pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1163/</guid><description>Method Matlab provides the functionality to import data from Excel files. First, click on &amp;lsquo;Import Data&amp;rsquo; from the Home menu. Select the Excel file that contains the data you want to import. Then, you can select which data to import, which is automatically selected initially. Confirm and click &amp;lsquo;Import Selected&amp;rsquo;. From &amp;lsquo;Import Selected&amp;rsquo;, click on &amp;lsquo;Import Data&amp;rsquo;. Then, the data from the Excel file is input into a variable with</description></item><item><title>Lagrangians and Euler-Lagrange Equations in Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1157/</link><pubDate>Thu, 27 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1157/</guid><description>Definition1 Lagrangian Let&amp;rsquo;s assume a smooth function $L : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ is given. This is called the Lagrangian and is denoted as follows. $$ L = L(v,x)=L(v_{1}, \dots, v_{n}, x_{1}, \dots, x_{n}) \quad v,x\in \mathbb{R}^{n} \\ D_{v}L = (L_{v_{1}}, \dots, L_{v_{n}}), \quad D_{x}L = (L_{x_{1}}, \dots, L_{x_{n}}) $$ The reason for using variables $v, x$ is because, in physics, each variable actually signifies velocity and position. Action,</description></item><item><title>One-Dimensional D'Alembert's Formula</title><link>https://freshrimpsushi.github.io/en/posts/1151/</link><pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1151/</guid><description>Review1 Suppose the Cauchy problem for the wave equation is given as follows. $$ \begin{align*} u_{tt}-u_{xx}&amp;amp;= 0 &amp;amp;&amp;amp; \text{in } \mathbb{R}^2=\mathbb{R}_{x} \times \mathbb{R}_{t} \\ u=g,\quad u_{t}=&amp;amp;\h &amp;amp;&amp;amp; \text{on } \mathbb{R}\times\left\{t=0\right\} \end{align*} $$ Then $g \in C^2(\mathbb{R}), h\in C^1(\mathbb{R})$. Let&amp;rsquo;s define $u(x,t)$ as follows. $$ \begin{equation} u(x,t)=\dfrac{1}{2} \left[ g(x+t)+g(x-t) \right] + \dfrac{1}{2} \int_{x-t}^{x+t}h(y)dy \quad \forall\ (x,t)\in \mathbb{R}^2 \end{equation} $$ Then, $u\in C^2(\mathbb{R}^2)$ is the solution to the given Cauchy problem. Explanation</description></item><item><title>How to Save Data Calculated in MATLAB to an Excel File</title><link>https://freshrimpsushi.github.io/en/posts/1150/</link><pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1150/</guid><description>Method When you want to organize the data calculated in MATLAB into Excel and the amount of data is not too much, you can manually copy and paste. However, for a matrix of data like 128*128 shown in the picture above, that method is not feasible. In this case, you can use xlswrite to save the data into an Excel file. Compared to the picture above, xlswrite('test', Y) has been</description></item><item><title>How to Comment and Uncomment Multiple Lines at Once in MATLAB</title><link>https://freshrimpsushi.github.io/en/posts/1149/</link><pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1149/</guid><description>Method To comment out a section that you want, drag to select the area and then press Ctrl+R. This will comment out the entire selected portion. To undo, drag to select the same area and press Ctrl+T, which will remove the % from each line.</description></item><item><title>Methods of Expressing an Arbitrary Function as Two Non-negative Functions</title><link>https://freshrimpsushi.github.io/en/posts/1145/</link><pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1145/</guid><description>Definitions1 Let&amp;rsquo;s define $f^{+}$ and $f^{-}$ for a function $f : X \to \mathbb{R}$ as follows. $$ \begin{align*} f^{+} (x) &amp;amp;:= \max \left\{ f(x),\ 0 \right\} \\ f^{-} (x) &amp;amp;:= \max \left\{ -f(x),\ 0 \right\} \end{align*} $$ We call $f^{+}$ the positive part of $f$, and $f^{-}$ the negative part of $f$. Description Despite their names, both $f^{+}$ and $f^{-}$ are non-negative functions. It might not be immediately clear why</description></item><item><title>Properties of Measurable Functions with Real Values</title><link>https://freshrimpsushi.github.io/en/posts/1136/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1136/</guid><description>Theorem1 If two functions defined in a measurable space $(X,\mathcal{E})$ are measurable functions, then the following functions are also measurable: $$ cf,\quad f^2,\quad f+g,\quad fg,\quad |f| $$ Proof Measurable Function A function $f : X \to \overline{\mathbb{R}}$ is called a measurable function if it satisfies the following equation for all $\alpha \in \mathbb{R}$: $$ S_{f}(\alpha):=\left\{ x\in X\ |\ f(x) &amp;gt;\alpha \right\} \in \mathcal{E},\quad \forall \alpha \in \mathbb{R} $$ $cf$ Case</description></item><item><title>Predictable Functions</title><link>https://freshrimpsushi.github.io/en/posts/1135/</link><pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1135/</guid><description>Definition1 Let’s call $(X, \mathcal{E})$ a measurable space. Let&amp;rsquo;s define the set $S_{f}(\alpha)$ as follows. $$ S_{f}(\alpha):=\left\{ x\in X\ |\ f(x) &amp;gt;\alpha \right\} = f^{-1}\left( (\alpha, \infty) \right),\quad \forall \alpha \in \mathbb{R} $$ If for every real number $\alpha \in \mathbb{R}$, $S_{f}(\alpha) \in \mathcal{E}$ holds, then the function $f : X \to \overline{\mathbb{R}}$ taking extended real values is called $\mathcal{E}$-measurable or simply measurable. Explanation Especially, if</description></item><item><title>Discrete Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/1121/</link><pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1121/</guid><description>Buildup1 Let&amp;rsquo;s assume that $f$ is some physical signal measured at specific intervals. Considering realistic conditions, there will be a moment when the measurement of the signal starts, $t=0$, and a moment when the measurement ends, $t=\Omega$. Thus, it can be assumed that the function value of $f$ is $0$ everywhere except for $[0, \Omega]$. Sampling Theorem Let&amp;rsquo;s assume that $\hat{f} \in L^{2}$ and $f (t) = 0\ \text{for }</description></item><item><title>Solving Differential Equations Using Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/1119/</link><pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1119/</guid><description>Description Fourier series and Fourier transforms emerged to solve heat equations. Of course, they can be used to solve other differential equations too, provided the conditions are met. Fourier series, in particular, are used in quantum physics to calculate the energy of particles through the Schrödinger equation. Many physics students use it without knowing it&amp;rsquo;s a Fourier series, but they</description></item><item><title>Properties of Radon Transform</title><link>https://freshrimpsushi.github.io/en/posts/1118/</link><pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1118/</guid><description>Properties1 The Radon transform $\mathcal{R} : L^{2}(\mathbb{R}^{n}) \to L^{2}(\Lambda)$ has the following properties. Linearity For $\alpha, \beta \in \mathbb{R}$ and $f, g \in L^{2}(\mathbb{R}^{2})$, the following holds. $$ \mathcal{R} \left( \alpha f + \beta g \right) = \alpha \mathcal{R}f + \beta \mathcal{R}g $$ Shift Invariance Let $T_{\mathbf{a}}$ be a translation for $\mathbf{a} \in \mathbb{R}^{n}$. $$ T_{\mathbf{a}}f(\mathbf{x}) := f(\mathbf{x}-\mathbf{a}) \text{ for } f\in L^{2}(\mathbb{R}^{n})\quad \text{and} \quad T_{t}g(s,\boldsymbol{\theta}) := g(s-t, \boldsymbol{\theta}) \text{</description></item><item><title>The Relationship between L1 Space and L2 Space</title><link>https://freshrimpsushi.github.io/en/posts/1114/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1114/</guid><description>Definition $L^{1}$ Space A function $f$ is said to be (absolutely) integrable on the interval $[a,\ b]$ if it satisfies the following equation. $$ \int_{a}^b |f(x)| dx &amp;lt; \infty $$ The set of functions that are integrable on the interval $[a,b]$ is denoted as $L^{1}(a,b)$. $$ L^{1}(a,b)= \left\{ f : \int_{-a}^{b} |f(x)| dx &amp;lt; \infty \right\} $$ $L^{2}$ Space A function satisfying the following equation is said to be square-integrable.</description></item><item><title>Fourier Inversion Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1112/</link><pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1112/</guid><description>Buildup The process of deriving the Fourier transform also derived the definition of the inverse transform. However, this was simply explained to aid understanding, and the transformation formula was not accurately derived. The Fourier inverse transformation is as follows: $$ \begin{equation} f(x) =\dfrac{1}{2\pi} \int \hat{f}(\xi) e^{i\xi x}d\xi \end{equation} $$ This equation implies that from $f$, we can obtain $\hat{f}$ and from $\hat{f}$, we can retrieve $f$ again. This might seem</description></item><item><title>Convergence of Mollification</title><link>https://freshrimpsushi.github.io/en/posts/1109/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1109/</guid><description>Theorem Let&amp;rsquo;s assume the following holds for a Mollifier $\eta_{\epsilon}$. $\displaystyle \alpha= \int_{-\infty}^{0} \eta_{\epsilon}(x) dx$ $\displaystyle \beta=\int_{0}^{\infty} \eta_{\epsilon} (x) dx$ Let&amp;rsquo;s say $\alpha + \beta = 1$. And say $f$ is piecewise continuous and bounded. Then, the mollification of $f$ converges as follows. $$ \lim \limits_{\epsilon \rightarrow 0} f \ast \eta_{\epsilon}(x) = \alpha f(x+) + \beta f(x-) $$ If $\eta_{\epsilon}(x)$ is an even function, $$ \lim \limits_{\epsilon \rightarrow 0} f</description></item><item><title>Exponential Function Set and Trigonometric Function Set are Orthogonal Bases</title><link>https://freshrimpsushi.github.io/en/posts/1108/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1108/</guid><description>Theorem Both sets $\left\{ e^{inx} \right\}_{n=-\infty}^\infty$ and $\left\{ \cos nx\ \right\}_{n=0}^\infty \cup \left\{ \sin nx \right\}_{n=1}^\infty$ are the orthonormal bases of $L^{2}(-\pi,\ \pi)$. Furthermore, $\left\{ \cos nx \right\}_{n=0}^{\infty}$ and $\left\{ \sin nx \right\}_{n=1}^{\infty}$ are the orthonormal bases of $L^{2}(0,\ \pi)$. Explanation This fact explains the reason why it is valid to express a given function as a series of trigonometric functions in Fourier series. Proof Let $\phi_{n}(x)=e^{inx}$. And assume $f</description></item><item><title>Lines Determined by Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/1110/</link><pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1110/</guid><description>Description Lines Determined by Polar Coordinates A line, as shown in Figure (1), is determined by the slope $a$ and the $y$ intercept $b$. It seems that all lines on a plane can be represented only by their slope and intercept, but this is not the case. Precisely, only lines can be depicted as functions. Therefore, a line perpendicular to the $x$ axis, as shown in Figure (2), cannot be</description></item><item><title>Fourier Transform of Characteristic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1105/</link><pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1105/</guid><description>Formulas The Fourier transform of the characteristic function is as follows: $$ \mathcal{F} \left[ \chi_{[-a,a]}(x) \right] = \dfrac{2 \sin(a\xi) }{\xi} $$ Proof $$ \begin{align*} \mathcal{F} \left[ \chi_{[-a,a]}(x) \right] &amp;amp;= \int_{-\infty}^{\infty} \chi_{[-a,a]}(x)e^{-i \xi x } dx \\ &amp;amp;= \int_{-a}^{a}e^{-i \xi x} dx \\ &amp;amp;= \dfrac{1}{-i\xi} \left. e^{-i\xi x}\right]_{-a}^{a} \\ &amp;amp;= \dfrac{1}{-i\xi} \left( e^{-i a \xi} - e^{i a \xi} \right) \\ &amp;amp;= \dfrac{2}{\xi} \dfrac{e^{ia\xi} -e^{-ia\xi}}{2i} \\ &amp;amp;= \dfrac{2}{\xi} \sin (a\xi) \end{align*}</description></item><item><title>Riemann-Lebesgue Lemma</title><link>https://freshrimpsushi.github.io/en/posts/1106/</link><pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1106/</guid><description>Theorem1 Let&amp;rsquo;s assume $f \in$ $L^{1}$ is given. Then, the following equation holds. $$ \lim \limits_{n \to \pm \infty} \hat{f}(\xi) = 0 $$ Here, $\hat{f}$ is the Fourier transform of $f$. Proof step 1 Proofs for the step function $f$, and generalization in step 2. Note that $f$ in step 1 and step 2 are not the same. case 1 Assume $f$ is the following step function. $$ f(x) =</description></item><item><title>Fourier Transform of Gaussian Functions</title><link>https://freshrimpsushi.github.io/en/posts/1104/</link><pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1104/</guid><description>Formulas The Fourier transform of a Gauss function $f(x)=e^{-Ax^2}$ is given as follows. $$ \mathcal{F}[f] (\xi) = \mathcal{F} \left[ e^{-Ax^2} \right] (\xi)=\sqrt{\frac{\pi}{A}}e^{-\frac{\xi ^2}{4A}} $$</description></item><item><title>Properties of Fourier Transform</title><link>https://freshrimpsushi.github.io/en/posts/1101/</link><pubDate>Thu, 23 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1101/</guid><description>Theorem[^1] Let&amp;rsquo;s consider $\cal{F}f, \hat{f}$ as the Fourier transform of $f$. Let $f \in L^{1}$. Then, the following properties hold for the Fourier transform: (a) For any real number $a$, $$ \mathcal{F} \left[ f(x-a) \right] ( \xi ) = e^{-ia\xi}\hat{f}(\xi) \quad \mathrm{and} \quad \mathcal{F} \left[ e^{iax}f(x)\right] (\xi) = \hat{f}(\xi-a) $$ (b) Define $f_\delta (x) := \frac{1}{\delta}f ( \frac{x}{\delta} )$ for $\delta &amp;gt;0$. Then, $$ \mathcal{F}\left[ f_\delta \right] (\xi ) =</description></item><item><title>Additive and Multiplicative Functions</title><link>https://freshrimpsushi.github.io/en/posts/1096/</link><pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1096/</guid><description>Given a function $f : X \to Y$, let $a, b \in X$, $a_{i} \in X\ (i=1,\cdots)$. Subadditive Function A function $f$ is called a subadditive function when it satisfies the following equation: $$ f(a+b) \le f(a)+f(b) $$ The absolute value is an example. $$ |3+(-4)| \le |3|+|-4| $$ Another example, if we have $f(x)=2x+3$ then $$ 13=f(2+3) \le f(2)+f(3)=7+9=16 $$ Additive Function A function $f$ is called an additive</description></item><item><title>Proof of the Karatheodory's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/1095/</link><pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1095/</guid><description>Definition1 For all $A \subset X$, if the following equation holds, then $E \subset X$ is said to satisfy the Caratheodory condition, or $E$ is said to be $\mu^{\ast}$-measurable. $$ \begin{equation} \mu^{\ast}(A) = \mu^{\ast}(A\cap E) + \mu^{\ast}(A \cap E^{c}) \label{def1} \end{equation} $$ $\mu^{\ast}$ is an outer measure. Theorem Let $L$ be the set that includes all $E \subset X$ that satisfy the Caratheodory condition. Then, $L$ is a $\sigma$-algebra. Also,</description></item><item><title>Linearization of Boundaries in Nonlinear First-Order Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1092/</link><pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1092/</guid><description>Buildup One method to easily solve the characteristic equation of a nonlinear first-order differential equation is to straighten out a small portion, $\Gamma$, of the boundary, $\partial \Omega$, of the domain, $\Omega$. Since this is always possible, in the vicinity of a point $x^{0}$ on the boundary, one can assume from the start that the boundary is a straight line and approach the problem. This is called straightening the boundary.</description></item><item><title>Generalized Hölder's Inequality, Corollaries of Hölder's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/1091/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1091/</guid><description>Description Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. Suppose we are given two constants $1 \lt p \lt \infty, 1 \lt p^{\prime} \lt \infty$ that satisfy the following equation: $$ \dfrac{1}{p} + \dfrac{1}{p^{\prime}} = 1 \left(\text{or } p^{\prime} = \frac{p}{p-1} \right) $$ If $u \in L^p(\Omega)$, $v\in L^{p^{\prime}}(\Omega)$, then $uv \in L^1(\Omega)$ and the inequality below holds. $$ \| uv \|_{1} = \int_{\Omega} |u(x)v(x)| dx \le \| u</description></item><item><title>Holder Continuous Function Spaces</title><link>https://freshrimpsushi.github.io/en/posts/1087/</link><pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1087/</guid><description>Definitions1 Space of Continuous Functions Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. For a non-negative integer $m$, for all multi-indices $\alpha$ where $|\alpha| \le m$, the set of $\phi$ that are continuous in $\Omega$ for $D^{\alpha}\phi$ is called the space of continuous functions. $$ C^{m}\left( \Omega \right) := \left\{ \phi : D^{\alpha} \phi \text{ is continuous on } \Omega, \forall \left| \alpha \right| \lt m \right\} $$ In</description></item><item><title/><link>https://freshrimpsushi.github.io/en/posts/1086/</link><pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1086/</guid><description>Definition Fourier Transform as a Function We define the Fourier transform of function $f \in$ $L^{1}$ as follows: $$ \hat{f}(\xi) := \int_{-\infty}^{\infty} f(t) e^{-i \xi t}dt $$ Fourier Transform as an Operator An operator defined as $\mathcal{F} : L^{1} \to$ $C_{0}$ is called the Fourier transform. $$ \mathcal{F}[f] (\xi) = \int_{-\infty}^{\infty} f(t) e^{-i \xi t}dt $$ Explanation As seen in the definition, the term Fourier transform can refer to the</description></item><item><title>Complete Orthonormal Basis and Complete Orthonormal Set</title><link>https://freshrimpsushi.github.io/en/posts/1082/</link><pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1082/</guid><description>Theorem: Equivalence Conditions of an Orthonormal Set Let $\left\{ \phi_{n} \right\}_{1}^\infty$ be an orthonormal set of $L^2(a,b)$ and denote $f \in L^2(a,b)$. Then, the following conditions are equivalent. $(a)$ For all $n$, if $\left\langle f, \phi_{n} \right\rangle=0$ then $f=0$. $(b)$ For all $f\in L^2(a,b)$, the series $\sum_{1}^\infty \left\langle f,\phi_{n}\right\rangle\phi_{n}$ converges to $f$ in the norm sense. That is, the following equation holds: $$ f=\sum_{1}^\infty \left\langle f,\phi_{n}\right\rangle\phi_{n} $$ $(c)$ For all</description></item><item><title>Bessel's Inequality in L2 Space</title><link>https://freshrimpsushi.github.io/en/posts/1081/</link><pubDate>Mon, 13 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1081/</guid><description>Theorem Let&amp;rsquo;s say $\left\{ \phi_{n} \right\}_{n=1}^{\infty}$ is an orthonormal set in $L^{2}(a,b)$. And let $f\in L^{2}(a,b)$. Then, the following inequality holds. $$ \sum \limits_{n=1}^\infty \left| \left\langle f, \phi_{n} \right\rangle \right|^{2} \le \| f \|^{2} $$ Explanation This is called Bessel&amp;rsquo;s inequality. $L^2$ space A function that satisfies the following equation is called square-integrable from $하다고 한다. $$ \int_{a}^b |f(x)|^2 dx &amp;lt; \infty $$ 구간</description></item><item><title>Convergence of Norms of Function Sequences</title><link>https://freshrimpsushi.github.io/en/posts/1080/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1080/</guid><description>Definitions Suppose a sequence of functions $\left\{ f_{n} \right\}$ is given. If $\| f_{n} - f \|$ converges to $0$, then $f_{n}$ is said to converge in norm, denoted as follows. $$ f_{n} \to f \text{ in norm } $$ or $$ \| f_{n} - f\| \to 0 $$ or $$ \lim \limits_{n \to 0} \| f_{n}-f\|=0 $$ Explanation To define the limit of a sequence, the concept of distance</description></item><item><title>Delayed Potential on Continuous Distribution of Delay Times</title><link>https://freshrimpsushi.github.io/en/posts/1075/</link><pubDate>Thu, 09 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1075/</guid><description>Overview1 The scalar and vector potentials for a moving point charge are called retarded potentials, and they are as follows. $$ \begin{align*} V(\mathbf{r},\ t) &amp;amp;= \dfrac{1}{4\pi\epsilon_{0}} \int \dfrac{ \rho (\mathbf{r}^{\prime},\ t_{r}) }{ \cR } d\tau^{\prime} \\[1em] \mathbf{A}( \mathbf{r},\ t) &amp;amp;= \dfrac{\mu_{0}}{4\pi} \int \dfrac{\mathbf{J}(\mathbf{r}^{\prime},\ t_{r})}{\cR}d\tau^{\prime} \end{align*} $$ Here, $t_{r}$ is the retarded time. Retarded Time If the charge and current distribution do not change over time, the scalar and vector potentials</description></item><item><title>Solution of Nonlinear First Order PDE Using Characteristic Equations</title><link>https://freshrimpsushi.github.io/en/posts/1074/</link><pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1074/</guid><description>Explanation1 When emphasizing that x and p are variables of a partial differential equation, they are denoted in normal font as $x,p \in \mathbb{R}^{n}$, and when emphasizing them as functions of $s$, they are denoted in bold font as $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Characteristic Equations $$ \begin{cases} \dot{\mathbf{p}} (s) = -D_{x}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big)-D_{z}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big)\mathbf{p}(s) \\ \dot{z}(s) = D_{p}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big) \cdot \mathbf{p}(s) \\ \dot{\mathbf{x}}(s) = D_{p}F\big(\mathbf{p}(s),\</description></item><item><title>Characteristics of Nonlinear First-Order Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1073/</link><pubDate>Tue, 07 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1073/</guid><description>Regarding x and p, when emphasizing that they are variables of a partial differential equation, they are denoted in normal font as $x,p \in \mathbb{R}^{n}$, and when emphasizing that they are functions of $s$, they are denoted in bold font as $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Characteristic Method1 Let&amp;rsquo;s suppose that an open set $\Omega \subset \mathbb{R}^{n}$ is given. Assume that $u\in C^{2}(\Omega)$ is a solution to the following nonlinear first-order</description></item><item><title>Notation for Nonlinear First-Order Partial Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/1071/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1071/</guid><description>Notation1 A nonlinear first-order partial differential equation is denoted as follows. $$ \begin{equation} F(Du, u, x) = F(p, z, x) = 0 \label{eq1} \end{equation} $$ $\Omega \subset \mathbb{R}^{n}$ is an open set $x\in \Omega$ $F : \mathbb{R}^n \times \mathbb{R}^n \times \bar{ \Omega } \to \mathbb{R}$ is the given function $u : \bar{ \Omega } \to \mathbb{R}$ is the variable of $F$ Description Solving a nonlinear first-order partial differential equation $F$</description></item><item><title>Coulomb Gauge and Lorentz Gauge</title><link>https://freshrimpsushi.github.io/en/posts/134/</link><pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/134/</guid><description>Overview1 A relationship exists between the potential and the charge density, current density as follows. $$ \begin{align*} \nabla ^2 V +\dfrac{\partial }{\partial t}(\nabla \cdot \mathbf{A}) &amp;amp;= -\frac{1}{\epsilon_{0}}\rho \\ \left( \nabla ^2 \mathbf{A}-\mu_{0}\epsilon_{0} \dfrac{\partial ^2 \mathbf{A} }{\partial t^2} \right) -\nabla\left( \nabla \cdot \mathbf{A} +\mu_{0}\epsilon_{0} \dfrac{\partial V}{\partial t}\right) &amp;amp;= -\mu_{0} \mathbf{J} \end{align*} $$ Depending on how assumptions about the potential are made, the expression changes. Coulomb Gauge As in magnetostatics, the divergence</description></item><item><title>Gauge Transformation</title><link>https://freshrimpsushi.github.io/en/posts/129/</link><pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/129/</guid><description>Overview1 A given scalar potential $V$ and vector potential $\mathbf{A}$ uniquely determine the electric field $\mathbf{E}$ and the magnetic field $\mathbf{B}$, but the converse is not true. In other words, there are multiple potentials $V$, $\mathbf{A}$ that can represent a single electromagnetic field $\mathbf{E}$, $\mathbf{B}$. Therefore, within the changes that do not alter $\mathbf{E}$ and $\mathbf{B}$, $V$ and $\mathbf{A}$ can be changed freely. Gauge Transformation There exist two pairs of</description></item><item><title>Vertical Waves, Parallel Waves, Plane Polarization</title><link>https://freshrimpsushi.github.io/en/posts/1070/</link><pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1070/</guid><description>Definition A wave whose direction of propagation and direction of vibration are perpendicular to each other is called a transverse wave. Conversely, a wave whose direction of propagation and direction of vibration are parallel to each other is called a longitudinal wave. Explanation The phenomenon of a wave vibrating in a specific direction is called polarization. Since there are two directions perpendicular to the direction of propagation for a transverse</description></item><item><title>Smoothing Effect of Harmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1063/</link><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1063/</guid><description>Theorem Mean Value Property $$ \begin{align*} u(x) = -\!\!\!\!\!\! \int_{\partial B(x,r)} udS = -\!\!\!\!\!\! \int _{B(x,r)} udy \end{align*} $$ Assuming $u \in C(\Omega)$ satisfies the mean value property in each open ball $B(x,r)\subset \Omega$, then the following holds. $$ u \in C^{\infty}(\Omega) $$ Description If it&amp;rsquo;s Harmonic, it means it&amp;rsquo;s smooth inside. It is important to note that smoothness or continuity is not guaranteed at the boundary $\partial \Omega$. Proof</description></item><item><title>Mollification</title><link>https://freshrimpsushi.github.io/en/posts/1060/</link><pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1060/</guid><description>Definition 1 $f \in {L^1_{\mathrm{Loc}}( \Omega)}$ and $\epsilon&amp;gt;0$ with respect to $f$, the $\epsilon$-mollification is defined as follows. $$ f^{\epsilon}(x) := \eta_{\epsilon} * f (x) =\int_{\mathbb{R}^{n}} \eta_{\epsilon}(x-y)f(y)dy, \quad x\in \Omega_{&amp;gt;\epsilon} $$ Here, $f$ is a function defined as $0$ outside of $\Omega$. $\eta_\epsilon$ is a mollifier. $\ast$ is a convolution. $\Omega_{&amp;gt;\epsilon} := \left\{ x \in \Omega : \mathrm{dist}(x, \partial \Omega) &amp;gt; \epsilon \right\}$ Properties (i) $f^{\epsilon} \in C^\infty( \Omega_{&amp;gt;\epsilon})$ (ii)</description></item><item><title>Multi Index Notation</title><link>https://freshrimpsushi.github.io/en/posts/1062/</link><pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1062/</guid><description>Definition[^1] A multi-index with order $|\alpha|$ is a tuple $\alpha=(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n})$ whose components are non-negative integers. Here, $| \alpha|$ is defined as follows. $$ |\alpha| = \sum _{i}^{n} \alpha_{i} = \alpha_{1} + \cdots + \alpha_{n} $$ Notation For $x = (x_{1}, x_{2}, \dots, x_{n}) \in \mathbb{R}^{n}$, $x^{\alpha}$ is defined as follows. $$ x^{\alpha} := x_{1}^{\alpha_{1}} x_{2}^{\alpha_{2}} \cdots x_{n}^{\alpha_{n}} $$ The multi-index is often used to represent partial derivatives</description></item><item><title>Mollifiers</title><link>https://freshrimpsushi.github.io/en/posts/1059/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1059/</guid><description>Definition1 Let&amp;rsquo;s define the function $\eta \in C^{\infty}(\mathbb{R}^{n})$ as follows. $$ \begin{equation} \eta (x) := \begin{cases} C \exp \left( \dfrac{1}{|x|^2-1} \right) &amp;amp; |x|&amp;lt;1 \\ 0 &amp;amp; |x| \ge 1\end{cases} \label{1} \end{equation} $$ Such $\eta$ is called a mollifier. In particular, when $C&amp;gt;0$ is a constant satisfying $\displaystyle \int_{\mathbb{R}^{n}} \eta dx=1$, $\eta$ is called a standard mollifier. Let&amp;rsquo;s define $\eta_{\epsilon}$ for $\epsilon&amp;gt;0$ as follows. $$ \eta_\epsilon (x) := \dfrac{1}{\epsilon^n}\eta\left( \dfrac{x}{\epsilon} \right)</description></item><item><title>Sine Waves and Complex Wave Functions</title><link>https://freshrimpsushi.github.io/en/posts/1066/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1066/</guid><description>Definitions A wave expressed as a sine function is called a sine wave. Description The general form of a sine wave is as follows. The reason why it&amp;rsquo;s referred to as a sine wave even though the equation is $\cos$ is explained below, as the real part of the complex wave function is $\cos$. $\sin$ is the imaginary part. $$ f(x,t) = A \cos \big( k(x-vt)+\delta \big) $$ Here, $A$</description></item><item><title>Wave Boundary Conditions: Reflection and Transmission</title><link>https://freshrimpsushi.github.io/en/posts/1058/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1058/</guid><description>Let us consider a situation where two different strings are tied together, and a wave propagates from left to right along string 1. Since the propagation speed of the wave is related to the mass, the speed changes as the wave passes through the point where the strings are tied. For convenience, let&amp;rsquo;s denote the location of the knot as $x=0$ and assume that the wave enters from the left.</description></item><item><title>Derivation of the One-Dimensional Wave Equation</title><link>https://freshrimpsushi.github.io/en/posts/1057/</link><pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1057/</guid><description>Overview The one-dimensional wave equation is as follows. $$ \dfrac{\partial ^{2} f }{\partial x^{2}} = \dfrac{1}{v^{2}}\dfrac{\partial ^{2} f}{\partial t^{2}} $$ Here, $v$ represents the propagation speed of the wave. Characteristics of Waves Let&amp;rsquo;s assume there is a wave with a constant speed of $v$ as shown in Figure 1. Let the displacement of the point at $x$ at time $t$ be $f(x,t)$. Assuming the initial displacement of the string is</description></item><item><title>Angular Momentum of the Electromagnetic Field</title><link>https://freshrimpsushi.github.io/en/posts/1056/</link><pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1056/</guid><description>Overview1 The angular momentum stored in the electromagnetic field is as follows. $$ \mathbf{\ell} = \mathbf{r} \times \mathbf{g}=\epsilon_{0}\big( \mathbf{r} \times (\mathbf{E} \times \mathbf{B} )\big) $$ $\mathbf{g}$ is the momentum density stored in the electromagnetic field. Description The electromagnetic field is not only a mediator of the electromagnetic forces acting between charges but also possesses energy itself. $$ u =\dfrac{1}{2} \left( \epsilon_{0} E^2 + \dfrac{1}{\mu_{0}} B^2 \right) $$ It also possesses</description></item><item><title>Conservation of Momentum in Electrodynamics</title><link>https://freshrimpsushi.github.io/en/posts/1055/</link><pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1055/</guid><description>Overview1 In electrodynamics, the law of conservation of momentum is as follows. $$ \dfrac{d \mathbf{p}}{dt} =-\epsilon_{0}\mu_{0}\dfrac{d}{dt}\int_{\mathcal{V}} \mathbf{S} d\tau + \oint_{\mathcal{S}} \mathbf{T} \cdot d\mathbf{a} $$ Explanation According to Newton&amp;rsquo;s second law, the force acting on an object and the change in the object&amp;rsquo;s momentum are equal. $$ \mathbf{F} = \dfrac{d \mathbf{p}}{dt} $$ $\mathbf{p}$ is the total mechanical momentum of particles within volume $\mathcal{V}$. To distinguish it from the momentum stored in</description></item><item><title>Uniqueness of the Solution to the Dirichlet Problem for the Poisson Equation</title><link>https://freshrimpsushi.github.io/en/posts/1046/</link><pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1046/</guid><description>Theorem1 Let&amp;rsquo;s assume that $\Omega \subset \mathbb{R}^n$ is open and bounded. And let $g \in C(\partial \Omega)$, $f \in C(\Omega)$. Then in the Dirichlet problem of the Poisson equation as below, the solution $u \in C^2(\Omega) \cap C(\bar{\Omega})$, if exists, is unique (=at most one exists). $$ \begin{equation} \left\{ \begin{aligned} -\Delta u &amp;amp;= f &amp;amp;&amp;amp; \text{in } \Omega \\ u &amp;amp;= g &amp;amp;&amp;amp; \text{on }\partial \Omega \end{aligned} \right. \label{eq1} \end{equation}</description></item><item><title>Maximum Principle of Harmonic Functions</title><link>https://freshrimpsushi.github.io/en/posts/1044/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1044/</guid><description>Theorem1 Let $\Omega \subset \mathbb{R}^n$ be open and bounded. Also, suppose $u : \Omega \to \mathbb{R}$ is equal to $u \in C^2(\Omega) \cap C(\bar \Omega)$ and satisfies the Laplace equation. Then, the following holds: (i) Maximum Principle $$ \max \limits_{\bar \Omega} u = \max \limits_{\partial \Omega} u \quad \left( \mathrm{or} \ \ \min \limits_{\bar \Omega} u= \min \limits_{\partial \Omega} u \right) $$ (ii) Strong Maximum Principle If $\Omega$ is a</description></item><item><title>Electromagnetic Force on a Charge inside a Volume</title><link>https://freshrimpsushi.github.io/en/posts/1042/</link><pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1042/</guid><description>Overview1 The electromagnetic force experienced by all charges in volume $\mathcal{V}$ is as follows. $$ \mathbf{F} =\oint_{\mathcal{S}} \mathbf{T} \cdot d\mathbf{a} -\epsilon_{0}\mu_{0}\dfrac{d}{dt}\int_{\mathcal{V}} \mathbf{S} d\tau $$ $\mathcal{S}$ is the boundary surface of volume $\mathcal{V}$, $\mathbf{T}$ is the Maxwell stress tensor, $\mathbf{S}$ is the Poynting vector. Derivation Part 1. By Lorentz force law, the force experienced by a charge is $$ \mathbf{F}=q(\mathbf{E} + \mathbf{v} \times \mathbf{B}) $$ If we express the charge quantity</description></item><item><title>Maxwell's Stress Tensor</title><link>https://freshrimpsushi.github.io/en/posts/1041/</link><pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1041/</guid><description>Definition1 The tensor $\mathbf{T}$ below is called the Maxwell stress tensor. $$ \mathbf{T}=\overleftrightarrow{\mathbf{T}}=\begin{pmatrix} T_{xx} &amp;amp; T_{xy} &amp;amp; T_{xz} \\ T_{yx} &amp;amp; T_{yy} &amp;amp; T_{yz} \\ T_{zx} &amp;amp; T_{zy} &amp;amp; T_{zz} \end{pmatrix} $$ $$ T_{ij}=\epsilon_{0} \left( E_{i}E_{j}-\dfrac{1}{2}\delta_{ij}E^2 \right) + \dfrac{1}{\mu_{0}}\left(B_{i}B_{j}-\dfrac{1}{2}\delta_{ij}B^2 \right) $$ Here, $\delta_{ij}$ is the Kronecker delta. Description $2$th order tensor is defined as above. It appears in the process of deriving the force experienced by a charge in a</description></item><item><title>In Physics, What is a Tensor</title><link>https://freshrimpsushi.github.io/en/posts/1040/</link><pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1040/</guid><description>Overview Without a doubt, this is the easiest explanation about tensors, so if you&amp;rsquo;re an undergraduate in physics who came here because you don&amp;rsquo;t know what a tensor is, I highly recommend reading this. We&amp;rsquo;re not accepting corrections about mathematical inaccuracies. Teaching someone who hasn&amp;rsquo;t learned about negative numbers that &amp;lsquo;you can&amp;rsquo;t subtract a larger number from a smaller number&amp;rsquo;, or someone who hasn&amp;rsquo;t learned about complex numbers that &amp;lsquo;you</description></item><item><title>Poynting's Theorem and Poynting Vector</title><link>https://freshrimpsushi.github.io/en/posts/1039/</link><pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1039/</guid><description>Theorem1 The work done by the electromagnetic force on a charge is equal to the decrease in energy stored in the electromagnetic field plus the energy that has leaked out through the boundary. This is called Poynting&amp;rsquo;s theorem. $$ \begin{align*} \dfrac{dW}{dt} &amp;amp;= -\dfrac{d}{dt} \int_{\mathcal{V}} \dfrac{1}{2} \left( \epsilon_{0} E^2 + \dfrac{1}{\mu_{0}} B^2 \right) d\tau - \dfrac{1}{\mu_{0}} \oint_{\mathcal{S}} (\mathbf{E} \times \mathbf{B}) \cdot d \mathbf{a} \\ &amp;amp;= -\dfrac{d}{dt} \int_{\mathcal{V}} u d\tau - \oint_{\mathcal{S}}\mathbf{S}</description></item><item><title>Maxwell's Equations</title><link>https://freshrimpsushi.github.io/en/posts/1038/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1038/</guid><description>Formulas Maxwell&amp;rsquo;s Equations $(\text{i}) \quad \nabla \cdot \mathbf{E}=\dfrac{1}{\epsilon_{0}}\rho$ (Gauss&amp;rsquo;s Law) $(\text{ii}) \quad \nabla \cdot \mathbf{B}=0$ (Gauss&amp;rsquo;s Law for Magnetism) $(\text{iii}) \quad \nabla \times \mathbf{E} = -\dfrac{\partial \mathbf{B}}{\partial t}$ (Faraday&amp;rsquo;s Law) $(\text{iv}) \quad \nabla \times \mathbf{B} = \mu_{0} \mathbf{J}+\mu_{0}\epsilon_{0}\dfrac{\partial \mathbf{E}}{\partial t}$ (Ampère&amp;rsquo;s Law) Description1 Before Maxwell completed the Maxwell&amp;rsquo;s equations, the four equations concerning the electric field</description></item><item><title>The Mean Value Theorem for Laplace's Equation</title><link>https://freshrimpsushi.github.io/en/posts/1037/</link><pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1037/</guid><description>Theorem1 Let us assume that an open set $\Omega \subset \mathbb{R}^{n}$ is given. Also, let&amp;rsquo;s assume that $u \in C^2(\Omega)$ satisfies the Laplace equation. Then, for each open ball $B(x,r)\subset \subset \Omega$, the following holds. $$ \begin{align*} u(x) &amp;amp;= \dfrac{1}{n \alpha (n)r^{n-1}} \int _{\partial B(x,r)} udS =: -\!\!\!\!\!\! \int_{\partial B(x,r)} udS \\ &amp;amp;= \dfrac{1}{\alpha (n)r^n}\int_{B(x,r)}udy =: -\!\!\!\!\!\! \int _{B(x,r)} udy \end{align*} $$ It is denoted as $V\subset \subset U$ when</description></item><item><title>Continuity Equations in Electromagnetism</title><link>https://freshrimpsushi.github.io/en/posts/1032/</link><pubDate>Sat, 13 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1032/</guid><description>Formulas The following formula is known as the continuity equation. $$ \dfrac{\partial \rho}{\partial t}=-\nabla \cdot \mathbf{J} $$ Explanation1 The continuity equation mathematically expresses the law of conservation of charge in a local region. The law of conservation of charge states that the original amount of charge does not suddenly disappear or newly appear; the initial amount of charge is maintained. This is true not only for the entire universe but</description></item><item><title>The sufficient condition for the Fourier series of a function to converge absolutely and uniformly to the function</title><link>https://freshrimpsushi.github.io/en/posts/1030/</link><pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1030/</guid><description>Theorem The function $f$, defined in $[L, -L)$, is continuous and piecewise smooth. Therefore, the Fourier series of $f$ absolutely and uniformly converges to $f$. When $f$ is piecewise smooth, its Fourier series converges pointwise to $f$. If the condition that $f$ is continuous is strengthened by removing the discontinuity points of $f$, then the Fourier series of $f$ absolutely and uniformly converges to $f$. The proof uses the Cauchy-Schwarz</description></item><item><title>Energy in a Magnetic Field</title><link>https://freshrimpsushi.github.io/en/posts/1029/</link><pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1029/</guid><description>Explanation1 Just like we considered the energy of the electric field created by a charge distribution in the energy of the electric field created by a charge distribution, we can think about the energy of the magnetic field created by a current distribution. When current flows through a circuit, energy is inputted. The identity of this energy is exactly the work done against electromotive force (EMF). Because of EMF, it&amp;rsquo;s</description></item><item><title>Self-Inductance</title><link>https://freshrimpsushi.github.io/en/posts/1028/</link><pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1028/</guid><description>Explanation1 In the situation shown in the figure above, if current $I_{1}$ flows through loop 1, magnetic field $\mathbf{B}_{1}$ will flow and the magnetic flux passing through loop 2 can be calculated as follows. $$ \Phi_{2} = M_{21}I_{1} $$ At this point, $M_{21}$ is referred to as mutual inductance. Now, let&amp;rsquo;s say the current $I_{1}$ flowing through loop 1 changes over time. Then, the magnetic flux passing through loop 2</description></item><item><title>Mutual Inductance</title><link>https://freshrimpsushi.github.io/en/posts/1027/</link><pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1027/</guid><description>Explanation1 Consider two fixed conducting loops as shown in the figure above. If a steady current $I_{1}$ flows through loop 1, it generates a magnetic field $\mathbf{B}_{1}$.(Ampère&amp;rsquo;s law) Some of the magnetic field lines $\mathbf{B}_{1}$ will pass through loop</description></item><item><title>Faraday's Law and Lenz's Law</title><link>https://freshrimpsushi.github.io/en/posts/1018/</link><pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1018/</guid><description>Faraday&amp;rsquo;s Law A changing magnetic field induces an electric field. $$ \nabla \times \mathbf{E} = -\dfrac{\partial \mathbf{B}}{\partial t} $$ Explanation1 In 1831, Faraday announced the results of experiments as follows: A wire loop placed in a magnetic field was pulled to the right. A current flowed through the loop. With the wire loop fixed in the magnetic field, a magnet was pushed to the left. A current flowed through the</description></item><item><title>Poisson's Equation Fundamental Solution</title><link>https://freshrimpsushi.github.io/en/posts/1015/</link><pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1015/</guid><description>Buildup1 Fundamental Solution to the Laplace Equation Let&amp;rsquo;s define the function $\Phi$ as the fundamental solution to the Laplace equation, where $x \in \mathbb{R}^{n}$ and $x \ne 0$. $$ \Phi (x) := \begin{cases} -\frac{1}{2\pi}\log |x| &amp;amp; n=2 \\ \frac{1}{n(n-2)\alpha (n)} \frac{1}{|x|^{n-2}} &amp;amp; n \ge 3 \end{cases} $$ Consider a function that maps as $x \mapsto \Phi (x)$. It is harmonic at places where $x \ne 0$. Suppose we symmetrize the</description></item><item><title>Fourier Coefficients of Odd Functions</title><link>https://freshrimpsushi.github.io/en/posts/1008/</link><pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1008/</guid><description>Theorem If a function $f$ with a period of $2L$ is antisymmetric, then the Fourier coefficients of $f$ are as follows. $$ \begin{align*} a_{0} &amp;amp;= 0 \\ a_{n} &amp;amp;= \begin{cases} \dfrac{2}{L} {\displaystyle \int_{0}^{L}} f(t) \cos \frac{n \pi t}{L} dt &amp;amp; (n=1, 3, \cdots ) \\ 0 &amp;amp; (n=0, 2, \cdots )\end{cases} \\ b_{n} &amp;amp;= \begin{cases} \dfrac{2}{L} {\displaystyle \int_{0}^{L}} f(t) \sin \frac{n \pi t}{L} dt &amp;amp; (n=1, 3, \cdots ) \\</description></item><item><title>Fourier Cosine Series, Sine Series, Fourier Coefficients of Even and Odd Functions</title><link>https://freshrimpsushi.github.io/en/posts/1007/</link><pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1007/</guid><description>Definition Let&amp;rsquo;s say $f$ is a piecewise smooth function on the interval $[0,L)$. The following defined $f_{e}$ on the interval $[-L, L)$ is called the even extension of $f$. $$ f_{e}(t) := \begin{cases} f(t) &amp;amp; -L \le t &amp;lt;0 \\ f(-t) &amp;amp; 0 \le t &amp;lt;L\end{cases} $$ Similarly, the following defined $f_{o}$ on the interval $[-L, L)$ is called the odd extension of $f$. $$ f_{o}(t) := \begin{cases} -f(-t) &amp;amp;</description></item><item><title>Half-Wave Symmetric Function</title><link>https://freshrimpsushi.github.io/en/posts/1003/</link><pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1003/</guid><description>Definition A periodic function $f$ with a period of $2L$ is said to have half-wave symmetry when it satisfies the following equation for all $t$. $$ f(t)=-f(t+L) $$ Description Expanding on the definition above, it means &amp;lsquo;when a wave is on the $xy$ plane, the pattern of the wave alternates symmetrically around the $y$ axis based on the midpoint of the period.&amp;rsquo; Example As the name implies, it means being</description></item><item><title>Any Function Can Always Be Expressed as the Sum of Odd and Even Functions</title><link>https://freshrimpsushi.github.io/en/posts/1002/</link><pubDate>Fri, 29 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1002/</guid><description>Theorem The arbitrary function $f$ defined in $\mathbb{R}$ can always be expressed as a sum of an even function and an odd function. Proof Let $f_{e}(t)$ and $f_o(t)$ be as follows. $$ f_{e}(t)=\dfrac{ f(t)+f(-t)}{2},\ \ \ f_o(t)=\dfrac{ f(t)-f(-t)}{2} $$ Then, $f_{e}(t)$ is an even function, and $f_o(t)$ is an odd function, and the following equation holds. $$ f_{e}(x)+f_o(x)=f(x) $$ ■</description></item><item><title>Heat Equation, Diffusion Equation</title><link>https://freshrimpsushi.github.io/en/posts/1001/</link><pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1001/</guid><description>Definition1 2 The following partial differential equation is referred to as the heat equation or the diffusion equation. $$ \dfrac{\partial u}{\partial t} = \dfrac{\partial^{2} u}{\partial x^{2}} $$ When the spatial coordinate is $n$-dimensional, $$ \dfrac{\partial u}{\partial t} = \Delta u = \nabla^{2}u $$ here, $\Delta = \nabla^{2} = \sum\limits_{i=1}^{n} \dfrac{\partial^{2} }{\partial x_{i}^{2}}$ refers to the Laplacian. When there is an external force $f = f(x,t)$, $$ \dfrac{\partial u}{\partial t} =</description></item><item><title>Definition of Convolution</title><link>https://freshrimpsushi.github.io/en/posts/1000/</link><pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1000/</guid><description>Definition Let&amp;rsquo;s assume that two functions $f$ and $g$ defined in $\mathbb{R}$ are given. If the integral below exists, it is called the convolution of the two functions $f$ and $g$, and is denoted by $f \ast g$. $$ f \ast g(x):=\int _{-\infty} ^{\infty} f(y)g(x-y)dy $$ If $f$ and $g$ are discrete functions, they are defined as follows. $$ (f \ast g)(m)=\sum \limits_{n}f(n)g(m-n) $$ Explanation Although there is a translation</description></item><item><title>Laplace's Equation and Poisson's Equation</title><link>https://freshrimpsushi.github.io/en/posts/997/</link><pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/997/</guid><description>Definition1 $\ U \in \mathbb{R}^n$ is an open set $\ x\in U$ $u=u(x) : \overline{U} \rightarrow \mathbb{R}^n$ Laplace&amp;rsquo;s Equation The partial differential equation below is called Laplace&amp;rsquo;s equation. $$ \Delta u=0 $$ Here, $\Delta$ is the Laplacian. A $u$ that satisfies Laplace&amp;rsquo;s equation is specifically called a harmonic function. Poisson&amp;rsquo;s Equation The nonhomogeneous Laplace&amp;rsquo;s equation is called Poisson&amp;rsquo;s equation. $$ -\Delta u = f $$ Explanation Laplace&amp;rsquo;s equation appears in</description></item><item><title>Proving the Invariance of the Laplace Equation with Respect to Orthogonal Transformations</title><link>https://freshrimpsushi.github.io/en/posts/995/</link><pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/995/</guid><description>Theorem1 Let&amp;rsquo;s say $u$ satisfies the Laplace equation. And let&amp;rsquo;s define $v(x)$ as follows. $$ v(x) :=u(Rx) $$ Then, $R$ is a rotation transformation. Therefore, $v(x)$ also satisfies the Laplace equation. $$ \Delta v=0 $$ Explanation In fact, the content above holds for all orthogonal transformations. Thus, the fact that the Laplace equation is invariant under rotation transformation is a specific case of the fact that the Laplace equation is</description></item><item><title>Exterior Unit Normal Vector</title><link>https://freshrimpsushi.github.io/en/posts/988/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/988/</guid><description>Definition1 Let $U\subset \mathbb{R}^{n}$ be an open set. Let the boundary of $U$ be $\partial U$, which is a $\partial U \in C^1$. Then, the following outward unit normal vector can be defined: $$ \boldsymbol{\nu}=(\nu^{1}, \nu^{2}, \dots, \nu^{n}) \quad \text{and} \quad |\boldsymbol{\nu}|=1 $$ $\boldsymbol{\nu}$ is a vector that touches a point on the boundary, has a magnitude of 1, and points outward. Let it be $u \in C^{1}(\bar{U})$. Then, the</description></item><item><title>Green's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/974/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/974/</guid><description>Theorem Let&amp;rsquo;s assume $u, v \in C^2( \bar{U})$. Then, the following expressions hold: (i) $\displaystyle \int_{U} \Delta u dx=\int_{\partial U} \dfrac{\partial u}{\partial \nu}dS$ (ii) $\displaystyle \int_{U} Dv \cdot Du dx = -\int_{U} u \Delta v dx+\int_{\partial U}\dfrac{\partial v}{\partial \nu}udS$ (iii) $\displaystyle \int_{U} (u\Delta v - v\Delta u )dx = \int_{\partial U} \left( \dfrac{\partial v}{\partial \nu}u - \dfrac{\partial u}{\partial \nu} v\right)dS$ These are collectively referred to as Green&amp;rsquo;s formula. $\Delta$ is</description></item><item><title>Green's Theorem, Integration by Parts Formula</title><link>https://freshrimpsushi.github.io/en/posts/990/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/990/</guid><description>Theorem Let $U\subset \mathbb{R}^{n}$ be an open set. Suppose $u : \bar{U} \to \mathbb{R}$ and $u \in C^1(\bar{U})$. Let $\nu$ be the outward unit normal vector. Then, the following equation holds: $$ \begin{equation} \int_{U} u_{x_{i}}dx=\int _{\partial U} u\nu^{i} dS\quad (i=1,\dots, n) \label{eq1} \end{equation} $$ Summing this over all $i$ gives the equation below. For each $u^{1} \in C^{1}(\bar{U})$, if we say $\mathbf{u} = (u^{1},\dots,u^{n}) : \bar{U} \to \mathbb{R}^{n}$, then: $$</description></item><item><title>Initial Value Problem and Inhomogeneous Problem Solutions for the Transport Equation</title><link>https://freshrimpsushi.github.io/en/posts/986/</link><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/986/</guid><description>Equation Below partial differential equation is known as the transport equation. $$ u_{t} + b \cdot Du=0\quad \text{in }\mathbb{R}^n \times (0,\ \infty) $$ Solution1 Initial Value Problem Let&amp;rsquo;s suppose the initial value problem of the transport equation is given as follows. $$ \begin{equation} \left\{ \begin{aligned} u_{t}+b \cdot Du &amp;amp;= 0 &amp;amp;&amp;amp; \text{in } \mathbb{R}^n \times [0,\ \infty) \\ u &amp;amp;= g &amp;amp;&amp;amp; \text{on } \mathbb{R}^n\times \left\{ t=0 \right\} \end{aligned} \right.</description></item><item><title>Derivative's Fourier Coefficients</title><link>https://freshrimpsushi.github.io/en/posts/979/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/979/</guid><description>Formula Given that a function $f$ defined on the interval $[-L,\ L)$ is continuous and piecewise smooth, then the Fourier coefficients of $f^{\prime}$ are as follows. $$ a^{\prime}_{n}=\dfrac{n\pi}{L}b_{n} $$ $$ b^{\prime}_{n}=-\dfrac{n\pi}{L}a_{n} $$ $$ c^{\prime}_{n}=\dfrac{in\pi}{L}c_{n} $$ Here, $a_{n},\ b_{n}$ are the Fourier coefficients of $f$, and $c_{n}$ are the complex Fourier coefficients of $f$. Proof $$ \begin{align*} c^{\prime}_{n} &amp;amp;=\dfrac{1}{2L}\int _{-L}^{L} f^{\prime}(t)e^{-i\frac{n\pi t}{L}}dt \\ &amp;amp;= \dfrac{1}{2L}\left[ f(t)e^{-i\frac{n\pi t}{L}} \right]_{-L}^{L} +\dfrac{in \pi}{L}\dfrac{1}{2L}\int_{-L}^{L} f(t)e^{-i\frac{n</description></item><item><title>Fourier Series Integration</title><link>https://freshrimpsushi.github.io/en/posts/980/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/980/</guid><description>Theorem Let&amp;rsquo;s say a periodic function $f$ with a period $2L$ is piecewise continuous in the interval $[-L,\ L)$. Then, the definite integral of $f$ can be expressed as follows. $$ \int_{t_{1}}^{t_{2}} f(t) dt= c_{0}(t_{2}-t_{1}) +\sum \limits_{n \ne 0} \dfrac{L}{in\pi}c_{n}\left( e^{i\frac{n\pi t_{2}}{L}}-e^{i\frac{n\pi t_{1}}{L}} \right) $$ Here, $c_{0},\ c_{n}$ is the complex Fourier coefficient. Thus, the definite integral of $f(t)$ is the same as summing the definite integrals of each term</description></item><item><title>Mean of Function Values</title><link>https://freshrimpsushi.github.io/en/posts/983/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/983/</guid><description>Definition The average value of a function between $[a,\ b]$ and $f(x)$ is equivalent to dividing the integral of the function over the interval by the length of the interval. $$ \dfrac{1}{b-a}\int_{a}^bf(x)dx $$ Derivation Let&amp;rsquo;s denote a partition of the interval $[a,\ b]$ as $P$. $$ P=\left\{ x_{1},\ x_{2},\ \cdots ,\ x_{n} \right\} $$ In this case, $a=x_{1} &amp;lt; x_{2} &amp;lt; \cdots &amp;lt; x_{n}=b$ and the distance between each point</description></item><item><title>The constant term of the Fourier series is equal to the average of one period of the function.</title><link>https://freshrimpsushi.github.io/en/posts/984/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/984/</guid><description>Theorem The constant term of the Fourier series of a function with period $2L$, namely $f$, equals the average of one period of the function $f$. Proof By definition The integral over one period of $f(t)$ is $$ \dfrac{1}{2L}\int_{-L}^{L} f(t)dt $$ According to the definition of the Fourier coefficients, this is equal to $\dfrac{1}{2}a_{0}$. Therefore, the integral over one period of $f(t)$ is the same as the constant term of</description></item><item><title>The Integral of a Periodic Function Over One Period is Constant Regardless of the Integration Interval</title><link>https://freshrimpsushi.github.io/en/posts/982/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/982/</guid><description>Theorem Let&amp;rsquo;s call $f$ a periodic function with $2L$. Then, the value below remains constant regardless of the value of $a$. $$ \int_{a}^{a+2L}f(t)dt $$ Explanation By definition of periodic functions, this is obvious. From this fact, when integrating periodic functions, techniques such as changing the interval of integration can be applied. Furthermore, if you consider it in conjunction with the average value of a function, it means that the average</description></item><item><title>The Limit of Fourier Coefficients is Zero</title><link>https://freshrimpsushi.github.io/en/posts/985/</link><pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/985/</guid><description>Theorem The Fourier Coefficients $a_{n}, b_{n}$ and Complex Fourier Coefficients $c_{\pm n}$ are the limit $n \rightarrow \infty$ $$ \begin{align*} \lim \limits_{n \rightarrow \infty} a_{n} &amp;amp;= 0 \\ \lim \limits_{n \rightarrow \infty} b_{n} &amp;amp;= 0 \\ \lim \limits_{n \rightarrow \infty} c_{\pm n} &amp;amp;= 0 \end{align*} $$ Proof By the Bessel&amp;rsquo;s Inequality, we know that the sum of the Fourier coefficients converges. $$ \dfrac{1}{4}|a_{0}|^2 +\dfrac{1}{2}\sum\limits_{n=1}^{\infty} \left(|a_{n}|^2 + |b_{n}|^2 \right) =\sum \limits_{-\infty}^{\infty}</description></item><item><title>Continuity in Every Piece, Smoothness in Every Segment</title><link>https://freshrimpsushi.github.io/en/posts/972/</link><pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/972/</guid><description>Definition A function $f$ is said to be piecewise continuous on an interval $I$ if it satisfies the conditions below: It has a finite number of discontinuities $x_{1},\ x_{2},\ \cdots ,\ x_{n} \in I$. At each point of discontinuity, it has both a left-hand limit and a right-hand limit. $$ \left|\lim \limits_{x\rightarrow x_{i}^{+}} f(x) \right| &amp;lt; \infty \quad \text{and} \quad \left|\lim_{x \rightarrow x_{i}^{-}}f(x)\right|&amp;lt;\infty \quad (i=1,\ \cdots ,\ n) $$ If</description></item><item><title>Convergence of Fourier Series at Discontinuities</title><link>https://freshrimpsushi.github.io/en/posts/973/</link><pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/973/</guid><description>Theorem1 Let&amp;rsquo;s say the function $f(t)$, defined in the interval $[-L,\ L)$, is piecewise continuous. Denoting the points of discontinuity as $t_{i}\ (i=1,\ \cdots m )$ and assuming at each point of discontinuity, there exist left-hand derivative $f(a-)$ and right-hand derivative $f(a+)$. Then, the Fourier series of $f(t)$ converges to the midpoint of the left-hand and right-hand limits at the point of discontinuity $t_{i}$. $$ \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{\infty}\left( a_{n} \cos \dfrac{n</description></item><item><title>Wronskian of Two Solutions of a Second Order Linear Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/965/</link><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/965/</guid><description>Theorem 1 Suppose $y_{1}$ and $y_{2}$ are solutions to the second-order linear differential equation $y^{\prime \prime}+p(t)y^{\prime}+q(t)y=0$. Then, The Wronskian of $y_{1}$ and $y_{2}$ is expressed in the form of an exponential function. $$ W [y_{1}, y_{2}] (t)=c e^{-\int p(t) dt} $$ Where $c$ is a constant that depends on $y_{1},\ y_{2}$. $W[y_{1},y_{2}] (t)$ is either always $0$ or never $0$ at all points. Explanation Also known as Abel&amp;rsquo;s theorem. Although</description></item><item><title>Electromotive Force and Kinetic Electromotive Force</title><link>https://freshrimpsushi.github.io/en/posts/963/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/963/</guid><description>Electromotive Force1 Let us denote the force that moves charges and generates current in a circuit as $\mathbf{f}$. This $\mathbf{f}$ can be divided into two types. One is the force of the circuit&amp;rsquo;s power source, $\mathbf{f}_{s}$, and the other is the electric force, $\mathbf{E}$, created by the charges accumulated in some part of the circuit. Here, the subscript $s$ stands for source. Therefore, $$ \mathbf{f}=\mathbf{f}_{s}+\mathbf{E} $$ The force of the</description></item><item><title>Partial Integration of Expressions Containing the Del Operator</title><link>https://freshrimpsushi.github.io/en/posts/959/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/959/</guid><description>Formulas The following expressions hold true for vector integration involving the del operator. (a) $$ \int_{\mathcal{V}}\mathbf{A} \cdot (\nabla f)d\tau = \oint_{\mathcal{S}}f\mathbf{A} \cdot d \mathbf{a}-\int_{\mathcal{V}}f(\nabla \cdot \mathbf{A})d\tau $$ (b) $$ \int_{\mathcal{S}} f \left( \nabla \times \mathbf{A} \right)\mathbf{A} \cdot d \mathbf{a} = \int_{\mathcal{S}} \left[ \mathbf{A} \times \left( \nabla f \right) \right] \cdot d\mathbf{a} + \oint_{\mathcal{P}} f\mathbf{A} \cdot d\mathbf{l} $$ (c) $$ \int_{\mathcal{V}} \mathbf{B} \cdot \left( \nabla \times \mathbf{A} \right) d\tau = \int_{\mathcal{V}}</description></item><item><title>Work and Energy in Electrostatics</title><link>https://freshrimpsushi.github.io/en/posts/961/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/961/</guid><description>Moving a Charge1 The following equation holds between potential and electric field. $$ -\int_\mathbf{a} ^\mathbf{b} \mathbf{E} \cdot d\mathbf{l} = \int_\mathbf{a} ^ \mathbf{b} \left( \nabla V \right) \cdot d\mathbf{l} = V(\mathbf{b}) - V(\mathbf{a}) $$ Thus, if there is a fixed source charge distribution, and we move a test charge $Q$ from point $\mathbf{a}$ to point $\mathbf{b}$, the work done is calculated as follows. $$ W=\int_{\mathbf{a}}^\mathbf{b} \mathbf{F} \cdot d\mathbf{l} = -Q\int_\mathbf{a}^\mathbf{b} \mathbf{E}</description></item><item><title>Chebyshev Differential Equations and Chebyshev Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/956/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/956/</guid><description>Definition The following differential equation is referred to as the Chebyshev Differential Equation. $$ \begin{equation} (1-x^2)\dfrac{d^2 y}{dx^2} -x\dfrac{dy}{dx}+n^2 y=0 \label{def1} \end{equation} $$ The solution to the Chebyshev differential equation is known as Chebyshev polynomials, commonly denoted by $T_{n}(x)$. The general term of $T_{n}(x)$ is as follows: When $n$ is even $$ 1-\dfrac{\lambda^2}{2!}x^2+\dfrac{\lambda^2(\lambda^2-2^2)}{4!}x^4+\sum \limits_{m=3}^\infty (-1)^m \dfrac{\lambda^2(\lambda^2-2^2)\cdots(\lambda^2-(2m-2)^2)}{(2m)!} x^{2m} $$ When $n$ is odd $$ x-\dfrac{\lambda^2-1^2}{3!}x^3+\dfrac{(\lambda^2-1^2)(\lambda^2-3^2)}{5!}x^5+\sum \limits_{m=3}^\infty (-1)^m\dfrac{(\lambda^2-1^2)(\lambda^2-3^2) \cdots (\lambda^2-(2m-1)^2)}{(2m+1)!} x^{2m+1} $$ Especially,</description></item><item><title>Series Solution of Chebyshev Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/955/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/955/</guid><description>Definition The following differential equation is referred to as the Chebyshev Differential Equation: $$ (1-x^2)\dfrac{d^2 y}{dx^2} -x\dfrac{dy}{dx}+n^2 y=0 $$ Description It&amp;rsquo;s a form that includes the independent variable $x$ in the coefficient, and assuming that the solution is in the form of a power series, it can be solved. The solution to the Chebyshev equation is called the Chebyshev polynomial, often denoted as $T_{n}(x)$. Solution $$ \begin{equation} (1-x^2)y^{\prime \prime} -xy^{\prime}+\lambda^2</description></item><item><title>Bound Current Density and the Vector Magnetic Field Created by a Magnetized Object</title><link>https://freshrimpsushi.github.io/en/posts/954/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/954/</guid><description>Explanation1 Suppose there is an object magnetized by an external magnetic field. This object will have a magnetization density $\mathbf{M}$, and this magnetization density will generate a new magnetic field. The vector potential created by a magnetic dipole is as follows. $$ \mathbf{A} (\mathbf{r}) = \dfrac{\mu_{0}}{4\pi}\dfrac{\mathbf{m} \times \crH }{\cR ^2} $$ Since magnetization density is the dipole moment per unit volume, $\mathbf{M}=\dfrac{\mathbf{m}}{d\tau}$. By substituting this into the above equation and</description></item><item><title>Changes in Electron Orbits Due to External Magnetic Fields and Diamagnetism</title><link>https://freshrimpsushi.github.io/en/posts/953/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/953/</guid><description>Explanation1 Let&amp;rsquo;s imagine an electron orbiting around the nucleus with a radius of $R$. Although the moving point charge does not become a steady current, it appears to do so because of its high velocity. The period is calculated by dividing the distance traveled by the speed, thus $$ T=\dfrac{2\pi R}{v} $$ The current $I$ is the amount of charge passing per unit time, and since the electron passes through</description></item><item><title>Self-Density and Ferromagnets</title><link>https://freshrimpsushi.github.io/en/posts/951/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/951/</guid><description>Description1 Let&amp;rsquo;s assume there is an object that appears to be non-magnetic at first glance. If we look into this object down to the atomic level, we find that tiny currents are created by electrons orbiting around the nucleus, which result in magnetic phenomena. This means that very small magnetic dipoles are generated in each atom. However, since the directions of the atoms are all different, summing all these dipole</description></item><item><title>The Torque on a Magnetic Dipole in an External Magnetic Field and Paramagnetism</title><link>https://freshrimpsushi.github.io/en/posts/952/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/952/</guid><description>Explanation1 Just like an electric dipole gains torque by an external electric field, a magnetic dipole does the same. Let&amp;rsquo;s assume there is a current loop in a uniform external magnetic field $\mathbf{B}=B\hat{\mathbf{z}}$ as shown below. Since a small rectangular current loop can be overlapped to approximate a current loop of any shape, let&amp;rsquo;s focus on the rectangular current loop. The magnetic force received by each side can be calculated</description></item><item><title>Probabilistic interpretation and normalization of the wave function in quantum mechanics</title><link>https://freshrimpsushi.github.io/en/posts/945/</link><pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/945/</guid><description>Wave Function The Wave Function is a function in quantum mechanics that represents the state of motion of a particle with respect to time and position. In a sushi restaurant, the wave function with respect to position and time is denoted as $\psi (x,t)$, and the wave function with respect to position and independent of time is denoted as $u(x)$. Probabilistic Interpretation The method of understanding the state of a</description></item><item><title>Alignment of Polar Molecules by a Constant External Electric Field</title><link>https://freshrimpsushi.github.io/en/posts/817/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/817/</guid><description>Overview 1 When an electrically neutral atom is placed in an external electric field, it becomes polarized and acquires a dipole moment $\mathbf{p}$. However, some molecules have a dipole moment even without the influence of an external electric field. Such molecules are referred to as polar molecules. Polar Molecules An example of a polar molecule is a water molecule. Water molecules are bent as shown in $105^{\circ}$, resulting in a</description></item><item><title>Alignment of Polar Molecules by a Non-uniform Electric Field</title><link>https://freshrimpsushi.github.io/en/posts/818/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/818/</guid><description>Explanation 1 Polar molecules possess a dipole moment even in the absence of an external electric field. If there is a constant external electric field, the dipole moment aligns with the direction of the electric field. However, if the external electric field is not constant, $\mathbf{F}_+$ and $\mathbf{F}_-$ are not the same, resulting in a net force as well as a torque. The net force can be calculated as follows.</description></item><item><title>Ampère's Law and Applications</title><link>https://freshrimpsushi.github.io/en/posts/922/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/922/</guid><description>Formulas The magnetic field $\mathbf{B}$ that arises from the volume current density $\mathbf{J}$ rotates in the direction that satisfies the right-hand rule with the direction of $\mathbf{J}$ as the axis. $$ \nabla \times \mathbf{B}=\mu_{0} \mathbf{J} $$ Explanation1 Ampère&amp;rsquo;s law indicates a special relationship between the current flowing through a conductor and the magnetic field around it. A</description></item><item><title>Current and Current Density</title><link>https://freshrimpsushi.github.io/en/posts/898/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/898/</guid><description>Definition1 The electric current is defined as the amount of charge that passes through a given point in a conductor per unit of time, denoted by $I$. Thus, a negative charge moving to the left and a positive charge moving to the right constitute an electric current of the same sign. The amount of Coulomb passing per unit of time is called ampere. $$ 1 [A] = 1 [C/s] $$</description></item><item><title>Divergence (Divergence) and Curl of Magnetic Fields</title><link>https://freshrimpsushi.github.io/en/posts/920/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/920/</guid><description>Theorem The divergence and curl of a magnetic field are as follows: $$ \begin{align*} \nabla \cdot \mathbf{B} =&amp;amp;\ 0 \\ \nabla \times \mathbf{B} =&amp;amp;\ \mu_{0} \mathbf{J} \end{align*} $$ Description Just like the electric field was always a special vector function with a curl of $\mathbf{0}$, so is the magnetic field. Let&amp;rsquo;s calculate the divergence and curl using the law of Biot-Savart for volume currents. $$ \mathbf{B} (\mathbf{r}) = \dfrac{\mu_{0}}{4 \pi}</description></item><item><title>Electric Field Created by a Dipole</title><link>https://freshrimpsushi.github.io/en/posts/153/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/153/</guid><description>Explanation1 The potential due to an electric dipole $\mathbf{p}$ is as follows. $$ V_{\text{dip}}(\mathbf{r}) = \dfrac{1}{4\pi\epsilon_{0}}\dfrac{\mathbf{p}\cdot\hat{\mathbf{r}}}{r^2} = \dfrac{1}{4\pi\epsilon_{0}}\dfrac{p\cos\theta}{r^{2}} $$ Now, let&amp;rsquo;s assume that $\mathbf{p}$ is at the origin and parallel to the $z$ axis, as shown in the figure above. Since the electric field is the gradient of the potential, in spherical coordinates it is as follows. $$ \mathbf{E} = - \nabla V = -\left( \dfrac{\partial V}{\partial r}\hat{\mathbf{r}} + \frac{1}{r}\dfrac{\partial</description></item><item><title>Electric Field Created by Bound Charges and Polarized Objects</title><link>https://freshrimpsushi.github.io/en/posts/880/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/880/</guid><description>Bound Charges External electric fields cause the dipoles in a material to align in one direction, polarizing the material and giving it a dipole moment $\mathbf{p}$. The electric field produced by these dipole moments is calculated as follows. The potential created by the dipole moment $\mathbf{p}$ is as follows. $$ \begin{equation} V(\mathbf{r}) = \dfrac{1}{4 \pi \epsilon_{0}} \dfrac{ \mathbf{p} \cdot \crH } {\cR ^2} \label{1} \end{equation} $$ $\mathbf{r}^{\prime}$ is the position</description></item><item><title>Magnetic Fields Produced by Magnetic Dipoles</title><link>https://freshrimpsushi.github.io/en/posts/156/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/156/</guid><description>Description The vector potential due to a magnetic dipole $\mathbf{m}$ is as given in magnetic dipole moment. $$ \mathbf{A}_{\text{dip}}(\mathbf{r}) = \dfrac{\mu_{0}}{4 \pi} \dfrac{\mathbf{m} \times \hat{\mathbf{r}}}{r^2} = \dfrac{\mu_{0}}{4 \pi} \dfrac{m\sin\theta}{r^{2}} \hat{\boldsymbol{\phi}} $$ Now, let $\mathbf{m}$ be located at the origin and parallel to the $z$ axis, as shown in the figure above. Since the magnetic field is the curl of the vector potential, in spherical coordinates, it is as follows. $$</description></item><item><title>Magnetic Forces Do Not Work</title><link>https://freshrimpsushi.github.io/en/posts/897/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/897/</guid><description>Theorem1 Magnetic forces do not do work. Explanation In situations with magnetic forces, when particles or objects move, it might seem as though the magnetic forces are doing work. However, this is not the case. Proof Work is the product of force and displacement. $$ W=\int \mathbf{F} \cdot d\mathbf{l} $$ The work done by the magnetic force is $$ W_{\text{mag}}=\int \mathbf{F}_{\text{mag}} \cdot d\mathbf{l} = \int Q(\mathbf{v}\times \mathbf{B})\cdot d\mathbf{l} $$ Given</description></item><item><title>Magnetic Vector Potential</title><link>https://freshrimpsushi.github.io/en/posts/923/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/923/</guid><description>Explanation1 In electrostatics, the electric field is easily handled by using a property called $\nabla \times \mathbf{E} = \mathbf{0}$ to define the scalar potential $V$. Similarly, in magnetostatics, the vector potential $A$ is defined and used by utilizing a property called $\nabla \cdot \mathbf{B} = 0$. Let&amp;rsquo;s say the magnetic field $\mathbf{B}$ is the curl of some vector $\mathbf{A}$. $$ \mathbf{B}=\nabla \times \mathbf{A} $$ Since the divergence of a curl</description></item><item><title>Magnetism and the Law of Lorentz Force</title><link>https://freshrimpsushi.github.io/en/posts/896/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/896/</guid><description>Definition1 A moving charge (current) creates a magnetic field $\mathbf{B}$ around it. The force experienced by a charge $Q$ moving at velocity $\mathbf{v}$ in a magnetic field $\mathbf{B}$ is given by: $$ \begin{equation} \mathbf{F}_{m}=Q(\mathbf{v} \times \mathbf{B}) \end{equation} $$ This force is called the magnetic force, and the above formula is known as the Lorentz force law. Explanation As with the definition of an electric field, when a moving charge experiences</description></item><item><title>Multipole Expansion of Vector Potentials and Magnetic Dipole Moments</title><link>https://freshrimpsushi.github.io/en/posts/924/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/924/</guid><description>Multipole Expansion of Vector Potentials1 The multipole expansion of vector potentials refers to the expression of vector potentials as a power series approximation to $\dfrac{1}{r^{n}}$ when currents are concentrated, sufficiently far from the source. Initially, the vector potential due to a current loop is as follows. $$ \mathbf{A}(\mathbf{r})=\dfrac{\mu_{0} I}{4\pi}\oint \dfrac{1}{\cR}d\mathbf{l}^{\prime} $$ Under the conditions shown in the figure, the following equation holds. $$ \dfrac{1}{\cR} =\dfrac{1}{\sqrt{r^2+(r^{\prime})^2-2rr^{\prime}\cos\alpha}} = \dfrac{1}{r}\sum \limits_{n=0}^{\infty} \left( \dfrac{r^{\prime}}{r}</description></item><item><title>Polarization density and genomes</title><link>https://freshrimpsushi.github.io/en/posts/819/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/819/</guid><description>Overview1 2 Conductors contain a high concentration of free charges. This means that many electrons are not bound to any particular nucleus and freely roam the interior of the conductor. Conversely, in dielectrics or insulators, the situation is different. All electrons are bound to specific atoms (molecules). While they can move slightly within the molecule, they cannot move freely like free charges. An example of this slight movement is polarization.</description></item><item><title>Steady Current and Biot-Savart Law</title><link>https://freshrimpsushi.github.io/en/posts/899/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/899/</guid><description>Definition1 A steady current refers to the flow of charge that continues without changing in amount or direction. Description Since the current does not change over time, the magnetic field created by the steady current also does not change over time. The &amp;lsquo;direction of progress&amp;rsquo; mentioned here is a different concept from the direction of a vector we commonly think of. It means that as long as the flow continues</description></item><item><title>Stokes' Theorem</title><link>https://freshrimpsushi.github.io/en/posts/937/</link><pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/937/</guid><description>Theorem1 Let&amp;rsquo;s call something a vector and an area in 3D space as $\mathbf{v}, \mathcal{S}$, respectively. The area vector of $\mathcal{S}$ is denoted as $d\mathbf{a}$, the border of $\mathcal{S}$ as $\mathcal{P}$, and the path moving along $\mathcal{P}$ as $d\mathbf{l}$. Then, the following equation holds: $$ \int_{\mathcal{S}} (\nabla \times \mathbf{v} )\cdot d\mathbf{a} = \oint_{\mathcal{P}} \mathbf{v} \cdot d\mathbf{l} $$ This is called Stokes&amp;rsquo; theorem or the fundamental theorem for curl. Incidentally, outside</description></item><item><title>Multipole Expansion of Potential and Dipole Moments</title><link>https://freshrimpsushi.github.io/en/posts/936/</link><pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/936/</guid><description>Multiple Expansion When a distribution of charges is viewed from sufficiently far away, it appears almost as though it were a point charge. In other words, if the total charge of the charge distribution is $Q$, it would feel as if there&amp;rsquo;s a single point charge with charge $Q$ when viewed from afar. This means that the potential can be approximated as $\dfrac{1}{4\pi\epsilon_{0}} \dfrac{Q}{r}$. But if the total charge is</description></item><item><title>Proof of the Second Cosine Law Using the Definition of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/935/</link><pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/935/</guid><description>Formula For the triangle given above, the following equations hold true, and they are collectively known as the law of cosines. $$ \begin{cases} a^{2} =b^{2}+c^{2}-2bc\cos\alpha \\ b^{2}=a^{2}+c^{2}-2ac\cos\beta \\ c^{2}=a^{2}+b^{2}-2ab\cos\gamma \end{cases} $$ Proof From the triangle in the upper left corner of the diagram, we obtain the following equation. $$ \begin{align} a &amp;amp;= \overline{BH_{a}}+\overline{H_{a}C} \nonumber \\ &amp;amp;= c\cos\beta + b\cos\gamma \label{eq1} \end{align} $$ Multiplying both sides by $a$ yields: $$ a^{2}=ac\cos\beta</description></item><item><title>리만적분가능한 함수의 푸리에 급수는 수렴한다</title><link>https://freshrimpsushi.github.io/en/posts/934/</link><pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/934/</guid><description>Theorem1 Let the function $f$ be Riemann integrable on the interval $[-L,\ L)$. Then for a point $t$ at which the function is continuous, the Fourier series $\lim \limits_{N \to \infty }S^{f}_{N}(t)$ of $f$ converges to $f(t)$. $$ \lim \limits_{N \rightarrow \infty} S^{f}_{N}(t)=f(t) $$ In this case $$ \begin{align*} S^{f}_{N}(t)&amp;amp;=\dfrac{a_{0}}{2}+\sum \limits_{n=1}^{N} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t} {L} \right) \\ a_{0} &amp;amp;=\dfrac{1}{L}\int_{-L}^{L}f(t)dt \\ a_{n} &amp;amp;= \dfrac{1}{L}\int_{-L}^{L} f(t)\cos\dfrac{n\pi t}{L}</description></item><item><title>Derivation of Fourier Series</title><link>https://freshrimpsushi.github.io/en/posts/929/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/929/</guid><description>Definition The series for $2L$-periodic function $f$ is defined as the Fourier series of $f$ as follows: $$ \begin{align*} \lim \limits_{N \rightarrow \infty} S^{f}_{N}(t) &amp;amp;= \lim \limits_{N \to \infty}\left[ \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{N} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t}{L} \right) \right] \\ &amp;amp;= \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{\infty} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t}{L} \right) \end{align*} $$ Here, each coefficient $a_{0}, a_{n}, b_{n}$ is called the Fourier coefficient, and its value</description></item><item><title>Dirichlet Kernel</title><link>https://freshrimpsushi.github.io/en/posts/932/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/932/</guid><description>Definition Dirichlet Kernel $D_{n}$ is defined as follows. $$ \begin{equation} D_{n}(t) := \dfrac{1}{2}+\sum \limits_{k=1}^{n} \cos kt \end{equation} $$ Explanation The Dirichlet kernel is related to delta functions, exponential functions, etc., and appears in Fourier analysis. Here are some related theorems and their proofs. Theorem 1 The Dirichlet Kernel satisfies the following equation. $$ D_{n}(t)=\dfrac{\sin\left(n+\frac{1}{2}\right) t}{2\sin \frac{1}{2}t} $$ Proof If we express the cosine function as a complex exponential form, we</description></item><item><title>Legendre Polynomials are orthogonal to any lower degree polynomial</title><link>https://freshrimpsushi.github.io/en/posts/933/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/933/</guid><description>Theorem When $P_{l}(x)$ is a Legendre Polynomial and $f(x)$ is any polynomial of lower degree than $l$, then $P_{l}(x)$ and $f(x)$ are orthogonal to each other. $$ \int_{-1}^{1}P_{l}(x)f(x)dx = 0 $$ Explanation The following lemma is essentially equivalent to the proof of the theorem. Lemma Let $f(x)$ be any polynomial of degree $n$. $f(x)$ can be expressed as a linear combination of Legendre polynomials up to degree $l \le n$.</description></item><item><title>Orthogonal Functions and Orthogonal Sets</title><link>https://freshrimpsushi.github.io/en/posts/926/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/926/</guid><description>Definition Inner Product The inner product of two functions $f$ and $g$ defined on the interval $[a,b]$ is defined as follows. $$ \braket{f , g} := \int_{a}^b f(x) g(x) dx $$ When $f, g$ is a complex function, then, $$ \braket{f, g} := \int_{a}^{b} f(x) \overline{g(x)} dx $$ In this case, $\overline{z}$ is the conjugate complex of $z$. Orthogonal Functions Two complex functions $f$, $g$ are said to be orthogonal</description></item><item><title>Orthogonality of Legendre Polynomials</title><link>https://freshrimpsushi.github.io/en/posts/931/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/931/</guid><description>Theorem In the interval $[-1,\ 1]$, Legendre polynomials form an orthogonal set. $$ \int_{-1}^{1} P_{l}(x)P_{m}(x) dx =\frac{2}{2l+1}\delta_{lm} \quad (l, m = 0, 1, 2, \dots) $$ Proof Case 1: $l \ne m$ Legendre Differential Equation The following differential equation is called the Legendre differential equation. $$ \dfrac{d}{d x}\left[ (1-x)^{2} \dfrac{d y}{d x} \right] +l(l+1)y = 0 $$ Since Legendre polynomials are solutions to the Legendre differential equation, they satisfy the</description></item><item><title>Proof of the Orthogonality of the Set of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/928/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/928/</guid><description>Theorem The set $\left\{ 1,\ \cos \dfrac{\pi x}{L},\ \cos \dfrac{2\pi x}{L}, \cdots ,\ \sin\dfrac{\pi x}{L},\ \sin\dfrac{2\pi x}{L},\ \cdots \right\}$ of functions $2L$ that are periodic functions is an orthogonal set in the interval $[-L,\ L)$. In other words, for $m,n = 1, 2, 3, \dots$, the following holds. $$ \begin{align} \dfrac{1}{L} \int _{-L}^{L} \cos\dfrac{m\pi x}{L} \cos\dfrac{n\pi x}{L} dx &amp;amp;= \delta_{mn} \label{eq1} \\ \dfrac{1}{L} \int _{-L}^{L} \sin \dfrac{m\pi x}{L}\sin \dfrac{n\pi x}{L}</description></item><item><title>Sum of Trigonometric Functions Orthogonal to Each Other</title><link>https://freshrimpsushi.github.io/en/posts/930/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/930/</guid><description>Formula Define $C_{n}$ and $S_{n}$ as follows. $$ \begin{align*} C_{n}: &amp;amp;= 1+\cos x + \cos 2x + \cdots +\cos nx \\ S_{n}: &amp;amp;= \sin x +\sin 2x + \cdots + \sin nx \end{align*} $$ Then, the following equation holds. $$ \begin{align*} C_{n} &amp;amp;= \dfrac{\sin \dfrac{n+1}{2}x}{\sin \dfrac{1}{2}x} \cos \dfrac{n}{2}x \\ S_{n} &amp;amp;= \dfrac{\sin \dfrac{n+1}{2}x}{\sin \dfrac{1}{2}x}\sin \dfrac{n}{2}x \end{align*} $$ Proof Use the Euler&amp;rsquo;s formula. $$ \begin{align*} &amp;amp; C_{n}+ i S_{n} \\ =&amp;amp;\</description></item><item><title>Rotation of Separation Vector</title><link>https://freshrimpsushi.github.io/en/posts/919/</link><pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/919/</guid><description>Equation $$ \nabla \times \dfrac{\crH }{\cR ^2} = \mathbf{0} $$ Explanation There is nothing particularly special about this formula. It emerges in the process of calculating the divergence of a magnetic field, and its calculation is not straightforward, hence the separate explanation. Proof If we refer to $\bcR=(x-x^{\prime})\hat{\mathbf{x}} + (y-y^{\prime})\hat{\mathbf{y}} + (z-z^{\prime})\hat{\mathbf{z}}$ as a component vector, it can be represented as follows: $$ | \bcR |=\cR=\sqrt{(x-x^{\prime})^2+(y-y^{\prime})^2 + (z-z^{\prime})^2} $$ $$</description></item><item><title>Proof that the Length of an Arc and the Length of a Chord are Approximately Equal When the Central Angle is Small</title><link>https://freshrimpsushi.github.io/en/posts/913/</link><pubDate>Wed, 20 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/913/</guid><description>Theorem When the central angle $\theta$ is sufficiently small, the length of the chord and the length of the arc approximate each other. When $\theta \rightarrow 0$, $$\overline{AB} \approx \stackrel\frown{AB}$$ Proof In the figure above, the length of the chord is $$\overline{AB} =2\overline{AM}=2r\sin \frac{\theta}{2}$$ The length of the arc with a central angle of $\theta$ and the radius length of $r$ is $$\stackrel\frown{AB}=r\theta$$ When the angle is sufficiently small, saying</description></item><item><title>Rodrigues Formula for Legendre Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/895/</link><pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/895/</guid><description>Formula The explicit formula for the Legendre polynomials is as follows. $$ P_{l}(x)=\dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \tag{1} $$ Description This formula is used to obtain the $l$th Legendre polynomial, known as the Rodrigues&amp;rsquo; formula. Originally, it referred to the explicit form of the Legendre polynomials, but later it became a universal name for formulas representing the explicit form of special functions expressed as polynomials. Derivation The Legendre polynomial $P_{l}$ refers to</description></item><item><title>Series Solution of Legendre Differential Equation: Legendre Polynomial</title><link>https://freshrimpsushi.github.io/en/posts/889/</link><pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/889/</guid><description>Definition1 The following differential equation is called the Legendre differential equation. $$ (1-x^2)\dfrac{d^2 y}{dx^2} -2x\dfrac{dy}{dx}+l(l+1) y=0 $$ The solution to the Legendre differential equation is called the Legendre polynomial, commonly denoted as $P_{l}(x)$. The first few Legendre polynomials according to $l$ are as follows. $$ \begin{align*} P_{0}(x) =&amp;amp;\ 1 \\ P_{1}(x) =&amp;amp;\ x \\ P_2(x) =&amp;amp;\ \dfrac{1}{2}(3x^2-1) \\ P_{3}(x) =&amp;amp;\ \dfrac{1}{2}(5x^3-3x) \\ P_{4}(x) =&amp;amp;\ \dfrac{1}{8}(35x^4-30x^2+3) \\ P_{5}(x) =&amp;amp;\ \dfrac{1}{8}(63x^5-70x^3+15x) \\</description></item><item><title>Proof of Leibniz's Theorem</title><link>https://freshrimpsushi.github.io/en/posts/884/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/884/</guid><description>Theorem $$ \dfrac{d}{dx} (fg)=\dfrac{df}{dx}g+f\dfrac{dg}{dx} $$ $$ \begin{align*} \dfrac{d^n}{dx^n}(fg)&amp;amp;=\sum \limits_{k=0}^{n}\frac{n!}{(n-k)!k!}\dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n}{}_{n}\mathrm{C}_{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n} \binom{n}{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \end{align*} $$ Description Also known as Leibniz&amp;rsquo;s rule. The first equation is a well-known formula, often referred to as the product rule or the rule of product for differentiation. It simply expresses the result when the product of two functions is differentiated once. More generally, the equation below represents</description></item><item><title>Series, Infinite Series</title><link>https://freshrimpsushi.github.io/en/posts/886/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/886/</guid><description>Definition1 Let&amp;rsquo;s assume a sequence $\left\{ a_{n} \right\}$ is given. Then, let&amp;rsquo;s define the following notation. $$ \sum \limits_{n=p}^{q} a_{n} = a_{p} + a_{p+1} + \cdots + a_{q}\quad (p \le q) $$ Define the partial sum $s_{n}$ of $\left\{ a_{n} \right\}$ as follows. $$ s_{n} = \sum \limits_{k=1}^{n} a_{k} $$ Then, we can think of a sequence $\left\{ s_{n} \right\}$ of these $s_{n}$. The limit of sequence $\left\{ s_{n} \right\}$</description></item><item><title>Equations Involving the Laplacian Operator, Second Order Partial Derivatives</title><link>https://freshrimpsushi.github.io/en/posts/876/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/876/</guid><description>Explanation Let&amp;rsquo;s call $T$ a scalar function and $\mathbf{A}$ a vector function. Divergence of the Gradient: $\nabla \cdot (\nabla T) = \dfrac{\partial^{2} T}{\partial x^{2}} + \dfrac{\partial ^{2} T} {\partial y^{2}} + \dfrac{\partial ^{2} T}{\partial z^{2}}$ Curl of the Gradient: $\nabla \times (\nabla T)= \mathbf{0}$ Gradient of the Divergence: $\nabla (\nabla \cdot \mathbf{A} )$ Divergence of the Curl: $\nabla \cdot (\nabla \times \mathbf{A})=0$ Curl of the Curl: $\nabla \times (\nabla \times</description></item><item><title>Fourier Series and Bessel's Inequality</title><link>https://freshrimpsushi.github.io/en/posts/3041/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3041/</guid><description>Formulas If a function $f$ defined on the interval $[-L,L)$ is Riemann integrable, the following inequality holds, known as the Bessel&amp;rsquo;s inequality. $$ \dfrac{1}{4}|a_{0}|^{2} +\dfrac{1}{2}\sum\limits_{n=1}^{\infty} \left(|a_{n}|^{2} + |b_{n}|^{2} \right) =\sum \limits_{n=-\infty}^{\infty} | c_{n} |^{2} \le \dfrac{1}{2L}\int_{-L}^{L} | f(t)|^{2} dt $$ Here, $a_{0},\ a_{n},\ b_{n}$ is the Fourier coefficient of $f$, and $c_{n}$ is the complex Fourier coefficient of $f$. Proof For any complex number $z$, since $|z|^{2}= z \overline{z}$, $$</description></item><item><title>Proof that Differentiating the Heaviside Step Function Yields the Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/878/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/878/</guid><description>Theorem The derivative of the Heaviside step function is the Dirac delta function. $$ \dfrac{dH}{dx}=\delta (x) $$ Here, $H=H(x)$ refers to the Heaviside step function or unit step function $$ H(x)=\begin{cases} 1 &amp;amp; x&amp;gt;0 \\ 0 &amp;amp; x \le 0 \end{cases} $$ Dirac Delta Function A function that satisfies the following two conditions is called the Dirac delta function. $$ \begin{equation} \delta (x) = \begin{cases} 0, &amp;amp; x\neq 0 \\</description></item><item><title>Solving the Laplace Equation Independent of the Jet Axis in Cylindrical Coordinates Using the Method of Separation of Variables</title><link>https://freshrimpsushi.github.io/en/posts/873/</link><pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/873/</guid><description>Theorem The general solution of the Laplace&amp;rsquo;s equation with cylindrical symmetry in the cylindrical coordinate system is as follows. $$ V(s,\phi) = A_{0} \ln s +B_{0} +\sum \limits _{k=1} ^\infty ( A_{k} s^k ++ B_{k} s^{-k} )( C_{k}\cos k\phi + D_{k}\sin k\phi) $$ Proof Step 0 When boundary conditions are more easily represented in cylindrical coordinates, one has to solve the Laplace&amp;rsquo;s equation for cylindrical coordinates. The Laplace&amp;rsquo;s equation in</description></item><item><title>Solution of the Laplace Equation Independent of Azimuthal Angle in Spherical Coordinates using the Method of Separation of Variables</title><link>https://freshrimpsushi.github.io/en/posts/872/</link><pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/872/</guid><description>Theorem The general solution of the Laplace&amp;rsquo;s equation with azimuthal symmetry in spherical coordinates is as follows: $$ V(r,\theta) = \sum \limits_{l=0} ^\infty \left( A_{l} r^l + \dfrac{B_{l}}{r^{l+1} } \right) P_{l}(\cos \theta) $$ Proof Step 0 When finding the potential in cases where the boundary condition is easily expressed in spherical coordinates, one must solve the Laplace&amp;rsquo;s equation for spherical coordinates. The Laplace&amp;rsquo;s equation in spherical coordinates is as follows.</description></item><item><title>Solving Nonhomogeneous Euler Differential Equations Using Substitution</title><link>https://freshrimpsushi.github.io/en/posts/871/</link><pubDate>Wed, 30 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/871/</guid><description>Definition The differential equation given as follows is called the Euler differential equation. $$ \begin{align} &amp;amp;&amp;amp; a_2 x^2 \dfrac{d^2 y}{d x^2} + a_{1} x \dfrac{dy}{dx} + a_{0} y &amp;amp;= f(x) \label{eq1} \\ \mathrm{or}&amp;amp;&amp;amp; a_2 x^2 y^{\prime \prime} + a_{1} x y^{\prime} +a_{0} y &amp;amp;= f(x) \nonumber \\ \mathrm{or}&amp;amp;&amp;amp; x^2 y^{\prime \prime} + \alpha x y^{\prime} + \beta y &amp;amp;= f(x) \nonumber \end{align} $$ Explanation It is also referred to as</description></item><item><title>Integrability is Preserved in the Multiplication of Two Functions</title><link>https://freshrimpsushi.github.io/en/posts/854/</link><pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/854/</guid><description>Theorem1 If two functions $f$ and $g$ are Riemann(-Stieltjes) integrable over the interval $[a,b]$, then $fg$ is also integrable. Proof Assume that $f, g$ is integrable. Since integration is linear, $-g,\ f+g,\ f-g$ is also integrable. Let the function $\phi$ be defined as $\phi (x)=x^2$. Then $\phi$ is continuous over the entire domain. Since integrability is preserved under the composition with continuous functions, $\phi (f+g),\ \phi (f-g)$ is also integrable.</description></item><item><title>Integrability is Preserved in the Composition with Continuous Functions</title><link>https://freshrimpsushi.github.io/en/posts/853/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/853/</guid><description>This document is based on the Riemann-Stieltjes Integral. If we set it as $\alpha=\alpha (x)=x$, it equals the Riemann Integral. Theorem1 Suppose that the function $f$ is Riemann(-Stieltjes) integrable on the interval $[a,b]$ and let $m \le f \le M$. Let $\phi$ be a function that is continuous on the interval $[m,M]$. Let the function $h$ be defined as $h=\phi \circ f$. Then, $h$ is Riemann(-Stieltjes) integrable on the interval</description></item><item><title>Monotone Functions are Riemann-Stieltjes Integrable</title><link>https://freshrimpsushi.github.io/en/posts/849/</link><pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/849/</guid><description>About Riemann Integration Let&amp;rsquo;s suppose that the function $f$ is monotonic over $[a,b]$. Then $f$ is Riemann integrable. Proof Assume that $f$ is a monotonically increasing function1. Let $\epsilon &amp;gt;0$ be given. Consider a partition $P= \left\{ x_{i} : a=x_{0} &amp;lt; x_{1} &amp;lt; x_{2} &amp;lt; \cdots &amp;lt; x_{n}=b \right\}$ of the interval $[a,b]$ that satifies the following for any natural number $n$: $$ \Delta x_{i} = x_{i}-x_{i-1} = \dfrac{b-a}{n},\quad (i=1,2,\dots,n)</description></item><item><title>Continuous Functions are Riemann-Stieltjes Integrable</title><link>https://freshrimpsushi.github.io/en/posts/847/</link><pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/847/</guid><description>= This article is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem If function $f$ is continuous on $[a,b]$, then it is Riemann(-Stieltjes) integrable on $[a,b]$. Proof Suppose $\epsilon &amp;gt;0$ is given. And let&amp;rsquo;s say we chose $\eta&amp;gt;0$ that satisfies $\left[ \alpha (b) - \alpha (a) \right] \eta &amp;lt; \epsilon$. Since $[a,b]$ is compact as it is closed and</description></item><item><title>Properties of Potential</title><link>https://freshrimpsushi.github.io/en/posts/846/</link><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/846/</guid><description>Reference Point of Potential1 The definition of potential is as follows. $$ V(\mathbf{r} ) \equiv - \int _\mathcal{O} ^{\mathbf{r}} \mathbf{E} \cdot d \mathbf{l} $$ Therefore, depending on the reference point $\mathcal{O}$, its value can vary. For instance, if a new reference point $\mathcal{O}^{\prime}$ is chosen, there will be a difference in value by a certain constant $K$. $$ \begin{align*} V^{\prime} (\mathbf{r} ) =&amp;amp;\ -\int _{\mathcal{O}^{\prime}}^\mathbf{r} \mathbf{E} \cdot d\mathbf{l} \\ =&amp;amp;\</description></item><item><title>Potential</title><link>https://freshrimpsushi.github.io/en/posts/845/</link><pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/845/</guid><description>Explanation1 The electric field is a special vector function that always has a curl (rotation) of $\mathbf{0}$. From this characteristic, we introduce a scalar function called electric potential. The potential is denoted as $V$ and has the following relationship with the electric field $\mathbf{E}$. $$ \mathbf{E} = -\nabla V $$ Therefore, if we know the potential $V$, we can know the electric field $\mathbf{E}$. Since the potential is a scalar</description></item><item><title>Electric Field Curl</title><link>https://freshrimpsushi.github.io/en/posts/844/</link><pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/844/</guid><description>정리 Electric Field&amp;rsquo;s Curl is always $\mathbf{0}$. $$ \nabla \times \mathbf{E} = \mathbf{0} $$ Proof1 We will derive a general result from the special case where a point charge is located at the origin. The electric field due to a point charge at a distance of $r$ from the origin is as follows. $$ \mathbf{E}=\dfrac{1}{4 \pi \epsilon_{0} } \dfrac{q}{r^2} \hat{\mathbf{r}} $$ If we perform a path integral of the</description></item><item><title>Applications of Gauss's Law in Integral Form</title><link>https://freshrimpsushi.github.io/en/posts/640/</link><pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/640/</guid><description>Explanation1 Gauss&amp;rsquo;s Law 1(../635) allows us to calculate the electric field very easily, but not always. Although Gauss&amp;rsquo;s Law itself always holds, its formulaic advantage can only be utilized in certain situations. As will be explained below, to easily calculate the electric field through Gauss&amp;rsquo;s Law, the magnitude of the electric field $\mathbf{E}$ must be constant, and its direction perpendicular, over the surface created by a specific coordinate system. In</description></item><item><title>Divergence of the Electric Field</title><link>https://freshrimpsushi.github.io/en/posts/839/</link><pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/839/</guid><description>Formulas1 The divergence of the electric field $\mathbf{E}$ produced by a volume charge with volume charge density $\rho$ is as follows. $$ \nabla \cdot \mathbf{E} = \dfrac{1}{\epsilon_{0}} \rho ( \mathbf{r} ) $$ Description The divergence of the electric field is also referred to as the differential form of Gauss&amp;rsquo;s law. Integrating both sides yields the integral form of Gauss&amp;rsquo;s law. Proof Electric field produced by the volume charge $$ \mathbf{E}(\mathbf</description></item><item><title>Electric Flux and Gauss's Law</title><link>https://freshrimpsushi.github.io/en/posts/635/</link><pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/635/</guid><description>Definition1 The flux of an electric field $\mathbf{E}$ passing through a surface $\mathcal S$ is defined as follows. $$ \Phi_{E} \equiv \int_{\mathcal S} \mathbf{E} \cdot d\mathbf{a} $$ Let&amp;rsquo;s consider $\mathcal{S}$ as some closed surface. Let the total charge inside the closed surface be $Q_{\text{in}}$. Then, the following equation holds. $$ \oint_{\mathcal{S}} \mathbf{E} \cdot d\mathbf{a} = \frac{1}{\epsilon_{0}}Q_{\mathrm{in}} $$ This is known as Gauss&amp;rsquo;s law. Flux Flux, or flux density, refers to</description></item><item><title>Divergence of a Separation Vector</title><link>https://freshrimpsushi.github.io/en/posts/837/</link><pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/837/</guid><description>Formula $$ \begin{align*} \nabla \cdot \left( \dfrac{1}{r^2}\hat{ \mathbf{r} } \right) =&amp;amp;\ 4\pi \delta^3(\mathbf{r}) \\[1em] \nabla \cdot \left( \dfrac{1}{\cR^{2}} \crH \right) =&amp;amp;\ 4\pi \delta^3(\bcR) \\[1em] \nabla^2 \left(\dfrac{1}{\cR} \right) =&amp;amp;\ -4\pi \delta^3 ( \bcR ) \end{align*} $$ Here, $\mathbf{r}$ is the position vector, and $\bcR$ is the separation vector. Explanation Let&amp;rsquo;s assume there is a vector function $\mathbf{v} = \dfrac{1}{r^2}\hat{\mathbf{r}}$. Its magnitude is inversely proportional to the square of the distance, and</description></item><item><title>Coulomb's Law and Electric Fields</title><link>https://freshrimpsushi.github.io/en/posts/836/</link><pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/836/</guid><description>Coulomb&amp;rsquo;s Law1 The force exerted on a test charge $Q$ placed at a distance $\cR$ away from a fixed point charge $q$ is known as the Coulomb force, and its equation is as follows. $$ \mathbf{F} = \dfrac{1}{4\pi \epsilon_{0}} \dfrac{qQ}{\cR ^2} \crH $$ This is referred to as Coulomb&amp;rsquo;s Law. Description Coulomb’s Law is an empirical law derived from repeated experiments. Therefore,</description></item><item><title>The Fundamental Theorem of Slopes</title><link>https://freshrimpsushi.github.io/en/posts/835/</link><pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/835/</guid><description>Theorem Let&amp;rsquo;s say $T$ is a scalar function in three dimensions. Let&amp;rsquo;s consider $a, b$ as an arbitrary point in three-dimensional space. The total change in $T$ along any path from point $a$ to point $b$ is given by: $$ \begin{equation} T(b)-T(a) = \int _{a}^{b} (\nabla T) \cdot d\mathbf{l} \label{1} \end{equation} $$ This is called the fundamental theorem for gradients or gradient theorem. Note that at Live Shrimp Sushi Restaurant,</description></item><item><title>Necessary and Sufficient Conditions for Riemann(-Stieltjes) Integrability</title><link>https://freshrimpsushi.github.io/en/posts/833/</link><pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/833/</guid><description>This article is based on the Riemann-Stieltjes integral. If we set $\alpha=\alpha (x)=x$, it is the same as Riemann integral. Theorem1 A necessary and sufficient condition for a function $f$ to be Riemann(-Stieltjes) integrable on $[a,b]$ is that for every $\epsilon &amp;gt;0$, there exists a partition $P$ of $[a,b]$ that satisfies $U(P,f,\alpha) - L(P,f,\alpha) &amp;lt; \epsilon$. $$ \begin{equation} f \in \mathscr{R} (\alpha) \text{ on } [a,b] \\ \iff \forall\epsilon &amp;gt;0,</description></item><item><title>Segmentation</title><link>https://freshrimpsushi.github.io/en/posts/830/</link><pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/830/</guid><description>This post is based on the Riemann-Stieltjes integral. If we set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Definition If $P^{\ast}$ and $P$ are partitions of $[a,b]$ and satisfy $P \subseteq P^{\ast}$, then $P^{\ast}$ is called a refinement of $P$. Hence, every point in $P$ is a point in $P^{\ast}$. For any two partitions $P_{1}$ and $P_{2}$, $P_{3}=P_{1} \cup P_{2}$ is called the common refinement of</description></item><item><title>Upper integral is greater than or equal to lower integral.</title><link>https://freshrimpsushi.github.io/en/posts/831/</link><pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/831/</guid><description>This article is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem1 For any partition, the Riemann(-Stieltjes) upper sum is always greater than or equal to the Riemann(-Stieltjes) lower sum. $$ \underline { \int _{a} ^b} f d\alpha \le \overline {\int _{a}^b} f d\alpha $$ Proof Before proving, let&amp;rsquo;s assume the following: $f : [a,b] \to \mathbb{R}$ is bounded. $\alpha</description></item><item><title>Riemann-Stieltjes Integral</title><link>https://freshrimpsushi.github.io/en/posts/829/</link><pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/829/</guid><description>Overview The Riemann-Stieltjes integral is a generalization of the Riemann integral, sometimes simply referred to as Stieltjes integral. The Riemann integral is a special case of the Riemann-Stieltjes integral where $\alpha (x)=x$. The process of defining the Riemann-Stieltjes integral is the same as the process of defining the Riemann integral, so details on the notation and buildup are omitted here. Definition Let $\alpha : [a,b] \to \mathbb{R}$ be a monotonically</description></item><item><title>Partition, Riemann Sum, Riemann Integral</title><link>https://freshrimpsushi.github.io/en/posts/828/</link><pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/828/</guid><description>Partition1 Let&amp;rsquo;s assume the interval $[a,b]$ is given. The partition $P$ of $[a,b]$ is defined as follows. $$ P := \left\{ x_{0},\ x_{1},\ \cdots, x_{n}\right\},\quad a=x_{0} &amp;lt;x_{1}&amp;lt;\cdots &amp;lt; x_{n} =b $$ And $\Delta x_{i}$ is defined as follows. $$ \Delta x_{i} :=x_{i}-x_{i-1},\quad i=1,2,\cdots,n $$ Explanation Simply put, a partition is a set that contains all points at the ends of an interval and all boundary points within the interval when</description></item><item><title>Laplace Transform of Periodic Functions</title><link>https://freshrimpsushi.github.io/en/posts/773/</link><pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/773/</guid><description>Formulas Let $f$ be a periodic function with period $T$. Then $f(t+T)=f(t)$ and the Laplace transform of $f(t)$ is as follows. $$ \mathcal{L} \left\{ f(t) \right\} = \int_{0}^\infty e^{-st}f(t)dt = \frac{\displaystyle \int_{0}^T e^{-st}f(t)dt}{1-e^{-st}} $$ Derivation From the definition of Laplace transform, split the integral like this. $$ \int_{0}^\infty e^{-st}f(t)dt = \int_{0}^T e^{-st}f(t)dt + \int_{T}^{2T} e^{-st}f(t)dt + \int_{2T}^{3T}e^{-st}f(t)dt + \cdots $$ At this point, to make the integration range of the</description></item><item><title>Laplace Transform of the Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/772/</link><pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/772/</guid><description>Theorem1 The Laplace Transform of the Dirac Delta Function is as follows. $$ \mathcal{L} \left\{ \delta (t-t_{0}) \right\} = e^{-st_{0}} $$ Proof Let&amp;rsquo;s define as shown in the picture above $d_\tau (t) = \dfrac{1}{2\tau}$ $-\tau \le t \le \tau$. Then, the limit below is the same as the Dirac Delta Function. $$ \lim \limits_{\tau \to 0^+}d_\tau (t)=\delta (t) \\ \lim \limits_{\tau \to 0^+}d_\tau (t-t_{0})=\delta (t-t_{0}) $$ Thus $\mathcal{L} \left\{ \delta</description></item><item><title>Laplace Transform of t^{n}f(t)</title><link>https://freshrimpsushi.github.io/en/posts/771/</link><pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/771/</guid><description>Formulas Let&amp;rsquo;s say the Laplace transform of the function $f(t)$ is $\mathcal{L} \left\{ f(t) \right\} = \displaystyle \int _{0} ^\infty e^{-st}f(t)dt = F(s)$. Then, the Laplace transform of $t^{n}f(t)$ is as follows. $$ \mathcal{L} \left\{ t^n f(t) \right\} = (-1)^nF^{(n)}(s) $$ Derivation First, the Laplace transform of $t^nf(t)$, by definition, is as follows. $$ \int _{0} ^\infty e^{-st}tf(t) dt $$ If we look closely at the integral, it can be</description></item><item><title>Inverse Laplace Transform of F(as+b)</title><link>https://freshrimpsushi.github.io/en/posts/767/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/767/</guid><description>공식1 Assuming that the Laplace transform $\mathcal{L} \left\{ f(t) \right\}= \displaystyle \int _{0} ^\infty e^{-st}f(t)dt =F(s)$ of function $f(t)$ exists as $s&amp;gt;\alpha \ge 0$, the inverse Laplace transform of $F(as+b)$ for constant $a&amp;gt;0 , b$ is as follows. $$ \mathcal{L^{-1}} \left\{ F(as+b) \right\} =\frac{1}{a}e^{-\frac{b}{a}t}f\left(\frac{t}{a}\right) $$ Derivation 1 Inverse Laplace transform of $F(ks)$: $$ \mathcal{L^{-1}} \left\{ F(ks) \right\} =\dfrac{1}{k}f\left(\frac{t}{k}\right) $$ Translation of Laplace transform: $$ \mathcal{L^{-1}} \left\{ F(s-c) \right\}=e^{ct}f(t)</description></item><item><title>Inverse Laplace Transform of F(ks)</title><link>https://freshrimpsushi.github.io/en/posts/766/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/766/</guid><description>Formulas1 Assuming that the Laplace transform $\mathcal{L} \left\{ f(t) \right\} = \displaystyle \int _{0} ^\infty e^{-st}f(t)dt = F(s)$ of the function $f(t)$ exists and is $s&amp;gt;a \ge 0$ for a positive number $k&amp;gt; 0$ then the inverse Laplace transform of $F(ks)$ is as follows. $$ \mathcal{L^{-1}} \left\{ F(ks) \right\} =\dfrac{1}{k}f\left(\frac{t}{k}\right),\quad s&amp;gt;\frac{a}{k} $$ Derivation 1 The Laplace transform of $f(ct)$ $$ \mathcal{L} \left\{ f(ct) \right\} =\dfrac{1}{c}F\left(\dfrac{s}{c}\right), \quad s&amp;gt;ca $$ By substituting</description></item><item><title>Laplace Transform of f(ct)</title><link>https://freshrimpsushi.github.io/en/posts/765/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/765/</guid><description>Formulas1 Let&amp;rsquo;s assume that the Laplace transform $\mathcal{L} \left\{ f(t) \right\} = \displaystyle \int _{0} ^\infty e^{-st}f(t)dt = F(s)$ of the function $f(t)$ exists and is $s&amp;gt;a \ge 0$. Then, for $c &amp;gt;0$, the Laplace transform of $f(ct)$ is as follows. $$ \mathcal{L} \left\{ f(ct) \right\} =\dfrac{1}{c}F\left(\dfrac{s}{c}\right), \quad s&amp;gt;ca $$ Derivation $$ \mathcal{L} \left\{ f(ct) \right\} = \int _{0} ^\infty e^{-st}f(ct)dt $$ Let&amp;rsquo;s substitute $ct=\tau$. Then, since $st=\dfrac{s}{c}\tau$ and $dt=\dfrac{1}{c}d\tau$,</description></item><item><title>Laplace Transform of the First Order Derivative</title><link>https://freshrimpsushi.github.io/en/posts/760/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/760/</guid><description>Theorem1 Let&amp;rsquo;s assume the following two conditions. Let function $f(t)$ be continuous on interval $0 \le t \le A$, and let its first derivative $f^{\prime}(t)$ be piecewise continuous. There exists real numbers $a$ and positive numbers $K$, $M$ such that when $t \ge M$, it satisfies $|f(t)| \le Ke^{at}$. Then, the first derivative of $f$&amp;rsquo;s Laplace transform $\mathcal{L} \left\{ f^{\prime}(t) \right\}$ exists when $s&amp;gt;a$ and its value is as follows.</description></item><item><title>Laplace Transform Translation</title><link>https://freshrimpsushi.github.io/en/posts/764/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/764/</guid><description>Formula1 Assuming the Laplace transform $F(s)=\mathcal{L} \left\{ f(t) \right\}$ of the function $f(t)$ exists as $s&amp;gt;a$. Then, the following holds for constant $c$. $$ \begin{align*} \mathcal{L} \left\{ e^{ct}f(t) \right\}&amp;amp;=F(s-c), &amp;amp;s&amp;gt;a+c \\ \mathcal{L^{-1}} \left\{ F(s-c) \right\}&amp;amp;=e^{ct}f(t) &amp;amp; \end{align*} $$ Explanation This means that multiplying an exponential function to $f$ is equivalent to translating $F$. Derivation $$ \begin{align*} \mathcal{L} \left\{ e^{ct}f(t) \right\} &amp;amp;=\int_{0}^\infty e^{-st}e^{ct}f(t)dt \\ &amp;amp;= \int_{0}^\infty e^{-(s-c)t}f(t)dt \\ &amp;amp;= F(s-c) \end{align*}</description></item><item><title>Solving Second-Order Linear Nonhomogeneous Differential Equations Using Laplace Transforms</title><link>https://freshrimpsushi.github.io/en/posts/763/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/763/</guid><description>Theorem1 $$ ay^{\prime \prime} + by^{\prime} + cy = g(t) $$ Let us assume that the above second-order linear inhomogeneous differential equation is given. And let&amp;rsquo;s say $\mathcal{L} \left\{ y \right\} =Y(s)$, $\mathcal{L} \left\{ g(t) \right\}=G(s)$. Then, $$ Y(s) = \dfrac{ (as + b)y(0) + ay^{\prime}(0) } {as^2+bs+c} + \dfrac{G(s) }{as^2+bs+c} $$ Explanation The above formula is easy to memorize if you remember the rules well. If you memorize according</description></item><item><title>The Laplace Transform of the n-th Order Derivative</title><link>https://freshrimpsushi.github.io/en/posts/762/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/762/</guid><description>Theorem1 Assuming the following two conditions: For any interval $0 \le t \le A$, let functions $f$, $f^{\prime}$, $\cdots$, $f^{(n-1)}$ be continuous and let the n-th derivative $f^{(n)}(t)$ be piecewise continuous. When $t \ge M$, there exist real numbers $a$ and positives $K$, $M$ satisfying $|f(t)| \le Ke^{at}$, $|f^{\prime}(t)| \le Ke^{at}$, $\cdots$, and $|f^{(n-1)}(t)| \le Ke^{at}$. Then, the Laplace transform of the n-th derivative of $f$, $\mathcal{L} \left\{ f^{(n)}(t) \right\}$,</description></item><item><title>Definition and Existence Proof of the Laplace Transform</title><link>https://freshrimpsushi.github.io/en/posts/761/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/761/</guid><description>Definition[^1] The Laplace transform of a function $f$ is defined as follows. $$ \mathcal{L} \left\{ f(t) \right\} := \int _{0}^\infty e^{-st}f(t) dt =F(s) $$</description></item><item><title>Laplace Transform of the Step Function</title><link>https://freshrimpsushi.github.io/en/posts/758/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/758/</guid><description>Definition1 Let&amp;rsquo;s denote the unit step function unit step function shifted by $c$ as follows: $$ u_{c}(t)=\begin{cases} 0 &amp;amp; t&amp;lt;c \\ 1 &amp;amp; t \ge c \end{cases} $$ Formula The Laplace transform of the step function $u_{c}(t)$ is as follows. $$ \begin{equation} \mathcal{L} \left\{ u_{c}(t) \right\} = \dfrac{e^{-cs}}{s},\quad s&amp;gt;0 \label{eq1} \end{equation} $$ Let&amp;rsquo;s assume that $c$ is an arbitrary constant, and when $s &amp;gt; a \ge 0$, the Laplace transform</description></item><item><title>Staircase Function</title><link>https://freshrimpsushi.github.io/en/posts/757/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/757/</guid><description>Definition A function that is a piecewise constant function is called a step function. Description As shown in the figure above, it looks like a staircase, hence the name step function. It is also known as the Heaviside function, named after Heaviside, who is known to be the first to propose it. Heaviside was the person who created a method for solving differential equations in electrical circuits, which is the</description></item><item><title>Laplace Transform of Constant Functions</title><link>https://freshrimpsushi.github.io/en/posts/745/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/745/</guid><description>Formulas1 $$ \mathcal{L} \left\{ 1 \right\} = \dfrac{1}{s},\quad s&amp;gt;0 $$ Derivation $$ \begin{align*} \mathcal{L}\left\{ 1 \right\} &amp;amp;= \int _{0}^\infty e^{-st} \cdot 1 dt \\ &amp;amp;= \lim \limits_{A \to \infty} \left[ -\dfrac{e^{-st}}{s} \right]_{0}^A \\ &amp;amp;= \lim \limits_{A \to \infty} \left[ -\dfrac{e^{-sA}}{s} +\dfrac{e^{-0t}}{s} \right] \\ &amp;amp;= \dfrac{1}{s} \end{align*} $$ Since it must follow $\lim \limits_{A \to \infty}\dfrac{e^{-sA}}{s}=0$,2 the condition that $s&amp;gt;0$ is added. ■ See Also Table of Laplace Transforms William E.</description></item><item><title>Laplace Transform of Exponential Functions</title><link>https://freshrimpsushi.github.io/en/posts/750/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/750/</guid><description>Formulas1 $$ \mathcal {L} \left\{ e^{at} \right\} = \dfrac{1}{s-a},\quad s&amp;gt;a $$ Description Let&amp;rsquo;s compare this with the result of the Laplace transform of a constant function (../745). $$ \mathcal{L} \left\{ 1 \right\} =\dfrac{1}{s} $$ The Laplace transform result of $e^{at}$ is the same as when $F(s)$ is shifted by $a$, when $f(t)=1$. This is inevitable because when $e^{at}$ is multiplied by the original function, $\displaystyle \int e^{-st}f(t) dt$ becomes $\displaystyle</description></item><item><title>Laplace Transform of Hyperbolic Functions</title><link>https://freshrimpsushi.github.io/en/posts/751/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/751/</guid><description>Formulas1 The Laplace transforms of hyperbolic sine and hyperbolic cosine functions are as follows. $$ \mathcal{L} \left\{ \sinh (at) \right\} = \dfrac{a}{s^2-a^2},\quad s&amp;gt;|a| \\ \mathcal{L} \left\{ \cosh (at) \right\} = \dfrac{s}{s^2-a^2},\quad s&amp;gt;|a| $$ Description The definition of hyperbolic functions is as follows. $$ \sinh (ax) = \dfrac{ e^{ax} - e^{-ax} }{ 2 } \\ \cosh (ax) = \dfrac{ e^{ax} + e^{-ax} }{ 2 } $$ Derivation Use the results of</description></item><item><title>Laplace Transform of Polynomial Functions</title><link>https://freshrimpsushi.github.io/en/posts/747/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/747/</guid><description>Formulas1 $$ \mathcal{L} \left\{ t^p \right\} = \dfrac{ \Gamma (p+1) } {s^{p+1}},\quad s&amp;gt;0 $$ Explanation The Laplace transform of a polynomial is represented by the Gamma function. If we use $x^p$ instead of $t^p$, it would be easier to recognize at a glance. Usually, in differential equations, variables represent time, so $x$ is replaced with $t$. Derivation $$ \begin{align*} \mathcal{L} \left\{ t^p \right\} &amp;amp;= \int_{0}^\infty e^{-st}t^p dt \\ &amp;amp;= \lim</description></item><item><title>Laplace Transform of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/746/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/746/</guid><description>Formulas1 The Laplace transforms of sine and cosine are as follows. $$ \mathcal{L} \left\{ \sin (at) \right\} = \dfrac{a}{s^2+a^2},\quad s&amp;gt;0 $$ $$ \mathcal{L} \left\{ \cos (at) \right\} = \dfrac{s}{s^2+a^2},\quad s&amp;gt;0 $$ Derivation $\sin (at)$ $$ \begin{align*} \mathcal{L} \left\{ \sin (at) \right\}&amp;amp; =\displaystyle \int_{0}^\infty e^{-st}\sin(at)dt \\ &amp;amp;= \lim \limits_{A \to \infty} \left[-\dfrac{1}{a}e^{-st}\cos (at) \right]_{0}^A+ \lim \limits_{A \to \infty} \int _{0}^\infty -\dfrac{s}{a}e^{-st} \cos (at)dt \\ &amp;amp;= \dfrac{1}{a} - \lim \limits_{A \to \infty}</description></item><item><title>Laplace Transform Table</title><link>https://freshrimpsushi.github.io/en/posts/743/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/743/</guid><description>Formula1 This is table of Laplace transform. $f(t)=\mathcal{L^{-1}}$ $F(s)=\mathcal{L} \left\{ f(t) \right\}$ Derivation $1$ $\dfrac{1}{s}$ link $e^{at}$ $\dfrac{1}{s-a}$ link $t^n$ $\dfrac{n!}{s^{n+1}}$ link $t^{p}$ $\dfrac{ \Gamma (p+1) }{ s^{p+1}}$ link $t^{p}e^{at}$ $\dfrac{ \Gamma (p+1) }{ (s-a)^{p+1}}$ link $\sin (at)$ $\dfrac{a}{s^2+a^2}$ link $\cos (at)$ $\dfrac{s}{s^2+a^2}$ link $e^{at}\sin(bt)$ $\dfrac{b}{(s-a)^2 +b^2}$ link $e^{at}\cos(bt)$ $\dfrac{s-a}{(s-a)^2+b^2}$ link $\sinh (at)$ $\dfrac{a}{s^2-a^2}$ link $\cosh (at)$ $\dfrac{s}{s^2-a^2}$ link $e^{at} \sinh (bt)$ $\dfrac{b}{(s-a)^2-b^2}$ link $e^{at} \cosh (bt)$ $\dfrac{s-a}{(s-a)^2-b^2}$ link $u_{c}(t)=</description></item><item><title>Linearity of Laplace Transform</title><link>https://freshrimpsushi.github.io/en/posts/749/</link><pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/749/</guid><description>Theorem1 Let $f_{1}$ and $f_2$ be functions for which the Laplace transform exists. Also, let $c_{1}, c_2$ be an arbitrary constant. Then $$ \mathcal{L} \left\{ c_{1}f_{1} + c_2f_2 \right\} = c_{1}\mathcal{L} \left\{f_{1} \right\} + c_2\mathcal{L} \left\{f_2 \right\} $$ Explanation It is obvious that the Laplace transform is an integral transform. Proof $$ \begin{align*} \mathcal{L} \left\{ c_{1}f_{1}+c_2f_2 \right\} &amp;amp;= \int_{0}^\infty e^{-st} \left( c_{1}f_{1}+c_2f_2 \right) dt \\ &amp;amp;= \int_{0}^\infty e^{-st}c_{1}f_{1} dt +</description></item><item><title>Definition and Criterion of Subrings</title><link>https://freshrimpsushi.github.io/en/posts/590/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/590/</guid><description>Definition 1 A subset $S$ of a ring $R$ is called a subring of $R$ if it satisfies the conditions of a ring with respect to the operations of $R$. Meanwhile, it is trivial that $\left\{ 0 \right\}$ and $R$ are subrings of the ring $R$, hence $\left\{ 0 \right\}$ and $R$ are referred to as trivial subrings. Theorem: Subring Criterion For a non-empty subset $S$ of a ring $R$,</description></item><item><title>Rings in Abstract Algebra</title><link>https://freshrimpsushi.github.io/en/posts/587/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/587/</guid><description>Definition 1 A set $R$ satisfying the following rules for two binary operations, addition$+$ and multiplication$\cdot$, is defined as a Ring. When $a$, $b$, $c$ are elements of $R$, Commutative law holds for addition. $$a+b=b+a$$ Associative law holds for addition. $$(a+b)+c=a+(b+c)$$ There exists an identity element for addition. $$\forall a \ \exists 0\ \ \mathrm{s.t} \ a+0=a$$ There exists an additive inverse for every element. $$\forall a \ \exists -a\</description></item><item><title>Rules for Multiplication in a Ring</title><link>https://freshrimpsushi.github.io/en/posts/588/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/588/</guid><description>Theorem Given that $a,\ b,\ c$ is an element of the ring $R$ and $0$ is the identity element for addition, the following properties hold: $a0=0a=0$ $a(-b)=(-a)b=-(ab)$ $(-a)(-b)=ab$ $a(b-c)=ac-ac \ \ \And\ \ (b-c)a=ba-ca$ If there exists a multiplicative identity element $1$, then the following properties also hold: $(-1)a=-a$ $(-1)(-1)=1$ Proof 1. It&amp;rsquo;s about the property that multiplying any element with the additive identity still results in the additive identity.</description></item><item><title>Properties and Proofs of Surplus Types</title><link>https://freshrimpsushi.github.io/en/posts/691/</link><pubDate>Fri, 21 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/691/</guid><description>Theorem Let $H$ be a subgroup of group $G$. Then, for an element $a$ of group $G$, set $aH= \left\{ ah | h\in H \right\}$ is called the left coset, and $Ha = \left\{ ha | h\in H \right\}$ is called the right coset. Let&amp;rsquo;s say $H &amp;lt; G,\enspace a,b \in G,\enspace h \in H$. Then, the following properties are satisfied: $a \in aH$ $aH=H \iff a \in H$ $aH=bH</description></item><item><title>Simplifying the Exponentiation of Two-digit Numbers Ending in 5</title><link>https://freshrimpsushi.github.io/en/posts/661/</link><pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/661/</guid><description>Formulas The square of a two-digit number whose ones place is 5 can be computed quickly and easily as shown in the photo above. It&amp;rsquo;s okay to just know and use the result, but some might be curious about why it works this way. Proof Let&amp;rsquo;s assume any two-digit number whose ones place is $5$ is $10a+5$. Then, the square can be calculated as follows. $$ \begin{align*} (10a+5)(10a+5) =&amp;amp;\ 100a^2+100a+25</description></item><item><title>What is a Pseudovector?</title><link>https://freshrimpsushi.github.io/en/posts/641/</link><pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/641/</guid><description>Description Studying physics, one might come across the term pseudovector or axial vector. The tricky part is that you might encounter pseudovectors without really knowing what they are. It is said that one can study undergraduate physics without understanding pseudovectors, but I&amp;rsquo;ve never seen a textbook that explains it properly. I first encountered pseudovectors as Pseudovectors in a problem in Griffith&amp;rsquo;s electromagnetism. However, just solving problems was not enough to</description></item><item><title>Distinguishing Between W and Omega in Mathematics and Physics Textbooks</title><link>https://freshrimpsushi.github.io/en/posts/611/</link><pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/611/</guid><description>Description When reading textbooks on physics and mathematics, it can be confusing to tell whether a symbol is omega or double-u. For sophomores who have just started studying their major subjects, some might not even be aware of omega and think it’s always double-u. In physics, it’s most likely omega when you encounter it as a symbol for angular frequency. Personally, I feel like</description></item><item><title>Definition and Test Method of Subgroups</title><link>https://freshrimpsushi.github.io/en/posts/589/</link><pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/589/</guid><description>Definition1 group $G$의 subset $H$가 $G$의 연산에 대해서 group일 때, $H$를 $G$의 subgroup이라 하고 다음과 같이 표기한다. $$ H \le G $$ Explanation If $H$ is a subgroup</description></item><item><title>Gauss's Theorem, Divergence Theorem</title><link>https://freshrimpsushi.github.io/en/posts/565/</link><pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/565/</guid><description>Theorem1 The following holds for a 3-dimensional vector function $\mathbf{F}$: $$ \begin{equation} \int_{\mathcal{V}} \nabla \cdot \mathbf{F} dV = \oint_{\mathcal{S}} \mathbf{F} \cdot d \mathbf{S} \label{1} \end{equation} $$ Here, $\nabla \cdot \mathbf{F}$ is divergence, $\int_{\mathcal{V}}$ is volume integration, and $\oint_{\mathcal{S}}$ is closed surface integration. Description This is called Gauss&amp;rsquo;s theorem, Green&amp;rsquo;s theorem, or divergence theorem. The divergence theorem is especially used in electromagnetics. Mathematical Meaning Mathematically, it means that a surface integral</description></item><item><title>Solution to Clairaut's Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/556/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/556/</guid><description>Definition The 1st order nonlinear differential equation below is called Clairaut&amp;rsquo;s equation. $$ y=xy^\prime+f(y^\prime ) $$ Explanation The Clairaut differential equation is comparatively easier to solve than other nonlinear differential equations such as the Bernoulli differential equation or the Riccati differential equation. Solution Differentiate both sides of the given differential equation $y=xy^\prime+f(y^\prime )$ and then organize. $$ \begin{align*} &amp;amp;&amp;amp; y^\prime = y^\prime+xy^{\prime \prime} + y^{\prime \prime}f^\prime(y^\prime ) \\ \implies &amp;amp;&amp;amp;</description></item><item><title>Riccati Differential Equation Solutions</title><link>https://freshrimpsushi.github.io/en/posts/555/</link><pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/555/</guid><description>Definition The first-order nonlinear differential equation below is called the Riccati equation. $$ y^\prime = P(x)y+Q(x)y^2+R(x) $$ Explanation If $y_{1}$ is known as a particular solution, the general solution is represented in the form of $y=y_{1}+u(x)$. Here, $u(x)$ is an arbitrary constant, and it can be obtained by solving the Bernoulli differential equation when $n=2$. Solution The Riccati equation looks too complicated at first glance to solve. Hence, we need</description></item><item><title>Solution to the Bernoulli Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/554/</link><pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/554/</guid><description>Definition The following first-order nonlinear differential equation is called the Bernoulli equation. $$ y^\prime + p(x)y = q(x)y^n $$ Here, $n$ is an integer greater than or equal to $2$, and when $n=0,\ 1$, it is a linear equation. Description It’s worth noting that the Bernoulli of the Bernoulli differential equation and the Bernoulli of the widely known Bernoulli&amp;rsquo;s principle in fluid dynamics are different people. The</description></item><item><title>Methods for Finding the Second Solution of Second Order Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/553/</link><pubDate>Sun, 06 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/553/</guid><description>Description1 $$ \begin{equation} y^{\prime \prime }+p(t)y^{\prime} + q(t)y=0 \end{equation} $$ Given the differential equation above, assume we know one solution $y_{1}$. Let&amp;rsquo;s assume the general solution is $y(t)=\nu (t) y_{1}(t)$. If we calculate the 1st and 2nd derivatives of $y$, we get the following. $$ \begin{align*} y^{\prime} &amp;amp;= \nu^{\prime} y_{1} + \nu y_{1}^{\prime} \\ y^{\prime \prime} &amp;amp;= \nu ^{\prime \prime}y_{1} + \nu^{\prime} y_{1}^{\prime} + \nu^ \prime y_{1}^{\prime} + \nu y_{1}^{\prime</description></item><item><title>General Solution to Second-Order Linear Nonhomogeneous Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/547/</link><pubDate>Fri, 04 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/547/</guid><description>Auxiliary Lemma1 Consider the following nonhomogeneous/homogeneous second-order linear differential equation: $$ \begin{align} y^{\prime \prime}+p(t)y^\prime + q(t)y &amp;amp;=g(t) \label{eq1} \\ y^{\prime \prime}+p(t)y^\prime + q(t)y &amp;amp;=0 \label{eq2} \end{align} $$ Assume that $y_{1} (t)$ and $y_{2} (t)$ are solutions to the nonhomogeneous differential equation $\eqref{eq1}$, and that $y_{1}(t)$ and $y_{2}(t)$ are the fundamental set of solutions to the homogeneous differential equation $\eqref{eq2}$. Then, the following equation holds: $$ y_{1} (t) – y_{2} (t)=</description></item><item><title>Solutions to the Homogeneous Second-Order Linear Differential Equation and the Wronskian</title><link>https://freshrimpsushi.github.io/en/posts/546/</link><pubDate>Thu, 03 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/546/</guid><description>Definition[^1] $$ ay^{\prime \prime}+ by^\prime +cy=0 $$ Let&amp;rsquo;s consider the second-order linear homogeneous differential equation given above. Let&amp;rsquo;s call $W$ the Wronskian. If $W (y_{1}, y_{2}) \ne 0$, then we call $\left\{ y_{1}, y_{2} \right\}$ the fundamental set of solution for the given differential equation.</description></item><item><title>Homogeneous Meaning in Homogeneous Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/545/</link><pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/545/</guid><description>Description $$ a_{n}(x)\dfrac{d^ny}{dx^n}+a_{n-1}(x)\dfrac{d^{n-1}y}{dx^{n-1}}+ \cdots + a_{1}(x)\dfrac{dy}{dx}+a_{0}(x)y=f(x) $$ When a differential equation is as above, if $f(x)=0$, it is called homogeneous $f(x) \ne 0$ if not, it is called non-homogeneous or inhomogeneous. Consider the following simple example of a 2nd order linear differential equation. $$ ay^{\prime \prime}+by^\prime +cy=g(t) $$ Here, if $g(t)$ equals $0$, it is homogeneous; if $0$ is not met, it is non-homogeneous. To elaborate on the term homogeneous,</description></item><item><title>Solution to Second Order Homogeneous Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/544/</link><pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/544/</guid><description>Theorem1 $$ ay^{\prime \prime} + by^\prime + cy=0 $$ Let&amp;rsquo;s say the solutions to the characteristic equation $ar^2+br+c=0$ given above are $r_{1}$ and $r_2$. Then, $\text{1.}$ If $r_{1}$ and $r_2$ are two distinct real numbers$(b^2-4ac&amp;gt;0)$, the general solution is as follows: $$ y(t)=c_{1}e^{r_{1}t}+c_2e^{r_2t} $$ $\text{2.}$ If $r_{1}$ and $r_2$ are complex conjugates $\lambda \pm i \mu$$(b^2-4ac&amp;lt;0)$, the general solution is as follows: $$ \begin{align*} y(t) &amp;amp;= c_{1}e^{(\lambda + i\mu)t} +</description></item><item><title>Restoring Force and One-Dimensional Simple Harmonic Oscillator</title><link>https://freshrimpsushi.github.io/en/posts/543/</link><pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/543/</guid><description>Simple Harmonic Motion1 Let&amp;rsquo;s consider the motion of an object hanging on a spring. It oscillates back and forth due to the restoring force of the spring. Such motion is called a harmonic oscillation. The functions representing harmonic oscillation, $\sin$ and $\cos$, were called harmonic functions a long time ago, which is why the motion is designated as such. Among harmonic oscillations, those with no friction or other external forces,</description></item><item><title>Second-Order Linear Homogeneous Differential Equations with Constant Coefficients and Characteristic Equation</title><link>https://freshrimpsushi.github.io/en/posts/540/</link><pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/540/</guid><description>Theorem1 The general solution of a second-order linear homogeneous differential equation with constant coefficients $a y^{\prime \prime} + by^\prime +cy=0$ is as follows. $$ y(x)=A e^{r_{1} x}+Be^{r_2 x} $$ At this time, $r_{1,2}=\dfrac{-b \pm \sqrt{b^2-4ac}} {2a}$ Corollary The solution of $a y^{\prime \prime} + cy = 0$ is as follows. $$ y(x) = A e^{i\sqrt{\frac{c}{a}} x}+Be^{-i\sqrt{\frac{c}{a}} x} = C\cos{\textstyle (\sqrt{\frac{c}{a}}x)} + D\sin{\textstyle (\sqrt{\frac{c}{a}}x)} $$ Solution $$ \begin{equation} a\dfrac{d^2}{dx^2}y+b\dfrac{d}{dx}y+cy = 0</description></item><item><title>Linear Combination of Solutions to Homogeneous Linear Differential Equations is Also a Solution</title><link>https://freshrimpsushi.github.io/en/posts/542/</link><pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/542/</guid><description>Theorem1 If $y_{1}, y_{2}$ is a solution to $ay^{\prime \prime}+by^\prime +cy=0$, then $d_{1}y_{1} + d_{2}y_{2}$ is also a solution. Here, $d_{1}, d_{2}$ is any constant. Description As can be seen in the proof, it also holds for any $n$ order linear homogeneous differential equation. Proof Assume that $y_{1}, y_{2}$ is a solution to $ay^{\prime \prime}+by^\prime +cy=0$. Then the following two equations are satisfied. $$ \begin{align*} d_{1} (ay_{1}^{\prime \prime}+by_{1}^\prime + cy_{1}</description></item><item><title>Sum and Difference Formulas and Product-to-Sum Formulas of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/539/</link><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/539/</guid><description>The sum-to-product and product-to-sum formulas aren&amp;rsquo;t used as often as the double angle/half angle formulas, so they&amp;rsquo;re not considered as important. However, this doesn&amp;rsquo;t mean they&amp;rsquo;re entirely unnecessary. Since the derivation process is very simple, it&amp;rsquo;s good to be familiar with it and be able to derive it quickly whenever needed. They are derived using only the addition formulas. Addition Formulas $$ \begin{align*} \sin ( \theta_{1} \pm \theta_{2}) =&amp;amp;\ \sin</description></item><item><title>Pressure of the Fluid Depending on the Depth when an Object is Placed on the Fluid</title><link>https://freshrimpsushi.github.io/en/posts/533/</link><pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/533/</guid><description>Explanation Simply put, when an object is on a fluid, the pressure depending on the depth can be obtained by substituting ${P_{0} }^\prime$ for $P_{0}$ in the case of calculating the pressure in a fluid depending on the depth. The original formula&amp;rsquo;s atmospheric pressure $P_{0}$ represented the pressure exerted from above the fluid. That is, if an object is placed on the fluid, adding the pressure due to the object</description></item><item><title>The Formula to Calculate the Pressure of a Fluid Based on Depth</title><link>https://freshrimpsushi.github.io/en/posts/520/</link><pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/520/</guid><description>Formulas The pressure of a fluid at a vertical distance $h$ below the surface, or simply, the fluid pressure at a depth of $h$, $P_{h}$, is as follows. $$ P_{h}=P_{0}+\rho g h $$ Here, $P_{0}$ is the atmospheric pressure, $\rho$ is the density of the object, and $g$ is the gravitational acceleration. Explanation This formula applies only in static situations. It does not apply to moving fluids, i.e., fluids with</description></item><item><title>Fluid Pressure</title><link>https://freshrimpsushi.github.io/en/posts/519/</link><pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/519/</guid><description>Definition A pressure is the force acting per unit area by a fluid. Let&amp;rsquo;s denote $F$ as the force, $A$ as the area upon which the force acts, and $P$ as the pressure. Then, $$ P=\dfrac{F}{A} [\mathrm{N/m^{2}}] $$ Description The unit of pressure is newtons per square meter $\mathrm{N/m^{2}}$, known as Pascal, and denoted by $\mathrm{Pa}$. $$ 1 \mathrm{Pa} = 1 \mathrm{N/m^{2}} $$ A characteristic feature of pressure is that</description></item><item><title>Definition and Discrimination Method of an Exact Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/516/</link><pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/516/</guid><description>Definition The given differential equation $\psi=\psi (x,y)$ is said to be an exact differential equation if there exists $\psi=\psi (x,y)$ that satisfies $\psi (x,y)$. Explanation If the given differential equation is exact, it can be represented as a total differential with respect to $\psi (x,y)$. $d\psi (x,y)=\dfrac{\partial \psi }{\partial x}dx + \dfrac{\partial \psi }{\partial y}dy$ Since $d\psi (x,y)=\dfrac{\partial \psi }{\partial x}dx + \dfrac{\partial \psi }{\partial y}dy$, it follows that $d\psi</description></item><item><title>Integrating Factor Method for First Order Linear Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/515/</link><pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/515/</guid><description>Theorem1 The solution to the first order linear differential equation $\dfrac{dy}{dx}+p(x)y=q(x)$ is given as follows. $$ \begin{align*} y(x)&amp;amp;=\dfrac{1}{e^{\int p(x) dx}} \left[ \int e^{\int p(x) dx} q(x) dx +C \right] \\ &amp;amp;=e^{-\int p(x) dx}\int e^{\int p(x) dx} q(x) dx + e^{-\int p(x) dx}C \end{align*} $$ Description A differential equation of form $y^\prime+p(x)y=q(x)$ is called a first order linear differential equation. Here, if $q(x)=0$, we can directly apply variable separation and solve</description></item><item><title>Solution to the Exact Differential Equation</title><link>https://freshrimpsushi.github.io/en/posts/517/</link><pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/517/</guid><description>Solution The solution to the given exact differential equation $M(x,y)+N(x,y)\dfrac{dy}{dx}=0$ is as follows. Step 0. Since the differential equation given is exact, an $\psi$ exists such that $\psi_{x}=M,\ \ \psi_{y}=N, \ \ \psi=c$. Step 1. Integrate $\psi_{x}$. Then, differentiate the obtained $\psi$ with respect to $y$ to find $h^\prime(y)$. The entire process including Step 1 can be done the opposite way with respect to $x$ and $y$ as well. $$</description></item><item><title>Moment of Inertia of a Spherical Shell</title><link>https://freshrimpsushi.github.io/en/posts/243/</link><pubDate>Sat, 07 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/243/</guid><description>Formulas The moment of inertia of a spherical shell with a radius of $a$ and a mass of $m$ is as follows. $$ I=\frac{2}{3}ma^{2} $$ Derivation Consider a uniform spherical shell with a radius of $a$ and a mass of $m$. The same idea is used as when calculating the moment of inertia of a sphere. However, there is a bit of a difference. Think of the spherical shell as</description></item><item><title>Socks-Shoes Property: The Inverse of ab is Equal to the Product of the Inverse of b and the Inverse of a</title><link>https://freshrimpsushi.github.io/en/posts/513/</link><pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/513/</guid><description>Theorem 1 For any element $a,b$ of a group $G$, it follows that $(ab)^{-1}=b^{-1}a^{-1}$. Proof Since $(ab)^{-1}$ is the inverse of $ab$, $$ ab(ab)^{-1}=e $$ multiplying both sides by $a^{-1}$ gives $$ b(ab)^{-1}=a^{-1}e=a^{-1} $$ then multiplying both sides by $b^{-1}$ gives $$ (ab)^{-1}=b^{-1}a^{-1} $$ ■ Explanation This theorem is referred to as the Socks-Shoes Property, which is an analogy to the process of putting on socks and then shoes. If</description></item><item><title>Differentiation of Vectors, Dot Product, and Cross Product in Cartesian Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/506/</link><pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/506/</guid><description>Formulas Let&amp;rsquo;s say $\mathbf{A} = A_{x}\hat{\mathbf{x}} + A_{y}\hat{\mathbf{y}} + A_{z}\hat{\mathbf{z}}, \mathbf{B} = B_{x}\hat{\mathbf{x}} + B_{y}\hat{\mathbf{y}} + B_{z}\hat{\mathbf{z}}$ is a vector in a 3-dimensional Cartesian coordinate system. Let $n$ be any scalar. Then, the following equations hold: (a) $\dfrac{ d \left( n \mathbf{A} \right) }{dt} = \dfrac{ dn }{dt} \mathbf{A} + n\dfrac{ d\mathbf{A}}{dt}$ (b) $\dfrac{ d ( \mathbf{A} \cdot \mathbf{B} )}{dt} = \dfrac{ \mathbf{A} }{dt} \cdot \mathbf{B} + \mathbf{A} \cdot \dfrac{</description></item><item><title>Kinetic and Potential Energy Definitions in Physics</title><link>https://freshrimpsushi.github.io/en/posts/507/</link><pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/507/</guid><description>Kinetic Energy1 When the force depends only on the position, i.e., it is independent of the velocity or time, the equation of motion (differential equation) for the straight-line motion of a particle is as follows. $$ \begin{equation} F(x)=m\ddot{x} \label{force1} \end{equation} $$ In this case, acceleration $\ddot{x}$ can be expressed in terms of velocity as follows. $$ \begin{align*} \ddot{x} &amp;amp;= \dfrac{d \dot{x}}{dt} \\ &amp;amp;=\dfrac{dv}{dt} \\ &amp;amp;=\dfrac{dv}{dx} \dfrac{dx}{dt} \\ &amp;amp;=v\dfrac{dv}{dx} \\ &amp;amp;=</description></item><item><title>Homogeneous Functions and First-Order Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/505/</link><pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/505/</guid><description>Definition When a function $f(x,y)$ satisfies $f(tx,ty)=t^nf(x,y)$ for any positive integer $n$, $f$ is called a $n$th degree homogeneous function.</description></item><item><title>Linear Expansion Coefficient and Volumetric Expansion Coefficient</title><link>https://freshrimpsushi.github.io/en/posts/504/</link><pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/504/</guid><description>Coefficient of Linear Expansion The coefficient of linear expansion refers to the change in length per unit length of a solid when it expands due to heat, as follows: $$ \alpha = \dfrac{\Delta L}{L} \dfrac{1}{\Delta T} \left[ ^\circ \mathrm{C} ^{-1} \right] $$ Here, $L$ is the original length of the solid, $\Delta T$ is the change in temperature, and $\Delta L$ is the change in length. Derivation Let&amp;rsquo;s assume that</description></item><item><title>Separable First-Order Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/503/</link><pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/503/</guid><description>Definition1 A first-order differential equation is said to be separable if it satisfies the following condition: $$ f(x)+g(y)\dfrac{dy}{dx}=0 \quad \text{or} \quad f(x)dx = -g(y)dy $$ Explanation It can be expressed in various forms, but the important point is that the variables on each side must be separated. The method of finding solutions by separating these two variables is called the method of separation of variables. The separability is a very</description></item><item><title>Wronskian Definition and Determination of Linear Independence</title><link>https://freshrimpsushi.github.io/en/posts/501/</link><pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/501/</guid><description>Definition1 Let us consider a set of functions that are differentiable up to n times, denoted by $S=\left\{ f_{1}, f_{2}, \dots, f_{n} \right\}$. The Wronskian $W$ of this set is defined by the following determinant. $$ W(x) = W(f_{1}, f_{2}, \dots, f_{n}) := \begin{vmatrix} f_{1} &amp;amp; f_{2} &amp;amp; \cdots &amp;amp; f_{n} \\ f_{1}^{\prime} &amp;amp; f_2^{\prime} &amp;amp; \cdots &amp;amp; f_{n}^{\prime} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ f_{1}^{(n-1)} &amp;amp;</description></item><item><title>Indefinite Integral of the Form e^{x^2}</title><link>https://freshrimpsushi.github.io/en/posts/487/</link><pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/487/</guid><description>Theorem $$ \int e^{x^2}dx = \sum\limits_{n=0}^\infty \dfrac{x^{2n+1}}{(2n+1)n!}+C $$ Explanation Just like the form $e^{-x^{2}}$, it is difficult to integrate using general methods. There is a method to integrate by defining the error function, imaginary error function, erfi, but this article introduces solving it using Taylor series expansion. Proof By the method of Taylor series expansion, $$ e^{x} = \sum\limits_{n=0}^{\infty} \dfrac{x^{n}}{n!} = 1 + x + \dfrac{x^{2}}{2!} + \cdots + \dfrac{x^{n}}{n!}</description></item><item><title>Classification of Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/483/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/483/</guid><description>Description Differential equations can be classified by various criteria. They are broadly divided into ordinary differential equations and partial differential equations. Further classification can be made based on coefficients and order, and whether they are linear or nonlinear. The reason for classifying differential equations is obviously to solve them. The method of solving a differential equation varies depending on its classification. Ordinary Differential Equations and Partial Differential Equations Ordinary differential</description></item><item><title>Double Angle and Half Angle Formulas of Trigonometric Functions</title><link>https://freshrimpsushi.github.io/en/posts/481/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/481/</guid><description>Overview Back in the day, when the owners of sushi restaurants were high school students, there used to be formulas like angle addition, double angle, and sum-difference formulas in the curriculum, but nowadays, it&amp;rsquo;s understood they are not. All the following formulas can be derived from the sum formulas, so it&amp;rsquo;s better to learn the derivation process and derive them as needed rather than memorizing them all. Addition Theorem $$</description></item><item><title>Proof that Sine Squared Plus Cosine Squared Equals 1</title><link>https://freshrimpsushi.github.io/en/posts/482/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/482/</guid><description>Formulas $$ \sin^2\theta+\cos^2\theta=1 $$ Proof 1-Addition Formula for Cosine Using the Addition Theorem for Cosine, we can understand it very easily. $$ \cos(\theta_{1}-\theta_2)=\cos\theta_{1}\cos\theta_2 + \sin\theta_{1}\sin\theta_2 $$ Here, if we substitute $\theta$ instead of $\theta_{1}$, $\theta_2$ $$\cos(\theta-\theta)=\cos^2\theta + \sin^2\theta$$ $$\implies \cos(\theta-\theta)=\cos 0=1$$ $$\implies \sin^2\theta+\cos^2\theta=1$$ ■ 2-Pythagorean Theorem There is a unit circle with a radius of 1. Let&amp;rsquo;s look at the triangle formed by the unit circle&amp;rsquo;s radius, the perpendicular dropped</description></item><item><title>Definition and Examples of Differential Equations</title><link>https://freshrimpsushi.github.io/en/posts/479/</link><pubDate>Sat, 17 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/479/</guid><description>Definition A differential equation is an equation that includes derivatives of one or more dependent variables with respect to one or more independent variables. $$ \dfrac{dy}{dx}=y $$ $$ \dfrac{d^2y}{dx^2} = y $$ Explanation Most physical situations can be described by first-order or second-order differential equations. Falling Body $$ F=ma=mg $$ $$ v=\dfrac{dy}{dt} $$ $$ a=\dfrac{dv}{dt}=\dfrac{d}{dt} \left( \dfrac{dy}{dt} \right)=\dfrac{d^2y}{dt^2} $$ $$ \dfrac{d^2y}{dt^2}=g $$ Spring Mass System $$ F=ma=-ky $$ $$ a=</description></item><item><title>Perfectly Elastic Collisions and the Conservation of Kinetic Energy</title><link>https://freshrimpsushi.github.io/en/posts/466/</link><pubDate>Thu, 08 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/466/</guid><description>Theorem When the coefficient of restitution $e$ is $1$, it is said to be a perfectly elastic collision. There are two important characteristics of a perfectly elastic collision. (a) The sum of the kinetic energy of each object before and after the collision is conserved. (b) If the masses of the two objects are the same, their velocities are exchanged after the collision. Proof (a) By the law of conservation</description></item><item><title>Collision of Two Objects and the Coefficient of Restitution</title><link>https://freshrimpsushi.github.io/en/posts/465/</link><pubDate>Tue, 06 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/465/</guid><description>Explanation When two objects collide, there are three types of collisions. Perfectly elastic collision (Elastic collision) Inelastic collision Perfectly inelastic collision The classification is based on the coefficient of restitution $e$. The coefficient of restitution is the ratio of the relative speed of the two objects before collision to the relative speed after collision. In other words, it&amp;rsquo;s defined as the ratio of the speed at which the two objects</description></item><item><title>Conservation of Momentum: An Easy Proof (High School Level)</title><link>https://freshrimpsushi.github.io/en/posts/464/</link><pubDate>Mon, 05 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/464/</guid><description>Theorem If no external force acts, the total momentum before and after the action of force (internal force) remains constant. In simpler terms, when two objects collide, the sum of each object&amp;rsquo;s momentum before and after the collision remains the same. $$ m_{1}v_{1}+m_2v_2=m_{1}{v_{1}}^{\prime}+m_2{v_2}^{\prime} $$ Proof (High School Level) When two objects $A, B$ collide, by the law of action and reaction, the force each exerts on the other is equal</description></item><item><title>Matrix Representation of Angular Momentum Operator</title><link>https://freshrimpsushi.github.io/en/posts/463/</link><pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/463/</guid><description>Formula The matrix representation of the angular momentum operator is as follows. When $\ell = 1$, $m = 1, 0, -1$ holds, $$ \underset{\normalsize L_{z} = }{\scriptsize} \begin{array}{cccc} \scriptstyle{m=1} &amp;amp; \scriptstyle m=0 &amp;amp; \scriptstyle m=-1 &amp;amp; \\ \hbar\left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array} \right. &amp;amp; \begin{array}{c} 0 \\ 0 \\ 0 \end{array} &amp;amp; \left. \begin{array}{c} 0 \\ 0 \\ -1 \end{array} \right] &amp;amp; \begin{array}{l} \scriptstyle m=1 \\ \scriptstyle</description></item><item><title>Matrix Representation of the Harmonic Oscillator Operator</title><link>https://freshrimpsushi.github.io/en/posts/367/</link><pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/367/</guid><description>Explanation Harmonic 🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 The Schrödinger equation is a linear equation, so a linear combination of multiple wavefunctions that satisfy the equation also satisfies the equation. If the eigenfunctions of each state of the harmonic oscillator are $\ket{\psi_{0}}$, $\ket{\psi_{1}}$,</description></item><item><title>Matrix Representation of Operators in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/366/</link><pubDate>Sun, 17 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/366/</guid><description>Build-up Let&amp;rsquo;s consider two unit vectors $\widehat{\mathbf{x}} = (1, 0)$ and $\widehat{\mathbf{y}} = (0, 1)$ in a 2-dimensional space. The coordinate vector of an arbitrary point $(a, b)$ in this space can be expressed as a linear combination of these two unit vectors as follows. $$ (a, b) = a(1, 0) + b(0, 1) \implies \begin{bmatrix} a \\ b \end{bmatrix} = a\begin{bmatrix} 1 \\ 0 \end{bmatrix} + b\begin{bmatrix} 0 \\</description></item><item><title>Solving Harmonic Oscillator Problems using the Operator Method: Definition of Ladder Operators</title><link>https://freshrimpsushi.github.io/en/posts/362/</link><pubDate>Wed, 13 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/362/</guid><description>Build-up The ladder operators of the harmonic oscillator are represented by $\mathrm{Ladder\ Operator}$. The Hamiltonian, which is the energy operator, $H$ can also be substituted, and the characteristic functions of the ladder operators can be used to find the eigenfunctions from the ground state. Hints for defining new operators can be obtained by factorizing the classical Hamiltonian of the harmonic oscillator $H$. $$ \begin{align*} H &amp;amp;= \frac{1}{2m}p^{2}+\frac{1}{2m}mw^{2}x^{2} \\ &amp;amp;= \frac{1}{2m}</description></item><item><title>Energy Levels in an Infinite Potential Well</title><link>https://freshrimpsushi.github.io/en/posts/361/</link><pubDate>Tue, 12 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/361/</guid><description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 To determine the wave functions (eigenfunctions) and energies (eigenvalues) in an infinite potential well, refer to here. Now, let&amp;rsquo;s bring in the results and examine their significance. Eigenfunction $\displaystyle \psi_{(x)} =\sqrt{\frac{2}{a}}\sin \frac{n\pi}{a}x$ Eigenvalue $\displaystyle E_{n}=\frac{n^2\pi^2\hbar^2}{2ma^2}$ For the wave function in an infinite potential well, the expectation value</description></item><item><title>What is Degeneracy of Wave Functions in Quantum Mechanics?</title><link>https://freshrimpsushi.github.io/en/posts/307/</link><pubDate>Mon, 11 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/307/</guid><description>Definition In quantum mechanics, degeneracy refers to the condition where two different (i.e., linearly independent) wave functions have the same eigenvalue. Explanation In simpler terms, when two wave functions are degenerate, it means that their energies are equal. Mathematically, this implies that the geometric multiplicity of the eigenvalue is 2 or more. In Griffiths&amp;rsquo; textbook, this is referred to as overlapping or overlapped states. According to the translator, before the</description></item><item><title>There is no solution to the time-independent Schrödinger equation when the energy is less than the potential.</title><link>https://freshrimpsushi.github.io/en/posts/360/</link><pubDate>Wed, 06 Dec 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/360/</guid><description>영어 및 일본어 번역 정리 에너지가 퍼텐셜보다 작은 구간에서는 시간에 무관한 슈뢰딩거 방정식의 해가 존재하지 않는다. 즉, 파동함수가 존재하지 않는다.</description></item><item><title>Relationship between simultaneous eigenfunctions of angular momentum and ladder operators</title><link>https://freshrimpsushi.github.io/en/posts/348/</link><pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/348/</guid><description>Summary Let&amp;rsquo;s denote the angular momentum operators $L^{2}$ and $L_{z}$, and their eigenvalues as $\ell(\ell+1)\hbar^{2}$ and $m\hbar$. The normalized simultaneous eigenfunctions corresponding to each eigenvalue are referred to as $\ket{\ell, m}$. $$ \begin{align*} L^{2} \ket{\ell, m} &amp;amp;= \ell(\ell+1)\hbar^{2}\ket{\ell, m} \\ L_{z}\ket{\ell, m} &amp;amp;= m\hbar\ket{\ell, m} \end{align*} $$ For the ladder operators for angular momentum $L_{\pm}$ and the eigenfunctions $\ket{\ell, m}$, the following relational expression holds. $$ \begin{align*} L_{+}\ket{\ell, m} &amp;amp;=</description></item><item><title>The condition for the product of two Hermitian operators to be a Hermitian operator</title><link>https://freshrimpsushi.github.io/en/posts/347/</link><pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/347/</guid><description>Summary Let the two operators $A$, $B$ be Hermitian operators. If $A$, $B$ are commutative, then $AB$ is also a Hermitian operator이다. 역도 성립한다. 설명 역의 대우를 생각해보면 교환 가능하지 않은 두 연산자를 곱하면 에르미</description></item><item><title>Ladder Operators for Angular Momentum</title><link>https://freshrimpsushi.github.io/en/posts/344/</link><pubDate>Sun, 26 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/344/</guid><description>Definition The Angular Momentum Operator corresponding to $L_{z}$ Ladder Operators are defined as follows. $$ L_{+} := L_{x} + \i L_{y} \\ L_{-} := L_{x} - \i L_{y} $$ $L_{+}$ is called the raising operator, and $L_{-}$ is called the lowering operator. Explanation 1 2 The names of the operators, raising/lowering, are due to the fact that $L_{\pm}$ raises or lowers the state of the simultaneous eigenfunction of the angular</description></item><item><title>Simultaneous Eigenfunctions of Angular Momentum</title><link>https://freshrimpsushi.github.io/en/posts/343/</link><pubDate>Sun, 26 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/343/</guid><description>Summary Angular momentum operator $L^{2}$ and $L_{z}$&amp;rsquo;s normalized simultaneous eigenfunctions are denoted as $\ket{l, m}$. The eigenvalue equations are given below. $$ \begin{align*} L^{2} \ket{\ell, m} &amp;amp;= \ell(\ell+1)\hbar^{2}\ket{\ell, m} \\ L_{z}\ket{\ell, m} &amp;amp;= m\hbar\ket{\ell, m} \end{align*} $$ In this case, $\ell$ can only be integers or half-integers. For a given $\ell$, the minimum value of $m$ is $-\ell$, and the maximum value is $\ell$. $$ \begin{align*} \ell &amp;amp;= 0, \frac{1}{2},</description></item><item><title>For any arbitrary operator, always in the Hermitian form</title><link>https://freshrimpsushi.github.io/en/posts/327/</link><pubDate>Sun, 19 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/327/</guid><description>공식 임의의 연산자 $A$에 대해서 아래의 꼴은 항상 에르미트 연산자이다. $$ A + A^{\dagger} \tag{1} $$ $$ \i (A - A^{\dagger}) \tag{2} $$ $$ A A^{\dagger} \tag{3} $$ 증명 원래의 식에 켤레전치 $^{\dagge</description></item><item><title>The Relationship Between the Powers of i and the Powers of e</title><link>https://freshrimpsushi.github.io/en/posts/325/</link><pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/325/</guid><description>Theorem Natural constant $e$ and imaginary number $i$ raised to a power satisfy the following relationship. $$ e^{i\frac{l \pi}{2}} = i^{l} $$ Proof Since $e^{ i \frac{l \pi}{2}}=\cos\frac{l \pi}{2}+i\sin \frac{l \pi}{2}$, when $l=0$, $$ e^{ 0}= 1 =i^{0} $$ When $l=1$, $$ e^{ i \frac{\pi}{2}}=\cos\frac{\pi}{2}+i\sin \frac{\pi}{2}=i=i^{1} $$ When $l=2$, $$ e^{ i \pi}=\cos \pi+i\sin \pi=-1=i^{2} $$ When $l=3$, $$ e^{ i \frac{3\pi}{2}}=\cos\frac{3\pi}{2}+i\sin \frac{3\pi}{2}=-i=i^{3} $$ As it repeats thereafter, $$ e^{</description></item><item><title>Two operators with simultaneous eigenfunctions are commutative.</title><link>https://freshrimpsushi.github.io/en/posts/321/</link><pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/321/</guid><description>Summary If two different operators have the same eigenfunction, then the two operators are commutative. In other words, if the following equation holds, it means $[A, B] = 0$. $$ \begin{cases} A\psi=a\psi \\ B\psi=b\psi \end{cases} $$ In this case, $\psi$ is the normalized eigenfunction. Converse The converse of the above theorem also holds. That is, the commutativity of two operators and having a common eigenfunction is a necessary and sufficient</description></item><item><title>Two eigenvectors with different eigenvalues are orthogonal.</title><link>https://freshrimpsushi.github.io/en/posts/318/</link><pubDate>Sun, 12 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/318/</guid><description>Summary Any two eigenfunctions corresponding to distinct eigenvalues of an Hermitian operator $A$ are orthogonal to each other. $$ \begin{cases} A\psi_{n}=a_{n}\psi_{n} \\ A\psi_{m}=a_{m}\psi_{m} \end{cases} $$ If $a_{n} \ne a_{m}$, $$ \braket{\psi_{n} | \psi_{m}} = 0 $$ Proof Since eigenvalues of Hermitian operators are always real, the following holds: $$ \braket{A\psi_{n}|\psi_{m} } ={a_{n}}^{\ast}\braket{\psi_{n}|\psi_{m} } =a_{n}\braket{\psi_{n}|\psi_{m}} $$ Moreover, according to the definition of Hermitian operator, $$ \braket{A\psi_{n}|\psi_{m}}=\braket{\psi_{n}|A^{\dagger}\psi_{m}}=\braket{\psi_{n}|A\psi_{m}} = a_{m} \braket{\psi_{n} | \psi_{m}}</description></item><item><title>The Meaning of Eigenvalue Equations in Quantum Mechanics</title><link>https://freshrimpsushi.github.io/en/posts/306/</link><pubDate>Sat, 11 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/306/</guid><description>Definition Matrix $n\times n$ Let us assume a matrix $A$ is given. We call $a$, which satisfies the following equation, the eigenvalue. A non-zero $n\times 1$ vector $\mathbf{x}$ corresponding to $a$ is called an eigen vector. $$ A \mathbf{x} = a \mathbf{x} \tag{1} $$ Operator Let us assume an operator $A$ is given. We call $a$, which satisfies the following equation, eigenvalue, () non-zero $\ket{a}$, the eigenfunction corresponding to $a$.</description></item><item><title>Proof that the expectation eigenvalue of a Hermitian operator is always real</title><link>https://freshrimpsushi.github.io/en/posts/305/</link><pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/305/</guid><description>정리 에르미트 연산자의 기댓값은 항상 실수이다. 증명 $A$를 에르미트 연산자라고 하자. $A$의 기댓값은 $$ \braket{A \rangle = \int \psi^{\ast}A\psi dx = \langle \psi | A\psi} $$ 실수임을 보이려먼 $\braket{\psi |</description></item><item><title>Hermitian Operator</title><link>https://freshrimpsushi.github.io/en/posts/304/</link><pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/304/</guid><description>Definition An operator $A$ is called a Hermitian operator if it satisfies the following equation. $$ A = A^{\dagger} $$ Here, $A^{\dagger}$ is the conjugate transpose of $A$. Explanation $A^{\dagger}$ is read as [A dagger], and a dagger means a small knife. It is named after the French mathematician Hermite. In English, it is called a Hermitian operator. All operators in quantum mechanics are Hermitian operators. The notation for the</description></item><item><title>Dimension of the Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/3018/</link><pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3018/</guid><description>Definition1 The number of elements (vectors) of a basis for a vector space $V$ is defined as the dimension of $V$ and is denoted as follows. $$ \dim (V) $$ Explanation Such a generalization of dimensions goes beyond merely exploring vector spaces and is being applied to various technologies that support this society. It might seem pointless to consider dimensions higher than the $3$ dimensions of our world and the</description></item><item><title>What is Dirac Notation?</title><link>https://freshrimpsushi.github.io/en/posts/303/</link><pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/303/</guid><description>Definition In quantum mechanics, wave functions are vectors and are fundamentally considered column vectors. A column vector is denoted using a right single-angle bracket and is referred to as a ket vector. $$ \psi = \ket{\psi} = \begin{pmatrix} \psi_{1} \\ \psi_{2} \\ \vdots \\ \psi_{n} \end{pmatrix} $$ The conjugate transpose matrix of $\ket{\psi}$ is denoted using a left single-angle bracket and is referred to as a bra vector. $$ \psi^{\ast}</description></item><item><title>What is an operator in physics (quantum mechanics)</title><link>https://freshrimpsushi.github.io/en/posts/301/</link><pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/301/</guid><description>영어 번역결과</description></item><item><title>Gradient, Divergence, Curl, and Laplacian in Curvilinear Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/299/</link><pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/299/</guid><description>Explanation In physics, the four operations involving the del operator $\nabla$, Gradient, Divergence, Curl, Laplacian, are very important. Therefore, one must know the operations in three coordinate systems. Of course, this does not mean that you have to memorize them. Since physics study is not about memorizing formulas, they will naturally be memorized as you study, so do not try to memorize them intentionally but instead keep a printout of</description></item><item><title>Commutation Relations of the Angular Momentum Operator</title><link>https://freshrimpsushi.github.io/en/posts/298/</link><pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/298/</guid><description>Formula The commutation relation of the angular momentum operators is as follows. $$ \left[L_{j}, L_{k} \right] = \i \hbar \epsilon_{jk\ell}L_{\ell} \tag{1} $$ Here, $\epsilon_{jk\ell}$ is the Levi-Civita symbol. When written out in full, $$ \left[ L_{x}, L_{y} \right] = \i \hbar L_{z} \\ \left[ L_{y}, L_{z} \right] = \i \hbar L_{x} \\ \left[ L_{z}, L_{x} \right] = \i \hbar L_{y} $$ Additionally, $L^{2} = L_{x}^{2} + L_{y}^{2} + L_{z}^{2}$ commutes</description></item><item><title>Properties of Commutators</title><link>https://freshrimpsushi.github.io/en/posts/297/</link><pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/297/</guid><description>Definition For two operators $A, B$, $AB - BA$ is defined as the commutator of $A, B$ and is denoted as follows. $$ [A,B]=AB-BA $$ Properties $$ \begin{align} [A, A] &amp;amp;= 0 \\[1em] [A, B] &amp;amp;= -[B, A] \\[1em] [A+B, C] &amp;amp;= [A, C] + [B, C] \\[1em] [AB, C] &amp;amp;= A[B, C]+[A, C]B \\[1em] [A,BC] &amp;amp;= B[A,C]+ [A,B]C \end{align} $$ Explanation The main method of describing quantum mechanics is</description></item><item><title>Conversion of Cartesian Coordinate System Unit Vectors to Spherical Coordinate System Unit Vectors</title><link>https://freshrimpsushi.github.io/en/posts/291/</link><pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/291/</guid><description>Formulas The expression for converting the unit vectors from the Cartesian coordinate system to the spherical coordinate system is as follows. $$ \begin{align*} \hat{ \mathbf{x} }&amp;amp;= \cos \phi \sin \theta \hat{ \mathbf{r} } + \cos \phi \cos \theta \hat{ \boldsymbol{\theta} } - \sin\phi\hat{ \boldsymbol{\phi} } \\ \hat{ \mathbf{y} } &amp;amp;= \sin\phi\sin\theta \hat{ \mathbf{r} } + \sin\phi\cos\theta\hat{ \boldsymbol{\theta} } + \cos\phi\hat{ \boldsymbol{\phi} } \\ \hat{ \mathbf{z} } &amp;amp;= \cos\theta\hat{ \mathbf{r} }</description></item><item><title>Finding the Wave Function Eigenfunctions and Energy Eigenvalues in an Infinite Potential Well</title><link>https://freshrimpsushi.github.io/en/posts/289/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/289/</guid><description>Proposition When the potential takes the form of an infinite well over the interval $[0, a]$, the energy (eigenvalue) $E_{n}$ and the wave function (eigenstate) $\psi_{n}$ of the wave function are as follows. $$ \begin{align*} E_{n} &amp;amp;=\frac{n^{2}\pi^{2}\hbar^{2}}{2ma^{2}} \\[1em] \psi_{n}{(x)} &amp;amp;= \textstyle \sqrt{\frac{2}{a}}\sin \left( \frac{n\pi}{a}x \right) \end{align*} \qquad\qquad n = 0, 1, 2, \dots \tag{0} $$ Explanation $$ V(x) = \begin{cases} \infty, &amp;amp; -\infty \lt x \lt 0 \\ 0, &amp;amp;</description></item><item><title>Linear Combination, Span</title><link>https://freshrimpsushi.github.io/en/posts/512/</link><pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/512/</guid><description>Definition: Linear Combination1 Let $\mathbf{w}$ be a vector in the vector space $V$. If $\mathbf{w}$ can be expressed as follows for vectors $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$ in $V$ and arbitrary constants $k_{1}, k_{2}, \cdots, k_{r}$, then $\mathbf{w}$ is called a linear combination of $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$. $$ \mathbf{w} = k_{1}\mathbf{v}_{1} + k_{2}\mathbf{v}_{2} + \cdots + k_{r}\mathbf{v}_{r} $$ Additionally, in this case, the constants $k_{1}, k_{2}, \cdots, k_{r}$ are referred to as the coefficients</description></item><item><title>Subspace of Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/285/</link><pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/285/</guid><description>Definition1 Let $W$ be a non-empty subset of the vector space $V$. If $W$ satisfies the definition of a vector space with respect to the addition and scalar multiplication defined in $V$, then $W$ is called a subspace of the vector space $V$, and is denoted as follows: $$ W \le V $$ Explanation To determine whether a subset $W$ of a vector space $V$ is a subspace of $V$,</description></item><item><title>Definition of Vector Space</title><link>https://freshrimpsushi.github.io/en/posts/282/</link><pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/282/</guid><description>Definition1 When the elements of a non-empty set $V$ satisfy the following ten rules for two operations, addition and scalar multiplication, $V$ is called a vector space over field2 $\mathbb{F}$, and the elements of $V$ are called vectors. For $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k, l \in \mathbb{F}$, (A1) If $\mathbf{u}, \mathbf{v}$ is an element of $V$, then $\mathbf{u}+\mathbf{v}$ is also an element of $V$. (A2) $\mathbf{u} + \mathbf{v}</description></item><item><title>Characteristics of Special Relativity due to Lorentz Transformation: Loss of Simultaneity</title><link>https://freshrimpsushi.github.io/en/posts/260/</link><pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/260/</guid><description>Characteristics of Lorentz Transformation The transformation between two coordinate systems in special relativity is different from classical transformation due to the principle that &amp;ldquo;the speed of light is the same for all observers&amp;rdquo;. Derived with this condition in mind, we get the Lorentz transformation. The Lorentz transformation introduces three new phenomena that do not appear in classical physics. Loss of simultaneity Time dilation Length contraction Loss of Simultaneity Among the</description></item><item><title>Implications of Special Relativity due to Lorentz Transformation: Length Contraction</title><link>https://freshrimpsushi.github.io/en/posts/263/</link><pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/263/</guid><description>Characteristics of the Lorentz Transformation In the theory of Special Relativity, the transformation between two coordinate systems is different from classical transformations. This is due to the fact that &amp;rsquo;the speed of light is the same for all observers&amp;rsquo;. Considering this condition, the Lorentz transformation was derived. As a result of the Lorentz transformation, there are three new phenomena that do not appear in classical physics. Loss of simultaneity Time</description></item><item><title>Matrix Rank, Nullity</title><link>https://freshrimpsushi.github.io/en/posts/3021/</link><pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/3021/</guid><description>Theorem1 The dimensions of the row space and column space of matrix $A$ are the same. Proof Let $R$ be the row echelon form matrix of $A$. Since basic row operations do not change the dimensions of the row space and column space of $A$, the following equation holds: $$ \begin{align*} \dim \big( \mathcal{R}(A) \big) &amp;amp;= \dim \big( \mathcal{R}(R) \big) \\ \dim \big( \mathcal{C}(A) \big) &amp;amp;= \dim \big( \mathcal{C}(R) \big)</description></item><item><title>Lorentz Transformation Derivation</title><link>https://freshrimpsushi.github.io/en/posts/251/</link><pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/251/</guid><description>Derivation The text might be a bit long, but it&amp;rsquo;s written very simply, so don&amp;rsquo;t be afraid to dive in. Let&amp;rsquo;s think about light (photon) moving in the plane of $xy$ inertial system (coordinate system) when $t=0$. It starts from the origin and is advancing at an angle of $\theta$ with the $x$ axis. The new transformation that will replace the Galilean transformation can be said to look like the</description></item><item><title>Relativity Theory and Lorentz Transformation</title><link>https://freshrimpsushi.github.io/en/posts/249/</link><pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/249/</guid><description>Buildup Relativity theory starts from the completion of electromagnetism. To say electromagnetism is complete means Maxwell finished the four partial differential equations for the electric field $\mathbf{E}$ and the magnetic field $\mathbf{B}$. From Maxwell&amp;rsquo;s equations, we learn that the speed of electromagnetic waves is equal to the speed of light. This leads us to the following two facts: Light is an electromagnetic wave. The speed of light is $\dfrac{1}{\sqrt{\epsilon_o \mu_o}}=300,000</description></item><item><title>World Line and Galilean Transformation</title><link>https://freshrimpsushi.github.io/en/posts/250/</link><pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/250/</guid><description>Definition The line that represents the track of a particle in space and time is called a world line. Description Let&amp;rsquo;s think only about a coordinate system moving at a constant speed in one direction. In the $A$ coordinate system, there is a particle at rest at the origin. The world line of this particle is as follows. And there is a $A^{\prime}$ coordinate system moving at a speed of</description></item><item><title>The Divergence of Curl is Always Zero</title><link>https://freshrimpsushi.github.io/en/posts/244/</link><pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/244/</guid><description>Formula The divergence of the curl of a vector function $\mathbf{A}$ is always $0$. $$ \nabla \cdot (\nabla \times \mathbf{A}) = 0 $$ Proof The curl of $\mathbf{A}$ is as follows. $$ \begin{align*} \nabla \times \mathbf{A} &amp;amp;= \begin{vmatrix} \hat{\mathbf{x}} &amp;amp; \hat{\mathbf{y}} &amp;amp; \hat{\mathbf{z}} \\ \displaystyle \frac{\partial}{\partial x} &amp;amp; \displaystyle \frac{\partial}{\partial y} &amp;amp; \displaystyle \frac{\partial}{\partial z} \\ A_{x} &amp;amp; A_{y} &amp;amp; A_{z} \end{vmatrix} \\ &amp;amp;= \hat{\mathbf{x}} \left( \frac{\partial A_{z}}{\partial y} -</description></item><item><title>Sphere's Moment of Inertia</title><link>https://freshrimpsushi.github.io/en/posts/240/</link><pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/240/</guid><description>Formula The moment of inertia of a sphere with radius $a$ and mass $m$ is as follows. $$ I=\frac{2}{5}ma^{2} $$ Proof The idea of finding the moment of inertia of a sphere is slightly different from other rigid bodies. The key idea is to think of the sphere as a sum of infinitely many discs, similar to the method of integration. Adding up the moment of inertia of these infinitely</description></item><item><title>Inertia Moments of Disks and Cylinders</title><link>https://freshrimpsushi.github.io/en/posts/239/</link><pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/239/</guid><description>Formula A disk with radius $a$ and mass $m$ has a moment of inertia perpendicular to the disk as $I=\dfrac{1}{2}ma^2$. parallel to the disk as $I=\dfrac{1}{4}ma^2$. Derivation When the axis of rotation passes through the center of the disk and is perpendicular to the disk Let $\rho$ be the mass per unit area. Then, the mass of the disk is $m=\rho \pi r^2$. Therefore, it follows that $$ dm=\rho \pi</description></item><item><title>Moment of Inertia of a Ring and Cylindrical Shell</title><link>https://freshrimpsushi.github.io/en/posts/238/</link><pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/238/</guid><description>Formulas The moment of inertia of a ring with radius $a$ and mass $m$, when the axis of rotation passes through the center of the ring, is: Perpendicular to the plane formed by the ring, it is $I=ma^{2}$. Parallel to the plane formed by the ring, it is $I=\dfrac{1}{2}ma^{2}$. Derivation Consider a thin, uniform circular ring (or cylindrical shell) with radius $a$ and mass $m$. There are cases where the</description></item><item><title>Parallel Axis Theorem</title><link>https://freshrimpsushi.github.io/en/posts/237/</link><pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/237/</guid><description>Parallel Axis Theorem The moment of inertia of a rigid body about any axis of rotation is equal to the sum of the moment of inertia about an axis parallel to it and passing through the center of mass and the product of the body&amp;rsquo;s mass and the square of the distance between the two axes. $$ \color{red}I=\color{blue}{I_{cm}}+\color{green}{md^{2}} $$ Proof Arbitrarily set the coordinate axis and let the moment of</description></item><item><title>Vertical Axis Theorem</title><link>https://freshrimpsushi.github.io/en/posts/236/</link><pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/236/</guid><description>Vertical Axis Theorem The moment of inertia about an axis perpendicular to a plane is equal to the sum of the moments of inertia about any two perpendicular axes lying in the plane and passing through the perpendicular axis. $$ \color{red}{I_{z}}=\color{blue}{I_{x}+I_{y}} $$ Proof $$ I_{z}=\sum\limits_{i} m_{i}{r_{i}}^{2} $$ By Pythagoras&amp;rsquo; theorem, since ${r_{i}}^{2}={x_{i}}^{2}+{y_{i}}^{2}$, substituting this into the above equation gives: $$ I_{z}=\sum\limits_{i} m_{i}({x_{i}}^{2}+{y_{i}}^{2})=\sum\limits_{i} m_{i}{x_{i}}^{2}+\sum\limits_{i} m_{i}{y_{i}}^{2} $$ $x$ is the distance from</description></item><item><title>Inertia Moment and Turning Radius</title><link>https://freshrimpsushi.github.io/en/posts/234/</link><pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/234/</guid><description>Moment of Inertia $$ \begin{align*} I &amp;amp;= \sum_{i} m_{i} {r_{i}}^2 \\ I &amp;amp;= \int r^2 dm \end{align*} $$ The moment of inertia is defined as the (mass of a particle)$\times$(distance from the rotation axis to the particle) and represents the physical quantity that indicates the characteristic of a body to continue rotating. Its symbol is $I$, which seems to be derived from the initial letter of the English word Inertia.</description></item><item><title>Inertia Moment of a Thin Rod</title><link>https://freshrimpsushi.github.io/en/posts/235/</link><pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/235/</guid><description>Formulas The moment of inertia for a rod with length $a$ and mass $m$ is: If the axis of rotation is at the end of the rod, it is $I=\dfrac{1}{3}ma^{2}$. If the axis of rotation is at the center of the rod, it is $I=\dfrac{1}{12}ma^{2}$. Derivation When the Axis of Rotation is at the End of the Rod If $\rho$ is defined as the mass per unit length, the mass</description></item><item><title>Curl of the Curl of Vector Functions</title><link>https://freshrimpsushi.github.io/en/posts/209/</link><pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/209/</guid><description>Formulas The curl of the curl of a vector function is as follows. $$ \nabla \times (\nabla \times \mathbf{A}) = \nabla (\nabla \cdot \mathbf{A}) - \nabla^2 \mathbf{A} $$ Explanation The first term, $\nabla(\nabla \cdot \mathbf{A})$, is the divergence of the gradient, which doesn&amp;rsquo;t have a specific name. The second term is important enough to have a name. $\nabla \cdot \nabla$ is called the Laplacian, specifically, the Laplacian of a vector</description></item><item><title>Finding the Speed of Electromagnetic Light from Maxwell's Equations</title><link>https://freshrimpsushi.github.io/en/posts/207/</link><pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/207/</guid><description>Formulas Vacuum Maxwell&amp;rsquo;s Equations $$ \begin{align} \nabla \cdot \mathbf{E} &amp;amp;= 0 \\[1em] \nabla \cdot \mathbf{B} &amp;amp;= 0 \\[1em] \nabla \times \mathbf{E} &amp;amp;= -\frac{\partial \mathbf{B}}{\partial t} \\[1em] \nabla \times \mathbf{B} &amp;amp;= \mu_{0}\epsilon_{0}\frac{\partial \mathbf{E}}{\partial t} \end{align} $$ One-dimensional wave equation $$ \frac{\partial^2 f}{\partial x^2}=\frac{1}{v^2}\frac{\partial^2 f}{\partial t^2} $$ Three-dimensional wave equation $$ \nabla ^2 f = \frac{1}{v^2}\frac{\partial ^2 f}{\partial t^2} $$ Derivation The goal is to derive a wave equation form from Maxwell&amp;rsquo;s</description></item><item><title>The Curl of a Gradient is Always Zero</title><link>https://freshrimpsushi.github.io/en/posts/208/</link><pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/208/</guid><description>Formulas The curl of the gradient of a scalar function eq1 is always eq2. eq1 Proof In Cartesian coordinates, the gradient of eq1 is as follows. eq2 When we compute the curl of eq4, it is as follows. eq3 Since the result is eq6 for all components, regardless of eq1, the rotation of the gradient is always eq2. ■</description></item><item><title>Euclidean Space</title><link>https://freshrimpsushi.github.io/en/posts/205/</link><pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/205/</guid><description>Definition For a natural number $n \in \mathbb{N}$, the Cartesian product $\mathbb{R}$ of the set of real numbers is called the Euclidean space. $$ \mathbb{R}^{n} = \mathbb{R} \times \cdots \times \mathbb{R} $$ $\mathbb{R}^{1}$ is referred to as real space or number line. $\mathbb{R}^{2}$ is called a plane. $\mathbb{R}^{3}$ is called a $3$-dimensional space. Here, $\mathbb{N} := \left\{ 1, 2, 3, \cdots \right\}$ means the set that includes all natural numbers.</description></item><item><title>The Importance of the Relative Phase of the Wave Function</title><link>https://freshrimpsushi.github.io/en/posts/201/</link><pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/201/</guid><description>설명 The wave function is often expressed as a complex exponential function as follows. $$ \psi = R e^{\i\theta} $$ At this time, in the equation, what has physical significance is not $\psi$, but $\left| \psi \right|^{2} = R^{2}$, so the value of the phase $\theta$ is not important and can be treated interchangeably. However, the story is different when the wave function is represented as the sum of</description></item><item><title>Commutator of Momentum and Position</title><link>https://freshrimpsushi.github.io/en/posts/200/</link><pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/200/</guid><description>Formula The commutator of position and momentum operator is given by the equation below. $$ \begin{align} [p, x] &amp;amp;= -\i \hbar \\ [x, p] &amp;amp;= \i \hbar \end{align} $$ This equation is termed the canonical commutation relation. The commutator of the square of position and momentum is as follows. $$ \begin{align} [x^{2}, p] &amp;amp;= 2 \i \hbar x \\ [p, x^{2}] &amp;amp;= -2 \i \hbar x \end{align} $$ Explanation Since</description></item><item><title>Prove that the expectation value of momentum is always a real number.</title><link>https://freshrimpsushi.github.io/en/posts/199/</link><pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/199/</guid><description>Summary The expectation value of the momentum operator $\langle p \rangle$ is always a real number. Explanation In fact, not only the momentum operator but all Hermitian operators have eigenvalues that are always real. Proof The expectation value of momentum is as follows. $$ \displaystyle \langle p \rangle = \int \psi^{\ast} \left( \frac{\hbar}{i}\frac{\partial}{\partial x} \right) \psi dx $$ Additionally, the complex conjugate of the expectation value of momentum is as</description></item><item><title>Electrons Cannot Be Constituents of the Nucleus</title><link>https://freshrimpsushi.github.io/en/posts/184/</link><pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/184/</guid><description>Theorem An electron cannot be a component of a nucleus. Explanation $10^{-14}\mathrm{m}$ A nucleus with a scale of $1\mathrm{MeV} \sim10\mathrm{MeV}$ emits an electron with energy in the range of $1\mathrm{MeV} \sim10\mathrm{MeV}$. In the early days of nuclear physics, it was believed that electrons existed within the nucleus. By using the uncertainty principle, it can be shown that an electron with such energy cannot be confined within the nucleus. Proof By</description></item><item><title>Momentum operator in quantum mechanics</title><link>https://freshrimpsushi.github.io/en/posts/100/</link><pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/100/</guid><description>Definition In quantum mechanics, the momentum operator is as follows: $$ P = \frac{\hbar}{\i}\frac{\partial}{\partial x} = -\i\hbar \dfrac{\partial }{\partial x} $$ Description The momentum operator is a function that allows one to calculate the momentum of a wave function. When a wave function with momentum $p = \hbar k$ is substituted, it satisfies the following equation. $$ P \psi = p \psi $$ In the case of dimensions higher than</description></item><item><title>Compton scattering</title><link>https://freshrimpsushi.github.io/en/posts/182/</link><pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/182/</guid><description>Formula Let $\lambda$ be the wavelength of the incident light and $\lambda^{\prime}$ be the wavelength of the scattered photon. The following equation then holds true: $$ \lambda^{\prime} -\lambda = \frac{h}{m_{e}c}(1-\cos\theta) $$ Here, $h$ is Planck&amp;rsquo;s constant, $m_{e}$ is the mass of the electron, $c$ is the speed of light, and $\theta$ is the scattering angle. In terms of energy, we have: $$ \cos \theta=1-\frac{m_{e}c^{2}(E-E^{\prime})}{E^{\prime}E} $$ Explanation Compton scattering1 refers to</description></item><item><title>The rest mass of a photon is zero.</title><link>https://freshrimpsushi.github.io/en/posts/183/</link><pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/183/</guid><description>Summary Let the speed of the photon be $c = 299,792,458 \mathrm{m/s}$. Then the rest mass of the photon is $0$. Proof 1. Relationship between relativistic energy, momentum, and speed $$p=\gamma m_{0} v$$ $$E=\gamma m_{0} c^2$$ $$\implies \gamma m_{0}=\dfrac{E}{c^2}$$ By solving these equations simultaneously, $$p=\dfrac{E}{c^2}v$$ $$\implies v=\dfrac{pc^2}{E}$$ 2. Relativistic relationship between energy and momentum of a particle $$E=\sqrt{{m_{0}}^2c^4+p^2c^2}$$ 3. By 1 and 2 $$v=\frac{pc^2}{\sqrt{{m_{0}}^2c^4+p^2c^2}}$$ At this point, since the speed</description></item><item><title>Velocity and Acceleration in Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/174/</link><pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/174/</guid><description>Velocity and Acceleration in Cartesian Coordinates $$ \begin{align*} \mathbf{r} &amp;amp;= x \hat{\mathbf{x}} + y \hat{\mathbf{y}} + z \hat{\mathbf{z}} \\ \mathbf{v} &amp;amp;= \dot{\mathbf{r}} = \dot{x} \hat{\mathbf{x}} + \dot{y} \hat{\mathbf{y}} + \dot{z} \hat{\mathbf{z}} \\ \mathbf{a} &amp;amp;= \dot{\mathbf{v}} = \ddot{\mathbf{r}} = \ddot{x} \hat{\mathbf{x}} + \ddot{y} \hat{\mathbf{y}} +\ddot{z}\hat{\mathbf{z}} \end{align*} $$ Derivation Determining velocity and acceleration in a Cartesian coordinate system is straightforward. Velocity Differentiating $\mathbf{r}$ with respect to $t$ yields the following. $$ \mathbf{v}=\frac{d}{dt}(x\hat{\mathbf{x}}</description></item><item><title>Equation of the Tangent to a Circle with Slope m</title><link>https://freshrimpsushi.github.io/en/posts/172/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/172/</guid><description>Formula The equation of the tangent line to the circle $x^{2}+y^{2}=r^{2}$ with slope $m$ is as follows. $$ y=mx \pm r\sqrt{m^{2}+1} $$ Proof Let&amp;rsquo;s denote the equation of the line with slope $m$ as $y=mx+n$. Substituting into the equation of the circle and rearranging for x, we get $$ \begin{align*} x^2+(mx+n)^2 =&amp;amp;\ r^2 \\ x^2+m^2x^2+2mnx+n^2-r^2 =&amp;amp;\ 0 \\ (1+m^2)x^2+2mnx+n^2-r^2 =&amp;amp;\ 0 \end{align*} $$ Since the circle and the line are</description></item><item><title>Finding the Equation of the Tangent Line at a Point on a Circle</title><link>https://freshrimpsushi.github.io/en/posts/173/</link><pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/173/</guid><description>Explanation Let&amp;rsquo;s find the equation of the tangent line at a point $(x_{1},y_{1})$ on the circle $x^2+y^2=r^2$. This can be divided into cases when $y_{1}\neq 0$ and when $y_{1}=0$. $y_{1}\neq 0$ The slope from the center of the circle to the tangent point is $\dfrac{y_{1}}{x_{1}}$. Since the product of the slopes of two perpendicular lines is -1, the slope of the tangent line is $-\dfrac{x_{1}}{y_{1}}$. The equation of the line</description></item><item><title>Multiplication Formula Table</title><link>https://freshrimpsushi.github.io/en/posts/171/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/171/</guid><description>Overview Introducing commonly used multiplication formulas. Formulas $$ \begin{align} (a+b)^2 &amp;amp;=a^2+2ab+b^2 \\ (a-b)^2 &amp;amp;= a^2-2ab+b^2 \end{align} $$ $$ \begin{equation} (a+b)(a-b)=a^2-b^2 \end{equation} $$ $$ \begin{align} (a+b)^3 &amp;amp;= a^3+3a^2b+3ab^2+b^3 \\ (a-b)^3 &amp;amp;= a^3-3a^2b+3ab^2-b^3 \end{align} $$ $$ \begin{equation} (a+b+c)^2=a^2+b^2+c^2+2ab+2bc+2ca \end{equation} $$ $$ \begin{align} (a+b)(a^2-ab+b^2) &amp;amp;= a^3+b^3 \\ (a-b)(a^2+ab+b^2) &amp;amp;= a^3-b^3 \end{align} $$ Proof (1), (2) $$ \begin{align*} (a \pm b)^{2} &amp;amp;= (a \pm b)(a \pm b) \\ &amp;amp;= a^{2} \pm ab \pm ba</description></item><item><title>Velocity and Acceleration in a Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/164/</link><pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/164/</guid><description>Spherical Coordinates: Velocity and Acceleration $$ \begin{align*} \mathbf{v} &amp;amp;=\dot{r} \hat {\mathbf{r}} +r \dot{\theta} \hat{ \boldsymbol{\theta}}+ r \dot{\phi} \sin{\theta} \hat{ \boldsymbol{\phi}} \\ \mathbf{a} &amp;amp;= (\ddot{r}-r\dot\theta^2-r\dot\phi^2\sin^2\theta)\hat{\mathbf{r}}+(r\ddot\theta+2\dot{r}\dot\theta-r\dot\phi^2\sin\theta\cos\theta)\hat{\boldsymbol{\theta}} \\ &amp;amp;\quad+(r\ddot\phi\sin\theta+2\dot{r}\dot\phi\sin\theta+2r\dot\theta\dot\phi\cos\theta)\hat{\boldsymbol{\phi}} \end{align*} $$ Derivation Unit vectors in spherical coordinates are as follows. $$ \begin{align*} \hat{\mathbf{r}} &amp;amp;= \cos \phi \sin \theta \hat{\mathbf{x}} + \sin \phi \sin \theta \hat{\mathbf{y}} + \cos\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\theta}} &amp;amp;= \cos\phi \cos\theta \hat{\mathbf{x}} + \sin\phi \cos\theta \hat{\mathbf{y}} - \sin\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\phi}} &amp;amp;= -\sin\phi</description></item><item><title>Velocity and Acceleration in Cylindrical Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/163/</link><pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/163/</guid><description>Velocity and Acceleration in Cylindrical Coordinates $$ \begin{align*} \mathbf{v}&amp;amp;=\dot{r} \hat{\mathbf{r}} + r \dot{\phi} \hat{\boldsymbol{\phi}}+\dot{z} \hat{\mathbf{z}} \\ \mathbf{a} &amp;amp;= (\ddot r -r\dot{\phi} ^2)\hat{\mathbf{r}} + (2\dot{r} \dot{\phi} + r\ddot{\phi})\hat{\boldsymbol{\phi}} + \ddot{z}\hat{\mathbf{z}} \end{align*} $$ Derivation In cylindrical coordinates, the unit vectors are as follows. $$ \begin{align*} \boldsymbol{\rho}&amp;amp;=x\hat{\mathbf{x}}+y \hat{\mathbf{y}} +z\hat{\mathbf{z}}=r\hat{\mathbf{r}} +z\hat{\mathbf{z}} \\ \hat{\mathbf{r}} &amp;amp;= \hat{\mathbf{r}}(\phi) = \cos\phi \hat{\mathbf{x}} + \sin\phi \hat{\mathbf{y}} \\ \hat{\boldsymbol{\phi}} &amp;amp;= \hat{\mathbf{r}}(\phi+\pi/2) = -\sin\phi \hat{\mathbf{x}} + \cos\phi \hat{\mathbf{y}} \\ \hat{\mathbf{z}} &amp;amp;=</description></item><item><title>Velocity and Acceleration in Polar Coordinates</title><link>https://freshrimpsushi.github.io/en/posts/158/</link><pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/158/</guid><description>Velocity and Acceleration in Polar Coordinates $$ \begin{align*} \mathbf{v}&amp;amp;=\dot{r} \hat{\mathbf{r}} + r \dot{\theta} \hat{\boldsymbol{\theta}} \\ \mathbf{a}&amp;amp;= (\ddot r -r\dot{\theta} ^2)\hat{\mathbf{r}} + (2\dot{r} \dot{\theta} + r\ddot{\theta})\hat{\boldsymbol{\theta}} \end{align*} $$ Derivation In the polar coordinate system, unit vectors can be described as follows. $$ \begin{align*} &amp;amp;&amp;amp; \mathbf{r}&amp;amp;=r\hat{\mathbf{r}}=x\hat{\mathbf{x}} + y \hat{\mathbf{y}} \\ \implies &amp;amp;&amp;amp; \hat{\mathbf{r}} &amp;amp;= \frac{x}{r}\hat{\mathbf{x}} +\frac{y}{r} \hat{\mathbf{y}}=\cos\theta \hat{\mathbf{x}} + \sin\theta \hat{\mathbf{y}} = \hat{\mathbf{r}} (\theta) \\ {} \\ &amp;amp;&amp;amp; \hat \theta &amp;amp;= \hat{\mathbf{r}}(\theta+\pi/2)=</description></item><item><title>Unit Vectors of the Spherical Coordinate System Expressed in Terms of Unit Vectors of the Cartesian Coordinate System</title><link>https://freshrimpsushi.github.io/en/posts/152/</link><pubDate>Sun, 16 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/152/</guid><description>Spherical Coordinate System&amp;rsquo;s Unit Vectors $$ \begin{align*} \hat{\mathbf{r}} &amp;amp;= \cos\phi \sin\theta\hat{\mathbf{x}} + \sin\phi \sin\theta\hat{\mathbf{y}} + \cos\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\theta}} &amp;amp;= \cos\phi \cos\theta \hat{\mathbf{x}} + \sin\phi \cos\theta \hat{\mathbf{y}} - \sin\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\phi}} &amp;amp;= -\sin\phi \hat{\mathbf{x}} + \cos\phi \hat{\mathbf{y}} \end{align*} $$ Derivation First, calculate $\hat{\mathbf{r}}$ and then use it to derive the other two. Radial Direction Unit Vector $\hat{\mathbf{r}}$ $$ \hat{\mathbf{r}}=r\hat{\mathbf{r}}=x\hat{\mathbf{x}}+y\hat{\mathbf{y}}+z\hat{\mathbf{z}} $$ Therefore, dividing both sides by $r$ gives: $$ \begin{align*} \hat{\mathbf{r}}&amp;amp;=\frac{x}{r}\hat{\mathbf{x}}+\frac{y}{r}\hat{\mathbf{y}}+\frac{z}{r}\hat{\mathbf{z}} \\</description></item><item><title>Scalar Triple Product</title><link>https://freshrimpsushi.github.io/en/posts/144/</link><pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/144/</guid><description>Definition The following expression is called the scalar triple product. $$ \mathbf{A}\cdot (\mathbf{B} \times \mathbf{C} ) $$ Explanation A scalar triple product is an operation involving the product of three vectors, where the result is a scalar. The operation resulting in a vector is called vector triple product. To get a scalar result, one must first cross multiply two vectors to produce another vector and then dot multiply it with</description></item><item><title>Finding Internal and External Division Points on a Line</title><link>https://freshrimpsushi.github.io/en/posts/138/</link><pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/138/</guid><description>Theorem The coordinates of the point $A(x_{1})$ and the point $B(x_{2})$ on the number line, dividing internally at $m:n$, are $\displaystyle x=\frac{mx_{2}+nx_{1}}{m+n}$, and the coordinates of the point that divides externally at $m:n$ are $\displaystyle x=\frac{mx_{2}-nx_{1}}{m-n}$. Explanation By examining the formulas for the internal and external division point, one can see that only the signs are different. There is no need to memorize both expressions; simply memorize the internal division</description></item><item><title>Gradient of the Magnitude of Separation Vectors</title><link>https://freshrimpsushi.github.io/en/posts/142/</link><pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/142/</guid><description>Formulas The square of the magnitude of the separation vector $\bcR$ and the gradient of $\cR ^{n}$ are as follows. $$ \nabla (\cR^n)=n\cR^{n-1}\crH $$ Explanation It is calculated in the same way as the derivative of a polynomial function, and then just attach the unit vector $\crH$. Since the separation vector is $\bcR=\mathbf{r}-\mathbf{r}^{\prime}$, it has variables $(x,y,z)$ and $(x^{\prime},y^{\prime},z^{\prime})$. Therefore, attention must be paid when differentiating. Gradients for coordinates with</description></item><item><title>Separation Vector</title><link>https://freshrimpsushi.github.io/en/posts/141/</link><pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/141/</guid><description>Definition1 The vector from the source point to the observation point is called the separation vector. $$ \bcR = \mathbf{r} - \mathbf{r}^{\prime} $$ Description Source vector $\mathbf{r}^{\prime}$: The place where there is a charge or current. That is, it represents the coordinates of the origin of the electromagnetic field. Position vector $\mathbf{r}$: Represents the coordinates of where the electric field $\mathbf{E}$ or magnetic field $\mathbf{B}$ is measured. Separation vector $\bcR$:</description></item><item><title>Prove that the Product of the Slopes of Two Perpendicular Lines is Always -1</title><link>https://freshrimpsushi.github.io/en/posts/111/</link><pubDate>Sat, 20 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/111/</guid><description>Theorem The product of the slopes of two perpendicular lines is always $-1$. Explanation This is a fact that can be very useful in many problems. We introduce two methods of proof. Proof 1 Use Pythagoras&amp;rsquo; theorem. See the figure below. Suppose the slopes of two perpendicular lines are $a$, $a^{\prime}$. Then, considering the right triangle $\triangle OAA^{\prime}$ as shown above, we obtain the following result by Pythagoras&amp;rsquo; theorem. $$</description></item><item><title>Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/103/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/103/</guid><description>Definition A function that satisfies the following two conditions is called the Dirac delta function. $$ \delta (x) = \begin{cases} 0, &amp;amp; x\neq 0 \\ \infty , &amp;amp; x=0 \end{cases} $$ $$ \int_{-\infty}^{\infty}{\delta (x) dx}=1 $$ Description ※Be careful not to confuse it with the Kronecker delta. In engineering, it is called the unit impulse function. Strictly speaking, mathematically, the Dirac delta function is not a function because</description></item><item><title>Properties of the Dirac Delta Function</title><link>https://freshrimpsushi.github.io/en/posts/104/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/104/</guid><description>Properties $$ \begin{equation} \delta (-x) =\delta (x) \end{equation} $$ $$ \begin{equation} \delta (kx)= \frac{1}{|k|} \delta (x) \end{equation} $$ Proof Proof of $(1)$ Substituting $\int_{-\infty }^ { \infty } f(x) \delta (-x) dx$ with $-x \equiv y$ gets us $x=-y$ and $dx=-dy$, $$ \begin{align*} \int_{-\infty } ^{ \infty } f(x) \delta (-x) dx =&amp;amp;\ -\int_{ \infty }^{-\infty} f(-y) \delta (y) dy \\ =&amp;amp;\ \int_{-\infty } ^{\infty } f(-y) \delta (y) dy</description></item><item><title>The Period of Simple Pendulum Motion is Independent of the Pendulum's Mass</title><link>https://freshrimpsushi.github.io/en/posts/102/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/102/</guid><description>Theorem The period $T$ of a simple pendulum motion is independent of the mass of the pendulum $m$. Description Therefore, the period $T$ of a simple pendulum motion is independent of the pendulum&amp;rsquo;s mass, the amplitude&amp;rsquo;s size, etc., and depends solely on the pendulum&amp;rsquo;s length and the acceleration due to gravity. Proof The restoring force of the pendulum is as follows: $$ F=-mg\sin\theta $$ Since $x=l\theta$, when $\theta$ is sufficiently</description></item><item><title>규격화된 파동함수의 상태는 시간의 변화에 무관하다</title><link>https://freshrimpsushi.github.io/en/posts/101/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/101/</guid><description>Theorem1 A normalized wave function remains in a normalized state even as time changes. Explanation Let us assume the wave function is normalized at time $t=0$. According to the theorem, it is guaranteed to remain in a normalized state as time progresses. This is a very crucial fact that allows us to treat the wave function as a probability density function. Proof Strategy: To show that it remains constant over</description></item><item><title>Einstein Notation</title><link>https://freshrimpsushi.github.io/en/posts/90/</link><pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/90/</guid><description>Notation The summation sign $\sum$ is omitted when a subscript is repeated two or more times. Description Also referred to as the Einstein summation convention. It&amp;rsquo;s not really a formula but rather a rule. When doing vector calculations, there are often cases where one needs to write the summation sign $\sum$ multiple times in a single formula, which can make the equation look cluttered and is very annoying to write</description></item><item><title>Parabolic Motion: Horizontal Range and Maximum Height Angle</title><link>https://freshrimpsushi.github.io/en/posts/89/</link><pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/89/</guid><description>Definition1 2 An object launched with angle $\alpha$ and initial speed $v_{0}$ performs a motion known as parabolic motion. Description It&amp;rsquo;s also referred to as projectile motion. Typically, external forces like air resistance are ignored, so the motion is uniform in the horizontal direction and free fall in the vertical direction. Analysis Motion in the $x$ direction (horizontal) is independent of gravity, while motion in the $y$ direction (vertical) is</description></item><item><title>Product of Two Levi-Civita Symbols</title><link>https://freshrimpsushi.github.io/en/posts/88/</link><pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/88/</guid><description>Theorem The $\epsilon_{ijk}$, defined as follows, is referred to as the Levi-Civita symbol. $$ \epsilon_{ijk} = \begin{cases} +1 &amp;amp; \text{if} \ \epsilon_{123}, \epsilon_{231}, \epsilon_{312} \\ -1 &amp;amp; \text{if} \ \epsilon_{132}, \epsilon_{213}, \epsilon_{321} \\ 0 &amp;amp; \text{if} \ i=j \ \text{or} \ j=k \ \text{or} \ k=i \end{cases} $$ The $\delta_{ij}$, defined as follows, is referred to as the Kronecker delta. $$ \delta_{ij} := \begin{cases} 1,&amp;amp;i=j \\ 0, &amp;amp; i\ne j</description></item><item><title>The minimum energy of a hydrogen atom in quantum mechanics</title><link>https://freshrimpsushi.github.io/en/posts/92/</link><pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/92/</guid><description>정리 수소원자의 최소 에너지는 다음과 같다. $$ E_{min}=-\frac{1}{2}mc^2\alpha^{2} $$ $m$은 수소원자의 질량, $c$는 광속, $\alpha$는 미세구조상수이다. 설명 여기서 $\alph</description></item><item><title>Kronecker Delta</title><link>https://freshrimpsushi.github.io/en/posts/84/</link><pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/84/</guid><description>Definition We define $\delta_{ij}$ as the Kronecker delta. $$ \delta_{ij} := \begin{cases} 1,&amp;amp;i=j \\ 0, &amp;amp; i\ne j \end{cases} $$ Explanation The Kronecker delta is used in many places, primarily to highlight the desired components (elements, possibilities, etc.) among all possible options. Physics students often encounter it in the context of dot products. If this concept isn&amp;rsquo;t immediately clear, consider the following example: Example Suppose we are given two vectors</description></item><item><title>Levi-Civita Symbol</title><link>https://freshrimpsushi.github.io/en/posts/83/</link><pubDate>Tue, 25 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/83/</guid><description>Definition The $\epsilon_{ijk}$ defined as follows is called the Levi-Civita symbol. $$ \epsilon_{ijk} = \begin{cases} +1 &amp;amp; \text{if} \ \epsilon_{123}, \epsilon_{231}, \epsilon_{312} \\ -1 &amp;amp; \text{if} \ \epsilon_{132}, \epsilon_{213}, \epsilon_{321} \\ 0 &amp;amp; \text{if} \ i=j \ \text{or} \ j=k \ \text{or} \ k=i \end{cases} $$ Description While the Kronecker delta only considers whether the indices are equal, the Levi-Civita symbol, as shown in its definition, is also affected by</description></item><item><title>Features of Special Relativity due to Lorentz Transformation: Time Dilation</title><link>https://freshrimpsushi.github.io/en/posts/75/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/75/</guid><description>Characteristics of Lorentz Transformation In special relativity, the transformation between two coordinate systems differs from the classical transformation. This is because &amp;rsquo;the speed of light is the same for all observers&amp;rsquo;. Taking this condition into account leads to the derivation of the Lorentz transformation. The Lorentz transformation introduces three new phenomena that do not appear in classical physics. Loss of simultaneity Time dilation Length contraction Time Dilation Simply put, time</description></item><item><title>Momentum and Impulse Relationship</title><link>https://freshrimpsushi.github.io/en/posts/73/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/73/</guid><description>Definition Momentum The product of an object&amp;rsquo;s mass and velocity is called momentum, denoted by $p$. While in high school physics, velocity $v$ is often used to represent the state of motion of an object, in college physics, momentum $p$ is more commonly used. $$ \vec{p}=m\vec{v}[kg\cdot m/s] $$ Since velocity $v$ is a vector, so is momentum. Since mass $m$ is always positive, the direction of velocity and momentum are</description></item><item><title>Uniform Acceleration Linear Motion and its Graphs</title><link>https://freshrimpsushi.github.io/en/posts/72/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/72/</guid><description>Definition When an object&amp;rsquo;s acceleration does not change over time $t$, it is said to undergo uniform acceleration. $$ a(t)=a $$ Uniformly accelerated motion refers to moving in a straight line with constant, unchanging acceleration. What matters here is whether the acceleration $a$ is positive or negative: if $a&amp;gt;0$, it will move faster in the initial direction, and if $a&amp;lt;0$, it will slow down to a velocity of 0 and</description></item><item><title>Uniform Motion and Graphs</title><link>https://freshrimpsushi.github.io/en/posts/74/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/74/</guid><description>Description Uniform motion, when the term is dissected, stands for &amp;lsquo;uniform (equal) + speed&amp;rsquo;. This implies motion where the speed, which is a vector, is kept constant. Both the magnitude (velocity) and direction must remain constant for it to be considered uniform motion. In other words, uniform motion refers to motion where both the speed and direction are constant. Graph The graph of uniform motion is as follows: In the</description></item><item><title>Vector Triple Product, BAC-CAB Rule</title><link>https://freshrimpsushi.github.io/en/posts/71/</link><pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/71/</guid><description>Formulas $$ \mathbf{A} \times (\mathbf{B} \times \mathbf{C} ) = \mathbf{B}(\mathbf{A} \cdot \mathbf{C} )-\mathbf{C}(\mathbf{A} \cdot \mathbf{B}) $$</description></item></channel></rss>