<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>J_s on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/j_/</link>
    <description>Recent content in J_s on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 24 Jan 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/j_/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Absolutely Continuous Real Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/3542/</link>
      <pubDate>Wed, 24 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3542/</guid>
      <description>Definition1 Let&amp;rsquo;s say a function $f : \mathbb{R} \to \mathbb{R}( \text{or } \mathbb{C})$ is given. If for any finite number of mutually disjoint intervals $(a_{i}, b_{i}) \sub [a,b]$, the following condition is satisfied, then it is said to be absolutely continuousabsolutely continuous on $[a, b]$. $$ \forall \epsilon \gt 0 \quad \exist \delta \gt 0 \text{ such that } \sum\limits_{i=1}^{N} (b_{i} - a_{i}) \lt \delta \implies \sum\limits_{i=1}^{N} \left| f(b_{j}) -</description>
    </item>
    <item>
      <title>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</title>
      <link>https://freshrimpsushi.github.io/en/posts/3529/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3529/</guid>
      <description>Overview1 2 Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam. Explanation In gradient descent, the learning rate learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\alpha$, $\eta$, and determines how much of the gradient is taken into account when updating the</description>
    </item>
    <item>
      <title>Momentum Method in Gradient Descent</title>
      <link>https://freshrimpsushi.github.io/en/posts/3528/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3528/</guid>
      <description>Overview1 2 In gradient descent, the momentum technique involves using all previous gradients when updating parameters. This is its essence and the end of the explanation. However, explanations involving strange update equations, the motivation from physics&amp;rsquo; momentum, setting the mass to $1$ and the initial velocity to $0$, only complicate understanding. This article explains the momentum technique as plainly as possible. Build-Up Let&amp;rsquo;s denote the parameters as $\boldsymbol{\theta}$ and the</description>
    </item>
    <item>
      <title>자기장의 기호로 B를 사용하는 이유</title>
      <link>https://freshrimpsushi.github.io/en/posts/3523/</link>
      <pubDate>Sun, 17 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3523/</guid>
      <description>Question Electromagnetism is literally the study of electric fields $\mathbf{E}$ and magnetic fields $\mathbf{B}$. While studying electromagnetism, one might have wondered the following at least once. Why is the symbol for magnetic fields $\mathbf{B}$ used? It&#39;s understandable that the electric field is $\mathbf{E}$, derived from the Electric field, but why is the magnetic field $\mathbf{B}$ when it should be from Magnetic field? This notation might feel oddly placed, and it&#39;s</description>
    </item>
    <item>
      <title>Monte Carlo Integration</title>
      <link>https://freshrimpsushi.github.io/en/posts/3515/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3515/</guid>
      <description>Overview Monte Carlo integration is a numerical approximation method used when calculating the integral of a given function is difficult. Consider the following scenario: For an integrable function $f$ in the given $[0, 1]$or generally $[0, 1]^{n}$, we know the formula of $f(x)$ but calculating its integral is not straightforward. However, we want to calculate the integral $I[f]$ of $f$. $$ \begin{equation} I[f] = \int_{[0,1]} f(x) dx \end{equation} $$ Definition</description>
    </item>
    <item>
      <title>How to Neatly Print without Axes, Scales, etc. in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3501/</link>
      <pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3501/</guid>
      <description>Code Plots.jl essentially outputs everything including grids, ticks, axes, and color bars by default, but if you want to make it clean without these, you can add the following options. colorbar=:none: Removes the color bar. showaxis = false: Removes the axes and ticks. grid=false: Removes the background grid. ticks=false: Removes both background grid and ticks. framestyle=:none: Removes both background grid and axes. using Plots surface(L, title=&amp;#34;default&amp;#34;) surface(L, title=&amp;#34;colorbar=:none&amp;#34;, colorbar=:none) surface(L,</description>
    </item>
    <item>
      <title>How to Create a Meshgrid in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3500/</link>
      <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3500/</guid>
      <description>Summary There is no direct equivalent to the meshgrid() function used in Python and MATLAB. If you only want to obtain the function values on a grid, there is a simpler method that does not require creating a grid. Code 2D Multiplying a column vector by a row vector gives the same result as taking the Kronecker product of a column vector and a row vector. U(t,x) = si</description>
    </item>
    <item>
      <title>Broadcasting of Multivariable Functions in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3499/</link>
      <pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3499/</guid>
      <description>Overview Introducing how to broadcast multivariable functions in Julia. Like in Python, you can create a meshgrid, or you can easily calculate by creating vectors for each dimension. Bivariate Functions $$ u(t,x) = \sin(\pi x) e^{-\pi^{2}t} $$ To plot the function $(t,x) \in [0, 0.35] \times [-1,1]$ as above, the function values can be calculated like this: x = LinRange(-1., 1, 100) t = LinRange(0., 0.35, 200)&amp;#39; u1 = @.</description>
    </item>
    <item>
      <title>The Fast Fourier Transform Algorithm</title>
      <link>https://freshrimpsushi.github.io/en/posts/3492/</link>
      <pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3492/</guid>
      <description>Overview1 The Discrete Fourier Transform (DFT), when computed naively following its mathematical definition, has a time complexity of $\mathcal{O}(N^{2})$. However, by using the algorithm described below, the time complexity can be reduced to $\mathcal{O}(N\log_{2}N)$. This efficient computation method of the Discrete Fourier Transform is known as the Fast Fourier Transform (FFT). Buildup Let&amp;rsquo;s define multiplying two numbers and then adding them to another number as one operation. To compute the</description>
    </item>
    <item>
      <title>Specifying the Color of Axes, Axis Names, Ticks, and Tick Values in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3490/</link>
      <pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3490/</guid>
      <description>Summary The keywords related to specifying the color of axes and ticks in Plots.jl are as follows. Keyword Name Function guidefontcolor Specify axis name color foreground_color_border, fgcolor_border Specify axis color foreground_color_axis, fgcolor_axis Specify tick color foreground_color_text, fgcolor_text Specify tick value color Adding x_ or y_ in front of the keyword name applies it to the respective axis only. Code1 Axis Names The keyword to specify the color of axis names</description>
    </item>
    <item>
      <title>Flux-PyTorch-TensorFlow Cheat Sheet</title>
      <link>https://freshrimpsushi.github.io/en/posts/3489/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3489/</guid>
      <description>Overview This document organizes code that performs the same functions in Flux, PyTorch, and TensorFlow. Julia-Matlab-Python-R Cheat Sheet Let&amp;rsquo;s assume the following environment for Flux. using Flux Let&amp;rsquo;s assume the following environment for PyTorch. import torch import torch.nn as nn import torch.nn.functional as F Let&amp;rsquo;s assume the following environment for TensorFlow. import tensorflow as tf from tensorflow import keras 1-Dimensional Tensor 줄리아Julia 파</description>
    </item>
    <item>
      <title>Summary of Measure Theory and Probability Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/3473/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3473/</guid>
      <description>Overview This is a summary of definitions and concepts for those who have already studied measure theory and probability. It is intended to be viewed when definitions are confusing or unrecognizable, and when a general review is needed. Measure Theory Algebras An algebra of sets on nonempty set $X$ is a nonempty collection $\mathcal{A}$ of subsets of $X$ is colsed under finite unions ans complements. $\sigma$-algebra is an algebra that</description>
    </item>
    <item>
      <title>Sampling Randomly from a Given Distribution in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3463/</link>
      <pubDate>Sat, 19 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3463/</guid>
      <description>설명 Using the Distributions.jl package, you can randomly sample from a given distribution.</description>
    </item>
    <item>
      <title>Sampling Randomly in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3462/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3462/</guid>
      <description>Description1 In Julia, the function for random sampling is as follows: rand([rng=default_rng()], [S], [dims...]) rng stands for Random Number Generator, which specifies the random number generation algorithm. If you don&amp;rsquo;t understand what this means, it&amp;rsquo;s okay to leave it untouched. S likely stands for Set, and it is a variable that specifies the set from which the random sampling will occur. The variables that can be input for S include</description>
    </item>
    <item>
      <title>CSS color name tags</title>
      <link>https://freshrimpsushi.github.io/en/posts/3459/</link>
      <pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3459/</guid>
      <description>Overview1 140+ CSS color palettes with names. Code</description>
    </item>
    <item>
      <title>General Linear Group</title>
      <link>https://freshrimpsushi.github.io/en/posts/3450/</link>
      <pubDate>Sat, 22 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3450/</guid>
      <description>Definition The set of real invertible $n \times n$ matrices is denoted by $\mathrm{GL}(n, \mathbb{R})$ or $\mathrm{GL}_{n}(\mathbb{R})$ and is called the general linear group of degree $n$general linear group of degree $n$. $$ \mathrm{GL}(n, \mathbb{R}) := \left\{ n \times n \text{ invertible matrix} \right\} = M_{n \times n}(\mathbb{R}) \setminus {\left\{ A \in M_{n \times n}(\mathbb{R}) : \det{A} = 0 \right\}} $$ Explanation Since it consists only of invertible matrices, it</description>
    </item>
    <item>
      <title>MNIST Database</title>
      <link>https://freshrimpsushi.github.io/en/posts/3444/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3444/</guid>
      <description>Overview1 $$ \includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$ The MNISTmodified national institute of standards and technology database refers to a dataset of digit handwriting from American high school students and Census Bureau employees. It is commonly known as [MNIST]. Official Website Description This dataset is frequently used as an example for beginners in machine learning/deep learning. NIST originally collected handwritten data in the following format for the evaluation of character recognition technology for automated</description>
    </item>
    <item>
      <title>How to Use Fast Fourier Transform (FFT) in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3440/</link>
      <pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3440/</guid>
      <description>Overview 1 2 The Fastest Fourier Transform in the West (FFTW) is a software library developed by Matteo Frigo and Steven G. Johnson at the Massachusetts Institute of Technology (MIT) for computing the Discrete Fourier Transform. While there exists a Julia package named AbstractFFTs.jl for FFT implementation, it is not intended to be used on its own but rather to aid in the implementation of fast Fourier transforms, such as</description>
    </item>
    <item>
      <title>How to Change Basic Data Types in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3439/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3439/</guid>
      <description>Overview In fields like machine learning, 32-bit floating point numbers are used instead of 64-bit ones for improving computation speed and saving memory. Therefore, in PyTorch, when tensors are created, their data type is fundamentally 32-bit floating point numbers by default. In Julia, there&amp;rsquo;s a machine learning package called Flux.jl, which takes Julia&amp;rsquo;s standard arrays as input for the neural networks it implements. The fact that it does not use</description>
    </item>
    <item>
      <title>Exchange Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3429/</link>
      <pubDate>Sat, 10 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3429/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Bit: Unit of Information Classical Computer</title>
      <link>https://freshrimpsushi.github.io/en/posts/3422/</link>
      <pubDate>Sat, 27 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3422/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Fredkin/CSWAP Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3412/</link>
      <pubDate>Sun, 07 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3412/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Toffoli/CCNOT Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3411/</link>
      <pubDate>Fri, 05 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3411/</guid>
      <description>Definition1 The following vector-valued Boolean function is called a Toffoli gateToffoli gate. $$ T : \left\{ 0, 1 \right\}^{3} \to \left\{ 0, 1 \right\}^{3} $$ $$ T (a, b, c) = (a, b, (a \land b) \oplus c) $$ The $\text{CCNOT}$ gateControlled Controlled NOT(CCNOT) gate is also known as. Description In the Toffoli gate, if the first two inputs are both $1$, the third input is inverted. In all other</description>
    </item>
    <item>
      <title>Controlled NOT(CNOT) Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3410/</link>
      <pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3410/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>NOR Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3407/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3407/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>NAND Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3406/</link>
      <pubDate>Tue, 25 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3406/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Exclusive Disjuction, XOR Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3405/</link>
      <pubDate>Sun, 23 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3405/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Negation, NOT Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3404/</link>
      <pubDate>Fri, 21 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3404/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Disjunction, OR Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3403/</link>
      <pubDate>Wed, 19 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3403/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Conjunction, AND Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3402/</link>
      <pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3402/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Shannon Entropy in Classical Information Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/3400/</link>
      <pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3400/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>고전정보이론에서 정보량이란?</title>
      <link>https://freshrimpsushi.github.io/en/posts/3398/</link>
      <pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3398/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>How to Change Axis Style in Julia Plots `framestyle`e`</title>
      <link>https://freshrimpsushi.github.io/en/posts/3376/</link>
      <pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3376/</guid>
      <description>Overview1 The framestyle attribute allows changing the style of the plot&amp;rsquo;s axes and border. The possible options are as follows: :box :semi :axes :origin :zerolines :grid :none Code The default setting is :axes. ▷code1◁ The styles for each attribute are as follows. ▷code2◁ https://docs.juliaplots.org/latest/generated/attributes_subplot/&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    <item>
      <title>Differences in Array Dimensions in Julia, Python (NumPy, PyTorch)</title>
      <link>https://freshrimpsushi.github.io/en/posts/3315/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3315/</guid>
      <description>Overview When dealing with high-dimensional arrays in Julia and NumPy, PyTorch (hereinafter referred to collectively as Python for simplicity), it is important to pay attention to what each dimension signifies as they differ. This distinction arises because Julia&amp;rsquo;s arrays are column-major, whereas Python&amp;rsquo;s arrays are row-major. Note that Matlab, being column-major like Julia, does not have this discrepancy, so those familiar with Matlab need not be overly cautious, but those</description>
    </item>
    <item>
      <title>Paper Review: Physics-Informed Neural Networks</title>
      <link>https://freshrimpsushi.github.io/en/posts/3313/</link>
      <pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3313/</guid>
      <description>Overview The notation and numbering of references and formulas follow the conventions of the original paper. Physics-informed neural networks (referred to as PINN[pronounced &amp;lsquo;pin&amp;rsquo;]) are artificial neural networks designed to numerically solve differential equations, introduced in the 2018 Journal of Computational Physics paper Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. The authors of the paper are M. Raissi, P.</description>
    </item>
    <item>
      <title>Methods for Symbolic Computation in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3311/</link>
      <pubDate>Sat, 15 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3311/</guid>
      <description>## Overview Symbolic operations in Julia can be used through the `SymEngine.jl`[^1] package. [^1]: https://symengine.org/SymEngine.jl/ ## Code ### Defining Symbols Symbols can be defined in the following way. julia&amp;gt; using SymEngine julia&amp;gt; x = symbols(:x) x julia&amp;gt; x, y = symbols(&amp;ldquo;x y&amp;rdquo;) (x, y) julia&amp;gt; @vars x, y (x, y) julia&amp;gt; x = symbols(:x) x julia&amp;gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x) ### Vectors</description>
    </item>
    <item>
      <title>Adding a New Column to a DataFrame in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3273/</link>
      <pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3273/</guid>
      <description>Code Let&amp;rsquo;s say we are given the Cosmic Girls dataframe as follows. WJSN = DataFrame( member = [&amp;#34;다영&amp;#34;,&amp;#34;다원&amp;#34;,&amp;#34;루다&amp;#34;,&amp;#34;소정&amp;#34;,</description>
    </item>
    <item>
      <title>Vector Field on Differentiable Manifold</title>
      <link>https://freshrimpsushi.github.io/en/posts/3270/</link>
      <pubDate>Mon, 25 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3270/</guid>
      <description>Buildup1 Consider the easy definition of a vector field. In 3-dimensional space, a vector fieldvector function, vector field is a function $X : \mathbb{R}^{3} \to \mathbb{R}^{3}$ that maps a 3-dimensional vector to another 3-dimensional vector. When considering this in the context of manifolds, $X$ maps a point $\mathbb{R}^{3}$ on the differential manifold $p$ to a vector $\mathbb{R}^{3}$ in $\mathbf{v}$, treating this vector $\mathbf{v}$ as an operator to consider as a</description>
    </item>
    <item>
      <title>Pull Back in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3262/</link>
      <pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3262/</guid>
      <description>Overview We define the pullback on a differential manifold. If differential manifolds are complex, one can think of $M = \mathbb{R}^{m}$ and $N = \mathbb{R}^{n}$. Definition1 Given two differential manifolds $M, N$ and a differentiable function $f : M \to N$, we can consider a function $f^{\ast}$ that maps $N$&amp;rsquo;s $k$-forms to $M$&amp;rsquo;s $k$-forms. Let $\omega$ be a $k$-form on the manifold $N$, then a $k$-form $f^{\ast}\omega$ on the manifold</description>
    </item>
    <item>
      <title>How to Perform Hierarchical Clustering in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3259/</link>
      <pubDate>Sun, 03 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3259/</guid>
      <description>Explanation Use the hclust() function from the Clustering.jl package. hclust(d::AbstractMatrix; [linkage], [uplo], [branchorder]) It takes a distance matrix as input and returns the result of hierarchical clustering. The default method for calculating distances between clusters is single linkage. To plot a dendrogram, use StatsPlots.jl instead of Plots.jl. Code using StatsPlots using Clustering using Distances using Distributions a = rand(Uniform(-1,1), 2, 25) scatt = scatter(a[1,:], a[2,:], label=false) savefig(scatt, &amp;#34;julia_hclust_scatter.png&amp;#34;) D_a =</description>
    </item>
    <item>
      <title>How to Draw a Dendrogram in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3257/</link>
      <pubDate>Wed, 29 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3257/</guid>
      <description>Explanation When attempting to draw a dendrogram by using the plot() function after performing hierarchical clustering with hclust() on the given data, the following error occurs. using Clustering using Distances using Plots a = rand(2, 10) D_a = pairwise(Euclidean(), a, a) SL = hclust(D_a, linkage=:single) dendrogram = plot(SL) ERROR: LoadError: Cannot convert Hclust{Float64} to series data for plotting To draw a dendrogram, one should use StatsPlots.jl instead of Plots.jl. using</description>
    </item>
    <item>
      <title>Gauss-Bonnet Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/3238/</link>
      <pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3238/</guid>
      <description>Gauss-Bonnet Theorem Let&amp;rsquo;s consider $\mathbf{x} : U \to \mathbb{R}^{3}$ as a simple connected geodesic coordinate chart, and $\boldsymbol{\gamma}(I) \subset \mathbf{x}(U)$, which is $\boldsymbol{\gamma}$, as piecewise regular curves. Also, let&amp;rsquo;s say that $\boldsymbol{\gamma}$ surrounds some region $\mathscr{R}$. Then, the following holds true. $$ \iint_{\mathscr{R}} K dA + \int_{\boldsymbol{\gamma}} \kappa_{g} ds + \sum \alpha_{i} = 2\pi $$ Here, $K$ denotes the Gaussian curvature, $\kappa_{g}$ denotes the geodesic curvature, and $\alpha_{i}$ denotes the</description>
    </item>
    <item>
      <title>How to Directly Define Multidimensional Arrays in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3223/</link>
      <pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3223/</guid>
      <description>Explanation 1D arrays (vectors) are defined as follows. julia&amp;gt; A = [1; 2; 3] 3-element Vector{Int64}: 1 2 3 Here, ; signifies moving to the next element based on the first dimension. By generalizing this, ;; signifies moving to the next element based on the second dimension. julia&amp;gt; A = [1; 2; 3;; 4; 5; 6] 3×2 Matrix{Int64}: 1 4 2 5 3 6 Similarly, arrays of three</description>
    </item>
    <item>
      <title>List of Available Commands in Julia Package Management Mode</title>
      <link>https://freshrimpsushi.github.io/en/posts/3217/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3217/</guid>
      <description>Description By typing the right bracket ] in the Julia REPL, you can switch to package management mode. The available commands in package management mode are as follows. Command Function add foo Adds the package foo. free foo Unpins the package version. help, ? Shows these commands. pin foo Pins the version of the package foo. remove foo, rm foo Removes the package foo. test foo Test-runs the package foo.</description>
    </item>
    <item>
      <title>How to load a npy file in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3215/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3215/</guid>
      <description>설명 This document outlines the process of calculating the Radon transform $\mathcal{R}f$ of a phantom $f$ in Python and saving the results as a *.npy file. To load this file in Julia, one can use the PyCall.jl package. using PyCall np = pyimport(&amp;#34;numpy&amp;#34;) The above code is equivalent to executing import numpy as np in Python. This allows one to directly use the code written for numpy in Python</description>
    </item>
    <item>
      <title>Overlaying Plots on Heatmaps in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3213/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3213/</guid>
      <description>Code Let&amp;rsquo;s say we want to draw a sine curve from $0$ to $2\pi$ on the heatmap of the array $(5,5)$. You might want to write the code like this, but as you can see in the figure, it doesn&amp;rsquo;t output as desired. using Plots A = rand(Bool, 5,5) heatmap(A, color=:greens) x = range(0, 2pi, length=100) y = sin.(x) plot!(x, y, color=:red, width=3) This is because the horizontal and vertical</description>
    </item>
    <item>
      <title>Performing Operations on Vectors of Different Sizes Component-wise in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3207/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3207/</guid>
      <description>Description julia&amp;gt; x = [1 2 3] 1×3 Matrix{Int64}: 1 2 3 julia&amp;gt; y = [1 2 3 4] 1×4 Matrix{Int64}: 1 2 3 4 julia&amp;gt; x .+ y ERROR: DimensionMismatch Two vectors of different sizes cannot perform element-wise operations by default. To implement this manually, one would have to use a double for loop, but fortunately, it can be easily calculated by treating one as</description>
    </item>
    <item>
      <title>Methods for Coloring Up to a Certain Value from Curves in Julia / Between Two Curves / Inside a Closed Curve</title>
      <link>https://freshrimpsushi.github.io/en/posts/3203/</link>
      <pubDate>Sun, 13 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3203/</guid>
      <description>Fill up to a Specific Value1 Using attributes fillrange=a, fillalpha=b, fillcolor=:color in plot(), it colors with :color to the value a from the plotted curve with the transparency b. It works the same by writing fill=(a,b,:color). That is, the following two codes are the same. plot(x,y, fillrange=a, fillalpha=b, fillcolor=:color) plot(x,y, fill=(a,b,:color)) It seems to be a bug, but selecting the value of fillrange as $(0,1)$ does not get colored. using</description>
    </item>
    <item>
      <title>How to Get Column and Row Labels of Data Frame in Python Pandas</title>
      <link>https://freshrimpsushi.github.io/en/posts/3189/</link>
      <pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3189/</guid>
      <description>Code import pandas as pd data = { &amp;#39;나이&amp;#39; : [26,23,22,22,21,21,20,20,20,20,18,17], &amp;#39;키&amp;#39; : [160, 163, 163, 162, 164, 163, 164, 150, 158, 162, 172, 173], &amp;#39;별명&amp;#39; : [&amp;#39;땡모&amp;#3</description>
    </item>
    <item>
      <title>Definition of Parallel Vector Field along a Curve on Surface</title>
      <link>https://freshrimpsushi.github.io/en/posts/3174/</link>
      <pubDate>Fri, 14 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3174/</guid>
      <description>Vector Field Along a Curve1 Definition Given a surface $M$ and a curve $\alpha : \left[ a, b \right] \to M$, let us consider a function $\mathbf{X}$ that maps each $t \in \left[ a,b \right]$ to a tangent vector at point $\alpha (t)$ on surface $M$. This function $\mathbf{X}$ is called a vector field along curve $\alpha$vector field along a curve $\alpha$. $$ \mathbf{X} : \left[ a, b \right] \to</description>
    </item>
    <item>
      <title>How to Read and Write Greek Characters and Their Meaning in Mathematics and Science</title>
      <link>https://freshrimpsushi.github.io/en/posts/3145/</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3145/</guid>
      <description>Alpha $\Alpha, \alpha$ Alpha is read as &amp;ldquo;alpha&amp;rdquo;. The TeX codes are \Alpha, \alpha respectively. It is the first letter of the Greek alphabet, and the phrase &amp;ldquo;alpha and omega&amp;rdquo; means &amp;ldquo;the beginning and the end.&amp;rdquo; Index of an index set $\alpha$ In differential geometry, a curve $\alpha$ Curve used to define tangent vectors on a differential manifold $\alpha$ Beta $\Beta, \beta$ Beta is read as &amp;ldquo;beta&amp;rdquo;. The TeX codes</description>
    </item>
    <item>
      <title>How to Find Derivatives in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3135/</link>
      <pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3135/</guid>
      <description>Overview1 The package is named Calculus.jl, but it does not support integration. If automatic differentiation, as discussed in machine learning, is needed, refer to the Zygote.jl package. Differentiation of Single Variable Function Derivative function derivative() It calculates the derivative of $f : \R \to \R$. derivative(f) or derivative(f, :x): Returns the derivative $f^{\prime}$. derivative(f, a): Returns the differential coefficient $f^{\prime}(a)$. julia&amp;gt; f(x) = 1 + 2x + 3x^2 f (generic</description>
    </item>
    <item>
      <title>Tangent Vector on Differentiable Manifold</title>
      <link>https://freshrimpsushi.github.io/en/posts/3132/</link>
      <pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3132/</guid>
      <description>Buildup1 To define a tangent vector at each point on a differentiable manifold $M$, let&amp;rsquo;s assume a differentiable curve $\alpha : (-\epsilon , \epsilon) \to M$ is given. We would like to define the derivative $\dfrac{d \alpha}{dt}(0)$ at $t=0$ in $\alpha$ as a tangent vector, like in differential geometry, but since the range of $\alpha$ is $M$ (since it&amp;rsquo;s not guaranteed to be a metric space), we cannot speak of</description>
    </item>
    <item>
      <title>Differentiable Manifolds</title>
      <link>https://freshrimpsushi.github.io/en/posts/3116/</link>
      <pubDate>Mon, 20 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3116/</guid>
      <description>Definition1 Let $M$ be an arbitrary set and $U_{\alpha} \subset \mathbb{R}^{n}$ be an open set. For the function $1-1$ $\mathbf{x}_{\alpha} : U_{\alpha} \to M$, the ordered pair $\left( M, \left\{ \mathbf{x}_{\alpha} \right\}_{\alpha\in \mathscr{A}} \right)$, or simply $M$, is defined as a differentiable manifold of dimension $n$ if the following conditions are met: $\bigcup \limits_{\alpha} \mathbf{x}_{\alpha} \left( U_{\alpha} \right) = M$ For $\varnothing \ne W = \mathbf{x}_{\alpha}\left( U_{\alpha} \right) \cap \mathbf{x}_{\beta}\left(</description>
    </item>
    <item>
      <title>Back Propagation Algorithm</title>
      <link>https://freshrimpsushi.github.io/en/posts/3077/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3077/</guid>
      <description>This article is written for math majors to understand the principles of the backpropagation algorithm. Notation Given an artificial neural network like the one shown above. Let $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n_{0}})$ be the inputinput, $y_{j}^{l}$ be the $j$th node of the $l$th layer, $\hat{\mathbf{y}} = (\hat{y}_{1}, \hat{y}_{2}, \dots, \hat{y}_{\hat{n}})$ is the output output. Let $L \in \mathbb{N}$ be the number of hidden layers, and the components of $\mathbf{n}=(n_{0},</description>
    </item>
    <item>
      <title>Linear Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3026/</link>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3026/</guid>
      <description>Definition1 A transformation is when a function $T : V \to W$ maps from one vector space to another, that is $V$, $W$ are both vector spaces, we call $T$ a transformation. If the transformation $T$ is a linear function, satisfying the following two conditions for any $\mathbf{v},\mathbf{u} \in V$ and scalar $k$, it is called a linear transformation: $T(k \mathbf{u}) = k T(\mathbf{u})$ $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) +</description>
    </item>
    <item>
      <title>Basis of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3017/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3017/</guid>
      <description>Definition1 Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$ be a subset of vector space $V$. If $S$ satisfies the following two conditions, then $S$ is called a basis of $V$. $S$ spans $V$. $$ V = \text{span}(S) $$ $S$ is linearly independent. Explanation As the name suggests, the concept of a basis corresponds to &amp;rsquo;the smallest thing that can create a vector space&amp;rsquo;. The condition of spanning has</description>
    </item>
    <item>
      <title>Eigenvalues and Eigenvectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/319/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/319/</guid>
      <description>Definition1 Given a matrix $n\times n$ $A$, for a non-zero column vector $\mathbf{0}$ $n\times 1$ and a constant $\mathbf{x}$, the following equation is referred to as the eigenvalue equation or the eigenvalue problem. $$ \begin{equation} A \mathbf{x} = \lambda \mathbf{x} \end{equation} $$ For a given $A$, a $\mathbf{x}$ that satisfies the eigenvalue equation above is called the eigenvalue of $A$, and $n\times 1$ is called the eigenvector corresponding to the</description>
    </item>
    <item>
      <title>Cheat Sheet : Equivalent Codes in Julia, Matlab, Python, R</title>
      <link>https://freshrimpsushi.github.io/en/posts/3031/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3031/</guid>
      <description>개요 In this post, we&amp;rsquo;ll organize code that does the same thing in Julia, Matlab, Python, and R. Cheat Sheet: Flux-PyTorch-TensorFlow Let&amp;rsquo;s say we have the following environment for Python. import numpy as np Common Julia Matlab Python R comment #comment %comment #comment #comment 2d grid X = kron(x, ones(size(y)))Y = kron(ones(size(x)), y) [X,Y] = meshgrid(x,y) np.meshgrid(x,y) How to create an n-dimensional meshgrid in Julia Type Julia Matlab Python</description>
    </item>
    <item>
      <title>What is Reinforcement Learning in Machine Learning</title>
      <link>https://freshrimpsushi.github.io/en/posts/3029/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3029/</guid>
      <description>Definition Reinforcement learning is the process where an agent interacts with the environment to find a policy that maximizes the cumulative reward. Description1 The elements comprising reinforcement learning are as follows: Agentagent: Decides actions based on a policy, given a state. Statestate: Refers to the situation in which the agent is placed. Actionaction: Refers to the choices available to the agent in a given state. Policypolicy: Refers to the strategy</description>
    </item>
    <item>
      <title>Inverse Matrix, Reversible Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3003/</link>
      <pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3003/</guid>
      <description>Definition Let $A$ be an arbitrary square matrix of size $n\times n$. A matrix $L$ is called the left inverse matrix of $A$ if it satisfies the following equation with $A$ in a matrix multiplication. $$ LA=I_{n} $$ Here, $I_{n}$ is the identity matrix of size $n\times n$. A matrix $R$ that is capable of matrix multiplication with $A$ and satisfies the following equation is called the right inverse matrix</description>
    </item>
    <item>
      <title>Matrix Definitions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1955/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1955/</guid>
      <description>Definition1 A matrix is an arrangement of numbers in the shape of a rectangle as follows: $$ A=\begin{bmatrix} 10 &amp;amp; 0 &amp;amp; 3 \\ 0 &amp;amp; 8 &amp;amp; 22 \end{bmatrix} $$ Each of the arranged numbers is called an entry or element. A horizontal line is called a row, and a vertical line is called a column. Moreover, if a certain matrix has $m$ rows and $n$ columns, its size</description>
    </item>
    <item>
      <title>Inner product spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1842/</link>
      <pubDate>Sun, 25 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1842/</guid>
      <description>Definition1 Let&amp;rsquo;s consider $X$ as a vector space. For $\mathbf{x}, \mathbf{y}, \mathbf{z} \in X$ and $\alpha, \beta \in \mathbb{C}$(or $\mathbb{R}$), the following conditions satisfied by a function $$ \langle \cdot , \cdot \rangle : X \times X \to \mathbb{C} $$ are defined as the inner product, and $\left( X, \langle \cdot ,\cdot \rangle \right)$ is called an inner product space. Linearity: $$\langle \alpha \mathbf{x} + \beta \mathbf{y} ,\mathbf{z} \rangle =\alpha</description>
    </item>
    <item>
      <title>Divergence of Vector Function in Cartesian Cooridenates System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1796/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1796/</guid>
      <description>Definition For a vector function $\mathbf{F}(x,y,z)=F_{x}\hat{\mathbf{x}}+F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$, the following scalar function is defined as the divergence $\mathbf{F}$ of $\mathbf{F}(x,y,z)=F_{x}\hat{\mathbf{x}}+F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$ and is denoted by $\nabla \cdot \mathbf{F}$. $$ \begin{equation} \nabla \cdot \mathbf{F} := \frac{ \partial F_{x}}{ \partial x} + \frac{ \partial F_{y}}{ \partial y }+ \frac{ \partial F_{z}}{ \partial z} \label{divergence} \end{equation} $$ Explanation Geometrically, if $\nabla \cdot \mathbf{F}&amp;gt;0$, it means that $\mathbf{F}$ is spreading out or diverging.</description>
    </item>
    <item>
      <title>Why Functional is Named Functional</title>
      <link>https://freshrimpsushi.github.io/en/posts/1780/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1780/</guid>
      <description>The term &amp;ldquo;functional analysis&amp;rdquo; is indeed intriguing, especially when considering the word &amp;ldquo;functional&amp;rdquo; instead of merely &amp;ldquo;function analysis.&amp;rdquo; At first glance, &amp;ldquo;functional&amp;rdquo; appears to be an adjective form of &amp;ldquo;function,&amp;rdquo; suggesting meanings like &amp;ldquo;function-like&amp;rdquo; or &amp;ldquo;pertaining to functions.&amp;rdquo; This notion can also be found in another name for functionals, &amp;ldquo;generalized functions.&amp;rdquo; The question arises as to why these are not simply called functions. To understand this, let&amp;rsquo;s look at the</description>
    </item>
    <item>
      <title>Gradient of Scalar Function in Cartesian Coordinate System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1778/</link>
      <pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1778/</guid>
      <description>Definition For a scalar function $f=f(x,y,z)$, the following vector function is defined as the gradient of $f$, denoted by $\nabla f$: $$ \nabla f := \frac{ \partial f}{ \partial x }\hat{\mathbf{x}}+\frac{ \partial f}{ \partial y}\hat{\mathbf{y}}+\frac{ \partial f}{ \partial z}\hat{\mathbf{z}} = \left( \dfrac{\partial f}{\partial x}, \dfrac{\partial f}{\partial y}, \dfrac{\partial f}{\partial z} \right) $$ Explanation The gradient is translated into English as gradient, slope, or incline. The terms &amp;lsquo;slope&amp;rsquo; and &amp;lsquo;incline&amp;rsquo; are</description>
    </item>
    <item>
      <title>Curl of Vector Functions in 3D Cartesian Coordinates</title>
      <link>https://freshrimpsushi.github.io/en/posts/1752/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1752/</guid>
      <description>Definition For a vector function $\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\hat{\mathbf{x}} + F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$, the following vector is defined as the curl of $\mathbf{F}$, denoted as $\nabla \times \mathbf{F}$. $$ \begin{align} \nabla \times \mathbf{F} &amp;amp;= \left( \dfrac{ \partial F_{z}}{ \partial y }-\dfrac{ \partial F_{y}}{ \partial z} \right)\hat{\mathbf{x}}+ \left( \dfrac{ \partial F_{x}}{ \partial z }-\dfrac{ \partial F_{z}}{ \partial x} \right)\hat{\mathbf{y}}+ \left( \dfrac{ \partial F_{y}}{ \partial x }-\dfrac{ \partial F_{x}}{ \partial y} \right)\hat{\mathbf{z}} \label{def1} \\ &amp;amp;=\begin{vmatrix}</description>
    </item>
    <item>
      <title>Every k Cell is Compact</title>
      <link>https://freshrimpsushi.github.io/en/posts/1711/</link>
      <pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1711/</guid>
      <description>Definition For $a_{i},b_{i} \in \mathbb{R} (1\le i \le k)$, the set $I=[a_{1},b_{1}] \times [a_{2},b_{2}]\times \cdots \times [a_{k},b_{k}]$ is called a $k$-cellk-cell. Here, $\times$ represents the Cartesian product of sets. Theorem 1 Let&amp;rsquo;s assume a sequence of closed intervals on $\mathbb{R}$, $\left\{ I_{n} \right\}$, satisfies $I_{n}\supset I_{n+1}\ (n=1,2,\cdots)$. Then, the following holds true. $$ \bigcap_{i=1}^{\infty}I_{n}\ne \varnothing $$ Proof Let&amp;rsquo;s denote $I_{n}=[a_{n},b_{n}]$. Also, let $E=\left\{ a_{n} : n=1,2,\cdots \right\}$. Then, $E\ne \varnothing$</description>
    </item>
    <item>
      <title>Closure and Derived Set in Metric Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/1701/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1701/</guid>
      <description>Definitions Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. A set that contains all $q$ satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhoodneighborhood of point $p$, denoted by $N_{r}(p)$. In this case, $r$ is called the radius of $N_{r}(p)$. If it&amp;rsquo;s permissible to omit the distance, it can also be denoted as $N_{p}$. If every neighborhood of $p$ includes a $q$ that is $q\ne</description>
    </item>
    <item>
      <title>Neighborhood, Limit Point, Open, Closed in Metric Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/1700/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1700/</guid>
      <description>Definition Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. The set that includes all $q$s satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhoodneighborhood of point $p$ and is denoted as $N_{r}(p)$. Here, $r$ is called the radius of $N_{r}(p)$. If the distance can be omitted, it may also be denoted as $N_{p}$. If all neighborhoods of $p$ contain $q$, which is $q\ne p$ and</description>
    </item>
    <item>
      <title>Linearity of Riemann(-Stieltjes) Iintegral</title>
      <link>https://freshrimpsushi.github.io/en/posts/1666/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1666/</guid>
      <description>Summary1 This article is based on the Riemann-Stieltjes integral. If set to $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Let&amp;rsquo;s say $f$ is integrable by Riemann(-Stieltjes) from $[a,b]$. Then, for a constant $c\in \mathbb{R}$, $cf$ is also integrable from $[a,b]$, and its value is as follows. $$ \int_{a}^{b}cf d\alpha = c\int_{a}^{b}f d\alpha $$ Let two functions $f_{1}$, $f_{2}$ be integrable by Riemann(-Stieltjes) from $[a,b]$. Then, $f_{1}+f_{2}$ is</description>
    </item>
    <item>
      <title>Vector, Inner Product, Wave Function, Hilbert Space in Quantum Mechanics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1509/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1509/</guid>
      <description>Generalization of Vectors Linear Algebra might be a new concept for science students who haven&amp;rsquo;t studied it yet. To them, a vector refers to a physical quantity with magnitude and direction, representing a point in 3-dimensional space, often denoted as $\vec{x} = (x_{1}, x_{2}, x_{3})$. This definition is sufficient for studying classical mechanics and electromagnetism. However, in quantum mechanics, concepts like Fourier Analysis, Inner Product of Functions emerge, making it</description>
    </item>
    <item>
      <title>How to Change Image Size in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1466/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1466/</guid>
      <description>Resizing Images To resize images, you can use the imresize function from the Images package. The function name is the same as in Matlab. imresize(X, ratio=a): Returns the image of array X scaled by a factor of a. Unlike Matlab, you must explicitly write ratio=a. imresize(X, m, n): Returns the image of array X resized to m rows and n columns. Below are example codes and their results. using Images</description>
    </item>
    <item>
      <title>How to Rotate Image Arrays in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1462/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1462/</guid>
      <description>Image Rotation imrotate(X, theta): Rotates array X by theta radians. Note that, unlike in MATLAB where the angle unit is degrees ($^{\circ})$, the angle unit here is radians. Additionally, unlike MATLAB, it rotates clockwise. If no other variables are inputted, the interpolation method defaults to bilinear, and the rotated image is not cropped. Examples of rotating the original image X by $90^\circ=\pi/2$, $180^\circ=\pi$, and $270^\circ=\frac{3}{2}\pi$, along with their results, are</description>
    </item>
    <item>
      <title>Functions for 2D Array Operations in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1460/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1460/</guid>
      <description>Let&amp;rsquo;s say $A = \begin{pmatrix} 1 &amp;amp; 2 &amp;amp; 1 \\ 0 &amp;amp; 3 &amp;amp; 0 \\ 2 &amp;amp; 3 &amp;amp; 4\end{pmatrix}$. Transpose Matrix julia&amp;gt; A =[1 2 1; 0 3 0; 2 3 4] 3×3 Array{Int64,2}: 1 2 1 0 3 0 2 3 4 julia&amp;gt; transpose(A) 3×3 LinearAlgebra.Transpose{Int64,Array{Int64,2}}: 1 0 2 2 3 3 1 0 4 julia&amp;gt; A&amp;#39; 3×3 LinearAlgebra.Adjoint{Int64,Array{Int64,2}}: 1</description>
    </item>
    <item>
      <title>How to output and save arrays as heatmap images in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1459/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1459/</guid>
      <description>Heatmap Using the heatmap function from the Plots package, you can output a 2D array as a heatmap image, and with the savefig function, you can save the resulting image. The @__DIR__ macro tells you the location of the Julia code file. # code1 However, if you compare array A with the heatmap image, you may notice that the top and bottom of the array are flipped in the heatmap</description>
    </item>
    <item>
      <title>Translating Arrays in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1453/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1453/</guid>
      <description>Description Using circshifr(A, (n,m)), you can shift the rows of the array A $n$ positions down, and the columns $m$ positions to the right. (n,m) must be a tuple of integers, and negative numbers are also possible. If negative, it shifts in the opposite direction. For arrays of 3 dimensions or more, it is applied to each smallest 2-dimensional array respectively. Code 2D array julia&amp;gt; A = transpose(reshape(1:25,5,5)) 5×</description>
    </item>
    <item>
      <title>Various Methods of Creating Vectors in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1452/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1452/</guid>
      <description>코드 julia&amp;gt; x1=[1 2 3] 1×3 Array{Int64,2}: 1 2 3 julia&amp;gt; x2=[1, 2, 3] 3-element Array{Int64,1}: 1 2 3 julia&amp;gt; x3=[i for i in 1:3] 3-element Array{Int64,1}: 1 2 3 julia&amp;gt; x4=[i for i in 1:3:10] 4-element Array{Int64,1}: 1 4 7 10 julia&amp;gt; x5=[i for i in 1:3:11] 4-element Array{Int64,1}: 1 4 7 10 x1 is a 2-dimensional array. Since it looks like a row vector, if</description>
    </item>
    <item>
      <title>Subspace Topology, Relative Topology</title>
      <link>https://freshrimpsushi.github.io/en/posts/1439/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1439/</guid>
      <description>Definition 1 Let&amp;rsquo;s assume that a topological space $(X,\mathscr{T})$ and a subset $A \subset X$ are given. Then, the following set $$ \mathscr{T}_{A} =\left\{ A\cap U\ :\ U\in \mathscr{T} \right\} $$ is a topology on $A$. In this case, $\mathscr{T}_{A}$ is referred to as the Subspace Topology or Relative Topology. Moreover, the topological space $(A, \mathscr{T}_{A})$ is called the Subspace of $(X,\mathscr{T})$. Theorem [0]: For a topological space $(X, \mathscr{T}$)</description>
    </item>
    <item>
      <title>Hahn Decomposition Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/1308/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1308/</guid>
      <description>Theorem1 (a) Let $\nu$ be a signed measure defined on a measurable space $(X, \mathcal{E})$. Then there exist a positive set $P$ and a negative set $N$ for $\nu$, satisfying the following: $$ P \cup N=X \quad \text{and} \quad P \cap N =\varnothing $$ Such a $X=P \cup N$ is called a Hahn decomposition for $\nu$. (b) Let $P^{\prime}, N^{\prime}$ be another pair of sets satisfying (a). Then the following</description>
    </item>
    <item>
      <title>Hahn Banach Theorem for Real, Complex, Seminorm</title>
      <link>https://freshrimpsushi.github.io/en/posts/1230/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1230/</guid>
      <description>The Hahn-Banach Theorem for Real Numbers1 Let $X$ be a $\mathbb{R}$-vector space and assume that $Y \subset X$. Let us define $p : X \to \mathbb{ R}$ as a sublinear linear functional of $X$. Now, assume that $y^{\ast} : Y \to \mathbb{ R}$ satisfies the following condition as a $\mathbb{R}$-linear functional of $Y$. $$ y^{\ast}(y) \le p(y)\quad \forall y\in Y $$ Then, there exists a linear functional $x^{\ast} : X</description>
    </item>
    <item>
      <title>What is a Norm Space?</title>
      <link>https://freshrimpsushi.github.io/en/posts/1225/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1225/</guid>
      <description>Definition1 Let&amp;rsquo;s call $X$ a vector space. If there exists a function $\left\| \cdot \right\| : X \to \mathbb{R}$ that satisfies the following three conditions, then $\left\| \cdot \right\|$ is called the norm of $X$, and $(X,\left\| \cdot \right\| )$ is called a normed space. (a) $\left\| x \right\| \ge 0,\quad \forall\ x \in X$ and $\left\| x \right\|=0 \iff x = 0$ (b) $|cx|=|c|\left\| x \right\|,\quad \forall\ x\in X,\</description>
    </item>
    <item>
      <title>Lagrangian Mechanics and Hamiltons Variational Principle</title>
      <link>https://freshrimpsushi.github.io/en/posts/1182/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1182/</guid>
      <description>Overview Hamilton&amp;rsquo;s principle, functionals, action, and variation are explained here in a way that is as simple as possible. If you have not found a satisfactory explanation elsewhere, it is recommended to read through to the end. This has been written so that even freshmen and sophomores in college can understand it. Lagrangian Mechanics1 When an object moves from time $t_{1}$ to $t_{2}$, the integral of the Lagrangian over the</description>
    </item>
    <item>
      <title>Hopf-Lax Formula</title>
      <link>https://freshrimpsushi.github.io/en/posts/1174/</link>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1174/</guid>
      <description>Buildup1 Let&amp;rsquo;s consider the initial value problem of the Hamilton-Jacobi equation that depends only on $H$ as $Du$ for the Hamilton-Jacobi equation. $$ \begin{equation} \left\{ \begin{aligned} u_{t} + H(Du)&amp;amp;=0 &amp;amp;&amp;amp; \text{in } \mathbb{R}^n \times (0,\infty) \\ u&amp;amp;=g &amp;amp;&amp;amp; \text{on } \mathbb{R}^n \times \left\{ t=0 \right\} \end{aligned} \right. \label{eq1} \end{equation} $$ Generally, the Hamiltonian depends on the spatial variables as in the form of $H(Du, x)$, but let&amp;rsquo;s say here it</description>
    </item>
    <item>
      <title>Solution of Nonlinear First Order PDE Using Characteristic Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/1074/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1074/</guid>
      <description>Explanation1 When emphasizing that x and p are variables of a partial differential equation, they are denoted in normal font as $x,p \in \mathbb{R}^{n}$, and when emphasizing them as functions of $s$, they are denoted in bold font as $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Characteristic Equations $$ \begin{cases} \dot{\mathbf{p}} (s) = -D_{x}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big)-D_{z}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big)\mathbf{p}(s) \\ \dot{z}(s) = D_{p}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big) \cdot \mathbf{p}(s) \\ \dot{\mathbf{x}}(s) = D_{p}F\big(\mathbf{p}(s),\</description>
    </item>
    <item>
      <title>Series Solution of Chebyshev Differential Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/955/</link>
      <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/955/</guid>
      <description>Definition The following differential equation is referred to as the Chebyshev Differential Equation: $$ (1-x^2)\dfrac{d^2 y}{dx^2} -x\dfrac{dy}{dx}+n^2 y=0 $$ Description It&amp;rsquo;s a form that includes the independent variable $x$ in the coefficient, and assuming that the solution is in the form of a power series, it can be solved. The solution to the Chebyshev equation is called the Chebyshev polynomial, often denoted as $T_{n}(x)$. Solution $$ \begin{equation} (1-x^2)y^{\prime \prime} -xy^{\prime}+\lambda^2</description>
    </item>
    <item>
      <title>Derivation of Fourier Series</title>
      <link>https://freshrimpsushi.github.io/en/posts/929/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/929/</guid>
      <description>Definition The series for $2L$-periodic function $f$ is defined as the Fourier series of $f$ as follows: $$ \begin{align*} \lim \limits_{N \rightarrow \infty} S^{f}_{N}(t) &amp;amp;= \lim \limits_{N \to \infty}\left[ \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{N} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t}{L} \right) \right] \\ &amp;amp;= \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{\infty} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t}{L} \right) \end{align*} $$ Here, each coefficient $a_{0}, a_{n}, b_{n}$ is called the Fourier coefficient, and its value</description>
    </item>
    <item>
      <title>Rodrigues Formula for Legendre Polynomial</title>
      <link>https://freshrimpsushi.github.io/en/posts/895/</link>
      <pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/895/</guid>
      <description>Formula The explicit formula for the Legendre polynomials is as follows. $$ P_{l}(x)=\dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \tag{1} $$ Description This formula is used to obtain the $l$th Legendre polynomial, known as the Rodrigues&amp;rsquo; formula. Originally, it referred to the explicit form of the Legendre polynomials, but later it became a universal name for formulas representing the explicit form of special functions expressed as polynomials. Derivation The Legendre polynomial $P_{l}$ refers to</description>
    </item>
    <item>
      <title>Series Solution of Legendre Differential Equation: Legendre Polynomial</title>
      <link>https://freshrimpsushi.github.io/en/posts/889/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/889/</guid>
      <description>Definition1 The following differential equation is called the Legendre differential equation. $$ (1-x^2)\dfrac{d^2 y}{dx^2} -2x\dfrac{dy}{dx}+l(l+1) y=0 $$ The solution to the Legendre differential equation is called the Legendre polynomial, commonly denoted as $P_{l}(x)$. The first few Legendre polynomials according to $l$ are as follows. $$ \begin{align*} P_{0}(x) =&amp;amp;\ 1 \\ P_{1}(x) =&amp;amp;\ x \\ P_2(x) =&amp;amp;\ \dfrac{1}{2}(3x^2-1) \\ P_{3}(x) =&amp;amp;\ \dfrac{1}{2}(5x^3-3x) \\ P_{4}(x) =&amp;amp;\ \dfrac{1}{8}(35x^4-30x^2+3) \\ P_{5}(x) =&amp;amp;\ \dfrac{1}{8}(63x^5-70x^3+15x) \\</description>
    </item>
    <item>
      <title>Partition, Riemann Sum, Riemann Integral</title>
      <link>https://freshrimpsushi.github.io/en/posts/828/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/828/</guid>
      <description>Partition1 Let&amp;rsquo;s assume the interval $[a,b]$ is given. The partition $P$ of $[a,b]$ is defined as follows. $$ P := \left\{ x_{0},\ x_{1},\ \cdots, x_{n}\right\},\quad a=x_{0} &amp;lt;x_{1}&amp;lt;\cdots &amp;lt; x_{n} =b $$ And $\Delta x_{i}$ is defined as follows. $$ \Delta x_{i} :=x_{i}-x_{i-1},\quad i=1,2,\cdots,n $$ Explanation Simply put, a partition is a set that contains all points at the ends of an interval and all boundary points within the interval when</description>
    </item>
    <item>
      <title>Laplace Transform Table</title>
      <link>https://freshrimpsushi.github.io/en/posts/743/</link>
      <pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/743/</guid>
      <description>Formula1 This is table of Laplace transform. $f(t)=\mathcal{L^{-1}}$ $F(s)=\mathcal{L} \left\{ f(t) \right\}$ Derivation $1$ $\dfrac{1}{s}$ link $e^{at}$ $\dfrac{1}{s-a}$ link $t^n$ $\dfrac{n!}{s^{n+1}}$ link $t^{p}$ $\dfrac{ \Gamma (p+1) }{ s^{p+1}}$ link $t^{p}e^{at}$ $\dfrac{ \Gamma (p+1) }{ (s-a)^{p+1}}$ link $\sin (at)$ $\dfrac{a}{s^2+a^2}$ link $\cos (at)$ $\dfrac{s}{s^2+a^2}$ link $e^{at}\sin(bt)$ $\dfrac{b}{(s-a)^2 +b^2}$ link $e^{at}\cos(bt)$ $\dfrac{s-a}{(s-a)^2+b^2}$ link $\sinh (at)$ $\dfrac{a}{s^2-a^2}$ link $\cosh (at)$ $\dfrac{s}{s^2-a^2}$ link $e^{at} \sinh (bt)$ $\dfrac{b}{(s-a)^2-b^2}$ link $e^{at} \cosh (bt)$ $\dfrac{s-a}{(s-a)^2-b^2}$ link $u_{c}(t)=</description>
    </item>
    <item>
      <title>Rings in Abstract Algebra</title>
      <link>https://freshrimpsushi.github.io/en/posts/587/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/587/</guid>
      <description>Definition 1 A set $R$ satisfying the following rules for two binary operations, addition$+$ and multiplication$\cdot$, is defined as a Ring. When $a$, $b$, $c$ are elements of $R$, Commutative law holds for addition. $$a+b=b+a$$ Associative law holds for addition. $$(a+b)+c=a+(b+c)$$ There exists an identity element for addition. $$\forall a \ \exists 0\ \ \mathrm{s.t} \ a+0=a$$ There exists an additive inverse for every element. $$\forall a \ \exists -a\</description>
    </item>
    <item>
      <title>연산자 방법으로 조화진동자 문제 풀기  사다리 연산자의 정의</title>
      <link>https://freshrimpsushi.github.io/en/posts/362/</link>
      <pubDate>Wed, 13 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/362/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 조화진동자 문제를 연산자 방법으로 풀 때 아주 유용한 연산자가 있다.바로 조화진동자의 사다리연산자$\ma</description>
    </item>
    <item>
      <title>Dimension of the Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3018/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3018/</guid>
      <description>Definition1 The number of elements (vectors) of a basis for a vector space $V$ is defined as the dimension of $V$ and is denoted as follows. $$ \dim (V) $$ Explanation Such a generalization of dimensions goes beyond merely exploring vector spaces and is being applied to various technologies that support this society. It might seem pointless to consider dimensions higher than the $3$ dimensions of our world and the</description>
    </item>
    <item>
      <title>Linear Combination, Span</title>
      <link>https://freshrimpsushi.github.io/en/posts/512/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/512/</guid>
      <description>Definition: Linear Combination1 Let $\mathbf{w}$ be a vector in the vector space $V$. If $\mathbf{w}$ can be expressed as follows for vectors $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$ in $V$ and arbitrary constants $k_{1}, k_{2}, \cdots, k_{r}$, then $\mathbf{w}$ is called a linear combination of $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$. $$ \mathbf{w} = k_{1}\mathbf{v}_{1} + k_{2}\mathbf{v}_{2} + \cdots + k_{r}\mathbf{v}_{r} $$ Additionally, in this case, the constants $k_{1}, k_{2}, \cdots, k_{r}$ are referred to as the coefficients</description>
    </item>
    <item>
      <title>Subspace of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/285/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/285/</guid>
      <description>Definition1 Let $W$ be a non-empty subset of the vector space $V$. If $W$ satisfies the definition of a vector space with respect to the addition and scalar multiplication defined in $V$, then $W$ is called a subspacesubspace of the vector space $V$, and is denoted as follows: $$ W \le V $$ Explanation To determine whether a subset $W$ of a vector space $V$ is a subspace of $V$,</description>
    </item>
    <item>
      <title>Definition of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/282/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/282/</guid>
      <description>Definition1 When the elements of a non-empty set $V$ satisfy the following ten rules for two operations, additionaddition and scalar multiplicationscalar multiplication, $V$ is called a vector spacevector space over field2 $\mathbb{F}$, and the elements of $V$ are called vectorsvector. For $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k, l \in \mathbb{F}$, (A1) If $\mathbf{u}, \mathbf{v}$ is an element of $V$, then $\mathbf{u}+\mathbf{v}$ is also an element of $V$. (A2) $\mathbf{u}</description>
    </item>
    <item>
      <title>Euclidean Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/205/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/205/</guid>
      <description>Definition For a natural number $n \in \mathbb{N}$, the Cartesian product $\mathbb{R}$ of the set of real numbers is called the Euclidean space. $$ \mathbb{R}^{n} = \mathbb{R} \times \cdots \times \mathbb{R} $$ $\mathbb{R}^{1}$ is referred to as real space or number line. $\mathbb{R}^{2}$ is called a plane. $\mathbb{R}^{3}$ is called a $3$-dimensional space. Here, $\mathbb{N} := \left\{ 1, 2, 3, \cdots \right\}$ means the set that includes all natural numbers.</description>
    </item>
    <item>
      <title>Product of Two Levi-Civita Symbols</title>
      <link>https://freshrimpsushi.github.io/en/posts/88/</link>
      <pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/88/</guid>
      <description>Theorem The $\epsilon_{ijk}$, defined as follows, is referred to as the Levi-Civita symbol. $$ \epsilon_{ijk} = \begin{cases} +1 &amp;amp; \text{if} \ \epsilon_{123}, \epsilon_{231}, \epsilon_{312} \\ -1 &amp;amp; \text{if} \ \epsilon_{132}, \epsilon_{213}, \epsilon_{321} \\ 0 &amp;amp; \text{if} \ i=j \ \text{or} \ j=k \ \text{or} \ k=i \end{cases} $$ The $\delta_{ij}$, defined as follows, is referred to as the Kronecker delta. $$ \delta_{ij} := \begin{cases} 1,&amp;amp;i=j \\ 0, &amp;amp; i\ne j</description>
    </item>
    <item>
      <title>파이토치에서 torch.nn과 torch.nn.functional의 차이</title>
      <link>https://freshrimpsushi.github.io/en/posts/3626/</link>
      <pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3626/</guid>
      <description>Description Many functions related to neural networks are included under the same name in torch.nn and torch.nn.functional. The functions in nn return a neural network as a function, while those in nn.functional are the neural network itself. For instance, nn.MaxPool2d takes the kernel size as input and returns a pooling layer. import torch import torch.nn as nn pool = nn.MaxPool2d(kernel_size = 2) # MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) A =</description>
    </item>
    <item>
      <title>Solution of Wave Equation with Zero Initial Condition</title>
      <link>https://freshrimpsushi.github.io/en/posts/3623/</link>
      <pubDate>Fri, 04 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3623/</guid>
      <description>Tidy up Let&amp;rsquo;s say that we have the following wave equation. where $\Delta_{\mathbf{x}}$ is Laplacian for the variable $\mathbf{x}$. $$ \begin{align} \partial_{t}^{2} p(\mathbf{x}, t) &amp;amp;= \Delta_{\mathbf{x}} p(\mathbf{x}, t) &amp;amp;\text{on } \mathbb{R} \times [0, \infty) \\ p(\mathbf{x}, 0) &amp;amp;= f(\mathbf{x}) &amp;amp;\text{on } \mathbb{R} \\ \partial_{t} p(\mathbf{x}, 0) &amp;amp;= 0 &amp;amp;\text{on } \mathbb{R} \end{align} $$ The solution of the above partial differential equation is as follows. $$ \begin{equation} p(\mathbf{x}, t) = \dfrac{1}{(2\pi)^{n}}</description>
    </item>
    <item>
      <title>파이토치에서 AdaBelief 옵티마이저 사용하는 방법</title>
      <link>https://freshrimpsushi.github.io/en/posts/3620/</link>
      <pubDate>Sat, 28 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3620/</guid>
      <description>Description AdaBelief, introduced by J. Zhuang et al. in 2020, is one of the variations of Adam1. Since PyTorch does not natively provide this optimizer, it must be installed separately. Code2 Installation The following command can be used to install it via cmd. pip install adabelief-pytorch==0.2.0 Usage The code below can be used to import and utilize it. from adabelief_pytorch import AdaBelief optimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple =</description>
    </item>
    <item>
      <title>How to Use Color Gradients in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3608/</link>
      <pubDate>Wed, 04 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3608/</guid>
      <description>Description A color gradient is one of the two color schemes supported by Julia&amp;rsquo;s visualization package Plots.jl (the other is palette), which is what we commonly refer to as gradation. Simply put, a type that implements gradation is ColorGradient. Gradients are used to draw charts such as heatmap(), surface(), contour(). If you want to differentiate the colors of various graphs, use a palette instead of a gradient. Code Symbol It</description>
    </item>
    <item>
      <title>How to Use Palettes in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3607/</link>
      <pubDate>Mon, 02 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3607/</guid>
      <description>Explanation A palette refers to a board where paints are squeezed out in advance. Mathematically, it can be explained as a &amp;lsquo;set of colors&amp;rsquo; or a &amp;lsquo;sequence of colors&amp;rsquo;. When drawing multiple graphs in one picture, the most common way is to distinguish them by using different colors. For this purpose, Julia has implemented a type called ColorPalette that collects various colors. It can be comfortably understood as a vector</description>
    </item>
    <item>
      <title>How to Plot Two Data Axes of Different Scales in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3606/</link>
      <pubDate>Sat, 31 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3606/</guid>
      <description>코드 When plotting two data sets that have a large scale difference on the same plot, the one with the smaller scale gets completely ignored as shown in the figure below. using Plots x = 0:0.01:2π plot(x, sin.(x)) plot!(x, exp.(x)) When plotting the second data set, if you input twinx() as the first argument, it shares the $x$ axis and the</description>
    </item>
    <item>
      <title>List of Plot Properties in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3605/</link>
      <pubDate>Thu, 29 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3605/</guid>
      <description>Description In Julia&amp;rsquo;s Plots.jl, a plot is also an object. If you draw an empty plot to check its type, it looks like this. julia&amp;gt; using Plots julia&amp;gt; p = plot() julia&amp;gt; p |&amp;gt; typeof Plots.Plot{Plots.GRBackend} Removing Plots., it becomes Plot{GRBackend}, meaning the plot&amp;rsquo;s backend is GR, similar to how a vector with elements of type Float64 is denoted as Vector{Float64}. Checking the properties of Plot, we find the following.</description>
    </item>
    <item>
      <title>Decorating the Background Grid in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3604/</link>
      <pubDate>Tue, 27 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3604/</guid>
      <description>Summary Keywords related to the grid background in Plots.jl are as follows: Keyword Name Function grid Display grid gridalpha, ga, gα Specify grid transparency foreground_color_grid, fgcolor_grid Specify grid color gridlinewidth, grid_lw Specify grid thickness gridstyle, grid_ls Specify grid line style minorgrid Display minor grid minorgridalpha Specify minor grid transparency foreground_color_minor_grid, fgcolor_minorgrid Specify minor grid color minorgridlinewidth, minorgrid_lw Specify minor grid thickness minorgridstyle, minorgrid_ls Specify minor grid line style Code</description>
    </item>
    <item>
      <title>Specifying Background Color in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3603/</link>
      <pubDate>Sun, 25 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3603/</guid>
      <description>Summary The keywords related to the background color of figures in Plots.jl are as follows. Keyword Name Function background_color, bg_color Specify the color of the overall background background_color_outside, bg_color_outside Specify the color of the area outside where the graph is drawn background_subplot, bg_subplot Specify the color of the area where the graph is drawn background_inside, bg_inside Specify the color of the area where the graph is drawn, excluding the legend</description>
    </item>
    <item>
      <title>How to Specify Graph Colors for Each Subplot in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3602/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3602/</guid>
      <description>Overview This section introduces three methods for specifying graph colors for each subplot. To learn how to specify colors for graph elements, refer here. Method 1 The first way to specify the graph color for a subplot is to predefine the color when defining each subplot. In Julia, since a picture is an object itself, you can define multiple pictures with different attributes and then combine them into one plot.</description>
    </item>
    <item>
      <title>Specifying the Color of Graph Elements in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3601/</link>
      <pubDate>Wed, 21 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3601/</guid>
      <description>Summary In Plots.jl, the keywords for specifying the color of each graph component are as follows. Keyword Function markercolor, mc Specify the marker&amp;rsquo;s inside color markerstrokecolor, msc Specify the marker&amp;rsquo;s border color linecolor, lc Specify the line color fillcolor, fc Specify the fill color seriescolor, c Specify the color of all components Keyword Function markeralpha, ma, mα Specify the marker&amp;rsquo;s inside transparency markerstrokealpha, msa, msα Specify the</description>
    </item>
    <item>
      <title>How to Use RGB Color Codes in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3600/</link>
      <pubDate>Mon, 19 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3600/</guid>
      <description>Code The package provided in Julia for dealing with colors is Colors.jl. By importing the visualization package Plots.jl, the features within Colors.jl can also be used. The color codes representing the RGB space include RGB, BGR, RGB24, RGBX, XRGB, which are subtypes of AbstractRGB. RGBA adds transparency to RGB. julia&amp;gt; using Plots julia&amp;gt; subtypes(AbstractRGB) 5-element Vector{Any}: BGR RGB RGB24 RGBX XRGB julia&amp;gt; subtypes(AbstractRGBA) 2-element Vector{Any}: BGRA RGBA Strings For the</description>
    </item>
    <item>
      <title>Package for Color Processing in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3599/</link>
      <pubDate>Sat, 17 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3599/</guid>
      <description>Introduction1 Introducing the capabilities of Colors.jl, a package for color processing in Julia. When using the visualization package Plots.jl, there&amp;rsquo;s no need to load Colors.jl separately. It provides the following functionalities: Color parsing and conversion Color maps Color scales Parsing and Conversion Assuming str is a string representing color information, you can parse the string into a color code of a specific color space using @colorant_str or parse(Colorant, str). Note</description>
    </item>
    <item>
      <title>How to Use Colors in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3598/</link>
      <pubDate>Thu, 15 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3598/</guid>
      <description>Overview The package that facilitates the convenient use of colors in Julia is Colors.jl. It can be used together just by importing the visualization package Plots.jl. Symbols and Strings The way to check the list of named colors is by entering Colors.color_names in the console window or checking the official documentation. julia&amp;gt; using Plots julia&amp;gt; Colors.color_names Dict{String, Tuple{Int64, Int64, Int64}} with 666 entries: &amp;#34;darkorchid&amp;#34; =&amp;gt; (153, 50, 204) &amp;#34;chocolate&amp;#34; =&amp;gt;</description>
    </item>
    <item>
      <title>Decorating Text Output with Built-in Functions in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3597/</link>
      <pubDate>Tue, 13 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3597/</guid>
      <description>Code Using the function printstyled(string; color = color) allows you to decorate the outputted function. As input for the keyword argument color, symbols and integers $(0 \le n \le 255)$ are possible. Note that strings are not allowed. The available symbols include not only colors but also options like :blink, :reverse, etc. These can also be applied by entering them as keyword arguments like blink = true, bold = true.</description>
    </item>
    <item>
      <title>Dimensionality Reduction in Data Science</title>
      <link>https://freshrimpsushi.github.io/en/posts/3563/</link>
      <pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3563/</guid>
      <description>Definition Let&amp;rsquo;s assume a data set $X \subset \mathbb{R}^{n}$ is given. The following mapping for $m \lt n$ is called dimension reductiondimension reduction. $$ r : X \to \mathbb{R}^{m} $$ Or more commonly in machine learning, any method that reduces the number of input variables in a way that retains as much of the performance as possible is called a dimension reduction technique. Explanation Dimension reduction, as the name suggests,</description>
    </item>
    <item>
      <title>How to Define and Train MLP with the Sequence Model and Functional API in TensorFlow and Keras</title>
      <link>https://freshrimpsushi.github.io/en/posts/3562/</link>
      <pubDate>Tue, 04 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3562/</guid>
      <description>Overview In TensorFlow, neural networks can be easily defined using Keras. Below, we introduce how to define and train a simple MLP using Sequential() and the functional API. However, Sequential() is only easy for defining models and can be challenging to use for designing complex structures. Similarly, if you plan to design complex structures using the functional API, it’s better to use the keras.Model class, and for</description>
    </item>
    <item>
      <title>How to Adjust the Size and Resolution of an Image in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3559/</link>
      <pubDate>Thu, 27 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3559/</guid>
      <description>Code Size plot(x, y, size=(600,400)) In Julia, the size of a plot is set using the size option. It must be input as a Tuple{Integer, Integer}, where each integer represents the width and height in pixels, respectively. The default value is (600,400). using Plots x = rand(10) plot(x) savefig(&amp;#34;size_default.png&amp;#34;) plot(x, size=(1200,800)) savefig(&amp;#34;size_(1200,800).png&amp;#34;) 1800x1200 image (left), 600x400 image (right) Resolution plot(x, y, dpi=100) The resolution of an image is set using</description>
    </item>
    <item>
      <title>Drawing Arrows in Graphics with Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3558/</link>
      <pubDate>Tue, 25 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3558/</guid>
      <description>Code plot!([x1, x2], [y1, y2], arrow=:true) This code plots an arrow from point $(x1, y1)$ to point $(x2, y2)$ on the plot. Naturally, the tip of the arrow is at the terminal point $(x2, y2)$. The maximum value of the sine function can be shown as follows. using Plots x = range(0, 2π, 100) plot(x, sin.(x), label=&amp;#34;&amp;#34;, ylims=(-1.3,1.3)) plot!([π/2</description>
    </item>
    <item>
      <title>How to Fix the Random Seed in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3555/</link>
      <pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3555/</guid>
      <description>Explanation1 In Julia, the random seed can be fixed as follows: seed!([rng=default_rng()], seed) -&amp;gt; rng seed!([rng=default_rng()]) -&amp;gt; rng The input variable rng stands for Random Number Generator, which refers to the algorithm used for drawing random numbers. The Random package offers the following options: TaskLocalRNG: This is the default setting. Xoshiro RandomDevice MersenneTwister Code By fixing the seed to 0, drawing three times, and then fixing it again to 0</description>
    </item>
    <item>
      <title>How to Draw a Box Plot in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3553/</link>
      <pubDate>Sat, 15 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3553/</guid>
      <description>English Translation Description To draw a box plot, the statistical visualization package StatsPlots.jl must be used. boxplot([data], labels=[label]) Code using StatsPlots x = rand(0:100, 100) y = rand(50:100, 100) z = cat(x,y, dims=1) boxplot(x, label=&amp;#34;x&amp;#34;) boxplot!(y, label=&amp;#34;y&amp;#34;) boxplot!(z, label=&amp;#34;z&amp;#34;) Or boxplot([x,y,z], label=[&amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;]) will draw the same figure. Note that there should be no commas in lable. That is, it needs to be an array, not an $3 \times</description>
    </item>
  </channel>
</rss>
