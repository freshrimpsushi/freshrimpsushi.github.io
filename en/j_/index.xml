<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>J_s on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/j_/</link>
    <description>Recent content in J_s on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 29 Dec 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/j_/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</title>
      <link>https://freshrimpsushi.github.io/en/posts/3529/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3529/</guid>
      <description>Overview1 2 Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam. Explanation In gradient descent, the learning rate learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\alpha$, $\eta$, and determines how much of the gradient is taken into account when updating the</description>
    </item>
    <item>
      <title>Momentum Method in Gradient Descent</title>
      <link>https://freshrimpsushi.github.io/en/posts/3528/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3528/</guid>
      <description>Overview1 2 In gradient descent, the momentum technique involves using all previous gradients when updating parameters. This is its essence and the end of the explanation. However, explanations involving strange update equations, the motivation from physics&amp;rsquo; momentum, setting the mass to $1$ and the initial velocity to $0$, only complicate understanding. This article explains the momentum technique as plainly as possible. Build-Up Let&amp;rsquo;s denote the parameters as $\boldsymbol{\theta}$ and the</description>
    </item>
    <item>
      <title>Monte Carlo Integration</title>
      <link>https://freshrimpsushi.github.io/en/posts/3515/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3515/</guid>
      <description>Overview Monte Carlo integration is a numerical approximation method used when calculating the integral of a given function is difficult. Consider the following scenario: For an integrable function $f$ in the given $[0, 1]$or generally $[0, 1]^{n}$, we know the formula of $f(x)$ but calculating its integral is not straightforward. However, we want to calculate the integral $I[f]$ of $f$. $$ \begin{equation} I[f] = \int_{[0,1]} f(x) dx \end{equation} $$ Definition</description>
    </item>
    <item>
      <title>MNIST Database</title>
      <link>https://freshrimpsushi.github.io/en/posts/3444/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3444/</guid>
      <description>Overview1 $$ \includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$ The MNISTmodified national institute of standards and technology database refers to a dataset of digit handwriting from American high school students and Census Bureau employees. It is commonly known as [MNIST]. Official Website Description This dataset is frequently used as an example for beginners in machine learning/deep learning. NIST originally collected handwritten data in the following format for the evaluation of character recognition technology for automated</description>
    </item>
    <item>
      <title>Paper Review: Physics-Informed Neural Networks</title>
      <link>https://freshrimpsushi.github.io/en/posts/3313/</link>
      <pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3313/</guid>
      <description>Overview The notation and numbering of references and formulas follow the conventions of the original paper. Physics-informed neural networks (referred to as PINN[pronounced &amp;lsquo;pin&amp;rsquo;]) are artificial neural networks designed to numerically solve differential equations, introduced in the 2018 Journal of Computational Physics paper Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. The authors of the paper are M. Raissi, P.</description>
    </item>
    <item>
      <title>Back Propagation Algorithm</title>
      <link>https://freshrimpsushi.github.io/en/posts/3077/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3077/</guid>
      <description>This article is written for math majors to understand the principles of the backpropagation algorithm. Notation Given an artificial neural network like the one shown above. Let $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n_{0}})$ be the inputinput, $y_{j}^{l}$ be the $j$th node of the $l$th layer, $\hat{\mathbf{y}} = (\hat{y}_{1}, \hat{y}_{2}, \dots, \hat{y}_{\hat{n}})$ is the output output. Let $L \in \mathbb{N}$ be the number of hidden layers, and the components of $\mathbf{n}=(n_{0},</description>
    </item>
    <item>
      <title>What is Reinforcement Learning in Machine Learning</title>
      <link>https://freshrimpsushi.github.io/en/posts/3029/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3029/</guid>
      <description>Definition Reinforcement learning is the process where an agent interacts with the environment to find a policy that maximizes the cumulative reward. Description1 The elements comprising reinforcement learning are as follows: Agentagent: Decides actions based on a policy, given a state. Statestate: Refers to the situation in which the agent is placed. Actionaction: Refers to the choices available to the agent in a given state. Policypolicy: Refers to the strategy</description>
    </item>
    <item>
      <title>Gradient of Scalar Function in Cartesian Coordinate System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1778/</link>
      <pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1778/</guid>
      <description>Definition For a scalar function $f=f(x,y,z)$, the following vector function is defined as the gradient of $f$, denoted by $\nabla f$: $$ \nabla f := \frac{ \partial f}{ \partial x }\hat{\mathbf{x}}+\frac{ \partial f}{ \partial y}\hat{\mathbf{y}}+\frac{ \partial f}{ \partial z}\hat{\mathbf{z}} = \left( \dfrac{\partial f}{\partial x}, \dfrac{\partial f}{\partial y}, \dfrac{\partial f}{\partial z} \right) $$ Explanation The gradient is translated into English as gradient, slope, or incline. The terms &amp;lsquo;slope&amp;rsquo; and &amp;lsquo;incline&amp;rsquo; are</description>
    </item>
    <item>
      <title>Curl of Vector Functions in 3D Cartesian Coordinates</title>
      <link>https://freshrimpsushi.github.io/en/posts/1752/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1752/</guid>
      <description>Definition For a vector function $\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\hat{\mathbf{x}} + F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$, the following vector is defined as the curl of $\mathbf{F}$, denoted as $\nabla \times \mathbf{F}$. $$ \begin{align} \nabla \times \mathbf{F} &amp;amp;= \left( \dfrac{ \partial F_{z}}{ \partial y }-\dfrac{ \partial F_{y}}{ \partial z} \right)\hat{\mathbf{x}}+ \left( \dfrac{ \partial F_{x}}{ \partial z }-\dfrac{ \partial F_{z}}{ \partial x} \right)\hat{\mathbf{y}}+ \left( \dfrac{ \partial F_{y}}{ \partial x }-\dfrac{ \partial F_{x}}{ \partial y} \right)\hat{\mathbf{z}} \label{def1} \\ &amp;amp;=\begin{vmatrix}</description>
    </item>
    <item>
      <title>Vector, Inner Product, Wave Function, Hilbert Space in Quantum Mechanics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1509/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1509/</guid>
      <description>Generalization of Vectors Linear Algebra might be a new concept for science students who haven&amp;rsquo;t studied it yet. To them, a vector refers to a physical quantity with magnitude and direction, representing a point in 3-dimensional space, often denoted as $\vec{x} = (x_{1}, x_{2}, x_{3})$. This definition is sufficient for studying classical mechanics and electromagnetism. However, in quantum mechanics, concepts like Fourier Analysis, Inner Product of Functions emerge, making it</description>
    </item>
    <item>
      <title>Hahn Decomposition Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/1308/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1308/</guid>
      <description>Theorem1 (a) Let $\nu$ be a signed measure defined on a measurable space $(X, \mathcal{E})$. Then there exist a positive set $P$ and a negative set $N$ for $\nu$, satisfying the following: $$ P \cup N=X \quad \text{and} \quad P \cap N =\varnothing $$ Such a $X=P \cup N$ is called a Hahn decomposition for $\nu$. (b) Let $P^{\prime}, N^{\prime}$ be another pair of sets satisfying (a). Then the following</description>
    </item>
    <item>
      <title>Product of Two Levi-Civita Symbols</title>
      <link>https://freshrimpsushi.github.io/en/posts/88/</link>
      <pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/88/</guid>
      <description>Theorem The $\epsilon_{ijk}$, defined as follows, is referred to as the Levi-Civita symbol. $$ \epsilon_{ijk} = \begin{cases} +1 &amp;amp; \text{if} \ \epsilon_{123}, \epsilon_{231}, \epsilon_{312} \\ -1 &amp;amp; \text{if} \ \epsilon_{132}, \epsilon_{213}, \epsilon_{321} \\ 0 &amp;amp; \text{if} \ i=j \ \text{or} \ j=k \ \text{or} \ k=i \end{cases} $$ The $\delta_{ij}$, defined as follows, is referred to as the Kronecker delta. $$ \delta_{ij} := \begin{cases} 1,&amp;amp;i=j \\ 0, &amp;amp; i\ne j</description>
    </item>
    <item>
      <title>How to Define and Train MLP with the Sequence Model and Functional API in TensorFlow and Keras</title>
      <link>https://freshrimpsushi.github.io/en/posts/3562/</link>
      <pubDate>Tue, 04 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3562/</guid>
      <description>Overview In TensorFlow, neural networks can be easily defined using Keras. Below, we introduce how to define and train a simple MLP using Sequential() and the functional API. However, Sequential() is only easy for defining models and can be challenging to use for designing complex structures. Similarly, if you plan to design complex structures using the functional API, it’s better to use the keras.Model class, and for</description>
    </item>
  </channel>
</rss>
