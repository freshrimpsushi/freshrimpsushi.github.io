<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>J_s on FreshrimpRestaurant</title>
    <link>https://freshrimpsushi.github.io/en/j_/</link>
    <description>Recent content in J_s on FreshrimpRestaurant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 24 Jan 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/en/j_/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Absolutely Continuous Real Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/3542/</link>
      <pubDate>Wed, 24 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3542/</guid>
      <description>Definition1 Let&amp;rsquo;s say a function $f : \mathbb{R} \to \mathbb{R}( \text{or } \mathbb{C})$ is given. If for any finite number of mutually disjoint intervals $(a_{i}, b_{i}) \sub [a,b]$, the following condition is satisfied, then it is said to be absolutely continuousabsolutely continuous on $[a, b]$. $$ \forall \epsilon \gt 0 \quad \exist \delta \gt 0 \text{ such that } \sum\limits_{i=1}^{N} (b_{i} - a_{i}) \lt \delta \implies \sum\limits_{i=1}^{N} \left| f(b_{j}) -</description>
    </item>
    <item>
      <title>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</title>
      <link>https://freshrimpsushi.github.io/en/posts/3529/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3529/</guid>
      <description>Overview1 2 Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam. Explanation In gradient descent, the learning rate learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\alpha$, $\eta$, and determines how much of the gradient is taken into account when updating the</description>
    </item>
    <item>
      <title>Momentum Method in Gradient Descent</title>
      <link>https://freshrimpsushi.github.io/en/posts/3528/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3528/</guid>
      <description>Overview1 2 In gradient descent, the momentum technique involves using all previous gradients when updating parameters. This is its essence and the end of the explanation. However, explanations involving strange update equations, the motivation from physics&amp;rsquo; momentum, setting the mass to $1$ and the initial velocity to $0$, only complicate understanding. This article explains the momentum technique as plainly as possible. Build-Up Let&amp;rsquo;s denote the parameters as $\boldsymbol{\theta}$ and the</description>
    </item>
    <item>
      <title>자기장의 기호로 B를 사용하는 이유</title>
      <link>https://freshrimpsushi.github.io/en/posts/3523/</link>
      <pubDate>Sun, 17 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3523/</guid>
      <description>Question Electromagnetism is literally the study of electric fields $\mathbf{E}$ and magnetic fields $\mathbf{B}$. While studying electromagnetism, one might have wondered the following at least once. Why is the symbol for magnetic fields $\mathbf{B}$ used? It&#39;s understandable that the electric field is $\mathbf{E}$, derived from the Electric field, but why is the magnetic field $\mathbf{B}$ when it should be from Magnetic field? This notation might feel oddly placed, and it&#39;s</description>
    </item>
    <item>
      <title>Monte Carlo Integration</title>
      <link>https://freshrimpsushi.github.io/en/posts/3515/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3515/</guid>
      <description>Overview Monte Carlo integration is a numerical approximation method used when calculating the integral of a given function is difficult. Consider the following scenario: For an integrable function $f$ in the given $[0, 1]$or generally $[0, 1]^{n}$, we know the formula of $f(x)$ but calculating its integral is not straightforward. However, we want to calculate the integral $I[f]$ of $f$. $$ \begin{equation} I[f] = \int_{[0,1]} f(x) dx \end{equation} $$ Definition</description>
    </item>
    <item>
      <title>Bilinear Forms and Hermitian Forms</title>
      <link>https://freshrimpsushi.github.io/en/posts/3513/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3513/</guid>
      <description>Definition1 Let’s say we have two vectors $\mathbf{x}, \mathbf{u} \in \mathbb{R}^{n}$ as follows. $$ \mathbf{x}=\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix},\quad \mathbf{u}^{T} = \begin{bmatrix} u_{1} &amp;amp; u_{2} &amp;amp; \cdots &amp;amp; u_{n} \end{bmatrix} $$ For a real constant $a_{ij} \in \mathbb{R} (1\le i,j \le n)$, the function $A : \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$, defined as follows, is called the bilinear form. $$ A(\mathbf{u},\mathbf{x}):=\sum \limits_{i,k=1}^{n}</description>
    </item>
    <item>
      <title>Quadratic Form</title>
      <link>https://freshrimpsushi.github.io/en/posts/3512/</link>
      <pubDate>Sat, 25 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3512/</guid>
      <description>Definition $V$ is called a $n$dimensional vector space. For a given constant $a_{ij} \in \mathbb{R}(\text{or } \mathbb{C})$, the following second order homogeneous function $A : V \to \mathbb{R}(\text{or } \mathbb{C})$ is called a quadratic form. $$ A(\mathbf{x}) := \sum\limits_{i,j=1}^{n} a_{ij}x_{i}x_{j},\qquad (a_{ij} = a_{ji}) $$ Here, $\mathbf{x} = \begin{bmatrix} x_{1} &amp;amp; \cdots &amp;amp; x_{n} \end{bmatrix}^{T}$ holds. The term $i \ne j$ for $a_{ij}x_{i}x_{j}$ is called the cross product terms. Explanation According</description>
    </item>
    <item>
      <title>How to Neatly Print without Axes, Scales, etc. in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3501/</link>
      <pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3501/</guid>
      <description>Code Plots.jl essentially outputs everything including grids, ticks, axes, and color bars by default, but if you want to make it clean without these, you can add the following options. colorbar=:none: Removes the color bar. showaxis = false: Removes the axes and ticks. grid=false: Removes the background grid. ticks=false: Removes both background grid and ticks. framestyle=:none: Removes both background grid and axes. using Plots surface(L, title=&amp;#34;default&amp;#34;) surface(L, title=&amp;#34;colorbar=:none&amp;#34;, colorbar=:none) surface(L,</description>
    </item>
    <item>
      <title>How to Create a Meshgrid in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3500/</link>
      <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3500/</guid>
      <description>Theorem There is no direct equivalent to the meshgrid() function used in Python and MATLAB. If you only want to obtain the function values on a grid, there is a simpler method that does not require creating a grid. Code 2D Multiplying a column vector by a row vector gives the same result as taking the Kronecker product of a column vector and a row vector. U(t,x) = si</description>
    </item>
    <item>
      <title>Broadcasting of Multivariable Functions in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3499/</link>
      <pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3499/</guid>
      <description>Overview Introducing how to broadcast multivariable functions in Julia. Like in Python, you can create a meshgrid, or you can easily calculate by creating vectors for each dimension. Bivariate Functions $$ u(t,x) = \sin(\pi x) e^{-\pi^{2}t} $$ To plot the function $(t,x) \in [0, 0.35] \times [-1,1]$ as above, the function values can be calculated like this: x = LinRange(-1., 1, 100) t = LinRange(0., 0.35, 200)&amp;#39; u1 = @.</description>
    </item>
    <item>
      <title>The Fast Fourier Transform Algorithm</title>
      <link>https://freshrimpsushi.github.io/en/posts/3492/</link>
      <pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3492/</guid>
      <description>Overview1 The Discrete Fourier Transform (DFT), when computed naively following its mathematical definition, has a time complexity of $\mathcal{O}(N^{2})$. However, by using the algorithm described below, the time complexity can be reduced to $\mathcal{O}(N\log_{2}N)$. This efficient computation method of the Discrete Fourier Transform is known as the Fast Fourier Transform (FFT). Buildup Let&amp;rsquo;s define multiplying two numbers and then adding them to another number as one operation. To compute the</description>
    </item>
    <item>
      <title>Specifying the Color of Axes, Axis Names, Ticks, and Tick Values in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3490/</link>
      <pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3490/</guid>
      <description>Theorem The keywords related to specifying the color of axes and ticks in Plots.jl are as follows. Keyword Name Function guidefontcolor Specify axis name color foreground_color_border, fgcolor_border Specify axis color foreground_color_axis, fgcolor_axis Specify tick color foreground_color_text, fgcolor_text Specify tick value color Adding x_ or y_ in front of the keyword name applies it to the respective axis only. Code1 Axis Names The keyword to specify the color of axis names</description>
    </item>
    <item>
      <title>Flux-PyTorch-TensorFlow Cheat Sheet</title>
      <link>https://freshrimpsushi.github.io/en/posts/3489/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3489/</guid>
      <description>Overview This document organizes code that performs the same functions in Flux, PyTorch, and TensorFlow. Julia-Matlab-Python-R Cheat Sheet Let&amp;rsquo;s assume the following environment for Flux. using Flux Let&amp;rsquo;s assume the following environment for PyTorch. import torch import torch.nn as nn import torch.nn.functional as F Let&amp;rsquo;s assume the following environment for TensorFlow. import tensorflow as tf from tensorflow import keras 1-Dimensional Tensor 줄리아Julia 파</description>
    </item>
    <item>
      <title>Summary of Measure Theory and Probability Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/3473/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3473/</guid>
      <description>Overview This is a summary of definitions and concepts for those who have already studied measure theory and probability. It is intended to be viewed when definitions are confusing or unrecognizable, and when a general review is needed. Measure Theory Algebras An algebra of sets on nonempty set $X$ is a nonempty collection $\mathcal{A}$ of subsets of $X$ is colsed under finite unions ans complements. $\sigma$-algebra is an algebra that</description>
    </item>
    <item>
      <title>Sampling Randomly from a Given Distribution in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3463/</link>
      <pubDate>Sat, 19 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3463/</guid>
      <description>설명 Using the Distributions.jl package, you can randomly sample from a given distribution.</description>
    </item>
    <item>
      <title>Sampling Randomly in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3462/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3462/</guid>
      <description>Description1 In Julia, the function for random sampling is as follows: rand([rng=default_rng()], [S], [dims...]) rng stands for Random Number Generator, which specifies the random number generation algorithm. If you don&amp;rsquo;t understand what this means, it&amp;rsquo;s okay to leave it untouched. S likely stands for Set, and it is a variable that specifies the set from which the random sampling will occur. The variables that can be input for S include</description>
    </item>
    <item>
      <title>CSS color name tags</title>
      <link>https://freshrimpsushi.github.io/en/posts/3459/</link>
      <pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3459/</guid>
      <description>Overview1 140+ CSS color palettes with names. Code</description>
    </item>
    <item>
      <title>General Linear Group</title>
      <link>https://freshrimpsushi.github.io/en/posts/3450/</link>
      <pubDate>Sat, 22 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3450/</guid>
      <description>Definition The set of real invertible $n \times n$ matrices is denoted by $\mathrm{GL}(n, \mathbb{R})$ or $\mathrm{GL}_{n}(\mathbb{R})$ and is called the general linear group of degree $n$general linear group of degree $n$. $$ \mathrm{GL}(n, \mathbb{R}) := \left\{ n \times n \text{ invertible matrix} \right\} = M_{n \times n}(\mathbb{R}) \setminus {\left\{ A \in M_{n \times n}(\mathbb{R}) : \det{A} = 0 \right\}} $$ Explanation Since it consists only of invertible matrices, it</description>
    </item>
    <item>
      <title>MNIST Database</title>
      <link>https://freshrimpsushi.github.io/en/posts/3444/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3444/</guid>
      <description>Overview1 $$ \includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$ The MNISTmodified national institute of standards and technology database refers to a dataset of digit handwriting from American high school students and Census Bureau employees. It is commonly known as [MNIST]. Official Website Description This dataset is frequently used as an example for beginners in machine learning/deep learning. NIST originally collected handwritten data in the following format for the evaluation of character recognition technology for automated</description>
    </item>
    <item>
      <title>How to Use Fast Fourier Transform (FFT) in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3440/</link>
      <pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3440/</guid>
      <description>Overview 1 2 The Fastest Fourier Transform in the West (FFTW) is a software library developed by Matteo Frigo and Steven G. Johnson at the Massachusetts Institute of Technology (MIT) for computing the Discrete Fourier Transform. While there exists a Julia package named AbstractFFTs.jl for FFT implementation, it is not intended to be used on its own but rather to aid in the implementation of fast Fourier transforms, such as</description>
    </item>
    <item>
      <title>How to Change Basic Data Types in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3439/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3439/</guid>
      <description>Overview In fields like machine learning, 32-bit floating point numbers are used instead of 64-bit ones for improving computation speed and saving memory. Therefore, in PyTorch, when tensors are created, their data type is fundamentally 32-bit floating point numbers by default. In Julia, there&amp;rsquo;s a machine learning package called Flux.jl, which takes Julia&amp;rsquo;s standard arrays as input for the neural networks it implements. The fact that it does not use</description>
    </item>
    <item>
      <title>Exchange Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3429/</link>
      <pubDate>Sat, 10 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3429/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Quantum Gates and Quantum Circuits</title>
      <link>https://freshrimpsushi.github.io/en/posts/3424/</link>
      <pubDate>Wed, 31 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3424/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Qubits: The Basic Unit of Information in Quantum Computers</title>
      <link>https://freshrimpsushi.github.io/en/posts/3423/</link>
      <pubDate>Mon, 29 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3423/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Bit: Unit of Information Classical Computer</title>
      <link>https://freshrimpsushi.github.io/en/posts/3422/</link>
      <pubDate>Sat, 27 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3422/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Tensor Product of Product Vectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/3415/</link>
      <pubDate>Sat, 13 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3415/</guid>
      <description>Buildup For convenience, we will develop the concept in the complex number space $\mathbb{C}$, but $\mathbb{R}$ or any vector space is also applicable. Let&amp;rsquo;s denote the set of functions from the finite set $\Gamma$ to the complex number space as indicated by $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ When $\Gamma = \mathbf{n} = \left\{ 1, \dots, n \right\}$, it essentially becomes $\mathbb{C}^{\mathbf{n}} = \mathbb{C}^{n}$,</description>
    </item>
    <item>
      <title>Tensor Product of Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/3414/</link>
      <pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3414/</guid>
      <description>Buildup1 For convenience, the discussion unfolds with respect to the complex number space $\mathbb{C}$, but it could equally apply to $\mathbb{R}$ or any arbitrary vector space. Let&amp;rsquo;s denote the set of functions from a finite set $\Gamma$ to a complex number space as $\mathbb{C}^{\Gamma}$. $$ \mathbb{C}^{\Gamma} = \left\{ f : \Gamma \to \mathbb{C} \right\} $$ Let $\Gamma$ be $\mathbf{n} = \left\{ 1, 2, \dots, n \right\}$. If we denote a</description>
    </item>
    <item>
      <title>Fredkin/CSWAP Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3412/</link>
      <pubDate>Sun, 07 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3412/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Toffoli/CCNOT Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3411/</link>
      <pubDate>Fri, 05 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3411/</guid>
      <description>Definition1 The following vector-valued Boolean function is called a Toffoli gateToffoli gate. $$ T : \left\{ 0, 1 \right\}^{3} \to \left\{ 0, 1 \right\}^{3} $$ $$ T (a, b, c) = (a, b, (a \land b) \oplus c) $$ The $\text{CCNOT}$ gateControlled Controlled NOT(CCNOT) gate is also known as. Description In the Toffoli gate, if the first two inputs are both $1$, the third input is inverted. In all other</description>
    </item>
    <item>
      <title>Controlled NOT(CNOT) Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3410/</link>
      <pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3410/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>What is a Functionally Complete Set?</title>
      <link>https://freshrimpsushi.github.io/en/posts/3408/</link>
      <pubDate>Sat, 29 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3408/</guid>
      <description>Definition1 Let&amp;rsquo;s assume that a set of Boolean functions $\left\{ f_{k} \right\} = \left\{ f_{k} : \left\{ 0, 1 \right\}^{n_{k}} \to \left\{ 0, 1 \right\} \right\}_{k\in \Gamma}$ is given. $\Gamma$ is a finite set. If any Boolean function $$ \left\{ 0, 1 \right\}^{n} \to \left\{ 0, 1 \right\}\quad (n \in \mathbb{N}) $$ can be expressed by the compositions of $\left\{ f_{k} \right\}$, then the set $\left\{ f_{k} \right\}$ is called</description>
    </item>
    <item>
      <title>NOR Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3407/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3407/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>NAND Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3406/</link>
      <pubDate>Tue, 25 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3406/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Exclusive Disjuction, XOR Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3405/</link>
      <pubDate>Sun, 23 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3405/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Negation, NOT Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3404/</link>
      <pubDate>Fri, 21 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3404/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Disjunction, OR Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3403/</link>
      <pubDate>Wed, 19 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3403/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Conjunction, AND Gate</title>
      <link>https://freshrimpsushi.github.io/en/posts/3402/</link>
      <pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3402/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Boolean Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3401/</link>
      <pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3401/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>고전정보이론에서 정보량이란?</title>
      <link>https://freshrimpsushi.github.io/en/posts/3398/</link>
      <pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3398/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Zero Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3394/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3394/</guid>
      <description>Definition The matrix of size $m\times n$ with all elements being $0$ is called a zero matrix, and is denoted as $O_{m\times n}$ or simply as $O$. Description Other notations include $Z_{m \times n}$, $Z$, $\mathbf{0}_{m\times n}$, or $\mathbf{0}$. It is better to write the number $0$ in bold to avoid confusion (actually, it is better just to use $O$). A zero matrix is the identity element for matrix addition.</description>
    </item>
    <item>
      <title>How to Change Axis Style in Julia Plots `framestyle`e`</title>
      <link>https://freshrimpsushi.github.io/en/posts/3376/</link>
      <pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3376/</guid>
      <description>Overview1 The framestyle attribute allows changing the style of the plot&amp;rsquo;s axes and border. The possible options are as follows: :box :semi :axes :origin :zerolines :grid :none Code The default setting is :axes. ▷code1◁ The styles for each attribute are as follows. ▷code2◁ https://docs.juliaplots.org/latest/generated/attributes_subplot/&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    <item>
      <title>Sum of Subspaces in a Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3371/</link>
      <pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3371/</guid>
      <description>Definition1 Let&amp;rsquo;s refer to $W_{1}, W_{2}$ as a subspace of the vector space $V$. The sum of $W_{1}$ and $W_{2}$ is denoted as $W_{1} + W_{2}$ and defined as follows. $$ W_{1} + W_{2} := \left\{ x + y : x\in W_{1}, y \in W_{2} \right\} $$ Generalization2 Let $W_{1}, W_{2}, \dots, W_{k}$ be a subspace of the vector space $V$. The sum of these subspaces is denoted as $W_{1}</description>
    </item>
    <item>
      <title>Shannon Entropy in Classical Information Theory</title>
      <link>https://freshrimpsushi.github.io/en/posts/3400/</link>
      <pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3400/</guid>
      <description>양자정보이론 [ 펼치기 · 접기 ] 양자계산 논리게이트 비트 · 부울함수(AND · OR · NOT · XOR · NAND · NOR · CNOT · CCNOT · CSWAP) · 범용 게이트 · 복제 함수 · 사영 · 주입 양자게</description>
    </item>
    <item>
      <title>Residual Classes and Quotient Spaces in Linear Algebra</title>
      <link>https://freshrimpsushi.github.io/en/posts/3359/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3359/</guid>
      <description>Definition1 Let&amp;rsquo;s refer to $V$ as $F$-vector space, and $W \le V$ as subspace. For $v \in V$, the following set $$ \left\{ v \right\} + W := \left\{ v + w : w \in W \right\} $$ is called the coset of $W$ containing $v$. $+$ is the sum of sets. Explanation We often abbreviate $\left\{ v \right\} + W$ as $v + W$. Considering the set of all</description>
    </item>
    <item>
      <title>Invariant Subspaces of Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/3353/</link>
      <pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3353/</guid>
      <description>Overview Let $\beta = v_{1}, \dots, v_{k}$ be the set of eigenvectors of the linear transformation $T : V \to V$. Then, it can be understood that $T$ maps $\span{\beta}$ to $\span{\beta}$. A subspace that maps itself to itself in this manner is defined as an invariant subspace. Definition1 Let $V$ be a vector space, and $T : V \to V$ a linear transformation. A subspace $W$ is called an</description>
    </item>
    <item>
      <title>Eigen Spaces of Linear Transformations and Geometric Multiplicity</title>
      <link>https://freshrimpsushi.github.io/en/posts/3349/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3349/</guid>
      <description>Definition1 Let&amp;rsquo;s define $V$ and $n$ as dimension vector spaces, $T : V \to V$ as linear transformation. Let&amp;rsquo;s also define $\lambda$ as the eigenvalue of $T$. The set defined as follows, $E_{\lambda}$, is called the eigenspace of $T$ corresponding to the eigenvalue $\lambda$. $$ E_{\lambda} = V_{\lambda} := \left\{ x \in V : Tx = \lambda x \right\} = N(T - \lambda I) $$ In this case, $N$ is</description>
    </item>
    <item>
      <title>Diagonalizable Linear Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3335/</link>
      <pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3335/</guid>
      <description>Definition 1 Let $V$ be called a finite-dimensional vector space. Let $T : V \to V$ be called a linear transformation. If there exists an ordered basis $\beta$ for which the matrix representation $\begin{bmatrix} T \end{bmatrix}_{\beta}$ of $T$ becomes a diagonal matrix, $T$ is said to be diagonalizable. For a square matrix $A$, if the $L_{A}$ is diagonalizable, then the matrix $A$ is said to be diagonalizable. Explanation Suppose the</description>
    </item>
    <item>
      <title>Characteristics Polynomial of Linear Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3339/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3339/</guid>
      <description>Overview The characteristic polynomial of linear transformation is defined. From the theorem below, it can be seen that solving equation $\det(A - \lambda I) = 0$ is equivalent to finding the eigenvalues. Therefore, it is quite natural to name $\det(A - \lambda I)$ the characteristic polynomial. Theorem1 Let&amp;rsquo;s say $F$ is any field, and $A \in M_{n\times n}(F)$. That $\lambda \in F$ is an eigenvalue of $A$ is equivalent to</description>
    </item>
    <item>
      <title>Eigenvalues and Eigenvectors of Finite-Dimensional Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3337/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3337/</guid>
      <description>Definition1 Let $V$ be a finite-dimensional $F$-vector space. Let $T : V \to V$ be a linear transformation. For $\lambda \in F$, $$ Tx = \lambda x $$ a non-zero vector $x \in V$ satisfying this is called an eigenvector of $T$. The scalar $\lambda \in F$ is called the eigenvalue corresponding to the eigenvector $x$. Explanation Although one might find the term eigenvector replaced by the terms characteristic vector</description>
    </item>
    <item>
      <title>Block Matrices</title>
      <link>https://freshrimpsushi.github.io/en/posts/3323/</link>
      <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3323/</guid>
      <description>Definition Let&amp;rsquo;s say $A$ is a matrix $m \times n$. $$ A = \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \\ \end{bmatrix} $$ Consider any vertical and horizontal lines that cut the matrix as follows. $$ A = \left[ \begin{array}{cc|ccc|c|} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}</description>
    </item>
    <item>
      <title>Expansion and Contraction of the Basis</title>
      <link>https://freshrimpsushi.github.io/en/posts/3321/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3321/</guid>
      <description>Theorem1 Let $S$ be a finite subset of the finite-dimensional vector space $V$. (a) If $S$ generates $V$ but is not a basis of $V$, then elements of $S$ can be appropriately removed to reduce it to a basis of $V$. (b) If $S$ is linearly independent but not a basis of $V$, then elements can be suitably added to $S$ to extend it to a basis of $V$. Corollary</description>
    </item>
    <item>
      <title>Differences in Array Dimensions in Julia, Python (NumPy, PyTorch)</title>
      <link>https://freshrimpsushi.github.io/en/posts/3315/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3315/</guid>
      <description>Overview When dealing with high-dimensional arrays in Julia and NumPy, PyTorch (hereinafter referred to collectively as Python for simplicity), it is important to pay attention to what each dimension signifies as they differ. This distinction arises because Julia&amp;rsquo;s arrays are column-major, whereas Python&amp;rsquo;s arrays are row-major. Note that Matlab, being column-major like Julia, does not have this discrepancy, so those familiar with Matlab need not be overly cautious, but those</description>
    </item>
    <item>
      <title>Differential Geometry of Curved Manifolds</title>
      <link>https://freshrimpsushi.github.io/en/posts/3314/</link>
      <pubDate>Fri, 21 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3314/</guid>
      <description>Definition1 Let&amp;rsquo;s denote $M$ as a Riemannian manifold, and $\frak{X}(M)$ as the set of all vector fields on $M$. $$ \frak{X}(M) = \text{the set of all vector fileds of calss } C^{\infty} \text{ on } M $$ The curvature $R$ of $M$ is a function that maps $X, Y \in \frak{X}(M)$ to $R(X, Y) : \frak{X}(M) \to \frak{X}(M)$. In this context, $R(X, Y)$ is given as follows. $$ \begin{equation} R(X,</description>
    </item>
    <item>
      <title>Paper Review: Physics-Informed Neural Networks</title>
      <link>https://freshrimpsushi.github.io/en/posts/3313/</link>
      <pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3313/</guid>
      <description>Overview The notation and numbering of references and formulas follow the conventions of the original paper. Physics-informed neural networks (referred to as PINN[pronounced &amp;lsquo;pin&amp;rsquo;]) are artificial neural networks designed to numerically solve differential equations, introduced in the 2018 Journal of Computational Physics paper Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. The authors of the paper are M. Raissi, P.</description>
    </item>
    <item>
      <title>Methods for Symbolic Computation in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3311/</link>
      <pubDate>Sat, 15 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3311/</guid>
      <description>## Overview Symbolic operations in Julia can be used through the `SymEngine.jl`[^1] package. [^1]: https://symengine.org/SymEngine.jl/ ## Code ### Defining Symbols Symbols can be defined in the following way. julia&amp;gt; using SymEngine julia&amp;gt; x = symbols(:x) x julia&amp;gt; x, y = symbols(&amp;ldquo;x y&amp;rdquo;) (x, y) julia&amp;gt; @vars x, y (x, y) julia&amp;gt; x = symbols(:x) x julia&amp;gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x) ### Vectors</description>
    </item>
    <item>
      <title>Power Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3307/</link>
      <pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3307/</guid>
      <description>Definition1 For a given matrix ▷ eq01◁ ▷ eq02◁, if there exists a positive number ▷ eq04◁ that satisfies ▷ eq03◁, then ▷ eq02◁ is referred to as nilpotent. In this case, ▷ eq06◁ is the zero matrix ▷ eq01◁</description>
    </item>
    <item>
      <title>Triangular Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3305/</link>
      <pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3305/</guid>
      <description>Definition1 A matrix $A = [a_{ij}]$ with all elements above the main diagonal being $0$ is called a lower triangular matrix. $$ A \text{ is lower triangluar matrix if } a_{ij} = 0 \text{ whenever } i \lt j $$ A matrix $A = [a_{ij}]$ with all elements below the main diagonal being $0$ is called an upper triangular matrix. $$ A \text{ is upper triangluar matrix if } a_{ij}</description>
    </item>
    <item>
      <title>Levi-Civita Connection, Riemannian Connection, Coefficients of Connection, Christoffel Symbols</title>
      <link>https://freshrimpsushi.github.io/en/posts/3292/</link>
      <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3292/</guid>
      <description>Theorem1 Let $(M,g)$ be a Riemannian manifold. Then, there uniquely exists an affine connection $\nabla$ on $M$ satisfying the following: $\nabla$ is symmetric. $\nabla$ is compatible with $g$. Such $\nabla$ specifically satisfies the following equation: $$ \begin{align*} g(Z, \nabla_{Y}X) =&amp;amp;\ \dfrac{1}{2}\Big( X g(Y, Z) + Y g(Z, X) - Z g(X, Y) \\ &amp;amp;\ - g([X, Z], Y) - g([Y, Z], X) - g([X, Y], Z) \Big) \tag{1} \end{align*} $$</description>
    </item>
    <item>
      <title>Homomorphism</title>
      <link>https://freshrimpsushi.github.io/en/posts/3287/</link>
      <pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3287/</guid>
      <description>Definition1 For two vector spaces $V, W$, if there exists an invertible linear transformation $T : V \to W$, then $V$ is said to be isomorphic to $W$, and is denoted as follows. $$ V \cong W $$ Furthermore, $T$ is called an isomorphism. Explanation By the equivalence condition of being invertible, saying $T$ is an isomorphism means that $T$ is a bijective function. Therefore, if there exists a bijective</description>
    </item>
    <item>
      <title>Inverse of Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3285/</link>
      <pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3285/</guid>
      <description>Definition1 Let $V, W$ be a vector space, and $T : V \to W$ be a linear transformation. If the linear transformation $U : W \to V$ satisfies the following, then $U$ is called the inverse or inverse transformation of $T$. $$ TU = I_{W} \quad \text{and} \quad UT = I_{V} $$ $TU$ is the composition of $U$ and $T$, $I_{X} : X \to X$ is the identity transformation. If</description>
    </item>
    <item>
      <title>Affine Connection</title>
      <link>https://freshrimpsushi.github.io/en/posts/3282/</link>
      <pubDate>Thu, 18 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3282/</guid>
      <description>Buildup Given a vector field $\mathbf{V}$ on a differentiable manifold, we can differentiate functions defined on the manifold using the vector field. Naturally, one might also want to differentiate the vector field itself. However, approaching the differentiation of the vector field $\mathbb{R}^{3}$ in the sense of differential geometry proves to be impossible as follows. First Case Let&amp;rsquo;s consider $S \subset \mathbb{R}^{3}$ as a surface and $c : I \to S$</description>
    </item>
    <item>
      <title>Linear Functional</title>
      <link>https://freshrimpsushi.github.io/en/posts/3281/</link>
      <pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3281/</guid>
      <description>Definitions1 Let&amp;rsquo;s call $V$ a vector space. A mapping $f$ from $V$ to $\mathbb{C}$ (or $\mathbb{R}$) is called a functional. $$ f : V \to \mathbb{C} $$ If $f$ is linear, it is called a linear functional. More Detailed Definitions2 Let&amp;rsquo;s call $V$ a vector space over the field $F$. Here, the field $F$ itself becomes a $1$-dimensional vector space over $F$. A linear transformation $f : V \to F$</description>
    </item>
    <item>
      <title>Order Basis and Coordinate Vectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/3279/</link>
      <pubDate>Fri, 12 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3279/</guid>
      <description>Definition1 Let&amp;rsquo;s say $V$ is a finite-dimensional vector space. When a specific order is assigned to a basis of $V$, it is called an ordered basis. Let&amp;rsquo;s say $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ is an ordered basis of $V$. Then, due to the uniqueness of basis representation, for $\mathbf{v} \in V$, scalars $a_{i}$ uniquely exist as follows. $$ \mathbf{v} = a_{1}\mathbf{v}_{1} + \dots a_{n}\mathbf{v}_{n} $$ $a_{1},\dots,a_{n}$ is called</description>
    </item>
    <item>
      <title>Riemann Metric and Riemann Manifolds</title>
      <link>https://freshrimpsushi.github.io/en/posts/3276/</link>
      <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3276/</guid>
      <description>Definition1 A Riemannian metric $g$ on a $n$-dimensional differentiable manifold $M$ is a function that maps each point $p \in M$ to $g_{p}$. Here, $g_{p}$ is an inner product defined in the tangent space $p$ over $T_{p}M$. $$ \begin{align*} g : M &amp;amp;\to \left\{ \text{all inner products on tangent space } T_{p}M \right\} \\ p &amp;amp;\mapsto g_{p}=\left\langle \cdot, \cdot \right\rangle_{p} \end{align*} $$ $$ \begin{align*} g_{p} : T_{p}M \times T_{p}M &amp;amp;\to</description>
    </item>
    <item>
      <title>Adding a New Column to a DataFrame in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3273/</link>
      <pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3273/</guid>
      <description>Code Let&amp;rsquo;s say we are given the Cosmic Girls dataframe as follows. WJSN = DataFrame( member = [&amp;#34;다영&amp;#34;,&amp;#34;다원&amp;#34;,&amp;#34;루다&amp;#34;,&amp;#34;소정&amp;#34;,</description>
    </item>
    <item>
      <title>Lie Brackets of Vector Fields</title>
      <link>https://freshrimpsushi.github.io/en/posts/3272/</link>
      <pubDate>Fri, 29 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3272/</guid>
      <description>Definition1 On two differentiable vector fields $X, Y$ on a differentiable manifold $M$, $[X, Y]$ is defined as follows, and is called the (Lie-)bracket or Lie algebra. $$ \begin{equation} [X, Y] := XY - YX \end{equation} $$ Explanation Vector field $X, Y$ can be seen as an operator acting on $\mathcal{D}(M)$, and $XY$ although not a vector field, $[X, Y] = XY - YX$ becomes a vector field. $(1)$ satisfying</description>
    </item>
    <item>
      <title>Vector Field on Differentiable Manifold</title>
      <link>https://freshrimpsushi.github.io/en/posts/3270/</link>
      <pubDate>Mon, 25 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3270/</guid>
      <description>Buildup1 Consider the easy definition of a vector field. In 3-dimensional space, a vector fieldvector function, vector field is a function $X : \mathbb{R}^{3} \to \mathbb{R}^{3}$ that maps a 3-dimensional vector to another 3-dimensional vector. When considering this in the context of manifolds, $X$ maps a point $\mathbb{R}^{3}$ on the differential manifold $p$ to a vector $\mathbb{R}^{3}$ in $\mathbf{v}$, treating this vector $\mathbf{v}$ as an operator to consider as a</description>
    </item>
    <item>
      <title>Tangent Bundles on Differentiable Manifolds</title>
      <link>https://freshrimpsushi.github.io/en/posts/3268/</link>
      <pubDate>Thu, 21 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3268/</guid>
      <description>Definition1 Let&amp;rsquo;s call a $M$ a $n$-dimensional differentiable manifold. Let&amp;rsquo;s denote the tangent space at point $p \in M$ as $T_{p}M$. The tangent bundle $TM$ of $M$ is defined as follows. $$ \begin{align*} TM &amp;amp;:= \bigsqcup \limits_{p \in M } T_{p}M \\ &amp;amp;= \bigcup_{p \in M} \left\{ p \right\} \times T_{p}M \\ &amp;amp;= \left\{ (p, v) : p \in M, v \in T_{p}M \right\} \end{align*} $$ Here, $\bigsqcup$ is a</description>
    </item>
    <item>
      <title>Supersaturated and Undersaturated Systems</title>
      <link>https://freshrimpsushi.github.io/en/posts/3265/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3265/</guid>
      <description>Definition1 Consider the following linear system for matrix $A$: $$ A \mathbf{x} = \mathbf{b} $$ If $m \gt n$, then there are more constraints than unknowns, and such a linear system is called an overdetermined system. If $m \lt n$, then there are less constraints than unknowns, and such a linear system is called an underdetermined system. Theorem 1 Consider the linear system for matrix $A$ with rank $r$ of</description>
    </item>
    <item>
      <title>Pull Back in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3262/</link>
      <pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3262/</guid>
      <description>Overview We define the pullback on a differential manifold. If differential manifolds are complex, one can think of $M = \mathbb{R}^{m}$ and $N = \mathbb{R}^{n}$. Definition1 Given two differential manifolds $M, N$ and a differentiable function $f : M \to N$, we can consider a function $f^{\ast}$ that maps $N$&amp;rsquo;s $k$-forms to $M$&amp;rsquo;s $k$-forms. Let $\omega$ be a $k$-form on the manifold $N$, then a $k$-form $f^{\ast}\omega$ on the manifold</description>
    </item>
    <item>
      <title>How to Perform Hierarchical Clustering in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3259/</link>
      <pubDate>Sun, 03 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3259/</guid>
      <description>Explanation Use the hclust() function from the Clustering.jl package. hclust(d::AbstractMatrix; [linkage], [uplo], [branchorder]) It takes a distance matrix as input and returns the result of hierarchical clustering. The default method for calculating distances between clusters is single linkage. To plot a dendrogram, use StatsPlots.jl instead of Plots.jl. Code using StatsPlots using Clustering using Distances using Distributions a = rand(Uniform(-1,1), 2, 25) scatt = scatter(a[1,:], a[2,:], label=false) savefig(scatt, &amp;#34;julia_hclust_scatter.png&amp;#34;) D_a =</description>
    </item>
    <item>
      <title>kth Order Differential Forms</title>
      <link>https://freshrimpsushi.github.io/en/posts/3258/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3258/</guid>
      <description>Overview Just as we defined the second-order differential form, we generalize to define the k-th order forms on the differential manifold $M$. If the concept of a differential manifold is challenging, it can be simply thought of as $M = \mathbb{R}^{n}$. Build-up Let&amp;rsquo;s say $M$ is a $n$-dimensional differential manifold. $p \in M$ is a point in $M$, and $T_{p}M$ is the tangent space at point $p$ in $M$. $T_{p}^{\ast}M$</description>
    </item>
    <item>
      <title>How to Draw a Dendrogram in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3257/</link>
      <pubDate>Wed, 29 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3257/</guid>
      <description>Explanation When attempting to draw a dendrogram by using the plot() function after performing hierarchical clustering with hclust() on the given data, the following error occurs. using Clustering using Distances using Plots a = rand(2, 10) D_a = pairwise(Euclidean(), a, a) SL = hclust(D_a, linkage=:single) dendrogram = plot(SL) ERROR: LoadError: Cannot convert Hclust{Float64} to series data for plotting To draw a dendrogram, one should use StatsPlots.jl instead of Plots.jl. using</description>
    </item>
    <item>
      <title>Second-Order Differential Form</title>
      <link>https://freshrimpsushi.github.io/en/posts/3256/</link>
      <pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3256/</guid>
      <description>Overview We define the binary operation $\wedge$ and, in the sense that we defined the first-order differential form, we define a second-order form for the differential manifold $M$. If differential manifolds seem difficult, one can think of them as $M = \mathbb{R}^{n}$. Buildup1 Let’s consider the first-order form $\omega$. $$ \begin{align*} \omega : M &amp;amp;\to T^{\ast}M \\ p &amp;amp;\mapsto \omega_{p} \end{align*} $$ This maps a point</description>
    </item>
    <item>
      <title>Cotangent Space and First-Order Differential Forms</title>
      <link>https://freshrimpsushi.github.io/en/posts/3254/</link>
      <pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3254/</guid>
      <description>Overview We define the cotangent space and the differential 1-form. If differential manifolds are challenging, one can think of it as $M = \mathbb{R}^{n}$. We use Einstein notation. Cotangent Space1 Let&amp;rsquo;s consider a $M$ as a $n$-dimensional differential manifold. Then, the tangent space $T_{p}M$ at point $p \in M$ becomes a $n$-dimensional vector space (function space), with the basis being $\left\{ \mathbf{e}_{i} = \left. \frac{\partial }{\partial x_{i}}\right|_{p} \right\}_{i}$. At this</description>
    </item>
    <item>
      <title>Gauss-Bonnet Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/3238/</link>
      <pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3238/</guid>
      <description>Gauss-Bonnet Theorem Let&amp;rsquo;s consider $\mathbf{x} : U \to \mathbb{R}^{3}$ as a simple connected geodesic coordinate chart, and $\boldsymbol{\gamma}(I) \subset \mathbf{x}(U)$, which is $\boldsymbol{\gamma}$, as piecewise regular curves. Also, let&amp;rsquo;s say that $\boldsymbol{\gamma}$ surrounds some region $\mathscr{R}$. Then, the following holds true. $$ \iint_{\mathscr{R}} K dA + \int_{\boldsymbol{\gamma}} \kappa_{g} ds + \sum \alpha_{i} = 2\pi $$ Here, $K$ denotes the Gaussian curvature, $\kappa_{g}$ denotes the geodesic curvature, and $\alpha_{i}$ denotes the</description>
    </item>
    <item>
      <title>Simple Connected Region</title>
      <link>https://freshrimpsushi.github.io/en/posts/3236/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3236/</guid>
      <description>Definitions Let $\mathscr{R}$ be a region of the surface $M$. If every closed curve within $\mathscr{R}$ is null-homotopic, then $\mathscr{R}$ is said to be simply connected. Description Easy examples such as $\mathbb{R}^{2}$, disk $\left\{ x^{2} + y^{2} = r^{2} \right\}$, and sphere $\mathbb{S}^{2}$ are immediately thought to be simply connected. However, as shown in the figure below, one can see that the torus $T^{2}$ is not simply connected. Unlike $\gamma$,</description>
    </item>
    <item>
      <title>Differentiable Surfaces and Boundaries of Regions in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3230/</link>
      <pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3230/</guid>
      <description>Region1 Consider a subset $\mathscr{R}$ of a surface $M$. If $\mathscr{R}$ is an open set, and for any two points in $\mathscr{R}$ there exists a curve on $\mathscr{R}$ containing both, then $\mathscr{R}$ is called a region of $M$. Boundary For a region $\mathscr{R}$ of a surface $M$, the following set $\partial \mathscr{R}$ is called the boundary of $\mathscr{R}$. $$ \partial \mathscr{R} = \left\{ p \notin \mathscr{R} : \exists \left\{ p_{j}</description>
    </item>
    <item>
      <title>How to Directly Define Multidimensional Arrays in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3223/</link>
      <pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3223/</guid>
      <description>Explanation 1D arrays (vectors) are defined as follows. julia&amp;gt; A = [1; 2; 3] 3-element Vector{Int64}: 1 2 3 Here, ; signifies moving to the next element based on the first dimension. By generalizing this, ;; signifies moving to the next element based on the second dimension. julia&amp;gt; A = [1; 2; 3;; 4; 5; 6] 3×2 Matrix{Int64}: 1 4 2 5 3 6 Similarly, arrays of three</description>
    </item>
    <item>
      <title>List of Available Commands in Julia Package Management Mode</title>
      <link>https://freshrimpsushi.github.io/en/posts/3217/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3217/</guid>
      <description>Description By typing the right bracket ] in the Julia REPL, you can switch to package management mode. The available commands in package management mode are as follows. Command Function add foo Adds the package foo. free foo Unpins the package version. help, ? Shows these commands. pin foo Pins the version of the package foo. remove foo, rm foo Removes the package foo. test foo Test-runs the package foo.</description>
    </item>
    <item>
      <title>How to load a npy file in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3215/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3215/</guid>
      <description>설명 This document outlines the process of calculating the Radon transform $\mathcal{R}f$ of a phantom $f$ in Python and saving the results as a *.npy file. To load this file in Julia, one can use the PyCall.jl package. using PyCall np = pyimport(&amp;#34;numpy&amp;#34;) The above code is equivalent to executing import numpy as np in Python. This allows one to directly use the code written for numpy in Python</description>
    </item>
    <item>
      <title>Overlaying Plots on Heatmaps in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3213/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3213/</guid>
      <description>Code Let&amp;rsquo;s say we want to draw a sine curve from $0$ to $2\pi$ on the heatmap of the array $(5,5)$. You might want to write the code like this, but as you can see in the figure, it doesn&amp;rsquo;t output as desired. using Plots A = rand(Bool, 5,5) heatmap(A, color=:greens) x = range(0, 2pi, length=100) y = sin.(x) plot!(x, y, color=:red, width=3) This is because the horizontal and vertical</description>
    </item>
    <item>
      <title>Performing Operations on Vectors of Different Sizes Component-wise in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3207/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3207/</guid>
      <description>Description julia&amp;gt; x = [1 2 3] 1×3 Matrix{Int64}: 1 2 3 julia&amp;gt; y = [1 2 3 4] 1×4 Matrix{Int64}: 1 2 3 4 julia&amp;gt; x .+ y ERROR: DimensionMismatch Two vectors of different sizes cannot perform element-wise operations by default. To implement this manually, one would have to use a double for loop, but fortunately, it can be easily calculated by treating one as</description>
    </item>
    <item>
      <title>Riemann Curvature Tensor, Gauss Equation, and Codazzi-Mainardi Equation in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3206/</link>
      <pubDate>Sat, 19 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3206/</guid>
      <description>Definition1 The coefficients of the Riemannian curvature tensor $R_{ijk}^{l}$ are defined as follows. $$ R_{ijk}^{l} = \dfrac{\partial \Gamma_{ik}^{l}}{\partial u^{j}} - \dfrac{\partial \Gamma_{ij}^{l}}{\partial u^{k}} + \sum_{p} \left( \Gamma_{ik}^{p} \Gamma_{pj}^{l} - \Gamma_{ij}^{p}\Gamma_{pk}^{l}\right) \text{ for } 1 \le i,j,k,l \le 2 $$ Here, $\Gamma_{ij}^{k}$ is the Christoffel symbol. Explanation Since Christoffel symbols are intrinsic, the Riemann curvature tensor is also intrinsic. The so-called coefficients that appear in differential geometry do not depend on</description>
    </item>
    <item>
      <title>Methods for Coloring Up to a Certain Value from Curves in Julia / Between Two Curves / Inside a Closed Curve</title>
      <link>https://freshrimpsushi.github.io/en/posts/3203/</link>
      <pubDate>Sun, 13 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3203/</guid>
      <description>Fill up to a Specific Value1 Using attributes fillrange=a, fillalpha=b, fillcolor=:color in plot(), it colors with :color to the value a from the plotted curve with the transparency b. It works the same by writing fill=(a,b,:color). That is, the following two codes are the same. plot(x,y, fillrange=a, fillalpha=b, fillcolor=:color) plot(x,y, fill=(a,b,:color)) It seems to be a bug, but selecting the value of fillrange as $(0,1)$ does not get colored. using</description>
    </item>
    <item>
      <title>Gaussian Curvature and Mean Curvature</title>
      <link>https://freshrimpsushi.github.io/en/posts/3200/</link>
      <pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3200/</guid>
      <description>Definition1 Let&amp;rsquo;s consider the principal curvature at a point $p$ on a surface $M$ be denoted as $\kappa_{1}, \kappa_{2}$. Let $L$ be referred to as the Weingarten map. The Gaussian curvature $K$ is defined as follows: $$ K := \kappa_{1} \kappa_{2} = \det L = \det ([{L^{i}}_{j}]) $$ where ${L^{i}}_{j} = \sum \limits_{k} L_{kj}g^{ki}$ applies. Formula The product of the principal curvatures $$ K = \kappa_{1} \kappa_{2} $$ Gaussian curvature</description>
    </item>
    <item>
      <title>Curvature of a Principal Curve</title>
      <link>https://freshrimpsushi.github.io/en/posts/3194/</link>
      <pubDate>Wed, 23 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3194/</guid>
      <description>Buildup1 To know in which direction and how much a surface $M$ is curved, it is sufficient to know the normal curvatures $\kappa_{n}$ in each direction. In other words, knowing all $\kappa_{n}$ at point $p$ allows us to understand how $M$ is bent. The first step towards this is to think about the maximum and minimum values of $\kappa_{n}$. The following theorem applies to the unit tangent curve $\boldsymbol{\gamma}$: Lemma</description>
    </item>
    <item>
      <title>How to Get Column and Row Labels of Data Frame in Python Pandas</title>
      <link>https://freshrimpsushi.github.io/en/posts/3189/</link>
      <pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3189/</guid>
      <description>Code import pandas as pd data = { &amp;#39;나이&amp;#39; : [26,23,22,22,21,21,20,20,20,20,18,17], &amp;#39;키&amp;#39; : [160, 163, 163, 162, 164, 163, 164, 150, 158, 162, 172, 173], &amp;#39;별명&amp;#39; : [&amp;#39;땡모&amp;#3</description>
    </item>
    <item>
      <title>Bingarten Map</title>
      <link>https://freshrimpsushi.github.io/en/posts/3188/</link>
      <pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3188/</guid>
      <description>Definition1 Let $M$ be a surface, and $p \in M$ be a point on the surface. The map $L : T_{p}M \to \mathbb{R}^{3}$, defined as follows, is called the Weingarten map. $$ L (\mathbf{X}) = - \mathbf{X}\mathbf{n} $$ Here, $\mathbf{X} \in T_{p}M$ is a tangent vector, $\mathbf{n}$ is a unit normal, and $\mathbf{X}\mathbf{n}$ is the directional derivative of $\mathbf{n}$. Properties $L$ is a linear transformation that is $L : T_{p}M</description>
    </item>
    <item>
      <title>Definition of Parallel Vector Field along a Curve on Surface</title>
      <link>https://freshrimpsushi.github.io/en/posts/3174/</link>
      <pubDate>Fri, 14 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3174/</guid>
      <description>Vector Field Along a Curve1 Definition Given a surface $M$ and a curve $\alpha : \left[ a, b \right] \to M$, let us consider a function $\mathbf{X}$ that maps each $t \in \left[ a,b \right]$ to a tangent vector at point $\alpha (t)$ on surface $M$. This function $\mathbf{X}$ is called a vector field along curve $\alpha$vector field along a curve $\alpha$. $$ \mathbf{X} : \left[ a, b \right] \to</description>
    </item>
    <item>
      <title>Rotational Surfaces in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3170/</link>
      <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3170/</guid>
      <description>Definition1 Let $z$ be the variable on the given axis, and $r&amp;gt;0$ be the distance from the $z-$ axis. Then, one can consider the curve $\alpha$ on the $rz-$ plane as shown in the figure below. As shown in the figure below, the surface obtained by rotating the curve $\alpha$ about the $z-$ axis is called a surface of revolution. The surface of revolution is expressed as follows. $$ \mathbf{x}(t,</description>
    </item>
    <item>
      <title>Identity Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/3167/</link>
      <pubDate>Fri, 31 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3167/</guid>
      <description>Definition1 Given a set $X$, the following function $I_{X} : X \to X$ is called the identity function. $$ I_{X}(x) = x,\quad \forall x \in X $$ Explanation The following notations are commonly used. $$ I,\quad \text{id},\quad \text{1} $$ Tangent vectors on a differentiable manifold are defined as follows in $\dfrac{d (f\circ \alpha)}{d t}$, where the function to be differentiated $$ f \circ \alpha = f \circ I \circ \alpha</description>
    </item>
    <item>
      <title>The Christoffel Symbols are Intrinsic</title>
      <link>https://freshrimpsushi.github.io/en/posts/3164/</link>
      <pubDate>Sat, 25 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3164/</guid>
      <description>Theorem1 The Christoffel symbols $\Gamma_{ij}^{k}$ satisfy the following equation. In other words, they are intrinsic. $$ \Gamma_{ij}^{k} = \dfrac{1}{2} \sum \limits_{l=1}^{2} g^{lk} \left( \dfrac{\partial g_{lj}}{\partial u_{i}} - \dfrac{\partial g_{ij}}{\partial u_{l}} + \dfrac{\partial g_{il}}{\partial u_{j}} \right) $$ Explanation Gauss proved it. The Christoffel symbols depend only on the Riemann metric and are independent of the normal vector. Therefore, by using Christoffel symbols, one can understand the structure of a surface without</description>
    </item>
    <item>
      <title>Taylor&#39;s Theorem for Multivariable Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3163/</link>
      <pubDate>Thu, 23 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3163/</guid>
      <description>Theorem1 Let $f : \mathbb{R}^{n} \to \mathbb{R}$ be $C^{k}$ function, and call it $\mathbf{a} = (a_{1}, \dots, a_{n}) \in \mathbb{R}^{n}$. Then, there exists $C^{k-2}$ function $h_{ij}$ that satisfies the following. $$ f(\mathbf{x}) = f(\mathbf{a}) + \sum_{i} (x_{i} - a_{i})\dfrac{\partial f}{\partial x_{i}}(\mathbf{a}) + \sum_{i,j}h_{ij}(\mathbf{x})(x_{i} - a_{i}) (x_{j} - a_{j}) $$ Description It generalizes the Taylor theorem to functions of several variables. second-order $$ \begin{align*} f(\mathbf{x}) &amp;amp;= f(\mathbf{a}) + \sum\limits_{i=1}^{n} (x_{i} -</description>
    </item>
    <item>
      <title>Definitions of Intrinsic/Essential in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3162/</link>
      <pubDate>Tue, 21 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3162/</guid>
      <description>Definition1 In differential geometry, a function that depends only on the coefficients of the first fundamental form ([eq2])(../3148) and not on the unit normal(../2110) is referred to as intrinsic. &amp;lt;!&amp;ndash; it means a quantity that depends only on the intrinsic g(Riemannian metric). Richard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p106&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    <item>
      <title>Gauss&#39;s Theorem in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3160/</link>
      <pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3160/</guid>
      <description>정리1 Let&amp;rsquo;s call $\mathbf{x} : U \to \R^{3}$ the coordinate patch. Let $(u_{1}, u_{2})$ be the coordinates of $U$. Let $\mathbf{n}$ be the unit normal, $L_{ij} = \left\langle \mathbf{x}_{ij}, \mathbf{n} \right\rangle$ the coefficients of the second fundamental form, and $\Gamma_{ij}^{k} = \sum \limits_{l=1}^{2} \left\langle \mathbf{x}_{ij}, \mathbf{x}_{l} \right\rangle g^{lk} = \left\langle \mathbf{x}_{ij}, \mathbf{x}_{l} \right\rangle g^{lk}$ the Christoffel symbols. Then, the following are true: (a) Gauss&amp;rsquo;s formulas: $$ \mathbf{x}_{ij} =</description>
    </item>
    <item>
      <title>Christoffel Symbols in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3158/</link>
      <pubDate>Mon, 13 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3158/</guid>
      <description>Buildup Let $\mathbf{x} : U \to \mathbb{R}^{3}$ represent a coordinate mapping. In differential geometry, the characteristics and properties of geometric objects are described through differentiation. Therefore, the derivatives of the coordinate fragments $\mathbf{x}$ appear in various theorems and formulas. For instance, the first-order derivatives $\left\{ \mathbf{x}_{1}, \mathbf{x}_{2} \right\}$ become the basis of the tangent space $T_{p}M$. Hence, any tangent vector $\mathbf{X} \in T_{p}M$ can be expressed as follows: $$ \mathbf{X}</description>
    </item>
    <item>
      <title>Second Fundamental Form in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3156/</link>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3156/</guid>
      <description>Build-up Let $\mathbf{x} : U \to \mathbb{R}^{3}$ be referred to as a chart. In differential geometry, the characteristics and properties of geometric objects are explained through differentiation. Hence, the derivatives of coordinate charts $\mathbf{x}$ appear in various theorems and formulas. For instance, the first-order derivatives $\left\{ \mathbf{x}_{1}, \mathbf{x}_{2} \right\}$ become the basis of the tangent space $T_{p}M$. Therefore, any tangent vector $\mathbf{X} \in T_{p}M$ can be expressed as follows. $$</description>
    </item>
    <item>
      <title>Gauss Curvature and Geodesic Curvature</title>
      <link>https://freshrimpsushi.github.io/en/posts/3154/</link>
      <pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3154/</guid>
      <description>Buildup1 $$ \left\{ T(s), N(s), B(s), \kappa (s), \tau (s) \right\} $$ Recall how we used the Frenet-Serret apparatus when analyzing curves. When studying surfaces, we will consider similar concepts. When $\boldsymbol{\alpha}$ is the unit speed curve, the curvature of the curve was defined as the magnitude of acceleration $\kappa = \left| T^{\prime} \right| = \left| \boldsymbol{\alpha}^{\prime \prime} \right|$. It is natural to think about how curved a surface is</description>
    </item>
    <item>
      <title>Parametric Curves on a Simple Surface</title>
      <link>https://freshrimpsushi.github.io/en/posts/3152/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3152/</guid>
      <description>Definition1 2 Let $\mathbf{x} : U \to \R^{3}$ be called a simple surface. Let the coordinates of $U$ be called $(u, v)$. For any point $(u_{0}, v_{0})$, the following curve is called the $u-$parameter curve at $v = v_{0}$ of $\mathbf{x}$. $$ u \mapsto \mathbf{x}(u, v_{0}) $$ The following curve is called the $v-$parameter curve at $u = u_{0}$ of $\mathbf{x}$. $$ v \mapsto \mathbf{x}(u_{0}, v) $$ The velocity vectors</description>
    </item>
    <item>
      <title>Boundary Value Problems in Partial Differential Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3151/</link>
      <pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3151/</guid>
      <description>Definition Given a partial differential equation defined in an open set $\Omega$, let&amp;rsquo;s assume the values of the unknown $u$ are given on the boundary $\partial \Omega$ of $\Omega$. This is called a boundary condition. The partial differential equation together with the boundary condition is referred to as a boundary value problem. Description The abbreviation BVP is commonly used. To solve a boundary value problem means to find a solution</description>
    </item>
    <item>
      <title>First Basic Forms, Riemannian Metrics</title>
      <link>https://freshrimpsushi.github.io/en/posts/3148/</link>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3148/</guid>
      <description>Buildup Riemannian metric is a concept that comes from the process of calculating the length of curves on a surface, and the process is as follows. Let&amp;rsquo;s say $\boldsymbol{\alpha}(t)$ is a regular curve moving on a simple surface $\mathbf{x} : U \to \mathbb{R}^{3}$. Let&amp;rsquo;s say $(u_{1}, u_{2})$ are the coordinates in $U$. Then, $\boldsymbol{\alpha}$ can be expressed as follows. $$ \boldsymbol{\alpha}(t) = \mathbf{x}(u_{1}(t), u_{2}(t)) $$ At this point, the length</description>
    </item>
    <item>
      <title>Sign function</title>
      <link>https://freshrimpsushi.github.io/en/posts/3147/</link>
      <pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3147/</guid>
      <description>Definition Sign function $\mathrm{sgn} : \mathbb{R} \to \mathbb{R}$ is defined as follows. $$ \mathrm{sgn}(x) :=\begin{cases} 1 &amp;amp; x&amp;gt;0 \\ 0 &amp;amp; x=0 \\ -1 &amp;amp; x&amp;lt;0 \end{cases} $$ Explanation It is mainly used to simplify the notation of equations or definitions. It is also written as $\mathrm{sign}$. See Also Sign of complex numbers</description>
    </item>
    <item>
      <title>Definition of Surfaces in Differential Geometry</title>
      <link>https://freshrimpsushi.github.io/en/posts/3146/</link>
      <pubDate>Fri, 19 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3146/</guid>
      <description>Definition1 If for every point $M \subset \R^{3}$ of $P \in M$, there exists an $C^{k}$ diffeomorphism $\mathbf{x} : U \subset \R^{2} \to M$ such that the image $\mathbf{x}(U)$ contains some $\epsilon-$neighborhood $N_{p}$ of $P$, then $M$ is called a $\R^{3}$ surface. Moreover, for such two diffeomorphisms $\mathbf{x} : U \to \R^{3}$ and $\mathbf{y} : V \to \R^{3}$, $$ \mathbf{y}^{-1} \circ \mathbf{x} : \mathbf{x}^{-1}\left( \mathbf{x}(U) \cap \mathbf{y}(V) \right) \to \mathbf{y}^{-1}\left(</description>
    </item>
    <item>
      <title>How to Read and Write Greek Characters and Their Meaning in Mathematics and Science</title>
      <link>https://freshrimpsushi.github.io/en/posts/3145/</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3145/</guid>
      <description>Alpha $\Alpha, \alpha$ Alpha is read as &amp;ldquo;alpha&amp;rdquo;. The TeX codes are \Alpha, \alpha respectively. It is the first letter of the Greek alphabet, and the phrase &amp;ldquo;alpha and omega&amp;rdquo; means &amp;ldquo;the beginning and the end.&amp;rdquo; Index of an index set $\alpha$ In differential geometry, a curve $\alpha$ Curve used to define tangent vectors on a differential manifold $\alpha$ Beta $\Beta, \beta$ Beta is read as &amp;ldquo;beta&amp;rdquo;. The TeX codes</description>
    </item>
    <item>
      <title>Tangent Vectors on Simple Surfaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/3142/</link>
      <pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3142/</guid>
      <description>Definition1 Consider a point $p = \mathbf{x}(a,b)$ on a coordinate patch $\mathbf{x} : U \to \mathbb{R}^{3}$. If a vector $\mathbf{X}$ is the velocity vector at $p$ of some curve $\mathbf{x}(U)$ on the curve passing through $p$, then $\mathbf{X}$ is defined as the tangent vector to the simple surface $\mathbf{x}$. In other words, if for any arbitrary $\epsilon &amp;gt; 0$, there exists a suitably short curve $\boldsymbol{\alpha} : (-\epsilon, \epsilon) \to</description>
    </item>
    <item>
      <title>Differentiation of Functions Defined on Differentiable Manifolds</title>
      <link>https://freshrimpsushi.github.io/en/posts/3136/</link>
      <pubDate>Sat, 30 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3136/</guid>
      <description>Theorem1 Let&amp;rsquo;s call $M_{1}^{n}, M_{2}^{m}$ and $m, n$, respectively, $m, n$-dimensional differentiable manifolds. Let&amp;rsquo;s say $\varphi : M_{1} \to M_{2}$ is a differentiable function. And for every point $p \in M_{1}$ and tangent vector $v \in T_{p}M$, choose a differentiable curve $$\alpha : (-\epsilon, \epsilon) \to M_{1} \text{ with } \alpha (0) = p,\ \alpha^{\prime}(0)=v$$ Let&amp;rsquo;s set it as $\beta = \varphi \circ \alpha$. Then, the following mapping $$ d\varphi_{p}</description>
    </item>
    <item>
      <title>How to Find Derivatives in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3135/</link>
      <pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3135/</guid>
      <description>Overview1 The package is named Calculus.jl, but it does not support integration. If automatic differentiation, as discussed in machine learning, is needed, refer to the Zygote.jl package. Differentiation of Single Variable Function Derivative function derivative() It calculates the derivative of $f : \R \to \R$. derivative(f) or derivative(f, :x): Returns the derivative $f^{\prime}$. derivative(f, a): Returns the differential coefficient $f^{\prime}(a)$. julia&amp;gt; f(x) = 1 + 2x + 3x^2 f (generic</description>
    </item>
    <item>
      <title>Chain Rule for Multivariable Vector Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3134/</link>
      <pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3134/</guid>
      <description>Theorem Let&amp;rsquo;s assume that two functions $\mathbf{g} : D \subset \mathbb{R}^{m} \to \mathbb{R}^{k}$, $\mathbf{f} : \mathbf{g}(\mathbb{R}^{k}) \subset \mathbb{R}^{k} \to \mathbb{R}^{n}$ are differentiable. Then, the composition of these two functions $\mathbf{F} = \mathbf{f} \circ \mathbf{g} : \mathbb{R}^{m} \to \mathbb{R}^{n}$ is also differentiable, and the (total) derivative of $\mathbf{F}$ satisfies the following. $$ \mathbf{F}^{\prime}(\mathbf{x}) = \mathbf{f}^{\prime}\left( \mathbf{g}(\mathbf{x}) \right) \mathbf{g}^{\prime}(\mathbf{x}) $$ Explanation This is called the chain rule. If we denote $\mathbf{x} =</description>
    </item>
    <item>
      <title>Tangent Vector on Differentiable Manifold</title>
      <link>https://freshrimpsushi.github.io/en/posts/3132/</link>
      <pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3132/</guid>
      <description>Buildup1 To define a tangent vector at each point on a differentiable manifold $M$, let&amp;rsquo;s assume a differentiable curve $\alpha : (-\epsilon , \epsilon) \to M$ is given. We would like to define the derivative $\dfrac{d \alpha}{dt}(0)$ at $t=0$ in $\alpha$ as a tangent vector, like in differential geometry, but since the range of $\alpha$ is $M$ (since it&amp;rsquo;s not guaranteed to be a metric space), we cannot speak of</description>
    </item>
    <item>
      <title>Differentiable Functions from a Differentiable Manifold to a Differentiable Manifold</title>
      <link>https://freshrimpsushi.github.io/en/posts/3130/</link>
      <pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3130/</guid>
      <description>Definition1 Given that $M_{1}, M_{2}$ are each a $n, m$-dimensional differentiable manifold, a mapping $\varphi : M_{1} \to M_{2}$ is defined to be differentiable at $p \in M_{1}$ if it satisfies the following conditions: Whenever a coordinate system $\mathbf{y} : V \subset \mathbb{R}^{m} \to M_{2}$ is given in $\varphi(p)$, there exists a coordinate system $\mathbf{x} : U \subset \mathbb{R}^{n} \to M_{1}$ in $p$ such that $\varphi\left( \mathbf{x}(U) \right) \subset \mathbf{y}(V)$</description>
    </item>
    <item>
      <title>Expansion and Contraction of a Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/3123/</link>
      <pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3123/</guid>
      <description>Definition1 Let&amp;rsquo;s assume that function $f : X \to Y$ is given. Let&amp;rsquo;s also assume that $U \subset X \subset V$ holds. Contraction Mapping We call $f |_{U} \to Y$ a contraction mapping of $f$ if it satisfies the following. $$ f|_{U} : U \to Y \quad \text{and} \quad f|_{U}(x) = f (x),\quad \forall x \in U $$ Extension We call $\tilde{f} \to Y$ an extension of $f$ if it</description>
    </item>
    <item>
      <title>Helmholtz Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3122/</link>
      <pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3122/</guid>
      <description>Definition The following partial differential equation is called the Helmholtz equation. $$ \nabla^{2}u(x) + k^{2} u(x) = \Delta u(x) + k^{2} u(x) = (\Delta + k^{2} )u(x) = 0,\quad x \in \mathbb{R}^{n} $$ Here, $\nabla ^{2} = \Delta$ is the Laplacian. Explanation It can also be expressed in the form of $-\Delta u = \lambda u$. Hence it is sometimes called the eigenvalue equation for the Laplace operator. It can</description>
    </item>
    <item>
      <title>Differentiable Manifolds</title>
      <link>https://freshrimpsushi.github.io/en/posts/3116/</link>
      <pubDate>Mon, 20 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3116/</guid>
      <description>Definition1 Let $M$ be an arbitrary set and $U_{\alpha} \subset \mathbb{R}^{n}$ be an open set. For the function $1-1$ $\mathbf{x}_{\alpha} : U_{\alpha} \to M$, the ordered pair $\left( M, \left\{ \mathbf{x}_{\alpha} \right\}_{\alpha\in \mathscr{A}} \right)$, or simply $M$, is defined as a differentiable manifold of dimension $n$ if the following conditions are met: $\bigcup \limits_{\alpha} \mathbf{x}_{\alpha} \left( U_{\alpha} \right) = M$ For $\varnothing \ne W = \mathbf{x}_{\alpha}\left( U_{\alpha} \right) \cap \mathbf{x}_{\beta}\left(</description>
    </item>
    <item>
      <title>Definition of Smooth Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3110/</link>
      <pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3110/</guid>
      <description>Definition If a function $f$ is infinitely differentiable, then $f$ is referred to as a smooth function. If a function $f$ is differentiable and $f^{\prime}$ is continuous, then $f$ is referred to as a smooth function. Explanation $y= \left| x \right|$ is sharp in $x=0$, making it impossible to differentiate in $x=0$. Therefore, it is described as smooth when differentiation works well at every point, and the differentiated function is</description>
    </item>
    <item>
      <title>Definition of Directional Derivative</title>
      <link>https://freshrimpsushi.github.io/en/posts/3109/</link>
      <pubDate>Mon, 06 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3109/</guid>
      <description>Buildup Let&amp;rsquo;s say a multivariable function $f = \mathbb{R}^{n} \to \mathbb{R}$ is given. When trying to calculate the derivative of $f$, unlike the case with a univariable function, one must consider the rate of change in &amp;lsquo;which direction&amp;rsquo;. A familiar example is the partial derivative. The partial derivative considers the rate of change with respect to only one variable. For instance, the partial derivative $\dfrac{\partial f}{\partial y}$ of $f=f(x,y,z)$ with</description>
    </item>
    <item>
      <title>Cauchy Problem, Initial Value Problem</title>
      <link>https://freshrimpsushi.github.io/en/posts/3093/</link>
      <pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3093/</guid>
      <description>Definition1 Let&amp;rsquo;s say we have a partial differential equation defined on an open set $\Omega=\mathbb{R}^{n}$. When the time is $t=0$, the value of the unknown $u$ at $\Omega$, that is, the initial value, is given. The problem of finding solutions to such partial differential equations is called the Cauchy problem or the initial value problem. Explanation The acronym IVP is commonly used. Example Solving the Cauchy problem for the heat</description>
    </item>
    <item>
      <title>Wave Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3092/</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3092/</guid>
      <description>Definition1 The following partial differential equation is called the wave equation.wave equation. $$ u_{tt} - \Delta u =0 $$ This equation assumes the propagation speed of the wave as a constant $1$. If the propagation speed of the wave is denoted as $c$, then the wave equation becomes, $$ u_{tt} - c^{2}\Delta u =0 $$ In the case of being nonhomogeneousnonhomogeneous, $$ u_{tt} - \Delta u = f $$ $U</description>
    </item>
    <item>
      <title>Dirichlet Boundary Conditions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3087/</link>
      <pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3087/</guid>
      <description>Definition1 Let us assume that a partial differential equation is given on an open set $\Omega$. The following boundary conditions are referred to as Dirichlet boundary conditions. The problem of finding solutions to partial differential equations with Dirichlet boundary conditions is called the Dirichlet problem. $$ u = 0 \quad \text{on } \partial \Omega $$ Explanation Nonhomogeneous Conditions The following boundary conditions are referred to as nonhomogeneous Dirichlet conditions, although,</description>
    </item>
    <item>
      <title>Absolute Value Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/3083/</link>
      <pubDate>Fri, 16 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3083/</guid>
      <description>Definition A function defined as $f$ is called the absolute value function, and its values are denoted as shown in $|x|$. $$ |x| := f(x) = \begin{cases} x &amp;amp;\text{if } x&amp;gt;0 \\ 0 &amp;amp;\text{if } x=0 \\ -x &amp;amp;\text{if } x&amp;lt;0 \end{cases},\quad x\in \mathbb{R} $$ Explanation Absolute value refers to the magnitude of a real number, and a generalization of this is the norm. The triangle inequality holds. $$ |x</description>
    </item>
    <item>
      <title>Partial Derivatives: Derivatives of Multivariable Vector Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3082/</link>
      <pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3082/</guid>
      <description>Buildup[^1] Recall the definition of the derivative of a univariate function. $$ \lim \limits_{h\to 0} \dfrac{f(x+h) - f(x)}{h} = f^{\prime}(x) $$ By approximating the numerator on the left-hand side as a linear function of $h$, we get the following. $$ \begin{equation} f(x+h) - f(x) = a h + r(h) \label{1} \end{equation} $$ Let&amp;rsquo;s call $r(h)$ the remainder, satisfying the condition below. $$ \lim \limits_{h \to 0} \dfrac{r(h)}{h}=0 $$ Then, dividing</description>
    </item>
    <item>
      <title>Laplacian of a Scalar Field</title>
      <link>https://freshrimpsushi.github.io/en/posts/3081/</link>
      <pubDate>Mon, 12 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3081/</guid>
      <description>Definition The divergence of the gradient of the scalar function $u : \mathbb{R}^{n} \to \mathbb{R}$ is called the Laplacian and is denoted as follows. $$ \begin{align*} \Delta u :&amp;amp;= \mathrm{div}(\nabla (u)) \\ &amp;amp;= \mathrm{div} \left( \left( u_{x_{1}}, u_{x_{2}}, \dots, u_{x_{n}} \right) \right) \\ &amp;amp;= u_{x_{1}x_{1}} + u_{x_{2}x_{2}} + \cdots + u_{x_{n}x_{n}} \\ &amp;amp;= \sum _{i=1}^{n} u_{x_{i}x_{i}} \end{align*} $$ Here, $u_{x_{i}}=\dfrac{\partial u}{\partial x_{i}}$ is. Explanation In mathematics, the divergence is often</description>
    </item>
    <item>
      <title>Matrix Representation of Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3078/</link>
      <pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3078/</guid>
      <description>Definition1 Let us consider $V, W$ as a finite-dimensional vector space. Let&amp;rsquo;s say $\beta = \left\{ \mathbf{v}_{1}, \dots, \mathbf{v}_{n} \right\}$ and $\gamma = \left\{ \mathbf{w}_{1}, \dots, \mathbf{w}_{m} \right\}$ are the ordered bases for $V$ and $W$, respectively. Assume $T : V \to W$ to be a linear transformation. Due to the uniqueness of basis representation, there exist unique scalars $a_{ij}$ that satisfy the following. $$ T(\mathbf{v}_{j}) = \sum_{i=1}^{m}a_{ij}\mathbf{w}_{i} = a_{1j}\mathbf{w}_{1}</description>
    </item>
    <item>
      <title>Back Propagation Algorithm</title>
      <link>https://freshrimpsushi.github.io/en/posts/3077/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3077/</guid>
      <description>This article is written for math majors to understand the principles of the backpropagation algorithm. Notation Given an artificial neural network like the one shown above. Let $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n_{0}})$ be the inputinput, $y_{j}^{l}$ be the $j$th node of the $l$th layer, $\hat{\mathbf{y}} = (\hat{y}_{1}, \hat{y}_{2}, \dots, \hat{y}_{\hat{n}})$ is the output output. Let $L \in \mathbb{N}$ be the number of hidden layers, and the components of $\mathbf{n}=(n_{0},</description>
    </item>
    <item>
      <title>Composition of Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3074/</link>
      <pubDate>Sat, 26 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3074/</guid>
      <description>Definition1 Given linear transformations $T_{1} : V \to W$ and $T_{2} : W \to Z$, the transformation defined by $T_{2} T_{1}$ is called the composition of $T_{1}$ and $T_{2}$. $$ (T_{2} \circ T_{1})(\mathbf{x}) = T_{2}\left( T_{1}(\mathbf{x}) \right) \quad \mathbf{x} \in V $$ Explanation The composition of linear transformations is often denoted simply as follows: $$ T_{2}T_{1}\mathbf{x} = (T_{2} \circ T_{1}) (\mathbf{x}) $$ In finite dimensions, this is essentially the same</description>
    </item>
    <item>
      <title>Rank, Nullity, and Dimension Theorems of Linear Transformations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3072/</link>
      <pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3072/</guid>
      <description>Definition1 Let $T : V \to W$ be a linear transformation. If the range $R(T)$ of $T$ is finite-dimensional, the dimension of $R(T)$ is called the rank of $T$, denoted by: $$ \mathrm{rank}(T) := \dim (R(T)) $$ If the null space $N(T)$ of $T$ is finite-dimensional, the dimension of $N(T)$ is called the nullity of $T$, denoted by: $$ \mathrm{nullity}(T) := \dim\left( N(T) \right) $$ Explanation This is a generalization</description>
    </item>
    <item>
      <title>Linear Transformation: Kernel and Range</title>
      <link>https://freshrimpsushi.github.io/en/posts/3071/</link>
      <pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3071/</guid>
      <description>Definition1 Let&amp;rsquo;s say $T : V \to W$ is a linear transformation. The set of elements of $V$ that are mapped to $\mathbf{0}$ by $T$ is called the kernel or null space, and is denoted as follows. $$ \text{ker}(T) = N(T) := \left\{ \mathbf{v} \in V : T( \mathbf{v} ) = \mathbf{0} \right\} $$ The set of images under $\mathbf{v} \in V$ by $T$ is called the range or image</description>
    </item>
    <item>
      <title>Weighted Lp Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1856/</link>
      <pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1856/</guid>
      <description>Definition1 A function space defined as follows is called a weighted $L^{p}$ space or specifically a $w$-weighted $L^{p}$ space. $$ L_{w}^{p}(a,b):= \left\{ f : \mathbb{R}\to \mathbb{C}\ \big|\ \int_{a}^{b} \left| f(x) \right|^{p}w(x)dx &amp;lt;\infty \right\} $$ Here, $w:\mathbb{R}\to[0,\infty)$ is called a weight function. Description It is one of the spaces that generalizes the $L^{p}$ space. When it is $w(x)=1$, $L_{w}^{p}=L^{p}$ holds. The norm of the weighted $L^{p}$ space is defined as follows</description>
    </item>
    <item>
      <title>Composition of Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/3048/</link>
      <pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3048/</guid>
      <description>Definitions A function $f: X \to Y$, $g: f(X) \to Z$ is defined as follows: the composition of $g$ with $f$ is called $h: X \to Z$, and it is denoted by $h=g \circ f$. $$ h(x) = (g\circ f) (x) := g\left( f(x) \right) $$</description>
    </item>
    <item>
      <title>Linear Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/3026/</link>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3026/</guid>
      <description>Definition1 A transformation is when a function $T : V \to W$ maps from one vector space to another, that is $V$, $W$ are both vector spaces, we call $T$ a transformation. If the transformation $T$ is a linear function, satisfying the following two conditions for any $\mathbf{v},\mathbf{u} \in V$ and scalar $k$, it is called a linear transformation: $T(k \mathbf{u}) = k T(\mathbf{u})$ $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) +</description>
    </item>
    <item>
      <title>Simultaneous Homogeneous Linear Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3020/</link>
      <pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3020/</guid>
      <description>Definition1 In a linear system, if the constant terms are all $0$, it is called homogeneous. $$ \begin{align*} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= 0 \\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= 0 \\ &amp;amp;\vdots \\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= 0 \end{align*} $$ Unlike general linear systems, every homogeneous linear system always has a solution because if the constant terms are $0$,</description>
    </item>
    <item>
      <title>Gaussian-Jordan Elimination</title>
      <link>https://freshrimpsushi.github.io/en/posts/3019/</link>
      <pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3019/</guid>
      <description>Definition1 An augmented matrix is said to be in echelon form if it satisfies the following conditions: In rows that have a non-zero element, the first non-zero number is a 1, referred to as the leading 1. Rows where all elements are zero are placed at the bottom. For consecutive rows that contain non-zero elements, the leading 1 in the upper row must be to the left of the leading</description>
    </item>
    <item>
      <title>Basis of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3017/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3017/</guid>
      <description>Definition1 Let $S = \left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \dots, \mathbf{v}_{r} \right\}$ be a subset of vector space $V$. If $S$ satisfies the following two conditions, then $S$ is called a basis of $V$. $S$ spans $V$. $$ V = \text{span}(S) $$ $S$ is linearly independent. Explanation As the name suggests, the concept of a basis corresponds to &amp;rsquo;the smallest thing that can create a vector space&amp;rsquo;. The condition of spanning has</description>
    </item>
    <item>
      <title>Various Function Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/3032/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3032/</guid>
      <description>Definition A set of functions $X$ is called a function space if it forms a vector space. Explanation In the function space $X$, the inner product is defined by integration as follows. $$ \langle f, g \rangle = \int f(x) g(x) dx,\quad f,g\in X $$ The main function spaces considered include the following. Space of continuous functions $C^{m}$ $$ C^{m}(\mathbb{R}) : =\left\{ f \in C(\mathbb{R}) : f^{(n)} \text{ is continuous</description>
    </item>
    <item>
      <title>Eigenvalues and Eigenvectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/319/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/319/</guid>
      <description>Definition1 Given a matrix $n\times n$ $A$, for a non-zero column vector $\mathbf{0}$ $n\times 1$ and a constant $\mathbf{x}$, the following equation is referred to as the eigenvalue equation or the eigenvalue problem. $$ \begin{equation} A \mathbf{x} = \lambda \mathbf{x} \end{equation} $$ For a given $A$, a $\mathbf{x}$ that satisfies the eigenvalue equation above is called the eigenvalue of $A$, and $n\times 1$ is called the eigenvector corresponding to the</description>
    </item>
    <item>
      <title>Partial Derivatives</title>
      <link>https://freshrimpsushi.github.io/en/posts/3036/</link>
      <pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3036/</guid>
      <description>Definitions1 Let us define $E\subset \mathbb{R}^{n}$ as an open set, and $\mathbf{x}\in E$, and $\mathbf{f} : E \to \mathbb{R}^{m}$. Let $\left\{ \mathbf{e}_{1}, \mathbf{e}_{2}, \dots, \mathbf{e}_{n} \right\}$, and $\left\{ \mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{m} \right\}$ be the standard basis of $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$, respectively. Then, the components $f_{i} : \mathbb{R}^{n} \to \mathbb{R}$ of $\mathbf{f}$ are defined as follows. $$ \mathbf{f} (\mathbf{x}) = \sum_{i=1}^{m} f_{i}(\mathbf{x})\mathbf{u}_{i}, \quad \mathbf{x} \in E $$ or $$ f_{i}</description>
    </item>
    <item>
      <title>Properties of Determinants</title>
      <link>https://freshrimpsushi.github.io/en/posts/3015/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3015/</guid>
      <description>Properties Let $A,B$ be a $n\times n$ matrix and $k$ be a constant. The determinant satisfies the following properties: (a) $\det(kA) = k^{n}\det(A)$ (b) $\det(AB) = \det(A)\det(B)$ (c) $\det(AB)=\det(BA)$ (d) If $A$ is an invertible matrix, then $\det(A^{-1}) = \dfrac{1}{\det(A)}$ (e) $\det(A^{T}) = \det(A)$. Here, $A^{T}$ is the transpose of $A$.</description>
    </item>
    <item>
      <title>Classification of Surfaces of Revolution According to Gaussian Curvature</title>
      <link>https://freshrimpsushi.github.io/en/posts/3034/</link>
      <pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3034/</guid>
      <description>Overview1 Rotational surfaces are classified into three types according to the sign of the Gaussian curvature. Within each classification, surfaces with the same curvature share the same local intrinsic characteristics, even though they might have different global, extrinsic properties. In other words, they are locally isometric. Richard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p153-154&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    <item>
      <title>Determinants</title>
      <link>https://freshrimpsushi.github.io/en/posts/252/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/252/</guid>
      <description>Definitions Let&amp;rsquo;s denote $A$ as the following $2 \times 2$ matrix. $$ A = \begin{bmatrix} a &amp;amp; b \\ c &amp;amp; d \end{bmatrix} $$ The determinant of $A$ is defined as follows and is denoted by $\det(A)$. $$ \det(A) := ad - bc $$ Explanation To talk about determinants, we cannot skip discussing the very purpose of linear algebra. It wouldn&amp;rsquo;t be an exaggeration to say that most problems in</description>
    </item>
    <item>
      <title>Augmented Matrices and Elementary Row Operations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3014/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3014/</guid>
      <description>Definition1 Let&amp;rsquo;s assume a linear system is given as follows. $$ \begin{equation} \begin{aligned} a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1n}x_{n} &amp;amp;= b_{1}\\ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} &amp;amp;= b_{2}\\ &amp;amp;\vdots\\ a_{m1}x_{1} + a_{m2}x_{2} + \cdots + a_{mn}x_{n} &amp;amp;= b_{m} \end{aligned} \label{linsys2} \end{equation} $$ The representation of constants of a linear system in a matrix is called an augmented matrix. $$ \begin{equation} \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp;</description>
    </item>
    <item>
      <title>Simultaneous Linear Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/3013/</link>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3013/</guid>
      <description>Definition1 For constants $a_{1}$, $a_{2}$, $\dots$, $a_{n}$, $b$, we define a linear equation for variables $x_{1}$, $x_{2}$, $\dots$, $x_{n}$ as follows. $$ \begin{equation} a_{1}x_{1} + a_{2}x_{2} + \cdots + a_{n}x_{n} = b \label{lineq} \end{equation} $$ At least one of $a$ is not $0$. In other words, not &amp;ldquo;all $a$ are $0$&amp;rdquo;. A finite set of linear equations is called a system of linear equations or simply a linear system, and</description>
    </item>
    <item>
      <title>Linear Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/3037/</link>
      <pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3037/</guid>
      <description>Definition A function $f : X \to Y$ is called linear if it satisfies the following two conditions for $x,x_{1},x_{2}\in X$ and $a \in \mathbb{R}$, $f(ax) = af(x)$ $f(x_{1} + x_{2}) = f(x_{1}) + f(x_{2})$ Explanation If it is not linear, it is called nonlinear. The two conditions are sometimes combined as follows $$ f(ax_{1} + x_{2}) = af(x_{1}) + f(x_{2}) $$ If in 2., instead of being equal, it</description>
    </item>
    <item>
      <title>Cheat Sheet : Equivalent Codes in Julia, Matlab, Python, R</title>
      <link>https://freshrimpsushi.github.io/en/posts/3031/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3031/</guid>
      <description>개요 In this post, we&amp;rsquo;ll organize code that does the same thing in Julia, Matlab, Python, and R. Cheat Sheet: Flux-PyTorch-TensorFlow Let&amp;rsquo;s say we have the following environment for Python. import numpy as np Common Julia Matlab Python R comment #comment %comment #comment #comment 2d grid X = kron(x, ones(size(y)))Y = kron(ones(size(x)), y) [X,Y] = meshgrid(x,y) np.meshgrid(x,y) How to create an n-dimensional meshgrid in Julia Type Julia Matlab Python</description>
    </item>
    <item>
      <title>Unitary Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3008/</link>
      <pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3008/</guid>
      <description>Definition Unitary Matrix Let $A$ be a square complex matrix. $A$ is called a unitary matrix if it satisfies the following equation: $$ A^{-1}=A^{\ast} $$ Here, $A^{-1}$ is the inverse of $A$, $A^{\ast}$ is the conjugate transpose of $A$. Unitary Diagonalization1 Consider a square matrix $A$ of size $n \times n$. $A$ is said to be unitarily diagonalizable if it satisfies the following equation for some diagonal matrix $D$ and</description>
    </item>
    <item>
      <title>Hermitian Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3007/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3007/</guid>
      <description>Definition Let $A$ be a square complex matrix. If $A$ satisfies the following equation, it is called a Hermitian matrix or self-adjoint matrix. $$ A^{\ast}=A $$ Here, $A^{\ast}$ is the conjugate transpose of $A$. If $A$ satisfies the following equation, it is called a skew-Hermitian matrix . $$ A^{\ast}=-A $$ Explanation If it is a real matrix, since $A^{\ast}=A^{T}$, if it is a symmetric matrix, it is a Hermitian matrix.</description>
    </item>
    <item>
      <title>Trace</title>
      <link>https://freshrimpsushi.github.io/en/posts/1924/</link>
      <pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1924/</guid>
      <description>Definition Let matrix $n\times n$ be given as follows. $$ A= \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{n1} &amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{bmatrix} $$ The sum of the diagonal elements of $A$ is defined as the trace of $A$ and is denoted as follows. $$ \text{tr}(A)=\text{Tr}(A)=a_{11}+a_{22}+\cdots + a_{nn}=\sum \limits_{i=1}^{n}</description>
    </item>
    <item>
      <title>What is Reinforcement Learning in Machine Learning</title>
      <link>https://freshrimpsushi.github.io/en/posts/3029/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3029/</guid>
      <description>Definition Reinforcement learning is the process where an agent interacts with the environment to find a policy that maximizes the cumulative reward. Description1 The elements comprising reinforcement learning are as follows: Agentagent: Decides actions based on a policy, given a state. Statestate: Refers to the situation in which the agent is placed. Actionaction: Refers to the choices available to the agent in a given state. Policypolicy: Refers to the strategy</description>
    </item>
    <item>
      <title>Orthogonal Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3009/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3009/</guid>
      <description>Definition Let $A$ be a square real matrix. $A$ is called an orthogonal matrix if it satisfies the following equation: $$ A^{-1} = A^{T} $$ Another way to express this condition is as follows: $$ AA^{T} = A^{T}A =I $$ Explanation To put the definition in words, an orthogonal matrix is a matrix whose row vectors or column vectors are orthogonal unit vectors to each other. When extended to complex</description>
    </item>
    <item>
      <title>Matrix Inner Product</title>
      <link>https://freshrimpsushi.github.io/en/posts/3011/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3011/</guid>
      <description>Definition: Inner Product of Two Column Vectors1 The inner product of two column vectors of size $n \times 1$, $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{R}^{n}$ is defined as follows. $$ \begin{equation} \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{T}\mathbf{v}=u_{1}v_{1} + u_{2}v_{2} + \cdots + u_{n}v_{n} \label{EuclideanIP} \end{equation} $$ In the case where $\mathbf{u}$, $\mathbf{v}$ $\in \mathbb{C}^{n}$, it is as follows. $$ \mathbf{u} \cdot \mathbf{v} := \mathbf{u}^{\ast}\mathbf{v}=u^{\ast}_{1}v_{1}^{\ } + u_{2}^{\ast}v_{2}^{\ } + \cdots + u_{n}^{\ast}v_{n}^{\ }</description>
    </item>
    <item>
      <title>Conjugate Transpose Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3006/</link>
      <pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3006/</guid>
      <description>Definition Let $A$ be a complex matrix of size $m \times n $. Define $\overline{A}$ as follows, and call it the conjugate matrix of $A$. $$ \overline{A} :=\begin{bmatrix} \overline{a_{11}} &amp;amp; \overline{a_{12}} &amp;amp; \cdots &amp;amp; \overline{a_{1n}} \\ \overline{a_{21}} &amp;amp; \overline{a_{22}} &amp;amp; \cdots &amp;amp; \overline{a_{2n}} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \overline{a_{m1}} &amp;amp; \overline{a_{m2}} &amp;amp; \cdots &amp;amp; \overline{a_{mn}} \end{bmatrix} = \left[ \overline{a_{ij}} \right] $$ Here, $\overline{a}$ is the conjugate</description>
    </item>
    <item>
      <title>Symmetric Matrices, Skew-Symmetric Matrices</title>
      <link>https://freshrimpsushi.github.io/en/posts/3005/</link>
      <pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3005/</guid>
      <description>Definition1 A square matrix $A$ is called a symmetric matrix if it satisfies the following equation: $$ A=A^{T} $$ Here, $A^{T}$ is the transpose of $A$. $A$ is called an anti-symmetric matrix if it satisfies the following equation: $$ A =-A^{T} $$ Explanation By the definition of the transpose, matrices that are not square cannot be symmetric or anti-symmetric. If $A$ is an anti-symmetric matrix, it follows from the definition</description>
    </item>
    <item>
      <title>Conditions for a Matrix Being Invertible</title>
      <link>https://freshrimpsushi.github.io/en/posts/3004/</link>
      <pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3004/</guid>
      <description>Theorem1 Let $A$ be a square matrix of size $n\times n$. Then the following propositions are all equivalent. (a) $A$ is an invertible matrix. (b) The homogeneous linear system $A\mathbf{x}=\mathbf{0}$ has only the trivial solution. (c) The reduced row echelon form of $A$ is $I_{n}$. (d) $A$ can be expressed as a product of elementary matrices. (e) $A\mathbf{x}=\mathbf{b}$ has a solution for all $n\times 1$ matrices $\mathbf{b}$. (f) $A\mathbf{x}=\mathbf{b}$ has</description>
    </item>
    <item>
      <title>Inverse Matrix, Reversible Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3003/</link>
      <pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3003/</guid>
      <description>Definition Let $A$ be an arbitrary square matrix of size $n\times n$. A matrix $L$ is called the left inverse matrix of $A$ if it satisfies the following equation with $A$ in a matrix multiplication. $$ LA=I_{n} $$ Here, $I_{n}$ is the identity matrix of size $n\times n$. A matrix $R$ that is capable of matrix multiplication with $A$ and satisfies the following equation is called the right inverse matrix</description>
    </item>
    <item>
      <title>Transpose Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3002/</link>
      <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3002/</guid>
      <description>Definition1 Let&amp;rsquo;s consider a matrix of size $m\times n$ as $A$. The matrix obtained by swapping the rows and columns of $A$ is called the transpose of $A$ and is denoted by $A^{T}$ or $A^{t}$. Description Following the definition, if $A$ is a $m \times n$ matrix then $A^{T}$ will be a $n \times m$ matrix. Also, the $i$th row of $A$ is the same as the $i$th column of</description>
    </item>
    <item>
      <title>Identity Matrix, Unit Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/3001/</link>
      <pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3001/</guid>
      <description>Definition A diagonal matrix of size $n\times n$ with all diagonal elements being $1$ is called an identity matrix or unit matrix, denoted as $I_{n}$ or $I_{n\times n}$. $$ I_{n\times n}= \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 \end{bmatrix} $$ Description The identity matrix is</description>
    </item>
    <item>
      <title>Diagonal Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/1958/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1958/</guid>
      <description>Diagonal Matrix1 Let&amp;rsquo;s consider a matrix $A$ of size $n\times m$. The elements whose row and column numbers are the same, that is, $a_{ii} (1 \le i \le \min(n,m))$, are called the main diagonal elements. The imaginary line connecting the main diagonal elements is referred to as the main diagonal, or principal diagonal. A matrix $A$, in which all elements except for the main diagonal elements are $0$, is called</description>
    </item>
    <item>
      <title>Square Matrix</title>
      <link>https://freshrimpsushi.github.io/en/posts/1956/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1956/</guid>
      <description>Definition A matrix $A$ is called a square matrix if the number of rows and columns of the matrix $A$ are equal. Explanation It is also referred to as a regular square matrix. Square matrices are easy to handle and possess various favourable properties. Examples Identity matrix Invertible matrix Elementary matrix Symmetric matrix Orthogonal matrix Hermitian matrix Unitary matrix</description>
    </item>
    <item>
      <title>Matrix Operations: Scalar Multiplication, Addition, and Multiplication</title>
      <link>https://freshrimpsushi.github.io/en/posts/1957/</link>
      <pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1957/</guid>
      <description>Scalar Multiplication The multiplication of an arbitrary matrix $A$ of size $m \times n$ by a scalar $k$ is defined as multiplying each element of $A$ by $k$ and is denoted as follows: $$ kA = k\begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{bmatrix} :=</description>
    </item>
    <item>
      <title>How to Print and Save a 2D Array as a Heatmap Image in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1948/</link>
      <pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1948/</guid>
      <description>Imagesc imagesc function allows you to display a 2D array as a heatmap. colorbar is a setting that outputs a color bar indicating the scale. N=2^8; p=phantom(&amp;#39;Modified Shepp-Logan&amp;#39;,N); figure() imagesc(p) colorbar Saving Method 1 You can use the saveas function to save the figure displayed above. The setting gcf refers to the current figure. Then, the picture below is saved. N=2^8; p=phantom(&amp;#39;Modified Shepp-Logan&amp;#39;,N); figure() imagesc(p) colorbar saveas(gcf,&amp;#39;phantom.png&amp;#39;) Method 2 You</description>
    </item>
    <item>
      <title>Matrix Definitions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1955/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1955/</guid>
      <description>Definition1 A matrix is an arrangement of numbers in the shape of a rectangle as follows: $$ A=\begin{bmatrix} 10 &amp;amp; 0 &amp;amp; 3 \\ 0 &amp;amp; 8 &amp;amp; 22 \end{bmatrix} $$ Each of the arranged numbers is called an entry or element. A horizontal line is called a row, and a vertical line is called a column. Moreover, if a certain matrix has $m$ rows and $n$ columns, its size</description>
    </item>
    <item>
      <title>Product Rule Involving the Del Operator</title>
      <link>https://freshrimpsushi.github.io/en/posts/93/</link>
      <pubDate>Thu, 07 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/93/</guid>
      <description>Formulas Let&amp;rsquo;s call $f=f(x,y,z)$ a scalar function. Let&amp;rsquo;s call $\mathbf{A} = A_{x}\hat{\mathbf{x}} + A_{y}\hat{\mathbf{y}} + A_{z}\hat{\mathbf{z}}, \mathbf{B} = B_{x}\hat{\mathbf{x}} + B_{y}\hat{\mathbf{y}} + B_{z}\hat{\mathbf{z}}$ a vector function. Then, the following equations hold. Gradient (a) $\nabla{(fg)}=f\nabla{g}+g\nabla{f}$ (b) $\nabla(\mathbf{A} \cdot \mathbf{B}) = \mathbf{A} \times (\nabla \times \mathbf{B}) + \mathbf{B} \times (\nabla \times \mathbf{A})+(\mathbf{A} \cdot \nabla)\mathbf{B}+(\mathbf{B} \cdot \nabla) \mathbf{A}$ Divergence (c) $\nabla \cdot (f\mathbf{A}) = f(\nabla \cdot \mathbf{A}) + \mathbf{A} \cdot (\nabla f)$ (d)</description>
    </item>
    <item>
      <title>Radon Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1945/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1945/</guid>
      <description>Definition Let&amp;rsquo;s assume that a function $f :D \to \mathbb{R}$ is defined on some 2D domain $D\subset \mathbb{R}^{2}$. The Radon transform $\mathcal{R}f$ of $f$ is defined as follows, for $s \in \mathbb{R}$, $\boldsymbol{\theta} = (\cos \theta, \sin \theta) \in S^{1}$, $$ \begin{align*} \mathcal{R} f(s, \boldsymbol{\theta}):=&amp;amp;\ \int \limits_{t=-\infty}^{\infty} f ( s \boldsymbol{\theta} + t \boldsymbol{\theta}^{\perp} )dt \\ =&amp;amp;\ \int \limits_{t=-\infty} ^{\infty} f \left( s\cos\theta-t\sin\theta, s\sin\theta + t\cos\theta \right)dt \end{align*} $$ Explanation</description>
    </item>
    <item>
      <title>Solution of Differential Equations Using Series Solutions</title>
      <link>https://freshrimpsushi.github.io/en/posts/888/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/888/</guid>
      <description>Description Differential equations with constant coefficients can be relatively easily solved using methods such as separation of variables or integrating factor method. However, differential equations with coefficients that include the independent variable, as shown below, cannot be easily solved. $$ \begin{equation} P(x)\dfrac{d^2 y}{dx^2} + Q(x)\dfrac{dy}{dx}+R(x)y=0 \label{1}\end{equation} $$ Here, $P$, $Q$, and $R$ are assumed to be polynomials without common factors. Equations of the above form include Bessel&amp;rsquo;s equation $$ x^2</description>
    </item>
    <item>
      <title>Convex Sets in Vector Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1914/</link>
      <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1914/</guid>
      <description>Definition A subset $M$ of a vector space $V$ is called a convex set if the following equation holds: $$ \lambda x +(1-\lambda)y \in M,\quad \forall \lambda\in[0,1],\ \forall x,y \in M $$ Description Verbally, this equation means &amp;quot;$M$ is a convex set implies that every vector lying between any two vectors in $M$ also belongs to $M$&amp;quot;. Also, if $M$ is a subspace, it is closed under addition and scalar</description>
    </item>
    <item>
      <title>Orthogonality, Orthogonal Sets, and Orthonormal Sets in Inner Product Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1912/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1912/</guid>
      <description>Definition1 Let $\left( X, \left\langle \cdot, \cdot \right\rangle \right)$ be an inner product space. If two elements $\mathbf{x}, \mathbf{y}\in X$ satisfy $\left\langle \mathbf{x}, \mathbf{y} \right\rangle =0$, then $\mathbf{y}$ and $\mathbf{x}$ are said to be orthogonal and denoted as follows. $$ \mathbf{x} \perp \mathbf{y} $$ If the set of elements $X$, $\left\{ \mathbf{x}_{k} \right\}_{k\in \mathbb{N}}$, satisfies the following equation, it is called an orthogonal system or an orthogonal set. $$ \left\langle</description>
    </item>
    <item>
      <title>Several Definitions and Notations of Fourier Transform</title>
      <link>https://freshrimpsushi.github.io/en/posts/1898/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1898/</guid>
      <description>Overview The definition and notation of the Fourier transform vary depending on the needs and preferences of the author. Therefore, before dealing with the Fourier transform in textbooks, lectures, research papers, etc., it is common to clarify the definition and notation. If you skip over the definition thinking it is a concept you know, you might find the equations odd, so it is necessary to check carefully. Of course, the</description>
    </item>
    <item>
      <title>Laplacian of a Scalar Function in the Three-Dimensional Cartesian Coordinate System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1879/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1879/</guid>
      <description>Definition The Laplacian of a 3D scalar function $f=f(x,y,z)$ is the divergence of its gradient $f$ and is denoted by $\nabla^{2}$. $$ \nabla ^{2} f := \nabla \cdot(\nabla f)= \frac{ \partial^{2} f}{ \partial x^{2} }+\frac{ \partial^{2} f}{ \partial y^{2}}+\frac{ \partial^{2} f}{ \partial z^{2}} $$ Explanation The name Laplacian comes from the French mathematician Laplace. The notation $\nabla^{2}$ is used for convenience. In mathematics (theory of partial differential equations), the notation</description>
    </item>
    <item>
      <title>Integration by Parts</title>
      <link>https://freshrimpsushi.github.io/en/posts/1867/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1867/</guid>
      <description>Theorem 1 Assuming $F$, $G$ are differentiable in the interval $[a,b]$, and $F^{\prime}=f$, $G^{\prime}=g$ are integrable. Then, the following equation holds: $$ \begin{align*} \int _{a} ^{b} F(x)g(x)dx &amp;amp;= F(b)G(b)-F(a)G(a)-\int _{a} ^{b}f(x)G(x)dx \\ &amp;amp;= \left[ F(x)G(x) \right]_{a}^{b} -\int _{a} ^{b}f(x)G(x)dx \end{align*} $$ Description This result is called the integration by parts. Memorizing it as Integration-Differential-Integration makes it easy. What to integrate is kept on both sides as is, and what to</description>
    </item>
    <item>
      <title>The Fundamental Theorem of Calculus in Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1866/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1866/</guid>
      <description>Theorem1 Given that function $f$ is Riemann integrable on the interval $[a,b]$, and there exists a function $F$ that is differentiable on $[a,b]$, satisfying $F^{\prime}=f$. Then, the following holds true. $$ \int_{a}^{b} f(x) dx= F(b)-F(a) $$ Explanation This theorem is famously known as the Fundamental Theorem of Calculus Part 2, often abbreviated as FTC2[^Funcamental Theorem of Calculus1]. It implies that the definite integral of $f$ is represented by the difference</description>
    </item>
    <item>
      <title>Convolution&#39;s General Definition</title>
      <link>https://freshrimpsushi.github.io/en/posts/1848/</link>
      <pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1848/</guid>
      <description>Definition Given the integral transform $J$ and two functions $f$, $g$, a function $f \ast g$ fulfilling the conditions below is defined as the convolution of $f$ and $g$ with respect to $J$. $$ J(f \ast g)=(Jf)(Jg) $$ Explanation According to the definition, the convolution, being the integral transform of a product, can be divided into the product of integral transforms. This means that two functions, which were bound in</description>
    </item>
    <item>
      <title>Integral Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1847/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1847/</guid>
      <description>Definition If a map $J$ from a function space to a function space is defined as the following integral, then $J$ is called an integral transform. $$ (Jf) (x) = \int_{a}^{b} K(x,t)f(t)dt $$ $$ J : f(\cdot) \mapsto \int_{a}^{b} K(\cdot,t)f(t)dt $$ In this case, $K$ is referred to as the kernel of $J$. If a map from $Jf$ to $f$ exists, it is denoted as $J^{-1}$ and called the inverse</description>
    </item>
    <item>
      <title>Inner Product Spaces and the Cauchy-Schwarz Inequality</title>
      <link>https://freshrimpsushi.github.io/en/posts/1843/</link>
      <pubDate>Mon, 26 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1843/</guid>
      <description>Theorem1 Let $(H, \langle \cdot ,\cdot \rangle)$ be an inner product space. Then, the following inequality holds, and it is called the Cauchy-Schwarz inequality. $$ \left| \langle x,y \rangle \right| \le \langle x,x \rangle^{1/2} \langle y,y \rangle ^{1/2},\quad \forall x,y \in H $$ Explanation Since a norm can be defined from the inner product, it can also be expressed as the following equation. $$ \left| \left\langle x, y \right\rangle \right|</description>
    </item>
    <item>
      <title>Inner product spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1842/</link>
      <pubDate>Sun, 25 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1842/</guid>
      <description>Definition1 Let&amp;rsquo;s consider $X$ as a vector space. For $\mathbf{x}, \mathbf{y}, \mathbf{z} \in X$ and $\alpha, \beta \in \mathbb{C}$(or $\mathbb{R}$), the following conditions satisfied by a function $$ \langle \cdot , \cdot \rangle : X \times X \to \mathbb{C} $$ are defined as the inner product, and $\left( X, \langle \cdot ,\cdot \rangle \right)$ is called an inner product space. Linearity: $$\langle \alpha \mathbf{x} + \beta \mathbf{y} ,\mathbf{z} \rangle =\alpha</description>
    </item>
    <item>
      <title>Relations among Inner Product Spaces, Normed Spaces, and Metric Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1840/</link>
      <pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1840/</guid>
      <description>Description Let&amp;rsquo;s say an inner space $\left( X, \langle\cdot, \cdot\rangle \right)$ is given. Then, one can naturally define a norm as follows from the inner product. $$ \begin{equation} \left\| x \right\| := \sqrt{ \langle x, x\rangle},\quad x\in X \end{equation} $$ Hence, if it is an inner space, then it&amp;rsquo;s a normed space. Subsequently, one can define a distance from the norm thus defined. $$ \begin{equation} d(x,y):=\left\| x -y \right\| =\sqrt{</description>
    </item>
    <item>
      <title>Limits from the Left and the Right Strictly Defined in Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1830/</link>
      <pubDate>Sat, 17 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1830/</guid>
      <description>Definition Let&amp;rsquo;s assume a function $f :X \to \mathbb{R}$ is given in a metric space $X$. If $f$ is not continuous at $x\in X$, $f$ is said to be discontinuous at $x$ or to have a discontinuity at $x$. $f: (a,b) \to \mathbb{R}$ is assumed. For any point $x$, let $a \le x &amp;lt;b$. Consider a sequence of points $(x,b)$ that converges to $x$ and call it $\left\{ t_{n} \right\}$.</description>
    </item>
    <item>
      <title>Monotonic Functions, Increasing Functions, Decreasing Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/848/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/848/</guid>
      <description>Definition Let&amp;rsquo;s assume the function $f:[a,b] \rightarrow \mathbb{R}$ is given. For $x_{1}$, $x_{2}$, $\in [a,b]$ $$ x_{1} \lt x_{2} \ \implies f(x_{1}) \le f(x_{2}) $$ If it satisfies, then $f$ is said to be monotonically increasing or $f$ is called a monotone increasing function. Conversely, $$ x_{1} \lt x_{2} \ \implies f(x_{1}) \ge f(x_{2}) $$ If it satisfies, then $f$ is said to be monotonically decreasing or $f$ is called</description>
    </item>
    <item>
      <title>The Chain Rule of Differentiation in Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1823/</link>
      <pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1823/</guid>
      <description>Theorem1 If $f :[a,b] \to \mathbb{R}$ is a continuous function and is differentiable at $x\in [a,b]$, and if $g : f([a,b])\to \mathbb{R}$ is differentiable at $f (x)\in f([a,b])$, and if we define $h : [a,b] \to \mathbb{R}$ as follows. $$ h(t)=g\left( f(t) \right)\quad (a\le t \le b) $$ Then, $h$ is differentiable at $x$ and its value is as follows. $$ h^{\prime}(x)=g^{\prime}(f(x))f^{\prime}(x) $$ Using the composite function symbol, it can</description>
    </item>
    <item>
      <title>Differentiable Function Properties</title>
      <link>https://freshrimpsushi.github.io/en/posts/1821/</link>
      <pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1821/</guid>
      <description>Theorem1 Let&amp;rsquo;s say $f, g : [a,b] \to \mathbb{R}$. If $f,g$ is differentiable at $x\in [a,b]$, then $f+g$, $fg$, and $f/g$ are also differentiable at $x$ and the following equation holds. $$ \begin{align} (f+g)^{\prime}(x) &amp;amp;=f^{\prime}(x)+g^{\prime}(x) \\ (fg)^{\prime}(x) &amp;amp;= f^{\prime}(x)g(x)+f(x)g^{\prime}(x) \\ \left( \frac{f}{g} \right)^{\prime}(x) &amp;amp;= \frac{f^{\prime}(x)g(x)-f(x)g^{\prime}(x)}{g^{2}(x)} \end{align} $$ However, $(3)$ holds when $g(x)\ne 0$. Description $(2)$ is commonly referred to as the product rule of differentiation. Proof $(1)$ By the definition</description>
    </item>
    <item>
      <title>Fourier Series in Complex Notation</title>
      <link>https://freshrimpsushi.github.io/en/posts/964/</link>
      <pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/964/</guid>
      <description>Formulas The complex Fourier series of a function $f$ defined over the interval $[-L,\ L)$ is given by: $$ f(t) = \sum \limits_{n=-\infty}^{\infty} c_{n} e^{i\frac{n\pi t}{L}} $$ Here, the complex Fourier coefficients are as follows: $$ c_{n} = \dfrac{1}{2L}\int_{-L}^{L}f(t)e^{-i\frac{n \pi t}{L} }dt $$ The Fourier coefficients satisfy the following equation: $$ \begin{align*} a_{0} &amp;amp; = 2 c_{0} \\ a_{n} &amp;amp;= c_{n}+c_{-n} \\ b_{n} &amp;amp;= i(c_{n}-c_{-n}) \\ c_{n} &amp;amp;= \frac{1}{2} (a_{n}-ib_{n})</description>
    </item>
    <item>
      <title>Partial Differential Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/1818/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1818/</guid>
      <description>Definitions1 Partial Differential Equations For a natural number $k \in \mathbb{N}$ and an open set $U \subset \mathbb{R}^{n}$, the following expression is called a $k$-order partial differential equation. $$ \begin{equation} F(D^{k}u(x), D^{k-1}u(x),\cdots,Du(x),u(x),x)=0\quad (x\in U) \end{equation} $$ Here, $D^{k}u$ is the multi-index notation. $F$ is given as follows, and the unknown $u$ is as follows. $$ F : {\mathbb{R}}^{n^{k}}\times{\mathbb{R}}^{n^{k-1}}\times \cdots \times \mathbb{R}^{n}\times \mathbb{R}\times U \to \mathbb{R} \\ u : U \to</description>
    </item>
    <item>
      <title>Divergence of Vector Functions in Curvilinear Coordinates</title>
      <link>https://freshrimpsushi.github.io/en/posts/1817/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1817/</guid>
      <description>Summary The divergence of the vector function $\mathbf{F}=\mathbf{F}(q_{1},q_{2},q_{3})=F_{1}\hat{\mathbf{q}}_{1}+F_{2}\hat{\mathbf{q}}_{2}+F_{3}\hat{\mathbf{q}}_{3}$ in curvilinear coordinates is as follows. $$ \nabla \cdot \mathbf{F}=\frac{1}{h_{1}h_{2}h_{3}}\left[ \frac{ \partial }{ \partial q_{1} }(h_{2}h_{3}F_{1})+\frac{ \partial }{ \partial q_{2} }(h_{1}h_{3}F_{2})+\frac{ \partial }{ \partial q_{3} }(h_{1}h_{2}F_{3}) \right] $$ $h_{i}$ is the scale factor. Formulas Cartesian coordinates: $$ h_{1}=h_{2}=h_{3}=1 $$ $$ \begin{align*} \nabla \cdot \mathbf{F} =\frac{\partial F_{x}}{\partial x}+\frac{\partial F_{y}}{\partial y}+\frac{\partial F_{z}}{\partial z} \end{align*} $$ Cylindrical coordinates: $$ h_{1}=1,\quad h_{2}=\rho,\quad h_{3}=1 $$ $$ \begin{align*}</description>
    </item>
    <item>
      <title>Gradient of a Scalar Function in Curvilinear Coordinates</title>
      <link>https://freshrimpsushi.github.io/en/posts/1816/</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1816/</guid>
      <description>Theorem In a curvilinear coordinate system, the gradient of a scalar function $f=f(q_{1},q_{2},q_{3})$ is as follows. $$ \nabla f= \frac{1}{h_{1}}\frac{ \partial f }{ \partial q_{1} } \hat{\mathbf{q}}_{1} + \frac{1}{h_{2}}\frac{ \partial f }{ \partial q _{2}}\hat{\mathbf{q}}_{2}+\frac{1}{h_{3}}\frac{ \partial f }{ \partial q_{3} } \hat{\mathbf{q}}_{3}=\sum \limits _{i=1} ^{3}\frac{1}{h_{i}}\frac{ \partial f}{ \partial q_{i}}\hat{\mathbf{q}}_{i} $$ $h_{i}$ is the scale factor. Formulas Cartesian Coordinate System: $$ h_{1}=h_{2}=h_{3}=1 $$ $$ \nabla f= \frac{\partial f}{\partial x}\mathbf{\hat{\mathbf{x}} }+ \frac{\partial</description>
    </item>
    <item>
      <title>Spline, B-Spline in Analysis</title>
      <link>https://freshrimpsushi.github.io/en/posts/1815/</link>
      <pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1815/</guid>
      <description>Definition1 If the function $f:\mathbb{R} \to \mathbb{R}$ is a piecewise polynomial on interval $\mathbb{R}$, it is called a spline on $\mathbb{R}$. The points where the polynomial changes are called knots. Explanation As can be seen from the definition, a spline does not have to be a continuous function. The following function $f$ is an example of a spline. $$ f(x) = \begin{cases} 0 &amp;amp; x\in[\infty,0] \\ 2x^{2}&amp;amp;x\in(0,1] \\ 2-x &amp;amp;</description>
    </item>
    <item>
      <title>What is Overfitting and Regularization in Machine Learning?</title>
      <link>https://freshrimpsushi.github.io/en/posts/1807/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1807/</guid>
      <description>Overfitting The phenomenon where the training loss decreases, but the test loss (or validation loss) does not decrease or rather increases is called overfitting. Explanation There is also a term called underfitting, which basically means the opposite, but frankly, it&amp;rsquo;s a meaningless term and not often used in practice. A crucial point in machine learning is that the function trained with the available data must also work well with new</description>
    </item>
    <item>
      <title>Schwartz Space and Schwartz Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1805/</link>
      <pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1805/</guid>
      <description>Definition The set of functions $\phi : \mathbb{R}^{n} \to \mathbb{C}$ that satisfy the following two conditions is called the Schwartz space, denoted by $\mathcal{S}(\mathbb{R}^{n})$. An element $\phi$ of the Schwartz space is called a Schwartz function. (a) $\phi \in $ $C^{\infty}$ (b) For any multi-index $\alpha$, $\beta$, the following holds: $\left| \mathbf{x}^{\beta}D^{\alpha}\phi (\mathbf{x}) \right| &amp;lt;\infty$. Here, for $\beta=(\beta_{1}, \beta_{2},\dots,\beta_{n})$, $$ \mathbf{x}^{\beta}=x_{1}^{\beta_{1}}x_{2}^{\beta_{2}}\dots x_{n}^{\beta_{n}} $$ (b) can be rewritten as: $$ \mathbf{x}^{</description>
    </item>
    <item>
      <title>Curvilinear Coordinates in Three-Dimensional Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/1774/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1774/</guid>
      <description>Buildup The most common way to express a position in three-dimensional space is the Cartesian coordinate system. Named after Descartes, who devised it, it is also widely known as the orthogonal coordinate system. However, in specific situations, it might be difficult to represent the position using the Cartesian coordinate system. For instance, let&amp;rsquo;s consider an object performing rotational motion on a two-dimensional plane. Then, it would be much simpler to</description>
    </item>
    <item>
      <title>Derivative Approximation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1085/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1085/</guid>
      <description>Buildup Let&amp;rsquo;s recall the idea of defining the differentiation of distributions. There exists a regular distribution $T_{u}$ for $u \in {L}_{\mathrm{loc}}^1(\Omega)$. If $u$ is differentiable, by applying the integration by parts, the following equation holds, and the derivative of $T_{u}$ is defined as $T_{u^{\prime}}$, which corresponds to the derivative of $u$, $u^{\prime}$. $$ \begin{align*} T_{u}^{\prime}(\phi) &amp;amp;:= T_{u^{\prime}}(\phi) \\ &amp;amp;= \int u^{\prime}(x)\phi (x)dx \\ &amp;amp;= \left[ u(x) \phi (x) \right]_{-\infty}^{\infty} -\int</description>
    </item>
    <item>
      <title>Convergence of Sequences in Normed Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1800/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1800/</guid>
      <description>Definition Let&amp;rsquo;s call $(X, \left\| \cdot \right\|)$ as a normed space. For a sequence $\left\{ x_{n} \right\}$ of $X$, $$ \lim \limits_{n \to \infty} \left\| x - x_{n} \right\| = 0,\quad x\in X $$ it is said to converge to $x$ if it satisfies the following condition, and it is represented as follows. $$ x_{n} \to x \text { as } n \to \infty \quad \text{or} \quad x=\lim \limits_{n\to\infty}x_{n} $$</description>
    </item>
    <item>
      <title>Total Differentiation, Exact Differentiation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1773/</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1773/</guid>
      <description>Definition Let&amp;rsquo;s assume that a multivariable function $f : \mathbb{R}^{n} \to \mathbb{R}$ is given. The change of $f(\mathbf{x})$ according to the change of variable $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{n})$ is denoted as $df$, and this is called the total differential or exact differential of $f$. $$ \begin{equation} df = \frac{ \partial f}{ \partial x_{1} }dx_{1} + \frac{ \partial f}{ \partial x_{2} }dx_{2} + \cdots + \frac{ \partial f}{ \partial</description>
    </item>
    <item>
      <title>Differentiation of Distributions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1084/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1084/</guid>
      <description>Buildup The distribution cannot be differentiated in the same manner as functions defined over real numbers since its domain is a function space. However, for regular distributions, there is a corresponding locally integrable function $u\in L_{\mathrm{loc}}^{1}$, expressed as follows. $$ T_{u}(\phi) =\int u(x)\phi (x) dx,\quad \phi \in \mathcal{D} $$ Hence, the action $S$ on $u$ could yield $Su=u^{\prime}$; if $u^{\prime}$ remains a locally integrable function, then there exists a corresponding</description>
    </item>
    <item>
      <title>Divergence of Vector Function in Cartesian Cooridenates System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1796/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1796/</guid>
      <description>Definition For a vector function $\mathbf{F}(x,y,z)=F_{x}\hat{\mathbf{x}}+F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$, the following scalar function is defined as the divergence $\mathbf{F}$ of $\mathbf{F}(x,y,z)=F_{x}\hat{\mathbf{x}}+F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$ and is denoted by $\nabla \cdot \mathbf{F}$. $$ \begin{equation} \nabla \cdot \mathbf{F} := \frac{ \partial F_{x}}{ \partial x} + \frac{ \partial F_{y}}{ \partial y }+ \frac{ \partial F_{z}}{ \partial z} \label{divergence} \end{equation} $$ Explanation Geometrically, if $\nabla \cdot \mathbf{F}&amp;gt;0$, it means that $\mathbf{F}$ is spreading out or diverging.</description>
    </item>
    <item>
      <title>Characteristic Function, Indicator Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1790/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1790/</guid>
      <description>Definition For $A \subset X$, the function defined as $\chi_{A} : X \to \mathbb{R}$ is referred to as the characteristic function or the indicator function. $$ \chi _{A}(x) := \begin{cases} 1, &amp;amp; x\in A \\ 0 ,&amp;amp; x \notin A \end{cases} $$ Explanation $\chi$ is the Greek letter chi. The reason our math teacher used to say you should not write the letter x as $\chi$ but should instead use</description>
    </item>
    <item>
      <title>Convergence in the Space of Test Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1077/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1077/</guid>
      <description>In the test function space, &amp;lsquo;convergence&amp;rsquo; is defined in a special way. Normally, when a space $X$ is given, convergence is defined using the norm or distance defined in $X$. However, in the test function space, convergence is defined under stronger conditions to properly define and handle distributions. Definition Let $\Omega \subset \mathbb{R}^n$ be an open set, and $\left\{ \phi _{j} \right\}$ be a sequence of test functions. We say</description>
    </item>
    <item>
      <title>Distributions, Generalized Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1009/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1009/</guid>
      <description>Definition1 2 Let $\Omega \subset \mathbb{R}^{n}$ be an open set. A continuous linear functional $T : \mathcal{D}(\Omega) \to \mathbb{C}$ on the space of test functions is defined as a distribution. That is, a distribution is an element of the dual space of the test function space. Thus $$ T \in \mathcal{D}^{\ast} $$ and we call $D^{\ast}$ the (Schwartz) distribution space. Explanation The name distribution seems to be influenced by the</description>
    </item>
    <item>
      <title>Locally Integrable Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1783/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1783/</guid>
      <description>Definition Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. Definition 11 For every bounded measurable set $K \subset \Omega$, $$ \int_{K} \left| u(x) \right| dx \lt \infty $$ a function $u : \Omega \to \mathbb{C}$ satisfying this is said to be locally integrable with respect to (the Lebesgue measure). Definition 22 Let the function $u$ be defined almost everywhere on $\Omega$. For every open set $U \Subset \Omega$ when</description>
    </item>
    <item>
      <title>Proving that All Locally Integrable Functions Can Be Extended to Distributions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1078/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1078/</guid>
      <description>Theorem1 For every $u \in L_{\mathrm{loc} }^1(\Omega) $, there exists a distribution $T_{u} \in D^{\ast}(\Omega)$ defined as follows: $$ T_{u} (\phi) := \int_{\Omega} u(x)\phi (x)dx, \quad \phi \in D(\Omega) $$ Description $\mathcal{D}(\Omega)$ is the space of test functions. The distribution defined as above is called a regular distribution. Moreover, the above expression can be regarded as the inner product of $u$ and $\phi$ from the viewpoint of inner product spaces,</description>
    </item>
    <item>
      <title>Test Functions and Test Function Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/1782/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1782/</guid>
      <description>Definition1 Let an open set $\Omega \subset \mathbb{R}^{n}$ and a function $\phi : \Omega \to \mathbb{C}$ be given. If $\phi$ is infinitely differentiable, and all its derivatives are continuous and have a compact support, it is called a test function. The function space of test functions is denoted by $C_{c}^{\infty}(\Omega)$ or simply as $\mathcal{D}(\Omega)$. Explanation It is also called a test function or testing function. The reason $\phi$ is named</description>
    </item>
    <item>
      <title>Why Functional is Named Functional</title>
      <link>https://freshrimpsushi.github.io/en/posts/1780/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1780/</guid>
      <description>The term &amp;ldquo;functional analysis&amp;rdquo; is indeed intriguing, especially when considering the word &amp;ldquo;functional&amp;rdquo; instead of merely &amp;ldquo;function analysis.&amp;rdquo; At first glance, &amp;ldquo;functional&amp;rdquo; appears to be an adjective form of &amp;ldquo;function,&amp;rdquo; suggesting meanings like &amp;ldquo;function-like&amp;rdquo; or &amp;ldquo;pertaining to functions.&amp;rdquo; This notion can also be found in another name for functionals, &amp;ldquo;generalized functions.&amp;rdquo; The question arises as to why these are not simply called functions. To understand this, let&amp;rsquo;s look at the</description>
    </item>
    <item>
      <title>Gradient of Scalar Function in Cartesian Coordinate System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1778/</link>
      <pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1778/</guid>
      <description>Definition For a scalar function $f=f(x,y,z)$, the following vector function is defined as the gradient of $f$, denoted by $\nabla f$: $$ \nabla f := \frac{ \partial f}{ \partial x }\hat{\mathbf{x}}+\frac{ \partial f}{ \partial y}\hat{\mathbf{y}}+\frac{ \partial f}{ \partial z}\hat{\mathbf{z}} = \left( \dfrac{\partial f}{\partial x}, \dfrac{\partial f}{\partial y}, \dfrac{\partial f}{\partial z} \right) $$ Explanation The gradient is translated into English as gradient, slope, or incline. The terms &amp;lsquo;slope&amp;rsquo; and &amp;lsquo;incline&amp;rsquo; are</description>
    </item>
    <item>
      <title>How to Initialize the Workspace and Remove All Variables in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1758/</link>
      <pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1758/</guid>
      <description>Method clear Command Typing clear in the command window will reset the workspace. Clearing Workspace (Alt+T+O) Right-clicking on the workspace window allows you to select &amp;lsquo;Clear Workspace (O)&amp;rsquo;. Pressing it resets the workspace. This can also be done using the shortcut Alt+T+O, but it does not work when an editor is opened. Deleting by Selection You can delete by dragging to select everything or by pressing Ctrl+a to select all</description>
    </item>
    <item>
      <title>Curl of Vector Functions in 3D Cartesian Coordinates</title>
      <link>https://freshrimpsushi.github.io/en/posts/1752/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1752/</guid>
      <description>Definition For a vector function $\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\hat{\mathbf{x}} + F_{y}\hat{\mathbf{y}} + F_{z}\hat{\mathbf{z}}$, the following vector is defined as the curl of $\mathbf{F}$, denoted as $\nabla \times \mathbf{F}$. $$ \begin{align} \nabla \times \mathbf{F} &amp;amp;= \left( \dfrac{ \partial F_{z}}{ \partial y }-\dfrac{ \partial F_{y}}{ \partial z} \right)\hat{\mathbf{x}}+ \left( \dfrac{ \partial F_{x}}{ \partial z }-\dfrac{ \partial F_{z}}{ \partial x} \right)\hat{\mathbf{y}}+ \left( \dfrac{ \partial F_{y}}{ \partial x }-\dfrac{ \partial F_{x}}{ \partial y} \right)\hat{\mathbf{z}} \label{def1} \\ &amp;amp;=\begin{vmatrix}</description>
    </item>
    <item>
      <title>Damped Harmonic Oscillation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1736/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1736/</guid>
      <description>Damped Harmonic Oscillation1 When the spring constant is denoted as $k$, the equation of motion for a simple harmonic oscillator is as follows. $$ m \ddot {x}+kx=0 $$ The simple harmonic motion only considers the restoring force by the spring. However, in reality, other external forces such as frictional forces also affect the motion of the object, so they cannot be ignored. So, let&amp;rsquo;s assume there is a frictional force</description>
    </item>
    <item>
      <title>Continuity and Compactness in Metric Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1724/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1724/</guid>
      <description>Theorem Let $X$ be a compact metric space, $Y$ be a metric space, and $f:X\to Y$ be continuous. Then $f(X)$ is compact. The compactness condition cannot be omitted. Proof Let $\left\{ O_\alpha \right\}$ be an open cover of $f(X)$. Since $f$ is continuous, by the equivalence condition, each preimage $f^{-1}(O_{\alpha})$ is also an open set in $X$. Therefore, $\left\{ f^{-1}(O_{\alpha}) \right\}$ is an open cover of $X$, and since $X$</description>
    </item>
    <item>
      <title>Equivalent Conditions for a Function to Be Continuous in a Metric Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/1722/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1722/</guid>
      <description>Theorem 1 For two metric spaces $(X,d_{X})$ and $(Y,d_{Y})$, suppose that $E\subset X$ and $p \in E$, and $f : E \to Y$. Then, the following three propositions are equivalent. (1a) $f$ is continuous at $p$. (1b) $ \lim \limits_{x \to p} f(x)=f(p)$. (1c) For $\lim \limits_{n\to\infty} p_{n}=p$ that is $\left\{ p_{n} \right\}$, $\lim \limits_{n\to\infty} f(p_{n})=f(p)$. Proof (1a) $\iff$ (1b) This is trivial by the definition of limit and continuity.</description>
    </item>
    <item>
      <title>Maximum and Minimum Theorem in Metric Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1725/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1725/</guid>
      <description>Theorem Let $X$ be a compact metric space, and let $f : X \to \mathbb{R}$ be continuous. Then, it is as follows. $$ M = \sup \limits_{x\in X} f(x),\quad m=\inf \limits_{x \in X}f(x) $$ Then, $$ M=f(p),\quad m=f(q) $$ there exists a $q,p\in X$ that satisfies this. In other words: for every $x$, $$ f(q)\le f(x) \le f(p) $$ there exists a $q,p \in X$ that satisfies this. This is</description>
    </item>
    <item>
      <title>Proof that Continuous Functions on Compact Metric Spaces are Uniformly Continuous</title>
      <link>https://freshrimpsushi.github.io/en/posts/1727/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1727/</guid>
      <description>Theorem Let $(X,d_{X})$ be a compact metric space, $(Y,d_{Y})$ a metric space, and $f:X\to Y$ continuous. Then, $f$ is uniformly continuous on $X$. Explanation The condition of being compact cannot be omitted. Proof Suppose we are given any positive number $\varepsilon &amp;gt;0$. Since $f$ is assumed to be continuous, by definition, for each point $p\in X$, there exists a positive number $\delta_{p}$ satisfying the following equation: $$ \forall q\in X,\quad</description>
    </item>
    <item>
      <title>Convergence of Cauchy Sequences in Metric Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1718/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1718/</guid>
      <description>Definition Let $\left\{ p_{n} \right\}$ be a sequence of points in a metric space $(X,d)$. If for every positive number $\varepsilon$, there exists a positive number $N$ such that $$ n\ge N,\ m\ge N \implies d(p_{n},p_{m})&amp;lt;\varepsilon $$ is satisfied, then $\left\{ p_{n} \right\}$ is called a Cauchy sequence. If every Cauchy sequence in a metric space $X$ converges to a point in $X$, then $X$ is called a complete space.</description>
    </item>
    <item>
      <title>Convergence of Sequences in Metric Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1713/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1713/</guid>
      <description>Definitions1 If there exists a point $p \in X$ such that the sequence $\left\{ p_{n} \right\}$ of points in a metric space $(X,d)$ satisfies the following condition, the sequence $\left\{ p_{n} \right\}$ is said to converge to $p$, and it is denoted by $p_{n} \rightarrow p$ or $\lim \limits_{n\to \infty}p_{n}=p$. $$ \forall \varepsilon &amp;gt;0,\ \exists N\in \mathbb{N}\ \mathrm{s.t}\ n\ge N \implies d(p_{n},p)&amp;lt;\varepsilon $$ If $\left\{ p_{n} \right\}$ does not converge,</description>
    </item>
    <item>
      <title>Every k Cell is Compact</title>
      <link>https://freshrimpsushi.github.io/en/posts/1711/</link>
      <pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1711/</guid>
      <description>Definition For $a_{i},b_{i} \in \mathbb{R} (1\le i \le k)$, the set $I=[a_{1},b_{1}] \times [a_{2},b_{2}]\times \cdots \times [a_{k},b_{k}]$ is called a $k$-cellk-cell. Here, $\times$ represents the Cartesian product of sets. Theorem 1 Let&amp;rsquo;s assume a sequence of closed intervals on $\mathbb{R}$, $\left\{ I_{n} \right\}$, satisfies $I_{n}\supset I_{n+1}\ (n=1,2,\cdots)$. Then, the following holds true. $$ \bigcap_{i=1}^{\infty}I_{n}\ne \varnothing $$ Proof Let&amp;rsquo;s denote $I_{n}=[a_{n},b_{n}]$. Also, let $E=\left\{ a_{n} : n=1,2,\cdots \right\}$. Then, $E\ne \varnothing$</description>
    </item>
    <item>
      <title>Closed Subsets of Compact Sets in Metric Spaces are Compact</title>
      <link>https://freshrimpsushi.github.io/en/posts/1706/</link>
      <pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1706/</guid>
      <description>Theorem1 In a metric space $X$, a closed (relative to $X$) subset of a compact set $K$ is compact. Proof Given a metric space $X$ where $F\subset K \subset X$ and assuming $F$ is a closed set in $X$ and $K$ is a compact set. Let $\left\{ V_{\alpha}\right\}$ be an arbitrary open cover of $F$. By adding $F^{c}$, let&amp;rsquo;s denote it as $\Omega=\left\{ V_\alpha \right\}\cup \left\{ F^{c} \right\}$. Then $\Omega$</description>
    </item>
    <item>
      <title>Compactness in Metric Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1705/</link>
      <pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1705/</guid>
      <description>Definition Open Cover Given a metric space $(X,d)$ and a subset $E\subset X$, a set $\left\{ O_{\alpha} \right\}$ of open sets that satisfies the following equation is called an open cover of $E$. $$ E\subset \bigcup _{\alpha} O_{\alpha} $$ A subset of an open cover is called a subcover. In particular, a subcover with a finite number of elements is called a finite subcover. Compactness Let&amp;rsquo;s assume we have a</description>
    </item>
    <item>
      <title>Properties of Open and Closed Sets in Metric Spaces</title>
      <link>https://freshrimpsushi.github.io/en/posts/1702/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1702/</guid>
      <description>Let $(X,d)$ be a metric space. Suppose $p \in X$ and $E \subset X$. The set that contains all $q$s satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhood of point $p$ and is denoted by $N_{r}(p)$. Here $r$ is called the radius of $N_{r}(p)$. When it&amp;rsquo;s possible to omit the metric, it can also be denoted as $N_{p}$. If every neighborhood of $p$ contains $q$s with $q\ne p$ and $q\in E$,</description>
    </item>
    <item>
      <title>Closure and Derived Set in Metric Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/1701/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1701/</guid>
      <description>Definitions Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. A set that contains all $q$ satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhoodneighborhood of point $p$, denoted by $N_{r}(p)$. In this case, $r$ is called the radius of $N_{r}(p)$. If it&amp;rsquo;s permissible to omit the distance, it can also be denoted as $N_{p}$. If every neighborhood of $p$ includes a $q$ that is $q\ne</description>
    </item>
    <item>
      <title>Neighborhood, Limit Point, Open, Closed in Metric Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/1700/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1700/</guid>
      <description>Definition Let&amp;rsquo;s say $(X,d)$ is a metric space. Suppose $p \in X$ and $E \subset X$. The set that includes all $q$s satisfying $d(q,p)&amp;lt;r$ is defined as the neighborhoodneighborhood of point $p$ and is denoted as $N_{r}(p)$. Here, $r$ is called the radius of $N_{r}(p)$. If the distance can be omitted, it may also be denoted as $N_{p}$. If all neighborhoods of $p$ contain $q$, which is $q\ne p$ and</description>
    </item>
    <item>
      <title>Ellipse</title>
      <link>https://freshrimpsushi.github.io/en/posts/1685/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1685/</guid>
      <description>Definition The set of points on a plane whose sum of distances to two fixed points $F$, $F^{\prime}$ is constant, are called an ellipse. The components of an ellipse are as follows. $F$, $F^{\prime}$ are called foci. $a$ is called the semimajor axis, and $b$ is called the semiminor axis. $b=\sqrt{1-\epsilon^{2}}a$ is satisfied. $\epsilon$ is called the eccentricity of the ellipse. It represents how ellipsed is compressed, and the foci</description>
    </item>
    <item>
      <title>Law of Universal Gravitation: Gravity</title>
      <link>https://freshrimpsushi.github.io/en/posts/1678/</link>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1678/</guid>
      <description>Law of Universal Gravity1 The law of universal gravity, announced by Newton through his Principia in 1687, is a physical law that simply states &amp;ldquo;every object attracts every other object&amp;rdquo;. To describe this concept in detail: Every particle of matter in the universe with mass attracts every other particle with a force that is directly proportional to the product of their masses and inversely proportional to the square of the</description>
    </item>
    <item>
      <title>Angular Momentum and Torque</title>
      <link>https://freshrimpsushi.github.io/en/posts/1674/</link>
      <pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1674/</guid>
      <description>Angular Momentum1 Momentum is a physical quantity that represents the state of motion of a moving object. The larger the mass and the faster the speed, the greater the momentum. In physics, there is an interest in how the motion of an object changes. Therefore, the force, which is the cause of changing the state of motion of an object, is expressed as a change in momentum. $$ \mathbf{F}=\frac{d \mathbf{p}}{dt}</description>
    </item>
    <item>
      <title>Newton&#39;s Laws of Motion</title>
      <link>https://freshrimpsushi.github.io/en/posts/1671/</link>
      <pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1671/</guid>
      <description>Newton&amp;rsquo;s Laws of Motion 1 English mathematician and physicist Isaac Newton presented three laws about motion in 1687 in Principia as follows: An object not subjected to an external force does not change its state of motion. The change in motion is proportional to the applied force on the object. When object 1 applies a force to object 2, object 2 simultaneously applies a force equal in magnitude and opposite</description>
    </item>
    <item>
      <title>Physics: The Definition of Mass, Force, and Momentum</title>
      <link>https://freshrimpsushi.github.io/en/posts/1673/</link>
      <pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1673/</guid>
      <description>Mass1 In Newton&amp;rsquo;s laws of motion, inertia is described as the property that resists changes in motion. That is, the greater the inertia, the harder it is to move, and the smaller the inertia, the easier it is to move. This exactly aligns with our experience that it&amp;rsquo;s harder to push a heavier object than a lighter one. Hence, the magnitude of inertia can be expressed by the magnitude of</description>
    </item>
    <item>
      <title>Center of Mass and Linear Momentum of a Particle System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1670/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1670/</guid>
      <description>Definition A system of particles is referred to as a particle system. Description 1 When the position vectors of particles with masses $m_{1}$, $m_2$, $\cdots$, and $m_{n}$ are $\mathbf{r}_{1}$, $\mathbf{r}_{2}$, $\cdots$, and $\mathbf{r}_{n}$, respectively, the center of mass of this particle system is defined as follows. $$ \mathbf{r}_{cm}=\frac{m_{1}\mathbf{r}_{1}+m_{2}\mathbf{r}_{2}+\cdots + m_{n}\mathbf{r}_{n}}{m_{1}+ m_{2}+ \cdots+ m_{n}}=\frac{\sum m_{i}\mathbf{r}_{i}}{m} $$ Here, $m=\sum \limits_{i}m_{i}$ denotes the total mass of the particle system. The subscript $cm$ stands</description>
    </item>
    <item>
      <title>Linearity of Riemann(-Stieltjes) Iintegral</title>
      <link>https://freshrimpsushi.github.io/en/posts/1666/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1666/</guid>
      <description>Theorem1 This article is based on the Riemann-Stieltjes integral. If set to $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Let&amp;rsquo;s say $f$ is integrable by Riemann(-Stieltjes) from $[a,b]$. Then, for a constant $c\in \mathbb{R}$, $cf$ is also integrable from $[a,b]$, and its value is as follows. $$ \int_{a}^{b}cf d\alpha = c\int_{a}^{b}f d\alpha $$ Let two functions $f_{1}$, $f_{2}$ be integrable by Riemann(-Stieltjes) from $[a,b]$. Then, $f_{1}+f_{2}$ is</description>
    </item>
    <item>
      <title>First-Order Linear Differential Equation System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1659/</link>
      <pubDate>Thu, 11 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1659/</guid>
      <description>Buildup1 When the mass is $m$, the damping factor is $\gamma$, and the spring constant is $k$, the equation of motion representing the vibration of an object hung on a spring is as follows. $$ m x^{\prime \prime} + \gamma x^{\prime} + kx = F $$ Letting $x_{1}=x$, $x_{2}=x_{1}^{\prime}$, the above equation of motion can be expressed as the following system. $$ \begin{align*} x_{1}^{\prime}(t) =&amp;amp;\ x_{2}(t) \\ x_{2}^{\prime} (t) =&amp;amp;\</description>
    </item>
    <item>
      <title>What is a Differential Operator in Physics?</title>
      <link>https://freshrimpsushi.github.io/en/posts/1638/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1638/</guid>
      <description>Explanation One of the methods to solve differential equations is to use the differential operator. Let&amp;rsquo;s define the differential operator $D$ as follows. $$ D:= \frac{d}{dx} $$ When explicitly expressing the variable being differentiated, it is also denoted as $D_{x}$. For partial differentiation, it is represented as follows. $$ \partial _{x}:=\frac{ \partial }{ \partial x},\quad \partial_{y}=\frac{ \partial }{ \partial y} $$ Using the differential operator, the differential equation is expressed</description>
    </item>
    <item>
      <title>Modified Bessel Equation and Modified Bessel Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1624/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1624/</guid>
      <description>Buildup The differential equation below is referred to as the modified Bessel equation. $$ x^2 y^{\prime \prime} + xy^{\prime}-(x^2-\nu^2)y=0 $$ It is a form of the Bessel equation where the sign of the term $y$ has been changed to $+ \rightarrow -$. The solution to this differential equation is given by the formula for differential equations that have Bessel equation solutions, as follows. $$ y=Z_{\nu}(ix)=AJ_{\nu}(ix)+BN_{\nu}(ix) $$ The two commonly used</description>
    </item>
    <item>
      <title>The Second Series Solution of the Bessel Equation: Bessel Functions of the Second Kind, Neumann Functions, Weber Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1618/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1618/</guid>
      <description>Definition[^1] A second solution of the Bessel equation is called the Neumann function, denoted by $N_{\nu}(x)$ or $Y_{\nu}(x)$. For non-integer $\nu$, $$ N_{\nu}(x)=Y_{\nu}(x)=\frac{\cos (\nu \pi)J_{\nu}(x)-J_{-\nu}(x)}{\sin (\nu\pi)} $$ For integer $\nu$, it is defined by the limit. For $n\in \mathbb{Z}$, $\nu \in \mathbb{R}\setminus\mathbb{Z}$, $$ N_{n}(x)=\lim \limits_{\nu \rightarrow n}N_{\nu}(x) $$ Here, $J_{\pm \nu}(x)$ is the first kind Bessel function. Thus, the general solution of the Bessel equation is as follows. $$ y(x)=AJ_{\nu}(x)+BN_{\nu}(x)</description>
    </item>
    <item>
      <title>Legendre Polynomials</title>
      <link>https://freshrimpsushi.github.io/en/posts/1611/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1611/</guid>
      <description>Definition Legendre Polynomials are defined in several ways. As Solutions to a Differential Equation Legendre polynomials are solutions to the following Legendre differential equation. $$ (1-x^{2}) \dfrac{d^{2} y}{dx^{2}} -2x\dfrac{dy}{dx} + l(l+1) y = 0 $$ Rodrigues&amp;rsquo; Formula The following function $P_{l}$ is called a Legendre polynomial. $$ P_{l}(x) = \dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} $$ This is known as Rodrigues&amp;rsquo; formula. Using Orthogonality Description By definition, $P_{n}$ is indeed a polynomial &amp;lsquo;function&amp;rsquo;,</description>
    </item>
    <item>
      <title>Associated Legendre Differential Equations and Polynomials</title>
      <link>https://freshrimpsushi.github.io/en/posts/1605/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1605/</guid>
      <description>Definition1 The differential equation given below is called the associated Legendre differential equation. $$ \begin{equation} \begin{aligned} &amp;amp;&amp;amp;(1-x^{2})\frac{ d^{2}y }{ dx^{2} }-2x \frac{dy}{dx}+\left[ +l(l+1)-\frac{m^{2}}{1-x^{2}} \right]y =&amp;amp;\ 0 \\ \mathrm{or} &amp;amp;&amp;amp; \frac{ d }{ dx } \left[ (1-x^{2})y^{\prime} \right] +\left[ l(l+1)-\frac{m^{2}}{1-x^{2}} \right]y =&amp;amp;\ 0 \end{aligned} \label{1} \end{equation} $$ The solution to the associated Legendre differential equation is denoted as $P_{l}^{m}(x)$, and this is called the associated Legendre polynomial or the generalized Legendre</description>
    </item>
    <item>
      <title>Derivation of the Schrödinger Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1598/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1598/</guid>
      <description>Overview Time-independent Schrödinger equation $$ H\psi=\left(-\frac{\hbar^{2}}{2m}\frac{ d ^{2} }{ d x^{2} }+V\right)\psi=E\psi \\ H\psi=\left(-\frac{\hbar^{2}}{2m}\nabla^{2}+V\right)\psi=E\psi $$ Time-dependent Schrödinger equation $$ i\hbar\frac{ \partial \psi}{ \partial t}=\left(-\frac{\hbar^{2}}{2m}\frac{ \partial ^{2} }{\partial x^{2} }+V\right)\psi \\ i\hbar\frac{ \partial \psi}{ \partial t}=\left(-\frac{\hbar^{2}}{2m}\nabla^{2}+V\right)\psi $$ The Schrödinger equation refers</description>
    </item>
    <item>
      <title>Spherical Harmonics: General Solutions for the Polar and Azimuthal Angles in the Spherical Coordinate Laplace&#39;s Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1580/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1580/</guid>
      <description>Definition The general solution for the polar and azimuthal angles in the spherical coordinate system for the Laplace equation is as follows, and this is called Spherical harmonics. $$ Y_{l}^{m}(\theta,\phi)=e^{im\phi}P_{l}^{m}(\cos \theta) $$ Here, $l$ is $l=0,1,2\cdots$ and $m$ is an integer that satisfies $ -l \le m \le l$. Also, $P_{l}^{m}(\cos\theta)$ is as follows. $$ \begin{align*} P_{l}^{m}(\cos \theta)&amp;amp;= (1-\cos ^{2}\theta)^{\frac{|m|}{2}} \frac{ d^{|m|} }{ dx^{|m|} } P_{l}(x) \\ &amp;amp; =(1-\cos ^{2}\theta)^{\frac{|m|}{2}}</description>
    </item>
    <item>
      <title>Physics에서의 Del 연산자</title>
      <link>https://freshrimpsushi.github.io/en/posts/1575/</link>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1575/</guid>
      <description>Explanation In physics, an operator refers to a function that maps a function to another function. Among these, the del operator refers to a function that, given a function, results in a function that has the derivative of the given function as its function value. If the term operator is unfamiliar, you can simply understand it as a rule that computes a target. For example, if you insert $f$ into</description>
    </item>
    <item>
      <title>In Quantum Mechanics, What is a Commutator?</title>
      <link>https://freshrimpsushi.github.io/en/posts/1574/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1574/</guid>
      <description>Definition For two operators $A, B$, $AB - BA$ is defined as the commutator of $A, B$ as follows. $$ [A,B]=AB-BA $$ Explanation When first encountering the definition of a commutator, one might wonder why it is not $AB - BA = 0$. However, since operators can be represented as matrices, and the multiplication of two matrices does not obey the commutative law, different results can appear depending on the</description>
    </item>
    <item>
      <title>Differential Equations for Physics: Solutions to Commonly Encountered Differential Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/1538/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1538/</guid>
      <description>Differential Equations This has been explained as intuitively as possible for those studying undergraduate physics. A differential equation is, simply put, an equation that involves derivatives. Without any complications, since acceleration is the second derivative of position, the most famous physics formula $F=ma$ is also a differential equation. The polynomial $x^{3}+3x+1=0$ is called a third-degree equation because its highest order is 3. Similarly, when the maximum number of times differentiated</description>
    </item>
    <item>
      <title>Vector, Inner Product, Wave Function, Hilbert Space in Quantum Mechanics</title>
      <link>https://freshrimpsushi.github.io/en/posts/1509/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1509/</guid>
      <description>Generalization of Vectors Linear Algebra might be a new concept for science students who haven&amp;rsquo;t studied it yet. To them, a vector refers to a physical quantity with magnitude and direction, representing a point in 3-dimensional space, often denoted as $\vec{x} = (x_{1}, x_{2}, x_{3})$. This definition is sufficient for studying classical mechanics and electromagnetism. However, in quantum mechanics, concepts like Fourier Analysis, Inner Product of Functions emerge, making it</description>
    </item>
    <item>
      <title>Series Solution of the Bessel Equation: Bessel Functions of the First Kind</title>
      <link>https://freshrimpsushi.github.io/en/posts/1503/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1503/</guid>
      <description>Definition1 For $\nu \in \mathbb{R}$, a differential equation of the following form is called a $\nu$ order Bessel equation. $$ \begin{align*} &amp;amp;&amp;amp; x^{2} y^{\prime \prime} +xy^{\prime}+(x^{2}-\nu^{2})y &amp;amp;= 0 \\ \text{or} &amp;amp;&amp;amp; y^{\prime \prime}+\frac{1}{x} y^{\prime} + \left( 1-\frac{\nu^{2}}{x^{2}} \right)y &amp;amp;= 0 \end{align*} $$ Explanation The Bessel equation emerges when solving the wave equation in spherical coordinates. The coefficients are not constant but depend on the independent variable $x$. Since, at $x=0$,</description>
    </item>
    <item>
      <title>Euler Integrals: Beta Function and Gamma Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1483/</link>
      <pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1483/</guid>
      <description>Definition Euler Integrals The following two integrals are referred to as Euler integrals. $(a)$ Euler integral of the first kind: Beta function $$ B(p,q)=\int_{0}^1 t^{p-1}(1-t)^{q-1}dt,\quad p&amp;gt;0,\quad q&amp;gt;0 $$ $(b)$ Euler integral of the second kind: Gamma function $$ \Gamma (p) = \int_{0}^\infty t^{p-1}e^{-t}dt,\quad p&amp;gt;0 $$ Explanation Euler Integral of the First Kind 1-1. Beta Function: If the gamma function is considered a generalization of the factorial, then the beta function</description>
    </item>
    <item>
      <title>Relationship between Beta Function and Gamma Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1481/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1481/</guid>
      <description>Theorem $$ B(p,q) = {{\Gamma (p) \Gamma (q)} \over {\Gamma (p+q) }} $$ Explanation The Beta function is defined as $\displaystyle B(p,q) := \int_{0}^{1} t^{p-1} (1-t)^{q-1} dt $, and, like the Gamma function, it is an important function applied in many fields. Since the Gamma function can be easily calculated using the recursive relationship, the Beta function can also be calculated easily using the above relation. Intuitively, it can be</description>
    </item>
    <item>
      <title>Factorial, Double Factorial, Multi Factorial</title>
      <link>https://freshrimpsushi.github.io/en/posts/1477/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1477/</guid>
      <description>Factorial For a natural number $n$, $n!$ is defined as $n$factorial이라 읽고 아래와 같이 정의한다. $$ n!=n\cdot(n-1)\cdot(n-2)\cdots 2\cdot 1 =\prod\limits_{k=1}^n k $$ 설명 많은 곳에서 식을 깔끔하게 표현하기 위해 사용된다. $0$</description>
    </item>
    <item>
      <title>Derivation of the Gamma Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1476/</link>
      <pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1476/</guid>
      <description>Non-negative Integers and the Gamma Function For $\alpha &amp;gt;0$, $$ \int_{0}^{\infty} e^{-\alpha x} dx=\left[-\frac{1}{\alpha}e^{-\alpha x}\right]_{0}^{\infty}=\frac{1}{\alpha} $$ Differentiating both sides with respect to $\alpha$, according to the Leibniz integral rule, allows the differentiation to move under the integration sign, thus giving $$ \begin{align*} &amp;amp;&amp;amp;\int_{0}^\infty -xe^{-\alpha x}dx&amp;amp;=-\frac{1}{\alpha^2} \\ \implies &amp;amp;&amp;amp; \int_{0}^\infty xe^{-\alpha x}dx &amp;amp;= \frac{1}{\alpha ^2} \end{align*} $$ Continuing to differentiate gives $$ \begin{align*} \int_{0}^\infty x^2e^{-\alpha x}dx&amp;amp;=\frac{2}{\alpha^3} \\ \int_{0}^\infty x^3e^{-\alpha x}dx&amp;amp;=\frac{3\cdot 2}{\alpha^4}</description>
    </item>
    <item>
      <title>Leibniz Integral Rule</title>
      <link>https://freshrimpsushi.github.io/en/posts/1475/</link>
      <pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1475/</guid>
      <description>Summary Let&amp;rsquo;s assume that $f(x,t)$ and $\dfrac{\partial f}{\partial x}(x,t)$ are consecutive. Then, the following equation holds. $$ \frac{d}{dx} \int_{a}^b f(x,t)dt = \int_{a}^b\frac{\partial f}{\partial x}(x,t)dt $$ Description Being able to interchange the order of differentiation and integration is undoubtedly useful. Besides, there are many theorems or formulas related to differentiation and integration named after Leibniz. Proof Since if continuous, then integrable, let&amp;rsquo;s assume $u$ as follows. $$ u(x):=\int_{a}^b f(x,t)dt $$ Then,</description>
    </item>
    <item>
      <title>How to Change Image Size in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1466/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1466/</guid>
      <description>Resizing Images To resize images, you can use the imresize function from the Images package. The function name is the same as in Matlab. imresize(X, ratio=a): Returns the image of array X scaled by a factor of a. Unlike Matlab, you must explicitly write ratio=a. imresize(X, m, n): Returns the image of array X resized to m rows and n columns. Below are example codes and their results. using Images</description>
    </item>
    <item>
      <title>Measuring Code Execution Time in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1467/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1467/</guid>
      <description>Methods tic X1=rand(2^7); X2=rand(2^8); X3=rand(2^9); X4=rand(2^10); X5=rand(2^11); toc Y1=imrotate(X1,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y2=imrotate(X2,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y3=imrotate(X3,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y4=imrotate(X4,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc Y5=imrotate(X5,45,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); toc tic: Starts a stopwatch for measuring execution time. toc: Returns the current time on the stopwatch. Note that it&amp;rsquo;s not measuring the time between toc and toc. To measure the computation time of calculating Y1~Y6 in the example code above, you should enter the code as follows. tic X1=rand(2^7); X2=rand(2^8); X3=rand(2^9); X4=rand(2^10);</description>
    </item>
    <item>
      <title>How to Rotate Image Arrays in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1462/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1462/</guid>
      <description>Image Rotation imrotate(X, theta): Rotates array X by theta radians. Note that, unlike in MATLAB where the angle unit is degrees ($^{\circ})$, the angle unit here is radians. Additionally, unlike MATLAB, it rotates clockwise. If no other variables are inputted, the interpolation method defaults to bilinear, and the rotated image is not cropped. Examples of rotating the original image X by $90^\circ=\pi/2$, $180^\circ=\pi$, and $270^\circ=\frac{3}{2}\pi$, along with their results, are</description>
    </item>
    <item>
      <title>Resizing Images in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1465/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1465/</guid>
      <description>Methods imresize(A, scale): Returns a new image by adjusting the size of A by a factor of scale. If A is a 10x10 image and a scale of 0.5 is input, it returns a 5x5 image. You can also adjust the size directly as follows. imresize(A, [m n]): Returns an image with m rows and n columns. Below are example codes and their results. X=imread(&amp;#39;test\_{i}mage.jpg&amp;#39;); figure() imshow(X) saveas(gcf,&amp;#39;X.png&amp;#39;) title(&amp;#39;X&amp;#39;) Y1=imresize(X,0.5);</description>
    </item>
    <item>
      <title>Functions for 2D Array Operations in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1460/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1460/</guid>
      <description>Let&amp;rsquo;s say $A = \begin{pmatrix} 1 &amp;amp; 2 &amp;amp; 1 \\ 0 &amp;amp; 3 &amp;amp; 0 \\ 2 &amp;amp; 3 &amp;amp; 4\end{pmatrix}$. Transpose Matrix julia&amp;gt; A =[1 2 1; 0 3 0; 2 3 4] 3×3 Array{Int64,2}: 1 2 1 0 3 0 2 3 4 julia&amp;gt; transpose(A) 3×3 LinearAlgebra.Transpose{Int64,Array{Int64,2}}: 1 0 2 2 3 3 1 0 4 julia&amp;gt; A&amp;#39; 3×3 LinearAlgebra.Adjoint{Int64,Array{Int64,2}}: 1</description>
    </item>
    <item>
      <title>How to output and save arrays as heatmap images in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1459/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1459/</guid>
      <description>Heatmap Using the heatmap function from the Plots package, you can output a 2D array as a heatmap image, and with the savefig function, you can save the resulting image. The @__DIR__ macro tells you the location of the Julia code file. # code1 However, if you compare array A with the heatmap image, you may notice that the top and bottom of the array are flipped in the heatmap</description>
    </item>
    <item>
      <title>Translating Arrays in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1453/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1453/</guid>
      <description>Description Using circshifr(A, (n,m)), you can shift the rows of the array A $n$ positions down, and the columns $m$ positions to the right. (n,m) must be a tuple of integers, and negative numbers are also possible. If negative, it shifts in the opposite direction. For arrays of 3 dimensions or more, it is applied to each smallest 2-dimensional array respectively. Code 2D array julia&amp;gt; A = transpose(reshape(1:25,5,5)) 5×</description>
    </item>
    <item>
      <title>Various Methods of Creating Vectors in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/1452/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1452/</guid>
      <description>코드 julia&amp;gt; x1=[1 2 3] 1×3 Array{Int64,2}: 1 2 3 julia&amp;gt; x2=[1, 2, 3] 3-element Array{Int64,1}: 1 2 3 julia&amp;gt; x3=[i for i in 1:3] 3-element Array{Int64,1}: 1 2 3 julia&amp;gt; x4=[i for i in 1:3:10] 4-element Array{Int64,1}: 1 4 7 10 julia&amp;gt; x5=[i for i in 1:3:11] 4-element Array{Int64,1}: 1 4 7 10 x1 is a 2-dimensional array. Since it looks like a row vector, if</description>
    </item>
    <item>
      <title>Subspace Topology, Relative Topology</title>
      <link>https://freshrimpsushi.github.io/en/posts/1439/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1439/</guid>
      <description>Definition 1 Let&amp;rsquo;s assume that a topological space $(X,\mathscr{T})$ and a subset $A \subset X$ are given. Then, the following set $$ \mathscr{T}_{A} =\left\{ A\cap U\ :\ U\in \mathscr{T} \right\} $$ is a topology on $A$. In this case, $\mathscr{T}_{A}$ is referred to as the Subspace Topology or Relative Topology. Moreover, the topological space $(A, \mathscr{T}_{A})$ is called the Subspace of $(X,\mathscr{T})$. Theorem [0]: For a topological space $(X, \mathscr{T}$)</description>
    </item>
    <item>
      <title>How to Create Equally Spaced Row Vectors in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1376/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1376/</guid>
      <description>Method linspace(a,b,n): Returns a row vector of $[a,b]$ divided into $n$ equal intervals. If the number of elements is not specified, it returns a $1\times 100$ vector. It is used when the number of intervals is important, not the length of the intervals. a: m :b : Returns a row vector of $[a,b]$ divided by equal intervals of $m$. If the interval is not specified, the interval is set to</description>
    </item>
    <item>
      <title>Selecting Specific Rows and Columns in a Matrix in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1362/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1362/</guid>
      <description>Methods $m \times n$ Given a data in the form of a matrix, let&amp;rsquo;s call it $A$. If you want to use only a specific part of matrix $A$, you can use the following method. B=A(a:b, c:d) Running the code as above, $B$ becomes a $(b-a) \times (d-c)$ matrix containing the data from row $a$ to $b$, column $c$ to $d$ of matrix $A$. Below is the example code and</description>
    </item>
    <item>
      <title>How to Specify Colors, Line Styles, and Marker Types in MATLAB Graphs</title>
      <link>https://freshrimpsushi.github.io/en/posts/1330/</link>
      <pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1330/</guid>
      <description>Properties The properties of a graph can be specified as follows. Graph Color Marker Line Style Red r Dot . Solid - Green g Star * Dotted : Blue b X x Dash-dot -. Black k Circle o (letter o) Dashed -- Yellow y Plus + Magenta m Square s White w Diamond d Cyan c Star p Triangle down v Triangle up ^ Triangle left &amp;lt; Triangle right &amp;gt;</description>
    </item>
    <item>
      <title>Rotating an Image in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1328/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1328/</guid>
      <description>Method imrotate(I,angle,method,bbox) I: Image to be rotated. angle: The angle of rotation in degrees. method: The interpolation method. Options are &amp;rsquo;nearest&amp;rsquo;, &amp;lsquo;bilinear&amp;rsquo;, &amp;lsquo;bicubic&amp;rsquo;. If nothing is specified, &amp;rsquo;nearet&amp;rsquo; is applied. X = phantom(&amp;#39;Modified Shepp-Logan&amp;#39;,64); figure() imagesc(X) title(&amp;#39;X&amp;#39;) Y1=imrotate(X,30,&amp;#39;nearest&amp;#39;,&amp;#39;crop&amp;#39;); Y2=imrotate(X,30,&amp;#39;bilinear&amp;#39;,&amp;#39;crop&amp;#39;); Y3=imrotate(X,30,&amp;#39;bicubic&amp;#39;,&amp;#39;crop&amp;#39;); figure() subplot(1,3,1) imagesc(Y1) title(&amp;#39;Y1 - nearest&amp;#39;) subplot(1,3,2) imagesc(Y2) title(&amp;#39;Y2 - bilinear&amp;#39;) subplot(1,3,3) imagesc(Y3) title(&amp;#39;Y3 - bicubic&amp;#39;) bbox: Specifies the size of the output image. &amp;rsquo;loose&amp;rsquo; enlarges the size of the</description>
    </item>
    <item>
      <title>Creating Special Matrices in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1327/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1327/</guid>
      <description>Zero Matrix zeros(): Returns a zero matrix. zeros(n): Returns a $n\times n$ zero matrix. zeros(m,n): Returns a $n\times m$ zero matrix. zeros(size(A)): Returns a zero matrix of the same size as matrix A. Matrix with All Elements as 1 ones(): Returns a matrix where all elements are 1. However, for operations between two matrices, it&amp;rsquo;s more convenient to just use 1. It&amp;rsquo;s obvious that the code below is much simpler</description>
    </item>
    <item>
      <title>How to Perform Element-wise Operations on Two Matrices in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1326/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1326/</guid>
      <description>Multiplication times(), .*: Returns the result of multiplying each element of two matrices. The operation can only proceed if the two matrices are of the exact same size, or one of them is a scalar, or if one is a row vector with the same row size, or a column vector with the same column size. If the sizes are different, the smaller matrix is treated as if it were</description>
    </item>
    <item>
      <title>Matrix Size and Related Functions in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1323/</link>
      <pubDate>Sat, 21 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1323/</guid>
      <description>Functions size(): Returns a row vector that contains the lengths of the rows and columns of the matrix. It is useful for creating a zero matrix of the same size as the matrix being dealt with. zeros(size(A)): Returns a zero matrix of the same size as A. length(): Returns the larger number among the rows and columns. In the case of row vectors and column vectors, it is the same</description>
    </item>
    <item>
      <title>Sets Outside/Inside a Certain Distance from the Boundary of a Set</title>
      <link>https://freshrimpsushi.github.io/en/posts/1317/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1317/</guid>
      <description>Definition Let us assume an open set $\Omega \subset \mathbb{R}^n$ is given. Then, $\Omega_{&amp;lt;\delta}$ and $\Omega_{&amp;gt;\delta}$ are defined as follows. $$ \begin{align*} \Omega_{&amp;lt;\delta} :=&amp;amp; \left\{ x\in\Omega : \mathrm{dist}(x, \mathrm{bdry}\Omega)&amp;lt;\delta \right\} \\ \Omega_{&amp;gt;\delta} :=&amp;amp; \left\{ x\in\Omega : \mathrm{dist}(x, \mathrm{bdry}\Omega)&amp;gt;\delta \right\} \end{align*} $$ Explanation Such sets are usefully employed in partial differential equations, functional analysis, etc. Depending on the textbook, there are cases where it&amp;rsquo;s $\Omega_\delta=\Omega_{&amp;lt;\delta}$1 and cases where it&amp;rsquo;s $\Omega_\delta=\Omega_{&amp;gt;\delta}$2. In</description>
    </item>
    <item>
      <title>Mutually Singular</title>
      <link>https://freshrimpsushi.github.io/en/posts/1310/</link>
      <pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1310/</guid>
      <description>Definition1 Given two signed measures $\nu$, $\nu$. If there exists a $E,F\ \in \mathcal{E}$ that satisfies the following three conditions for $\nu$, $\mu$, we say that the two signed measures $\nu$, $\mu$ are and denote it as $\nu \perp \mu$ or $\mu \perp \nu$: $E \cup F=X$ $E \cap F=\varnothing$ $E$ is a null set with respect to $\nu$, and $F$ is a null set with respect to $\mu$. Also,</description>
    </item>
    <item>
      <title>Hahn Decomposition Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/1308/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1308/</guid>
      <description>Theorem1 (a) Let $\nu$ be a signed measure defined on a measurable space $(X, \mathcal{E})$. Then there exist a positive set $P$ and a negative set $N$ for $\nu$, satisfying the following: $$ P \cup N=X \quad \text{and} \quad P \cap N =\varnothing $$ Such a $X=P \cup N$ is called a Hahn decomposition for $\nu$. (b) Let $P^{\prime}, N^{\prime}$ be another pair of sets satisfying (a). Then the following</description>
    </item>
    <item>
      <title>Positive Set, Negative Set, Null Set</title>
      <link>https://freshrimpsushi.github.io/en/posts/1303/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1303/</guid>
      <description>Definition1 Let us call $\nu$ on $(X,\mathcal{E})$ a sign measure. And let us denote $E,F \in \mathcal{E}$. Then When $\nu (F) \ge 0,\ \forall F\subset E$, we call $E$ regarding $\nu$ a positive set or simply positive. When $\nu (F) \le 0,\ \forall F\subset E$, we call $E$ regarding $\nu$ a negative set or simply negative. When $\nu (F)=0,\ \forall F\subset E$, we call $E$ regarding $\nu$ a null set</description>
    </item>
    <item>
      <title>Signed Measures</title>
      <link>https://freshrimpsushi.github.io/en/posts/1301/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1301/</guid>
      <description>Definition1 Let $(X, \mathcal{E})$ be a measurable space. A function $\nu : \mathcal{E} \to \overline{\mathbb{R}}$ which takes extended real values and satisfies the conditions below is called a signed measure. $\nu ( \varnothing ) = 0$ At most one of $\pm \infty$ can have a function value of $\nu$. In other words, if $-\infty \in \nu (\mathcal{E})$ then $+\infty \notin \nu (\mathcal{E})$, and if $+\infty \in \nu (\mathcal{E})$ then $-\infty</description>
    </item>
    <item>
      <title>General Definitions of Measure</title>
      <link>https://freshrimpsushi.github.io/en/posts/1302/</link>
      <pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1302/</guid>
      <description>Definition Let $(X,\mathcal{E})$ be a measurable space. A function $\mu : \mathcal{E} \to \overline{\mathbb{R}}$ that takes extended real values is called a measure if it satisfies the following three conditions: (a) $\mu ( \varnothing ) = 0$ (b) $\mu (E) \ge 0,\quad \forall E\in \mathcal{E}$ (c) Suppose $\left\{E_{j}\right\}$ are sequences of mutually disjoint sets in $\mathcal{E}$. Then the following holds: $$ \mu \left( \bigcup _{j=1}^\infty E_{j} \right) =\sum \limits_{j=1}^\infty \mu</description>
    </item>
    <item>
      <title>Extended Real Number System</title>
      <link>https://freshrimpsushi.github.io/en/posts/1252/</link>
      <pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1252/</guid>
      <description>Definition The set defined as follows is called the extended real number system. $$ \overline{ \mathbb{R} } := \mathbb{R} \cup \left\{ -\infty, +\infty\right\} $$ Explanation In fields such as analysis, for convenience, the set $\mathbb{R}$ is often replaced with $\overline{ \mathbb{R} }$. $\pm \infty$ is not a number, but for convenience, it is treated as one and added to $\mathbb{R}$. Within the extended real number system, the rules for comparison</description>
    </item>
    <item>
      <title>Borel Sigma-Algebra, Borel Measurable Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/1251/</link>
      <pubDate>Wed, 21 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1251/</guid>
      <description>Theorem Let $X$ be an arbitrary set. Given a non-empty set $A \subset \mathcal{P}(X)$, there exists the smallest $\sigma$-algebra, $\mathcal{E}_{A}$, that contains $A$. Proof We define $\mathcal{E}_{A}$ and show that it is an $\sigma$-algebra and then prove that it is the smallest1. Let $S$ be the set of all $\sigma$-algebras that contain $A$. $$ S:= \left\{ \mathcal{E} \subset \mathcal{P}(X)\ :\ \mathcal{E}\ \mathrm{is\ } \sigma \mathrm{-algebra, \ } A \subset \mathcal{E}</description>
    </item>
    <item>
      <title>How to Output Multiple Figures on One Page in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1247/</link>
      <pubDate>Sat, 17 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1247/</guid>
      <description>Method The subplot() function can be used to print multiple figures on one page. The first and second parameters respectively indicate the rows and columns of the chessboard on which images will be displayed, deciding the layout of the figures. The third parameter determines the sequence in which the specific figure will be placed. Below is the code and the actual output. X1=Phantom(); X2=radon(X1); X3=fft(X2); X4=iradon(X2,0:179); subplot(2,2,1) imagesc(X1) title(&amp;#34;Phantom&amp;#34;); subplot(2,2,2)</description>
    </item>
    <item>
      <title>Reflection and Transmission of Wave Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1241/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1241/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 Reflection Coefficients and Transmission Coefficients The reflection coefficient (reflectivity) and transmission coefficient (transmissibility) of a wave function can be represented as follows: $$ R=\left| \frac{j_{ref}}{j_{inc}} \right|,\quad T=\left| \frac{j_{trans}}{j_{inc}}\right| $$ Here, $j$ is the probability flow. $inc$ refers to the incident $(\mathrm{incident})$. $R$, and $ref$ represent the reflected</description>
    </item>
    <item>
      <title>Hahn Banach Theorem for Real, Complex, Seminorm</title>
      <link>https://freshrimpsushi.github.io/en/posts/1230/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1230/</guid>
      <description>The Hahn-Banach Theorem for Real Numbers1 Let $X$ be a $\mathbb{R}$-vector space and assume that $Y \subset X$. Let us define $p : X \to \mathbb{ R}$ as a sublinear linear functional of $X$. Now, assume that $y^{\ast} : Y \to \mathbb{ R}$ satisfies the following condition as a $\mathbb{R}$-linear functional of $Y$. $$ y^{\ast}(y) \le p(y)\quad \forall y\in Y $$ Then, there exists a linear functional $x^{\ast} : X</description>
    </item>
    <item>
      <title>Semi Norm</title>
      <link>https://freshrimpsushi.github.io/en/posts/1229/</link>
      <pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1229/</guid>
      <description>Definition1 Let $X$ be a vector space. A function $\left\| \cdot \right\| : X \to \mathbb{R}$ is called a semi norm of $X$ if it satisfies the following three conditions: (a) $\left\| x \right\| \ge 0,\quad \forall\ x \in X$ (b) $|cx|=|c|\left\| x \right\|,\quad \forall\ x\in X,\ \forall\ c \in\mathbb{C}$ (c) $\left\| x + y \right\| \le \left\| x \right\| + \left\| y \right\|,\quad \forall\ x,y\in X$ Explanation The definition</description>
    </item>
    <item>
      <title>Semi-linear Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/1333/</link>
      <pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1333/</guid>
      <description>Definition1 If a function $f : X \to Y$ satisfies the following two conditions, it is called sublinear. For $x,x_{1},x_{2}\in X$ and $a \in \mathbb{R}$, $f(ax) = af(x)$ $f(x_{1} + x_{2}) \le f(x_{1}) + f(x_{2})$ Explanation If the second condition holds as an equality, it is linear, and if it holds as an inequality, it is sublinear. Robert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003),</description>
    </item>
    <item>
      <title>What is a Norm Space?</title>
      <link>https://freshrimpsushi.github.io/en/posts/1225/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1225/</guid>
      <description>Definition1 Let&amp;rsquo;s call $X$ a vector space. If there exists a function $\left\| \cdot \right\| : X \to \mathbb{R}$ that satisfies the following three conditions, then $\left\| \cdot \right\|$ is called the norm of $X$, and $(X,\left\| \cdot \right\| )$ is called a normed space. (a) $\left\| x \right\| \ge 0,\quad \forall\ x \in X$ and $\left\| x \right\|=0 \iff x = 0$ (b) $|cx|=|c|\left\| x \right\|,\quad \forall\ x\in X,\</description>
    </item>
    <item>
      <title>Embeddings in Mathematics, Insertion Mappings</title>
      <link>https://freshrimpsushi.github.io/en/posts/1214/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1214/</guid>
      <description>imbedding and embedding mean the same thing. Embedding is translated as insertion, embedding, incorporating, burying, etc. Definition1 Let $(X, \left\| \cdot \right\|_{X}), (Y, \left\| \cdot \right\|_{Y})$ be a normed space. If the following two conditions are satisfied for $X$ and $Y$, then $X$ is said to be embedded into $Y$, and $I : X \to Y$ is called the embedding. $X$ is a subspace of $Y$. For all $x \in</description>
    </item>
    <item>
      <title>Derivation of Bessel&#39;s Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1195/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1195/</guid>
      <description>Definition The differential equation below is called the $\nu$th order Bessel&amp;rsquo;s equation. $$ \begin{align*} x^2 y^{\prime \prime} +xy^{\prime} +(x^2-\nu^2)y =&amp;amp;\ 0 \\ x(xy^{\prime})^{\prime} + (x^2- \nu ^2) y =&amp;amp;\ 0 \\ y^{\prime \prime}+\frac{1}{x} y^{\prime} + \left( 1-\frac{\nu^{2}}{x^{2}} \right)y =&amp;amp;\ 0 \end{align*} $$ Description The solution to the Bessel&amp;rsquo;s equation is called the Bessel function. Bessel functions are often seen in physics, engineering, and more, especially in problems involving cylindrical symmetry.</description>
    </item>
    <item>
      <title>List of Special Symbols Available for Graphs in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1191/</link>
      <pubDate>Wed, 17 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1191/</guid>
      <description>Method When labeling graphs in MATLAB to indicate what each axis represents, you can use xlabel and ylabel. It&amp;rsquo;s also possible to use special symbols, bold, and italic styles. x=-3*pi:0.2:3* pi; y=sin(x-pi/6); plot(x,y); xlabel(&amp;#39;\beta&amp;#39;), ylabel(&amp;#39;\nabla f(x)&amp;#39;),; x=-3*pi:0.2:3* pi; y=sin(x-pi/6); plot(x,y); xlabel(&amp;#39;진폭{\bf Volt}&amp;#39;), ylabel(&amp;#39</description>
    </item>
    <item>
      <title>Lagrangian Mechanics and Hamiltons Variational Principle</title>
      <link>https://freshrimpsushi.github.io/en/posts/1182/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1182/</guid>
      <description>Overview Hamilton&amp;rsquo;s principle, functionals, action, and variation are explained here in a way that is as simple as possible. If you have not found a satisfactory explanation elsewhere, it is recommended to read through to the end. This has been written so that even freshmen and sophomores in college can understand it. Lagrangian Mechanics1 When an object moves from time $t_{1}$ to $t_{2}$, the integral of the Lagrangian over the</description>
    </item>
    <item>
      <title>Hopf-Lax Formula</title>
      <link>https://freshrimpsushi.github.io/en/posts/1174/</link>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1174/</guid>
      <description>Buildup1 Let&amp;rsquo;s consider the initial value problem of the Hamilton-Jacobi equation that depends only on $H$ as $Du$ for the Hamilton-Jacobi equation. $$ \begin{equation} \left\{ \begin{aligned} u_{t} + H(Du)&amp;amp;=0 &amp;amp;&amp;amp; \text{in } \mathbb{R}^n \times (0,\infty) \\ u&amp;amp;=g &amp;amp;&amp;amp; \text{on } \mathbb{R}^n \times \left\{ t=0 \right\} \end{aligned} \right. \label{eq1} \end{equation} $$ Generally, the Hamiltonian depends on the spatial variables as in the form of $H(Du, x)$, but let&amp;rsquo;s say here it</description>
    </item>
    <item>
      <title>Hamilton-Jacobi Equation and Hamiltonian Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1162/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1162/</guid>
      <description>There are two ways to derive the Hamilton equations. One is from the Euler-Lagrangian equations, and the other, which will be introduced in this article, is from the characteristic equations of the Hamilton-Jacobi equation. Definition1 The following partial differential equation is called the general Hamilton-Jacobi equation. $$ G(Du, u_{t}, u, x, t)=u_{t}+H(Du, x)=0 $$ $t &amp;gt;0 \in \mathbb{R}$ $x \in \mathbb{R}^{n}$ $u : \mathbb{R}^{n} \to \mathbb{R}$ Here, the differential operator</description>
    </item>
    <item>
      <title>Importing Excel Data into MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1163/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1163/</guid>
      <description>Method Matlab provides the functionality to import data from Excel files. First, click on &amp;lsquo;Import Data&amp;rsquo; from the Home menu. Select the Excel file that contains the data you want to import. Then, you can select which data to import, which is automatically selected initially. Confirm and click &amp;lsquo;Import Selected&amp;rsquo;. From &amp;lsquo;Import Selected&amp;rsquo;, click on &amp;lsquo;Import Data&amp;rsquo;. Then, the data from the Excel file is input into a variable with</description>
    </item>
    <item>
      <title>Lagrangians and Euler-Lagrange Equations in Partial Differential Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/1157/</link>
      <pubDate>Thu, 27 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1157/</guid>
      <description>Definition1 Lagrangian Let&amp;rsquo;s assume a smooth function $L : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ is given. This is called the Lagrangian and is denoted as follows. $$ L = L(v,x)=L(v_{1}, \dots, v_{n}, x_{1}, \dots, x_{n}) \quad v,x\in \mathbb{R}^{n} \\ D_{v}L = (L_{v_{1}}, \dots, L_{v_{n}}), \quad D_{x}L = (L_{x_{1}}, \dots, L_{x_{n}}) $$ The reason for using variables $v, x$ is because, in physics, each variable actually signifies velocity and position. Action,</description>
    </item>
    <item>
      <title>How to Save Data Calculated in MATLAB to an Excel File</title>
      <link>https://freshrimpsushi.github.io/en/posts/1150/</link>
      <pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1150/</guid>
      <description>Method When you want to organize the data calculated in MATLAB into Excel and the amount of data is not too much, you can manually copy and paste. However, for a matrix of data like 128*128 shown in the picture above, that method is not feasible. In this case, you can use xlswrite to save the data into an Excel file. Compared to the picture above, xlswrite(&#39;test&#39;, Y) has been</description>
    </item>
    <item>
      <title>How to Comment and Uncomment Multiple Lines at Once in MATLAB</title>
      <link>https://freshrimpsushi.github.io/en/posts/1149/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1149/</guid>
      <description>Method To comment out a section that you want, drag to select the area and then press Ctrl+R. This will comment out the entire selected portion. To undo, drag to select the same area and press Ctrl+T, which will remove the % from each line.</description>
    </item>
    <item>
      <title>Methods of Expressing an Arbitrary Function as Two Non-negative Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1145/</link>
      <pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1145/</guid>
      <description>Definitions1 Let&amp;rsquo;s define $f^{+}$ and $f^{-}$ for a function $f : X \to \mathbb{R}$ as follows. $$ \begin{align*} f^{+} (x) &amp;amp;:= \max \left\{ f(x),\ 0 \right\} \\ f^{-} (x) &amp;amp;:= \max \left\{ -f(x),\ 0 \right\} \end{align*} $$ We call $f^{+}$ the positive part of $f$, and $f^{-}$ the negative part of $f$. Description Despite their names, both $f^{+}$ and $f^{-}$ are non-negative functions. It might not be immediately clear why</description>
    </item>
    <item>
      <title>Predictable Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1135/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1135/</guid>
      <description>Definition1 Let’s call $(X, \mathcal{E})$ a measurable space. Let&amp;rsquo;s define the set $S_{f}(\alpha)$ as follows. $$ S_{f}(\alpha):=\left\{ x\in X\ |\ f(x) &amp;gt;\alpha \right\} = f^{-1}\left( (\alpha, \infty) \right),\quad \forall \alpha \in \mathbb{R} $$ If for every real number $\alpha \in \mathbb{R}$, $S_{f}(\alpha) \in \mathcal{E}$ holds, then the function $f : X \to \overline{\mathbb{R}}$ taking extended real values is called $\mathcal{E}$-measurable or simply measurable. Explanation Especially, if</description>
    </item>
    <item>
      <title>Fourier Inversion Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/1112/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1112/</guid>
      <description>Buildup The process of deriving the Fourier transform also derived the definition of the inverse transform. However, this was simply explained to aid understanding, and the transformation formula was not accurately derived. The Fourier inverse transformation is as follows: $$ \begin{equation} f(x) =\dfrac{1}{2\pi} \int \hat{f}(\xi) e^{i\xi x}d\xi \end{equation} $$ This equation implies that from $f$, we can obtain $\hat{f}$ and from $\hat{f}$, we can retrieve $f$ again. This might seem</description>
    </item>
    <item>
      <title>Properties of Fourier Transform</title>
      <link>https://freshrimpsushi.github.io/en/posts/1101/</link>
      <pubDate>Thu, 23 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1101/</guid>
      <description>Theorem[^1] Let&amp;rsquo;s consider $\cal{F}f, \hat{f}$ as the Fourier transform of $f$. Let $f \in L^{1}$. Then, the following properties hold for the Fourier transform: (a) For any real number $a$, $$ \mathcal{F} \left[ f(x-a) \right] ( \xi ) = e^{-ia\xi}\hat{f}(\xi) \quad \mathrm{and} \quad \mathcal{F} \left[ e^{iax}f(x)\right] (\xi) = \hat{f}(\xi-a) $$ (b) Define $f_\delta (x) := \frac{1}{\delta}f ( \frac{x}{\delta} )$ for $\delta &amp;gt;0$. Then, $$ \mathcal{F}\left[ f_\delta \right] (\xi ) =</description>
    </item>
    <item>
      <title>Additive and Multiplicative Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1096/</link>
      <pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1096/</guid>
      <description>Given a function $f : X \to Y$, let $a, b \in X$, $a_{i} \in X\ (i=1,\cdots)$. Subadditive Function A function $f$ is called a subadditive function when it satisfies the following equation: $$ f(a+b) \le f(a)+f(b) $$ The absolute value is an example. $$ |3+(-4)| \le |3|+|-4| $$ Another example, if we have $f(x)=2x+3$ then $$ 13=f(2+3) \le f(2)+f(3)=7+9=16 $$ Additive Function A function $f$ is called an additive</description>
    </item>
    <item>
      <title>Generalized Hölder&#39;s Inequality, Corollaries of Hölder&#39;s Inequality</title>
      <link>https://freshrimpsushi.github.io/en/posts/1091/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1091/</guid>
      <description>Description Let $\Omega \subset \mathbb{R}^{n}$ be called an open set. Suppose we are given two constants $1 \lt p \lt \infty, 1 \lt p^{\prime} \lt \infty$ that satisfy the following equation: $$ \dfrac{1}{p} + \dfrac{1}{p^{\prime}} = 1 \left(\text{or } p^{\prime} = \frac{p}{p-1} \right) $$ If $u \in L^p(\Omega)$, $v\in L^{p^{\prime}}(\Omega)$, then $uv \in L^1(\Omega)$ and the inequality below holds. $$ \| uv \|_{1} = \int_{\Omega} |u(x)v(x)| dx \le \| u</description>
    </item>
    <item>
      <title>Fourier Transform</title>
      <link>https://freshrimpsushi.github.io/en/posts/1086/</link>
      <pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1086/</guid>
      <description>Definition Fourier Transform as a Function The Fourier transform of function $f \in$ $L^{1}$ is defined as follows. $$ \hat{f}(\xi) := \int_{-\infty}^{\infty} f(t) e^{-i \xi t}dt $$ Fourier Transform as an Operator The operator $\mathcal{F} : L^{1} \to$ $C_{0}$ defined as follows is called the Fourier transform. $$ \mathcal{F}[f] (\xi) = \int_{-\infty}^{\infty} f(t) e^{-i \xi t}dt $$ Explanation As seen in the definition, the term Fourier transform refers to both</description>
    </item>
    <item>
      <title>Complete Orthonormal Basis and Complete Orthonormal Set</title>
      <link>https://freshrimpsushi.github.io/en/posts/1082/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1082/</guid>
      <description>Summary: Equivalence Conditions of an Orthonormal Set Let $\left\{ \phi_{n} \right\}_{1}^\infty$ be an orthonormal set of $L^2(a,b)$ and denote $f \in L^2(a,b)$. Then, the following conditions are equivalent. $(a)$ For all $n$, if $\left\langle f, \phi_{n} \right\rangle=0$ then $f=0$. $(b)$ For all $f\in L^2(a,b)$, the series $\sum_{1}^\infty \left\langle f,\phi_{n}\right\rangle\phi_{n}$ converges to $f$ in the norm sense. That is, the following equation holds: $$ f=\sum_{1}^\infty \left\langle f,\phi_{n}\right\rangle\phi_{n} $$ $(c)$ For all</description>
    </item>
    <item>
      <title>Delayed Potential on Continuous Distribution of Delay Times</title>
      <link>https://freshrimpsushi.github.io/en/posts/1075/</link>
      <pubDate>Thu, 09 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1075/</guid>
      <description>Overview1 The scalar and vector potentials for a moving point charge are called retarded potentials, and they are as follows. $$ \begin{align*} V(\mathbf{r},\ t) &amp;amp;= \dfrac{1}{4\pi\epsilon_{0}} \int \dfrac{ \rho (\mathbf{r}^{\prime},\ t_{r}) }{ \cR } d\tau^{\prime} \\[1em] \mathbf{A}( \mathbf{r},\ t) &amp;amp;= \dfrac{\mu_{0}}{4\pi} \int \dfrac{\mathbf{J}(\mathbf{r}^{\prime},\ t_{r})}{\cR}d\tau^{\prime} \end{align*} $$ Here, $t_{r}$ is the retarded time. Retarded Time If the charge and current distribution do not change over time, the scalar and vector potentials</description>
    </item>
    <item>
      <title>Solution of Nonlinear First Order PDE Using Characteristic Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/1074/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1074/</guid>
      <description>Explanation1 When emphasizing that x and p are variables of a partial differential equation, they are denoted in normal font as $x,p \in \mathbb{R}^{n}$, and when emphasizing them as functions of $s$, they are denoted in bold font as $\mathbf{x}, \mathbf{p} \in \mathbb{R}^{n}$. Characteristic Equations $$ \begin{cases} \dot{\mathbf{p}} (s) = -D_{x}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big)-D_{z}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big)\mathbf{p}(s) \\ \dot{z}(s) = D_{p}F\big(\mathbf{p}(s),\ z(s),\ \mathbf{x}(s) \big) \cdot \mathbf{p}(s) \\ \dot{\mathbf{x}}(s) = D_{p}F\big(\mathbf{p}(s),\</description>
    </item>
    <item>
      <title>Notation for Nonlinear First-Order Partial Differential Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/1071/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1071/</guid>
      <description>Notation1 A nonlinear first-order partial differential equation is denoted as follows. $$ \begin{equation} F(Du, u, x) = F(p, z, x) = 0 \label{eq1} \end{equation} $$ $\Omega \subset \mathbb{R}^{n}$ is an open set $x\in \Omega$ $F : \mathbb{R}^n \times \mathbb{R}^n \times \bar{ \Omega } \to \mathbb{R}$ is the given function $u : \bar{ \Omega } \to \mathbb{R}$ is the variable of $F$ Description Solving a nonlinear first-order partial differential equation $F$</description>
    </item>
    <item>
      <title>Multi Index Notation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1062/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1062/</guid>
      <description>Definition[^1] A multi-index with order $|\alpha|$ is a tuple $\alpha=(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n})$ whose components are non-negative integers. Here, $| \alpha|$ is defined as follows. $$ |\alpha| = \sum _{i}^{n} \alpha_{i} = \alpha_{1} + \cdots + \alpha_{n} $$ Notation For $x = (x_{1}, x_{2}, \dots, x_{n}) \in \mathbb{R}^{n}$, $x^{\alpha}$ is defined as follows. $$ x^{\alpha} := x_{1}^{\alpha_{1}} x_{2}^{\alpha_{2}} \cdots x_{n}^{\alpha_{n}} $$ The multi-index is often used to represent partial derivatives</description>
    </item>
    <item>
      <title>Sine Waves and Complex Wave Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/1066/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1066/</guid>
      <description>Definitions A wave expressed as a sine function is called a sine wave. Description The general form of a sine wave is as follows. The reason why it&amp;rsquo;s referred to as a sine wave even though the equation is $\cos$ is explained below, as the real part of the complex wave function is $\cos$. $\sin$ is the imaginary part. $$ f(x,t) = A \cos \big( k(x-vt)+\delta \big) $$ Here, $A$</description>
    </item>
    <item>
      <title>In Physics, What is a Tensor</title>
      <link>https://freshrimpsushi.github.io/en/posts/1040/</link>
      <pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1040/</guid>
      <description>Overview Without a doubt, this is the easiest explanation about tensors, so if you&amp;rsquo;re an undergraduate in physics who came here because you don&amp;rsquo;t know what a tensor is, I highly recommend reading this. We&amp;rsquo;re not accepting corrections about mathematical inaccuracies. Teaching someone who hasn&amp;rsquo;t learned about negative numbers that &amp;lsquo;you can&amp;rsquo;t subtract a larger number from a smaller number&amp;rsquo;, or someone who hasn&amp;rsquo;t learned about complex numbers that &amp;lsquo;you</description>
    </item>
    <item>
      <title>Maxwell&#39;s Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/1038/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1038/</guid>
      <description>Formulas Maxwell&amp;rsquo;s Equations $(\text{i}) \quad \nabla \cdot \mathbf{E}=\dfrac{1}{\epsilon_{0}}\rho$ (Gauss&amp;rsquo;s Law) $(\text{ii}) \quad \nabla \cdot \mathbf{B}=0$ (Gauss&amp;rsquo;s Law for Magnetism) $(\text{iii}) \quad \nabla \times \mathbf{E} = -\dfrac{\partial \mathbf{B}}{\partial t}$ (Faraday&amp;rsquo;s Law) $(\text{iv}) \quad \nabla \times \mathbf{B} = \mu_{0} \mathbf{J}+\mu_{0}\epsilon_{0}\dfrac{\partial \mathbf{E}}{\partial t}$ (Ampère&amp;rsquo;s Law) Description1 Before Maxwell completed the Maxwell&amp;rsquo;s equations, the four equations concerning the electric field</description>
    </item>
    <item>
      <title>Heat Equation, Diffusion Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/1001/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1001/</guid>
      <description>Definition1 2 The following partial differential equation is referred to as the heat equation or the diffusion equation. $$ \dfrac{\partial u}{\partial t} = \dfrac{\partial^{2} u}{\partial x^{2}} $$ When the spatial coordinate is $n$-dimensional, $$ \dfrac{\partial u}{\partial t} = \Delta u = \nabla^{2}u $$ here, $\Delta = \nabla^{2} = \sum\limits_{i=1}^{n} \dfrac{\partial^{2} }{\partial x_{i}^{2}}$ refers to the Laplacian. When there is an external force $f = f(x,t)$, $$ \dfrac{\partial u}{\partial t} =</description>
    </item>
    <item>
      <title>Definition of Convolution</title>
      <link>https://freshrimpsushi.github.io/en/posts/1000/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/1000/</guid>
      <description>Definition Let&amp;rsquo;s assume that two functions $f$ and $g$ defined in $\mathbb{R}$ are given. If the integral below exists, it is called the convolution of the two functions $f$ and $g$, and is denoted by $f \ast g$. $$ f \ast g(x):=\int _{-\infty} ^{\infty} f(y)g(x-y)dy $$ If $f$ and $g$ are discrete functions, they are defined as follows. $$ (f \ast g)(m)=\sum \limits_{n}f(n)g(m-n) $$ Explanation Although there is a translation</description>
    </item>
    <item>
      <title>Laplace&#39;s Equation and Poisson&#39;s Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/997/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/997/</guid>
      <description>Definition1 $\ U \in \mathbb{R}^n$ is an open set $\ x\in U$ $u=u(x) : \overline{U} \rightarrow \mathbb{R}^n$ Laplace&amp;rsquo;s Equation The partial differential equation below is called Laplace&amp;rsquo;s equation. $$ \Delta u=0 $$ Here, $\Delta$ is the Laplacian. A $u$ that satisfies Laplace&amp;rsquo;s equation is specifically called a harmonic function. Poisson&amp;rsquo;s Equation The nonhomogeneous Laplace&amp;rsquo;s equation is called Poisson&amp;rsquo;s equation. $$ -\Delta u = f $$ Explanation Laplace&amp;rsquo;s equation appears in</description>
    </item>
    <item>
      <title>Exterior Unit Normal Vector</title>
      <link>https://freshrimpsushi.github.io/en/posts/988/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/988/</guid>
      <description>Definition1 Let $U\subset \mathbb{R}^{n}$ be an open set. Let the boundary of $U$ be $\partial U$, which is a $\partial U \in C^1$. Then, the following outward unit normal vector can be defined: $$ \boldsymbol{\nu}=(\nu^{1}, \nu^{2}, \dots, \nu^{n}) \quad \text{and} \quad |\boldsymbol{\nu}|=1 $$ $\boldsymbol{\nu}$ is a vector that touches a point on the boundary, has a magnitude of 1, and points outward. Let it be $u \in C^{1}(\bar{U})$. Then, the</description>
    </item>
    <item>
      <title>Green&#39;s Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/974/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/974/</guid>
      <description>Theorem Let&amp;rsquo;s assume $u, v \in C^2( \bar{U})$. Then, the following expressions hold: (i) $\displaystyle \int_{U} \Delta u dx=\int_{\partial U} \dfrac{\partial u}{\partial \nu}dS$ (ii) $\displaystyle \int_{U} Dv \cdot Du dx = -\int_{U} u \Delta v dx+\int_{\partial U}\dfrac{\partial v}{\partial \nu}udS$ (iii) $\displaystyle \int_{U} (u\Delta v - v\Delta u )dx = \int_{\partial U} \left( \dfrac{\partial v}{\partial \nu}u - \dfrac{\partial u}{\partial \nu} v\right)dS$ These are collectively referred to as Green&amp;rsquo;s formula. $\Delta$ is</description>
    </item>
    <item>
      <title>Mean of Function Values</title>
      <link>https://freshrimpsushi.github.io/en/posts/983/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/983/</guid>
      <description>Definition The average value of a function between $[a,\ b]$ and $f(x)$ is equivalent to dividing the integral of the function over the interval by the length of the interval. $$ \dfrac{1}{b-a}\int_{a}^bf(x)dx $$ Derivation Let&amp;rsquo;s denote a partition of the interval $[a,\ b]$ as $P$. $$ P=\left\{ x_{1},\ x_{2},\ \cdots ,\ x_{n} \right\} $$ In this case, $a=x_{1} &amp;lt; x_{2} &amp;lt; \cdots &amp;lt; x_{n}=b$ and the distance between each point</description>
    </item>
    <item>
      <title>Continuity in Every Piece, Smoothness in Every Segment</title>
      <link>https://freshrimpsushi.github.io/en/posts/972/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/972/</guid>
      <description>Definition A function $f$ is said to be piecewise continuous on an interval $I$ if it satisfies the conditions below: It has a finite number of discontinuities $x_{1},\ x_{2},\ \cdots ,\ x_{n} \in I$. At each point of discontinuity, it has both a left-hand limit and a right-hand limit. $$ \left|\lim \limits_{x\rightarrow x_{i}^{+}} f(x) \right| &amp;lt; \infty \quad \text{and} \quad \left|\lim_{x \rightarrow x_{i}^{-}}f(x)\right|&amp;lt;\infty \quad (i=1,\ \cdots ,\ n) $$ If</description>
    </item>
    <item>
      <title>Partial Integration of Expressions Containing the Del Operator</title>
      <link>https://freshrimpsushi.github.io/en/posts/959/</link>
      <pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/959/</guid>
      <description>Formulas The following expressions hold true for vector integration involving the del operator. (a) $$ \int_{\mathcal{V}}\mathbf{A} \cdot (\nabla f)d\tau = \oint_{\mathcal{S}}f\mathbf{A} \cdot d \mathbf{a}-\int_{\mathcal{V}}f(\nabla \cdot \mathbf{A})d\tau $$ (b) $$ \int_{\mathcal{S}} f \left( \nabla \times \mathbf{A} \right)\mathbf{A} \cdot d \mathbf{a} = \int_{\mathcal{S}} \left[ \mathbf{A} \times \left( \nabla f \right) \right] \cdot d\mathbf{a} + \oint_{\mathcal{P}} f\mathbf{A} \cdot d\mathbf{l} $$ (c) $$ \int_{\mathcal{V}} \mathbf{B} \cdot \left( \nabla \times \mathbf{A} \right) d\tau = \int_{\mathcal{V}}</description>
    </item>
    <item>
      <title>Series Solution of Chebyshev Differential Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/955/</link>
      <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/955/</guid>
      <description>Definition The following differential equation is referred to as the Chebyshev Differential Equation: $$ (1-x^2)\dfrac{d^2 y}{dx^2} -x\dfrac{dy}{dx}+n^2 y=0 $$ Description It&amp;rsquo;s a form that includes the independent variable $x$ in the coefficient, and assuming that the solution is in the form of a power series, it can be solved. The solution to the Chebyshev equation is called the Chebyshev polynomial, often denoted as $T_{n}(x)$. Solution $$ \begin{equation} (1-x^2)y^{\prime \prime} -xy^{\prime}+\lambda^2</description>
    </item>
    <item>
      <title>Probabilistic Interpretation and Normalization of Wave Functions in Quantum Mechanics</title>
      <link>https://freshrimpsushi.github.io/en/posts/945/</link>
      <pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/945/</guid>
      <description>Wave Function The wave function is a function in quantum mechanics that represents the motion state of a particle depending on time and position. It is usually denoted by $u$, $\psi$, $\Psi$. In the context of Shrimp Sushi Restaurant, the wave function regarding position and time is denoted by $\psi (x,t)$, and the wave function regarding position, independent of time, is denoted by $u(x)$. Probabilistic Interpretation The method of understanding</description>
    </item>
    <item>
      <title>Current and Current Density</title>
      <link>https://freshrimpsushi.github.io/en/posts/898/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/898/</guid>
      <description>Definition1 The electric current is defined as the amount of charge that passes through a given point in a conductor per unit of time, denoted by $I$. Thus, a negative charge moving to the left and a positive charge moving to the right constitute an electric current of the same sign. The amount of Coulomb passing per unit of time is called ampere. $$ 1 [A] = 1 [C/s] $$</description>
    </item>
    <item>
      <title>Magnetic Vector Potential</title>
      <link>https://freshrimpsushi.github.io/en/posts/923/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/923/</guid>
      <description>Explanation1 In electrostatics, the electric field is easily handled by using a property called $\nabla \times \mathbf{E} = \mathbf{0}$ to define the scalar potential $V$. Similarly, in magnetostatics, the vector potential $A$ is defined and used by utilizing a property called $\nabla \cdot \mathbf{B} = 0$. Let&amp;rsquo;s say the magnetic field $\mathbf{B}$ is the curl of some vector $\mathbf{A}$. $$ \mathbf{B}=\nabla \times \mathbf{A} $$ Since the divergence of a curl</description>
    </item>
    <item>
      <title>Magnetism and the Law of Lorentz Force</title>
      <link>https://freshrimpsushi.github.io/en/posts/896/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/896/</guid>
      <description>Definition1 A moving charge (current) creates a magnetic field $\mathbf{B}$ around it. The force experienced by a charge $Q$ moving at velocity $\mathbf{v}$ in a magnetic field $\mathbf{B}$ is given by: $$ \begin{equation} \mathbf{F}_{m}=Q(\mathbf{v} \times \mathbf{B}) \end{equation} $$ This force is called the magnetic force, and the above formula is known as the Lorentz force law. Explanation As with the definition of an electric field, when a moving charge experiences</description>
    </item>
    <item>
      <title>Steady Current and Biot-Savart Law</title>
      <link>https://freshrimpsushi.github.io/en/posts/899/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/899/</guid>
      <description>Definition1 A steady current refers to the flow of charge that continues without changing in amount or direction. Description Since the current does not change over time, the magnetic field created by the steady current also does not change over time. The &amp;lsquo;direction of progress&amp;rsquo; mentioned here is a different concept from the direction of a vector we commonly think of. It means that as long as the flow continues</description>
    </item>
    <item>
      <title>Stokes&#39; Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/937/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/937/</guid>
      <description>Theorem1 Let&amp;rsquo;s call something a vector and an area in 3D space as $\mathbf{v}, \mathcal{S}$, respectively. The area vector of $\mathcal{S}$ is denoted as $d\mathbf{a}$, the border of $\mathcal{S}$ as $\mathcal{P}$, and the path moving along $\mathcal{P}$ as $d\mathbf{l}$. Then, the following equation holds: $$ \int_{\mathcal{S}} (\nabla \times \mathbf{v} )\cdot d\mathbf{a} = \oint_{\mathcal{P}} \mathbf{v} \cdot d\mathbf{l} $$ This is called Stokes&amp;rsquo; theorem or the fundamental theorem for curl. Incidentally, outside</description>
    </item>
    <item>
      <title>Multipole Expansion of Potential and Dipole Moments</title>
      <link>https://freshrimpsushi.github.io/en/posts/936/</link>
      <pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/936/</guid>
      <description>Multiple Expansion When a distribution of charges is viewed from sufficiently far away, it appears almost as though it were a point charge. In other words, if the total charge of the charge distribution is $Q$, it would feel as if there&amp;rsquo;s a single point charge with charge $Q$ when viewed from afar. This means that the potential can be approximated as $\dfrac{1}{4\pi\epsilon_{0}} \dfrac{Q}{r}$. But if the total charge is</description>
    </item>
    <item>
      <title>Derivation of Fourier Series</title>
      <link>https://freshrimpsushi.github.io/en/posts/929/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/929/</guid>
      <description>Definition The series for $2L$-periodic function $f$ is defined as the Fourier series of $f$ as follows: $$ \begin{align*} \lim \limits_{N \rightarrow \infty} S^{f}_{N}(t) &amp;amp;= \lim \limits_{N \to \infty}\left[ \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{N} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t}{L} \right) \right] \\ &amp;amp;= \dfrac{a_{0}}{2}+\sum \limits_{n=1}^{\infty} \left( a_{n} \cos \dfrac{n\pi t}{L} + b_{n}\sin\dfrac{n\pi t}{L} \right) \end{align*} $$ Here, each coefficient $a_{0}, a_{n}, b_{n}$ is called the Fourier coefficient, and its value</description>
    </item>
    <item>
      <title>Dirichlet Kernel</title>
      <link>https://freshrimpsushi.github.io/en/posts/932/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/932/</guid>
      <description>Definition Dirichlet Kernel $D_{n}$ is defined as follows. $$ \begin{equation} D_{n}(t) := \dfrac{1}{2}+\sum \limits_{k=1}^{n} \cos kt \end{equation} $$ Explanation The Dirichlet kernel is related to delta functions, exponential functions, etc., and appears in Fourier analysis. Here are some related theorems and their proofs. Theorem 1 The Dirichlet Kernel satisfies the following equation. $$ D_{n}(t)=\dfrac{\sin\left(n+\frac{1}{2}\right) t}{2\sin \frac{1}{2}t} $$ Proof If we express the cosine function as a complex exponential form, we</description>
    </item>
    <item>
      <title>Orthogonal Functions and Orthogonal Sets: Normalized Orthogonal Sets and Norms of Functions</title>
      <link>https://freshrimpsushi.github.io/en/posts/926/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/926/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 ** **Inner Product The inner product of two complex functions $f$ and $g$ defined in the interval $[a,b]$ is defined as follows. $$ \left\langlef, g\right\rangle:=\int_{a}^b f(x) \overline{g(x)} dx $$ Therefore, the inner product of the same two functions is $$ \left\langle f,f \right\rangle=\int_{a}^b f(x) \overline{f(x)} dx =</description>
    </item>
    <item>
      <title>Rodrigues Formula for Legendre Polynomial</title>
      <link>https://freshrimpsushi.github.io/en/posts/895/</link>
      <pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/895/</guid>
      <description>Formula The explicit formula for the Legendre polynomials is as follows. $$ P_{l}(x)=\dfrac{1}{2^{l} l!} \dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \tag{1} $$ Description This formula is used to obtain the $l$th Legendre polynomial, known as the Rodrigues&amp;rsquo; formula. Originally, it referred to the explicit form of the Legendre polynomials, but later it became a universal name for formulas representing the explicit form of special functions expressed as polynomials. Derivation The Legendre polynomial $P_{l}$ refers to</description>
    </item>
    <item>
      <title>Series Solution of Legendre Differential Equation: Legendre Polynomial</title>
      <link>https://freshrimpsushi.github.io/en/posts/889/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/889/</guid>
      <description>Definition1 The following differential equation is called the Legendre differential equation. $$ (1-x^2)\dfrac{d^2 y}{dx^2} -2x\dfrac{dy}{dx}+l(l+1) y=0 $$ The solution to the Legendre differential equation is called the Legendre polynomial, commonly denoted as $P_{l}(x)$. The first few Legendre polynomials according to $l$ are as follows. $$ \begin{align*} P_{0}(x) =&amp;amp;\ 1 \\ P_{1}(x) =&amp;amp;\ x \\ P_2(x) =&amp;amp;\ \dfrac{1}{2}(3x^2-1) \\ P_{3}(x) =&amp;amp;\ \dfrac{1}{2}(5x^3-3x) \\ P_{4}(x) =&amp;amp;\ \dfrac{1}{8}(35x^4-30x^2+3) \\ P_{5}(x) =&amp;amp;\ \dfrac{1}{8}(63x^5-70x^3+15x) \\</description>
    </item>
    <item>
      <title>Proof of Leibniz&#39;s Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/884/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/884/</guid>
      <description>Summary $$ \dfrac{d}{dx} (fg)=\dfrac{df}{dx}g+f\dfrac{dg}{dx} $$ $$ \begin{align*} \dfrac{d^n}{dx^n}(fg)&amp;amp;=\sum \limits_{k=0}^{n}\frac{n!}{(n-k)!k!}\dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n}{}_{n}\mathrm{C}_{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \\ &amp;amp;=\sum \limits_{k=0}^{n} \binom{n}{k} \dfrac{d^{n-k}f}{dx^{n-k}}\dfrac{d^k g}{dx^k} \end{align*} $$ Description Also known as Leibniz&amp;rsquo;s rule. The first equation is a well-known formula, often referred to as the product rule or the rule of product for differentiation. It simply expresses the result when the product of two functions is differentiated once. More generally, the equation below represents</description>
    </item>
    <item>
      <title>Series, Infinite Series</title>
      <link>https://freshrimpsushi.github.io/en/posts/886/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/886/</guid>
      <description>Definition1 Let&amp;rsquo;s assume a sequence $\left\{ a_{n} \right\}$ is given. Then, let&amp;rsquo;s define the following notation. $$ \sum \limits_{n=p}^{q} a_{n} = a_{p} + a_{p+1} + \cdots + a_{q}\quad (p \le q) $$ Define the partial sum $s_{n}$ of $\left\{ a_{n} \right\}$ as follows. $$ s_{n} = \sum \limits_{k=1}^{n} a_{k} $$ Then, we can think of a sequence $\left\{ s_{n} \right\}$ of these $s_{n}$. The limit of sequence $\left\{ s_{n} \right\}$</description>
    </item>
    <item>
      <title>Continuous Functions are Riemann-Stieltjes Integrable</title>
      <link>https://freshrimpsushi.github.io/en/posts/847/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/847/</guid>
      <description>= This article is based on the Riemann-Stieltjes integral. If set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Theorem If function $f$ is continuous on $[a,b]$, then it is Riemann(-Stieltjes) integrable on $[a,b]$. Proof Suppose $\epsilon &amp;gt;0$ is given. And let&amp;rsquo;s say we chose $\eta&amp;gt;0$ that satisfies $\left[ \alpha (b) - \alpha (a) \right] \eta &amp;lt; \epsilon$. Since $[a,b]$ is compact as it is closed and</description>
    </item>
    <item>
      <title>Potential</title>
      <link>https://freshrimpsushi.github.io/en/posts/845/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/845/</guid>
      <description>Explanation1 The electric field is a special vector function that always has a curl (rotation) of $\mathbf{0}$. From this characteristic, we introduce a scalar function called electric potential. The potential is denoted as $V$ and has the following relationship with the electric field $\mathbf{E}$. $$ \mathbf{E} = -\nabla V $$ Therefore, if we know the potential $V$, we can know the electric field $\mathbf{E}$. Since the potential is a scalar</description>
    </item>
    <item>
      <title>Electric Field Curl</title>
      <link>https://freshrimpsushi.github.io/en/posts/844/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/844/</guid>
      <description>정리 Electric Field&amp;rsquo;s Curl is always $\mathbf{0}$. $$ \nabla \times \mathbf{E} = \mathbf{0} $$ Proof1 We will derive a general result from the special case where a point charge is located at the origin. The electric field due to a point charge at a distance of $r$ from the origin is as follows. $$ \mathbf{E}=\dfrac{1}{4 \pi \epsilon_{0} } \dfrac{q}{r^2} \hat{\mathbf{r}} $$ If we perform a path integral of the</description>
    </item>
    <item>
      <title>Electric Flux and Gauss&#39;s Law</title>
      <link>https://freshrimpsushi.github.io/en/posts/635/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/635/</guid>
      <description>Definition1 The flux of an electric field $\mathbf{E}$ passing through a surface $\mathcal S$ is defined as follows. $$ \Phi_{E} \equiv \int_{\mathcal S} \mathbf{E} \cdot d\mathbf{a} $$ Let&amp;rsquo;s consider $\mathcal{S}$ as some closed surface. Let the total charge inside the closed surface be $Q_{\text{in}}$. Then, the following equation holds. $$ \oint_{\mathcal{S}} \mathbf{E} \cdot d\mathbf{a} = \frac{1}{\epsilon_{0}}Q_{\mathrm{in}} $$ This is known as Gauss&amp;rsquo;s law. Flux Flux, or flux density, refers to</description>
    </item>
    <item>
      <title>Coulomb&#39;s Law and Electric Fields</title>
      <link>https://freshrimpsushi.github.io/en/posts/836/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/836/</guid>
      <description>Coulomb&amp;rsquo;s Law1 The force exerted on a test charge $Q$ placed at a distance $\cR$ away from a fixed point charge $q$ is known as the Coulomb force, and its equation is as follows. $$ \mathbf{F} = \dfrac{1}{4\pi \epsilon_{0}} \dfrac{qQ}{\cR ^2} \crH $$ This is referred to as Coulomb&amp;rsquo;s Law. Description Coulomb’s Law is an empirical law derived from repeated experiments. Therefore,</description>
    </item>
    <item>
      <title>Necessary and Sufficient Conditions for Riemann(-Stieltjes) Integrability</title>
      <link>https://freshrimpsushi.github.io/en/posts/833/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/833/</guid>
      <description>This article is based on the Riemann-Stieltjes integral. If we set $\alpha=\alpha (x)=x$, it is the same as Riemann integral. Theorem1 A necessary and sufficient condition for a function $f$ to be Riemann(-Stieltjes) integrable on $[a,b]$ is that for every $\epsilon &amp;gt;0$, there exists a partition $P$ of $[a,b]$ that satisfies $U(P,f,\alpha) - L(P,f,\alpha) &amp;lt; \epsilon$. $$ \begin{equation} f \in \mathscr{R} (\alpha) \text{ on } [a,b] \\ \iff \forall\epsilon &amp;gt;0,</description>
    </item>
    <item>
      <title>Segmentation</title>
      <link>https://freshrimpsushi.github.io/en/posts/830/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/830/</guid>
      <description>This post is based on the Riemann-Stieltjes integral. If we set as $\alpha=\alpha (x)=x$, it is the same as the Riemann integral. Definition If $P^{\ast}$ and $P$ are partitions of $[a,b]$ and satisfy $P \subseteq P^{\ast}$, then $P^{\ast}$ is called a refinement of $P$. Hence, every point in $P$ is a point in $P^{\ast}$. For any two partitions $P_{1}$ and $P_{2}$, $P_{3}=P_{1} \cup P_{2}$ is called the common refinement of</description>
    </item>
    <item>
      <title>Riemann-Stieltjes Integral</title>
      <link>https://freshrimpsushi.github.io/en/posts/829/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/829/</guid>
      <description>Overview The Riemann-Stieltjes integral is a generalization of the Riemann integral, sometimes simply referred to as Stieltjes integral. The Riemann integral is a special case of the Riemann-Stieltjes integral where $\alpha (x)=x$. The process of defining the Riemann-Stieltjes integral is the same as the process of defining the Riemann integral, so details on the notation and buildup are omitted here. Definition Let $\alpha : [a,b] \to \mathbb{R}$ be a monotonically</description>
    </item>
    <item>
      <title>Partition, Riemann Sum, Riemann Integral</title>
      <link>https://freshrimpsushi.github.io/en/posts/828/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/828/</guid>
      <description>Partition1 Let&amp;rsquo;s assume the interval $[a,b]$ is given. The partition $P$ of $[a,b]$ is defined as follows. $$ P := \left\{ x_{0},\ x_{1},\ \cdots, x_{n}\right\},\quad a=x_{0} &amp;lt;x_{1}&amp;lt;\cdots &amp;lt; x_{n} =b $$ And $\Delta x_{i}$ is defined as follows. $$ \Delta x_{i} :=x_{i}-x_{i-1},\quad i=1,2,\cdots,n $$ Explanation Simply put, a partition is a set that contains all points at the ends of an interval and all boundary points within the interval when</description>
    </item>
    <item>
      <title>Laplace Transform Translation</title>
      <link>https://freshrimpsushi.github.io/en/posts/764/</link>
      <pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/764/</guid>
      <description>Formula1 Assuming the Laplace transform $F(s)=\mathcal{L} \left\{ f(t) \right\}$ of the function $f(t)$ exists as $s&amp;gt;a$. Then, the following holds for constant $c$. $$ \begin{align*} \mathcal{L} \left\{ e^{ct}f(t) \right\}&amp;amp;=F(s-c), &amp;amp;s&amp;gt;a+c \\ \mathcal{L^{-1}} \left\{ F(s-c) \right\}&amp;amp;=e^{ct}f(t) &amp;amp; \end{align*} $$ Explanation This means that multiplying an exponential function to $f$ is equivalent to translating $F$. Derivation $$ \begin{align*} \mathcal{L} \left\{ e^{ct}f(t) \right\} &amp;amp;=\int_{0}^\infty e^{-st}e^{ct}f(t)dt \\ &amp;amp;= \int_{0}^\infty e^{-(s-c)t}f(t)dt \\ &amp;amp;= F(s-c) \end{align*}</description>
    </item>
    <item>
      <title>Definition and Existence Proof of the Laplace Transform</title>
      <link>https://freshrimpsushi.github.io/en/posts/761/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/761/</guid>
      <description>Definition[^1] The Laplace transform of a function $f$ is defined as follows. $$ \mathcal{L} \left\{ f(t) \right\} := \int _{0}^\infty e^{-st}f(t) dt =F(s) $$</description>
    </item>
    <item>
      <title>Staircase Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/757/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/757/</guid>
      <description>Definition A function that is a piecewise constant function is called a step function. Description As shown in the figure above, it looks like a staircase, hence the name step function. It is also known as the Heaviside function, named after Heaviside, who is known to be the first to propose it. Heaviside was the person who created a method for solving differential equations in electrical circuits, which is the</description>
    </item>
    <item>
      <title>Laplace Transform Table</title>
      <link>https://freshrimpsushi.github.io/en/posts/743/</link>
      <pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/743/</guid>
      <description>Formula1 This is table of Laplace transform. $f(t)=\mathcal{L^{-1}}$ $F(s)=\mathcal{L} \left\{ f(t) \right\}$ Derivation $1$ $\dfrac{1}{s}$ link $e^{at}$ $\dfrac{1}{s-a}$ link $t^n$ $\dfrac{n!}{s^{n+1}}$ link $t^{p}$ $\dfrac{ \Gamma (p+1) }{ s^{p+1}}$ link $t^{p}e^{at}$ $\dfrac{ \Gamma (p+1) }{ (s-a)^{p+1}}$ link $\sin (at)$ $\dfrac{a}{s^2+a^2}$ link $\cos (at)$ $\dfrac{s}{s^2+a^2}$ link $e^{at}\sin(bt)$ $\dfrac{b}{(s-a)^2 +b^2}$ link $e^{at}\cos(bt)$ $\dfrac{s-a}{(s-a)^2+b^2}$ link $\sinh (at)$ $\dfrac{a}{s^2-a^2}$ link $\cosh (at)$ $\dfrac{s}{s^2-a^2}$ link $e^{at} \sinh (bt)$ $\dfrac{b}{(s-a)^2-b^2}$ link $e^{at} \cosh (bt)$ $\dfrac{s-a}{(s-a)^2-b^2}$ link $u_{c}(t)=</description>
    </item>
    <item>
      <title>Rings in Abstract Algebra</title>
      <link>https://freshrimpsushi.github.io/en/posts/587/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/587/</guid>
      <description>Definition 1 A set $R$ satisfying the following rules for two binary operations, addition$+$ and multiplication$\cdot$, is defined as a Ring. When $a$, $b$, $c$ are elements of $R$, Commutative law holds for addition. $$a+b=b+a$$ Associative law holds for addition. $$(a+b)+c=a+(b+c)$$ There exists an identity element for addition. $$\forall a \ \exists 0\ \ \mathrm{s.t} \ a+0=a$$ There exists an additive inverse for every element. $$\forall a \ \exists -a\</description>
    </item>
    <item>
      <title>Definition and Test Method of Subgroups</title>
      <link>https://freshrimpsushi.github.io/en/posts/589/</link>
      <pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/589/</guid>
      <description>Definition 1 A subset $H$ of a group $G$ is called a subgroup of $G$ if $H$ itself is a group under the operation of $G$. Theorem Subgroup Test: For a non-empty subset $H$ of a group $G$, if for every element $a,\ b$ in $H$, $ab^{-1}$ is also an element of $H$, then $H$ is a subgroup of $G$. In other words, if whenever $a,\ b$ is in $H$,</description>
    </item>
    <item>
      <title>Gauss&#39;s Theorem, Divergence Theorem</title>
      <link>https://freshrimpsushi.github.io/en/posts/565/</link>
      <pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/565/</guid>
      <description>Theorem1 The following holds for a 3-dimensional vector function $\mathbf{F}$: $$ \begin{equation} \int_{\mathcal{V}} \nabla \cdot \mathbf{F} dV = \oint_{\mathcal{S}} \mathbf{F} \cdot d \mathbf{S} \label{1} \end{equation} $$ Here, $\nabla \cdot \mathbf{F}$ is divergence, $\int_{\mathcal{V}}$ is volume integration, and $\oint_{\mathcal{S}}$ is closed surface integration. Description This is called Gauss&amp;rsquo;s theorem, Green&amp;rsquo;s theorem, or divergence theorem. The divergence theorem is especially used in electromagnetics. Mathematical Meaning Mathematically, it means that a surface integral</description>
    </item>
    <item>
      <title>Solution to Second Order Homogeneous Differential Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/544/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/544/</guid>
      <description>Summary1 $$ ay^{\prime \prime} + by^\prime + cy=0 $$ Let&amp;rsquo;s say the solutions to the characteristic equation $ar^2+br+c=0$ given above are $r_{1}$ and $r_2$. Then, $\text{1.}$ If $r_{1}$ and $r_2$ are two distinct real numbers$(b^2-4ac&amp;gt;0)$, the general solution is as follows: $$ y(t)=c_{1}e^{r_{1}t}+c_2e^{r_2t} $$ $\text{2.}$ If $r_{1}$ and $r_2$ are complex conjugates $\lambda \pm i \mu$$(b^2-4ac&amp;lt;0)$, the general solution is as follows: $$ \begin{align*} y(t) &amp;amp;= c_{1}e^{(\lambda + i\mu)t} +</description>
    </item>
    <item>
      <title>Restoring Force and One-Dimensional Simple Harmonic Oscillator</title>
      <link>https://freshrimpsushi.github.io/en/posts/543/</link>
      <pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/543/</guid>
      <description>Simple Harmonic Motion1 Let&amp;rsquo;s consider the motion of an object hanging on a spring. It oscillates back and forth due to the restoring force of the spring. Such motion is called a harmonic oscillation. The functions representing harmonic oscillation, $\sin$ and $\cos$, were called harmonic functions a long time ago, which is why the motion is designated as such. Among harmonic oscillations, those with no friction or other external forces,</description>
    </item>
    <item>
      <title>Second-Order Linear Homogeneous Differential Equations with Constant Coefficients and Characteristic Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/540/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/540/</guid>
      <description>Summary1 The general solution of a second-order linear homogeneous differential equation with constant coefficients $a y^{\prime \prime} + by^\prime +cy=0$ is as follows. $$ y(x)=A e^{r_{1} x}+Be^{r_2 x} $$ At this time, $r_{1,2}=\dfrac{-b \pm \sqrt{b^2-4ac}} {2a}$ Corollary The solution of $a y^{\prime \prime} + cy = 0$ is as follows. $$ y(x) = A e^{i\sqrt{\frac{c}{a}} x}+Be^{-i\sqrt{\frac{c}{a}} x} = C\cos{\textstyle (\sqrt{\frac{c}{a}}x)} + D\sin{\textstyle (\sqrt{\frac{c}{a}}x)} $$ Solution $$ \begin{equation} a\dfrac{d^2}{dx^2}y+b\dfrac{d}{dx}y+cy = 0</description>
    </item>
    <item>
      <title>Definition and Discrimination Method of an Exact Differential Equation</title>
      <link>https://freshrimpsushi.github.io/en/posts/516/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/516/</guid>
      <description>Definition The given differential equation $\psi=\psi (x,y)$ is said to be an exact differential equation if there exists $\psi=\psi (x,y)$ that satisfies $\psi (x,y)$. Explanation If the given differential equation is exact, it can be represented as a total differential with respect to $\psi (x,y)$. $d\psi (x,y)=\dfrac{\partial \psi }{\partial x}dx + \dfrac{\partial \psi }{\partial y}dy$ Since $d\psi (x,y)=\dfrac{\partial \psi }{\partial x}dx + \dfrac{\partial \psi }{\partial y}dy$, it follows that $d\psi</description>
    </item>
    <item>
      <title>Differentiation of Vectors, Dot Product, and Cross Product in Cartesian Coordinates</title>
      <link>https://freshrimpsushi.github.io/en/posts/506/</link>
      <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/506/</guid>
      <description>Formulas Let&amp;rsquo;s say $\mathbf{A} = A_{x}\hat{\mathbf{x}} + A_{y}\hat{\mathbf{y}} + A_{z}\hat{\mathbf{z}}, \mathbf{B} = B_{x}\hat{\mathbf{x}} + B_{y}\hat{\mathbf{y}} + B_{z}\hat{\mathbf{z}}$ is a vector in a 3-dimensional Cartesian coordinate system. Let $n$ be any scalar. Then, the following equations hold: (a) $\dfrac{ d \left( n \mathbf{A} \right) }{dt} = \dfrac{ dn }{dt} \mathbf{A} + n\dfrac{ d\mathbf{A}}{dt}$ (b) $\dfrac{ d ( \mathbf{A} \cdot \mathbf{B} )}{dt} = \dfrac{ \mathbf{A} }{dt} \cdot \mathbf{B} + \mathbf{A} \cdot \dfrac{</description>
    </item>
    <item>
      <title>Kinetic and Potential Energy Definitions in Physics</title>
      <link>https://freshrimpsushi.github.io/en/posts/507/</link>
      <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/507/</guid>
      <description>Kinetic Energy1 When the force depends only on the position, i.e., it is independent of the velocity or time, the equation of motion (differential equation) for the straight-line motion of a particle is as follows. $$ \begin{equation} F(x)=m\ddot{x} \label{force1} \end{equation} $$ In this case, acceleration $\ddot{x}$ can be expressed in terms of velocity as follows. $$ \begin{align*} \ddot{x} &amp;amp;= \dfrac{d \dot{x}}{dt} \\ &amp;amp;=\dfrac{dv}{dt} \\ &amp;amp;=\dfrac{dv}{dx} \dfrac{dx}{dt} \\ &amp;amp;=v\dfrac{dv}{dx} \\ &amp;amp;=</description>
    </item>
    <item>
      <title>Separable First-Order Differential Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/503/</link>
      <pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/503/</guid>
      <description>Definition1 A first-order differential equation is said to be separable if it satisfies the following condition: $$ f(x)+g(y)\dfrac{dy}{dx}=0 \quad \text{or} \quad f(x)dx = -g(y)dy $$ Explanation It can be expressed in various forms, but the important point is that the variables on each side must be separated. The method of finding solutions by separating these two variables is called the method of separation of variables. The separability is a very</description>
    </item>
    <item>
      <title>Wronskian Definition and Determination of Linear Independence</title>
      <link>https://freshrimpsushi.github.io/en/posts/501/</link>
      <pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/501/</guid>
      <description>Definition1 Let us consider a set of functions that are differentiable up to n times, denoted by $S=\left\{ f_{1}, f_{2}, \dots, f_{n} \right\}$. The Wronskian $W$ of this set is defined by the following determinant. $$ W(x) = W(f_{1}, f_{2}, \dots, f_{n}) := \begin{vmatrix} f_{1} &amp;amp; f_{2} &amp;amp; \cdots &amp;amp; f_{n} \\ f_{1}^{\prime} &amp;amp; f_2^{\prime} &amp;amp; \cdots &amp;amp; f_{n}^{\prime} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ f_{1}^{(n-1)} &amp;amp;</description>
    </item>
    <item>
      <title>Classification of Differential Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/483/</link>
      <pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/483/</guid>
      <description>Description Differential equations can be classified by various criteria. They are broadly divided into ordinary differential equations and partial differential equations. Further classification can be made based on coefficients and order, and whether they are linear or nonlinear. The reason for classifying differential equations is obviously to solve them. The method of solving a differential equation varies depending on its classification. Ordinary Differential Equations and Partial Differential Equations Ordinary differential</description>
    </item>
    <item>
      <title>Definition and Examples of Differential Equations</title>
      <link>https://freshrimpsushi.github.io/en/posts/479/</link>
      <pubDate>Sat, 17 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/479/</guid>
      <description>Definition A differential equation is an equation that includes derivatives of one or more dependent variables with respect to one or more independent variables. $$ \dfrac{dy}{dx}=y $$ $$ \dfrac{d^2y}{dx^2} = y $$ Explanation Most physical situations can be described by first-order or second-order differential equations. Falling Body $$ F=ma=mg $$ $$ v=\dfrac{dy}{dt} $$ $$ a=\dfrac{dv}{dt}=\dfrac{d}{dt} \left( \dfrac{dy}{dt} \right)=\dfrac{d^2y}{dt^2} $$ $$ \dfrac{d^2y}{dt^2}=g $$ Spring Mass System $$ F=ma=-ky $$ $$ a=</description>
    </item>
    <item>
      <title>연산자 방법으로 조화진동자 문제 풀기  사다리 연산자의 정의</title>
      <link>https://freshrimpsushi.github.io/en/posts/362/</link>
      <pubDate>Wed, 13 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/362/</guid>
      <description>🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧 조화진동자 문제를 연산자 방법으로 풀 때 아주 유용한 연산자가 있다.바로 조화진동자의 사다리연산자$\ma</description>
    </item>
    <item>
      <title>Ladder Operators for Angular Momentum: Raising and Lowering Operators</title>
      <link>https://freshrimpsushi.github.io/en/posts/344/</link>
      <pubDate>Sun, 26 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/344/</guid>
      <description>Definition Raising Operator: $L_+ \equiv L_{x} + iL_{y}$ Lowering Operator: $L_- \equiv L_{x} - iL_{y}$, i.e., $(L_-)^{\ast}=L_+$ Explanation Why are these two operators named &amp;lsquo;raising&amp;rsquo; and &amp;rsquo;lowering&amp;rsquo;? It&amp;rsquo;s because when applied to the simultaneous eigenfunction of the square of the angular momentum magnitude and an angular momentum component, it raises or lowers the state of the eigenfunction. This can be seen in Finding the Eigenvalues of the Square of Angular</description>
    </item>
    <item>
      <title>Hermite Operators</title>
      <link>https://freshrimpsushi.github.io/en/posts/304/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/304/</guid>
      <description>Definition For an operator $A$, we denote its conjugate transpose as $A^{\dagger}$. An operator $A$ that satisfies $A = A^{\dagger}$ is called a Hermitian operator.</description>
    </item>
    <item>
      <title>Dimension of the Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/3018/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3018/</guid>
      <description>Definition1 The number of elements (vectors) of a basis for a vector space $V$ is defined as the dimension of $V$ and is denoted as follows. $$ \dim (V) $$ Explanation Such a generalization of dimensions goes beyond merely exploring vector spaces and is being applied to various technologies that support this society. It might seem pointless to consider dimensions higher than the $3$ dimensions of our world and the</description>
    </item>
    <item>
      <title>What is Dirac Notation?</title>
      <link>https://freshrimpsushi.github.io/en/posts/303/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/303/</guid>
      <description>Definition In quantum mechanics, a wave function is a vector and is fundamentally treated as a column vector. Column vectors are denoted by a right single arrow bracket, and this is called a ket vector. $$ \psi = \ket{\psi} = \begin{pmatrix} \psi_{1} \\ \psi_{2} \\ \vdots \\ \psi_{n} \end{pmatrix} $$ The conjugate transpose matrix of $\ket{\psi}$ is denoted by a left single arrow bracket and is called a bra vector.</description>
    </item>
    <item>
      <title>In Physics (Quantum Mechanics), an Operator Refers to</title>
      <link>https://freshrimpsushi.github.io/en/posts/301/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/301/</guid>
      <description>Buildup In mathematics, what is called a function is a relationship that maps exactly one element of some set $X$ to an element of another set $Y$, and it is commonly denoted by $f$, following the first letter of the word function. If we consider a function that maps an element $x$ of the set $X$ to an element $y$ of the set $Y$, we denote it as follows. $$</description>
    </item>
    <item>
      <title>Gradient, Divergence, Curl, and Laplacian in Curvilinear Coordinates</title>
      <link>https://freshrimpsushi.github.io/en/posts/299/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/299/</guid>
      <description>Explanation In physics, the four operations involving the del operator $\nabla$, Gradient, Divergence, Curl, Laplacian, are very important. Therefore, one must know the operations in three coordinate systems. Of course, this does not mean that you have to memorize them. Since physics study is not about memorizing formulas, they will naturally be memorized as you study, so do not try to memorize them intentionally but instead keep a printout of</description>
    </item>
    <item>
      <title>Linear Combination, Span</title>
      <link>https://freshrimpsushi.github.io/en/posts/512/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/512/</guid>
      <description>Definition: Linear Combination1 Let $\mathbf{w}$ be a vector in the vector space $V$. If $\mathbf{w}$ can be expressed as follows for vectors $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$ in $V$ and arbitrary constants $k_{1}, k_{2}, \cdots, k_{r}$, then $\mathbf{w}$ is called a linear combination of $\mathbf{v}_{1},\mathbf{v}_{2},\cdots ,\mathbf{v}_{r}$. $$ \mathbf{w} = k_{1}\mathbf{v}_{1} + k_{2}\mathbf{v}_{2} + \cdots + k_{r}\mathbf{v}_{r} $$ Additionally, in this case, the constants $k_{1}, k_{2}, \cdots, k_{r}$ are referred to as the coefficients</description>
    </item>
    <item>
      <title>Subspace of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/285/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/285/</guid>
      <description>Definition1 Let $W$ be a non-empty subset of the vector space $V$. If $W$ satisfies the definition of a vector space with respect to the addition and scalar multiplication defined in $V$, then $W$ is called a subspacesubspace of the vector space $V$, and is denoted as follows: $$ W \le V $$ Explanation To determine whether a subset $W$ of a vector space $V$ is a subspace of $V$,</description>
    </item>
    <item>
      <title>Definition of Vector Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/282/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/282/</guid>
      <description>Definition1 When the elements of a non-empty set $V$ satisfy the following ten rules for two operations, additionaddition and scalar multiplicationscalar multiplication, $V$ is called a vector spacevector space over field2 $\mathbb{F}$, and the elements of $V$ are called vectorsvector. For $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $k, l \in \mathbb{F}$, (A1) If $\mathbf{u}, \mathbf{v}$ is an element of $V$, then $\mathbf{u}+\mathbf{v}$ is also an element of $V$. (A2) $\mathbf{u}</description>
    </item>
    <item>
      <title>Matrix Rank, Nullity</title>
      <link>https://freshrimpsushi.github.io/en/posts/3021/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3021/</guid>
      <description>Theorem1 The dimensions of the row space and column space of matrix $A$ are the same. Proof Let $R$ be the row echelon form matrix of $A$. Since basic row operations do not change the dimensions of the row space and column space of $A$, the following equation holds: $$ \begin{align*} \dim \big( \mathcal{R}(A) \big) &amp;amp;= \dim \big( \mathcal{R}(R) \big) \\ \dim \big( \mathcal{C}(A) \big) &amp;amp;= \dim \big( \mathcal{C}(R) \big)</description>
    </item>
    <item>
      <title>World Line and Galilean Transformation</title>
      <link>https://freshrimpsushi.github.io/en/posts/250/</link>
      <pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/250/</guid>
      <description>Definition The line that represents the track of a particle in space and time is called a world line. Description Let&amp;rsquo;s think only about a coordinate system moving at a constant speed in one direction. In the $A$ coordinate system, there is a particle at rest at the origin. The world line of this particle is as follows. And there is a $A^{\prime}$ coordinate system moving at a speed of</description>
    </item>
    <item>
      <title>Inertia Moment and Turning Radius</title>
      <link>https://freshrimpsushi.github.io/en/posts/234/</link>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/234/</guid>
      <description>Moment of Inertia $$ \begin{align*} I &amp;amp;= \sum_{i} m_{i} {r_{i}}^2 \\ I &amp;amp;= \int r^2 dm \end{align*} $$ The moment of inertia is defined as the (mass of a particle)$\times$(distance from the rotation axis to the particle) and represents the physical quantity that indicates the characteristic of a body to continue rotating. Its symbol is $I$, which seems to be derived from the initial letter of the English word Inertia.</description>
    </item>
    <item>
      <title>Euclidean Space</title>
      <link>https://freshrimpsushi.github.io/en/posts/205/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/205/</guid>
      <description>Definition For a natural number $n \in \mathbb{N}$, the Cartesian product $\mathbb{R}$ of the set of real numbers is called the Euclidean space. $$ \mathbb{R}^{n} = \mathbb{R} \times \cdots \times \mathbb{R} $$ $\mathbb{R}^{1}$ is referred to as real space or number line. $\mathbb{R}^{2}$ is called a plane. $\mathbb{R}^{3}$ is called a $3$-dimensional space. Here, $\mathbb{N} := \left\{ 1, 2, 3, \cdots \right\}$ means the set that includes all natural numbers.</description>
    </item>
    <item>
      <title>Momentum Operators in Quantum Mechanics</title>
      <link>https://freshrimpsushi.github.io/en/posts/100/</link>
      <pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/100/</guid>
      <description>Definition In quantum mechanics, the momentum operator is as follows. $$ p_{\text{op}} = \frac{\hbar}{i}\frac{\partial}{\partial x} = -i\hbar \dfrac{\partial }{\partial x} $$</description>
    </item>
    <item>
      <title>Velocity and Acceleration in Polar Coordinates</title>
      <link>https://freshrimpsushi.github.io/en/posts/158/</link>
      <pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/158/</guid>
      <description>Velocity and Acceleration in Polar Coordinates $$ \begin{align*} \mathbf{v}&amp;amp;=\dot{r} \hat{\mathbf{r}} + r \dot{\theta} \hat{\boldsymbol{\theta}} \\ \mathbf{a}&amp;amp;= (\ddot r -r\dot{\theta} ^2)\hat{\mathbf{r}} + (2\dot{r} \dot{\theta} + r\ddot{\theta})\hat{\boldsymbol{\theta}} \end{align*} $$ Derivation In the polar coordinate system, unit vectors can be described as follows. $$ \begin{align*} &amp;amp;&amp;amp; \mathbf{r}&amp;amp;=r\hat{\mathbf{r}}=x\hat{\mathbf{x}} + y \hat{\mathbf{y}} \\ \implies &amp;amp;&amp;amp; \hat{\mathbf{r}} &amp;amp;= \frac{x}{r}\hat{\mathbf{x}} +\frac{y}{r} \hat{\mathbf{y}}=\cos\theta \hat{\mathbf{x}} + \sin\theta \hat{\mathbf{y}} = \hat{\mathbf{r}} (\theta) \\ {} \\ &amp;amp;&amp;amp; \hat \theta &amp;amp;= \hat{\mathbf{r}}(\theta+\pi/2)=</description>
    </item>
    <item>
      <title>Unit Vectors of the Spherical Coordinate System Expressed in Terms of Unit Vectors of the Cartesian Coordinate System</title>
      <link>https://freshrimpsushi.github.io/en/posts/152/</link>
      <pubDate>Sun, 16 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/152/</guid>
      <description>Spherical Coordinate System&amp;rsquo;s Unit Vectors $$ \begin{align*} \hat{\mathbf{r}} &amp;amp;= \cos\phi \sin\theta\hat{\mathbf{x}} + \sin\phi \sin\theta\hat{\mathbf{y}} + \cos\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\theta}} &amp;amp;= \cos\phi \cos\theta \hat{\mathbf{x}} + \sin\phi \cos\theta \hat{\mathbf{y}} - \sin\theta\hat{\mathbf{z}} \\ \hat{\boldsymbol{\phi}} &amp;amp;= -\sin\phi \hat{\mathbf{x}} + \cos\phi \hat{\mathbf{y}} \end{align*} $$ Derivation First, calculate $\hat{\mathbf{r}}$ and then use it to derive the other two. Radial Direction Unit Vector $\hat{\mathbf{r}}$ $$ \hat{\mathbf{r}}=r\hat{\mathbf{r}}=x\hat{\mathbf{x}}+y\hat{\mathbf{y}}+z\hat{\mathbf{z}} $$ Therefore, dividing both sides by $r$ gives: $$ \begin{align*} \hat{\mathbf{r}}&amp;amp;=\frac{x}{r}\hat{\mathbf{x}}+\frac{y}{r}\hat{\mathbf{y}}+\frac{z}{r}\hat{\mathbf{z}} \\</description>
    </item>
    <item>
      <title>Scalar Triple Product</title>
      <link>https://freshrimpsushi.github.io/en/posts/144/</link>
      <pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/144/</guid>
      <description>Definition The following expression is called the scalar triple product. $$ \mathbf{A}\cdot (\mathbf{B} \times \mathbf{C} ) $$ Explanation A scalar triple product is an operation involving the product of three vectors, where the result is a scalar. The operation resulting in a vector is called vector triple product. To get a scalar result, one must first cross multiply two vectors to produce another vector and then dot multiply it with</description>
    </item>
    <item>
      <title>Gradient of the Magnitude of Separation Vectors</title>
      <link>https://freshrimpsushi.github.io/en/posts/142/</link>
      <pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/142/</guid>
      <description>Formulas The square of the magnitude of the separation vector $\bcR$ and the gradient of $\cR ^{n}$ are as follows. $$ \nabla (\cR^n)=n\cR^{n-1}\crH $$ Explanation It is calculated in the same way as the derivative of a polynomial function, and then just attach the unit vector $\crH$. Since the separation vector is $\bcR=\mathbf{r}-\mathbf{r}^{\prime}$, it has variables $(x,y,z)$ and $(x^{\prime},y^{\prime},z^{\prime})$. Therefore, attention must be paid when differentiating. Gradients for coordinates with</description>
    </item>
    <item>
      <title>Separation Vector</title>
      <link>https://freshrimpsushi.github.io/en/posts/141/</link>
      <pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/141/</guid>
      <description>Definition1 The vector from the source point to the observation point is called the separation vector. $$ \bcR = \mathbf{r} - \mathbf{r}^{\prime} $$ Description Source vector $\mathbf{r}^{\prime}$: The place where there is a charge or current. That is, it represents the coordinates of the origin of the electromagnetic field. Position vector $\mathbf{r}$: Represents the coordinates of where the electric field $\mathbf{E}$ or magnetic field $\mathbf{B}$ is measured. Separation vector $\bcR$:</description>
    </item>
    <item>
      <title>Dirac Delta Function</title>
      <link>https://freshrimpsushi.github.io/en/posts/103/</link>
      <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/103/</guid>
      <description>Definition A function that satisfies the following two conditions is called the Dirac delta function. $$ \delta (x) = \begin{cases} 0, &amp;amp; x\neq 0 \\ \infty , &amp;amp; x=0 \end{cases} $$ $$ \int_{-\infty}^{\infty}{\delta (x) dx}=1 $$ Description ※Be careful not to confuse it with the Kronecker delta. In engineering, it is called the unit impulse function. Strictly speaking, mathematically, the Dirac delta function is not a function because</description>
    </item>
    <item>
      <title>Einstein Notation</title>
      <link>https://freshrimpsushi.github.io/en/posts/90/</link>
      <pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/90/</guid>
      <description>Notation The summation sign $\sum$ is omitted when a subscript is repeated two or more times. Description Also referred to as the Einstein summation convention. It&amp;rsquo;s not really a formula but rather a rule. When doing vector calculations, there are often cases where one needs to write the summation sign $\sum$ multiple times in a single formula, which can make the equation look cluttered and is very annoying to write</description>
    </item>
    <item>
      <title>Product of Two Levi-Civita Symbols</title>
      <link>https://freshrimpsushi.github.io/en/posts/88/</link>
      <pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/88/</guid>
      <description>Theorem The $\epsilon_{ijk}$, defined as follows, is referred to as the Levi-Civita symbol. $$ \epsilon_{ijk} = \begin{cases} +1 &amp;amp; \text{if} \ \epsilon_{123}, \epsilon_{231}, \epsilon_{312} \\ -1 &amp;amp; \text{if} \ \epsilon_{132}, \epsilon_{213}, \epsilon_{321} \\ 0 &amp;amp; \text{if} \ i=j \ \text{or} \ j=k \ \text{or} \ k=i \end{cases} $$ The $\delta_{ij}$, defined as follows, is referred to as the Kronecker delta. $$ \delta_{ij} := \begin{cases} 1,&amp;amp;i=j \\ 0, &amp;amp; i\ne j</description>
    </item>
    <item>
      <title>Kronecker Delta</title>
      <link>https://freshrimpsushi.github.io/en/posts/84/</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/84/</guid>
      <description>Definition The $\delta_{ij}$, defined as follows, is called the Kronecker delta. $$ \delta_{ij} := \begin{cases} 1,&amp;amp;i=j \\ 0, &amp;amp; i\ne j \end{cases} $$ Description The Kronecker delta is used in many places, and its main role is to indicate only what one wants among all components (elements, possibilities, etc.). For students of physics, it is mainly encountered in the expression for the inner product. This might not be immediately clear,</description>
    </item>
    <item>
      <title>Levi-Civita Symbol</title>
      <link>https://freshrimpsushi.github.io/en/posts/83/</link>
      <pubDate>Tue, 25 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/83/</guid>
      <description>Definition The $\epsilon_{ijk}$ defined as follows is called the Levi-Civita symbol. $$ \epsilon_{ijk} = \begin{cases} +1 &amp;amp; \text{if} \ \epsilon_{123}, \epsilon_{231}, \epsilon_{312} \\ -1 &amp;amp; \text{if} \ \epsilon_{132}, \epsilon_{213}, \epsilon_{321} \\ 0 &amp;amp; \text{if} \ i=j \ \text{or} \ j=k \ \text{or} \ k=i \end{cases} $$ Description While the Kronecker delta only considers whether the indices are equal, the Levi-Civita symbol, as shown in its definition, is also affected by</description>
    </item>
    <item>
      <title>Vector Triple Product, BAC-CAB Rule</title>
      <link>https://freshrimpsushi.github.io/en/posts/71/</link>
      <pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/71/</guid>
      <description>Formulas $$ \mathbf{A} \times (\mathbf{B} \times \mathbf{C} ) = \mathbf{B}(\mathbf{A} \cdot \mathbf{C} )-\mathbf{C}(\mathbf{A} \cdot \mathbf{B}) $$</description>
    </item>
    <item>
      <title>Difference Between torch.nn and torch.nn.functional in PyTorch</title>
      <link>https://freshrimpsushi.github.io/en/posts/3626/</link>
      <pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3626/</guid>
      <description>Description PyTorch contains many functions related to neural networks, which are included under the same names in torch.nn and torch.nn.functional. The functions in nn return a neural network as a function, while those in nn.functional are the neural network itself. For instance, nn.MaxPool2d takes the kernel size as input and returns a pooling layer. import torch import torch.nn as nn pool = nn.MaxPool2d(kernel_size = 2) # MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,</description>
    </item>
    <item>
      <title>Solution of Wave Equation with Zero Initial Condition</title>
      <link>https://freshrimpsushi.github.io/en/posts/3623/</link>
      <pubDate>Fri, 04 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3623/</guid>
      <description>Tidy up Let&amp;rsquo;s say that we have the following wave equation. where $\Delta_{\mathbf{x}}$ is Laplacian for the variable $\mathbf{x}$. $$ \begin{align} \partial_{t}^{2} p(\mathbf{x}, t) &amp;amp;= \Delta_{\mathbf{x}} p(\mathbf{x}, t) &amp;amp;\text{on } \mathbb{R} \times [0, \infty) \\ p(\mathbf{x}, 0) &amp;amp;= f(\mathbf{x}) &amp;amp;\text{on } \mathbb{R} \\ \partial_{t} p(\mathbf{x}, 0) &amp;amp;= 0 &amp;amp;\text{on } \mathbb{R} \end{align} $$ The solution of the above partial differential equation is as follows. $$ \begin{equation} p(\mathbf{x}, t) = \dfrac{1}{(2\pi)^{n}}</description>
    </item>
    <item>
      <title>파이토치에서 AdaBelief 옵티마이저 사용하는 방법</title>
      <link>https://freshrimpsushi.github.io/en/posts/3620/</link>
      <pubDate>Sat, 28 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3620/</guid>
      <description>Description AdaBelief, introduced by J. Zhuang et al. in 2020, is one of the variations of Adam1. Since PyTorch does not natively provide this optimizer, it must be installed separately. Code2 Installation The following command can be used to install it via cmd. pip install adabelief-pytorch==0.2.0 Usage The code below can be used to import and utilize it. from adabelief_pytorch import AdaBelief optimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple =</description>
    </item>
    <item>
      <title>How to Use Color Gradients in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3608/</link>
      <pubDate>Wed, 04 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3608/</guid>
      <description>Description A color gradient is one of the two color schemes supported by Julia&amp;rsquo;s visualization package Plots.jl (the other is palette), which is what we commonly refer to as gradation. Simply put, a type that implements gradation is ColorGradient. Gradients are used to draw charts such as heatmap(), surface(), contour(). If you want to differentiate the colors of various graphs, use a palette instead of a gradient. Code Symbol It</description>
    </item>
    <item>
      <title>How to Use Palettes in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3607/</link>
      <pubDate>Mon, 02 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3607/</guid>
      <description>Explanation A palette refers to a board where paints are squeezed out in advance. Mathematically, it can be explained as a &amp;lsquo;set of colors&amp;rsquo; or a &amp;lsquo;sequence of colors&amp;rsquo;. When drawing multiple graphs in one picture, the most common way is to distinguish them by using different colors. For this purpose, Julia has implemented a type called ColorPalette that collects various colors. It can be comfortably understood as a vector</description>
    </item>
    <item>
      <title>How to Plot Two Data Axes of Different Scales in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3606/</link>
      <pubDate>Sat, 31 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3606/</guid>
      <description>코드 When plotting two data sets that have a large scale difference on the same plot, the one with the smaller scale gets completely ignored as shown in the figure below. using Plots x = 0:0.01:2π plot(x, sin.(x)) plot!(x, exp.(x)) When plotting the second data set, if you input twinx() as the first argument, it shares the $x$ axis and the</description>
    </item>
    <item>
      <title>List of Plot Properties in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3605/</link>
      <pubDate>Thu, 29 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3605/</guid>
      <description>Description In Julia&amp;rsquo;s Plots.jl, a plot is also an object. If you draw an empty plot to check its type, it looks like this. julia&amp;gt; using Plots julia&amp;gt; p = plot() julia&amp;gt; p |&amp;gt; typeof Plots.Plot{Plots.GRBackend} Removing Plots., it becomes Plot{GRBackend}, meaning the plot&amp;rsquo;s backend is GR, similar to how a vector with elements of type Float64 is denoted as Vector{Float64}. Checking the properties of Plot, we find the following.</description>
    </item>
    <item>
      <title>Decorating the Background Grid in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3604/</link>
      <pubDate>Tue, 27 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3604/</guid>
      <description>Theorem Keywords related to the grid background in Plots.jl are as follows: Keyword Name Function grid Display grid gridalpha, ga, gα Specify grid transparency foreground_color_grid, fgcolor_grid Specify grid color gridlinewidth, grid_lw Specify grid thickness gridstyle, grid_ls Specify grid line style minorgrid Display minor grid minorgridalpha Specify minor grid transparency foreground_color_minor_grid, fgcolor_minorgrid Specify minor grid color minorgridlinewidth, minorgrid_lw Specify minor grid thickness minorgridstyle, minorgrid_ls Specify minor grid line style Code</description>
    </item>
    <item>
      <title>Specifying Background Color in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3603/</link>
      <pubDate>Sun, 25 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3603/</guid>
      <description>Theorem The keywords related to the background color of figures in Plots.jl are as follows. Keyword Name Function background_color, bg_color Specify the color of the overall background background_color_outside, bg_color_outside Specify the color of the area outside where the graph is drawn background_subplot, bg_subplot Specify the color of the area where the graph is drawn background_inside, bg_inside Specify the color of the area where the graph is drawn, excluding the legend</description>
    </item>
    <item>
      <title>How to Specify Graph Colors for Each Subplot in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3602/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3602/</guid>
      <description>Overview This section introduces three methods for specifying graph colors for each subplot. To learn how to specify colors for graph elements, refer here. Method 1 The first way to specify the graph color for a subplot is to predefine the color when defining each subplot. In Julia, since a picture is an object itself, you can define multiple pictures with different attributes and then combine them into one plot.</description>
    </item>
    <item>
      <title>Specifying the Color of Graph Elements in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3601/</link>
      <pubDate>Wed, 21 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3601/</guid>
      <description>Theorem In Plots.jl, the keywords for specifying the color of each graph component are as follows. Keyword Function markercolor, mc Specify the marker&amp;rsquo;s inside color markerstrokecolor, msc Specify the marker&amp;rsquo;s border color linecolor, lc Specify the line color fillcolor, fc Specify the fill color seriescolor, c Specify the color of all components Keyword Function markeralpha, ma, mα Specify the marker&amp;rsquo;s inside transparency markerstrokealpha, msa, msα Specify the</description>
    </item>
    <item>
      <title>How to Use RGB Color Codes in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3600/</link>
      <pubDate>Mon, 19 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3600/</guid>
      <description>Code The package provided in Julia for dealing with colors is Colors.jl. By importing the visualization package Plots.jl, the features within Colors.jl can also be used. The color codes representing the RGB space include RGB, BGR, RGB24, RGBX, XRGB, which are subtypes of AbstractRGB. RGBA adds transparency to RGB. julia&amp;gt; using Plots julia&amp;gt; subtypes(AbstractRGB) 5-element Vector{Any}: BGR RGB RGB24 RGBX XRGB julia&amp;gt; subtypes(AbstractRGBA) 2-element Vector{Any}: BGRA RGBA Strings For the</description>
    </item>
    <item>
      <title>Package for Color Processing in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3599/</link>
      <pubDate>Sat, 17 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3599/</guid>
      <description>Introduction1 Introducing the capabilities of Colors.jl, a package for color processing in Julia. When using the visualization package Plots.jl, there&amp;rsquo;s no need to load Colors.jl separately. It provides the following functionalities: Color parsing and conversion Color maps Color scales Parsing and Conversion Assuming str is a string representing color information, you can parse the string into a color code of a specific color space using @colorant_str or parse(Colorant, str). Note</description>
    </item>
    <item>
      <title>How to Use Colors in Julia Plots</title>
      <link>https://freshrimpsushi.github.io/en/posts/3598/</link>
      <pubDate>Thu, 15 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3598/</guid>
      <description>Overview The package that facilitates the convenient use of colors in Julia is Colors.jl. It can be used together just by importing the visualization package Plots.jl. Symbols and Strings The way to check the list of named colors is by entering Colors.color_names in the console window or checking the official documentation. julia&amp;gt; using Plots julia&amp;gt; Colors.color_names Dict{String, Tuple{Int64, Int64, Int64}} with 666 entries: &amp;#34;darkorchid&amp;#34; =&amp;gt; (153, 50, 204) &amp;#34;chocolate&amp;#34; =&amp;gt;</description>
    </item>
    <item>
      <title>Decorating Text Output with Built-in Functions in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3597/</link>
      <pubDate>Tue, 13 May 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3597/</guid>
      <description>Code Using the function printstyled(string; color = color) allows you to decorate the outputted function. As input for the keyword argument color, symbols and integers $(0 \le n \le 255)$ are possible. Note that strings are not allowed. The available symbols include not only colors but also options like :blink, :reverse, etc. These can also be applied by entering them as keyword arguments like blink = true, bold = true.</description>
    </item>
    <item>
      <title>Dimensionality Reduction in Data Science</title>
      <link>https://freshrimpsushi.github.io/en/posts/3563/</link>
      <pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3563/</guid>
      <description>Definition Let&amp;rsquo;s assume a data set $X \subset \mathbb{R}^{n}$ is given. The following mapping for $m \lt n$ is called dimension reductiondimension reduction. $$ r : X \to \mathbb{R}^{m} $$ Or more commonly in machine learning, any method that reduces the number of input variables in a way that retains as much of the performance as possible is called a dimension reduction technique. Explanation Dimension reduction, as the name suggests,</description>
    </item>
    <item>
      <title>How to Define and Train MLP with the Sequence Model and Functional API in TensorFlow and Keras</title>
      <link>https://freshrimpsushi.github.io/en/posts/3562/</link>
      <pubDate>Tue, 04 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3562/</guid>
      <description>Overview In TensorFlow, neural networks can be easily defined using Keras. Below, we introduce how to define and train a simple MLP using Sequential() and the functional API. However, Sequential() is only easy for defining models and can be challenging to use for designing complex structures. Similarly, if you plan to design complex structures using the functional API, it’s better to use the keras.Model class, and for</description>
    </item>
    <item>
      <title>How to Adjust the Size and Resolution of an Image in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3559/</link>
      <pubDate>Thu, 27 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3559/</guid>
      <description>Code Size plot(x, y, size=(600,400)) In Julia, the size of a plot is set using the size option. It must be input as a Tuple{Integer, Integer}, where each integer represents the width and height in pixels, respectively. The default value is (600,400). using Plots x = rand(10) plot(x) savefig(&amp;#34;size_default.png&amp;#34;) plot(x, size=(1200,800)) savefig(&amp;#34;size_(1200,800).png&amp;#34;) 1800x1200 image (left), 600x400 image (right) Resolution plot(x, y, dpi=100) The resolution of an image is set using</description>
    </item>
    <item>
      <title>Drawing Arrows in Graphics with Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3558/</link>
      <pubDate>Tue, 25 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3558/</guid>
      <description>Code plot!([x1, x2], [y1, y2], arrow=:true) This code plots an arrow from point $(x1, y1)$ to point $(x2, y2)$ on the plot. Naturally, the tip of the arrow is at the terminal point $(x2, y2)$. The maximum value of the sine function can be shown as follows. using Plots x = range(0, 2π, 100) plot(x, sin.(x), label=&amp;#34;&amp;#34;, ylims=(-1.3,1.3)) plot!([π/2</description>
    </item>
    <item>
      <title>How to Fix the Random Seed in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3555/</link>
      <pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3555/</guid>
      <description>Explanation1 In Julia, the random seed can be fixed as follows: seed!([rng=default_rng()], seed) -&amp;gt; rng seed!([rng=default_rng()]) -&amp;gt; rng The input variable rng stands for Random Number Generator, which refers to the algorithm used for drawing random numbers. The Random package offers the following options: TaskLocalRNG: This is the default setting. Xoshiro RandomDevice MersenneTwister Code By fixing the seed to 0, drawing three times, and then fixing it again to 0</description>
    </item>
    <item>
      <title>How to Draw a Box Plot in Julia</title>
      <link>https://freshrimpsushi.github.io/en/posts/3553/</link>
      <pubDate>Sat, 15 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/en/posts/3553/</guid>
      <description>English Translation Description To draw a box plot, the statistical visualization package StatsPlots.jl must be used. boxplot([data], labels=[label]) Code using StatsPlots x = rand(0:100, 100) y = rand(50:100, 100) z = cat(x,y, dims=1) boxplot(x, label=&amp;#34;x&amp;#34;) boxplot!(y, label=&amp;#34;y&amp;#34;) boxplot!(z, label=&amp;#34;z&amp;#34;) Or boxplot([x,y,z], label=[&amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;]) will draw the same figure. Note that there should be no commas in lable. That is, it needs to be an array, not an $3 \times</description>
    </item>
  </channel>
</rss>
