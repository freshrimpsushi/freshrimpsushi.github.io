<!doctype html><html class=blog lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=preload href=https://freshrimpsushi.github.io/en/css/style.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=preload href=https://freshrimpsushi.github.io/en/css/comment.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=icon href=https://freshrimpsushi.github.io/ko/logo/favicon.ico><meta name=msapplication-TileColor content="#FFFFFF"><meta name=msapplication-TileImage content="logo/basic.png"><meta name=NaverBot content="All"><meta name=NaverBot content="index,follow"><meta name=Yeti content="All"><meta name=Yeti content="index,follow"><meta name=google-site-verification content="KYAokS7-6C5YuXOjatJQsiK1T0O8x4YncYFIF4tneYI"><meta name=naver-site-verification content="e5651d6f97899061897203413efc84994f04bbba"><link rel=alternate type=application/rss+xml title=FreshrimpRestaurant href=https://freshrimpsushi.github.io/en/index.xml><title>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</title></head><meta name=title content="Adaptive Learning Rates: AdaGrad, RMSProp, Adam"><meta name=description content="국내 최대의 수학, 물리학, 통계학 블로그"><meta property="og:title" content="Adaptive Learning Rates: AdaGrad, RMSProp, Adam"><meta property="og:description" content><meta property="og:image" content="https://freshrimpsushi.github.io/en/logo/basic.png/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"j_","name":"Adaptive Learning Rates: AdaGrad, RMSProp, Adam","headline":"Adaptive Learning Rates: AdaGrad, RMSProp, Adam","alternativeHeadline":"","description":"Overview1 2 Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam. Explanation In gradient descent, the learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\\alpha$, $\\eta$, and determines how much of the gradient is taken into account when updating the parameter. Optimization","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/freshrimpsushi.github.io\/en\/posts\/3529\/"},"author":{"@type":"Person","name":"전기현","url":"https://math.stackexchange.com/users/459895/ryu-dae-sick"},"creator":{"@type":"Person","name":"전기현"},"accountablePerson":{"@type":"Person","name":"전기현"},"copyrightHolder":"FreshrimpRestaurant","copyrightYear":"2023","dateCreated":"2023-12-29T00:00:00.00Z","datePublished":"2023-12-29T00:00:00.00Z","dateModified":"2023-12-29T00:00:00.00Z","publisher":{"@type":"Organization","name":"FreshrimpRestaurant","url":"https://freshrimpsushi.github.io/en/","logo":{"@type":"ImageObject","url":"https:\/\/freshrimpsushi.github.io\/en\/logo\/basic.png","width":"32","height":"32"}},"image":"https://freshrimpsushi.github.io/en/logo/basic.png","url":"https:\/\/freshrimpsushi.github.io\/en\/posts\/3529\/","wordCount":"1221","genre":[],"keywords":[]}</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/mhchem.min.js integrity=sha384-F2ptQFZqNJuqfGGl28mIXyQ5kXH48spn7rcoS0Y9psqIKAcZPLd1NzwFlm/bl1mH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>function renderKaTex(e){renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],trust:!0,trust:e=>["\\htmlId","\\href","\\includegraphics"].includes(e.command),macros:{"\\eqref":"(\\text{#1})","\\ref":"\\href{###1}{\\text{#1}}","\\label":"\\htmlId{#1}{}","\\sech":"\\operatorname{sech}","\\csch":"\\operatorname{csch}","\\sgn":"\\operatorname{sgn}","\\sign":"\\operatorname{sign}","\\sinc":"\\operatorname{sinc}","\\diag":"\\operatorname{diag}","\\diam":"\\operatorname{diam}","\\trace":"\\operatorname{trace}","\\Tr":"\\operatorname{Tr}","\\tr":"\\operatorname{tr}","\\re":"\\operatorname{Re}","\\im":"\\operatorname{Im}","\\Var":"\\operatorname{Var}","\\Poi":"\\operatorname{Poi}","\\Cov":"\\operatorname{Cov}","\\span":"\\operatorname{span}","\\supp":"\\operatorname{supp}","\\rank":"\\operatorname{rank}","\\nullity":"\\operatorname{nullity}","\\ad":"\\operatorname{ad}","\\Ric":"\\operatorname{Ric}","\\i":"\\mathrm{i}","\\d":"\\mathrm{d}","\\cR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5863.png}","\\acR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.4371.png}","\\bcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5899.png}","\\abcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.0596.png}","\\crH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\smallcrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\acrH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}","\\smallacrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}"},throwOnError:!1})}</script><body class=main><header><a href=https://freshrimpsushi.github.io/en/ rel=home><p style=text-align:center;font-size:1rem;color:#000><img src=https://freshrimpsushi.github.io/en/logo/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D.png style=height:80px alt=logo></p></a></header><form method=get action=/en/search style=border:1px;text-align:center><div class=field><input type=text id=searchtext placeholder=🔍︎ class=input_text name=s style=background-color:#eee;text-align:center;width:200px;font-size:1.5rem;border:1px;border-radius:5px;padding-top:5px;padding-bottom:5px;margin-bottom:1.5rem></div></form><aside style=text-align:center;margin-bottom:1rem><a href=https://freshrimpsushi.github.io/ko//posts/3529/>한국어</a> |
<a href=https://freshrimpsushi.github.io/en//posts/3529/>English</a> |
<a href=https://freshrimpsushi.github.io/jp//posts/3529/>日本語</a></aside><div class=wrapper><div class=content><div class=content-box><title>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</title>
<a href=https://freshrimpsushi.github.io/en/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/ style=background-color:rgba(0,0,0,.8);color:orange;border-radius:10px;padding:5px>📂Machine Learning</a><h1>Adaptive Learning Rates: AdaGrad, RMSProp, Adam</h1><aside><div class=innerheader><div class=innertoc><b>Table of Contents</b><nav id=TableOfContents><ul><li><a href=#overview1-2>Overview</a></li><li><a href=#explanation>Explanation</a></li><li><a href=#adagrad>AdaGrad</a></li><li><a href=#rmsprop>RMSProp</a></li><li><a href=#adam>Adam</a></li></ul></nav></div></div></aside><h2 id=overview1-2>Overview<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></h2><p>Describe the adaptive learning rate used in gradient descent and the models that apply it: AdaGrad, RMSProp, and Adam.</p><h2 id=explanation>Explanation</h2><p>In <a href=../987>gradient descent</a>, the learning rate is an important parameter that determines how fast the parameter converges, how successful the method is, and so on. It is often written as $\alpha$, $\eta$, and determines how much of the gradient is taken into account when updating the parameter.</p><figure><img src=overshooting.png width=320><img src=undershooting.png width=320><figcaption align=middle>Optimization Patterns with Learning Rate: Large Learning Rate (Left), Small Learning Rate (Right)"</figcaption></figure><p>$$
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_{i} - \alpha \nabla L (\boldsymbol{\theta}_{i})
$$</p><p>In basic gradient descent, $\alpha$ is described as a constant, and in this case, <a href=../1010>gradient</a> is a vector, so the same learning rate applies to all variables (parameters) as follows</p><p>$$
\alpha \nabla L (\boldsymbol{\theta})
= \alpha \begin{bmatrix}
\dfrac{\partial L}{\partial \theta_{1}} & \dfrac{\partial L}{\partial \theta_{2}} & \cdots & \dfrac{\partial L}{\partial \theta_{k}}
\end{bmatrix}
= \begin{bmatrix}
\alpha\dfrac{\partial L}{\partial \theta_{1}} & \alpha\dfrac{\partial L}{\partial \theta_{2}} & \cdots & \alpha\dfrac{\partial L}{\partial \theta_{k}}
\end{bmatrix}
$$</p><p>So, by thinking of the learning rate as a vector like $\boldsymbol{\alpha} = (\alpha_{1}, \alpha_{2}, \dots, \alpha_{k})$, we can generalize the gradient term to the expression below.</p><p>$$
\boldsymbol{\alpha} \odot \nabla L (\boldsymbol{\theta})
= \begin{bmatrix}
\alpha_{1}\dfrac{\partial L}{\partial \theta_{1}} & \alpha_{2}\dfrac{\partial L}{\partial \theta_{2}} & \cdots & \alpha_{k}\dfrac{\partial L}{\partial \theta_{k}}
\end{bmatrix}
$$</p><p>where $\odot$ is the matrix&rsquo;s [Adamar product (componentwise product)] (../3436). The learning rate $\boldsymbol{\alpha}$, which varies from parameter to parameter in this way, is called the <strong>adaptive learning rate</strong>. Since the following techniques rely on the gradient to determine the adaptive learning rate, $\boldsymbol{\alpha}$ can be viewed as a function of $\boldsymbol{\alpha}$.</p><p>$$
\boldsymbol{\alpha} (\nabla L(\boldsymbol{\theta}))
= \begin{bmatrix}
\alpha_{1}(\nabla L(\boldsymbol{\theta})) & \alpha_{2}(\nabla L(\boldsymbol{\theta})) & \dots & \alpha_{k}(\nabla L(\boldsymbol{\theta}))
\end{bmatrix}
$$</p><p>Below we introduce AdaGrad, RMSProp, and Adam. It is important to note that there is no absolute winner among these optimizers, including the [momentum] (../3528) technique. Different fields and different tasks require different optimizers, so it&rsquo;s not a good idea to make a judgment or question about &ldquo;what&rsquo;s best&rdquo;. It&rsquo;s helpful to find out what&rsquo;s commonly used in your field, and if you don&rsquo;t have that or aren&rsquo;t sure, you can try <a href=../987>SGD</a>+Momentum or Adam.</p><h2 id=adagrad>AdaGrad</h2><p>AdaGrad is an adaptive learning rate technique introduced in the paper &ldquo;Adaptive subgradient methods for online learning and stochastic optimization&rdquo; by Duchi et al. (2011). The name is short for adaptive gradient and is pronounced as [AY-duh-grad] or [AH-duh-grad]. In AdaGrad, the learning rate for each parameter is set inversely proportional to the gradient. Consider the vector $\mathbf{r}$ as follows.</p><p>$$
\mathbf{r} = (\nabla L) \odot (\nabla L) = \begin{bmatrix}
\left( \dfrac{\partial L}{\partial \theta_{1}} \right)^{2} & \left( \dfrac{\partial L}{\partial \theta_{2}} \right)^{2} & \cdots & \left( \dfrac{\partial L}{\partial \theta_{k}} \right)^{2}
\end{bmatrix}
$$</p><p>For a global learning rate $\epsilon$, an arbitrary small number $\delta$, the adaptive learning rate $\boldsymbol{\alpha}$ is given by</p><p>$$
\boldsymbol{\alpha} = \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}}}
$$</p><p>As you can see from the formula, we apply a smaller learning rate for variables with larger components of the gradient and a larger learning rate for variables with smaller components of the gradient. The $\delta$, as you might guess, prevents the denominator from being $0$ or too small a number, and is often used between $10^{-5} \sim 10^{-7}$, and is often used for values between $10^{-5} and $10^{-7}. Also, the learning rate is cumulative from iteration to iteration. The gradient at the $i$th iteration is called $\nabla L _{i} = \nabla L (\boldsymbol{\theta}_{i})$,</p><p>$$
\begin{align}
\mathbf{r}_{i} &= (\nabla L_{i}) \odot (\nabla L_{i}) \nonumber \\
\boldsymbol{\alpha}_{i} &= \boldsymbol{\alpha}_{i-1} + \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}_{i}}} = \sum_{j=1}^{i} \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}_{j}}} \\
\boldsymbol{\theta}_{i+1} &= \boldsymbol{\theta}_{i} - \boldsymbol{\alpha}_{i} \odot \nabla L_{i} \nonumber
\end{align}
$$</p><div class=pseudocode-table><table><thead><tr><th>Algorithm: AdaGrad</th><th></th><th></th></tr></thead><tbody><tr><td>In</td><td>Global learning rate $\epsilon$, small positive value $\delta$, epochs $N$</td><td></td></tr><tr><td>1.</td><td>Initialize parameters $\boldsymbol{\theta}$ with random values.</td><td></td></tr><tr><td>2.</td><td>Initialize learning rates $\boldsymbol{\alpha} = \mathbf{0}$.</td><td></td></tr><tr><td>3.</td><td><strong>for $i = 1, \cdots, N$ do</strong></td><td></td></tr><tr><td>4.</td><td>　　 $\mathbf{r} \leftarrow \nabla L(\boldsymbol{\theta}) \odot \nabla L(\boldsymbol{\theta})$</td><td># Compute gradients and square each component</td></tr><tr><td>5.</td><td>　　 $\boldsymbol{\alpha} \leftarrow \boldsymbol{\alpha} + \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}}}$</td><td># Update adaptive learning rates</td></tr><tr><td>6.</td><td>　　 $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \boldsymbol{\alpha} \odot \nabla L(\boldsymbol{\theta})$</td><td># Update parameters</td></tr><tr><td>7.</td><td><strong>end for</strong></td><td></td></tr></tbody></table></div><h2 id=rmsprop>RMSProp</h2><p>RMSProp, short for <strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quare <strong>Prop</strong>agation, is an adaptive learning rate technique proposed in &rsquo;s lecture <a href=https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>Neural networks for machine learning</a>. It is basically a variant of AdaGrad, only with the addition of $(1)$ replaced by a [weighted sum] (../2470/#exponentially weighted average) so that the added term decreases exponentially. For $\rho \in (0,1)$,</p><p>$$
\boldsymbol{\alpha}_{i} = \rho \boldsymbol{\alpha}_{i-1} + (1-\rho) \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}_{i}}} = (1-\rho) \sum_{j=1}^{i} \rho^{i-j} \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}_{j}}}
$$</p><p>Large values such as $\rho = 0.9, 0.99$ are commonly used.</p><div class=pseudocode-table><table><thead><tr><th>Algorithm: RMSProp</th><th></th><th></th></tr></thead><tbody><tr><td>In</td><td>Global learning rate $\epsilon$, small positive value $\delta$, decay rate $\rho$, epochs $N$</td><td></td></tr><tr><td>1.</td><td>Initialize parameters $\boldsymbol{\theta}$ with random values.</td><td></td></tr><tr><td>2.</td><td>Initialize learning rates $\boldsymbol{\alpha} = \mathbf{0}$.</td><td></td></tr><tr><td>3.</td><td><strong>for $i = 1, \dots, N$ do</strong></td><td></td></tr><tr><td>4.</td><td>　　 $\mathbf{r} \leftarrow \nabla L(\boldsymbol{\theta}) \odot \nabla L(\boldsymbol{\theta})$</td><td># Compute gradients and square each component</td></tr><tr><td>5.</td><td>　　 $\boldsymbol{\alpha} \leftarrow \rho \boldsymbol{\alpha} + (1-\rho) \dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}}}$</td><td># Update adaptive learning rates</td></tr><tr><td>6.</td><td>　　 $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \boldsymbol{\alpha} \odot \nabla L(\boldsymbol{\theta})$</td><td># Update parameters</td></tr><tr><td>7.</td><td><strong>end for</strong></td><td></td></tr></tbody></table></div><h2 id=adam>Adam</h2><p>Adam is an optimizer introduced in the paper "Adam: A method for stochastic optimization" (Kingma and Ba, 2014). It combines the adaptive learning rate and the <a href=../3528>momentum</a> technique and can be thought of as RMSProp + momentum. Once you understand RMSProp and momentum, it&rsquo;s not hard to understand Adam. Here&rsquo;s how RMSProp, momentum, and Adam compare to each other If $\nabla L_{i} = \nabla L(\boldsymbol{\theta}_{i})$,</p><table width=724 cellpadding=10><tr><td align=middle width=125><b>Momentum</b></td><td>$\mathbf{p}_{i} = \beta_{1}\mathbf{p}_{i-1} + \nabla L_{i} \quad (\mathbf{p}_{0}=\mathbf{0}) \\
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_{i} - \alpha \mathbf{p}_{i}$</td></tr><tr><td align=middle width=125><b>RMSProp</b></td><td align=left>$\mathbf{r}_{i} = \nabla L_{i} \odot \nabla L_{i} \\
\boldsymbol{\alpha}_{i} = \beta_{2} \boldsymbol{\alpha}_{i-1} + (1-\beta_{2})\dfrac{\epsilon}{\delta + \sqrt{\mathbf{r}_{i}}} \quad (\boldsymbol{\alpha}_{0}=\mathbf{0})\\
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_{i} - \boldsymbol{\alpha}_{i} \odot \nabla L_{i}$</td></tr><tr><td align=middle><b>Adam</b></td><td align=left>$\mathbf{p}_{i} = \beta_{1}\mathbf{p}_{i-1} + (1-\beta_{1}) \nabla L_{i-1} \quad (\mathbf{p}_{0}=\mathbf{0}) \\\\[0.5em]
\hat{\mathbf{p}}_{i} = \dfrac{\mathbf{p}_{i}}{1-(\beta_{1})^{i}} \\\\[0.5em]
\mathbf{r}_{i} = \beta_{2} \mathbf{r}_{i-1} + (1-\beta_{2}) \nabla L_{i} \odot \nabla L_{i} \\\\[0.5em]
\hat{\mathbf{r}}_{i} = \dfrac{\mathbf{r}}{1-(\beta_{2})^{i}} \\\\[0.5em]
\hat{\boldsymbol{\alpha}}_{i} = \dfrac{\epsilon}{\delta + \sqrt{\hat{\mathbf{r}}_{i}}} \\\\[0.5em]
\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_{i} - \hat{\boldsymbol{\alpha}_{i}} \odot \hat{\mathbf{p}_{i}}
$</td></tr></table><p>The reason we divide $1 - \beta^{i}$ when calculating $\hat{\mathbf{p}}_{i}$ and $\hat{\mathbf{r}}_{i}$ is to make them <a href=../2470/#%EC%A7%80%EC%88%98%EA%B0%80%EC%A4%91%ED%8F%89%EA%B7%A0>weighted averages</a> since $\mathbf{p}_{i}$ and $\mathbf{r}_{i}$ are <a href=../2470/##exponential-weighted-averages>weighted sums</a>.</p><div class=pseudocode-table><table><th colspan=3, align=left>Algorithm: Adam</th><tbody><tr><td>In</td><td>Global learning rate $\epsilon$ (recommended value is $0.001$), epochs $N$</td></tr><tr><td></td><td>Small constant $\delta$ (recommended value is $10^{-8}$)</td></tr><tr><td></td><td>Decay rates $\beta_{1}, \beta_{2}$ (recommended values are $0.9$ and $0.999$ respectively)</td></tr><tr><td>1.</td><td>Initialize parameters $\boldsymbol{\theta}$ with random values.</td></tr><tr><td>2.</td><td>Initialize learning rates $\boldsymbol{\alpha} = \mathbf{0}$.</td></tr><tr><td>3.</td><td>Initialize momentum $\mathbf{p} = \mathbf{0}$.</td></tr><tr><td>4.</td><td><b>for $i = 1, \dots, N$ do</b></td></tr><tr><td>5.</td><td>　　 $\mathbf{p} \leftarrow \beta_{1}\mathbf{p} + (1-\beta_{1}) \nabla L$</td><td># Update momentum using weighted sum</td></tr><tr><td>6.</td><td>　　 $\hat{\mathbf{p}} \leftarrow \dfrac{\mathbf{p}}{1-(\beta_{1})^{i}}$</td><td># Correct with weighted average</td></tr><tr><td>7.</td><td>　　 $\mathbf{r} \leftarrow \beta_{2} \mathbf{r} + (1-\beta_{2}) \nabla L \odot \nabla L$</td><td># Update gradient square vector with weighted sum</td></tr><tr><td>8.</td><td>　　 $\hat{\mathbf{r}} \leftarrow \dfrac{\mathbf{r}}{1-(\beta_{2})^{i}}$</td><td># Correct with weighted average</td></tr><tr><td>9.</td><td>　　 $\hat{\boldsymbol{\alpha}} \leftarrow \dfrac{\epsilon}{\delta + \sqrt{\hat{\mathbf{r}}}}$</td><td># Update adaptive learning rates</td></tr><tr><td>10.</td><td>　　 $\boldsymbol{\theta} = \boldsymbol{\theta} - \hat{\boldsymbol{\alpha}} \odot \hat{\mathbf{p}}$</td><td># Update parameters</td></tr><tr><td>11.</td><td><b>end for</b></td></tr></tbody></table></div><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Ian Goodfellow, <em>Deep Learning</em>, 8.5 Algorithms with Adaptive Learning Rates&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>오일석, <em>기계 학습(MACHINE LEARNING)</em> (2017), ch 5.4 적응적 학습률&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><aside style=text-align:right>2023-12-29&emsp;
전기현&emsp;
<a href=../2042>🎲 3529</a></aside><script>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="Adaptive Learning Rates: AdaGrad, RMSProp, Adam",c="https://freshrimpsushi.github.io/en/posts/3529/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script></div><aside><h2>Comment</h2><div class=area-reply><div class=list-reply></div></div><div class=write-box><div class="write-author-info box-account"><input type=hidden name=cmt_idx>
<input class=ai-author type=text placeholder=Name>
<input class=ai-password type=password maxlength=20 placeholder=Password></div><div class=write-content><textarea class=ai-content value placeholder='Feel free to ask in english' style=ime-mode:active></textarea></div><button class=write-button onclick=write_comment() aria-label=send><i class='fa-solid fa-paper-plane'></i></button><aside class=tex>$\TeX$ is also applied to comments.</aside></div></aside><script>let commentRows=[];const listReply=document.querySelector(".list-reply");document.addEventListener("DOMContentLoaded",()=>{get_all_comment()});function get_all_comment(){fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=getAllComment";return postFetch(t,{board_idx:"3529",user_ip:e})}).then(e=>e.json()).then(e=>{e.ok&&(commentRows=e.rows,render_comment(commentRows))}).catch(e=>console.error(e))}function render_comment(e){const n=["류대식","전기현","ㅇㅇ","질문"],s=["류대식","전기현"];render_blank();let t="";e.map(e=>{t+=`<div id="comment${e.cmt_idx}" class="parents">`,t+=`<div class="content-info">`;let o="";e.cmt_cnt>125?o="🥇":e.cmt_cnt>25?o="🥈":e.cmt_cnt>5&&(o="🥉"),n.includes(e.author)&&(o="");let i="";e.ip_address==null?i="(-)":(ipParts=e.ip_address.split("."),i=`(${ipParts[0]}.${ipParts[1]})`),s.includes(e.author)&&(i=""),t+=`<div class="list-author">${o} ${e.author} ${i}</div>`,t+=`<sup class="list-date">${e.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,e.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> Waiting for approvement</div>`);let a=e.content;a=a.replace(/\n/g,"<br />"),t+=`<div class="content-text">${a} <span class="re-comment-button" onclick="re_comment('${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`,e.child.map(o=>{let c="child";e.ip_address&&o.ip_address===e.ip_address&&(c="parentself"),t+=`<div id="comment${o.cmt_idx}" class="${c}">`,t+=`<div class="content-info">`;let i="";o.cmt_cnt>5?i="🥉":o.cmt_cnt>25?i="🥈":o.cmt_cnt>125&&(i="🥇"),n.includes(o.author)&&(i="");let a="";o.ip_address==null?a="(-)":a=`(${o.ip_address})`,s.includes(o.author)&&(a=""),t+=`<div class="list-author">${i} ${o.author} ${a}</div>`,t+=`<sup class="list-date">${o.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,o.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> Waiting for approvement</div>`);let r=o.content;r=r.replace(/\n/g,"<br />"),t+=`<div class="content-text">${r} <span class="re-comment-button" onclick="re_comment('${o.cmt_idx}', '${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`})}),listReply.innerHTML=t,renderKaTex(listReply),location.href.indexOf("#")!=-1&&(location.href=location.href.substr(location.href.indexOf("#")))}function render_blank(){listReply.innerHTML=""}function write_comment(){const e=document.querySelector(".ai-author"),t=document.querySelector(".ai-password"),n=document.querySelector(".ai-content"),s=e.value,o=t.value,i=n.value;if(s===""||o===""||i===""){alert("빈칸을 채워주세요");return}const a="3529",r="",c="Adaptive Learning Rates: AdaGrad, RMSProp, Adam";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeComment";return postFetch(t,{board_idx:a,board_slug:r,board_title:c,author:s.replace(/\+/g,"%2B"),password:o,content:i.replace(/\+/g,"%2B"),user_ip:e})}).then(e=>e.json()).then(s=>{s.ok?(commentRows.push(s.row),render_comment(commentRows),e.value="",t.value="",n.value=""):s.status===606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),e.value="",t.value="",n.value=""):s.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}async function update_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 수정",input:"password",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(s=>{if(s.ok){const s=document.querySelector(`#comment${e}`),o="https://freshrimpsushi.com/blog/ajax/comment.php?action=getComment";postFetch(o,{cmt_idx:e}).then(e=>e.json()).then(o=>{if(o.ok){const a=o.row,r=a.author,c=a.content;let i="";i+=`<div class="update-author-info">`,i+=`<input class="update-author" type="text" value="${r}" placeholder="이름" />`,i+=`<input class="update-password" type="password" value="${n}" placeholder="비밀번호" disabled />`,i+="</div>",i+=`<textarea class="update-content" value="" style="IME-MODE:active;">${c}</textarea>`,i+=`<input class="update_comment-button" type="submit" value="수정" onclick="update_comment_click(${e}, '${t}')" />`,i+=`<input class="update_comment-button" type="submit" value="취소" onclick="cancel_click(${e})" />`,s.innerHTML=i}}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}async function delete_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 삭제",text:"삭제하면 되돌릴 수 없습니다.",input:"password",icon:"warning",showCancelButton:!0,confirmButtonColor:"#3085d6",cancelButtonColor:"#d33",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(n=>{if(n.ok){const n="https://freshrimpsushi.com/blog/ajax/comment.php?action=deleteComment";postFetch(n,{cmt_idx:e}).then(e=>e.json()).then(n=>{Swal.fire("삭제되었습니다."),t==="parents"?commentRows=commentRows.filter(t=>t.cmt_idx!==e):t==="child"&&commentRows.map(t=>{t.child=t.child.filter(t=>t.cmt_idx!==e),t.child_cnt=t.child.length}),render_comment(commentRows)}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}function update_comment_click(e,t){const i=document.querySelector(`#comment${e} .update-author`),a=document.querySelector(`#comment${e} .update-password`),r=document.querySelector(`#comment${e} .update-content`),n=i.value,s=a.value,o=r.value;(n===""||s===""||o==="")&&alert("빈칸을 채워주세요");const c="https://freshrimpsushi.com/blog/ajax/comment.php?action=updateComment";postFetch(c,{cmt_idx:e,author:n.replace(/\+/g,"%2B"),password:s,content:o.replace(/\+/g,"%2B")}).then(e=>e.json()).then(n=>{n.ok&&(Swal.fire("수정되었습니다."),t==="parents"?commentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}):t==="child"&&commentRows.map(t=>{t.cmt_idx==n.parent_cmt_idx&&t.child.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)})}),render_comment(commentRows),recentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}),render_recent_comment(recentRows))}).catch(e=>console.error(e))}function cancel_click(){render_comment(commentRows)}function re_comment(e,t){const s=document.querySelector(`#comment${e} .re-comment`);let n="";n+='<div class="re-comment-box">',n+='<div class="re-comment-author-info">',n+='<input class="re-comment-author" type="text" value="" placeholder="이름" />',n+='<input class="re-comment-password" type="password" value="" placeholder="비밀번호" />',n+="</div>",n+='<textarea class="re-comment-content" placeholder="내용" value="" style="IME-MODE:active;"></textarea>',t===void 0?n+=`<button class="write-button" onclick="re_comment_click(${e})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`:n+=`<button class="write-button" onclick="re_comment_click(${e}, ${t})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`,n+=`<button class="write-button" onclick="cancel_click(${e})" value="close"><i class='fa-solid fa-solid fa-ban'></i></button>`,n+="</div>",s.innerHTML=n}function re_comment_click(e,t){const n=document.querySelector(`#comment${e} .re-comment-author`),s=document.querySelector(`#comment${e} .re-comment-password`),o=document.querySelector(`#comment${e} .re-comment-content`),i=n.value,a=s.value,r=o.value;if(i===""||a===""||r===""){alert("빈칸을 채워주세요");return}const c="3529",l="",d="Adaptive Learning Rates: AdaGrad, RMSProp, Adam";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(n=>{const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeReComment";return postFetch(s,{board_idx:c,board_slug:l,board_title:d,cmt_idx:t===void 0?e:t,author:i.replace(/\+/g,"%2B"),password:a,content:r.replace(/\+/g,"%2B"),user_ip:n})}).then(e=>e.json()).then(i=>{i.ok?(commentRows.map(n=>{t===void 0?n.cmt_idx==e&&n.child.push(i.row):n.cmt_idx==t&&n.child.push(i.row)}),render_comment(commentRows)):i.status==606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),n.value="",s.value="",o.value=""):i.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}</script></div><aside class=sidebar><aside><style>.reddot a:link{color:#67d10f;text-shadow:0 0 10px #d3d3d3;font-weight:700}.reddot a:visited{color:#f5f5f5}</style><p class=reddot style=text-align:center;margin:0><a href=https://freshrimpsushi.github.io/en/posts/2707/>Summer Special Omakase<br>「Imaginary Numbers」</a></p></aside><br><div class=category></div><div style=display:flex>Click ● to highlight only you interested.<div class=resetmute><button class="sb-btn btnReset" style=border-radius:5px;border:0>reset</button>
<button class="sb-btn btnMute" style=border-radius:5px;border:0>mute</button></div></div><script defer>const color={green:"#33cc33",yellow:"#ffcc00",red:"#ff3333",black:"#000000"},categoryRows=[[{idx:1,name:"함수",color:color.green,show:"Functions",size:"146"},{idx:2,name:"보조정리",color:color.green,show:"Lemmas",size:"55"},{idx:3,name:"미분적분학",color:color.green,show:"Calculus",size:"45"},{idx:4,name:"행렬대수",color:color.green,show:"Matrix Algebra",size:"117"}],[{idx:1,name:"정수론",color:color.green,show:"Number Theory",size:"90"},{idx:2,name:"집합론",color:color.green,show:"Set Theory",size:"49"},{idx:3,name:"그래프이론",color:color.green,show:"Graph Theory",size:"65"},{idx:4,name:"선형대수",color:color.green,show:"Linear Algebra",size:"97"},{idx:5,name:"해석개론",color:color.green,show:"Analysis",size:"84"},{idx:6,name:"추상대수",color:color.green,show:"Abstract Algebra",size:"98"},{idx:7,name:"위상수학",color:color.green,show:"Topology",size:"64"},{idx:8,name:"기하학",color:color.green,show:"Geometry",size:"167"}],[{idx:1,name:"다변수벡터해석",color:color.green,show:"Vector Analysis",size:"37"},{idx:2,name:"복소해석",color:color.green,show:"Complex Anaylsis",size:"71"},{idx:3,name:"측도론",color:color.green,show:"Measure Theory",size:"53"},{idx:4,name:"푸리에해석",color:color.green,show:"Fourier Analysis",size:"54"},{idx:5,name:"표현론",color:color.red,show:"Representation Theory",size:"7"},{idx:6,name:"초함수론",color:color.green,show:"Distribution Theory",size:"22"},{idx:7,name:"단층촬영",color:color.green,show:"Tomography",size:"20"}],[{idx:1,name:"거리공간",color:color.green,show:"Metric Space",size:"38"},{idx:2,name:"바나흐공간",color:color.green,show:"Banach Space",size:"38"},{idx:3,name:"힐베르트공간",color:color.green,show:"Hilbert Space",size:"31"},{idx:4,name:"르벡공간",color:color.green,show:"Lebesgue Space",size:"33"}],[{idx:1,name:"상미분방정식",color:color.green,show:"ODE",size:"58"},{idx:2,name:"편미분방정식",color:color.green,show:"PDE",size:"60"},{idx:3,name:"확률미분방정식",color:color.green,show:"SDE",size:"26"}],[{idx:1,name:"줄리아",color:color.green,show:"Julia",size:"233"},{idx:2,name:"알고리즘",color:color.green,show:"Algorithm",size:"28"},{idx:3,name:"수치해석",color:color.green,show:"Numerical Analysis",size:"63"},{idx:4,name:"최적화이론",color:color.green,show:"Optimization Theory",size:"37"},{idx:5,name:"머신러닝",color:color.green,show:"Machine Learning",size:"114"},{idx:6,name:"프로그래밍",color:color.yellow,show:"Programming",size:"121"},{idx:7,name:"세이버메트릭스",color:color.green,show:"Sabermetrics",size:"233",size:"13"}],[{idx:1,name:"물리학",color:color.green,show:"Physics",size:"30"},{idx:2,name:"수리물리",color:color.green,show:"Mathematical Physics",size:"77"},{idx:3,name:"고전역학",color:color.green,show:"Classical Mechanics",size:"48"},{idx:4,name:"전자기학",color:color.green,show:"Electrodynamics",size:"51"},{idx:5,name:"양자역학",color:color.green,show:"Quantum Mechanics",size:"57"},{idx:6,name:"열물리학",color:color.green,show:"Thermal Physics",size:"29"}],[{idx:1,name:"R",color:color.green,show:"R",size:"54"},{idx:2,name:"데이터확보",color:color.green,show:"Data Sets",size:"29"},{idx:3,name:"데이터과학",color:color.green,show:"Data Science",size:"41"},{idx:4,name:"통계적검정",color:color.green,show:"Statistical Test",size:"33"},{idx:5,name:"통계적분석",color:color.green,show:"Statistical Analysis",size:"76"},{idx:6,name:"수리통계학",color:color.green,show:"Mathematical Statistics",size:"123"},{idx:7,name:"확률분포론",color:color.green,show:"Probability Distribution",size:"84"},{idx:8,name:"확률론",color:color.green,show:"Probability Theory",size:"80"},{idx:9,name:"위상데이터분석",color:color.green,show:"TDA",size:"40"}],[{idx:1,name:"논문작성",color:color.red,show:"Writing",size:"63"},{idx:2,name:"생새우초밥지",color:color.black,show:"JOF",size:"7"}]];document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("blindList"));(e==""||e==null||e==null||e==0||e==NaN)&&localStorage.setItem("blindList",null),render_category(categoryRows),blind_category(e);const t=document.querySelector(".btnReset");t.addEventListener("click",()=>{let e=new Array;localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)});const n=document.querySelector(".btnMute");n.addEventListener("click",()=>{let e=new Array;for(let t=0;t<categoryRows.length;t++)categoryRows[t].map(n=>{const s={mainIdx:t,subIdx:n.idx};e.push(s)});localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)})});function render_category(e){const n=document.querySelector(".category");let t="";for(let n=0;n<e.length;n++)e[n].map(e=>{t+=`<span id="cate${n}-${e.idx}" class="cate etewsert">`,t+=`<span onclick="check_blind(${n}, ${e.idx});" style="cursor: pointer; color: ${e.color}">●</span>`,t+=`<a href="https://freshrimpsushi.github.io/en/categories/${e.name.toLowerCase()}/">`,t+=` ${e.show} (${e.size})</a>`,t+="</span>",screen.width>954?t+="<br>":t+=" "}),t+="<hr>";n.innerHTML=t}function check_blind(e,t){const o=document.querySelector(`#cate${e}-${t}`);let n=new Array;const i={mainIdx:e,subIdx:t};let s=JSON.parse(localStorage.getItem("blindList"));if(s==""||s==null||s==null||s==0||s==NaN)n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind";else{n=s;let a=null;for(let s=0;s<n.length;s++)if(n[s].mainIdx==e&&n[s].subIdx==t){a=n.filter(n=>n.mainIdx!=e||n.subIdx!=t);break}a===null?(n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind"):(localStorage.setItem("blindList",JSON.stringify(a)),o.className="cate")}}function blind_category(e){const t=document.querySelectorAll(".cate");t.forEach(e=>{e.className="cate"}),e!==null&&e.map(e=>{const t=document.querySelector(`#cate${e.mainIdx}-${e.subIdx}`);t.className+=" blind"})}</script><br><br><b>Viewed posts</b><div class=lately-viewed-list style=padding-left:4px></div><script defer>document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("latelyViewPostList")),n=document.querySelector(".lately-viewed-list");let t="";if(e==""||e==null||e==null||e==0||e==NaN)localStorage.setItem("latelyViewPostList",null),t+='<div class="lv-list">',t+=" · 열람한 포스트가 없습니다.",t+="</div>",n.innerHTML=t;else{for(let n=e.length-1;n>=0;n--)t+='<div class="lv-list">',t+=`<a href="${e[n].link}"> · ${e[n].title}</a>`,t+="</div>";n.innerHTML=t}})</script><script defer>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="Adaptive Learning Rates: AdaGrad, RMSProp, Adam",c="https://freshrimpsushi.github.io/en/posts/3529/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script><br><b>Recent comment</b><div class=current-reply></div><script defer>const url="https://freshrimpsushi.com/blog/ajax/recent_comment.php?action=getCurrentComment";let recentRows=[];fetch(url).then(e=>e.json()).then(e=>{e.ok&&(recentRows=e.rows,render_recent_comment(recentRows))}).catch(e=>console.error(e));function render_recent_comment(e){const n=document.querySelector(".current-reply");let t="";e.map(e=>{let n="https://freshrimpsushi.github.io/en/";e.board_idx>-1?n+=`posts/${e.board_idx}#comment${e.cmt_idx}`:e.board_idx==-1?n+=`#comment${e.cmt_idx}`:n+=`categories/${e.board_title}#comment${e.cmt_idx}`,t+='<div class="current-reply-list">',t+=`<a href="${n}">`,t+=`<b> - ${e.author}</b>: `,t+=`${e.content}`,t+=`</a>`,t+=`</div>`}),n.innerHTML=t,renderKaTex(n)}</script><br></aside></div><footer><aside><div><p id=mirror-link style=text-align:center><a style=cursor:text href=http://localhost:1313//en/posts/3529/>© FreshrimpRestaurant / Powered by 류대식, 전기현</a><br>Contact:
<img src=https://freshrimpsushi.github.io/en/logo/gmail.png width=12px alt=mail> freshrimpsushi@gmail.com
<a href=https://freshrimpsushi.github.io/en/index.xml><img src=https://freshrimpsushi.github.io/en/logo/RSS.png width=12px alt=RSS> RSS</a></p></div><script type=text/javascript>var goIndex=function(){var e=document.getElementsByName("idx")[0].value,t="https://freshrimpsushi.github.io/en/posts/"+e;location.replace(t)};document.addEventListener("keydown",function(e){const n=document.getElementById("navigator");if(e.altKey&&e.ctrlKey&&e.key==="l"){e.preventDefault();var t=document.querySelector("#mirror-link a");t?t.click():console.log("거울 링크를 찾지 못했습니다.")}})</script></aside></footer></body><script async src="https://www.googletagmanager.com/gtag/js?id=G-NLV8Y9PRK1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NLV8Y9PRK1")</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4751085325232621" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/sweetalert2@11></script><script src=https://freshrimpsushi.github.io/en/js/fontawsome.min.js></script><script src=https://freshrimpsushi.github.io/en/js/common.js></script><script>document.addEventListener("DOMContentLoaded",()=>{fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/ip_checker.php";return postFetch(t,{user_ip:e})}).then(e=>e.json()).then(e=>{e.ok||(alert(`차단된 IP입니다.
Contact:
freshrimpsushi@gmail.com`),window.location.href="https://google.com")}).catch(e=>console.error(e))})</script></html>