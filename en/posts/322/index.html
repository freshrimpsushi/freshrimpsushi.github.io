<!doctype html><html class=blog lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=preload href=https://freshrimpsushi.github.io/en/css/style.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=preload href=https://freshrimpsushi.github.io/en/css/comment.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=icon href=https://freshrimpsushi.github.io/ko/logo/favicon.ico><meta name=msapplication-TileColor content="#FFFFFF"><meta name=msapplication-TileImage content="logo/basic.png"><meta name=NaverBot content="All"><meta name=NaverBot content="index,follow"><meta name=Yeti content="All"><meta name=Yeti content="index,follow"><meta name=google-site-verification content="KYAokS7-6C5YuXOjatJQsiK1T0O8x4YncYFIF4tneYI"><meta name=naver-site-verification content="e5651d6f97899061897203413efc84994f04bbba"><link rel=alternate type=application/rss+xml title=FreshrimpRestaurant href=https://freshrimpsushi.github.io/en/index.xml><title>Paper Review: Kolmogorov-Arnold Neural Network (KAN)</title></head><meta name=title content="Paper Review: Kolmogorov-Arnold Neural Network (KAN)"><meta name=description content="Íµ≠ÎÇ¥ ÏµúÎåÄÏùò ÏàòÌïô, Î¨ºÎ¶¨Ìïô, ÌÜµÍ≥ÑÌïô Î∏îÎ°úÍ∑∏"><meta property="og:title" content="Paper Review: Kolmogorov-Arnold Neural Network (KAN)"><meta property="og:description" content><meta property="og:image" content="https://freshrimpsushi.github.io/en/logo/basic.png/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"r_","name":"Paper Review: Kolmogorov-Arnold Neural Network (KAN)","headline":"Paper Review: Kolmogorov-Arnold Neural Network (KAN)","alternativeHeadline":"","description":"Overview and Summary Kolmogorov‚ÄìArnold Networks (KAN) are neural networks inspired by the Kolmogorov‚ÄìArnold representation theorem. Despite the idea having been discussed for decades, authors like Girosi and Poggio pointed out in the 1989 paper \u0026lsquo;Representation Properties of Networks:","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/freshrimpsushi.github.io\/en\/posts\/322\/"},"author":{"@type":"Person","name":"Î•òÎåÄÏãù","url":"https://math.stackexchange.com/users/459895/ryu-dae-sick"},"creator":{"@type":"Person","name":"Î•òÎåÄÏãù"},"accountablePerson":{"@type":"Person","name":"Î•òÎåÄÏãù"},"copyrightHolder":"FreshrimpRestaurant","copyrightYear":"2024","dateCreated":"2024-09-29T00:00:00.00Z","datePublished":"2024-09-29T00:00:00.00Z","dateModified":"2024-09-29T00:00:00.00Z","publisher":{"@type":"Organization","name":"FreshrimpRestaurant","url":"https://freshrimpsushi.github.io/en/","logo":{"@type":"ImageObject","url":"https:\/\/freshrimpsushi.github.io\/en\/logo\/basic.png","width":"32","height":"32"}},"image":"https://freshrimpsushi.github.io/en/logo/basic.png","url":"https:\/\/freshrimpsushi.github.io\/en\/posts\/322\/","wordCount":"2609","genre":[],"keywords":[]}</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/mhchem.min.js integrity=sha384-F2ptQFZqNJuqfGGl28mIXyQ5kXH48spn7rcoS0Y9psqIKAcZPLd1NzwFlm/bl1mH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>function renderKaTex(e){renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],trust:!0,trust:e=>["\\htmlId","\\href","\\includegraphics"].includes(e.command),macros:{"\\eqref":"(\\text{#1})","\\ref":"\\href{###1}{\\text{#1}}","\\label":"\\htmlId{#1}{}","\\sech":"\\operatorname{sech}","\\csch":"\\operatorname{csch}","\\sgn":"\\operatorname{sgn}","\\sign":"\\operatorname{sign}","\\sinc":"\\operatorname{sinc}","\\diag":"\\operatorname{diag}","\\diam":"\\operatorname{diam}","\\Tr":"\\operatorname{Tr}","\\tr":"\\operatorname{tr}","\\re":"\\operatorname{Re}","\\im":"\\operatorname{Im}","\\Var":"\\operatorname{Var}","\\Poi":"\\operatorname{Poi}","\\Cov":"\\operatorname{Cov}","\\span":"\\operatorname{span}","\\supp":"\\operatorname{supp}","\\rank":"\\operatorname{rank}","\\nullity":"\\operatorname{nullity}","\\Ric":"\\operatorname{Ric}","\\i":"\\mathrm{i}","\\d":"\\mathrm{d}","\\cR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5863.png}","\\acR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.4371.png}","\\bcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5899.png}","\\abcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.0596.png}","\\crH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\smallcrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\acrH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}","\\smallacrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}"},throwOnError:!1})}</script><body class=main><header><a href=https://freshrimpsushi.github.io/en/ rel=home><p style=text-align:center;font-size:1rem;color:#000><img src=https://freshrimpsushi.github.io/en/logo/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D.png style=height:80px alt=logo></p></a></header><form method=get action=/en/search style=border:1px;text-align:center><div class=field><input type=text id=searchtext placeholder=üîçÔ∏é class=input_text name=s style=background-color:#eee;text-align:center;width:200px;font-size:1.5rem;border:1px;border-radius:5px;padding-top:5px;padding-bottom:5px;margin-bottom:1.5rem></div></form><aside style=text-align:center;margin-bottom:1rem><a href=https://freshrimpsushi.github.io/ko//posts/322/>ÌïúÍµ≠Ïñ¥</a> |
<a href=https://freshrimpsushi.github.io/en//posts/322/>English</a> |
<a href=https://freshrimpsushi.github.io/jp//posts/322/>Êó•Êú¨Ë™û</a></aside><div class=wrapper><div class=content><div class=content-box><title>Paper Review: Kolmogorov-Arnold Neural Network (KAN)</title>
<a href=https://freshrimpsushi.github.io/en/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/ style=background-color:rgba(0,0,0,.8);color:orange;border-radius:10px;padding:5px>üìÇMachine Learning</a><h1>Paper Review: Kolmogorov-Arnold Neural Network (KAN)</h1><aside><div class=innerheader><div class=innertoc><b>Table of Contents</b><nav id=TableOfContents><ul><li><a href=#overview-and-summary>Overview and Summary</a></li><li><a href=#0-abstract>0. Abstract</a></li><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-kolmogorovarnold-networks-kan>2. Kolmogorov‚ÄìArnold Networks (KAN)</a><ul><li><a href=#21-kolmogorov-arnold-representation-theorem>2.1 Kolmogorov-Arnold Representation Theorem</a></li><li><a href=#22-kan-architecture>2.2 KAN Architecture</a></li><li><a href=#23-kans-approximation-abilities-and-scaling-laws>2.3 KAN&rsquo;s Approximation Abilities and Scaling Laws</a></li><li><a href=#24-for-accuracy-grid-extension>2.4 For Accuracy: Grid Extension</a></li><li><a href=#25-for-interpretability-simplifying-kans-and-making-them-interactive>2.5 For Interpretability: Simplifying KANs and Making them Interactive</a></li></ul></li><li><a href=#3-kans-are-accurate>3. KANs are Accurate</a><ul><li><a href=#33-feynman-datasets>3.3 Feynman Datasets</a></li><li><a href=#34-solving-partial-differential-equations>3.4 Solving Partial Differential Equations</a></li></ul></li><li><a href=#4-kans-are-interpretable>4. KANs are Interpretable</a><ul><li><a href=#43-application-to-mathematics-knot-theory>4.3 Application to Mathematics: Knot Theory</a></li><li><a href=#44-application-to-physics-anderson-localization>4.4 Application to Physics: Anderson Localization</a></li></ul></li><li><a href=#5-related-works>5. Related Works</a></li><li><a href=#6-discussion>6. Discussion</a><ul><li><a href=#final-takeaway-should-i-use-kans-or-mlps>Final Takeaway: Should I Use KANs or MLPs?</a></li></ul></li><li><a href=#epilogue>Epilogue</a></li></ul></nav></div></div></aside><h2 id=overview-and-summary>Overview and Summary</h2><p>Kolmogorov‚ÄìArnold Networks (KAN) are neural networks inspired by the <strong>Kolmogorov‚ÄìArnold representation theorem</strong>. Despite the idea having been discussed for decades, authors like <strong>Girosi</strong> and <strong>Poggio</strong> pointed out in the 1989 paper &lsquo;Representation Properties of Networks: Kolmogorov&rsquo;s Theorem Is Irrelevant&rsquo;<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> that there is not much direct correlation between the theorem and neural networks. Nonetheless, naming these networks after mathematicians emphasizes their strong theoretical foundation. The paper, submitted to ArXiv<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> in April &lsquo;24, has already been cited nearly 200 times by September 27th, &lsquo;24, which indicates some degree of success.</p><p>The concept behind KAN is solid: to rival the modern and dominant MLP in AI by breaking the limitations of the black-box nature of existing MLPs and deep learning. Clear understanding and transparency are crucial, especially for applications in finance, healthcare, space, and military, where research has already shown quite novel approaches.</p><p>However, it&rsquo;s necessary to be cautious about claims related to model performance and learning speed, as some parts of the mathematical formalization seem somewhat haphazard.</p><h2 id=0-abstract>0. Abstract</h2><p><img src=image-1.png#center alt="alt text"></p><p>KAN draws inspiration from the Kolmogorov-Arnold representation theorem and differs from traditional MLPs, which use predefined <a href=../991>activation functions</a> to update <a href=../2470>weights</a> at nodes. Instead, KAN aims to modify the activation function itself through learning while updating weights on links. The core theme of KAN is <strong>interpretability</strong>, suggesting that in several cases proposed by the authors, KAN could collaborate with mathematicians and physicists and prove to be a promising alternative to MLPs.</p><h2 id=1-introduction>1. Introduction</h2><p>The major criticism of MLPs is their lack of interpretability. While MLPs developed theoretically on the <a href=../1853>Universal Approximation Theorem</a>, KAN is based on the Kolmogorov-Arnold representation theorem. MLPs simulate the brain by applying nonlinear functions to <a href=../512>linear combinations</a> of data, and although the computations can be complex, GPUs can process these tasks using <a href=../1955>matrix</a> operations, driving significant advancements. Conversely, KAN&rsquo;s basic setup involves a $1$-dimensional function, more specifically learning a model through <a href=../1036>splines</a>. This naturally raises concerns like &lsquo;Wouldn&rsquo;t the computations take too long?&rsquo; and &lsquo;Can&rsquo;t we use GPUs?&rsquo;, but typically, KAN requires much smaller network sizes compared to MLPs for similar performance.</p><p>Efforts to build neural networks using the Kolmogorov-Arnold representation theorem previously struggled because modern techniques couldn&rsquo;t easily adhere to the rigorously specified conditions of the theorem. The authors introduce structural innovations to address this, though they admit that KAN essentially combines splines and MLP concepts.</p><p>Despite repeated claims from the authors that KAN overcomes the <strong>curse of dimensionality</strong>, this point remains contentious. KAN showcases various problems and appeals to mathematicians and scientists, but it‚Äôs still hard to see it dethrone the engineering and industrial dominance of existing neural network models, suggesting its primary focus remains basic science. The code for this research is available on the GitHub repository <a href=https://github.com/KindXiaoming/pykan>https://github.com/KindXiaoming/pykan</a>.</p><h2 id=2-kolmogorovarnold-networks-kan>2. Kolmogorov‚ÄìArnold Networks (KAN)</h2><p>Reemphasizing the relationship between MLPs and the Universal Approximation Theorem, and KANs and the Kolmogorov-Arnold representation theorem.</p><h3 id=21-kolmogorov-arnold-representation-theorem>2.1 Kolmogorov-Arnold Representation Theorem</h3><p>The Kolmogorov-Arnold representation theorem states that a <a href=../432>continuous</a> <a href=../970>multivariable function</a> on a <a href=../180>bounded</a> domain can be expressed as a composition and <a href=../275>addition binary operation</a> of a finite number of continuous single-variable functions. Specifically, a <a href=../1594>smooth</a> function $f : [0, 1]^{n} \to \mathbb{R}$ can be represented as
$$
f \left( x_{1} , \cdots , x_{n} \right) = \sum_{q=1}^{2n+1} \Phi_{q} \left( \sum_{p=1}^{n} \phi_{q,p} \left( x_{p} \right) \right)
$$
where $\phi_{q,p} : [0, 1] \to \mathbb{R}$ and $\Phi_{q} : \mathbb{R} \to \mathbb{R}$ hold. The issue with this theorem is that it only asserts existence without constraining the functions $\phi_{q,p}$ and $\Phi_{q}$, making it practically irrelevant for discovering such functions in machine learning‚Äîa point the authors acknowledge pessimistically.</p><p>Nevertheless, the authors view this theorem optimistically, willing to adapt it without obsessing over exact structures. They note that in both scientific and ordinary contexts, smooth and sparse structures are often encountered and can work well.</p><h3 id=22-kan-architecture>2.2 KAN Architecture</h3><p>Here, several notations are introduced. The structure of KAN is represented by an array of <a href=../587>integers</a>:
$$
\left[ n_{0} , n_{1} , \cdots , n_{L} \right]
$$
$n_{l}$ denotes the number of nodes in the $l$-th layer, and this KAN includes $\left( L+1 \right)$ layers including input and output layers. The $i$-th node in the $l$-th layer is denoted as $\left( l, i \right)$, and its value is represented as $x_{l,i}$. There are exactly $n_{l} n_{l+1}$ links (activation functions) between the $l$-th and $(l+1)$-th layers, and the activation function for a link connecting neurons $(l,i)$ and $\left( l+1, j \right)$ is denoted as $\phi_{l,j,i}$. The index ranges are naturally defined by $l = 0, \cdots, L-1$, front layer $i = 1 , \cdots , n_{l}$, and back layer $j = 1 , \cdots , n_{l+1}$.</p><p>The value of a back layer node $x_{l+1, j}$ is given by
$$
x_{l+1, j} = \sum_{i=1}^{n_{l}} \phi_{l,j,i} \left( x_{l, i} \right)
$$
and these <a href=../1947>vectors</a> are represented in bold notation as $\mathbf{x}_{l} = \left( x_{l, 1} , \cdots , x_{l, n_{l}} \right)$. To better understand this process in <a href=../1955>matrix</a> form:
$$
\begin{align*}
& \mathbf{x}_{l+1}
\\ =& \Phi_{l} \left( \mathbf{x}_{l+1} \right)
\\ =& \begin{bmatrix}
\phi_{l,1,1} (\cdot) & \phi_{l,1,2} (\cdot) & \cdots & \phi_{l,1,n_{l}} (\cdot)
\\ \phi_{l,2,1} (\cdot) & \phi_{l,2,2} (\cdot) & \cdots & \phi_{l,2,n_{l}} (\cdot)
\\ \vdots & \vdots & \ddots & \vdots
\\ \phi_{l,n_{l+1},1} (\cdot) & \phi_{l,n_{l+1},2} (\cdot) & \cdots & \phi_{l,n_{l+1},n_{l}} (\cdot)
\end{bmatrix} \mathbf{x}_{l}
\\ =& \begin{bmatrix}
\phi_{l,1,1} \left( x_{l, 1} \right) + \phi_{l,1,2} \left( x_{l, 2} \right) + \cdots + \phi_{l,1,n_{l}} \left( x_{l, n_{l}} \right)
\\ \phi_{l,2,1} \left( x_{l, 1} \right) + \phi_{l,2,2} \left( x_{l, 2} \right) + \cdots + \phi_{l,2,n_{l}} \left( x_{l, n_{l}} \right)
\\ \vdots
\\ \phi_{l,n_{l+1},1} \left( x_{l, 1} \right) + \phi_{l,n_{l+1},2} \left( x_{l, 2} \right) + \cdots + \phi_{l,n_{l+1},n_{l}} \left( x_{l, n_{l}} \right)
\end{bmatrix}
\\ =& \begin{bmatrix}
\sum_{i=1}^{n_{l}} \phi_{l,1,i} \left( x_{l, i} \right)
\\ \sum_{i=1}^{n_{l}} \phi_{l,2,i} \left( x_{l, i} \right)
\\ \vdots
\\ \sum_{i=1}^{n_{l}} \phi_{l,n_{l+1},i} \left( x_{l, i} \right)
\end{bmatrix}
\end{align*}
$$
This is not mathematically rigorous and should be approached with caution. It mirrors matrix operations because it involves summing functions $\phi$ over each vector component, not actual matrix multiplication of $\Phi_{l}$ and $\mathbf{x}_{l}$.</p><p>This allows KAN to be expressed as a composite function of $\Phi_{l}$ such that:
$$
\operatorname{KAN} \left( \mathbf{x} \right) = \left( \Phi_{L-1} \circ \cdots \circ \Phi_{1} \circ \Phi_{0} \right) \left( \mathbf{x} \right)
$$
Comparable to MLPs represented as:
$$
\operatorname{MLP} \left( \mathbf{x} \right) = \left( W_{L-1} \circ \sigma \circ \cdots \circ \sigma \circ W_{1} \circ \sigma \circ W_{0} \right) \left( \mathbf{x} \right)
$$</p><h4 id=implementation-details>Implementation Details</h4><p>$$
\phi (x) = w_{b} b(x) + w_{s} \operatorname{spline} (x)
$$
The activation function $\phi$ principally follows this form. $w_{b}$ and $w_{s}$ serve as <a href=../2594>hyperparameters</a>, not essential and particularly $w_{s}$ meaningless during training, yet retained for model adjustment. The activation function $b(x)$ and <a href=../1036>spline</a> $\operatorname{spline} (x)$ can be altered freely; however, for this study:
$$
\begin{align*}
b(x) =& \operatorname{SiLU} := x \sigma (x) = {\frac{ x }{ 1 + e^{-x} }}
\\ \operatorname{spline} =& \sum_{i} c_{i} B_{i} (x)
\end{align*}
$$
Here, $\operatorname{SiLU}$ is the SiLU function, $\sigma$ the <a href=../1775>logistic function</a>, and $B_{i}$ the <a href=../1045>B-spline</a>, with the B-spline coefficients $c_{i}$ being the trainable variables. As B-splines evaluate $0$ outside the given domain, spline grids need updates according to incoming values during training. Though efficiency remains questionable, the authors‚Äô claim that &ldquo;activation functions learn&rdquo; stands.</p><h3 id=23-kans-approximation-abilities-and-scaling-laws>2.3 KAN&rsquo;s Approximation Abilities and Scaling Laws</h3><p>Now, the fundamental theorem of KAN is introduced.</p><blockquote><p><strong>Approximation theory, KAT</strong>: Given $\mathbf{x} = \left( x_{1} , \cdots , x_{n} \right)$, consider $\phi_{l,j,i}$, differentiable up to $(k-1)$ times. Suppose $f$ is represented as:
$$
\begin{align*}
\Phi_{l} := & \left( \sum_{i=1}^{n_{l}} \phi_{l,1,i} , \cdots , \sum_{i=1}^{n_{l}} \phi_{l,n_{l+1},i} \right)
\\ f =& \left( \Phi_{L-1} \circ \cdots \circ \Phi_{1} \circ \Phi_{0} \right)
\end{align*}
$$
Then, for B-splines of degree $k$, with grid size $G$, expressed as $\phi_{l,j,i}^{G}$, there exists a constant $C$ depending on $f$ such that for all $0 \le m \le k$, the following is satisfied:
$$
\left\| f - \left( \Phi_{L-1}^{G} \circ \cdots \circ \Phi_{1}^{G} \circ \Phi_{0}^{G} \right) \right\| \le C G^{-k-1+m}
$$
Here, the <a href=../1225>norm</a> $\left\| \cdot \right\|$ is the $C^{m}$-norm defined by the <a href=../728>differential operator</a> $D$ to measure the magnitude of derivatives.
$$
\left\| g \right\| := \max_{| \beta | \le m} \sup_{x \in [0, 1]^{n}} \left| D^{\beta} g(x) \right|
$$</p></blockquote><p>Though the theorem appears straightforward to prove from B-spline properties, significant background knowledge is needed. Despite deviating from the Kolmogorov-Arnold theorem, the introduction of B-splines transparently reveals the intention. The bound on error $G^{-k-1+m}$ signifies that the approximation accuracy enhances by investing more in B-splines cost, as shown:
$$
\lim_{G , k \to \infty} G^{-k-1+m} = \lim_{G , k \to \infty} {\frac{ G^{m-1} }{ G^{k} }} = 0
$$
Specifically, increasing the number of B-spline grid points $G$ and the polynomial degree $k$ reduces the upper bound on error. Indeed, this theorem is the novel theoretical foundation for new neural networks. Nonetheless, while acknowledging B-splines as the true breakthrough, the Kolmogorov-Arnold theorem only remains a motivic reference.</p><p>The authors emphasize overcoming the curse of dimensionality through a dimension-independent bound, yet practical situations may render such bounds insignificant. Future benchmark validations on image data could substantiate KAN&rsquo;s claim of surmounting the curse of dimensionality.</p><h3 id=24-for-accuracy-grid-extension>2.4 For Accuracy: Grid Extension</h3><p>It is intuitive; functions approximated by B-splines can improve performance by extending grids, meaning subdividing the spline domain or adding out-of-domain points. While appealing compared to MLPs, which are sensitive to single weight changes, practical applications requiring such granular control seem unclear. Even in the text, &ldquo;Small KANs generalize better&rdquo; suggests uncertainty about the grid extension&rsquo;s effectiveness, so it may not warrant deep consideration.</p><h3 id=25-for-interpretability-simplifying-kans-and-making-them-interactive>2.5 For Interpretability: Simplifying KANs and Making them Interactive</h3><p>Next, an idea to enhance interpretability by sparsifying KAN&rsquo;s structure is introduced.</p><h4 id=251-simplification-techniques>2.5.1 Simplification Techniques</h4><p>Unlike MLPs, KAN allows complete traceability of values between layers. If activation functions converge to constant function $\mathbf{0} (\cdot)$, mapping all values to $0$, such links can be removed and replaced with human-readable expressions. Two terms are added to the <a href=../967>loss function</a> for this purpose.</p><p>Firstly, adopting the <a href=../2571>lasso regression</a> idea, $L_{1}$ norm $\left| \cdot \right|_{1}$ is used, a concept intuitive for those familiar with data science. The $L_{1}$ norm $\left| \phi \right|_{1}$ for the activation function $\phi$ is defined as the average across all output values $\phi \left( x^{(s)} \right)$ for the $s$-th input value $x^{(s)}$, mathematically:
$$
\left| \phi \right|_{1} := {\frac{ 1 }{ N }} \sum_{s=1}^{N} \left| \phi \left( x^{(s)} \right) \right|
$$
Accordingly, the layer $\Phi$&rsquo;s $L_{1}$ norm $\left| \Phi \right|_{1}$ is also naturally defined:
$$
\left| \Phi \right|_{1} := \sum_{i=1}^{n_{\text{in}}} \sum_{j=1}^{n_{\text{out}}} \left| \phi_{j,i} \right|_{1}
$$
However, the authors find this inadequate solely, thus defining the layer $\Phi$&rsquo;s <a href=../2035>entropy</a> $S \left( \Phi \right)$:
$$
S \left( \Phi \right) := - \sum_{i=1}^{n_{\text{in}}} \sum_{j=1}^{n_{\text{out}}} {\frac{ \left| \phi_{j,i} \right|_{1} }{ \left| \Phi \right|_{1} }} \log {\frac{ \left| \phi_{j,i} \right|_{1} }{ \left| \Phi \right|_{1} }}
$$
Lastly, the total loss $\mathscr{L}$, adding weight-modified original loss $\mathscr{l}$, and combining $\mu_{1}$ and $\mu_{2}$ is defined as:
$$
\mathscr{L} = \mathscr{l} + \mu_{1} \sum_{l=0}^{L-1} \left| \Phi_{l} \right|_{1} + \mu_{2} \sum_{l=0}^{L-1} S \left( \Phi_{l} \right)
$$
Minimizing the entropy, defined as the relative size of $\phi$, implies inducing the values of $\left| \phi \right|_{1}$ to either grow or converge to $0$.</p><p><img src=image.png#center alt="alt text"></p><p>However, the manual process of symbolification resulting from this model is somewhat ambiguous. As explained in Figure 2.4, initial visualization assists in identifying significant links using transparency $\tanh \left( 3 \left| \phi \right|_{1} \right)$ given axes $x$ and $y$. Subsequent steps include defining <strong>incoming score</strong> $I_{l,i}$ and <strong>outgoing score</strong> $O_{l,i}$ for each node:
$$
\begin{align*}
I_{l,i} :=& \max_{k} \left| \phi_{l-1, i, k} \right|
\\ O_{l,i} :=& \max_{k} \left| \phi_{l+1, k, i} \right|
\end{align*}
$$
Nodes below a threshold score $\theta = 10^{-2}$ are removed. Finally, models seek suitable candidates $f$ and replace $\phi$ with $f$ sampled by inserting several $x$ values.</p><p>ü§î ‚Ä¶ the essence of finding such expressions isn&rsquo;t the core of KAN. Although the equation extraction process isn&rsquo;t the main value, it may appear underwhelming compared to the previous ingenious development. The authors conclude Section 2 by comparing symbolic regression and KAN, suggesting that while symbolic regression is tricky, KAN‚Äôs equation extraction is akin to revealing and modifying KAN‚Äôs neural structure.</p><p>Following sections involve comparing KAN and MLP, experimenting with various problems to demonstrate KAN‚Äôs potential, typically spanning 30 pages. Only highlighting key points is sufficient for this review.</p><h2 id=3-kans-are-accurate>3. KANs are Accurate</h2><h3 id=33-feynman-datasets>3.3 Feynman Datasets</h3><p>The <a href=https://space.mit.edu/home/tegmark/aifeynman.html>Feynman dataset</a>, collected from Feynman&rsquo;s books, consists of numerous equations demonstrating the efficacy of KAN across diverse equations.</p><h3 id=34-solving-partial-differential-equations>3.4 Solving Partial Differential Equations</h3><p><a href=../3313>PINN</a> primarily solves <a href=../1818>partial differential equations</a>, with KAN and PINN shown to co-exist using an example with the <a href=../997>Poisson equation</a>. PINN‚Äôs core idea of including equation information in the loss function aligns with KAN‚Äôs structure.</p><h2 id=4-kans-are-interpretable>4. KANs are Interpretable</h2><h3 id=43-application-to-mathematics-knot-theory>4.3 Application to Mathematics: Knot Theory</h3><p>As an example helpful to pure math research, <strong>knot theory</strong> illustrates its application. Knot theory, focusing on distinguishing knots using invariants, raises questions like &lsquo;Can we find functions defining these invariants?&rsquo; Here, the authors emphasize KAN‚Äôs potential contribution to &ldquo;AI for Math.&rdquo;</p><h3 id=44-application-to-physics-anderson-localization>4.4 Application to Physics: Anderson Localization</h3><p>I am not familiar with <strong>Anderson localization</strong> to explain in detail, but the gist remains the same: KAN could aid physics research.</p><h2 id=5-related-works>5. Related Works</h2><p>Discussions on related studies to KAN within this field cover current trends as of 2024 and ideas leading up to KAN.</p><h2 id=6-discussion>6. Discussion</h2><h3 id=final-takeaway-should-i-use-kans-or-mlps>Final Takeaway: Should I Use KANs or MLPs?</h3><p>Which should you use, KAN or MLP? The authors recommend trying KAN in scenarios where interpretability is vital, and training time is not a significant concern, especially for moderately scaled AI and natural science problems.</p><h2 id=epilogue>Epilogue</h2><p>KAN, as introduced, updates B-spline domains with each training iteration, adopting a fundamentally more complex method than existing MLPs. While popular neural networks using MLP leverage GPU‚Äôs powerful linear algebra computations, KAN inherently struggles with such brute-force calculations. Authors mention KAN‚Äôs training speed being about ten times slower than MLP models with the same number of parameters, though this estimate might even be optimistic. <a href=../996>Deep learning</a> excels in heuristic neural network structures, offering potential for exponential performance improvement.</p><p>Nonetheless, KAN‚Äôs theoretical foundation is clear, positioning it as a strong candidate to address the opaque nature of black-box techniques in deep learning. Naturally, attempts to resolve KAN‚Äôs speed issues have emerged:</p><ul><li><a href=https://github.com/Blealtan/efficient-kan>efficient KAN</a>: Modifies the calculation of $\left| \phi \right|_{1}$, substituting parameter (B-spline coefficient) absolute values for the average computation of output values, maintaining similar mathematical significance while suggesting a clearer inheritance of the lasso regression method.</li><li><a href=https://github.com/ZiyaoLi/fast-kan>FastKAN</a>: Replaces B-splines with Gaussian radial functions, maintaining similar performance due to the resemblance to $k=3$ B-splines, albeit with significant speed improvement. Although a pragmatic engineering modification, it further distances from the Kolmogorov-Arnold representation theorem by abandoning B-splines.</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Federico Girosi, Tomaso Poggio; Representation Properties of Networks: Kolmogorov&rsquo;s Theorem Is Irrelevant. Neural Comput 1989; 1 (4): 465‚Äì469. doi: <a href=https://doi.org/10.1162/neco.1989.1.4.465>https://doi.org/10.1162/neco.1989.1.4.465</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Soljaƒçiƒá, M., &mldr; & Tegmark, M. (2024). Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756. <a href=https://arxiv.org/abs/2404.19756>https://arxiv.org/abs/2404.19756</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><aside style=text-align:right>2024-09-29&emsp;
Î•òÎåÄÏãù&emsp;
<a href=../1434>üé≤ 322</a></aside><script>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="Paper Review: Kolmogorov-Arnold Neural Network (KAN)",c="https://freshrimpsushi.github.io/en/posts/322/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script></div><aside><h2>Comment</h2><div class=area-reply><div class=list-reply></div></div><div class=write-box><div class="write-author-info box-account"><input type=hidden name=cmt_idx>
<input class=ai-author type=text placeholder=Name>
<input class=ai-password type=password maxlength=20 placeholder=Password></div><div class=write-content><textarea class=ai-content value placeholder='Feel free to ask in english' style=ime-mode:active></textarea></div><button class=write-button onclick=write_comment() aria-label=send><i class='fa-solid fa-paper-plane'></i></button><aside class=tex>$\TeX$ is also applied to comments.</aside></div></aside><script>let commentRows=[];const listReply=document.querySelector(".list-reply");document.addEventListener("DOMContentLoaded",()=>{get_all_comment()});function get_all_comment(){fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=getAllComment";return postFetch(t,{board_idx:"322",user_ip:e})}).then(e=>e.json()).then(e=>{e.ok&&(commentRows=e.rows,render_comment(commentRows))}).catch(e=>console.error(e))}function render_comment(e){const n=["Î•òÎåÄÏãù","Ï†ÑÍ∏∞ÌòÑ","„Öá„Öá","ÏßàÎ¨∏"],s=["Î•òÎåÄÏãù","Ï†ÑÍ∏∞ÌòÑ"];render_blank();let t="";e.map(e=>{t+=`<div id="comment${e.cmt_idx}" class="parents">`,t+=`<div class="content-info">`;let o="";e.cmt_cnt>125?o="ü•á":e.cmt_cnt>25?o="ü•à":e.cmt_cnt>5&&(o="ü•â"),n.includes(e.author)&&(o="");let i="";e.ip_address==null?i="(-)":(ipParts=e.ip_address.split("."),i=`(${ipParts[0]}.${ipParts[1]})`),s.includes(e.author)&&(i=""),t+=`<div class="list-author">${o} ${e.author} ${i}</div>`,t+=`<sup class="list-date">${e.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,e.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> Waiting for approvement</div>`);let a=e.content;a=a.replace(/\n/g,"<br />"),t+=`<div class="content-text">${a} <span class="re-comment-button" onclick="re_comment('${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`,e.child.map(o=>{let c="child";e.ip_address&&o.ip_address===e.ip_address&&(c="parentself"),t+=`<div id="comment${o.cmt_idx}" class="${c}">`,t+=`<div class="content-info">`;let i="";o.cmt_cnt>5?i="ü•â":o.cmt_cnt>25?i="ü•à":o.cmt_cnt>125&&(i="ü•á"),n.includes(o.author)&&(i="");let a="";o.ip_address==null?a="(-)":a=`(${o.ip_address})`,s.includes(o.author)&&(a=""),t+=`<div class="list-author">${i} ${o.author} ${a}</div>`,t+=`<sup class="list-date">${o.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,o.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> Waiting for approvement</div>`);let r=o.content;r=r.replace(/\n/g,"<br />"),t+=`<div class="content-text">${r} <span class="re-comment-button" onclick="re_comment('${o.cmt_idx}', '${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`})}),listReply.innerHTML=t,renderKaTex(listReply),location.href.indexOf("#")!=-1&&(location.href=location.href.substr(location.href.indexOf("#")))}function render_blank(){listReply.innerHTML=""}function write_comment(){const e=document.querySelector(".ai-author"),t=document.querySelector(".ai-password"),n=document.querySelector(".ai-content"),s=e.value,o=t.value,i=n.value;if(s===""||o===""||i===""){alert("ÎπàÏπ∏ÏùÑ Ï±ÑÏõåÏ£ºÏÑ∏Ïöî");return}const a="322",r="",c="Paper Review: Kolmogorov-Arnold Neural Network (KAN)";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeComment";return postFetch(t,{board_idx:a,board_slug:r,board_title:c,author:s.replace(/\+/g,"%2B"),password:o,content:i.replace(/\+/g,"%2B"),user_ip:e})}).then(e=>e.json()).then(s=>{s.ok?(commentRows.push(s.row),render_comment(commentRows),e.value="",t.value="",n.value=""):s.status===606?(Swal.fire(`Warning!
ÏßÄÏÜçÏ†ÅÏù∏ ÎèÑÎ∞∞ ÏãúÎèÑÏãú
IPÍ∞Ä Ï∞®Îã®Îê† Ïàò ÏûàÏäµÎãàÎã§.`),e.value="",t.value="",n.value=""):s.status===607&&(alert(`ÏßÄÎÇòÏπú ÎåìÍ∏Ä ÎèÑÎ∞∞Î•º ÌôïÏù∏ÌïòÏó¨
Ï†ëÍ∑ºÏùÑ Ï∞®Îã®Ìï©ÎãàÎã§.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}async function update_comment(e,t){const{value:n}=await Swal.fire({title:"ÎåìÍ∏Ä ÏàòÏ†ï",input:"password",inputPlaceholder:"Í∏Ä ÏûëÏÑ± Ïãú ÏûÖÎ†•ÌñàÎçò Ìå®Ïä§ÏõåÎìúÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(s=>{if(s.ok){const s=document.querySelector(`#comment${e}`),o="https://freshrimpsushi.com/blog/ajax/comment.php?action=getComment";postFetch(o,{cmt_idx:e}).then(e=>e.json()).then(o=>{if(o.ok){const a=o.row,r=a.author,c=a.content;let i="";i+=`<div class="update-author-info">`,i+=`<input class="update-author" type="text" value="${r}" placeholder="Ïù¥Î¶Ñ" />`,i+=`<input class="update-password" type="password" value="${n}" placeholder="ÎπÑÎ∞ÄÎ≤àÌò∏" disabled />`,i+="</div>",i+=`<textarea class="update-content" value="" style="IME-MODE:active;">${c}</textarea>`,i+=`<input class="update_comment-button" type="submit" value="ÏàòÏ†ï" onclick="update_comment_click(${e}, '${t}')" />`,i+=`<input class="update_comment-button" type="submit" value="Ï∑®ÏÜå" onclick="cancel_click(${e})" />`,s.innerHTML=i}}).catch(e=>console.error(e))}else Swal.fire("Ìå®Ïä§ÏõåÎìú ÏùºÏπò Ïò§Î•ò")}).catch(e=>console.error(e))}}async function delete_comment(e,t){const{value:n}=await Swal.fire({title:"ÎåìÍ∏Ä ÏÇ≠Ï†ú",text:"ÏÇ≠Ï†úÌïòÎ©¥ ÎêòÎèåÎ¶¥ Ïàò ÏóÜÏäµÎãàÎã§.",input:"password",icon:"warning",showCancelButton:!0,confirmButtonColor:"#3085d6",cancelButtonColor:"#d33",inputPlaceholder:"Í∏Ä ÏûëÏÑ± Ïãú ÏûÖÎ†•ÌñàÎçò Ìå®Ïä§ÏõåÎìúÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(n=>{if(n.ok){const n="https://freshrimpsushi.com/blog/ajax/comment.php?action=deleteComment";postFetch(n,{cmt_idx:e}).then(e=>e.json()).then(n=>{Swal.fire("ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§."),t==="parents"?commentRows=commentRows.filter(t=>t.cmt_idx!==e):t==="child"&&commentRows.map(t=>{t.child=t.child.filter(t=>t.cmt_idx!==e),t.child_cnt=t.child.length}),render_comment(commentRows)}).catch(e=>console.error(e))}else Swal.fire("Ìå®Ïä§ÏõåÎìú ÏùºÏπò Ïò§Î•ò")}).catch(e=>console.error(e))}}function update_comment_click(e,t){const i=document.querySelector(`#comment${e} .update-author`),a=document.querySelector(`#comment${e} .update-password`),r=document.querySelector(`#comment${e} .update-content`),n=i.value,s=a.value,o=r.value;(n===""||s===""||o==="")&&alert("ÎπàÏπ∏ÏùÑ Ï±ÑÏõåÏ£ºÏÑ∏Ïöî");const c="https://freshrimpsushi.com/blog/ajax/comment.php?action=updateComment";postFetch(c,{cmt_idx:e,author:n.replace(/\+/g,"%2B"),password:s,content:o.replace(/\+/g,"%2B")}).then(e=>e.json()).then(n=>{n.ok&&(Swal.fire("ÏàòÏ†ïÎêòÏóàÏäµÎãàÎã§."),t==="parents"?commentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}):t==="child"&&commentRows.map(t=>{t.cmt_idx==n.parent_cmt_idx&&t.child.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)})}),render_comment(commentRows),recentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}),render_recent_comment(recentRows))}).catch(e=>console.error(e))}function cancel_click(){render_comment(commentRows)}function re_comment(e,t){const s=document.querySelector(`#comment${e} .re-comment`);let n="";n+='<div class="re-comment-box">',n+='<div class="re-comment-author-info">',n+='<input class="re-comment-author" type="text" value="" placeholder="Ïù¥Î¶Ñ" />',n+='<input class="re-comment-password" type="password" value="" placeholder="ÎπÑÎ∞ÄÎ≤àÌò∏" />',n+="</div>",n+='<textarea class="re-comment-content" placeholder="ÎÇ¥Ïö©" value="" style="IME-MODE:active;"></textarea>',t===void 0?n+=`<button class="write-button" onclick="re_comment_click(${e})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`:n+=`<button class="write-button" onclick="re_comment_click(${e}, ${t})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`,n+=`<button class="write-button" onclick="cancel_click(${e})" value="close"><i class='fa-solid fa-solid fa-ban'></i></button>`,n+="</div>",s.innerHTML=n}function re_comment_click(e,t){const n=document.querySelector(`#comment${e} .re-comment-author`),s=document.querySelector(`#comment${e} .re-comment-password`),o=document.querySelector(`#comment${e} .re-comment-content`),i=n.value,a=s.value,r=o.value;if(i===""||a===""||r===""){alert("ÎπàÏπ∏ÏùÑ Ï±ÑÏõåÏ£ºÏÑ∏Ïöî");return}const c="322",l="",d="Paper Review: Kolmogorov-Arnold Neural Network (KAN)";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(n=>{const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeReComment";return postFetch(s,{board_idx:c,board_slug:l,board_title:d,cmt_idx:t===void 0?e:t,author:i.replace(/\+/g,"%2B"),password:a,content:r.replace(/\+/g,"%2B"),user_ip:n})}).then(e=>e.json()).then(i=>{i.ok?(commentRows.map(n=>{t===void 0?n.cmt_idx==e&&n.child.push(i.row):n.cmt_idx==t&&n.child.push(i.row)}),render_comment(commentRows)):i.status==606?(Swal.fire(`Warning!
ÏßÄÏÜçÏ†ÅÏù∏ ÎèÑÎ∞∞ ÏãúÎèÑÏãú
IPÍ∞Ä Ï∞®Îã®Îê† Ïàò ÏûàÏäµÎãàÎã§.`),n.value="",s.value="",o.value=""):i.status===607&&(alert(`ÏßÄÎÇòÏπú ÎåìÍ∏Ä ÎèÑÎ∞∞Î•º ÌôïÏù∏ÌïòÏó¨
Ï†ëÍ∑ºÏùÑ Ï∞®Îã®Ìï©ÎãàÎã§.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}</script></div><aside class=sidebar><aside><style>.reddot a:link{color:#67d10f;text-shadow:0 0 10px #d3d3d3;font-weight:700}.reddot a:visited{color:#f5f5f5}</style><p class=reddot style=text-align:center;margin:0><a href=https://freshrimpsushi.github.io/en/posts/2707/>Summer Special Omakase<br>„ÄåImaginary Numbers„Äç</a></p></aside><br><div class=category></div><div style=display:flex>Click ‚óè to highlight only you interested.<div class=resetmute><button class="sb-btn btnReset" style=border-radius:5px;border:0>reset</button>
<button class="sb-btn btnMute" style=border-radius:5px;border:0>mute</button></div></div><script defer>const color={green:"#33cc33",yellow:"#ffcc00",red:"#ff3333",black:"#000000"},categoryRows=[[{idx:1,name:"Ìï®Ïàò",color:color.green,show:"Functions",size:"146"},{idx:2,name:"Î≥¥Ï°∞Ï†ïÎ¶¨",color:color.green,show:"Lemmas",size:"55"},{idx:3,name:"ÎØ∏Î∂ÑÏ†ÅÎ∂ÑÌïô",color:color.green,show:"Calculus",size:"45"},{idx:4,name:"ÌñâÎ†¨ÎåÄÏàò",color:color.green,show:"Matrix Algebra",size:"117"}],[{idx:1,name:"Ï†ïÏàòÎ°†",color:color.green,show:"Number Theory",size:"90"},{idx:2,name:"ÏßëÌï©Î°†",color:color.green,show:"Set Theory",size:"49"},{idx:3,name:"Í∑∏ÎûòÌîÑÏù¥Î°†",color:color.green,show:"Graph Theory",size:"65"},{idx:4,name:"ÏÑ†ÌòïÎåÄÏàò",color:color.green,show:"Linear Algebra",size:"97"},{idx:5,name:"Ìï¥ÏÑùÍ∞úÎ°†",color:color.green,show:"Analysis",size:"84"},{idx:6,name:"Ï∂îÏÉÅÎåÄÏàò",color:color.green,show:"Abstract Algebra",size:"105"},{idx:7,name:"ÏúÑÏÉÅÏàòÌïô",color:color.green,show:"Topology",size:"64"},{idx:8,name:"Í∏∞ÌïòÌïô",color:color.green,show:"Geometry",size:"167"}],[{idx:1,name:"Îã§Î≥ÄÏàòÎ≤°ÌÑ∞Ìï¥ÏÑù",color:color.green,show:"Vector Analysis",size:"37"},{idx:2,name:"Î≥µÏÜåÌï¥ÏÑù",color:color.green,show:"Complex Anaylsis",size:"71"},{idx:3,name:"Ï∏°ÎèÑÎ°†",color:color.green,show:"Measure Theory",size:"53"},{idx:4,name:"Ìë∏Î¶¨ÏóêÌï¥ÏÑù",color:color.green,show:"Fourier Analysis",size:"54"},{idx:5,name:"Ï¥àÌï®ÏàòÎ°†",color:color.green,show:"Distribution Theory",size:"22"},{idx:6,name:"Îã®Ï∏µÏ¥¨ÏòÅ",color:color.green,show:"Tomography",size:"20"}],[{idx:1,name:"Í±∞Î¶¨Í≥µÍ∞Ñ",color:color.green,show:"Metric Space",size:"38"},{idx:2,name:"Î∞îÎÇòÌùêÍ≥µÍ∞Ñ",color:color.green,show:"Banach Space",size:"38"},{idx:3,name:"ÌûêÎ≤†Î•¥Ìä∏Í≥µÍ∞Ñ",color:color.green,show:"Hilbert Space",size:"31"},{idx:4,name:"Î•¥Î≤°Í≥µÍ∞Ñ",color:color.green,show:"Lebesgue Space",size:"33"}],[{idx:1,name:"ÏÉÅÎØ∏Î∂ÑÎ∞©Ï†ïÏãù",color:color.green,show:"ODE",size:"58"},{idx:2,name:"Ìé∏ÎØ∏Î∂ÑÎ∞©Ï†ïÏãù",color:color.green,show:"PDE",size:"60"},{idx:3,name:"ÌôïÎ•†ÎØ∏Î∂ÑÎ∞©Ï†ïÏãù",color:color.green,show:"SDE",size:"26"}],[{idx:1,name:"Ï§ÑÎ¶¨ÏïÑ",color:color.green,show:"Julia",size:"229"},{idx:2,name:"ÏïåÍ≥†Î¶¨Ï¶ò",color:color.green,show:"Algorithm",size:"28"},{idx:3,name:"ÏàòÏπòÌï¥ÏÑù",color:color.green,show:"Numerical Analysis",size:"63"},{idx:4,name:"ÏµúÏ†ÅÌôîÏù¥Î°†",color:color.green,show:"Optimization Theory",size:"37"},{idx:5,name:"Î®∏Ïã†Îü¨Îãù",color:color.green,show:"Machine Learning",size:"114"},{idx:6,name:"ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç",color:color.yellow,show:"Programming",size:"114"},{idx:7,name:"ÏÑ∏Ïù¥Î≤ÑÎ©îÌä∏Î¶≠Ïä§",color:color.green,show:"Sabermetrics",size:"229",size:"13"}],[{idx:1,name:"Î¨ºÎ¶¨Ìïô",color:color.green,show:"Physics",size:"27"},{idx:2,name:"ÏàòÎ¶¨Î¨ºÎ¶¨",color:color.green,show:"Mathematical Physics",size:"77"},{idx:3,name:"Í≥†Ï†ÑÏó≠Ìïô",color:color.green,show:"Classical Mechanics",size:"48"},{idx:4,name:"Ï†ÑÏûêÍ∏∞Ìïô",color:color.green,show:"Electrodynamics",size:"51"},{idx:5,name:"ÏñëÏûêÏó≠Ìïô",color:color.green,show:"Quantum Mechanics",size:"57"},{idx:6,name:"Ïó¥Î¨ºÎ¶¨Ìïô",color:color.green,show:"Thermal Physics",size:"29"}],[{idx:1,name:"R",color:color.green,show:"R",size:"54"},{idx:2,name:"Îç∞Ïù¥ÌÑ∞ÌôïÎ≥¥",color:color.green,show:"Data Sets",size:"29"},{idx:3,name:"Îç∞Ïù¥ÌÑ∞Í≥ºÌïô",color:color.green,show:"Data Science",size:"41"},{idx:4,name:"ÌÜµÍ≥ÑÏ†ÅÍ≤ÄÏ†ï",color:color.green,show:"Statistical Test",size:"33"},{idx:5,name:"ÌÜµÍ≥ÑÏ†ÅÎ∂ÑÏÑù",color:color.green,show:"Statistical Analysis",size:"76"},{idx:6,name:"ÏàòÎ¶¨ÌÜµÍ≥ÑÌïô",color:color.green,show:"Mathematical Statistics",size:"123"},{idx:7,name:"ÌôïÎ•†Î∂ÑÌè¨Î°†",color:color.green,show:"Probability Distribution",size:"84"},{idx:8,name:"ÌôïÎ•†Î°†",color:color.green,show:"Probability Theory",size:"80"},{idx:9,name:"ÏúÑÏÉÅÎç∞Ïù¥ÌÑ∞Î∂ÑÏÑù",color:color.green,show:"TDA",size:"40"}],[{idx:1,name:"ÎÖºÎ¨∏ÏûëÏÑ±",color:color.red,show:"Writing",size:"63"},{idx:2,name:"ÏÉùÏÉàÏö∞Ï¥àÎ∞•ÏßÄ",color:color.black,show:"JOF",size:"7"}]];document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("blindList"));(e==""||e==null||e==null||e==0||e==NaN)&&localStorage.setItem("blindList",null),render_category(categoryRows),blind_category(e);const t=document.querySelector(".btnReset");t.addEventListener("click",()=>{let e=new Array;localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)});const n=document.querySelector(".btnMute");n.addEventListener("click",()=>{let e=new Array;for(let t=0;t<categoryRows.length;t++)categoryRows[t].map(n=>{const s={mainIdx:t,subIdx:n.idx};e.push(s)});localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)})});function render_category(e){const n=document.querySelector(".category");let t="";for(let n=0;n<e.length;n++)e[n].map(e=>{t+=`<span id="cate${n}-${e.idx}" class="cate etewsert">`,t+=`<span onclick="check_blind(${n}, ${e.idx});" style="cursor: pointer; color: ${e.color}">‚óè</span>`,t+=`<a href="https://freshrimpsushi.github.io/en/categories/${e.name.toLowerCase()}/">`,t+=` ${e.show} (${e.size})</a>`,t+="</span>",screen.width>954?t+="<br>":t+=" "}),t+="<hr>";n.innerHTML=t}function check_blind(e,t){const o=document.querySelector(`#cate${e}-${t}`);let n=new Array;const i={mainIdx:e,subIdx:t};let s=JSON.parse(localStorage.getItem("blindList"));if(s==""||s==null||s==null||s==0||s==NaN)n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind";else{n=s;let a=null;for(let s=0;s<n.length;s++)if(n[s].mainIdx==e&&n[s].subIdx==t){a=n.filter(n=>n.mainIdx!=e||n.subIdx!=t);break}a===null?(n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind"):(localStorage.setItem("blindList",JSON.stringify(a)),o.className="cate")}}function blind_category(e){const t=document.querySelectorAll(".cate");t.forEach(e=>{e.className="cate"}),e!==null&&e.map(e=>{const t=document.querySelector(`#cate${e.mainIdx}-${e.subIdx}`);t.className+=" blind"})}</script><br><br><b>Viewed posts</b><div class=lately-viewed-list style=padding-left:4px></div><script defer>document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("latelyViewPostList")),n=document.querySelector(".lately-viewed-list");let t="";if(e==""||e==null||e==null||e==0||e==NaN)localStorage.setItem("latelyViewPostList",null),t+='<div class="lv-list">',t+=" ¬∑ Ïó¥ÎûåÌïú Ìè¨Ïä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§.",t+="</div>",n.innerHTML=t;else{for(let n=e.length-1;n>=0;n--)t+='<div class="lv-list">',t+=`<a href="${e[n].link}"> ¬∑ ${e[n].title}</a>`,t+="</div>";n.innerHTML=t}})</script><script defer>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="Paper Review: Kolmogorov-Arnold Neural Network (KAN)",c="https://freshrimpsushi.github.io/en/posts/322/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script><br><b>Recent comment</b><div class=current-reply></div><script defer>const url="https://freshrimpsushi.com/blog/ajax/recent_comment.php?action=getCurrentComment";let recentRows=[];fetch(url).then(e=>e.json()).then(e=>{e.ok&&(recentRows=e.rows,render_recent_comment(recentRows))}).catch(e=>console.error(e));function render_recent_comment(e){const n=document.querySelector(".current-reply");let t="";e.map(e=>{let n="https://freshrimpsushi.github.io/en/";e.board_idx>-1?n+=`posts/${e.board_idx}#comment${e.cmt_idx}`:e.board_idx==-1?n+=`#comment${e.cmt_idx}`:n+=`categories/${e.board_title}#comment${e.cmt_idx}`,t+='<div class="current-reply-list">',t+=`<a href="${n}">`,t+=`<b> - ${e.author}</b>: `,t+=`${e.content}`,t+=`</a>`,t+=`</div>`}),n.innerHTML=t,renderKaTex(n)}</script><br></aside></div><footer><aside><div><p id=mirror-link style=text-align:center><a style=cursor:text href=http://localhost:1313//en/posts/322/>¬© FreshrimpRestaurant / Powered by Î•òÎåÄÏãù, Ï†ÑÍ∏∞ÌòÑ</a><br>Contact:
<img src=https://freshrimpsushi.github.io/en/logo/gmail.png width=12px alt=mail> freshrimpsushi@gmail.com
<a href=https://freshrimpsushi.github.io/en/index.xml><img src=https://freshrimpsushi.github.io/en/logo/RSS.png width=12px alt=RSS> RSS</a></p></div><script type=text/javascript>var goIndex=function(){var e=document.getElementsByName("idx")[0].value,t="https://freshrimpsushi.github.io/en/posts/"+e;location.replace(t)};document.addEventListener("keydown",function(e){const n=document.getElementById("navigator");if(e.altKey&&e.ctrlKey&&e.key==="l"){e.preventDefault();var t=document.querySelector("#mirror-link a");t?t.click():console.log("Í±∞Ïö∏ ÎßÅÌÅ¨Î•º Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.")}})</script></aside></footer></body><script async src="https://www.googletagmanager.com/gtag/js?id=G-NLV8Y9PRK1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NLV8Y9PRK1")</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4751085325232621" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/sweetalert2@11></script><script src=https://freshrimpsushi.github.io/en/js/fontawsome.min.js></script><script src=https://freshrimpsushi.github.io/en/js/common.js></script><script>document.addEventListener("DOMContentLoaded",()=>{fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/ip_checker.php";return postFetch(t,{user_ip:e})}).then(e=>e.json()).then(e=>{e.ok||(alert(`Ï∞®Îã®Îêú IPÏûÖÎãàÎã§.
Contact:
freshrimpsushi@gmail.com`),window.location.href="https://google.com")}).catch(e=>console.error(e))})</script></html>