<!doctype html><html class=blog lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=preload href=https://freshrimpsushi.github.io/en/css/style.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=preload href=https://freshrimpsushi.github.io/en/css/comment.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=icon href=https://freshrimpsushi.github.io/ko/logo/favicon.ico><meta name=msapplication-TileColor content="#FFFFFF"><meta name=msapplication-TileImage content="logo/basic.png"><meta name=NaverBot content="All"><meta name=NaverBot content="index,follow"><meta name=Yeti content="All"><meta name=Yeti content="index,follow"><meta name=google-site-verification content="KYAokS7-6C5YuXOjatJQsiK1T0O8x4YncYFIF4tneYI"><meta name=naver-site-verification content="e5651d6f97899061897203413efc84994f04bbba"><link rel=alternate type=application/rss+xml title=FreshrimpRestaurant href=https://freshrimpsushi.github.io/en/index.xml><title>Paper Review: Score Matching</title></head><meta name=title content="Paper Review: Score Matching"><meta name=description content="국내 최대의 수학, 물리학, 통계학 블로그"><meta property="og:title" content="Paper Review: Score Matching"><meta property="og:description" content><meta property="og:image" content="https://freshrimpsushi.github.io/en/logo/basic.png/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"j_","name":"Paper Review: Score Matching","headline":"Paper Review: Score Matching","alternativeHeadline":"","description":"Overview Score Matching is a statistical technique introduced in the 2005 paper by Aapo Hyvarinen, Estimation of Non-Normalized Statistical Models by Score Matching, which provides a method for estimating non-normalized models without considering the normalization constant. 1. Introduction In many cases, probabilistic models are given as non-normalized models containing a normalization constant $Z$. For instance, a probability density function $p_{\\boldsymbol{\\theta}}$ with parameters $\\boldsymbol{\\theta}$ is defined as follows: $$ p(\\boldsymbol{\\xi}; \\boldsymbol{\\theta})","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/freshrimpsushi.github.io\/en\/posts\/3676\/"},"author":{"@type":"Person","name":"전기현","url":"https://math.stackexchange.com/users/459895/ryu-dae-sick"},"creator":{"@type":"Person","name":"전기현"},"accountablePerson":{"@type":"Person","name":"전기현"},"copyrightHolder":"FreshrimpRestaurant","copyrightYear":"2025","dateCreated":"2025-07-21T00:00:00.00Z","datePublished":"2025-07-21T00:00:00.00Z","dateModified":"2025-07-21T00:00:00.00Z","publisher":{"@type":"Organization","name":"FreshrimpRestaurant","url":"https://freshrimpsushi.github.io/en/","logo":{"@type":"ImageObject","url":"https:\/\/freshrimpsushi.github.io\/en\/logo\/basic.png","width":"32","height":"32"}},"image":"https://freshrimpsushi.github.io/en/logo/basic.png","url":"https:\/\/freshrimpsushi.github.io\/en\/posts\/3676\/","wordCount":"1737","genre":[],"keywords":[]}</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/mhchem.min.js integrity=sha384-F2ptQFZqNJuqfGGl28mIXyQ5kXH48spn7rcoS0Y9psqIKAcZPLd1NzwFlm/bl1mH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>function renderKaTex(e){renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],trust:!0,trust:e=>["\\htmlId","\\href","\\includegraphics"].includes(e.command),macros:{"\\eqref":"(\\text{#1})","\\ref":"\\href{###1}{\\text{#1}}","\\label":"\\htmlId{#1}{}","\\sech":"\\operatorname{sech}","\\csch":"\\operatorname{csch}","\\sgn":"\\operatorname{sgn}","\\sign":"\\operatorname{sign}","\\sinc":"\\operatorname{sinc}","\\diag":"\\operatorname{diag}","\\diam":"\\operatorname{diam}","\\trace":"\\operatorname{trace}","\\Tr":"\\operatorname{Tr}","\\tr":"\\operatorname{tr}","\\re":"\\operatorname{Re}","\\im":"\\operatorname{Im}","\\Var":"\\operatorname{Var}","\\Poi":"\\operatorname{Poi}","\\Cov":"\\operatorname{Cov}","\\span":"\\operatorname{span}","\\supp":"\\operatorname{supp}","\\rank":"\\operatorname{rank}","\\nullity":"\\operatorname{nullity}","\\ad":"\\operatorname{ad}","\\Ric":"\\operatorname{Ric}","\\i":"\\mathrm{i}","\\d":"\\mathrm{d}","\\cR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5863.png}","\\acR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.4371.png}","\\bcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5899.png}","\\abcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.0596.png}","\\crH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\smallcrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\acrH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}","\\smallacrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}"},throwOnError:!1})}</script><body class=main><header><a href=https://freshrimpsushi.github.io/en/ rel=home><p style=text-align:center;font-size:1rem;color:#000><img src=https://freshrimpsushi.github.io/en/logo/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D.png style=height:80px alt=logo></p></a></header><form method=get action=/en/search style=border:1px;text-align:center><div class=field><input type=text id=searchtext placeholder=🔍︎ class=input_text name=s style=background-color:#eee;text-align:center;width:200px;font-size:1.5rem;border:1px;border-radius:5px;padding-top:5px;padding-bottom:5px;margin-bottom:1.5rem></div></form><aside style=text-align:center;margin-bottom:1rem><a href=https://freshrimpsushi.github.io/ko//posts/3676/>한국어</a> |
<a href=https://freshrimpsushi.github.io/en//posts/3676/>English</a> |
<a href=https://freshrimpsushi.github.io/jp//posts/3676/>日本語</a></aside><div class=wrapper><div class=content><div class=content-box><title>Paper Review: Score Matching</title>
<a href=https://freshrimpsushi.github.io/en/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/ style=background-color:rgba(0,0,0,.8);color:orange;border-radius:10px;padding:5px>📂Machine Learning</a><h1>Paper Review: Score Matching</h1><aside><div class=innerheader><div class=innertoc><b>Table of Contents</b><nav id=TableOfContents><ul><li><a href=#overview>Overview</a></li><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-estimation-by-score-matching>2. Estimation by Score Matching</a></li><li><a href=#3-examples>3. Examples</a><ul><li><a href=#31-multivariate-gaussian-density>3.1 Multivariate Gaussian Density</a></li></ul></li><li><a href=#appendix-a-proof-of-theorem-1>Appendix A. Proof of Theorem 1</a></li><li><a href=#appendix-b-proof-of-theorem-2>Appendix B. Proof of Theorem 2</a></li></ul></nav></div></div></aside><h2 id=overview>Overview</h2><p><strong>Score Matching</strong> is a statistical technique introduced in the 2005 paper by Aapo Hyvarinen, <a href=https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf>Estimation of Non-Normalized Statistical Models by Score Matching</a>, which provides a method for estimating <a href=../3664>non-normalized models</a> without considering the normalization constant.</p><h2 id=1-introduction>1. Introduction</h2><p>In many cases, probabilistic models are given as <a href=../3664>non-normalized models</a> containing a normalization constant $Z$. For instance, a probability density function $p_{\boldsymbol{\theta}}$ with <a href=../2440>parameters</a> $\boldsymbol{\theta}$ is defined as follows:</p><p>$$
p(\boldsymbol{\xi}; \boldsymbol{\theta}) = \dfrac{1}{Z(\boldsymbol{\theta})} q(\boldsymbol{\xi}; \boldsymbol{\theta})
$$</p><p>Here, the issue encountered is that while $q$ is analytically well-defined or easy to compute, $Z(\boldsymbol{\theta}) = \int q(\boldsymbol{\xi}; \boldsymbol{\theta}) d \boldsymbol{\xi}$ is often difficult to calculate. Particularly when $\boldsymbol{\theta}$ is a high-dimensional vector, it can be practically impossible to compute, including problems such as the <a href=../708>curse of dimensionality</a>. Previously, methods like Markov chain Monte Carlo were commonly used for estimating non-normalized models, but these methods are slow, and other methods often underperform.</p><h2 id=2-estimation-by-score-matching>2. Estimation by Score Matching</h2><p>The crux of the proposed method is the [score function]. Denoting the score function of a model&rsquo;s <a href=../1433>probability density function</a> $p(\boldsymbol{\xi}; \boldsymbol{\theta})$ that approximates the distribution of data as $\psi(\boldsymbol{\xi}; \boldsymbol{\theta})$, it is defined as follows:</p><p>$$
\psi(\boldsymbol{\xi}; \boldsymbol{\theta})
= \begin{bmatrix} \psi_{1}(\boldsymbol{\xi}; \boldsymbol{\theta}) \\ \vdots \\ \psi_{n}(\boldsymbol{\xi}; \boldsymbol{\theta}) \end{bmatrix}
:= \begin{bmatrix} \dfrac{\partial \log p(\boldsymbol{\xi}; \boldsymbol{\theta})}{\partial \xi_{1}} \\ \vdots \\ \dfrac{\partial \log p(\boldsymbol{\xi}; \boldsymbol{\theta})}{\partial \xi_{n}} \end{bmatrix}
= \nabla_{\boldsymbol{\xi}} \log p(\boldsymbol{\xi}; \boldsymbol{\theta})
$$</p><p>In other words, the score function is the <a href=../1010>gradient</a> of the log-probability density function. The method proposed in the paper allows us to ignore the normalization constant, thus redefining the score function as follows:</p><p>$$
\psi(\boldsymbol{\xi}; \boldsymbol{\theta}) = \nabla_{\boldsymbol{\xi}} \log q(\boldsymbol{\xi}; \boldsymbol{\theta})
$$</p><p>Let&rsquo;s denote the score function for the actual data distribution $\mathbf{x}$ as follows:</p><p>$$
\psi_{\mathbf{x}}( \cdot ) = \nabla_{\boldsymbol{\xi}} \log p_{\mathbf{x}}( \cdot )
$$</p><p>The paper sets up the objective function such that the expected value difference between the data&rsquo;s score and the model&rsquo;s score decreases, as follows:</p><p>$$
\begin{equation}
\begin{aligned}
J(\boldsymbol{\theta})
&= \dfrac{1}{2} \int p_{\mathbf{x}}(\boldsymbol{\xi}) \left\| \psi(\boldsymbol{\xi}; \boldsymbol{\theta}) - \psi_{\mathbf{x}}(\boldsymbol{\xi}) \right\|^{2} \mathrm{d}\boldsymbol{\xi} \\
&= \dfrac{1}{2} \int p_{\mathbf{x}}(\boldsymbol{\xi}) \left\| \nabla_{\boldsymbol{\xi}} \log q(\boldsymbol{\xi}; \boldsymbol{\theta}) - \nabla_{\boldsymbol{\xi}} \log p_{\mathbf{x}}(\boldsymbol{\xi}) \right\|^{2} \mathrm{d}\boldsymbol{\xi}
\end{aligned}
\end{equation}
$$</p><p>Thus, <strong>Score Matching</strong> refers to a method for estimating $\boldsymbol{\theta}$ as follows:</p><p>$$
\hat{\boldsymbol{\theta}} = \argmin\limits_{\boldsymbol{\theta}} J(\boldsymbol{\theta})
$$</p><p>However, if we inspect $(1)$, there is indeed an issue: calculating $\psi_{\mathbf{x}}(\boldsymbol{\xi}) = \nabla_{\boldsymbol{\xi}} \log p_{\mathbf{x}}(\boldsymbol{\xi})$ requires knowing $p_{\mathbf{x}}$. Since $p_{\mathbf{x}}$ is unknown, we approximate it using model $p(\boldsymbol{\xi}; \boldsymbol{\theta})$, but this appears to be contradictory because approximating requires knowing $p_{\mathbf{x}}$. In fact, from the below theorem, $(1)$ can be restructured without $\psi_{\mathbf{x}}$.</p><hr><p>Assume the score function $\psi(\boldsymbol{\xi}; \boldsymbol{\theta})$ of model $\textbf{Theorem 1}$ is <a href=../1210>differentiable</a>. Then $(1)$ can be expressed as:</p><p>$$
\begin{align*}
J(\boldsymbol{\theta})
&= \int p_{\mathbf{x}}(\boldsymbol{\xi}) \sum\limits_{i=1}^{n} \left[ \partial_{i} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) + \dfrac{1}{2} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta})^{2} \right] \mathrm{d}\boldsymbol{\xi} + \text{constant} \tag{2} \\
&= \int p_{\mathbf{x}}(\boldsymbol{\xi}) \left[\sum\limits_{i=1}^{n} \partial_{i} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) + \dfrac{1}{2} \Braket{ \psi(\boldsymbol{\xi}; \boldsymbol{\theta}), \psi(\boldsymbol{\xi}; \boldsymbol{\theta})} \right] \mathrm{d}\boldsymbol{\xi} + \text{constant} \\
&= \int p_{\mathbf{x}}(\boldsymbol{\xi}) \left[\Delta \log q(\boldsymbol{\xi}; \boldsymbol{\theta}) + \dfrac{1}{2} \| \psi(\boldsymbol{\xi}; \boldsymbol{\theta}) \|_{2}^{2} \right] \mathrm{d}\boldsymbol{\xi} + \text{constant}
\end{align*}
$$</p><p>Here, $\text{constant}$ is a constant that does not depend on $\boldsymbol{\theta}$. $\psi_{i}$ represents the $i$-th component of the score, and $\partial_{i} \psi_{i}$ is the partial derivative of the $i$-th component of the score function with respect to the $i$-th variable.</p><p>$$
\psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) = \dfrac{\partial \log q(\boldsymbol{\xi}; \boldsymbol{\theta})}{\partial \xi_{i}}
$$
$$
\partial_{i} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) = \dfrac{\partial \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta})}{\partial \xi_{i}} = \dfrac{\partial^{2} \log q(\boldsymbol{\xi}; \boldsymbol{\theta})}{\partial \xi_{i}^{2}}
$$</p><hr><p>Refer to the Appendix for the proof. The first term inside the brackets is identical to the <a href=../3081>Laplacian</a> of the probability density function $q$ when factored by $\sum$.</p><p>$$
\Delta q = \nabla^{2} \log q = \sum\limits_{i=1}^{n} \dfrac{\partial^{2} \log q}{\partial \xi_{i}^{2}}
$$</p><p>Since, in reality, only a finite amount of data can be processed, if $T$ samples $\mathbf{x}(1), \dots, \mathbf{x}(T)$ are given, the expression for the samples is as follows:</p><p>$$
\tilde{J}(\boldsymbol{\theta}) = \dfrac{1}{T} \sum\limits_{t=1}^{T} \sum\limits_{i=1}^{n} \left[ \partial_{i} \psi_{i}(\mathbf{x}(t); \boldsymbol{\theta}) + \dfrac{1}{2} \psi_{i}(\mathbf{x}(t); \boldsymbol{\theta})^{2} \right] + \text{constant}
$$</p><p>Subsequent to this, from the following theorem, we can see that minimizing $(2)$ is indeed sufficient for model estimation.</p><hr><p>Assume $\textbf{Theorem 2}$ exists uniquely such that $p_{\mathbf{x}}(\cdot) = p(\cdot; \boldsymbol{\theta}^{\ast})$ holds. Also assume $q(\boldsymbol{\xi}; \boldsymbol{\theta}) > 0$. Then the following holds:</p><p>$$
J(\boldsymbol{\theta}) = 0 \iff \boldsymbol{\theta} = \boldsymbol{\theta}^{\ast}
$$</p><hr><p>Under the assumptions of the preceding theorems, the score matching estimator obtained by minimizing $\tilde{J}$ is a <a href=../2021>consistent estimator</a>. That is, as the sample size increases indefinitely, the estimator statistically converges to the true value $\boldsymbol{\theta}^{\ast}$, assuming that the optimization algorithm can find the <a href=../1463>global minimum</a>.</p><hr><p>As the sample size increases, $\tilde{J}$ converges to $J$, Therefore by the <a href=../32>law of large numbers</a>, $\text{Corollary}$ holds.</p><h2 id=3-examples>3. Examples</h2><h3 id=31-multivariate-gaussian-density>3.1 Multivariate Gaussian Density</h3><p>Consider a very simple case with a <a href=../1954>multivariate normal distribution</a>:</p><p>$$
p(\mathbf{x}; \mathbf{M}, \boldsymbol{\mu}) = \dfrac{1}{Z(\mathbf{M}, \boldsymbol{\mu} )} \exp \left( -\dfrac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^{\mathsf{T}} \mathbf{M} (\mathbf{x} - \boldsymbol{\mu}) \right)
$$</p><p>Here, $\mathbf{M} \in \mathbb{R}^{n \times n}$ is the <a href=../3003>inverse</a> of the <a href=../1950>covariance matrix</a>, which is <a href=../336>positive definite</a> and a <a href=../3005>symmetric matrix</a>. $\boldsymbol{\mu} \in \mathbb{R}^{n}$ is the mean vector. Although $Z(\mathbf{M}, \boldsymbol{\mu}) = ((2\pi)^{n} \det \mathbf{M})^{1/2}$ is well-known in this case, let&rsquo;s consider it as a simple example.</p><h4 id=311-estimation>3.1.1 Estimation</h4><p>In this case, $q$, $\psi$, and $\partial_{i} \psi$ are expressed as follows:</p><p>$$
q(\mathbf{x}) = \exp \left( -\dfrac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^{\mathsf{T}} \mathbf{M} (\mathbf{x} - \boldsymbol{\mu}) \right)
$$</p><p>For a symmetric matrix $\mathbf{R}$, the <a href=../1926>gradient of the quadratic form</a> is $\nabla_{\mathbf{x}} (\mathbf{x}^{\mathsf{T}} \mathbf{R} \mathbf{x}) = 2 \mathbf{R} \mathbf{x}$, thus:</p><p>$$
\psi(\mathbf{x}; \mathbf{M}, \boldsymbol{\mu}) = -\mathbf{M} (\mathbf{x} - \boldsymbol{\mu})
= - \begin{bmatrix} \sum\limits_{j}m_{1j}(x_{j}-\mu_{j}) \\ \vdots \\[1em] \sum\limits_{j}m_{nj}(x_{j}-\mu_{j})\end{bmatrix}
$$</p><p>Here, $\mathbf{M} = [m_{ij}]$. $\partial_{i} \psi_{i} = \dfrac{\partial \psi_{i}}{\partial x_{i}}$ is as follows:</p><p>$$
\partial_{i} \psi_{i}(\mathbf{x}; \mathbf{M}, \boldsymbol{\mu}) = -m_{ii}
$$</p><p>Therefore, $\tilde{J}$ is as follows. Given $\sum_{i} \psi_{i} = \braket{\psi, \psi} = \psi^{\mathsf{T}} \psi$:</p><p>$$
\begin{align*}
\tilde{J}(\mathbf{M}, \boldsymbol{\mu})
&= \dfrac{1}{T} \sum\limits_{t=1}^{T} \left[ \sum\limits_{i} -m_{ii} + \dfrac{1}{2}\left( \mathbf{x}(t) - \boldsymbol{\mu} \right)^{\mathsf{T}} \mathbf{M}^{\mathsf{T}} \mathbf{M} \left( \mathbf{x}(t) - \boldsymbol{\mu} \right) \right] \\
&= \dfrac{1}{T} \sum\limits_{t=1}^{T} \left[ - \Tr (\mathbf{M}) + \dfrac{1}{2}\left( \mathbf{x}(t) - \boldsymbol{\mu} \right)^{\mathsf{T}} \mathbf{M}^{\mathsf{T}} \mathbf{M} \left( \mathbf{x}(t) - \boldsymbol{\mu} \right) \right]
\end{align*}
$$</p><p>Here, $\Tr$ represents the <a href=../1924>trace</a>. To find $\boldsymbol{\mu}$ that minimizes the above equation, we compute the gradient, and by the gradient formula of a quadratic matrix:</p><p>$$
\begin{align*}
\nabla_{\boldsymbol{\mu}} \tilde{J}
&= \dfrac{1}{T} \sum\limits_{t=1}^{T} \left[ \mathbf{M}^{\mathsf{T}} \mathbf{M} \left( \boldsymbol{\mu} - \mathbf{x}(t) \right) \right] \\
&= \dfrac{1}{T} \sum\limits_{t=1}^{T} \mathbf{M}^{\mathsf{T}} \mathbf{M} \boldsymbol{\mu} - \dfrac{1}{T} \sum\limits_{t=1}^{T} \mathbf{M}^{\mathsf{T}} \mathbf{M} \mathbf{x}(t) \\
&= \mathbf{M}^{\mathsf{T}} \mathbf{M} \boldsymbol{\mu} - \mathbf{M}^{\mathsf{T}} \mathbf{M} \dfrac{1}{T} \sum\limits_{t=1}^{T} \mathbf{x}(t) \\
\end{align*}
$$</p><p>So the $\boldsymbol{\mu}$ that satisfies $\nabla_{\boldsymbol{\mu}} \tilde{J} = \mathbf{0}$ is the <a href=../2438>sample mean</a>.</p><p>$$
\boldsymbol{\mu}^{\ast} = \dfrac{1}{T} \sum\limits_{t=1}^{T} \mathbf{x}(t)
$$</p><blockquote><p><a href=../3675>Matrix Derivative of a Scalar Function:</a></p><p>$$
\nabla_{\mathbf{X}} (\Tr \mathbf{X}) = I
$$</p><p>$$
\nabla_{\mathbf{X}} (\mathbf{a}^{\mathsf{T}}\mathbf{X}^{\mathsf{T}}\mathbf{X}\mathbf{a}) = 2\mathbf{X}\mathbf{a}\mathbf{a}^{\mathsf{T}}
$$</p></blockquote><p>By this formula, computing $\nabla_{\mathbf{M}} \tilde{J}$ (which matches the result in the paper, though the following expression is a more simplified presentation):</p><p>$$
\begin{align*}
\nabla_{\mathbf{M}} \tilde{J}
&= \dfrac{1}{T} \sum\limits_{t=1}^{T} [-I + \mathbf{M}\left( \mathbf{x}(t) - \boldsymbol{\mu} \right)\left( \mathbf{x}(t) - \boldsymbol{\mu} \right)^{\mathsf{T}}] \\
&= -I + \mathbf{M} \sum\limits_{t=1}^{T}\left( \mathbf{x}(t) - \boldsymbol{\mu} \right)\left( \mathbf{x}(t) - \boldsymbol{\mu} \right)^{\mathsf{T}}
\end{align*}
$$</p><p>For the above equation to be $\mathbf{0}$, $\mathbf{M}$ should become the inverse of the sample covariance matrix $\sum\limits_{t=1}^{T}\left( \mathbf{x}(t) - \boldsymbol{\mu} \right)\left( \mathbf{x}(t) - \boldsymbol{\mu} \right)^{\mathsf{T}}$.</p><p>As evident from the result, Score Matching provides an estimator equivalent to the maximum likelihood estimation.</p><h2 id=appendix-a-proof-of-theorem-1>Appendix A. Proof of Theorem 1</h2><p>Expanding the norm $\| \cdot \|^{2} = \Braket{\cdot, \cdot}$ of $(1)$ gives the following:</p><p>$$
\begin{align*}
J(\boldsymbol{\theta})
&= \dfrac{1}{2} \int p_{\mathbf{x}}(\boldsymbol{\xi}) \left\| \psi(\boldsymbol{\xi}; \boldsymbol{\theta}) - \psi_{\mathbf{x}}(\boldsymbol{\xi}) \right\|^{2} \mathrm{d}\boldsymbol{\xi} \\
&= \dfrac{1}{2} \int p_{\mathbf{x}}(\boldsymbol{\xi}) \Braket{ \psi(\boldsymbol{\xi}; \boldsymbol{\theta}) - \psi_{\mathbf{x}}(\boldsymbol{\xi}), \psi(\boldsymbol{\xi}; \boldsymbol{\theta}) - \psi_{\mathbf{x}}(\boldsymbol{\xi})} \mathrm{d}\boldsymbol{\xi} \\
&= \dfrac{1}{2} \int p_{\mathbf{x}}(\boldsymbol{\xi}) \Big[ \Braket{ \psi(\boldsymbol{\xi}; \boldsymbol{\theta}), \psi(\boldsymbol{\xi}; \boldsymbol{\theta}) } + \Braket{\psi_{\mathbf{x}}(\boldsymbol{\xi}), \psi_{\mathbf{x}}(\boldsymbol{\xi})} - 2\Braket{\psi(\boldsymbol{\xi}; \boldsymbol{\theta}), \psi_{\mathbf{x}}(\boldsymbol{\xi})} \Big] \mathrm{d}\boldsymbol{\xi} \\
&= \int p_{\mathbf{x}}(\boldsymbol{\xi}) \Big[ \dfrac{1}{2}\| \psi(\boldsymbol{\xi}; \boldsymbol{\theta})\|^{2} + \dfrac{1}{2}\| \psi_{\mathbf{x}}(\boldsymbol{\xi})\|^{2} - \Braket{\psi(\boldsymbol{\xi}; \boldsymbol{\theta}), \psi_{\mathbf{x}}(\boldsymbol{\xi})} \Big] \mathrm{d}\boldsymbol{\xi}
\end{align*}
$$</p><p>Examining the integral of only the third term, we have:</p><p>$$
-\sum\limits_{i} \int p_{\mathbf{x}}(\boldsymbol{\xi}) \psi_{\mathbf{x}, i}(\boldsymbol{\xi}) \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \mathrm{d}\boldsymbol{\xi}
$$</p><p>For each $i$, the integral can be rewritten as follows, applying the <a href=../3047>derivative rule of the logarithm</a>:</p><p>$$
\begin{align*}
-\int p_{\mathbf{x}}(\boldsymbol{\xi}) \psi_{\mathbf{x}, i}(\boldsymbol{\xi}) \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \mathrm{d}\boldsymbol{\xi}
&= -\int p_{\mathbf{x}}(\boldsymbol{\xi}) \dfrac{\partial \log p_{\mathbf{x}}(\boldsymbol{\xi})}{\partial \xi_{i}} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \mathrm{d}\boldsymbol{\xi} \\
&= -\int p_{\mathbf{x}}(\boldsymbol{\xi})\left( \dfrac{1}{p_{\mathbf{x}}(\boldsymbol{\xi})} \dfrac{\partial p_{\mathbf{x}}(\boldsymbol{\xi})}{\partial \xi_{i}} \right)\psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \mathrm{d}\boldsymbol{\xi} \\
&= -\int \dfrac{\partial p_{\mathbf{x}}(\boldsymbol{\xi})}{\partial \xi_{i}} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \mathrm{d}\boldsymbol{\xi} \\
\end{align*}
$$</p><p>This can be further rewritten using the <a href=../1867>integration by parts</a>:</p><p>$$
\begin{align*}
& -\int \dfrac{\partial p_{\mathbf{x}}(\boldsymbol{\xi})}{\partial \xi_{i}} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \mathrm{d}\boldsymbol{\xi} \\
&= -\int \left( \int \dfrac{\partial p_{\mathbf{x}}(\boldsymbol{\xi})}{\partial \xi_{i}} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \mathrm{d}\xi_{1} \right) \mathrm{d}(\xi_{2},\dots,\xi_{n}) \\
&= -\int \left( \left[p_{\mathbf{x}}(\boldsymbol{\xi}) \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \right]_{\xi_{1}=-\infty}^{\infty} - \int p_{\mathbf{x}}(\boldsymbol{\xi}) \dfrac{\partial \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta})}{\partial \xi_{i}} \mathrm{d}\xi_{1} \right) \mathrm{d}(\xi_{2},\dots,\xi_{n}) \\
&= \int \int p_{\mathbf{x}}(\boldsymbol{\xi}) \dfrac{\partial \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta})}{\partial \xi_{i}} \mathrm{d}\xi_{1}\mathrm{d}(\xi_{2},\dots,\xi_{n}) \\
&= \int p_{\mathbf{x}}(\boldsymbol{\xi}) \dfrac{\partial \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta})}{\partial \xi_{i}} \mathrm{d}\boldsymbol{\xi} = \int p_{\mathbf{x}}(\boldsymbol{\xi}) \partial_{i} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \mathrm{d}\boldsymbol{\xi}
\end{align*}
$$</p><p>Here, since $p_{\mathbf{x}}$ is a probability density function, it must be integrable, leading to $\lim\limits_{\boldsymbol{\xi} \to \pm \infty}p_{\mathbf{x}}(\boldsymbol{\xi}) = 0$, thus the definite integral value in brackets is $0$. Substituting and rearranging, we get:</p><p>$$
\begin{align*}
J(\boldsymbol{\theta})
&= \int p_{\mathbf{x}}(\boldsymbol{\xi}) \Big[ \dfrac{1}{2}\| \psi(\boldsymbol{\xi}; \boldsymbol{\theta})\|^{2} + \dfrac{1}{2}\| \psi_{\mathbf{x}}(\boldsymbol{\xi})\|^{2} - \Braket{\psi(\boldsymbol{\xi}; \boldsymbol{\theta}), \psi_{\mathbf{x}}(\boldsymbol{\xi})} \Big] \mathrm{d}\boldsymbol{\xi} \\
&= \int p_{\mathbf{x}}(\boldsymbol{\xi}) \Big[ \dfrac{1}{2}\| \psi(\boldsymbol{\xi}; \boldsymbol{\theta})\|^{2} + \sum\limits_{i}\partial_{i} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \Big] \mathrm{d}\boldsymbol{\xi} + \dfrac{1}{2}\int p_{\mathbf{x}}(\boldsymbol{\xi}) \| \psi_{\mathbf{x}}(\boldsymbol{\xi})\|^{2} \mathrm{d}\boldsymbol{\xi} \\
&= \int p_{\mathbf{x}}(\boldsymbol{\xi}) \Big[ \dfrac{1}{2}\| \psi(\boldsymbol{\xi}; \boldsymbol{\theta})\|^{2} + \sum\limits_{i}\partial_{i} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \Big] \mathrm{d}\boldsymbol{\xi} + \dfrac{1}{2}\int p_{\mathbf{x}}(\boldsymbol{\xi}) \| \psi_{\mathbf{x}}(\boldsymbol{\xi})\|^{2} \mathrm{d}\boldsymbol{\xi} \\
\end{align*}
$$</p><p>The last term is a constant not depending on $\boldsymbol{\theta}$. Hence, we obtain:</p><p>$$
J(\boldsymbol{\theta}) = \int p_{\mathbf{x}}(\boldsymbol{\xi}) \sum\limits_{i}\Big[ \frac{1}{2} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta})^{2} + \partial_{i} \psi_{i}(\boldsymbol{\xi}; \boldsymbol{\theta}) \Big] \mathrm{d}\boldsymbol{\xi} + \text{constant}
$$</p><p style=text-align:right;margin:0;margin-top:-2em>■</p><h2 id=appendix-b-proof-of-theorem-2>Appendix B. Proof of Theorem 2</h2><p>$$
J(\boldsymbol{\theta}) = 0 \iff \boldsymbol{\theta} = \boldsymbol{\theta}^{\ast}
$$</p><p><strong>($\implies$)</strong></p><p>Assume $J(\boldsymbol{\theta}) = 0$.</p><p>$$
J = \int p_{x}(\boldsymbol{\xi}) \| \psi(\boldsymbol{\xi}; \boldsymbol{\theta}) - \psi_{\mathbf{x}}(\boldsymbol{\xi}) \|^{2} \mathrm{d}\boldsymbol{\xi} = 0 = \left\langle p_{x}, \| \psi - \psi_{\mathbf{x}} \|^{2} \right\rangle
$$</p><p>For all $\boldsymbol{\xi}$, since $q \gt 0$, $p(\boldsymbol{\xi}) \gt 0$ is $\forall \boldsymbol{\xi}$, and since $\| \psi - \psi_{\mathbf{x}} \|^{2} \ge 0$, for all $\boldsymbol{\xi}$, it must be $\| \psi - \psi_{\mathbf{x}} \|^{2} = 0$.</p><p>$$
J = 0 = \left\langle p_{x}, \| \psi - \psi_{\mathbf{x}} \|^{2} \right\rangle \implies \| \psi - \psi_{\mathbf{x}} \|^{2} = 0 \implies \psi = \psi_{\mathbf{x}}
$$</p><p>This implies the following:</p><p>$$
\psi_{\mathbf{x}} = \psi \implies \log p_{\mathbf{x}}(\cdot) = \log p( \cdot; \boldsymbol{\theta}) + \text{constant}
$$</p><p>Given $p$, probability density function $p_{\mathbf{x}}$ should integrate to $1$, meaning the constant must be $0$. Hence, $p_{\mathbf{x}}(\cdot) = p( \cdot; \boldsymbol{\theta})$. By assumption, $\boldsymbol{\theta} = \boldsymbol{\theta}^{\ast}$.</p><p style=text-align:right;margin:0;margin-top:-2em>■</p><p><strong>($\impliedby$)</strong></p><p>It is obvious.</p><p style=text-align:right;margin:0;margin-top:-2em>■</p><aside style=text-align:right>2025-07-21&emsp;
전기현&emsp;
<a href=../513>🎲 3676</a></aside><script>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="Paper Review: Score Matching",c="https://freshrimpsushi.github.io/en/posts/3676/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script></div><aside><h2>Comment</h2><div class=area-reply><div class=list-reply></div></div><div class=write-box><div class="write-author-info box-account"><input type=hidden name=cmt_idx>
<input class=ai-author type=text placeholder=Name>
<input class=ai-password type=password maxlength=20 placeholder=Password></div><div class=write-content><textarea class=ai-content value placeholder='Feel free to ask in english' style=ime-mode:active></textarea></div><button class=write-button onclick=write_comment() aria-label=send><i class='fa-solid fa-paper-plane'></i></button><aside class=tex>$\TeX$ is also applied to comments.</aside></div></aside><script>let commentRows=[];const listReply=document.querySelector(".list-reply");document.addEventListener("DOMContentLoaded",()=>{get_all_comment()});function get_all_comment(){fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=getAllComment";return postFetch(t,{board_idx:"3676",user_ip:e})}).then(e=>e.json()).then(e=>{e.ok&&(commentRows=e.rows,render_comment(commentRows))}).catch(e=>console.error(e))}function render_comment(e){const n=["류대식","전기현","ㅇㅇ","질문"],s=["류대식","전기현"];render_blank();let t="";e.map(e=>{t+=`<div id="comment${e.cmt_idx}" class="parents">`,t+=`<div class="content-info">`;let o="";e.cmt_cnt>125?o="🥇":e.cmt_cnt>25?o="🥈":e.cmt_cnt>5&&(o="🥉"),n.includes(e.author)&&(o="");let i="";e.ip_address==null?i="(-)":(ipParts=e.ip_address.split("."),i=`(${ipParts[0]}.${ipParts[1]})`),s.includes(e.author)&&(i=""),t+=`<div class="list-author">${o} ${e.author} ${i}</div>`,t+=`<sup class="list-date">${e.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,e.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> Waiting for approvement</div>`);let a=e.content;a=a.replace(/\n/g,"<br />"),t+=`<div class="content-text">${a} <span class="re-comment-button" onclick="re_comment('${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`,e.child.map(o=>{let c="child";e.ip_address&&o.ip_address===e.ip_address&&(c="parentself"),t+=`<div id="comment${o.cmt_idx}" class="${c}">`,t+=`<div class="content-info">`;let i="";o.cmt_cnt>5?i="🥉":o.cmt_cnt>25?i="🥈":o.cmt_cnt>125&&(i="🥇"),n.includes(o.author)&&(i="");let a="";o.ip_address==null?a="(-)":a=`(${o.ip_address})`,s.includes(o.author)&&(a=""),t+=`<div class="list-author">${i} ${o.author} ${a}</div>`,t+=`<sup class="list-date">${o.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,o.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> Waiting for approvement</div>`);let r=o.content;r=r.replace(/\n/g,"<br />"),t+=`<div class="content-text">${r} <span class="re-comment-button" onclick="re_comment('${o.cmt_idx}', '${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`})}),listReply.innerHTML=t,renderKaTex(listReply),location.href.indexOf("#")!=-1&&(location.href=location.href.substr(location.href.indexOf("#")))}function render_blank(){listReply.innerHTML=""}function write_comment(){const e=document.querySelector(".ai-author"),t=document.querySelector(".ai-password"),n=document.querySelector(".ai-content"),s=e.value,o=t.value,i=n.value;if(s===""||o===""||i===""){alert("빈칸을 채워주세요");return}const a="3676",r="",c="Paper Review: Score Matching";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeComment";return postFetch(t,{board_idx:a,board_slug:r,board_title:c,author:s.replace(/\+/g,"%2B"),password:o,content:i.replace(/\+/g,"%2B"),user_ip:e})}).then(e=>e.json()).then(s=>{s.ok?(commentRows.push(s.row),render_comment(commentRows),e.value="",t.value="",n.value=""):s.status===606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),e.value="",t.value="",n.value=""):s.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}async function update_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 수정",input:"password",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(s=>{if(s.ok){const s=document.querySelector(`#comment${e}`),o="https://freshrimpsushi.com/blog/ajax/comment.php?action=getComment";postFetch(o,{cmt_idx:e}).then(e=>e.json()).then(o=>{if(o.ok){const a=o.row,r=a.author,c=a.content;let i="";i+=`<div class="update-author-info">`,i+=`<input class="update-author" type="text" value="${r}" placeholder="이름" />`,i+=`<input class="update-password" type="password" value="${n}" placeholder="비밀번호" disabled />`,i+="</div>",i+=`<textarea class="update-content" value="" style="IME-MODE:active;">${c}</textarea>`,i+=`<input class="update_comment-button" type="submit" value="수정" onclick="update_comment_click(${e}, '${t}')" />`,i+=`<input class="update_comment-button" type="submit" value="취소" onclick="cancel_click(${e})" />`,s.innerHTML=i}}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}async function delete_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 삭제",text:"삭제하면 되돌릴 수 없습니다.",input:"password",icon:"warning",showCancelButton:!0,confirmButtonColor:"#3085d6",cancelButtonColor:"#d33",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(n=>{if(n.ok){const n="https://freshrimpsushi.com/blog/ajax/comment.php?action=deleteComment";postFetch(n,{cmt_idx:e}).then(e=>e.json()).then(n=>{Swal.fire("삭제되었습니다."),t==="parents"?commentRows=commentRows.filter(t=>t.cmt_idx!==e):t==="child"&&commentRows.map(t=>{t.child=t.child.filter(t=>t.cmt_idx!==e),t.child_cnt=t.child.length}),render_comment(commentRows)}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}function update_comment_click(e,t){const i=document.querySelector(`#comment${e} .update-author`),a=document.querySelector(`#comment${e} .update-password`),r=document.querySelector(`#comment${e} .update-content`),n=i.value,s=a.value,o=r.value;(n===""||s===""||o==="")&&alert("빈칸을 채워주세요");const c="https://freshrimpsushi.com/blog/ajax/comment.php?action=updateComment";postFetch(c,{cmt_idx:e,author:n.replace(/\+/g,"%2B"),password:s,content:o.replace(/\+/g,"%2B")}).then(e=>e.json()).then(n=>{n.ok&&(Swal.fire("수정되었습니다."),t==="parents"?commentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}):t==="child"&&commentRows.map(t=>{t.cmt_idx==n.parent_cmt_idx&&t.child.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)})}),render_comment(commentRows),recentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}),render_recent_comment(recentRows))}).catch(e=>console.error(e))}function cancel_click(){render_comment(commentRows)}function re_comment(e,t){const s=document.querySelector(`#comment${e} .re-comment`);let n="";n+='<div class="re-comment-box">',n+='<div class="re-comment-author-info">',n+='<input class="re-comment-author" type="text" value="" placeholder="이름" />',n+='<input class="re-comment-password" type="password" value="" placeholder="비밀번호" />',n+="</div>",n+='<textarea class="re-comment-content" placeholder="내용" value="" style="IME-MODE:active;"></textarea>',t===void 0?n+=`<button class="write-button" onclick="re_comment_click(${e})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`:n+=`<button class="write-button" onclick="re_comment_click(${e}, ${t})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`,n+=`<button class="write-button" onclick="cancel_click(${e})" value="close"><i class='fa-solid fa-solid fa-ban'></i></button>`,n+="</div>",s.innerHTML=n}function re_comment_click(e,t){const n=document.querySelector(`#comment${e} .re-comment-author`),s=document.querySelector(`#comment${e} .re-comment-password`),o=document.querySelector(`#comment${e} .re-comment-content`),i=n.value,a=s.value,r=o.value;if(i===""||a===""||r===""){alert("빈칸을 채워주세요");return}const c="3676",l="",d="Paper Review: Score Matching";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(n=>{const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeReComment";return postFetch(s,{board_idx:c,board_slug:l,board_title:d,cmt_idx:t===void 0?e:t,author:i.replace(/\+/g,"%2B"),password:a,content:r.replace(/\+/g,"%2B"),user_ip:n})}).then(e=>e.json()).then(i=>{i.ok?(commentRows.map(n=>{t===void 0?n.cmt_idx==e&&n.child.push(i.row):n.cmt_idx==t&&n.child.push(i.row)}),render_comment(commentRows)):i.status==606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),n.value="",s.value="",o.value=""):i.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}</script></div><aside class=sidebar><aside><style>.reddot a:link{color:#67d10f;text-shadow:0 0 10px #d3d3d3;font-weight:700}.reddot a:visited{color:#f5f5f5}</style><p class=reddot style=text-align:center;margin:0><a href=https://freshrimpsushi.github.io/en/posts/2707/>Summer Special Omakase<br>「Imaginary Numbers」</a></p></aside><br><div class=category></div><div style=display:flex>Click ● to highlight only you interested.<div class=resetmute><button class="sb-btn btnReset" style=border-radius:5px;border:0>reset</button>
<button class="sb-btn btnMute" style=border-radius:5px;border:0>mute</button></div></div><script defer>const color={green:"#33cc33",yellow:"#ffcc00",red:"#ff3333",black:"#000000"},categoryRows=[[{idx:1,name:"함수",color:color.green,show:"Functions",size:"146"},{idx:2,name:"보조정리",color:color.green,show:"Lemmas",size:"55"},{idx:3,name:"미분적분학",color:color.green,show:"Calculus",size:"45"},{idx:4,name:"행렬대수",color:color.green,show:"Matrix Algebra",size:"117"}],[{idx:1,name:"정수론",color:color.green,show:"Number Theory",size:"90"},{idx:2,name:"집합론",color:color.green,show:"Set Theory",size:"49"},{idx:3,name:"그래프이론",color:color.green,show:"Graph Theory",size:"65"},{idx:4,name:"선형대수",color:color.green,show:"Linear Algebra",size:"97"},{idx:5,name:"해석개론",color:color.green,show:"Analysis",size:"84"},{idx:6,name:"추상대수",color:color.green,show:"Abstract Algebra",size:"98"},{idx:7,name:"위상수학",color:color.green,show:"Topology",size:"64"},{idx:8,name:"기하학",color:color.green,show:"Geometry",size:"167"}],[{idx:1,name:"다변수벡터해석",color:color.green,show:"Vector Analysis",size:"37"},{idx:2,name:"복소해석",color:color.green,show:"Complex Anaylsis",size:"71"},{idx:3,name:"측도론",color:color.green,show:"Measure Theory",size:"53"},{idx:4,name:"푸리에해석",color:color.green,show:"Fourier Analysis",size:"54"},{idx:5,name:"표현론",color:color.red,show:"Representation Theory",size:"7"},{idx:6,name:"초함수론",color:color.green,show:"Distribution Theory",size:"22"},{idx:7,name:"단층촬영",color:color.green,show:"Tomography",size:"20"}],[{idx:1,name:"거리공간",color:color.green,show:"Metric Space",size:"38"},{idx:2,name:"바나흐공간",color:color.green,show:"Banach Space",size:"38"},{idx:3,name:"힐베르트공간",color:color.green,show:"Hilbert Space",size:"31"},{idx:4,name:"르벡공간",color:color.green,show:"Lebesgue Space",size:"33"}],[{idx:1,name:"상미분방정식",color:color.green,show:"ODE",size:"58"},{idx:2,name:"편미분방정식",color:color.green,show:"PDE",size:"60"},{idx:3,name:"확률미분방정식",color:color.green,show:"SDE",size:"26"}],[{idx:1,name:"줄리아",color:color.green,show:"Julia",size:"234"},{idx:2,name:"알고리즘",color:color.green,show:"Algorithm",size:"28"},{idx:3,name:"수치해석",color:color.green,show:"Numerical Analysis",size:"63"},{idx:4,name:"최적화이론",color:color.green,show:"Optimization Theory",size:"37"},{idx:5,name:"머신러닝",color:color.green,show:"Machine Learning",size:"114"},{idx:6,name:"프로그래밍",color:color.yellow,show:"Programming",size:"123"},{idx:7,name:"세이버메트릭스",color:color.green,show:"Sabermetrics",size:"234",size:"13"}],[{idx:1,name:"물리학",color:color.green,show:"Physics",size:"30"},{idx:2,name:"수리물리",color:color.green,show:"Mathematical Physics",size:"77"},{idx:3,name:"고전역학",color:color.green,show:"Classical Mechanics",size:"48"},{idx:4,name:"전자기학",color:color.green,show:"Electrodynamics",size:"51"},{idx:5,name:"양자역학",color:color.green,show:"Quantum Mechanics",size:"57"},{idx:6,name:"열물리학",color:color.green,show:"Thermal Physics",size:"29"}],[{idx:1,name:"R",color:color.green,show:"R",size:"54"},{idx:2,name:"데이터확보",color:color.green,show:"Data Sets",size:"29"},{idx:3,name:"데이터과학",color:color.green,show:"Data Science",size:"41"},{idx:4,name:"통계적검정",color:color.green,show:"Statistical Test",size:"33"},{idx:5,name:"통계적분석",color:color.green,show:"Statistical Analysis",size:"76"},{idx:6,name:"수리통계학",color:color.green,show:"Mathematical Statistics",size:"123"},{idx:7,name:"확률분포론",color:color.green,show:"Probability Distribution",size:"84"},{idx:8,name:"확률론",color:color.green,show:"Probability Theory",size:"80"},{idx:9,name:"위상데이터분석",color:color.green,show:"TDA",size:"40"}],[{idx:1,name:"논문작성",color:color.red,show:"Writing",size:"63"},{idx:2,name:"생새우초밥지",color:color.black,show:"JOF",size:"7"}]];document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("blindList"));(e==""||e==null||e==null||e==0||e==NaN)&&localStorage.setItem("blindList",null),render_category(categoryRows),blind_category(e);const t=document.querySelector(".btnReset");t.addEventListener("click",()=>{let e=new Array;localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)});const n=document.querySelector(".btnMute");n.addEventListener("click",()=>{let e=new Array;for(let t=0;t<categoryRows.length;t++)categoryRows[t].map(n=>{const s={mainIdx:t,subIdx:n.idx};e.push(s)});localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)})});function render_category(e){const n=document.querySelector(".category");let t="";for(let n=0;n<e.length;n++)e[n].map(e=>{t+=`<span id="cate${n}-${e.idx}" class="cate etewsert">`,t+=`<span onclick="check_blind(${n}, ${e.idx});" style="cursor: pointer; color: ${e.color}">●</span>`,t+=`<a href="https://freshrimpsushi.github.io/en/categories/${e.name.toLowerCase()}/">`,t+=` ${e.show} (${e.size})</a>`,t+="</span>",screen.width>954?t+="<br>":t+=" "}),t+="<hr>";n.innerHTML=t}function check_blind(e,t){const o=document.querySelector(`#cate${e}-${t}`);let n=new Array;const i={mainIdx:e,subIdx:t};let s=JSON.parse(localStorage.getItem("blindList"));if(s==""||s==null||s==null||s==0||s==NaN)n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind";else{n=s;let a=null;for(let s=0;s<n.length;s++)if(n[s].mainIdx==e&&n[s].subIdx==t){a=n.filter(n=>n.mainIdx!=e||n.subIdx!=t);break}a===null?(n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind"):(localStorage.setItem("blindList",JSON.stringify(a)),o.className="cate")}}function blind_category(e){const t=document.querySelectorAll(".cate");t.forEach(e=>{e.className="cate"}),e!==null&&e.map(e=>{const t=document.querySelector(`#cate${e.mainIdx}-${e.subIdx}`);t.className+=" blind"})}</script><br><br><b>Viewed posts</b><div class=lately-viewed-list style=padding-left:4px></div><script defer>document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("latelyViewPostList")),n=document.querySelector(".lately-viewed-list");let t="";if(e==""||e==null||e==null||e==0||e==NaN)localStorage.setItem("latelyViewPostList",null),t+='<div class="lv-list">',t+=" · 열람한 포스트가 없습니다.",t+="</div>",n.innerHTML=t;else{for(let n=e.length-1;n>=0;n--)t+='<div class="lv-list">',t+=`<a href="${e[n].link}"> · ${e[n].title}</a>`,t+="</div>";n.innerHTML=t}})</script><script defer>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="Paper Review: Score Matching",c="https://freshrimpsushi.github.io/en/posts/3676/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script><br><b>Recent comment</b><div class=current-reply></div><script defer>const url="https://freshrimpsushi.com/blog/ajax/recent_comment.php?action=getCurrentComment";let recentRows=[];fetch(url).then(e=>e.json()).then(e=>{e.ok&&(recentRows=e.rows,render_recent_comment(recentRows))}).catch(e=>console.error(e));function render_recent_comment(e){const n=document.querySelector(".current-reply");let t="";e.map(e=>{let n="https://freshrimpsushi.github.io/en/";e.board_idx>-1?n+=`posts/${e.board_idx}#comment${e.cmt_idx}`:e.board_idx==-1?n+=`#comment${e.cmt_idx}`:n+=`categories/${e.board_title}#comment${e.cmt_idx}`,t+='<div class="current-reply-list">',t+=`<a href="${n}">`,t+=`<b> - ${e.author}</b>: `,t+=`${e.content}`,t+=`</a>`,t+=`</div>`}),n.innerHTML=t,renderKaTex(n)}</script><br></aside></div><footer><aside><div><p id=mirror-link style=text-align:center><a style=cursor:text href=http://localhost:1313//en/posts/3676/>© FreshrimpRestaurant / Powered by 류대식, 전기현</a><br>Contact:
<img src=https://freshrimpsushi.github.io/en/logo/gmail.png width=12px alt=mail> freshrimpsushi@gmail.com
<a href=https://freshrimpsushi.github.io/en/index.xml><img src=https://freshrimpsushi.github.io/en/logo/RSS.png width=12px alt=RSS> RSS</a></p></div><script type=text/javascript>var goIndex=function(){var e=document.getElementsByName("idx")[0].value,t="https://freshrimpsushi.github.io/en/posts/"+e;location.replace(t)};document.addEventListener("keydown",function(e){const n=document.getElementById("navigator");if(e.altKey&&e.ctrlKey&&e.key==="l"){e.preventDefault();var t=document.querySelector("#mirror-link a");t?t.click():console.log("거울 링크를 찾지 못했습니다.")}})</script></aside></footer></body><script async src="https://www.googletagmanager.com/gtag/js?id=G-NLV8Y9PRK1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NLV8Y9PRK1")</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4751085325232621" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/sweetalert2@11></script><script src=https://freshrimpsushi.github.io/en/js/fontawsome.min.js></script><script src=https://freshrimpsushi.github.io/en/js/common.js></script><script>document.addEventListener("DOMContentLoaded",()=>{fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/ip_checker.php";return postFetch(t,{user_ip:e})}).then(e=>e.json()).then(e=>{e.ok||(alert(`차단된 IP입니다.
Contact:
freshrimpsushi@gmail.com`),window.location.href="https://google.com")}).catch(e=>console.error(e))})</script></html>