<!doctype html><html class=blog lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=preload href=https://freshrimpsushi.github.io/en/css/style.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=preload href=https://freshrimpsushi.github.io/en/css/comment.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=icon href=https://freshrimpsushi.github.io/ko/logo/favicon.ico><meta name=msapplication-TileColor content="#FFFFFF"><meta name=msapplication-TileImage content="logo/basic.png"><meta name=NaverBot content="All"><meta name=NaverBot content="index,follow"><meta name=Yeti content="All"><meta name=Yeti content="index,follow"><meta name=google-site-verification content="KYAokS7-6C5YuXOjatJQsiK1T0O8x4YncYFIF4tneYI"><meta name=naver-site-verification content="e5651d6f97899061897203413efc84994f04bbba"><link rel=alternate type=application/rss+xml title=FreshrimpRestaurant href=https://freshrimpsushi.github.io/en/index.xml><title>Paper Review: Neural Ordinary Differential Equations</title></head><meta name=title content="Paper Review: Neural Ordinary Differential Equations"><meta name=description content="국내 최대의 수학, 물리학, 통계학 블로그"><meta property="og:title" content="Paper Review: Neural Ordinary Differential Equations"><meta property="og:description" content><meta property="og:image" content="https://freshrimpsushi.github.io/en/logo/basic.png/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"j_","name":"Paper Review: Neural Ordinary Differential Equations","headline":"Paper Review: Neural Ordinary Differential Equations","alternativeHeadline":"","description":"Overview and Summary \u0026ldquo;Neural Ordinary Differential Equations\u0026rdquo; is a paper published in 2018 by Ricky T. Q. Chen and three others, and it was selected for 2018 NeurIPS Best Papers. It proposes a method to approximate a simple first-order differential equation, which is a non-autonomous system, using neural networks. $$ \\dfrac{\\mathrm{d}y}{\\mathrm{d}t} = f(y, t) $$ Notably, what the neural network approximates (predicts) is not the $y$ but the rate of","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/freshrimpsushi.github.io\/en\/posts\/3159\/"},"author":{"@type":"Person","name":"전기현","url":"https://math.stackexchange.com/users/459895/ryu-dae-sick"},"creator":{"@type":"Person","name":"전기현"},"accountablePerson":{"@type":"Person","name":"전기현"},"copyrightHolder":"FreshrimpRestaurant","copyrightYear":"2021","dateCreated":"2021-12-15T00:00:00.00Z","datePublished":"2021-12-15T00:00:00.00Z","dateModified":"2021-12-15T00:00:00.00Z","publisher":{"@type":"Organization","name":"FreshrimpRestaurant","url":"https://freshrimpsushi.github.io/en/","logo":{"@type":"ImageObject","url":"https:\/\/freshrimpsushi.github.io\/en\/logo\/basic.png","width":"32","height":"32"}},"image":"https://freshrimpsushi.github.io/en/logo/basic.png","url":"https:\/\/freshrimpsushi.github.io\/en\/posts\/3159\/","wordCount":"2068","genre":[],"keywords":[]}</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/mhchem.min.js integrity=sha384-F2ptQFZqNJuqfGGl28mIXyQ5kXH48spn7rcoS0Y9psqIKAcZPLd1NzwFlm/bl1mH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>function renderKaTex(e){renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],trust:!0,trust:e=>["\\htmlId","\\href","\\includegraphics"].includes(e.command),macros:{"\\eqref":"(\\text{#1})","\\ref":"\\href{###1}{\\text{#1}}","\\label":"\\htmlId{#1}{}","\\sech":"\\operatorname{sech}","\\csch":"\\operatorname{csch}","\\sgn":"\\operatorname{sgn}","\\sign":"\\operatorname{sign}","\\sinc":"\\operatorname{sinc}","\\diag":"\\operatorname{diag}","\\diam":"\\operatorname{diam}","\\Tr":"\\operatorname{Tr}","\\tr":"\\operatorname{tr}","\\re":"\\operatorname{Re}","\\im":"\\operatorname{Im}","\\Var":"\\operatorname{Var}","\\Poi":"\\operatorname{Poi}","\\Cov":"\\operatorname{Cov}","\\span":"\\operatorname{span}","\\supp":"\\operatorname{supp}","\\rank":"\\operatorname{rank}","\\nullity":"\\operatorname{nullity}","\\Ric":"\\operatorname{Ric}","\\i":"\\mathrm{i}","\\d":"\\mathrm{d}","\\cR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5863.png}","\\acR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.4371.png}","\\bcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5899.png}","\\abcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.0596.png}","\\crH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\smallcrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\acrH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}","\\smallacrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}"},throwOnError:!1})}</script><body class=main><header><a href=https://freshrimpsushi.github.io/en/ rel=home><p style=text-align:center;font-size:1rem;color:#000><img src=https://freshrimpsushi.github.io/en/logo/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D.png style=height:80px alt=logo></p></a></header><form method=get action=/en/search style=border:1px;text-align:center><div class=field><input type=text id=searchtext placeholder=🔍︎ class=input_text name=s style=background-color:#eee;text-align:center;width:200px;font-size:1.5rem;border:1px;border-radius:5px;padding-top:5px;padding-bottom:5px;margin-bottom:1.5rem></div></form><aside style=text-align:center;margin-bottom:1rem><a href=https://freshrimpsushi.github.io/ko//posts/3159/>한국어</a> |
<a href=https://freshrimpsushi.github.io/en//posts/3159/>English</a> |
<a href=https://freshrimpsushi.github.io/jp//posts/3159/>日本語</a></aside><div class=wrapper><div class=content><div class=content-box><title>Paper Review: Neural Ordinary Differential Equations</title>
<a href=https://freshrimpsushi.github.io/en/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/ style=background-color:rgba(0,0,0,.8);color:orange;border-radius:10px;padding:5px>📂Machine Learning</a><h1>Paper Review: Neural Ordinary Differential Equations</h1><aside><div class=innerheader><div class=innertoc><b>Table of Contents</b><nav id=TableOfContents><ul><li><a href=#overview-and-summary>Overview and Summary</a></li><li><a href=#1-introduction>1 Introduction</a></li><li><a href=#2-reverse-mode-automatic-differentiation-of-ode-solutions>2 Reverse-mode Automatic Differentiation of ODE Solutions</a></li><li><a href=#3-replacing-residual-networks-with-odes-for-supervised-learning>3 Replacing Residual Networks with ODEs for Supervised Learning</a></li><li><a href=#4-continuous-normalizing-flows>4 Continuous Normalizing Flows</a><ul><li><a href=#41-experiments-with-continuous-normalizing-flows>4.1 Experiments with Continuous Normalizing Flows</a></li></ul></li><li><a href=#5-a-generative-latent-function-time-series-model>5 A Generative Latent Function Time-Series Model</a><ul><li><a href=#51-time-series-latent-ode-experiments>5.1 Time-series Latent ODE Experiments</a></li></ul></li></ul></nav></div></div></aside><h2 id=overview-and-summary>Overview and Summary</h2><p>&ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf>Neural Ordinary Differential Equations</a>&rdquo; is a paper published in 2018 by Ricky T. Q. Chen and three others, and it was selected for <a href=https://neurips.cc/Conferences/2018/Awards>2018 NeurIPS Best Papers</a>. It proposes a method to approximate a <a href=../1660>simple first-order differential equation</a>, which is a <a href=../1505>non-autonomous system</a>, using neural networks.</p><p>$$
\dfrac{\mathrm{d}y}{\mathrm{d}t} = f(y, t)
$$</p><p>Notably, what the neural network approximates (predicts) is not the $y$ but the rate of change $f$. By integrating both sides of the above equation from any $0$ to $T$, we get the following:</p><p>$$
y(T) - y(0) = \int\limits_{0}^{T} f(y, t) \mathrm{d}t \implies y(T) = y(0) + \int\limits_{0}^{T} f(y, t) \mathrm{d}t
$$</p><p>Therefore, if $f$ can be accurately approximated, we can obtain $y(T)$ for any $T$.</p><h2 id=1-introduction>1 Introduction</h2><p>Consider a <a href=../900>time series dataset</a> consisting of 8 data points with equal time intervals, as depicted in the figure below.</p><p><img src=3159_1.png#center alt></p><p>In such a scenario, our typical objective is to predict data $\mathbf{h}_{9}$, $\mathbf{h}_{10}$, $\mathbf{h}_{11}$, $\dots$ for $t_{9}$, $t_{10}$, $t_{11}$, $\dots$. One straightforward approach is to model this using residual networks, <a href=../3634>RNN</a>, <a href=../3247>normalizing flow</a>, etc., as follows:</p><p>$$
\mathbf{h}_{t+1} = \mathbf{h}_{t} + f(\mathbf{h}_{t}; \theta) \tag{1}
$$</p><p>Here, $f$ is a <a href=../962>neural network</a> and $\theta$ are the <a href=../962>parameters</a> (weights) to be learned by $f$. While this method is simple and intuitive, and one of the first to be tried, it has the following disadvantages:</p><ul><li>It is difficult to <a href=../1016>interpolate</a> data. It is hard to predict data between $t_{3}$ and $t_{4}$ using a model like $(1)$, especially for arbitrary time $t_{3.472}$.</li><li>It is challenging to apply this to data with uneven time intervals. In $(1)$, the rule for updating $\mathbf{h}_{t+1}$ is adding $f$ one, two, or integer times, so if the time intervals are not uniform, the equation in $(1)$ does not hold.</li></ul><p>The method proposed in this paper is to conceptualize the discrete <a href=../1660>autonomous system</a> $(1)$ as a continuous <a href=../1660>non-autonomous system</a> as follows:</p><p>$$
\dfrac{\mathrm{d}\mathbf{h}}{\mathrm{d}t} = f(\mathbf{h}(t), t; \theta) \tag{2}
$$</p><p>A &ldquo;non-autonomous system where the external force (or velocity) $f$ is defined by an artificial neural network with parameters $\theta$&rdquo; is referred to as Neural Ordinary Differential Equations (Neural ODE, NODE).</p><p>Many high-performance <a href=../1093>solvers</a> (i.e., integrators of $f$) for solving such ODEs have already been researched, thus allowing us to predict $\mathbf{h}(t)$ for any time $t$ if $f$ is well approximated via neural networks. By integrating both sides:</p><p>$$
\mathbf{h}(t) = \mathbf{h}(0) + \int\limits_{0}^{t} f(\mathbf{h}(t), s; \theta) \mathrm{d}s \tag{3}
$$</p><p>The right-hand side can be obtained using an ODE solver given an initial value $\mathbf{h}(0)$ and $f$, which is denoted in the paper as follows:</p><p>$$
\mathbf{h}(t_{1}) = \operatorname{ODESolve}(\mathbf{h}(t_{0}), f, t_{0}, t_{1}, \theta)
$$</p><p>The advantages of defining the model using an ODE solver and calculating values are explained in the paper as follows:</p><ul><li><p><strong>Memory efficiency:</strong></p><p>In Chapter 2 below, a method for calculating the <a href=../1010>gradient</a> of the loss function without using the <a href=../3077>backpropagation algorithm</a> is introduced. Thus, we do not need to <a href=../3442>store intermediate computation results</a> when computing the model&rsquo;s function value, meaning memory usage is fixed regardless of model size.</p></li><li><p><strong>Adaptive computation:</strong></p><p>Over 120 years since the <a href=../687>Euler method</a>, more accurate and efficient ODE solvers have been developed, especially recent solvers offering dynamic evaluation strategy adjustments for error monitoring and high accuracy.</p></li><li><p><strong>Scalable and invertible normalizing flows:</strong></p><p>One advantage of continuous models is that they make variable transformation formula calculations easy, as explained in Chapter 4, meaning efficient <a href=../3247>normalizing flow</a> models can be created.</p></li><li><p><strong>Continuous time-series models:</strong></p><p>Unlike discrete RNNs that require evenly spaced data for training and prediction, continuously defined Neural ODEs can perform predictions at any time $t$, which is further discussed in Chapter 5.</p></li></ul><h2 id=2-reverse-mode-automatic-differentiation-of-ode-solutions>2 Reverse-mode Automatic Differentiation of ODE Solutions</h2><p>The core idea and principle of Neural ODE is explained under <a href=#1-Introduction>Introduction</a>. Chapter 2 covers the training methods. The author has developed a method to implement Neural ODE and <a href=https://github.com/rtqichen/torchdiffeq>publicly available it on GitHub</a>. If implementing it oneself is not necessary and there is no particular interest in the training methods, this chapter can be skipped. Chapters 3-5 cover potential applications of Neural ODE, therefore if one is interested in applying Neural ODE in their field, it is recommended to read Chapters 3-5 closely. Notably, Chapter 4 relates to flow matching, which is a leading trend in generative models following diffusion models.</p><p>The observed values (ground truth data) are constants independent of the neural network parameter $\theta$, so the loss function can be simplified as follows:</p><p>$$
L(\mathbf{z}(t_{1})) = L \left( \mathbf{z}(t_{0}) + \int_{t_{0}}^{t_{1}} f(\mathbf{z}(t), t; \theta) \mathrm{d}t \right) = L \left( \operatorname{OSESolve}(\mathbf{z}(t_{0}), f_{\theta}, t_{0}, t_{1}) \right)
$$</p><p>For calculating the <a href=../1010>gradient</a> of the loss function as discussed in the paper, the adjoint sensitivity method is used, which involves integrating another ODE in reverse time. The outcome is as follows:</p><p>$$
\dfrac{\mathrm{d}L}{\mathrm{d}\theta} = \int\limits_{t_{1}}^{t_{0}} \left( \dfrac{\partial L}{\partial \mathbf{z}(t)} \right)^{\mathsf{T}} \dfrac{\partial f(\mathbf{z}(t), t; \theta)}{\partial \theta} \mathrm{d}t
$$</p><p>Detailed information is provided in Appendix B, C, D <a href=https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Supplemental.zip>(Download Link)</a>. Also, the code for the examples described below and learning-related methods are included in the <code>torchdiffeq</code> package, publicly available on <a href=https://github.com/rtqichen/torchdiffeq/tree/master>GitHub</a>.</p><h2 id=3-replacing-residual-networks-with-odes-for-supervised-learning>3 Replacing Residual Networks with ODEs for Supervised Learning</h2><p>If Neural ODE is considered a replacement of $(1)$ with $(3)$, it can be seen as a generalization of residual networks.</p><p>$$
\text{residual: } \mathbf{h}_{t+1} = \mathbf{h}_{t} + f(\mathbf{h}_{t}; \theta) \quad\overset{\text{generalization}}{\implies}\quad \dfrac{\mathrm{d}\mathbf{h}}{\mathrm{d}t} = f(\mathbf{h}(t), t; \theta)
$$</p><p>Hence, Neural ODE can be used in supervised learning that utilizes ResNet. The authors compared performances by replacing ResNet parts with Neural ODE in the classification problem of the <a href=../3444>MNIST dataset</a>.</p><p>For numerically solving the initial value problems of ODEs, the chosen <a href=../1093>solver</a> is the <a href=../724>Adams method</a>, using the <a href=../2671>implicit method</a> provided by the <code>scipy.integrate</code> package. The optimization method, the adjoint sensitivity method, was implemented with <code>pytorch.autograd</code>.</p><p>Experimental results showed that ODE-Net demonstrated nearly similar performance to ResNet despite having approximately $1/3$ amount of parameters compared to ResNet. $L$ indicates the number of layers in ResNet; the computation time and memory required for backpropagation in ResNet are proportional to $L$. $\tilde{L}$ refers to ODE solver&rsquo;s number of function evaluations (NFE), which indicates how much the integration interval will be divided. ODE solvers, conducting numerical integration, get divided <a href=../1128>into integration intervals</a>, and NFE refers to the number of these intervals. If, for instance, $[0, 1]$ interval is integrated at $0.05$ intervals, it becomes $\text{NFE} = \dfrac{1}{0.05} = 20$. The paper mentions this can be interpreted as the layer count of ODE-Net. A larger NFE allows for more precise computation, but takes longer calculation time, so choosing an appropriate number is crucial. Neural ODE allows the model to choose NFE automatically.</p><p><img src=3159_3.png alt></p><p>Figures 3a and 3b respectively show the error and computation time with NFE during forward calculation, presenting results aligning well with intuition. Figure 3c demonstrates that backward calculations are approximately half the level of forward calculations, indicating the proposed adjoint sensitivity method is quite efficient. Figure 3d illustrates that NFE increases as training progresses, suggesting the model becomes more complex during training and that it becomes more refined to represent it.</p><p><img src=3159_4.png alt></p><h2 id=4-continuous-normalizing-flows>4 Continuous Normalizing Flows</h2><p>Discrete modeling like $(1)$ also occurs in <a href=../3247>normalizing flow</a>.</p><p>$$
\mathbf{z}_{1} = f(\mathbf{z}_{0})
$$</p><p>$$
\log p(\mathbf{z}_{1}) = \log p(\mathbf{z}_{0}) - \log \left| \det \dfrac{\partial f}{\partial \mathbf{z}_{0}} \right| \tag{a4}
$$</p><p>Take the planar flow, for example:</p><p>$$
\mathbf{z}_{t+1} = \mathbf{z}_{t} + \mathbf{u} \tanh (\mathbf{w}^{\mathsf{T}}\mathbf{z}_{t} + b)
$$</p><p>$$
\log p(\mathbf{z}_{t+1}) = \log p(\mathbf{z}_{t}) - \log \left| 1 + \mathbf{u}^{\mathsf{T}} \dfrac{\partial \tanh}{\partial \mathbf{z}} \right|
$$</p><p>The most critical aspect to consider when training a normalizing flow is the calculation of the <a href=../252>determinant</a> of $(a4)$. It has a cubic computational cost in terms of the dimension or the number of weights of $\mathbf{z}$. Interestingly, switching from the discrete structure $(1)$ to the continuous structure $(3)$ simplifies this related computation.</p><hr><p>$\textbf{Theorem 1 }\text{(Instantaneous Change of Variables).}$</p><p>Let $\mathbf{z}(t)$ be a <a href=../1433>continuous random variable</a> and $p(\mathbf{z}(t))$ its <a href=../1433>probability density function</a>. Suppose a <a href=../1660>non-autonomous differential equation</a> is given by:</p><p>$$
\dfrac{\mathrm{d} \mathbf{z}}{\mathrm{d}t} = f(\mathbf{z}(t), t) \tag{a5}
$$</p><p>Assume $f$ is <a href=../3541>Lipschitz continuous</a> concerning variable $\mathbf{z}$ and <a href=../1206>continuous</a> regarding variable $t$, the derivative of the log probability density is:</p><p>$$
\dfrac{\partial \log p(\mathbf{z}(t))}{\partial t} = -\Tr \left( \dfrac{\partial f}{\partial \mathbf{z}(t)} \right)
$$</p><p>Here, $\Tr$ denotes the <a href=../1924>trace</a>.</p><hr><p>The determinant calculation of $(a4)$ is transformed into a simple trace calculation. Unlike typical discrete normalizing flows, $f$ need not be <a href=../471>bijective</a>. Reformulating the example planar flow into a continuous form consistent with $\textbf{Theorem 1}$ yields the following:</p><p>$$
\dfrac{\mathrm{d}\mathbf{z}(t)}{\mathrm{d}t} = \mathbf{u} \tanh(\mathbf{w}^{\mathsf{T}}\mathbf{z}(t) + b), \qquad \dfrac{\partial \log p (\mathbf{z}(t))}{\partial t} = -\Tr \left( \mathbf{u} \dfrac{\partial \tanh(\mathbf{w}^{\mathsf{T}}\mathbf{z}(t) + b)}{\partial \mathbf{z}(t)} \right)
$$</p><p><strong>Using multiple hidden units with linear cost</strong></p><p>Although determinants are not linear, the <a href=../1924>trace is linear</a>, so $\Tr \sum\limits_{n} J_{n} = \sum\limits_{n} \Tr J_{n}$ holds. Therefore, if $f$ in $(a5)$ is expressed as a linear combination, the derivative of the log density function can be easily expressed as follows:</p><p>$$
\dfrac{\mathrm{d} \mathbf{z}(t)}{\mathrm{d}t} = \sum\limits_{n=1}^{M} f_{n}(\mathbf{z}(t)), \qquad \dfrac{\mathrm{d}\log p(\mathbf{z}(t))}{\mathrm{d}t} = -\sum\limits_{n=1}^{M} \Tr \left( \dfrac{\partial f_{n}}{\partial \mathbf{z}(t)} \right)
$$</p><p>This indicates a linear increase in computational cost concerning the number of hidden layer nodes $M$. Normalizing flows are typically constructed with a single node in many stacked layers due to the computational cost of $\mathcal{O}(M^{3})$.</p><p><strong>Time-dependent dynamics</strong></p><p>The paper employs a method called gating to define flows as follows:</p><p>$$
\dfrac{\mathrm{d} \mathbf{z}}{\mathrm{d}t} = \sum\limits_{n} \sigma_{n}(t)f_{n}(\mathbf{z}), \qquad \sigma_{n}(t) \in (0 1)
$$</p><p>That is, it is assumed that $f(\mathbf{z}, t)$ is <a href=../1985>variable-separable</a>. Here, both $\sigma_{n}$ and $f_{n}$ are neural networks to be learned. The paper calls this <strong>continuous normalizing flow</strong>. [Flow matching] is a method to train CNF more efficiently.</p><h3 id=41-experiments-with-continuous-normalizing-flows>4.1 Experiments with Continuous Normalizing Flows</h3><p>Compare CNF and NF. CNF is implemented as described above and trained for 10,000 iterations with the <a href=../3529>Adam</a> optimizer. NF is implemented per its <a href=../3247>originating paper</a> and trained for 500,000 iterations with the <a href=../3529>RMSprop</a> optimizer. CNF requires substantially fewer iterations. Results are visible in the figure below.</p><p><img src=3159_5.png alt></p><p>In the lower Figure 5, the first two rows appear to be results for CNF, and the last row for NF. CNF undergoes a smooth transformation, while NF does not, and fails to model the Two Moons dataset correctly.</p><p><img src=3159_6.png alt></p><h2 id=5-a-generative-latent-function-time-series-model>5 A Generative Latent Function Time-Series Model</h2><p>As Neural ODE approximates non-autonomous systems with neural networks, it can be applied to forecasting, interpolation, imputation of <a href=../900>time series data</a>. The training method proposed in the paper is as follows:</p><ol start=0><li>Suppose a time series dataset $\left\{ \mathbf{x}_{i}, t_{i} \right\}_{i=1}^{N}$ is given.</li><li>Use an <a href=../3380>encoder</a> implemented with RNN to extract <a href=../3589>latent variables</a> $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$.
$$
(\boldsymbol{\mu}, \log\boldsymbol{\sigma}^{2}) = \operatorname{RNNencoder}\left( \left\{ \mathbf{x}_{i}, t_{i} \right\} \right)
$$</li><li>Sample $\boldsymbol{\epsilon}$ from a Gaussian distribution $N(\mathbf{0}, I)$ to generate an initial value $\mathbf{z}_{t_{0}}$ of the latent variables using $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$.
$$
\mathbf{z}_{t_{0}} = \exp(0.5 \log\boldsymbol{\sigma}^{2}) \odot \boldsymbol{\epsilon} + \boldsymbol{\mu}
$$
This gives an effect akin to mapping $\operatorname{RNNencoder}$ to latent space $N(\boldsymbol{\mu}, \diag(\boldsymbol{\sigma}^{2}))$ while allowing backpropagation.</li><li>Compute $\mathbf{z}_{t_{i}}$ using the initial value of the latent variable $\mathbf{z}_{t_{0}}$ and Neural ODE.
$$
(\mathbf{z}_{t_{1}}, \mathbf{z}_{t_{2}}, \dots, \mathbf{z}_{t_{N}})
= \operatorname{ODESolve}(\mathbf{z}_{t_{0}}, f, t_{0}, \dots, t_{N}, \theta)
$$</li><li>Obtain predicted time series data $\mathbf{x}_{i}$ by inputting $\mathbf{z}_{t_{i}}$ into a <a href=../3447>fully connected network (FCN)</a> defined as a <a href=../3380>decoder</a>.
$$
\mathbf{x}_{t_{i}} = \operatorname{FCNdecoder}(\mathbf{z}_{t_{i}})
$$</li><li>Maximize ELBO.
$$
\text{ELBO} = \sum\limits_{i} \log p(\mathbf{x}_{t_{i}} | \mathbf{z}_{t_{i}}) + \log p(\mathbf{z}_{t_{0}}) - \log q(\mathbf{z}_{t_{0}} | \left\{ \mathbf{x}_{t_{i}}, t_{i} \right\})
$$</li></ol><h3 id=51-time-series-latent-ode-experiments>5.1 Time-series Latent ODE Experiments</h3><p>The neural network used for the experiment is as follows:</p><ul><li>$\operatorname{RNNencoder}$: RNN with 25 nodes</li><li>The latent space is 4-dimensional. $(\boldsymbol{\mu}), (\boldsymbol{\sigma}) \in \mathbb{R}^{2 \times 2}$</li><li>NeuralODE $f$: MLP with one hidden layer of 20 nodes</li><li>$\operatorname{FCNdecoder}$: Another MLP with one hidden layer of 20 nodes</li></ul><p>The dataset condition is as follows:</p><ul><li>Generate 1,000 samples of 2D spiral data.</li><li>Half are clockwise, the other half counterclockwise.</li><li>Each trajectory begins at a different initial value, sampling 100 points at equal time intervals.</li><li>Add Gaussian noise to observations.</li><li>During training, use 30/50/100 points, randomly drawn without replacement, among the 100 points in each trajectory.</li></ul><p>In the table below, you can observe the significantly superior performance of NeuralODE, and it maintains performance even when data is scarce compared to when there is abundant data.</p><p><img src=3159_7.png alt></p><p>The figures below compare predictions made by RNN and NeuralODE, showing that NeuralODE produces smoother results and makes better predictions in untrained outer regions.</p><p><img src=3159_8.png alt></p><p>The following figure depicts the trajectory of latent variables for the first two dimensions, where the trajectories split into two clusters of clockwise and counterclockwise spirals, indicating that the proposed method describes <a href=../3589>latent variables</a> well.</p><p><img src=3159_9.png alt></p><aside style=text-align:right>2021-12-15&emsp;
전기현&emsp;
<a href=../1230>🎲 3159</a></aside><script>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="Paper Review: Neural Ordinary Differential Equations",c="https://freshrimpsushi.github.io/en/posts/3159/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script></div><aside><h2>Comment</h2><div class=area-reply><div class=list-reply></div></div><div class=write-box><div class="write-author-info box-account"><input type=hidden name=cmt_idx>
<input class=ai-author type=text placeholder=Name>
<input class=ai-password type=password maxlength=20 placeholder=Password></div><div class=write-content><textarea class=ai-content value placeholder='Feel free to ask in english' style=ime-mode:active></textarea></div><button class=write-button onclick=write_comment() aria-label=send><i class='fa-solid fa-paper-plane'></i></button><aside class=tex>$\TeX$ is also applied to comments.</aside></div></aside><script>let commentRows=[];const listReply=document.querySelector(".list-reply");document.addEventListener("DOMContentLoaded",()=>{get_all_comment()});function get_all_comment(){fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=getAllComment";return postFetch(t,{board_idx:"3159",user_ip:e})}).then(e=>e.json()).then(e=>{e.ok&&(commentRows=e.rows,render_comment(commentRows))}).catch(e=>console.error(e))}function render_comment(e){const n=["류대식","전기현","ㅇㅇ","질문"],s=["류대식","전기현"];render_blank();let t="";e.map(e=>{t+=`<div id="comment${e.cmt_idx}" class="parents">`,t+=`<div class="content-info">`;let o="";e.cmt_cnt>125?o="🥇":e.cmt_cnt>25?o="🥈":e.cmt_cnt>5&&(o="🥉"),n.includes(e.author)&&(o="");let i="";e.ip_address==null?i="(-)":(ipParts=e.ip_address.split("."),i=`(${ipParts[0]}.${ipParts[1]})`),s.includes(e.author)&&(i=""),t+=`<div class="list-author">${o} ${e.author} ${i}</div>`,t+=`<sup class="list-date">${e.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,e.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> Waiting for approvement</div>`);let a=e.content;a=a.replace(/\n/g,"<br />"),t+=`<div class="content-text">${a} <span class="re-comment-button" onclick="re_comment('${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`,e.child.map(o=>{let c="child";e.ip_address&&o.ip_address===e.ip_address&&(c="parentself"),t+=`<div id="comment${o.cmt_idx}" class="${c}">`,t+=`<div class="content-info">`;let i="";o.cmt_cnt>5?i="🥉":o.cmt_cnt>25?i="🥈":o.cmt_cnt>125&&(i="🥇"),n.includes(o.author)&&(i="");let a="";o.ip_address==null?a="(-)":a=`(${o.ip_address})`,s.includes(o.author)&&(a=""),t+=`<div class="list-author">${i} ${o.author} ${a}</div>`,t+=`<sup class="list-date">${o.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,o.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> Waiting for approvement</div>`);let r=o.content;r=r.replace(/\n/g,"<br />"),t+=`<div class="content-text">${r} <span class="re-comment-button" onclick="re_comment('${o.cmt_idx}', '${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`})}),listReply.innerHTML=t,renderKaTex(listReply),location.href.indexOf("#")!=-1&&(location.href=location.href.substr(location.href.indexOf("#")))}function render_blank(){listReply.innerHTML=""}function write_comment(){const e=document.querySelector(".ai-author"),t=document.querySelector(".ai-password"),n=document.querySelector(".ai-content"),s=e.value,o=t.value,i=n.value;if(s===""||o===""||i===""){alert("빈칸을 채워주세요");return}const a="3159",r="",c="Paper Review: Neural Ordinary Differential Equations";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeComment";return postFetch(t,{board_idx:a,board_slug:r,board_title:c,author:s.replace(/\+/g,"%2B"),password:o,content:i.replace(/\+/g,"%2B"),user_ip:e})}).then(e=>e.json()).then(s=>{s.ok?(commentRows.push(s.row),render_comment(commentRows),e.value="",t.value="",n.value=""):s.status===606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),e.value="",t.value="",n.value=""):s.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}async function update_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 수정",input:"password",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(s=>{if(s.ok){const s=document.querySelector(`#comment${e}`),o="https://freshrimpsushi.com/blog/ajax/comment.php?action=getComment";postFetch(o,{cmt_idx:e}).then(e=>e.json()).then(o=>{if(o.ok){const a=o.row,r=a.author,c=a.content;let i="";i+=`<div class="update-author-info">`,i+=`<input class="update-author" type="text" value="${r}" placeholder="이름" />`,i+=`<input class="update-password" type="password" value="${n}" placeholder="비밀번호" disabled />`,i+="</div>",i+=`<textarea class="update-content" value="" style="IME-MODE:active;">${c}</textarea>`,i+=`<input class="update_comment-button" type="submit" value="수정" onclick="update_comment_click(${e}, '${t}')" />`,i+=`<input class="update_comment-button" type="submit" value="취소" onclick="cancel_click(${e})" />`,s.innerHTML=i}}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}async function delete_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 삭제",text:"삭제하면 되돌릴 수 없습니다.",input:"password",icon:"warning",showCancelButton:!0,confirmButtonColor:"#3085d6",cancelButtonColor:"#d33",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(n=>{if(n.ok){const n="https://freshrimpsushi.com/blog/ajax/comment.php?action=deleteComment";postFetch(n,{cmt_idx:e}).then(e=>e.json()).then(n=>{Swal.fire("삭제되었습니다."),t==="parents"?commentRows=commentRows.filter(t=>t.cmt_idx!==e):t==="child"&&commentRows.map(t=>{t.child=t.child.filter(t=>t.cmt_idx!==e),t.child_cnt=t.child.length}),render_comment(commentRows)}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}function update_comment_click(e,t){const i=document.querySelector(`#comment${e} .update-author`),a=document.querySelector(`#comment${e} .update-password`),r=document.querySelector(`#comment${e} .update-content`),n=i.value,s=a.value,o=r.value;(n===""||s===""||o==="")&&alert("빈칸을 채워주세요");const c="https://freshrimpsushi.com/blog/ajax/comment.php?action=updateComment";postFetch(c,{cmt_idx:e,author:n.replace(/\+/g,"%2B"),password:s,content:o.replace(/\+/g,"%2B")}).then(e=>e.json()).then(n=>{n.ok&&(Swal.fire("수정되었습니다."),t==="parents"?commentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}):t==="child"&&commentRows.map(t=>{t.cmt_idx==n.parent_cmt_idx&&t.child.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)})}),render_comment(commentRows),recentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}),render_recent_comment(recentRows))}).catch(e=>console.error(e))}function cancel_click(){render_comment(commentRows)}function re_comment(e,t){const s=document.querySelector(`#comment${e} .re-comment`);let n="";n+='<div class="re-comment-box">',n+='<div class="re-comment-author-info">',n+='<input class="re-comment-author" type="text" value="" placeholder="이름" />',n+='<input class="re-comment-password" type="password" value="" placeholder="비밀번호" />',n+="</div>",n+='<textarea class="re-comment-content" placeholder="내용" value="" style="IME-MODE:active;"></textarea>',t===void 0?n+=`<button class="write-button" onclick="re_comment_click(${e})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`:n+=`<button class="write-button" onclick="re_comment_click(${e}, ${t})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`,n+=`<button class="write-button" onclick="cancel_click(${e})" value="close"><i class='fa-solid fa-solid fa-ban'></i></button>`,n+="</div>",s.innerHTML=n}function re_comment_click(e,t){const n=document.querySelector(`#comment${e} .re-comment-author`),s=document.querySelector(`#comment${e} .re-comment-password`),o=document.querySelector(`#comment${e} .re-comment-content`),i=n.value,a=s.value,r=o.value;if(i===""||a===""||r===""){alert("빈칸을 채워주세요");return}const c="3159",l="",d="Paper Review: Neural Ordinary Differential Equations";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(n=>{const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeReComment";return postFetch(s,{board_idx:c,board_slug:l,board_title:d,cmt_idx:t===void 0?e:t,author:i.replace(/\+/g,"%2B"),password:a,content:r.replace(/\+/g,"%2B"),user_ip:n})}).then(e=>e.json()).then(i=>{i.ok?(commentRows.map(n=>{t===void 0?n.cmt_idx==e&&n.child.push(i.row):n.cmt_idx==t&&n.child.push(i.row)}),render_comment(commentRows)):i.status==606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),n.value="",s.value="",o.value=""):i.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}</script></div><aside class=sidebar><aside><style>.reddot a:link{color:#67d10f;text-shadow:0 0 10px #d3d3d3;font-weight:700}.reddot a:visited{color:#f5f5f5}</style><p class=reddot style=text-align:center;margin:0><a href=https://freshrimpsushi.github.io/en/posts/2707/>Summer Special Omakase<br>「Imaginary Numbers」</a></p></aside><br><div class=category></div><div style=display:flex>Click ● to highlight only you interested.<div class=resetmute><button class="sb-btn btnReset" style=border-radius:5px;border:0>reset</button>
<button class="sb-btn btnMute" style=border-radius:5px;border:0>mute</button></div></div><script defer>const color={green:"#33cc33",yellow:"#ffcc00",red:"#ff3333",black:"#000000"},categoryRows=[[{idx:1,name:"함수",color:color.green,show:"Functions",size:"146"},{idx:2,name:"보조정리",color:color.green,show:"Lemmas",size:"55"},{idx:3,name:"미분적분학",color:color.green,show:"Calculus",size:"45"},{idx:4,name:"행렬대수",color:color.green,show:"Matrix Algebra",size:"117"}],[{idx:1,name:"정수론",color:color.green,show:"Number Theory",size:"90"},{idx:2,name:"집합론",color:color.green,show:"Set Theory",size:"49"},{idx:3,name:"그래프이론",color:color.green,show:"Graph Theory",size:"65"},{idx:4,name:"선형대수",color:color.green,show:"Linear Algebra",size:"97"},{idx:5,name:"해석개론",color:color.green,show:"Analysis",size:"84"},{idx:6,name:"추상대수",color:color.green,show:"Abstract Algebra",size:"105"},{idx:7,name:"위상수학",color:color.green,show:"Topology",size:"64"},{idx:8,name:"기하학",color:color.green,show:"Geometry",size:"165"}],[{idx:1,name:"다변수벡터해석",color:color.green,show:"Vector Analysis",size:"37"},{idx:2,name:"복소해석",color:color.green,show:"Complex Anaylsis",size:"71"},{idx:3,name:"측도론",color:color.green,show:"Measure Theory",size:"53"},{idx:4,name:"푸리에해석",color:color.green,show:"Fourier Analysis",size:"54"},{idx:5,name:"초함수론",color:color.green,show:"Distribution Theory",size:"22"},{idx:6,name:"단층촬영",color:color.green,show:"Tomography",size:"20"}],[{idx:1,name:"거리공간",color:color.green,show:"Metric Space",size:"38"},{idx:2,name:"바나흐공간",color:color.green,show:"Banach Space",size:"38"},{idx:3,name:"힐베르트공간",color:color.green,show:"Hilbert Space",size:"31"},{idx:4,name:"르벡공간",color:color.green,show:"Lebesgue Space",size:"33"}],[{idx:1,name:"상미분방정식",color:color.green,show:"ODE",size:"58"},{idx:2,name:"편미분방정식",color:color.green,show:"PDE",size:"60"},{idx:3,name:"확률미분방정식",color:color.green,show:"SDE",size:"26"}],[{idx:1,name:"줄리아",color:color.green,show:"Julia",size:"229"},{idx:2,name:"알고리즘",color:color.green,show:"Algorithm",size:"28"},{idx:3,name:"수치해석",color:color.green,show:"Numerical Analysis",size:"63"},{idx:4,name:"최적화이론",color:color.green,show:"Optimization Theory",size:"37"},{idx:5,name:"머신러닝",color:color.green,show:"Machine Learning",size:"114"},{idx:6,name:"프로그래밍",color:color.yellow,show:"Programming",size:"111"},{idx:7,name:"세이버메트릭스",color:color.green,show:"Sabermetrics",size:"229",size:"13"}],[{idx:1,name:"물리학",color:color.green,show:"Physics",size:"27"},{idx:2,name:"수리물리",color:color.green,show:"Mathematical Physics",size:"77"},{idx:3,name:"고전역학",color:color.green,show:"Classical Mechanics",size:"48"},{idx:4,name:"전자기학",color:color.green,show:"Electrodynamics",size:"51"},{idx:5,name:"양자역학",color:color.green,show:"Quantum Mechanics",size:"57"},{idx:6,name:"열물리학",color:color.green,show:"Thermal Physics",size:"29"}],[{idx:1,name:"R",color:color.green,show:"R",size:"54"},{idx:2,name:"데이터확보",color:color.green,show:"Data Sets",size:"29"},{idx:3,name:"데이터과학",color:color.green,show:"Data Science",size:"41"},{idx:4,name:"통계적검정",color:color.green,show:"Statistical Test",size:"33"},{idx:5,name:"통계적분석",color:color.green,show:"Statistical Analysis",size:"76"},{idx:6,name:"수리통계학",color:color.green,show:"Mathematical Statistics",size:"123"},{idx:7,name:"확률분포론",color:color.green,show:"Probability Distribution",size:"84"},{idx:8,name:"확률론",color:color.green,show:"Probability Theory",size:"80"},{idx:9,name:"위상데이터분석",color:color.green,show:"TDA",size:"40"}],[{idx:1,name:"논문작성",color:color.red,show:"Writing",size:"63"},{idx:2,name:"생새우초밥지",color:color.black,show:"JOF",size:"7"}]];document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("blindList"));(e==""||e==null||e==null||e==0||e==NaN)&&localStorage.setItem("blindList",null),render_category(categoryRows),blind_category(e);const t=document.querySelector(".btnReset");t.addEventListener("click",()=>{let e=new Array;localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)});const n=document.querySelector(".btnMute");n.addEventListener("click",()=>{let e=new Array;for(let t=0;t<categoryRows.length;t++)categoryRows[t].map(n=>{const s={mainIdx:t,subIdx:n.idx};e.push(s)});localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)})});function render_category(e){const n=document.querySelector(".category");let t="";for(let n=0;n<e.length;n++)e[n].map(e=>{t+=`<span id="cate${n}-${e.idx}" class="cate etewsert">`,t+=`<span onclick="check_blind(${n}, ${e.idx});" style="cursor: pointer; color: ${e.color}">●</span>`,t+=`<a href="https://freshrimpsushi.github.io/en/categories/${e.name.toLowerCase()}/">`,t+=` ${e.show} (${e.size})</a>`,t+="</span>",screen.width>954?t+="<br>":t+=" "}),t+="<hr>";n.innerHTML=t}function check_blind(e,t){const o=document.querySelector(`#cate${e}-${t}`);let n=new Array;const i={mainIdx:e,subIdx:t};let s=JSON.parse(localStorage.getItem("blindList"));if(s==""||s==null||s==null||s==0||s==NaN)n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind";else{n=s;let a=null;for(let s=0;s<n.length;s++)if(n[s].mainIdx==e&&n[s].subIdx==t){a=n.filter(n=>n.mainIdx!=e||n.subIdx!=t);break}a===null?(n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind"):(localStorage.setItem("blindList",JSON.stringify(a)),o.className="cate")}}function blind_category(e){const t=document.querySelectorAll(".cate");t.forEach(e=>{e.className="cate"}),e!==null&&e.map(e=>{const t=document.querySelector(`#cate${e.mainIdx}-${e.subIdx}`);t.className+=" blind"})}</script><br><br><b>Viewed posts</b><div class=lately-viewed-list style=padding-left:4px></div><script defer>document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("latelyViewPostList")),n=document.querySelector(".lately-viewed-list");let t="";if(e==""||e==null||e==null||e==0||e==NaN)localStorage.setItem("latelyViewPostList",null),t+='<div class="lv-list">',t+=" · 열람한 포스트가 없습니다.",t+="</div>",n.innerHTML=t;else{for(let n=e.length-1;n>=0;n--)t+='<div class="lv-list">',t+=`<a href="${e[n].link}"> · ${e[n].title}</a>`,t+="</div>";n.innerHTML=t}})</script><script defer>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="Paper Review: Neural Ordinary Differential Equations",c="https://freshrimpsushi.github.io/en/posts/3159/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script><br><b>Recent comment</b><div class=current-reply></div><script defer>const url="https://freshrimpsushi.com/blog/ajax/recent_comment.php?action=getCurrentComment";let recentRows=[];fetch(url).then(e=>e.json()).then(e=>{e.ok&&(recentRows=e.rows,render_recent_comment(recentRows))}).catch(e=>console.error(e));function render_recent_comment(e){const n=document.querySelector(".current-reply");let t="";e.map(e=>{let n="https://freshrimpsushi.github.io/en/";e.board_idx>-1?n+=`posts/${e.board_idx}#comment${e.cmt_idx}`:e.board_idx==-1?n+=`#comment${e.cmt_idx}`:n+=`categories/${e.board_title}#comment${e.cmt_idx}`,t+='<div class="current-reply-list">',t+=`<a href="${n}">`,t+=`<b> - ${e.author}</b>: `,t+=`${e.content}`,t+=`</a>`,t+=`</div>`}),n.innerHTML=t,renderKaTex(n)}</script><br></aside></div><footer><aside><div><p id=mirror-link style=text-align:center><a style=cursor:text href=http://localhost:1313//en/posts/3159/>© FreshrimpRestaurant / Powered by 류대식, 전기현</a><br>Contact:
<img src=https://freshrimpsushi.github.io/en/logo/gmail.png width=12px alt=mail> freshrimpsushi@gmail.com
<a href=https://freshrimpsushi.github.io/en/index.xml><img src=https://freshrimpsushi.github.io/en/logo/RSS.png width=12px alt=RSS> RSS</a></p></div><script type=text/javascript>var goIndex=function(){var e=document.getElementsByName("idx")[0].value,t="https://freshrimpsushi.github.io/en/posts/"+e;location.replace(t)};document.addEventListener("keydown",function(e){const n=document.getElementById("navigator");if(e.altKey&&e.ctrlKey&&e.key==="l"){e.preventDefault();var t=document.querySelector("#mirror-link a");t?t.click():console.log("거울 링크를 찾지 못했습니다.")}})</script></aside></footer></body><script async src="https://www.googletagmanager.com/gtag/js?id=G-NLV8Y9PRK1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NLV8Y9PRK1")</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4751085325232621" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/sweetalert2@11></script><script src=https://freshrimpsushi.github.io/en/js/fontawsome.min.js></script><script src=https://freshrimpsushi.github.io/en/js/common.js></script><script>document.addEventListener("DOMContentLoaded",()=>{fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/ip_checker.php";return postFetch(t,{user_ip:e})}).then(e=>e.json()).then(e=>{e.ok||(alert(`차단된 IP입니다.
Contact:
freshrimpsushi@gmail.com`),window.location.href="https://google.com")}).catch(e=>console.error(e))})</script></html>