<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>R on FreshrimpRestaurant</title><link>https://freshrimpsushi.github.io/en/tags/r/</link><description>Recent content in R on FreshrimpRestaurant</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 28 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/en/tags/r/index.xml" rel="self" type="application/rss+xml"/><item><title>Kurtosis in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1271/</link><pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1271/</guid><description>Kurtosis Given a random variable $X$ with mean $\mu$ and variance $\sigma^2$, the kurtosis of $X$ is defined as follows: $$ \gamma_{2} := {{ E \left( X - \mu \right)^4 } \over { \sigma^4 }} $$ For data $\left\{ X_{i} \right\}_{i}^{n}$, with sample mean $\overline{X}$ and sample variance $\widehat{\sigma}^2$, sample kurtosis $g_{2}$ is obtained as follows: $$ g_{2} := \sum_{i=1}^{n} {{ \left( X - \overline{X} \right)^4 } \over { n</description></item><item><title>Skewness in Mathematical Statistics</title><link>https://freshrimpsushi.github.io/en/posts/1268/</link><pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1268/</guid><description>Definition When the mean of a random variable $X$ is $\mu$, and its variance is $\sigma^2$, the following defined $\gamma_{1}$ is called the Skewness of $X$. $$ \gamma_{1} := {{ E \left( X - \mu \right)^3 } \over { \sigma^3 }} $$ When the sample mean of data $\left\{ X_{i} \right\}_{i}^{n}$ is $\overline{X}$, and the sample variance is $\widehat{\sigma}^2$, the sample skewness $g_{1}$ is calculated as follows. $$ g_{1} :=</description></item><item><title>Analyzing Time Series with Valuation Models in R</title><link>https://freshrimpsushi.github.io/en/posts/1282/</link><pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1282/</guid><description>Practice Value model serves as a useful tool for explaining archetypes, and the analytical procedure itself is similar to that of the ARMA model. The graph above shows the German DAX index from 1991 to 1999, drawn by extracting only DAX from EuStockMarkets. When we look at the square of returns, there seems to be an ARCH effect almost certainly. To check if the squared returns follow the ARMA model,</description></item><item><title>Arch Effect</title><link>https://freshrimpsushi.github.io/en/posts/1278/</link><pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1278/</guid><description>Definition 1 The term ARCH effect refers to the &amp;lsquo;AutoRegressive Conditional Heteroscedasticity,&amp;rsquo; which literally translates to &amp;lsquo;autoregressive conditional heteroscedastic effect.&amp;rsquo; Therefore, it is not neutralized because it is interpreted as such. Description In simpler terms, if the volatility of data changes and can be explained by previous data, it is said that the data exhibits the ARCH effect. The model that statistically explains this ARCH effect is called the ARCH</description></item><item><title>Heteroskedasticity and Volatility Clustering in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1272/</link><pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1272/</guid><description>Definition 1 Given a time series data $\left\{ p_{t} \right\}$. When the variance of $\left\{ p_{t} \right\}$ depends on $t$, $\left\{ p_{t} \right\}$ is said to have Heteroscedasticity. The phenomenon of the variance of $\left\{ p_{t} \right\}$, which has Heteroscedasticity, increasing and decreasing repeatedly is referred to as Volatility Clustering. The following defined $r_{t}$ is referred to as (Log) Return in $t$. $$ r_{t} := \nabla \log p_{t} = \log</description></item><item><title>Time Series Analysis and Innovative Outliers</title><link>https://freshrimpsushi.github.io/en/posts/1260/</link><pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1260/</guid><description>Build-up In the graph above, a significant outlier can be found in September 2001. However, unlike additive outliers, it continues to have an effect thereafter. The number of air passengers was steadily increasing, showing seasonality, but the fear from the 9/11 terrorist attacks can be interpreted as sharply reducing the number of users itself. Definition 1 Outliers that change the landscape of analysis itself are called Innovative Outliers. Practice Such</description></item><item><title>Time Series Analysis of Additive Outliers</title><link>https://freshrimpsushi.github.io/en/posts/1258/</link><pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1258/</guid><description>Buildup The most noticeable point in the graph above is the huge outlier near February 2015. Such extreme values can have a detrimental effect on analysis. Luckily, it was just a brief, momentary outlier. Definition 1 Outliers that do not change the fluctuation of data itself are called additive outliers. Practice Such additive outliers can be intuitively found, or you can use the detectAO() function of the TSA package. The</description></item><item><title>Encyclopedia</title><link>https://freshrimpsushi.github.io/en/posts/1236/</link><pubDate>Sat, 17 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1236/</guid><description>Definition Prewhitening is a method that transforms time series data into white noise when calculating the Cross-Correlation Function (CCF) to more accurately identify the correlation between two datasets. Practical Exercise 1 If possible, it is recommended to fully understand mathematically how this is achievable. As an example, let&amp;rsquo;s look at the following data. bluebird consists of two time series data including the average price and sales volume of potato chips</description></item><item><title>Time Series Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/1223/</link><pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1223/</guid><description>Definition Time series regression analysis refers precisely to the technique of performing regression analysis using time series data. It&amp;rsquo;s true that regression analysis is not inherently well-suited to handling time series data, yet, when dealing with multiple series of time-series data, it can be beneficial to borrow the ideas and tools of regression analysis. Practice Suppose we are given two types of data, x and y, as shown above. Of</description></item><item><title>Residual Analysis of ARIMA Models</title><link>https://freshrimpsushi.github.io/en/posts/1218/</link><pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1218/</guid><description>Explanation Like regression analysis, time series analysis also involves residual analysis. According to the assumptions of the ARIMA model, residuals are all white noise, thus they should follow linearity, homoscedasticity, independence, and normality. Compared to regression analysis, it&amp;rsquo;s generally not as strict, but independence is rigorously checked. After all, the purpose of time series analysis is to understand autocorrelation; if residuals still lack independence, it means the analysis is incomplete.</description></item><item><title>Selecting an ARMA Model Using EACF in R</title><link>https://freshrimpsushi.github.io/en/posts/1216/</link><pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1216/</guid><description>Practice 1 PACF is very helpful in determining the order of $AR(p)$, while ACF aids in determining the order of $MA(q)$. Let&amp;rsquo;s directly observe an example. ma1.2.s data comes from a $MA(1)$ model, and ar1.s data comes from a $AR(1)$ model, both from the TSA package. By using the acf() and pacf() functions from the TSA package, it generates a Correlogram for various lags $k$ as follows. Merely looking at</description></item><item><title>Autocorrelation Function</title><link>https://freshrimpsushi.github.io/en/posts/1209/</link><pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1209/</guid><description>Definition 1 Let&amp;rsquo;s say $\left\{ Y_{t} \right\}_{t=1}^{n}$ is a stochastic process. $\mu_{t} := E ( Y_{t} )$ is called the mean function. The following defined $\gamma_{ t , s }$ is called the autocovariance function. $$ \gamma_{t , s} : = \text{cov} ( Y_{t} , Y_{s} ) = E ( Y_{t} - \mu_{t} ) E ( Y_{s} - \mu_{s} ) $$ The following defined $\rho_{ t , s }$ is</description></item><item><title>Predicting with ARIMA Models in R</title><link>https://freshrimpsushi.github.io/en/posts/1205/</link><pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1205/</guid><description>Practice The R built-in data UKDriverDeaths contains monthly data on road casualties in the UK from 1969 to 1984. It obviously follows a seasonal ARIMA model, and finding the actual model is not very difficult. However, performing calculations directly using the formula from the final model is quite laborious and complex. Therefore, we use the predict() function. You can set how far into the future you would like to predict</description></item><item><title>How to View Time Series Analysis Results Obtained with ARIMA Model in R</title><link>https://freshrimpsushi.github.io/en/posts/1200/</link><pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1200/</guid><description>Practice The built-in R dataset AirPassenger consists of monthly airline passenger numbers from 1949 to 1960. (1) Model: In fact, if the coefficients can be precisely identified, that&amp;rsquo;s not the most critical aspect. Represents the Seasonal ARIMA model $ARIMA(p,d,q)\times(P,D,Q)_{s}$. For instance, the result of the above analysis ARIMA(0,1,1)(0,1,1)[12] means $ARIMA(0,1,1)\times(0,1,1)_{12}$. (2) Coefficients: Represents the coefficients that fit the model. ma1 is the moving average process coefficient $\theta_{1}$, and sma1 is</description></item><item><title>How to Analyze Time Series with ARIMA Models in R</title><link>https://freshrimpsushi.github.io/en/posts/1197/</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1197/</guid><description>Practice Let&amp;rsquo;s load the built-in data WWWusage in R and draw a graph to check it. WWWusage represents a time series data indicating the number of internet users a long time ago. To understand its trend, time series analysis is necessary. Among the time series analysis models, the most representative one is the ARIMA model. However, even within the ARIMA models, there are various methods to find the appropriate model.</description></item><item><title>Drift in the ARIMA Model</title><link>https://freshrimpsushi.github.io/en/posts/1115/</link><pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/1115/</guid><description>Explanation When analyzing time series, one often comes across a coefficient called Drift. Of course, in the case above, the coefficient is too small compared to the standard error to matter. However, if you actually have a significant coefficient and need to write it down in a formula, it&amp;rsquo;s necessary to understand what a drift is. Unfortunately, there are no good explanations about what drift actually is domestically, and without</description></item><item><title>Transformation in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/938/</link><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/938/</guid><description>Buildup The reason why transformations are necessary in time series is to give a &amp;ldquo;penalty&amp;rdquo; for increasing variance over time, to keep the variance constant, and to achieve stationarity. The square root $\sqrt{}$ and log $\log$ are often used because the amount reduced is greater for larger values. Of course, when variance decreases, it means that the trend of data converges to some point, thus no time series analysis is</description></item><item><title>Differencing in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/916/</link><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/916/</guid><description>Definition 1 Define operator $B$ as $B Y_{t} = Y_{t-1}$, referred to as Backshift. Define operator $\nabla$ as $\nabla := 1 - B$ and $\nabla^{r+1} = \nabla \left( \nabla^{r} Y_{t} \right)$, referred to as Differencing. Explanation According to the definition of differencing, the $1$th difference is calculated as $$ \nabla Y_{t} = Y_{t} - Y_{t-1} $$, and the $2$th difference is calculated as $$ \begin{align*} \nabla^2 Y_{t} =&amp;amp; \nabla \left(</description></item><item><title>Stability in Time Series Analysis</title><link>https://freshrimpsushi.github.io/en/posts/907/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/907/</guid><description>Definition 1 Time series data is said to have stationarity when its mean and variance are constant over time. Description It&amp;rsquo;s not normal正常 as in standard, but stationarity定常. The fact that data is stationary means that its mean and variance are stabilized, making it easier to analyze. If the</description></item><item><title>Comparing Models Using the AUC of ROC Curves</title><link>https://freshrimpsushi.github.io/en/posts/887/</link><pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/887/</guid><description>Theorem ROC curves are essentially better as they fill up the rectangle $[0,1]^2$ more, and it is preferable for the curve&amp;rsquo;s turning point in the upper left to be closer to $(0,1)$. Description Given the two ROC curves above, the right side can comfortably be considered &amp;lsquo;better&amp;rsquo;. This &amp;lsquo;better&amp;rsquo; refers to the model generating the ROC curve being superior. Naturally, the two models being compared are derived from the same</description></item><item><title>Finding the optimal cutoff using ROC curves</title><link>https://freshrimpsushi.github.io/en/posts/877/</link><pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/877/</guid><description>Overview Drawing an ROC curve is useful because it gives a quick visual insight into how well the model developed from training data explains the test data. However, since this curve calculates and connects classification rates for all cutoffs, it ultimately does not provide information on &amp;lsquo;what cutoff to use to classify 0 and 1&amp;rsquo;. To find this out, let’s apply the methodology of cross-validation. Validation</description></item><item><title>Drawing ROC Curves in R</title><link>https://freshrimpsushi.github.io/en/posts/868/</link><pubDate>Wed, 20 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/868/</guid><description>Definition The plot with the False Positive Rate and True Positive Rate of an error matrix as its axes is called the ROC curveReceiver Operating Characteristic Curve. Explanation An ROC curve not only provides a quick look at the performance of a model but is also useful in finding the optimal cutoff, comparing models, and more. Let&amp;rsquo;s draw an ROC curve in R and understand its meaning through an example.</description></item><item><title>What is a Stochastic Process?</title><link>https://freshrimpsushi.github.io/en/posts/857/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/857/</guid><description>Definition The range of the random variable $X: \Omega \to E$ is called the state space. The set of random variables $\left\{ X_{t} \mid t \in [ 0 , \infty ) \right\}$ is called a continuous stochastic process. The sequence of random variables $\left\{ X_{n} \mid n = 0, 1, 2, \cdots \right\}$ is called a discrete stochastic process. Explanation The term &amp;lsquo;process&amp;rsquo; in stochastic process often makes the concept</description></item><item><title>Variable Selection Procedures in Statistical Analysis</title><link>https://freshrimpsushi.github.io/en/posts/821/</link><pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/821/</guid><description>Buildup Let&amp;rsquo;s consider doing a multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$. Here, if we have $p$ independent variables, it would be ideal if they satisfy the various assumptions of regression analysis well, there is no multicollinearity, and the explanatory power is high. Of course, the more information, the better, but a regression model obtained from too much data also requires a lot of data to use. Therefore,</description></item><item><title>How to Perform Principal Component Regression in R</title><link>https://freshrimpsushi.github.io/en/posts/814/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/814/</guid><description>Overview Principal Components Regression (PCR) combines Principal Component Analysis and Multiple Regression Analysis. It involves using the principal components derived from PCA as new independent variables for regression analysis. From a statistical perspective, PCA itself might not be necessary, and its relevance usually comes into play for regression analysis. Practice (Following the method to detect multicollinearity) Although generating principal components involves complex computations including matrix decomposition, in R, this can</description></item><item><title>Variance Inflation Factor VIF</title><link>https://freshrimpsushi.github.io/en/posts/810/</link><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/810/</guid><description>Definition 1 When performing multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$, let&amp;rsquo;s define the multiple regression coefficient for the $i$th independent variable as $R_{i}^2$. The following is called the Variance Inflation Factor for $X_{i}$. $$\displaystyle \text{VIF}_{i}: = {{1} \over {1 - R_{i}^{2} }}$$ Explanation First, it is recommended to read about multicollinearity. VIF is sometimes translated as the variance expansion index, but it is usually too long, so</description></item><item><title>Multicollinearity</title><link>https://freshrimpsushi.github.io/en/posts/808/</link><pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/808/</guid><description>Definition 1 Consider performing multiple regression analysis $Y \gets X_{1} , \cdots, X_{p}$. If among the independent variables $ X_{1} , \cdots, X_{p}$ there is a strong correlation between the independent variables, then it is said that there is multicollinearity. Practice Initially, the very idea that independent variables are dependent violates the assumptions of regression analysis and indeed leads to numerical problems that make the analysis results unreliable. It can</description></item><item><title>Nonlinear Regression Analysis: Variable Transformation in Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/805/</link><pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/805/</guid><description>Overview 1 Regression analysis is essentially a method to elucidate the linear relationships among variables; however, if necessary, the data can be &amp;lsquo;flattened&amp;rsquo; to analyze them linearly. This inherently involves explaining the dependent variable through a nonlinear combination of independent variables. Practice Let&amp;rsquo;s load the built-in data Pressure. Statistical analysis of the Pressure data is in fact unnecessary. This is merely a natural phenomenon which requires mathematical proof just as</description></item><item><title>Influence of Interaction in Regression Analysis</title><link>https://freshrimpsushi.github.io/en/posts/696/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/696/</guid><description>Buildup It is recommended to read about regression analysis including qualitative variables first. Imagine guessing this year&amp;rsquo;s graduates&amp;rsquo; starting salaries based on their college entrance exam scores $X_{1}$, age $X_{2}$, gender $S$, and final educational attainment $E$. Firstly, with the presence of qualitative variables, gender is defined as $$ S = \begin{cases} 1 &amp;amp; ,\text{여성} \\ 0 &amp;amp; ,\t</description></item><item><title>Regression Analysis Including Qualitative Variables</title><link>https://freshrimpsushi.github.io/en/posts/686/</link><pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/686/</guid><description>Overview Regression analysis does not always guarantee that quantitative variables are used as independent variables. There is also a need to reflect categorical data in the analysis, such as what gender someone is, which company they belong to, what color something is, whether it&amp;rsquo;s a metal, etc. Build-up 1 Imagine guessing the starting salary $Y$ with the nationwide exam score $X_{1}$, age $X_{2}$, gender $S$, and the highest education level</description></item><item><title>How to Interpret Multiple Regression Analysis Results in R</title><link>https://freshrimpsushi.github.io/en/posts/670/</link><pubDate>Wed, 05 Sep 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/670/</guid><description>Data Exploration tail(attitude) In R, let&amp;rsquo;s load the built-in data attitude and check it using the tail() function. We are interested in performing multiple regression analysis on this data. We are interested in how the other independent variables affect the rating, which is our dependent variable. It&amp;rsquo;s difficult to see if there is a linear relationship between rating and the other variables just by looking at the data, so let&amp;rsquo;s</description></item><item><title>How to View Simple Regression Analysis Results in R</title><link>https://freshrimpsushi.github.io/en/posts/652/</link><pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/en/posts/652/</guid><description>Practice How to Do Regression Analysis head(faithful) In R, load the built-in data faithful using head() function and check. It&amp;rsquo;s difficult to confirm if there&amp;rsquo;s a linear relationship between the two variables just by looking at the data, so let&amp;rsquo;s plot it to see. win.graph(6,3) par(mfrow=c(1,2)) plot(faithful, main =&amp;#34;faithful&amp;#34;,asp=T) plot(faithful, main =&amp;#34;faithful&amp;#34;) points(head(faithful),col=&amp;#39;red&amp;#39;,pch=19) The one on the left is with the aspect ratio kept constant, which is accurate but hard</description></item></channel></rss>