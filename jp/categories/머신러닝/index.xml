<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>머신러닝 on 生エビ寿司屋</title>
    <link>https://freshrimpsushi.github.io/jp/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/</link>
    <description>Recent content in 머신러닝 on 生エビ寿司屋</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>jp</language>
    <lastBuildDate>Fri, 29 Dec 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://freshrimpsushi.github.io/jp/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>適応的な学習率: AdaGrad, RMSProp, Adam</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3529/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3529/</guid>
      <description>概要1 2 勾配降下法で使用されるアダプティブラーニングレートと、これを適用したモデルであるAdaGrad、RMSProp、Adamについて説明</description>
    </item>
    <item>
      <title>勾配降下における運動量法</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3528/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3528/</guid>
      <description>概要1 2 勾配降下法におけるモーメンタム技術は、パラメーターを更新する際に以前の勾配もすべて使用することである。これが本質であり、これに尽きる</description>
    </item>
    <item>
      <title>モンテカルロ積分</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3515/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3515/</guid>
      <description>概要 モンテカルロ積分は、与えられた関数の積分を計算するのが困難な場合に使用される数値的近似方法の一つである。次のような状況を想定しよう。与え</description>
    </item>
    <item>
      <title>Flux-PyTorch-TensorFlowチートシート</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3489/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3489/</guid>
      <description>概要 Flux、PyTorch、TensorFlowで同じ機能をするコードを整理します。 Julia-MATLAB-Python-R チートシート Fluxについて次のような環境とします。</description>
    </item>
    <item>
      <title>MNIST Database</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3444/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3444/</guid>
      <description>概要1 $$ \includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$ MNISTmodified national institute of standards and technology データベースとは、アメリカの高校生と人口調査局の職員の数字の手書き文字に関するデータセットを指す。一般に[エムニス</description>
    </item>
    <item>
      <title>表現者の定理の証明</title>
      <link>https://freshrimpsushi.github.io/jp/posts/2408/</link>
      <pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/2408/</guid>
      <description>定理 インプット集合Input Set $X \ne \emptyset$ と正定値カーネル $k: X \times X \to \mathbb{R}$ が与えられているとする。学習データセットTraining Datasetを $$ D</description>
    </item>
    <item>
      <title>機械学習における政府号カーネルと再生カーネルのヒルベルト空間</title>
      <link>https://freshrimpsushi.github.io/jp/posts/2406/</link>
      <pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/2406/</guid>
      <description>定義 1 2 入力空間Input Space $X \ne \emptyset$ が定義域であり値域が複素数の集合 $\mathbb{C}$ の写像 $f: X \to \mathbb{C}$ で構成される関数空間 $\left( H , \left&amp;lt; \cdot , \cdot \right&amp;gt; \right) \subset \mathbb{C}^{X}$ がヒルベルト空間</description>
    </item>
    <item>
      <title>サポートベクターマシン</title>
      <link>https://freshrimpsushi.github.io/jp/posts/2402/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/2402/</guid>
      <description>モデル 1 簡単な定義 二値分類Binary Classificationが可能なデータを最もよく区別する直線や平面を見つける方法をサポートベクター</description>
    </item>
    <item>
      <title>論文レビュー: 物理情報基盤ニューラルネットワーク(PINN)</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3313/</link>
      <pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3313/</guid>
      <description>概要 レファレンスと数式の番号や表記法は、論文をそのまま踏襲する。 Physics-informed neural networks (PINN[ピン]と読む)は、数値的に解くために設計された微分方程式の</description>
    </item>
    <item>
      <title>逆伝播アルゴリズム</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3077/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3077/</guid>
      <description>この文は逆転派アルゴリズムの原理を数学専攻者が理解しやすいように作成された。 表記法 上図のような 人工ニューラルネットワーク が与えられたとする。</description>
    </item>
    <item>
      <title>機械学習における強化学習とは？</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3029/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3029/</guid>
      <description>定義 強化学習とは、エージェントが環境と相互作用して累積報酬を最大化するポリシーを見つけることができるようにすることである。 説明1 強化学習を構</description>
    </item>
    <item>
      <title>マシンラーニングにおけるオーバーフィッティングと正則化とは？</title>
      <link>https://freshrimpsushi.github.io/jp/posts/1807/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/1807/</guid>
      <description>オーバーフィッティング トレーニングロスは減っていくが、テストロス（バリデーションロス）が減らない、あるいは増えてしまう現象をオーバーフィッテ</description>
    </item>
    <item>
      <title>ディープラーニングにおける活性化関数</title>
      <link>https://freshrimpsushi.github.io/jp/posts/991/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/991/</guid>
      <description>定義 実際の生物の閾値を模倣した非線形関数を活性化関数activation functionと言う。 数学的定義 ディープラーニングでは非線形スカラ</description>
    </item>
    <item>
      <title>機械学習における勾配降下法と確率的勾配降下法</title>
      <link>https://freshrimpsushi.github.io/jp/posts/987/</link>
      <pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/987/</guid>
      <description>概要 損失関数の勾配を利用して損失関数の極小値を見つけるアルゴリズムの中でもっとも単純な方法として 勾配降下法Gradient Descent Algorith</description>
    </item>
    <item>
      <title>機械学習における損失関数</title>
      <link>https://freshrimpsushi.github.io/jp/posts/967/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/967/</guid>
      <description>定義 データ$Y = \begin{bmatrix} y_{1} \\ \vdots \\ y_{n} \end{bmatrix}$の推定値が$\widehat{Y} = \begin{bmatrix} \widehat{ y_{1} } \\ \vdots \\ \widehat{y_{n}} \end{bmatrix}$とし</description>
    </item>
    <item>
      <title>パイトーチでtorch.nnとtorch.nn.functionalの違い</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3626/</link>
      <pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3626/</guid>
      <description>説明 PyTorchには、多くのニューラルネットワーク関連の関数がtorch.nnとtorch.nn.functionalに同じ名前で含まれて</description>
    </item>
    <item>
      <title>파이토치에서 AdaBelief 옵티마이저 사용하는 방법</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3620/</link>
      <pubDate>Sat, 28 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3620/</guid>
      <description>説明 AdaBeliefは2020年にJ. Zhuangらによって紹介されたオプティマイザで、Adamの変形の一つです1。PyTorchではこの</description>
    </item>
    <item>
      <title>TensorFlow-Kerasでシーケンスモデル、関数型APIでMLPを定義してトレーニングする方法</title>
      <link>https://freshrimpsushi.github.io/jp/posts/3562/</link>
      <pubDate>Tue, 04 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://freshrimpsushi.github.io/jp/posts/3562/</guid>
      <description>概要 TensorFlowでは、Kerasを使用して簡単にニューラルネットワークを定義することができます。以下では、Sequential()と</description>
    </item>
  </channel>
</rss>
