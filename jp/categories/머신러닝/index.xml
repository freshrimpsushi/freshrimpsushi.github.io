<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>機械学習 on 生エビ寿司屋</title><link>https://freshrimpsushi.github.io/jp/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/</link><description>Recent content in 機械学習 on 生エビ寿司屋</description><generator>Hugo -- gohugo.io</generator><language>jp</language><lastBuildDate>Mon, 21 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://freshrimpsushi.github.io/jp/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/index.xml" rel="self" type="application/rss+xml"/><item><title>論文レビュー: スコアマッチング</title><link>https://freshrimpsushi.github.io/jp/posts/3676/</link><pubDate>Mon, 21 Jul 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3676/</guid><description>概要 スコアマッチングscore matchingは2005年に発表されたAapo Hyvarinenの論文 [Estimation of Non-Normalized Statistical Models by Score Matching](ht</description></item><item><title>エネルギーに基づくモデル</title><link>https://freshrimpsushi.github.io/jp/posts/3664/</link><pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3664/</guid><description>概要1 2 3 エネルギーベースモデルとは、データのエネルギーenergyという関数を定義し、エネルギーが低いデータほどもっともらしいデータ（=確</description></item><item><title>機械学習における線形回帰モデルの最尤推定推定</title><link>https://freshrimpsushi.github.io/jp/posts/3643/</link><pubDate>Wed, 14 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3643/</guid><description>定理 データ$\mathbf{x}_{i} \in \mathbb{R}^{n}$とそのラベル$y_{i} \in \mathbb{R}$間の関係が、次のよう</description></item><item><title>機械学習におけるベイズ推論</title><link>https://freshrimpsushi.github.io/jp/posts/3642/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3642/</guid><description>概要 ベイズ推論Bayesian inferenceとは、ベイズの定理に基づいて事前の知識と観測されたデータを通じて母数の分布を推定する統計的方</description></item><item><title>機械学習における線形回帰モデルの最大事後確率推定</title><link>https://freshrimpsushi.github.io/jp/posts/3641/</link><pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3641/</guid><description>定理 データ$\mathbf{x}_{i} \in \mathbb{R}^{n}$とそのラベル$y_{i} \in \mathbb{R}$の関係が以下の線形モ</description></item><item><title>論文レビュー：デノイジング拡散確率モデル (DDPM)</title><link>https://freshrimpsushi.github.io/jp/posts/3638/</link><pubDate>Sun, 04 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3638/</guid><description>概要と要約 生成モデルとは、与えられたランダムサンプル $\left\{ y_{i} \right\}$が従う確率分布 $Y$またはそれを見つける方法を指す。何もないところ</description></item><item><title>生成モデル</title><link>https://freshrimpsushi.github.io/jp/posts/3637/</link><pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3637/</guid><description>概要 我々が持っているデータが従う確率分布を正確に把握することは、多くの応用分野で重要な問題であるが、非常に難しい問題でもある。例えば、人の顔</description></item><item><title>最適化器</title><link>https://freshrimpsushi.github.io/jp/posts/1019/</link><pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1019/</guid><description>定義 最適化問題とは、関数$f : \mathbb{R}^{n} \to \mathbb{R}$の関数値が最小になるような$x_{\ast}$を見つけることを指す。 $$ x_{\ast} = \argmin\limits_{x} f(x) $$ 最適</description></item><item><title>機械学習におけるSiLUまたはSwish関数</title><link>https://freshrimpsushi.github.io/jp/posts/883/</link><pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/883/</guid><description>定義 1 2 シルーSiLU, Sigmoid-weighted Linear Unit または スウィッシュSwish 関数は次のように定義される。 $$ \operatorname{SiLU}(x) = x \cdot \sigma(x) $$ ここで $\sigma$ はシグモイド関数のうち特にロジス</description></item><item><title>オートエンコーダー</title><link>https://freshrimpsushi.github.io/jp/posts/1181/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1181/</guid><description>定義 2つの自然数 $m \ll n$に対して、関数 $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$を符号器encoderとする。関数 $g : \mathbb{R}^{m} \to \mathbb{R}^</description></item><item><title>画像（信号、データ）におけるノイズとアーティファクトの違い</title><link>https://freshrimpsushi.github.io/jp/posts/1192/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1192/</guid><description>概要 ノイズとアーティファクトは共通して元の信号（データ）を損なう要素であり、除去すべき対象である。本書ではこの二つの特徴と、互いにどのような</description></item><item><title>インターネットで無料公開された人工知能、機械学習、ディープラーニングの教材</title><link>https://freshrimpsushi.github.io/jp/posts/1253/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1253/</guid><description>説明 人工知能、機械学習、ディープラーニングの分野の教材は、インターネットで無料で公開されている場合が多い。特に有名な教材でも無料公開されてい</description></item><item><title>Juliaで双対数を使用して自動微分フォワードモードを実装する</title><link>https://freshrimpsushi.github.io/jp/posts/1498/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1498/</guid><description>概要 自動微分の前進モードは二元数を利用すれば簡単に実装できる。ジュリアで前進モードを実装する方法を説明する。二元数と自動微分に関する背景知識</description></item><item><title>自動微分と二重数</title><link>https://freshrimpsushi.github.io/jp/posts/1501/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1501/</guid><description>概要 二元数とは、二つの実数 $a, b \in \mathbb{R}$ に対して次のような形で表現される数を指す。 $$ a + b\epsilon, \quad (\epsilon^{2} = 0,\ \epsilon \neq 0) $$ 二元数の加算と乗算の体系は、自動微分の前</description></item><item><title>Juliaフラックスで関数型APIを使用してニューラルネットワークを定義する方法</title><link>https://freshrimpsushi.github.io/jp/posts/1539/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1539/</guid><description>説明 簡単な構造のニューラルネットワークは Flux.Chain を使用して定義できるが、複雑な構造のニューラルネットワークは Chain で定義するのが難しい。この場合、@f</description></item><item><title>ソルト・アンド・ペッパーノイズ</title><link>https://freshrimpsushi.github.io/jp/posts/76/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/76/</guid><description>定義 画像に小さな点として白や黒で表されるノイズをソルト・アンド・ペッパーノイズsalt-and pepper noiseという。 例示 例として、上の画像にソ</description></item><item><title>論文レビュー：コルモゴロフ・アーノルドニューラルネットワーク（KAN）</title><link>https://freshrimpsushi.github.io/jp/posts/322/</link><pubDate>Sun, 29 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/322/</guid><description>概要と要約 Kolmogorov–Arnold Networks（KAN）は、その名の通りコルモゴロフ-アルノルト表現定理Kolmogorov–</description></item><item><title>DeepONet論文の実装を無計画に追いかける (PyTorch)</title><link>https://freshrimpsushi.github.io/jp/posts/1153/</link><pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1153/</guid><description>概要 DeepONetは非線形演算子を学習するためのニューラルネットワーク構造として論文が公開された後、偏微分方程式の解法など多くの分野で応用</description></item><item><title>PyTorch で自動微分を使う方法</title><link>https://freshrimpsushi.github.io/jp/posts/1966/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1966/</guid><description>説明 パイトーチで自動微分する方法を紹介する。パイトーチでの自動微分はtorch.autograd.grad関数で実装されている。 torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True,</description></item><item><title>파이토치로 PINN 논문 구현하기</title><link>https://freshrimpsushi.github.io/jp/posts/1967/</link><pubDate>Mon, 16 Sep 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1967/</guid><description>説明 PINNはPhysics-Informed Neural Networksの略称であり、自動微分と人工ニューラルネットワークを用いて微分方程式の解を数</description></item><item><title>Julia Fluxでニューラルネットワークトレーニングモード、テストモードの設定方法</title><link>https://freshrimpsushi.github.io/jp/posts/1975/</link><pubDate>Sat, 31 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1975/</guid><description>説明 ニューラルネットワークの構造では、トレーニングプロセスとテストプロセスで異なる動作が必要な部分があります。例えば、ドロップアウトはトレー</description></item><item><title>Julia・フラックスでGPUを使用する方法</title><link>https://freshrimpsushi.github.io/jp/posts/2611/</link><pubDate>Mon, 12 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/2611/</guid><description>概要 Juliaの機械学習ライブラリーであるFlux.jl1を使ってディープラーニングを実装する方法と、GPUを使って学習のパフォーマンスを加</description></item><item><title>論文レビュー: DeepONet</title><link>https://freshrimpsushi.github.io/jp/posts/1180/</link><pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1180/</guid><description>概要及び要約 リファレンス、数式の番号、表記法などはできるだけ論文に従う。 アクセシビリティのためにジャーナルに掲載されたバージョンではなく、ア</description></item><item><title>大学院生の降下法</title><link>https://freshrimpsushi.github.io/jp/posts/2598/</link><pubDate>Wed, 17 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/2598/</guid><description>ビルドアップ 冷蔵庫-象問題 伝統的に、象を冷蔵庫に入れる方法は大学院生に依存してきた。どれほど難しいか、大変か、どんな方法が良いかはよくわから</description></item><item><title>グリッドサーチ、ブルートフォース、肉体労働</title><link>https://freshrimpsushi.github.io/jp/posts/2596/</link><pubDate>Sat, 13 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/2596/</guid><description>用語 グリッドサーチ 主に最適化問題で、ユークリッド空間 $\mathbb{R}^{n}$ をグリッドgridに分割し、可能な限り多くの点で試行を繰り返し最適解を見つける方法をグ</description></item><item><title>PyTorchでtorch.nnとtorch.nn.functionalの違い</title><link>https://freshrimpsushi.github.io/jp/posts/3626/</link><pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3626/</guid><description>説明 PyTorchには、多くのニューラルネットワーク関連の関数がtorch.nnとtorch.nn.functionalに同じ名前で含まれて</description></item><item><title>機械学習における重みとは？</title><link>https://freshrimpsushi.github.io/jp/posts/3625/</link><pubDate>Mon, 08 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3625/</guid><description>定義 機械学習では、最適化しなければならないパラメーターを重みweightと呼ぶ。</description></item><item><title>人工ニューラルネットワークにおけるスキップ接続とは？</title><link>https://freshrimpsushi.github.io/jp/posts/3624/</link><pubDate>Sat, 06 Jul 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3624/</guid><description>定義 $\mathbf{W}$を重みとし、$\mathbf{x}$を入力、$\sigma$を非線形の活性化関数としよう。レイヤー $L_{\ma</description></item><item><title>PyTorchでAdaBeliefオプティマイザを使う</title><link>https://freshrimpsushi.github.io/jp/posts/3620/</link><pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3620/</guid><description>説明 AdaBeliefは2020年にJ. Zhuangらによって紹介されたオプティマイザで、Adamの変形の一つです1。PyTorchではこの</description></item><item><title>潜在変数と潜在空間</title><link>https://freshrimpsushi.github.io/jp/posts/3589/</link><pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3589/</guid><description>定義 データ集合$X \subset \mathbb{R}^{n}$が与えられているとする。このデータ集合を定義域とする関数をエンコーダーという。 $$ \begin{align*} f : X &amp;amp;\to</description></item><item><title>TensorFlow-Kerasでシーケンスモデル、関数型APIでMLPを定義してトレーニングする方法</title><link>https://freshrimpsushi.github.io/jp/posts/3562/</link><pubDate>Mon, 04 Mar 2024 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3562/</guid><description>概要 TensorFlowでは、Kerasを使用して簡単にニューラルネットワークを定義することができます。以下では、Sequential()と</description></item><item><title>適応的な学習率: AdaGrad, RMSProp, Adam</title><link>https://freshrimpsushi.github.io/jp/posts/3529/</link><pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3529/</guid><description>概要1 2 勾配降下法で使用されるアダプティブラーニングレートと、これを適用したモデルであるAdaGrad、RMSProp、Adamについて説明</description></item><item><title>勾配降下における運動量法</title><link>https://freshrimpsushi.github.io/jp/posts/3528/</link><pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3528/</guid><description>概要1 2 勾配降下法におけるモーメンタム技術は、パラメーターを更新する際に以前の勾配もすべて使用することである。これが本質であり、これに尽きる</description></item><item><title>PyTorchでモジュラー演算</title><link>https://freshrimpsushi.github.io/jp/posts/3527/</link><pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3527/</guid><description>説明 モジュラー演算とは、剰余演算とも呼ばれ、$a$を$b$で割った時の余りを返す関数のことを言う。PyTorchには二つの関数がある。 torch.remainder(a,b) torch.fmod(a,b) ど</description></item><item><title>マシンラーニングにおけるオンライン学習とバッチ学習とは？</title><link>https://freshrimpsushi.github.io/jp/posts/3526/</link><pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3526/</guid><description>概要 オンライン学習、バッチ学習、ミニバッチ学習について説明する。これらの名前や違いが実際に重要なわけではなく、大したことではないと言いたい。</description></item><item><title>データ拡張とは何か？</title><link>https://freshrimpsushi.github.io/jp/posts/3522/</link><pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3522/</guid><description>定義 データセット $X = \left\{ x \in \mathbb{R}^{n} \right\}$が与えられたとしよう。適当な変換 $f_{i} : \mathbb{R}^{n} \to \mathbb{R}^{n}$を使って $X$から $X^</description></item><item><title>モンテカルロ法</title><link>https://freshrimpsushi.github.io/jp/posts/3521/</link><pubDate>Wed, 13 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3521/</guid><description>定義 簡単に言うと、モンテカルロ法Monte-Carlo methodは、ランダムにたくさん試すことだ。 説明 シンプルな方法だが、たくさん試すこと</description></item><item><title>棄却サンプリング</title><link>https://freshrimpsushi.github.io/jp/posts/3518/</link><pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3518/</guid><description>概要 1 棄却サンプリングは、与えられた分布 $p$からサンプリングするのが難しいときに、サンプリングしやすい提案分布 $q$を利用して $p$に従う</description></item><item><title>중요도 샘플링</title><link>https://freshrimpsushi.github.io/jp/posts/3516/</link><pubDate>Sun, 03 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3516/</guid><description>概要1 重要度サンプリングはモンテカルロ法の一つで、有限和で積分（期待値）を近似するときに使える一種のサンプリングトリックだ。 ビルドアップ 標準</description></item><item><title>モンテカルロ積分</title><link>https://freshrimpsushi.github.io/jp/posts/3515/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3515/</guid><description>概要 モンテカルロ積分は、与えられた関数の積分を計算するのが困難な場合に使用される数値的近似方法の一つである。次のような状況を想定しよう。与え</description></item><item><title>PyTorchでモデルを保存する際の「RuntimeError: Parent directory does not exists」エラーの解決法</title><link>https://freshrimpsushi.github.io/jp/posts/3502/</link><pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3502/</guid><description>エラー PyTorchでモデルや重みを保存する時、確かに存在するパスであるにも関わらず、次のようなエラーに遭遇することがある。 &amp;gt;&amp;gt;&amp;gt; print(&amp;#34;Is exists path?: &amp;#34;, os.path.exists(directory)) Is exists</description></item><item><title>Flux-PyTorch-TensorFlowチートシート</title><link>https://freshrimpsushi.github.io/jp/posts/3489/</link><pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3489/</guid><description>概要 Flux、PyTorch、TensorFlowで同じ機能をするコードを整理します。 Julia-MATLAB-Python-R チートシート Fluxについて次のような環境とします。</description></item><item><title>PyTorchでのテンソルソートに関する関数</title><link>https://freshrimpsushi.github.io/jp/posts/3487/</link><pubDate>Fri, 06 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3487/</guid><description>torch.sort() torch.sort()にテンソルを入力すると、ソートされた値とインデックスが返される。 1次元テンソル &amp;gt;&amp;gt;&amp;gt; x = torch.tensor([1, 3, -2, 5, -1, 7, 0]) &amp;gt;&amp;gt;&amp;gt; values, indices = torch.sort(x) &amp;gt;&amp;gt;&amp;gt; values tensor([-2,</description></item><item><title>PyTorchで「RuntimeError: Boolean value of Tensor with more than one value is ambiguous」エラーの解決方法</title><link>https://freshrimpsushi.github.io/jp/posts/3486/</link><pubDate>Wed, 04 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3486/</guid><description>エラー 損失関数nn.MESLoss()を使用したとき、次のようなエラーが発生した。 RuntimeError Traceback (most recent call last) &amp;lt;ipython-input-75-8c6e9ea829d4&amp;gt; in &amp;lt;module&amp;gt; ----&amp;gt; 1 nn.MSELoss(y_pred, y) 2 frames /usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py in legacy_get_string(size_average, reduce, emit_warning) 33 reduce = True 34 ---&amp;gt; 35 if size_average</description></item><item><title>PyTorchで与えられた分布からランダムサンプリングする方法</title><link>https://freshrimpsushi.github.io/jp/posts/3485/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3485/</guid><description>概要 PyTorchで与えられた分布からランダムサンプリングする方法を紹介する。ベータ、ベルヌーイ、コーシー、ガンマ、パレート、ポアソンなど多</description></item><item><title>딥러닝에서 레이어란?</title><link>https://freshrimpsushi.github.io/jp/posts/3484/</link><pubDate>Sat, 30 Sep 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3484/</guid><description>定義 ディープラーニングにおいて、線形変換 $L^{mn} : \mathbb{R}^{n} \to \mathbb{R}^{m}$はレイヤーlayer, 層と呼ばれる。 一般化 ディープラーニングに</description></item><item><title>畳み込みニューラルネットワーク</title><link>https://freshrimpsushi.github.io/jp/posts/3449/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3449/</guid><description>定義 畳み込み層、プーリング層、活性化関数などを適切に合成した合成関数を畳み込みニューラルネットワークconvolutional neural network, CNNと言</description></item><item><title>딥러닝에서 풀링층이란?</title><link>https://freshrimpsushi.github.io/jp/posts/3448/</link><pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3448/</guid><description>概要 人工ニューラルネットワークにおけるプーリング層とは、入力データの次元を局所的な単位で減らす関数を指す。指定された領域で最大値のみを残すも</description></item><item><title>多層パーセプトロン(MLP), 全結合ニューラルネットワーク(FCNN)</title><link>https://freshrimpsushi.github.io/jp/posts/3447/</link><pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3447/</guid><description>定義 $L_{i} : \mathbb{R}^{n_{i}} \to \mathbb{R}^{n_{i+1}}$を完全連結層とする。$\sigma : \mathbb{R} \to \mathbb{R}$を活性化関数とする。これ</description></item><item><title>アイリスデータセット</title><link>https://freshrimpsushi.github.io/jp/posts/3445/</link><pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3445/</guid><description>概要1 Irisデータセットは、アメリカの植物学者、エドガーアンダーソンEdgar Andersonによって作成され、イギリスの統計学者、ロナル</description></item><item><title>MNIST Database</title><link>https://freshrimpsushi.github.io/jp/posts/3444/</link><pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3444/</guid><description>概要1 $$ \includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$ MNISTmodified national institute of standards and technology データベースとは、アメリカの高校生と人口調査局の職員の数字の手書き文字に関するデータセットを指す。一般に[エムニス</description></item><item><title>Juliaのさまざまなディープラーニングフレームワーク</title><link>https://freshrimpsushi.github.io/jp/posts/3443/</link><pubDate>Sat, 08 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3443/</guid><description>概要 最終更新日: 2022年11月22日 Juliaの代表的なディープラーニングフレームワークには、Flux.jlがある。これと一緒に、他のフレ</description></item><item><title>自動微分</title><link>https://freshrimpsushi.github.io/jp/posts/3442/</link><pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3442/</guid><description>정의1 2 自動微分automatic differentiationとは、コンピュータプログラムのコードで定義された関数の導関数を求める方法のこ</description></item><item><title>表現者の定理の証明</title><link>https://freshrimpsushi.github.io/jp/posts/2408/</link><pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/2408/</guid><description>定理 インプット集合input set $X \ne \emptyset$ と正定値カーネル $k: X \times X \to \mathbb{R}$ が与えられているとする。学習データセットtraining datasetを $$ D</description></item><item><title>マシンラーニングにおけるワンホットエンコーディングとは？</title><link>https://freshrimpsushi.github.io/jp/posts/3438/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3438/</guid><description>定義 集合$X \subset \mathbb{R}^{n}$の部分集合$X_{i}$が次を満たすとしよう。 $$ X = X_{1} \cup \cdots \cup X_{N} \quad \text{and} \quad X_{i} \cap X_{j} = \varnothing \enspace (i \ne j) $$ $\beta = \left\{ e_{1},</description></item><item><title>機械学習における政府号カーネルと再生カーネルのヒルベルト空間</title><link>https://freshrimpsushi.github.io/jp/posts/2406/</link><pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/2406/</guid><description>定義 1 2 入力空間input space $X \ne \emptyset$ が定義域であり値域が複素数の集合 $\mathbb{C}$ の写像 $f: X \to \mathbb{C}$ で構成される関数空間 $\left( H , \left&amp;lt; \cdot , \cdot \right&amp;gt; \right) \subset \mathbb{C}^{X}$ がヒルベルト空間</description></item><item><title>줄리아에서 U-net 구현하기</title><link>https://freshrimpsushi.github.io/jp/posts/3434/</link><pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3434/</guid><description>概要 論文「U-Net: Convolutional networks for Biomedical Image Segmentation」で紹介されたU-Netをジュリアで実装する方法を紹介する。 コード U-Netの構造は</description></item><item><title>サポートベクターマシン</title><link>https://freshrimpsushi.github.io/jp/posts/2402/</link><pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/2402/</guid><description>モデル 1 簡単な定義 二値分類binary classificationが可能なデータを最もよく区別する直線や平面を見つける方法をサポートベクター</description></item><item><title>PyTorchでモデル/テンソルがロードされたデバイスを確認する方法</title><link>https://freshrimpsushi.github.io/jp/posts/3364/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3364/</guid><description>コード1 2 get_device()で確認できる。 &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import torch.nn as nn &amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available() True &amp;gt;&amp;gt;&amp;gt; Device = torch.device(&amp;#34;cuda:0&amp;#34; if torch.cuda.is_available() else &amp;#34;cpu&amp;#34;) # Model &amp;gt;&amp;gt;&amp;gt; model = nn.Sequential(nn.Linear(5,10), nn.ReLU(), nn.Linear(10,10), nn.ReLU(), nn.Linear(10,1)) &amp;gt;&amp;gt;&amp;gt; next(model.parameters()).get_device() -1 &amp;gt;&amp;gt;&amp;gt; model.to(Device) Sequential( (0): Linear(in_features=5, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=10, bias=True) (3):</description></item><item><title>ソフトプラス関数とは?</title><link>https://freshrimpsushi.github.io/jp/posts/3396/</link><pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3396/</guid><description>説明 2001年にDugasらによる論文『Incorporating Second-Order Functional Knowledge for Better Option Pricing』で紹介された。 $x^{+} = \max (0, x)$のなめらかなバージ</description></item><item><title>全結合層(線形層, 密接続層)</title><link>https://freshrimpsushi.github.io/jp/posts/3384/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3384/</guid><description>定義 $L: \mathbb{R}^{n} \to \mathbb{R}^{m}$をレイヤーとする。$\mathbf{W}$を$L$の行列表現とする。$\mathbf{W}$が$0$</description></item><item><title>機械学習における訓練/検証/テストセット</title><link>https://freshrimpsushi.github.io/jp/posts/3382/</link><pubDate>Wed, 08 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3382/</guid><description>定義 訓練中に使われるデータセット： モデルのパラメータを最適化するために使われるデータセットをトレーニングセットと言う。 モデルのハイパーパラメ</description></item><item><title>機械学習におけるエンコーダとデコーダ</title><link>https://freshrimpsushi.github.io/jp/posts/3380/</link><pubDate>Sat, 04 Mar 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3380/</guid><description>定義 データ集合$X \subset \mathbb{R}^{n}$が与えられたとする。機械学習の文脈でエンコーダencoder, 符号化器とは、適当な集合$Z</description></item><item><title>機械学習におけるReLUとは？</title><link>https://freshrimpsushi.github.io/jp/posts/3362/</link><pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3362/</guid><description>定義 機械学習において、次の関数を整流線形ユニットRectified Linear Unit(ReLU), レルという。 $$ f(x) = x^{+} := \max \left\{ 0, x \right\} $$ 説明 電気電子工学では、これをランプ関</description></item><item><title>딥러닝에서 인공신경망(ANN), 심층신경망(DNN), 순방향신경망(FNN)의 뜻과 차이점</title><link>https://freshrimpsushi.github.io/jp/posts/3446/</link><pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3446/</guid><description>概要 人工ニューラルネットワーク、深層ニューラルネットワーク、順方向ニューロネットワークなど、ディープラーニングで使われる用語について整理する</description></item><item><title>畳み込み層</title><link>https://freshrimpsushi.github.io/jp/posts/3386/</link><pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3386/</guid><description>定義 $\mathbf{W}$を$k \times k$行列としよう。$M^{n\times n} = M^{n\times n}(\mathbb{R})$をサイズが$n \times n$の実数行</description></item><item><title>論文レビュー: 物理情報基盤ニューラルネットワーク(PINN)</title><link>https://freshrimpsushi.github.io/jp/posts/3313/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3313/</guid><description>概要 レファレンスと数式の番号や表記法は、論文をそのまま踏襲する。 Physics-informed neural networks (PINN[ピン]と読む)は、数値的に解くために設計された微分方程式の</description></item><item><title>머신러닝에서 선형회귀모델의 최소제곱법 학습</title><link>https://freshrimpsushi.github.io/jp/posts/3263/</link><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3263/</guid><description>概要1 線形回帰モデルの学習方法の一つである最小二乗法least squaresを用いた方法を紹介する。 説明 データ集合を$X = \left\{ \mathbf{x}_{i} \right\</description></item><item><title>機械学習における線形回帰モデルの勾配降下法学習</title><link>https://freshrimpsushi.github.io/jp/posts/3261/</link><pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3261/</guid><description>概要1 線形回帰モデルの学習方法の一つである勾配降下法gradient descentを使った方法を紹介する。 説明 データセットを$X = \left\{ \mathbf{x}_{i} \ri</description></item><item><title>階層的クラスタリング</title><link>https://freshrimpsushi.github.io/jp/posts/3253/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3253/</guid><description>アルゴリズム 入力 $p$ 次元のデータ $N$個と距離 $d$が与えられているとする。 ステップ 1. それぞれの点を一つのクラスタと考える。最も近い二つのクラ</description></item><item><title>Julia FluxでMLPを実装して非線形関数を近似する方法</title><link>https://freshrimpsushi.github.io/jp/posts/3227/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3227/</guid><description>スタート 必要なパッケージをインポートし、近似したい非線形関数を定義しよう。 学習セットの生成 関数の定義域である$[-5, 5]$から無作為に10</description></item><item><title>PyTorchでリストのタイプエラー「TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.」の解決方法</title><link>https://freshrimpsushi.github.io/jp/posts/3225/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3225/</guid><description>エラー TypeError: can&amp;#39;t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first. リストを扱っているのに、PyTorchテンソルやNumPy配列ではないにもかかわらず、上記のよう</description></item><item><title>Julia FluxでMLPを実装し、MNISTで学習する方法</title><link>https://freshrimpsushi.github.io/jp/posts/3221/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3221/</guid><description>MNIST データセットをロードする 古い例ではFlux.Dataを使うコードが見られるが、もうフラックスではサポートされていない。 julia&amp;gt; Flux.Data.MNIST.images() ┌ Warning: Flux&amp;#39;s datasets are deprecated, please use</description></item><item><title>Julia・フラックスでワンホットエンコーディングする方法</title><link>https://freshrimpsushi.github.io/jp/posts/3219/</link><pubDate>Thu, 14 Apr 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3219/</guid><description>概要 ワンホットエンコーディングとは、データを分類classに応じて標準基底ベクトルにマッピングすることだ。Fluxではこれを実現するための関</description></item><item><title>Julia FluxでMLPを実装し、勾配降下法で最適化する方法</title><link>https://freshrimpsushi.github.io/jp/posts/3211/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3211/</guid><description>MLPの実装 まず、Juliaの機械学習パッケージFlux.jlと、オプティマイザーの更新メソッドupdate!を読み込もう。 using Flux using Flux: update! Den</description></item><item><title>Juliaフラックスで隠れ層を扱う方法</title><link>https://freshrimpsushi.github.io/jp/posts/3209/</link><pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3209/</guid><description>線形1 Fluxで、線形層はDense()で実装できる。 Dense(in, out, σ=identity; bias=true, init=glorot_uniform) Dense(W::AbstractMatrix, [bias, σ] 活性化関数のデフォルト値は恒等関数だ。reluや</description></item><item><title>PyTorchでテンソルの次元とサイズの扱い方</title><link>https://freshrimpsushi.github.io/jp/posts/3205/</link><pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3205/</guid><description>定義 $A$をパイトーチのテンソルとしよう。 次の順序ペア$(a_{0}, a_{1}, \dots, a_{n-1})$を$A$のサイズと呼ぶ。 $$ \text{A.size() = torch.Size}([a_{0}, a_{1}, \dots, a_{n-1} ]) $$ $\prod \limits_{i=0}^{n-1} a_{i}</description></item><item><title>PyTorchでテンソルをパディングする方法</title><link>https://freshrimpsushi.github.io/jp/posts/3199/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3199/</guid><description>コード 1 torch.nn.functional.pad(input, pad, mode='constant', value=0.0) input: パディングするテンソル pad: パディングの位置 mode: パディングの方法 value: パディングの値 説明 pad 次元$n$のテンソルを入力とする時、最大で</description></item><item><title>Juliaで機械学習データセットを使用する方法</title><link>https://freshrimpsushi.github.io/jp/posts/3191/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3191/</guid><description>説明 MLDatasets.jl1 2パッケージを使用すると、以下のデータセットを使用できる。リンクがあるデータセットは、それぞれの文書で使用方法を説明している。 ビジョン</description></item><item><title>PyTorchでテンソルを結合またはスタックする方法</title><link>https://freshrimpsushi.github.io/jp/posts/3187/</link><pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3187/</guid><description>テンソルを連結する cat()1 cat(tensors, dim=0)は、指定された次元を基準に2つ以上のテンソルを連結する。つまり、指定された次元のサイズが増加するようにテン</description></item><item><title>PyTorchでモデルの重み値を得る方法</title><link>https://freshrimpsushi.github.io/jp/posts/3183/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3183/</guid><description>説明 次のようなモデルを定義しよう。 import torch import torch.nn as nn class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.linear = nn.Linear(3, 3, bias=True) self.conv = nn.Conv2d(3, 5, 2) f = Model() すると、.weightや.biasメソッドで各層の重みや</description></item><item><title>PyTorchでテンソルをディープコピーする方法</title><link>https://freshrimpsushi.github.io/jp/posts/3177/</link><pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3177/</guid><description>説明 PyTorchのテンソルも他のオブジェクトと同様にcopy.deepcopy()を使って深いコピーができる。 &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; import copy &amp;gt;&amp;gt;&amp;gt; a = torch.ones(2,2) &amp;gt;&amp;gt;&amp;gt; b = a</description></item><item><title>PyTorchでリストとループを使用して人工ニューラルネットワークレイヤーを定義する方法</title><link>https://freshrimpsushi.github.io/jp/posts/3173/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3173/</guid><description>説明 階層を多く積み上げる必要がある場合や、頻繁にニューラルネットワークの構造を変える必要があるなど、人工ニューラルネットワークの定義を自動化</description></item><item><title>論文レビュー: Neural Ordinary Differential Equations</title><link>https://freshrimpsushi.github.io/jp/posts/3159/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3159/</guid><description>概要および要約 「Neural Ordinary Differential Equations」はRicky T. Q. Chenら3名が2018年に発表した論文であり、2018 NeurIPS Best Papers</description></item><item><title>PyTorchでランダム順列を作成し、テンソルの順序をシャッフルする方法</title><link>https://freshrimpsushi.github.io/jp/posts/3140/</link><pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3140/</guid><description>torch.randperm()1 torch.randperm(n): 0からn-1までのランダムな整数の順列を返す。もちろん、整数型でなければ入力に使えない。 &amp;gt;&amp;gt;&amp;gt; torch.randperm(4) tensor([2, 1, 0, 3]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(8) tensor([4, 0, 1, 3, 2, 5, 6, 7]) &amp;gt;&amp;gt;&amp;gt; torch.randperm(16) tensor([12, 5, 6, 3, 15, 13,</description></item><item><title>PyTorchでの重み、モデル、オプティマイザの保存と読み込み方法</title><link>https://freshrimpsushi.github.io/jp/posts/3114/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3114/</guid><description>再学習しない場合1 2 3 保存する 再学習しないのなら、ウェイトやモデルのみを保存すればいい。ただし、以下で述べるように、再学習をする予定なら、オ</description></item><item><title>PyTorchでNumpy配列からカスタムデータセットを作成して使用する方法</title><link>https://freshrimpsushi.github.io/jp/posts/3108/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3108/</guid><description>説明 &amp;gt;&amp;gt;&amp;gt; import numpy as np &amp;gt;&amp;gt;&amp;gt; import torch &amp;gt;&amp;gt;&amp;gt; from torch.utils.data import TensorDataset, DataLoader $32\times 32$のサイズの「モノクロ」写真100枚を積み重ねたnumpy配列$X$と、それに対するラベル$Y$が用意</description></item><item><title>PyTorchでの重みの初期化方法</title><link>https://freshrimpsushi.github.io/jp/posts/3104/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3104/</guid><description>コード1 次のようにニューラルネットワークを定義したとする。forward部分は省略されている。 import torch import torch.nn as nn class Custom_Net(nn.Module): def __init__(self): super(Custom_Net, self).__init__() self.linear_1 = nn.Linear(1024, 1024, bias=False) self.linear_2 = nn.Linear(1024, 512, bias=False) self.linear_3</description></item><item><title>PyTorchでMLPを実装する方法</title><link>https://freshrimpsushi.github.io/jp/posts/3103/</link><pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3103/</guid><description>環境 OS: Windows10 Version: Python 3.9.2, torch 1.8.1+cu111</description></item><item><title>PyTorch RuntimeError:「gradはスカラー出力に対してのみ暗黙的に作成できます」の解決法</title><link>https://freshrimpsushi.github.io/jp/posts/3095/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3095/</guid><description>事例1 もし損失関数をloss = sum(a,b)と設定していたら、loss.backward()でバックプロパゲーションを行う際にこのエラーが</description></item><item><title>逆伝播アルゴリズム</title><link>https://freshrimpsushi.github.io/jp/posts/3077/</link><pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3077/</guid><description>この文は逆転派アルゴリズムの原理を数学専攻者が理解しやすいように作成された。 表記法 上図のような 人工ニューラルネットワーク が与えられたとする。</description></item><item><title>パーセプトロン収束定理</title><link>https://freshrimpsushi.github.io/jp/posts/3023/</link><pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3023/</guid><description>$X^{+}$, $X^{-}$が線形分離可能なトレーニングセットだとしよう。$y$を以下のラベルとする。 $$ y_{i} = \pm 1\ (\mathbf{x}_{i} \in X^{\pm}) $$ トレーニングセット全体$X = X^{+} \cup</description></item><item><title>機械学習における強化学習とは？</title><link>https://freshrimpsushi.github.io/jp/posts/3029/</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/3029/</guid><description>定義 強化学習とは、エージェントが環境と相互作用して累積報酬を最大化するポリシーを見つけることができるようにすることである。 説明1 強化学習を構</description></item><item><title>딥러닝의 수학적 근거, 시벤코 정리 증명</title><link>https://freshrimpsushi.github.io/jp/posts/1853/</link><pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1853/</guid><description>定理 $\sigma$ が連続シグモイド関数だとすると $$ S := \left\{ G(x) = \sum_{k=1}^{N} \alpha_{k} \sigma \left( y_{k}^{T} x+ \theta_{k} \right) : y_{k} \in \mathbb{R}^{n} \land \alpha_{k} , \theta_{k} \in \mathbb{R} \land N \in \mathbb{N} \right\} $$ は $C\left( I_{n} \right)$ において均等密である。すなわち、すべ</description></item><item><title>シグモイド関数とは?</title><link>https://freshrimpsushi.github.io/jp/posts/1851/</link><pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1851/</guid><description>定義 次のを満たす関数$\sigma : \mathbb{R} \to \mathbb{R}$をシグモイダル関数sigmoidal functionと呼ぶ。 $$ \sigma (t) \to \begin{cases} 1 &amp;amp; \text{as }</description></item><item><title>識別関数とは何か？</title><link>https://freshrimpsushi.github.io/jp/posts/1838/</link><pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1838/</guid><description>定義 すべての$y \in \mathbb{R}^{n}$と$\theta \in \mathbb{R}$、そしてある$\mu \in M \left( I_{n} \right)$に対して</description></item><item><title>機械学習における回帰のための線形モデル</title><link>https://freshrimpsushi.github.io/jp/posts/1887/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1887/</guid><description>定義1 単純モデル データ集合$X = \left\{ \mathbf{x}_{i} \right\}$とラベル集合$Y = \left\{ y_{i} \right\}$の間のターゲット関数$f : X \to Y$を次のように</description></item><item><title>ロジスティック関数とは？</title><link>https://freshrimpsushi.github.io/jp/posts/1775/</link><pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1775/</guid><description>定義 1 ロジスティック関数は、微分方程式の解として$y ' = y(1-y)$で求められるものである。 $$ y(t) = {{ 1 } \over { 1 + e^{-t} }} $$ 説明 もっと一般的な形</description></item><item><title>シグモイド関数とは?</title><link>https://freshrimpsushi.github.io/jp/posts/1769/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1769/</guid><description>定義 1 すべての$x \in \mathbb{R}$で定義され$\sigma ' (x) \ge 0$であり、ただ一つの変曲点を持つ、有界で微分可能なスカラ関数$\s</description></item><item><title>パーセプトロンの定義</title><link>https://freshrimpsushi.github.io/jp/posts/1846/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1846/</guid><description>정의 線形関数 $f(x) = wx + b$と単位ステップ関数 $H$の合成をパーセプトロンperceptronと定義する。 $$ \text{Perceptron} := H \circ f (x) = H(wx + b) $$ 多変数関数の場</description></item><item><title>コンピュータビジョンとは何か</title><link>https://freshrimpsushi.github.io/jp/posts/1839/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1839/</guid><description>説明 コンピュータビジョン とは主に人間の視覚に相当する機能をコンピュータが実行できるようにする研究分野で、画像や映像を扱う。コンピュータビジョ</description></item><item><title>ディープラーニングにおける連続学習</title><link>https://freshrimpsushi.github.io/jp/posts/1837/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1837/</guid><description>説明 ディープラーニングでの連続学習とは、人工ニューラルネットワークが複数のタスクを順番に学習することを言い、生涯学習、段階的学習と同義である</description></item><item><title>論文レビュー：ゼロトレーニングエラー達成後にゼロトレーニングロスが必要ですか？</title><link>https://freshrimpsushi.github.io/jp/posts/1809/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1809/</guid><description>論文レビュー フラッディングfloodingは、ICML 2020で発表されたDo We Need Zero Training Loss After Achieving Zero Training Error?で紹介された正則化技術を指す。こ</description></item><item><title>機械学習でよく使用されるデータセット</title><link>https://freshrimpsushi.github.io/jp/posts/1808/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1808/</guid><description>コンピュータービジョン MNIST 機械学習を学ぶときに最初に出会うデータセットだ。[エムニスト]と読み、$28\times 28$サイズの手書き数字画像</description></item><item><title>マシンラーニングにおけるオーバーフィッティングと正則化とは？</title><link>https://freshrimpsushi.github.io/jp/posts/1807/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1807/</guid><description>オーバーフィッティング トレーニングロスは減っていくが、テストロス（バリデーションロス）が減らない、あるいは増えてしまう現象をオーバーフィッテ</description></item><item><title>k-平均クラスタリング</title><link>https://freshrimpsushi.github.io/jp/posts/1365/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1365/</guid><description>アルゴリズム 入力 $p$次元のデータ$N$個と、自然数$k$が与えられているとする。 ステップ 1. 初期化 $k$個の点$\mu_{1} , \cdots , \mu_</description></item><item><title>教師あり学習と教師なし学習</title><link>https://freshrimpsushi.github.io/jp/posts/1013/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1013/</guid><description>定義 機械学習では、従属変数が決まっている場合を教師あり学習、そうでない場合を教師なし学習という。 例 教師あり学習と教師なし学習の違いを簡単に言</description></item><item><title>ディープラーニングにおけるドロップアウト</title><link>https://freshrimpsushi.github.io/jp/posts/1004/</link><pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/1004/</guid><description>定義 ドロップアウトdropoutは、人工ニューラルネットワークのニューロンを確率的に使用しないことで過学習を防ぐ技術だ。 説明 一見するとちょっ</description></item><item><title>ディープラーニングにおけるソフトマックス関数</title><link>https://freshrimpsushi.github.io/jp/posts/993/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/993/</guid><description>定義 $\mathbf{x} := (x_{1} , \cdots , x_{n}) \in \mathbb{R}^{n}$ としよう。 $\displaystyle \sigma_{j} ( \mathbf{x} ) = {{ e^{x_{j}} } \over {\sum_{i=1}^{n} e^{x_{i}} }}$ に対して、$\sigma ( \mathbf{x} ) := \left( \sigma_{1} (\mathbf{x}) , \cdots , \sigma_{n} (\mathbf{x} ) \right)$のように定義され</description></item><item><title>ディープラーニングにおける活性化関数</title><link>https://freshrimpsushi.github.io/jp/posts/991/</link><pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/991/</guid><description>定義 実際の生物の閾値を模倣した非線形関数を活性化関数activation functionと言う。 数学的定義 ディープラーニングでは非線形スカラ</description></item><item><title>ディープラーニングとは？</title><link>https://freshrimpsushi.github.io/jp/posts/996/</link><pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/996/</guid><description>定義 ディープラーニングは、人工神経網を使用した機械学習の一種で、特に人工神経網を構成する際に複数のレイヤーを使用する技術を言います。 モチベー</description></item><item><title>機械学習における勾配降下法と確率的勾配降下法</title><link>https://freshrimpsushi.github.io/jp/posts/987/</link><pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/987/</guid><description>概要 損失関数の勾配を利用して損失関数の極小値を見つけるアルゴリズムの中でもっとも単純な方法として 勾配降下法gradient Descent Algorith</description></item><item><title>機械学習における損失関数</title><link>https://freshrimpsushi.github.io/jp/posts/967/</link><pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/967/</guid><description>定義 データ$Y = \begin{bmatrix} y_{1} \\ \vdots \\ y_{n} \end{bmatrix}$の推定値が$\widehat{Y} = \begin{bmatrix} \widehat{ y_{1} } \\ \vdots \\ \widehat{y_{n}} \end{bmatrix}$とし</description></item><item><title>인공 신경망이란?</title><link>https://freshrimpsushi.github.io/jp/posts/962/</link><pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate><guid>https://freshrimpsushi.github.io/jp/posts/962/</guid><description>定義 現実の生物の神経系を模倣したネットワークを人工ニューラルネットワークartificial neural network (ANN)と呼ぶ。 数学的定義 スカラー関数 $\sigma :</description></item></channel></rss>