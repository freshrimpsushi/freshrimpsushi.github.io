[{"categories":"측도론","contents":"定義1 関数 $f : \\mathbb{R} \\to \\mathbb{R} (\\text{または } \\mathbb{C})$ が与えられたとする。$f$が任意の有限個の互いに素な区間 $(a_{i}, b_{i}) \\subset [a,b]$に対しても以下の条件を満たす場合、$[a, b]$ 上で 絶対連続absolutely continuousと言われる。\n$$ \\forall \\epsilon \\gt 0 \\quad \\exist \\delta \\gt 0 \\text{ such that } \\sum\\limits_{i=1}^{N} (b_{i} - a_{i}) \\lt \\delta \\implies \\sum\\limits_{i=1}^{N} \\left| f(b_{j}) - f(a_{j}) \\right| \\lt \\epsilon $$\n説明 定義により、絶対連続であれば一様連続でもある。\n性質 $f$が微分可能であり、導関数 $f\u0026rsquo;$ が有界であれば、$f$は絶対連続である。\n参照 実数関数の絶対連続性 測度の絶対連続性 符号付き測度の絶対連続性 Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p105\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3542,"permalink":"https://freshrimpsushi.github.io/jp/posts/3542/","tags":null,"title":"絶対連続実関数"},{"categories":"줄리아","contents":"概要 InfiniteArrays.jlは無限のサイズを持つ配列を使えるようにするパッケージ1で、実際にはレイジー配列と多くの関連がある。レイジー評価とは、配列に計算すべきものがなんであるかは知っているけど、本当に必要になるまでは計算を先延ばしにする方法のことだ。もちろん、コンピューターは無限を理解できないけど、この方法を使ってコンピューターでも無限配列を実装したんだ。\nコード ∞ julia\u0026gt; using InfiniteArrays\rjulia\u0026gt; 3141592 \u0026lt; ∞\rtrue\rjulia\u0026gt; Inf == ∞\rtrue\rjulia\u0026gt; Inf === ∞\rfalse InfiniteArrays.jlを読み込むと、まず ∞ 記号で無限を表せるようになる。無理にパッケージを使わなくても Infで無限を表せるけど、こっちの方が直感的に使えるようになるんだ。大小関係の比較では同じ無限の大きさを持つけど、ポインターとして見た場合は違っていて区別できる。\nℵ₀ julia\u0026gt; x = zeros(Int64, ∞);\rjulia\u0026gt; length(x)\rℵ₀ 0でいっぱいの無限配列を作ると、これは無限可算集合になり、その大きさはアレフゼロ $\\aleph_{0}$だ。\n普通に使える julia\u0026gt; x[2] = 3; x[94124843] = 7; x\rℵ₀-element LazyArrays.CachedArray{Int64, 1, Vector{Int64}, Zeros{Int64, 1, Tuple{InfiniteArrays.OneToInf{Int64}}}} with indices OneToInf():\r0\r3\r0\r0\r0\r0\r0\r0\r0\r⋮\rjulia\u0026gt; sum(x)\r10 無限配列だとしても、インターフェイスが大きく変わる訳ではない。普段扱ってる配列と同様に扱えば、考えている通りに動くよ。\n全コード using InfiniteArrays\r3141592 \u0026lt; ∞\rInf == ∞\rInf === ∞\rx = zeros(Int64, ∞);\rlength(x)\rx[2] = 3; x[94124843] = 7; x\rsum(x) 環境 OS: Windows julia: v1.7.3 https://github.com/JuliaArrays/InfiniteArrays.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2511,"permalink":"https://freshrimpsushi.github.io/jp/posts/2511/","tags":null,"title":"ジュリアで無限配列を使用する方法"},{"categories":"줄리아","contents":"概要 MAT.jlは MATLABで使用されるデータ保存形式である*.matファイルを読み書きするライブラリだ1。 Juliaがそうであるように、このパッケージは既存のプログラミング言語や習慣を捨てさせるのではなく、できるだけ馴染みのある環境を提供することでユーザーを獲得する戦略を示している。\nJuliaの速さと利便性は大きな利点だが、MATLABは研究目的の視覚化に独特の利点を持っている。既にMATLABで図を描く作業に習熟している場合、\u0026lsquo;Juliaへの完全な移行\u0026rsquo;は大きな犠牲を伴うため、魅力的ではない。MAT.jlの存在は「そんな心配するな、計算はJuliaで速くこなしてから、図はMATLABで描き直せばいいんだ」という誘惑そのものだ。 逆に、\u0026lsquo;MATLABで既に作業をして構築したものが多いが、何か限界を感じてJuliaに移りたい状況\u0026rsquo;でも役立つ。 mat形式より進化したJulia独自の保存方式は、JLD2.jlパッケージを参照すればいい。\nコード X = rand(0:9, 8, 3)\rusing MAT\rmatwrite(\u0026#34;example.mat\u0026#34;, Dict(\u0026#34;Y\u0026#34; =\u0026gt; X))\rmatfile = matopen(\u0026#34;elpmaxe.mat\u0026#34;)\rA = read(matfile, \u0026#34;A\u0026#34;)\rclose(matfile) Julia → MATLAB MATLAB → Julia 環境 OS: Windows julia: v1.7.3 MAT v0.10.3 MATLAB: R2022b https://github.com/JuliaIO/MAT.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2509,"permalink":"https://freshrimpsushi.github.io/jp/posts/2509/","tags":null,"title":"ジュリアでmatファイルを読み書きする方法"},{"categories":"줄리아","contents":"概要 UnicodePlots.jlはジュリア REPLでユニコード文字を使って図を出力するライブラリ1で、プログラムが進行する中で軽量でありながら高品質の視覚化を可能にする。\nコード using UnicodePlots\rp1 = lineplot(100 |\u0026gt; randn |\u0026gt; cumsum)\rp1 = lineplot!(p1, 100 |\u0026gt; randn |\u0026gt; cumsum); p1\rUnicodePlots.heatmap(cumsum(abs.(randn(100,100)), dims=2)) 上の例のコードを実行した結果は次の通りです。\n環境 OS: Windows julia: v1.7.3 UnicodePlots v3.0.4 https://github.com/JuliaPlots/UnicodePlots.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2507,"permalink":"https://freshrimpsushi.github.io/jp/posts/2507/","tags":null,"title":"ジュリアコンソールでシンプルなグラフィックを出力する方法"},{"categories":"줄리아","contents":"方法 コンソールでCtrl + Lを押すと、コンソールが一見してクリアされるが、一部の環境では本当にリセットされるわけではなく、ウィンドウが上にスクロールされたように見える場合もある。きれいに消去したり、キーボード入力に頼らないためには、ASCII文字\\033cを出力するといい12。\nprint(\u0026#34;\\033c\u0026#34;) また、\\007を出力すると通知音が鳴る。3 簡易的なシミュレーションの終了など、音で知りたい場合に意外と便利だ。\nprintln(\u0026#34;\\007\u0026#34;) 環境 OS: Windows julia: v1.7.3 https://stackoverflow.com/questions/26548687/julia-how-to-clear-console\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/questions/47503734/what-does-printf-033c-mean\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://discourse.julialang.org/t/how-to-play-a-sound-or-tone-when-a-program-ends/41239/13\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2505,"permalink":"https://freshrimpsushi.github.io/jp/posts/2505/","tags":null,"title":"ジュリアでコンソールを初期化する方法"},{"categories":"줄리아","contents":"概要 1 Juliaでは、dropmissing()関数を使って簡単に欠損値を削除できる。\nコード julia\u0026gt; df = DataFrame(x = [\u0026#34;i\u0026#34;, missing, \u0026#34;k\u0026#34;, \u0026#34;j\u0026#34;], y = [1, 2, 3, missing])\r4×2 DataFrame\rRow │ x y │ String? Int64? ─────┼──────────────────\r1 │ i 1\r2 │ missing 2\r3 │ k 3\r4 │ j missing 上記のように欠損値missingがあるデータフレームが与えられているとしよう。\njulia\u0026gt; dropmissing(df, :x)\r3×2 DataFrame\rRow │ x y │ String Int64? ─────┼─────────────────\r1 │ i 1\r2 │ k 3\r3 │ j missing julia\u0026gt; dropmissing(df, :y)\r3×2 DataFrame\rRow │ x y │ String? Int64 ─────┼────────────────\r1 │ i 1\r2 │ missing 2\r3 │ k 3 欠損値を削除したい列のシンボルを引数に入れればいい。\njulia\u0026gt; dropmissing(df)\r2×2 DataFrame\rRow │ x y │ String Int64\r─────┼───────────────\r1 │ i 1\r2 │ k 3 データフレーム全体から欠損値をすべて削除したい場合は、列を何も入力しなければいい。\n全コード using DataFrames\rdf = DataFrame(x = [\u0026#34;i\u0026#34;, missing, \u0026#34;k\u0026#34;, \u0026#34;j\u0026#34;], y = [1, 2, 3, missing])\rdropmissing(df, :x)\rdropmissing(df, :y)\rdropmissing(df) 環境 OS: Windows julia: v1.7.3 https://discourse.julialang.org/t/how-to-remove-rows-containing-missing-from-dataframe/12234/7\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2503,"permalink":"https://freshrimpsushi.github.io/jp/posts/2503/","tags":null,"title":"ジュリアでデータフレームの欠損値を削除する方法"},{"categories":"머신러닝","contents":"概要1 2 勾配降下法で使用されるアダプティブラーニングレートと、これを適用したモデルであるAdaGrad、RMSProp、Adamについて説明する。\n説明 勾配降下法で学習率learning rateは、パラメータが収束する速度、メソッドの成功の有無などを決定する重要なパラメータである。通常 $\\alpha$、$\\eta$と表記され、パラメータをアップデートする際、どれだけ勾配を反映するかを決める因子である。\n学習率による最適化の様子：大きな学習率(左)、小さな学習率(右)\r$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L (\\boldsymbol{\\theta}_{i}) $$\n基本的な勾配降下法では $\\alpha$ は定数として説明されているが、この場合 勾配はベクトルなので、全ての変数（パラメータ）に対して同じ学習率が適用される。\n$$ \\alpha \\nabla L (\\boldsymbol{\\theta}) = \\alpha \\begin{bmatrix} \\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} = \\begin{bmatrix} \\alpha\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nしたがって、学習率を $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{k})$ のようなベクトルとして考え、勾配項を以下の式のように一般化することができる。\n$$ \\boldsymbol{\\alpha} \\odot \\nabla L (\\boldsymbol{\\theta}) = \\begin{bmatrix} \\alpha_{1}\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha_{2}\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha_{k}\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nここで $\\odot$ は行列のアダマール積（要素毎の積）を表す。このようにパラメータ毎に異なる適用される学習率 $\\boldsymbol{\\alpha}$ を アダプティブラーニングレートadaptive learning rateと呼ぶ。以下の技術はアダプティブラーニングレートを勾配に依存して決定するため、$\\boldsymbol{\\alpha}$ は以下のような関数と見なすことができる。\n$$ \\boldsymbol{\\alpha} (\\nabla L(\\boldsymbol{\\theta})) = \\begin{bmatrix} \\alpha_{1}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\alpha_{2}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\dots \u0026amp; \\alpha_{k}(\\nabla L(\\boldsymbol{\\theta})) \\end{bmatrix} $$\n以下ではAdaGrad、RMSProp、Adamを紹介する。ここで重要な事実は、モーメンタム技術を含むこれらのオプティマイザー間に絶対的優位性はないということである。分野によって、作業によって最適なオプティマイザーは異なるので、単純に「何が最も良いか」についての判断や質問は良くない。自分が属する分野で主に使用されているものが何かを把握することが役に立ち、それがない場合やよくわからない場合は、SGD+モーメンタムまたはAdamを使用することが無難である。\nAdaGrad AdaGradは論文\u0026quot;(Duchi et al., 2011)Adaptive subgradient methods for online learning and stochastic optimization\u0026quot;で紹介されたアダプティブラーニングレート技術です。この名前はadaptive gradientの略で、[エイダグラード]または[アダグラード]と読みます。AdaGradでは、各パラメータに対する学習率を勾配に反比例するように設定します。ベクトル $\\mathbf{r}$ を次のように定義します。\n$$ \\mathbf{r} = (\\nabla L) \\odot (\\nabla L) = \\begin{bmatrix} \\left( \\dfrac{\\partial L}{\\partial \\theta_{1}} \\right)^{2} \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{2}} \\right)^{2} \u0026amp; \\cdots \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{k}} \\right)^{2} \\end{bmatrix} $$\n全体の学習率global learning rate $\\epsilon$、任意の小さい数 $\\delta$ に対して、アダプティブラーニングレート $\\boldsymbol{\\alpha}$ は次のようになります。\n$$ \\boldsymbol{\\alpha} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} $$\n式からわかるように、勾配の成分が大きい変数には学習率を小さくし、勾配の成分が小さい変数には学習率を大きく適用します。$\\delta$ は分母が $0$ や非常に小さい数になるのを防ぐためのもので、通常は $10^{-5} \\sim 10^{-7}$ の値を使用することが多いです。また、学習率は反復ごとに累積されます。$i$ 番目の反復での勾配を $\\nabla L _{i} = \\nabla L (\\boldsymbol{\\theta}_{i})$ とすると、\n$$ \\begin{align} \\mathbf{r}_{i} \u0026amp;= (\\nabla L_{i}) \\odot (\\nabla L_{i}) \\nonumber \\\\ \\boldsymbol{\\alpha}_{i} \u0026amp;= \\boldsymbol{\\alpha}_{i-1} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = \\sum_{j=1}^{i} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} \\\\ \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i} \\nonumber \\end{align} $$\nアルゴリズム: AdaGrad 入力 全体の学習率 $\\epsilon$、小さな正数 $\\delta$、エポック $N$ 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. for $i = 1, \\cdots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算し、各成分を二乗する 5. $\\boldsymbol{\\alpha} \\leftarrow \\boldsymbol{\\alpha} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # アダプティブ学習率更新 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # パラメータ更新 7. end for RMSProp RMSPropは Root Mean Square Propagationの略で、ジェフリー・ヒントンGeoffrey Hintonの講義 Neural networks for machine learningで提案されたアダプティブラーニングレート技術です。基本的にAdaGradの変形であり、追加される項が指数的に減少するように$(1)$の加算を加重和に変えただけです。$\\rho \\in (0,1)$に対して、\n$$ \\boldsymbol{\\alpha}_{i} = \\rho \\boldsymbol{\\alpha}_{i-1} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = (1-\\rho) \\sum_{j=1}^{i} \\rho^{i-j} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} $$\n通常は $\\rho = 0.9, 0.99$ のような大きな値が使用されます。\nアルゴリズム: RMSProp 入力 全体の学習率 $\\epsilon$、小さな正数 $\\delta$、減衰率 $\\rho$、エポック $N$ 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. for $i = 1, \\dots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算し、各成分を二乗する 5. $\\boldsymbol{\\alpha} \\leftarrow \\rho \\boldsymbol{\\alpha} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # アダプティブ学習率更新 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # パラメータ更新 7. end for Adam Adam\u0026ldquo;adaptive moments\u0026quot;から派生は論文\u0026rdquo;(Kingma and Ba, 2014)Adam: A method for stochastic optimization\u0026quot;で紹介されたオプティマイザです。アダプティブラーニングレートとモーメンタム技術を組み合わせたもので、RMSProp + モーメンタムと見ることができます。RMSPropとモーメンタムを理解していれば、Adamを理解するのは難しくありません。RMSProp、モーメンタム、Adamをそれぞれ比較すると、以下のようになります。$\\nabla L_{i} = \\nabla L(\\boldsymbol{\\theta}_{i})$とすると、\nMomentum\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + \\nabla L_{i} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\mathbf{p}_{i}$\rRMSProp\r$\\mathbf{r}_{i} = \\nabla L_{i} \\odot \\nabla L_{i} \\\\\r\\boldsymbol{\\alpha}_{i} = \\beta_{2} \\boldsymbol{\\alpha}_{i-1} + (1-\\beta_{2})\\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} \\quad (\\boldsymbol{\\alpha}_{0}=\\mathbf{0})\\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i}$\rAdam\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + (1-\\beta_{1}) \\nabla L_{i-1} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\\\\[0.5em]\r\\hat{\\mathbf{p}}_{i} = \\dfrac{\\mathbf{p}_{i}}{1-(\\beta_{1})^{i}} \\\\\\\\[0.5em]\r\\mathbf{r}_{i} = \\beta_{2} \\mathbf{r}_{i-1} + (1-\\beta_{2}) \\nabla L_{i} \\odot \\nabla L_{i} \\\\\\\\[0.5em]\r\\hat{\\mathbf{r}}_{i} = \\dfrac{\\mathbf{r}}{1-(\\beta_{2})^{i}} \\\\\\\\[0.5em]\r\\hat{\\boldsymbol{\\alpha}}_{i} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}_{i}}} \\\\\\\\[0.5em]\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\hat{\\boldsymbol{\\alpha}_{i}} \\odot \\hat{\\mathbf{p}_{i}}\r$\r$\\hat{\\mathbf{p}}_{i}$および$\\hat{\\mathbf{r}}_{i}$を計算する際に$1 - \\beta^{i}$で割る理由は、$\\mathbf{p}_{i}$および$\\mathbf{r}_{i}$が加重和であるため、これを加重平均に変換するためです。\nアルゴリズム: Adam\r入力 全体の学習率 $\\epsilon$ (推奨値は $0.001$), エポック $N$ 小さな定数 $\\delta$ (推奨値は $10^{-8}$) 減衰率 $\\beta\\_{1}, \\beta\\_{2}$ (推奨値はそれぞれ $0.9$ と $0.999$) 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. モーメンタムを $\\mathbf{p} = \\mathbf{0}$ で初期化する。 4. for $i = 1, \\dots, N$ do 5. $\\mathbf{p} \\leftarrow \\beta\\_{1}\\mathbf{p} + (1-\\beta\\_{1}) \\nabla L$ # 加重和でモーメンタム更新 6. $\\hat{\\mathbf{p}} \\leftarrow \\dfrac{\\mathbf{p}}{1-(\\beta\\_{1})^{i}}$ # 和を加重平均に補正 7. $\\mathbf{r} \\leftarrow \\beta\\_{2} \\mathbf{r} + (1-\\beta\\_{2}) \\nabla L \\odot \\nabla L$ # 加重和で勾配の二乗ベクトル更新 8. $\\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1-(\\beta\\_{2})^{i}}$ # 和を加重平均に補正 9. $\\hat{\\boldsymbol{\\alpha}} \\leftarrow \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}}}$ # アダプティブ学習率更新 10. $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\alpha}} \\odot \\hat{\\mathbf{p}}$ # パラメータ更新 11. end for Ian Goodfellow, Deep Learning, 8.5 アダプティブラーニングレートを用いたアルゴリズム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nオ・イルソク, 機械学習(MACHINE LEARNING) (2017), ch 5.4 アダプティブラーニングレート\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3529,"permalink":"https://freshrimpsushi.github.io/jp/posts/3529/","tags":null,"title":"適応的な学習率: AdaGrad, RMSProp, Adam"},{"categories":"줄리아","contents":"概要 Juliaで環境変数を参照する方法を説明する1。\nコード Base.ENV\rBase.ENV[\u0026#34;JULIA_NUM_THREADS\u0026#34;] 見るように、別のパッケージをロードする必要はなく、Base.ENVを通じて直接アクセスできる。辞書として読まれるため、求める環境変数の名前をキーとして置くと、その環境変数を文字列で得る。上のコード2行を実行した結果は以下の通りだ。\njulia\u0026gt; Base.ENV\rBase.EnvDict with 62 entries:\r\u0026#34;ALLUSERSPROFILE\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\u0026#34;\r\u0026#34;APPDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\AppData\\\\Roaming\u0026#34;\r\u0026#34;CHROME_CRASHPAD_PIPE_NAME\u0026#34; =\u0026gt; \u0026#34;\\\\\\\\.\\\\pipe\\\\crashpad_14984_WLSYYXMTXMJWXZQG\u0026#34;\r\u0026#34;COMMONPROGRAMFILES\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\Common Files\u0026#34;\r\u0026#34;COMMONPROGRAMFILES(X86)\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files (x86)\\\\Common Files\u0026#34;\r\u0026#34;COMMONPROGRAMW6432\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\Common Files\u0026#34;\r\u0026#34;COMPUTERNAME\u0026#34; =\u0026gt; \u0026#34;SICKRIGHT\u0026#34;\r\u0026#34;COMSPEC\u0026#34; =\u0026gt; \u0026#34;C:\\\\WINDOWS\\\\system32\\\\cmd.exe\u0026#34;\r\u0026#34;CUDA_PATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.5\u0026#34;\r\u0026#34;CUDA_PATH_V11_5\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.5\u0026#34;\r\u0026#34;DRIVERDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData\u0026#34;\r\u0026#34;GOPATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\go\u0026#34;\r\u0026#34;HOMEDRIVE\u0026#34; =\u0026gt; \u0026#34;C:\u0026#34;\r\u0026#34;HOMEPATH\u0026#34; =\u0026gt; \u0026#34;\\\\Users\\\\rmsms\u0026#34;\r\u0026#34;JULIA_NUM_THREADS\u0026#34; =\u0026gt; \u0026#34;16\u0026#34;\r\u0026#34;LOCALAPPDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\AppData\\\\Local\u0026#34;\r\u0026#34;LOGONSERVER\u0026#34; =\u0026gt; \u0026#34;\\\\\\\\SICKRIGHT\u0026#34;\r\u0026#34;NAVER\u0026#34; =\u0026gt; \u0026#34;e=2.718281\u0026#34;\r\u0026#34;NUMBER_OF_PROCESSORS\u0026#34; =\u0026gt; \u0026#34;16\u0026#34;\r\u0026#34;NVCUDASAMPLES11_5_ROOT\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v11.5\u0026#34;\r\u0026#34;NVCUDASAMPLES_ROOT\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v11.5\u0026#34;\r\u0026#34;NVTOOLSEXT_PATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NvToolsExt\\\\\u0026#34;\r\u0026#34;ONEDRIVE\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive - knu.ac.kr\u0026#34;\r\u0026#34;ONEDRIVECOMMERCIAL\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive - knu.ac.kr\u0026#34;\r\u0026#34;ONEDRIVECONSUMER\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive\u0026#34;\r\u0026#34;OPENBLAS_MAIN_FREE\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;\r\u0026#34;OPENBLAS_NUM_THREADS\u0026#34; =\u0026gt; \u0026#34;8\u0026#34;\r⋮ =\u0026gt; ⋮\rjulia\u0026gt; Base.ENV[\u0026#34;JULIA_NUM_THREADS\u0026#34;]\r\u0026#34;16\u0026#34; 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/base/#Base.ENV\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2499,"permalink":"https://freshrimpsushi.github.io/jp/posts/2499/","tags":null,"title":"ジュリアで環境変数を参照する方法"},{"categories":"머신러닝","contents":"概要1 2 勾配降下法におけるモーメンタム技術は、パラメーターを更新する際に以前の勾配もすべて使用することである。これが本質であり、これに尽きる。しかし、奇妙な更新式や物理学の運動量が動機となったとか、質量を$1$に設定し初期速度を$0$にするといった説明は理解を難しくするだけである。本稿では、モーメンタム技術をできるだけシンプルに説明する。\nビルドアップ パラメーターを$\\boldsymbol{\\theta}$、損失関数を$L$とするとき、標準的な勾配降下法は、以下のように反復的にパラメーターを更新する方法である。\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} $$\nここで$L_{i} = L(\\boldsymbol{\\theta}_{i})$は、$i$番目の反復で計算された損失関数を意味する。モーメンタム技術とは、これに単に前の反復で計算された損失関数の勾配$\\nabla L_{i-1}$を加えることに過ぎない。\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\alpha \\nabla L_{i-1} - \\cdots - \\alpha \\nabla L_{0} $$\nここで、反復が進むにつれて勾配の影響を減らし、勾配の合計が発散するのを防ぐために係数$\\beta \\in (0,1)$を追加すると、次のようになる。\n$$ \\begin{align} \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\beta \\alpha \\nabla L_{i-1} - \\cdots - \\beta^{i}\\alpha \\nabla L_{0} \\nonumber \\\\ \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha\\sum_{j=0}^{i} \\beta^{j} \\nabla L_{i-j} \\end{align} $$\n定義 $(1)$のようにパラメーターを更新することをモーメンタム技術momentum methodと呼び、追加される項$\\alpha\\sum\\limits_{j=0}^{i} \\beta^{j} \\nabla L_{i-j}$をモーメンタムmomentumと呼ぶ。\n説明 上記の定義によれば、モーメンタム技術は一般化された勾配降下法であり、むしろ勾配降下法はモーメンタム技術の$\\beta = 0$の特別なケースに過ぎないと見ることができる。$\\beta$が$1$に近いほど以前の勾配を多く反映し、$0$に近いほど少なく反映する。\n勾配降下法は、現在の傾きが最も大きい方向へパラメーターを更新するために貪欲アルゴリズムである。モーメンタム技術は勾配降下法の貪欲な部分を少し和らげ、現在最善の選択ではないが長期的にはより有効な選択をすることができるようにする。また、勾配の方向が急激に変わるのを防ぐことができる。\n当然ながら、パラメーターを更新する際の勾配の大きさが勾配降下法より大きいため、収束速度が速いという利点がある。また、経験的に局所的最小値local minimaから比較的脱出しやすいことが知られており、坂を転がり下るボールが十分な速さであれば、下り坂の途中にある小さな坂も越えて通り過ぎることができると説明される。\nここで重要な事実は、適応的学習率技術を含むこれらのオプティマイザー間に絶対的な優位性はないということである。分野や作業によって最適なオプティマイザーが異なるため、「何が最も良いか」という判断や質問は適切ではない。自分が所属する分野で主に使用されているものが何かを知ることが役立ち、それがないか分からなければSGD+モーメンタムまたはAdamを使用するのが無難である。\nネステロフのモーメンタム モーメンタム技術を再検討すると、次のパラメーター$\\boldsymbol{\\theta}_{i+1}$を得るために、現在のパラメーター$\\boldsymbol{\\theta}_{i}$に現在のパラメーターで計算された勾配$\\alpha \\nabla L(\\boldsymbol{\\theta}_{i})$を蓄積しながら加えていく。\nネステロフのモーメンタムNesterov momentumまたはネステロフ加速勾配Nesterov accelerated gradient, NAGと呼ばれる技術は、「現在のパラメーターに前の勾配を加えた値」で勾配を求め、これを現在のパラメーターに加えて次のパラメーターを求める。言葉ではやや複雑だが、モーメンタム技術を理解していれば、以下のアルゴリズムを見ることでネステロフのモーメンタムを理解するのが簡単になるかもしれない。\nアルゴリズム モーメンタム項を$\\mathbf{p}$と表す。\nアルゴリズム: モーメンタム技術 入力 学習率 $\\alpha$, モーメンタムパラメーター $\\beta$, エポック $N$ 1. パラメーター $\\boldsymbol{\\theta}$を任意の値で初期化する。 2. モーメンタムを $\\mathbf{p} = \\mathbf{0}$で初期化する。 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算しモーメンタムを更新 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # パラメーターを更新 6. end for アルゴリズム: ネステロフのモーメンタム 入力 学習率 $\\alpha$, モーメンタムパラメーター $\\beta$, エポック $N$ 1. パラメーター $\\boldsymbol{\\theta}$を任意の値で初期化する。 2. モーメンタムを $\\mathbf{p} = \\mathbf{0}$で初期化する。 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta} + \\beta \\mathbf{p})$ # 勾配を計算しモーメンタムを更新 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # パラメーターを更新 6. end for 両方の方法について初めの数回の計算を見ると、以下のようになる。簡単に$\\mathbf{p}_i = \\alpha \\nabla L_i$、および$\\mathbf{p}^i = \\alpha \\nabla L(\\boldsymbol{\\theta}_i - \\beta^{1}\\mathbf{p}^{i-1} - \\beta^{2}\\mathbf{p}^{i-2} - \\cdots - \\beta^{i}\\mathbf{p}^{0})$（このとき$\\mathbf{p}^{0} = \\mathbf{p}_0$）と表記すると、\nモーメンタム ネステロフのモーメンタム $\\boldsymbol{\\theta}\\_{0} =$ 初期値\r$\\boldsymbol{\\theta}\\_{0} =$ 初期値\r$\\boldsymbol{\\theta}_{1} = \\boldsymbol{\\theta}_{0} - \\alpha \\nabla L_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{0} - \\mathbf{p}_{0}$\r$\\boldsymbol{\\theta}_{1} = \\boldsymbol{\\theta}_{0} - \\alpha \\nabla L_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{0} - \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}_{2} = \\boldsymbol{\\theta}_{1} - \\alpha\\nabla L_{1} - \\beta \\mathbf{p}_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{1} - \\mathbf{p}_{1} - \\beta \\mathbf{p}_{0}$\r$\\boldsymbol{\\theta}_{2} = \\boldsymbol{\\theta}_{1} - \\alpha \\nabla L(\\boldsymbol{\\theta}_{1} - \\beta \\mathbf{p}^{0}) - \\beta \\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{1} - \\mathbf{p}^{1} - \\beta \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}_{3} = \\boldsymbol{\\theta}_{2} - \\mathbf{p}_{2} - \\beta \\mathbf{p}_{1} - \\beta^{2} \\mathbf{p}_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{2} - \\sum\\limits_{j=0}^{2}\\beta^{j}\\mathbf{p}_{2-j}$\r$\\boldsymbol{\\theta}_{3} = \\boldsymbol{\\theta}_{2} - \\alpha \\nabla L(\\boldsymbol{\\theta}_{2} - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0}) - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{2} - \\mathbf{p}^{2} - \\beta \\mathbf{p}^{1} - \\beta^{2} \\mathbf{p}^{0}$\r$$\\vdots$$\r$$\\vdots$$\r$\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\sum\\limits_{j=0}^{i}\\beta^{j}\\mathbf{p}_{i-j}$\r$\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\sum\\limits_{j=0}^{i}\\beta^{j}\\mathbf{p}^{i-j}$\rイアン・グッドフェロー, ディープラーニング, 第8.3.2節 モーメンタム, 第8.3.3節 ネステロフのモーメンタム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nオ・イルソク, 機械学習(MACHINE LEARNING) (2017), 第5.3節 モーメンタム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3528,"permalink":"https://freshrimpsushi.github.io/jp/posts/3528/","tags":null,"title":"勾配降下における運動量法"},{"categories":"줄리아","contents":"概要 ジュリアでもプログラムの進行状況を知らせてくれるグラスバーを手軽に使うことができる。\nコード ProgressMeter.jl 「ProgressMeter.jl」パッケージの「@showprogress」マクロを「for」ループに置けばよい1。\nusing ProgressMeter\rchi2 = []\r@showprogress for n in 1:20000\rpush!(chi2, sum(randn(n) .^ 2))\rend 下の「ProgressBars.jl」に比べるとマクロを使うのでコードがより簡潔である。\nProgressBars.jl 「ProgressBars.jl」パッケージの「ProgressBar()」関数で「for」ループの反復子Iteratorを包めば良い2。\nusing ProgressBars\rchi2 = []\rfor n in ProgressBar(1:20000)\rpush!(chi2, sum(randn(n) .^ 2))\rend 実際の作業内容はどうであれ構わないが、プログラムの進行状況は次のようにきれいに出力される。 当然だが、「for」ループ文で正確に何回目の繰り返しになっているかだけ分かるので、1回の繰り返し当たりの平均遂行時間を知らせるだけで、正確な所要時間を予測することはできない。\n環境 OS: Windows julia: v1.7.3 https://github.com/timholy/ProgressMeter.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/cloud-oak/ProgressBars.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2497,"permalink":"https://freshrimpsushi.github.io/jp/posts/2497/","tags":null,"title":"ジュリアでプログレスバーの使い方"},{"categories":"줄리아","contents":"概要 R言語の合計sum()や平均mean()には、関数自体がその欠損値を無視するオプションを持っているが、ジュリアではそのようなオプションがない代わりに関数型プログラミングFunctional Programming的な方法を積極的に使っている。\nコード julia\u0026gt; data = [0,1,2,3,0]\r5-element Vector{Int64}:\r0\r1\r2\r3\r0\rjulia\u0026gt; sum(data) / length(data)\r1.2\rjulia\u0026gt; sum(data) / sum(!iszero, data)\r2.0 上は$0$まで含んで全ての標本数で割った1.2、下は!iszeroという関数を引数に与えて$0$でない値だけをカウントして得た標本数で割った2.0を得た。Rよりも強力だと言える部分は、関数自体のオプションに頼らずにisnan(), isinf(), ismissing()などの多数の例外処理関数を同じ方法で使用でき、カスタムが自由であることだ。\nパフォーマンスに大きな違いはないが、is~系の関数のリターンが必ずブーリアンであるなら、分母のsum()がcount()に変わっても問題ない。\n環境 OS: Windows julia: v1.7.0 ","id":2495,"permalink":"https://freshrimpsushi.github.io/jp/posts/2495/","tags":null,"title":"ジュリアで0または欠損値を除外した平均値の計算方法"},{"categories":"전자기학","contents":"질문 電磁気学は文字通り電場 $\\mathbf{E}$と磁場 $\\mathbf{B}$について学ぶ学問です。電磁気学を学ぶ中で一度は次のような疑問を持ったことでしょう。\nなぜ磁場の記号として$\\mathbf{B}$を使用するのか？\n電場が$\\mathbf{E}$であるのはElectric fieldから来ているからとしても、磁場はMagnetic fieldなのになぜ$\\mathbf{B}$なのでしょうか？一見不自然に感じる記号ですが、これは実際に大きな理由なく定められたものです。\r回答 マックスウェルの記法1 2 マックスウェルはマックスウェル方程式を通じて古典電磁気学3を完成させ、「電磁気学の父」と呼ばれています。ニュートン、ライプニッツ、オイラーなど、数学/科学で顕著な成果を上げた人々は、名前と業績だけでなく、彼らの記法までもが後世に残ることになります。これはマックスウェルにも当てはまり、磁場を$\\mathbf{B}$、補助場を$\\mathbf{H}$と記すことも、マックスウェルがそう記したために自然と続いていると考えられています。\nマックスウェルが電磁気学で登場する様々なベクトルの記号に使用した文字4は以下の通りです。5 マックスウェルはこれらのベクトルをAからJまでのアルファベットで表記しましたが、C, D, Fなど記号に相応しいものがある場合はそれを用い、残りはマックスウェルの裁量で定められたようです。\n記法\r意味\rマックスウェル\r現在\r$\\frak{A}$$(A)$\r$\\mathbf{A}$\rその点の電磁気モーメンタム\r現在ではベクトルポテンシャルと呼ばれている。\r$\\frak{B}$$(B)$\r$\\mathbf{B}$\r磁気誘導\r現在では磁場と呼ばれている。\r$\\frak{C}$$(C)$\r$I$\r（全）電流、電流\r$\\frak{D}$$(D)$\r$\\mathbf{D}$\r電気'D'isplacement、変位場\r$\\frak{E}$$(E)$\r$\\mathcal{E}$\r電気'E'motive intensity\r現在では起電力electromotive force, emfと言われている。\r$\\frak{F}$$(F)$\r$\\mathbf{F}$\r機械的'F'orce\r現在ではローレンツ力と呼ばれている。\r$\\frak{G}$$(G)$\r点の速度\r$\\frak{H}$$(H)$\r$\\mathbf{H}$\r磁気力\r現在ではH-フィールドH-field、補助場auxiliary field、磁場強度magnetic field intensityなどと呼ばれている。\n$\\frak{I}$$(I)$\r$\\mathbf{M}$\r磁化'I'ntensity\r現在では磁化密度と呼ばれる物理量のようだ。\r$\\frak{J}$$(J)$\r$\\mathbf{J}$\r導電性の電流、導電電流\rほとんどの記号は今もそのまま使用されており、電流は現在、currentのintensityの頭文字を取って$I$と表記されています。\nまた、この関連で検索すると'ビオ・サバールの法則ではBiotの名前から取った'という主張も見つかりますが、私の意見ではそうではありません。まず'磁場の記号はなぜ$\\mathbf{B}$なのか?'という質問は'現在、磁場の記号としてなぜ$\\mathbf{B}$を使用するのか?'という質問と同じであり、これに対する答えは'マックスウェルがそう使用したから'が妥当だと思います。それならば、'マックスウェルが磁場の記号として$\\mathbf{B}$を使用したのはBiotの名前から取ったからではないか?'と考えることもできます。しかし、この回答には明確な根拠があるわけではないようです。もしそうだとしてもBiotの名前から$\\mathbf{B}$であるというよりは、'上記のベクトルの中で記号$\\mathbf{B}$と最も適合するのは磁気誘導、つまりビオ・サバールの法則と関連があるものだ'という説明の方が適切ではないでしょうか？（正直、Biot、bi-polar field、borealから取ったというのは無理矢理感があると思います）\nBとHのうち磁場はどちら？ 一方で$\\mathbf{B}$と$\\mathbf{H}$のうち、どちらを磁場magnetic fieldと呼ぶべきかについての議論もあります。$\\mathbf{H}$は媒質に関係なく実験で制御可能な値です。そのため、一般に工学関連の分野（例えば電気技師の教科書）では$\\mathbf{H}$を磁場と呼び、物理学関連の分野では$\\mathbf{B}$を磁場と呼ぶことが一般的です。しかし、ウィキの磁場の記事でも説明されているように、ローレンツ力を媒介するのが$\\mathbf{B}$であるため、$\\mathbf{E}$を電場と呼ぶのと同様に$\\mathbf{B}$を磁場と呼ぶのが一貫性があり、妥当だと考えられます。\n$$ \\text{Lorentz force}: \\mathbf{F}=Q\\left[ \\mathbf{E} + (\\mathbf{v}\\times\\mathbf{B}) \\right] $$\nhttps://www.johndcook.com/blog/2012/02/12/why-magnetic-field-b/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.cantorsparadise.com/why-the-symbol-for-magnetic-field-is-b-e40658e17ece\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n量子力学的現象を考慮しない\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nフラクトゥールFraktur書体である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMaxwell, James Clerk. A treatise on electricity and magnetism. Vol. 2. Oxford: Clarendon Press, 1873. page 257\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3523,"permalink":"https://freshrimpsushi.github.io/jp/posts/3523/","tags":null,"title":"磁場の記号にBを使う理由"},{"categories":"줄리아","contents":"概要 Juliaの回帰分析を行うためのGLM.jlパッケージを簡単に紹介する1。この説明では、Rのインターフェースとどれくらい似ているかを強調するため、詳細な説明は省略する。\nコード ジュリア using GLM, RDatasets\rfaithful = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;faithful\u0026#34;)\rout1 = lm(@formula(Waiting ~ Eruptions), faithful) 上記のコードを実行した結果は以下の通りである。\njulia\u0026gt; out1 = lm(@formula(Waiting ~ Eruptions), faithful)\rStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\rWaiting ~ 1 + Eruptions\rCoefficients:\r───────────────────────────────────────────────────────────────────────\rCoef. Std. Error t Pr(\u0026gt;|t|) Lower 95% Upper 95%\r───────────────────────────────────────────────────────────────────────\r(Intercept) 33.4744 1.15487 28.99 \u0026lt;1e-84 31.2007 35.7481\rEruptions 10.7296 0.314753 34.09 \u0026lt;1e-99 10.11 11.3493\r─────────────────────────────────────────────────────────────────────── Rでの回帰分析の結果と比較してみてください。\nRとの比較 out1\u0026lt;-lm(waiting~eruptions,data=faithful); summary(out1) out1 = lm(@formula(Waiting ~ Eruptions), faithful) 上はRのコードで、下はJuliaのコードである。変数を入力するために@formulaマクロを使用し、Rの慣習をほぼ完璧に再現できていることがわかる。\n環境 OS: Windows julia: v1.7.0 GLM v1.8.0 一緒に見る Rで回帰分析を行う方法 https://juliastats.org/GLM.jl/v0.11/index.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2493,"permalink":"https://freshrimpsushi.github.io/jp/posts/2493/","tags":null,"title":"ジュリアで回帰分析を行う方法"},{"categories":"머신러닝","contents":"概要 モンテカルロ積分は、与えられた関数の積分を計算するのが困難な場合に使用される数値的近似方法の一つである。次のような状況を想定しよう。与えられた $[0, 1]$または一般的に $[0, 1]^{n}$で積分可能な関数 $f$に対して、私たちは $f(x)$の式を知っているが、その積分を計算するのは簡単ではない。しかし、私たちは $f$の積分 $I[f]$を計算したい。\n$$ \\begin{equation} I[f] = \\int_{[0,1]} f(x) dx \\end{equation} $$\n定義 モンテカルロ積分Monte Carlo integrationとは、与えられた $[0, 1]$ 上での分布に基づきサンプル $\\left\\{ x_{i} \\right\\}$を抽出し、$f$の積分を次のように推定estimateする方法である。\n$$ I[f] \\approx I_{n}[f] := \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\n区分求積法との違い 区分求積法のアイデアは、区間 $[0,1]$を $n$等分し、点 $\\left\\{ x_{i} = \\frac{i-1}{n} \\right\\}_{i=1}^{n}$を得て、これらの点での関数値を全て加算することである。\n$$ \\text{区分求積法}[f] = \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\n式の見た目だけではモンテカルロ積分と区分求積法は異なるもののないように見えるが、その意味は全く異なる。区分求積法での $\\left\\{ x_{i} \\right\\}$は区間 $[0, 1]$を $n$等分して得た点であるのに対し、モンテカルロ積分では $x$が従う分布 $p(x)$から抽出された $n$個のサンプルを意味する。したがって、区分求積法で得られた値は単純に $f$が描くグラフの下の面積を意味するが、モンテカルロ積分で得られた値は $f$の期待値である。\n性質 式 $(1)$が持つ統計的な意味は「$I[f]$は $X$が一様分布に従うときの $f(X)$の期待値と同じである」ということである。\n$$ X \\sim U(0,1) \\implies I[f] = \\int_{[0,1]} f(x) dx = E\\left[ f(X) \\right] $$\n期待値 確率変数 $X$が一様分布に従うとしよう。$I_{n}[f]$は $I[f]$の不偏推定量である。\n$$ E\\left[ I_{n}[f] \\right] = I[f] $$\n証明 $$ \\begin{align*} E\\left[ I_{n}[f] \\right] \u0026amp;= E\\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} E\\left[ f(X_{i}) \\right] \\qquad \\text{by linearity of $E$} \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} I\\left[ f \\right] \\\\ \u0026amp;= I\\left[ f \\right] \\end{align*} $$\n■\n分散 証明 分散の性質\n[a] $\\Var (aX) = a^{2} \\Var (X)$\n[b] $X, Y$が独立ならば、$\\Var (X + Y) = \\Var(X) + \\Var(Y)$\n$f(X)$の分散を $\\sigma^{2}$としよう。すると分散の性質により、\n$$ \\begin{align*} \\Var \\left[ I_{n}[f] \\right] \u0026amp;= \\Var \\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\Var \\left[ \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\Var \\left[ f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\sigma^{2} \\\\ \u0026amp;= \\dfrac{\\sigma^{2}}{n} \\end{align*} $$\n■\n一般化 ここで $p(x) \\ge 0$で $\\int_{[0,1]} p = 1$となる関数 $p$について、積分 $I[fp]$を考えよう。\n$$ I[fp] = \\int_{[0, 1]}f(x)p(x) dx $$\nこれは確率密度関数が $p$である確率変数 $X$について、$f(X)$の期待値と同じである。この値を近似する方法として、次の二つの方法が考えられる。\nサンプル $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$を一様分布から抽出し、$I[fp]$を次のように近似する。 $$ X_{i} \\sim U(0,1) \\qquad I[fp] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i})p(x_{i}) $$\nサンプル $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$を $p(x)$から抽出し、$I[fp]$を次のように近似する。 $$ X_{i} \\sim p(x) \\qquad I[fp] = I_{p}[f] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i}) $$\n言い換えれば、1.は $f(x)p(x)$を一様分布でサンプリングして平均を求めたものであり、2.は $f(x)$を $p(x)$でサンプリングして平均を求めたものである。これらのうち分散がより小さいのは1.である。$I = I[fp] = I[fp]$と簡単に記しよう。\n1.の場合 $$ \\begin{align*} \\sigma_{1}^{2} = \\Var [fp] \u0026amp;= E \\left[ (fp - I)^{2} \\right] \\\\ \u0026amp;= \\int (fp - I)^{2} dx \\\\ \u0026amp;= \\int (fp)^{2} dx - 2I\\int fp dx + I^{2}\\int dx\\\\ \u0026amp;= \\int (fp)^{2} dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int (fp)^{2} dx - I^{2}\\\\ \\end{align*} $$\n2.の場合 $$ \\begin{align*} \\sigma_{2}^{2} = \\Var [f] \u0026amp;= E_{p} \\left[ (f - I)^{2} \\right] \\\\ \u0026amp;= \\int (f - I)^{2}p dx \\\\ \u0026amp;= \\int f^{2}p dx - 2I\\int fp dx + I^{2}\\int pdx\\\\ \u0026amp;= \\int f^{2}p dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int f^{2}p dx - I^{2}\\\\ \\end{align*} $$\nしかし $0 \\le p \\le 1$であるため、$f^{2}p \\ge f^{2}p^{2}$である。したがって\n$$ \\sigma_{1}^{2} \\le \\sigma_{2}^{2} $$\n","id":3515,"permalink":"https://freshrimpsushi.github.io/jp/posts/3515/","tags":null,"title":"モンテカルロ積分"},{"categories":"줄리아","contents":"コード Plots.jlは基本的にグリッド、目盛り、軸、カラーバーなどを全て出力するけど、これらをなくしてすっきりと描きたい場合は、次のオプションを追加すればいい。\ncolorbar=:none：カラーバーを消す。 showaxis = false：軸と目盛りを消す。 grid=false：背景のグリッドを消す。 ticks=false：背景のグリッドと目盛りを消す。 framestyle=:none：背景のグリッドと軸を消す。 using Plots\rsurface(L, title=\u0026#34;default\u0026#34;)\rsurface(L, title=\u0026#34;colorbar=:none\u0026#34;, colorbar=:none)\rsurface(L, title=\u0026#34;showaxis=false\u0026#34;, showaxis=false)\rsurface(L, title=\u0026#34;grid=false\u0026#34;, grid=false)\rsurface(L, title=\u0026#34;ticks=false\u0026#34;, ticks=false)\rsurface(L, title=\u0026#34;framestyle=:none\u0026#34;, framestyle=:none)\rsurface(L, title=\u0026#34;all off\u0026#34;, ticks=false, framestyle=:none, colorbar=:none) 環境 OS: Windows11 Version: Julia v1.8.3, Plots v1.38.6 ","id":3501,"permalink":"https://freshrimpsushi.github.io/jp/posts/3501/","tags":null,"title":"ジュリアで軸、目盛りなどをすべて無くしてきれいに出力する方法"},{"categories":"줄리아","contents":"概要 PythonやMATLABで使うmeshgrid()のような直接的な関数はない。グリッド上での関数値だけを求めたいなら、格子を作らないもっと簡単な方法がある。\nコード 2次元 列ベクトルと行ベクトルを掛けるのは、列ベクトルと行ベクトルのクロネッカー積を取るのと同じ結果を出す。\nU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\r# Fig. 1\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru1 = U.(T,X)\rheatmap(t\u0026#39;, x, u1, title=\u0026#34;Fig 1\u0026#34;) クロネッカー積を使えば、\nusing LinearAlgebra\rX = kron(x, ones(size(t)))\rT = kron(ones(size(x)), t)\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, title=\u0026#34;Fig 2\u0026#34;)\rjulia\u0026gt; u1 == u2\rtrue 3次元1 U(x,y,t) = exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\ry = LinRange(-1., 1, 100)\rt = LinRange(0.,0.35, 50)\rX = getindex.(Iterators.product(x, y, t), 1)\rY = getindex.(Iterators.product(x, y, t), 2)\rT = getindex.(Iterators.product(x, y, t), 3)\ru3 = U.(X,Y,T)\ranim = @animate for i ∈ 1:50\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1), title=\u0026#34;Anim. 1\u0026#34;)\rend 完全なコード using Plots\rcd = @__DIR__\rU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\r# Fig. 1\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru1 = U.(T,X)\rheatmap(t\u0026#39;, x, u1, title=\u0026#34;Fig 1\u0026#34;)\rsavefig(cd*\u0026#34;/fig1.png\u0026#34;)\r# kron\rusing LinearAlgebra\rX = kron(x, ones(size(t)))\rT = kron(ones(size(x)), t)\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, title=\u0026#34;Fig 2\u0026#34;)\rsavefig(cd*\u0026#34;/fig2.png\u0026#34;)\ru1 == u2\r# 3d\rU(x,y,t) = (1/4) * exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\rx = LinRange(-2., 2, 100)\ry = LinRange(-2., 2, 100)\rt = LinRange(0.,0.35, 50)\rX = getindex.(Iterators.product(x, y, t), 1)\rY = getindex.(Iterators.product(x, y, t), 2)\rT = getindex.(Iterators.product(x, y, t), 3)\ru3 = U.(X,Y,T)\ranim = @animate for i ∈ 1:50\rsurface(u3[:,:,i], zlims=(0,0.5), clim=(0,0.3), title=\u0026#34;Anim. 1\u0026#34;)\rend\rgif(anim, cd*\u0026#34;/anim1.gif\u0026#34;, fps=10) 環境 OS: Windows11 バージョン: Julia v1.8.3、Plots v1.38.6 https://discourse.julialang.org/t/meshgrid-function-in-julia/48679/26\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3500,"permalink":"https://freshrimpsushi.github.io/jp/posts/3500/","tags":null,"title":"ジュリアでメッシュグリッドを作成する方法"},{"categories":"줄리아","contents":"概要 Juliaで多変数関数をブロードキャストする方法を紹介する。Pythonなどで行うように、meshgridを作成する方法もあるし、各次元ごとにベクトルを作成して簡単に計算することもできる。\n2変数関数 $$ u(t,x) = \\sin(\\pi x) e^{-\\pi^{2}t} $$\n上のような関数を$(t,x) \\in [0, 0.35] \\times [-1,1]$でプロットしたい場合、次のように関数値を計算できる。\nx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\ru1 = @. sin(π*x)*exp(- π^2 * t)\rheatmap(t\u0026#39;, x, u1, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 1\u0026#34;) 関数自体を定義し、次のように2次元グリッドを作成して同じ結果を得ることができる。\nU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 2\u0026#34;) 3変数関数 $$ u(x,y,t) = e^{-x^{2} - 2y^{2}}e^{-\\pi^{2}t} $$\n時空間ドメイン$(x,y,t) \\in [-1,1] \\times [-1,1] \\times [0, 0.35]$で$u$の関数値を得たい場合、各変数の次元にのみサイズがあるようにベクトルを作成してブロードキャストすればいい。\n3次元メッシュを作成してブロードキャストしたい場合は、ここを参照。\njulia\u0026gt; x = reshape(LinRange(-1., 1, 100), (100,1,1))\r100×1×1 reshape(::LinRange{Float64, Int64}, 100, 1, 1) with eltype Float64:\rjulia\u0026gt; y = reshape(LinRange(-1., 1, 100), (1,100,1))\r1×100×1 reshape(::LinRange{Float64, Int64}, 1, 100, 1) with eltype Float64:\rjulia\u0026gt; t = reshape(LinRange(0.,0.35, 200), (1,1,200))\r1×1×200 reshape(::LinRange{Float64, Int64}, 1, 1, 200) with eltype Float64:\rjulia\u0026gt; u3 = @. exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\r100×100×200 Array{Float64, 3}:\ranim = @animate for i ∈ 1:200\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1))\rend コード詳細 using Plots\rcd = @__DIR__\r# Fig. 1\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\ru1 = @. sin(π*x)*exp(- π^2 * t)\rheatmap(t\u0026#39;, x, u1, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 1\u0026#34;)\rsavefig(cd*\u0026#34;/fig1.png\u0026#34;)\r# Fig. 2\rU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 2\u0026#34;)\rsavefig(cd*\u0026#34;/fig2.png\u0026#34;)\r# gif 1\rx = reshape(LinRange(-1., 1, 100), (100,1,1))\ry = reshape(LinRange(-1., 1, 100), (1,100,1))\rt = reshape(LinRange(0.,0.35, 200), (1,1,200))\ru3 = @. exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\ranim = @animate for i ∈ 1:200\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1), title=\u0026#34;Anim. 1\u0026#34;)\rend\rgif(anim, cd*\u0026#34;/anim1.gif\u0026#34;, fps=30) 環境 OS: Windows11 Version: Julia v1.8.3, Plots v1.38.6 ","id":3499,"permalink":"https://freshrimpsushi.github.io/jp/posts/3499/","tags":null,"title":"ジュリアにおける多変数関数のブロードキャス팅"},{"categories":"푸리에해석","contents":"概要1 離散フーリエ変換DFTは、数式的な定義に従って計算すると、$\\mathcal{O}(N^{2})$の時間計算量を持ちますが、以下で説明するアルゴリズムに従って計算すると、時間計算量が$\\mathcal{O}(N\\log_{2}N)$に低減します。この高速フーリエ変換を使用して離散フーリエ変換を高速に実行することができます。これは高速フーリエ変換fast Fourier transform, FFTと呼ばれています。\n構築 2つの数字を掛けて、それを別の数字に加える操作を$1$回の演算operationと呼びましょう。すると、$\\sum\\limits_{i=0}^{n-1}a_{n}b_{n}$の値を求めるには$n$回の演算が必要です。\n$$ \\begin{align*} \\sum\\limits_{n=0}^{0} a_{n}b_{b} \u0026amp;= a_{0}b_{0} = \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\\\ \\sum\\limits_{n=0}^{1} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} = \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\\\ \\sum\\limits_{n=0}^{2} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} + a_{2}b_{2} = \\overbrace{\\bigg( \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\bigg) {\\color{#FE9A2E} + } a_{2} {\\color{#FE9A2E} \\times} b_{2}}^{\\color{#FE9A2E}3 \\text{ operations}} \\\\ \\end{align*} $$\nさて、離散フーリエ変換の定義を思い出してみましょう。\n線型変換 $\\mathcal{F}_{N} : \\mathbb{C}^{N} \\to \\mathbb{C}^{N}$を離散フーリエ変換と呼びます。\n$$ \\mathcal{F}_{N}(\\mathbf{a}) = \\hat{\\mathbf{a}} = \\begin{bmatrix} \\hat{a}_{0} \\\\ \\hat{a}_{1} \\\\ \\dots \\\\ \\hat{a}_{N-1} \\end{bmatrix} ,\\quad \\hat{a}_{m} = \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n}\\quad (0\\le m \u0026lt; N) \\tag{1} $$\nこのとき、$\\mathbf{a} = \\begin{bmatrix} a_{0}\u0026amp; a_{1}\u0026amp; \\dots\u0026amp; a_{N-1} \\end{bmatrix}^{T}$です。\n$\\hat{a}_{m}$を計算するには$N$回の演算が必要で、$\\hat{\\mathbf{a}}$を計算するにはこれを$N$回繰り返す必要があるため、離散フーリエ変換を計算するには合計で$N^{2}$回の演算が必要です。つまり、$\\mathcal{O}(N^{2})$の時間計算量を持っています。これは、コンピュータ計算の観点からフーリエ変換がかなりのコストを要することを意味します。\nアルゴリズム データの長さ$N$を合成数$N = N_{1}N_{2}$としましょう。そして、インデックス$m, n$を次のように定義します。\n$$ m = m^{\\prime}N_{1} + m^{\\prime \\prime},\\quad n = n^{\\prime}N_{2} + n^{\\prime \\prime} $$\nすると、$0 \\le m^{\\prime}, n^{\\prime \\prime} \\le N_{2}-1$および$0 \\le m^{\\prime \\prime}, n^{\\prime} \\le N_{1}-1$です。$(1)$の指数部分を次のように表現できます。\n$$ \\begin{align*} e^{-i2\\pi mn /N} \u0026amp;= e^{-i2\\pi (m^{\\prime}N_{1} + m^{\\prime \\prime})(n^{\\prime}N_{2} + n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi (m^{\\prime}n^{\\prime}N_{1}N_{2} + m^{\\prime}n^{\\prime \\prime}N_{1} + m^{\\prime \\prime}n^{\\prime}N_{2} + m^{\\prime \\prime}n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi m^{\\prime}n^{\\prime}} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \u0026amp;= e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \\end{align*} $$\nこれを$(1)$に代入すると、\n$$ \\begin{align*} \\hat{a}_{m} \u0026amp;= \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi m^{\\prime \\prime}n^{\\prime}/N_{1}}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\right] e^{-i2\\pi [ (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) ] } \\end{align*} $$\n上記の式に従うと、各括弧内の$\\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} \\right]$を計算するのに$N_{1}$回の演算、括弧の外の$\\sum_{n^{\\prime \\prime}=0}^{N_{2}-1}$を計算するのに$N_{2}$回の演算が必要です。したがって、$\\hat{a}_{m}$を計算するには合計で$(N_{1} + N_{2})$回の演算が必要です。$\\hat{\\mathbf{a}}$を得るにはこれを$N$回繰り返す必要があるため、合計で$N(N_{1} + N_{2})$のコストがかかり、$N^{2}$よりも減少することが確認できます。\n括弧内を注意深く見ると、$N_{1}$が再び合成数の場合、同じロジックを適用できることがわかるでしょう。したがって、データの長さが$N = N_{1} N_{2} \\cdots N_{k}$といった合成数の場合、時間計算量は次のように減少します。\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}\\big( N(N_{1} + N_{2} + \\cdots + N_{k}) \\big) $$\nここで、$N$を$2$のべき乗$N = 2^{k}$と仮定してみましょう。すると、$\\log_{2}N = k$であり、$N^{2} = 2^{k}$から$2^{k}(2k)$だけ減少するため、時間計算量は次のように減少します。\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}(2N \\log_{2}N) $$\n補足 これは1965年にCooleyとTukey2によって提案されたため、Cooley-Tukeyアルゴリズムとも呼ばれています。ただし、彼らが最初に発明したわけではありません。ガウスも同様のアルゴリズムを研究しましたが、正しく発表しなかったため、この事実は後に明らかになりました3。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p252-253\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. W. Cooley and J. W. Tukey, An algorithm for the machine calculation of complex Fourier series, Mathematics of Computation 19 (1965), 297-301.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. T. Heideman, D. H. Johnson, and C. S. Burms, Gauss and the history of the fast Fourier transform, Archive for the History of the Exact Sciences 34 (1985), 264-277.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3492,"permalink":"https://freshrimpsushi.github.io/jp/posts/3492/","tags":null,"title":"高速フーリエ変換アルゴリズム"},{"categories":"줄리아","contents":"概要 これを実現するには、Dates モジュールの canonicalize() 関数を使用する1。\nコード using Dates\rtic = DateTime(2022,3,7,7,1,11)\rtoc = now()\rDates.canonicalize(toc-tic) 上のコードを実行した結果は次のとおりである。\njulia\u0026gt; using Dates\rjulia\u0026gt; tic = DateTime(2022,3,7,7,1,11)\r2022-03-07T07:01:11\rjulia\u0026gt; toc = now()\r2022-07-19T22:26:22.070\rjulia\u0026gt; Dates.canonicalize(toc-tic)\r19 weeks, 1 day, 15 hours, 25 minutes, 11 seconds, 70 milliseconds 小さい単位の倍数として正確に、週単位まで自動で計算して出力されることが確認できる。\n環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/get-difference-between-two-dates-in-seconds/11641/4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2461,"permalink":"https://freshrimpsushi.github.io/jp/posts/2461/","tags":null,"title":"ジュリアで2つの時刻の差を秒単位で計算する方法"},{"categories":"줄리아","contents":"要約 Plots.jlで軸と目盛りの色を指定する関連キーワードは以下の通りである。\nキーワード名 機能 guidefontcolor 軸名の色を指定 foreground_color_border, fgcolor_border 軸の色を指定 foreground_color_axis, fgcolor_axis 目盛りの色を指定 foreground_color_text, fgcolor_text 目盛りの値の色を指定 キーワード名の前にx_やy_を付けると、その軸にのみ適用される。\nコード1 軸名 軸名の色を指定するキーワードはguidefontcolorである。軸名は、xlabel, ylabelで指定できる。\nx = randn(10, 3)\rplot(plot(x, guidefontcolor = :red),\rplot(x, x_guidefontcolor = :red),\rplot(x, y_guidefontcolor = :red),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 軸 軸の色を指定するキーワードはforeground_color_borderである。\nplot(plot(x, foreground_color_border = :red),\rplot(x, x_foreground_color_border = :red),\rplot(x, y_foreground_color_border = :red),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 目盛り 目盛りの色を指定するキーワードはforeground_color_axisである。これをfalseにすると、目盛りだけを消すことができる。\nplot(plot(x, foreground_color_axis = :red),\rplot(x, x_foreground_color_axis = :red),\rplot(x, y_foreground_color_axis = false),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 目盛りの値 目盛りの値の色を指定するキーワードはforeground_color_textである。これをfalseにすると、目盛りの値だけを消すことができる。\nplot(plot(x, foreground_color_text = :red),\rplot(x, x_foreground_color_text = :red),\rplot(x, y_foreground_color_text = flase),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 環境 OS：Windows11 バージョン：Julia 1.9.4, Plots v1.39.0 関連項目 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理をするためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色を指定する方法 サブプロットごとにグラフの色を指定する方法 軸、軸名、目盛り、目盛りの値の色を指定する方法 背景色の指定方法 https://docs.juliaplots.org/stable/generated/attributes_axis/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3490,"permalink":"https://freshrimpsushi.github.io/jp/posts/3490/","tags":null,"title":"Julia Plotsで軸、軸名、目盛り、目盛り値の色を指定する方法"},{"categories":"머신러닝","contents":"概要 Flux、PyTorch、TensorFlowで同じ機能をするコードを整理します。\nJulia-MATLAB-Python-R チートシート Fluxについて次のような環境とします。\nusing Flux PyTorchについて次のような環境とします。\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F TensorFlowについて次のような環境とします。\nimport tensorflow as tf\rfrom tensorflow import keras 1次元テンソル 줄리아Julia\r파이토치PyTorch\r텐서플로우TensorFlow\r列ベクトルcolumn vector\r[1 4 -1 2] [1;4;-1;2] ","id":3489,"permalink":"https://freshrimpsushi.github.io/jp/posts/3489/","tags":null,"title":"Flux-PyTorch-TensorFlowチートシート"},{"categories":"줄리아","contents":"概要 Juliaで2次元配列と行列の間を切り替えるヒントを紹介する1。おそらくJulia 1.7以下の環境では、最もJuliaらしく、シンプルで、速く、美しい方法だろう。\nコード ここで紹介された方法だけではなく、行列と2次元配列の間を行き来する方法は数えきれないほどある。ただコードを書くだけでなく、目標そのものが難しくないから、Julia独特の構文がどのように使用されたのかも考えながら読む方がいい。\n行列から2次元配列へ julia\u0026gt; M = rand(0:9, 3, 10)\r3×10 Matrix{Int64}:\r2 4 0 1 8 0 9 2 5 7\r5 2 1 5 4 3 7 2 7 3\r7 8 1 9 0 3 2 4 1 3 上のような行列を2次元配列に変えてみよう。\njulia\u0026gt; [eachrow(M)...]\r3-element Vector{SubArray{Int64, 1, Matrix{Int64}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}}:\r[2, 4, 0, 1, 8, 0, 9, 2, 5, 7]\r[5, 2, 1, 5, 4, 3, 7, 2, 7, 3]\r[7, 8, 1, 9, 0, 3, 2, 4, 1, 3]\rjulia\u0026gt; [eachcol(M)...]\r10-element Vector{SubArray{Int64, 1, Matrix{Int64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}}:\r[2, 5, 7]\r[4, 2, 8]\r[0, 1, 1]\r[1, 5, 9]\r[8, 4, 0]\r[0, 3, 3]\r[9, 7, 2]\r[2, 2, 4]\r[5, 7, 1]\r[7, 3, 3] eachrow()とeachcol()は行列の行と列を一行ずつ抽出するジェネレータを返す2、そしてスプラットオペレータを通じてこれらを可変配列として扱い3、角括弧[]の中に入れることで、自然に配列になる。\n2次元配列から行列へ julia\u0026gt; A = [rand(0:9,3) for _ in 1:10]\r10-element Vector{Vector{Int64}}:\r[5, 4, 9]\r[9, 7, 6]\r[9, 9, 6]\r[5, 9, 0]\r[0, 2, 8]\r[3, 9, 5]\r[1, 6, 0]\r[5, 7, 7]\r[1, 3, 5]\r[5, 4, 1] 上のような2次元配列を行列にしてみよう。\njulia\u0026gt; hcat(A...)\r3×10 Matrix{Int64}:\r5 9 9 5 0 3 1 5 1 5\r4 7 9 9 2 9 6 7 3 4\r9 6 6 0 8 5 0 7 5 1\rjulia\u0026gt; hcat(A...)\u0026#39;\r10×3 adjoint(::Matrix{Int64}) with eltype Int64:\r5 4 9\r9 7 6\r9 9 6\r5 9 0\r0 2 8\r3 9 5\r1 6 0\r5 7 7\r1 3 5\r5 4 1\rjulia\u0026gt; vcat(A...)\r30-element Vector{Int64}:\r5\r4\r9\r9\r7\r6\r9\r9\r⋮\r7\r1\r3\r5\r5\r4\r1 配列を統合するhcat()関数を使えばいい。4 基本的にhcat()とvcat()はフォールド関数であり、可変引数関数であるため、2次元配列の要素である1次元配列を直接引数としてスプラットオペレータを通じて渡さなければならない。\n全体のコード # matrix to 2d array\rM = rand(0:9, 3, 10)\r[eachrow(M)...]\r[eachcol(M)...]\r# 2d array to matrix\rA = [rand(0:9,3) for _ in 1:10]\rhcat(A...)\rhcat(A...)\u0026#39;\rvcat(A...) 環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/plots-efficient-scatter-plot-of-a-2xn-array/31803/6\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/arrays/#Base.eachcol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/arrays/#Base.cat\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2459,"permalink":"https://freshrimpsushi.github.io/jp/posts/2459/","tags":null,"title":"ジュリアで2次元配列と行列の間の変換方法"},{"categories":"줄리아","contents":"概要 SMTPClient.jlパッケージを使って、SMTPSimple Mail Transfer Protocolでナヴァーからメールを送る方法を紹介する1。長時間かかるシミュレーションが終わったらカカオメールにレポートを送るようにして、研究のスピードを上げるのに使っている。\nこのようにジョルディが個人トークで知らせてくれるから、自分でサーバーを確認しなくても、シミュレーションがいつ終わるか分かる。\nコード どの言語で実装しても、まず最初に以下のようにナヴァーメールでSMTPを「使用する」に設定する必要がある。\nジュリア using Dates\rtic = now()\rfor t in 1:1000\rprintln(t)\rend\rtoc = now()\rusing SMTPClient\ropt = SendOptions(\risSSL = true,\rusername = \u0026#34;네이버아이디\u0026#34;,\rpasswd = \u0026#34;비밀번호\u0026#34;)\r#Provide the message body as RFC5322 within an IO\rbody = IOBuffer(\r\u0026#34;Date: ▷eq1◁tic\\r\\n\u0026#34; *\r\u0026#34;▷eq2◁(Dates.canonicalize(toc - tic))\u0026#34; *\r\u0026#34;\\r\\n\u0026#34;)\rurl = \u0026#34;smtps://smtp.naver.com:465\u0026#34;\rrcpt = [\u0026#34;\u0026lt;수신자@kakao.com\u0026gt;\u0026#34;]\rfrom = \u0026#34;\u0026lt;발신자@naver.com\u0026gt;\u0026#34;\rresp = send(url, rcpt, from, body, opt) 上の例では、最も重要な部分はurl = \u0026quot;smtps://smtp.naver.com:465\u0026quot;だ。ナヴァーでなくても、どのサーバーを使うにしても、ここを適切に変える必要がある。送信時刻の場合はDatesモジュールのnow()を使って、メールを送る時点に固定したが、これが実際の時計と合わないと、10分ほど遅れて送られる問題を経験した。\nパイソン ジュリアを試す前に、まずはパイソンで試したコード。不思議なことに、SSLを使ってポートを456にしてもうまくいかなかったが、SSLを切って587にしたらうまくいった。参考にしたブログ2ではグーグルを基準に説明していたが、以下のコードはナヴァーを基準にうまく動作することを確認した。\nimport smtplib\rfrom email.mime.text import MIMEText\rsendEmail = \u0026#34;발신자@naver.com\u0026#34;\rrecvEmail = \u0026#34;수신자@kakao.com\u0026#34;\rpassword = \u0026#34;비밀번호\u0026#34;\rsmtpName = \u0026#34;smtp.naver.com\u0026#34; #smtp 서버 주소\rsmtpPort = 587 #smtp 포트 번호\rtext = \u0026#34;매일 내용\u0026#34;\rmsg = MIMEText(text) #MIMEText(text , _charset = \u0026#34;utf8\u0026#34;)\rmsg[\u0026#39;Subject\u0026#39;] = \u0026#34;시뮬레이션 종료\u0026#34;\rmsg[\u0026#39;From\u0026#39;] = sendEmail\rmsg[\u0026#39;To\u0026#39;] = recvEmail\rprint(msg.as_string())\rs=smtplib.SMTP( smtpName , smtpPort ) #메일 서버 연결\rs.starttls() #TLS 보안 처리\rs.login( sendEmail , password ) #로그인\rs.sendmail( sendEmail, recvEmail, msg.as_string() ) #메일 전송, 문자열로 변환하여 보냅니다.\rs.close() #smtp 서버 연결을 종료합니다. 環境 OS: Windows julia: v1.7.0 https://github.com/aviks/SMTPClient.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gosmcom.tistory.com/72\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2457,"permalink":"https://freshrimpsushi.github.io/jp/posts/2457/","tags":null,"title":"ジュリアでネイバーからメールを送る方法"},{"categories":"위상데이터분석","contents":"概要 ZomorodianとCarlssonの論文「Computing Persistent Homology」で紹介されたアルゴリズムの擬似コードを説明し、実装する。1 抽象的な単体複体で作られたフィルター付き複体を受け取り、$\\mathcal{P}$-インターバルをリターンし、コンピュータで扱いにくい永続的なモジュールの構築を省略し、行列のリダクションによって永続的なホモロジーを計算する。さらに、実際の実装では行列演算さえも使用しない。\n導出 Zomorodianのアルゴリズムの導出: アルゴリズムの理論的な内容を完全に無視すると、擬似コードをいくら見てもアルゴリズムを理解することはできないだろう。完全に理解する程度でなくても、なぜ突然行列がなくなったのか、なぜマーキングのようなものが必要なのか、少なくともその程度は理解できるように勉強してから実装に取り組むべきである。 アルゴリズム アルゴリズムが実行される前にフィルター付き複体をデータとして受け取ったとする。\nデータを保存し、アルゴリズムから得られた情報を記録するためのディクショナリやテーブル$T$を上記のように作成する。例えばJuliaのデータフレームでは、データ内の数字をepsilon、アルファベットが記された部分をsimplexに転写し、マーキングの有無を保存するブーリアンカラムmarked、チェインコンプレックスのチェインを保存するslot、計算過程で出てくる整数を保存するカラムJを追加する。\nJuliaは配列が$0$ではなく$1$から始まる言語であり、実装の便宜上インデックスを気にしない部分があるため、上のスクリーンショットのように数字が$1~2$ずつすべて異なる場合があるが、これは全く重要ではないので気にしないでほしい。 注意すべきは、slotが集合でありながらもチェインとして両方の表現を自由に行き来することである。例えば、$\\mathbb{Z}_{2}$上での計算で $$ a + (a - b) = 2 a - b = b \\pmod{2} $$ のような計算が行われるが、これは次のような集合演算 $$ \\left\\{ a \\right\\} \\cup \\left\\{ a, -b \\right\\} = \\left\\{ b \\right\\} $$ とも同じことである。代数的な演算での要素$0$は集合では「ないもの」として扱われ、それをそのまま受け入れなければならない。厳密で慎重な表記を好む人には不快かもしれないが、そこまで無理なことではないので、そのまま受け入れよう。 また、元の論文では一般的なフィールド$F$上でアルゴリズムを導出しており、すなわちすべての$q \\in F$の逆元$q^{-1} \\in F$が存在して $$ d = d - q^{-1} T[i] $$ のような計算を行うが、この実装ではバイナリフィールド$\\mathbb{Z}_{2}$で十分なので、$q^{-1}$を別途計算せずに$A \\Delta B := \\left( A \\cup B \\right) \\setminus \\left( A \\cap B \\right)$と定義された$\\Delta$に対して次のように代替した。 $$ d = d \\Delta T[i] $$ $\\deg$という表現が（プログラムの）関数としても出てくるし、インデックスとしても出てくるし、多項関数の次数としても出てくるし、非常に頻繁に出てくるので、$T$ではdegではなくepsilonと表記した。実際、トポロジカルデータアナリシスの単純なレベルでは、通常このカラムの値である半径$\\varepsilon \u0026gt; 0$が大きくなるにつれてフィルター付き複体を構成することになるためである。 $T$はシンプレックスの次元に従って完全にソートされていると仮定し、それに従ってepsilonも部分順序を持つことを期待する。 擬似コード $\\left\\{ L_{k} \\right\\}$ = COMPUTEINTERVALS$(K)$\nInput: フィルター付き複体$K$を受け取る。フィルター付き複体には、少なくともどのタイミング$\\deg \\in \\mathbb{Z}$にどのシンプレックス$\\sigma$が追加されたかについての情報が必要である。 Output: $k = 0, \\cdots , \\dim K$に対する$\\mathcal{P}$-インターバルの集合$L_{k}$の集合$\\left\\{ L_{k} \\right\\}_{k=0}^{\\dim K}$を得る。 Side Effect: データが記録されたテーブル$T$のmarkedを変更する。 $d$ = REMOVEPIVOTROWS$(\\sigma)$\nInput: $k$次元のシンプレックス$\\sigma$を受け取る。 Output: $(k-1)$次元のチェイン、つまり$k$次元のシンプレックス同士を演算したある$\\mathsf{C}_{k-1}$の要素を得る。 $i$ = maxindex$d$\nInput: チェイン$d$を受け取る。 Output: テーブル$T$でチェイン$d$に含まれるすべてのsimplexの中で最大のインデックス$i$を返す。例えばmaxindex(abc)の場合はabの$5$、bcの$6$、acの$9$の中で最大の$9$を返す必要がある。 $k$ = dim$d$\nInput: $k$次元のチェイン$d$を受け取る。 Output: 整数$k$を返す。 $k$ = dim$\\sigma$\nInput: $k$次元のシンプレックス$\\sigma$を受け取る。 Output: 整数$k$を返す。 $k$ = deg$(\\sigma)$\nInput: シンプレックス$\\sigma$を受け取る。 Output: テーブル$T$でシンプレックス$\\sigma$に対応する整数epsilonを返す。例えばdeg(cd)の場合はcdのepsilonが$2$なので$2$を返す必要がある。 キーワード\nMarkはMark$\\sigma$の形で書かれ、該当するシンプレックス$\\sigma$のmarkedをtrueに変更する。 StoreはStore$j$ and $d$ in $T[i]$の形で書かれ、$T[i]$のJに整数$j$、slotにチェイン$d$を保存する。 RemoveはRemove$x$ in $d$の形で書かれ、チェイン$d$にある$x$項を削除する。 $\\sigma^{i}$はテーブル$T$で$i$番目にあるsimplexであり、$m$は$T$の長さである。\nfunction COMPUTEINTERVALS$(K)$\n# 初期化\nfor $k \\in 0:\\dim K$\n$L_{k} := \\emptyset$\nend for\nfor $j \\in 0:(m-1)$\n$d$ = REMOVEPIVOTROWS$\\left( \\sigma^{j} \\right)$\nif $d = \\emptyset$\n# $d$が空であることは、（ピボットではない）ゼロ列の候補である\nmark $\\sigma^{j}$\nelse\n# $d$の次元を計算する必要があるため、すべての項のmaxでなければならない\n# $d$は$\\sigma^{j}$より一次元低いチェインであり、$i \u0026lt; j$しかあり得ない\n$i$ = maxindex$d$\n$k$ = dim$d$\n$L_{k}$ = $L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\deg \\sigma^{j} \\right) \\right\\}$\nend if\nend for\nfor $j \\in 0:(m-1)$\n# まだマークされていない場合は、明らかにゼロ列である\nif $\\sigma^{j}$ ismarked and $T[j]$ isempty\n$k$ = dim$d$\n# $H_{k-1}$から$\\sum^{\\hat{e}_{i}} F[t]$に該当、無限大処理\n$L_{k}$ = $L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\infty \\right) \\right\\}$\nend if\nend for\nreturn $\\left\\{ L_{k} \\right\\}$\nend function\nfunction REMOVEPIVOTROWS$(\\sigma)$\n$k$ = $\\dim \\sigma$\n# $\\partial abc = ab - bc + ca$とすると∂(\u0026quot;abc\u0026quot;) = [\u0026quot;ab\u0026quot;, \u0026quot;bc\u0026quot;, \u0026quot;ca\u0026quot;]\n$d$ = $\\partial_{k} \\sigma$\nRemove not marked $(k-1)$-dimensional simplex in $d$\nwhile $d \\ne \\emptyset$\n$i$ = maxindex$d$\nif $T[i]$ isempty\nbreak\nend if\n# $\\mathbb{Z}_{2}$なので、symdiff（対称差）で代用\n$d$ = $d \\Delta T[i]$\nend while\nreturn $d$\nend function\n実装 与えられた例でのアルゴリズムの実行結果は次のようになるべきである。 $$ \\begin{align*} L_{0} =\u0026amp; \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} \\\\ L_{1} =\u0026amp; \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} \\end{align*} $$\nJuliaで実装した結果は次のようになる。インデックスが正確に異なる部分を除けば、正しく実装されていることが確認できる。\n全体のコード 読めばわかるが、元の論文のノーテーションをほぼそのままにしてコードを書いた。例えば $$ L_{k} = L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\deg \\sigma^{j} \\right) \\right\\} $$ のJulia風のコードはpush!(L_[k], (deg(σⁱ), deg(σʲ)))であるが、論文とほぼ同じように見えるようにL_[k] = L_[k] ∪ [(deg(σⁱ), deg(σʲ))]として実装した。\nusing DataFrames data = DataFrame([ 0 \u0026#34;a\u0026#34; 0 \u0026#34;b\u0026#34; 1 \u0026#34;c\u0026#34; 1 \u0026#34;d\u0026#34; 1 \u0026#34;ab\u0026#34; 1 \u0026#34;bc\u0026#34; 2 \u0026#34;cd\u0026#34; 2 \u0026#34;ad\u0026#34; 3 \u0026#34;ac\u0026#34; 4 \u0026#34;abc\u0026#34; 5 \u0026#34;acd\u0026#34; ], [\u0026#34;epsilon\u0026#34;, \u0026#34;simplex\u0026#34;]) T = copy(data) T[!, :\u0026#34;marked\u0026#34;] .= false T[!, :\u0026#34;slot\u0026#34;] .= [[]] T[!, :\u0026#34;J\u0026#34;] .= 0 dimK = 2 m = nrow(T) test_σ = \u0026#34;abc\u0026#34; dim(σ) = length(σ) function deg(σ) return T.epsilon[findfirst(T.simplex .== σ)] end deg(test_σ) function ∂(σ) k = dim(σ) return [σ[(1:k)[Not(t)]] for t = 1:k] end ∂(test_σ) function maxindex(chain) return (T.simplex .∈ Ref(chain)) |\u0026gt; findall |\u0026gt; maximum end maxindex(∂(test_σ)) function REMOVEPIVOTROWS(σ) k = dim(σ); d = ∂(σ) d = d[d .∈ Ref(T[T.marked,:simplex])] # Remove unmarked terms in ▷eq029◁ while !(d |\u0026gt; isempty) i = maxindex(d) if T[i,:slot] |\u0026gt; isempty break end d = symdiff(d, T[i,:slot]) # print(\u0026#34;d in empty\u0026#34;) end return d end REMOVEPIVOTROWS(test_σ) L_ = [[] for k = 0:dimK] for j0 = 0:(m-1) j = j0+1 σʲ = T[j,:simplex] d = REMOVEPIVOTROWS(σʲ) if d |\u0026gt; isempty T[j,:marked] = true else i = maxindex(d); k = dim(σʲ) σⁱ = T[i,:simplex] T[i,[:J,:slot]] = j0,d L_[k] = L_[k] ∪ [(deg(σⁱ), deg(σʲ))] end end for j0 = 0:(m-1) j = j0+1 σʲ = T[j,:simplex] if (T[j,:marked]) \u0026amp;\u0026amp; (T[j,:slot] |\u0026gt; isempty) \u0026amp;\u0026amp; (T[j,:J] |\u0026gt; iszero) k = dim(σʲ); L_[k] = L_[k] ∪ [(deg(σʲ), Inf)] print(\u0026#34;j: $j\u0026#34;) end end Zomorodian. (2005). Computing Persistent Homology: ch4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2449,"permalink":"https://freshrimpsushi.github.io/jp/posts/2449/","tags":null,"title":"ジョモロジアンのアルゴリズムの実装"},{"categories":"위상데이터분석","contents":"概要 ZomorodianとCarlssonの論文「Computing Persistent Homology」で紹介されたアルゴリズムの導出プロセスを説明する1。抽象的なシンプレクシャルコンプレックスで作られたフィルタードコンプレックスを受け取り、$\\mathcal{P}$-インターバルを返す。計算機で扱いにくいパーシステントモジュールの構築を省略し、行列のリダクションにより持続的ホモロジーを計算する。\n導出 Part 0. 事前調査\nアルゴリズムの本格的な導出に先立ち、上の図で描写されるパーシステンスコンプレックスが数式的にどのような形式であるかをまず検討する。このプロセスをしっかりと固めておかないと、論文を読むのが非常に苦痛になるだろう。\nまず、下部にある数字を$\\deg$とし、これが$0$から$5$まで増加しながら次のようにフィルタードコンプレックスを形成する。 $$ \\left\\{ a,b \\right\\} = K_{0} \\subset K_{1} \\subset K_{2} \\subset K_{3} \\subset K_{4} \\subset \\left( K_{4} \\cup \\left\\{ acd \\right\\} \\right) = K_{5} $$ $\\deg$とは関係なく、$K$は$2$-シンプレックスとして、ホモロジーを考慮する文脈で次のようなチェインコンプレックスを形成する。 $$ \\mathsf{C}_{2} \\overset{\\partial_{2}}{\\longrightarrow} \\mathsf{C}_{1} \\overset{\\partial_{1}}{\\longrightarrow} \\mathsf{C}_{0} $$ アルゴリズムの目標は、このような$\\partial_{2}$と$\\partial_{1}$がデータに与える代数的トポロジカル情報、たとえばベッチ数$\\beta_{k}$などが、どの$\\deg$で現れていつ$\\deg$で消えるかを次のように計算することである。 $$ \\begin{align*} L_{0} =\u0026amp; \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} \\\\ L_{1} =\u0026amp; \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} \\end{align*} $$\n$L_{0}$は$\\beta_{0}$に該当する情報、つまりコンポーネントがいつ現れて消えるかを示す$\\mathcal{P}$-インターバルで構成され、$L_{1}$は$\\beta_{1}$に該当する情報、つまり空間で「穴」と呼べるものがいつ現れて消える$\\mathcal{P}$-インターバルで構成されている。\nPart 1. $\\partial_{1}$\n論文では、著者たちはその計算が全てのフィールドで可能であると主張しているが、簡単にグレード付きモジュールである$\\mathbb{Z}_{2} [t]$-モジュールでどのような計算が行われるかを見てみよう。これから$\\mathsf{C}_{k}$の同次基底を$\\left\\{ e_{j} \\right\\}$、$\\mathsf{C}_{k-1}$の同次基底を$\\left\\{ \\hat{e}_{i} \\right\\}$と表記することにする。ここで同次とは、$\\mathsf{C}_{k}$をグレード付きモジュールと見たとき、項が一つしかないという意味で受け取ってもよく、つまり$t^{2} + t$のようではなく$t^{4}$のような単項形式であると考えても構わないということである。\n$$ \\deg M_{k} (i,j) = \\deg e_{j} - \\deg \\hat{e}_{i} $$ ホモロジー代数にある程度慣れていれば、今度は$\\partial_{k}$に対応する境界行列$M_{k}$を上のテーブルと方程式に合わせて構成し、そのスミス標準形$\\tilde{M}_{1}$を求めに行く感じがするだろう。まず$k=1$の場合を考えてみると、先に行列の基底が同次であると言ったので、次のように唯一の$M_{1}$を得ることができる。\nこのように基底を持って行列を構成することは、$\\partial_{k}$の一つの役割が$t^{n}$を掛けること（群作用を取ることでグレード付きモジュールで次数が上がること）の逆を行うことだと見ると理解できる。感覚を掴むために、直接計算してみよう。 $$ \\begin{align*} \\deg M_{1} (2,5) =\u0026amp; \\deg ac - \\deg c = 3 - 1 = 2 = \\deg t^{2} \\\\ \\deg M_{1} (4,5) =\u0026amp; \\deg ac - \\deg a = 3 - 0 = 3 = \\deg t^{3} \\\\ \\deg M_{1} (2,2) =\u0026amp; \\deg bc - \\deg c = 1 - 1 = 0 = \\deg t^{0} = \\deg 1 \\end{align*} $$\n先に言ったように、今度はこのエシュロン形、特にカラムエシュロン形を作\nると次のようになる。\n大学で学んだ線形代数を思い出してみると、各カラムで一番上にありながら$0$でない、図のように四角で囲んだ部分のようなものをピボットと呼んでいた。ここで次の2つの補助定理を紹介する。\n(1): カラムエシュロン形の対角成分はスミス標準形の対角成分と同じである。 (2): $\\tilde{M}_{k}$の$i$行のピボットが$\\tilde{M}_{k} (i,j) = t^{n}$であればホモロジーグループ$H_{k-1}$の$\\sum^{\\deg \\hat{e}_{i}} F[t] / t^{n}$に該当し、それ以外は$H_{k-1}$の$\\sum^{\\deg \\hat{e}_{i}} F[t]$に該当する。これは$L_{k-1}$が$\\left( \\deg \\hat{e}_{i} , \\deg \\hat{e}_{i} + n \\right)$と$\\left( \\deg \\hat{e}_{i} , \\infty \\right)$で構成されることと同値である。 つまり、\n補助定理(1)により、持続的ホモロジーを計算する際は行操作が必要なく、列操作だけで良いことになる。 補助定理(2)により、$L_{k-1}$は$\\left( \\deg \\hat{e}_{i} , \\deg \\hat{e}_{i} + n \\right)$と$\\left( \\deg \\hat{e}_{i} , \\infty \\right)$で構成される。 最初の行のピボットが$t^{1}$であり、$\\deg d = 1$であるため、$(1,1+1)$を得る。 2番目の行のピボットが$t^{0}$であり、$\\deg c = 1$であるため、$(1,1+0)$を得る。 3番目の行のピボットが$t^{1}$であり、$\\deg b = 0$であるため、$(0,0+1)$を得る。 4番目の行にピボットがなく、$\\deg a = 0$であるため、$(0,\\infty)$を得る。 これは、アルゴリズムを導出する前に言及した$L_{0}$と完全に一致する。 $$ L_{0} = \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} $$\nPart 2. $\\partial_{2}$\n$L_{1}$を得るための$\\partial_{2}$の行列形$M_{2}$は上のようである。しかし、次の補助定理で計算を減らし、簡単に進めることができる。\n(3): $\\mathsf{C}_{k+1}$の標準基底と$\\mathsf{Z}_{k}$に対する$\\partial_{k+1}$を表現するためには、$\\tilde{M}_{k}$に対応する行を$M_{k+1}$から単に削除しても良い。 言葉は少し難しく聞こえるが、現在の具体的な状況では、$\\tilde{M}_{1}$の$1$-シンプレックス$ab,bc,cd,ad,ac$の中で、$cd,bc,ab$のピボットだけが残っているので、これを単に$M_{2}$から削除しても良いということである。直感的に考えると、これらがすでに$k$次元で使用されたので、$k+1$では見る必要もないという程度に受け取っても構わない。こうしてカラムエシュロン形$\\tilde{M}_{2}$を直接構築する過程を省略し、その3つの行を削除してみると、次のように下が切り取られた$\\check{M}_{2}$を得る。\n$$ \\begin{align*} z_{2} =\u0026amp; ac - bc - ab \\\\ z_{1} =\u0026amp; ad - bc - cd - ab \\end{align*} $$\n再び補助定理(2)に従って計算してみよう。\n最初の行のピボットが$t^{1}$であり、 $$ \\deg z_{2} = \\deg \\left( ac - bc - ab \\right) = \\max \\deg \\left\\{ ac , bc , ab \\right\\} = 3 $$ であるため、$(3,3+1)$を得る。 2番目の行のピボットが$t^{3}$であり、 $$ \\deg z_{1} = \\deg \\left( ad - bc - cd - ab \\right) = \\max \\deg \\left\\{ ad , bc , cd , ab \\right\\} = 2 $$ であるため、$(2,2+3)$を得る。 これは、アルゴリズムを導出する前に言及した$L_{1}$と完全に一致する。 $$ L_{1} = \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} $$\nこのようなプロセスをコンプレックス$K$の次元$\\dim K$まで繰り返すと、求めていたアルゴリズムを得ることができる。行列の左右の大きさは$\\partial_{k}$に従い、その成分は$\\deg$に従って充填されると考えると少し混乱しなくなるだろう。\n■\n一方、補助定理(1)で列操作だけで十分だということは、これまでの導出で見たように行列表現に固執する理由がないということでもある。また、補助定理(3)により、「過去にすでに計算が終わった」部分に対して大胆に行を捨てるような効率的なプロシージャが含まれており、これにはピボットでないカラムを「マーキング」する能力などが必要である。結果として、実際のアルゴリズムの擬似コードPseudo Codeは、行列をそのまま使用するのではなく、もう少し高度なデータ型、ディクショナリーやデータフレームなどで説明されることになる。これは実際に体験すると非常に戸惑いやすく難しい。\n実装 Zomorodianのアルゴリズム実装: 科学界で働くなら誰でも読みやすいJulia言語を通じて、論文の擬似コードをほぼ文学的に翻訳した実装を紹介する。 Zomorodian. (2005). Computing Persistent Homology: ch4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2447,"permalink":"https://freshrimpsushi.github.io/jp/posts/2447/","tags":null,"title":"ジョモロジアンのアルゴリズム誘導"},{"categories":"확률론","contents":"概要 測度論と確率論を学んだ人向けの定義と概念の要約資料です。迅速な復習と定義の参照のために作成されました。\n測度論 代数 $X \\ne \\varnothing$の部分集合たちのコレクション $\\mathcal{A}$が 有限 合集合と補集合に対して閉じている時、これを代数と言います。\n可算合集合に対して閉じている代数を$\\sigma$-代数と言います。\nNote:\n定義により $\\mathcal{A}$はまた交集合に対しても閉じています $\\big( \\because E_{1} \\cap E_{2} = \\left( E_{1} \\cup E_{2} \\right)^{c} \\in \\mathcal{A}$ for $E_{1}, E_{2} \\in \\mathcal{A} \\big)$ $\\mathcal{A}$は空集合 $\\varnothing$と全集合 $X$を含みます。 $\\big( \\because E \\in \\mathcal{A}$ $\\implies$ $\\varnothing = E \\cap E^{c} \\in \\mathcal{A} \\text{ and } X = E \\cup E^{c} \\in \\mathcal{A} \\big)$ $X$が位相空間なら、$X$の開集合たちのコレクションから作られる$\\sigma$-代数を$X$上のボレル $\\sigma$-代数と言い、$\\mathcal{B}_{X}$と表記します。\nボレル $\\sigma$-代数は全ての開集合を含む最も小さい唯一の$\\sigma$-代数です。 $\\mathcal{E}$を$X$上の$\\sigma$-代数としましょう。順序対 $(X, \\mathcal{E})$を可測空間と言い、$E \\in \\mathcal{E}$を可測集合と言います。\n特に言及がない限り、以下では固定された可測空間 $(X, \\mathcal{E})$について扱います。\n可測関数 全ての実数 $\\alpha \\in \\mathbb{R}$に対して、次を満たす関数 $f : X \\to \\mathbb{R}$を($\\mathcal{E}$-)可測と言います。 $$ \\left\\{ x \\in X : f(x) \\gt \\alpha \\right\\} \\in \\mathcal{E}\\qquad \\forall \\alpha \\in \\mathbb{R}. $$\n一般化 $(X, \\mathcal{E})$、$(Y, \\mathcal{F})$を可測空間とします。関数 $f : X \\to Y$が次を満たす時、これを$(\\mathcal{E}, \\mathcal{F})$-可測と言います。 $$ f^{-1}(F) = \\left\\{ x \\in X : f(x) \\in F \\right\\} \\in \\mathcal{E}\\qquad \\forall F \\in \\mathcal{F}. $$\nNote: $\\mathcal{E}$-可測関数は上の定義で$(Y, \\mathcal{F}) = (\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$の場合と同じです。\n測度 $\\mathcal{E}$ (または $(X, \\mathcal{E})$、$X$)上の測度とは、次を満たす関数 $\\mu : \\mathcal{E} \\to [0, \\infty]$です。\nNull empty set: $\\mu (\\varnothing) = 0$。 Countable additivity: $\\left\\{ E_{j} \\right\\}$が$\\mathcal{E}$の互いに素な集合たちなら、$\\displaystyle \\mu \\left( \\bigcup\\limits_{j} E_{j} \\right) = \\sum\\limits_{j} \\mu (E_{j})$。 三つ組 $(X, \\mathcal{E}, \\mu)$を測度空間と言います。特に言及がない限り以下では固定された測度空間 $(X, \\mathcal{E}, \\mu)$について扱います。\nボレル測度とは、定義域がボレル $\\sigma$-代数$\\mathcal{B}_{\\mathbb{R}}$の測度を言います： $$ \\mu : \\mathcal{B}_{\\mathbb{R}} \\to [0, \\infty] $$\n$(X, \\mathcal{E})$、$(Y, \\mathcal{F})$上の二つの測度 $\\mu$、$\\nu$に対して、次を満たす$\\mathcal{E} \\times \\mathcal{F}$上の唯一の測度 $\\mu \\times \\nu$を$\\mu$と$\\nu$の積測度と言います。 $$ \\mu \\times \\nu (E \\times F) = \\mu (E) \\nu (F)\\qquad \\text{ for all rectangles } E \\times F. $$\n積分 実関数 $f$が有限な関数値を持つ時、これを単純と言います。\n単純可測関数 $\\varphi$は次のような形で表されます。 $$ \\begin{equation} \\varphi = \\sum\\limits_{j=1}^{n} a_{j}\\chi_{E_{j}}, \\text{ where } E_{j} = \\varphi^{-1}(\\left\\{ a_{j} \\right\\}) \\text{ and } \\operatorname{range} (\\varphi) = \\left\\{ a_{1}, \\dots, a_{n} \\right\\}. \\end{equation} $$ ここで $\\chi_{E_{j}}$は$E_{j}$の特性関数です。これを$\\varphi$のstandard representationと言います。\n$\\varphi$がstandard representation $(1)$を持つ単純可測関数の時、測度 $\\mu$に対する**$\\varphi$の積分**を次のように定義します。 $$ \\int \\varphi d\\mu := \\sum\\limits_{j=1}^{n} a_{j}\\mu (E_{j}). $$ Notation: $$ \\int \\varphi d\\mu = \\int \\varphi = \\int \\varphi(x) d\\mu (x), \\qquad \\int = \\int_{X}. $$\n$f$が$(X, \\mathcal{E})$上の可測関数の時、$\\mu$に対する**$f$の積分**を次のように定義します。 $$ \\int f d\\mu := \\sup \\left\\{ \\int \\varphi d\\mu : 0 \\le \\varphi \\le f, \\varphi \\text{ is simple and measurable} \\right\\}. $$\n$f : X \\to \\mathbb{R}$の正の部分と負の部分をそれぞれ次のように定義します。 $$ f^{+}(x) := \\max \\left( f(x), 0 \\right)),\\qquad f^{-1}(x) := \\min \\left(-f(x), 0 \\right)). $$ もし二つの積分$\\displaystyle \\int f^{+}$、$\\displaystyle \\int f^{-}$が有限なら、$f$が積分可能と言います。また$\\left| f \\right| = f^{+} - f^{-}$が成立します。\n積分可能な実関数たちの集合はベクトル空間であり、積分はこのベクトル空間上の線形汎関数です。このベクトル空間を次のように表記します。 $$ L = L(X, \\mathcal{E}, \\mu) = L(X, \\mu) = L(X) = L(\\mu), \\qquad L = L^{1} $$\n$L^{p}$空間\n測度空間$(X, \\mathcal{E}, \\mu)$と$0 \\lt p \\lt \\infty$に対して、$L^{p}$を次のように定義します。 $$ L^{p}(X, \\mathcal{E}, \\mu) := \\left\\{ f : X \\to \\mathbb{R} \\left| f \\text{ is measurable and } \\left( \\int \\left| f \\right|^{p} d\\mu \\right)^{1/p} \\lt \\infty \\right. \\right\\}. $$\n確率論 表記法と用語 $$ \\begin{array}{lll} \\text{Analysts\u0026rsquo; Term} \u0026amp;\u0026amp; \\text{Probabilists\u0026rsquo; Term} \\\\ \\hline \\text{Measure space } (X, \\mathcal{E}, \\mu) \\text{ such that } \\mu (X) = 1 \u0026amp;\u0026amp; \\text{Probability space } (\\Omega, \\mathcal{F}, P) \\\\ \\text{Measure } \\mu : \\mathcal{E} \\to \\mathbb{R} \\text{ such that } \\mu (X) = 1 \u0026amp;\u0026amp; \\text{Probability } P : \\mathcal{F} \\to \\mathbb{R} \\\\ (\\sigma\\text{-)algebra $\\mathcal{E}$ on $X$} \u0026amp;\u0026amp; (\\sigma\\text{-)field $\\mathcal{F}$ on $\\Omega$} \\\\ \\text{Mesurable set } E \\in \\mathcal{E} \u0026amp;\u0026amp; \\text{Event } E \\in \\mathcal{F} \\\\ \\text{Measurable real-valued function } f : X \\to \\mathbb{R} \u0026amp;\u0026amp; \\text{Random variable } X : \\Omega \\to \\mathbb{R} \\\\ \\text{Integral of } f, {\\displaystyle \\int f d\\mu} \u0026amp;\u0026amp; \\text{Expextation of } f, E(X) \\\\ f \\text{ is } L^{p} \u0026amp;\u0026amp; X \\text{ has finite $p$th moment} \\\\ \\text{Almost everywhere, a.e.} \u0026amp;\u0026amp; \\text{Almost surely, a.s.} \\end{array} $$\n$$ \\begin{align*} \\left\\{ X \\gt a \\right\\} \u0026amp;:= \\left\\{ w : X(w) \\gt a \\right\\} \\\\ P\\left( X \\gt a \\right) \u0026amp;:= P\\left( \\left\\{ w : X(w) \\gt a \\right\\} \\right) \\end{align*} $$\n基礎定義 可測空間$(\\Omega, \\mathcal{F})$、$(\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$に対して、$(\\mathcal{F}, \\mathcal{B}_{\\mathbb{R}})$-可測関数$X : \\Omega \\to \\mathbb{R}$を確率変数と言います。つまり、 $$ X^{-1}(B) \\in \\mathcal{F}\\qquad \\forall B \\in \\mathcal{B}_{\\mathbb{R}}. $$\n$(\\Omega, \\mathcal{F})$上の確率(または確率測度)とは、$P(\\Omega) = 1$を満たす測度$P : \\mathcal{F} \\to \\mathbb{R}$です。\n$X$を確率変数とする時、\n期待値: $\\displaystyle E(X) := \\int X dP$ 分散: $\\sigma^{2}(X) := E\\left[ (X - E(X))^{2} \\right] = E(X^{2}) - E(X)^{2}$ $X$の(確率)分布とは、次を満たす$\\mathbb{R}$上の確率$P_{X} : \\mathcal{B}_{\\mathbb{R}} \\to \\mathbb{R}$です： $$ P_{X}(B) := P(X^{-1}(B)). $$\n$X$の分布関数$F_{X}$は次のように定義されます： $$ F_{X}(a) := P_{X}\\left( (-\\infty, a] \\right) = P(X \\le a). $$\n確率変数の数列$\\left\\{ X_{i} \\right\\}_{i=1}^{n}$に対して、確率ベクトル$(X_{1}, \\dots, X_{n})$は次のように定義される関数を言います： $$ (X_{1}, \\dots, X_{n}) : \\Omega \\to \\mathbb{R}^{n} $$ $$ (X_{1}, \\dots, X_{n})(x) := (X_{1}(x), \\dots, X_{n}(x)). $$\nNote: $(X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n})= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})$。\n$n=2$の場合を先に見ましょう。$(X, Y) : \\Omega \\to \\mathbb{R}^{2}$に対して次が成立します。 $$ (X, Y)^{-1} (a, b) = \\left\\{ x \\in \\Omega : X(x) = a \\right\\} \\cap \\left\\{ x \\in \\Omega : Y(x) = b \\right\\}. $$ 従って、全てのボレル集合$B_{1}$、$B_{2} \\in \\mathcal{B}_{\\mathbb{R}}$に対して次を得ます。 $$ (X, Y)^{-1}(B_{1} \\times B_{2}) = (X, Y)^{-1}(B_{1}, B_{2}) = X^{-1}(B_{1}) \\cap Y^{-1}(B_{2}). $$ これを任意の$\\mathbb{R}^{n}$に対して拡張すると、 $$ \\begin{equation} \\begin{aligned} (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \u0026amp;= (X_{1}, \\dots, X_{n})^{-1}(B_{1}, \\dots, B_{n}) \\\\ \u0026amp;= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n}). \\end{aligned} \\end{equation} $$\n$X_{1}, \\dots, X_{n}$の結合分布とは確率ベクトル$(X_{1}, \\dots, X_{n})$の確率分布で定義されます： $$ P_{(X_{1}, \\dots, X_{n})} : \\mathcal{B}_{\\mathbb{R}^{n}} \\to \\mathbb{R}, $$ $$ P_{(X_{1}, \\dots, X_{n})}(B_{1} \\times \\cdots \\times B_{n}) := P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right). $$\n独立 $P(E) \\gt 0$の事象$E$に対して、$\\Omega$上の確率 $$ P_{E}(F) = P(E|F) := P(E \\cap F)/P(E) $$ を$E$上の条件付き確率と言います。\nもし$P_{E}(F) = P(F)$なら、$F$を$E$と独立と言います： $$ \\text{$F$ is independent of $E$} \\iff P(E \\cap F) = P(E)P(F). $$ 次が成立する時、$\\Omega$の事象たちのコレクション$\\left\\{ E_{j} \\right\\}$が独立と言います： $$ P(E_{1} \\cap \\cdots \\cap E_{n}) = P(E_{1}) P(E_{2}) \\cdots P(E_{n}) = \\prod \\limits_{i=1}^{n} P(E_{j}). $$\n$\\Omega$上の確率変数たちのコレクション$\\left\\{ X_{j} \\right\\}$が独立ということは、全てのボレル集合$B_{j} \\in \\mathcal{B}_{\\mathbb{R}}$に対して事象たち$\\left\\{ X_{j}^{-1}(B_{j}) \\right\\}$が独立ということを言います。つまり次の式が成立することを意味します： $$ P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) = \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})). $$\n確率分布の定義と$(2)$により、上記式の左辺から次を得ます。 $$ \\begin{align*} P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) \u0026amp;= P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right) \\\\ \u0026amp;= P_{(X_{1}, \\dots, X_{n})} \\left( B_{1} \\times \\cdots \\times B_{n} \\right). \\end{align*} $$ 一方、積測度と確率分布の定義により、右辺から次を得ます。 $$ \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})) = \\prod \\limits_{j=1}^{n} P_{X_{j}}(B_{j}) = \\left( \\prod \\limits_{j=1}^{n} P_{X_{j}} \\right) \\left( B_{1} \\times \\cdots \\times B_{n} \\right). $$ 従って$\\left\\{ X_{j} \\right\\}$が独立なら、 $$ P_{(X_{1}, \\dots, X_{n})} = \\prod\\limits_{j=1}^{n}P_{X_{j}}. $$\n$\\left\\{ X_{j} \\right\\}$が独立な確率変数の集合であることは、$\\left\\{ X_{j} \\right\\}$の結合分布がそれぞれの分布の積と同じであることと同値です。\n参考文献 Robert G. Bartle, The Elements of Integration and Lebesgue Measure (1995) Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (1999) ","id":3473,"permalink":"https://freshrimpsushi.github.io/jp/posts/3473/","tags":null,"title":"측도론과 확률론 요약 정리"},{"categories":"데이터과학","contents":"定義 1 母集団に関連する数値的に記述される尺度数値的記述尺度を母数パラメータといい、サンプルから算出されたものを統計量スタティスティックという。\n説明 統計学の定義については様々な考え方があるが、基本的には推論統計学、特に数理統計学での「母数とは何か」に関心を持つ学問とされている。この観点から、統計学はデータを通じて母集団の母数を知る方法を研究する学問だと言える。\n一方、面白いことに、統計量Statisticにsを一つ加えるとStatistics、つまり統計学そのものになる。これは、統計学が統計量に関する研究であり、特にそれが母数に関するものである時は推定量エスティメータと言われることを意味している。例えば、ほとんどの場合、サンプルを全て加算し、その数で割る統計量である標本平均 $\\overline{x}$ は母平均 $\\mu$ を知るための推定量である。\n関連項目を見る 数理統計学での統計量と推定量 ハイパーパラメータ メンデンホール. (2012).「確率と統計の紹介」 (13版): p50.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2440,"permalink":"https://freshrimpsushi.github.io/jp/posts/2440/","tags":null,"title":"基礎統計学における母数と統計量"},{"categories":"줄리아","contents":"正規分布\njulia\u0026gt; using Distributions\rjulia\u0026gt; d = Normal()\rNormal{Float64}(μ=0.0, σ=1.0)\rjulia\u0026gt; rand(d, 2,2)\r2×2 Matrix{Float64}:\r-0.618228 -0.729552\r-1.46898 -0.636276 一様分布\njulia\u0026gt; rand(Uniform(), 2,2)\r2×2 Matrix{Float64}:\r0.0952175 0.348995\r0.845515 0.768308\rjulia\u0026gt; rand(Uniform(1,10), 2,2)\r2×2 Matrix{Float64}:\r7.09885 1.65445\r6.14428 7.31004 コーシー分布\njulia\u0026gt; rand(Cauchy(), 2,2)\r2×2 Matrix{Float64}:\r-20.1142 0.118282\r-0.110452 -0.420331\rjulia\u0026gt; rand(Cauchy(), 2,2)\r2×2 Matrix{Float64}:\r2.96951 -0.0587456\r0.0388744 -0.422848 ","id":3463,"permalink":"https://freshrimpsushi.github.io/jp/posts/3463/","tags":null,"title":"ジュリアで与えられた分布からランダムにサンプリングする方法"},{"categories":"줄리아","contents":"説明1 Juliaでランダム抽出する関数は以下の通りです。\nrand([rng=default_rng()], [S], [dims...]) rngはRandom Number Generatorの略で、乱数抽出アルゴリズムを指定します。何を意味しているのかわからなければ、触らなくても大丈夫です。\nSは（おそらく）Setの略で、ランダム抽出をする集合を指定する変数です。Sに入力可能な変数は以下のものがあります。\nインデックスがあるオブジェクト AbstractDictまたはAbstractSet 文字列 タイプ（整数、浮動小数点のみ可能です。有理数、無理数は不可。） 抽出集合をタイプで指定した場合、整数型ならtypemin(S):type(S)の範囲から抽出します（BigIntは対応していません）。\njulia\u0026gt; typemin(Int16), typemax(Int16)\r(-32768, 32767)\rjulia\u0026gt; typemin(Int32), typemax(Int32)\r(-2147483648, 2147483647)\rjulia\u0026gt; typemin(Int64), typemax(Int64)\r(-9223372036854775808, 9223372036854775807) 浮動小数点なら$[0, 1)$の範囲から抽出します。\njulia\u0026gt; rand(Float64)\r0.4949745522302659\rjulia\u0026gt; rand(ComplexF64)\r0.8560168003603014 + 0.16478582700545064im [dims...]は抽出する配列の次元を表します。rand(S, m, n)ならば、集合Sの要素から（重複を含めて）$m \\times n$個を抽出して$m \\times n$形の配列を返します。次元を入力しなければ、実数が返されます。実数と1次元ベクトルが明確に区別されるので、注意してください。さらに、$2\\times 3$形の配列を得たいと思って次元を(2,3)のようにタプルで入力すると、Sの変数として受け取られるので、全く異なる結果が出るので注意してください。\njulia\u0026gt; rand(Float64) # 실수 추출\r0.42226201756172266\rjulia\u0026gt; rand(Float64, 1) # 성분이 실수인 1x1 배열로 추출\r1-element Vector{Float64}:\r0.7361136057571305\rjulia\u0026gt; rand(2,3) # 성분이 실수인 2x3 배열로 추출 2×3 Matrix{Float64}:\r0.648742 0.364548 0.0550352\r0.0350098 0.56055 0.83297\rjulia\u0026gt; rand((2,3)) # 2와 3중에서 추출\r3 より高度な内容は次を参考にしてください。\nランダムシードを固定する方法 重みを付けてランダム抽出する方法 分布を与えてランダム抽出する方法 コード インデックスがあるオブジェクト julia\u0026gt; rand((2,5))\r5\rjulia\u0026gt; rand(2:5)\r3\rjulia\u0026gt; rand([2,3,4,5])\r4\rjulia\u0026gt; rand([\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, 4])\r\u0026#34;x\u0026#34; ディクショナリ 抽出集合をディクショナリにすると、キー-値ペアPair自体が抽出されます。\njulia\u0026gt; d = Dict(2=\u0026gt;4, 3=\u0026gt;5, 4=\u0026gt;\u0026#34;6\u0026#34;)\rDict{Int64, Any} with 3 entries:\r4 =\u0026gt; \u0026#34;6\u0026#34;\r2 =\u0026gt; 4\r3 =\u0026gt; 5\rjulia\u0026gt; rand(d)\r4 =\u0026gt; \u0026#34;6\u0026#34;\rjulia\u0026gt; rand(d)\r2 =\u0026gt; 4 文字列 抽出集合を文字列にすると、文字列内の文字の中からランダムに一つが抽出されます。\njulia\u0026gt; str = \u0026#34;freshrimpsushi\u0026#34;\r\u0026#34;freshrimpsushi\u0026#34;\rjulia\u0026gt; rand(str)\r\u0026#39;e\u0026#39;: ASCII/Unicode U+0065 (category Ll: Letter, lowercase)\rjulia\u0026gt; rand(str)\r\u0026#39;h\u0026#39;: ASCII/Unicode U+0068 (category Ll: Letter, lowercase) タイプ julia\u0026gt; rand(Int32, 3)\r3-element Vector{Int32}:\r1552806175\r-384901411\r-1580189675\rjulia\u0026gt; rand(UInt32, 3)\r3-element Vector{UInt32}:\r0xd2f44f99\r0x166a8b9e\r0x92fe22dc\rjulia\u0026gt; rand(Float32, 3)\r3-element Vector{Float32}:\r0.59852564\r0.6247238\r0.23303497\rjulia\u0026gt; rand(ComplexF32, 3)\r3-element Vector{ComplexF32}:\r0.10872495f0 + 0.6622572f0im\r0.6408408f0 + 0.46815878f0im\r0.7766515f0 + 0.73314756f0im 環境 OS: Windows11 Version: Julia 1.9.0 https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3462,"permalink":"https://freshrimpsushi.github.io/jp/posts/3462/","tags":null,"title":"ジュリアでランダムに抽出する方法"},{"categories":"프로그래밍","contents":"개요1 名前のある140以上のCSSカラーパレットです。\n코드 ","id":3459,"permalink":"https://freshrimpsushi.github.io/jp/posts/3459/","tags":null,"title":"CSSカラー名札"},{"categories":"데이터과학","contents":"定義 1 質的変数 質的Qualitativeな特性を測定した変数を質的変数という。\n食べ物が\u0026hellip; 美味しい / まあまあ / まずい 色が\u0026hellip; 赤い / 青い / 黄色い 専攻が\u0026hellip; 数学 / 統計学 / 物理学 このような質的変数は、一般的にカテゴリカルCategoricalデータとも言われる。\n量的変数 量的Quantitativeな特性を測定した変数を量的変数という。\n年齢が\u0026hellip; 20歳 / 31歳 / 11歳 身長が\u0026hellip; 170.0cm / 170.5cm / 162.1cm 年齢や視力のようにはっきりとした値をとる量的変数を離散Discrete変数、身長や体重のように連続的な値をとる量的変数を連続Continuous変数という。\n説明 定義がなんだか奇妙に思えるかもしれないが、実際に「質的」と「量的」という言葉は、元々知っていた言葉ではなく、このような学術的な用語から日常的な表現を学ぶほうがむしろ正しいかもしれない。例えば、何かの品質を評価するときに、私たちは文字通り「クオリティが高い」という表現をよく使う。しかし、「質が高い」「質が低い」という言葉自体が、「1432ほど良い」や「17%ほど良い」とどう違うのかを考えてみよう。\n質的とは、このようにある順序（良い-まあまあ-悪い）を持つことはできるが、通常数値で表すのが難しいものを指す。もちろん、カテゴリー化されている（ドイツ語-フランス語-日本語）も問題ない。 量的はその反対で、量Amountを表すものを指す。ただし、ここで離散変数と連続変数の定義は少し難解かもしれない。 はっきりとした値とは？ はっきりとした値とは、いわゆる自然数や目盛りがあるような、ある単位で間隔を置いた値を説明する表現である。もちろん、どんな本にもそんなことは書いてないだろうし、私も見たことがない。そして、書きながらもあまり良い表現ではないと認める。代わりに私がとても気に入っている表現は以下の通りである。\nカウンタブルな値を取る変数を離散変数という。その値が限定的であるか数えることができるときのみを想定する。\n問題は、このように数学的に正確な表現が、すぐに離散変数が何なのか混乱しているあなたには何の役にも立たないことである。このような表現を理解することは、離散変数が何であるかを知っている人が離散変数について学ぶのと変わらない。\n何かがCountableであるとは、インド・ヨーロッパ語族、例えば私たちに馴染み深い英語、フランス語、スペイン語などで「1つ、2つ、\u0026hellip;」と数えられるものを指す。英語でそのようなものを表す名詞があれば、それを可算名詞と呼び、数学的に言えば自然数の集合と一対一対応が存在する。\nあまり役に立たない説明かもしれない。例を見て理解してみよう。以下の数は大抵離散変数である:\n牧場にいる豚の数 年間交通事故の死者数 専門書のページ数 幼児の年齢\u0026hellip;「24ヶ月の男の子」、「1歳2ヶ月の女の子」など 1Lの水筒の数 次に、離散変数かどうか迷うかもしれない例を見てみよう:\n1Lの水筒3つに入っている水の量\u0026hellip; 水筒の数ではなく、水の量なら連続型である。 視力\u0026hellip; 通常は0.1刻みだが、もし0.5、1.0、1.5の3つのグループしかなければ、離散変数 と見なすことができ、データの構成によっては質的変数と見なす余地もある。\n分類問題と回帰問題 通常、データサイエンスでは、従属変数が質的変数か量的変数かによって、分類問題と回帰問題を区別する。\n注意事項 実際にデータを扱いながら、経験が少ない初心者が犯しやすいミスがある。質的変数と量的変数を理解していないわけではなく、単に慣れていないために起こり得るミスであり、誰もが犯す可能性のあるミスである。多くの場合、回帰分析のような難しいものを勉強する頃にこのような罠に陥り、その直感を人工的に養う機会はほとんどない。次の投稿を見ると、正確に何を意味するのかはわからないかもしれないが、それがどのような罠なのかは大まかに理解できるかもしれない。\n質的変数を含む回帰分析 エンコーディング 性別を示す際に、男性を$0$、女性を$1$とエンコーディングEncodingする場合がよくあるが、目に見える数字があるからといって、これが離散変数（量的変数）になるわけではない。\nこのようなエンコーディングは、プライバシーのためにも使用される。想像してみよう。医療データは、個人の敏感な情報を多く含み、場合によってはデータだけで個々の人を特定できるほど特徴的な変数が多い。このような場合、データを公開する際に特定の情報を単に数字で隠すこともある。例えば、精神病歴、女性の中絶の有無などがある。\nレーティング 同様にエンコーディングの場合、レーティングが存在する場合がある。例えば、高卒が$0$、大卒が$1$、博士が$2$と表される場合、これが量的変数のように見えるが、依然として質的変数である。いわゆる低学歴、高学歴などは、一般社会の通念に過ぎず、データ的にこれらの数字は特に順序を示さない。現実のさまざまな例でこの主張を続けることができるが、ただちに高卒が$1$、大卒が$0$、博士が$2$とエンコーディングされるだけで、すでに量的変数ではないことがわかるだろう。\nヘックスコード 赤と青を区別することは質的変数だが、ピンク、ローズピンク、ディープピンクを区別するデータはどうだろう？これが口紅の話であれば、依然として質的変数で十分だが、例えば布の色であり、何千もの色がある場合、これらをRGBヘックスコードで表現できる。このようなデータに接する機会はほとんどないかもしれないが、直感的に質的変数だと思っても、量的変数として表現できる可能性があることを念頭に置く必要がある。\nジェンダー データにジェンダーGenderというカテゴリーが登場することもあるが、あなたが政治的正しさPolitical Correctnessに共感するか、うんざりするかにかかわらず、データがそう提供されているならば、まずはそのまま受け入れる必要がある。\nこれは本当の話だ。上で性別の例として挙げたように、ジェンダーが$0,1,2,3, \\cdots$でエンコーディングされたデータがあり、ジェンダー問題に全く関心がなかったある先輩が、「これ、ジェンダーで2と3は何？」と戸惑っていたのを見たことがある。アメリカ社会で調査されたデータでは、よくあることだ。 ポイントは、このようなことが起こらないように、ジェンダー問題に関心を持って勉強することではなく、特定のドメインDomainに関する知識が不足している場合は、直感に頼ってデータを検討しないことである。\nなぜ私たちはこれを知 る必要があるのか？\nこれらは非常に簡単で単純なことであるため、私たちはこれらを正確に区別し理解することができなければならない。ここでの私たちとは、統計学を応用する研究者を含め、統計学専攻者や、他の分野にバックグラウンドを持ちながらもデータサイエンスに従事する可能性のある人々を指す。\nこのように私たちが説明を探し、勉強し、課題をこなし、発表に慣れていく間に、皆さんの同僚たちはそれぞれ社会に適した何かをしていたであろう。残念ながら、それらの仕事は多くの場合大変だったため、私たちほどデータに精通していない可能性が高い。\n彼らはデータに無関心であったり、無知であったりするため、ここで述べられた注意事項を守らず、これらのばかげたミスを犯している可能性がある。そして、それらについて疑わない一般の人々を想像してみよう。あなたの上司Bossも例外ではない。\n私たちはそれを防がなければならない。\nMendenhall. (2012). 『確率と統計の入門』(13版): p10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2420,"permalink":"https://freshrimpsushi.github.io/jp/posts/2420/","tags":null,"title":"質的変数と連続変数"},{"categories":"추상대수","contents":"定義 実数の可逆な $n \\times n$ 行列の集合を $\\mathrm{GL}(n, \\mathbb{R})$ または $\\mathrm{GL}_{n}(\\mathbb{R})$ と表記し、$n$次の一般線型群general linear group of degree $n$と呼ぶ。\n$$ \\mathrm{GL}(n, \\mathbb{R}) := \\left\\{ n \\times n \\text{ invertible matrix} \\right\\} = M_{n \\times n}(\\mathbb{R}) \\setminus {\\left\\{ A \\in M_{n \\times n}(\\mathbb{R}) : \\det{A} = 0 \\right\\}} $$\n説明 可逆な行列だけを集めたので、行列の積に関して群になる。また、微分可能な構造を持つため、リー群でもある。\n","id":3450,"permalink":"https://freshrimpsushi.github.io/jp/posts/3450/","tags":null,"title":"一般リニア群"},{"categories":"위상데이터분석","contents":"定理 1 2 カバーとリフトの定義: 単位区間を$I = [0,1]$のように表す。\n$X$のオープンセット$U \\subset X$が**$p$によって均等にカバーされる**Evenly Covered by $p$とは、全ての$\\alpha \\in \\forall$に対応する全ての制限関数$p |_{\\widetilde{U}_{\\alpha}}$がホメオモルフィズムであり $$ \\alpha_{1} \\ne \\alpha_{2} \\implies \\widetilde{U}_{\\alpha_{1}} \\cap \\widetilde{U}_{\\alpha_{2}} = \\emptyset $$ を満たす、つまり互いに素な$\\widetilde{X}$のオープンセット$\\widetilde{U}_{\\alpha} \\subset \\widetilde{X}$について $$ p^{-1} \\left( U \\right) = \\bigsqcup_{\\alpha \\in \\forall} \\widetilde{U}_{\\alpha} $$ が成り立つことを意味する。 $p : \\widetilde{X} \\to X$が全射関数であり、全ての$x \\in X$に対して$p$によって均等にカバーされる$x$のオープンネイバーフッド$U_{x} \\subset X$が存在する場合、$p : \\widetilde{X} \\to X$をカバーCoveringという。 カバー$p$の定義域$\\widetilde{X}$をカバースペースCovering Space、値域$X$をベーススペースBase Spaceという。 $n \\in \\mathbb{N}$とする。$f : I^{n} \\to X$と$\\widetilde{f} : I^{n} \\to \\widetilde{X}$が次を満たす場合、$\\widetilde{f}$を$f$のリフトLiftという。 $$ f = p \\circ \\widetilde{f} $$ $1$-スフィア$S^{1}$を値域に持つカバーを$p : \\mathbb{R} \\to S^{1}$としよう。\nパスリフティング定理 連続関数$f : I \\to S^{1}$はリフト$\\widetilde{f} : I \\to \\mathbb{R}$を持つ。特に与えられた$x_{0} \\in S^{1}$と$\\widetilde{x}_{0} \\in p^{-1} \\left( x_{0} \\right)$に対して、$\\widetilde{f} \\left( 0 \\right) = \\widetilde{x}_{0}$である$\\widetilde{f}$は一意に存在する。\nホモトピー・リフティング定理 連続関数$F : I^{2} \\to S^{1}$はリフト$\\widetilde{F} : I^{2} \\to \\mathbb{R}$を持つ。特に与えられた$x_{0} \\in S^{1}$と$\\widetilde{x}_{0} \\in p^{-1} \\left( x_{0} \\right)$に対して、$\\widetilde{F} \\left( 0 , 0 \\right) = \\widetilde{x}_{0}$である$\\widetilde{F}$は一意に存在する。\n説明 リフティング定理Lifting Theoremは一般に単位円$S^{1}$の性質を研究するための補助定理として言及され、形式的にFormally見た場合、パスリフティングか、ホモトピー・リフティングかという区別はあまり意味がない。\nむしろ、ほとんどの数学者が気にすべき質問は$X \\ne S^{1}$である$f: I^{m} \\to X$に対する一般化が可能かという点であり、実際にはコンパクト空間$Y$に対する連続関数$f: Y \\times I^{m} \\to X$に対するリフティング定理まで論じることができる。ただし、このような拡張が実際には全く役に立たないため、直接学ぶには過剰だと言われている。\n証明 戦略: パスリフティング定理のみを証明する。本質的にホモトピー・リフティング定理の証明はパスリフティング定理の証明と同じである。パスリフティング定理ではコンパクト空間である$I$から区間を有限に分割して証明するように、ホモトピー・リフティング定理では同様にコンパクトな空間である$I^{2}$を有限に分割して同じ議論を繰り返す。\nPart 1. 設定\n$p : \\widetilde{X} \\to X$が全射関数であり、全ての$x \\in X$に対して$p$によって均等にカバーされる$x$のオープンネイバーフッド$U_{x} \\subset X$が存在する場合、$p : \\widetilde{X} \\to X$をカバーCoveringという。 $p : \\mathbb{R} \\to S^{1}$はカバーとされているので、全ての$x \\in S^{1}$に対して$p$によって均等にカバーされる$x$のネイバーフッド$U_{x} \\subset S^{1}$が存在する。\n$I = [0,1]$はコンパクトなので$I \\subset \\bigcup_{k=1}^{n} \\left[ a_{k-1} , a_{k} \\right]$を満たすような有限の点の集合$\\left\\{ a_{k} \\right\\}_{k=0}^{n} \\subset I$が存在し、 $$ 0 = a_{0} \u0026lt; a_{1} \u0026lt; \\cdots \u0026lt; a_{n-1} \u0026lt; a_{n} = 1 $$ その区間$\\left[ a_{k-1} , a_{k} \\right] \\subset I$に対する$f$のイメージは$S^{1}$に含まれ、特にあるオープンセット$U \\subset S^{1}$に対して以下の包含関係を満たす。 $$ f \\left( \\left[ a_{k-1} , a_{k} \\right] \\right) \\subset U \\subset S^{1} $$ このような$U$に対するカバー$p$の互いに素なプレイメージを$\\widetilde{U}_{t} := p^{-1} \\left( U_{t} \\right)$とすれば、それぞれ$t \\in \\mathbb{Z}$に対して$U$とホメオモルフィックである。\nPart 2. 帰納的構築\n任意の$x \\in S^{1}$ではなく、具体的に$x_{0} \\in S^{1}$を選び、その$p$のプレイメージの要素の一つを$\\widetilde{x}_{0} := p^{-1} \\left( x_{0} \\right) \\in \\mathbb{R}$と表す。元の設定によれば、これらの要素の集合は$\\mathbb{Z}$との間に全単射が存在するが、どれがどうであれ関係ない。\n私たちは$I$全体ではなく、$\\left[ 0, a_{k} \\right]$に対して$\\widetilde{f}_{k} (0) = \\widetilde{x}_{0}$を満たすリフト$\\widetilde{f}_{k}$を帰納的に定義して、結果的に$\\widetilde{f}$を見つけようとしている。\n$k = 0$の場合は単に$\\widetilde{f}_{0} (0) = \\widetilde{x}_{0}$とし、他に選択肢はない。 $k \\ne 0$の場合、連続関数$\\widetilde{f}_{k} : \\left[ 0 , a_{k} \\right] \\to \\mathbb{R}$が一意に定義されると仮定する。 ある一意の$\\widetilde{U} \\in \\left\\{ \\widetilde{U}_{t} \\right\\}_{t \\in \\mathbb{Z}}$に対して$\\widetilde{f} \\left( a_{k} \\right) \\in \\widetilde{U}$である。 $\\widetilde{f}_{k}$は連続であり、区間$\\left[ a_{k} , a_{k+1} \\right]$は経路連結であるため、$\\widetilde{f}_{k}$の拡張関数$\\widetilde{f}_{k+1}$がどのように定義されても、少なくとも$\\left[ a_{k} , a_{k+1} \\right]$は必ず$\\widetilde{U}$内にマッピングされなければならない。 $p$がカバーであるため、全ての$t \\in \\mathbb{Z}$に対してホメオモルフィズム$p | \\widetilde{U}_{t} : \\widetilde{U}_{t} \\to U$が存在し、それにより $$ p \\circ \\rho_{k} = f | \\left[ a_{k} , a_{k+1} \\right] $$ を満たす一意の関数$\\rho_{k} : \\left[ a_{k} , a_{k+1} \\right] \\to \\widetilde{U}$が存在する。このような関数$\\rho_{k}$の存在は、$p$の制限関数がホメオモルフィズムであること―すなわち単射であることに基づくため、$\\rho_{k} \\left( a_{k} \\right) = \\widetilde{f}_{k} \\left( a_{k} \\right)$であり、$\\rho_{k}$の連続性も保証される。 接着補題: 位相空間$X,Y$に対して、二つの閉集合$A,B \\subset X$が$A \\cup B = X$を満たし、二つの連続関数$f : A \\to Y$と$g : B \\to Y$が全ての$x \\in A \\cap B$に対して$f(x) = g(x)$であるとする。すると、以下のように定義された$h$は連続関数である。 $$ h(x) : = \\begin{cases} f(x), \u0026amp; x \\in A \\\\ g(x), \u0026amp; x \\in B \\end{cases} $$\n接着補題により、以下のような連続関数$\\widetilde{f}_{k+1} : \\left[ 0 , a_{k+1} \\right] \\to \\mathbb{R}$を一意に定義できる。 $$ \\widetilde{f}_{k+1} := \\begin{cases} \\widetilde{f}_{k} (s) \u0026amp; , \\text{if } s \\in \\left[ 0, a_{k} \\right] \\\\ \\rho_{k} (s) \u0026amp; , \\text{if } s \\in \\left[ a_{k} , a_{k+1} \\right] \\end{cases} $$ 数学的帰納法により、$S^{1}$は$t \\in \\mathbb{Z}$周回する螺旋に向かうリフトが具体的に存在する。ここで、$k = 0, 1, \\cdots , n$は$\\mathbb{R}$で上下に動くインデックスではなく、$S^{1}$を回転させながら有限に分割するインデックスであることをよく想像しなければならない。$k$が$1$ずつ増えるごとに、$\\mathbb{R}$では整数の数だけ多くの区間の集合$\\left\\{ \\widetilde{U}_{t} \\right\\}_{t \\in \\mathbb{Z}}$も同様に回転して動く。\nPart 3. ホモトピー・リフティング定理\n$0$から$n-1$までの整数を集めた集合$\\left\\{ 0, 1, \\cdots , n-1 \\right\\}$を簡単に$0:n$と書こう。 $I$がコンパクトであることに基づいて$0 = a_{0} \u0026lt; \\cdots \u0026lt; a_{n} = 1$を選べたように、$I^{2}$もコンパクトであるため、 $$ \\begin{align*} 0 = a_{0} \u0026lt; \\cdots \u0026lt; a_{n} = 1 \\\\ 0 = b_{0} \u0026lt; \\cdots \u0026lt; b_{m} = 1 \\end{align*} $$ のように正方形を格子に切る有限の二つの自然数$n , m \\in \\mathbb{N}$が存在し、それぞれの小さなマスを$i = 0:n$、$j = 0:m$に対して $$ R_{i,j} := \\left[ a_{i-1}, a_{i} \\right] \\times \\left[ b_{j-1} , b_{j} \\right] \\subset I^{2} $$ と定義すると、 $$ R_{0,0} , R_{0,1} , \\cdots , R_{0,m} , R_{1,0} \\cdots, R_{n,m} $$ のような小さな長方形のシーケンスが得られる。これに対してパスリフティング定理で行った議論を繰り返せば、ホモトピー・リフティング定理が証明される。\n■\nKosniowski. (1980). A First Course in Algebraic Topology: p137~138.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHatcher. (2002). Algebraic Topology: p29~31.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2419,"permalink":"https://freshrimpsushi.github.io/jp/posts/2419/","tags":null,"title":"代数的トポロジーにおけるリフティング定理の証明"},{"categories":"데이터과학","contents":"概要 現代社会では、データについて全く知らない知識人はいない。全く関心がない非専門家でも、「何かについての知識」や「通信するための資源」のようなデータや情報といった類義語を容易に思い浮かべることができるほど、データという概念は普遍化し、大衆化された。以下の記述はほんの少しでも、データサイエンスの観点から、データをより厳密に定義しようとする試みに過ぎない。\n定義 1 変数Variableは、時間や個人Individual、または対象Objectによって変わる特性Characteristicを指す。 変数が測定される個人や対象を 実験単位Experimental Unitといい、実験単位から実際に測定された結果を 測定値Measurementという。 測定値の集合を データDataという。 説明 データの語源 2 英語のデータは「与えられた、または認められた事実」を意味し、ラテン語で「与える」という意味の動詞「Do-」の過去分詞であるDatumから派生しており、「与えられた」という意味を持つ。Dataは、そのDatumの複数形にあたる。\n皮肉にも、このようなデータの語源は、上述のように、何か特徴であるとか、実験をしながらどうにか定義しようとしていたことよりも、より正確にデータの本質を指している。データサイエンスの世界では、データはすでに与えられたもの、またはこれから我々に与えられるべきものであり、新しい発見や創造の対象とは明らかに異なる属性を持つ。\nつまり、データはどうしようもなく、すなわち 与えられたものだ。粗野な比喩として、長持ちする電球を発明する状況を想像してみよう。平均寿命が100時間の電球Aから電球Bを改良した場合、各電球B（Object）の寿命を測定することができるだろう。この測定値を集めたものがまさに電球Bの寿命データであり、それらの数値は電球Bによって与えられたものであって、電球Aのデータ自体をどうにか変えて得たものではない。\n変数と実験？ 変数變數は、字のごとく変わる数値として考えがちで、データを簡単に説明するときにはよく数字が登場するけれど、非構造データに対する理解が深まった現代社会では、データを数字やカテゴリーに限定する必要はない。データの種類には、写真、文書、信号、株価、動画、ネットワーク構造など、人が認識できるすべてが対象である。同様に、測定値測定値も、「値」という文字を使うために数字のように見えるかもしれないが、そのように数値として考える必要はない。可能な限り、英語の表現Measurementをそのまま使うことをお勧めする。\nまた、実験単位の実験は、白衣を着た科学者たちが研究所で行うものだけを指すわけではない。基礎確率論で事象が起こることを「任意の試行」と呼ぶように、表現のための表現として受け入れても十分だ。\n母集団と標本 調査者Investigatorが関心を持つすべての測定値の集合を 母集団Populationという。 母集団の部分集合を サンプルSampleと呼ぶ。 \u0026hellip;このような定義から、現実的には多くのデータが母集団のサンプルであることが推測できる。一方で、母集団の英語表現Populationは、統計学とも密接に関係している人口という意味も持っているので注意してほしい。\n統計学のコンセプトは基本的に「母集団について知りたいが、実際には母集団をすべて調査することはできないので、サンプルを通じて母集団の特性を把握すること」、つまり、データを通じて関心ある対象の本質を推測することと言える。\n参考までに 数理統計学におけるサンプルの定義 学部2～3年生レベルで触れる数理統計学では、このポストで説明するサンプルについて数理的な定義を下し、データの別の表現である実現Realizationを紹介している。\nMendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p8.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.etymonline.com/word/data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2418,"permalink":"https://freshrimpsushi.github.io/jp/posts/2418/","tags":null,"title":"データの定義と語源"},{"categories":"줄리아","contents":"概要 元々ジュリアでは、データを出力する時にREPLのサイズに合わせてきれいに出力されるが、時には全体のデータを楽に見たい時がある。データがfooであれば、show(stdout, \u0026quot;text/plain\u0026quot;, foo)を通じて全体のデータを出力させることができる1。\nコード julia\u0026gt; foo = rand(100,2)\r100×2 Matrix{Float64}:\r0.956438 0.663427\r0.790117 0.472821\r0.976134 0.198475\r0.727601 0.472336\r0.0469046 0.991999\r0.625807 0.26634\r0.490773 0.588481\r0.352966 0.426474\r0.585632 0.00185974\r⋮\r0.615713 0.707071\r0.891683 0.622176\r0.370576 0.937107\r0.430644 0.0439135\r0.535987 0.551992\r0.43273 0.217023\r0.345366 0.500033\r0.551719 0.0761454 元々は上のように⋮が印刷されるが、プレーンテキストで印刷すると次のように全体が出力される。\njulia\u0026gt; show(stdout, \u0026#34;text/plain\u0026#34;, foo)\r100×2 Matrix{Float64}:\r0.956438 0.663427\r0.790117 0.472821\r0.976134 0.198475\r0.727601 0.472336\r0.0469046 0.991999\r0.625807 0.26634\r0.490773 0.588481\r0.352966 0.426474\r0.585632 0.00185974\r0.13357 0.90977\r0.789999 0.137833\r0.11626 0.385958\r0.629265 0.40623\r0.111327 0.483414\r0.22717 0.0960839\r0.854027 0.690618\r0.00862816 0.426555\r0.292845 0.588308\r0.475157 0.935968\r0.936422 0.116917\r0.421748 0.335614\r0.354324 0.444122\r0.52423 0.311464\r0.306786 0.873037\r0.308008 0.70787\r0.0885757 0.558464\r0.0510476 0.840701\r0.320569 0.28571\r0.89837 0.517027\r0.218359 0.622536\r0.563148 0.488849\r0.508919 0.818068\r0.880726 0.550501\r0.555517 0.953056\r0.466298 0.29687\r0.816757 0.528656\r0.789289 0.294199\r0.51256 0.173814\r0.972556 0.11602\r0.438784 0.815105\r0.218237 0.257226\r0.0838205 0.535666\r0.287095 0.877342\r0.176927 0.942882\r0.855193 0.577759\r0.813356 0.488643\r0.407358 0.970933\r0.224252 0.455783\r0.430215 0.727\r0.0585314 0.727251\r0.77538 0.777196\r0.114963 0.610359\r0.445436 0.472755\r0.0565616 0.153393\r0.695217 0.00669471\r0.673818 0.284351\r0.308611 0.386984\r0.761394 0.32279\r0.017963 0.114759\r0.465956 0.788791\r0.970691 0.264864\r0.0953205 0.359958\r0.437556 0.283858\r0.323666 0.893141\r0.971015 0.109052\r0.117792 0.919322\r0.898883 0.947123\r0.248386 0.462831\r0.895525 0.434108\r0.526593 0.288652\r0.891208 0.848443\r0.344758 0.412774\r0.697527 0.592066\r0.531953 0.50251\r0.0565245 0.449993\r0.168528 0.783811\r0.129681 0.22014\r0.489568 0.232417\r0.875734 0.380527\r0.0207026 0.915546\r0.210948 0.476037\r0.822661 0.517793\r0.579839 0.0221691\r0.455027 0.920253\r0.932968 0.771582\r0.960643 0.841065\r0.0835567 0.943408\r0.578494 0.502968\r0.0655954 0.528926\r0.590831 0.41364\r0.840604 0.790515\r0.327964 0.269113\r0.615713 0.707071\r0.891683 0.622176\r0.370576 0.937107\r0.430644 0.0439135\r0.535987 0.551992\r0.43273 0.217023\r0.345366 0.500033\r0.551719 0.0761454 環境 OS: Windows julia: v1.7.0 https://stackoverflow.com/questions/49304329/how-to-show-all-elements-of-vectors-and-matrices-in-julia/67090474#67090474\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2416,"permalink":"https://freshrimpsushi.github.io/jp/posts/2416/","tags":null,"title":"ジュリアでデータを省略せずに出力する方法"},{"categories":"머신러닝","contents":"概要1 $$ \\includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$\nMNISTmodified national institute of standards and technology データベースとは、アメリカの高校生と人口調査局の職員の数字の手書き文字に関するデータセットを指す。一般に[エムニスト]と呼ばれる。\n公式ホームページ 機械学習/ディープラーニング入門の例としてよく使用されるデータセットである。NISTでは、手書きの郵便番号の自動分類のための文字認識技術の評価のため、以下のような形式で手書きデータを収集した。ここでヤン・ルカンYann LeCunが高校生と人口調査局の職員の手書きデータを取り、前処理を行い、MNISTを作成した。画像のサイズは28 x 28で、60,000枚のトレーニングセットと10,000枚のテストセットで構成されている。\n$$ \\includegraphics[height=30em]{https://www.nist.gov/sites/default/files/styles/960_x_960_limit/public/images/2019/04/27/sd19.jpg?itok=oETq77cZ} $$\n使用方法 Julia Juliaでは、機械学習データセットパッケージであるMLDatasets.jlを使用できる。基本的にはFloat32型のトレーニングセットを読み込む。オプションでこれを変更して読み込むことができる。使用できるメソッドは以下の通り。\ndataset[i]: i番目の特徴量とターゲットのタプルを返す。 dataset[:]: 全ての特徴量とターゲットのタプルを返す。 length(dataset): データの数を返す。 convert2image(dataset, i): i番目のデータをグレースケールの画像に変換する。ImageShow.jlパッケージが必要である。 julia\u0026gt; using MLDatasets\rjulia\u0026gt; train = MNIST()\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :train\rfeatures =\u0026gt; 28×28×60000 Array{Float32, 3}\rtargets =\u0026gt; 60000-element Vector{Int64}\rjulia\u0026gt; test = MNIST(Float64, :test)\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :test\rfeatures =\u0026gt; 28×28×10000 Array{Float64, 3}\rtargets =\u0026gt; 10000-element Vector{Int64}\rjulia\u0026gt; length(train), length(test)\r(60000, 10000)\rjulia\u0026gt; using Plots\rjulia\u0026gt; using ImageShow\rjulia\u0026gt; train.targets[1]\r5\rjulia\u0026gt; heatmap(convert2image(train, 1)) ラベルは整数で与えられるため、ワンホットエンコーディングを別途行う必要がある。\njulia\u0026gt; train.targets[1:5]\r5-element Vector{Int64}:\r5\r0\r4\r1\r9\rjulia\u0026gt; using Flux\rjulia\u0026gt; Flux.onehotbatch(train.targets[1:5], 0:9)\r10×5 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\r⋅ 1 ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ 1 ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ 1 ⋅ ⋅\r1 ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ 1 Julia Fluxでワンホットエンコーディングする方法 Julia FluxでMLPを実装し、MNISTを学習する方法 環境 OS: Windows11 Version: Julia v1.8.2, MLDatasets v0.7.6, Plots v1.36.1, ImageShow v0.3.6, Flux v0.13.7 권건우·허령, 人工知能をマンガと野史で学ぶ 2, p68\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3444,"permalink":"https://freshrimpsushi.github.io/jp/posts/3444/","tags":null,"title":"MNIST Database"},{"categories":"줄리아","contents":"概要 Juliaは、MATLABレベルの線形代数をサポートしている。むしろMATLABよりも進化した、直感的で美しい構文を見ると、Juliaが作られた時点でよく設計されていたと感じられる1。\nコード julia\u0026gt; A = [ 1 0 3\r0 5 1\r3 1 9\r] 3×3 Matrix{Int64}:\r1 0 3\r0 5 1\r3 1 9 見ての通り、行列を定義する段階で既に直感的で便利だ。ここで、普通に必要とされるいくつかの関数について学ぶ。小見出しに関連記事をリンクし、別の説明は省略する。\nトレース tr() julia\u0026gt; tr(A)\r15 行列式 det() julia\u0026gt; det(A)\r-1.000000000000003 逆行列 inv() julia\u0026gt; inv(A)\r3×3 Matrix{Float64}:\r-44.0 -3.0 15.0\r-3.0 6.10623e-16 1.0\r15.0 1.0 -5.0\rjulia\u0026gt; round.(Int64, inv(A))\r3×3 Matrix{Int64}:\r-44 -3 15\r-3 0 1\r15 1 -5 対角行列と対角成分 diag(), diagm() julia\u0026gt; diag(A)\r3-element Vector{Int64}:\r1\r5\r9\rjulia\u0026gt; diagm([1,5,9])\r3×3 Matrix{Int64}:\r1 0 0\r0 5 0\r0 0 9 ノルム norm() julia\u0026gt; norm(A, 1)\r23.0 固有値 eigvals() julia\u0026gt; eigvals(A)\r3-element Vector{Float64}:\r-0.020282065792505244\r4.846013411157458\r10.174268654635046\rjulia\u0026gt; eigvecs(A)\r3×3 Matrix{Float64}:\r-0.944804 0.117887 0.305692\r-0.0640048 -0.981459 0.180669\r0.321322 0.151132 0.934832\rjulia\u0026gt; eigmax(A)\r10.174268654635046 行列分解 factorize() julia\u0026gt; factorize(A)\rBunchKaufman{Float64, Matrix{Float64}}\rD factor:\r3×3 Tridiagonal{Float64, Vector{Float64}}:\r-0.0227273 0.0 ⋅ 0.0 4.88889 0.0\r⋅ 0.0 9.0\rU factor:\r3×3 UnitUpperTriangular{Float64, Matrix{Float64}}:\r1.0 -0.0681818 0.333333\r⋅ 1.0 0.111111\r⋅ ⋅ 1.0\rpermutation:\r3-element Vector{Int64}:\r1\r2\r3\rjulia\u0026gt; svd(A)\rSVD{Float64, Float64, Matrix{Float64}}\rU factor:\r3×3 Matrix{Float64}:\r-0.305692 0.117887 -0.944804\r-0.180669 -0.981459 -0.0640048\r-0.934832 0.151132 0.321322\rsingular values:\r3-element Vector{Float64}:\r10.174268654635044\r4.846013411157461\r0.02028206579250516\rVt factor:\r3×3 Matrix{Float64}:\r-0.305692 -0.180669 -0.934832\r0.117887 -0.981459 0.151132\r0.944804 0.0640048 -0.321322 行列代数カテゴリの行列分解を参照せよ。行列の形に応じて適切な分解法を自動的に選んで分解してくれる。もちろん、条件を満たすなら、具体的な分解関数を直接使っても良い。\n行列の操作 julia\u0026gt; B = [\r1 0 1\r1 1 0\r2 1 1\r]\r3×3 Matrix{Int64}:\r1 0 1\r1 1 0\r2 1 1\rjulia\u0026gt; A + B\r3×3 Matrix{Int64}:\r2 0 4\r1 6 1\r5 2 10\rjulia\u0026gt; A - B\r3×3 Matrix{Int64}:\r0 0 2\r-1 4 1\r1 0 8\rjulia\u0026gt; A * B\r3×3 Matrix{Int64}:\r7 3 4\r7 6 1\r22 10 12\rjulia\u0026gt; A .* B\r3×3 Matrix{Int64}:\r1 0 3\r0 5 0\r6 1 9\rjulia\u0026gt; B / A\r3×3 Matrix{Float64}:\r-29.0 -2.0 10.0\r-47.0 -3.0 16.0\r-76.0 -5.0 26.0\rjulia\u0026gt; B * inv(A)\r3×3 Matrix{Float64}:\r-29.0 -2.0 10.0\r-47.0 -3.0 16.0\r-76.0 -5.0 26.0\rjulia\u0026gt; A / B\rERROR: SingularException(3) 我々が考える常識的な操作は全て通用する。割り算は、当然乗算の逆元である逆行列を掛けるのと同じで、Bのように逆行列が存在しない場合は、シンギュラー例外をレイズする。\nブロック行列 [] 他の言語と比べて、ブロック行列を非常に便利に作ることができる。\njulia\u0026gt; [A B]\r3×6 Matrix{Int64}:\r1 0 3 1 0 1\r0 5 1 1 1 0\r3 1 9 2 1 1\rjulia\u0026gt; [A;B]\r6×3 Matrix{Int64}:\r1 0 3\r0 5 1\r3 1 9\r1 0 1\r1 1 0\r2 1 1\rjulia\u0026gt; [A,B]\r2-element Vector{Matrix{Int64}}:\r[1 0 3; 0 5 1; 3 1 9]\r[1 0 1; 1 1 0; 2 1 1] 二つの行列の間にスペースを置くと横に積み上げ、セミコロンを置くと縦に積み上げる。カンマは行列を積み上げるわけではなく、一般的に配列で使用していた構文そのままで、行列の配列になる。\n全体のコード 複素行列や内積に関連する内容は省略したが、全体のコードには含まれている。\nusing LinearAlgebra\rA = [\r1 0 3\r0 5 1\r3 1 9\r]\rtr(A)\rdet(A)\rinv(A)\rround.(Int64, inv(A))\rdiag(A)\rdiagm([1,5,9])\rnorm(A, 1)\reigvals(A)\reigvecs(A)\reigmax(A)\rfactorize(A)\rsvd(A)\rB = [\r1 0 1\r1 1 0\r2 1 1\r]\rdet(B)\rrank(B)\reigvals(B)\rSymmetric(B) # |\u0026gt; issymmetric\rtranspose(B)\rB\u0026#39;\rC = [\rim im 1\r2 im 0\rim 1 2\r]\rC\u0026#39;\rB\u0026#39;B\rx = [1,2,3]\ry = [0,1,2]\rx\u0026#39;y\rA + B\rA - B\rA * B\rA .* B\rB / A\rB * inv(A)\r[A B]\r[A;B]\r[A,B]\rx\u0026#39; * y\ry * x\u0026#39; 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2412,"permalink":"https://freshrimpsushi.github.io/jp/posts/2412/","tags":null,"title":"ジュリアで線形代数パッケージを使用する方法"},{"categories":"줄리아","contents":"概要 1 Datesは、日付や時間に関連する関数をまとめたモジュールだ。一般的なプログラミングはもちろん、時系列に関する、いやそれに関係なく多くのデータを扱う上で、非常に役に立つものに違いない1。\nコード 全コード using Dates\r오늘 = DateTime(2022,3,10)\rtypeof(오늘)\rpropertynames(오늘)\r오늘.instant\rmyformat = DateFormat(\u0026#34;d-m-y\u0026#34;)\r내일 = Date(\u0026#34;11-3-2022\u0026#34;, myformat)\rDates.dayname(내일)\r일주일뒤까지 = 오늘:Day(1):DateTime(2022,3,17)\rcollect(일주일뒤까지)\rDates.Day(일주일뒤까지[end]) - Dates.Day(오늘) DateTime タイプ julia\u0026gt; 오늘 = DateTime(2022,3,10)\r2022-03-10T00:00:00\rjulia\u0026gt; typeof(오늘)\rDateTime 例えば DateTime() 関数で22年3月10日の日付を 今日 に割り当てたなら、今日 は DateTime というタイプを持つことになる。DateTime は instant というプロパティを持ち、ミリ秒単位で時間を記録している。\njulia\u0026gt; propertynames(오늘)\r(:instant,)\rjulia\u0026gt; 오늘.instant\rDates.UTInstant{Millisecond}(Millisecond(63782553600000)) フォーマット DateFormat() julia\u0026gt; myformat = DateFormat(\u0026#34;d-m-y\u0026#34;)\rdateformat\u0026#34;d-m-y\u0026#34;\rjulia\u0026gt; 내일 = Date(\u0026#34;11-3-2022\u0026#34;, myformat)\r2022-03-11 よく、東西の違いで日付が違って記述される時に使われる。\n曜日 Dates.dayname() julia\u0026gt; Dates.dayname(내일)\r\u0026#34;Friday\u0026#34; 指定された日付の曜日を返してくれる。グレゴリオ暦の不合理さのため、自分で作ると意外と難しいものがこんなものだ。\n日付のベクタ julia\u0026gt; 일주일뒤까지 = 오늘:Day(1):DateTime(2022,3,17)\rDateTime(\u0026#34;2022-03-10T00:00:00\u0026#34;):Day(1):DateTime(\u0026#34;2022-03-17T00:00:00\u0026#34;)\rjulia\u0026gt; collect(일주일뒤까지)\r8-element Vector{DateTime}:\r2022-03-10T00:00:00\r2022-03-11T00:00:00\r2022-03-12T00:00:00\r2022-03-13T00:00:00\r2022-03-14T00:00:00\r2022-03-15T00:00:00\r2022-03-16T00:00:00\r2022-03-17T00:00:00 ジュリアの日付パッケージで最も役立つ部分だと言えるだろう。上のように特定の時点間の区間をネイティブのジュリア文法そのままでベクタ化すれば、まさに想像していた通りの結果が出る。作るのは二の次で、同じ機能を持つ関数が他の言語にもあるかもしれないが、この程度に文法にうまく溶け込んで、類まれな直感性を持つことは稀だろう。\n日付の引き算 - julia\u0026gt; Dates.Day(일주일뒤까지[end]) - Dates.Day(오늘)\r7 days 当たり前のように、引き算で2つの時点の間隔を計算できる。Dates.canonicalize() を使えば、時間、分、秒単位できれいに表示できる。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/stdlib/Dates/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2410,"permalink":"https://freshrimpsushi.github.io/jp/posts/2410/","tags":null,"title":"ジュリアでの日付と時刻関連関数の使用方法"},{"categories":"줄리아","contents":"概要 1 2 The Fastest Fourier Transform in the West(FFTW)は、マサチューセッツ工科大学(MIT)のMatteo FrigoとSteven G. Johnsonによって開発された、離散フーリエ変換を計算するためのソフトウェアライブラリです。FFTの実装用にAbstractFFTs.jlというパッケージがありますが、これは自体を直接使用するためではなく、FFTW.jlなどの高速フーリエ変換を実装するときに支援するために作られたものです。\nこのパッケージは主に直接使われることを意図していません。代わりに、FFTs（例えばFFTW.jlやFastTransforms.jl）を実装するパッケージの開発者がAbstractFFTsで定義された型/関数を拡張します。これにより、同じ基本的なfft(x)とplan_fft(x)インターフェイスを持つ複数のFFTパッケージが共存できます。3\n概要 フーリエ変換: fft() $2$次元配列で列ごとに変換: fft( ,[1]) $2$次元配列で行ごとに変換: fft( ,[2]) 配列の特定の次元だけを変換: fft( ,[n₁, n₂, ...]) フーリエ逆変換: ifft() $0$ 中央の周波数: fftshift() 逆変換: ifftshift() 周波数サンプリング: fftfreq(n, fs=1) コード フーリエ変換 Juliaでは、フーリエ変換の表記として$\\mathcal{F}[f]$, $\\hat{f}$が直接コードに使われます。周波数が$100$, $200$, $350$のサイン波を$1/1000$間隔でサンプリングし、これを加算しましょう。\nusing FFTW\rusing Plots\rusing LaTeXStrings\rFs = 1000 #진동수\rT = 1/1000 #샘플링 간격\rL = 1000 #신호의 길이\rx = [i for i in 0:L-1].*T #신호의 도메인\rf₁ = sin.(2π*100*x) #진동수가 100인 사인파\rf₂ = 0.5sin.(2π*200*x) #진동수가 100인 사인파\rf₃ = 2sin.(2π*350*x) #진동수가 100인 사인파\rf = f₁ + f₂ + f₃ フーリエ変換: fft() フーリエ逆変換: ifft() 定義によると、$f$のフーリエ変換$\\mathcal{F}f$は$50$, $100$, $200$でのみ非ゼロ値を持ちます。また、離散フーリエ変換の定義により、$y$軸の周りに対称な値を得ますが、基本的には周波数が$0$の値が最初の値になっています。したがって、信号の周波数と振幅を確認することが目的なら、前半だけをプロットしてもよいでしょう。\nFs = 1000 # 샘플링 주파수\rℱf = fft(f) # 푸리에 변환\rξ = Fs*[i for i in 0:L/2-1]/L #주파수 도메인(절반)\rplot(ξ, abs.(ℱf[1:Int(L/2)])*2/L, title=L\u0026#34;Fourier transform of ▷eq10◁\u0026#34;, label=\u0026#34;\u0026#34;) xlabel!(\u0026#34;frequency\u0026#34;)\rylabel!(\u0026#34;amplitude\u0026#34;)\rsavefig(\u0026#34;fft.png\u0026#34;) $0$ 中央の周波数 フーリエ変換の出力は、基本的に周波数が$0$の値を最初に置きます。周波数が$0$の値を中央にしたい場合は、fftshift()を使います。これを元に戻す場合はifftshift()を使いますが、これはifft+shiftではなく、逆変換 + fftshift、つまりfftshift()の逆操作であるため、混乱しないようにしましょう。\np1 = plot(ξ, abs.(ℱf), title=L\u0026#34;▷eq11◁\u0026#34;, label=\u0026#34;\u0026#34;, yticks=[], xticks=[0, 100, 200, 350, 500, 1000]) p2 = plot(ξ.-500, abs.(fftshift(ℱf)), title=L\u0026#34;▷eq22◁\u0026#34;, label=\u0026#34;\u0026#34;, yticks=[], xticks=[-500,-350,-200,-100,0,100,200,350,500]) plot(p1, p2, size=(800,400))\rsavefig(\u0026#34;fftshift.png\u0026#34;) 高次元フーリエ変換 2次元フーリエ変換の値と比較するため、まず$x = [1\\ 2\\ 3\\ 4]^{T}$のフーリエ変換値を計算しておきましょう。\njulia\u0026gt; x = [1.0; 2; 3; 4]\r4-element Vector{Float64}:\r1.0\r2.0\r3.0\r4.0\rjulia\u0026gt; fft(x)\r4-element Vector{ComplexF64}:\r10.0 + 0.0im\r-2.0 + 2.0im\r-2.0 + 0.0im\r-2.0 - 2.0im fft()は2次元配列を入力として受け取ると自動的に2次元フーリエ変換を返します。または、fft(, [1,2])は、第一および第二次元での変換を計算するという意味で、同じ結果を返します。\njulia\u0026gt; y = [x x x x]\r4×4 Matrix{Float64}:\r1.0 1.0 1.0 1.0\r2.0 2.0 2.0 2.0\r3.0 3.0 3.0 3.0\r4.0 4.0 4.0 4.0\rjulia\u0026gt; fft(y)\r4×4 Matrix{ComplexF64}:\r40.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0-8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\rjulia\u0026gt; fft(y, [1,2])\r4×4 Matrix{ComplexF64}:\r40.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0-8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im したがって、各列で変換を行いたい場合は、fft(, [1])を使い、各行で変換を行いたい場合は、fft(, [2])を使います。\njulia\u0026gt; fft(y, [1])\r4×4 Matrix{ComplexF64}:\r10.0+0.0im 10.0+0.0im 10.0+0.0im 10.0+0.0im\r-2.0+2.0im -2.0+2.0im -2.0+2.0im -2.0+2.0im\r-2.0+0.0im -2.0+0.0im -2.0+0.0im -2.0+0.0im\r-2.0-2.0im -2.0-2.0im -2.0-2.0im -2.0-2.0im\rjulia\u0026gt; fft(y, [2])\r4×4 Matrix{ComplexF64}:\r4.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r12.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r16.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im 周波数サンプリング fftfreq(n, fs=1) 長さが$n$で、間隔が$fs/n$の周波数ドメインを返します。既に説明したように、フーリエ変換は最初に周波数$0$の値を置くため、fftfreq()でサンプリングした周波数の最初の値は$0$です。前半は正の周波数、後半は負の周波数です。したがって、fftshift()を使うと、インデックスに応じて昇順に並べ替えられます。\njulia\u0026gt; fftfreq(4, 1)\r4-element Frequencies{Float64}:\r0.0\r0.25\r-0.5\r-0.25\rjulia\u0026gt; fftfreq(5, 1)\r5-element Frequencies{Float64}:\r0.0\r0.2\r0.4\r-0.4\r-0.2\rjulia\u0026gt; fftshift(fftfreq(4, 1))\r-0.5:0.25:0.25 環境 OS: Windows11 Version: Julia 1.8.2, FFTW 1.5.0 http://www.fftw.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaMath/FFTW.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaMath/AbstractFFTs.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3440,"permalink":"https://freshrimpsushi.github.io/jp/posts/3440/","tags":null,"title":"ジュリアで高速フーリエ変換（FFT）を使用する方法"},{"categories":"줄리아","contents":"概要 機械学習のような分野では、計算速度の向上やメモリの節約などのために、64ビットの実数ではなく32ビットの実数が配列のデータ型として使われます。そのため、PyTorchではテンソルを作ると、基本的にテンソルのデータ型は32ビットの浮動小数点数になってます。Juliaの機械学習パッケージにはFlux.jlがあり、これで実装された人工ニューラルネットワークは、Juliaの基本配列を入力として受け取ります。テンソルのような別のデータ構造を使わないという点は利点と言えますが、データ型を手動でFloat32に設定しなければならない面倒くささもあります。以下で、デフォルトのデータ型を変更する方法を紹介します。\nコード1 ChangePrecision.jl @changeprecision マクロを使うと、begin ... endで囲まれたコード内でデフォルトのデータ型が変わります。\njulia\u0026gt; Pkg.add(\u0026#34;ChangePrecision\u0026#34;)\rjulia\u0026gt; using ChangePrecision\rjulia\u0026gt; rand(3)\r3-element Vector{Float64}:\r0.580516564576538\r0.33915094423556424\r0.3612907828959878\rjulia\u0026gt; @changeprecision Float32 begin\rrand(3)\rend\r3-element Vector{Float32}:\r0.0459705\r0.0033969283\r0.579983 環境 OS: Windows10 バージョン: Julia 1.8.2, ChangePrecision 1.0.0 https://stackoverflow.com/questions/68068823/how-to-change-default-float-to-float32-in-a-local-julia-environment\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3439,"permalink":"https://freshrimpsushi.github.io/jp/posts/3439/","tags":null,"title":"ジュリアで基本データ型を変更する方法"},{"categories":"머신러닝","contents":"定理 インプット集合Input Set $X \\ne \\emptyset$ と正定値カーネル $k: X \\times X \\to \\mathbb{R}$ が与えられているとする。学習データセットTraining Datasetを $$ D := \\left\\{ \\left( x_{i} , y_{i} \\right) \\right\\}_{i=1}^{m} \\subset X \\times \\mathbb{R} $$ とし、再生カーネルヒルベルト空間 $H_{k}$ のクラス $$ \\mathcal{F} := \\left\\{ f \\in \\mathbb{R}^{X} : f \\left( \\cdot \\right) = \\sum_{i=1}^{\\infty} \\beta_{i} k \\left( \\cdot , z_{i} \\right) \\land \\beta_{i} \\in \\mathbb{R} \\land z_{i} \\in X \\land \\left\\| f \\right\\| \u0026lt; \\infty \\right\\} \\subset H_{k} $$ を上記のように設定する。任意の目的関数 $c : \\left( D \\times \\mathbb{R} \\right) ^{m} \\to \\overline{\\mathbb{R}}$ と単調増加関数であるレギュライザーRegulizer $g : \\mathbb{R} \\to [0,\\infty)$ に対して、以下のように正則化された目的汎関数Regulized Objective Functional $L : \\mathcal{F} \\to \\overline{\\mathbb{R}}$ が定義されているとする。 $$ L (f) := c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) $$ ここで、$H_{k}$ のノルム $\\left\\| \\cdot \\right\\|$ は、$k$ の正定値性Positive Definitenessによって次のように与えられる。 $$ \\left\\| \\sum_{i=1}^{\\infty} \\beta_{i} k \\left( \\cdot , z_{i} \\right) \\right\\|^{2} := \\sum_{i=1}^{\\infty} \\sum_{j=1}^{\\infty} \\beta_{i} \\beta_{j} k \\left( z_{i} , z_{j} \\right) \\ge 0 $$\n$\\mathbb{R}$ は実数の集合であり、$\\overline{\\mathbb{R}}$ は無限大 $\\infty$ を含む拡張実数である。 $\\mathbb{R}^{X}$ は定義域が $X$ で値域が $\\mathbb{R}$ である関数を集めた関数空間である。 レギュライザーとは、データに対する過学習を防ぐためのペナルティPenalty関数である。 汎関数とは、ざっくり言えば関数自体をインプットとして受け取る関数のことである。 ノンパラメトリックNonparametric $L (f)$ を最小化する関数 $f \\in \\mathcal{F}$ は、ある $\\left\\{ \\alpha_{i} \\right\\}_{i=1}^{m} \\subset \\mathbb{R}$ に対して次のような形で表される。 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$\nセミパラメトリックSemiparametric $X$ で定義された実関数の集合が $\\left\\{ \\psi_{p} : X \\to \\mathbb{R} \\right\\}_{p=1}^{M}$ に対して行列 $\\left( \\psi_{p} \\left( x_{i} \\right) \\right)_{ip}$ のランクが $M$ であるとする。すると、$f \\in \\mathcal{F}$ と $h \\in \\span \\left\\{ \\psi_{p} \\right\\}$ に対して $$ c \\left( \\left( x_{1}, y_{1}, \\tilde{f} \\left( x_{1} \\right) \\right) , \\cdots , \\left( x_{m}, y_{m}, \\tilde{f} \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) $$ を最小化する $\\tilde{f} = f + h$ は、ある $\\left\\{ \\alpha_{i} \\right\\}_{i=1}^{m}, \\left\\{ \\beta_{p} \\right\\}_{p=1}^{M} \\subset \\mathbb{R}$ に対して次のような形で表される。 $$ \\tilde{f} (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) + \\sum_{p=1}^{M} \\beta_{p} \\psi_{p} (\\cdot) $$\n説明 可能であれば、以下の二つのポストを先に読むことをお勧めする:\n再生カーネルヒルベルト空間 サポートベクターマシン 表現者 表現者定理Representer Theoremは、古典的な機械学習、特にサポートベクターマシンの文脈で最も重要な定理の一つとして、与えられたデータに対して私たちが近似しようとしている目的関数 $f$ が適切なカーネル $k$ に対して $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$ のような形で表されるという強力な定理である。ここでカーネルのインプットの一つが $x_{i}$ で固定された関数 $$ k \\left( \\cdot , x_{i} \\right) = \\phi \\left( x_{i} \\right) (\\cdot)\\in H_{k} $$ を表現者Representerと呼ぶ。これにより、表現者定理は「再生カーネルヒルベルト空間で学習データに適合したFitted任意の関数は、表現者たちの有限な線形結合で表すことができる」と要約することができる。特に、非線形回帰のためのサポートベクターマシンのカーネルトリックは、これに正確に合致している。\nこれはディープラーニングとシーベンコの定理の関係に似ている。 データサイエンスの文脈では、表現者 $\\phi \\left( x_{i} \\right) (\\cdot)$ はフィーチャーマップFeature Mapとも呼ばれ、任意のデータ $X$ を私たちがその特徴Featureを知ることができるヒルベルト空間に移し、その有限な和で私たちが求める関数を表現できるということは、これまで私たちが学んできた多くの機械学習技術がなぜ機能してきたのかを正当化する。もちろん、数学的な保証がなく\nてもそれらの技術はそれ自体で有効であるが、表現者定理があることで、それらの技術が理論的な基盤を築くことになるという点で非常に重要である。\n目的関数とレギュライザー 定理のステートメントでは、目的関数 $c$ とレギュライザー $g$ を非常に一般的に定義しているが、実際には多くの場合、$c$ はデータと $f$ の適合度を測る平均残差二乗、つまり $$ c = {{ 1 } \\over { m }} \\sum_{i=1}^{n} \\left( y_{i} - f \\left( x_{i} \\right) \\right)^{2} $$ と見なし、$g$ は二乗セミノルムペナルティ $g \\left( \\left\\| f \\right\\| \\right) = \\lambda \\left\\| f \\right\\|^{2}$ と見なしても問題ない1。\n注意すべき点は、この式の形や単語の意味だけを見て目的関数とレギュライザーを区別してはいけないということである。表現者定理の最も代表的な応用がサポートベクターマシンであるが、ソフトマージンを許容するソフトマージンSVMで扱う最小化問題は次のようである。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\lambda \\sum_{k=1}^{n} \\xi_{k} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k} \\\\ \u0026amp; \\xi_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nここで、最適化自体だけを考えた場合、目的関数は実際には $$ {{ 1 } \\over { 2 \\lambda }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\sum_{k=1}^{n} \\xi_{k} $$ と変わらず、この場合はデータとの乖離を考える $\\sum_{k=1}^{n} \\xi_{k}$ が $c$ であり、サポートベクターマシンの超平面 $f (\\mathbf{x}) = \\mathbf{w}^{T} + b$ から導かれる ${{ 1 } \\over { 2 \\lambda }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2}$ が $g$ として読まれるべきである。この最適化の意味を数学に置くか機械学習に置くかによって混乱することがあるが、\n数学の色が強い人はSVMを「まず線形回帰を終えてから例外を設けるもの」と見なし、$\\left\\| \\mathbf{w} \\right\\|_{2}^{2}$ を最初に最小化しようとする一方で、 データサイエンスの色が強い人はSVMを「まずデータをうまく分類し、その中で最もマージンの大きな超平面を見つけるもの」と見なし、$\\sum_{k=1}^{n} \\xi_{k}$ を最初に最小化しようとするためである。 どちらの視点も十分に共感できるものであり、表現者定理の応用がSVMだけでないことを考えると、ここでは暗記術のようなものを探そうとせず、問題に応じて能動的に考え受け入れる必要がある。\n証明 2 参考文献にあるように、ノンパラメトリック表現者定理のみを証明する。\nPart 1. $f = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v$\n再生カーネルの定義: 関数 $k : X \\times X \\to \\mathbb{C}$ が以下の二つの条件を満たす場合、$H$ の再生カーネルReproducing Kernelと言う。\n(i): 表現者Representer: すべての $x \\in X$ に対して $$ k \\left( \\cdot , x \\right) \\in H $$ (ii) 再生性質Reproducing Property: すべての $x \\in X$ とすべての $f \\in H$ に対して $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) = \\delta_{x} (f) $$ 特に、すべての $x_{1} , x_{2} \\in X$ に対して以下が成り立つ。 $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; k \\left( \\cdot , x_{2} \\right), k \\left( \\cdot , x_{1} \\right) \\right\u0026gt; $$ 再生カーネル $k : X \\times X \\to \\mathbb{R}$ に対して $x \\mapsto k (\\cdot ,x)$ である(表現者)関数 $\\phi : X \\to \\mathbb{R}^{X}$ を定義する。$k$ は再生カーネルであるため、$x' \\in X$ で関数 $\\left( \\phi (x) \\right) (\\cdot)$ の関数値はすべての $x, x' \\in X$ に対して $$ \\left( \\phi (x) \\right) (x ') = k \\left( x' , x \\right) = \\left\u0026lt; \\phi \\left( x ' \\right) , \\phi (x) \\right\u0026gt; $$ である。ここで $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ は $H_{k}$ の内積である。与えられた $\\left\\{ x_{i} \\right\\}_{i=1}^{m}$ に対して、任意の関数 $f \\in \\mathcal{F}$ は $\\span \\left\\{ \\phi \\left( x_{i} \\right) \\right\\}_{i=1}^{m}$ の部分とそれに直交するすべての $j$ に対して $$ \\left\u0026lt; v , \\phi \\left( x_{j} \\right) \\right\u0026gt; = 0 $$ を満たす $v \\in \\mathcal{F}$ とある $\\left( \\alpha_{1} , \\cdots , \\alpha_{m} \\right) \\subset \\mathbb{R}^{m}$ に対して次のように表現できる。 $$ f = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v $$ ここで、 $$ \\begin{align*} L (f) :=\u0026amp; c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) \\\\ =\u0026amp; c + g \\end{align*} $$ で、$c$ は $v$ に依存しないこと、$v = 0$ のとき $f$ が $L(f)$ を最小化することを議論する。\nPart 2. $c$ と $v$ は独立である\n関数 $f = f(\\cdot)$ と再生カーネル $k \\left( \\cdot , x_{j} \\right)$ の内積は再生性質により $$ \\begin{align*} =\u0026amp; \\left\u0026lt; f , k \\left( \\cdot , x_{j} \\right) \\right\u0026gt; \\\\ f \\left( x_{j} \\right) =\u0026amp; \\left\u0026lt; \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v , \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\sum_{i=1}^{m} \\alpha_{i} \\left\u0026lt; \\phi \\left( x_{i} \\right) , \\phi \\left( x_{j} \\right) \\right\u0026gt; + 0 \\end{align*} $$ である。これは $v$ に独立であるため、$L (f) = c + g$ で学習データ $D$ と $f$ にのみ依存する $$ c = c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) $$ も $v$ に独立であることがわかる。\nPart 3. $g$ は $v = 0$ のとき最小化される\n(1): $v$ は $\\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right)$ に直交し、 (2): $g$ が単調関数であると仮定したため、 $$ \\begin{align*} \u0026amp; g \\left( \\left\\| f \\right\\| \\right) \\\\ =\u0026amp; g \\left( \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v \\right\\| \\right) \\\\ =\u0026amp; g \\left( \\sqrt{\\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v \\right\\|^{2} + \\left\\| v \\right\\|^{2}} \\right) \u0026amp; \\because (1) \\\\ \\ge\u0026amp; g \\left( \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) \\right\\| \\right) \u0026amp; \\because (2) \\end{align*} $$ を得る。明らかに $v = 0$ のとき等式が成立し、$g$ が最小化されるためには $v=0$ でなければならない。一方、Part 2で $v$ は $c$ に影響を与えることができないことが確認されたため、$v = 0$ としても問題なく、$L = c + g$ を最小化する関数 $f$ は次のような形で表現できる。 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$\n■\nWahba. (2019). Representer Theorem. https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchölkopf. (2001). A Generalized Representer Theorem. https://link.springer.com/chapter/10.1007/3-540-44581-1_27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2408,"permalink":"https://freshrimpsushi.github.io/jp/posts/2408/","tags":null,"title":"表現者の定理の証明"},{"categories":"머신러닝","contents":"定義 1 2 入力空間Input Space $X \\ne \\emptyset$ が定義域であり値域が複素数の集合 $\\mathbb{C}$ の写像 $f: X \\to \\mathbb{C}$ で構成される関数空間 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right) \\subset \\mathbb{C}^{X}$ がヒルベルト空間であるとする。\n再生核ヒルベルト空間 固定された一つのデータDatum $x \\in X$ に対して、関数 $f \\in H$ を取り出す汎関数 $\\delta_{x} : H \\to \\mathbb{C}$ を**$x$ における(ディラックの)評価汎関数**(Dirac) Evaluation Functional at $x$という。 $$ \\delta_{x} (f) := f (x) $$ 全ての $x \\in X$ において評価汎関数 $\\delta_{x}$ が連続である場合、$H$ を再生核ヒルベルト空間RKHS, Reproducing Kernel Hilbert Spaceと呼び、$H_{k}$ と表記することもある。 関数 $k : X \\times X \\to \\mathbb{C}$ が以下の二つの条件を満たす場合、$H$ の再生核Reproducing Kernelという。 (i): 表現者Representer: 全ての $x \\in X$ に対して $$ k \\left( \\cdot , x \\right) \\in H $$ (ii) 再生性質Reproducing Property: 全ての $x \\in X$ と全ての $f \\in H$ に対して $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) = \\delta_{x} (f) $$ 特に全ての $x_{1} , x_{2} \\in X$ に対して以下が成立する。 $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; k \\left( \\cdot , x_{2} \\right), k \\left( \\cdot , x_{1} \\right) \\right\u0026gt; $$ 正定値カーネル 入力空間 $X \\ne \\emptyset$ からヒルベルト空間 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ への写像 $\\phi : X \\to H$ を特徴写像Feature Mapと呼ぶ。この文脈では、$H$ を特徴空間Featureと呼ぶこともある。 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ の内積 $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; : H \\times H \\to \\mathbb{C}$ に対して、以下のように定義される関数 $k : X \\times X \\to \\mathbb{C}$ をカーネルKernelと呼ぶ。 $$ k \\left( x_{1} , x_{2} \\right) := \\left\u0026lt; \\phi \\left( x_{1} \\right) , \\phi \\left( x_{2} \\right) \\right\u0026gt; $$ $m$個のデータData $\\left\\{ x_{1} , \\cdots , x_{m} \\right\\} \\subset X$ に対して、以下のような行列 $K \\in \\mathbb{C}^{m \\times m}$ をカーネル $k$ のグラム行列Gram Matrixと呼ぶ。 $$ K := \\left( k \\left( x_{i} , x_{j} \\right) \\right)_{ij} $$ $k$ のグラム行列が正定値行列である場合、$k$ を正定値カーネルPositive Definite Kernelと呼ぶ。言い換えると、全ての $\\left\\{ c_{1} , \\cdots , c_{m} \\right\\} \\subset \\mathbb{C}$ に対して以下を満たすグラム行列を持つカーネル $k$ を正定値カーネルと呼ぶ。 $$ \\sum_{i=1}^{m} \\sum_{j=1}^{m} c_{i} \\bar{c_{j}} K_{ij} \\ge 0 $$ 説明 難しい内容だが、できるだけわかりやすく解説してみよう。\nデータ科学におけるヒルベルト空間の意味 ヒルベルト空間は内積が定義された完備空間である。通常、数学では内積とは何か特別な意味を持たせずに、単にいくつかの条件を満たす二変数スカラー関数として扱うが、機械学習の文脈では類似性の測定Measure of Similarityという概念として考えることができる。実際に、文書間の単語の頻度を比較するために使用されるコサイン類似度も内積を使用しており、別の例として三つのベクトル $$ A := \\left( 3, 0, 1 \\right) \\\\ B := \\left( 4, 1, 0 \\right) \\\\ C := \\left( 0, 2, 5 \\right) $$ がある場合、$A$ と $B$ が類似しており、$C$ とは異なると直感的に理解できる。しかし、これはまだ直感的な推論に過ぎず、内積を通して量化すると以下のようになる。 $$ A \\cdot B = 12 + 0 + 0 = 12 \\\\ A \\cdot C = 0 + 0 + 5 = 5 \\\\ B \\cdot C = 0 + 2 + 0 = 2 $$ 単に内積の絶対値が大きいか小さいかを見ただけでも、\u0026lsquo;見ればわかる\u0026rsquo;よりもはるかにデータをよく説明している。 定義で入力空間と呼んでいる $X$ には特に仮定がないことに注意する。実際のフィールドでは、どのような悪いデータを扱うか保証できない。例えば、$X$ が写真や文書データの場合、写真同士や文書同士を内積することは意味がない。 Q. $X$ が白黒写真の集合である場合、写真を行列と見なしてピクセルごとの値で内積を取れば良いのではないか？ A. それで良く、それが特徴写像 $\\phi : X \\to H$ である。この場合、$H$ は長方形 $[a,b] \\times [c,d]$ で定義された関数の空間となる。 このように考えると、カーネルの存在自体が既に\u0026rsquo;扱いにくいデータ\u0026rsquo;を私たちがよく知っている空間に持ち込むことと同じである。 上で述べた内積の意味が全て無意味であっても、内積空間ならばノルム空間であり距離空間であるため、私たちが常識的に存在すると考えるほとんどの仮定が成立する。内積 $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ によって導かれるノルム $$ \\left\\| f \\right\\| := \\sqrt{ \\left\u0026lt; f , f \\right\u0026gt; } $$ があり、ノルム $\\left\\| f \\right\\|$ によってメトリック $$ d (f,g) = \\left\\| f -g \\right\\| $$ が導かれる。 データ科学の観点からノルムはデータに対する量化そのものである。例えば、白黒写真の全てのピクセルの値を合計した値をノルムとする場合、単にこれだけで写真がどれだけ明るいか暗いかを大まかに評価できる。 データ科学の観点から距離は二つのデータがどれだけ異なるかを教えてくれる。正しいか間違っているか、同じか異なるかを区別することは言うまでもなく重要である。 これらの理由をすべて抜きにしても、数式を展開していくと内積が必要になる場合がある。関連する例をここにすべて書くと非常に散漫になるので省略する。\u0026lsquo;サポートベクターマシン\u0026rsquo;の投稿のカーネルトリックの節を参照。 なぜ関数空間なのか？ これほどまでに難しくなければならないのか？ 数学というものはほとんどの応用で「私たちが探している関数」を見つけることである。\n補間は与えられたデータの間を埋める多項式を見つけることである。 統計的回帰分析はデータを最もよく説明する直線を見つける技術である。その直線は線形関数である。 ディープラーニングはそれをうまくやれないので活性化関数などを投入して非線形関数を近似する技術である。 フーリエ変換は関数を三角関数の線形結合として表す変換である。 これらの例を一つ一つ挙げていくときりがない。再び機械学習に戻って、私たちが関数空間を考える理由は、私たちが探しているものが結局のところ関数だからである。私たちは、その形が明示的Explicitではないかもしれないが、私たちが興味を持っているものを入れるInputと、\n私たちが望む結果を出すReturn関数を求めている。例えば、数字が書かれた写真を入れたときにその数字を返す、個人情報を入れたときにローンを返済できる確率を計算するなどの関数である。このような役に立つ関数が単純であるはずがなく、それらを知っている関数たちの合成のようなものを探したいと思っている。想像してみてほしい。健康診断の結果データ $x$ を受け取り、どれだけ健康かを計算してくれる関数を $f$ とすると、 $$ f( \\cdot ) = \\sum_{k=1}^{m} \\alpha_{k} \\phi_{k} \\left( x_{k} \\right)(\\cdot) $$ のように有限個の $\\phi_{k} (x) (\\cdot)$ を基底Basisとして持つ $f$ を探しているのである。特に $\\phi (x) = k (\\cdot , x)$ に対して、ある $f$ を見つけることができるという命題がまさに表現者定理である。\n表現者定理: 再生核ヒルベルト空間内で学習データに適合したFitted任意の関数は、表現者たちの有限の線形結合で表すことができる。\n要するに、機械学習（特にサポートベクターマシンの文脈）で私たちが見つけたいものが結局は関数であるため、それらが存在する関数空間について探求することは避けられない。\nもちろん、数学的な証明がなければ動かないプログラムはこの世に存在しない。必然的な学問であるとしても、全員に必須というわけではない。数学専攻でなければ、非常に難しいことが普通であり、どうしても難しいと思う場合は大まかに読み飛ばしても良い。\n評価関数の前になぜディラックの名前がついているのか？ $$ \\delta_{x_{0}} (x) = \\begin{cases} 1 \u0026amp; , \\text{if } x = x_{0} \\\\ 0 \u0026amp; , \\text{if } x \\ne x_{0} \\end{cases} $$ 元々ディラックのデルタ関数は上記のように一点でのみ値を持つ関数として知られている。正確な定義や用途はともかく、その変形は一点でのみ$0$でないという点を保持すれば、大抵ディラックの名前がつく。この意味を理解するための例として、二つの関数 $f : \\mathbb{R} \\to \\mathbb{R}$, $\\delta_{x_{0}} : \\mathbb{R} \\to \\mathbb{R}$ とその内積として $$ \\left\u0026lt; f, \\delta_{x_{0}} \\right\u0026gt; = \\sum_{x \\in \\mathbb{R}} f (x) \\delta_{x_{0}} (x) = f \\left( x_{0} \\right) $$ を想像してみる。通常関数の内積には積分を行うが、和ではないことや全ての $x \\in \\mathbb{R}$ に対して加算することが危険であることは理解しているが、最終的には概念と感覚が一致する部分があることがわかる。\nこのセンスで、$\\delta_{x_{0}} (f)$ は上記の議論を隠して単にその結果である $x_{0}$ で評価された $f \\left( x_{0} \\right)$ を、\u0026rsquo;$x_{0}$ において一点だけを得る\u0026rsquo;関数としている。\n再生性質と呼ぶ理由 再生核ヒルベルト空間の定義を読むと非常に興味深い。通常、数学で「何かの空間」と言うと、その定義自体が「何か」が存在する空間としているが、RKHSは突然「評価汎関数が全ての点で連続である」というヒルベルト空間として定義されているためである。\nリース表現定理: $\\left( H, \\left\\langle \\cdot,\\cdot \\right\\rangle \\right)$がヒルベルト空間であるとする。$H$の線形汎関数 $f \\in H^{ \\ast }$と$\\mathbf{x} \\in H$ に対して $f ( \\mathbf{x} ) = \\left\\langle \\mathbf{x} , \\mathbf{w} \\right\\rangle$および$\\| f \\|_{H^{\\ast}} = \\| \\mathbf{w} \\|_{H}$ を満たす$\\mathbf{w} \\in H$ が一意に存在する。\nムーア-アロンサジン定理Moore-Aronsajn Theorem: 正定値カーネルが存在する場合、それに対応するRKHSが一意に存在する。\nこの定義によれば、RKHSに再生核が一意に存在するという命題さえ自明ではなく、実際にはリース表現定理によってRKHSに再生核が一意に存在することが保証される。興味深いことに、逆に再生核\nに対応するRKHSも一意に存在する。\nこれで、定義にある数式を一つ一つ詳しく見てみよう。\n元々$k : X \\times X \\to \\mathbb{C}$ において、関数$k$に入れることができるのは$x_{1}, x_{2} \\in X$だが、定義で述べたように$x$を一つ固定すると、$k$は実質的に$k : y \\mapsto k (y,x)$となる$k : X \\to \\mathbb{C}$となる。関数として扱う立場からは、片方の入力を塞いだものであり、 $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) $$ のような表現は、単に二つの関数$f (\\cdot) : X \\to \\mathbb{C}$と$k \\left( \\cdot , x \\right): X \\to \\mathbb{C}$を内積したものに過ぎない。「それがどうして$f$から出てきて、外にある内積が$x$とどう関連しているのか\u0026hellip;」と複雑に考える必要はない。$f(x) \\in \\mathbb{C}$も単に内積の結果であり、値域が複素数集合であるため、出てきた何らかの複素数に過ぎない。\nここで、再生再生, Reproducingという性質の命名について触れておきたい。Reproductionという単語自体が、その生成原理に従ってRe-(再び, 再) -produce(作る, 生)という意味を持ち、その最初の翻訳は繁殖/生殖、二番目の翻訳はコピー/複製、三番目の翻訳は再生である。繁殖は明らかに無意味であり、コピーと言うには元がない。\nしかし、$f(\\cdot)$と$k (\\cdot, x)$を内積したときに$f(x)$を得るということを、$f$が持っていた情報をカーネルによって「再生」したものと考えたらどうだろうか？私たちが時刻$t$に依存するYouTubeの動画$y(t)$という関数を持っていると想像してみよう。私たちは$y$そのものを見るのではなく、$t$が増加するにつれて再生される$\\left\\{ y(t) : t \\in [0,T] \\right\\}$自体を見ている。このような比喩から、カーネル$k$は$f$を関数そのものとしてではなく、関数値を再生してくれる「再生カーネル」と呼ばれる資格がある。\n特徴マップと不便な表記について カーネルと再生カーネルの定義をよく見ると、実際にはこれらは定義のために相互に必要としていないことがわかる。カーネルはカーネルであり、再生カーネルは再生カーネルであり、これらが一致するのは特徴マップFeature Mapが表現者Representerであるとき、つまり $$ \\phi (x) = k \\left( \\cdot , x \\right) $$ のときである。特徴マップはその名の通り、元のデータを私たちが扱いやすい形に変換してくれる変換であり、このような関数たちによって何らかの関数が表されるということは、その関数がデータから来る何らかの特徴Featureによって説明されるということと同じである。一つの問題は、ここまで直感的に何となく理解できたとしても、依然として$k \\left( \\cdot , x \\right)$のような表記が不便であり、特徴マップではなく内積から始まって別々に定義されるカーネルの動機Motiveに共感するのが難しいということである。\n特徴マップは$\\phi : X \\to H$であるため、その関数値は$x \\in X$に対応する何らかの関数$\\lambda : X \\to \\mathbb{C}$であり、これが通常混乱を招くものではない。$\\phi (x)$をもっと正確に書くと $$ \\left( \\phi (x) \\right) (\\cdot) = k \\left( \\cdot , x \\right) $$ であり、なぜこのようにしてまで点$\\cdot$を保持し、不便な表記を使用するのか疑問に思うかもしれない。ほとんどの人間は、このようにしなかった場合にもっと苦労する例を見ると、理解しやすくなる。前述したように、カーネルであれ再生カーネルであれ、結局私たちが一貫して関心を持っている空間は関数空間$H$であり、$H$の内積は関数の内積である。まず、ある関数$f$がデータ$\\left\\{ x_{i} \\right\\}_{i=1}^{m}$の表現者$\\phi \\left( x_{i} \\right)$たちの線形結合として表されるとすると $$ f (y) = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) (y) = \\sum_{i=1}^{m} \\alpha_{i} \\left( \\phi \\left( x_{i} \\right) \\right) (y) $$ となり、すでにかなり複\n雑になっていることがわかる。これに新しい関数$g$とデータ$\\left\\{ x'_{j} \\right\\}_{j=1}^{n}$を考えると $$ g (y) = \\sum_{j=1}^{n} \\beta_{j} \\left( \\phi \\left( x'_{j} \\right) \\right) (y) $$ となる。一方で、$f$と$g$の内積を使わないのであれば、内積空間を考える理由がないが、$\\left\u0026lt; f,g \\right\u0026gt;$を書くと $$ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} \\left\u0026lt; \\phi \\left( x_{i} \\right) , \\phi \\left( x'_{j} \\right) \\right\u0026gt; $$ となり、余計なものが多くなる。内積をする前には、いずれにせよ関数空間を扱う上で$y \\in X$を実際に扱うことはほとんどなく、内積をした後には、既に知っている$\\phi$と内積を続けて書く必要がある。これを見ると、 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) \\\\ g (\\cdot) = \\sum_{j=1}^{n} \\beta_{j} k \\left( \\cdot , x'_{j} \\right) \\\\ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} k \\left( x_{i} , x'_{j} \\right) $$ のような表記が面倒であるだけではないことに気がつくかもしれない。\n再生カーネルは正定値である データ$\\left\\{ x_{k} \\right\\}_{k=1}^{m}$が与えられたとすると、$k$がカーネルである場合、以下が成立する。 $$ \\begin{align*} \u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\bar{\\alpha_{i}} \\alpha_{j} k \\left( x_{i} , x_{j} \\right) \\\\ =\u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\left\u0026lt; \\alpha_{i} \\phi \\left( x_{i} \\right) , \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\u0026lt; \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) , \\sum_{j=1}^{m} \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) \\right\\|^{2} \\\\ \\ge \u0026amp; 0 \\end{align*} $$ 前述したように$\\phi : x \\mapsto k (\\cdot , x)$とすると、再生カーネル$k$はカーネルであるため正定値である。このようなカーネルの正定値性は、カーネルに関連するさまざまな性質で自然に現れる。\n関数解析以外のカーネル (1) 通常、数学でカーネルと言えば抽象代数のカーネル$\\ker$を指す。代数構造$Y$で$0$が定義されている場合、関数$f : X \\to Y$に対して$\\ker f := f^{-1} \\left( \\left\\{ 0 \\right\\} \\right)$を$f$のカーネルと言う。 (2) この概念が線形代数で特殊化されたものが線形変換のカーネルである。 カーネルが難しいと感じる場合、関数解析を専攻していない数学者にいきなりカーネルについて聞いても、十中八九(1)の意味で理解するだろう。あなたのバックグラウンドが数学に基づいているならば、当然(1)くらいは知っている必要があり、そうでなくても(2)くらいは知っているべきである。\n名前のついたカーネル 機械学習の文脈では、以下のようなカーネルが知られている。3 これらは一見カーネルのように見えないかもしれないが、カーネルの和と積が依然としてカーネルであるという事実を通じて導かれる。\nリニアカーネル: $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; $$ ポリノミアルカーネル: $c \\ge 0$ と $d \\in \\mathbb{N}$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\left( \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + c \\right) ^{d} $$ ガウシアンカーネル: $\\sigma^{2} \u0026gt; 0$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\exp \\left( - {{ \\left\\| x_{1} - x_{2} \\right\\| } \\over { 2 \\sigma^{2} }} \\right) $$ シグモイドカーネル: $w, b \\in \\mathbb{C}$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\tanh \\left( w \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + b \\right) $$ Sejdinovic, Gretton. (2014). What is an RKHS?: p7~11. http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchölkopf. (2001). A Generalized Representer Theorem. https://link.springer.com/chapter/10.1007/3-540-44581-1_27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJakkula. (2006). Tutorial on Support Vector Machine (SVM). https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2406,"permalink":"https://freshrimpsushi.github.io/jp/posts/2406/","tags":null,"title":"機械学習における政府号カーネルと再生カーネルのヒルベルト空間"},{"categories":"머신러닝","contents":"モデル 1 簡単な定義 二値分類Binary Classificationが可能なデータを最もよく区別する直線や平面を見つける方法をサポートベクターマシンという。\n難しい定義 内積空間 $X = \\mathbb{R}^{p}$ とラベリングLabeling $Y = \\left\\{ -1, +1 \\right\\}$ に対し、$n$ 個のデータを集めた学習データセットTraining Datasetを $D = \\left\\{ \\left( \\mathbf{x}_{k} , y_{k} \\right) \\right\\}_{k=1}^{n} \\subset X \\times Y$ とし、 $$ \\begin{align*} X^{+} :=\u0026amp; \\left\\{ \\mathbf{x}_{k} \\in X : y_{k} = +1 \\right\\} \\\\ X^{-} :=\u0026amp; \\left\\{ \\mathbf{x}_{k} \\in X : y_{k} = -1 \\right\\} \\end{align*} $$\nとする。あるウェイトWeight $\\mathbf{w} \\in \\mathbb{R}^{p}$ とバイアスBias $b \\in \\mathbb{R}$ を持つ線形関数 $f \\left( \\mathbf{x} \\right) = \\mathbf{w}^{T} \\mathbf{x} + b$ によって作られる超平面を $H : \\mathbf{w}^{T} \\mathbf{x} + b = 0$ とするとき、$H$ と最も距離が近い $\\mathbf{x}^{+} \\in X^{+}$ と $\\mathbf{x}^{-} \\in X^{-}$ をサポートベクターSupport Vectorといい、これらの間の距離 $\\delta$ をマージンMarginという。これに対して $$ \\begin{align*} f \\left( \\mathbf{x}^{+} \\right) =\u0026amp; +1 \\\\ f \\left( \\mathbf{x}^{-} \\right) =\u0026amp; -1 \\end{align*} $$\nを満たしながらマージンが最大になるような $\\mathbf{w} , b$ を見つける機械学習技術をサポートベクターマシンSVM, Support Vector Machineという。\n$\\mathbb{R}$ は実数の集合であり、$\\mathbb{R}^{p}$ は$p$次元ユークリッド空間である。 $X \\times Y$ は二つの集合のデカルト積を意味する。 $\\mathbf{w}^{T}$ は $\\mathbf{w}$ の転置行列であり、$\\mathbf{w}^{T} \\mathbf{x}$ は二つのベクトル $\\mathbf{w}, \\mathbf{x}$ の内積 $\\left\u0026lt; \\mathbf{w} , \\mathbf{x} \\right\u0026gt;$ である。 説明 簡単に言えば、次の図のようにオレンジ色と空色のデータを二分する線や平面を見つけることである。平面図では赤い矢印で示されているのがサポートベクターに該当する。\n図では $2$次元なので線を見つけ、$3$次元なので平面を見つけたが、さらに大きな$p$次元になると超平面を見つけなければならず、図示するのは難しくなる。しかし、このように空間を二つに分けるという点は変わらない。学習データセットで二値分類が完了すれば、新しいデータを受け取ったときも$f$ に入れて線形分類器Linear Classifierとして使えばよい。\n当然ながら、同じデータを二値分類しても、左側が右側よりも良い。右側の場合、空色のデータに対するマージンが過度である。具体的にこれを求める方法は、いずれにせよパッケージがすべて自動で処理するため、知らなくてもよい。\n学部生レベルであれば、ここまでの簡単な定義を受け入れて図で大まかに理解するだけでも、今後実際に使用する際や用語を理解する上で大きな問題はない。これより少し難しい内容、実践的な要点の要約、Pythonの例示コードなどは、国内のウェブでもよく整理された文書がたくさんある。 2 3 4\n内積空間 ご覧の通り、SVM自体は概念的にそれほど難しくはないが、数学的な定義を引き出し数式を記述した理由は、今後具体的に、理論的に話すことが多いためである。\nユークリッド空間 $\\mathbb{R}^{p}$ はもちろんベクトル空間であり、内積空間でもあり、内積空間は距離空間であるため距離空間でもある。これを強調するのは、実際のデータの世界で内積空間というのが思ったよりも良い仮定であるためである。例えば、画像や文書、分子構造などをSVMにそのまま入れてもいいのか、頭を悩ませることになる。定義では暗黙のうちに「距離が近い」やベクトルの内積が含まれる線形関数 $f$ を使用しているが、理論に近づくほど、これらの仮定を当然とすることはできない。\nサポートベクター 元々このような幾何問題では、境界Boundary上にあるものをサポートと呼ぶが、例えば最小包含円問題でも円を決定する円周上の点をサポートとする。SVMの起源となったサポートベクターも同様で、$\\mathbf{x}^{+}, \\mathbf{x}^{-}$ は二つの集合 $X^{+}, X^{-}$ の観点から見ても、$\\delta/2$ から$X^{+}, X^{-}$ の距離に位置する境界上にある。\nサポートベクターが$H$ それぞれで一意である保証はないが、今後の議論で一意性が重要ではないため、一般性を失わずに一意であると仮定しよう。$\\left\\{ \\mathbf{x}_{k} \\right\\}_{k=1}^{n}$ のマージンにはデータが存在せず、 $$ f \\left( \\mathbf{x} \\right) \\begin{cases} \\ge +1 \u0026amp; , \\text{if } \\mathbf{x} \\in X^{+} \\\\ \\le -1 \u0026amp; , \\text{if } \\mathbf{x} \\in X^{-} \\end{cases} $$ であるため、すべての$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ に対して$H$ でなければならない。\nマージンの最大化 サポートベクターは$H$ と最も近い点であるため、$\\delta/2$ との距離$H$ はサポートベクターが$\\mathbf{w}$ 方向に垂直に離れたときの距離である。このマージンは$\\mathbf{x}^{+}$ でも$\\mathbf{x}^{-}$ でも同じであり、両方とも超平面$H$ との距離が$\\delta/2$ であることは、二つのサポートベクター間の距離が $$ \\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-} $$ として表されることを意味する。ここで$\\mathbf{x}^{+} - \\mathbf{x}^{-}$ のような演算は$X$ がベクトル空間であるという仮定に基づいて許可される。$\\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-}$ の両辺に$\\mathbf{w}$ と内積を取ると、つまり$\\mathbf{w}^{T}$ を左側に掛けると$f$ の定義に従って $$ \\begin{align*} \u0026amp; \\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-} \\\\ \\implies \u0026amp; \\delta \\mathbf{w}^{T} \\mathbf{w} = \\mathbf{w}^{T} \\mathbf{x}^{+} - \\mathbf{w}^{T} \\mathbf{x}^{-} \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = \\left( \\mathbf{w}^{T} \\mathbf{x}^{+} + b \\right) - \\left( \\mathbf{w}^{T} \\mathbf{x}^{-} + b \\right) \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = +1 - (-1) \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = 2 \\\\ \\implies \u0026amp; \\delta = {{ 2 } \\over { \\left\\| \\mathbf{w} \\right\\|_{2}^{2} }} \\end{align*} $$ を得る。つまり、マージンを最大化することは目的関数 $\\left\\| \\mathbf{w} \\right\\|_{2}^{2} / 2$ を最小化することであり、要約するとSVMとは次のような最適化問題を解くオプティマイザーOptimizerである。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 \\end{matrix} \\\\ k = 1, \\cdots , n $$\n派生モデル 難しい定義に従えば、SVMは直線であれ超平面であれ、いずれにしても線形関数を見つける線形回帰モデルであるが、当然ながらここで満足するわけがない。\nソフトマージンSVM 例えば、次のようなデータが入ってきたとしよう。SVMはデータが混在している中央部分のために、これを完全に二値分類することができない。\nここで、サポートベクターのマージンにデータが存在できないという制約のもとで$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ という条件を満たさなければならなかったことに注目してみよう。この不等式を$1$ より小さい値に許容すれば、完全な二値分類ではないにしても、完全に諦めるよりは良い結果をもたらすだろう。そして、この許容を各データごとに$\\xi_{k} \\ge 0$ とすると、新たな制約条件$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k}$ を得る。このように条件が緩和されたマージンをソフトマージンSoft Marginという。\nもちろん、制約が少し緩和されたとはいえ、すべてを$\\xi_{l} = \\cdots = \\xi_{n} = 1$ にしてしまうとSVM自体を放棄してしまうことになる。これを防ぐためには、目的関数に$\\sum_{k} \\xi_{k}$ のような項を加えることがある。これは不可能な二値分類を可能にしたことに対する代償Penaltyである。もちろん、このような単純なペナルティはデータのスケールによっては全く意味がなかったり、逆に過敏に反応したりするため、$0 \\le \\sum_{k} \\xi_{k} \\le n$ そのままではなく、適切な正の数$\\lambda \u0026gt; 0$ を掛けて追加することにしよう。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\lambda \\sum_{k=1}^{n} \\xi_{k} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k} \\\\ \u0026amp; \\xi_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nカーネルトリック 例えば、上のようなデータが与えられた場合、ソフトマージンであろうとなかろうと、SVMでは決して二値分類することができないように見える。しかし、よく見ると$0$ に近い側には空色の点が集まっており、外側にはオレンジ色の点が現れていることが明らかである。この情報を活用するために、次のように$z$ 軸を新たに作ってみよう。 $$ \\phi (x,y) := (x,y, x^{2} + y^{2}) $$\n上の図は、下の図を適切にキャプチャしたものである。下はマウスで対話可能な3D空間なので、いろいろと回して見てみてください。\n元の$\\mathbb{R}^{2}$ ではデータを二分する直線を見つけるのが難しかったが、このようにデータを説明する次元を増やした$\\mathbb{R}^{3}$ では、適切な平面でデータを分類するSVMを使用できるようになった。ここで自然に思い浮かぶ疑問は、「それでは、このように便利な変換$\\phi$ をカーネルKernelと呼び、カーネルを使用する方法をカーネルトリックKernel Trickと呼ぶのか？」ということである。半分正しく、半分間違っている。$\\phi$ にさらに一歩進んで、内積まで含まれたものがカーネルである。\n再びマージンの最大化に戻って、我々に与えられた最適化問題を再検討してみよう。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 \\end{matrix} \\\\ k = 1, \\cdots , n $$\n制約条件$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ は見た目はすっきりしているが、実際にこの問題を解く際にはあまり役に立たない。元の学習データセットでの形に戻すと、$k = 1 , \\cdots , n$ に対して $$ \\begin{cases} f \\left( \\mathbf{x}_{k} \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = 1 \\\\ f \\left( \\mathbf{x}_{k} \\right) \\le -1 \u0026amp; , \\text{if } y_{k} = -1 \\end{cases} \\\\ \\implies \\begin{cases} y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = 1 \\\\ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = -1 \\end{cases} \\\\ \\implies y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 $$ でなければならない。このような制約条件自体を目的関数に反映させて、制約条件がないかのように扱う方法がラグランジュ乗数法である。$y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\ge 0$ に$\\alpha_{k} \\ge 0$ を掛けた項を元の目的関数から引いた$L(\\mathbf{w}, b)$ に対して、次の最適化問題を得る。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} - \\sum_{k=1}^{n} \\alpha_{k} \\left[ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\right] \\\\ \\text{subject to} \u0026amp; \\alpha_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\n再び強調するが、我々の目的はこの目的関数を最小化する$\\mathbf{w}, b$ を見つけることであった。$\\mathbf{w}, b$ に対する目的関数の偏微分が$0$ となる条件は次の通りである。 $$ \\begin{align*} {{ \\partial L } \\over { \\partial \\mathbf{w} }} = 0 \\implies \u0026amp; \\mathbf{w} = \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{x}_{k} \\\\ {{ \\partial L } \\over { \\partial b }} = 0 \\implies \u0026amp; 0 = \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\end{align*} $$\nこれをそのまま$L$ に代入してみると $$ \\begin{align*} \u0026amp; L(\\mathbf{w},b) \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} - \\sum_{k=1}^{n} \\alpha_{k} \\left[ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\right] \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\mathbf{w}^{T} \\mathbf{w} - \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) + \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\mathbf{w}^{T} \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{x}_{k} - \\sum_{k=1}^{n} \\alpha_{k} y_{k}\\mathbf{w}^{T} \\mathbf{x}_{k} - b \\sum_{k=1}^{n} \\alpha_{k} y_{k} - \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; - {{ 1 } \\over { 2 }} \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{w}^{T} \\mathbf{x}_{k} - b \\cdot 0 + \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} y_{i} a_{j} y_{j} \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} \\\\ =\u0026amp; \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} a_{j} y_{i} y_{j} \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} \\\\ =\u0026amp; L \\left( \\alpha_{1} , \\cdots , \\alpha_{n} \\right) \\end{align*} $$\nを得る。当然ながら、具体的な$\\mathbf{w}$ と$b$ を計算するためには、学習データ$\\left\\{ \\left( \\mathbf{x}_{k}, y_{k} \\right) \\right\\}_{k=1}^{n}$ が必要である。\nここで注目すべき点は、数式で$\\mathbf{x}_{i}$ と$\\mathbf{x}_{j}$ の内積が使用されていることである。結局のところ、最終的に、我々は内積を取らなければならず、$X$ が内積空間でなければ、このように順調に進む保証はない。逆に言えば、$X$ が内積空間でなくても、変換$\\phi$ が$X$ を内積空間に送ることができれば、その目的関数が $$ \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} a_{j} y_{i} y_{j} \\phi \\left( \\mathbf{x}_{i} \\right) ^{T} \\phi \\left( \\mathbf{x}_{j} \\right) $$ であるSVMを検討する価値がある。機械学習では、このように二つのベクトルに対する変換、内積まで含まれた関数 $$ K \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right) := \\left\u0026lt; \\phi \\left( \\mathbf{x}_{i} \\right) , \\phi \\left( \\mathbf{x}_{j} \\right) \\right\u0026gt; $$ をカーネルKernelと呼ぶこともある。[ 注: データサイエンスでは、これと混同される別のカーネルも存在する。元々、数学全般でのカーネルは、名前は同じでも全く異なる機能の関数である。 ]\n数式的にここまでの内容を受け入れることができれば、なぜカーネルではなく変換$\\phi$ を導入することをカーネルトリックと呼び、変換後に内積空間であることが保証されることが重要なのかを理解したことになる。\n条件を満たす限り、カーネルはいくつかの種類を考えることができる。特に元のSVMも線形カーネルLinear Kernel $$ K \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right) = \\left\u0026lt; \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right\u0026gt;^{1} = \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} $$ を使用したものと見なすことができる。\n参照 カーネルトリックの部分で数学的に簡単な内容を扱ったが、より深い理論に興味がある場合は、SVMを超えて以下の内容を学ぶことをお勧めする。\n機械学習におけるカーネルと再生カーネルヒルベルト空間 表現者定理の証明 コード 以下はカーネルトリックを実装したJuliaのコードである。\nstruct Sphere\rd::Int64\rend\rSphere(d) = Sphere(d)\rimport Base.rand\rfunction rand(Topology::Sphere, n::Int64)\rdirection = randn(Topology.d, n)\rboundary = direction ./ sqrt.(sum(abs2, direction, dims = 1))\rreturn boundary\rend\rusing Plots\rA = 0.3rand(Sphere(2), 200) + 0.1randn(2, 200)\rB = rand(Sphere(2), 200) + 0.1randn(2, 200)\rscatter(A[1,:],A[2,:], ratio = :equal, label = \u0026#34;+1\u0026#34;)\rscatter!(B[1,:],B[2,:], ratio = :equal, label = \u0026#34;-1\u0026#34;)\rpng(\u0026#34;raw.png\u0026#34;)\rPlots.plotly()\rϕ(z) = z[1]^2 + z[2]^2\rscatter(A[1,:],A[2,:],ϕ.(eachcol(A)), ms = 1, label = \u0026#34;+1\u0026#34;)\rscatter!(B[1,:],B[2,:],ϕ.(eachcol(B)), ms = 1, label = \u0026#34;-1\u0026#34;)\rsavefig(\u0026#34;kernel.html\u0026#34;) Jakkula. (2006). サポートベクターマシン（SVM）に関するチュートリアル. https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ratsgo.github.io/machine%20learning/2017/05/23/SVM/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-2%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0-SVM\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://hleecaster.com/ml-svm-concept/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2402,"permalink":"https://freshrimpsushi.github.io/jp/posts/2402/","tags":null,"title":"サポートベクターマシン"},{"categories":"위상데이터분석","contents":"概要 代数位相Algebraic Topologyにおいて、幾何学的な意味を考えずに単に定義だけを述べると、ベッチ数Betti Numberとは、単にチェインコンプレックスでのホモロジーグループのランクに過ぎない。問題は、このような説明がベッチ数の意味を知りたい人にとって全く役に立たず、その具体的な計算も難解であり、例を通して学ぶことも困難であることである。\nこの投稿では、少なくとも2つ目の質問に対する答え―ベッチ数をどのように計算するかについての整理とその詳細な証明を紹介する。以下に紹介される定理によれば、与えられたチェインコンプレックスに従ってある行列を見つけることができ、それに関する一連の計算プロセスを通じて、以下のような明示的Explicitな公式を導出することができる。 $$ \\beta_{p} = \\rank ?_{1} - \\rank ?_{2} $$\n本来、数学的な内容は数学を使わずに伝えられることが最も良い説明であるが、ベッチ数の場合は、その公式の導出過程の中でその根本的な原理を理解することができると考えられる。学部生程度では証明の難易度がかなり高く、追いかけるのが難しいかもしれないが、できるだけ省略せずに詳細に書いたので、少なくとも一度は試みることをお勧めする。\n定理 ホモロジーグループの定義:\n$n \\in \\mathbb{N}_{0}$ とする。アーベル群 $C_{n}$ と ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェイン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ これがすべての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、$\\mathcal{C} := \\left\\{ \\left( C_{n}, \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ をチェインコンプレックスChain Complexという。 商群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の**$n$番目のホモロジーグループ**$n$-th Homology Groupという。 ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界Boundaryまたは微分Differentialオペレーターという。 $Z_{n} := \\ker \\partial_{n}$ の要素を**$n$-サイクル**Cycles、$B_{n} := \\text{Im} \\partial_{n+1}$ の要素を**$n$-境界**Boundaryという。 フリーチェインコンプレックスの標準基底分解 チェインコンプレックス $\\mathcal{C} := \\left\\{ \\left( C_{p}, \\partial_{p} \\right) \\right\\}$ のすべての $C_{p}$ が有限ランクのフリーグループであるとする。するとすべての $p$ と $Z_{p} := \\ker \\partial_{p}$ に対して、次を満たす部分群 $U_{p}, V_{p}, W_{p} \\subset C_{p}$ と が存在する。 $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ =\u0026amp; U_{p} \\oplus Z_{p} \\end{align*} $$ $$ \\begin{align*} \\partial_{p} \\left( U_{p} \\right) \\subset \u0026amp; W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$ もちろん、$Z_{p}$ は $\\partial_{p}$ の核であるため、$\\partial_{p} \\left( V_{p} \\right) = 0$ であり、$\\partial_{p} \\left( W_{p} \\right) = 0$ である。さらに、$U_{p}$ での $\\partial_{p}$ の制限関数 ${\\partial_{p}}_{| U_{p}} : U_{p} \\to W_{p-1}$ は、次のような形のスミス標準形を持つ。 $$ \\begin{bmatrix} b_{1} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; \\cdots \u0026amp; b_{l} \\end{bmatrix} $$ ここで、$b_{i} \\in \\mathbb{N}$ であり、$b_{1} \\mid \\cdots \\mid b_{l}$ である。\nホモロジーグループの効率的な計算可能性 1 $H_{p} \\left( \\mathcal{C} \\right)$ のベッチ数を$\\mathcal{C}$ の$p$番目のベッチ数Betti Numberという。有限コンプレックス$K$ の$\\beta_{p}$ は次のようである。 $$ \\beta_{p} = \\rank Z_{p} - \\rank B_{p} $$ その具体的な値は、次のように$\\partial_{p}$ のスミス標準形によって計算することができる。図では、青い点線が$1$ の対角成分を、オレンジの実線が$1$ でない対角成分を示し、その他のすべての成分は$0$ である。2\nここで重要なのは、スミス標準形における$1$ の数$\\rank B_{p-1}$ と、ゼロベクトルの列の数$\\rank Z_{p}$ である。\n証明 3 Part 1. $B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$\n$$ \\begin{align*} Z_{p} :=\u0026amp; \\ker \\partial_{p} \\\\ B_{p} :=\u0026amp; \\text{Im} \\partial_{p+1} \\\\ W_{p} :=\u0026amp; \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\} \\end{align*} $$ と置く。特に、$W_{p}$ は$C_{p}$ の部分群となり、$\\lambda = 1$ のみを考えた場合に$B_{p} = W_{p}$ であるという点で、境界Boundary$B_{p}$ の条件を弱めたものと見なすことができるため、弱い境界Weak Boundariesと呼ばれる。\n$W_{p}$ の定義から、$\\lambda \\ne 1$ を考えると $$ B_{p} \\subset W_{p} $$ $Z_{p}$ の定義から、$\\forall z_{p} \\in Z_{p}$ は$\\partial_{p} z_{p} = 0$ であり、$Z_{p} = \\ker \\partial_{p}$ は$\\partial_{p} : C_{p} \\to C_{p-1}$ であるため $$ Z_{p} \\subset C_{p} $$ $C_{p}$ はフリーグループと仮定されているため、トーションフリー、すなわち$\\forall z_{p} \\in Z_{p} \\subset C_{p}$ に対して$\\lambda z_{p} = 0$ を満たす$\\lambda \\ne 0$ が存在しない。一方、すべての$c_{p+1} \\in C_{p+1}$ に対して $$ \\partial_{p+1} c_{p+1} = \\lambda z_{p} \\in W_{p} $$ の両辺に$\\partial_{p}$ を適用すると $$ 0 = \\partial_{p} \\partial_{p+1} c_{p+1} = \\partial_{p} \\lambda z_{p} = \\lambda \\partial_{p} z_{p} $$ であるため、$\\partial_{p} z_{p} = 0$ でなければならない。これは、$\\lambda z_{p} \\in W_{p}$ ならば$\\lambda z_{p} \\in Z_{p}$ であることを意味するため $$ W_{p} \\subset Z_{p} $$ このような考察から、次の包含関係を得る。 $$ B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p} $$\nPart 2. $W_{p} \\subset Z_{p}$ は$Z_{p}$ の直和群Direct Summandである\n$p$番目のホモロジーグループ$H_{p} \\left( \\mathcal{C} \\right) = Z_{p} / B_{p}$ の定義から $$ \\text{proj}_{1} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) $$ は剰余類$B_{p}$ に相当するだけのランクが下がった射影であり $H_{p} \\left( \\mathcal{C} \\right)$ のトーション部分群$T_{p} \\left( \\mathcal{C} \\right) \\subset H_{p} \\left( \\mathcal{C} \\right)$ に対して $$ \\text{proj}_{2} : H_{p} \\left( \\mathcal{C} \\right) \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ も射影である。 第1同型定理: 準同型写像$\\phi : G \\to G'$ が存在する場合 $$G / \\ker ( \\phi ) \\simeq \\phi (G)$$\nこれにより、$\\text{proj} := \\text{proj}_{1} \\circ \\text{proj}_{2}$ として定義された $$ \\text{proj} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ も射影である。$W_{p}$ の要素は$\\partial_{p+1} d_{p+1}$ のように表されるため、この射影$\\text{proj}$ の核は$W_{p}$ であり、すべての射影は全射Surjectionであるため、第1同型定理により $$ Z_{p} / W_{p} \\simeq H_{p} / T_{p} $$ が成立する。ここで、右辺の$H_{p}$ がどのようになっているかにかかわらず、トーション部分群$T_{p}$ で取り除いたため、トーションフリーであり、これにより、左辺の$Z_{p} / W_{p}$ もトーションフリーであることが保証される。したがって、$\\alpha_{1} , \\cdots , \\alpha_{k}$ が$Z_{p} / W_{p}$ の基底であり、$\\alpha'_{1} , \\cdots , \\alpha'_{l} \\in W_{p}$ が$W_{p}$ の基底であるとした場合、$\\alpha_{1} , \\cdots , \\alpha_{k}, \\alpha'_{1} , \\cdots , \\alpha'_{l}$ は$Z_{p}$ の基底となる。したがって、$Z_{p}$ は $$ Z_{p} = V_{p} \\oplus W_{p} $$ のように、$\\alpha_{1} , \\cdots , \\alpha_{k}$ を基底とする部分群$V_{p}$ と$W_{p}$ の直和として表現できる。\nPart 3. $Z_{p}, B_{p-1}, W_{p-1}$ の基底\nホモモルフィズムのスミス標準形: フリーアーベル群$G$ と$G'$ のランクがそれぞれ$n,m$ であり、$f : G \\to G'$ がホモモルフィズムである場合、次のような行列を持つホモモルフィズム$g$ が存在する。 $$ \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\in \\mathbb{Z}^{m \\times n} $$ ここで、$d_{1} , \\cdots, d_{r} \\in \\mathbb{N}$ であり、$d_{1} \\mid \\cdots \\mid d_{r}$、つまり$d_{k}$ は$d_{k+1}$ の約数Divisorである必要がある。\n$\\partial_{p} : C_{p} \\to C_{p-1}$ は、次のようなスミス標準形の$m \\times n$ 行列を持つ。\n$$ \\begin{matrix} \u0026amp; \\begin{matrix} e_{1} \u0026amp; \\cdots \u0026amp; e_{l} \u0026amp; e_{l} \u0026amp; \\cdots \u0026amp; e_{n} \\end{matrix} \\\\ \\begin{matrix} e'_{1} \\\\ \\vdots \\\\ e'_{l} \\\\ e'_{l} \\\\ \\vdots \\\\ e'_{m} \\end{matrix} \u0026amp; \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\end{matrix} $$\nこれにより、我々は直接的な計算を通じて次の3つを示すことになる:\n(1): $e_{l+1} , \\cdots , e_{n}$ は$Z_{p}$ の基底である。 (2): $b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ は$B_{p-1}$ の基底である。 (3): $e'_{1} , \\cdots , e'_{l}$ は$W_{p-1}$ の基底である。 補題\n$\\partial_{p}$ の定義により、一般的な$c_{p} \\in C_{p}$ に対して次が成立する。 $$ c_{p} = \\sum_{i=1}^{n} a_{i} e_{i} \\implies \\partial_{p} c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ (1): $b_{i} \\ne 0$ であるため、$Z_{p} = \\ker \\partial_{p}$ である必要十分条件は、$i = 1 \\cdots , l$ に対して$a_{i} = 0$ であることである。したがって、$e_{l+1} , \\cdots , e_{n}$ は$Z_{p}$ の基底である。 (2): すべての$\\partial_{p} c_{p} \\in B_{p-1}$ は$b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ の線形結合として表現され、$b_{i} \\ne 0$ であるため、$b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ は$B_{p-1}$ の基底である。 (3): $b_{i} e'_{i} = \\partial e_{i}$ であるため、まず$e'_{1}, \\cdots, e'_{l} \\in W_{p-1}$ である。逆に、$c_{p-1} \\in C_{p-1}$ を $$ c_{p-1} = \\sum_{i=1}^{m} d_{i} e'_{i} $$ と置き、$c_{p-1} \\in W_{p-1}$ と仮定すると、$W_{p-1}$ が$W_{p-1} = \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\}$ のように定義されていたため、$c_{p-1}$ はある$\\lambda \\ne 0$ に対して $$ \\lambda c_{p-1} = \\partial c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ の形で表現できる。係数を比較すると、$i \u0026gt; l$ に対して $$ \\lambda d_{i} = 0 \\implies d_{i} = 0 $$ を得る。したがって、$e'_{1} , \\cdots , e'_{l}$ は$W_{p-1}$ の基底である。 Part 4. \u0026lsquo;フリーチェインコンプレックスの標準基底分解\u0026rsquo;の証明\n$C_{p}$ と$C_{p-1}$ に対して、これまでの議論で登場する$e_{1} , \\cdots , e_{l}$ によって生成されるフリーグループを$U_{p}$ とすると、$Z_{p} = V_{p} \\oplus W_{p}$ であるため、$\\partial V_{p} = \\partial W_{p} = 0$ であり $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus Z_{p} \\\\ =\u0026amp; U_{p} \\oplus \\left( V_{p} \\oplus W_{p} \\right) \\end{align*} $$ を得る。ここで、$W_{p}$ と$Z_{p}$ は$C_{p}$ により一意であるが、$U_{p}$ と$V_{p}$ は必ずしも一意である必要はないことに注意されたい。\nPart 5. \u0026lsquo;ホモロジーグループの効率的な計算可能性\u0026rsquo;の証明\nPart 4により、コンプレックス$K$ に対して、次の分解が存在することが保証される。 $$ \\begin{align*} C_{p} \\left( K \\right) =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$\n直和の性質: $G = G_{1} \\oplus G_{2}$ としよう。もし$H_{1}$ が$G_{1}$ の部分群であり、$H_{2}$ が$G_{2}$ の部分群である場合、$H_{1}$ と$H_{2}$ も直和として表現でき、特に次が成立する。 $${{ G } \\over { H_{1} \\oplus H_{2} }} \\simeq {{ G_{1} } \\over { H_{1} }} \\oplus {{ G_{2} } \\over { H_{2} }}$$\n[1]: $H_{1} \\simeq G_{1}$ であり、$H_{2} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ G / G_{1} \\simeq G_{2} $$ [2]: $H_{1} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ {{ G } \\over { H_{2} }} \\simeq G_{1} \\oplus {{ G_{2} } \\over { H_{2} }}$$ Part 1で$B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$ であったため、直和の性質により $$ \\begin{align*} H_{p} \\left( K \\right) =\u0026amp; Z_{p} / B_{p} \\\\ =\u0026amp; \\left( {{ V_{p} \\oplus W_{p} } \\over { B_{p} }} \\right) \\\\ =\u0026amp; V_{p} \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [2] \\\\ =\u0026amp; \\left( {{ Z_{p} } \\over { W_{p} }} \\right) \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [1] \\end{align*} $$ を得る。ここで、$H_{p} \\left( K \\right) = \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right)$ の\n$Z_{p} / W_{p}$ はフリーパートであり $W_{p} / B_{p}$ はトーションパートである。 これにより、$K$ の$p$番目のベッチ数$\\beta_{p}$ は、次のように求められる。 $$ \\begin{align*} \\beta_{p} =\u0026amp; \\rank H_{p} \\left( K \\right) \\\\ =\u0026amp; \\rank \\left[ \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right) \\right] \\\\ =\u0026amp; \\rank \\left( Z_{p} / W_{p} \\right) + \\rank \\left( W_{p} / B_{p} \\right) \\\\ =\u0026amp; \\left[ \\rank Z_{p} - \\rank W_{p} \\right] + \\left[ \\rank W_{p} - \\rank B_{p} \\right] \\\\ =\u0026amp; \\rank Z_{p} - \\rank B_{p} \\end{align*} $$\n一方、$H_{p-1}(K)$ のトーションパートと$b_{1} | \\cdots | b_{l} \\in \\mathbb{N}$ に対しては、次のようなアイソモルフィズムが存在することが分かる。 $$ W_{p-1} / B_{p-1} \\simeq \\left( {{ \\mathbb{Z} } \\over { b_{1} \\mathbb{Z} }} \\right) \\oplus \\cdots \\oplus \\left( {{ \\mathbb{Z} } \\over { b_{l} \\mathbb{Z} }} \\right) $$ ここで、$i \\le l$ に対して$b_{i} = 1$ であること、つまり$B_{p-1}$ のランクが$l$ であることは $$ \\mathbb{Z} / b_{i} \\mathbb{Z} = \\mathbb{Z} / \\mathbb{Z} = \\left\\{ 0 \\right\\} $$ であるため、$W_{p-1}$ のランクが$l$ 分だけ減少することを覚えておく。\n■\n例 トーラス $$ \\begin{align*} \\beta_{0} =\u0026amp; 1 \\\\ \\beta_{1} =\u0026amp; 2 \\\\ \\beta_{2} =\u0026amp; 1 \\end{align*} $$\nトーラスのベッチ数は上記のように知られている。このトーラスのチェインコンプレックスが上の図のように定義されている場合、例として$\\beta_{1} = 2$ のみを計算してみよう。上で導出された公式を使用せずに単に数学的に考えて計算する方法もあるが、読めば分かる通り、頭が痛くなるほど難しい。これと対照的に、「ホモロジーを効率的に計算する」ということがどれほど便利かを見てみよう。\nホモモルフィズムのスミス標準形: フリーアーベルグループ$G$ と$G'$ に対して、$a_{1} , \\cdots , a_{n}$ が$G$ の基底であり、$a_{1}' , \\cdots , a_{m}'$ が$G'$ の基底であるとする。もし関数$f : G \\to G'$ がホモモルフィズムであれば、次を満たす唯一の整数の集合$\\left\\{ \\lambda_{ij} \\right\\} \\subset \\mathbb{Z}$ が存在する。 $$ f \\left( a_{j} \\right) = \\sum_{i=1}^{m} \\lambda_{ij} a_{i}' $$ この時行列$\\left( \\lambda_{ij} \\right) \\in \\mathbb{Z}^{m \\times n}$ を($G$ と$G'$ の基底に関する)$f$ の行列という。\n$\\beta_{1} = \\rank Z_{1} - \\rank B_{1}$ であるため、少なくとも境界行列$\\left( \\partial_{1} \\right)$ と$\\left( \\partial_{2} \\right)$ を求める必要がある。すべての$a , b, c \\in C_{1} (T)$ に対して $$ \\begin{align*} \\partial_{1} (a) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (b) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (c) =\u0026amp; v - v = 0 = 0v \\end{align*} $$ であるため $$ \\left( \\partial_{1} \\right) = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\implies Z_{1} = 3 , B_{0} = 0 $$ を得る。$Z_{p}$ は行列の右側のゼロベクターの数であり、$B_{p-1}$ は行列内の$1$ の数である。次に、$\\partial_{2}$ を考えると $$ \\begin{align*} \\partial_{2} (U) =\u0026amp; -a -b +c \\\\ \\partial_{2} (L) =\u0026amp; a + b - c \\end{align*} $$ であるため $$ \\left( \\partial_{2} \\right) = \\begin{bmatrix} -1 \u0026amp; 1 \\\\ -1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{bmatrix} \\sim \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} \\implies Z_{2} = 1 , B_{1} = 1 $$ を得る。これを総合すると、トーラスの$1$番目のベッチ数$\\beta_{1}$ は、次のように計算される。 $$ \\beta_{1} = \\rank Z_{1} - \\rank B_{1} = 3 - 1 = 2 $$ 当然ながら、この結果は、この投稿に紹介された定理に従って、フリーグループがどうであり、アイソモルフィズムがどうであるかといった、あらゆる数学的知識を駆使して得た値と一致することが保証されている。少し大胆に言えば、頭を使わずに指示された通りに計算すれば、ベッチ数、つまり「ホモロジー」を「計算」することができると要約できるだろう。もう少し良い言い方をすると、コンピュータを通じて位相数学を研究する道が開かれたということだ。\nMunkres. (1984). Elements of Algebraic Topology: p58.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p104.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (1984). Elements of Algebraic Topology: p58~61.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2399,"permalink":"https://freshrimpsushi.github.io/jp/posts/2399/","tags":null,"title":"ホモロジーグループのベッチ数"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 $2$キュービット $\\ket{a, b} = \\ket{a} \\otimes \\ket{b}$に対して 交換 ゲートexchange gate $\\text{ex}$を次のように定義する。\n$$ \\begin{align*} \\text{ex} : (\\mathbb{C}^{2})^{\\otimes 2} \u0026amp;\\to (\\mathbb{C}^{2})^{\\otimes 2} \\\\ \\ket{a, b} \u0026amp;\\mapsto \\ket{b, a},\\quad \\forall a,b \\in \\left\\{ 0, 1 \\right\\} \\end{align*} $$\n$$ \\text{ex} (\\ket{a} \\otimes \\ket{b}) = \\ket{b} \\otimes \\ket{a} $$\n説明 交換ゲートは二つのキュービットの状態を互いに交換する。具体的な入出力は次の通りである。\n$$ \\text{ex} (\\ket{00}) = \\ket{00} \\\\[0.5em] \\text{ex} (\\ket{01}) = \\ket{10} \\\\[0.5em] \\text{ex} (\\ket{10}) = \\ket{01} \\\\[0.5em] \\text{ex} (\\ket{11}) = \\ket{11} $$\n行列表現は次のようである。\n$$ \\text{ex} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p97\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3429,"permalink":"https://freshrimpsushi.github.io/jp/posts/3429/","tags":null,"title":"交換ゲート"},{"categories":"줄리아","contents":"概要 Juliaでは、複数のデバイスに計算タスクをスケジューリングする方法を紹介する1。正直、自分もよくわからない。\nコード using Distributed\rip_ = []\rfor last in [160,161,162,163,164,32,33,34,35,36,43,44,45,46,47]\rpush!(ip_, join([155,230,211,last],\u0026#39;.\u0026#39;))\rend\rsort!(ip_)\rfor ip in ip_\raddprocs([(\u0026#34;chaos@\u0026#34; * ip, 8)]; dir =\u0026#34;/home/chaos\u0026#34;, exename = \u0026#34;julia\u0026#34;) #add slave node\\\u0026#39;s workers\rprintln(\u0026#34;ip $ip\u0026#34; * \u0026#34; passed\u0026#34;)\rend\rnworkers()\r@everywhere function f(n)\rreturn n^2 - n end\rA = pmap(f,1:20000)\rX = []\r@async @distributed for i in 1:200\rprint(f(i))\rpush!(X, f(i))\rend pmapはうまくいくけど、@distributedはダメだ。\n環境 OS: Windows julia: v1.7.0 https://thomaswiemann.com/assets/teaching/Fall2021-Econ-31720/Econ_31720_discussion_6.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2398,"permalink":"https://freshrimpsushi.github.io/jp/posts/2398/","tags":null,"title":"ジュリアでの分散コンピューティングの方法"},{"categories":"줄리아","contents":"概要 Juliaでは、多次元配列を参照するためのインデックスタイプであるCatesianIndexを提供している1。もちろんCatesianという名前は、集合の積であるデカルト積から来ている。\nコード julia\u0026gt; M = rand(0:9, 4,4)\r4×4 Matrix{Int64}:\r9 3 7 0\r8 6 2 1\r3 8 4 9\r5 6 8 2 例えば、行列Mの3行4列目の要素、9にアクセスしたいとしよう。\njulia\u0026gt; pt = (3,4)\r(3, 4)\rjulia\u0026gt; M[pt]\rERROR: LoadError: ArgumentError: invalid index: (3, 4) of type Tuple{Int64, Int64}\rjulia\u0026gt; M[pt[1],pt[2]]\r9 直感的には、タプルpt = (3,4)をそのまま使えば良さそうだが、プログラミングに慣れている人なら、この方法に問題があることがわかるだろう。一般的に、このような二次元配列、特に行列を参照する時には、pt[1],pt[2]のように、二つの整数をはっきりと分けて入れなければならない。\njulia\u0026gt; pt = CartesianIndex(3,4)\rCartesianIndex(3, 4)\rjulia\u0026gt; M[pt]\r9 ありがたいことに、Juliaではこのインデックスを丸ごと渡すことができるCatesianIndexが提供されている。タプルをそのままCatesianIndexに変換して参照すれば、望んでいた結果を得ることができる。\n全コード M = rand(0:9, 4,4)\rpt = (3,4)\rM[pt]\rM[pt[1],pt[2]]\rpt = CartesianIndex(3,4)\rM[pt] 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/arrays/#Base.IteratorsMD.CartesianIndex\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2394,"permalink":"https://freshrimpsushi.github.io/jp/posts/2394/","tags":null,"title":"ジュリアの多次元インデックス"},{"categories":"줄리아","contents":"概要 Juliaでは、\u0026amp;\u0026amp;と||は論理積、論理和だけでなく、ショートサーキット評価Short-circuit Evaluationを実行する1。例えば、A \u0026amp;\u0026amp; BはAとBが両方とも真の時に真を返すが、実際にはAが偽なら、Bが真か偽かを見る必要はなく、A \u0026amp;\u0026amp; Bは偽になる。ショートサーキット評価は、その見る必要のないBを実際に見ずに済ますことだ。Bに対する計算を省略することにより、場合によっては速度が改善される。\n見てほしい 条件文を簡潔に書く方法 速度比較 M = rand(0:2, 10^4, 10^4);\rprint(first(M))\r@time if first(M) == 2 \u0026amp;\u0026amp; sum(M) \u0026lt; 1000\rprint(\u0026#34;!!\u0026#34;)\rend\r@time if sum(M) \u0026lt; 1000 \u0026amp;\u0026amp; first(M) == 2\rprint(\u0026#34;!!\u0026#34;)\rend 二つの条件文はsum(M) \u0026lt; 1000とfirst(M) == 2の順序が変えられただけで、正確に同じ作業を行う。しかし、first(M) == 2は行列Mの最初の要素が2かどうかだけをチェックし、sum(M)は全要素を走査して加算するため、相対的に計算に時間がかかる。\njulia\u0026gt; M = rand(0:2, 10^4, 10^4);\rjulia\u0026gt; print(first(M))\r0 もしMの最初の要素が上記のように0なら、sum(M) \u0026lt; 1000かどうかを確認するためにsum(M)を計算する必要はない。その速度は、単に順序を変えるだけで有意な差が出ることがある。\njulia\u0026gt; @time if first(M) == 2 \u0026amp;\u0026amp; sum(M) \u0026lt; 1000\rprint(\u0026#34;!!\u0026#34;)\rend\r0.000009 seconds\rjulia\u0026gt; @time if sum(M) \u0026lt; 1000 \u0026amp;\u0026amp; first(M) == 2\rprint(\u0026#34;!!\u0026#34;)\rend\r0.040485 seconds (1 allocation: 16 bytes) 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/control-flow/#Short-Circuit-Evaluation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2392,"permalink":"https://freshrimpsushi.github.io/jp/posts/2392/","tags":null,"title":"ジュリアのショートサーキット"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義 集合 $\\left\\{ 0, 1 \\right\\}$ の元を ビットbitと呼ぶ。集合 $\\left\\{ 0, 1 \\right\\}^{n}$ の元を $n$ビット$n$bitと呼ぶ。\n説明 ビットは binary digitの略称である。通常、「$0$または$1$の値を取りうるもの」と説明される。古典コンピュータが処理する最小の情報単位であり、コンピュータの回路では$1$は電気信号があることを、$0$は電気信号がないことを意味する。\n量子コンピュータで処理される情報の最小単位は、ビットにクォンタムを付けて quantum bit量子ビットと呼ばれる。\n関連項目 ブール関数 量子ビット ","id":3422,"permalink":"https://freshrimpsushi.github.io/jp/posts/3422/","tags":null,"title":"ビット: 古典的なコンピュータにおける情報の基本単位"},{"categories":"줄리아","contents":"概要 ジュリアの基本組み込み関数は知れば知るほど便利だ。早速、例を見て学ぼう。\nコード x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rargmin(x)\rargmax(x)\rfindmin(x)\rfindmax(x)\rextrema(x)\rfindfirst(x .== 3)\rfindlast(x .== 3)\rfindall(x .== 3)\rfindnext(x .== 3, 5)\rfindprev(x .== 3, 5) 最適解 argmin(),argmax(),findmin(),findmax(),extrema() 最適解を見つける。\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; argmin(x)\r9\rjulia\u0026gt; argmax(x)\r7\ry = [7, 8, 9, 9, 7, 9]\rjulia\u0026gt; argmax(y)\r3\rjulia\u0026gt; findall(y.==maximum(y))\r3-element Vector{Int64}:\r3\r4\r6 argmin(),argmax() はただの最適解、つまり値が最も大きく小さい場所のインデックスを返す。そんなインデックスが複数ある場合は最も小さいものを返す。だから、実際は argmax(x) $= \\min(\\argmax(x))$だ。本当の$\\argmax$は、maximum() と下の findall() 関数と一緒に使えばいい。\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findmin(x)\r(2, 9)\rjulia\u0026gt; findmax(x)\r(12, 7) findmin(),findmax() は最適解とその値まで返す。\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; extrema(x)\r(2, 12) ちなみに extrema() はインデックスではなく、その値だけを返す。これは R の range() 関数と同じだ1。\n条件を満たした最初・最後のインデックス findfirst(), findlast() x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findfirst(x .== 3)\r1\rjulia\u0026gt; findlast(x .== 3)\r8 3が存在する最初と最後のインデックスを見つけた。配列の形が大まかに予想できる場合、これらを使ってコードの速度を向上させることができるだろう。\n条件を満たした全てのインデックス findall() x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findall(x .== 3)\r3-element Vector{Int64}:\r1\r6\r8 使いやすく、一般的なプログラミングで最も役に立つ関数だ。maximum(), minimum()と一緒に使えば、全ての $\\text{argmin}\n条件を満たした特定範囲のインデックス findnext(), findprev() julia\u0026gt; findnext(x .== 3, 5)\r6\rjulia\u0026gt; findprev(x .== 3, 5)\r1 前後である程度は例外として検索する必要がある時もある。例えば、配列の最初の要素と同じ要素を探す時にfindall()を使うと、その最初の要素も見つかってしまうので、煩わしいことになるかもしれない。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/collections/#Base.extrema\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2390,"permalink":"https://freshrimpsushi.github.io/jp/posts/2390/","tags":null,"title":"ジュリアのfind関数들"},{"categories":"줄리아","contents":"概要 1 ジュリアでは、関数名の最後に感嘆符Bang!を追加することをバンク規約と呼ぶ。これらの関数は、与えられた引数を変更する特徴がある。\nコード function add_1!(x)\rx .+= 1\rreturn x\rend\rfoo = [2,5,-1]\radd_1!(foo)\rfoo 例えば、上のコードを実行すると、以下の結果が得られる。\njulia\u0026gt; foo = [2,5,-1]\r3-element Vector{Int64}:\r2\r5\r-1\rjulia\u0026gt; add_1!(foo)\r3-element Vector{Int64}:\r3\r6\r0\rjulia\u0026gt; foo\r3-element Vector{Int64}:\r3\r6\r0 配列fooは関数の外で定義され、「add_1!()」によって要素が$1$ずつ増えて返されただけでなく、引数自体が変更された。\n説明 代表的なメソッドであるpop!()は、配列の最後の要素を削除しつつ返すが、この関数が元の配列を変更できない場合、MatlabやRのように広く認知されているデータ構造を使用するのが難しく、一般的なプログラミングに慣れているユーザーにとっては非常に不便だったかもしれない。\nPythonで関数ではなくメソッドを使用したときにクラスのデータまで変更される感覚と同様に捉えればいい。ジュリアは言語設計上クラスをサポートしていないので、正確な説明ではないが、Pythonでメソッドを使っていた時が便利だった時に役立つことができる。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/style-guide/#bang-convention\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2388,"permalink":"https://freshrimpsushi.github.io/jp/posts/2388/","tags":null,"title":"ジュリアの感嘆符の規約"},{"categories":"위상데이터분석","contents":"定義 最小包含円 $n \u0026gt; d$ としよう。$d$次元のユークリッド空間で与えられた有限な集合$P = \\left\\{ p_{k} \\right\\}_{k=1}^{n} \\subset \\mathbb{R}^{d}$に対して、以下のような最適化問題を最小包含円問題Smallest Enclosing Disk Problemという。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; r \\ge 0 \\\\ \\text{subject to} \u0026amp; \\left\\| c - p_{k} \\right\\|_{2} \\le r \\end{matrix} \\\\ c \\in \\mathbb{R}^{d} , k = 1, \\cdots , n $$\nヒント これは正確には定理と呼ぶほどのものではないが、この問題を扱う際に知っておくと良い事実である。\n$P$がアフィン独立である場合、円の境界には$P$の点が最少で$2$から最大で$d+1$個まで存在する。つまり、点が重なっていたり、3点以上が一直線上にあるような場合を除き、正確に$2 \\le m \\le d+1$個の点で最小包含円が一意に定まる。 例えば、$d = 2$次元平面では、円は正確に3つの点で一意に定まる。 一般に$n \u0026gt; d+1$個の点が与えられた場合、アルゴリズムAlgorithmではなく明示的な公式Explicit Formulaを見つけることは不可能とされている。境界上に最小包含円を一意に定める点をサポートSupportと呼ぶが、単に点を持っているだけでは誰がサポートかを知る方法がないためである。 このように境界上の点をサポートと呼ぶのは、幾何学の問題で一般的である。サポートベクターマシンでは、サポートが境界上の点を意味する。 したがって、この問題を解くアルゴリズムを開発するということは、境界上のサポートを見つけることを保証するか、それを迅速に見つける方法に関する研究と言っても過言ではない。ただし、現在使用されているアルゴリズムでは、その根本的なアイデアはほぼウェルツルのものに基づいており、その改良や変種をまとめて単にウェルツルアルゴリズムWelzl Algorithmと呼んでいる1。少なくともこの投稿で紹介されている後続の研究はすべてその系統に属している。 解法 ウェルツルアルゴリズム 2 ウェルツルアルゴリズムWelzl Algorithmは、最小包含円問題を解く再帰的なRecursive解法である。基本的には、点を一つずつ追加したり削除したりしながらサポートを見つけ、そのようにして得られた円が与えられたすべての点を包含しているかどうかを繰り返し確認する。\n点が$n \\le d+1$個の場合にそれらを包含する円を正確に見つけることは比較的簡単であるため、そのような関数が存在すると仮定する。実際の実装では、これが思ったほど単純ではないが、最小包含円問題の核心ではない。\n擬似コード 3 $(c,r)$ = welzl$\\left( P, S \\right)$\nInput: 与えられた点の集合$P = \\left\\{ p_{k} \\right\\}_{k=1}^{n} \\subset \\mathbb{R}^{d}$とサポーターの候補$S \\subset P$を受け取る。ヒントで述べたように、$\\left| S \\right| \\le d+1$である。 Output: $P$のすべての点を包含する最も小さい円の中心$c$と半径$r$のタプル$(c,r)$を得る。中心が$c$で半径が$r$の閉じた球を$D = B \\left[ c,r \\right]$と表す。 $(c,r)$ = trivial$\\left( S \\right)$\nInput: $\\left| S \\right| \\le d+1$の点の集合$S \\subset P$を受け取る。 Output: $S$のすべての点を包含する最も小さい円の中心$c$と半径$r$のタプル$(c,r)$を得る。welzlに比べて単純であるという仮定に従い、trivialの擬似コードは別途記述しない。 function welzl$\\left( P, S \\right)$\n$S := \\emptyset$\nif $P = \\emptyset$ or $\\left| S \\right| = d+1$ then\nreturn trivial$\\left( S \\right)$\nelse\nchoose $p \\in P$\n$D := $ welzl$\\left( P \\setminus \\left\\{ p \\right\\}, S \\right)$\nif $p \\in D$ then\nreturn $D$\nend if\nend if\nreturn welzl$\\left( P \\setminus \\left\\{ p \\right\\} , S \\cup \\left\\{ p \\right\\} \\right)$\nend function\nwelzlは再帰関数として記述されるため、実際のプログラムではtrivialから計算が始まることになる。welzlの擬似コードで登場するchooseは、choose$x \\in X$のように書かれ、集合$X$から要素$x$を一様ランダムに選ぶキーワードKeywordである。\n最後にwelzl$\\left( P \\setminus \\left\\{ p \\right\\} , S \\cup \\left\\{ p \\right\\} \\right)$をリターンすること自体が、$P$にある点を一つずつ取り除いて$S$に入れてみてサポートを見つけるプロセスであることが分かれば、このアルゴリズムを理解したことになる。\n説明 最小包含円の制約条件を文字通りに解釈してみよう。$\\left\\| c - p_{k} \\right\\|_{2} \\le r$を満たす$c$と$r$を見つけるということは、少なくとも与えられた点をすべて包むことができる中心Centre$c$と半径Radius$r$を見つけることを意味する。しかし、ユークリッド空間は非常に広いため、$c$がどこであっても$r$を無制限に大きくすれば、この制約条件は必ず満たせる。当然のことながら、私たちの関心事は、それをしながら$r$を最小化すること、つまり与えられた点を包む最も小さい球を見つけることである。\n単純化の問題点 包含Enclosingは、数学全般で広く使われている表現ではないが、発音そのままにインクロージングと言うにはあまりに長くて感じがこないため、任意に翻訳した。そもそもインクロージングEnclosing自体がバウンディングBoundingを圧倒しているわけではない。また、円盤Diskも内部を含む閉じた球の意味で使われたが、実際の英語表現では単にボールBallやスフィアSphereもよく使われる。単語や翻訳にこだわらないようにしよう。\n歴史 早くもライムント・ザイデルRaimund Seidelによって線形計画法に基づく解法が知られていた。\n1991年にエモ・ヴェルツルEmo Welzlが彼の論文で再帰的なRecursiveアルゴリズムを提案し、いわゆるSOTAState Of The Artを記録した。2022年現在でも、一般的な最小包含ディスク問題ではこのヴェルツルのアルゴリズムが最も優れているとされており、以降の多くの研究はこれを改良する形で行われてきた。\n1999年にはベルント・ゲルトナーBernd Gärtnerが、ヴェルツルのアルゴリズムに従いつつも、二次計画法Quadratic Programmingの応用を導入してこれを改善した。4 彼のコードはC++で書かれており、チューリッヒ連邦工科大学のウェブサイト5で実際の実装を見ることができる。\n2003年にはカスパー・フィッシャーKaspar Fischerが、ゲルトナーとの研究で線形計画法のシンプレックス法で登場するブランドのルールを導入し、高速なコードを作成した。6 2013年にはトーマス・ラーソンThomas Larssonが、近似的なものではなく、速度と堅牢性Robustnessを備えた方法を提案した。7\nここまで紹介された研究を参照すると、ヴェルツル-ゲルトナー-フィッシャーと続く大きな流れを確認することができる。\n応用 ヴェルツルアルゴリズムの代表的な応用は、チェックコンプレックスの構築である。\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p73~75.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWelzl. (1991). Smallest enclosing disks (balls and ellipsoids). https://doi.org/10.1007/BFb0038202\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Smallest-circle_problem#Welzl's_algorithm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGärtner. (1999). Fast and robust smallest enclosing balls. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.5783\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://people.inf.ethz.ch/gaertner/subdir/software/miniball.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKaspar Fischer. (2003). Fast Smallest-Enclosing-Ball Computation in High Dimensions. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.5783\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThomas Larsson. (2013). Fast and Robust Approximation of Smallest Enclosing Balls in Arbitrary Dimensions. https://doi.org/10.1111/cgf.12176\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2385,"permalink":"https://freshrimpsushi.github.io/jp/posts/2385/","tags":null,"title":"ベルツルアルゴリズム: 最小内包ディスク問題の解法"},{"categories":"줄리아","contents":"概要 ジュリアで、viewは配列のサブアレイを素早く参照させるデータ構造だ。実際に使う立場から見れば面倒で差がないように見えるけど、怠惰に参照されてもっと軽い配列を返す。だから、基本的なレベルで最適化されたジュリアのコードでは、@viewsというマクロを簡単に見つけることができる。\nコード 行列Mのサブマトリックスを参照してみよう。\n関数形式：view() view(A, inds...)\nAのinds...に従ったviewを返す。 しかし、この形式は一般的にコードを読みにくくするため、好まれない。次のマクロを使用すると、viewを使用しながらも基本的なジュリアの文法と大きな違いはない。\nマクロ：@view @viewマクロは、サブアレイを参照する文脈のコードにviewが適用されたかのように替えるマクロだ。\nブロック全体に適用：@views @viewsマクロは、続くブロック全体に@viewを適用する。これのおかげで、viewなしで快適に書いた関数の前に@views f(x) ... endと@viewsだけを付ければ、自動的にviewが適用される。\n全体のコード 速度比較 fcopy()とfview()は、まったく同じ機能を持つ関数だが、速度に違いがある。一見、速度は似ているように見えるが、ほとんどがコンパイル時間だ。これを除いて、単純な実行時間だけを比較すると、約4倍の差がある。\n環境 OS: Windows julia: v1.7.0 ","id":2384,"permalink":"https://freshrimpsushi.github.io/jp/posts/2384/","tags":null,"title":"ジュリアで部分配列を迅速に参照する方法"},{"categories":"위상데이터분석","contents":"ビルドアップ 難しい内容ですが、できるだけ理解しやすいように、すべての計算と説明を省略せずに丁寧に残しました。ホモロジーに興味がある方は、ぜひお読みください。\n実際に、私たちが興味を持っている位相空間 $X$ があり、これが特定のシンプリシャルコンプレックスに従って$\\Delta$-コンプレックス構造を通して表現されるとしましょう。小さな例として、上の図では右側のトーラスが $X$ であり、左側がシンプリシャルコンプレックスに相当します。\nシンプレックスの定義:\nアフィン独立な $v_{0}, v_{1} , \\cdots , v_{n} \\in \\mathbb{R}^{n+1}$ の凸包を**$n$-シンプレックス** $\\Delta^{n}$ と呼び、ベクトル $v_{k}$ を頂点と呼びます。数式的には以下のようになります。 $$ \\Delta^{n} := \\left\\{ \\sum_{k} t_{k} v_{k} : v_{k} \\in \\mathbb{R}^{n+1} , t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$ $\\Delta^{n}$ から一つの頂点が除かれて作られる $n-1$-シンプレックス $\\Delta^{n-1}$ を $\\Delta^{n}$ の面と呼びます。$\\Delta^{n}$ のすべての面の和集合を $\\Delta^{n}$ の境界と呼び、$\\partial \\Delta^{n}$ と表します。 シンプレックスの内部 $\\left( \\Delta^{n} \\right)^{\\circ} := \\Delta^{n} \\setminus \\partial \\Delta^{n}$ をオープンシンプレックスと呼びます。 ここで、シンプリシャルコンプレックスとはシンプレックスで構成されるコンプレックスで、具体的には以下のようなCWコンプレックスで構成されているとしましょう。\n$n$-セルの定義:\n以下のように定義された $D^{n} \\subset \\mathbb{R}^{n}$ を $n$-ユニットディスクと呼びます。 $$ D^{n} := \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{n} : \\left\\| \\mathbf{x} \\right\\| \\le 1 \\right\\} $$ $D^{n} \\setminus \\partial D^{n}$ とホモトピー同値な開集合 $e^{n}$ を $n$-セルとも呼びます。 CWコンプレックスの定義:\n離散的な集合 $X^{0} \\ne \\emptyset$ を**$0$-セル**とみなします。 $n$-スケルトン $X^{n}$ は $X^{n-1}$ に$n$-セル $e_{\\alpha}^{n}$ を $\\phi_{\\alpha} : S^{n-1} \\to X^{n-1}$ で結合することによって作られます。 $X := \\bigcup_{n \\in \\mathbb{N}} X^{n}$ が弱位相を持つ位相空間になるとき、$X$ をセルコンプレックスと呼びます。 定義 1 $\\Delta$-コンプレックス構造を持つ位相空間 $X$ が与えられているとしましょう。\n$X$ のオープン $n$-シンプレックスである$n$-セル $e_{\\alpha}^{n}$ を基底を持つ自由アーベル群 $\\Delta_{n} (X)$ と表しましょう。$\\Delta_{n} (X)$ の要素を**$n$-チェインと呼び、係数 $k_{\\alpha} \\in \\mathbb{Z}$ に対して以下のような形式的和で表します。 $$ \\sum_{\\alpha} k_{\\alpha} e_{\\alpha}^{n} $$ 一方、CWコンプレックスの定義から、各 $n$-セル $e_{\\alpha}^{n}$ にはそれに対応する特性写像** $\\sigma_{\\alpha} : \\Delta^{n} \\to X$ が存在するため、単に次のように表すこともあります。 $$ \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} $$ 次のように定義される準同型 $\\partial_{n} : \\Delta_{n} (X) \\to \\Delta_{n-1} (X)$ を境界準同型と呼びます。ここで、$\\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right]$ は、$\\sigma_{\\alpha}$ の $X$ の $n-1$-シンプレックス に対する制限関数であることを意味します。 $$ \\partial _{n} \\left( \\sigma_{\\alpha} \\right) := \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right] $$ 3. 商群 $\\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $H_{n}^{\\Delta}$ と表し、$H_{n}^{\\Delta}$ はホモロジーグループであるため、$X$ の第 $n$ シンプリシャルホモロジーグループと呼びます。\n群 $0$ は $\\left\\{ 0 \\right\\}$ で定義されたマグマです。つまり、空の代数構造です。 準同型 $\\partial^{2} = 0$ はゼロ準同型です。 $\\text{Im}$ は像です。 $\\ker$ はカーネルです。 集合でハット表記 $\\hat{v}_{i}$ は、次のように $v_{i}$ だけを除くことを意味します。 $$ \\left\\{ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right\\} := \\left\\{ v_{1} , \\cdots , v_{n} \\right\\} \\setminus \\left\\{ v_{i} \\right\\} $$ 説明 定義に文字が多いので、理解する前に目に入りにくいのは普通です。血となり肉となる説明なので、丁寧に読むようにしましょう。個人的に勉強している間に苦労した部分をできるだけわかりやすく書くように努めました。\n$\\Delta_{n} (X)$ の要素をなぜチェーンと呼ぶのか？ $$ \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} $$ のような記法で $\\sigma_{\\alpha} : \\Delta^{n} \\to X$ を考えることで、これで $e_{\\alpha}^{n}$ が $\\Delta^{n}$ の要素なのか $X$ の要素なのかといったことはあまり考える必要がなくなりました。$n=2$ で全ての係数が $k_{\\alpha} = 1$ の場合、幾何学的に想像できる例として、以下の図の右側のような図形 $\\sum_{i=1}^{7} \\sigma_{i}$ を考えてみましょう。\nここで鎖という表現が理解できれば幸いですが、そうでなくても実際にはあまり関係ありません。とにかく後で重要なのは、それぞれの $n$-チェイン $\\Delta_{n} (X)$ でチェーンコンプレックスを構築することです。\n$\\Delta_{n} (X)$ は本当にグループなのか？ 非常に重要ですが、定義でチェインを説明するときに、形式的和という表現を使いました。これは $\\Delta_{n} (X)$ の要素を説明したに過ぎず、$\\Delta_{n} (X)$ 上で定義された二項演算ではありません。形式的和という言葉が示すように、これはあくまで形式的なものです。小学校の時に使っていた記法を借りてくれば、\n2😀 + 💎 - 3🍌\rのように、とりあえずその位置を絵などで埋めたものと考えても問題ありません。上の式は数学的には意味がありません。なぜなら、笑顔 😀 の2倍が何であり、そこに宝石 💎 を加えることが何であり、バナナ 🍌 を3つ引くことが何なのか、定義されておらず、定義するのも困難だからです。これらを扱うのが難しい状況は、正確に $\\sum_{\\alpha} k_{\\alpha} e_{\\alpha} \\simeq \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ で\n(そもそも加算を定義できない)オープンシンプレックス $e_{\\alpha}^{n}$ 対応する $\\sigma_{\\alpha}$ が関数である（関数そのものなのか関数値を指しているのかがわかりにくい） それを任意の整数倍して加算した $-3 e_{1}^{n} + 7 e_{2}^{n} \\simeq -3 \\sigma_{1} + 7 \\sigma_{2}$ の意味がわからない という問題と同じです。代数的構造どころか、この集合がどのように見えるのかすらわかりにくいですが、幸いにもこれらの問題は $\\Delta_{n} (X)$ にとっては関係がありません。もし\n$\\sigma=$2😀 + 💎 - 3🍌\rが $\\Delta_{n} (X)$ の要素、つまり $n$-チェインであるとするならば、これらの要素の逆元は、すべての係数 $k_{\\alpha} \\in \\left( \\mathbb{Z} , + \\right)$ の逆元 $-k_{\\alpha} \\in \\left( \\mathbb{Z} , + \\right)$ を係数として持つ\n$-\\sigma=$ (-2)😀 + (-1)💎 + (-(-3))🍌\rで定義するだけで十分です。これにより $\\Delta_{n} (X)$ の単位元は、任意の $\\sigma \\in \\Delta_{n} (X)$ に対して $0 := \\sigma + (-\\sigma)$ で定義され、$\\mathbb{Z}$ がアーベル群であるため、$\\Delta_{n} (X)$ もアーベル群になります。ここで、群 $\\left( \\Delta_{n} (X) , + \\right)$ の演算 $+$ は $\\left( \\mathbb{Z} , + \\right)$ の $+$ から導かれたものですが、同じものではありません。$n$-チェイン $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} \\in \\Delta_{n} (X)$ で登場する $\\sum$ とも異なります。\n要約すると以下のようになります。\n最初に定義したときの $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ で加算のように見えるものは、そもそも演算ではなく記法に過ぎませんでした。 $\\left( \\Delta_{n} (X) , + \\right)$ の $+$ は $\\left( \\mathbb{Z} , + \\right)$ の $+$ から導かれましたが、同じものではありません。 $\\left( \\Delta_{n} (X) , + \\right)$ は自由アーベル群であり、これで $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ も二項演算 $+$ の関数値になります。 $\\partial$ をなぜ境界と呼ぶのか？ $$ \\partial _{n} \\left( \\sigma_{\\alpha} \\right) := \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right] $$\n定義にある数式だけを見ても理解しにくいですが、以下の図を見ればすぐに理解できるでしょう。\n例えば $\\partial_{2}$ を考えると、次のような計算を行うことができます。 $$ \\begin{align*} \u0026amp; \\partial _{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\\\ =\u0026amp; \\sum_{i=0}^{2} (-1)^{i} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\setminus \\left[ v_{i} \\right] \\\\ =\u0026amp; (-1)^{0} \\left[ v_{1}, v_{2} \\right] + (-1)^{1} \\left[ v_{0}, v_{2} \\right] + (-1)^{2} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\end{align*} $$\nホモロジーグループを学ぶレベルなら、三角形 $\\left[ v_{0} ,v_{1}, v_{2} \\right]$ の境界が $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$ で構成されること自体を受け入れられない人はほとんどいないでしょう。本当に理解しにくいのは、一体 $\\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right]$ が何なのかということです。1-シンプレックスである線分同士を引くことが意味を成すのでしょうか？それをベクトルとして扱い、2-シンプレックスである三角形同士の演算はどうなるのでしょうか？\nすべて間違っています。しっかりと頭を整理してもう一度見てみましょう。$\\partial_{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\in \\Delta_{1} (X)$ は、その幾何学的な意味を離れて、単に3つの要素 $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$ の形式的和である $$ (+1) \\left[ v_{1}, v_{2} \\right] + (-1) \\left[ v_{0}, v_{2} \\right] + (+1) \\left[ v_{0}, v_{1} \\right] $$\nに過ぎません。これを順番に $$ \\begin{align*} a := \\left[ v_{1}, v_{2} \\right] \\ b:= \\left[ v_{0}, v_{2} \\right] \\ c:= \\left[ v_{0} , v_{1} \\right] \\end{align*} $$ と置くと、$\\Delta_{1} (X)$ の正体がようやく見えてきます。例えば、$1$-チェイン $x \\in \\Delta_{1} (X)$ は、ある係数 $k_{a} , k_{b} , k_{c} \\in \\mathbb{Z}$ に対して $$ x = k_{a} a + k_{b} b + k_{c} c $$ のように表される要素です。逆に $a,b,c$ の立場から自由群 $\\Delta_{1} (X) := F[\\left\\{ a,b,c \\right\\}]$ を構築する過程を考えると、$\\Delta_{1} (X)$ とは、3つの未知数で作られる群、つまり $\\mathbb{Z}^{3} \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} \\oplus \\mathbb{Z}$ と同型な群に過ぎないことがわかります。\nこのような考え方の転換は、続く例を理解する上で必須です。幾何を置いて、代数的に考えましょう。\n例 $$ \\begin{align*} \\\\ \\partial_{n} :\u0026amp; \\Delta_{n} (X) \\to \\Delta_{n-1} (X) \\\\ H_{n}^{\\Delta} (X) =\u0026amp; \\ker \\partial_{n} / \\text{Im} \\partial_{n+1} \\end{align*} $$\n特に $n = 0$ の場合、$\\partial_{0} : \\Delta_{0} \\left( X \\right) \\to 0$ なので $\\ker \\partial_{0} = \\Delta_{0} \\left( X \\right)$ です。\n円 $S^{1}$ $1$-ユニットスフィア、つまり円 $X = S^{1}$を考えると、$0$-シンプレックスは頂点 $v$ 一つ、$1$-シンプレックスはエッジ $e$ 一つ、$n \\ge 2$ で $n$-シンプレックスは存在しないので、チェーンコンプレックス自体は以下のように構成されるでしょう。\n$$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{1}\\left( S^{1} \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( S^{1} \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\n自由群 $\\Delta_{1}\\left( S^{1} \\right)$ は $e$ 一つで生成されるので $\\Delta_{1}\\left( S^{1} \\right) \\simeq \\mathbb{Z}$ であり、$\\Delta_{0}\\left( S^{1} \\right)$ も $v$ 一つで生成されるので $\\Delta_{0}\\left( S^{1} \\right) \\simeq \\mathbb{Z}$ です。一方 $$ \\partial e = v - v = 0 $$ なので $\\partial_{1}$ はゼロ準同型です。\n$n = 0$ の場合、$\\ker \\partial_{0} = \\Delta_{0} \\left( S^{1} \\right)$ であり、$\\partial_{1}$ がゼロ準同型なのでその像は $\\left\\{ 0 \\right\\}$ となり、以下が得られます。 $$ \\begin{align*} H_{0}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n = 1$ の場合、$\\partial_{2}$ の定義域が $0$ なので $\\text{Im} \\partial_{2} = \\left\\{ 0 \\right\\}$ であり、$\\partial_{1}$ がゼロ準同型なので $\\ker \\partial_{1}$ はその定義域である $\\Delta_{1} \\left( S^{1} \\right)$ 自体となり、以下が得られます。 $$ \\begin{align*} H_{1}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{1} / \\text{Im} \\partial_{2} \\\\ \\simeq\u0026amp; \\Delta_{1} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n \\ge 2$ に対しては、$H_{n}^{\\Delta} \\left( S_{1} \\right) \\simeq 0$ なので、以下のように要約できます。 $$ H_{n}^{\\Delta} \\left( S_{1} \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0, 1 \\\\ 0 \u0026amp; , \\text{if } n \\ge 2 \\end{cases} $$\nトーラス $T^{2}$ 上の図のようなトーラス $T^{2}$を考えると、$0$-シンプレックスは頂点 $v$ 一つ、$1$-シンプレックスはエッジ $a$、$b$、$c$ 三つ、$2$-シンプレックスは $U$、$L$ 二つ、$n \\ge 3$ で $n$-シンプレックスは存在しないので、チェーンコンプレックス自体は以下のように構成されるでしょう。\n$$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{2}\\left( T \\right) \\overset{\\partial_{2}}{\\longrightarrow} \\Delta_{1}\\left( T \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( T \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\nこれにより、自由群 $\\Delta_{n} \\left( T \\right)$ は $$ \\Delta_{n} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z}^{1} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z}^{3} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z}^{2} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\nとなります。一方、エッジ $a$、$b$、$c$ の両端点は $v$ に接続されているので $$ \\begin{align*} \\partial a =\u0026amp; v - v = 0 \\\\ \\partial b =\u0026amp; v - v = 0 \\\\ \\partial c =\u0026amp; v - v = 0 \\end{align*} $$ であり、円の場合と同様に $\\partial_{1}$ はゼロ準同型です。\n$n = 0$ の場合、円の場合と同様に以下が成立します。 $$ \\begin{align*} H_{0}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( T \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n = 1$ の場合、$\\partial_{1}$ がゼロ準同型なので $\\ker \\partial_{1}$ はその定義域である $\\Delta_{1} \\left( T \\right)$ 自体です。一方で $\\partial_{2} : \\Delta_{2}\\left( T \\right) \\to \\Delta_{1}\\left( T \\right)$ について $$ \\partial_{2} U = a + b - c = \\partial_{2} L $$ であり、$\\left\\{ a, b, a + b - c \\right\\}$ は $\\Delta_{1}\\left( T \\right)$ の基底なので $H_{1}^{\\Delta}$ は$a$ と $b$ で生成される自由群と同型です。つまり、以下が成立します。 $$ H_{1}^{\\Delta} \\left( T \\right) \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} $$\n$n = 2$ の場合、$\\partial_{3}$ の定義域が $0$ なので $\\text{Im} \\partial_{3} = \\left\\{ 0 \\right\\}$ であり、$\\partial_{2} : \\Delta_{2}\\left( T \\right) \\to \\Delta_{1}\\left( T \\right)$ では $\\Delta_{2}\\left( T \\right) \\simeq \\mathbb{Z}^{2}$ で $\\Delta_{1}\\left( T \\right) \\simeq \\mathbb{Z}^{3}$ なので $\\ker \\partial_{2} \\simeq \\mathbb{Z}^{3-2}$ です。これを整理すると、以下が得られます。 $$ \\begin{align*} H_{2}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{2} / \\text{Im} \\partial_{3} \\\\ \\simeq\u0026amp; \\mathbb{Z}^{3-2} / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n \\ge 3$ に対しては、$H_{n}^{\\Delta} \\left( T \\right) \\simeq 0$ なので、以下のように要約できます。 $$ H_{n}^{\\Delta} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z} \\oplus \\mathbb{Z} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\n定理 $H_{n}^{\\Delta}$ はホモロジーグループである ホモロジーグループの定義:\n$n \\in \\mathbb{N}_{0}$ とします。アーベル群 $C_{n}$ と準同型 $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェーン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ がすべての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、$\\mathcal{C} := \\left\\{ \\left( C_{n}, \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ をチェーンコンプレックスと呼びます。 商群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の第 $n$ ホモロジーグループと呼びます。 準同型 $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界または微分オペレータと呼びます。 $$ \\cdots \\longrightarrow \\Delta_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} \\Delta_{n} \\overset{\\partial_{n}}{\\longrightarrow} \\Delta_{n-1} \\longrightarrow \\cdots $$\nチェーンコンプレックス $\\left\\{ \\left( \\Delta_{n} (X) , \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ に対して $H_{n}^{\\Delta} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ はホモロジーグループです。つまり、すべての $n \\in \\mathbb{N}$ に対して $\\partial_{n} \\circ \\partial_{n+1}$ はゼロ準同型です。\n証明 $\\sigma \\in \\Delta_{n}$ に $\\partial_{n-1} \\circ \\partial_{n}$ を適用してみると、以下が得られます。 $$ \\begin{align*} \u0026amp; \\left( \\partial_{n-1} \\circ \\partial_{n} \\right) \\left( \\sigma \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\partial_{n} \\left( \\sigma \\right) \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , v_{n} \\right] \\right) \\\\ =\u0026amp; \\sum_{j \u0026lt; i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ \u0026amp; + \\left( -1 \\right) \\sum_{j \u0026gt;i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\n実際、このような証明は、一般的に証明するよりも、帰納的な例を示すことがより役立ちます。 $$ \\begin{align*} \u0026amp; \\partial_{1} \\left( \\partial_{2} \\left[ v_{0}, v_{1} , v_{2} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left( \\left[ v_{1} , v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left[ v_{1} , v_{2} \\right] - \\partial_{1} \\left[ v_{0}, v_{2} \\right] + \\partial_{1} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{2} \\right] - \\left[ v_{1} \\right] - \\left( \\left[ v_{2} \\right] - \\left[ v_{0} \\right] \\right) + \\left[ v_{1} \\right] - \\left[ v_{0} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\n■\nHatcher. (2002). Algebraic Topology: p104~106.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2383,"permalink":"https://freshrimpsushi.github.io/jp/posts/2383/","tags":null,"title":"シンプリシアルホモロジーグループの定義"},{"categories":"줄리아","contents":"概要 ブロードキャスティングは Juliaで最も重要な概念の一つであり、ベクトル化されたコードを書く際に非常に便利な文法だ1。二項演算の前に.を置いたり、関数の後に.を置くことで使用する。これは点ごとに関数を適用するという意味であり、その目的にぴったりの表現だ。\nプログラミング的にブロードキャスティングは、マップとリデュースのマップを使いやすくしたものと見ることができる。\nコード 二項演算 二項演算には.を付けて使用する。例えば、行列$A \\in \\mathbb{Z}_{9}^{3 \\times 4}$の全要素にスカラ$a \\in \\mathbb{R}$を足すコードは以下の通りだ。\njulia\u0026gt; A = rand(0:9, 3,4)\r3×4 Matrix{Int64}:\r5 6 3 3\r7 4 8 8\r0 2 2 7\rjulia\u0026gt; a = rand()\r0.23234165065465284\rjulia\u0026gt; A .+ a\r3×4 Matrix{Float64}:\r5.23234 6.23234 3.23234 3.23234\r7.23234 4.23234 8.23234 8.23234\r0.232342 2.23234 2.23234 7.23234 一般関数 julia\u0026gt; f(x) = x^2 - 1\rf (generic function with 3 methods)\rjulia\u0026gt; f(a)\r-0.9460173573710713 例えば関数$f : \\mathbb{R} \\to \\mathbb{R}$を考えてみよう。これはスカラ関数なので、$a \\in \\mathbb{R}$に対して上記のようにうまく計算される。\njulia\u0026gt; f(A)\rERROR: LoadError: DimensionMismatch しかし、行列$A$を入れてみるとLoadErrorが発生する。考えてみれば、行列の平方、特に$A \\in \\mathbb{Z}_{9}^{3 \\times 4}$のような直方体の行列の平方とは何か、という問題から始まる。そのため、$f(x) = x^{2} - 1$のような関数に無闇に入れることはできない。しかし、行列$A$の全ての値に対してそれぞれ平方を取り、その後$1$を引いた行列を得たい場合は、f.のように点を打つことで、行列の全要素に関数$f : \\mathbb{R} \\to \\mathbb{R}$を適用できる。\njulia\u0026gt; f.(A)\r3×4 Matrix{Int64}:\r24 35 8 8\r48 15 63 63\r-1 3 3 48 速度比較 多くの場合、ブロードキャスティングは性能面でも優れている。しかし、速度を性能の基準として性能を評価する部分には、かなり難しい点があるので、必ず以下の内容を確認してほしい。\n例として、以下は1から10万までの数に平方根を取るコードだ。\njulia\u0026gt; @time for x in 1:100000\rsqrt(x)\rend\r0.000001 seconds\rjulia\u0026gt; @time sqrt.(1:100000);\r0.000583 seconds (2 allocations: 781.297 KiB) 単純な速度だけを比較すると、ブロードキャスティングはforループよりも約500倍遅い。しかし、これは単純な計算から得られたベンチマークで、保存するプロセスまで含めた場合は話が変わる。\njulia\u0026gt; z = []\rAny[]\rjulia\u0026gt; @time for x in 1:100000\rpush!(z, sqrt(x))\rend\r0.005155 seconds (100.01 k allocations: 3.353 MiB)\rjulia\u0026gt; @time y = sqrt.(1:100000);\r0.000448 seconds (2 allocations: 781.297 KiB) 保存するプロセスを含めても、ブロードキャスティングを適用したコードには変わりはないが、空の配列に値を追加しなければならない反復文の場合、ベクトル化されたコードと比べて約10倍遅いことがわかる。これはsqrt()自体よりもpush!()が動的配列を扱う際に消費するコストが大きいと言えるが、とにかく結果としてブロードキャスティング側が速い。当然、反復文をより速くする方法もあるが（例えばAny[]がFloat64[]に変わるだけで改善されるだろう）、実際に遭遇するほとんどのコーディングで、ブロードキャスティングを使用する方が扱いやすく、速度面でも優れている。\nこれは単なる概念的な部分を超え、ジュリアがインタープリターよりもコンパイラ言語に近い1こととも関連している。次のループで何が起こるかわからないfor反復文よりも、タイプとサイズが具体的に決まっているベクトルに対してコンパイルする方が、コンパイラにとって楽ではないだろうか？\n99%程度の関数では、私たちが独自に反復文を使うよりも、ジュリアを作った人たちが考案した方法をそのまま使う方が速いと断言できる。コードを無理にベクトル化する必要はないが、ベクトル化できるコードなら、ほとんどの場合、ベクトル化した方が圧倒的に\u0026hellip;本当に圧倒的に速い。これはジュリアに限らず、マトラボ、Rのようにベクトル演算に特化した言語なら誰もが持っている特徴だが、関数型プログラミングのパラダイムを最もよく受け入れている新しい言語であり、速度面で自信を示している点が異なるだけだ。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/arrays/#Broadcasting\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2382,"permalink":"https://freshrimpsushi.github.io/jp/posts/2382/","tags":null,"title":"ジュリアのブロードキャスティング文法"},{"categories":"양자정보이론","contents":"定義1 以下のようなベクトル値ブール関数をフレドキンゲートFredkin gateと呼ぶ。\n$$ F : \\left\\{ 0, 1 \\right\\}^{3} \\to \\left\\{ 0, 1 \\right\\}^{3} $$\n$$ F (a, b, c) = \\Big(a, (\\lnot a \\land b) \\lor (a \\land c), (\\lnot a \\land c) \\lor (a \\land b) \\Big) $$\n$\\text{CSWAP}$ ゲートControlled SWAP(CSWAP) gateとも呼ばれる。 説明 エドワード・フレドキンEdward Fredkinによって紹介された。フレドキンゲートは、最初の入力を変えずに、最初の入力が$1$の場合には、残りの二つの値を交換swapして出力する。その具体的な計算は次のようである。\n$$ \\begin{align*} F (0,0,0) \u0026amp;= (0, (\\lnot 0 \\land 0) \\lor (0 \\land 0), (\\lnot 0 \\land 0) \\lor (0 \\land 0)) = (0, 0 \\lor 0, 0 \\lor 0) = (0, 0, 0) \\\\ F (0,0,1) \u0026amp;= (0, (\\lnot 0 \\land 0) \\lor (0 \\land 1), (\\lnot 0 \\land 1) \\lor (0 \\land 0)) = (0, 0 \\lor 0, 1 \\lor 0) = (0, 0, 1) \\\\ F (0,1,0) \u0026amp;= (0, (\\lnot 0 \\land 1) \\lor (0 \\land 0), (\\lnot 0 \\land 0) \\lor (0 \\land 1)) = (0, 1 \\lor 0, 0 \\lor 0) = (0, 1, 0) \\\\ F (0,1,1) \u0026amp;= (0, (\\lnot 0 \\land 1) \\lor (0 \\land 1), (\\lnot 0 \\land 1) \\lor (0 \\land 1)) = (0, 1 \\lor 0, 1 \\lor 0) = (0, 1, 1) \\\\ F (1,0,0) \u0026amp;= (1, (\\lnot 1 \\land 0) \\lor (1 \\land 0), (\\lnot 1 \\land 0) \\lor (1 \\land 0)) = (1, 0 \\lor 0, 0 \\lor 0) = (1, 0, 0) \\\\ F (1,0,1) \u0026amp;= (1, (\\lnot 1 \\land 0) \\lor (1 \\land 1), (\\lnot 1 \\land 1) \\lor (1 \\land 0)) = (1, 0 \\lor 1, 0 \\lor 0) = (1, 1, 0) \\\\ F (1,1,0) \u0026amp;= (1, (\\lnot 1 \\land 1) \\lor (1 \\land 0), (\\lnot 1 \\land 0) \\lor (1 \\land 1)) = (1, 0 \\lor 0, 0 \\lor 1) = (1, 0, 1) \\\\ F (1,1,1) \u0026amp;= (1, (\\lnot 1 \\land 1) \\lor (1 \\land 1), (\\lnot 1 \\land 1) \\lor (1 \\land 1)) = (1, 0 \\lor 1, 0 \\lor 1) = (1, 1, 1) \\\\ \\end{align*} $$\n上の表を見れば、$F$が可逆関数であることと、$F$を二回合成すると恒等関数になることが容易にわかる。\n$$ \\operatorname{Id} = F \\circ F $$\nまた、$\\left\\{ F \\right\\}$が機能的に完全であるため、$F$は汎用ゲートである。\nブール関数\rシンボル\r$F$\r真理値表\r入力\r出力\r$a$\r$b$\r$c$\r$a$\r$ (\\lnot a \\land b) \\lor (a \\land c)$\r$ (\\lnot a \\land c) \\lor (a \\land b)$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r整理 (射影と注入を制約なく使えると仮定すると)フレドキンゲート$F$は汎用ゲートである。つまり、$\\left\\{ F \\right\\}$は機能的に完全である。\n証明 定理\n$\\text{NOT}$ゲートと$\\text{AND}$ゲートの集合$\\left\\{ \\lnot, \\land \\right\\}$は機能的に完全である。\n上の定理に従って、射影、注入、$F$を適切に使用して$\\text{NOT}$、$\\text{AND}$を表現できることを示せば、証明は終了する。\n$\\text{NOT}$ゲート\n$$ \\lnot = p_{0} \\circ p_{1} \\circ F \\circ \\jmath_{2} \\circ \\imath_{1} $$\nが成り立つ。まず$\\jmath_{2} \\circ \\imath_{1} (a) = (a, 0, 1)$であるため、次を得る。\n$$ \\begin{equation} F \\circ \\jmath_{2} \\circ \\imath_{1}(a) = F(a, 0, 1) = (a, a, \\lnot a) \\end{equation} $$\nここで、最初の二つの値を消去するために$p_{0} \\circ p_{1}$を取ると、\n$$ p_{0} \\circ p_{1} \\circ F \\circ \\jmath_{2} \\circ \\imath_{1} (a) = p_{0} \\circ p_{1} (a, a, \\lnot a) = \\lnot a $$\n※ さらに$(1)$に$p_{2}$を適用すると、複製関数を得る。\n$\\text{AND}$ゲート\n$$ \\land = p_{0} \\circ p_{1} \\circ F \\circ \\imath_{2} $$\nが成り立つ。まず$F \\circ \\jmath_{2} (a, b)$は次のようである。\n$$ \\begin{align*} F \\circ \\jmath_{2} (a, b) = F(a, b, 0) \u0026amp;= (a, (\\lnot a \\land b) \\lor (a \\land 0), (\\lnot a \\land 0) \\lor (a \\land b)) \\\\ \u0026amp;= (a, (\\lnot a \\land b) \\lor 0, 0 \\lor (a \\land b)) \\\\ \u0026amp;= (a, \\lnot a \\land b, a \\land b) \\end{align*} $$\nしたがって、$p_{0} \\circ p_{1}$を取ると、\n$$ p_{0} \\circ p_{1} \\circ F \\circ \\imath_{2} (a, b) = p_{0} \\circ p_{1} (a, \\lnot a \\land b, a \\land b) = a \\land b $$\n■\n関連項目 $\\text{AND}$ゲート論理積 $\\text{OR}$ゲート論理和 $\\text{NOT}$ゲート論理否定 $\\text{XOR}$ゲート排他的論理和 $\\text{NAND}$ゲート否定論理積 $\\text{NOR}$ゲート否定論理和 $\\operatorname{CNOT}$ゲート トフォリゲート$\\text{CCNOT}$ゲート 金永勳·許在成, 量子情報理論 (2020), p90-93\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3412,"permalink":"https://freshrimpsushi.github.io/jp/posts/3412/","tags":null,"title":"フレドキン・CSWAPゲート"},{"categories":"양자정보이론","contents":"定義1 以下のようなベクトル値ブール関数をトフォリゲートToffoli gateと呼ぶ。\n$$ T : \\left\\{ 0, 1 \\right\\}^{3} \\to \\left\\{ 0, 1 \\right\\}^{3} $$\n$$ T (a, b, c) = (a, b, (a \\land b) \\oplus c) $$\n$\\text{CCNOT}$ ゲートControlled Controlled NOT(CCNOT) gateとも呼ばれる。 説明 トフォリゲートでは、最初の二つの入力が両方とも$1$であれば、三番目の入力が反転する。その他の場合は、入力と出力が同じである。具体的な計算は以下の通りである。\n$$ \\begin{align*} T (0,0,0) \u0026amp;= (0, 0, (0 \\land 0) \\oplus 0) = (0, 0, 0 \\oplus 0) = (0, 0, 0) \\\\ T (0,0,1) \u0026amp;= (0, 0, (0 \\land 0) \\oplus 1) = (0, 0, 0 \\oplus 1) = (0, 0, 1) \\\\ T (0,1,0) \u0026amp;= (0, 1, (0 \\land 1) \\oplus 0) = (0, 1, 0 \\oplus 0) = (0, 1, 0) \\\\ T (0,1,1) \u0026amp;= (0, 1, (0 \\land 1) \\oplus 1) = (0, 1, 0 \\oplus 1) = (0, 1, 1) \\\\ T (1,0,0) \u0026amp;= (1, 0, (1 \\land 0) \\oplus 0) = (1, 0, 0 \\oplus 0) = (1, 0, 0) \\\\ T (1,0,1) \u0026amp;= (1, 0, (1 \\land 0) \\oplus 1) = (1, 0, 0 \\oplus 1) = (1, 0, 1) \\\\ T (1,1,0) \u0026amp;= (1, 1, (1 \\land 1) \\oplus 0) = (1, 1, 1 \\oplus 0) = (1, 1, 1) \\\\ T (1,1,1) \u0026amp;= (1, 1, (1 \\land 1) \\oplus 1) = (1, 1, 1 \\oplus 1) = (1, 1, 0) \\\\ \\end{align*} $$\n上の表を見れば$T$が可逆関数であることと$T$を二回合成すると恒等関数になることが容易に分かる。\n$$ \\operatorname{Id} = T \\circ T $$\nまた、$\\left\\{ T \\right\\}$が機能的に完全であるため、$T$は汎用ゲートである。\n부울 함수\r기호\r진리표\r$T$\r입력\r출력\r$a$\r$b$\r$c$\r$a$\r$b$\r$(a \\land b) \\oplus c$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$0$\r整理 (射影と注入を制約なく使えると仮定すると)トフォリゲート$T$は汎用ゲートである。つまり$\\left\\{ T \\right\\}$は機能的に完全である。\n証明 定理\n複製関数を許可すると、$\\left\\{ \\uparrow \\right\\}$は機能的に完全である。つまり$\\text{NAND}$ゲート$\\uparrow$は汎用ゲートである。\n上の定理に従い、射影、注入、$T$を適切に使用して複製関数$\\text{cl}$と$\\text{NAND}$ゲートを表現できることを示せば証明が完了する。\n複製関数\n$$ \\operatorname{cl} = p_{1} \\circ T \\circ \\imath_{2} \\circ \\jmath_{1} $$\nが成り立つ。まず$T \\circ \\imath_{2} \\circ \\jmath_{1} (a)$を計算すると以下のようになる。\n$$ T \\circ \\imath_{2} \\circ \\jmath_{1} (a) = T \\circ \\imath_{2} (a, 1) = T (a, 1, 0) $$\nここで$a = 1$であれば$T(1, 1, 0) = (1, 1, 1)$であり、$a = 0$であれば$T(0, 1, 0) = (0, 1, 0)$であるため、次が成り立つ。\n$$ T(a, 1, 0) = (a, 1, a) $$\nしたがって$p_{1}$を取れば、\n$$ p_{1} \\circ T \\circ \\imath_{2} \\circ \\jmath_{1} (a) = p_{1} (a, 1, a) = (a, a) = \\operatorname{cl}(a) $$\n$\\text{NAND}$ゲート\n$$ p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} = \\uparrow \\\\ p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} (a, b) = a \\uparrow b $$\nが成り立つ。順に計算すると以下のようになる。\n$$ \\begin{align*} p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} (a, b) \u0026amp;= p_{0} \\circ p_{1} \\circ T (a, b, 1) \\\\ \u0026amp;= p_{0} \\circ p_{1} (a, b, (a \\land b) \\oplus 1) \\\\ \u0026amp;= p_{0} (a, (a \\land b) \\oplus 1) \\\\ \u0026amp;= (a \\land b) \\oplus 1 \\\\ \u0026amp;= \\lnot(a \\land b) = a \\uparrow b \\end{align*} $$\n最後の行は$\\text{XOR}$ゲートの性質によって成り立つ。\n■\n木村泰宏・許在成, 量子情報理論 (2020), p89-93\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3411,"permalink":"https://freshrimpsushi.github.io/jp/posts/3411/","tags":null,"title":"トッフォリ/CCNOTゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなベクトル値ブール関数を**$\\operatorname{CNOT}$ゲート**Controlled NOT(CNOT) gateという。\n$$ \\operatorname{CNOT} : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\}^{2} $$\n$$ \\operatorname{CNOT} (a,b) = (a, a \\oplus b) $$\nファインマンゲートFeynman gateとも呼ばれる。2 説明 $\\operatorname{CNOT}$ゲートの入出力の具体的な計算は次のようになる。\n$$ \\begin{align*} \\operatorname{CNOT} (0,0) \u0026amp;= (0, 0 \\oplus 0) = (0, 0) \\\\ \\operatorname{CNOT} (0,1) \u0026amp;= (0, 0 \\oplus 1) = (0, 1) \\\\ \\operatorname{CNOT} (1,0) \u0026amp;= (1, 1 \\oplus 0) = (1, 1) \\\\ \\operatorname{CNOT} (1,1) \u0026amp;= (1, 1 \\oplus 1) = (1, 0) \\end{align*} $$\n上の表を見ると、$\\operatorname{CNOT}$が可逆関数であることと、$\\operatorname{CNOT}$を二回合成すると恒等関数になることが容易に分かる。\n$$ \\operatorname{Id} = \\operatorname{CNOT} \\circ \\operatorname{CNOT} $$\n出力の二番目の値だけを見ると、$\\text{XOR}$ゲートと同じであるため、可逆$\\text{XOR}$ゲートとも呼ばれる。\n부울 함수\r기호\r진리표\r$\\operatorname{CNOT}$\r입력\r출력\r$a$\r$b$\r$a$\r$a \\oplus b$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$0$\rキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p88-89\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Controlled_NOT_gate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3410,"permalink":"https://freshrimpsushi.github.io/jp/posts/3410/","tags":null,"title":"制御NOT(CNOT)ゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下のようなブール関数を $\\text{NOR}$ゲートNOR gateまたは否定論理和と呼び、次のように表記する。\n$$ \\downarrow : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\downarrow 0 = 1,\\quad 0\\downarrow 1 = 0,\\quad 1\\downarrow 0 = 0,\\quad 1\\downarrow 1 = 0 $$\n説明 $\\text{NOT}$ゲートと$\\text{OR}$ゲートの合成であり、$\\text{N(OT)}$と$\\text{OR}$を取り入れて$\\text{NOR}$と名付けた。\n$$ \\begin{equation} \\downarrow = \\lnot \\circ \\lor \\end{equation} $$\n$$ a \\downarrow b = \\lnot (a \\lor b) $$\n$\\text{OR}$ゲートとは逆に動作し、すべての入力が偽のときのみ真を出力する。また、$\\left\\{ \\downarrow \\right\\}$は機能的に完全であり、$(1)$により当然と言える。\n부울 함수\r기호\r진리표\r$\\text{NOR}$\r$a$\r$b$\r$a \\downarrow b$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r結論 (複製関数を許容するなら) $\\left\\{ \\downarrow \\right\\}$は機能的に完全である。言い換えると、$\\downarrow$は汎用ゲートである。\n証明 定理\n$\\text{NOT}$と$\\text{OR}$ゲートの集合$\\left\\{ \\lnot, \\lor \\right\\}$は機能的に完全である。\n上の定理に従い、複製関数$\\text{cl}$と$\\downarrow$だけで$\\text{NOT}$ゲートと$\\text{OR}$ゲートを作ることができることを示せばよい。\n$\\text{NOT}$ゲート\n$$ \\lnot = \\downarrow \\circ \\operatorname{cl} \\\\ \\lnot a = a \\downarrow a $$\nが成立する。\n$$ \\begin{align*} \\downarrow \\circ \\operatorname{cl}(0) = 0 \\downarrow 0 = 1 = \\lnot 0 \\\\ \\downarrow \\circ \\operatorname{cl}(1) = 1 \\downarrow 1 = 0 = \\lnot 1 \\\\ \\end{align*} $$\n$\\text{OR}$ゲート\n$$ \\lor = \\downarrow \\circ \\operatorname{cl} \\circ \\downarrow \\\\ a \\lor b = (a \\downarrow b) \\downarrow (a \\downarrow b) $$\nが成立する。\n$$ \\begin{align*} (0 \\downarrow 0) \\downarrow (0 \\downarrow 0) = (1 \\downarrow 1) = 0 = 0 \\lor 0 \\\\ (0 \\downarrow 1) \\downarrow (0 \\downarrow 1) = (0 \\downarrow 0) = 1 = 0 \\lor 1 \\\\ (1 \\downarrow 0) \\downarrow (1 \\downarrow 0) = (0 \\downarrow 0) = 1 = 1 \\lor 0 \\\\ (1 \\downarrow 1) \\downarrow (1 \\downarrow 1) = (1 \\downarrow 1) = 1 = 1 \\lor 1 \\\\ \\end{align*} $$\n■\nキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3407,"permalink":"https://freshrimpsushi.github.io/jp/posts/3407/","tags":null,"title":"否定論理和、NORゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下のブール関数を**$\\text{NAND}$ゲート**NAND gate、または否定論理積と呼び、次のように記す。\n$$ \\uparrow : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\uparrow 0 = 1,\\quad 0\\uparrow 1 = 1,\\quad 1\\uparrow 0 = 1,\\quad 1\\uparrow 1 = 0 $$\n説明 $\\text{NOT}$ゲートと$\\text{AND}$ゲートの合成であり、$\\text{N(OT)}$と$\\text{AND}$を引用して$\\text{NAND}$と命名されている。\n$$ \\begin{equation} \\uparrow = \\lnot \\circ \\land \\end{equation} $$\n$$ a \\uparrow b = \\lnot (a \\land b) $$\n$\\text{AND}$ゲートとは逆に動作し、すべての入力が真のときのみ偽を出力する。また、$\\left\\{ \\uparrow \\right\\}$は関数的に完全であるとされ、$(1)$により当然であると考えられる。\n부울 함수\r기호\r진리표\r$\\text{NAND}$\r$a$\r$b$\r$a \\uparrow b$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r定理 (複製関数を許容すると) $\\left\\{ \\uparrow \\right\\}$は関数的に完全である。つまり、$\\uparrow$はユニバーサルゲートである。\n証明 定理\n$\\text{NOT}$と$\\text{AND}$ゲートのセット$\\left\\{ \\lnot, \\land \\right\\}$は関数的に完全である。\n上記の定理に従い、複製関数$\\text{cl}$と$\\uparrow$のみで$\\text{NOT}$ゲートと$\\text{AND}$ゲートを作ることができることを示せばよい。\n$\\text{NOT}$ゲート\n$$ \\lnot = \\uparrow \\circ \\operatorname{cl} \\\\ \\lnot a = a \\uparrow a $$\nが成立する。\n$$ \\begin{align*} \\uparrow \\circ \\operatorname{cl}(0) = 0 \\uparrow 0 = 1 = \\lnot 0 \\\\ \\uparrow \\circ \\operatorname{cl}(1) = 1 \\uparrow 1 = 0 = \\lnot 1 \\\\ \\end{align*} $$\n$\\text{AND}$ゲート\n$$ \\land = \\uparrow \\circ \\operatorname{cl} \\circ \\uparrow \\\\ a \\land b = (a \\uparrow b) \\uparrow (a \\uparrow b) $$\nが成立する。\n$$ \\begin{align*} (0 \\uparrow 0) \\uparrow (0 \\uparrow 0) = (1 \\uparrow 1) = 0 = 0 \\land 0 \\\\ (0 \\uparrow 1) \\uparrow (0 \\uparrow 1) = (0 \\uparrow 0) = 0 = 0 \\land 1 \\\\ (1 \\uparrow 0) \\uparrow (1 \\uparrow 0) = (0 \\uparrow 0) = 0 = 1 \\land 0 \\\\ (1 \\uparrow 1) \\uparrow (1 \\uparrow 1) = (1 \\uparrow 1) = 1 = 1 \\land 1 \\\\ \\end{align*} $$\n■\nキム・ヨンフン, ホ・ジェソン, 量子情報理論 (2020), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3406,"permalink":"https://freshrimpsushi.github.io/jp/posts/3406/","tags":null,"title":"負論理積、NANDゲート"},{"categories":"줄리아","contents":"コード 1 julia\u0026gt; Dict([\u0026#34;a\u0026#34;, \u0026#34;bc\u0026#34;] .=\u0026gt; [2,8])\rDict{String, Int64} with 2 entries:\r\u0026#34;a\u0026#34; =\u0026gt; 2\r\u0026#34;bc\u0026#34; =\u0026gt; 8 キーKeyとバリューValueとして使いたい二つの配列が与えられた時、Dict(Key .=\u0026gt; Value)を通じて辞書を作ることができる。本質的にはペアPairを作る演算子=\u0026gt;のブロードキャスティングBroadcastingに過ぎない。\n環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/create-a-dictionary-from-arrays-of-keys-and-values/13908/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2375,"permalink":"https://freshrimpsushi.github.io/jp/posts/2375/","tags":null,"title":"ジュリアで配列から辞書を作成する方法"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなブール関数を $\\text{XOR}$ ゲートXOR gateまたは排他的論理和exclusive disjuction/orと呼び、以下のように表記する。\n$$ \\oplus : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\oplus 0 = 0,\\quad 0\\oplus 1 = 1,\\quad 1\\oplus 0 = 1,\\quad 1\\oplus 1 = 0 $$\n説明 $\\text{XOR}$ ゲートは、二つの真理値のうち一つだけが真のとき、つまり真が奇数のときに真を返す。つまり、二つの値が同じならば$0$、異なれば$1$を返すので、二つの値が同じかどうかを比較する機能を実装するのに役立つ。\n「パーセプトロンは$\\text{XOR}$問題を解くことができない」という指摘のため、AIの発展が停滞した1974年から1980年までをAIの冬AI winterと言う。\n부울 함수\r기호\r진리표\r$\\text{XOR}$\r$a$\r$b$\r$a \\oplus b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r特性 $\\text{NOT}$ ゲート、$\\text{AND}$ ゲート、$\\text{OR}$ ゲートで表現可能である。\n$$ \\begin{align*} a \\oplus b \u0026amp;= (a \\land \\lnot b) \\lor (\\lnot a \\land b) \\\\ \u0026amp;= (a \\lor b) \\land (\\lnot a \\lor \\lnot b) \\\\ \u0026amp;= (a \\lor b) \\land \\lnot (a \\land b) \\end{align*} $$\n$a \\oplus 1 = \\lnot a$が成立する。\n$a \\oplus 0 = a$が成立する。\nキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3405,"permalink":"https://freshrimpsushi.github.io/jp/posts/3405/","tags":null,"title":"排他的論理和、XORゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなブール関数を $\\text{NOT}$ ゲートNOT gateまたは論理否定negationと言い、次のように表記する。\n$$ \\lnot : \\left\\{ 0, 1 \\right\\} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ \\lnot 0 = 1,\\quad \\lnot 1 = 0 $$\n説明 $\\text{NOT}$ ゲートは入力の反対を返す。\n$\\text{NOT}$ 게이트는 입력의 반대를 반환한다.\n부울 함수\r기호\r진리표\r$\\text{NOT}$\r$a$\r$\\lnot a$\r$0$\r$1$\r$1$\r$0$\rキム・ヨンフン、ホ・ジェソン, 量子情報理論 (2020), p84-85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3404,"permalink":"https://freshrimpsushi.github.io/jp/posts/3404/","tags":null,"title":"論理否定、NOTゲート"},{"categories":"줄리아","contents":"概要 Juliaは、基本的にRと同じように複素数をサポートしている。\nコード 虚数単位 im julia\u0026gt; z = 3 + 4im\r3 + 4im imは純虚数 $i = \\sqrt{-1}$ を表す。常識的に使われている四則演算は全部使える。\njulia\u0026gt; typeof(z)\rComplex{Int64}\rjulia\u0026gt; typeof(3.0 + 4.0im)\rComplexF64 (alias for Complex{Float64}) タイプをチェックすると、同じ複素数でも、どんな複素数で構成されているかが違う。まるで抽象代数で整数の場合 $\\mathbb{Z} [i]$、あるいは実数の場合 $\\mathbb{R} [i]$と区別される感じが似ている。\n実部、虚部 real(), imag() julia\u0026gt; real(z)\r3\rjulia\u0026gt; imag(z)\r4 共役複素数、モジュラス conj(), abs() julia\u0026gt; conj(z)\r3 - 4im\rjulia\u0026gt; abs(z)\r5.0 一方で、ここでのモジュラス abs()は、特に複素数に対して新たに定義されたわけではなく、絶対値そのものとして使われている点に注意。Juliaは多態性を持っているので、このような設計が自然にうまく行われている。\n一般複素関数 julia\u0026gt; cos(z)\r-27.034945603074224 - 3.851153334811777im\rjulia\u0026gt; log(z)\r1.6094379124341003 + 0.9272952180016122im 当然だが、絶対値と同様に、三角関数や対数関数も複素数 $\\mathbb{C}$ でうまく定義されており、Juliaで特別な操作なしに直接使用できる。\n全コード z = 3 + 4im\rreal(z)\rimag(z)\rconj(z)\rabs(z)\rcos(z)\rlog(z) 環境 OS: Windows julia: v1.7.0 ","id":2373,"permalink":"https://freshrimpsushi.github.io/jp/posts/2373/","tags":null,"title":"ジュリアで複素数を使用する方法"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下のようなブール関数を**$\\text{OR}$ ゲート**OR gateまたは論理和disjunctionと呼び、以下のように表記する。\n$$ \\lor : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\lor 0 = 0,\\quad 0\\lor 1 = 1,\\quad 1\\lor 0 = 1,\\quad 1\\lor 1 = 1 $$\n説明 $\\text{OR}$ ゲートは2つの真理値を1つの真理値に変換し、2つの真理値のうち一方でも真であれば真を返す。\n부울 함수\r기호\r진리표\r$\\text{OR}$\r$a$\r$b$\r$a \\lor b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$\\text{NOT}$ ゲートと$\\text{AND}$ ゲートで表現可能である。\n$$ a \\lor b = \\lnot(\\lnot a \\land \\lnot b) $$\n김영훈·허재성, 양자 정보 이론 (2020), p84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3403,"permalink":"https://freshrimpsushi.github.io/jp/posts/3403/","tags":null,"title":"論理和、ORゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなブール関数を $\\text{AND}$ ゲートAND gate、または論理積conjunctionと呼び、以下のように表記する。\n$$ \\land : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\land 0 = 0,\\quad 0\\land 1 = 0,\\quad 1\\land 0 = 0,\\quad 1\\land 1 = 1 $$\n説明 $\\text{AND}$ ゲートは二つの真理値を一つの真理値に変換し、二つの真理値が共に真の場合のみ真を返す。\n부울 함수\r기호\r진리표\r$\\text{AND}$\r$a$\r$b$\r$a \\land b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$1$\r$\\text{NOT}$ ゲート と $\\text{OR}$ ゲート で表現可能である。\n$$ a \\land b = \\lnot(\\lnot a \\lor \\lnot b) $$\nキム・ヨンフン、ホ・ジェソン、 量子情報理論 (2020)、p84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3402,"permalink":"https://freshrimpsushi.github.io/jp/posts/3402/","tags":null,"title":"論理積、ANDゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 2 離散確率変数 $X$が $n$個の値 $x_{1}, x_{2}, \\dots, x_{n}$を取るとする。 $X$の確率質量関数を $p$とする。すると、$X$あるいは$p$のエントロピーShannon entropy$H$を次のように定義する。\n$$ \\begin{equation} H(X) = H(p) := E\\left[ I(x_{i}) \\right] = \\sum_{i=1}^{n} p(x_{i}) I(x_{i}) = -\\sum_{i=1}^{n} p(x_{i}) \\log_{2}p(x_{i}) \\end{equation} $$\nこの時、$I$は情報量、$E$は期待値である。\n$X$が連続確率変数の場合、\n$$ H(X) = H(p) = - \\int_{-\\infty}^{\\infty} p(x)\\log_{2}p(x) dx $$\n説明 簡単に言えば、エントロピーは情報の期待値(平均)です。エントロピーを通じて、符号化の効率や通信の限界について数学的に扱うことができます。\nエントロピーは一般に無秩序度と説明されますが、ここで言う秩序とは規則、傾向、パターンなどの意味で考えれば良いです。従って、エントロピーが高いとは無秩序度が高いことを意味し、確率変数$X$に対して規則やパターンを把握することが難しいという話です。\nここで、確率が操作されたコイン投げを考えてみましょう。表が出る確率を$p$とすれば、裏が出る確率は$1-p$で、エントロピーは次のようになります。\n$$ H = -p\\log_{2}p - (1-p)\\log_{2}(1-p) $$\n$p$に対する$H$をグラフにすると、次のようになります。\n表が出る確率が$\\dfrac{1}{2}$の時、エントロピーは$H = -\\dfrac{1}{2}\\log_{2}\\dfrac{1}{2}-\\dfrac{1}{2}\\log_{2}\\dfrac{1}{2} = 1$で最大値です。つまり、コイン投げのパターンや規則をよく知ることができないという意味です。実際にコイン投げの場合、私たちはコインのどの面が出るかを確信することはできません。ここで表が出る確率が少し変わると、エントロピーが下がります。例えば、表が出る確率が$\\dfrac{95}{100}$であれば、エントロピーは約$0.28$で無秩序度が低く、つまり何らかの規則やパターン(この例ではほぼ表が出るというパターン)があるという意味です。この内容を次のようにまとめることができます。\nエントロピーが高い = 無秩序度が高い = 規則性やパターンがない = 結果を予測するのが難しい エントロピーが低い = 無秩序度が低い = 規則性やパターンがある = 結果を予測するのが容易\r上の例から予想できるように、一般的に$n$個の場合があるとすると、エントロピーが最も高くなるのは全ての確率が$\\dfrac{1}{n}$で等しい時です。\n性質 確率変数$X$が$n$個の値 $x_{1}, x_{2}, \\dots, x_{n}$を取るとする。エントロピー$H$は次のような性質を持ちます。\n$H$は凹concave関数です。 ある$x_{i}$に対して$p(x_{i}) = 1$ならば、$H(X) = 0$です。 全ての確率が$p(x_{i}) = \\dfrac{1}{n}$で同じ時、エントロピーは最大で、その値は$\\log_{2}n$です。 平均が$\\mathbf{0}$で共分散行列が$K$のランダムベクトル$X \\in \\mathbb{R}^{n}$のエントロピーについて次が成立します。 $$ \\begin{equation} H(X) \\le \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{p} \\left| K \\right| \\right] \\end{equation} $$ $\\left| K \\right|$は共分散行列の行列式です。$X$が正規分布なら等号が成立します。 平均$\\mu$と分散$\\sigma^{2}$が与えられた時、エントロピーが最大の分布は正規分布です。 確率変数$X$と推定量$\\hat{X}$に対して次が成立します。 $$ E\\left[ (X - \\hat{X})^{2} \\right] \\ge \\dfrac{1}{2\\pi e} e^{2H(X)} $$ 証明 4 便宜上$\\mathbf{x} = X$と表記しましょう。$g$を$\\displaystyle \\int g(\\mathbf{x})x_{i}x_{j} d \\mathbf{x} = K_{ij}$を満たす任意の確率密度関数とします。$\\phi$を\n正規分布$N(\\mathbf{0}, K)$の確率密度関数とします。 $$ \\phi (\\mathbf{x}) = \\dfrac{1}{\\sqrt{(2\\pi)^{p} \\left| K \\right|}} \\exp \\left( -\\dfrac{1}{2}\\mathbf{x}^{T} K^{-1} \\mathbf{x} \\right) $$ まず式$\\displaystyle \\int g(\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} = \\int \\phi (\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x}$が成立することを示します。$\\ln \\phi (\\mathbf{x})$を先に計算すると、\n$$ \\begin{align*} \\ln \\phi (\\mathbf{x}) \u0026amp;= \\ln\\dfrac{1}{\\sqrt{(2\\pi)^{p} \\left| K \\right|}} - \\dfrac{1}{2}\\mathbf{x}^{T} K^{-1} \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} x_{i} x_{j} \\end{align*} $$\n第一項はある定数$C$として表せ、第二項も$K^{-1}$に依存するある定数$a_{ji}$の二次形式として表せます。従って、\n$$ \\begin{align*} \\int g(\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} \u0026amp;= C \\int g(\\mathbf{x}) d \\mathbf{x} + \\int g(\\mathbf{x})\\sum a_{ij}x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} \\int g(\\mathbf{x}) x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij}K_{ij} \\qquad \\text{by assumption for $g$} \\end{align*} $$\nまた、\n$$ \\begin{align*} \\int \\phi (\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} \u0026amp;= C \\int \\phi (\\mathbf{x}) d \\mathbf{x} + \\int \\phi (\\mathbf{x})\\sum a_{ij}x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} \\int \\phi (\\mathbf{x}) x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij}K_{ij} \\qquad \\text{by definition of covariance} \\end{align*} $$\n相対エントロピーは常に$0$以上であるため、\n$$ \\begin{align*} 0 \u0026amp;\\le D(g \\| \\phi) \\\\ \u0026amp;= \\int g \\ln \\dfrac{g}{\\phi} \\\\ \u0026amp;= \\int g \\ln g - \\int g \\ln \\phi \\\\ \u0026amp;= - H(g) - \\int \\phi \\ln \\phi \\\\ \u0026amp;= - H(g) + H(\\phi) \\end{align*} $$\n正規分布のエントロピーは$\\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right]$なので、\n$$ H(X) = H(g) \\le H(\\phi) = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right] $$\nここで$X$を1次元確率変数としましょう。\n$$ \\begin{align*} E\\left[ (X - \\hat{X})^{2} \\right] \u0026amp;\\ge \\min_{X} E\\left[ (X - \\hat{X})^{2} \\right] \\\\ \u0026amp;= E\\left[ (X - E(X))^{2} \\right] \\\\ \u0026amp;= \\Var(X) \\end{align*} $$\n$(2)$が1次元の時、次の式を得ます。\n$$ \\begin{align*} \u0026amp;\u0026amp; H(X) \u0026amp;\\le \\dfrac{1}{2} \\ln(2\\pi e \\sigma^{2}) \\\\ \\implies \u0026amp;\u0026amp; 2H(X) \u0026amp;\\le \\ln(2\\pi e \\sigma^{2}) \\\\ \\implies \u0026amp;\u0026amp; e^{2H(X)} \u0026amp;\\le 2\\pi e \\sigma^{2} \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{2\\pi e}e^{2H(X)} \u0026amp;\\le \\sigma^{2} = \\Var(X) \\\\ \\end{align*} $$\nこの式に代入すると、\n$$ E\\left[ (X - \\hat{X})^{2} \\right] \\ge \\dfrac{1}{2\\pi e} e^{2H(X)} $$\n正規分布のエントロピー 正規分布$N(\\mu, \\sigma^{2})$のエントロピーは(自然対数を用いた場合)次のようになります。\n$$ H = \\dfrac{1}{2} \\ln (2\\pi e \\sigma^{2}) = \\ln \\sqrt{2\\pi e \\sigma^{2}} $$\n多変量正規分布$N_{n}(\\boldsymbol{\\mu}, K)$のエントロピーは次のようになります。\n$$ H = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right] = \\dfrac{1}{2}\\ln (\\det (2\\pi e K)) $$\n関連項目 確率情報理論で定義されるシャノンエントロピー 熱力学で定義されるエントロピー ギブスのエントロピー表現 キム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen M. Barnett, Quantum Information (2009), p7-10\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3400,"permalink":"https://freshrimpsushi.github.io/jp/posts/3400/","tags":null,"title":"古典情報理論におけるシャノン・エントロピー"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r정의1 이산확률변수 $X$에 대해서, $X=x$인 사건의 정보(량)information $I$를 다음과 같이 정의한다.\n$$ \\begin{equation} I(x) = -\\log_{2} p(x) \\end{equation} $$\n$p$는 $X$의 확률질량함수이다.\n설명 추상적 개념인 정보에 대한 정량적인 정의를 제시한 사람은 디지털 논리회로 이론과 정보이론을 창시한 클래드 섀넌Claude Shannon이다. 정보량를 '확률의 마이너스 로그'로 정의한 것을 처음 볼 때는 이해가 안되겠지만, 설명을 듣고 나면 이보다 자연스러울 수 없다는 생각이 들 것이다.\n정보의 가치는 일어나기 힘든 일일수록, 그러니까 일어날 확률이 희박할수록 크다. 가령 \u0026quot;내일 물리과 건물에 물리과 학과장님이 오신다\u0026quot;는 문장이 갖고 있는 정보량은 거의 없다고 볼 수 있다. 당연히 내일 학과장이 출근할 것이기 때문이다. 반면에 \u0026quot;내일 물리과 건물에 아이브가 온다\u0026quot;는 문장은 완전히 특급 정보이다. 아이브가 뜬금없이 물리과 건물에 등장할 확률은 거의 없다시피하므로, 이런 정보는 가치가 아주 높은 정보라고 할 수 있다. 다른 예로 \u0026quot;내일 삼성전자의 주식 상승폭이 $1 \\%$ 포인트 이내이다\u0026quot;는 거의 가치가 없는 정보이겠지만, \u0026quot;내일 삼성전자의 주식이 상한가를 친다\u0026quot;는 엄청난 정보이다. 따라서 일어날 확률이 적은 사건이 많은 정보를 갖고있다고 볼 수 있다.\n확률의 함숫값은 $0 \\le p \\le 1$이므로, $p$가 작을수록 정보의 함숫값이 커지도록 하려면 마이너스 로그를 취하면 된다. 따라서 자연스럽게 정보를 $(1)$과 같이 정의할 수 있다.\n$-\\log_{2}(x)$의 치역이 $[0, \\infty)$이므로 확률인 $1$인 사건, 그러니까 반드시 일어나는 일은 정보량이 $0$이다. 또한 일어날 확률이 낮아질수록 정보의 가치는 계속 커진다.\n확률변수 $X$ 자체에 대한 정보량은 엔트로피라 부른다.\n같이보기 확률정보이론에서 정의되는 정보 김영훈·허재성, 양자 정보 이론 (2020), p246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3398,"permalink":"https://freshrimpsushi.github.io/jp/posts/3398/","tags":null,"title":"고전정보이론에서 정보량이란?"},{"categories":"줄리아","contents":"##概要1\nframestyle 属性を使って図の軸や枠線のスタイルを変更できる。可能なオプションは次の通りだ。\n:box :semi :axes :origin :zerolines :grid :none コード デフォルト設定は :axes だ。\nusing Plots\rx = rand(10)\ry = rand(10)\rp = plot(scatter(y, title=\u0026#34;dafault\u0026#34;, label=\u0026#34;\u0026#34;), scatter(y, title=\u0026#34;:axes\u0026#34;, framestyle=:axse, label=\u0026#34;\u0026#34;), size=(600,300))\rsavefig(p, \u0026#34;default.png\u0026#34;) 各属性によるスタイルは次の通りだ。\np = scatter(fill(x, 6), fill(y, 6), framestyle=[:box :semi :origin :zerolines :grid :none], title=[\u0026#34;:box\u0026#34; \u0026#34;:semi\u0026#34; \u0026#34;:origin\u0026#34; \u0026#34;:zerolines\u0026#34; \u0026#34;:grid\u0026#34; \u0026#34;:none\u0026#34;], layout=6, label=\u0026#34;\u0026#34;, size=(800,450))\rsavefig(p, \u0026#34;framestyle.png\u0026#34;) https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3376,"permalink":"https://freshrimpsushi.github.io/jp/posts/3376/","tags":null,"title":"ジュリアプロットにおける軸のスタイルの変更方法 `framestyle`e`"},{"categories":"줄리아","contents":"概要 ジュリアで、\u0026lt;condition\u0026gt; \u0026amp;\u0026amp; \u0026lt;statement\u0026gt;は\u0026lt;condition\u0026gt;が真のとき\u0026lt;statement\u0026gt;が実行される。関数としては真の場合\u0026lt;statement\u0026gt;の結果が返され、偽の場合\u0026lt;statement\u0026gt;は評価Evaluationさえされない。\n効率的かつ簡潔にコードを書くことができる一方で、可読性が落ちる可能性がある点は受け入れなければならない。また、自分が好んで使わないとしても、他人が書いたコードを読むためには理解しておく必要がある。何の文脈もなく突然出てきたとき、このような文法を知らないと全く理解できない。\n参照 ショートサーキット コード 基本使用例 julia\u0026gt; num = []\rAny[]\rjulia\u0026gt; iseven(2) \u0026amp;\u0026amp; push!(num, 2)\r1-element Vector{Any}:\r2 2は偶数なので、push!(num, 2)が評価され空の配列numに2が入った。\nリターン julia\u0026gt; check = iseven(4) \u0026amp;\u0026amp; push!(num, 4)\r2-element Vector{Any}:\r2\r4\rjulia\u0026gt; check\r2-element Vector{Any}:\r2\r4 \u0026amp;\u0026amp;も関数として何らかの値をリターンできる。この時、checkはcheck = push!(num, 4)をリターンされたと見ることができる。\njulia\u0026gt; check = iseven(5) \u0026amp;\u0026amp; push!(num, 5)\rfalse\rjulia\u0026gt; num\r2-element Vector{Any}:\r2\r4\rjulia\u0026gt; check\rfalse 一方で\u0026lt;statement\u0026gt;が偽の場合、\u0026lt;statement\u0026gt;は評価されず\u0026amp;\u0026amp;自体がfalseをリターンした。\n否定 julia\u0026gt; iseven(6) || push!(num, 6)\rtrue \u0026amp;\u0026amp;の代わりに||を使用する。\n全コード num = []\riseven(2) \u0026amp;\u0026amp; push!(num, 2)\rcheck = iseven(4) \u0026amp;\u0026amp; push!(num, 4)\rcheck\rcheck = iseven(5) \u0026amp;\u0026amp; push!(num, 5)\rnum\rcheck\riseven(6) || push!(num, 6) 環境 OS: Windows julia: v1.6.3 ","id":2341,"permalink":"https://freshrimpsushi.github.io/jp/posts/2341/","tags":null,"title":"ジュリアで条件文を簡潔に書く方法"},{"categories":"줄리아","contents":"概要 特定の値に変更する方法は、列ごとに変更するので不便で、データフレーム全体でNaNを扱うときはもっといいトリックを使ってみる価値がある。\nコード julia\u0026gt; df = DataFrame(rand(1:9,3,3), :auto) ./ DataFrame(rand(0:1,3,3), :auto)\r3×3 DataFrame\rRow │ x1 x2 x3 │ Float64 Float64 Float64\r─────┼───────────────────────────\r1 │ 5.0 Inf 7.0\r2 │ Inf 8.0 Inf\r3 │ 4.0 1.0 4.0 例えば、上のデータフレームでInfを0に置換したい場合、次のようにたった一行で変更できる。\njulia\u0026gt; ifelse.(isinf.(df), 0, df)\r3×3 DataFrame\rRow │ x1 x2 x3 │ Float64 Float64 Float64\r─────┼───────────────────────────\r1 │ 5.0 0.0 7.0\r2 │ 0.0 8.0 0.0\r3 │ 4.0 1.0 4.0 もちろん、ifelse.(isinf.(df), 0, df)のisinfをisnanに変更すればNaNを扱い、0を任意の値に変更することができる。\n全コード using DataFrames, Random\rRandom.seed!(0)\rdf = DataFrame(rand(1:9,3,3), :auto) ./ DataFrame(rand(0:1,3,3), :auto)\rifelse.(isinf.(df), 0, df) 参考 データフレームの特定の値を変更する方法 環境 OS: Windows julia: v1.6.3 ","id":2330,"permalink":"https://freshrimpsushi.github.io/jp/posts/2330/","tags":null,"title":"JuliaのデータフレームでNaNを0に置き換える方法"},{"categories":"줄리아","contents":"概要 ジュリアでのA ? B : Cは、いわゆる三項演算子Ternary Operatorで、Aが真ならB、偽ならCを返す関数だ。数学的に二項演算が関数として定義されるように、三項演算もまた関数だ。条件文と似ているけれども、このような本質的な違いがあるため、慣れれば非常に便利に使える。ただし、読みやすいコードからはちょっと離れることがあるので、無理に乱用する必要はないし、逆に気に入らなくても他の人が使うかもしれないから、ある程度は慣れておく必要がある。\nコード julia\u0026gt; x = iseven(2) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; x\r\u0026#34;even\u0026#34;\rjulia\u0026gt; y = iseven(3) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; y\r\u0026#34;odd\u0026#34; 上の命令は、与えられた数が偶数か奇数かに応じて、変数x、yに\u0026quot;even\u0026quot;または\u0026quot;odd\u0026quot;という文字列を代入している。\njulia\u0026gt; x * y\r\u0026#34;evenodd\u0026#34; 条件文じゃなく関数だから、こんなに便利なコードが書ける。同じ機能を条件文だけでやろうとすると、スコープScopeなどの問題で無駄に長くなりがちだ。\n全体のコード x = iseven(2) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; x\ry = iseven(3) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; y\rx * y 環境 OS: Windows julia: v1.6.3 ","id":2328,"permalink":"https://freshrimpsushi.github.io/jp/posts/2328/","tags":null,"title":"ジュリアの三項演算子 ? :"},{"categories":"줄리아","contents":"概要 replace!() メソッドを使えばいい1。最初の引数には変更するデータフレームのカラムが入り、二番目の引数にはペア [ペア](../2201) A =\u0026gt; B` が入る。ここで、データフレームのカラムが入ることが重要だ。\nコード julia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 보스즈\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 보스즈\r8 │ 지연 95 163 보스즈\r9 │ 진숙 99 162 쪼꼬미\r10 │ 현정 94 165 보스즈 例として使用する WJSN データフレームは上記の通りだ。\njulia\u0026gt; replace!(WJSN.member, \u0026#34;진숙\u0026#34; =\u0026gt; \u0026#34;여름\u0026#34;); WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 보스즈\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 보스즈\r8 │ 지연 95 163 보스즈\r9 │ 여름 99 162 쪼꼬미\r10 │ 현정 94 165 보스즈 :member 列の \u0026quot;진숙\u0026quot; を \u0026quot;여름\u0026quot; に変えた。ここで replace() ではなく replace!() を使用した点、データフレームそのものではなくその特定の列を指定した点に注意しよう。\njulia\u0026gt; replace!(WJSN.unit, \u0026#34;보스즈\u0026#34; =\u0026gt; \u0026#34;더블랙\u0026#34;); WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 더블랙\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 더블랙\r8 │ 지연 95 163 더블랙\r9 │ 여름 99 162 쪼꼬미\r10 │ 현정 94 165 더블랙 :unit 列の \u0026quot;보스즈\u0026quot; を \u0026quot;더블랙\u0026quot; に一括変更した。\n全体のコード using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;보스즈\u0026#34;]\r)\rWJSN\rreplace!(WJSN.member, \u0026#34;진숙\u0026#34; =\u0026gt; \u0026#34;여름\u0026#34;); WJSN\rreplace!(WJSN.unit, \u0026#34;보스즈\u0026#34; =\u0026gt; \u0026#34;더블랙\u0026#34;); WJSN 併せて見る データフレーム全体を一度にNaNを0に変える方法 環境 OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Replacing-Data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2326,"permalink":"https://freshrimpsushi.github.io/jp/posts/2326/","tags":null,"title":"ジュリアでのデータフレーム特定値の変更方法"},{"categories":"줄리아","contents":"概要 1 FreqTables.jlパッケージのfreqtable()関数を使えばいい。Rのfreq()関数と似た機能を持っている。\nコード 配列 julia\u0026gt; compartment = rand([\u0026#39;S\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;], 1000);\rjulia\u0026gt; freqtable(compartment)\r3-element Named Vector{Int64}\rDim1 │\r──────┼────\r\u0026#39;I\u0026#39; │ 316\r\u0026#39;R\u0026#39; │ 342\r\u0026#39;S\u0026#39; │ 342 上記のように配列を入れると、各階級ごとにカウントしてくれる。\nデータフレーム freqtable()は特にデータフレームに便利だ。Rでの質的変数を含む回帰分析の例と同様に、組み込みデータToothGrowthを読み込んでみよう。\njulia\u0026gt; ToothGrowth = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;ToothGrowth\u0026#34;)\r60×3 DataFrame\rRow │ Len Supp Dose │ Float64 Cat… Float64\r─────┼────────────────────────\r1 │ 4.2 VC 0.5\r2 │ 11.5 VC 0.5\r3 │ 7.3 VC 0.5\r4 │ 5.8 VC 0.5\r⋮ │ ⋮ ⋮ ⋮\r58 │ 27.3 OJ 2.0\r59 │ 29.4 OJ 2.0\r60 │ 23.0 OJ 2.0\r53 rows omitted\rjulia\u0026gt; freqtable(ToothGrowth, :Len)\r43-element Named Vector{Int64}\rLen │\r─────┼──\r4.2 │ 1\r5.2 │ 1\r5.8 │ 1\r6.4 │ 1\r⋮ ⋮\r29.5 │ 1\r30.9 │ 1\r32.5 │ 1\r33.9 │ 1\rjulia\u0026gt; freqtable(ToothGrowth, :Supp)\r2-element Named Vector{Int64}\rSupp │\r──────┼───\r\u0026#34;OJ\u0026#34; │ 30\r\u0026#34;VC\u0026#34; │ 30\rjulia\u0026gt; freqtable(ToothGrowth, :Dose)\r3-element Named Vector{Int64}\rDose │\r──────┼───\r0.5 │ 20\r1.0 │ 20\r2.0 │ 20 ToothGrowthは、ビタミンCまたはオレンジジュース:Suppを異なる量:Doseで餌として与えられたモルモットの歯の長さ:Lenを記録したデータだ。各カラムごとに頻度を計算すると、上記のようにきれいに整理される。ここで、データが必ずしもカテゴリカルデータである必要はないことが確認できる。\njulia\u0026gt; freqtable(ToothGrowth, :Supp, :Dose)\r2×3 Named Matrix{Int64}\rSupp ╲ Dose │ 0.5 1.0 2.0\r────────────┼──────────────\r\u0026#34;OJ\u0026#34; │ 10 10 10\r\u0026#34;VC\u0026#34; │ 10 10 10 もちろん、このようなテーブルはカテゴリカルデータを扱う時に最も効果的だ。:Supp, :Doseに対する頻度を計算すると、自動的に2次元にカテゴリを分けて頻度を計算してくれた。\njulia\u0026gt; freqtable(ToothGrowth, :Len, :Dose, :Supp)\r43×3×2 Named Array{Int64, 3}\r[:, :, Supp=\u0026#34;OJ\u0026#34;] =\rLen ╲ Dose │ 0.5 1.0 2.0\r───────────┼──────────────\r4.2 │ 0 0 0\r⋮ ⋮ ⋮ ⋮\r33.9 │ 0 0 0\r[:, :, Supp=\u0026#34;VC\u0026#34;] =\rLen ╲ Dose │ 0.5 1.0 2.0\r───────────┼──────────────\r4.2 │ 1 0 0\r⋮ ⋮ ⋮ ⋮\r33.9 │ 0 0 1 3つ以上のカラムに対する計算は、ただの2次元テーブルを階級の数だけリターンする。この辺りになると、データを探索したり要約する意味はほとんどなくなる。\n性能比較 julia\u0026gt; @time for t in 1:10^4\rfreqtable(compartment)\rend\r@time for t in 1:10^4\rcount(compartment .== \u0026#39;S\u0026#39;)\rcount(compartment .== \u0026#39;I\u0026#39;)\rcount(compartment .== \u0026#39;R\u0026#39;)\rend\r0.068229 seconds (340.00 k allocations: 27.466 MiB)\r0.059198 seconds (180.00 k allocations: 134.125 MiB, 36.71% gc time) テーブルを離れて、頻度数の計算自体が有用だと思われる。直接カウントすることとfreqtable()を通じて一度に頻度数を計算する速度を色々と計測してみたけど、どちらかが必ずしも速かったわけではなかった。データの量や階級の数によって、その都度前後したが、全体的にfreqtable()が遅い傾向にあった。それでも大きく遅れを取るわけではないので、速度を考慮に入れずに、使うときはよく考えて使おう。\n全コード using FreqTables\rcompartment = rand([\u0026#39;S\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;], 1000);\rfreqtable(compartment)\rusing RDatasets\rToothGrowth = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;ToothGrowth\u0026#34;)\rfreqtable(ToothGrowth, :Len)\rfreqtable(ToothGrowth, :Supp)\rfreqtable(ToothGrowth, :Dose)\rfreqtable(ToothGrowth, :Supp, :Dose)\rfreqtable(ToothGrowth, :Len, :Dose, :Supp)\r@time for t in 1:10^4\rfreqtable(compartment)\rend\r@time for t in 1:10^4\rcount(compartment .== \u0026#39;S\u0026#39;)\rcount(compartment .== \u0026#39;I\u0026#39;)\rcount(compartment .== \u0026#39;R\u0026#39;)\rend 環境 OS: Windows julia: v1.6.3 FreqTables v0.4.5 https://github.com/nalimilan/FreqTables.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2324,"permalink":"https://freshrimpsushi.github.io/jp/posts/2324/","tags":null,"title":"ジュリアで周波数を計算する方法"},{"categories":"줄리아","contents":"ガイド 上のようなexample.csvファイルがあるとしよう。このデータフレームに読み込むとき、データ全体ではなく、列名だけを保持し、中身が空のデータフレームを作りたい場合がある。空のデータフレームが必要な場合があるから、このような場合も間違いなくある。\nusing CSV # 行なしでデータフレームを読み込む df_empty = CSV.read(\u0026#34;example.csv\u0026#34;, DataFrame; limit = 0) 上の実行結果で作成されたデータフレームは3つの列をそのまま持っているが、その内容物は全く持っていなかった。\nCSV.read(limit = 1)\nlimit = 1オプションを通じて、データフレームを1行だけ読んだ。 CSV.read()[[false],:]\n長さ$1$のビット配列Bit Array[false]で、どの行も参照していなかった。これによって空のデータフレームが残った。 # 列名が正常に保持されていることを確認 column_names = names(df_empty) names()関数で列名を確認すると、列名が正常に保持されていることが分かる。\n# 空のデータフレームに新しいデータを挿入 push!(df_empty, [1, \u0026#34;new_data\u0026#34;, 3.14]) push!()で新しいデータを挿入すると、元々あった配列のようにきちんと動作することが確認できる。\n単にlimit = 0をすると？ # limit 0でデータフレームを読み込み、空のデータフレームを得ようとする試み df_empty_error = CSV.read(\u0026#34;example.csv\u0026#34;, DataFrame; limit = 0) StackOverflowErrorエラーが発生する。\n環境 OS: Windows julia: v1.6.3 ","id":2322,"permalink":"https://freshrimpsushi.github.io/jp/posts/2322/","tags":null,"title":"JuliaでCSVファイルから列だけを読み込む方法"},{"categories":"줄리아","contents":"ガイド 1 using RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rdescribe(iris) describe() 関数を使えばいい。iris データを要約してみよう。\njulia\u0026gt; describe(iris)\r5×7 DataFrame\rRow │ variable mean min median max nmissing eltype\r│ Symbol Union… Any Union… Any Int64 DataType\r─────┼────────────────────────────────────────────────────────────────────────────────────────────\r1 │ SepalLength 5.84333 4.3 5.8 7.9 0 Float64\r2 │ SepalWidth 3.05733 2.0 3.0 4.4 0 Float64\r3 │ PetalLength 3.758 1.0 4.35 6.9 0 Float64\r4 │ PetalWidth 1.19933 0.1 1.3 2.5 0 Float64\r5 │ Species setosa virginica 0 CategoricalValue{String, UInt8} 環境 OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Summarizing-Data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2320,"permalink":"https://freshrimpsushi.github.io/jp/posts/2320/","tags":null,"title":"ジュリアでデータフレームの要約を見る方法"},{"categories":"줄리아","contents":"概要 JuliaのCategoricalArrays.jlパッケージは、Rのfactorと似た機能を果たす。\nコード julia\u0026gt; A = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;]\r4-element Vector{String}:\r\u0026#34;red\u0026#34;\r\u0026#34;blue\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; B = categorical(A)\r4-element CategoricalArray{String,1,UInt32}:\r\u0026#34;red\u0026#34;\r\u0026#34;blue\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; levels(B)\r3-element Vector{String}:\r\u0026#34;blue\u0026#34;\r\u0026#34;green\u0026#34;\r\u0026#34;red\u0026#34; categorical() categorical()関数で通常の配列をカテゴリカル配列にキャストCastできる。\nlevels() levels()関数では、カテゴリーを確認できる。当然、カテゴリーに重複はなく、配列にそのカテゴリーに対応する要素がなくてもカテゴリー自体は維持される。\njulia\u0026gt; B[2] = \u0026#34;red\u0026#34;; B\r4-element CategoricalArray{String,1,UInt32}:\r\u0026#34;red\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; levels(B)\r3-element Vector{String}:\r\u0026#34;blue\u0026#34;\r\u0026#34;green\u0026#34;\r\u0026#34;red\u0026#34; このように配列の状態に関係なくカテゴリーが維持される特徴は、特定のコーディングで非常に便利な特性となる。特にデータ分析と関連した作業では、データセットのサブセットSubsetを多く扱うが、その時カテゴリカル配列を知っていれば大きな助けとなる。\n最適化 わざわざlevels()を使わなくても、ただの通常の配列でunique()を使えば似たような実装はできる。\njulia\u0026gt; @time for t in 1:10^6\runique(A)\rend\r0.543157 seconds (6.00 M allocations: 579.834 MiB, 17.33% gc time)\rjulia\u0026gt; @time for t in 1:10^6\rlevels(B)\rend\r0.013324 seconds しかし、その速さは約40倍の差がある。元々配列が変化するたびにカテゴリーが更新されるため、別途計算する必要なく直接参照できる。\n全コード using CategoricalArrays\rA = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;]\rB = categorical(A)\rlevels(B)\rB[2] = \u0026#34;red\u0026#34;; B\rlevels(B)\r@time for t in 1:10^6\runique(A)\rend\r@time for t in 1:10^6\rlevels(B)\rend 環境 OS: Windows julia: v1.6.3 CategoricalArrays v0.10.2 ","id":2318,"permalink":"https://freshrimpsushi.github.io/jp/posts/2318/","tags":null,"title":"ジュリアのカテゴリカル配列"},{"categories":"줄리아","contents":"ガイド RDatasets.jl パッケージを使えば大丈夫。以下は最も簡単な iris データセットを読み込む例です。基本組み込みデータセットの他にも様々なデータセットが含まれているから、GitHubをチェックするといい1。\njulia\u0026gt; using RDatasets\rjulia\u0026gt; iris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\r150×5 DataFrame\rRow │ SepalLength SepalWidth PetalLength PetalWidth Species\r│ Float64 Float64 Float64 Float64 Cat…\r─────┼─────────────────────────────────────────────────────────────\r1 │ 5.1 3.5 1.4 0.2 setosa\r2 │ 4.9 3.0 1.4 0.2 setosa\r3 │ 4.7 3.2 1.3 0.2 setosa\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮\r149 │ 6.2 3.4 5.4 2.3 virginica\r150 │ 5.9 3.0 5.1 1.8 virginica\r145 rows omitted 参考 Rで組み込みデータセットを読み込む方法 環境 OS: Windows julia: v1.6.3 https://github.com/JuliaStats/RDatasets.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2316,"permalink":"https://freshrimpsushi.github.io/jp/posts/2316/","tags":null,"title":"ジュリアでRで使用されていた組み込みデータセットを読み込む方法"},{"categories":"줄리아","contents":"## ガイド\r例として`Plots.jl`パッケージのバージョンを確認してみよう。REPLで`]`キーを押すとパッケージモードに入る。ここで`status foo`と入力すれば、次のように`foo`パッケージのバージョンを確認できる。\r![20211204_193048.png](20211204_193048.png#center)\r## 環境\r- OS: Windows\r- julia: v1.6.3 ","id":2313,"permalink":"https://freshrimpsushi.github.io/jp/posts/2313/","tags":null,"title":"ジュリアでパッケージバージョンを確認する方法"},{"categories":"줄리아","contents":"概要 isempty() 関数を使用すればいい。\nコード julia\u0026gt; isempty([])\rtrue\rjulia\u0026gt; isempty(Set())\rtrue\rjulia\u0026gt; isempty(\u0026#34;\u0026#34;)\rtrue タイトルでは配列とされているが、実際には集合や文字列でも良い。\n最適化 もちろん配列が空かどうかは、length()が $0$ かどうかで確認しても構わない。\njulia\u0026gt; @time for t in 1:10^6\risempty([])\rend\r0.039721 seconds (1000.00 k allocations: 76.294 MiB, 27.85% gc time)\rjulia\u0026gt; @time for t in 1:10^6\rlength([]) == 0\rend\r0.041762 seconds (1000.00 k allocations: 76.294 MiB, 19.18% gc time) 見てのとおり、空の配列の場合、二つの方法の性能差はない。\njulia\u0026gt; x = 1:10^6;\rjulia\u0026gt; @time for t in 1:10^6\risempty(x)\rend\r0.017158 seconds\rjulia\u0026gt; @time for t in 1:10^6\rlength(x) == 0\rend\r0.043243 seconds (1000.00 k allocations: 15.259 MiB) 一方で、配列が空でない場合、上記のように2倍以上の速度差が見られる。length()は具体的な長さを返さなければならないのに対し、isempty()は最初の要素が存在するかだけを確認すれば良いので、これは当然のことである。可読性の側面や条件文を使用する状況まで考えると、isempty()を使用する方がより望ましい。\n全コード isempty([])\risempty(Set())\risempty(\u0026#34;\u0026#34;)\r@time for t in 1:10^6\risempty([])\rend\r@time for t in 1:10^6\rlength([]) == 0\rend\rx = 1:10^6\r@time for t in 1:10^6\risempty(x)\rend\r@time for t in 1:10^6\rlength(x) == 0\rend 環境 OS: Windows julia: v1.6.3 ","id":2311,"permalink":"https://freshrimpsushi.github.io/jp/posts/2311/","tags":null,"title":"ジュリアで配列が空かどうかを確認する方法"},{"categories":"줄리아","contents":"概要 地の果てまで一人で居る辛さを知ってる人は、ああ、分かるんだ\nコーディング中にわからないエラーに苦労した人は、プログラミングにおいてエラーがとても大事だということを分かっている\u0026hellip;\nJuliaでは、error() 関数や @error マクロを使ってエラーを出すことができる。現在のJulia v1.63を基準に、25種類の組み込み例外が定義されている1。\nコード julia\u0026gt; log(1 + 2im)\r0.8047189562170501 + 1.1071487177940904im 例えば、プログラムで 対数関数 $\\log$ を使う時、入力は実数だけ許されるべきだと考えられる。しかし、Juliaでは基本的に複素数に拡張された $\\log_{\\mathbb{C}}$ を提供している。プログラムがエラーなしで動いていることは良いことではない。意図しない計算は予期しない問題を引き起こすから、望まない計算が行われたら、最初からエラーを出してはいけない。\n元のlogの定義域を実数 $\\mathbb{R}$ に限定するコードを作ってみよう。\nerror() 関数 julia\u0026gt; function Rlog(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\rerror(DomainError, \u0026#34;: Rlog allow real number only\u0026#34;)\rend\rend\rRlog(1 + 2im)\rERROR: LoadError: DomainError: Rlog allow real number only\rStacktrace:\r[1] error(::Type, ::String)\r@ Base .\\error.jl:42\r[2] Rlog(x::Complex{Int64})\r@ Main c:\\admin\\REPL.jl:7\r[3] top-level scope\r@ c:\\admin\\REPL.jl:11\rin expression starting at c:\\admin\\REPL.jl:11 上の Rlogでは、入力が実数でなければDomainErrorをレイズするように限定した。\n@error マクロ julia\u0026gt; function Rlog2(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\r@error \u0026#34;Rlog2 also allow Real number only\u0026#34;\rend\rend\rRlog2(1 + 2im)\r┌ Error: Rlog2 also allow Real number only\r└ @ Main c:\\admin\\REPL.jl:17 上の Rlog2では、入力が実数でなければ、とりあえずエラーをスローするように限定した。\nレイズとスローは、どちらもエラーを起こすという意味で、大きな文脈での違いはない。レイズはPythonなどで使われる表現で、スローはJavaなどで使われる表現だ。\n全体のコード log(1 + 2im)\rfunction Rlog(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\rerror(DomainError, \u0026#34;: Rlog allow real number only\u0026#34;)\rend\rend\rRlog(1 + 2im)\rfunction Rlog2(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\r@error \u0026#34;Rlog2 also allow Real number only\u0026#34;\rend\rend\rRlog2(1 + 2im) 環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/control-flow/#Built-in-Exceptions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2309,"permalink":"https://freshrimpsushi.github.io/jp/posts/2309/","tags":null,"title":"ジュリアで例外処理する方法"},{"categories":"줄리아","contents":"概要 nrow(), ncol(), size() を使用できる。Rと違って、length()はエラーになる。\nコード julia\u0026gt; df = DataFrame(rand(100000,5), :auto)\r100000×5 DataFrame\rRow │ x1 x2 x3 x4 x5 │ Float64 Float64 Float64 Float64 Float64\r────────┼─────────────────────────────────────────────────────\r1 │ 0.474921 0.942137 0.0523668 0.588696 0.0176242\r2 │ 0.842828 0.910385 0.216194 0.794668 0.664883\r3 │ 0.0350312 0.96542 0.837923 0.920311 0.748409\r4 │ 0.613249 0.731643 0.941826 0.688649 0.161736\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮\r99998 │ 0.767794 0.242687 0.965885 0.557483 0.723849\r99999 │ 0.743936 0.67815 0.529923 0.247698 0.861302\r100000 │ 0.628269 0.252583 0.985485 0.24541 0.942741\r99993 rows omitted dfは、行が10万で、列が5つのデータフレームだ。\njulia\u0026gt; nrow(df)\r100000\rjulia\u0026gt; ncol(df)\r5\rjulia\u0026gt; size(df)\r(100000, 5) nrow()とncol()はそれぞれ行と列の数を返し、size()は行と列のサイズをタプルで返す。これを行、列の順序で参照すると、行と列のサイズを別々に知ることができる。一見すると、size()の方がずっと便利に見えるが、その性能を比較してみよう。\n最適化 julia\u0026gt; @time for i in 1:10^6\rnrow(df)\rend\r0.051730 seconds (1000.00 k allocations: 15.259 MiB)\rjulia\u0026gt; @time for i in 1:10^6\rsize(df)[1]\rend\r0.536297 seconds (3.00 M allocations: 61.035 MiB, 5.44% gc time) 上はnrow()とsize()の速度比較だ。当たり前だが、一つの機能しかしていないnrow()の方が速い。データフレームが大きくなるケース―ビッグデータを扱う場合や、size()を使ってもあまり差がないと思って時間の無駄を省ける。\nまた、コードの可読性の面では大きな差がある。nrow()とncol()は他の言語でも一般的に使用される関数名で、疑問の余地なく行、列の数だが、size()は後ろに付くインデックスのために、コードの可読性を大きく下げる。可能であれば、nrow()とncol()を使用することにしよう。\n全コード using DataFrames\rdf = DataFrame(rand(100000,5), :auto)\rnrow(df)\rncol(df)\rsize(df)\r@time for i in 1:10^6\rnrow(df)\rend\r@time for i in 1:10^6\rsize(df)[1]\rend 環境 OS: Windows julia: v1.6.3 ","id":2307,"permalink":"https://freshrimpsushi.github.io/jp/posts/2307/","tags":null,"title":"ジュリアでデータフレームのサイズを確認する方法"},{"categories":"줄리아","contents":"概要 ネームド・タプルが使える。ネームド・タプルを作る方法は、左の括弧のすぐ後ろにセミコロン;をつけることだ。例えば、DataFrame(; x, y)とすると、カラム名が:x、:yで、内容もそれぞれx、yのデータフレームが作られる。\nコード julia\u0026gt; MyCol7 = rand(5); B = 1:5;\rjulia\u0026gt; DataFrame(; MyCol7, B)\r5×2 DataFrame\rRow │ MyCol7 B │ Float64 Int64\r─────┼─────────────────\r1 │ 0.911763 1\r2 │ 0.93374 2\r3 │ 0.116779 3\r4 │ 0.467364 4\r5 │ 0.473437 5 環境 OS: Windows julia: v1.6.3 ","id":2305,"permalink":"https://freshrimpsushi.github.io/jp/posts/2305/","tags":null,"title":"ジュリアで変数名をカラム名として持つデータフレームを作成する方法"},{"categories":"줄리아","contents":"概要 名前付きタプルは、一般的なタプルとは異なり、辞書や構造体のように使用できるタプルだ。シンボルの配列をキーとして持ち、キーを使ってバリューにアクセスしつつ、タプルのようにも使用できる。\nコード x = rand(Bool, 5); y = rand(Bool, 5);\rz = (; x, y)\rtypeof(z)\rz.x 上のコードを実行して、名前付きタプルの使用方法を確認してみよう。\njulia\u0026gt; z = (; x, y)\r(x = Bool[0, 0, 1, 1, 0], y = Bool[1, 1, 0, 0, 0])\rjulia\u0026gt; typeof(z)\rNamedTuple{(:x, :y), Tuple{Vector{Bool}, Vector{Bool}}} 名前付きタプルを簡単に作る方法は、タプルを作りながら開き括弧の直後にセミコロン ; を付けることだ。例えば、(; x) は (; x=x) と同じだ。\njulia\u0026gt; z.x\r5-element Vector{Bool}:\r0\r0\r1\r1\r0\rjulia\u0026gt; z[2]\r5-element Vector{Bool}:\r1\r1\r0\r0\r0 名前付きタプルは、上のように名前のシンボルでアクセスすることも、インデックスでアクセスすることもできる。\n環境 OS: Windows julia: v1.6.3 ","id":2303,"permalink":"https://freshrimpsushi.github.io/jp/posts/2303/","tags":null,"title":"ジュリアのネームドタプル"},{"categories":"줄리아","contents":"概要 JuliaとNumPy、PyTorch（以降、便宜上Pythonと呼ぶ）の高次元配列を扱う際、各次元が意味するものが異なるため注意が必要だ。この違いはJuliaの配列が列優先であり、Pythonの配列が行優先であるために生じる。ちなみに同じ列優先のMatlabはJuliaとの違いがないため、Matlabに慣れているユーザーは特に注意する必要はないが、Pythonに慣れている人はインデックスの間違いに注意しよう。\n配列の次元とベクトルの次元を混同して使っているので、しっかり理解しよう。 説明 1次元配列 Juliaでは、サイズが$n$の配列は$n$次元の列ベクトルを意味する。\njulia\u0026gt; ones(3)\r3-element Vector{Float64}:\r1.0\r1.0\r1.0 Pythonでは、サイズが$n$の配列は$n$次元の行ベクトルを意味する。\n\u0026gt;\u0026gt;\u0026gt; import numpy as np\r\u0026gt;\u0026gt;\u0026gt; np.ones(3)\rarray([1., 1., 1.]) 列か行かの違いはあるが、1次元配列であるため、インデックスに特に注意する点はない。\n2次元配列 表面上は2次元までは違いがないように見える。しかし、その意味は異なるので注意が必要だ。まず、Juliaでは配列の次元が後方に伸びる。つまり$(m,n)$配列とは、サイズが$m$の1次元配列（列ベクトル）が$n$個あることを意味する。具体的には$(3,2)$配列は、3次元の列ベクトルが2個あるということだ。\njulia\u0026gt; ones(3,2)\r3×2 Matrix{Float64}:\r1.0 1.0\r1.0 1.0\r1.0 1.0 また、Juliaは「列優先」なので、成分のインデックスは上から下へ先に、そして左から右に大きくなる。\njulia\u0026gt; A = reshape(range(1,6), (3,2))\r3×2 reshape(::UnitRange{Int64}, 3, 2) with eltype Int64:\r1 4\r2 5\r3 6\rjulia\u0026gt; for i ∈ 1:6\rprintln(A[i])\rend\r1\r2\r3\r4\r5\r6 一方、Pythonの配列では新しい次元が前方へ伸びる。つまり$(m,n)$配列とは、サイズが$n$の1次元配列（行ベクトル）が$m$個あるということだ。以下の結果からは、配列が行単位で区分されているのがわかる。\n\u0026gt;\u0026gt;\u0026gt; np.ones([3,2])\rarray([[1., 1.],\r[1., 1.],\r[1., 1.]]) つまり、見た目だけでは、JuliaとPythonの$(m,n)$配列はどちらも$m \\times n$の行列だが、列優先/行優先の違いのためにインデックスの順序が異なる。インデックスの方向はJuliaでは上下左右、Pythonでは左右上下だ。\n# julia에서 2차원 배열의 인덱싱은 위에서 아래로, 그 다음 좌에서 우로\rjulia\u0026gt; A = reshape(range(1,6), (3,2))\r3×2 reshape(::UnitRange{Int64}, 3, 2) with eltype Int64:\r1 4\r2 5\r3 6\r# python에서 2차원 배열의 인덱싱은 좌에서 우로, 그 다음 위에서 아래로 \u0026gt;\u0026gt;\u0026gt; np.arange(6).reshape(3,2)\rarray([[0, 1],\r[2, 3],\r[4, 5]]) 3次元配列 Juliaでは、配列の新しい次元が後ろに追加されると言った。従って、$(m,n,k)$配列は、$(m,n)$配列が$k$個あることを意味する。\njulia\u0026gt; ones(3,2,4)\r3×2×4 Array{Float64, 3}:\r[:, :, 1] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 2] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 3] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 4] =\r1.0 1.0\r1.0 1.0\r1.0 1.0 一方、Pythonでは$(m,n,k)$配列は、$(n,k)$配列が$m$個あるということだ。\n\u0026gt;\u0026gt;\u0026gt; np.ones([3,2,4])\rarray([[[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.]]]) 機械学習で 画像データで、$H=\\text{hieht}$が高さ、$W=\\text{width}$が幅、$C=\\text{channel}$がチャンネル数、$B=\\text{batch size}$がバッチサイズとした場合、PyTorchでは$(B,C,H,W)$配列であり、Juliaでは$(H,W,C,B)$である。\n環境 OS: Windows11 バージョン: Julia 1.7.1, Python 3.9.2, numpy 1.19.5 ","id":3315,"permalink":"https://freshrimpsushi.github.io/jp/posts/3315/","tags":null,"title":"ジュリア、Python（NumPy、PyTorch）の配列の次元の違い"},{"categories":"머신러닝","contents":"概要 レファレンスと数式の番号や表記法は、論文をそのまま踏襲する。 Physics-informed neural networks (PINN[ピン]と読む)は、数値的に解くために設計された微分方程式の人工ニューラルネットワークであり、2018年Journal of Computational Physicsに発表された論文Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equationsで紹介されました。この論文の著者は、応用数学、機械工学のM. Raissi, P. Perdikaris, G.E. Karniadakisです。\nこの論文で述べられている物理情報physics informationは、壮大に見えるかもしれませんが、実際には与えられた微分方程式自体を意味すると考えても良いでしょう。つまり、\u0026lsquo;微分方程式を人工ニューラルネットワークで解く際、与えられた微分方程式を利用します\u0026rsquo;と言っているのと同じです。機械学習の論文を読むときは、このように見栄えの良い名前に惑わされないよう注意が必要です。\n微分方程式の数値的解法においてPINNが注目される理由は、損失関数に関するアイデアがシンプルで理解しやすく、実装も簡単だからでしょう。実際に論文の例では非常にシンプルなDNNが紹介されています。\n一般的に言われるPINNはSection 3.1で紹介されるモデルを指します。\n0. 抄録 著者はPINNを\u0026rsquo;与えられた非線形偏微分方程式を満たしながら、教師あり学習問題を解くために訓練された人工ニューラルネットワーク\u0026rsquo;と紹介しています。この論文で主に扱う2つの問題は、\u0026lsquo;data-driven solution and data-driven discovery of partial differential equations\u0026rsquo;です。性能評価のために、流体力学、量子力学、拡散方程式などの問題を解いてみました。\n1. 序論 最近の機械学習とデータ分析の進歩は、画像認識image recognition、認知科学cognitive science、ゲノム学genomicsなどの科学分野で革新的な結果をもたらしていますが、複雑な物理的、生物学的、工学的システムに対しては（データ収集コストが高いため）少ない情報で望ましい結果を導き出す必要がある困難があります。このような*小さなデータ領域small data regime*では、DNN、CNN、RNNなどの先進技術の収束性が保証されていません。\n[4-6]で、データ効率が良く（=少ないデータで）、物理情報を学習できる（=微分方程式を解くことができる）方法についての研\n究が進んでいます。非線形問題への拡張は、この論文の著者であるRaissiの後続研究[8,9]で提案されました。\n2. 問題設定 人工ニューラルネットワークで表される関数は、入力値（偏微分方程式でのソリューション$u$の座標$x, t$を指す）とパラメータによって関数値が決定されるが、これら2種類の変数に対して微分を行うために自動微分automatic differentiationを活用する。\nこのようなニューラルネットワークは、観測されたデータを支配する物理法則に起因する任意の対称性、不変性、または保存原理を尊重するように制約されている。これは、一般的な時間依存かつ非線形の偏微分方程式によってモデル化される。\nこの論文でこの文章が難しいと感じられるかもしれないが、私の考えでは簡単に言えば提案された人工ニューラルネットワークであるPINNが、与えられた微分方程式を満たす必要があるということだ。後述するが、微分方程式を満たす必要があるという条件を損失関数として使用するためである。\nこの論文の目的は、数理物理学におけるディープラーニングを進化させる新しいパラダイムのモデリングと計算パラダイムを提示することである。そのために、前述したように、この論文では主に2つの問題を扱う。一つは偏微分方程式のデータ駆動ソリューションdata-driven solutionであり、もう一つは偏微分方程式のデータ駆動発見data-driven discoveryである。使用された全てのコードとデータセットはhttps://github.com/maziarraissi/PINNsで確認できる。この論文では、$L1$、$L2$、ドロップアウトなどの正則化なしに、ハイパーボリックタンジェントを活性化関数として用いたシンプルなMLPが使用されている。各例では、ニューラルネットワークの構造、オプティマイザー、学習率などが具体的に紹介される。\nこの論文では、以下のようなパラメータ化された非線形偏微分方程式の一般的な形parameterized and nonlinear partial differential equations of the general formを扱う。\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u; \\lambda] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nここで、$u=u(t,x)$は(1)を満たす隠れた(=与えられていない=知られていない)関数、つまり(1)のソリューションであり、$\\mathcal{N}[\\cdot; \\lambda]$は$\\lambda$でパラメータ化された非線形演算子（NonlinearのNに由来する）であり、$\\Omega \\subset \\mathbb{R}^{D}$である。多くの数理物理学の問題problems in mathematical physicsは上記のような形で表される。例えば、1次\n元粘性バーガース方程式を見てみよう。\n$$ u_{t} + uu_{x} = \\nu u_{xx} $$\nこれは(1)で$\\mathcal{N}[u; \\lambda] = \\lambda_{1} uu_{x} - \\lambda_{2}u_{xx}$、$\\lambda = (\\lambda_{1}, \\lambda_{2})$の場合である。与えられた方程式(1)に対して、扱うべき2つの問題はそれぞれ以下の通りである。\n偏微分方程式のデータ駆動ソリューション: 固定された$\\lambda$に対して、システムのソリューション$u(t,x)$は何か？ 偏微分方程式のデータ駆動発見: 観測されたデータを最もよく表現するパラメータ$\\lambda$は何か？ 3. 偏微分方程式のデータ駆動ソリューション セクション3では、以下の形式の偏微分方程式からデータに基づいたソリューションを見つける問題について扱う。\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nつまり、$(1)$でパラメータ $\\lambda$ が固定されている状況である。セクション3.1とセクション3.2ではそれぞれ連続時間モデルと離散時間モデルを扱う。方程式を見つける問題はセクション4で扱う。ここで言う\u0026rsquo;データ\u0026rsquo;の意味は以下で詳しく説明する。\n3.1. 連続時間モデル $(t,x) \\in \\mathbb{R} \\times \\mathbb{R}$ とすると、$u : \\mathbb{R}^{2} \\to \\mathbb{R}$ である。これを人工ニューラルネットワークで近似するが、次のように実装されるシンプルなMLPを使用する。Juliaでは、\nusing Flux\ru = Chain(\rDense(2, 10, relu),\rDense(10, 10, relu),\rDense(10, 1)\r) PyTorchでは、\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rlayers = [2, 10, 10, 1]\rclass network(nn.Module):\rdef __init__(self):\rsuper(network, self).__init__()\rlayer_list = [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\rself.linears = nn.ModuleList(layer_list)\rdef forward(self, tx):\ru = tx\rfor i in range(len(layers)-2):\ru = self.linears[i](u)\ru = F.relu(u)\ru = self.linears[-1](u)\rreturn u\ru = network() これで $u$ は入力ノードが $2$ つ、出力ノードが $1$ つの人工ニューラルネットワークとなる。$(2)$ の左辺を次のような関数 $f = f(t,x; u)$ として定義しよう。\n$$ \\begin{equation} f := u_{t} + \\mathcal{N}[u] \\end{equation} $$\nここで $u$ は人工ニューラルネットワークであるため、$f$ も隠れ層のパラメータを持つ一種の人工ニューラルネットワークである。上記のような $f$ を物理情報に基づいたニューラルネットワークphysics-informed neural network, PINNと呼ぶ。言い換えれば、与えられた偏微分方程式そのものである。$f$ に含まれる微分は自動微分で実装され、$u$ と同じパラメータを共有する。人工ニューラルネットワーク $u$ が $(2)$ のソリューションを適切に近似していれば、$f$ の関数値はどこでも $0$ であるべきだ。ここから、$ f \\to 0$ になるように人工ニューラルネットワークを学習させることが推測できる。\n$(t_{u}^{i}, x_{u}^{i})$ を初期値、境界値が定義された領域の点とする。\n$$ (t_{u}^{i}, x_{u}^{i}) \\in( \\Omega \\times \\left\\{ 0 \\right\\}) \\cup (\\partial \\Omega \\times [0, T]) $$\n$u_{\\ast}$ を実際のソリューションとすると、初期条件と境界条件が与えられたということは、次のような値が与えられたということと同じである。\n$$ \\left\\{ t_{u}^{i}, x_{u}^{i}, u^{i} \\right\\}_{i=1}^{N_{u}},\\quad u^{i} = u_{\\ast}(t_{u}^{i}, x_{u}^{i}) $$\n理論上はこれらの値を無限に持つことになるが、数値的な問題では有限の点のみを扱えるので、$N_{u}$ 個を持っているとする。人工ニューラルネットワーク $u$ は $(t_{u}^{i}, x_{u}^{i})$ を入力として受け取り、$u^{i}$ を出力する必要があるので、これらがそれぞれ入力と対応するラベルとなる。\n$$ \\text{input} = (t_{u}^{i}, x_{u}^{i}),\\qquad \\text{label} = u^{i} $$\nこれがPINNで学習する\u0026lsquo;データ\u0026rsquo;である。それでは、損失関数を次のように設定できる。\n$$ MSE_{u} = \\dfrac{1}{N_{u}} \\sum\\limits_{i=1}^{N_{u}} \\left| u(t_{u}^{i},x_{u}^{i}) - u^{i} \\right|^{2} $$\nまた、$f$は適切な点集合（理論的には解 $u_{\\ast}$ が定義されるすべての点で満たされるべきだが、数値的には有限の点しか扱えない）$\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$で$(2)$を満たさなければならない。これらの適切な点を論文ではコロケーションポイントcollocation pointsと呼ぶ。コロケーションポイントに対して以下の損失関数を設定する。\n$$ MSE_{f} = \\dfrac{1}{N_{f}}\\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} $$\nつまり、$MSE_{f}$が$0$に近づくことは、物理的情報（偏微分方程式）を満たすことを意味する。したがって、人工ニューラルネットワーク $u$ を訓練するための最終的な損失関数は以下の通りである。\n$$ MSE = MSE_{u} + MSE_{f} $$\n論文では、$MSE_{f}$を使用することで物理的情報を制約として設けることは、[15, 16]で初めて研究されたが、PINN論文ではこれを現代的な計算ツールで検討し、より困難なダイナミックシステムに適用したと説明されている。\n物理情報に基づく機械学習physics-informed machine learningという用語自体は、Wangの乱流モデリングturbulence modelingに関する研究[17]で初めて使用されたとされる。しかし、PINN以前の研究では、サポートベクターマシン、ランダムフォレスト、FNNなどの機械学習アルゴリズムが単に使用されていたと説明されている。PINNがこれらと区別される点は、一般的に機械学習に使用されるパラメータに対する微分だけでなく、解の座標 $x, t$ に関する微分も考慮している点である。つまり、パラメータ $w$ を持つ人工ニューラルネットワークで近似された解を $u(t,x; w)$ とするとき、以前に提案された方法は偏微分 $u_{w}$ のみを利用したが、PINNは $u_{t}$ や $u_{x}$ などを利用して解を求める。このようなアプローチにより、少量のデータでも解をうまく見つけることができると説明されている。\nこの手続きがグローバル最小値に収束するという理論的な保証はないにもかかわらず、与えられた偏微分方程式が適切に定義されており、その解が一意であり、十分に表現力のあるニューラルネットワークアーキテクチャと十分な数のコロケーションポイント $N_{f}$ が与えられている場合、我々の方法は良好な\n予測精度good prediction accuracyを達成することが経験的に確認されていると論文には述べられている。\n3.1.1. 例（シュレーディンガー方程式） この例では、周期的な境界条件と複素数値を取る解に対して、提案された方法がうまく機能するかを重点的に確認する。例として、以下の初期条件と境界条件が与えられるシュレーディンガー方程式を扱う。\n$$ \\begin{align*} ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2}h \u0026amp;= 0,\\quad x\\in [-5, 5], t\\in[0, \\pi/2], \\\\ h(0,x) \u0026amp;= 2\\operatorname{sech} (x), \\\\ h(t,-5) \u0026amp;= h(t,5), \\\\ h_{x}(t,-5) \u0026amp;= h_{x}(t,5) \\end{align*} $$\n問題の解 $h_{\\ast}(t,x)$ は $h_{\\ast} : [0, \\pi/2] \\times [-5, 5] \\to \\mathbb{C}$ として複素関数値を持つ。しかし、関数の出力が複素数になるように人工ニューラルネットワークを定義するのではなく、実部を担当する $u(t,x)$ と虚部を担当する $v(t,x)$ の2次元ベクトルが出力されるように定義する。簡単に言えば、入力と出力のノードがそれぞれ2つのMLPとして定義することである。\n$$ h(t,x) = \\begin{bmatrix} u(t,x) \\\\[0.5em] v(t,x) \\end{bmatrix} $$\nこの問題におけるPINN $f$ は以下の通りである。\n$$ f := ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2} h $$\n$h(t,x)$ と $f(t,x)$ のパラメータは、初期値に対する損失 $MSE_{0}$、境界値に対する損失 $MSE_{b}$、物理情報に対する損失 $MSE_{f}$ を最小化するように学習される。\n$$ MSE = MSE_{0} + MSE_{b} + MSE_{f} $$\n$$ \\begin{align*} \\text{where } MSE_{0} \u0026amp;= \\dfrac{1}{N_{0}}\\sum_{i=1}^{N_{0}} \\left| h(0, x_{0}^{i}) - h_{0}^{i} \\right|^{2} \\quad (h_{0}^{i} = 2\\operatorname{sech} (x_{0}^{i})) \\\\ MSE_{b} \u0026amp;= \\dfrac{1}{N_{b}}\\sum_{i=1}^{N_{b}} \\left( \\left| h(t_{b}^{i}, -5) - h(t_{b}^{i}, 5) \\right|^{2} + \\left| h_{x}(t_{b}^{i},-5) - h_{x}(t_{b}^{i},5) \\right|^{2} \\right) \\\\ MSE_{f} \u0026amp;= \\dfrac{1}{N_{f}} \\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} \\end{align*} $$\n論文には $MSE_{b}$ の式にタイプミスがあるので注意すること。 ここで、$\\left\\{ x_{0}^{i}, h_{0}^{i} \\right\\}_{i=1}^{N_{0}}$ は初期値データ、$\\left\\{ t_{b}^{i} \\right\\}_{i=1}^{N_{b}}$ は境界でのコロケーションポイント、$\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$ は $f$ に対するコロケーションポイントである。\nデータセットを生成するために、従来のスペクトルメソッドspectral methodsを使用した。$h(0,x)$ での初期値データの数は $N_{0} = 50$、境界値データの数は $N_{b} = 50$ とし、ランダムに選んだ。また、$f$ のコロケーションポイントの数は $N_{f} = 20,000$ である。人工ニューラルネットワークは、100個のノードを持つ線形層を5層、層間の活性化関数としてハイパーボリックタンジェント $\\tanh$ を使用して構築した。\nFigure 1.\nFigure 1では、上の図は予測された解 $\\left| h(t, x) \\right|$ のヒートマップを示している。下の図は、時間がそれぞれ $t = 0.59, 0.79, 0.98$ のときの予測された解と実際の解がどれだけ一致しているかを示している。相対的 $L_{2}$ ノルムrelative $L_{2}$-norm は $0.00197 = 1.97 \\cdot 10^{-3}$ で、予測された解が正確な解と比較して約 $0.02%$ の差異があることを意味している。したがって、PINNは少ない初期値データでシュレーディンガー方程式の非線形挙動を正確に捉えることができる。\n現在扱っている連続時間モデルは、初期値が少なくてもうまく機能するが、コロケーションポイントの数 $N_{f}$ が十分に多くなければならないという潜在的な制約がある。これは空間の次元が2以下の場合はあまり問題にならないが、高次元の場合、必要なコロケーションポイントの数が指数関数的に増加する可能性があるため、問題になる可能性がある。そのため、次のセクションでは、多くのコロケーションポイントを必要としないようにするために、古典的なルンゲ・クッタ時間ステップスキームを活用した、より構造化されたニューラルネットワークを提案する。\n3.2. 離散時間モデル セクション3.1では、解を連続時間に対して近似した。この場合、人工ニューラルネットワークは全体の領域に対して同時に学習され、任意の点 $(x,t)$ に対して出力がある。このセクションでは、セクション3.1とは異なり、離散時間について扱う。つまり $t_{n}$ の値を知っているとき、$t_{n+1}$ の値を人工ニューラルネットワークで近似する方法について説明する。$(2)$ に $q$ ステージのルンゲ・クッタ法を適用すると、以下のようになる。\n$$ u(t_{n+1}, x) = u(t_{n}, x) - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u(t_{n}+c_{j} \\Delta t, x) \\right] $$\nここで $u^{n}(x) = u(t_{n}, x)$, $u^{n+c_{j}} = u(t_{n} + c_{j}\\Delta t, x)$ と表記すると、\n$$ \\begin{equation} \\begin{aligned} u^{n+1} \u0026amp;= u^{n} - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] \\\\ \\text{where } u^{n+c_{j}} \u0026amp;= u^{n} - \\Delta t \\sum_{i=1}^{q} a_{j,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] \\quad j=1,\\dots,q \\end{aligned}\\tag{7} \\end{equation} $$\n上記の $q+1$ 個の式で、右辺の $\\sum$ 項を全て左辺に移行する。そして、左辺を $u_{i}^{n}$ のように表記する。\n$$ \\begin{equation} \\begin{aligned} u_{q+1}^{n} \u0026amp;:= u^{n+1} + \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] = u^{n} \\\\ \\\\ u_{1}^{n} \u0026amp;:= u^{n+c_{1}} + \\Delta t \\sum_{i=1}^{q} a_{1,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ u_{2}^{n} \u0026amp;:= u^{n+c_{2}} + \\Delta t \\sum_{i=1}^{q} a_{2,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ \u0026amp;\\vdots \\\\ u_{q}^{n} \u0026amp;:= u^{n+c_{q}} + \\Delta t \\sum_{i=1}^{q} a_{q,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\end{aligned}\\tag{9} \\end{equation} $$\nすると、これらの全ての値が $u^{n}$ に等しくなることがわかる。\n$$ u^{n} = u_{1}^{n} = u_{2}^{n} = \\cdots = u_{q+1}^{n} \\tag{8} $$\nしたがって、セクション3.2で言及されている物理情報とは、与えられた初期条件・境界条件と $(8)$ を指す。今度は $u(t_{n+1}, x)$ を求めるために二つの人工ニューラルネットワークを定義する。セクション3.1で使用した人工ニューラルネットワークは、正確な解 $u_{\\ast}$ に収束することを期待する $u$ と、$u$ が満たすべき微分方程式 $f$ だったが、ここでは少し異なる。まず、人工ニューラルネットワーク $U$ を次の関数として定義する。\n$$ U : \\mathbb{R} \\to \\mathbb{R}^{q+1} $$\nつまり、入力層のノードが $1$ つ、出力層のノードが $q+1$ つのニューラルネットワークである。このニューラルネットワークの出力を以下の値とする。\n$$ U(x) = \\begin{bmatrix} u^{n+c_{1}}(x) \\\\[0.5em] u^{n+c_{2}}(x) \\\\ \\vdots \\\\[0.5em] u^{n+c_{q}}(x) \\\\[0.5em] u^{n+1}(x) \\end{bmatrix} \\tag{10} $$\nこのニューラルネットワークは、添付されたコード内の PhysicsInformedNN クラスで定義されている neural_net に該当する。\nつまり、以下の学習プロセスで $U$ の出力の最後の成分が $u(t_{n+1}, x)$ に収束することを期待している。二番目のニューラルネットワークは、$U$ の出力と $(7)$ の定義を利用して、次のように定義される関数である。\n3.2.1. 例（アレン・カーン方程式） 離散時間モデルにおける例として、以下の初期条件と周期的な境界条件が与えられるアレン・カーン方程式を取り上げる。\n$$ \\begin{equation} \\begin{aligned} \u0026amp;u_{t} - 0.0001u_{xx} + 5 u^{3} - 5u = 0,\\qquad x\\in [-1, 1], t\\in[0, 1], \\\\ \u0026amp;u(0,x) = x^{2} \\cos (\\pi x), \\\\ \u0026amp;u(t,-1) = u(t,1), \\\\ \u0026amp;u_{x}(t,-1) = u_{x}(t,1) \\end{aligned}\\tag{12} \\end{equation} $$\nこの例における $(9)$ に含まれる非線形演算子は以下の通りである。\n$$ \\mathcal{N}[u^{n+c_{j}}] = -0.0001u_{xx}^{n+c_{j}} + 5(u^{n+c_{j}})^{3} - 5u^{n+c_{j}} $$\nタイムステップ $t^{n}$ での $u$ の値を $u^{n,i}$ と表記する。\n$$ u^{n,i} = u^{n}(x^{n,i}) = u(t^{n}, x^{n,i}),\\qquad i=1,\\dots,N_{n} $$\n問題は $u^{n}$ が与えられたときに $u^{n+1}$ を計算することであり、$\\left\\{ x^{n,i}, u^{n,i} \\right\\}_{i=1}^{N_{n}}$ は与えられたデータセットである。$(8)$ により、このデータセットに対して以下が成り立つ。\n$$ u^{n,i} = u_{1}^{n}(x^{n,i}) = \\cdots = u_{q+1}^{n}(x^{n,i}) $$\nしたがって、以下のような二乗誤差の合計sum of squared error (SSE)を損失関数とする。\nここではなぜ $MSE$ ではなく $SSE$ を使用するのかは明確ではない。連続時間モデルでは $MSE$ を使用していたが、離散時間モデルでは $SSE$ を使用しており、何らかの実験的な理由があると思われる。 $$ SSE_{n} = \\sum\\limits_{j=1}^{q+1} \\sum\\limits_{i=1}^{N_{n}} \\left| u_{j}^{n} (x^{n,i}) - u^{n,i} \\right|^{2} $$\n各 $u_{j}^{n}$ は $(9)$ によって計算され、この計算にはニューラルネットワーク $U$ の出力が使用される。このロスは、添付されたコード内の PhysicsInformedNN クラスで定義されている net_U0 に対応する。そして $U$ の出力は $(12)$ の境界条件を満たさなければならないため、以下のような損失関数を設定する。\n$$ \\begin{align*} SSE_{b} \u0026amp;= \\sum\\limits_{i=1}^{q} \\left| u^{n+c_{i}}(-1) - u^{n+c_{i}}(1) \\right|^{2} + \\left| u^{n+1}(-1) - u^{n+1}(1) \\right|^{2} \\\\ \u0026amp;\\quad+ \\sum\\limits_{i=1}^{q} \\left| u_{x}^{n+c_{i}}(-1) - u_{x}^{n+c_{i}}(1) \\right|^{2} + \\left| u_{x}^{n+1}(-1) - u_{x}^{n+1}(1) \\right|^{2} \\ \\end{align*} $$\nこれらの合計が最終的なロスである。\n$$ SSE = SSE_{n} + SSE_{b} $$\nFigure 2.\nFig. 2では、上の図が正確な解のヒートマップを示している。下の図では、$t=0.1$ での $u$ を知っているときに $t=0.9$ での値を予測した結果を示している。下の左側の図では、青い線が正確な解であり、$\\color{red}\\mathsf{X}$ がデータとして使用された点を示している。下の右側の図では、青い線が正確な解であり、赤い線が予測された解である。\n暗黙のルンゲ・クッタ法 (IRK) では $u^{n+c_{j}}$ を計算するためにすべての $j$ に対する連立方程式を解く必要があるため、$q$ が大きくなると計算コストが大幅に増加するが、この論文で提案されている方法では $q$ が大きくなってもそれに伴う追加コストは非常に少ないと説明されている。また、$q$ が小さい場合、IRK ではタイムステップ $\\Delta t$ が大きいと正確な予測ができないが、PINN の場合は $\\Delta t$ が大きくても正確に予測できると説明されている。\n4. 偏微分方程式のデータ駆動発見 この章では、観測データがある場合に、偏微分方程式 $(1)$ のパラメータ $\\lambda$ を見つける問題について扱う。詳細は以下の例を通じて説明する。\n4.1. 連続時間モデル $f$ を以下のように $(1)$ の左辺として定義しよう。\n$$ f = u_{t} + \\mathcal{N}[u; \\lambda] $$\nセクション3の $(3)$ と異なる点は、$\\lambda$ が固定された定数ではなく、学習が必要な未知のパラメータになったことである。\n4.1.1. 例（ナヴィエ–ストークス方程式） セクション4.1.1では、非圧縮性流体の実際のデータに関する例として、ナヴィエ–ストークス方程式によって表されるケースを紹介する。以下の2次元ナヴィエ–ストークス方程式を考える。\n$$ \\begin{equation} \\begin{aligned} u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) \u0026amp;= -p_{x} + \\lambda_{2}(u_{xx} + u_{yy}) \\\\ v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) \u0026amp;= -p_{y} + \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned} \\tag{15} \\end{equation} $$\nここで、$u(t,x,y)$ は流体の速度ベクトルの $x$ 成分、$v(t,x,y)$ は $y$ 成分である。また、$p(t,x,y)$ は圧力、$\\lambda = (\\lambda_{1}, \\lambda_{2})$ は未知のパラメータである。ナヴィエ–ストークス方程式の解は発散が $0$ となる条件を満たすため、以下が成立する。\n$$ \\begin{equation} u_{x} + v_{y} = 0 \\tag{17} \\end{equation} $$\nある潜在関数 $\\psi (t, x, y)$ に対して、以下のように仮定する。\n$$ u = \\psi_{y},\\quad v = -\\psi_{x} $$\nつまり、流体の速度ベクトルを $\\begin{bmatrix} \\psi_{y} \u0026amp; -\\psi_{x}\\end{bmatrix}$ とすると、$u_{x} + v_{y} = \\psi_{yx} - \\psi_{xy} = 0$ であるため、自然に $(17)$ を満たす。$u$ と $v$ を個別に求めるのではなく、$\\psi$ を人工ニューラルネットワークで近似し、その偏微分によって $u, v$ を得る。実際の速度ベクトル場に対して、以下のように測定された情報があるとする。\n$$ \\left\\{ t^{i}, x^{i}, y^{i}, u^{i}, v^{i} \\right\\}_{i=1}^{N} $$\nこれに基づいて損失関数を以下のようにする。ここで、$u = \\phi_{y}$, $v = -\\psi_{x}$ であることを思い出す。\n$$ \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) $$\nそして、$(15)$ の右辺を左辺に定理し、それぞれを $f$ と $g$ として定義する。\n$$ \\begin{equation} \\begin{aligned} f \u0026amp;:= u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) + p_{x} - \\lambda_{2}(u_{xx} + u_{yy}) \\\\ g \u0026amp;:= v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) + p_{y} - \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned}\\tag{18} \\end{equation} $$\nすると $f, g$ の値は $\\psi$ によって以下のように表される。（$p$ もニューラルネットワークで近似する）\n$$ \\begin{align*} f \u0026amp;= \\phi_{yt} + \\lambda_{1}(\\psi_{y} \\psi_{yx} - \\psi_{x}\\psi_{yy}) + p_{x} -\\lambda_{2}(\\psi_{yxx} + \\psi_{yyy}) \\\\ g \u0026amp;= -\\phi_{xt} + \\lambda_{1}(-\\psi_{y} \\psi_{xx} + \\psi_{x}\\psi_{xy}) + p_{y} + \\lambda_{2}(\\psi_{xxx} + \\psi_{xyy}) \\\\ \\end{align*} $$\n損失関数に $f(t^{i}, x^{i}, y^{i}) = 0 = g(t^{i}, x^{i}, y^{i})$ という情報を加え、最終的に以下のようにする。\n$$ \\begin{aligned} MSE \u0026amp;:= \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) \\\\ \u0026amp;\\qquad + \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| f(t^{i}, x^{i}, y^{i}) \\right|^{2} + \\left| g(t^{i}, x^{i}, y^{i}) \\right|^{2} \\right) \\end{aligned} \\tag{19} $$\n次に、入力ノードが $3$ つ、出力ノードが $2$ つの人工ニューラルネットワークを定義する。この出力を $\\begin{bmatrix} \\psi (t, x, y) \u0026amp; p(t, x, y) \\end{bmatrix}$ とする。すると、上記の損失関数を計算することができる。\nノイズがある場合とない場合のデータについて実験を行い、どちらの場合も高い精度で $\\lambda_{1}, \\lambda_{2}$ を予測できたことが示されている。また、圧力 $p$ に関するデータが与えられていない場合でも、人工ニューラルネットワークがパラメータと共に $p$ もかなり正確に近似できることが示された。具体的な実験設定、結果、参照解の求め方については、論文に詳しく記載されている。\n5. 結論 この論文では、与えられたデータが満たす物理法則をエンコードする能力があり、偏微分方程式で説明できる新しい種類のニューラルネットワーク構造である物理情報に基づいたニューラルネットワークを紹介した。この結果から、物理モデルに対してディープラーニングが学習できることがわかった。これは多くの物理的シミュレーションに応用可能である。\nしかし、著者は提案された方法が偏微分方程式を解くための既存の方法、例えば有限要素法finite element method、スペクトル方法spectral methodsなどを置き換えるものだと考えるべきではないと述べている。実際にセクション3.2.ではルンゲ・クッタ法をPINNに適用している。\nPINNを実装するために、どれくらいの深さのニューラルネットワークが必要か、どれくらいのデータが必要かなどのハイパーパラメータに関する問題についても、著者は解決策を提案しようとした。しかし、ある方程式で効果的な設定が他の方程式ではそうではないことが観察されたと述べている。\n","id":3313,"permalink":"https://freshrimpsushi.github.io/jp/posts/3313/","tags":null,"title":"論文レビュー: 物理情報基盤ニューラルネットワーク(PINN)"},{"categories":"줄리아","contents":"日本語訳 コード println(ARGS[1] * \u0026#34; + \u0026#34; * ARGS[2] * \u0026#34; = \u0026#34; * string(parse(Float64, ARGS[1]) + parse(Float64, ARGS[2]))) 上記の通り、example.jlというファイルが1行で構成されているとしよう。Juliaでは、ARGSを通じてコマンドラインからの引数を配列で受け取ることができ、Pythonのsys.argvがコマンドライン引数を配列で受け取るのと似ている。書かれたコードは、二つの数字を受け取ってその和を出力するプログラムだ。実行結果は以下の通り。\n環境 OS: Windows julia: v1.6.3 ","id":2280,"permalink":"https://freshrimpsushi.github.io/jp/posts/2280/","tags":null,"title":"ジュリアでコマンドライン引数を挿入する方法"},{"categories":"줄리아","contents":"## 概要\rJuliaでの記号演算は、`SymEngine.jl`[^1]パッケージを通じて使うことができる。\r[^1]: https://symengine.org/SymEngine.jl/\r## コード\r### シンボルの定義\rシンボルは、以下の方法で定義することができる。 julia\u0026gt; using SymEngine\njulia\u0026gt; x = symbols(:x) x\njulia\u0026gt; x, y = symbols(\u0026ldquo;x y\u0026rdquo;) (x, y)\njulia\u0026gt; @vars x, y (x, y)\njulia\u0026gt; x = symbols(:x) x\njulia\u0026gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x)\n### ベクトルと行列 julia\u0026gt; v = [symbols(\u0026ldquo;v_▷eq1◁i$j\u0026rdquo;) for i in 1:2, j in 1:3] 2×3 Matrix{Basic}: a_11 a_12 a_13 a_21 a_22 a_23\njulia\u0026gt; Av 2-element Vector{Basic}: v_1a_11 + v_2a_12 + v_3a_13 v_1a_21 + v_2a_22 + v_3*a_23\njulia\u0026gt; @vars a, b, c, d, x, y (a, b, c, d, x, y)\njulia\u0026gt; [a b; c d] * [a x; b y] 2×2 Matrix{Basic}: a^2 + b^2 ax + by ac + bd cx + dy\n### 微分\r記号微分は、[`Calculus.jl`](../3135)パッケージでも使用することができる。 julia\u0026gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x)\njulia\u0026gt; diff(f, x) 2 + 2*x - sin(x)\n## 環境\r- OS: Windows10\r- バージョン: Julia v1.7.1, SymEngine v0.8.7 ","id":3311,"permalink":"https://freshrimpsushi.github.io/jp/posts/3311/","tags":null,"title":"ジュリアでのシンボリック演算の方法"},{"categories":"줄리아","contents":"コード ジュリアでは、run()関数を使ってバックティックBacktickで囲まれた文字列を実行します。Pythonで言うところの[osモジュールのos.system()`](../2147)を使用したことに似ています。\njulia\u0026gt; txt = \u0026#34;helloworld\u0026#34;\r\u0026#34;helloworld\u0026#34;\rjulia\u0026gt; typeof(`echo $txt`)\rCmd 위와 같이 백틱으로 감싸진 문자열은 Cmd라는 타입을 가지고, run() 함수로써 실행할 수 있다.\njulia\u0026gt; run(`cmd /C echo $txt`)\rhelloworld\rProcess(`cmd /C echo helloworld`, ProcessExited(0)) この例に限って言えば、Windowsではcmd内のechoを実行しなければならず、少し複雑になりますが、Linuxでは単にecho $txtを使用することができます。Windowsでこのようなコマンドを頻繁に使用する場合は、環境変数を修正することを検討してください1。\n環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/running-external-programs/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2278,"permalink":"https://freshrimpsushi.github.io/jp/posts/2278/","tags":null,"title":"Juliaで外部プログラムを実行する方法"},{"categories":"줄리아","contents":"コード parse(type, str)を使えばいいんだ。文字列strをtypeタイプの数字に変更してくれる。\njulia\u0026gt; parse(Int, \u0026#34;21\u0026#34;)\r21\rjulia\u0026gt; parse(Float64, \u0026#34;3.14\u0026#34;)\r3.14 なんでPythonみたいにInt64(\u0026quot;21\u0026quot;)みたいなのができないんだろう\u0026hellip;それは、\u0026quot;21\u0026quot;を21に変えることがタイプを変えることではなく、文字列\u0026quot;21\u0026quot;を読んで数字として解釈することだから、parseを使うのが妥当だって言われてるんだ1。\n環境 OS: Windows julia: v1.6.3 https://discourse.julialang.org/t/why-are-there-all-these-strange-stumbling-blocks-in-julia/92644/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2276,"permalink":"https://freshrimpsushi.github.io/jp/posts/2276/","tags":null,"title":"ジュリアで文字列を数値に変換する方法"},{"categories":"줄리아","contents":"概要 1 可変引数関数とは、プログラミングで一般にVarargs Functionと呼ばれるもので、複数の引数を制限なく受け入れることができる関数のことだ。Juliaでは、変数の後ろに...を付けることで簡単に可変引数を設定できる。例のコードを見て理解しよう。\nちなみに、この...は スプラットオペレータSplat Operatorと呼ばれている。2\nコード アイザック・ニュートンは、単純に階乗の逆数を足すと$e$に収束する次の定理を発見した。 $$ e = {{ 1 } \\over { 0! }} + {{ 1 } \\over { 1! }} + {{ 1 } \\over { 2! }} + \\cdots = \\sum_{k=0}^{\\infty} {{ 1 } \\over { k! }} $$ この例では、オイラー定数$e = 2.71828182 \\cdots$に収束する数列を見ることにする。\nfunction f(x...)\rzeta = 0\rfor x_ in x\rzeta += 1/prod(1:x_)\rend\rreturn zeta\rend 上記のように、xの後ろにドットを付けてx...と書くと、与えられた引数が自動的に配列と捉えられる。関数の内容は上の数式からわかるように、階乗の逆数を順番に取り、足してリターンするだけだ。\njulia\u0026gt; f(0)\r1.0\rjulia\u0026gt; f(0,1)\r2.0\rjulia\u0026gt; f(0,1,2)\r2.5\rjulia\u0026gt; f(0,1,2,3,4,5,6,7,8,9,10)\r2.7182818011463845 実行結果は、自然数を長く与えるほど、オイラー定数に近づくことが確認できる。ここで注目すべき点は、可変的に入った引数が自動的にxという配列にまとめられて使用された点だ。例えば、次のように概念的に配列を入れるとエラーになる可能性がある。\njulia\u0026gt; f(0:10)\rERROR: MethodError: no method matching (::Colon)(::Int64, ::UnitRange{Int64}) 環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/functions/#Varargs-Functions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2266,"permalink":"https://freshrimpsushi.github.io/jp/posts/2266/","tags":null,"title":"ジュリアで可変引数関数を定義する方法"},{"categories":"줄리아","contents":"概要 eltype() 関数を使うだけだ。多分 element typeからきた名前だろう。\nコード julia\u0026gt; set_primes = Set([2,3,5,7,11,13])\rSet{Int64} with 6 elements:\r5\r13\r7\r2\r11\r3\rjulia\u0026gt; arr_primes = Array([2,3,5,7,11,13])\r6-element Vector{Int64}:\r2\r3\r5\r7\r11\r13 次のように$13$までの素数を要素とする二つのコンテナを考えてみよう。正直、同じデータを含んでいるが、上はセットで、下は配列という違いがあるだけだ。\njulia\u0026gt; typeof(set_primes)\rSet{Int64}\rjulia\u0026gt; eltype(set_primes)\rInt64\rjulia\u0026gt; typeof(arr_primes)\rVector{Int64} (alias for Array{Int64, 1})\rjulia\u0026gt; eltype(arr_primes)\rInt64 これらにtypeof()を適用すれば、セットか配列かの区別がつくが、eltype()はコンテナが何であれ、内部の要素がどのようなタイプかを返す。\njulia\u0026gt; typeof(1:10)\rUnitRange{Int64}\rjulia\u0026gt; eltype(1:10)\rInt64\rjulia\u0026gt; typeof(1:2:10)\rStepRange{Int64, Int64}\rjulia\u0026gt; eltype(1:2:10)\rInt64 上に示された1:10と1:2:10の違いは、タイプに過度にこだわるジュリアプログラミングの世界で、eltype()がどのように役立つかを示している。\n環境 OS: Windows julia: v1.6.3 ","id":2264,"permalink":"https://freshrimpsushi.github.io/jp/posts/2264/","tags":null,"title":"ジュリアのコンテナ内部の要素タイプをチェックする方法"},{"categories":"줄리아","contents":"コード default() 関数を使用すればいい。\nusing Plots\rdefault(size = (400,400), color = :red)\rdefault(:size, (400,400))\rfor key in [:size, :color], value in [(400,400), :red]\rdefault(key, value)\rend 普通の plot() 関数のように設定する方法と、キーとバリューを与えて一つずつ変更する方法がある。普通は前者の方が便利だが、設定が非常に複雑な場合は、ループを利用して下の方法を使用することもできる。\n初期化 全てのデフォルト設定を初期化したい場合は、default()を使用すればいい。\n環境 OS: Windows julia: v1.6.3 ","id":2262,"permalink":"https://freshrimpsushi.github.io/jp/posts/2262/","tags":null,"title":"ジュリアプロットの基本設定を変更する方法"},{"categories":"줄리아","contents":"概要 インデックスを取るときは、Not() 関数を使用すればいいんだ1。カラム名そのままのシンボル、またはシンボルの配列を入れると、それらのカラムのみが除外されてインデックス化されるんだ。\nコード using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r) 上の例のコードを実行して、その結果を確認しよう。\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 WJSN データフレームは、上のようになっているんだ。\njulia\u0026gt; WJSN[:,Not(:height)]\r10×3 DataFrame\rRow │ member birth unit │ String Int64 String ─────┼───────────────────────\r1 │ 다영 99 쪼꼬미\r2 │ 다원 97 메보즈\r3 │ 루다 97 쪼꼬미\r4 │ 소정 95 더블랙\r5 │ 수빈 96 쪼꼬미\r6 │ 연정 99 메보즈\r7 │ 주연 98 더블랙\r8 │ 지연 95 더블랙\r9 │ 진숙 99 쪼꼬미\r10 │ 현정 94 더블랙 :height だけが入って、:height 列が削除されたよ。\njulia\u0026gt; WJSN[:,Not([:birth, :unit])]\r10×2 DataFrame\rRow │ member height │ String Int64 ─────┼────────────────\r1 │ 다영 161\r2 │ 다원 167\r3 │ 루다 157\r4 │ 소정 166\r5 │ 수빈 159\r6 │ 연정 165\r7 │ 주연 172\r8 │ 지연 163\r9 │ 진숙 162\r10 │ 현정 165 シンボルの配列 [:birth, :unit] が入って、:birth 列と :unit 列が削除されたんだ。\n環境 OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Taking-a-Subset\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2260,"permalink":"https://freshrimpsushi.github.io/jp/posts/2260/","tags":null,"title":"ジュリアでデータフレームの特定の行を削除する方法"},{"categories":"줄리아","contents":"概要 縦線と横線を引くには、vline!() と hline!() 関数を使用すればいい。\nコード @time using Plots\rplot(rand(100))\rhline!([0.5], linewidth = 2)\rvline!([25, 75], linewidth = 2)\rpng(\u0026#34;result\u0026#34;) 線が引かれる位置は配列で渡す。配列の要素が複数あれば、一度に複数の線を引くことができる。\n環境 OS: Windows julia: v1.6.3 ","id":2258,"permalink":"https://freshrimpsushi.github.io/jp/posts/2258/","tags":null,"title":"ジュリアで図に垂直線と水平線を挿入する方法"},{"categories":"줄리아","contents":"概要 RecipesBase.jlは、ユーザーが新しい図のスタイルを自分で作れるパッケージだ。Rプログラミング言語でのggplotがそうであるように、元のジュリアとはまた別の独自の文法1がある。例を通して覚えよう。\nコード using Plots\rusing DataFrames\rdf = DataFrame(x = 1:10, y = rand(10))\rplot(df)\r@userplot TimeEvolution\r@recipe function f(te::TimeEvolution)\rdf = te.args[1]\rlinealpha --\u0026gt; 0.5\rcolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend\rend\rtimeevolution(df); png(\u0026#34;1\u0026#34;)\rtimeevolution(df, legend = :left); png(\u0026#34;2\u0026#34;) 最初に、上記のコードを実行すると、次のようなエラーが発生する。\njulia\u0026gt; df = DataFrame(x = 1:10, y = rand(10))\r10×2 DataFrame\rRow │ x y\r│ Int64 Float64 ─────┼──────────────────\r1 │ 1 0.636532\r2 │ 2 0.463764\r3 │ 3 0.604559\r4 │ 4 0.654089\r5 │ 5 0.883409\r6 │ 6 0.91667\r7 │ 7 0.0609783\r8 │ 8 0.602259\r9 │ 9 0.460372\r10 │ 10 0.479944\rjulia\u0026gt; plot(df)\rERROR: Cannot convert DataFrame to series data for plotting これは、基本的にplot()がデータフレームを受け取って図を描く方法が定義されていないためだ。\n@userplotと@recipe @userplot TimeEvolution\r@recipe function f(te::TimeEvolution)\r...\rend 例では、時系列データをそのまま折れ線チャートで描いてみる。\n@userplot：plot()関数の性質を継承する関数の名前を指定する。ここでは大文字と小文字の区別があるが、完成する関数は小文字のみ使うことができることに注意しろ。 @recipe：具体的に図のスタイルを指定する。これの後にくる関数の名前は、通常慣習的にfを使用する。 f(te::TimeEvolution) df = te.args[1]\rlinealpha --\u0026gt; 0.5\rcolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend では、上記のコードを一行一行理解していこう。\ndf = te.args[1] 受け取った引数の中で最初のものをdfとみなす。この例では、与える引数がデータフレームであると仮定されているため、その略称であるdfを使った。このように、fは私たちが使用する関数でもなく、その引数teも直接使用されないことに注意せよ。\nlinealpha --\u0026gt; 0.5 この図で描かれる線の透明度を0.5に設定する。オプションを与えるようにlinealpha = 0.5ではなく、--\u0026gt;で値を与えることに注意せよ。これは、通常のジュリアの文法と完全に異なる。\ncolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r...\rend plot()が2次元配列を直接図に描くとしても、ラベルはy1、y2などと自動的に与えられる。これを防ぐために、上記のようにデータフレームの列名を取得して別々にラベルを与える。enumerate()関数を使って、そのインデックスと列名を同時に巡回する。詳しくはenumerate()に関する説明を参照せよ。\n@series for ...\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend @seriesマクロを通じて、繰り返し描かれるデータとそのスタイルを指定する。label --\u0026gt; column_nameを通じて列名をラベルとして与え、一番下の行にdf[:,column_index]と書くことで、その列名のデータが描かれることになる。この時、描かれるデータは一番下の行になければならないことに注意しろ。\n結果 この結果によって、timeevolution()またはtimeevolution!()で、私たちが指定したスタイルで図を描くことができるようになった。\ntimeevolution(df) データフレームを入れてもエラーなく図がうまく描かれていることを確認できる。折れ線の透明度は0.5で、ラベルもデータフレームにあった列名をそのまま引き継いでいる。\ntimeevolution(df, legend = :left) 任意に凡例の位置を調整してみた。fを定義する時にlegendについて何の言及もなかったにも関わらず、うまく適用されていることを確認できる。これは、timeevolution()がplot()の残りの要素を継承しているためだ。\n環境 OS: Windows julia: v1.6.3 https://docs.juliahub.com/RecipesBase/8e2Mm/1.0.1/syntax/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2256,"permalink":"https://freshrimpsushi.github.io/jp/posts/2256/","tags":null,"title":"ジュリアでアートスタイルを作る方法"},{"categories":"줄리아","contents":"概要 groupby()を使ってグループ別に分け、combine()を使って計算すればいいんだ1。\ngroupby(df, :colname)\n:colnameを基準にしてGroupedDataFrameを返す。 combine(gdf, :colname =\u0026gt; fun)\ngdfはグループ別に分かれたGroupedDataFrameだ。 :colname =\u0026gt; funは、計算したい値が入った列の名前のシンボル:colnameと、計算する関数funのペアだ。 コード using DataFrames\rusing StatsBase\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r)\rsort!(WJSN, :birth)\runique(WJSN, :unit)\runits = groupby(WJSN, :unit)\runits[1]\runits[2]\runits[3]\rcombine(units, :height =\u0026gt; mean) 上の例のコードを実行して、その結果を確認してみよう。\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 WJSNのデータフレームは上のようだ。\nグループ別に分ける groupby() julia\u0026gt; units = groupby(WJSN, :unit)\rGroupedDataFrame with 3 groups based on key: unit\rFirst Group (4 rows): unit = \u0026#34;더블랙\u0026#34;\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 주연 98 172 더블랙\r⋮\rLast Group (2 rows): unit = \u0026#34;메보즈\u0026#34;\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다원 97 167 메보즈\r2 │ 연정 99 165 메보즈 :unit列を基準にデータフレームが三つのグループに分かれた。\njulia\u0026gt; units[1]\r4×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 주연 98 172 더블랙\rjulia\u0026gt; units[2]\r4×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 수빈 96 159 쪼꼬미\r2 │ 루다 97 157 쪼꼬미\r3 │ 다영 99 161 쪼꼬미\r4 │ 진숙 99 162 쪼꼬미\rjulia\u0026gt; units[3]\r2×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다원 97 167 메보즈\r2 │ 연정 99 165 메보즈 上のようにGroupedDataFrameにインデクシングをすることで、分かれたデータフレームにアクセスできる。\nグループ別に計算する combine() julia\u0026gt; combine(units, :height =\u0026gt; mean)\r3×2 DataFrame\rRow │ unit height_mean │ String Float64 ─────┼─────────────────────\r1 │ 더블랙 166.5\r2 │ 쪼꼬미 159.75\r3 │ 메보즈 166.0 上のコードは、:unitを基準にグループ化されたデータフレームunitsの中で、WJSNのデータフレームの:heightの平均meanを計算したものだ。概要で言及された通り、このStatBase.mean()は平均を計算する関数だ。これをsum()に変えれば合計、min()に変えればグループ別の最小値を計算する。この例では、:unit別に:heightの平均を計算し、쪼꼬미グループが159.75で一番低いことがわかる。\nhttps://stackoverflow.com/questions/64226866/groupby-with-sum-on-julia-dataframe\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2254,"permalink":"https://freshrimpsushi.github.io/jp/posts/2254/","tags":null,"title":"ジュリアでデータフレームをグループ分けして計算する方法"},{"categories":"줄리아","contents":"概要 これを実現するためには、unique()を使えばいい。正確には、重複した行を削除するというよりも、一つだけ残すことだ。\nコード using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r)\rsort!(WJSN, :birth)\runique(WJSN, :unit) 上の例のコードを実行して、その結果を確認してみよう。\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 WJSNデータフレームは上のようになっている。\n単一の列で重複した行を削除する unique() julia\u0026gt; unique(WJSN, :unit)\r3×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 수빈 96 159 쪼꼬미\r3 │ 다원 97 167 메보즈 :unitシンボルごとに、一つの行だけが残っているのを確認できる。\n環境 OS: Windows julia: v1.6.3 ","id":2252,"permalink":"https://freshrimpsushi.github.io/jp/posts/2252/","tags":null,"title":"JuliaでDataFrameの重複した行を削除する方法"},{"categories":"줄리아","contents":"概要 Juliaでは、サブプロットに関連するオプションはlayoutオプションを通して制御できる。\n整数を入力すると、その数だけのグリッドをうまく作ってくれる。 整数の2-タプルを入力すると、指定どおりのグリッドを作ってくれる。 @layoutマクロを使ってPlots.GridLayoutタイプの複雑なレイアウトを構成する。 コード using Plots\rleft = plot(randn(100), color = :red)\rright = plot(randn(100), color = :blue)\rplot(left, right)\rpng(\u0026#34;easyone\u0026#34;)\rdata = rand(10, 6)\rplot(data, layout = 6)\rpng(\u0026#34;easytwo\u0026#34;)\rplot(data, layout = (3,2))\rpng(\u0026#34;easygrid\u0026#34;)\rl = @layout [p1 ; p2 p2]\rp = plot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l)\rpng(\u0026#34;hardgrid\u0026#34;) 簡単な列挙 left = plot(randn(100), color = :red)\rright = plot(randn(100), color = :blue)\rplot(left, right) ただ、プロットを複数同時にまとめてプロットし直しても、とりあえずサブプロットにはなる。\n簡単なレイアウトlayout plot(data, layout = 6) plot(data, layout = (3,2)) 簡単に整数でやるのもいいし、具体的にタプルを渡して望んだ通りのグリッドを作ることができる。もちろん、整数を渡さなくても同じように動くから、タプルを渡す場合だけ覚えておけばいい。\n複雑なレイアウト@layout l = @layout [p1 ; p2 p2] 単純なグリッドではなく、1行目に1つの絵、2行目に2列の絵が入るように指示するPlots.GridLayoutタイプのlを定義した。これをlayoutの入力として与えると、次のようにずっと複雑なレイアウトが表現される。\np = plot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l) 空白の作成_ 上記の例で、1行目の絵を下の行と同じサイズで、中央に配置したい場合は、両側に空間を作ればいい。_で空白を示すことができる。{0.5w}で幅を全体の半分に合わせれば、\nl = @layout([_ p{0.5w} _; p p])\rplot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l) 環境 OS: Windows julia: v1.6.3 ","id":2250,"permalink":"https://freshrimpsushi.github.io/jp/posts/2250/","tags":null,"title":"ジュリアでレイアウトを使ってサブプロットを描く方法"},{"categories":"줄리아","contents":"概要 1 plot() 関数の legend オプションで、凡例の位置を自由に調整できる。$0$ から $1$ までの値で構成された2-タプルを与えると、正確にその位置に表示されるし、それ以外はシンボルで制御できる。\nシンボルの場合は、top/bottom と left/right を順に結びつけて組み合わせる。最初に outer を付けると、図の外側に凡例が表示される。組み合わせで作られるシンボルの例は以下の通り:\n:bottom :left :bottomleft :outertopright したがって、:leftbottom や :toprightouter のようなシンボルは許されない。\nコード data = randn(100, 2)\rplot(data)\rplot(data, legend = (0.5, 0.7)); png(\u0026#34;tuple\u0026#34;)\rSymbols = [:none, :bottom, :left, :bottomleft, :outertopright, :inline]\rfor symbol ∈ Symbols\rplot(data, legend = symbol)\rpng(string(symbol))\rend 正確な位置の指定 legend = (0.5, 0.7) タプル (0.5, 0.7) は、横軸の50%、縦の高さの70%ぐらいに凡例が表示される。\n凡例の除去 :none 上下左右 :bottom, :left 組み合わせ :bottomleft 外側 :outertopright 図の外側に凡例を出力した。このため、図が歪むことに注意が必要だ。\n線の終わり :inline 多くの線があり、色で区別するのが難しい、または最後の値が特に重要な場合に便利だ。\n環境 OS: Windows julia: v1.6.3 https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2248,"permalink":"https://freshrimpsushi.github.io/jp/posts/2248/","tags":null,"title":"ジュリアで図の凡例の位置を調整する方法"},{"categories":"줄리아","contents":"概要 1 グラフの幅と高さを調整するには、オプションにratioを入れるといい。他の推奨される別名には、aspect_ratios, axis_ratioがある。\nratio = :none: デフォルト値で、グラフのサイズに合わせて比率が調整される。 ratio = :equal: グラフのサイズにかかわらず、横軸と縦軸が1対1の比率に調整される。 ratio = Number: Numberに従って比率が調整される。Numberは${{세로} \\over {가로}} = {{\\Delta y} \\over {\\Delta x}}$の比率として与えられる。 コード using Plots\rx = rand(100)\ry = randn(100)\rplot(x,y,seriestype = :scatter, ratio = :none)\rpng(\u0026#34;none\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = :equal)\rpng(\u0026#34;equal\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 0.5)\rpng(\u0026#34;0.5\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 1)\rpng(\u0026#34;1\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 2)\rpng(\u0026#34;2\u0026#34;) デフォルト :none plot(x,y,seriestype = :scatter, ratio = :none) 1対1 :equal plot(x,y,seriestype = :scatter, ratio = :equal) 特定の比率 Number Numberは${{세로} \\over {가로}}$の比率として与えられる。\nplot(x,y,seriestype = :scatter, ratio = 0.5) plot(x,y,seriestype = :scatter, ratio = 1) plot(x,y,seriestype = :scatter, ratio = 2) 環境 OS: Windows julia: v1.6.3 https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2246,"permalink":"https://freshrimpsushi.github.io/jp/posts/2246/","tags":null,"title":"ジュリア集合の絵のアスペクト比を調整する方法"},{"categories":"줄리아","contents":"エラー using DataFrames, CSV\rexample = DataFrame(x = 1:10, 가 = \u0026#34;나다\u0026#34;)\rCSV.write(\u0026#34;example.csv\u0026#34;, example) JuliaでCSVファイルに出力するとき、上のように韓国語が文字化けする現象が見られる。\n原因 実際には韓国語が文字化けするわけではなく、Unicodeエンコーディングの問題で、特にUTF-8エンコーディングのBOMバイトオーダーマークが原因で起こる。PythonなどでエンコーディングをUTF-8-sigとすることで解決できる。\n解決方法 1 CSV.write(\u0026#34;example.csv\u0026#34;, example, bom = true) CSV.jlでは、単にbom = trueというオプションを指定すると、以下のように文字化けせずに出力される。\n環境 OS: Windows julia: v1.6.3 https://csv.juliadata.org/stable/writing.html#CSV.write\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2244,"permalink":"https://freshrimpsushi.github.io/jp/posts/2244/","tags":null,"title":"ジュリアでCSV出力時の文字化け解決方法"},{"categories":"줄리아","contents":"概要 ジュリアでテキスト出力を飾るパッケージとしてCrayons.jlが知られている1。\n組み込み関数だけで飾りたい場合は、printstyled()を使えばいい。\nコード using Crayons\rprint(Crayon(background = :red), \u0026#34;빨강\u0026#34;)\rprint(Crayon(foreground = :blue), \u0026#34;파랑\u0026#34;)\rprint(Crayon(bold = true), \u0026#34;볼드\u0026#34;)\rprint(Crayon(italics = true), \u0026#34;이탤릭\u0026#34;)\rprint(Crayon(bold = true, italics = true), \u0026#34;볼드 이탤릭\u0026#34;) 上記のコンソールを実行すると、次のように飾られた結果が得られる。\nCrayon(...)\nforeground: テキスト自体の色を変更する。シンボルや整数のトリプル(r,g,b)、0から255の間の整数で値を与えることができる。 background: テキストの背景色を変更する。引数を指定する方法はforegroundと同じだ。 ブール値で指定するオプションは以下のものがある。上記の例では、それぞれ個別に実行した結果と同時に実行した結果が示されている。\nbold: 太字。 italics: 斜体。 underline: 下線。 環境 OS: Windows julia: v1.6.3 https://github.com/KristofferC/Crayons.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2242,"permalink":"https://freshrimpsushi.github.io/jp/posts/2242/","tags":null,"title":"- ジュリアでのテキスト出力装飾パッケージ"},{"categories":"줄리아","contents":"コード 宇宙少女のデータフレームが以下のように与えられたとしよう。\nWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\r) julia\u0026gt; WJSN\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 다원 97 167\r3 │ 루다 97 157\r4 │ 소정 95 166\r5 │ 수빈 96 159\r6 │ 연정 99 165\r7 │ 주연 98 172\r8 │ 지연 95 163\r9 │ 진숙 99 162\r10 │ 현정 94 165 データフレームに新しい列を追加するコードは以下の通りである。\ndataframe[!, :\u0026quot;column_name\u0026quot;] = values ユニットに関する列を追加してみると、\nWJSN[!, :\u0026#34;unit\u0026#34;] = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\rjulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 더블랙\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 더블랙\r8 │ 지연 95 163 더블랙\r9 │ 진숙 99 162 쪼꼬미\r10 │ 현정 94 165 더블랙 環境 OS: Windows10 Version: Julia 1.7.1, DataFrames 1.3.2 ","id":3273,"permalink":"https://freshrimpsushi.github.io/jp/posts/3273/","tags":null,"title":"ジュリアでデータフレームに新しい列を追加する方法"},{"categories":"줄리아","contents":"コード using Plots\rscatter(rand(100), randn(100))\rplot!([0,1],[0,1])\rpng(\u0026#34;example1\u0026#34;)\rplot!([.00,.25,.50],[-2,0,-2])\rpng(\u0026#34;example2\u0026#34;)\rθ = 0:0.01:2π\rplot!(.5 .+ cos.(θ)/3, 1.5sin.(θ))\rpng(\u0026#34;example3\u0026#34;) このコードを実行して、図に線分を入れる方法を見てみよう。\n線分 plot!([0,1],[0,1]) 単に一つの線分を引くか何か別のものを描くか、方法は同じだ。線分には二つの点が必要だから、x座標の配列とy座標の配列を与えればいい。\n複数の線分 plot!([.00,.25,.50],[-2,0,-2]) y3は一度に二つの線分を描いたものだ。線分間の始点と終点が繋がっている。\n楕円 plot!(.5 .+ cos.(θ)/3, 1.5sin.(θ)) 複数の線分を引く方法を応用して、楕円を描くことができる。文法的にも計算的にも、他の言語に比べて描きやすい方だ。\n環境 OS: Windows julia: v1.6.3 ","id":2240,"permalink":"https://freshrimpsushi.github.io/jp/posts/2240/","tags":null,"title":"ジュリア集合の画像に線を挿入する方法"},{"categories":"기하학","contents":"ビルドアップ1 ベクトル場の簡単な定義を考えてみよう。3次元空間でのベクトル場ベクトル関数、ベクトル場とは、3次元ベクトルを3次元ベクトルにマッピングする関数 $X : \\mathbb{R}^{3} \\to \\mathbb{R}^{3}$である。これを多様体と考えると、$X$は微分多様体 $\\mathbb{R}^{3}$の点 $p$を$\\mathbb{R}^{3}$のベクトル $\\mathbf{v}$にマッピングするが、このベクトル $\\mathbf{v}$をオペレーターとして扱い、方向微分(=接ベクトル)と考えることができる。したがって、ベクトル場とは、多様体 $\\mathbb{R}^{3}$の点 $p$を$p$の接ベクトル $\\mathbf{v}_{p} \\in T_{p}\\mathbb{R}^{3}$にマッピングする関数である。\nすると、ベクトル場の値域は全ての点の接ベクトルの集合である。したがって、ベクトル場 $X$は以下のように定義される関数である。\n$$ X : \\mathbb{R}^{3} \\to \\bigcup \\limits_{p\\in \\mathbb{R}^{3}} T_{p}\\mathbb{R}^{3} $$\nこの概念を多様体に一般化するために、微分多様体 $M$の接束tangent bundle $TM$を次のように定義しよう。\n$$ TM := \\bigsqcup \\limits_{p\\in M} T_{p}M $$\nこのとき $\\bigsqcup$は直和である。\n定義 微分多様体 $M$上のベクトル場vector field $X$とは、各点 $p \\in M$を$p$の接ベクトル $X_{p} \\in T_{p}M$にマッピングする関数である。\n$$ \\begin{align*} X : M \u0026amp;\\to TM \\\\ p \u0026amp;\\mapsto X_{p} \\end{align*} $$\n説明 ベクトル場の関数値 接束の定義を考えると、$TM$の要素は$(p, X_{p})$であるが、定義で$X_{p}$をマッピングすると書かれているので、疑問に思うことがあるかもしれない。\n$$ \\begin{equation} TM := \\bigsqcup \\limits_{p \\in M } T_{p}M = \\bigcup_{p \\in M} \\left\\{ p \\right\\} \\times T_{p}M = \\left\\{ (p, X_{p}) : p \\in M, X_{p} \\in T_{p}M \\right\\} \\end{equation} $$\nつまり、正確に言うと、直和の定義によると$TM$の要素は順序対 $(p, X_{p})$が正しいが、実際には$X_{p}$と同じものとして扱われる。\n接束の定義を再考する。接束の定義で本当にしたいのは、順序対 $(p, X_{p})$を集めることではない。各点 $p$上の接ベクトルをすべて集めたいのである。しかし、各$T_{p}M$は$\\mathbb{R}^{n}$と同型であるため、和集合をとるときに曖昧さが生じる可能性がある。\n$$ T_{p}M \\approxeq \\mathbb{R}^{n} \\approxeq T_{q}M $$\n例えば、$M$が3次元多様体であるとすると、$T_{p}M \\approxeq \\mathbb{R}^{3}$で表されるベクトル $X_{p}$と$T_{q}M \\approxeq \\mathbb{R}^{3}$で表されるベクトル $X_{q}$を同じものとして扱う曖昧さがある。したがって、$TM$を順序対の集合として定義する理由は、$X_{p}$と$X_{q}$が同じ対象ではなく、明確に異なるものとして区別するためである。ここで自然に$\\iota_{p} : (p, X_{p}) \\mapsto X_{p}$のような全単射関数を考え、$(p, X_{p}) \\approx X_{p}$として扱うことができる。\nある教科書では、このような説明を特にしたくない場合や、読者が十分に理解していると仮定する場合に、接束 $TM$を次のように定義することもある。\n$$ TM := \\bigcup\\limits_{p\\in M} T_{p}M = \\left\\{ X_{p} \\in T_{p}M : \\forall p \\in M \\right\\} $$\nもちろん、再度言うが、上の定義も$(1)$も本質的には同じである。また、上の定義によると$X$の関数値は関数 $X_{p}$であることに注意しよう。\n$$ X_{p} : \\mathcal{D} \\to \\mathbb{R} $$\nオペレーターとしてのベクトル場 $M$を$n$次元微分多様体としよう。$M$の微分可能な関数の集合を$\\mathcal{D} = \\mathcal{D}(M)$としよう。\n$$ \\mathcal{D} = \\mathcal{D}(M) := \\left\\{ \\text{all real-valued functions of class } C^{\\infty} \\text{ defined on } M \\right\\} $$\n参照 微分多様体上の微分可能な関数の集合 $\\mathcal{D}(M)$ 微分多様体上の微分可能なベクトル場の集合 $\\frak{X}(M)$ Manfredo P. Do Carmo, Riemannian Geometry (英語版, 1992), p25-27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3270,"permalink":"https://freshrimpsushi.github.io/jp/posts/3270/","tags":null,"title":"微分可能多様体上のベクトル場"},{"categories":"줄리아","contents":"コード using DataFrames\rUnit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\rUnit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\rWJSN = vcat(Unit1, Unit2)\rpush!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\rpush!(WJSN, [\u0026#34;연정\u0026#34;,99,165])\rsort(WJSN, 3)\rsort(WJSN, :birth)\rsort(WJSN, [:birth, :height])\rsort(WJSN, :birth, rev = true) 上の例のコードを実行して、その結果を確認してみましょう。\njulia\u0026gt; WJSN\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\r10 │ 연정 99 165 WJSNのデータフレームは上記の通りです。\n列番号でソート sort(df, cols::integer) cols番目の列を基準にソートします。\njulia\u0026gt; sort(WJSN, 3)\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 루다 97 157\r2 │ 수빈 96 159\r3 │ 다영 99 161\r4 │ 진숙 99 162\r5 │ 지연 95 163\r6 │ 현정 94 165\r7 │ 연정 99 165\r8 │ 소정 95 166\r9 │ 다원 97 167\r10 │ 주연 98 172 3番目の列であるheightを基準にソートされたことが確認できます。\n列名でソート sort(df, cols::Symbol) 名前のシンボルcolsの列を基準にソートします。\njulia\u0026gt; sort(WJSN, :birth)\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 현정 94 165\r2 │ 소정 95 166\r3 │ 지연 95 163\r4 │ 수빈 96 159\r5 │ 루다 97 157\r6 │ 다원 97 167\r7 │ 주연 98 172\r8 │ 다영 99 161\r9 │ 진숙 99 162\r10 │ 연정 99 165 :birthが入ってbirthを基準にソートされたことが確認できます。\nソートの優先順位 sort(df, cols::Array) colsの順番に従って優先順位をつけてソートします。\njulia\u0026gt; sort(WJSN, [:birth, :height])\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 현정 94 165\r2 │ 지연 95 163\r3 │ 소정 95 166\r4 │ 수빈 96 159\r5 │ 루다 97 157\r6 │ 다원 97 167\r7 │ 주연 98 172\r8 │ 다영 99 161\r9 │ 진숙 99 162\r10 │ 연정 99 165 birthでソートしつつ、heightもソートされました。ただbirthを基準にソートしただけと比較すると、2行目と3行目が逆転しています。\n逆順でソート sort(df, rev::Bool=false) rev = trueを指定すればOKです。デフォルトはfalseです。\njulia\u0026gt; sort(WJSN, :birth, rev = true)\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 진숙 99 162\r3 │ 연정 99 165\r4 │ 주연 98 172\r5 │ 루다 97 157\r6 │ 다원 97 167\r7 │ 수빈 96 159\r8 │ 소정 95 166\r9 │ 지연 95 163\r10 │ 현정 94 165 環境 OS: Windows julia: v1.6.3 ","id":2238,"permalink":"https://freshrimpsushi.github.io/jp/posts/2238/","tags":null,"title":"Juliaでデータフレームを並べ替える方法"},{"categories":"줄리아","contents":"コード using DataFrames\rUnit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\rUnit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\rWJSN = vcat(Unit1, Unit2)\rpush!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\rpush!(WJSN, [\u0026#34;연정\u0026#34;,99,165]) 上の例のコードを実行して、その結果を確認しよう。\n二つのデータフレームの行を結合する vcat() julia\u0026gt; Unit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\r4×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161 2 │ 루다 97 157 3 │ 수빈 96 159 4 │ 진숙 99 162 julia\u0026gt; Unit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\r4×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 소정 95 166\r2 │ 주연 98 172\r3 │ 지연 95 163\r4 │ 현정 94 165\rjulia\u0026gt; WJSN = vcat(Unit1, Unit2)\r8×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165 当然だけど、二つのデータフレームの列は同じでなければならない。\n一行を挿入する push!() julia\u0026gt; push!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\r9×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\rjulia\u0026gt; push!(WJSN, [\u0026#34;연정\u0026#34;,99,165])\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\r10 │ 연정 99 165 push!()でデータを追加する時は、列の数が一致する配列を入れてやる必要がある。\n環境 OS: Windows julia: v1.6.2 ","id":2236,"permalink":"https://freshrimpsushi.github.io/jp/posts/2236/","tags":null,"title":"ジュリアでデータフレームに新しい行を挿入する方法"},{"categories":"줄리아","contents":"概要 Infinities.jlは、Juliaで無限大記号を使用できるように支援するパッケージだ1。科学計算のコーディングにおいて、無限大は意外と便利である。\nコード julia\u0026gt; 8 \u0026lt; Inf\rtrue 序論で無限大を使用するのではなく、無限大記号を使用できるように支援すると述べた理由は、実際にパッケージなしでそれを使用できるからである。\njulia\u0026gt; using Infinities\rjulia\u0026gt; 8 \u0026lt; ∞\rtrue\rjulia\u0026gt; -∞ \u0026lt; 8\rtrue\rjulia\u0026gt; max(∞, 10, 11)\r∞\rjulia\u0026gt; sin(∞)\rERROR: MethodError: no method matching AbstractFloat(::Infinities.Infinity) 見ての通り、無限大の一般的な性質を全て備えている。\njulia\u0026gt; ℵ₀ \u0026lt; ℵ₁\rtrue\rjulia\u0026gt; ℵ₀ \u0026gt; ℵ₁\rfalse\rjulia\u0026gt; ∞ == ℵ₁\rfalse\rjulia\u0026gt; ∞ == ℵ₀\rtrue\rjulia\u0026gt; ∞ === ℵ₀\rfalse さらに、無限集合の基数である$\\aleph_{0}$と$\\aleph_{1}$を使用できるが、これにより計算上では同じ無限大でありながら、並び替えや比較などに使用できるように順序を付けることができる。\n環境 OS: Windows julia: v1.6.2 https://github.com/JuliaMath/Infinities.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2234,"permalink":"https://freshrimpsushi.github.io/jp/posts/2234/","tags":null,"title":"ジュリアで無限大を使う方法"},{"categories":"줄리아","contents":"ガイド 1 (@v1.6) pkg\u0026gt; status JuMP\rStatus `C:\\Users\\rmsms\\.julia\\environments\\v1.6\\Project.toml`\r[4076af6c] JuMP v0.20.0 REPLで]キーを押すとパッケージモードに入る。例えば、v0.20.0のバージョンのパッケージをv0.21にバージョンアップしたい場合は、以下のようにパッケージの後に@x.yyを付けてインストールすればいい。\n(@v1.6) pkg\u0026gt; add JuMP@0.21\rResolving package versions...\r...\r(@v1.6) pkg\u0026gt; status JuMP\rStatus `C:\\Users\\rmsms\\.julia\\environments\\v1.6\\Project.toml`\r[4076af6c] JuMP v0.21.4 もう一度バージョンを確認すると、パッケージのバージョンが正常に変更されたことを確認できる。\n環境 OS: Windows julia: v1.6.2 https://pkgdocs.julialang.org/dev/api/#General-API-Reference\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2232,"permalink":"https://freshrimpsushi.github.io/jp/posts/2232/","tags":null,"title":"ジュリアで特定バージョンのパッケージをインストールする方法"},{"categories":"기하학","contents":"概要 微分多様体上のプルバックを定義する。微分多様体が難しい場合は、$M = \\mathbb{R}^{m}$、$N = \\mathbb{R}^{n}$と考えてもよい。\n定義1 二つの微分多様体 $M, N$と微分可能な関数 $f : M \\to N$が与えられたとする。そこで、$N$の$k$-形式を$M$の$k$-形式に送る関数$f^{\\ast}$を考えることができる。$\\omega$を多様体$N$の$k$-形式とするとき、多様体$M$の$k$-形式$f^{\\ast}\\omega$を$\\omega$のプルバックpull back, 引き戻しと呼び、以下のように定義する。\n$$ \\begin{equation} (f^{\\ast}\\omega)(p) (v_{1}, \\dots, v_{k}) := \\omega (f(p))\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M \\end{equation} $$\n説明 プルバックという名前には、($f$が$M$から$N$へのマッピングであるのに対して)$f^{\\ast}$は$N$から$M$へのマッピングであるという意味がある。定義と表記法がかなり難しいが、少しずつ理解していこう。\n$f^{\\ast}$ $f^{\\ast}$は$N$の$k$-形式を$M$の$k$-形式に送るマップである。したがって、$\\omega$を$N$の$k$-形式とすると、$f^{\\ast}\\omega = f^{\\ast}(\\omega)$は$M$の$k$-形式である。\n$f^{\\ast}\\omega (p)$ 多様体$M$上の$k$-形式は、$p \\in M$を$\\Lambda^{k}(T_{p}^{\\ast}M)$の元にマッピングする。\n$$ f^{\\ast}\\omega : M \\to \\Lambda^{k}(T_{p}^{\\ast}M) $$\n$$ \\Lambda^{k} (T_{p}^{\\ast}M) := \\left\\{ \\varphi : \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\nつまり、$f^{\\ast}\\omega (p) \\in \\Lambda^{k} (T_{p}^{\\ast}M)$もまた、一つの関数である。$\\Lambda^{k} (T_{p}^{\\ast}M)$の定義により、$f^{\\ast}\\omega (p)$は「$p$上の接ベクトル」$k$個を変数とする。これで、$(1)$はこの関数の関数値を具体的に定義した式であることがわかる。$f^{\\ast}(p)$自体が一つの関数であることをより強調するため、以下のような表記を使うことにしよう。\n$$ (f^{\\ast}\\omega)_{p} = f^{\\ast}\\omega (p) $$\n$\\omega (f(p))$ $\\omega$は$N$の$k$-形式であるため、$N$の点$f(p)$を$\\Lambda^{k}(T_{f(p)}^{\\ast}N)$の元にマッピングする。\n$$ \\Lambda^{k} (T_{f(p)}^{\\ast}N) := \\left\\{ \\varphi : \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\n$\\Lambda^{k} (T_{f(p)}^{\\ast}N)$の定義により、$\\omega (f(p))$もまた、一つの関数である。$\\omega (f(p))$は「$f(p)$上の接ベクトル」$k$個を変数とする。ここでも同様に、$\\omega (f(p))$自体が一つの関数であることを強調するために、以下のような表記を使おう。\n$$ \\omega_{f(p)} = \\omega (f(p)) $$\n$df_{p}v_{i}$ $$ df_{p} : T_{p}M \\to T_{f(p)}N $$\n$f : M \\to N$に対して、$f$の微分 $df_{p}$は上記のように定義される。したがって、$v_{i} \\in T_{p\n}M$であれば、$df_{p}v_{i} = df_{p}(v_{i})$は$T_{f(p)}N$の元である。\nこれを総合すると、$(1)$を得る。\n$$ (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) := \\omega_{f(p)}\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M $$\n上記の二つの関数の定義域を見ると、以下のような違いがある。\n$$ \\begin{align*} (f^{\\ast}\\omega)_{p} : \u0026amp;\u0026amp; \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\\\ \\omega_{f(p)} : \u0026amp;\u0026amp; \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\end{align*} $$\nこの違いを微分$df_{p} : T_{p}M \\to T_{f(p)}N$が繋いでいると考えればよい。そのため、$df_{p}$をプッシュフォワードpush forward, 押し出しとも呼ぶ。$1$-形式$\\varphi$に対して、以下が成立する。\n$$ \\begin{equation} \\varphi( dfv) = f^{\\ast}\\varphi(v) \\end{equation} $$\n$0$-形式のプルバック $f : M \\to N$を二つの微分多様体間で定義された関数とする。$g : N \\to \\mathbb{R}$を関数($N$での$0$-形式)とする。$g$のプルバック$f^{\\ast}g : M \\to \\mathbb{R}$は、以下のように定義される関数($M$での$0$-形式)である。\n$$ f^{\\ast}g := g \\circ f $$\n座標変換 関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$が与えられたとする。$\\mathbf{x} = (x_{1}, \\dots ,x_{n}) \\in \\mathbb{R}^{n}$であり、$\\mathbf{y} = (y_{1}, \\dots ,y_{m}) \\in \\mathbb{R}^{m}$である。\n$$ f(x_{1}, \\dots, x_{n}) = (f_{1}(\\mathbf{x}), \\dots, f_{m}(\\mathbf{x}) )= (y_{1}, \\dots ,y_{m}) $$\nそして、$\\omega = \\sum\\limits_{I} a_{I} dy_{I}$を$\\mathbb{R}^{m}$上の$k$-形式とする。そのとき、$\\omega$のプルバック$f^{\\ast}\\omega$は以下の特性により、次のようになる。\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= f^{\\ast} \\left( \\sum a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast} \\left( a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}dy_{I} \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}(dy_{i1} \\wedge \\cdots \\wedge dy_{ik}) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} (f^{\\ast}dy_{i1} \\wedge \\cdots \\wedge f^{\\ast}dy_{ik}) \\end{align*} $$\nこの時、$(2)$により$f^{\\ast}dy_{i1}(v) = dy_{i1}(df(v)) = d(y_{i1}\\circ f)(v) = df_{i1}(v)$であり、$f^{\\ast}a_{I} = a_{I} \\circ f$であるため、\n$$ \\begin{equation} f^{\\ast} \\omega = \\sum a_{I}(f_{1}, \\dots f_{m}) df_{i1} \\wedge \\cdots \\wedge df_{ik} \\end{equation} $$\n上記の式は座標変換を意味し、具体的にどのようになるかは以下の例で見てみよう。\n例 $\\mathbb{R}^{2} \\setminus \\left{ 0, 0 \\right}$上の$1$-形式$\\omega$が以下のようであるとする。\n$$ \\omega = - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = a_{1}dx + a_{2}dy $$\nこの直交座標上の$1$-形式を極座標に変換してみよう。$U = \\left{ (r,\\theta) : 0 \u0026lt; r, 0 \\le \\theta \u0026lt; 2\\pi \\right}$とする。そして、$f : U \\to \\mathbb{R}^{2}$を以下のようにする。\n$$ f(r,\\theta) = (r\\cos\\theta, r\\sin\\theta) = (f_{1}, f_{2}) $$\nここで、$df_{1}, df_{2}$を計算してみよう。$f_{1} = r\\cos\\theta, f_{2}=r\\sin\\theta$であるため、\n$$ \\begin{align*} df_{1} \u0026amp;= \\dfrac{\\partial f_{1}}{\\partial r}dr + \\dfrac{\\partial f_{1}}{\\partial \\theta}d\\theta = \\cos\\theta dr - r \\sin \\theta d\\theta \\\\ df_{2} \u0026amp;= \\dfrac{\\partial f_{2}}{\\partial r}dr + \\dfrac{\\partial f_{2}}{\\partial \\theta}d\\theta = \\sin\\theta dr + r \\cos \\theta d\\theta \\\\ \\end{align*} $$\nそれにより、$(3)$に従い、\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= a_{1}(f_{1}, f_{2})df_{1} + a_{2}(f_{1}, f_{2})df_{2} \\\\ \u0026amp;= - \\dfrac{f_{2}}{f_{1}^{2} + f_{2}^{2}}(\\cos\\theta dr - r \\sin \\theta d\\theta) + \\dfrac{f_{1}}{f_{1}^{2} + f_{2}^{2}}df_{2}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= - \\dfrac{r\\sin\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\cos\\theta dr - r \\sin \\theta d\\theta) \\\\ \u0026amp;\\quad + \\dfrac{r\\cos\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= -\\dfrac{\\sin\\theta \\cos\\theta}{r}dr + \\sin^{2}\\theta d\\theta + \\dfrac{\\cos\\theta \\sin\\theta}{r}dr + \\cos^{2}\\theta d\\theta \\\\ \u0026amp;= d\\theta \\end{align*} $$\nしたがって、\n$$ \\int - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = \\int d\\theta $$\n■\n特性 $M, N$をそれぞれ$m, n$次元微分多様体、$f : M \\to N$とする。$\\omega, \\varphi$を$N$上の$k$-形式とする。$g$を$N$上の$0$-形式とする。$\\varphi_{i}$たちを$N$上の$1$-形式とする。すると、以下が成立する。\n$$ \\begin{align} f^{\\ast} (\\omega + \\varphi) =\u0026amp;\\ f^{\\ast}\\omega + f^{\\ast}\\varphi \\tag{a} \\\\ f^{\\ast} (g \\omega) =\u0026amp;\\ (f^{\\ast}g) (f^{\\ast}\\omega) \\tag{b} \\\\ f^{\\ast} (\\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k}) =\u0026amp;\\ f^{\\ast}(\\varphi_{1}) \\wedge \\cdots \\wedge f^{\\ast}(\\varphi_{k}) \\tag{c} \\end{align} $$\nこのとき、$+$と$\\wedge$はそれぞれ$k$-形式の合計とくさび積である。\n$\\omega, \\varphi$を$N$上の任意の二つの形式とする。$L$を$l$次元微分多様体、$g : L \\to N$とする。\n$$ \\begin{align*} f^{\\ast}(\\omega \\wedge \\varphi) \u0026amp;= (f^{\\ast}\\omega) \\wedge (f^{\\ast}\\varphi) \\tag{d} \\\\ (f \\circ g)^{\\ast} \\omega \u0026amp;= g^{\\ast}(f^{\\ast}\\omega) \\tag{e} \\end{align*} $$\n証明 証明 $(a)$ $$ \\begin{align*} (f^{\\ast}(\\omega + \\varphi))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\omega + \\varphi)_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ \\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) + \\varphi_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ (f^{\\ast} \\omega)_{p}(v_{1}, \\dots, v_{k}) + (f^{\\ast} \\varphi)_{p}(v_{1}, \\dots, v_{k}) \\\\ =\u0026amp;\\ \\left( f^{\\ast}\\omega + f^{\\ast}\\varphi \\right)_{p}(v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\n証明 $(b)$ $0$-形式$g$と$k$-形式$\\omega$の積を以下のように定義する。\n$$ (g\\omega)(p) = g(p) \\omega (p) $$\nここで、$g(p) = g_{p}$はスカラー、$\\omega (p) = \\omega_{p}$は関数であることに注意。それにより、\n$$ \\begin{align*} (f^{\\ast} (g\\omega))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ g\\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ g_{f(p)} \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ g\\circ f(p) \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ (f^{\\ast}g)_{p} (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\n証明 $(c)$ $$ \\begin{align*} (f^{\\ast}\\left( \\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k} \\right))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\varphi_{1} \\wedge \\dots \\wedge \\varphi_{k})_{f(p)} \\left( df_{1}, \\dots, df_{k} \\right) \\\\ =\u0026amp;\\ \\det [\\varphi_{i}df(v_{j})] \\\\ =\u0026amp;\\ \\det [ f^{\\ast} \\varphi_{i}(v_{j})] \\\\ \\end{align*} $$\n■\nManfredo P. Do Carmo, Differential Forms and Applications, p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3262,"permalink":"https://freshrimpsushi.github.io/jp/posts/3262/","tags":null,"title":"微分幾何学におけるプルバック"},{"categories":"줄리아","contents":"概要 多くの言語でデータフレームがサポートされているにも関わらず、毎回新しくてイライラすることが空の配列の作成です。\nコード タイプ指定 julia\u0026gt; using DataFrames\rjulia\u0026gt; df1 = DataFrame(x = Int64[], y = String[])\r0×2 DataFrame 実際に空の配列をデータとして入れればいいです。この時、タイプが指定され、データが全くない場合は、カラム名とタイプも表示されません。\njulia\u0026gt; push!(df1, [3, \u0026#34;three\u0026#34;])\r1×2 DataFrame\r│ Row │ x │ y │\r│ │ Int64 │ String │\r├─────┼───────┼────────┤\r│ 1 │ 3 │ three │\rjulia\u0026gt; push!(df1, [3.14, \u0026#34;pi\u0026#34;])\r┌ Error: Error adding value to column :x.\r└ @ DataFrames C:\\Users\\rmsms\\.julia\\packages\\DataFrames\\GtZ1l\\src\\dataframe\\dataframe.jl:1606\rERROR: InexactError: Int64(3.14) データを入れると、正常にカラム名とタイプが出力されます。タイプが合わない場合は、データが追加されないので注意が必要です。\nタイプ未指定 julia\u0026gt; df2 = DataFrame(x = [], y = String[])\r0×2 DataFrame\rjulia\u0026gt; push!(df2, [3, \u0026#34;three\u0026#34;])\r1×2 DataFrame\r│ Row │ x │ y │\r│ │ Any │ String │\r├─────┼─────┼────────┤\r│ 1 │ 3 │ three │\rjulia\u0026gt; push!(df2, [3.14, \u0026#34;pi\u0026#34;])\r2×2 DataFrame\r│ Row │ x │ y │\r│ │ Any │ String │\r├─────┼──────┼────────┤\r│ 1 │ 3 │ three │\r│ 2 │ 3.14 │ pi │ データフレームのタイプでストレスを感じたくない場合は、上記のようにAnyの空の配列を作ればいいです。タイプ指定と違って、データがうまく入ったことが確認できます。\n環境 OS: Windows julia: v1.6.2 ","id":2230,"permalink":"https://freshrimpsushi.github.io/jp/posts/2230/","tags":null,"title":"Juliaで空のデータフレームを作成する方法"},{"categories":"줄리아","contents":"説明 Clustering.jl パッケージの hclust() 関数を使えばいい。\nhclust(d::AbstractMatrix; [linkage], [uplo], [branchorder]) 距離行列 を入力として受け取り、階層的クラスタリング の結果を返す。クラスタ間の距離のデフォルトは 単一連結 だ。\nデンドログラム を描くには、Plots.jl ではなく StatsPlots.jl を使わなければならない。\nコード using StatsPlots using Clustering using Distances using Distributions\ra = rand(Uniform(-1,1), 2, 25)\rscatt = scatter(a[1,:], a[2,:], label=false)\rsavefig(scatt, \u0026#34;julia_hclust_scatter.png\u0026#34;) D_a = pairwise(Euclidean(), a, a)\rSL = hclust(D_a, linkage=:single)\rplot_SL = plot(SL)\rp = plot(scatt, plot_SL, size=(800,400))\rsavefig(p, \u0026#34;julia_hclust.png\u0026#34;) ","id":3259,"permalink":"https://freshrimpsushi.github.io/jp/posts/3259/","tags":null,"title":"ジュリアで階層的クラスタリングを行う方法"},{"categories":"줄리아","contents":"コード julia\u0026gt; findfirst(\u0026#34;li\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r8:9\rjulia\u0026gt; findlast(\u0026#34;li\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r14:15\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 1)\r3:3\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 4)\r8:8\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 9)\r14:14\rjulia\u0026gt; findfirst(r\u0026#34;t.+t\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r4:16 findfirst(pattern, A)\n文字列Aでpatternに合致する区間をRangeでリターンする。 パターンには正規表現が入れることができる。最後の例では、最初のtから最後のtまでの区間を見つけてリターンした。 環境 OS: Windows julia: v1.6.2 ","id":2226,"permalink":"https://freshrimpsushi.github.io/jp/posts/2226/","tags":null,"title":"ジュリア文字列で特定のパターン位置を見つける方法"},{"categories":"줄리아","contents":"説明 与えられたデータをhclust()で階層的クラスタリングした後、plot()関数を使ってデンドログラムを描こうとすると、以下のようなエラーが発生する。\nusing Clustering using Distances using Plots\ra = rand(2, 10)\rD_a = pairwise(Euclidean(), a, a)\rSL = hclust(D_a, linkage=:single)\rdendrogram = plot(SL)\rERROR: LoadError: Cannot convert Hclust{Float64} to series data for plotting デンドログラムを描くにはPlots.jlではなくStatsPlots.jlを使用する必要がある。\nusing StatsPlots\rdendrogram = plot(SL)\rsavefig(dendrogram, \u0026#34;julia_dendrogram.png\u0026#34;) ","id":3257,"permalink":"https://freshrimpsushi.github.io/jp/posts/3257/","tags":null,"title":"ジュリアでデンドログラムを描く方法"},{"categories":"줄리아","contents":"コード julia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, \u0026#34;er\u0026#34;)\rtrue\rjulia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, \u0026#34;et\u0026#34;)\rfalse\rjulia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, r\u0026#34;q?\u0026#34;)\rtrue contains(haystack::AbstractString, needle)\nhaystackにneedleが含まれているかをブーリアンで返す。needleにはr\u0026quot;...\u0026quot;のような正規表現が入れられる。 ちなみに、\u0026lsquo;haystack\u0026rsquo;は干し草の山という意味で、「干し草の山から針を探す」という意味の\u0026quot;a needle in a haystack\u0026quot;という表現がある。\n環境 OS: Windows julia: v1.6.2 ","id":2224,"permalink":"https://freshrimpsushi.github.io/jp/posts/2224/","tags":null,"title":"Juliaで特定の文字列を含むかどうかを確認する方法"},{"categories":"줄리아","contents":"概要 Primes.jlは、素数関連の関数や素因数分解を取り扱うパッケージだ。解析的整数論に関する関数の実装はまだ不足している。\nパッケージの全ての機能がまとめられているわけではなく、有用なものだけを選んだので、詳細はリポジトリをチェックしてくれ1。\nタイプ 素因数分解 Primes.Factorization julia\u0026gt; factor(12)\r2^2 * 3\rjulia\u0026gt; factor(12)[1]\r0\rjulia\u0026gt; factor(12)[2]\r2\rjulia\u0026gt; factor(12)[3]\r1\rjulia\u0026gt; factor(12)[4]\r0 素因数分解は、底と指数が区別され、独自のデータ型を使用する。インデックスとして底にアクセスすると、その指数を参照できる。\n関数 素数生成 prime(), primes() julia\u0026gt; using Primes\rjulia\u0026gt; prime(4)\r7\rjulia\u0026gt; primes(10)\r4-element Vector{Int64}:\r2\r3\r5\r7 prime(::Type{\u0026lt;:Integer}=Int, i::Integer)\ni番目の素数を返す。 primes([lo,] hi)\nhiまでの素数の配列を返す。 素数判定 isprime() julia\u0026gt; isprime(7)\rtrue\rjulia\u0026gt; isprime(8)\rfalse isprime(n::Integer)\nnが素数かどうかを判断した結果をブール値で返す。この関数の実装には、ミラーラビン判定法などが使用されている。 素因数分解 factor() julia\u0026gt; factor(24)\r2^3 * 3\rjulia\u0026gt; factor(Vector, 24)\r4-element Vector{Int64}:\r2\r2\r2\r3\rjulia\u0026gt; factor(Array, 24)\r4-element Vector{Int64}:\r2\r2\r2\r3\rjulia\u0026gt; factor(Set, 24)\rSet{Int64} with 2 elements:\r2\r3 factor(n::Integer) -\u0026gt; Primes.Factorization factor(ContainerType, n::Integer) -\u0026gt; ContainerType\nnの素因数分解を返す。この関数の実装には、ポラードの$p-1$ロー素因数分解アルゴリズムなどが使用されている。 ContainerTypeを指定すると、そのコンテナに合わせて結果を返し、別に指定しない場合は、自身のデータ型Primes.Factorizationで返す。 オイラーのφ関数 julia\u0026gt; totient(12)\r4 totient(n::Integer)\nn=$n$に対して、オイラーのφ関数$\\phi$を使用して、$\\displaystyle n \\prod_{p \\mid n} \\left( 1 - {{ 1 } \\over { p }} \\right)$を返す。 https://github.com/JuliaMath/Primes.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2222,"permalink":"https://freshrimpsushi.github.io/jp/posts/2222/","tags":null,"title":"ジュリアでの因数分解および素数関数の使用方法"},{"categories":"줄리아","contents":"概要 Polynomials.jlは多項式関数の表現や計算を含むパッケージだ。数学的に単純な多項式だからコーディングも簡単に考えがちだが、実際に必要な機能を実装し始めると、結構面倒だ。もちろんものすごく難しいわけではないが、できればパッケージを使用しよう。\nパッケージの全ての機能をまとめたわけではなく、役に立ちそうなものだけをピックアップしたので、詳しくはリポジトリをチェックしてほしい1。\n一般的な多項式関数 係数で多項式関数を定義する Polynomial() julia\u0026gt; using Polynomials\rjulia\u0026gt; p = Polynomial([1,0,0,1])\rPolynomial(1 + x^3)\rjulia\u0026gt; q = Polynomial([1,1])\rPolynomial(1 + x)\rjulia\u0026gt; r = Polynomial([1,1], :t)\rPolynomial(1 + t)\rjulia\u0026gt; p(0)\r1\rjulia\u0026gt; p(2)\r9 Polynomial{T, X}(coeffs::AbstractVector{T}, [var = :x])\nそれ自体が多項式関数を返す。coeffsは係数の配列で、最初が定数項で、後ろに行くほど高次項になる。 varでは多項式関数の変数を与える。例のようにシンボル :tを入れるとtの多項式になる。 データで多項式関数を定義する fit() julia\u0026gt; fit([-1,0,1], [2,0,2])\rPolynomial(2.0*x^2) fit(::Type{RationalFunction}, xs::AbstractVector{S}, ys::AbstractVector{T}, m, n; var=:x)\n$x$の座標がxsで、$y$の座標がysの点を通る多項式関数を返す。 根で多項式関数を定義する roots() julia\u0026gt; fromroots([-2,2])\rPolynomial(-4 + x^2) fromroots(::AbstractVector{\u0026lt;:Number}; var=:x)\n与えられた配列の要素が根になるような多項式関数を返す。 演算 +, -, *, ÷ julia\u0026gt; p + 1\rPolynomial(2 + x^3)\rjulia\u0026gt; 2p\rPolynomial(2 + 2*x^3) スカラーを加えたり乗じたりすると、上のように直感的に演算される。\njulia\u0026gt; p + q\rPolynomial(2 + x + x^3)\rjulia\u0026gt; p - q\rPolynomial(-x + x^3)\rjulia\u0026gt; p * q\rPolynomial(1 + x + x^3 + x^4)\rjulia\u0026gt; p ÷ q\rPolynomial(1.0 - 1.0*x + 1.0*x^2) 多項式関数間の四則演算は、+, -, *, ÷でオーバーライドされる。\n根を求める roots() julia\u0026gt; roots(p)\r3-element Vector{ComplexF64}:\r-1.0 + 0.0im\r0.4999999999999998 - 0.8660254037844383im\r0.4999999999999998 + 0.8660254037844383im roots(f::AbstractPolynomial)\nfの根をベクトルとして返す。 微分 derivative() julia\u0026gt; derivative(p, 3)\rPolynomial(6) derivative(f::AbstractPolynomial, order::Int = 1)\nfのorder次の導関数を返す。 積分 integrate() julia\u0026gt; integrate(p, 7)\rPolynomial(7.0 + 1.0*x + 0.25*x^4) integrate(f::AbstractPolynomial, C = 0)\n積分定数がCのfの不定積分を返す。 特別な多項式関数 ローラン多項式関数 LaurentPolynomial() julia\u0026gt; LaurentPolynomial([4,3,2,1], -1)\rLaurentPolynomial(4*x⁻¹ + 3 + 2*x + x²) LaurentPolynomial{T,X}(coeffs::AbstractVector, [m::Integer = 0], [var = :x])\n次数が整数に拡張されたローラン多項式関数を返す。 最小項の次数はmで与えられる。 チェビシェフ多項式関数 ChebyshevT() julia\u0026gt; ChebyshevT([3,2,1])\rChebyshevT(3⋅T_0(x) + 2⋅T_1(x) + 1⋅T_2(x)) ChebyshevT{T, X}(coeffs::AbstractVector)\n第一種チェビシェフ多項式関数を返す。式のT_n(x)は$T_{n}(x) = \\cos \\left( n \\cos^{-1} x \\right)$を表す。 環境 OS: Windows julia: v1.6.2 https://juliamath.github.io/Polynomials.jl/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2220,"permalink":"https://freshrimpsushi.github.io/jp/posts/2220/","tags":null,"title":"ジュリアで多項式を使用する方法"},{"categories":"줄리아","contents":"コード 文字列の連結 * julia\u0026gt; \u0026#34;oh\u0026#34; * \u0026#34;my\u0026#34; * \u0026#34;girl\u0026#34;\r\u0026#34;ohmygirl\u0026#34; Pythonの+に相当する。\n複数の文字列を連結する string() julia\u0026gt; string(\u0026#34;oh\u0026#34;,\u0026#34;my\u0026#34;, \u0026#34;girl\u0026#34;)\r\u0026#34;ohmygirl\u0026#34; Rのpaste0()に相当する。\n文字列のリストのアイテムとして連結する join() julia\u0026gt; OMG = [\u0026#34;oh\u0026#34;,\u0026#34;my\u0026#34;, \u0026#34;girl\u0026#34;]\r3-element Vector{String}:\r\u0026#34;oh\u0026#34;\r\u0026#34;my\u0026#34;\r\u0026#34;girl\u0026#34;\rjulia\u0026gt; join(OMG)\r\u0026#34;ohmygirl\u0026#34; Pythonのjoin()に相当する。\n同じ文字列を繰り返す ^ julia\u0026gt; \u0026#34;=-\u0026#34; ^ 10\r\u0026#34;=-=-=-=-=-=-=-=-=-=-\u0026#34; Pythonの*に相当する。繰り返しをべき乗で表現することは、偶然ではなく、Pythonで文字列を連結する二項演算が+(和)で、これを繰り返すことが*(積)であるように、Juliaでは連結する演算が*(積)で、これを繰り返すことが^(べき乗)になるのだ。\nなぜ？ なぜ、他の言語と同じように直感的に理解しやすい+ではなく*を使うのか？それは代数的、数学的観点から、文字列の結合は加法よりも乗法に近く、自然だからだ1。代数学の自由群というのを理解できれば最高だけど、そういう背景知識がなくても、数学でxとyの積をx * y = xyのように表すのは納得できるだろう。 $$ x \\ast y = xy $$ 今、\u0026quot;xy\u0026quot;という文字列に\u0026quot;litol\u0026quot;という文字列を付け足して、\u0026quot;xylitol\u0026quot;キシリトールを作ると考えてみよう。 $$ xy \\ast litol = xylitol $$ 納得できるだろう。今更\u0026quot;xy\u0026quot; + \u0026quot;litol\u0026quot;を考えると、何か変に感じるかもしれない。Juliaを作った人たちは、そういう数学的直感に真剣そのものだ。\n環境 OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/why-are-there-all-these-strange-stumbling-blocks-in-julia/92644/40\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2218,"permalink":"https://freshrimpsushi.github.io/jp/posts/2218/","tags":null,"title":"ジュリアで文字列を結合する方法"},{"categories":"줄리아","contents":"コード 1 using Plots\rx = rand(30)\ry = rand(30)\rz = rand(30)\rplot(x)\rplot!(y)\rplot!(z)\rpng(\u0026#34;result1\u0026#34;) 上のように、特定のデータだけ凡例に表示させたくない場合がある。\nlabel = \u0026quot;\u0026quot; plot(x, label = \u0026#34;\u0026#34;)\rplot!(y)\rpng(\u0026#34;result2\u0026#34;) そんな時は、label = \u0026quot;\u0026quot;というオプションを使えばいい。図には最初のデータが表示されているけど、凡例には現れないのがわかる。\nprimary = false plot!(z, primary = false)\rpng(\u0026#34;result3\u0026#34;) もうひとつの方法として、primary = falseというオプションを使うこともできるらしい。見ての通り、最後のデータがオレンジ色でプロットされ、凡例からは隠されている。「primary」をオフにすることで生じる副作用なので、可能ならば「label」オプションだけ触るようにしよう。\n環境 OS: Windows julia: v1.6.2 https://github.com/JuliaPlots/Plots.jl/issues/1388#issuecomment-363940741\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2216,"permalink":"https://freshrimpsushi.github.io/jp/posts/2216/","tags":null,"title":"ジュリアプロットで特定のデータラベルを隠す方法"},{"categories":"줄리아","contents":"コード 1 annotate!()を使えばいいんだ。以下のコードはブラウン運動で最大点と最小点をマークした絵を描くコードだよ。\nusing Plots\rcd(@__DIR__)\rdata = cumsum(randn(100))\rplot(data, color = :black, legend = :none)\rannotate!(argmax(data), maximum(data), \u0026#34;max\\n\u0026#34;)\rannotate!(argmin(data), minimum(data), \u0026#34;\\nmin\u0026#34;)\rpng(\u0026#34;result\u0026#34;) 環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/plots-annotate/37784\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2214,"permalink":"https://freshrimpsushi.github.io/jp/posts/2214/","tags":null,"title":"ジュリアプロットにテキストを挿入する方法"},{"categories":"줄리아","contents":"環境 OS: Windows julia: v1.6.2 エラー julia\u0026gt; plot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rGKS: glyph missing from current font: 48652\rGKS: glyph missing from current font: 46972\rGKS: glyph missing from current font: 50868\rGKS: glyph missing from current font: 47784\rGKS: glyph missing from current font: 49496 原因 韓国語フォントが見つからないためだ。\n解決法 二つの方法は特に理想的じゃないし、他にいい方法があれば、いつでも提案してほしい。Juliaを使用する上で韓国語があまり必要ないため、韓国語のサポートが不十分であることは事実だ。\ndefault(fontfamily = \u0026quot;\u0026quot;) 1 plot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;, fontfamily = \u0026#34;\u0026#34;)\rdefault(fontfamily = \u0026#34;\u0026#34;)\rplot(data, color = :red,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;) plot()のfontfamily = \u0026quot;\u0026quot;オプションやdefault(fontfamily = \u0026quot;\u0026quot;)を通じて韓国語を表示させることはできる。しかし、具体的なフォント名を変えても、うまく認識されず、保存する際には結局フォントを見つけられず、一枚目の画像のように文字化けすることなく、テキストが空白で表示される問題があることを確認した。\nPlots.plotly() 2 Plots.plotly()\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.html\u0026#34;) plotlyバックエンドを使用すると、韓国語の文字自体は表示される。しかし、*.pngに直接保存することはできず、*.htmlに出力した後、別途保存する必要がある。\nコード using Plots\r#Plots.gr()\rdata = cumsum(randn(100))\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;, fontfamily = \u0026#34;\u0026#34;)\rdefault(fontfamily = \u0026#34;\u0026#34;)\rplot(data, color = :red,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.png\u0026#34;)\rPlots.plotly()\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.html\u0026#34;) https://discourse.julialang.org/t/nice-fonts-with-plots-gr-and-latexstrings/60037\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://besixdouze.net/16\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2212,"permalink":"https://freshrimpsushi.github.io/jp/posts/2212/","tags":null,"title":"ジュリアプロットに韓国語テキストを挿入する方法"},{"categories":"기하학","contents":"ガウス・ボンネの定理 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$を単連結な測地線座標切片写像、$\\boldsymbol{\\gamma}(I) \\subset \\mathbf{x}(U)$である$\\boldsymbol{\\gamma}$を区間ごとに正則曲線としよう。そして、$\\boldsymbol{\\gamma}$がある領域$\\mathscr{R}$を囲むとする。すると、以下が成立する。\n$$ \\iint_{\\mathscr{R}} K dA + \\int_{\\boldsymbol{\\gamma}} \\kappa_{g} ds + \\sum \\alpha_{i} = 2\\pi $$\nここで$K$はガウス曲率、$\\kappa_{g}$は測地曲率、$\\alpha_{i}$は$\\boldsymbol{\\gamma}$の区間と区間の間の接する点juntion pointでの角度差jump anglesである。\n説明 $\\boldsymbol{\\gamma}$を区間ごとに正則な曲線と仮定したので、タンジェントの方向が突然大きく変わる点があるが、その場所での角度差を$\\alpha_{i}$とした。$\\boldsymbol{\\gamma}$が全体的に滑らかにつながる曲線なら、角度がジャンプする場所はないので、$\\alpha_{i}$は$0$である。(図(が))\n上の定理は$\\mathbf{x}$を測地線座標切片写像という強い条件を置いたときの結果である。より一般的な結果では、式にオイラー指標が登場し、次のようである。\n$$ \\iint_{\\mathscr{R}} K dA + \\int_{C_{i}}\\kappa_{g}ds + \\sum\\alpha_{i} = 2\\pi \\chi(\\mathscr{R}) $$\n証明 $\\mathbf{x}$が測地線座標切片写像であるので、第1基本形式の係数を次のようにしよう。\n$$ \\left[ g_{ij} \\right] = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; h^{2} \\end{bmatrix} $$\nそして、$\\boldsymbol{\\gamma}(t) = \\mathbf{x}\\left( \\gamma^{1}(t), \\gamma^{2}(t) \\right)$としよう。今、$\\mathbf{x}_{1}$と$\\boldsymbol{\\gamma}$のタンジェント$T = \\boldsymbol{\\gamma}^{\\prime}$間の角度を$\\alpha$としよう。\n$$ \\alpha (t) := \\angle ( \\mathbf{x}_{1}, T) $$\n私たちは、$\\boldsymbol{\\gamma}$がパスに沿って一周するとき、$\\mathbf{x}_{1}$を基準にしたときの$T$の角度変化が$2 \\pi$であることを利用して、定理を証明するだろう。まず、$\\boldsymbol{\\gamma}$を単位速度曲線と仮定しよう。そして、$P$を次を満たす$\\boldsymbol{\\gamma}$に沿った平行なベクトル場としよう。(上の図(な)参照)\n$$ P(t) = \\text{parallel vector field starting from a juction point s.t. } \\dfrac{P \\times T}{\\left\\| P \\times T \\right\\|} = \\mathbf{n} $$\nそして、$\\phi$と$\\theta$をそれぞれ$\\mathbf{x_{1}}$と$P$および$P$と$T$間の角度としよう。\n$$ \\phi (t) = \\angle(\\mathbf{x}_{1}, P),\\quad \\theta (t) = \\angle(P, T) $$\n言い換えると、$\\left\\langle \\mathbf{x}_{1}, P(t) \\right\\rangle = \\cos\\phi (t)$であり、これを微分すると、\n$$ -\\sin \\phi (t) \\dfrac{d \\phi}{d t}(t) = \\left\\langle \\dfrac{d \\mathbf{x}_{1}(\\gamma^{1}(t), \\gamma^{2}(t))}{d t}, P(t) \\right\\rangle + \\left\\langle \\mathbf{x}_{1}, \\dfrac{d P}{d t}(t) \\right\\rangle $$\nこの時、$P$が$\\gamma$に沿って平行なベクトル場であるため、$\\dfrac{dP}{dt}$は定義により$M$と垂直である。$\\mathbf{x}_{1}$は$M$と接するので、後ろの項は$0$である。さらに計算すると、\n$$ \\begin{align*} \u0026amp;\\quad -\\sin \\phi (t) \\dfrac{d \\phi}{d t}(t) \\\\ \u0026amp;= \\left\\langle \\dfrac{d \\mathbf{x}_{1}(\\gamma^{1}(t), \\gamma^{2}(t))}{d t}, P(t) \\right\\rangle \\\\ \u0026amp;= \\Big[ \\mathbf{x}_{11}(\\gamma^{1}(t), \\gamma^{2}(t)) (\\gamma^{1})^{\\prime}(t) + \\mathbf{x}_{12}(\\gamma^{1}(t), \\gamma^{2}(t)) (\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\\\ \u0026amp;= \\Big[ \\left( L_{11}\\mathbf{n} + \\Gamma_{11}^{1}\\mathbf{x}_{1} + \\Gamma_{11}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{1})^{\\prime}(t) + \\left( L_{12}\\mathbf{n} + \\Gamma_{12}^{1}\\mathbf{x}_{1} + \\Gamma_{12}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\\\ \u0026amp;= \\Big[ \\left(\\Gamma_{11}^{1}\\mathbf{x}_{1} + \\Gamma_{11}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{1})^{\\prime}(t) + \\left(\\Gamma_{12}^{1}\\mathbf{x}_{1} + \\Gamma_{12}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\end{align*} $$\n二番目の等号は連鎖律により、三番目の等号は第2基本形式とクリストッフェル記号の定義によって成立する。四番目の等号は、$P$と$\\mathbf{n}$が互いに垂直であるために成立する。\n測地線座標切片写像のクリストッフェル記号\n下のもの以外はすべて$0$である。\n$$ \\Gamma_{22}^{1} = -hh_{1},\\quad \\Gamma_{12}^{2} = \\Gamma_{21}^{2} = \\dfrac{h_{1}}{h},\\quad \\Gamma_{22}^{2} = \\dfrac{h_{2}}{h} $$\nこれで、$0$になる項をすべて整理すると、以下のようになる。\n$$ -\\sin\\phi (t) \\phi^{\\prime}(t) = \\left\\langle \\dfrac{h_{1}}{h}(\\gamma^{2})^{\\prime}(t)\\mathbf{x}_{2}, P(t) \\right\\rangle = \\dfrac{h_{1}}{h}(\\gamma^{2})^{\\prime}(t) \\left\\langle \\mathbf{x}_{2}, P(t) \\right\\rangle\\tag{1} $$\n$g_{11} = \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{1} \\right\\rangle = 1$であるため、$\\mathbf{x}_{1}$は単位ベクトルであり、$g_{12} = \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\rangle = 0$であるため、$\\mathbf{x}_{1} \\perp \\mathbf{x}_{2}$である。したがって、$\\left\\{ \\mathbf{x}_{1}, \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|} \\right\\}$はタンジェント平面の正規直交基底となる。従って、タンジェント平面の要素$P$は、以下のように表される。\n$$ P = \\left\\langle \\mathbf{x}_{1}, P \\right\\rangle\\mathbf{x}_{1} + \\left\\langle \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|}, P \\right\\rangle \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|} = \\cos\\phi \\mathbf{x}_{1} + \\sin\\phi \\dfrac{\\mathbf{x}_{2}}{h} $$\nまた、$\\left\\langle \\mathbf{x}_{2}, P \\right\\rangle = \\left\\| x_{2} \\right\\|^{2} \\dfrac{\\sin \\phi}{h} = h\\sin \\phi$を$(1)$に代入すると、\n$$ \\phi^{\\prime}(t) = -h_{1}(\\gamma^{2})^{\\prime}(t) $$\nしたがって、$\\phi$の全角変動は\n$$ \\delta \\phi = \\int_{\\boldsymbol{\\gamma}} \\phi^{\\prime} dt = - \\int_{\\boldsymbol{\\gamma}}h_{1}(\\gamma^{2})^{\\prime}(t)dt = - \\int_{\\boldsymbol{\\gamma}}h_{1} d\\gamma^{2} = - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} \\tag{2} $$\nさらに、以下の式が成立することを示す。\n$$ \\text{Claim: } \\theta^{\\prime} = k_{g} $$\n$\\theta (t) = \\angle(P, T)$としたので、$\\cos\\theta (t) = \\left\\langle P, T \\right\\rangle$であり、これを微分すると、\n$$ -\\sin\\theta (t)\\theta^{\\prime}(t) = \\left\\langle \\dfrac{d P}{d t}, T \\right\\rangle + \\left\\langle P, \\dfrac{d T}{d t} \\right\\rangle = \\left\\langle P, T^{\\prime} \\right\\rangle $$\n二番目の等号は、$dP/dt$が$\\mathbf{n}$と平行であるために成立する。測地曲率の定義により、示したいものを以下のように得る。\n$$ \\begin{align*} \\kappa_{g} = \\left\\langle \\mathbf{S}, T^{\\prime} \\right\\rangle \u0026amp;= \\left\\langle (\\mathbf{n} \\times T), T^{\\prime} \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\mathbf{n}, (T \\times T^{\\prime}) \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\dfrac{P \\times T}{\\sin \\theta}, (T\\times T^{\\prime}) \\right\\rangle \u0026amp; \\because \\dfrac{P \\times T}{\\left\\| P \\times T \\right\\|} = \\mathbf{n} \\\\ \u0026amp;= \\left\\langle \\dfrac{1}{\\sin\\theta} P, (T\\times (T\\times T^{\\prime})) \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\dfrac{1}{\\sin\\theta} P, -T^{\\prime} \\right\\rangle \\\\ \u0026amp;= \\theta^{\\prime}(t) \\end{align*} $$\n三番目、五番目の等号はスカラー三重積が交換可能であるために成立する。したがって、以下を得る。\n$$ \\delta \\theta = \\int_{\\boldsymbol{\\gamma}} \\theta^{\\prime} dt = \\int_{\\boldsymbol{\\gamma}} k_{g}dt \\tag{3} $$\n$\\alpha = \\phi + \\theta$であるため、\n$$ \\int_{\\boldsymbol{\\gamma}} \\alpha^{\\prime}dt = \\int_{\\boldsymbol{\\gamma}} \\phi^{\\prime}dt + \\int_{\\boldsymbol{\\gamma}} \\theta^{\\prime}dt $$\n$(2)$と$(3)$により、以下を得る。\n$$ \\int_{\\boldsymbol{\\gamma}} \\alpha^{\\prime}dt + \\sum_{i}\\alpha_{i} = - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} + \\int_{\\boldsymbol{\\gamma}} k_{g}dt + \\sum_{i}\\alpha_{i} $$\n$\\boldsymbol{\\gamma}$は$\\mathscr{R}$を囲むため、上の式の左辺は明らかに一周したときの角度変化、すなわち$2 \\pi$である。\n$$ {} - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} + \\int_{\\boldsymbol{\\gamma}} k_{g}dt + \\sum_{i}\\alpha_{i} = 2 \\pi $$\nグリーンの定理\n$$ \\oint_{\\partial \\mathscr{R}} Pdx = - \\iint_{\\mathscr{R}} P_{y} dy dx $$\n測地線座標切片写像のガウス曲率\n$$ K = -\\dfrac{h_{11}}{h} $$\n曲面の面積要素\n$$ dA = \\sqrt{g} du^{1} du^{2} $$\n左辺の最初の項は、グリーンの定理を利用すると、以下のように変更できる。\n$$ \\begin{align*} {} - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} \u0026amp;= - \\iint_{\\mathscr{R}}h_{11} du^{1}du^{2} \\\\ \u0026amp;= - \\iint_{\\mathscr{R}}\\dfrac{h_{11}}{h} h du^{1}du^{2} \\\\ \u0026amp;= - \\iint_{\\mathscr{R}}\\dfrac{h_{11}}{h} \\sqrt{g} du^{1}du^{2} \\\\ \u0026amp;= \\iint_{\\mathscr{R}} K dA \\end{align*} $$\n最後に、以下の結論を得る。\n$$ \\iint_{R} K dA + \\int_{\\gamma} \\kappa_{g} ds + \\sum \\alpha_{i} = 2\\pi $$\n","id":3238,"permalink":"https://freshrimpsushi.github.io/jp/posts/3238/","tags":null,"title":"ガウス・ボーネの定理"},{"categories":"줄리아","contents":"コード 1 2 3 julia\u0026gt; replace(\u0026#34;qwerty\u0026#34;, \u0026#34;q\u0026#34;=\u0026gt;\u0026#34;Q\u0026#34;)\r\u0026#34;Qwerty\u0026#34;\rjulia\u0026gt; join(\u0026#34;qwerty\u0026#34;, \u0026#34;,\u0026#34;)\r\u0026#34;q,w,e,r,t,y\u0026#34;\rjulia\u0026gt; split(\u0026#34;qwerty\u0026#34;, \u0026#34;\u0026#34;)\r6-element Vector{SubString{String}}:\r\u0026#34;q\u0026#34;\r\u0026#34;w\u0026#34;\r\u0026#34;e\u0026#34;\r\u0026#34;r\u0026#34;\r\u0026#34;t\u0026#34;\r\u0026#34;y\u0026#34; ジュリアは文字列処理に特出している言語ではないけど、そのせいか、Pythonをたくさん真似して、簡単にそして早く学べる。既に知っている機能のほとんどが実装されていて、モジュールかどうかの部分を除けば、使い方はほとんど似ている。ちなみに、replace()を使う時、\u0026quot;q\u0026quot;=\u0026gt;\u0026quot;Q\u0026quot;は何か独特の文法ではなくて、ペアを直接使ったものだ。\n## 環境\r- OS: Windows\r- julia: v1.7.0 https://docs.julialang.org/en/v1/base/collections/#Base.replace-Tuple{Any,%20Vararg{Pair,%20N}%20where%20N}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/strings/#Base.join\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/strings/#Base.split\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2205,"permalink":"https://freshrimpsushi.github.io/jp/posts/2205/","tags":null,"title":"PythonのようにJuliaで文字列を扱う方法"},{"categories":"줄리아","contents":"コード 比較演算子として$\\approx$を使えば、二つの値が十分に似ている時だけ真を返す。≈は$\\TeX$でと同じように、\\approxと入力してTabを押せば使える。\njulia\u0026gt; π ≈ 3.141592653\rtrue\rjulia\u0026gt; π ≈ 3.14159265\rtrue\rjulia\u0026gt; π ≈ 3.1415926\rfalse\rjulia\u0026gt; π ≈ 3.141592\rfalse 環境 OS: Windows julia: v1.7.0 ","id":2203,"permalink":"https://freshrimpsushi.github.io/jp/posts/2203/","tags":null,"title":"ジュリアで近似値をチェックする方法"},{"categories":"줄리아","contents":"コード 1 julia\u0026gt; d = Dict(\u0026#34;A\u0026#34;=\u0026gt;1, \u0026#34;B\u0026#34;=\u0026gt;2)\rDict{String, Int64} with 2 entries:\r\u0026#34;B\u0026#34; =\u0026gt; 2\r\u0026#34;A\u0026#34; =\u0026gt; 1\rjulia\u0026gt; push!(d,(\u0026#34;C\u0026#34;,3))\rERROR: MethodError: no method matching push!(::Dict{String, Int64}, ::Tuple{String, Int64})\rjulia\u0026gt; push!(d,\u0026#34;C\u0026#34; =\u0026gt; 3)\rDict{String, Int64} with 3 entries:\r\u0026#34;B\u0026#34; =\u0026gt; 2\r\u0026#34;A\u0026#34; =\u0026gt; 1\r\u0026#34;C\u0026#34; =\u0026gt; 3\rjulia\u0026gt; typeof(\u0026#34;C\u0026#34; =\u0026gt; 3)\rPair{String, Int64} ジュリアの辞書Dictionaryは、他のプログラミング言語でよく見られる、キーKeyと値Valueがペアになったデータ型だ。ジュリアの少し違う点は、辞書を各ペアPairの集まりと見なすことだ。提供された実行例で確認できるように、ペアは辞書を構成する要素だ。キーと値は右向きの矢印 =\u0026gt; を通して繋がれ、それ自体も Pair というデータ型を持つ。\n次の例はジュリアで文字列の一部を置換する方法を示している。\njulia\u0026gt; replace(\u0026#34;qwerty\u0026#34;, \u0026#34;q\u0026#34;=\u0026gt;\u0026#34;Q\u0026#34;)\r\u0026#34;Qwerty\u0026#34; Pythonと区別される特徴としては、ペアは辞書なしでもペア自体が存在できることだ。一つのペアだけを含む辞書ではなく、ペア自体をデータとして見るからに、ジュリアのコードはこのように新しい文法のようにペアを活用することもある。直接使うかどうかは別として、読むべきものだ。\n環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/base/collections/#Base.Dict\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2201,"permalink":"https://freshrimpsushi.github.io/jp/posts/2201/","tags":null,"title":"ジュリアから：辞書とペア"},{"categories":"줄리아","contents":"概要 JLD.jlは、Juliaを使用している間に発生する一時データを保存することができるパッケージだ1。純粋なJuliaプロジェクトを進行している際、データの入出力が面倒であれば役立つ。一方で、JLD.jlのインタフェースをより直感的に改善したJLD2.jlも利用可能である。2このポストに紹介された内容は、おおよそこのような機能があるということを認識し、可能な限りJLD2.jlを使用することをお勧めする。下位互換性も問題なくサポートされる。\n一方で、matファイルのようなものではなく、正確にマットラボのmatファイルを読み書きしたい場合は、MAT.jlパッケージを参照してください。\nコード using JLD\rcd(@__DIR__); pwd()\rnumpad = reshape(1:9, 3,3)\rcube = zeros(Int64, 3,3,3)\rsave(\u0026#34;mydata.jld\u0026#34;, \u0026#34;numpad\u0026#34;,numpad, \u0026#34;cube\u0026#34;,cube)\rmydata = load(\u0026#34;mydata.jld\u0026#34;)\rmydata[\u0026#34;numpad\u0026#34;]\rmydata[\u0026#34;cube\u0026#34;] 実行結果 julia\u0026gt; numpad = reshape(1:9, 3,3)\r3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\r1 4 7\r2 5 8\r3 6 9\rjulia\u0026gt; cube = zeros(Int64, 3,3,3)\r3×3×3 Array{Int64, 3}:\r[:, :, 1] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 2] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 3] =\r0 0 0\r0 0 0\r0 0 0\rjulia\u0026gt; save(\u0026#34;mydata.jld\u0026#34;, \u0026#34;numpad\u0026#34;,numpad, \u0026#34;cube\u0026#34;,cube)\r┌ Warning: JLD incorrectly extends FileIO functions (see FileIO documentation)\r└ @ FileIO C:\\Users\\rmsms\\.julia\\packages\\FileIO\\5JdlO\\src\\loadsave.jl:215 保存されるファイルの拡張子は*.jldでなければならない。保存される各データの名前を文字列で与え、割り当てられた変数を連続してつなげると、そのデータが一つにまとめて保存される。\njulia\u0026gt; mydata = load(\u0026#34;mydata.jld\u0026#34;)\r┌ Warning: JLD incorrectly extends FileIO functions (see FileIO documentation)\r└ @ FileIO C:\\Users\\rmsms\\.julia\\packages\\FileIO\\5JdlO\\src\\loadsave.jl:215 Dict{String, Any} with 7 entries:\r\u0026#34;_creator\\\\JULIA_PATCH\u0026#34; =\u0026gt; 0x00000001\r\u0026#34;cube\u0026#34; =\u0026gt; [0 0 0; 0 0 0; 0 0 0]…\r\u0026#34;_creator\\\\WORD_SIZE\u0026#34; =\u0026gt; 64\r\u0026#34;numpad\u0026#34; =\u0026gt; [1 4 7; 2 5 8; 3 6 9]\r\u0026#34;_creator\\\\JULIA_MINOR\u0026#34; =\u0026gt; 0x00000006\r\u0026#34;_creator\\\\ENDIAN_BOM\u0026#34; =\u0026gt; 0x04030201\r\u0026#34;_creator\\\\JULIA_MAJOR\u0026#34; =\u0026gt; 0x00000001 結果を見ると、辞書が返された。保存時に文字列で与えられた名前がキーKeyとして入り、実際のデータは値Valueにある。次のように、辞書として参照すればいい。\njulia\u0026gt; mydata[\u0026#34;numpad\u0026#34;]\r3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\r1 4 7\r2 5 8\r3 6 9\rjulia\u0026gt; mydata[\u0026#34;cube\u0026#34;]\r3×3×3 Array{Int64, 3}:\r[:, :, 1] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 2] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 3] =\r0 0 0\r0 0 0\r0 0 0 JLD2 例で、文字列で辞書を作る過程が不便だったが、JLD2.jlではネームドタプルを使って、同じ機能をより便利に利用することができる。\nhttps://github.com/JuliaIO/JLD.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaIO/JLD2.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2199,"permalink":"https://freshrimpsushi.github.io/jp/posts/2199/","tags":null,"title":"ジュリアで.matのようにデータを保存する方法"},{"categories":"줄리아","contents":"コード 1 Base.Iterators.enumerate() は、Pythonのように配列のインデックスと値の両方を参照できるイテレーターIteratorを返す。\njulia\u0026gt; x = [3,5,4,1,2]\r5-element Vector{Int64}:\r3\r5\r4\r1\r2\rjulia\u0026gt; for (idx, value) in enumerate(x)\rprintln(\u0026#34;x[▷eq1◁value\u0026#34;)\rend\rx[1]: 3\rx[2]: 5\rx[3]: 4\rx[4]: 1\rx[5]: 2\rjulia\u0026gt; typeof(enumerate(x))\rBase.Iterators.Enumerate{Vector{Int64}} https://docs.julialang.org/en/v1/base/iterators/#Base.Iterators.enumerate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2197,"permalink":"https://freshrimpsushi.github.io/jp/posts/2197/","tags":null,"title":"ジュリアのループでインデックスと値の両方を参照する方法"},{"categories":"줄리아","contents":"概要 Juliaに初めて接すると、戸惑うことも少なくないのがシンボルSymbolデータタイプである。シンボルは冒頭に:を付けて使用され、内部データなしにその名前そのもので機能する。主に名前やラベル、辞書のキーなどとして使われる1。\n説明 他のプログラミング言語では、関数にオプションを付ける時に数字を使ったり、意味を正確にするために文字列を使用することが多い。例えば、次の二つの関数がそうである。\njulia\u0026gt; function foo0(x, option = 0)\rif option == 0\rreturn string(x)\relseif option == 1\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo0 (generic function with 2 methods)\rjulia\u0026gt; foo0(3.0, 0)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo0(3.0, 1)\r3\rjulia\u0026gt; function foo1(x, option = \u0026#34;string\u0026#34;)\rif option == \u0026#34;string\u0026#34;\rreturn string(x)\relseif option == \u0026#34;Int\u0026#34;\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo1 (generic function with 2 methods)\rjulia\u0026gt; foo1(3.0, \u0026#34;string\u0026#34;)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo1(3.0, \u0026#34;Int\u0026#34;)\r3 その一方で、以下はシンボルを使用した定義である。一見すると、上記の二つの関数と違いがないように見える。\njulia\u0026gt; function foo2(x, option = :string)\rif option == :string\rreturn string(x)\relseif option == :Int\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo2 (generic function with 2 methods)\rjulia\u0026gt; foo2(3.0, :string)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo2(3.0, :Int)\r3 シンボルを使用する理由は、プログラムの途中で変更されることがないから簡単に説明できる。時にはこの点が不便と感じることもあるが、整数や文字列とは異なり、予期せぬところで変化する可能性が全くない。\nまた、シンボルは真の意味での指定、命令である。インターフェース的な観点では、文字列とシンボルは似ているが、例えば\u0026quot;Int\u0026quot;という文字列を受け取り、その文字列が整数を返す意味であるのに対し、シンボルが:Intとして直接来た場合は、質問もせずに整数型で返すという程度の違いである。この違いが理解できなくても、無理に共感する必要はない。\nその他、シンボルを使用する場合には、データフレームのカラム名など、文字列で表現すると変数と区別しにくい、または区別したくない場合などがある。慣れない表記法のため難しく感じるかもしれないが、用途と違いを理解すれば、特に問題はないという点を押さえておけば良い。\nhttps://docs.julialang.org/en/v1/base/base/#Core.Symbol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2195,"permalink":"https://freshrimpsushi.github.io/jp/posts/2195/","tags":null,"title":"ジュリアでのシンボル"},{"categories":"줄리아","contents":"ガイド 1 julia\u0026gt; x = rand(\u0026#39;a\u0026#39;:\u0026#39;c\u0026#39;, 10)\r10-element Vector{Char}:\r\u0026#39;a\u0026#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\r\u0026#39;a\u0026#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase) 上に示されたような配列があるとしよう。例で、私たちの目標は'a' と 'b' の両方を選ぶことだとしよう。自然には包含演算子 $\\in$でブロードキャストすればいいと思うかもしれないが、結果は以下の通りだ。\njulia\u0026gt; x .∈ [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]\rERROR: DimensionMismatch(\u0026#34;arrays could not be broadcast to a common size; got a dimension with lengths 10 and 2\u0026#34;) DimensionMismatch エラーが発生した。これは配列 x とカテゴリー ['a', 'b'] の両方に同時にブロードキャストが行われたために起きたエラーだ。エラーメッセージを解釈すると、x の長さ10と ['a', 'b'] の長さ2が同時に入ってきて混乱しているということだ。\njulia\u0026gt; x .∈ Ref([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;])\r10-element BitVector:\r1\r1\r1\r0\r1\r0\r0\r0\r1\r1 この場合は Ref() 関数を使ってブロードキャスト問題を解決できる。これにより ['a', 'b'] 内の 'a' と 'b' がスカラとして扱われ、このように二つのキャラクターがある場所だけを見つけることができた。\n注意事項 julia\u0026gt; y = rand(\u0026#39;a\u0026#39;:\u0026#39;c\u0026#39;, 1, 10)\r1×10 Matrix{Char}:\r\u0026#39;b\u0026#39; \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026#39;b\u0026#39; \u0026#39;a\u0026#39; \u0026#39;c\u0026#39; \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026#39;b\u0026#39; \u0026#39;c\u0026#39; 上に示されたような $1 \\times 10$ 行列の場合を考えてみよう。一見すると、上のガイドで見た場合と何も変わらないように思えるかもしれないが、.∈が全く異なる方法で使われている。\njulia\u0026gt; y .∈ [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]\r2×10 BitMatrix:\r0 1 0 0 1 0 1 0 0 0\r1 0 1 1 0 0 0 1 1 0 見ての通り、最初の行は 'a' の位置を、二行目は 'b' の位置を示している。これはベクトルか行列かという違いに由来するものだ。\njulia\u0026gt; y .∈ Ref([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;])\r1×10 BitMatrix:\r1 1 1 1 1 0 1 1 1 0 Ref() を使う場合、一貫した結果を得ることができる。\n環境 OS: Windows julia: v1.7.0 https://stackoverflow.com/a/59978386/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2193,"permalink":"https://freshrimpsushi.github.io/jp/posts/2193/","tags":null,"title":"ジュリアで配列の要素がリストに属しているかを確認する方法"},{"categories":"줄리아","contents":"説明 1次元配列（ベクトル）は次のように定義される。\njulia\u0026gt; A = [1; 2; 3]\r3-element Vector{Int64}:\r1\r2\r3 ここで、;は第一次元を基準に次の要素に移る意味を持つ。これを一般化すると、;;は第二次元を基準に次の要素に移る意味を持つ。\njulia\u0026gt; A = [1; 2; 3;; 4; 5; 6]\r3×2 Matrix{Int64}:\r1 4\r2 5\r3 6 同じ方法で3次元以上の配列を定義することができる。ちなみにこのコードはジュリアバージョン1.7以降で可能である。\njulia\u0026gt; A = [1 2; 3 4;;; 5 6; 7 8]\r2×2×2 Array{Int64, 3}:\r[:, :, 1] =\r1 2\r3 4\r[:, :, 2] =\r5 6\r7 8\rjulia\u0026gt; A = [1 2; 3 4;;; 5 6; 7 8 ;;;; 9 10; 11 12;;; 13 14; 15 16]\r2×2×2×2 Array{Int64, 4}:\r[:, :, 1, 1] =\r1 2\r3 4\r[:, :, 2, 1] =\r5 6\r7 8\r[:, :, 1, 2] =\r9 10\r11 12\r[:, :, 2, 2] =\r13 14 環境 OS: Windows10 Version: Julia 1.7.1 ","id":3223,"permalink":"https://freshrimpsushi.github.io/jp/posts/3223/","tags":null,"title":"ジュリアにおいて多次元配列を直接定義する方法"},{"categories":"줄리아","contents":"ガイド while while文は他の言語と変わらない。\njulia\u0026gt; while x \u0026lt; 10\rx += 1\rprint(\u0026#34;▷eq1◁i - \u0026#34;)\rend\r1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 -\rjulia\u0026gt; for i = 1:10\rprint(\u0026#34;▷eq2◁i - \u0026#34;)\rend\r1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 - ジュリアで主に使われるループのスタイルは、上に示した3つがある。一番上はRやパイソンで使われる方法に似ているし、二番目はマトラブに似ている。最もエレガントな表現は三番目の集合の内包表記を使用した方法だ。\nネストしたループ 以下の2つのループは機能的に全く同じだ。\njulia\u0026gt; X = 1:4; Y = 8:(-1):5;\rjulia\u0026gt; for x ∈ X\rfor y ∈ Y\rprint(\u0026#34; (▷eq3◁y) = ▷eq4◁x + ▷eq5◁(x + y)\u0026#34;)\rif y == 5 println() end\rend\r(1 + 8) = 9 (1 + 7) = 8 (1 + 6) = 7 (1 + 5) = 6\r(2 + 8) = 10 (2 + 7) = 9 (2 + 6) = 8 (2 + 5) = 7\r(3 + 8) = 11 (3 + 7) = 10 (3 + 6) = 9 (3 + 5) = 8\r(4 + 8) = 12 (4 + 7) = 11 (4 + 6) = 10 (4 + 5) = 9 まるで擬似コードPseudo Codeを書くかのようにコードが書かれているのがわかる。注意事項としては、以下のように反復子Iteratorとしてタプルを与えた場合だ。\njulia\u0026gt; for (x,y) ∈ (X, Y)\rprint(\u0026#34; (▷eq3◁y) = ▷eq7◁x + ▷eq5◁(x + y)\u0026#34;)\rif y == 5 println() end\rend\r(1 + 8) = 9 (2 + 7) = 9 (3 + 6) = 9 (4 + 5) = 9 ","id":2191,"permalink":"https://freshrimpsushi.github.io/jp/posts/2191/","tags":null,"title":"ジュリアでエレガントなループを使用する方法"},{"categories":"줄리아","contents":"説明 Julia REPLで右の角括弧 ] を入力すると、パッケージ管理モードに切り替えることができる。パッケージ管理モードで利用可能なコマンドは以下の通りだ。\nコマンド 機能 add foo foo パッケージを追加する。 free foo パッケージの固定を解除する。 help, ? これらのコマンドを表示する。 pin foo foo パッケージのバージョンを固定する。 remove foo, rm foo foo パッケージを削除する。 test foo foo パッケージをテスト実行する。 status, st インストールされている全てのパッケージとそのバージョンを表示する。パッケージ名を入力すると、そのパッケージのバージョンだけが表示される。 undo 最近の実行内容を取り消す。 update, up 全てのパッケージを最新バージョンに更新する。パッケージ名を入力することで、特定のパッケージだけを更新される。 環境 OS: Windows10 Version: Julia 1.7.1 ","id":3217,"permalink":"https://freshrimpsushi.github.io/jp/posts/3217/","tags":null,"title":"ジュリアパッケージ管理モードで使用可能なコマンドのリスト"},{"categories":"줄리아","contents":"説明 この写真は、Pythonでファントム$f$のラドン変換$\\mathcal{R}f$を計算し、それを*.npyファイルとして保存する過程を撮影したものです。ジュリアでこのファイルを読み込む場合は、PyCall.jlパッケージを使用すれば良いです。\nusing PyCall\rnp = pyimport(\u0026#34;numpy\u0026#34;) このコードはPythonでimport numpy as npを実行するのと同じです。そうすることで、Pythonのnumpyで使っているコードをそのまま$f$と$\\mathcal{R}f$を読み込むことができます。\nf = np.load(\u0026#34;f.npy\u0026#34;)\rRf = np.load(\u0026#34;Rf.npy\u0026#34;) しっかり読み込めているか、ヒートマップで確認してみましょう。\np1 = heatmap(reverse(f, dims=1), color=:viridis)\rp2 = heatmap(reverse(Rf, dims=1), color=:viridis)\rplot(p1, p2, size=(728,250)) 環境 OS: Windows10 バージョン: Julia 1.6.2, PyCall 1.93.0 ","id":3215,"permalink":"https://freshrimpsushi.github.io/jp/posts/3215/","tags":null,"title":"ジュリアでnpyファイルを読み込む方法"},{"categories":"줄리아","contents":"コード 例えば、$(5,5)$の配列のヒートマップの上に、$0$から$2\\pi$までのサイン曲線を描きたいとしよう。こんなコードを書きたくなるだろうが、図に見えるように、望んだ通りに出力されない。\nusing Plots\rA = rand(Bool, 5,5)\rheatmap(A, color=:greens)\rx = range(0, 2pi, length=100)\ry = sin.(x)\rplot!(x, y, color=:red, width=3) これは、配列$A$の横と縦の範囲が$1$から$5$までと認識されるからだ。これを解決するためには、下のようなコードで$A$の横と縦の範囲をそれぞれどこからどこまでか指定してやればいい。ちなみに、配列$A$の範囲を指定せずに、ヒートマップが表示される範囲だけを指定すると、下の(な)みたいに表示される。\nxₐ = range(0,2pi, length=5)\ryₐ = range(-1.5,1.5, length=5)\rp1 = heatmap(xₐ, yₐ, A, color=:greens, xlims=(0,2pi), ylims=(-1.5,1.5)) #그림 (가)\rp2 = heatmap(A, color=:greens, xlims=(0,2pi), ylims=(-1.5,1.5)) #그림 (나) これで、p1の上にサイン曲線を再度描くと、望んだ通りに正しく描かれる。\nplot!(x, y, color=:red, width=3) 環境 OS: Windows10 Version: Julia 1.7.1, Plots 1.25.3 ","id":3213,"permalink":"https://freshrimpsushi.github.io/jp/posts/3213/","tags":null,"title":"ジュリアでヒートマップにプロットを重ねて描く方法"},{"categories":"줄리아","contents":"コード 1 LaTeXStrings ライブラリを使うには、文字列の前に L を付けて、L\u0026quot;...\u0026quot; のように書く。\n@time using Plots\r@time using LaTeXStrings\rplot(0:0.1:2π, sin.(0:0.1:2π), xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;)\rtitle!(L\u0026#34;\\mathrm{TeX\\,representation:\\,} y = \\sin x , x \\in [0, 2 \\pi]\u0026#34;) 注意するべき点は、パッケージ名が正確に LaTeXStrings であり、通常のテキストには \\text{} が効かないため、代わりに \\mathrm{} を使う必要があること、スペースは \\, で行うことである。\n環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/latex-code-for-titles-labels-with-plots-jl/1967/18\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2180,"permalink":"https://freshrimpsushi.github.io/jp/posts/2180/","tags":null,"title":"ジュリアでプロットにTeXを使用する方法"},{"categories":"줄리아","contents":"説明 julia\u0026gt; x = [1 2 3]\r1×3 Matrix{Int64}:\r1 2 3\rjulia\u0026gt; y = [1 2 3 4]\r1×4 Matrix{Int64}:\r1 2 3 4\rjulia\u0026gt; x .+ y\rERROR: DimensionMismatch サイズが異なる二つのベクトルは、基本的に要素ごとの演算を行うことができない。これを直接実装するなら、二重for文を使う必要があるが、幸いにも一つを行ベクトル、もう一つを列ベクトルとして与えることで、次のように要素ごとに演算した二次元配列を返すことができる。これは、MATLABやPython NumPyでも可能だ。\nサイズが異なってもエラーにならない点で、意図しない計算になっているか注意が必要だ。\njulia\u0026gt; x\u0026#39; .+ y\r3×4 Matrix{Int64}:\r2 3 4 5\r3 4 5 6\r4 5 6 7\rjulia\u0026gt; x\u0026#39; .* y\r3×4 Matrix{Int64}:\r1 2 3 4\r2 4 6 8\r3 6 9 12\rjulia\u0026gt; x\u0026#39; ./ y\r3×4 Matrix{Float64}:\r1.0 0.5 0.333333 0.25\r2.0 1.0 0.666667 0.5\r3.0 1.5 1.0 0.75 もちろん、x' の代わりに transpose(x) を使ってもできる。\n環境 OS: Windows10 Version: Julia 1.6.2 ","id":3207,"permalink":"https://freshrimpsushi.github.io/jp/posts/3207/","tags":null,"title":"ジュリアで異なるサイズのベクトル成分ごとに操作する方法"},{"categories":"줄리아","contents":"コード 1 ブラウザがダークモードになっていれば、背景が透明になっているのをはっきりと確認できる。\nbackground_color オプションに :transparent シンボルを入れればいいんだ。*.pngとしてはちゃんと保存されるけど、*.pdfとしてはうまく保存されないそうだ。\nusing Plots\rplot(rand(10), background_color = :transparent)\rpng(\u0026#34;example\u0026#34;) オプション名から推測できるように、カラーシンボルを入れれば、その色で出力される。たとえば、黄色の :yellow で描いた絵はこんな感じだ。\n環境 OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/save-figure-with-transparent-background-color-in-plots-jl/18808/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2173,"permalink":"https://freshrimpsushi.github.io/jp/posts/2173/","tags":null,"title":"ジュリアでグラフィックスの背景を透明にする方法"},{"categories":"줄리아","contents":"特定の値まで塗る1 plot()の属性でfillrange=a、fillalpha=b、fillcolor=:colorを使うと、プロットされた曲線から値aまで:colorの色をbの透明度で塗る。fill=(a,b,:color)と書いても同じ機能をする。つまり、以下の2つのコードは同じだ。\nplot(x,y, fillrange=a, fillalpha=b, fillcolor=:color)\rplot(x,y, fill=(a,b,:color)) バグみたいだけど、fillrangeの値を$(0,1)$から選ぶと、塗りつぶしがされない。\nusing Plots\rrandom_walk = cumsum(rand(20).-.5)\rp1 = plot(random_walk,fill=(1,0.2,:lime), lw=3, legend=:bottomright)\rp2 = plot(random_walk,fill=(2,0.2,:tomato), lw=3, legend=:bottomright)\rplot(p1, p2) ２つの曲線の間を塗る fillalphaの値に片方の曲線の関数値を入れると、2つの曲線の間が色付けされる。\nrw = random_walk\rplot([rw rw.+1],fill=(rw.+1,0.2,:lime), lw=3, legend=:bottomright) 閉曲線の内部を塗る fillalphaの値を$(0,1)$だけではなく選ぶと、内部が塗りつぶされる。\ntheta = range(0,2pi, length=40)\rx = cos.(theta)\ry = sin.(theta)\rplot(x, y, fill=(1,0.2,:lime), xlim=(-3,3), ylim=(-1.5,1.5), size=(800,400), lw=3) 環境 OS: Windows10 Version: Julia 1.6.2, Plots 1.23.6 http://docs.juliaplots.org/latest/attributes/#fill\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3203,"permalink":"https://freshrimpsushi.github.io/jp/posts/3203/","tags":null,"title":"ジュリアでの曲線から特定の値まで/二つの曲線の間/閉曲線の内部の塗り方"},{"categories":"정수론","contents":"定義 1 $p \\ge 2$の自然数が$1$と$p$だけを約数に持つ場合、素数Prime Numberと言う。 $m \\ge 2$の自然数が素数ではない場合、合成数Composite Numberと言う。 説明 定義によると、$2$は明らかに素数だ。\n数論で扱う数は非常に広く有理数にまで及ぶが、実際その研究対象は「素数論」とも呼べるほどの関心が集まっている。合成数は他の素数の積で表すことができ、素数について何らかの性質が明らかになれば、その一般化は比較的簡単なので、整数論の多くの定理では条件として素数を求めている。\nSilverman. (2012). 数論へのやさしい入門 (第4版): p46.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2163,"permalink":"https://freshrimpsushi.github.io/jp/posts/2163/","tags":null,"title":"素数と合成数"},{"categories":"프로그래밍","contents":"コード import pandas as pd\rdata = { \u0026#39;나이\u0026#39; : [26,23,22,22,21,21,20,20,20,20,18,17], \u0026#39;키\u0026#39; : [160, 163, 163, 162, 164, 163, 164, 150, 158, 162, 172, 173], \u0026#39;별명\u0026#39; : [\u0026#39;땡모\u0026#39;, \u0026#39;김쿠라\u0026#39;, \u0026#39;광배\u0026#39;, \u0026#39;오리\u0026#39;, \u0026#39;깃털\u0026#39;, \u0026#39;쌈무\u0026#39;, \u0026#39;밍구리\u0026#39;, \u0026#39;나부키 야코\u0026#39;, \u0026#39;월클토미\u0026#39;, \u0026#39;쪼율\u0026#39;, \u0026#39;안댕댕\u0026#39;, \u0026#39;워뇨\u0026#39;], \u0026#39;국적\u0026#39; : [\u0026#39;한국\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;] }\rindexName = [\u0026#39;권은비\u0026#39;,\u0026#39;미야와키 사쿠라\u0026#39;,\u0026#39;강혜원\u0026#39;, \u0026#39;최예나\u0026#39;, \u0026#39;이채연\u0026#39;, \u0026#39;김채원\u0026#39;, \u0026#39;김민주\u0026#39;, \u0026#39;야부키 나코\u0026#39;, \u0026#39;혼다 히토미\u0026#39;, \u0026#39;조유리\u0026#39;, \u0026#39;안유진\u0026#39;, \u0026#39;장원영\u0026#39;]\rdf = pd.DataFrame(data, index = indexName) 列のラベルは .columns で取得する。\n\u0026gt;\u0026gt;\u0026gt; df.columns Index([\u0026#39;나이\u0026#39;, \u0026#39;키\u0026#39;, \u0026#39;별명\u0026#39;, \u0026#39;국적\u0026#39;], dtype=\u0026#39;object\u0026#39;) \u0026gt;\u0026gt;\u0026gt; df.columns[2] \u0026#39;별명\u0026#39; 行のラベルは .index で取得する。.rowsではないことに注意しよう。\n\u0026gt;\u0026gt;\u0026gt; df.index Index([\u0026#39;권은비\u0026#39;, \u0026#39;미야와키 사쿠라\u0026#39;, \u0026#39;강혜원\u0026#39;, \u0026#39;최예나\u0026#39;, \u0026#39;이채연\u0026#39;, \u0026#39;김채원\u0026#39;, \u0026#39;김민주\u0026#39;, \u0026#39;야부키 나코\u0026#39;, \u0026#39;혼다 히토미\u0026#39;, \u0026#39;조유리\u0026#39;, \u0026#39;안유진\u0026#39;, \u0026#39;장원영\u0026#39;], dtype=\u0026#39;object\u0026#39;) \u0026gt;\u0026gt;\u0026gt; df.index[10] \u0026#39;안유진\u0026#39; ","id":3189,"permalink":"https://freshrimpsushi.github.io/jp/posts/3189/","tags":null,"title":"Python Pandasデータフレームの列と行の名前を取得する方法"},{"categories":"줄리아","contents":"コード 1 ==は値が同じかどうかを比較し、===は比較する値が可変Mutableかどうかによって異なる動作をする。\nMutable: 二項が同じオブジェクトを参照しているか確認する。つまり、プログラム上で二つの変数が区別できるかどうかを返す。 Immutable: 二項のタイプが同じかどうかをチェックし、 二項のストラクチャーが同じかどうかをチェックし、 各要素が==で同じかどうかを再帰的にチェックする。 julia\u0026gt; X = 1; Y = 1;\rjulia\u0026gt; X == Y\rtrue\rjulia\u0026gt; X === Y\rtrue\rjulia\u0026gt; X = [1]; Y = [1];\rjulia\u0026gt; X == Y\rtrue\rjulia\u0026gt; X === Y\rfalse 例えば、Pythonでよく見られる上記の実行結果を見てみよう。整数1はImmutableでプログラム上で区別が付かないため、==と===は同じように真を返したが、1だけを含む配列と見た場合にはXに新しい要素が追加されたとしたらXとYが異なる可能性があるMutableため、単に値を比較した==は真を返し、オブジェクト自体を比較した===は偽を返した。\nオブジェクトが何かわからなくても、この用法だけ理解すれば十分だ。\n最適化 オブジェクト指向性が弱いJuliaでは、このような差はそれほど大きく感じられない。コード最適化の観点から見ると、==と===の比較では、シングルトンSingtoneの比較で以下のような性能差が出る。\nN = 10^7\rx = rand(0:9, N)\rjulia\u0026gt; @time for t ∈ 1:N\rx[t] == 0\rend\r1.292501 seconds (30.00 M allocations: 610.336 MiB, 2.63% gc time)\rjulia\u0026gt; @time for t ∈ 1:N\rx[t] === 0\rend\r1.016211 seconds (30.00 M allocations: 610.336 MiB, 2.77% gc time) 値は通常Immutableなので、==よりも===の方が早いと理解すればいい。\nこのような差はほとんどの場合、それほど大きくないかもしれない。当然ながら、イテラティブIterativeではない作業は、次のようなベクター演算の方がずっと速く、この際の速度差はほぼないか無意味である。\njulia\u0026gt; @time x .== 0;\r0.009509 seconds (6 allocations: 1.196 MiB)\rjulia\u0026gt; @time x .=== 0;\r0.009478 seconds (6 allocations: 1.196 MiB) 環境 OS: Windows julia: v1.6.1 https://stackoverflow.com/a/38638838/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2157,"permalink":"https://freshrimpsushi.github.io/jp/posts/2157/","tags":null,"title":"ジュリアにおける==と===の違い"},{"categories":"확률미분방정식","contents":"モデル 1 $t$ 時点で $S_{t}$ を基礎資産 $1$単位の価格とし、$S_{t}$ が幾何ブラウン運動をすると仮定しよう。すなわち、標準ブラウン運動 $W_{t}$ とトレンドDrift $\\mu \\in \\mathbb{R}$ および拡散Diffusion $\\sigma^{2} \u0026gt; 0$ に対して、$S_{t}$ は次の確率微分方程式の解である。 $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ 無リスク金利 $r \\in \\mathbb{R}$ が与えられたとき、$t$ 時点での派生商品 $1$単位の価格 $F = F \\left( t, S_{t} \\right)$ は次の偏微分方程式に従う。 $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\n変数 $F \\left( t, S_{t} \\right)$: 派生商品Derivativesは、先物、オプションなどの金融商品を指す。 $S_{t}$: 基礎資産Underlying Assetsは、通貨、債券、株式など、派生商品の取引対象となる商品を指す。 パラメータ $r \\in \\mathbb{R}$: 無リスク資産Risk-free Assetsの利率を示す。無リスク資産の代表的な例には預金がある。 $\\sigma^{2} \u0026gt; 0$: 市場のボラティリティVolatilityを示す。 説明 派生商品に対する一般的な誤解とは異なり、先物FutureやオプションOptionは、不確実な未来に対するヘッジEdgeの手段として作られた。不確実な未来に備えるために、ある程度の費用Premiumを支払ってもリスクを減らすための方法であった。問題は、その価格を設定する適切な方法がなかったことであり、取引者は経験に基づいて感覚的に派生商品を取引していた。ブラック-ショールズモデルは、そのような派生商品の価格を数学的に説明できるようにした方程式である。\n一般的に、ブラック-ショールズモデル(1973)の貢献者としては、フィッシャー・ブラックFischer Blackとマイロン・ショールズMyron Scholesのほか、本投稿の「ヘッジを用いた導出」を紹介したロバート・マートンRobert K. Mertonの3人が挙げられる。残念ながらブラックは1995年に亡くなり、ショールズとマートンは1997年にノーベル経済学賞を受賞した。ブラック-ショールズ-マートン方程式の発見以降、オプション市場は急速に発展し、学界には金融工学という新たな分野の誕生をもたらした。\nブラックの喜劇 ウィキペディア2によると、ブラックは博士課程の時に専攻を頻繁に変え、どこかに定着するのが苦手だったという。物理学から数学、コンピューター、人工知能へと変更したが、結局名を残した分野は経済学になった。\n信頼できるリファレンスは見つからなかったが、筆者がどこかで聞いた話によると、ブラックは物理学を専攻していた時に、周囲の狂った天才たちを見て「ここでは生き残れない」と思ったという。後に経済/金融を学んでみると、数学を積極的に使う先駆者がいなく、理工学の怪物たちがいない荒れ地で、数学を武器に自らが先駆者となったという。\nショールズの悲劇 ナムウィキ3によると、ショールズは1997年のノーベル経済学賞の記者会見で、賞金で株投資をすると答えてセンセーションを巻き起こしたという。当時、ショールズが運用していたヘッジファンドは過度の自信から過剰なレバレッジLeverageを使用し、1998年のロシアの債務不履行で破綻したという。危機を乗り越えた後、ショールズ は最終的に投資家に利益を返し、その後もファンドマネージャーとして活動を続けたが、サブプライムモーゲージ危機が起こる直前に引退したという。\n前提 本格的な導出に先立ち、いくつかの前提について確認しておこう。\n手数料、税金、配当などの言及されていない要素は考慮しない 物理学モデルで興味の対象でない抵抗や温度、気圧などを考慮しないのと同じ程度に受け止めればよい。そこに加えて、トレンド $\\mu$ と $\\sigma$ などは単純に定数と仮定する。\n派生商品は基礎資産と時点に依存している 派生商品の価格が基礎資産に独立していれば、派生、基礎という言葉を使う理由がない。基礎資産の価格が変わるにつれて派生商品の価格が変わるのが妥当である。また、時間の経過によって変わらない（定数である）ならば、派生商品の価格を検討する意味がない。したがって、$F$ の形を正確には言えないが、少なくとも二つの要素 $t$ と $S_{t}$ に対する関数であると仮定する。 $$ F = F \\left( t, S_{t} \\right) $$\n基礎資産は幾何ブラウン運動をする 幾何ブラウン運動 GBMの代表的な応用は、まさに株価などの基礎資産の価格変動を説明することである。人口の変動量が全体の人口に比例するように、資産の価格変動も資産の価格に比例し、上場廃止にならない限りマイナスになることはないなど、良い前提を多く持っている。 $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$\nある株式の価格 $p_{t}$ が GBMに従うと仮定しよう。$t$日目の終値を$t-1$日目の終値で割り、対数を取った $$ r_{t} = \\nabla \\log p_{t} = \\log {{ p_{t} } \\over { p_{t-1} }} $$ をリターン―リターンReturnと呼ぶが、株価の大きさに関係なく価格が上がれば正の値、下がれば負の値になり、直感と一致する。対数正規分布の項で説明したように、このリターンは正規分布に従い、単純な上下ではなく、株価の成長と逆成長その本質に関心を持つものと見ることができる。\n無リスク資産はメルサス成長をする メルサス成長モデルは、人口動態学Population Dynamicsで、資源の制限や介入などがない場合の集団の成長を説明する最も単純なモデルであり、経済/金融のセンスでは無リスク資産の増殖を説明する前提になる。無リスク収益率は$r$ 定数として仮定され、その金融収益は資産 $N_{t}$ の規模に比例するため、次のような常微分方程式で表現できる。 $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$\n無裁定価格: ポートフォリオ間には価値の差がない ポートフォリオPortfolioに関する数式的な説明は、この証明でさらに詳しく説明する。 無裁定価格Arbitrage-free Pricingの前提とは、我々が考慮するすべてのポートフォリオが同じ価値でバランスを保っているということである。例えば、ポートフォリオ $A$ の価値が $B$ よりも高い場合、合理的な取引主体はより価値のある $A$ の比重を増やして裁定利益を得ることができるため、$B$ を考える理由がない。したがって、我々が考慮するポートフォリオは、このような裁定取引によってこれ以上利益を得ることができない状態であると仮定する。\n摩擦のない市場: 分割と空売りに制限がない 株をやったことがある人ならわかるが、この\n取引というのは最小限の値段単位があり、私が望む金額で取引できないし、空売りをしたくても日本の株式市場では貸借空売りが原則であり、制限がある。その取引単位を自由に分割でき、どんな制約もなく空売りできるということは、行動を妨げる摩擦がないと見ることができる。\n導出 Part 1. ポートフォリオの構成\n我々が保有できる資産は、次の3つの種類だけとしよう。\n基礎資産: $s$ 単位保有しているとしよう。 派生商品: $f$ 単位保有しているとしよう。 無リスク資産: 基礎資産でも派生商品でもない資産で、現金と考えても構わない。 $t$ 時点で我々が保有するすべての資産の価値を $V_{t}$ とすると、$S_{t}$ が基礎資産 $1$単位の価格であり、$F \\left( t , S_{t} \\right)$ が派生商品 $1$単位の価格であったので、次のように表せる。 $$ V_{t} = f F \\left( t, S_{t} \\right) + s S_{t} $$ ポートフォリオを構成するとは、この $f$ と $s$ の量を調整すること、つまりどのように投資するかについての戦略を立てることである。このようなポートフォリオ構成によって発生する取引量が多すぎて市場に影響を与えるという前提は非合理的であるため、基礎資産と派生商品の価格は、$f$ と $s$ の選択に関係なく一定と仮定しよう。つまり、$f$ と $s$ をどのように定めても、以下の数学的議論は変わらないということである。\n注意すべきは、$V_{t}$ は全資産の合計ではないということである。株式口座の残高だけを見ると考えればわかりやすい。ポートフォリオの例としてどのようなものがあるか考えてみよう：\n貯蓄 $V_{t} = 0$：株式口座を整理してすべて貯蓄し、利息だけを受け取る。数学しか知らない士人の目にはあまりにもトリビアルTrivialに見えるかもしれないが、暴落市場や不況に対処できる立派な戦略である。 アリAnt $V_{t} = 5 S_{t}$：個人ならば、派生商品には手を出さないようにしよう。個人が空売りが禁止されている国では、ほとんどの個人投資家はこのようなポートフォリオを持っている。例として、数式で $S_{t} = 81,200$ がサムスン電子の株価であれば、このポートフォリオはサムスン電子 $5$株を保有している私の友人「キム・スヒョン」の口座である。 ヘッジHedge $\\displaystyle V_{t} = 1 \\cdot F- {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t}$：コールオプションCall Optionを $1$だけ買い、その基礎資産を ${{ \\partial F } \\over { \\partial S_{t} }}$ だけ空売りしたとしよう。オプションの満期日に基礎資産の価格が大幅に上昇した場合、コールオプションが大きな利益をもたらし、基礎資産の価格がむしろ下落した場合は、空売りで既に利益を得ている。 ヘッジについての説明で触れたオプションは、ヨーロピアンオプションEuropean Optionであり、通常、私たちが知っている「満期日にのみ権利を行使できるオプション」である。アメリカンオプションAmerican Optionは満期前でも常に権利を行使できるが、大して知る必要はなく、ヨーロピアン、アメリカンという言葉に怖がることはないようにしよう。\n我々は最後の例、現物空売りで派生商品をヘッジするポートフォリオ $$ V_{t} = 1 \\cdot F \\left( t, S_{t} \\right) - {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t} $$ からブラック-ショールズ方程式を導出する。正確にヘッジしているので、このポートフォリオは無リスク資産であり、時間 $t$ に対する増分Incrementは $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} d S_{t} $$ である。ここで、$S_{t}$ は幾何ブラウン運動をすると仮定したので、$d S_{t}$ に $\\displaystyle S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right)$ を代入すると $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ である。一方、無裁定価格の前提を考えると、このポートフォリオと無リスク資産のポートフォリオの増分は同じでなければならない。もしポートフォリオ間に価格差があると仮定すると、他方のポートフォリオを処分して異なるポートフォリオに投資することで裁定利益を得ることができるためである。無リスク資産はメルサス成長をするという前提をしたので、無リスク金利 $r$ に対して、次のような常微分方程式で表される。 $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$ これを整理して表すと $$ \\begin{align*} d V_{t} =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\\\ d V_{t} =\u0026amp; r V_{t} dt \\end{align*} $$ であるため、 $$ \\begin{equation} r V_{t} dt = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\label{1} \\end{equation} $$ を得る。これで、$dF$ を求めるために伊藤積分を通じて計算してみよう。\nPart 2. 伊藤計算\n伊藤の公式: 伊藤過程 $\\left\\{ X_{t} \\right\\}_{t \\ge 0}$ が与えられているとする。 $$ d X_{t} = u dt + v d W_{t} $$ 関数 $V \\left( t, X_{t} \\right) = V \\in C^{2} \\left( [0,\\infty) \\times \\mathbb{R} \\right)$ に対して $Y_{t} := V \\left( t, X_{t} \\right)$ と置くと、$\\left\\{ Y_{t} \\right\\}$ も伊藤過程であり、次が成立する。 $$ \\begin{align*} d Y_{t} =\u0026amp; V_{t} dt + V_{x} d X_{t} + {{ 1 } \\over { 2 }} V_{xx} \\left( d X_{t} \\right)^{2} \\\\ =\u0026amp; \\left( V_{t} + V_{x} u + {{ 1 } \\over { 2 }} V_{xx} v^{2} \\right) dt + V_{x} v d W_{t} \\end{align*} $$\n幾何ブラウン運動で $S_{t}$ を分配法則に従って展開すると $$ d S_{t} = \\mu S_{t} dt + \\sigma S_{t} d W_{t} $$ であり、伊藤の公式で $u = \\mu S_{t}$ および $v = \\sigma S_{t}$ なので $$ d F = \\left( {{ \\partial F } \\over { \\partial t }} + {{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} $$ を得る。$\\eqref{1}$ の $d F$ にこれを代入してみると $$ \\begin{align*} r V_{t} dt =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt - {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} \\\\ =\u0026amp; \\left( {{ \\partial F } \\over { \\partial t }} + {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t}} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ \u0026amp; - {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt} - {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ =\u0026amp; {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt \\end{align*} $$ である。左辺のポートフォリオの価値 $V_{t}$ が $\\displaystyle V_{t} = F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t}$ のように定義されていたので、これを代入すると $$ r \\left( F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\right) dt = {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt $$ である。$rF$ に対して式を整理すると、求めていた次の方程式を得る。 $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\n■\n崔炳善. (2012). ブラック-ショールズ式の様々な導出\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Fischer_Black\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://namu.wiki/w/%EB%B8%94%EB%9E%99-%EC%88%84%EC%A6%88%20%EB%AA%A8%ED%98%95#s-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2156,"permalink":"https://freshrimpsushi.github.io/jp/posts/2156/","tags":null,"title":"ブラック-ショールズモデルの導出"},{"categories":"줄리아","contents":"コード 1 すごく簡単なんだけど、否定演算子の ! と ~ を単項演算子じゃなくて関数として見てしまって、!. や ~. を使う間違いをよくするよ。.! や .~ と書けばいいんだ。\njulia\u0026gt; a = rand(1,10) .\u0026lt; 0.5\r1×10 BitMatrix:\r1 1 0 0 1 0 1 0 0 0\rjulia\u0026gt; .!(a)\r1×10 BitMatrix:\r0 0 1 1 0 1 0 1 1 1\rjulia\u0026gt; .~(a)\r1×10 BitMatrix:\r0 0 1 1 0 1 0 1 1 1 環境 OS: Windows julia: v1.6.0 https://discourse.julialang.org/t/negation-of-boolean-array/16159/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2149,"permalink":"https://freshrimpsushi.github.io/jp/posts/2149/","tags":null,"title":"ジュリアでビット配列を反転させる方法"},{"categories":"줄리아","contents":"コード 1 using Gtk\rfile_name = open_dialog(\u0026#34;파일 열기\u0026#34;) 最初の引数として与えられる文字列は、ダイアログのタイトルだ。実行すると、こんな感じで「ファイルを開く」ダイアログがポップアップするのが確認できる。\n環境 OS: Windows julia: v1.6.0 https://discourse.julialang.org/t/choose-a-file-interactively/10910/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2143,"permalink":"https://freshrimpsushi.github.io/jp/posts/2143/","tags":null,"title":"ジュリアでfile.choose()のようにダイアログボックスを開いてファイルを選択する方法"},{"categories":"기하학","contents":"曲線に沿ったベクトル場[^1] 定義 曲面 $M$と曲線 $\\alpha : \\left[ a, b \\right] \\to M$が与えられたとする。それぞれの$t \\in \\left[ a,b \\right]$を点$\\alpha (t)$上で曲面$M$に対する接ベクトルに対応させる関数$\\mathbf{X}$を曲線$\\alpha$に沿ったベクトル場vector field along a curve$\\alpha$という。\n$$ \\mathbf{X} : \\left[ a, b \\right] \\to \\mathbb{R}^{3} \\\\ \\mathbf{X}(t) \\in T_{\\alpha (t)}M $$\n説明 定義で言う接ベクトルは、曲線$\\alpha$の接ベクトルではなく、$T_{\\alpha (t)}M$の要素である点$\\alpha (t)$での接ベクトルであることに注意しよう。曲面上の各点$\\alpha (t)$での接ベクトルは一意ではないため、曲線$\\alpha$に沿ったベクトル場も一意ではない。接平面に無限のベクトルがあるため$\\mathbf{X}$も無限に存在する。\n簡単な例として$M$上の曲線$\\alpha (t)$が与えられた時、$\\alpha (t)$の接ベクトル場である$\\mathbf{T}(t)$は$\\alpha$に沿ったベクトル場となる。$\\mathbf{S} = \\mathbf{n} \\times \\mathbf{T}$もまた$\\alpha$ベクトル場である。\n$\\mathbf{S}$と$\\mathbf{T}$は接空間の基底となるため、全ての$\\alpha$ベクトル場$\\mathbf{X}$は次のような線形結合で表される。\n$$ \\mathbf{X}(t) = A(t)\\mathbf{T}(t) + B(t)\\mathbf{S}(t)\\quad \\text{for some } A,B:[a,b]\\to \\mathbb{R} $$\n微分可能なベクトル場 定義 $\\alpha (t)$に沿ったベクトル場$\\mathbf{X}(t)$が微分可能であるdifferentiableとは、関数$\\mathbf{X} : \\left[ a, b \\right] \\to \\mathbb{R}^{3}$が微分可能であることを意味する。\n説明 正確には'$\\mathbf{X}$が微分可能である'と言うべきだが、'$\\mathbf{X}(t)$が微分可能である'とも便宜上言う。\n平行なベクトル場 定義 微分可能な$\\alpha$ベクトル場$\\mathbf{X}(t)$が与えられたとする。$\\dfrac{d \\mathbf{X}}{dt}$が曲面$M$と垂直なら、$\\mathbf{X}(t)$が$\\alpha (t)$に沿って平行であるparallel along$\\alpha (t)$と定義する。\n説明 上で説明したように、$\\alpha$ベクトル場は本当に任意に選ぶことができるが、「微分可能な$\\alpha$ベクトル場」という条件は、「平行な線」という概念を話すために制限を設けるものである。\n曲面$M$と垂直であるということは、$\\dfrac{d \\mathbf{X}}{dt}$が接方向の成分は持たず、法線方向の成分のみを持つということと同じである。定義を見ただけではなぜこのようなベクトル場を平行であるというのか理解しにくいかもしれないので、次の例を見よう。\n例 2次元平面で $xy-$平面上の曲線$\\boldsymbol{\\gamma}(t) = \\left( a(t), b(t), 0 \\right)$を考えよう。そして$\\mathbf{X}(t) = \\left( A(t), B(t), 0 \\right)$を$\\boldsymbol{\\gamma}$に沿ったベクトル場としよう。そうすると、\n$$ \\dfrac{d \\mathbf{X}}{dt} = \\left( \\dfrac{d A}{dt}, \\dfrac{d B}{dt}, 0 \\right) $$\nこのベクトルが$xy-$平面と垂直であるためには、任意の全てのベクトル$(x,y,0)$との内積が$0$である必要があるので、次を得る。\n$$ \\dfrac{d A}{dt} = 0 = \\dfrac{d B}{dt} $$\nしたがって$A(t), B(t)$は定数である。これを図で表すと次のようになり、私たちが直感的に考える「曲線$\\boldsymbol{\\gamma}$に沿って平行なベクトルたち」によく合っている。\n球面上で $M$を単位球面としよう。${\\color{6699CC}\\boldsymbol{\\gamma}(t)}$を赤道線としよう。そして$\\boldsymbol{\\gamma}$に沿ったベクトル場${\\color{295F2E}\\mathbf{X}_{\\boldsymbol{\\gamma}}(t) = (0, 0, 1)}$を考えてみよう。そうすると$\\dfrac{d \\mathbf{X}_{\\boldsymbol{\\gamma}}}{dt} = (0,0,0)$であるため、常に▷\n","id":3174,"permalink":"https://freshrimpsushi.github.io/jp/posts/3174/","tags":null,"title":"曲面に沿った平行ベクトル場の定義"},{"categories":"줄리아","contents":"コード 実のところ、Juliaは文字列のフォーマットなどが特に便利な言語ではない。コンソールに出力する際に文字列自体の機能を使う方法もあるが、round()関数のデフォルトオプションであるdigitsを使用する方が便利なことが多いだろう。\njulia\u0026gt; for k in 0:8\rprintln(round(π, digits = k))\rend\r3.0\r3.1\r3.14\r3.142\r3.1416\r3.14159\r3.141593\r3.1415927\r3.14159265 環境 OS: Windows julia: v1.6.0 ","id":2133,"permalink":"https://freshrimpsushi.github.io/jp/posts/2133/","tags":null,"title":"ジュリアで小数点以下特定の桁で丸める方法"},{"categories":"줄리아","contents":"コード 1 ヒートマップを描く時、数値に応じて値のスケールが固定されないと困ることがある。基本のヒートマップ関数でclimオプションを通じて色の範囲を固定することができる。\nusing Plots\rcd(@__DIR__)\rheatmap(rand(4,4)); png(\u0026#34;1.png\u0026#34;)\rheatmap(rand(4,4), clim = (0,1)); png(\u0026#34;2.png\u0026#34;) 結果は以下の通りだ。最初のヒートマップは範囲がないが、二番目のヒートマップは0と1で範囲が固定されているのを確認できる。\n一方だけを制限 heatmap(rand(4,4), clim = (0,Inf))\rheatmap(rand(4,4), clim = (-Inf,1)) 上限だけを置くか、下限だけを置きたい場合は、上に示すようにInfを通じて上下限を開けることができる。\n環境 OS: Windows julia: v1.6.0 同じく見る Pythonのmatplotlib.pyplotで https://discourse.julialang.org/t/setting-min-and-max-values-in-a-heatmap-plots-jl/36496\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2126,"permalink":"https://freshrimpsushi.github.io/jp/posts/2126/","tags":null,"title":"ジュリアでヒートマップの色範囲を指定する方法"},{"categories":"줄리아","contents":"概要 1 Pythonでは、zfill()は文字列クラスのメソッドとして、左側を0で埋める機能を持っている。しかし、Juliaではもっと汎用的で使い勝手の良い組み込み関数としてlpad()を提供している。zfill()はゼロで埋めるって意味で、lpad()は左のパディングって意味だ。\nコード julia\u0026gt; lpad(\u0026#34;12\u0026#34;, 4, \u0026#34;0\u0026#34;)\r\u0026#34;0012\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;0\u0026#34;)\r\u0026#34;0012\u0026#34; 概要での説明を続けると、Juliaでのlpad()はzfill()に比べてもっとジェネリックだ。文字列のメソッドではないから、文字列を引数にしても数字を引数にしても、勝手に文字列にして返してくれる。\njulia\u0026gt; lpad(12, 4)\r\u0026#34; 12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_\u0026#34;)\r\u0026#34;__12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_!\u0026#34;)\r\u0026#34;_!12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_?!\u0026#34;)\r\u0026#34;_?12\u0026#34;\rjulia\u0026gt; lpad(12, 7, \u0026#34;_?!\u0026#34;)\r\u0026#34;_?!_?12\u0026#34; こういう関数を使う一般的な理由は、出力をきれいに整えるためで、必ずしも0が必要だからではない。埋める文字を何も与えなければ、スペースを入れて、キャラクターかストリングを与えれば、上に示したように賢く埋めてくれる。\njulia\u0026gt; rpad(\u0026#34;left\u0026#34;, 6, \u0026#39;0\u0026#39;)\r\u0026#34;left00\u0026#34; もちろん、rpad()関数もある。基本的な機能は同じで、右のパディングという点だけが異なる。\n環境 OS: Windows julia: v1.6.0 https://docs.julialang.org/en/v1/base/strings/#Base.lpad\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2124,"permalink":"https://freshrimpsushi.github.io/jp/posts/2124/","tags":null,"title":"ジュリアでzfill()を使う方法"},{"categories":"줄리아","contents":"コード propertynames() propertynames()関数で確認するといい1。Juliaにはクラスがなく、構造体だけが存在するから2、この関数で返されるすべてのシンボルは、正確にプロパティだけの名前ということになる。\n次はGraphsパッケージでエルデシュ-レニィネットワークを生成し、ノードの数と各ノードのネイバーフッドを確認するコードである。このネットワークにpropertynames()関数を適用し、:neとfadjlistというプロパティがシンボルとして返された。\njulia\u0026gt; using Graphs\rjulia\u0026gt; G_nm = erdos_renyi(50,200)\r{50, 200} undirected simple Int64 graph\rjulia\u0026gt; propertynames(G_nm)\r(:ne, :fadjlist)\rjulia\u0026gt; G_nm.ne\r200\rjulia\u0026gt; G_nm.fadjlist\r50-element Vector{Vector{Int64}}:\r[3, 4, 11, 25, 26, 27, 33, 40, 44, 48]\r[11, 17, 20, 23, 24, 38, 45, 50]\r[1, 4, 15, 24, 29, 30, 34, 42, 46]\r[1, 3, 18, 24, 30, 32, 43, 45]\r[6, 7, 8, 24, 26, 29, 37, 39, 50]\r⋮\r[3, 13, 17, 28, 29, 32, 39, 44, 47, 49]\r[10, 14, 18, 26, 32, 36, 41, 44, 46]\r[1, 21, 23, 24, 25, 32, 41, 44, 45]\r[9, 13, 14, 17, 21, 31, 43, 46, 50]\r[2, 5, 13, 28, 31, 32, 35, 42, 44, 49] fieldnames() 次は少し難しい話だけど、具体的に知らなくてもJuliaのプログラミングには何の問題もない。\npropertynames(x)は根本的にfieldnames(typeof(x))と同じだと言われている3。実際に使う関数としては大きな意味はないが、これを通じて分かる事実は、Juliaでは構造体StructureのインスタンスInstanceをオブジェクトObjectと呼び、構造体そのものが持っている属性AttributeはフィールドField、そしてそのインスタンスとして実際に存在するオブジェクトの属性はプロパティPropertyと呼ばれることである。\n環境 OS: Windows julia: v1.6.0 https://docs.julialang.org/en/v1/base/base/#Base.propertynames\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/a/56352954\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#Base.fieldnames\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2120,"permalink":"https://freshrimpsushi.github.io/jp/posts/2120/","tags":null,"title":"ジュリアで構造体の属性を確認する方法"},{"categories":"논문작성","contents":"アルファ $\\Alpha, \\alpha$ アルファalphaと読む。TeXコードはそれぞれ\\Alpha、\\alpha\nギリシャ文字の最初の文字で、「アルファでありオメガ」は「始まりであり終わり」という意味である。\nインデックス集合のインデックス $\\alpha$ 微分幾何学での曲線 $\\alpha$ 微分多様体上の接ベクトルを定義するときに使われる曲線 $\\alpha$ ベータ $\\Beta, \\beta$ ベータbetaと読む。TeXコードはそれぞれ\\Beta、\\beta\nベータ関数 $B$ ガンマ $\\Gamma, \\gamma$ ガンマgammaと読む。TeXコードはそれぞれ\\Gamma、\\gamma\nガンマ関数 $\\Gamma$ 微分幾何学での曲線 $\\gamma$ クリストッフェル記号 $\\Gamma_{ij}^{k}$ デルタ $\\Delta, \\delta$ デルタdeltaと読む。TeXコードはそれぞれ\\Delta、\\delta\n微積分学で$x$の非常に小さな変化量 $\\Delta x$ 物理学でのラプラシアン $\\Delta$ 偏微分方程式でのラプラシアン $\\Delta$ 数学で非常に小さい正数 $\\delta$ : $\\epsilon - \\delta$ 論法 ディラックのデルタ関数 $\\delta$ クロネッカーのデルタ $\\delta_{ij}$ イプシロン $\\Epsilon, \\epsilon, \\varepsilon$ イプシロンepsilonと読む。TeXコードはそれぞれ\\Epsilon、\\epsilon、\\varepsilon\nイプシロンが正しい発音だが、エプシロンと読まれることが多い。\n数学で非常に小さい正数 $\\epsilon$ : $\\epsilon - \\delta$ 論法 電磁気学での誘電率 $\\epsilon$ レビ-チビタ記号 $\\epsilon_{ijk}$ ゼータ $\\Zeta, \\zeta$ ゼータzetaと読む。TeXコードはそれぞれ\\Zeta、\\zeta\nあまり使われない。変数が足りないときによく使われる。\nリーマンゼータ関数 $\\zeta$ エータ $\\Eta, \\eta$ エータetaと読む。TeXコードはそれぞれ\\Eta、\\eta\nゼータと同様に、適切な変数がないときに使われることがある。\n粒子物理学でバリオン $\\eta$ シータ $\\Theta, \\theta, \\vartheta$ シータthetaと読む。TeXコードはそれぞれ\\Theta、\\theta、\\vartheta\nほとんどの場合、角度を意味すると見なせる。\n角度 $\\theta$ 変数分離時の関数 $\\theta$に対する変数 $\\Theta (\\theta)$ イオタ $\\Iota, \\iota$ イオタiotaと読む。TeXコードはそれぞれ\\Iota、\\iota\nあまり使われない。\nカッパ $\\Kappa, \\kappa$ カッパkappaと読む。TeXコードはそれぞれ\\Kappa、\\kappa\n微分幾何学での曲率 $\\kappa$ :\n法曲率 $\\kappa _{n}$、測地曲率 $\\kappa_{g}$、[主曲率 $\\kappa$] 量子力学で負のエネルギーの置換定数 $\\kappa$ ラムダ $\\Lambda, \\lambda$ ラムダlambdaと読む。TeXコードはそれぞれ\\Lambda、\\lambda\n固有値 $\\lambda$ 物理学での波長 $\\lambda$ ミュー $\\Mu, \\mu$ ミューmuと読む。TeXコードはそれぞれ\\Mu、\\mu\n測度 $\\mu$ 電磁気学での透磁率 $\\mu$ 粒子物理学でミュオン $\\mu$ 統計学での平均 $\\mu$ ニュー $\\Nu, \\nu$ ニューnuと読む。TeXコードはそれぞれ\\Nu、\\nu\n物理学での振動数 $\\nu$ 粒子物理学でニュートリノ $\\nu$ クシー $\\Xi, \\xi$ クシーxiと読む。クサイ、ザイとも読む。ザイ アパートのザイがこれだ。TeXコードはそれぞれ\\Xi、\\xi\n変数が足りないときによく使われる。\n$x$に対するフーリエ変換の変数 $\\xi$ リーマンザイ関数 $\\xi$この場合はザイと読む。 オミクロン $\\Omicron, \\omicron$ オミクロンOmicronと読むが、アルファベット$o$と形がほとんど同じで、ほとんど使われない。TeXコードはそれぞれ\\Omicron、\\omicron\nパイ $\\Pi, \\pi$ パイpiと読む。円周率を意味する。TeXコードはそれぞれ\\Pi、\\pi\n積記号 $\\Pi$ 円周率 $\\pi$ 粒子物理学でパイオン中間子 $\\pi$ ロー $\\Rho, \\rho$ ローrhoと読む。TeXコードはそれぞれ\\Rho、\\rho\n円柱座標系の半径変数 $\\rho$ 物理学で密度 $\\rho$\n体積電荷密度 $\\rho$ シグマ $\\Sigma, \\sigma$ シグマsigmaと読む。TeXコードはそれぞれ\\Sigma、\\sigma\n和記号 $\\Sigma$ 統計学での分散 $\\sigma^{2}$ 電磁気学での表面電荷密度 $\\sigma$ 熱力学で[衝突断面積 $\\sigma$] タウ $\\Tau, \\tau$ 力学でのトルク $\\tau$ : $N$としてもよく使われる。 時間に対する変数として$t$の代わりに使われることがある。 円周率の2倍の数 $\\tau = 2\\pi$ ウプシロン $\\Upsilon, \\upsilon, \\varUpsilon$ ウプシロンupsilonと読む。TeXコードはそれぞれ\\Upsilon、\\upsilon、\\varUpsilon\n粒子物理学でウプシロン中間子 $\\varUpsilon$ カイ $\\Chi, \\chi$ カイchiと読む。先生が$x$を$\\chi$として使わないようにと言うのは、これが実際にはエックス$x$ではなくカイ$\\chi$だからである。是非ともこのように書かないでほしい。TeXコードはそれぞれ\\Chi、\\chi\n特性関数 $\\chi$ プサイ $\\Psi, \\psi$ プサイpsiと読む。TeXコードは\\Psi、\\psi\n$\\phi$とともに、任意の関数を表す際によく使われる。\n量子力学での波動関数 $\\psi$ ファイ $\\Phi, \\phi ,\\varphi$ ファイphiまたはパイと読む。経験上、物理学ではパイ、数学ではファイと読むことが多い。TeXコードはそれぞれ\\Phi、\\phi、\\varphi\n$\\psi$とともに、任意の関数を表す際によく使われる。\n円柱座標系の変数 $\\phi$ 球座標系の変数 $\\phi$ 量子力学での波動関数 $\\phi$ オメガ $\\Omega, \\omega$ オメガomegaと読む。TeXコードはそれぞれ\\Omega、\\omega\nギリシャ文字の最後の文字で、「アルファでありオメガ」という言葉は「始まりであり終わり」、「全て」という意味である。\n偏微分方程式、関数解析学での開集合 $\\Omega$ : $U$とともによく使われる記法である。 物理学で抵抗の単位 $\\Omega$ 球の立体角 $\\Omega$ 物理学での角振動数 $\\omega$ $t$に対するフーリエ変換の変数 $\\omega$ 多項式の複素数根 $\\omega$ ","id":3145,"permalink":"https://freshrimpsushi.github.io/jp/posts/3145/","tags":null,"title":"ギリシャ文字の読み方・書き方と数学・科学における意味"},{"categories":"줄리아","contents":"概要1 名前はCalculus.jlだけど、積分はサポートしない。\n機械学習などで話される自動微分が必要ならZygote.jlパッケージを参照してほしい。\n一変数関数の微分 導関数 derivative() $f : \\R \\to \\R$の導関数を求めてくれる。\nderivative(f)またはderivative(f, :x): 導関数$f^{\\prime}$を返す。 derivative(f, a): 微分係数$f^{\\prime}(a)$を返す。 julia\u0026gt; f(x) = 1 + 2x + 3x^2\rf (generic function with 1 method)\rjulia\u0026gt; g(x) = sin(x)\rg (generic function with 1 method)\rjulia\u0026gt; derivative(f)\r#1 (generic function with 1 method)\rjulia\u0026gt; derivative(f, 1)\r7.99999999996842\rjulia\u0026gt; Df = derivative(f)\r#1 (generic function with 1 method)\rjulia\u0026gt; Dg = derivative(g)\r#1 (generic function with 1 method)\r#f\u0026#39;(x) = 2 + 6x\rjulia\u0026gt; Df(1)\r7.99999999996842\r#g\u0026#39;(x) = cos x\rjulia\u0026gt; Dg(pi)\r-0.9999999999441258 合成関数も微分することができる。\n#f∘g(x) = (2 + 6 sin x)cos x\rjulia\u0026gt; derivative(f∘g)\r#1 (generic function with 1 method)\rjulia\u0026gt; derivative(f∘g, pi/4)\r4.414213562300037\rjulia\u0026gt; (2+6sin(pi/4))cos(pi/4)\r4.414213562373095 二階導関数 second_derivative() $f : \\R \\to \\R$の二階導関数を求めてくれる。\nderivative()で返される関数は整数を入力値に使えるが、second_derivative()は整数型を使えない。無理数型も使えない。\njulia\u0026gt; derivative(f, 1)\r7.99999999996842\rjulia\u0026gt; second_derivative(f, 1)\rERROR: MethodError: no method matching eps(::Type{Int64})\rClosest candidates are:\reps() at float.jl:764\reps(::AbstractFloat) at float.jl:760\reps(::Type{Float16}) at float.jl:761\rjulia\u0026gt; second_derivative(f, 1.)\r5.9999956492003905\rjulia\u0026gt; second_derivative(g, pi)\rERROR: MethodError: no method matching eps(::Type{Irrational{:π}})\rClosest candidates are:\reps() at float.jl:764\reps(::AbstractFloat) at float.jl:760\reps(::Type{Float16}) at float.jl:761\rjulia\u0026gt; second_derivative(g, convert(Float64, pi))\r-1.3553766145945872e-7\rjulia\u0026gt; second_derivative(g, 1pi)\r-1.3553766145945872e-7 多変数関数の微分 グラディエント gradient() $f : \\mathbb{R}^{n} \\to \\mathbb{R}$のグラディエントを返す。\n注意するべき点は、多変数関数を定義するとき、実際に変数が複数ある関数として定義してはいけないということだ。ベクトルを入力として受け取る一変数関数として定義しなければならない。ベクトルを入力として受け取る関数でなければ、微分はできても、値を計算することができないということだ。例えば、$f_{1}$のように定義してはいけないが、$f_{2}$のように定義する必要があるということだ。\njulia\u0026gt; f₁(x,y,z) = x*y + z^2\rf₁ (generic function with 1 method)\rjulia\u0026gt; Calculus.gradient(f₁)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₁ = Calculus.gradient(f₁)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₁(1,1,1)\rERROR: MethodError: no method matching (::Calculus.var\u0026#34;#2#4\u0026#34;{typeof(f₁), Symbol})(::Int64, ::Int64, ::Int64)\rjulia\u0026gt; ∇f₁([1,1,1])\rERROR: MethodError: no method matching f₁(::Vector{Float64})\rjulia\u0026gt; Calculus.gradient(f₁, 1,1,1)\rERROR: MethodError: no method matching gradient(::typeof(f₁), ::Int64, ::Int64, ::Int64)\rjulia\u0026gt; Calculus.gradient(f₁, [1,1,1])\rERROR: MethodError: no method matching f₁(::Vector{Float64})\rjulia\u0026gt; f₂(x) = x[1]*x[2] + x[3]^2\rf₂ (generic function with 1 method)\rjulia\u0026gt; Calculus.gradient(f₂, [1,1,1])\r3-element Vector{Float64}:\r1.0000000000235538\r1.0000000000235538\r1.9999999999737708\rjulia\u0026gt; ∇f₂ = Calculus.gradient(f₂)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₂(1,1,1)\rERROR: MethodError: no method matching (::Calculus.var\u0026#34;#2#4\u0026#34;{typeof(f₂), Symbol})(::Int64, ::Int64, ::Int64)\rjulia\u0026gt; ∇f₂([1,1,1])\r3-element Vector{Float64}:\r1.0000000000235538\r1.0000000000235538\r1.9999999999737708 ヘッシアン hessian() $f : \\mathbb{R}^{n} \\to \\mathbb{R}$のヘッシアンを返す。\nsecond_derivative()と同様に、Floatデータ型のみ入力を受け付ける。gradient()と同様に、ベクトルを入力として受け取る関数に対してのみ値を返すことができる。\njulia\u0026gt; hessian(f₂, [1.,1.,1.])\r3×3 Matrix{Float64}:\r3.88008e-7 1.0 0.0\r1.0 3.88008e-7 0.0\r0.0 0.0 2.0\rjulia\u0026gt; H = hessian(f₂)\r#7 (generic function with 1 method)\rjulia\u0026gt; H([1.,1.,1.])\r3×3 Matrix{Float64}:\r3.88008e-7 1.0 0.0\r1.0 3.88008e-7 0.0\r0.0 0.0 2.0 ヤコビアン jacobian() $f : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$のヤコビアンを返す。\nsecond_derivative()と同様に、Floatデータ型のみ入力を受け付ける。gradient()と同様に、ベクトルを入力として受け取る関数に対してのみ値を返すことができる。\n他の関数とは異なり、jacobian(f, [x, y, z])のように使うことはできない。\njulia\u0026gt; h(x) = [x[1], x[1]*x[2], x[1]*x[3]^2]\rh (generic function with 2 methods)\rjulia\u0026gt; jacobian(h, [1.,1.,1.])\rERROR: MethodError: no method matching jacobian(::typeof(h), ::Vector{Float64})\rjulia\u0026gt; Jh = jacobian(h)\r(::Calculus.var\u0026#34;#g#5\u0026#34;{typeof(h), Symbol}) (generic function with 1 method)\rjulia\u0026gt; Jh([1.,1.,1.])\r3×3 Matrix{Float64}:\r1.0 0.0 0.0\r1.0 1.0 0.0\r1.0 0.0 2.0 記号微分 記号微分はSymEngine.jlパッケージでも使用できる。\ndifferentiate() 記号微分を実行する。\n定数項と$x$はキレイに返すが、$ax$や$x^{n}$のような場合は、積の微分法の形で返す。例えば$3x^{2}$を微分すると、それを$3$と$x^{2}$の積と見て$\\dfrac{d 3}{dx} x^{2} + 3\\dfrac{d x^{2}}{dx}$のように返すということだ。さらに$x^{2}$も$1$と$x^{2}$の積と見る。\njulia\u0026gt; differentiate(\u0026#34;1\u0026#34;, :x)\r0\rjulia\u0026gt; differentiate(\u0026#34;1 + x\u0026#34;, :x)\r1\rjulia\u0026gt; differentiate(\u0026#34;x^2\u0026#34;, :x)\r:(2 * 1 * x ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;x^3\u0026#34;, :x)\r:(3 * 1 * x ^ (3 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + x^2\u0026#34;, :x)\r:(1 + 2 * 1 * x ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + 2x + 3x^2 + 4x^3\u0026#34;, :x)\r:((0 * x + 2 * 1) + (0 * x ^ 2 + 3 * (2 * 1 * x ^ (2 - 1))) + (0 * x ^ 3 + 4 * (3 * 1 * x ^ (3 - 1))))\rjulia\u0026gt; differentiate(\u0026#34;x^2 * sin(x) + exp(x) * cos(x)\u0026#34;, :x)\r:(((2 * 1 * x ^ (2 - 1)) * sin(x) + x ^ 2 * (1 * cos(x))) + ((1 * exp(x)) * cos(x) + exp(x) * (1 * -(sin(x))))) 入力された記号でない文字は定数として扱い、二つ以上の記号を入力した場合は、それぞれの記号に対する微分を返す。ただし、\u0026quot;3yx\u0026quot;と書くとxy自体を一つの変数と見てしまうので、必ず掛け算記号を入れて3x*yのように表さなければならない。\njulia\u0026gt; differentiate(\u0026#34;1 + x + 3yx + y^2\u0026#34;, :x)\r:(1 + (0yx + 3 * 0))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3y*x + y^2\u0026#34;, :x)\r:(1 + ((0y + 3 * 0) * x + (3y) * 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3yx + y^2\u0026#34;, [:x, :y])\r2-element Vector{Any}:\r:(1 + (0yx + 3 * 0))\r:((0yx + 3 * 0) + 2 * 1 * y ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, [:x, :y])\r2-element Vector{Any}:\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\r:(((0 * x + 3 * 0) * y + (3x) * 1) + 2 * 1 * y ^ (2 - 1)) simplify() differentiate()の返り値は読みにくいが、simplify()はこれをきれいに整理してくれる。ただし、ベクトルを入力として使うときはうまく実行されない。\njulia\u0026gt; simplify(differentiate(\u0026#34;1 + 2x + 3x^2 + 4x^3\u0026#34;, :x))\r:(2 + 3 * (2x) + 4 * (3 * x ^ 2))\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :x))\r:(1 + 3y)\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :y))\r:(3x + 2y)\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, [:x, :y]))\r2-element Vector{Any}:\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\r:(((0 * x + 3 * 0) * y + (3x) * 1) + 2 * 1 * y ^ (2 - 1)) deparse() 記号微分の返り値を文字列に変換する。\njulia\u0026gt; a = differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :x)\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\rjulia\u0026gt; deparse(a)\r\u0026#34;1 + ((0 * x + 3 * 1) * y + (3 * x) * 0)\u0026#34;\rjulia\u0026gt; deparse(simplify(a))\r\u0026#34;1 + 3 * y\u0026#34; 検算 check_derivative() derivative()で得た導関数が本当の導関数とどれくらい違うかを確認することができる。jacobian()を除いた他の4つの導関数に対して実装されている。\ncheck_derivative(f, Df, a): derivative(f, a)-Df(a)の絶対値を返す。 julia\u0026gt; f(x) = 1 + x^2\rf (generic function with 1 method)\rjulia\u0026gt; Df(x) = 2x\rDf (generic function with 1 method)\rjulia\u0026gt; Calculus.check_derivative(f, Df, 1)\r2.6229241001374248e-11 応用 Polynomials.jl Polynomials.jl自体にもderivativeが実装されているが、Calculus.derivative()でも導関数を求めることができる。\njulia\u0026gt; p = Polynomial([1,2,4,1])\rPolynomial(1 + 2*x + 4*x^2 + x^3)\rjulia\u0026gt; Polynomials.derivative(p)\rPolynomial(2 + 8*x + 3*x^2)\rjulia\u0026gt; Calculus.derivative(p)\r#1 (generic function with 1 method)\rjulia\u0026gt; Polynomials.derivative(p,2)\rPolynomial(8 + 6*x)\rjulia\u0026gt; Calculus.second_derivative(p)\r#6 (generic function with 1 method) 環境 OS: Windows10 Version: Julia 1.6.2, Calculus 0.5.1 https://github.com/JuliaMath/Calculus.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3135,"permalink":"https://freshrimpsushi.github.io/jp/posts/3135/","tags":null,"title":"ジュリアでの微分の求め方"},{"categories":"줄리아","contents":"コード fill() 関数を使えばいい。Rの rep() 関数と似た機能をする。\n","id":2101,"permalink":"https://freshrimpsushi.github.io/jp/posts/2101/","tags":null,"title":"ジュリアで特定の値で埋めた配列を作る方法"},{"categories":"기하학","contents":"ビルドアップ1 微分多様体 $M$ の各点で接ベクトルを定義しようとしている。微分可能な曲線 $\\alpha : (-\\epsilon , \\epsilon) \\to M$が与えられたとする。これから、微分幾何学でのように、$\\alpha$の$t=0$での微分係数$\\dfrac{d \\alpha}{dt}(0)$を接ベクトルと定義したいが、$\\alpha$の値域が$M$であるため（距離空間とは限らないため）、$\\alpha$の導関数を言及することができない。このため、多様体上の接ベクトルを関数、つまりオペレーターとして定義することになる。微分幾何学を学んだなら、ベクトルをオペレーターとして扱うことに慣れているはずだ。次の説明を見てみよう。\n方向微分\n$\\mathbf{X} \\in T_{p}M$を曲面$M$の点$p$での接ベクトル、$\\alpha (t)$を$M$上の曲線とする。この時、$\\alpha : (-\\epsilon, \\epsilon) \\to M$であり、$\\alpha (0) = p$を満たす。つまり、$\\mathbf{X} = \\dfrac{d \\alpha}{d t} (0)$である。ここで関数$f$を曲面$M$上の点$p \\in M$のある近傍で定義された微分可能な関数とする。すると$\\mathbf{X}$方向への$f$の方向微分directional derivative$\\mathbf{X}f$を次のように定義する。\n$$ \\mathbf{X} : \\mathcal{D} \\to \\mathbb{R}, \\quad \\text{where } \\mathcal{D} \\text{ is set of all differentiable functions near } p $$\n$$ \\mathbf{X} f := \\dfrac{d}{dt_{}} (f \\circ \\alpha) (0) $$\n上の定義から見て、固定された接ベクトル$\\mathbf{X}$があれば、$f$が与えられるたびに$\\mathbf{X}f$が決定される。したがって、接ベクトルはそれ自体がオペレーターとして扱われる。$\\mathbf{X}f$のような記法も、オペレーターの観点から見るために使われる。微分多様体上の接ベクトルも同様に、$M$上で微分可能な関数$f$が与えられるたびに、$f$とある曲線$\\alpha$との合成を通じて実数空間をマッピングする関数として定義される。\n定義 $M$を$n$次元の微分多様体とする。微分可能な関数 $\\alpha : (-\\epsilon , \\epsilon) \\to M$を**$M$で微分可能な曲線**とする。$\\alpha (0)=p\\in M$と仮定する。そして集合$\\mathcal{D}$を次のように$p$で微分可能な関数の集合として定義する。\n$$ \\mathcal{D} := \\left\\{ f : M \\to \\mathbb{R} | \\text{functions on } M \\text{that are differentiable at } p \\right\\} $$\nすると、$\\alpha (0) = p$での接ベクトル$\\alpha^{\\prime}(0) : \\mathcal{D} \\to \\mathbb{R}$を次のような関数として定義する。\n$$ \\alpha^{\\prime} (0) f = \\dfrac{d}{dt} (f\\circ \\alpha)(0),\\quad f\\in \\mathcal{D} $$\n点$p\\in M$でのすべての接ベクトルの集合を接空間tangent spaceと呼び、$T_{p}M$のように表す。\n説明 $f : M \\to \\mathbb{R}$と$\\alpha : (-\\epsilon, \\epsilon) \\to M$はそれぞれ定義域と値域が距離空間であることが保証されていないため、古典的な意味で微分できないが、これらの合成である$f \\circ \\alpha : (-\\epsilon, \\epsilon) \\to \\mathbb{R}$は微分可能である。\nある微分可能な曲線$\\alpha$が与えられるたびに接ベクトルが決定されるので、微分可能な曲線が存在する限り接ベクトルが存在すると考えることができる。また、二つの接ベクトル$\\mathbf{X}, \\mathbf{Y}$が異なる二つの曲線$\\alpha$、$\\beta$によって決定されたとしても、すべての$f \\in \\mathcal{D}$に対して$\\mathbf{X}f = \\mathbf{Y}f$が成立する場合、$\\mathbf{X}$と$\\mathbf{Y}$を同じ接ベクトルとして扱う。\n接ベクトルの集合$T_{p}M$を接空間と呼ぶ理由は、これが実際に$n$次元のベクトル空間であるからである。\n以下に紹介する定理から、点$p$での接ベクトルの関数値$\\alpha^{\\prime}(0)f$は、任意の座標系$\\mathbf{x} : U \\to M$を一つ選択することでこれに対して表すことができ、この値は$\\mathbf{x}$の選択に依存しないことがわかる。\n例 $T_{p}\\mathbb{R}^{3}$を考えよう。ある微分可能な曲線$\\alpha : (-\\epsilon, \\epsilon) \\to \\mathbb{R}^{3}$が決定されると、3次元ベクトル$\\alpha^{\\prime}(0) = \\mathbf{v} = (v_{1}, v_{2}, v_{3}) \\in \\mathbb{R}^{3}$が決定される。定義により、接ベクトルは次のようになる。$f : \\mathbb{R}^{3} \\to \\mathbb{R}$に対して、\n$$ \\mathbf{X}f = \\dfrac{d (f\\circ \\alpha)}{d t}(0) = \\sum \\limits_{i} \\dfrac{\\partial f}{\\partial x_{i}}\\dfrac{d \\alpha_{i}}{d t}(0) = \\sum\\limits_{i} v_{i} \\dfrac{\\partial f}{\\partial x_{i}} $$\nこれはユークリッド空間での方向微分と同じである。\n$$ \\mathbf{v}[f] = \\nabla _{\\mathbf{v}}f = \\mathbf{v} \\cdot \\nabla f = \\sum \\limits_{i} v_{i} \\dfrac{\\partial f}{\\partial v_{i}} $$\n方向微分はベクトルをオペレーターとして扱ったものであり、実質的にベクトルと同じである。したがって、$\\mathbf{X}$は$\\mathbb{R}^{3}$の要素として扱うことができ、次が成立する。\n$$ T_{p}\\mathbb{R}^{3} \\approxeq \\mathbb{R}^{3} $$\n定理 $\\alpha (0) = p$である微分可能な曲線と点$p$での座標系$\\mathbf{x} : U \\to M$が与えられたとする。$(u_{1}, \\dots, u_{n})$は$\\mathbb{R}^{n}$の座標であり、\n$$ (x_{1}(p), \\dots, x_{n}(p)) = \\mathbf{x}^{-1}(p) $$\nとする。すると、次の式が成立する。\n$$ \\begin{align*} \\alpha ^{\\prime} (0) f =\u0026amp;\\ \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(p) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{p} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(\\alpha (0)) \\left.\\dfrac{\\partial f}{\\partial x_{i}}\\right|_{t=0} \\end{align*} $$\nこの時、単純に$x_{i}^{\\prime}(0) = x_{i}^{\\prime}(\\alpha (0))$と表記する。したがって、$\\alpha^{\\prime}(0)$は次のような微分オペレーターである。\n$$ \\begin{equation} \\alpha ^{\\prime} (0) = \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} \\end{equation} $$\n基底$\\left\\{ \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} \\right\\}$に対して座標ベクトルで表記すると、次のようになる。\n$$ \\alpha ^{\\prime} (0) = \\begin{bmatrix} x_{1}^{\\prime}(0) \\\\ \\vdots \\\\ x_{n}^{\\prime}(0) \\end{bmatrix} $$\n証明 $p = \\mathbf{x}(0)$となるような$M$の座標系 $\\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to M$を一つ選ぼう。接ベクトルを座標系で表現できるように$f\\circ \\alpha = f \\circ \\mathbf{x} \\circ \\mathbf{x}^{-1} \\circ \\alpha$のように考える。すると、$\\mathbf{x} \\circ \\mathbf{x}^{-1} = I$は恒等関数であるため、どの座標系を選んでも関係ないことがわかる。これから、$f \\circ \\mathbf{x}$と$\\mathbf{x}^{-1} \\circ \\alpha$をそれぞれ全体として一つの関数とみなし、$f \\circ \\alpha$をこれら二つの合成関数と考える。\n$$ f \\circ \\alpha = (f \\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha) $$\nまず$f \\circ \\mathbf{x}$を考える。$f \\circ \\mathbf{x} : \\mathbb{R}^{n} \\to \\mathbb{R}$であるため、次のように表現され、古典的な意味で微分が可能である。\n$$ f \\circ \\mathbf{x} = f \\circ \\mathbf{x} (u) = f \\circ \\mathbf{x} (u_{1}, u_{2}, \\dots, u_{n}),\\quad u=(u_{1},\\dots,u_{n}) \\in \\mathbb{R}^{n} $$\n$\\mathbf{x}^{-1} \\circ \\alpha$もまた、$\\mathbf{x}^{-1} \\circ \\alpha : \\mathbb{R} \\to \\mathbb{R}^{n}$であるため、次のように表現され、古典的な意味で微分が可能である。\n$$ \\begin{align*} \\mathbf{x}^{-1} \\circ \\alpha (t) =\u0026amp;\\ (x_{1}(\\alpha (t)), x_{2}(\\alpha (t)), \\dots, x_{n}(\\alpha (t))) \\\\ =\u0026amp;\\ (x_{1}(t), x_{2}(t), \\dots, x_{n}(t)) \\end{align*} $$\nこの時、$x_{i}$は$x_{i} : M \\to \\mathbb{R}$である関数であり、$x_{i}(t)$は$x_{i}(\\alpha (t))$を簡単に表記したものであることに注意する。\nこのように考えると、$f \\circ \\alpha$は二つの関数の合成であり、$\\mathbb{R} \\overset{\\mathbf{x}^{-1} \\circ \\alpha}{\\longrightarrow} \\mathbb{R}^{n} \\overset{f\\circ \\mathbf{x}}{\\longrightarrow} \\mathbb{R}$のようにマッピングされる。したがって、連鎖律により、次が成立する。\n$$ \\dfrac{d}{d t}(f \\circ \\alpha) = \\dfrac{d}{dt} \\left( (f\\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha) \\right) = \\sum \\limits_{i=1}^{n}\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}} \\dfrac{d (\\mathbf{x}^{-1} \\circ \\alpha )_{i}}{d t} = \\sum \\limits_{i=1}^{n}\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}} \\dfrac{d x_{i}}{d t} $$\nしたがって、次を得る。\n$$ \\begin{align*} \\alpha^{\\prime}(0) f :=\u0026amp;\\ \\dfrac{d}{dt} (f\\circ \\alpha)(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\dfrac{d x_{i}}{d t}(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} x_{i}^{\\prime}(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\end{align*} $$\nここで、$\\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0}$を次のようなオペレーターとして定義しよう。\n$$ \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} f := \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} $$\n$\\dfrac{\\partial f}{\\partial x_{i}}$の意味をまとめると、次のようになる。\n$f$は定義域が$M$であるため微分できない。したがって、座標系$\\mathbf{x} : \\mathbb{R}^{n} \\to M$との合成を通じて$f\\circ \\mathbf{x}$を考える。これは$\\mathbb{R}^{n}$を$\\mathbb{R}$にマッピングするため、古典的な意味で微分が可能である。したがって、$\\dfrac{\\partial f}{\\partial x_{i}}$は$f$を$\\mathbf{x}$と合成した後、これをユークリッド空間$\\mathbb{R}^{n}$の$i$番目の変数$u_{i}$に対して微分したものとして定義する。\n最終的に次を得る。\n$$ \\begin{align*} \\alpha^{\\prime}(0) f =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0}f = \\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial f}{\\partial x_{i}}\\right|_{t=0} \\end{align*} $$\n$$ \\implies \\alpha^{\\prime}(0) = \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} $$\n■\n関連項目 解析学での方向微分 微分幾何学での方向微分 Manfredo P. Do Carmo, Riemannian Geometry (英語版, 1992), p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3132,"permalink":"https://freshrimpsushi.github.io/jp/posts/3132/","tags":null,"title":"微分可能多様体上の接線ベクトル"},{"categories":"줄리아","contents":"コード XsDB_주거인구_100M_TM.shpというshpファイルを読み込むコードは以下の通りだ。\nusing Shapefile\rcd(@__DIR__)\rpath = \u0026#34;XsDB_주거인구_100M_TM.shp\u0026#34;\rtable = Shapefile.Table(path)\rusing DataFrames\rdf = DataFrame(table) もちろん、ファイルを読み込むだけではできることが限られており、データを調べるためにはデータフレームに変換する必要がある。\n実行結果 959660×16 DataFrame\rRow │ geometry MEGA_NM MEGA_CD CTY_NM CTY_CD X_AXIS Y_AXIS HOUS POP POP_10 POP_20 POP_30 POP_40 POP_50 POP_60_O \\xb9\\xe8\\xc6\\xf7ó │ Point…? String String String String Int64 Int64 Float64 Float64 Float64 Float64 Float64 Float64 Float64 Float64 String\r────────┼─────────────────────────────────────────────────────────────────────────────────────────────────\r1 │ Point(254298.0, 4.26549e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 365950 526450 10.08 24.56 4.64 1.92 1.84 4.72 4.32 7.12 biz-gis.com\r2 │ Point(2.59622e5, 4.24405e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 371250 524250 1.42 3.47 0.81 0.29 0.52 0.52 0.59 0.74 biz-gis.com\r3 │ Point(2.61134e5, 423221.0) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 372750 523050 1.26 3.08 0.68 0.28 0.35 0.49 0.45 0.83 biz-gis.com\r4 │ Point(2.50311e5, 4.15806e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 361850 515750 10.08 25.2 3.68 2.96 1.68 4.4 6.0 6.48 biz-gis.com\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮\r959658 │ Point(2.09955e5, 2.46768e5) \\xc0\\xfc\\xb6\\xf3\\xbaϵ\\xb5 45 \\xbf\\xcf\\xc1ֱ\\xba 45710 319750 347150 1.83 4.53 1.92 0.24 0.45 0.72 0.69 0.51 biz-gis.com\r959659 │ Point(215588.0, 4.55344e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xb3\\xb2\\xbe\\xe7\\xc1ֽ\\xc3 41360 327550 555650 2.38 5.31 0.91 0.74 0.57 0.97 1.05 1.07 biz-gis.com\r959660 │ Point(2.54717e5, 4.24754e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 366350 524650 1.26 3.07 0.58 0.24 0.23 0.59 0.54 0.89 biz-gis.com\r959653 rows omitted 環境 OS: Windows julia: v1.5.0 ","id":2097,"permalink":"https://freshrimpsushi.github.io/jp/posts/2097/","tags":null,"title":"ジュリアでSHPファイルを読む方法"},{"categories":"줄리아","contents":"## 概要\r`trunc`関数を使うには、第一引数に`Int`を入れるだけだ。\r## コード julia\u0026gt; @time for t in 1:10^8 Int64(ceil(t/1000)) end 0.189653 seconds\njulia\u0026gt; @time for t in 1:10^8 trunc(Int64, ceil(t/1000)) end 0.128472 seconds\n二つのループは全く同じ機能をするけど、1.5倍ほどの速度差がある。上は`ceil`で小数点以下を切り捨てて`Int64`に型キャストしたんだけど、下は`trunc`関数の内蔵機能を使ってネイティブに整数を返すから、より速いんだ。\r他の言語を使ってた人には、上のループみたいなストレートな命令が自然に感じるだろうけど、Juliaにはデータ型を第一引数に入れて結果を返すような内蔵関数が多いから、下のループみたいな使い方の方が慣れると思うよ。\r## 環境\r- OS: Windows\r- julia: v1.5.0 ","id":2095,"permalink":"https://freshrimpsushi.github.io/jp/posts/2095/","tags":null,"title":"ジュリアで小数点以下を切り捨てて整数に変換する方法"},{"categories":"줄리아","contents":"概要 rename!() 関数を使って変更するといい1。\n文字列のリストを与えて一度に変更する方法もあるし、個別に変更する方法もある。\nコード using DataFrames\rdf = DataFrame(rand(1:9, 10, 3), :auto)\rrename!(df, [\u0026#34;X\u0026#34;, \u0026#34;Y\u0026#34;, \u0026#34;Z\u0026#34;])\rrename!(df, :X =\u0026gt; :A) 実行すると、最初に次のようなデータフレームが生成される。\njulia\u0026gt; df = DataFrame(rand(1:9, 10, 3), :auto)\r10×3 DataFrame\rRow │ x1 x2 x3 │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 一度に変更する方法 julia\u0026gt; rename!(df, [\u0026#34;X\u0026#34;, \u0026#34;Y\u0026#34;, \u0026#34;Z\u0026#34;])\r10×3 DataFrame\rRow │ X Y Z │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 文字列のリストを与えればいい。\n一つずつ変更する方法 julia\u0026gt; rename!(df, :X =\u0026gt; :A)\r10×3 DataFrame\rRow │ A Y Z │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 他の言語ではあまり見られない方法だが、列名の前に : を付けて、=\u0026gt; でマッピングする。ジュリアで : で始まる変数はシンボルSymbolだ。\n環境 OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/change-column-names-of-a-dataframe-previous-methods-dont-work/48026/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2093,"permalink":"https://freshrimpsushi.github.io/jp/posts/2093/","tags":null,"title":"ジュリアでデータフレームの列名を変更する方法"},{"categories":"줄리아","contents":"概要 $n$個の座標同士の距離を計算するにあたり、行列を作る必要はなく、単に距離を計算する場合、多次元検索に有利なデータ構造であるk-dツリー1を使用して速度を上げることができます。NearestNeighbors.jlに関連するアルゴリズムがすべて実装されているので、公式GitHubページを参照してください。\n速度比較 pairwise()関数で距離行列の計算に最適化された技術と比較してみましょう。\nusing Distances\rusing StatsBase\rusing Random\rusing NearestNeighbors\rRandom.seed!(0);\rε = 0.01\rN = 10^4\rcoordinate = rand(2, N);\rstate = sample([\u0026#39;S\u0026#39;, \u0026#39;I\u0026#39;], Weights([0.1, 0.9]), N);\rS = coordinate[:, state .== \u0026#39;S\u0026#39;]\rI = coordinate[:, state .== \u0026#39;I\u0026#39;]\r@time sum(pairwise(Euclidean(),S,I) .\u0026lt; ε, dims = 1)\r@time kdtree = KDTree(S); contact = length.(inrange(kdtree, I, ε, true)) 上記のコードを実行した結果は以下の通りです。一番下の二つのコマンドラインは同じ作業を行いますが、速度差は約500倍になります。実際、k-dツリーで検索する際の時間複雑度は$\\log n$で非常に効率的です。\njulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; ε, dims = 1)\r0.098394 seconds (14 allocations: 69.639 MiB, 8.23% gc Time)\r1×9004 Array{Int64,2}:\r0 0 1 0 0 0 0 … 1 0 0 0 0 0\rjulia\u0026gt; @time kdtree = KDTree(S); contact = length.(inrange(kdtree, I, ε, true))\r0.000213 seconds (22 allocations: 51.609 KiB)\r9004-element Array{Int64,1}:\r0\r0\r1\r0\r⋮\r0\r0\r0 単純な速度の問題を置いておいても、k-dツリーを使用した方法では1次元の配列を返すため、結果物を使用する側面でもさらに便利です。\n環境 OS: Windows julia: v1.6.2 https://en.wikipedia.org/wiki/K-d_tree\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2088,"permalink":"https://freshrimpsushi.github.io/jp/posts/2088/","tags":null,"title":"JuliaでNearstNeighbors.jlを使用して距離を素早く計算する方法"},{"categories":"기하학","contents":"定義1 $M$を任意の集合、$U_{\\alpha} \\subset \\mathbb{R}^{n}$を開集合とする。関数$1-1$ $\\mathbf{x}_{\\alpha} : U_{\\alpha} \\to M$に対して、以下の条件を満たす順序対$\\left( M, \\left\\{ \\mathbf{x}_{\\alpha} \\right\\}_{\\alpha\\in \\mathscr{A}} \\right)$、または簡単に$M$を**$n$次元の微分可能多様体**dimension $n$の differentiable manifoldと定義する。\n$\\bigcup \\limits_{\\alpha} \\mathbf{x}_{\\alpha} \\left( U_{\\alpha} \\right) = M$ $\\varnothing \\ne W = \\mathbf{x}_{\\alpha}\\left( U_{\\alpha} \\right) \\cap \\mathbf{x}_{\\beta}\\left( U_{\\beta} \\right)$に対して、写像$\\mathbf{x}_{\\beta}^{-1} \\circ \\mathbf{x}_{\\alpha} : \\mathbf{x}_{\\alpha}^{-1}(W) \\to \\mathbf{x}_{\\beta}^{-1}(W)$が微分可能であること。 条件1および2を満たす可能なすべての$\\alpha$に対して、指数族$\\left\\{ \\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right) \\right\\}$を構成する。 説明 単に微分多様体または滑らかな多様体とも言う。$n$次元の微分多様体は、時に$M^{n}$と表記する。\n$p \\in \\mathbf{x}_{\\alpha}(U_{\\alpha})$のとき、$\\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right)$または単に$\\mathbf{x}_{\\alpha}$を$M$の$p$における座標系$M$ at $p$の system of coordinates、局所座標系、またはパラメータ化と呼ぶ。\n$\\mathbf{x}_{\\alpha}(U_{\\alpha})$を$p \\in M$における座標近傍と呼ぶ。\n条件3.の指数族$\\left\\{ \\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right) \\right\\}$を$M$上の微分可能構造と呼ぶ。\n$p \\in M$に対して、$\\mathbf{x}_{\\alpha}^{-1}(p) = \\left( x_{1}(p), \\dots, x_{n}(p) \\right)$を満たす$x_{i}$を座標関数と呼ぶ。\n$M$は完全に任意の集合として与えられる(つまり、通常は距離空間ではない)ので、$\\mathbf{x}_{\\alpha}$が微分可能かどうかについての議論ができない。さらに、$M$は様々なイメージの合併であるため、各交差点$W = \\mathbf{x}_{\\alpha}\\left( U_{\\alpha} \\right) \\cap \\mathbf{x}_{\\beta}\\left( U_{\\beta} \\right)$で適切に良い条件が必要であり、ここではそれを微分可能であるという条件で与えられている。\n写像$\\mathbf{x}_{\\beta}^{-1} \\circ \\mathbf{x}_{\\alpha}$の条件によって、多様体が様々な名前で呼ばれることになる。例えば、微分の代わりに連続であるという条件が与えられた場合、$M$は位相多様体になる。正則という条件が与えられた場合、$M$は複素多様体になる。また、$\\mathbf{x}_{\\beta}^{-1} \\circ \\mathbf{x}_{\\alpha} \\in C^{k}$の場合、$M$は$C^{k}$多様体と呼ばれる。微分幾何学では、微分という道具を使って幾何学を説明したいため、微分可能な多様体が扱われる。\nこの内容は技術的な部分であり、二つの微分可能構造が同じか違うか等の話を避けるために存在する条件である。1および2を満たすそのようなものすべてが集められていると仮定した方が良い。「こんなのはどうだ?」「これも含まれるか?」といったタックルをかけないで欲しいという意味である。\n例 ユークリッド空間 $\\mathbb{R}^{n}$ $$ \\mathbb{R}^{n} = \\left\\{ (x_{1}, x_{2}, \\dots, x_{n}) : x_{i} \\in \\mathbb{R} \\right\\} $$\n多様体が局所的にユークリッド空間と似ているため、$\\mathbb{R}^{n}$が微分可能な多様体であることは自然なことである。${\\rm id}$を恒等作用素とする。\n微分可能構造を$\\left\\{ \\left( U_{\\alpha}, {\\rm id} \\right) | U_{\\alpha} \\subset \\mathbb{R}^{n} \\text{ is open.} \\right\\}$のようにすると成立する。\n恒等作用素は微分可能なので成立する。\nこのようなすべての順序対に対して、指数族$\\left\\{ \\left( U_{\\alpha}, {\\rm id} \\right)\\right\\}$を構成する。\nしたがって、$\\left( \\mathbb{R}^{n}, \\left\\{ {\\rm id} \\right\\} \\right)$は微分可能な多様体である。\n2次元球面 $\\mathbb{S}^{2}$ $$ \\mathbb{S}^{2} = \\left\\{ p \\in \\mathbb{R}^{3} : \\left\\| p \\right\\|=1 \\right\\} $$\n2次元球面は、以下のように6つの座標パッチで表現できる。$(u,v) \\in U = \\left\\{ (u,v) : u^{2} + v^{2} \\lt 1 \\right\\}$について、\n座標パッチ 定義 逆 $\\mathbf{x}_{1} = \\mathbf{x}_{(0,0,1)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,0,1)}(u, v) = \\left( u, v , \\sqrt{1- u^{2} -v^{2} } \\right)$ $\\mathbf{x}_{(0,0,1)}^{-1}(x, y, z) = (x,y)$ $\\mathbf{x}_{2} = \\mathbf{x}_{(0,0,-1)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,0,-1)}(u, v) = \\left( u, v , -\\sqrt{1- u^{2} -v^{2} } \\right)$ $\\mathbf{x}_{(0,0,-1)}^{-1}(x, y, z) = (x,y)$ $\\mathbf{x}_{3} = \\mathbf{x}_{(0,1,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,1,0)}(u, v) = \\left( u, \\sqrt{1- u^{2} -v^{2}}, v \\right)$ $\\mathbf{x}_{(0,1,0)}^{-1}(x, y, z) = (x,z)$ $\\mathbf{x}_{4} = \\mathbf{x}_{(0,-1,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,-1,0)}(u, v) = \\left( u, -\\sqrt{1- u^{2} -v^{2}}, v \\right)$ $\\mathbf{x}_{(0,-1,0)}^{-1}(x, y, z) = (x,z)$ $\\mathbf{x}_{5} = \\mathbf{x}_{(1,0,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(1,0,0)}(u, v) = \\left( \\sqrt{1- u^{2} -v^{2}}, u, v \\right)$ $\\mathbf{x}_{(1,0,0)}^{-1}(x, y, z) = (y,z)$ $\\mathbf{x}_{6} = \\mathbf{x}_{(-1,0,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(-1,0,0)}(u, v) = \\left( -\\sqrt{1- u^{2} -v^{2}}, u, v \\right)$ $\\mathbf{x}_{(-1,0,0)}^{-1}(x, y, z) = (y,z)$ $\\bigcup \\limits_{i=1}^6 \\mathbf{x}_{i} = \\mathbb{S}^{2}$が成立する。\n$\\mathbf{x}_{(0,0,1)}^{-1} \\circ \\mathbf{x}_{(1,0,0)}$は次のようになるため、微分可能である。\n$$\\mathbf{x}_{(0,0,1)}^{-1} \\circ \\mathbf{x}_{(1,0,0)}(u,v) = \\mathbf{x}_{(0,0,1)}^{-1} \\left( \\sqrt{1- u^{2} -v^{2}}, u, v \\right) = \\left( \\sqrt{1- u^{2} -v^{2}}, u \\right) \\in C^{\\infty}$$\nこの方法で1, 2を満たすすべての順序対を集め、指数族$\\left\\{ \\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right) \\right\\}$を構成する。 したがって、$\\left( \\mathbb{S}^{2} , \\left\\{ \\mathbf{x}_{\\alpha} \\right\\} \\right)$は微分可能な多様体である。\nManfredo P. Do Carmo, Riemannian Geometry (英語版, 1992), p2-3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3116,"permalink":"https://freshrimpsushi.github.io/jp/posts/3116/","tags":null,"title":"微分可能な多様体"},{"categories":"줄리아","contents":"コード using CSV, DataFrames\rA = rand(1:10, 10)\rB = zeros(10)\rAB = DataFrame(hcat(A,B), [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;])\rCSV.write(\u0026#34;AB.csv\u0026#34;, AB) CSVパッケージのwrite関数を通じて簡単に2次元配列を出力できる。A, Bは1次元配列で、hcat関数で束ねてデータフレームに変換した。\n実行結果 julia\u0026gt; using CSV, DataFrames\rjulia\u0026gt; A = rand(1:10, 10)\r10-element Array{Int64,1}:\r8\r5\r4\r3\r6\r4\r10\r6\r2\r9\rjulia\u0026gt; B = zeros(10)\r10-element Array{Float64,1}:\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\rjulia\u0026gt; AB = DataFrame(hcat(A,B), [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;])\r10×2 DataFrame\rRow │ A B │ Float64 Float64\r─────┼──────────────────\r1 │ 8.0 0.0\r2 │ 5.0 0.0\r3 │ 4.0 0.0\r⋮ │ ⋮ ⋮\r9 │ 2.0 0.0\r10 │ 9.0 0.0\r5 rows omitted\rjulia\u0026gt; CSV.write(\u0026#34;AB.csv\u0026#34;, AB)\r\u0026#34;AB.csv\u0026#34; 以下は実際に出力されたcsvファイルだ。\n一緒に見る CSV 出力時に文字化け解決法 CSV.write(bom = true) 環境 OS: Windows julia: v1.6.3 ","id":2073,"permalink":"https://freshrimpsushi.github.io/jp/posts/2073/","tags":null,"title":"ジュリアで2次元配列をCSVファイルに出力する方法"},{"categories":"줄리아","contents":"概要 ジュリアでは、変数名にユニコード(UTF-8)を許可している。だから、ギリシャ文字はもちろん、上付き文字、下付き文字、さらには韓国語や絵文字まで使える。わざわざ使う必要はないが、次のような奇妙なコードもちゃんと動く。\njulia\u0026gt; α₁ = 2\r2\rjulia\u0026gt; α₂ = 1\r1\rjulia\u0026gt; println(α₁ \\ast\\ α₂)\r2\rjulia\u0026gt; 사인(t) = sin(t)\r사인 (generic function with 1 method)\rjulia\u0026gt; 😂 = 1:20\r1:20\rjulia\u0026gt; 사인.(😂)\r20-element Array{Float64,1}:\r0.8414709848078965\r0.9092974268256817\r⋮\r0.14987720966295234\r0.9129452507276277 ギリシャ文字 texで使うギリシャ文字を全部、上のような方法で使える。\n上付き文字、下付き文字 上付き文字と下付き文字は\\_、\\^の後に数字を入力して使う。あまりに小さすぎてよく使われないが、ギリシャ文字や英語、括弧も使える。\n絵文字 絵文字は他のエディタと同じようにwindow + .(cmd + コンマ)を押して入力できる。\n","id":2065,"permalink":"https://freshrimpsushi.github.io/jp/posts/2065/","tags":null,"title":"ジュリア変数名にグリーク文字と添え字を書く方法"},{"categories":"기하학","contents":"定義 1 写像 $\\alpha : (a,b) \\to \\mathbb{R}^{3}$ を曲線Curveと呼ぶ。 $\\alpha^{\\prime} = \\dfrac{d \\alpha}{d t} = \\mathbf{0}$ の時の点 $t = t_{0}$ を特異点Singular Pointと言う。 ある $k \\in \\mathbb{N}$ に対して、全ての $t \\in (a,b)$ で $\\displaystyle {{ d \\alpha } \\over { d t }} \\ne \\mathbf{0}$ となる曲線 $\\alpha \\in C^{k}$ を正則曲線Regular Curveと呼ぶ。つまり、正則曲線とは特異点がない曲線のことだ。 曲線 $\\alpha$の $t=t_{0}$ での微分係数 $\\alpha^{\\prime}(t_{0})$を$t = t_{0}$ の時の$\\alpha$ の速度(ベクトル)velocity vectorと呼び、$\\alpha$の導関数 $\\alpha^{\\prime}$を$\\alpha$の速度ベクトル場velocity vector fieldと言う。従って、正則曲線は速度が$\\mathbf{0}$にならない曲線のことを言い、物理的に見た時、進行方向が絶対に変わらないことを意味する。 $t = t_{0}$ の時の $\\alpha$ の速度の大きさ $\\left|\\alpha^{\\prime}(t_{0}) \\right|$を速さspeedと呼ぶ。 $\\left| \\alpha^{\\prime} \\right| = 1$ の曲線を$\\alpha : (a,b) \\to \\mathbb{R}^{3}$ を単位速度曲線Unit Speed Vectorと言う。 $C^{k}$ は $k$ 回微分可能でその導関数が連続関数の集合である。 説明 $$ \\alpha (t) := \\left( \\alpha_{1} (t) , \\alpha_{2} (t) , \\alpha_{3} (t) \\right) $$\n幾何学で扱いたい対象は図形で、定義ではその図形をパラメーター $t$ に対する関数として表していることに注意しろ。これにより、多くの数学的なツールを使って図形を研究することができるようになり、特に、微分幾何学では多くの微積分が使用されるだろう。\n特異点とは、簡単に言えば曲がっているか停止している点のことだ。曲がっている点では、見方によっては2つの方向が出てくることがある。このような点は扱いにくいので、学部レベルの微分幾何学では扱わない。\u0026lsquo;停止する点\u0026rsquo;は例で説明する。\nある $k \\in \\mathbb{N}$ に対して、$\\alpha \\in C^{k}$ ということは、少なくとも一度は微分可能であることを強く意味し、実際に何回微分可能かはあまり重要ではない。通常、$k=1$ 回だけであっても、単にスムースSmoothであると言われる。\n例 直線 $$ l(t) := \\left( t, t, t \\right) $$ 定義によれば、直線 $l : \\mathbb{R} \\to \\mathbb{R}^{3}$ が曲線でない理由は全くない。韓国語では、曲がることを意味する曲曲のため、何かが曲がっているというニュアンスが混乱を招くかもしれないが、そのまま英語の発音でカーブと読んだ方が良いかもしれない。\n螺旋 $$ \\zeta (t) := \\left( \\cos t , \\sin t , t \\right) $$\n$0 \\to t \\to \\infty$ によると、螺旋は以下のように描かれる。\n不規則曲線 $$ \\beta (t) := \\left( t^{2} , t^{3} , t^{4} \\right) $$ 上記の曲線 $\\beta$ を微分すると、 $$ \\beta^{\\prime} (t) := \\left( 2t , 3t^{2} , 4t^{3} \\right) $$ その結果、$t = 0$ で $\\displaystyle {{ d \\beta } \\over { d t }} (0) = \\mathbf{0}$ になる。この特異点は曲がっていないが$t$ を沿って進んでいると $t=0$ で文字通り停止する。従って、$\\beta$ の定義域が $\\mathbb{R}$ なら正則曲線ではない。もちろん、定義域が $0$ を含まない範囲、例えば、$(0,\\infty)$ なら正則曲線だ。定義域によって正則曲線であったり、そうでなかったりすることに注意しよう。\nコード 以下は、螺旋の例で見た動画をJuliaで作成するコードだ。\nusing Plots\rζ(t) = (cos(t), sin(t), t)\ranim = @animate for T ∈ 0.1:0.1:10\rt = 0:0.1:(T*π)\rhelix = plot(ζ.(t), camera = (45,45), legend = :none)\rxlims!(-2,2); ylims!(-2,2); zlims!(0,40)\rend\rgif(anim, \u0026#34;helix.gif\u0026#34;) Millman. (1977). Elements of Differential Geometry: p15.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2062,"permalink":"https://freshrimpsushi.github.io/jp/posts/2062/","tags":["줄리아"],"title":"曲線の定義"},{"categories":"수리통계학","contents":"定義 数式的な定義 1 パラメータ$\\theta \\in \\Theta$に対するランダムサンプル$X_{1} , \\cdots , X_{n}$の確率質量/密度関数を$f(x;\\theta)$、統計量$Y_{1} := u_{1} \\left( X_{1} , \\cdots , X_{n} \\right)$の確率質量/密度関数を$f_{Y_{1}} \\left( y_{1}; \\theta \\right)$とする。\n$\\theta \\in \\Theta$に依存しない$H \\left( x_{1} , \\cdots , x_{n} \\right)$に対して $$ {{ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) } \\over { f_{Y_{1}} \\left( u_{1} \\left( x_{1} , \\cdots, x_{n} \\right) ; \\theta \\right) }} = H \\left( x_{1} , \\cdots , x_{n} \\right) $$ であれば、$Y_{1}$を$\\theta$に対する十分統計量という。\n一般的な定義 2 統計量$T(\\mathbf{X})$が、与えられたサンプルの$\\mathbf{X}$条件付き確率分布がパラメータ$\\theta$に依存しない場合、$T(\\mathbf{X})$を$\\theta$に対する十分統計量という。\n説明 定義の数式が意味するのは、直感的に見ると、分子と分母で$\\theta$がキャンセルされること―つまり十分統計量$Y_{1}$が、ランダムサンプル$X_{1} , \\cdots , X_{n}$の情報を正確に保持しているという意味になるだろう。十分統計量の「十分」とは、$\\theta$に関する情報が「十分」に与えられていると受け取れば良く、十分統計量を除いた後は、$\\theta$に関する情報が全く残ってはいけない。\n十分統計量を理解するために、以下の定理を用いよう。\nネイマン分解定理: ランダムサンプル$X_{1} , \\cdots , X_{n}$が、パラメータ$\\theta \\in \\Theta$に対して同じ確率質量/密度関数$f \\left( x ; \\theta \\right)$を持つとする。統計量$Y = u_{1} \\left( X_{1} , \\cdots , X_{n} \\right)$が$\\theta$の十分統計量であるためには、次を満たす非負の二つの関数$k_{1} , k_{2} \\ge 0$が存在することである。 $$ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) = k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) $$ ただし、$k_{2}$は$\\theta$に依存してはならない。\n響かない例 $$ X_{1} , \\cdots , X_{n} \\sim N \\left( \\mu , \\sigma^{2} \\right) $$\n経験的に、十分統計量は、なぜそんなものを計算するのか、理解することから始める必要がある。典型的に響かない例として、正規分布$N \\left( \\mu , \\sigma^{2} \\right)$の母平均$\\mu$に対する十分統計量を見ることだ。分解定理によれば、$\\mu$の十分統計量は $$ \\begin{align*} \\prod_{k=1}^{n} f \\left( x_{k} ; \\mu \\right) =\u0026amp; \\prod_{k=1}^{n} {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\exp \\left( - {{ \\left( x_{i} - \\mu \\right)^{2} } \\over { 2 \\sigma^{2} }} \\right) \\\\ =\u0026amp; \\left( {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\right)^{n} \\exp \\left( - \\sum_{k=1}^{n} {{ \\left( x_{i} - \\mu \\right)^{2} } \\over { 2 \\sigma^{2} }} \\right) \\\\ =\u0026amp; \\left( {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\right)^{n} \\exp \\left( - \\sum_{k=1}^{n} {{ x_{i}^{2} } \\over { 2 \\sigma^{2} }} \\right) \\exp \\left( - \\sum_{k=1}^{n} {{ \\left( 2 x_{i} - \\mu^{2} \\right) } \\over { 2 \\sigma^{2} }} \\right) \\\\ =\u0026amp; \\left( {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\right)^{n} \\exp \\left( - \\sum_{k=1}^{n} {{ x_{i}^{2} } \\over { 2 \\sigma^{2} }} \\right) \\cdot \\exp \\left( - {{ 1 } \\over { \\sigma^{2} }} \\sum_{k=1}^{n} x_{i} + {{ n(\\mu/\\sigma)^{2} } \\over { 2 \\ }} \\right) \\\\ =\u0026amp; k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\cdot k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\mu \\right] \\end{align*} $$ であり、サンプル和$\\sum_{k=1}^{n} X_{k}$であろうと、分子分母に$n$を掛けてサンプル平均$\\overline{X}$になろうと、関係ない。直感に従って、$\\mu$の十分統計量として、その不偏推定量であり、一致推定量であり、最尤推定量でもあるサンプル平均が出てきたのは良い。数式的には理解できる。しかし、それが一体何を意味するのか、感じることは難しいだろう。\n響く例 $$ X_{1} , \\cdots , X_{n} \\sim U (0,\\theta) \\text{ with } f \\left( x ; \\theta \\right) = \\begin{cases} 1 \u0026amp; , \\text{if } x \\in (0,\\theta) \\\\ 0 \u0026amp; , \\text{otherwise} \\end{cases} = {{ 1 } \\over { \\theta }} I_{(0,\\theta)} (x) $$\n例えば、最大値のパラメータが$\\theta$である一様分布から得られたランダムサンプルを考えてみよう。実際の実現が $$ \\begin{bmatrix}2.3 \\\\ 1.2 \\\\ 1.7 \\\\ 0.1 \\\\ 1.1\\end{bmatrix} $$ であり、これ以上のサンプルを得られない場合、一様分布$U(a,b)$の母平均が${{ b+a } \\over { 2 }}$であるため、次のような推定量を考えることができる。 $$ {{ \\hat{\\theta} + 0 } \\over { 2 }} = {{ \\sum_{k} x_{k} } \\over { n }} \\implies \\hat{\\theta} \\overset{?}{=} {{ 2 \\sum_{k} x_{k} } \\over { n }} $$ 数理統計学的にそんなに悪くない推測のようだ。実際、このデータで計算されたサンプル平均の$2$倍は$2.16$で、かなりもっともらしい。しかし、$2.3$がサンプルにあることを考えると、$\\theta = 2.16$であるはずがない。どう考えても、$\\theta$は$2.3$以上でなければならず、直感的に見て、$\\theta$に対する合理的な推定は、単純に$\\hat{\\theta} = 2.3$になる。今のサンプルを見たとき、$2.3$より大きいと考える理由が全くないからだ。さあ、実際に十分統計量を探してみよう。\n指示関数の積: $$ \\prod_{i=1}^{n} I_{(-\\infty, \\theta]} \\left( x_{i} \\right) = I_{(-\\infty, \\theta]} \\left( \\max_{i \\in [n]} x_{i} \\right) $$\nこの補題と分解定理により考えると、$\\theta$に対する十分統計量は $$ \\begin{align*} \\prod_{k=1}^{n} f \\left( x_{k} ; \\mu \\right) =\u0026amp; \\prod_{k=1}^{n} {{ 1 } \\over { \\theta }} I_{(0,\\theta)} \\left( x_{k} \\right) \\\\ = \u0026amp; {{ 1 } \\over { \\theta^{n} }} I_{(0,\\theta)} \\left( \\max x_{k} \\right) \\cdot 1 \\\\ = \u0026amp; k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\end{align*} $$ であるため、サンプルの最大値$\\max_{k} X_{k} = X_{(n)}$が十分となる。これが意味するのは、$\\theta$に関する情報を考えるとき、他のサンプルは必要なく、$\\max_{k} X_{k}$だけを考えれば「十分」であるということだ。\nこのアイデアは、データをたくさん引き出してパラメータを推定し、それをどこかに近似する考え方とは全く異なる。これは、直感的な推測に対して、数学と形式でアプローチする統計的推論であり、これを通じて、統計学のさらに深い世界に入れる。\n最小十分統計量 響く例で、$\\max_{k} X_{k}$が$\\theta$に対する十分統計量であることを直感と照らし合わせて確認した。これ以上の十分統計量はないと見えるが、最小十分統計量に関する議論がその答えとなるだろう。\nHogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p391.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCasella. (2001). Statistical Inference(2nd Edition): p272.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2061,"permalink":"https://freshrimpsushi.github.io/jp/posts/2061/","tags":null,"title":"十分統計量"},{"categories":"머신러닝","contents":"この文は逆転派アルゴリズムの原理を数学専攻者が理解しやすいように作成された。\n表記法 上図のような 人工ニューラルネットワーク が与えられたとする。$\\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n_{0}})$は入力input、 $y_{j}^{l}$は$l$番目の層の$j$ノード、$\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}})$ドルは出力outputである。\n$L \\in \\mathbb{N}$は、隠匿層hidden layerの個数であり、$\\mathbf{n}=(n_{0}、n_{1}、\\dots、n_{L}、\\hat{n}) \\in \\mathbb{N}^{N}=(n)、$の成分は順に入力層、$L$個の隠匿層と出力層のノード数を意味する。 また、便宜上、$0$番目の隠匿層は入力層を意味し、$L+1$番目の隠匿層は出力層を意味するとする。\n$w_{ji}^{l}$は、$l$の次の層の$i$のノードとその次の層の$j$のノードを連結する加重値を表す。 すると、各階から次の階への伝播は、以下のGIFのように起こる。\nここで $\\phi$ は任意の活性化関数 である。 $l$ 番目の層から次の層の $j$ 番目のノードに伝達される線形結合を $v_{i}^{l}$で表記しよう。\n$$ \\begin{align*} v_{j}^{l} \u0026amp;= \\sum _{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\\\ y_{j}^{l+1} \u0026amp;= \\phi ( v_{j}^{l} ) = \\phi \\left( \\sum \\nolimits_{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\right) \\end{align*} $$\nこれを定理すると次のようになる。\n記号 意味 $\\mathbf{x}=(x_{1}, x_{2}, \\dots, x_{n_{0}})$ 入力 $y^{l}_{j}$ $l$ 番目の層の $j$ 番目のノード $\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}} )$ 出力 $n_{l}$ $l$ 番目の層のノード数 $w_{ji}^{l}$ $l$ 番目の層の $i$ 番目のノードと その次の層の $j$ 番目のノードを接続する重み付け $\\phi$ 活性化関数 $v_{j}^{l} = \\sum \\limits _{i=1} ^{n_{l}} w_{ji}^{l}y_{i}^{l}$ 線形結合 $y^{l+1}_{j} = \\phi (v_{j}^{l})$ $l$ 番目の階から次の階への 電波 定理 $E = E(\\hat{\\mathbf{y}})$を微分可能な適切な損失関数とする。 それでは、$E$を最適化する方法は、各層での加重値$w_{ji}^{l}$を次のようにアップデートするものである。\n$$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} \\label{thm} \\end{equation} $$\nこの時、$\\alpha$は学習率で、$\\delta_{j}^{l}$ は以下の通りである。\n$l=L$の時、\n$$ -\\delta_{j}^{L} = \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\n$l \\in \\left\\{ 0,\\dots, L-1 \\right\\}$の時、\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i=1}^{n_{l}} \\delta_{i}^{l+1} w_{i j}^{l+1} $$\n説明 $(1)$を見てみよう。 $l$番目の層と$l+1$番目の層の間の加重値を更新する時、$l$番目のノードの$y_{j}^{l}$に依存するということですが、各層の出力に応じて最終的に出力$\\hat{\\mathbf{y}}$が決定されるので当然と見ることができる。 また、$y_{j}^{l}$は$l$番目から$l+1$番目の層に伝播される時の入力と見ることができるが、これは線形回帰モデルでLMSLeast Mean Squaresで学習する方法と似ている。\n$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha (\\mathbf{w}^{T}\\mathbf{x} - \\mathbf{y}) \\mathbf{x} $$\n一方、各層での出力$y_{j}^{l}$は入力層から出力層として計算される反面、最適化のための$\\delta_{j}^{l}$ は次のように出力層から入力層に逆に計算されるため、このような最適化手法を逆伝播アルゴリズムback propagation algorithmという。\n$$ \\begin{align*} \\delta_{j}^{L} \u0026amp;= - \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} \\\\ \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{j}^{L} w_{ij}^{L} \\\\ \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\\\ \\delta_{j}^{L-3} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-3}) \\sum _{i} \\delta_{i}^{L-2} w_{ij}^{L-2} \\\\ \u0026amp;\\vdots \\\\ \\delta_{j}^{1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{1}) \\sum _{i} \\delta_{i}^{2} w_{ij}^{2} \\\\ \\delta_{j}^{0} \u0026amp;= \\phi ^{\\prime} (v_{j}^{0}) \\sum _{i} \\delta_{i}^{1} w_{ij}^{1} \\end{align*} $$\n証明 入力層から出力層への計算が終わったとする。 加重値を損失関数$E$が減る方向に修正する方法は傾斜下降法を使えば次のようになる。\n$$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} - \\alpha \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l} } \\label{gradesent} \\end{equation} $$\nそれぞれの$y_{i}^{l}$は与えられた値なので、偏微分部分を計算できる形で解くことができる。 右辺の偏微分は連鎖法則によって次のようになる。\n$$ \\begin{equation} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}}) }{\\partial v_{j}^{l}} \\dfrac{\\partial v_{j}^{l}}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial v_{j}^{l}} y_{i}^{l} \\label{chainrule} \\end{equation} $$\n$(3)$の右辺の偏微分を$-\\delta_{j}^{l}$ とすると、$(2)$ から $(1)$ を得る。\n$$ w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} $$\n各層で $\\delta_{j}^{l}$ を次のように求める。\n$l=L$の場合\n$j \\in \\left\\{ 1, \\dots, \\hat{n} \\right\\}$ に対して次が成立する。\n$$ \\begin{equation} -\\delta_{j}^{L} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial \\hat{y}_{j}} \\dfrac{d \\hat{y}_{j}}{d v_{j}^{L}} \\label{deltamL} \\end{equation} $$\nこの時、$\\hat{y}_{j} =\\phi (v_{j}^{L})$ であるから次を得る。\n$$ -\\delta_{j}^{L} (t) =\\phi ^{\\prime} (v_{j}^{L}(t)) \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\n■\n$l=L-1$の場合\n$j \\in \\left\\{ 1, \\dots, n_{L-1} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{L-1} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-1}} = = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L}} \\dfrac{d y_{j}^{L}}{d v_{j}^{L-1}} $$\nこの時$y_{j}^{L} =\\phi (v_{j}^{L-1})$ であるので、次を得る。\n$$ -\\delta_{j}^{L-1} = = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\dfrac{\\partial y_{j}^{L}}{\\partial v_{j}^{L-1}} = = \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} $$\n右辺の偏微分は連鎖法則によって次のように計算される。\n$$ \\begin{align*} -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L}} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L}} \\end{align*} $$\nここで $(4)$ と ${\\color{green}v_{i}^{L}=\\sum_{j}w_{ij}^{L}y_{j}^{L}}$ により、次を得る。\n$$ \\begin{align} \u0026amp;\u0026amp; -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i=1} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial v_{i}^{L}}} {\\color{green} \\dfrac{d v_{i}^{L}}{d y_{j^{L}}} } \\nonumber \\\\ \u0026amp;\u0026amp; \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{green} w_{ij}^{L} }\\nonumber \\\\ {}\\nonumber \\\\ \\implies \u0026amp;\u0026amp; \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ij}^{L} \\label{deltajL-1} \\end{align} $$\n■\n$l=L-2$の場合\n$j \\in \\left\\{ 1, \\dots, n_{L-2} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-2}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} $$\nこの時$y_{j}^{L-1} =\\phi (v_{j}^{L-2})$ であるから次を得る。\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} = \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} $$\n右辺の偏微分は連鎖法則によって次のように計算される。\n$$ \\begin{align*} -\\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{\\partial y_{k}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}} \\dfrac{\\partial v_{k}^{L-1}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}}} {\\color{red}\\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} } {\\color{green}\\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}}} {\\color{purple}\\dfrac{d v_{k}^{L-1}}{\\partial y_{j}^{L-1}}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{red} w_{ik}^{L}} {\\color{green} \\phi^{\\prime}(v_{k}^{L-1})} {\\color{purple} w_{kj}^{L-1}} \\end{align*} $$\nしたがって、次を得る。\n$$ \\delta_{j}^{L-2} = -\\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) w_{kj}^{L-1} $$\nこのとき、$(5)$ によって次が成立する。\n$$ \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) = \\phi^{\\prime}(v_{k}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} = \\delta_{k}^{L-1} $$\nしたがって、次を得る。\n$$ \\begin{align*} \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\delta_{k}^{L-1} w_{kj}^{L-1} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\end{align*} $$\n■\n一般化: $l \\in \\left\\{1, \\dots, L-1 \\right\\}$\n上記の結果に基づき、次のように一般化することができる。$j \\in \\left\\{ 1, \\dots, n_{l} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} $$\n右辺の偏微分を連鎖法則で解くと次のようになる。\n$$ \\begin{align*} \u0026amp;\\quad \\delta_{j}^{l} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{\\partial \\hat{y}_{i_{(1)}}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{\\partial y_{i_{(2)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{\\partial y_{i_{(3)}}^{L-1} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{j}^{l}} \\\\ \u0026amp; \\quad \\vdots \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{i_{(4)}}^{L-2}} \\cdots \\frac{d y_{i_{(L-l+1)}}^{l+1} }{d v_{i_{(L-l+1)}}^{l} } \\frac{\\partial v_{i_{(L-l+1)}}^{l} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} -\\delta_{i_{(1)}}^{L} w_{i_{(1)}i_{(2)}}^{L} \\phi^{\\prime}(v_{i_{(2)}}^{L-1}) w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\delta_{i_{(2)}}^{L-1}w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\delta_{i_{(3)}}^{L-2} w_{i_{(3)} i_{(4)}}^{L-2} \\cdots w_{i_{(L-l)} j}^{L} \\\\ \u0026amp;\\quad \\vdots \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\delta_{i_{(L-l)}}^{l+1} w_{i_{(l-l)} j}^{l} \\end{align*} $$\nしたがって、定理すると次のようになる。\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i} \\delta_{i}^{l+1} w_{ij}^{l+1} $$\n■\n","id":3077,"permalink":"https://freshrimpsushi.github.io/jp/posts/3077/","tags":null,"title":"逆伝播アルゴリズム"},{"categories":"줄리아","contents":"概要 Juliaの便利な機能である補間Interpolationについて説明する。補間をうまく使うと、出力文を簡単できれいに書くことができるので非常に便利だ。数値解析の補間法とは関係ないが、言葉の意味は通じる。数値解析の補間法に関連する機能はInterpolations.jlの使用法を参照してほしい。\nコード 使い方は非常に単純だ。以下のように文字列の中で変数の前にドル記号$를 붙이면 변수가 알아서 문자열처럼 읽힌다. 변수 그대로가 아닌 계산이 필요하면 굳이 밖에서 계산할 필요 없이 $()を書くだけでいい。\njulia\u0026gt; x = 12\r12\rjulia\u0026gt; y = -2\r-2\rjulia\u0026gt; println(\u0026#34;value of x, y: ▷eq2◁y\u0026#34;)\rvalue of x, y: 12, -2\rjulia\u0026gt; println(\u0026#34;value of x+y: $(x+y)\u0026#34;)\rvalue of x+y: 10 環境 OS: Windows julia: v1.5.0 ","id":2041,"permalink":"https://freshrimpsushi.github.io/jp/posts/2041/","tags":null,"title":"ジュリアで変数の値を便利に出力する方法、補間"},{"categories":"줄리아","contents":"ガイド ステップ0. julia 1.6 以上をインストール\nバージョン1.6からは、インストール過程で環境変数に追加できる。示されたオプションをチェックしてインストールすればいい。古いバージョンを使っている場合は、1.6以上にアップデートするか、以下の指示に従えばいい。\nステップ1. Juliaのインストールパスを確認\nJuliaのインストールパスを確認する。特にいじっていなければ、次のパスに保存されているはずだ。\nC:\\Users\\사용자명\\AppData\\Local\\Programs\\Julia x.x.x\\bin 普通、C:\\Users\\ユーザー名\\AppDataは隠しフォルダだから見えなくても焦らないで。\n該当するパスで、上のようにjulia.exeファイルがあるか確認する必要がある。ステップ3で使うためにパスをコピーしておこう。\nステップ2. 環境変数を編集\nウィンドウ+sを押すか、コントロールパネルで「システム環境変数の編集」を検索する。\n「環境変数(N)」をクリックする。\nユーザー変数ウィンドウでPathを探し、「編集(E)」をクリックする。\nステップ3. Juliaのパスを追加\n「新規作成(N)」か、最下行を押してステップ1でコピーしたパスを上のように入力し、OKを押して環境変数の編集を終了する。\nステップ4. 再起動\n再起動後、powershellなどでjuliaコマンドを実行するとJuliaが起動することを確認できる。\n環境 OS: Windows julia: v1.5.2 ","id":2036,"permalink":"https://freshrimpsushi.github.io/jp/posts/2036/","tags":null,"title":"WindowsのCMDとPowerShellでJuliaを使用する方法"},{"categories":"줄리아","contents":"## コード [^1]\r[^1]: https://docs.julialang.org/en/v1/manual/metaprogramming/\rJuliaでは[メタプログラミング](../1457)を言語レベルでサポートしている。これは文字列をそのままのコードとして読み込んで実行した結果だ。 julia\u0026gt; text = \u0026ldquo;f(x) = 2x + 1; f(2)\u0026rdquo; \u0026ldquo;f(x) = 2x + 1; f(2)\u0026rdquo;\njulia\u0026gt; code = Meta.parse(text) :($(Expr(:toplevel, :(f(x) = begin #= none:1 =# 2x + 1 end), :(f(2)))))\njulia\u0026gt; eval(code) 5\n- `Meta.Parse()`: 이 함수를 통해 입력된 문자열을 **표현식**\u0026lt;sup\u0026gt;Expression\u0026lt;/sup\u0026gt;으로 바꿔 반환한다.\r- `eval()`: 표현식을 **평가**\u0026lt;sup\u0026gt;Evaluate\u0026lt;/sup\u0026gt;한다. 위 예제코드에서는 $f(2)$ 가 실제로 평가되어 함숫값인 $5)\r## 環境\r- OS: Windows\r- julia: v1.5.0 ","id":2024,"permalink":"https://freshrimpsushi.github.io/jp/posts/2024/","tags":null,"title":"ジュリアのメタプログラミング"},{"categories":"줄리아","contents":"コード vec() 関数を使えばいい。\njulia\u0026gt; A = rand(0:9, 3,4)\r3×4 Array{Int64,2}:\r6 8 7 3\r2 9 3 2\r5 0 6 7\rjulia\u0026gt; vec(A)\r12-element Array{Int64,1}:\r6\r2\r5\r8\r9\r0\r7\r3\r6\r3\r2\r7 人間の目には、1次元配列と同じように見えるが、タイプ上では2次元配列でエラーが出るケースも、この方法で解決できる。次の2つの命令は完全に同じ配列に見えるが、$\\mathbb{N}^{10 \\times 1}$ 行列か $\\mathbb{N}^{10 }$ ベクトルかの違いがある。\njulia\u0026gt; b = rand(0:9, 10,1)\r10×1 Array{Int64,2}:\r4\r8\r0\r4\r7\r4\r4\r2\r4\r7\rjulia\u0026gt; vec(b)\r10-element Array{Int64,1}:\r4\r8\r0\r4\r7\r4\r4\r2\r4\r7 実際の flatten() 関数 実は Base.Iterators に、本当の名前がフラッテンの flatten() が実装されている。実行結果は以下の通りで、正直、使いたくないかもしれない。正確に言うと、配列を変えるというよりは、ループなどに入る時にイテレータIteratorsとして使う時に必要になるかもしれない。正直、必要ない。\njulia\u0026gt; c = rand(0:9, 3,3)\r3×3 Matrix{Int64}:\r7 7 4\r9 3 8\r4 4 5\rjulia\u0026gt; Iterators.flatten(c)\rBase.Iterators.Flatten{Matrix{Int64}}([7 7 4; 9 3 8; 4 4 5])\rjulia\u0026gt; vec(c)\r9-element Vector{Int64}:\r7\r9\r4\r7\r3\r4\r4\r8\r5 環境 OS: Windows julia: v1.5.0 ","id":2022,"permalink":"https://freshrimpsushi.github.io/jp/posts/2022/","tags":null,"title":"ジュリアで配列をフラット化する方法"},{"categories":"줄리아","contents":"結論 $n$ 個の座標間の距離を計算しようとする。\n全ての座標間を計算する必要がなければ、グループに分けて長方形の距離行列を作ればいい。 長方形の距離行列は pairwise() 関数で簡単かつ速く計算できる。 速度比較 例えば、SIRモデルに対して移動するエージェントベースのシミュレーションを行うと考えてみよう。元の時間計算量は $O \\left( n^{2} \\right)$ だが、$S$ と $I$ のグループに分けて計算すると、時間計算量は $O \\left( n_{S} n_{I} \\right)$ に大きく下がる。通常、病気の伝播は $S$ と $I$ 間の距離行列を計算して一定の半径 $\\varepsilon$ 内に入るかを判断し、どれだけ接触したかによって実装される。この作業で速度を比較してみよう。\nusing Distances\rusing StatsBase\rusing Random\rRandom.seed!(0);\rN = 10^4\rlocation = rand(2, N);\rstate = sample([\u0026#39;S\u0026#39;, \u0026#39;I\u0026#39;], Weights([0.1, 0.9]), N);\rS = location[:, state .== \u0026#39;S\u0026#39;]\rI = location[:, state .== \u0026#39;I\u0026#39;]\rfunction foo(S, I)\rcontact = Int64[]\rfor s in 1:996\rpush!(contact, sum(sum((I .- S[:,s]).^2, dims = 1) .\u0026lt; 0.1))\rend\rreturn contact\rend\r@time foo(S, I) もちろん、グループに分けて計算するという発想は、ジュリアだけでなくどんな手法を使っても性能向上をもたらす。ポイントは、無理にループを回す必要がなく、Distance パッケージの pairwise() 関数を上手く使えばいいということだ。\njulia\u0026gt; @time foo(S, I);\r0.170835 seconds (7.98 k allocations: 210.854 MiB, 12.56% gc Time)\rjulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; 0.1, dims = 1);\r0.087875 seconds (14 allocations: 69.639 MiB, 4.15% gc Time) これら二つの命令は正確に同じ機能を持つが、速度面では約2倍の差があり、allocationに関しては計算したくもないほど大きな差が出るし、コーディングの難易度もループに比べてずっと簡単だ。\n追加研究 1 ユークリッド距離が距離関数である場合、Euclidean() の代わりに SqEuclidean() を使うと、ルートを取る計算を省略できるため、速度がさらに上がる。次は正確に同じ結果を出すが、速度面では約1.5倍の差が出る。\njulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; 0.1, dims = 1);\r0.091917 seconds (14 allocations: 69.639 MiB, 7.60% gc Time)\rjulia\u0026gt; @time sum(pairwise(SqEuclidean(),S,I) .\u0026lt; 0.01, dims = 1);\r0.061776 seconds (14 allocations: 69.639 MiB, 11.37% gc Time) さらに、もっと速くなることができる。ここでは、単純なコード最適化だけでは速度を上げるのが難しく、多次元検索に有利なデータ構造であるk-d木2を使用しなければならない。NearstNeighbors.jlで速く距離を計算する方法を参照。\n環境 OS: Windows julia: v1.5.0 https://github.com/JuliaStats/Distances.jl#pairwise-benchmark\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/K-d_tree\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2020,"permalink":"https://freshrimpsushi.github.io/jp/posts/2020/","tags":null,"title":"ジュリアで距離行列計算を最適化する方法"},{"categories":"줄리아","contents":"概要 Juliaで、Rのsample()やPythonパッケージnumpyのrandom.choice()と同じ役割をするsample()関数とWeights関数の使用方法です。\nコード 1 ■コード１■\n実行結果 ■コード２■\n環境 OS: Windows julia: v1.5.0 https://stackoverflow.com/a/27560273/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2018,"permalink":"https://freshrimpsushi.github.io/jp/posts/2018/","tags":null,"title":"ジュリアで重み付けとランダムサンプリングをする方法"},{"categories":"줄리아","contents":"結論 配列の各要素をEqualオペレータ==で比較すると、整数よりもCharの方が早い。\n速度比較 julia\u0026gt; integer = rand(1:5, N); print(typeof(integer))\rArray{Int64,1}\rjulia\u0026gt; character = rand([\u0026#39;S\u0026#39;,\u0026#39;E\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;,\u0026#39;D\u0026#39;], N); print(typeof(character))\rArray{Char,1}\rjulia\u0026gt; @time integer .== 1;\r0.009222 seconds (6 allocations: 1.196 MiB)\rjulia\u0026gt; @time character .== \u0026#39;S\u0026#39;;\r0.005266 seconds (7 allocations: 1.196 MiB) 上のコードは、整数と文字で構成された配列で1とSがどこにあるかを特定するプログラムだ。整数か文字列かの違いを除いて、全て正確に同じだが、時間の消費はほぼ二倍の大きな差がある。したがって、コード最適化の過程で一般的に使用される方法なので、可能な限り文字を使用することが推奨される。\n追加研究 julia\u0026gt; string = rand([\u0026#34;S\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;I\u0026#34;,\u0026#34;R\u0026#34;,\u0026#34;D\u0026#34;], N); print(typeof(string))\rArray{String,1}\rjulia\u0026gt; @time string .== \u0026#34;S\u0026#34;;\r0.072692 seconds (7 allocations: 1.196 MiB) 当然ながら、文字Characterではなく文字列Stringを使用すると、10倍以上の速度の低下が発生する。\n環境 OS: Windows julia: v1.5.0 ","id":2016,"permalink":"https://freshrimpsushi.github.io/jp/posts/2016/","tags":null,"title":"ジュリアでの文字と整数の等価オペレータ==の速度比較"},{"categories":"프로그래밍","contents":"概要 よく使われるRGB色の商標だ。\nコード ","id":2013,"permalink":"https://freshrimpsushi.github.io/jp/posts/2013/","tags":null,"title":"RGBカラーチートシート"},{"categories":"선형대수","contents":"定義1 関数 $T : V \\to W$がベクトル空間からベクトル空間への写像である場合、つまり $V$、$W$がベクトル空間である場合、$T$を変換transformationと呼ぶ。\n変換 $T$が線形関数である場合、すなわち全ての$\\mathbf{v},\\mathbf{u} \\in V$とスカラー$k$について次の二つの条件を満たす場合、線形変換linear transformationと呼ぶ。\n$T(k \\mathbf{u}) = k T(\\mathbf{u})$ $T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$ 特に$W=\\mathbb{C}$であれば、$T$を線形汎関数と呼ぶ。\n説明 関数、写像、変換は事実上同じ言葉として考えてもいい。ただし、線形代数、関数解析学などベクトル空間からベクトル空間への写像を扱う場合は、主に変換と呼び、transformationの頭文字を取って$T$と表記する。\n有限次元から有限次元への線形変換の場合は、行列の積と同様に扱うので、以下のように表記する。\n$$ T(\\mathbf{x}) = T\\mathbf{x} $$\n$T : V \\to V$を満たす線形変換を$V$上の線形作用素linear operator on $V$と呼ぶこともある。しかし、定義域と値域が同じでなければ作用素と呼ばれるわけではない。実用的な理由から、多くの教科書では$T : V \\to V$を線形作用素として定義している。\n$$ \\text{linear transformation form } V \\text{ to } V \\to \\text{linear operator on } V $$\nベクトル空間$X$から$Y$への全ての線形変換の集合は$L(X, Y)$のように表記する。2\n$$ L(X,Y) = \\mathcal{L}(X, Y) := \\left\\{ T : X \\to Y\\enspace |\\enspace T \\text{ is linear } \\right\\} $$\n行列変換は線形変換の一種である。\n恒等変換 線形変換$I : V \\to V$が全ての$\\mathbf{v} \\in V$に対して\n$$ I(\\mathbf{v}) = \\mathbf{v} $$\nを満たす場合、恒等変換identity transformationと呼ぶ。具体的には$I_{V}$のように表記することもある。\n零変換 線形変換$T_{0} : V \\to W$が全ての$\\mathbf{v} \\in V$に対して\n$$ T_{0}(\\mathbf{v}) = \\mathbf{0}_{W} $$\nを満たす場合、零変換zero transformationと呼ぶ。この時$\\mathbf{0}_{W}$は$W$の零ベクトルである。$O$、$0$などで表記することもある。簡単に言うと、零関数である。\n性質 $T : V \\to W$が線形変換であれば、以下が成立する。\n(a) $T(\\mathbf{0}) = \\mathbf{0}$\n(b) 全ての$\\mathbf{u}, \\mathbf{v} \\in V$に対して、$T(\\mathbf{u} - \\mathbf{v}) = T(\\mathbf{u}) - T(\\mathbf{v})$\n証明 (a) ベクトル空間の性質により、$0\\mathbf{v} = \\mathbf{0}$であるので、\n$$ T(\\mathbf{0}) = T( 0\\mathbf{u}) = 0T(\\mathbf{u}) = \\mathbf{0} $$\n■\n(b) 同様に、ベクトル空間の性質により$-\\mathbf{v} = (-1)\\mathbf{v}$であるので、\n$$ \\begin{align*} T(\\mathbf{u} - \\mathbf{v}) \u0026amp;= T \\big( \\mathbf{u} + (-1)\\mathbf{v} \\big) \\\\ \u0026amp;= T(\\mathbf{u}) + T\\big( (-1)\\mathbf{v} \\big) \\\\ \u0026amp;= T(\\mathbf{u}) + (-1)T(\\mathbf{v}) \\\\ \u0026amp;= T(\\mathbf{u}) - T(\\mathbf{v}) \\end{align*} $$\n■\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p446-447\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), p207\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3026,"permalink":"https://freshrimpsushi.github.io/jp/posts/3026/","tags":null,"title":"線形変換"},{"categories":"줄리아","contents":"エラー ERROR: SystemError: opening file \u0026quot;C:\\\\Users\\\\rmsms\\\\.julia\\\\registries\\\\General\\\\Registry.toml\u0026quot;: No such file or directory\r原因 人を本当にイライラさせるエラーだけど、言葉通りこのパスにRegistry.tomlファイルがなくて発生するエラーだ。\n解決法 C:\\Users\\사용자이름\\.julia\\registries\\General フォルダを削除してもう一度試してみる。\nその後、上のようにRegistry.tomlファイルも生まれて、インストールも正常に進行することを確認できる。\n","id":2069,"permalink":"https://freshrimpsushi.github.io/jp/posts/2069/","tags":null,"title":"Juliaパッケージのインストール時に\\General\\Registry.toml: No such file or directoryというエラーを解決"},{"categories":"선형대수","contents":"定義1 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$をベクトル空間$V$の部分集合としよう。$S$が下記の二条件を満たす時、$S$を$V$の基底basisという。\n$S$が$V$を生成する。\n$$ V = \\text{span}(S) $$\n$S$が線形独立である。\n説明 基底の名前から推測できるように、「ベクトル空間を作り出すことができる最も小さいもの」の概念に当たる。生成という条件が「ベクトル空間を作る」という意味を持ち、線形独立という条件が「最も小さい」という意味を持つ。ベクトル空間を作ることは分かるが、最も小さなものでなければならない理由については直ちに理解できないかもしれない。しかし、簡単な例を一つ見ればすぐに理解できるだろう。例えば、私たちは$(2,3)$というベクトルを\n$$ (2,3)=1(1,0) + 2(0,1) + 1(1,1) $$\nのように表さない。$(1,1)$を$(1,0), (0,1)$の線形結合で表すことができるからだ。つまり、上の式は不必要に長く記載した表現にすぎないということだ。従って、線形独立という条件はそのベクトルを基底の線形結合で表す時、最もすっきりと、必要なものだけをまとめた形で表されるようにしてくれる。\nここで注意すべきことは、一つのベクトル空間に対して基底が特に一意に存在する必要はないということだ。例を挙げると、$\\left\\{ (1,0) , (0,1) \\right\\}$は$\\mathbb{R}^{2}$を生成する基底だ。しかし、定義によれば$\\left\\{ (2,0) , (0,2) \\right\\}$も$\\mathbb{R}^2$の基底になることができる。それだけか？実は$\\left\\{ (1,1) , (-1,1) \\right\\}$も$\\mathbb{R}^2$を生成する上で全く問題がない。ただ、一般的に$\\mathbb{R}^{n}$では、下記のベクトルから成る基底を扱う。\n$$ \\mathbf{e}_{1} = (1,0,0,\\dots,0), \\quad \\mathbf{e}_{2}=(0,1,0,\\dots,0),\\quad \\mathbf{e}_{n}=(0,0,0,\\dots,1) $$\nこのような基底を$\\left\\{ \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\dots, \\mathbf{e}_{n} \\right\\}$を$\\mathbb{R}^{n}$上の標準基底standard basis for $\\mathbb{R}^{n}$という。各$\\mathbf{e}_{i}$は標準単位ベクトルstandard unit vectorと呼ばれる。特に$n=3$の場合は、一般的に以下のように表記される。\n$$ \\begin{align*} \\hat{\\mathbf{x}} =\u0026amp;\\ \\mathbf{e}_{1} = \\hat{\\mathbf{x}}_{1} = \\mathbf{i}=(1,0,0) \\\\ \\hat{\\mathbf{y}} =\u0026amp;\\ \\mathbf{e}_{2} = \\hat{\\mathbf{x}}_{2} = \\mathbf{j}=(0,1,0) \\\\ \\hat{\\mathbf{z}} =\u0026amp;\\ \\mathbf{e}_{3} = \\hat{\\mathbf{x}}_{3} = \\mathbf{k}=(0,0,1) \\end{align*} $$\n以下の定理から、座標の概念を抽象化されたベクトル空間でも話すことができる。$\\mathbf{v} \\in V$が$(1)$のように表される時、$[\\mathbf{v}]_{S}$を基底$S$に対する$\\mathbf{v}$の座標ベクトルcoordinate vector $\\mathbf{x}$ of relative of $S$という。\n$$ [\\mathbf{v}]_{S} = \\begin{bmatrix} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{bmatrix} $$\n定理: 基底表現の一意性 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{n} \\right\\}$をベクトル空間$V$の基底としよう。すると、全てのベクトル$\\mathbf{v} \\in V$に対して\n$$ \\begin{equation} \\mathbf{v} = c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{n}\\mathbf{v}_{n} \\end{equation} $$\nと表現する方法は一意である。つまり、上記の式を満たす係数の組$(c_{1},c_{2},\\dots,c_{n})$が一意に存在する。\n証明 $S$が$V$を生成するため、生成の定義に従い、$V$の全てのベクトルは$S$の線形結合で表せる。あるベクトル$\\mathbf{v}$が下記の二つの線形結合で表せるとしよう。\n$$ \\begin{align*} \\mathbf{v} \u0026amp;= c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{n}\\mathbf{v}_{n} \\\\ \\mathbf{v} \u0026amp;= k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + \\cdots + k_{n}\\mathbf{v}_{n} \\end{align*} $$\n上記の式から下の式を引くと、次のようになる。\n$$ \\mathbf{0} = (c_{1} - k_{1}) \\mathbf{v}_{1} + (c_{2} - k_{2}) \\mathbf{v}_{2} + \\cdots + (c_{n} - k_{n}) \\mathbf{v}_{n} $$\nしかし$\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{n}$は線形独立であるため、上記の式を満たす解はオロジ\n$$ c_{1} - k_{1} = 0,\\quad c_{2} - k_{2} = 0,\\quad \\dots,\\quad c_{n} - k_{n} = 0 $$\nのみである。従って、次が成り立つ。\n$$ c_{1} = k_{1},\\quad c_{2} = k_{2},\\quad \\dots,\\quad c_{n} = k_{n} $$\nよって、二つの線形結合表現は同一である。\n■\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p240\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3017,"permalink":"https://freshrimpsushi.github.io/jp/posts/3017/","tags":null,"title":"ベクトル空間の基底"},{"categories":"줄리아","contents":"ガイド ステップ1. ジュリアのインストール\nジュリアのダウンロードページからインストールファイルをダウンロードして、実行する。\nステップ2. VSコードのインストール\nビジュアルスタジオコードのダウンロードページからインストールファイルをダウンロードして、実行する。\nステップ3. ジュリア拡張のインストール\n左側から5番目のアイコンをクリックするか、Ctrl + Shift + XでExtensionsを開く。\u0026lsquo;julia\u0026rsquo;と検索すると、最上段にJulia Language Supportが表示される。\nInstallをクリックしてインストールする。\nエディターで拡張子が.jlのファイルを作り、ジュリアコードを書き、Shift + Enterで全体を実行してみる。上のスクリーンショットでは、\u0026ldquo;helloworld\u0026quot;を出力するためにprintln(helloworld)という1行だけ書かれている。\n環境 OS: Windows julia: v1.5.4 ","id":2067,"permalink":"https://freshrimpsushi.github.io/jp/posts/2067/","tags":null,"title":"WindowsでJuliaの最新バージョンをインストールする方法"},{"categories":"행렬대수","contents":"定義1 $n\\times n$ 行列 $A$が与えられたとしよう。$\\mathbf{0}$でない$n\\times 1$列ベクトル$\\mathbf{x}$、そして定数$\\lambda$に対して、次の式を固有値方程式または固有値問題という。\n$$ \\begin{equation} A \\mathbf{x} = \\lambda \\mathbf{x} \\end{equation} $$\n与えられた$A$に対して、上のように固有値方程式を満たす$\\lambda$を$A$の固有値と言い、$\\mathbf{x}$を$\\lambda$に対応する$A$の固有ベクトルという。\n説明 上の定義は$\\lambda \\in \\mathbb{R}$、$\\mathbf{x} \\in \\mathbb{R}^{n}$の時だけでなく、$\\lambda \\in \\mathbb{C}$、$\\mathbf{x} \\in \\mathbb{C}^{n}$の時にもそのまま適用される。「$\\mathbf{0}$でない」という条件がついているのは、下の式から分かるように、$\\mathbf{x} = \\mathbf{0}$ならば常に成り立つからだ。\n$$ A \\mathbf{0} = \\mathbf{0} = \\lambda \\mathbf{0} $$\n幾何学的な動機 ベクトル$\\mathbf{x}$を行列$A$で変換した$A \\mathbf{x}$と$\\mathbf{x}$の方向が同じだとすると、何か実数$\\lambda$に対して\n$$ A \\mathbf{x} = \\lambda \\mathbf{x} $$\nが成り立つことになる。行列$A$は本来、どんな方向の概念も持たないが、$A$の固有ベクトルが存在するならば、$A$が何か特有の方向を指していると言えるだろう。だから、このようなベクトル$\\mathbf{x}$を固有ベクトルと呼ぶのだ。例えば、以下のような$2\\times 2$行列を考えてみよう。\n$$ A = \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix} $$\nすると、ベクトル$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$は$\\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix}$に変換された時、$\\begin{bmatrix} 14 \\\\ 7 \\end{bmatrix}$となって方向が同じである。ここでベクトル$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$に$\\lambda = 7$を掛けると、ベクトルの長さも同じになり、固有値方程式\n$$ \\begin{align*} A \\mathbf{x} \u0026amp;= \\lambda \\mathbf{x} \\\\ \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \u0026amp;= 7 \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\end{align*} $$\nの形の等式を満たす。このような理由で$\\lambda=7$を固有値と呼ぶのだ。よく見ると、固有ベクトルは$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$を伸ばしたり縮めたりして無数に見つけることができるが、固有値は変わらないことが分かる。だから、$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$を固有値$7$に対応する$A$の固有ベクトルと表現するのだ。\nこのように幾何学的に説明した議論を一般的に拡張すると、固有値は代数的に方程式$A \\mathbf{x} = \\lambda \\mathbf{x}$を満たす$\\lambda$であり、固有ベクトルは与えられた$\\lambda$に対する方程式の非自明な解である。\n固有値方程式の解法 固有値を求めることは、固有値方程式から始まる。$(1)$の式を整理すると、次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; A \\mathbf{x} \u0026amp;= \\lambda \\mathbf{x} \\\\ \\implies \u0026amp;\u0026amp; A \\mathbf{x} - \\lambda \\mathbf{x} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; A \\mathbf{x} - \\lambda I \\mathbf{x} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; \\left( A - \\lambda I \\right) \\mathbf{x} \u0026amp;= \\mathbf{0} \\end{align*} $$\nこの時、固有ベクトルは条件$\\mathbf{x} \\ne \\mathbf{0}$を満たさなければならない。上記線形システムが$\\mathbf{0}$でない解を持つ同値条件は、$\\left( A - \\lambda I \\right)$の逆行列が存在しないことであり、これは次の式が成り立つことと同値である。\n$$ \\det (A -\\lambda I) = 0 $$\nしたがって、上の式を満たす$\\lambda$が$A$の固有値になる。上記の式を$A$の特性方程式と言い、$\\det (A -\\lambda I)$は$A$が$n\\times n$行列の時、$n$次の多項式になり、これを特性多項式と呼ぶ。\nちなみに、$A+B$の固有値は$A$、$B$の固有値の和と異なる場合があり、$AB$の固有値も$A$、$B$の固有値の積と異なる場合がある。また、方程式の解として固有値を求めることから分かるように、必ずしも実数であるという保証は全くない。\n例 固有値を求める 解の例として、再び$A = \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix}$を考えてみよう。$A-\\lambda I = \\begin{bmatrix} 6 - \\lambda \u0026amp; 2 \\\\ 2 \u0026amp; 3 - \\lambda \\end{bmatrix}$なので、$A$の特性方程式を解くと次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\det (A - \\lambda I) \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; (6 - \\lambda)(3 - \\lambda) - 4 \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; \\lambda^2 - 9 \\lambda + 18 - 4 \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; (\\lambda - 2)(\\lambda - 7) \u0026amp;= 0 \\end{align*} $$\nしたがって、$A$の固有値は$\\lambda = 2$と$\\lambda = 7$である。$2$と$7$を$\\lambda$に代入してみると、それぞれの固有値に対応する固有ベクトルを求めることができる。ここでは、$\\lambda = 7$の場合のみ紹介する。\n$\\lambda = 7$に対応する固有ベクトルを求める $\\lambda = 7$を$(1)$に代入して整理すると、次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} \u0026amp;= 7\\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} \\\\ \\implies \u0026amp;\u0026amp; \\begin{bmatrix} 6x_{1} + 2x_{2} \\\\ 2x_{1} + 3x_{2} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} 7x_{1} \\\\ 7x_{2} \\end{bmatrix} \\\\ \\implies \u0026amp;\u0026amp; \\begin{bmatrix} -x_{1} + 2x_{2} \\\\ 2x_{1} - 4x_{2} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\end{align*} $$\nこれを解くと、次のようになる。\n$$ \\left\\{ \\begin{align*} -x_{1} + 2x_{2} \u0026amp;= 0 \\\\ 2x_{1} - 4x_{2} \u0026amp;= 0 \\end{align*} \\right. $$\n$$ \\implies x_{1} = 2x_{2} $$\nしたがって、$0$でないすべての$x_{2}$に対して、ベクトル$\\begin{bmatrix} 2x_{2} \\\\ x_{2} \\end{bmatrix}$が$\\lambda = 7$に対応する固有ベクトルになる。通常、最も単純な形または大きさが$1$になる単位ベクトルを選ぶ。$x_{2} = 1$を代入すると、以下の固有ベクトルを得る。\n$$ A = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} $$\n性質 正の整数$k$に対して、$\\lambda$が行列$A$の固有値であり、$\\mathbf{x}$が$\\lambda$に対応する固有ベクトルであれば、$\\lambda ^{k}$は$A^{k}$の固有値であり、$\\mathbf{x}$は$\\lambda ^{k}$に対応する固有ベクトルである。 Howard Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p291-292\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":319,"permalink":"https://freshrimpsushi.github.io/jp/posts/319/","tags":null,"title":"固有値と固有ベクトル"},{"categories":"머신러닝","contents":"定義 強化学習とは、エージェントが環境と相互作用して累積報酬を最大化するポリシーを見つけることができるようにすることである。\n説明1 強化学習を構成する要素は次のとおりである。\nエージェントagent: 与えられた状態において、ポリシーに従って行動を決定する。 ステートstate, 状態: エージェントが置かれている状況を指す。 アクションaction, 行動: エージェントが与えられた状態で選ぶことができる選択肢を指す。 ポリシーpolicy, 方針: エージェントが与えられた状態で行動を決定する戦略を指す。 リワードreward, 報酬: エージェントが与えられた状態で選んだ行動によって得られる点数を指す。エージェントが達成すべき目標と見なすことができる。 環境environment: エージェントが与えられた状態でどのような行動を決定すれば、MDPに従って次の状態とそれに伴う報酬を決定するか。 エピソードepisode: エージェントと環境の相互作用が始まった時から終わるまでを指す。 これをさまざまな状況に例えると次のようになる。\n強化学習 試験勉強 囲碁 エージェント 学生 囲碁の棋士 ステート 試験まで残り日数 碁盤 アクション 勉強、飲酒、ゲームなど 着手 ポリシー 日付別勉強計画 戦略 リワード 試験点数 勝敗 エピソード 試験期間 一局 強化学習の問題：グリッドモデル 強化学習を説明するための代表的な例としてグリッドワールドgrid worldがある。これから次のグリッドモデルを例に各要素を具体的に説明する。一度に上下左右の4方向のうち一つに一マスずつ動けるロボットが下記のような$4 \\times 4$のグリッドで動く場合を考えてみよう。スタート地点は$\\boxed{\\ 2\\ }$から$\\boxed{15}$まで任意に決められ、ロボットが$\\fcolorbox{black}{darkgray}{\\ 1\\ }$または$\\fcolorbox{black}{darkgray}{16}$まで最短距離で行くことが目標とする。\nエージェント 強化学習におけるエージェントは学習する主体として説明されるが、実際には存在しない。後述する他の概念が確率変数などで定義されるのに対し、エージェントには明確な数学的定義がない。したがって、強化学習に関する理論的な勉強はエージェントという対象がなくても可能であり、実際にそうである。強化学習理論において本質的にエージェントを意味するのはポリシーである。しかし直感的には、学習する対象があると考える方が便利なため、「エージェントが行動する」「エージェントの状態が変わった」といった表現を用いる。エージェントは単にコンピュータシミュレーション（特にゲーム）においてキャラクターのように学習しているように見えるものに過ぎない。たとえば、グリッドモデルではエージェントが下の右側の図のように移動するのは、単純に状態の列挙で表すこともできる。 $$ \\boxed{\\ 3\\ } \\to \\boxed{\\ 2\\ } \\to \\fcolorbox{black}{darkgray}{\\ 1\\ } $$ $3, 2, 1$を順番にprintするだけでよい。強化学習の最終的に私たちが得たいのは本質的にポリシーであるため、エージェントというものを定義しなくても学習することができる。一言で言えば、エージェントはポリシーの視覚化（実現化）であると言える。\nもちろん、上記の話は理論やコンピュータシミュレーションでの話であり、自動運転のような実際の応用では、ポリシーに従って実際に動くドローンや自動車が必要である。この場合、ドローンや自動車などのロボットや機械がエージェントとなり、それがなければポリシーの学習は不可能である。\n状態 状態stateは確率変数であり、stateの頭文字をとって$S$と表記する。エピソードは時間に沿って順次進行するため、インデックスとして$t$を使用する。したがって、タイムステップが$t$のときのステート関数を$S_{t}$と表記する。初期ステートは通常$t=0$で表される。まとめると、$S_{t}$は時間が$t$のとき、各グリッドに対して次のような関数値を与える関数である。\n$$ S_{t} \\left( \\boxed{ N } \\right) = n,\\quad 1\\le n \\le 16 $$\nこのとき、可能なすべての状態値（状態関数の関数値）の集合を$\\mathcal{S}\\subset \\mathbb{R}$と表記し、その要素を$s$と表記する。\n$$ \\mathcal{S} = \\left\\{ s_{1}, s_{2},\\dots \\right\\} $$\nそれでは上記の格子モデルに対する状態関数は次のようになります。\n$$ S_{t} : \\left\\{ \\fcolorbox{black}{darkgray}{\\ 1\\ } , \\boxed{\\ 2\\ }, \\dots, \\boxed{15}, \\fcolorbox{black}{darkgray}{16} \\right\\} \\to \\mathcal{S} \\\\ S_{t} \\left( \\boxed{\\ n\\ } \\right) = s_{n} = n,\\quad 1\\le n \\le 16 $$\nそれでは時間が$t$のときの状態値が$s_{6}$から次のタイムステップで状態値が$s_{10}$に変わる確率は次のようになります。\n$$ P \\left( S_{t+1} = s_{10} | S_{t} = s_{6} \\right) $$\n到達した瞬間にエピソードが終了する状態をターミナルステートterminal stateと呼びます。上記の格子モデルではターミナルステートは$\\fcolorbox{black}{darkgray}{1}, \\fcolorbox{black}{darkgray}{16}$です。\n行動 行動actionとはエージェントが現在の状態で取ることができる選択肢のことであり、これもまた確率変数です。actionの頭文字を取って$A_{t}$と表記します。上記の格子モデルの例では、$\\boxed{2}$ ~ $\\boxed{15}$の各々で上下左右を選択することができます。可能な全ての行動値（行動関数の関数値）の集合を$\\mathcal{A}\\subset \\mathbb{R}$と表記し、その要素を$a$と表記します。\n$$ \\mathcal{A} = \\left\\{ a_{1}, a_{2}, \\dots \\right\\} $$\nそれではタイムステップ$t$での行動関数は次のようになります。\n$$ A_{t} : \\left\\{ \\uparrow, \\rightarrow, \\downarrow, \\leftarrow \\right\\} \\to \\mathcal{A} \\\\ \\begin{cases} A_{t}(\\uparrow) = a_{1} \\\\ A_{t}(\\rightarrow) = a_{2} \\\\ A_{t}(\\downarrow) = a_{3} \\\\ A_{t}(\\leftarrow) = a_{4} \\end{cases} $$\nエージェントは与えられた状態で確率に従って行動を決定します。例えばタイムステップが$t$のときの状態値が$s_{6}$で行動$a_{1}$を選択した確率は次のようになります。\n$$ P(A_{t} = a_{1} | S_{t} = s_{6}) $$\n方針 方針policyとは状態$s$で行動$a$を決定する確率を全ての$s$と$a$に対して明記したものを言い、$\\pi$で表記します。ゲームや戦争に例えると戦略です。格子モデルの例で行動を決定する確率が$\\dfrac{1}{4}$で全て同じだとすると、方針$\\pi$は次のようになります。\n$$ \\pi \\begin{cases} P(a_{1} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{2} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{2}) = \\dfrac{1}{4} \\\\ \\vdots \\\\ P(a_{2} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{4} | s_{15}) = \\dfrac{1}{4} \\end{cases} \\quad \\text{or} \\quad \\pi : \\mathcal{S} \\times \\mathcal{A} \\to [0,1] $$\nもちろんこれは最適化された方針ではありません。簡単に$\\boxed{2}$の場合だけ考えても、上に行くと格子の外に出てしまうため、上に行く確率自体が全く無い方がより良い方針です。したがって、下の図で$\\pi_{1}$よりも$\\pi_{2}$がより良い方針だと言えます。\n強化学習アルゴリズムの目標は最適な方針を見つけることです。では、最適な方針をどのように見つけるかというと、方針の良さを評価する価値関数value functionを通じて見つけることができます。\n報酬 報酬rewardとは、与えられた状態でエージェントが選択した行動に対して実数をマッピングする関数であり、rewardの頭文字を取って$R_{t}$と表記します。全ての報酬値（報酬関数の関数値）の集合を$\\mathcal{R} \\subset \\mathbb{R}$と表記し、その要素を$r$と表記します。\n$$ \\mathcal{R} = \\left\\{ r_{1}, r_{2}, \\dots \\right\\} \\\\ R_{t} = \\mathcal{S} \\times \\mathcal{A} \\to \\mathcal{R} $$\n報酬は一回のタイムステップごとに一回ずつ受け取り、一回のエピソードで受け取った総報酬、つまり蓄積された報酬が最も大きくなるような方針を見つけることが強化学習の究極的な目標です。\nでは、なぜ各タイムステップの報酬よりも蓄積された報酬が大きくなるようにするのか疑問に思うかもしれません。これは試験勉強に例えると簡単に理解できます。試験期間中に毎晩勉強する代わりにお酒を飲んだり遊んだりゲームをした場合、当面は勉強するよりも楽しいでしょう。しかし、蓄積された報酬、つまり試験の成績は散々なものになります。したがって、今は勉強することが疲れて大変だとしても、将来の大きな報酬のために勉強する方が良いと判断し、試験勉強をするわけです。\n報酬は人が設定するハイパーパラメータです。したがって、エージェントが行うべき仕事に応じて適切に設定する必要があります。例えば、格子モデルの例で格子が迷路であり、エージェントが迷路を脱出するロボットである場合、一マス移動するごとに$-1$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。格子が公園であり、エージェントがペットの散歩をするロボットである場合、一マス移動するごとに$0$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。\n環境 環境environmentとはエージェントが与えられた状態で選択した行動に応じて次の状態と報酬を決定する関数、すなわち$f : (s,a) \\mapsto (s^{\\prime},r)$です。したがって、常に現実にぴったりと当てはまる比喩を見つけるのは難しいです。\nタイムステップが$t$のときの状態を$s_{t}$、$s_{t}$で選択した行動を$a_{t}$とします。これにより、環境が決定した次の状態を$s_{t+1}$、報酬を$r_{t+1}$とすると次のように表されます。\n$$ f(s_{t}, a_{t}) = (s_{t+1}, r_{t+1}) $$\n格子モデルの例について具体的に説明すると、エージェントが$\\boxed{7}$で$\\uparrow$を選択し、環境が次の状態$\\boxed{3}$と報酬$-1$を決定した場合は、次のような数式で\n表されます。\n$$ f(s_{7}, a_{1}) = (s_{3}, -1) $$\nエージェントが行動を決定する戦略を方針と呼ぶならば、環境が次の状態と報酬を決定することをMDPmarkov decision process, マルコフ決定プロセスと言います。エージェントと環境の相互作用を図で表すと次のようになります。\nエピソード エージェントと環境が相互作用しながら決定された状態、行動、報酬の数列を経路trajectory, 軌跡または履歴historyと言います。経路が有限の場合をepisode taskと言います。上で例に挙げた試験期間、囲碁、格子モデルもこれに該当します。\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{T-1}, s_{T}, r_{T} \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{T-1}}{\\to} (s_{T}, r_{T}) $$\n経路が無限の場合をcontinuing taskと言います。ただし、非常に長い時間にわたって続くエピソードは無限の場合とみなされることもあります。\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{t-1}, s_{t}, r_{t}, a_{t}, s_{t+1}, r_{t+1},\\dots \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{t-1}}{\\to} (s_{t}, r_{t}) \\overset{a_{t}}{\\to} (s_{t+1}, r_{t+1}) \\overset{a_{t+1}}{\\to} \\cdots $$\nオ・イルソク, 機械学習(MACHINE LEARNING). 2017, p466-480\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3029,"permalink":"https://freshrimpsushi.github.io/jp/posts/3029/","tags":null,"title":"機械学習における強化学習とは？"},{"categories":"복소해석","contents":"定義 開集合$A \\subset \\mathbb{C}$と$f: A \\to \\mathbb{C}$が定義されていて、$\\alpha \\in A$としよう。\n$\\displaystyle \\lim_{z \\to \\alpha } f(z) = f (\\alpha)$ならば、$f$は$\\alpha$で連続だといい、複素領域 $\\mathscr{R}$の全ての点で連続ならば、$f$は$\\mathscr{R}$上で連続だという。特に$f$が定義域上で連続ならば、連続関数と呼ばれる1。\n$\\alpha$での$f$の微分係数を以下のように定義し、$\\alpha$で微分係数が存在すれば、$f$は$\\alpha$で微分可能であるという。 $$ f ' (\\alpha) := \\lim_{h \\to 0} {{ f ( \\alpha + h ) - f ( \\alpha ) } \\over { h }} $$ ここで$h \\in \\mathbb{C}$とし、複素平面上のどの方向でも関係なくなければならない。\n$f$が複素領域 $\\mathscr{R}$の全ての点で微分可能ならば、$f$は$\\mathscr{R}$で解析的であるという。特に$f:\\mathbb{C} \\to \\mathbb{C}$が$\\mathbb{C}$で解析的ならば、全解析Entire関数という2。\n説明 実数集合$\\mathbb{R}$を定義域とする関数とは異なり、一般的に$\\mathbb{C}$を定義域とする関数も同じ幾何学的意味を持つわけではないが、形式的な定義上、複素解析における微分が微分と呼ばれる理由が全くないわけではない。もちろん、複素平面として$\\mathbb{C} \\simeq \\mathbb{R}^{2}$を考えるならば、やはり傾きと似た意味で見ることができる。 解析的関数は、正則関数Regular Function、ホロモーフィック関数Holomorphic Functionとも呼ばれる。しかし、解析的連続の条件として使われる点で、解析的関数という表現が最もメジャーだ。\u0026ldquo;なぜこれを微分可能な関数ではなく、わざわざ解析的関数という言葉を作って呼ぶのか？\u0026ldquo;については、複素解析が発展した当時の視点が入っていると見ることができるだろう。言及したように、複素平面での微分というのは、形式的な定義であって、我々が実数空間$\\mathbb{R}$で扱ったように考えるべきではない、という意味だったのではないかと思われる。 Osborne (1999). Complex variables and their applications: p39.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOsborne (1999). Complex variables and their applications: p50.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1929,"permalink":"https://freshrimpsushi.github.io/jp/posts/1929/","tags":null,"title":"解析関数"},{"categories":"줄리아","contents":"コード Juliaで色を取り扱うために提供されるパッケージはColors.jlだ。可視化パッケージのPlots.jlを読み込めば、Colors.jl内の機能も一緒に使える。RGB空間を表す色コードには、RGB、BGR、RGB24、RGBX、XRGBがサポートされており、これらはAbstractRGBのサブタイプだ。RGBAはRGBに透明度が加わったものだ。\njulia\u0026gt; using Plots\rjulia\u0026gt; subtypes(AbstractRGB)\r5-element Vector{Any}:\rBGR\rRGB\rRGB24\rRGBX\rXRGB\rjulia\u0026gt; subtypes(AbstractRGBA)\r2-element Vector{Any}:\rBGRA\rRGBA 文字列 plot()関数の色を指定するキーワードに文字列として\u0026quot;#FF0000\u0026quot;のように入力すると、16進RGBコードであるHEXコードを使える。下を見ればわかるが、文字列を入力しても良い理由は、plot()が文字列を自動でパースしてくれるからと見られる。\nusing Plots\rr = \u0026#34;#FF0000\u0026#34; # R빨간색 RGB(255, 0, 0)의 6자리 HEX 코드\rg = \u0026#34;#00FF0033\u0026#34; # 투명도가 0.2인 초록색 RGBA(0, 255, 0, 0.2)의 8자리 HEX 코드\rp = \u0026#34;#80F\u0026#34; # 보라색 RGB(255, 0, 136)의 3자리 HEX 코드\rplot([1 2 3; 2 3 4], ones(2, 3), fillrange = 2,\rfillcolor = [r g p],\rlabel = [r g p]) パーシング colorant\u0026quot;#FF0000\u0026quot;のような形でHEXコードをパースできる。\njulia\u0026gt; r = colorant\u0026#34;#FF0000\u0026#34; # R빨간색 RGB(255, 0, 0)의 6자리 HEX 코드\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; g = colorant\u0026#34;#00FF0033\u0026#34; # 투명도가 0.2인 초록색 RGBA(0, 255, 0, 0.2)의 8자리 HEX 코드\rRGBA{N0f8}(0.0,1.0,0.0,0.2)\rjulia\u0026gt; p = colorant\u0026#34;#80F\u0026#34; # 보라색 RGB(255, 0, 136)의 3자리 HEX 코드\rRGB{N0f8}(0.533,0.0,1.0)\rjulia\u0026gt; plot([1 2 3; 2 3 4], ones(2, 3), fillrange = 2,\rfillcolor = [r g p]) parse(RGB, \u0026quot;#FF0000\u0026quot;)のようにパースすることもできる。\njulia\u0026gt; parse(RGB, \u0026#34;#FF0000\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(RGBA, \u0026#34;#FF000080\u0026#34;)\rRGBA{N0f8}(1.0,0.0,0.0,0.502) 色名を取得 hex()関数は色のHEXコードを文字列で返す。\njulia\u0026gt; hex(colorant\u0026#34;red\u0026#34;)\r\u0026#34;FF0000\u0026#34;\rjulia\u0026gt; hex(colorant\u0026#34;rgb(0, 255, 128)\u0026#34;)\r\u0026#34;00FF80\u0026#34;\rjulia\u0026gt; hex(RGBA(1, 0.5, 0, 0.5), :RRGGBB)\r\u0026#34;FF8000\u0026#34;\rjulia\u0026gt; hex(RGBA(1, 0.5, 0, 0.5), :RRGGBBAA)\r\u0026#34;FF800080\u0026#34;\rjulia\u0026gt; hex(HSV(30,1.0,1.0), :AARRGGBB)\r\u0026#34;FFFF8000\u0026#34; 併せて見る Plotsで色を使う方法 RGB色コードを使う方法 HEX色コードを使う方法 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10, FixedPointNumbers v0.8.4 併せて見る 色を使う方法 パレットを使う方法 カラーグラデーションを使う方法 色処理のためのパッケージ Colors.jl RGBコードを使う方法 RGB(1, 0, 0) HEXコードを使う方法 \u0026quot;#000000\u0026quot; グラフ要素の色を指定する方法 サブプロットごとにグラフの色を指定する方法 軸、軸名、目盛り、目盛り値の色を指定する方法 背景色を指定する方法 ","id":1921,"permalink":"https://freshrimpsushi.github.io/jp/posts/1921/","tags":null,"title":"ジュリアで16進数RGBコード（HEX）を使用する方法"},{"categories":"줄리아","contents":"環境 OS: Windows11 バージョン: Julia 1.9.0, DataFrames v1.5.0 ","id":1930,"permalink":"https://freshrimpsushi.github.io/jp/posts/1930/","tags":null,"title":"ジュリアでのデータフレームと2次元配列間の変換方法"},{"categories":"줄리아","contents":"ガイド 旧バージョン julia v1.5.0では、*.csvファイルを以下のように読み込んだ。 実際、Juliaはまだデータ入力に特別便利な言語ではない。しかし、速さを求めるならば、PythonやR、MatlabよりもJuliaを選ばなければならない時が来るかもしれない。例えば、Eドライブ直下にある*.csvファイルを読み込みたい場合、次のように入力すればいい。\nusing CSV\rdata = CSV.read(\u0026#34;E:/example.csv\u0026#34;) 実行結果を見ると、*.csvファイルがデータフレームとしてうまく読み込まれたことが確認できる。\n新バージョン 正確な時期はわからないが、julia v1.7.0以降では、データフレームを別途読み込む必要がある。 using CSV, DataFrames\rdata = CSV.read(\u0026#34;E:/example.csv\u0026#34;, DataFrame) 環境 OS: Windows ","id":1923,"permalink":"https://freshrimpsushi.github.io/jp/posts/1923/","tags":null,"title":"ジュリアで *.csvファイルを読み込む方法"},{"categories":"줄리아","contents":"ガイド Juliaでは並列計算が日常的に使用されるため、場合によってはコンピュータの全リソースを計算に集中させる必要がある。スレッド数を変更する方法はいくつかあるが、最もスタティックで便利な方法は環境変数を編集することだ。\nステップ1. システム環境変数の編集\nWindowsキーまたはWindows+Sを押して「システム環境変数の編集」を探す。\nシステムプロパティというウィンドウが表示されたら、「環境変数」をクリックする。\nステップ2. JULIA_NUM_THREADS を探す\nユーザー変数で上記のような変数を探す。この値がスレッドの数だ。存在する場合は「編集」を、存在しない場合は「新規作成」を選択して ステップ3. に進む。\nステップ3. 変数の値を変更\n上のスクリーンショットで示された部分に、希望するスレッド数を記入する。適切なスレッド数はコンピュータのスペックによって異なるが、私たちは例を扱っているので、$5$個に変更してみよう。\nステップ4. 確認\nusing Base.Threads\rnthreads() 上のコードをJuliaコンソールで実行して確認しよう。\n反映されない場合はまず再起動を試みて、それでもだめならシステム変数で変更を試みよう。\n環境 OS: Windows julia: v1.5.0 ","id":1933,"permalink":"https://freshrimpsushi.github.io/jp/posts/1933/","tags":null,"title":"WindowsでJuliaの並列計算に使用するスレッド数を変更する方法"},{"categories":"행렬대수","contents":"定義 $A$をサイズ$n\\times n$の任意の正方行列としよう。$A$と行列の積が可能で、以下の式を満たす行列$L$を$A$の左逆行列という。\n$$ LA=I_{n} $$\nここで$I_{n}$はサイズ$n\\times n$の単位行列である。$A$と行列の積が可能で、以下の式を満たす行列$R$を$A$の右逆行列という。\n$$ AR=I_{n} $$\n$A$が左/右逆行列を両方持っていれば、これらは互いに等しく$A^{-1}$と表記され、$A$の逆行列という。\n$$ A^{-1}A=I_{n}=AA^{-1} $$\n$A$が逆行列を持つ場合、$A$を可逆行列または非特異行列という。$A$が逆行列を持たない場合、$A$を特異行列という。\n説明 定義により、$LA$のサイズが$n\\times n$でなければならないので、$L$は必ず$n \\times n$行列でなければならず、$R$も同様である。$A$を正方行列に限定した理由は、$A^{-1}$が$A$の両側から掛けられる必要があるためである。同様に行列の積は交換可能ではないため、左/右逆行列が両方存在する必要がある。実際に、任意の行列が左/右逆行列を持つ場合、これらは常に同じである。\n性質 $A$と$B$を任意の$n \\times n$正方行列としよう。すると、以下が成り立つ。\n(a) $A$が左逆行列$L$と右逆行列$R$を持つ場合、これらは同じである。\n$$ L=A^{-1}=R $$\n(b) $A$の逆行列が存在する場合、それは一意である。\n(c) $AB = I \\iff BA = I$\n(d) $A$と$B$を可逆行列としよう。すると、2つの行列の積$AB$も可逆であり、その逆行列は次の通りである。\n$$ (AB)^{-1}=B^{-1}A^{-1} $$\n(d\u0026rsquo;) 同じサイズの可逆行列の積も可逆であり、その逆行列はそれぞれの逆行列を逆順に掛けたものと同じである。つまり、$A_{1},A_{2},\\dots,A_{n}$が可逆行列ならば、次が成り立つ。\n$$ \\left( A_{1}A_{2}\\cdots A_{n} \\right)^{-1} = A_{n}^{-1}\\cdots A_{2}^{-1} A_{1}^{-1} $$\n(e) $AB$が可逆ならば、$A$と$B$も可逆である。\n(f) $A$が可逆ならば、転置も可逆であり、その逆行列は次の通りである。\n$$ \\left( A^{T} \\right)^{-1} = \\left( A^{-1} \\right)^{T} $$\n従って、(c) $\\iff$ (d) であることがわかる。\n証明 (a) $n\\times n$行列$A$が与えられたとしよう。$L$が$A$の左逆行列だとしよう。すると、下記の式が成り立つ。\n$$ LA=I_{n} $$\n$R$を$A$の右逆行列としよう。$R$を上記式の右辺に掛けると、次のようになる。\n$$ LAR = I_{n}R =R $$\nしかし、$R$は$A$の右逆行列であるため、$LAR=LI_{n}=L$が成り立つ。従って、上記の式は次のようになる。\n$$ L=R $$\n■\n(b) 任意の正方行列$A$が異なる2つの逆行列$B$と$C$を持つと仮定しよう。それから下記の計算ができる。\n$$ B=BI=B(AC)=(BA)C=IC=C $$\nしかし、この結果は$B$と$C$が異なるという仮定に矛盾する。従って、仮定は誤りであり、逆行列が存在する場合、それは一意である。\n■\n(c) 一般性を失うことなく、$BA = I \\implies AB = I$だけを証明しよう。$BA = I$と仮定しよう。これから式$A \\mathbf{x} = \\mathbf{0}$を考える。\n$$ \\begin{align*} A\\mathbf{x} = \\mathbf{0} \u0026amp;\\implies B(A\\mathbf{x}) = B \\mathbf{0} \\\\ \u0026amp;\\implies (BA)\\mathbf{x} = \\mathbf{0} \\end{align*} $$\nここで、$BA = I$と仮定したので、$\\mathbf{x} = \\mathbf{0}$である。従って、$A \\mathbf{x} = \\mathbf{0}$は自明の解だけを持つ。\n可逆行列の同値条件\n$A$をサイズ$n\\times n$の正方行列としよう。すると、以下の命題は全て同値である。\n$A$は可逆行列である。 同次線形システム$A\\mathbf{x}=\\mathbf{0}$は自明の解だけを持つ。 可逆行列の同値条件により、$A$は可逆である。従って、$A^{-1}$が存在し、\n$$ BA = I \\implies A(BA)A^{-1} = AIA^{-1} \\implies AB = I $$\n■\n(d) $A$と$B$をサイズ$n\\times n$の可逆行列としよう。その時、$A^{-1}$と$B^{-1}$が存在する。まず、$B^{-1}A^{-1}$を$AB$の右に掛けてみよう。それは次のようになる。\n$$ \\begin{align*} (AB)(B^{-1}A^{-1}) \u0026amp;= ABB^{-1}A^{-1} \\\\ \u0026amp;= AI_{n}A^{-1} = AA^{-1} \\\\ \u0026amp;= I_{n}\\end{align*} $$\n左に掛けると、次のようになる。\n$$ \\begin{align*} (B^{-1}A^{-1})(AB) \u0026amp;= B^{-1}A^{-1}AB \\\\ \u0026amp;= B^{-1}I_{n}B = B^{-1}B \\\\ \u0026amp;= I_{n}\\end{align*} $$\n従って、$AB$は可逆行列であり、その逆行列は$B^{-1}A^{-1}$である。\n■\n(d') これは**(d)**の帰結として成立する。\n■\n(e) $AB$の逆行列を$C$としよう。すると、$ABC=I_{n}$が成り立つ。従って、(c)により、$A$は可逆であり、$A^{-1}=BC$が成り立つ。また、$CAB=I_{n}$であるため、$B$も可逆であり、$B^{-1}=CA$が成り立つ。\n(f) 2つの行列を掛け合わせて単位行列が出るか確認すればよい。転置行列の性質により、次のようになる。\n$$ A^{T} \\left( A^{-1} \\right)^{T} = \\left( A^{-1} A \\right) ^{T} = I^{T} = I $$\n$$ \\left( A^{-1} \\right)^{T} A^{T} = \\left( A A^{-1} \\right)^{T} = I^{T} = I $$\n従って\n$$ \\left( A^{T} \\right)^{-1} = \\left( A^{-1} \\right)^{T} $$\n■\n","id":3003,"permalink":"https://freshrimpsushi.github.io/jp/posts/3003/","tags":null,"title":"逆行列、可逆行列"},{"categories":"줄리아","contents":"ガイド ジュリアを使っている人なら、サーバーを含む複数のオペレーティングシステムやコンピューターを使うことに慣れている可能性が高い。ファイル入出力がある場合、開発環境が変わるたびにそのパスを設定するのはとても面倒くさいだろう。これを解決してくれるのが@__DIR__マクロだ。例えば、次のようなジュリアのコードファイルがあるとしよう。\n基本的に、ターミナルから実行するとき、pwd()と@__DIR__は以下のように区別されないように見える。\nこれらの違いは、アトムなどのIDE(統合開発環境)を使う時に出る。pwd()は単に現在の作業ディレクトリを返すのに対し、@__DIR__は実際のコードファイルがある場所を教えてくれるのがその違いだ。複雑で繰り返しの多い作業をしていると、作業ディレクトリをこっちに変えたり、あっちに変えたりすることは多いが、実行されているコードファイルの場所が変わることはあまりないから、便利に使える。\n","id":1935,"permalink":"https://freshrimpsushi.github.io/jp/posts/1935/","tags":null,"title":"ジュリアで実行されるコードファイルの位置を確認する方法"},{"categories":"줄리아","contents":"ガイド Juliaでは、並列計算を日常的に使用するため、場合によってはコンピューターの全てのリソースを計算に集中させる必要がある。スレッド数を変更する方法はいくつかあるが、最もスタティックで便利な方法は、環境変数を編集することだ。\nステップ1. システム環境変数の編集\nCtrl + Alt + T を押してターミナルを開き、gedit ~/.bashrcと入力する。そうすると、以下のように環境変数を編集できるウィンドウが表示される。\nステップ2. 修正\n一番下にexport JULIA_NUM_THREADS=5を追加する。スクリーンショットで指示されている場所に希望のスレッド数を記入すると修正される。適切なスレッド数はコンピュータのスペックによって異なるが、我々は例を扱っているので、偶然に決まることがないように$5$個に修正してみよう。\nステップ3. 確認\nusing Base.Threads\rnthreads() 上記のコードをJuliaコンソールで実行して確認してみよう。\n","id":1937,"permalink":"https://freshrimpsushi.github.io/jp/posts/1937/","tags":null,"title":"Linux上のJuliaでの並列計算に使用するスレッド数の変更方法"},{"categories":"줄리아","contents":"コード julia\u0026gt; f(x) = 2x + 1\rf (generic function with 1 method)\rjulia\u0026gt; g(x) = x^2\rg (generic function with 1 method)\rjulia\u0026gt; (g ∘ f)(3)\r49 説明 Juliaでは、関数の合成はプログラミングでのパイプオペレーターに似ている。この合成の最大の利点は、数学者が式をコードとして表現しやすくなることだ。上の例は、以下の数式をコードに翻訳したものに過ぎない。\n$$ f(x) := 2x + 1 \\\\ g(x) := x^2 \\\\ (g \\circ f) (3) $$\n最近の多くの言語がそうであるように、関数をファーストクラスオブジェクトとして扱うことは同じだが、純粋数学で関数空間を扱うように、もう少し極端な哲学を経て文法にまで発展したと見ることができる。ちなみに、関数合成演算子はtexの文法そのままに\\circを入力することで使用できる。\n","id":1942,"permalink":"https://freshrimpsushi.github.io/jp/posts/1942/","tags":null,"title":"ジュリアで合成関数を使用する方法"},{"categories":"행렬대수","contents":"定義1 数を次のように長方形の形に並べたものを行列matrixという。\n$$ A=\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} $$\n並べた各々の数をエントリーentryまたは要素elementと呼ぶ。横の列を行row、縦の列を列columnという。また、ある行列が$m$行と$n$列を持つ場合、その行列のサイズを$m \\times n$と表す。\n上の例では、行列$A$は2行と3列を持ち、サイズは$2\\times 3$である。ここで注意すべき点は、$\\times$が乗算を意味するわけではないということである。サイズは必ず総行数と列数が明らかになるように$2\\times 3$のように表記しなければならず、絶対に$6$と書いてはいけない。ちなみに'$2 \\times 3$行列'は [ツーバイスリー行列]と読む。\n表記法 行列は主に下のように角括弧[]または丸括弧()で表記されるが、どちらの表現も一般的に見られる。ただし、手で書くときは丸括弧を使うとキレイに書きにくい。また、2次元、3次元空間の座標を表記するときとは違い、成分と成分の間にカンマ(,)を書かないのが基本である。\n$$ A=\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} \\quad A=\\begin{pmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{pmatrix} $$\n通常、行列は大文字で、成分は小文字で表記する。例えば、行列$A$の1行目3列目の成分は$3$であり、次のように表記される。\n$$ a_{13}=3 $$\n最初の下付き添え字は行の位置を、2番目の下付き添え字は列の位置を示す。同様に、$i$行目、$j$列目の成分が$a_{ij}$である行列を$\\begin{bmatrix} a_{ij} \\end{bmatrix}$のように表記する。$A$の$(i,j)$成分は$[A]_{ij}$と表記される。\n$$ A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\end{bmatrix} = \\begin{bmatrix} a_{ij} \\end{bmatrix},\\qquad [A]_{ij} = a_{ij} $$\nすべての$m\\times n$行列の集合を次のように表記する。\n$$ M_{m \\times n} $$\nサイズが$m\\times n$で成分が実数$\\mathbb{R}$、複素数$\\mathbb{C}$の行列の集合は、それぞれ次のように表記される。\n$$ M_{m\\times n}(\\mathbb{R}),\\quad M_{m \\times n}(\\mathbb{C}) $$\nもう少し抽象的に、成分が体$F$の$n \\times n$行列の集合を$M_{m \\times n}(F)$と表記する。\n列ベクトルと行ベクトル ベクトルとは、数を横または縦に並べたものをいう。この点を考えると、ある行列は列ベクトルまたは行ベクトルを並べたものと見ることができる。例として続けて使用されてきた行列$A$を見てみよう。\n$$ A= \\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} $$\n$A$の各列は列ベクトル$\\begin{bmatrix} 10 \\\\ 0 \\end{bmatrix}$、$\\begin{bmatrix} 0 \\\\ 8 \\end{bmatrix}$、$\\begin{bmatrix} 3 \\\\ 22 \\end{bmatrix}$で構成されていると考えられる。または、各行が行ベクトル$\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\end{bmatrix}$、$\\begin{bmatrix} 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix}$で構成されていると見ることができる。\nJim Hefferon, Linear Algebra(4th Edition). 2020, p15\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1955,"permalink":"https://freshrimpsushi.github.io/jp/posts/1955/","tags":null,"title":"行列の定義"},{"categories":"줄리아","contents":"コード もともとさくらすし店では、もっと詳しい説明を加えることが多いが、ジュリアでアニメGIFを作るのがどれほど簡単かを強調するために、できるだけ説明を短くする。\nランダムウォークをシミュレーションすることはさておき、上のようなアニメGIFを作ることは、言語によってはとても難しく、大変なことがある。しかし、ジュリアでは@animateマクロとgif()関数を使用することで、信じられないほど簡単にアニメGIFを作ることができる。原理は単純だ。ループの前にマクロを付け、ループを回してその都度フレームを直接描くだけだ。そうして集めたフレームを変数に入れ、gif()関数に入れればそれだけである。fpsオプションでは、秒間フレーム数を指定してアニメGIFの速さを調整できる。\nusing Plots\rrandom\\_walk = cumsum(rand(100).-.5)\ranim = @animate for t in 1:100\rplot(random\\_walk[1:t], legend = :none)\rend; gif(anim, \u0026#34;example.gif\u0026#34;, fps = 10) 別のパスを指定しなければ、ドキュメントに保存されることに注意しよう。これをうまく利用すれば、下に示すような素晴らしいアニメGIFを作ることもできる。\n","id":1863,"permalink":"https://freshrimpsushi.github.io/jp/posts/1863/","tags":null,"title":"ジュリアでGIFを作る方法"},{"categories":"줄리아","contents":"概要 距離行列Distance Matrixは、パーティクルダイナミクスParticle DynamicsやムービングエージェントMoving Agentベースのシミュレーションなどによく使用されるが、実際には準備された関数がなく、自分で計算するコードを書くことは大変なことが多い。Juliaでは、pairwise()やDistancesパッケージのEuclidean()関数を使用して、以下のように簡単に距離行列を計算できる1。\ndimsオプションを使用すると、行と列の方向を指定できる。見ての通り、$\\mathbb{R}^{5 \\times 3}$行列が与えられたときに、$5$次元ベクトルの$3$個の距離を計算したり、$3$次元ベクトルの$5$個の距離を計算することができる。\nコード using Distances\rcoordinate = [2 3 4; 5 1 3; 1 7 5; 1 7 6; 2 4 3]\rpairwise(Euclidean(), coordinate; dims=1)\rpairwise(Euclidean(), coordinate; dims=2) 上記のコードを実行した結果は、以下の通りである。\njulia\u0026gt; using Distances\rjulia\u0026gt; coordinate = [2 3 4; 5 1 3; 1 7 5; 1 7 6; 2 4 3]\r5×3 Array{Int64,2}:\r2 3 4\r5 1 3\r1 7 5\r1 7 6\r2 4 3\rjulia\u0026gt; pairwise(Euclidean(), coordinate; dims=1)\r5×5 Array{Float64,2}:\r0.0 3.74166 4.24264 4.58258 1.41421\r3.74166 0.0 7.48331 7.81025 4.24264\r4.24264 7.48331 0.0 1.0 3.74166\r4.58258 7.81025 1.0 0.0 4.3589\r1.41421 4.24264 3.74166 4.3589 0.0\rjulia\u0026gt; pairwise(Euclidean(), coordinate; dims=2)\r3×3 Array{Float64,2}:\r0.0 9.64365 7.07107\r9.64365 0.0 3.31662\r7.07107 3.31662 0.0 最適化 距離行列計算の最適化方法 https://discourse.julialang.org/t/pairwise-distances-from-a-single-column-or-vector/29415/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1799,"permalink":"https://freshrimpsushi.github.io/jp/posts/1799/","tags":null,"title":"ジュリアで距離行列を計算する方法"},{"categories":"줄리아","contents":"コード サイズ指定 julia\u0026gt; empty = Array{Float64, 2}(undef, 3, 4)\r3×4 Array{Float64,2}:\r3.39519e-313 3.18299e-313 4.66839e-313 1.061e-313\r4.03179e-313 5.51719e-313 1.6976e-313 4.24399e-314\r2.97079e-313 4.66839e-313 7.00259e-313 5.0e-324 上のコードを実行すると、空の配列が作成される。たまに1.76297e-315のような変な値が入っているように見えるが、これは0に非常に近い値で、初期化には大きな問題がない。\nArray{X, Y}(undef, ...)はデータ型XでY次元配列を、該当するデータ型の未定値でサイズ...だけ埋めた配列になる。ここでのポイントはundefだ。\n可変配列 一次元配列の場合、括弧の中に何も入れずに、簡単に空の配列を作ることができる。\njulia\u0026gt; empty = Array{Float64, 1}()\rFloat64[]\rjulia\u0026gt; empty = Array{Float64, 2}()\rERROR: MethodError: no method matching Array{Float64,2}()\rClosest candidates are:\rArray{Float64,2}(::UndefInitializer, ::Int64, ::Int64) where T at boot.jl:408\rArray{Float64,2}(::UndefInitializer, ::Int64...) where {T, N} at boot.jl:412\rArray{Float64,2}(::UndefInitializer, ::Tuple{Int64,Int64}) where T at boot.jl:416\r...\rStacktrace:\r[1] top-level scope at REPL[85]:1 しかし、同じ方法で二次元配列を作ろうとすると、上述のようにMethodErrorが発生する。もちろん自然な二次元配列ではないが、一次元配列の一次元配列を作るような形で空の配列を作ることは可能だが、速度の面ではネイティブな文法をそのまま使うことを推奨する。\njulia\u0026gt; empty = Array{Array{Float64, 1}, 1}()\rArray{Float64,1}[] もっと簡単な方法 下のように波括弧を使うと、もっと簡単に配列を作ることができる。\njulia\u0026gt; empty = Float64[]\rFloat64[]\rjulia\u0026gt; empty = Array{Float64, 1}[]\rArray{Float64,1}[]\rjulia\u0026gt; empty = Array{Float64, 2}[]\rArray{Float64,2}[] 環境 OS: Windows julia: v1.5.0 ","id":1797,"permalink":"https://freshrimpsushi.github.io/jp/posts/1797/","tags":null,"title":"ジュリアで空の配列を作成する方法"},{"categories":"동역학","contents":"정리 $2$次元の多様体 $\\mathcal{P}$ と関数 $f,g \\in C^{r} \\left( \\mathcal{P} \\right)$ に対して、次のようなベクトル場が微分方程式として与えられているとする。 $$ \\dot{x} = f(x,y) \\\\ \\dot{y} = g(x,y) $$ $\\mathcal{M}$ このベクトル場が有限個の不動点を持つ不変集合である場合、$p \\in \\mathcal{M}$ のオメガリミットセット $\\omega (p)$ は次の三つのうちの一つを満たす：\n(1): $\\omega (p)$ は単元素集合である。つまり、ただ一つの不動点のみを含む。 (2): $\\omega (p)$ は閉じた軌道である。 (3): $\\omega (p)$ は有限個の不動点 $p_{1} , \\cdots , p_{n}$ のいくつかの $i,j \\in [1,n]$ に対して次を満たす軌道 $\\gamma$ から構成される。 $$ \\alpha ( \\gamma ) = \\left\\{ p_{i} \\right\\} \\\\ \\omega ( \\gamma ) = \\left\\{ p_{j} \\right\\} $$ 説明 距離空間は当然 $T_{1}$ 空間であり、$T_{1}$ では単元素集合が閉集合であることが保証されるため、$\\omega (p) = \\left\\{ p \\right\\}$ は当然閉軌道と言えるが、ステートメントの文脈上、一つの不動点のみを含む場合は別のものとして区別しよう。\n実際、ポアンカレ・ベンディクソンの定理では、カオスというものは定義される必要もなく、カオスが起こらないというステートメント自体は系に近い。定理が言うのは単にオメガリミットセットの分類であり、それが正確に我々が知っているもので構成されているため、カオスが起こることはないという事実が導かれるのである。しかし、このような定理があることにより、カオス理論の関心は$2$次元を確実に超えることができるようになる。\n定理の直観的な理解はそれほど難しくない。$\\mathcal{M}$ がバウンドされていない場合はそもそもカオスにならず、バウンドされている場合は永遠に伸び続けることはできず、フローが狭まって回るか広がって回るかしなければならない。しかし、$3$次元とは異なり、$2$次元では線が平面を二つの領域に分けてしまうため、そのうちの一つの領域を諦めなければならない状況が常に起こる。これは、限られた空間である$\\mathcal{M}$ の残りの部分を時が経つにつれて捨てていくようなものと見ることができる。まだ通過していない領域を使用するために、すでに通過したフローを通過しようとすると、その瞬間それは閉じた軌道となり、結局閉じた軌道か不動点に収束することになり、カオスを引き起こすことはできない。\n証明 1 戦略: ポアンカレの名前が付いた定理らしく、位相数学的である。$\\mathcal{P}$ 内部で連続で連結なアーク(continuous, connected arc)を一つ$\\Sigma$としよう。\n$\\Sigma$のすべての点での法線ベクトルとベクトル場の内積が$0$でなく、符号も変わらない場合、$\\Sigma$は$\\mathcal{P}$上のベクトル場を横切るTransverseと言う。この概念は一点に対してのみ考えることもできるが、その点ではベクトル場と$\\Sigma$は接触しないだろう。フローの観点からは、一点で出会うだけでなく、$\\Sigma$を貫通することになる。\n与えられたベクトル場で作られるフローを$\\phi_{t}$、フロー$\\phi_{t}$の下で一点$p \\in \\mathcal{P}$の正の時間に対する軌道を$O_{+}(p)$として表そう。一点$p_{i}$がフロー$\\phi_{t}$の下で時間$t$の流れに従って$p_{j}$に到達するまでの軌道を$\\widehat{p_{i} p_{j}} \\subset O_{+} (p)$として表そう。また、オメガリミットセットを表す$\\omega ( \\cdot )$は元々与えられた一点に対して定義されていたが、ある集合$X$に対する$\\omega \\left( X \\right)$は次のように考えればよい。\n$$ \\omega (X) := \\bigcup_{x \\in X} \\omega (x) $$ これはアルファリミットセット$\\alpha ( \\cdot )$も同様に定義されたと考えればよい。\nその他、次のような補助定理を続けて使用することになる。\n補助定理(オメガリミットセットの性質): 全体空間がユークリッド空間$X = \\mathbb{R}^{n}$であり、フロー$\\phi_{t} ( \\cdot )$でコンパクト不変集合$\\mathcal{M}$の一点$p \\in \\mathcal{M}$が与えられているとする：\n[1]: $\\omega (p) \\ne \\emptyset$ [2]: $\\omega (p)$は閉集合である。 [3]: $\\omega (p)$はフローに不変である。つまり、$\\omega (p)$は軌道の合併である。 [4]: $\\omega (p)$は連結空間である。 まず、$2$次元で生じるオメガリミットセットは何らかの面積を持つ形状ではないため、以降言及されるオメガリミットセットは何らかの曲線の形状と考えればよい。\nPart 1.\n$\\Sigma \\subset \\mathcal{M}$がベクトル場を横切るアークである場合、$\\mathcal{M}$が$2$次元ベクトル場の不変集合であるため、$\\Sigma$がベクトル場の流れに逆らって$\\mathcal{M}$の外へ出ることはできない。したがって、任意の$p \\in \\mathcal{M}$に対して、$O_{+} (p)$と$\\Sigma$が交わる$k$番目の点を$p_{k}$とすると、$p_{k}\\subset \\widehat{p_{k-1} p_{k+1}} \\subset O_{+} (p)$でなければならない。つまり、フローが$\\mathcal{M}$内部に向かって収束していくが、その過程で$\\Sigma$と交わる交点が近づいてまた遠ざかることは起こらないということである。\nPart 2. $p \\in \\mathcal{M}$のオメガリミットセット$\\omega (p)$は$\\Sigma$と多くとも一点でしか交差しない。\n背理法で示す。$\\omega (p)$と$\\Sigma$が異なる二点$q , \\overline{q}$で交差すると仮定してみる。\nその場合、オメガリミットセットの定義により、$n \\to \\infty$のとき $$ q_{n} \\to q \\\\ \\overline{q}_{n} \\to \\overline{q} $$ を満たすシーケンス$\\left\\{ q_n \\right\\}_{n \\in \\mathbb{N}} , \\left\\{ \\overline{q}_n \\right\\}_{n \\in \\mathbb{N}} \\subset O_{+} (p)$が存在する。しかし、Part 1によれば、これらの交点はある順序$p_{1} , p_{2} , \\cdots$に並べられるため、仮定に矛盾する。したがって、$\\omega (p)$と$\\Sigma$は最初から交差しないか、交差するとしてもただ一点でのみ交差する。[ 注: トーラスの場合には、この論理をそのまま適用することはできないが、いくつかの部分に分けて$\\mathcal{M}$と同じ形状にすることで同じ結論を得ることができる。 ]\nPart 3. $\\omega (p)$が不動点を含まない場合、閉じた軌道である。\n$q \\in \\omega (p)$の軌道$O_{+}(q)$が閉じた軌道であることを示し、その後$\\omega (p) = O_{+} (q)$であることを示せばよい。\nPart 3-1. 軌道$O_{+}(q)$は閉じている。 点$x \\in \\omega (q)$を一つ選んでみると、補助定理[2]により$\\omega (p)$が閉じており、不動点を持たない軌道の合併であるため、$x$も不動点であってはならない。$p,q$が混乱しないように、仮定は$\\omega (p)$が不動点を持たないことであり、$x$は$x \\in \\omega (q)$であるため、必ずしも$x \\in \\omega (p)$である保証はないが、いずれにせよ不動点ではないと言える。この不動点でない一点$x$のベクトル場を横切る一つのアーク$\\Sigma_{x}$を選ぼう。**Part 1.**によれば、$\\Sigma_{x}$と$O_{+} (q)$の交点のシーケンス$\\left\\{ q_{n} \\right\\}_{n \\in \\mathbb{N}}$は$n \\to \\infty$のとき$q_{n} \\to x$であり、$x \\in \\mathcal{M}$であるため、**Part 2.**により$\\forall n \\in \\mathbb{N}$に対して$q_{n} = x$でなければならない。$x$は不動点ではないため、$O_{+} (q)$が$x$と交差する場合、離れた後に再び戻って交差しなければならない。ここで$x \\in \\omega (q)$としたので、$O_{+}(q)$は$x$に近づいて止まることなく、実際に$x$と交差し、したがって$O_{+}(q)$は閉じた軌道となる。 Part 3-2. $O_{+}(q) = \\omega (p)$ 点$q \\in \\omega (p)$からベクトル場を横切る一つのアーク$\\Sigma_{q}$を選んでみると、Part 2により$\\omega (p)$と$\\Sigma_{q}$はただ$q$でのみ出会う。補助定理[3]により$\\omega (p)$は軌道の合併であるため、$q \\in \\omega (p)$であれば$O_{+} (q) \\subset \\omega (p)$であるが、$\\omega (p)$は不動点を含まず連結空間であるため、正確に$O_{+}(q) = \\omega (p)$でなければならない。 Part 4. $p \\in \\mathcal{M}$に対して異なる$p_{1} , p_{2} \\in \\omega (p)$がベクトル場の不動点である場合、$\\alpha (\\gamma) = \\left\\{ p_{1} \\right\\}$と$\\omega (\\gamma) = \\left\\{ p_{2} \\right\\}$を満たす軌道$\\gamma \\subset \\omega (p)$は多くても一つしか存在しない。\n背理法で示す。二点を結ぶ異なる二つの軌道があれば、その二つの軌道の間に面積を持つ何らかの領域$\\mathcal{K}$が生じるだろう、そこから矛盾を導く。次の条件を満たす異なる二つの軌道$\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$が存在すると仮定しよう。 $$ \\alpha \\left( \\gamma_{i} \\right) = \\left\\{ p_{1} \\right\\} \\\\ \\omega \\left( \\gamma_{i} \\right) = \\left\\{ p_{2} \\right\\} $$ これらの軌道から一点ずつ$q_{1} \\in \\gamma_{1}$、$q_{2} \\in \\gamma_{2}$を選び、$q_{1}$と$q_{2}$からベクトル場を横切るアークを$\\Sigma_{1}, \\Sigma_{2}$として選ぶ。\n$\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$であるため、Part 2により、$O_{+} (p)$が$\\Sigma_{1}$と一点$a$で交差した後、$\\Sigma_{2}$は一点$b$で交差するとしよう。すると、$2$次元多様体上で次のような経路に囲まれた部分領域$\\color{red}{\\mathcal{K}}$が生じるだろう。\n$$ q_{1} \\overset{\\Sigma_{1}}{\\to} a \\overset{ O_{+} (p) }{ \\to } b \\overset{\\Sigma_{2}}{\\to} q_{2} \\overset{ \\omega (\\gamma) }{ \\to } p_{2} \\overset{ \\gamma_{1} }{ \\gets } q_{1} $$ 記法$\\displaystyle x \\overset{\\mathcal{C}}{\\to} y$は点$x,y$がカーブ$\\mathcal{C}$に繋がれたことを意味して使用された。$\\color{red}{\\mathcal{K}}$から始まったフローは$\\gamma_{1} , \\gamma_{2}$を超えることができないため、$\\color{red}{\\mathcal{K}}$は不変集合となる。しかし、$p$から始まった軌道$O_{+}(p)$が$\\color{red}{\\mathcal{K}}$に入ると、二度と出ることはできないということは、$\\gamma_{1}$や$\\gamma_{2}$が$\\omega (p)$に属することはできないということである。例えば$\\gamma_{2}$を考えると、$q_{2} \\overset{\\gamma_{2}}{\\to} p_{2}$は$\\omega (p)$に属することができるかもしれないが、その前部分である$p_{1} \\overset{\\gamma_{2}}{\\to} q_{2}$には行けない。したがって、$\\gamma_{2}$全体が$\\omega (p)$に属するという主張はできず、$\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$と矛盾する。\nPart 5.\nこのパートでは、不動点でない点を正則点Regular Pointと呼ぼう。必ずしもこのパートに限定する必要はないが、不動点の否定という文脈が頻繁に出てこないのに対し、正則Regularという表現は学問を問わず頻繁に使用されるため、注意や警告なしに使用すると大きな混乱を引き起こす可能性があるためである。\nケース1. $\\omega (p)$が不動点のみを持つ場合 $\\mathcal{M}$は有限個の不動点を持ち、$\\omega (p)$は連結空間であるため、ただ一つの不動点のみを持たなければならない。 ケース2. $\\omega (p)$が正則点のみを持つ場合 Part 3により、$\\omega (p)$は閉じた軌道である。 ケース3. $\\omega (p)$が不動点と正則点の両方を持つ場合 正則点のみからなる軌道$\\gamma \\subset \\omega (p)$を考える。 $\\gamma$は正則点のみからなっているため、Part 3により、$\\omega ( \\gamma )$と$\\alpha (\\gamma)$は閉じた軌道であるが、その一方で不動点を持たなければならない。しかし、補助定理[4]により、$\\omega ( \\gamma )$は連結空間であるため、閉じた軌道と不動点が離れていることはできず、不動点は閉じた軌道のどこかに位置していなければならないが、これはすなわち$\\omega ( \\gamma )$が不動点のみを含む単元素集合であるということである。同じ議論を$\\alpha ( \\gamma )$で繰り返すと、$\\omega (p)$のすべての正則点はそのオメガリミットポイントとアルファリミットポイントとして不動点を持つことがわかる。\n$\\omega (p)$は上記の三つのケースのいずれかに属していなければならない。これで証明は終わりである。\n■\nWiggins. (2003). Introduction to Applied Nonlinear Dynamical Systems and Chaos Second Edition(2nd Edition): 118~120.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1788,"permalink":"https://freshrimpsushi.github.io/jp/posts/1788/","tags":null,"title":"プアンカレ-ベンディクソン定理の証明"},{"categories":"힐베르트공간","contents":"定義1 $X$をベクトル空間とする。$\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in X$ 及び $\\alpha, \\beta \\in \\mathbb{C}$(または $\\mathbb{R}$)に対して、次の条件を満たす関数\n$$ \\langle \\cdot , \\cdot \\rangle : X \\times X \\to \\mathbb{C} $$\nを内積と定義し、$\\left( X, \\langle \\cdot ,\\cdot \\rangle \\right)$を内積空間と呼ぶ。\n線形性: $$\\langle \\alpha \\mathbf{x} + \\beta \\mathbf{y} ,\\mathbf{z} \\rangle =\\alpha \\langle \\mathbf{x},\\mathbf{z}\\rangle + \\beta \\langle \\mathbf{y},\\mathbf{z}\\rangle$$ 共役対称性: $$\\langle \\mathbf{x},\\mathbf{y} \\rangle = \\overline{ \\langle \\mathbf{y},\\mathbf{x} \\rangle}$$ 正定値性: $$\\langle \\mathbf{x},\\mathbf{x} \\rangle \\ge 0 \\quad \\text{and} \\quad \\langle \\mathbf{x},\\mathbf{x} \\rangle = 0\\iff \\mathbf{x}=0$$ 説明 線形性と共役対称性から、次の式が得られる。\n$$ \\begin{align*} \\langle \\mathbf{x},\\alpha \\mathbf{y}+\\beta \\mathbf{z} \\rangle =\u0026amp;\\ \\overline{\\langle \\alpha \\mathbf{y}+\\beta \\mathbf{z} ,\\mathbf{x} \\rangle} \\\\ =\u0026amp;\\ \\overline{\\alpha \\langle \\mathbf{y},\\mathbf{x} \\rangle +\\beta \\langle \\mathbf{z},\\mathbf{x} \\rangle} \\\\ =\u0026amp;\\ \\overline{\\alpha}\\overline{\\langle \\mathbf{y},\\mathbf{x} \\rangle}+\\overline{\\beta} \\overline{\\langle \\mathbf{z},\\mathbf{x} \\rangle} \\\\ =\u0026amp;\\ \\overline{\\alpha}\\langle \\mathbf{x},\\mathbf{y}\\rangle + \\overline{\\beta} \\langle \\mathbf{x},\\mathbf{z} \\rangle \\end{align*} $$\nこれは二番目の要素に対してアンチリニアであることを意味する。物理学、工学等では、内積が少しだけ異なって定義されることもある。たとえば、第一成分に対してアンチリニアで、第二成分に対してはリニアに定義されることもある。一方で内積空間では、以下のようにコーシー・シュワルツの不等式が成り立つ。\n$(X, \\langle \\cdot ,\\cdot \\rangle)$が内積空間であるとする。すると、以下の不等式が成り立ち、これをコーシー・シュワルツの不等式と呼ぶ。\n$$ \\left| \\langle \\mathbf{x},\\mathbf{y} \\rangle \\right| \\le \\langle \\mathbf{x},\\mathbf{x} \\rangle^{1/2} \\langle \\mathbf{y},\\mathbf{y} \\rangle ^{1/2},\\quad \\forall \\mathbf{x},\\mathbf{y} \\in X $$\nまた、内積から以下のようにノルムを定義できる。\n$$ \\left\\| \\mathbf{x} \\right\\| := \\sqrt{\\langle \\mathbf{x},\\mathbf{x} \\rangle},\\quad \\mathbf{x}\\in X $$\nこのように内積から自然に導出されたノルムをassociated normとも呼ぶ。また、ノルムが与えられた場合には、ノルムから距離を定義できるので、距離空間の性質である完備性についても語ることができる。完備な内積空間をヒルベルト空間と呼ぶ。\n特性 コーシー・シュワルツの不等式: 任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\left| \\langle \\mathbf{x},\\mathbf{y} \\rangle \\right| \\le \\left\\| \\mathbf{x} \\right\\| \\left\\| \\mathbf{y} \\right\\| $$\n平行四辺形の法則: 任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\left\\| \\mathbf{x} + \\mathbf{y} \\right\\|^{2} + \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|^{2} = 2 \\left( \\left\\| \\mathbf{x} \\right\\| ^{2}+ \\left\\| \\mathbf{y} \\right\\| ^{2} \\right) $$\n複素内積空間における偏極アイデンティティ: 複素内積空間 $X$及び任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\langle \\mathbf{x},\\mathbf{y} \\rangle = \\frac{1}{4} \\Big( \\left\\| \\mathbf{x} + \\mathbf{y} \\right\\|^{2} - \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|^{2} + i \\left( \\left\\| \\mathbf{x} + iy \\right\\|^{2} - \\left\\| \\mathbf{x} - iy \\right\\|^{2} \\right) \\Big) $$\n実内積空間における偏極アイデンティティ: 実内積空間 $X$及び任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\langle \\mathbf{x},\\mathbf{y}\\rangle = \\frac{1}{4} \\left( \\left\\| \\mathbf{x}+\\mathbf{y} \\right\\|^{2} - \\left\\| \\mathbf{x}-\\mathbf{y} \\right\\| ^{2} \\right) $$\nノルム対内積: 任意の $\\mathbf{x} \\in X$に対して、\n$$ \\left\\| \\mathbf{x} \\right\\| =\\sup \\left\\{ \\left| \\langle \\mathbf{x},\\mathbf{y} \\rangle \\right| : \\mathbf{y}\\in X, \\left\\| \\mathbf{y} \\right\\| =1 \\right\\} $$\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p61-65\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1842,"permalink":"https://freshrimpsushi.github.io/jp/posts/1842/","tags":null,"title":"内積空間"},{"categories":"수리통계학","contents":"定義 12 確率変数 $X$ のサンプル $X_{1} , \\cdots , X_{n}$ の関数 $T$ を統計量Statisticと言う。 $$ T := T \\left( X_{1} , \\cdots , X_{n} \\right) $$ $X$ の分布関数が $f(x; \\theta)$ あるいは $p(x; \\theta)$ のように表される時、$T$ が $\\theta$ を把握するための統計量であれば、$T$ を$\\theta$ の推定量Estimatorと言う。 統計量の確率分布をサンプリング分布Sampling Distributionと言う。 説明 推定量(Estimator)の実現を推定値Estimateと言う。パラメータは通常スカラー $\\theta \\in \\mathbb{R}$ の場合が多く、この場合は$T$を$\\theta$の点推定量Point Estimatorとも言う。例えば、正規分布 $N \\left( \\mu, \\sigma^{2} \\right)$ に従うランダムサンプルがあるとき、母平均 $\\mu$ の推定量は次の通りである。 $$ \\overline{X} := {{ 1 } \\over { n }} \\sum_{k = 1}^{n} X_{k} $$ 実際のデータ $x_{1} , \\cdots , x_{n}$ がある場合、$\\mu$ の推定値は次の通りである。 $$ \\overline{x} := {{ 1 } \\over { n }} \\sum_{k = 1}^{n} x_{k} $$\n参考文献 基礎統計学での統計量 基礎統計学ではサンプルの関数とは言わずもっと直感的に「計算されたもの」という表現を使って定義している。本質的には同じ意味だが、数学に馴染みのない新入生や人にとってより良い定義かもしれない。\n統計量の例 平均や分散などを除外して、「統計量」と名前についている統計量には以下のような例がある:\n十分統計量: 分布内のパラメータに関するすべての情報を持つ統計量である。 最小十分統計量: 特定の条件を満たす十分統計量である。 補助統計量: 十分統計量とは反対に、パラメータに関するどんな情報も持たない統計量である。 完全統計量: 統計量としてこのような性質を持っているべきだと言われる時、実際にその性質を持つ統計量である。 推定量の例 推定量には以下のような例がある:\n不偏推定量: 偏りを持たない推定量である。 一致推定量: 極限概念でパラメータを的確に推定する推定量である。 最尤推定量: 尤度Likelihoodを最大化する推定量である。 効率的推定量: 統計量の分散と関連した推定量である。 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCasella. (2001). Statistical Inference(2nd Edition): p211.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1730,"permalink":"https://freshrimpsushi.github.io/jp/posts/1730/","tags":null,"title":"数理統計学における統計量と推定量"},{"categories":"수리통계학","contents":"定義 1 確率変数 $X$の実際に引き出された結果を実現Realizationと言い、普通、小文字の$x$で表す。 確率変数$X$と同じ確率分布からサンプルサイズSample Size$n$分の確率変数をサンプルSampleと呼び、次のように表す。 $$ X_{1} , X_{2} , \\cdots , X_{n} $$ 確率変数$X_{1} , \\cdots , X_{n}$がiidであれば、サイズ$n$のランダムサンプルと呼ぶ。 説明 これらの定義により、数理統計学は実際の統計解析との接点を持つことになる。ランダムサンプルの実現を扱うことは、統計解析に該当し、数理統計学はそのデータをどのように扱うかについての大きな灯台となる。関心を持つデータ、得たい結論、利用する方法は異なるかもしれないが、その下には数理統計学が理論的な基盤として支えていなければならない。\n実際には、数理統計学の教科書を離れると、実現という表現はあまり使われず、通常はその実現を直接言及する言葉がある。たとえばランダムサンプルの実現は、値、データ、観測値などと呼ばれる。しかし、確率変数は大文字、データは小文字という慣習は、ほとんどすべての統計学教科書で守られている。\n参照 統計入門におけるデータの定義 学部1〜2年生向けの統計学入門では、実験単位や試行などで実際に測定された結果の集まりをデータと定義している。\nHogg et al. (2013).《数理統計学入門》(第7版): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1715,"permalink":"https://freshrimpsushi.github.io/jp/posts/1715/","tags":null,"title":"数理統計学におけるランダムサンプリング"},{"categories":"수리물리","contents":"定義 ベクトル関数 $\\mathbf{F}(x,y,z)=F_{x}\\hat{\\mathbf{x}}+F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$について、以下のようなスカラー関数を$\\mathbf{F}$のダイバージェンスdivergence, 発散と定義し、$\\nabla \\cdot \\mathbf{F}$と表記する。\n$$ \\begin{equation} \\nabla \\cdot \\mathbf{F} := \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} \\label{divergence} \\end{equation} $$\n説明 幾何学的に$\\nabla \\cdot \\mathbf{F}\u0026gt;0$の場合、$\\mathbf{F}$が広がり出る、外へ出る形をしていることを意味し、$\\nabla \\cdot \\mathbf{F}\u0026lt;0$の場合は$\\mathbf{F}$が集まる、内へ入る形をしていることを意味し、$\\nabla \\cdot \\mathbf{F}=0$の場合は$\\mathbf{F}$が広がりも集まりもしない、出入りの量が同じ形をしていることを意味する。\nダイバージェンスは発散と翻訳される。生エビ寿司屋では、勾配をグラディエント、回転をカールと使うため、統一感のために発散ではなくダイバージェンスと表記する。\n定義で$\\dfrac{ \\partial F_{x}}{ \\partial x} + \\dfrac{ \\partial F_{y}}{ \\partial y }+ \\dfrac{ \\partial F_{z}}{ \\partial z}$という値を$\\nabla \\cdot \\mathbf{F}$で表記することに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体に何か意味を持つと考えると$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積と外積と間違えやすい。だから$\\nabla$は単なる便利な表記法程度にしか理解しないほうがよく、グラディエント、ダイバージェンス、カールをまとめてデル演算子たちと呼んだり、むしろデル演算子=グラディエントと考えるほうがよいかもしれない。詳細は以下で続ける。\n注意点 $\\nabla \\cdot \\mathbf{F}$は$\\nabla$と$\\mathbf{F}$の内積ではない $\\nabla \\cdot \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の内積ではない。\r内積は基本的に二つのベクトル同士の演算である。$\\nabla \\cdot \\mathbf{F}$を内積と考えることは、$\\nabla$を以下のようなベクトルと見なすことである。\n$$ \\nabla \\overset{?}{=}\\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} +\\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}}+\\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{ \\partial }{ \\partial x},\\ \\dfrac{ \\partial }{ \\partial y},\\ \\dfrac{ \\partial }{ \\partial z} \\right) $$\n確かにこのように考えると、次のようにダイバージェンスの定義$(1)$通りに計算がうまくいくので便利であることは事実である。\n$$ \\nabla \\cdot \\mathbf{F} = \\left( \\dfrac{ \\partial }{ \\partial x},\\ \\dfrac{ \\partial }{ \\partial y},\\ \\dfrac{ \\partial }{ \\partial z} \\right) \\cdot \\left( F_{x}, F_{y}, F_{z} \\right) = \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} $$\nしかし、これが実際に内積であれば、内積は交換法則が成立するため、以下のような等式が成立するという奇妙な結論になる。\n$$ \\mathbf{F} \\cdot \\nabla = F_{x} \\dfrac{\\partial }{\\partial x} + F_{y} \\dfrac{\\partial }{\\partial y} + F_{z} \\dfrac{\\partial }{\\partial z} \\overset{?}{=} \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} = \\nabla \\cdot \\mathbf{F} $$\n実際には$\\nabla \\cdot$の実体はベクトル関数 $\\mathbf{F}(x,y,z)$をスカラー関数 $\\frac{ \\partial F_{x}(x,y,z)}{ \\partial x} + \\frac{ \\partial F_{y}(x,y,z)}{ \\partial y }+ \\frac{ \\partial F_{z}(x,y,z)}{ \\partial z}$に対応させる演算子である。これが何を意味するかというと、$\\operatorname{div}$という関数を次のように定義してみよう。\n$$ \\operatorname{div}(\\mathbf{F}) := \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z},\\qquad \\mathbf{F} = (F_{x}, F_{y}, F_{z}) $$\nここには内積だとかそういう言葉は一切ない。$\\operatorname{div}$という関数は単に変数$\\mathbf{F}$が代入されるたびに$\\dfrac{ \\partial F_{x}}{ \\partial x} + \\dfrac{ \\partial F_{y}}{ \\partial y }+ \\dfrac{ \\partial F_{z}}{ \\partial z}$を与える関数である。これを定義してみると、$\\nabla$というものを$\\nabla = \\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} +\\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}}+\\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}}$と同じベクトルと考えると$\\operatorname{div}$の関数値を表記するのが非常に便利で直感的であることがわかる。だから$\\operatorname{div}(\\mathbf{F})$と表記する代わりに、$\\nabla \\cdot \\mathbf{F}$と表記するのである。実際に専門の数学の教科書では、ダイバージェンスを$\\operatorname{div}$と表記することが容易に見つかるが、これは物理学部と異なり3次元ベクトルを直感的に扱わないためであると考えられる。\nでは$\\nabla \\cdot \\mathbf{F}$を交換法則が成立しない内積として考えてはいけないのか？ いけない。なぜなら$\\nabla \\cdot \\mathbf{F}$で$\\nabla \\cdot$自体が関数(演算子)であり$\\mathbf{F}$が変数である。一方で$\\mathbf{F} \\cdot \\nabla$はそのもの自体が関数(演算子)であるためである。したがって$\\nabla \\cdot \\mathbf{F}$は関数$\\nabla \\cdot$の関数値であり、$\\mathbf{F} \\cdot \\nabla$は（まだ変数が代入されていない）関数である。具体的に$\\mathbf{F} \\cdot \\nabla$という表記は次のような関数$f$を直感的\nに簡単に表記したものである。$f$はベクトル関数$\\mathbf{A}$を変数とする演算子であり、$\\mathbf{A}$の各成分に$\\left( F_{x}\\dfrac{\\partial }{\\partial x} + F_{y}\\dfrac{\\partial }{\\partial y} + F_{z}\\dfrac{\\partial }{\\partial z} \\right)$を適用する関数である。\n$$ \\begin{align*} f (\\mathbf{A}) \\overset{\\text{definition}}{=}\u0026amp; \\left( F_{x}\\dfrac{\\partial A_{x}}{\\partial x} + F_{y}\\dfrac{\\partial A_{x}}{\\partial y} + F_{z}\\dfrac{\\partial A_{x}}{\\partial z} \\right)\\hat{\\mathbf{x}} \\\\ \u0026amp;\\quad+ \\left( F_{x}\\dfrac{\\partial A_{y}}{\\partial x} + F_{y}\\dfrac{\\partial A_{y}}{\\partial y} + F_{z}\\dfrac{\\partial A_{y}}{\\partial z} \\right)\\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad+ \\left( F_{x}\\dfrac{\\partial A_{z}}{\\partial x} + F_{y}\\dfrac{\\partial A_{z}}{\\partial y} + F_{z}\\dfrac{\\partial A_{z}}{\\partial z} \\right)\\hat{\\mathbf{z}} \\\\ \\overset{\\text{notation}}{=}\u0026amp; (\\mathbf{F}\\cdot \\nabla) (\\mathbf{A}) \\end{align*} $$\n何かスカラー関数$\\phi$が変数としてある場合、次のような演算子と考える。\n$$ \\begin{align*} f (\\phi) \\overset{\\text{definition}}{=}\u0026amp; F_{x}\\dfrac{\\partial \\phi}{\\partial x} + F_{y}\\dfrac{\\partial \\phi}{\\partial y} + F_{z}\\dfrac{\\partial \\phi}{\\partial z} \\\\ \\overset{\\text{notation}}{=}\u0026amp; (\\mathbf{F}\\cdot \\nabla) (\\phi) \\end{align*} $$\nしたがって$\\nabla \\cdot \\mathbf{F}$と$\\mathbf{F}\\cdot \\nabla$は$\\nabla$と$\\mathbf{F}$の内積として理解してはいけず、$\\nabla \\cdot$と$\\mathbf{F} \\cdot \\nabla$自体を一つの関数として考えなければならない。これはもちろんダイバージェンスに限った説明ではなく、グラディエント$\\nabla f$やカール$\\nabla \\times \\mathbf{F}$も同様に理解しなければならない。\n導出 まず以下のように3次元空間で微小体積を考えてみよう。\n今、我々の目的は$\\mathbf{F}$がその微小体積内の各座標でどのように見えるかを知ることである。現実に例えるならば$\\mathbf{F}$が熱であればどの方向に、どの速度で流れているかを、$\\mathbf{F}$が水であればこれが蛇口から出ている水なのか、排水溝に入っていく水なのかを知りたいということである。まず$x$軸方向だけを計算してみよう。$\\mathbf{F}$が$d\\mathbf{a}_{1}$を通過する量は二つのベクトルの内積で求めることができる。\n$$ \\begin{align} \\mathbf{F}(x+dx) \\cdot d\\mathbf{a}_{1} \u0026amp;= \\left( F_{x}(x+dx)\\hat{\\mathbf{x}}+F_{y}(x+dx)\\hat{\\mathbf{y}}+F_{z}(x+dx)\\hat{\\mathbf{z}} \\right) \\cdot dydz\\hat{\\mathbf{x}} \\nonumber \\\\ \u0026amp;= F_{x}(x+dx)dydz \\end{align} $$\n$F_{x}(x+dx)dydz \u0026gt;0$の場合、$\\mathbf{F}$が微小体積を抜け出る量であり、$F_{x}(x+dx)dydz\u0026lt;0$の場合は$\\mathbf{F}$が微小体積に入る量である。同様に$\\mathbf{F}$が$d\\mathbf{a}_{2}$を抜け出る量は以下のようである。\n$$ \\begin{equation} \\mathbf{F}(x) \\cdot d \\mathbf{a}_{2} = F_{x}(x)\\hat{\\mathbf{x}} \\cdot(-dydz\\hat{\\mathbf{x}})=-F_{x}(x)dydz \\end{equation} $$\nしたがって$(2) + (3)$は微小体積での$\\mathbf{F}$の$x$軸方向の流入量（流出量）である。\n$$ \\begin{align*} (2) + (3) \u0026amp;=\\left[ F_{x}(x+dx) -F_{x}(x)\\right]dydz \\\\ \u0026amp;= \\frac{F_{x}(x+dx) -F_{x}(x) }{dx}dxdydz \\end{align*} $$\nしかし$dx$が微小距離であるため、$\\dfrac{F_{x}(x+dx) -F_{x}(x) }{dx}\\approx \\dfrac{ \\partial F_{x}}{ \\partial x }$と同様に近似できる。したがって$\\mathbf{F}$が$x$軸方向へ微小体積に入るまたは出る量は以下のように表される。\n$$ \\frac{ \\partial F_{x}}{ \\partial x}dxdydz $$ 同様に$y$軸方向、$z$軸方向について計算すると以下の結果を得る。\n$$ \\frac{ \\partial F_{y}}{ \\partial y}dxdydz \\quad \\text{and} \\quad \\frac{ \\partial F_{z}}{ \\partial z}dxdydz $$\nこれを全て足すと$\\mathbf{F}$が微小体積に入るまたは出る量となり、$dxdydz$で割ると単位体積あたりの流入量（流出量）となる。\n$$ \\frac{ \\partial F_{x}}{ \\partial x}+\\frac{ \\partial F_{y}}{ \\partial y}+\\frac{ \\partial F_{z}}{ \\partial z} $$\nこれからこれを$\\mathbf{F}$のダイバージェンスと呼び、$\\nabla \\cdot \\mathbf{F}$と表記しよう。\n$$ \\nabla \\cdot \\mathbf{F} := \\frac{ \\partial F_{x}}{ \\partial x}+\\frac{ \\partial F_{y}}{ \\partial y}+\\frac{ \\partial F_{z}}{ \\partial z} $$\n■\n導出される過程を見ても分かるように、上で述べた通り$\\nabla \\cdot \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の内積ではない。この点に注意しよう。\n関連する公式 線形性:\n積の規則:\n$$ \\nabla \\cdot (f\\mathbf{A}) = f(\\nabla \\cdot \\mathbf{A}) + \\mathbf{A} \\cdot (\\nabla f) $$ $$ \\nabla \\cdot (\\mathbf{A} \\times \\mathbf{B}) = \\mathbf{B} \\cdot (\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\cdot (\\nabla \\times \\mathbf{B}) $$\n二階導関数:\n$$ \\nabla \\cdot (\\nabla T) = \\dfrac{\\partial^{2} T}{\\partial x^{2}} + \\dfrac{\\partial ^{2} T} {\\partial y^{2}} + \\dfrac{\\partial ^{2} T}{\\partial z^{2}} $$ $$ \\nabla (\\nabla \\cdot \\mathbf{A} ) $$ $$ \\nabla \\cdot (\\nabla \\times \\mathbf{A})=0 $$\nガウスの定理 (発散定理)\n$$ \\int_\\mathcal{V} \\nabla \\cdot \\mathbf{ F} dV = \\oint _\\mathcal{S} \\mathbf{F} \\cdot d \\mathbf{S} $$\n積分公式\n$$ \\int_{\\mathcal{V}} \\left[ T \\nabla^{2} U + (\\nabla T) \\cdot (\\nabla U) \\right] d \\tau = \\oint_{\\mathcal{S}} (T \\nabla U) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left( T \\nabla^{2} U - U \\nabla^{2} T \\right) d \\tau = \\oint_{\\mathcal{S}} \\left( T \\nabla U - U \\nabla T \\right) \\cdot d \\mathbf{a} $$\n部分積分\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$\n参照 デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ ","id":1796,"permalink":"https://freshrimpsushi.github.io/jp/posts/1796/","tags":null,"title":"直交座標系におけるベクトル関数の発散"},{"categories":"바나흐공간","contents":"関数解析学は英語でfunctional analysisです。function analysisではなくfunctionalは一体何を意味しているのか、疑問に思うかもしれません。まず、functionalという単語を見ると、function+alで構成されているように見えます。つまり、functionの形容詞形のように見え、この感じで解釈すれば、functionalは「関数的な（もの）」や「関数のような（もの）」程度の意味を含んでいるように思われます。この感じは、別の名前であるgeneralized functionでも見つけることができます。なぜ関数ではなく、関数のようなものと名付けられたのか、以下のfunctionalの一般的な定義を見ながら考えてみましょう。\nベクトル空間$X$に対して以下のような関数$f$をfunctionalと呼びます。\n$$ f : X \\to \\mathbb{C} $$\nこの定義を見て「定義上は$f$はfunctionなのに、なぜfunctionalという名前を付けたのか？」と思うかもしれません。上記のような条件を満たす関数に特別な名前を付けるのは納得できるものの、なぜその名前がfunctional（関数的なもの）でなければならないのかは、納得がいかないかもしれません。\n関数の定義によれば、上の$f$は関数ですが、なぜ「関数的なもの」という名前を付けたのかを理解するためには、関数解析学が発展し始めた時代の数学について知る必要があります。現代に生まれ、数学を学ぶ人は、関数を以下のように知っています。\n全ての$x_{1}, x_{2} \\in X$に対して$x_{1} = x_{2} \\implies f(x_{1}) = f(x_{2})$を満たす$f(x_{1})$と$f(x_{2})$が$Y$に存在するならば、対応$f$を以下のように表記し、$X$から$Y$への関数と言います。\n$$ f : X \\to Y $$\n集合論で厳密に定義すると、以下のようになります。\n空集合でない二つの集合$X$、$Y$が与えられたとします。二項関係$f \\subset (X,Y)$が以下を満たすならば、関数と呼び、$f : X \\to Y$のように表されます。\n$$ (x ,y_{1}) \\in f \\land (x,y_{2}) \\in f \\implies y_{1} = y_{2} $$\n上の定義からわかるように、二つの集合$X$、$Y$には何の条件もありません。したがって、$X$が$\\mathbb{R}$であろうと、関数空間であろうと、何の問題もありません。しかし、19世紀後半の数学者にとって、関数とは上記のようではありませんでした。当時の数学者は、関数を値から値へのマッピング、つまり、$f:\\mathbb{R} \\to \\mathbb{R}$に限定して考えていました1。値を与えると、ルールに従って別の値を与える「公式」のように扱ったのです。これは、中学校で初めて関数を学ぶ時の受け入れ方と同じです。\nなぜ関数をそのように考えたのか疑問に思うかもしれませんが、ある意味で当然です。関数の厳密な定義は、上記で見たように集合論を通じて作られました。現代集合論の創始者であるカントールが1845年生まれであることを思い出せば、19世紀後半〜20世紀初頭の数学者までもが関数を数字と数字の間の公式程度に考えていた事実は全く不思議ではありません。元々、関数をなぜ機能functionと呼んだのでしょうか。\n以下のような関数を考えてみましょう。\n微分可能な関数$f$に対して、閉区間$[a,b]$での曲線$y=f(x)$の長さは以下のようになります。\n$$ L(f)=\\int_{a}^{b} \\sqrt{1+ f^{\\prime}(x)^{2}}dx $$\n当時の数学者にとって、$L$は関数ではありませんでした。値を値へ送るのではなく、関数を値へ送るためです。したがって、$L$を「関数の関数」と呼ばなければならないのですが、関数ではないため、用語に関する曖昧さが存在しました。そのため、ボルテラVolterraはfunctions of linesとも呼びました。この時、フランスの数学者アダマールHadamardがこのような「関数ではないが関数のような関数の関数」をfoncionnellesと呼ぶことを初めて提案しました。これは後に英語表現でfunctionalとなりました。\nもちろん、集合論で関数を厳密に定義した後は、functionalもfunctionになりましたが、定義域が関数空間であることが明確になるため、functionalという表現が続けられたようです。集合の集合、set of setsをcollectionやfamillyと表現するのと同様です。functionalがfunctionであることに概念的な問題はなくても、関数の関数という表現は混乱を招きやすいため、functionalという用語が生き残らなかったのではないでしょうか。この学問の名前がfunctional analysisと固まったのも影響があるでしょう。functionalは後に一般化され、ベクトル空間から複素数空間へのマッピングを意味するようになりました。\nDistribution Theory 上述のように、最初にfunctionalは関数ではないが関数のようなものを指すために作られた言葉です。最終的に関数が集合論を通じて定義された後は、functionalも関数になりましたが。面白いことに、このようなfunctionalが実際に「関数ではないが関数のようなもの」を説明するために使われるようになったのです。ディラックのデルタ関数は、ポアソンとコーシーがフーリエ解析を研究する過程で最初に考案され、理論物理学者のポール・ディラックが量子力学で広く使用されることで有名になりました。2 3 デルタ関数のナイーブnaiveな定義は、以下の条件を満たす関数です。\n$$ \\delta (x)=\\begin{cases} \\infty, \u0026amp; x=0 \\\\ 0, \u0026amp; x\\ne 0\\end{cases} \\quad \\\u0026amp; \\quad \\int_{-\\infty}^{\\infty}\\delta (x)dx=1 $$\nしかし、発散するということは値ではなく状態であるため、厳密に言えばデルタ関数は関数ではありませんでした。しかし、単に関数として扱って良い結果を得られました。1935年4にこの概念を知ったフランスの数学者ローラン-モワーズ・シュワルツLaurent-moise Schwartzが15年間の研究の末、1950年5にTheorie des distributionsという本でデルタ関数を数学的に厳密に定義しました。6 いくつかの良い条件を持つスムース関数をテスト関数と呼び、テスト関数の空間を$\\mathcal{D}$と表記します。distributionは$\\mathcal{D}$から$\\mathbb{C}$への写像であり、これはfunctionalになります。functionalという名前は、当初は関数だと思われていなかったものに付けられましたが、時間が経つにつれて、実際に関数ではないが関数のように扱うものに対する理論を立てるのに使われるようになりました。驚くべき偶然です。\nhttps://courses.mai.liu.se/GU/TATM85/FA-history.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Dirac_delta_function\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Dirac_delta_function\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n21歳でした。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n36歳でした。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://horizon.kias.re.kr/11905/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1780,"permalink":"https://freshrimpsushi.github.io/jp/posts/1780/","tags":null,"title":"ファンクショナルがファンクショナルと名付けられた理由"},{"categories":"수리물리","contents":"定義 スカラー関数 $f=f(x,y,z)$に対して、以下のようなベクトル関数を $f$のグラディエントgradient, 勾配と定義し、$\\nabla f$と表記する。\n$$ \\nabla f := \\frac{ \\partial f}{ \\partial x }\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) $$\n説明 グラディエントは勾配、坂、水勾配などと翻訳される。坂、水勾配はグラディエントの古い翻訳で、最近ではあまり使われない。また、坂は勾配の漢字語であるため、勾配と同じ意味である。グラディエントは実際にベクトルであるため、勾配という言葉はグラディエントが持つ意味をすべて含むには不十分であるように思われる。生しらす寿司店では、勾配という言葉の代わりにグラディエントと統一する。\n幾何学的には $\\nabla f$は $f$が最も急激に変化する方向を意味する。つまり点 $(x,y,z)$で $f$の増加率が最も大きい方向はベクトル $\\left( \\dfrac{\\partial f(x,y,z)}{\\partial x}, \\dfrac{\\partial f(x,y,z)}{\\partial y}, \\dfrac{\\partial f(x,y,z)}{\\partial z} \\right)$であるということである。これは微分係数を多次元に拡張したものに過ぎない。$f$が増加していれば微分係数が正、$f$が減少していれば微分係数が負であるという概念と同じである。\n一方で定義で $\\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right)$という値を $\\nabla f$と表記するとしたことに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体に何か意味を持つと考えると$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積や外積と誤解するのにちょうどよい。したがって、$\\nabla$は単なる便利な表記法としてのみ理解するべきであり、グラディエント、ダイバージェンス、カールをまとめてデル演算子と呼んだり、デル演算子=グラディエントと考える方が良いかもしれない。詳細は以下で続く。\n注意点 $\\nabla f$は $\\nabla$と $f$の積ではない グラディエントを理解する上で重要なのは、$\\nabla f$がベクトル $\\nabla = (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})$とスカラー $f$の積ではないという事実である。もちろん、そう考えると直感的で良さそうだが、実際は逆である。$\\nabla$を $(\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\\npartial y}, \\frac{\\partial }{\\partial z})$というベクトルとして説明することで、ベクトルとスカラーの積のように見えるようにするのである。もし $\\nabla f$がベクトル $\\nabla$とスカラー $f$の積であれば、ベクトルとスカラーの積は交換可能であるため、次のような奇妙な数式が成り立つことになる。\n$$ \\nabla f = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) \\overset{?}{=} \\left( f\\dfrac{\\partial }{\\partial x}, f\\dfrac{\\partial }{\\partial y}, f\\dfrac{\\partial }{\\partial z} \\right) = f\\nabla $$\nこの奇妙な数式が飛び出したのは、実際には $\\nabla$はベクトルではなく、$\\nabla f$はベクトルとスカラーの積ではないためである。$\\nabla$はベクトルではなく、$f(x,y,z)$というスカラー関数を $\\left( \\frac{\\partial f(x,y,z)}{\\partial x}, \\frac{\\partial f(x,y,z)}{\\partial y}, \\frac{\\partial f(x,y,z)}{\\partial z} \\right)$というベクトル関数に対応させる演算子である。関数自体を変数とする $\\operatorname{grad}$という関数を次のように定義してみよう。\n$$ \\begin{equation} \\operatorname{grad} (f) = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right), \\quad f=f(x,y,z) \\end{equation} $$\nこの定義から、ベクトルとスカラーの積という説明は必要ない。$\\operatorname{grad}$は単に変数として $f$が入力されると、$(1)$の規則に従って関数値を持つ関数（演算子）に過ぎない。しかし $\\operatorname{grad} (f)$の関数値をよく見ると、$\\operatorname{grad} = \\nabla$と表記し、これを $\\nabla = (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})$というベクトルとして説明すると直感的で便利な表記法になるのである。\nこれは本質的な意味を正確に説明するものではないが、計算や理解の便利さのために使われる他の表記法には微分のライプニッツ表記法がある。$\\dfrac{dy}{dx}$という表記法を採用し、分数のように扱うと、変化率という意味を理解するのに便利で、無意識に掛け算や約分などの計算をしても実際の結果とピタリと合う。しかし、皆さんは $\\dfrac{dy}{dx}$は分数ではないことを知っている。そう見えるだけで、そう扱うと計算が便利なだけである。$\\nabla f$も同様に、ベクトルとスカラーの積に見えるだけで、そう扱うと計算が便利なのであって、実際にそうであるわけではない。\nでは $f\\nabla$は何か？ 上の説明に従えば、$\\nabla$は一つの関数であるため、$\\nabla f = \\nabla(f)$は $\\nabla$という関数に $f$という変数を代入したときに得られる関数値である。一方で $f \\nabla$はそれ自体が一つの関数であり、$g$という関数を変数として代入したときに以下のように関数値を対応させる関数（演算子）である。\n$$ (f\\nabla) (g) = f\\left( \\dfrac{\\partial g}{\\partial x}, \\dfrac{\\partial g}{\\partial y}, \\dfrac{\\partial g}{\\partial z} \\right) = \\left( f\\dfrac{\\partial g}{\\partial x}, f\\dfrac{\\partial g}{\\partial y}, f\\dfrac{\\partial g}{\\partial z} \\right) $$\nもちろん、$f \\nabla g$\nという関数値を見たときには、$f \\nabla$に $g$を代入したものと考えても良いし、スカラー関数 $f$とベクトル関数 $\\nabla g$の積と見ても良い。\n導出 1次元 上の図を見よう。$f_{1}$の点 $x=2$での微分係数は $4$である。$4$という値は関数 $f_{1}$が点 $x=2$でどれほど傾いているかを教えてくれる量だけでなく、それだけではない。$4$の前にある $+$という符号が $f_{1}$のグラフは $x$が増加する方向に増加するという事実も教えてくれる。したがって、微分係数 $4$は単なるスカラーではなく、1次元ベクトル $4\\hat{\\mathbf{x}}$として理解すべきである。\n同様に、$f_{2}$の $x=2$での微分係数は $-3$であり、これは傾きの程度が $3$であることと、$x$が増加する方向に進むと $f_{2}$のグラフが減少するという意味も含んでいる。つまり、符号を方向と考えた場合、微分係数の方向は関数のグラフが大きくなる方向を向いているという話である。別の言い方をすると、微分係数が指し示す方向に進めば、グラフの頂点を見つけることができるということである。\n3次元に拡張する前に、$y$の $x$での微分係数 $\\dfrac{ d y}{ d x}=a$をまるで分数のように扱えることを思い出そう。これは微分を数学的に厳密に扱う方法ではないが、幾何学的な意味を理解する上での助けとなり、その利点がある。ライプニッツは $dy$、$dx$を $y$と $x$の非常に小さな変化量、微分素と考え、その変化量の比率を微分係数と呼んだ。1\n$$ dy=adx $$\n余談だが、このように考えるとなぜ $a$を微分 \u0026lsquo;係数\u0026rsquo;と呼ぶのか理解できる。\n3次元 ここで3次元スカラー関数 $f=f(x,y,z)$と位置ベクトル $\\mathbf{r}=x\\hat{\\mathbf{x}}+y\\hat{\\mathbf{y}}+z\\hat{\\mathbf{z}}$が与えられたとしよう。$f$の変化量は全微分で表される。\n$$ \\begin{equation} df=\\frac{ \\partial f}{ \\partial x }dx + \\frac{ \\partial f}{ \\partial y}dy+\\frac{ \\partial f}{ \\partial z}dz \\end{equation} $$\n$\\mathbf{r}$の変化量は以下のようである。\n$$ d\\mathbf{r}=dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}} $$\nこれで1次元の時と同じように、$df$と $d\\mathbf{r}$の間の比率を表す何かを探してみよう。しかし、$df$はスカラーで $d\\mathbf{r}$はベクトルであるため、その \u0026lsquo;何か\u0026rsquo;はベクトルであり、$df$はその何かと $d\\mathbf{r}$の内積として表現されることを想像できる。したがって\n、とりあえずその何かを $\\mathbf{a}=a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}}$と表記して、以下のように表現してみよう。\n$$ \\begin{align*} df=\\mathbf{a}\\cdot d\\mathbf{r}\u0026amp;=(a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}})\\cdot(dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}}) \\\\ \u0026amp;= a_{1}dx+a_{2}dy+a_{3}dz \\end{align*} $$\nこれを $(2)$と比較すると、以下の結果を得る。\n$$ \\mathbf{a}=\\frac{ \\partial f}{ \\partial x}\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} $$\nこれから、このベクトル $\\mathbf{a}$を $\\nabla f$と表記し、$f$のグラディエントと呼ぶことにしよう。グラディエントの方向は関数 $f$のグラフが最も大きく増加する方向を指し、その大きさはその程度を示す。\n関連する公式 線形性:\n$$ \\nabla (f + g) = \\nabla f + \\nabla g $$\n積の規則:\n$$ \\nabla{(fg)}=f\\nabla{g}+g\\nabla{f} $$ $$ \\nabla(\\mathbf{A} \\cdot \\mathbf{B}) = \\mathbf{A} \\times (\\nabla \\times \\mathbf{B}) + \\mathbf{B} \\times (\\nabla \\times \\mathbf{A})+(\\mathbf{A} \\cdot \\nabla)\\mathbf{B}+(\\mathbf{B} \\cdot \\nabla) \\mathbf{A} $$\n2次導関数:\n$$ \\nabla \\cdot (\\nabla T) = \\dfrac{\\partial^{2} T}{\\partial x^{2}} + \\dfrac{\\partial ^{2} T} {\\partial y^{2}} + \\dfrac{\\partial ^{2} T}{\\partial z^{2}} $$ $$ \\nabla \\times (\\nabla T)= \\mathbf{0} $$ $$\\nabla (\\nabla \\cdot \\mathbf{A} ) $$\n勾配の基本定理\n$$ T(b)-T(a) = \\int _{a}^{b} (\\nabla T) \\cdot d\\mathbf{l} $$\n積分公式\n$$ \\int_{\\mathcal{V}} (\\nabla T) d \\tau = \\oint_{\\mathcal{S}} T d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left[ T \\nabla^{2} U + (\\nabla T) \\cdot (\\nabla U) \\right] d \\tau = \\oint_{\\mathcal{S}} (T \\nabla U) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left( T \\nabla^{2} U - U \\nabla^{2} T \\right) d \\tau = \\oint_{\\mathcal{S}} \\left( T \\nabla U - U \\nabla T \\right) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\n部分積分\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$ $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n一緒に見る デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ https://pomp.tistory.com/941?category=37772\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1778,"permalink":"https://freshrimpsushi.github.io/jp/posts/1778/","tags":null,"title":"3次元デカルト座標系におけるスカラー関数の勾配"},{"categories":"분포이론","contents":"定義 平均 $\\mu \\in \\mathbb{R}$ と分散 $\\sigma^{2} \u0026gt; 0$ に対し、以下のような確率密度関数を持つ連続確率分布 $N \\left( \\mu,\\sigma^{2} \\right)$ を正規分布Normal Distributionという。\n$$ f(x) = {{ 1 } \\over { \\sqrt{2 \\pi} \\sigma }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( {{ x - \\mu } \\over { \\sigma }} \\right)^{2} \\right] \\qquad, x \\in \\mathbb{R} $$\n特に、以下のような確率密度関数を持つ正規分布 $N \\left( 0,1^{2} \\right)$ を標準正規分布という。\n$$ f(z) = {{ 1 } \\over { \\sqrt{2 \\pi} }} \\exp \\left[ - {{ z^{2} } \\over { 2 }} \\right] $$\n説明 正規分布の別名はガウス分布Gaussian Distributionだ。歴史的には、ガウスが1809年に最小二乗法に関する研究で正規分布を紹介したことで広く知られるようになった。正規分布の本質を最初に理解した人がガウスであると断言することはできないが、ガウスは正規分布の異名を持つにふさわしい人物である。\n1794年、たった17歳のガウスは、日常や研究で遭遇する測定値から真値を求める方法についてのインスピレーションを得た。ガウスは頻繁に通る道で自分の歩数を数え、そのデータを収集してグラフに描き、鐘型の曲線を得た。それはヒストグラムという概念がなかった時代の発見だったが、ガウス自身はこれらの正規分布と最小二乗法の概念がすでに広く知られていて、誰もが使用している技術だと思っていた1。まさに圧倒的な天才性だ。また、正規分布に関連する多くの計算にガウス積分が使われることもある。\nその後、正規分布は広く研究され、科学全般になくてはならないツールになった。それほど馴染み深いため、一般人は統計学とは、結局のところ、データが正規分布に従うと仮定して平均分散を求めるだけではないかという誤解を持つことがある。そのような過小評価が統計学への進学につながった場合、それは残念なことだが、非専門家にはその程度の説明で十分かもしれない。それほど正規分布が重要で強力であるという意味での話だ。\n基本性質 モーメント生成関数 [1]: $$m(t) = \\exp \\left( \\mu t + {{ \\sigma^{2} t^{2} } \\over { 2 }} \\right) \\qquad , t \\in \\mathbb{R}$$ 平均と分散 [2] : $X \\sim N\\left( \\mu , \\sigma^{2} \\right)$ の場合 $$ \\begin{align*} E(X) =\u0026amp; \\mu \\\\ \\text{Var} (X) =\u0026amp; \\sigma^{2} \\end{align*} $$ 十分統計量と最尤推定量 [3] : 正規分布に従うランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim N \\left( \\mu , \\sigma^{2} \\right)$ が与えられたとする。 十分統計量 $T$ と最尤推定量 $\\left( \\hat{\\mu}, \\widehat{\\sigma^{2}} \\right)$ は以下の通りである。 $$ \\begin{align*} T =\u0026amp; \\left( \\sum_{k} X_{k}, \\sum_{k} X_{k}^{2} \\right) \\\\ \\left( \\hat{\\mu}, \\widehat{\\sigma^{2}} \\right) =\u0026amp; \\left( {{ 1 } \\over { n }} \\sum_{k} X_{k}, {{ 1 } \\over { n }} \\sum_{k} \\left( X_{k} - \\overline{X} \\right)^{2} \\right) \\end{align*} $$\nエントロピー [4] : (自然対数を選んだ場合)正規分布のエントロピーは以下の通りである。 $$ H = \\ln \\sqrt{2\\pi e \\sigma^{2}} $$ 定理 正規分布の具体的な重要性を長々と説明する必要はなく、以下のように単に定理を並べるだけで十分である。見てみよう。\n中心極限定理 [a]: $\\left\\{ X_{k} \\right\\}_{k=1}^{n}$ がiid 確率変数で、確率分布 $\\left( \\mu, \\sigma^2 \\right) $ に従うとすると、$n \\to \\infty$ の時 $$ \\sqrt{n} {{ \\overline{X}_n - \\mu } \\over {\\sigma}} \\overset{D}{\\to} N (0,1) $$ カイ二乗分布との関係 [b]: $X \\sim N(\\mu,\\sigma ^2)$ ならば $$ V=\\left( { X - \\mu \\over \\sigma} \\right) ^2 \\sim \\chi ^2 (1) $$ 二項分布の極限分布としての標準正規分布の導出 [c]: $X_i \\sim B(1,p)$ であり、$Y_n = X_1 + X_2 + \\cdots + X_n$ の場合、$Y_n \\sim B(n,p)$ である $$ { { Y_n - np } \\over {\\sqrt{ np(1-p) } } }\\overset{D}{\\to} N(0,1) $$ ポアソン分布の極限分布としての標準正規分布の導出 [d]: $X_{n} \\sim \\text{Poi} \\left( n \\right)$ であり、$\\displaystyle Y_{n} := {{ X_{n} - n } \\over { \\sqrt{n} }}$ の場合 $$ Y_{n} \\overset{D}{\\to} N(0,1) $$ スチューデントのt分布の極限分布としての標準正規分布の導出 [e]: $T_n \\sim t(n)$ の場合 $$ T_n \\ \\overset{D}{\\to} N(0,1) $$ 正規分布とカイ二乗分布からt分布の導出 [f]: 二つの確率変数 $W,V$ が独立であり、$W \\sim N(0,1)$、$V \\sim \\chi^{2} (r)$ の場合 $$ T = { {W} \\over {\\sqrt{V/r} } } \\sim t(r) $$ 証明 戦略：ガウス積分が使用できるように指数部分を完全平方形にして標準正規分布のモーメント生成関数から導き出し、置換により正規分布のモーメント生成関数を得る。\nガウス積分: $$ \\int_{-\\infty}^{\\infty} e^{-x^2} dx= \\sqrt{\\pi} $$\n[1] 2 $\\displaystyle Z := {{ X - \\mu } \\over { \\sigma }} \\sim N(0,1)$ とすると、そのモーメント生成関数は\n$$ \\begin{align*} m_{Z}(t) =\u0026amp; \\int_{-\\infty}^{\\infty} \\exp (tz) {{ 1 } \\over { \\sqrt{2 \\pi} }} \\exp \\left[ - {{ 1 } \\over { 2 }} z^{2} \\right] dz \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - {{ 1 } \\over { 2 }} z^{2} + tz \\right] dz \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( z - t \\right)^{2} + {{ t^{2} } \\over { 2 }} \\right] dz \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( z - t \\right)^{2} \\right] \\exp \\left[ {{ t^{2} } \\over { 2 }} \\right] dz \\\\ =\u0026amp; \\exp \\left[ {{ t^{2} } \\over { 2 }} \\right] {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - w^{2} \\right] \\sqrt{2} dw \\\\ =\u0026amp; \\exp \\left[ {{ t^{2} } \\over { 2 }} \\right] \\end{align*} $$\nすると、$X \\sim N \\left( \\mu , \\sigma^{2} \\right)$ のモーメント生成関数は\n$$ \\begin{align*} m_{X}(t) =\u0026amp; E \\left[ \\exp ( t X ) \\right] \\\\ =\u0026amp; E \\left[ \\exp \\left( t (\\sigma Z + \\mu) \\right) \\right] \\\\ =\u0026amp; \\exp(\\mu t) E \\left[ \\exp \\left( t \\sigma Z \\right) \\right] \\\\ =\u0026amp; \\exp(\\mu t) \\exp \\left( {{ t^{2} \\sigma^{2} } \\over { 2 }} \\right) \\\\ =\u0026amp; \\exp \\left( \\mu t + {{ \\sigma^{2} t^{2} } \\over { 2 }} \\right) \\end{align*} $$\n■\n[2] モーメント生成関数を使用して直接導く。\n■\n[3] 直接導く。\n■\n[4] 直接導く。\n■\n[a] モーメント法を応用する。\n■\n[b] 確率密度関数を直接導く。ガンマ関数とガンマ分布、カイ二乗分布との関係が使われる。\n■\n[c] 中心極限定理を使用して証明される。\n■\n[d] モーメント生成関数を使用して証明される。\n■\n[e] 難しい。スターリング近似を通じて確率密度関数が収束することを証明する。\n■\n[f] 簡単だが複雑。確率密度関数を直接導く。\n■\nコード 以下はコーシー分布、t分布、コーシー分布の確率密度関数を示すJuliaのコードである。\n@time using LaTeXStrings @time using Distributions @time using Plots cd(@__DIR__) x = -4:0.1:4 plot(x, pdf.(Cauchy(), x), color = :red, label = \u0026#34;Cauchy\u0026#34;, size = (400,300)) plot!(x, pdf.(TDist(3), x), color = :orange, label = \u0026#34;t(3)\u0026#34;, size = (400,300)) plot!(x, pdf.(TDist(30), x), color = :black, linestyle = :dash, label = \u0026#34;t(30)\u0026#34;, size = (400,300)) plot!(x, pdf.(Normal(), x), color = :black, label = \u0026#34;Standard Normal\u0026#34;, size = (400,300)) xlims!(-4,5); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pdf\\,of\\, t}(\\nu)\u0026#34;) png(\u0026#34;pdf\u0026#34;) フーベルト・マニア. (2010). 熱中すること (冷たい数字の世界で絶対的な秩序を見つけ出した、ガウスの伝記): p69~72.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nホッグ他. (2013). Introduction to Mathematical Statistics(第7版): p171~172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1645,"permalink":"https://freshrimpsushi.github.io/jp/posts/1645/","tags":null,"title":"正規分布"},{"categories":"수리물리","contents":"定義 ベクトル関数 $\\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\\hat{\\mathbf{x}} + F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$に対して、以下のようなベクトルを$\\mathbf{F}$のカールcurlと定義し、$\\nabla \\times \\mathbf{F}$と表記する。\n$$ \\begin{align} \\nabla \\times \\mathbf{F} \u0026amp;= \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} \\label{def1} \\\\ \u0026amp;=\\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z}\\end{vmatrix} \\label{def2} \\end{align} $$\n$(2)$は$\\mathbf{F}$のカールを簡単に覚えるための公式である。行列式と考えてそのまま展開すればよい。 説明 カールは回転と翻訳される。しかし、回転という言葉は日常的すぎる上に、カールではなくrotationと誤解される可能性があるため、생새우초밥집では回転の代わりにカールを使用する。\n$\\nabla \\times \\mathbf{F}$は$\\mathbf{F}$という物理量がどの方向に回転しているかを教えてくれるベクトルである。$\\nabla \\times \\mathbf{F}$の方向を軸(親指)にして右手の法則を適用すると、右手が包む方向と$\\mathbf{F}$が回転する方向が一致する。ベクトル$\\nabla \\times \\mathbf{F}$の大きさは回転の程度を示す。\nアインシュタインの表記法とレヴィ-チヴィタ記号を使用すれば、以下のように表すことができる。$\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$と表記するなら、\n$$ \\nabla \\times \\mathbf{F} = \\epsilon_{ijk}\\hat{\\mathbf{e}}_{i}\\nabla_{j}F_{k} $$\n一方、定義で$(1)$という値を$\\nabla \\times \\mathbf{F}$と表記するとしたことに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体が何かの意味を持つと考えると、$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積や外積と誤解することになりかねない。したがって、$\\nabla$は便利な表記法程度にしか理解してはならず、勾配、ダイバージェンス、カールをまとめてデル演算子と呼ぶこともあるし、むしろデル演算子=勾配と考える方が良いかもしれない。詳細は以下で続く。\n注意点 $\\nabla \\times \\mathbf{F}$は$\\nabla$と$\\mathbf{F}$の外積ではない $\\nabla \\times \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の外積ではない。\r単に$\\nabla \\times \\mathbf{F}$は$\\mathbf{F}$に関する何らかの情報を含むベクトルである。$\\nabla$を$\\nabla = \\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} + \\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}} + \\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}}$のようなベクトルと考えて計算すると、結果が$(1)$と完全に一致するため、便宜上$\\nabla \\times \\mathbf{F}$と表記しているだけである。もし$\\nabla$を実際のベクトルと仮定すると、おかしな結果になる。\n二つのベクトル$\\mathbf{A}, \\mathbf{B}$に対して次の式が成り立つ。\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\nもし$\\nabla$が本当にベクトルだったら、上の公式に代入することができ、次の結果が得られるだろう。\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=(\\mathbf{F} \\cdot \\nabla)\\nabla - (\\nabla \\cdot \\nabla)\\mathbf{F} + \\nabla (\\nabla \\cdot \\mathbf{F}) - \\mathbf{F} (\\nabla \\cdot \\nabla) $$\nしかし、正しい結果は次のようになる。\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=\\nabla(\\nabla \\cdot \\mathbf{F})-\\nabla ^{2} \\mathbf{F} $$\n他にも例がある。ベクトルの外積は反交換性を持つため、$\\nabla \\times \\mathbf{F}$が外積であるならば、次の式が成り立つはずだ。\n$$ \\nabla \\times \\mathbf{F} \\overset{?}{=} - \\mathbf{F} \\times \\nabla $$\nしたがって、$\\nabla$はベクトルではなく、$\\nabla \\times \\mathbf{F}$を$\\nabla$と$\\mathbf{F}$の外積ではないことが分かる。ベクトルではなく、$\\nabla \\times$自体を一つの関数と考えるべきだ。このように関数を変数とする関数を物理学では演算子と呼ぶ。\nでは $\\nabla \\times \\mathbf{F}$と$\\mathbf{F} \\times \\nabla$の違いは？ $\\nabla \\times$はベクトル関数を変数とする、次のように定義される演算子である。\n$$ \\nabla \\times (\\mathbf{F}) = \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} $$\nつまり $\\nabla \\times \\mathbf{F}$は $\\nabla \\times$という演算子（関数）に$\\mathbf{F}$という変数を代入したときの関数値である。もちろんこれは再び$(x,y,z)$を変数とするベクトル関数である。$\\nabla \\times \\mathbf{F}$が$\\nabla \\times$の関数値であるのに対し、$\\mathbf{F} \\times \\nabla$はそれ自体が一つの演算子である。よく使われる数式ではないが、定義するなら次のような微分演算子であると言える。\n$$ \\begin{align*} \\mathbf{F} \\times \\nabla \u0026amp;= \\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\end{vmatrix} \\\\ \u0026amp;= \\left( F_{y}\\dfrac{ \\partial }{ \\partial z} - F_{z}\\dfrac{ \\partial }{ \\partial y} \\right)\\hat{\\mathbf{x}} + \\left( F_{z}\\dfrac{ \\partial }{ \\partial x} - F_{x}\\dfrac{ \\partial }{ \\partial z} \\right)\\hat{\\mathbf{y}} + \\left( F_{x}\\dfrac{ \\partial }{ \\partial y} - F_{y}\\dfrac{ \\partial }{ \\partial x} \\right)\\hat{\\mathbf{z}} \\end{align*} $$\n導出 ここで、ベクトル関数が回転する方向（時計回りか反時計回りか）を示す関数について考えてみましょう。重要なのは、回転面内のどの方向も回転の方向を特定できないということです。下の図を見てください。\nベクトル $-\\hat{\\mathbf{x}}$は点 $A$での動きは説明できますが、$B$での動きは説明できません。 ベクトル $\\hat{\\mathbf{y}}$は点 $C$での動きは説明できますが、$D$での動きは説明できません。 ベクトル $\\hat{\\mathbf{x}} + \\hat{\\mathbf{y}}$は経路 $F$を説明できますが、$G$を説明できません。 これは時計回りの場合にも同じです。回転方向を特定するためには回転面を離れる必要があることが理解できるでしょう。実際、これを決定するための良い方法が既にあります。それは、右手の法則を使うことです。右手が巻き込む方向の回転軸を親指の方向として決定します。したがって、$xy$平面で反時計回りに回る回転の軸（方向）は$\\hat{\\mathbf{z}}$であり、時計回りに回る回転の軸（方向）は$-\\hat{\\mathbf{z}}$です。\nそれでは、$\\mathbf{F}$が$xy$平面で反時計回りに回っている場合、$\\hat{\\mathbf{z}}$方向を示す値、つまり正の値を見つけてみましょう。回転は簡単に以下のように四角形で表現しましょう。\n経路①は点 $a$から点 $b$まで動き、$\\mathbf{F}(a) = (1,0,0)$, $\\mathbf{F}(b) = (0,1,0)$としましょう。すると、点 $a$から点 $b$まで$x$は$+1$だけ変化し、$F_{y}$も$+1$だけ変化するので、次のようになります。\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 $$\n同様に、点 $b$から点 $c$までの経路で、$y$は$+1$だけ変化し、$F_{x}$は$-1$だけ変化します。4つの経路すべてを確認すると、\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 \\quad \\text{in path $\\textcircled{1}$, $\\textcircled{3}$} $$\n$$ \\dfrac{\\partial F_{x}}{\\partial y} \\lt 0 \\quad \\text{in path $\\textcircled{2}$, $\\textcircled{4}$} $$\nしたがって、上記のように反時計回りに回転するベクトル $\\mathbf{F}$に対して、以下の値は常に正です。\n$$ \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\gt 0 $$\n逆に、$\\mathbf{F}$が時計回りに回転している場合、上記の値は常に負です。それでは、ベクトル関数 $\\mathbf{F}$を代入すると、$xy$平面で回転する方向と大きさを示す演算子 $\\operatorname{curl}_{xy}$を次のように定義できます。\n$$ \\operatorname{curl}_{xy} (\\mathbf{F}) = \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right) \\hat{\\mathbf{z}} $$\nこの関数の $\\hat{\\mathbf{z}}$ 成分の符号は、$\\mathbf{F}$が$xy$平面で回転する方向を示す。 $+$の場合、$\\mathbf{F}$は$xy$平面で反時計回りに回転する。 $-$の場合、$\\mathbf{F}$は$xy$平面で時計回りに回転する。 $0$の場合、回転しない。 この関数の $\\hat{\\mathbf{z}}$ 成分の大きさは、$\\mathbf{F}$が$xy$平面でどれだけ速く回転しているかを示す。 このような議論を$yz$平面と$zx$平面にも適用することで、$\\mathbf{F}$が3次元空間で回転している方向と大きさを示すベクトル$\\nabla \\times \\mathbf{F}$を次のように定義することができます。\n$$ \\nabla \\times \\mathbf{F} := \\left( \\dfrac{\\partial F_{z}}{\\partial y} - \\dfrac{\\partial F_{y}}{\\partial z} \\right)\\hat{\\mathbf{x}} + \\left( \\dfrac{\\partial F_{x}}{\\partial z} - \\dfrac{\\partial F_{z}}{\\partial x} \\right)\\hat{\\mathbf{y}} + \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right)\\hat{\\mathbf{z}} $$\n■\n関連する公式 リニアリティ: $$ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) = \\nabla \\times \\mathbf{A} + \\nabla \\times \\mathbf{B} $$\n乗算規則:\n$$ \\nabla \\times (f\\mathbf{A}) = f(\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\times (\\nabla f) $$\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\n二次関数:\n$$ \\nabla \\times (\\nabla f) = \\mathbf{0} $$\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F}) = \\nabla (\\nabla \\cdot \\mathbf{F}) - \\nabla^{2} \\mathbf{F} $$\nストークスまとめ $$ \\int_{\\mathcal{S}} (\\nabla \\times \\mathbf{v} )\\cdot d\\mathbf{a} = \\oint_{\\mathcal{P}} \\mathbf{v} \\cdot d\\mathbf{l} $$\n積分式 $$ \\int_{\\mathcal{V}} (\\nabla \\times \\mathbf{v}) d \\tau = - \\oint_{\\mathcal{S}} \\mathbf{v} \\times d \\mathbf{a} $$\n$$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\n部分積分 $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n$$ \\int_{\\mathcal{V}} \\mathbf{B} \\cdot \\left( \\nabla \\times \\mathbf{A} \\right) d\\tau = \\int_{\\mathcal{V}} \\mathbf{A} \\cdot \\left( \\nabla \\times \\mathbf{B} \\right) d\\tau + \\oint_{\\mathcal{S}} \\left( \\mathbf{A} \\times \\mathbf{B} \\right) \\cdot d \\mathbf{a} $$\n証明 線形性 アインシュタイン表記法, レヴィ・チビタ記号を使います。$\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$ とすると、\n$$ \\begin{align*} \\left[ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) \\right]_{i} \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (\\mathbf{A} + \\mathbf{B})_{k} \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (A_{k} + B_{k}) \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j}A_{k} + \\epsilon_{ijk} \\nabla_{j}B_{k} \\\\ \u0026amp;= [\\nabla \\times \\mathbf{A}]_{i} + [\\nabla \\times \\mathbf{B}]_{i} \\\\ \\end{align*} $$\n第三の等号は、$\\dfrac{\\partial (A_{k} + B_{k})}{\\partial x_{j}} = \\dfrac{\\partial A_{k}}{\\partial x_{j}} + \\dfrac{\\partial B_{k}}{\\partial x_{j}}$であるため成立します。\n■\n参照 デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ ","id":1752,"permalink":"https://freshrimpsushi.github.io/jp/posts/1752/","tags":null,"title":"3次元デカルト座標系におけるベクトル関数のカール(回転)"},{"categories":"거리공간","contents":"定義 二つの距離空間$\\left( X , d_{X} \\right)$、$\\left( Y , d_{Y} \\right)$と部分集合$E\\subset X$に対して、関数$f : E \\to Y$を定義しよう。\n$p \\in E$としよう。ある$\\varepsilon \u0026gt; 0$に対して、\n$$ x \\in E \\quad \\text{and} \\quad d_{X}(p, x ) \u0026lt; \\delta \\implies d_{Y}(f(p) , f(x) ) \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在するならば、$f$は$p \\in E$で連続であるという。$f$が$E$の全ての点で連続ならば、$f$を$E$上での連続関数continuous functionという。\nある$ \\varepsilon \u0026gt; 0$に対して、\n$$ d_{X}(x_{1}, x_{2} ) \u0026lt; \\delta \\land x_{1}, x_{2} \\in E \\implies d_{Y}(f(x_{1}) , f(x_{2}) ) \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在するならば、$f$が$E$上で一様連続uniformly continuousであるという。\n$\\land$は論理的に「そして」を表す論理積の記号だ。 説明 連続と一様連続は、$\\mathbb{R}$を超えて距離空間に対しても定義できる。$\\mathbb{R}$の連続と異なる点は、$d_{1}$と$d_{2}$を変えての一般化が可能であることだ。\n一方で、もっと難しい表現を使って、ある$B_{d_{Y}} (f(p) , \\varepsilon )$に対して$f(B_{d_{X}} (p , \\delta)) \\subset B_{d_{Y}} (f(p) , \\varepsilon )$を満たす$B_{d_{X}} (p , \\delta)$が存在する時、$f$が$p \\in X$で連続であるとも言える。初めは抽象的すぎて避けがちだが、見ているうちにこの表現の方が便利になるかもしれない。位相空間への一般化を考えれば、早めに慣れておいた方が良いかもしれない。\n定理: 連続関数である同値条件 関数$f:X \\to Y$に対して、以下の条件は互いに同値である。\n$f : X \\to Y$は連続である。\n$\\forall x \\in X,\\ \\displaystyle \\lim_{n \\to \\infty} p_{n} = p \\implies \\lim_{n \\to \\infty} f(p_{n}) = f(p)$\n$Y$の全ての開集合$O$に対して、$f^{-1} ( O )$は$X$で開集合である。\n$Y$の全ての閉集合$C$に対して、$f^{-1} ( C )$は$X$で閉集合である。\nこれらの性質は与えられた関数が連続であることを証明するのに役立つことがある。\n上の図を見ると、一見、四番目の条件の反例に見える。閉区間$[c,d]$に対してその逆像$f^{-1} [c,d]$が$(a,b)$であり、知っての通り、$(a,b)$は開区間である。しかし、$f : (a,b) \\to \\mathbb{R}$なので、$(a,b)$は全空間になり、全空間は閉集合であるため、命題に反していない。\n","id":384,"permalink":"https://freshrimpsushi.github.io/jp/posts/384/","tags":null,"title":"距離空間における連続性と一様連続性"},{"categories":"거리공간","contents":"定義 $a_i, b_i \\in \\mathbb{R} (1 \\le i \\le k)$に対して、集合$I=[a_{1}, b_{1}] \\times [a_{2}, b_{2}] \\times \\cdots \\times [a_{k}, b_{k}]$を**$k$-セル**と言う。ここで$\\times$は集合のデカルト積である。\n定理1 $\\mathbb{R}$上の閉区間の数列$\\left\\{ I_{n} \\right\\}$が$I_{n} \\supset I_{n+1}\\ (n=1,2,\\cdots)$を満たすとする。すると以下が成立する。\n$$ \\bigcap_{i=1}^{\\infty}I_{n}\\ne \\varnothing $$\n証明 $I_{n}=[a_{n}, b_{n}]$とする。そして$E=\\left\\{ a_{n} : n=1,2,\\cdots \\right\\}$とする。すると$E\\ne \\varnothing$であり、$b_{1}$1によって上限がある。今$x=\\sup E$とする。そして任意の二つの正数$m$、$n$に対して\n$$ a_{n} \\le a_{m+n} \\le b_{m+n} \\le b_{m} $$\nが成立するので、すべての$n$に対して$x\\le b_{n}$である。また$x$が$E$の上限であるため、すべての$n$に対して$a_{n} \\le x$であることは明らかである。したがって、すべての$n$に対して$a_{n}\\le x \\le b_{n}$なので、$x\\in I_{n}\\ \\forall n$である。したがって\n$$ x\\in \\bigcap _{i=1}^{n}I_{n} $$\n■\n定理2 $\\left\\{ I_{n} \\right\\}$が$I_{n}\\supset I_{n+1}(n=1,2,\\cdots)$を満たす$k-$セルの数列であるとする。すると$\\bigcap_{i=1}^{n}I_{n}\\ne\\varnothing$である。\n定理2は定理1を$\\mathbb{R}^{k}$に拡張したものである。\n証明 $I_{n}$を以下のようにする。\n$$ I_{n}=\\left\\{ \\mathbf{x}=(x_{1},\\cdots,x_{k}) : a_{n,j} \\le x_{j} \\le b_{nj},\\quad(1\\le j \\le k;\\ n=1,2,\\cdots) \\right\\} $$\nすなわち$I_{n}=I_{n,1}\\times \\cdots\\times I_{n,k}\\ (I_{n,j}=[a_{n,j},b_{n,j}])$である。すると定理1によって、それぞれの$I_{n,j}$に対して$x_{j}^{\\ast}\\in I_{n,j} \\ (a_{n,j} \\le x_{j}^{\\ast} \\le b_{n,j})$が存在する。したがって\n$$ \\mathbf{x^{\\ast}} =(x_{1}^{\\ast},\\cdots ,x_{k}^{\\ast})\\in I_{n} ,\\quad (n=1,2,\\cdots) $$\n■\n定理3 すべての$k-$セルはコンパクトである。\n証明 $I$を以下のような任意の$k$-セルとする。\n$$ I=I^{1}\\times \\cdots \\times I^{k}=[a_{1},b_{1}]\\times \\cdots \\times [a_{k},b_{k}] $$\nそして以下のようにする。\n$$ \\mathbf{x}=(x_{1},\\cdots,x_{k}) \\quad \\text{and} \\quad a_{j} \\le x_{j} \\le b_{j}(1\\le j \\le k) $$\n今$\\delta$を以下のようにする。\n$$ \\delta =\\left( \\sum \\limits_{j=1}^{k}(b_{j})-a_{j})^{2} \\right)^{{\\textstyle \\frac{1}{2}}}=|\\mathbf{b}-\\mathbf{a}| $$\nこのとき$\\mathbf{a}=(a_{1},\\cdots,a_{n})$、$\\mathbf{b}=(b_{1},\\cdots,b_{n})$である。すると$\\delta$は$\\mathbf{b}$と$\\mathbf{a}$の間の距離と同じである。したがって\n$$ |\\mathbf{x}-\\mathbf{y}| \\le \\delta \\quad \\forall \\mathbf{x},\\mathbf{y}\\in I $$\nが成立する。今から証明が本格的に始まるが、背理法を使用する。つまり$k-$セルがコンパクトでないと仮定する。するとコンパクトの定義によって、$I$のいくつかのオープンカバー$\\left\\{ O_{\\alpha} \\right\\}$が有限部分カバーを持たないと仮定することと同じである。$c_{j}=(a_{j}+b_{j})/2$とする。すると$c_{j}$を使って各$I^{j}$を$[a_{j},c_{j}]$、$[c_{j},b_{j}]$に分けて$2^{k}$個の$1-$セルを作ることができる。これらの和集合は当然$I$になり、仮定によりこれらの中で少なくとも一つは$\\left\\{ O_{\\alpha} \\right\\}$のいくつかの有限部分カバーでカバーされなければならない。そのセルを$I_{1}$とする。すると$I$から$I_{1}$を選んだのと同じ方法で続けて区間を選ぶと、以下の三つの規則を満たす数列$\\left\\{ I_{n} \\right\\}$を得ることができる。\n$(\\mathrm{i})$ $I\\supset I_{1} \\supset I_{2}\\supset \\cdots$\n$(\\mathrm{ii})$ それぞれの$I_{n}$は$\\left\\{ O_{\\alpha} \\right\\}$のいくつかの有限部分カバーでもカバーされない。\n$(\\mathrm{iii})$ $|\\mathbf{x}-\\mathbf{y}|\\le 2^{-n}\\delta,\\quad \\forall \\mathbf{x},\\mathbf{y}\\in I_{n}$\nすると$(\\mathrm{i})$と定理2によって、すべての$n$に対して$\\mathbf{x}^{\\ast}\\in I_{n}$である$\\mathbf{x}^{\\ast}$が存在する。すると$\\left\\{ O_{\\alpha} \\right\\}$が$I$のオープンカバーであるため、いくつかの$\\alpha$に対して$\\mathbf{x}^{\\ast\n}\\in O_{\\alpha}$が成立する。$O_{\\alpha}$が開集合であるため、$|\\mathbf{x}^{\\ast}-\\mathbf{y}|\u0026lt;r \\implies \\mathbf{y}\\in O_{\\alpha}$を満たす$r\u0026gt;0$が存在する。一方で、$n$を十分大きくして$2^{-n}\\delta\u0026lt;r$を満たすようにすることができる。すると$(\\mathrm{iii})$によって$I_{n}\\subset O_{\\alpha}$である。しかし、これは$(\\mathrm{ii})$と矛盾するので、仮定が間違っていることがわかる。したがって、すべての$k-$セルはコンパクトである。\n■\n上記の事実から以下の有用な定理を証明することができる。\nユークリッド空間でコンパクトである同値条件 実数（または複素数）空間の部分集合$E\\subset \\mathbb{R}^{k}(\\mathrm{or}\\ \\mathbb{C}^{k})$に対して、以下の三つの命題は同値である。\n(a) $E$は閉じており有界である。\n(b) $E$はコンパクトである。\n(c) $E$のすべての無限部分集合は集積点 $p \\in E$を持つ。\nここで**(a)、(b)が同値であることはハイネ・ボレルの定理と呼ばれる。(c)を満たす$E$に対して\u0026rsquo;$E$は\u0026rsquo;集積点コンパクトである\u0026rsquo;または\u0026rsquo;$E$は\u0026rsquo;ボルツァーノ-ワイエルシュトラスの性質を持つ\u0026rsquo;と言う。(b)と(c)**が同値であることは距離空間では成立するが、位相空間では一般的には成立しない。\n証明 (a) $\\implies$ (b)\n**(a)**を仮定すると、$E \\subset I$を満たす$k-$セル$I$が存在する。すると$I$がコンパクトであり、コンパクト集合の閉じた部分集合はコンパクトであるため、$E$はコンパクトである。\n(b) $\\implies$ (c)\n背理法で証明する。\n$S$がコンパクト集合$E$の無限部分集合であるとする。そして$S$の集積点が存在しないと仮定する。するとすべての$p\\in E$は、せいぜい$S$の点をただ一つだけ含む$p$の近傍$N_{p}$を持つ。$p \\in S$の場合、そのただ一つの点は$p$である。そしてこれは、オープンカバー$\\left\\{ N_{p} \\right\\}$が$S$をカバーする有限部分カバーを持たないことを意味する。$S \\subset E$なので、同様に$E$をカバーする有限部分カバーも存在しない。これは$E$がコンパクトであるという仮定に矛盾するので、$S$は集積点$p \\in E$を持つ。\n(c) $\\implies$ (a)\n背理法で証明する。\npart 1. $E$は有界である\n$E$は有界ではないと仮定してみる。すると$E$は以下の不等式を満たす点$\\mathbf{x}_{n}$を含む。\n$$ |\\mathbf{x}_{n}| \u0026gt;n\\quad (n=1,2,\\cdots) $$\n今$S=\\left\\{ \\mathbf{x}_{n} : n=1,2,\\cdots\\right\\}$とする。すると$S$は無限集合であり、$\\mathbb{R}^{k}$で集積点を持たないことは明らかである。これは$(c)$に対する矛盾である。したがって$E$は有界である。\npart 2. $E$は閉じている。\n$E$は閉じていないと仮定してみる。すると定義により$E$に含まれない$E$の集積点$\\mathbf{x}_{0}$が存在する。今$n=1,2,\\cdots$に対して$\\mathbf{x}_{n} \\in E$を以下の条件を満たす点とする。\n$$ \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \u0026lt; {\\textstyle \\frac{1}{n}} $$\nそしてこのような$\\mathbf{x}_{n}$の集合を$S$とする。すると$S$は無限集合であり、$\\mathbf{x}_{0}$を集積点として持つ。今$\\mathbf{x}_{0}$が$S$の唯一の集積点であれば、$\\mathbf{x}_{0}\\notin E$であるため$(c)$に矛盾し、$E$は閉じていることがわかる。それでは$\\mathbf{y} \\ne \\mathbf{x}_{0}$である$\\mathbf{y} \\in \\mathbb{R}^{k}$を考える。すると\n$$ \\begin{align*} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \u0026amp; \\ge \\left|\\mathbf{x}_{0} - \\mathbf{y} \\right| - \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \\\\ \u0026amp; \\ge \\left| \\mathbf{x}_{0} - \\mathbf{y} \\right| -\\frac{1}{n} \\end{align*} $$\nこのとき十分に大きな$n$に対して以下の式が成立する。\n$$ \\begin{equation} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \\ge \\left| \\mathbf{x}_{0}- \\mathbf{y} \\right|-\\frac{1}{n} \\ge \\frac{1}{2}\\left|\\mathbf{x}_{0}-\\mathbf{y} \\right| \\label{eq1} \\end{equation} $$\nまた$\\mathbf{x}_{n\n}$の条件により、$n$が大きくなるにつれて$\\mathbf{x}_{n}$は$\\mathbf{x}_{0}$に近づく。この事実と$\\eqref{eq1}$により、$n$を続けて大きくすると$\\mathbf{y}$を含まない$\\mathbf{y}$の近傍を見つけることができる。したがって$\\mathbf{y}$は$S$の集積点ではなく、$\\mathbf{x}_{0}$が$S$の唯一の集積点であることから$(c)$に矛盾し、$E$は閉じている。\n■\nボルツァーノ-ワイエルシュトラスの定理 $\\mathbb{R}^{k}$のすべての有界な無限部分集合は集積点$p \\in \\mathbb{R}^{k}$を持つ。\n証明 $E$を$\\mathbb{R}^{k}$の有界な無限部分集合とする。すると$E$が有界であるため、$E \\subset I$を満たす$k-$セル$I$が存在する。$k-$セルはコンパクトであるため、$I$はコンパクトである。すると$I$がコンパクトである同値条件$(b)\\implies (c)$によって、$E$は集積点$p \\in I \\subset \\mathbb{R}^{k}$を持つ。\n■\n任意の$b_{n}$で問題ない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1711,"permalink":"https://freshrimpsushi.github.io/jp/posts/1711/","tags":null,"title":"すべてのk-cellはコンパクトである：ユークリッド空間でコンパクトである同値条件。"},{"categories":"거리공간","contents":"定義 $(X,d)$が距離空間だとしよう。$p \\in X$であり、$E \\subset X$とする。\n$d(q,p)\u0026lt;r$を満たすすべての$q$を含む集合を点$p$の近傍neighborhoodと定義し、$N_{r}(p)$と表記する。このとき$r$を$N_{r}(p)$の半径と呼ぶ。距離を省略できる場合は$N_{p}$のように表記することもある。\n$p$のすべての近傍が$q\\ne p$であり、$q\\in E$の$q$を含む場合、$p$を$E$の集積点limit pointと呼ぶ。\n$p\\in E$でありながら$p$が$E$の集積点でない場合、$p$を$E$の孤立点isolated pointと呼ぶ。\n$E$のすべての集積点が$E$に含まれる場合、$E$が閉じているclosedという。\n$N\\subset E$を満たす$p$の近傍$N$が存在する場合、$p$を$E$の内点interior pointと呼ぶ。\n$E$のすべての点が$E$の内点である場合、$E$が開いているopenという。\n$p \\in X$であり$p \\notin E$のすべての$p$を含む集合を$E$の補集合complementと呼び、$E^{c}$と表記する。\n$E$が閉じており$E$のすべての点が$E$の集積点である場合、$E$が完全perfectであるという。\n$\\forall p\\in E,\\ d(p,q)\u0026lt;M$を満たす点$q\\in X$と実数$M$が存在する場合、$E$を有界boundedと呼ぶ。\n$X$のすべての点が$E$の集積点であるか$E$の点である場合、$E$は$X$で密denseであるという。\n$E$のすべての集積点の集合を$E$の導出集合derived setと呼び、$E^{\\prime}$と表記する。\n$E$と$E^{\\prime}$の和集合を閉包closureと呼び、$\\overline{E}=E\\cup E^{\\prime}$と表記する。\n説明 上で述べる開、集積点、密、内点などは他のステートメントで定義されることもあるが、本質的には同じである。それぞれの概念をなぜ上のように定義し、名前を付けたのかは、1次元、2次元で直接図を描いてみれば感覚が簡単につかめるだろう。孤立点は集積点でない点と定義されるため、孤立点でありながら同時に集積点であることはできない。これとは異なり、開集合と閉集合はそれぞれ独立した条件で定義される。したがって、名前から感じられる直感とは異なり、開いていると同時に閉じている集合や、開いても閉じてもいない集合が存在することがある。前者の例として$\\mathbb{R}^{2}$があり、後者の例として$\\left\\{ {\\textstyle \\frac{1}{n}}\\ |\\ n\\in \\mathbb{N} \\right\\}$がある。内点と近傍の定義をよく考えると、$x$が$E$の内点である条件は\n$$ d(x,p) \u0026lt;\\varepsilon \\implies x \\in E $$\nが成立するようなある正数$\\varepsilon\u0026gt;0$が存在することと同じである。上の概念と関連するいくつかの定理と証明を紹介する。上の定義での表記に従う。\n定理1 すべての近傍は開集合である。\n証明 $E=N_{r}(p)$としよう。また、任意の$q \\in E$を考える。すると、近傍の定義により、以下の式を満たす正の実数$h$が必ず存在する。\n$$ d(p,q)=r-h\u0026lt;r $$\nすると、距離の定義により、$d(q,s)\u0026lt;h$を満たすすべての$s$に対して、以下の式が成立する。\n$$ d(p,s)\\le d(p,q)+d(q,s)\u0026lt;(r-h)+h=r $$\nしたがって、近傍の定義により、$s \\in E$である。これは、▷eq68\n◁の近傍$N_{h}(q)$内の任意の点$s$も$E$の要素であることを示している。したがって、$N_{h}(q) \\subset E$であるため、$q$は$E$の内点である。最初に$q$を$E$の任意の点としたので、$E$のすべての点は内点である。よって、$E$は開集合である。■\n定理2 集合$E$が開集合であることと$E^c$が閉集合であることは同値である。\n証明 $(\\impliedby)$\n$E^c$が閉じていると仮定する。今、任意の$p\\in E$について考える。すると$p \\notin E^c$であり、閉じている定義により$p$は$E^c$の集積点ではない。したがって、$N \\cap E^c=\\varnothing$を満たす$p$の近傍$N$が存在する。これは$N \\subset E$を意味し、内点の定義により$p$は$E$の内点である。任意の$p\\in E$がすべて$E$の内点であるため、定義により$E$は開集合である。\n$(\\implies)$\n$E$が開いていると仮定する。そして、$p$を$E^{c}$の集積点とする。すると、集積点の定義により、$p$のすべての近傍は少なくとも一つの$E^{c}$の点を含む。すると、$p$のすべての近傍は$E$に含まれず、これは$p$が$E$の内点ではないことを意味する。$E$は開いていると仮定したので、$p\\notin E$である。したがって、$E^{c}$のすべての集積点$p$が$E^{c}$に含まれるので、$E^{c}$は閉じている。\n■\n定理3 $p$を$E$の集積点としよう。すると、$p$の近傍は無数に多くの$E$の点を要素として持つ。\nこれを別の言い方をすると、「有限集合は集積点を持たない」「集積点を持つ集合は無限集合である」ということである。\n証明 $p$の近傍$N$が$E$の有限個の要素のみを含むと仮定しよう。そして、$q_{1},q_{2},\\cdots,q_{n}$を$p$ではない$N\\cap E$の点としよう。そして、$p$と$q_{i}$の距離の中で最小値を$r$とする。\n$$ r= \\min \\limits _{1\\le i \\le n}d(p,q_{i}) $$\n各々の$q_{i}$は$p$と異なる点であるため、すべての距離は正であり、正の数の中で最小値を選んでも正であるため、$r\u0026gt;0$である。今、$p$の別の近傍$N_{r}(p)$を考える。すると、近傍と距離の定義により、$N_{r}(p)$にはいかなる$q_{i}$も含まれない。すると、集積点の定義により、$p$は$E$の集積点ではない。これは、$p$が$E$の集積点であるという事実に矛盾する。したがって、帰納法により仮定が間違っていることがわかる。したがって、上の定理は成立する。\n■\n系 有限個の点のみを持つ集合は集積点を持たない。\n定理4 距離空間$(X,d)$と$E \\subset X$に対して、以下の事実が成立する。$(a)$ $\\overline{E}$は閉じている。$(b)$ $E=\\overline{E}$であることと同値は$E$が閉じていることである。$(c)$ $E\\subset F$を満たすすべての閉集合$F\\subset X$に対して$\\overline{E} \\subset F$が成立する。\n$(a)$と$(c)$によって、$\\overline{E}$は$E$を含む最小の$X$の閉部分集合である。\n","id":1700,"permalink":"https://freshrimpsushi.github.io/jp/posts/1700/","tags":null,"title":"メートル空間における近傍、限界点、オープン、クローズド"},{"categories":"거리공간","contents":"定義 $(X,d)$が距離空間であるとする。$p \\in X$であり、$E \\subset X$であるとする。\n$d(q,p)\u0026lt;r$を満たす全ての$q$を含む集合を点$p$の近傍と定義し、$N_{r}(p)$と記す。この時、$r$を$N_{r}(p)$の半径と呼ぶ。距離を省略して良い場合は$N_{p}$とも記す。\n$p$の全ての近傍が$q\\ne p$であり、$q\\in E$である$q$を含んでいれば、$p$を$E$の集積点と呼ぶ。\n$E$の全ての集積点が$E$に含まれる場合、$E$が閉じていると言う。\n$N\\subset E$を満たす$p$の近傍$N$が存在すれば、$p$を$E$の内点と呼ぶ。\n$E$の全ての点が$E$の内点である場合、$E$が開いていると言う。\n$E$の全ての集積点の集合を$E$の導集合と呼び、$E^{\\prime}$と記す。\n$E$と$E^{\\prime}$の合併集合を閉包と呼び、$\\overline{E}=E\\cup E^{\\prime}$と記す。\n定理1 $A,B\\subset X$に対して以下の式が成立する。\n(1a) $A\\subset B \\implies A^{\\prime} \\subset B^{\\prime}$\n(1b) $(A\\cup B)^{\\prime}=A^{\\prime}\\cup B^{\\prime}$\n(1c) $(A \\cap B)^{\\prime} \\subset A^{\\prime}\\cap B^{\\prime}$\n証明 (1a) $A\\subset B$と仮定する。そして$p\\in A^{\\prime}$とする。すると$p$は$A$の集積点であるため、集積点の定義により以下の文が成立する。$p$の全ての近傍$N$は$q\\ne p$であり、$q\\in A$である$q$を含む。この時、$A\\subset B$と仮定したので、上記の文は以下の文を意味する。$p$の全ての近傍$N$は$q\\ne p$であり、$q\\in B$である$q$を含む。したがって、集積点の定義により$p \\in B^{\\prime}$である。\n■\n(1b) 部分 1. $A^{\\prime} \\cup B^{\\prime} \\subset (A\\cup B)^{\\prime}$\n$A\\subset A\\cup B$であり、$B \\subset A\\cup B$であるため、$(a1)$によって以下のようになる。\n$$ A^{\\prime} \\subset (A\\cup B)^{\\prime} \\quad \\text{and} \\quad B^{\\prime} \\subset (A \\cup B)^{\\prime} $$\nしたがって\n$$ A^{\\prime} \\cup B^{\\prime} \\subset (A\\cup B)^{\\prime} $$\n部分 2. $(A\\cup B)^{\\prime} \\subset A^{\\prime}\\cup B^{\\prime}$\n$p \\in (A\\cup B)^{\\prime}$とする。すると集積点の定義により$p$の全ての近傍$N$は$q\\ne p$であり、$q\\in A\\cup B$である$q$を含む。$q\\in A\\cup B$を再び書くと$q\\in A \\text{ or } q\\in B$であるため、これは$p \\in A^{\\prime} \\text{ or } p\\in B^{\\prime}$と同じである。したがって$p\\in A^{\\prime}\\cup B^{\\prime}$であるため、以下のようになる。\n$$ (A\\cup B)^{\\prime} \\subset A^{\\prime}\\cup B^{\\prime} $$\n部分 3.\n上記の結果を総合すると以下のようになる。\n$$ A^{\\prime}\\cup B^{\\prime} = (A\\cup B)^{\\prime} $$\n■\n(1c) $A\\cap B \\subset A$であり、$A\\cap B \\subset B$であるため、(1a) により以下のようになる。\n$$ (A\\cap B)^{\\prime} \\subset A^{\\prime} \\quad \\text{and} \\quad (A\\cap B)^{\\prime} \\subset B^{\\prime} $$\nしたがって\n$$ (A\\cap B)^{\\prime} \\subset A^{\\prime}\\cap B^{\\prime} $$\n■\n定理2 $A,B \\subset X$に対して以下の式が成立する。\n(2a) $A\\subset B \\implies \\overline{A} \\subset \\overline{B}$\n(2b) $\\overline{A\\cup B} = \\overline{A}\\cup \\overline{B}$\n(2c) $\\overline{A\\cap B} \\subset \\overline{A}\\cap \\overline{B}$\n証明 (2a) $A \\subset B$と仮定する。すると**(1a)** により$A^{\\prime} \\subset B^{\\prime}$である。したがって\n$$ \\overline{A} = A\\cup A^{\\prime} \\subset B \\cup B^{\\prime} = \\overline{B} $$\n■\n(2b) 部分 1. $\\overline{A\\cup B}\\subset \\overline{A}\\cup \\overline{B}$\n$p \\in \\overline{A\\cup B}$とする。すると$p\\in A\\cup B$であるか$p \\in (A\\cup B)^{\\prime}$であるという意味である。\nケース 1-1. $p \\in A\\cup B$\nこの場合$p \\in A$であるか$p \\in B$である。しかし$A \\subset \\overline{A}$であり、$B \\subset \\overline{B}$であるため\n$$ p\\in \\overline{A}\\ \\text{or} \\ p \\in \\overline{B}\\implies p \\in \\overline{A}\\cup \\overline{B} $$\nケース 1-2. $p\\in (A\\cup B)^{\\prime}$\n(1b) によって$p\\in (A\\cup B)^{\\prime}=A^{\\prime}\\cup B^{\\prime}$である。これは$p\\in A^{\\prime}$であるか$p\\in B^{\\prime}$であるという意味である。しかし$A^{\\prime} \\subset \\overline{A}$であり、$B^{\\prime} \\subset \\overline{B}$であるため、上記のケースと同様に\n$$ p\\in \\overline{A}\\ \\text{or} \\ p \\in \\overline{B}\\implies p \\in \\overline{A}\\cup\\overline{B} $$\nケース 1-1, 1-2によって以下が成立する。\n$$ \\overline{A\\cup B}\\subset \\overline{A}\\cup \\overline{B} $$\n部分 2. $\\overline{A}\\cup \\overline{B} \\subset \\overline{A\\cup B}$\n$A \\subset A\\cup B$であり、$B\\subset A\\cup B$であるため、$(b1)$によって以下が成立する。\n$$ \\overline{A} \\subset \\overline{A\\cup B}\\quad \\text{and} \\quad \\overline{B}\\subset \\overline{A\\cup B} $$\nしたがって\n$$ \\overline{A}\\cup \\overline{B} \\subset \\overline{A\\cup B} $$\n■\n(2c) $p \\in \\overline{A\\cap B}$とする。すると$p\\in A\\cap B$であるか$p\\in (A \\cap B)^{\\prime}$である。\nケース 1. $p\\in A\\cap B$\nこの場合$p \\in A$でありながら$p \\in B$である。しかし$A\\subset \\overline{A}$であり、$B\\subset \\overline{B}$であるため\n$$ p\\in A \\ \\text{and} \\ \\ p \\in B \\implies p\\in \\overline{A} \\ \\text{and} \\ p \\in \\overline{B} \\implies p\\in \\overline{A}\\cap \\overline{B} $$\nケース 2. $p \\in (A\\cap B)^{\\prime}$\n(1a) によって$(A\\cap B)^{\\prime}\\subset A^{\\prime}$であり、$(A\\cap B)^{\\prime} \\subset B^{\\prime}$である。しかし$A^{\\prime}\\subset \\overline{A}$であり、$B^{\\prime} \\subset \\overline{B}$であるため\n$$ p\\in A^{\\prime} \\ \\text{and} \\ p\\in B^{\\prime} \\implies p\\in \\overline{A}\\quad \\text{and} \\quad p\\in \\overline{B}\\implies p\\in \\overline{A}\\cap \\overline{B} $$\n■\n定理3 距離空間$(X,d)$と$E \\subset X$に対して以下の事実が成立する。\n(3a) $\\overline{E}$は閉じている。\n(3b) $E=\\overline{E}$であることと同等であるのは、$E$が閉じていることである。\n(3c) $E\\subset F$を満たす閉集合$F\\subset X$に対して、$\\overline{E} \\subset F$が成立する。\n(3a) と (3c) によって、$\\overline{E}$は$E$を含む最小の$X$の閉部分集合である。\n証明 (3a) $p \\in X$であり、$p \\notin \\overline{E}$とする。すなわち$p \\in (\\overline{E})^{c}$である。すると$p$は$E$の点でも$E^{\\prime}$の点でもない。したがって、集積点の定義により$p$は少なくとも一つの$N\\cap E=\\varnothing$である近傍$N$を持つ。したがって$N\\subset (\\overline{E})^{c}$であり、$p$は$(\\overline{E})^{c}$の任意の点であったので、内点の定義により$(\\overline{E})^{c}$の全ての点が内点であり、これは$(\\overline{E})^{c}$が開集合であることを意味する。$(\\overline{E})^{c}$が開集合であるため、$\\overline{E}$は閉集合である。1\n■\n(3b) $(\\implies)$\n$E=\\overline{E}=E \\cup E^{\\prime}$であるため、$E$の全ての集積点は$E$の要素である。これは閉集合の定義であるため、$E$は閉じている。または、閉包と閉じることの定義から直ちに成立することがわかる。\n$(\\impliedby)$\n閉集合の定義により、$E$の全ての集積点は$E$に含まれる。したがって、$\\overline{E}=E\\cup E^{\\prime}=E$である。\n■\n(3c) $F$を$E\\subset F \\subset X$である閉集合とする。すると**(3b)** によって$F^{\\prime} \\subset \\overline{F}=F$である。また、(2a) によって$E^{\\prime} \\subset F^{\\prime} \\subset F$である。したがって、以下が成立する。\n$$ E \\subset F \\quad \\text{and} \\quad E^{\\prime}\\subset F $$\nしたがって\n$$ E\\cup E^{\\prime} =\\overline{E} \\subset F $$\n■\n定理4 $E$を空集合ではない実数集合であり、上に有界とする。そして$y=\\sup E$とする。すると$y \\in \\overline{E}$である。また、$E$が閉じていれば、$y \\in E$である。\n証明 $y \\in \\overline{E}$であることが成立すれば、その後の命題は (3a) により自明であるので、$y \\in \\overline{E}$のみ証明することにする。2つの場合に分けて証明する。\nケース 1. $y \\in E$\n$$ y \\in E \\subset \\overline{E} $$\nであるため、成立する。\nケース 2. $y \\notin E$\nすると全ての正数$h\u0026gt;0$に対して、$y-h\u0026lt;x\u0026lt;y$を満たす$x\\in E$が存在する。これは$y$の全ての近傍である$N_{h}(y)$内に$E$の要素が必ず含まれることを意味する。したがって、定義により$y$は$E$の集積点である。したがって$y\\in E\\cup E^{\\prime}=\\overline{E}$である。\n■\n定理2 参照\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1701,"permalink":"https://freshrimpsushi.github.io/jp/posts/1701/","tags":null,"title":"計量空間における閉包と派生集合"},{"categories":"힐베르트공간","contents":"定義 関数空間 $\\mathbb{C}^{\\mathbb{R}}$ の関数 $f : \\mathbb{R} \\to \\mathbb{C}$ を考えてみよう。\n関数 $f$ のサポートsupportは、関数値が $0$ ではない点の集合にクロージャを取ったクローズセットとして次のように定義される。 $$ \\text{supp} f = \\overline{\\left\\{ x \\in \\mathbb{R} : f(x) \\ne 0 \\right\\}} $$\n$\\text{supp} f$が有界なら、$f$がコンパクトサポートを持つと言う。クロージャは閉集合であり、実数空間で閉じていて有界な集合はコンパクトだからである。\n$U\\Subset V$は$\\overline{U} \\subset V$であり$\\overline{U}$がコンパクトであることを意味する。つまり、$\\mathrm{supp}(f) \\Subset U$は$f$が$U$でコンパクトサポートを持つことを意味する。$\\subset \\subset$として書くこともある。\n連続関数の集合はベクトル空間になり、これを連続関数空間と呼び、次のように表記する。\n$$ C(\\mathbb{R}) := \\left\\{f \\text{ is continuous} \\right\\} $$\n$C^{1}$と混同する可能性がある場合は、$C^{0}$と書くこともある。\nコンパクトサポートを持つ連続関数のベクトル空間を次のように表記する。\n$$ C_{c} (\\mathbb{R}) := \\left\\{ f \\in C(\\mathbb{R}) : f \\text{ has compact support} \\right\\} $$\n$x \\to \\pm \\infty$の時、関数値が$0$に収束する連続関数のベクトル空間を次のように表記する。\n$$ C_{0} ( \\mathbb{R} ) := \\left\\{ f \\in C(\\mathbb{R}) : f(x) \\to 0 \\text{ as } x \\to \\pm \\infty \\right\\} $$\n$m$回まで微分可能であり、その導関数がすべて連続である連続関数のベクトル空間を次のように表記する。\n$$ C^{m}(\\mathbb{R}) :=\\left\\{ f \\in C(\\mathbb{R}) : f^{(n)} \\text{ is continuous } \\forall n \\le m \\right\\} $$\nこの場合$C^{0}(\\mathbb{R})$は$C(\\mathbb{R})$を意味する。この時の$C^{m}$の要素を$m$回 連続的に微分可能な関数continuously differentiable functionと呼ぶ。\n無限に微分可能で、その導関数がすべて連続である連続関数のベクトル空間を次のように表記する。 $$ C^{\\infty}(\\mathbb{R})=\\bigcap _{m=0}^{\\infty}C^{m}(\\mathbb{R}) $$ この時の$C^{\\infty}$の要素をスムース関数smooth functionと呼ぶ。\n※ 著者によっては$C_{0}$を$C_{c}$の意味で使う場合があるので、教科書で定義された表記をよく確認しよう。\n説明 ソボレフ空間、超関数論などでは$C_{c}^{\\infty}$を主に扱うことになる。\n当然ながら$C_{c} (\\mathbb{R})$は$C_{0} (\\mathbb{R})$の部分空間になる。二つとも単なる連続関数の空間$C (\\mathbb{R})$に比べて良い空間だが、作用素ノルム $\\left\\| \\cdot \\right\\|_{\\infty} $に対してバナッハ空間にならないことに注意する必要がある。例えば、次のような$\\left\\{ f_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset C_{c} (\\mathbb{R})$を考えてみよう\n$$ f_{k} (x) := \\begin{cases} {{ \\sin x } \\over { x }} \\chi_{[ - k \\pi , k \\pi ]} (x) \u0026amp; , x \\ne 0 \\\\ 1 \u0026amp; , x = 0 \\end{cases} $$\n$f_{k}$はすべての$k \\in \\mathbb{N}$に対してコンパクトサポート$[-k \\pi , k \\pi]$を持つが、次のようなシンク関数 $\\text{sinc} \\in C_{0} (\\mathbb{R}) \\setminus C_{c} (\\mathbb{R})$に収束する。\n$$ \\text{sinc} x = \\begin{cases} {{ \\sin x } \\over { x }} \u0026amp; , x \\ne 0 \\\\ 1 \u0026amp; , x = 0 \\end{cases} $$\n距離空間として1 区間$[0, 1]$上で連続な実数値関数の集合を$X = C[0, 1]$としよう。そして、[距離] $d$を次のように定義しよう。\n$$ d(x, y) := \\int\\limits_{0}^{1} \\left| x(t) - y(t) \\right| dt \\qquad \\forall x, y \\in X $$\nすると、距離空間$(X, d)$は完備空間ではない。以下の図(a)に示すような関数$x_{m}$を考えよう。\n$n \\gt m$とすると、任意の$\\varepsilon \\gt 0$に対して$m \\gt 1/\\varepsilon$の時はいつでも$1 \\cdot \\frac{1}{m} \\lt \\varepsilon$が成立するので、$d(x_{m}, x_{n}) \\lt \\varepsilon$によって$\\left\\{ x_{m} \\right\\}$はコーシー数列である。\nしかし、$x_{m}(t) = 0$と$(t \\in [0, 1/2])$であり、$x_{m}(t) = 1$と$(t \\in [a_{m}, 1])$なので、次のようになる。\n$$ \\begin{align*} d(x_{m}, x) \u0026amp;= \\int\\limits_{0}^{1} \\left| x_{m(t)} - x(t) \\right| dt \\\\ \u0026amp;= \\int\\limits_{0}^{\\frac{1}{2}} \\left| 0 - x(t) \\right| dt + \\int\\limits_{\\frac{1}{2}}^{a_{m}} \\left| x_{m(t)} - x(t) \\right| dt + \\int\\limits_{a_{m}}^{1} \\left| 1 - x(t) \\right| dt \\\\ \u0026amp;= \\int\\limits_{0}^{\\frac{1}{2}} \\left| x(t) \\right| dt + \\int\\limits_{\\frac{1}{2}}^{a_{m}} \\left| x_{m(t)} - x(t) \\right| dt + \\int\\limits_{a_{m}}^{1} \\left| 1 - x(t) \\right| dt \\\\ \\end{align*} $$\n各被積分関数が$0$以上であるため、$d(x_{m}, x)$が$0$に収束するためには、各被積分関数が$0$でなければならない。つまり、$x$は$t\\in[0, \\frac{1}{2})$で$x(t) = 0$であり、$t\\in (\\frac{1}{2}, 1]$では$x(t) = 1$である。これは明らかに連続関数ではないため、$x \\notin X$であり、$\\left\\{ x_{m} \\right\\}$は$X$に収束しない。\nノルム空間として2 連続関数空間$C[0, 1]$は、積分ではなく、最大値をノルムとして与えると完備空間、つまり完備ノルム空間(バナッハ空間)となる。つまり、以下のように定義された$\\left\\| \\cdot \\right\\|$に対して$(C[0, 1], \\left\\| \\cdot \\right\\|)$はバナッハ空間である。\n$$ \\left\\| f \\right\\| := \\max\\limits_{t \\in [0, 1]} \\left| f(t) \\right|,\\qquad f \\in C[0, 1] $$\nErwin Kreyszig, Introductory Functional Analysis with Applications (1978), p38\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nErwin Kreyszig, Introductory Functional Analysis with Applications (1978), p61-62\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1594,"permalink":"https://freshrimpsushi.github.io/jp/posts/1594/","tags":null,"title":"関数のサポートと連続関数空間のクラス"},{"categories":"해석개론","contents":"정리1 この記事はリーマン・スティルチェス積分を基準に書かれています。$\\alpha=\\alpha (x)=x$とすると、リーマン積分と同じです。 $f$が$[a,b]$でリーマン（-スティルチェス）積分可能だとしましょう。すると、定数$c\\in \\mathbb{R}$に対して$cf$も$[a,b]$で積分可能であり、その値は以下の通りです。 $$ \\int_{a}^{b}cf d\\alpha = c\\int_{a}^{b}f d\\alpha $$\n二つの関数$f_{1}$、$f_{2}$が$[a,b]$でリーマン（-スティルチェス）積分可能であるとしましょう。すると、$f_{1}+f_{2}$も積分可能であり、その値は以下の通りです。 $$ \\int _{a} ^{b}(f_{1}+f_{2})d\\alpha = \\int _{a} ^{b} f_{1}d\\alpha + \\int_{a}^{b} f_{2} d\\alpha $$\n積分は線形であるということです。\n$$ \\int _{a} ^{b}(f_{1}+cf_{2})d\\alpha = \\int _{a} ^{b} f_{1}d\\alpha + c\\int_{a}^{b} f_{2} d\\alpha $$\nわざわざ加算と定数倍を別々に書いた理由は、証明を別々にするためです。\n補助定理 $[a,b]$でリーマン（-スティルチェス）積分可能な関数$f$と任意の正数$\\varepsilon\u0026gt; 0$に対して、以下の式を満たす$[a,b]$の分割$P$が存在します。\n$$ \\begin{align} U(P,f,\\alpha) \\lt \\int_{a}^{b}f d\\alpha +\\varepsilon \\tag{L1} \\\\ \\int_{a}^{b}f d\\alpha -\\varepsilon \\lt L(P,f,\\alpha) \\tag{L2} \\end{align} $$\n$U$、$L$はそれぞれリーマン（-スティルチェス）上積分、下積分です。\n証明 $\\eqref{L1}$ 任意の正数$\\varepsilon \\gt 0$が与えられたとします。すると、積分可能の必要十分条件により、以下の式を満たす分割$P$が存在します。\n$$ U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\nこの時$L(P,f,\\alpha) \\le \\displaystyle \\int_{a}^{b}fd\\alpha$なので、次が成立します。\n$$ U(P,f,\\alpha)-\\int_{a}^{b}f d\\alpha\\le U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\n従って、要約すると次のようになります。\n$$ U(P,f,\\alpha ) \\lt \\int_{a}^{b}f d\\alpha +\\varepsilon $$\n■\n$\\eqref{L2}$ 証明$\\eqref{L1}$でと同様に、次を満たす分割$P$が存在します。\n$$ U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\n$\\displaystyle \\int_{a}^{b}fd\\alpha \\le U(P,f,\\alpha)$なので、次が成立します。\n$$ \\int_{a}^{b}f d\\alpha-L(P,f,\\alpha)\\le U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\n従って、要約すると次のようになります。\n$$ \\int_{a}^{b}f d\\alpha -\\varepsilon \\lt L(P,f,\\alpha) $$\n■\n証明 $f_{1}, f_{2}, f$が積分可能の時、$f_{1}+f_{2}, cf$も積分可能であり、その値が実際に$\\displaystyle \\int f_{1} + \\int f_{2}, c\\int f$と同じであることを示します。\n1. Case 1. $c=0$\n$cf=0$が積分可能であることは自明です。また、次の等式が成立することも自明です。\n$$ \\int_{a}^{b}0fd\\alpha=0=0\\int_{a}^{b}fd\\alpha $$\nCase 2. $c\u0026gt;0$\n任意の正数$\\varepsilon \u0026gt;0$が与えられたとします。すると、積分可能の必要十分条件によって、次を満たす分割$P=\\left\\{ a=x_{0} \\lt \\cdots \\lt x_{i} \\lt \\cdots \\lt x_{n}=b\\right\\}$が存在します。\n$$ \\begin{equation} U(P,f,\\alpha) - L(P,f,\\alpha)\u0026lt;\\frac{\\varepsilon}{c} \\end{equation} $$\nそして、次のようにしましょう。\n$$ \\begin{align*} M_{i} \u0026amp;= \\sup _{[x_{i-1}, x_{i}]} f(x) \\\\ m_{i} \u0026amp;= \\inf _{[x_{i-1}, x_{i}]} f(x) \\\\ M_{i}^{c} \u0026amp;= \\sup _{[x_{i-1}, x_{i}]} cf(x) \\\\ m_{i}^{c} \u0026amp;= \\inf _{[x_{i-1}, x_{i}]} cf(x) \\end{align*} $$\nすると$c\u0026gt;0$なので$cM_{i} = M_{i}^{c}$であり、$cm_{i} = m_{i}^{c}$です。すると、リーマン（-スティルチェス）和の定義と$(1)$によって、次が成立します。\n$$ \\begin{align} U(P,cf,\\alpha)- L(P,cf,\\alpha) \u0026amp;= \\sum \\limits_{i=1}^{n}M_{i}^{c}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}m_{i}^{c}\\Delta \\alpha_{i} \\nonumber\\\\ \u0026amp;= \\sum \\limits_{i=1}^{n}cM_{i}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}cm_{i}\\Delta \\alpha_{i} \\nonumber\\\\ \u0026amp;= c\\left( \\sum \\limits_{i=1}^{n}M_{i}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}m_{i}\\Delta \\alpha_{i} \\right) \\nonumber\\\\ \u0026amp;= c\\Big[ U(P,f,\\alpha)-L(P,f,\\alpha)\\Big] \\nonumber\\\\ \u0026amp;\\lt \\varepsilon \\end{align} $$\n従って、積分可能の必要十分条件により、$cf$は積分可能です。 積分は上積分より小さいので、次が成立します。\n$$ c \\int_{a}^{b}fd \\alpha \\le cU(P,f,\\alpha) = U(P,cf,\\alpha) $$\nこれは、$(2)$と補助定理によって、次が成立します。\n$$ c\\int _{a}^{b}f d\\alpha \\le U(P,cf,\\alpha) lt \\int _{a}^{b} cf d\\alpha +\\varepsilon $$\nこの時、$\\varepsilon$は任意の正数と仮定したので、次が成立します。\n$$ \\begin{equation} c\\int_{a}^{b}fd\\alpha \\le \\int_{a}^{b}cfd\\alpha \\end{equation} $$\n反対方向の不等号を示す過程も似ています。$(1)$と補助定理によって、次が成立します。\n$$ cU(P,f,\\alpha) \\le c\\int_{a}^{b}fd\\alpha +\\varepsilon $$\nまた、次の式が成立します。\n$$ \\int_{a}^{b} cfd\\alpha \\le U(P,cf,\\alpha)=cU(P,f,\\alpha) $$\n上の二つの式から、下の式を得ます。\n$$ \\int_{a}^{b} cfd \\alpha \\le cU(P,f,\\alpha)\u0026lt; c\\int_{a}^{b}fd\\alpha +\\varepsilon $$\nこの時、$\\varepsilon$は任意の正数なので、次が成立します。\n$$ \\begin{equation} \\int_{a}^{b} cf d\\alpha \\le c\\int_{a}^{b}fd\\alpha \\end{equation} $$\n$(3)$と$(4)$によって、次が成立します。\n$$ \\int_{a}^{b}cfd\\alpha = c\\int_{a}^{b}fd\\alpha $$\nCase 3. $c=-1$\n証明の過程はCase 2. と似ています。まず、任意の正数$\\varepsilon$が与えられたとします。$f$は積分可能なので、積分可能の必要十分条件により、与えられた$\\varepsilon$に対して、次を満たす分割$P$が存在します。\n$$ U(P,f,\\alpha) - L(P,f,\\alpha) \u0026lt;\\varepsilon $$\n今、次のようにしましょう。\n$$ \\begin{align*} M_{i} \u0026amp;= \\sup _{[x_{i-1},x_{i}]}f \\\\ m_{i} \u0026amp;= \\inf_{[x_{i-1},x_{i}]}f \\\\ M_{i}^{\\ast} \u0026amp;= \\sup _{[x_{i-1},x_{i}]}(-f) \\\\ m_{i}^{\\ast} \u0026amp;= \\inf_{[x_{i-1},x_{i}]}(-f) \\end{align*} $$\nすると$M_{i}=-m_{i}^{\\ast}$であり、$m_{i}=-M_{i}^{\\ast}$です。従って$M_{i}-m_{i}=M_{i}^{\\ast}-m_{i}^{\\ast}$です。それゆえ、次が成立します。\n$$ \\begin{align*} U(P,-f,\\alpha)-L(P,-f,\\alpha) \u0026amp;= \\sum\\limits_{i=1}^{n}M_{i}^{\\ast}\\Delta \\alpha_{i}-\\sum\\limits_{i=1}^{n}m_{i}^{\\ast}\\Delta \\alpha_{i} \\\\ \u0026amp;= \\sum\\limits_{i=1}^{n}M_{i}\\Delta \\alpha_{i} - \\sum\\limits_{i=1}^{n}m_{i}\\Delta\\alpha_{i} \\\\ \u0026amp;= U(P,f,\\alpha) -L(P,f,\\alpha) \\\\ \u0026amp;\\lt \\varepsilon \\end{align*} $$\n従って、$-f$は積分可能です。\nCase 2. の証明と同様に、補助定理によって、次が成立します。\n$$ U(P,-f,\\alpha) \\lt \\int_{a}^{b}(-f)d\\alpha +\\varepsilon $$\nまた、次の式が成立します。\n$$ -\\int_{a}^{b}fd\\alpha\\le -L(P,f,\\alpha)=U(P,-f,\\alpha) \\lt \\int_{a}^{b}(-f)d\\alpha + \\varepsilon $$\nこの時、$\\varepsilon$は任意の正数なので、次が成立します。 $$ -\\int_{a}^{b}fd\\alpha \\le \\int_{a}^{b}(-f)d\\alpha $$\nそれから、補助定理によって、次の式が成立します。\n$$ \\int_{a}^{b}(-f)d\\alpha -\\varepsilon \\lt L(P,-f,\\alpha)=-U(P,f,\\alpha)\\le-\\int_{a}^{b}fd\\alpha $$\n$\\varepsilon$は任意の正数なので、次が成立します。\n$$ \\int_{a}^{b}(-f)d\\alpha \\le -\\int_{a}^{b}fd\\alpha $$\n従って、次を得ます。\n$$ \\int_{a}^{b}(-f)d\\alpha =-\\int_{a}^{b}fd\\alpha $$\nCase 4. $c \\lt 0 \\quad \\text{and} \\quad c\\ne -1$\nCase 2. と Case 3. によって成立します。\n■\n2. $f=f_{1}+f_{2}$としましょう。$P$を$[a,b]$の任意の分割とします。すると、リーマン（-スティルチェス）上積分、下積分の定義によって、次が成立します。\n$$ \\begin{equation} \\begin{aligned} L(P,f_{1},\\alpha) + L(P,f_{2},\\alpha)\u0026amp; \\le L(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f_{1},\\alpha) +U(P,f_{2},\\alpha) \\end{aligned} \\end{equation} $$\n任意の正数$\\varepsilon \u0026gt; 0$が与えられたとします。すると、積分可能の必要十分条件によって、次を満たす分割$P_{j}$が存在します。\n$$ U(P_{j},f_{j},\\alpha)-L(P_{j},f_{j},\\alpha)\u0026lt;\\varepsilon,\\quad (j=1,2) $$\n今、$P$を再び$P_{1}$と$P_{2}$の共通細分としましょう。すると、$(5)$によって、次が成立します。\n$$ \\begin{align*} U(P,f,\\alpha)-L(P,f,\\alpha) \u0026amp;\\le \\left[ U(P,f_{1},\\alpha)-L(P,f_{1},\\alpha) \\right] + \\left[ U(P,f_{2},\\alpha)-L(P,f_{2},\\alpha) \\right] \\\\ \u0026amp;\u0026lt; \\varepsilon \\end{align*} $$\n従って、積分可能の必要十分条件により、$f$は積分可能です。 それから、補助定理によって、下の式が成立します。\n$$ U(P,f_{j},\\alpha)\u0026lt;\\int _{a}^{b}f_{j}d\\alpha+\\varepsilon,\\quad (j=1,2) $$\nまた、定義によって積分より上積分が大きいため、次が成立します。\n$$ \\int_{a}^{b}fd\\alpha \\le U(P,f,\\alpha) $$\n上の式と$(5)$の三番目の不等式によって、次が成立します。\n$$ \\begin{align*} \\int_{a}^{b}fd\\alpha \u0026amp;\\le U(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f_{1},\\alpha)+U(P,f_{2},\\alpha) \\\\ \u0026amp;\u0026lt; \\int_{a}^{b}f_{1}d\\alpha +\\int_{a}^{b}f_{2}d\\alpha + 2\\varepsilon \\end{align*} $$\nこの時、$\\varepsilon$は任意の正数なので、次が成立します。\n$$ \\begin{equation} \\int_{a}^{b} fd\\alpha \\le \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha \\label{6} \\end{equation} $$\n反対方向の不等式が成立することを示せば、証明は完了です。積分可能な関数の定数倍も積分可能であることを上で示したので、$-f_{1}, -f_{2}$も積分可能であることがわかります。従って、これら二つの関数に対して上の過程を繰り返せば、下の式を得ます\n$$ \\int_{a}^{b}(-f)d\\alpha \\le \\int_{a}^{b}(-f_{1})d\\alpha + \\int_{a}^{b} (-f_{2})d\\alpha $$\nまた、$\\displaystyle \\int (-f)d\\alpha=-\\int fd\\alpha$なので、両辺に$-1$を掛けると、次を得ます。\n$$ \\begin{equation} \\int_{a}^{b}fd\\alpha \\ge \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha \\label{7} \\end{equation} $$\n従って、$(6)$と$(7)$によって、次を得ます。\n$$ \\int_{a}^{b}fd\\alpha = \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha $$\n■\nウォルター・ルーディン, 数学解析の原理 (第3版, 1976), p128-129\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1666,"permalink":"https://freshrimpsushi.github.io/jp/posts/1666/","tags":null,"title":"リーマン(-シュティールス)積分の線形性"},{"categories":"줄리아","contents":"このポストの時点でのJuliaの最新バージョンはv1.3.1です。\nガイド ステップ1. Juliaのダウンロード Generic Linux Binaries for x86から自分のCPUのビットに合ったファイルをダウンロードする。\nステップ2. 圧縮を解除して移動 圧縮を解除する。\nJuliaが保存される場所へフォルダを移動する。どこでも好きな場所で構わないが、このポストでは/home/[ユーザー名]/julia-1.3.1へ移動させた。\nステップ3. シンボリックリンク 次のコマンドを使ってシンボリックリンクを作成する。\nsudo ln -s /home/[유저이름]/julia-1.3.1/bin/julia /usr/bin/julia Juliaコマンドを使って実行すれば、1.3.1バージョンが正常にインストールされたことを確認できる。\n最新バージョンが必要でない場合 sudo apt-get install julia 上のコマンドを使って、シンボリックリンクを設定することなく素早くインストールすることもできる。ただし、この方法では最新の安定版がインストールされない。\n環境 OS: Ubuntu 18.04 ","id":1511,"permalink":"https://freshrimpsushi.github.io/jp/posts/1511/","tags":null,"title":"LinuxでJuliaの最新バージョンをインストールする方法"},{"categories":"정수론","contents":"素数 1万番目までの素数のリストである。\nダウンロード\r2 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97 101 103 107 109 113 127 131 137 139 149 151 157 163 167 173 179 181 191 193 197 199 211 223 227 229 233 239 241 251 257 263 269 271 277 281 283 293 307 311 313 317 331 337 347 349 353 359 367 373 379 383 389 397 401 409 419 421 431 433 439 443 449 457 461 463 467 479 487 491 499 503 509 521 523 541 547 557 563 569 571 577 587 593 599 601 607 613 617 619 631 641 643 647 653 659 661 673 677 683 691 701 709 719 727 733 739 743 751 757 761 769 773 787 797 809 811 821 823 827 829 839 853 857 859 863 877 881 883 887 907 911 919 929 937 941 947 953 967 971 977 983 991 997 1009 1013 1019 1021 1031 1033 1039 1049 1051 1061 1063 1069 1087 1091 1093 1097 1103 1109 1117 1123 1129 1151 1153 1163 1171 1181 1187 1193 1201 1213 1217 1223 1229 1231 1237 1249 1259 1277 1279 1283 1289 1291 1297 1301 1303 1307 1319 1321 1327 1361 1367 1373 1381 1399 1409 1423 1427 1429 1433 1439 1447 1451 1453 1459 1471 1481 1483 1487 1489 1493 1499 1511 1523 1531 1543 1549 1553 1559 1567 1571 1579 1583 1597 1601 1607 1609 1613 1619 1621 1627 1637 1657 1663 1667 1669 1693 1697 1699 1709 1721 1723 1733 1741 1747 1753 1759 1777 1783 1787 1789 1801 1811 1823 1831 1847 1861 1867 1871 1873 1877 1879 1889 1901 1907 1913 1931 1933 1949 1951 1973 1979 1987 1993 1997 1999 2003 2011 2017 2027 2029 2039 2053 2063 2069 2081 2083 2087 2089 2099 2111 2113 2129 2131 2137 2141 2143 2153 2161 2179 2203 2207 2213 2221 2237 2239 2243 2251 2267 2269 2273 2281 2287 2293 2297 2309 2311 2333 2339 2341 2347 2351 2357 2371 2377 2381 2383 2389 2393 2399 2411 2417 2423 2437 2441 2447 2459 2467 2473 2477 2503 2521 2531 2539 2543 2549 2551 2557 2579 2591 2593 2609 2617 2621 2633 2647 2657 2659 2663 2671 2677 2683 2687 2689 2693 2699 2707 2711 2713 2719 2729 2731 2741 2749 2753 2767 2777 2789 2791 2797 2801 2803 2819 2833 2837 2843 2851 2857 2861 2879 2887 2897 2903 2909 2917 2927 2939 2953 2957 2963 2969 2971 2999 3001 3011 3019 3023 3037 3041 3049 3061 3067 3079 3083 3089 3109 3119 3121 3137 3163 3167 3169 3181 3187 3191 3203 3209 3217 3221 3229 3251 3253 3257 3259 3271 3299 3301 3307 3313 3319 3323 3329 3331 3343 3347 3359 3361 3371 3373 3389 3391 3407 3413 3433 3449 3457 3461 3463 3467 3469 3491 3499 3511 3517 3527 3529 3533 3539 3541 3547 3557 3559 3571 3581 3583 3593 3607 3613 3617 3623 3631 3637 3643 3659 3671 3673 3677 3691 3697 3701 3709 3719 3727 3733 3739 3761 3767 3769 3779 3793 3797 3803 3821 3823 3833 3847 3851 3853 3863 3877 3881 3889 3907 3911 3917 3919 3923 3929 3931 3943 3947 3967 3989 4001 4003 4007 4013 4019 4021 4027 4049 4051 4057 4073 4079 4091 4093 4099 4111 4127 4129 4133 4139 4153 4157 4159 4177 4201 4211 4217 4219 4229 4231 4241 4243 4253 4259 4261 4271 4273 4283 4289 4297 4327 4337 4339 4349 4357 4363 4373 4391 4397 4409 4421 4423 4441 4447 4451 4457 4463 4481 4483 4493 4507 4513 4517 4519 4523 4547 4549 4561 4567 4583 4591 4597 4603 4621 4637 4639 4643 4649 4651 4657 4663 4673 4679 4691 4703 4721 4723 4729 4733 4751 4759 4783 4787 4789 4793 4799 4801 4813 4817 4831 4861 4871 4877 4889 4903 4909 4919 4931 4933 4937 4943 4951 4957 4967 4969 4973 4987 4993 4999 5003 5009 5011 5021 5023 5039 5051 5059 5077 5081 5087 5099 5101 5107 5113 5119 5147 5153 5167 5171 5179 5189 5197 5209 5227 5231 5233 5237 5261 5273 5279 5281 5297 5303 5309 5323 5333 5347 5351 5381 5387 5393 5399 5407 5413 5417 5419 5431 5437 5441 5443 5449 5471 5477 5479 5483 5501 5503 5507 5519 5521 5527 5531 5557 5563 5569 5573 5581 5591 5623 5639 5641 5647 5651 5653 5657 5659 5669 5683 5689 5693 5701 5711 5717 5737 5741 5743 5749 5779 5783 5791 5801 5807 5813 5821 5827 5839 5843 5849 5851 5857 5861 5867 5869 5879 5881 5897 5903 5923 5927 5939 5953 5981 5987 6007 6011 6029 6037 6043 6047 6053 6067 6073 6079 6089 6091 6101 6113 6121 6131 6133 6143 6151 6163 6173 6197 6199 6203 6211 6217 6221 6229 6247 6257 6263 6269 6271 6277 6287 6299 6301 6311 6317 6323 6329 6337 6343 6353 6359 6361 6367 6373 6379 6389 6397 6421 6427 6449 6451 6469 6473 6481 6491 6521 6529 6547 6551 6553 6563 6569 6571 6577 6581 6599 6607 6619 6637 6653 6659 6661 6673 6679 6689 6691 6701 6703 6709 6719 6733 6737 6761 6763 6779 6781 6791 6793 6803 6823 6827 6829 6833 6841 6857 6863 6869 6871 6883 6899 6907 6911 6917 6947 6949 6959 6961 6967 6971 6977 6983 6991 6997 7001 7013 7019 7027 7039 7043 7057 7069 7079 7103 7109 7121 7127 7129 7151 7159 7177 7187 7193 7207 7211 7213 7219 7229 7237 7243 7247 7253 7283 7297 7307 7309 7321 7331 7333 7349 7351 7369 7393 7411 7417 7433 7451 7457 7459 7477 7481 7487 7489 7499 7507 7517 7523 7529 7537 7541 7547 7549 7559 7561 7573 7577 7583 7589 7591 7603 7607 7621 7639 7643 7649 7669 7673 7681 7687 7691 7699 7703 7717 7723 7727 7741 7753 7757 7759 7789 7793 7817 7823 7829 7841 7853 7867 7873 7877 7879 7883 7901 7907 7919 7927 7933 7937 7949 7951 7963 7993 8009 8011 8017 8039 8053 8059 8069 8081 8087 8089 8093 8101 8111 8117 8123 8147 8161 8167 8171 8179 8191 8209 8219 8221 8231 8233 8237 8243 8263 8269 8273 8287 8291 8293 8297 8311 8317 8329 8353 8363 8369 8377 8387 8389 8419 8423 8429 8431 8443 8447 8461 8467 8501 8513 8521 8527 8537 8539 8543 8563 8573 8581 8597 8599 8609 8623 8627 8629 8641 8647 8663 8669 8677 8681 8689 8693 8699 8707 8713 8719 8731 8737 8741 8747 8753 8761 8779 8783 8803 8807 8819 8821 8831 8837 8839 8849 8861 8863 8867 8887 8893 8923 8929 8933 8941 8951 8963 8969 8971 8999 9001 9007 9011 9013 9029 9041 9043 9049 9059 9067 9091 9103 9109 9127 9133 9137 9151 9157 9161 9173 9181 9187 9199 9203 9209 9221 9227 9239 9241 9257 9277 9281 9283 9293 9311 9319 9323 9337 9341 9343 9349 9371 9377 9391 9397 9403 9413 9419 9421 9431 9433 9437 9439 9461 9463 9467 9473 9479 9491 9497 9511 9521 9533 9539 9547 9551 9587 9601 9613 9619 9623 9629 9631 9643 9649 9661 9677 9679 9689 9697 9719 9721 9733 9739 9743 9749 9767 9769 9781 9787 9791 9803 9811 9817 9829 9833 9839 9851 9857 9859 9871 9883 9887 9901 9907 9923 9929 9931 9941 9949 9967 9973 10007 10009 10037 10039 10061 10067 10069 10079 10091 10093 10099 10103 10111 10133 10139 10141 10151 10159 10163 10169 10177 10181 10193 10211 10223 10243 10247 10253 10259 10267 10271 10273 10289 10301 10303 10313 10321 10331 10333 10337 10343 10357 10369 10391 10399 10427 10429 10433 10453 10457 10459 10463 10477 10487 10499 10501 10513 10529 10531 10559 10567 10589 10597 10601 10607 10613 10627 10631 10639 10651 10657 10663 10667 10687 10691 10709 10711 10723 10729 10733 10739 10753 10771 10781 10789 10799 10831 10837 10847 10853 10859 10861 10867 10883 10889 10891 10903 10909 10937 10939 10949 10957 10973 10979 10987 10993 11003 11027 11047 11057 11059 11069 11071 11083 11087 11093 11113 11117 11119 11131 11149 11159 11161 11171 11173 11177 11197 11213 11239 11243 11251 11257 11261 11273 11279 11287 11299 11311 11317 11321 11329 11351 11353 11369 11383 11393 11399 11411 11423 11437 11443 11447 11467 11471 11483 11489 11491 11497 11503 11519 11527 11549 11551 11579 11587 11593 11597 11617 11621 11633 11657 11677 11681 11689 11699 11701 11717 11719 11731 11743 11777 11779 11783 11789 11801 11807 11813 11821 11827 11831 11833 11839 11863 11867 11887 11897 11903 11909 11923 11927 11933 11939 11941 11953 11959 11969 11971 11981 11987 12007 12011 12037 12041 12043 12049 12071 12073 12097 12101 12107 12109 12113 12119 12143 12149 12157 12161 12163 12197 12203 12211 12227 12239 12241 12251 12253 12263 12269 12277 12281 12289 12301 12323 12329 12343 12347 12373 12377 12379 12391 12401 12409 12413 12421 12433 12437 12451 12457 12473 12479 12487 12491 12497 12503 12511 12517 12527 12539 12541 12547 12553 12569 12577 12583 12589 12601 12611 12613 12619 12637 12641 12647 12653 12659 12671 12689 12697 12703 12713 12721 12739 12743 12757 12763 12781 12791 12799 12809 12821 12823 12829 12841 12853 12889 12893 12899 12907 12911 12917 12919 12923 12941 12953 12959 12967 12973 12979 12983 13001 13003 13007 13009 13033 13037 13043 13049 13063 13093 13099 13103 13109 13121 13127 13147 13151 13159 13163 13171 13177 13183 13187 13217 13219 13229 13241 13249 13259 13267 13291 13297 13309 13313 13327 13331 13337 13339 13367 13381 13397 13399 13411 13417 13421 13441 13451 13457 13463 13469 13477 13487 13499 13513 13523 13537 13553 13567 13577 13591 13597 13613 13619 13627 13633 13649 13669 13679 13681 13687 13691 13693 13697 13709 13711 13721 13723 13729 13751 13757 13759 13763 13781 13789 13799 13807 13829 13831 13841 13859 13873 13877 13879 13883 13901 13903 13907 13913 13921 13931 13933 13963 13967 13997 13999 14009 14011 14029 14033 14051 14057 14071 14081 14083 14087 14107 14143 14149 14153 14159 14173 14177 14197 14207 14221 14243 14249 14251 14281 14293 14303 14321 14323 14327 14341 14347 14369 14387 14389 14401 14407 14411 14419 14423 14431 14437 14447 14449 14461 14479 14489 14503 14519 14533 14537 14543 14549 14551 14557 14561 14563 14591 14593 14621 14627 14629 14633 14639 14653 14657 14669 14683 14699 14713 14717 14723 14731 14737 14741 14747 14753 14759 14767 14771 14779 14783 14797 14813 14821 14827 14831 14843 14851 14867 14869 14879 14887 14891 14897 14923 14929 14939 14947 14951 14957 14969 14983 15013 15017 15031 15053 15061 15073 15077 15083 15091 15101 15107 15121 15131 15137 15139 15149 15161 15173 15187 15193 15199 15217 15227 15233 15241 15259 15263 15269 15271 15277 15287 15289 15299 15307 15313 15319 15329 15331 15349 15359 15361 15373 15377 15383 15391 15401 15413 15427 15439 15443 15451 15461 15467 15473 15493 15497 15511 15527 15541 15551 15559 15569 15581 15583 15601 15607 15619 15629 15641 15643 15647 15649 15661 15667 15671 15679 15683 15727 15731 15733 15737 15739 15749 15761 15767 15773 15787 15791 15797 15803 15809 15817 15823 15859 15877 15881 15887 15889 15901 15907 15913 15919 15923 15937 15959 15971 15973 15991 16001 16007 16033 16057 16061 16063 16067 16069 16073 16087 16091 16097 16103 16111 16127 16139 16141 16183 16187 16189 16193 16217 16223 16229 16231 16249 16253 16267 16273 16301 16319 16333 16339 16349 16361 16363 16369 16381 16411 16417 16421 16427 16433 16447 16451 16453 16477 16481 16487 16493 16519 16529 16547 16553 16561 16567 16573 16603 16607 16619 16631 16633 16649 16651 16657 16661 16673 16691 16693 16699 16703 16729 16741 16747 16759 16763 16787 16811 16823 16829 16831 16843 16871 16879 16883 16889 16901 16903 16921 16927 16931 16937 16943 16963 16979 16981 16987 16993 17011 17021 17027 17029 17033 17041 17047 17053 17077 17093 17099 17107 17117 17123 17137 17159 17167 17183 17189 17191 17203 17207 17209 17231 17239 17257 17291 17293 17299 17317 17321 17327 17333 17341 17351 17359 17377 17383 17387 17389 17393 17401 17417 17419 17431 17443 17449 17467 17471 17477 17483 17489 17491 17497 17509 17519 17539 17551 17569 17573 17579 17581 17597 17599 17609 17623 17627 17657 17659 17669 17681 17683 17707 17713 17729 17737 17747 17749 17761 17783 17789 17791 17807 17827 17837 17839 17851 17863 17881 17891 17903 17909 17911 17921 17923 17929 17939 17957 17959 17971 17977 17981 17987 17989 18013 18041 18043 18047 18049 18059 18061 18077 18089 18097 18119 18121 18127 18131 18133 18143 18149 18169 18181 18191 18199 18211 18217 18223 18229 18233 18251 18253 18257 18269 18287 18289 18301 18307 18311 18313 18329 18341 18353 18367 18371 18379 18397 18401 18413 18427 18433 18439 18443 18451 18457 18461 18481 18493 18503 18517 18521 18523 18539 18541 18553 18583 18587 18593 18617 18637 18661 18671 18679 18691 18701 18713 18719 18731 18743 18749 18757 18773 18787 18793 18797 18803 18839 18859 18869 18899 18911 18913 18917 18919 18947 18959 18973 18979 19001 19009 19013 19031 19037 19051 19069 19073 19079 19081 19087 19121 19139 19141 19157 19163 19181 19183 19207 19211 19213 19219 19231 19237 19249 19259 19267 19273 19289 19301 19309 19319 19333 19373 19379 19381 19387 19391 19403 19417 19421 19423 19427 19429 19433 19441 19447 19457 19463 19469 19471 19477 19483 19489 19501 19507 19531 19541 19543 19553 19559 19571 19577 19583 19597 19603 19609 19661 19681 19687 19697 19699 19709 19717 19727 19739 19751 19753 19759 19763 19777 19793 19801 19813 19819 19841 19843 19853 19861 19867 19889 19891 19913 19919 19927 19937 19949 19961 19963 19973 19979 19991 19993 19997 20011 20021 20023 20029 20047 20051 20063 20071 20089 20101 20107 20113 20117 20123 20129 20143 20147 20149 20161 20173 20177 20183 20201 20219 20231 20233 20249 20261 20269 20287 20297 20323 20327 20333 20341 20347 20353 20357 20359 20369 20389 20393 20399 20407 20411 20431 20441 20443 20477 20479 20483 20507 20509 20521 20533 20543 20549 20551 20563 20593 20599 20611 20627 20639 20641 20663 20681 20693 20707 20717 20719 20731 20743 20747 20749 20753 20759 20771 20773 20789 20807 20809 20849 20857 20873 20879 20887 20897 20899 20903 20921 20929 20939 20947 20959 20963 20981 20983 21001 21011 21013 21017 21019 21023 21031 21059 21061 21067 21089 21101 21107 21121 21139 21143 21149 21157 21163 21169 21179 21187 21191 21193 21211 21221 21227 21247 21269 21277 21283 21313 21317 21319 21323 21341 21347 21377 21379 21383 21391 21397 21401 21407 21419 21433 21467 21481 21487 21491 21493 21499 21503 21517 21521 21523 21529 21557 21559 21563 21569 21577 21587 21589 21599 21601 21611 21613 21617 21647 21649 21661 21673 21683 21701 21713 21727 21737 21739 21751 21757 21767 21773 21787 21799 21803 21817 21821 21839 21841 21851 21859 21863 21871 21881 21893 21911 21929 21937 21943 21961 21977 21991 21997 22003 22013 22027 22031 22037 22039 22051 22063 22067 22073 22079 22091 22093 22109 22111 22123 22129 22133 22147 22153 22157 22159 22171 22189 22193 22229 22247 22259 22271 22273 22277 22279 22283 22291 22303 22307 22343 22349 22367 22369 22381 22391 22397 22409 22433 22441 22447 22453 22469 22481 22483 22501 22511 22531 22541 22543 22549 22567 22571 22573 22613 22619 22621 22637 22639 22643 22651 22669 22679 22691 22697 22699 22709 22717 22721 22727 22739 22741 22751 22769 22777 22783 22787 22807 22811 22817 22853 22859 22861 22871 22877 22901 22907 22921 22937 22943 22961 22963 22973 22993 23003 23011 23017 23021 23027 23029 23039 23041 23053 23057 23059 23063 23071 23081 23087 23099 23117 23131 23143 23159 23167 23173 23189 23197 23201 23203 23209 23227 23251 23269 23279 23291 23293 23297 23311 23321 23327 23333 23339 23357 23369 23371 23399 23417 23431 23447 23459 23473 23497 23509 23531 23537 23539 23549 23557 23561 23563 23567 23581 23593 23599 23603 23609 23623 23627 23629 23633 23663 23669 23671 23677 23687 23689 23719 23741 23743 23747 23753 23761 23767 23773 23789 23801 23813 23819 23827 23831 23833 23857 23869 23873 23879 23887 23893 23899 23909 23911 23917 23929 23957 23971 23977 23981 23993 24001 24007 24019 24023 24029 24043 24049 24061 24071 24077 24083 24091 24097 24103 24107 24109 24113 24121 24133 24137 24151 24169 24179 24181 24197 24203 24223 24229 24239 24247 24251 24281 24317 24329 24337 24359 24371 24373 24379 24391 24407 24413 24419 24421 24439 24443 24469 24473 24481 24499 24509 24517 24527 24533 24547 24551 24571 24593 24611 24623 24631 24659 24671 24677 24683 24691 24697 24709 24733 24749 24763 24767 24781 24793 24799 24809 24821 24841 24847 24851 24859 24877 24889 24907 24917 24919 24923 24943 24953 24967 24971 24977 24979 24989 25013 25031 25033 25037 25057 25073 25087 25097 25111 25117 25121 25127 25147 25153 25163 25169 25171 25183 25189 25219 25229 25237 25243 25247 25253 25261 25301 25303 25307 25309 25321 25339 25343 25349 25357 25367 25373 25391 25409 25411 25423 25439 25447 25453 25457 25463 25469 25471 25523 25537 25541 25561 25577 25579 25583 25589 25601 25603 25609 25621 25633 25639 25643 25657 25667 25673 25679 25693 25703 25717 25733 25741 25747 25759 25763 25771 25793 25799 25801 25819 25841 25847 25849 25867 25873 25889 25903 25913 25919 25931 25933 25939 25943 25951 25969 25981 25997 25999 26003 26017 26021 26029 26041 26053 26083 26099 26107 26111 26113 26119 26141 26153 26161 26171 26177 26183 26189 26203 26209 26227 26237 26249 26251 26261 26263 26267 26293 26297 26309 26317 26321 26339 26347 26357 26371 26387 26393 26399 26407 26417 26423 26431 26437 26449 26459 26479 26489 26497 26501 26513 26539 26557 26561 26573 26591 26597 26627 26633 26641 26647 26669 26681 26683 26687 26693 26699 26701 26711 26713 26717 26723 26729 26731 26737 26759 26777 26783 26801 26813 26821 26833 26839 26849 26861 26863 26879 26881 26891 26893 26903 26921 26927 26947 26951 26953 26959 26981 26987 26993 27011 27017 27031 27043 27059 27061 27067 27073 27077 27091 27103 27107 27109 27127 27143 27179 27191 27197 27211 27239 27241 27253 27259 27271 27277 27281 27283 27299 27329 27337 27361 27367 27397 27407 27409 27427 27431 27437 27449 27457 27479 27481 27487 27509 27527 27529 27539 27541 27551 27581 27583 27611 27617 27631 27647 27653 27673 27689 27691 27697 27701 27733 27737 27739 27743 27749 27751 27763 27767 27773 27779 27791 27793 27799 27803 27809 27817 27823 27827 27847 27851 27883 27893 27901 27917 27919 27941 27943 27947 27953 27961 27967 27983 27997 28001 28019 28027 28031 28051 28057 28069 28081 28087 28097 28099 28109 28111 28123 28151 28163 28181 28183 28201 28211 28219 28229 28277 28279 28283 28289 28297 28307 28309 28319 28349 28351 28387 28393 28403 28409 28411 28429 28433 28439 28447 28463 28477 28493 28499 28513 28517 28537 28541 28547 28549 28559 28571 28573 28579 28591 28597 28603 28607 28619 28621 28627 28631 28643 28649 28657 28661 28663 28669 28687 28697 28703 28711 28723 28729 28751 28753 28759 28771 28789 28793 28807 28813 28817 28837 28843 28859 28867 28871 28879 28901 28909 28921 28927 28933 28949 28961 28979 29009 29017 29021 29023 29027 29033 29059 29063 29077 29101 29123 29129 29131 29137 29147 29153 29167 29173 29179 29191 29201 29207 29209 29221 29231 29243 29251 29269 29287 29297 29303 29311 29327 29333 29339 29347 29363 29383 29387 29389 29399 29401 29411 29423 29429 29437 29443 29453 29473 29483 29501 29527 29531 29537 29567 29569 29573 29581 29587 29599 29611 29629 29633 29641 29663 29669 29671 29683 29717 29723 29741 29753 29759 29761 29789 29803 29819 29833 29837 29851 29863 29867 29873 29879 29881 29917 29921 29927 29947 29959 29983 29989 30011 30013 30029 30047 30059 30071 30089 30091 30097 30103 30109 30113 30119 30133 30137 30139 30161 30169 30181 30187 30197 30203 30211 30223 30241 30253 30259 30269 30271 30293 30307 30313 30319 30323 30341 30347 30367 30389 30391 30403 30427 30431 30449 30467 30469 30491 30493 30497 30509 30517 30529 30539 30553 30557 30559 30577 30593 30631 30637 30643 30649 30661 30671 30677 30689 30697 30703 30707 30713 30727 30757 30763 30773 30781 30803 30809 30817 30829 30839 30841 30851 30853 30859 30869 30871 30881 30893 30911 30931 30937 30941 30949 30971 30977 30983 31013 31019 31033 31039 31051 31063 31069 31079 31081 31091 31121 31123 31139 31147 31151 31153 31159 31177 31181 31183 31189 31193 31219 31223 31231 31237 31247 31249 31253 31259 31267 31271 31277 31307 31319 31321 31327 31333 31337 31357 31379 31387 31391 31393 31397 31469 31477 31481 31489 31511 31513 31517 31531 31541 31543 31547 31567 31573 31583 31601 31607 31627 31643 31649 31657 31663 31667 31687 31699 31721 31723 31727 31729 31741 31751 31769 31771 31793 31799 31817 31847 31849 31859 31873 31883 31891 31907 31957 31963 31973 31981 31991 32003 32009 32027 32029 32051 32057 32059 32063 32069 32077 32083 32089 32099 32117 32119 32141 32143 32159 32173 32183 32189 32191 32203 32213 32233 32237 32251 32257 32261 32297 32299 32303 32309 32321 32323 32327 32341 32353 32359 32363 32369 32371 32377 32381 32401 32411 32413 32423 32429 32441 32443 32467 32479 32491 32497 32503 32507 32531 32533 32537 32561 32563 32569 32573 32579 32587 32603 32609 32611 32621 32633 32647 32653 32687 32693 32707 32713 32717 32719 32749 32771 32779 32783 32789 32797 32801 32803 32831 32833 32839 32843 32869 32887 32909 32911 32917 32933 32939 32941 32957 32969 32971 32983 32987 32993 32999 33013 33023 33029 33037 33049 33053 33071 33073 33083 33091 33107 33113 33119 33149 33151 33161 33179 33181 33191 33199 33203 33211 33223 33247 33287 33289 33301 33311 33317 33329 33331 33343 33347 33349 33353 33359 33377 33391 33403 33409 33413 33427 33457 33461 33469 33479 33487 33493 33503 33521 33529 33533 33547 33563 33569 33577 33581 33587 33589 33599 33601 33613 33617 33619 33623 33629 33637 33641 33647 33679 33703 33713 33721 33739 33749 33751 33757 33767 33769 33773 33791 33797 33809 33811 33827 33829 33851 33857 33863 33871 33889 33893 33911 33923 33931 33937 33941 33961 33967 33997 34019 34031 34033 34039 34057 34061 34123 34127 34129 34141 34147 34157 34159 34171 34183 34211 34213 34217 34231 34253 34259 34261 34267 34273 34283 34297 34301 34303 34313 34319 34327 34337 34351 34361 34367 34369 34381 34403 34421 34429 34439 34457 34469 34471 34483 34487 34499 34501 34511 34513 34519 34537 34543 34549 34583 34589 34591 34603 34607 34613 34631 34649 34651 34667 34673 34679 34687 34693 34703 34721 34729 34739 34747 34757 34759 34763 34781 34807 34819 34841 34843 34847 34849 34871 34877 34883 34897 34913 34919 34939 34949 34961 34963 34981 35023 35027 35051 35053 35059 35069 35081 35083 35089 35099 35107 35111 35117 35129 35141 35149 35153 35159 35171 35201 35221 35227 35251 35257 35267 35279 35281 35291 35311 35317 35323 35327 35339 35353 35363 35381 35393 35401 35407 35419 35423 35437 35447 35449 35461 35491 35507 35509 35521 35527 35531 35533 35537 35543 35569 35573 35591 35593 35597 35603 35617 35671 35677 35729 35731 35747 35753 35759 35771 35797 35801 35803 35809 35831 35837 35839 35851 35863 35869 35879 35897 35899 35911 35923 35933 35951 35963 35969 35977 35983 35993 35999 36007 36011 36013 36017 36037 36061 36067 36073 36083 36097 36107 36109 36131 36137 36151 36161 36187 36191 36209 36217 36229 36241 36251 36263 36269 36277 36293 36299 36307 36313 36319 36341 36343 36353 36373 36383 36389 36433 36451 36457 36467 36469 36473 36479 36493 36497 36523 36527 36529 36541 36551 36559 36563 36571 36583 36587 36599 36607 36629 36637 36643 36653 36671 36677 36683 36691 36697 36709 36713 36721 36739 36749 36761 36767 36779 36781 36787 36791 36793 36809 36821 36833 36847 36857 36871 36877 36887 36899 36901 36913 36919 36923 36929 36931 36943 36947 36973 36979 36997 37003 37013 37019 37021 37039 37049 37057 37061 37087 37097 37117 37123 37139 37159 37171 37181 37189 37199 37201 37217 37223 37243 37253 37273 37277 37307 37309 37313 37321 37337 37339 37357 37361 37363 37369 37379 37397 37409 37423 37441 37447 37463 37483 37489 37493 37501 37507 37511 37517 37529 37537 37547 37549 37561 37567 37571 37573 37579 37589 37591 37607 37619 37633 37643 37649 37657 37663 37691 37693 37699 37717 37747 37781 37783 37799 37811 37813 37831 37847 37853 37861 37871 37879 37889 37897 37907 37951 37957 37963 37967 37987 37991 37993 37997 38011 38039 38047 38053 38069 38083 38113 38119 38149 38153 38167 38177 38183 38189 38197 38201 38219 38231 38237 38239 38261 38273 38281 38287 38299 38303 38317 38321 38327 38329 38333 38351 38371 38377 38393 38431 38447 38449 38453 38459 38461 38501 38543 38557 38561 38567 38569 38593 38603 38609 38611 38629 38639 38651 38653 38669 38671 38677 38693 38699 38707 38711 38713 38723 38729 38737 38747 38749 38767 38783 38791 38803 38821 38833 38839 38851 38861 38867 38873 38891 38903 38917 38921 38923 38933 38953 38959 38971 38977 38993 39019 39023 39041 39043 39047 39079 39089 39097 39103 39107 39113 39119 39133 39139 39157 39161 39163 39181 39191 39199 39209 39217 39227 39229 39233 39239 39241 39251 39293 39301 39313 39317 39323 39341 39343 39359 39367 39371 39373 39383 39397 39409 39419 39439 39443 39451 39461 39499 39503 39509 39511 39521 39541 39551 39563 39569 39581 39607 39619 39623 39631 39659 39667 39671 39679 39703 39709 39719 39727 39733 39749 39761 39769 39779 39791 39799 39821 39827 39829 39839 39841 39847 39857 39863 39869 39877 39883 39887 39901 39929 39937 39953 39971 39979 39983 39989 40009 40013 40031 40037 40039 40063 40087 40093 40099 40111 40123 40127 40129 40151 40153 40163 40169 40177 40189 40193 40213 40231 40237 40241 40253 40277 40283 40289 40343 40351 40357 40361 40387 40423 40427 40429 40433 40459 40471 40483 40487 40493 40499 40507 40519 40529 40531 40543 40559 40577 40583 40591 40597 40609 40627 40637 40639 40693 40697 40699 40709 40739 40751 40759 40763 40771 40787 40801 40813 40819 40823 40829 40841 40847 40849 40853 40867 40879 40883 40897 40903 40927 40933 40939 40949 40961 40973 40993 41011 41017 41023 41039 41047 41051 41057 41077 41081 41113 41117 41131 41141 41143 41149 41161 41177 41179 41183 41189 41201 41203 41213 41221 41227 41231 41233 41243 41257 41263 41269 41281 41299 41333 41341 41351 41357 41381 41387 41389 41399 41411 41413 41443 41453 41467 41479 41491 41507 41513 41519 41521 41539 41543 41549 41579 41593 41597 41603 41609 41611 41617 41621 41627 41641 41647 41651 41659 41669 41681 41687 41719 41729 41737 41759 41761 41771 41777 41801 41809 41813 41843 41849 41851 41863 41879 41887 41893 41897 41903 41911 41927 41941 41947 41953 41957 41959 41969 41981 41983 41999 42013 42017 42019 42023 42043 42061 42071 42073 42083 42089 42101 42131 42139 42157 42169 42179 42181 42187 42193 42197 42209 42221 42223 42227 42239 42257 42281 42283 42293 42299 42307 42323 42331 42337 42349 42359 42373 42379 42391 42397 42403 42407 42409 42433 42437 42443 42451 42457 42461 42463 42467 42473 42487 42491 42499 42509 42533 42557 42569 42571 42577 42589 42611 42641 42643 42649 42667 42677 42683 42689 42697 42701 42703 42709 42719 42727 42737 42743 42751 42767 42773 42787 42793 42797 42821 42829 42839 42841 42853 42859 42863 42899 42901 42923 42929 42937 42943 42953 42961 42967 42979 42989 43003 43013 43019 43037 43049 43051 43063 43067 43093 43103 43117 43133 43151 43159 43177 43189 43201 43207 43223 43237 43261 43271 43283 43291 43313 43319 43321 43331 43391 43397 43399 43403 43411 43427 43441 43451 43457 43481 43487 43499 43517 43541 43543 43573 43577 43579 43591 43597 43607 43609 43613 43627 43633 43649 43651 43661 43669 43691 43711 43717 43721 43753 43759 43777 43781 43783 43787 43789 43793 43801 43853 43867 43889 43891 43913 43933 43943 43951 43961 43963 43969 43973 43987 43991 43997 44017 44021 44027 44029 44041 44053 44059 44071 44087 44089 44101 44111 44119 44123 44129 44131 44159 44171 44179 44189 44201 44203 44207 44221 44249 44257 44263 44267 44269 44273 44279 44281 44293 44351 44357 44371 44381 44383 44389 44417 44449 44453 44483 44491 44497 44501 44507 44519 44531 44533 44537 44543 44549 44563 44579 44587 44617 44621 44623 44633 44641 44647 44651 44657 44683 44687 44699 44701 44711 44729 44741 44753 44771 44773 44777 44789 44797 44809 44819 44839 44843 44851 44867 44879 44887 44893 44909 44917 44927 44939 44953 44959 44963 44971 44983 44987 45007 45013 45053 45061 45077 45083 45119 45121 45127 45131 45137 45139 45161 45179 45181 45191 45197 45233 45247 45259 45263 45281 45289 45293 45307 45317 45319 45329 45337 45341 45343 45361 45377 45389 45403 45413 45427 45433 45439 45481 45491 45497 45503 45523 45533 45541 45553 45557 45569 45587 45589 45599 45613 45631 45641 45659 45667 45673 45677 45691 45697 45707 45737 45751 45757 45763 45767 45779 45817 45821 45823 45827 45833 45841 45853 45863 45869 45887 45893 45943 45949 45953 45959 45971 45979 45989 46021 46027 46049 46051 46061 46073 46091 46093 46099 46103 46133 46141 46147 46153 46171 46181 46183 46187 46199 46219 46229 46237 46261 46271 46273 46279 46301 46307 46309 46327 46337 46349 46351 46381 46399 46411 46439 46441 46447 46451 46457 46471 46477 46489 46499 46507 46511 46523 46549 46559 46567 46573 46589 46591 46601 46619 46633 46639 46643 46649 46663 46679 46681 46687 46691 46703 46723 46727 46747 46751 46757 46769 46771 46807 46811 46817 46819 46829 46831 46853 46861 46867 46877 46889 46901 46919 46933 46957 46993 46997 47017 47041 47051 47057 47059 47087 47093 47111 47119 47123 47129 47137 47143 47147 47149 47161 47189 47207 47221 47237 47251 47269 47279 47287 47293 47297 47303 47309 47317 47339 47351 47353 47363 47381 47387 47389 47407 47417 47419 47431 47441 47459 47491 47497 47501 47507 47513 47521 47527 47533 47543 47563 47569 47581 47591 47599 47609 47623 47629 47639 47653 47657 47659 47681 47699 47701 47711 47713 47717 47737 47741 47743 47777 47779 47791 47797 47807 47809 47819 47837 47843 47857 47869 47881 47903 47911 47917 47933 47939 47947 47951 47963 47969 47977 47981 48017 48023 48029 48049 48073 48079 48091 48109 48119 48121 48131 48157 48163 48179 48187 48193 48197 48221 48239 48247 48259 48271 48281 48299 48311 48313 48337 48341 48353 48371 48383 48397 48407 48409 48413 48437 48449 48463 48473 48479 48481 48487 48491 48497 48523 48527 48533 48539 48541 48563 48571 48589 48593 48611 48619 48623 48647 48649 48661 48673 48677 48679 48731 48733 48751 48757 48761 48767 48779 48781 48787 48799 48809 48817 48821 48823 48847 48857 48859 48869 48871 48883 48889 48907 48947 48953 48973 48989 48991 49003 49009 49019 49031 49033 49037 49043 49057 49069 49081 49103 49109 49117 49121 49123 49139 49157 49169 49171 49177 49193 49199 49201 49207 49211 49223 49253 49261 49277 49279 49297 49307 49331 49333 49339 49363 49367 49369 49391 49393 49409 49411 49417 49429 49433 49451 49459 49463 49477 49481 49499 49523 49529 49531 49537 49547 49549 49559 49597 49603 49613 49627 49633 49639 49663 49667 49669 49681 49697 49711 49727 49739 49741 49747 49757 49783 49787 49789 49801 49807 49811 49823 49831 49843 49853 49871 49877 49891 49919 49921 49927 49937 49939 49943 49957 49991 49993 49999 50021 50023 50033 50047 50051 50053 50069 50077 50087 50093 50101 50111 50119 50123 50129 50131 50147 50153 50159 50177 50207 50221 50227 50231 50261 50263 50273 50287 50291 50311 50321 50329 50333 50341 50359 50363 50377 50383 50387 50411 50417 50423 50441 50459 50461 50497 50503 50513 50527 50539 50543 50549 50551 50581 50587 50591 50593 50599 50627 50647 50651 50671 50683 50707 50723 50741 50753 50767 50773 50777 50789 50821 50833 50839 50849 50857 50867 50873 50891 50893 50909 50923 50929 50951 50957 50969 50971 50989 50993 51001 51031 51043 51047 51059 51061 51071 51109 51131 51133 51137 51151 51157 51169 51193 51197 51199 51203 51217 51229 51239 51241 51257 51263 51283 51287 51307 51329 51341 51343 51347 51349 51361 51383 51407 51413 51419 51421 51427 51431 51437 51439 51449 51461 51473 51479 51481 51487 51503 51511 51517 51521 51539 51551 51563 51577 51581 51593 51599 51607 51613 51631 51637 51647 51659 51673 51679 51683 51691 51713 51719 51721 51749 51767 51769 51787 51797 51803 51817 51827 51829 51839 51853 51859 51869 51871 51893 51899 51907 51913 51929 51941 51949 51971 51973 51977 51991 52009 52021 52027 52051 52057 52067 52069 52081 52103 52121 52127 52147 52153 52163 52177 52181 52183 52189 52201 52223 52237 52249 52253 52259 52267 52289 52291 52301 52313 52321 52361 52363 52369 52379 52387 52391 52433 52453 52457 52489 52501 52511 52517 52529 52541 52543 52553 52561 52567 52571 52579 52583 52609 52627 52631 52639 52667 52673 52691 52697 52709 52711 52721 52727 52733 52747 52757 52769 52783 52807 52813 52817 52837 52859 52861 52879 52883 52889 52901 52903 52919 52937 52951 52957 52963 52967 52973 52981 52999 53003 53017 53047 53051 53069 53077 53087 53089 53093 53101 53113 53117 53129 53147 53149 53161 53171 53173 53189 53197 53201 53231 53233 53239 53267 53269 53279 53281 53299 53309 53323 53327 53353 53359 53377 53381 53401 53407 53411 53419 53437 53441 53453 53479 53503 53507 53527 53549 53551 53569 53591 53593 53597 53609 53611 53617 53623 53629 53633 53639 53653 53657 53681 53693 53699 53717 53719 53731 53759 53773 53777 53783 53791 53813 53819 53831 53849 53857 53861 53881 53887 53891 53897 53899 53917 53923 53927 53939 53951 53959 53987 53993 54001 54011 54013 54037 54049 54059 54083 54091 54101 54121 54133 54139 54151 54163 54167 54181 54193 54217 54251 54269 54277 54287 54293 54311 54319 54323 54331 54347 54361 54367 54371 54377 54401 54403 54409 54413 54419 54421 54437 54443 54449 54469 54493 54497 54499 54503 54517 54521 54539 54541 54547 54559 54563 54577 54581 54583 54601 54617 54623 54629 54631 54647 54667 54673 54679 54709 54713 54721 54727 54751 54767 54773 54779 54787 54799 54829 54833 54851 54869 54877 54881 54907 54917 54919 54941 54949 54959 54973 54979 54983 55001 55009 55021 55049 55051 55057 55061 55073 55079 55103 55109 55117 55127 55147 55163 55171 55201 55207 55213 55217 55219 55229 55243 55249 55259 55291 55313 55331 55333 55337 55339 55343 55351 55373 55381 55399 55411 55439 55441 55457 55469 55487 55501 55511 55529 55541 55547 55579 55589 55603 55609 55619 55621 55631 55633 55639 55661 55663 55667 55673 55681 55691 55697 55711 55717 55721 55733 55763 55787 55793 55799 55807 55813 55817 55819 55823 55829 55837 55843 55849 55871 55889 55897 55901 55903 55921 55927 55931 55933 55949 55967 55987 55997 56003 56009 56039 56041 56053 56081 56087 56093 56099 56101 56113 56123 56131 56149 56167 56171 56179 56197 56207 56209 56237 56239 56249 56263 56267 56269 56299 56311 56333 56359 56369 56377 56383 56393 56401 56417 56431 56437 56443 56453 56467 56473 56477 56479 56489 56501 56503 56509 56519 56527 56531 56533 56543 56569 56591 56597 56599 56611 56629 56633 56659 56663 56671 56681 56687 56701 56711 56713 56731 56737 56747 56767 56773 56779 56783 56807 56809 56813 56821 56827 56843 56857 56873 56891 56893 56897 56909 56911 56921 56923 56929 56941 56951 56957 56963 56983 56989 56993 56999 57037 57041 57047 57059 57073 57077 57089 57097 57107 57119 57131 57139 57143 57149 57163 57173 57179 57191 57193 57203 57221 57223 57241 57251 57259 57269 57271 57283 57287 57301 57329 57331 57347 57349 57367 57373 57383 57389 57397 57413 57427 57457 57467 57487 57493 57503 57527 57529 57557 57559 57571 57587 57593 57601 57637 57641 57649 57653 57667 57679 57689 57697 57709 57713 57719 57727 57731 57737 57751 57773 57781 57787 57791 57793 57803 57809 57829 57839 57847 57853 57859 57881 57899 57901 57917 57923 57943 57947 57973 57977 57991 58013 58027 58031 58043 58049 58057 58061 58067 58073 58099 58109 58111 58129 58147 58151 58153 58169 58171 58189 58193 58199 58207 58211 58217 58229 58231 58237 58243 58271 58309 58313 58321 58337 58363 58367 58369 58379 58391 58393 58403 58411 58417 58427 58439 58441 58451 58453 58477 58481 58511 58537 58543 58549 58567 58573 58579 58601 58603 58613 58631 58657 58661 58679 58687 58693 58699 58711 58727 58733 58741 58757 58763 58771 58787 58789 58831 58889 58897 58901 58907 58909 58913 58921 58937 58943 58963 58967 58979 58991 58997 59009 59011 59021 59023 59029 59051 59053 59063 59069 59077 59083 59093 59107 59113 59119 59123 59141 59149 59159 59167 59183 59197 59207 59209 59219 59221 59233 59239 59243 59263 59273 59281 59333 59341 59351 59357 59359 59369 59377 59387 59393 59399 59407 59417 59419 59441 59443 59447 59453 59467 59471 59473 59497 59509 59513 59539 59557 59561 59567 59581 59611 59617 59621 59627 59629 59651 59659 59663 59669 59671 59693 59699 59707 59723 59729 59743 59747 59753 59771 59779 59791 59797 59809 59833 59863 59879 59887 59921 59929 59951 59957 59971 59981 59999 60013 60017 60029 60037 60041 60077 60083 60089 60091 60101 60103 60107 60127 60133 60139 60149 60161 60167 60169 60209 60217 60223 60251 60257 60259 60271 60289 60293 60317 60331 60337 60343 60353 60373 60383 60397 60413 60427 60443 60449 60457 60493 60497 60509 60521 60527 60539 60589 60601 60607 60611 60617 60623 60631 60637 60647 60649 60659 60661 60679 60689 60703 60719 60727 60733 60737 60757 60761 60763 60773 60779 60793 60811 60821 60859 60869 60887 60889 60899 60901 60913 60917 60919 60923 60937 60943 60953 60961 61001 61007 61027 61031 61043 61051 61057 61091 61099 61121 61129 61141 61151 61153 61169 61211 61223 61231 61253 61261 61283 61291 61297 61331 61333 61339 61343 61357 61363 61379 61381 61403 61409 61417 61441 61463 61469 61471 61483 61487 61493 61507 61511 61519 61543 61547 61553 61559 61561 61583 61603 61609 61613 61627 61631 61637 61643 61651 61657 61667 61673 61681 61687 61703 61717 61723 61729 61751 61757 61781 61813 61819 61837 61843 61861 61871 61879 61909 61927 61933 61949 61961 61967 61979 61981 61987 61991 62003 62011 62017 62039 62047 62053 62057 62071 62081 62099 62119 62129 62131 62137 62141 62143 62171 62189 62191 62201 62207 62213 62219 62233 62273 62297 62299 62303 62311 62323 62327 62347 62351 62383 62401 62417 62423 62459 62467 62473 62477 62483 62497 62501 62507 62533 62539 62549 62563 62581 62591 62597 62603 62617 62627 62633 62639 62653 62659 62683 62687 62701 62723 62731 62743 62753 62761 62773 62791 62801 62819 62827 62851 62861 62869 62873 62897 62903 62921 62927 62929 62939 62969 62971 62981 62983 62987 62989 63029 63031 63059 63067 63073 63079 63097 63103 63113 63127 63131 63149 63179 63197 63199 63211 63241 63247 63277 63281 63299 63311 63313 63317 63331 63337 63347 63353 63361 63367 63377 63389 63391 63397 63409 63419 63421 63439 63443 63463 63467 63473 63487 63493 63499 63521 63527 63533 63541 63559 63577 63587 63589 63599 63601 63607 63611 63617 63629 63647 63649 63659 63667 63671 63689 63691 63697 63703 63709 63719 63727 63737 63743 63761 63773 63781 63793 63799 63803 63809 63823 63839 63841 63853 63857 63863 63901 63907 63913 63929 63949 63977 63997 64007 64013 64019 64033 64037 64063 64067 64081 64091 64109 64123 64151 64153 64157 64171 64187 64189 64217 64223 64231 64237 64271 64279 64283 64301 64303 64319 64327 64333 64373 64381 64399 64403 64433 64439 64451 64453 64483 64489 64499 64513 64553 64567 64577 64579 64591 64601 64609 64613 64621 64627 64633 64661 64663 64667 64679 64693 64709 64717 64747 64763 64781 64783 64793 64811 64817 64849 64853 64871 64877 64879 64891 64901 64919 64921 64927 64937 64951 64969 64997 65003 65011 65027 65029 65033 65053 65063 65071 65089 65099 65101 65111 65119 65123 65129 65141 65147 65167 65171 65173 65179 65183 65203 65213 65239 65257 65267 65269 65287 65293 65309 65323 65327 65353 65357 65371 65381 65393 65407 65413 65419 65423 65437 65447 65449 65479 65497 65519 65521 65537 65539 65543 65551 65557 65563 65579 65581 65587 65599 65609 65617 65629 65633 65647 65651 65657 65677 65687 65699 65701 65707 65713 65717 65719 65729 65731 65761 65777 65789 65809 65827 65831 65837 65839 65843 65851 65867 65881 65899 65921 65927 65929 65951 65957 65963 65981 65983 65993 66029 66037 66041 66047 66067 66071 66083 66089 66103 66107 66109 66137 66161 66169 66173 66179 66191 66221 66239 66271 66293 66301 66337 66343 66347 66359 66361 66373 66377 66383 66403 66413 66431 66449 66457 66463 66467 66491 66499 66509 66523 66529 66533 66541 66553 66569 66571 66587 66593 66601 66617 66629 66643 66653 66683 66697 66701 66713 66721 66733 66739 66749 66751 66763 66791 66797 66809 66821 66841 66851 66853 66863 66877 66883 66889 66919 66923 66931 66943 66947 66949 66959 66973 66977 67003 67021 67033 67043 67049 67057 67061 67073 67079 67103 67121 67129 67139 67141 67153 67157 67169 67181 67187 67189 67211 67213 67217 67219 67231 67247 67261 67271 67273 67289 67307 67339 67343 67349 67369 67391 67399 67409 67411 67421 67427 67429 67433 67447 67453 67477 67481 67489 67493 67499 67511 67523 67531 67537 67547 67559 67567 67577 67579 67589 67601 67607 67619 67631 67651 67679 67699 67709 67723 67733 67741 67751 67757 67759 67763 67777 67783 67789 67801 67807 67819 67829 67843 67853 67867 67883 67891 67901 67927 67931 67933 67939 67943 67957 67961 67967 67979 67987 67993 68023 68041 68053 68059 68071 68087 68099 68111 68113 68141 68147 68161 68171 68207 68209 68213 68219 68227 68239 68261 68279 68281 68311 68329 68351 68371 68389 68399 68437 68443 68447 68449 68473 68477 68483 68489 68491 68501 68507 68521 68531 68539 68543 68567 68581 68597 68611 68633 68639 68659 68669 68683 68687 68699 68711 68713 68729 68737 68743 68749 68767 68771 68777 68791 68813 68819 68821 68863 68879 68881 68891 68897 68899 68903 68909 68917 68927 68947 68963 68993 69001 69011 69019 69029 69031 69061 69067 69073 69109 69119 69127 69143 69149 69151 69163 69191 69193 69197 69203 69221 69233 69239 69247 69257 69259 69263 69313 69317 69337 69341 69371 69379 69383 69389 69401 69403 69427 69431 69439 69457 69463 69467 69473 69481 69491 69493 69497 69499 69539 69557 69593 69623 69653 69661 69677 69691 69697 69709 69737 69739 69761 69763 69767 69779 69809 69821 69827 69829 69833 69847 69857 69859 69877 69899 69911 69929 69931 69941 69959 69991 69997 70001 70003 70009 70019 70039 70051 70061 70067 70079 70099 70111 70117 70121 70123 70139 70141 70157 70163 70177 70181 70183 70199 70201 70207 70223 70229 70237 70241 70249 70271 70289 70297 70309 70313 70321 70327 70351 70373 70379 70381 70393 70423 70429 70439 70451 70457 70459 70481 70487 70489 70501 70507 70529 70537 70549 70571 70573 70583 70589 70607 70619 70621 70627 70639 70657 70663 70667 70687 70709 70717 70729 70753 70769 70783 70793 70823 70841 70843 70849 70853 70867 70877 70879 70891 70901 70913 70919 70921 70937 70949 70951 70957 70969 70979 70981 70991 70997 70999 71011 71023 71039 71059 71069 71081 71089 71119 71129 71143 71147 71153 71161 71167 71171 71191 71209 71233 71237 71249 71257 71261 71263 71287 71293 71317 71327 71329 71333 71339 71341 71347 71353 71359 71363 71387 71389 71399 71411 71413 71419 71429 71437 71443 71453 71471 71473 71479 71483 71503 71527 71537 71549 71551 71563 71569 71593 71597 71633 71647 71663 71671 71693 71699 71707 71711 71713 71719 71741 71761 71777 71789 71807 71809 71821 71837 71843 71849 71861 71867 71879 71881 71887 71899 71909 71917 71933 71941 71947 71963 71971 71983 71987 71993 71999 72019 72031 72043 72047 72053 72073 72077 72089 72091 72101 72103 72109 72139 72161 72167 72169 72173 72211 72221 72223 72227 72229 72251 72253 72269 72271 72277 72287 72307 72313 72337 72341 72353 72367 72379 72383 72421 72431 72461 72467 72469 72481 72493 72497 72503 72533 72547 72551 72559 72577 72613 72617 72623 72643 72647 72649 72661 72671 72673 72679 72689 72701 72707 72719 72727 72733 72739 72763 72767 72797 72817 72823 72859 72869 72871 72883 72889 72893 72901 72907 72911 72923 72931 72937 72949 72953 72959 72973 72977 72997 73009 73013 73019 73037 73039 73043 73061 73063 73079 73091 73121 73127 73133 73141 73181 73189 73237 73243 73259 73277 73291 73303 73309 73327 73331 73351 73361 73363 73369 73379 73387 73417 73421 73433 73453 73459 73471 73477 73483 73517 73523 73529 73547 73553 73561 73571 73583 73589 73597 73607 73609 73613 73637 73643 73651 73673 73679 73681 73693 73699 73709 73721 73727 73751 73757 73771 73783 73819 73823 73847 73849 73859 73867 73877 73883 73897 73907 73939 73943 73951 73961 73973 73999 74017 74021 74027 74047 74051 74071 74077 74093 74099 74101 74131 74143 74149 74159 74161 74167 74177 74189 74197 74201 74203 74209 74219 74231 74257 74279 74287 74293 74297 74311 74317 74323 74353 74357 74363 74377 74381 74383 74411 74413 74419 74441 74449 74453 74471 74489 74507 74509 74521 74527 74531 74551 74561 74567 74573 74587 74597 74609 74611 74623 74653 74687 74699 74707 74713 74717 74719 74729 74731 74747 74759 74761 74771 74779 74797 74821 74827 74831 74843 74857 74861 74869 74873 74887 74891 74897 74903 74923 74929 74933 74941 74959 75011 75013 75017 75029 75037 75041 75079 75083 75109 75133 75149 75161 75167 75169 75181 75193 75209 75211 75217 75223 75227 75239 75253 75269 75277 75289 75307 75323 75329 75337 75347 75353 75367 75377 75389 75391 75401 75403 75407 75431 75437 75479 75503 75511 75521 75527 75533 75539 75541 75553 75557 75571 75577 75583 75611 75617 75619 75629 75641 75653 75659 75679 75683 75689 75703 75707 75709 75721 75731 75743 75767 75773 75781 75787 75793 75797 75821 75833 75853 75869 75883 75913 75931 75937 75941 75967 75979 75983 75989 75991 75997 76001 76003 76031 76039 76079 76081 76091 76099 76103 76123 76129 76147 76157 76159 76163 76207 76213 76231 76243 76249 76253 76259 76261 76283 76289 76303 76333 76343 76367 76369 76379 76387 76403 76421 76423 76441 76463 76471 76481 76487 76493 76507 76511 76519 76537 76541 76543 76561 76579 76597 76603 76607 76631 76649 76651 76667 76673 76679 76697 76717 76733 76753 76757 76771 76777 76781 76801 76819 76829 76831 76837 76847 76871 76873 76883 76907 76913 76919 76943 76949 76961 76963 76991 77003 77017 77023 77029 77041 77047 77069 77081 77093 77101 77137 77141 77153 77167 77171 77191 77201 77213 77237 77239 77243 77249 77261 77263 77267 77269 77279 77291 77317 77323 77339 77347 77351 77359 77369 77377 77383 77417 77419 77431 77447 77471 77477 77479 77489 77491 77509 77513 77521 77527 77543 77549 77551 77557 77563 77569 77573 77587 77591 77611 77617 77621 77641 77647 77659 77681 77687 77689 77699 77711 77713 77719 77723 77731 77743 77747 77761 77773 77783 77797 77801 77813 77839 77849 77863 77867 77893 77899 77929 77933 77951 77969 77977 77983 77999 78007 78017 78031 78041 78049 78059 78079 78101 78121 78137 78139 78157 78163 78167 78173 78179 78191 78193 78203 78229 78233 78241 78259 78277 78283 78301 78307 78311 78317 78341 78347 78367 78401 78427 78437 78439 78467 78479 78487 78497 78509 78511 78517 78539 78541 78553 78569 78571 78577 78583 78593 78607 78623 78643 78649 78653 78691 78697 78707 78713 78721 78737 78779 78781 78787 78791 78797 78803 78809 78823 78839 78853 78857 78877 78887 78889 78893 78901 78919 78929 78941 78977 78979 78989 79031 79039 79043 79063 79087 79103 79111 79133 79139 79147 79151 79153 79159 79181 79187 79193 79201 79229 79231 79241 79259 79273 79279 79283 79301 79309 79319 79333 79337 79349 79357 79367 79379 79393 79397 79399 79411 79423 79427 79433 79451 79481 79493 79531 79537 79549 79559 79561 79579 79589 79601 79609 79613 79621 79627 79631 79633 79657 79669 79687 79691 79693 79697 79699 79757 79769 79777 79801 79811 79813 79817 79823 79829 79841 79843 79847 79861 79867 79873 79889 79901 79903 79907 79939 79943 79967 79973 79979 79987 79997 79999 80021 80039 80051 80071 80077 80107 80111 80141 80147 80149 80153 80167 80173 80177 80191 80207 80209 80221 80231 80233 80239 80251 80263 80273 80279 80287 80309 80317 80329 80341 80347 80363 80369 80387 80407 80429 80447 80449 80471 80473 80489 80491 80513 80527 80537 80557 80567 80599 80603 80611 80621 80627 80629 80651 80657 80669 80671 80677 80681 80683 80687 80701 80713 80737 80747 80749 80761 80777 80779 80783 80789 80803 80809 80819 80831 80833 80849 80863 80897 80909 80911 80917 80923 80929 80933 80953 80963 80989 81001 81013 81017 81019 81023 81031 81041 81043 81047 81049 81071 81077 81083 81097 81101 81119 81131 81157 81163 81173 81181 81197 81199 81203 81223 81233 81239 81281 81283 81293 81299 81307 81331 81343 81349 81353 81359 81371 81373 81401 81409 81421 81439 81457 81463 81509 81517 81527 81533 81547 81551 81553 81559 81563 81569 81611 81619 81629 81637 81647 81649 81667 81671 81677 81689 81701 81703 81707 81727 81737 81749 81761 81769 81773 81799 81817 81839 81847 81853 81869 81883 81899 81901 81919 81929 81931 81937 81943 81953 81967 81971 81973 82003 82007 82009 82013 82021 82031 82037 82039 82051 82067 82073 82129 82139 82141 82153 82163 82171 82183 82189 82193 82207 82217 82219 82223 82231 82237 82241 82261 82267 82279 82301 82307 82339 82349 82351 82361 82373 82387 82393 82421 82457 82463 82469 82471 82483 82487 82493 82499 82507 82529 82531 82549 82559 82561 82567 82571 82591 82601 82609 82613 82619 82633 82651 82657 82699 82721 82723 82727 82729 82757 82759 82763 82781 82787 82793 82799 82811 82813 82837 82847 82883 82889 82891 82903 82913 82939 82963 82981 82997 83003 83009 83023 83047 83059 83063 83071 83077 83089 83093 83101 83117 83137 83177 83203 83207 83219 83221 83227 83231 83233 83243 83257 83267 83269 83273 83299 83311 83339 83341 83357 83383 83389 83399 83401 83407 83417 83423 83431 83437 83443 83449 83459 83471 83477 83497 83537 83557 83561 83563 83579 83591 83597 83609 83617 83621 83639 83641 83653 83663 83689 83701 83717 83719 83737 83761 83773 83777 83791 83813 83833 83843 83857 83869 83873 83891 83903 83911 83921 83933 83939 83969 83983 83987 84011 84017 84047 84053 84059 84061 84067 84089 84121 84127 84131 84137 84143 84163 84179 84181 84191 84199 84211 84221 84223 84229 84239 84247 84263 84299 84307 84313 84317 84319 84347 84349 84377 84389 84391 84401 84407 84421 84431 84437 84443 84449 84457 84463 84467 84481 84499 84503 84509 84521 84523 84533 84551 84559 84589 84629 84631 84649 84653 84659 84673 84691 84697 84701 84713 84719 84731 84737 84751 84761 84787 84793 84809 84811 84827 84857 84859 84869 84871 84913 84919 84947 84961 84967 84977 84979 84991 85009 85021 85027 85037 85049 85061 85081 85087 85091 85093 85103 85109 85121 85133 85147 85159 85193 85199 85201 85213 85223 85229 85237 85243 85247 85259 85297 85303 85313 85331 85333 85361 85363 85369 85381 85411 85427 85429 85439 85447 85451 85453 85469 85487 85513 85517 85523 85531 85549 85571 85577 85597 85601 85607 85619 85621 85627 85639 85643 85661 85667 85669 85691 85703 85711 85717 85733 85751 85781 85793 85817 85819 85829 85831 85837 85843 85847 85853 85889 85903 85909 85931 85933 85991 85999 86011 86017 86027 86029 86069 86077 86083 86111 86113 86117 86131 86137 86143 86161 86171 86179 86183 86197 86201 86209 86239 86243 86249 86257 86263 86269 86287 86291 86293 86297 86311 86323 86341 86351 86353 86357 86369 86371 86381 86389 86399 86413 86423 86441 86453 86461 86467 86477 86491 86501 86509 86531 86533 86539 86561 86573 86579 86587 86599 86627 86629 86677 86689 86693 86711 86719 86729 86743 86753 86767 86771 86783 86813 86837 86843 86851 86857 86861 86869 86923 86927 86929 86939 86951 86959 86969 86981 86993 87011 87013 87037 87041 87049 87071 87083 87103 87107 87119 87121 87133 87149 87151 87179 87181 87187 87211 87221 87223 87251 87253 87257 87277 87281 87293 87299 87313 87317 87323 87337 87359 87383 87403 87407 87421 87427 87433 87443 87473 87481 87491 87509 87511 87517 87523 87539 87541 87547 87553 87557 87559 87583 87587 87589 87613 87623 87629 87631 87641 87643 87649 87671 87679 87683 87691 87697 87701 87719 87721 87739 87743 87751 87767 87793 87797 87803 87811 87833 87853 87869 87877 87881 87887 87911 87917 87931 87943 87959 87961 87973 87977 87991 88001 88003 88007 88019 88037 88069 88079 88093 88117 88129 88169 88177 88211 88223 88237 88241 88259 88261 88289 88301 88321 88327 88337 88339 88379 88397 88411 88423 88427 88463 88469 88471 88493 88499 88513 88523 88547 88589 88591 88607 88609 88643 88651 88657 88661 88663 88667 88681 88721 88729 88741 88747 88771 88789 88793 88799 88801 88807 88811 88813 88817 88819 88843 88853 88861 88867 88873 88883 88897 88903 88919 88937 88951 88969 88993 88997 89003 89009 89017 89021 89041 89051 89057 89069 89071 89083 89087 89101 89107 89113 89119 89123 89137 89153 89189 89203 89209 89213 89227 89231 89237 89261 89269 89273 89293 89303 89317 89329 89363 89371 89381 89387 89393 89399 89413 89417 89431 89443 89449 89459 89477 89491 89501 89513 89519 89521 89527 89533 89561 89563 89567 89591 89597 89599 89603 89611 89627 89633 89653 89657 89659 89669 89671 89681 89689 89753 89759 89767 89779 89783 89797 89809 89819 89821 89833 89839 89849 89867 89891 89897 89899 89909 89917 89923 89939 89959 89963 89977 89983 89989 90001 90007 90011 90017 90019 90023 90031 90053 90059 90067 90071 90073 90089 90107 90121 90127 90149 90163 90173 90187 90191 90197 90199 90203 90217 90227 90239 90247 90263 90271 90281 90289 90313 90353 90359 90371 90373 90379 90397 90401 90403 90407 90437 90439 90469 90473 90481 90499 90511 90523 90527 90529 90533 90547 90583 90599 90617 90619 90631 90641 90647 90659 90677 90679 90697 90703 90709 90731 90749 90787 90793 90803 90821 90823 90833 90841 90847 90863 90887 90901 90907 90911 90917 90931 90947 90971 90977 90989 90997 91009 91019 91033 91079 91081 91097 91099 91121 91127 91129 91139 91141 91151 91153 91159 91163 91183 91193 91199 91229 91237 91243 91249 91253 91283 91291 91297 91303 91309 91331 91367 91369 91373 91381 91387 91393 91397 91411 91423 91433 91453 91457 91459 91463 91493 91499 91513 91529 91541 91571 91573 91577 91583 91591 91621 91631 91639 91673 91691 91703 91711 91733 91753 91757 91771 91781 91801 91807 91811 91813 91823 91837 91841 91867 91873 91909 91921 91939 91943 91951 91957 91961 91967 91969 91997 92003 92009 92033 92041 92051 92077 92083 92107 92111 92119 92143 92153 92173 92177 92179 92189 92203 92219 92221 92227 92233 92237 92243 92251 92269 92297 92311 92317 92333 92347 92353 92357 92363 92369 92377 92381 92383 92387 92399 92401 92413 92419 92431 92459 92461 92467 92479 92489 92503 92507 92551 92557 92567 92569 92581 92593 92623 92627 92639 92641 92647 92657 92669 92671 92681 92683 92693 92699 92707 92717 92723 92737 92753 92761 92767 92779 92789 92791 92801 92809 92821 92831 92849 92857 92861 92863 92867 92893 92899 92921 92927 92941 92951 92957 92959 92987 92993 93001 93047 93053 93059 93077 93083 93089 93097 93103 93113 93131 93133 93139 93151 93169 93179 93187 93199 93229 93239 93241 93251 93253 93257 93263 93281 93283 93287 93307 93319 93323 93329 93337 93371 93377 93383 93407 93419 93427 93463 93479 93481 93487 93491 93493 93497 93503 93523 93529 93553 93557 93559 93563 93581 93601 93607 93629 93637 93683 93701 93703 93719 93739 93761 93763 93787 93809 93811 93827 93851 93871 93887 93889 93893 93901 93911 93913 93923 93937 93941 93949 93967 93971 93979 93983 93997 94007 94009 94033 94049 94057 94063 94079 94099 94109 94111 94117 94121 94151 94153 94169 94201 94207 94219 94229 94253 94261 94273 94291 94307 94309 94321 94327 94331 94343 94349 94351 94379 94397 94399 94421 94427 94433 94439 94441 94447 94463 94477 94483 94513 94529 94531 94541 94543 94547 94559 94561 94573 94583 94597 94603 94613 94621 94649 94651 94687 94693 94709 94723 94727 94747 94771 94777 94781 94789 94793 94811 94819 94823 94837 94841 94847 94849 94873 94889 94903 94907 94933 94949 94951 94961 94993 94999 95003 95009 95021 95027 95063 95071 95083 95087 95089 95093 95101 95107 95111 95131 95143 95153 95177 95189 95191 95203 95213 95219 95231 95233 95239 95257 95261 95267 95273 95279 95287 95311 95317 95327 95339 95369 95383 95393 95401 95413 95419 95429 95441 95443 95461 95467 95471 95479 95483 95507 95527 95531 95539 95549 95561 95569 95581 95597 95603 95617 95621 95629 95633 95651 95701 95707 95713 95717 95723 95731 95737 95747 95773 95783 95789 95791 95801 95803 95813 95819 95857 95869 95873 95881 95891 95911 95917 95923 95929 95947 95957 95959 95971 95987 95989 96001 96013 96017 96043 96053 96059 96079 96097 96137 96149 96157 96167 96179 96181 96199 96211 96221 96223 96233 96259 96263 96269 96281 96289 96293 96323 96329 96331 96337 96353 96377 96401 96419 96431 96443 96451 96457 96461 96469 96479 96487 96493 96497 96517 96527 96553 96557 96581 96587 96589 96601 96643 96661 96667 96671 96697 96703 96731 96737 96739 96749 96757 96763 96769 96779 96787 96797 96799 96821 96823 96827 96847 96851 96857 96893 96907 96911 96931 96953 96959 96973 96979 96989 96997 97001 97003 97007 97021 97039 97073 97081 97103 97117 97127 97151 97157 97159 97169 97171 97177 97187 97213 97231 97241 97259 97283 97301 97303 97327 97367 97369 97373 97379 97381 97387 97397 97423 97429 97441 97453 97459 97463 97499 97501 97511 97523 97547 97549 97553 97561 97571 97577 97579 97583 97607 97609 97613 97649 97651 97673 97687 97711 97729 97771 97777 97787 97789 97813 97829 97841 97843 97847 97849 97859 97861 97871 97879 97883 97919 97927 97931 97943 97961 97967 97973 97987 98009 98011 98017 98041 98047 98057 98081 98101 98123 98129 98143 98179 98207 98213 98221 98227 98251 98257 98269 98297 98299 98317 98321 98323 98327 98347 98369 98377 98387 98389 98407 98411 98419 98429 98443 98453 98459 98467 98473 98479 98491 98507 98519 98533 98543 98561 98563 98573 98597 98621 98627 98639 98641 98663 98669 98689 98711 98713 98717 98729 98731 98737 98773 98779 98801 98807 98809 98837 98849 98867 98869 98873 98887 98893 98897 98899 98909 98911 98927 98929 98939 98947 98953 98963 98981 98993 98999 99013 99017 99023 99041 99053 99079 99083 99089 99103 99109 99119 99131 99133 99137 99139 99149 99173 99181 99191 99223 99233 99241 99251 99257 99259 99277 99289 99317 99347 99349 99367 99371 99377 99391 99397 99401 99409 99431 99439 99469 99487 99497 99523 99527 99529 99551 99559 99563 99571 99577 99581 99607 99611 99623 99643 99661 99667 99679 99689 99707 99709 99713 99719 99721 99733 99761 99767 99787 99793 99809 99817 99823 99829 99833 99839 99859 99871 99877 99881 99901 99907 99923 99929 99961 99971 99989 99991 100003 100019 100043 100049 100057 100069 100103 100109 100129 100151 100153 100169 100183 100189 100193 100207 100213 100237 100267 100271 100279 100291 100297 100313 100333 100343 100357 100361 100363 100379 100391 100393 100403 100411 100417 100447 100459 100469 100483 100493 100501 100511 100517 100519 100523 100537 100547 100549 100559 100591 100609 100613 100621 100649 100669 100673 100693 100699 100703 100733 100741 100747 100769 100787 100799 100801 100811 100823 100829 100847 100853 100907 100913 100927 100931 100937 100943 100957 100981 100987 100999 101009 101021 101027 101051 101063 101081 101089 101107 101111 101113 101117 101119 101141 101149 101159 101161 101173 101183 101197 101203 101207 101209 101221 101267 101273 101279 101281 101287 101293 101323 101333 101341 101347 101359 101363 101377 101383 101399 101411 101419 101429 101449 101467 101477 101483 101489 101501 101503 101513 101527 101531 101533 101537 101561 101573 101581 101599 101603 101611 101627 101641 101653 101663 101681 101693 101701 101719 101723 101737 101741 101747 101749 101771 101789 101797 101807 101833 101837 101839 101863 101869 101873 101879 101891 101917 101921 101929 101939 101957 101963 101977 101987 101999 102001 102013 102019 102023 102031 102043 102059 102061 102071 102077 102079 102101 102103 102107 102121 102139 102149 102161 102181 102191 102197 102199 102203 102217 102229 102233 102241 102251 102253 102259 102293 102299 102301 102317 102329 102337 102359 102367 102397 102407 102409 102433 102437 102451 102461 102481 102497 102499 102503 102523 102533 102539 102547 102551 102559 102563 102587 102593 102607 102611 102643 102647 102653 102667 102673 102677 102679 102701 102761 102763 102769 102793 102797 102811 102829 102841 102859 102871 102877 102881 102911 102913 102929 102931 102953 102967 102983 103001 103007 103043 103049 103067 103069 103079 103087 103091 103093 103099 103123 103141 103171 103177 103183 103217 103231 103237 103289 103291 103307 103319 103333 103349 103357 103387 103391 103393 103399 103409 103421 103423 103451 103457 103471 103483 103511 103529 103549 103553 103561 103567 103573 103577 103583 103591 103613 103619 103643 103651 103657 103669 103681 103687 103699 103703 103723 103769 103787 103801 103811 103813 103837 103841 103843 103867 103889 103903 103913 103919 103951 103963 103967 103969 103979 103981 103991 103993 103997 104003 104009 104021 104033 104047 104053 104059 104087 104089 104107 104113 104119 104123 104147 104149 104161 104173 104179 104183 104207 104231 104233 104239 104243 104281 104287 104297 104309 104311 104323 104327 104347 104369 104381 104383 104393 104399 104417 104459 104471 104473 104479 104491 104513 104527 104537 104543 104549 104551 104561 104579 104593 104597 104623 104639 104651 104659 104677 104681 104683 104693 104701 104707 104711 104717 104723 104729\n","id":2339,"permalink":"https://freshrimpsushi.github.io/jp/posts/2339/","tags":null,"title":"1万番目までの素数点以下のリスト"},{"categories":"줄리아","contents":"コード 最初に、生えび寿司レストランには詳しい説明が含まれているが、ジュリアは並列処理をどれだけ容易にできるかを強調するために、わざと説明を省略したいと思っている。\nusing Base.Threads\rfor i in 1:10\rprintln(i^2)\rend 上のループを並列処理したい場合は、for文の前に@threadsをつけるだけでいい。\n@threads for i in 1:10\rprintln(i^2)\rend でも、一言だけアドバイスをすると、並列処理をしても全てが速くなるわけではないということだ。並列処理をうまく使えば非常に高いパフォーマンスを出すことができるが、コードの書きやすさが向上したからといって最適化も簡単になるわけではない。時間を測ってみるなどして、実行時間に注意しよう。\n環境 OS: Windows julia: v1.5.0 ","id":1474,"permalink":"https://freshrimpsushi.github.io/jp/posts/1474/","tags":null,"title":"ジュリアでの並列処理の方法"},{"categories":"양자역학","contents":"ベクトルの一般化 線形代数学を学んでいない理科生にとって、ベクトルは大きさと方向を持つ物理量であり、3次元空間の点を意味し、一般に $\\vec{x} = (x_{1}, x_{2}, x_{3})$ のように表される。この定義で古典力学や電磁気学を学ぶ上では大きな問題はないだろう。しかし、量子力学ではフーリエ解析、関数の内積などの概念が登場するため、ベクトルの一般化された定義を知らないと学習に大きな困難を経験する可能性がある。\n線形代数学において、ベクトルとは我々が直感的に考えるそのベクトルを抽象化したものである。3次元空間のベクトルと同じ性質を持つものを全てベクトルと呼び、ベクトルを集めた集合をベクトル空間と呼ぶ。その性質とは、我々が3次元空間の点を考えた時に当然満たされるべき性質のことである。例えば\nベクトルとベクトルを加えたものもベクトルである。 ベクトルに定数を乗じたものもベクトルである。 などがそれにあたる。その結果、3次元空間の点はベクトルになり、3次元空間はベクトル空間になる。以下には量子力学で最も重要な二つの例を紹介する。行列と関数もベクトルである。\n例 行列 サイズが $m \\times n$ の行列を集めた集合を考えてみよう。これらを加えても依然として $m \\times n$ 行列であり、何らかの定数を乗じても依然として $m \\times n$ 行列なので、この集合はベクトル空間になり、各行列はベクトルになる。\n実際、$\\mathbf{x} = (x_{1}, x_{2}, x_{3})$ のように組み合わせで表記することと $\\mathbf{x} = \\begin{bmatrix} x_{1} \u0026amp; x_{2} \u0026amp; x_{3} \\end{bmatrix}$ のように $1 \\times 3$ 行列で表記することに本質的な違いがないことを思い出してみると、行列がベクトルであるということがより理解しやすくなるだろう。\n関数 連続関数の集合を考えてみよう。$f$ と $g$ が連続関数であれば、これらを加えた $f+g$ も依然として連続関数である。また、任意の定数を乗じた $cf$ も依然として連続関数である。したがって、連続関数の集合はベクトル空間になり、各連続関数はベクトルになる。\n実際、関数値が3次元ベクトルであるベクトル関数の場合、以下のように記述されることを思い出してみよう。\n$$ f(x,y,z) = (xy, yz, z^{2}) $$\n内積の一般化 内積はベクトルを扱う際に非常に便利に使われる演算である。ベクトルという概念を一般化したように、内積の概念も一般化してみよう。まず一般化された内積の表記では、点 $\\cdot$ の代わりに二重山括弧 $\\left\\langle \\ ,\\ \\right\\rangle$ を使用する。$\\mathbf{x} = \\left( x_{1}, x_{2}, x_{3} \\right)$、$\\mathbf{y}=\\left( y_{1}, y_{2}, y_{3} \\right)$ とすると、以下のように表記される。\n$$ \\mathbf{x} \\cdot \\mathbf{y} = x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} = \\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle $$\n量子力学では、中にコンマの代わりに線 $|$ を使用する。\n$$ \\mathbf{x} \\cdot \\mathbf{y} = \\braket{\\mathbf{x} \\vert \\mathbf{y} } $$\nこれをディラック記法という。ベクトルを一般化する際の核心は、「私たちがベクトルだと思うものが満たすべき性質」を満たすならば、それが何であれベクトルと呼ぶ点にある。内積の一般化でも同様に、「各成分を掛け合わせてすべて加算する」というコンセプトをそのまま保持する。どのようなベクトル空間を扱うかによって、内積の定義は以下のように異なる。\n例 行列 二つの行列 $A = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{21} \u0026amp; a_{22} \\end{pmatrix}, B = \\begin{pmatrix} b_{11} \u0026amp; b_{12} \\\\ b_{21} \u0026amp; b_{22} \\end{pmatrix}$ があるとする。これらの内積は、3次元ベクトルの内積と同じように「各成分の掛け算の和」として定義される。\n$$ \\braket{ A \\vert B } = a_{11}b_{11} + a_{12}b_{12} + a_{21}b_{21} + a_{22}b_{22} $$\n関数 上で述べたように、関数もベクトルであるので、二つの関数の内積も定義できる。関数の内積は以下のように定積分で定義される。\n$$ \\braket{\\psi \\vert \\phi} = \\int \\psi^{\\ast}(x) \\phi (x) dx $$\nここで、$\\psi^{\\ast}$ は $\\psi$ の複素共役を意味する。ただし、表記法にあいまいさがあるので注意しよう。関数の内積をなぜこのように定義するのかについては、\u0026lsquo;関数の内積を定積分で定義する理由\u0026rsquo;に詳しく説明されているので参照しよう。\n波動関数 量子力学において波動関数は、位置と時間に応じた粒子の状態を表現する関数であり、以下のように指数関数で表現される。\n$$ \\psi (x,t) = e^{i(kx + \\omega t)} $$\n主に $\\psi$ と $\\phi$ で表記され、それぞれ [プサイ]、[ファイ] と読む。$k$ は波数wave numberであり、運動量との関係 $p = \\hbar k$ を満たす。ここで $\\hbar$ は定数であるので、量子力学\nにおいては $k$ を運動量と同じと理解しても構わない。$\\omega$ は角振動数angular frequencyであり、エネルギーとの関係 $E = \\hbar \\omega$ を満たす。\nヒルベルト空間 ヒルベルト空間の厳密な定義は完備内積空間である。その数学的意味を理解しているともちろん良いが、物理学部の学生にとっては必須ではない。重要な点は、非常に良い性質を持つ集合にヒルベルト空間という名前を付けることと、波動関数を集めた集合がヒルベルト空間になるという点である。したがって、様々な良い数学的ツールを使って波動関数を扱うことができる。\n","id":1509,"permalink":"https://freshrimpsushi.github.io/jp/posts/1509/","tags":null,"title":"量子力学でベクトル、内積、波動関数, ヒルベルト空間"},{"categories":"최적화이론","contents":"定義 関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}$の関数値を最小にする$x^{ \\ast } = \\argmin_{x} f(x)$を見つける問題を最適化問題Optimization Problemと呼び、その問題を解くアルゴリズムを最適化技法と呼ぶ。最適化問題で与えられた関数$f$は特に目的関数Objective Functionと言う。 全ての$x$に対して$f(x^{ \\ast }) \\le f(x)$を満たす$x^{ \\ast }$を全域最適解Global Optimizerと呼ぶ。 全ての$x \\in \\mathcal{N}$に対して$f(x^{ \\ast }) \\le f(x)$を満たす$x^{ \\ast }$の近傍$\\mathcal{N}$が存在する場合、その$x^{ \\ast }$を局所最適解Local Optimizerと呼ぶ。 これらの定義では、不等式の向きが逆でも、つまり最大化について説明されても、それらは総じて最適化と呼ばれる。\n説明 最適化技法を使用する人々は「利益を最大化」または「コストを最小化」と言うことができるが、数学者にとっては最大か最小かは重要な問題ではない。最小化が最適化とほぼ同義とされる理由は、最小化問題で使用されるアルゴリズムが、$-1$を単に乗じることで最大化問題にも適用できるからであり、数学的に非常に重要な関数であるメトリックやノルムが$0$以上の実数の集合を値域に持つから、つまり最小値が存在するという理由からである。\n近年、ディープラーニングが流行しているが、目的関数（またはコスト関数、損失関数）は通常、スムーズであると想定されているが、必ずそうであるとは限らない。したがって、それを克服するためのアルゴリズムとメソッドも研究されてきた。目的関数の定義域が必ずしも$\\mathbb{R}^{n}$でなければならないわけではない。\n全域最適解 最適解の存在性は、数々の条件によって示すことができるかもしれないが、局所最適解が全域最適解であることを示す定理はない。理想的には誰もが最適解を見つけたいと思うが、実際に見つけた解が最適解であることを心から期待することはほとんどない。最適化問題は多くあるが、すべての問題にピッタリ合う「最適化された」最適化技法はないため、数多くの改善アルゴリズムが開発されてきた。\n最適解の厳密さと孤立性 通常は以下の定義は無意味であるが、一応言及しておく。\n全ての$x \\in \\mathcal{N}$に対して$f(x^{ \\ast }) \u0026lt; f(x)$を満たす$x^{ \\ast }$の近傍$\\mathcal{N}$が存在する場合、その$x^{ \\ast }$を厳密な局所最適解Strict Local Optimizerと呼ぶ。 全ての$x \\in \\mathcal{N}$に対して$f(x^{ \\ast }) \u0026lt; f(x)$を満たす$x^{ \\ast }$の唯一の近傍$\\mathcal{N}$が存在する場合、その$x^{ \\ast }$を孤立した局所最適解Isolated Local Optimizerと呼ぶ。 ","id":1463,"permalink":"https://freshrimpsushi.github.io/jp/posts/1463/","tags":null,"title":"数学における最適化技術"},{"categories":"줄리아","contents":"概要 マクロは、Juliaでコーディングする時の便利機能であり、スコープの前に置いて実行される。例えば、自分のプログラムがどれくらいの時間を消費するか知りたい場合、次のように書くといい。\n@time for t in 1:10\rfoo()\rbar()\rend 例 多くの種類があるが、以下のマクロが特に広く使われている:\n@time：後に続く関数やスコープの実行時間を測定する。どんな状況でどう最適化すべきか分からない時、まず時間を測って、良い方を選ぶのが楽になる。言語によっては、時間を測るためのコードを書くのが面倒な場合があるが、Juliaの場合は、マクロ一つで実行時間だけでなく、メモリの使用量まで教えてくれる。 @.：その後に続く式の演算にドット(.)を追加する。 @threads：並列処理を簡単に実装できるマクロだ。 @animate：GIFを簡単に焼くマクロだ。 環境 OS: Windows julia: v1.5.0 ","id":1454,"permalink":"https://freshrimpsushi.github.io/jp/posts/1454/","tags":null,"title":"ジュリアの強力な便利機能、マクロ"},{"categories":"줄리아","contents":"概要 ジュリアはデータを扱う上での強みを生かして、パイプラインオペレーターをサポートしている。\nコード julia\u0026gt; (1:5) .|\u0026gt; (x -\u0026gt; sqrt(x+2)) .|\u0026gt; sin |\u0026gt; minimum\r0.4757718381527513\rjulia\u0026gt; minimum(sin.((x -\u0026gt; sqrt(x+2)).(1:5)))\r0.4757718381527513 上のサンプルコードは、配列 $[1,2,3,4,5]$ を $\\sqrt{x + 2}$ に入れ、その結果を $\\sin$ に入れて、その中の最小値を得るコードであり、上記のコードと以下のコードは完全に同じ結果を出す。複雑なコードを書く中でパイプラインがどれだけ便利であるかは、説明するまでもないだろう。配列を入れる時は、各要素を個別に扱うために必ずドットを使用する点だけ注意すれば、他の言語のパイプラインと同じように使用できる。\n他の言語 R でのパイプラインオペレーター 環境 OS: Windows julia: v1.5.0 ","id":1450,"permalink":"https://freshrimpsushi.github.io/jp/posts/1450/","tags":null,"title":"ジュリアでパイプオペレータを使用する方法"},{"categories":"수리통계학","contents":"定義 1 標本空間 $\\Omega$で定義された$n$個の確率変数 $X_{i}$に対し$X = (X_{1} , \\cdots , X_{n})$を$n$次元ランダムベクトルRandom Vectorという。$X$の値域$X(\\Omega)$を空間とも呼ぶ。 次のを満たす関数$F_{X} : \\mathbb{R}^{n} \\to [0,1]$を$X$のジョイントJoint累積分布関数という。 $$ F_{X}\\left( x_{1}, \\cdots , x_{n} \\right) := P \\left[ X_{1} \\le x_{1} , \\cdots , X_{n} \\le x_{n} \\right] $$ ある$h_{1} , \\cdots , h_{n} \u0026gt;0$に対し、次のを満たす関数$M_{X}$が存在するなら、$X$の積率生成関数という。 $$ M_{X} (t_{1}, \\cdots , t_{n}) := E \\left[ e^{\\sum_{k=1}^{n} t_{k} X_{k} } \\right] = E \\left[ \\prod_{k=1}^{n} e^{t_{k} X_{k}} \\right] \\\\ |t_{1}| \u0026lt; h_{1} , \\cdots , |t_{n} | \u0026lt; h_{n} $$ 離散 D1: $X$の空間が可算集合なら、$X$は離散ランダムベクトルという。 D2: 次を満たす$p_{X} : \\mathbb{R}^{n} \\to [0,1]$を離散ランダムベクトル$X$のジョイント確率質量関数という。 $$ p_{X} (x_{1} , \\cdots , x_{n}) := P \\left[ X_{1} = x_{1} , \\cdots , X_{n} = x_{n} \\right] $$ D3: $1 \\le k \\le n$に対し、次のような$P_{X_{k}} (x_{k})$をマージナル確率質量関数という。 $$ P_{X_{k}} (x_{k}) := \\sum_{x_{1}} \\cdots \\sum_{x_{k-1}}\\sum_{x_{k+1}} \\cdots \\sum_{x_{n}} p_{X} (x_{1} , \\cdots , x_{n}) $$ D4: $S_{X}:= \\left\\{ \\mathbb{x} \\in \\mathbb{R}^{n} : p_{X}(\\mathbb{x}) \u0026gt; 0 \\right\\}$を$X$のサポートという。 連続 C1: 確率変数$X$の累積分布関数$F_{X} = F_{X_{1} , \\cdots , X_{n}}$が全ての$\\mathbb{x} \\in \\mathbb{R}^{n}$で連続なら、$X$は連続ランダムベクトルという。 C2: 次を満たす$f_{X} : \\mathbb{R}^{n} \\to [0,\\infty)$を、連続ランダムベクトル$X$のジョイント確率密度関数という。 $$ F_{X} (x_{1}, \\cdots, x_{n}) = \\int_{-\\infty}^{x_{1}} \\cdots \\int_{-\\infty}^{x_{n}} f_{\\mathbb{x}} (t_{1} , \\cdots , t_{n}) dt_{1} \\cdots d t_{n} $$ C3: $1 \\le k \\le n$に対し、次のような$f_{X_{k}} (t_{k})$をマージナル確率密度関数という。 $$ f_{X_{k}}(t_{k}) := \\int_{\\infty}^{x_{1}} \\cdots \\int_{\\infty}^{x_{k-1}} \\int_{\\infty}^{x_{k+1}} \\cdots \\int_{\\infty}^{x_{n}} f_{X}(t_{1} , \\cdots , t_{n}) dt_{1} \\cdots d_{k-1} d_{k+1} \\cdots d_{n} $$ C4: $S_{X} := \\left\\{ \\mathbb{t} \\in \\mathbb{R}^{n} : f_{X} ( \\mathbb{t} ) \u0026gt; 0 \\right\\}$を$X$のサポートという。 元々ランダムベクトルRandom Vectorは、確率ベクトルと訳されるが、高校卒業以上でStochasticやProbabilisticなどと混同されることを避けるため、原語をそのまま使う。 元々ジョイント累積分布関数Joint Cumulative Distribution Functionは、結合確率分布と訳されるが、独立や依存に対する誤解を招く可能性があるため、原語をそのまま使う。 元々マージナル分布Marginal Distributionは、周辺分布と訳されるが、経済学の限界Marginalのようにその意味が伝わりにくいと思われるため、原語をそのまま使う。 説明 多変量確率分布は、一変量確率分布を多次元に一般化したものであり、変数が複数ある点で根本的に大きな違いがあるが、少なくとも学部レベルの数理統計学では、微積分学的なスキルでも十分に異なることができる。どのように異なるか見てみよう：\n1: 混同してはいけないのは、ランダムベクトル$X : \\Omega^{n} \\to \\mathbb{R}^{n}$も依然として関数であることだ。そのため、その値域を考えることができ、これにより多変量に関しても離散型と連続型に分類する。 C2: 連続のジョイント密度関数は、一般的に確率が$0$の$A \\subset \\mathbb{R}^{n}$を除き、微積分学の基本定理に従って次のを満たすように定義される。 $$ {{ \\partial^{n} } \\over { \\partial x_{1} \\cdots \\partial x_{n} }} F_{X} (\\mathbb{x}) = f(\\mathbb{x}) $$ D3, C3: 式は複雑だが、簡単に言えば、ジョイント確率分布を純粋に確率変数$X_{k}$に関する分布に変えたものだ。経済学でマージナルという言葉が微分の概念と通じるのと反対に、数理統計学では関心のない変数を一掃するために積分や合計をすることだ。 Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p75~84.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1449,"permalink":"https://freshrimpsushi.github.io/jp/posts/1449/","tags":null,"title":"数理統計学における多変量確率分布"},{"categories":"줄리아","contents":"概要 Juliaでは、ラムダ式は以下のように定義される。\n(x -\u0026gt; 3x^2 - 2x + 3)(1) これは、匿名関数$\\lambda : \\mathbb{Z} \\to \\mathbb{Z}$を定義し、そこに$1$を代入して、$4$という関数値を得たものに相当する。 $$ \\lambda : x \\mapsto ( 3 x^{2} - 2 x + 3 ) \\\\ \\lambda (1) = 4 $$ 実際、ラムダ式自体はJuliaの特徴ではなく、MATLABやPythonを含む関数型言語に影響を受けて、ほぼ自然にサポートされているもので、Juliaに興味を持つ学習者ならすでにラムダ式を使った経験が多いかもしれない。しかし、これまで使ってきた「それ」がラムダ式であるかどうか知らなかったり、まだその真の価値を知らない読者のために、特にすぐに適用できるであろう例を2つ紹介する。\n例1：リストを異なる尺度でソート リストをソートするのは組み込み関数を使えば非常に簡単だが、多次元配列でカテゴリー別に優先度を設定してソーティングしたり、元のデータを異なる基準でソートしたい場合がある。そんな時は、sort()関数のbyオプションに該当する関数をラムダ式として入れることで、簡単にコードを書くことができる。\njulia\u0026gt; # Example 1\rjulia\u0026gt; example = rand(-20:20,10)\r10-element Array{Int64,1}:\r3\r8\r19\r-12\r-20\r9\r-13\r19\r13\r2\rjulia\u0026gt; sort(example, by=(x -\u0026gt; abs(x)))\r10-element Array{Int64,1}:\r2\r3\r8\r9\r-12\r-13\r13\r19\r19\r-20 上記の作業は、ランダムに選ばれた整数を絶対値が小さいものから大きいものに基づいてソートすることを示している。ラムダ式がなくても不可能ではないが、思ったより単純ではない。与えられたラムダ式(x -\u0026gt; abs(x))を上手く変えれば、コーダーが望むコードを簡単に書くことができるだろう。\n例1の応用 valueという辞書が以下のように作成されているとする。 この時、辞書の値の大きさ順でソートするコードは、ラムダ式を活用してsort(value,by=(x -\u0026gt; value[x]))のように簡単に組むことができ、その実行結果は以下の通りだ。 例2：リストの頻度計算 Rのようにデータを最優先にする言語では、最初から組み込み関数で作られているが、この頻度計算が思うほど単純ではない。アルゴリズムと呼べるほど複雑な作業ではないが、実際にやってみるとかなり手がかかる。これもまた、ラムダ関数を利用して簡単に解決できる！\njulia\u0026gt; # Example 2\rjulia\u0026gt; example = rand(1:3,10); println(example)\r[3, 1, 2, 2, 3, 2, 3, 1, 3, 3]\rjulia\u0026gt; uexample = sort(unique(example))\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; counts = map(x-\u0026gt;count(y-\u0026gt;x==y,example),uexample)\r3-element Array{Int64,1}:\r2\r3\r5 上記の作業は、ランダムに選ばれた整数の頻度を数えたものである。unique()でデータの階級を把握し、それぞれの階級に該当する要素をカウントする方式で、単純に問題を解決した。\n環境 OS: Windows julia: v1.5.0 ","id":1448,"permalink":"https://freshrimpsushi.github.io/jp/posts/1448/","tags":null,"title":"ジュリアでのラムダ式"},{"categories":"줄리아","contents":"画像サイズの変更 Images パッケージの imresize を使えばいい。関数名はMatlabと同じだ。\nimresize(X, ratio=a): 配列Xをa倍に調整した画像を返す。Matlabとは違って、ただ比率を書くだけではなく、必ず ratio=a と書かなければならない。\nimresize(X, m, n): 配列Xをm行n列に拡大/縮小した画像を返す。以下は例示コードとその結果だ。\nusing Images\rX=load(\u0026#34;example\\_{i}mage2.jpg\u0026#34;)\rY1=imresize(X, ratio=0.5)\rY2=imresize(X,500,500)\rY3=imresize(X,1500,1500)\rY4=imresize(X,700,1000)\rY5=imresize(X,1000,1300)\rY6=imresize(X,300,300)\rsave(\u0026#34;X.png\u0026#34;,colorview(RGB,X))\rsave(\u0026#34;Y1=imresize(0.5).png\u0026#34;,colorview(RGB,Y1))\rsave(\u0026#34;Y2=imresize(500,500).png\u0026#34;,colorview(RGB,Y2))\rsave(\u0026#34;Y3=imresize(1500,1500).png\u0026#34;,colorview(RGB,Y3))\rsave(\u0026#34;Y4=imresize(700,1000).png\u0026#34;,colorview(RGB,Y4))\rsave(\u0026#34;Y5=imresize(1000,1300).png\u0026#34;,colorview(RGB,Y5))\rsave(\u0026#34;Y6=imresize(300,300).png\u0026#34;,colorview(RGB,Y6)) 参照 Matlabで画像サイズを調整する方法 環境 OS: Windows10 Version: 1.5.3 (2020-11-09) ","id":1466,"permalink":"https://freshrimpsushi.github.io/jp/posts/1466/","tags":null,"title":"ジュリアで画像サイズを変更する方法"},{"categories":"줄리아","contents":"コード using Images\rcd(\u0026#34;C:/Users/rmsms/OneDrive/examples\u0026#34;)\rpwd()\rexample = load(\u0026#34;example.jpg\u0026#34;)\rtypeof(example)\rsize(example)\rgray1 = Gray.(example)\rtypeof(gray1)\rsize(gray1)\rM = convert(Array{Float64},gray1)\rtypeof(M)\rsize(M)\rcolorview(Gray, M.^(1/2))\rsave(\u0026#34;rgb.png\u0026#34;, colorview(RGB, example))\rsave(\u0026#34;gray1.png\u0026#34;, colorview(Gray, gray1))\rsave(\u0026#34;gray2.png\u0026#34;, colorview(Gray, transpose(gray1)))\rsave(\u0026#34;gray3.png\u0026#34;, colorview(Gray, M.^(1/2))) 上から順にサンプルコードを簡単に理解してみよう:\ncd() : Change Directory, 作業ディレクトリを望む場所に変える。\npwd() : Print Working Directory, 作業ディレクトリを出力する。例をそのまま試したいなら、上のファイルを作業ディレクトリにダウンロードして、ファイル名をexample.jpgに変更しよう。\nload() : 作業ディレクトリ内の画像ファイルを読み込む。読み込まれた画像のタイプは Array{RGB{Normed{UInt8,8}},2}だ。 これは他の言語でカラー画像を3つの行列を1つのテンソルとして表現するのとは少し違う。実際、そのような方法は数学的には理解しやすいかもしれないが、色空間やライブラリによって統一された規格がなく多くの混乱を招いてきた。Juliaではタイプを1つにして幅と高さが2次元の配列として理解する。画像は読み込まれた瞬間にPlotパネルにプリントされる。 Gray() : 画像を白黒に変換するのに使われた。実際に使われるのはGray()ではなくGray.()であることに注意しよう。この関数自体は1つのピクセルを白黒に変えるもので、ドットをつけることで全てのピクセルに適用されることを意味する。\nsize() : 画像のサイズを返す。述べたように、カラーと白黒を形が違う別のテンソルとして扱わず、データタイプが異なりサイズが同じ配列として扱っている。 Convert() : Array{Float64}、すなわち行列にgray1画像を変換した。すると、0が黒色で1が白色の行列によって白黒画像が表される。\ncolorview() : この関数自体が画像や行列を出力する関数だ。わざわざ行列を画像に再変換する必要はなく、直接Plotパネルで画像を確認できる。サンプルコードでは、行列 $M$ の各成分にルートを取っている。行列の全ての成分は $[0,1]$ に属するので、この変換は画像を全体的に明るく補正することに該当する。 save() : 作業ディレクトリに画像を保存する:gray1.png : 白黒で保存された。gray2.png : 白黒であり、転置行列状態で保存された。gray3.png : 白黒であり、明るく補正された状態で保存された。rgb.png : オリジナルの色をそのまま持った状態で保存された。\n環境 OS: Windows julia: v1.5.0 ","id":1446,"permalink":"https://freshrimpsushi.github.io/jp/posts/1446/","tags":null,"title":"ジュリアで画像を読み込み、行列として保存する方法"},{"categories":"줄리아","contents":"画像の回転 imrotate(X, theta) : 配列Xをthetaラジアンで回転させる。ここで注意すべき点は、角度の単位が度（$^{\\circ})$のMATLABと異なり、角度の単位はラジアンであることだ。さらに、MATLABとは異なり時計回りに回転する。他の変数を入力しない場合の補間法はバイリニアであり、回転された画像は切り取られない。元の画像Xを$90^\\circ=\\pi/2$、$180^\\circ=\\pi$、$270^\\circ=\\frac{3}{2}\\pi$だけ回転させた例とその結果は下のようになる。\nusing Images\rusing Interpolations\rX=load(\u0026#34;example\\_{i}mage.png\u0026#34;)\rY1=imrotate(X,pi/2)\rY2=imrotate(X,pi)\rY3=imrotate(X,pi*3/2) 上のように回転させると、元の配列がぴったりと収まるので、画像のサイズは変わらない。しかし、$90$の倍数ではない角度で回転させると、元の形に合わなくなる。そのため、元の画像のすべての点を表現するために、以下のように画像が大きくなる。元の画像のサイズを維持したい場合は、変数axes()を追加すればいい。\nY4=imrotate(X,pi/6)\rY5=imrotate(X,pi/6,axes(X)) また、InterpolationsパッケージのConstant()を使用すると、補間法をnearest1に適用することができる。計算に最も近い点のみを使用するので、精度は低下するが、計算は速い。対照的に、bilinear2の場合は周囲の四点すべてを計算に使用するので、相対的に速度は遅いが、より精確である。ここで精確とは、回転した際に画像が損傷する度合いが低いという意味だ。以下の画像を見てみよう。画像が大きく、一見すると違いが分からないかもしれないが、拡大すると二つの補間法の違いがはっきりとわかる。\nY6=imrotate(X,pi/6,Constant()) 参照 MATLABで画像を回転させる方法 環境 OS: Windows10 Version: 1.5.3 (2020-11-09) 最近傍補間法, 最近隣補間法\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n双線形補間法, 二線形補間法\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1462,"permalink":"https://freshrimpsushi.github.io/jp/posts/1462/","tags":null,"title":"ジュリアで画像配列を回転する方法"},{"categories":"줄리아","contents":"$A = \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \\\\ 2 \u0026amp; 3 \u0026amp; 4\\end{pmatrix}$としよう。\n転置行列 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; transpose(A)\r3×3 LinearAlgebra.Transpose{Int64,Array{Int64,2}}:\r1 0 2\r2 3 3\r1 0 4\rjulia\u0026gt; A\u0026#39;\r3×3 LinearAlgebra.Adjoint{Int64,Array{Int64,2}}:\r1 0 2\r2 3 3\r1 0 4 行列の要素が実数の場合、transpose()と'は同じ行列を返すが、データ型が微妙に異なる。これは'が正確には転置ではなく共役転置であるためだ。したがって、実数の行列の場合は実質的に同じ行列を返し、複素数の行列の場合は全く異なる結果を返す。\njulia\u0026gt; A_complex=[1+im 2 1+im;\r0 3 0+im;\r2 3+im 4]\r3×3 Array{Complex{Int64},2}:\r1+1im 2+0im 1+1im\r0+0im 3+0im 0+1im\r2+0im 3+1im 4+0im\rjulia\u0026gt; transpose(A_complex)\r3×3 LinearAlgebra.Transpose{Complex{Int64},Array{Complex{Int64},2}}:\r1+1im 0+0im 2+0im\r2+0im 3+0im 3+1im\r1+1im 0+1im 4+0im\rjulia\u0026gt; A_complex\u0026#39;\r3×3 LinearAlgebra.Adjoint{Complex{Int64},Array{Complex{Int64},2}}:\r1-1im 0+0im 2+0im\r2+0im 3+0im 3-1im\r1-1im 0-1im 4+0im 繰り返し乗算 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; A^2\r3×3 Array{Int64,2}:\r3 11 5\r0 9 0\r10 25 18\rjulia\u0026gt; A*A\r3×3 Array{Int64,2}:\r3 11 5\r0 9 0\r10 25 18\rjulia\u0026gt; A^3\r3×3 Array{Int64,2}:\r13 54 23\r0 27 0\r46 149 82\rjulia\u0026gt; A*A*A\r3×3 Array{Int64,2}:\r13 54 23\r0 27 0\r46 149 82 A^2とA*Aは完全に同じ結果を返す。同様に、A^3とA*A*Aも同じである。\n要素ごとの乗算、要素ごとの除算 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; A.*A\r3×3 Array{Int64,2}:\r1 4 1\r0 9 0\r4 9 16\rjulia\u0026gt; A./A\r3×3 Array{Float64,2}:\r1.0 1.0 1.0\rNaN 1.0 NaN\r1.0 1.0 1.0 各要素を乗算または除算した結果を返す。\n左右反転、上下反転 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; reverse(A,dims=1)\r3×3 Array{Int64,2}:\r2 3 4\r0 3 0\r1 2 1\rjulia\u0026gt; reverse(A,dims=2)\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r4 3 2 reverse(A,dims=1)は行列$A$を上下に反転した行列を返し、MATLABでのflipud(A)に相当する。reverse(A,dims=2)は行列$A$を左右に反転した行列を返し、MATLABでのfliplr(A)に相当する。\n逆行列 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; inv(A)\r3×3 Array{Float64,2}:\r2.0 -0.833333 -0.5\r0.0 0.333333 0.0\r-1.0 0.166667 0.5 行列$A$の逆行列を返す。逆行列を見つけることができない場合は、エラーが発生する。\n環境 OS: Windows10 Version: 1.5.3 (2020-11-09) ","id":1460,"permalink":"https://freshrimpsushi.github.io/jp/posts/1460/","tags":null,"title":"ジュリアでの2次元配列操作の関数들"},{"categories":"줄리아","contents":"Heatmap Plots パッケージのheatmap関数を使えば、2次元配列をヒートマップ画像として出力でき、savefig関数でその画像を保存できる。@__DIR__はジュリアコードファイルの位置を教えてくれるマクロだ。\n# code1 だが、配列Aとヒートマップ画像を比較すると、配列の上下が逆さまに作成されたヒートマップ画像になっていることが分かる。出力画像がこのように作成される理由は、それぞれの成分の位置を行と列ではなく直交座標系の座標として考えるからだというのが公式のカニの話だ。つまり、例えば行列$A$では、19という値は4行4列の成分ではなく、直交座標$(4,4)$の成分と見なすわけだ。これが行列と画像が上下逆さまになっている理由を説明する。\nしたがって、配列と同じ見た目に出力されるようにしたいのであれば、yflip=trueオプションを追加すればいい1。\n# code2 また、MATLABに慣れているユーザーは、color=:bgyオプションを使用すれば、MATLABの基本色と似た出力が得られる。\n# code3 色のテーマ 使用できる色のテーマは、次のようなものがある。\n参照 マットラボから 環境 OS: Windows10 バージョン: 1.5.3 (2020-11-09) https://github.com/JuliaPlots/Makie.jl/issues/46#issuecomment-357023505\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1459,"permalink":"https://freshrimpsushi.github.io/jp/posts/1459/","tags":null,"title":"ジュリアで配列をヒートマップ画像として出力保存する方法"},{"categories":"줄리아","contents":"概要 Juliaでは、Pythonと同様にセットデータ型をサポートしています。元来のセットデータ型がそうであるように、使用する人にとっては非常に便利で、使用しない人にはまったく使われないものですが、Juliaは言語設計自体が数学に近いため、セットの概念と操作がしっかりと実装されており、必ず理解しておくべきです。 特に、他の言語、特にPythonと最も異なる点は、ユニコード記号もコードの一部として使用できることです。AtomエディタでJunoを使用している場合、上記のようにTeXコードを自動補完することができます。このコンテキストでは、$\\in$は単なる記号ではなく、実際に要素がセットに属しているかを表しています。\nコード julia\u0026gt; X = Set([1,2,3,1]); print(X)\rSet([2, 3, 1])\rjulia\u0026gt; X[1]\rERROR: MethodError: no method matching getindex(::Set{Int64}, ::Int64)\rStacktrace:\r[1] top-level scope at REPL[23]:1\rjulia\u0026gt; for i in X print(i) end\r231 上記のコードは、セット $X$ を $X := \\left\\{ 1, 2, 3, 1 \\right\\} = \\left\\{ 2,3,1 \\right\\}$ として定義することを意味します。数学でのセットと同じように、重複と順序の概念は存在しません。したがって、最初のインデックスを参照するとエラーが出ます。しかし、データ型自体はPythonと同様にイテラブルであるため、ループ内で使用することができます。\njulia\u0026gt; if 1∈X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if 0∈X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r?\rjulia\u0026gt; if 0∉X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if [1,2] ⊆ X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if [0,1,2] ⊆ X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r? これが数式を読むような親しみを感じるなら、セットデータ型を有効に使用する準備ができていることです。特に注意すべき点は、上記のような計算がセットデータ型に限定されて動作するわけではないことです。つまり、リストに対しても同様の操作を気軽に使用することができます。セットデータ型が不慣れであっても、セットにだけ慣れていれば、Juliaのセット演算子を利用することに問題はありません。なお、包含関係は \\subset $\\subset$ ではなく \\subseteq $\\subseteq$ を使用する必要があります。\njulia\u0026gt; Y = [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;,3]\r3-element Array{Any,1}:\r\u0026#34;1\u0026#34;\r\u0026#34;2\u0026#34;\r3\rjulia\u0026gt; ∪(X,Y)\rSet{Any} with 5 elements:\r\u0026#34;1\u0026#34;\r2\r3\r\u0026#34;2\u0026#34;\r1\rjulia\u0026gt; ∩(X,Y)\rSet{Int64} with 1 element:\r3\rjulia\u0026gt; ∩(Y,X)\r1-element Array{Any,1}:\r3\rjulia\u0026gt; setdiff(X,Y); X\rSet{Int64} with 3 elements:\r2\r3\r1\rjulia\u0026gt; setdiff!(X,Y); X\rSet{Int64} with 2 elements:\r2\r1 基本的にセットを扱うため、和集合と交差点も数式と同様に表現することができます。2行目と3行目の違いは操作の順序です。$X$ はJuliaでセットデータ型、$Y$はただの配列として定義されており、返される値は最初の引数のデータ型に従います。このような違いは、Juliaのような強い型付け言語では非常に重要であるため、必ず理解しておく必要があります。差集合は、setdiff()はただの差集合を返し、setdiff!()はセット自体を更新するという違いがあります。\n環境 OS: Windows julia: v1.5.0 ","id":1442,"permalink":"https://freshrimpsushi.github.io/jp/posts/1442/","tags":null,"title":"ジュリアでの集合データ型と演算子"},{"categories":"줄리아","contents":"概要 ジュリアは、R、パイソン、マトラボの利点が混在する言語だ。配列はプログラミングの基本であり、その利用で複数の言語の痕跡が見られる。\nコード 行列 julia\u0026gt; M = [1. 2. ; 3. 4.]\r2×2 Array{Float64,2}:\r1.0 2.0\r3.0 4.0\rjulia\u0026gt; size(M)\r(2, 2)\rjulia\u0026gt; length(M)\r4 行列の場合、ほぼマトラボの文法で定義され、使われる。size()関数はマトラボと同じように使用され、パイソンのnumpyパッケージのプロパティ.shapeに相当する機能をする。length()はマトラボと異なり、全要素の数を返す。\n二次配列 julia\u0026gt; x = [[1,2,3,4] for _ in 1:4]; x\r4-element Array{Array{Int64,1},1}:\r[1, 2, 3, 4]\r[1, 2, 3, 4]\r[1, 2, 3, 4]\r[1, 2, 3, 4] 配列内にループを配置して使用することは、パイソンでよく見られる方法だ。これにより、Rのrep()関数などを似たように再現できる。\nスライシング julia\u0026gt; y = [3,2,5,1,4]\r5-element Array{Int64,1}:\r3\r2\r5\r1\r4\rjulia\u0026gt; y[[4,2,1,5,3]]\r5-element Array{Int64,1}:\r1\r2\r3\r4\r5\rjulia\u0026gt; y[3:end]\r3-element Array{Int64,1}:\r5\r1\r4\rjulia\u0026gt; y[3:4] .= -1; y\r5-element Array{Int64,1}:\r3\r2\r-1\r-1\r4 インデキシングは、Rと似ており、インデックスの配列を与えると、その順序で出力する。配列の最後のインデックスをendで表現することから、スライシングはマトラボからの影響があると分かる。最後に、.=を使って、3、4番目の要素に-1を直接代入することも、マトラボに似ている。\nインデキシング julia\u0026gt; x = [1 2; 3 4]\r2×2 Array{Int64,2}:\r1 2\r3 4\rjulia\u0026gt; x[1,:]\r2-element Array{Int64,1}:\r1\r2\rjulia\u0026gt; x[[1],:]\r1×2 Array{Int64,2}:\r1 2\rjulia\u0026gt; x[1,1] = -1; x\r2×2 Array{Int64,2}:\r-1 2\r3 4 特殊なのは、インデキシングの方法によって、結果が異なる可能性があることだ。概念的には、同じものを入れても、結果が同じになると思うが、入れる時に要素が入れば、結果も要素として得られ、配列が入れば、結果も配列として得ることができる。これはジュリアを使いにくくするが、同時に高度な機能を実装するのに大いに役立つ。\n環境 OS: Windows julia: v1.5.0 ","id":1437,"permalink":"https://freshrimpsushi.github.io/jp/posts/1437/","tags":null,"title":"ジュリアにおける配列のスライシングとインデックス化"},{"categories":"수리통계학","contents":"定義: 期待値、平均、分散 確率変数 $X$ が与えられたとする。\n連続確率変数$X$ の確率密度関数$f(x)$ が $\\displaystyle \\int_{-\\infty}^{\\infty} |x| f(x) dx \u0026lt; \\infty$ を満たす場合、以下のように定義された $E(X)$ を$X$ の期待値Expectationと言う。 $$ E(X) := \\int_{-\\infty}^{\\infty} x f(x) dx $$\n離散確率変数$X$ の確率質量関数$p(x)$ が $\\displaystyle \\sum_{x} |x| p(x) \u0026lt; \\infty$ を満たす場合、以下のように定義された $E(X)$ を$X$ の期待値Expectationと言う。 $$ E(X) := \\sum_{x} x p(x) $$\n$\\mu = E(X)$ が存在する場合、$X$ の 平均Meanと定義する。\n$\\sigma^2 = E((X - \\mu)^2)$ が存在する場合、$X$ の 分散Varianceと定義する。\n抽象化の意味 統計学を簡単な言葉で一言で言い表すなら、「だから平均的にどうなの？」を研究する学問だと言えるかもしれない。「平均」は代表値としてかなり直感的で、計算も簡単な統計量である。しかし、この世にある様々な現象を説明するには、そんな単純なレベルでは足りず、「期待値」という形で抽象化される。期待値は離散分布だけでなく区分求積法の発想を借りて連続分布にも対応する概念となる。この文章で議論するのはまさにその「抽象化」についての話である。\n数理統計学は確かに統計理論に関する内容を含んでいるが、学問の性質を見たとき、本質的には数学の一分野に近い。従って、他の数学の分野のように数学者と同じマインドセットで数理統計学を理解しようとする努力が必要である。数学者の仕事の一つは、直感や既に世に出ている理論と矛盾せず、厳格な定義を考案して、万物、対象物、現象、またそれらを超えたすべてを記号に変え、そのすべてをその場で研究できるようにすることである。\n平均と分散の定義を見ると、直感的な意味は全く残っておらず、単に確率変数に期待値を取った形で表されている。定義からこれらがどのように平均や実際につながるか、どのように分散と実際につながるかには関心がない。むしろ、平均と分散という概念を当たり前のように受け入れているほど習熟した学習者が、記号に逆らって適応することが期待される。実際、数理統計学を学ぶレベルの学習者であれば、それらの定義に大きな疑問を持つことはないだろう。\n平均の抽象化と期待値という概念が定着すると、学者たちはより多くの可能性を発見する。単純な合計や積分といった限界を超え、基本的な代数学や解析学の理論を導入してこれらを扱い始めたのである。自然と、学者たちの関心は抽象化から一般化へと移行する。\n定義: モーメント 自然数 $m$ に対して、$E( X^m )$ を$X$ の**$m$番目のモーメント**$m$th Momentと定義する。\n一般化の意味 モーメントの定義を読むだけで即座に分かる事実は、第一モーメント、つまり$E(X^1)$ が$X$ の平均になるということである。もう少し考えると、第二モーメントである$E(X^2)$ も少しの操作を通じて分散になることもわかる。\n平均は第一モーメント、分散は第二モーメントと実質的に同じ概念と見ることができる。逆に考えれば、モーメントの中で第一が平均に対応し、第二が分散に対応すると言えるだろう。\nそれでは、直感を持つ学者はもちろん、第三や第四のモーメントも何か重要な情報を与えることができるだろうと推測することになる。[ NOTE: 具体的には、これを歪度、尖度と言う。] 今では研究の方向性が逆転し、明らかになった事実を理論で説明するのではなく、理論で隠された事実を見つけ出そうとするものになっている。\nこのような方法論は、統計学だけでなく、自然科学全般で容易に見ることができる。数理統計学は統計よりも数学に近いといえるが、学問の存在意義に近づくと再び統計学の姿を取り戻す。モーメントMomentという言葉自体は物理学などでも使用されるが、統計学ではモーメントがどのような意味を持つか考える必要はない。ただ、数理統計学を支える理論を説明する際に使用する言葉として知っておけば十分である。\n一方で、期待値の存在条件である$\\displaystyle \\int_{-\\infty}^{\\infty} |x| f(x) dx \u0026lt; \\infty$、つまり$x$ でわざわざ絶対値を取る必要があるのか疑問に思うかもしれない。存在を保証することと具体的な値を計算することは別としても、$X$ の期待値$E(X)$ は別の式を使う必要がないように見えるからである。\nこの点については、次の定理を見れば少しは役立つかもしれない。最も単純な$X$ の議論ではなく、$g(X)$ への一般化まで考慮すると、むしろ$E(X)$ こそ恒等関数 $g(x) = x$ に対する特別なケースとして定義されたと見ることもできるだろう。\n定理 確率変数$X$ について、$Y$ が何らかの関数$g$ に対して$Y := g(X)$ のように表れるとする。\n[1]: $X$ が確率密度関数$f_{X}$ を持つ連続確率変数で、$\\displaystyle \\int_{-\\infty}^{\\infty} |g(x)| f_{X} (x) dx \u0026lt; \\infty$ を満たす場合、 $$ E (Y) = \\int_{-\\infty}^{\\infty} g(x) f_{X} (x) dx $$ [2]: $X$ が確率質量関数$p_{X}$ を持つ離散確率変数で、$\\displaystyle \\sum_{x} |g(x)| p_{X} (x) \u0026lt; \\infty$ を満たす場合、 $$ E (Y) = \\sum_{x} g(x) p_{X} (x) $$ 参照 代表値としての平均 測度論で定義される期待値 ","id":246,"permalink":"https://freshrimpsushi.github.io/jp/posts/246/","tags":null,"title":"数理統計学における期待値、平均、分散、モーメントの定義"},{"categories":"수리통계학","contents":"定義 1 標本空間 $\\Omega$ で 確率 $P$ が定義されているとする。\n定義域が標本空間の関数 $X : \\Omega \\to \\mathbb{R}$ を 確率変数Random Variableと呼ぶ。確率変数の値域 $X(\\Omega)$ を 空間Spaceとも呼ぶ。 以下を満たす関数 $F_{X} : \\mathbb{R} \\to [0,1]$ を $X$ の累積分布関数(Cumulative Distribution Function, cdf) とする。 $$ F_{X}(x) = P_{X}\\left( (-\\infty,x] \\right) = P \\left( \\left\\{ \\omega \\in \\Omega : X(\\omega) \\le x \\right\\} \\right) $$ 離散 D1: 確率変数 $X$ の空間が可算集合ならば $X$ を 離散確率変数Discrete Random Variableと呼び、離散確率分布に従うとする。 D2: 以下を満たす $p_{X} : \\mathbb{R} \\to [0,1]$ を離散確率変数 $X$ の確率質量関数(Probability Mass Function, pmf) と呼ぶ。 $$ p_{X}(x) := P\\left( X=x \\right) $$ D3: $\\mathcal{S}_{X} := \\left\\{ x \\in \\mathbb{R} : p_{X}(x) \u0026gt; 0 \\right\\}$ を $X$ のサポートSupportと呼ぶ。 連続 C1: 確率変数 $X$ の累積分布関数 $F_{X}$ が全ての $x \\in \\mathbb{R}$ で連続ならば $X$ を 連続確率変数Continuous Random Variableと呼び、連続確率分布に従うとする。 C2: 以下を満たす関数 $f_{X} : \\mathbb{R} \\to [0,\\infty)$ を連続確率変数 $X$ の確率密度関数(Probability Density Function, pdf) と呼び、$X$ が 絶対連続Absolutely Continuousであるとする。 $$ F_{X}(x) = \\int_{-\\infty}^{x} f_{X}(t) dt $$ C3. $\\mathcal{S}_{X} := \\left\\{ t \\in \\mathbb{R} : f_{X}(t) \u0026gt; 0 \\right\\}$ を $X$ のサポートSupportと呼ぶ。 解説 サポート 、または 支持集合 は、簡単に言えば、私たちが興味を持つ部分だけを選び出した集合である。よく使われる表現ではないが、確率論が何を表現したいのかを確かに伝える。確率は確定的な何かに関心がなく、確率が $0$ とは決して起こらないということなので、無関心で良い。だから $\\mathcal{S}$ は「本当に重要な集合」や「私たちが知るべき集合」と見なせるようになり、限られたエネルギーを $\\Omega$ 全体ではなく $\\mathcal{S}$ にだけ注ぐことができるようになる。\n高校で確率に触れたときも、教師が「確率変数は関数だ」と強調した記憶があるだろう。しかし、それとは別に、本当に確率変数を関数として考えて扱うことは、もう少し高いレベルの抽象化能力を必要とする。ここで紹介されている定義はまだ数学的に厳密ではないが、集合と関数で確率の概念を描写することは簡単ではない。わからないからといって絶望することもなければ、わかったと思って軽く見ることもない。\n定義を読めば、離散確率変数と連続確率変数には本質的な違いがあり、それが形式的な違いにもつながることがわかる。学部生レベルでは混乱することもあるかもしれないが、連続確率変数を扱うときのみヤコビアンが付くことをしっかり理解しておこう。\n要約 サポート $\\mathcal{S}_{X}$ を持つ連続確率変数 $X$ と微分可能な単射関数 $g$ に対して、確率変数 $Y$ を $Y:=g(X)$ のように定義すると、$Y$ の確率密度関数は $y \\in \\mathcal{S}_{Y}$ に関して次のように求められる。[ 注: 実際には$g$ は全単射とは仮定されていないため、逆関数 $g^{-1}$ の存在が常に保証されるわけではない。] $$ f_{Y} (y) = f_{X} \\left( g^{-1}(y) \\right) \\left| {{ d x } \\over { d y }} \\right| $$\nここで $\\mathcal{S}_{Y}$ は $Y$ のサポート、$x$ は $x = g^{-1}(y)$ を意味する。 証明 $g$ は単射で連続なので、増加関数か減少関数である。ケースを分けて考えよう。\nケース 1. $g$ が増加関数の場合 $$ \\begin{align*} F_{Y}(y) =\u0026amp; P \\left( Y \\le y \\right) \\\\ =\u0026amp; P \\left( g(X) \\le y \\right) \\\\ =\u0026amp; P \\left( X \\le g^{-1}(y) \\right) \\\\ =\u0026amp; F_{X}\\left( g^{-1}(y) \\right) \\end{align*} $$ 微積分の基本定理により、$Y$ の確率密度関数は $$ \\begin{align*} f_{Y}(y) =\u0026amp; {{ d } \\over { d y }} F_{Y}(y) \\\\ =\u0026amp; {{ d } \\over { d y }} \\int_{-\\infty}^{x} f_{X}(t) dt \\\\ =\u0026amp; {{ d } \\over { d x }} \\int_{-\\infty}^{x} f_{X}(t) dt {{ d x } \\over { d y }} \\\\ =\u0026amp; f_{X} \\left( x \\right) {{ d x } \\over { d y }} \\\\ =\u0026amp; f_{X} \\left( g^{-1} (y) \\right) {{ d x } \\over { d y }} \\end{align*} $$ $g$ が増加関数なので $\\displaystyle {{ d x } \\over { d y }} = {{ d g^{-1}(y) } \\over { d y }} \u0026gt;0$ であり、したがって $$ {{ d x } \\over { d y }} = \\left| {{ d x } \\over { d y }} \\right| $$\nケース 2. $g$ が減少関数の場合 $$ \\begin{align*} F_{Y}(y) =\u0026amp; P \\left( Y \\le y \\right) \\\\ =\u0026amp; P \\left( g(X) \\le y \\right) \\\\ =\u0026amp; P \\left( X \\le g^{-1}(y) \\right) \\\\ =\u0026amp; 1- F_{X}\\left( g^{-1}(y) \\right) \\end{align*} $$ 同様に $\\displaystyle f_{Y}(y) = - f_{X} \\left( g^{-1} (y) \\right) {{ d x } \\over { d y }}$ である。$g$ が減少関数なので $\\displaystyle {{ d x } \\over { d y }} \u0026lt; 0$ であり、したがって $$ - {{ d x } \\over { d y }} = \\left| {{ d x } \\over { d y }} \\right| $$\n■\n厳密な定義 測度論で定義される確率変数と確率分布 測度論で定義される累積分布関数 測度論で定義される離散確率分布 測度論における絶対連続 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p32~41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1433,"permalink":"https://freshrimpsushi.github.io/jp/posts/1433/","tags":null,"title":"数理統計学における確率変数と確率分布"},{"categories":"줄리아","contents":"코드 julia\u0026gt; x1=[1 2 3]\r1×3 Array{Int64,2}:\r1 2 3\rjulia\u0026gt; x2=[1, 2, 3]\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; x3=[i for i in 1:3]\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; x4=[i for i in 1:3:10]\r4-element Array{Int64,1}:\r1\r4\r7\r10\rjulia\u0026gt; x5=[i for i in 1:3:11]\r4-element Array{Int64,1}:\r1\r4\r7\r10 x1は2次元配列です。行ベクトルと同じように見えるため、成分座標を1つだけ入力すると、行ベクトルのように認識されます。x2, x3, x4, x5は1次元配列です。\nx=[i for i in n:m]と入力すると、$n$から$m$までの間隔が$1$の配列を返します。 x=[i for i in n:k:m]と入力すると、$n$から$m$までの間隔が$k$の配列を返します。 最後の要素は$m$以下で最も大きい数です。このようにリスト内にfor文を含めてリストを作ることを、Pythonなどでリスト内包List Comprehensionと言います。\njulia\u0026gt; x6=1:3\r1:3\rjulia\u0026gt; x7=1:3:10\r1:3:10\rjulia\u0026gt; x9=1:3:11\r1:3:10 上記のように書くと実際にはそうではないが、事実上x3, x4, x5のような配列が作成されたと考えても良いです。下の写真に示されるように、データタイプは異なるが上で作成されたものと同様に使用できます。\nrange(n,stop=m,length=k): これはマットラボでのlispace(n,m,k)と全く同じです。具体的な違いは以下の例のコードと結果を通じて確認しましょう。 julia\u0026gt; x9=range(1,stop=10)\r1:10\rjulia\u0026gt; x10=range(1,length=15)\r1:15\rjulia\u0026gt; x11=range(1,stop=10,length=15)\r1.0:0.6428571428571429:10.0\rjulia\u0026gt; x12=range(1,length=15,stop=10)\r1.0:0.6428571428571429:10.0 このコードも同様に、実際のデータタイプは異なるが事実上同じベクトルを生成すると考えても良いです。マットラボとは異なり、2番目、3番目の変数のうちの1つだけを入力することもでき、入力する順序を変えても構いません。\n最初の行 は、最初の要素が$1$、最後の要素が$10$のベクトルを返します。他に入力されたものがないので、要素間の間隔は$1$です。 二番目の行 は、最初の要素が$1$で、合計$15$個の要素を持つベクトルを返します。間隔は自動的に$1$となり、x=range(1,stop=15)あるいはx=1:15で作成されたベクトルと同じです。 三番目の行 は、最初の要素が$1$、最後の要素が$10$で、合計$15$個の要素を持つベクトルを返します。したがって、間隔は自動的に$9/14=0.6428571428571429$となります。これは整数ではないので、自然と実数要素を持つベクトルを返します。また、x=1.0:0.6428571428571429:10.0で作成されたものと同じです。 四番目の行 は、三番目の行で返されたベクトルと正確に同じベクトルを返します。 타언어 マットラボで等間隔の行ベクトルを生成する方法 환경 OS: Windows10 Version: 1.5.0 ","id":1452,"permalink":"https://freshrimpsushi.github.io/jp/posts/1452/","tags":null,"title":"ジュリアでベクターを生成する様々な方法"},{"categories":"줄리아","contents":"説明 circshifr(A, (n,m))を使用すると、配列Aの行を$n$カン下にシフトさせ、列を$m$カン右にシフトさせることができる。(n,m)は整数から成るタプルでなければならず、もちろん負の数も可能だ。負の数の場合は逆方向に平行移動される。\n3次元以上の配列の場合は、一番小さい2次元配列にそれぞれ適用される。\nコード 2次元配列 julia\u0026gt; A = transpose(reshape(1:25,5,5))\r5×5 LinearAlgebra.Transpose{Int64,Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}}}:\r1 2 3 4 5\r6 7 8 9 10\r11 12 13 14 15\r16 17 18 19 20\r21 22 23 24 25\rjulia\u0026gt; circshift(A, (-1,0))\r5×5 Array{Int64,2}:\r6 7 8 9 10\r11 12 13 14 15\r16 17 18 19 20\r21 22 23 24 25\r1 2 3 4 5\rjulia\u0026gt; circshift(A, (0,3))\r5×5 Array{Int64,2}:\r3 4 5 1 2\r8 9 10 6 7\r13 14 15 11 12\r18 19 20 16 17\r23 24 25 21 22\rjulia\u0026gt; circshift(A, (-1,3))\r5×5 Array{Int64,2}:\r8 9 10 6 7\r13 14 15 11 12\r18 19 20 16 17\r23 24 25 21 22\r3 4 5 1 2 高次元配列 julia\u0026gt; B = reshape(1:4*4*3,4,4,3)\r4×4×3 reshape(::UnitRange{Int64}, 4, 4, 3) with eltype Int64:\r[:, :, 1] =\r1 5 9 13\r2 6 10 14\r3 7 11 15\r4 8 12 16\r[:, :, 2] =\r17 21 25 29\r18 22 26 30\r19 23 27 31\r20 24 28 32\r[:, :, 3] =\r33 37 41 45\r34 38 42 46\r35 39 43 47\r36 40 44 48\rjulia\u0026gt; circshift(B,(-1,0))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r2 6 10 14\r3 7 11 15\r4 8 12 16\r1 5 9 13\r[:, :, 2] =\r18 22 26 30\r19 23 27 31\r20 24 28 32\r17 21 25 29\r[:, :, 3] =\r34 38 42 46\r35 39 43 47\r36 40 44 48\r33 37 41 45\rjulia\u0026gt; circshift(B,(0,2))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r9 13 1 5\r10 14 2 6\r11 15 3 7\r12 16 4 8\r[:, :, 2] =\r25 29 17 21\r26 30 18 22\r27 31 19 23\r28 32 20 24\r[:, :, 3] =\r41 45 33 37\r42 46 34 38\r43 47 35 39\r44 48 36 40\rjulia\u0026gt; circshift(B,(-1,2))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r10 14 2 6\r11 15 3 7\r12 16 4 8\r9 13 1 5\r[:, :, 2] =\r26 30 18 22\r27 31 19 23\r28 32 20 24\r25 29 17 21\r[:, :, 3] =\r42 46 34 38\r43 47 35 39\r44 48 36 40\r41 45 33 37 julia\u0026gt; C = reshape(1:3*3*2*4,3,3,2,4)\r3×3×2×4 reshape(::UnitRange{Int64}, 3, 3, 2, 4) with eltype Int64:\r[:, :, 1, 1] =\r1 4 7\r2 5 8\r3 6 9\r[:, :, 2, 1] =\r10 13 16\r11 14 17\r12 15 18\r[:, :, 1, 2] =\r19 22 25\r20 23 26\r21 24 27\r[:, :, 2, 2] =\r28 31 34\r29 32 35\r30 33 36\r[:, :, 1, 3] =\r37 40 43\r38 41 44\r39 42 45\r[:, :, 2, 3] =\r46 49 52\r47 50 53\r48 51 54\r[:, :, 1, 4] =\r55 58 61\r56 59 62\r57 60 63\r[:, :, 2, 4] =\r64 67 70\r65 68 71\r66 69 72\rjulia\u0026gt; circshift(C,(1,1))\r3×3×2×4 Array{Int64,4}:\r[:, :, 1, 1] =\r9 3 6\r7 1 4\r8 2 5\r[:, :, 2, 1] =\r18 12 15\r16 10 13\r17 11 14\r[:, :, 1, 2] =\r27 21 24\r25 19 22\r26 20 23\r[:, :, 2, 2] =\r36 30 33\r34 28 31\r35 29 32\r[:, :, 1, 3] =\r45 39 42\r43 37 40\r44 38 41\r[:, :, 2, 3] =\r54 48 51\r52 46 49\r53 47 50\r[:, :, 1, 4] =\r63 57 60\r61 55 58\r62 56 59\r[:, :, 2, 4] =\r72 66 69\r70 64 67\r71 65 68 環境 OS: Windows10 バージョン: 1.5.3 (2020-11-09) ","id":1453,"permalink":"https://freshrimpsushi.github.io/jp/posts/1453/","tags":null,"title":"ジュリアで配列を平行移動する方法"},{"categories":"줄리아","contents":"方法1 using LinearAlgebra\rusing Pkg\rPkg.add(\u0026#34;Plots\u0026#34;)\rPkg.add(\u0026#34;Distributions\u0026#34;)\rusing Plots 上のコードは、LinearAlgebraパッケージとPkgパッケージを読み込むこと、そして.add()関数を使ってPlots、Distributionパッケージをインストールするコードを示している。パッケージを読み込むキーワードusingは、数学である定理や論法を使う時に使う言葉に似ている。パッケージをインストールすること自体はPythonよりはRにもっと近いけど、使用法はPythonにもっと似ている。Rと同じようにパッケージ名をダブルクオートで囲む必要があり、一般的にパッケージ名はパスカルケース1で書き、-sをよく付けて複数形にすることが多く2混乱することがある。\n方法2 REPLで]を入力すると、上記のようにパッケージマネージャ環境に切り替わる。バックスペースを押すと、再びREPL環境に戻る。パッケージマネージャ環境でadd package_nameを入力すると、指定したパッケージがインストールされる。\n(@v1.5) pkg\u0026gt; add Plots\rResolving package versions...\rUpdating `C:\\Users\\rydbr\\.julia\\environments\\v1.5\\Project.toml`\r[91a5bcdd] + Plots v1.0.14\rNo Changes to `C:\\Users\\rydbr\\.julia\\environments\\v1.5\\Manifest.toml 単語の最初の文字を大文字で書く表示方法を言う。例のコードで確認できるように、linear algebraはLinearAlgebraのように各単語の最初の文字を大文字にして、スペースを省略する。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n例のコードで見るように、PlotとDistributionはPlotsとDistributionsとして呼ばなければならない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1416,"permalink":"https://freshrimpsushi.github.io/jp/posts/1416/","tags":null,"title":"Juliaでパッケージをインストールして使用する方法"},{"categories":"위상수학","contents":"定義 1 位相空間 $(X,\\mathscr{T})$と部分集合 $A \\subset X$が与えられたとしよう。すると以下の集合 $$ \\mathscr{T}_{A} =\\left\\{ A\\cap U\\ :\\ U\\in \\mathscr{T} \\right\\} $$ は $A$上の位相である。このとき $\\mathscr{T}_{A}$を部分空間位相Subspace Topologyまたは相対位相と呼ぶ。また、位相空間 $(A, \\mathscr{T}_{A})$を $(X,\\mathscr{T})$の部分空間Subspaceと呼ぶ。\n定理 [0]: 位相空間 $(X, \\mathscr{T}$) と部分集合 $A \\subset X$に対して $$ \\mathscr{T}_{A} = \\left\\{ A\\cap U\\ :\\ U \\in \\mathscr{T}\\right\\} $$ は $A$上の位相となる。 位相空間 $X$と部分空間 $A$が与えられたとしよう。部分空間 $A$で開集合、閉集合の同値条件は以下のようである。\n[a1]: $V\\subset A$が $A$で開集合であるための必要十分条件は、$V= A\\cap U$を満たす $X$での開集合 $U$が存在することである。 [b1]: $F\\subset A$が $A$で閉集合であるための必要十分条件は、$F=A\\cap E$を満たす $X$での閉集合 $E$が存在することである。 部分空間で開集合（閉集合）であっても、全体空間で開集合（閉集合）である保証はない。部分空間が全体空間に対して開集合（閉集合）であれば、その性質が全体空間でも保証される。位相空間 $(X,\\mathscr{T})$と部分空間 $(A,\\mathscr{T}_{A})$、部分集合 $B\\subset A\\subset X$が与えられたとしよう。\n[a2]: $B$が部分空間 $A$で開集合であり、$A$が $X$で開集合であれば、$B$は $X$で開集合である。 [b2]: $B$が部分空間 $A$で閉集合であり、$A$が $X$で閉集合であれば、$B$は $X$で閉集合である。 [3]: $\\mathscr{B}$を位相空間 $(X,\\mathscr{T})$の基底としよう。すると $$ \\mathscr{B}_{A} =\\left\\{ A\\cap B\\ :\\ B\\in \\mathscr{B} \\right\\} $$ は部分空間 $(A,\\mathscr{T}_{A})$の基底である。 説明 混乱しないように、いくつかの表記についてはっきりと確認しておこう。$(X,\\mathscr{T})$が全体空間で $$ \\mathscr{T}_{A}=\\left\\{A\\cap U\\ :\\ U \\in \\mathscr{T} \\right\\} $$ は部分集合 $A$の位相である。つまり、部分空間 $(A,\\mathscr{T}_{A})$を形成する。$\\mathscr{B}$は全体集合 $X$の基底である。$\\mathscr{B}_{A}$は全体集合の基底の各要素と $A$の交差集合のコレクションである。これが部分集合 $A$の基底となるというのが定理の内容である。 $$ \\mathscr{T}_{\\mathscr{B}_{A}}=\\left\\{U_{A}\\subset A\\ :\\ \\forall\\ x \\in U_{A},\\ \\exists\\ (A\\cap B) \\in \\mathscr{B}_{A}\\ \\ \\text{s.t.}\\ x\\in (A\\cap B) \\subset U_{A}\\right\\} $$ さらに進んで、$\\mathscr{T}_{\\mathscr{B}_{A}}$が $\\mathscr{T}_{A}$と同じであることが核心である。内容が複雑に感じられるかもしれない。整理して説明すると次のようになる。\n全体空間 $X$の基底の要素と $A$を交差させたものを集めると、$A$の基底になる。 その基底 $\\mathscr{B}_{A}$で生成した位相は $\\mathscr{T}_{\\mathscr{B}_{A}}$である。 2で生成した位相 $\\mathscr{T}_{\\mathscr{B}_{A}}$は $X$の開集合と $A$を交差させたものの集合である $\\mathscr{T}_{A}$と同じである。 証明 [0] $(T1)$: $A \\cap \\varnothing =\\varnothing$、$A \\cap X=A$であるから、空集合と全体集合が $\\mathscr{T}_{A}$に属する。 $(T2)$: $V_\\alpha \\in \\mathscr{T}_{A}( \\alpha \\in \\Lambda)$としよう。$\\mathscr{T}_{A}$の定義により、各々の $V_\\alpha$に対して、$V_\\alpha = A \\cap U_\\alpha$を満たす $U_\\alpha$が存在する。位相の定義により $U=\\cup_{\\alpha \\in \\Lambda} U_\\alpha \\in \\mathscr{T}$である。そうすると $$ \\bigcup_{\\alpha \\in \\Lambda} V_\\alpha = \\bigcup_{\\alpha \\in \\Lambda} (A \\cap U_\\alpha ) =A\\cap (\\cup_{\\alpha \\in \\Lambda} U_\\alpha ) =A\\cap U \\in \\mathscr{T}_{A} $$ であるから $\\bigcup _{\\alpha \\in \\Lambda} V_\\alpha \\in \\mathscr{T}_{A}$である。 $(T3)$: $V_{1},\\ \\cdots\\ ,V_{n} \\in \\mathscr{T}_{A}$としよう。同様に、各々の $V_{i}$に対して、$V_{i} =A \\cap U_{i}$を満たす $U_{i}$が存在する。そして $U=\\cap _{i} U_{i} \\in \\mathscr{T}$であるから $$ \\bigcap _{i=1}^n V_{i} = \\bigcap_{i=1}^n (A\\cap U_{i}) = A\\cap \\left( \\bigcap_{i=1}^n U_{i} \\right) =A\\cap U \\in \\mathscr{T}_{A} $$ である。従って $\\bigcap_{i=1}^n V_{i} \\in \\mathscr{T}_{A}$である。 位相の条件 三つを満たすので $\\mathscr{T}_{A}$は $A$の上の位相である。\n■\n[a1] 距離空間での証明を参照せよ。$\\mathscr{T}_{A}$の定義により自明である。\n■\n[b1] $(\\implies)$ $F$が $A$で閉集合であるため、$A-F$は $A$で開集合である。そこで[a1]により、$A-F=A\\cap U$を満たす $X$での開集合 $U$が存在する。$U$が開集合であるため、$E=X-U$は $X$で閉集合である。それで $$ A\\cap E=A\\cap (X-U)=A-(A\\cap U)=A-(A-F)=F $$\n$(\\Longleftarrow )$ $E$は $X$で閉集合であるため、$X-E$は $X$で開集合である。そこで[a1]により、$A \\cap (X-E)$は $A$で開集合である。$F^c=A-(A\\cap E)=A\\cap(X-E)$であるから、$F ^c$は $A$で開集合である。従って、$F$は $A$で閉集合である。\n■\n[a2] $B$が $A$で開集合である場合、[a1]により、$B=A\\cap U\\ (U\\in \\mathscr{T})$を満たす $X$での開集合 $U$が存在する。仮定により、$A$は $X$で開集合である。従って、$B$は $X$で開集合の交差であり、$X$で開集合である。\n■\n[b2] $B$が $A$で閉集合である場合、[b1]により、$B=A\\cap E$を満たす $X$での閉集合 $E$が存在する。仮定により、$A$は $X$で閉集合であり、$B$は閉集合同士の交差であるため、$B$も $X$で閉集合である。\n■\n[3] パート1. $\\mathscr{B}_{A}$は $A$の基底である。\n[b1]: 任意の $x\\in A$に対して、$A\\subset X$であるから、$x\\in X$である。$\\mathscr{B}$が $X$の基底であるため、定義により、$x \\in B \\in \\mathscr{B}$を満たす $B$が存在する。従って、$x\\in (A\\cap B ) \\in \\mathscr{B}_{A}$を満たす $A\\cap B \\in \\mathscr{B}_{A}$が存在する。[b2]: 任意の $A\\cap B_{1}$、$A\\cap B_{2}$と $x\\in \\Big( (A\\cap B_{1} ) \\cap (A \\cap B_{2}) \\Big)$に対して $$ (A\\cap B_{1})\\cap (A \\cap B_{2})=A\\cap B_{1}\\cap B_{2} $$ であるため、$x\\in (B_{1}\\cap B_{2})$である。$\\mathscr{B}$が $X$の基底であるため、定義により、$x\\in B_{3} \\subset ( B_{1}\\cap B_{2})$を満たす $B_{3}$が存在する。従って $$ x \\in (A\\cap B_{3})\\subset \\Big( A\\cap (B_{1}\\cap B_{2}) \\Big)=(A\\cap B_{1}) \\cap (A\\cap B_{2}) $$ である。基底となる二つの条件を満たしたので、$\\mathscr{B}_{A}$は部分集合 $A$の基底である。\nパート2. $\\mathscr{T}_{\\mathscr{B}_{A}}=\\mathscr{T}_{A}$である。\n$(\\subset)$ $\\mathscr{B}$が $(X,\\mathscr{T})$の基底であるため、$\\mathscr{T}_{\\mathscr{B}}=\\mathscr{T}$であり、$\\mathscr{B}\\subset \\mathscr{T_{\\mathscr{B}}}=\\mathscr{T}$である。従って、全ての $B \\in \\mathscr{B}$に対して、$B\\in \\mathscr{T}$である。$\\mathscr{T}_{A}$の定義によって、$A\\cap B \\in \\mathscr{T}_{A}$である。従って $$ \\mathscr{B}_{A} \\subset \\mathscr{T}_{A} $$ $\\mathscr{T}_{\\mathscr{B}_{A}}$は $\\mathscr{B}_{A}$を含む最も小さい位相であるため $$ \\mathscr{T}_{\\mathscr{B}_{A}} \\subset \\mathscr{T}_{A} $$\n$(\\supset )$ $V \\in \\mathscr{T}_{A}$と仮定する。[a1]によって、$V=A\\cap U$を満たす $U\\in \\mathscr{T}$が存在する。また、$V$の任意の点 $x\\in V \\subset A$に対して、$x \\in U$である。$\\mathscr{B}$は$\\mathscr{T}$を生成する基底であるため、$x\\in B \\subset U$を満たす $B\\in \\mathscr{B}$が存在する。従って、$A \\cap B \\in \\mathscr{B}_{A}$が $$ x\\in (A\\cap B) \\subset (A\\cap U) =V $$ を満たす。これは、$V$がその $\\mathscr{B}_{A}$が生成する $A$上の位相 $\\mathscr{T}_{\\mathscr{B}_{A}}$に属する条件であるため、$V \\in \\mathscr{T}_{\\mathscr{B}_{A}}$である。従って $$ \\mathscr{T}_{\\mathscr{B}_{A}} \\supset \\mathscr{T}_{A} $$ ■\nMunkres. (2000). 『Topology(2nd Edition)』: p89.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1439,"permalink":"https://freshrimpsushi.github.io/jp/posts/1439/","tags":null,"title":"部分空間トポロジー、相対トポロジー"},{"categories":"줄리아","contents":"julia\u0026gt; function add1(a,b)\rreturn a+b\rend\radd1 (generic function with 1 method)\rjulia\u0026gt; add1(1,2)\r3\rjulia\u0026gt; add(1,2.0)\rERROR: UndefVarError: add not defined\rStacktrace:\r[1] top-level scope at REPL[43]:1\rjulia\u0026gt; function add2(a::Int64, b::Float64)\rreturn a+b\rend\radd2 (generic function with 1 method)\rjulia\u0026gt; add2(1,2)\rERROR: MethodError: no method matching add2(::Int64, ::Int64)\rClosest candidates are:\radd2(::Int64, ::Float64) at REPL[44]:1\rStacktrace:\r[1] top-level scope at REPL[45]:1\rjulia\u0026gt; add2(1,2.0)\r3.0 上のように :: を使って変数が具体的に何であるかを知らせると、タイプが合わない時にエラーが出る。こうしてアノテーションがされていれば、タイプをチェックする必要がなくなるから、当然パフォーマンスが向上する。\nEnvironment OS: Windows julia: v1.5.0 ","id":1379,"permalink":"https://freshrimpsushi.github.io/jp/posts/1379/","tags":null,"title":"ジュリアのタイプとアノテーション"},{"categories":"집합론","contents":"定義 1 $x \\in X$ と $y \\in Y$ と $f: X \\to Y$が関数だとしよう。\nあらゆる $x_{1}, x_{2} \\in X$ に対して $x_{1} \\ne x_{2} \\implies f(x_{1}) \\ne f(x_{2})$ ならば $f$ を単射injectiveという。 $f(X) = Y$ ならば $f$ を全射surjectiveという。 $f$ が単射であり、かつ全射ならば全単射bijectiveという。 $I(x) = x$ を満たす $I : X \\to X$ を恒等関数Identity Functionという。 あらゆる $x, y$ に対して $f(x) = y$ かつ $f^{-1} (y) = x$ を満たす時、$f^{-1} : Y \\to X$ を$f$ の逆関数Inverse Functionという。 基礎的性質 [2]: $f$ が全単射であることと逆関数 $f^{-1}$ が存在することは同値だ。 説明 単射を一対一one-to-one、あるいは一対一関数ともいう。 全射をontoともいう。 全単射を一対一対応1-1 correspondingともいう。 入試数学では本当に重要でないと思われがちだが、一対一対応はとても重要な概念である。数学で苦労してる多くの学生は、この事実を聞いたことがないか、聞いても問題解決に役立たないと思うことが多い。まったく間違っているわけではないが、このレベルを知らなければ、問題解決に必要な他のこともよく知らない可能性が高い。\nそれなりにできる学生も、大学レベルの数学に触れた時に初めて全単射を本当に理解することが多い。一対一対応は集合論に限らず、広大な数学の世界で、それがどんな科目であれ、最も重要な概念である。しかし、そのほど強力で良好な条件なので、逆説的に、数学者は全単射の条件を緩和する方向で研究することになる。どのようにして他の条件で全単射であるかを導出するか、実際には全単射ではないが全単射として使うことができるかなどだ。\nどれほど重要かを簡単に説明すると、「本当に重要だから正確に知っておくべきだ」と言う必要もないほどだ。好むと好まざるとにかかわらず、全単射はあらゆる科目で出てくるので、むしろ卒業するまでに全単射を正確に知らない方が難しいだろう。\n李興天 訳, 林游峰. (2011). 集合論(Set Theory: An Intuitive Approach): p165, 181~187.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":471,"permalink":"https://freshrimpsushi.github.io/jp/posts/471/","tags":null,"title":"単射, 全射, 全単射, 逆関数"},{"categories":"줄리아","contents":"概要 ジュリアはMITで開発され、2012年に公開されたプログラミング言語で、高い生産性と速度を目指している。Cやフォートランと同等の速度を実現しながらも、PythonやRのような高レベルの構文を備えており、その他の多くの言語の利点を取り入れている。2019年11月現在、GPUの急速な発展とディープラーニングの流行により少し遅れているのは事実だが、ジュリアを超えるほど便利で速い言語は学界にはない。\nジュリアについて質問やコメントがあれば、生エビ寿司屋コミュニティ 💬に投稿してみよう。\n特長 ジュリアの特長を見てみよう：\n速い。他の言葉は必要ない。ベンチマークを見れば、Cやフォートランと肩を並べていることがわかる。 簡単だ。最適化まで気にしなければ、ジュリアのプログラミングは、マットラボ、R、パイソンと似たり寄ったりだ。ただし、これらの言語で何かプログラムを実装した後、最適化のためにCで同じコードを書き直す必要がない。ジュリアに適したコーディング技術を通じて改善されるだけだ。同じ理由で、自然科学系のユーザーがマットラボ、R、パイソンを知っていれば、ジュリアもすぐに慣れることができる。実際、これらの言語の生産性は非常に高いため、どんなプログラミング言語がベースであっても、ジュリアは簡単に感じられる。 無料だ。マットラボは強力な線形代数をサポートしているが、まず高価であり、実際には最適化によってジュリアがマットラボよりも10倍から1,000倍速いと言われている。さらに、MATLAB.jlパッケージを使用すると、マットラボと同じスタイルでコードを書くことができ、マットラボに熟練したユーザーが移行しやすい。 他言語のパッケージを簡単に取り込める。新しい言語の最大の弱点であり、フォートランやパイソンなどの言語が使用される主な理由の一つは、まさにパッケージである。パイソンの場合は、PyCall.jlパッケージを使用することで、パイソンの関数を直接呼び出すことができる。もちろん、ジュリアはすでに十分な高性能を備えているが、ccall()関数を使用することでCやフォートランの関数を呼び出すことができる。C++もCpp.jlパッケージでサポートされている。 並行処理に特化している。他の言語がその後に関連するパッケージを開発したのとは異なり、ジュリアは最初から並行処理を念頭に置いて開発された。実際、プログラムによっては、便利というよりも最適化の核心となることもある。 ","id":1374,"permalink":"https://freshrimpsushi.github.io/jp/posts/1374/","tags":null,"title":"ジュリアプログラミング言語"},{"categories":"집합론","contents":"定義 1 空集合じゃない二つの集合 $X$、$Y$ が与えられたとする。\n二項関係 $f \\subset (X,Y)$ が次を満たすとき、関数と呼び、$f : X \\to Y$ と表す。 $$ (x ,y_{1}) \\in f \\land (x,y_{2}) \\in f \\implies y_{1} = y_{2} $$ 関数 $f : X \\to Y$ において、$\\text{Dom} (f) = X$ を$f$ の定義域Domainといい、$Y$ を$f$ の共変域Codomainという。定義域の部分集合 $A \\subset X$ が与えられたとき、$f(A):= \\left\\{ f(x) \\in Y \\ | \\ x \\in A \\right\\}$ を$f$ に対する$A$ の像Imageという。特に $f$ に対する定義域 $X$ の像 $\\text{Im} f := f(X)$ を$f$ の値域Rangeという。 定義域が自然数の集合 $\\mathbb{N}$ の関数を数列Sequenceという。 定義域 $A$ と共変域 $B$ を持つすべての関数 $\\lambda : A \\to B$ の集合を $B^{A}$ と表す。 説明 教科書レベルでは、\u0026lsquo;全ての元 $x_{1}, x_{2} \\in X$ に対して、$x_{1} = x_{2} \\implies f(x_{1}) = f(x_{2})$ を満たす $f(x_{1})$ と $f(x_{2})$ が $Y$ に存在するなら、対応 $f : X \\to Y$ を$X$ から $Y$ への関数とする\u0026rsquo; と言われがちだ。しかし、この \u0026lsquo;対応\u0026rsquo; あるいは \u0026lsquo;写像\u0026rsquo; は、表現がやや曖昧だった。大学レベル以上の数学では、集合論を用いて、関数に関連する概念を厳格に定義する。入力があれば出力がただ一つだけ出る、という説明は、コンピュータ科学の関数にもっと適した説明だ。 なぜ値域を別に定義するのか疑問に思うかもしれない。例えば、$f(x) = x^2$ を考えると、明らかに関数の値は $\\left[ 0,\\infty \\right)$ に属していて、$f : \\mathbb{R} \\to \\mathbb{R}$ のように無駄に大きくする必要はないと見える。もともと値域は共変域の部分集合であり、実際には使われない値をなぜ別にするのか理解しにくいことだ。これは、あまりにも簡単な例だけを考えることからくる誤解で、すべての関数が定義時から値域を簡単に予測できるわけではない。関数を定義するときに保証できるのは、$x \\in X$ に対して $f(x) \\in Y$ が存在することだけで、それが何かはわからない。もし $$ f(x) = \\sin \\ln \\sqrt{x} + \\int_{1}^{3^x} {{1} \\over {7t+t^2}} dt $$ のような複雑な関数があれば、定義時からその値域を知ることは不可能であり、必要もない。値域を知ることが重要なのは、合成関数を定義するときくらいだ。 数列が単に数を並べたものだという説明よりも、もっとシンプルで一般的な定義ができたことを確認できる。共変域が「数」であることに限らず、関数であれ、いかにも珍しい集合であれ、全てをカバーできるようになった。このような抽象化は、ただちに数列の概念をきれいに表現できるだけでなく、無限を扱う数学のさまざまな分野で柔軟に使用できる。 関数の集合という概念自体が新しいかもしれないが、抽象数学では、関数空間のような集合を日常的に言及する。$B^{A}$ のような記法をなぜ使うのかは、基数を思い出せば理解しやすい。例えば、$B$ が共変域であり、$A$ が定義域であるすべての関数を考えると、 $$ e \\mapsto 1 \\text{ or } 2 \\text{ or } 3 \\\\ \\pi \\mapsto 1 \\text{ or } 2 \\text{ or } 3 $$ 全ての組み合わせは $9 = 3^2 = |B|^{|A|}$ になる。 李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p157~159.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":470,"permalink":"https://freshrimpsushi.github.io/jp/posts/470/","tags":null,"title":"集合論によって厳密に定義される関数と写像、数列"},{"categories":"집합론","contents":"公理 $$ \\exists U \\left( \\emptyset \\in U \\land \\forall X ( X \\in U \\implies S(X) \\in U) \\right) $$ 空集合と$X$を要素として持ち、$S(X)$も要素として持つ集合$U$が存在する。\n集合$X$に対して、$S(X)$は$S(X):= X \\cup \\left\\{ X \\right\\}$と同じように定義される集合である。 説明 なぜこれが無限公理と呼ばれるのかを長々と説明するより、自然数集合$\\mathbb{N}$の存在性を証明することが良いだろう。\n定理：自然数集合の存在性 $\\mathbb{N}$は存在する。\n証明 戦略：フォン・ノイマンが提案した構築法を用い、自然数自体を集合と対応させて自然数の集合$\\mathbb{N}$を直接構築する。これにより、$\\mathbb{N}$は存在し、同時に自然数の性質も即座に有する。\n空集合$\\emptyset$とその$S(n)$について、次のように定義しよう。 $$ 0 : = \\emptyset \\\\ ( n + 1 ):= S(n) = n \\cup \\left\\{ n \\right\\} $$ すると $$ 1 = 0+1 = S ( 0 ) = \\left\\{ 0 \\right\\} \\\\ 2 = 1+1 = S ( 1 ) = \\left\\{ 0, \\left\\{ 0 \\right\\} \\right\\} = \\left\\{ 0, 1 \\right\\} \\\\ 3 = 2+1 = S ( 2 ) = \\left\\{ 0, \\left\\{ 0 \\right\\}, \\left\\{ 0, \\left\\{ 0 \\right\\} \\right\\} \\right\\} = \\left\\{ 0, 1, 2 \\right\\} \\\\ \\vdots $$ 無限公理により、$\\mathbb{N} = \\left\\{ 1, 2, 3, \\cdots \\right\\}$は次の性質を満たしながら存在する。 $$ n_{1} \\in n_{2} \\iff n_{1} \u0026lt; n_{2} \\\\ n_{1} \\subset n_{2} \\iff n_{1} \\le n_{2} $$\n■\n自然数が無限に多いという主張はどうあれ真実だろうが、実際にこの宇宙の誰もが無限に多い自然数を見たことはない。いくら長く、一貫して、多くの自然数を探しても、無限集合が存在することを帰納的に証明することは不可能だ。無限公理はこのような無限を説明するために導入されたものであり、直感的にこれを拒否する理由は全くないだろう。\n","id":1348,"permalink":"https://freshrimpsushi.github.io/jp/posts/1348/","tags":null,"title":"無限公理"},{"categories":"확률론","contents":"定理 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられているとする。\n[1] 測度論での定理: 可測関数 $f$, $g$ が $\\mathcal{F}$-可測であれば、$g = h (f)$ を満たすボレル関数 $h : \\mathbb{R} \\to \\mathbb{R}$ が存在する。 [2] 確率論での応用: 確率変数 $X$, $Y$ が $\\sigma (X)$-可測であれば、$E(Y | X) = h(X)$ を満たすボレル関数 $h : \\mathbb{R} \\to \\mathbb{R}$ が存在する。 [3]: $X$ が $\\mathcal{F}$-可測であれば $$E(X|\\mathcal{F}) =X \\text{ a.s.}$$ [4]: シグマ場 $\\mathcal{G} = \\left\\{ \\emptyset , \\Omega \\right\\}$ に対して $$E(X|\\mathcal{G}) = E(X) \\text{ a.s.}$$ [5]: 定数 $c$ と全てのシグマ場 $\\mathcal{G}$ に対して $$E(c|\\mathcal{F}) = c \\text{ a.s.}$$ [6]: 定数 $c$ に対して $$E(cX | \\mathcal{G}) = c E(X | \\mathcal{G}) \\text{ a.s.}$$ [7]: $$E(X+Y | \\mathcal{G}) = E(X | \\mathcal{G}) + E(Y| \\mathcal{G}) \\text{ a.s.}$$ [8]: $X \\ge 0 \\text{ a.s.}$ であれば $$E(X | \\mathcal{G}) \\ge 0 \\text{ a.s.}$$ [9]: $X \\ge Y \\text{ a.s.}$ であれば $$E(X | \\mathcal{G}) \\ge E(Y | \\mathcal{G}) \\text{ a.s.}$$ [10]: $$\\left| E( X | \\mathcal{G} ) \\right| \\le E ( | X | | \\mathcal{G} ) \\text{ a.s.}$$ [11]: 全てのシグマ場 $\\mathcal{G}$ に対して $$E \\left[ E ( X | \\mathcal{G} ) \\right] = E(X)$$ $\\sigma (X) = \\left\\{ X^{-1} (B) : B \\in \\mathcal{B}(\\mathbb{R}) \\right\\}$ は確率変数 $X$ によって生成される $\\Omega$ の最小のシグマ場を表す。これについて $E(Y|\\sigma (X)) = E(Y|X)$ のように表記できる。 $Z$ が $\\mathcal{F}$-可測関数であることは、全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $Z^{-1} (B) \\in \\mathcal{F}$ という意味である。 ボレル関数とは、全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $f^{-1} (B)$ もボレル集合である関数 $f : \\mathbb{R} \\to \\mathbb{R}$ を指す。 説明 [1],[2]: これらの二つの定理は、$X$ に関する $Y$ の条件付き期待値が $X$ に依存するある関数として表されることを示している。特に、$X$ の値が与えられた場合は、$E(Y | X = a) = h(a)$ のように表される。[2]は[1]の系として、これにより、基本的な確率論でも日常的に使用される期待値の性質がほとんど確実に保証される。 線形性 [5]～[7]: $E(aX + b | \\mathcal{G}) = aE(X | \\mathcal{G}) + b$: 期待値の線形性は、条件付きであっても保持される。 シグマ場は情報である [3] $E(X | \\mathcal{F}) = X$: 式の意味を考えると、確率変数 $X$ が $\\mathcal{F}$-可測であることは、シグマ場 $\\mathcal{F}$ が $X$ の全ての情報を持っていることを意味する。逆に考えると、そのために可測と呼ぶのである。したがって、$E(X|\\mathcal{F})$ はいかなる妨害もなく $X$ をそのまま把握できる。$\\mathcal{F}$ 上で全ての情報が与えられた $X$ は、わざわざ $E$ で計算する必要はない。次の例を考えてみよう： 6面のサイコロを振って、目ごとに1ドルもらうゲームをするとき、もらえるお金の期待値は3.5ドルである。これを計算する理由は、実際にサイコロの目が何になるかわからないからである。しかし、サイコロを振る前に私の頭の中にシグマ場 $\\mathcal{F}$ が正確に与えられるならば、サイコロの目 $X$ を正確に測定できるため、正確に何ドルもらえるかがわかる。毎回3.5ドルを支払うとしても、勝つゲームはして、負けるゲームはしなければそれでよい。この意味で、乱数ハッキングは、シグマ場（乱数表）を盗んで、本来ランダムであるべきものを決定的にする攻撃技術に相当する。これが成功すれば、銀行のセキュリティカードやOTPなど、乱数に依存する暗号システムが破られる。 一方、$\\sigma (X)$ は $X$ の全ての情報を持ちながら最小のシグマ場として定義されているので、当\n然 $E(X| \\sigma (X)) = X$ である。これは上で紹介した表記に従って、$E(X|X) = X$ のようである。\n[4] $E(X|\\mathcal{G}) = E(X)$: 式の意味を考えると、トリビアルなシグマ場 $\\mathcal{G} = \\left\\{ \\emptyset , \\Omega \\right\\}$ は $X$ に関してどのような情報も与えないため、途方に暮れて確率空間 $\\Omega$ 全体を探して $\\displaystyle \\int_{\\Omega} X d P$ を計算するしかない。 [10] $\\left| E( X | \\mathcal{G} ) \\right| \\le E ( | X | | \\mathcal{G} )$: 絶対値の性質により $$ - E ( | X | | \\mathcal{G} ) \\le E( X | \\mathcal{G} ) \\le E ( | X | | \\mathcal{G} ) $$ [11] $E \\left[ E ( X | \\mathcal{G} ) \\right] = E(X)$: 確率論の様々な証明で有用に使用される等式で、主に $E(X)$ は直接計算が難しいが、何らかの $\\mathcal{G}$ が与えられると $E(X|\\mathcal{G})$ が計算しやすくなる場合にトリックとして使用される。 証明 [1] $h : \\mathbb{R} \\to \\mathbb{R}$ を $z \\in \\mathbb{R}$ に対して $h(z) := \\left( g \\circ f^{-1} ( \\left\\{ z \\right\\} ) \\right)$ のように定義する。\n$\\left\\{ z \\right\\} \\in \\mathcal{B}(\\mathbb{R})$ の場合、$f$ は $\\mathcal{F}$-可測なので、$f^{-1}(\\left\\{ z \\right\\}) \\in \\mathcal{F}$ であり、$g$ も $\\mathcal{F}$-可測なので、$h$ はよく定義され、$g (\\omega) = ( h \\circ f ) ( \\omega )$ を満たす。\n全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $$ h^{-1}(B) = (f \\circ g^{-1})(B) = f \\left( g^{-1} (B) \\right) $$ を考えると、$g^{-1} (B) \\in \\mathcal{F}$ なので $f(g^{-1} (B) ) \\in \\mathcal{B}(\\mathbb{R})$ である。全ての $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $h^{-1}(B) \\in \\mathcal{B}(\\mathbb{R})$ なので、$h$ はボレル関数である。\n■\n[2] $E ( Y | X ) = E ( Y | \\sigma (X) )$ は条件付き期待値の定義により $\\sigma (X)$-可測な確率変数であり、$X$ も $\\sigma (X)$ の定義に従って明らかに $\\sigma (X)$-可測な確率変数である。したがって、[1]により $\\mathcal{F} = \\sigma (X)$ とし、 $$ f = X \\\\ g = E ( Y | X ) $$ とすると、$E(Y|X) = h(X)$ を満たすボレル関数 $h : \\mathbb{R} \\to \\mathbb{R}$ が存在する。\n■\n戦略 [3]～[7]: 積分形に変換して展開し、定積分が同じであることを示した後、次の定理を適用する。元々特別な名前はないが、この投稿でのみルベーグ積分の補題と命名することにする。\nルベーグ積分の性質 $$ \\forall A \\in \\mathcal{F}, \\int_{A} f dm = 0 \\iff f = 0 \\text{ a.e.} $$\n[3] 全ての $A \\in \\mathcal{F}$ に対して $\\displaystyle \\int_{A} X dP = \\int_{A} X dP$ を満たす $X$ が一意に存在するので、条件付き期待値の定義により、$X = E(X| \\mathcal{F})$ は $\\mathcal{F}$ に対する $X$ の条件付き期待値である。したがって、全ての $A \\in \\mathcal{F}$ に対して $$ \\int_{A} E(X |\\mathcal{F}) dP = \\int_{A} X dP $$ となり、ルベーグ積分の補題により $X = E(X |\\mathcal{F}) \\text{ a.s.}$\n■\n[4] 条件付き期待値の定義により、$\\displaystyle \\int_{A} E(X |\\mathcal{G}) dP = \\int_{A} X dP$ である。\nケース 1. $A = \\emptyset$\n$$ 0 = \\int_{\\emptyset} E(X |\\mathcal{G}) dP = \\int_{\\emptyset} X dP = 0 $$\nケース 2. $A = \\Omega$\n$$ \\int_{\\Omega} E(X |\\mathcal{G}) dP = \\int_{\\Omega} X dP = E(X) = E(X) P(\\Omega) = E(X) \\int_{\\Omega} 1 dP = \\int_{\\Omega} E(X) dP $$\nしたがって、どちらの場合も、ルベーグ積分の補題により $X = E(X |\\mathcal{G}) \\text{ a.s.}$\n■\n[5] $c \\in \\mathcal{G}$ であり、$E(c | \\mathcal{G}) \\in \\mathcal{G}$ なので、条件付き期待値の定義により、全ての $A \\in \\mathcal{G}$ に対して $$ \\int_{A} E(c |\\mathcal{G}) dP = \\int_{A} X dP $$ となり、したがってルベーグ積分の補題により $c = E(c | \\mathcal{G}) \\text{ a.s.}$\n■\n[6] 条件付き期待値の定義とルベーグ積分の線形性により、全ての $A \\in \\mathcal{G}$ に対して $$ \\begin{align*} \\int_{A} E( cX |\\mathcal{G}) dP =\u0026amp; \\int_{A} cX dP \\\\ =\u0026amp; c \\int_{A} X dP \\\\ =\u0026amp; c \\int_{A} E(X|\\mathcal{G}) dP \\\\ =\u0026amp; \\int_{A} c E(X|\\mathcal{G}) dP \\end{align*} $$ となり、ルベーグ積分の補題により $E( cX |\\mathcal{G}) = c E(X|\\mathcal{G}) dP \\text{ a.s.}$\n■\n[7] 条件付き期待値の定義とルベーグ積分の線形性により、全ての $A \\in \\mathcal{G}$ に対して $$ \\begin{align*} \\int_{A} E( X+Y |\\mathcal{G}) dP =\u0026amp; \\int_{A} (X+Y) dP \\\\ =\u0026amp; \\int_{A} X dP +\\int_{A} Y dP \\\\ =\u0026amp; \\int_{A} E(X|\\mathcal{G}) dP + \\int_{A} E(Y|\\mathcal{G}) dP \\\\ =\u0026amp; \\int_{A} \\left[ E(X|\\mathcal{G}) + E(Y|\\mathcal{G}) \\right] dP \\end{align*} $$ となり、ルベーグ積分の補題により $$ E( X +Y |\\mathcal{G}) = E(X|\\mathcal{G}) + E(Y|\\mathcal{G}) dP \\text{ a.s.} $$\n■\n[8] $E( X |\\mathcal{G}) \u0026lt; 0$ と仮定すると $$ \\begin{align*} \\int_{A} E( X |\\mathcal{G}) dP =\u0026amp; \\int_{A} X dP \\\\ \\ge\u0026amp; \\int_{A} 0 dP \\\\ =\u0026amp; 0 \\end{align*} $$ となるため、矛盾が生じる。したがって、$E( X |\\mathcal{G}) \\ge 0 \\text{ a.s.}$ でなければならない。\n■\n[9] $Z := X - Y \\ge 0$ とすると、[8] により $$ E(X-Y | \\mathcal{G}) \\ge 0 $$ となり、条件付き期待値の線形性により $$ E(X| \\mathcal{G}) - E(Y | \\mathcal{G}) \\ge 0 \\text{ a.s.} $$\n■\n[10] パート 1. $X \\ge 0$\n$X \\ge 0$ の場合、$|X| = X$ となるため $$ E( |X| |\\mathcal{G}) = E(X|\\mathcal{G}) $$\n[8]により $E(X|\\mathcal{G}) \\ge 0$ となるため、同様に $E(X|\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right|$ となり $$ E( |X| |\\mathcal{G}) = E(X|\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right| $$\nパート 2. $X \u0026lt; 0$\n[6]により $$ E( |X| |\\mathcal{G}) = E( -X |\\mathcal{G}) = - E(X |\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right| $$\nパート 3. $X = X^{+} - X^{-}$\n三角不等式により $$ \\left| E(X|\\mathcal{G}) \\right| \\le \\left| E( X^{+} |\\mathcal{G}) \\right| + \\left| E( X^{-} |\\mathcal{G}) \\right| $$ $X^{+} , X^{-} \\ge 0$ であるため、パート 1により $$ \\left| E(X|\\mathcal{G}) \\right| \\le E( \\left| X^{+} \\right| |\\mathcal{G}) + E( \\left| X^{-} \\right| | \\mathcal{G}) $$\n[7]と絶対値の表示 $|f| = |f^{+}| + |f^{-}|$ により $$ \\begin{align*} \\left| E(X|\\mathcal{G}) \\right| \\le \u0026amp; E( \\left| X^{+} \\right| + \\left| X^{-} \\right| | \\mathcal{G}) \\\\ =\u0026amp; E( \\left| X \\right| | \\mathcal{G}) \\text{ a.s.} \\end{align*} $$\n■\n[11] $$ \\begin{align*} E \\left[ E( X | \\mathcal{G} ) \\right] =\u0026amp; \\int_{\\Omega} E ( X | \\mathcal{G} ) d P \\\\ =\u0026amp; \\int_{\\Omega} X d P \\\\ =\u0026amp; E(X) \\end{align*} $$\n■\n","id":1322,"permalink":"https://freshrimpsushi.github.io/jp/posts/1322/","tags":null,"title":"条件付き期待値の性質"},{"categories":"집합론","contents":"定義 1 集合: 我々の直感や思考の対象として互いに明確に区別される対象の集まりを集合という。 要素: 集合に含まれる対象を要素という。 命題関数: 集合$U$の要素$x$に対して真または偽のいずれかである命題$p(x)$を$U$の命題関数という。 説明 数学で集合はほとんど母国言語に匹敵するほど重要な概念だ。自然言語よりも優れているかもしれない。なぜなら、必然的についてくる曖昧さを排除して、その定義と形式だけで論理を展開できるからだ。 通常、要素は小文字で、集合は大文字で表される。$a$が$A$に属している場合、$a \\in A$と表され**$a$は$A$の要素**と言われる。もちろん、要素と集合を必ずアルファベットの大文字と小文字で表示する必要はない。全ての自然数の集合は通常$\\mathbb{N}$と表示され、$N \\in \\mathbb{N}$のように表示しても何の問題もない。 列挙: 自然数の集合$\\mathbb{N}$は$\\left\\{ 1 , 2, 3, \\cdots \\right\\}$のように表すことができる。こうして集合の要素を直接書き出して表示する表記法を列挙法という。 条件表示法: 列挙法とは異なり、特定の条件を満たす要素のみを集めた集合として表現することもできる。例えば、$5$より大きい自然数のみを持つ集合を表現したい場合には、命題関数$p(x): x \u0026gt; 5$に対して$\\left\\{ x \\in \\mathbb{N} : p(x) \\text{ is truth} \\right\\}$のように表現される。これをさらに簡単に、命題関数を別に定義せずに$\\left\\{ x \\in \\mathbb{N} : x \u0026gt; 5 \\right\\}$のように表現する。この表記法を条件表示法という。 命題関数は命題関数そのものとして定義される点に注意する必要がある。集合論で語る関数の定義に合致しているが、命題のみで定義できるというのが重要だ。この点が不明確な場合、関数が関数として循環定義される事態になりかねず、条件表示法の自由な使用が困難になる。一方、命題関数は論理式とも呼ばれる。 李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p47, 73, 81, 85.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1316,"permalink":"https://freshrimpsushi.github.io/jp/posts/1316/","tags":null,"title":"集合と命題関数の定義"},{"categories":"측도론","contents":"定理1 (a) $\\nu$を可測空間 $(X, \\mathcal{E})$上で定義された符号測度とする。すると、以下を満たす $\\nu$の正集合 $P$と負集合 $N$が存在する。\n$$ P \\cup N=X \\quad \\text{and} \\quad P \\cap N =\\varnothing $$\nこのような $X=P \\cup N$を $\\nu$に対するハーン分解Hahn decompositionという。\n(b) $P^{\\prime}, N^{\\prime}$が (a) を満たす別の集合であるとする。その場合、以下の集合は $\\nu$に対する零集合である。\n$$ (P-P^{\\prime}) \\cup (P^{\\prime}-P)=(N-N^{\\prime}) \\cup (N^{\\prime}-N) $$\n対称差symmetric difference記号を使用して以下のように表記される。\n$$ P\\Delta P^{\\prime}=N\\Delta N^{\\prime} $$\n説明 (a) 任意の可測空間が与えられた時、集合 $X$を $\\nu$に対して正の集合と負の集合に分けることができるということである。\n(b) 上述のように集合 $X$を分ける方法が複数存在しても、実質的な違いはないということである。$P$と$P^{\\prime}$、$N$と$N^{\\prime}$は常に互いに零集合だけの差があるため、集合の観点では異なるかもしれないが、測度の観点では同じである。\n証明 この定理の証明自体はそれほど難しくないが、証明の流れが単純ではないため、これを事前に具体的に説明し、始める。まず、ある正の集合 $P$を定義する。そして $N$を $N:=X-P$と定義する。この時、$N$が負の集合であれば、(a) に対する証明が完了する。$N$が負の集合であることを証明する前に、上述のように定義された $N$が持つ2つの性質を確認することにする。そして、最終的な証明では背理法を使用する。$N$が負の集合でないと仮定し、2つの性質を使用して矛盾が生じることを示す。\n一般性を失わずに、$\\nu$が$+\\infty$の値を持たないと仮定する。他の場合は $-\\nu$に対して同じ方法で証明すればよい。$C$を $\\mathcal{E}$のすべてのポジティブセットのコレクションとする。すると、仮定により $\\nu$は $+\\infty$の値を持たないため、以下のように定義される $M$が存在する。\n$$ M:=\\sup \\limits_{P \\in C } \\nu (P) \u0026lt; \\infty $$\nここで、$\\nu (P)=M$を満たすマキシマイザー $P$の存在を示すことができる。以下のようなマキシマイジングシーケンス $\\left\\{ P_{j} \\right\\}$を考える。\n$$ \\lim \\limits_{j \\rightarrow \\infty} \\nu (P_{j})=M $$\nこの時、$P_{j}$同士には含まれる関係がないため、以下のような $\\tilde{P_{j}}$を考える。\n$$ \\tilde{P_{j}} :=\\bigcup \\limits_{k=1}^j P_{k} $$\nすると、$\\nu (P_{j}) \\le \\nu (\\tilde{P_{j}}) \\le M$であるため、$\\left\\{ \\tilde{P_{j}} \\right\\}$はマキシマイジングシーケンスである。また、$\\tilde{P_{1}} \\subset \\tilde{P_2}\\subset \\cdots $であることは定義によって明らかである。ここで、$P$を以下のように定義する。\n$$ P := \\bigcup \\limits_{j=1}^\\infty \\tilde{P_{j}} $$\nすると、次が成り立つ。\n$$ \\nu (P)=\\lim \\limits_{j\\rightarrow \\infty} \\nu (\\tilde{P_{j}})=M $$\nしたがって、$\\nu (P)=M$を満たすマキシマイザーが存在することを示した。また、$P$は正の集合の可算和であるため、正の集合である。実際にこのように作り出された $P$と $N:=X-P$は、定理で述べられているような一つの分解である。$N$がそのような負の集合であることを示すプロセスが残されている。ここで、$N:=X \\setminus P$とする。上述のように、$N$が負の集合であることを示せば証明が完了する。まず、このような $N$が以下の2つの性質を持つことを証明する。\n主張 1 $N$は測度値が0より大きい正の集合を含まない。つまり、0ではない正の集合を含まない。すなわち $\\nu (E)\u0026gt;0$であり、$E$が正の集合であれば、$E \\not \\subset N$である。\nこの時、注意すべき点は、正の集合でも、負の集合でもない $E \\subset N$が存在する可能性があることである。つまり、$N$の部分集合になり得るのは、1. 空集合、2. 負の集合、3. 正の集合でも負の集合でもない集合である。\n証明\n$E\\subset N$が正の集合で $\\nu (E) \u0026gt;0$であるとする。すると、$N$の定義により、$E$と $P$は互いに素な集合である。したがって、次が成り立つ。\n$$ \\nu (P \\cup E)=\\nu (P)+\\nu (E) $$\nしかし、$\\nu (P)=M$であるため、次が成り立つ。\n$$ \\nu (P \\cup E)=\\nu (P)+\\nu (E)\u0026gt;M $$\nしかし、これは $M=\\sup \\nu (F)\\ \\forall F\\in \\mathcal{E}$という仮定に矛盾する。したがって、$\\nu (E)\u0026gt;0$である正の集合 $E \\subset N$は存在しない。\n主張 2 もし $A \\subset N$で $\\nu (A)\u0026gt;0$であれば、$\\nu (B) \u0026gt; \\nu (A)$を満たす $B \\subset A$が存在する。\n証明\n$A \\subset N$で $\\nu (A)\u0026gt;0$であるとする。すると、主張 1 により、$A$は正の集合ではない。したがって、$A$は空集合でもなく、正の集合でもない。従って、次を満たす $C$が存在する2。\n$$ C \\subset A,\\ \\nu (C) \u0026lt;0 $$\nここで、$B:=A-C$とする。すると、次が成り立つ。\n$$ \\nu (A)=\\nu (B)+\\nu (C) \u0026lt; \\nu (B) $$\nここで、$N$が負の集合でないと仮定する。 上の2つの性質を利用して矛盾が生じることを示せば、$N$が負の集合であることが証明される。\nパート 1.\n$\\left\\{ A_{j} \\right\\}$を $N$の部分集合の列とし、$\\left\\{ n_{j} \\right\\}$を自然数の列とする。$N$が負の集合でないと仮定したので、$\\nu (B) \u0026gt;0$となるある $B \\subset N$が存在する。そして、$\\nu (B) \u0026gt; \\frac{1}{n_{j}}$を満たす最小の $n_{j}$を $n_{1}$とし、$n_{1}$に対してこれを満たす $B$を $A_{1}$とする。$\\nu (B)=\\nu (A_{1})\u0026gt;0$であるため、上で $N$に対して行ったプロセスを $A_{1}$に対して同じように適用することができる。\nパート 2\n再び $\\nu (B)\u0026gt;0$となるある $B\\subset A_{1}$が存在し、主張 2 により $\\nu (B) \u0026gt; \\nu (A_{1})$である。したがって、$\\nu (B) \u0026gt; \\nu (A_{1})+\\frac{1}{n}$を満たす自然数 $n$が存在する。この中で最も小さい自然数を $n_2$とし、そのような $B$を $A_2$とする。\nパート 3\n同じプロセスを繰り返すと、$n_{j}$は $\\nu (B)\u0026gt;0$となるある $B \\subset A_{j-1}$に対して $\\nu (B)\u0026gt;\\nu (A_{j-1}) + \\dfrac{1}{n_{j}}$を満たす最も小さい自然数である。また、そのような $B$を $A_{j}$とする。ここで、$A=\\bigcap \\nolimits_{1}^\\infty A_{j}$とする。$\\nu$が $+\\infty$の値を持たないと仮定した上で、符号測度の性質 $(B)$により、次が成り立つ。\n$$ \\begin{align*} +\\infty \\gt \\nu (A) \u0026amp;= \\nu \\left(\\bigcap \\nolimits_{1}^\\infty A_{j} \\right) \\\\ \u0026amp;= \\lim \\limits_{j \\rightarrow \\infty} \\nu (A_{j}) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{j-1}) +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{j-2}) + \\frac{1}{n_{j-1}} +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\vdots \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{1}) + \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\frac{1}{n_{1}}+ \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;= \\sum \\limits_{j=1}^\\infty \\frac{1}{n_{j}} \\end{align*} $$\n数列が有限であるため、極限は0である。\n$$ \\lim \\limits_{j\\rightarrow \\infty} \\frac{1}{n_{j}} =0 $$\nしたがって、次を得る。\n$$ \\begin{equation} \\lim \\limits_{j\\rightarrow \\infty} n_{j} =\\infty \\label{eq1} \\end{equation} $$\nしかし、パート 1 で見たように、主張 2 により、ある自然数 $n$に対して $\\nu (B) \u0026gt; \\nu (A) +\\dfrac{1}{n}$を満たす $B \\subset A$が存在する。すると、$A$の定義により $A \\subset A_{j-1}$であり、主張 2 により $\\left\\{ \\nu (A_{j}) \\right\\}$は増加列であることが分かる。したがって、$\\nu (A) =\\lim \\limits_{j \\rightarrow \\infty} \\nu (A_{j})$であるため、$\\nu (A) \u0026gt; \\nu (A_{j-1})$である。\nまた、$(1)$により、十分に大きな $j$に対して $n_{j} \u0026gt;n$である。したがって、次が成り立つ。\n$$ \\nu (B) \u0026gt; \\nu (A) +\\frac{1}{n}\u0026gt;\\nu (A_{j-1}) +\\frac{1}{n} \u0026gt; \\nu (A_{j-1}) +\\frac{1}{n_{j}} $$\nしかし、これは $n_{j}$と $A_{j}$の定義に対する矛盾である。したがって、$N$が負の集合でないという仮定は誤りである。すなわち、$N$は負の集合である。\n$P^{\\prime}$, $N^{\\prime}$を上記の定理を満たす別の一つの分解とする。すると、次が成り立つ。\n$$ P^{\\prime} \\cup N^{\\prime} =X \\quad \\text{and} \\quad P^{\\prime}\\cap N^{\\prime} =\\varnothing $$\nしたがって、$P-P^{\\prime} \\subset P$、$P-P^{\\prime}\\subset N^{\\prime}$であることが分かる。すると、$P-P^{\\prime}$は正の集合でありながら負の集合であるが、これを満たすのは零集合だけであるため、$P-P^{\\prime}$は$\\nu-\\mathrm{null}$である。同様に、$P^{\\prime}-P$、$N-N^{\\prime}$、$N^{\\prime}-N$に対しても同じ方法で $\\nu -\\mathrm{null}$であることを示すことができる。\n■\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (第2版, 1999), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n存在しなければ、定義により Aは空集合か、あるいは正の集合であるべきである。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1308,"permalink":"https://freshrimpsushi.github.io/jp/posts/1308/","tags":null,"title":"ハーン分解定理"},{"categories":"바나흐공간","contents":"実数に関するハーン・バナッハの定理1 $X$は$\\mathbb{R}$-ベクトル空間であり、$Y \\subset X$とする。$p : X \\to \\mathbb{ R}$を$X$の準線形 線形汎関数とする。今、$y^{\\ast} : Y \\to \\mathbb{ R}$が以下の条件を満たす$Y$の$\\mathbb{R}$-線形汎関数であると仮定する。\n$$ y^{\\ast}(y) \\le p(y)\\quad \\forall y\\in Y $$\nすると、以下の条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{R}$が存在する。\n(a) $x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$\n(b) $x^{\\ast}(x) \\le p(x),\\quad \\forall x \\in X$\n説明 $\\mathbb{R}-$ベクトル空間とは、体$\\mathbb{R}$に関するベクトル空間のことである。つまり、ベクトル空間のスカラー倍に関する条件$(M1)$〜$(M5)$が実数に対して成立するという意味である。同様に、$\\mathbb{R}$-線形とは、線形の二つの性質のうちスカラー倍に関する内容が実数に対して成立するという意味である。\n$X, Y$が$\\mathbb{R}$-ベクトル空間であるため、$y^{\\ast}$、$x^{\\ast}$が線形であることと$\\mathbb{R}$-線形であることは同じ意味である。この部分が混乱する場合は、**$\\mathbb{R}$-、$\\mathbb{C}$-はこの記事では存在しない文字と考えても、証明を理解する上で問題はない。**後にハーン・バナッハの定理をノルム空間に適用する際には、関数$p$がノルムに対応する。この定理の証明は省略し、複素数に関するハーン・バナッハの定理の証明に使用する補助定理として利用する。\n複素数に関するハーン・バナッハの定理2 $X$は$\\mathbb{C}$-ベクトル空間であり、$Y \\subset X$とする。$p : X \\to \\mathbb{ R}$を以下のように定義された準線形汎関数とする。\n$$ p(\\lambda x)=|\\lambda| p(x),\\quad x\\in X, \\lambda \\in \\mathbb{C} $$\nそして、$y^{\\ast} : Y \\to \\mathbb{ C}$が以下の条件を満たす$Y$の線形汎関数であると仮定する。\n$$ \\begin{equation} \\text{Re}\\left( y^{\\ast}(y) \\right) \\le p(y),\\quad \\forall y\\in Y \\end{equation} $$\nすると、以下の条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{C}$が存在する。\n$x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$ $\\text{Re}(x^{\\ast}(x)) \\le p(x),\\quad \\forall x \\in X$ 説明 実数に関する定理と比較した場合、$p$の値域が$\\mathbb{R}$であることは変わらないが、これは上述のように$X$がノルム空間の場合、$p$がノルムに対応するからである。$X$、$Y$は$\\mathbb{C}$-ベクトル空間であり、$\\mathbb{R} \\subset \\mathbb{C}$であるため、$\\mathbb{R}$-ベクトル空間である条件も満たされる。すべての複素数に対してベクトル空間の条件$(M1)$〜$(M5)$が成立する場合、自動的にすべての実数に対しても成立するからである。同様に、$y^{\\ast}$、$x^{\\ast}$は$\\mathbb{C}$-線形であるため、$\\mathbb{R}-$線形である条件も満たされる。\n証明 関数$\\psi : Y \\to \\mathbb{ R}$を以下のように定義する。\n$$ \\psi (y) = \\text{Re} ( y^{\\ast}(y) ) $$\nすると、$\\psi$も$Y$の$\\mathbb{C}$-線形汎関数であることが示される。これは$\\mathrm{ Re}$と$y^{\\ast}$が線形であるために自明な結果であり、示す過程は非常に簡単なので省略する。$\\psi$の定義と$(1)$により、以下の式が成立する。\n$$ \\psi(y)= \\text{Re} \\left( y^{\\ast}(y) \\right) \\le |y^{\\ast}(y)| \\le p(y) $$\nすると、実数に関するハーン・バナッハの定理により、以下の条件を満たす$X$の$\\mathbb{R}$-線形汎関数$\\Psi : X \\to \\mathbb{ R}$が存在する。\n$$ \\Psi (y) = \\psi (y),\\quad \\forall y \\in Y $$\n$$ \\Psi (x) \\le p(x),\\quad \\forall x \\in X $$\nそして、新たに関数$\\Phi : X \\to \\mathbb{ C}$を以下のように定義しよう。最終的な目標は、以下のように定義された$\\Phi$が、定理で存在すると言われていた$x^{\\ast}$であることを示すことである。\n$$ \\Phi (x) := \\Psi (x) -i \\Psi(ix) $$\nすると、$\\Phi$が$X$の線形汎関数であることが確認できる。$\\Psi$が$\\mathbb{R}$-線形であるため、加法と実数乗に関しては線形性が自明であるため、$\\Phi(ix)=i\\Phi(x)$のみを確認すればよい。\n$$ \\begin{align*} \\Phi(ix) =\u0026amp;\\ \\Psi(ix) -i \\Psi( -x) \\\\ =\u0026amp;\\ \\Psi(ix)+i\\Psi(x) \\\\ =\u0026amp;\\ -i^2 \\Psi(ix)+i\\Psi(x) \\\\ =\u0026amp;\\ i \\big( \\Psi(x)-i\\Psi(ix) \\big) \\\\ =\u0026amp;\\ i\\Phi(x) \\end{align*} $$\n$\\Phi$が**(a)**を満たすことは、以下のように示すことができる。$y \\in Y$とすると、\n$$ \\begin{align*} \\Phi(y) =\u0026amp;\\ \\Psi (y) -i \\Psi(iy) \\\\ =\u0026amp;\\ \\psi(y) -i\\psi(iy) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right)-i\\text{Re} \\left( y^{\\ast}(iy) \\right) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right) +\\text{Im} \\left(-iy^{\\ast}(iy) \\right) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right) +\\text{Im} \\left( y^{\\ast}(y) \\right) \\\\ =\u0026amp;\\ y^{\\ast}(y) \\end{align*} $$\n$\\Phi$が**(b)**を満たすことを示すのはさらに簡単である。\n$$ \\mathrm{Re }\\left( \\Phi(x) \\right) = \\Psi(x) \\le p(x) $$\nしたがって、$\\Phi$が$X$の線形汎関数であり、**(a), (b)**を満たすため、$x^{\\ast}=\\Phi$が存在する。\n■\nセミノルムに関するハーン・バナッハの定理 $X$は$\\mathbb{C}$-ベクトル空間であり、$Y \\subset X$とする。$p : X \\to \\mathbb{ R}$を$X$のセミノルムとする。そして、$y^{\\ast} : Y \\to \\mathbb{ C}$が以下の条件を満たす$Y$の線形汎関数であると仮定する。\n$$ | y^{\\ast}(y) | \\le p(y),\\quad \\forall y\\in Y $$\nすると、以下の条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{C}$が存在する。\n$x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$\n$| x^{\\ast}(x) | \\le p(x),\\quad \\forall x \\in X$\n証明 セミノルムと準線形の定義から、$p$がセミノルムであれば準線形の条件も自動的に満たされる。\nまず、以下の式が成立することは自明である\n$$ \\text{Re} \\left( y^{\\ast}(y) \\right) \\le |y^{\\ast}(y) | \\le p(y) $$\nしたがって、複素数に関するハーン・バナッハの定理により、以下の二つの条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{C}$が存在する。\n$$ x^{\\ast}(y)=y^{\\ast}(y) \\quad \\forall y \\in Y $$\n$$ \\text{Re} \\left( x^{\\ast}(x) \\right) \\le p(x) \\quad \\forall x \\in X $$\n$S = \\left\\{ \\lambda \\in \\mathbb{C} : | \\lambda | =1 \\right\\}$とする。すると、\n$$ \\begin{align*} \\text{Re} \\left( \\lambda x^{\\ast}(x) \\right) =\u0026amp;\\ \\text{Re} \\left( \\lambda x^{\\ast}(\\lambda x) \\right) \\\\ \\le \u0026amp; p(\\lambda x) \\\\ =\u0026amp;\\ |\\lambda| p(x)=p(x) \\quad \\forall x \\in X \\end{align*} $$\nこの時、固定された$x \\in X$に対して$|x^{\\ast}(x)|=\\lambda x^{\\ast}(x)$を満たす$\\lambda \\in S$を常に見つけることができる。したがって、$x$とその特定の$\\lambda$に対して、以下の式が成立する。\n$$ | x^{\\ast}(x) | =\\lambda x^{\\ast}(x) = \\text{Re} \\left( \\lambda x^{\\ast}(x) \\right) \\le p(x), \\quad \\forall x \\in X $$\n$X$の線形汎関수$x^{\\ast}$が二つの条件を満たすため、証明完了。\n■\n付録 固定された$x$に対して$x^{\\ast}(x)=a+ib$とする。$\\lambda=c+id$とする。$\\lambda$の条件により$c^2+d^2 =1$であるため、$\\lambda=c+i\\sqrt{1-c^2}$である。また、$|x^{\\ast}(x)|=\\sqrt{a^2+b^2}$である。$\\lambda x^{\\ast}(x)=(ac-b\\sqrt{1-c^2})+i(a\\sqrt{1-c^2}+bc)$であり、$|x^{\\ast}(x)|$が非負の実数であるため、\n$$ \\begin{align*} \u0026amp;\u0026amp; a\\sqrt{1-c^2}+bc =\u0026amp;\\ 0 \\\\ \\implies\u0026amp;\u0026amp; a^2(1-c^2) =\u0026amp;\\ b^2c^2 \\\\ \\implies\u0026amp;\u0026amp; a^2 =\u0026amp;\\ (a^2+b^2)c^2 \\\\ \\implies\u0026amp;\u0026amp; c^2 =\u0026amp;\\ \\dfrac{a^2}{a^2+b^2} \\tag{2} \\end{align*} $$\n便宜上$c=\\dfrac{a}{\\sqrt{a^2+b^2}}$とし、$d=\\dfrac{-b}{\\sqrt{a^2+b^2}}$とする。すると、$(2)$と$c^2+d^2=1$が成立する。また、$|x^{\\ast}(x)|=ac-bd=\\sqrt{a^2+b^2}$が成立する。したがって、固定された$x$に対して$x^{\\ast}(x)=a+ib$であれば、$\\lambda=\\dfrac{a}{\\sqrt{a^2+b^2}}-i\\dfrac{b}{\\sqrt{a^2+b^2}}\\in S$に対して$|x^{\\ast}(x)|=\\lambda x^{\\ast}(x)$が成立する。\nhttp://mathonline.wikidot.com/the-hahn-banach-theorem-real-version\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://mathonline.wikidot.com/the-hahn-banach-theorem-complex-version\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1230,"permalink":"https://freshrimpsushi.github.io/jp/posts/1230/","tags":null,"title":"実数、複素数、セミノルムに対するハーン・バナッハの定理"},{"categories":"해석개론","contents":"定義1 $a$を含むある$E$で$f$が定義されていて、限界\n$$ f^{\\prime} (a) := \\lim_{h \\to 0} {{ f (a + h ) - f(a) } \\over { h }}=\\lim \\limits_{x\\rightarrow a}\\frac{f(x)-f(a)}{x-a} $$\nが存在するならば、$f$は$a$で微分可能differentiableであるといい、$f^{\\prime} (a)$を$a$での$f$の微分係数という。\n全ての点$a \\in E$に対して$f$が微分可能なら、$f$は$E$で微分可能であるという。$f$が$E$で微分可能な時、$E$上で定義された$f^{\\prime}$を$f$の導関数derivativeと呼ぶ。\n説明 解析学を学ぶ上で最も歓迎されるのが微分だ。なぜなら、数列であれ積分であれ本来の姿をそのまま持っているだけでなく、複雑になることに比べて、微分だけが比較的簡単で理解しやすいからだ。多重積分や偏微分も登場するが、他の概念に比べれば簡単で分かりやすい。このように微分の定義をあえて「実数空間」に限定し、偏微分が言及されるのは、微分が多次元に拡張されることを示唆しているためだ。\n要約 (a) 連続性: $f$が$a \\in E$で微分可能ならば、$a \\in E$で連続である。\n(b) 連鎖律: $( g \\circ f)' ( a ) = g \u0026rsquo; ( f (a) ) f '(a)$\n(c) 逆関数の定理: 開区間$E$で$f : E \\to \\mathbb{R}$が一対一の連続関数であるとする。(i) ある$a \\in E$に対して$b = f(a)$であり、(ii): $f ' (a) \\ne 0$が存在するならば、$f^{-1}$は$a$で微分可能であり、\n$$ \\left( f^{-1} \\right)' (b) = {{ 1 } \\over { f '(a) }} $$\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), p98-99\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1210,"permalink":"https://freshrimpsushi.github.io/jp/posts/1210/","tags":null,"title":"実数空間で定義された関数の微分"},{"categories":"바나흐공간","contents":"定義1 $X$をベクター空間としよう。次の三条件を満たす関数$\\left\\| \\cdot \\right\\| : X \\to \\mathbb{R}$が存在すれば、$\\left\\| \\cdot \\right\\|$を$X$のノルムと呼び、$(X,\\left\\| \\cdot \\right\\| )$をノルム空間と呼ぶ。\n(a) $\\left\\| x \\right\\| \\ge 0,\\quad \\forall\\ x \\in X$かつ$\\left\\| x \\right\\|=0 \\iff x = 0$\n(b) $|cx|=|c|\\left\\| x \\right\\|,\\quad \\forall\\ x\\in X,\\ \\forall\\ c \\in\\mathbb{C}$\n(c) $\\left\\| x + y \\right\\| \\le \\left\\| x \\right\\| + \\left\\| y \\right\\|,\\quad \\forall\\ x,y\\in X$\n説明 ノルム空間$X$のノルムは以下のように表される。\n$$ \\left\\| x \\right\\|_{X},\\quad \\left\\| x, X \\right\\|, \\quad \\left\\| x ; X \\right\\| $$\n(a) $\\left\\| x \\right\\|=0 \\iff x = 0$の条件がない場合は、セミノルムになる。\n(b) は$\\left\\| x - y \\right\\| =|y -x|$が成立するという意味である。\n(c) を三角不等式と呼び、以下の不等式を逆三角不等式と呼ぶ。ノルム空間$(X, \\left\\| \\cdot \\right\\| )$と$x, y \\in X$に対して、以下の不等式が成立する。\n$$ \\left| \\left\\| x \\right\\| - \\left\\| y \\right\\|\\ \\right| \\le \\left\\| x- y \\right\\| $$\nノルムは連続写像である。\nノルム空間としての距離空間、位相空間 ノルムが与えられると、以下のように自然に距離を定義できる。したがって、ノルム空間は距離空間になる。\n$$ d(x,y) = d_{X}(x,y) = \\left\\| x - y \\right\\|_{X} $$\n距離が与えられると、以下のようにオープンボールを定義できる。\n$$ B_{d}(x,r)=B_{r}(x):=\\left\\{ y\\in X\\ :\\ \\left\\| x - y \\right\\|_{X} \u0026lt;r \\right\\} $$\n全てのオープンボールの集合は$X$上の(位相数学での)基底になる。つまり、$X$のノルムで定義されたオープンボールによって$X$上の位相を作ることができるということである。このようにして作られた位相を$X$上のノルム位相2と呼ぶ。さらに、位相ベクター空間$X$の位相がノルム位相ならば、$X$をノルマブルと呼ぶ。\n以上の内容をまとめると、$X$がノルム空間であるということは、$X$がベクター空間であり、距離空間であり、位相空間であるという意味を全て含んでいるということである。したがって、関数解析学では与えられたノルム空間を自然に距離空間、位相空間としても扱う。\n証明3 三角不等式により、\n$$ \\left\\| x \\right\\|= | (x-y) +y| \\le |x-y| + \\left\\| y \\right\\| $$\nが成立する。したがって、\n$$ \\begin{equation} \\left\\| x \\right\\| - \\left\\| y \\right\\| \\le \\left\\| x- y \\right\\| \\end{equation} $$\n同様に、\n$$ \\left\\| y \\right\\| = | (y - x) + x| \\le \\left\\| y- x \\right\\| + \\left\\| x \\right\\| $$\nなので、\n$$ \\begin{equation} \\left\\| y \\right\\| - \\left\\| x \\right\\| \\le \\left\\| y- x \\right\\|=\\left\\| x - y \\right\\| \\end{equation} $$\nが成立する。したがって、$(1), (2)$により、\n$$ \\left| \\ \\left\\| x \\right\\| -\\left\\| y \\right\\|\\ \\right| \\le \\left\\| x- y \\right\\| $$\n■\nRobert A. Adams and John J. F. Foutnier、Sobolev Space (第2版、2003)、p4-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n距離の観点では、これを距離位相と呼ぶ。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOle Christensen、Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010)、p30\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1225,"permalink":"https://freshrimpsushi.github.io/jp/posts/1225/","tags":null,"title":"ノルム空間とは何か"},{"categories":"고전역학","contents":"概要 ハミルトンの原理、汎関数、作用、変分などについて、可能な限り簡単に説明しています。他の場所で満足のいく説明を見つけられなかった場合は、最後まで読むことをお勧めします。特に、大学1〜2年生でも十分に読めるように作成しました。\nラグランジュ力学1 物体が時間 $t_{1}$ から $t_{2}$ まで運動するとき、運動経路に対するラグランジアンの積分を作用actionといい、以下のように $J$ で表します。\n$$ \\begin{equation} J=\\int_{t_{1}}^{t_{2}} L dt \\end{equation} $$\nこのとき、可能なすべての運動経路の作用の中で、実際の運動経路の作用が最小になります。ラグランジアンLagrangianは、運動エネルギーとポテンシャルエネルギーの差で定義され、一般に$L$で表されます。\n$$ L = T-V $$\nこの内容は、ハミルトンの原理Hamilton\u0026rsquo;s variational principleまたは最小作用の原理principle of least actionと呼ばれます。最小作用の原理という名前は$(1)$の積分を作用と呼ぶからです。元々最小値と極小値は異なる概念ですが、ここでは同じ意味を持つとします。正確には極値（極大または極小）が適切です。マリオンの教科書を基準にすれば、恐らく1学期、ファウルズの教科書を基準にすれば2学期に学ぶラグランジュ力学の最初の内容です。しかし、教科書に忠実であるだけでは、この内容を理解するのが非常に難しかったです。新しい概念が登場しますが、それが何であるかを親切に説明してくれません。例えば、ファウルズの教科書では以下のような式が登場します。\n$$ \\begin{equation} \\delta J =\\delta \\int_{t_{1}}^{t_{2}} L dt = 0 \\end{equation} $$\nそして、新しく登場した記号$\\delta$についての説明は以下のようです。\n\"$\\delta$は、全体の積分の變分(variation)に対する極値である。\"\nこれを読んで$\\delta$が何を意味するのかどうやって分かるでしょうか。変分が何かもちゃんと教えてくれず、その後の計算はどんどん進みます。等式がなぜ成立するのかも分からないので、一行一行読むスピードも非常に遅く、内容を理解すること自体が非常に困難でした。そこで、ラグランジュ力学を初めて学ぶ学生のために、できるだけ親切に説明しようと思います。まず、ハミルトンの原理を記述する際に使用される用語を整理する必要があります。\n汎関数 多くの資料で$(2)$の積分を汎関数と言いますが、普通に勉強してきた物理学部の学生であれば、汎関数が何であるか知らないのが普通です。皆さんは、実数を入力すると実数（または複素数）が出力されるものを関数として知っているでしょう。\n$$ f(x)=x^2,\\quad g(x)=e^{2x} $$\nしかし、関数の数学的定義を考えると、数字を入力して数字が出力される必要はありません。何かを入力してそれに対応する結果が出力されるものが関数なので、入力するものに制限はありません。このとき、ある関数に関数を入力してそれに応じてある数が出力される場合、その関数を汎関数functionalと言います。例えば、以下のように定義された関数$F$は汎関数です。\n$$ {\\color{blue}F\\big( {\\color{orange}f(x)} \\big)} := {\\color{red}\\int_{1}^{2} f(x) dx} $$\nつまり、関数$F$はある関数を$1$から$2$まで定積分した値を関数値として持ちます。実際に計算してみると、\n$$ {\\color{blue}F( {\\color{orange} e^{x} })} = \\int_{1}^2 e^x dx = {\\color{red}e^2-e},\\quad {\\color{blue}F({\\color{orange}x^2})}=\\int_{1}^2 x^{2} dx = {\\color{red}\\frac{7}{3} } $$\n上記のように、関数を入力したときに実数（または複素数）が出力される関数を汎関数と言います。続く内容ですが、最小作用の原理で作用はまさに汎関数です。\u0026lsquo;各運動経路に対するラグランジアン\u0026rsquo;という関数を入力したときにある値が出るので、汎関数です。汎関数に関する数学的な内容を含む記事がブログにありますが、リンクは紹介しません。おそらく読めばさらに混乱するでしょうから、できれば読まないことをお勧めします。本当に興味があれば、右上の検索バーで汎関数を検索して読んでみてください。よく分からなければ、忘れてしまいましょう。\n作用とラグランジアン 運動エネルギーからポテンシャルエネルギーを引いたものをラグランジアンと呼び、$L$で表します。\n$$ L=T-V $$\nラグランジアンは速度、位置、時間に影響を受けるため、位置を$y$とすると、以下のように表すこともできます。\n$$ L=L(y^{\\prime},\\ y,\\ t) $$\nラグランジアンという名前は、フランスの数学者ジョゼフ・ルイ・ラグランジュの名前から付けられました。ラグランジアンを時間に対して定積分したものを作用、またはアクションと呼び、一般に$J$で表します。\n$$ J = \\int_{t_{1}}^{t_{2}} L dt = \\int_{t_{1}}^{t_{2}} L(y^{\\prime},\\ y,\\ t) dt $$\nハミルトンの原理 1834年、イギリスの数学者ウィリアム・ローアン・ハミルトンが考案したもので、物体が実際に動く経路は作用が最小になるような原理です。これは証明可能な事実ではなく、$F=ma$のように自然界に存在する基本原理の一つと受け入れれば良いです。例えば、私たちが物体を高い場所から投げて落とすとき、物体がどのような経路で地面まで動くか知りたいとします。私たちが予想できる経路は数えきれないほど多いでしょうが、その中で実際に物体が動く経路には何か特別な点があるということです。それは、各経路に対するラグランジアンを時間に対して積分したとき、実際に動く経路に対するラグランジアンの積分値が最も小さいということです。つまり、作用が最小になる経路を見つければ、それが実際に物体が動く経路です。そのため、ハミルトンの原理は最小作用の原理とも呼ばれます。この原理を基に物体の運動を扱うことがラグランジュ力学Lagrangian mechanicsです。驚くべきことに、ラグランジュ力学はニュートン力学とは全く異なって見えますが、同じ結果を与えるということです。つまり、表現方法は異なるものの、本質は同じです。ニュートン力学はベクトル計算に基づいて物体の動きを扱い、ラグランジュ力学はスカラー（エネルギー）の計算によって力学を記述します。\n変分 簡単に言うと、上で詳しく説明した内容を数学的に整理したものです。まず、簡単な例として2次関数の最小値を見つける問題を考えてみましょう。\n上の図のような2次関数が与えられたとします。関数値の最小値は$1$で、関数値が最小になる場所は$x=3$です。最小値（極小値）を持つ点では傾きが$0$なので、微分したとき$0$であることが分かります。したがって、\n$$ \\dfrac{dy}{dx} \\bigg|_{x=3}=0 $$\nです。この内容を最小作用の原理にそのまま適用することになります。\n上の図に示されているように、物体が実際に運動する経路を$y(0, t)$としましょう。物体が運動できる任意の経路を上の図のように$y(\\alpha, t)=y(0,t)+\\alpha \\eta (t)$としましょう。参考までに$\\eta$はギリシャ文字のエタです。$\\alpha \\eta (t)$は実際の経路と比較したときの誤差と考えれば良いです。図と数式を見れば分かるように、誤差がないとき、つまり$\\alpha=0$のとき、可能な任意の経路$y(\\alpha, t)$は実際の経路になります。また、最小作用の原理は、可能なすべての経路に対する作用の中で、実際の経路に対する作用が最小の値であるという内容です。両方の内容を組み合わせて、上で挙げた例を適用すれば、作用を微分して$\\alpha=0$を代入したとき、その値が$0$であるという結果を得ます。\n$$ \\dfrac{\\partial J}{\\partial \\alpha}=\\dfrac{\\partial }{\\partial \\alpha} \\int_{t_{1}}^{t_{2}} L\\big( y^{\\prime}(\\alpha,t),\\ y(\\alpha,t),\\ t \\big) dt =0 $$\nこれを簡単に表記すると、以下のようになり、$\\delta J$を$J$の変分と呼びます。\n$$ \\delta J = 0 $$\nつまり、$\\delta=\\dfrac{\\partial }{\\partial \\alpha}$と理解すれば良いです。したがって、以下のような等式が成立します。\n$$ \\delta \\dot{y}=\\dfrac{\\partial }{\\partial \\alpha}\\frac{dy}{dt}=\\dfrac{d}{dt}\\frac{\\partial y}{\\partial \\alpha}=\\dfrac{d}{dt}\\delta y $$\n関連項目 オイラー-ラグランジュ方程式 Grant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p417-420\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1182,"permalink":"https://freshrimpsushi.github.io/jp/posts/1182/","tags":null,"title":"ラグランジュ力学とハミルトンの変分原理"},{"categories":"거리공간","contents":"定義 距離空間 $\\left( X, d \\right)$ について、$A \\subset X$ とする。\n$x \\in O \\subset A$ を満たす開集合 $O$ が存在する時、$x$ を $A$ の内点という。\n$A$ の内点の集合 $A^{\\circ}$ を $A$ の内部という。\n$A$ とその値域の和集合 $\\overline{A} : = A \\cup a '$ を $A$ の閉包という。\n$x \\in \\overline{A}$ であり、かつ $x \\in \\overline{X \\setminus A}$ の時、$x$ を $A$ の境界点という。\n$\\partial A : = \\overline{A} \\cap \\overline{X \\setminus A}$ を $A$ の境界という。\n説明 定義する必要はないかもしれないが、インテリアと対照的な $\\overline{A}$ の外側の集合をエクステリアと呼ぶ。\n開集合とこれらの概念は異なり定義されることもあるが、本質的には同じである。\n定義は慎重に読めば誰でも理解できるものであり、図を通じて早く理解しよう。\n$$ A $$\n与えられた集合が上のような時、これらの概念を考えてみよう。\n$$ A^{\\circ} $$\nインテリアは $A$ が含む $X$ の部分集合の中で最も大きな開集合である。\n$$ \\overline{A} $$\nクロージャーは $A$ を含む $X$ の部分集合の中で最も小さな閉集合である。\n$$ \\partial A $$\nバウンダリーはクロージャーからインテリアを引いた $X$ の部分集合と見ることができる。\nインテリアとクロージャーの区別はさほど難しくないが、バウンダリーは一見すると点線か実線かによって混乱するかもしれない。境界であれば、迷わずバウンダリーと考えればいい。\nこのような定義を通じて、以下の性質は事実上、開集合と閉集合の定義と見ることができる。\n性質: 開集合と閉集合 $A$ が距離空間 $X$ の部分集合とする。\n$A$ が開集合であることと $A = A^{\\circ}$ は等価である。\n$A$ が閉集合であることと $A = \\overline{A}$ は等価である。\nもちろん、これらの性質は証明可能だが、ただの事実として受け入れても問題ない。\n","id":383,"permalink":"https://freshrimpsushi.github.io/jp/posts/383/","tags":null,"title":"距離空間における内部閉包境界"},{"categories":"편미분방정식","contents":"ビルドアップ1 ハミルトニアン$H$が$Du$のみに依存するハミルトン-ヤコビ方程式の初期値問題を見てみよう。\n$$ \\begin{equation} \\left\\{ \\begin{aligned} u_{t} + H(Du)\u0026amp;=0 \u0026amp;\u0026amp; \\text{in } \\mathbb{R}^n \\times (0,\\infty) \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\mathbb{R}^n \\times \\left\\{ t=0 \\right\\} \\end{aligned} \\right. \\label{eq1} \\end{equation} $$\n一般に、ハミルトニアンは空間変数に依存して$H(Du, x)$のような形を取るが、ここでは$x$に対して影響を受けないとする。そして、ハミルトニアン$H\\in C^\\infty$に対して次のような仮定をする。\n$$ \\begin{cases} H \\mathrm{\\ is\\ convex} \\\\ \\lim \\limits_{|p|\\to \\infty} \\dfrac{H(p)}{|p|}=\\infty \\end{cases} $$\nそして$L=H^{\\ast}$とすると、ラグランジアン$L$も同様の特性を満たす。最後に、初期値$g : \\mathbb{R}^n \\to \\mathbb{R}$がリプシッツ連続であるとする。すなわち、\n$$ \\mathrm{Lip}(g):=\\sup \\limits_{x,y\\in \\mathbb{R}^n \\\\ x \\ne y} \\dfrac{ |g(x)-g(y)| }{|x-y|} \u0026lt; \\infty $$\nまた、与えられたハミルトン-ヤコビ方程式$\\eqref{eq1}$の特性方程式は次のようになる。\n$$ \\begin{align*} \\dot{\\mathbf{p}}(s) \u0026amp;= -D_{x}H \\big( \\mathbf{p}(s), \\mathbf{x}(s) \\big) \\\\ \\dot{z}(s) \u0026amp;= D_{p} H\\big( \\mathbf{p}(s),\\ \\mathbf{x}(s)\\big)\\cdot \\mathbf{p}(s) -H\\big( \\mathbf{p}(s), \\mathbf{x}(s)\\big) \\\\ \\dot{\\mathbf{x}}(s) \u0026amp;= D_{p}H\\big( \\mathbf{p}(s), \\mathbf{x}(s) \\big) \\end{align*} $$\nここで$H$は$x$に無関係であると仮定したので、再び書くと次のようになる。\n$$ \\begin{align*} \\dot{\\mathbf{p}} \u0026amp;= 0 \\\\ \\dot{z} \u0026amp;= D H( \\mathbf{p} )\\cdot \\mathbf{p} -H ( \\mathbf{p} ) \\\\ \\dot{\\mathbf{x}} \u0026amp;= DH ( \\mathbf{p}) \\end{align*} $$\nこのとき$t(s)=s, p(s)=Du(x(s), s), z(s)=u(x(s), s)$である。$p$に対する微分と$x$に対する微分を区別する必要がないため、$D$の下付き文字を省略した。オイラー-ラグランジュ方程式は固定された開始点と終了点に対して成立するため、与えられたハミルトン-ヤコビ方程式$\\eqref{eq1}$の解が存在する場合、以下のようなlocal in time solutionである。\n$$ u= u(x,t) \\in C^2\\big( \\mathbb{R}^n \\times [0,T]\\big) $$\n上記の特性方程式では、第一式と第三式はラグランジアン$L=H*$によって定義される作用の最小化問題から導かれるオイラー-ラグランジュ方程式を満たすハミルトン方程式である。\n$H$と$L$が$p$, $v\\in \\mathbb{R}^n$で微分可能であれば、以下の内容はすべて同等である。\n$$ \\begin{cases} p\\cdot v=L(v) + H(p) \\\\ p=DL(v) \\\\ v=DH(p) \\end{cases} $$\nこのとき$p=D_{v}L(v)$で定義されるため、上記の補題を使用すると次を得る。\n$$ \\begin{align*} \\dot{z}(s) \u0026amp;= DH(\\mathbf{p})\\cdot \\mathbf{p}-H(\\mathbf{p}) \\\\ \u0026amp;= \\mathbf{v} \\cdot \\mathbf{p}-H(\\mathbf{p}) \\\\ \u0026amp;= L(\\mathbf{v})+H(\\mathbf{p})-H(\\mathbf{p}) \\\\ \u0026amp;= L(\\mathbf{v}) = L\\big(\\dot{\\mathbf{x}}(s)\\big) \\end{align*} $$\nしたがって$z(t)$を求めると、次のようになる。\n$$ \\begin{align*} z(t) \u0026amp;= \\int_{0}^t \\dot{z}(s)dx +z(0) \\\\ \u0026amp;= \\int_{0}^tL \\big( \\dot{\\mathbf{x}}(s) \\big) + u\\big( \\mathbf{x}(0),\\ 0\\big) \\\\ \u0026amp;= \\int_{0}^t L\\big( \\dot{\\mathbf{x}}(s)\\big) ds +g\\big(\\mathbf{x}(0) \\big) \\end{align*} $$\nしかし、このとき上記の条件では$z(t)=u(x(t), t)$であったので、次を得る。\n$$ u(x,t)=\\int_{0}^t L\\big( \\dot{\\mathbf{x}}(s)\\big) ds +g\\big(\\mathbf{x}(0) \\big) \\quad (0 \\le t \u0026lt;T) $$\nこれはlocal in time smooth solutionであるため、global in time weak solutionを求めることができるかという問題が残る。再び作用の最小化問題に戻るが、オイラー-ラグランジュ方程式を導いた際と異なる点は終点のみを固定することである。\n固定された$x \\in \\mathbb{R}^n, t\u0026gt;0$が与えられたとする。そして、許容クラス$\\mathcal{A}$を次のようにする。\n$$ \\mathcal{A}=\\left\\{ \\mathbf{w}\\in C^1\\big( [0,t];\\mathbb{R}^n \\big)\\ :\\ \\mathbf{w}(t)=x \\right\\} $$\nそして、以下のような作用に対する最小化問題を考えてみよう。\n$$ \\mathbf{w}(\\cdot) \\in \\mathcal{A} \\mapsto \\int_{0}^t L\\big( \\dot{\\mathbf{w}}(s)\\big) ds + g(\\mathbf{w}(0)) $$\nもし上記の作用のミニマイザー$\\mathbf{x}(\\cdot)$が存在するならば、$\\mathbf{p}(s):=DL(\\dot{\\mathbf{x}}(s))$であり、オイラー-ラグランジュ方程式を満たし、したがってハミルトン方程式も満たす。したがって、上記のlocal in time solutionの場合と同様に、解は以下のように与えられるだろう。\n$$ u(x,t)=\\int_{0}^tL\\big( \\dot{\\mathbf{x}}(s)\\big)ds +g \\big( \\mathbf{x}(0) \\big) $$\n上記の内容をモチーフに、global in time weak solutionが存在する場合、次のように定義できる。\n$$ \\begin{equation} u(x,t):=\\inf \\limits_{\\mathbf{w} \\in \\mathcal{A}} \\left\\{ \\int_{0}^t L\\big( \\dot{\\mathbf{w}}(s) \\big)ds + g\\big( \\mathbf{w}(0) \\big) \\right\\} \\label{eq2} \\end{equation} $$\n定理 $x \\in \\mathbb{R}^n$であり、$t\u0026gt;0$とする。それならば、$\\eqref{eq2}$の最小化問題の解は次のように与えられる。\n$$ u(x,t) = \\min \\limits_{y \\in \\mathbb{R}^n} \\left\\{ tL\\left( \\dfrac{x-y}{t} \\right) +g(y) \\right\\} $$\nこれをホップ-ラックス公式Hopf-Lax formulaと呼ぶ。\n証明 まず$\\inf$に対して成立することを示し、その次に実際に$\\min$になることを示す順序で証明する。\nステップ 1.\n固定された任意の$y \\in \\mathbb{R}^n, t\\in \\mathbb{R}$がある。そして、$\\mathbf{w}$を次のように定義しよう。\n$$ \\mathbf{w}(s) :=y+\\frac{s}{t}(x-y) \\quad (0 \\le s \\le t) $$\nすると$\\mathbf{w}(0)=y$であり、$\\mathbf{w}(t)=x$である。それならば$\\mathbf{w}$は許容クラス$\\mathcal{A}$の要素である。\n$$ \\mathcal{A}= \\left\\{ \\mathbf{w}(\\cdot) \\ \\big| \\ \\mathbf{w}(0)=y,\\ \\mathbf{w}(t)=x\\right\\} $$\nそれならば～の定義により、次の不等式が成立する。\n$$ \\begin{align*} u(x,t) \u0026amp; \\le\u0026amp; \\int_{0}^t L \\left( \\frac{x-y}{t}\\right)ds + g(y) \\\\ \u0026amp;= tL\\left( \\frac{x-y}{t}\\right)+g(y) \\end{align*} $$\nこの不等式はすべての$y \\in \\mathbb{R}^n$に対して成立するので、次を得る。\n$$ u(x,t) \\le \\inf \\limits_{y \\in \\mathbb{R}^n} \\left(t L\\left(\\frac{x-y}{t} \\right) +g(y)\\right) $$\nステップ 2.\n▷eq41\n◁としよう。それならば$\\mathbf{w}(\\cdot) \\in C^1([0;t];\\mathbb{R}^n)$であり、$\\mathbf{w}(t)=x$である。\nイェンセンの不等式\n関数$f$が凸関数であると仮定しよう。それならば、以下の式が成立する。 $$ f \\left( -\\!\\!\\!\\!\\!\\! \\int_{U} u dx \\right) \\le -\\!\\!\\!\\!\\!\\! \\int_{U} f(u) dx $$\nそれならば、上記の補題により、次が成立する。\n$$ L \\left( \\frac{1}{t}\\int_{0}^t \\dot{\\mathbf{w}}(s) dx\\right) \\le \\dfrac{1}{t}\\int_{0}^t L \\big( \\dot{\\mathbf{w}(s)} \\big)ds $$\nそして、開始点を$y$としよう$\\mathbf{w}(0)=y$。それならば、上記の不等式は以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; L\\left( \\dfrac{1}{t} \\big( \\mathbf{w}(t)-\\mathbf{w}(0) \\big) \\right) \u0026amp;\\le \\dfrac{1}{t}\\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds\n\\\\ \\implies\u0026amp;\u0026amp; L\\left( \\dfrac{x-y}{t} \\right) \u0026amp;\\le \\dfrac{1}{t}\\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds \\end{align*} $$\n両辺に$t$を掛けて$g(y)$を足すと、次のようになる。\n$$ tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\le \\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds + g(y) $$\n右辺の$\\inf$が$u(x,t)$であるので、次のようになる。\n$$ tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\le u(x,t) $$\n最後に、両辺に$\\inf \\limits_{y\\in \\mathbb{R}^n}$を取ると、次を得る。\n$$ \\inf \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\right) \\le u(x,t) $$\nしたがって、ステップ 1. と ステップ 2. により、次が成立する。\n$$ \\begin{equation} u(x,t) = \\inf \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\right) \\label{eq3} \\end{equation} $$\nステップ 3.\n$\\left\\{y_{k} \\right\\}_{k=1}^\\infty$を$\\eqref{eq3}$の最小化シーケンスminimizing sequenceとしよう。それならば、次が成立する。\n$$ \\begin{equation} tL\\left( \\dfrac{x-y_{k}}{t} \\right) + g(y_{k}) \\to u(x,t)\\in [-\\infty, \\infty) \\quad \\mathrm{as}\\ k\\to \\infty \\label{eq4} \\end{equation} $$\nまず、$\\left\\{y_{k} \\right\\}$が有界でないと仮定しよう。これが矛盾であることを確認して、$\\left\\{ y_{k} \\right\\}$が有界であることを示す。仮定により、$|y_{k}| \\to \\infty$であり、$y_{k}=0$の$k$は多くても有限個である。したがって、$y_{k}\\ne 0$を満たすものだけを集めた部分列を再び$\\left\\{ y_{k} \\right\\}$としよう。次が成立する。\n$$ \\left| \\dfrac{x-y_{k}}{t} \\right| \\to \\infty $$\nそれならば、ラグランジアン$L$の性質により、次が成立する。\n$$ a_{k}:= \\dfrac{L\\left( \\dfrac{x-y_{k}}{t}\\right)}{\\left| \\dfrac{x-y_{k}}{t}\\right|} \\to \\infty $$\nしたがって、$L\\left( \\dfrac{x-y_{l}}{t}\\right) \\to \\infty$であり、ここに定数を掛けても同じ結果を得る。\n$$ \\begin{equation} tL\\left( \\dfrac{x-y_{k}}{t}\\right) \\to \\infty \\label{eq5} \\end{equation} $$\n$g$のリプシッツ条件を再び書くと、次のようになる。\n$$ \\dfrac{|g(x)-g(y_{k})|}{|x-y_{k}|} \\le \\mathrm{Lip}(g)=C \\quad \\forall \\ k \\in \\mathbb{N} $$\nしたがって、次を得る。\n$$ g(x) -g(y_{k}) \\le C|x-y_{k}| $$\n両辺に$\\eqref{eq5}$を足すと、次のようになる。\n$$ tL\\left( \\dfrac{x-y_{k}}{t}\\right)+ g(x) -g(y_{k}) \\le C|x-y_{k}|+ tL\\left( \\dfrac{x-y_{k}}{t}\\right) \\quad \\mathrm{for\\ large}\\ k $$\n上記の式を適切に移項すると、以下のようになる。\n$$ tL\\left( \\dfrac{x-y_{k}}{t}\\right)-C|x-y_{k}| + g(x) \\le tL\\left( \\dfrac{x-y_{k}}{t}\\right) + g(y_{k}) $$\n再び書くと、次のようになる。\n$$ a_{k}|x-y_{k}| -C|x-y_{k}| + g(x) =|x-y_{k}|(a_{k}-C)+g(x) \\le tL\\left( \\dfrac{x-y_{k}}{t}\\right) + g(y_{k}) $$\n$a_{k}\\to \\infty$であり、$|x-y_{k}| \\to \\infty$であるため、左辺が$\\infty$に発散し、右辺も発散する。したがって、$u(x,t)$の定義により、$u(x,t)\\to \\infty$である。これは$\\eqref{eq4}$に矛盾するため、$\\left\\{ y_{k} \\right\\}$は有界である。\n$\\left\\{ y_{k} \\right\\}$が有界であるため、$y_{k} \\to y_{0}$と仮定しよう。すると、次が成立する。\n$$ tL \\left( \\dfrac{x-y_{k}}{t} \\right)+g(y_{k}) \\to tL \\left( \\dfrac{x-y_{0}}{t}\\right)+g(y_{0}) =\\min\\limits_{y \\in \\mathbb{R}^n}\\left( tL \\left( \\dfrac{x-y}{t}\\right)+g(y) \\right) $$\nそれならば、$\\eqref{eq4}$により、次が成立する。\n$$ tL\\left( \\dfrac{x-y_{k}}{t} \\right) + g(y_{k}) \\to u(x,t)\\in [-\\infty, \\infty) \\quad \\mathrm{as}\\ k\\to \\infty $$\nしたがって、次を得る。\n$$ u(x,t) = \\min \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) +g(y) \\right) $$\n■\nローレンス・C・エヴァンス, 偏微分方程式 (第2版, 2010年), p122-124\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1174,"permalink":"https://freshrimpsushi.github.io/jp/posts/1174/","tags":null,"title":"ホップ・ラックス・フォーミュラ"},{"categories":"편미분방정식","contents":"説明1 $x$と$p$について、偏微分方程式の変数であることを強調する場合、通常のフォントで $x,p \\in \\mathbb{R}^{n}$ と表示し、$s$に関する関数であることを強調する場合、太字のフォントで $\\mathbf{x}, \\mathbf{p} \\in \\mathbb{R}^{n}$ と表示します。 特性方程式\n$$ \\begin{cases} \\dot{\\mathbf{p}} (s) = -D_{x}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)-D_{z}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)\\mathbf{p}(s) \\\\ \\dot{z}(s) = D_{p}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\cdot \\mathbf{p}(s) \\\\ \\dot{\\mathbf{x}}(s) = D_{p}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\end{cases} $$\n特性方程式を用いた非線形1階偏微分方程式の解法は、微分方程式がどのように与えられるかによって少しずつ異なります。これは与えられた微分方程式の線形性によって区別され、線形、準線形、完全非線形の場合に応じて解法が異なります。非線形性が高いほど難易度が高くなります。\n解法 同次線形 与えられた偏微分方程式が完全に線形であれば、最も簡単に解くことができます。特性方程式の $\\mathbf{p}(s)$ に関する条件は必要ないほど単純です。次の線形および同次の微分方程式を考えてみましょう。\n$$ \\begin{equation} F(Du, u, x) = \\mathbf{b}(x)\\cdot Du(x)+c(x)u(x)=0 \\quad (x\\in \\Omega \\subset \\mathbb{R}^{n}) \\label{eq1} \\end{equation} $$\nここで、各変数 $p, z, x$ を $p, z, x$とします。\n$$ \\begin{equation} F(p,\\ z,\\ x)=\\mathbf{b}(x)\\cdot p +c(x)z=b_{1}p_{1}+\\cdots +b_{n}p_{n}+cz = 0 \\label{eq2} \\end{equation} $$\n$D_{p}F$ を計算すると、次のようになります。\n$$ D_{p}F=(F_{p_{1}}, \\dots, F_{p_{n}})=(b_{1}, \\dots, b_{n})=\\mathbf{b}(x) $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s))\\cdot \\mathbf{p}(s) \\end{align*} $$\nこのとき、$(2)$により、$\\dot{z}(s)$ は次のようになります。\n$$ \\dot{z}(s) = -c(\\mathbf{x}(s))z $$\nしたがって、同次線形1階偏微分方程式の特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{\\mathbf{x}}(s)\u0026amp;=\\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= -c(\\mathbf{x}(s))z \\end{align*} \\right. $$\nこのとき、$\\mathbf{p}(s)$ に関する特性方程式は問題を解くのに必要ありませんことを例を通じて確認できます。\n例 次のような微分方程式が与えられたとします。\n$$ \\left\\{ \\begin{align*} x_{1} u_{x_{2}} - x_{2} u_{x_{1}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0, x_{2}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{ x_{1}\u0026gt;0, x_{2}=0 \\right\\}$ その場合、$(1)$ から $\\mathbf{b}=(-x_{2}, x_{1}), c=-1$ です。したがって、特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;= -x^{2} \\\\ \\dot{x}^{2} \u0026amp;=x^{1} \\\\ \\dot{z}\u0026amp;=z \\end{align*} \\right. $$\nこれは簡単な常微分方程式なので、次のように簡単に解けます。\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;=x^{0}\\cos s \\\\ x^{2}(s)\u0026amp;=x^{0} \\sin s \\\\ z(s)\u0026amp;=z^{0}e^s=g(x^{0})e^s \\end{align*} \\right. $$\nここで、 $x^{0}$ は $s=0$ のときに $x_{1}-$軸($\\Gamma$) を通過するように選ばれた定数です。その後、点 $(x_{1}, x_{2}) \\in \\Omega$ を固定しましょう。\n$$ (x_{1},\\ x_{2})=(x^{1}(s),\\ x^{2}(s)) = (x^{0} \\cos (s),\\ x^{0} \\sin (s)) $$\nすると、 $s\u0026gt;0, x^{0}\u0026gt;0$ の場合、次の結果が得られます。\n$$ x_{1}^{2} + x_{2}^{2} = (x^{0})^{2}\\cos^{2}(s) + (x^{0})^{2}\\sin^{2}(s) = (x^{0})^{2} \\implies x^{0}=({x_{1}}^{2}+{x_{2}}^{2})^{1/2} \\\\ \\dfrac{x_{2}}{x_{1}} = \\dfrac{x^{0}\\sin (s)}{x^{0} \\cos (s)} = \\tan (s) \\implies s=\\arctan \\left( \\frac{x_{2}}{x_{1}} \\right) $$\nしたがって、方程式の解は次のようになります。\n$$ \\begin{align*} u(x)\u0026amp;=u(x^{1}(s),\\ x^{2}(s)) \\\\ \u0026amp;= z(s) \\\\ \u0026amp;=g(x^{0})e^s \\\\ \u0026amp;= g(({x_{1}}^{2}+{x_{2}}^{2})^{1/2})e^{\\arctan \\left(\\frac{x_{2}}{x_{1}}\\right)} \\end{align*} $$\n■\n準線形 次に、与えられた微分方程式が最高微分項に関して線形である場合を考えます。今扱っているのは1階微分方程式なので、1階微分項に関して線形な場合です。\n$$ F(Du,\\ u,\\ x)=\\mathbf{b}(x,\\ u(x))\\cdot Du(x)+c(x,\\ u(x))=0 $$\nここで、各変数 $p, z, x$ を $p, z, x$ とします。\n$$ \\begin{equation} F(p, z, x)=\\mathbf{b}(x, z)\\cdot p + c(x, z)=b_{1}p_{1} + \\cdots + b_{n} p_{n} +c=0 \\label{eq3} \\end{equation} $$\n$D_{p}F$ を計算すると、次のようになります。\n$$ D_{p}F=(F_{p_{1}},\\ \\cdots,\\ F_{p_{n}})=(b_{1},\\ \\cdots,\\ b_{n})=\\mathbf{b}(x,\\ z) $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s)) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s))\\mathbf{p}(s)=-c(\\mathbf{x}(s),\\ z(s)) \\end{align*} $$\n$\\dot{z}$ の2つ目の等号は $(3)$ によって成立します。この場合も $\\mathbf{p}(s)$ に関する条件は問題を解くのに必要ありません。\n例 次のような微分方程式が与えられたとします。\n$$ \\left\\{ \\begin{align*} u_{x_{1}} + u_{x_{2}} \u0026amp;= u^{2} \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{2} \\gt 0 \\right\\}$ $\\Gamma=\\left\\{ x_{2} = 0 \\right\\}$ その場合、$(3)$ から $\\mathbf{b}=(1, 1)$, $c=-z^{2}$ です。したがって、特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;=1, \\dot{x}^{2}=1 \\\\ \\dot{z} \u0026amp;= z^{2} \\end{align*} \\right. $$\nこれはそれぞれ単純な常微分方程式なので、次のように解けます。\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;= x^{0}+s, x^{2}(s)=s \\\\ z(s)\u0026amp;=\\frac{z^{0}}{1-sz^{0}}=\\frac{g(x^{0})}{1-sg(x^{0})} \\end{align*} \\right. $$\nここで、$x^{0}$ は $s=0$ のときに $x_{2}-$軸($\\Gamma$) を通過するように選ばれた定数です。また、与えられた微分方程式は $u_{x_{1}}u_{x_{2}}=u$ ですから、$p_{1}^{0}p_{2}^{0}=z^{0}=(x^{0})^{2}$ となり、$p_{1}^{0}=\\frac{x^{0}}{2}$ です。これらを全て $(4)$ に代入すると、次の結果が得られます。\n$$ \\left\\{ \\begin{align*} p^{1}(s) \u0026amp;= \\frac{x^{0}}{2}e^s \\\\ p^{2}(s) \u0026amp;= 2x^{0}e^s \\\\ z(s) \u0026amp;= (x^{0})^{2}e^{2s} \\\\ x^{1}(s) \u0026amp;= 2x^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+\\frac{x^{0}}{2}(e^s-1) \\end{align*} \\right. $$\nその後、点\n$(x_{1}, x_{2}) \\in \\Omega$ を固定しましょう。\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=\\left( 2x^{0}(e^s -1), \\frac{x^{0}}{2}(e^s+1) \\right) $$\nすると、$s, x^{0}$ に関して次の結果が得られます。\n$$ x^{0}=\\frac{4x_{2}-x_{1}}{4},\\ \\ e^s=\\frac{x_{1}+4x_{2}}{4x_{2}-x_{1}} $$\nしたがって、方程式の解は次のようになります。\n$$ u(x)=u(x^{1}(s),\\ x^{2}(s))=z(s)=(x^{0})^{2}e^{2s}=\\dfrac{(x_{1}+4x_{2})^{2}}{16} $$\n■\n完全非線形 最後に、次のような微分方程式が与えられたとします。\n$$ \\begin{align*} u_{x_{1}}u_{x_{2}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u \u0026amp;= x_{2}^{2} \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{ x_{1}=0 \\right\\}$ $F$ の変数を $P, z, x$ とすると、次のようになります。\n$$ F(p, z, x)=p_{1}p_{2}-z $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{p}^{1} \u0026amp;= p^{1},\\quad \\dot{p}^{2}=p^{2} \\\\ \\dot{z} \u0026amp;= 2p^{1}p^{2} \\\\ \\dot{x}^{1} \u0026amp;= p^{2},\\quad \\dot{x}^{2}=p^{1} \\end{align*} $$\nまず、 $p$ に関する微分方程式を解くと、次のようになります。\n$$ p^{1}(s)=p_{1}^{0}e^s,\\ \\ p^{2}(s)=p_{2}^{0}e^s $$\nこのとき、 $p_{1}^{0}=p(0)$ および $p_{2}^{0}=p(0)$ です。したがって、$\\dot{z}(s)=2p_{1}^{0}p_{2}^{0}e^{2s}$ であるため、$z$ は次のようになります。\n$$ z(s)=p_{1}^{0}p_{2}^{0}e^{2s}+C $$\n$z(0)=z^{0}=p_{1}^{0}p_{2}^{0}+C$ なので、$C=z^{0}-p_{1}^{0}p_{2}^{0}$ です。したがって、次のようになります。\n$$ z(s)=z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) $$\n同様の方法で $x^{1}$ および $x^{2}$ も計算すると、次のようになります。\n$$ \\begin{equation} \\left\\{ \\begin{aligned} p^{1}(s) \u0026amp;= p_{1}^{0}e^s \\\\ p^{2}(s) \u0026amp;= p_{2}^{0}e^s \\\\ z(s) \u0026amp;= z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) \\\\ x^{1}(s) \u0026amp;= p_{2}^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+p_{1}^{0}(e^s-1) \\end{aligned} \\right. \\label{eq4} \\end{equation} $$\nこのとき、 $x^{0}$ は $s=0$ のときに $x_{1}-$軸($\\Gamma$) を通過するように選ばれた定数です。$u_{x_{2}}=p^{2}$ および境界条件により、$x_{2}^{0}=u(0, x^{0})=2x^{0}$ です。また、与えられた微分方程式は $u_{x_{1}}u_{x_{2}}=u$ であるため、$p_{1}^{0}p_{2}^{0}=z^{0}=(x^{0})^{2}$ であり、$p_{1}^{0}=\\frac{x^{0}}{2}$ です。これらを全て $(4)$ に代入すると、次の結果が得られます。\n$$ \\left\\{ \\begin{align*} p^{1}(s) \u0026amp;= \\frac{x^{0}}{2}e^s \\\\ p^{2}(s) \u0026amp;= 2x^{0}e^s \\\\ z(s) \u0026amp;= (x^{0})^{2}e^{2s} \\\\ x^{1}(s) \u0026amp;= 2x^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+\\frac{x^{0}}{2}(e^s-1) \\end{align*} \\right. $$\nその後、点 $(x_{1}, x_{2})\\in \\Omega$ を固定しましょう。\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=\\left( 2x^{0}(e^s -1), \\frac{x^{0}}{2}(e^s+1) \\right) $$\nすると、 $s, x^{0}$ に関して次の結果が得られます。\n$$ x^{0}=\\frac{4x_{2}-x_{1}}{4},\\ \\ e^s=\\frac{x_{1}+4x_{2}}{4x_{2}-x_{1}} $$\nしたがって、方程式の解は次のようになります。\n$$ u(x)=u(x^{1}(s),\\ x^{2}(s))=z(s)=(x^{0})^{2}e^{2s}=\\dfrac{(x_{1}+4x_{2})^{2}}{16} $$\n■\nLawrence C. Evans, Partial Differential Equations (第2版, 2010年), p99-102\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1074,"permalink":"https://freshrimpsushi.github.io/jp/posts/1074/","tags":null,"title":"特性方程式を利用した非線形1系偏微分方程式の解法。"},{"categories":"다변수벡터해석","contents":"定義 集合 $D$ を $n$次元のユークリッド空間の部分集合 $D\\subset \\mathbb{R}^{n}$ とする。\n$D$ を定義域とする関数を多変数関数function of several variablesと呼ぶ。 $f : D \\to \\mathbb{R}$ をスカラー関数scalar functionと呼ぶ。 スカラー関数 $f_{1} , \\cdots , f_{m} : D \\to \\mathbb{R}$ に対して次のように定義された $\\mathbb{f} : D \\to \\mathbb{R}^{m}$ をベクトル値関数vector-valued functionと呼ぶ。 $$ \\mathbb{f} ( x_{1} , \\cdots , x_{n} ) : = \\begin{bmatrix} f_{1} ( x_{1} , \\cdots , x_{n} ) \\\\ \\vdots \\\\ f_{m} ( x_{1} , \\cdots , x_{n} ) \\end{bmatrix} $$ 説明 多変数関数 多変数関数を意味する英語には、function of several variables, multivariable function, multivariate function 等がある。\n多変数関数という表現は特に微分積分学を含む解析学で使われる。もともとスカラー関数であれベクトル値関数であれ、ただの関数に過ぎないが、その値域を簡単に区別するために使われる言葉だ。線型代数学の観点から見れば、ベクトル値関数が $m=1$ であればスカラー関数になると言えるので、概念的な差は全くないと言える。\nスカラー関数 スカラー関数の例として、$ F ( m , a ) := ma$ を考えることができる。$m$ が質量であろうと$a$ が加速度であろうと、数学者の目には $(m , a) \\in \\left( [0,\\infty) \\times \\mathbb{R} \\right) \\subset \\mathbb{R}^2$ のような $2$次元ベクトルに見えるべきだ。$ma$ は単に2つの実数 $m$ と $a$ の積であり、$ma \\in \\mathbb{R}$ であるため、スカラー関数の条件をよく満たしている。一方、ベクトル解析学では、与えられた空間上の全ての点に対してスカラー値が1つずつ対応している点で、スカラー場Scalar Fieldとも呼ばれる。\nベクトル値関数 ベクトル値関数の例として、 $$ \\mathbb{q} ( m , v , a ) : = \\begin{bmatrix} ma \\\\ mv \\\\ {{1} \\over {2}} m v^2 \\end{bmatrix} $$ を考えることができる。物理学者の目には、最初の成分から順に力、運動量、運動エネルギーだと思うかもしれないが、ベクトル値関数として考えれば、単に $\\mathbb{q} : D \\to \\mathbb{R}^3$ に過ぎない。一方、ベクトル解析学では、与えられた空間上の全ての点に対してベクトルが1つずつ対応している点で、ベクトル場Vector Fieldとも呼ばれる。\n","id":970,"permalink":"https://freshrimpsushi.github.io/jp/posts/970/","tags":null,"title":"スカラー関数とベクトル値関数"},{"categories":"그래프이론","contents":"定義1 頂点とそれらを結ぶ線から成る集合をグラフまたはネットワークと呼ぶ。頂点の集合を$V$、線の集合を$E$としよう。 $V(G) := V$の要素を$G$の ヴァーテックスVertexまたは ノードNodeと呼ぶ。 $E(G) := E$の要素を$G$の エッジEdgeまたは リンクLinkと呼ぶ。 自分自身に繋がるエッジをループLoopと呼ぶ。 二つのヴァーテックスがエッジで繋がっている場合、隣接しているAdjacentと言う。 エッジに向きがあるグラフを有向グラフDigraphと言う。 有限なヴァーテックスを持ち、二つのヴァーテックスを結ぶエッジが一つだけで、ループが存在せず、有向グラフでないグラフを単純グラフSimple Graphと言う。 説明 必ずしもそうではないが、同じ概念であっても純粋数学ではグラフという言葉を好んで用い、応用数学ではネットワークという言葉を好んで用いる傾向がある。ただし、どちらかと言えば、同義語があり、それぞれの分野ではかなりの影響力を持っているので、混在して使うことはあまりない。\n一般的に言うグラフとは、上の図のように自由な形をしている。 黒い丸はそれぞれのヴァーテックスを意味し、位置の概念は持たない。純粋なグラフ理論では、グラフのオーダーOrderは通常、このヴァーテックス集合の基数 $n = |V(G)|$を指す。上のグラフのオーダーは$5$である。 ヴァーテックスを結ぶエッジも同様に、その関係のみを表すもので、形や長さの概念はない。ちなみに、エッジは通常、韓国人が直感的に思うように[エッヂ]ではなく[エッジ]と発音されるのが正しい。純粋なグラフ理論では、グラフのサイズSizeは通常、このエッジ集合の基数$m = |E(G)|$を指す。上のグラフのサイズは$8$である。しかし、応用ネットワーク理論では、サイズは単にグラフのオーダー$|V(G)|$を呼ぶことが多い。これは分野による文脈で区別する必要がある。 左上のヴァーテックスは自分自身に繋がるエッジを持ち、このためにループと呼ばれる。 二つのヴァーテックスが隣接しているとは、エッジで繋がっているということを意味し、再び、その関係のみが重要であり、目に見える距離は重要ではない。二つのヴァーテックス$u, v$が隣接している場合、$u \\sim v$のように表され、グラフ$G$でヴァーテックス$v \\in V(G)$に隣接するヴァーテックスを集めた集合を$v$のネイバーフッド$N_{G} (v)$のように表現することもある。 有向グラフとは、上の図のようにエッジに方向性があるグラフを言う。有向グラフでは、エッジはアークArcとも呼ばれる。ヴァーテックス$u$から$v$へ入るエッジは$u \\to v$と表記され、$u$をテイルtail、$v$をヘッドheadと呼ぶ。 単純グラフという言葉は難しそうだが、簡単に言えば、ループやマルチエッジ、ディレクションなどがなく、上の図のようなきれいなグラフを指す。一般に、グラフ理論と言えば、このような単純な形に興味を持つのが普通である。 難しい定義 これらの定義は、グラフの概念を簡単に説明するが、厳密さにはいくらか問題がある。したがって、以下のより複雑な定義を紹介する。概念は文字通り上で簡単に定義されたものと同じなので、数学的な表現に慣れていれば理解するのに大きな困難はないだろう。\n集合 $V \\ne \\emptyset$ と 二項関係 $\\sim \\subset V^2$ について$G := \\left( V, \\sim \\right)$をグラフまたはネットワークと呼ぶ。 $V(G) := V$の要素を$G$のヴァーテックスまたは ノードと呼ぶ。 $E(G) := \\sim$の要素を$G$のエッジまたは リンクと呼ぶ。 $v \\in V(G)$に対して$(v,v) \\in E(G)$をループと呼ぶ。 $v_{1} , v_{2} \\in V(G)$について$(v_{1} , v_{2} ) \\in E(G)$であれば$v_{1}$と$v_{2}$が隣接していると言う。 $\\sim$が対称関係でなければ有向グラフDigraphと呼ぶ。 有限集合$V$に対し、対称関係$\\sim \\subset \\left\\{ (v_{1} , v_{2} ) \\in V^2 \\mid v_{1} \\ne v_{2} \\right\\}$をエッジとして、二つのヴァーテックスを結ぶエッジが一つだけのグラフを単純グラフと呼ぶ。 Wilson. (1970). Introduction to Graph Theory: p8~9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":966,"permalink":"https://freshrimpsushi.github.io/jp/posts/966/","tags":null,"title":"数学におけるグラフとネットワーク"},{"categories":"상미분방정식","contents":"定義 次の微分方程式をチェビシェフChebyshev 微分方程式という。\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -x\\dfrac{dy}{dx}+n^2 y=0 $$\n説明 係数に独立変数 $x$が含まれる形式であり、解がべき級数の形であると仮定すると、解くことができる。チェビシェフ方程式の解をチェビシェフ多項式と言い、解は一般的に$T_{n}(x)$と表される。\n解法 $$ \\begin{equation} (1-x^2)y^{\\prime \\prime} -xy^{\\prime}+\\lambda^2 y=0 \\label{1} \\end{equation} $$\n上で示したチェビシェフ微分方程式の解を次のように仮定しよう。\n$$ y=a_{0}+a_{1}(x-x_{0})+a_2(x-x_{0})^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}(x-x_{0})^n $$\nこのとき$x=0$であるとき$y^{\\prime \\prime}$の係数が$(1-x^2)|_{x=0}=1\\ne 0$であるので、$x_{0}=0$としよう。すると\n$$ \\begin{equation} y=a_{0}+a_{1}x+a_2x^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}x^n \\label{2} \\end{equation} $$\nべき級数解として解法を始めるが、解法の最後に実際には$y$の項が有限であることが分かる。今$\\eqref{1}$に代入するために、$y^{\\prime}$と$y^{\\prime \\prime}$を求めよう。\n$$ y^{\\prime}=a_{1}+2a_2x+3a_{3}x^2+\\cdots=\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1} $$\n$$ y^{\\prime \\prime}=2a_2+3\\cdot 2a_{3}x+4\\cdot 3 a_{4}x^2 +\\cdots = \\sum \\limits_{n=2} n(n-1)a_{n}x^{n-2} $$\n$\\eqref{1}$に$y, y^{\\prime}, y^{\\prime \\prime}$を代入すると次のようになる。\n$$ (1-x^2)\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n一番目の項の係数$(1-x^2)$の括弧を外して整理すると\n$$ \\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x^2\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n = 0 $$\n$$ \\implies \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n = 0 $$\nここでのポイントは$x$の次数を合わせることである。残りはすべて$x^n$として表されるが、最初の級数だけが$x^{n-2}$で表されているので、$n$の代わりに$n+2$を代入すると\n$$ \\sum \\limits_{n=0} ^\\infty (n+2)(n+1)a_{n+2}x^{n} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n二番目の級数が$x^2$項から始まるので、残りの級数から$n=0,1$の項を取り除いて、定数項は定数項同士、一次項は一次項同士をまとめると\n$$ \\left[ 2\\cdot 1 a_2+\\lambda^2 a_{0} \\right]+\\left[ 3\\cdot 2 a_{3}-a_{1}+\\lambda^2a_{1} \\right]x \\\\ + \\sum \\limits_{n=2}^\\infty \\left[ (n+2)(n+1)a_{n+2}-n(n-1)a_{n}-na_{n}+\\lambda^2a_{n} \\right] x^n=0 $$\n上の式が成り立つためには、すべての係数が$0$でなければならない。\n$$ 2\\cdot 1 a_2+\\lambda^2 a_{0} = 0 $$\n$$ 3\\cdot 2 a_{3}-a_{1}+\\lambda^2a_{1} =0 $$\n$$ (n+2)(n+1)a_{n+2}-n(n-1)a_{n}-na_{n}+\\lambda^2a_{n}=0 $$\nそれぞれを整理すると\n$$ \\begin{align} a_2 \u0026amp;= -\\dfrac{\\lambda^2}{2 \\cdot 1}a_{0} \\label{3} \\\\ a_{3} \u0026amp;=-\\dfrac{\\lambda^2-1^2}{3\\cdot 2} a_{1} \\label{4} \\\\ a_{n+2} \u0026amp;= -\\dfrac{\\lambda^2-n^2}{(n+2)(n+1)}a_{n} \\label{5} \\end{align} $$\n漸化式$\\eqref{5}$を得たので、$a_{0}$と$a_{1}$の値さえ分かれば、すべての係数が分かる。$\\eqref{3}, \\eqref{5}$から偶数次の項の係数を求めると\n$$ \\begin{align*} a_{4} \u0026amp;= -\\dfrac{\\lambda^2-2^2}{4\\cdot 3}a_2=\\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}a_{0} \\\\ a_{6} \u0026amp;= -\\dfrac{\\lambda^2-4^2}{6\\cdot 5}a_{4}= -\\dfrac{\\lambda^2(\\lambda^2-2^2)(\\lambda^2-4^2)}{6!}a_{0} \\\\ \u0026amp;\\vdots \\end{align*} $$\nここで$n=2m (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m}=(-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!}a_{0} $$\n同様に$\\eqref{4}, \\eqref{5}$から奇数次の項の係数を求めると\n$$ \\begin{align*} a_{5} \u0026amp;= -\\dfrac{\\lambda^2-3^2}{5\\cdot 4}a_{3}=\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}a_{1} \\\\ a_{7} \u0026amp;= -\\dfrac{\\lambda^2-5^2}{7\\cdot 6 }a_{5}=-\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)(\\lambda^2-5^2)}{7!}a_{1} \\\\ \u0026amp;\\vdots \\end{align*} $$\nここで$n=2m+1 (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m+1}=(-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!}a_{1} $$\nこのように求めた係数を$\\eqref{2}$に代入して解を求めると\n$$ \\begin{align*} y = \u0026amp;a_{0}+a_{1}x -\\dfrac{\\lambda^2}{2!}a_{0}x^2-\\dfrac{\\lambda^2-1^2}{3!} a_{1}x^3 + \\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}a_{0}x^4 \\\\ \u0026amp;+\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}a_{1}x^5+ \\cdots +(-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!}a_{0}x^{2m} \\\\ \u0026amp;+(-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!}a_{1}x^{2m+1}+\\cdots\\quad(m=1,2,3,\\cdots) \\end{align*} $$\nこのとき偶数次の項は$a_{0}$で、奇数次の項は$a_{1}$でまとめると\n$$ \\begin{align*} y\u0026amp;=a_{0}\\left[1-\\dfrac{\\lambda^2}{2!}x^2+\\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}x^4+\\sum \\limits_{m=3}^\\infty (-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!} x^{2m} + \\cdots \\right] \\\\ \u0026amp; + a_{1}\\left[x-\\dfrac{\\lambda^2-1^2}{3!}x^3+\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}x^5+\\sum \\limits_{m=3}^\\infty (-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!} x^{2m+1} + \\cdots\\right] \\end{align*} $$\n最初の括弧を$y_{0}$、二番目の括弧を$y_{1}$とすると、チェビシェフ方程式の一般解は次のようになる。\n$$ y=a_{0}y_{0}+a_{1}y_{1} $$\n二つの級数$y_{0}$と$y_{1}$は比率判定法により$|x|\u0026lt;1$の範囲で収束することが分かる。$\\eqref{5}$により$\\dfrac{a_{n+2}}{a_{n}}=\\dfrac{n^2-\\lambda^2}{(n+2)(n+1)}=\\dfrac{n^2-\\lambda^2}{n^2+3n+2}$であるため比率判定法を使うと\n$$ \\lim \\limits_{n \\rightarrow \\infty} \\dfrac{n^2-\\lambda^2}{n^2+3n+2}x^2=x^2\u0026lt;1 $$\n$$ \\implies -1\u0026lt;x\u0026lt;1 $$\nしかし、多くの問題では$x=\\cos \\theta$、$\\lambda$は非負の整数の形で表され、すべての$\\theta$に対して収束する解を求めることが目標である。すなわち、$x=\\pm 1$でも収束する解を見つけることが目標である。幸いにも$\\lambda$が整数の場合、求める解が存在するが、このとき$\\lambda$の値によって必ず$y_{0}, y_{1}$のどちらか一方の解のみが存在する。$\\lambda$が$0$または偶数の場合は$y_{1}$が発散し、$y_{0}$は偶数次の項だけを持つ有限項の多項式となる。$\\lambda$が奇数の場合は$y_{0}$が発散し、$y_{1}$は奇数次の項だけを持つ有限項の多項式となる。表で整理すると以下のようになる。\n$\\lambda$の値 $y_{0}$ $y_{1}$ 方程式の解 $0$または偶数 有限項の多項式 発散 $y=a_{0}y_{0}$ 奇数 発散 有限項の多項式 $y=a_{1}y_{1}$ ケース 1. $\\lambda$が$0$または偶数の場合\n$\\lambda=0$の場合、2次項から$\\lambda^2$を因数として持っており、すべて$0$になるため$y_{0}=1$\n$\\lambda=2$の場合、4次項から$(\\lambda^2-2^2)$を因数として持っており、すべて$0$になるため$y_{0}=1-x^2$\n$\\lambda=4$の場合、6次項から$(\\lambda^2-4^2)$を因数として持っており、すべて$0$になるため$y_{0}=1-8x^2+8x^4$\nそして$\\lambda=0$の場合、$x=1$の$y_{1}=1+\\frac{1}{3!}+\\frac{1\\cdot3^2}{5!}+\\cdots$は発散する。他の偶数の場合も同じである。したがって、$\\lambda$が$0$または偶数の場合は、解が偶数次の項のみを持つ有限項の多項式となる。つまり、級数$y_{0}$の特定の項までのみ残る形の解を得る。$\\lambda$が奇数の場合は、反対の結果を得る。\nケース 2. $\\lambda$が奇数の場合\n$\\lambda=1$の場合、3次項から$(\\lambda^2-1^2)$を因数として持っており、すべて$0$になるため$y_{1}=x$\n$\\lambda=3$の場合、5次項から$(\\lambda^2-3^2)$を因数として持っており、すべて$0$になるため$y_{1}=-3x+4x^3$\n$\\lambda=5$の場合、7次項から$(\\lambda^2-5^2)$を因数として持っており、すべて$0$になるため$y_{1}=5x-20x^3+16x^5$\n$\\lambda=1$の場合、$x^2=1$の$y_{0}$は発散する。他の奇数の場合も同じである。したがって、$\\lambda$が奇数の場合は、解が奇数次の項のみを持つ有限項の多項式となる。つまり、級数$y_{1}$の特定の項までのみ残る形の解を得る。\nそして$\\lambda$が負の場合は、$\\lambda$が正の場合と同じであることが、$y_{0}$と$y_{1}$を見ると分かる。例えば、$\\lambda=2$の場合と$\\lambda=-2$の場合が同じであり、$\\lambda=1$の場合と$\\lambda=-1$の場合が同じである。したがって、$\\lambda$は非負の整数の範囲で考えればよい。$a_{0}$と$a_{1}$の値をうまく選択して、$x=1$のときの解が$y(x)=1$になるようにすれば、これをチェビシェフ多項式Chebyshev polynomialと言い、通常$T_{n}(x)$と表記される。初めのいくつかのチェビシェフ多項式は以下のようである。\n$$ \\begin{align*} T_{0}(x) \u0026amp;= 1 \\\\ T_{1}(x) \u0026amp;= x \\\\ T_2(x) \u0026amp;= 2x^2-1 \\\\ T_{3}(x) \u0026amp;= 4x^3-3x \\\\ T_{4}(x) \u0026amp;= 8x^4-8x^2+1 \\\\ T_{5}(x) \u0026amp;= 16x^5-20x^3+5x \\\\ \\vdots \u0026amp; \\end{align*} $$\n関連項目 チェビシェフ微分方程式とチェビシェフ多項式 ","id":955,"permalink":"https://freshrimpsushi.github.io/jp/posts/955/","tags":null,"title":"チェビシェフ微分方程式の直列解法"},{"categories":"푸리에해석","contents":"定義 $2L$-周期関数 $f$に対して次のような級数を $f$のフーリエ級数Fourier series of $f$と定義する。\n$$ \\begin{align*} \\lim \\limits_{N \\rightarrow \\infty} S^{f}_{N}(t) \u0026amp;= \\lim \\limits_{N \\to \\infty}\\left[ \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right] \\\\ \u0026amp;= \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\end{align*} $$\nこの時、各々の係数 $a_{0}, a_{n}, b_{n}$を フーリエ係数Fourier coefficientと言い、値は次のようになる。\n$$ \\begin{align*} \\\\ a_{0} \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin \\dfrac{n\\pi t}{L}dt \\end{align*} $$\n説明 フーリエ級数は任意の関数を三角関数の級数展開で表現するもので、フランスの数学者 ジョセフ・フーリエJoseph Fourierが熱方程式を解くために考案したことでよく知られている。任意の関数と表現した理由は、ある区間 $(a,b)$で定義された関数があれば、これをCtrl+C, Ctrl+Vして $(b-a)$-周期関数にすることができるからである。\n核心原理は互いに直交する三角関数たちの線形結合で表現されることであり、3次元ベクトルにたとえると、$(4,-1,7)$を次のように分けることに似ている。\n$$ (4,-1,7) = a_{1}\\hat{\\mathbf{e}}_{1} + a_{2}\\hat{\\mathbf{e}}_{1} + a_{3}\\hat{\\mathbf{e}}_{1} $$\n実際に、$f$のフーリエ級数は$f$との誤差が非常に小さく、条件がよく満たされれば $f$に点ごとに収束する。\n$$ f(t) = \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L}t + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) $$\n導出 回帰分析1 パート 1\n関数 $f(t)$を $1, \\cos \\dfrac{\\pi t}{L}, \\cos\\dfrac{2\\pi t}{L}, \\cdots, \\sin \\dfrac{\\pi t}{L}, \\sin \\dfrac{2\\pi t}{L}, \\cdots $たちの線形結合で表現することが目的である。したがって、$S^{f}_{N}(t)=\\dfrac{1}{2}{\\alpha_{0}}+\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right)$とした時、$f(t)$は以下のように表現できる。\n$$ f(t)=S^{f}_{N}(t)+e_{N}(t) $$\n$e_{N}(t)$は $f(t)$と近似式 $S_{N}^{f} (t)$の差である。この差が最も小さくなる$S_{N}^{f}(t)$を見つければ、それが$f(t)$との差が最も小さい級数展開になる。$e_{N}$を 平均二乗誤差mean square error2としよう。\n$$ e_{N}=\\dfrac{1}{2L}\\int_{-L}^{L} [e_{N}(t) ]^{2}dt=\\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N} (t) \\right]^{2} dt $$\nパート 2\n$$ \\begin{align*} e_{N} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N}(t) \\right]^{2} dt \\\\ \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\end{align*} $$\n平均二乗誤差 $e_{N}$が最小になる時の係数 $\\alpha_0,\\ \\alpha_{n},\\ \\beta_{n}$をそれぞれ $a_0$, $a_{n}$, $b_{n}$としよう。$e_{N}$を最小化する条件は次のようであり、正規方程式normal equationと言われる。\n$$ \\dfrac{\\partial e_{N}}{\\partial \\alpha_{0}}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\alpha_{n}}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\beta_{n}}=0\\quad (m=1,\\ 2,\\ \\cdots,\\ N) $$\nそれでは、$a_{0}$, $a_{n}$, $b_{n}$は以下のように求めることができる。\nパート 2.1 $a_{0}$\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_{0}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_{0}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{-1}{2} \\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_ {N} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac {n\\pi t}{L} \\right) \\right] dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_{0} dt +\\dfrac{1}{2L}\\int_{-L}^{L} \\sum \\limits_ {n=1}^{N}\\left( \\alpha_{n}\\cos \\dfrac{n\\pi t}{L}+\\beta_{n} \\sin \\dfrac{n \\pi t}{L} \\right) dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_{0} dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt +\\dfrac{1}{2}\\alpha_{0} \\\\ \u0026amp;= 0 \\end{align*} $$\n4番目の等号は三角関数の1周期積分が0であるため成り立つ。したがって\n$$ a_{0} = \\dfrac{1}{L} \\int_{-L}^{L}f(t)dt $$\nパート 2.2 $a_{n}$\nある$m \\in \\left\\{ 1,2,\\dots,N \\right\\}$を一つ選ぼう。\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\cos \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_{0}\\cos\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad + \\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\cos\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\alpha_{m} \\int_{-L}^{L}\\cos\\dfrac{m\\pi t}{L}\\cos\\dfrac{m\\pi t} {L} dt\\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\alpha_{m} \\\\ \u0026amp;= 0 \\end{align*} $$\n4番目、5番目の等号は三角関数の直交性によって成り立つ。したがって\n$$ a_{n}= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\quad (n=1, 2, \\cdots, N) $$\nパート 2.3 $b_{n}$\nある$m \\in \\left\\{ 1,2,\\dots,N \\right\\}$を一つ選ぼう。\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\beta_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\beta_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\sin \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_{0}\\sin\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad +\\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\sin\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\beta_{m} \\int_{-L}^{L}\\sin\\dfrac{m\\pi t}{L}\\sin\\dfrac{m\\pi t} {L} dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\beta_{m} \\\\ \u0026amp;=0 \\end{align*} $$\n4番目、5番目の等号は三角関数の直交性によって成り立つ。したがって\n$$ b_{n}=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\quad (n=1, 2, \\cdots, N) $$\nパート 3 ここで得られた$a_0$, $a_{n}$, $b_{n}$で$f(t)$を表現すると同じになる。\n$$ \\begin{align*} f(t) \u0026amp;= S^{f}_{N}(t)+e_{N}(t) \\\\[1em] \\text{where } S^{f}_{N}(t) \u0026amp;= \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t} {L} \\right) \\\\ a_{0} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\end{align*} $$\n$N$に対して極限をとれば\n$$ \\lim \\limits_{N \\rightarrow \\infty} S_{N}^{f} (t)=\\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n} \\sin\\dfrac{n\\pi t}{L} \\right) $$\n上の級数を $f$のフーリエ級数 と呼び、$a_0$, $a_{n}$, $b_{n}$を $f$のフーリエ係数 という。\n■\nチェ・ビョンソン, フーリエ解析入門 (2002), p51-53\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRSSが平均二乗誤差である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":929,"permalink":"https://freshrimpsushi.github.io/jp/posts/929/","tags":null,"title":"フーリエ級数の導出"},{"categories":"추상대수","contents":"まとめ 1 素数 $p$ と 自然数 $n$ に対して、基数が $p^{n}$ の 有限 体有限体を $p^{n}$ 次のガロア体ガロア体と定義し、$\\text{GF} \\left( p^{n} \\right)$ のように表す。有限体はガロア体だけであり、与えられた $p$ と $n$ に対してガロア体は唯一に存在する。\nここで「唯一である」とは、異なる体であっても同型写像が存在し、実質的に同一の体であるという意味である。 説明 ガウスが最初に有限体の概念を思いついたときは、その実体を信じる人はいなかったが、現在では有限体が存在するだけでなく、その具体的な形まで明らかにされている。すべての有限体の形が解明されたので、無駄な研究をする必要はない。\n例えば、元が $10$ 個の体が存在するかどうかは、考える必要さえなく、$\\text{GF} \\left( p \\right) = \\mathbb{Z}_{p}$ は整数環であるため、すでに多くのことがわかっている。さらに知りたいことがあれば、抽象的な定義に固執する必要はなく、$\\mathbb{Z}_{p}$ を通じてアプローチすればよく、その逆もまた然りである。\n証明 2 パート1. すべての有限体はガロア体である。\n体 $F$ の有限拡大体を $E$ とし、$F$ 上の 次数を $n := \\left[ E : F \\right]$ とする。\n$| F | = q$ とすると、$E$ は $F$ の $n$ 次のベクトル空間であるため、$|E| = q^{n}$ である。体は単位元を持つが、標数が $0$ であれば $\\mathbb{Z}$ と同型の部分環が存在して無限体となる。したがって、有限体の標数は有限の自然数でなければならない。有限体 $E$ の標数を $p \\ne 0$ とすると、$E$ は単位元 $1$ を持つため、$p \\cdot 1 = 0$ でなければならない。体は整域であるため、 $$ p \\cdot 1 = ( p_{1} \\cdot 1 ) ( p_{2} \\cdot 1 ) = 0 $$ を満たす $p_{1}, p_{2} \\in \\mathbb{Z}$ が存在することはなく、$p$ は必ず素数である。したがって、$E$ は素体 $\\mathbb{Z}_{p}$ と同型の部分体を持ち、$\\left| \\mathbb{Z}_{p} \\right| = p$ であるため、$|E| = p^{n}$ である。\nパート2. ガロア体の存在\nパート2-1. $x^{p^{n}} - x$ のゼロ\n$\\left( x^{p^{n}} - x \\right)$ の標数が $p$ の体 $F$ の代数的閉包 $\\overline{F}$ を考える。\n$\\overline{F}$ は代数的に閉じているため、$\\left( x^{p^{n}} - x \\right) \\in \\overline{F} [ x ]$ は $1$ 次の項で因数分解される。すぐにわかる事実は $$ x^{p^{n}} - x = ( x - 0 ) \\left( x^{p^{n}-1} - 1 \\right) $$ であるため、$0$ は $\\left( x^{p^{n}} - x \\right)$ のゼロになる。$f(x) := x^{p^{n}-1} - 1$ の別のゼロ $\\alpha \\ne 0$ を考えると、 $f \\left( \\alpha \\right) = 0$ であるため、 $$ 0 = f \\left( \\alpha \\right) = \\alpha^{p^{n} - 1} - 1 \\implies \\alpha^{p^{n} - 1} = 1 $$ となり、これにより $f(x)$ を $\\left( x - \\alpha \\right)$ の積として表すと、 $$ \\begin{align*} f(x) =\u0026amp; x^{p^{n}-1} - 1 \\\\ =\u0026amp; x^{p^{n}-1} - \\alpha^{p^{n}-1} \\\\ =\u0026amp; (x - \\alpha ) \\left( x^{p^{n} - 2 } + \\alpha x^{p^{n} - 3 } + \\cdots + \\alpha^{p^{n} - 3 } x + \\alpha^{p^{n} - 2} \\right) \\end{align*} $$ である。一方、便宜上第二の因数を、 $$ g(x) := \\left( x^{p^{n} - 2 } + \\alpha x^{p^{n} - 3 } + \\cdots + \\alpha^{p^{n} - 3 } x + \\alpha^{p^{n} - 2} \\right) $$ とすると、$g(x)$ の項の数は $p^{n} - 1$ 個である。したがって、$x = \\alpha$ を代入してみると、 $$ g ( \\alpha ) = \\alpha^{p^{n} - 2} \\cdot \\left( p^{n} - 1 \\right) = {{\\alpha^{p^{n} - 1}} \\over { \\alpha }} \\left( p^{n} - 1 \\right) $$ を得る。上記で $\\alpha \\ne 0$ は $f(x)$ のゼロであるため、$\\alpha^{p^{n}-1} - 1 = 0$ としたし、標数を素数 $p$ と仮定したので、 $$ g ( \\alpha ) = {{1} \\over { \\alpha }} \\cdot (0 - 1) = - {{1} \\over { \\alpha }} \\ne 0 $$ である。したがって、$\\alpha$ は $f(x) = 0$ の重根ではなく、これは $\\alpha$ 以外の他のゼロにも当てはまる。結局、$\\left( x^{p^{n}} - x \\right)$ は正確に $p^{n}$ 個の異なるゼロを持つ。\nパート2-2. 新入生の夢\n一方で、$\\alpha , \\beta \\in F$ に対して $\\left( \\alpha + \\beta \\right)^{p}$ を計算すると、二項定理により、 $$ \\begin{align*} \\left( \\alpha + \\beta \\right)^{p} =\u0026amp; \\sum_{k=1}^{p} \\binom{p}{k} \\alpha^{k} \\beta^{p - k} \\\\ =\u0026amp; \\alpha^{p} + \\sum_{k=2}^{p-1} {{p!} \\over { ( p - k )! ( k )! }} \\alpha^{k} \\beta^{p - k} + \\beta^{p} \\\\ =\u0026amp; \\alpha^{p} + \\beta^{p} + p \\sum_{k=2}^{p-1} {{ ( p - 1 )! } \\over { ( p - k )! ( k )! }} \\alpha^{k} \\beta^{p - k} \\end{align*} $$ $F$ の標数が $p$ であるため、最後の項は $0$ となり、したがって、 $$ \\left( \\alpha + \\beta \\right)^{p} = \\alpha^{p} + \\beta^{p} $$ もう一度両辺に $p$ 乗をすると、 $$ \\left( \\left( \\alpha + \\beta \\right)^{p} \\right)^{p} = \\left( \\alpha^{p} \\right)^{p} + \\left( \\beta^{p} \\right)^{p} $$ 整理すると $\\left( \\alpha + \\beta \\right)^{p^{2}} =\\alpha^{p^2} + \\beta^{p^2}$ であり、これを $n$ 回繰り返すと、次を得る。 $$ \\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n} $$\n今度は $\\mathbb{Z}_{p}$ の代数的閉包 $\\overline{ \\mathbb{Z}_{p} }$ を考える。\n$\\left( x^{p^{n}} - x \\right) \\in \\overline{ \\mathbb{Z}_{p} } [ x ]$ のゼロをすべて集めた集合を $K \\subset \\overline{ \\mathbb{Z}_{p} } $、その元を $\\alpha , \\beta \\in K$ とする。\nパート2-3. $K$ はガロア体である。\n(i) 加算に対する閉包: $$ \\begin{cases} \\alpha^{p^{n}} - \\alpha = 0 \\\\ \\beta^{p^{n}} - \\beta = 0 \\end{cases} $$ である。両辺を加えると、パート2-2 $\\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n}$ により、 $$ \\left( \\alpha^{p^{n}} + \\beta^{p^{n}} \\right) - ( \\alpha + \\beta ) = \\left( \\alpha + \\beta \\right)^{p^{n}} - ( \\alpha + \\beta ) = 0 $$ であるため、$( \\alpha + \\beta ) \\in K$ である。 (ii) 加算に対する単位元: $0^{p^{n}} - 0 = 0$ であるため、$0 \\in K$ である。 (iii) 加算に対する逆元: $\\left( - \\alpha \\right)^{p^{n}} = \\left( - 1 \\right)^{^{p^{n}}} \\left( \\alpha \\right)^{p^{n}} = \\left( - 1 \\right)^{^{p^{n}}} \\alpha$ である。 $p=2$ の場合、$-1 = 1$ であるため、$\\left( -\\alpha \\right) = \\alpha \\in K$ である。 $p \\ne 2$ は奇数の素数であるため、$\\left( - \\alpha \\right)^{p^{n}} - ( - \\alpha ) = 0$、つまり $( - \\alpha ) \\in K$ である。 (iv) 乗算に対する閉包: $\\left( \\alpha \\beta \\right)^{p^{n}} = \\alpha^{p^{n}} \\beta^{p^{n}} = \\alpha \\beta$ であるため、$\\left( \\alpha \\beta \\right)^{p^{n}} - \\alpha \\beta = 0$、すなわち $\\alpha \\beta \\in K$ である。 (v) 乗算に対する単位元: $1^{p^{n}} - 1 = 0$ であるため、$1 \\in K$ である。 (vi) 乗算に対する逆元: $\\alpha \\ne 0$ に対して $\\displaystyle \\left( \\alpha \\right)^{p^{n}} = \\alpha$ の逆数を取ると、$\\displaystyle {{1} \\over {\\left( \\alpha \\right)^{p^{n}} }} = {{1} \\over { \\alpha }}$、すなわち $$ \\left( {{1} \\over { \\alpha }} \\right)^{p^{n}} - {{1} \\over { \\alpha }} = 0 $$ であるため、$\\alpha^{-1} \\in K$ である。 (vii): $| K | = p^{n}$ : $\\mathbb{Z}_{p}$ の標数は $p$ であるため、パート2-1により $\\left( x^{p^{n}} - x \\right)$ は正確に $p^{n}$ 個の異なるゼロを持つ。 したがって、$K$ は $p^{n}$ 次のガロア体である。\nパート3. ガロア体の一意性\nパート1では、$F$ の標数は素数 $p$ であり、パート2-1では、$F$ の代数的閉包 $\\overline{F}$ での演算が、$F$ の単位元 $1_{F}$ を $1_{\\mathbb{Z}_{p}}$ と見た場合、実際には $\\mathbb{Z}_{p}$ の代数的閉包 $\\overline{\\mathbb{Z}}_{p}$ での演算と変わらないことを指摘しておく。\nパート3-1. 基数が $p^{n}$ の体 $E \\subset \\overline{\\mathbb{Z}}_{p}$ の正体 3\nラグランジュの定理: $H$ が有限群 $G$ の部分群であれば、$|H|$ は $|G|$ の約数である。\n基数が $p^{n}$ の体 $\\left( E , + , \\times \\right)$ において、乗算 $\\times$ に対する群 $\\left( E^{\\ast} , \\times \\right)$ を考えると、$E^{\\ast}$ は $E$ で $+$ に対する単位元 $0 \\in E$ を除く $p^{n} - 1$ 個の元と単位元 $1 \\in E^{\\ast}$ を持つ。$\\alpha \\in E^{\\ast}$ のオーダーOrder、つまり $\\alpha$ によって生成される巡回群の基数である $\\left| \\alpha \\right| = \\left| \\left\u0026lt; \\alpha \\right\u0026gt; \\right|$ はラグランジュの定理により $p^{n} - 1$ の約数であり、したがって、 $$ \\alpha^{p^{n} - 1} = 1 \\implies a^{p^{n}} = \\alpha $$ を得る。つまり、$E$ のすべての元は $x^{p^{n}} - x$ のゼロであり、代数学の基本定理により、$\\mathbb{Z}_{p}$ の代数的閉包 $\\overline{\\mathbb{Z}}_{p}$ に含まれる基数が $p^{n}$ の体 $E$ の元は正確に $\\left( x^{p^{n}} - x \\right) \\in \\mathbb{Z}_{p} [x]$ のゼロである。\nパート3-2. 最小分解体\nパート2-1とパート3-1により、与えられた $p$ と $n$ に対して、すべての元が正確に $\\left( x^{p^{n}} - x \\right)$ のゼロで構成される体 $E$ が存在し、$F$ の標数が $p$ であることにより、その係数に対する演算も素体 $\\mathbb{Z}_{p}$ での演算と同じであったことに注意せよ。パート2-3とパート1により、$E$ は素体 $\\mathbb{Z}_{p}$ を素体として持ち、$|E| = p^{n}$ を満たす必要があるガロア体であり、さらにパート2-1により、$E$ は $\\left( x^{p^{n}} - x \\right)$ の最小分解体であることがわかる。\n最小分解体の性質: $f(x) \\in F [ x ]$ の最小分解体はすべて同型である。\n最小分解体の性質により、与えられた $p$ と $n$ に対して、ガロア体は一意である。\n■\n補助定理: 新入生の夢 単に面白い事実として、パート2-2で登場した等式 $$ \\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n} $$ を新入生の夢Freshman\u0026rsquo;s Dreamと呼ぶ。学校に入ったばかりの新入生の立場からすると、累乗が括弧の中に入れば、複雑な展開なしにも難しい問題を解くことができるからである。ちなみに、数論では、標数に関する言及がなくても、同様の方法で合同式 $\\left( \\alpha + \\beta \\right)^{p^{n}} \\equiv \\alpha^{p^n} + \\beta^{p^n} \\pmod{ p }$ を導くことができる。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p300.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p302~304.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p301\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":820,"permalink":"https://freshrimpsushi.github.io/jp/posts/820/","tags":null,"title":"ガロア体"},{"categories":"함수","contents":"公式 ルジャンドル多項式の明示的explicitな公式は以下の通りです。\n$$ P_{l}(x)=\\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \\tag{1} $$\n説明 $l$番目のルジャンドル多項式を得る公式であり、これをロドリゲスの公式と言います。元々はルジャンドル多項式の明示的な形を示す言葉でしたが、その後、多項式で表される特殊関数の明示的な形を示す公式の一般的な名称となりました。\n導出 ルジャンドル多項式$P_{l}$は以下のようなルジャンドルの微分方程式の解を指します。\n$$ (1 - x^{2}) \\dfrac{d^{2} y}{d x^{2}} - 2x \\dfrac{d y}{d x} + l(l+1)y = 0 $$\nしたがって、$(1)$が上記の微分方程式の解であることを示せば、証明が完了します。\nまず、$v=(x^2-1)^l$としたとき、$\\dfrac{d^lv}{dx^l}$がルジャンドル方程式の解であることを示すつもりです。その後、$P_{l}(1) = 1$を満たすように正規化して、$(1)$を得ます。\n$$ \\dfrac{dv}{dx}=l(2x)(x^2-1)^{l-1} $$\n両辺に$(x^2-1)$を掛けると、以下の式を得ます。\n$$ (x^2-1)\\dfrac{dv}{dx}=2lx(x^2-1)^l=2lxv $$\n両辺を$l+1$回微分すると、ライプニッツの法則により以下のようになります。\n$$ \\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C}_{k} \\dfrac{ d^{l+1-k}}{dx^{l+1-k} } \\left( \\dfrac{dv}{dx} \\right) \\dfrac{d^k}{dx^k} (x^2-1) = 2l\\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C} _{k} \\dfrac{d^{l+1-k} v}{dx^{l+1-k}} \\dfrac{d^k x}{dx^k} $$\nこのとき、左辺は$k \\ge 3$のとき$\\dfrac{d^k}{dx^k}(x^2-1)=0$であるため、$k=0,2,3$の項のみが残ります。右辺は$k \\ge 2$のとき$\\dfrac{d^kx}{dx^k}=0$であるため、$k=1,2$の項のみが残ります。したがって、次のように得られます。\n$$ (x^2-1)\\dfrac{d^{l+2} v}{dx^{l+2}} + (l+1)(2x)\\dfrac{d^{l+1}v}{dx^{l+1}}+\\dfrac{l(l+1)}{2!}2\\dfrac{d^l v}{dx^l}=2lx\\dfrac{d^{l+1} v}{dx^{l+1}} + 2l(l+1)\\dfrac{d^lv}{dx^l} $$\n同じ係数項をまとめて整理すると、以下のようになります。\n$$ (1-x^2)\\left( \\dfrac{d^l v}{dx^l} \\right)^{\\prime \\prime} -2x\\left( \\dfrac{d^lv}{dx^l} \\right)^{\\prime} + l(l+1)\\dfrac{d^lv}{dx^l}=0 $$\nこれはルジャンドル方程式と同じ形です。つまり、$\\dfrac{d^l v}{dx^l}$がルジャンドル方程式の解になります。\n$$ P_{l}(x)= \\dfrac{d^l}{dx^l}(x^2-1)^l $$\n$P_{l}(1) = 1$を満たす係数を求めてみましょう。$(x^2-1)^l$を$(x-1)^l(x+1)^l$で因数分解し、ライプニッツの法則で$l$回微分すると、以下のようになります。\n$$ \\begin{align*} \u0026amp;\\quad \\ P_{l}(x) \\\\ \u0026amp;= \\dfrac{d^l}{dx^l} \\left[ (x-1)^l (x+1)^l \\right] \\\\ \u0026amp;= \\sum\\limits_{k=0}^l {}_{l}\\mathrm{C}_{k} \\dfrac{d^{l-k}}{dx^{l-k}}(x-1)^l \\dfrac{d^k}{dx^k}(x+1)^l \\\\ \u0026amp;= {}_{l}\\mathrm{C}_{0} l! (x+1)^l + {}_{l}\\mathrm{C}_{1} l!(x-1) l(x+1)^{l-1}+{}_{l}\\mathrm{C}_2\\dfrac{l!}{2}(x-1)^2l(l-1)(x+1)^{l-2}+\\cdots \\end{align*} $$\n2番目の項からは因数として$(x-1)$を含むため、$x=1$のとき$0$です。したがって、$P_{l}(1)=l! 2^l$であり、この値が$1$になるためには、$\\dfrac{1}{2^l l!}$で割ればよいです。したがって、最終的に以下のようなロドリゲスの公式を得ます。\n$$ P_{l}(x)=\\dfrac{1}{2^l l!}\\dfrac{d^l}{dx^l}(x^2-1)^l $$\n■\n","id":895,"permalink":"https://freshrimpsushi.github.io/jp/posts/895/","tags":null,"title":"ルジャンドル多項式のロドリゲスの公式"},{"categories":"상미분방정식","contents":"定義1 以下の微分方程式をルジャンドルLegendre微分方程式と言う。\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -2x\\dfrac{dy}{dx}+l(l+1) y=0 $$\nルジャンドル微分方程式の解をルジャンドル多項式と言い、通常$P_{l}(x)$で示される。最初のいくつかの$l$によるルジャンドル多項式は次のようである。\n$$ \\begin{align*} P_{0}(x) =\u0026amp;\\ 1 \\\\ P_{1}(x) =\u0026amp;\\ x \\\\ P_2(x) =\u0026amp;\\ \\dfrac{1}{2}(3x^2-1) \\\\ P_{3}(x) =\u0026amp;\\ \\dfrac{1}{2}(5x^3-3x) \\\\ P_{4}(x) =\u0026amp;\\ \\dfrac{1}{8}(35x^4-30x^2+3) \\\\ P_{5}(x) =\u0026amp;\\ \\dfrac{1}{8}(63x^5-70x^3+15x) \\\\ \\vdots\u0026amp; \\end{align*} $$\n説明 ルジャンドル微分方程式は、次のような形で紹介されることもある。\n$$ \\dfrac{d}{dx}\\left[ (1-x)^2 \\dfrac{dy}{dx} \\right] +l(l+1)y=0 $$\nこれはシュツルム-リウヴィル理論Sturm-Liouville theoryで表されるものである。第一項を展開して整理すると、同じ式が得られる。ルジャンドル微分方程式を以下のように一般化したものを関連ルジャンドル微分方程式associated Legendre differential equationと言う。\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -2x\\dfrac{dy}{dx}+\\left( \\dfrac{-m^2}{1-x^2} +l(l+1) \\right) y=0 $$\nここで$m=0$の場合、ルジャンドル微分方程式となる。\nルジャンドル方程式は物理学や工学などで登場し、特に球面座標系でのラプラス方程式を解く時に見ることができる。物理学科ならば、電磁気学で球面座標系での電位を計算する時、量子力学で球面座標系でのシュレディンガー方程式を解く時に出会うことがある。解法の過程が長いため、教科書では通常、ロドリゲス公式で表される解答のみを記載することが多い。実際、物理学の学生は解法が非常に非常に気になるわけではなければ、知らなくても問題はない。\n解法 係数に独立変数$x$が含まれた形で、解が冪級数の形であると仮定すれば解くことができる。\n$$ \\begin{equation} (1-x^2)y^{\\prime \\prime} -2xy^{\\prime}+l(l+1) y=0 \\label{1} \\end{equation} $$\nルジャンドル微分方程式の解を次のように仮定しよう。\n$$ y=a_{0}+a_{1}(x-x_{0})+a_2(x-x_{0})^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}(x-x_{0})^n $$\nこの時$x=0$の時、$y^{\\prime \\prime}$の係数が$(1-x^2)|_{x=0}=1\\ne 0$であるため、$x_{0}=0$と置く。すると級数解は\n$$ \\begin{equation} y=a_{0}+a_{1}x+a_2x^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}x^n \\label{2} \\end{equation} $$\n解を級数と仮定したが、解法の最後に実際には$y$の項が有限であることがわかる。これで$\\eqref{1}$に代入するために$y^{\\prime}$と$y^{\\prime \\prime}$を求めよう。\n$$ y^{\\prime}=a_{1}+2a_2x+3a_{3}x^2+\\cdots=\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1} $$\n$$ y^{\\prime \\prime}=2a_2+3\\cdot 2a_{3}x+4\\cdot 3 a_{4}x^2 +\\cdots = \\sum \\limits_{n=2} n(n-1)a_{n}x^{n-2} $$\nこれで$\\eqref{1}$に$y, y^{\\prime}, y^{\\prime \\prime}$を代入すると\n$$ (1-x^2)\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -2x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n第一項の係数$(1-x^2)$の括弧を展開して整理すると\n$$ \\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x^2\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -2x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n$$ \\implies \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -2\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\nここでのポイントは**$x$の次数を合わせること**である。他は全て$x^n$で表されるのに対し、最初の級数だけが$x^{n-2}$で表されているため、$n$の代わりに$n+2$を代入すると\n$$ \\sum \\limits_{n=0} ^\\infty (n+2)(n+1)a_{n+2}x^{n} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -2\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n二番目の級数が$x^2$項から始まるので、他の級数から$n=0,1$の項を外して、定数項は定数項同士、1次項は1次項同士をまとめると\n$$ \\left[ 2\\cdot 1 a_2+l(l+1)a_{0} \\right]+\\left[ 3\\cdot 2 a_{3}-2a_{1}+l(l+1)a_{1} \\right]x \\\\ + \\sum \\limits_{n=2}^\\infty \\left[ (n+2)(n+1)a_{n+2}-n(n+1)a_{n}-2na_{n}+l(l+1)a_{n} \\right] x^n=0 $$\n上の式が成り立つためには全ての係数が$0$でなければならない。\n$$ 2\\cdot 1 a_2+l(l+1)a_{0} =0 $$\n$$ 3\\cdot 2 a_{3}-2a_{1}+l(l+1)a_{1} =0 $$\n$$ (n+2)(n+1)a_{n+2}-n(n+1)a_{n}-2na_{n}+l(l+1)a_{n}=0 $$\nそれぞれを整理すると\n$$ \\begin{equation} a_2=-\\dfrac{l(l+1)}{2 \\cdot 1}a_{0} \\label{3} \\end{equation} $$\n$$ \\begin{equation} a_{3}=-\\dfrac{(l+2)(l-1)}{3\\cdot 2} a_{1} \\label{4} \\end{equation} $$\n$$ \\begin{equation} a_{n+2}=-\\dfrac{(l+n+1)(l-n)}{(n+2)(n+1)}a_{n} \\label{5} \\end{equation} $$\n$\\eqref{3}, \\eqref{4}, \\eqref{5}$を利用すると、$a_{0}$と$a_{1}$の値だけを知っていれば全ての係数を知ることができる。$\\eqref{3}$と$\\eqref{5}$で偶数次項の係数を求めると\n$$ \\begin{align*} a_{4} =\u0026amp;\\ - \\dfrac{(l+3)(l-2)}{ 4 \\cdots 3}a_2 = \\dfrac{l(l-2)(l+1)(l+3)}{4!}a_{0} \\\\ a_{6} =\u0026amp;\\ -\\dfrac{(l+5)(l-4)}{6\\cdot5} a_{4} = -\\dfrac{ l(l-2)(l-4)(l+1)(l+3)(l+5)}{6!} a_{0} \\\\ \\vdots\u0026amp; \\end{align*} $$\n$n=2m\\ (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m}=(-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!}a_{0} $$\n同様に$\\eqref{4}$、$\\eqref{5}$で奇数次項の係数を求めると\n$$ \\begin{align*} a_{5} =\u0026amp;\\ -\\dfrac{(l+4)(l-3)}{5\\cdot 4}a_{3} = \\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}a_{1} \\\\ a_{7} =\u0026amp;\\ -\\dfrac{(l+6)(l-5)}{7\\cdot 6}a_{5} = -\\dfrac{(l+2)(l+4)(l+6)(l-1)(l-3)(l-5)}{7!}a_{1} \\\\ \\vdots\u0026amp; \\end{align*} $$\n$n=2m+1\\ (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m+1}=(-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!}a_{1} $$\nこれで求めた係数を$\\eqref{2}$に代入して解を求めると\n$$ \\begin{align*} y =\u0026amp;\\a_{0}+a_{1}x -\\dfrac{l(l+1)}{2!}a_{0}x^2-\\dfrac{(l+2)(l-1)}{3!}a_{1}x^3 + \\dfrac{l(l-2)(l+1)(l+3)}{4!}a_{0}x^4+\\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}a_{1}x^5 \\\\ \u0026amp;+ \\cdots +(-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!}a_{0}x^{2m} \\\\ \u0026amp;+ (-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!}a_{1}x^{2m+1} +\\cdots \\end{align*} $$\n$(m=1,2,3,\\cdots)$偶数次項は$a_{0}$で、奇数次項は$a_{1}$でまとめると\n$$ \\begin{align*} y =\u0026amp;\\a_{0}\\left[1-\\dfrac{l(l+1)}{2!}x^2+\\dfrac{l(l-2)(l+1)(l+3)}{4!}x^4 \\right. \\\\ \u0026amp;\\left.+\\sum \\limits_{m=3}^\\infty (-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!} x^{2m} \\right] \\\\ \u0026amp;+ a_{1}\\left[x- \\dfrac{(l+2)(l-1)}{3!}x^3+\\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}x^5 \\right. \\\\ \u0026amp; \\left. +\\sum \\limits_{m=3}^\\infty (-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!} x^{2m+1} \\right] \\end{align*} $$\n最初の括弧を$y_{0}$、二番目の括弧を$y_{1}$とすると、ルジャンドル方程式の一般解は次のようになる。\n$$ y=a_{0}y_{0}+a_{1}y_{1} $$\n二つの級数$y_{0}$と$y_{1}$は比率判定法により、$|x|\u0026lt;1$の範囲で収束\nすることがわかる。$\\eqref{5}$により$\\dfrac{a_{n+2}}{a_{n}}=-\\dfrac{(l+n+1)(l-n)}{(n+2)(n+1)}=\\dfrac{(n+l+1)(n-l)}{(n+2)(n+1)}$であるため、比率判定法を使うと\n$$ \\lim \\limits_{n \\rightarrow \\infty} \\dfrac{(n+l+1)(n-l)}{(n+2)(n+1)}x^2=x^2\u0026lt;1 $$\n$$ \\implies -1\u0026lt;x\u0026lt;1 $$\nしかし、多くの問題で$x=\\cos \\theta$、$l$は非負の整数の形で式が現れ、全ての$\\theta$に対して収束する解を得たい。つまり、$x=\\pm 1$でも収束する解を見つけることが目標である。幸いにも$l$が整数の時は、欲しい解が存在し、その時$l$の値によって必ず$y_{0}, y_{1}$のどちらかの解のみが存在する。$l$が$0$か偶数の時は$y_{1}$が発散し、$y_{0}$は偶数次項のみを持つ有限項の多項式となる。$l$が奇数ならば$y_{0}$が発散し、$y_{1}$は奇数次項のみを持つ有限項の多項式となる。表にまとめると以下のようになる。\n$l$の値 $y_{0}$ $y_{1}$ 方程式の解 $0$か偶数 有限項の多項式 発散 $y=a_{0}y_{0}$ 奇数 発散 有限項の多項式 $y=a_{1}y_{1}$ ケース1. $l$が$0$か偶数\n$l=0$の時、2次項から$l$を因数に持ち、全て$0$になるので、$y_{0}=1$\n$l=2$の時、4次項から$(l-2)$を因数に持ち、全て$0$になるので、$y_{0}=1-3x^2$\n$l=4$の時、6次項から$(l-4)$を因数に持ち、全て$0$になるので、$y_{0}= 1-10x^2+\\dfrac{35}{3}x^4$\nそして$l=0$の時、$x^2=1$から$y_{1}=1+\\frac{1}{3}+\\frac{1}{5}+\\cdots$であるが、これは積分判定法により発散する。他の偶数の時も同様である。したがって、$l$が$0$か偶数の時は、解が偶数次項のみを持つ有限項の多項式となる。つまり、級数$y_{0}$の特定の項までのみ残る形の解を得る。\nケース2. $l$が奇数\n偶数の時と反対の結果が現れる。\n$l=1$の時、3次項から$(l-1)$を因数に持ち、全て$0$になるので、$y_{1}=x$\n$l=3$の時、5次項から$(l-3)$を因数に持ち、全て$0$になるので、$y_{1}=x-\\dfrac{5}{3}x^3$\n$l=5$の時、7次項から$(l-5)$を因数に持ち、全て$0$になるので、$y_{1}=x-\\dfrac{14}{3}x^3+\\dfrac{21}{5}x^5$\n$l=1$の時、$x^2=1$から$y_{0}$は発散し、他の奇数の時も同様である。したがって、$l$が奇数の時は、解が奇数次項のみを持つ有限項の多項式となる。つまり、級数$y_{1}$の特定の項までのみ残る形の解を得る。\nそして、$l$が負の場合は、$l$が0ではない整数の場合と同じであることが$y_{0}$と$y_{1}$を見ればわかる。例えば、$l=2$の場合と$l=-3$の場合が同じであり、$l=1$の場合と$l=-2$の場合が同じである。したがって、$l$が非負の整数についてのみ考えれば良い。$a_{0}$と$a_{1}$の値を上手く選んで$x=1$の時の解が$y(x)=1$になるようにすると、これをルジャンドル多項式Legendre polynomialと言い、$P_{l}(x)$と書く。最初のいくつかのルジャンドル多項式は以下の通りである。\n$$ \\begin{align*} P_{0}(x) =\u0026amp;\\ 1 \\\\ P_{1}(x) =\u0026amp;\\ x \\\\ P_2(x) =\u0026amp;\\ \\dfrac{1}{2}(3x^2-1) \\\\ P_{3}(x) =\u0026amp;\\ \\dfrac{1}{2}(5x^3-3x) \\\\ P_{4}(x) =\u0026amp;\\ \\dfrac{1}{8}(35x^4-30x^2+3) \\\\ P_{5}(x) =\u0026amp;\\ \\dfrac{1}{8}(63x^5-70x^3+15x) \\end{align*} $$\nこの結果はロドリゲス公式Rodrigues\u0026rsquo; formulaで直接得ることもできる。\n■\nMary L. Boas, 数理物理学(Mathematical Methods in the Physical Sciences, 최준곤 訳) (3rd Edition, 2008), p577-580\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":889,"permalink":"https://freshrimpsushi.github.io/jp/posts/889/","tags":null,"title":"ルジャンドル微分方程式の直列解法：ルジャンドル多項式"},{"categories":"확률론","contents":"定義 確率変数 $X: \\Omega \\to E$ の値域を状態空間という。 確率変数の集合 $\\left\\{ X_{t} \\mid t \\in [ 0 , \\infty ) \\right\\}$ を連続的確率過程という。 確率変数の数列 $\\left\\{ X_{n} \\mid n = 0, 1, 2, \\cdots \\right\\}$ を離散的確率過程という。 説明 過程Processという言葉が含まれているため、確率過程を理解するのは難しい、典型的には言葉が難しいために難しい概念だ。「プロセス」とは通常、あるアルゴリズムや、言葉そのままの「過程」を意味するため、上の定義と全く合わないためである。高校を卒業すると、数列を「定義域が自然数の関数」と定義するため、「確率変数の数列」や「確率変数の集合」という説明を敢えてする。\n関数？数列？集合？ この意味で確率過程とは結局のところ、「時間的な変数 $t$ か $n$ に対して確率変数を対応させる関数」である。重要なのは結局「いつ、どのように確率が出るか？」であり、集合だの数列だの複雑に考える必要はない。人々は今日雨が降る確率も気になるし、明日の雨の確率も気になるし、明後日の雨の確率も気になる。今日をD+0、明日をD+1、明後日をD+2として、$p ( X_{n} = \\text{ rain } )$ をD+nの降水確率と表せるなら、確率過程の概念を素晴らしく理解したことになる。\n確率過程論は、その性質上、多くの学問分野で様々なレベルで学ばれるため、教科書によって言葉が異なる。少なくとも確率情報論を学び始めるときは、正確な定義よりも直観的な概念をうまく受け入れることがさらに重要だ。\n例 ギャンブル 例として、コイン投げゲームCash Processを考えてみよう。このゲームでは、コインを投げて表が出れば$1$ポイントを得て、裏が出れば$1$ポイントを失う。プレイヤーが終了を宣言した時点でゲームは終わり、最後にスコアが正ならばポイント一つにつき千円を受け取り、負ならばポイント一つにつき千円を支払わなければならないゲームである。スコアは$0$ポイントから始まる。\nまずこのゲームの状態空間はプレイヤーのスコアであり、整数の集合$\\left\\{ \\cdots, (- 2) , (-1) , 0 , 1 , 2 , \\cdots \\right\\}$になるだろう。プレイヤーが$n$回コインを投げた時のスコアが$x$ポイントである確率は$p( X_{n} = x)$のように表せる。特に、コインを一度も投げていない場合、私のスコアは必ず$0$ポイントであり、$p ( X_{0} = 0 ) = 1$を確信することができる。\nここで$X_{1}$は$(-1)$か$1$であり、$X_{2}$は$(-2) , 0 , 2$のうちの1つであることが確実だ。このように、試行回数$n$が変わると、確率変数$X_{n}$も変わっている。\n上述のゲームのシミュレーションを行うと、スコアの波は上のようにランダムに現れる。このようなランダムな波をブラウン運動Brownian Motionという。$10$回繰り返し、それでやめたなら2千円の賞金を手にしていただろうし、$400$回くらいでやめたならかなりの損失があっただろうし、$900$回くらいでやめたならかなりの大金を手にしていただろう。\n確率過程論は「では、いつやめるのがよいか」に対する答えも提供することができる。適切な目標を達成した時、次の機会はいつか、どれほどのリスクを負うかについても誰もが疑問に思う。\n株 もう一つの例は株式である。\nもちろん、株は完全にランダムではない。しかし、上のようなチャートを見て1年後の動向を予測するのは非常に難しい。確率過程論を勉強することは、チャーチストになることではなく、むしろその逆である。何が価格の変動に影響を与えるかを把握し、迅速に情報を取得し、正確なモデルを作成し、それを自分だけが知っていれば、一生懸命にならずとも生計を立てることができる（もちろん、不可能だが）。\n一方、数学的には、確率過程は非決定論的な動力学系とも見なすことができる。\nコード 以下はRを通じてキャッシュプロセスをシミュレーションした例のコードである。\nset.seed(150421)\rtoss\u0026lt;-sample(c(-1,1),10,replace=T)\rwin.graph(4,4)\rplot(cumsum(toss),type=\u0026#39;l\u0026#39;,main=\u0026#39;10회 반복\u0026#39;)\rabline(h=0)\rtoss\u0026lt;-sample(c(-1,1),1000,replace=T)\rwin.graph(4,4)\rplot(cumsum(toss),type=\u0026#39;l\u0026#39;,main=\u0026#39;1000회 반복\u0026#39;)\rabline(h=0) ","id":857,"permalink":"https://freshrimpsushi.github.io/jp/posts/857/","tags":["R"],"title":"確率過程とは何か？"},{"categories":"추상대수","contents":"定義 1 整域 $D$ の $p \\ne 0$ が単元でないとする。\nPID $D$ の全てのイデアルが主イデアルである場合、$D$ を主イデアル整域PIDと呼ぶ。\n従属定義 可換環 $R$ が単位元 $1$ を持つとする。$a,b \\in R$ に対して $b=ac$ を満たす $c \\in R$ が存在する場合、$a$ が $b$ を割るDivideまたは$a$ が $b$ の因子Factorであると言い、$a \\mid b$ のように表す。 $a \\mid b$ かつ $b \\mid a$ の場合、$a,b$ が連想Associatesであると言う。 $\\forall a,b \\in D$ と $p=ab$ に対して、$a$ か $b$ のいずれかが単元である場合、$p$ を既約元Irreducible Elementと言う。 $\\forall a,b \\in D$ に対して、$p \\mid ab$ の場合、$p \\mid a$ または $p \\mid b$ の $p$ を素元Prime Elementと言う。 単位元は乗算に対する単位元 $1$、単元は乗算に対する逆元を持つ要素である。 定理 2 $D$ が主イデアル整域であるとする。\n[1]: $D$ はネーター環である。 [2]: $0$ でも単元でもない $d \\in D$ は、$D$ の既約元の積として表される。 [3]: $\\left\u0026lt; p \\right\u0026gt;$ が $D$ の極大イデアルである場合、$p$ は $D$ の既約元である。 [4]: $D$ の既約元は素元である。 説明 「主イデアル整域」という言葉は長いため、通常はPIDという略語がよく使用される。\n連想は結合則とスペルは同じだが名詞形であることに注意し、$-3,3 \\in \\mathbb{Z}$ のように互いに単元の積で表すことができる関係である。\n例 整数環 $\\mathbb{Z}$ 整数環 $\\mathbb{Z}$ は全てのイデアルが $n \\mathbb{Z} = \\left\u0026lt; n \\right\u0026gt;$ のように主イデアルとして表される。\n全ての体 $\\mathbb{F}$ ガウス整数環 $\\mathbb{Z} [i]$ とアイゼンシュタイン整数環 $\\mathbb{Z} [\\omega]$ ガウス整数環とアイゼンシュタイン整数環はそれぞれ整数環 $\\mathbb{Z}$ に純虚数 $i := \\sqrt{-1}$ または $\\omega := (-1)^{1/3}$ を加えた環である。\n証明 [1] ネーター環の定義: $N$ を環とする。\n$N$ のイデアルが $S_{1} \\le S_{2} \\le \\cdots$ を満たす場合、これを昇鎖Ascending Chainという。 昇鎖 $\\left\\{ S_{i} \\right\\}_{i \\in \\mathbb{N} }$ に対して、$S_{n} = S_{n+1} = \\cdots$ を満たす $n \\in \\mathbb{n}$ が存在する場合、定常Stationaryであるという。つまり、定常昇鎖では、ある時点からイデアルがこれ以上大きくならない。 すべての昇鎖が定常である環をネーター環という。 $D$ のイデアルの昇鎖 $N_{1} \\le N_{2} \\le \\cdots$ とその和集合 $\\displaystyle N := \\bigcup_{k=1}^{ \\infty } N_{k}$ を考える。ある $i, j \\in \\mathbb{N}$ に対して $$ a \\in N_{i} \\\\ b \\in N_{j} \\\\ N_{i} \\le N_{j} $$ とすると、$( N_{j} , + , \\cdot )$ はイデアルによって定義されるため、部分環であり、$b$ の加算に対する逆元 $(-b) \\in N_{j}$ が存在する。また、$ab \\in N_{j}$ であるため、$(a-b), ab \\in N$ であり、部分環判定法により、$N$ は $D$ の部分環である。それだけでなく、$N_{i}$ がイデアルである\nため、全ての $d \\in D$ に対して $d a = a d$ であり、$da \\in N$ であるため、$N$ は $D$ のイデアルである。\n$D$ はPIDであるため、全てのイデアルが主イデアルであり、ある $c \\in N$ に対して $N = \\left\u0026lt; c \\right\u0026gt;$ のように表せる。ここで、$\\displaystyle N = \\bigcup_{k=1}^{ \\infty } N_{k}$ であるため、$c \\in N$ であれば、$c \\in N_{r}$ を満たす自然数 $r \\in \\mathbb{N}$ が存在しなければならない。$c \\in N_{r}$ は、$N_{r}$ より小さいイデアルの中に $c$ を生成元とする主イデアルが存在することを意味する。数式で表すと $$ \\left\u0026lt; c \\right\u0026gt; \\le N_{r} \\le N_{r+1} \\le \\cdots \\le N = \\left\u0026lt; c \\right\u0026gt; $$ となり、$N_{r} = N_{r+1} = \\cdots$ である。したがって、$D$ はネーター環である。\n■\n[2] $d$ が既約元であれば証明する必要はないため、単元でない $d_{1}, c_{1} \\in D$ に対して、$d = d_{1} c_{1}$ のように表されるとする。\nすると、$\\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; d_{1} \\right\u0026gt;$ であり、$d_{i} := d_{i+1} c_{i+1}$ を続けて定義すると、昇鎖 $$ \\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; d_{1} \\right\u0026gt; \\le \\left\u0026lt; d_{2} \\right\u0026gt; \\le \\cdots $$ を得る。しかし、定理[1]により、この鎖が終わる $a_{r}$ が存在し、$a_{r}$ は同時に $a$ の因子である既約元となる。このように、$d$ を割る既約元を $p_{1}$ とし、単元でない $f_{1}$ に対して、$d = p_{1} f_{1}$ とすると、$\\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; f_{1} \\right\u0026gt;$ となり、$f_{i} := p_{i+1} f_{i+1}$ を続けて定義すると、昇鎖 $$ \\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; f_{1} \\right\u0026gt; \\le \\left\u0026lt; f_{2} \\right\u0026gt; \\le \\cdots $$ を得る。これも定理[1]により、この鎖が終わる $f_{s}$ が存在し、$f_{s}$ は同時に $f_{i}$ の因子である既約元となる。\nこのプロセスを有限回繰り返すことで、$d$ が既約元の積として表されることが確認できる。\n■\n[3] $( \\implies )$\n$D$ の極大イデアル $\\left\u0026lt; p \\right\u0026gt;$ の $p$ が、$D$ の単元でない $a,b$ に対して、$p=ab$ のように表されると仮定する。\nすると、$\\left\u0026lt; p \\right\u0026gt; \\le \\left\u0026lt; a \\right\u0026gt;$ であり、$\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ の場合、$b$ は単元でなければならないため、実際には $\\left\u0026lt; p \\right\u0026gt; \\lneq \\left\u0026lt; a \\right\u0026gt;$ を得る。しかし、$\\left\u0026lt; p \\right\u0026gt;$ が極大イデアルであるため、$\\left\u0026lt; a \\right\u0026gt; = D = \\left\u0026lt; 1 \\right\u0026gt;$ でなければならず、$a$ と $1$ は連想される。要約すると、\n$\\left\u0026lt; p \\right\u0026gt; \\ne \\left\u0026lt; a \\right\u0026gt;$ の場合、$a$ は単元であり、 $\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ の場合、$b$ は単元であるため、 $p$ は既約元である。\n$( \\impliedby )$\n既約元 $p=ab$ に対して、$\\left\u0026lt; p \\right\u0026gt; \\le \\left\u0026lt; a \\right\u0026gt;$ と仮定する。\n$a$ が単元であれば、$\\left\u0026lt; a \\right\u0026gt; = D$ で問題はないが、$a$ が単元でない場合、$b$ は必ず単元でなければならない。\n$b$ が単元であるということは、ある $u \\in D$ に対して、$bu =1$ という意味であるが、 $$ pu = abu = a $$ となるため、$\\left\u0026lt; p \\right\u0026gt; \\ge \\left\u0026lt; a \\right\u0026gt;$、つまり$\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ でなければならない。要約すると、\n$\\left\u0026lt; a \\right\u0026gt; = D$ であるか、 $\\left\u0026lt; a \\right\u0026gt; = \\left\u0026lt; p \\right\u0026gt;$ である必要があるため、 $\\left\u0026lt; p \\right\u0026gt;$ は極大イデアルとなる。\n■\n[4] $p$ が既約元であるとすると、$\\left\u0026lt; p \\right\u0026gt;$ は定理[3]により極大イデアルであり、$1 \\in D$ であるため、素イデアルである。\n$p$ が $ab$ を割るとすると、$(ab) \\in \\left\u0026lt; p \\right\u0026gt;$ であり、$\\left\u0026lt; p \\right\u0026gt;$ が素イデアルであるため、$a \\in \\left\u0026lt; p \\right\u0026gt;$ または $b \\in \\left\u0026lt; p \\right\u0026gt;$ である。これを別の形で表すと、$p \\mid ab$ の場合、$p \\mid a$ または $p \\mid b$ であるため、$p$ は素元となる。\n■\n関連項目 ユークリッド整域 $\\implies$ 主イデアル整域 $\\implies$ 一意分解整域 $\\implies$ 整域 ユークリッド整域 $\\implies$ 主イデアル整域 $\\implies$ ネーター環 Fraleigh. (2003). A First Course in Abstract Algebra(7th Edition): p389~391, 394.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A First Course in Abstract Algebra(7th Edition): p392~393.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":825,"permalink":"https://freshrimpsushi.github.io/jp/posts/825/","tags":null,"title":"主イデアル整域"},{"categories":"해석개론","contents":"分割1 区間$[a,b]$が与えられたとしよう。$[a,b]$の分割partition$P$を下のように定義する。\n$$ P := \\left\\{ x_{0},\\ x_{1},\\ \\cdots, x_{n}\\right\\},\\quad a=x_{0} \u0026lt;x_{1}\u0026lt;\\cdots \u0026lt; x_{n} =b $$\nそして、$\\Delta x_{i}$を次のように定義する。\n$$ \\Delta x_{i} :=x_{i}-x_{i-1},\\quad i=1,2,\\cdots,n $$\n説明 簡単に言えば、分割とはある区間を分割した時、区間の両端と区間内のすべての境界点を要素として持つ集合のことだ。重要な点は、分割について話す場合、必ずどの区間についてのものかが必要だということだ。つまり、単に分割と言うことはできず、ある区間の分割と言うべきだ。\nリーマン和 $f$を$[a,b]$で定義された有界関数、$P$を$[a,b]$の分割としよう。そして、$M_{i}$, $m_{i}$を以下のようだとしよう。\n$$ \\begin{align*} M_{i} \u0026amp;=\\sup f(x),\u0026amp;(x_{i-1} \\le x \\le x_{i}) \\\\ m_{i}\u0026amp;=\\inf f(x), \u0026amp;(x_{i-1} \\le x \\le x_{i}) \\end{align*} $$\nすると、$U(P,f), L(P,f)$を以下のように定義し、それぞれを**$P$に対する$f$のリーマン上和、下和**upper and lower Riemann sumという。\n$$ \\begin{align*} U(P,f) \u0026amp;:=\\sum \\limits _{i=1} ^n M_{i} \\Delta x_{i} \\\\ L(P,f) \u0026amp;:= \\sum \\limits _{i=1} ^{n} m_{i}\\Delta x_{i} \\end{align*} $$\n説明 リーマン和は、関数の面積を区間を分割して近似するもので、区分求積法と同じだ。与えられた分割$P$に対して、上和は最大値を、下和は最小値を意味する。上和と下和の差がないほど近似した場合、それを$f$のグラフの下の面積と見なしてもいいだろう。\nリーマン積分 区間$[a,b]$のすべての分割$P$に対して$\\inf$を取ったものを**$[a,b]$上での$f$のリーマン上積分**upper Riemann integralという。\nそれぞれの$P$に対するリーマン上和の最小上界として定義し、以下のように示す。\n$$ \\begin{equation} \\overline{\\int _{a}^{b}} f dx := \\inf \\limits_{P} U(P,f) \\label{eq1} \\end{equation} $$\n同様に、区間$[a,b]$のすべての分割$P$に対して$\\sup$を取ったものを**$[a,b]$上での$f$のリーマン下積分**lower Riemann integralという。\n$$ \\begin{equation} \\underline {\\int _{a}^b } f dx := \\sup \\limits_{P} L(P,f) \\label{eq2} \\end{equation} $$\n$f$のリーマン上積分とリーマン下積分が同じである場合、$f$は$[a,b]$でリーマン積分可能Riemann integrableであると言い、以下のように表記する。\n$$ f \\in \\mathscr{R}= \\left\\{ f : f \\text{ is Riemann integrable} \\right\\} $$\n$\\mathscr R$はリーマン積分可能な関数の集合である。そして、$(1)$と$(2)$の共通値を以下のように表記し、これを**$[a,b]$上での$f$のリーマン積分**Riemann integralという。\n$$ \\underline {\\int _{a}^b } f dx = \\int _{a} ^b f dx = \\overline {\\int _{a}^b} f dx $$\nまたは\n$$ \\int _{a} ^b f(x) dx $$\n説明 上積分は$f$の面積を少し大きく近似したもの(上和)の中で最小のものであり、下積分は$f$の面積を少し小さく近似したもの(下和)の中で最大のものだ。だから、この二つが同じである時、$f$のグラフの下の面積を正確に近似したと言えるだろう。\nさらに、$f$が有界であるため、次を満たす二つの定数$M$、$m$が存在する。\n$$ m \\le f(x) \\le M \\ \\ \\ (a\\le x\\le b) $$\nしたがって、すべての分割$P$に対して次が成り立つ。\n$$ m(b-a) \\le L(P,f) \\le U(P,f) \\le M(b-a) $$\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), p120-121\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":828,"permalink":"https://freshrimpsushi.github.io/jp/posts/828/","tags":null,"title":"分割、リーマン和、リーマン積分"},{"categories":"힐베르트공간","contents":"定義1 完備 内積空間をヒルベルト空間Hilbert spaceと言う。ヒルベルトの名前から、主に$H$と表記される。\n説明 完備空間とは、すべてのコーシー数列が収束する空間のことだ。バナッハ空間も完備空間なので、内積が定義されたバナッハ空間としてヒルベルト空間を説明することもできる。例えば、以下のような空間がある。\nルベーグ空間 $L^{2}$ $\\ell^{2}$ 空間 実数空間 $\\mathbb{R}^{n}$ 複素数空間 $\\mathbb{C}^{n}$ 性質 ヒルベルト空間は一様に凸である 最短ベクトル定理 直交分解定理 リース表現定理 Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p65\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":776,"permalink":"https://freshrimpsushi.github.io/jp/posts/776/","tags":null,"title":"関数解析学におけるヒルベルト空間"},{"categories":"바나흐공간","contents":"定義1 $(X, \\left\\| \\cdot \\right\\|_{X}), (Y, \\left\\| \\cdot \\right\\|_{Y})$をノルム空間と呼ぶ。\nノルム空間からノルム空間への写像を作用素と呼ぶ。\n$x,x_{1},x_{2}\\in X$に対して、$T : X \\to Y$が $$ T( x_{1} + x_{2} ) = T( x_{1} ) + T( x_{2} ) \\quad \\text{and} \\quad T( a x ) = a T( x ) $$ を満たす場合、線形作用素と呼ぶ。\nすべての$x \\in X$に対して、$\\left\\| T(x) \\right\\|_{Y} \\le C \\left\\| x \\right\\|_{X}$を満たす$C \\ge 0$が存在する場合、$T$は有界であるという。\n3.を満たす$C$の中で最も小さい$C$を$T$の作用素ノルムと定義し、以下のように記す。 $$ \\left\\| T \\right\\| :=\\min \\left\\{ C : \\left\\| T(x) \\right\\|_{Y} \\le C \\left\\| x \\right\\|_{X} \\right\\} $$\n有界線形作用素$T : X \\to Y$をすべて集めた集合を$B(X,Y)$のように表す。\n説明 ベクトル空間からベクトル空間への写像を特に変換と呼ぶように、ノルム空間からノルム空間への関数を特に作用素と呼ぶ。\n便宜上、多くの教科書では、ベクトル空間間の任意の関数$X \\to Y$を変換と呼び、$X \\to X$のような変換を作用素と呼ぶ。\n4.の定義から次のことが得られる。\n$$ \\left\\| T \\right\\| = \\sup \\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\left\\| T(x) \\right\\|_{Y} $$\nこれは$T$のノルムとも定義される。$\\left\\| x \\right\\|_{X}=1$という条件がなぜ存在するのか理解できなければ、3.を考えればいい。\n作用素は代数的には演算を保持するホモモルフィズムであり、当然、これに関する定理もすべて使うことができる。\n「演算」という表現の代わりに「作用素」という表現を使うのは、過去のように「演算」に焦点を当てるのではなく、ある空間での「作用」に興味を持ち、数学的に抽象化して扱うためである。回転変換のようなものを考えると、空間上である点を回転させて移動させると見ることができる。座標をベクトルとしてとり、行列を乗算して「計算」した結果を得るという説明も正しいが、点の位置を「移動させる」という行動として考えれば、作用素という表現も十分適切である。\nこのように、与えられた空間内でベクトルとして表される数学的な対象に対して、ある「作用$T$を加える」という表現を使うことができるようになった。その中でも特に私たちが関心を持つのは線形作用素であり、例として次のようなものがある。\n恒等作用素 $I : X \\to X, Ix = x$ その名の通り、作用を加えても変わらない、あるいは作用を加えないのと同じである作用である。$1$や${\\rm id}$とも記される。\n零作用素 $\\mathbb{0} (x) : = 0$\nどんな元も$0$にする作用で、作用素のベクトルスペースでゼロベクトルの役割を果たす。\n微分作用素 $D : C^{1} \\to C^{1}, Df = \\dfrac{d f}{d x}=f^{\\prime}$\n微分を行う作用素であり、実際には高校から誰もが知らず知らずのうちに使ってきた事実である。\n積分作用素 $T : C[0, 1] \\to C[0, 1]$, $\\displaystyle y(t) = Tx(t) = \\int_{0}^{1}K(t, s)x(s) ds$ 積分もまた一つの作用素であり、このとき$K$をカーネルという。積分変換とも言う。\n行列 $T_{A} ( \\mathbb{x} ) := A \\mathbb{x}$ $m \\times n$行列$A$は、$\\mathbb{C}^{n}$から$\\mathbb{C}^{m}$への関数と考えることができる。\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p37\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":728,"permalink":"https://freshrimpsushi.github.io/jp/posts/728/","tags":null,"title":"関数解析学における作用素"},{"categories":"추상대수","contents":"定義 1 環 $(R , + , \\cdot)$が乗算$\\cdot$に対する単位元$1 \\in R$を持っている時、$1$を単位元Unityと言う。 単位元を持つ環$R$で、乗算に対する逆元が存在する元素$r \\ne 0$を単元Unitと言う。 単位元を持つ環$R$で、$0$以外の全ての元が単元であれば、それを除算環Division Ringと言う。 除算環$R$が乗算に関して可換ならば、その環を体Fieldと言う。 説明 簡単に言えば、体$(F , + , \\cdot )$とは、加算に対する単位元$0 \\in F$を除くすべての元が逆元を持つ可換環である。抽象代数の観点から考えると難しそうだが、解析学で学んだ$\\mathbb{R}$を思い出してみれば、実際にはこれが「代数構造」らしいと見ることができるだろう。\nなぜ逆元を持つ元がユニットと呼ばれるのか 単位元の英語表現であるUnityは簡単に受け入れられるが、なぜ逆元を持つ元素をUnitと呼ぶのか理解し難い人も多いのではないだろうか。通常、Unitは「単位」と訳され、「ある量を測るときの基準」として良く使われるからだ。逆元が存在することと単位は関係ないように見えるが、なぜUnitと定義したのか？ここで面白い脳内提案をしてみたい。\n代数学の発展初期には、当然ながら整数に関する研究が活発だった。実際、私たちが整数集合を$\\mathbb{Z}$と書くのも、ドイツ語のZahlringからきており、\u0026lsquo;Zahl-\u0026lsquo;が「数」を意味し、\u0026rsquo;-ring\u0026rsquo;はご存知の通り環に翻訳されている。代数学で使われる多くの概念が数論のセンスから出てきたと受け入れるのはそう難しくないだろう。\nここで整数体$\\mathbb{Z}$を考えてみよう。\n$\\mathbb{Z}$は無限に多くの整数を元として持つ。ここで、乗算に対して単位元となるのは$1$のみで、逆元を持つ元素は$-1$と$1$のみである。抽象代数を学ぶだけの数学に親しんでいれば、$-1$と$1$が「ユニット」と呼ばれるのに違和感を感じないはずだ。この背景から、整数を超えて様々な代数構造を見ていくうちに、これらをユニットと呼ぶのが適切だったのではないかと思われる。\n$\\mathbb{R}$に至って、$0$を除く全ての$r \\in \\mathbb{R}$に対して乗算に対する逆元$\\displaystyle {{1} \\over {r}} \\in \\mathbb{R}$が存在するので、$0$を除く全ての元がユニットである。考えてみれば、ある数$a$を$r$に掛けて欲しい数である$x$を作ることができるので、$r \\ne 1$も単位としての役割を果たす理由が全くない。そして、そのある数$a$は当然$a = r^{-1}x$であり、$r^{-1}$の存在無しには確信できないことだ。\n参照 解析学での体の公理 Fraleigh. (2003). 「抽象代数入門」(第7版): p173。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":715,"permalink":"https://freshrimpsushi.github.io/jp/posts/715/","tags":null,"title":"抽象代数学における体"},{"categories":"상미분방정식","contents":"公式1 これはラプラス変換の表です。\n$f(t)=\\mathcal{L^{-1}}$ $F(s)=\\mathcal{L} \\left\\{ f(t) \\right\\}$ 導く정 $1$ $\\dfrac{1}{s}$ link $e^{at}$ $\\dfrac{1}{s-a}$ link $t^n$ $\\dfrac{n!}{s^{n+1}}$ link $t^{p}$ $\\dfrac{ \\Gamma (p+1) }{ s^{p+1}}$ link $t^{p}e^{at}$ $\\dfrac{ \\Gamma (p+1) }{ (s-a)^{p+1}}$ link $\\sin (at)$ $\\dfrac{a}{s^2+a^2}$ link $\\cos (at)$ $\\dfrac{s}{s^2+a^2}$ link $e^{at}\\sin(bt)$ $\\dfrac{b}{(s-a)^2 +b^2}$ link $e^{at}\\cos(bt)$ $\\dfrac{s-a}{(s-a)^2+b^2}$ link $\\sinh (at)$ $\\dfrac{a}{s^2-a^2}$ link $\\cosh (at)$ $\\dfrac{s}{s^2-a^2}$ link $e^{at} \\sinh (bt)$ $\\dfrac{b}{(s-a)^2-b^2}$ link $e^{at} \\cosh (bt)$ $\\dfrac{s-a}{(s-a)^2-b^2}$ link $u_{c}(t)= \\begin{cases} 0 \u0026amp; t\u0026lt;c \\\\ 1 \u0026amp; t\\ge c\\end{cases}$ $\\dfrac{e^{-cs}}{s}$ link $u_{c}(t)f(t-c)$ $e^{-cs}F(s)$ link $f^{\\prime}(t)$ $s\\mathcal{L} \\left\\{ f(t) \\right\\} -f(0)$ link $f^{(n)}$ ${s^n\\mathcal {L}\\left\\{ f(t) \\right\\} -s^{n-1}f(0) - \\cdots -f^{(n-1)}(0) }$ link $f(t)=f(t+T)$ $\\dfrac{\\displaystyle \\int_{0}^T e^{-st}f(t)dt}{1-e^{-st}}$ link $\\delta (t-t_{0})$ $e^{-st_{0}}$ link $f(ct)$ $\\frac{1}{c}F \\left( \\frac{s}{c} \\right)$ link $\\frac{1}{k}f (\\frac{t}{k} )$ $F(ks)$ link $\\frac{1}{a} e^{-\\frac{b} {a}t}f\\left(\\frac{t}{a}\\right)$ $F(as+b)$ link $t^{n}f(t)$ $(-1)^{n}F^{(n)}(s)$ link William E. Boyce , Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), Chapter6 The Laplace Transform\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":743,"permalink":"https://freshrimpsushi.github.io/jp/posts/743/","tags":null,"title":"ラプラス変換の表"},{"categories":"추상대수","contents":"定義 1 二つの二項演算、足し算$+$と掛け算$\\cdot$に関して以下のルールを満たす集合$R$を環と定義する。\n$a$、$b$、$c$が$R$の元の時、\n足し算に対して交換法則が成り立つ。$$a+b=b+a$$ 足し算に対して結合法則が成り立つ。$$(a+b)+c=a+(b+c)$$ 足し算に対する単位元が存在する。$$\\forall a \\ \\exists 0\\ \\ \\mathrm{s.t} \\ a+0=a$$ すべての元の足し算に対する逆元が存在する。$$\\forall a \\ \\exists -a\\ \\ \\mathrm{s.t}\\ a+(-a)=0$$ 掛け算に対して結合法則が成り立つ。$$(ab)c=a(bc)$$ 足し算と掛け算に対して分配法則が成り立つ。$$a(b+c)=ab+ac\\ \\mathrm{and} \\ (b+c)a=ba+ca$$ 説明 要するに、集合$R$が足し算に対して可換群であり、掛け算に対して半群であり、二つの演算に対して分配法則が成り立つ時、$R$を環という。\n特に、掛け算に対しても交換法則が成り立つ場合、可換環またはアーベル環と呼ばれる。また、環の定義からわかる通り、掛け算に対する単位元や逆元が存在する必要はない。単位元が存在しても、逆元が存在する必要もない。上記の6つの条件を満たせば、環と言える。\n群を扱う時、演算に対する単位元を$e$と表わす。環では演算が二つあるため、どちらの演算に対する単位元か簡単に分かるように異なる記号を使う。足し算に対する単位元は$0$とし、単位元と呼ぶ。掛け算に対する単位元が存在すれば$1$とし、単位と呼ぶ。ある元$a$の掛け算に対する逆元が存在する時、$a$を環$R$の単位と呼ぶ。\n群と同じく、環の掛け算に対する単位元も存在するならば、その存在は一意である。各元の逆元も存在すれば、それも一意である。この証明は群で行った方法と同じなので、ここでは書かないが、詳細はこちらを参照。\n例 整数の集合$\\mathbb{Z}$を考える。上記の6つの条件を満たすため、足し算、掛け算に対する環だ。また、掛け算に対して交換法則も満たすため、可換環だ。単位$1$が存在し、その元は整数の1であり、単位は1と-1だ。（それぞれの逆元として1と-1を持つ）\n注意 環では、掛け算に対する単位元や逆元が存在する「必要」がない。だから、群でのように安易に消去することができない。つまり、$a,\\ b,\\ c$が環$R$の元の時、$ab=ac$として$b=c$と結論づけることはできないのだ。$a$の逆元が必ず存在するわけではないからだ。\n同様に、$a^2=a$としても、安易に$a=0$や$a=1$という結論を出すことはできない。環を扱う際には、この点に注意しよう。\nFraleigh. (2003). 「抽象代数入門(第7版)」: p167.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":587,"permalink":"https://freshrimpsushi.github.io/jp/posts/587/","tags":null,"title":"抽象代数学における環"},{"categories":"측도론","contents":"定義 1 関数 function $f : E \\to \\overline{\\mathbb{R}}$ が、$E_{0} \\subset E$ の集合(ここで $m(E_{0}) = 0$)を除いて、ある性質 $P$ を持つ場合、$f$ は $E$ のほとんど至る所で $P$ の性質を持つと言われる。\n表記 確率を話す時に、ほとんど至る所ではほとんど確実にと表現され、短く書くために $$ f = g \\text{ a.e.} \\\\ P(E) = 0 \\text{ a.s.} $$ という略語を使うことがある。\n説明 簡単に言えば、零集合を除いた全ての点を\u0026rsquo;ほとんど至る所\u0026rsquo;と見ることである。この概念は正式に定義されただけで、高校で定積分を学んだ時に既に知っていたことだ。そのため、上限と下限が同じなら、その定積分は必ず $0$ であり、端点が含まれるかどうかを確率を計算する時には無視した。\n基本的な性質 [1]: $f : E \\to \\mathbb{R}$ が計測可能で、$E$ のほとんど至る所で $f = g$ ならば、$g$ は $E$ で計測可能である。 [2]: $f,g$ が $E$ で計測可能で、$E$ のほとんど至る所で $|f| , |g| \u0026lt; \\infty$ ならば、$\\alpha f + \\beta g$ は $E$ で計測可能である。 [3]: $f,g$ が $E$ で計測可能で、$E$ のほとんど至る所で $|f| , |g| \u0026lt; \\infty$ ならば、$f g$ は計測可能である。 証明 これらの性質は、一度は手で直接証明してみることが良いが、[3]を除いてはそれほど面白くなさそうだ。\n[1] $E_{0} = \\left\\{ x \\in E \\ | \\ f(x) \\ne g(x) \\right\\}$ とすると、$E_{0} \\subset E$ かつ $m(E_{0}) = 0$。任意の $c$ に対して $$ \\left\\{ x \\in E \\ | \\ g(x) \u0026gt; c \\right\\} = \\left\\{ x \\in E_{0} \\ | \\ g(x) \u0026gt; c \\right\\} \\cup \\left[ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; c \\right\\} \\cap ( E \\setminus E_{0} ) \\right] $$, 右辺の項を一つずつ見ると、$\\left\\{ x \\in E_{0} \\ | \\ g(x) \u0026gt; c \\right\\} \\subset E_{0}$ のため、 $$ \\left\\{ x \\in E_{0} \\ | \\ g(x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$, $f$ は $E$ で計測可能なので、 $$ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$, 最後に、 $$ E \\cap (\\mathbb{R} \\setminus E_{0}) = ( E \\setminus E_{0} ) \\in \\mathcal{M} $$, よって $\\left\\{ x \\in E \\ | \\ g(x) \u0026gt; c \\right\\} \\in \\mathcal{M}$ であり、$g$ は $E$ で計測可能である。\n■\n[2] $\\alpha = 0$ なら、$\\alpha f$ は計測可能で、$\\beta = 0$ なら、$\\beta g $ は計測可能である。\n$\\alpha \\ne 0$ なら、$f$ が計測可能なので、任意の $\\displaystyle {{c} \\over {\\alpha}}$ に対して $$ \\left\\{ x \\in E \\ \\left| \\ f(x) \u0026gt; {{c} \\over {\\alpha}} \\right. \\right\\} \\in \\mathcal{M} $$, ここで $\\alpha\u0026gt; 0$ なら、 $$ \\left\\{ x \\in E \\ | \\ \\alpha f(x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$, そして $\\alpha \u0026lt;0$ なら、 $$ \\left\\{ x \\in E \\ | \\ \\alpha f(x) \u0026lt; c \\right\\} \\in \\mathcal{M} $$, 従って、$\\alpha f$ は計測可能であり、同じ方法で $\\beta \\ne 0$ の時に $\\beta g$ も計測可能であるとも示せる。\n今、$(f + g)$ が計測可能、つまり $\\left\\{ x \\in E \\ | \\ f(x) + g(x) \u0026lt; c \\right\\} \\in \\mathcal{M}$ を示せば、証明は終了する。両関数は有限の値を持つため、全ての $x \\in E$ に対して $f(x) + g(x) \u0026lt; c$ を満たす $c \\in \\mathbb{R}$ が存在するであろう。再び表示すると、$f(x) \u0026lt; c - g(x)$ で、有理数の密集性により、$f(x) \u0026lt; q \u0026lt; c - g(x)$ を満たす $q \\in \\mathbb{Q}$ が存在する。そうすると、 $$ \\bigcup_{q \\in \\mathbb{Q}} \\left\\{ x \\in E \\ | \\ g(x) \u0026lt; c - q \\right\\} \\cap \\left\\{ x \\in \\ | \\ E f(x) \u0026lt; q \\right\\} = \\left\\{ x \\in E \\ | \\ f(x) + g(x) \u0026lt; c \\right\\} \\in \\mathcal{M} $$\n■\nStrategy[3]**: $fg$ が計測可能であることを示すアイデアは、$\\displaystyle fg = {{1} \\over {2}} \\left[ (f+ g)^2 - f^2 - g^2\\right]$ の等式一つに要約される。\n[3] すでに [2]で、発散しない計測可能関数の和が計測可能であることを示したので、$f^2$ が計測可能であることを示せば十分である。$f$ が計測可能なので、全ての $c$ に対して $$ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; \\sqrt{c} \\right\\} \\in \\mathcal{M} \\\\ \\left\\{ x \\in E \\ | \\ f(x) \u0026lt; - \\sqrt{c} \\right\\} \\in \\mathcal{M} $$, 従って、 $$ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; \\sqrt{c} \\right\\} \\cup \\left\\{ x \\in E \\ | \\ f(x) \u0026lt; - \\sqrt{c} \\right\\} = \\left\\{ x \\in E \\ | \\ f^2 (x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$\n■\n参照 ほとんど至る所での収束 $\\implies$ 測度による収束 ほとんど確実に収束 $\\implies$ 確率による収束 Capinski. (1999). Measure, Integral and Probability: p55.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":524,"permalink":"https://freshrimpsushi.github.io/jp/posts/524/","tags":null,"title":"測度論でのほとんど至る所とほとんど確実に"},{"categories":"확률론","contents":"定義 1 $\\mathcal{F}$が集合$\\Omega$のシグマ場だとしよう。\n可測集合 $E \\in \\mathcal{F}$を事象と呼ぶ。 $\\mathcal{F}$上の測度 $P : \\mathcal{F} \\to \\mathbb{R}$が$P(\\Omega) = 1$を満たすなら、$P$を確率と呼ぶ。 $( \\Omega, \\mathcal{F} , P )$を確率空間と呼ぶ。 説明 測度論の力を借りれば、確率論のさまざまな概念に対する数学的基盤を提供し、あいまいさを取り除くことができる。\n高校の課程や確率論、または数理統計学で、事象とは任意の試行で起こり得るケースだった。数理統計学では確率が全ての事象を集めた集合を定義域とする関数として定義されていたのと異なり、今では$\\mathcal{F}$の元を事象とし、標本空間という言葉はもはや使わない。シグマ場$\\mathcal{F}$は、試行が正確に何であるかについて心配することなく、全体集合$\\Omega$とそれに関する形式的な代数体系としてのみ定義される。したがって、誰が、何を、どう話すかによって生じうるあいまいさは存在しない。 確率は標本空間が定義域であり、$[0,1]$が値域であり、確率の加法則を満たす関数であった。測度論で再定義された確率の概念は、「任意の試行」や「ケースの数」などの言葉さえ許容しない。測度の定義を考えれば、このような確率の定義は、既に馴染み深い確率の概念を完全にカバーしつつ、厳密に一般化したものである。 「確率空間」という新しい言葉をわざわざ定義したのは、今や空間$\\Omega$自体を$P$として捉えようという意図があるからだ。基礎的な数理統計学でのように、$\\Omega = \\mathbb{R}$ならば$\\mathcal{F}$はボレルシグマ場 $\\mathcal{B}$となり、$(\\Omega , \\mathcal{F})$を論じる意味はない。あまりにも簡単すぎるということであり、言い換えれば、応用できる範囲が限られているということだ。測度論の導入によって、確率の世界は、広大な一般化の段階に入る。しっかり勉強するつもりなら、この$\\Omega$がどれだけ驚異的に与えられるかに注意が必要だ。 参照 数理統計学で定義された確率 Capinski. (1999). Measure, Integral and Probability: p46.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":498,"permalink":"https://freshrimpsushi.github.io/jp/posts/498/","tags":null,"title":"測度論で定義される確率"},{"categories":"측도론","contents":"定義 集合$X \\ne \\emptyset$に対して、以下の条件を満たす$\\mathcal{E} \\subset \\mathscr{P} (X)$を$X$上のシグマ代数またはシグマ場という。集合$X$とシグマ場$\\mathcal{E}$の順序対$(X , \\mathcal{E})$を可測空間と呼ぶ。\n(i): $\\emptyset \\in \\mathcal{E}$ (ii): $E \\in \\mathcal{E} \\implies E^{c} \\in \\mathcal{E}$ (iii): $\\displaystyle \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{E} \\implies \\bigcup_{n=1}^{\\infty} E_{n} \\in \\mathcal{E}$ (iv): $\\displaystyle \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{E} \\implies \\bigcap_{n=1}^{\\infty} E_{n} \\in \\mathcal{E}$ 説明 ある空間$X$に対してシグマ場$\\mathcal{E}$が与えられた場合、$(X , \\mathcal{E})$を可測空間と呼ぶ。測度$\\mu$が与えられた場合は測度空間と呼び、特に測度$\\mu$が確率である場合は確率空間と呼ぶ。\n同じ概念だが、数学ではシグマ代数、統計学ではシグマ場と呼ばれることに注意。\nカラテオドリの条件: $E \\subset \\mathbb{R}$が$A \\subset \\mathbb{R}$に対して$m^{ \\ast }(A) = m^{ \\ast } ( A \\cap E ) + m^{ \\ast } ( A \\cap E^{c} )$を満たす場合、$E$を可測集合と呼び、$E \\in \\mathcal{M}$と表記する。\n\u0026lsquo;可測集合\u0026rsquo;は、文字通り測ることができる集合を意味する。外測度の単調性から $$m^{ \\ast }(A) \\le m^{ \\ast } ( A \\cap E ) + m^{ \\ast } ( A \\cap E^{c} )$$ は自明であるので、ある集合が可測かどうかを確認することは、 $$m^{ \\ast }(A) \\ge m^{ \\ast } ( A \\cap E ) + m^{ \\ast } ( A \\cap E^{c} )$$ であるかを確認することと同じである。\n可測集合の集合のシグマ代数 上記の定義から、$X = \\mathbb{R}$の可測集合の集合である$\\mathcal{M}$は、以下の性質を持つシグマ代数となる。\n$\\mathcal{M}$は、以下の性質を持つシグマ代数である。\n[1]: $$ \\emptyset \\in \\mathcal{M} $$ [2]: $$ E \\in \\mathcal{M} \\implies E^{c} \\in \\mathcal{M} $$ [3]: $$ \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{M} \\implies \\bigcup_{n=1}^{\\infty} E_{n} \\in \\mathcal{M} $$ [4]: $$ \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{M} \\implies \\bigcap_{n=1}^{\\infty} E_{n} \\in \\mathcal{M} $$ [5]: $$ \\mathcal{N} \\subset \\mathcal{M} $$ [6]: $$ \\mathcal{I} \\subset \\mathcal{M} $$ [7]: $E_{i} , E_{j} \\in \\mathcal{M}$とすると、以下が成立する。 $$ E_{i} \\cap E_{j} = \\emptyset , \\forall i \\ne j \\implies m^{ \\ast } \\left( \\bigcup_{n=1}^{\\infty} E_{n} \\right) = \\sum_{n = 1} ^{\\infty} m^{ \\ast } ( E_{n}) $$ $\\mathcal{I}$はすべての区間の集合、$\\mathcal{N}$はすべての零集合の集合である。 特に[7]は、ルベーグが夢見た「長さの一般化」に絶対に必要な性質であることに注目してください。\n","id":490,"permalink":"https://freshrimpsushi.github.io/jp/posts/490/","tags":null,"title":"シグマ代数と可測空間"},{"categories":"위상수학","contents":"定義 1 位相空間 $\\left( X, \\mathscr{T} \\right)$ に対して $A \\subset X$ としよう。\n$X$ の開集合からなる集合 $\\mathscr{O} \\subset \\mathscr{T}$ が次を満たす場合、$\\mathscr{O}$ を $A$ の開被覆Open Coveringという。 $$ A \\subset \\bigcup_{O \\in \\mathscr{O}} O $$ $\\mathscr{O} ' \\subset \\mathscr{O}$ である $\\mathscr{O} ' $ を $\\mathscr{O}$ の部分被覆Subcoverという。特に $\\mathscr{O} ' $ の基数が自然数である場合は、有限部分被覆Finite Subcoverという。 $X$ の全ての開被覆が有限部分被覆を持つ場合、$X$ はコンパクトであるという。言い換えると、全ての開被覆 $\\mathscr{O}$ に対して、次を満たす有限集合 $\\mathscr{O} ' = \\left\\{ O_{1} , \\cdots , O_{n} \\right\\} \\subset \\mathscr{O}$ が存在する場合、$X$ はコンパクトである。 $$ X = \\bigcup_{i=1}^{n} O_{i} $$ $A$ が $X$ の部分空間としてコンパクトである場合、$A$ をコンパクトであるという。 位相空間 $X$ とする。部分集合 $K \\subset X$ の閉包 $\\overline{K}$ がコンパクトである場合、$K$ はプリコンパクトである、あるいは相対的にコンパクトrelatively compactであると言う。 説明 コンパクト 解析概論でコンパクトという条件がどれほど有用であったかを考えれば、その一般化を追求することは当然と言えるだろう。一般化されると、言葉は少し難しくなるが、本質的な部分は変わらない。\n実際に、コンパクトはさまざまな理論で非常に重要な応用を持つ。ある集合がコンパクトであるということは、有限の部分に分割して考えることができるということであり、厳密性が要求される証明では良い条件となる。逆に言えば、ある定理を証明する際に現れる集合 $A$ が本当にコンパクトであるかを示すことが鍵となる場合が多い。\nプリコンパクト プリコンパクトは、$K$ 自体はコンパクトではないが、$K$ に閉包を取るとコンパクトになるという点で、「まだコンパクトではないが、すぐにコンパクトになり得る」という概念をよく表している。距離空間では完全有界空間とも呼ばれ、別名相対的コンパクトは、閉じられた性質が相対的なものから来ていることを表す表現である。$K$ を $X$ の部分空間ではなく、それ自体で全体空間とした場合、$K$ は $K$ で閉じているため、$K = \\overline{K}$ であり、したがって$\\overline{K}$ がコンパクトであるということは、$K$ が（相対的に）コンパクトであるということになる。\n一方、数列によるプリコンパクトの定義も可能である。その定義は次のようである：\n$K \\subset X$ がプリコンパクトであるとは、$K$ で定義された全ての数列 $\\left\\{ x_{n} \\right\\} \\subset K$ に対して、$x \\in X$ に収束する部分数列 $\\left\\{ x_{n '} \\right\\} \\subset \\left\\{ x_{n} \\right\\}$ が存在することである。\n数式で再表現すると次のようになる：\n$$ K : \\text{precompact} \\iff \\forall \\left\\{ x_{n} \\right\\} \\subset K, \\exists \\left\\{ x_{n '} \\right\\} \\subset \\left\\{ x_{n} \\right\\} : x_{n '} \\to x \\in X \\text{ as } n \\to \\infty $$\n特に条件で $x \\in X$ ではなく $x \\in K$ の場合、$K$ を点列コンパクトSequentially Compactと呼ぶ。\n定理 [1]: $A$ がコンパクトであることは、$A$ の全ての開被覆が有限部分被覆を持つことと同値である。 [2]: コンパクト集合 $K$ の部分集合 $F$ が閉集合である場合、$F$ はコンパクト集合である。 [3]: $X$ がコンパクトであることは、$X$ の閉集合のみを含む全ての集合族が有限交差性を持ち、それを単に交差させても空集合にならないことと同値である。 証明 [1] $\\Gamma$ は指標集合である。\n$( \\implies )$\n$A \\subset X$ がコンパクトであり、$\\mathscr{U} := \\left\\{ U_{\\alpha} : \\alpha \\in \\Gamma \\right\\}$ が $A$ の開被覆であるとする。すると、$U_{\\alpha} \\cap A$ は $X$ の部分空間 $A$ で開集合であり、$\\mathscr{O} := \\left\\{ U_{\\alpha} \\cap A : U_{\\alpha} \\in \\mathscr{U} \\right\\}$ は $A$ の開被覆となる。$A$ はコンパクトであるため、$\\displaystyle A \\subset \\bigcup_{i=1}^{n} \\left( U_{\\alpha_{i}} \\cap A \\right)$ を満たす $\\alpha_{1} , \\cdots , \\alpha_{n} \\in \\Gamma$ が存在する。したがって、$\\left\\{ U_{\\alpha_{1}} , \\cdots , U_{\\alpha_{n}} \\right\\}$ が $\\mathscr{U}$ の有限部分被覆として存在することが確認できる。\n$( \\impliedby )$\n$A$ で開集合からなる開被覆 $\\mathscr{O} := \\left\\{ O_{\\alpha} : O_{\\alpha} \\text{ is open in } A, \\alpha \\in \\Gamma \\right\\}$ を考える。$O_{\\alpha}$ が $A$ で開集合であるため、各 $\\alpha \\in \\Gamma$ に対して $U_{\\alpha} \\cap A = O_{\\alpha}$ を満たす開集合 $U_{\\alpha}$ が存在する。これらの集合 $\\mathscr{U} := \\left\\{ U_{\\alpha} : \\alpha \\in \\Gamma \\right\\}$ は $A$ の開被覆である。全ての開被覆が有限部分被覆 $\\left\\{ U_{\\alpha_{1}} , \\cdots , U_{\\alpha_{n}} \\right\\}$ を持つという仮定から、$\\left\\{ O_{\\alpha_{1}} , \\cdots , O_{\\alpha_{n}} \\right\\}$ は $\\mathscr{O}$ の有限部分被覆となる。\n■\n[2] $$ F \\subset K \\subset X $$ $F$ が $X$ で閉集合であり、$K$ がコンパクトであるとする。$F$ は閉集合であるため、$F^{c}$ は $X$ で開集合であり、$K \\subset F^{c}$ であるため、$F^{c} \\cup \\left\\{ U_{\\alpha} \\right\\}$ は $K$ の開被覆の一つとなり、$K$ がコンパクトであるため、$F \\subset K \\subset \\Phi$ を満たす $F^{c}\\cup \\left\\{ U_{\\alpha} \\right\\}$ の有限部分被覆 $\\Phi$ が存在する。\nもし $F^{c}\\notin \\Phi$ であれば、$\\Phi$ は $\\left\\{ U_{\\alpha} \\right\\}$ の有限部分被覆となるため、$F$ はコンパクトである。 もし $F^{c}\\in \\Phi$ であれば、$\\Phi \\setminus \\left\\{ F^{c} \\right\\}$ が $\\left\\{ U_{\\alpha} \\right\\}$ の有限部分被覆となるため、$F$ はコンパクトである。 どちらの場合も、$F$ はコンパクトであるため、$F$ はコンパクトである。\n■\n[3] 戦略：言葉が非常に複雑なので、言葉を理解することが鍵である。$\\mathscr{C}$ が有限交差性を持つとしても、$\\displaystyle \\bigcap_{C \\in \\mathscr{C}} C \\ne \\emptyset$ が保証されるわけではなく、コンパクトという条件が必要である。一方で、コンパクトの定義では開集合の和集合を考慮しており、この定理では閉集合の交差を考慮している点に注意する必要がある。これらの考察から、有限交差性がどのようにコンパクトと関連しているのかを感じ取り、証明に入る必要がある。\n$\\Gamma$ は指標集合である。\n有限交差性: $X$ の部分集合からなる集合族 $\\mathscr{A} \\subset \\mathscr{P}(X)$ が有限交差性(f.i.p, finite intersection property)を持つとは、$\\mathscr{A}$ の全ての有限部分集合 $A \\subset \\mathscr{A}$ が交差を取ったときに空集合でないことである。数式で表すと、次のようになる。 $$ \\forall A \\subset \\mathscr{A}, \\bigcap_{a \\in A} a \\ne \\emptyset $$\n$( \\implies )$\n$X$ がコンパクトであり、$\\mathscr{C} := \\left\\{ C_{\\alpha} : C_{\\alpha} \\text{ is closed in } X, \\alpha \\in \\Gamma \\right\\}$ が有限交差性を持つとする。ここで、$\\displaystyle \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} = \\emptyset$ と仮定し、$\\mathscr{O} := \\left\\{ X \\setminus C_{\\alpha} : C_{\\alpha} \\in \\mathscr{C} \\right\\}$ を選ぶ。すると、 $$ \\begin{align*} \\bigcup_{\\alpha \\in \\Gamma} ( X \\setminus C_{\\alpha}) =\u0026amp; X \\setminus \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} \\\\ =\u0026amp; X \\setminus \\emptyset \\\\ =\u0026amp; X \\end{align*} $$ となるため、$\\mathscr{O}$ は $X$ の開被覆となる。$X$ がコンパクトであるため、$\\mathscr{O}$ は有限部分被覆 $\\displaystyle \\left\\{ (X \\setminus C_{\\alpha_{1}}) , \\cdots ,(X \\setminus C_{\\alpha_{n}}) \\right\\}$ を持つ。これは、つまり $$ X = \\bigcup_{i=1}^{n} ( X \\setminus C_{\\alpha_{i}}) = X \\setminus \\bigcap_{i=1}^{n} C_{\\alpha_{i}} $$ となり、$\\displaystyle \\bigcap_{i=1}^{n} C_{\\alpha_{i}} = \\emptyset$ であることを意味する。これは、$\\mathscr{C}$ が有限交差性を持つという仮定に矛盾する。したがって、$\\displaystyle \\bigcap_{C \\in \\mathscr{C}} C \\ne \\emptyset$ でなければならない。\n$( \\impliedby )$\n$X$ の開被覆 $\\mathscr{O} := \\left\\{ O_{\\alpha} : O_{\\alpha} \\text{ is open in } A, \\alpha \\in \\Gamma \\right\\}$ と $\\mathscr{C} := \\left\\{ X \\setminus O_{\\alpha} : O_{\\alpha} \\in \\mathscr{O} \\right\\}$ を考える。 $$ \\begin{align*} \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} \u0026amp;= \\bigcap_{\\alpha \\in \\Gamma} ( X \\setminus O_{\\alpha}) \\\\ =\u0026amp; X \\setminus \\bigcup_{\\alpha \\in \\Gamma} O_{\\alpha} \\\\ =\u0026amp; X \\setminus X \\\\ =\u0026amp; \\emptyset \\end{align*} $$ となるため、対偶により、$\\mathscr{C}$ は有限交差性を持たない。これは、言い換えると、$\\displaystyle \\bigcap_{i=1}^{n} C_{\\alpha_{i}} = \\emptyset$ を満たす $C_{\\alpha_{1}} , \\cdots , C_{\\alpha_{n}} \\in \\mathscr{C}$ が存在することを意味する。すると、\n$$ \\begin{align*} X \\setminus \\bigcup_{i=1}^{n} O_{i} =\u0026amp; X \\setminus \\bigcup_{i=1}^{n} (X \\setminus C_{i}) \\\\ =\u0026amp; X \\setminus \\left( X \\setminus \\bigcap_{i=1}^{n} C_{i} \\right) \\\\ =\u0026amp; \\bigcap_{i=1}^{n} C_{i} \\\\ =\u0026amp; \\emptyset \\end{align*} $$ となるため、$\\displaystyle X = \\bigcup_{i=1}^{n} O_{i}$ である。つまり、開被覆 $\\mathscr{O}$ に対して有限部分被覆が存在するため、コンパクトである。\n■\n関連項目 距離空間におけるコンパクト Munkres. (2000). Topology(2nd Edition): p164.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":489,"permalink":"https://freshrimpsushi.github.io/jp/posts/489/","tags":null,"title":"位相空間におけるコンパクトとプレコンパクトとは？"},{"categories":"위상수학","contents":"$(X, \\mathscr{T}_{X} )$と$(Y, \\mathscr{T}_{Y} )$の位相空間で、$f: X \\to Y$とする。$f(a)$を含む全ての$V \\in \\mathscr{T}_{Y}$に対して、$f(U) \\subset V$を満たしながら$a$を含む$U \\in \\mathscr{T}_{X}$が存在する場合、$f$を$a$で連続Continuousという。$f$が$X$の全ての点で連続であれば、連続関数といい、$f \\in C(X,Y)$と表すことができる。\nExplanation Initially, this definition might seem hard to understand, but if you think about it, it\u0026rsquo;s exactly the same sense as talking about the existence of $\\delta$ satisfying $\\left| x - a \\right| \\lt \\delta \\implies \\left| f(x) - f(a) \\right| \\lt \\epsilon$ whenever $\\epsilon \u0026gt; 0$ is given, as when defining continuity in analysis. Noting that $\\left\\{ x : \\left| x - a \\right| \\lt \\delta \\right\\}$ and $\\left\\{ f(x) : \\left| f(x) - f(a) \\right| \\lt \\epsilon \\right\\}$ are open sets, it should be understood that the statement of being able to find $\\delta$ whenever $\\epsilon$ is given, is the same as saying that an open set satisfying the condition in $X$ can be found whenever an open set is given in $Y$.\nFor example, $C(X,Y)$ is the set of continuous functions whose domain is $X$ and codomain is $Y$. If you\u0026rsquo;re studying Topology, you\u0026rsquo;re probably very accustomed to the epsilon-delta argument, preferring formulas and symbols over text.\nContinuity has been generalized from the Euclidean space to the metric space, and now beyond to the topological space. If the reason for discussing continuity in analysis is differentiation, then in Topology, the concept of continuity is necessary for discussing homeomorphism.\nThe following are various equivalent conditions for continuity points and continuous functions. As these are equivalent conditions, some textbooks might even use these as definitions.\nEquivalent Conditions for Continuity Points If $a \\in X$, then the following propositions are equivalent.\n(1): $f : X \\to Y$ is continuous at $a$. (2): For every neighborhood $V \\in \\mathscr{T}_{Y}$ containing $f(a)$, there exists a neighborhood $ U \\in \\mathscr{T}_{X}$ satisfying $a \\in U \\subset f^{-1} (V)$. (3): For all $\\mathcal{N} ( f(a) )$, $f^{-1} ( \\mathcal{N} ( f(a) ) )$ is a neighborhood of $a$. (4): For all $V \\subset Y$ satisfying $f(a) \\in V^{\\circ}$, $a \\in (f^{-1} (V))^{\\circ} $ For reference, $\\mathcal{N} (a)$ is called the neighborhoodNeighborhood of $a$ in $X$, as an open set containing $a$.\nEquivalent Conditions for Continuous Functions The following propositions are equivalent.\n[1]: $f : X \\to Y$ is a continuous function. [2]: For every neighborhood $V \\in \\mathscr{T}_{Y}$ containing $f(a)$ and all points $a \\in f^{-1} (V)$, there exists a $ U_{a} \\in \\mathscr{T}_{X}$ satisfying $a \\in U_{a} \\subset f^{-1} (V)$. [3]: For every open set $V \\subset Y$, $f^{-1} (V)$ is an open set in $X$1. [4]: For every closed set $C \\subset Y$, $f^{-1} (C)$ is a closed set in $X$. [5]: For all $A \\subset X$, $f( \\overline{A} ) \\subset \\overline{ f(A) } $ [6]: For all $B \\in \\mathscr{B}$, there exists a basis $\\mathscr{B}$ of $\\mathscr{T}_{Y}$ satisfying $f^{-1} (B) \\in \\mathscr{T}_{X}$. [7]: For all $S \\in \\mathscr{S}$, there exists a sub-basis $\\mathscr{S}$ of $\\mathscr{T}_{Y}$ satisfying $f^{-1} (S) \\in \\mathscr{T}_{X}$. [8] Composition of Continuous Functions: If $f : X \\to Y$ and $g : Y \\to Z$ are continuous functions, then the composition function $g \\circ f : X \\to Z$ is also continuous. Another Definition of Continuous Functions Especially, \u0026lsquo;Theorem [3]: For every open set $V \\subset Y$, $f^{-1} (V)$ is an open set in $X$\u0026rsquo; is very frequently used and is often defined as continuous functions by [3] itself. It\u0026rsquo;s not necessary to memorize all the conditions listed above, but it\u0026rsquo;s crucial to remember [3] and be able to use it whenever needed.\nMunkres. (2000). Topology(2nd Edition): p102.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":432,"permalink":"https://freshrimpsushi.github.io/jp/posts/432/","tags":null,"title":"位相数学での連続성"},{"categories":"위상수학","contents":"定義 位相空間 1 集合 $X$ が与えられた時、$\\mathscr{T} \\subset \\mathscr{P} (X)$ が $T \\in \\mathscr{T}$ に対して次の三つの条件を満たすなら、$\\mathscr{T}$ を $X$ の位相と呼び、$\\left( X , \\mathscr{T} \\right)$ を位相空間と呼ぶ。\n(i): $$\\emptyset , X \\in \\mathscr{T}$$ (ii): $$\\displaystyle \\bigcup_{ \\alpha \\in \\forall } T_{\\alpha} \\in \\mathscr{T}$$ (iii): $$\\displaystyle \\bigcap_{ i= 1}^{n} T_{i} \\in \\mathscr{T}$$ 条件 (i)~(iii) を言葉で説明すると以下の通り:\n(i): $\\mathscr{T}$ は空集合 $\\emptyset$ と全集合 $X$ を含む。 (ii): $\\mathscr{T}$ の元の和集合は $\\mathscr{T}$ に属する。 (iii): $\\mathscr{T}$ の元の有限交わりは $\\mathscr{T}$ に属する。 開集合と閉集合 2 $O \\in \\mathscr{T}$ を開集合と定義する。 $C \\subset X$ に対して $ X \\setminus C \\in \\mathscr{T}$ ならば $C$ を閉集合と定義する。 開集合であり、かつ、閉集合であるならば開かつ閉な集合と言う。 説明 位相空間 定義上、$\\mathscr{T}$ は $\\cup$ と $\\cap$ に対して閉じていて、代数的な考えが思い浮かぶかもしれないが、この定義だけでは代数的な性質を見つけるのが難しい。\n高校で習ったように「力と方向を持つ量」ではなく、条件を満たせばベクトルになるように、位相空間の位相も単に条件を満たす部分集合の集合として一般化されたものである。\n開集合と閉集合 位相を定義すると同時に、開けることと閉じることも新たに定義される。従来の距離空間では、開区間と閉区間の概念から続くものとして直感的に定義されていたが、一般的な位相では集合を使うため、抽象的で奇妙な空間を生み出すことができる。\n定義を見ると、開けることは完全に位相の概念を借りて新たに定義されているが、閉じることは距離空間でほとんど同じであることがわかる。\n位相と開けること、閉じることの定義に従って、以下の性質を簡単に確認できる。\n定理 [1-1]: $\\displaystyle \\bigcup_{ \\alpha \\in \\forall } O_{\\alpha} \\in \\mathscr{T}$ は開集合である。 [1-2]: $\\displaystyle \\bigcap_{ i= 1}^{n} O_{i} \\in \\mathscr{T}$ は開集合である。 [2-1]: $\\displaystyle \\bigcap_{ \\alpha \\in \\forall } C_{\\alpha} \\in \\mathscr{T}$ は閉集合である。 [2-2]: $\\displaystyle \\bigcup_{ i= 1}^{n} C_{i} \\in \\mathscr{T}$ は閉集合である。 [3]: $\\emptyset$ と $X$ は開集合であり、かつ、閉集合である。 例 以下の例を通して位相についての感覚を掴んでみよう。\n$X:=\\left\\{ a,b,c,d \\right\\}$ に対して $\\mathscr{T} : = \\left\\{ \\emptyset , \\left\\{ b \\right\\} , \\left\\{ a, b \\right\\} , \\left\\{ b,c \\right\\} , \\left\\{ a,b,c \\right\\} , \\left\\{ a,b,c,d \\right\\} \\right\\}$ が $X$ の位相であることを示せ。\n(i): $\\emptyset \\in \\mathscr{T}$ であり、$\\left\\{ a,b,c,d \\right\\} =X \\in \\mathscr{T}$ である。 (ii): 全集合 $X$ を除いては、$d$ は使われず、$\\left\\{ a,b,c \\right\\} \\in \\mathscr{T}$ である。 (iii): 空集合 $\\emptyset$ を除いては、すべて $b$ を共有し、$\\left\\{ b \\right\\} \\in \\mathscr{T}$ である。 ■\nMunkres. (2000). Topology(2nd Edition): p76.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (2000). Topology(2nd Edition): p93.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":398,"permalink":"https://freshrimpsushi.github.io/jp/posts/398/","tags":null,"title":"位相空間とは？"},{"categories":"거리공간","contents":"定義 距離空間 $\\left( X, d \\right)$ とするとき、$a \\in X$ であり、$r \u0026gt; 0$ とする。\n中心が $a$ で、半径が $r$ の開いた球Open Ballを $B_{d} (a,r) = \\left\\{ x \\in X \\ | \\ d(a,x) \u0026lt; r \\right\\}$ という。 中心が $a$ で、半径が $r$ の閉じた球Closed Ballを $B_{d} [a,r] = \\left\\{ x \\in X \\ | \\ d(a,x) \\le r \\right\\}$ という。 $O \\subset X$ が開いた球の和集合なら、$O$ を $X$ での開集合Open Setという。 $C \\subset X$ に対し、$X \\setminus C$ が開集合なら、$C$ を $X$ での閉集合Closed Setという。 説明 開集合と閉集合は別に定義できるが、本質的には同じだ。\n球とは区間、開区間、閉区間を一般化した概念であり、区間も $1$次元の球だと考えると、これは自然な話だ。もちろん、ユークリッド空間 $\\mathbb{R}$ の次元に対する一般化に止まらず、距離がきちんと与えられればどこでもしっかり定義される。\n開集合と閉集合は一般的に以下の性質を満たす。\n性質 全空間 $X$ 上の開集合を $O_{\\alpha}$ 、閉集合を $C_{\\alpha}$ とする。\n[1]: $X$ と $\\emptyset$ は開いていると同時に閉じている。 [2]: 開集合の和集合 $\\displaystyle \\bigcup_{\\alpha \\in \\forall} O_{\\alpha}$ は $X$ で開集合である。 [3]: 開集合の有限交差 $\\displaystyle \\bigcap_{i = 1}^{n} O_{i} $ は $X$ で開集合である。 [4]: 閉集合の交差 $\\displaystyle \\bigcap_{\\alpha \\in \\forall} C_{\\alpha}$ は $X$ で閉集合である。 [5]: 閉集合の有限和集合 $\\displaystyle \\bigcup_{i = 1}^{n} C_{i}$ は $X$ で閉集合である。 [3]で有限という条件がなければ、$\\displaystyle \\bigcap_{n = 1}^{ \\infty } \\left( -{{1} \\over {n}} , {{1} \\over {n}} \\right) = \\left\\{ 0 \\right\\}$ という反例が出せる。**[5]**で有限という条件がなければ、$\\displaystyle \\bigcup_{n = 1}^{ \\infty } \\left[ 0 , 1-{{1} \\over {n}} \\right] = [ 0 , 1 )$ という反例が出せる。\n証明 [1] このポストで紹介されている。\n[2]~[5] このポストで紹介されている。\n","id":382,"permalink":"https://freshrimpsushi.github.io/jp/posts/382/","tags":null,"title":"距離空間での球と開集合閉集合"},{"categories":"거리공간","contents":"定義 集合 set $X$ に対して、関数 $d : X \\times X \\to [0, \\infty)$が $x,y,z \\in X$ について以下の条件を満たす場合、$d$ を距離metricと呼び、$\\left( X, d\\right)$を距離空間metric spaceという。距離が自明の場合、簡単に $X$ と記されることもある。\n$d(x,y)=0 \\iff x = y$\n$d(x,y) = d(y,x)$\n$d(x,y) + d(y,z) \\ge d(x,z)$\n説明 線型代数学でノルムの概念を理解したなら、大きさや距離が直感的にのみ定義される必要はないことがわかるだろう。以下の3つの例は特に $\\mathbb{R}^{n}$上で定義され、前述のように、線型代数学で見たノルムと大きく変わらない。これは、ノルム $\\left\\| \\cdot \\right\\|$がどのように定義されても常に距離 $d ( \\mathbf{x} , \\mathbf{y} ) := \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|$ を定義できるためで、ある種のノルムがあればそれに対応する距離も存在する。\n例 $\\mathbf{x} = (x_{1} , x_{2} , \\cdots , x_{n} )$ そして $\\mathbf{y} = (y_{1} , y_{2} , \\cdots , y_{n} ) $ とする。\nユークリッド距離: $d(\\mathbf{x} , \\mathbf{y}) = \\sqrt{ \\sum \\limits_{i = 1}^{n} (x_{i} - y_{i} )^2 }$\nタクシーキャブ距離: $d^{\\prime}(\\mathbf{x} , \\mathbf{y}) = \\sum \\limits_{i = 1}^{n} | x_{i} - y_{i} |$\nマックス距離: $d^{\\prime \\prime}(\\mathbf{x} , \\mathbf{y}) = \\max \\left\\{ | x_{i} - y_{i} | \\right\\}_{i=1}^{n}$\n基本的な解析学では、主に $\\mathbb{R}^{1}$ を扱い、ユークリッド距離だけが使われると考えても良い。解析学に限って言えば、距離空間について詳細に学ぶ必要はなく、実数集合 $\\mathbb{R}$ を距離空間 $\\left( \\mathbb{R} , d \\right)$として受け入れるだけで十分だ。以下の二つの例は、ユークリッド空間を超えた距離の概念だ。\n離散距離:\n$$ d_{0} (x,y) = \\delta_{xy} = \\begin{cases}1, \u0026amp; \\ x \\ne y \\\\ 0, \u0026amp; \\ x = y \\end{cases} $$\n離散距離はクロネッカーのデルタを使用し、二つの要素が同じかどうかだけで判断する。三角不等式を満たしているかは、場合分けをして簡単に証明できる。\n積分距離:\n$$ \\rho (f,g) = \\int_{a}^{b} | f(x) - g(x) | dx $$\n積分距離は連続関数の集合 $C[a,b]$ で定義できる距離だ。二つの関数のグラフが完全に同じであれば、その値は $0$ になる。\n図で示すと、実線で囲まれた部分がちょうど $\\rho (f,g)$ になる。\nこれらの定義から、metric は従来の意味での‘距離’よりも、二つの間の‘距離感’として理解する方が適していることがわかる。完全に同じものは必ず $0$ であるため、‘無限大にどれだけ近いか’ではなく、‘$0$ からどれだけ遠いか’が重要だ。より抽象的な思考のために、距離が大きくなるほど‘場所’が遠くなるという直感的な考えから離れよう。\n","id":381,"permalink":"https://freshrimpsushi.github.io/jp/posts/381/","tags":null,"title":"距離空間の定義"},{"categories":"선형대수","contents":"定義1 ベクトル空間 $V$ の 基底 の要素（ベクトルの数）を $V$ の 次元dimension と定義し、以下のように表記する。\n$$ \\dim (V) $$\n説明 このような次元の一般化は、単にベクトル空間に対する探求を超えて、この社会を支える様々な技術に応用されている。世界が $3$ 次元で、描けもしない $4$ 次元が何の役に立つのかと思うかもしれないが、ユークリッド空間だけがベクトル空間ではないからである。例えば、統計学で使われる データセットを考えると、それをベクトルとして見ることができる。例えば、「アダム」という人が身長が175、体重が62、年齢が22、IQが103、視力が1.2であるとすると、「アダム=(175,62,22,103,1.2)」と表せるのである。こんな単純なデータでさえ、既に $5$ 次元を使用しており、些細な制限があると役に立たなくなる。\n一方で、ベクトル空間の基底が一意ではないことを考慮すると、上記の定義が妥当な定義であるためには、すべての基底が同じ数の要素を持つ必要があるという条件が必要である。以下の二つの定理から、有限次元ベクトル空間のすべての基底が同じ数のベクトルを持たなければならないことがわかる。\n定理 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots \\mathbf{v}_{n} \\right\\}$ をベクトル空間 $V$ の任意の基底とする。\n(a) 基底よりもベクトルの数が多い $V$ の部分集合は 線形従属である。\n(b) 基底よりもベクトルの数が少ない $V$ の部分集合は、$V$ を 生成できない。\n証明2 (a) $W=\\left\\{ \\mathbf{w}_{1},\\ \\mathbf{w}_{2},\\ \\cdots ,\\ \\mathbf{w}_{m} \\right\\} \\subset V$ とすると、$m \u0026gt; n$ である。$S$ が $V$ の基底であるので、$W$ の要素は $S$ のベクトルの 線形結合 で表すことができる。\n$$ \\begin{equation} \\begin{aligned} \\mathbf{w}_{1} \u0026amp;= a_{11}\\mathbf{v}_{1}+a_{21}\\mathbf{v}_{2} + \\cdots + a_{n1}\\mathbf{v}_{n}=\\sum \\limits _{i}^{n} a_{i1}\\mathbf{v}_{i} \\\\ \\mathbf{w}_{2} \u0026amp;= a_{12}\\mathbf{v}_{1}+a_{22}\\mathbf{v}_{2} + \\cdots + a_{n2}\\mathbf{v}_{n}=\\sum \\limits _{i}^{n} a_{i2}\\mathbf{v}_{i} \\\\ \u0026amp; \\vdots \\\\ \\mathbf{w}_{m} \u0026amp;= a_{1m}\\mathbf{v}_{1}+a_{2m}\\mathbf{v}_{2} + \\cdots + a_{nm}\\mathbf{v}_{n}=\\sum \\limits _{i}^{n} a_{im}\\mathbf{v}_{i} \\end{aligned} \\label{wlincom1} \\end{equation} $$\n$W$ が線形従属であることを示すために、\n$$ \\begin{equation} k_{1}\\mathbf{w}_{1} + k_2\\mathbf{w}_{2} + \\cdots + k_{m}\\mathbf{w}_{m}= \\mathbf{0} \\label{wlincom2} \\end{equation} $$\n$(k_{1},k_{2},\\dots,k_{m}) \\ne (0,0,\\dots,0)$ が存在することを示せばよい。$(1)$ を $(2)$ に代入すると、次のようになる。\n$$ \\begin{align*} \u0026amp;k_{1}(a_{11}\\mathbf{v}_{1} + a_{21}\\mathbf{v}_{2} + \\cdots + a_{n1}\\mathbf{v}_{n}) \\\\ + \u0026amp;k_2(a_{12}\\mathbf{v}_{1} + a_{22}\\mathbf{v}_{2} + \\cdots + a_{n2}\\mathbf{v}_{n}) \\\\ + \u0026amp;\\cdots \\\\ + \u0026amp;k_{m}(a_{1m}\\mathbf{v}_{1} + a_{2m}\\mathbf{v}_{2} + \\cdots + a_{nm}\\mathbf{v}_{n}) = \\mathbf{0} \\end{align*} $$\nこれを $\\mathbf{v}_{i}$ に対して整理すると、次のようになる。\n$$ \\left( \\sum \\limits _{j} ^{m} k_{j}a_{1j} \\right)\\mathbf{v}_{1} + \\left( \\sum \\limits _{j} ^{m} k_{j}a_{2j} \\right)\\mathbf{v}_{2} + \\cdots + \\left( \\sum \\limits _{j} ^{m} k_{j}a_{nj} \\right)\\mathbf{v}_{n} = \\mathbf{0} $$\nこの時、$S$ が $V$ の基底であり、線形独立であるため、上記の方程式を満たす解は、係数がすべて $0$ の場合のみである。したがって、次の方程式が成り立つ。\n$$ \\begin{align*} a_{11}k_{1} + a_{12}k_{2} + \\cdots + a_{1m}k_{m} = 0 \\\\ a_{21}k_{1} + a_{22}k_{2} + \\cdots + a_{2m}k_{m} = 0 \\\\ \\vdots \\\\ a_{n1}k_{1} + a_{n2}k_{2} + \\cdots + a_{nm}k_{m} = 0 \\end{align*} $$\n連立方程式を見ると、方程式の数は $n$ 個、未知数 $k$ の数は $m$ 個である。方程式の数よりも未知数の数が多いため、この連立方程式は無数の非自明な解を持つ。したがって、$(2)$ を満たすすべてが $0$ ではない $k_{1},\\dots,k_{m}$ が存在する。したがって、$W$ は線形従属である。また、この証明は基底よりも要素の数が多い任意の集合にも適用される。\n■\n(b) 背理法で証明する。\n$W=\\left\\{ \\mathbf{w}_{1},\\ \\mathbf{w}_{2},\\ \\cdots ,\\ \\mathbf{w}_{m} \\right\\} \\subset V$ とすると、$m \u0026lt; n$ である。そして、$W$ が $V$ を生成すると仮定してみる。すると、$V$ のすべてのベクトルを $W$ の線形結合で表すことができる。\n$$ \\begin{equation} \\begin{aligned} \\mathbf{v}_{1} \u0026amp;= a_{11}\\mathbf{w}_{1}+a_{21}\\mathbf{w}_{2} + \\cdots + a_{m1}\\mathbf{w}_{m} \\\\ \\mathbf{v}_{2} \u0026amp;= a_{12}\\mathbf{w}_{1}+a_{22}\\mathbf{w}_{2} + \\cdots + a_{m2}\\mathbf{w}_{m} \\\\ \u0026amp; \\vdots \\\\ \\mathbf{v}_{n} \u0026amp;= a_{1n}\\mathbf{w}_{1}+a_{2n}\\mathbf{w}_{2} + \\cdots + a_{mn}\\mathbf{w}_{m} \\end{aligned} \\label{vlincom1} \\end{equation} $$\nすると、$\\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots \\mathbf{v}_{n} \\right\\}$ が線形従属であるという矛盾が生じる。次の同次方程式を見てみよう。\n$$ k_{1}\\mathbf{v}_{1} + k_2\\mathbf{v}_{2} + \\cdots + k_{n}\\mathbf{v}_{n}= \\mathbf{0} $$\nここに $(1)$ を代入すると、次のようになる。\n$$ \\begin{align*} \u0026amp;k_{1}(a_{11}\\mathbf{w}_{1} + a_{21}\\mathbf{w}_{2} + \\cdots + a_{m1}\\mathbf{w}_{m}) \\\\ + \u0026amp;k_2(a_{12}\\mathbf{w}_{1} + a_{22}\\mathbf{w}_{2} + \\cdots + a_{m2}\\mathbf{w}_{m}) \\\\ + \u0026amp;\\cdots \\\\ + \u0026amp;k_{n}(a_{1n}\\mathbf{w}_{1} + a_{2n}\\mathbf{w}_{2} + \\cdots + a_{mn}\\mathbf{w}_{m}) = \\mathbf{0} \\end{align*} $$\nこれを $\\mathbf{w}_{i}$ に対して整理すると、次のようになる。\n$$ \\left( \\sum \\limits _{j} ^{n} k_{j}a_{1j} \\right)\\mathbf{w}_{1} + \\left( \\sum \\limits _{j} ^{n} k_{j}a_{2j} \\right)\\mathbf{w}_{2} + \\cdots + \\left( \\sum \\limits _{j} ^{n} k_{j}a_{mj} \\right)\\mathbf{w}_{m} = \\mathbf{0} $$\nすると、未知数 $k$ に関する次の同次線形システムが得られる。\n$$ \\begin{align*} a_{11}k_{1} + a_{12}k_{2} + \\cdots + a_{1n}k_{n} = 0 \\\\ a_{21}k_{1} + a_{22}k_{2} + \\cdots + a_{2n}k_{n} = 0 \\\\ \\vdots \\\\ a_{m1}k_{1} + a_{m2}k_{2} + \\cdots + a_{mn}k_{n} = 0 \\end{align*} $$\n未知数の数が $n$ で、方程式の数が $m$ であり、$m \u0026lt; n$ であるので、この線形システムは無数の非自明な解を持つ。したがって、$S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots \\mathbf{v}_{n} \\right\\}$ が線形従属であるという結果が得られ、これは $S$ が線形独立であるという事実と矛盾する。したがって、仮定は間違っていることがわかる。よって、$W$ は $V$ を生成することができない。\n■\nHoward Anton, Elementary Linear Algebra: Applications Version (第12版, 2019), p248\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Applications Version (第12版, 2019), p252-253\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3018,"permalink":"https://freshrimpsushi.github.io/jp/posts/3018/","tags":null,"title":"ベクトル空間の次元"},{"categories":"추상대수","contents":"定義 1 モノイド $\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$ の元 $a$ と単位元 $e$ に対し、$a \\ast\\ a\u0026rsquo; = a\u0026rsquo; \\ast\\ a = e$ を満たす $a '$ が存在すれば、$\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$を群Groupと定義する。すなわち、群は以下の性質を満たす二項演算構造だ。\n(i): 演算に対して結合法則が成立する。 (ii): すべての元に対して単位元が存在する。 (iii): すべての元に対して逆元が存在する。 説明 マグマから始まり半群、モノイドを経て、ついに群に到達した。大したことないように見えるかもしれないが、マグマと比較すると、条件はかなり増えている。演算で閉じており、結合法則が成立し、単位元と逆元の存在が必要なので、何でもかんでも群とは言えなくなっている。\n群を研究する理由は、他の代数的構造よりもずっとシンプルで簡単だからだ。群より条件が少なければ有用な性質も少なくなり、群より条件が多ければ使い道が減る。\n代数学で興味を持つ代数的構造のほとんどは基本的に群に基づいており、代数学は純粋数学の数論から、暗号理論のような日常生活に溶け込んだ応用数学にまで応用されている。数学以外では、驚くべきことに物理学でも群論が使われている。\nモノイドになりながら、群にならない例を見てみよう。\n正方行列の集合 $\\mathbb{R}^{n \\times n}$ に対して、モノイド $\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$ は群ではない。\n行列 $A \\in \\mathbb{R}^{n \\times n}$ に対して、$\\det A = 0$ ならば、$A^{-1}$ が存在しない。 もちろん、集合に制限を加えれば、これも群になりえる。\n逆行列が存在する正方行列の集合 $\\text{GL}_{n} (\\mathbb{R}) = \\left\\{ A \\in \\mathbb{R}^{n \\times n} \\ | \\ \\det A \\ne 0 \\right\\}$ に対して、モノイド $\\left\u0026lt; \\text{GL}_{n} (\\mathbb{R}) , \\cdot \\right\u0026gt;$ は群である。\n$\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$ の部分モノイド $\\left\u0026lt; \\text{GL}_{n} (\\mathbb{R}) , \\cdot \\right\u0026gt;$ は、乗算の逆元を持つので、定義 $\\text{GL}_{n} (\\mathbb{R})$ により群である。 対称性？ 群の概念を掴むとき、よく対称性についての話から始まったり、完全に数学的な定義だけで築き上げられたりする。\n対称性の例としては、ルービックキューブを回したりそのままにしたり(単位元)、元に戻したり(逆元)することがよく言及される。しかし、このような説明は構造が対称性を持つものが群の構造を持つことを理解しやすいが、群の構造が対称性を持つことを把握するのは難しい。群の形で数学的な定義に従うと、対称とは逆元の概念を思い浮かべるのが良い。\n$a$ が存在するなら、群の定義により、それに対応する $a '$ が必ず存在する。\n一方で、$a '$ もそれに対応する $a\u0026rsquo;\u0026rsquo;=a$ が存在するので、このような関係から対称を考えるのは自然だと言える。モノイドと群の違いは逆元だけなので、概念と定義がより妥当に一致していることが確認される。\n対称の話が出たからには、対称にピッタリの群の例を見てみよう。\nモノイド $\\left\u0026lt; \\mathbb{Z} , + \\right\u0026gt;$ は群である。\n整数 $a$ に対して、$-a$ は常に $a + (-a) = 0$ を満たす逆元になる。 $1$ が存在すれば、単位元 $0$ を中心に対称の $-1$ が存在し、$-2$ に対しては $2$ が存在し\u0026hellip;$n$ に対しては $-n$ が存在する。対称という意味で考えれば、かなり自然な例だ。\n一般的に、群だけを扱う場合、群 $\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$ はただ $G$ と表し、操作は特に言及がなければ $\\cdot$ と書く。ただし$\\left\u0026lt; \\mathbb{Z} , + \\right\u0026gt;$ のように加算が明確なコンテキストでは、$+$ のように場合に応じて適切な操作を使う。\nFraleigh. (2003). \u0026ldquo;A first course in abstract algebra(7th Edition)\u0026rdquo;: p37.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":278,"permalink":"https://freshrimpsushi.github.io/jp/posts/278/","tags":null,"title":"抽象代数学における群"},{"categories":"선형대수","contents":"定義: 線形組み合わせ1 $\\mathbf{w}$をベクトル空間$V$のベクトルとする。もし$\\mathbf{w}$が$V$のベクトル$\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots ,\\mathbf{v}_{r}$と任意の定数$k_{1}, k_{2}, \\cdots, k_{r}$によって以下のように表されるなら、$\\mathbf{w}$は$\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots ,\\mathbf{v}_{r}$の線形組み合わせlinear combination、または一次組み合わせという。\n$$ \\mathbf{w} = k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + \\cdots + k_{r}\\mathbf{v}_{r} $$\nこの場合、定数$k_{1}, k_{2}, \\cdots, k_{r}$は線形組み合わせ$\\mathbf{w}$の係数coefficientsと呼ばれる。\n解説 式で示されると見慣れないかもしれないが、難しい概念ではない。2次元直交座標系でのベクトル表示は、まさに二つの単位ベクトル$\\hat{\\mathbf{x}} = (0,1)$と$\\hat{\\mathbf{y}} = (0,1)$の線形組み合わせである。\n$$ \\mathbf{v} = (v_{1}, v_{2}) = (v_{1},0)+(0,v_{2}) = v_{1}(1,0) + v_{2}(0,1) = v_{1}\\hat{\\mathbf{x}} + v_{2} \\hat{\\mathbf{y}} $$\n定理 $S = \\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\}$をベクトル空間$V$の空でない部分集合とする。すると、以下が成り立つ。\n(a) $S$の要素の全ての可能な線形組み合わせの集合を$W$としよう。$W$は$V$の部分空間である。\n(b) **(a)**の$W$は$S$を含む$V$の部分空間の中で最小の部分空間である。つまり、$W^{\\prime}$を$S$を含む$V$の部分空間とすると、次の式が成立する。\n$$ S \\subset W \\le W^{\\prime} $$\n証明 (a) $W$が加算とスカラー倍に対して閉じているかを確認するためには、部分空間判定法を適用する。\n$$ \\mathbf{u} = c_{1} \\mathbf{w}_{1} + c_{2} \\mathbf{w}_{2} + \\cdots + c_{r} \\mathbf{w}_{r}, \\quad \\mathbf{v} = k_{1} \\mathbf{w}_{1} + k_{2} \\mathbf{w}_{2} + \\cdots + k_{r} \\mathbf{w}_{r} $$\n(A1)\n$\\mathbf{u}+\\mathbf{v}$は以下のようになる。\n$$ \\mathbf{u} +\\mathbf{v} = ( c_{1} + k_{1} ) \\mathbf{w}_{1} + ( c_{2} + k_{2} ) \\mathbf{w}_{2} + \\cdots + ( c_{r} + k_{r} ) \\mathbf{w}_{r} $$\nこれは$\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r}$の線形組み合わせなので、$\\mathbf{u} + \\mathbf{v} \\in W$が成り立つ。\n(M1)\n任意の定数$k$に対して、$k\\mathbf{u}$は以下のようになる。\n$$ k\\mathbf{u} = ( k c_{1} ) \\mathbf{w}_{1} + ( k c_{2} ) \\mathbf{w}_{2} + \\cdots + ( k c_{r} ) \\mathbf{w}_{r} $$\nこれは$\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r}$の線形組み合わせなので、$k\\mathbf{u} \\in W$が成り立つ。\n結論\n$W$が加算とスカラー倍に対して閉じているので、部分空間判定法により、$W$は$V$の部分空間である。\n$$ W \\le V $$\n■\n(b) $W^{\\prime}$を$S$を含む$V$の部分空間とする。すると、$W^{\\prime}$は加算とスカラー倍に対して閉じているので、$S$の要素の全ての線形組み合わせは$W^{\\prime}$の要素である。従って、\n$$ W \\le W^{\\prime} $$\n■\n定義: 生成 定理の$W$は$S$によって生成されたspanned$V$の部分空間という。また、ベクトル$\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r}$が$W$を生成するspanと言い、以下のように表記される。\n$$ W = \\text{span}\\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\} \\quad \\text{or} \\quad W = \\text{span}(S) $$\n解説 生成という概念が必要な理由は、ある要素を含む最小の集合を考えるためである。実際、上の定理でこの点を確認することができる。さらに、$S$自体から重複する要素をすべて除くと、これはベクトル空間の基底になる。\n定理 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$と$S^{\\prime} = \\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\}$をベクトル空間$V$の空でない部分集合とする。すると、\n$$ \\text{span} \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\} = \\text{span} \\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\} $$\n必要十分条件は、$S$の全てのベクトルが$S^{\\prime}$のベクトルの線形組み合わせとして表され、$S^{\\prime}$の全てのベクトルが$S$のベクトルの線形組み合わせとして表されることである。\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p220-222\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":512,"permalink":"https://freshrimpsushi.github.io/jp/posts/512/","tags":null,"title":"線形結合、生成"},{"categories":"선형대수","contents":"定義1 $W$がベクトル空間$V$の空集合でない部分集合とする。$W$が$V$で定義された加算とスカラー乗算に対してベクトル空間の定義を満たす時、$W$をベクトル空間$V$の部分空間subspaceと呼び、以下のように表記する。\n$$ W \\le V $$\n説明 ベクトル空間$V$の部分集合$W$が$V$の部分空間かどうかを判断するためには、ベクトル空間になるための10の規則を全て満たさなければならない。ベクトル空間の部分集合を取るたびに10の規則を全て確認するのはかなり面倒で困難なことになるだろう。しかし幸いなことに、あるベクトル空間の部分集合である理由だけで、いくつかの規則は自明に成立する。\n例えば$\\mathbf{u},\\mathbf{v}$が$W$の要素であれば、同時に$V$の要素であるため、(A2), (A3), (M2)-(M5) は自然に成立する。したがって、加算に対する閉包性**(A1)、ゼロベクトルの存在(A4)、逆の存在(A5)、スカラー乗算に対する閉包性(M1)だけを確認すれば、$W$は部分空間であることがわかる。しかし実際にはもっと単純である。条件(A1)、(M1)**を満たすことが部分空間であるための同値条件となる。\n例 ベクトル空間$V$の部分空間の例には、以下のものがある。\n自分自身$V$ 剰余類$v + W$ 線形変換$T : V \\to W$に対して、\n$T$の零空間$N(T) \\le V$ $T$の値域$R(T) \\le W$ 線形変換$T : V \\to V$に対して、\n固有空間$E_{\\lambda}$ $T$-不変空間 $T$-循環空間 定理: 部分空間判定法 $W$をベクトル空間$V$の空集合でない部分集合とする。$W$が$V$の部分空間であることと$W$が以下の二つの条件を満たすことは必要十分条件である。\n(A1) 部分集合$W$が$V$で定義された加算に対して閉じている。\n(M1) 部分集合$W$が$V$で定義されたスカラー乗算に対して閉じている。\n証明 $(\\implies)$\n$W$が$V$の部分空間であると仮定する。$W$が部分空間であれば、ベクトル空間の定義により$W$が**(A1)、(M1)**を満たすことは自明である。\n$(\\impliedby)$\n$W$が$(A1)$、$(M1)$を満たすと仮定する。そして$\\mathbf{u} \\in W$とする。そうすると$W$はスカラー乗算に対して閉じていて、$0\\mathbf{u}=\\mathbf{0}$であるため、以下が成立する。\n$$ 0 \\mathbf{u} = \\mathbf{0} \\in W $$\n同じ理由で$(-1)\\mathbf{u}=-\\mathbf{u}$によって以下が成立する。\n$$ (-1)\\mathbf{u} = -\\mathbf{u} \\in W $$\nしたがって$W$は**(A1)-(M5)**を全て満たすので$V$の部分空間である。\n■\n定理: 部分空間の交差も部分空間である2 $W_{1}, W_2$をベクトル空間$V$の部分空間とする。すると$W_{1} \\cap W_2$も$V$の部分空間である。\n証明 部分空間判定法により、$W_{1} \\cap W_2$が**(A1)、(M1)**を満たしているか確認すればよい。$W= W_{1} \\cap W_2$とする。\n(A1)\n$W = W_{1} \\cap W_2$であるため、$W$内の任意の二つのベクトル$\\mathbf u,\\mathbf v$はそれぞれ$W_{1}$、$W_2$にも含まれている。$W_{1}, W_2$は部分空間であるため、加算に対して閉じている。したがって\n、以下が成立する。\n$$ \\mathbf u + \\mathbf v \\in W_{1}, \\quad \\mathbf u + \\mathbf v \\in W_2 $$\nしたがって、交差の定義により以下が成立する。\n$$ \\mathbf u + \\mathbf v \\in W $$\n$W$内の任意の二つのベクトル$\\mathbf u,\\ \\mathbf v$に対して、$\\mathbf u + \\mathbf v$も$W$の要素であるため、$W$は加算に対して閉じており、**(A1)**を満たす。\n(M1)\n上記の場合と同様に証明する。\n$W = W_{1} \\cap W_2$であるため、$W$内の任意のベクトル$\\mathbf u$は$W_{1}$、$W_2$にも含まれている。$W_{1},\\ W_2$は部分空間であるため、スカラー乗算に対して閉じている。したがって、あるスカラー$k$に対して以下が成立する。\n$$ k\\mathbf{u} \\in W_{1} \\quad k \\mathbf{u} \\in W_2 $$\nしたがって、交差の定義により以下が成立する。\n$$ k\\mathbf u \\in W $$\n$W$内の任意のベクトル$\\mathbf u$に対して、$k\\mathbf u$も$W$の要素であるため、$W$はスカラー乗算に対して閉じており、**(M1)**を満たす。\n結論\n$W_{1}, W_{2}$が部分空間の時、$W = W_{1} \\cap W_2$が**(A1)、(M1)**を満たすので、$W$も部分空間である。\n■\n従来の定理 $W_{1}, W_{2}, \\dots W_{n}$がベクトル空間$V$の部分空間とする。すると$W = W_{1} \\cap \\cdots \\cap \\dots W_{n}$も$V$の部分空間である。\nHoward Anton, 基礎線形代数: アプリケーションバージョン (第12版, 2019), p211-212\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, 基礎線形代数: アプリケーションバージョン (第12版, 2019), p216\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":285,"permalink":"https://freshrimpsushi.github.io/jp/posts/285/","tags":null,"title":"ベクトル空間の部分空間"},{"categories":"선형대수","contents":"定義1 空集合ではない集合 $V$ の要素が二つの演算 加算additionと スカラー乗算scalar multiplicationに対して下記の10個の規則を満たす時、$V$を体2 $\\mathbb{F}$に対するベクトル空間vector spaceまたは$\\mathbb{F}$-ベクトル空間と呼び、$V$の要素をベクトルvectorという。\n$\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V$と$k, l \\in \\mathbb{F}$に対して、\n(A1) $\\mathbf{u}, \\mathbf{v}$が$V$の要素であれば$\\mathbf{u}+\\mathbf{v}$も$V$の要素である。\n(A2) $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$\n(A3) $(\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})$\n(A4) $V$内の全ての$\\mathbf{u}$に対して、$\\mathbf{u} + \\mathbf{0} = \\mathbf{0} + \\mathbf{u} = \\mathbf{u}$を満たす$\\mathbf{0}$が$V$内に存在する。この時$\\mathbf{0}$を零ベクトルzero vectorと呼ぶ。\n(A5) $V$内の全ての$\\mathbf{u}$に対して$\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u} = \\mathbf{0}$を満たす$\\mathbf{v}$が$V$内に存在する。この時$\\mathbf{v}$を**$\\mathbf{u}$の負**negative of $\\mathbf{u}$と呼び、$\\mathbf{v} = -\\mathbf{u}$と表記する。\n(M1) $\\mathbf{u}$が$V$の要素であれば$k \\mathbf{u}$も$V$の要素である。\n(M2) $k(\\mathbf{u} + \\mathbf{v})=k\\mathbf{u} + k\\mathbf{v}$\n(M3) $(k+l)\\mathbf{u}=k\\mathbf{u}+ l\\mathbf{u}$\n(M4) $k(l\\mathbf{u})=(kl)(\\mathbf{u})$\n(M5) $1\\in \\mathbb{F}$に対して、$1\\mathbf{u} = \\mathbf{u}$\n説明 線形空間linear spaceという言葉も使われる。 当然ながらスカラー(体)が実数である必要はない。特に$\\mathbb{F} = \\mathbb{R}$の場合を実ベクトル空間real vector spaceと呼び、$\\mathbb{F} = \\mathbb{C}$の場合を複素ベクトル空間complex vector spaceと呼ぶ。\n数学部の線形代数学では主に$\\mathbb{R}^{n}$や$\\mathbb{C}^{n}$を扱う。$\\mathbb{R}^{n}$は実数$n$個の順序対を要素とするベクトル空間を意味し、即ち$n$次元ユークリッド空間を意味し、具体的に$\\mathbb{R}^{3}$は高校数学、微分積分学でよく見た3次元空間を意味する。\nベクトル空間となる集合は様々ある。関数の集合もベクトル空間となり得て、これを関数空間と呼ぶ。\n物理学では大きさと方向があるものをベクトルと呼ぶ。その概念を一般化したものが線形代数学のベクトルである。例えば大きさが$m\\times n$の実数行列を集めた集合$M_{m\\times n}(\\mathbb{R})$を考えると、$M_{m\\times n}(\\mathbb{R})$は上記の10個の規則を全て満たすことがわかる。したがって同じ大きさの行列を集めた集合はベクトル空間となり、各々の行列はその中でのベクトルとなる。このような抽象的なベクトル空間に初めて接したならば、行列もベクトルだという事実に驚くかもしれないが、これまでに座標空間のベクトルをどのように表記していたかを考えれば驚くこともない。\nある集合がベクトル空間であるかどうかを判断するには、上記の定義を満たしているか一つ一つ確かめればよい。一見ベクトル空間に思えるけれどもそうでない場合もあり、また一見ベクトル空間でなさそうに思えるけれども実はベクトル空間である場合もある。直感とは全く異なる場合があるので、問題を解く時は一つ一つしっかりと確認することが良い。また零ベクトル$\\mathbf{0}$とスカラー$0$は全く異なるものであるので、しっかり区別するようにしよう。通常、教科書ではベクトルは太字で表される。\n定理1 $V$をベクトル空間、$\\mathbf{u}$を$V$の要素とする。\n(1a) $V$の零ベクトルは唯一である。\n(1b) $\\mathbf{u}$の負は唯一である。\n証明 ベクトル空間の定義を利用した証\n明である。\n(1a) $\\mathbf{0},\\mathbf{0}^{\\prime}$が$V$の零ベクトルであるとする。するとベクトル空間の定義により次が成立する。\n$$ \\begin{align*} \\mathbf{0} \u0026amp;= \\mathbf{0} + \\mathbf{0}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\\\ \u0026amp;= \\mathbf{0}^{\\prime} + \\mathbf{0} \u0026amp;\u0026amp; \\text{by (A2)} \\\\ \u0026amp;= \\mathbf{0}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\end{align*} $$\nしたがって、二つの零ベクトルは互いに等しい。\n■\n(1b) $\\mathbf{v}, \\mathbf{v}^{\\prime}$が$\\mathbf{u}$の負であるとする。するとベクトル空間の定義により次が成立する。\n$$ \\begin{align*} \\mathbf{v} \u0026amp;= \\mathbf{v} + \\mathbf{0} \u0026amp;\u0026amp; \\text{by (A4)} \\\\ \u0026amp;= \\mathbf{v} + \\left( \\mathbf{u} + \\mathbf{v}^{\\prime} \\right) \u0026amp;\u0026amp; \\text{by (A5)} \\\\ \u0026amp;= \\left( \\mathbf{v} + \\mathbf{u} \\right) + \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A3)} \\\\ \u0026amp;= \\mathbf{0} + \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A5)} \\\\ \u0026amp;= \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\end{align*} $$\nしたがって、$\\mathbf{u}$の二つの負は互いに等しい。\n■\n定理2 $V$をベクトル空間、$\\mathbf{u}$を$V$の要素、$k$をスカラーとする。\n(2a) $0 \\mathbf{u} = \\mathbf{0}$\n(2b) $k \\mathbf{0} = \\mathbf{0}$\n(2c) $(-1) \\mathbf{u} = -\\mathbf{u}$\n(2d) もし$k \\mathbf{u} = \\mathbf{0}$であれば、$k = 0$か$\\mathbf{u} = \\mathbf{0}$である。\n証明 ベクトル空間の定義を利用した証明である。\n(2a) $$ \\begin{align*} \u0026amp;\u0026amp; 0\\mathbf{u} \u0026amp;= (0 + 0)\\mathbf{u} \\\\ \u0026amp;\u0026amp; \u0026amp;= 0\\mathbf{u} + 0\\mathbf{u} \u0026amp;\u0026amp;\\text{by (M3)} \\\\ \u0026amp; \u0026amp; \\\\ \\implies \u0026amp;\u0026amp; 0\\mathbf{u}+(-0\\mathbf{u}) \u0026amp;= 0\\mathbf{u} + 0\\mathbf{u} +(-0\\mathbf{u}) \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{0} \u0026amp;= 0\\mathbf{u} \u0026amp;\u0026amp;\\text{by (A5)} \\end{align*} $$\n■\n(2b) $$ \\begin{align*} \u0026amp;\u0026amp; k\\mathbf{0} \u0026amp;= k(\\mathbf{0} + \\mathbf{0}) \u0026amp;\u0026amp;\\text{by (A4)} \\\\ \u0026amp;\u0026amp; \u0026amp;= k\\mathbf{0} + k\\mathbf{0} \u0026amp;\u0026amp;\\text{by (M2)} \\\\ \u0026amp; \u0026amp; \\\\ \\implies \u0026amp;\u0026amp; k\\mathbf{0}+(-k\\mathbf{0}) \u0026amp;= k\\mathbf{0} + k\\mathbf{0} +(-k\\mathbf{0}) \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{0} \u0026amp;= k\\mathbf{0} \u0026amp;\u0026amp;\\text{by (A5)} \\end{align*} $$\n■\n(2c) $$ \\begin{align*} \\mathbf{u} + (-1)\\mathbf{u} \u0026amp;= 1 \\mathbf{u} + (-1)\\mathbf{u} \u0026amp;\u0026amp;\\text{by (M5)} \\\\ \u0026amp;= \\big( 1 + (-1) \\big) \\mathbf{u} \u0026amp;\u0026amp;\\text{by (M3)} \\\\ \u0026amp;= 0 \\mathbf{u} \\\\ \u0026amp;= \\mathbf{0} \u0026amp;\u0026amp;\\text{by (a2)} \\end{align*} $$\nすると**（A5）により$(-1)\\mathbf{u}$は$\\mathbf{u}$の負であり、（1b）**により$\\mathbf{u}$の負は唯一であるため、\n$$ (-1)\\mathbf{u} = -\\mathbf{u} $$\n■\n(2d) $k$は必ず$0$か$0$のどちらか一方の場合にのみ該当するので、二つの場合に分けて考える。\n$k=0$の場合\n結論を満たす。\n$k\\ne 0$の場合\n$k$が$0$でないため、$k$で割ることができる。したがって\n$$ \\begin{align*} \u0026amp;\u0026amp; k \\mathbf{u} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{u} \u0026amp;= \\frac{1}{k}\\mathbf{0} \\\\ \u0026amp;\u0026amp; \u0026amp;= \\mathbf{0} \u0026amp;\u0026amp; \\text{by (2b)} \\end{align*} $$\n■\n一緒に見る ベクトルの簡単な定義 抽象代数 線形代数学でのベクトル空間 抽象代数学でのベクトル空間 下記の文書で述べられている$F$-ベクトル空間は、実際に上記の文書のベクトル空間と何の差異もない。ただ視点が少し異なるだけで、線形代数学でのベクトル空間が直感的なユークリッド空間の抽象化であり、抽象代数学でのベクトル空間はそれを真の意味での\u0026rsquo;代数\u0026rsquo;として扱うことである。\n逆に$R$-モジュールは$F$-ベクトル空間のスカラー体$F$をスカラー環$R$に一般化することに意義があり、したがって$F$-ベクトルフィールドの歴史と意味に関心がないネーミングでそのアイデンティティを示している。群$G$の立場から見れば、環$R$と新しい演算$\\mu$が加えられたことであるため、その逆も加群加群である。\n抽象代数学でのR-モジュール 抽象代数学での$F$-ベクトル空間 Howard Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p202-203\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n体をよくわからなければ、簡単に$\\mathbb{F}=\\mathbb{R}$または$\\mathbb{F}=\\mathbb{C}$と考えればよい。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":282,"permalink":"https://freshrimpsushi.github.io/jp/posts/282/","tags":null,"title":"ベクトル空間の定義"},{"categories":"수리물리","contents":"定義 自然数$n \\in \\mathbb{N}$に対し、実数の集合$\\mathbb{R}$のデカルト積$\\mathbb{R}^{n}$をユークリッド空間と呼ぶ。\n$$ \\mathbb{R}^{n} = \\mathbb{R} \\times \\cdots \\times \\mathbb{R} $$\n$\\mathbb{R}^{1}$は実数空間または数直線と呼ばれる。 $\\mathbb{R}^{2}$は平面と呼ばれる。 $\\mathbb{R}^{3}$は**$3$次元空間**と呼ばれる。 ここで、$\\mathbb{N} := \\left\\{ 1, 2, 3, \\cdots \\right\\}$は自然数を全部集めた集合を意味する。$\\mathbb{R}$は実数を全部集めた集合を意味する。\n説明 ユークリッド空間は、幾何学原論の著者であるユークリッドの名を冠した空間で、私たちが生きている$3$次元空間を含め、平面、数直線はもちろん、それ以上の多次元空間まで表現する空間だ。\n私たちの生活と密接に関連しているため、多くの理論で全体の空間としてユークリッド空間が仮定されることが多い。もちろん、ユークリッド空間だけでは、深く複雑な科学技術の理論を全て説明することはできず、YouTubeに生息する疑似数学者、疑似科学者たちのおもちゃになることもある。\n多次元への拡張は、時空間が$\\mathbb{R}^{1+3}$であるからとか、弦理論で$\\mathbb{R}^{11}$となるといった派手な目的がなくとも必ず必要なものだ。代表的には統計学での応用もあり、空間自体が$3$次元であっても、速度や加速度を導入するために$9$次元が必要になることもある。 もしこの文章を読んでいる読者が主に工学、または実用的な物理学までの学びを希望しているなら、実際には通常のユークリッド空間を抜け出すことはないかもしれない。式や証明なしにロマンを歌う教養書を見て夢を育てても仕方がない。しかし、それを越えた数学や物理の世界に興呀があれば、ユークリッド空間を足場と見なして早く慣れるべきだ。\n","id":205,"permalink":"https://freshrimpsushi.github.io/jp/posts/205/","tags":null,"title":"ユークリッド空間"},{"categories":"해석개론","contents":"公理1 集合 $E \\subset \\mathbb{R}$ が空集合ではなく、もし $E$ が上界を持つならば、上限 $\\sup(E) \u0026lt; \\infty$ が存在する。\n説明 体の公理や順序の公理は、知っていることを複雑に書き直した感じだが、完備性の公理は一見そうではない。まずはここで登場する単語に対する定義が必要だろう。\n定義 $E$ の全ての元 $a$ に対して $a \\le M$ が成り立つならば、$E$ を上に有界bounded aboveと呼ぶ。このような条件を満たす $M$ を全て、$E$ の上界upper boundと呼ぶ。$\\sup(E)$ は$E$ の最小の上界であり、全ての$E$ の上界 $M$ に対して $\\sup (E) \\le M$ を満たす数である。これを$E$ の最小上界supremum, 上限と呼ぶ。\n反対の不等号の場合は、以下のようになる。\n$E$ の全ての元 $a$ に対して $a \\ge m$ が成り立つならば、$E$ を下に有界bounded belowと呼ぶ。このような条件を満たす $m$ を全て、$E$ の下界lower boundと呼ぶ。$\\inf(E)$ は$E$ の最大の下界であり、全ての$E$ の下界 $m$ に対して $\\inf (E) \\ge m$ を満たす数である。これを$E$ の最大下界infimum, 下限と呼ぶ。\n突然定義がたくさん出てきて混乱するかもしれないが、根本的に我々の概念を揺るがすわけではない。ただどんな集合がいつ限界を持つか、その時限界を何と呼ぶかを定義するだけだ。\n完備性公理に戻ってみると、上で紹介された定義を繰り返す感じがする。違いは簡単だ。定義では存在する時に何と呼ぶかだけを言っていて、本当に存在するかについては話していない。完備性公理では、その「存在」について話している。\nしかし、これが公理であるべきかという疑問が生じるだろう。公理として必要なほど基本的な事実なのか？証明できないのか？定義を読むと、定義によって当然のようにこのような上限が存在し、証明できるように思えるが、そうではない。\n反証 $E$ が上に有界としたならば、条件を満たす上界 $M$ があり、その中で最も小さい値が存在して、上限 $\\sup(E)$ が存在するだろう。しかし、逆に考えてみると、$\\sup(E)$ は$-M$ の中で最も大きい値、つまり上限である。そもそも最も小さい値が存在するという主張自体が上限の存在性を根拠にしている。これにより、循環論法に陥るしかない。\n■\n上でも下でも、大きさに関係なく、議論の方向は逆になる。結局、我々はこのような上限や下限の存在性を明らかにできない。だから、新しい公理として完備性公理を作り出すしかなかった。\n定理 整数の集合 $\\mathbb{Z}$ の部分集合 $E$ が上限を持つならば $\\sup(E) \\in E$\n完備性公理がなければ、こんなにも自明な事実でさえ、その仮定が疑わしいために信じられない。\n完備？ 完備とは、Completeの和訳で、実数空間 $\\mathbb{R}$ を超えて距離空間で一般化されるとき、コーシー列の収束点を含む空間を完備空間と定義する。ただし、日常の中でCompleteという英語は完全に(完)備える(備)という意味で使われることは少なく、「完成」や「完結」など、何か続いているもののその終わりと共に使われることが多い。これは、述べられたように、収束点の存在(その空間内に)を保証する点から、completeという表現が適切であることが分かる。\nもちろん、コーシー列を捉えることと$E \\subset \\mathbb{R}$ を捉えることは異なるが、$\\mathbb{R}$ が可分性を持つといった説明はまだ早い。後でそんなことをまた学ぶんだと思って、先に進んでもいい。\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), p16-18\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":180,"permalink":"https://freshrimpsushi.github.io/jp/posts/180/","tags":null,"title":"解析学の三つの公理：完備性公理"},{"categories":"수리물리","contents":"まとめ 次のように定義される $\\epsilon_{ijk}$ を レビ-チビタ記号 と呼ぶ。\n$$ \\epsilon_{ijk} = \\begin{cases} +1 \u0026amp; \\text{if} \\ \\epsilon_{123}, \\epsilon_{231}, \\epsilon_{312} \\\\ -1 \u0026amp; \\text{if} \\ \\epsilon_{132}, \\epsilon_{213}, \\epsilon_{321} \\\\ 0 \u0026amp; \\text{if} \\ i=j \\ \\text{or} \\ j=k \\ \\text{or} \\ k=i \\end{cases} $$\n次のように定義される $\\delta_{ij}$ を クロネッカーのデルタ と呼ぶ。\n$$ \\delta_{ij} := \\begin{cases} 1,\u0026amp;i=j \\\\ 0, \u0026amp; i\\ne j \\end{cases} $$\n二つのレビ-チビタ記号の積とクロネッカーのデルタとの間には、次の関係が成り立つ。\n(a) 一つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ilm} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl}$\n(b) 二つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijm}=2\\delta_{km}$\n(c) 三つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijk}=6$\n説明 文章全体で $\\sum$ を省略する アインシュタインの記法 を使用していることに注意してください。これは上記の式においても同じです。 (a)は使用頻度が高いため、覚えておくと便利です。簡単に覚える方法は次のとおりです。\n証明 (a) $\\mathbf{e}_{i}$ $(i=1,2,3)$ を3次元における 標準単位ベクトル としよう。\n$$ \\mathbf{e}_{1} = (1, 0, 0),\\quad \\mathbf{e}_{2} = (0, 1, 0),\\quad \\mathbf{e}_{3} = (0, 0, 1) $$\n$P_{ijk}$ を1行目が $\\mathbf{e}_{i}$、2行目が $\\mathbf{e}_{j}$、3行目が $\\mathbf{e}_{k}$ の $3 \\times 3$ 行列 とする。\n$$ P_{ijk} = \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} $$\nすると、行列式の性質により $\\det P_{ijk} = \\epsilon_{ijk}$ と簡単にわかる。まず $P_{123}$ は 単位行列 であるため、行列式は $1$ である。また、異なる行の順序を偶数回変えると行列式の値は変わらないため、\n$$ \\det P_{123} = \\det P_{231} = \\det P_{312} = 1 $$\n異なる行の順序を奇数回変えると行列式の符号が逆になるため、\n$$ \\det P_{132} = \\det P_{213} = \\det P_{321} = -1 $$\n同じ行を二つ以上含む行列の行列式は $0$ であるため、残りの場合は全て $0$ となる。したがって $\\det P_{ijk} = \\epsilon_{ijk}$ が成立する。一つのインデックスが同じ二つのレビ-チビタ記号の積は、 行列式の性質 をうまく使うと、次のようになる。\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ilm} \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{l} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{m} \\text{ \u0026mdash;} \\end{bmatrix} \\\\ \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \u0026amp; (\\because \\det A = \\det A^{T}) \\\\ \u0026amp;= \\det \\left( \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \\right) \u0026amp; \\Big(\\because (\\det A) (\\det B) = \\det (AB) \\Big) \\\\ \u0026amp;= \\det \\begin{bmatrix} \\mathbf{e}_{i} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{j} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{k} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{m} \\end{bmatrix} \\end{align*} $$\n$\\mathbf{e}_{i}$ は標準単位ベクトルであるため、$\\mathbf{e}_{i} \\cdot \\mathbf{e}_{j} = \\delta_{ij}$ が成立する。\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} \\delta_{ii} \u0026amp; \\delta_{il} \u0026amp; \\delta_{im} \\\\ \\delta_{ji} \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ \\delta_{ki} \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} $$\nこのとき、$i$ が $j, k, l, m$ と全て異なる場合のみを考えていることに注意しよう。なぜなら $j, k, l, m$ のいずれかが $i$ と同じであれば、$\\epsilon_{ijk}\\epsilon_{ilm} = 0$ となり、意味のない結果だからである。したがって結果\nは次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ `` 0 \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl} $$\n■\n(b) (a) で $l=j$ の場合である。したがって、次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} $$\nこのとき $\\delta_{jj}=3$ が成立し、また $\\delta_{jm}\\delta_{kj}=\\delta_{mk}$ も成立するため、結果は次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} = 3\\delta_{km} - \\delta_{mk} = 2\\delta_{km} $$\n■\n(c) (b) で $m=k$ の場合であるため、\n$$ \\epsilon_{ijk}\\epsilon_{ijk} = \\sum_{k=1}^{3}2\\delta_{kk} = 2\\delta_{11} + 2\\delta_{22} + 2\\delta_{33} = 2 + 2 + 2 = 6 $$\nまたは、0でない全ての項を展開すると、次を得る。\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ijk} \u0026amp;=\\sum \\limits _{i=1} ^{3}\\sum \\limits _{j=1} ^{3}\\sum \\limits _{k=1} ^{1} \\epsilon_{ijk}\\epsilon_{ijk} \\\\ \u0026amp;=\\epsilon_{123}\\epsilon_{123}+\\epsilon_{231}\\epsilon_{231}+\\epsilon_{312}\\epsilon_{312}+\\epsilon_{132}\\epsilon_{132}+\\epsilon_{213}\\epsilon_{213}+\\epsilon_{321}\\epsilon_{321} \\\\ \u0026amp;=6 \\end{align*} $$\n■\n","id":88,"permalink":"https://freshrimpsushi.github.io/jp/posts/88/","tags":null,"title":"二つのレビ-チビタ記号の積"},{"categories":"복소해석","contents":"定理 1 $\\left\\{ a_{i} \\right\\}_{i=0}^{n} \\subset \\mathbb{R}$ で $a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$ としましょう。すると、多項関数 $$ P(z) := a_0 + a_1 z + \\cdots + a_{n-1} z^{n-1} + a_n z^n $$ において、あらゆる根 $z \\in \\mathbb{C}$ は $|z| \\ge 1$ を満たします。\n証明 もし $P(z) = 0$ の根が $z=1$ である場合、$\\displaystyle 0 = P(1) = \\sum_{i=0}^{n} a_{i} \u0026gt; 0$ より、根は $z \\ne 1$ でなければなりません。式 $P(z) = 0$ の両辺に $z$ を乗じて元の式から引き、$a_0$ を以下のように表せます。 $$ a_0 = (1-z)P(z) + (a_0 - a_1) z + \\cdots + (a_{n-1} - a_n) z^n + a_n z^{n+1} $$ ここで、$P(z) = 0$ の根 $z \\ne 1$ で $|z| \u0026lt; 1$ を仮定してみると、$a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$ より $$ \\begin{align*} \u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + (a_0 - a_1) + \\cdots + (a_{n-1} - a_n) + a_n \\\\ \\implies\u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + a_0 + (- a_1 + a_1) + \\cdots + (- a_{n-1} + a_{n-1} )+ (- a_n + a_n ) \\\\ \\implies\u0026amp; a_0 = |a_0| \u0026lt; |(1-z)P(z)| + a_0 \\\\ \\implies\u0026amp; 0 \u0026lt; |(1-z)P(z)| \\end{align*} $$ ですが、$z \\ne 1$ が $P(z) = 0$ の根であることを仮定しているので、以下の矛盾が生じます。 $$ 0 \u0026lt; |(1-z)P(z)| = 0 $$ これは、$| z | \u0026lt; 1$ という仮定が誤りであることを意味し、結果として $|z | \\ge 1$ でなければなりません。\n■\nOsborne. (1999). 複素変数とその応用: p. 6.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":5,"permalink":"https://freshrimpsushi.github.io/jp/posts/5/","tags":null,"title":"エーネストローム-カケヤ定理の証明"},{"categories":"교과과정","contents":"式 $$ d=\\frac {|2k|}{\\sqrt{m^2+1}} $$\n説明 双曲線の接線の問題を解いていると、二つの接線の間の距離を求めることがよくあります。点から直線までの距離を求める公式があるため、それ自体を解くことは難しくありません。しかし、その距離を簡単かつ迅速に計算できる公式を知っていれば、少しでも計算量を減らすことができるでしょう。\n導出 二つの平行な直線の方程式を $y=mx\\pm k$ とします。ある点 $(x,y)$ から直線 $y=mx+k$ までの距離は $$ \\frac {|mx-y+k|}{\\sqrt{m^2+1}} $$ 直線 $y=mx-k$ 上の点 $(x_1,y_1)$ に対しては $$ k=mx_1-y_1 $$ これを距離の公式に代入すると $$ \\frac {|mx_1-y_1+k|}{\\sqrt{m^2+1}} = \\frac {|k+k|}{\\sqrt{m^2+1}} $$ したがって、二つの平行な直線 $y=mx\\pm k$ の間の距離は $$ \\frac {|2k|}{\\sqrt{m^2+1}} $$\n■\n","id":4,"permalink":"https://freshrimpsushi.github.io/jp/posts/4/","tags":null,"title":"二本の平行な直線の間の距離を求める公式の導出"},{"categories":"보조정리","contents":"定義 $n$ 個の正数 ${x}_1,{x}_2,\\cdots,{x}_n$ に対して算術平均、幾何平均、調和平均は以下のように定義される。\n算術平均 : $$ \\sum_{ k=1 }^{ n }{ \\frac { {x}_k }{ n } }=\\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n } $$ 幾何平均 : $$ \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } }=\\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n } $$ 調和平均 : $$ \\left( \\frac { \\sum_{ k=1 }^{ n }{ \\frac { 1 }{ {x}_k } } }{ n } \\right)^{-1}=\\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ n{x}_2 }+\\cdots+\\frac { 1 }{ n{x}_n } } $$ 定理 これらの平均に対して、次の不等式が成り立つ。\n$$ \\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n }\\ge \\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ n{x}_2 }+\\cdots+\\frac { 1 }{ n{x}_n } } $$\n説明 高校生であれば、算術・幾何平均について一度は耳にするかもしれないが、特定の名称で定義されることはあまりなく、通常は「算術幾何」という略称で口伝えにされることが一般的である。$n=2$ の場合には証明も簡単で、高校レベルの問題解決にも役立つ。高校生レベルで一般的な証明には複雑な式を使った数学的帰納法を用いる必要があるが、より洗練されたが難しい証明を紹介する。\n証明 戦略：次の補助定理を利用する。\nジェンセンの不等式： $f$ が 凸関数 で、$E(X) \u0026lt; \\infty$ の場合、以下の不等式が成り立つ。 $$ E{f(X)}\\ge f{E(X)} $$\n算術-幾何 $f(x)=-\\ln x$ とすると、$f$ は区間 $(0,\\infty )$ で凸関数である。確率変数 $X$ が確率質量関数\n$$ p(X=x)=\\begin{cases}{1 \\over n} \u0026amp; , x={x}_1,{x}_2, \\cdots ,{x}_n \\\\ 0 \u0026amp; , その他の場合\\end{cases} $$\nを持つとする。すると $E(X)$ は\n$$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\u0026lt;\\infty $$\nであり有限である。これはジェンセンの不等式に必要な全ての条件を満たすため、次を得る。\n$$ E(-\\ln X)\\ge –\\ln E(X) $$\n左辺は\n$$ \\begin{align*} E(-\\ln X)\u0026amp;=-E(\\ln X) \\\\ \u0026amp;=-\\frac { 1 }{ n } \\sum_{ k=1 }^{ n }{ \\ln{x}_k } \\\\ \u0026amp;=-\\frac { 1 }{ n }\\ln \\prod_{ k=1 }^{ n }{ {x}_k } \\\\ \u0026amp;=-\\ln { \\left( \\prod_{ k=1 }^{ n }{ {x}_k } \\right) }^{ \\frac { 1 }{ n } } \\\\ \u0026amp;=-\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\end{align*} $$\n右辺は\n$$ \\begin{align*} -\\ln E(X)=-\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\end{align*} $$\nこの両者を定理すると\n$$ \\begin{align*} -\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\ge\u0026amp; -\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\\\ \\implies \\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n } \\ge\u0026amp; \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } \\end{align*} $$\n■\nこれにより、算術平均と幾何平均の間の不等式が証明された。これを用いて、幾何平均と調和平均の間の不等式を証明しよう。\n幾何-調和 $$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } $$\n$\\displaystyle {x}_k=\\frac { 1 }{ n{y}_k }$ と置くと、\n$$ \\begin{align*} \\frac { \\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } }{ n }\\ge \\sqrt [ n ]{ \\frac { 1 }{ n{y}_1 }\\frac { 1 }{ n{y}_2 }\u0026hellip;\\frac { 1 }{ n{y}_n } } \\\\ \\implies \\frac { 1 }{ n\\sqrt [ n ]{ \\frac { 1 }{ n{y}_1 }\\frac { 1 }{ n{y}_2 }\u0026hellip;\\frac { 1 }{ n{y}_n } } }\\ge \\frac { n }{ n\\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\\\ \\implies \\sqrt [ n ]{ {y}_1{y}_2\u0026hellip;{y}_n }\\ge \\frac { n }{ n\\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\end{align*} $$ ■\n","id":3,"permalink":"https://freshrimpsushi.github.io/jp/posts/3/","tags":null,"title":"算術平均と幾何平均、調和平均の間の不等式"},{"categories":"머신러닝","contents":"説明 ニューラルネットワークに関連する多くの関数が torch.nn と torch.nn.functional に同じ名前で含まれています。 nn の関数はニューラルネットワークを関数として返し、 nn.functional の関数はニューラルネットワークそのものです。\n例えば nn.MaxPool2d はカーネルサイズを入力として受け取り、プーリング層を返す。\nimport torch import torch.nn as nn pool = nn.MaxPool2d(kernel_size = 2) # MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) A = torch.arange(16.).reshape(1, 4, 4) # tensor([[[ 0., 1., 2., 3.], # [ 4., 5., 6., 7.], # [ 8., 9., 10., 11.], # [12., 13., 14., 15.]]]) pool(A) # tensor([[[ 5., 7.], # [13., 15.]]]) 一方で nn.functional.MaxPool2d はそのものが2次元マックスプーリングレイヤーです。そのため、この関数はプーリングを適用するテンソルとプーリングの条件をすべて入力として受け取り、実際に入力テンソルをプーリングした結果を返す。\nimport torch import torch.nn.functional as F A = torch.arange(16.).reshape(1, 4, 4) F.max_pool2d(A, kernel_size=2) #tensor([[[ 5., 7.], # [13., 15.]]]) つまり nn.MaxPool2d(kernel_size=(n,m)) が返す関数の forward が max_pool2d( ,kernel_size(n,m)) として定義されているわけです。コードを見ると、実際には次のようになっています。\nclass MaxPool2d(_MaxPoolNd): kernel_size: _size_2_t stride: _size_2_t padding: _size_2_t dilation: _size_2_t def forward(self, input: Tensor): return F.max_pool2d(input, self.kernel_size, self.stride, self.padding, self.dilation, ceil_mode=self.ceil_mode, return_indices=self.return_indices) パラメータが含まれるレイヤー、例えば線形層であれば、F.linear(input, weight, bias) のようにパラメータも入力として受け取ります。\n環境 OS: Windows11 バージョン: Python 3.11.5, torch==2.0.1+cu118 ","id":3626,"permalink":"https://freshrimpsushi.github.io/jp/posts/3626/","tags":null,"title":"파이토치에서 torch.nn과 torch.nn.functional의 차이"},{"categories":"편미분방정식","contents":"整理 次のような波動方程式が与えられたとする。 この時、$\\Delta_{\\mathbf{x}}$は変数$\\mathbf{x}$に対するラプラシアンである。\n$$ \\begin{align} \\partial_{t}^{2} p(\\mathbf{x}, t) \u0026amp;= \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;\\text{on } \\mathbb{R} \\times [0, \\infty) \\\\ p(\\mathbf{x}, 0) \u0026amp;= f(\\mathbf{x}) \u0026amp;\\text{on } \\mathbb{R} \\\\ \\partial_{t} p(\\mathbf{x}, 0) \u0026amp;= 0 \u0026amp;\\text{on } \\mathbb{R} \\end{align} $$\n上辺微分方程式の解は次の通りである。\n$$ \\begin{equation} p(\\mathbf{x}, t) = \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\end{equation} $$\nこのとき$\\hat{f}$は$f$の「フーリエ変換」（../1086）である。 今回は初期条件が以下のように与えられた波動方程式を考えてみよう。\n$$ \\begin{align*} \\partial_{t}^{2} p(\\mathbf{x}, t) \u0026amp;= \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;\\text{on } \\mathbb{R} \\times [0, \\infty) \\\\ p(\\mathbf{x}, 0) \u0026amp;= 0 \u0026amp;\\text{on } \\mathbb{R} \\\\ \\partial_{t} p(\\mathbf{x}, 0) \u0026amp;= g(\\mathbf{x}) \u0026amp;\\text{on } \\mathbb{R} \\end{align*} $$\n上辺微分方程式の解は次の通りである。\n$$ p(\\mathbf{x}, t) = \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{g} (\\boldsymbol{\\xi}) \\dfrac{\\sin (t \\left| \\boldsymbol{\\xi} \\right|)}{\\left| \\boldsymbol{\\xi} \\right|} e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\n説明 「フーリエ変換」(../1086)と「逆変換」(../1112)の定義を以下のようにしておこう。\n$$ \\hat{f}(\\boldsymbol{\\xi}) = \\int\\limits_{\\mathbb{R}^{n}} f(\\mathbf{x}) e^{\\mathrm{i} \\boldsymbol{\\xi} \\cdot \\mathbf{x}} \\mathrm{d} \\mathbf{x}, \\qquad f(\\mathbf{x}) = \\dfrac{1}{(2\\pi)^{n}}\\int\\limits_{\\mathbb{R}^{n}} f(\\mathbf{x}) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\n後者の証明法は前者と大同小異なので省略する。\n証明 $(4)$が$(1)$、$(2)$、$(3)$を満足させるか確認するだけだ。 まず、時間に対する2階導関数を計算してみると、\n$$ \\partial_{t}^{2} p(\\mathbf{x}, t) = -\\left| \\boldsymbol{\\xi} \\right|^{2} \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\nラプラシアンを計算してみると次のようになる。\n$$ \\begin{align*} \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) (\\Delta_{\\mathbf{x}} e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}}) \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= (- \\left| \\boldsymbol{\\xi} \\right|^{2}) \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \\end{align*} $$\nしたがって$(1)$が成立する。 $p(\\mathbf{x}, 0)$を計算してみると次のようになるので$(2)$が成立する。\n$$ \\begin{align*} p(\\mathbf{x}, 0) \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos ( 0 \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= f(\\mathbf{x}) \\end{align*} $$\n$(3)$が成立することも容易に確認できる。\n$$ \\begin{align*} \\partial_{t}p(\\mathbf{x}, 0) \u0026amp;= - \\left| \\boldsymbol{\\xi} \\right| \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\sin ( 0 \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= 0 \\end{align*} $$\n■\n","id":3623,"permalink":"https://freshrimpsushi.github.io/jp/posts/3623/","tags":null,"title":"初期条件が0の波動方程式の解。"},{"categories":"머신러닝","contents":"説明 AdaBeliefは2020年にJ. Zhuangらによって紹介されたオプティマイザで、Adamの変形の一つです1。PyTorchではこのオプティマイザをデフォルトで提供していないため、別途インストールして使用する必要があります。\nコード2 インストール cmdで以下のコードでインストールできます。\npip install adabelief-pytorch==0.2.0 使用方法 以下のコードで読み込んで使用できます。\nfrom adabelief_pytorch import AdaBelief\roptimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False) 環境 OS: Windows11 Version: Python 3.11.5, torch==2.0.1+cu118, adabelief-pytorch==0.2.0 https://arxiv.org/abs/2010.07468\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/juntang-zhuang/Adabelief-Optimizer?tab=readme-ov-file#installation-and-usage\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3620,"permalink":"https://freshrimpsushi.github.io/jp/posts/3620/","tags":null,"title":"파이토치에서 AdaBelief 옵티마이저 사용하는 방법"},{"categories":"줄리아","contents":"説明 カラーグラディエントは、Juliaの視覚化パッケージ Plots.jlがサポートする2つのカラースキームのうちの1つ（もう1つはパレット）で、私たちが一般的にグラデーションと呼んでいるものと同じだ。つまり、簡単に言うとグラデーションが実装されているタイプが ColorGradientである。\nグラディエントは、heatmap(), surface(), contour()などの図表を描くのに使われる。複数のグラフの色をそれぞれ変えたい場合は、グラディエントではなくパレットを使用する。\nコード シンボル cgrad(シンボル)で使用できる。デフォルトのグラディエントは cgrad(:inferno)で、色は以下の通りである。\nusing Plots\rcgrad(:inferno) heatmap(reshape(1:25, (5, 5))) Plots.jlに事前定義されているパレットとグラディエントは、公式ドキュメントで確認できる。(パッケージ ColorSchemes.jlの公式ドキュメントでさらに多様なパレットとグラディエントを探すことができる。)\nPythonのmatplotlibで imshowのデフォルトカラーマップでグラディエントに似たものは :viridisである。\nheatmap(reshape(1:25, (5, 5)), fillcolor = cgrad(:viridis)) 直接定義 cgrad([開始色, 終了色])で直接パレットを定義することができる。色が変わるポイントを設定するには、オプション引数として$0$と$1$の間の値を要素に持つベクトルを入力する。\ncgrad([:blue, :orange]) cgrad([:blue, :orange], [0.1, 0.9]) cgrad([:blue, :orange], [0.5, 0.50001]) キーワード rev キーワード引数として rev = trueを入力すると、順序が逆になる。\ncgrad(:darktest) cgrad(:darktest, rev = true) scale キーワードscaleは、グラディエントのスケールを指定する。:logまたは:expを入力できる。\ncgrad(:rainbow) cgrad(:rainbow, scale = :log) cgrad(:rainbow, scale = :exp) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参照 色の使い方 パレットの使い方 カラーグラディエント（グラデーション）の使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1,0,0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色指定の仕方 サブプロット毎にグラフの色を指定する方法 軸、軸の名前、目盛、目盛の値の色指定の仕方 背景色の指定の仕方 ","id":3608,"permalink":"https://freshrimpsushi.github.io/jp/posts/3608/","tags":null,"title":"ジュリアプロットでカラーグラデーションを使用する方法"},{"categories":"줄리아","contents":"説明 パレットとは、予め絞り出された絵の具がおかれている板のことを指します。数学的に説明すると、「色の集合」や「色の数列」と言えるでしょう。1つの絵に複数のグラフを描く際、最も一般的な方法は異なる色を使って区別することですが、その目的のためにJuliaでは、様々な色を集めたColorPaletteというタイプが実装されています。色のベクトルとして理解すると便利です。実際に、デフォルトのパレット：defaultを読み込んでみると、とても複雑に見えますが、中を見れば、ただの色のベクトルです。\nヒートマップを描くときは、パレットではなくグラデーションを使用します。\njulia\u0026gt; using Plots\rjulia\u0026gt; palette(:default)\rColorPalette(ColorSchemes.ColorScheme{Vector{RGB{Float64}}, String, String}(RGB{Float64}[RG\rB{Float64}(0.0,0.6056031611752245,0.9786801175696073), RGB{Float64}(0.8888735002725198,0.43\r564919034818994,0.2781229361419438), RGB{Float64}(0.2422242978521988,0.6432750931576305,0.3\r044486515341153), RGB{Float64}(0.7644401754934356,0.4441117794687767,0.8242975359232758), R\rGB{Float64}(0.6755439572114057,0.5556623322045815,0.09423433626639477), RGB{Float64}(4.8211\r81644776295e-7,0.6657589812923561,0.6809969518707945), RGB{Float64}(0.930767491919665,0.367\r4771896571412,0.5757699667547829), RGB{Float64}(0.7769816661712932,0.5097431319944513,0.146\r4252569555497), RGB{Float64}(3.8077343661790943e-7,0.6642678029460116,0.5529508754522481), RGB{Float64}(0.558464964115081,0.5934846564332882,0.11748125233232104), RGB{Float64}(5.9476\r23898072685e-7,0.6608785231434254,0.7981787608414297), RGB{Float64}(0.6096707676128648,0.49\r918492100827777,0.9117812665042642), RGB{Float64}(0.3800016049820351,0.5510532724353506,0.9\r665056985227146), RGB{Float64}(0.942181647954218,0.37516423354097583,0.4518168202944593), R\rGB{Float64}(0.8684020893043971,0.3959893639954845,0.7135147524811879), RGB{Float64}(0.42314\r674364630817,0.6224954944199981,0.19877060252130468)], \u0026#34;\u0026#34;, \u0026#34;\u0026#34;))\rjulia\u0026gt; palette(:default).colors.colors\r16-element Array{RGB{Float64},1} with eltype RGB{Float64}:\rRGB{Float64}(0.0,0.6056031611752245,0.9786801175696073)\rRGB{Float64}(0.8888735002725198,0.43564919034818994,0.2781229361419438)\rRGB{Float64}(0.2422242978521988,0.6432750931576305,0.3044486515341153)\rRGB{Float64}(0.7644401754934356,0.4441117794687767,0.8242975359232758)\rRGB{Float64}(0.6755439572114057,0.5556623322045815,0.09423433626639477)\rRGB{Float64}(4.821181644776295e-7,0.6657589812923561,0.6809969518707945)\rRGB{Float64}(0.930767491919665,0.3674771896571412,0.5757699667547829)\rRGB{Float64}(0.7769816661712932,0.5097431319944513,0.1464252569555497)\rRGB{Float64}(3.8077343661790943e-7,0.6642678029460116,0.5529508754522481)\rRGB{Float64}(0.558464964115081,0.5934846564332882,0.11748125233232104)\rRGB{Float64}(5.947623898072685e-7,0.6608785231434254,0.7981787608414297)\rRGB{Float64}(0.6096707676128648,0.49918492100827777,0.9117812665042642)\rRGB{Float64}(0.3800016049820351,0.5510532724353506,0.9665056985227146)\rRGB{Float64}(0.942181647954218,0.37516423354097583,0.4518168202944593)\rRGB{Float64}(0.8684020893043971,0.3959893639954845,0.7135147524811879)\rRGB{Float64}(0.42314674364630817,0.6224954944199981,0.19877060252130468) 既に定義されたパレットのシンボルを入力するか、色と長さを入力してpalette()関数でパレットを読み込んだり作ったりすることができます。図表を描くときは、plot()関数のpaletteキーワードに代入すればOKです。\nコード シンボル palette(シンボル)のように使用します。デフォルトのパレットのシンボルは:defaultで、色は以下の通りです。\n1つの絵に複数のグラフを描くと、上記の色が順番に適用されます。色を使い切った後は、再び最初から循環します。\nusing Plots x = 0:0.01:2π plot([x -\u0026gt; sin(x - a) for a in range(0, π, length = 5)], 0, 2π) Plots.jlに予め定義されたパレットやグラデーションは、公式ドキュメントで確認できます。(パッケージColorSchemes.jlの公式ドキュメントでも、より多様なパレットやグラデーションを見つけることができます。)\n:rainbowで描いてみると、\npalette(:rainbow) plot([x -\u0026gt; sin(x - a) for a in range(0, π, length = 5)], 0, 2π,\rpalette = palette(:rainbow)) 直接定義 palette([開始色、終了色], 長さ)で、直接パレットを定義することができます。また、range()を使用して色を補間することもできます。\npalette([:blue, :orange], 10) palette([RGB(0.5, 0.6, 0.2), RGB(1.0, 0.2, 0.9)], 10) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 併せて見る 色の使用方法 パレットの使用方法 カラーグラデーション(グラデーション)の使用方法 色処理のためのパッケージ Colors.jl RGBコードの使用方法 RGB(1, 0, 0) HEXコードの使用方法 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフの色を指定する方法 軸、軸名、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3607,"permalink":"https://freshrimpsushi.github.io/jp/posts/3607/","tags":null,"title":"ジュリアプロットでパレットを使用する方法"},{"categories":"줄리아","contents":"コード 大きくスケールの異なる2つのデータを同じプロットに描いた場合、下の図のようにスケールが小さい方が完全に無視されてしまう。\nusing Plots\rx = 0:0.01:2π\rplot(x, sin.(x))\rplot!(x, exp.(x)) 2つ目のデータをプロットするとき、twinx()を最初の引数に入力すれば、$x$軸を共有し、新しい$y$軸に対してグラフが描かれる。\nplot(x, sin.(x), ylabel = \u0026#34;sin x\u0026#34;)\rplot!(twinx(), x, exp.(x), ylabel = \u0026#34;exp x\u0026#34;) 逆に、$y$軸を共有して描くときは、twiny()を最初の引数に入力すればいい。\n環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3606,"permalink":"https://freshrimpsushi.github.io/jp/posts/3606/","tags":null,"title":"ジュリアプロットで異なるスケールの2つのデータ軸を共有して描く方法"},{"categories":"줄리아","contents":"説明 JuliaのPlots.jlでは、プロットも一つのオブジェクトだ。空のプロットを描いてタイプを確認すると、以下のようになる。\njulia\u0026gt; using Plots\rjulia\u0026gt; p = plot()\rjulia\u0026gt; p |\u0026gt; typeof\rPlots.Plot{Plots.GRBackend} Plots.を外してみると、Plot{GRBackend}となり、要素のデータタイプがFloat64のベクターがVector{Float64}と表示されるのと同様に、バックエンドがGRのプロットだという意味だ。Plotのプロパティを確認してみると、以下のようになる。\njulia\u0026gt; p |\u0026gt; propertynames\r(:backend, :n, :attr, :series_list, :o, :subplots, :spmap, :layout, :inset_subplots, :init) 各プロパティは、図の属性を含むベクターか、辞書か、そういうものだ。\np.backend プロットのバックエンドだ。\njulia\u0026gt; p.backend\rPlots.GRBackend() p.attr 図の属性に関する辞書だ。以下のような30個のキー・バリューを含んでいる。\njulia\u0026gt; plot(rand(10, 4), layout = 4).attr\rRecipesPipeline.DefaultsDict with 30 entries:\r:dpi =\u0026gt; 100\r:background_color_outside =\u0026gt; :match\r:plot_titlefontvalign =\u0026gt; :vcenter\r:warn_on_unsupported =\u0026gt; true\r:background_color =\u0026gt; RGBA{Float64}(1.0,1.0,1.0,1.0)\r:inset_subplots =\u0026gt; nothing\r:size =\u0026gt; (600, 400)\r:display_type =\u0026gt; :auto\r:overwrite_figure =\u0026gt; true\r:html_output_format =\u0026gt; :auto\r:plot_titlefontfamily =\u0026gt; :match\r:plot_titleindex =\u0026gt; 0\r:foreground_color =\u0026gt; RGB{N0f8}(0.0,0.0,0.0)\r:window_title =\u0026gt; \u0026#34;Plots.jl\u0026#34;\r:plot_titlefontrotation =\u0026gt; 0.0\r:extra_plot_kwargs =\u0026gt; Dict{Any, Any}()\r:pos =\u0026gt; (0, 0)\r:plot_titlefonthalign =\u0026gt; :hcenter\r:tex_output_standalone =\u0026gt; false\r:extra_kwargs =\u0026gt; :series\r:thickness_scaling =\u0026gt; 1\r:layout =\u0026gt; 4\r:plot_titlelocation =\u0026gt; :center\r:plot_titlefontsize =\u0026gt; 16\r:plot_title =\u0026gt; \u0026#34;\u0026#34;\r:show =\u0026gt; false\r:link =\u0026gt; :none\r:plot_titlefontcolor =\u0026gt; :match\r:plot_titlevspan =\u0026gt; 0.05\r:fontfamily =\u0026gt; \u0026#34;sans-serif\u0026#34;\rjulia\u0026gt; plot(rand(10, 4), layout = 4).attr[:size]\r(600, 400) p.series_list 各データのグラフに関する属性の辞書を要素とするベクターだ。\njulia\u0026gt; plot(rand(10,5)).series_list\r5-element Vector{Plots.Series}:\rjulia\u0026gt; plot(plot(rand(10, 4)), plot(rand(10, 3))).series_list\r7-element Vector{Plots.Series}: 各辞書に含まれるキー・バリューは、以下の通りだ。\njulia\u0026gt; plot(rand(10, 2)).series_list[1].plotattributes\rRecipesPipeline.DefaultsDict with 62 entries:\r:plot_object =\u0026gt; Plot{Plots.GRBackend() n=2}\r:subplot =\u0026gt; Subplot{1}\r:label =\u0026gt; \u0026#34;y1\u0026#34;\r:fillalpha =\u0026gt; nothing\r:linealpha =\u0026gt; nothing\r:linecolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:x_extrema =\u0026gt; (NaN, NaN)\r:series_index =\u0026gt; 1\r:markerstrokealpha =\u0026gt; nothing\r:markeralpha =\u0026gt; nothing\r:seriestype =\u0026gt; :path\r:z_extrema =\u0026gt; (NaN, NaN)\r:x =\u0026gt; Base.OneTo(10)\r:markerstrokecolor =\u0026gt; RGBA{Float64}(0.0,0.0,0.0,1.0)\r:fillcolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:clims_calculated =\u0026gt; (NaN, NaN)\r:seriescolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:extra_kwargs =\u0026gt; Dict{Symbol, Any}()\r:z =\u0026gt; nothing\r:series_plotindex =\u0026gt; 1\r:y =\u0026gt; [0.477103, 0.00362131, 0.864524, 0.391488, 0.663659, 0.89787, 0.157973, 0.964416, 0.806635, 0.243531]\r:markercolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:y_extrema =\u0026gt; (0.00362131, 0.964416)\r:linewidth =\u0026gt; 1\r:group =\u0026gt; nothing\r:stride =\u0026gt; (1, 1)\r:permute =\u0026gt; :none\r:marker_z =\u0026gt; nothing\r:show_empty_bins =\u0026gt; false\r:seriesalpha =\u0026gt; nothing\r:smooth =\u0026gt; false\r:zerror =\u0026gt; nothing\r:arrow =\u0026gt; nothing\r:normalize =\u0026gt; false\r:linestyle =\u0026gt; :solid\r:contours =\u0026gt; false\r:bar_width =\u0026gt; nothing\r:bins =\u0026gt; :auto\r:markerstrokestyle =\u0026gt; :solid\r:weights =\u0026gt; nothing\r:z_order =\u0026gt; :front\r:fill_z =\u0026gt; nothing\r:markershape =\u0026gt; :none\r:markerstrokewidth =\u0026gt; 1\r:xerror =\u0026gt; nothing\r:bar_position =\u0026gt; :overlay\r:contour_labels =\u0026gt; false\r:hover =\u0026gt; nothing\r:primary =\u0026gt; true\r:yerror =\u0026gt; nothing\r:ribbon =\u0026gt; nothing\r:fillstyle =\u0026gt; nothing\r:line_z =\u0026gt; nothing\r:orientation =\u0026gt; :vertical\r:markersize =\u0026gt; 4\r:bar_edges =\u0026gt; false\r:quiver =\u0026gt; nothing\r:fillrange =\u0026gt; nothing\r:colorbar_entry =\u0026gt; true\r:series_annotations =\u0026gt; nothing\r:levels =\u0026gt; 15\r:connections =\u0026gt; nothing\rjulia\u0026gt; plot(rand(10, 2)).series_list[1][:label]\r\u0026#34;y1\u0026#34; 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3605,"permalink":"https://freshrimpsushi.github.io/jp/posts/3605/","tags":null,"title":"ジュリアプロッツでのプロットのプロパティリスト"},{"categories":"줄리아","contents":"概要 Plots.jlでの図の背景の格子に関連するキーワードは次の通りだ。\nキーワード名 機能 grid 格子表示 gridalpha, ga, gα 格子の透明度指定 foreground_color_grid, fgcolor_grid 格子の色指定 gridlinewidth, grid_lw 格子の太さ指定 gridstyle, grid_ls 格子の線スタイル指定 minorgrid 補助格子表示 minorgridalpha 補助格子の透明度指定 foreground_color_minor_grid, fgcolor_minorgrid 補助格子の色指定 minorgridlinewidth, minorgrid_lw 補助格子の太さ指定 minorgridstyle, minorgrid_ls 補助格子の線スタイル指定 コード 格子表示 格子を表示するキーワードはgridだ。:xや:yを入力するとそれぞれ$x$軸の目盛り補助線、$y$軸の目盛り補助線のみを表示する。falseを入力すると格子を表示しない。\nplot(plot(rand(10)), plot(rand(10), grid = :x), plot(rand(10), grid = :y), plot(rand(10), grid = false)) 透明度 背景の格子は基本的に0.1の透明度で描かれる。格子の透明度を調節するキーワードはgridalpha(=ga)(=gα)だ。\nplot(rand(10, 3), layout = (3, 1), gridalpha = [0.1 0.5 1]) 色 格子の基本色は黒で、キーワードforeground_color_grid(=fgcolor_grid)で他の色を指定できる。\nplot(rand(10, 3), layout = (3, 1), gridalpha = 1, fgcolor_grid = [:red :green :orange]) 太さ 格子の太さを指定するキーワードはgridlinewidth(=grid_lw)で、基本値は0.5だ。\nplot(rand(10, 3), layout = (3, 1), grid_lw = [0.5 5 10]) 格子スタイル キーワードgridstyle(=grid_ls)で格子の線スタイルを指定できる。可能なシンボルは:auto, :solid, :dash, :dot, :dashdot, :dashdotdot。\nplot(rand(10, 2), layout = 2, ga = 1, gridstyle = [:solid :dash]) 補助格子 キーワード引数としてminorgrid = trueを入力すると補助格子を描く。補助格子の透明度、色、線の太さ、線スタイルを指定するキーワードはそれぞれminorgridalpha, foreground_color_minor_grid minorgrid_lw, minorgrid_lsだ。\nplot(plot(rand(10)), plot(rand(10), minorgrid = true), gridalpha = 0.8, minorgridalpha = 0.2) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3604,"permalink":"https://freshrimpsushi.github.io/jp/posts/3604/","tags":null,"title":"ジュリアプロットで背景のグリッドを飾る方法"},{"categories":"줄리아","contents":"概要 Plots.jlで図の背景色に関連するキーワードは次の通りです。\nキーワード名 機能 background_color, bg_color 全体の背景の色を指定 background_color_outside, bg_color_outside グラフが描かれた外側の領域の色を指定 background_subplot, bg_subplot グラフが描かれた領域の色を指定 background_inside, bg_inside 凡例を除いたグラフが描かれた領域の色を指定 コード 背景色を指定するキーワードはbackground_color(=bg_color)です。凡例、グラフが描かれた場所とその他の全ての背景色を入力した値で指定します。\nplot(rand(10), bg_color = :tomato) グラフが描かれた領域外側の色を指定するキーワードはbackground_color_outside(=bg_color_outside)です。\nplot(rand(10), bg_color_outside = :palegreen) グラフが描かれた領域の色を指定するキーワードはbackground_subplot(=bg_subplot)です。\nplot(rand(10), bg_subplot = :violet) 凡例を除いてグラフが描かれた領域の色を指定するキーワードはbackground_inside(=bg_inside)です。\nplot(rand(10), bg_inside = :brown4) サブプロット 複数のサブプロットがある場合、bg_subplotやbg_insideで色を指定する必要があり、全体のプロットにまとめたときそれぞれの背景色を維持します。\np₁ = plot(rand(10), bg_subplot = :tomato)\rp₂ = scatter(rand(10), bg_inside = :yellow)\rp = plot(p₁, p₂) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参照 色の使用方法 パレットの使用方法 カラーグラデーションの使用方法 色処理のためのパッケージ Colors.jl RGBコードの使用方法 RGB(1, 0, 0) HEXコードの使用方法 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフ色を指定する方法 軸、軸名、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3603,"permalink":"https://freshrimpsushi.github.io/jp/posts/3603/","tags":null,"title":"ジュリア・プロットで背景色を指定する方法"},{"categories":"줄리아","contents":"概要 サブプロットごとにグラフの色を指定する3つの方法を紹介する。グラフ要素に色を指定する方法はここを参照してください。\n方法 1 サブプロットのグラフの色を指定する最初の方法は、各サブプロットを定義するときにあらかじめ色を指定することです。Juliaでは、1枚の絵が各オブジェクトであるため、属性を異にする絵を複数定義してから、それらを1つのプロットにまとめればよいです。\np₁ = plot(rand(10), lc = :red)\rp₂ = scatter(rand(10), mc = :blue)\rp₃ = bar(rand(10), fc = :green)\rplot(p₁, p₂, p₃,\rlayout = (3, 1),\rtitle = [\u0026#34;p₁\u0026#34; \u0026#34;p₂\u0026#34; \u0026#34;p₃\u0026#34;],\r) 方法 2 2つ目の方法は、全体のプロットを定義するときに、キーワード引数として色の行ベクトルを入力することです。列ベクトルではなく行ベクトルでなければならないことに注意してください。\np₄ = plot(rand(10))\rp₅ = plot(rand(10))\rp₆ = plot(rand(10))\rplot(p₄, p₅, p₆,\rlayout = (3, 1),\rlinecolor = [:brown :purple :orange],\rtitle = [\u0026#34;p₄\u0026#34; \u0026#34;p₅\u0026#34; \u0026#34;p₆\u0026#34;],\r) 方法 3 3つ目の方法は、全体のプロットを定義した後に、各サブプロットのプロパティ値を直接変更することです。プロパティ.series_listは、各サブプロットのシリーズ属性情報を含む辞書のベクトルです。すなわち、p.series_list[1]は、最初のサブプロットのシリーズ属性辞書を返します。この辞書に:linecolorキーを入力してその値を変更することで、最初のサブプロットの線の色が変わります。\np₇ = plot(rand(10))\rp₈ = scatter(rand(10))\rp₉ = bar(rand(10))\rp = plot(p₇, p₈, p₉,\rlayout = (3, 1),\rtitle = [\u0026#34;p₇\u0026#34; \u0026#34;p₈\u0026#34; \u0026#34;p₉\u0026#34;],\r)\rp.series_list[1][:linecolor] = :goldenrod1\rp.series_list[2][:markercolor] = :olivedrab3\rp.series_list[3][:fillcolor] = :hotpink3 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参照 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとのグラフの色の指定方法 軸、軸の名前、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3602,"permalink":"https://freshrimpsushi.github.io/jp/posts/3602/","tags":null,"title":"Julia Plotsで各サブプロットごとにグラフの色を指定する方法"},{"categories":"줄리아","contents":"要約 Plots.jlでは、グラフの各構成要素の色を指定するキーワードは以下の通りだ。\nキーワード 機能 markercolor, mc マーカー内部の色を指定 markerstrokecolor, msc マーカーの縁の色を指定 linecolor, lc 線の色を指定 fillcolor, fc 面積の色を指定 seriescolor, c すべての要素の色を指定 キーワード 機能 markeralpha, ma, mα マーカー内部の透明度を指定 markerstrokealpha, msa, msα マーカーの縁の透明度を指定 linealpha, la, lα 線の透明度を指定 fillalpha, fa, fα 面積の透明度を指定 seriesalpha, a, α すべての要素の透明度を指定 色 Plots.jlでは、変更可能な対象は点、線、面の三つだ。それぞれの色を指定するキーワード引数は、markercolor(=mc)、linecolor(=lc)、そしてfillcolor(=fc)である。これらのキーワードで指定された属性は互いに影響を与えないので、線グラフを描いてmc = :redと入力しても、線の色が赤になることはない。実際にp = plot(rand(10), mc = :red)のプロパティを確認すると、以下のようになる。\njulia\u0026gt; p = plot(rand(10), mc = :red)\rjulia\u0026gt; p.series_list[1][:linecolor]\rRGBA{Float64}(0.0,0.6056031611752245,0.9786801175696073,1.0)\rjulia\u0026gt; p.series_list[1][:markercolor]\rRGBA{Float64}(1.0,0.0,0.0,1.0) プロットされた線グラフの線色は依然としてデフォルトの色であり、赤ではない。\nだから、複数のサブプロットを描き、上記の三つのキーワードで色を指定すると、それぞれが適用される。点（マーカー）を紫色の:purple、線をダークグリーンの:darkgreen、面をスカイブルーの:skyblueで色付けすると、以下のようになる。\nst = [:line :scatter :barhist :steppre :scatterhist :bar]\rx = rand(20)\ry = repeat(x, outer = (1, length(st)))\rplot(y, seriestype = st, layout = 6, mc = :purple,\rlc = :darkgreen,\rfc = :skyblue\r) 透明度 色の透明度を決定するキーワードは、色を指定するキーワード名でcolorをalphaに置き換えたものだ。また、ギリシャ文字のαを直接使用してもよい。\nまたは、RGBAのように、透明度が含まれる色コードを、色を指定するキーワードに入力してもよい。\nplot(rand(20, 3), layout = (3,1), seriestype = [:scatter :line :bar],\rmc = :red,\rlc = :green,\rfc = :blue\r) plot(rand(20, 3), layout = (3,1), seriestype = [:scatter :line :bar],\rmarkeralpha = 0.5, mc = :red,\rla = 0.5, lc = :green,\rfα = 0.5, fc = :blue\r) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参考 色の使い方 パレットの使い方 カラーグラデーション（グラデーション）の使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフの色の指定方法 軸、軸の名前、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3601,"permalink":"https://freshrimpsushi.github.io/jp/posts/3601/","tags":null,"title":"ジュリアプロットでグラフ要素の色を指定する方法"},{"categories":"줄리아","contents":"コード Juliaで色を扱うために提供されるパッケージはColors.jlだ。視覚化パッケージのPlots.jlを読み込むと、Colors.jlの中の機能も一緒に使用できる。RGB空間を表す色コードにはRGB, BGR, RGB24, RGBX, XRGBがサポートされており、これらはAbstractRGBのサブタイプだ。RGBAはRGBに透明度が加わったものだ。\njulia\u0026gt; using Plots\rjulia\u0026gt; subtypes(AbstractRGB)\r5-element Vector{Any}:\rBGR\rRGB\rRGB24\rRGBX\rXRGB\rjulia\u0026gt; subtypes(AbstractRGBA)\r2-element Vector{Any}:\rBGRA\rRGBA 文字列 関数plot()の色を指定するキーワードに文字列で\u0026quot;rgb(255, 0, 0)\u0026quot;のように入力すれば、RGBコードが(255, 0, 0)の色を使用できる。下を見ればわかるが、文字列を入力してもいい理由は、plot()が文字列を自動でパースしてくれるからのようだ。名前がある色の場合は、\u0026quot;red\u0026quot;や:redのように文字列やシンボルで使用できる。\nusing Plots\rr = \u0026#34;rgb(255, 0, 0)\u0026#34; # 정수로 표현된 RGB 빨간색\rg = \u0026#34;rgba(0, 255, 0, 0.2)\u0026#34; # 투명도가 0.2인 RGB 초록색\rp = \u0026#34;rgb(50%, 0%, 100%)\u0026#34; # 퍼센테이지로 표현된 RGB 보라색\rplot(\rplot(rand(15), lc = r),\rbar(rand(15), fc = g),\rscatter(rand(15), mc = p),\rlayout = (3, 1)\r) パース colorant\u0026quot;rgb(0, 0, 0)\u0026quot;のようにRGB色コードをパースできる。\njulia\u0026gt; r = colorant\u0026#34;rgb(255, 0, 0)\u0026#34; # 정수로 표현된 RGB 빨간색\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; g = colorant\u0026#34;rgba(0, 255, 0, 0.2)\u0026#34; # 투명도가 0.2인 초록색\rRGBA{N0f8}(0.0,1.0,0.0,0.502)\rjulia\u0026gt; p = colorant\u0026#34;rgb(50%, 0%, 100%)\u0026#34; # 퍼센테이지로 표현된 RGB 보라색\rRGB{N0f8}(0.502,0.0,1.0)\rjulia\u0026gt; plot(\rplot(rand(15), lc = r),\rbar(rand(15), fc = g),\rscatter(rand(15), mc = p),\rlayout = (3, 1)) parse(RGB, \u0026quot;rgb(0, 255, 255)\u0026quot;)と同様にパースすることができる。\njulia\u0026gt; parse(RGB, \u0026#34;rgb(0, 255, 255)\u0026#34;)\rRGB{N0f8}(0.0,1.0,1.0)\rjulia\u0026gt; parse(RGBA, \u0026#34;rgba(0, 255, 0, 0.5)\u0026#34;)\rRGBA{N0f8}(0.0,1.0,0.0,0.502) 直接定義 関数RGB(), RGBA()などを使えば、色を直接定義できる。\njulia\u0026gt; RGB(1, 0, 0)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0, 0.0, 0.0)\rRGB{Float64}(1.0,0.0,0.0)\rjulia\u0026gt; RGBA(1, 0, 0.5, 0.5)\rRGBA{Float64}(1.0,0.0,0.5,0.5) colorantでパースされた色と正確に同じタイプを得るには、入力としてN0f8タイプの数を入力する必要がある。これを使用するためにはFixedPointNumbers.jlが必要だ。または、直接1.0N0f8のように定義してもいい。以下は、colorant\u0026quot;rgb(255, 0, 0)\u0026quot;と同じ、赤色の色を返すコードだ。\njulia\u0026gt; using FixedPointNumbers\r# RGB 빨간색 RGB\rjulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0, 0.0, 0.0)\rRGB{Float64}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0N0f8, 0N0f8, 0N0f8)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(reinterpret(N0f8, UInt8(255)), reinterpret(N0f8, UInt8(0)), reinterpret(N0f8, UInt8(0)))\rRGB{N0f8}(1.0,0.0,0.0) 他の色空間からの変換 関数convert()は、他の色空間の色コードをRGBコードに変換する。\njulia\u0026gt; using Colors\rjulia\u0026gt; convert(RGB{N0f8}, HSL(270, 0.5, 0.5))\rERROR: UndefVarError: `N0f8` not defined\rjulia\u0026gt; using FixedPointNumbers\rjulia\u0026gt; convert(RGB{N0f8}, HSL(270, 0.5, 0.5))\rRGB{N0f8}(0.502,0.251,0.749) 色名を得る 関数rgb_string()とrgba_string()はそれぞれ色のRGB、RGBAコードを文字列で返す。\njulia\u0026gt; rgb_string(colorant\u0026#34;rgba(255, 0, 0, 0.5)\u0026#34;)\r\u0026#34;rgb(255, 0, 0)\u0026#34;\rjulia\u0026gt; rgba_string(colorant\u0026#34;rgba(255, 0, 0, 0.5)\u0026#34;)\r\u0026#34;rgba(255, 0, 0, 0.502)\u0026#34;\rjulia\u0026gt; rgb_string(colorant\u0026#34;red\u0026#34;)\r\u0026#34;rgb(255, 0, 0)\u0026#34;\rjulia\u0026gt; rgb_string(parse(RGB, :blue))\r\u0026#34;rgb(0, 0, 255)\u0026#34;\rjulia\u0026gt; rgb_string(colorant\u0026#34;#00FF00\u0026#34;)\r\u0026#34;rgb(0, 255, 0)\u0026#34; 併せて見る Plotsで色を使う方法 RGB色コードの使い方 HEX色コードの使い方 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10, FixedPointNumbers v0.8.4 併せて見る 色を使う方法 パレットの使い方 カラーグラデーションの使い方 色の処理のためのパッケージColors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフ色を指定する方法 軸、軸名、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3600,"permalink":"https://freshrimpsushi.github.io/jp/posts/3600/","tags":null,"title":"ジュリアでRGBカラーコードを使用する方法"},{"categories":"줄리아","contents":"概要1 Juliaで色処理のためのパッケージであるColors.jlの機能について紹介する。視覚化パッケージであるPlots.jlを使う場合はColors.jlを別に読み込む必要はない。以下の機能を提供する。\n色のパースと変換 色マップ 色スケール パースと変換 strを色情報を表した文字列とすると、@colorant_strまたはparse(Colorant, str)を通じて文字列を特定の色空間の色コードにパースできる。なお、colorantは染料、色素などの意味を持つ英単語である。\nRGBコードの使い方 HEXコードの使い方 julia\u0026gt; using Colors\rjulia\u0026gt; colorant\u0026#34;red\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(Colorant, \u0026#34;rgba(0, 255, 0, 0.5)\u0026#34;)\rRGBA{N0f8}(0.0,1.0,0.0,0.502)\rjulia\u0026gt; parse(Colorant, \u0026#34;#FF0000\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(Colorant, \u0026#34;hsl(120, 100%, 25%)\u0026#34;)\rHSL{Float32}(120.0f0,1.0f0,0.25f0) convert()関数で他の色空間の色コードに変換できる。\njulia\u0026gt; convert(RGB, HSL(270, 0.5, 0.5))\rRGB{Float64}(0.5,0.25,0.75) 色の補間 range()関数を通じて色を補間interpolationできる。この動作は非常に論理的で直感的である。例えば、RGBコードを考えてみよう。赤はRGBコードで$(255, 0, 0)$または$(1, 0, 0)$で表され、これは実質的に3次元ベクトルと同じだ。したがって、二つのベクトル間を補間するrange()関数の引数として色コードを使う理由はない。\njulia\u0026gt; v1 = [1.0, 0.0, 0.0];\rjulia\u0026gt; v2 = [0.0, 0.5, 0.0];\rjulia\u0026gt; collect(range(v1, v2, length = 15))\r15-element Vector{Vector{Float64}}:\r[1.0, 0.0, 0.0]\r[0.9285714285714286, 0.03571428571428571, 0.0]\r[0.8571428571428572, 0.07142857142857142, 0.0]\r[0.7857142857142857, 0.10714285714285714, 0.0]\r[0.7142857142857143, 0.14285714285714285, 0.0]\r[0.6428571428571428, 0.17857142857142858, 0.0]\r[0.5714285714285714, 0.21428571428571427, 0.0]\r[0.5, 0.25, 0.0]\r[0.4285714285714286, 0.2857142857142857, 0.0]\r[0.3571428571428571, 0.32142857142857145, 0.0]\r[0.2857142857142857, 0.35714285714285715, 0.0]\r[0.2142857142857143, 0.39285714285714285, 0.0]\r[0.1428571428571429, 0.42857142857142855, 0.0]\r[0.0714285714285714, 0.4642857142857143, 0.0]\r[0.0, 0.5, 0.0]\rjulia\u0026gt; c1 = colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; c2 = colorant\u0026#34;rgb(0, 128, 0)\u0026#34;\rRGB{N0f8}(0.0,0.502,0.0)\rjulia\u0026gt; range(colorant\u0026#34;rgb(255,0,0)\u0026#34;, colorant\u0026#34;rgb(0,128,0)\u0026#34;, length=15)\r15-element Array{RGB{N0f8},1} with eltype RGB{FixedPointNumbers.N0f8}:\rRGB{N0f8}(1.0,0.0,0.0)\rRGB{N0f8}(0.929,0.035,0.0)\rRGB{N0f8}(0.859,0.071,0.0)\rRGB{N0f8}(0.784,0.11,0.0)\rRGB{N0f8}(0.714,0.145,0.0)\rRGB{N0f8}(0.643,0.18,0.0)\rRGB{N0f8}(0.573,0.216,0.0)\rRGB{N0f8}(0.502,0.251,0.0)\rRGB{N0f8}(0.427,0.286,0.0)\rRGB{N0f8}(0.357,0.322,0.0)\rRGB{N0f8}(0.286,0.357,0.0)\rRGB{N0f8}(0.216,0.392,0.0)\rRGB{N0f8}(0.141,0.431,0.0)\rRGB{N0f8}(0.071,0.467,0.0)\rRGB{N0f8}(0.0,0.502,0.0) 上記の色レンジの視覚化は、VS CodeでJulia拡張機能をインストールして実行すると以下のように得られる。\nまた、range()の戻り値はパレットとして使用できる。\nmy_palette = range(colorant\u0026#34;rgb(255,0,0)\u0026#34;, colorant\u0026#34;rgb(0,128,0)\u0026#34;, length=15)\rplot(rand(10, 15), palette = my_palette) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10 関連項目 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理のためのパッケージColors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロット毎にグラフの色を指定する方法 軸、軸ラベル、目盛り、目盛り値の色の指定方法 背景色の設定方法 https://juliagraphics.github.io/Colors.jl/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3599,"permalink":"https://freshrimpsushi.github.io/jp/posts/3599/","tags":null,"title":"ジュリアのカラー処理のためのパッケージ"},{"categories":"줄리아","contents":"概要 Juliaで色を便利に使うためのパッケージにはColors.jlがある。「Plots.jl」という視覚化パッケージを読み込めば一緒に使うことができる。\nシンボルと文字列 名前がついた色のリストを確認する方法は、コンソールにColors.color_namesを入力するか、公式ドキュメントを確認することだ。\njulia\u0026gt; using Plots\rjulia\u0026gt; Colors.color_names\rDict{String, Tuple{Int64, Int64, Int64}} with 666 entries:\r\u0026#34;darkorchid\u0026#34; =\u0026gt; (153, 50, 204)\r\u0026#34;chocolate\u0026#34; =\u0026gt; (210, 105, 30)\r\u0026#34;chocolate2\u0026#34; =\u0026gt; (238, 118, 33)\r\u0026#34;grey69\u0026#34; =\u0026gt; (176, 176, 176)\r\u0026#34;grey97\u0026#34; =\u0026gt; (247, 247, 247)\r\u0026#34;olivedrab3\u0026#34; =\u0026gt; (154, 205, 50)\r\u0026#34;deeppink2\u0026#34; =\u0026gt; (238, 18, 137)\r\u0026#34;mediumpurple2\u0026#34; =\u0026gt; (159, 121, 238)\r\u0026#34;ivory1\u0026#34; =\u0026gt; (255, 255, 240)\r⋮ =\u0026gt; ⋮ 色を指定できるキーワード引数には基本的にシンボルと文字列が使用できる。色名をシンボル、文字列で入力すれば、その色が反映される。入力するものが何であれ、Colors.parse(Colorant, 色名)に渡されるため、シンボルでも文字列でも結果は同じである。\njulia\u0026gt; Colors.parse(Colorant, :red)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; Colors.parse(Colorant, \u0026#34;red\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0) 様々なグラフに色を指定してみると、その結果は次のようになる。\nplot(randn(50, 6),\rseriescolor = [:red :hotpink1 :purple3 \u0026#34;blue\u0026#34; \u0026#34;lime\u0026#34; \u0026#34;brown4\u0026#34;],\rseriestype = [:line :scatter :histogram :shape :sticks :steppre],\rlayout = (3,2)\r) RGB RGB色コードはcolorant\u0026quot;rgb(255, 0, 0)\u0026quot;で使用できる。rgb()には$[0, 255]$内の整数のみ入力できる。\njulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34; # rgb() notation with integers in [0, 255]\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;rgba(0, 0, 255, 0.5)\u0026#34; # with alpha in [0, 1]\rRGBA{N0f8}(0.0,0.0,1.0,0.502)\rplot(rand(20, 2),\rseriescolor = [colorant\u0026#34;rgb(255, 0, 0)\u0026#34; colorant\u0026#34;rgba(0, 0, 255, 0.5)\u0026#34;],\rlayout = 2\r) RGB色コードを扱う詳細はこちらを参照。\nHEX 6桁のHEXコードはcolorant\u0026quot;#FF0000\u0026quot;、3桁のHEXコードはcolorant\u0026quot;#00f\u0026quot;のように使用できる。\njulia\u0026gt; colorant\u0026#34;#FF0000\u0026#34; # 6-digit hex notation\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;#00f\u0026#34; # 3-digit hex notation\rRGB{N0f8}(0.0,0.0,1.0)\rjulia\u0026gt; plot(rand(20, 2),\rseriescolor = [colorant\u0026#34;#FF0000\u0026#34; colorant\u0026#34;#00f\u0026#34;],\rlayout = 2\r) HEX色コードを扱う詳細はこちらを参照。\n環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10 一緒に見る 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定の仕方 サブプロットごとのグラフ色の指定の仕方 軸、軸名、目盛り、目盛り値の色の指定の仕方 背景色の指定の仕方 ","id":3598,"permalink":"https://freshrimpsushi.github.io/jp/posts/3598/","tags":null,"title":"ジュリアプロットでの色の使用方法"},{"categories":"줄리아","contents":"コード 関数 printstyled(文字列; color = 色)を使用すると、出力される関数を装飾できる。キーワード引数 colorの入力としては、シンボル、自然数$(0 \\le n \\le 255)$が可能である。文字列は不可能であることに注意。\n利用可能なシンボルには、:blink、:reverse 等の色ではないものも含まれている。これらはキーワード引数として blink = true、bold = true等と入力して適用することもできる。\n:normal :default :blink :bold :hidden :nothing :reverse :underline :white :light_white :black :light_black :blue :light_blue :cyan :light_cyan :green :light_green :magenta :light_magenta :red :light_red :yellow :light_yellow symbols = [:normal :default :blink :bold :hidden :nothing :reverse :underline :white :light_white :black :light_black :blue :light_blue :cyan :light_cyan :green :light_green :magenta :light_magena :red :light_red :yellow :light_yellow]\rfor i ∈ 1:length(symbols)\rprintstyled(\u0026#34;Hello ($(symbols[i]))\\n\u0026#34;, color = symbols[i])\rend Base.text_colorsを入力すると、キーワード引数（シンボルを含む）で可能な全ての値を返す。\njulia\u0026gt; Base.text_colors\rDict{Union{Int64, Symbol}, String} with 280 entries:\r56 =\u0026gt; \u0026#34;\\e[38;5;56m\u0026#34;\r35 =\u0026gt; \u0026#34;\\e[38;5;35m\u0026#34;\r60 =\u0026gt; \u0026#34;\\e[38;5;60m\u0026#34;\r220 =\u0026gt; \u0026#34;\\e[38;5;220m\u0026#34;\r:blink =\u0026gt; \u0026#34;\\e[5m\u0026#34;\r67 =\u0026gt; \u0026#34;\\e[38;5;67m\u0026#34;\r215 =\u0026gt; \u0026#34;\\e[38;5;215m\u0026#34;\r73 =\u0026gt; \u0026#34;\\e[38;5;73m\u0026#34;\r251 =\u0026gt; \u0026#34;\\e[38;5;251m\u0026#34;\r115 =\u0026gt; \u0026#34;\\e[38;5;115m\u0026#34;\r⋮ =\u0026gt; ⋮ 参照 パッケージ Crayons.jlも使用できる。\n環境 OS: Windows11 Version: Julia 1.9.4 ","id":3597,"permalink":"https://freshrimpsushi.github.io/jp/posts/3597/","tags":null,"title":"ジュリアでテキスト出力を装飾する組み込み関数"},{"categories":"데이터과학","contents":"定義 データセット$X \\subset \\mathbb{R}^{n}$が与えられたとする。$m \\lt n$に対して、次のようなマッピングを次元削減dimension reductionという。\n$$ r : X \\to \\mathbb{R}^{m} $$\nまたは、主に機械学習で、パフォーマンスを可能な限り維持しながら入力変数を減らす方法の全体を次元削減技術という。\n説明 次元削減とは、その名の通りベクトルの次元を減らすことを言う。データをより簡単に、より直感的に理解するために使用されることが多い。次元を減らす方法はアルゴリズムによって異なる。特定の成分をそのまま削除することもあれば、既存のデータを用いて定められたルールに従って次元が小さい新しいデータを生成することもある。以下のような技術がある。\n主成分分析PCA 数理統計学での主成分分析 目的 可視化 我々は4次元以上のデータを効率的に可視化することが事実上不可能である。さらに3次元のデータでさえ、その形状によっては可視化に困難を感じることがある。可視化に困難を感じるとは、データの特徴をよく表す図を描くことが難しいということである。3次元データであれば、視点によってその形状が異なって見えるだろう。このような時に次元を減らして描いてみると、データの特徴を把握しやすくなるかもしれない。以下の図は、同じデータだが見る方向によってその形状が著しく異なる例を示している。右の図は左のデータを$xy$-平面に射影したものである。\n4次元データであるアイリスデータセットを以下のように複数の2次元図に分けて可視化することが、多くのデータサイエンスの教科書で紹介されている。\n選択と集中 あまり重要でない情報を排除して、より重要な情報に集中するために次元削減を使用することができる。ここで言う「あまり重要でない情報」とは、ノイズとして扱われるか、重複した情報を指す。例えば、下の左の表を見ると、最初の列がすべてのデータに対して同じ値であることがわかる。また、2番目の列と3番目の列は異なる値だが、実質的には同じ値であることがわかる。したがって、最初の列と2番目（または3番目）の列を削除することで次元削減を行うことができる。また、右の表は大邱の天気情報をまとめたものである。一見すると、ここで不要な情報はないように見えるが、「日差 = 最高気温 - 最低気温」であるため、この3つは線形独立ではなく、実際には回帰分析時にエラーを引き起こす可能性がある。したがって、この場合には多重共線性を除去するために4番目の列を削除することが次元削減である。\n学校\r学年\r所属\r名前\rハイブ高等学校\r3学年\rプロミスナイン\nfromis_9\rイ・ナギョン\n이나경\rハイブ高等学校\r3学年\rプロミスナイン\nfromis_9\rペク・ジホン\n백지헌\rハイブ高等学校\r2学年\rル・セラフィム\nLE SSERAFIM\rキム・チェウォン\n김채원\rハイブ高等学校\r2学年\rル・セラフィム\nLE SSERAFIM\rホ・ユンジン\n허윤진\rハイブ高等学校\r1学年\rニュージンス\nNewJeans\rヘリン\n해린\rハイブ高等学校\r1学年\rニュージンス\nNewJeans\rミンジ\n민지\r日付\r最高気温\r最低気温\r日差\r降水確率\r19일\r32º\r24º\r8º\r60%\r20일\r33º\r22º\r11º\r0%\r21일\r32º\r23º\r9º\r30%\r22일\r30º\r21º\r9º\r60%\r23일\r31º\r24º\r7º\r60%\r24일\r33º\r25º\r8º\r60%\r軽量化 データの次元が減ると、それだけ保存しなければならない数字が少なくなるため、データ自体の容量が減る。人工神経網の場合、MLPは線形層で構成されており、入力データの次元がモデルのパラメータ数に影響を与える。この場合、次元削減を使用してモデルのパラメータを減らすことができる。CNNのように、入力データの次元がモデルのパラメータ数に影響を与えない場合でも、計算速度での利点をもたらすことができる。\n過学習防止 適切な次元削減は、オーバーフィッティングをある程度防ぐことができるとされている。\n","id":3563,"permalink":"https://freshrimpsushi.github.io/jp/posts/3563/","tags":null,"title":"データサイエンスにおける次元削減"},{"categories":"머신러닝","contents":"概要 TensorFlowでは、Kerasを使用して簡単にニューラルネットワークを定義することができます。以下では、Sequential()と関数型APIを使用してシンプルなMLPを定義し、訓練する方法を紹介します。ただし、Sequential()はモデルの定義自体は簡単ですが、それを使用して複雑な構造を設計するには適していません。同様に、関数型APIを使用して複雑な構造を設計する場合は、keras.Modelクラスの使用が適しており、より複雑で自由なカスタマイズを求める場合は、Kerasを使用せずに低レベルで実装する方が良いでしょう。どのような作業にディープラーニングを使用するかによって異なりますが、もし自分が理工学の研究者であり、専門分野にディープラーニングを応用したい場合は、以下の方法を主に使用する可能性は低いでしょう。ディープラーニングを初めて学び、実践する際は、「これが使用法だ」と感じ取る程度だと考えられます。\nシーケンシャルモデル モデル定義 サイン関数 $\\sin : \\mathbb{R} \\to \\mathbb{R}$ の近似のために、入力と出力の次元が1のMLPを次のように定義しましょう。\nimport tensorflow as tf\rfrom tensorflow.keras import Sequential\rfrom tensorflow.keras.layers import Dense\r# モデル定義\rmodel = Sequential([Dense(10, input_dim = 1, activation = \u0026#34;relu\u0026#34;),\rDense(10, input_dim = 10, activation = \u0026#34;relu\u0026#34;),\rDense(1, input_dim = 10)])\rmodel.summary() # output↓\r# Model: \u0026#34;sequential_3\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param # # =================================================================\r# dense_9 (Dense) (None, 10) 20 # # dense_10 (Dense) (None, 10) 110 # # dense_11 (Dense) (None, 1) 11 # # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ keras.layers.Dense()の特徴の一つに、入力の次元を記述する必要がないという点があります。なぜこのような許容がされているのかは分かりませんが、コードの可読性のためには（特に他の人が見る可能性があるコードであれば）入力の次元を明示的に記述することが良いでしょう。このために、出力の次元が左、入力の次元が右に記述されるという特徴があります。したがって、モデルの構造を読むためには、アラビア語ではなく、右から左に読む必要があります。もし線形層を線形変換としての行列と考えた場合、$\\mathbf{y} = A\\mathbf{x}$ なので、入力が右、出力が左に来るのが自然です。しかし、TensorFlowはこのような数学的な厳密さを考慮して設計された言語ではないので、この理由だけでそう設計されたとは考えにくいです。数学的な厳密さを非常に重視するJuliaでも、線形層は Dense(in, out) のように実装されています。これは、左から右へ読む方が便利で分かりやすいためです。元々、$X$ から $Y$ への関数 $f$ の記述自体が $f : X \\to Y$ であり、（Kerasを除いて）世界のどこにも右から左へのマッピングで記述される関数はありません。\nデータ生成 サイン関数を訓練するため、データをサイン関数の関数値とし、モデルの出力とサイン関数のグラフを比較すると以下のようになります。\n# データ生成\rfrom math import pi\rx = tf.linspace(0., 2*pi, num=1000) # 入力データ\ry = tf.sin(x) # 出力データ(label)\r# モデルの出力確認\rimport matplotlib.pyplot as plt\rplt.plot(x, model(x), label=\u0026#34;model\u0026#34;)\rplt.plot(x, y, label=\u0026#34;sin\u0026#34;)\rplt.legend()\rplt.show() 訓練及び結果 from tensorflow.keras.optimizers import Adam\rmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\u0026#39;mse\u0026#39;) model.compile(optimizer, loss, metric) .compile() メソッドでオプティマイザと損失関数を指定します。他の主要なオプションには metric があり、これはモデルを評価する関数を意味します。これは loss と同じになることもありますし、異なることもあります。例えば、MLPで MNISTデータセット を学習する場合、lossは出力とラベルのMSEであり、metricは全データの中で予測に成功した割合になるでしょう。\n\u0026gt; model.fit(x, y, epochs=10000, batch_size=1000, verbose=\u0026#39;auto\u0026#39;)\r.\r.\r.\rEpoch 9998/10000\r1/1 [==============================] - 0s 8ms/step - loss: 6.2260e-06\rEpoch 9999/10000\r1/1 [==============================] - 0s 4ms/step - loss: 6.2394e-06\rEpoch 10000/10000\r1/1 [==============================] - 0s 3ms/step - loss: 6.2385e-06 .fit() メソッドに入力とラベル、エポック数、バッチサイズなどを入力すると訓練が実行されます。verboseは訓練の進行状況をどのように表示するかを決めるオプションで、0、1、2の中から選択でき、0は何も表示しません。他のオプションは以下のフォーマットで表示されます。 # verbose=1\rEpoch (現在のエポック)/(全エポック)\r(現在のバッチ)/(全体のバッチ) [==============================] - 0s 8ms/step - loss: 0.7884\r# verbose=2\rEpoch (現在のエポック)/(全エポック)\r(現在のバッチ)/(全体のバッチ) - 0s - loss: 0.7335 - 16ms/epoch - 8ms/step 訓練が終わり、サイン関数とモデルの関数値を比較すると、学習がうまく行われたことがわかります。\n関数型API Input() 関数と Model() 関数でレイヤーを直接連結する方法です。MLPのようなシンプルなモデルであれば、上記のシーケンシャルモデルで定義する方がはるかに簡単です。上のシーケンシャルモデルで定義したニューラルネットワークと同じ構造のモデルを定義する方法は次のようになります。\nfrom tensorflow.keras import Model\rfrom tensorflow.keras.layers import Input, Dense\rinput = Input(shape=(10)) # 変数は \u0026#34;出力の次元 = 最初の層の入力の次元\u0026#34;\rdense1 = Dense(10, activation = \u0026#34;relu\u0026#34;)(input)\rdense2 = Dense(10, activation = \u0026#34;relu\u0026#34;)(dense1)\routput = Dense(1)(dense2)\rmodel = Model(inputs=input, outputs=output)\rmodel.summary() # output↓\r# Model: \u0026#34;model_10\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param #\r# =================================================================\r# input_13 (InputLayer) [(None, 1)] 0\r# # dense_19 (Dense) (None, 10) 20\r# # dense_20 (Dense) (None, 10) 110\r# # dense_21 (Dense) (None, 1) 11\r# # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ Inputはインプットレイヤーを定義する関数です。正確にはレイヤーではなくテンソルですが、重要な点ではないので、ただの入力層として受け入れても良いでしょう。混乱する点は、出力の次元を入力する必要があるという点です。つまり、最初の層の入力の次元を入力する必要があります。これを定義した後、Dense関数の入力として入力し、明示的に直接各層を連結します。最後に、Model関数で入力と出力を引数に入れると、モデルを定義することができます。\nその後、モデルを .compile() メソッドでコンパイルし、.fit() メソッドで訓練するプロセスは、上で紹介した通りです。\n環境 OS: Windows11 Version: Python 3.9.13, tensorflow==2.12.0, keras==2.12.0 ","id":3562,"permalink":"https://freshrimpsushi.github.io/jp/posts/3562/","tags":null,"title":"TensorFlow-Kerasでシーケンスモデル、関数型APIでMLPを定義してトレーニングする方法"},{"categories":"줄리아","contents":"コード サイズ plot(x, y, size=(600,400)) Juliaでは、図のサイズは size オプションで設定する。Tuple{Integer, Integer} 型で入力する必要があり、各整数はそれぞれ横ピクセルと縦ピクセルを意味する。デフォルト値は (600,400) だ。\nusing Plots\rx = rand(10)\rplot(x)\rsavefig(\u0026#34;size_default.png\u0026#34;)\rplot(x, size=(1200,800))\rsavefig(\u0026#34;size_(1200,800).png\u0026#34;) 1800x1200の画像 (左)、600x400の画像 (右)\r解像度 plot(x, y, dpi=100) 画像の解像度は dpi オプションで設定し、デフォルト値は 100 だ。論文やレポート、PowerPointなどに添付する場合は、300くらいにするのがいい。\nusing Plots\rx = rand(10)\rplot(x)\rsavefig(\u0026#34;dpi_default.png\u0026#34;)\rplot(x, dpi=300) savefig(\u0026#34;dpi_300.png\u0026#34;) dpi=100の画像 (上)、dpi=300の画像 (下)\rまた、サイズを増やす時は、解像度も一緒に増やさないと、画像が綺麗さを保てないので注意。dpiを300にして保存すると、画像のサイズが1800x1200に増えるが、サイズだけを増やしてdpiをデフォルト値の100のままにしておくと、画像が醜くなるから気をつけて。\ndpi=300, size=1800x1200の画像 (左)、dpi=100, size=1800x1200の画像 (右)\r環境 OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12 ","id":3559,"permalink":"https://freshrimpsushi.github.io/jp/posts/3559/","tags":null,"title":"ジュリアで画像のサイズと解像度を調整する方法"},{"categories":"줄리아","contents":"コード plot!([x1, x2], [y1, y2], arrow=:true) このコードは、プロット上に点$(x1, y1)$から点$(x2, y2)$までの矢印を描く。当然ながら、矢印の先端は終点$(x2, y2)$にある。サイン関数の最大値は以下のように示すことができる。\nusing Plots\rx = range(0, 2π, 100)\rplot(x, sin.(x), label=\u0026#34;\u0026#34;, ylims=(-1.3,1.3))\rplot!([π/2, 3], [1, 1.1], arrow=:true, color=:black, label=\u0026#34;\u0026#34;)\rannotate!(3.7, 1.1, \u0026#34;maximum\u0026#34;) 矢印の先 矢印の先のスタイルは:openか:closedで選ぶことができる。\n指定しないか:trueの場合：折れ線$\\to$ plot!([3π/2, 3], [-1, -1.1], arrow=:open, color=:red, label=\u0026#34;\u0026#34;)\rannotate!(2.3, -1.1, \u0026#34;minimum\u0026#34;) 矢印の方向 矢印の先の方向は:head、:tail、:bothで設定でき、:headがデフォルトである。\nplot!([π/2, π/2], [0, 1], arrow=(:closed, :both), color=:purple, label=\u0026#34;\u0026#34;)\rannotate!(0.75π, 0.5, \u0026#34;amplitude\u0026#34;) 公式ドキュメントで1、headlengthとheadwidthのオプションについての説明があるが、使ってみるとエラーしか出なくてどう使うかよくわからない。\n環境 OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12 https://docs.juliaplots.org/v1.38/api/#Plots.arrow-Tuple\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3558,"permalink":"https://freshrimpsushi.github.io/jp/posts/3558/","tags":null,"title":"ジュリアでグラフィックスに矢印を描く方法"},{"categories":"줄리아","contents":"説明1 Juliaでは、ランダムシードは以下のように固定する。\nseed!([rng=default_rng()], seed) -\u0026gt; rng seed!([rng=default_rng()]) -\u0026gt; rng 入力変数rngはランダムナンバージェネレータの略で、乱数を抽出するアルゴリズムを意味する。Randomパッケージでは、以下のオプションを提供している。\nTaskLocalRNG：デフォルトの設定値だ。 Xoshiro RandomDevice MersenneTwister コード シードを0に固定して三回抽出した後、再び0に固定して三回抽出すると、同じ値が得られることが確認できる。\njulia\u0026gt; using Random\rjulia\u0026gt; Random.seed!(0)\rTaskLocalRNG()\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.4056994708920292\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.06854582438651502\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.8621408571954849\rjulia\u0026gt; Random.seed!(0)\rTaskLocalRNG()\rjulia\u0026gt; rand(3)\r3-element Vector{Float64}:\r0.4056994708920292\r0.06854582438651502\r0.8621408571954849 https://docs.julialang.org/ja/v1/stdlib/Random/index.html#Random.seed!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3555,"permalink":"https://freshrimpsushi.github.io/jp/posts/3555/","tags":null,"title":"ジュリアでランダムシードを固定する方法"},{"categories":"줄리아","contents":"日本語訳 説明 ボックスプロットを描くには、統計的可視化パッケージであるStatsPlots.jlを使用する必要がある。\nboxplot([data], labels=[label]) コード using StatsPlots\rx = rand(0:100, 100)\ry = rand(50:100, 100)\rz = cat(x,y, dims=1)\rboxplot(x, label=\u0026#34;x\u0026#34;)\rboxplot!(y, label=\u0026#34;y\u0026#34;)\rboxplot!(z, label=\u0026#34;z\u0026#34;) または、boxplot([x,y,z], label=[\u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot;])も同じ図を描く。lableにはコンマがないことに注意しよう。つまり、$3 \\times 1$ベクターではなく、$1 \\times 3$配列である必要がある。\nx軸の目盛り 文字列でx軸の目盛りを表したい場合、\nboxplot([x, y, z], xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;]) または、以下のコードも同じ図を描く。違いは、実際の座標がどうなっているかだ。上のコードでは、実際に各ボックスが描かれるx座標が1, 2, 3だが、グラフの目盛り値だけx, y, zと見えるように変えたものだ。下のコードは、実際に\u0026quot;x\u0026quot;, \u0026ldquo;y\u0026rdquo;, \u0026ldquo;z\u0026quot;の座標上にボックスを描く。\n2次元配列で描く a = rand(100, 3)\rboxplot(a, xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;]) データフレームで描く データフレームそのもので描くことはできず、配列に変換する必要がある。\nusing MLDatasets\rusing DataFrames\rdf = Iris().features\rboxplot(Array(df), xticks=(1:4, names(df)), label=reshape(names(df), (1,4))) 平均 平均を表示するオプションは別にない。scatterで打ってみよう。\nusing Statistics\rboxplot(fill(\u0026#34;x\u0026#34;, length(x)), x, labels=\u0026#34;x\u0026#34;)\rboxplot!(fill(\u0026#34;y\u0026#34;, length(y)), y, labels=\u0026#34;y\u0026#34;)\rboxplot!(fill(\u0026#34;z\u0026#34;, length(z)), z, labels=\u0026#34;z\u0026#34;)\rscatter!([\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;], [mean(x), mean(y), mean(z)], color=palette(:default)[1:3], label=\u0026#34;\u0026#34;) または、次のコードも同じ図を描く。\nboxplot([x, y, z], xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;])\rscatter!([1, 2, 3], [mean(x), mean(y), mean(z)], color=palette(:default)[1:3], label=\u0026#34;\u0026#34;) 環境 OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12, StatsPlots v0.15.5, DataFrames v1.5.0, MLDatasets v0.7.11 併せて見る Python matplotlibで描く方法 ","id":3553,"permalink":"https://freshrimpsushi.github.io/jp/posts/3553/","tags":null,"title":"ジュリアでボックスプロットを描く方法"},{"categories":"줄리아","contents":"概要 ジュリアで決定木Decision Treeを実装したDecisionTree.jlパッケージを紹介する1。\nコード 例としては、Rの組み込みデータであるirisデータセットを使う。目標は、四つの変数SepalLength, SepalWidth, PetalLength, PetalWidthを用いてSpeciesを予測する決定木を作り、そのパフォーマンスを評価することである。\njulia\u0026gt; iris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\r150×5 DataFrame\rRow │ SepalLength SepalWidth PetalLength PetalWidth Speci ⋯\r│ Float64 Float64 Float64 Float64 Cat… ⋯\r─────┼──────────────────────────────────────────────────────────\r1 │ 5.1 3.5 1.4 0.2 setos ⋯\r2 │ 4.9 3.0 1.4 0.2 setos 3 │ 4.7 3.2 1.3 0.2 setos 4 │ 4.6 3.1 1.5 0.2 setos 5 │ 5.0 3.6 1.4 0.2 setos ⋯\r6 │ 5.4 3.9 1.7 0.4 setos 7 │ 4.6 3.4 1.4 0.3 setos 8 │ 5.0 3.4 1.5 0.2 setos 9 │ 4.4 2.9 1.4 0.2 setos ⋯\r10 │ 4.9 3.1 1.5 0.1 setos 11 │ 5.4 3.7 1.5 0.2 setos 12 │ 4.8 3.4 1.6 0.2 setos 13 │ 4.8 3.0 1.4 0.1 setos ⋯\r14 │ 4.3 3.0 1.1 0.1 setos 15 │ 5.8 4.0 1.2 0.2 setos ⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋱\r136 │ 7.7 3.0 6.1 2.3 virgi 137 │ 6.3 3.4 5.6 2.4 virgi ⋯\r138 │ 6.4 3.1 5.5 1.8 virgi 139 │ 6.0 3.0 4.8 1.8 virgi 140 │ 6.9 3.1 5.4 2.1 virgi 141 │ 6.7 3.1 5.6 2.4 virgi ⋯\r142 │ 6.9 3.1 5.1 2.3 virgi 143 │ 5.8 2.7 5.1 1.9 virgi 144 │ 6.8 3.2 5.9 2.3 virgi 145 │ 6.7 3.3 5.7 2.5 virgi ⋯\r146 │ 6.7 3.0 5.2 2.3 virgi 147 │ 6.3 2.5 5.0 1.9 virgi 148 │ 6.5 3.0 5.2 2.0 virgi 149 │ 6.2 3.4 5.4 2.3 virgi ⋯\r150 │ 5.9 3.0 5.1 1.8 virgi 1 column and 120 rows omitted モデル作成 julia\u0026gt; using DecisionTree\rjulia\u0026gt; model = DecisionTreeClassifier(max_depth=2)\rDecisionTreeClassifier\rmax_depth: 2\rmin_samples_leaf: 1\rmin_samples_split: 2\rmin_purity_increase: 0.0\rpruning_purity_threshold: 1.0\rn_subfeatures: 0\rclasses: nothing\rroot: nothing モデルを作る。DecisionTreeClassifier()を通じて、決定木で使われるパラメーターを指定できる。\nモデルフィッティング julia\u0026gt; features = Matrix(iris[:, Not(:Species)]);\rjulia\u0026gt; labels = iris.Species;\rjulia\u0026gt; fit!(model, features, labels) DecisionTreeClassifier\rmax_depth: 2\rmin_samples_leaf: 1\rmin_samples_split: 2\rmin_purity_increase: 0.0\rpruning_purity_threshold: 1.0\rn_subfeatures: 0\rclasses: [\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;]\rroot: Decision Tree Leaves: 3\rDepth: 2 データを独立変数と従属変数に分けて、fit!()関数でモデルを学習させる。\nパフォーマンス確認 julia\u0026gt; print_tree(model)\rFeature 3 \u0026lt; 2.45 ?\r├─ setosa : 50/50\r└─ Feature 4 \u0026lt; 1.75 ?\r├─ versicolor : 49/54\r└─ virginica : 45/46 学習が終わったモデルは、print_tree()関数を通じてどんな構造をしているか確認できる。\njulia\u0026gt; sum(labels .== predict(model, features)) / length(labels)\r0.96 簡単に正解率を確認した結果、96%程度でかなり良いことがわかった。\n全コード using RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rusing DecisionTree\rmodel = DecisionTreeClassifier(max_depth=2)\rfeatures = Matrix(iris[:, Not(:Species)]);\rlabels = iris.Species;\rfit!(model, features, labels)\rprint_tree(model)\rsum(labels .== predict(model, features)) / length(labels) 環境 OS: Windows julia: v1.9.0 https://github.com/JuliaAI/DecisionTree.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2618,"permalink":"https://freshrimpsushi.github.io/jp/posts/2618/","tags":null,"title":"ジュリアで決定木を使う方法"},{"categories":"줄리아","contents":"概要 ジュリアでコレクションの重複をなくし、チェックする方法を紹介する。重複をなくすunique()関数は、アルゴリズム的に見て難しくないが、自分で実装しようとすると面倒で、効率的でないかもしれない。重複要素がないかをチェックするallunique()関数は、実装も簡単なほどに見つけることがない関数だから、この機会にしっかり覚えておくべきだ。\nコード unique() julia\u0026gt; x = [3, 1, 4, 1, 5, 9, 2, 6, 5];\rjulia\u0026gt; y = unique(x)\r7-element Vector{Int64}:\r3\r1\r4\r5\r9\r2\r6 allunique() 1 実はこのポストは、allunique()を紹介することが目的だ。コレクションが重複要素を持っているかをチェックする常識的な方法の一つは、length(unique(x)) == length(x)としてunique()を適用し、要素が減ったかをチェックすることだ。\nこの方法はあまりにも簡単で、効率に対して過信しやすいが、一旦unique()関数は長さ$n$の配列を少なくとも一度は全ての要素を見る必要があるので、時間複雑度は$O (n)$だ。これは、コード内で要素の重複チェックを頻繁に行う場合、確かに負担になるレベルのコストであり、allunique()は配列の長さによって実装が異なり、途中で重複要素が見つかれば計算を中断し、チェックに成功するなど、性能面で明らかな利点がある。\njulia\u0026gt; allunique(x)\rfalse\rjulia\u0026gt; allunique(y)\rtrue 完全なコード x = [3, 1, 4, 1, 5, 9, 2, 6, 5];\ry = unique(x)\rallunique(x)\rallunique(y) 環境 OS: Windows julia: v1.9.0 https://docs.julialang.org/en/v1/base/collections/#Base.allunique\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2616,"permalink":"https://freshrimpsushi.github.io/jp/posts/2616/","tags":null,"title":"ジュリアでコレクションの重複を削除する方法"},{"categories":"줄리아","contents":"概要 Juliaでは、クラスタリング用のパッケージとしてClustering.jlが提供されている1。実装されているアルゴリズムは次の通りです:\nK-means K-medoids Affinity Propagation Density-based spatial clustering of applications with noise (DBSCAN) Markov Clustering Algorithm (MCL) Fuzzy C-Means Clustering 階層クラスタリング Single Linkage Average Linkage Complete Linkage Ward\u0026rsquo;s Linkage コード DBSCAN DBSCAN (Density-based spatial clustering of applications with noise)はdbscan()関数で実装されています。$p$次元のデータが$n$個ある場合、$p \\times n$サイズの行列と半径Radiusが引数として与えられなければなりません。\njulia\u0026gt; points = [iris.PetalLength iris.PetalWidth]\u0026#39;\r2×150 adjoint(::Matrix{Float64}) with eltype Float64:\r1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 … 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.0 5.2 5.4 5.1 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 0.2 2.1 2.4 2.3 1.9 2.3 2.5 2.3 1.9 2.0 2.3 1.8 julia\u0026gt; dbscaned = dbscan(points, 0.5)\rDbscanResult(DbscanCluster[DbscanCluster(50, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], Int64[]), DbscanCluster(100, [51, 52, 53, 54, 55, 56, 57, 58, 59, 60 … 141, 142, 143, 144, 145, 146, 147, 148, 149, 150], Int64[])], [1, 51], [50, 100], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1 … 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\rjulia\u0026gt; dbscaned |\u0026gt; propertynames\r(:clusters, :seeds, :counts, :assignments) DBSCANの結果はDbscanResultという構造体で返されます。.assignmentsと.clusterが重要です。\n各クラスタにどのデータポイントが属しているかは、次のようにgetproperty()関数を通じて得ることができます。\njulia\u0026gt; getproperty.(dbscaned.clusters, :core_indices)\r2-element Vector{Vector{Int64}}:\r[1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\r[51, 52, 53, 54, 55, 56, 57, 58, 59, 60 … 141, 142, 143, 144, 145, 146, 147, 148, 149, 150] 各データポイントがどのクラスタに属しているかは、次のように.assignmentsプロパティを通じて知ることができます。\njulia\u0026gt; dbscaned.assignments\r150-element Vector{Int64}:\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r⋮\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2 視覚化のコツとして、クラスタは任意の整数に割り当てられるので、散布図を描く際に*.assignmentsをそのままcolorオプションに入れると、次のように各クラスタに対応する色が指定されます。\nscatter(iris.PetalLength, iris.PetalWidth,\rxlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;,\rcolor = dbscaned.assignments) クラスタリングがうまく実行されたことが確認できます。\n全コード using Clustering\rusing RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rscatter(iris.PetalLength, iris.PetalWidth, xlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;)\rpng(\u0026#34;iris\u0026#34;)\rpoints = [iris.PetalLength iris.PetalWidth]\u0026#39;\rdbscaned = dbscan(points, 0.5)\rdbscaned |\u0026gt; propertynames\rgetproperty.(dbscaned.clusters, :core_indices)\rdbscaned.assignments\rscatter(iris.PetalLength, iris.PetalWidth,\rxlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;,\rcolor = dbscaned.assignments) 環境 OS: Windows julia: v1.9.0 Clustering v0.15.4 https://github.com/JuliaStats/Clustering.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2613,"permalink":"https://freshrimpsushi.github.io/jp/posts/2613/","tags":null,"title":"ジュリアでクラスタリングパッケージを使用する方法"},{"categories":"줄리아","contents":"概要 ジュリアでは、マシンラーニング、特にディープラーニングに関連した自動微分Automatic DifferentiationのためにZygote.jlというパッケージを使っている1。開発者たちは、このパッケージは次世代の自動微分システムとして、ジュリアで微分可能プログラミングDifferentiable Programmingができるようにすると宣伝していて、実際に使ってみると驚くほど直感的だと分かる。\n自動微分ではなく、導関数に関連したパッケージ自体が気になるなら、Calculus.jl パッケージを参照してほしい。\nコード 単変数関数 信じられないくらい簡単だ。普段私たちが微分するのと同じように、関数名の後ろにプライム'をつけると、まるで本当に導関数を使って計算しているように、微分係数が計算される。\njulia\u0026gt; using Zygote\rjulia\u0026gt; p(x) = 2x^2 + 3x + 1\rp (generic function with 1 method)\rjulia\u0026gt; p(2)\r15\rjulia\u0026gt; p\u0026#39;(2)\r11.0\rjulia\u0026gt; p\u0026#39;\u0026#39;(2)\r4.0 多変数関数 gradient() 関数を使う。\njulia\u0026gt; g(x,y) = 3x^2 + 2y + x*y\rg (generic function with 1 method)\rjulia\u0026gt; gradient(g, 2,-1)\r(11.0, 4.0) もう少し直感的にコードを書きたいなら、次のように\\nabla、すなわち∇で再び関数を定義して試してみるのも良い。\njulia\u0026gt; ∇(f, v...) = gradient(f, v...)\r∇ (generic function with 1 method)\rjulia\u0026gt; ∇(g, 2, -1)\r(11.0, 4.0) 全体のコード using Zygote\rp(x) = 2x^2 + 3x + 1\rp(2)\rp\u0026#39;(2)\rp\u0026#39;\u0026#39;(2)\rg(x,y) = 3x^2 + 2y + x*y\rgradient(g, 2,-1)\r∇(f, v...) = gradient(f, v...)\r∇(g, 2, -1) 環境 OS: Windows julia: v1.9.0 Zygote: v0.6.62 https://github.com/FluxML/Zygote.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2609,"permalink":"https://freshrimpsushi.github.io/jp/posts/2609/","tags":null,"title":"ジュリアの自動微分パッケージZygote.jl"},{"categories":"줄리아","contents":"概要 Juliaで構造体のプロパティを参照する方法は主に二つある。文法的な便宜または実際の用途に応じて適切に使用するべきだ。\nコード 例として、Juliaで//演算子は以下のように有理数(Rational)タイプの数を作る。有理数が持つプロパティの名前には、分子Numeratorを意味する:numと分母Denominatorを意味する:denがある。\njulia\u0026gt; q = 7 // 12\r7//12\rjulia\u0026gt; q |\u0026gt; typeof\rRational{Int64}\rjulia\u0026gt; q |\u0026gt; propertynames\r(:num, :den) getproperty(x, :y)とx.y julia\u0026gt; getproperty(q, :den)\r12\rjulia\u0026gt; q.den\r12 基本的には、getproperty()関数の二番目の引数にそのプロパティの名前をシンボルで与えればいい。あるいは、普通のプログラミング言語でのフィールド、プロパティへの参照のように、オブジェクト変数名の後ろに点を付けてそのプロパティにアクセスできる。\n配列に対するプロパティの参照 一方、上記の方法は一回だけ必要な時に使用できる方法だが、配列にある複数の要素にアクセスする必要があれば、以下のようにブロードキャストを使用しなければならない。あるいは、性能が重要でなくてただ早くコーディングが必要なら、Pythonみたいにリストコンプリヘンションを使用することも一つの方法だ。\njulia\u0026gt; Q = [k // 12 for k in 1:12]\r12-element Vector{Rational{Int64}}:\r1//12\r1//6\r1//4\r1//3\r5//12\r1//2\r7//12\r2//3\r3//4\r5//6\r11//12\r1//1\rjulia\u0026gt; getproperty.(Q, :num)\r12-element Vector{Int64}:\r1\r1\r1\r1\r5\r1\r7\r2\r3\r5\r11\r1\rjulia\u0026gt; [q.num for q in Q]\r12-element Vector{Int64}:\r1\r1\r1\r1\r5\r1\r7\r2\r3\r5\r11\r1 全コード q = 7 // 12 q |\u0026gt; typeof q |\u0026gt; propertynames getproperty(q, :den) q.den Q = [k // 12 for k in 1:12] getproperty.(Q, :num) [q.num for q in Q] 環境 OS: Windows julia: v1.9.0 ","id":2607,"permalink":"https://freshrimpsushi.github.io/jp/posts/2607/","tags":null,"title":"ジュリアで関数として構造体のプロパティを参照する方法"},{"categories":"줄리아","contents":"コード quiver(, quiver=) Juliaでは、quiver()関数を使ってベクトルフィールドを視覚化することができる。\nθ = 0:0.2:2π\rquiver(cos.(θ),sin.(θ), quiver = (-sin.(θ), cos.(θ)), size = (600,600), lims = (-2,2)); png(\u0026#34;1\u0026#34;) 矢印の長さの変更 矢印の大きさを変えるもっといい方法があるかもしれないけど、基本的にはquiver=オプションで提供されているベクトルの長さを伸ばしたり縮めたりして、もっといい図を描くことができる。\nquiver(cos.(θ),sin.(θ), quiver = (-sin.(θ), cos.(θ)) ./ 2, size = (600,600), lims = (-2,2)); png(\u0026#34;2\u0026#34;) 環境 OS: Windows julia: v1.9.0 ","id":2605,"permalink":"https://freshrimpsushi.github.io/jp/posts/2605/","tags":null,"title":"ジュリアでベクトル場を描く方法"},{"categories":"줄리아","contents":"概要 複数の配列が与えられた時、例えば、それぞれの配列の3番目の要素にアクセスしたいという状況は意外と多い。Juliaでは、getindex()関数のブロードキャストを通じてこれを実装できる。\nコード getindex.() julia\u0026gt; seq_ = [collect(1:k:100) for k in 1:10]\r10-element Vector{Vector{Int64}}:\r[1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\r[1, 3, 5, 7, 9, 11, 13, 15, 17, 19 … 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\r[1, 4, 7, 10, 13, 16, 19, 22, 25, 28 … 73, 76, 79, 82, 85, 88, 91, 94, 97, 100]\r[1, 5, 9, 13, 17, 21, 25, 29, 33, 37 … 61, 65, 69, 73, 77, 81, 85, 89, 93, 97]\r[1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81, 86, 91, 96]\r[1, 7, 13, 19, 25, 31, 37, 43, 49, 55, 61, 67, 73, 79, 85, 91, 97]\r[1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99]\r[1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97]\r[1, 10, 19, 28, 37, 46, 55, 64, 73, 82, 91, 100]\r[1, 11, 21, 31, 41, 51, 61, 71, 81, 91]\rjulia\u0026gt; getindex.(seq_, 3)\r10-element Vector{Int64}:\r3\r5\r7\r9\r11\r13\r15\r17\r19\r21 first(), last() first()はgetindex(, 1)と同じだが、last()はgetindex(, end)と同じ表現が存在しないため、特別な関数と言える。プログラムが繰り返される中で最後にある結果が必要な場合が多く、その最後の要素のインデックスは様々な場合が多いため、last()関数は知っておくべきだ。\njulia\u0026gt; first.(seq_)\r10-element Vector{Int64}:\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\rjulia\u0026gt; last.(seq_)\r10-element Vector{Int64}:\r100\r99\r100\r97\r96\r97\r99\r97\r100\r91 環境 OS: Windows julia: v1.9.0 ","id":2603,"permalink":"https://freshrimpsushi.github.io/jp/posts/2603/","tags":null,"title":"ジュリアで配列の特定の位置を関数で参照する方法"},{"categories":"줄리아","contents":"概要 ジュリアでパッケージを読み込む方法はusingを使うことだけど、プログラムが大きくなるとそれを一つ一つ書くのも大変だ。ループを通してパッケージを読み込む方法を紹介する1。\nコード メタプログラミング packages = [:CSV, :DataFrames, :LinearAlgebra, :Plots]\rfor package in packages\r@eval using ▷eq1◁(package)\rend 実際の使い方としては、プログレスバーだけ別に読み込んで、その他のパッケージの読み込みは目で確認する方がいい。\n環境 OS: Windows julia: v1.9.0 https://discourse.julialang.org/t/programmatically-load-packages/52435/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2601,"permalink":"https://freshrimpsushi.github.io/jp/posts/2601/","tags":null,"title":"ジュリアからRへのパッケージのインポート方法"},{"categories":"줄리아","contents":"概要 Juliaで行列の正規化を簡単にするヒントを紹介する1。基本的には行列を行ごと、列ごとにスカラー倍する方法とeachcol()関数、LinearAlgebraモジュールのnorm()関数を混ぜて使っただけだが、一行で終わり、使う機会が多いので覚えておくと役に立つ。\nコード julia\u0026gt; using LinearAlgebra\rjulia\u0026gt; X = reshape(1:15, 5, :)\r5×3 reshape(::UnitRange{Int64}, 5, 3) with eltype Int64:\r1 6 11\r2 7 12\r3 8 13\r4 9 14\r5 10 15 与えられた行列Xを列ごとに正規化Normalizeするのは、X ./ norm.(eachcol(X))'の一行で可能だ。実行自体と実際にうまく正規化されたかを確認した結果は以下の通りだ。\njulia\u0026gt; Z = X ./ norm.(eachcol(X))\u0026#39;\r5×3 Matrix{Float64}:\r0.13484 0.330289 0.376192\r0.26968 0.385337 0.410391\r0.40452 0.440386 0.444591\r0.53936 0.495434 0.47879\r0.6742 0.550482 0.512989\rjulia\u0026gt; norm.(eachcol(Z))\r3-element Vector{Float64}:\r1.0\r1.0\r1.0 全コード using LinearAlgebra\rX = reshape(1:15, 5, :)\rZ = X ./ norm.(eachcol(X))\u0026#39;\rnorm.(eachcol(Z)) 環境 OS: Windows julia: v1.9.0 https://stackoverflow.com/a/72627341/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2599,"permalink":"https://freshrimpsushi.github.io/jp/posts/2599/","tags":null,"title":"ジュリアで列ごとに行列を正規化する方法"},{"categories":"행렬대수","contents":"定義 1 2 置換行列 $P^{T}$ と 可逆行列 $A \\in \\mathbb{R}^{n \\times n}$ に対し、その 行列の積 $P^{T} A$ は $LU$ を与える。この分解を $A$ の PLU分解Permutation LU Decomposition と言う。$P$ は置換行列であるため、直交行列 となり、すなわち $P^{-1} = P^{T}$ であり、次のように表すことができる。 $$ P^{T} A = LU \\iff A = PLU $$\n説明 LU分解のアルゴリズム：$(a_{ij}) \\in \\mathbb{R}^{n \\times n}$ を可逆行列とする。\nStep 1. $k = 1$\n$u_{1j} = a_{1j}$ とし、$\\displaystyle l_{i1} = {{1} \\over {u_{11}}} a_{i1}$ を計算する。\nStep 2. $k = 2, 3, \\cdots , n-1$\nStep 2-1. 以下を計算する。 $$ u_{kk} = a_{kk} - \\sum_{s = 1}^{k-1} l_{ks} u_{sk} $$ Step 2-2. $j = k+1 , k+2 , \\cdots , n-1$ に対し以下を計算する。 $$ u_{kj} = a_{kj} - \\sum_{s = 1}^{k-1} l_{ks} u_{sj} $$ Step 2-3. $i = k+1 , k+2 , \\cdots , n-1$ に対し以下を計算する。 $$ l_{ik} = {{1} \\over {u_{kk}}} \\left{ a_{ik} - \\sum_{s = 1}^{k-1} l_{is} u_{sk} \\right} $$ Step 3. $k = n$ に対し以下を計算する。 $$ u_{nn} = a_{nn} - \\sum_{s = 1}^{n-1} l_{ns} u_{sn} $$\n行列のLU分解を行うには $u_{11} = a_{11}$ や $u_{kk}$ の逆数をとることが可能でなければならないが、 $$ A = \\begin{bmatrix} 0 \u0026amp; 3\\\\ 2 \u0026amp; 1 \\end{bmatrix} $$ のような 行列でもこのアルゴリズムを適用することはできない。LU分解を可能にするためにある置換行列 $P^{T}$ を乗じて $A$ を $PLU$ として表すことを PLU分解 と呼ぶ。もちろん、左または右、行または列が重要というわけではないため、 $$ A P^{T} = LU \\iff A = LUP $$ と書き LUP分解 と呼んでも差し支えない。\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://math.unm.edu/~loring/links/linear_s08/LU\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2,"permalink":"https://freshrimpsushi.github.io/jp/posts/2/","tags":null,"title":"PLU分解"},{"categories":"행렬대수","contents":"定義 1 各行で成分が一つだけ$1$で、残りがすべて$0$である正方行列$P \\in \\mathbb{R}^{n \\times n}$を順列行列と呼ぶ。\n基本的性質 直交性 すべての順列行列は直交行列である: $$P^{-1} = P^{T}$$\nスパース性 十分に大きな$n$に対して、$P \\in \\mathbb{R}^{n \\times n}$はスパース行列となる。\n説明 順列行列はその名前が示す通り、行列の乗算によって行と列の順列を与える。次の例では、左側に乗算すると行の順列となり、右側に乗算すると列の順列となることがわかる。 $$ \\begin{align*} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\\\ \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{12} \u0026amp; a_{11} \u0026amp; a_{13} \\\\ a_{22} \u0026amp; a_{21} \u0026amp; a_{23} \\\\ a_{32} \u0026amp; a_{31} \u0026amp; a_{33} \\end{bmatrix} \\end{align*} $$\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1,"permalink":"https://freshrimpsushi.github.io/jp/posts/1/","tags":null,"title":"順列行列"},{"categories":"줄리아","contents":"エラー Juliaでデータフレームを使っていると、文字列データがString7やString15、String31などと読み込まれて様々なエラーを発生させることがある。具体的にどんなエラーが発生するというより、普段からよく使っている関数がここでは通用しなくて色々な問題を起こす。\n原因 パフォーマンス上の理由で、Stringをより早く処理できるString7などに変更したのだ。意図通りに設計されているので、仕方がない。\n解決法 CSV.read()にオプションとしてstringtype = Stringを渡せばいい。\n実際に使ってみるとかなり不便だけど、我慢して使うしかない。\n環境 OS: Windows julia: v1.8.5 ","id":2574,"permalink":"https://freshrimpsushi.github.io/jp/posts/2574/","tags":null,"title":"ジュリアでString7, String15なしでデータフレームを呼び出す方法"},{"categories":"다변수벡터해석","contents":"質問 偏微分では、通常の微分と異なり、$\\displaystyle {{ d f } \\over { d t }}$ の代わりに $\\displaystyle {{ \\partial f } \\over { \\partial t }}$ のような表現を使用します。$\\partial$ は[ラウンドディー]Round Deeまたは[パーシャル]Partialと読み、歴史的にも$d$ を丸めて書いた[カーリーディー]Curly Deeから由来しています。1 $\\TeX$ のコードでは \\partial であり、韓国では[ラウンドディー]さえも長いと考えるのか、単に[ラウンド]と読む人も多いです。\nなぜ $d$ を $\\partial$ で書くのか？ 問題は、偏微分が単に他の変数に関して微分するだけなのに、なぜ記号を異なるものにする必要があるのかということが納得できないということです。学部の授業レベルでは、偏微分が初めて登場するたびに必ず出てくる質問ですが、実際の答えは、数学科でなければ「そんなことは数学科で考えることだ」または数学科であっても「ただの表記の違いとして受け入れても問題ない」という程度で返ってくることがあります。これが決して間違っているわけではないのは、$d$ で書こうが $\\partial$ で書こうが、数学科でなければそれが特に重要なわけではなく、数学科であっても式の意味自体が変わるわけではないからです。 例えば、熱方程式を学ぶ場合、 $$ {{ \\partial u } \\over { \\partial t }} = {{ \\partial u } \\over { \\partial x^{2} }} $$ の $\\partial$ を通常の微分表記 $d$ に変えて $$ {{ d u } \\over { d t }} = {{ d u } \\over { d x^{2} }} $$ と書いた場合、2つの方程式が同じかどうかを尋ねることができます。非常に混乱することに、その答えは「実際には同じ」なので、この時点で多くの学生が $d$ と $\\partial$ の区別に意味がないと感じたり、定義レベルで受け入れてしまったりすることになります。\n回答 ニュートンとライプニッツ 本格的な偏微分の話に入る前に、微分の2人の父、ニュートンNewtonとライプニッツLeibnizの話を面白い読み物として取り上げたいと思います。現代において、両者は独自に微分の概念および記法を考案したと認められていますが、関数$y = f(x)$ の導関数を表す際、ニュートンは $$ y ' = f ' (x) $$ のような表記を使用し、ライプニッツは $$ {{ dy } \\over { dx }} = {{ d f(x) } \\over { dx }} $$ のような表記を使用しました。同じ微分であっても、このように表現の違いが生じるのは、両者の思考方法や微積分に対する見方自体が異なっていたためです。現在では、同時代に独自に微分を考案した人がもう一人いても良かったと思えるほど、幸運なことです。ニュートンは古典力学の巨匠として、「位置を一度微分すると速度、二度微分すると加速度」といった話をしなければならず、この時 $$ \\begin{align*} v =\u0026amp; x ' \\\\ a =\u0026amp; v ' = x '' \\end{align*} $$ のような表現は非常にすっきりして効率的です。ライプニッツは幾何学的Geometricな観点から見るとより理にかなっており、直線の傾きが横と縦の変化量の比として定義されるため、曲線では非常に小さな単位を与えて $$ {{ \\Delta y } \\over { \\Delta x }} \\approx {{ d y } \\over { d x }} $$ のように接線の傾きに自然に近づくことができます。興味深いことに、ここまで述べたのは全て通常の微分に関するものであり、分野によっては以下のような表記の分化が起こり、ニュートンとライプニッツの表記が共存することができるという事実です。\n微分幾何学における$s$ に対する微分と$t$ に対する微分の表記： $$ {{ df } \\over { ds }} = f^{\\prime} \\quad \\text{and} \\quad {{ df } \\over { dt }} = \\dot{f} $$ ドット$\\dot{}$ やプライム$'$ はどちらも微分を表していますが、微分幾何学の文脈では上記のように記号を区別することができます。通常、$s$ は単位速度曲線のパラメータであり、$t = t(s)$ は曲線の長さの再パラメータ化によって表されます。\nこの表記は、微分という概念が変形されて出てきたわけではありません。微分幾何学では、単に$s$ で多くの微分を行い、$t$ でも多くの微分を行う必要があるため、ニュートンの表記では何に対して微分しているのか区別できず、ライプニッツの表記では数式が複雑すぎるため、両者の長所を取り入れるために新たな表記を作成したものです。\n本当に興味深いのは、このように幾何学的な観点から$s$ や $t$ は単なるパラメータに過ぎないにもかかわらず、常微分方程式の中でも特に時間timeによる変化を表す場合には、その頭文字を取って$t$ に対する$v$ の導関数を$v '$ ではなく$\\dot{v}$ と書くようになりました。これにより、ダイナミクスなどのほとんどのシステムで時間による変化を記述する際には、$v '$ の代わりに $$ \\dot{v} = f(v) $$ という表現を好んで使用するようになりました。ポイントは、「何によって微分するか」を明確かつすっきりと表現するための検討自体が、偏微分という枠組みに縛られなくても自然に浮かぶことができるということです。\n多変数関数の暗示 前節では、$f '$ と $\\dot{f}$ が単に表現の違いだけで、どの変数によって微分されたかを区別できること、特にダイナミクスシステムでは、時間$\\dot{v} = f(v)$ が現れなくても、一般的な規約とコンテキストからそれが時間による微分であることを暗示できることを指摘しました。このように表現によって暗黙的Implicitにわかる情報についてもう少し話してみたいと思います。\n再び偏微分に戻ると、$d$ と $\\partial$ の表記がどのように異なるかを実感するのが難しいのは、その式自体が示す偏導関数に違いがないからです。例えば、$f$ を$t$ で微分した導関数が$g$ である場合、その$g$ は $$ g = {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} $$ のように$d$ で表されても$\\partial$ で表されてもあまり関係がない。記号がどうであれ、$t$ で微分された「結果」である$g$ が同じだからです。しかし、$\\partial$ が暗黙的に与える情報は$g$ ではなく$f$ に関するものです。ある関数$h$ が$H$ に関して$x$ で微分された結果だとすると、次のように2つの表現を比較してみましょう：\n偏微分表現を使用しない場合：$\\displaystyle h = {{ d H } \\over { d x }} \\implies$ $H$ を微分すると$h$ になるらしい。\n偏微分表現を使用する場合：$\\displaystyle h = {{ \\partial H } \\over { \\partial x }} \\implies$ なぜこれだけ？何か$y$ があって$H = H (x , y)$ になるんだろう？\rつまり、$\\partial$ という記号は、与えられた関数が多変数関数であることを暗示しているのです。多くの場合、偏微分に初めて本格的に触れるのは通常偏微分方程式であり、 $$ {{ \\partial u } \\over { \\partial t }} = {{ \\partial u } \\over { \\partial x^{2} }} $$ のような方程式があれば、私たちは$u$ を$t$ で微分した偏導関数$u_{t}$ が気になるわけではなく、$u$ を$x$ で2回微分した2階偏導関数$u_{xx}$ が気になるわけでもなく、その両方が等しいときの$t$ と$x$ の関数$u = u (t,x)$ が何であるかが気になるのです。この観点から、偏微分に使用される$\\partial$ が偏微分方程式の記述に使用されるのは妥当で自然だと主張することができます。\n一方で、このような慣習が広く受け入れられることにより、$d$ 自体の意味も変わります。多変数関数ではない関数をわざわざ$\\partial$ で微分することは意味がないため、導関数の表現に$d$ が使用されていれば、それは多変数関数ではないことを暗示することになります。例えば、2変数関数$u = u (t,x)$ に対して位置を一点に固定して$u = u \\left( t , x_{0} \\right)$ とすると、 $$ \\left. {{ \\partial u } \\over { \\partial t }} \\right|_{x = x_{0} } = {{ d u } \\over { d t }} = \\dot{u} $$ のような式は、$\\partial$ と$d$ の暗黙的な情報伝達を非常にうまく活用しています。これは、単なる表現の違いにとどまらず、実際に式を扱う思考方法にも影響を与え、偏微分方程式の問題を比較的簡単な常微分方程式に変換して解くといったアイデアにつながることもあります。\n✅ 全微分における混乱を避けるために $$ df = \\frac{ \\partial f}{ \\partial x_{1} }dx_{1} + \\frac{ \\partial f}{ \\partial x_{2} }dx_{2} + \\cdots + \\frac{ \\partial f}{ \\partial x_{n} }dx_{n} $$ 多変数関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}$ に対する数理物理学などで使用される全微分は、通常上記のような形で表され、もう少し直感的に書くために$n = 3$ のとき次のように$t,x,y,z$ のみを書き、$x,y,z$ は互いに独立であるとしましょう。 $$ df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz $$ 一見すると、$d$ と$\\partial$ が混在していて複雑に見えますが、ライプニッツの遺産に従って「両辺を$dt$ や$dx$ で割る」ような操作を行うと、 $$ \\begin{align*} df =\u0026amp; {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz \\\\ {{ d f } \\over { d t }} =\u0026amp; {{ \\partial f } \\over { \\partial x }} {{ d x } \\over { d t }} + {{ \\partial f } \\over { \\partial y }} {{ d y } \\over { d t }} + {{ \\partial f } \\over { \\partial z }} {{ d z } \\over { d t }} \\\\ {{ d f } \\over { d x }} =\u0026amp; {{ \\partial f } \\over { \\partial x }} {{ d x } \\over { d x }} + {{ \\partial f } \\over { \\partial y }} {{ d y } \\over { d x }} + {{ \\partial f } \\over { \\partial z }} {{ d z } \\over { d x }} = {{ \\partial f } \\over { \\partial x }} \\end{align*} $$ のように$f$ を$t$ で微分する意味と$x$ で偏微分する意味が同時によく表現されていることがわかります。これは全微分の形が数式的に扱う上で非常に便利であることを示していますが、全微分で$\\partial$ をすべて取り除いて$d$ で統一して再度書き直すと次のようになります。 $$ df = {{ d f } \\over { d x }} dx + {{ d f } \\over { d y }} dy + {{ d f } \\over { d z }} dz $$ もちろん、ライプニッツの微分記法が分数の分子と分母を扱うときのように非常に直感的であることは事実ですが、この記事を読んでいる皆さんであれば、$dx$ や$dy$、$dz$ を本当にそのように扱ってはいけないことを知っているでしょう。それにもかかわらず、皆さんの内なる本能はこのように約分するように叫ぶでしょう。 $$ \\begin{align*} df =\u0026amp; {{ d f } \\over { dx }} dx + {{ d f } \\over { dy }} dy + {{ d f } \\over { dz }} dz \\\\ \\overset{?}{=} \u0026amp; {{ d f } \\over { \\cancel{dx} }} \\cancel{dx} + {{ d f } \\over { \\cancel{dy} }} \\cancel{dy} + {{ d f } \\over { \\cancel{dz} }} \\cancel{dz} \\\\ =\u0026amp; df + df + df \\\\ \\overset{???}{=}\u0026amp; 3 df \\end{align*} $$ このような惨事は、$d$ が$\\partial$ と同じになる条件を見落としたために起こった循環論法と見ることができます。\u0026rsquo;$\\partial$ をすべて取り除いて$d$ で統一して再度書き直す\u0026rsquo;という展開を無造作に行うことがあまりにも大胆であるため、何らかの方法で$\\partial$ を$d$ で置き換えてもよいと考えること自体が、$x,y,z$ が独立である場合 $$ df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz \\implies {{ d f } \\over { d x }} = {{ \\partial f } \\over { \\partial x }} \\implies d \\equiv \\partial $$ から出てきたものです。その一方で、$d \\equiv \\partial$ の根拠となる$df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz$ を無闇にいじると、どのような方法でも必ず問題が発生します。$d$ と$\\partial$ が等しくなるには、例で仮定したように多変数関数の変数が互いに独立である場合や、何らかの特別な条件の下での何らかの驚くべき定理を通じて、$d$ と$\\partial$ が本当に同じである必要があります。\nこれまでの考察から、偏微分で$d$ の代わりに$\\partial$ を使用する理由は、実際にそれらが異なるためであると要約することができます。これまで見てきた、$d$ と$\\partial$ が同じだったすべての例は、必ずそのための仮定を暗黙的に含んでいます。その良い仮定の中で、$\\partial$ が実質的に$d$ と同じになるかもしれませんが、だからといってわざわざ$\\partial$ を$d$ で書き直す必要もないのです。\n❌ 微分する変数以外は定数とみなすために？ 結論から言うと、間違った答えです。\nもっと正確に言うと、現象を説明する因果関係が逆転しています。例えば、$f(t,x) = \\left( t^{2} + x^{2} \\right)$ であれば、形式的にFormally$\\partial t$ 以外の変数を定数として $$ {{ \\partial f } \\over { \\partial t }} = 2t + 0 = 2t = {{ d f } \\over { d t }} $$ ではないのは、前節で見たように、$t$ と$t$ が独立であるという仮定$x$ の下で $$ \\begin{align*} \u0026amp; df = {{ \\partial f } \\over { \\partial t }} dt + {{ \\partial f } \\over { \\partial x }} dx \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} {{ dt } \\over { dt }} + {{ \\partial f } \\over { \\partial x }} {{ dx } \\over { dt }} \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} \\cdot 1 + {{ \\partial f } \\over { \\partial x }} \\cdot 0 \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} \\end{align*} $$ が成立するからです。偏微分$\\displaystyle {{ dx } \\over { dt }} = 0$ 自体が$\\partial$ という結果をもたらしたのではなく、$\\displaystyle {{ dx } \\over { dt }} = 0$ という原因が$\\displaystyle {{ dx } \\over { dt }} = 0$ という結果をもたらしたのです。このように「偏微分は微分する変数以外を定数として扱う」という説明は、まるで通常の微分$\\partial \\equiv d$ と異なり、偏微分$d$ がより強力なオペレーターであるかのような印象と誤解を与えます。また、$\\partial$ を定数として扱った場合、$x$ で微分した後には消えるはずですが、単純に$t$ のような例を考えると、$f(t,x) = t^{2} + x^{2} + 2tx$ は依然として変数が$\\dfrac{\\partial f}{\\partial t}$ である2変数関数です。\nこのような誤解がなくならない理由は、これがかなりもっともらしいからです。実際には、変数間に$(t,x)$ のような関係があると仮定する場合、そもそも$x = x(t)$ で偏微分するという表現自体を使用する必要がありません。チェーンルールに従えば、 $$ \\begin{align*} {{ d f } \\over { d t }} =\u0026amp; {{ d } \\over { d t }} \\left( t^{2} + x^{2} \\right) \\\\ =\u0026amp; 2t + {{ d x^{2} } \\over { d x }} {{ dx } \\over { dt }} \\\\ =\u0026amp; 2t + 2x \\dot{x} \\end{align*} $$ のように最初から誤解の余地なく式を展開することができます。少なくともこの例では、$t$ は実質的に$f = f(t,x)$ と同じか、むしろ難しいですし、結局のところ教科書ではこのような無意味なケースをすべて排除して、変数間が独立でありながらも依然として多変数関数である形だけが残ります。通常はきれいな例だけを見ながら学び、時間が経ち、偏微分に慣れ、誤った直感が定着し、他の人もそうです。しかし、違うものは違うものです。単に微分の記号を変えるだけで与えられた関数の従属関係を勝手に変えることはできません。\nhttps://math.stackexchange.com/a/2000353/459895\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2573,"permalink":"https://freshrimpsushi.github.io/jp/posts/2573/","tags":null,"title":"偏微分の記号を使い分ける理由"},{"categories":"줄리아","contents":"概要 ジュリアで図を描く時、titleでタイトルを入れるとサブプロット全てに適用されるので、plot_titleを使うべきだ1。これはプロットがサブプロットを含む場合、つまり\nplot(\rplot1, plot2, ...\r) のような形で構成された時、最も外側にあるplot()関数の引数が内部のサブプロットに継承Inheritanceみたいなことが起こるからだ。これを明確に区分するためにtitleとplot_titleが別々にある。\nコード plot(p1, p2, title = \u0026#34;Two Plots\u0026#34;) 見ての通り、title = \u0026quot;Two Plots\u0026quot;をすると全てのサブプロットにそのタイトルが適用される。\nplot(p1, p2, plot_title = \u0026#34;Two Plots\u0026#34;) plot_title = \u0026quot;Two Plots\u0026quot;では全体の図にただ一つのタイトルが適用されているのが確認できる。\n全体コード using Plots\rp1 = scatter(rand(100))\rp2 = histogram(rand(100))\rplot(p1, p2, title = \u0026#34;Two Plots\u0026#34;)\rplot(p1, p2, plot_title = \u0026#34;Two Plots\u0026#34;) 環境 OS: Windows julia: v1.8.5 https://stackoverflow.com/a/69713616/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2572,"permalink":"https://freshrimpsushi.github.io/jp/posts/2572/","tags":null,"title":"Juliaのサブプロットにメインタイトルを追加する方法"},{"categories":"줄리아","contents":"概要 Juliaで、カラーバー、軸、目盛り、グリッドなどの図のグラフィック要素を消す方法があるけれども、グラフィカルな要素をいじるから数字だけをきれいに消すことができず、formatterというオプションを使わないといけない。\nformatter = (_...) -\u0026gt; \u0026quot;\u0026quot; plot()関数のオプションにformatter = (_...) -\u0026gt; \u0026quot;\u0026quot;を与えるといい。\nusing Plots\rx = rand(10)\ry = rand(10)\rplot(\rplot(x,y)\r,plot(x,y, formatter = (_...) -\u0026gt; \u0026#34;\u0026#34;)\r) 上の画像では、左が普通の画像で、右が値を全部消した画像だ。元々formatterはこう使うだけじゃなくて、もっと多機能を持っている。原理を簡単に説明すると、元の画像に表示されるべきだった値に与えられた関数を適用する方式だ。上の例では、(_...) -\u0026gt; \u0026quot;\u0026quot;というラムダ式を受け取って、どんな数値が入っても空白の文字列を返して、軸の値を消した1。\nxformatter, yformatter 当然、xformatter、yformatterがあり、軸別に指定もできる。x軸だけを消したいなら、yformatterに、y軸だけを消したいなら、xformatterに(_...) -\u0026gt; \u0026quot;\u0026quot;を伝えればいい。\n環境 OS: Windows julia: v1.8.5 foreground_color_text = false plot()関数のキーワードにforeground_color_text = falseを入力すればいい。目盛り値(名前)の色を指定するキーワードだけど、falseを入力すると値が全然表示されなくなる。\nx_foreground_color_textとy_foreground_color_textでも軸別に指定できる。\nusing Plots\rplot(\rplot(rand(10)),\rplot(rand(10), foreground_color_text = false)\r) 環境 OS: Windows11 Version: Julia 1.9.3, Plots v1.39.0 https://stackoverflow.com/questions/74842089/remove-only-axis-values-in-plot-julia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2570,"permalink":"https://freshrimpsushi.github.io/jp/posts/2570/","tags":null,"title":"ジュリアプロットで軸の値を削除する方法"},{"categories":"줄리아","contents":"概要 Juliaで有限差分法を使うには、特に有限差分の係数を求めるためには、FiniteDifferences.jlを使うのがいいだろう1。ノイズに弱い場合は、TVDiffとして知られるTotal Variation Regularized Numerical Differentiationを使ってみる価値がある。これはNoiseRobustDifferentiation.jlに実装されている。\nコード FiniteDifferenceMethod() julia\u0026gt; f′ = FiniteDifferenceMethod([-2, 0, 5], 1)\rFiniteDifferenceMethod:\rorder of method: 3\rorder of derivative: 1\rgrid: [-2, 0, 5]\rcoefficients: [-0.35714285714285715, 0.3, 0.05714285714285714]\rjulia\u0026gt; typeof(f′)\rFiniteDifferences.UnadaptedFiniteDifferenceMethod{3, 1} 上記の例では、関数の中央の点、左から2番目の点、右から5番目の点を使って1次微分を計算して、これら3点の重みを求める。基本的にFiniteDifferenceMethod()を通じて得られるのは、これらの係数である。\njulia\u0026gt; propertynames(f′)\r(:grid, :coefs, :coefs_neighbourhood, :condition, :factor, :max_range, :∇f_magnitude_mult, :f_error_mult)\rjulia\u0026gt; f′.grid\r3-element StaticArraysCore.SVector{3, Int64} with indices SOneTo(3):\r-2\r0\r5\rjulia\u0026gt; f′.coefs\r3-element StaticArraysCore.SVector{3, Float64} with indices SOneTo(3):\r-0.35714285714285715\r0.3\r0.05714285714285714 構造体で使用するプロパティには、主に.gridと.coefsの2つがある。\njulia\u0026gt; f′(sin, π/2)\r-1.2376571459669071e-11\rjulia\u0026gt; f′(cos, π/2)\r-1.0000000000076525 データではなく関数自体が与えられた場合は、上記のように直接関数形式で書いても問題ない。\n_fdm() central_fdm(2, 1)\rcentral_fdm(3, 2)\rbackward_fdm(4, 1)\rforward_fdm(5, 1) 特にカスタマイズせずに一般的に知られているFDMが必要な場合は、上記のようにすでに用意されている関数を使う方が便利だ。第1引数は、関数の種類に応じて何点を使用するかに依存し、第2引数$n$は$n$次微分を計算することを決定する。central_fdm()の第1引数は奇数が与えられた場合、中心にある点の係数が確実に0であることは言うまでもない。\n全コード using FiniteDifferences\rf′ = FiniteDifferenceMethod([-2, 0, 5], 1)\rtypeof(f′)\rpropertynames(f′)\rf′.grid\rf′.coefs\rf′(sin, π/2)\rf′(cos, π/2)\rcentral_fdm(2, 1)\rcentral_fdm(3, 2)\rbackward_fdm(4, 1)\rforward_fdm(5, 1) 環境 OS: Windows julia: v1.8.5 https://github.com/JuliaDiff/FiniteDifferences.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2568,"permalink":"https://freshrimpsushi.github.io/jp/posts/2568/","tags":null,"title":"ジュリアで有限差分を使用する方法"},{"categories":"줄리아","contents":"概要 ジュリアでは、数値解析的な補間のためにInterpolations.jlパッケージを使用する1。ジュリアで変数の値を出力する際に使用する補間法と混同しないように注意しよう。\nコード Interpolate() julia\u0026gt; y = rand(10)\r10-element Vector{Float64}:\r0.8993801321974316\r0.12988982511901515\r0.49781160399025925\r0.22555299914088356\r0.4848674643768577\r0.6089318286915111\r0.10444895196527337\r0.5921775799940143\r0.2149546302906653\r0.32749334953170317\rjulia\u0026gt; f = interpolate(y, BSpline(Linear()));\rjulia\u0026gt; f(1.2)\r0.7454820707817483\rjulia\u0026gt; f(0.1)\rERROR: BoundsError: attempt to access 10-element interpolate(::Vector{Float64}, BSpline(Linear())) with element type Float64 at index [0.1] 基本的には上記のようにデータを渡して、補間関数 f=$f$ 自体をリターンしてもらって使用することができる。例では、10個の点が与えられ、1番目(0.899)と2番目(0.129)の間の1.2あたりにある値(0.745)がどのように補間されるかを示している。具体的にどの方法を使用するかは、公式ドキュメントのAPIセクションを参照しよう2。\nlinear_interpolation() x = 1:10\rf_lin = linear_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_lin.(1:0.01:10), label = \u0026#34;Linear\u0026#34;) cubic_spline_interpolation() f_cub = cubic_spline_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_cub.(1:0.01:10), label = \u0026#34;Cubic\u0026#34;) constant_interpolation() f_con = constant_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_con.(1:0.01:10), label = \u0026#34;Constant\u0026#34;) 全体のコード using Interpolations, Plots\ry = rand(10)\rf = interpolate(y, BSpline(Linear()));\rf(1.2)\rf(0.1)\rx = 1:10\rf_lin = linear_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_lin.(1:0.01:10), label = \u0026#34;Linear\u0026#34;)\rf_cub = cubic_spline_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_cub.(1:0.01:10), label = \u0026#34;Cubic\u0026#34;)\rf_con = constant_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_con.(1:0.01:10), label = \u0026#34;Constant\u0026#34;) 環境 OS: Windows julia: v1.8.5 https://github.com/JuliaMath/Interpolations.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://juliamath.github.io/Interpolations.jl/stable/api/#Public-API\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2566,"permalink":"https://freshrimpsushi.github.io/jp/posts/2566/","tags":null,"title":"ジュリアでの数値解析的補間"},{"categories":"줄리아","contents":"概要 Juliaでは、差分を計算するためにdiff()関数が提供されている1。circshift()関数も使って簡単に書けるけど、端点の処理などがちょっと面倒に感じるから、知ってるとずっと楽になる。RやMatlabのdiff()関数とほぼ同じ方法で使えるが、これらと違い2次差分（差分を二回とること）などは特に実装されていない。\nコード 基本的な使い方 julia\u0026gt; x = rand(0:9, 12)\r12-element Vector{Int64}:\r3\r1\r9\r7\r1\r0\r6\r5\r3\r2\r9\r9 例えば上記のような配列がある場合、ただdiff()を適用して、前の要素と後ろの要素の差を計算した結果を得ることができる。結果から配列のサイズが正確に1だけ縮小されたのを確認できる。\njulia\u0026gt; diff(x)\r11-element Vector{Int64}:\r-2\r8\r-2\r-6\r-1\r6\r-1\r-2\r-1\r7\r0 多次元配列 julia\u0026gt; X = reshape(x, 3, :)\r3×4 Matrix{Int64}:\r3 7 6 2\r1 1 5 9\r9 0 3 9\rjulia\u0026gt; diff(X)\rERROR: UndefKeywordError: keyword argument dims not assigned\rStacktrace:\r[1] diff(a::Matrix{Int64})\r@ Base .\\multidimensional.jl:997\r[2] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\DataDrivenModel\\REPL.jl:7 例えば、上記のような多次元配列がある場合、ただdiff()を適用するとエラーになる。これは、配列を押す方向が与えられていないためであり、次のようにdimsを引数にしてどの次元で差分を計算するかを決める必要がある。1次元の場合と同じように、差分をとった方向で長さが1ずつ短くなることに注意。\njulia\u0026gt; diff(X, dims = 1)\r2×4 Matrix{Int64}:\r-2 -6 -1 7\r8 -1 -2 0\rjulia\u0026gt; diff(X, dims = 2)\r3×3 Matrix{Int64}:\r4 -1 -4\r0 4 4\r-9 3 6 全コード x = rand(0:9, 12)\rdiff(x)\rX = reshape(x, 3, :)\rdiff(X)\rdiff(X, dims = 1)\rdiff(X, dims = 2) 環境 OS: Windows julia: v1.8.5 https://docs.julialang.org/en/v1/base/arrays/#Base.diff\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2564,"permalink":"https://freshrimpsushi.github.io/jp/posts/2564/","tags":null,"title":"ジュリアで配列の差分を計算する方法"},{"categories":"줄리아","contents":"概要 実は、Juliaではネイティブに円形配列Circular Arrayをサポートしていないが、要素を円形にCircularlyシフトしてくれるcircshift()関数を提供していて、事実上それを使うことができる1。自分で書くのはそれほど難しくないが、知っていればわざわざ書かなくても良い。この関数はマットラボのcircshift()とほぼ同じ方法で使える。\nコード この関数は、配列を平行移動する方法のポストでも紹介された。\n基本的な使い方 julia\u0026gt; circshift(1:4, 1)\r4-element Vector{Int64}:\r4\r1\r2\r3\rjulia\u0026gt; circshift(1:4, -1)\r4-element Vector{Int64}:\r2\r3\r4\r1 circshift()は基本的に二番目の引数に整数を入れて、要素をシフトする。上の例では、正の整数で後ろ(下)にシフトし、負の整数で前(上)にシフトすることが確認できる。\n多次元配列 julia\u0026gt; ca = reshape(1:20, 5, :)\r5×4 reshape(::UnitRange{Int64}, 5, 4) with eltype Int64:\r1 6 11 16\r2 7 12 17\r3 8 13 18\r4 9 14 19\r5 10 15 20 上のような多次元配列が与えられた場合、次のように同じ次元のタプルを与えて、各次元をどれだけシフトするかを指定する。\njulia\u0026gt; circshift(ca, (0,1))\r5×4 Matrix{Int64}:\r16 1 6 11\r17 2 7 12\r18 3 8 13\r19 4 9 14\r20 5 10 15\rjulia\u0026gt; circshift(ca, (-1,0))\r5×4 Matrix{Int64}:\r2 7 12 17\r3 8 13 18\r4 9 14 19\r5 10 15 20\r1 6 11 16\rjulia\u0026gt; circshift(ca, (-1,1))\r5×4 Matrix{Int64}:\r17 2 7 12\r18 3 8 13\r19 4 9 14\r20 5 10 15\r16 1 6 11 全体のコード circshift(1:4, 1)\rcircshift(1:4, -1)\rca = reshape(1:20, 5, :)\rcircshift(ca, (0,1))\rcircshift(ca, (-1,0))\rcircshift(ca, (-1,1)) 環境 OS: Windows julia: v1.8.5 https://docs.julialang.org/en/v1/base/arrays/#Base.circshift\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2562,"permalink":"https://freshrimpsushi.github.io/jp/posts/2562/","tags":null,"title":"ジュリアで円形配列を使う方法"},{"categories":"줄리아","contents":"コード 1 長々と説明する必要はなく、文字通りマーカースタイルとラインスタイルが実際にどう見えるかを示す。\nlinesytle [:auto, :solid, :dash, :dot, :dashdot, :dashdotdot] の中から一つ選ぶ。\nshape [:none, :auto, :circle, :rect, :star5, :diamond, :hexagon, :cross, :xcross, :utriangle, :dtriangle, :rtriangle, :ltriangle, :pentagon, :heptagon, :octagon, :star4, :star6, :star7, :star8, :vline, :hline, :+, :x] の中から一つ選ぶ。\n全体のコード using Plots\rlines = [:auto, :solid, :dash, :dot, :dashdot, :dashdotdot]\rplt = plot(grid = :none, showaxis = :false, legend = :outerright)\rfor k in 1:6\rplot!(plt, [0, 1], [-k, -k], lw = 2, color = :black, linestyle = lines[k], label = string(lines[k]))\rend\rplt\rshapes = [:none, :auto, :circle, :rect, :star5, :diamond, :hexagon, :cross, :xcross, :utriangle, :dtriangle, :rtriangle, :ltriangle, :pentagon, :heptagon, :octagon, :star4, :star6, :star7, :star8, :vline, :hline, :+, :x]\rplot(grid = :none, showaxis = :false, xlims = (-0.1,4), size = (600, 600))\rscatter!((0:23) .% 4, -(0:23) .÷ 4, shape = shapes, color = :black, legend = :none, markersize = 10)\rannotate!(.15 .+ ((0:23) .% 4), -(0:23) .÷ 4, text.(shapes, :left)) 環境 OS: Windows julia: v1.8.5 Plots v1.38.5 https://docs.juliaplots.org/latest/generated/attributes_series/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2560,"permalink":"https://freshrimpsushi.github.io/jp/posts/2560/","tags":null,"title":"ジュリアでのマーカーとラインスタイルのリスト"},{"categories":"줄리아","contents":"コード Juliaの散布図に回帰直線を入れる方法は、オプションでsmooth = trueを使うことだ。\nusing Plots\rx = rand(100)\rscatter(x, 2x .+ 0.1randn(100), smooth = true)\rsavefig(\u0026#34;plot.svg\u0026#34;) 環境 OS: Windows julia: v1.8.3 Plots v1.38.5 ","id":2558,"permalink":"https://freshrimpsushi.github.io/jp/posts/2558/","tags":null,"title":"ジュリアプロットで回帰直線を描く方法"},{"categories":"줄리아","contents":"概要 Juliaで頻繁に使われるsplatの...の用途について、オプショナル引数を伝える方法を説明する。基本的に、どんなオプションにどんな引数を入れるかを事前に名前付きタプルの形で決めた後、そのタプルにsplatオペレーターを適用する方式で使う。\nコード 複数の関数に伝える args1 = (; dims = 1)\n上の名前付きタプルargs1はdimsというオプショナル引数がある全ての関数に共通して使うことができる。次の例でsum()とminimum()は全く異なる関数だが、共にdimsを持っているため、関数の種類に関係なく適用された。\njulia\u0026gt; sum(rand(100,100); args1...)\r1×100 Matrix{Float64}:\r47.0704 45.7637 44.4513 48.2325 50.5745 51.9176 … 49.9548 47.6825 50.7284 50.0861 50.0168 50.5116\rjulia\u0026gt; minimum(rand(100,100); args1...)\r1×100 Matrix{Float64}:\r0.00702003 0.0163299 0.00665818 0.0174564 0.00589048 … 0.002967 0.00460205 0.0116248 0.0114521 0.0698425 複数の引数を伝える args2 = (; dims = 2, rev = true)\n上の名前付きタプルargs2はdimsに加えてrevというオプショナル引数を含んでいる。次の例でsort()関数は入力データに関係なく二つのオプションをよく反映して計算結果を返した。\njulia\u0026gt; sort(rand(0:9, 3,3); args2...)\r3×3 Matrix{Int64}:\r9 4 4\r6 5 2\r8 0 0\rjulia\u0026gt; sort(rand(3,3); args2...)\r3×3 Matrix{Float64}:\r0.438682 0.211154 0.108741\r0.72113 0.445214 0.00910109\r0.971441 0.666732 0.0227372 全体コード args1 = (; dims = 1)\rsum(rand(100,100); args1...)\rminimum(rand(100,100); args1...)\rargs2 = (; dims = 2, rev = true)\rsort(rand(0:9, 3,3); args2...)\rsort(rand(3,3); args2...) 環境 OS: Windows julia: v1.8.3 ","id":2554,"permalink":"https://freshrimpsushi.github.io/jp/posts/2554/","tags":null,"title":"Juliaスプラットオペレーターを通じたオプション引数の渡し方のヒント"},{"categories":"줄리아","contents":"概要 Juliaで...はスプラット・オペレーターと呼ばれ、関数を使用したり、配列を定義する際に便利に使われる1。このオペレーターはJuliaに限定されているわけではないが、他の言語に比べて直感的に定義されており、学びやすく覚えやすいところが突出している。個人的な経験だと、...を使うようになるとJuliaプログラミングに関する何かしらの気づきを得る気がする。\nコード 関数入力 基本的に...は配列やジェネレータの後に付けて使い、そのまま前にあるコンテナ/イテレータの要素を全部展開して出す。\njulia\u0026gt; min([1,2,3,4,5,6,7,8,9,10])\rERROR: MethodError: no method matching min(::Vector{Int64})\rjulia\u0026gt; min(1,2,3,4,5,6,7,8,9,10)\r1 例えばJuliaのmin()関数はリデュースとして働くから、そのまま配列を渡すのではなく、複数の数を直接引数として渡さなければならない。もちろん、配列が大きくなるほど手で全部解いて書くのは難しくなり、配列の要素を展開して入れることができるように...を使うことができる。\njulia\u0026gt; min(1:10)\rERROR: MethodError: no method matching min(::UnitRange{Int64})\rjulia\u0026gt; min((1:10)...)\r1 もちろん、実際は配列に使えるminimum()関数があるから、わざわざこうする必要はない。\n配列定義 julia\u0026gt; [(1:10)]\r1-element Vector{UnitRange{Int64}}:\r1:10 上で定義された配列はユニットレンジの配列なので、直接要素にアクセスするのが少し面倒になる。時間があれば、ただ単に1から10までの数字を書き込んだらいいけど、スプラットオペレータを使えば、次のように簡単に定義できる。\njulia\u0026gt; [(1:10)...]\r10-element Vector{Int64}:\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10 もちろん、collect()関数のような代替手段もあるが、知っておくときれいでセンスがある表現だという点が魅力的だ。ただし、速度の面ではあまり推奨されていないので、必要以上に...を使わないようにしよう。\njulia\u0026gt; [eachrow(rand(3,3))...]\r3-element Vector{SubArray{Float64, 1, Matrix{Float64}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}}:\r[0.6695368164913422, 0.69695509795356, 0.12084083239135612]\r[0.6833475867141307, 0.5368141950494666, 0.7877252857572066]\r[0.2810163716135018, 0.04317485597011517, 0.44214186775440534] ...が面白くなるのは、上のようにジェネレータを一度通るときだ。eachrow()は行列を行単位に切ったベクトルのジェネレータをリターンし、スプラットオペレータを通じて、その各ベクトルを配列表記[]に入れて、ベクトルのベクトルを作ったものだ。\n全コード min([1,2,3,4,5,6,7,8,9,10])\rmin(1,2,3,4,5,6,7,8,9,10)\rmin(1:10)\rmin((1:10)...)\r[(1:10)]\r[(1:10)...]\r[eachrow(rand(3,3))...] 環境 OS: Windows julia: v1.8.3 https://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2552,"permalink":"https://freshrimpsushi.github.io/jp/posts/2552/","tags":null,"title":"ジュリアのスプラットオペレータ"},{"categories":"줄리아","contents":"概要 他のプログラム言語がそうであるように、ジュリアでは英語をASCIIコードASCII Codeで書き、漢字、韓国語などをユニコードUnicodeで書く。問題は、他の言語たちと違って、この文字列たちを扱うことがかなり厄介であることだけど、これは性能上の理由から意図されたものであるため1、不便でも我慢して使うしかない。\nコード julia\u0026gt; str1 = \u0026#34;English\u0026#34;\r\u0026#34;English\u0026#34;\rjulia\u0026gt; str2 = \u0026#34;日本語\u0026#34;\r\u0026#34;日本語\u0026#34;\rjulia\u0026gt; str3 = \u0026#34;한국어\u0026#34;\r\u0026#34;한국어\u0026#34; 例えば、上のような文字列たちが与えられているとしよう。\njulia\u0026gt; str1[2:end]\r\u0026#34;nglish\u0026#34; str1の場合は何も特別なことがない英語の文字列で、ASCIIコードであるため、上のように普通の配列にアクセスするようにスライシングが可能である。\njulia\u0026gt; str2[2:end]\rERROR: StringIndexError: invalid index [2], valid nearby indices [1]=\u0026gt;\u0026#39;日\u0026#39;, [4]=\u0026gt;\u0026#39;本\u0026#39;\rStacktrace:\r[1] string_index_err(s::String, i::Int64)\r@ Base .\\strings\\string.jl:12\r[2] getindex(s::String, r::UnitRange{Int64})\r@ Base .\\strings\\string.jl:266\r[3] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\population_dynamics\\REPL.jl:6 しかし、str2は漢字があるためにユニコードで書かれており、上のようにインデックスエラーを発生させる。エラーメッセージを見ると、2番目の文字のインデックスは2ではなく4であることが推測でき、実際に4からインデキシングをすると、意図された通りにスライシングされる。\njulia\u0026gt; str2[4:end]\r\u0026#34;本語\u0026#34; これは以下のように韓国語にも同様に適用される。同じユニコードであるため、違う理由がない。\njulia\u0026gt; str3[4:end]\r\u0026#34;국어\u0026#34;\rjulia\u0026gt; str3[6]\rERROR: StringIndexError: invalid index [6], valid nearby indices [4]=\u0026gt;\u0026#39;국\u0026#39;, [7]=\u0026gt;\u0026#39;어\u0026#39;\rStacktrace:\r[1] string_index_err(s::String, i::Int64)\r@ Base .\\strings\\string.jl:12\r[2] getindex_continued(s::String, i::Int64, u::UInt32)\r@ Base .\\strings\\string.jl:237\r[3] getindex(s::String, i::Int64)\r@ Base .\\strings\\string.jl:230\r[4] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\population_dynamics\\REPL.jl:9\rjulia\u0026gt; str3[7]\r\u0026#39;어\u0026#39;: Unicode U+C5B4 (category Lo: Letter, other) トリック julia\u0026gt; String(collect(str3)[2:3])\r\u0026#34;국어\u0026#34; まあまあ楽に使う方法としては、上のようにcollect()で文字の配列に解いてスライシングし、また文字列にまとめる方法がある。\n環境 OS: Windows julia: v1.8.3 https://discourse.julialang.org/t/weird-string-slicing-in-korean/92252/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2550,"permalink":"https://freshrimpsushi.github.io/jp/posts/2550/","tags":null,"title":"ジュリアでユニコード文字列の一部だけをスライスする方法"},{"categories":"줄리아","contents":"概要 ジュリアのStatsPlotsパッケージでは、図を描く時に@dfマクロを通して、繰り返されるデータフレーム名を省略することができる1。マクロを使う文法は、データフレームXのa列を使う場合、@df X というようにどのデータフレームを使うか指定した後、直ぐに続くスコープでaをシンボルの:aとして引数に渡し、plot (:a)という風に書くことだ。コードで要約すると、@df X plot(:a)と書く。\nコード 以下は、アイリスデータのSepalLengthとSepalWidthで描いた散布図だ。\n次のコードscatter(iris.SepalLength, iris.SepalWidth)と@df iris scatter(:SepalLength, :SepalWidth)は同じである。\nusing RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rusing StatsPlots\rscatter(iris.SepalLength, iris.SepalWidth)\r@df iris scatter(:SepalLength, :SepalWidth) 環境 OS: Windows julia: v1.8.3 StatsPlots v0.15.4 https://github.com/JuliaPlots/StatsPlots.jl#original-author-thomas-breloff-tbreloff-maintained-by-the-juliaplots-members\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2548,"permalink":"https://freshrimpsushi.github.io/jp/posts/2548/","tags":null,"title":"Julia StatsPlotsでデータフレーム名を省略するマクロ@df"},{"categories":"줄리아","contents":"概要 他のファイルにある関数を使えるようにするために、ジュリアコード自体を実行するinclude()関数を紹介する。マットラボでは、同じディレクトリ内にあれば自動的に関数を見つけてくれるため、このプロセスを難しく考える人もいる。ちなみに、ちゃんとモジュール化してエクスポートする方法があるが1、難しくて複雑なので、機能が急に必要な初心者にはお勧めしない。パッケージを自分で作るか、プログラムの規模が扱えないほど大きくなった後にモジュール化を学んでも遅くはない。\nガイド 上記のように、foo/bar.jlファイルにあるbaz()関数をmain.jlから実行したいとしよう。スクリーンショットで確認できるように、モジュールのようなものを別に使用せず、ただ普通にジュリアコードを書けばいい。\nそして、include()でパスを指定して実行した結果は、次のようになる。\ninclude()の実行結果で23が表示された理由は、bar.jlファイルの最下部にy = 23という値の割り当てがあったからだ。見ての通り、関数だけでなく変数も移せるし、ファイル自体を実行する方式なので、データのロードやログ出力も全部できる。\n環境 OS: Windows julia: v1.8.3 https://docs.julialang.org/en/v1/manual/modules/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2544,"permalink":"https://freshrimpsushi.github.io/jp/posts/2544/","tags":null,"title":"ジュリアで他のファイルに定義された関数の使用方法"},{"categories":"줄리아","contents":"コード using Plots\rx, y = rand(100), rand(100) 上記のようなデータが与えられたとしよう。データが連続かカテゴリカルかによって、図の形や描く方法が異なる。\n連続型 scatter(marker_z=) z = x + y\rscatter(x, y, marker_z = z) カテゴリカル scatter(group=) 1 team = rand(\u0026#39;A\u0026#39;:\u0026#39;C\u0026#39;, 100)\rscatter(x, y, group = team) 環境 OS: Windows julia: v1.8.3 https://stackoverflow.com/a/60846501/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2537,"permalink":"https://freshrimpsushi.github.io/jp/posts/2537/","tags":null,"title":"ジュリア集合でマーカーに色をつける方法"},{"categories":"통계적분석","contents":"モデル オーディナリークリギング 空間データ分析で、ランダムフィールド $\\mathbf{Y} = \\left( Y \\left( s_{1} \\right) , \\cdots , Y \\left( s_{n} \\right) \\right)$ の平均 $\\mu \\in \\mathbb{R}$ と共分散行列 $\\Sigma \\in \\mathbb{R}^{n \\times n}$ が多変量正規分布に従う $\\varepsilon \\sim N_{n} \\left( \\mathbf{0} , \\Sigma \\right)$ ことについて、次のモデル $$ \\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon $$ を通じて、新しいサイトSite $s_{0}$ の $Y \\left( s_{0} \\right)$ を推定した値をオーディナリークリギング推定値Ordinary Kriging Estimateと呼ぶ。このようにモデルを立てて推定する行為自体をクリギングとも呼ぶ。\n$\\mathbf{1} = (1 , \\cdots , 1)$ は全ての成分が $1$ の1ベクトルだ。 $N_{n} \\left( \\mathbf{0} , \\Sigma \\right)$ は多変量正規分布を意味する。 説明 語源 クリギングKrigingは、ダニエル・G・クリージDaine G. Krigeという巨匠の名前がそのまま動詞化されたものである。通常の統計学で予測、予想、適合、推定などと使う表現はもちろん、「空白の空間」の値を埋めるという意味で補間技術のように説明する場合もかなりあるが、これら全ての説明を短くしてクリギングするという一般動詞になったと考えられる。\n狭義の応用数学、コンピュータアルゴリズム、機械学習技術などと区別されるクリギングの特徴は、とにかく統計学らしくその平均（点推定量）だけでなくその分散まで考慮することである。想像してみてほしいが、各地点で分散が高い場所のクリギング推定値は同様に分散が大きく、分散が低い場所同士の地点では分散が低いだろう。これはあるデータの観測所の位置を選定するのにも使われるが、例えば微粒子の濃度を測定するとしたら、微粒子をどのように測定するかが気になるのではなく、その測定がどれだけ正確か―つまり、測定値の分散が最も高い場所を選定するようなアプローチがある。\n依存性 $$ \\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon \\qquad , \\text{where } \\varepsilon \\sim N_{n} \\left( 0, \\Sigma \\right) $$ モデルの数式を見ると、回帰分析や時系列分析と異なり $\\varepsilon$ こそが我々の関心事である。$\\Sigma$ が対角行列、つまり観測値ごとの依存性がなければ、そもそも空間的な構造がないという意味であり、わざわざクリギングをする理由がない。実際の分析ではこの $\\Sigma$ はセミバリオグラムのモデルを通じて次のように決定される。 $$ \\Sigma = \\sigma^{2} H \\left( \\phi \\right) + \\tau^{2} I $$ ここで $\\tau^{2}$ はナゲット効果分散（理論と異なり実際のデータで距離に関係なく基本的に見られる共分散性）であり、$I$ は単位行列である。\n一般化 次のように他の独立変数に対して一般化モデルを使用するクリギングをユニバーサルクリギングと呼ぶ。 $$ \\mathbf{Y} = \\mathbf{X} \\beta + \\varepsilon $$\n公式 ランダムフィールド $\\left\\{ Y (s_{k}) \\right\\}_{k=1}^{n}$ が固有的定常空間過程であるとし、新たに予測したい地点を $s_{0}$ とする。バリオグラム $2 \\gamma$ に対して行列 $\\Gamma \\in \\mathbb{R}^{n \\times n}$ を $\\left( \\Gamma \\right)_{ij} := \\gamma \\left( s_{i} - s_{j} \\right)$ のように定義し、ベクトル $\\gamma_{0} \\in \\mathbb{R}^{n}$ を $\\left( \\gamma_{0} \\right)_{i} := \\left( \\gamma \\left( s_{0} - s_{i} \\right) \\right)$ のように定義する。あるベクトル $l = \\left( l_{1} , \\cdots , l_{n} \\right)$ に対して $Y \\left( s_{0} \\right)$ の最良線形不偏予測量BLUP, Best Linear Unbiased Predictorは $l$ と $\\mathbf{Y}$ の内積であり、 $$ l^{T} \\mathbf{Y} = \\begin{bmatrix} l_{1} \u0026amp; \\cdots \u0026amp; l_{n} \\end{bmatrix} \\begin{bmatrix} Y \\left( s_{1} \\right)\\\\ Y \\left( s_{2} \\right)\\\\ \\vdots\\\\ Y \\left( s_{n} \\right) \\end{bmatrix} = \\sum_{k=1}^{n} l_{k} Y \\left( s_{k} \\right) $$ ベクトル $l$ は具体的に次のように求められる。 $$ l = \\Gamma^{-1} \\left( \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\right) $$\n導出 1 具体的にクリギングがどのように行われるか、数式で調べるプロセスである。この公式の仮定にガウス過程という仮定まで追加されれば、オーディナリークリギングになる。\nパート1. 最適化問題\nある定数 $l_{1} , \\cdots , l_{n} , \\delta_{0} \\in \\mathbb{R}$ に対して新しい $Y \\left( s_{0} \\right)$ を既存データの線形結合 $$ \\hat{y} \\left( s_{0} \\right) = l_{1} y_{1} + \\cdots + l_{n} y_{n} + \\delta_{0} $$ で予測したいとする。これはつまり、目的関数 $$ E \\left[ Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) + \\delta_{0} \\right) \\right]^{2} $$ を最小化する最適解 $l_{1} , \\cdots , l_{n} , \\delta_{0}$ を見つけることになる。\n固有的定常性の定義: ユークリッド空間の固定された部分集合 $D \\subset \\mathbb{R}^{r}$ で、確率変数 $Y(s) : \\Omega \\to \\mathbb{R}^{1}$ の集合である空間過程 $\\left\\{ Y(s) \\right\\}_{s \\in D}$ と方向ベクトル $\\mathbf{h} \\in \\mathbb{R}^{r}$ を考える。具体的に $n \\in \\mathbb{N}$ 個のサイトSiteを $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$ のように表し、$Y(s)$ はすべての $s \\in D$ に対して分散が存在すると仮定する。$\\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]$ の平均が $0$ でありながら分散が唯一 $\\mathbf{h}$ にのみ依存する場合、$\\left\\{ Y \\left( s_{k} \\right) \\right\\}$ が固有的定常性Intrinsic Stationarityを持つと言われる。 $$ \\begin{align*} E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 0 \\\\ \\text{Var} \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 2 \\gamma ( \\mathbf{h} ) \\end{align*} $$\nここで $\\left\\{ Y \\left( s_{k} \\right) \\right\\}_{k=1}^{n}$ が固有的定常性を持つ場合、$\\sum_{k} l_{k} = 1$ という制約条件を置くことにより、 $$ \\begin{align*} \u0026amp; E \\left[ Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right) \\right] \\\\ =\u0026amp; E \\left[ \\sum_{k} l_{k} Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right) \\right] \\\\ =\u0026amp; \\sum_{k} l_{k} E \\left[ Y \\left( s_{0} \\right) - Y \\left( s_{k} \\right) \\right] \\\\ =\u0026amp; 0 \\end{align*} $$ となるようにできる。これにより、我々が最小化するべき目的関数は $\\delta_{0}$ が外れた次の形になる。 $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right]^{2} + \\delta_{0}^{2} $$ ここで $\\delta_{0}^{2}$ は予測と関係ない。実際にもモデルが $\\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon$ であれば、$\\delta_{0}$ は $\\mu$ に該当し、$\\delta_{0} = 0$ として $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right]^{2} $$ としても問題ない。今、$a_{0} = 1$ とし、$a_{k} = - l_{k}$ とすると、 $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k=1}^{n} l_{k} Y \\left( s_{k} \\right) \\right]^{2} = E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2} $$ となるため、我々は次の最適化問題をラグランジュ乗数法で解くことになる。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; \\displaystyle E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2} \\\\ \\text{subject to} \u0026amp; \\displaystyle \\sum_{k=0}^{n} a_{k} = 0 \\end{matrix} $$\nパート2. セミバリオグラム $\\gamma$\n今、$E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2}$ を「我々がデータから計算できると仮定できる」セミバリオグラムに依存した形で表してみよう。\nセミバリオグラムの定義: ユークリッド空間の固定された部分集合 $D \\subset \\mathbb{R}^{r}$ で、確率変数 $Y(s) : \\Omega \\to \\mathbb{R}^{1}$ の集合である空間過程 $\\left\\{ Y(s) \\right\\}_{s \\in D}$ と方向ベクトル $\\mathbf{h} \\in \\mathbb{R}^{r}$ を考える。具体的に $n \\in \\mathbb{N}$ 個のサイトを $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$ のように表し、$Y(s)$ はすべての $s \\in D$ に対して分散が存在すると仮定する。次のように定義される $2 \\gamma ( \\mathbf{h} )$ をバリオグラムVariogramと呼ぶ。 $$ 2 \\gamma ( \\mathbf{h} ) := E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]^{2} $$ 特にバリオグラムの半分 $\\gamma ( \\mathbf{h} )$ をセミバリオグラムSemivariogramと呼ぶ。\n$\\gamma \\left( s_{i} - s_{j} \\right)$ を2つのサイト $s_{i}, s_{j}$ の間の方向ベクトルによるセミバリオグラムとしよう。$\\sum_{0=1}^{n} a_{k} = 0$ を満たす任意の集合 $\\left\\{ a_{k} : k = 1 , \\cdots , n \\right\\} \\subset \\mathbb{R}$ に対して次が成立する。 $$ \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) = - E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} $$ これは次の展開 $$ \\begin{align*} \u0026amp; \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} \\text{Var} \\left[ Y \\left( s_{i} \\right) - Y \\left( s_{j} \\right) \\right] \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left[ Y \\left( s_{i} \\right) - Y \\left( s_{j} \\right) \\right]^{2} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left( \\left[ Y \\left( s_{i} \\right) \\right]^{2} - 2 Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) + \\left[ Y \\left( s_{j} \\right) \\right]^{2} \\right) \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left( Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) \\right) \u0026amp; \\because \\text{cases of } i = j \\\\ =\u0026amp; - E \\sum_{i} \\sum_{j} a_{i} a_{j} Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) \\\\ =\u0026amp; - E \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\sum_{j} a_{j} Y \\left( s_{j} \\right) \\\\ =\u0026amp; - E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} \\end{align*} $$ で確認できる。今、$\\gamma_{ij} = \\gamma \\left( s_{i} - s_{j} \\right)$ とし、$\\gamma_{0j} = \\gamma \\left( s_{0} - s_{j} \\right)$ とすれば、我々の目的関数は $$ \\begin{align*} \u0026amp; E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} l_{i} l_{j} \\gamma_{ij} + 2 \\sum_{i} l_{i} \\gamma_{0i} \\end{align*} $$ のように表され、ラグランジュ乗数法によって制約条件 $\\sum_{i} l_{i} = 1$ にラグランジュ乗数Lagrange Multiplierを掛けて引くことで、 $$ \\ - \\sum_{i} \\sum_{j} l_{i} l_{j} \\gamma_{ij} + 2 \\sum_{i} l_{i} \\gamma_{0i} - \\lambda \\sum_{i} l_{i} $$ を得る。したがって、$l_{i}$ に対して偏微分して $$ \\ - \\sum_{j} l_{j} \\gamma_{ij} + \\gamma_{0i} - \\lambda = 0 $$ となる時、目的関数を最小化できることがわかる。\nパート3. 最適解\nここで、具体的な最適解の公式を導出する。行列 $\\Gamma \\in \\mathbb{R}^{n \\times n}$ の $(i,j)$ 成分を ▷\neq74◁ とし、つまり $\\left( \\Gamma \\right)_{ij} := \\gamma_{ij}$ とし、ベクトル $\\gamma_{0} \\in \\mathbb{R}^{n}$ を $\\left( \\gamma_{0} \\right)_{i} := \\gamma_{0i}$ のように定義しよう。係数のベクトルも同様に $l := \\left( l_{1} , \\cdots , l_{n} \\right) \\in \\mathbb{R}^{n}$ とすると、パート2で得た式は次のような行列/ベクトル形式で表すことができる。 $$ \\begin{align*} \u0026amp; - \\sum_{j} l_{j} \\gamma_{ij} + \\gamma_{0i} - \\lambda = 0 \\\\ \\implies \u0026amp; - \\Gamma l + \\gamma_{0} - \\lambda \\mathbf{1} = 0 \\\\ \\implies \u0026amp; \\Gamma l + \\lambda \\mathbf{1} = \\gamma_{0} \\end{align*} $$ 一方、制約条件で $$ \\sum_{i} l_{i} = 1 \\iff \\begin{bmatrix} 1 \u0026amp; \\cdots \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} l_{1} \\\\ \\vdots \\\\ l_{n} \\end{bmatrix} = 1 \\iff \\mathbf{1}^{T} l = 1 $$ となるので、$\\mathbf{1}^{T} l = 1$ も得られる。ここで、$\\mathbf{x}^{T}$ は $\\mathbf{x}$ の転置を表す。まず単独で $\\lambda$ は $$ \\begin{align*} 1 =\u0026amp; \\mathbf{1}^T l \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\Gamma l \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\left( \\gamma_{0} - \\lambda \\mathbf{1} \\right) \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} - \\lambda \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} \\end{align*} $$ となり、整理すると $$ \\ - \\lambda = {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} $$ のように表すことができる。今、$l$ はほぼ求めたも同然である。 $$ \\begin{align*} \u0026amp; \\Gamma l + \\lambda \\mathbf{1} = \\gamma_{0} \\\\ \\implies \u0026amp; \\Gamma l = \\gamma_{0} - \\lambda \\mathbf{1} \\\\ \\implies \u0026amp; \\Gamma l = \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\\\ \\implies \u0026amp; l = \\Gamma^{-1} \\left( \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\right) \\end{align*} $$\n■\nBanerjee. (2015). Hierarchical Modeling and Analysis for Spatial Data(2nd Edition): p25, 40~41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2521,"permalink":"https://freshrimpsushi.github.io/jp/posts/2521/","tags":null,"title":"空間データ分析におけるクリギングとは？"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","id":null,"permalink":"https://freshrimpsushi.github.io/jp/search/","tags":null,"title":"Search Results"}]