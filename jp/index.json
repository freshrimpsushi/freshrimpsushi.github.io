[{"categories":"측도론","contents":"定義1 関数 $f : \\mathbb{R} \\to \\mathbb{R} (\\text{または } \\mathbb{C})$ が与えられたとする。$f$が任意の有限個の互いに素な区間 $(a_{i}, b_{i}) \\subset [a,b]$に対しても以下の条件を満たす場合、$[a, b]$ 上で 絶対連続absolutely continuousと言われる。\n$$ \\forall \\epsilon \\gt 0 \\quad \\exist \\delta \\gt 0 \\text{ such that } \\sum\\limits_{i=1}^{N} (b_{i} - a_{i}) \\lt \\delta \\implies \\sum\\limits_{i=1}^{N} \\left| f(b_{j}) - f(a_{j}) \\right| \\lt \\epsilon $$\n説明 定義により、絶対連続であれば一様連続でもある。\n性質 $f$が微分可能であり、導関数 $f\u0026rsquo;$ が有界であれば、$f$は絶対連続である。\n参照 実数関数の絶対連続性 測度の絶対連続性 符号付き測度の絶対連続性 Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p105\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3542,"permalink":"https://freshrimpsushi.github.io/jp/posts/3542/","tags":null,"title":"絶対連続実関数"},{"categories":"머신러닝","contents":"概要1 2 勾配降下法で使用されるアダプティブラーニングレートと、これを適用したモデルであるAdaGrad、RMSProp、Adamについて説明する。\n説明 勾配降下法で学習率learning rateは、パラメータが収束する速度、メソッドの成功の有無などを決定する重要なパラメータである。通常 $\\alpha$、$\\eta$と表記され、パラメータをアップデートする際、どれだけ勾配を反映するかを決める因子である。\n学習率による最適化の様子：大きな学習率(左)、小さな学習率(右)\r$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L (\\boldsymbol{\\theta}_{i}) $$\n基本的な勾配降下法では $\\alpha$ は定数として説明されているが、この場合 勾配はベクトルなので、全ての変数（パラメータ）に対して同じ学習率が適用される。\n$$ \\alpha \\nabla L (\\boldsymbol{\\theta}) = \\alpha \\begin{bmatrix} \\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} = \\begin{bmatrix} \\alpha\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nしたがって、学習率を $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{k})$ のようなベクトルとして考え、勾配項を以下の式のように一般化することができる。\n$$ \\boldsymbol{\\alpha} \\odot \\nabla L (\\boldsymbol{\\theta}) = \\begin{bmatrix} \\alpha_{1}\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha_{2}\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha_{k}\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nここで $\\odot$ は行列のアダマール積（要素毎の積）を表す。このようにパラメータ毎に異なる適用される学習率 $\\boldsymbol{\\alpha}$ を アダプティブラーニングレートadaptive learning rateと呼ぶ。以下の技術はアダプティブラーニングレートを勾配に依存して決定するため、$\\boldsymbol{\\alpha}$ は以下のような関数と見なすことができる。\n$$ \\boldsymbol{\\alpha} (\\nabla L(\\boldsymbol{\\theta})) = \\begin{bmatrix} \\alpha_{1}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\alpha_{2}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\dots \u0026amp; \\alpha_{k}(\\nabla L(\\boldsymbol{\\theta})) \\end{bmatrix} $$\n以下ではAdaGrad、RMSProp、Adamを紹介する。ここで重要な事実は、モーメンタム技術を含むこれらのオプティマイザー間に絶対的優位性はないということである。分野によって、作業によって最適なオプティマイザーは異なるので、単純に「何が最も良いか」についての判断や質問は良くない。自分が属する分野で主に使用されているものが何かを把握することが役に立ち、それがない場合やよくわからない場合は、SGD+モーメンタムまたはAdamを使用することが無難である。\nAdaGrad AdaGradは論文\u0026quot;(Duchi et al., 2011)Adaptive subgradient methods for online learning and stochastic optimization\u0026quot;で紹介されたアダプティブラーニングレート技術です。この名前はadaptive gradientの略で、[エイダグラード]または[アダグラード]と読みます。AdaGradでは、各パラメータに対する学習率を勾配に反比例するように設定します。ベクトル $\\mathbf{r}$ を次のように定義します。\n$$ \\mathbf{r} = (\\nabla L) \\odot (\\nabla L) = \\begin{bmatrix} \\left( \\dfrac{\\partial L}{\\partial \\theta_{1}} \\right)^{2} \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{2}} \\right)^{2} \u0026amp; \\cdots \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{k}} \\right)^{2} \\end{bmatrix} $$\n全体の学習率global learning rate $\\epsilon$、任意の小さい数 $\\delta$ に対して、アダプティブラーニングレート $\\boldsymbol{\\alpha}$ は次のようになります。\n$$ \\boldsymbol{\\alpha} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} $$\n式からわかるように、勾配の成分が大きい変数には学習率を小さくし、勾配の成分が小さい変数には学習率を大きく適用します。$\\delta$ は分母が $0$ や非常に小さい数になるのを防ぐためのもので、通常は $10^{-5} \\sim 10^{-7}$ の値を使用することが多いです。また、学習率は反復ごとに累積されます。$i$ 番目の反復での勾配を $\\nabla L _{i} = \\nabla L (\\boldsymbol{\\theta}_{i})$ とすると、\n$$ \\begin{align} \\mathbf{r}_{i} \u0026amp;= (\\nabla L_{i}) \\odot (\\nabla L_{i}) \\nonumber \\\\ \\boldsymbol{\\alpha}_{i} \u0026amp;= \\boldsymbol{\\alpha}_{i-1} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = \\sum_{j=1}^{i} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} \\\\ \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i} \\nonumber \\end{align} $$\nアルゴリズム: AdaGrad 入力 全体の学習率 $\\epsilon$、小さな正数 $\\delta$、エポック $N$ 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. for $i = 1, \\cdots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算し、各成分を二乗する 5. $\\boldsymbol{\\alpha} \\leftarrow \\boldsymbol{\\alpha} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # アダプティブ学習率更新 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # パラメータ更新 7. end for RMSProp RMSPropは Root Mean Square Propagationの略で、ジェフリー・ヒントンGeoffrey Hintonの講義 Neural networks for machine learningで提案されたアダプティブラーニングレート技術です。基本的にAdaGradの変形であり、追加される項が指数的に減少するように$(1)$の加算を加重和に変えただけです。$\\rho \\in (0,1)$に対して、\n$$ \\boldsymbol{\\alpha}_{i} = \\rho \\boldsymbol{\\alpha}_{i-1} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = (1-\\rho) \\sum_{j=1}^{i} \\rho^{i-j} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} $$\n通常は $\\rho = 0.9, 0.99$ のような大きな値が使用されます。\nアルゴリズム: RMSProp 入力 全体の学習率 $\\epsilon$、小さな正数 $\\delta$、減衰率 $\\rho$、エポック $N$ 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. for $i = 1, \\dots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算し、各成分を二乗する 5. $\\boldsymbol{\\alpha} \\leftarrow \\rho \\boldsymbol{\\alpha} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # アダプティブ学習率更新 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # パラメータ更新 7. end for Adam Adam\u0026ldquo;adaptive moments\u0026quot;から派生は論文\u0026rdquo;(Kingma and Ba, 2014)Adam: A method for stochastic optimization\u0026quot;で紹介されたオプティマイザです。アダプティブラーニングレートとモーメンタム技術を組み合わせたもので、RMSProp + モーメンタムと見ることができます。RMSPropとモーメンタムを理解していれば、Adamを理解するのは難しくありません。RMSProp、モーメンタム、Adamをそれぞれ比較すると、以下のようになります。$\\nabla L_{i} = \\nabla L(\\boldsymbol{\\theta}_{i})$とすると、\nMomentum\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + \\nabla L_{i} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\mathbf{p}_{i}$\rRMSProp\r$\\mathbf{r}_{i} = \\nabla L_{i} \\odot \\nabla L_{i} \\\\\r\\boldsymbol{\\alpha}_{i} = \\beta_{2} \\boldsymbol{\\alpha}_{i-1} + (1-\\beta_{2})\\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} \\quad (\\boldsymbol{\\alpha}_{0}=\\mathbf{0})\\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i}$\rAdam\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + (1-\\beta_{1}) \\nabla L_{i-1} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\\\\[0.5em]\r\\hat{\\mathbf{p}}_{i} = \\dfrac{\\mathbf{p}_{i}}{1-(\\beta_{1})^{i}} \\\\\\\\[0.5em]\r\\mathbf{r}_{i} = \\beta_{2} \\mathbf{r}_{i-1} + (1-\\beta_{2}) \\nabla L_{i} \\odot \\nabla L_{i} \\\\\\\\[0.5em]\r\\hat{\\mathbf{r}}_{i} = \\dfrac{\\mathbf{r}}{1-(\\beta_{2})^{i}} \\\\\\\\[0.5em]\r\\hat{\\boldsymbol{\\alpha}}_{i} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}_{i}}} \\\\\\\\[0.5em]\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\hat{\\boldsymbol{\\alpha}_{i}} \\odot \\hat{\\mathbf{p}_{i}}\r$\r$\\hat{\\mathbf{p}}_{i}$および$\\hat{\\mathbf{r}}_{i}$を計算する際に$1 - \\beta^{i}$で割る理由は、$\\mathbf{p}_{i}$および$\\mathbf{r}_{i}$が加重和であるため、これを加重平均に変換するためです。\nアルゴリズム: Adam\r入力 全体の学習率 $\\epsilon$ (推奨値は $0.001$), エポック $N$ 小さな定数 $\\delta$ (推奨値は $10^{-8}$) 減衰率 $\\beta\\_{1}, \\beta\\_{2}$ (推奨値はそれぞれ $0.9$ と $0.999$) 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. モーメンタムを $\\mathbf{p} = \\mathbf{0}$ で初期化する。 4. for $i = 1, \\dots, N$ do 5. $\\mathbf{p} \\leftarrow \\beta\\_{1}\\mathbf{p} + (1-\\beta\\_{1}) \\nabla L$ # 加重和でモーメンタム更新 6. $\\hat{\\mathbf{p}} \\leftarrow \\dfrac{\\mathbf{p}}{1-(\\beta\\_{1})^{i}}$ # 和を加重平均に補正 7. $\\mathbf{r} \\leftarrow \\beta\\_{2} \\mathbf{r} + (1-\\beta\\_{2}) \\nabla L \\odot \\nabla L$ # 加重和で勾配の二乗ベクトル更新 8. $\\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1-(\\beta\\_{2})^{i}}$ # 和を加重平均に補正 9. $\\hat{\\boldsymbol{\\alpha}} \\leftarrow \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}}}$ # アダプティブ学習率更新 10. $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\alpha}} \\odot \\hat{\\mathbf{p}}$ # パラメータ更新 11. end for Ian Goodfellow, Deep Learning, 8.5 アダプティブラーニングレートを用いたアルゴリズム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nオ・イルソク, 機械学習(MACHINE LEARNING) (2017), ch 5.4 アダプティブラーニングレート\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3529,"permalink":"https://freshrimpsushi.github.io/jp/posts/3529/","tags":null,"title":"適応的な学習率: AdaGrad, RMSProp, Adam"},{"categories":"머신러닝","contents":"概要1 2 勾配降下法におけるモーメンタム技術は、パラメーターを更新する際に以前の勾配もすべて使用することである。これが本質であり、これに尽きる。しかし、奇妙な更新式や物理学の運動量が動機となったとか、質量を$1$に設定し初期速度を$0$にするといった説明は理解を難しくするだけである。本稿では、モーメンタム技術をできるだけシンプルに説明する。\nビルドアップ パラメーターを$\\boldsymbol{\\theta}$、損失関数を$L$とするとき、標準的な勾配降下法は、以下のように反復的にパラメーターを更新する方法である。\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} $$\nここで$L_{i} = L(\\boldsymbol{\\theta}_{i})$は、$i$番目の反復で計算された損失関数を意味する。モーメンタム技術とは、これに単に前の反復で計算された損失関数の勾配$\\nabla L_{i-1}$を加えることに過ぎない。\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\alpha \\nabla L_{i-1} - \\cdots - \\alpha \\nabla L_{0} $$\nここで、反復が進むにつれて勾配の影響を減らし、勾配の合計が発散するのを防ぐために係数$\\beta \\in (0,1)$を追加すると、次のようになる。\n$$ \\begin{align} \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\beta \\alpha \\nabla L_{i-1} - \\cdots - \\beta^{i}\\alpha \\nabla L_{0} \\nonumber \\\\ \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha\\sum_{j=0}^{i} \\beta^{j} \\nabla L_{i-j} \\end{align} $$\n定義 $(1)$のようにパラメーターを更新することをモーメンタム技術momentum methodと呼び、追加される項$\\alpha\\sum\\limits_{j=0}^{i} \\beta^{j} \\nabla L_{i-j}$をモーメンタムmomentumと呼ぶ。\n説明 上記の定義によれば、モーメンタム技術は一般化された勾配降下法であり、むしろ勾配降下法はモーメンタム技術の$\\beta = 0$の特別なケースに過ぎないと見ることができる。$\\beta$が$1$に近いほど以前の勾配を多く反映し、$0$に近いほど少なく反映する。\n勾配降下法は、現在の傾きが最も大きい方向へパラメーターを更新するために貪欲アルゴリズムである。モーメンタム技術は勾配降下法の貪欲な部分を少し和らげ、現在最善の選択ではないが長期的にはより有効な選択をすることができるようにする。また、勾配の方向が急激に変わるのを防ぐことができる。\n当然ながら、パラメーターを更新する際の勾配の大きさが勾配降下法より大きいため、収束速度が速いという利点がある。また、経験的に局所的最小値local minimaから比較的脱出しやすいことが知られており、坂を転がり下るボールが十分な速さであれば、下り坂の途中にある小さな坂も越えて通り過ぎることができると説明される。\nここで重要な事実は、適応的学習率技術を含むこれらのオプティマイザー間に絶対的な優位性はないということである。分野や作業によって最適なオプティマイザーが異なるため、「何が最も良いか」という判断や質問は適切ではない。自分が所属する分野で主に使用されているものが何かを知ることが役立ち、それがないか分からなければSGD+モーメンタムまたはAdamを使用するのが無難である。\nネステロフのモーメンタム モーメンタム技術を再検討すると、次のパラメーター$\\boldsymbol{\\theta}_{i+1}$を得るために、現在のパラメーター$\\boldsymbol{\\theta}_{i}$に現在のパラメーターで計算された勾配$\\alpha \\nabla L(\\boldsymbol{\\theta}_{i})$を蓄積しながら加えていく。\nネステロフのモーメンタムNesterov momentumまたはネステロフ加速勾配Nesterov accelerated gradient, NAGと呼ばれる技術は、「現在のパラメーターに前の勾配を加えた値」で勾配を求め、これを現在のパラメーターに加えて次のパラメーターを求める。言葉ではやや複雑だが、モーメンタム技術を理解していれば、以下のアルゴリズムを見ることでネステロフのモーメンタムを理解するのが簡単になるかもしれない。\nアルゴリズム モーメンタム項を$\\mathbf{p}$と表す。\nアルゴリズム: モーメンタム技術 入力 学習率 $\\alpha$, モーメンタムパラメーター $\\beta$, エポック $N$ 1. パラメーター $\\boldsymbol{\\theta}$を任意の値で初期化する。 2. モーメンタムを $\\mathbf{p} = \\mathbf{0}$で初期化する。 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算しモーメンタムを更新 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # パラメーターを更新 6. end for アルゴリズム: ネステロフのモーメンタム 入力 学習率 $\\alpha$, モーメンタムパラメーター $\\beta$, エポック $N$ 1. パラメーター $\\boldsymbol{\\theta}$を任意の値で初期化する。 2. モーメンタムを $\\mathbf{p} = \\mathbf{0}$で初期化する。 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta} + \\beta \\mathbf{p})$ # 勾配を計算しモーメンタムを更新 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # パラメーターを更新 6. end for 両方の方法について初めの数回の計算を見ると、以下のようになる。簡単に$\\mathbf{p}_i = \\alpha \\nabla L_i$、および$\\mathbf{p}^i = \\alpha \\nabla L(\\boldsymbol{\\theta}_i - \\beta^{1}\\mathbf{p}^{i-1} - \\beta^{2}\\mathbf{p}^{i-2} - \\cdots - \\beta^{i}\\mathbf{p}^{0})$（このとき$\\mathbf{p}^{0} = \\mathbf{p}_0$）と表記すると、\nモーメンタム ネステロフのモーメンタム $\\boldsymbol{\\theta}\\_{0} =$ 初期値\r$\\boldsymbol{\\theta}\\_{0} =$ 初期値\r$\\boldsymbol{\\theta}_{1} = \\boldsymbol{\\theta}_{0} - \\alpha \\nabla L_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{0} - \\mathbf{p}_{0}$\r$\\boldsymbol{\\theta}_{1} = \\boldsymbol{\\theta}_{0} - \\alpha \\nabla L_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{0} - \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}_{2} = \\boldsymbol{\\theta}_{1} - \\alpha\\nabla L_{1} - \\beta \\mathbf{p}_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{1} - \\mathbf{p}_{1} - \\beta \\mathbf{p}_{0}$\r$\\boldsymbol{\\theta}_{2} = \\boldsymbol{\\theta}_{1} - \\alpha \\nabla L(\\boldsymbol{\\theta}_{1} - \\beta \\mathbf{p}^{0}) - \\beta \\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{1} - \\mathbf{p}^{1} - \\beta \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}_{3} = \\boldsymbol{\\theta}_{2} - \\mathbf{p}_{2} - \\beta \\mathbf{p}_{1} - \\beta^{2} \\mathbf{p}_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{2} - \\sum\\limits_{j=0}^{2}\\beta^{j}\\mathbf{p}_{2-j}$\r$\\boldsymbol{\\theta}_{3} = \\boldsymbol{\\theta}_{2} - \\alpha \\nabla L(\\boldsymbol{\\theta}_{2} - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0}) - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{2} - \\mathbf{p}^{2} - \\beta \\mathbf{p}^{1} - \\beta^{2} \\mathbf{p}^{0}$\r$$\\vdots$$\r$$\\vdots$$\r$\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\sum\\limits_{j=0}^{i}\\beta^{j}\\mathbf{p}_{i-j}$\r$\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\sum\\limits_{j=0}^{i}\\beta^{j}\\mathbf{p}^{i-j}$\rイアン・グッドフェロー, ディープラーニング, 第8.3.2節 モーメンタム, 第8.3.3節 ネステロフのモーメンタム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nオ・イルソク, 機械学習(MACHINE LEARNING) (2017), 第5.3節 モーメンタム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3528,"permalink":"https://freshrimpsushi.github.io/jp/posts/3528/","tags":null,"title":"勾配降下における運動量法"},{"categories":"줄리아","contents":"概要 ジュリアでもプログラムの進行状況を知らせてくれるグラスバーを手軽に使うことができる。\nコード ProgressMeter.jl 「ProgressMeter.jl」パッケージの「@showprogress」マクロを「for」ループに置けばよい1。\nusing ProgressMeter\rchi2 = []\r@showprogress for n in 1:20000\rpush!(chi2, sum(randn(n) .^ 2))\rend 下の「ProgressBars.jl」に比べるとマクロを使うのでコードがより簡潔である。\nProgressBars.jl 「ProgressBars.jl」パッケージの「ProgressBar()」関数で「for」ループの反復子Iteratorを包めば良い2。\nusing ProgressBars\rchi2 = []\rfor n in ProgressBar(1:20000)\rpush!(chi2, sum(randn(n) .^ 2))\rend 実際の作業内容はどうであれ構わないが、プログラムの進行状況は次のようにきれいに出力される。 当然だが、「for」ループ文で正確に何回目の繰り返しになっているかだけ分かるので、1回の繰り返し当たりの平均遂行時間を知らせるだけで、正確な所要時間を予測することはできない。\n環境 OS: Windows julia: v1.7.3 https://github.com/timholy/ProgressMeter.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/cloud-oak/ProgressBars.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2497,"permalink":"https://freshrimpsushi.github.io/jp/posts/2497/","tags":null,"title":"ジュリアでプログレスバーの使い方"},{"categories":"머신러닝","contents":"概要 モンテカルロ積分は、与えられた関数の積分を計算するのが困難な場合に使用される数値的近似方法の一つである。次のような状況を想定しよう。与えられた $[0, 1]$または一般的に $[0, 1]^{n}$で積分可能な関数 $f$に対して、私たちは $f(x)$の式を知っているが、その積分を計算するのは簡単ではない。しかし、私たちは $f$の積分 $I[f]$を計算したい。\n$$ \\begin{equation} I[f] = \\int_{[0,1]} f(x) dx \\end{equation} $$\n定義 モンテカルロ積分Monte Carlo integrationとは、与えられた $[0, 1]$ 上での分布に基づきサンプル $\\left\\{ x_{i} \\right\\}$を抽出し、$f$の積分を次のように推定estimateする方法である。\n$$ I[f] \\approx I_{n}[f] := \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\n区分求積法との違い 区分求積法のアイデアは、区間 $[0,1]$を $n$等分し、点 $\\left\\{ x_{i} = \\frac{i-1}{n} \\right\\}_{i=1}^{n}$を得て、これらの点での関数値を全て加算することである。\n$$ \\text{区分求積法}[f] = \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\n式の見た目だけではモンテカルロ積分と区分求積法は異なるもののないように見えるが、その意味は全く異なる。区分求積法での $\\left\\{ x_{i} \\right\\}$は区間 $[0, 1]$を $n$等分して得た点であるのに対し、モンテカルロ積分では $x$が従う分布 $p(x)$から抽出された $n$個のサンプルを意味する。したがって、区分求積法で得られた値は単純に $f$が描くグラフの下の面積を意味するが、モンテカルロ積分で得られた値は $f$の期待値である。\n性質 式 $(1)$が持つ統計的な意味は「$I[f]$は $X$が一様分布に従うときの $f(X)$の期待値と同じである」ということである。\n$$ X \\sim U(0,1) \\implies I[f] = \\int_{[0,1]} f(x) dx = E\\left[ f(X) \\right] $$\n期待値 確率変数 $X$が一様分布に従うとしよう。$I_{n}[f]$は $I[f]$の不偏推定量である。\n$$ E\\left[ I_{n}[f] \\right] = I[f] $$\n証明 $$ \\begin{align*} E\\left[ I_{n}[f] \\right] \u0026amp;= E\\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} E\\left[ f(X_{i}) \\right] \\qquad \\text{by linearity of $E$} \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} I\\left[ f \\right] \\\\ \u0026amp;= I\\left[ f \\right] \\end{align*} $$\n■\n分散 証明 分散の性質\n[a] $\\Var (aX) = a^{2} \\Var (X)$\n[b] $X, Y$が独立ならば、$\\Var (X + Y) = \\Var(X) + \\Var(Y)$\n$f(X)$の分散を $\\sigma^{2}$としよう。すると分散の性質により、\n$$ \\begin{align*} \\Var \\left[ I_{n}[f] \\right] \u0026amp;= \\Var \\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\Var \\left[ \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\Var \\left[ f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\sigma^{2} \\\\ \u0026amp;= \\dfrac{\\sigma^{2}}{n} \\end{align*} $$\n■\n一般化 ここで $p(x) \\ge 0$で $\\int_{[0,1]} p = 1$となる関数 $p$について、積分 $I[fp]$を考えよう。\n$$ I[fp] = \\int_{[0, 1]}f(x)p(x) dx $$\nこれは確率密度関数が $p$である確率変数 $X$について、$f(X)$の期待値と同じである。この値を近似する方法として、次の二つの方法が考えられる。\nサンプル $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$を一様分布から抽出し、$I[fp]$を次のように近似する。 $$ X_{i} \\sim U(0,1) \\qquad I[fp] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i})p(x_{i}) $$\nサンプル $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$を $p(x)$から抽出し、$I[fp]$を次のように近似する。 $$ X_{i} \\sim p(x) \\qquad I[fp] = I_{p}[f] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i}) $$\n言い換えれば、1.は $f(x)p(x)$を一様分布でサンプリングして平均を求めたものであり、2.は $f(x)$を $p(x)$でサンプリングして平均を求めたものである。これらのうち分散がより小さいのは1.である。$I = I[fp] = I[fp]$と簡単に記しよう。\n1.の場合 $$ \\begin{align*} \\sigma_{1}^{2} = \\Var [fp] \u0026amp;= E \\left[ (fp - I)^{2} \\right] \\\\ \u0026amp;= \\int (fp - I)^{2} dx \\\\ \u0026amp;= \\int (fp)^{2} dx - 2I\\int fp dx + I^{2}\\int dx\\\\ \u0026amp;= \\int (fp)^{2} dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int (fp)^{2} dx - I^{2}\\\\ \\end{align*} $$\n2.の場合 $$ \\begin{align*} \\sigma_{2}^{2} = \\Var [f] \u0026amp;= E_{p} \\left[ (f - I)^{2} \\right] \\\\ \u0026amp;= \\int (f - I)^{2}p dx \\\\ \u0026amp;= \\int f^{2}p dx - 2I\\int fp dx + I^{2}\\int pdx\\\\ \u0026amp;= \\int f^{2}p dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int f^{2}p dx - I^{2}\\\\ \\end{align*} $$\nしかし $0 \\le p \\le 1$であるため、$f^{2}p \\ge f^{2}p^{2}$である。したがって\n$$ \\sigma_{1}^{2} \\le \\sigma_{2}^{2} $$\n","id":3515,"permalink":"https://freshrimpsushi.github.io/jp/posts/3515/","tags":null,"title":"モンテカルロ積分"},{"categories":"푸리에해석","contents":"概要1 離散フーリエ変換DFTは、数式的な定義に従って計算すると、$\\mathcal{O}(N^{2})$の時間計算量を持ちますが、以下で説明するアルゴリズムに従って計算すると、時間計算量が$\\mathcal{O}(N\\log_{2}N)$に低減します。この高速フーリエ変換を使用して離散フーリエ変換を高速に実行することができます。これは高速フーリエ変換fast Fourier transform, FFTと呼ばれています。\n構築 2つの数字を掛けて、それを別の数字に加える操作を$1$回の演算operationと呼びましょう。すると、$\\sum\\limits_{i=0}^{n-1}a_{n}b_{n}$の値を求めるには$n$回の演算が必要です。\n$$ \\begin{align*} \\sum\\limits_{n=0}^{0} a_{n}b_{b} \u0026amp;= a_{0}b_{0} = \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\\\ \\sum\\limits_{n=0}^{1} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} = \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\\\ \\sum\\limits_{n=0}^{2} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} + a_{2}b_{2} = \\overbrace{\\bigg( \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\bigg) {\\color{#FE9A2E} + } a_{2} {\\color{#FE9A2E} \\times} b_{2}}^{\\color{#FE9A2E}3 \\text{ operations}} \\\\ \\end{align*} $$\nさて、離散フーリエ変換の定義を思い出してみましょう。\n線型変換 $\\mathcal{F}_{N} : \\mathbb{C}^{N} \\to \\mathbb{C}^{N}$を離散フーリエ変換と呼びます。\n$$ \\mathcal{F}_{N}(\\mathbf{a}) = \\hat{\\mathbf{a}} = \\begin{bmatrix} \\hat{a}_{0} \\\\ \\hat{a}_{1} \\\\ \\dots \\\\ \\hat{a}_{N-1} \\end{bmatrix} ,\\quad \\hat{a}_{m} = \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n}\\quad (0\\le m \u0026lt; N) \\tag{1} $$\nこのとき、$\\mathbf{a} = \\begin{bmatrix} a_{0}\u0026amp; a_{1}\u0026amp; \\dots\u0026amp; a_{N-1} \\end{bmatrix}^{T}$です。\n$\\hat{a}_{m}$を計算するには$N$回の演算が必要で、$\\hat{\\mathbf{a}}$を計算するにはこれを$N$回繰り返す必要があるため、離散フーリエ変換を計算するには合計で$N^{2}$回の演算が必要です。つまり、$\\mathcal{O}(N^{2})$の時間計算量を持っています。これは、コンピュータ計算の観点からフーリエ変換がかなりのコストを要することを意味します。\nアルゴリズム データの長さ$N$を合成数$N = N_{1}N_{2}$としましょう。そして、インデックス$m, n$を次のように定義します。\n$$ m = m^{\\prime}N_{1} + m^{\\prime \\prime},\\quad n = n^{\\prime}N_{2} + n^{\\prime \\prime} $$\nすると、$0 \\le m^{\\prime}, n^{\\prime \\prime} \\le N_{2}-1$および$0 \\le m^{\\prime \\prime}, n^{\\prime} \\le N_{1}-1$です。$(1)$の指数部分を次のように表現できます。\n$$ \\begin{align*} e^{-i2\\pi mn /N} \u0026amp;= e^{-i2\\pi (m^{\\prime}N_{1} + m^{\\prime \\prime})(n^{\\prime}N_{2} + n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi (m^{\\prime}n^{\\prime}N_{1}N_{2} + m^{\\prime}n^{\\prime \\prime}N_{1} + m^{\\prime \\prime}n^{\\prime}N_{2} + m^{\\prime \\prime}n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi m^{\\prime}n^{\\prime}} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \u0026amp;= e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \\end{align*} $$\nこれを$(1)$に代入すると、\n$$ \\begin{align*} \\hat{a}_{m} \u0026amp;= \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi m^{\\prime \\prime}n^{\\prime}/N_{1}}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\right] e^{-i2\\pi [ (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) ] } \\end{align*} $$\n上記の式に従うと、各括弧内の$\\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} \\right]$を計算するのに$N_{1}$回の演算、括弧の外の$\\sum_{n^{\\prime \\prime}=0}^{N_{2}-1}$を計算するのに$N_{2}$回の演算が必要です。したがって、$\\hat{a}_{m}$を計算するには合計で$(N_{1} + N_{2})$回の演算が必要です。$\\hat{\\mathbf{a}}$を得るにはこれを$N$回繰り返す必要があるため、合計で$N(N_{1} + N_{2})$のコストがかかり、$N^{2}$よりも減少することが確認できます。\n括弧内を注意深く見ると、$N_{1}$が再び合成数の場合、同じロジックを適用できることがわかるでしょう。したがって、データの長さが$N = N_{1} N_{2} \\cdots N_{k}$といった合成数の場合、時間計算量は次のように減少します。\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}\\big( N(N_{1} + N_{2} + \\cdots + N_{k}) \\big) $$\nここで、$N$を$2$のべき乗$N = 2^{k}$と仮定してみましょう。すると、$\\log_{2}N = k$であり、$N^{2} = 2^{k}$から$2^{k}(2k)$だけ減少するため、時間計算量は次のように減少します。\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}(2N \\log_{2}N) $$\n補足 これは1965年にCooleyとTukey2によって提案されたため、Cooley-Tukeyアルゴリズムとも呼ばれています。ただし、彼らが最初に発明したわけではありません。ガウスも同様のアルゴリズムを研究しましたが、正しく発表しなかったため、この事実は後に明らかになりました3。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p252-253\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. W. Cooley and J. W. Tukey, An algorithm for the machine calculation of complex Fourier series, Mathematics of Computation 19 (1965), 297-301.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. T. Heideman, D. H. Johnson, and C. S. Burms, Gauss and the history of the fast Fourier transform, Archive for the History of the Exact Sciences 34 (1985), 264-277.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3492,"permalink":"https://freshrimpsushi.github.io/jp/posts/3492/","tags":null,"title":"고속 푸리에 변환"},{"categories":"확률론","contents":"概要 測度論と確率論を学んだ人向けの定義と概念の要約資料です。迅速な復習と定義の参照のために作成されました。\n測度論 代数 $X \\ne \\varnothing$の部分集合たちのコレクション $\\mathcal{A}$が 有限 合集合と補集合に対して閉じている時、これを代数と言います。\n可算合集合に対して閉じている代数を$\\sigma$-代数と言います。\nNote:\n定義により $\\mathcal{A}$はまた交集合に対しても閉じています $\\big( \\because E_{1} \\cap E_{2} = \\left( E_{1} \\cup E_{2} \\right)^{c} \\in \\mathcal{A}$ for $E_{1}, E_{2} \\in \\mathcal{A} \\big)$ $\\mathcal{A}$は空集合 $\\varnothing$と全集合 $X$を含みます。 $\\big( \\because E \\in \\mathcal{A}$ $\\implies$ $\\varnothing = E \\cap E^{c} \\in \\mathcal{A} \\text{ and } X = E \\cup E^{c} \\in \\mathcal{A} \\big)$ $X$が位相空間なら、$X$の開集合たちのコレクションから作られる$\\sigma$-代数を$X$上のボレル $\\sigma$-代数と言い、$\\mathcal{B}_{X}$と表記します。\nボレル $\\sigma$-代数は全ての開集合を含む最も小さい唯一の$\\sigma$-代数です。 $\\mathcal{E}$を$X$上の$\\sigma$-代数としましょう。順序対 $(X, \\mathcal{E})$を可測空間と言い、$E \\in \\mathcal{E}$を可測集合と言います。\n特に言及がない限り、以下では固定された可測空間 $(X, \\mathcal{E})$について扱います。\n可測関数 全ての実数 $\\alpha \\in \\mathbb{R}$に対して、次を満たす関数 $f : X \\to \\mathbb{R}$を($\\mathcal{E}$-)可測と言います。 $$ \\left\\{ x \\in X : f(x) \\gt \\alpha \\right\\} \\in \\mathcal{E}\\qquad \\forall \\alpha \\in \\mathbb{R}. $$\n一般化 $(X, \\mathcal{E})$、$(Y, \\mathcal{F})$を可測空間とします。関数 $f : X \\to Y$が次を満たす時、これを$(\\mathcal{E}, \\mathcal{F})$-可測と言います。 $$ f^{-1}(F) = \\left\\{ x \\in X : f(x) \\in F \\right\\} \\in \\mathcal{E}\\qquad \\forall F \\in \\mathcal{F}. $$\nNote: $\\mathcal{E}$-可測関数は上の定義で$(Y, \\mathcal{F}) = (\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$の場合と同じです。\n測度 $\\mathcal{E}$ (または $(X, \\mathcal{E})$、$X$)上の測度とは、次を満たす関数 $\\mu : \\mathcal{E} \\to [0, \\infty]$です。\nNull empty set: $\\mu (\\varnothing) = 0$。 Countable additivity: $\\left\\{ E_{j} \\right\\}$が$\\mathcal{E}$の互いに素な集合たちなら、$\\displaystyle \\mu \\left( \\bigcup\\limits_{j} E_{j} \\right) = \\sum\\limits_{j} \\mu (E_{j})$。 三つ組 $(X, \\mathcal{E}, \\mu)$を測度空間と言います。特に言及がない限り以下では固定された測度空間 $(X, \\mathcal{E}, \\mu)$について扱います。\nボレル測度とは、定義域がボレル $\\sigma$-代数$\\mathcal{B}_{\\mathbb{R}}$の測度を言います： $$ \\mu : \\mathcal{B}_{\\mathbb{R}} \\to [0, \\infty] $$\n$(X, \\mathcal{E})$、$(Y, \\mathcal{F})$上の二つの測度 $\\mu$、$\\nu$に対して、次を満たす$\\mathcal{E} \\times \\mathcal{F}$上の唯一の測度 $\\mu \\times \\nu$を$\\mu$と$\\nu$の積測度と言います。 $$ \\mu \\times \\nu (E \\times F) = \\mu(E) \\nu(F)\\qquad \\text{ for all rectangles } E \\times F. $$\n積分 実関数 $f$が有限な関数値を持つ時、これを単純と言います。\n単純可測関数 $\\varphi$は次のような形で表されます。 $$ \\begin{equation} \\varphi = \\sum\\limits_{j=1}^{n} a_{j}\\chi_{E_{j}}, \\text{ where } E_{j} = \\varphi^{-1}(\\left\\{ a_{j} \\right\\}) \\text{ and } \\operatorname{range} (\\varphi) = \\left\\{ a_{1}, \\dots, a_{n} \\right\\}. \\end{equation} $$ ここで $\\chi_{E_{j}}$は$E_{j}$の特性関数です。これを$\\varphi$のstandard representationと言います。\n$\\varphi$がstandard representation $(1)$を持つ単純可測関数の時、測度 $\\mu$に対する**$\\varphi$の積分**を次のように定義します。 $$ \\int \\varphi d\\mu := \\sum\\limits_{j=1}^{n} a_{j}\\mu(E_{j}). $$ Notation: $$ \\int \\varphi d\\mu = \\int \\varphi = \\int \\varphi(x) d\\mu(x), \\qquad \\int = \\int_{X}. $$\n$f$が$(X, \\mathcal{E})$上の可測関数の時、$\\mu$に対する**$f$の積分**を次のように定義します。 $$ \\int f d\\mu := \\sup \\left\\{ \\int \\varphi d\\mu : 0 \\le \\varphi \\le f, \\varphi \\text{ is simple and measurable} \\right\\}. $$\n$f : X \\to \\mathbb{R}$の正の部分と負の部分をそれぞれ次のように定義します。 $$ f^{+}(x) := \\max \\left( f(x), 0 \\right)),\\qquad f^{-1}(x) := \\min \\left(-f(x), 0 \\right)). $$ もし二つの積分$\\displaystyle \\int f^{+}$、$\\displaystyle \\int f^{-}$が有限なら、$f$が積分可能と言います。また$\\left| f \\right| = f^{+} - f^{-}$が成立します。\n積分可能な実関数たちの集合はベクトル空間であり、積分はこのベクトル空間上の線形汎関数です。このベクトル空間を次のように表記します。 $$ L = L(X, \\mathcal{E}, \\mu) = L(X, \\mu) = L(X) = L(\\mu), \\qquad L = L^{1} $$\n$L^{p}$空間\n測度空間$(X, \\mathcal{E}, \\mu)$と$0 \\lt p \\lt \\infty$に対して、$L^{p}$を次のように定義します。 $$ L^{p}(X, \\mathcal{E}, \\mu) := \\left\\{ f : X \\to \\mathbb{R} \\left| f \\text{ is measurable and } \\left( \\int \\left| f \\right|^{p} d\\mu \\right)^{1/p} \\lt \\infty \\right. \\right\\}. $$\n確率論 表記法と用語 $$ \\begin{array}{lll} \\text{Analysts\u0026rsquo; Term} \u0026amp;\u0026amp; \\text{Probabilists\u0026rsquo; Term} \\\\ \\hline \\text{Measure space } (X, \\mathcal{E}, \\mu) \\text{ such that } \\mu(X) = 1 \u0026amp;\u0026amp; \\text{Probability space } (\\Omega, \\mathcal{F}, P) \\\\ \\text{Measure } \\mu : \\mathcal{E} \\to \\mathbb{R} \\text{ such that } \\mu(X) = 1 \u0026amp;\u0026amp; \\text{Probability } P : \\mathcal{F} \\to \\mathbb{R} \\\\ (\\sigma\\text{-)algebra $\\mathcal{E}$ on $X$} \u0026amp;\u0026amp; (\\sigma\\text{-)field $\\mathcal{F}$ on $\\Omega$} \\\\ \\text{Mesurable set } E \\in \\mathcal{E} \u0026amp;\u0026amp; \\text{Event } E \\in \\mathcal{F} \\\\ \\text{Measurable real-valued function } f : X \\to \\mathbb{R} \u0026amp;\u0026amp; \\text{Random variable } X : \\Omega \\to \\mathbb{R} \\\\ \\text{Integral of } f, {\\displaystyle \\int f d\\mu} \u0026amp;\u0026amp; \\text{Expextation of } f, E(X) \\\\ f \\text{ is } L^{p} \u0026amp;\u0026amp; X \\text{ has finite $p$th moment} \\\\ \\text{Almost everywhere, a.e.} \u0026amp;\u0026amp; \\text{Almost surely, a.s.} \\end{array} $$\n$$ \\begin{align*} \\left\\{ X \\gt a \\right\\} \u0026amp;:= \\left\\{ w : X(w) \\gt a \\right\\} \\\\ P\\left( X \\gt a \\right) \u0026amp;:= P\\left( \\left\\{ w : X(w) \\gt a \\right\\} \\right) \\end{align*} $$\n基礎定義 可測空間$(\\Omega, \\mathcal{F})$、$(\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$に対して、$(\\mathcal{F}, \\mathcal{B}_{\\mathbb{R}})$-可測関数$X : \\Omega \\to \\mathbb{R}$を確率変数と言います。つまり、 $$ X^{-1}(B) \\in \\mathcal{F}\\qquad \\forall B \\in \\mathcal{B}_{\\mathbb{R}}. $$\n$(\\Omega, \\mathcal{F})$上の確率(または確率測度)とは、$P(\\Omega) = 1$を満たす測度$P : \\mathcal{F} \\to \\mathbb{R}$です。\n$X$を確率変数とする時、\n期待値: $\\displaystyle E(X) := \\int X dP$ 分散: $\\sigma^{2}(X) := E\\left[ (X - E(X))^{2} \\right] = E(X^{2}) - E(X)^{2}$ $X$の(確率)分布とは、次を満たす$\\mathbb{R}$上の確率$P_{X} : \\mathcal{B}_{\\mathbb{R}} \\to \\mathbb{R}$です： $$ P_{X}(B) := P(X^{-1}(B)). $$\n$X$の分布関数$F_{X}$は次のように定義されます： $$ F_{X}(a) := P_{X}\\left( (-\\infty, a] \\right) = P(X \\le a). $$\n確率変数の数列$\\left\\{ X_{i} \\right\\}_{i=1}^{n}$に対して、確率ベクトル$(X_{1}, \\dots, X_{n})$は次のように定義される関数を言います： $$ (X_{1}, \\dots, X_{n}) : \\Omega \\to \\mathbb{R}^{n} $$ $$ (X_{1}, \\dots, X_{n})(x) := (X_{1}(x), \\dots, X_{n}(x)). $$\nNote: $(X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n})= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})$。\n$n=2$の場合を先に見ましょう。$(X, Y) : \\Omega \\to \\mathbb{R}^{2}$に対して次が成立します。 $$ (X, Y)^{-1} (a, b) = \\left\\{ x \\in \\Omega : X(x) = a \\right\\} \\cap \\left\\{ x \\in \\Omega : Y(x) = b \\right\\}. $$ 従って、全てのボレル集合$B_{1}$、$B_{2} \\in \\mathcal{B}_{\\mathbb{R}}$に対して次を得ます。 $$ (X, Y)^{-1}(B_{1} \\times B_{2}) = (X, Y)^{-1}(B_{1}, B_{2}) = X^{-1}(B_{1}) \\cap Y^{-1}(B_{2}). $$ これを任意の$\\mathbb{R}^{n}$に対して拡張すると、 $$ \\begin{equation} \\begin{aligned} (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \u0026amp;= (X_{1}, \\dots, X_{n})^{-1}(B_{1}, \\dots, B_{n}) \\\\ \u0026amp;= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n}). \\end{aligned} \\end{equation} $$\n$X_{1}, \\dots, X_{n}$の結合分布とは確率ベクトル$(X_{1}, \\dots, X_{n})$の確率分布で定義されます： $$ P_{(X_{1}, \\dots, X_{n})} : \\mathcal{B}_{\\mathbb{R}^{n}} \\to \\mathbb{R}, $$ $$ P_{(X_{1}, \\dots, X_{n})}(B_{1} \\times \\cdots \\times B_{n}) := P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right). $$\n独立 $P(E) \\gt 0$の事象$E$に対して、$\\Omega$上の確率 $$ P_{E}(F) = P(E|F) := P(E \\cap F)/P(E) $$ を$E$上の条件付き確率と言います。\nもし$P_{E}(F) = P(F)$なら、$F$を$E$と独立と言います： $$ \\text{$F$ is independent of $E$} \\iff P(E \\cap F) = P(E)P(F). $$ 次が成立する時、$\\Omega$の事象たちのコレクション$\\left\\{ E_{j} \\right\\}$が独立と言います： $$ P(E_{1} \\cap \\cdots \\cap E_{n}) = P(E_{1}) P(E_{2}) \\cdots P(E_{n}) = \\prod \\limits_{i=1}^{n} P(E_{j}). $$\n$\\Omega$上の確率変数たちのコレクション$\\left\\{ X_{j} \\right\\}$が独立ということは、全てのボレル集合$B_{j} \\in \\mathcal{B}_{\\mathbb{R}}$に対して事象たち$\\left\\{ X_{j}^{-1}(B_{j}) \\right\\}$が独立ということを言います。つまり次の式が成立することを意味します： $$ P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) = \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})). $$\n確率分布の定義と$(2)$により、上記式の左辺から次を得ます。 $$ \\begin{align*} P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) \u0026amp;= P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right) \\\\ \u0026amp;= P_{(X_{1}, \\dots, X_{n})} \\left( B_{1} \\times \\cdots \\times B_{n} \\right). \\end{align*} $$ 一方、積測度と確率分布の定義により、右辺から次を得ます。 $$ \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})) = \\prod \\limits_{j=1}^{n} P_{X_{j}}(B_{j}) = \\left( \\prod \\limits_{j=1}^{n} P_{X_{j}} \\right) \\left( B_{1} \\times \\cdots \\times B_{n} \\right). $$ 従って$\\left\\{ X_{j} \\right\\}$が独立なら、 $$ P_{(X_{1}, \\dots, X_{n})} = \\prod\\limits_{j=1}^{n}P_{X_{j}}. $$\n$\\left\\{ X_{j} \\right\\}$が独立な確率変数の集合であることは、$\\left\\{ X_{j} \\right\\}$の結合分布がそれぞれの分布の積と同じであることと同値です。\n参考文献 Robert G. Bartle, The Elements of Integration and Lebesgue Measure (1995) Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (1999) ","id":3473,"permalink":"https://freshrimpsushi.github.io/jp/posts/3473/","tags":null,"title":"측도론과 확률론 요약 정리"},{"categories":"프로그래밍","contents":"개요1 名前のある140以上のCSSカラーパレットです。\n코드 ","id":3459,"permalink":"https://freshrimpsushi.github.io/jp/posts/3459/","tags":null,"title":"CSSカラー名札"},{"categories":"추상대수","contents":"定義 $n \\times n$ 実数の可逆な行列の集合を $\\mathrm{GL}(n, \\mathbb{R})$ または $\\mathrm{GL}_{n}(\\mathbb{R})$ と表記し、$n$次の一般線型群general linear group of degree $n$と呼ぶ。\n$$ \\mathrm{GL}(n, \\mathbb{R}) := \\left\\{ n \\times n \\text{ invertible matrix} \\right\\} = M_{n \\times n}(\\mathbb{R}) \\setminus {\\left\\{ A \\in M_{n \\times n}(\\mathbb{R}) : \\det{A} = 0 \\right\\}} $$\n説明 可逆な行列だけを集めたので、行列の積に関して群になる。また、微分可能な構造を持つため、リー群でもある。\n","id":3450,"permalink":"https://freshrimpsushi.github.io/jp/posts/3450/","tags":null,"title":"一般リニア群"},{"categories":"머신러닝","contents":"概要1 $$ \\includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$\nMNISTmodified national institute of standards and technology データベースとは、アメリカの高校生と人口調査局の職員の数字の手書き文字に関するデータセットを指す。一般に[エムニスト]と呼ばれる。\n公式ホームページ 機械学習/ディープラーニング入門の例としてよく使用されるデータセットである。NISTでは、手書きの郵便番号の自動分類のための文字認識技術の評価のため、以下のような形式で手書きデータを収集した。ここでヤン・ルカンYann LeCunが高校生と人口調査局の職員の手書きデータを取り、前処理を行い、MNISTを作成した。画像のサイズは28 x 28で、60,000枚のトレーニングセットと10,000枚のテストセットで構成されている。\n$$ \\includegraphics[height=30em]{https://www.nist.gov/sites/default/files/styles/960_x_960_limit/public/images/2019/04/27/sd19.jpg?itok=oETq77cZ} $$\n使用方法 Julia Juliaでは、機械学習データセットパッケージであるMLDatasets.jlを使用できる。基本的にはFloat32型のトレーニングセットを読み込む。オプションでこれを変更して読み込むことができる。使用できるメソッドは以下の通り。\ndataset[i]: i番目の特徴量とターゲットのタプルを返す。 dataset[:]: 全ての特徴量とターゲットのタプルを返す。 length(dataset): データの数を返す。 convert2image(dataset, i): i番目のデータをグレースケールの画像に変換する。ImageShow.jlパッケージが必要である。 julia\u0026gt; using MLDatasets\rjulia\u0026gt; train = MNIST()\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :train\rfeatures =\u0026gt; 28×28×60000 Array{Float32, 3}\rtargets =\u0026gt; 60000-element Vector{Int64}\rjulia\u0026gt; test = MNIST(Float64, :test)\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :test\rfeatures =\u0026gt; 28×28×10000 Array{Float64, 3}\rtargets =\u0026gt; 10000-element Vector{Int64}\rjulia\u0026gt; length(train), length(test)\r(60000, 10000)\rjulia\u0026gt; using Plots\rjulia\u0026gt; using ImageShow\rjulia\u0026gt; train.targets[1]\r5\rjulia\u0026gt; heatmap(convert2image(train, 1)) ラベルは整数で与えられるため、ワンホットエンコーディングを別途行う必要がある。\njulia\u0026gt; train.targets[1:5]\r5-element Vector{Int64}:\r5\r0\r4\r1\r9\rjulia\u0026gt; using Flux\rjulia\u0026gt; Flux.onehotbatch(train.targets[1:5], 0:9)\r10×5 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\r⋅ 1 ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ 1 ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ 1 ⋅ ⋅\r1 ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ 1 Julia Fluxでワンホットエンコーディングする方法 Julia FluxでMLPを実装し、MNISTを学習する方法 環境 OS: Windows11 Version: Julia v1.8.2, MLDatasets v0.7.6, Plots v1.36.1, ImageShow v0.3.6, Flux v0.13.7 권건우·허령, 人工知能をマンガと野史で学ぶ 2, p68\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3444,"permalink":"https://freshrimpsushi.github.io/jp/posts/3444/","tags":null,"title":"MNIST Database"},{"categories":"머신러닝","contents":"定義 1 2 入力空間Input Space $X \\ne \\emptyset$ が定義域であり値域が複素数の集合 $\\mathbb{C}$ の写像 $f: X \\to \\mathbb{C}$ で構成される関数空間 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right) \\subset \\mathbb{C}^{X}$ がヒルベルト空間であるとする。\n再生核ヒルベルト空間 固定された一つのデータDatum $x \\in X$ に対して、関数 $f \\in H$ を取り出す汎関数 $\\delta_{x} : H \\to \\mathbb{C}$ を**$x$ における(ディラックの)評価汎関数**(Dirac) Evaluation Functional at $x$という。 $$ \\delta_{x} (f) := f (x) $$ 全ての $x \\in X$ において評価汎関数 $\\delta_{x}$ が連続である場合、$H$ を再生核ヒルベルト空間RKHS, Reproducing Kernel Hilbert Spaceと呼び、$H_{k}$ と表記することもある。 関数 $k : X \\times X \\to \\mathbb{C}$ が以下の二つの条件を満たす場合、$H$ の再生核Reproducing Kernelという。 (i): 表現者Representer: 全ての $x \\in X$ に対して $$ k \\left( \\cdot , x \\right) \\in H $$ (ii) 再生性質Reproducing Property: 全ての $x \\in X$ と全ての $f \\in H$ に対して $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) = \\delta_{x} (f) $$ 特に全ての $x_{1} , x_{2} \\in X$ に対して以下が成立する。 $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; k \\left( \\cdot , x_{2} \\right), k \\left( \\cdot , x_{1} \\right) \\right\u0026gt; $$ 正定値カーネル 入力空間 $X \\ne \\emptyset$ からヒルベルト空間 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ への写像 $\\phi : X \\to H$ を特徴写像Feature Mapと呼ぶ。この文脈では、$H$ を特徴空間Featureと呼ぶこともある。 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ の内積 $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; : H \\times H \\to \\mathbb{C}$ に対して、以下のように定義される関数 $k : X \\times X \\to \\mathbb{C}$ をカーネルKernelと呼ぶ。 $$ k \\left( x_{1} , x_{2} \\right) := \\left\u0026lt; \\phi \\left( x_{1} \\right) , \\phi \\left( x_{2} \\right) \\right\u0026gt; $$ $m$個のデータData $\\left\\{ x_{1} , \\cdots , x_{m} \\right\\} \\subset X$ に対して、以下のような行列 $K \\in \\mathbb{C}^{m \\times m}$ をカーネル $k$ のグラム行列Gram Matrixと呼ぶ。 $$ K := \\left( k \\left( x_{i} , x_{j} \\right) \\right)_{ij} $$ $k$ のグラム行列が正定値行列である場合、$k$ を正定値カーネルPositive Definite Kernelと呼ぶ。言い換えると、全ての $\\left\\{ c_{1} , \\cdots , c_{m} \\right\\} \\subset \\mathbb{C}$ に対して以下を満たすグラム行列を持つカーネル $k$ を正定値カーネルと呼ぶ。 $$ \\sum_{i=1}^{m} \\sum_{j=1}^{m} c_{i} \\bar{c_{j}} K_{ij} \\ge 0 $$ 説明 難しい内容だが、できるだけわかりやすく解説してみよう。\nデータ科学におけるヒルベルト空間の意味 ヒルベルト空間は内積が定義された完備空間である。通常、数学では内積とは何か特別な意味を持たせずに、単にいくつかの条件を満たす二変数スカラー関数として扱うが、機械学習の文脈では類似性の測定Measure of Similarityという概念として考えることができる。実際に、文書間の単語の頻度を比較するために使用されるコサイン類似度も内積を使用しており、別の例として三つのベクトル $$ A := \\left( 3, 0, 1 \\right) \\\\ B := \\left( 4, 1, 0 \\right) \\\\ C := \\left( 0, 2, 5 \\right) $$ がある場合、$A$ と $B$ が類似しており、$C$ とは異なると直感的に理解できる。しかし、これはまだ直感的な推論に過ぎず、内積を通して量化すると以下のようになる。 $$ A \\cdot B = 12 + 0 + 0 = 12 \\\\ A \\cdot C = 0 + 0 + 5 = 5 \\\\ B \\cdot C = 0 + 2 + 0 = 2 $$ 単に内積の絶対値が大きいか小さいかを見ただけでも、\u0026lsquo;見ればわかる\u0026rsquo;よりもはるかにデータをよく説明している。 定義で入力空間と呼んでいる $X$ には特に仮定がないことに注意する。実際のフィールドでは、どのような悪いデータを扱うか保証できない。例えば、$X$ が写真や文書データの場合、写真同士や文書同士を内積することは意味がない。 Q. $X$ が白黒写真の集合である場合、写真を行列と見なしてピクセルごとの値で内積を取れば良いのではないか？ A. それで良く、それが特徴写像 $\\phi : X \\to H$ である。この場合、$H$ は長方形 $[a,b] \\times [c,d]$ で定義された関数の空間となる。 このように考えると、カーネルの存在自体が既に\u0026rsquo;扱いにくいデータ\u0026rsquo;を私たちがよく知っている空間に持ち込むことと同じである。 上で述べた内積の意味が全て無意味であっても、内積空間ならばノルム空間であり距離空間であるため、私たちが常識的に存在すると考えるほとんどの仮定が成立する。内積 $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ によって導かれるノルム $$ \\left\\| f \\right\\| := \\sqrt{ \\left\u0026lt; f , f \\right\u0026gt; } $$ があり、ノルム $\\left\\| f \\right\\|$ によってメトリック $$ d (f,g) = \\left\\| f -g \\right\\| $$ が導かれる。 データ科学の観点からノルムはデータに対する量化そのものである。例えば、白黒写真の全てのピクセルの値を合計した値をノルムとする場合、単にこれだけで写真がどれだけ明るいか暗いかを大まかに評価できる。 データ科学の観点から距離は二つのデータがどれだけ異なるかを教えてくれる。正しいか間違っているか、同じか異なるかを区別することは言うまでもなく重要である。 これらの理由をすべて抜きにしても、数式を展開していくと内積が必要になる場合がある。関連する例をここにすべて書くと非常に散漫になるので省略する。\u0026lsquo;サポートベクターマシン\u0026rsquo;の投稿のカーネルトリックの節を参照。 なぜ関数空間なのか？ これほどまでに難しくなければならないのか？ 数学というものはほとんどの応用で「私たちが探している関数」を見つけることである。\n補間は与えられたデータの間を埋める多項式を見つけることである。 統計的回帰分析はデータを最もよく説明する直線を見つける技術である。その直線は線形関数である。 ディープラーニングはそれをうまくやれないので活性化関数などを投入して非線形関数を近似する技術である。 フーリエ変換は関数を三角関数の線形結合として表す変換である。 これらの例を一つ一つ挙げていくときりがない。再び機械学習に戻って、私たちが関数空間を考える理由は、私たちが探しているものが結局のところ関数だからである。私たちは、その形が明示的Explicitではないかもしれないが、私たちが興味を持っているものを入れるInputと、\n私たちが望む結果を出すReturn関数を求めている。例えば、数字が書かれた写真を入れたときにその数字を返す、個人情報を入れたときにローンを返済できる確率を計算するなどの関数である。このような役に立つ関数が単純であるはずがなく、それらを知っている関数たちの合成のようなものを探したいと思っている。想像してみてほしい。健康診断の結果データ $x$ を受け取り、どれだけ健康かを計算してくれる関数を $f$ とすると、 $$ f( \\cdot ) = \\sum_{k=1}^{m} \\alpha_{k} \\phi_{k} \\left( x_{k} \\right)(\\cdot) $$ のように有限個の $\\phi_{k} (x) (\\cdot)$ を基底Basisとして持つ $f$ を探しているのである。特に $\\phi (x) = k (\\cdot , x)$ に対して、ある $f$ を見つけることができるという命題がまさに表現者定理である。\n表現者定理: 再生核ヒルベルト空間内で学習データに適合したFitted任意の関数は、表現者たちの有限の線形結合で表すことができる。\n要するに、機械学習（特にサポートベクターマシンの文脈）で私たちが見つけたいものが結局は関数であるため、それらが存在する関数空間について探求することは避けられない。\nもちろん、数学的な証明がなければ動かないプログラムはこの世に存在しない。必然的な学問であるとしても、全員に必須というわけではない。数学専攻でなければ、非常に難しいことが普通であり、どうしても難しいと思う場合は大まかに読み飛ばしても良い。\n評価関数の前になぜディラックの名前がついているのか？ $$ \\delta_{x_{0}} (x) = \\begin{cases} 1 \u0026amp; , \\text{if } x = x_{0} \\\\ 0 \u0026amp; , \\text{if } x \\ne x_{0} \\end{cases} $$ 元々ディラックのデルタ関数は上記のように一点でのみ値を持つ関数として知られている。正確な定義や用途はともかく、その変形は一点でのみ$0$でないという点を保持すれば、大抵ディラックの名前がつく。この意味を理解するための例として、二つの関数 $f : \\mathbb{R} \\to \\mathbb{R}$, $\\delta_{x_{0}} : \\mathbb{R} \\to \\mathbb{R}$ とその内積として $$ \\left\u0026lt; f, \\delta_{x_{0}} \\right\u0026gt; = \\sum_{x \\in \\mathbb{R}} f (x) \\delta_{x_{0}} (x) = f \\left( x_{0} \\right) $$ を想像してみる。通常関数の内積には積分を行うが、和ではないことや全ての $x \\in \\mathbb{R}$ に対して加算することが危険であることは理解しているが、最終的には概念と感覚が一致する部分があることがわかる。\nこのセンスで、$\\delta_{x_{0}} (f)$ は上記の議論を隠して単にその結果である $x_{0}$ で評価された $f \\left( x_{0} \\right)$ を、\u0026rsquo;$x_{0}$ において一点だけを得る\u0026rsquo;関数としている。\n再生性質と呼ぶ理由 再生核ヒルベルト空間の定義を読むと非常に興味深い。通常、数学で「何かの空間」と言うと、その定義自体が「何か」が存在する空間としているが、RKHSは突然「評価汎関数が全ての点で連続である」というヒルベルト空間として定義されているためである。\nリース表現定理: $\\left( H, \\left\\langle \\cdot,\\cdot \\right\\rangle \\right)$がヒルベルト空間であるとする。$H$の線形汎関数 $f \\in H^{ \\ast }$と$\\mathbf{x} \\in H$ に対して $f ( \\mathbf{x} ) = \\left\\langle \\mathbf{x} , \\mathbf{w} \\right\\rangle$および$\\| f \\|_{H^{\\ast}} = \\| \\mathbf{w} \\|_{H}$ を満たす$\\mathbf{w} \\in H$ が一意に存在する。\nムーア-アロンサジン定理Moore-Aronsajn Theorem: 正定値カーネルが存在する場合、それに対応するRKHSが一意に存在する。\nこの定義によれば、RKHSに再生核が一意に存在するという命題さえ自明ではなく、実際にはリース表現定理によってRKHSに再生核が一意に存在することが保証される。興味深いことに、逆に再生核\nに対応するRKHSも一意に存在する。\nこれで、定義にある数式を一つ一つ詳しく見てみよう。\n元々$k : X \\times X \\to \\mathbb{C}$ において、関数$k$に入れることができるのは$x_{1}, x_{2} \\in X$だが、定義で述べたように$x$を一つ固定すると、$k$は実質的に$k : y \\mapsto k (y,x)$となる$k : X \\to \\mathbb{C}$となる。関数として扱う立場からは、片方の入力を塞いだものであり、 $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) $$ のような表現は、単に二つの関数$f (\\cdot) : X \\to \\mathbb{C}$と$k \\left( \\cdot , x \\right): X \\to \\mathbb{C}$を内積したものに過ぎない。「それがどうして$f$から出てきて、外にある内積が$x$とどう関連しているのか\u0026hellip;」と複雑に考える必要はない。$f(x) \\in \\mathbb{C}$も単に内積の結果であり、値域が複素数集合であるため、出てきた何らかの複素数に過ぎない。\nここで、再生再生, Reproducingという性質の命名について触れておきたい。Reproductionという単語自体が、その生成原理に従ってRe-(再び, 再) -produce(作る, 生)という意味を持ち、その最初の翻訳は繁殖/生殖、二番目の翻訳はコピー/複製、三番目の翻訳は再生である。繁殖は明らかに無意味であり、コピーと言うには元がない。\nしかし、$f(\\cdot)$と$k (\\cdot, x)$を内積したときに$f(x)$を得るということを、$f$が持っていた情報をカーネルによって「再生」したものと考えたらどうだろうか？私たちが時刻$t$に依存するYouTubeの動画$y(t)$という関数を持っていると想像してみよう。私たちは$y$そのものを見るのではなく、$t$が増加するにつれて再生される$\\left\\{ y(t) : t \\in [0,T] \\right\\}$自体を見ている。このような比喩から、カーネル$k$は$f$を関数そのものとしてではなく、関数値を再生してくれる「再生カーネル」と呼ばれる資格がある。\n特徴マップと不便な表記について カーネルと再生カーネルの定義をよく見ると、実際にはこれらは定義のために相互に必要としていないことがわかる。カーネルはカーネルであり、再生カーネルは再生カーネルであり、これらが一致するのは特徴マップFeature Mapが表現者Representerであるとき、つまり $$ \\phi (x) = k \\left( \\cdot , x \\right) $$ のときである。特徴マップはその名の通り、元のデータを私たちが扱いやすい形に変換してくれる変換であり、このような関数たちによって何らかの関数が表されるということは、その関数がデータから来る何らかの特徴Featureによって説明されるということと同じである。一つの問題は、ここまで直感的に何となく理解できたとしても、依然として$k \\left( \\cdot , x \\right)$のような表記が不便であり、特徴マップではなく内積から始まって別々に定義されるカーネルの動機Motiveに共感するのが難しいということである。\n特徴マップは$\\phi : X \\to H$であるため、その関数値は$x \\in X$に対応する何らかの関数$\\lambda : X \\to \\mathbb{C}$であり、これが通常混乱を招くものではない。$\\phi(x)$をもっと正確に書くと $$ \\left( \\phi (x) \\right) (\\cdot) = k \\left( \\cdot , x \\right) $$ であり、なぜこのようにしてまで点$\\cdot$を保持し、不便な表記を使用するのか疑問に思うかもしれない。ほとんどの人間は、このようにしなかった場合にもっと苦労する例を見ると、理解しやすくなる。前述したように、カーネルであれ再生カーネルであれ、結局私たちが一貫して関心を持っている空間は関数空間$H$であり、$H$の内積は関数の内積である。まず、ある関数$f$がデータ$\\left\\{ x_{i} \\right\\}_{i=1}^{m}$の表現者$\\phi \\left( x_{i} \\right)$たちの線形結合として表されるとすると $$ f (y) = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) (y) = \\sum_{i=1}^{m} \\alpha_{i} \\left( \\phi \\left( x_{i} \\right) \\right) (y) $$ となり、すでにかなり複\n雑になっていることがわかる。これに新しい関数$g$とデータ$\\left\\{ x'_{j} \\right\\}_{j=1}^{n}$を考えると $$ g (y) = \\sum_{j=1}^{n} \\beta_{j} \\left( \\phi \\left( x'_{j} \\right) \\right) (y) $$ となる。一方で、$f$と$g$の内積を使わないのであれば、内積空間を考える理由がないが、$\\left\u0026lt; f,g \\right\u0026gt;$を書くと $$ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} \\left\u0026lt; \\phi \\left( x_{i} \\right) , \\phi \\left( x'_{j} \\right) \\right\u0026gt; $$ となり、余計なものが多くなる。内積をする前には、いずれにせよ関数空間を扱う上で$y \\in X$を実際に扱うことはほとんどなく、内積をした後には、既に知っている$\\phi$と内積を続けて書く必要がある。これを見ると、 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) \\\\ g (\\cdot) = \\sum_{j=1}^{n} \\beta_{j} k \\left( \\cdot , x'_{j} \\right) \\\\ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} k \\left( x_{i} , x'_{j} \\right) $$ のような表記が面倒であるだけではないことに気がつくかもしれない。\n再生カーネルは正定値である データ$\\left\\{ x_{k} \\right\\}_{k=1}^{m}$が与えられたとすると、$k$がカーネルである場合、以下が成立する。 $$ \\begin{align*} \u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\bar{\\alpha_{i}} \\alpha_{j} k \\left( x_{i} , x_{j} \\right) \\\\ =\u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\left\u0026lt; \\alpha_{i} \\phi \\left( x_{i} \\right) , \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\u0026lt; \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) , \\sum_{j=1}^{m} \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) \\right\\|^{2} \\\\ \\ge \u0026amp; 0 \\end{align*} $$ 前述したように$\\phi : x \\mapsto k (\\cdot , x)$とすると、再生カーネル$k$はカーネルであるため正定値である。このようなカーネルの正定値性は、カーネルに関連するさまざまな性質で自然に現れる。\n関数解析以外のカーネル (1) 通常、数学でカーネルと言えば抽象代数のカーネル$\\ker$を指す。代数構造$Y$で$0$が定義されている場合、関数$f : X \\to Y$に対して$\\ker f := f^{-1} \\left( \\left\\{ 0 \\right\\} \\right)$を$f$のカーネルと言う。 (2) この概念が線形代数で特殊化されたものが線形変換のカーネルである。 カーネルが難しいと感じる場合、関数解析を専攻していない数学者にいきなりカーネルについて聞いても、十中八九(1)の意味で理解するだろう。あなたのバックグラウンドが数学に基づいているならば、当然(1)くらいは知っている必要があり、そうでなくても(2)くらいは知っているべきである。\n名前のついたカーネル 機械学習の文脈では、以下のようなカーネルが知られている。3 これらは一見カーネルのように見えないかもしれないが、カーネルの和と積が依然としてカーネルであるという事実を通じて導かれる。\nリニアカーネル: $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; $$ ポリノミアルカーネル: $c \\ge 0$ と $d \\in \\mathbb{N}$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\left( \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + c \\right) ^{d} $$ ガウシアンカーネル: $\\sigma^{2} \u0026gt; 0$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\exp \\left( - {{ \\left\\| x_{1} - x_{2} \\right\\| } \\over { 2 \\sigma^{2} }} \\right) $$ シグモイドカーネル: $w, b \\in \\mathbb{C}$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\tanh \\left( w \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + b \\right) $$ Sejdinovic, Gretton. (2014). What is an RKHS?: p7~11. http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchölkopf. (2001). A Generalized Representer Theorem. https://link.springer.com/chapter/10.1007/3-540-44581-1_27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJakkula. (2006). Tutorial on Support Vector Machine (SVM). https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2406,"permalink":"https://freshrimpsushi.github.io/jp/posts/2406/","tags":null,"title":"機械学習における政府号カーネルと再生カーネルのヒルベルト空間"},{"categories":"위상데이터분석","contents":"概要 代数位相Algebraic Topologyにおいて、幾何学的な意味を考えずに単に定義だけを述べると、ベッチ数Betti Numberとは、単にチェインコンプレックスでのホモロジーグループのランクに過ぎない。問題は、このような説明がベッチ数の意味を知りたい人にとって全く役に立たず、その具体的な計算も難解であり、例を通して学ぶことも困難であることである。\nこの投稿では、少なくとも2つ目の質問に対する答え―ベッチ数をどのように計算するかについての整理とその詳細な証明を紹介する。以下に紹介される定理によれば、与えられたチェインコンプレックスに従ってある行列を見つけることができ、それに関する一連の計算プロセスを通じて、以下のような明示的Explicitな公式を導出することができる。 $$ \\beta_{p} = \\rank ?_{1} - \\rank ?_{2} $$\n本来、数学的な内容は数学を使わずに伝えられることが最も良い説明であるが、ベッチ数の場合は、その公式の導出過程の中でその根本的な原理を理解することができると考えられる。学部生程度では証明の難易度がかなり高く、追いかけるのが難しいかもしれないが、できるだけ省略せずに詳細に書いたので、少なくとも一度は試みることをお勧めする。\n定理 ホモロジーグループの定義:\n$n \\in \\mathbb{N}_{0}$ とする。アーベル群 $C_{n}$ と ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェイン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ これがすべての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、$\\mathcal{C} := \\left\\{ \\left( C_{n}, \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ をチェインコンプレックスChain Complexという。 商群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の**$n$番目のホモロジーグループ**$n$-th Homology Groupという。 ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界Boundaryまたは微分Differentialオペレーターという。 $Z_{n} := \\ker \\partial_{n}$ の要素を**$n$-サイクル**Cycles、$B_{n} := \\text{Im} \\partial_{n+1}$ の要素を**$n$-境界**Boundaryという。 フリーチェインコンプレックスの標準基底分解 チェインコンプレックス $\\mathcal{C} := \\left\\{ \\left( C_{p}, \\partial_{p} \\right) \\right\\}$ のすべての $C_{p}$ が有限ランクのフリーグループであるとする。するとすべての $p$ と $Z_{p} := \\ker \\partial_{p}$ に対して、次を満たす部分群 $U_{p}, V_{p}, W_{p} \\subset C_{p}$ と が存在する。 $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ =\u0026amp; U_{p} \\oplus Z_{p} \\end{align*} $$ $$ \\begin{align*} \\partial_{p} \\left( U_{p} \\right) \\subset \u0026amp; W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$ もちろん、$Z_{p}$ は $\\partial_{p}$ の核であるため、$\\partial_{p} \\left( V_{p} \\right) = 0$ であり、$\\partial_{p} \\left( W_{p} \\right) = 0$ である。さらに、$U_{p}$ での $\\partial_{p}$ の制限関数 ${\\partial_{p}}_{| U_{p}} : U_{p} \\to W_{p-1}$ は、次のような形のスミス標準形を持つ。 $$ \\begin{bmatrix} b_{1} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; \\cdots \u0026amp; b_{l} \\end{bmatrix} $$ ここで、$b_{i} \\in \\mathbb{N}$ であり、$b_{1} \\mid \\cdots \\mid b_{l}$ である。\nホモロジーグループの効率的な計算可能性 1 $H_{p} \\left( \\mathcal{C} \\right)$ のベッチ数を$\\mathcal{C}$ の$p$番目のベッチ数Betti Numberという。有限コンプレックス$K$ の$\\beta_{p}$ は次のようである。 $$ \\beta_{p} = \\rank Z_{p} - \\rank B_{p} $$ その具体的な値は、次のように$\\partial_{p}$ のスミス標準形によって計算することができる。図では、青い点線が$1$ の対角成分を、オレンジの実線が$1$ でない対角成分を示し、その他のすべての成分は$0$ である。2\nここで重要なのは、スミス標準形における$1$ の数$\\rank B_{p-1}$ と、ゼロベクトルの列の数$\\rank Z_{p}$ である。\n証明 3 Part 1. $B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$\n$$ \\begin{align*} Z_{p} :=\u0026amp; \\ker \\partial_{p} \\\\ B_{p} :=\u0026amp; \\text{Im} \\partial_{p+1} \\\\ W_{p} :=\u0026amp; \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\} \\end{align*} $$ と置く。特に、$W_{p}$ は$C_{p}$ の部分群となり、$\\lambda = 1$ のみを考えた場合に$B_{p} = W_{p}$ であるという点で、境界Boundary$B_{p}$ の条件を弱めたものと見なすことができるため、弱い境界Weak Boundariesと呼ばれる。\n$W_{p}$ の定義から、$\\lambda \\ne 1$ を考えると $$ B_{p} \\subset W_{p} $$ $Z_{p}$ の定義から、$\\forall z_{p} \\in Z_{p}$ は$\\partial_{p} z_{p} = 0$ であり、$Z_{p} = \\ker \\partial_{p}$ は$\\partial_{p} : C_{p} \\to C_{p-1}$ であるため $$ Z_{p} \\subset C_{p} $$ $C_{p}$ はフリーグループと仮定されているため、トーションフリー、すなわち$\\forall z_{p} \\in Z_{p} \\subset C_{p}$ に対して$\\lambda z_{p} = 0$ を満たす$\\lambda \\ne 0$ が存在しない。一方、すべての$c_{p+1} \\in C_{p+1}$ に対して $$ \\partial_{p+1} c_{p+1} = \\lambda z_{p} \\in W_{p} $$ の両辺に$\\partial_{p}$ を適用すると $$ 0 = \\partial_{p} \\partial_{p+1} c_{p+1} = \\partial_{p} \\lambda z_{p} = \\lambda \\partial_{p} z_{p} $$ であるため、$\\partial_{p} z_{p} = 0$ でなければならない。これは、$\\lambda z_{p} \\in W_{p}$ ならば$\\lambda z_{p} \\in Z_{p}$ であることを意味するため $$ W_{p} \\subset Z_{p} $$ このような考察から、次の包含関係を得る。 $$ B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p} $$\nPart 2. $W_{p} \\subset Z_{p}$ は$Z_{p}$ の直和群Direct Summandである\n$p$番目のホモロジーグループ$H_{p} \\left( \\mathcal{C} \\right) = Z_{p} / B_{p}$ の定義から $$ \\text{proj}_{1} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) $$ は剰余類$B_{p}$ に相当するだけのランクが下がった射影であり $H_{p} \\left( \\mathcal{C} \\right)$ のトーション部分群$T_{p} \\left( \\mathcal{C} \\right) \\subset H_{p} \\left( \\mathcal{C} \\right)$ に対して $$ \\text{proj}_{2} : H_{p} \\left( \\mathcal{C} \\right) \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ も射影である。 第1同型定理: 準同型写像$\\phi : G \\to G'$ が存在する場合 $$G / \\ker ( \\phi ) \\simeq \\phi (G)$$\nこれにより、$\\text{proj} := \\text{proj}_{1} \\circ \\text{proj}_{2}$ として定義された $$ \\text{proj} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ も射影である。$W_{p}$ の要素は$\\partial_{p+1} d_{p+1}$ のように表されるため、この射影$\\text{proj}$ の核は$W_{p}$ であり、すべての射影は全射Surjectionであるため、第1同型定理により $$ Z_{p} / W_{p} \\simeq H_{p} / T_{p} $$ が成立する。ここで、右辺の$H_{p}$ がどのようになっているかにかかわらず、トーション部分群$T_{p}$ で取り除いたため、トーションフリーであり、これにより、左辺の$Z_{p} / W_{p}$ もトーションフリーであることが保証される。したがって、$\\alpha_{1} , \\cdots , \\alpha_{k}$ が$Z_{p} / W_{p}$ の基底であり、$\\alpha'_{1} , \\cdots , \\alpha'_{l} \\in W_{p}$ が$W_{p}$ の基底であるとした場合、$\\alpha_{1} , \\cdots , \\alpha_{k}, \\alpha'_{1} , \\cdots , \\alpha'_{l}$ は$Z_{p}$ の基底となる。したがって、$Z_{p}$ は $$ Z_{p} = V_{p} \\oplus W_{p} $$ のように、$\\alpha_{1} , \\cdots , \\alpha_{k}$ を基底とする部分群$V_{p}$ と$W_{p}$ の直和として表現できる。\nPart 3. $Z_{p}, B_{p-1}, W_{p-1}$ の基底\nホモモルフィズムのスミス標準形: フリーアーベル群$G$ と$G'$ のランクがそれぞれ$n,m$ であり、$f : G \\to G'$ がホモモルフィズムである場合、次のような行列を持つホモモルフィズム$g$ が存在する。 $$ \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\in \\mathbb{Z}^{m \\times n} $$ ここで、$d_{1} , \\cdots, d_{r} \\in \\mathbb{N}$ であり、$d_{1} \\mid \\cdots \\mid d_{r}$、つまり$d_{k}$ は$d_{k+1}$ の約数Divisorである必要がある。\n$\\partial_{p} : C_{p} \\to C_{p-1}$ は、次のようなスミス標準形の$m \\times n$ 行列を持つ。\n$$ \\begin{matrix} \u0026amp; \\begin{matrix} e_{1} \u0026amp; \\cdots \u0026amp; e_{l} \u0026amp; e_{l} \u0026amp; \\cdots \u0026amp; e_{n} \\end{matrix} \\\\ \\begin{matrix} e'_{1} \\\\ \\vdots \\\\ e'_{l} \\\\ e'_{l} \\\\ \\vdots \\\\ e'_{m} \\end{matrix} \u0026amp; \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\end{matrix} $$\nこれにより、我々は直接的な計算を通じて次の3つを示すことになる:\n(1): $e_{l+1} , \\cdots , e_{n}$ は$Z_{p}$ の基底である。 (2): $b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ は$B_{p-1}$ の基底である。 (3): $e'_{1} , \\cdots , e'_{l}$ は$W_{p-1}$ の基底である。 補題\n$\\partial_{p}$ の定義により、一般的な$c_{p} \\in C_{p}$ に対して次が成立する。 $$ c_{p} = \\sum_{i=1}^{n} a_{i} e_{i} \\implies \\partial_{p} c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ (1): $b_{i} \\ne 0$ であるため、$Z_{p} = \\ker \\partial_{p}$ である必要十分条件は、$i = 1 \\cdots , l$ に対して$a_{i} = 0$ であることである。したがって、$e_{l+1} , \\cdots , e_{n}$ は$Z_{p}$ の基底である。 (2): すべての$\\partial_{p} c_{p} \\in B_{p-1}$ は$b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ の線形結合として表現され、$b_{i} \\ne 0$ であるため、$b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ は$B_{p-1}$ の基底である。 (3): $b_{i} e'_{i} = \\partial e_{i}$ であるため、まず$e'_{1}, \\cdots, e'_{l} \\in W_{p-1}$ である。逆に、$c_{p-1} \\in C_{p-1}$ を $$ c_{p-1} = \\sum_{i=1}^{m} d_{i} e'_{i} $$ と置き、$c_{p-1} \\in W_{p-1}$ と仮定すると、$W_{p-1}$ が$W_{p-1} = \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\}$ のように定義されていたため、$c_{p-1}$ はある$\\lambda \\ne 0$ に対して $$ \\lambda c_{p-1} = \\partial c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ の形で表現できる。係数を比較すると、$i \u0026gt; l$ に対して $$ \\lambda d_{i} = 0 \\implies d_{i} = 0 $$ を得る。したがって、$e'_{1} , \\cdots , e'_{l}$ は$W_{p-1}$ の基底である。 Part 4. \u0026lsquo;フリーチェインコンプレックスの標準基底分解\u0026rsquo;の証明\n$C_{p}$ と$C_{p-1}$ に対して、これまでの議論で登場する$e_{1} , \\cdots , e_{l}$ によって生成されるフリーグループを$U_{p}$ とすると、$Z_{p} = V_{p} \\oplus W_{p}$ であるため、$\\partial V_{p} = \\partial W_{p} = 0$ であり $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus Z_{p} \\\\ =\u0026amp; U_{p} \\oplus \\left( V_{p} \\oplus W_{p} \\right) \\end{align*} $$ を得る。ここで、$W_{p}$ と$Z_{p}$ は$C_{p}$ により一意であるが、$U_{p}$ と$V_{p}$ は必ずしも一意である必要はないことに注意されたい。\nPart 5. \u0026lsquo;ホモロジーグループの効率的な計算可能性\u0026rsquo;の証明\nPart 4により、コンプレックス$K$ に対して、次の分解が存在することが保証される。 $$ \\begin{align*} C_{p} \\left( K \\right) =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$\n直和の性質: $G = G_{1} \\oplus G_{2}$ としよう。もし$H_{1}$ が$G_{1}$ の部分群であり、$H_{2}$ が$G_{2}$ の部分群である場合、$H_{1}$ と$H_{2}$ も直和として表現でき、特に次が成立する。 $${{ G } \\over { H_{1} \\oplus H_{2} }} \\simeq {{ G_{1} } \\over { H_{1} }} \\oplus {{ G_{2} } \\over { H_{2} }}$$\n[1]: $H_{1} \\simeq G_{1}$ であり、$H_{2} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ G / G_{1} \\simeq G_{2} $$ [2]: $H_{1} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ {{ G } \\over { H_{2} }} \\simeq G_{1} \\oplus {{ G_{2} } \\over { H_{2} }}$$ Part 1で$B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$ であったため、直和の性質により $$ \\begin{align*} H_{p} \\left( K \\right) =\u0026amp; Z_{p} / B_{p} \\\\ =\u0026amp; \\left( {{ V_{p} \\oplus W_{p} } \\over { B_{p} }} \\right) \\\\ =\u0026amp; V_{p} \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [2] \\\\ =\u0026amp; \\left( {{ Z_{p} } \\over { W_{p} }} \\right) \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [1] \\end{align*} $$ を得る。ここで、$H_{p} \\left( K \\right) = \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right)$ の\n$Z_{p} / W_{p}$ はフリーパートであり $W_{p} / B_{p}$ はトーションパートである。 これにより、$K$ の$p$番目のベッチ数$\\beta_{p}$ は、次のように求められる。 $$ \\begin{align*} \\beta_{p} =\u0026amp; \\rank H_{p} \\left( K \\right) \\\\ =\u0026amp; \\rank \\left[ \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right) \\right] \\\\ =\u0026amp; \\rank \\left( Z_{p} / W_{p} \\right) + \\rank \\left( W_{p} / B_{p} \\right) \\\\ =\u0026amp; \\left[ \\rank Z_{p} - \\rank W_{p} \\right] + \\left[ \\rank W_{p} - \\rank B_{p} \\right] \\\\ =\u0026amp; \\rank Z_{p} - \\rank B_{p} \\end{align*} $$\n一方、$H_{p-1}(K)$ のトーションパートと$b_{1} | \\cdots | b_{l} \\in \\mathbb{N}$ に対しては、次のようなアイソモルフィズムが存在することが分かる。 $$ W_{p-1} / B_{p-1} \\simeq \\left( {{ \\mathbb{Z} } \\over { b_{1} \\mathbb{Z} }} \\right) \\oplus \\cdots \\oplus \\left( {{ \\mathbb{Z} } \\over { b_{l} \\mathbb{Z} }} \\right) $$ ここで、$i \\le l$ に対して$b_{i} = 1$ であること、つまり$B_{p-1}$ のランクが$l$ であることは $$ \\mathbb{Z} / b_{i} \\mathbb{Z} = \\mathbb{Z} / \\mathbb{Z} = \\left\\{ 0 \\right\\} $$ であるため、$W_{p-1}$ のランクが$l$ 分だけ減少することを覚えておく。\n■\n例 トーラス $$ \\begin{align*} \\beta_{0} =\u0026amp; 1 \\\\ \\beta_{1} =\u0026amp; 2 \\\\ \\beta_{2} =\u0026amp; 1 \\end{align*} $$\nトーラスのベッチ数は上記のように知られている。このトーラスのチェインコンプレックスが上の図のように定義されている場合、例として$\\beta_{1} = 2$ のみを計算してみよう。上で導出された公式を使用せずに単に数学的に考えて計算する方法もあるが、読めば分かる通り、頭が痛くなるほど難しい。これと対照的に、「ホモロジーを効率的に計算する」ということがどれほど便利かを見てみよう。\nホモモルフィズムのスミス標準形: フリーアーベルグループ$G$ と$G'$ に対して、$a_{1} , \\cdots , a_{n}$ が$G$ の基底であり、$a_{1}' , \\cdots , a_{m}'$ が$G'$ の基底であるとする。もし関数$f : G \\to G'$ がホモモルフィズムであれば、次を満たす唯一の整数の集合$\\left\\{ \\lambda_{ij} \\right\\} \\subset \\mathbb{Z}$ が存在する。 $$ f \\left( a_{j} \\right) = \\sum_{i=1}^{m} \\lambda_{ij} a_{i}' $$ この時行列$\\left( \\lambda_{ij} \\right) \\in \\mathbb{Z}^{m \\times n}$ を($G$ と$G'$ の基底に関する)$f$ の行列という。\n$\\beta_{1} = \\rank Z_{1} - \\rank B_{1}$ であるため、少なくとも境界行列$\\left( \\partial_{1} \\right)$ と$\\left( \\partial_{2} \\right)$ を求める必要がある。すべての$a , b, c \\in C_{1} (T)$ に対して $$ \\begin{align*} \\partial_{1} (a) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (b) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (c) =\u0026amp; v - v = 0 = 0v \\end{align*} $$ であるため $$ \\left( \\partial_{1} \\right) = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\implies Z_{1} = 3 , B_{0} = 0 $$ を得る。$Z_{p}$ は行列の右側のゼロベクターの数であり、$B_{p-1}$ は行列内の$1$ の数である。次に、$\\partial_{2}$ を考えると $$ \\begin{align*} \\partial_{2} (U) =\u0026amp; -a -b +c \\\\ \\partial_{2} (L) =\u0026amp; a + b - c \\end{align*} $$ であるため $$ \\left( \\partial_{2} \\right) = \\begin{bmatrix} -1 \u0026amp; 1 \\\\ -1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{bmatrix} \\sim \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} \\implies Z_{2} = 1 , B_{1} = 1 $$ を得る。これを総合すると、トーラスの$1$番目のベッチ数$\\beta_{1}$ は、次のように計算される。 $$ \\beta_{1} = \\rank Z_{1} - \\rank B_{1} = 3 - 1 = 2 $$ 当然ながら、この結果は、この投稿に紹介された定理に従って、フリーグループがどうであり、アイソモルフィズムがどうであるかといった、あらゆる数学的知識を駆使して得た値と一致することが保証されている。少し大胆に言えば、頭を使わずに指示された通りに計算すれば、ベッチ数、つまり「ホモロジー」を「計算」することができると要約できるだろう。もう少し良い言い方をすると、コンピュータを通じて位相数学を研究する道が開かれたということだ。\nMunkres. (1984). Elements of Algebraic Topology: p58.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p104.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (1984). Elements of Algebraic Topology: p58~61.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2399,"permalink":"https://freshrimpsushi.github.io/jp/posts/2399/","tags":null,"title":"ホモロジーグループのベッチ数"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義 集合 $\\left{ 0, 1 \\right}$ の元を ビットbitと呼ぶ。集合 $\\left{ 0, 1 \\right}^{n}$ の元を $n$ビット$n$bitと呼ぶ。\n説明 ビットは binary digitの略称である。通常、「$0$または$1$の値を取りうるもの」と説明される。古典コンピュータが処理する最小の情報単位であり、コンピュータの回路では$1$は電気信号があることを、$0$は電気信号がないことを意味する。\n量子コンピュータで処理される情報の最小単位は、ビットにクォンタムを付けて quantum bit量子ビットと呼ばれる。\n関連項目 ブール関数 量子ビット ","id":3422,"permalink":"https://freshrimpsushi.github.io/jp/posts/3422/","tags":null,"title":"ビット: 古典的なコンピュータにおける情報の基本単位"},{"categories":"위상데이터분석","contents":"ビルドアップ 難しい内容ですが、できるだけ理解しやすいように、すべての計算と説明を省略せずに丁寧に残しました。ホモロジーに興味がある方は、ぜひお読みください。\n実際に、私たちが興味を持っている位相空間 $X$ があり、これが特定のシンプリシャルコンプレックスに従って$\\Delta$-コンプレックス構造を通して表現されるとしましょう。小さな例として、上の図では右側のトーラスが $X$ であり、左側がシンプリシャルコンプレックスに相当します。\nシンプレックスの定義:\nアフィン独立な $v_{0}, v_{1} , \\cdots , v_{n} \\in \\mathbb{R}^{n+1}$ の凸包を**$n$-シンプレックス** $\\Delta^{n}$ と呼び、ベクトル $v_{k}$ を頂点と呼びます。数式的には以下のようになります。 $$ \\Delta^{n} := \\left\\{ \\sum_{k} t_{k} v_{k} : v_{k} \\in \\mathbb{R}^{n+1} , t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$ $\\Delta^{n}$ から一つの頂点が除かれて作られる $n-1$-シンプレックス $\\Delta^{n-1}$ を $\\Delta^{n}$ の面と呼びます。$\\Delta^{n}$ のすべての面の和集合を $\\Delta^{n}$ の境界と呼び、$\\partial \\Delta^{n}$ と表します。 シンプレックスの内部 $\\left( \\Delta^{n} \\right)^{\\circ} := \\Delta^{n} \\setminus \\partial \\Delta^{n}$ をオープンシンプレックスと呼びます。 ここで、シンプリシャルコンプレックスとはシンプレックスで構成されるコンプレックスで、具体的には以下のようなCWコンプレックスで構成されているとしましょう。\n$n$-セルの定義:\n以下のように定義された $D^{n} \\subset \\mathbb{R}^{n}$ を $n$-ユニットディスクと呼びます。 $$ D^{n} := \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{n} : \\left\\| \\mathbf{x} \\right\\| \\le 1 \\right\\} $$ $D^{n} \\setminus \\partial D^{n}$ とホモトピー同値な開集合 $e^{n}$ を $n$-セルとも呼びます。 CWコンプレックスの定義:\n離散的な集合 $X^{0} \\ne \\emptyset$ を**$0$-セル**とみなします。 $n$-スケルトン $X^{n}$ は $X^{n-1}$ に$n$-セル $e_{\\alpha}^{n}$ を $\\phi_{\\alpha} : S^{n-1} \\to X^{n-1}$ で結合することによって作られます。 $X := \\bigcup_{n \\in \\mathbb{N}} X^{n}$ が弱位相を持つ位相空間になるとき、$X$ をセルコンプレックスと呼びます。 定義 1 $\\Delta$-コンプレックス構造を持つ位相空間 $X$ が与えられているとしましょう。\n$X$ のオープン $n$-シンプレックスである$n$-セル $e_{\\alpha}^{n}$ を基底を持つ自由アーベル群 $\\Delta_{n} (X)$ と表しましょう。$\\Delta_{n} (X)$ の要素を**$n$-チェインと呼び、係数 $k_{\\alpha} \\in \\mathbb{Z}$ に対して以下のような形式的和で表します。 $$ \\sum_{\\alpha} k_{\\alpha} e_{\\alpha}^{n} $$ 一方、CWコンプレックスの定義から、各 $n$-セル $e_{\\alpha}^{n}$ にはそれに対応する特性写像** $\\sigma_{\\alpha} : \\Delta^{n} \\to X$ が存在するため、単に次のように表すこともあります。 $$ \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} $$ 次のように定義される準同型 $\\partial_{n} : \\Delta_{n} (X) \\to \\Delta_{n-1} (X)$ を境界準同型と呼びます。ここで、$\\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right]$ は、$\\sigma_{\\alpha}$ の $X$ の $n-1$-シンプレックス に対する制限関数であることを意味します。 $$ \\partial _{n} \\left( \\sigma_{\\alpha} \\right) := \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right] $$ 3. 商群 $\\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $H_{n}^{\\Delta}$ と表し、$H_{n}^{\\Delta}$ はホモロジーグループであるため、$X$ の第 $n$ シンプリシャルホモロジーグループと呼びます。\n群 $0$ は $\\left\\{ 0 \\right\\}$ で定義されたマグマです。つまり、空の代数構造です。 準同型 $\\partial^{2} = 0$ はゼロ準同型です。 $\\text{Im}$ は像です。 $\\ker$ はカーネルです。 集合でハット表記 $\\hat{v}_{i}$ は、次のように $v_{i}$ だけを除くことを意味します。 $$ \\left\\{ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right\\} := \\left\\{ v_{1} , \\cdots , v_{n} \\right\\} \\setminus \\left\\{ v_{i} \\right\\} $$ 説明 定義に文字が多いので、理解する前に目に入りにくいのは普通です。血となり肉となる説明なので、丁寧に読むようにしましょう。個人的に勉強している間に苦労した部分をできるだけわかりやすく書くように努めました。\n$\\Delta_{n} (X)$ の要素をなぜチェーンと呼ぶのか？ $$ \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} $$ のような記法で $\\sigma_{\\alpha} : \\Delta^{n} \\to X$ を考えることで、これで $e_{\\alpha}^{n}$ が $\\Delta^{n}$ の要素なのか $X$ の要素なのかといったことはあまり考える必要がなくなりました。$n=2$ で全ての係数が $k_{\\alpha} = 1$ の場合、幾何学的に想像できる例として、以下の図の右側のような図形 $\\sum_{i=1}^{7} \\sigma_{i}$ を考えてみましょう。\nここで鎖という表現が理解できれば幸いですが、そうでなくても実際にはあまり関係ありません。とにかく後で重要なのは、それぞれの $n$-チェイン $\\Delta_{n} (X)$ でチェーンコンプレックスを構築することです。\n$\\Delta_{n} (X)$ は本当にグループなのか？ 非常に重要ですが、定義でチェインを説明するときに、形式的和という表現を使いました。これは $\\Delta_{n} (X)$ の要素を説明したに過ぎず、$\\Delta_{n} (X)$ 上で定義された二項演算ではありません。形式的和という言葉が示すように、これはあくまで形式的なものです。小学校の時に使っていた記法を借りてくれば、\n2😀 + 💎 - 3🍌\rのように、とりあえずその位置を絵などで埋めたものと考えても問題ありません。上の式は数学的には意味がありません。なぜなら、笑顔 😀 の2倍が何であり、そこに宝石 💎 を加えることが何であり、バナナ 🍌 を3つ引くことが何なのか、定義されておらず、定義するのも困難だからです。これらを扱うのが難しい状況は、正確に $\\sum_{\\alpha} k_{\\alpha} e_{\\alpha} \\simeq \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ で\n(そもそも加算を定義できない)オープンシンプレックス $e_{\\alpha}^{n}$ 対応する $\\sigma_{\\alpha}$ が関数である（関数そのものなのか関数値を指しているのかがわかりにくい） それを任意の整数倍して加算した $-3 e_{1}^{n} + 7 e_{2}^{n} \\simeq -3 \\sigma_{1} + 7 \\sigma_{2}$ の意味がわからない という問題と同じです。代数的構造どころか、この集合がどのように見えるのかすらわかりにくいですが、幸いにもこれらの問題は $\\Delta_{n} (X)$ にとっては関係がありません。もし\n$\\sigma=$2😀 + 💎 - 3🍌\rが $\\Delta_{n} (X)$ の要素、つまり $n$-チェインであるとするならば、これらの要素の逆元は、すべての係数 $k_{\\alpha} \\in \\left( \\mathbb{Z} , + \\right)$ の逆元 $-k_{\\alpha} \\in \\left( \\mathbb{Z} , + \\right)$ を係数として持つ\n$-\\sigma=$ (-2)😀 + (-1)💎 + (-(-3))🍌\rで定義するだけで十分です。これにより $\\Delta_{n} (X)$ の単位元は、任意の $\\sigma \\in \\Delta_{n} (X)$ に対して $0 := \\sigma + (-\\sigma)$ で定義され、$\\mathbb{Z}$ がアーベル群であるため、$\\Delta_{n} (X)$ もアーベル群になります。ここで、群 $\\left( \\Delta_{n} (X) , + \\right)$ の演算 $+$ は $\\left( \\mathbb{Z} , + \\right)$ の $+$ から導かれたものですが、同じものではありません。$n$-チェイン $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} \\in \\Delta_{n} (X)$ で登場する $\\sum$ とも異なります。\n要約すると以下のようになります。\n最初に定義したときの $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ で加算のように見えるものは、そもそも演算ではなく記法に過ぎませんでした。 $\\left( \\Delta_{n} (X) , + \\right)$ の $+$ は $\\left( \\mathbb{Z} , + \\right)$ の $+$ から導かれましたが、同じものではありません。 $\\left( \\Delta_{n} (X) , + \\right)$ は自由アーベル群であり、これで $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ も二項演算 $+$ の関数値になります。 $\\partial$ をなぜ境界と呼ぶのか？ $$ \\partial _{n} \\left( \\sigma_{\\alpha} \\right) := \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right] $$\n定義にある数式だけを見ても理解しにくいですが、以下の図を見ればすぐに理解できるでしょう。\n例えば $\\partial_{2}$ を考えると、次のような計算を行うことができます。 $$ \\begin{align*} \u0026amp; \\partial _{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\\\ =\u0026amp; \\sum_{i=0}^{2} (-1)^{i} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\setminus \\left[ v_{i} \\right] \\\\ =\u0026amp; (-1)^{0} \\left[ v_{1}, v_{2} \\right] + (-1)^{1} \\left[ v_{0}, v_{2} \\right] + (-1)^{2} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\end{align*} $$\nホモロジーグループを学ぶレベルなら、三角形 $\\left[ v_{0} ,v_{1}, v_{2} \\right]$ の境界が $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$ で構成されること自体を受け入れられない人はほとんどいないでしょう。本当に理解しにくいのは、一体 $\\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right]$ が何なのかということです。1-シンプレックスである線分同士を引くことが意味を成すのでしょうか？それをベクトルとして扱い、2-シンプレックスである三角形同士の演算はどうなるのでしょうか？\nすべて間違っています。しっかりと頭を整理してもう一度見てみましょう。$\\partial_{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\in \\Delta_{1} (X)$ は、その幾何学的な意味を離れて、単に3つの要素 $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$ の形式的和である $$ (+1) \\left[ v_{1}, v_{2} \\right] + (-1) \\left[ v_{0}, v_{2} \\right] + (+1) \\left[ v_{0}, v_{1} \\right] $$\nに過ぎません。これを順番に $$ \\begin{align*} a := \\left[ v_{1}, v_{2} \\right] \\ b:= \\left[ v_{0}, v_{2} \\right] \\ c:= \\left[ v_{0} , v_{1} \\right] \\end{align*} $$ と置くと、$\\Delta_{1} (X)$ の正体がようやく見えてきます。例えば、$1$-チェイン $x \\in \\Delta_{1} (X)$ は、ある係数 $k_{a} , k_{b} , k_{c} \\in \\mathbb{Z}$ に対して $$ x = k_{a} a + k_{b} b + k_{c} c $$ のように表される要素です。逆に $a,b,c$ の立場から自由群 $\\Delta_{1} (X) := F[\\left\\{ a,b,c \\right\\}]$ を構築する過程を考えると、$\\Delta_{1} (X)$ とは、3つの未知数で作られる群、つまり $\\mathbb{Z}^{3} \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} \\oplus \\mathbb{Z}$ と同型な群に過ぎないことがわかります。\nこのような考え方の転換は、続く例を理解する上で必須です。幾何を置いて、代数的に考えましょう。\n例 $$ \\begin{align*} \\\\ \\partial_{n} :\u0026amp; \\Delta_{n} (X) \\to \\Delta_{n-1} (X) \\\\ H_{n}^{\\Delta} (X) =\u0026amp; \\ker \\partial_{n} / \\text{Im} \\partial_{n+1} \\end{align*} $$\n特に $n = 0$ の場合、$\\partial_{0} : \\Delta_{0} \\left( X \\right) \\to 0$ なので $\\ker \\partial_{0} = \\Delta_{0} \\left( X \\right)$ です。\n円 $S^{1}$ $1$-ユニットスフィア、つまり円 $X = S^{1}$を考えると、$0$-シンプレックスは頂点 $v$ 一つ、$1$-シンプレックスはエッジ $e$ 一つ、$n \\ge 2$ で $n$-シンプレックスは存在しないので、チェーンコンプレックス自体は以下のように構成されるでしょう。\n$$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{1}\\left( S^{1} \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( S^{1} \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\n自由群 $\\Delta_{1}\\left( S^{1} \\right)$ は $e$ 一つで生成されるので $\\Delta_{1}\\left( S^{1} \\right) \\simeq \\mathbb{Z}$ であり、$\\Delta_{0}\\left( S^{1} \\right)$ も $v$ 一つで生成されるので $\\Delta_{0}\\left( S^{1} \\right) \\simeq \\mathbb{Z}$ です。一方 $$ \\partial e = v - v = 0 $$ なので $\\partial_{1}$ はゼロ準同型です。\n$n = 0$ の場合、$\\ker \\partial_{0} = \\Delta_{0} \\left( S^{1} \\right)$ であり、$\\partial_{1}$ がゼロ準同型なのでその像は $\\left\\{ 0 \\right\\}$ となり、以下が得られます。 $$ \\begin{align*} H_{0}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n = 1$ の場合、$\\partial_{2}$ の定義域が $0$ なので $\\text{Im} \\partial_{2} = \\left\\{ 0 \\right\\}$ であり、$\\partial_{1}$ がゼロ準同型なので $\\ker \\partial_{1}$ はその定義域である $\\Delta_{1} \\left( S^{1} \\right)$ 自体となり、以下が得られます。 $$ \\begin{align*} H_{1}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{1} / \\text{Im} \\partial_{2} \\\\ \\simeq\u0026amp; \\Delta_{1} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n \\ge 2$ に対しては、$H_{n}^{\\Delta} \\left( S_{1} \\right) \\simeq 0$ なので、以下のように要約できます。 $$ H_{n}^{\\Delta} \\left( S_{1} \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0, 1 \\\\ 0 \u0026amp; , \\text{if } n \\ge 2 \\end{cases} $$\nトーラス $T^{2}$ 上の図のようなトーラス $T^{2}$を考えると、$0$-シンプレックスは頂点 $v$ 一つ、$1$-シンプレックスはエッジ $a$、$b$、$c$ 三つ、$2$-シンプレックスは $U$、$L$ 二つ、$n \\ge 3$ で $n$-シンプレックスは存在しないので、チェーンコンプレックス自体は以下のように構成されるでしょう。\n$$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{2}\\left( T \\right) \\overset{\\partial_{2}}{\\longrightarrow} \\Delta_{1}\\left( T \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( T \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\nこれにより、自由群 $\\Delta_{n} \\left( T \\right)$ は $$ \\Delta_{n} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z}^{1} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z}^{3} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z}^{2} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\nとなります。一方、エッジ $a$、$b$、$c$ の両端点は $v$ に接続されているので $$ \\begin{align*} \\partial a =\u0026amp; v - v = 0 \\\\ \\partial b =\u0026amp; v - v = 0 \\\\ \\partial c =\u0026amp; v - v = 0 \\end{align*} $$ であり、円の場合と同様に $\\partial_{1}$ はゼロ準同型です。\n$n = 0$ の場合、円の場合と同様に以下が成立します。 $$ \\begin{align*} H_{0}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( T \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n = 1$ の場合、$\\partial_{1}$ がゼロ準同型なので $\\ker \\partial_{1}$ はその定義域である $\\Delta_{1} \\left( T \\right)$ 自体です。一方で $\\partial_{2} : \\Delta_{2}\\left( T \\right) \\to \\Delta_{1}\\left( T \\right)$ について $$ \\partial_{2} U = a + b - c = \\partial_{2} L $$ であり、$\\left\\{ a, b, a + b - c \\right\\}$ は $\\Delta_{1}\\left( T \\right)$ の基底なので $H_{1}^{\\Delta}$ は$a$ と $b$ で生成される自由群と同型です。つまり、以下が成立します。 $$ H_{1}^{\\Delta} \\left( T \\right) \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} $$\n$n = 2$ の場合、$\\partial_{3}$ の定義域が $0$ なので $\\text{Im} \\partial_{3} = \\left\\{ 0 \\right\\}$ であり、$\\partial_{2} : \\Delta_{2}\\left( T \\right) \\to \\Delta_{1}\\left( T \\right)$ では $\\Delta_{2}\\left( T \\right) \\simeq \\mathbb{Z}^{2}$ で $\\Delta_{1}\\left( T \\right) \\simeq \\mathbb{Z}^{3}$ なので $\\ker \\partial_{2} \\simeq \\mathbb{Z}^{3-2}$ です。これを整理すると、以下が得られます。 $$ \\begin{align*} H_{2}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{2} / \\text{Im} \\partial_{3} \\\\ \\simeq\u0026amp; \\mathbb{Z}^{3-2} / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n \\ge 3$ に対しては、$H_{n}^{\\Delta} \\left( T \\right) \\simeq 0$ なので、以下のように要約できます。 $$ H_{n}^{\\Delta} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z} \\oplus \\mathbb{Z} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\n定理 $H_{n}^{\\Delta}$ はホモロジーグループである ホモロジーグループの定義:\n$n \\in \\mathbb{N}_{0}$ とします。アーベル群 $C_{n}$ と準同型 $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェーン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ がすべての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、$\\mathcal{C} := \\left\\{ \\left( C_{n}, \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ をチェーンコンプレックスと呼びます。 商群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の第 $n$ ホモロジーグループと呼びます。 準同型 $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界または微分オペレータと呼びます。 $$ \\cdots \\longrightarrow \\Delta_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} \\Delta_{n} \\overset{\\partial_{n}}{\\longrightarrow} \\Delta_{n-1} \\longrightarrow \\cdots $$\nチェーンコンプレックス $\\left\\{ \\left( \\Delta_{n} (X) , \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ に対して $H_{n}^{\\Delta} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ はホモロジーグループです。つまり、すべての $n \\in \\mathbb{N}$ に対して $\\partial_{n} \\circ \\partial_{n+1}$ はゼロ準同型です。\n証明 $\\sigma \\in \\Delta_{n}$ に $\\partial_{n-1} \\circ \\partial_{n}$ を適用してみると、以下が得られます。 $$ \\begin{align*} \u0026amp; \\left( \\partial_{n-1} \\circ \\partial_{n} \\right) \\left( \\sigma \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\partial_{n} \\left( \\sigma \\right) \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , v_{n} \\right] \\right) \\\\ =\u0026amp; \\sum_{j \u0026lt; i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ \u0026amp; + \\left( -1 \\right) \\sum_{j \u0026gt;i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\n実際、このような証明は、一般的に証明するよりも、帰納的な例を示すことがより役立ちます。 $$ \\begin{align*} \u0026amp; \\partial_{1} \\left( \\partial_{2} \\left[ v_{0}, v_{1} , v_{2} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left( \\left[ v_{1} , v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left[ v_{1} , v_{2} \\right] - \\partial_{1} \\left[ v_{0}, v_{2} \\right] + \\partial_{1} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{2} \\right] - \\left[ v_{1} \\right] - \\left( \\left[ v_{2} \\right] - \\left[ v_{0} \\right] \\right) + \\left[ v_{1} \\right] - \\left[ v_{0} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\n■\nHatcher. (2002). Algebraic Topology: p104~106.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2383,"permalink":"https://freshrimpsushi.github.io/jp/posts/2383/","tags":null,"title":"シンプリシアルホモロジーグループの定義"},{"categories":"머신러닝","contents":"概要 レファレンスと数式の番号や表記法は、論文をそのまま踏襲する。 Physics-informed neural networks (PINN[ピン]と読む)は、数値的に解くために設計された微分方程式の人工ニューラルネットワークであり、2018年Journal of Computational Physicsに発表された論文Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equationsで紹介されました。この論文の著者は、応用数学、機械工学のM. Raissi, P. Perdikaris, G.E. Karniadakisです。\nこの論文で述べられている物理情報physics informationは、壮大に見えるかもしれませんが、実際には与えられた微分方程式自体を意味すると考えても良いでしょう。つまり、\u0026lsquo;微分方程式を人工ニューラルネットワークで解く際、与えられた微分方程式を利用します\u0026rsquo;と言っているのと同じです。機械学習の論文を読むときは、このように見栄えの良い名前に惑わされないよう注意が必要です。\n微分方程式の数値的解法においてPINNが注目される理由は、損失関数に関するアイデアがシンプルで理解しやすく、実装も簡単だからでしょう。実際に論文の例では非常にシンプルなDNNが紹介されています。\n一般的に言われるPINNはSection 3.1で紹介されるモデルを指します。\n0. 抄録 著者はPINNを\u0026rsquo;与えられた非線形偏微分方程式を満たしながら、教師あり学習問題を解くために訓練された人工ニューラルネットワーク\u0026rsquo;と紹介しています。この論文で主に扱う2つの問題は、\u0026lsquo;data-driven solution and data-driven discovery of partial differential equations\u0026rsquo;です。性能評価のために、流体力学、量子力学、拡散方程式などの問題を解いてみました。\n1. 序論 最近の機械学習とデータ分析の進歩は、画像認識image recognition、認知科学cognitive science、ゲノム学genomicsなどの科学分野で革新的な結果をもたらしていますが、複雑な物理的、生物学的、工学的システムに対しては（データ収集コストが高いため）少ない情報で望ましい結果を導き出す必要がある困難があります。このような*小さなデータ領域small data regime*では、DNN、CNN、RNNなどの先進技術の収束性が保証されていません。\n[4-6]で、データ効率が良く（=少ないデータで）、物理情報を学習できる（=微分方程式を解くことができる）方法についての研\n究が進んでいます。非線形問題への拡張は、この論文の著者であるRaissiの後続研究[8,9]で提案されました。\n2. 問題設定 人工ニューラルネットワークで表される関数は、入力値（偏微分方程式でのソリューション$u$の座標$x, t$を指す）とパラメータによって関数値が決定されるが、これら2種類の変数に対して微分を行うために自動微分automatic differentiationを活用する。\nこのようなニューラルネットワークは、観測されたデータを支配する物理法則に起因する任意の対称性、不変性、または保存原理を尊重するように制約されている。これは、一般的な時間依存かつ非線形の偏微分方程式によってモデル化される。\nこの論文でこの文章が難しいと感じられるかもしれないが、私の考えでは簡単に言えば提案された人工ニューラルネットワークであるPINNが、与えられた微分方程式を満たす必要があるということだ。後述するが、微分方程式を満たす必要があるという条件を損失関数として使用するためである。\nこの論文の目的は、数理物理学におけるディープラーニングを進化させる新しいパラダイムのモデリングと計算パラダイムを提示することである。そのために、前述したように、この論文では主に2つの問題を扱う。一つは偏微分方程式のデータ駆動ソリューションdata-driven solutionであり、もう一つは偏微分方程式のデータ駆動発見data-driven discoveryである。使用された全てのコードとデータセットはhttps://github.com/maziarraissi/PINNsで確認できる。この論文では、$L1$、$L2$、ドロップアウトなどの正則化なしに、ハイパーボリックタンジェントを活性化関数として用いたシンプルなMLPが使用されている。各例では、ニューラルネットワークの構造、オプティマイザー、学習率などが具体的に紹介される。\nこの論文では、以下のようなパラメータ化された非線形偏微分方程式の一般的な形parameterized and nonlinear partial differential equations of the general formを扱う。\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u; \\lambda] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nここで、$u=u(t,x)$は(1)を満たす隠れた(=与えられていない=知られていない)関数、つまり(1)のソリューションであり、$\\mathcal{N}[\\cdot; \\lambda]$は$\\lambda$でパラメータ化された非線形演算子（NonlinearのNに由来する）であり、$\\Omega \\subset \\mathbb{R}^{D}$である。多くの数理物理学の問題problems in mathematical physicsは上記のような形で表される。例えば、1次\n元粘性バーガース方程式を見てみよう。\n$$ u_{t} + uu_{x} = \\nu u_{xx} $$\nこれは(1)で$\\mathcal{N}[u; \\lambda] = \\lambda_{1} uu_{x} - \\lambda_{2}u_{xx}$、$\\lambda = (\\lambda_{1}, \\lambda_{2})$の場合である。与えられた方程式(1)に対して、扱うべき2つの問題はそれぞれ以下の通りである。\n偏微分方程式のデータ駆動ソリューション: 固定された$\\lambda$に対して、システムのソリューション$u(t,x)$は何か？ 偏微分方程式のデータ駆動発見: 観測されたデータを最もよく表現するパラメータ$\\lambda$は何か？ 3. 偏微分方程式のデータ駆動ソリューション セクション3では、以下の形式の偏微分方程式からデータに基づいたソリューションを見つける問題について扱う。\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nつまり、$(1)$でパラメータ $\\lambda$ が固定されている状況である。セクション3.1とセクション3.2ではそれぞれ連続時間モデルと離散時間モデルを扱う。方程式を見つける問題はセクション4で扱う。ここで言う\u0026rsquo;データ\u0026rsquo;の意味は以下で詳しく説明する。\n3.1. 連続時間モデル $(t,x) \\in \\mathbb{R} \\times \\mathbb{R}$ とすると、$u : \\mathbb{R}^{2} \\to \\mathbb{R}$ である。これを人工ニューラルネットワークで近似するが、次のように実装されるシンプルなMLPを使用する。Juliaでは、\nusing Flux\ru = Chain(\rDense(2, 10, relu),\rDense(10, 10, relu),\rDense(10, 1)\r) PyTorchでは、\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rlayers = [2, 10, 10, 1]\rclass network(nn.Module):\rdef __init__(self):\rsuper(network, self).__init__()\rlayer_list = [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\rself.linears = nn.ModuleList(layer_list)\rdef forward(self, tx):\ru = tx\rfor i in range(len(layers)-2):\ru = self.linears[i](u)\ru = F.relu(u)\ru = self.linears[-1](u)\rreturn u\ru = network() これで $u$ は入力ノードが $2$ つ、出力ノードが $1$ つの人工ニューラルネットワークとなる。$(2)$ の左辺を次のような関数 $f = f(t,x; u)$ として定義しよう。\n$$ \\begin{equation} f := u_{t} + \\mathcal{N}[u] \\end{equation} $$\nここで $u$ は人工ニューラルネットワークであるため、$f$ も隠れ層のパラメータを持つ一種の人工ニューラルネットワークである。上記のような $f$ を物理情報に基づいたニューラルネットワークphysics-informed neural network, PINNと呼ぶ。言い換えれば、与えられた偏微分方程式そのものである。$f$ に含まれる微分は自動微分で実装され、$u$ と同じパラメータを共有する。人工ニューラルネットワーク $u$ が $(2)$ のソリューションを適切に近似していれば、$f$ の関数値はどこでも $0$ であるべきだ。ここから、$ f \\to 0$ になるように人工ニューラルネットワークを学習させることが推測できる。\n$(t_{u}^{i}, x_{u}^{i})$ を初期値、境界値が定義された領域の点とする。\n$$ (t_{u}^{i}, x_{u}^{i}) \\in( \\Omega \\times \\left\\{ 0 \\right\\}) \\cup (\\partial \\Omega \\times [0, T]) $$\n$u_{\\ast}$ を実際のソリューションとすると、初期条件と境界条件が与えられたということは、次のような値が与えられたということと同じである。\n$$ \\left\\{ t_{u}^{i}, x_{u}^{i}, u^{i} \\right\\}_{i=1}^{N_{u}},\\quad u^{i} = u_{\\ast}(t_{u}^{i}, x_{u}^{i}) $$\n理論上はこれらの値を無限に持つことになるが、数値的な問題では有限の点のみを扱えるので、$N_{u}$ 個を持っているとする。人工ニューラルネットワーク $u$ は $(t_{u}^{i}, x_{u}^{i})$ を入力として受け取り、$u^{i}$ を出力する必要があるので、これらがそれぞれ入力と対応するラベルとなる。\n$$ \\text{input} = (t_{u}^{i}, x_{u}^{i}),\\qquad \\text{label} = u^{i} $$\nこれがPINNで学習する\u0026lsquo;データ\u0026rsquo;である。それでは、損失関数を次のように設定できる。\n$$ MSE_{u} = \\dfrac{1}{N_{u}} \\sum\\limits_{i=1}^{N_{u}} \\left| u(t_{u}^{i},x_{u}^{i}) - u^{i} \\right|^{2} $$\nまた、$f$は適切な点集合（理論的には解 $u_{\\ast}$ が定義されるすべての点で満たされるべきだが、数値的には有限の点しか扱えない）$\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$で$(2)$を満たさなければならない。これらの適切な点を論文ではコロケーションポイントcollocation pointsと呼ぶ。コロケーションポイントに対して以下の損失関数を設定する。\n$$ MSE_{f} = \\dfrac{1}{N_{f}}\\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} $$\nつまり、$MSE_{f}$が$0$に近づくことは、物理的情報（偏微分方程式）を満たすことを意味する。したがって、人工ニューラルネットワーク $u$ を訓練するための最終的な損失関数は以下の通りである。\n$$ MSE = MSE_{u} + MSE_{f} $$\n論文では、$MSE_{f}$を使用することで物理的情報を制約として設けることは、[15, 16]で初めて研究されたが、PINN論文ではこれを現代的な計算ツールで検討し、より困難なダイナミックシステムに適用したと説明されている。\n物理情報に基づく機械学習physics-informed machine learningという用語自体は、Wangの乱流モデリングturbulence modelingに関する研究[17]で初めて使用されたとされる。しかし、PINN以前の研究では、サポートベクターマシン、ランダムフォレスト、FNNなどの機械学習アルゴリズムが単に使用されていたと説明されている。PINNがこれらと区別される点は、一般的に機械学習に使用されるパラメータに対する微分だけでなく、解の座標 $x, t$ に関する微分も考慮している点である。つまり、パラメータ $w$ を持つ人工ニューラルネットワークで近似された解を $u(t,x; w)$ とするとき、以前に提案された方法は偏微分 $u_{w}$ のみを利用したが、PINNは $u_{t}$ や $u_{x}$ などを利用して解を求める。このようなアプローチにより、少量のデータでも解をうまく見つけることができると説明されている。\nこの手続きがグローバル最小値に収束するという理論的な保証はないにもかかわらず、与えられた偏微分方程式が適切に定義されており、その解が一意であり、十分に表現力のあるニューラルネットワークアーキテクチャと十分な数のコロケーションポイント $N_{f}$ が与えられている場合、我々の方法は良好な\n予測精度good prediction accuracyを達成することが経験的に確認されていると論文には述べられている。\n3.1.1. 例（シュレーディンガー方程式） この例では、周期的な境界条件と複素数値を取る解に対して、提案された方法がうまく機能するかを重点的に確認する。例として、以下の初期条件と境界条件が与えられるシュレーディンガー方程式を扱う。\n$$ \\begin{align*} ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2}h \u0026amp;= 0,\\quad x\\in [-5, 5], t\\in[0, \\pi/2], \\\\ h(0,x) \u0026amp;= 2\\operatorname{sech} (x), \\\\ h(t,-5) \u0026amp;= h(t,5), \\\\ h_{x}(t,-5) \u0026amp;= h_{x}(t,5) \\end{align*} $$\n問題の解 $h_{\\ast}(t,x)$ は $h_{\\ast} : [0, \\pi/2] \\times [-5, 5] \\to \\mathbb{C}$ として複素関数値を持つ。しかし、関数の出力が複素数になるように人工ニューラルネットワークを定義するのではなく、実部を担当する $u(t,x)$ と虚部を担当する $v(t,x)$ の2次元ベクトルが出力されるように定義する。簡単に言えば、入力と出力のノードがそれぞれ2つのMLPとして定義することである。\n$$ h(t,x) = \\begin{bmatrix} u(t,x) \\\\[0.5em] v(t,x) \\end{bmatrix} $$\nこの問題におけるPINN $f$ は以下の通りである。\n$$ f := ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2} h $$\n$h(t,x)$ と $f(t,x)$ のパラメータは、初期値に対する損失 $MSE_{0}$、境界値に対する損失 $MSE_{b}$、物理情報に対する損失 $MSE_{f}$ を最小化するように学習される。\n$$ MSE = MSE_{0} + MSE_{b} + MSE_{f} $$\n$$ \\begin{align*} \\text{where } MSE_{0} \u0026amp;= \\dfrac{1}{N_{0}}\\sum_{i=1}^{N_{0}} \\left| h(0, x_{0}^{i}) - h_{0}^{i} \\right|^{2} \\quad (h_{0}^{i} = 2\\operatorname{sech} (x_{0}^{i})) \\\\ MSE_{b} \u0026amp;= \\dfrac{1}{N_{b}}\\sum_{i=1}^{N_{b}} \\left( \\left| h(t_{b}^{i}, -5) - h(t_{b}^{i}, 5) \\right|^{2} + \\left| h_{x}(t_{b}^{i},-5) - h_{x}(t_{b}^{i},5) \\right|^{2} \\right) \\\\ MSE_{f} \u0026amp;= \\dfrac{1}{N_{f}} \\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} \\end{align*} $$\n論文には $MSE_{b}$ の式にタイプミスがあるので注意すること。 ここで、$\\left\\{ x_{0}^{i}, h_{0}^{i} \\right\\}_{i=1}^{N_{0}}$ は初期値データ、$\\left\\{ t_{b}^{i} \\right\\}_{i=1}^{N_{b}}$ は境界でのコロケーションポイント、$\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$ は $f$ に対するコロケーションポイントである。\nデータセットを生成するために、従来のスペクトルメソッドspectral methodsを使用した。$h(0,x)$ での初期値データの数は $N_{0} = 50$、境界値データの数は $N_{b} = 50$ とし、ランダムに選んだ。また、$f$ のコロケーションポイントの数は $N_{f} = 20,000$ である。人工ニューラルネットワークは、100個のノードを持つ線形層を5層、層間の活性化関数としてハイパーボリックタンジェント $\\tanh$ を使用して構築した。\nFigure 1.\nFigure 1では、上の図は予測された解 $\\left| h(t, x) \\right|$ のヒートマップを示している。下の図は、時間がそれぞれ $t = 0.59, 0.79, 0.98$ のときの予測された解と実際の解がどれだけ一致しているかを示している。相対的 $L_{2}$ ノルムrelative $L_{2}$-norm は $0.00197 = 1.97 \\cdot 10^{-3}$ で、予測された解が正確な解と比較して約 $0.02%$ の差異があることを意味している。したがって、PINNは少ない初期値データでシュレーディンガー方程式の非線形挙動を正確に捉えることができる。\n現在扱っている連続時間モデルは、初期値が少なくてもうまく機能するが、コロケーションポイントの数 $N_{f}$ が十分に多くなければならないという潜在的な制約がある。これは空間の次元が2以下の場合はあまり問題にならないが、高次元の場合、必要なコロケーションポイントの数が指数関数的に増加する可能性があるため、問題になる可能性がある。そのため、次のセクションでは、多くのコロケーションポイントを必要としないようにするために、古典的なルンゲ・クッタ時間ステップスキームを活用した、より構造化されたニューラルネットワークを提案する。\n3.2. 離散時間モデル セクション3.1では、解を連続時間に対して近似した。この場合、人工ニューラルネットワークは全体の領域に対して同時に学習され、任意の点 $(x,t)$ に対して出力がある。このセクションでは、セクション3.1とは異なり、離散時間について扱う。つまり $t_{n}$ の値を知っているとき、$t_{n+1}$ の値を人工ニューラルネットワークで近似する方法について説明する。$(2)$ に $q$ ステージのルンゲ・クッタ法を適用すると、以下のようになる。\n$$ u(t_{n+1}, x) = u(t_{n}, x) - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u(t_{n}+c_{j} \\Delta t, x) \\right] $$\nここで $u^{n}(x) = u(t_{n}, x)$, $u^{n+c_{j}} = u(t_{n} + c_{j}\\Delta t, x)$ と表記すると、\n$$ \\begin{equation} \\begin{aligned} u^{n+1} \u0026amp;= u^{n} - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] \\\\ \\text{where } u^{n+c_{j}} \u0026amp;= u^{n} - \\Delta t \\sum_{i=1}^{q} a_{j,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] \\quad j=1,\\dots,q \\end{aligned}\\tag{7} \\end{equation} $$\n上記の $q+1$ 個の式で、右辺の $\\sum$ 項を全て左辺に移行する。そして、左辺を $u_{i}^{n}$ のように表記する。\n$$ \\begin{equation} \\begin{aligned} u_{q+1}^{n} \u0026amp;:= u^{n+1} + \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] = u^{n} \\\\ \\\\ u_{1}^{n} \u0026amp;:= u^{n+c_{1}} + \\Delta t \\sum_{i=1}^{q} a_{1,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ u_{2}^{n} \u0026amp;:= u^{n+c_{2}} + \\Delta t \\sum_{i=1}^{q} a_{2,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ \u0026amp;\\vdots \\\\ u_{q}^{n} \u0026amp;:= u^{n+c_{q}} + \\Delta t \\sum_{i=1}^{q} a_{q,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\end{aligned}\\tag{9} \\end{equation} $$\nすると、これらの全ての値が $u^{n}$ に等しくなることがわかる。\n$$ u^{n} = u_{1}^{n} = u_{2}^{n} = \\cdots = u_{q+1}^{n} \\tag{8} $$\nしたがって、セクション3.2で言及されている物理情報とは、与えられた初期条件・境界条件と $(8)$ を指す。今度は $u(t_{n+1}, x)$ を求めるために二つの人工ニューラルネットワークを定義する。セクション3.1で使用した人工ニューラルネットワークは、正確な解 $u_{\\ast}$ に収束することを期待する $u$ と、$u$ が満たすべき微分方程式 $f$ だったが、ここでは少し異なる。まず、人工ニューラルネットワーク $U$ を次の関数として定義する。\n$$ U : \\mathbb{R} \\to \\mathbb{R}^{q+1} $$\nつまり、入力層のノードが $1$ つ、出力層のノードが $q+1$ つのニューラルネットワークである。このニューラルネットワークの出力を以下の値とする。\n$$ U(x) = \\begin{bmatrix} u^{n+c_{1}}(x) \\\\[0.5em] u^{n+c_{2}}(x) \\\\ \\vdots \\\\[0.5em] u^{n+c_{q}}(x) \\\\[0.5em] u^{n+1}(x) \\end{bmatrix} \\tag{10} $$\nこのニューラルネットワークは、添付されたコード内の PhysicsInformedNN クラスで定義されている neural_net に該当する。\nつまり、以下の学習プロセスで $U$ の出力の最後の成分が $u(t_{n+1}, x)$ に収束することを期待している。二番目のニューラルネットワークは、$U$ の出力と $(7)$ の定義を利用して、次のように定義される関数である。\n3.2.1. 例（アレン・カーン方程式） 離散時間モデルにおける例として、以下の初期条件と周期的な境界条件が与えられるアレン・カーン方程式を取り上げる。\n$$ \\begin{equation} \\begin{aligned} \u0026amp;u_{t} - 0.0001u_{xx} + 5 u^{3} - 5u = 0,\\qquad x\\in [-1, 1], t\\in[0, 1], \\\\ \u0026amp;u(0,x) = x^{2} \\cos (\\pi x), \\\\ \u0026amp;u(t,-1) = u(t,1), \\\\ \u0026amp;u_{x}(t,-1) = u_{x}(t,1) \\end{aligned}\\tag{12} \\end{equation} $$\nこの例における $(9)$ に含まれる非線形演算子は以下の通りである。\n$$ \\mathcal{N}[u^{n+c_{j}}] = -0.0001u_{xx}^{n+c_{j}} + 5(u^{n+c_{j}})^{3} - 5u^{n+c_{j}} $$\nタイムステップ $t^{n}$ での $u$ の値を $u^{n,i}$ と表記する。\n$$ u^{n,i} = u^{n}(x^{n,i}) = u(t^{n}, x^{n,i}),\\qquad i=1,\\dots,N_{n} $$\n問題は $u^{n}$ が与えられたときに $u^{n+1}$ を計算することであり、$\\left\\{ x^{n,i}, u^{n,i} \\right\\}_{i=1}^{N_{n}}$ は与えられたデータセットである。$(8)$ により、このデータセットに対して以下が成り立つ。\n$$ u^{n,i} = u_{1}^{n}(x^{n,i}) = \\cdots = u_{q+1}^{n}(x^{n,i}) $$\nしたがって、以下のような二乗誤差の合計sum of squared error (SSE)を損失関数とする。\nここではなぜ $MSE$ ではなく $SSE$ を使用するのかは明確ではない。連続時間モデルでは $MSE$ を使用していたが、離散時間モデルでは $SSE$ を使用しており、何らかの実験的な理由があると思われる。 $$ SSE_{n} = \\sum\\limits_{j=1}^{q+1} \\sum\\limits_{i=1}^{N_{n}} \\left| u_{j}^{n} (x^{n,i}) - u^{n,i} \\right|^{2} $$\n各 $u_{j}^{n}$ は $(9)$ によって計算され、この計算にはニューラルネットワーク $U$ の出力が使用される。このロスは、添付されたコード内の PhysicsInformedNN クラスで定義されている net_U0 に対応する。そして $U$ の出力は $(12)$ の境界条件を満たさなければならないため、以下のような損失関数を設定する。\n$$ \\begin{align*} SSE_{b} \u0026amp;= \\sum\\limits_{i=1}^{q} \\left| u^{n+c_{i}}(-1) - u^{n+c_{i}}(1) \\right|^{2} + \\left| u^{n+1}(-1) - u^{n+1}(1) \\right|^{2} \\\\ \u0026amp;\\quad+ \\sum\\limits_{i=1}^{q} \\left| u_{x}^{n+c_{i}}(-1) - u_{x}^{n+c_{i}}(1) \\right|^{2} + \\left| u_{x}^{n+1}(-1) - u_{x}^{n+1}(1) \\right|^{2} \\ \\end{align*} $$\nこれらの合計が最終的なロスである。\n$$ SSE = SSE_{n} + SSE_{b} $$\nFigure 2.\nFig. 2では、上の図が正確な解のヒートマップを示している。下の図では、$t=0.1$ での $u$ を知っているときに $t=0.9$ での値を予測した結果を示している。下の左側の図では、青い線が正確な解であり、$\\color{red}\\mathsf{X}$ がデータとして使用された点を示している。下の右側の図では、青い線が正確な解であり、赤い線が予測された解である。\n暗黙のルンゲ・クッタ法 (IRK) では $u^{n+c_{j}}$ を計算するためにすべての $j$ に対する連立方程式を解く必要があるため、$q$ が大きくなると計算コストが大幅に増加するが、この論文で提案されている方法では $q$ が大きくなってもそれに伴う追加コストは非常に少ないと説明されている。また、$q$ が小さい場合、IRK ではタイムステップ $\\Delta t$ が大きいと正確な予測ができないが、PINN の場合は $\\Delta t$ が大きくても正確に予測できると説明されている。\n4. 偏微分方程式のデータ駆動発見 この章では、観測データがある場合に、偏微分方程式 $(1)$ のパラメータ $\\lambda$ を見つける問題について扱う。詳細は以下の例を通じて説明する。\n4.1. 連続時間モデル $f$ を以下のように $(1)$ の左辺として定義しよう。\n$$ f = u_{t} + \\mathcal{N}[u; \\lambda] $$\nセクション3の $(3)$ と異なる点は、$\\lambda$ が固定された定数ではなく、学習が必要な未知のパラメータになったことである。\n4.1.1. 例（ナヴィエ–ストークス方程式） セクション4.1.1では、非圧縮性流体の実際のデータに関する例として、ナヴィエ–ストークス方程式によって表されるケースを紹介する。以下の2次元ナヴィエ–ストークス方程式を考える。\n$$ \\begin{equation} \\begin{aligned} u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) \u0026amp;= -p_{x} + \\lambda_{2}(u_{xx} + u_{yy}) \\\\ v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) \u0026amp;= -p_{y} + \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned} \\tag{15} \\end{equation} $$\nここで、$u(t,x,y)$ は流体の速度ベクトルの $x$ 成分、$v(t,x,y)$ は $y$ 成分である。また、$p(t,x,y)$ は圧力、$\\lambda = (\\lambda_{1}, \\lambda_{2})$ は未知のパラメータである。ナヴィエ–ストークス方程式の解は発散が $0$ となる条件を満たすため、以下が成立する。\n$$ \\begin{equation} u_{x} + v_{y} = 0 \\tag{17} \\end{equation} $$\nある潜在関数 $\\psi(t, x, y)$ に対して、以下のように仮定する。\n$$ u = \\psi_{y},\\quad v = -\\psi_{x} $$\nつまり、流体の速度ベクトルを $\\begin{bmatrix} \\psi_{y} \u0026amp; -\\psi_{x}\\end{bmatrix}$ とすると、$u_{x} + v_{y} = \\psi_{yx} - \\psi_{xy} = 0$ であるため、自然に $(17)$ を満たす。$u$ と $v$ を個別に求めるのではなく、$\\psi$ を人工ニューラルネットワークで近似し、その偏微分によって $u, v$ を得る。実際の速度ベクトル場に対して、以下のように測定された情報があるとする。\n$$ \\left\\{ t^{i}, x^{i}, y^{i}, u^{i}, v^{i} \\right\\}_{i=1}^{N} $$\nこれに基づいて損失関数を以下のようにする。ここで、$u = \\phi_{y}$, $v = -\\psi_{x}$ であることを思い出す。\n$$ \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) $$\nそして、$(15)$ の右辺を左辺に定理し、それぞれを $f$ と $g$ として定義する。\n$$ \\begin{equation} \\begin{aligned} f \u0026amp;:= u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) + p_{x} - \\lambda_{2}(u_{xx} + u_{yy}) \\\\ g \u0026amp;:= v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) + p_{y} - \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned}\\tag{18} \\end{equation} $$\nすると $f, g$ の値は $\\psi$ によって以下のように表される。（$p$ もニューラルネットワークで近似する）\n$$ \\begin{align*} f \u0026amp;= \\phi_{yt} + \\lambda_{1}(\\psi_{y} \\psi_{yx} - \\psi_{x}\\psi_{yy}) + p_{x} -\\lambda_{2}(\\psi_{yxx} + \\psi_{yyy}) \\\\ g \u0026amp;= -\\phi_{xt} + \\lambda_{1}(-\\psi_{y} \\psi_{xx} + \\psi_{x}\\psi_{xy}) + p_{y} + \\lambda_{2}(\\psi_{xxx} + \\psi_{xyy}) \\\\ \\end{align*} $$\n損失関数に $f(t^{i}, x^{i}, y^{i}) = 0 = g(t^{i}, x^{i}, y^{i})$ という情報を加え、最終的に以下のようにする。\n$$ \\begin{aligned} MSE \u0026amp;:= \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) \\\\ \u0026amp;\\qquad + \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| f(t^{i}, x^{i}, y^{i}) \\right|^{2} + \\left| g(t^{i}, x^{i}, y^{i}) \\right|^{2} \\right) \\end{aligned} \\tag{19} $$\n次に、入力ノードが $3$ つ、出力ノードが $2$ つの人工ニューラルネットワークを定義する。この出力を $\\begin{bmatrix} \\psi(t, x, y) \u0026amp; p(t, x, y) \\end{bmatrix}$ とする。すると、上記の損失関数を計算することができる。\nノイズがある場合とない場合のデータについて実験を行い、どちらの場合も高い精度で $\\lambda_{1}, \\lambda_{2}$ を予測できたことが示されている。また、圧力 $p$ に関するデータが与えられていない場合でも、人工ニューラルネットワークがパラメータと共に $p$ もかなり正確に近似できることが示された。具体的な実験設定、結果、参照解の求め方については、論文に詳しく記載されている。\n5. 結論 この論文では、与えられたデータが満たす物理法則をエンコードする能力があり、偏微分方程式で説明できる新しい種類のニューラルネットワーク構造である物理情報に基づいたニューラルネットワークを紹介した。この結果から、物理モデルに対してディープラーニングが学習できることがわかった。これは多くの物理的シミュレーションに応用可能である。\nしかし、著者は提案された方法が偏微分方程式を解くための既存の方法、例えば有限要素法finite element method、スペクトル方法spectral methodsなどを置き換えるものだと考えるべきではないと述べている。実際にセクション3.2.ではルンゲ・クッタ法をPINNに適用している。\nPINNを実装するために、どれくらいの深さのニューラルネットワークが必要か、どれくらいのデータが必要かなどのハイパーパラメータに関する問題についても、著者は解決策を提案しようとした。しかし、ある方程式で効果的な設定が他の方程式ではそうではないことが観察されたと述べている。\n","id":3313,"permalink":"https://freshrimpsushi.github.io/jp/posts/3313/","tags":null,"title":"물리정보기반 신경망(PINN) 논문 리뷰"},{"categories":"기하학","contents":"概要 微分多様体上のプルバックを定義する。微分多様体が難しい場合は、$M = \\mathbb{R}^{m}$、$N = \\mathbb{R}^{n}$と考えてもよい。\n定義1 二つの微分多様体 $M, N$と微分可能な関数 $f : M \\to N$が与えられたとする。そこで、$N$の$k$-形式を$M$の$k$-形式に送る関数$f^{\\ast}$を考えることができる。$\\omega$を多様体$N$の$k$-形式とするとき、多様体$M$の$k$-形式$f^{\\ast}\\omega$を$\\omega$のプルバックpull back, 引き戻しと呼び、以下のように定義する。\n$$ \\begin{equation} (f^{\\ast}\\omega)(p) (v_{1}, \\dots, v_{k}) := \\omega(f(p))\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M \\end{equation} $$\n説明 プルバックという名前には、($f$が$M$から$N$へのマッピングであるのに対して)$f^{\\ast}$は$N$から$M$へのマッピングであるという意味がある。定義と表記法がかなり難しいが、少しずつ理解していこう。\n$f^{\\ast}$ $f^{\\ast}$は$N$の$k$-形式を$M$の$k$-形式に送るマップである。したがって、$\\omega$を$N$の$k$-形式とすると、$f^{\\ast}\\omega = f^{\\ast}(\\omega)$は$M$の$k$-形式である。\n$f^{\\ast}\\omega(p)$ 多様体$M$上の$k$-形式は、$p \\in M$を$\\Lambda^{k}(T_{p}^{\\ast}M)$の元にマッピングする。\n$$ f^{\\ast}\\omega : M \\to \\Lambda^{k}(T_{p}^{\\ast}M) $$\n$$ \\Lambda^{k} (T_{p}^{\\ast}M) := \\left\\{ \\varphi : \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\nつまり、$f^{\\ast}\\omega(p) \\in \\Lambda^{k} (T_{p}^{\\ast}M)$もまた、一つの関数である。$\\Lambda^{k} (T_{p}^{\\ast}M)$の定義により、$f^{\\ast}\\omega(p)$は「$p$上の接ベクトル」$k$個を変数とする。これで、$(1)$はこの関数の関数値を具体的に定義した式であることがわかる。$f^{\\ast}(p)$自体が一つの関数であることをより強調するため、以下のような表記を使うことにしよう。\n$$ (f^{\\ast}\\omega)_{p} = f^{\\ast}\\omega(p) $$\n$\\omega(f(p))$ $\\omega$は$N$の$k$-形式であるため、$N$の点$f(p)$を$\\Lambda^{k}(T_{f(p)}^{\\ast}N)$の元にマッピングする。\n$$ \\Lambda^{k} (T_{f(p)}^{\\ast}N) := \\left\\{ \\varphi : \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\n$\\Lambda^{k} (T_{f(p)}^{\\ast}N)$の定義により、$\\omega(f(p))$もまた、一つの関数である。$\\omega(f(p))$は「$f(p)$上の接ベクトル」$k$個を変数とする。ここでも同様に、$\\omega(f(p))$自体が一つの関数であることを強調するために、以下のような表記を使おう。\n$$ \\omega_{f(p)} = \\omega(f(p)) $$\n$df_{p}v_{i}$ $$ df_{p} : T_{p}M \\to T_{f(p)}N $$\n$f : M \\to N$に対して、$f$の微分 $df_{p}$は上記のように定義される。したがって、$v_{i} \\in T_{p\n}M$であれば、$df_{p}v_{i} = df_{p}(v_{i})$は$T_{f(p)}N$の元である。\nこれを総合すると、$(1)$を得る。\n$$ (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) := \\omega_{f(p)}\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M $$\n上記の二つの関数の定義域を見ると、以下のような違いがある。\n$$ \\begin{align*} (f^{\\ast}\\omega)_{p} : \u0026amp;\u0026amp; \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\\\ \\omega_{f(p)} : \u0026amp;\u0026amp; \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\end{align*} $$\nこの違いを微分$df_{p} : T_{p}M \\to T_{f(p)}N$が繋いでいると考えればよい。そのため、$df_{p}$をプッシュフォワードpush forward, 押し出しとも呼ぶ。$1$-形式$\\varphi$に対して、以下が成立する。\n$$ \\begin{equation} \\varphi( dfv) = f^{\\ast}\\varphi(v) \\end{equation} $$\n$0$-形式のプルバック $f : M \\to N$を二つの微分多様体間で定義された関数とする。$g : N \\to \\mathbb{R}$を関数($N$での$0$-形式)とする。$g$のプルバック$f^{\\ast}g : M \\to \\mathbb{R}$は、以下のように定義される関数($M$での$0$-形式)である。\n$$ f^{\\ast}g := g \\circ f $$\n座標変換 関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$が与えられたとする。$\\mathbf{x} = (x_{1}, \\dots ,x_{n}) \\in \\mathbb{R}^{n}$であり、$\\mathbf{y} = (y_{1}, \\dots ,y_{m}) \\in \\mathbb{R}^{m}$である。\n$$ f(x_{1}, \\dots, x_{n}) = (f_{1}(\\mathbf{x}), \\dots, f_{m}(\\mathbf{x}) )= (y_{1}, \\dots ,y_{m}) $$\nそして、$\\omega = \\sum\\limits_{I} a_{I} dy_{I}$を$\\mathbb{R}^{m}$上の$k$-形式とする。そのとき、$\\omega$のプルバック$f^{\\ast}\\omega$は以下の特性により、次のようになる。\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= f^{\\ast} \\left( \\sum a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast} \\left( a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}dy_{I} \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}(dy_{i1} \\wedge \\cdots \\wedge dy_{ik}) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} (f^{\\ast}dy_{i1} \\wedge \\cdots \\wedge f^{\\ast}dy_{ik}) \\end{align*} $$\nこの時、$(2)$により$f^{\\ast}dy_{i1}(v) = dy_{i1}(df(v)) = d(y_{i1}\\circ f)(v) = df_{i1}(v)$であり、$f^{\\ast}a_{I} = a_{I} \\circ f$であるため、\n$$ \\begin{equation} f^{\\ast} \\omega = \\sum a_{I}(f_{1}, \\dots f_{m}) df_{i1} \\wedge \\cdots \\wedge df_{ik} \\end{equation} $$\n上記の式は座標変換を意味し、具体的にどのようになるかは以下の例で見てみよう。\n例 $\\mathbb{R}^{2} \\setminus \\left{ 0, 0 \\right}$上の$1$-形式$\\omega$が以下のようであるとする。\n$$ \\omega = - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = a_{1}dx + a_{2}dy $$\nこの直交座標上の$1$-形式を極座標に変換してみよう。$U = \\left{ (r,\\theta) : 0 \u0026lt; r, 0 \\le \\theta \u0026lt; 2\\pi \\right}$とする。そして、$f : U \\to \\mathbb{R}^{2}$を以下のようにする。\n$$ f(r,\\theta) = (r\\cos\\theta, r\\sin\\theta) = (f_{1}, f_{2}) $$\nここで、$df_{1}, df_{2}$を計算してみよう。$f_{1} = r\\cos\\theta, f_{2}=r\\sin\\theta$であるため、\n$$ \\begin{align*} df_{1} \u0026amp;= \\dfrac{\\partial f_{1}}{\\partial r}dr + \\dfrac{\\partial f_{1}}{\\partial \\theta}d\\theta = \\cos\\theta dr - r \\sin \\theta d\\theta \\\\ df_{2} \u0026amp;= \\dfrac{\\partial f_{2}}{\\partial r}dr + \\dfrac{\\partial f_{2}}{\\partial \\theta}d\\theta = \\sin\\theta dr + r \\cos \\theta d\\theta \\\\ \\end{align*} $$\nそれにより、$(3)$に従い、\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= a_{1}(f_{1}, f_{2})df_{1} + a_{2}(f_{1}, f_{2})df_{2} \\\\ \u0026amp;= - \\dfrac{f_{2}}{f_{1}^{2} + f_{2}^{2}}(\\cos\\theta dr - r \\sin \\theta d\\theta) + \\dfrac{f_{1}}{f_{1}^{2} + f_{2}^{2}}df_{2}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= - \\dfrac{r\\sin\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\cos\\theta dr - r \\sin \\theta d\\theta) \\\\ \u0026amp;\\quad + \\dfrac{r\\cos\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= -\\dfrac{\\sin\\theta \\cos\\theta}{r}dr + \\sin^{2}\\theta d\\theta + \\dfrac{\\cos\\theta \\sin\\theta}{r}dr + \\cos^{2}\\theta d\\theta \\\\ \u0026amp;= d\\theta \\end{align*} $$\nしたがって、\n$$ \\int - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = \\int d\\theta $$\n■\n特性 $M, N$をそれぞれ$m, n$次元微分多様体、$f : M \\to N$とする。$\\omega, \\varphi$を$N$上の$k$-形式とする。$g$を$N$上の$0$-形式とする。$\\varphi_{i}$たちを$N$上の$1$-形式とする。すると、以下が成立する。\n$$ \\begin{align} f^{\\ast} (\\omega + \\varphi) =\u0026amp;\\ f^{\\ast}\\omega + f^{\\ast}\\varphi \\tag{a} \\\\ f^{\\ast} (g \\omega) =\u0026amp;\\ (f^{\\ast}g) (f^{\\ast}\\omega) \\tag{b} \\\\ f^{\\ast} (\\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k}) =\u0026amp;\\ f^{\\ast}(\\varphi_{1}) \\wedge \\cdots \\wedge f^{\\ast}(\\varphi_{k}) \\tag{c} \\end{align} $$\nこのとき、$+$と$\\wedge$はそれぞれ$k$-形式の合計とくさび積である。\n$\\omega, \\varphi$を$N$上の任意の二つの形式とする。$L$を$l$次元微分多様体、$g : L \\to N$とする。\n$$ \\begin{align*} f^{\\ast}(\\omega \\wedge \\varphi) \u0026amp;= (f^{\\ast}\\omega) \\wedge (f^{\\ast}\\varphi) \\tag{d} \\\\ (f \\circ g)^{\\ast} \\omega \u0026amp;= g^{\\ast}(f^{\\ast}\\omega) \\tag{e} \\end{align*} $$\n証明 証明 $(a)$ $$ \\begin{align*} (f^{\\ast}(\\omega + \\varphi))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\omega + \\varphi)_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ \\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) + \\varphi_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ (f^{\\ast} \\omega)_{p}(v_{1}, \\dots, v_{k}) + (f^{\\ast} \\varphi)_{p}(v_{1}, \\dots, v_{k}) \\\\ =\u0026amp;\\ \\left( f^{\\ast}\\omega + f^{\\ast}\\varphi \\right)_{p}(v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\n証明 $(b)$ $0$-形式$g$と$k$-形式$\\omega$の積を以下のように定義する。\n$$ (g\\omega)(p) = g(p) \\omega(p) $$\nここで、$g(p) = g_{p}$はスカラー、$\\omega(p) = \\omega_{p}$は関数であることに注意。それにより、\n$$ \\begin{align*} (f^{\\ast} (g\\omega))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ g\\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ g_{f(p)} \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ g\\circ f(p) \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ (f^{\\ast}g)_{p} (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\n証明 $(c)$ $$ \\begin{align*} (f^{\\ast}\\left( \\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k} \\right))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\varphi_{1} \\wedge \\dots \\wedge \\varphi_{k})_{f(p)} \\left( df_{1}, \\dots, df_{k} \\right) \\\\ =\u0026amp;\\ \\det [\\varphi_{i}df(v_{j})] \\\\ =\u0026amp;\\ \\det [ f^{\\ast} \\varphi_{i}(v_{j})] \\\\ \\end{align*} $$\n■\nManfredo P. Do Carmo, Differential Forms and Applications, p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3262,"permalink":"https://freshrimpsushi.github.io/jp/posts/3262/","tags":null,"title":"微分幾何学におけるプルバック"},{"categories":"확률미분방정식","contents":"モデル 1 $t$ 時点で $S_{t}$ を基礎資産 $1$単位の価格とし、$S_{t}$ が幾何ブラウン運動をすると仮定しよう。すなわち、標準ブラウン運動 $W_{t}$ とトレンドDrift $\\mu \\in \\mathbb{R}$ および拡散Diffusion $\\sigma^{2} \u0026gt; 0$ に対して、$S_{t}$ は次の確率微分方程式の解である。 $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ 無リスク金利 $r \\in \\mathbb{R}$ が与えられたとき、$t$ 時点での派生商品 $1$単位の価格 $F = F \\left( t, S_{t} \\right)$ は次の偏微分方程式に従う。 $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\n変数 $F \\left( t, S_{t} \\right)$: 派生商品Derivativesは、先物、オプションなどの金融商品を指す。 $S_{t}$: 基礎資産Underlying Assetsは、通貨、債券、株式など、派生商品の取引対象となる商品を指す。 パラメータ $r \\in \\mathbb{R}$: 無リスク資産Risk-free Assetsの利率を示す。無リスク資産の代表的な例には預金がある。 $\\sigma^{2} \u0026gt; 0$: 市場のボラティリティVolatilityを示す。 説明 派生商品に対する一般的な誤解とは異なり、先物FutureやオプションOptionは、不確実な未来に対するヘッジEdgeの手段として作られた。不確実な未来に備えるために、ある程度の費用Premiumを支払ってもリスクを減らすための方法であった。問題は、その価格を設定する適切な方法がなかったことであり、取引者は経験に基づいて感覚的に派生商品を取引していた。ブラック-ショールズモデルは、そのような派生商品の価格を数学的に説明できるようにした方程式である。\n一般的に、ブラック-ショールズモデル(1973)の貢献者としては、フィッシャー・ブラックFischer Blackとマイロン・ショールズMyron Scholesのほか、本投稿の「ヘッジを用いた導出」を紹介したロバート・マートンRobert K. Mertonの3人が挙げられる。残念ながらブラックは1995年に亡くなり、ショールズとマートンは1997年にノーベル経済学賞を受賞した。ブラック-ショールズ-マートン方程式の発見以降、オプション市場は急速に発展し、学界には金融工学という新たな分野の誕生をもたらした。\nブラックの喜劇 ウィキペディア2によると、ブラックは博士課程の時に専攻を頻繁に変え、どこかに定着するのが苦手だったという。物理学から数学、コンピューター、人工知能へと変更したが、結局名を残した分野は経済学になった。\n信頼できるリファレンスは見つからなかったが、筆者がどこかで聞いた話によると、ブラックは物理学を専攻していた時に、周囲の狂った天才たちを見て「ここでは生き残れない」と思ったという。後に経済/金融を学んでみると、数学を積極的に使う先駆者がいなく、理工学の怪物たちがいない荒れ地で、数学を武器に自らが先駆者となったという。\nショールズの悲劇 ナムウィキ3によると、ショールズは1997年のノーベル経済学賞の記者会見で、賞金で株投資をすると答えてセンセーションを巻き起こしたという。当時、ショールズが運用していたヘッジファンドは過度の自信から過剰なレバレッジLeverageを使用し、1998年のロシアの債務不履行で破綻したという。危機を乗り越えた後、ショールズ は最終的に投資家に利益を返し、その後もファンドマネージャーとして活動を続けたが、サブプライムモーゲージ危機が起こる直前に引退したという。\n前提 本格的な導出に先立ち、いくつかの前提について確認しておこう。\n手数料、税金、配当などの言及されていない要素は考慮しない 物理学モデルで興味の対象でない抵抗や温度、気圧などを考慮しないのと同じ程度に受け止めればよい。そこに加えて、トレンド $\\mu$ と $\\sigma$ などは単純に定数と仮定する。\n派生商品は基礎資産と時点に依存している 派生商品の価格が基礎資産に独立していれば、派生、基礎という言葉を使う理由がない。基礎資産の価格が変わるにつれて派生商品の価格が変わるのが妥当である。また、時間の経過によって変わらない（定数である）ならば、派生商品の価格を検討する意味がない。したがって、$F$ の形を正確には言えないが、少なくとも二つの要素 $t$ と $S_{t}$ に対する関数であると仮定する。 $$ F = F \\left( t, S_{t} \\right) $$\n基礎資産は幾何ブラウン運動をする 幾何ブラウン運動 GBMの代表的な応用は、まさに株価などの基礎資産の価格変動を説明することである。人口の変動量が全体の人口に比例するように、資産の価格変動も資産の価格に比例し、上場廃止にならない限りマイナスになることはないなど、良い前提を多く持っている。 $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$\nある株式の価格 $p_{t}$ が GBMに従うと仮定しよう。$t$日目の終値を$t-1$日目の終値で割り、対数を取った $$ r_{t} = \\nabla \\log p_{t} = \\log {{ p_{t} } \\over { p_{t-1} }} $$ をリターン―リターンReturnと呼ぶが、株価の大きさに関係なく価格が上がれば正の値、下がれば負の値になり、直感と一致する。対数正規分布の項で説明したように、このリターンは正規分布に従い、単純な上下ではなく、株価の成長と逆成長その本質に関心を持つものと見ることができる。\n無リスク資産はメルサス成長をする メルサス成長モデルは、人口動態学Population Dynamicsで、資源の制限や介入などがない場合の集団の成長を説明する最も単純なモデルであり、経済/金融のセンスでは無リスク資産の増殖を説明する前提になる。無リスク収益率は$r$ 定数として仮定され、その金融収益は資産 $N_{t}$ の規模に比例するため、次のような常微分方程式で表現できる。 $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$\n無裁定価格: ポートフォリオ間には価値の差がない ポートフォリオPortfolioに関する数式的な説明は、この証明でさらに詳しく説明する。 無裁定価格Arbitrage-free Pricingの前提とは、我々が考慮するすべてのポートフォリオが同じ価値でバランスを保っているということである。例えば、ポートフォリオ $A$ の価値が $B$ よりも高い場合、合理的な取引主体はより価値のある $A$ の比重を増やして裁定利益を得ることができるため、$B$ を考える理由がない。したがって、我々が考慮するポートフォリオは、このような裁定取引によってこれ以上利益を得ることができない状態であると仮定する。\n摩擦のない市場: 分割と空売りに制限がない 株をやったことがある人ならわかるが、この\n取引というのは最小限の値段単位があり、私が望む金額で取引できないし、空売りをしたくても日本の株式市場では貸借空売りが原則であり、制限がある。その取引単位を自由に分割でき、どんな制約もなく空売りできるということは、行動を妨げる摩擦がないと見ることができる。\n導出 Part 1. ポートフォリオの構成\n我々が保有できる資産は、次の3つの種類だけとしよう。\n基礎資産: $s$ 単位保有しているとしよう。 派生商品: $f$ 単位保有しているとしよう。 無リスク資産: 基礎資産でも派生商品でもない資産で、現金と考えても構わない。 $t$ 時点で我々が保有するすべての資産の価値を $V_{t}$ とすると、$S_{t}$ が基礎資産 $1$単位の価格であり、$F \\left( t , S_{t} \\right)$ が派生商品 $1$単位の価格であったので、次のように表せる。 $$ V_{t} = f F \\left( t, S_{t} \\right) + s S_{t} $$ ポートフォリオを構成するとは、この $f$ と $s$ の量を調整すること、つまりどのように投資するかについての戦略を立てることである。このようなポートフォリオ構成によって発生する取引量が多すぎて市場に影響を与えるという前提は非合理的であるため、基礎資産と派生商品の価格は、$f$ と $s$ の選択に関係なく一定と仮定しよう。つまり、$f$ と $s$ をどのように定めても、以下の数学的議論は変わらないということである。\n注意すべきは、$V_{t}$ は全資産の合計ではないということである。株式口座の残高だけを見ると考えればわかりやすい。ポートフォリオの例としてどのようなものがあるか考えてみよう：\n貯蓄 $V_{t} = 0$：株式口座を整理してすべて貯蓄し、利息だけを受け取る。数学しか知らない士人の目にはあまりにもトリビアルTrivialに見えるかもしれないが、暴落市場や不況に対処できる立派な戦略である。 アリAnt $V_{t} = 5 S_{t}$：個人ならば、派生商品には手を出さないようにしよう。個人が空売りが禁止されている国では、ほとんどの個人投資家はこのようなポートフォリオを持っている。例として、数式で $S_{t} = 81,200$ がサムスン電子の株価であれば、このポートフォリオはサムスン電子 $5$株を保有している私の友人「キム・スヒョン」の口座である。 ヘッジHedge $\\displaystyle V_{t} = 1 \\cdot F- {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t}$：コールオプションCall Optionを $1$だけ買い、その基礎資産を ${{ \\partial F } \\over { \\partial S_{t} }}$ だけ空売りしたとしよう。オプションの満期日に基礎資産の価格が大幅に上昇した場合、コールオプションが大きな利益をもたらし、基礎資産の価格がむしろ下落した場合は、空売りで既に利益を得ている。 ヘッジについての説明で触れたオプションは、ヨーロピアンオプションEuropean Optionであり、通常、私たちが知っている「満期日にのみ権利を行使できるオプション」である。アメリカンオプションAmerican Optionは満期前でも常に権利を行使できるが、大して知る必要はなく、ヨーロピアン、アメリカンという言葉に怖がることはないようにしよう。\n我々は最後の例、現物空売りで派生商品をヘッジするポートフォリオ $$ V_{t} = 1 \\cdot F \\left( t, S_{t} \\right) - {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t} $$ からブラック-ショールズ方程式を導出する。正確にヘッジしているので、このポートフォリオは無リスク資産であり、時間 $t$ に対する増分Incrementは $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} d S_{t} $$ である。ここで、$S_{t}$ は幾何ブラウン運動をすると仮定したので、$d S_{t}$ に $\\displaystyle S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right)$ を代入すると $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ である。一方、無裁定価格の前提を考えると、このポートフォリオと無リスク資産のポートフォリオの増分は同じでなければならない。もしポートフォリオ間に価格差があると仮定すると、他方のポートフォリオを処分して異なるポートフォリオに投資することで裁定利益を得ることができるためである。無リスク資産はメルサス成長をするという前提をしたので、無リスク金利 $r$ に対して、次のような常微分方程式で表される。 $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$ これを整理して表すと $$ \\begin{align*} d V_{t} =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\\\ d V_{t} =\u0026amp; r V_{t} dt \\end{align*} $$ であるため、 $$ \\begin{equation} r V_{t} dt = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\label{1} \\end{equation} $$ を得る。これで、$dF$ を求めるために伊藤積分を通じて計算してみよう。\nPart 2. 伊藤計算\n伊藤の公式: 伊藤過程 $\\left\\{ X_{t} \\right\\}_{t \\ge 0}$ が与えられているとする。 $$ d X_{t} = u dt + v d W_{t} $$ 関数 $V \\left( t, X_{t} \\right) = V \\in C^{2} \\left( [0,\\infty) \\times \\mathbb{R} \\right)$ に対して $Y_{t} := V \\left( t, X_{t} \\right)$ と置くと、$\\left\\{ Y_{t} \\right\\}$ も伊藤過程であり、次が成立する。 $$ \\begin{align*} d Y_{t} =\u0026amp; V_{t} dt + V_{x} d X_{t} + {{ 1 } \\over { 2 }} V_{xx} \\left( d X_{t} \\right)^{2} \\\\ =\u0026amp; \\left( V_{t} + V_{x} u + {{ 1 } \\over { 2 }} V_{xx} v^{2} \\right) dt + V_{x} v d W_{t} \\end{align*} $$\n幾何ブラウン運動で $S_{t}$ を分配法則に従って展開すると $$ d S_{t} = \\mu S_{t} dt + \\sigma S_{t} d W_{t} $$ であり、伊藤の公式で $u = \\mu S_{t}$ および $v = \\sigma S_{t}$ なので $$ d F = \\left( {{ \\partial F } \\over { \\partial t }} + {{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} $$ を得る。$\\eqref{1}$ の $d F$ にこれを代入してみると $$ \\begin{align*} r V_{t} dt =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt - {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} \\\\ =\u0026amp; \\left( {{ \\partial F } \\over { \\partial t }} + {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t}} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ \u0026amp; - {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt} - {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ =\u0026amp; {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt \\end{align*} $$ である。左辺のポートフォリオの価値 $V_{t}$ が $\\displaystyle V_{t} = F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t}$ のように定義されていたので、これを代入すると $$ r \\left( F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\right) dt = {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt $$ である。$rF$ に対して式を整理すると、求めていた次の方程式を得る。 $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\n■\n崔炳善. (2012). ブラック-ショールズ式の様々な導出\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Fischer_Black\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://namu.wiki/w/%EB%B8%94%EB%9E%99-%EC%88%84%EC%A6%88%20%EB%AA%A8%ED%98%95#s-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2156,"permalink":"https://freshrimpsushi.github.io/jp/posts/2156/","tags":null,"title":"ブラック-ショールズモデルの導出"},{"categories":"머신러닝","contents":"この文は逆転派アルゴリズムの原理を数学専攻者が理解しやすいように作成された。\n表記法 上図のような 人工ニューラルネットワーク が与えられたとする。$\\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n_{0}})$は入力input、 $y_{j}^{l}$は$l$番目の層の$j$ノード、$\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}})$ドルは出力outputである。\n$L \\in \\mathbb{N}$は、隠匿層hidden layerの個数であり、$\\mathbf{n}=(n_{0}、n_{1}、\\dots、n_{L}、\\hat{n}) \\in \\mathbb{N}^{N}=(n)、$の成分は順に入力層、$L$個の隠匿層と出力層のノード数を意味する。 また、便宜上、$0$番目の隠匿層は入力層を意味し、$L+1$番目の隠匿層は出力層を意味するとする。\n$w_{ji}^{l}$は、$l$の次の層の$i$のノードとその次の層の$j$のノードを連結する加重値を表す。 すると、各階から次の階への伝播は、以下のGIFのように起こる。\nここで $\\phi$ は任意の活性化関数 である。 $l$ 番目の層から次の層の $j$ 番目のノードに伝達される線形結合を $v_{i}^{l}$で表記しよう。\n$$ \\begin{align*} v_{j}^{l} \u0026amp;= \\sum _{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\\\ y_{j}^{l+1} \u0026amp;= \\phi ( v_{j}^{l} ) = \\phi \\left( \\sum \\nolimits_{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\right) \\end{align*} $$\nこれを定理すると次のようになる。\n記号 意味 $\\mathbf{x}=(x_{1}, x_{2}, \\dots, x_{n_{0}})$ 入力 $y^{l}_{j}$ $l$ 番目の層の $j$ 番目のノード $\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}} )$ 出力 $n_{l}$ $l$ 番目の層のノード数 $w_{ji}^{l}$ $l$ 番目の層の $i$ 番目のノードと その次の層の $j$ 番目のノードを接続する重み付け $\\phi$ 活性化関数 $v_{j}^{l} = \\sum \\limits _{i=1} ^{n_{l}} w_{ji}^{l}y_{i}^{l}$ 線形結合 $y^{l+1}_{j} = \\phi (v_{j}^{l})$ $l$ 番目の階から次の階への 電波 定理 $E = E(\\hat{\\mathbf{y}})$を微分可能な適切な損失関数とする。 それでは、$E$を最適化する方法は、各層での加重値$w_{ji}^{l}$を次のようにアップデートするものである。\n$$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} \\label{thm} \\end{equation} $$\nこの時、$\\alpha$は学習率で、$\\delta_{j}^{l}$ は以下の通りである。\n$l=L$の時、\n$$ -\\delta_{j}^{L} = \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\n$l \\in \\left\\{ 0,\\dots, L-1 \\right\\}$の時、\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i=1}^{n_{l}} \\delta_{i}^{l+1} w_{i j}^{l+1} $$\n説明 $(1)$を見てみよう。 $l$番目の層と$l+1$番目の層の間の加重値を更新する時、$l$番目のノードの$y_{j}^{l}$に依存するということですが、各層の出力に応じて最終的に出力$\\hat{\\mathbf{y}}$が決定されるので当然と見ることができる。 また、$y_{j}^{l}$は$l$番目から$l+1$番目の層に伝播される時の入力と見ることができるが、これは線形回帰モデルでLMSLeast Mean Squaresで学習する方法と似ている。\n$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha (\\mathbf{w}^{T}\\mathbf{x} - \\mathbf{y}) \\mathbf{x} $$\n一方、各層での出力$y_{j}^{l}$は入力層から出力層として計算される反面、最適化のための$\\delta_{j}^{l}$ は次のように出力層から入力層に逆に計算されるため、このような最適化手法を逆伝播アルゴリズムback propagation algorithmという。\n$$ \\begin{align*} \\delta_{j}^{L} \u0026amp;= - \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} \\\\ \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{j}^{L} w_{ij}^{L} \\\\ \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\\\ \\delta_{j}^{L-3} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-3}) \\sum _{i} \\delta_{i}^{L-2} w_{ij}^{L-2} \\\\ \u0026amp;\\vdots \\\\ \\delta_{j}^{1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{1}) \\sum _{i} \\delta_{i}^{2} w_{ij}^{2} \\\\ \\delta_{j}^{0} \u0026amp;= \\phi ^{\\prime} (v_{j}^{0}) \\sum _{i} \\delta_{i}^{1} w_{ij}^{1} \\end{align*} $$\n証明 入力層から出力層への計算が終わったとする。 加重値を損失関数$E$が減る方向に修正する方法は傾斜下降法を使えば次のようになる。\n$$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} - \\alpha \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l} } \\label{gradesent} \\end{equation} $$\nそれぞれの$y_{i}^{l}$は与えられた値なので、偏微分部分を計算できる形で解くことができる。 右辺の偏微分は連鎖法則によって次のようになる。\n$$ \\begin{equation} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}}) }{\\partial v_{j}^{l}} \\dfrac{\\partial v_{j}^{l}}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial v_{j}^{l}} y_{i}^{l} \\label{chainrule} \\end{equation} $$\n$(3)$の右辺の偏微分を$-\\delta_{j}^{l}$ とすると、$(2)$ から $(1)$ を得る。\n$$ w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} $$\n各層で $\\delta_{j}^{l}$ を次のように求める。\n$l=L$の場合\n$j \\in \\left\\{ 1, \\dots, \\hat{n} \\right\\}$ に対して次が成立する。\n$$ \\begin{equation} -\\delta_{j}^{L} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial \\hat{y}_{j}} \\dfrac{d \\hat{y}_{j}}{d v_{j}^{L}} \\label{deltamL} \\end{equation} $$\nこの時、$\\hat{y}_{j} =\\phi (v_{j}^{L})$ であるから次を得る。\n$$ -\\delta_{j}^{L} (t) =\\phi ^{\\prime} (v_{j}^{L}(t)) \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\n■\n$l=L-1$の場合\n$j \\in \\left\\{ 1, \\dots, n_{L-1} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{L-1} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-1}} = = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L}} \\dfrac{d y_{j}^{L}}{d v_{j}^{L-1}} $$\nこの時$y_{j}^{L} =\\phi (v_{j}^{L-1})$ であるので、次を得る。\n$$ -\\delta_{j}^{L-1} = = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\dfrac{\\partial y_{j}^{L}}{\\partial v_{j}^{L-1}} = = \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} $$\n右辺の偏微分は連鎖法則によって次のように計算される。\n$$ \\begin{align*} -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L}} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L}} \\end{align*} $$\nここで $(4)$ と ${\\color{green}v_{i}^{L}=\\sum_{j}w_{ij}^{L}y_{j}^{L}}$ により、次を得る。\n$$ \\begin{align} \u0026amp;\u0026amp; -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i=1} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial v_{i}^{L}}} {\\color{green} \\dfrac{d v_{i}^{L}}{d y_{j^{L}}} } \\nonumber \\\\ \u0026amp;\u0026amp; \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{green} w_{ij}^{L} }\\nonumber \\\\ {}\\nonumber \\\\ \\implies \u0026amp;\u0026amp; \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ij}^{L} \\label{deltajL-1} \\end{align} $$\n■\n$l=L-2$の場合\n$j \\in \\left\\{ 1, \\dots, n_{L-2} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-2}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} $$\nこの時$y_{j}^{L-1} =\\phi (v_{j}^{L-2})$ であるから次を得る。\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} = \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} $$\n右辺の偏微分は連鎖法則によって次のように計算される。\n$$ \\begin{align*} -\\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{\\partial y_{k}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}} \\dfrac{\\partial v_{k}^{L-1}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}}} {\\color{red}\\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} } {\\color{green}\\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}}} {\\color{purple}\\dfrac{d v_{k}^{L-1}}{\\partial y_{j}^{L-1}}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{red} w_{ik}^{L}} {\\color{green} \\phi^{\\prime}(v_{k}^{L-1})} {\\color{purple} w_{kj}^{L-1}} \\end{align*} $$\nしたがって、次を得る。\n$$ \\delta_{j}^{L-2} = -\\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) w_{kj}^{L-1} $$\nこのとき、$(5)$ によって次が成立する。\n$$ \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) = \\phi^{\\prime}(v_{k}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} = \\delta_{k}^{L-1} $$\nしたがって、次を得る。\n$$ \\begin{align*} \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\delta_{k}^{L-1} w_{kj}^{L-1} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\end{align*} $$\n■\n一般化: $l \\in \\left\\{1, \\dots, L-1 \\right\\}$\n上記の結果に基づき、次のように一般化することができる。$j \\in \\left\\{ 1, \\dots, n_{l} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} $$\n右辺の偏微分を連鎖法則で解くと次のようになる。\n$$ \\begin{align*} \u0026amp;\\quad \\delta_{j}^{l} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{\\partial \\hat{y}_{i_{(1)}}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{\\partial y_{i_{(2)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{\\partial y_{i_{(3)}}^{L-1} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{j}^{l}} \\\\ \u0026amp; \\quad \\vdots \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{i_{(4)}}^{L-2}} \\cdots \\frac{d y_{i_{(L-l+1)}}^{l+1} }{d v_{i_{(L-l+1)}}^{l} } \\frac{\\partial v_{i_{(L-l+1)}}^{l} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} -\\delta_{i_{(1)}}^{L} w_{i_{(1)}i_{(2)}}^{L} \\phi^{\\prime}(v_{i_{(2)}}^{L-1}) w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\delta_{i_{(2)}}^{L-1}w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\delta_{i_{(3)}}^{L-2} w_{i_{(3)} i_{(4)}}^{L-2} \\cdots w_{i_{(L-l)} j}^{L} \\\\ \u0026amp;\\quad \\vdots \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\delta_{i_{(L-l)}}^{l+1} w_{i_{(l-l)} j}^{l} \\end{align*} $$\nしたがって、定理すると次のようになる。\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i} \\delta_{i}^{l+1} w_{ij}^{l+1} $$\n■\n","id":3077,"permalink":"https://freshrimpsushi.github.io/jp/posts/3077/","tags":null,"title":"逆伝播アルゴリズム"},{"categories":"프로그래밍","contents":"概要 よく使われるRGB色の商標だ。\nコード ","id":2013,"permalink":"https://freshrimpsushi.github.io/jp/posts/2013/","tags":null,"title":"RGBカラーチートシート"},{"categories":"머신러닝","contents":"定義 強化学習とは、エージェントが環境と相互作用して累積報酬を最大化するポリシーを見つけることができるようにすることである。\n説明1 強化学習を構成する要素は次のとおりである。\nエージェントagent: 与えられた状態において、ポリシーに従って行動を決定する。 ステートstate, 状態: エージェントが置かれている状況を指す。 アクションaction, 行動: エージェントが与えられた状態で選ぶことができる選択肢を指す。 ポリシーpolicy, 方針: エージェントが与えられた状態で行動を決定する戦略を指す。 リワードreward, 報酬: エージェントが与えられた状態で選んだ行動によって得られる点数を指す。エージェントが達成すべき目標と見なすことができる。 環境environment: エージェントが与えられた状態でどのような行動を決定すれば、MDPに従って次の状態とそれに伴う報酬を決定するか。 エピソードepisode: エージェントと環境の相互作用が始まった時から終わるまでを指す。 これをさまざまな状況に例えると次のようになる。\n強化学習 試験勉強 囲碁 エージェント 学生 囲碁の棋士 ステート 試験まで残り日数 碁盤 アクション 勉強、飲酒、ゲームなど 着手 ポリシー 日付別勉強計画 戦略 リワード 試験点数 勝敗 エピソード 試験期間 一局 強化学習の問題：グリッドモデル 強化学習を説明するための代表的な例としてグリッドワールドgrid worldがある。これから次のグリッドモデルを例に各要素を具体的に説明する。一度に上下左右の4方向のうち一つに一マスずつ動けるロボットが下記のような$4 \\times 4$のグリッドで動く場合を考えてみよう。スタート地点は$\\boxed{\\ 2\\ }$から$\\boxed{15}$まで任意に決められ、ロボットが$\\fcolorbox{black}{darkgray}{\\ 1\\ }$または$\\fcolorbox{black}{darkgray}{16}$まで最短距離で行くことが目標とする。\nエージェント 強化学習におけるエージェントは学習する主体として説明されるが、実際には存在しない。後述する他の概念が確率変数などで定義されるのに対し、エージェントには明確な数学的定義がない。したがって、強化学習に関する理論的な勉強はエージェントという対象がなくても可能であり、実際にそうである。強化学習理論において本質的にエージェントを意味するのはポリシーである。しかし直感的には、学習する対象があると考える方が便利なため、「エージェントが行動する」「エージェントの状態が変わった」といった表現を用いる。エージェントは単にコンピュータシミュレーション（特にゲーム）においてキャラクターのように学習しているように見えるものに過ぎない。たとえば、グリッドモデルではエージェントが下の右側の図のように移動するのは、単純に状態の列挙で表すこともできる。 $$ \\boxed{\\ 3\\ } \\to \\boxed{\\ 2\\ } \\to \\fcolorbox{black}{darkgray}{\\ 1\\ } $$ $3, 2, 1$を順番にprintするだけでよい。強化学習の最終的に私たちが得たいのは本質的にポリシーであるため、エージェントというものを定義しなくても学習することができる。一言で言えば、エージェントはポリシーの視覚化（実現化）であると言える。\nもちろん、上記の話は理論やコンピュータシミュレーションでの話であり、自動運転のような実際の応用では、ポリシーに従って実際に動くドローンや自動車が必要である。この場合、ドローンや自動車などのロボットや機械がエージェントとなり、それがなければポリシーの学習は不可能である。\n状態 状態stateは確率変数であり、stateの頭文字をとって$S$と表記する。エピソードは時間に沿って順次進行するため、インデックスとして$t$を使用する。したがって、タイムステップが$t$のときのステート関数を$S_{t}$と表記する。初期ステートは通常$t=0$で表される。まとめると、$S_{t}$は時間が$t$のとき、各グリッドに対して次のような関数値を与える関数である。\n$$ S_{t} \\left( \\boxed{ N } \\right) = n,\\quad 1\\le n \\le 16 $$\nこのとき、可能なすべての状態値（状態関数の関数値）の集合を$\\mathcal{S}\\subset \\mathbb{R}$と表記し、その要素を$s$と表記する。\n$$ \\mathcal{S} = \\left\\{ s_{1}, s_{2},\\dots \\right\\} $$\nそれでは上記の格子モデルに対する状態関数は次のようになります。\n$$ S_{t} : \\left\\{ \\fcolorbox{black}{darkgray}{\\ 1\\ } , \\boxed{\\ 2\\ }, \\dots, \\boxed{15}, \\fcolorbox{black}{darkgray}{16} \\right\\} \\to \\mathcal{S} \\\\ S_{t} \\left( \\boxed{\\ n\\ } \\right) = s_{n} = n,\\quad 1\\le n \\le 16 $$\nそれでは時間が$t$のときの状態値が$s_{6}$から次のタイムステップで状態値が$s_{10}$に変わる確率は次のようになります。\n$$ P \\left( S_{t+1} = s_{10} | S_{t} = s_{6} \\right) $$\n到達した瞬間にエピソードが終了する状態をターミナルステートterminal stateと呼びます。上記の格子モデルではターミナルステートは$\\fcolorbox{black}{darkgray}{1}, \\fcolorbox{black}{darkgray}{16}$です。\n行動 行動actionとはエージェントが現在の状態で取ることができる選択肢のことであり、これもまた確率変数です。actionの頭文字を取って$A_{t}$と表記します。上記の格子モデルの例では、$\\boxed{2}$ ~ $\\boxed{15}$の各々で上下左右を選択することができます。可能な全ての行動値（行動関数の関数値）の集合を$\\mathcal{A}\\subset \\mathbb{R}$と表記し、その要素を$a$と表記します。\n$$ \\mathcal{A} = \\left\\{ a_{1}, a_{2}, \\dots \\right\\} $$\nそれではタイムステップ$t$での行動関数は次のようになります。\n$$ A_{t} : \\left\\{ \\uparrow, \\rightarrow, \\downarrow, \\leftarrow \\right\\} \\to \\mathcal{A} \\\\ \\begin{cases} A_{t}(\\uparrow) = a_{1} \\\\ A_{t}(\\rightarrow) = a_{2} \\\\ A_{t}(\\downarrow) = a_{3} \\\\ A_{t}(\\leftarrow) = a_{4} \\end{cases} $$\nエージェントは与えられた状態で確率に従って行動を決定します。例えばタイムステップが$t$のときの状態値が$s_{6}$で行動$a_{1}$を選択した確率は次のようになります。\n$$ P(A_{t} = a_{1} | S_{t} = s_{6}) $$\n方針 方針policyとは状態$s$で行動$a$を決定する確率を全ての$s$と$a$に対して明記したものを言い、$\\pi$で表記します。ゲームや戦争に例えると戦略です。格子モデルの例で行動を決定する確率が$\\dfrac{1}{4}$で全て同じだとすると、方針$\\pi$は次のようになります。\n$$ \\pi \\begin{cases} P(a_{1} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{2} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{2}) = \\dfrac{1}{4} \\\\ \\vdots \\\\ P(a_{2} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{4} | s_{15}) = \\dfrac{1}{4} \\end{cases} \\quad \\text{or} \\quad \\pi : \\mathcal{S} \\times \\mathcal{A} \\to [0,1] $$\nもちろんこれは最適化された方針ではありません。簡単に$\\boxed{2}$の場合だけ考えても、上に行くと格子の外に出てしまうため、上に行く確率自体が全く無い方がより良い方針です。したがって、下の図で$\\pi_{1}$よりも$\\pi_{2}$がより良い方針だと言えます。\n強化学習アルゴリズムの目標は最適な方針を見つけることです。では、最適な方針をどのように見つけるかというと、方針の良さを評価する価値関数value functionを通じて見つけることができます。\n報酬 報酬rewardとは、与えられた状態でエージェントが選択した行動に対して実数をマッピングする関数であり、rewardの頭文字を取って$R_{t}$と表記します。全ての報酬値（報酬関数の関数値）の集合を$\\mathcal{R} \\subset \\mathbb{R}$と表記し、その要素を$r$と表記します。\n$$ \\mathcal{R} = \\left\\{ r_{1}, r_{2}, \\dots \\right\\} \\\\ R_{t} = \\mathcal{S} \\times \\mathcal{A} \\to \\mathcal{R} $$\n報酬は一回のタイムステップごとに一回ずつ受け取り、一回のエピソードで受け取った総報酬、つまり蓄積された報酬が最も大きくなるような方針を見つけることが強化学習の究極的な目標です。\nでは、なぜ各タイムステップの報酬よりも蓄積された報酬が大きくなるようにするのか疑問に思うかもしれません。これは試験勉強に例えると簡単に理解できます。試験期間中に毎晩勉強する代わりにお酒を飲んだり遊んだりゲームをした場合、当面は勉強するよりも楽しいでしょう。しかし、蓄積された報酬、つまり試験の成績は散々なものになります。したがって、今は勉強することが疲れて大変だとしても、将来の大きな報酬のために勉強する方が良いと判断し、試験勉強をするわけです。\n報酬は人が設定するハイパーパラメータです。したがって、エージェントが行うべき仕事に応じて適切に設定する必要があります。例えば、格子モデルの例で格子が迷路であり、エージェントが迷路を脱出するロボットである場合、一マス移動するごとに$-1$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。格子が公園であり、エージェントがペットの散歩をするロボットである場合、一マス移動するごとに$0$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。\n環境 環境environmentとはエージェントが与えられた状態で選択した行動に応じて次の状態と報酬を決定する関数、すなわち$f : (s,a) \\mapsto (s^{\\prime},r)$です。したがって、常に現実にぴったりと当てはまる比喩を見つけるのは難しいです。\nタイムステップが$t$のときの状態を$s_{t}$、$s_{t}$で選択した行動を$a_{t}$とします。これにより、環境が決定した次の状態を$s_{t+1}$、報酬を$r_{t+1}$とすると次のように表されます。\n$$ f(s_{t}, a_{t}) = (s_{t+1}, r_{t+1}) $$\n格子モデルの例について具体的に説明すると、エージェントが$\\boxed{7}$で$\\uparrow$を選択し、環境が次の状態$\\boxed{3}$と報酬$-1$を決定した場合は、次のような数式で\n表されます。\n$$ f(s_{7}, a_{1}) = (s_{3}, -1) $$\nエージェントが行動を決定する戦略を方針と呼ぶならば、環境が次の状態と報酬を決定することをMDPmarkov decision process, マルコフ決定プロセスと言います。エージェントと環境の相互作用を図で表すと次のようになります。\nエピソード エージェントと環境が相互作用しながら決定された状態、行動、報酬の数列を経路trajectory, 軌跡または履歴historyと言います。経路が有限の場合をepisode taskと言います。上で例に挙げた試験期間、囲碁、格子モデルもこれに該当します。\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{T-1}, s_{T}, r_{T} \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{T-1}}{\\to} (s_{T}, r_{T}) $$\n経路が無限の場合をcontinuing taskと言います。ただし、非常に長い時間にわたって続くエピソードは無限の場合とみなされることもあります。\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{t-1}, s_{t}, r_{t}, a_{t}, s_{t+1}, r_{t+1},\\dots \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{t-1}}{\\to} (s_{t}, r_{t}) \\overset{a_{t}}{\\to} (s_{t+1}, r_{t+1}) \\overset{a_{t+1}}{\\to} \\cdots $$\nオ・イルソク, 機械学習(MACHINE LEARNING). 2017, p466-480\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3029,"permalink":"https://freshrimpsushi.github.io/jp/posts/3029/","tags":null,"title":"머신러닝에서 강화학습이란"},{"categories":"수리물리","contents":"定義 スカラー関数 $f=f(x,y,z)$に対して、以下のようなベクトル関数を $f$のグラディエントgradient, 勾配と定義し、$\\nabla f$と表記する。\n$$ \\nabla f := \\frac{ \\partial f}{ \\partial x }\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) $$\n説明 グラディエントは勾配、坂、水勾配などと翻訳される。坂、水勾配はグラディエントの古い翻訳で、最近ではあまり使われない。また、坂は勾配の漢字語であるため、勾配と同じ意味である。グラディエントは実際にベクトルであるため、勾配という言葉はグラディエントが持つ意味をすべて含むには不十分であるように思われる。生しらす寿司店では、勾配という言葉の代わりにグラディエントと統一する。\n幾何学的には $\\nabla f$は $f$が最も急激に変化する方向を意味する。つまり点 $(x,y,z)$で $f$の増加率が最も大きい方向はベクトル $\\left( \\dfrac{\\partial f(x,y,z)}{\\partial x}, \\dfrac{\\partial f(x,y,z)}{\\partial y}, \\dfrac{\\partial f(x,y,z)}{\\partial z} \\right)$であるということである。これは微分係数を多次元に拡張したものに過ぎない。$f$が増加していれば微分係数が正、$f$が減少していれば微分係数が負であるという概念と同じである。\n一方で定義で $\\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right)$という値を $\\nabla f$と表記するとしたことに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体に何か意味を持つと考えると$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積や外積と誤解するのにちょうどよい。したがって、$\\nabla$は単なる便利な表記法としてのみ理解するべきであり、グラディエント、ダイバージェンス、カールをまとめてデル演算子と呼んだり、デル演算子=グラディエントと考える方が良いかもしれない。詳細は以下で続く。\n注意点 $\\nabla f$は $\\nabla$と $f$の積ではない グラディエントを理解する上で重要なのは、$\\nabla f$がベクトル $\\nabla = (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})$とスカラー $f$の積ではないという事実である。もちろん、そう考えると直感的で良さそうだが、実際は逆である。$\\nabla$を $(\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\\npartial y}, \\frac{\\partial }{\\partial z})$というベクトルとして説明することで、ベクトルとスカラーの積のように見えるようにするのである。もし $\\nabla f$がベクトル $\\nabla$とスカラー $f$の積であれば、ベクトルとスカラーの積は交換可能であるため、次のような奇妙な数式が成り立つことになる。\n$$ \\nabla f = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) \\overset{?}{=} \\left( f\\dfrac{\\partial }{\\partial x}, f\\dfrac{\\partial }{\\partial y}, f\\dfrac{\\partial }{\\partial z} \\right) = f\\nabla $$\nこの奇妙な数式が飛び出したのは、実際には $\\nabla$はベクトルではなく、$\\nabla f$はベクトルとスカラーの積ではないためである。$\\nabla$はベクトルではなく、$f(x,y,z)$というスカラー関数を $\\left( \\frac{\\partial f(x,y,z)}{\\partial x}, \\frac{\\partial f(x,y,z)}{\\partial y}, \\frac{\\partial f(x,y,z)}{\\partial z} \\right)$というベクトル関数に対応させる演算子である。関数自体を変数とする $\\operatorname{grad}$という関数を次のように定義してみよう。\n$$ \\begin{equation} \\operatorname{grad} (f) = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right), \\quad f=f(x,y,z) \\end{equation} $$\nこの定義から、ベクトルとスカラーの積という説明は必要ない。$\\operatorname{grad}$は単に変数として $f$が入力されると、$(1)$の規則に従って関数値を持つ関数（演算子）に過ぎない。しかし $\\operatorname{grad} (f)$の関数値をよく見ると、$\\operatorname{grad} = \\nabla$と表記し、これを $\\nabla = (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})$というベクトルとして説明すると直感的で便利な表記法になるのである。\nこれは本質的な意味を正確に説明するものではないが、計算や理解の便利さのために使われる他の表記法には微分のライプニッツ表記法がある。$\\dfrac{dy}{dx}$という表記法を採用し、分数のように扱うと、変化率という意味を理解するのに便利で、無意識に掛け算や約分などの計算をしても実際の結果とピタリと合う。しかし、皆さんは $\\dfrac{dy}{dx}$は分数ではないことを知っている。そう見えるだけで、そう扱うと計算が便利なだけである。$\\nabla f$も同様に、ベクトルとスカラーの積に見えるだけで、そう扱うと計算が便利なのであって、実際にそうであるわけではない。\nでは $f\\nabla$は何か？ 上の説明に従えば、$\\nabla$は一つの関数であるため、$\\nabla f = \\nabla(f)$は $\\nabla$という関数に $f$という変数を代入したときに得られる関数値である。一方で $f \\nabla$はそれ自体が一つの関数であり、$g$という関数を変数として代入したときに以下のように関数値を対応させる関数（演算子）である。\n$$ (f\\nabla) (g) = f\\left( \\dfrac{\\partial g}{\\partial x}, \\dfrac{\\partial g}{\\partial y}, \\dfrac{\\partial g}{\\partial z} \\right) = \\left( f\\dfrac{\\partial g}{\\partial x}, f\\dfrac{\\partial g}{\\partial y}, f\\dfrac{\\partial g}{\\partial z} \\right) $$\nもちろん、$f \\nabla g$\nという関数値を見たときには、$f \\nabla$に $g$を代入したものと考えても良いし、スカラー関数 $f$とベクトル関数 $\\nabla g$の積と見ても良い。\n導出 1次元 上の図を見よう。$f_{1}$の点 $x=2$での微分係数は $4$である。$4$という値は関数 $f_{1}$が点 $x=2$でどれほど傾いているかを教えてくれる量だけでなく、それだけではない。$4$の前にある $+$という符号が $f_{1}$のグラフは $x$が増加する方向に増加するという事実も教えてくれる。したがって、微分係数 $4$は単なるスカラーではなく、1次元ベクトル $4\\hat{\\mathbf{x}}$として理解すべきである。\n同様に、$f_{2}$の $x=2$での微分係数は $-3$であり、これは傾きの程度が $3$であることと、$x$が増加する方向に進むと $f_{2}$のグラフが減少するという意味も含んでいる。つまり、符号を方向と考えた場合、微分係数の方向は関数のグラフが大きくなる方向を向いているという話である。別の言い方をすると、微分係数が指し示す方向に進めば、グラフの頂点を見つけることができるということである。\n3次元に拡張する前に、$y$の $x$での微分係数 $\\dfrac{ d y}{ d x}=a$をまるで分数のように扱えることを思い出そう。これは微分を数学的に厳密に扱う方法ではないが、幾何学的な意味を理解する上での助けとなり、その利点がある。ライプニッツは $dy$、$dx$を $y$と $x$の非常に小さな変化量、微分素と考え、その変化量の比率を微分係数と呼んだ。1\n$$ dy=adx $$\n余談だが、このように考えるとなぜ $a$を微分 \u0026lsquo;係数\u0026rsquo;と呼ぶのか理解できる。\n3次元 ここで3次元スカラー関数 $f=f(x,y,z)$と位置ベクトル $\\mathbf{r}=x\\hat{\\mathbf{x}}+y\\hat{\\mathbf{y}}+z\\hat{\\mathbf{z}}$が与えられたとしよう。$f$の変化量は全微分で表される。\n$$ \\begin{equation} df=\\frac{ \\partial f}{ \\partial x }dx + \\frac{ \\partial f}{ \\partial y}dy+\\frac{ \\partial f}{ \\partial z}dz \\end{equation} $$\n$\\mathbf{r}$の変化量は以下のようである。\n$$ d\\mathbf{r}=dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}} $$\nこれで1次元の時と同じように、$df$と $d\\mathbf{r}$の間の比率を表す何かを探してみよう。しかし、$df$はスカラーで $d\\mathbf{r}$はベクトルであるため、その \u0026lsquo;何か\u0026rsquo;はベクトルであり、$df$はその何かと $d\\mathbf{r}$の内積として表現されることを想像できる。したがって\n、とりあえずその何かを $\\mathbf{a}=a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}}$と表記して、以下のように表現してみよう。\n$$ \\begin{align*} df=\\mathbf{a}\\cdot d\\mathbf{r}\u0026amp;=(a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}})\\cdot(dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}}) \\\\ \u0026amp;= a_{1}dx+a_{2}dy+a_{3}dz \\end{align*} $$\nこれを $(2)$と比較すると、以下の結果を得る。\n$$ \\mathbf{a}=\\frac{ \\partial f}{ \\partial x}\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} $$\nこれから、このベクトル $\\mathbf{a}$を $\\nabla f$と表記し、$f$のグラディエントと呼ぶことにしよう。グラディエントの方向は関数 $f$のグラフが最も大きく増加する方向を指し、その大きさはその程度を示す。\n関連する公式 線形性:\n$$ \\nabla (f + g) = \\nabla f + \\nabla g $$\n積の規則:\n$$ \\nabla{(fg)}=f\\nabla{g}+g\\nabla{f} $$ $$ \\nabla(\\mathbf{A} \\cdot \\mathbf{B}) = \\mathbf{A} \\times (\\nabla \\times \\mathbf{B}) + \\mathbf{B} \\times (\\nabla \\times \\mathbf{A})+(\\mathbf{A} \\cdot \\nabla)\\mathbf{B}+(\\mathbf{B} \\cdot \\nabla) \\mathbf{A} $$\n2次導関数:\n$$ \\nabla \\cdot (\\nabla T) = \\dfrac{\\partial^{2} T}{\\partial x^{2}} + \\dfrac{\\partial ^{2} T} {\\partial y^{2}} + \\dfrac{\\partial ^{2} T}{\\partial z^{2}} $$ $$ \\nabla \\times (\\nabla T)= \\mathbf{0} $$ $$\\nabla (\\nabla \\cdot \\mathbf{A} ) $$\n勾配の基本定理\n$$ T(b)-T(a) = \\int _{a}^{b} (\\nabla T) \\cdot d\\mathbf{l} $$\n積分公式\n$$ \\int_{\\mathcal{V}} (\\nabla T) d \\tau = \\oint_{\\mathcal{S}} T d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left[ T \\nabla^{2} U + (\\nabla T) \\cdot (\\nabla U) \\right] d \\tau = \\oint_{\\mathcal{S}} (T \\nabla U) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left( T \\nabla^{2} U - U \\nabla^{2} T \\right) d \\tau = \\oint_{\\mathcal{S}} \\left( T \\nabla U - U \\nabla T \\right) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\n部分積分\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$ $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n一緒に見る デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ https://pomp.tistory.com/941?category=37772\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1778,"permalink":"https://freshrimpsushi.github.io/jp/posts/1778/","tags":null,"title":"3次元デカルト座標系におけるスカラー関数の勾配"},{"categories":"수리물리","contents":"定義 ベクトル関数 $\\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\\hat{\\mathbf{x}} + F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$に対して、以下のようなベクトルを$\\mathbf{F}$のカールcurlと定義し、$\\nabla \\times \\mathbf{F}$と表記する。\n$$ \\begin{align} \\nabla \\times \\mathbf{F} \u0026amp;= \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} \\label{def1} \\\\ \u0026amp;=\\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z}\\end{vmatrix} \\label{def2} \\end{align} $$\n$(2)$は$\\mathbf{F}$のカールを簡単に覚えるための公式である。行列式と考えてそのまま展開すればよい。 説明 カールは回転と翻訳される。しかし、回転という言葉は日常的すぎる上に、カールではなくrotationと誤解される可能性があるため、생새우초밥집では回転の代わりにカールを使用する。\n$\\nabla \\times \\mathbf{F}$は$\\mathbf{F}$という物理量がどの方向に回転しているかを教えてくれるベクトルである。$\\nabla \\times \\mathbf{F}$の方向を軸(親指)にして右手の法則を適用すると、右手が包む方向と$\\mathbf{F}$が回転する方向が一致する。ベクトル$\\nabla \\times \\mathbf{F}$の大きさは回転の程度を示す。\nアインシュタインの表記法とレヴィ-チヴィタ記号を使用すれば、以下のように表すことができる。$\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$と表記するなら、\n$$ \\nabla \\times \\mathbf{F} = \\epsilon_{ijk}\\hat{\\mathbf{e}}_{i}\\nabla_{j}F_{k} $$\n一方、定義で$(1)$という値を$\\nabla \\times \\mathbf{F}$と表記するとしたことに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体が何かの意味を持つと考えると、$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積や外積と誤解することになりかねない。したがって、$\\nabla$は便利な表記法程度にしか理解してはならず、勾配、ダイバージェンス、カールをまとめてデル演算子と呼ぶこともあるし、むしろデル演算子=勾配と考える方が良いかもしれない。詳細は以下で続く。\n注意点 $\\nabla \\times \\mathbf{F}$は$\\nabla$と$\\mathbf{F}$の外積ではない $\\nabla \\times \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の外積ではない。\r単に$\\nabla \\times \\mathbf{F}$は$\\mathbf{F}$に関する何らかの情報を含むベクトルである。$\\nabla$を$\\nabla = \\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} + \\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}} + \\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}}$のようなベクトルと考えて計算すると、結果が$(1)$と完全に一致するため、便宜上$\\nabla \\times \\mathbf{F}$と表記しているだけである。もし$\\nabla$を実際のベクトルと仮定すると、おかしな結果になる。\n二つのベクトル$\\mathbf{A}, \\mathbf{B}$に対して次の式が成り立つ。\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\nもし$\\nabla$が本当にベクトルだったら、上の公式に代入することができ、次の結果が得られるだろう。\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=(\\mathbf{F} \\cdot \\nabla)\\nabla - (\\nabla \\cdot \\nabla)\\mathbf{F} + \\nabla (\\nabla \\cdot \\mathbf{F}) - \\mathbf{F} (\\nabla \\cdot \\nabla) $$\nしかし、正しい結果は次のようになる。\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=\\nabla(\\nabla \\cdot \\mathbf{F})-\\nabla ^{2} \\mathbf{F} $$\n他にも例がある。ベクトルの外積は反交換性を持つため、$\\nabla \\times \\mathbf{F}$が外積であるならば、次の式が成り立つはずだ。\n$$ \\nabla \\times \\mathbf{F} \\overset{?}{=} - \\mathbf{F} \\times \\nabla $$\nしたがって、$\\nabla$はベクトルではなく、$\\nabla \\times \\mathbf{F}$を$\\nabla$と$\\mathbf{F}$の外積ではないことが分かる。ベクトルではなく、$\\nabla \\times$自体を一つの関数と考えるべきだ。このように関数を変数とする関数を物理学では演算子と呼ぶ。\nでは $\\nabla \\times \\mathbf{F}$と$\\mathbf{F} \\times \\nabla$の違いは？ $\\nabla \\times$はベクトル関数を変数とする、次のように定義される演算子である。\n$$ \\nabla \\times (\\mathbf{F}) = \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} $$\nつまり $\\nabla \\times \\mathbf{F}$は $\\nabla \\times$という演算子（関数）に$\\mathbf{F}$という変数を代入したときの関数値である。もちろんこれは再び$(x,y,z)$を変数とするベクトル関数である。$\\nabla \\times \\mathbf{F}$が$\\nabla \\times$の関数値であるのに対し、$\\mathbf{F} \\times \\nabla$はそれ自体が一つの演算子である。よく使われる数式ではないが、定義するなら次のような微分演算子であると言える。\n$$ \\begin{align*} \\mathbf{F} \\times \\nabla \u0026amp;= \\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\end{vmatrix} \\\\ \u0026amp;= \\left( F_{y}\\dfrac{ \\partial }{ \\partial z} - F_{z}\\dfrac{ \\partial }{ \\partial y} \\right)\\hat{\\mathbf{x}} + \\left( F_{z}\\dfrac{ \\partial }{ \\partial x} - F_{x}\\dfrac{ \\partial }{ \\partial z} \\right)\\hat{\\mathbf{y}} + \\left( F_{x}\\dfrac{ \\partial }{ \\partial y} - F_{y}\\dfrac{ \\partial }{ \\partial x} \\right)\\hat{\\mathbf{z}} \\end{align*} $$\n導出 ここで、ベクトル関数が回転する方向（時計回りか反時計回りか）を示す関数について考えてみましょう。重要なのは、回転面内のどの方向も回転の方向を特定できないということです。下の図を見てください。\nベクトル $-\\hat{\\mathbf{x}}$は点 $A$での動きは説明できますが、$B$での動きは説明できません。 ベクトル $\\hat{\\mathbf{y}}$は点 $C$での動きは説明できますが、$D$での動きは説明できません。 ベクトル $\\hat{\\mathbf{x}} + \\hat{\\mathbf{y}}$は経路 $F$を説明できますが、$G$を説明できません。 これは時計回りの場合にも同じです。回転方向を特定するためには回転面を離れる必要があることが理解できるでしょう。実際、これを決定するための良い方法が既にあります。それは、右手の法則を使うことです。右手が巻き込む方向の回転軸を親指の方向として決定します。したがって、$xy$平面で反時計回りに回る回転の軸（方向）は$\\hat{\\mathbf{z}}$であり、時計回りに回る回転の軸（方向）は$-\\hat{\\mathbf{z}}$です。\nそれでは、$\\mathbf{F}$が$xy$平面で反時計回りに回っている場合、$\\hat{\\mathbf{z}}$方向を示す値、つまり正の値を見つけてみましょう。回転は簡単に以下のように四角形で表現しましょう。\n経路①は点 $a$から点 $b$まで動き、$\\mathbf{F}(a) = (1,0,0)$, $\\mathbf{F}(b) = (0,1,0)$としましょう。すると、点 $a$から点 $b$まで$x$は$+1$だけ変化し、$F_{y}$も$+1$だけ変化するので、次のようになります。\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 $$\n同様に、点 $b$から点 $c$までの経路で、$y$は$+1$だけ変化し、$F_{x}$は$-1$だけ変化します。4つの経路すべてを確認すると、\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 \\quad \\text{in path $\\textcircled{1}$, $\\textcircled{3}$} $$\n$$ \\dfrac{\\partial F_{x}}{\\partial y} \\lt 0 \\quad \\text{in path $\\textcircled{2}$, $\\textcircled{4}$} $$\nしたがって、上記のように反時計回りに回転するベクトル $\\mathbf{F}$に対して、以下の値は常に正です。\n$$ \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\gt 0 $$\n逆に、$\\mathbf{F}$が時計回りに回転している場合、上記の値は常に負です。それでは、ベクトル関数 $\\mathbf{F}$を代入すると、$xy$平面で回転する方向と大きさを示す演算子 $\\operatorname{curl}_{xy}$を次のように定義できます。\n$$ \\operatorname{curl}_{xy} (\\mathbf{F}) = \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right) \\hat{\\mathbf{z}} $$\nこの関数の $\\hat{\\mathbf{z}}$ 成分の符号は、$\\mathbf{F}$が$xy$平面で回転する方向を示す。 $+$の場合、$\\mathbf{F}$は$xy$平面で反時計回りに回転する。 $-$の場合、$\\mathbf{F}$は$xy$平面で時計回りに回転する。 $0$の場合、回転しない。 この関数の $\\hat{\\mathbf{z}}$ 成分の大きさは、$\\mathbf{F}$が$xy$平面でどれだけ速く回転しているかを示す。 このような議論を$yz$平面と$zx$平面にも適用することで、$\\mathbf{F}$が3次元空間で回転している方向と大きさを示すベクトル$\\nabla \\times \\mathbf{F}$を次のように定義することができます。\n$$ \\nabla \\times \\mathbf{F} := \\left( \\dfrac{\\partial F_{z}}{\\partial y} - \\dfrac{\\partial F_{y}}{\\partial z} \\right)\\hat{\\mathbf{x}} + \\left( \\dfrac{\\partial F_{x}}{\\partial z} - \\dfrac{\\partial F_{z}}{\\partial x} \\right)\\hat{\\mathbf{y}} + \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right)\\hat{\\mathbf{z}} $$\n■\n関連する公式 リニアリティ: $$ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) = \\nabla \\times \\mathbf{A} + \\nabla \\times \\mathbf{B} $$\n乗算規則:\n$$ \\nabla \\times (f\\mathbf{A}) = f(\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\times (\\nabla f) $$\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\n二次関数:\n$$ \\nabla \\times (\\nabla f) = \\mathbf{0} $$\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F}) = \\nabla (\\nabla \\cdot \\mathbf{F}) - \\nabla^{2} \\mathbf{F} $$\nストークスまとめ $$ \\int_{\\mathcal{S}} (\\nabla \\times \\mathbf{v} )\\cdot d\\mathbf{a} = \\oint_{\\mathcal{P}} \\mathbf{v} \\cdot d\\mathbf{l} $$\n積分式 $$ \\int_{\\mathcal{V}} (\\nabla \\times \\mathbf{v}) d \\tau = - \\oint_{\\mathcal{S}} \\mathbf{v} \\times d \\mathbf{a} $$\n$$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\n部分積分 $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n$$ \\int_{\\mathcal{V}} \\mathbf{B} \\cdot \\left( \\nabla \\times \\mathbf{A} \\right) d\\tau = \\int_{\\mathcal{V}} \\mathbf{A} \\cdot \\left( \\nabla \\times \\mathbf{B} \\right) d\\tau + \\oint_{\\mathcal{S}} \\left( \\mathbf{A} \\times \\mathbf{B} \\right) \\cdot d \\mathbf{a} $$\n証明 線形性 アインシュタイン表記法, レヴィ・チビタ記号を使います。$\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$ とすると、\n$$ \\begin{align*} \\left[ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) \\right]_{i} \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (\\mathbf{A} + \\mathbf{B})_{k} \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (A_{k} + B_{k}) \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j}A_{k} + \\epsilon_{ijk} \\nabla_{j}B_{k} \\\\ \u0026amp;= [\\nabla \\times \\mathbf{A}]_{i} + [\\nabla \\times \\mathbf{B}]_{i} \\\\ \\end{align*} $$\n第三の等号は、$\\dfrac{\\partial (A_{k} + B_{k})}{\\partial x_{j}} = \\dfrac{\\partial A_{k}}{\\partial x_{j}} + \\dfrac{\\partial B_{k}}{\\partial x_{j}}$であるため成立します。\n■\n参照 デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ ","id":1752,"permalink":"https://freshrimpsushi.github.io/jp/posts/1752/","tags":null,"title":"3次元デカルト座標系におけるベクトル関数のカール(回転)"},{"categories":"거리공간","contents":"定義 $a_i, b_i \\in \\mathbb{R} (1 \\le i \\le k)$に対して、集合$I=[a_{1}, b_{1}] \\times [a_{2}, b_{2}] \\times \\cdots \\times [a_{k}, b_{k}]$を**$k$-セル**と言う。ここで$\\times$は集合のデカルト積である。\n定理1 $\\mathbb{R}$上の閉区間の数列$\\left\\{ I_{n} \\right\\}$が$I_{n} \\supset I_{n+1}\\ (n=1,2,\\cdots)$を満たすとする。すると以下が成立する。\n$$ \\bigcap_{i=1}^{\\infty}I_{n}\\ne \\varnothing $$\n証明 $I_{n}=[a_{n}, b_{n}]$とする。そして$E=\\left\\{ a_{n} : n=1,2,\\cdots \\right\\}$とする。すると$E\\ne \\varnothing$であり、$b_{1}$1によって上限がある。今$x=\\sup E$とする。そして任意の二つの正数$m$、$n$に対して\n$$ a_{n} \\le a_{m+n} \\le b_{m+n} \\le b_{m} $$\nが成立するので、すべての$n$に対して$x\\le b_{n}$である。また$x$が$E$の上限であるため、すべての$n$に対して$a_{n} \\le x$であることは明らかである。したがって、すべての$n$に対して$a_{n}\\le x \\le b_{n}$なので、$x\\in I_{n}\\ \\forall n$である。したがって\n$$ x\\in \\bigcap _{i=1}^{n}I_{n} $$\n■\n定理2 $\\left\\{ I_{n} \\right\\}$が$I_{n}\\supset I_{n+1}(n=1,2,\\cdots)$を満たす$k-$セルの数列であるとする。すると$\\bigcap_{i=1}^{n}I_{n}\\ne\\varnothing$である。\n定理2は定理1を$\\mathbb{R}^{k}$に拡張したものである。\n証明 $I_{n}$を以下のようにする。\n$$ I_{n}=\\left\\{ \\mathbf{x}=(x_{1},\\cdots,x_{k}) : a_{n,j} \\le x_{j} \\le b_{nj},\\quad(1\\le j \\le k;\\ n=1,2,\\cdots) \\right\\} $$\nすなわち$I_{n}=I_{n,1}\\times \\cdots\\times I_{n,k}\\ (I_{n,j}=[a_{n,j},b_{n,j}])$である。すると定理1によって、それぞれの$I_{n,j}$に対して$x_{j}^{\\ast}\\in I_{n,j} \\ (a_{n,j} \\le x_{j}^{\\ast} \\le b_{n,j})$が存在する。したがって\n$$ \\mathbf{x^{\\ast}} =(x_{1}^{\\ast},\\cdots ,x_{k}^{\\ast})\\in I_{n} ,\\quad (n=1,2,\\cdots) $$\n■\n定理3 すべての$k-$セルはコンパクトである。\n証明 $I$を以下のような任意の$k$-セルとする。\n$$ I=I^{1}\\times \\cdots \\times I^{k}=[a_{1},b_{1}]\\times \\cdots \\times [a_{k},b_{k}] $$\nそして以下のようにする。\n$$ \\mathbf{x}=(x_{1},\\cdots,x_{k}) \\quad \\text{and} \\quad a_{j} \\le x_{j} \\le b_{j}(1\\le j \\le k) $$\n今$\\delta$を以下のようにする。\n$$ \\delta =\\left( \\sum \\limits_{j=1}^{k}(b_{j})-a_{j})^{2} \\right)^{{\\textstyle \\frac{1}{2}}}=|\\mathbf{b}-\\mathbf{a}| $$\nこのとき$\\mathbf{a}=(a_{1},\\cdots,a_{n})$、$\\mathbf{b}=(b_{1},\\cdots,b_{n})$である。すると$\\delta$は$\\mathbf{b}$と$\\mathbf{a}$の間の距離と同じである。したがって\n$$ |\\mathbf{x}-\\mathbf{y}| \\le \\delta \\quad \\forall \\mathbf{x},\\mathbf{y}\\in I $$\nが成立する。今から証明が本格的に始まるが、背理法を使用する。つまり$k-$セルがコンパクトでないと仮定する。するとコンパクトの定義によって、$I$のいくつかのオープンカバー$\\left\\{ O_{\\alpha} \\right\\}$が有限部分カバーを持たないと仮定することと同じである。$c_{j}=(a_{j}+b_{j})/2$とする。すると$c_{j}$を使って各$I^{j}$を$[a_{j},c_{j}]$、$[c_{j},b_{j}]$に分けて$2^{k}$個の$1-$セルを作ることができる。これらの和集合は当然$I$になり、仮定によりこれらの中で少なくとも一つは$\\left\\{ O_{\\alpha} \\right\\}$のいくつかの有限部分カバーでカバーされなければならない。そのセルを$I_{1}$とする。すると$I$から$I_{1}$を選んだのと同じ方法で続けて区間を選ぶと、以下の三つの規則を満たす数列$\\left\\{ I_{n} \\right\\}$を得ることができる。\n$(\\mathrm{i})$ $I\\supset I_{1} \\supset I_{2}\\supset \\cdots$\n$(\\mathrm{ii})$ それぞれの$I_{n}$は$\\left\\{ O_{\\alpha} \\right\\}$のいくつかの有限部分カバーでもカバーされない。\n$(\\mathrm{iii})$ $|\\mathbf{x}-\\mathbf{y}|\\le 2^{-n}\\delta,\\quad \\forall \\mathbf{x},\\mathbf{y}\\in I_{n}$\nすると$(\\mathrm{i})$と定理2によって、すべての$n$に対して$\\mathbf{x}^{\\ast}\\in I_{n}$である$\\mathbf{x}^{\\ast}$が存在する。すると$\\left\\{ O_{\\alpha} \\right\\}$が$I$のオープンカバーであるため、いくつかの$\\alpha$に対して$\\mathbf{x}^{\\ast\n}\\in O_{\\alpha}$が成立する。$O_{\\alpha}$が開集合であるため、$|\\mathbf{x}^{\\ast}-\\mathbf{y}|\u0026lt;r \\implies \\mathbf{y}\\in O_{\\alpha}$を満たす$r\u0026gt;0$が存在する。一方で、$n$を十分大きくして$2^{-n}\\delta\u0026lt;r$を満たすようにすることができる。すると$(\\mathrm{iii})$によって$I_{n}\\subset O_{\\alpha}$である。しかし、これは$(\\mathrm{ii})$と矛盾するので、仮定が間違っていることがわかる。したがって、すべての$k-$セルはコンパクトである。\n■\n上記の事実から以下の有用な定理を証明することができる。\nユークリッド空間でコンパクトである同値条件 実数（または複素数）空間の部分集合$E\\subset \\mathbb{R}^{k}(\\mathrm{or}\\ \\mathbb{C}^{k})$に対して、以下の三つの命題は同値である。\n(a) $E$は閉じており有界である。\n(b) $E$はコンパクトである。\n(c) $E$のすべての無限部分集合は集積点 $p \\in E$を持つ。\nここで**(a)、(b)が同値であることはハイネ・ボレルの定理と呼ばれる。(c)を満たす$E$に対して\u0026rsquo;$E$は\u0026rsquo;集積点コンパクトである\u0026rsquo;または\u0026rsquo;$E$は\u0026rsquo;ボルツァーノ-ワイエルシュトラスの性質を持つ\u0026rsquo;と言う。(b)と(c)**が同値であることは距離空間では成立するが、位相空間では一般的には成立しない。\n証明 (a) $\\implies$ (b)\n**(a)**を仮定すると、$E \\subset I$を満たす$k-$セル$I$が存在する。すると$I$がコンパクトであり、コンパクト集合の閉じた部分集合はコンパクトであるため、$E$はコンパクトである。\n(b) $\\implies$ (c)\n背理法で証明する。\n$S$がコンパクト集合$E$の無限部分集合であるとする。そして$S$の集積点が存在しないと仮定する。するとすべての$p\\in E$は、せいぜい$S$の点をただ一つだけ含む$p$の近傍$N_{p}$を持つ。$p \\in S$の場合、そのただ一つの点は$p$である。そしてこれは、オープンカバー$\\left\\{ N_{p} \\right\\}$が$S$をカバーする有限部分カバーを持たないことを意味する。$S \\subset E$なので、同様に$E$をカバーする有限部分カバーも存在しない。これは$E$がコンパクトであるという仮定に矛盾するので、$S$は集積点$p \\in E$を持つ。\n(c) $\\implies$ (a)\n背理法で証明する。\npart 1. $E$は有界である\n$E$は有界ではないと仮定してみる。すると$E$は以下の不等式を満たす点$\\mathbf{x}_{n}$を含む。\n$$ |\\mathbf{x}_{n}| \u0026gt;n\\quad (n=1,2,\\cdots) $$\n今$S=\\left\\{ \\mathbf{x}_{n} : n=1,2,\\cdots\\right\\}$とする。すると$S$は無限集合であり、$\\mathbb{R}^{k}$で集積点を持たないことは明らかである。これは$(c)$に対する矛盾である。したがって$E$は有界である。\npart 2. $E$は閉じている。\n$E$は閉じていないと仮定してみる。すると定義により$E$に含まれない$E$の集積点$\\mathbf{x}_{0}$が存在する。今$n=1,2,\\cdots$に対して$\\mathbf{x}_{n} \\in E$を以下の条件を満たす点とする。\n$$ \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \u0026lt; {\\textstyle \\frac{1}{n}} $$\nそしてこのような$\\mathbf{x}_{n}$の集合を$S$とする。すると$S$は無限集合であり、$\\mathbf{x}_{0}$を集積点として持つ。今$\\mathbf{x}_{0}$が$S$の唯一の集積点であれば、$\\mathbf{x}_{0}\\notin E$であるため$(c)$に矛盾し、$E$は閉じていることがわかる。それでは$\\mathbf{y} \\ne \\mathbf{x}_{0}$である$\\mathbf{y} \\in \\mathbb{R}^{k}$を考える。すると\n$$ \\begin{align*} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \u0026amp; \\ge \\left|\\mathbf{x}_{0} - \\mathbf{y} \\right| - \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \\\\ \u0026amp; \\ge \\left| \\mathbf{x}_{0} - \\mathbf{y} \\right| -\\frac{1}{n} \\end{align*} $$\nこのとき十分に大きな$n$に対して以下の式が成立する。\n$$ \\begin{equation} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \\ge \\left| \\mathbf{x}_{0}- \\mathbf{y} \\right|-\\frac{1}{n} \\ge \\frac{1}{2}\\left|\\mathbf{x}_{0}-\\mathbf{y} \\right| \\label{eq1} \\end{equation} $$\nまた$\\mathbf{x}_{n\n}$の条件により、$n$が大きくなるにつれて$\\mathbf{x}_{n}$は$\\mathbf{x}_{0}$に近づく。この事実と$\\eqref{eq1}$により、$n$を続けて大きくすると$\\mathbf{y}$を含まない$\\mathbf{y}$の近傍を見つけることができる。したがって$\\mathbf{y}$は$S$の集積点ではなく、$\\mathbf{x}_{0}$が$S$の唯一の集積点であることから$(c)$に矛盾し、$E$は閉じている。\n■\nボルツァーノ-ワイエルシュトラスの定理 $\\mathbb{R}^{k}$のすべての有界な無限部分集合は集積点$p \\in \\mathbb{R}^{k}$を持つ。\n証明 $E$を$\\mathbb{R}^{k}$の有界な無限部分集合とする。すると$E$が有界であるため、$E \\subset I$を満たす$k-$セル$I$が存在する。$k-$セルはコンパクトであるため、$I$はコンパクトである。すると$I$がコンパクトである同値条件$(b)\\implies (c)$によって、$E$は集積点$p \\in I \\subset \\mathbb{R}^{k}$を持つ。\n■\n任意の$b_{n}$で問題ない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1711,"permalink":"https://freshrimpsushi.github.io/jp/posts/1711/","tags":null,"title":"すべてのk-cellはコンパクトである：ユークリッド空間でコンパクトである同値条件。"},{"categories":"정수론","contents":"素数 1万番目までの素数のリストである。\nダウンロード\r2 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97 101 103 107 109 113 127 131 137 139 149 151 157 163 167 173 179 181 191 193 197 199 211 223 227 229 233 239 241 251 257 263 269 271 277 281 283 293 307 311 313 317 331 337 347 349 353 359 367 373 379 383 389 397 401 409 419 421 431 433 439 443 449 457 461 463 467 479 487 491 499 503 509 521 523 541 547 557 563 569 571 577 587 593 599 601 607 613 617 619 631 641 643 647 653 659 661 673 677 683 691 701 709 719 727 733 739 743 751 757 761 769 773 787 797 809 811 821 823 827 829 839 853 857 859 863 877 881 883 887 907 911 919 929 937 941 947 953 967 971 977 983 991 997 1009 1013 1019 1021 1031 1033 1039 1049 1051 1061 1063 1069 1087 1091 1093 1097 1103 1109 1117 1123 1129 1151 1153 1163 1171 1181 1187 1193 1201 1213 1217 1223 1229 1231 1237 1249 1259 1277 1279 1283 1289 1291 1297 1301 1303 1307 1319 1321 1327 1361 1367 1373 1381 1399 1409 1423 1427 1429 1433 1439 1447 1451 1453 1459 1471 1481 1483 1487 1489 1493 1499 1511 1523 1531 1543 1549 1553 1559 1567 1571 1579 1583 1597 1601 1607 1609 1613 1619 1621 1627 1637 1657 1663 1667 1669 1693 1697 1699 1709 1721 1723 1733 1741 1747 1753 1759 1777 1783 1787 1789 1801 1811 1823 1831 1847 1861 1867 1871 1873 1877 1879 1889 1901 1907 1913 1931 1933 1949 1951 1973 1979 1987 1993 1997 1999 2003 2011 2017 2027 2029 2039 2053 2063 2069 2081 2083 2087 2089 2099 2111 2113 2129 2131 2137 2141 2143 2153 2161 2179 2203 2207 2213 2221 2237 2239 2243 2251 2267 2269 2273 2281 2287 2293 2297 2309 2311 2333 2339 2341 2347 2351 2357 2371 2377 2381 2383 2389 2393 2399 2411 2417 2423 2437 2441 2447 2459 2467 2473 2477 2503 2521 2531 2539 2543 2549 2551 2557 2579 2591 2593 2609 2617 2621 2633 2647 2657 2659 2663 2671 2677 2683 2687 2689 2693 2699 2707 2711 2713 2719 2729 2731 2741 2749 2753 2767 2777 2789 2791 2797 2801 2803 2819 2833 2837 2843 2851 2857 2861 2879 2887 2897 2903 2909 2917 2927 2939 2953 2957 2963 2969 2971 2999 3001 3011 3019 3023 3037 3041 3049 3061 3067 3079 3083 3089 3109 3119 3121 3137 3163 3167 3169 3181 3187 3191 3203 3209 3217 3221 3229 3251 3253 3257 3259 3271 3299 3301 3307 3313 3319 3323 3329 3331 3343 3347 3359 3361 3371 3373 3389 3391 3407 3413 3433 3449 3457 3461 3463 3467 3469 3491 3499 3511 3517 3527 3529 3533 3539 3541 3547 3557 3559 3571 3581 3583 3593 3607 3613 3617 3623 3631 3637 3643 3659 3671 3673 3677 3691 3697 3701 3709 3719 3727 3733 3739 3761 3767 3769 3779 3793 3797 3803 3821 3823 3833 3847 3851 3853 3863 3877 3881 3889 3907 3911 3917 3919 3923 3929 3931 3943 3947 3967 3989 4001 4003 4007 4013 4019 4021 4027 4049 4051 4057 4073 4079 4091 4093 4099 4111 4127 4129 4133 4139 4153 4157 4159 4177 4201 4211 4217 4219 4229 4231 4241 4243 4253 4259 4261 4271 4273 4283 4289 4297 4327 4337 4339 4349 4357 4363 4373 4391 4397 4409 4421 4423 4441 4447 4451 4457 4463 4481 4483 4493 4507 4513 4517 4519 4523 4547 4549 4561 4567 4583 4591 4597 4603 4621 4637 4639 4643 4649 4651 4657 4663 4673 4679 4691 4703 4721 4723 4729 4733 4751 4759 4783 4787 4789 4793 4799 4801 4813 4817 4831 4861 4871 4877 4889 4903 4909 4919 4931 4933 4937 4943 4951 4957 4967 4969 4973 4987 4993 4999 5003 5009 5011 5021 5023 5039 5051 5059 5077 5081 5087 5099 5101 5107 5113 5119 5147 5153 5167 5171 5179 5189 5197 5209 5227 5231 5233 5237 5261 5273 5279 5281 5297 5303 5309 5323 5333 5347 5351 5381 5387 5393 5399 5407 5413 5417 5419 5431 5437 5441 5443 5449 5471 5477 5479 5483 5501 5503 5507 5519 5521 5527 5531 5557 5563 5569 5573 5581 5591 5623 5639 5641 5647 5651 5653 5657 5659 5669 5683 5689 5693 5701 5711 5717 5737 5741 5743 5749 5779 5783 5791 5801 5807 5813 5821 5827 5839 5843 5849 5851 5857 5861 5867 5869 5879 5881 5897 5903 5923 5927 5939 5953 5981 5987 6007 6011 6029 6037 6043 6047 6053 6067 6073 6079 6089 6091 6101 6113 6121 6131 6133 6143 6151 6163 6173 6197 6199 6203 6211 6217 6221 6229 6247 6257 6263 6269 6271 6277 6287 6299 6301 6311 6317 6323 6329 6337 6343 6353 6359 6361 6367 6373 6379 6389 6397 6421 6427 6449 6451 6469 6473 6481 6491 6521 6529 6547 6551 6553 6563 6569 6571 6577 6581 6599 6607 6619 6637 6653 6659 6661 6673 6679 6689 6691 6701 6703 6709 6719 6733 6737 6761 6763 6779 6781 6791 6793 6803 6823 6827 6829 6833 6841 6857 6863 6869 6871 6883 6899 6907 6911 6917 6947 6949 6959 6961 6967 6971 6977 6983 6991 6997 7001 7013 7019 7027 7039 7043 7057 7069 7079 7103 7109 7121 7127 7129 7151 7159 7177 7187 7193 7207 7211 7213 7219 7229 7237 7243 7247 7253 7283 7297 7307 7309 7321 7331 7333 7349 7351 7369 7393 7411 7417 7433 7451 7457 7459 7477 7481 7487 7489 7499 7507 7517 7523 7529 7537 7541 7547 7549 7559 7561 7573 7577 7583 7589 7591 7603 7607 7621 7639 7643 7649 7669 7673 7681 7687 7691 7699 7703 7717 7723 7727 7741 7753 7757 7759 7789 7793 7817 7823 7829 7841 7853 7867 7873 7877 7879 7883 7901 7907 7919 7927 7933 7937 7949 7951 7963 7993 8009 8011 8017 8039 8053 8059 8069 8081 8087 8089 8093 8101 8111 8117 8123 8147 8161 8167 8171 8179 8191 8209 8219 8221 8231 8233 8237 8243 8263 8269 8273 8287 8291 8293 8297 8311 8317 8329 8353 8363 8369 8377 8387 8389 8419 8423 8429 8431 8443 8447 8461 8467 8501 8513 8521 8527 8537 8539 8543 8563 8573 8581 8597 8599 8609 8623 8627 8629 8641 8647 8663 8669 8677 8681 8689 8693 8699 8707 8713 8719 8731 8737 8741 8747 8753 8761 8779 8783 8803 8807 8819 8821 8831 8837 8839 8849 8861 8863 8867 8887 8893 8923 8929 8933 8941 8951 8963 8969 8971 8999 9001 9007 9011 9013 9029 9041 9043 9049 9059 9067 9091 9103 9109 9127 9133 9137 9151 9157 9161 9173 9181 9187 9199 9203 9209 9221 9227 9239 9241 9257 9277 9281 9283 9293 9311 9319 9323 9337 9341 9343 9349 9371 9377 9391 9397 9403 9413 9419 9421 9431 9433 9437 9439 9461 9463 9467 9473 9479 9491 9497 9511 9521 9533 9539 9547 9551 9587 9601 9613 9619 9623 9629 9631 9643 9649 9661 9677 9679 9689 9697 9719 9721 9733 9739 9743 9749 9767 9769 9781 9787 9791 9803 9811 9817 9829 9833 9839 9851 9857 9859 9871 9883 9887 9901 9907 9923 9929 9931 9941 9949 9967 9973 10007 10009 10037 10039 10061 10067 10069 10079 10091 10093 10099 10103 10111 10133 10139 10141 10151 10159 10163 10169 10177 10181 10193 10211 10223 10243 10247 10253 10259 10267 10271 10273 10289 10301 10303 10313 10321 10331 10333 10337 10343 10357 10369 10391 10399 10427 10429 10433 10453 10457 10459 10463 10477 10487 10499 10501 10513 10529 10531 10559 10567 10589 10597 10601 10607 10613 10627 10631 10639 10651 10657 10663 10667 10687 10691 10709 10711 10723 10729 10733 10739 10753 10771 10781 10789 10799 10831 10837 10847 10853 10859 10861 10867 10883 10889 10891 10903 10909 10937 10939 10949 10957 10973 10979 10987 10993 11003 11027 11047 11057 11059 11069 11071 11083 11087 11093 11113 11117 11119 11131 11149 11159 11161 11171 11173 11177 11197 11213 11239 11243 11251 11257 11261 11273 11279 11287 11299 11311 11317 11321 11329 11351 11353 11369 11383 11393 11399 11411 11423 11437 11443 11447 11467 11471 11483 11489 11491 11497 11503 11519 11527 11549 11551 11579 11587 11593 11597 11617 11621 11633 11657 11677 11681 11689 11699 11701 11717 11719 11731 11743 11777 11779 11783 11789 11801 11807 11813 11821 11827 11831 11833 11839 11863 11867 11887 11897 11903 11909 11923 11927 11933 11939 11941 11953 11959 11969 11971 11981 11987 12007 12011 12037 12041 12043 12049 12071 12073 12097 12101 12107 12109 12113 12119 12143 12149 12157 12161 12163 12197 12203 12211 12227 12239 12241 12251 12253 12263 12269 12277 12281 12289 12301 12323 12329 12343 12347 12373 12377 12379 12391 12401 12409 12413 12421 12433 12437 12451 12457 12473 12479 12487 12491 12497 12503 12511 12517 12527 12539 12541 12547 12553 12569 12577 12583 12589 12601 12611 12613 12619 12637 12641 12647 12653 12659 12671 12689 12697 12703 12713 12721 12739 12743 12757 12763 12781 12791 12799 12809 12821 12823 12829 12841 12853 12889 12893 12899 12907 12911 12917 12919 12923 12941 12953 12959 12967 12973 12979 12983 13001 13003 13007 13009 13033 13037 13043 13049 13063 13093 13099 13103 13109 13121 13127 13147 13151 13159 13163 13171 13177 13183 13187 13217 13219 13229 13241 13249 13259 13267 13291 13297 13309 13313 13327 13331 13337 13339 13367 13381 13397 13399 13411 13417 13421 13441 13451 13457 13463 13469 13477 13487 13499 13513 13523 13537 13553 13567 13577 13591 13597 13613 13619 13627 13633 13649 13669 13679 13681 13687 13691 13693 13697 13709 13711 13721 13723 13729 13751 13757 13759 13763 13781 13789 13799 13807 13829 13831 13841 13859 13873 13877 13879 13883 13901 13903 13907 13913 13921 13931 13933 13963 13967 13997 13999 14009 14011 14029 14033 14051 14057 14071 14081 14083 14087 14107 14143 14149 14153 14159 14173 14177 14197 14207 14221 14243 14249 14251 14281 14293 14303 14321 14323 14327 14341 14347 14369 14387 14389 14401 14407 14411 14419 14423 14431 14437 14447 14449 14461 14479 14489 14503 14519 14533 14537 14543 14549 14551 14557 14561 14563 14591 14593 14621 14627 14629 14633 14639 14653 14657 14669 14683 14699 14713 14717 14723 14731 14737 14741 14747 14753 14759 14767 14771 14779 14783 14797 14813 14821 14827 14831 14843 14851 14867 14869 14879 14887 14891 14897 14923 14929 14939 14947 14951 14957 14969 14983 15013 15017 15031 15053 15061 15073 15077 15083 15091 15101 15107 15121 15131 15137 15139 15149 15161 15173 15187 15193 15199 15217 15227 15233 15241 15259 15263 15269 15271 15277 15287 15289 15299 15307 15313 15319 15329 15331 15349 15359 15361 15373 15377 15383 15391 15401 15413 15427 15439 15443 15451 15461 15467 15473 15493 15497 15511 15527 15541 15551 15559 15569 15581 15583 15601 15607 15619 15629 15641 15643 15647 15649 15661 15667 15671 15679 15683 15727 15731 15733 15737 15739 15749 15761 15767 15773 15787 15791 15797 15803 15809 15817 15823 15859 15877 15881 15887 15889 15901 15907 15913 15919 15923 15937 15959 15971 15973 15991 16001 16007 16033 16057 16061 16063 16067 16069 16073 16087 16091 16097 16103 16111 16127 16139 16141 16183 16187 16189 16193 16217 16223 16229 16231 16249 16253 16267 16273 16301 16319 16333 16339 16349 16361 16363 16369 16381 16411 16417 16421 16427 16433 16447 16451 16453 16477 16481 16487 16493 16519 16529 16547 16553 16561 16567 16573 16603 16607 16619 16631 16633 16649 16651 16657 16661 16673 16691 16693 16699 16703 16729 16741 16747 16759 16763 16787 16811 16823 16829 16831 16843 16871 16879 16883 16889 16901 16903 16921 16927 16931 16937 16943 16963 16979 16981 16987 16993 17011 17021 17027 17029 17033 17041 17047 17053 17077 17093 17099 17107 17117 17123 17137 17159 17167 17183 17189 17191 17203 17207 17209 17231 17239 17257 17291 17293 17299 17317 17321 17327 17333 17341 17351 17359 17377 17383 17387 17389 17393 17401 17417 17419 17431 17443 17449 17467 17471 17477 17483 17489 17491 17497 17509 17519 17539 17551 17569 17573 17579 17581 17597 17599 17609 17623 17627 17657 17659 17669 17681 17683 17707 17713 17729 17737 17747 17749 17761 17783 17789 17791 17807 17827 17837 17839 17851 17863 17881 17891 17903 17909 17911 17921 17923 17929 17939 17957 17959 17971 17977 17981 17987 17989 18013 18041 18043 18047 18049 18059 18061 18077 18089 18097 18119 18121 18127 18131 18133 18143 18149 18169 18181 18191 18199 18211 18217 18223 18229 18233 18251 18253 18257 18269 18287 18289 18301 18307 18311 18313 18329 18341 18353 18367 18371 18379 18397 18401 18413 18427 18433 18439 18443 18451 18457 18461 18481 18493 18503 18517 18521 18523 18539 18541 18553 18583 18587 18593 18617 18637 18661 18671 18679 18691 18701 18713 18719 18731 18743 18749 18757 18773 18787 18793 18797 18803 18839 18859 18869 18899 18911 18913 18917 18919 18947 18959 18973 18979 19001 19009 19013 19031 19037 19051 19069 19073 19079 19081 19087 19121 19139 19141 19157 19163 19181 19183 19207 19211 19213 19219 19231 19237 19249 19259 19267 19273 19289 19301 19309 19319 19333 19373 19379 19381 19387 19391 19403 19417 19421 19423 19427 19429 19433 19441 19447 19457 19463 19469 19471 19477 19483 19489 19501 19507 19531 19541 19543 19553 19559 19571 19577 19583 19597 19603 19609 19661 19681 19687 19697 19699 19709 19717 19727 19739 19751 19753 19759 19763 19777 19793 19801 19813 19819 19841 19843 19853 19861 19867 19889 19891 19913 19919 19927 19937 19949 19961 19963 19973 19979 19991 19993 19997 20011 20021 20023 20029 20047 20051 20063 20071 20089 20101 20107 20113 20117 20123 20129 20143 20147 20149 20161 20173 20177 20183 20201 20219 20231 20233 20249 20261 20269 20287 20297 20323 20327 20333 20341 20347 20353 20357 20359 20369 20389 20393 20399 20407 20411 20431 20441 20443 20477 20479 20483 20507 20509 20521 20533 20543 20549 20551 20563 20593 20599 20611 20627 20639 20641 20663 20681 20693 20707 20717 20719 20731 20743 20747 20749 20753 20759 20771 20773 20789 20807 20809 20849 20857 20873 20879 20887 20897 20899 20903 20921 20929 20939 20947 20959 20963 20981 20983 21001 21011 21013 21017 21019 21023 21031 21059 21061 21067 21089 21101 21107 21121 21139 21143 21149 21157 21163 21169 21179 21187 21191 21193 21211 21221 21227 21247 21269 21277 21283 21313 21317 21319 21323 21341 21347 21377 21379 21383 21391 21397 21401 21407 21419 21433 21467 21481 21487 21491 21493 21499 21503 21517 21521 21523 21529 21557 21559 21563 21569 21577 21587 21589 21599 21601 21611 21613 21617 21647 21649 21661 21673 21683 21701 21713 21727 21737 21739 21751 21757 21767 21773 21787 21799 21803 21817 21821 21839 21841 21851 21859 21863 21871 21881 21893 21911 21929 21937 21943 21961 21977 21991 21997 22003 22013 22027 22031 22037 22039 22051 22063 22067 22073 22079 22091 22093 22109 22111 22123 22129 22133 22147 22153 22157 22159 22171 22189 22193 22229 22247 22259 22271 22273 22277 22279 22283 22291 22303 22307 22343 22349 22367 22369 22381 22391 22397 22409 22433 22441 22447 22453 22469 22481 22483 22501 22511 22531 22541 22543 22549 22567 22571 22573 22613 22619 22621 22637 22639 22643 22651 22669 22679 22691 22697 22699 22709 22717 22721 22727 22739 22741 22751 22769 22777 22783 22787 22807 22811 22817 22853 22859 22861 22871 22877 22901 22907 22921 22937 22943 22961 22963 22973 22993 23003 23011 23017 23021 23027 23029 23039 23041 23053 23057 23059 23063 23071 23081 23087 23099 23117 23131 23143 23159 23167 23173 23189 23197 23201 23203 23209 23227 23251 23269 23279 23291 23293 23297 23311 23321 23327 23333 23339 23357 23369 23371 23399 23417 23431 23447 23459 23473 23497 23509 23531 23537 23539 23549 23557 23561 23563 23567 23581 23593 23599 23603 23609 23623 23627 23629 23633 23663 23669 23671 23677 23687 23689 23719 23741 23743 23747 23753 23761 23767 23773 23789 23801 23813 23819 23827 23831 23833 23857 23869 23873 23879 23887 23893 23899 23909 23911 23917 23929 23957 23971 23977 23981 23993 24001 24007 24019 24023 24029 24043 24049 24061 24071 24077 24083 24091 24097 24103 24107 24109 24113 24121 24133 24137 24151 24169 24179 24181 24197 24203 24223 24229 24239 24247 24251 24281 24317 24329 24337 24359 24371 24373 24379 24391 24407 24413 24419 24421 24439 24443 24469 24473 24481 24499 24509 24517 24527 24533 24547 24551 24571 24593 24611 24623 24631 24659 24671 24677 24683 24691 24697 24709 24733 24749 24763 24767 24781 24793 24799 24809 24821 24841 24847 24851 24859 24877 24889 24907 24917 24919 24923 24943 24953 24967 24971 24977 24979 24989 25013 25031 25033 25037 25057 25073 25087 25097 25111 25117 25121 25127 25147 25153 25163 25169 25171 25183 25189 25219 25229 25237 25243 25247 25253 25261 25301 25303 25307 25309 25321 25339 25343 25349 25357 25367 25373 25391 25409 25411 25423 25439 25447 25453 25457 25463 25469 25471 25523 25537 25541 25561 25577 25579 25583 25589 25601 25603 25609 25621 25633 25639 25643 25657 25667 25673 25679 25693 25703 25717 25733 25741 25747 25759 25763 25771 25793 25799 25801 25819 25841 25847 25849 25867 25873 25889 25903 25913 25919 25931 25933 25939 25943 25951 25969 25981 25997 25999 26003 26017 26021 26029 26041 26053 26083 26099 26107 26111 26113 26119 26141 26153 26161 26171 26177 26183 26189 26203 26209 26227 26237 26249 26251 26261 26263 26267 26293 26297 26309 26317 26321 26339 26347 26357 26371 26387 26393 26399 26407 26417 26423 26431 26437 26449 26459 26479 26489 26497 26501 26513 26539 26557 26561 26573 26591 26597 26627 26633 26641 26647 26669 26681 26683 26687 26693 26699 26701 26711 26713 26717 26723 26729 26731 26737 26759 26777 26783 26801 26813 26821 26833 26839 26849 26861 26863 26879 26881 26891 26893 26903 26921 26927 26947 26951 26953 26959 26981 26987 26993 27011 27017 27031 27043 27059 27061 27067 27073 27077 27091 27103 27107 27109 27127 27143 27179 27191 27197 27211 27239 27241 27253 27259 27271 27277 27281 27283 27299 27329 27337 27361 27367 27397 27407 27409 27427 27431 27437 27449 27457 27479 27481 27487 27509 27527 27529 27539 27541 27551 27581 27583 27611 27617 27631 27647 27653 27673 27689 27691 27697 27701 27733 27737 27739 27743 27749 27751 27763 27767 27773 27779 27791 27793 27799 27803 27809 27817 27823 27827 27847 27851 27883 27893 27901 27917 27919 27941 27943 27947 27953 27961 27967 27983 27997 28001 28019 28027 28031 28051 28057 28069 28081 28087 28097 28099 28109 28111 28123 28151 28163 28181 28183 28201 28211 28219 28229 28277 28279 28283 28289 28297 28307 28309 28319 28349 28351 28387 28393 28403 28409 28411 28429 28433 28439 28447 28463 28477 28493 28499 28513 28517 28537 28541 28547 28549 28559 28571 28573 28579 28591 28597 28603 28607 28619 28621 28627 28631 28643 28649 28657 28661 28663 28669 28687 28697 28703 28711 28723 28729 28751 28753 28759 28771 28789 28793 28807 28813 28817 28837 28843 28859 28867 28871 28879 28901 28909 28921 28927 28933 28949 28961 28979 29009 29017 29021 29023 29027 29033 29059 29063 29077 29101 29123 29129 29131 29137 29147 29153 29167 29173 29179 29191 29201 29207 29209 29221 29231 29243 29251 29269 29287 29297 29303 29311 29327 29333 29339 29347 29363 29383 29387 29389 29399 29401 29411 29423 29429 29437 29443 29453 29473 29483 29501 29527 29531 29537 29567 29569 29573 29581 29587 29599 29611 29629 29633 29641 29663 29669 29671 29683 29717 29723 29741 29753 29759 29761 29789 29803 29819 29833 29837 29851 29863 29867 29873 29879 29881 29917 29921 29927 29947 29959 29983 29989 30011 30013 30029 30047 30059 30071 30089 30091 30097 30103 30109 30113 30119 30133 30137 30139 30161 30169 30181 30187 30197 30203 30211 30223 30241 30253 30259 30269 30271 30293 30307 30313 30319 30323 30341 30347 30367 30389 30391 30403 30427 30431 30449 30467 30469 30491 30493 30497 30509 30517 30529 30539 30553 30557 30559 30577 30593 30631 30637 30643 30649 30661 30671 30677 30689 30697 30703 30707 30713 30727 30757 30763 30773 30781 30803 30809 30817 30829 30839 30841 30851 30853 30859 30869 30871 30881 30893 30911 30931 30937 30941 30949 30971 30977 30983 31013 31019 31033 31039 31051 31063 31069 31079 31081 31091 31121 31123 31139 31147 31151 31153 31159 31177 31181 31183 31189 31193 31219 31223 31231 31237 31247 31249 31253 31259 31267 31271 31277 31307 31319 31321 31327 31333 31337 31357 31379 31387 31391 31393 31397 31469 31477 31481 31489 31511 31513 31517 31531 31541 31543 31547 31567 31573 31583 31601 31607 31627 31643 31649 31657 31663 31667 31687 31699 31721 31723 31727 31729 31741 31751 31769 31771 31793 31799 31817 31847 31849 31859 31873 31883 31891 31907 31957 31963 31973 31981 31991 32003 32009 32027 32029 32051 32057 32059 32063 32069 32077 32083 32089 32099 32117 32119 32141 32143 32159 32173 32183 32189 32191 32203 32213 32233 32237 32251 32257 32261 32297 32299 32303 32309 32321 32323 32327 32341 32353 32359 32363 32369 32371 32377 32381 32401 32411 32413 32423 32429 32441 32443 32467 32479 32491 32497 32503 32507 32531 32533 32537 32561 32563 32569 32573 32579 32587 32603 32609 32611 32621 32633 32647 32653 32687 32693 32707 32713 32717 32719 32749 32771 32779 32783 32789 32797 32801 32803 32831 32833 32839 32843 32869 32887 32909 32911 32917 32933 32939 32941 32957 32969 32971 32983 32987 32993 32999 33013 33023 33029 33037 33049 33053 33071 33073 33083 33091 33107 33113 33119 33149 33151 33161 33179 33181 33191 33199 33203 33211 33223 33247 33287 33289 33301 33311 33317 33329 33331 33343 33347 33349 33353 33359 33377 33391 33403 33409 33413 33427 33457 33461 33469 33479 33487 33493 33503 33521 33529 33533 33547 33563 33569 33577 33581 33587 33589 33599 33601 33613 33617 33619 33623 33629 33637 33641 33647 33679 33703 33713 33721 33739 33749 33751 33757 33767 33769 33773 33791 33797 33809 33811 33827 33829 33851 33857 33863 33871 33889 33893 33911 33923 33931 33937 33941 33961 33967 33997 34019 34031 34033 34039 34057 34061 34123 34127 34129 34141 34147 34157 34159 34171 34183 34211 34213 34217 34231 34253 34259 34261 34267 34273 34283 34297 34301 34303 34313 34319 34327 34337 34351 34361 34367 34369 34381 34403 34421 34429 34439 34457 34469 34471 34483 34487 34499 34501 34511 34513 34519 34537 34543 34549 34583 34589 34591 34603 34607 34613 34631 34649 34651 34667 34673 34679 34687 34693 34703 34721 34729 34739 34747 34757 34759 34763 34781 34807 34819 34841 34843 34847 34849 34871 34877 34883 34897 34913 34919 34939 34949 34961 34963 34981 35023 35027 35051 35053 35059 35069 35081 35083 35089 35099 35107 35111 35117 35129 35141 35149 35153 35159 35171 35201 35221 35227 35251 35257 35267 35279 35281 35291 35311 35317 35323 35327 35339 35353 35363 35381 35393 35401 35407 35419 35423 35437 35447 35449 35461 35491 35507 35509 35521 35527 35531 35533 35537 35543 35569 35573 35591 35593 35597 35603 35617 35671 35677 35729 35731 35747 35753 35759 35771 35797 35801 35803 35809 35831 35837 35839 35851 35863 35869 35879 35897 35899 35911 35923 35933 35951 35963 35969 35977 35983 35993 35999 36007 36011 36013 36017 36037 36061 36067 36073 36083 36097 36107 36109 36131 36137 36151 36161 36187 36191 36209 36217 36229 36241 36251 36263 36269 36277 36293 36299 36307 36313 36319 36341 36343 36353 36373 36383 36389 36433 36451 36457 36467 36469 36473 36479 36493 36497 36523 36527 36529 36541 36551 36559 36563 36571 36583 36587 36599 36607 36629 36637 36643 36653 36671 36677 36683 36691 36697 36709 36713 36721 36739 36749 36761 36767 36779 36781 36787 36791 36793 36809 36821 36833 36847 36857 36871 36877 36887 36899 36901 36913 36919 36923 36929 36931 36943 36947 36973 36979 36997 37003 37013 37019 37021 37039 37049 37057 37061 37087 37097 37117 37123 37139 37159 37171 37181 37189 37199 37201 37217 37223 37243 37253 37273 37277 37307 37309 37313 37321 37337 37339 37357 37361 37363 37369 37379 37397 37409 37423 37441 37447 37463 37483 37489 37493 37501 37507 37511 37517 37529 37537 37547 37549 37561 37567 37571 37573 37579 37589 37591 37607 37619 37633 37643 37649 37657 37663 37691 37693 37699 37717 37747 37781 37783 37799 37811 37813 37831 37847 37853 37861 37871 37879 37889 37897 37907 37951 37957 37963 37967 37987 37991 37993 37997 38011 38039 38047 38053 38069 38083 38113 38119 38149 38153 38167 38177 38183 38189 38197 38201 38219 38231 38237 38239 38261 38273 38281 38287 38299 38303 38317 38321 38327 38329 38333 38351 38371 38377 38393 38431 38447 38449 38453 38459 38461 38501 38543 38557 38561 38567 38569 38593 38603 38609 38611 38629 38639 38651 38653 38669 38671 38677 38693 38699 38707 38711 38713 38723 38729 38737 38747 38749 38767 38783 38791 38803 38821 38833 38839 38851 38861 38867 38873 38891 38903 38917 38921 38923 38933 38953 38959 38971 38977 38993 39019 39023 39041 39043 39047 39079 39089 39097 39103 39107 39113 39119 39133 39139 39157 39161 39163 39181 39191 39199 39209 39217 39227 39229 39233 39239 39241 39251 39293 39301 39313 39317 39323 39341 39343 39359 39367 39371 39373 39383 39397 39409 39419 39439 39443 39451 39461 39499 39503 39509 39511 39521 39541 39551 39563 39569 39581 39607 39619 39623 39631 39659 39667 39671 39679 39703 39709 39719 39727 39733 39749 39761 39769 39779 39791 39799 39821 39827 39829 39839 39841 39847 39857 39863 39869 39877 39883 39887 39901 39929 39937 39953 39971 39979 39983 39989 40009 40013 40031 40037 40039 40063 40087 40093 40099 40111 40123 40127 40129 40151 40153 40163 40169 40177 40189 40193 40213 40231 40237 40241 40253 40277 40283 40289 40343 40351 40357 40361 40387 40423 40427 40429 40433 40459 40471 40483 40487 40493 40499 40507 40519 40529 40531 40543 40559 40577 40583 40591 40597 40609 40627 40637 40639 40693 40697 40699 40709 40739 40751 40759 40763 40771 40787 40801 40813 40819 40823 40829 40841 40847 40849 40853 40867 40879 40883 40897 40903 40927 40933 40939 40949 40961 40973 40993 41011 41017 41023 41039 41047 41051 41057 41077 41081 41113 41117 41131 41141 41143 41149 41161 41177 41179 41183 41189 41201 41203 41213 41221 41227 41231 41233 41243 41257 41263 41269 41281 41299 41333 41341 41351 41357 41381 41387 41389 41399 41411 41413 41443 41453 41467 41479 41491 41507 41513 41519 41521 41539 41543 41549 41579 41593 41597 41603 41609 41611 41617 41621 41627 41641 41647 41651 41659 41669 41681 41687 41719 41729 41737 41759 41761 41771 41777 41801 41809 41813 41843 41849 41851 41863 41879 41887 41893 41897 41903 41911 41927 41941 41947 41953 41957 41959 41969 41981 41983 41999 42013 42017 42019 42023 42043 42061 42071 42073 42083 42089 42101 42131 42139 42157 42169 42179 42181 42187 42193 42197 42209 42221 42223 42227 42239 42257 42281 42283 42293 42299 42307 42323 42331 42337 42349 42359 42373 42379 42391 42397 42403 42407 42409 42433 42437 42443 42451 42457 42461 42463 42467 42473 42487 42491 42499 42509 42533 42557 42569 42571 42577 42589 42611 42641 42643 42649 42667 42677 42683 42689 42697 42701 42703 42709 42719 42727 42737 42743 42751 42767 42773 42787 42793 42797 42821 42829 42839 42841 42853 42859 42863 42899 42901 42923 42929 42937 42943 42953 42961 42967 42979 42989 43003 43013 43019 43037 43049 43051 43063 43067 43093 43103 43117 43133 43151 43159 43177 43189 43201 43207 43223 43237 43261 43271 43283 43291 43313 43319 43321 43331 43391 43397 43399 43403 43411 43427 43441 43451 43457 43481 43487 43499 43517 43541 43543 43573 43577 43579 43591 43597 43607 43609 43613 43627 43633 43649 43651 43661 43669 43691 43711 43717 43721 43753 43759 43777 43781 43783 43787 43789 43793 43801 43853 43867 43889 43891 43913 43933 43943 43951 43961 43963 43969 43973 43987 43991 43997 44017 44021 44027 44029 44041 44053 44059 44071 44087 44089 44101 44111 44119 44123 44129 44131 44159 44171 44179 44189 44201 44203 44207 44221 44249 44257 44263 44267 44269 44273 44279 44281 44293 44351 44357 44371 44381 44383 44389 44417 44449 44453 44483 44491 44497 44501 44507 44519 44531 44533 44537 44543 44549 44563 44579 44587 44617 44621 44623 44633 44641 44647 44651 44657 44683 44687 44699 44701 44711 44729 44741 44753 44771 44773 44777 44789 44797 44809 44819 44839 44843 44851 44867 44879 44887 44893 44909 44917 44927 44939 44953 44959 44963 44971 44983 44987 45007 45013 45053 45061 45077 45083 45119 45121 45127 45131 45137 45139 45161 45179 45181 45191 45197 45233 45247 45259 45263 45281 45289 45293 45307 45317 45319 45329 45337 45341 45343 45361 45377 45389 45403 45413 45427 45433 45439 45481 45491 45497 45503 45523 45533 45541 45553 45557 45569 45587 45589 45599 45613 45631 45641 45659 45667 45673 45677 45691 45697 45707 45737 45751 45757 45763 45767 45779 45817 45821 45823 45827 45833 45841 45853 45863 45869 45887 45893 45943 45949 45953 45959 45971 45979 45989 46021 46027 46049 46051 46061 46073 46091 46093 46099 46103 46133 46141 46147 46153 46171 46181 46183 46187 46199 46219 46229 46237 46261 46271 46273 46279 46301 46307 46309 46327 46337 46349 46351 46381 46399 46411 46439 46441 46447 46451 46457 46471 46477 46489 46499 46507 46511 46523 46549 46559 46567 46573 46589 46591 46601 46619 46633 46639 46643 46649 46663 46679 46681 46687 46691 46703 46723 46727 46747 46751 46757 46769 46771 46807 46811 46817 46819 46829 46831 46853 46861 46867 46877 46889 46901 46919 46933 46957 46993 46997 47017 47041 47051 47057 47059 47087 47093 47111 47119 47123 47129 47137 47143 47147 47149 47161 47189 47207 47221 47237 47251 47269 47279 47287 47293 47297 47303 47309 47317 47339 47351 47353 47363 47381 47387 47389 47407 47417 47419 47431 47441 47459 47491 47497 47501 47507 47513 47521 47527 47533 47543 47563 47569 47581 47591 47599 47609 47623 47629 47639 47653 47657 47659 47681 47699 47701 47711 47713 47717 47737 47741 47743 47777 47779 47791 47797 47807 47809 47819 47837 47843 47857 47869 47881 47903 47911 47917 47933 47939 47947 47951 47963 47969 47977 47981 48017 48023 48029 48049 48073 48079 48091 48109 48119 48121 48131 48157 48163 48179 48187 48193 48197 48221 48239 48247 48259 48271 48281 48299 48311 48313 48337 48341 48353 48371 48383 48397 48407 48409 48413 48437 48449 48463 48473 48479 48481 48487 48491 48497 48523 48527 48533 48539 48541 48563 48571 48589 48593 48611 48619 48623 48647 48649 48661 48673 48677 48679 48731 48733 48751 48757 48761 48767 48779 48781 48787 48799 48809 48817 48821 48823 48847 48857 48859 48869 48871 48883 48889 48907 48947 48953 48973 48989 48991 49003 49009 49019 49031 49033 49037 49043 49057 49069 49081 49103 49109 49117 49121 49123 49139 49157 49169 49171 49177 49193 49199 49201 49207 49211 49223 49253 49261 49277 49279 49297 49307 49331 49333 49339 49363 49367 49369 49391 49393 49409 49411 49417 49429 49433 49451 49459 49463 49477 49481 49499 49523 49529 49531 49537 49547 49549 49559 49597 49603 49613 49627 49633 49639 49663 49667 49669 49681 49697 49711 49727 49739 49741 49747 49757 49783 49787 49789 49801 49807 49811 49823 49831 49843 49853 49871 49877 49891 49919 49921 49927 49937 49939 49943 49957 49991 49993 49999 50021 50023 50033 50047 50051 50053 50069 50077 50087 50093 50101 50111 50119 50123 50129 50131 50147 50153 50159 50177 50207 50221 50227 50231 50261 50263 50273 50287 50291 50311 50321 50329 50333 50341 50359 50363 50377 50383 50387 50411 50417 50423 50441 50459 50461 50497 50503 50513 50527 50539 50543 50549 50551 50581 50587 50591 50593 50599 50627 50647 50651 50671 50683 50707 50723 50741 50753 50767 50773 50777 50789 50821 50833 50839 50849 50857 50867 50873 50891 50893 50909 50923 50929 50951 50957 50969 50971 50989 50993 51001 51031 51043 51047 51059 51061 51071 51109 51131 51133 51137 51151 51157 51169 51193 51197 51199 51203 51217 51229 51239 51241 51257 51263 51283 51287 51307 51329 51341 51343 51347 51349 51361 51383 51407 51413 51419 51421 51427 51431 51437 51439 51449 51461 51473 51479 51481 51487 51503 51511 51517 51521 51539 51551 51563 51577 51581 51593 51599 51607 51613 51631 51637 51647 51659 51673 51679 51683 51691 51713 51719 51721 51749 51767 51769 51787 51797 51803 51817 51827 51829 51839 51853 51859 51869 51871 51893 51899 51907 51913 51929 51941 51949 51971 51973 51977 51991 52009 52021 52027 52051 52057 52067 52069 52081 52103 52121 52127 52147 52153 52163 52177 52181 52183 52189 52201 52223 52237 52249 52253 52259 52267 52289 52291 52301 52313 52321 52361 52363 52369 52379 52387 52391 52433 52453 52457 52489 52501 52511 52517 52529 52541 52543 52553 52561 52567 52571 52579 52583 52609 52627 52631 52639 52667 52673 52691 52697 52709 52711 52721 52727 52733 52747 52757 52769 52783 52807 52813 52817 52837 52859 52861 52879 52883 52889 52901 52903 52919 52937 52951 52957 52963 52967 52973 52981 52999 53003 53017 53047 53051 53069 53077 53087 53089 53093 53101 53113 53117 53129 53147 53149 53161 53171 53173 53189 53197 53201 53231 53233 53239 53267 53269 53279 53281 53299 53309 53323 53327 53353 53359 53377 53381 53401 53407 53411 53419 53437 53441 53453 53479 53503 53507 53527 53549 53551 53569 53591 53593 53597 53609 53611 53617 53623 53629 53633 53639 53653 53657 53681 53693 53699 53717 53719 53731 53759 53773 53777 53783 53791 53813 53819 53831 53849 53857 53861 53881 53887 53891 53897 53899 53917 53923 53927 53939 53951 53959 53987 53993 54001 54011 54013 54037 54049 54059 54083 54091 54101 54121 54133 54139 54151 54163 54167 54181 54193 54217 54251 54269 54277 54287 54293 54311 54319 54323 54331 54347 54361 54367 54371 54377 54401 54403 54409 54413 54419 54421 54437 54443 54449 54469 54493 54497 54499 54503 54517 54521 54539 54541 54547 54559 54563 54577 54581 54583 54601 54617 54623 54629 54631 54647 54667 54673 54679 54709 54713 54721 54727 54751 54767 54773 54779 54787 54799 54829 54833 54851 54869 54877 54881 54907 54917 54919 54941 54949 54959 54973 54979 54983 55001 55009 55021 55049 55051 55057 55061 55073 55079 55103 55109 55117 55127 55147 55163 55171 55201 55207 55213 55217 55219 55229 55243 55249 55259 55291 55313 55331 55333 55337 55339 55343 55351 55373 55381 55399 55411 55439 55441 55457 55469 55487 55501 55511 55529 55541 55547 55579 55589 55603 55609 55619 55621 55631 55633 55639 55661 55663 55667 55673 55681 55691 55697 55711 55717 55721 55733 55763 55787 55793 55799 55807 55813 55817 55819 55823 55829 55837 55843 55849 55871 55889 55897 55901 55903 55921 55927 55931 55933 55949 55967 55987 55997 56003 56009 56039 56041 56053 56081 56087 56093 56099 56101 56113 56123 56131 56149 56167 56171 56179 56197 56207 56209 56237 56239 56249 56263 56267 56269 56299 56311 56333 56359 56369 56377 56383 56393 56401 56417 56431 56437 56443 56453 56467 56473 56477 56479 56489 56501 56503 56509 56519 56527 56531 56533 56543 56569 56591 56597 56599 56611 56629 56633 56659 56663 56671 56681 56687 56701 56711 56713 56731 56737 56747 56767 56773 56779 56783 56807 56809 56813 56821 56827 56843 56857 56873 56891 56893 56897 56909 56911 56921 56923 56929 56941 56951 56957 56963 56983 56989 56993 56999 57037 57041 57047 57059 57073 57077 57089 57097 57107 57119 57131 57139 57143 57149 57163 57173 57179 57191 57193 57203 57221 57223 57241 57251 57259 57269 57271 57283 57287 57301 57329 57331 57347 57349 57367 57373 57383 57389 57397 57413 57427 57457 57467 57487 57493 57503 57527 57529 57557 57559 57571 57587 57593 57601 57637 57641 57649 57653 57667 57679 57689 57697 57709 57713 57719 57727 57731 57737 57751 57773 57781 57787 57791 57793 57803 57809 57829 57839 57847 57853 57859 57881 57899 57901 57917 57923 57943 57947 57973 57977 57991 58013 58027 58031 58043 58049 58057 58061 58067 58073 58099 58109 58111 58129 58147 58151 58153 58169 58171 58189 58193 58199 58207 58211 58217 58229 58231 58237 58243 58271 58309 58313 58321 58337 58363 58367 58369 58379 58391 58393 58403 58411 58417 58427 58439 58441 58451 58453 58477 58481 58511 58537 58543 58549 58567 58573 58579 58601 58603 58613 58631 58657 58661 58679 58687 58693 58699 58711 58727 58733 58741 58757 58763 58771 58787 58789 58831 58889 58897 58901 58907 58909 58913 58921 58937 58943 58963 58967 58979 58991 58997 59009 59011 59021 59023 59029 59051 59053 59063 59069 59077 59083 59093 59107 59113 59119 59123 59141 59149 59159 59167 59183 59197 59207 59209 59219 59221 59233 59239 59243 59263 59273 59281 59333 59341 59351 59357 59359 59369 59377 59387 59393 59399 59407 59417 59419 59441 59443 59447 59453 59467 59471 59473 59497 59509 59513 59539 59557 59561 59567 59581 59611 59617 59621 59627 59629 59651 59659 59663 59669 59671 59693 59699 59707 59723 59729 59743 59747 59753 59771 59779 59791 59797 59809 59833 59863 59879 59887 59921 59929 59951 59957 59971 59981 59999 60013 60017 60029 60037 60041 60077 60083 60089 60091 60101 60103 60107 60127 60133 60139 60149 60161 60167 60169 60209 60217 60223 60251 60257 60259 60271 60289 60293 60317 60331 60337 60343 60353 60373 60383 60397 60413 60427 60443 60449 60457 60493 60497 60509 60521 60527 60539 60589 60601 60607 60611 60617 60623 60631 60637 60647 60649 60659 60661 60679 60689 60703 60719 60727 60733 60737 60757 60761 60763 60773 60779 60793 60811 60821 60859 60869 60887 60889 60899 60901 60913 60917 60919 60923 60937 60943 60953 60961 61001 61007 61027 61031 61043 61051 61057 61091 61099 61121 61129 61141 61151 61153 61169 61211 61223 61231 61253 61261 61283 61291 61297 61331 61333 61339 61343 61357 61363 61379 61381 61403 61409 61417 61441 61463 61469 61471 61483 61487 61493 61507 61511 61519 61543 61547 61553 61559 61561 61583 61603 61609 61613 61627 61631 61637 61643 61651 61657 61667 61673 61681 61687 61703 61717 61723 61729 61751 61757 61781 61813 61819 61837 61843 61861 61871 61879 61909 61927 61933 61949 61961 61967 61979 61981 61987 61991 62003 62011 62017 62039 62047 62053 62057 62071 62081 62099 62119 62129 62131 62137 62141 62143 62171 62189 62191 62201 62207 62213 62219 62233 62273 62297 62299 62303 62311 62323 62327 62347 62351 62383 62401 62417 62423 62459 62467 62473 62477 62483 62497 62501 62507 62533 62539 62549 62563 62581 62591 62597 62603 62617 62627 62633 62639 62653 62659 62683 62687 62701 62723 62731 62743 62753 62761 62773 62791 62801 62819 62827 62851 62861 62869 62873 62897 62903 62921 62927 62929 62939 62969 62971 62981 62983 62987 62989 63029 63031 63059 63067 63073 63079 63097 63103 63113 63127 63131 63149 63179 63197 63199 63211 63241 63247 63277 63281 63299 63311 63313 63317 63331 63337 63347 63353 63361 63367 63377 63389 63391 63397 63409 63419 63421 63439 63443 63463 63467 63473 63487 63493 63499 63521 63527 63533 63541 63559 63577 63587 63589 63599 63601 63607 63611 63617 63629 63647 63649 63659 63667 63671 63689 63691 63697 63703 63709 63719 63727 63737 63743 63761 63773 63781 63793 63799 63803 63809 63823 63839 63841 63853 63857 63863 63901 63907 63913 63929 63949 63977 63997 64007 64013 64019 64033 64037 64063 64067 64081 64091 64109 64123 64151 64153 64157 64171 64187 64189 64217 64223 64231 64237 64271 64279 64283 64301 64303 64319 64327 64333 64373 64381 64399 64403 64433 64439 64451 64453 64483 64489 64499 64513 64553 64567 64577 64579 64591 64601 64609 64613 64621 64627 64633 64661 64663 64667 64679 64693 64709 64717 64747 64763 64781 64783 64793 64811 64817 64849 64853 64871 64877 64879 64891 64901 64919 64921 64927 64937 64951 64969 64997 65003 65011 65027 65029 65033 65053 65063 65071 65089 65099 65101 65111 65119 65123 65129 65141 65147 65167 65171 65173 65179 65183 65203 65213 65239 65257 65267 65269 65287 65293 65309 65323 65327 65353 65357 65371 65381 65393 65407 65413 65419 65423 65437 65447 65449 65479 65497 65519 65521 65537 65539 65543 65551 65557 65563 65579 65581 65587 65599 65609 65617 65629 65633 65647 65651 65657 65677 65687 65699 65701 65707 65713 65717 65719 65729 65731 65761 65777 65789 65809 65827 65831 65837 65839 65843 65851 65867 65881 65899 65921 65927 65929 65951 65957 65963 65981 65983 65993 66029 66037 66041 66047 66067 66071 66083 66089 66103 66107 66109 66137 66161 66169 66173 66179 66191 66221 66239 66271 66293 66301 66337 66343 66347 66359 66361 66373 66377 66383 66403 66413 66431 66449 66457 66463 66467 66491 66499 66509 66523 66529 66533 66541 66553 66569 66571 66587 66593 66601 66617 66629 66643 66653 66683 66697 66701 66713 66721 66733 66739 66749 66751 66763 66791 66797 66809 66821 66841 66851 66853 66863 66877 66883 66889 66919 66923 66931 66943 66947 66949 66959 66973 66977 67003 67021 67033 67043 67049 67057 67061 67073 67079 67103 67121 67129 67139 67141 67153 67157 67169 67181 67187 67189 67211 67213 67217 67219 67231 67247 67261 67271 67273 67289 67307 67339 67343 67349 67369 67391 67399 67409 67411 67421 67427 67429 67433 67447 67453 67477 67481 67489 67493 67499 67511 67523 67531 67537 67547 67559 67567 67577 67579 67589 67601 67607 67619 67631 67651 67679 67699 67709 67723 67733 67741 67751 67757 67759 67763 67777 67783 67789 67801 67807 67819 67829 67843 67853 67867 67883 67891 67901 67927 67931 67933 67939 67943 67957 67961 67967 67979 67987 67993 68023 68041 68053 68059 68071 68087 68099 68111 68113 68141 68147 68161 68171 68207 68209 68213 68219 68227 68239 68261 68279 68281 68311 68329 68351 68371 68389 68399 68437 68443 68447 68449 68473 68477 68483 68489 68491 68501 68507 68521 68531 68539 68543 68567 68581 68597 68611 68633 68639 68659 68669 68683 68687 68699 68711 68713 68729 68737 68743 68749 68767 68771 68777 68791 68813 68819 68821 68863 68879 68881 68891 68897 68899 68903 68909 68917 68927 68947 68963 68993 69001 69011 69019 69029 69031 69061 69067 69073 69109 69119 69127 69143 69149 69151 69163 69191 69193 69197 69203 69221 69233 69239 69247 69257 69259 69263 69313 69317 69337 69341 69371 69379 69383 69389 69401 69403 69427 69431 69439 69457 69463 69467 69473 69481 69491 69493 69497 69499 69539 69557 69593 69623 69653 69661 69677 69691 69697 69709 69737 69739 69761 69763 69767 69779 69809 69821 69827 69829 69833 69847 69857 69859 69877 69899 69911 69929 69931 69941 69959 69991 69997 70001 70003 70009 70019 70039 70051 70061 70067 70079 70099 70111 70117 70121 70123 70139 70141 70157 70163 70177 70181 70183 70199 70201 70207 70223 70229 70237 70241 70249 70271 70289 70297 70309 70313 70321 70327 70351 70373 70379 70381 70393 70423 70429 70439 70451 70457 70459 70481 70487 70489 70501 70507 70529 70537 70549 70571 70573 70583 70589 70607 70619 70621 70627 70639 70657 70663 70667 70687 70709 70717 70729 70753 70769 70783 70793 70823 70841 70843 70849 70853 70867 70877 70879 70891 70901 70913 70919 70921 70937 70949 70951 70957 70969 70979 70981 70991 70997 70999 71011 71023 71039 71059 71069 71081 71089 71119 71129 71143 71147 71153 71161 71167 71171 71191 71209 71233 71237 71249 71257 71261 71263 71287 71293 71317 71327 71329 71333 71339 71341 71347 71353 71359 71363 71387 71389 71399 71411 71413 71419 71429 71437 71443 71453 71471 71473 71479 71483 71503 71527 71537 71549 71551 71563 71569 71593 71597 71633 71647 71663 71671 71693 71699 71707 71711 71713 71719 71741 71761 71777 71789 71807 71809 71821 71837 71843 71849 71861 71867 71879 71881 71887 71899 71909 71917 71933 71941 71947 71963 71971 71983 71987 71993 71999 72019 72031 72043 72047 72053 72073 72077 72089 72091 72101 72103 72109 72139 72161 72167 72169 72173 72211 72221 72223 72227 72229 72251 72253 72269 72271 72277 72287 72307 72313 72337 72341 72353 72367 72379 72383 72421 72431 72461 72467 72469 72481 72493 72497 72503 72533 72547 72551 72559 72577 72613 72617 72623 72643 72647 72649 72661 72671 72673 72679 72689 72701 72707 72719 72727 72733 72739 72763 72767 72797 72817 72823 72859 72869 72871 72883 72889 72893 72901 72907 72911 72923 72931 72937 72949 72953 72959 72973 72977 72997 73009 73013 73019 73037 73039 73043 73061 73063 73079 73091 73121 73127 73133 73141 73181 73189 73237 73243 73259 73277 73291 73303 73309 73327 73331 73351 73361 73363 73369 73379 73387 73417 73421 73433 73453 73459 73471 73477 73483 73517 73523 73529 73547 73553 73561 73571 73583 73589 73597 73607 73609 73613 73637 73643 73651 73673 73679 73681 73693 73699 73709 73721 73727 73751 73757 73771 73783 73819 73823 73847 73849 73859 73867 73877 73883 73897 73907 73939 73943 73951 73961 73973 73999 74017 74021 74027 74047 74051 74071 74077 74093 74099 74101 74131 74143 74149 74159 74161 74167 74177 74189 74197 74201 74203 74209 74219 74231 74257 74279 74287 74293 74297 74311 74317 74323 74353 74357 74363 74377 74381 74383 74411 74413 74419 74441 74449 74453 74471 74489 74507 74509 74521 74527 74531 74551 74561 74567 74573 74587 74597 74609 74611 74623 74653 74687 74699 74707 74713 74717 74719 74729 74731 74747 74759 74761 74771 74779 74797 74821 74827 74831 74843 74857 74861 74869 74873 74887 74891 74897 74903 74923 74929 74933 74941 74959 75011 75013 75017 75029 75037 75041 75079 75083 75109 75133 75149 75161 75167 75169 75181 75193 75209 75211 75217 75223 75227 75239 75253 75269 75277 75289 75307 75323 75329 75337 75347 75353 75367 75377 75389 75391 75401 75403 75407 75431 75437 75479 75503 75511 75521 75527 75533 75539 75541 75553 75557 75571 75577 75583 75611 75617 75619 75629 75641 75653 75659 75679 75683 75689 75703 75707 75709 75721 75731 75743 75767 75773 75781 75787 75793 75797 75821 75833 75853 75869 75883 75913 75931 75937 75941 75967 75979 75983 75989 75991 75997 76001 76003 76031 76039 76079 76081 76091 76099 76103 76123 76129 76147 76157 76159 76163 76207 76213 76231 76243 76249 76253 76259 76261 76283 76289 76303 76333 76343 76367 76369 76379 76387 76403 76421 76423 76441 76463 76471 76481 76487 76493 76507 76511 76519 76537 76541 76543 76561 76579 76597 76603 76607 76631 76649 76651 76667 76673 76679 76697 76717 76733 76753 76757 76771 76777 76781 76801 76819 76829 76831 76837 76847 76871 76873 76883 76907 76913 76919 76943 76949 76961 76963 76991 77003 77017 77023 77029 77041 77047 77069 77081 77093 77101 77137 77141 77153 77167 77171 77191 77201 77213 77237 77239 77243 77249 77261 77263 77267 77269 77279 77291 77317 77323 77339 77347 77351 77359 77369 77377 77383 77417 77419 77431 77447 77471 77477 77479 77489 77491 77509 77513 77521 77527 77543 77549 77551 77557 77563 77569 77573 77587 77591 77611 77617 77621 77641 77647 77659 77681 77687 77689 77699 77711 77713 77719 77723 77731 77743 77747 77761 77773 77783 77797 77801 77813 77839 77849 77863 77867 77893 77899 77929 77933 77951 77969 77977 77983 77999 78007 78017 78031 78041 78049 78059 78079 78101 78121 78137 78139 78157 78163 78167 78173 78179 78191 78193 78203 78229 78233 78241 78259 78277 78283 78301 78307 78311 78317 78341 78347 78367 78401 78427 78437 78439 78467 78479 78487 78497 78509 78511 78517 78539 78541 78553 78569 78571 78577 78583 78593 78607 78623 78643 78649 78653 78691 78697 78707 78713 78721 78737 78779 78781 78787 78791 78797 78803 78809 78823 78839 78853 78857 78877 78887 78889 78893 78901 78919 78929 78941 78977 78979 78989 79031 79039 79043 79063 79087 79103 79111 79133 79139 79147 79151 79153 79159 79181 79187 79193 79201 79229 79231 79241 79259 79273 79279 79283 79301 79309 79319 79333 79337 79349 79357 79367 79379 79393 79397 79399 79411 79423 79427 79433 79451 79481 79493 79531 79537 79549 79559 79561 79579 79589 79601 79609 79613 79621 79627 79631 79633 79657 79669 79687 79691 79693 79697 79699 79757 79769 79777 79801 79811 79813 79817 79823 79829 79841 79843 79847 79861 79867 79873 79889 79901 79903 79907 79939 79943 79967 79973 79979 79987 79997 79999 80021 80039 80051 80071 80077 80107 80111 80141 80147 80149 80153 80167 80173 80177 80191 80207 80209 80221 80231 80233 80239 80251 80263 80273 80279 80287 80309 80317 80329 80341 80347 80363 80369 80387 80407 80429 80447 80449 80471 80473 80489 80491 80513 80527 80537 80557 80567 80599 80603 80611 80621 80627 80629 80651 80657 80669 80671 80677 80681 80683 80687 80701 80713 80737 80747 80749 80761 80777 80779 80783 80789 80803 80809 80819 80831 80833 80849 80863 80897 80909 80911 80917 80923 80929 80933 80953 80963 80989 81001 81013 81017 81019 81023 81031 81041 81043 81047 81049 81071 81077 81083 81097 81101 81119 81131 81157 81163 81173 81181 81197 81199 81203 81223 81233 81239 81281 81283 81293 81299 81307 81331 81343 81349 81353 81359 81371 81373 81401 81409 81421 81439 81457 81463 81509 81517 81527 81533 81547 81551 81553 81559 81563 81569 81611 81619 81629 81637 81647 81649 81667 81671 81677 81689 81701 81703 81707 81727 81737 81749 81761 81769 81773 81799 81817 81839 81847 81853 81869 81883 81899 81901 81919 81929 81931 81937 81943 81953 81967 81971 81973 82003 82007 82009 82013 82021 82031 82037 82039 82051 82067 82073 82129 82139 82141 82153 82163 82171 82183 82189 82193 82207 82217 82219 82223 82231 82237 82241 82261 82267 82279 82301 82307 82339 82349 82351 82361 82373 82387 82393 82421 82457 82463 82469 82471 82483 82487 82493 82499 82507 82529 82531 82549 82559 82561 82567 82571 82591 82601 82609 82613 82619 82633 82651 82657 82699 82721 82723 82727 82729 82757 82759 82763 82781 82787 82793 82799 82811 82813 82837 82847 82883 82889 82891 82903 82913 82939 82963 82981 82997 83003 83009 83023 83047 83059 83063 83071 83077 83089 83093 83101 83117 83137 83177 83203 83207 83219 83221 83227 83231 83233 83243 83257 83267 83269 83273 83299 83311 83339 83341 83357 83383 83389 83399 83401 83407 83417 83423 83431 83437 83443 83449 83459 83471 83477 83497 83537 83557 83561 83563 83579 83591 83597 83609 83617 83621 83639 83641 83653 83663 83689 83701 83717 83719 83737 83761 83773 83777 83791 83813 83833 83843 83857 83869 83873 83891 83903 83911 83921 83933 83939 83969 83983 83987 84011 84017 84047 84053 84059 84061 84067 84089 84121 84127 84131 84137 84143 84163 84179 84181 84191 84199 84211 84221 84223 84229 84239 84247 84263 84299 84307 84313 84317 84319 84347 84349 84377 84389 84391 84401 84407 84421 84431 84437 84443 84449 84457 84463 84467 84481 84499 84503 84509 84521 84523 84533 84551 84559 84589 84629 84631 84649 84653 84659 84673 84691 84697 84701 84713 84719 84731 84737 84751 84761 84787 84793 84809 84811 84827 84857 84859 84869 84871 84913 84919 84947 84961 84967 84977 84979 84991 85009 85021 85027 85037 85049 85061 85081 85087 85091 85093 85103 85109 85121 85133 85147 85159 85193 85199 85201 85213 85223 85229 85237 85243 85247 85259 85297 85303 85313 85331 85333 85361 85363 85369 85381 85411 85427 85429 85439 85447 85451 85453 85469 85487 85513 85517 85523 85531 85549 85571 85577 85597 85601 85607 85619 85621 85627 85639 85643 85661 85667 85669 85691 85703 85711 85717 85733 85751 85781 85793 85817 85819 85829 85831 85837 85843 85847 85853 85889 85903 85909 85931 85933 85991 85999 86011 86017 86027 86029 86069 86077 86083 86111 86113 86117 86131 86137 86143 86161 86171 86179 86183 86197 86201 86209 86239 86243 86249 86257 86263 86269 86287 86291 86293 86297 86311 86323 86341 86351 86353 86357 86369 86371 86381 86389 86399 86413 86423 86441 86453 86461 86467 86477 86491 86501 86509 86531 86533 86539 86561 86573 86579 86587 86599 86627 86629 86677 86689 86693 86711 86719 86729 86743 86753 86767 86771 86783 86813 86837 86843 86851 86857 86861 86869 86923 86927 86929 86939 86951 86959 86969 86981 86993 87011 87013 87037 87041 87049 87071 87083 87103 87107 87119 87121 87133 87149 87151 87179 87181 87187 87211 87221 87223 87251 87253 87257 87277 87281 87293 87299 87313 87317 87323 87337 87359 87383 87403 87407 87421 87427 87433 87443 87473 87481 87491 87509 87511 87517 87523 87539 87541 87547 87553 87557 87559 87583 87587 87589 87613 87623 87629 87631 87641 87643 87649 87671 87679 87683 87691 87697 87701 87719 87721 87739 87743 87751 87767 87793 87797 87803 87811 87833 87853 87869 87877 87881 87887 87911 87917 87931 87943 87959 87961 87973 87977 87991 88001 88003 88007 88019 88037 88069 88079 88093 88117 88129 88169 88177 88211 88223 88237 88241 88259 88261 88289 88301 88321 88327 88337 88339 88379 88397 88411 88423 88427 88463 88469 88471 88493 88499 88513 88523 88547 88589 88591 88607 88609 88643 88651 88657 88661 88663 88667 88681 88721 88729 88741 88747 88771 88789 88793 88799 88801 88807 88811 88813 88817 88819 88843 88853 88861 88867 88873 88883 88897 88903 88919 88937 88951 88969 88993 88997 89003 89009 89017 89021 89041 89051 89057 89069 89071 89083 89087 89101 89107 89113 89119 89123 89137 89153 89189 89203 89209 89213 89227 89231 89237 89261 89269 89273 89293 89303 89317 89329 89363 89371 89381 89387 89393 89399 89413 89417 89431 89443 89449 89459 89477 89491 89501 89513 89519 89521 89527 89533 89561 89563 89567 89591 89597 89599 89603 89611 89627 89633 89653 89657 89659 89669 89671 89681 89689 89753 89759 89767 89779 89783 89797 89809 89819 89821 89833 89839 89849 89867 89891 89897 89899 89909 89917 89923 89939 89959 89963 89977 89983 89989 90001 90007 90011 90017 90019 90023 90031 90053 90059 90067 90071 90073 90089 90107 90121 90127 90149 90163 90173 90187 90191 90197 90199 90203 90217 90227 90239 90247 90263 90271 90281 90289 90313 90353 90359 90371 90373 90379 90397 90401 90403 90407 90437 90439 90469 90473 90481 90499 90511 90523 90527 90529 90533 90547 90583 90599 90617 90619 90631 90641 90647 90659 90677 90679 90697 90703 90709 90731 90749 90787 90793 90803 90821 90823 90833 90841 90847 90863 90887 90901 90907 90911 90917 90931 90947 90971 90977 90989 90997 91009 91019 91033 91079 91081 91097 91099 91121 91127 91129 91139 91141 91151 91153 91159 91163 91183 91193 91199 91229 91237 91243 91249 91253 91283 91291 91297 91303 91309 91331 91367 91369 91373 91381 91387 91393 91397 91411 91423 91433 91453 91457 91459 91463 91493 91499 91513 91529 91541 91571 91573 91577 91583 91591 91621 91631 91639 91673 91691 91703 91711 91733 91753 91757 91771 91781 91801 91807 91811 91813 91823 91837 91841 91867 91873 91909 91921 91939 91943 91951 91957 91961 91967 91969 91997 92003 92009 92033 92041 92051 92077 92083 92107 92111 92119 92143 92153 92173 92177 92179 92189 92203 92219 92221 92227 92233 92237 92243 92251 92269 92297 92311 92317 92333 92347 92353 92357 92363 92369 92377 92381 92383 92387 92399 92401 92413 92419 92431 92459 92461 92467 92479 92489 92503 92507 92551 92557 92567 92569 92581 92593 92623 92627 92639 92641 92647 92657 92669 92671 92681 92683 92693 92699 92707 92717 92723 92737 92753 92761 92767 92779 92789 92791 92801 92809 92821 92831 92849 92857 92861 92863 92867 92893 92899 92921 92927 92941 92951 92957 92959 92987 92993 93001 93047 93053 93059 93077 93083 93089 93097 93103 93113 93131 93133 93139 93151 93169 93179 93187 93199 93229 93239 93241 93251 93253 93257 93263 93281 93283 93287 93307 93319 93323 93329 93337 93371 93377 93383 93407 93419 93427 93463 93479 93481 93487 93491 93493 93497 93503 93523 93529 93553 93557 93559 93563 93581 93601 93607 93629 93637 93683 93701 93703 93719 93739 93761 93763 93787 93809 93811 93827 93851 93871 93887 93889 93893 93901 93911 93913 93923 93937 93941 93949 93967 93971 93979 93983 93997 94007 94009 94033 94049 94057 94063 94079 94099 94109 94111 94117 94121 94151 94153 94169 94201 94207 94219 94229 94253 94261 94273 94291 94307 94309 94321 94327 94331 94343 94349 94351 94379 94397 94399 94421 94427 94433 94439 94441 94447 94463 94477 94483 94513 94529 94531 94541 94543 94547 94559 94561 94573 94583 94597 94603 94613 94621 94649 94651 94687 94693 94709 94723 94727 94747 94771 94777 94781 94789 94793 94811 94819 94823 94837 94841 94847 94849 94873 94889 94903 94907 94933 94949 94951 94961 94993 94999 95003 95009 95021 95027 95063 95071 95083 95087 95089 95093 95101 95107 95111 95131 95143 95153 95177 95189 95191 95203 95213 95219 95231 95233 95239 95257 95261 95267 95273 95279 95287 95311 95317 95327 95339 95369 95383 95393 95401 95413 95419 95429 95441 95443 95461 95467 95471 95479 95483 95507 95527 95531 95539 95549 95561 95569 95581 95597 95603 95617 95621 95629 95633 95651 95701 95707 95713 95717 95723 95731 95737 95747 95773 95783 95789 95791 95801 95803 95813 95819 95857 95869 95873 95881 95891 95911 95917 95923 95929 95947 95957 95959 95971 95987 95989 96001 96013 96017 96043 96053 96059 96079 96097 96137 96149 96157 96167 96179 96181 96199 96211 96221 96223 96233 96259 96263 96269 96281 96289 96293 96323 96329 96331 96337 96353 96377 96401 96419 96431 96443 96451 96457 96461 96469 96479 96487 96493 96497 96517 96527 96553 96557 96581 96587 96589 96601 96643 96661 96667 96671 96697 96703 96731 96737 96739 96749 96757 96763 96769 96779 96787 96797 96799 96821 96823 96827 96847 96851 96857 96893 96907 96911 96931 96953 96959 96973 96979 96989 96997 97001 97003 97007 97021 97039 97073 97081 97103 97117 97127 97151 97157 97159 97169 97171 97177 97187 97213 97231 97241 97259 97283 97301 97303 97327 97367 97369 97373 97379 97381 97387 97397 97423 97429 97441 97453 97459 97463 97499 97501 97511 97523 97547 97549 97553 97561 97571 97577 97579 97583 97607 97609 97613 97649 97651 97673 97687 97711 97729 97771 97777 97787 97789 97813 97829 97841 97843 97847 97849 97859 97861 97871 97879 97883 97919 97927 97931 97943 97961 97967 97973 97987 98009 98011 98017 98041 98047 98057 98081 98101 98123 98129 98143 98179 98207 98213 98221 98227 98251 98257 98269 98297 98299 98317 98321 98323 98327 98347 98369 98377 98387 98389 98407 98411 98419 98429 98443 98453 98459 98467 98473 98479 98491 98507 98519 98533 98543 98561 98563 98573 98597 98621 98627 98639 98641 98663 98669 98689 98711 98713 98717 98729 98731 98737 98773 98779 98801 98807 98809 98837 98849 98867 98869 98873 98887 98893 98897 98899 98909 98911 98927 98929 98939 98947 98953 98963 98981 98993 98999 99013 99017 99023 99041 99053 99079 99083 99089 99103 99109 99119 99131 99133 99137 99139 99149 99173 99181 99191 99223 99233 99241 99251 99257 99259 99277 99289 99317 99347 99349 99367 99371 99377 99391 99397 99401 99409 99431 99439 99469 99487 99497 99523 99527 99529 99551 99559 99563 99571 99577 99581 99607 99611 99623 99643 99661 99667 99679 99689 99707 99709 99713 99719 99721 99733 99761 99767 99787 99793 99809 99817 99823 99829 99833 99839 99859 99871 99877 99881 99901 99907 99923 99929 99961 99971 99989 99991 100003 100019 100043 100049 100057 100069 100103 100109 100129 100151 100153 100169 100183 100189 100193 100207 100213 100237 100267 100271 100279 100291 100297 100313 100333 100343 100357 100361 100363 100379 100391 100393 100403 100411 100417 100447 100459 100469 100483 100493 100501 100511 100517 100519 100523 100537 100547 100549 100559 100591 100609 100613 100621 100649 100669 100673 100693 100699 100703 100733 100741 100747 100769 100787 100799 100801 100811 100823 100829 100847 100853 100907 100913 100927 100931 100937 100943 100957 100981 100987 100999 101009 101021 101027 101051 101063 101081 101089 101107 101111 101113 101117 101119 101141 101149 101159 101161 101173 101183 101197 101203 101207 101209 101221 101267 101273 101279 101281 101287 101293 101323 101333 101341 101347 101359 101363 101377 101383 101399 101411 101419 101429 101449 101467 101477 101483 101489 101501 101503 101513 101527 101531 101533 101537 101561 101573 101581 101599 101603 101611 101627 101641 101653 101663 101681 101693 101701 101719 101723 101737 101741 101747 101749 101771 101789 101797 101807 101833 101837 101839 101863 101869 101873 101879 101891 101917 101921 101929 101939 101957 101963 101977 101987 101999 102001 102013 102019 102023 102031 102043 102059 102061 102071 102077 102079 102101 102103 102107 102121 102139 102149 102161 102181 102191 102197 102199 102203 102217 102229 102233 102241 102251 102253 102259 102293 102299 102301 102317 102329 102337 102359 102367 102397 102407 102409 102433 102437 102451 102461 102481 102497 102499 102503 102523 102533 102539 102547 102551 102559 102563 102587 102593 102607 102611 102643 102647 102653 102667 102673 102677 102679 102701 102761 102763 102769 102793 102797 102811 102829 102841 102859 102871 102877 102881 102911 102913 102929 102931 102953 102967 102983 103001 103007 103043 103049 103067 103069 103079 103087 103091 103093 103099 103123 103141 103171 103177 103183 103217 103231 103237 103289 103291 103307 103319 103333 103349 103357 103387 103391 103393 103399 103409 103421 103423 103451 103457 103471 103483 103511 103529 103549 103553 103561 103567 103573 103577 103583 103591 103613 103619 103643 103651 103657 103669 103681 103687 103699 103703 103723 103769 103787 103801 103811 103813 103837 103841 103843 103867 103889 103903 103913 103919 103951 103963 103967 103969 103979 103981 103991 103993 103997 104003 104009 104021 104033 104047 104053 104059 104087 104089 104107 104113 104119 104123 104147 104149 104161 104173 104179 104183 104207 104231 104233 104239 104243 104281 104287 104297 104309 104311 104323 104327 104347 104369 104381 104383 104393 104399 104417 104459 104471 104473 104479 104491 104513 104527 104537 104543 104549 104551 104561 104579 104593 104597 104623 104639 104651 104659 104677 104681 104683 104693 104701 104707 104711 104717 104723 104729\n","id":2339,"permalink":"https://freshrimpsushi.github.io/jp/posts/2339/","tags":null,"title":"1万番目までの素数点以下のリスト"},{"categories":"양자역학","contents":"ベクトルの一般化 線形代数学を学んでいない理科生にとって、ベクトルは大きさと方向を持つ物理量であり、3次元空間の点を意味し、一般に $\\vec{x} = (x_{1}, x_{2}, x_{3})$ のように表される。この定義で古典力学や電磁気学を学ぶ上では大きな問題はないだろう。しかし、量子力学ではフーリエ解析、関数の内積などの概念が登場するため、ベクトルの一般化された定義を知らないと学習に大きな困難を経験する可能性がある。\n線形代数学において、ベクトルとは我々が直感的に考えるそのベクトルを抽象化したものである。3次元空間のベクトルと同じ性質を持つものを全てベクトルと呼び、ベクトルを集めた集合をベクトル空間と呼ぶ。その性質とは、我々が3次元空間の点を考えた時に当然満たされるべき性質のことである。例えば\nベクトルとベクトルを加えたものもベクトルである。 ベクトルに定数を乗じたものもベクトルである。 などがそれにあたる。その結果、3次元空間の点はベクトルになり、3次元空間はベクトル空間になる。以下には量子力学で最も重要な二つの例を紹介する。行列と関数もベクトルである。\n例 行列 サイズが $m \\times n$ の行列を集めた集合を考えてみよう。これらを加えても依然として $m \\times n$ 行列であり、何らかの定数を乗じても依然として $m \\times n$ 行列なので、この集合はベクトル空間になり、各行列はベクトルになる。\n実際、$\\mathbf{x} = (x_{1}, x_{2}, x_{3})$ のように組み合わせで表記することと $\\mathbf{x} = \\begin{bmatrix} x_{1} \u0026amp; x_{2} \u0026amp; x_{3} \\end{bmatrix}$ のように $1 \\times 3$ 行列で表記することに本質的な違いがないことを思い出してみると、行列がベクトルであるということがより理解しやすくなるだろう。\n関数 連続関数の集合を考えてみよう。$f$ と $g$ が連続関数であれば、これらを加えた $f+g$ も依然として連続関数である。また、任意の定数を乗じた $cf$ も依然として連続関数である。したがって、連続関数の集合はベクトル空間になり、各連続関数はベクトルになる。\n実際、関数値が3次元ベクトルであるベクトル関数の場合、以下のように記述されることを思い出してみよう。\n$$ f(x,y,z) = (xy, yz, z^{2}) $$\n内積の一般化 内積はベクトルを扱う際に非常に便利に使われる演算である。ベクトルという概念を一般化したように、内積の概念も一般化してみよう。まず一般化された内積の表記では、点 $\\cdot$ の代わりに二重山括弧 $\\left\\langle \\ ,\\ \\right\\rangle$ を使用する。$\\mathbf{x} = \\left( x_{1}, x_{2}, x_{3} \\right)$、$\\mathbf{y}=\\left( y_{1}, y_{2}, y_{3} \\right)$ とすると、以下のように表記される。\n$$ \\mathbf{x} \\cdot \\mathbf{y} = x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} = \\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle $$\n量子力学では、中にコンマの代わりに線 $|$ を使用する。\n$$ \\mathbf{x} \\cdot \\mathbf{y} = \\braket{\\mathbf{x} \\vert \\mathbf{y} } $$\nこれをディラック記法という。ベクトルを一般化する際の核心は、「私たちがベクトルだと思うものが満たすべき性質」を満たすならば、それが何であれベクトルと呼ぶ点にある。内積の一般化でも同様に、「各成分を掛け合わせてすべて加算する」というコンセプトをそのまま保持する。どのようなベクトル空間を扱うかによって、内積の定義は以下のように異なる。\n例 行列 二つの行列 $A = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{21} \u0026amp; a_{22} \\end{pmatrix}, B = \\begin{pmatrix} b_{11} \u0026amp; b_{12} \\\\ b_{21} \u0026amp; b_{22} \\end{pmatrix}$ があるとする。これらの内積は、3次元ベクトルの内積と同じように「各成分の掛け算の和」として定義される。\n$$ \\braket{ A \\vert B } = a_{11}b_{11} + a_{12}b_{12} + a_{21}b_{21} + a_{22}b_{22} $$\n関数 上で述べたように、関数もベクトルであるので、二つの関数の内積も定義できる。関数の内積は以下のように定積分で定義される。\n$$ \\braket{\\psi \\vert \\phi} = \\int \\psi^{\\ast}(x) \\phi(x) dx $$\nここで、$\\psi^{\\ast}$ は $\\psi$ の複素共役を意味する。ただし、表記法にあいまいさがあるので注意しよう。関数の内積をなぜこのように定義するのかについては、\u0026lsquo;関数の内積を定積分で定義する理由\u0026rsquo;に詳しく説明されているので参照しよう。\n波動関数 量子力学において波動関数は、位置と時間に応じた粒子の状態を表現する関数であり、以下のように指数関数で表現される。\n$$ \\psi (x,t) = e^{i(kx + \\omega t)} $$\n主に $\\psi$ と $\\phi$ で表記され、それぞれ [プサイ]、[ファイ] と読む。$k$ は波数wave numberであり、運動量との関係 $p = \\hbar k$ を満たす。ここで $\\hbar$ は定数であるので、量子力学\nにおいては $k$ を運動量と同じと理解しても構わない。$\\omega$ は角振動数angular frequencyであり、エネルギーとの関係 $E = \\hbar \\omega$ を満たす。\nヒルベルト空間 ヒルベルト空間の厳密な定義は完備内積空間である。その数学的意味を理解しているともちろん良いが、物理学部の学生にとっては必須ではない。重要な点は、非常に良い性質を持つ集合にヒルベルト空間という名前を付けることと、波動関数を集めた集合がヒルベルト空間になるという点である。したがって、様々な良い数学的ツールを使って波動関数を扱うことができる。\n","id":1509,"permalink":"https://freshrimpsushi.github.io/jp/posts/1509/","tags":null,"title":"量子力学でベクトル、内積、波動関数, ヒルベルト空間"},{"categories":"측도론","contents":"定理1 (a) $\\nu$を可測空間 $(X, \\mathcal{E})$上で定義された符号測度とする。すると、以下を満たす $\\nu$の正集合 $P$と負集合 $N$が存在する。\n$$ P \\cup N=X \\quad \\text{and} \\quad P \\cap N =\\varnothing $$\nこのような $X=P \\cup N$を $\\nu$に対するハーン分解Hahn decompositionという。\n(b) $P^{\\prime}, N^{\\prime}$が (a) を満たす別の集合であるとする。その場合、以下の集合は $\\nu$に対する零集合である。\n$$ (P-P^{\\prime}) \\cup (P^{\\prime}-P)=(N-N^{\\prime}) \\cup (N^{\\prime}-N) $$\n対称差symmetric difference記号を使用して以下のように表記される。\n$$ P\\Delta P^{\\prime}=N\\Delta N^{\\prime} $$\n説明 (a) 任意の可測空間が与えられた時、集合 $X$を $\\nu$に対して正の集合と負の集合に分けることができるということである。\n(b) 上述のように集合 $X$を分ける方法が複数存在しても、実質的な違いはないということである。$P$と$P^{\\prime}$、$N$と$N^{\\prime}$は常に互いに零集合だけの差があるため、集合の観点では異なるかもしれないが、測度の観点では同じである。\n証明 この定理の証明自体はそれほど難しくないが、証明の流れが単純ではないため、これを事前に具体的に説明し、始める。まず、ある正の集合 $P$を定義する。そして $N$を $N:=X-P$と定義する。この時、$N$が負の集合であれば、(a) に対する証明が完了する。$N$が負の集合であることを証明する前に、上述のように定義された $N$が持つ2つの性質を確認することにする。そして、最終的な証明では背理法を使用する。$N$が負の集合でないと仮定し、2つの性質を使用して矛盾が生じることを示す。\n一般性を失わずに、$\\nu$が$+\\infty$の値を持たないと仮定する。他の場合は $-\\nu$に対して同じ方法で証明すればよい。$C$を $\\mathcal{E}$のすべてのポジティブセットのコレクションとする。すると、仮定により $\\nu$は $+\\infty$の値を持たないため、以下のように定義される $M$が存在する。\n$$ M:=\\sup \\limits_{P \\in C } \\nu(P) \u0026lt; \\infty $$\nここで、$\\nu(P)=M$を満たすマキシマイザー $P$の存在を示すことができる。以下のようなマキシマイジングシーケンス $\\left\\{ P_j \\right\\}$を考える。\n$$ \\lim \\limits_{j \\rightarrow \\infty} \\nu (P_j)=M $$\nこの時、$P_j$同士には含まれる関係がないため、以下のような $\\tilde{P_j}$を考える。\n$$ \\tilde{P_j} :=\\bigcup \\limits_{k=1}^j P_k $$\nすると、$\\nu(P_j) \\le \\nu (\\tilde{P_j}) \\le M$であるため、$\\left\\{ \\tilde{P_j} \\right\\}$はマキシマイジングシーケンスである。また、$\\tilde{P_1} \\subset \\tilde{P_2}\\subset \\cdots $であることは定義によって明らかである。ここで、$P$を以下のように定義する。\n$$ P := \\bigcup \\limits_{j=1}^\\infty \\tilde{P_j} $$\nすると、次が成り立つ。\n$$ \\nu(P)=\\lim \\limits_{j\\rightarrow \\infty} \\nu(\\tilde{P_j})=M $$\nしたがって、$\\nu(P)=M$を満たすマキシマイザーが存在することを示した。また、$P$は正の集合の可算和であるため、正の集合である。実際にこのように作り出された $P$と $N:=X-P$は、定理で述べられているような一つの分解である。$N$がそのような負の集合であることを示すプロセスが残されている。ここで、$N:=X \\setminus P$とする。上述のように、$N$が負の集合であることを示せば証明が完了する。まず、このような $N$が以下の2つの性質を持つことを証明する。\n主張 1 $N$は測度値が0より大きい正の集合を含まない。つまり、0ではない正の集合を含まない。すなわち $\\nu(E)\u0026gt;0$であり、$E$が正の集合であれば、$E \\not \\subset N$である。\nこの時、注意すべき点は、正の集合でも、負の集合でもない $E \\subset N$が存在する可能性があることである。つまり、$N$の部分集合になり得るのは、1. 空集合、2. 負の集合、3. 正の集合でも負の集合でもない集合である。\n証明\n$E\\subset N$が正の集合で $\\nu(E) \u0026gt;0$であるとする。すると、$N$の定義により、$E$と $P$は互いに素な集合である。したがって、次が成り立つ。\n$$ \\nu(P \\cup E)=\\nu(P)+\\nu(E) $$\nしかし、$\\nu(P)=M$であるため、次が成り立つ。\n$$ \\nu(P \\cup E)=\\nu(P)+\\nu(E)\u0026gt;M $$\nしかし、これは $M=\\sup \\nu (F)\\ \\forall F\\in \\mathcal{E}$という仮定に矛盾する。したがって、$\\nu(E)\u0026gt;0$である正の集合 $E \\subset N$は存在しない。\n主張 2 もし $A \\subset N$で $\\nu(A)\u0026gt;0$であれば、$\\nu(B) \u0026gt; \\nu(A)$を満たす $B \\subset A$が存在する。\n証明\n$A \\subset N$で $\\nu(A)\u0026gt;0$であるとする。すると、主張 1 により、$A$は正の集合ではない。したがって、$A$は空集合でもなく、正の集合でもない。従って、次を満たす $C$が存在する2。\n$$ C \\subset A,\\ \\nu(C) \u0026lt;0 $$\nここで、$B:=A-C$とする。すると、次が成り立つ。\n$$ \\nu(A)=\\nu(B)+\\nu(C) \u0026lt; \\nu(B) $$\nここで、$N$が負の集合でないと仮定する。 上の2つの性質を利用して矛盾が生じることを示せば、$N$が負の集合であることが証明される。\nパート 1.\n$\\left\\{ A_j \\right\\}$を $N$の部分集合の列とし、$\\left\\{ n_j \\right\\}$を自然数の列とする。$N$が負の集合でないと仮定したので、$\\nu (B) \u0026gt;0$となるある $B \\subset N$が存在する。そして、$\\nu (B) \u0026gt; \\frac{1}{n_j}$を満たす最小の $n_j$を $n_1$とし、$n_1$に対してこれを満たす $B$を $A_1$とする。$\\nu (B)=\\nu (A_1)\u0026gt;0$であるため、上で $N$に対して行ったプロセスを $A_1$に対して同じように適用することができる。\nパート 2\n再び $\\nu(B)\u0026gt;0$となるある $B\\subset A_1$が存在し、主張 2 により $\\nu(B) \u0026gt; \\nu (A_1)$である。したがって、$\\nu(B) \u0026gt; \\nu (A_1)+\\frac{1}{n}$を満たす自然数 $n$が存在する。この中で最も小さい自然数を $n_2$とし、そのような $B$を $A_2$とする。\nパート 3\n同じプロセスを繰り返すと、$n_j$は $\\nu(B)\u0026gt;0$となるある $B \\subset A_{j-1}$に対して $\\nu(B)\u0026gt;\\nu (A_{j-1}) + \\dfrac{1}{n_j}$を満たす最も小さい自然数である。また、そのような $B$を $A_j$とする。ここで、$A=\\bigcap \\nolimits_1^\\infty A_j$とする。$\\nu$が $+\\infty$の値を持たないと仮定した上で、符号測度の性質 $(B)$により、次が成り立つ。\n$$ \\begin{align*} +\\infty \\gt \\nu(A) \u0026amp;= \\nu \\left(\\bigcap \\nolimits_1^\\infty A_j \\right) \\\\ \u0026amp;= \\lim \\limits_{j \\rightarrow \\infty} \\nu (A_j) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu(A_{j-1}) +\\frac{1}{n_j} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{j-2}) + \\frac{1}{n_{j-1}} +\\frac{1}{n_j} \\right) \\\\ \u0026amp;\\vdots \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{1}) + \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_j} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\frac{1}{n_1}+ \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_j} \\right) \\\\ \u0026amp;= \\sum \\limits_{j=1}^\\infty \\frac{1}{n_j} \\end{align*} $$\n数列が有限であるため、極限は0である。\n$$ \\lim \\limits_{j\\rightarrow \\infty} \\frac{1}{n_j} =0 $$\nしたがって、次を得る。\n$$ \\begin{equation} \\lim \\limits_{j\\rightarrow \\infty} n_j =\\infty \\label{eq1} \\end{equation} $$\nしかし、パート 1 で見たように、主張 2 により、ある自然数 $n$に対して $\\nu(B) \u0026gt; \\nu(A) +\\dfrac{1}{n}$を満たす $B \\subset A$が存在する。すると、$A$の定義により $A \\subset A_{j-1}$であり、主張 2 により $\\left\\{ \\nu (A_j) \\right\\}$は増加列であることが分かる。したがって、$\\nu (A) =\\lim \\limits_{j \\rightarrow \\infty} \\nu(A_j)$であるため、$\\nu(A) \u0026gt; \\nu(A_{j-1})$である。\nまた、$(1)$により、十分に大きな $j$に対して $n_j \u0026gt;n$である。したがって、次が成り立つ。\n$$ \\nu (B) \u0026gt; \\nu (A) +\\frac{1}{n}\u0026gt;\\nu (A_{j-1}) +\\frac{1}{n} \u0026gt; \\nu(A_{j-1}) +\\frac{1}{n_j} $$\nしかし、これは $n_j$と $A_j$の定義に対する矛盾である。したがって、$N$が負の集合でないという仮定は誤りである。すなわち、$N$は負の集合である。\n$P^{\\prime}$, $N^{\\prime}$を上記の定理を満たす別の一つの分解とする。すると、次が成り立つ。\n$$ P^{\\prime} \\cup N^{\\prime} =X \\quad \\text{and} \\quad P^{\\prime}\\cap N^{\\prime} =\\varnothing $$\nしたがって、$P-P^{\\prime} \\subset P$、$P-P^{\\prime}\\subset N^{\\prime}$であることが分かる。すると、$P-P^{\\prime}$は正の集合でありながら負の集合であるが、これを満たすのは零集合だけであるため、$P-P^{\\prime}$は$\\nu-\\mathrm{null}$である。同様に、$P^{\\prime}-P$、$N-N^{\\prime}$、$N^{\\prime}-N$に対しても同じ方法で $\\nu -\\mathrm{null}$であることを示すことができる。\n■\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (第2版, 1999), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n存在しなければ、定義により Aは空集合か、あるいは正の集合であるべきである。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1308,"permalink":"https://freshrimpsushi.github.io/jp/posts/1308/","tags":null,"title":"ハーン分解定理"},{"categories":"편미분방정식","contents":"説明1 $x$と$p$について、偏微分方程式の変数であることを強調する場合、通常のフォントで $x,p \\in \\mathbb{R}^{n}$ と表示し、$s$に関する関数であることを強調する場合、太字のフォントで $\\mathbf{x}, \\mathbf{p} \\in \\mathbb{R}^{n}$ と表示します。 特性方程式\n$$ \\begin{cases} \\dot{\\mathbf{p}} (s) = -D_{x}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)-D_{z}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)\\mathbf{p}(s) \\\\ \\dot{z}(s) = D_pF\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\cdot \\mathbf{p}(s) \\\\ \\dot{\\mathbf{x}}(s) = D_pF\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\end{cases} $$\n特性方程式を用いた非線形1階偏微分方程式の解法は、微分方程式がどのように与えられるかによって少しずつ異なります。これは与えられた微分方程式の線形性によって区別され、線形、準線形、完全非線形の場合に応じて解法が異なります。非線形性が高いほど難易度が高くなります。\n解法 同次線形 与えられた偏微分方程式が完全に線形であれば、最も簡単に解くことができます。特性方程式の $\\mathbf{p}(s)$ に関する条件は必要ないほど単純です。次の線形および同次の微分方程式を考えてみましょう。\n$$ \\begin{equation} F(Du, u, x) = \\mathbf{b}(x)\\cdot Du(x)+c(x)u(x)=0 \\quad (x\\in \\Omega \\subset \\mathbb{R}^{n}) \\label{eq1} \\end{equation} $$\nここで、各変数 $p, z, x$ を $p, z, x$とします。\n$$ \\begin{equation} F(p,\\ z,\\ x)=\\mathbf{b}(x)\\cdot p +c(x)z=b_{1}p_{1}+\\cdots +b_{n}p_{n}+cz = 0 \\label{eq2} \\end{equation} $$\n$D_{p}F$ を計算すると、次のようになります。\n$$ D_{p}F=(F_{p_{1}}, \\dots, F_{p_{n}})=(b_{1}, \\dots, b_{n})=\\mathbf{b}(x) $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s))\\cdot \\mathbf{p}(s) \\end{align*} $$\nこのとき、$(2)$により、$\\dot{z}(s)$ は次のようになります。\n$$ \\dot{z}(s) = -c(\\mathbf{x}(s))z $$\nしたがって、同次線形1階偏微分方程式の特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{\\mathbf{x}}(s)\u0026amp;=\\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= -c(\\mathbf{x}(s))z \\end{align*} \\right. $$\nこのとき、$\\mathbf{p}(s)$ に関する特性方程式は問題を解くのに必要ありませんことを例を通じて確認できます。\n例 次のような微分方程式が与えられたとします。\n$$ \\left\\{ \\begin{align*} x_{1} u_{x_{2}} - x_{2} u_{x_{1}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0, x_{2}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{ x_{1}\u0026gt;0, x_{2}=0 \\right\\}$ その場合、$(1)$ から $\\mathbf{b}=(-x_{2}, x_{1}), c=-1$ です。したがって、特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;= -x^{2} \\\\ \\dot{x}^{2} \u0026amp;=x^{1} \\\\ \\dot{z}\u0026amp;=z \\end{align*} \\right. $$\nこれは簡単な常微分方程式なので、次のように簡単に解けます。\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;=x^{0}\\cos s \\\\ x^{2}(s)\u0026amp;=x^{0} \\sin s \\\\ z(s)\u0026amp;=z^{0}e^s=g(x^{0})e^s \\end{align*} \\right. $$\nここで、 $x^{0}$ は $s=0$ のときに $x_{1}-$軸($\\Gamma$) を通過するように選ばれた定数です。その後、点 $(x_{1}, x_{2}) \\in \\Omega$ を固定しましょう。\n$$ (x_{1},\\ x_{2})=(x^{1}(s),\\ x^{2}(s)) = (x^{0} \\cos (s),\\ x^{0} \\sin (s)) $$\nすると、 $s\u0026gt;0, x^{0}\u0026gt;0$ の場合、次の結果が得られます。\n$$ x_{1}^{2} + x_{2}^{2} = (x^{0})^{2}\\cos^{2}(s) + (x^{0})^{2}\\sin^{2}(s) = (x^{0})^{2} \\implies x^{0}=({x_{1}}^{2}+{x_{2}}^{2})^{1/2} \\\\ \\dfrac{x_{2}}{x_{1}} = \\dfrac{x^{0}\\sin (s)}{x^{0} \\cos (s)} = \\tan (s) \\implies s=\\arctan \\left( \\frac{x_{2}}{x_{1}} \\right) $$\nしたがって、方程式の解は次のようになります。\n$$ \\begin{align*} u(x)\u0026amp;=u(x^{1}(s),\\ x^{2}(s)) \\\\ \u0026amp;= z(s) \\\\ \u0026amp;=g(x^{0})e^s \\\\ \u0026amp;= g(({x_{1}}^{2}+{x_{2}}^{2})^{1/2})e^{\\arctan \\left(\\frac{x_{2}}{x_{1}}\\right)} \\end{align*} $$\n■\n準線形 次に、与えられた微分方程式が最高微分項に関して線形である場合を考えます。今扱っているのは1階微分方程式なので、1階微分項に関して線形な場合です。\n$$ F(Du,\\ u,\\ x)=\\mathbf{b}(x,\\ u(x))\\cdot Du(x)+c(x,\\ u(x))=0 $$\nここで、各変数 $p, z, x$ を $p, z, x$ とします。\n$$ \\begin{equation} F(p, z, x)=\\mathbf{b}(x, z)\\cdot p + c(x, z)=b_{1}p_{1} + \\cdots + b_{n} p_{n} +c=0 \\label{eq3} \\end{equation} $$\n$D_{p}F$ を計算すると、次のようになります。\n$$ D_pF=(F_{p_{1}},\\ \\cdots,\\ F_{p_{n}})=(b_{1},\\ \\cdots,\\ b_{n})=\\mathbf{b}(x,\\ z) $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s)) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s))\\mathbf{p}(s)=-c(\\mathbf{x}(s),\\ z(s)) \\end{align*} $$\n$\\dot{z}$ の2つ目の等号は $(3)$ によって成立します。この場合も $\\mathbf{p}(s)$ に関する条件は問題を解くのに必要ありません。\n例 次のような微分方程式が与えられたとします。\n$$ \\left\\{ \\begin{align*} u_{x_{1}} + u_{x_{2}} \u0026amp;= u^{2} \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{2} \\gt 0 \\right\\}$ $\\Gamma=\\left\\{ x_{2} = 0 \\right\\}$ その場合、$(3)$ から $\\mathbf{b}=(1, 1)$, $c=-z^{2}$ です。したがって、特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;=1, \\dot{x}^{2}=1 \\\\ \\dot{z} \u0026amp;= z^{2} \\end{align*} \\right. $$\nこれはそれぞれ単純な常微分方程式なので、次のように解けます。\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;= x^{0}+s, x^{2}(s)=s \\\\ z(s)\u0026amp;=\\frac{z^{0}}{1-sz^{0}}=\\frac{g(x^{0})}{1-sg(x^{0})} \\end{align*} \\right. $$\nここで、$x^{0}$ は $s=0$ のときに $x_{2}-$軸($\\Gamma$) を通過するように選ばれた定数です。また、与えられた微分方程式は $u_{x_{1}}u_{x_{2}}=u$ ですから、$p_{1}^{0}p_{2}^{0}=z^{0}=(x^{0})^{2}$ となり、$p_{1}^{0}=\\frac{x^{0}}{2}$ です。これらを全て $(4)$ に代入すると、次の結果が得られます。\n$$ \\left\\{ \\begin{align*} p^{1}(s) \u0026amp;= \\frac{x^{0}}{2}e^s \\\\ p^{2}(s) \u0026amp;= 2x^{0}e^s \\\\ z(s) \u0026amp;= (x^{0})^{2}e^{2s} \\\\ x^{1}(s) \u0026amp;= 2x^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+\\frac{x^{0}}{2}(e^s-1) \\end{align*} \\right. $$\nその後、点\n$(x_{1}, x_{2}) \\in \\Omega$ を固定しましょう。\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=\\left( 2x^{0}(e^s -1), \\frac{x^{0}}{2}(e^s+1) \\right) $$\nすると、$s, x^{0}$ に関して次の結果が得られます。\n$$ x^{0}=\\frac{4x_{2}-x_{1}}{4},\\ \\ e^s=\\frac{x_{1}+4x_{2}}{4x_{2}-x_{1}} $$\nしたがって、方程式の解は次のようになります。\n$$ u(x)=u(x^{1}(s),\\ x^{2}(s))=z(s)=(x^{0})^{2}e^{2s}=\\dfrac{(x_{1}+4x_{2})^{2}}{16} $$\n■\n完全非線形 最後に、次のような微分方程式が与えられたとします。\n$$ \\begin{align*} u_{x_{1}}u_{x_{2}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u \u0026amp;= x_{2}^{2} \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{ x_{1}=0 \\right\\}$ $F$ の変数を $P, z, x$ とすると、次のようになります。\n$$ F(p, z, x)=p_{1}p_{2}-z $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{p}^{1} \u0026amp;= p^{1},\\quad \\dot{p}^{2}=p^{2} \\\\ \\dot{z} \u0026amp;= 2p^{1}p^{2} \\\\ \\dot{x}^{1} \u0026amp;= p^{2},\\quad \\dot{x}^{2}=p^{1} \\end{align*} $$\nまず、 $p$ に関する微分方程式を解くと、次のようになります。\n$$ p^{1}(s)=p_{1}^{0}e^s,\\ \\ p^{2}(s)=p_{2}^{0}e^s $$\nこのとき、 $p_{1}^{0}=p(0)$ および $p_{2}^{0}=p(0)$ です。したがって、$\\dot{z}(s)=2p_{1}^{0}p_{2}^{0}e^{2s}$ であるため、$z$ は次のようになります。\n$$ z(s)=p_{1}^{0}p_{2}^{0}e^{2s}+C $$\n$z(0)=z^{0}=p_{1}^{0}p_{2}^{0}+C$ なので、$C=z^{0}-p_{1}^{0}p_{2}^{0}$ です。したがって、次のようになります。\n$$ z(s)=z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) $$\n同様の方法で $x^{1}$ および $x^{2}$ も計算すると、次のようになります。\n$$ \\begin{equation} \\left\\{ \\begin{aligned} p^{1}(s) \u0026amp;= p_{1}^{0}e^s \\\\ p^{2}(s) \u0026amp;= p_{2}^{0}e^s \\\\ z(s) \u0026amp;= z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) \\\\ x^{1}(s) \u0026amp;= p_{2}^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+p_{1}^{0}(e^s-1) \\end{aligned} \\right. \\label{eq4} \\end{equation} $$\nこのとき、 $x^{0}$ は $s=0$ のときに $x_{1}-$軸($\\Gamma$) を通過するように選ばれた定数です。$u_{x_{2}}=p^{2}$ および境界条件により、$x_{2}^{0}=u(0, x^{0})=2x^{0}$ です。また、与えられた微分方程式は $u_{x_{1}}u_{x_{2}}=u$ であるため、$p_{1}^{0}p_{2}^{0}=z^{0}=(x^{0})^{2}$ であり、$p_{1}^{0}=\\frac{x^{0}}{2}$ です。これらを全て $(4)$ に代入すると、次の結果が得られます。\n$$ \\left\\{ \\begin{align*} p^{1}(s) \u0026amp;= \\frac{x^{0}}{2}e^s \\\\ p^{2}(s) \u0026amp;= 2x^{0}e^s \\\\ z(s) \u0026amp;= (x^{0})^{2}e^{2s} \\\\ x^{1}(s) \u0026amp;= 2x^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+\\frac{x^{0}}{2}(e^s-1) \\end{align*} \\right. $$\nその後、点 $(x_{1}, x_{2})\\in \\Omega$ を固定しましょう。\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=\\left( 2x^{0}(e^s -1), \\frac{x^{0}}{2}(e^s+1) \\right) $$\nすると、 $s, x^{0}$ に関して次の結果が得られます。\n$$ x^{0}=\\frac{4x_{2}-x_{1}}{4},\\ \\ e^s=\\frac{x_{1}+4x_{2}}{4x_{2}-x_{1}} $$\nしたがって、方程式の解は次のようになります。\n$$ u(x)=u(x^{1}(s),\\ x^{2}(s))=z(s)=(x^{0})^{2}e^{2s}=\\dfrac{(x_{1}+4x_{2})^{2}}{16} $$\n■\nLawrence C. Evans, Partial Differential Equations (第2版, 2010年), p99-102\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1074,"permalink":"https://freshrimpsushi.github.io/jp/posts/1074/","tags":null,"title":"特性方程式を利用した非線形1系偏微分方程式の解法。"},{"categories":"푸리에해석","contents":"定義 $2L$-周期関数 $f$に対して次のような級数を $f$のフーリエ級数Fourier series of $f$と定義する。\n$$ \\begin{align*} \\lim \\limits_{N \\rightarrow \\infty} S^{f}_{N}(t) \u0026amp;= \\lim \\limits_{N \\to \\infty}\\left[ \\dfrac{a_0}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right] \\\\ \u0026amp;= \\dfrac{a_0}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\end{align*} $$\nこの時、各々の係数 $a_{0}, a_{n}, b_{n}$を フーリエ係数Fourier coefficientと言い、値は次のようになる。\n$$ \\begin{align*} \\\\ a_0 \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin \\dfrac{n\\pi t}{L}dt \\end{align*} $$\n説明 フーリエ級数は任意の関数を三角関数の級数展開で表現するもので、フランスの数学者 ジョセフ・フーリエJoseph Fourierが熱方程式を解くために考案したことでよく知られている。任意の関数と表現した理由は、ある区間 $(a,b)$で定義された関数があれば、これをCtrl+C, Ctrl+Vして $(b-a)$-周期関数にすることができるからである。\n核心原理は互いに直交する三角関数たちの線形結合で表現されることであり、3次元ベクトルにたとえると、$(4,-1,7)$を次のように分けることに似ている。\n$$ (4,-1,7) = a_{1}\\hat{\\mathbf{e}}_{1} + a_{2}\\hat{\\mathbf{e}}_{1} + a_{3}\\hat{\\mathbf{e}}_{1} $$\n実際に、$f$のフーリエ級数は$f$との誤差が非常に小さく、条件がよく満たされれば $f$に点ごとに収束する。\n$$ f(t) = \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L}t + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) $$\n導出 回帰分析1 パート 1\n関数 $f(t)$を $1, \\cos \\dfrac{\\pi t}{L}, \\cos\\dfrac{2\\pi t}{L}, \\cdots, \\sin \\dfrac{\\pi t}{L}, \\sin \\dfrac{2\\pi t}{L}, \\cdots $たちの線形結合で表現することが目的である。したがって、$S^{f}_{N}(t)=\\dfrac{1}{2}{\\alpha_0}+\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right)$とした時、$f(t)$は以下のように表現できる。\n$$ f(t)=S^{f}_{N}(t)+e_{N}(t) $$\n$e_{N}(t)$は $f(t)$と近似式 $S_{N}^{f} (t)$の差である。この差が最も小さくなる$S_{N}^{f}(t)$を見つければ、それが$f(t)$との差が最も小さい級数展開になる。$e_{N}$を 平均二乗誤差mean square error2としよう。\n$$ e_{N}=\\dfrac{1}{2L}\\int_{-L}^{L} [e_{N}(t) ]^{2}dt=\\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N} (t) \\right]^{2} dt $$\nパート 2\n$$ \\begin{align*} e_{N} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N}(t) \\right]^{2} dt \\\\ \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_0}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\end{align*} $$\n平均二乗誤差 $e_{N}$が最小になる時の係数 $\\alpha_0,\\ \\alpha_{n},\\ \\beta_{n}$をそれぞれ $a_0$, $a_{n}$, $b_{n}$としよう。$e_{N}$を最小化する条件は次のようであり、正規方程式normal equationと言われる。\n$$ \\dfrac{\\partial e_{N}}{\\partial \\alpha_0}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\alpha_{n}}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\beta_{n}}=0\\quad (m=1,\\ 2,\\ \\cdots,\\ N) $$\nそれでは、$a_{0}$, $a_{n}$, $b_{n}$は以下のように求めることができる。\nパート 2.1 $a_{0}$\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_0} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_0} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_0}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{-1}{2} \\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_0}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_ {N} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac {n\\pi t}{L} \\right) \\right] dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_0 dt +\\dfrac{1}{2L}\\int_{-L}^{L} \\sum \\limits_ {n=1}^{N}\\left( \\alpha_{n}\\cos \\dfrac{n\\pi t}{L}+\\beta_{n} \\sin \\dfrac{n \\pi t}{L} \\right) dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_0 dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt +\\dfrac{1}{2}\\alpha_0 \\\\ \u0026amp;= 0 \\end{align*} $$\n4番目の等号は三角関数の1周期積分が0であるため成り立つ。したがって\n$$ a_0 = \\dfrac{1}{L} \\int_{-L}^{L}f(t)dt $$\nパート 2.2 $a_{n}$\nある$m \\in \\left\\{ 1,2,\\dots,N \\right\\}$を一つ選ぼう。\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_0}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\cos \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_0}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_0\\cos\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad + \\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\cos\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\alpha_{m} \\int_{-L}^{L}\\cos\\dfrac{m\\pi t}{L}\\cos\\dfrac{m\\pi t} {L} dt\\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\alpha_{m} \\\\ \u0026amp;= 0 \\end{align*} $$\n4番目、5番目の等号は三角関数の直交性によって成り立つ。したがって\n$$ a_{n}= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\quad (n=1, 2, \\cdots, N) $$\nパート 2.3 $b_{n}$\nある$m \\in \\left\\{ 1,2,\\dots,N \\right\\}$を一つ選ぼう。\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\beta_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\beta_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_0}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\sin \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_0}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_0\\sin\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad +\\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\sin\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\beta_{m} \\int_{-L}^{L}\\sin\\dfrac{m\\pi t}{L}\\sin\\dfrac{m\\pi t} {L} dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\beta_{m} \\\\ \u0026amp;=0 \\end{align*} $$\n4番目、5番目の等号は三角関数の直交性によって成り立つ。したがって\n$$ b_{n}=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\quad (n=1, 2, \\cdots, N) $$\nパート 3 ここで得られた$a_0$, $a_{n}$, $b_{n}$で$f(t)$を表現すると同じになる。\n$$ \\begin{align*} f(t) \u0026amp;= S^{f}_{N}(t)+e_{N}(t) \\\\[1em] \\text{where } S^{f}_{N}(t) \u0026amp;= \\dfrac{a_0}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t} {L} \\right) \\\\ a_0 \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\end{align*} $$\n$N$に対して極限をとれば\n$$ \\lim \\limits_{N \\rightarrow \\infty} S_{N}^{f} (t)=\\dfrac{a_0}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n} \\sin\\dfrac{n\\pi t}{L} \\right) $$\n上の級数を $f$のフーリエ級数 と呼び、$a_0$, $a_{n}$, $b_{n}$を $f$のフーリエ係数 という。\n■\nチェ・ビョンソン, フーリエ解析入門 (2002), p51-53\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRSSが平均二乗誤差である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":929,"permalink":"https://freshrimpsushi.github.io/jp/posts/929/","tags":null,"title":"フーリエ級数の導出"},{"categories":"関数","contents":"公式 ルジャンドル 多項式の明示的explicit公式は次のようである。\n$$ P_{l}(x)=\\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \\tag{1} $$\n説明 $l$番目のルジャンドル多項式を得る公式であり、これをロドリゲスの公式という。元々はルジャンドル多項式の明示的な形を示す言葉であったが、その後、多項式で表される特殊関数の明示的な形を示す公式の一般的な名称となった。\n導出 ルジャンドル多項式 $P_{l}$はルジャンドルの微分方程式\n$$ (1 - x^{2}) \\dfrac{d^{2} y}{d x^{2}} - 2x \\dfrac{d y}{d x} + l(l+1)y = 0 $$\nの解を意味する。従って、(1)が上記の微分方程式の解になることを示せば、証明が完了する。\nロドリゲスの公式がルジャンドル方程式の解になることを示す過程は以下の通りである。まず $v=(x^2-1)^l$としたとき、$\\dfrac{d^lv}{dx^l}$がルジャンドル方程式の解になることを示す。\n$$ \\dfrac{dv}{dx}=l(2x)(x^2-1)^{l-1} $$ 両辺に $(x^2-1)$を掛けると、 $$ (x^2-1)\\dfrac{dv}{dx}=2lx(x^2-1)^l=2lxv $$ 両辺を $l+1$回微分すると、**ライプニッツの規則により、 $$ \\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C}_k \\dfrac{ d^{l+1-k}}{dx^{l+1-k} } \\left( \\dfrac{dv}{dx} \\right) \\dfrac{d^k}{dx^k} (x^2-1) = 2l\\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C} _{k} \\dfrac{d^{l+1-k} v}{dx^{l+1-k}} \\dfrac{d^k x}{dx^k} $$ このとき、左辺は $k \\ge 3$のとき $\\dfrac{d^k}{dx^k}(x^2-1)=0$であるため、$k=0,2,3$の項のみが残る。右辺は $k \\ge 2$のとき $\\dfrac{d^kx}{dx^k}=0$であるため、$k=1,2$の項のみが残る。従って、 $$ (x^2-1)\\dfrac{d^{l+2} v}{dx^{l+2}} + (l+1)(2x)\\dfrac{d^{l+1}v}{dx^{l+1}}+\\dfrac{l(l+1)}{2!}2\\dfrac{d^l v}{dx^l}=2lx\\dfrac{d^{l+1} v}{dx^{l+1}} + 2l(l+1)\\dfrac{d^lv}{dx^l} $$ 同じ係数の項をまとめて定理すると、 $$ (1-x^2)\\left( \\dfrac{d^l v}{dx^l} \\right)^{\\prime \\prime} -2x\\left( \\dfrac{d^lv}{dx^l} \\right)^{\\prime} + l(l+1)\\dfrac{d^lv}{dx^l}=0 $$ これはルジャンドルの方程式と同じ形である。つまり、$\\dfrac{d^l v}{dx^l}$がルジャンドルの方程式の解であることが示される。 $$ P_l(x)= \\dfrac{d^l}{dx^l}(x^2-1)^l $$ これだけでは初めに紹介した公式と形が異なる。最初の公式は $P_l(1)=1$を満たす形である。前の係数を求める方法は以下の通りである。$(x^2-1)^l$を $(x-1)^l(x+1)^l$に因数分解してライプニッツの規則で $l$回微分すると、 $$ \\begin{align} P_l(x) \u0026amp;= \\dfrac{d^l}{dx^l} \\left[ (x-1)^l (x+1)^l \\right] \\\\ \u0026amp;= \\sum\\limits_{k=0}^l {}_{l}\\mathrm{C}_k \\dfrac{d^{l-k}}{dx^{l-k}}(x-1)^l \\dfrac{d^k}{dx^k}(x+1)^l \\\\ \u0026amp;= {}_{l}\\mathrm{C}_0 l! (x+1)^l + {}_{l}\\mathrm{C}_1 l!(x-1) l(x+1)^{l-1}+{}_{l}\\mathrm{C}_2\\dfrac{l!}{2}(x-1)^2l(l-1)(x+1)^{l-2}+\\cdots \\end{align} $$ 2番目の項以降は因数に $(x-1)$を含むため、$x=1$のときに$0$になる。$P_l(1)=l! 2^l$であり、この値が$1$になるように$\\dfrac{1}{2^l l!}$だけ割ってやればよい。したがって、最終的に求めたロドリゲスの公式は、 $$ P_l(x)=\\dfrac{1}{2^l l!}\\dfrac{d^l}{dx^l}(x^2-1)^l\n","id":895,"permalink":"https://freshrimpsushi.github.io/jp/posts/895/","tags":null,"title":"르장드르 다항함수의 로드리게스 공식"},{"categories":"힐베르트공간","contents":"정의1 완비 내적 공간을 힐베르트 공간Hilbert space라고 한다. 힐베르트의 이름을 따서 주로 $H$라고 표기한다.\n설명 완비 공간이란, 모든 코시수열이 수렴하는 공간을 말한다. 바나흐 공간도 완비공간이므로, 힐베르트 공간을 내적이 주어지는 바나흐 공간이라고 설명할 수도 있다. 예시로는 다음과 같은 공간들이 있다.\n르벡 공간 $L^{2}$ $\\ell^{2}$ 공간 실수 공간 $\\mathbb{R}^{n}$ 복소수 공간 $\\mathbb{C}^{n}$ 성질 힐베르트 공간은 균등하게 볼록하다 최단 벡터 정리 직교 분해 정리 리즈 표현 정리 Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p65\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":776,"permalink":"https://freshrimpsushi.github.io/jp/posts/776/","tags":null,"title":"함수해석학에서 힐베르트 공간"},{"categories":"수리물리","contents":"まとめ 次のように定義される $\\epsilon_{ijk}$ を レビ-チビタ記号 と呼ぶ。\n$$ \\epsilon_{ijk} = \\begin{cases} +1 \u0026amp; \\text{if} \\ \\epsilon_{123}, \\epsilon_{231}, \\epsilon_{312} \\\\ -1 \u0026amp; \\text{if} \\ \\epsilon_{132}, \\epsilon_{213}, \\epsilon_{321} \\\\ 0 \u0026amp; \\text{if} \\ i=j \\ \\text{or} \\ j=k \\ \\text{or} \\ k=i \\end{cases} $$\n次のように定義される $\\delta_{ij}$ を クロネッカーのデルタ と呼ぶ。\n$$ \\delta_{ij} := \\begin{cases} 1,\u0026amp;i=j \\\\ 0, \u0026amp; i\\ne j \\end{cases} $$\n二つのレビ-チビタ記号の積とクロネッカーのデルタとの間には、次の関係が成り立つ。\n(a) 一つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ilm} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl}$\n(b) 二つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijm}=2\\delta_{km}$\n(c) 三つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijk}=6$\n説明 文章全体で $\\sum$ を省略する アインシュタインの記法 を使用していることに注意してください。これは上記の式においても同じです。 (a)は使用頻度が高いため、覚えておくと便利です。簡単に覚える方法は次のとおりです。\n証明 (a) $\\mathbf{e}_{i}$ $(i=1,2,3)$ を3次元における 標準単位ベクトル としよう。\n$$ \\mathbf{e}_{1} = (1, 0, 0),\\quad \\mathbf{e}_{2} = (0, 1, 0),\\quad \\mathbf{e}_{3} = (0, 0, 1) $$\n$P_{ijk}$ を1行目が $\\mathbf{e}_{i}$、2行目が $\\mathbf{e}_{j}$、3行目が $\\mathbf{e}_{k}$ の $3 \\times 3$ 行列 とする。\n$$ P_{ijk} = \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} $$\nすると、行列式の性質により $\\det P_{ijk} = \\epsilon_{ijk}$ と簡単にわかる。まず $P_{123}$ は 単位行列 であるため、行列式は $1$ である。また、異なる行の順序を偶数回変えると行列式の値は変わらないため、\n$$ \\det P_{123} = \\det P_{231} = \\det P_{312} = 1 $$\n異なる行の順序を奇数回変えると行列式の符号が逆になるため、\n$$ \\det P_{132} = \\det P_{213} = \\det P_{321} = -1 $$\n同じ行を二つ以上含む行列の行列式は $0$ であるため、残りの場合は全て $0$ となる。したがって $\\det P_{ijk} = \\epsilon_{ijk}$ が成立する。一つのインデックスが同じ二つのレビ-チビタ記号の積は、 行列式の性質 をうまく使うと、次のようになる。\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ilm} \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{l} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{m} \\text{ \u0026mdash;} \\end{bmatrix} \\\\ \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \u0026amp; (\\because \\det A = \\det A^{T}) \\\\ \u0026amp;= \\det \\left( \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \\right) \u0026amp; \\Big(\\because (\\det A) (\\det B) = \\det (AB) \\Big) \\\\ \u0026amp;= \\det \\begin{bmatrix} \\mathbf{e}_{i} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{j} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{k} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{m} \\end{bmatrix} \\end{align*} $$\n$\\mathbf{e}_{i}$ は標準単位ベクトルであるため、$\\mathbf{e}_{i} \\cdot \\mathbf{e}_{j} = \\delta_{ij}$ が成立する。\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} \\delta_{ii} \u0026amp; \\delta_{il} \u0026amp; \\delta_{im} \\\\ \\delta_{ji} \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ \\delta_{ki} \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} $$\nこのとき、$i$ が $j, k, l, m$ と全て異なる場合のみを考えていることに注意しよう。なぜなら $j, k, l, m$ のいずれかが $i$ と同じであれば、$\\epsilon_{ijk}\\epsilon_{ilm} = 0$ となり、意味のない結果だからである。したがって結果\nは次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ `` 0 \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl} $$\n■\n(b) (a) で $l=j$ の場合である。したがって、次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} $$\nこのとき $\\delta_{jj}=3$ が成立し、また $\\delta_{jm}\\delta_{kj}=\\delta_{mk}$ も成立するため、結果は次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} = 3\\delta_{km} - \\delta_{mk} = 2\\delta_{km} $$\n■\n(c) (b) で $m=k$ の場合であるため、\n$$ \\epsilon_{ijk}\\epsilon_{ijk} = \\sum_{k=1}^{3}2\\delta_{kk} = 2\\delta_{11} + 2\\delta_{22} + 2\\delta_{33} = 2 + 2 + 2 = 6 $$\nまたは、0でない全ての項を展開すると、次を得る。\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ijk} \u0026amp;=\\sum \\limits _{i=1} ^{3}\\sum \\limits _{j=1} ^{3}\\sum \\limits _{k=1} ^{1} \\epsilon_{ijk}\\epsilon_{ijk} \\\\ \u0026amp;=\\epsilon_{123}\\epsilon_{123}+\\epsilon_{231}\\epsilon_{231}+\\epsilon_{312}\\epsilon_{312}+\\epsilon_{132}\\epsilon_{132}+\\epsilon_{213}\\epsilon_{213}+\\epsilon_{321}\\epsilon_{321} \\\\ \u0026amp;=6 \\end{align*} $$\n■\n","id":88,"permalink":"https://freshrimpsushi.github.io/jp/posts/88/","tags":null,"title":"二つのレビ-チビタ記号の積"},{"categories":"복소해석","contents":"定理 1 $\\left\\{ a_{i} \\right\\}_{i=0}^{n} \\subset \\mathbb{R}$ で $a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$ としましょう。すると、多項関数 $$ P(z) := a_0 + a_1 z + \\cdots + a_{n-1} z^{n-1} + a_n z^n $$ において、あらゆる根 $z \\in \\mathbb{C}$ は $|z| \\ge 1$ を満たします。\n証明 もし $P(z) = 0$ の根が $z=1$ である場合、$\\displaystyle 0 = P(1) = \\sum_{i=0}^{n} a_{i} \u0026gt; 0$ より、根は $z \\ne 1$ でなければなりません。式 $P(z) = 0$ の両辺に $z$ を乗じて元の式から引き、$a_0$ を以下のように表せます。 $$ a_0 = (1-z)P(z) + (a_0 - a_1) z + \\cdots + (a_{n-1} - a_n) z^n + a_n z^{n+1} $$ ここで、$P(z) = 0$ の根 $z \\ne 1$ で $|z| \u0026lt; 1$ を仮定してみると、$a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$ より $$ \\begin{align*} \u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + (a_0 - a_1) + \\cdots + (a_{n-1} - a_n) + a_n \\\\ \\implies\u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + a_0 + (- a_1 + a_1) + \\cdots + (- a_{n-1} + a_{n-1} )+ (- a_n + a_n ) \\\\ \\implies\u0026amp; a_0 = |a_0| \u0026lt; |(1-z)P(z)| + a_0 \\\\ \\implies\u0026amp; 0 \u0026lt; |(1-z)P(z)| \\end{align*} $$ ですが、$z \\ne 1$ が $P(z) = 0$ の根であることを仮定しているので、以下の矛盾が生じます。 $$ 0 \u0026lt; |(1-z)P(z)| = 0 $$ これは、$| z | \u0026lt; 1$ という仮定が誤りであることを意味し、結果として $|z | \\ge 1$ でなければなりません。\n■\nOsborne. (1999). 複素変数とその応用: p. 6.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":5,"permalink":"https://freshrimpsushi.github.io/jp/posts/5/","tags":null,"title":"エーネストローム-カケヤ定理の証明"},{"categories":"교과과정","contents":"式 $$ d=\\frac {|2k|}{\\sqrt{m^2+1}} $$\n説明 双曲線の接線の問題を解いていると、二つの接線の間の距離を求めることがよくあります。点から直線までの距離を求める公式があるため、それ自体を解くことは難しくありません。しかし、その距離を簡単かつ迅速に計算できる公式を知っていれば、少しでも計算量を減らすことができるでしょう。\n導出 二つの平行な直線の方程式を $y=mx\\pm k$ とします。ある点 $(x,y)$ から直線 $y=mx+k$ までの距離は $$ \\frac {|mx-y+k|}{\\sqrt{m^2+1}} $$ 直線 $y=mx-k$ 上の点 $(x_1,y_1)$ に対しては $$ k=mx_1-y_1 $$ これを距離の公式に代入すると $$ \\frac {|mx_1-y_1+k|}{\\sqrt{m^2+1}} = \\frac {|k+k|}{\\sqrt{m^2+1}} $$ したがって、二つの平行な直線 $y=mx\\pm k$ の間の距離は $$ \\frac {|2k|}{\\sqrt{m^2+1}} $$\n■\n","id":4,"permalink":"https://freshrimpsushi.github.io/jp/posts/4/","tags":null,"title":"二本の平行な直線の間の距離を求める公式の導出"},{"categories":"보조정리","contents":"定義 $n$ 個の正数 ${x}_1,{x}_2,\\cdots,{x}_n$ に対して算術平均、幾何平均、調和平均は以下のように定義される。\n算術平均 : $$ \\sum_{ k=1 }^{ n }{ \\frac { {x}_k }{ n } }=\\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n } $$ 幾何平均 : $$ \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } }=\\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n } $$ 調和平均 : $$ \\left( \\frac { \\sum_{ k=1 }^{ n }{ \\frac { 1 }{ {x}_k } } }{ n } \\right)^{-1}=\\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ n{x}_2 }+\\cdots+\\frac { 1 }{ n{x}_n } } $$ 定理 これらの平均に対して、次の不等式が成り立つ。\n$$ \\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n }\\ge \\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ n{x}_2 }+\\cdots+\\frac { 1 }{ n{x}_n } } $$\n説明 高校生であれば、算術・幾何平均について一度は耳にするかもしれないが、特定の名称で定義されることはあまりなく、通常は「算術幾何」という略称で口伝えにされることが一般的である。$n=2$ の場合には証明も簡単で、高校レベルの問題解決にも役立つ。高校生レベルで一般的な証明には複雑な式を使った数学的帰納法を用いる必要があるが、より洗練されたが難しい証明を紹介する。\n証明 戦略：次の補助定理を利用する。\nジェンセンの不等式： $f$ が 凸関数 で、$E(X) \u0026lt; \\infty$ の場合、以下の不等式が成り立つ。 $$ E{f(X)}\\ge f{E(X)} $$\n算術-幾何 $f(x)=-\\ln x$ とすると、$f$ は区間 $(0,\\infty )$ で凸関数である。確率変数 $X$ が確率質量関数\n$$ p(X=x)=\\begin{cases}{1 \\over n} \u0026amp; , x={x}_1,{x}_2, \\cdots ,{x}_n \\\\ 0 \u0026amp; , その他の場合\\end{cases} $$\nを持つとする。すると $E(X)$ は\n$$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\u0026lt;\\infty $$\nであり有限である。これはジェンセンの不等式に必要な全ての条件を満たすため、次を得る。\n$$ E(-\\ln X)\\ge –\\ln E(X) $$\n左辺は\n$$ \\begin{align*} E(-\\ln X)\u0026amp;=-E(\\ln X) \\\\ \u0026amp;=-\\frac { 1 }{ n } \\sum_{ k=1 }^{ n }{ \\ln{x}_k } \\\\ \u0026amp;=-\\frac { 1 }{ n }\\ln \\prod_{ k=1 }^{ n }{ {x}_k } \\\\ \u0026amp;=-\\ln { \\left( \\prod_{ k=1 }^{ n }{ {x}_k } \\right) }^{ \\frac { 1 }{ n } } \\\\ \u0026amp;=-\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\end{align*} $$\n右辺は\n$$ \\begin{align*} -\\ln E(X)=-\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\end{align*} $$\nこの両者を定理すると\n$$ \\begin{align*} -\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\ge\u0026amp; -\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\\\ \\implies \\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n } \\ge\u0026amp; \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } \\end{align*} $$\n■\nこれにより、算術平均と幾何平均の間の不等式が証明された。これを用いて、幾何平均と調和平均の間の不等式を証明しよう。\n幾何-調和 $$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } $$\n$\\displaystyle {x}_k=\\frac { 1 }{ n{y}_k }$ と置くと、\n$$ \\begin{align*} \\frac { \\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } }{ n }\\ge \\sqrt [ n ]{ \\frac { 1 }{ n{y}_1 }\\frac { 1 }{ n{y}_2 }\u0026hellip;\\frac { 1 }{ n{y}_n } } \\\\ \\implies \\frac { 1 }{ n\\sqrt [ n ]{ \\frac { 1 }{ n{y}_1 }\\frac { 1 }{ n{y}_2 }\u0026hellip;\\frac { 1 }{ n{y}_n } } }\\ge \\frac { n }{ n\\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\\\ \\implies \\sqrt [ n ]{ {y}_1{y}_2\u0026hellip;{y}_n }\\ge \\frac { n }{ n\\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\end{align*} $$ ■\n","id":3,"permalink":"https://freshrimpsushi.github.io/jp/posts/3/","tags":null,"title":"算術平均と幾何平均、調和平均の間の不等式"},{"categories":"머신러닝","contents":"概要 TensorFlowでは、Kerasを使用して簡単にニューラルネットワークを定義することができます。以下では、Sequential()と関数型APIを使用してシンプルなMLPを定義し、訓練する方法を紹介します。ただし、Sequential()はモデルの定義自体は簡単ですが、それを使用して複雑な構造を設計するには適していません。同様に、関数型APIを使用して複雑な構造を設計する場合は、keras.Modelクラスの使用が適しており、より複雑で自由なカスタマイズを求める場合は、Kerasを使用せずに低レベルで実装する方が良いでしょう。どのような作業にディープラーニングを使用するかによって異なりますが、もし自分が理工学の研究者であり、専門分野にディープラーニングを応用したい場合は、以下の方法を主に使用する可能性は低いでしょう。ディープラーニングを初めて学び、実践する際は、「これが使用法だ」と感じ取る程度だと考えられます。\nシーケンシャルモデル モデル定義 サイン関数 $\\sin : \\mathbb{R} \\to \\mathbb{R}$ の近似のために、入力と出力の次元が1のMLPを次のように定義しましょう。\nimport tensorflow as tf\rfrom tensorflow.keras import Sequential\rfrom tensorflow.keras.layers import Dense\r# モデル定義\rmodel = Sequential([Dense(10, input_dim = 1, activation = \u0026#34;relu\u0026#34;),\rDense(10, input_dim = 10, activation = \u0026#34;relu\u0026#34;),\rDense(1, input_dim = 10)])\rmodel.summary() # output↓\r# Model: \u0026#34;sequential_3\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param # # =================================================================\r# dense_9 (Dense) (None, 10) 20 # # dense_10 (Dense) (None, 10) 110 # # dense_11 (Dense) (None, 1) 11 # # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ keras.layers.Dense()の特徴の一つに、入力の次元を記述する必要がないという点があります。なぜこのような許容がされているのかは分かりませんが、コードの可読性のためには（特に他の人が見る可能性があるコードであれば）入力の次元を明示的に記述することが良いでしょう。このために、出力の次元が左、入力の次元が右に記述されるという特徴があります。したがって、モデルの構造を読むためには、アラビア語ではなく、右から左に読む必要があります。もし線形層を線形変換としての行列と考えた場合、$\\mathbf{y} = A\\mathbf{x}$ なので、入力が右、出力が左に来るのが自然です。しかし、TensorFlowはこのような数学的な厳密さを考慮して設計された言語ではないので、この理由だけでそう設計されたとは考えにくいです。数学的な厳密さを非常に重視するJuliaでも、線形層は Dense(in, out) のように実装されています。これは、左から右へ読む方が便利で分かりやすいためです。元々、$X$ から $Y$ への関数 $f$ の記述自体が $f : X \\to Y$ であり、（Kerasを除いて）世界のどこにも右から左へのマッピングで記述される関数はありません。\nデータ生成 サイン関数を訓練するため、データをサイン関数の関数値とし、モデルの出力とサイン関数のグラフを比較すると以下のようになります。\n# データ生成\rfrom math import pi\rx = tf.linspace(0., 2*pi, num=1000) # 入力データ\ry = tf.sin(x) # 出力データ(label)\r# モデルの出力確認\rimport matplotlib.pyplot as plt\rplt.plot(x, model(x), label=\u0026#34;model\u0026#34;)\rplt.plot(x, y, label=\u0026#34;sin\u0026#34;)\rplt.legend()\rplt.show() 訓練及び結果 from tensorflow.keras.optimizers import Adam\rmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\u0026#39;mse\u0026#39;) model.compile(optimizer, loss, metric) .compile() メソッドでオプティマイザと損失関数を指定します。他の主要なオプションには metric があり、これはモデルを評価する関数を意味します。これは loss と同じになることもありますし、異なることもあります。例えば、MLPで MNISTデータセット を学習する場合、lossは出力とラベルのMSEであり、metricは全データの中で予測に成功した割合になるでしょう。\n\u0026gt; model.fit(x, y, epochs=10000, batch_size=1000, verbose=\u0026#39;auto\u0026#39;)\r.\r.\r.\rEpoch 9998/10000\r1/1 [==============================] - 0s 8ms/step - loss: 6.2260e-06\rEpoch 9999/10000\r1/1 [==============================] - 0s 4ms/step - loss: 6.2394e-06\rEpoch 10000/10000\r1/1 [==============================] - 0s 3ms/step - loss: 6.2385e-06 .fit() メソッドに入力とラベル、エポック数、バッチサイズなどを入力すると訓練が実行されます。verboseは訓練の進行状況をどのように表示するかを決めるオプションで、0、1、2の中から選択でき、0は何も表示しません。他のオプションは以下のフォーマットで表示されます。 # verbose=1\rEpoch (現在のエポック)/(全エポック)\r(現在のバッチ)/(全体のバッチ) [==============================] - 0s 8ms/step - loss: 0.7884\r# verbose=2\rEpoch (現在のエポック)/(全エポック)\r(現在のバッチ)/(全体のバッチ) - 0s - loss: 0.7335 - 16ms/epoch - 8ms/step 訓練が終わり、サイン関数とモデルの関数値を比較すると、学習がうまく行われたことがわかります。\n関数型API Input() 関数と Model() 関数でレイヤーを直接連結する方法です。MLPのようなシンプルなモデルであれば、上記のシーケンシャルモデルで定義する方がはるかに簡単です。上のシーケンシャルモデルで定義したニューラルネットワークと同じ構造のモデルを定義する方法は次のようになります。\nfrom tensorflow.keras import Model\rfrom tensorflow.keras.layers import Input, Dense\rinput = Input(shape=(10)) # 変数は \u0026#34;出力の次元 = 最初の層の入力の次元\u0026#34;\rdense1 = Dense(10, activation = \u0026#34;relu\u0026#34;)(input)\rdense2 = Dense(10, activation = \u0026#34;relu\u0026#34;)(dense1)\routput = Dense(1)(dense2)\rmodel = Model(inputs=input, outputs=output)\rmodel.summary() # output↓\r# Model: \u0026#34;model_10\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param #\r# =================================================================\r# input_13 (InputLayer) [(None, 1)] 0\r# # dense_19 (Dense) (None, 10) 20\r# # dense_20 (Dense) (None, 10) 110\r# # dense_21 (Dense) (None, 1) 11\r# # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ Inputはインプットレイヤーを定義する関数です。正確にはレイヤーではなくテンソルですが、重要な点ではないので、ただの入力層として受け入れても良いでしょう。混乱する点は、出力の次元を入力する必要があるという点です。つまり、最初の層の入力の次元を入力する必要があります。これを定義した後、Dense関数の入力として入力し、明示的に直接各層を連結します。最後に、Model関数で入力と出力を引数に入れると、モデルを定義することができます。\nその後、モデルを .compile() メソッドでコンパイルし、.fit() メソッドで訓練するプロセスは、上で紹介した通りです。\n環境 OS: Windows11 Version: Python 3.9.13, tensorflow==2.12.0, keras==2.12.0 ","id":3562,"permalink":"https://freshrimpsushi.github.io/jp/posts/3562/","tags":null,"title":"텐서플로-케라스에서 시퀄션 모델, 함수형 API로 MLP 정의하고 훈련하는 법"},{"categories":"행렬대수","contents":"定義 1 2 置換行列 $P^{T}$ と 可逆行列 $A \\in \\mathbb{R}^{n \\times n}$ に対し、その 行列の積 $P^{T} A$ は $LU$ を与える。この分解を $A$ の PLU分解Permutation LU Decomposition と言う。$P$ は置換行列であるため、直交行列 となり、すなわち $P^{-1} = P^{T}$ であり、次のように表すことができる。 $$ P^{T} A = LU \\iff A = PLU $$\n説明 LU分解のアルゴリズム：$(a_{ij}) \\in \\mathbb{R}^{n \\times n}$ を可逆行列とする。\nStep 1. $k = 1$\n$u_{1j} = a_{1j}$ とし、$\\displaystyle l_{i1} = {{1} \\over {u_{11}}} a_{i1}$ を計算する。\nStep 2. $k = 2, 3, \\cdots , n-1$\nStep 2-1. 以下を計算する。 $$ u_{kk} = a_{kk} - \\sum_{s = 1}^{k-1} l_{ks} u_{sk} $$ Step 2-2. $j = k+1 , k+2 , \\cdots , n-1$ に対し以下を計算する。 $$ u_{kj} = a_{kj} - \\sum_{s = 1}^{k-1} l_{ks} u_{sj} $$ Step 2-3. $i = k+1 , k+2 , \\cdots , n-1$ に対し以下を計算する。 $$ l_{ik} = {{1} \\over {u_{kk}}} \\left{ a_{ik} - \\sum_{s = 1}^{k-1} l_{is} u_{sk} \\right} $$ Step 3. $k = n$ に対し以下を計算する。 $$ u_{nn} = a_{nn} - \\sum_{s = 1}^{n-1} l_{ns} u_{sn} $$\n行列のLU分解を行うには $u_{11} = a_{11}$ や $u_{kk}$ の逆数をとることが可能でなければならないが、 $$ A = \\begin{bmatrix} 0 \u0026amp; 3\\\\ 2 \u0026amp; 1 \\end{bmatrix} $$ のような 行列でもこのアルゴリズムを適用することはできない。LU分解を可能にするためにある置換行列 $P^{T}$ を乗じて $A$ を $PLU$ として表すことを PLU分解 と呼ぶ。もちろん、左または右、行または列が重要というわけではないため、 $$ A P^{T} = LU \\iff A = LUP $$ と書き LUP分解 と呼んでも差し支えない。\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://math.unm.edu/~loring/links/linear_s08/LU\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2,"permalink":"https://freshrimpsushi.github.io/jp/posts/2/","tags":null,"title":"PLU分解"},{"categories":"行列代数学","contents":"定義 1 各行で成分が一つだけ$1$で、残りがすべて$0$である正方行列$P \\in \\mathbb{R}^{n \\times n}$を順列行列と呼ぶ。\n基本的性質 直交性 すべての順列行列は直交行列である: $$P^{-1} = P^{T}$$\nスパース性 十分に大きな$n$に対して、$P \\in \\mathbb{R}^{n \\times n}$はスパース行列となる。\n説明 順列行列はその名前が示す通り、行列の乗算によって行と列の順列を与える。次の例では、左側に乗算すると行の順列となり、右側に乗算すると列の順列となることがわかる。 $$ \\begin{align*} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\\\ \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{12} \u0026amp; a_{11} \u0026amp; a_{13} \\\\ a_{22} \u0026amp; a_{21} \u0026amp; a_{23} \\\\ a_{32} \u0026amp; a_{31} \u0026amp; a_{33} \\end{bmatrix} \\end{align*} $$\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1,"permalink":"https://freshrimpsushi.github.io/jp/posts/1/","tags":null,"title":"順列行列"}]