[{"categories":"측도론","contents":"定義1 関数 $f : \\mathbb{R} \\to \\mathbb{R} (\\text{または } \\mathbb{C})$ が与えられたとする。$f$が任意の有限個の互いに素な区間 $(a_{i}, b_{i}) \\subset [a,b]$に対しても以下の条件を満たす場合、$[a, b]$ 上で 絶対連続absolutely continuousと言われる。\n$$ \\forall \\epsilon \\gt 0 \\quad \\exist \\delta \\gt 0 \\text{ such that } \\sum\\limits_{i=1}^{N} (b_{i} - a_{i}) \\lt \\delta \\implies \\sum\\limits_{i=1}^{N} \\left| f(b_{j}) - f(a_{j}) \\right| \\lt \\epsilon $$\n説明 定義により、絶対連続であれば一様連続でもある。\n性質 $f$が微分可能であり、導関数 $f\u0026rsquo;$ が有界であれば、$f$は絶対連続である。\n参照 実数関数の絶対連続性 測度の絶対連続性 符号付き測度の絶対連続性 Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p105\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3542,"permalink":"https://freshrimpsushi.github.io/jp/posts/3542/","tags":null,"title":"絶対連続実関数"},{"categories":"줄리아","contents":"概要 InfiniteArrays.jlは無限のサイズを持つ配列を使えるようにするパッケージ1で、実際にはレイジー配列と多くの関連がある。レイジー評価とは、配列に計算すべきものがなんであるかは知っているけど、本当に必要になるまでは計算を先延ばしにする方法のことだ。もちろん、コンピューターは無限を理解できないけど、この方法を使ってコンピューターでも無限配列を実装したんだ。\nコード ∞ julia\u0026gt; using InfiniteArrays\rjulia\u0026gt; 3141592 \u0026lt; ∞\rtrue\rjulia\u0026gt; Inf == ∞\rtrue\rjulia\u0026gt; Inf === ∞\rfalse InfiniteArrays.jlを読み込むと、まず ∞ 記号で無限を表せるようになる。無理にパッケージを使わなくても Infで無限を表せるけど、こっちの方が直感的に使えるようになるんだ。大小関係の比較では同じ無限の大きさを持つけど、ポインターとして見た場合は違っていて区別できる。\nℵ₀ julia\u0026gt; x = zeros(Int64, ∞);\rjulia\u0026gt; length(x)\rℵ₀ 0でいっぱいの無限配列を作ると、これは無限可算集合になり、その大きさはアレフゼロ $\\aleph_{0}$だ。\n普通に使える julia\u0026gt; x[2] = 3; x[94124843] = 7; x\rℵ₀-element LazyArrays.CachedArray{Int64, 1, Vector{Int64}, Zeros{Int64, 1, Tuple{InfiniteArrays.OneToInf{Int64}}}} with indices OneToInf():\r0\r3\r0\r0\r0\r0\r0\r0\r0\r⋮\rjulia\u0026gt; sum(x)\r10 無限配列だとしても、インターフェイスが大きく変わる訳ではない。普段扱ってる配列と同様に扱えば、考えている通りに動くよ。\n全コード using InfiniteArrays\r3141592 \u0026lt; ∞\rInf == ∞\rInf === ∞\rx = zeros(Int64, ∞);\rlength(x)\rx[2] = 3; x[94124843] = 7; x\rsum(x) 環境 OS: Windows julia: v1.7.3 https://github.com/JuliaArrays/InfiniteArrays.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2511,"permalink":"https://freshrimpsushi.github.io/jp/posts/2511/","tags":null,"title":"ジュリアで無限配列を使用する方法"},{"categories":"줄리아","contents":"概要 MAT.jlは MATLABで使用されるデータ保存形式である*.matファイルを読み書きするライブラリだ1。 Juliaがそうであるように、このパッケージは既存のプログラミング言語や習慣を捨てさせるのではなく、できるだけ馴染みのある環境を提供することでユーザーを獲得する戦略を示している。\nJuliaの速さと利便性は大きな利点だが、MATLABは研究目的の視覚化に独特の利点を持っている。既にMATLABで図を描く作業に習熟している場合、\u0026lsquo;Juliaへの完全な移行\u0026rsquo;は大きな犠牲を伴うため、魅力的ではない。MAT.jlの存在は「そんな心配するな、計算はJuliaで速くこなしてから、図はMATLABで描き直せばいいんだ」という誘惑そのものだ。 逆に、\u0026lsquo;MATLABで既に作業をして構築したものが多いが、何か限界を感じてJuliaに移りたい状況\u0026rsquo;でも役立つ。 mat形式より進化したJulia独自の保存方式は、JLD2.jlパッケージを参照すればいい。\nコード X = rand(0:9, 8, 3)\rusing MAT\rmatwrite(\u0026#34;example.mat\u0026#34;, Dict(\u0026#34;Y\u0026#34; =\u0026gt; X))\rmatfile = matopen(\u0026#34;elpmaxe.mat\u0026#34;)\rA = read(matfile, \u0026#34;A\u0026#34;)\rclose(matfile) Julia → MATLAB MATLAB → Julia 環境 OS: Windows julia: v1.7.3 MAT v0.10.3 MATLAB: R2022b https://github.com/JuliaIO/MAT.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2509,"permalink":"https://freshrimpsushi.github.io/jp/posts/2509/","tags":null,"title":"ジュリアでmatファイルを読み書きする方法"},{"categories":"줄리아","contents":"概要 UnicodePlots.jlはジュリア REPLでユニコード文字を使って図を出力するライブラリ1で、プログラムが進行する中で軽量でありながら高品質の視覚化を可能にする。\nコード using UnicodePlots\rp1 = lineplot(100 |\u0026gt; randn |\u0026gt; cumsum)\rp1 = lineplot!(p1, 100 |\u0026gt; randn |\u0026gt; cumsum); p1\rUnicodePlots.heatmap(cumsum(abs.(randn(100,100)), dims=2)) 上の例のコードを実行した結果は次の通りです。\n環境 OS: Windows julia: v1.7.3 UnicodePlots v3.0.4 https://github.com/JuliaPlots/UnicodePlots.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2507,"permalink":"https://freshrimpsushi.github.io/jp/posts/2507/","tags":null,"title":"ジュリアコンソールでシンプルなグラフィックを出力する方法"},{"categories":"줄리아","contents":"方法 コンソールでCtrl + Lを押すと、コンソールが一見してクリアされるが、一部の環境では本当にリセットされるわけではなく、ウィンドウが上にスクロールされたように見える場合もある。きれいに消去したり、キーボード入力に頼らないためには、ASCII文字\\033cを出力するといい12。\nprint(\u0026#34;\\033c\u0026#34;) また、\\007を出力すると通知音が鳴る。3 簡易的なシミュレーションの終了など、音で知りたい場合に意外と便利だ。\nprintln(\u0026#34;\\007\u0026#34;) 環境 OS: Windows julia: v1.7.3 https://stackoverflow.com/questions/26548687/julia-how-to-clear-console\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/questions/47503734/what-does-printf-033c-mean\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://discourse.julialang.org/t/how-to-play-a-sound-or-tone-when-a-program-ends/41239/13\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2505,"permalink":"https://freshrimpsushi.github.io/jp/posts/2505/","tags":null,"title":"ジュリアでコンソールを初期化する方法"},{"categories":"줄리아","contents":"概要 1 Juliaでは、dropmissing()関数を使って簡単に欠損値を削除できる。\nコード julia\u0026gt; df = DataFrame(x = [\u0026#34;i\u0026#34;, missing, \u0026#34;k\u0026#34;, \u0026#34;j\u0026#34;], y = [1, 2, 3, missing])\r4×2 DataFrame\rRow │ x y │ String? Int64? ─────┼──────────────────\r1 │ i 1\r2 │ missing 2\r3 │ k 3\r4 │ j missing 上記のように欠損値missingがあるデータフレームが与えられているとしよう。\njulia\u0026gt; dropmissing(df, :x)\r3×2 DataFrame\rRow │ x y │ String Int64? ─────┼─────────────────\r1 │ i 1\r2 │ k 3\r3 │ j missing julia\u0026gt; dropmissing(df, :y)\r3×2 DataFrame\rRow │ x y │ String? Int64 ─────┼────────────────\r1 │ i 1\r2 │ missing 2\r3 │ k 3 欠損値を削除したい列のシンボルを引数に入れればいい。\njulia\u0026gt; dropmissing(df)\r2×2 DataFrame\rRow │ x y │ String Int64\r─────┼───────────────\r1 │ i 1\r2 │ k 3 データフレーム全体から欠損値をすべて削除したい場合は、列を何も入力しなければいい。\n全コード using DataFrames\rdf = DataFrame(x = [\u0026#34;i\u0026#34;, missing, \u0026#34;k\u0026#34;, \u0026#34;j\u0026#34;], y = [1, 2, 3, missing])\rdropmissing(df, :x)\rdropmissing(df, :y)\rdropmissing(df) 環境 OS: Windows julia: v1.7.3 https://discourse.julialang.org/t/how-to-remove-rows-containing-missing-from-dataframe/12234/7\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2503,"permalink":"https://freshrimpsushi.github.io/jp/posts/2503/","tags":null,"title":"ジュリアでデータフレームの欠損値を削除する方法"},{"categories":"머신러닝","contents":"概要1 2 勾配降下法で使用されるアダプティブラーニングレートと、これを適用したモデルであるAdaGrad、RMSProp、Adamについて説明する。\n説明 勾配降下法で学習率learning rateは、パラメータが収束する速度、メソッドの成功の有無などを決定する重要なパラメータである。通常 $\\alpha$、$\\eta$と表記され、パラメータをアップデートする際、どれだけ勾配を反映するかを決める因子である。\n学習率による最適化の様子：大きな学習率(左)、小さな学習率(右)\r$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L (\\boldsymbol{\\theta}_{i}) $$\n基本的な勾配降下法では $\\alpha$ は定数として説明されているが、この場合 勾配はベクトルなので、全ての変数（パラメータ）に対して同じ学習率が適用される。\n$$ \\alpha \\nabla L (\\boldsymbol{\\theta}) = \\alpha \\begin{bmatrix} \\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} = \\begin{bmatrix} \\alpha\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nしたがって、学習率を $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{k})$ のようなベクトルとして考え、勾配項を以下の式のように一般化することができる。\n$$ \\boldsymbol{\\alpha} \\odot \\nabla L (\\boldsymbol{\\theta}) = \\begin{bmatrix} \\alpha_{1}\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha_{2}\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha_{k}\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nここで $\\odot$ は行列のアダマール積（要素毎の積）を表す。このようにパラメータ毎に異なる適用される学習率 $\\boldsymbol{\\alpha}$ を アダプティブラーニングレートadaptive learning rateと呼ぶ。以下の技術はアダプティブラーニングレートを勾配に依存して決定するため、$\\boldsymbol{\\alpha}$ は以下のような関数と見なすことができる。\n$$ \\boldsymbol{\\alpha} (\\nabla L(\\boldsymbol{\\theta})) = \\begin{bmatrix} \\alpha_{1}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\alpha_{2}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\dots \u0026amp; \\alpha_{k}(\\nabla L(\\boldsymbol{\\theta})) \\end{bmatrix} $$\n以下ではAdaGrad、RMSProp、Adamを紹介する。ここで重要な事実は、モーメンタム技術を含むこれらのオプティマイザー間に絶対的優位性はないということである。分野によって、作業によって最適なオプティマイザーは異なるので、単純に「何が最も良いか」についての判断や質問は良くない。自分が属する分野で主に使用されているものが何かを把握することが役に立ち、それがない場合やよくわからない場合は、SGD+モーメンタムまたはAdamを使用することが無難である。\nAdaGrad AdaGradは論文\u0026quot;(Duchi et al., 2011)Adaptive subgradient methods for online learning and stochastic optimization\u0026quot;で紹介されたアダプティブラーニングレート技術です。この名前はadaptive gradientの略で、[エイダグラード]または[アダグラード]と読みます。AdaGradでは、各パラメータに対する学習率を勾配に反比例するように設定します。ベクトル $\\mathbf{r}$ を次のように定義します。\n$$ \\mathbf{r} = (\\nabla L) \\odot (\\nabla L) = \\begin{bmatrix} \\left( \\dfrac{\\partial L}{\\partial \\theta_{1}} \\right)^{2} \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{2}} \\right)^{2} \u0026amp; \\cdots \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{k}} \\right)^{2} \\end{bmatrix} $$\n全体の学習率global learning rate $\\epsilon$、任意の小さい数 $\\delta$ に対して、アダプティブラーニングレート $\\boldsymbol{\\alpha}$ は次のようになります。\n$$ \\boldsymbol{\\alpha} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} $$\n式からわかるように、勾配の成分が大きい変数には学習率を小さくし、勾配の成分が小さい変数には学習率を大きく適用します。$\\delta$ は分母が $0$ や非常に小さい数になるのを防ぐためのもので、通常は $10^{-5} \\sim 10^{-7}$ の値を使用することが多いです。また、学習率は反復ごとに累積されます。$i$ 番目の反復での勾配を $\\nabla L _{i} = \\nabla L (\\boldsymbol{\\theta}_{i})$ とすると、\n$$ \\begin{align} \\mathbf{r}_{i} \u0026amp;= (\\nabla L_{i}) \\odot (\\nabla L_{i}) \\nonumber \\\\ \\boldsymbol{\\alpha}_{i} \u0026amp;= \\boldsymbol{\\alpha}_{i-1} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = \\sum_{j=1}^{i} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} \\\\ \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i} \\nonumber \\end{align} $$\nアルゴリズム: AdaGrad 入力 全体の学習率 $\\epsilon$、小さな正数 $\\delta$、エポック $N$ 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. for $i = 1, \\cdots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算し、各成分を二乗する 5. $\\boldsymbol{\\alpha} \\leftarrow \\boldsymbol{\\alpha} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # アダプティブ学習率更新 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # パラメータ更新 7. end for RMSProp RMSPropは Root Mean Square Propagationの略で、ジェフリー・ヒントンGeoffrey Hintonの講義 Neural networks for machine learningで提案されたアダプティブラーニングレート技術です。基本的にAdaGradの変形であり、追加される項が指数的に減少するように$(1)$の加算を加重和に変えただけです。$\\rho \\in (0,1)$に対して、\n$$ \\boldsymbol{\\alpha}_{i} = \\rho \\boldsymbol{\\alpha}_{i-1} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = (1-\\rho) \\sum_{j=1}^{i} \\rho^{i-j} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} $$\n通常は $\\rho = 0.9, 0.99$ のような大きな値が使用されます。\nアルゴリズム: RMSProp 入力 全体の学習率 $\\epsilon$、小さな正数 $\\delta$、減衰率 $\\rho$、エポック $N$ 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. for $i = 1, \\dots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算し、各成分を二乗する 5. $\\boldsymbol{\\alpha} \\leftarrow \\rho \\boldsymbol{\\alpha} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # アダプティブ学習率更新 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # パラメータ更新 7. end for Adam Adam\u0026ldquo;adaptive moments\u0026quot;から派生は論文\u0026rdquo;(Kingma and Ba, 2014)Adam: A method for stochastic optimization\u0026quot;で紹介されたオプティマイザです。アダプティブラーニングレートとモーメンタム技術を組み合わせたもので、RMSProp + モーメンタムと見ることができます。RMSPropとモーメンタムを理解していれば、Adamを理解するのは難しくありません。RMSProp、モーメンタム、Adamをそれぞれ比較すると、以下のようになります。$\\nabla L_{i} = \\nabla L(\\boldsymbol{\\theta}_{i})$とすると、\nMomentum\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + \\nabla L_{i} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\mathbf{p}_{i}$\rRMSProp\r$\\mathbf{r}_{i} = \\nabla L_{i} \\odot \\nabla L_{i} \\\\\r\\boldsymbol{\\alpha}_{i} = \\beta_{2} \\boldsymbol{\\alpha}_{i-1} + (1-\\beta_{2})\\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} \\quad (\\boldsymbol{\\alpha}_{0}=\\mathbf{0})\\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i}$\rAdam\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + (1-\\beta_{1}) \\nabla L_{i-1} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\\\\[0.5em]\r\\hat{\\mathbf{p}}_{i} = \\dfrac{\\mathbf{p}_{i}}{1-(\\beta_{1})^{i}} \\\\\\\\[0.5em]\r\\mathbf{r}_{i} = \\beta_{2} \\mathbf{r}_{i-1} + (1-\\beta_{2}) \\nabla L_{i} \\odot \\nabla L_{i} \\\\\\\\[0.5em]\r\\hat{\\mathbf{r}}_{i} = \\dfrac{\\mathbf{r}}{1-(\\beta_{2})^{i}} \\\\\\\\[0.5em]\r\\hat{\\boldsymbol{\\alpha}}_{i} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}_{i}}} \\\\\\\\[0.5em]\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\hat{\\boldsymbol{\\alpha}_{i}} \\odot \\hat{\\mathbf{p}_{i}}\r$\r$\\hat{\\mathbf{p}}_{i}$および$\\hat{\\mathbf{r}}_{i}$を計算する際に$1 - \\beta^{i}$で割る理由は、$\\mathbf{p}_{i}$および$\\mathbf{r}_{i}$が加重和であるため、これを加重平均に変換するためです。\nアルゴリズム: Adam\r入力 全体の学習率 $\\epsilon$ (推奨値は $0.001$), エポック $N$ 小さな定数 $\\delta$ (推奨値は $10^{-8}$) 減衰率 $\\beta\\_{1}, \\beta\\_{2}$ (推奨値はそれぞれ $0.9$ と $0.999$) 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. モーメンタムを $\\mathbf{p} = \\mathbf{0}$ で初期化する。 4. for $i = 1, \\dots, N$ do 5. $\\mathbf{p} \\leftarrow \\beta\\_{1}\\mathbf{p} + (1-\\beta\\_{1}) \\nabla L$ # 加重和でモーメンタム更新 6. $\\hat{\\mathbf{p}} \\leftarrow \\dfrac{\\mathbf{p}}{1-(\\beta\\_{1})^{i}}$ # 和を加重平均に補正 7. $\\mathbf{r} \\leftarrow \\beta\\_{2} \\mathbf{r} + (1-\\beta\\_{2}) \\nabla L \\odot \\nabla L$ # 加重和で勾配の二乗ベクトル更新 8. $\\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1-(\\beta\\_{2})^{i}}$ # 和を加重平均に補正 9. $\\hat{\\boldsymbol{\\alpha}} \\leftarrow \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}}}$ # アダプティブ学習率更新 10. $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\alpha}} \\odot \\hat{\\mathbf{p}}$ # パラメータ更新 11. end for Ian Goodfellow, Deep Learning, 8.5 アダプティブラーニングレートを用いたアルゴリズム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nオ・イルソク, 機械学習(MACHINE LEARNING) (2017), ch 5.4 アダプティブラーニングレート\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3529,"permalink":"https://freshrimpsushi.github.io/jp/posts/3529/","tags":null,"title":"適応的な学習率: AdaGrad, RMSProp, Adam"},{"categories":"줄리아","contents":"概要 Juliaで環境変数を参照する方法を説明する1。\nコード Base.ENV\rBase.ENV[\u0026#34;JULIA_NUM_THREADS\u0026#34;] 見るように、別のパッケージをロードする必要はなく、Base.ENVを通じて直接アクセスできる。辞書として読まれるため、求める環境変数の名前をキーとして置くと、その環境変数を文字列で得る。上のコード2行を実行した結果は以下の通りだ。\njulia\u0026gt; Base.ENV\rBase.EnvDict with 62 entries:\r\u0026#34;ALLUSERSPROFILE\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\u0026#34;\r\u0026#34;APPDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\AppData\\\\Roaming\u0026#34;\r\u0026#34;CHROME_CRASHPAD_PIPE_NAME\u0026#34; =\u0026gt; \u0026#34;\\\\\\\\.\\\\pipe\\\\crashpad_14984_WLSYYXMTXMJWXZQG\u0026#34;\r\u0026#34;COMMONPROGRAMFILES\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\Common Files\u0026#34;\r\u0026#34;COMMONPROGRAMFILES(X86)\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files (x86)\\\\Common Files\u0026#34;\r\u0026#34;COMMONPROGRAMW6432\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\Common Files\u0026#34;\r\u0026#34;COMPUTERNAME\u0026#34; =\u0026gt; \u0026#34;SICKRIGHT\u0026#34;\r\u0026#34;COMSPEC\u0026#34; =\u0026gt; \u0026#34;C:\\\\WINDOWS\\\\system32\\\\cmd.exe\u0026#34;\r\u0026#34;CUDA_PATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.5\u0026#34;\r\u0026#34;CUDA_PATH_V11_5\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.5\u0026#34;\r\u0026#34;DRIVERDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData\u0026#34;\r\u0026#34;GOPATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\go\u0026#34;\r\u0026#34;HOMEDRIVE\u0026#34; =\u0026gt; \u0026#34;C:\u0026#34;\r\u0026#34;HOMEPATH\u0026#34; =\u0026gt; \u0026#34;\\\\Users\\\\rmsms\u0026#34;\r\u0026#34;JULIA_NUM_THREADS\u0026#34; =\u0026gt; \u0026#34;16\u0026#34;\r\u0026#34;LOCALAPPDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\AppData\\\\Local\u0026#34;\r\u0026#34;LOGONSERVER\u0026#34; =\u0026gt; \u0026#34;\\\\\\\\SICKRIGHT\u0026#34;\r\u0026#34;NAVER\u0026#34; =\u0026gt; \u0026#34;e=2.718281\u0026#34;\r\u0026#34;NUMBER_OF_PROCESSORS\u0026#34; =\u0026gt; \u0026#34;16\u0026#34;\r\u0026#34;NVCUDASAMPLES11_5_ROOT\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v11.5\u0026#34;\r\u0026#34;NVCUDASAMPLES_ROOT\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v11.5\u0026#34;\r\u0026#34;NVTOOLSEXT_PATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NvToolsExt\\\\\u0026#34;\r\u0026#34;ONEDRIVE\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive - knu.ac.kr\u0026#34;\r\u0026#34;ONEDRIVECOMMERCIAL\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive - knu.ac.kr\u0026#34;\r\u0026#34;ONEDRIVECONSUMER\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive\u0026#34;\r\u0026#34;OPENBLAS_MAIN_FREE\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;\r\u0026#34;OPENBLAS_NUM_THREADS\u0026#34; =\u0026gt; \u0026#34;8\u0026#34;\r⋮ =\u0026gt; ⋮\rjulia\u0026gt; Base.ENV[\u0026#34;JULIA_NUM_THREADS\u0026#34;]\r\u0026#34;16\u0026#34; 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/base/#Base.ENV\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2499,"permalink":"https://freshrimpsushi.github.io/jp/posts/2499/","tags":null,"title":"ジュリアで環境変数を参照する方法"},{"categories":"머신러닝","contents":"概要1 2 勾配降下法におけるモーメンタム技術は、パラメーターを更新する際に以前の勾配もすべて使用することである。これが本質であり、これに尽きる。しかし、奇妙な更新式や物理学の運動量が動機となったとか、質量を$1$に設定し初期速度を$0$にするといった説明は理解を難しくするだけである。本稿では、モーメンタム技術をできるだけシンプルに説明する。\nビルドアップ パラメーターを$\\boldsymbol{\\theta}$、損失関数を$L$とするとき、標準的な勾配降下法は、以下のように反復的にパラメーターを更新する方法である。\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} $$\nここで$L_{i} = L(\\boldsymbol{\\theta}_{i})$は、$i$番目の反復で計算された損失関数を意味する。モーメンタム技術とは、これに単に前の反復で計算された損失関数の勾配$\\nabla L_{i-1}$を加えることに過ぎない。\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\alpha \\nabla L_{i-1} - \\cdots - \\alpha \\nabla L_{0} $$\nここで、反復が進むにつれて勾配の影響を減らし、勾配の合計が発散するのを防ぐために係数$\\beta \\in (0,1)$を追加すると、次のようになる。\n$$ \\begin{align} \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\beta \\alpha \\nabla L_{i-1} - \\cdots - \\beta^{i}\\alpha \\nabla L_{0} \\nonumber \\\\ \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha\\sum_{j=0}^{i} \\beta^{j} \\nabla L_{i-j} \\end{align} $$\n定義 $(1)$のようにパラメーターを更新することをモーメンタム技術momentum methodと呼び、追加される項$\\alpha\\sum\\limits_{j=0}^{i} \\beta^{j} \\nabla L_{i-j}$をモーメンタムmomentumと呼ぶ。\n説明 上記の定義によれば、モーメンタム技術は一般化された勾配降下法であり、むしろ勾配降下法はモーメンタム技術の$\\beta = 0$の特別なケースに過ぎないと見ることができる。$\\beta$が$1$に近いほど以前の勾配を多く反映し、$0$に近いほど少なく反映する。\n勾配降下法は、現在の傾きが最も大きい方向へパラメーターを更新するために貪欲アルゴリズムである。モーメンタム技術は勾配降下法の貪欲な部分を少し和らげ、現在最善の選択ではないが長期的にはより有効な選択をすることができるようにする。また、勾配の方向が急激に変わるのを防ぐことができる。\n当然ながら、パラメーターを更新する際の勾配の大きさが勾配降下法より大きいため、収束速度が速いという利点がある。また、経験的に局所的最小値local minimaから比較的脱出しやすいことが知られており、坂を転がり下るボールが十分な速さであれば、下り坂の途中にある小さな坂も越えて通り過ぎることができると説明される。\nここで重要な事実は、適応的学習率技術を含むこれらのオプティマイザー間に絶対的な優位性はないということである。分野や作業によって最適なオプティマイザーが異なるため、「何が最も良いか」という判断や質問は適切ではない。自分が所属する分野で主に使用されているものが何かを知ることが役立ち、それがないか分からなければSGD+モーメンタムまたはAdamを使用するのが無難である。\nネステロフのモーメンタム モーメンタム技術を再検討すると、次のパラメーター$\\boldsymbol{\\theta}_{i+1}$を得るために、現在のパラメーター$\\boldsymbol{\\theta}_{i}$に現在のパラメーターで計算された勾配$\\alpha \\nabla L(\\boldsymbol{\\theta}_{i})$を蓄積しながら加えていく。\nネステロフのモーメンタムNesterov momentumまたはネステロフ加速勾配Nesterov accelerated gradient, NAGと呼ばれる技術は、「現在のパラメーターに前の勾配を加えた値」で勾配を求め、これを現在のパラメーターに加えて次のパラメーターを求める。言葉ではやや複雑だが、モーメンタム技術を理解していれば、以下のアルゴリズムを見ることでネステロフのモーメンタムを理解するのが簡単になるかもしれない。\nアルゴリズム モーメンタム項を$\\mathbf{p}$と表す。\nアルゴリズム: モーメンタム技術 入力 学習率 $\\alpha$, モーメンタムパラメーター $\\beta$, エポック $N$ 1. パラメーター $\\boldsymbol{\\theta}$を任意の値で初期化する。 2. モーメンタムを $\\mathbf{p} = \\mathbf{0}$で初期化する。 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算しモーメンタムを更新 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # パラメーターを更新 6. end for アルゴリズム: ネステロフのモーメンタム 入力 学習率 $\\alpha$, モーメンタムパラメーター $\\beta$, エポック $N$ 1. パラメーター $\\boldsymbol{\\theta}$を任意の値で初期化する。 2. モーメンタムを $\\mathbf{p} = \\mathbf{0}$で初期化する。 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta} + \\beta \\mathbf{p})$ # 勾配を計算しモーメンタムを更新 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # パラメーターを更新 6. end for 両方の方法について初めの数回の計算を見ると、以下のようになる。簡単に$\\mathbf{p}_i = \\alpha \\nabla L_i$、および$\\mathbf{p}^i = \\alpha \\nabla L(\\boldsymbol{\\theta}_i - \\beta^{1}\\mathbf{p}^{i-1} - \\beta^{2}\\mathbf{p}^{i-2} - \\cdots - \\beta^{i}\\mathbf{p}^{0})$（このとき$\\mathbf{p}^{0} = \\mathbf{p}_0$）と表記すると、\nモーメンタム ネステロフのモーメンタム $\\boldsymbol{\\theta}\\_{0} =$ 初期値\r$\\boldsymbol{\\theta}\\_{0} =$ 初期値\r$\\boldsymbol{\\theta}_{1} = \\boldsymbol{\\theta}_{0} - \\alpha \\nabla L_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{0} - \\mathbf{p}_{0}$\r$\\boldsymbol{\\theta}_{1} = \\boldsymbol{\\theta}_{0} - \\alpha \\nabla L_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{0} - \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}_{2} = \\boldsymbol{\\theta}_{1} - \\alpha\\nabla L_{1} - \\beta \\mathbf{p}_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{1} - \\mathbf{p}_{1} - \\beta \\mathbf{p}_{0}$\r$\\boldsymbol{\\theta}_{2} = \\boldsymbol{\\theta}_{1} - \\alpha \\nabla L(\\boldsymbol{\\theta}_{1} - \\beta \\mathbf{p}^{0}) - \\beta \\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{1} - \\mathbf{p}^{1} - \\beta \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}_{3} = \\boldsymbol{\\theta}_{2} - \\mathbf{p}_{2} - \\beta \\mathbf{p}_{1} - \\beta^{2} \\mathbf{p}_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{2} - \\sum\\limits_{j=0}^{2}\\beta^{j}\\mathbf{p}_{2-j}$\r$\\boldsymbol{\\theta}_{3} = \\boldsymbol{\\theta}_{2} - \\alpha \\nabla L(\\boldsymbol{\\theta}_{2} - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0}) - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{2} - \\mathbf{p}^{2} - \\beta \\mathbf{p}^{1} - \\beta^{2} \\mathbf{p}^{0}$\r$$\\vdots$$\r$$\\vdots$$\r$\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\sum\\limits_{j=0}^{i}\\beta^{j}\\mathbf{p}_{i-j}$\r$\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\sum\\limits_{j=0}^{i}\\beta^{j}\\mathbf{p}^{i-j}$\rイアン・グッドフェロー, ディープラーニング, 第8.3.2節 モーメンタム, 第8.3.3節 ネステロフのモーメンタム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nオ・イルソク, 機械学習(MACHINE LEARNING) (2017), 第5.3節 モーメンタム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3528,"permalink":"https://freshrimpsushi.github.io/jp/posts/3528/","tags":null,"title":"勾配降下における運動量法"},{"categories":"줄리아","contents":"概要 ジュリアでもプログラムの進行状況を知らせてくれるグラスバーを手軽に使うことができる。\nコード ProgressMeter.jl 「ProgressMeter.jl」パッケージの「@showprogress」マクロを「for」ループに置けばよい1。\nusing ProgressMeter\rchi2 = []\r@showprogress for n in 1:20000\rpush!(chi2, sum(randn(n) .^ 2))\rend 下の「ProgressBars.jl」に比べるとマクロを使うのでコードがより簡潔である。\nProgressBars.jl 「ProgressBars.jl」パッケージの「ProgressBar()」関数で「for」ループの反復子Iteratorを包めば良い2。\nusing ProgressBars\rchi2 = []\rfor n in ProgressBar(1:20000)\rpush!(chi2, sum(randn(n) .^ 2))\rend 実際の作業内容はどうであれ構わないが、プログラムの進行状況は次のようにきれいに出力される。 当然だが、「for」ループ文で正確に何回目の繰り返しになっているかだけ分かるので、1回の繰り返し当たりの平均遂行時間を知らせるだけで、正確な所要時間を予測することはできない。\n環境 OS: Windows julia: v1.7.3 https://github.com/timholy/ProgressMeter.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/cloud-oak/ProgressBars.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2497,"permalink":"https://freshrimpsushi.github.io/jp/posts/2497/","tags":null,"title":"ジュリアでプログレスバーの使い方"},{"categories":"줄리아","contents":"概要 R言語の合計sum()や平均mean()には、関数自体がその欠損値を無視するオプションを持っているが、ジュリアではそのようなオプションがない代わりに関数型プログラミングFunctional Programming的な方法を積極的に使っている。\nコード julia\u0026gt; data = [0,1,2,3,0]\r5-element Vector{Int64}:\r0\r1\r2\r3\r0\rjulia\u0026gt; sum(data) / length(data)\r1.2\rjulia\u0026gt; sum(data) / sum(!iszero, data)\r2.0 上は$0$まで含んで全ての標本数で割った1.2、下は!iszeroという関数を引数に与えて$0$でない値だけをカウントして得た標本数で割った2.0を得た。Rよりも強力だと言える部分は、関数自体のオプションに頼らずにisnan(), isinf(), ismissing()などの多数の例外処理関数を同じ方法で使用でき、カスタムが自由であることだ。\nパフォーマンスに大きな違いはないが、is~系の関数のリターンが必ずブーリアンであるなら、分母のsum()がcount()に変わっても問題ない。\n環境 OS: Windows julia: v1.7.0 ","id":2495,"permalink":"https://freshrimpsushi.github.io/jp/posts/2495/","tags":null,"title":"ジュリアで0または欠損値を除外した平均値の計算方法"},{"categories":"통계적분석","contents":"定義 1 特に$r \u0026gt; 1$の時、ユークリッド空間の固定された部分集合$D \\in \\mathbb{R}^{r}$に対して、以下の$p$-変量ランダムベクトルの集合$Y(s) : \\Omega \\to \\mathbb{R}^{p}$を空間過程とも呼ぶ。 $$ \\left\\{ Y(s) : s \\in D \\right\\} $$ 特に空間過程が有限集合で、次のようにベクトルで表現される時は、ランダムフィールドと呼ぶ。 $$ \\left( Y \\left( s_{1} \\right) , \\cdots , Y \\left( s_{n} \\right) \\right) $$\n説明 特に空間データの中でポイント参照データを扱う時には、$Y(s)$は$s$に対して連続的にサンプリングできると仮定するが、実際に得られる実現の$D = \\left\\{ s_{1} , \\cdots , s_{n} \\right\\}$は有限集合だろう。\n大学の確率過程論の授業では、通常$r = p = 1$と$[ 0 , \\infty ) \\subset \\mathbb{R}$に関する次のような確率過程だけを学ぶ。 $$ \\left\\{ Y_{t} : t \\in [ 0 , \\infty ) \\right\\} $$ こうして時系列データに対する背景のような感じで確率過程に触れてきたなら、空間過程の定義は多少戸惑うかもしれない。しかし、一般的な確率過程の定義はただ「ランダムエレメントの集合」で十分なので、$\\left\\{ Y(s) \\right\\} _{s \\in D}$を確率過程と呼ばない理由はない。\n空間過程を時間過程の一般化と呼ぶよりは、最初から彼らは区別されていなかったと言った方が正しい。よく分からなければ、時系列を扱う時の時間だけの1次元軸$\\mathbb{R}^{1}$もちゃんとしたユークリッド空間だと思い出すといい。よく考えてみれば、時間$t \\in \\mathbb{R}$の流れに沿う確率'過程'自体も、日常用語とそんなに通じなかったから、空間'過程'という表現にあまり違和感を持たなくてもいい。\nBanerjee. (2003). Hierarchical Modeling and Analysis for Spatial Data: p23.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2494,"permalink":"https://freshrimpsushi.github.io/jp/posts/2494/","tags":null,"title":"空間プロセス"},{"categories":"전자기학","contents":"질문 電磁気学は文字通り電場 $\\mathbf{E}$と磁場 $\\mathbf{B}$について学ぶ学問です。電磁気学を学ぶ中で一度は次のような疑問を持ったことでしょう。\nなぜ磁場の記号として$\\mathbf{B}$を使用するのか？\n電場が$\\mathbf{E}$であるのはElectric fieldから来ているからとしても、磁場はMagnetic fieldなのになぜ$\\mathbf{B}$なのでしょうか？一見不自然に感じる記号ですが、これは実際に大きな理由なく定められたものです。\r回答 マックスウェルの記法1 2 マックスウェルはマックスウェル方程式を通じて古典電磁気学3を完成させ、「電磁気学の父」と呼ばれています。ニュートン、ライプニッツ、オイラーなど、数学/科学で顕著な成果を上げた人々は、名前と業績だけでなく、彼らの記法までもが後世に残ることになります。これはマックスウェルにも当てはまり、磁場を$\\mathbf{B}$、補助場を$\\mathbf{H}$と記すことも、マックスウェルがそう記したために自然と続いていると考えられています。\nマックスウェルが電磁気学で登場する様々なベクトルの記号に使用した文字4は以下の通りです。5 マックスウェルはこれらのベクトルをAからJまでのアルファベットで表記しましたが、C, D, Fなど記号に相応しいものがある場合はそれを用い、残りはマックスウェルの裁量で定められたようです。\n記法\r意味\rマックスウェル\r現在\r$\\frak{A}$$(A)$\r$\\mathbf{A}$\rその点の電磁気モーメンタム\r現在ではベクトルポテンシャルと呼ばれている。\r$\\frak{B}$$(B)$\r$\\mathbf{B}$\r磁気誘導\r現在では磁場と呼ばれている。\r$\\frak{C}$$(C)$\r$I$\r（全）電流、電流\r$\\frak{D}$$(D)$\r$\\mathbf{D}$\r電気'D'isplacement、変位場\r$\\frak{E}$$(E)$\r$\\mathcal{E}$\r電気'E'motive intensity\r現在では起電力electromotive force, emfと言われている。\r$\\frak{F}$$(F)$\r$\\mathbf{F}$\r機械的'F'orce\r現在ではローレンツ力と呼ばれている。\r$\\frak{G}$$(G)$\r点の速度\r$\\frak{H}$$(H)$\r$\\mathbf{H}$\r磁気力\r現在ではH-フィールドH-field、補助場auxiliary field、磁場強度magnetic field intensityなどと呼ばれている。\n$\\frak{I}$$(I)$\r$\\mathbf{M}$\r磁化'I'ntensity\r現在では磁化密度と呼ばれる物理量のようだ。\r$\\frak{J}$$(J)$\r$\\mathbf{J}$\r導電性の電流、導電電流\rほとんどの記号は今もそのまま使用されており、電流は現在、currentのintensityの頭文字を取って$I$と表記されています。\nまた、この関連で検索すると'ビオ・サバールの法則ではBiotの名前から取った'という主張も見つかりますが、私の意見ではそうではありません。まず'磁場の記号はなぜ$\\mathbf{B}$なのか?'という質問は'現在、磁場の記号としてなぜ$\\mathbf{B}$を使用するのか?'という質問と同じであり、これに対する答えは'マックスウェルがそう使用したから'が妥当だと思います。それならば、'マックスウェルが磁場の記号として$\\mathbf{B}$を使用したのはBiotの名前から取ったからではないか?'と考えることもできます。しかし、この回答には明確な根拠があるわけではないようです。もしそうだとしてもBiotの名前から$\\mathbf{B}$であるというよりは、'上記のベクトルの中で記号$\\mathbf{B}$と最も適合するのは磁気誘導、つまりビオ・サバールの法則と関連があるものだ'という説明の方が適切ではないでしょうか？（正直、Biot、bi-polar field、borealから取ったというのは無理矢理感があると思います）\nBとHのうち磁場はどちら？ 一方で$\\mathbf{B}$と$\\mathbf{H}$のうち、どちらを磁場magnetic fieldと呼ぶべきかについての議論もあります。$\\mathbf{H}$は媒質に関係なく実験で制御可能な値です。そのため、一般に工学関連の分野（例えば電気技師の教科書）では$\\mathbf{H}$を磁場と呼び、物理学関連の分野では$\\mathbf{B}$を磁場と呼ぶことが一般的です。しかし、ウィキの磁場の記事でも説明されているように、ローレンツ力を媒介するのが$\\mathbf{B}$であるため、$\\mathbf{E}$を電場と呼ぶのと同様に$\\mathbf{B}$を磁場と呼ぶのが一貫性があり、妥当だと考えられます。\n$$ \\text{Lorentz force}: \\mathbf{F}=Q\\left[ \\mathbf{E} + (\\mathbf{v}\\times\\mathbf{B}) \\right] $$\nhttps://www.johndcook.com/blog/2012/02/12/why-magnetic-field-b/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.cantorsparadise.com/why-the-symbol-for-magnetic-field-is-b-e40658e17ece\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n量子力学的現象を考慮しない\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nフラクトゥールFraktur書体である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMaxwell, James Clerk. A treatise on electricity and magnetism. Vol. 2. Oxford: Clarendon Press, 1873. page 257\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3523,"permalink":"https://freshrimpsushi.github.io/jp/posts/3523/","tags":null,"title":"磁場の記号にBを使う理由"},{"categories":"줄리아","contents":"概要 Juliaの回帰分析を行うためのGLM.jlパッケージを簡単に紹介する1。この説明では、Rのインターフェースとどれくらい似ているかを強調するため、詳細な説明は省略する。\nコード ジュリア using GLM, RDatasets\rfaithful = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;faithful\u0026#34;)\rout1 = lm(@formula(Waiting ~ Eruptions), faithful) 上記のコードを実行した結果は以下の通りである。\njulia\u0026gt; out1 = lm(@formula(Waiting ~ Eruptions), faithful)\rStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\rWaiting ~ 1 + Eruptions\rCoefficients:\r───────────────────────────────────────────────────────────────────────\rCoef. Std. Error t Pr(\u0026gt;|t|) Lower 95% Upper 95%\r───────────────────────────────────────────────────────────────────────\r(Intercept) 33.4744 1.15487 28.99 \u0026lt;1e-84 31.2007 35.7481\rEruptions 10.7296 0.314753 34.09 \u0026lt;1e-99 10.11 11.3493\r─────────────────────────────────────────────────────────────────────── Rでの回帰分析の結果と比較してみてください。\nRとの比較 out1\u0026lt;-lm(waiting~eruptions,data=faithful); summary(out1) out1 = lm(@formula(Waiting ~ Eruptions), faithful) 上はRのコードで、下はJuliaのコードである。変数を入力するために@formulaマクロを使用し、Rの慣習をほぼ完璧に再現できていることがわかる。\n環境 OS: Windows julia: v1.7.0 GLM v1.8.0 一緒に見る Rで回帰分析を行う方法 https://juliastats.org/GLM.jl/v0.11/index.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2493,"permalink":"https://freshrimpsushi.github.io/jp/posts/2493/","tags":null,"title":"ジュリアで回帰分析を行う方法"},{"categories":"머신러닝","contents":"概要 モンテカルロ積分は、与えられた関数の積分を計算するのが困難な場合に使用される数値的近似方法の一つである。次のような状況を想定しよう。与えられた $[0, 1]$または一般的に $[0, 1]^{n}$で積分可能な関数 $f$に対して、私たちは $f(x)$の式を知っているが、その積分を計算するのは簡単ではない。しかし、私たちは $f$の積分 $I[f]$を計算したい。\n$$ \\begin{equation} I[f] = \\int_{[0,1]} f(x) dx \\end{equation} $$\n定義 モンテカルロ積分Monte Carlo integrationとは、与えられた $[0, 1]$ 上での分布に基づきサンプル $\\left\\{ x_{i} \\right\\}$を抽出し、$f$の積分を次のように推定estimateする方法である。\n$$ I[f] \\approx I_{n}[f] := \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\n区分求積法との違い 区分求積法のアイデアは、区間 $[0,1]$を $n$等分し、点 $\\left\\{ x_{i} = \\frac{i-1}{n} \\right\\}_{i=1}^{n}$を得て、これらの点での関数値を全て加算することである。\n$$ \\text{区分求積法}[f] = \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\n式の見た目だけではモンテカルロ積分と区分求積法は異なるもののないように見えるが、その意味は全く異なる。区分求積法での $\\left\\{ x_{i} \\right\\}$は区間 $[0, 1]$を $n$等分して得た点であるのに対し、モンテカルロ積分では $x$が従う分布 $p(x)$から抽出された $n$個のサンプルを意味する。したがって、区分求積法で得られた値は単純に $f$が描くグラフの下の面積を意味するが、モンテカルロ積分で得られた値は $f$の期待値である。\n性質 式 $(1)$が持つ統計的な意味は「$I[f]$は $X$が一様分布に従うときの $f(X)$の期待値と同じである」ということである。\n$$ X \\sim U(0,1) \\implies I[f] = \\int_{[0,1]} f(x) dx = E\\left[ f(X) \\right] $$\n期待値 確率変数 $X$が一様分布に従うとしよう。$I_{n}[f]$は $I[f]$の不偏推定量である。\n$$ E\\left[ I_{n}[f] \\right] = I[f] $$\n証明 $$ \\begin{align*} E\\left[ I_{n}[f] \\right] \u0026amp;= E\\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} E\\left[ f(X_{i}) \\right] \\qquad \\text{by linearity of $E$} \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} I\\left[ f \\right] \\\\ \u0026amp;= I\\left[ f \\right] \\end{align*} $$\n■\n分散 証明 分散の性質\n[a] $\\Var (aX) = a^{2} \\Var (X)$\n[b] $X, Y$が独立ならば、$\\Var (X + Y) = \\Var(X) + \\Var(Y)$\n$f(X)$の分散を $\\sigma^{2}$としよう。すると分散の性質により、\n$$ \\begin{align*} \\Var \\left[ I_{n}[f] \\right] \u0026amp;= \\Var \\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\Var \\left[ \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\Var \\left[ f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\sigma^{2} \\\\ \u0026amp;= \\dfrac{\\sigma^{2}}{n} \\end{align*} $$\n■\n一般化 ここで $p(x) \\ge 0$で $\\int_{[0,1]} p = 1$となる関数 $p$について、積分 $I[fp]$を考えよう。\n$$ I[fp] = \\int_{[0, 1]}f(x)p(x) dx $$\nこれは確率密度関数が $p$である確率変数 $X$について、$f(X)$の期待値と同じである。この値を近似する方法として、次の二つの方法が考えられる。\nサンプル $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$を一様分布から抽出し、$I[fp]$を次のように近似する。 $$ X_{i} \\sim U(0,1) \\qquad I[fp] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i})p(x_{i}) $$\nサンプル $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$を $p(x)$から抽出し、$I[fp]$を次のように近似する。 $$ X_{i} \\sim p(x) \\qquad I[fp] = I_{p}[f] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i}) $$\n言い換えれば、1.は $f(x)p(x)$を一様分布でサンプリングして平均を求めたものであり、2.は $f(x)$を $p(x)$でサンプリングして平均を求めたものである。これらのうち分散がより小さいのは1.である。$I = I[fp] = I[fp]$と簡単に記しよう。\n1.の場合 $$ \\begin{align*} \\sigma_{1}^{2} = \\Var [fp] \u0026amp;= E \\left[ (fp - I)^{2} \\right] \\\\ \u0026amp;= \\int (fp - I)^{2} dx \\\\ \u0026amp;= \\int (fp)^{2} dx - 2I\\int fp dx + I^{2}\\int dx\\\\ \u0026amp;= \\int (fp)^{2} dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int (fp)^{2} dx - I^{2}\\\\ \\end{align*} $$\n2.の場合 $$ \\begin{align*} \\sigma_{2}^{2} = \\Var [f] \u0026amp;= E_{p} \\left[ (f - I)^{2} \\right] \\\\ \u0026amp;= \\int (f - I)^{2}p dx \\\\ \u0026amp;= \\int f^{2}p dx - 2I\\int fp dx + I^{2}\\int pdx\\\\ \u0026amp;= \\int f^{2}p dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int f^{2}p dx - I^{2}\\\\ \\end{align*} $$\nしかし $0 \\le p \\le 1$であるため、$f^{2}p \\ge f^{2}p^{2}$である。したがって\n$$ \\sigma_{1}^{2} \\le \\sigma_{2}^{2} $$\n","id":3515,"permalink":"https://freshrimpsushi.github.io/jp/posts/3515/","tags":null,"title":"モンテカルロ積分"},{"categories":"선형대수","contents":"定義 $V$を$n$次元のベクトル空間と呼ぶ。与えられた定数$a_{ij} \\in \\mathbb{R}(\\text{or } \\mathbb{C})$について、以下の2次斉次関数$A : V \\to \\mathbb{R}(\\text{or } \\mathbb{C})$を二次形式という。\n$$ A(\\mathbf{x}) := \\sum\\limits_{i,j=1}^{n} a_{ij}x_{i}x_{j},\\qquad (a_{ij} = a_{ji}) $$\nこの時、$\\mathbf{x} = \\begin{bmatrix} x_{1} \u0026amp; \\cdots \u0026amp; x_{n} \\end{bmatrix}^{T}$である。$i \\ne j$に対する$a_{ij}x_{i}x_{j}$を混合項という。\n説明 定義によると、$A(\\lambda \\mathbf{x}) = \\lambda^{2} A(\\mathbf{x})$が成り立つ。\n行列形 $A$を$n\\times n$対称行列$A = \\begin{bmatrix} a_{ij} \\end{bmatrix}$とする。行列$A$に対する二次形式を、Quadraticの頭文字を取って、$Q_{A}(\\mathbf{x})$と表記し、$A$に関連する二次形式と呼ぶ。\n$$ Q_{A}(\\mathbf{x}) = \\mathbf{x}^{T}A\\mathbf{x} =\\begin{bmatrix} x_{1} \u0026amp; \\cdots \u0026amp;x_{n} \\end{bmatrix} \\begin{bmatrix} a_{11} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{n1} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}=\\sum \\limits _{i=1} ^{n}\\sum \\limits _{j=1} ^{n}a_{ij}x_{i}x_{j} $$\n例えば、$\\mathbb{R}^{2}$上の二次形式は以下の通り。\n$$ \\begin{align*} \u0026amp; a_{11}^{\\ }x_{1}^{2} + a_{22}^{\\ }x_{2}^{2} + a_{12}^{\\ }x_{1}^{\\ }x_{2}^{\\ } + a_{21}^{\\ }x_{2}^{\\ }x_{1}^{\\ } \\\\ =\u0026amp;\\ a_{11}^{\\ }x_{1}^{2} + a_{22}^{\\ }x_{2}^{2} + 2a_{12}^{\\ }x_{1}^{\\ }x_{2}^{\\ } \\end{align*} $$\n$\\mathbb{R}^{3}$上の二次形式は以下の通り。\n$$ a_{11}^{\\ }x_{1}^{2} + a_{22}^{\\ }x_{2}^{2} + a_{33}^{\\ }x_{3}^{2} + 2a_{12}^{\\ }x_{1}^{\\ }x_{2}^{\\ } + 2a_{13}^{\\ }x_{1}^{\\ }x_{3}^{\\ } + 2a_{23}^{\\ }x_{2}^{\\ }x_{3}^{\\ } $$\n繰り返しを避けるために、混合項を上記のように組み合わせて記述することが一般的である。二次形式は行列の内積の性質によって以下のように表すことができる。実数、複素数に対してそれぞれ以下のようだ。\n$$ \\begin{align*} Q_{A}(\\mathbf{x}) \u0026amp;= \\mathbf{x}^{T} A \\mathbf{x} = \\mathbf{x} \\cdot A\\mathbf{x} = A\\mathbf{x} \\cdot \\mathbf{x} = \\left\u0026lt; A\\mathbf{x}, \\mathbf{x}\\right\u0026gt; = \\left\u0026lt; \\mathbf{x}, A \\mathbf{x} \\right\u0026gt; \\\\ Q_{A}(\\mathbf{x}) \u0026amp;= \\mathbf{x}^{\\ast} A \\mathbf{x} = \\mathbf{x} \\cdot A\\mathbf{x} = A\\mathbf{x} \\cdot \\mathbf{x} = \\left\u0026lt; A\\mathbf{x}, \\mathbf{x}\\right\u0026gt; = \\left\u0026lt; \\mathbf{x}, A \\mathbf{x} \\right\u0026gt; \\end{align*} $$\n$A$が対角行列の場合、$a_{ij}=0 (i \\ne j)$が成り立つため、二次形式$Q_{A}(\\mathbf{x})$は混合項を持たない。\n$$ Q_{A}(\\mathbf{x}) = \\mathbf{x}^{T}A\\mathbf{x} =\\begin{bmatrix} x_{1} \u0026amp; \\cdots \u0026amp;x_{n} \\end{bmatrix} \\begin{bmatrix} a_{11} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; a_{nn} \\end{bmatrix}\\begin{bmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}=\\sum \\limits _{i=1}^{n} a_{ii}x_{i}^{2} $$\n関連項目 一次形式 二次形式 双線形形式 エルミート形式 ","id":3512,"permalink":"https://freshrimpsushi.github.io/jp/posts/3512/","tags":null,"title":"二次形式"},{"categories":"줄리아","contents":"コード Plots.jlは基本的にグリッド、目盛り、軸、カラーバーなどを全て出力するけど、これらをなくしてすっきりと描きたい場合は、次のオプションを追加すればいい。\ncolorbar=:none：カラーバーを消す。 showaxis = false：軸と目盛りを消す。 grid=false：背景のグリッドを消す。 ticks=false：背景のグリッドと目盛りを消す。 framestyle=:none：背景のグリッドと軸を消す。 using Plots\rsurface(L, title=\u0026#34;default\u0026#34;)\rsurface(L, title=\u0026#34;colorbar=:none\u0026#34;, colorbar=:none)\rsurface(L, title=\u0026#34;showaxis=false\u0026#34;, showaxis=false)\rsurface(L, title=\u0026#34;grid=false\u0026#34;, grid=false)\rsurface(L, title=\u0026#34;ticks=false\u0026#34;, ticks=false)\rsurface(L, title=\u0026#34;framestyle=:none\u0026#34;, framestyle=:none)\rsurface(L, title=\u0026#34;all off\u0026#34;, ticks=false, framestyle=:none, colorbar=:none) 環境 OS: Windows11 Version: Julia v1.8.3, Plots v1.38.6 ","id":3501,"permalink":"https://freshrimpsushi.github.io/jp/posts/3501/","tags":null,"title":"ジュリアで軸、目盛りなどをすべて無くしてきれいに出力する方法"},{"categories":"줄리아","contents":"概要 PythonやMATLABで使うmeshgrid()のような直接的な関数はない。グリッド上での関数値だけを求めたいなら、格子を作らないもっと簡単な方法がある。\nコード 2次元 列ベクトルと行ベクトルを掛けるのは、列ベクトルと行ベクトルのクロネッカー積を取るのと同じ結果を出す。\nU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\r# Fig. 1\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru1 = U.(T,X)\rheatmap(t\u0026#39;, x, u1, title=\u0026#34;Fig 1\u0026#34;) クロネッカー積を使えば、\nusing LinearAlgebra\rX = kron(x, ones(size(t)))\rT = kron(ones(size(x)), t)\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, title=\u0026#34;Fig 2\u0026#34;)\rjulia\u0026gt; u1 == u2\rtrue 3次元1 U(x,y,t) = exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\ry = LinRange(-1., 1, 100)\rt = LinRange(0.,0.35, 50)\rX = getindex.(Iterators.product(x, y, t), 1)\rY = getindex.(Iterators.product(x, y, t), 2)\rT = getindex.(Iterators.product(x, y, t), 3)\ru3 = U.(X,Y,T)\ranim = @animate for i ∈ 1:50\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1), title=\u0026#34;Anim. 1\u0026#34;)\rend 完全なコード using Plots\rcd = @__DIR__\rU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\r# Fig. 1\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru1 = U.(T,X)\rheatmap(t\u0026#39;, x, u1, title=\u0026#34;Fig 1\u0026#34;)\rsavefig(cd*\u0026#34;/fig1.png\u0026#34;)\r# kron\rusing LinearAlgebra\rX = kron(x, ones(size(t)))\rT = kron(ones(size(x)), t)\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, title=\u0026#34;Fig 2\u0026#34;)\rsavefig(cd*\u0026#34;/fig2.png\u0026#34;)\ru1 == u2\r# 3d\rU(x,y,t) = (1/4) * exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\rx = LinRange(-2., 2, 100)\ry = LinRange(-2., 2, 100)\rt = LinRange(0.,0.35, 50)\rX = getindex.(Iterators.product(x, y, t), 1)\rY = getindex.(Iterators.product(x, y, t), 2)\rT = getindex.(Iterators.product(x, y, t), 3)\ru3 = U.(X,Y,T)\ranim = @animate for i ∈ 1:50\rsurface(u3[:,:,i], zlims=(0,0.5), clim=(0,0.3), title=\u0026#34;Anim. 1\u0026#34;)\rend\rgif(anim, cd*\u0026#34;/anim1.gif\u0026#34;, fps=10) 環境 OS: Windows11 バージョン: Julia v1.8.3、Plots v1.38.6 https://discourse.julialang.org/t/meshgrid-function-in-julia/48679/26\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3500,"permalink":"https://freshrimpsushi.github.io/jp/posts/3500/","tags":null,"title":"ジュリアでメッシュグリッドを作成する方法"},{"categories":"줄리아","contents":"概要 Juliaで多変数関数をブロードキャストする方法を紹介する。Pythonなどで行うように、meshgridを作成する方法もあるし、各次元ごとにベクトルを作成して簡単に計算することもできる。\n2変数関数 $$ u(t,x) = \\sin(\\pi x) e^{-\\pi^{2}t} $$\n上のような関数を$(t,x) \\in [0, 0.35] \\times [-1,1]$でプロットしたい場合、次のように関数値を計算できる。\nx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\ru1 = @. sin(π*x)*exp(- π^2 * t)\rheatmap(t\u0026#39;, x, u1, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 1\u0026#34;) 関数自体を定義し、次のように2次元グリッドを作成して同じ結果を得ることができる。\nU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 2\u0026#34;) 3変数関数 $$ u(x,y,t) = e^{-x^{2} - 2y^{2}}e^{-\\pi^{2}t} $$\n時空間ドメイン$(x,y,t) \\in [-1,1] \\times [-1,1] \\times [0, 0.35]$で$u$の関数値を得たい場合、各変数の次元にのみサイズがあるようにベクトルを作成してブロードキャストすればいい。\n3次元メッシュを作成してブロードキャストしたい場合は、ここを参照。\njulia\u0026gt; x = reshape(LinRange(-1., 1, 100), (100,1,1))\r100×1×1 reshape(::LinRange{Float64, Int64}, 100, 1, 1) with eltype Float64:\rjulia\u0026gt; y = reshape(LinRange(-1., 1, 100), (1,100,1))\r1×100×1 reshape(::LinRange{Float64, Int64}, 1, 100, 1) with eltype Float64:\rjulia\u0026gt; t = reshape(LinRange(0.,0.35, 200), (1,1,200))\r1×1×200 reshape(::LinRange{Float64, Int64}, 1, 1, 200) with eltype Float64:\rjulia\u0026gt; u3 = @. exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\r100×100×200 Array{Float64, 3}:\ranim = @animate for i ∈ 1:200\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1))\rend コード詳細 using Plots\rcd = @__DIR__\r# Fig. 1\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\ru1 = @. sin(π*x)*exp(- π^2 * t)\rheatmap(t\u0026#39;, x, u1, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 1\u0026#34;)\rsavefig(cd*\u0026#34;/fig1.png\u0026#34;)\r# Fig. 2\rU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 2\u0026#34;)\rsavefig(cd*\u0026#34;/fig2.png\u0026#34;)\r# gif 1\rx = reshape(LinRange(-1., 1, 100), (100,1,1))\ry = reshape(LinRange(-1., 1, 100), (1,100,1))\rt = reshape(LinRange(0.,0.35, 200), (1,1,200))\ru3 = @. exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\ranim = @animate for i ∈ 1:200\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1), title=\u0026#34;Anim. 1\u0026#34;)\rend\rgif(anim, cd*\u0026#34;/anim1.gif\u0026#34;, fps=30) 環境 OS: Windows11 Version: Julia v1.8.3, Plots v1.38.6 ","id":3499,"permalink":"https://freshrimpsushi.github.io/jp/posts/3499/","tags":null,"title":"ジュリアにおける多変数関数のブロードキャス팅"},{"categories":"함수","contents":"定義 関数 $c : X \\to Y$ が全ての $x_{1} , x_{2} \\in X$ に対して次を満足するなら、定数関数Constant Functionと言う。 $$ c \\left( x_{1} \\right) = c \\left( x_{2} \\right) $$\n説明 普通、定数関数を関数として最初に「認識」する出発点は、定数関数の微分法を学ぶ時だ。 $$ \\lim_{h \\to 0} {{ c \\left( x + h \\right) - c \\left( x \\right) } \\over { h }} = 0 $$ それまでの教育課程では、関数や数が何であるか理解するのが難しく、優等生でない場合は、項を「文字」と「数字」に区別するようなナンセンスな視点で数式を見ることもあった（著者もそうだった）。しかし、両辺を微分することで、その文字でない―多項関数でない部分をどう扱うべきかを考え始める。その直後に、不定積分を扱いながら $$ \\int f(x) dx = F(x) + c $$ のように「ある定数 $c$」を記述し、定数という概念に慣れる。面白いことに、冗談でも、数学で重要と言えない「定数関数」がある分野で普遍的に現れるタイミングがある。\n連続性 $X, Y$ が位相空間なら、関数の連続性について論じることができる。定数関数は、どのような空間でも自明に連続関数であり、通常、$f : X \\to \\mathbb{Z}$ のような連続関数は「整数である関数値が連続的に変わることはできないので、$f$ は他ならぬ定数関数である」と言う形で登場する。\nモノドロミー定理の証明 ","id":2465,"permalink":"https://freshrimpsushi.github.io/jp/posts/2465/","tags":null,"title":"定数関数の定義"},{"categories":"푸리에해석","contents":"概要1 離散フーリエ変換DFTは、数式的な定義に従って計算すると、$\\mathcal{O}(N^{2})$の時間計算量を持ちますが、以下で説明するアルゴリズムに従って計算すると、時間計算量が$\\mathcal{O}(N\\log_{2}N)$に低減します。この高速フーリエ変換を使用して離散フーリエ変換を高速に実行することができます。これは高速フーリエ変換fast Fourier transform, FFTと呼ばれています。\n構築 2つの数字を掛けて、それを別の数字に加える操作を$1$回の演算operationと呼びましょう。すると、$\\sum\\limits_{i=0}^{n-1}a_{n}b_{n}$の値を求めるには$n$回の演算が必要です。\n$$ \\begin{align*} \\sum\\limits_{n=0}^{0} a_{n}b_{b} \u0026amp;= a_{0}b_{0} = \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\\\ \\sum\\limits_{n=0}^{1} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} = \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\\\ \\sum\\limits_{n=0}^{2} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} + a_{2}b_{2} = \\overbrace{\\bigg( \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\bigg) {\\color{#FE9A2E} + } a_{2} {\\color{#FE9A2E} \\times} b_{2}}^{\\color{#FE9A2E}3 \\text{ operations}} \\\\ \\end{align*} $$\nさて、離散フーリエ変換の定義を思い出してみましょう。\n線型変換 $\\mathcal{F}_{N} : \\mathbb{C}^{N} \\to \\mathbb{C}^{N}$を離散フーリエ変換と呼びます。\n$$ \\mathcal{F}_{N}(\\mathbf{a}) = \\hat{\\mathbf{a}} = \\begin{bmatrix} \\hat{a}_{0} \\\\ \\hat{a}_{1} \\\\ \\dots \\\\ \\hat{a}_{N-1} \\end{bmatrix} ,\\quad \\hat{a}_{m} = \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n}\\quad (0\\le m \u0026lt; N) \\tag{1} $$\nこのとき、$\\mathbf{a} = \\begin{bmatrix} a_{0}\u0026amp; a_{1}\u0026amp; \\dots\u0026amp; a_{N-1} \\end{bmatrix}^{T}$です。\n$\\hat{a}_{m}$を計算するには$N$回の演算が必要で、$\\hat{\\mathbf{a}}$を計算するにはこれを$N$回繰り返す必要があるため、離散フーリエ変換を計算するには合計で$N^{2}$回の演算が必要です。つまり、$\\mathcal{O}(N^{2})$の時間計算量を持っています。これは、コンピュータ計算の観点からフーリエ変換がかなりのコストを要することを意味します。\nアルゴリズム データの長さ$N$を合成数$N = N_{1}N_{2}$としましょう。そして、インデックス$m, n$を次のように定義します。\n$$ m = m^{\\prime}N_{1} + m^{\\prime \\prime},\\quad n = n^{\\prime}N_{2} + n^{\\prime \\prime} $$\nすると、$0 \\le m^{\\prime}, n^{\\prime \\prime} \\le N_{2}-1$および$0 \\le m^{\\prime \\prime}, n^{\\prime} \\le N_{1}-1$です。$(1)$の指数部分を次のように表現できます。\n$$ \\begin{align*} e^{-i2\\pi mn /N} \u0026amp;= e^{-i2\\pi (m^{\\prime}N_{1} + m^{\\prime \\prime})(n^{\\prime}N_{2} + n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi (m^{\\prime}n^{\\prime}N_{1}N_{2} + m^{\\prime}n^{\\prime \\prime}N_{1} + m^{\\prime \\prime}n^{\\prime}N_{2} + m^{\\prime \\prime}n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi m^{\\prime}n^{\\prime}} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \u0026amp;= e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \\end{align*} $$\nこれを$(1)$に代入すると、\n$$ \\begin{align*} \\hat{a}_{m} \u0026amp;= \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi m^{\\prime \\prime}n^{\\prime}/N_{1}}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\right] e^{-i2\\pi [ (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) ] } \\end{align*} $$\n上記の式に従うと、各括弧内の$\\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} \\right]$を計算するのに$N_{1}$回の演算、括弧の外の$\\sum_{n^{\\prime \\prime}=0}^{N_{2}-1}$を計算するのに$N_{2}$回の演算が必要です。したがって、$\\hat{a}_{m}$を計算するには合計で$(N_{1} + N_{2})$回の演算が必要です。$\\hat{\\mathbf{a}}$を得るにはこれを$N$回繰り返す必要があるため、合計で$N(N_{1} + N_{2})$のコストがかかり、$N^{2}$よりも減少することが確認できます。\n括弧内を注意深く見ると、$N_{1}$が再び合成数の場合、同じロジックを適用できることがわかるでしょう。したがって、データの長さが$N = N_{1} N_{2} \\cdots N_{k}$といった合成数の場合、時間計算量は次のように減少します。\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}\\big( N(N_{1} + N_{2} + \\cdots + N_{k}) \\big) $$\nここで、$N$を$2$のべき乗$N = 2^{k}$と仮定してみましょう。すると、$\\log_{2}N = k$であり、$N^{2} = 2^{k}$から$2^{k}(2k)$だけ減少するため、時間計算量は次のように減少します。\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}(2N \\log_{2}N) $$\n補足 これは1965年にCooleyとTukey2によって提案されたため、Cooley-Tukeyアルゴリズムとも呼ばれています。ただし、彼らが最初に発明したわけではありません。ガウスも同様のアルゴリズムを研究しましたが、正しく発表しなかったため、この事実は後に明らかになりました3。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p252-253\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. W. Cooley and J. W. Tukey, An algorithm for the machine calculation of complex Fourier series, Mathematics of Computation 19 (1965), 297-301.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. T. Heideman, D. H. Johnson, and C. S. Burms, Gauss and the history of the fast Fourier transform, Archive for the History of the Exact Sciences 34 (1985), 264-277.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3492,"permalink":"https://freshrimpsushi.github.io/jp/posts/3492/","tags":null,"title":"高速フーリエ変換アルゴリズム"},{"categories":"수리통계학","contents":"定義 1 ある推定量Estimator $T$ について、$T$ の標準偏差の推定値Estimateを標準誤差Standard Errorと言う。 $$ \\text{s.e.} \\left( T \\right) := \\sqrt{ \\widehat{ \\text{Var} \\left( T \\right) } } $$\n説明 定義で統計量ではなくて、正確に推定量と言われた理由がある。僕が当てたいパラメータ $\\theta$ と「合ってるかどうか」を議論する時じゃなければ、標準誤差は意味をなさないから、式で$\\theta$ が一度も登場しないにしても、わざわざ推定量について定義するんだ。だから$T$ の候補は明らかに標本平均 $\\overline{X}$ や回帰係数 $\\beta_{k}$ などで、その信頼区間が気になるから$\\text{s.e.} \\left( T \\right)$ が必要になるんだ。\n普通は、$\\overline{X} = \\sum_{k=1}^{n} X_{k}$ の標準誤差 $S / \\sqrt{n}$ をこのような定義から学んで、これだけが標準誤差だと思うことが多いけど、実際にはそれも定義ではなくて、計算を通じて得られる公式なんだ。できるだけ省略せずに計算してみよう。 $$ \\begin{align*} \\text{s.e.} \\left( \\overline{X} \\right) =\u0026amp; \\sqrt{ \\widehat{ \\text{Var} \\left( \\overline{X} \\right) } } \\\\ =\u0026amp; \\sqrt{ \\widehat{ \\text{Var} \\left( {{ 1 } \\over { n }} \\sum_{k=1}^{n} X_{k} \\right) } } \\\\ =\u0026amp; \\sqrt{ \\widehat{ {{ 1 } \\over { n^{2} }} \\text{Var} \\left( \\sum_{k=1}^{n} X_{k} \\right) } } \\\\ \\overset{\\text{iid}}{=} \u0026amp; \\sqrt{ \\widehat{ {{ 1 } \\over { n^{2} }} \\sum_{k=1}^{n} \\text{Var} \\left( X_{k} \\right) } } \\\\ =\u0026amp; \\sqrt{ {{ 1 } \\over { n^{2} }} \\sum_{k=1}^{n} \\widehat{ \\text{Var} \\left( X_{k} \\right) } } \\\\ =\u0026amp; \\sqrt{ {{ 1 } \\over { n^{2} }} \\sum_{k=1}^{n} S^{2} } \\\\ =\u0026amp; \\sqrt{ {{ 1 } \\over { n^{2} }} n S^{2} } \\\\ =\u0026amp; \\sqrt{ {{ 1 } \\over { n }} S^{2} } \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{n} }} S \\end{align*} $$ 見ての通り、推定量Estimatorと推定値Estimateが違っていて、こんな簡単な例でもかなり混乱する。そこに、実際に標準誤差を使う多くの場面で、標本分散を自由度で割ってルートを取る形を多用するから、その形を標準誤差そのものと間違えやすい。しかし、そのような直感がよく通じるとしても、標準誤差はそのような方法で決まるものではなく、上に示したような数式的な展開で導出するのが正しいんだ。\nHadi. (2006). Regression Analysis by Example(4th Edition): p33.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2462,"permalink":"https://freshrimpsushi.github.io/jp/posts/2462/","tags":null,"title":"標準誤差の一般的な定義"},{"categories":"줄리아","contents":"概要 これを実現するには、Dates モジュールの canonicalize() 関数を使用する1。\nコード using Dates\rtic = DateTime(2022,3,7,7,1,11)\rtoc = now()\rDates.canonicalize(toc-tic) 上のコードを実行した結果は次のとおりである。\njulia\u0026gt; using Dates\rjulia\u0026gt; tic = DateTime(2022,3,7,7,1,11)\r2022-03-07T07:01:11\rjulia\u0026gt; toc = now()\r2022-07-19T22:26:22.070\rjulia\u0026gt; Dates.canonicalize(toc-tic)\r19 weeks, 1 day, 15 hours, 25 minutes, 11 seconds, 70 milliseconds 小さい単位の倍数として正確に、週単位まで自動で計算して出力されることが確認できる。\n環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/get-difference-between-two-dates-in-seconds/11641/4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2461,"permalink":"https://freshrimpsushi.github.io/jp/posts/2461/","tags":null,"title":"ジュリアで2つの時刻の差を秒単位で計算する方法"},{"categories":"줄리아","contents":"要約 Plots.jlで軸と目盛りの色を指定する関連キーワードは以下の通りである。\nキーワード名 機能 guidefontcolor 軸名の色を指定 foreground_color_border, fgcolor_border 軸の色を指定 foreground_color_axis, fgcolor_axis 目盛りの色を指定 foreground_color_text, fgcolor_text 目盛りの値の色を指定 キーワード名の前にx_やy_を付けると、その軸にのみ適用される。\nコード1 軸名 軸名の色を指定するキーワードはguidefontcolorである。軸名は、xlabel, ylabelで指定できる。\nx = randn(10, 3)\rplot(plot(x, guidefontcolor = :red),\rplot(x, x_guidefontcolor = :red),\rplot(x, y_guidefontcolor = :red),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 軸 軸の色を指定するキーワードはforeground_color_borderである。\nplot(plot(x, foreground_color_border = :red),\rplot(x, x_foreground_color_border = :red),\rplot(x, y_foreground_color_border = :red),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 目盛り 目盛りの色を指定するキーワードはforeground_color_axisである。これをfalseにすると、目盛りだけを消すことができる。\nplot(plot(x, foreground_color_axis = :red),\rplot(x, x_foreground_color_axis = :red),\rplot(x, y_foreground_color_axis = false),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 目盛りの値 目盛りの値の色を指定するキーワードはforeground_color_textである。これをfalseにすると、目盛りの値だけを消すことができる。\nplot(plot(x, foreground_color_text = :red),\rplot(x, x_foreground_color_text = :red),\rplot(x, y_foreground_color_text = flase),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 環境 OS：Windows11 バージョン：Julia 1.9.4, Plots v1.39.0 関連項目 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理をするためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色を指定する方法 サブプロットごとにグラフの色を指定する方法 軸、軸名、目盛り、目盛りの値の色を指定する方法 背景色の指定方法 https://docs.juliaplots.org/stable/generated/attributes_axis/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3490,"permalink":"https://freshrimpsushi.github.io/jp/posts/3490/","tags":null,"title":"Julia Plotsで軸、軸名、目盛り、目盛り値の色を指定する方法"},{"categories":"머신러닝","contents":"概要 Flux、PyTorch、TensorFlowで同じ機能をするコードを整理します。\nJulia-MATLAB-Python-R チートシート Fluxについて次のような環境とします。\nusing Flux PyTorchについて次のような環境とします。\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F TensorFlowについて次のような環境とします。\nimport tensorflow as tf\rfrom tensorflow import keras 1次元テンソル 줄리아Julia\r파이토치PyTorch\r텐서플로우TensorFlow\r列ベクトルcolumn vector\r[1 4 -1 2] [1;4;-1;2] ","id":3489,"permalink":"https://freshrimpsushi.github.io/jp/posts/3489/","tags":null,"title":"Flux-PyTorch-TensorFlowチートシート"},{"categories":"줄리아","contents":"概要 Juliaで2次元配列と行列の間を切り替えるヒントを紹介する1。おそらくJulia 1.7以下の環境では、最もJuliaらしく、シンプルで、速く、美しい方法だろう。\nコード ここで紹介された方法だけではなく、行列と2次元配列の間を行き来する方法は数えきれないほどある。ただコードを書くだけでなく、目標そのものが難しくないから、Julia独特の構文がどのように使用されたのかも考えながら読む方がいい。\n行列から2次元配列へ julia\u0026gt; M = rand(0:9, 3, 10)\r3×10 Matrix{Int64}:\r2 4 0 1 8 0 9 2 5 7\r5 2 1 5 4 3 7 2 7 3\r7 8 1 9 0 3 2 4 1 3 上のような行列を2次元配列に変えてみよう。\njulia\u0026gt; [eachrow(M)...]\r3-element Vector{SubArray{Int64, 1, Matrix{Int64}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}}:\r[2, 4, 0, 1, 8, 0, 9, 2, 5, 7]\r[5, 2, 1, 5, 4, 3, 7, 2, 7, 3]\r[7, 8, 1, 9, 0, 3, 2, 4, 1, 3]\rjulia\u0026gt; [eachcol(M)...]\r10-element Vector{SubArray{Int64, 1, Matrix{Int64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}}:\r[2, 5, 7]\r[4, 2, 8]\r[0, 1, 1]\r[1, 5, 9]\r[8, 4, 0]\r[0, 3, 3]\r[9, 7, 2]\r[2, 2, 4]\r[5, 7, 1]\r[7, 3, 3] eachrow()とeachcol()は行列の行と列を一行ずつ抽出するジェネレータを返す2、そしてスプラットオペレータを通じてこれらを可変配列として扱い3、角括弧[]の中に入れることで、自然に配列になる。\n2次元配列から行列へ julia\u0026gt; A = [rand(0:9,3) for _ in 1:10]\r10-element Vector{Vector{Int64}}:\r[5, 4, 9]\r[9, 7, 6]\r[9, 9, 6]\r[5, 9, 0]\r[0, 2, 8]\r[3, 9, 5]\r[1, 6, 0]\r[5, 7, 7]\r[1, 3, 5]\r[5, 4, 1] 上のような2次元配列を行列にしてみよう。\njulia\u0026gt; hcat(A...)\r3×10 Matrix{Int64}:\r5 9 9 5 0 3 1 5 1 5\r4 7 9 9 2 9 6 7 3 4\r9 6 6 0 8 5 0 7 5 1\rjulia\u0026gt; hcat(A...)\u0026#39;\r10×3 adjoint(::Matrix{Int64}) with eltype Int64:\r5 4 9\r9 7 6\r9 9 6\r5 9 0\r0 2 8\r3 9 5\r1 6 0\r5 7 7\r1 3 5\r5 4 1\rjulia\u0026gt; vcat(A...)\r30-element Vector{Int64}:\r5\r4\r9\r9\r7\r6\r9\r9\r⋮\r7\r1\r3\r5\r5\r4\r1 配列を統合するhcat()関数を使えばいい。4 基本的にhcat()とvcat()はフォールド関数であり、可変引数関数であるため、2次元配列の要素である1次元配列を直接引数としてスプラットオペレータを通じて渡さなければならない。\n全体のコード # matrix to 2d array\rM = rand(0:9, 3, 10)\r[eachrow(M)...]\r[eachcol(M)...]\r# 2d array to matrix\rA = [rand(0:9,3) for _ in 1:10]\rhcat(A...)\rhcat(A...)\u0026#39;\rvcat(A...) 環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/plots-efficient-scatter-plot-of-a-2xn-array/31803/6\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/arrays/#Base.eachcol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/arrays/#Base.cat\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2459,"permalink":"https://freshrimpsushi.github.io/jp/posts/2459/","tags":null,"title":"ジュリアで2次元配列と行列の間の変換方法"},{"categories":"통계적분석","contents":"定義 1 $$ Y = \\beta_{0} + \\beta_{1} X_{1} + \\cdots + \\beta_{p} X_{p} + \\varepsilon $$ 多重回帰分析で、$p$個の独立変数$X_{1} , \\cdots , X_{p}$に対して上のような線形モデルを設定するとき、$\\beta_{0} , \\beta_{1} , \\cdots , \\beta_{p}$を回帰係数という。$Y$は従属変数を、$\\varepsilon$はランダムに分布したエラーを意味する。\n公式 $$ \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdots \u0026amp; x_{p1} \\\\ 1 \u0026amp; x_{12} \u0026amp; \\cdots \u0026amp; x_{p2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{1n} \u0026amp; \\cdots \u0026amp; x_{pn} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{bmatrix} $$ $n$個のデータが与えられ、$p \u0026lt; n$とするとき、線形多重回帰モデルを計画行列で表すと上のようになり、簡単に$Y = X \\beta + \\varepsilon$と表そう。$\\beta$に対する最小二乗の推定量ベクトル$\\hat{\\beta}$は以下の通りだ。 $$ \\hat{\\beta} = \\begin{bmatrix} \\hat{\\beta}_{0} \\\\ \\hat{\\beta}_{1} \\\\ \\vdots \\\\ \\hat{\\beta}_{p} \\end{bmatrix} = \\left( X^{T} X \\right)^{-1} X^{T} Y $$ それだけでなく、$\\hat{\\beta}$は$\\beta$の最良不偏推定量であるから、最良線形不偏推定量（Best Linear Unbiased Estimator, BLUE）とも呼ばれる。\n導出 2 3 私たちの目標は $$ \\left\\| \\varepsilon \\right\\|_{2}^{2} = \\sum_{k=0}^{n} \\varepsilon_{k} = \\begin{bmatrix} \\varepsilon_{0} \u0026amp; \\varepsilon_{1} \u0026amp; \\cdots \u0026amp; \\varepsilon_{n} \\end{bmatrix} \\begin{bmatrix} \\varepsilon_{0} \\\\ \\varepsilon_{1} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{bmatrix} = \\varepsilon^{T} \\varepsilon $$ これを最小化することだ。$\\varepsilon = Y - X \\beta$であるから、$\\varepsilon^{T} \\varepsilon = \\left( Y - X \\beta \\right)^{T} \\left( Y - X \\beta \\right)$を最小化する$\\beta$を見つければいい。両辺を$\\beta$で微分すると $$ \\begin{align*} {{ d } \\over { d \\beta }} \\varepsilon^{T} \\varepsilon =\u0026amp; - 2 X^{T} \\left( Y - X \\beta \\right) \\\\ = \u0026amp; - 2 X^{T} \\left( Y - X \\beta \\right) \\\\ = \u0026amp; - 2 X^{T} Y + 2 X^{T} X \\beta \\end{align*} $$ が$0$となるような$\\hat{\\beta}$は、以下の形になる。 $$ \\hat{\\beta} = \\argmin_{\\beta} \\varepsilon^{T} \\varepsilon = \\left( X^{T} X \\right)^{-1} X^{T} Y $$ 一方で、$\\hat{\\beta}$は$\\beta$に対する不偏推定量であることが容易に分かり、最小二乗法を通じて導出されたので、これより分散が小さい$\\beta$の不偏推定量は存在せず、最良不偏推定量である。\n■\n導出過程で$\\beta$に対する微分が気に入らない場合は、行列代数でのアプローチが代替案としてある。行列代数での最小二乗法では $$ X^{\\ast} Y = X^{\\ast} X \\hat{\\beta} $$ を満たす$\\hat{\\beta}$が最小二乗解となり、$X \\in \\mathbb{R}^{n \\times p}$であるから、$X^{\\ast} = X^{T}$であり、結果として$\\hat{\\beta} = \\left( X^{T} X \\right)^{-1} X^{T} Y$を得る。\n参照 単純回帰係数の推定量の導出 回帰係数ベクトルの多変量正規性 Hadi. (2006). 回帰分析の例(第4版): p53.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHadi. (2006). 回帰分析の例(第4版): p82~84.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.stat.purdue.edu/~boli/stat512/lectures/topic3.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2458,"permalink":"https://freshrimpsushi.github.io/jp/posts/2458/","tags":null,"title":"回帰係数の定義と推定量の公式導出"},{"categories":"줄리아","contents":"概要 SMTPClient.jlパッケージを使って、SMTPSimple Mail Transfer Protocolでナヴァーからメールを送る方法を紹介する1。長時間かかるシミュレーションが終わったらカカオメールにレポートを送るようにして、研究のスピードを上げるのに使っている。\nこのようにジョルディが個人トークで知らせてくれるから、自分でサーバーを確認しなくても、シミュレーションがいつ終わるか分かる。\nコード どの言語で実装しても、まず最初に以下のようにナヴァーメールでSMTPを「使用する」に設定する必要がある。\nジュリア using Dates\rtic = now()\rfor t in 1:1000\rprintln(t)\rend\rtoc = now()\rusing SMTPClient\ropt = SendOptions(\risSSL = true,\rusername = \u0026#34;네이버아이디\u0026#34;,\rpasswd = \u0026#34;비밀번호\u0026#34;)\r#Provide the message body as RFC5322 within an IO\rbody = IOBuffer(\r\u0026#34;Date: ▷eq1◁tic\\r\\n\u0026#34; *\r\u0026#34;▷eq2◁(Dates.canonicalize(toc - tic))\u0026#34; *\r\u0026#34;\\r\\n\u0026#34;)\rurl = \u0026#34;smtps://smtp.naver.com:465\u0026#34;\rrcpt = [\u0026#34;\u0026lt;수신자@kakao.com\u0026gt;\u0026#34;]\rfrom = \u0026#34;\u0026lt;발신자@naver.com\u0026gt;\u0026#34;\rresp = send(url, rcpt, from, body, opt) 上の例では、最も重要な部分はurl = \u0026quot;smtps://smtp.naver.com:465\u0026quot;だ。ナヴァーでなくても、どのサーバーを使うにしても、ここを適切に変える必要がある。送信時刻の場合はDatesモジュールのnow()を使って、メールを送る時点に固定したが、これが実際の時計と合わないと、10分ほど遅れて送られる問題を経験した。\nパイソン ジュリアを試す前に、まずはパイソンで試したコード。不思議なことに、SSLを使ってポートを456にしてもうまくいかなかったが、SSLを切って587にしたらうまくいった。参考にしたブログ2ではグーグルを基準に説明していたが、以下のコードはナヴァーを基準にうまく動作することを確認した。\nimport smtplib\rfrom email.mime.text import MIMEText\rsendEmail = \u0026#34;발신자@naver.com\u0026#34;\rrecvEmail = \u0026#34;수신자@kakao.com\u0026#34;\rpassword = \u0026#34;비밀번호\u0026#34;\rsmtpName = \u0026#34;smtp.naver.com\u0026#34; #smtp 서버 주소\rsmtpPort = 587 #smtp 포트 번호\rtext = \u0026#34;매일 내용\u0026#34;\rmsg = MIMEText(text) #MIMEText(text , _charset = \u0026#34;utf8\u0026#34;)\rmsg[\u0026#39;Subject\u0026#39;] = \u0026#34;시뮬레이션 종료\u0026#34;\rmsg[\u0026#39;From\u0026#39;] = sendEmail\rmsg[\u0026#39;To\u0026#39;] = recvEmail\rprint(msg.as_string())\rs=smtplib.SMTP( smtpName , smtpPort ) #메일 서버 연결\rs.starttls() #TLS 보안 처리\rs.login( sendEmail , password ) #로그인\rs.sendmail( sendEmail, recvEmail, msg.as_string() ) #메일 전송, 문자열로 변환하여 보냅니다.\rs.close() #smtp 서버 연결을 종료합니다. 環境 OS: Windows julia: v1.7.0 https://github.com/aviks/SMTPClient.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gosmcom.tistory.com/72\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2457,"permalink":"https://freshrimpsushi.github.io/jp/posts/2457/","tags":null,"title":"ジュリアでネイバーからメールを送る方法"},{"categories":"데이터과학","contents":"用語 ある統計量を計算する際、その値を変えることができる独立したデータの数を自由度Degree of Freedomと呼ぶ1。\n説明 自由度を説明するのが難しい理由 新入生になって統計学を勉強してみると、この「自由度」というのがなんなのか、本当に腹が立つ。まず難しいし、頻繁に出てくるのを置いておいて、どの教科書でもその定義をはっきりさせることができないからだ。このポストも具体的に自由度を定義していないし、ただ「用語」として紹介しているだけで、「計算する時」とか「値を変えることができる」など、厳密な数学的な表現とは言えない表現を使っている。\n問題は、それが理解できるということだ。みんなが面倒くさがっているわけではなく、実際の自由度という概念自体が勉強して「理解する」よりも、経験が積み重なるうちに「体得」する感じが強いからだ。2〜3年生くらいになると、自由度が何なのか大体の感じがつかめてきて、大学院に行く頃には普通に説明もそこそこできるが、定義を暗誦することはやはり難しい。\nまず自由度という表現自体が与える「良い感情」が問題だ。それがファッションであれ、オープンワールドゲームであれ、民主主義であれ、自由度は高いほど、大きいほど良いものと考えられている。さらに新入生が最初に接する自由度は、通常「サンプルの数が$n$だから、そこから$1$を引いた$(n-1)$だけの自由度を持つ」というように計算される。深く考えずに聞いてみれば、サンプルの数も少ないより多い方が良さそうだから、統計学の自由度さえも何か「良し悪しを持つ数値」という認識を持たれるかもしれない。しかし、正確に数式で扱い、探求する文脈で、自由度は単なる数値に過ぎない。\nまた、どんな文脈でもなく、あまりにも突然、さらには頻繁に登場するのも問題だ。分散分析ANOVAや回帰分析を学ぼうとすると、突然$n-1$だとか$n-p-1$だとか、「どう計算されたかの説明があまりにも不足している」自由度がわんさか出てくる。その上で数理統計学を学んでいると、今度は突然t-分布やカイ二乗分布などが自由度だと言い出す。更にF-分布には自由度が二つあるとか言われるが、その意味が正確には把握されずに、なんだか知っているような不思議な気持ちになれる。これが大体2〜3年生の時なんだけど、この頃になって自由度についてわざわざ質問するのも恥ずかしいし、全く知らないわけではないから、なんとなく乗り切ってしまうのが普通だ。\n実際にそれらの数字が必要だと理解することは置いておいても、「自由度」と呼ぶこと自体が一見無意味に見えるまである。それでは、なぜ自由度という言葉が必要かに共感してみよう。\n極端な例：自由度という概念がなかったら？ ある役に立たなそうな概念を説明する際の良い方法の一つは、その概念がなかった場合にどのような「反則」が許されるかを説明することだ。統計量がどうのこうのという数式的な説明は置いておいて、ただ面白い想像をしてみよう。次のようなサンプル$A$が与えられたとしよう。 $$ A = \\left\\{ 13, 7, 17, 3 \\right\\} $$ この場合、サンプルの数は$n = 4$だ。しかし、後輩が自分がサンプルを「発展」させたと言って持ってきたサンプル$B$を見てみよう。 $$ B = \\left\\{ 13, 7, 17, 3 , 14, 8, 18, 4 \\right\\} $$ 後輩はこのサンプルの数が$8$個で、$A$に比べてなんと二倍も多いと言った。ここで止まらず、「自分は好きなだけサンプルを増やせるし、$n \\to \\infty$レベルまで繰り返し可能だから、大きなサンプルで使える全ての統計手法を適用できる」と主張する。しかし、一目見てもこのサンプルは粗雑に偽造されたもので、その方法は単に既存のデータに$1$を足してサンプルの数を増やしただけだ。\nhttp://www.animatedsoftware.com/statglos/sgdegree.htm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2456,"permalink":"https://freshrimpsushi.github.io/jp/posts/2456/","tags":null,"title":"統計学における自由度"},{"categories":"보조정리","contents":"定義 $$ [a,b] := \\left\\{ x \\in \\mathbb{R} : a \\le x \\le b \\right\\} \\subset \\mathbb{R} $$\n二つの実数$a \\le b$に対して、上のような集合を区間Intervalと言う。 特に、両端$a,b$を含む場合、ブラケット[]を使って$\\left[ a,b \\right]$と書き、クローズドClosedされたと言う。 両端$a,b$を含まない場合、カッコ()を使って$\\left( a,b \\right)$と書き、オープンOpenされたと言う。 2つの端のうち一方のみを含まない場合、クロープンClopenと言い、$a$のみを含む場合は$[a,b)$と書き、$b$のみを含む場合は$(a,b]$と書く。 端が一方しかない場合、つまり無限大の場合は次のような表現を使う。 $$ \\begin{align*} (-\\infty, b) \u0026amp;:= \\left\\{ x \\in \\mathbb{R} : x \\lt b \\right\\} \\subset \\mathbb{R} \\\\ (a, \\infty) \u0026amp;:= \\left\\{ x \\in \\mathbb{R} : a \\lt x \\right\\} \\subset \\mathbb{R} \\end{align*} $$ 説明 区間は、1次元ユークリッド空間$\\mathbb{R}^{1}$で連結性を持つ部分集合として、我々が最もよく知る集合の一つで、理解しやすく、親しみやすく、どんな勉強をしても長く見ることになる。\n数値解析 数値解析のような分野では、$a,b$のようにちょうど二つだけではなく、順序なしに混ざった多くの点を扱う時がある。だから、多くの点の集合$S := \\left\\{ x_{1} , \\cdots , x_{n} \\right\\}$を含む最小の区間を次のように示すことがある。 $$ \\mathscr{H} \\left\\{ x_{1} , \\cdots , x_{n} \\right\\} := \\left[ \\min S , \\max S \\right] $$ 区間を使った演算に興味を持つ分野では、区間算術Interval Arithmeticという分野があると言われている1。\nプログラミング言語 コードを書いていてふと気になって探した面白い記事を見たことがある。2 要するに、プログラミングではなぜ$(0,n]$や$[1,n]$ではなくクロープンインターバル$[0,n)$を使うのかということだが、数学者の観点から同意する内容だけを簡単にまとめて書いてみたいと思った。\nn = 10\rfor i in 0:n\rprint(n) PythonやMATLABなど多くのプログラミング言語では、類似したコードがよく使われていて、大抵の場合、そのコードの実行結果は次のようになる。 (ちなみにこのコードはPythonでもMATLABでもない、仮想の言語だ。)\n0123456789 これが何を意味するかというと、0:nだけ制御するとしたら、それをクロープンインターバル$[0,n)$と考えるということだ。このような考え方、慣習の利点は、ほとんどの言語でインデックスを$0$から使い$i = 0, 1, \\cdots , n-1$まで回すと、その\u0026rsquo;繰り返し回数自体\u0026rsquo;が見事に$n$になるということだ。このような直感的な表記を使うと、誤りが大幅に減り、実用的な習慣になる。\nそして、C言語などでは同じ表現をfor(i=0; i\u0026lt;10; i++)のように書かなければならないが、1から始めて正確に10で終わるfor(i=1; i\u0026lt;=10; i++)と比較すると、オペレータ自体が\u0026lt;から\u0026lt;=へとより汚くなり、実際Cで使う配列にアクセスする時は0も含まれなければならないので、ちょうどこのような繰り返しはバカバカしくfor(i=0; i\u0026lt;=(10-1); i++)のように書かなければならないかもしれない。\nつまり、コーディングを学ぶ人を混乱させたり、1つずれることは、みんなをいじめるためではなく、なんとなくそれなりの理由があるかもしれないということだ。\nhttps://en.wikipedia.org/wiki/Interval_arithmetic\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://nanite.tistory.com/56\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2453,"permalink":"https://freshrimpsushi.github.io/jp/posts/2453/","tags":null,"title":"数学における区間の定義"},{"categories":"데이터과학","contents":"定義 1 量的データが与えられたとしよう。\n全体から$p \\%$より大きく、$(100-p) \\%$より小さい値を**$p$-パーセンタイル**$p$番目のパーセンタイルと言う。 $100$-パーセンタイルと$0$-パーセンタイル（データで最も大きな値と小さな値）をそれぞれ最大値, 最小値と言う。 最大値と最小値の差をデータの範囲範囲$R$と言う。 $25$-パーセンタイルを第1四分位数$Q_{1}$と言い、$75$-パーセンタイルを第3四分位数$Q_{3}$と言う。 $\\left( Q_{3} - Q_{1} \\right)$を四分位範囲四分位範囲$\\text{IQR}$と言う。 最小値、第1四分位数、中央値、第3四分位数、最大値の5つの統計量を五数要約Fiveと言う。 $$ \\min \\qquad Q_{1} \\qquad \\text{median} \\qquad Q_{3} \\qquad \\max $$ 経験上、以下の範囲を超えたデータを外れ値Outlierとも呼ぶ。 $$ \\left[ Q_{1} - 1.5 \\text{IQR} , Q_{3} + 1.5 \\text{IQR} \\right] $$ この区間の下限をロワーフェンス下限、上限をアッパーフェンス上限と言う。 説明 第2四分位数 $50$-パーセンタイル、つまり第2四分位数は中央値そのものであるため、五数要約を語る上で別途定義する必要はない。これらの要約は十分なデータがある時に、その数字だけでデータの分布を大まかに推測できるように助けてくれ、どんなデータを見ても最初に確認すべきものだ。\n外れ値 外れ値Outlierは文字通り外OutにあるものLierという意味で、一般的なデータの範囲から外れていたためにそう呼ばれる。$Q_{1} - 1.5 \\text{IQR}$はかなり小さい値で、$Q_{3} + 1.5 \\text{IQR}$はかなり大きな値だが、それらが期待される範囲から外れているため外れ値と呼ばれる。これは「経験的」や「一般的なデータ」という表現を使用しているため、数学的に厳密な定義ではないことに注意しよう。\nMendenhall. (2012). 確率統計入門 (13版): p76, 60, 78~80.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2452,"permalink":"https://freshrimpsushi.github.io/jp/posts/2452/","tags":null,"title":"パーセンタイルと外れ値"},{"categories":"보조정리","contents":"定義 幾何学的な定義 平面で与えられた一点から距離$r \u0026gt; 0$だけ離れた点の集合を円Circleと定義する。 円の周$l$と直径$2r$の比を円周率$\\pi$と定義する。 $$ \\pi := {{ l } \\over { 2r }} $$ 解析学的定義 1 $$ E (z) := \\sum_{k=0}^{\\infty} {{ z^{k} } \\over { k! }} $$ 複素関数$E : \\mathbb{C} \\to \\mathbb{C}$を上記のように指数関数の級数展開として定義し、それによりコサイン関数に類似した次の関数$C$を定義しよう。 $$ C(x) := {{ E (ix) + E(-ix) } \\over { 2 }} $$ $C(x)$の根、すなわち$C(x) = 0$を満たす解の中で最も小さい正数を$x_{0}$とするとき、その倍数を円周率$\\pi$と定義する。 $$ \\pi := 2 x_{0} $$\n説明 このポストでは、幾何学的な（簡単な）定義と解析学的な（難しい）定義を紹介したが、大学3年生以上の数学の学生なら、解析学的な定義を見て微笑むことができるだろう。\n人類の歴史において、円周率は非常に重要な定数として、遅くとも車輪が発明された時点で、その具体的な値が実用的に使用されるようになった。特に、効率的で精密な近似値としては $$ {{ 22 } \\over { 7 }} = 3.142857 \\cdots \\approx \\pi $$ といった数値も知られていた。この値は、いわゆるゆとり教育時代ゆとり世代の20世紀末の日本の教育水準をはるかに超えるほど正確であった。（教育をゆったりと行うという名目で円周率を$3$と教えていた時期だった） 2\n関連項目 円周率が無理数であることの証明 Walter Rudin, Principles of Mathmatical Analysis (3rd Edition, 1976): p178~183.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.joongang.co.kr/article/2572535\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2451,"permalink":"https://freshrimpsushi.github.io/jp/posts/2451/","tags":null,"title":"円周率の定義"},{"categories":"위상데이터분석","contents":"概要 ZomorodianとCarlssonの論文「Computing Persistent Homology」で紹介されたアルゴリズムの擬似コードを説明し、実装する。1 抽象的な単体複体で作られたフィルター付き複体を受け取り、$\\mathcal{P}$-インターバルをリターンし、コンピュータで扱いにくい永続的なモジュールの構築を省略し、行列のリダクションによって永続的なホモロジーを計算する。さらに、実際の実装では行列演算さえも使用しない。\n導出 Zomorodianのアルゴリズムの導出: アルゴリズムの理論的な内容を完全に無視すると、擬似コードをいくら見てもアルゴリズムを理解することはできないだろう。完全に理解する程度でなくても、なぜ突然行列がなくなったのか、なぜマーキングのようなものが必要なのか、少なくともその程度は理解できるように勉強してから実装に取り組むべきである。 アルゴリズム アルゴリズムが実行される前にフィルター付き複体をデータとして受け取ったとする。\nデータを保存し、アルゴリズムから得られた情報を記録するためのディクショナリやテーブル$T$を上記のように作成する。例えばJuliaのデータフレームでは、データ内の数字をepsilon、アルファベットが記された部分をsimplexに転写し、マーキングの有無を保存するブーリアンカラムmarked、チェインコンプレックスのチェインを保存するslot、計算過程で出てくる整数を保存するカラムJを追加する。\nJuliaは配列が$0$ではなく$1$から始まる言語であり、実装の便宜上インデックスを気にしない部分があるため、上のスクリーンショットのように数字が$1~2$ずつすべて異なる場合があるが、これは全く重要ではないので気にしないでほしい。 注意すべきは、slotが集合でありながらもチェインとして両方の表現を自由に行き来することである。例えば、$\\mathbb{Z}_{2}$上での計算で $$ a + (a - b) = 2 a - b = b \\pmod{2} $$ のような計算が行われるが、これは次のような集合演算 $$ \\left\\{ a \\right\\} \\cup \\left\\{ a, -b \\right\\} = \\left\\{ b \\right\\} $$ とも同じことである。代数的な演算での要素$0$は集合では「ないもの」として扱われ、それをそのまま受け入れなければならない。厳密で慎重な表記を好む人には不快かもしれないが、そこまで無理なことではないので、そのまま受け入れよう。 また、元の論文では一般的なフィールド$F$上でアルゴリズムを導出しており、すなわちすべての$q \\in F$の逆元$q^{-1} \\in F$が存在して $$ d = d - q^{-1} T[i] $$ のような計算を行うが、この実装ではバイナリフィールド$\\mathbb{Z}_{2}$で十分なので、$q^{-1}$を別途計算せずに$A \\Delta B := \\left( A \\cup B \\right) \\setminus \\left( A \\cap B \\right)$と定義された$\\Delta$に対して次のように代替した。 $$ d = d \\Delta T[i] $$ $\\deg$という表現が（プログラムの）関数としても出てくるし、インデックスとしても出てくるし、多項関数の次数としても出てくるし、非常に頻繁に出てくるので、$T$ではdegではなくepsilonと表記した。実際、トポロジカルデータアナリシスの単純なレベルでは、通常このカラムの値である半径$\\varepsilon \u0026gt; 0$が大きくなるにつれてフィルター付き複体を構成することになるためである。 $T$はシンプレックスの次元に従って完全にソートされていると仮定し、それに従ってepsilonも部分順序を持つことを期待する。 擬似コード $\\left\\{ L_{k} \\right\\}$ = COMPUTEINTERVALS$(K)$\nInput: フィルター付き複体$K$を受け取る。フィルター付き複体には、少なくともどのタイミング$\\deg \\in \\mathbb{Z}$にどのシンプレックス$\\sigma$が追加されたかについての情報が必要である。 Output: $k = 0, \\cdots , \\dim K$に対する$\\mathcal{P}$-インターバルの集合$L_{k}$の集合$\\left\\{ L_{k} \\right\\}_{k=0}^{\\dim K}$を得る。 Side Effect: データが記録されたテーブル$T$のmarkedを変更する。 $d$ = REMOVEPIVOTROWS$(\\sigma)$\nInput: $k$次元のシンプレックス$\\sigma$を受け取る。 Output: $(k-1)$次元のチェイン、つまり$k$次元のシンプレックス同士を演算したある$\\mathsf{C}_{k-1}$の要素を得る。 $i$ = maxindex$d$\nInput: チェイン$d$を受け取る。 Output: テーブル$T$でチェイン$d$に含まれるすべてのsimplexの中で最大のインデックス$i$を返す。例えばmaxindex(abc)の場合はabの$5$、bcの$6$、acの$9$の中で最大の$9$を返す必要がある。 $k$ = dim$d$\nInput: $k$次元のチェイン$d$を受け取る。 Output: 整数$k$を返す。 $k$ = dim$\\sigma$\nInput: $k$次元のシンプレックス$\\sigma$を受け取る。 Output: 整数$k$を返す。 $k$ = deg$(\\sigma)$\nInput: シンプレックス$\\sigma$を受け取る。 Output: テーブル$T$でシンプレックス$\\sigma$に対応する整数epsilonを返す。例えばdeg(cd)の場合はcdのepsilonが$2$なので$2$を返す必要がある。 キーワード\nMarkはMark$\\sigma$の形で書かれ、該当するシンプレックス$\\sigma$のmarkedをtrueに変更する。 StoreはStore$j$ and $d$ in $T[i]$の形で書かれ、$T[i]$のJに整数$j$、slotにチェイン$d$を保存する。 RemoveはRemove$x$ in $d$の形で書かれ、チェイン$d$にある$x$項を削除する。 $\\sigma^{i}$はテーブル$T$で$i$番目にあるsimplexであり、$m$は$T$の長さである。\nfunction COMPUTEINTERVALS$(K)$\n# 初期化\nfor $k \\in 0:\\dim K$\n$L_{k} := \\emptyset$\nend for\nfor $j \\in 0:(m-1)$\n$d$ = REMOVEPIVOTROWS$\\left( \\sigma^{j} \\right)$\nif $d = \\emptyset$\n# $d$が空であることは、（ピボットではない）ゼロ列の候補である\nmark $\\sigma^{j}$\nelse\n# $d$の次元を計算する必要があるため、すべての項のmaxでなければならない\n# $d$は$\\sigma^{j}$より一次元低いチェインであり、$i \u0026lt; j$しかあり得ない\n$i$ = maxindex$d$\n$k$ = dim$d$\n$L_{k}$ = $L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\deg \\sigma^{j} \\right) \\right\\}$\nend if\nend for\nfor $j \\in 0:(m-1)$\n# まだマークされていない場合は、明らかにゼロ列である\nif $\\sigma^{j}$ ismarked and $T[j]$ isempty\n$k$ = dim$d$\n# $H_{k-1}$から$\\sum^{\\hat{e}_{i}} F[t]$に該当、無限大処理\n$L_{k}$ = $L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\infty \\right) \\right\\}$\nend if\nend for\nreturn $\\left\\{ L_{k} \\right\\}$\nend function\nfunction REMOVEPIVOTROWS$(\\sigma)$\n$k$ = $\\dim \\sigma$\n# $\\partial abc = ab - bc + ca$とすると∂(\u0026quot;abc\u0026quot;) = [\u0026quot;ab\u0026quot;, \u0026quot;bc\u0026quot;, \u0026quot;ca\u0026quot;]\n$d$ = $\\partial_{k} \\sigma$\nRemove not marked $(k-1)$-dimensional simplex in $d$\nwhile $d \\ne \\emptyset$\n$i$ = maxindex$d$\nif $T[i]$ isempty\nbreak\nend if\n# $\\mathbb{Z}_{2}$なので、symdiff（対称差）で代用\n$d$ = $d \\Delta T[i]$\nend while\nreturn $d$\nend function\n実装 与えられた例でのアルゴリズムの実行結果は次のようになるべきである。 $$ \\begin{align*} L_{0} =\u0026amp; \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} \\\\ L_{1} =\u0026amp; \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} \\end{align*} $$\nJuliaで実装した結果は次のようになる。インデックスが正確に異なる部分を除けば、正しく実装されていることが確認できる。\n全体のコード 読めばわかるが、元の論文のノーテーションをほぼそのままにしてコードを書いた。例えば $$ L_{k} = L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\deg \\sigma^{j} \\right) \\right\\} $$ のJulia風のコードはpush!(L_[k], (deg(σⁱ), deg(σʲ)))であるが、論文とほぼ同じように見えるようにL_[k] = L_[k] ∪ [(deg(σⁱ), deg(σʲ))]として実装した。\nusing DataFrames\rdata = DataFrame([\r0 \u0026#34;a\u0026#34;\r0 \u0026#34;b\u0026#34;\r1 \u0026#34;c\u0026#34;\r1 \u0026#34;d\u0026#34;\r1 \u0026#34;ab\u0026#34;\r1 \u0026#34;bc\u0026#34;\r2 \u0026#34;cd\u0026#34;\r2 \u0026#34;ad\u0026#34;\r3 \u0026#34;ac\u0026#34;\r4 \u0026#34;abc\u0026#34;\r5 \u0026#34;acd\u0026#34;\r], [\u0026#34;epsilon\u0026#34;, \u0026#34;simplex\u0026#34;])\rT = copy(data)\rT[!, :\u0026#34;marked\u0026#34;] .= false\rT[!, :\u0026#34;slot\u0026#34;] .= [[]]\rT[!, :\u0026#34;J\u0026#34;] .= 0\rdimK = 2\rm = nrow(T)\rtest_σ = \u0026#34;abc\u0026#34;\rdim(σ) = length(σ)\rfunction deg(σ)\rreturn T.epsilon[findfirst(T.simplex .== σ)]\rend\rdeg(test_σ)\rfunction ∂(σ)\rk = dim(σ)\rreturn [σ[(1:k)[Not(t)]] for t = 1:k]\rend\r∂(test_σ)\rfunction maxindex(chain)\rreturn (T.simplex .∈ Ref(chain)) |\u0026gt; findall |\u0026gt; maximum\rend\rmaxindex(∂(test_σ))\rfunction REMOVEPIVOTROWS(σ)\rk = dim(σ); d = ∂(σ)\rd = d[d .∈ Ref(T[T.marked,:simplex])] # Remove unmarked terms in ▷eq029◁\rwhile !(d |\u0026gt; isempty)\ri = maxindex(d)\rif T[i,:slot] |\u0026gt; isempty break end\rd = symdiff(d, T[i,:slot])\r# print(\u0026#34;d in empty\u0026#34;)\rend\rreturn d\rend\rREMOVEPIVOTROWS(test_σ)\rL_ = [[] for k = 0:dimK]\rfor j0 = 0:(m-1)\rj = j0+1\rσʲ = T[j,:simplex]\rd = REMOVEPIVOTROWS(σʲ)\rif d |\u0026gt; isempty\rT[j,:marked] = true\relse\ri = maxindex(d); k = dim(σʲ)\rσⁱ = T[i,:simplex]\rT[i,[:J,:slot]] = j0,d\rL_[k] = L_[k] ∪ [(deg(σⁱ), deg(σʲ))]\rend\rend\rfor j0 = 0:(m-1)\rj = j0+1\rσʲ = T[j,:simplex]\rif (T[j,:marked]) \u0026amp;\u0026amp; (T[j,:slot] |\u0026gt; isempty) \u0026amp;\u0026amp; (T[j,:J] |\u0026gt; iszero)\rk = dim(σʲ); L_[k] = L_[k] ∪ [(deg(σʲ), Inf)]\rprint(\u0026#34;j: $j\u0026#34;)\rend\rend Zomorodian. (2005). Computing Persistent Homology: ch4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2449,"permalink":"https://freshrimpsushi.github.io/jp/posts/2449/","tags":null,"title":"ジョモロジアンのアルゴリズムの実装"},{"categories":"데이터과학","contents":"定義 1 $n$個の量的データが与えられたとしよう。\n標本平均$\\overline{x}$とデータの差$\\left( \\overline{x} - x_{i} \\right)$を偏差Deviationという。 偏差の二乗の和を$n-1$で割った値$s^{2}$を標本分散Variance of a Sampleと呼ぶ。 $$ s^{2} := {{ \\sum \\left( x_{i} - \\overline{x} \\right)^{2} } \\over { n-1 }} $$ 標本分散の平方根$s = \\sqrt{s^{2}}$を標準偏差Standard Deviationと言う。 説明 分散度はデータがどれだけ広がっているかを示す量で、可変性Variabilityや拡散Dispersionとも呼ばれる。分散はその分散度の尺度Measure of Variabilityとして、平均に次いで重要な統計量である。\n参照 統計学を初めて学ぶと、なぜわざわざ二乗をして、$n$ではなく$n-1$で割るのかなど、面倒に感じる点が多い。統計学を専攻し、学年が上がるにつれて（決して簡単ではない）数理的理論を学んでそれらの質問に答えることができるようになる。新入生であれば、とりあえずそのまま受け入れてもいい。\n標本分散を$n-1$で割る理由 代表値の数理的性質：平均は分散を最小化する性質を持っている。 統計学における分散の数理的定義 Mendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p60~63。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2448,"permalink":"https://freshrimpsushi.github.io/jp/posts/2448/","tags":null,"title":"基礎統計学における分散の定義"},{"categories":"위상데이터분석","contents":"概要 ZomorodianとCarlssonの論文「Computing Persistent Homology」で紹介されたアルゴリズムの導出プロセスを説明する1。抽象的なシンプレクシャルコンプレックスで作られたフィルタードコンプレックスを受け取り、$\\mathcal{P}$-インターバルを返す。計算機で扱いにくいパーシステントモジュールの構築を省略し、行列のリダクションにより持続的ホモロジーを計算する。\n導出 Part 0. 事前調査\nアルゴリズムの本格的な導出に先立ち、上の図で描写されるパーシステンスコンプレックスが数式的にどのような形式であるかをまず検討する。このプロセスをしっかりと固めておかないと、論文を読むのが非常に苦痛になるだろう。\nまず、下部にある数字を$\\deg$とし、これが$0$から$5$まで増加しながら次のようにフィルタードコンプレックスを形成する。 $$ \\left\\{ a,b \\right\\} = K_{0} \\subset K_{1} \\subset K_{2} \\subset K_{3} \\subset K_{4} \\subset \\left( K_{4} \\cup \\left\\{ acd \\right\\} \\right) = K_{5} $$ $\\deg$とは関係なく、$K$は$2$-シンプレックスとして、ホモロジーを考慮する文脈で次のようなチェインコンプレックスを形成する。 $$ \\mathsf{C}_{2} \\overset{\\partial_{2}}{\\longrightarrow} \\mathsf{C}_{1} \\overset{\\partial_{1}}{\\longrightarrow} \\mathsf{C}_{0} $$ アルゴリズムの目標は、このような$\\partial_{2}$と$\\partial_{1}$がデータに与える代数的トポロジカル情報、たとえばベッチ数$\\beta_{k}$などが、どの$\\deg$で現れていつ$\\deg$で消えるかを次のように計算することである。 $$ \\begin{align*} L_{0} =\u0026amp; \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} \\\\ L_{1} =\u0026amp; \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} \\end{align*} $$\n$L_{0}$は$\\beta_{0}$に該当する情報、つまりコンポーネントがいつ現れて消えるかを示す$\\mathcal{P}$-インターバルで構成され、$L_{1}$は$\\beta_{1}$に該当する情報、つまり空間で「穴」と呼べるものがいつ現れて消える$\\mathcal{P}$-インターバルで構成されている。\nPart 1. $\\partial_{1}$\n論文では、著者たちはその計算が全てのフィールドで可能であると主張しているが、簡単にグレード付きモジュールである$\\mathbb{Z}_{2} [t]$-モジュールでどのような計算が行われるかを見てみよう。これから$\\mathsf{C}_{k}$の同次基底を$\\left\\{ e_{j} \\right\\}$、$\\mathsf{C}_{k-1}$の同次基底を$\\left\\{ \\hat{e}_{i} \\right\\}$と表記することにする。ここで同次とは、$\\mathsf{C}_{k}$をグレード付きモジュールと見たとき、項が一つしかないという意味で受け取ってもよく、つまり$t^{2} + t$のようではなく$t^{4}$のような単項形式であると考えても構わないということである。\n$$ \\deg M_{k} (i,j) = \\deg e_{j} - \\deg \\hat{e}_{i} $$ ホモロジー代数にある程度慣れていれば、今度は$\\partial_{k}$に対応する境界行列$M_{k}$を上のテーブルと方程式に合わせて構成し、そのスミス標準形$\\tilde{M}_{1}$を求めに行く感じがするだろう。まず$k=1$の場合を考えてみると、先に行列の基底が同次であると言ったので、次のように唯一の$M_{1}$を得ることができる。\nこのように基底を持って行列を構成することは、$\\partial_{k}$の一つの役割が$t^{n}$を掛けること（群作用を取ることでグレード付きモジュールで次数が上がること）の逆を行うことだと見ると理解できる。感覚を掴むために、直接計算してみよう。 $$ \\begin{align*} \\deg M_{1} (2,5) =\u0026amp; \\deg ac - \\deg c = 3 - 1 = 2 = \\deg t^{2} \\\\ \\deg M_{1} (4,5) =\u0026amp; \\deg ac - \\deg a = 3 - 0 = 3 = \\deg t^{3} \\\\ \\deg M_{1} (2,2) =\u0026amp; \\deg bc - \\deg c = 1 - 1 = 0 = \\deg t^{0} = \\deg 1 \\end{align*} $$\n先に言ったように、今度はこのエシュロン形、特にカラムエシュロン形を作\nると次のようになる。\n大学で学んだ線形代数を思い出してみると、各カラムで一番上にありながら$0$でない、図のように四角で囲んだ部分のようなものをピボットと呼んでいた。ここで次の2つの補助定理を紹介する。\n(1): カラムエシュロン形の対角成分はスミス標準形の対角成分と同じである。 (2): $\\tilde{M}_{k}$の$i$行のピボットが$\\tilde{M}_{k} (i,j) = t^{n}$であればホモロジーグループ$H_{k-1}$の$\\sum^{\\deg \\hat{e}_{i}} F[t] / t^{n}$に該当し、それ以外は$H_{k-1}$の$\\sum^{\\deg \\hat{e}_{i}} F[t]$に該当する。これは$L_{k-1}$が$\\left( \\deg \\hat{e}_{i} , \\deg \\hat{e}_{i} + n \\right)$と$\\left( \\deg \\hat{e}_{i} , \\infty \\right)$で構成されることと同値である。 つまり、\n補助定理(1)により、持続的ホモロジーを計算する際は行操作が必要なく、列操作だけで良いことになる。 補助定理(2)により、$L_{k-1}$は$\\left( \\deg \\hat{e}_{i} , \\deg \\hat{e}_{i} + n \\right)$と$\\left( \\deg \\hat{e}_{i} , \\infty \\right)$で構成される。 最初の行のピボットが$t^{1}$であり、$\\deg d = 1$であるため、$(1,1+1)$を得る。 2番目の行のピボットが$t^{0}$であり、$\\deg c = 1$であるため、$(1,1+0)$を得る。 3番目の行のピボットが$t^{1}$であり、$\\deg b = 0$であるため、$(0,0+1)$を得る。 4番目の行にピボットがなく、$\\deg a = 0$であるため、$(0,\\infty)$を得る。 これは、アルゴリズムを導出する前に言及した$L_{0}$と完全に一致する。 $$ L_{0} = \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} $$\nPart 2. $\\partial_{2}$\n$L_{1}$を得るための$\\partial_{2}$の行列形$M_{2}$は上のようである。しかし、次の補助定理で計算を減らし、簡単に進めることができる。\n(3): $\\mathsf{C}_{k+1}$の標準基底と$\\mathsf{Z}_{k}$に対する$\\partial_{k+1}$を表現するためには、$\\tilde{M}_{k}$に対応する行を$M_{k+1}$から単に削除しても良い。 言葉は少し難しく聞こえるが、現在の具体的な状況では、$\\tilde{M}_{1}$の$1$-シンプレックス$ab,bc,cd,ad,ac$の中で、$cd,bc,ab$のピボットだけが残っているので、これを単に$M_{2}$から削除しても良いということである。直感的に考えると、これらがすでに$k$次元で使用されたので、$k+1$では見る必要もないという程度に受け取っても構わない。こうしてカラムエシュロン形$\\tilde{M}_{2}$を直接構築する過程を省略し、その3つの行を削除してみると、次のように下が切り取られた$\\check{M}_{2}$を得る。\n$$ \\begin{align*} z_{2} =\u0026amp; ac - bc - ab \\\\ z_{1} =\u0026amp; ad - bc - cd - ab \\end{align*} $$\n再び補助定理(2)に従って計算してみよう。\n最初の行のピボットが$t^{1}$であり、 $$ \\deg z_{2} = \\deg \\left( ac - bc - ab \\right) = \\max \\deg \\left\\{ ac , bc , ab \\right\\} = 3 $$ であるため、$(3,3+1)$を得る。 2番目の行のピボットが$t^{3}$であり、 $$ \\deg z_{1} = \\deg \\left( ad - bc - cd - ab \\right) = \\max \\deg \\left\\{ ad , bc , cd , ab \\right\\} = 2 $$ であるため、$(2,2+3)$を得る。 これは、アルゴリズムを導出する前に言及した$L_{1}$と完全に一致する。 $$ L_{1} = \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} $$\nこのようなプロセスをコンプレックス$K$の次元$\\dim K$まで繰り返すと、求めていたアルゴリズムを得ることができる。行列の左右の大きさは$\\partial_{k}$に従い、その成分は$\\deg$に従って充填されると考えると少し混乱しなくなるだろう。\n■\n一方、補助定理(1)で列操作だけで十分だということは、これまでの導出で見たように行列表現に固執する理由がないということでもある。また、補助定理(3)により、「過去にすでに計算が終わった」部分に対して大胆に行を捨てるような効率的なプロシージャが含まれており、これにはピボットでないカラムを「マーキング」する能力などが必要である。結果として、実際のアルゴリズムの擬似コードPseudo Codeは、行列をそのまま使用するのではなく、もう少し高度なデータ型、ディクショナリーやデータフレームなどで説明されることになる。これは実際に体験すると非常に戸惑いやすく難しい。\n実装 Zomorodianのアルゴリズム実装: 科学界で働くなら誰でも読みやすいJulia言語を通じて、論文の擬似コードをほぼ文学的に翻訳した実装を紹介する。 Zomorodian. (2005). Computing Persistent Homology: ch4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2447,"permalink":"https://freshrimpsushi.github.io/jp/posts/2447/","tags":null,"title":"ジョモロジアンのアルゴリズム誘導"},{"categories":"확률론","contents":"概要 測度論と確率論を学んだ人向けの定義と概念の要約資料です。迅速な復習と定義の参照のために作成されました。\n測度論 代数 $X \\ne \\varnothing$の部分集合たちのコレクション $\\mathcal{A}$が 有限 合集合と補集合に対して閉じている時、これを代数と言います。\n可算合集合に対して閉じている代数を$\\sigma$-代数と言います。\nNote:\n定義により $\\mathcal{A}$はまた交集合に対しても閉じています $\\big( \\because E_{1} \\cap E_{2} = \\left( E_{1} \\cup E_{2} \\right)^{c} \\in \\mathcal{A}$ for $E_{1}, E_{2} \\in \\mathcal{A} \\big)$ $\\mathcal{A}$は空集合 $\\varnothing$と全集合 $X$を含みます。 $\\big( \\because E \\in \\mathcal{A}$ $\\implies$ $\\varnothing = E \\cap E^{c} \\in \\mathcal{A} \\text{ and } X = E \\cup E^{c} \\in \\mathcal{A} \\big)$ $X$が位相空間なら、$X$の開集合たちのコレクションから作られる$\\sigma$-代数を$X$上のボレル $\\sigma$-代数と言い、$\\mathcal{B}_{X}$と表記します。\nボレル $\\sigma$-代数は全ての開集合を含む最も小さい唯一の$\\sigma$-代数です。 $\\mathcal{E}$を$X$上の$\\sigma$-代数としましょう。順序対 $(X, \\mathcal{E})$を可測空間と言い、$E \\in \\mathcal{E}$を可測集合と言います。\n特に言及がない限り、以下では固定された可測空間 $(X, \\mathcal{E})$について扱います。\n可測関数 全ての実数 $\\alpha \\in \\mathbb{R}$に対して、次を満たす関数 $f : X \\to \\mathbb{R}$を($\\mathcal{E}$-)可測と言います。 $$ \\left\\{ x \\in X : f(x) \\gt \\alpha \\right\\} \\in \\mathcal{E}\\qquad \\forall \\alpha \\in \\mathbb{R}. $$\n一般化 $(X, \\mathcal{E})$、$(Y, \\mathcal{F})$を可測空間とします。関数 $f : X \\to Y$が次を満たす時、これを$(\\mathcal{E}, \\mathcal{F})$-可測と言います。 $$ f^{-1}(F) = \\left\\{ x \\in X : f(x) \\in F \\right\\} \\in \\mathcal{E}\\qquad \\forall F \\in \\mathcal{F}. $$\nNote: $\\mathcal{E}$-可測関数は上の定義で$(Y, \\mathcal{F}) = (\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$の場合と同じです。\n測度 $\\mathcal{E}$ (または $(X, \\mathcal{E})$、$X$)上の測度とは、次を満たす関数 $\\mu : \\mathcal{E} \\to [0, \\infty]$です。\nNull empty set: $\\mu (\\varnothing) = 0$。 Countable additivity: $\\left\\{ E_{j} \\right\\}$が$\\mathcal{E}$の互いに素な集合たちなら、$\\displaystyle \\mu \\left( \\bigcup\\limits_{j} E_{j} \\right) = \\sum\\limits_{j} \\mu (E_{j})$。 三つ組 $(X, \\mathcal{E}, \\mu)$を測度空間と言います。特に言及がない限り以下では固定された測度空間 $(X, \\mathcal{E}, \\mu)$について扱います。\nボレル測度とは、定義域がボレル $\\sigma$-代数$\\mathcal{B}_{\\mathbb{R}}$の測度を言います： $$ \\mu : \\mathcal{B}_{\\mathbb{R}} \\to [0, \\infty] $$\n$(X, \\mathcal{E})$、$(Y, \\mathcal{F})$上の二つの測度 $\\mu$、$\\nu$に対して、次を満たす$\\mathcal{E} \\times \\mathcal{F}$上の唯一の測度 $\\mu \\times \\nu$を$\\mu$と$\\nu$の積測度と言います。 $$ \\mu \\times \\nu (E \\times F) = \\mu (E) \\nu (F)\\qquad \\text{ for all rectangles } E \\times F. $$\n積分 実関数 $f$が有限な関数値を持つ時、これを単純と言います。\n単純可測関数 $\\varphi$は次のような形で表されます。 $$ \\begin{equation} \\varphi = \\sum\\limits_{j=1}^{n} a_{j}\\chi_{E_{j}}, \\text{ where } E_{j} = \\varphi^{-1}(\\left\\{ a_{j} \\right\\}) \\text{ and } \\operatorname{range} (\\varphi) = \\left\\{ a_{1}, \\dots, a_{n} \\right\\}. \\end{equation} $$ ここで $\\chi_{E_{j}}$は$E_{j}$の特性関数です。これを$\\varphi$のstandard representationと言います。\n$\\varphi$がstandard representation $(1)$を持つ単純可測関数の時、測度 $\\mu$に対する**$\\varphi$の積分**を次のように定義します。 $$ \\int \\varphi d\\mu := \\sum\\limits_{j=1}^{n} a_{j}\\mu (E_{j}). $$ Notation: $$ \\int \\varphi d\\mu = \\int \\varphi = \\int \\varphi(x) d\\mu (x), \\qquad \\int = \\int_{X}. $$\n$f$が$(X, \\mathcal{E})$上の可測関数の時、$\\mu$に対する**$f$の積分**を次のように定義します。 $$ \\int f d\\mu := \\sup \\left\\{ \\int \\varphi d\\mu : 0 \\le \\varphi \\le f, \\varphi \\text{ is simple and measurable} \\right\\}. $$\n$f : X \\to \\mathbb{R}$の正の部分と負の部分をそれぞれ次のように定義します。 $$ f^{+}(x) := \\max \\left( f(x), 0 \\right)),\\qquad f^{-1}(x) := \\min \\left(-f(x), 0 \\right)). $$ もし二つの積分$\\displaystyle \\int f^{+}$、$\\displaystyle \\int f^{-}$が有限なら、$f$が積分可能と言います。また$\\left| f \\right| = f^{+} - f^{-}$が成立します。\n積分可能な実関数たちの集合はベクトル空間であり、積分はこのベクトル空間上の線形汎関数です。このベクトル空間を次のように表記します。 $$ L = L(X, \\mathcal{E}, \\mu) = L(X, \\mu) = L(X) = L(\\mu), \\qquad L = L^{1} $$\n$L^{p}$空間\n測度空間$(X, \\mathcal{E}, \\mu)$と$0 \\lt p \\lt \\infty$に対して、$L^{p}$を次のように定義します。 $$ L^{p}(X, \\mathcal{E}, \\mu) := \\left\\{ f : X \\to \\mathbb{R} \\left| f \\text{ is measurable and } \\left( \\int \\left| f \\right|^{p} d\\mu \\right)^{1/p} \\lt \\infty \\right. \\right\\}. $$\n確率論 表記法と用語 $$ \\begin{array}{lll} \\text{Analysts\u0026rsquo; Term} \u0026amp;\u0026amp; \\text{Probabilists\u0026rsquo; Term} \\\\ \\hline \\text{Measure space } (X, \\mathcal{E}, \\mu) \\text{ such that } \\mu (X) = 1 \u0026amp;\u0026amp; \\text{Probability space } (\\Omega, \\mathcal{F}, P) \\\\ \\text{Measure } \\mu : \\mathcal{E} \\to \\mathbb{R} \\text{ such that } \\mu (X) = 1 \u0026amp;\u0026amp; \\text{Probability } P : \\mathcal{F} \\to \\mathbb{R} \\\\ (\\sigma\\text{-)algebra $\\mathcal{E}$ on $X$} \u0026amp;\u0026amp; (\\sigma\\text{-)field $\\mathcal{F}$ on $\\Omega$} \\\\ \\text{Mesurable set } E \\in \\mathcal{E} \u0026amp;\u0026amp; \\text{Event } E \\in \\mathcal{F} \\\\ \\text{Measurable real-valued function } f : X \\to \\mathbb{R} \u0026amp;\u0026amp; \\text{Random variable } X : \\Omega \\to \\mathbb{R} \\\\ \\text{Integral of } f, {\\displaystyle \\int f d\\mu} \u0026amp;\u0026amp; \\text{Expextation of } f, E(X) \\\\ f \\text{ is } L^{p} \u0026amp;\u0026amp; X \\text{ has finite $p$th moment} \\\\ \\text{Almost everywhere, a.e.} \u0026amp;\u0026amp; \\text{Almost surely, a.s.} \\end{array} $$\n$$ \\begin{align*} \\left\\{ X \\gt a \\right\\} \u0026amp;:= \\left\\{ w : X(w) \\gt a \\right\\} \\\\ P\\left( X \\gt a \\right) \u0026amp;:= P\\left( \\left\\{ w : X(w) \\gt a \\right\\} \\right) \\end{align*} $$\n基礎定義 可測空間$(\\Omega, \\mathcal{F})$、$(\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$に対して、$(\\mathcal{F}, \\mathcal{B}_{\\mathbb{R}})$-可測関数$X : \\Omega \\to \\mathbb{R}$を確率変数と言います。つまり、 $$ X^{-1}(B) \\in \\mathcal{F}\\qquad \\forall B \\in \\mathcal{B}_{\\mathbb{R}}. $$\n$(\\Omega, \\mathcal{F})$上の確率(または確率測度)とは、$P(\\Omega) = 1$を満たす測度$P : \\mathcal{F} \\to \\mathbb{R}$です。\n$X$を確率変数とする時、\n期待値: $\\displaystyle E(X) := \\int X dP$ 分散: $\\sigma^{2}(X) := E\\left[ (X - E(X))^{2} \\right] = E(X^{2}) - E(X)^{2}$ $X$の(確率)分布とは、次を満たす$\\mathbb{R}$上の確率$P_{X} : \\mathcal{B}_{\\mathbb{R}} \\to \\mathbb{R}$です： $$ P_{X}(B) := P(X^{-1}(B)). $$\n$X$の分布関数$F_{X}$は次のように定義されます： $$ F_{X}(a) := P_{X}\\left( (-\\infty, a] \\right) = P(X \\le a). $$\n確率変数の数列$\\left\\{ X_{i} \\right\\}_{i=1}^{n}$に対して、確率ベクトル$(X_{1}, \\dots, X_{n})$は次のように定義される関数を言います： $$ (X_{1}, \\dots, X_{n}) : \\Omega \\to \\mathbb{R}^{n} $$ $$ (X_{1}, \\dots, X_{n})(x) := (X_{1}(x), \\dots, X_{n}(x)). $$\nNote: $(X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n})= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})$。\n$n=2$の場合を先に見ましょう。$(X, Y) : \\Omega \\to \\mathbb{R}^{2}$に対して次が成立します。 $$ (X, Y)^{-1} (a, b) = \\left\\{ x \\in \\Omega : X(x) = a \\right\\} \\cap \\left\\{ x \\in \\Omega : Y(x) = b \\right\\}. $$ 従って、全てのボレル集合$B_{1}$、$B_{2} \\in \\mathcal{B}_{\\mathbb{R}}$に対して次を得ます。 $$ (X, Y)^{-1}(B_{1} \\times B_{2}) = (X, Y)^{-1}(B_{1}, B_{2}) = X^{-1}(B_{1}) \\cap Y^{-1}(B_{2}). $$ これを任意の$\\mathbb{R}^{n}$に対して拡張すると、 $$ \\begin{equation} \\begin{aligned} (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \u0026amp;= (X_{1}, \\dots, X_{n})^{-1}(B_{1}, \\dots, B_{n}) \\\\ \u0026amp;= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n}). \\end{aligned} \\end{equation} $$\n$X_{1}, \\dots, X_{n}$の結合分布とは確率ベクトル$(X_{1}, \\dots, X_{n})$の確率分布で定義されます： $$ P_{(X_{1}, \\dots, X_{n})} : \\mathcal{B}_{\\mathbb{R}^{n}} \\to \\mathbb{R}, $$ $$ P_{(X_{1}, \\dots, X_{n})}(B_{1} \\times \\cdots \\times B_{n}) := P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right). $$\n独立 $P(E) \\gt 0$の事象$E$に対して、$\\Omega$上の確率 $$ P_{E}(F) = P(E|F) := P(E \\cap F)/P(E) $$ を$E$上の条件付き確率と言います。\nもし$P_{E}(F) = P(F)$なら、$F$を$E$と独立と言います： $$ \\text{$F$ is independent of $E$} \\iff P(E \\cap F) = P(E)P(F). $$ 次が成立する時、$\\Omega$の事象たちのコレクション$\\left\\{ E_{j} \\right\\}$が独立と言います： $$ P(E_{1} \\cap \\cdots \\cap E_{n}) = P(E_{1}) P(E_{2}) \\cdots P(E_{n}) = \\prod \\limits_{i=1}^{n} P(E_{j}). $$\n$\\Omega$上の確率変数たちのコレクション$\\left\\{ X_{j} \\right\\}$が独立ということは、全てのボレル集合$B_{j} \\in \\mathcal{B}_{\\mathbb{R}}$に対して事象たち$\\left\\{ X_{j}^{-1}(B_{j}) \\right\\}$が独立ということを言います。つまり次の式が成立することを意味します： $$ P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) = \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})). $$\n確率分布の定義と$(2)$により、上記式の左辺から次を得ます。 $$ \\begin{align*} P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) \u0026amp;= P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right) \\\\ \u0026amp;= P_{(X_{1}, \\dots, X_{n})} \\left( B_{1} \\times \\cdots \\times B_{n} \\right). \\end{align*} $$ 一方、積測度と確率分布の定義により、右辺から次を得ます。 $$ \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})) = \\prod \\limits_{j=1}^{n} P_{X_{j}}(B_{j}) = \\left( \\prod \\limits_{j=1}^{n} P_{X_{j}} \\right) \\left( B_{1} \\times \\cdots \\times B_{n} \\right). $$ 従って$\\left\\{ X_{j} \\right\\}$が独立なら、 $$ P_{(X_{1}, \\dots, X_{n})} = \\prod\\limits_{j=1}^{n}P_{X_{j}}. $$\n$\\left\\{ X_{j} \\right\\}$が独立な確率変数の集合であることは、$\\left\\{ X_{j} \\right\\}$の結合分布がそれぞれの分布の積と同じであることと同値です。\n参考文献 Robert G. Bartle, The Elements of Integration and Lebesgue Measure (1995) Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (1999) ","id":3473,"permalink":"https://freshrimpsushi.github.io/jp/posts/3473/","tags":null,"title":"측도론과 확률론 요약 정리"},{"categories":"통계적검정","contents":"定義 1 2 科学で母集団についてのある推測を統計的仮説と言い、仮説を採用または棄却する統計的意思決定プロセスを統計的仮説検定Testing of Statistical Hypothesisという。この過程には2つの競合する仮説があり、主に研究者が支持したい仮説を対立仮説Alternative Hypothesis $H_{1}$ と言い、それに反して対立仮説が真である確かな根拠がない場合に受け入れる仮説を帰無仮説Null Hypothesis $H_{0}$ と言う。仮説検定のための統計量を検定統計量Test Statisticと言う。\n説明 もしお前が統計学を専攻するなら、帰無仮説 $H_{0}$ vs 対立仮説 $H_{1}$ … という永遠に繰り返される戦いに直面するだろう。最初はすごく難しそうに見えるけど、知っていくうちに愛憎交じりあう感情が出てくるから、怖がらずに理解してみよう。\n教科書に出てくる難しい数学の話（棄却域、検定統計量、有意水準等）は一時的に置いておいて、日常的な状況で仮説検定がどのように生じ得るかを想像してみよう。例えば、架空の製薬会社Aが肝機能に役立つ、具体的には一つの肝数値であるASTを下げる新薬aを発売する場面を考えてみよう：\naが市販化されるためには、食品医薬品局のような機関のあるテストをパスしなければならず、その場合は当然aがASTを下げることを証明しなければならない。もちろんその方法は「実際にASTが下がった人を10人以上連れてくる」ような手探りではなく、統計的に意味があるべきだ。 100人の中で10人以上、または「全体の臨床試験参加者の10%以上」が超えるのはどうだろうか？先ほどよりは合理的だが、単純に11〜40が正常なAST数値を500から490程度に下げたとしても、効果があったと見なすかどうかは問題があるかもしれない。 一つの方法はaを継続的に摂取した $1$ グループと摂取していない（プラシーボ）$2$ グループに分けて、それぞれの肝数値の平均を比較することだ。$1$ グループの平均を $\\mu_{1}$ 、$2$ グループの平均を $\\mu_{2}$ とすると、製薬会社Aが望む結果はおそらく次のようになるだろう。 $$ \\mu_{1} \u0026lt; \\mu_{2} $$ 上で紹介された定義に従って、対立仮説は次のように定められる。 $$ H_{1}: \\mu_{1} \u0026lt; \\mu_{2} $$ 式だけを見てもまだ500と490のレベルを比較する問題はあるように見えるが、今は一人や二人の個人ではなく、標本集団という統計について話している。例えば同じく500 vs 490でも分散が200だとしたら、それはたまたまの偶然かもしれない。しかし、分散が2程度に小さいなら、新薬aは明らかにASTを下げたと見える。[ 注：二つの集団の平均を比較するためにその分散を使うアイデアはかなり使えそうだ。それを発展させたのがまさに分散分析ANOVAである。 ] しかしとりあえず仮説検定に戻ってみよう。対立仮説がこのように定められたら、帰無仮説は次のような反対の内容になるかもしれない。 $$ H_{0}: \\mu_{1} \\ge \\mu_{2} $$ ここで重要なのは、帰無仮説が受け入れられる条件が「対立仮説が真である確かな根拠がない」ということであり、それが帰無仮説自体が積極的に採用されるわけではないということ。帰無仮説が受け入れられるのは、それを棄却できないためであり、それが真実であると証明されたためではない。 例えば、探検家コロンブスの対立仮説が「アメリカ大陸は存在する」とした場合、コロンブスが最初の探検でアメリカを見つけられなかったからといって「アメリカ大陸は存在しない」という帰無仮説が真になるわけではない。まだ確かな根拠がないので一旦は「アメリカ大陸は存在しない」と受け入れるだけであり、証拠の欠如が欠如の証拠ではない。 幸いにも対立仮説 $H_{1}$ が統計的な根拠によって真であるとしよう。ただし、厳密に言えば、この分析を通じて明らかにされたのは、新薬aを飲んだ集団のASTが低下したということだけに注意が必要だ。臨床医や病理学者ほどのドメインDomain, 分野に対する専門性を持たない分析者が自信を持って言えるのは「どんな理由があれ、新薬の効果は確実に証明された」ということであり、新薬aがどのような原理でASTを下げたかという因果関係までを言及する根拠にはならない。 参照 仮説検定の分かりやすい定義：厳密さよりもむしろ手軽に受け入れやすい定義を紹介する。 帰無仮説と対立仮説を定める方法：その定義にどんな問題があるかを説明する。 仮説検定の難しい定義：比較的厳密な数理統計的定義を紹介する。 慶北大学校統計学科。 (2008)。エクセルを使った統計学: p199。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMendenhall. (2012)。確率と統計の導入 (第13版): p344。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2442,"permalink":"https://freshrimpsushi.github.io/jp/posts/2442/","tags":null,"title":"仮説検定の簡単な定義"},{"categories":"데이터과학","contents":"定義 1 母集団に関連する数値的に記述される尺度数値的記述尺度を母数パラメータといい、サンプルから算出されたものを統計量スタティスティックという。\n説明 統計学の定義については様々な考え方があるが、基本的には推論統計学、特に数理統計学での「母数とは何か」に関心を持つ学問とされている。この観点から、統計学はデータを通じて母集団の母数を知る方法を研究する学問だと言える。\n一方、面白いことに、統計量Statisticにsを一つ加えるとStatistics、つまり統計学そのものになる。これは、統計学が統計量に関する研究であり、特にそれが母数に関するものである時は推定量エスティメータと言われることを意味している。例えば、ほとんどの場合、サンプルを全て加算し、その数で割る統計量である標本平均 $\\overline{x}$ は母平均 $\\mu$ を知るための推定量である。\n関連項目を見る 数理統計学での統計量と推定量 ハイパーパラメータ メンデンホール. (2012).「確率と統計の紹介」 (13版): p50.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2440,"permalink":"https://freshrimpsushi.github.io/jp/posts/2440/","tags":null,"title":"基礎統計学における母数と統計量"},{"categories":"데이터과학","contents":"定義 1 $$ \\overline{x} := {{ 1 } \\over { n }} \\sum_{k=1}^{n} x_{k} $$\n$n$量的データが与えられた時、その値を全て足し$n$で割った値$\\overline{x}$を標本平均Sample Mean、算術平均Arithmetic Mean、アベレージAverageなどと呼ぶ。\n説明 一般に、平均がデータをどれだけよく要約できるか、効果的かは、わざわざ説明する必要はない。大学レベル以上の統計学を勉強している人なら、以下の疑問を理解し、平均に注意すべき時を知っていなければならない：\n平均はいつも信頼できるか？ 明らかにそうではない。インターネット上に「ノースカロライナ大学で平均年収が最も高い学部は地理学部だ」という内容の有名なジョークがある。2そのような面白いエピソードからも分かるように、平均は異常値に弱く、代表値として適切ではない場合がある。 特に危険な時は？ 標本が極端に少ない時、異常値が多い時、分布が単峰性でない時などがある。ほとんどないが、理論的に母平均が存在しないと仮定する状況も考えられる。 それでも最も重要とされているのはなぜか？ 中心極限定理のためだ。どのような分布からのランダムサンプルでも、標本平均の確率分布は正規分布に分布収束するという強力な定理で、単純だが統計学の基礎をなす統計量としてその価値がある。 統計学を勉強している人は、平均がいつその意味を失うかを理解し、データを慎重に確認する習慣をつけなければならない。言い換えれば、平均を正しく使うだけでなく、使わないタイミングを知ることも非常に重要だ。以下のツイッターは、データを無視した時に平均がどれだけ意味をなさなくなるか、少し誇張を交えて警告している：\n参照 統計学の三つの代表値：最頻値、中央値、平均 平均 中央値 最頻値 代表値の数学的性質: 平均は分散を最小化する性質を持っている。 Mendenhall. (2012). 確率と統計の導入 (13版): p54.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; ","id":2438,"permalink":"https://freshrimpsushi.github.io/jp/posts/2438/","tags":null,"title":"基礎統計学における平均の定義"},{"categories":"줄리아","contents":"正規分布\njulia\u0026gt; using Distributions\rjulia\u0026gt; d = Normal()\rNormal{Float64}(μ=0.0, σ=1.0)\rjulia\u0026gt; rand(d, 2,2)\r2×2 Matrix{Float64}:\r-0.618228 -0.729552\r-1.46898 -0.636276 一様分布\njulia\u0026gt; rand(Uniform(), 2,2)\r2×2 Matrix{Float64}:\r0.0952175 0.348995\r0.845515 0.768308\rjulia\u0026gt; rand(Uniform(1,10), 2,2)\r2×2 Matrix{Float64}:\r7.09885 1.65445\r6.14428 7.31004 コーシー分布\njulia\u0026gt; rand(Cauchy(), 2,2)\r2×2 Matrix{Float64}:\r-20.1142 0.118282\r-0.110452 -0.420331\rjulia\u0026gt; rand(Cauchy(), 2,2)\r2×2 Matrix{Float64}:\r2.96951 -0.0587456\r0.0388744 -0.422848 ","id":3463,"permalink":"https://freshrimpsushi.github.io/jp/posts/3463/","tags":null,"title":"ジュリアで与えられた分布からランダムにサンプリングする方法"},{"categories":"줄리아","contents":"説明1 Juliaでランダム抽出する関数は以下の通りです。\nrand([rng=default_rng()], [S], [dims...]) rngはRandom Number Generatorの略で、乱数抽出アルゴリズムを指定します。何を意味しているのかわからなければ、触らなくても大丈夫です。\nSは（おそらく）Setの略で、ランダム抽出をする集合を指定する変数です。Sに入力可能な変数は以下のものがあります。\nインデックスがあるオブジェクト AbstractDictまたはAbstractSet 文字列 タイプ（整数、浮動小数点のみ可能です。有理数、無理数は不可。） 抽出集合をタイプで指定した場合、整数型ならtypemin(S):type(S)の範囲から抽出します（BigIntは対応していません）。\njulia\u0026gt; typemin(Int16), typemax(Int16)\r(-32768, 32767)\rjulia\u0026gt; typemin(Int32), typemax(Int32)\r(-2147483648, 2147483647)\rjulia\u0026gt; typemin(Int64), typemax(Int64)\r(-9223372036854775808, 9223372036854775807) 浮動小数点なら$[0, 1)$の範囲から抽出します。\njulia\u0026gt; rand(Float64)\r0.4949745522302659\rjulia\u0026gt; rand(ComplexF64)\r0.8560168003603014 + 0.16478582700545064im [dims...]は抽出する配列の次元を表します。rand(S, m, n)ならば、集合Sの要素から（重複を含めて）$m \\times n$個を抽出して$m \\times n$形の配列を返します。次元を入力しなければ、実数が返されます。実数と1次元ベクトルが明確に区別されるので、注意してください。さらに、$2\\times 3$形の配列を得たいと思って次元を(2,3)のようにタプルで入力すると、Sの変数として受け取られるので、全く異なる結果が出るので注意してください。\njulia\u0026gt; rand(Float64) # 실수 추출\r0.42226201756172266\rjulia\u0026gt; rand(Float64, 1) # 성분이 실수인 1x1 배열로 추출\r1-element Vector{Float64}:\r0.7361136057571305\rjulia\u0026gt; rand(2,3) # 성분이 실수인 2x3 배열로 추출 2×3 Matrix{Float64}:\r0.648742 0.364548 0.0550352\r0.0350098 0.56055 0.83297\rjulia\u0026gt; rand((2,3)) # 2와 3중에서 추출\r3 より高度な内容は次を参考にしてください。\nランダムシードを固定する方法 重みを付けてランダム抽出する方法 分布を与えてランダム抽出する方法 コード インデックスがあるオブジェクト julia\u0026gt; rand((2,5))\r5\rjulia\u0026gt; rand(2:5)\r3\rjulia\u0026gt; rand([2,3,4,5])\r4\rjulia\u0026gt; rand([\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, 4])\r\u0026#34;x\u0026#34; ディクショナリ 抽出集合をディクショナリにすると、キー-値ペアPair自体が抽出されます。\njulia\u0026gt; d = Dict(2=\u0026gt;4, 3=\u0026gt;5, 4=\u0026gt;\u0026#34;6\u0026#34;)\rDict{Int64, Any} with 3 entries:\r4 =\u0026gt; \u0026#34;6\u0026#34;\r2 =\u0026gt; 4\r3 =\u0026gt; 5\rjulia\u0026gt; rand(d)\r4 =\u0026gt; \u0026#34;6\u0026#34;\rjulia\u0026gt; rand(d)\r2 =\u0026gt; 4 文字列 抽出集合を文字列にすると、文字列内の文字の中からランダムに一つが抽出されます。\njulia\u0026gt; str = \u0026#34;freshrimpsushi\u0026#34;\r\u0026#34;freshrimpsushi\u0026#34;\rjulia\u0026gt; rand(str)\r\u0026#39;e\u0026#39;: ASCII/Unicode U+0065 (category Ll: Letter, lowercase)\rjulia\u0026gt; rand(str)\r\u0026#39;h\u0026#39;: ASCII/Unicode U+0068 (category Ll: Letter, lowercase) タイプ julia\u0026gt; rand(Int32, 3)\r3-element Vector{Int32}:\r1552806175\r-384901411\r-1580189675\rjulia\u0026gt; rand(UInt32, 3)\r3-element Vector{UInt32}:\r0xd2f44f99\r0x166a8b9e\r0x92fe22dc\rjulia\u0026gt; rand(Float32, 3)\r3-element Vector{Float32}:\r0.59852564\r0.6247238\r0.23303497\rjulia\u0026gt; rand(ComplexF32, 3)\r3-element Vector{ComplexF32}:\r0.10872495f0 + 0.6622572f0im\r0.6408408f0 + 0.46815878f0im\r0.7766515f0 + 0.73314756f0im 環境 OS: Windows11 Version: Julia 1.9.0 https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3462,"permalink":"https://freshrimpsushi.github.io/jp/posts/3462/","tags":null,"title":"ジュリアでランダムに抽出する方法"},{"categories":"프로그래밍","contents":"개요1 名前のある140以上のCSSカラーパレットです。\n코드 ","id":3459,"permalink":"https://freshrimpsushi.github.io/jp/posts/3459/","tags":null,"title":"CSSカラー名札"},{"categories":"데이터과학","contents":"定義 1 統計学は、データを収集し、分析し、表示し、解釈し、決定する方法の集合だ。\n記述統計学は、図表やグラフと要約尺度などを使用してデータを構成し、表示し、説明する方法で構成されている。 推測統計学は、標本から母集団についての決定をするまたは予測をする方法で構成されている。 私見 以下は教科書外の話だ。\n個人的に統計学を「確率に関する理論を積極的に使用する応用数学の一分野」と定義したい。\nこれは一見統計学の特徴に過ぎないかもしれないが、実際に専攻レベルで学ぶ統計学―特に推測統計学を支える理論は、数理統計学であり、統計的な推論Statistical Inferenceとするものは大体確率論的な議論に基づいている。 統計学と直接的な関連はないが、確率論を導入して微細世界について研究する物理理論も統計力学Statistical Mechanicsと呼ばれている。 また、2010年代に入って機械学習、特にディープラーニングが大きく発展し、非構造データに対する技術水準が急速に上がった。彼らは、古典的な統計学がうまく扱えなかった分野、つまり自然言語処理、コンピュータビジョン、強化学習などの分野で非常に良い結果を出している。残念ながら、そのような分野を統計学の一部と見なす見解はほとんど見られない。 これらの理由から、定義で言及された統計学の定義はむしろデータサイエンスData Scienceの定義と呼ぶ方が正確かもしれない。ディープラーニングが流行する前にも、従来の機械学習は非パラメトリックNonparametricな方法として統計学の一部だったが、今となっては統計学が唯一のデータサイエンスではないことを認め、そのアイデンティティを確立する時が来ている。\nだが、悲しむ必要はない。生まれながらに統計学はディープラーニングなどとは異なり、その理論的基盤がしっかりしており、実際にパフォーマンス万能主義に失望して疲れた人が増えている。データサイエンスの全てだった時代に比べると少し小さくなったかもしれないが、応用数学の中でも依然として最も大きいものは変わらない。\n慶北大学校統計学科。(2008)。エクセルを使った統計学：p2~3。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2424,"permalink":"https://freshrimpsushi.github.io/jp/posts/2424/","tags":null,"title":"統計学の定義"},{"categories":"데이터과학","contents":"定義 1 質的変数 質的Qualitativeな特性を測定した変数を質的変数という。\n食べ物が\u0026hellip; 美味しい / まあまあ / まずい 色が\u0026hellip; 赤い / 青い / 黄色い 専攻が\u0026hellip; 数学 / 統計学 / 物理学 このような質的変数は、一般的にカテゴリカルCategoricalデータとも言われる。\n量的変数 量的Quantitativeな特性を測定した変数を量的変数という。\n年齢が\u0026hellip; 20歳 / 31歳 / 11歳 身長が\u0026hellip; 170.0cm / 170.5cm / 162.1cm 年齢や視力のようにはっきりとした値をとる量的変数を離散Discrete変数、身長や体重のように連続的な値をとる量的変数を連続Continuous変数という。\n説明 定義がなんだか奇妙に思えるかもしれないが、実際に「質的」と「量的」という言葉は、元々知っていた言葉ではなく、このような学術的な用語から日常的な表現を学ぶほうがむしろ正しいかもしれない。例えば、何かの品質を評価するときに、私たちは文字通り「クオリティが高い」という表現をよく使う。しかし、「質が高い」「質が低い」という言葉自体が、「1432ほど良い」や「17%ほど良い」とどう違うのかを考えてみよう。\n質的とは、このようにある順序（良い-まあまあ-悪い）を持つことはできるが、通常数値で表すのが難しいものを指す。もちろん、カテゴリー化されている（ドイツ語-フランス語-日本語）も問題ない。 量的はその反対で、量Amountを表すものを指す。ただし、ここで離散変数と連続変数の定義は少し難解かもしれない。 はっきりとした値とは？ はっきりとした値とは、いわゆる自然数や目盛りがあるような、ある単位で間隔を置いた値を説明する表現である。もちろん、どんな本にもそんなことは書いてないだろうし、私も見たことがない。そして、書きながらもあまり良い表現ではないと認める。代わりに私がとても気に入っている表現は以下の通りである。\nカウンタブルな値を取る変数を離散変数という。その値が限定的であるか数えることができるときのみを想定する。\n問題は、このように数学的に正確な表現が、すぐに離散変数が何なのか混乱しているあなたには何の役にも立たないことである。このような表現を理解することは、離散変数が何であるかを知っている人が離散変数について学ぶのと変わらない。\n何かがCountableであるとは、インド・ヨーロッパ語族、例えば私たちに馴染み深い英語、フランス語、スペイン語などで「1つ、2つ、\u0026hellip;」と数えられるものを指す。英語でそのようなものを表す名詞があれば、それを可算名詞と呼び、数学的に言えば自然数の集合と一対一対応が存在する。\nあまり役に立たない説明かもしれない。例を見て理解してみよう。以下の数は大抵離散変数である:\n牧場にいる豚の数 年間交通事故の死者数 専門書のページ数 幼児の年齢\u0026hellip;「24ヶ月の男の子」、「1歳2ヶ月の女の子」など 1Lの水筒の数 次に、離散変数かどうか迷うかもしれない例を見てみよう:\n1Lの水筒3つに入っている水の量\u0026hellip; 水筒の数ではなく、水の量なら連続型である。 視力\u0026hellip; 通常は0.1刻みだが、もし0.5、1.0、1.5の3つのグループしかなければ、離散変数 と見なすことができ、データの構成によっては質的変数と見なす余地もある。\n分類問題と回帰問題 通常、データサイエンスでは、従属変数が質的変数か量的変数かによって、分類問題と回帰問題を区別する。\n注意事項 実際にデータを扱いながら、経験が少ない初心者が犯しやすいミスがある。質的変数と量的変数を理解していないわけではなく、単に慣れていないために起こり得るミスであり、誰もが犯す可能性のあるミスである。多くの場合、回帰分析のような難しいものを勉強する頃にこのような罠に陥り、その直感を人工的に養う機会はほとんどない。次の投稿を見ると、正確に何を意味するのかはわからないかもしれないが、それがどのような罠なのかは大まかに理解できるかもしれない。\n質的変数を含む回帰分析 エンコーディング 性別を示す際に、男性を$0$、女性を$1$とエンコーディングEncodingする場合がよくあるが、目に見える数字があるからといって、これが離散変数（量的変数）になるわけではない。\nこのようなエンコーディングは、プライバシーのためにも使用される。想像してみよう。医療データは、個人の敏感な情報を多く含み、場合によってはデータだけで個々の人を特定できるほど特徴的な変数が多い。このような場合、データを公開する際に特定の情報を単に数字で隠すこともある。例えば、精神病歴、女性の中絶の有無などがある。\nレーティング 同様にエンコーディングの場合、レーティングが存在する場合がある。例えば、高卒が$0$、大卒が$1$、博士が$2$と表される場合、これが量的変数のように見えるが、依然として質的変数である。いわゆる低学歴、高学歴などは、一般社会の通念に過ぎず、データ的にこれらの数字は特に順序を示さない。現実のさまざまな例でこの主張を続けることができるが、ただちに高卒が$1$、大卒が$0$、博士が$2$とエンコーディングされるだけで、すでに量的変数ではないことがわかるだろう。\nヘックスコード 赤と青を区別することは質的変数だが、ピンク、ローズピンク、ディープピンクを区別するデータはどうだろう？これが口紅の話であれば、依然として質的変数で十分だが、例えば布の色であり、何千もの色がある場合、これらをRGBヘックスコードで表現できる。このようなデータに接する機会はほとんどないかもしれないが、直感的に質的変数だと思っても、量的変数として表現できる可能性があることを念頭に置く必要がある。\nジェンダー データにジェンダーGenderというカテゴリーが登場することもあるが、あなたが政治的正しさPolitical Correctnessに共感するか、うんざりするかにかかわらず、データがそう提供されているならば、まずはそのまま受け入れる必要がある。\nこれは本当の話だ。上で性別の例として挙げたように、ジェンダーが$0,1,2,3, \\cdots$でエンコーディングされたデータがあり、ジェンダー問題に全く関心がなかったある先輩が、「これ、ジェンダーで2と3は何？」と戸惑っていたのを見たことがある。アメリカ社会で調査されたデータでは、よくあることだ。 ポイントは、このようなことが起こらないように、ジェンダー問題に関心を持って勉強することではなく、特定のドメインDomainに関する知識が不足している場合は、直感に頼ってデータを検討しないことである。\nなぜ私たちはこれを知 る必要があるのか？\nこれらは非常に簡単で単純なことであるため、私たちはこれらを正確に区別し理解することができなければならない。ここでの私たちとは、統計学を応用する研究者を含め、統計学専攻者や、他の分野にバックグラウンドを持ちながらもデータサイエンスに従事する可能性のある人々を指す。\nこのように私たちが説明を探し、勉強し、課題をこなし、発表に慣れていく間に、皆さんの同僚たちはそれぞれ社会に適した何かをしていたであろう。残念ながら、それらの仕事は多くの場合大変だったため、私たちほどデータに精通していない可能性が高い。\n彼らはデータに無関心であったり、無知であったりするため、ここで述べられた注意事項を守らず、これらのばかげたミスを犯している可能性がある。そして、それらについて疑わない一般の人々を想像してみよう。あなたの上司Bossも例外ではない。\n私たちはそれを防がなければならない。\nMendenhall. (2012). 『確率と統計の入門』(13版): p10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2420,"permalink":"https://freshrimpsushi.github.io/jp/posts/2420/","tags":null,"title":"質的変数と連続変数"},{"categories":"추상대수","contents":"定義 実数の可逆な $n \\times n$ 行列の集合を $\\mathrm{GL}(n, \\mathbb{R})$ または $\\mathrm{GL}_{n}(\\mathbb{R})$ と表記し、$n$次の一般線型群general linear group of degree $n$と呼ぶ。\n$$ \\mathrm{GL}(n, \\mathbb{R}) := \\left\\{ n \\times n \\text{ invertible matrix} \\right\\} = M_{n \\times n}(\\mathbb{R}) \\setminus {\\left\\{ A \\in M_{n \\times n}(\\mathbb{R}) : \\det{A} = 0 \\right\\}} $$\n説明 可逆な行列だけを集めたので、行列の積に関して群になる。また、微分可能な構造を持つため、リー群でもある。\n","id":3450,"permalink":"https://freshrimpsushi.github.io/jp/posts/3450/","tags":null,"title":"一般リニア群"},{"categories":"위상데이터분석","contents":"定理 1 2 カバーとリフトの定義: 単位区間を$I = [0,1]$のように表す。\n$X$のオープンセット$U \\subset X$が**$p$によって均等にカバーされる**Evenly Covered by $p$とは、全ての$\\alpha \\in \\forall$に対応する全ての制限関数$p |_{\\widetilde{U}_{\\alpha}}$がホメオモルフィズムであり $$ \\alpha_{1} \\ne \\alpha_{2} \\implies \\widetilde{U}_{\\alpha_{1}} \\cap \\widetilde{U}_{\\alpha_{2}} = \\emptyset $$ を満たす、つまり互いに素な$\\widetilde{X}$のオープンセット$\\widetilde{U}_{\\alpha} \\subset \\widetilde{X}$について $$ p^{-1} \\left( U \\right) = \\bigsqcup_{\\alpha \\in \\forall} \\widetilde{U}_{\\alpha} $$ が成り立つことを意味する。 $p : \\widetilde{X} \\to X$が全射関数であり、全ての$x \\in X$に対して$p$によって均等にカバーされる$x$のオープンネイバーフッド$U_{x} \\subset X$が存在する場合、$p : \\widetilde{X} \\to X$をカバーCoveringという。 カバー$p$の定義域$\\widetilde{X}$をカバースペースCovering Space、値域$X$をベーススペースBase Spaceという。 $n \\in \\mathbb{N}$とする。$f : I^{n} \\to X$と$\\widetilde{f} : I^{n} \\to \\widetilde{X}$が次を満たす場合、$\\widetilde{f}$を$f$のリフトLiftという。 $$ f = p \\circ \\widetilde{f} $$ $1$-スフィア$S^{1}$を値域に持つカバーを$p : \\mathbb{R} \\to S^{1}$としよう。\nパスリフティング定理 連続関数$f : I \\to S^{1}$はリフト$\\widetilde{f} : I \\to \\mathbb{R}$を持つ。特に与えられた$x_{0} \\in S^{1}$と$\\widetilde{x}_{0} \\in p^{-1} \\left( x_{0} \\right)$に対して、$\\widetilde{f} \\left( 0 \\right) = \\widetilde{x}_{0}$である$\\widetilde{f}$は一意に存在する。\nホモトピー・リフティング定理 連続関数$F : I^{2} \\to S^{1}$はリフト$\\widetilde{F} : I^{2} \\to \\mathbb{R}$を持つ。特に与えられた$x_{0} \\in S^{1}$と$\\widetilde{x}_{0} \\in p^{-1} \\left( x_{0} \\right)$に対して、$\\widetilde{F} \\left( 0 , 0 \\right) = \\widetilde{x}_{0}$である$\\widetilde{F}$は一意に存在する。\n説明 リフティング定理Lifting Theoremは一般に単位円$S^{1}$の性質を研究するための補助定理として言及され、形式的にFormally見た場合、パスリフティングか、ホモトピー・リフティングかという区別はあまり意味がない。\nむしろ、ほとんどの数学者が気にすべき質問は$X \\ne S^{1}$である$f: I^{m} \\to X$に対する一般化が可能かという点であり、実際にはコンパクト空間$Y$に対する連続関数$f: Y \\times I^{m} \\to X$に対するリフティング定理まで論じることができる。ただし、このような拡張が実際には全く役に立たないため、直接学ぶには過剰だと言われている。\n証明 戦略: パスリフティング定理のみを証明する。本質的にホモトピー・リフティング定理の証明はパスリフティング定理の証明と同じである。パスリフティング定理ではコンパクト空間である$I$から区間を有限に分割して証明するように、ホモトピー・リフティング定理では同様にコンパクトな空間である$I^{2}$を有限に分割して同じ議論を繰り返す。\nPart 1. 設定\n$p : \\widetilde{X} \\to X$が全射関数であり、全ての$x \\in X$に対して$p$によって均等にカバーされる$x$のオープンネイバーフッド$U_{x} \\subset X$が存在する場合、$p : \\widetilde{X} \\to X$をカバーCoveringという。 $p : \\mathbb{R} \\to S^{1}$はカバーとされているので、全ての$x \\in S^{1}$に対して$p$によって均等にカバーされる$x$のネイバーフッド$U_{x} \\subset S^{1}$が存在する。\n$I = [0,1]$はコンパクトなので$I \\subset \\bigcup_{k=1}^{n} \\left[ a_{k-1} , a_{k} \\right]$を満たすような有限の点の集合$\\left\\{ a_{k} \\right\\}_{k=0}^{n} \\subset I$が存在し、 $$ 0 = a_{0} \u0026lt; a_{1} \u0026lt; \\cdots \u0026lt; a_{n-1} \u0026lt; a_{n} = 1 $$ その区間$\\left[ a_{k-1} , a_{k} \\right] \\subset I$に対する$f$のイメージは$S^{1}$に含まれ、特にあるオープンセット$U \\subset S^{1}$に対して以下の包含関係を満たす。 $$ f \\left( \\left[ a_{k-1} , a_{k} \\right] \\right) \\subset U \\subset S^{1} $$ このような$U$に対するカバー$p$の互いに素なプレイメージを$\\widetilde{U}_{t} := p^{-1} \\left( U_{t} \\right)$とすれば、それぞれ$t \\in \\mathbb{Z}$に対して$U$とホメオモルフィックである。\nPart 2. 帰納的構築\n任意の$x \\in S^{1}$ではなく、具体的に$x_{0} \\in S^{1}$を選び、その$p$のプレイメージの要素の一つを$\\widetilde{x}_{0} := p^{-1} \\left( x_{0} \\right) \\in \\mathbb{R}$と表す。元の設定によれば、これらの要素の集合は$\\mathbb{Z}$との間に全単射が存在するが、どれがどうであれ関係ない。\n私たちは$I$全体ではなく、$\\left[ 0, a_{k} \\right]$に対して$\\widetilde{f}_{k} (0) = \\widetilde{x}_{0}$を満たすリフト$\\widetilde{f}_{k}$を帰納的に定義して、結果的に$\\widetilde{f}$を見つけようとしている。\n$k = 0$の場合は単に$\\widetilde{f}_{0} (0) = \\widetilde{x}_{0}$とし、他に選択肢はない。 $k \\ne 0$の場合、連続関数$\\widetilde{f}_{k} : \\left[ 0 , a_{k} \\right] \\to \\mathbb{R}$が一意に定義されると仮定する。 ある一意の$\\widetilde{U} \\in \\left\\{ \\widetilde{U}_{t} \\right\\}_{t \\in \\mathbb{Z}}$に対して$\\widetilde{f} \\left( a_{k} \\right) \\in \\widetilde{U}$である。 $\\widetilde{f}_{k}$は連続であり、区間$\\left[ a_{k} , a_{k+1} \\right]$は経路連結であるため、$\\widetilde{f}_{k}$の拡張関数$\\widetilde{f}_{k+1}$がどのように定義されても、少なくとも$\\left[ a_{k} , a_{k+1} \\right]$は必ず$\\widetilde{U}$内にマッピングされなければならない。 $p$がカバーであるため、全ての$t \\in \\mathbb{Z}$に対してホメオモルフィズム$p | \\widetilde{U}_{t} : \\widetilde{U}_{t} \\to U$が存在し、それにより $$ p \\circ \\rho_{k} = f | \\left[ a_{k} , a_{k+1} \\right] $$ を満たす一意の関数$\\rho_{k} : \\left[ a_{k} , a_{k+1} \\right] \\to \\widetilde{U}$が存在する。このような関数$\\rho_{k}$の存在は、$p$の制限関数がホメオモルフィズムであること―すなわち単射であることに基づくため、$\\rho_{k} \\left( a_{k} \\right) = \\widetilde{f}_{k} \\left( a_{k} \\right)$であり、$\\rho_{k}$の連続性も保証される。 接着補題: 位相空間$X,Y$に対して、二つの閉集合$A,B \\subset X$が$A \\cup B = X$を満たし、二つの連続関数$f : A \\to Y$と$g : B \\to Y$が全ての$x \\in A \\cap B$に対して$f(x) = g(x)$であるとする。すると、以下のように定義された$h$は連続関数である。 $$ h(x) : = \\begin{cases} f(x), \u0026amp; x \\in A \\\\ g(x), \u0026amp; x \\in B \\end{cases} $$\n接着補題により、以下のような連続関数$\\widetilde{f}_{k+1} : \\left[ 0 , a_{k+1} \\right] \\to \\mathbb{R}$を一意に定義できる。 $$ \\widetilde{f}_{k+1} := \\begin{cases} \\widetilde{f}_{k} (s) \u0026amp; , \\text{if } s \\in \\left[ 0, a_{k} \\right] \\\\ \\rho_{k} (s) \u0026amp; , \\text{if } s \\in \\left[ a_{k} , a_{k+1} \\right] \\end{cases} $$ 数学的帰納法により、$S^{1}$は$t \\in \\mathbb{Z}$周回する螺旋に向かうリフトが具体的に存在する。ここで、$k = 0, 1, \\cdots , n$は$\\mathbb{R}$で上下に動くインデックスではなく、$S^{1}$を回転させながら有限に分割するインデックスであることをよく想像しなければならない。$k$が$1$ずつ増えるごとに、$\\mathbb{R}$では整数の数だけ多くの区間の集合$\\left\\{ \\widetilde{U}_{t} \\right\\}_{t \\in \\mathbb{Z}}$も同様に回転して動く。\nPart 3. ホモトピー・リフティング定理\n$0$から$n-1$までの整数を集めた集合$\\left\\{ 0, 1, \\cdots , n-1 \\right\\}$を簡単に$0:n$と書こう。 $I$がコンパクトであることに基づいて$0 = a_{0} \u0026lt; \\cdots \u0026lt; a_{n} = 1$を選べたように、$I^{2}$もコンパクトであるため、 $$ \\begin{align*} 0 = a_{0} \u0026lt; \\cdots \u0026lt; a_{n} = 1 \\\\ 0 = b_{0} \u0026lt; \\cdots \u0026lt; b_{m} = 1 \\end{align*} $$ のように正方形を格子に切る有限の二つの自然数$n , m \\in \\mathbb{N}$が存在し、それぞれの小さなマスを$i = 0:n$、$j = 0:m$に対して $$ R_{i,j} := \\left[ a_{i-1}, a_{i} \\right] \\times \\left[ b_{j-1} , b_{j} \\right] \\subset I^{2} $$ と定義すると、 $$ R_{0,0} , R_{0,1} , \\cdots , R_{0,m} , R_{1,0} \\cdots, R_{n,m} $$ のような小さな長方形のシーケンスが得られる。これに対してパスリフティング定理で行った議論を繰り返せば、ホモトピー・リフティング定理が証明される。\n■\nKosniowski. (1980). A First Course in Algebraic Topology: p137~138.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHatcher. (2002). Algebraic Topology: p29~31.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2419,"permalink":"https://freshrimpsushi.github.io/jp/posts/2419/","tags":null,"title":"代数的トポロジーにおけるリフティング定理の証明"},{"categories":"데이터과학","contents":"概要 現代社会では、データについて全く知らない知識人はいない。全く関心がない非専門家でも、「何かについての知識」や「通信するための資源」のようなデータや情報といった類義語を容易に思い浮かべることができるほど、データという概念は普遍化し、大衆化された。以下の記述はほんの少しでも、データサイエンスの観点から、データをより厳密に定義しようとする試みに過ぎない。\n定義 1 変数Variableは、時間や個人Individual、または対象Objectによって変わる特性Characteristicを指す。 変数が測定される個人や対象を 実験単位Experimental Unitといい、実験単位から実際に測定された結果を 測定値Measurementという。 測定値の集合を データDataという。 説明 データの語源 2 英語のデータは「与えられた、または認められた事実」を意味し、ラテン語で「与える」という意味の動詞「Do-」の過去分詞であるDatumから派生しており、「与えられた」という意味を持つ。Dataは、そのDatumの複数形にあたる。\n皮肉にも、このようなデータの語源は、上述のように、何か特徴であるとか、実験をしながらどうにか定義しようとしていたことよりも、より正確にデータの本質を指している。データサイエンスの世界では、データはすでに与えられたもの、またはこれから我々に与えられるべきものであり、新しい発見や創造の対象とは明らかに異なる属性を持つ。\nつまり、データはどうしようもなく、すなわち 与えられたものだ。粗野な比喩として、長持ちする電球を発明する状況を想像してみよう。平均寿命が100時間の電球Aから電球Bを改良した場合、各電球B（Object）の寿命を測定することができるだろう。この測定値を集めたものがまさに電球Bの寿命データであり、それらの数値は電球Bによって与えられたものであって、電球Aのデータ自体をどうにか変えて得たものではない。\n変数と実験？ 変数變數は、字のごとく変わる数値として考えがちで、データを簡単に説明するときにはよく数字が登場するけれど、非構造データに対する理解が深まった現代社会では、データを数字やカテゴリーに限定する必要はない。データの種類には、写真、文書、信号、株価、動画、ネットワーク構造など、人が認識できるすべてが対象である。同様に、測定値測定値も、「値」という文字を使うために数字のように見えるかもしれないが、そのように数値として考える必要はない。可能な限り、英語の表現Measurementをそのまま使うことをお勧めする。\nまた、実験単位の実験は、白衣を着た科学者たちが研究所で行うものだけを指すわけではない。基礎確率論で事象が起こることを「任意の試行」と呼ぶように、表現のための表現として受け入れても十分だ。\n母集団と標本 調査者Investigatorが関心を持つすべての測定値の集合を 母集団Populationという。 母集団の部分集合を サンプルSampleと呼ぶ。 \u0026hellip;このような定義から、現実的には多くのデータが母集団のサンプルであることが推測できる。一方で、母集団の英語表現Populationは、統計学とも密接に関係している人口という意味も持っているので注意してほしい。\n統計学のコンセプトは基本的に「母集団について知りたいが、実際には母集団をすべて調査することはできないので、サンプルを通じて母集団の特性を把握すること」、つまり、データを通じて関心ある対象の本質を推測することと言える。\n参考までに 数理統計学におけるサンプルの定義 学部2～3年生レベルで触れる数理統計学では、このポストで説明するサンプルについて数理的な定義を下し、データの別の表現である実現Realizationを紹介している。\nMendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p8.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.etymonline.com/word/data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2418,"permalink":"https://freshrimpsushi.github.io/jp/posts/2418/","tags":null,"title":"データの定義と語源"},{"categories":"줄리아","contents":"概要 元々ジュリアでは、データを出力する時にREPLのサイズに合わせてきれいに出力されるが、時には全体のデータを楽に見たい時がある。データがfooであれば、show(stdout, \u0026quot;text/plain\u0026quot;, foo)を通じて全体のデータを出力させることができる1。\nコード julia\u0026gt; foo = rand(100,2)\r100×2 Matrix{Float64}:\r0.956438 0.663427\r0.790117 0.472821\r0.976134 0.198475\r0.727601 0.472336\r0.0469046 0.991999\r0.625807 0.26634\r0.490773 0.588481\r0.352966 0.426474\r0.585632 0.00185974\r⋮\r0.615713 0.707071\r0.891683 0.622176\r0.370576 0.937107\r0.430644 0.0439135\r0.535987 0.551992\r0.43273 0.217023\r0.345366 0.500033\r0.551719 0.0761454 元々は上のように⋮が印刷されるが、プレーンテキストで印刷すると次のように全体が出力される。\njulia\u0026gt; show(stdout, \u0026#34;text/plain\u0026#34;, foo)\r100×2 Matrix{Float64}:\r0.956438 0.663427\r0.790117 0.472821\r0.976134 0.198475\r0.727601 0.472336\r0.0469046 0.991999\r0.625807 0.26634\r0.490773 0.588481\r0.352966 0.426474\r0.585632 0.00185974\r0.13357 0.90977\r0.789999 0.137833\r0.11626 0.385958\r0.629265 0.40623\r0.111327 0.483414\r0.22717 0.0960839\r0.854027 0.690618\r0.00862816 0.426555\r0.292845 0.588308\r0.475157 0.935968\r0.936422 0.116917\r0.421748 0.335614\r0.354324 0.444122\r0.52423 0.311464\r0.306786 0.873037\r0.308008 0.70787\r0.0885757 0.558464\r0.0510476 0.840701\r0.320569 0.28571\r0.89837 0.517027\r0.218359 0.622536\r0.563148 0.488849\r0.508919 0.818068\r0.880726 0.550501\r0.555517 0.953056\r0.466298 0.29687\r0.816757 0.528656\r0.789289 0.294199\r0.51256 0.173814\r0.972556 0.11602\r0.438784 0.815105\r0.218237 0.257226\r0.0838205 0.535666\r0.287095 0.877342\r0.176927 0.942882\r0.855193 0.577759\r0.813356 0.488643\r0.407358 0.970933\r0.224252 0.455783\r0.430215 0.727\r0.0585314 0.727251\r0.77538 0.777196\r0.114963 0.610359\r0.445436 0.472755\r0.0565616 0.153393\r0.695217 0.00669471\r0.673818 0.284351\r0.308611 0.386984\r0.761394 0.32279\r0.017963 0.114759\r0.465956 0.788791\r0.970691 0.264864\r0.0953205 0.359958\r0.437556 0.283858\r0.323666 0.893141\r0.971015 0.109052\r0.117792 0.919322\r0.898883 0.947123\r0.248386 0.462831\r0.895525 0.434108\r0.526593 0.288652\r0.891208 0.848443\r0.344758 0.412774\r0.697527 0.592066\r0.531953 0.50251\r0.0565245 0.449993\r0.168528 0.783811\r0.129681 0.22014\r0.489568 0.232417\r0.875734 0.380527\r0.0207026 0.915546\r0.210948 0.476037\r0.822661 0.517793\r0.579839 0.0221691\r0.455027 0.920253\r0.932968 0.771582\r0.960643 0.841065\r0.0835567 0.943408\r0.578494 0.502968\r0.0655954 0.528926\r0.590831 0.41364\r0.840604 0.790515\r0.327964 0.269113\r0.615713 0.707071\r0.891683 0.622176\r0.370576 0.937107\r0.430644 0.0439135\r0.535987 0.551992\r0.43273 0.217023\r0.345366 0.500033\r0.551719 0.0761454 環境 OS: Windows julia: v1.7.0 https://stackoverflow.com/questions/49304329/how-to-show-all-elements-of-vectors-and-matrices-in-julia/67090474#67090474\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2416,"permalink":"https://freshrimpsushi.github.io/jp/posts/2416/","tags":null,"title":"ジュリアでデータを省略せずに出力する方法"},{"categories":"머신러닝","contents":"概要1 $$ \\includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$\nMNISTmodified national institute of standards and technology データベースとは、アメリカの高校生と人口調査局の職員の数字の手書き文字に関するデータセットを指す。一般に[エムニスト]と呼ばれる。\n公式ホームページ 機械学習/ディープラーニング入門の例としてよく使用されるデータセットである。NISTでは、手書きの郵便番号の自動分類のための文字認識技術の評価のため、以下のような形式で手書きデータを収集した。ここでヤン・ルカンYann LeCunが高校生と人口調査局の職員の手書きデータを取り、前処理を行い、MNISTを作成した。画像のサイズは28 x 28で、60,000枚のトレーニングセットと10,000枚のテストセットで構成されている。\n$$ \\includegraphics[height=30em]{https://www.nist.gov/sites/default/files/styles/960_x_960_limit/public/images/2019/04/27/sd19.jpg?itok=oETq77cZ} $$\n使用方法 Julia Juliaでは、機械学習データセットパッケージであるMLDatasets.jlを使用できる。基本的にはFloat32型のトレーニングセットを読み込む。オプションでこれを変更して読み込むことができる。使用できるメソッドは以下の通り。\ndataset[i]: i番目の特徴量とターゲットのタプルを返す。 dataset[:]: 全ての特徴量とターゲットのタプルを返す。 length(dataset): データの数を返す。 convert2image(dataset, i): i番目のデータをグレースケールの画像に変換する。ImageShow.jlパッケージが必要である。 julia\u0026gt; using MLDatasets\rjulia\u0026gt; train = MNIST()\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :train\rfeatures =\u0026gt; 28×28×60000 Array{Float32, 3}\rtargets =\u0026gt; 60000-element Vector{Int64}\rjulia\u0026gt; test = MNIST(Float64, :test)\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :test\rfeatures =\u0026gt; 28×28×10000 Array{Float64, 3}\rtargets =\u0026gt; 10000-element Vector{Int64}\rjulia\u0026gt; length(train), length(test)\r(60000, 10000)\rjulia\u0026gt; using Plots\rjulia\u0026gt; using ImageShow\rjulia\u0026gt; train.targets[1]\r5\rjulia\u0026gt; heatmap(convert2image(train, 1)) ラベルは整数で与えられるため、ワンホットエンコーディングを別途行う必要がある。\njulia\u0026gt; train.targets[1:5]\r5-element Vector{Int64}:\r5\r0\r4\r1\r9\rjulia\u0026gt; using Flux\rjulia\u0026gt; Flux.onehotbatch(train.targets[1:5], 0:9)\r10×5 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\r⋅ 1 ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ 1 ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ 1 ⋅ ⋅\r1 ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ 1 Julia Fluxでワンホットエンコーディングする方法 Julia FluxでMLPを実装し、MNISTを学習する方法 環境 OS: Windows11 Version: Julia v1.8.2, MLDatasets v0.7.6, Plots v1.36.1, ImageShow v0.3.6, Flux v0.13.7 권건우·허령, 人工知能をマンガと野史で学ぶ 2, p68\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3444,"permalink":"https://freshrimpsushi.github.io/jp/posts/3444/","tags":null,"title":"MNIST Database"},{"categories":"줄리아","contents":"概要 Juliaは、MATLABレベルの線形代数をサポートしている。むしろMATLABよりも進化した、直感的で美しい構文を見ると、Juliaが作られた時点でよく設計されていたと感じられる1。\nコード julia\u0026gt; A = [ 1 0 3\r0 5 1\r3 1 9\r] 3×3 Matrix{Int64}:\r1 0 3\r0 5 1\r3 1 9 見ての通り、行列を定義する段階で既に直感的で便利だ。ここで、普通に必要とされるいくつかの関数について学ぶ。小見出しに関連記事をリンクし、別の説明は省略する。\nトレース tr() julia\u0026gt; tr(A)\r15 行列式 det() julia\u0026gt; det(A)\r-1.000000000000003 逆行列 inv() julia\u0026gt; inv(A)\r3×3 Matrix{Float64}:\r-44.0 -3.0 15.0\r-3.0 6.10623e-16 1.0\r15.0 1.0 -5.0\rjulia\u0026gt; round.(Int64, inv(A))\r3×3 Matrix{Int64}:\r-44 -3 15\r-3 0 1\r15 1 -5 対角行列と対角成分 diag(), diagm() julia\u0026gt; diag(A)\r3-element Vector{Int64}:\r1\r5\r9\rjulia\u0026gt; diagm([1,5,9])\r3×3 Matrix{Int64}:\r1 0 0\r0 5 0\r0 0 9 ノルム norm() julia\u0026gt; norm(A, 1)\r23.0 固有値 eigvals() julia\u0026gt; eigvals(A)\r3-element Vector{Float64}:\r-0.020282065792505244\r4.846013411157458\r10.174268654635046\rjulia\u0026gt; eigvecs(A)\r3×3 Matrix{Float64}:\r-0.944804 0.117887 0.305692\r-0.0640048 -0.981459 0.180669\r0.321322 0.151132 0.934832\rjulia\u0026gt; eigmax(A)\r10.174268654635046 行列分解 factorize() julia\u0026gt; factorize(A)\rBunchKaufman{Float64, Matrix{Float64}}\rD factor:\r3×3 Tridiagonal{Float64, Vector{Float64}}:\r-0.0227273 0.0 ⋅ 0.0 4.88889 0.0\r⋅ 0.0 9.0\rU factor:\r3×3 UnitUpperTriangular{Float64, Matrix{Float64}}:\r1.0 -0.0681818 0.333333\r⋅ 1.0 0.111111\r⋅ ⋅ 1.0\rpermutation:\r3-element Vector{Int64}:\r1\r2\r3\rjulia\u0026gt; svd(A)\rSVD{Float64, Float64, Matrix{Float64}}\rU factor:\r3×3 Matrix{Float64}:\r-0.305692 0.117887 -0.944804\r-0.180669 -0.981459 -0.0640048\r-0.934832 0.151132 0.321322\rsingular values:\r3-element Vector{Float64}:\r10.174268654635044\r4.846013411157461\r0.02028206579250516\rVt factor:\r3×3 Matrix{Float64}:\r-0.305692 -0.180669 -0.934832\r0.117887 -0.981459 0.151132\r0.944804 0.0640048 -0.321322 行列代数カテゴリの行列分解を参照せよ。行列の形に応じて適切な分解法を自動的に選んで分解してくれる。もちろん、条件を満たすなら、具体的な分解関数を直接使っても良い。\n行列の操作 julia\u0026gt; B = [\r1 0 1\r1 1 0\r2 1 1\r]\r3×3 Matrix{Int64}:\r1 0 1\r1 1 0\r2 1 1\rjulia\u0026gt; A + B\r3×3 Matrix{Int64}:\r2 0 4\r1 6 1\r5 2 10\rjulia\u0026gt; A - B\r3×3 Matrix{Int64}:\r0 0 2\r-1 4 1\r1 0 8\rjulia\u0026gt; A * B\r3×3 Matrix{Int64}:\r7 3 4\r7 6 1\r22 10 12\rjulia\u0026gt; A .* B\r3×3 Matrix{Int64}:\r1 0 3\r0 5 0\r6 1 9\rjulia\u0026gt; B / A\r3×3 Matrix{Float64}:\r-29.0 -2.0 10.0\r-47.0 -3.0 16.0\r-76.0 -5.0 26.0\rjulia\u0026gt; B * inv(A)\r3×3 Matrix{Float64}:\r-29.0 -2.0 10.0\r-47.0 -3.0 16.0\r-76.0 -5.0 26.0\rjulia\u0026gt; A / B\rERROR: SingularException(3) 我々が考える常識的な操作は全て通用する。割り算は、当然乗算の逆元である逆行列を掛けるのと同じで、Bのように逆行列が存在しない場合は、シンギュラー例外をレイズする。\nブロック行列 [] 他の言語と比べて、ブロック行列を非常に便利に作ることができる。\njulia\u0026gt; [A B]\r3×6 Matrix{Int64}:\r1 0 3 1 0 1\r0 5 1 1 1 0\r3 1 9 2 1 1\rjulia\u0026gt; [A;B]\r6×3 Matrix{Int64}:\r1 0 3\r0 5 1\r3 1 9\r1 0 1\r1 1 0\r2 1 1\rjulia\u0026gt; [A,B]\r2-element Vector{Matrix{Int64}}:\r[1 0 3; 0 5 1; 3 1 9]\r[1 0 1; 1 1 0; 2 1 1] 二つの行列の間にスペースを置くと横に積み上げ、セミコロンを置くと縦に積み上げる。カンマは行列を積み上げるわけではなく、一般的に配列で使用していた構文そのままで、行列の配列になる。\n全体のコード 複素行列や内積に関連する内容は省略したが、全体のコードには含まれている。\nusing LinearAlgebra\rA = [\r1 0 3\r0 5 1\r3 1 9\r]\rtr(A)\rdet(A)\rinv(A)\rround.(Int64, inv(A))\rdiag(A)\rdiagm([1,5,9])\rnorm(A, 1)\reigvals(A)\reigvecs(A)\reigmax(A)\rfactorize(A)\rsvd(A)\rB = [\r1 0 1\r1 1 0\r2 1 1\r]\rdet(B)\rrank(B)\reigvals(B)\rSymmetric(B) # |\u0026gt; issymmetric\rtranspose(B)\rB\u0026#39;\rC = [\rim im 1\r2 im 0\rim 1 2\r]\rC\u0026#39;\rB\u0026#39;B\rx = [1,2,3]\ry = [0,1,2]\rx\u0026#39;y\rA + B\rA - B\rA * B\rA .* B\rB / A\rB * inv(A)\r[A B]\r[A;B]\r[A,B]\rx\u0026#39; * y\ry * x\u0026#39; 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2412,"permalink":"https://freshrimpsushi.github.io/jp/posts/2412/","tags":null,"title":"ジュリアで線形代数パッケージを使用する方法"},{"categories":"위상데이터분석","contents":"定義1 単位閉区間$I := [0,1]$と位相空間$X$が与えられているとする。\n固定された二点$x_{0} , x_{1} \\in X$に対して、次を満たす連続関数$p : I \\to X$を$x_{0}$から$x_{1}$へのパスまたはパスPathという。 $$ \\begin{align*} p(0) =\u0026amp; x_{0} \\\\ p(1) =\u0026amp; x_{1} \\end{align*} $$ 二つのパス$f \\equiv h_{0}$と$g \\equiv h_{1}$に対して、以下の二条件を満たすパスの集合$\\left\\{ h_{t} \\right\\}_{t \\in [0,1]}$をホモトピーHomotopyという。 (i): $t$に独立して$h_{t} (0) = x_{0}$、かつ$h_{t} (1) = x_{1}$である。 (ii): 全ての$s,t \\in I$に対して、$H(s,t) := h_{t} (s)$として定義された$H : I \\times I \\to X$が連続である。文脈によって、$H$または$h_{t}$自体をホモトピーと呼ぶこともある。 二つのパス$f$と$g$にホモトピーが存在する場合、$f$と$g$がホモトピックHomotopicであると言い、$f \\simeq g$のように表す。 説明 ホモトピーとは、簡単に言えば、与えられた二つの点を連続的に繋げる関数のことであり、二つの点を繋ぐ方法が本質的に同じならば、数学的にそれらを同じものとして扱うためのものだ。例えば、上の図で左の点と右の点を繋ぐ方法は無数にあるが、（一般に知られている）位相数学の観点から見て直線で行くか少し曲がって行くか、正直何の違いがあるだろうか？[ NOTE: ここでの「少し」という表現は、定義で言及されている$H$の「連続性」を意味する。]\n上の図を見ると、$H$は$s$に従って変化する関数$h_{t}$が単位正方形$I^{2}$内で連続的に変化することを示している。\nホモトピーの意味 前述のように、二つのパスがホモトピックであるとは、一方のパスを少し変えれば反対側にあるパスになるという意味として受け取ることができる。しかし、位相数学でこのような「事実上同じ」ということを探究するのは、「真に異なる」を見るためである。\n例えば、上記のようにトーラス上で二点を繋ぐ二つのパスを見てみよう。トーラスは中央が空いているため、青いパスと赤いパスを繋ぐホモトピーは存在せず、これらはホモトピックではない。重要なのは、「点と点」だけでなく「点と点の関係の関係」を見ることにより、トーラスと非トーラス凸形を区別Classificationする段階に達したことだ。大げさに言えば、ホモトピーを研究することは、単純に「関数の関数」のような非専門家には理解しにくい言葉遊びではなく、空間の本質を見る新しい方法論である。\n同値条件 それほど証明するほどの大した事実ではなく、ただ紹介するだけなので、実際に使うときは以下の定義がもっと便利かもしれないが、言及しておく。\n$f$と$g$がホモトピックであることは、以下の二条件を満たす連続関数$H : I \\times I \\to X$が存在することと同値である。\n(i): 全ての$t \\in I$に対して、$H(0,t) = x_{0}$であり、$H(1,t) = x_{1}$である。 (ii): 全ての$s \\in I$に対して、$H (s, 0) = f(s)$であり、$H(s,1) = g(s)$である。 ホモトピーはパスのパスだ 以下の話は少し難しいので、ややこしいと思ったらスキップしてもいい。 $$ \\begin{align*} h_{t} (s) =\u0026amp; x_{0} \\to x_{1} \u0026amp; \\text{ as } s = 0 \\to 1 \\\\ h_{t} = \u0026amp; f \\to g \u0026amp; \\text{ as } t = 0 \\to 1 \\end{align*} \\qquad \\cdots 🤔 ! $$\n公式に見れば、$X$で二点$x_{0}, x_{1}$を繋ぐパス$f,g : I \\to X$は、連続関数の空間の要素である$f,g \\in C \\left( I, X \\right)$であり、$h_{t} : I \\to C \\left( I , X \\right)$はその関数空間における二つのパス$f, g$のパス $$ h_{t} \\in C \\left( I , C \\left( I , X \\right) \\right) $$ と呼ぶことができる。わざわざこのような表現を定義で使わない理由は、ホモトピーの定義自体で$C \\left( I , X \\right)$を位相空間として考えるには自然な位相を言及するのが過ぎるからである。定義は短ければ短いほど良いものだし、ホモトピーを論じるためには、二変数関数$H$の連続性만を要求することで十分である。\n$H$の連続性ではなく、関数空間での連続性について話す場合、まず関数空間のコンパクト-オープン位相などの関数空間の位相が必要だが、これは過剰である。もちろん、文献で定義が簡単に提示されているからといって、われわれも定義だけを知っていれば良いわけではない。既にホモトピーは上で簡単に定義されたので、残りの時間でその先を少し想像してみよう。数学では、$Y, X$がどんな集合であれ、関数 $$ F : Y \\to X $$ に関心を持たざるを得ない。関数の関数に関心を持つこと、つまり、定義域が$Z$で、値域が関数空間$X^{Y}$である新しい関数 $$ H : Z \\to X^{Y} \\iff H : Z \\times Y \\to X $$ を考えることは、関数のシーケンスや内積空間など、終わりのない新しい研究テーマを生み出す。このような好奇心が自然であると同意するなら、今度は$Z$と$Y$の場に単位閉区間$I = [0,1]$を入れてみよう。 $$ H : I \\times I \\to X $$\nこれは他ならぬ、私たちが定義で見た$H$である。言い換えれば、ホモトピーはその複雑な名前とは裏腹に\n単に$I \\times I$から定義され、 明らかに関心を持つべき 連続関数の連続関数であり、 始点と終点が指定されただけの 関数の集合 に過ぎない。ホモトピーが見知らずで嫌だと感じるなら、$Y,Z$という広大な領域ではなく、$I \\times I$という小さな範囲に留まることに感謝しよう。\nHatcher. (2002). Algebraic Topology: p25.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2411,"permalink":"https://freshrimpsushi.github.io/jp/posts/2411/","tags":null,"title":"ホモトピーの定義"},{"categories":"줄리아","contents":"概要 1 Datesは、日付や時間に関連する関数をまとめたモジュールだ。一般的なプログラミングはもちろん、時系列に関する、いやそれに関係なく多くのデータを扱う上で、非常に役に立つものに違いない1。\nコード 全コード using Dates\r오늘 = DateTime(2022,3,10)\rtypeof(오늘)\rpropertynames(오늘)\r오늘.instant\rmyformat = DateFormat(\u0026#34;d-m-y\u0026#34;)\r내일 = Date(\u0026#34;11-3-2022\u0026#34;, myformat)\rDates.dayname(내일)\r일주일뒤까지 = 오늘:Day(1):DateTime(2022,3,17)\rcollect(일주일뒤까지)\rDates.Day(일주일뒤까지[end]) - Dates.Day(오늘) DateTime タイプ julia\u0026gt; 오늘 = DateTime(2022,3,10)\r2022-03-10T00:00:00\rjulia\u0026gt; typeof(오늘)\rDateTime 例えば DateTime() 関数で22年3月10日の日付を 今日 に割り当てたなら、今日 は DateTime というタイプを持つことになる。DateTime は instant というプロパティを持ち、ミリ秒単位で時間を記録している。\njulia\u0026gt; propertynames(오늘)\r(:instant,)\rjulia\u0026gt; 오늘.instant\rDates.UTInstant{Millisecond}(Millisecond(63782553600000)) フォーマット DateFormat() julia\u0026gt; myformat = DateFormat(\u0026#34;d-m-y\u0026#34;)\rdateformat\u0026#34;d-m-y\u0026#34;\rjulia\u0026gt; 내일 = Date(\u0026#34;11-3-2022\u0026#34;, myformat)\r2022-03-11 よく、東西の違いで日付が違って記述される時に使われる。\n曜日 Dates.dayname() julia\u0026gt; Dates.dayname(내일)\r\u0026#34;Friday\u0026#34; 指定された日付の曜日を返してくれる。グレゴリオ暦の不合理さのため、自分で作ると意外と難しいものがこんなものだ。\n日付のベクタ julia\u0026gt; 일주일뒤까지 = 오늘:Day(1):DateTime(2022,3,17)\rDateTime(\u0026#34;2022-03-10T00:00:00\u0026#34;):Day(1):DateTime(\u0026#34;2022-03-17T00:00:00\u0026#34;)\rjulia\u0026gt; collect(일주일뒤까지)\r8-element Vector{DateTime}:\r2022-03-10T00:00:00\r2022-03-11T00:00:00\r2022-03-12T00:00:00\r2022-03-13T00:00:00\r2022-03-14T00:00:00\r2022-03-15T00:00:00\r2022-03-16T00:00:00\r2022-03-17T00:00:00 ジュリアの日付パッケージで最も役立つ部分だと言えるだろう。上のように特定の時点間の区間をネイティブのジュリア文法そのままでベクタ化すれば、まさに想像していた通りの結果が出る。作るのは二の次で、同じ機能を持つ関数が他の言語にもあるかもしれないが、この程度に文法にうまく溶け込んで、類まれな直感性を持つことは稀だろう。\n日付の引き算 - julia\u0026gt; Dates.Day(일주일뒤까지[end]) - Dates.Day(오늘)\r7 days 当たり前のように、引き算で2つの時点の間隔を計算できる。Dates.canonicalize() を使えば、時間、分、秒単位できれいに表示できる。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/stdlib/Dates/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2410,"permalink":"https://freshrimpsushi.github.io/jp/posts/2410/","tags":null,"title":"ジュリアでの日付と時刻関連関数の使用方法"},{"categories":"줄리아","contents":"概要 1 2 The Fastest Fourier Transform in the West(FFTW)は、マサチューセッツ工科大学(MIT)のMatteo FrigoとSteven G. Johnsonによって開発された、離散フーリエ変換を計算するためのソフトウェアライブラリです。FFTの実装用にAbstractFFTs.jlというパッケージがありますが、これは自体を直接使用するためではなく、FFTW.jlなどの高速フーリエ変換を実装するときに支援するために作られたものです。\nこのパッケージは主に直接使われることを意図していません。代わりに、FFTs（例えばFFTW.jlやFastTransforms.jl）を実装するパッケージの開発者がAbstractFFTsで定義された型/関数を拡張します。これにより、同じ基本的なfft(x)とplan_fft(x)インターフェイスを持つ複数のFFTパッケージが共存できます。3\n概要 フーリエ変換: fft() $2$次元配列で列ごとに変換: fft( ,[1]) $2$次元配列で行ごとに変換: fft( ,[2]) 配列の特定の次元だけを変換: fft( ,[n₁, n₂, ...]) フーリエ逆変換: ifft() $0$ 中央の周波数: fftshift() 逆変換: ifftshift() 周波数サンプリング: fftfreq(n, fs=1) コード フーリエ変換 Juliaでは、フーリエ変換の表記として$\\mathcal{F}[f]$, $\\hat{f}$が直接コードに使われます。周波数が$100$, $200$, $350$のサイン波を$1/1000$間隔でサンプリングし、これを加算しましょう。\nusing FFTW\rusing Plots\rusing LaTeXStrings\rFs = 1000 #진동수\rT = 1/1000 #샘플링 간격\rL = 1000 #신호의 길이\rx = [i for i in 0:L-1].*T #신호의 도메인\rf₁ = sin.(2π*100*x) #진동수가 100인 사인파\rf₂ = 0.5sin.(2π*200*x) #진동수가 100인 사인파\rf₃ = 2sin.(2π*350*x) #진동수가 100인 사인파\rf = f₁ + f₂ + f₃ フーリエ変換: fft() フーリエ逆変換: ifft() 定義によると、$f$のフーリエ変換$\\mathcal{F}f$は$50$, $100$, $200$でのみ非ゼロ値を持ちます。また、離散フーリエ変換の定義により、$y$軸の周りに対称な値を得ますが、基本的には周波数が$0$の値が最初の値になっています。したがって、信号の周波数と振幅を確認することが目的なら、前半だけをプロットしてもよいでしょう。\nFs = 1000 # 샘플링 주파수\rℱf = fft(f) # 푸리에 변환\rξ = Fs*[i for i in 0:L/2-1]/L #주파수 도메인(절반)\rplot(ξ, abs.(ℱf[1:Int(L/2)])*2/L, title=L\u0026#34;Fourier transform of ▷eq10◁\u0026#34;, label=\u0026#34;\u0026#34;) xlabel!(\u0026#34;frequency\u0026#34;)\rylabel!(\u0026#34;amplitude\u0026#34;)\rsavefig(\u0026#34;fft.png\u0026#34;) $0$ 中央の周波数 フーリエ変換の出力は、基本的に周波数が$0$の値を最初に置きます。周波数が$0$の値を中央にしたい場合は、fftshift()を使います。これを元に戻す場合はifftshift()を使いますが、これはifft+shiftではなく、逆変換 + fftshift、つまりfftshift()の逆操作であるため、混乱しないようにしましょう。\np1 = plot(ξ, abs.(ℱf), title=L\u0026#34;▷eq11◁\u0026#34;, label=\u0026#34;\u0026#34;, yticks=[], xticks=[0, 100, 200, 350, 500, 1000]) p2 = plot(ξ.-500, abs.(fftshift(ℱf)), title=L\u0026#34;▷eq22◁\u0026#34;, label=\u0026#34;\u0026#34;, yticks=[], xticks=[-500,-350,-200,-100,0,100,200,350,500]) plot(p1, p2, size=(800,400))\rsavefig(\u0026#34;fftshift.png\u0026#34;) 高次元フーリエ変換 2次元フーリエ変換の値と比較するため、まず$x = [1\\ 2\\ 3\\ 4]^{T}$のフーリエ変換値を計算しておきましょう。\njulia\u0026gt; x = [1.0; 2; 3; 4]\r4-element Vector{Float64}:\r1.0\r2.0\r3.0\r4.0\rjulia\u0026gt; fft(x)\r4-element Vector{ComplexF64}:\r10.0 + 0.0im\r-2.0 + 2.0im\r-2.0 + 0.0im\r-2.0 - 2.0im fft()は2次元配列を入力として受け取ると自動的に2次元フーリエ変換を返します。または、fft(, [1,2])は、第一および第二次元での変換を計算するという意味で、同じ結果を返します。\njulia\u0026gt; y = [x x x x]\r4×4 Matrix{Float64}:\r1.0 1.0 1.0 1.0\r2.0 2.0 2.0 2.0\r3.0 3.0 3.0 3.0\r4.0 4.0 4.0 4.0\rjulia\u0026gt; fft(y)\r4×4 Matrix{ComplexF64}:\r40.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0-8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\rjulia\u0026gt; fft(y, [1,2])\r4×4 Matrix{ComplexF64}:\r40.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0-8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im したがって、各列で変換を行いたい場合は、fft(, [1])を使い、各行で変換を行いたい場合は、fft(, [2])を使います。\njulia\u0026gt; fft(y, [1])\r4×4 Matrix{ComplexF64}:\r10.0+0.0im 10.0+0.0im 10.0+0.0im 10.0+0.0im\r-2.0+2.0im -2.0+2.0im -2.0+2.0im -2.0+2.0im\r-2.0+0.0im -2.0+0.0im -2.0+0.0im -2.0+0.0im\r-2.0-2.0im -2.0-2.0im -2.0-2.0im -2.0-2.0im\rjulia\u0026gt; fft(y, [2])\r4×4 Matrix{ComplexF64}:\r4.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r12.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r16.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im 周波数サンプリング fftfreq(n, fs=1) 長さが$n$で、間隔が$fs/n$の周波数ドメインを返します。既に説明したように、フーリエ変換は最初に周波数$0$の値を置くため、fftfreq()でサンプリングした周波数の最初の値は$0$です。前半は正の周波数、後半は負の周波数です。したがって、fftshift()を使うと、インデックスに応じて昇順に並べ替えられます。\njulia\u0026gt; fftfreq(4, 1)\r4-element Frequencies{Float64}:\r0.0\r0.25\r-0.5\r-0.25\rjulia\u0026gt; fftfreq(5, 1)\r5-element Frequencies{Float64}:\r0.0\r0.2\r0.4\r-0.4\r-0.2\rjulia\u0026gt; fftshift(fftfreq(4, 1))\r-0.5:0.25:0.25 環境 OS: Windows11 Version: Julia 1.8.2, FFTW 1.5.0 http://www.fftw.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaMath/FFTW.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaMath/AbstractFFTs.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3440,"permalink":"https://freshrimpsushi.github.io/jp/posts/3440/","tags":null,"title":"ジュリアで高速フーリエ変換（FFT）を使用する方法"},{"categories":"줄리아","contents":"概要 機械学習のような分野では、計算速度の向上やメモリの節約などのために、64ビットの実数ではなく32ビットの実数が配列のデータ型として使われます。そのため、PyTorchではテンソルを作ると、基本的にテンソルのデータ型は32ビットの浮動小数点数になってます。Juliaの機械学習パッケージにはFlux.jlがあり、これで実装された人工ニューラルネットワークは、Juliaの基本配列を入力として受け取ります。テンソルのような別のデータ構造を使わないという点は利点と言えますが、データ型を手動でFloat32に設定しなければならない面倒くささもあります。以下で、デフォルトのデータ型を変更する方法を紹介します。\nコード1 ChangePrecision.jl @changeprecision マクロを使うと、begin ... endで囲まれたコード内でデフォルトのデータ型が変わります。\njulia\u0026gt; Pkg.add(\u0026#34;ChangePrecision\u0026#34;)\rjulia\u0026gt; using ChangePrecision\rjulia\u0026gt; rand(3)\r3-element Vector{Float64}:\r0.580516564576538\r0.33915094423556424\r0.3612907828959878\rjulia\u0026gt; @changeprecision Float32 begin\rrand(3)\rend\r3-element Vector{Float32}:\r0.0459705\r0.0033969283\r0.579983 環境 OS: Windows10 バージョン: Julia 1.8.2, ChangePrecision 1.0.0 https://stackoverflow.com/questions/68068823/how-to-change-default-float-to-float32-in-a-local-julia-environment\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3439,"permalink":"https://freshrimpsushi.github.io/jp/posts/3439/","tags":null,"title":"ジュリアで基本データ型を変更する方法"},{"categories":"머신러닝","contents":"定理 インプット集合Input Set $X \\ne \\emptyset$ と正定値カーネル $k: X \\times X \\to \\mathbb{R}$ が与えられているとする。学習データセットTraining Datasetを $$ D := \\left\\{ \\left( x_{i} , y_{i} \\right) \\right\\}_{i=1}^{m} \\subset X \\times \\mathbb{R} $$ とし、再生カーネルヒルベルト空間 $H_{k}$ のクラス $$ \\mathcal{F} := \\left\\{ f \\in \\mathbb{R}^{X} : f \\left( \\cdot \\right) = \\sum_{i=1}^{\\infty} \\beta_{i} k \\left( \\cdot , z_{i} \\right) \\land \\beta_{i} \\in \\mathbb{R} \\land z_{i} \\in X \\land \\left\\| f \\right\\| \u0026lt; \\infty \\right\\} \\subset H_{k} $$ を上記のように設定する。任意の目的関数 $c : \\left( D \\times \\mathbb{R} \\right) ^{m} \\to \\overline{\\mathbb{R}}$ と単調増加関数であるレギュライザーRegulizer $g : \\mathbb{R} \\to [0,\\infty)$ に対して、以下のように正則化された目的汎関数Regulized Objective Functional $L : \\mathcal{F} \\to \\overline{\\mathbb{R}}$ が定義されているとする。 $$ L (f) := c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) $$ ここで、$H_{k}$ のノルム $\\left\\| \\cdot \\right\\|$ は、$k$ の正定値性Positive Definitenessによって次のように与えられる。 $$ \\left\\| \\sum_{i=1}^{\\infty} \\beta_{i} k \\left( \\cdot , z_{i} \\right) \\right\\|^{2} := \\sum_{i=1}^{\\infty} \\sum_{j=1}^{\\infty} \\beta_{i} \\beta_{j} k \\left( z_{i} , z_{j} \\right) \\ge 0 $$\n$\\mathbb{R}$ は実数の集合であり、$\\overline{\\mathbb{R}}$ は無限大 $\\infty$ を含む拡張実数である。 $\\mathbb{R}^{X}$ は定義域が $X$ で値域が $\\mathbb{R}$ である関数を集めた関数空間である。 レギュライザーとは、データに対する過学習を防ぐためのペナルティPenalty関数である。 汎関数とは、ざっくり言えば関数自体をインプットとして受け取る関数のことである。 ノンパラメトリックNonparametric $L (f)$ を最小化する関数 $f \\in \\mathcal{F}$ は、ある $\\left\\{ \\alpha_{i} \\right\\}_{i=1}^{m} \\subset \\mathbb{R}$ に対して次のような形で表される。 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$\nセミパラメトリックSemiparametric $X$ で定義された実関数の集合が $\\left\\{ \\psi_{p} : X \\to \\mathbb{R} \\right\\}_{p=1}^{M}$ に対して行列 $\\left( \\psi_{p} \\left( x_{i} \\right) \\right)_{ip}$ のランクが $M$ であるとする。すると、$f \\in \\mathcal{F}$ と $h \\in \\span \\left\\{ \\psi_{p} \\right\\}$ に対して $$ c \\left( \\left( x_{1}, y_{1}, \\tilde{f} \\left( x_{1} \\right) \\right) , \\cdots , \\left( x_{m}, y_{m}, \\tilde{f} \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) $$ を最小化する $\\tilde{f} = f + h$ は、ある $\\left\\{ \\alpha_{i} \\right\\}_{i=1}^{m}, \\left\\{ \\beta_{p} \\right\\}_{p=1}^{M} \\subset \\mathbb{R}$ に対して次のような形で表される。 $$ \\tilde{f} (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) + \\sum_{p=1}^{M} \\beta_{p} \\psi_{p} (\\cdot) $$\n説明 可能であれば、以下の二つのポストを先に読むことをお勧めする:\n再生カーネルヒルベルト空間 サポートベクターマシン 表現者 表現者定理Representer Theoremは、古典的な機械学習、特にサポートベクターマシンの文脈で最も重要な定理の一つとして、与えられたデータに対して私たちが近似しようとしている目的関数 $f$ が適切なカーネル $k$ に対して $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$ のような形で表されるという強力な定理である。ここでカーネルのインプットの一つが $x_{i}$ で固定された関数 $$ k \\left( \\cdot , x_{i} \\right) = \\phi \\left( x_{i} \\right) (\\cdot)\\in H_{k} $$ を表現者Representerと呼ぶ。これにより、表現者定理は「再生カーネルヒルベルト空間で学習データに適合したFitted任意の関数は、表現者たちの有限な線形結合で表すことができる」と要約することができる。特に、非線形回帰のためのサポートベクターマシンのカーネルトリックは、これに正確に合致している。\nこれはディープラーニングとシーベンコの定理の関係に似ている。 データサイエンスの文脈では、表現者 $\\phi \\left( x_{i} \\right) (\\cdot)$ はフィーチャーマップFeature Mapとも呼ばれ、任意のデータ $X$ を私たちがその特徴Featureを知ることができるヒルベルト空間に移し、その有限な和で私たちが求める関数を表現できるということは、これまで私たちが学んできた多くの機械学習技術がなぜ機能してきたのかを正当化する。もちろん、数学的な保証がなく\nてもそれらの技術はそれ自体で有効であるが、表現者定理があることで、それらの技術が理論的な基盤を築くことになるという点で非常に重要である。\n目的関数とレギュライザー 定理のステートメントでは、目的関数 $c$ とレギュライザー $g$ を非常に一般的に定義しているが、実際には多くの場合、$c$ はデータと $f$ の適合度を測る平均残差二乗、つまり $$ c = {{ 1 } \\over { m }} \\sum_{i=1}^{n} \\left( y_{i} - f \\left( x_{i} \\right) \\right)^{2} $$ と見なし、$g$ は二乗セミノルムペナルティ $g \\left( \\left\\| f \\right\\| \\right) = \\lambda \\left\\| f \\right\\|^{2}$ と見なしても問題ない1。\n注意すべき点は、この式の形や単語の意味だけを見て目的関数とレギュライザーを区別してはいけないということである。表現者定理の最も代表的な応用がサポートベクターマシンであるが、ソフトマージンを許容するソフトマージンSVMで扱う最小化問題は次のようである。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\lambda \\sum_{k=1}^{n} \\xi_{k} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k} \\\\ \u0026amp; \\xi_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nここで、最適化自体だけを考えた場合、目的関数は実際には $$ {{ 1 } \\over { 2 \\lambda }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\sum_{k=1}^{n} \\xi_{k} $$ と変わらず、この場合はデータとの乖離を考える $\\sum_{k=1}^{n} \\xi_{k}$ が $c$ であり、サポートベクターマシンの超平面 $f (\\mathbf{x}) = \\mathbf{w}^{T} + b$ から導かれる ${{ 1 } \\over { 2 \\lambda }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2}$ が $g$ として読まれるべきである。この最適化の意味を数学に置くか機械学習に置くかによって混乱することがあるが、\n数学の色が強い人はSVMを「まず線形回帰を終えてから例外を設けるもの」と見なし、$\\left\\| \\mathbf{w} \\right\\|_{2}^{2}$ を最初に最小化しようとする一方で、 データサイエンスの色が強い人はSVMを「まずデータをうまく分類し、その中で最もマージンの大きな超平面を見つけるもの」と見なし、$\\sum_{k=1}^{n} \\xi_{k}$ を最初に最小化しようとするためである。 どちらの視点も十分に共感できるものであり、表現者定理の応用がSVMだけでないことを考えると、ここでは暗記術のようなものを探そうとせず、問題に応じて能動的に考え受け入れる必要がある。\n証明 2 参考文献にあるように、ノンパラメトリック表現者定理のみを証明する。\nPart 1. $f = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v$\n再生カーネルの定義: 関数 $k : X \\times X \\to \\mathbb{C}$ が以下の二つの条件を満たす場合、$H$ の再生カーネルReproducing Kernelと言う。\n(i): 表現者Representer: すべての $x \\in X$ に対して $$ k \\left( \\cdot , x \\right) \\in H $$ (ii) 再生性質Reproducing Property: すべての $x \\in X$ とすべての $f \\in H$ に対して $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) = \\delta_{x} (f) $$ 特に、すべての $x_{1} , x_{2} \\in X$ に対して以下が成り立つ。 $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; k \\left( \\cdot , x_{2} \\right), k \\left( \\cdot , x_{1} \\right) \\right\u0026gt; $$ 再生カーネル $k : X \\times X \\to \\mathbb{R}$ に対して $x \\mapsto k (\\cdot ,x)$ である(表現者)関数 $\\phi : X \\to \\mathbb{R}^{X}$ を定義する。$k$ は再生カーネルであるため、$x' \\in X$ で関数 $\\left( \\phi (x) \\right) (\\cdot)$ の関数値はすべての $x, x' \\in X$ に対して $$ \\left( \\phi (x) \\right) (x ') = k \\left( x' , x \\right) = \\left\u0026lt; \\phi \\left( x ' \\right) , \\phi (x) \\right\u0026gt; $$ である。ここで $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ は $H_{k}$ の内積である。与えられた $\\left\\{ x_{i} \\right\\}_{i=1}^{m}$ に対して、任意の関数 $f \\in \\mathcal{F}$ は $\\span \\left\\{ \\phi \\left( x_{i} \\right) \\right\\}_{i=1}^{m}$ の部分とそれに直交するすべての $j$ に対して $$ \\left\u0026lt; v , \\phi \\left( x_{j} \\right) \\right\u0026gt; = 0 $$ を満たす $v \\in \\mathcal{F}$ とある $\\left( \\alpha_{1} , \\cdots , \\alpha_{m} \\right) \\subset \\mathbb{R}^{m}$ に対して次のように表現できる。 $$ f = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v $$ ここで、 $$ \\begin{align*} L (f) :=\u0026amp; c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) \\\\ =\u0026amp; c + g \\end{align*} $$ で、$c$ は $v$ に依存しないこと、$v = 0$ のとき $f$ が $L(f)$ を最小化することを議論する。\nPart 2. $c$ と $v$ は独立である\n関数 $f = f(\\cdot)$ と再生カーネル $k \\left( \\cdot , x_{j} \\right)$ の内積は再生性質により $$ \\begin{align*} =\u0026amp; \\left\u0026lt; f , k \\left( \\cdot , x_{j} \\right) \\right\u0026gt; \\\\ f \\left( x_{j} \\right) =\u0026amp; \\left\u0026lt; \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v , \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\sum_{i=1}^{m} \\alpha_{i} \\left\u0026lt; \\phi \\left( x_{i} \\right) , \\phi \\left( x_{j} \\right) \\right\u0026gt; + 0 \\end{align*} $$ である。これは $v$ に独立であるため、$L (f) = c + g$ で学習データ $D$ と $f$ にのみ依存する $$ c = c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) $$ も $v$ に独立であることがわかる。\nPart 3. $g$ は $v = 0$ のとき最小化される\n(1): $v$ は $\\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right)$ に直交し、 (2): $g$ が単調関数であると仮定したため、 $$ \\begin{align*} \u0026amp; g \\left( \\left\\| f \\right\\| \\right) \\\\ =\u0026amp; g \\left( \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v \\right\\| \\right) \\\\ =\u0026amp; g \\left( \\sqrt{\\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v \\right\\|^{2} + \\left\\| v \\right\\|^{2}} \\right) \u0026amp; \\because (1) \\\\ \\ge\u0026amp; g \\left( \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) \\right\\| \\right) \u0026amp; \\because (2) \\end{align*} $$ を得る。明らかに $v = 0$ のとき等式が成立し、$g$ が最小化されるためには $v=0$ でなければならない。一方、Part 2で $v$ は $c$ に影響を与えることができないことが確認されたため、$v = 0$ としても問題なく、$L = c + g$ を最小化する関数 $f$ は次のような形で表現できる。 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$\n■\nWahba. (2019). Representer Theorem. https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchölkopf. (2001). A Generalized Representer Theorem. https://link.springer.com/chapter/10.1007/3-540-44581-1_27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2408,"permalink":"https://freshrimpsushi.github.io/jp/posts/2408/","tags":null,"title":"表現者の定理の証明"},{"categories":"머신러닝","contents":"定義 1 2 入力空間Input Space $X \\ne \\emptyset$ が定義域であり値域が複素数の集合 $\\mathbb{C}$ の写像 $f: X \\to \\mathbb{C}$ で構成される関数空間 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right) \\subset \\mathbb{C}^{X}$ がヒルベルト空間であるとする。\n再生核ヒルベルト空間 固定された一つのデータDatum $x \\in X$ に対して、関数 $f \\in H$ を取り出す汎関数 $\\delta_{x} : H \\to \\mathbb{C}$ を**$x$ における(ディラックの)評価汎関数**(Dirac) Evaluation Functional at $x$という。 $$ \\delta_{x} (f) := f (x) $$ 全ての $x \\in X$ において評価汎関数 $\\delta_{x}$ が連続である場合、$H$ を再生核ヒルベルト空間RKHS, Reproducing Kernel Hilbert Spaceと呼び、$H_{k}$ と表記することもある。 関数 $k : X \\times X \\to \\mathbb{C}$ が以下の二つの条件を満たす場合、$H$ の再生核Reproducing Kernelという。 (i): 表現者Representer: 全ての $x \\in X$ に対して $$ k \\left( \\cdot , x \\right) \\in H $$ (ii) 再生性質Reproducing Property: 全ての $x \\in X$ と全ての $f \\in H$ に対して $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) = \\delta_{x} (f) $$ 特に全ての $x_{1} , x_{2} \\in X$ に対して以下が成立する。 $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; k \\left( \\cdot , x_{2} \\right), k \\left( \\cdot , x_{1} \\right) \\right\u0026gt; $$ 正定値カーネル 入力空間 $X \\ne \\emptyset$ からヒルベルト空間 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ への写像 $\\phi : X \\to H$ を特徴写像Feature Mapと呼ぶ。この文脈では、$H$ を特徴空間Featureと呼ぶこともある。 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ の内積 $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; : H \\times H \\to \\mathbb{C}$ に対して、以下のように定義される関数 $k : X \\times X \\to \\mathbb{C}$ をカーネルKernelと呼ぶ。 $$ k \\left( x_{1} , x_{2} \\right) := \\left\u0026lt; \\phi \\left( x_{1} \\right) , \\phi \\left( x_{2} \\right) \\right\u0026gt; $$ $m$個のデータData $\\left\\{ x_{1} , \\cdots , x_{m} \\right\\} \\subset X$ に対して、以下のような行列 $K \\in \\mathbb{C}^{m \\times m}$ をカーネル $k$ のグラム行列Gram Matrixと呼ぶ。 $$ K := \\left( k \\left( x_{i} , x_{j} \\right) \\right)_{ij} $$ $k$ のグラム行列が正定値行列である場合、$k$ を正定値カーネルPositive Definite Kernelと呼ぶ。言い換えると、全ての $\\left\\{ c_{1} , \\cdots , c_{m} \\right\\} \\subset \\mathbb{C}$ に対して以下を満たすグラム行列を持つカーネル $k$ を正定値カーネルと呼ぶ。 $$ \\sum_{i=1}^{m} \\sum_{j=1}^{m} c_{i} \\bar{c_{j}} K_{ij} \\ge 0 $$ 説明 難しい内容だが、できるだけわかりやすく解説してみよう。\nデータ科学におけるヒルベルト空間の意味 ヒルベルト空間は内積が定義された完備空間である。通常、数学では内積とは何か特別な意味を持たせずに、単にいくつかの条件を満たす二変数スカラー関数として扱うが、機械学習の文脈では類似性の測定Measure of Similarityという概念として考えることができる。実際に、文書間の単語の頻度を比較するために使用されるコサイン類似度も内積を使用しており、別の例として三つのベクトル $$ A := \\left( 3, 0, 1 \\right) \\\\ B := \\left( 4, 1, 0 \\right) \\\\ C := \\left( 0, 2, 5 \\right) $$ がある場合、$A$ と $B$ が類似しており、$C$ とは異なると直感的に理解できる。しかし、これはまだ直感的な推論に過ぎず、内積を通して量化すると以下のようになる。 $$ A \\cdot B = 12 + 0 + 0 = 12 \\\\ A \\cdot C = 0 + 0 + 5 = 5 \\\\ B \\cdot C = 0 + 2 + 0 = 2 $$ 単に内積の絶対値が大きいか小さいかを見ただけでも、\u0026lsquo;見ればわかる\u0026rsquo;よりもはるかにデータをよく説明している。 定義で入力空間と呼んでいる $X$ には特に仮定がないことに注意する。実際のフィールドでは、どのような悪いデータを扱うか保証できない。例えば、$X$ が写真や文書データの場合、写真同士や文書同士を内積することは意味がない。 Q. $X$ が白黒写真の集合である場合、写真を行列と見なしてピクセルごとの値で内積を取れば良いのではないか？ A. それで良く、それが特徴写像 $\\phi : X \\to H$ である。この場合、$H$ は長方形 $[a,b] \\times [c,d]$ で定義された関数の空間となる。 このように考えると、カーネルの存在自体が既に\u0026rsquo;扱いにくいデータ\u0026rsquo;を私たちがよく知っている空間に持ち込むことと同じである。 上で述べた内積の意味が全て無意味であっても、内積空間ならばノルム空間であり距離空間であるため、私たちが常識的に存在すると考えるほとんどの仮定が成立する。内積 $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ によって導かれるノルム $$ \\left\\| f \\right\\| := \\sqrt{ \\left\u0026lt; f , f \\right\u0026gt; } $$ があり、ノルム $\\left\\| f \\right\\|$ によってメトリック $$ d (f,g) = \\left\\| f -g \\right\\| $$ が導かれる。 データ科学の観点からノルムはデータに対する量化そのものである。例えば、白黒写真の全てのピクセルの値を合計した値をノルムとする場合、単にこれだけで写真がどれだけ明るいか暗いかを大まかに評価できる。 データ科学の観点から距離は二つのデータがどれだけ異なるかを教えてくれる。正しいか間違っているか、同じか異なるかを区別することは言うまでもなく重要である。 これらの理由をすべて抜きにしても、数式を展開していくと内積が必要になる場合がある。関連する例をここにすべて書くと非常に散漫になるので省略する。\u0026lsquo;サポートベクターマシン\u0026rsquo;の投稿のカーネルトリックの節を参照。 なぜ関数空間なのか？ これほどまでに難しくなければならないのか？ 数学というものはほとんどの応用で「私たちが探している関数」を見つけることである。\n補間は与えられたデータの間を埋める多項式を見つけることである。 統計的回帰分析はデータを最もよく説明する直線を見つける技術である。その直線は線形関数である。 ディープラーニングはそれをうまくやれないので活性化関数などを投入して非線形関数を近似する技術である。 フーリエ変換は関数を三角関数の線形結合として表す変換である。 これらの例を一つ一つ挙げていくときりがない。再び機械学習に戻って、私たちが関数空間を考える理由は、私たちが探しているものが結局のところ関数だからである。私たちは、その形が明示的Explicitではないかもしれないが、私たちが興味を持っているものを入れるInputと、\n私たちが望む結果を出すReturn関数を求めている。例えば、数字が書かれた写真を入れたときにその数字を返す、個人情報を入れたときにローンを返済できる確率を計算するなどの関数である。このような役に立つ関数が単純であるはずがなく、それらを知っている関数たちの合成のようなものを探したいと思っている。想像してみてほしい。健康診断の結果データ $x$ を受け取り、どれだけ健康かを計算してくれる関数を $f$ とすると、 $$ f( \\cdot ) = \\sum_{k=1}^{m} \\alpha_{k} \\phi_{k} \\left( x_{k} \\right)(\\cdot) $$ のように有限個の $\\phi_{k} (x) (\\cdot)$ を基底Basisとして持つ $f$ を探しているのである。特に $\\phi (x) = k (\\cdot , x)$ に対して、ある $f$ を見つけることができるという命題がまさに表現者定理である。\n表現者定理: 再生核ヒルベルト空間内で学習データに適合したFitted任意の関数は、表現者たちの有限の線形結合で表すことができる。\n要するに、機械学習（特にサポートベクターマシンの文脈）で私たちが見つけたいものが結局は関数であるため、それらが存在する関数空間について探求することは避けられない。\nもちろん、数学的な証明がなければ動かないプログラムはこの世に存在しない。必然的な学問であるとしても、全員に必須というわけではない。数学専攻でなければ、非常に難しいことが普通であり、どうしても難しいと思う場合は大まかに読み飛ばしても良い。\n評価関数の前になぜディラックの名前がついているのか？ $$ \\delta_{x_{0}} (x) = \\begin{cases} 1 \u0026amp; , \\text{if } x = x_{0} \\\\ 0 \u0026amp; , \\text{if } x \\ne x_{0} \\end{cases} $$ 元々ディラックのデルタ関数は上記のように一点でのみ値を持つ関数として知られている。正確な定義や用途はともかく、その変形は一点でのみ$0$でないという点を保持すれば、大抵ディラックの名前がつく。この意味を理解するための例として、二つの関数 $f : \\mathbb{R} \\to \\mathbb{R}$, $\\delta_{x_{0}} : \\mathbb{R} \\to \\mathbb{R}$ とその内積として $$ \\left\u0026lt; f, \\delta_{x_{0}} \\right\u0026gt; = \\sum_{x \\in \\mathbb{R}} f (x) \\delta_{x_{0}} (x) = f \\left( x_{0} \\right) $$ を想像してみる。通常関数の内積には積分を行うが、和ではないことや全ての $x \\in \\mathbb{R}$ に対して加算することが危険であることは理解しているが、最終的には概念と感覚が一致する部分があることがわかる。\nこのセンスで、$\\delta_{x_{0}} (f)$ は上記の議論を隠して単にその結果である $x_{0}$ で評価された $f \\left( x_{0} \\right)$ を、\u0026rsquo;$x_{0}$ において一点だけを得る\u0026rsquo;関数としている。\n再生性質と呼ぶ理由 再生核ヒルベルト空間の定義を読むと非常に興味深い。通常、数学で「何かの空間」と言うと、その定義自体が「何か」が存在する空間としているが、RKHSは突然「評価汎関数が全ての点で連続である」というヒルベルト空間として定義されているためである。\nリース表現定理: $\\left( H, \\left\\langle \\cdot,\\cdot \\right\\rangle \\right)$がヒルベルト空間であるとする。$H$の線形汎関数 $f \\in H^{ \\ast }$と$\\mathbf{x} \\in H$ に対して $f ( \\mathbf{x} ) = \\left\\langle \\mathbf{x} , \\mathbf{w} \\right\\rangle$および$\\| f \\|_{H^{\\ast}} = \\| \\mathbf{w} \\|_{H}$ を満たす$\\mathbf{w} \\in H$ が一意に存在する。\nムーア-アロンサジン定理Moore-Aronsajn Theorem: 正定値カーネルが存在する場合、それに対応するRKHSが一意に存在する。\nこの定義によれば、RKHSに再生核が一意に存在するという命題さえ自明ではなく、実際にはリース表現定理によってRKHSに再生核が一意に存在することが保証される。興味深いことに、逆に再生核\nに対応するRKHSも一意に存在する。\nこれで、定義にある数式を一つ一つ詳しく見てみよう。\n元々$k : X \\times X \\to \\mathbb{C}$ において、関数$k$に入れることができるのは$x_{1}, x_{2} \\in X$だが、定義で述べたように$x$を一つ固定すると、$k$は実質的に$k : y \\mapsto k (y,x)$となる$k : X \\to \\mathbb{C}$となる。関数として扱う立場からは、片方の入力を塞いだものであり、 $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) $$ のような表現は、単に二つの関数$f (\\cdot) : X \\to \\mathbb{C}$と$k \\left( \\cdot , x \\right): X \\to \\mathbb{C}$を内積したものに過ぎない。「それがどうして$f$から出てきて、外にある内積が$x$とどう関連しているのか\u0026hellip;」と複雑に考える必要はない。$f(x) \\in \\mathbb{C}$も単に内積の結果であり、値域が複素数集合であるため、出てきた何らかの複素数に過ぎない。\nここで、再生再生, Reproducingという性質の命名について触れておきたい。Reproductionという単語自体が、その生成原理に従ってRe-(再び, 再) -produce(作る, 生)という意味を持ち、その最初の翻訳は繁殖/生殖、二番目の翻訳はコピー/複製、三番目の翻訳は再生である。繁殖は明らかに無意味であり、コピーと言うには元がない。\nしかし、$f(\\cdot)$と$k (\\cdot, x)$を内積したときに$f(x)$を得るということを、$f$が持っていた情報をカーネルによって「再生」したものと考えたらどうだろうか？私たちが時刻$t$に依存するYouTubeの動画$y(t)$という関数を持っていると想像してみよう。私たちは$y$そのものを見るのではなく、$t$が増加するにつれて再生される$\\left\\{ y(t) : t \\in [0,T] \\right\\}$自体を見ている。このような比喩から、カーネル$k$は$f$を関数そのものとしてではなく、関数値を再生してくれる「再生カーネル」と呼ばれる資格がある。\n特徴マップと不便な表記について カーネルと再生カーネルの定義をよく見ると、実際にはこれらは定義のために相互に必要としていないことがわかる。カーネルはカーネルであり、再生カーネルは再生カーネルであり、これらが一致するのは特徴マップFeature Mapが表現者Representerであるとき、つまり $$ \\phi (x) = k \\left( \\cdot , x \\right) $$ のときである。特徴マップはその名の通り、元のデータを私たちが扱いやすい形に変換してくれる変換であり、このような関数たちによって何らかの関数が表されるということは、その関数がデータから来る何らかの特徴Featureによって説明されるということと同じである。一つの問題は、ここまで直感的に何となく理解できたとしても、依然として$k \\left( \\cdot , x \\right)$のような表記が不便であり、特徴マップではなく内積から始まって別々に定義されるカーネルの動機Motiveに共感するのが難しいということである。\n特徴マップは$\\phi : X \\to H$であるため、その関数値は$x \\in X$に対応する何らかの関数$\\lambda : X \\to \\mathbb{C}$であり、これが通常混乱を招くものではない。$\\phi (x)$をもっと正確に書くと $$ \\left( \\phi (x) \\right) (\\cdot) = k \\left( \\cdot , x \\right) $$ であり、なぜこのようにしてまで点$\\cdot$を保持し、不便な表記を使用するのか疑問に思うかもしれない。ほとんどの人間は、このようにしなかった場合にもっと苦労する例を見ると、理解しやすくなる。前述したように、カーネルであれ再生カーネルであれ、結局私たちが一貫して関心を持っている空間は関数空間$H$であり、$H$の内積は関数の内積である。まず、ある関数$f$がデータ$\\left\\{ x_{i} \\right\\}_{i=1}^{m}$の表現者$\\phi \\left( x_{i} \\right)$たちの線形結合として表されるとすると $$ f (y) = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) (y) = \\sum_{i=1}^{m} \\alpha_{i} \\left( \\phi \\left( x_{i} \\right) \\right) (y) $$ となり、すでにかなり複\n雑になっていることがわかる。これに新しい関数$g$とデータ$\\left\\{ x'_{j} \\right\\}_{j=1}^{n}$を考えると $$ g (y) = \\sum_{j=1}^{n} \\beta_{j} \\left( \\phi \\left( x'_{j} \\right) \\right) (y) $$ となる。一方で、$f$と$g$の内積を使わないのであれば、内積空間を考える理由がないが、$\\left\u0026lt; f,g \\right\u0026gt;$を書くと $$ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} \\left\u0026lt; \\phi \\left( x_{i} \\right) , \\phi \\left( x'_{j} \\right) \\right\u0026gt; $$ となり、余計なものが多くなる。内積をする前には、いずれにせよ関数空間を扱う上で$y \\in X$を実際に扱うことはほとんどなく、内積をした後には、既に知っている$\\phi$と内積を続けて書く必要がある。これを見ると、 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) \\\\ g (\\cdot) = \\sum_{j=1}^{n} \\beta_{j} k \\left( \\cdot , x'_{j} \\right) \\\\ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} k \\left( x_{i} , x'_{j} \\right) $$ のような表記が面倒であるだけではないことに気がつくかもしれない。\n再生カーネルは正定値である データ$\\left\\{ x_{k} \\right\\}_{k=1}^{m}$が与えられたとすると、$k$がカーネルである場合、以下が成立する。 $$ \\begin{align*} \u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\bar{\\alpha_{i}} \\alpha_{j} k \\left( x_{i} , x_{j} \\right) \\\\ =\u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\left\u0026lt; \\alpha_{i} \\phi \\left( x_{i} \\right) , \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\u0026lt; \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) , \\sum_{j=1}^{m} \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) \\right\\|^{2} \\\\ \\ge \u0026amp; 0 \\end{align*} $$ 前述したように$\\phi : x \\mapsto k (\\cdot , x)$とすると、再生カーネル$k$はカーネルであるため正定値である。このようなカーネルの正定値性は、カーネルに関連するさまざまな性質で自然に現れる。\n関数解析以外のカーネル (1) 通常、数学でカーネルと言えば抽象代数のカーネル$\\ker$を指す。代数構造$Y$で$0$が定義されている場合、関数$f : X \\to Y$に対して$\\ker f := f^{-1} \\left( \\left\\{ 0 \\right\\} \\right)$を$f$のカーネルと言う。 (2) この概念が線形代数で特殊化されたものが線形変換のカーネルである。 カーネルが難しいと感じる場合、関数解析を専攻していない数学者にいきなりカーネルについて聞いても、十中八九(1)の意味で理解するだろう。あなたのバックグラウンドが数学に基づいているならば、当然(1)くらいは知っている必要があり、そうでなくても(2)くらいは知っているべきである。\n名前のついたカーネル 機械学習の文脈では、以下のようなカーネルが知られている。3 これらは一見カーネルのように見えないかもしれないが、カーネルの和と積が依然としてカーネルであるという事実を通じて導かれる。\nリニアカーネル: $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; $$ ポリノミアルカーネル: $c \\ge 0$ と $d \\in \\mathbb{N}$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\left( \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + c \\right) ^{d} $$ ガウシアンカーネル: $\\sigma^{2} \u0026gt; 0$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\exp \\left( - {{ \\left\\| x_{1} - x_{2} \\right\\| } \\over { 2 \\sigma^{2} }} \\right) $$ シグモイドカーネル: $w, b \\in \\mathbb{C}$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\tanh \\left( w \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + b \\right) $$ Sejdinovic, Gretton. (2014). What is an RKHS?: p7~11. http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchölkopf. (2001). A Generalized Representer Theorem. https://link.springer.com/chapter/10.1007/3-540-44581-1_27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJakkula. (2006). Tutorial on Support Vector Machine (SVM). https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2406,"permalink":"https://freshrimpsushi.github.io/jp/posts/2406/","tags":null,"title":"機械学習における政府号カーネルと再生カーネルのヒルベルト空間"},{"categories":"머신러닝","contents":"モデル 1 簡単な定義 二値分類Binary Classificationが可能なデータを最もよく区別する直線や平面を見つける方法をサポートベクターマシンという。\n難しい定義 内積空間 $X = \\mathbb{R}^{p}$ とラベリングLabeling $Y = \\left\\{ -1, +1 \\right\\}$ に対し、$n$ 個のデータを集めた学習データセットTraining Datasetを $D = \\left\\{ \\left( \\mathbf{x}_{k} , y_{k} \\right) \\right\\}_{k=1}^{n} \\subset X \\times Y$ とし、 $$ \\begin{align*} X^{+} :=\u0026amp; \\left\\{ \\mathbf{x}_{k} \\in X : y_{k} = +1 \\right\\} \\\\ X^{-} :=\u0026amp; \\left\\{ \\mathbf{x}_{k} \\in X : y_{k} = -1 \\right\\} \\end{align*} $$\nとする。あるウェイトWeight $\\mathbf{w} \\in \\mathbb{R}^{p}$ とバイアスBias $b \\in \\mathbb{R}$ を持つ線形関数 $f \\left( \\mathbf{x} \\right) = \\mathbf{w}^{T} \\mathbf{x} + b$ によって作られる超平面を $H : \\mathbf{w}^{T} \\mathbf{x} + b = 0$ とするとき、$H$ と最も距離が近い $\\mathbf{x}^{+} \\in X^{+}$ と $\\mathbf{x}^{-} \\in X^{-}$ をサポートベクターSupport Vectorといい、これらの間の距離 $\\delta$ をマージンMarginという。これに対して $$ \\begin{align*} f \\left( \\mathbf{x}^{+} \\right) =\u0026amp; +1 \\\\ f \\left( \\mathbf{x}^{-} \\right) =\u0026amp; -1 \\end{align*} $$\nを満たしながらマージンが最大になるような $\\mathbf{w} , b$ を見つける機械学習技術をサポートベクターマシンSVM, Support Vector Machineという。\n$\\mathbb{R}$ は実数の集合であり、$\\mathbb{R}^{p}$ は$p$次元ユークリッド空間である。 $X \\times Y$ は二つの集合のデカルト積を意味する。 $\\mathbf{w}^{T}$ は $\\mathbf{w}$ の転置行列であり、$\\mathbf{w}^{T} \\mathbf{x}$ は二つのベクトル $\\mathbf{w}, \\mathbf{x}$ の内積 $\\left\u0026lt; \\mathbf{w} , \\mathbf{x} \\right\u0026gt;$ である。 説明 簡単に言えば、次の図のようにオレンジ色と空色のデータを二分する線や平面を見つけることである。平面図では赤い矢印で示されているのがサポートベクターに該当する。\n図では $2$次元なので線を見つけ、$3$次元なので平面を見つけたが、さらに大きな$p$次元になると超平面を見つけなければならず、図示するのは難しくなる。しかし、このように空間を二つに分けるという点は変わらない。学習データセットで二値分類が完了すれば、新しいデータを受け取ったときも$f$ に入れて線形分類器Linear Classifierとして使えばよい。\n当然ながら、同じデータを二値分類しても、左側が右側よりも良い。右側の場合、空色のデータに対するマージンが過度である。具体的にこれを求める方法は、いずれにせよパッケージがすべて自動で処理するため、知らなくてもよい。\n学部生レベルであれば、ここまでの簡単な定義を受け入れて図で大まかに理解するだけでも、今後実際に使用する際や用語を理解する上で大きな問題はない。これより少し難しい内容、実践的な要点の要約、Pythonの例示コードなどは、国内のウェブでもよく整理された文書がたくさんある。 2 3 4\n内積空間 ご覧の通り、SVM自体は概念的にそれほど難しくはないが、数学的な定義を引き出し数式を記述した理由は、今後具体的に、理論的に話すことが多いためである。\nユークリッド空間 $\\mathbb{R}^{p}$ はもちろんベクトル空間であり、内積空間でもあり、内積空間は距離空間であるため距離空間でもある。これを強調するのは、実際のデータの世界で内積空間というのが思ったよりも良い仮定であるためである。例えば、画像や文書、分子構造などをSVMにそのまま入れてもいいのか、頭を悩ませることになる。定義では暗黙のうちに「距離が近い」やベクトルの内積が含まれる線形関数 $f$ を使用しているが、理論に近づくほど、これらの仮定を当然とすることはできない。\nサポートベクター 元々このような幾何問題では、境界Boundary上にあるものをサポートと呼ぶが、例えば最小包含円問題でも円を決定する円周上の点をサポートとする。SVMの起源となったサポートベクターも同様で、$\\mathbf{x}^{+}, \\mathbf{x}^{-}$ は二つの集合 $X^{+}, X^{-}$ の観点から見ても、$\\delta/2$ から$X^{+}, X^{-}$ の距離に位置する境界上にある。\nサポートベクターが$H$ それぞれで一意である保証はないが、今後の議論で一意性が重要ではないため、一般性を失わずに一意であると仮定しよう。$\\left\\{ \\mathbf{x}_{k} \\right\\}_{k=1}^{n}$ のマージンにはデータが存在せず、 $$ f \\left( \\mathbf{x} \\right) \\begin{cases} \\ge +1 \u0026amp; , \\text{if } \\mathbf{x} \\in X^{+} \\\\ \\le -1 \u0026amp; , \\text{if } \\mathbf{x} \\in X^{-} \\end{cases} $$ であるため、すべての$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ に対して$H$ でなければならない。\nマージンの最大化 サポートベクターは$H$ と最も近い点であるため、$\\delta/2$ との距離$H$ はサポートベクターが$\\mathbf{w}$ 方向に垂直に離れたときの距離である。このマージンは$\\mathbf{x}^{+}$ でも$\\mathbf{x}^{-}$ でも同じであり、両方とも超平面$H$ との距離が$\\delta/2$ であることは、二つのサポートベクター間の距離が $$ \\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-} $$ として表されることを意味する。ここで$\\mathbf{x}^{+} - \\mathbf{x}^{-}$ のような演算は$X$ がベクトル空間であるという仮定に基づいて許可される。$\\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-}$ の両辺に$\\mathbf{w}$ と内積を取ると、つまり$\\mathbf{w}^{T}$ を左側に掛けると$f$ の定義に従って $$ \\begin{align*} \u0026amp; \\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-} \\\\ \\implies \u0026amp; \\delta \\mathbf{w}^{T} \\mathbf{w} = \\mathbf{w}^{T} \\mathbf{x}^{+} - \\mathbf{w}^{T} \\mathbf{x}^{-} \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = \\left( \\mathbf{w}^{T} \\mathbf{x}^{+} + b \\right) - \\left( \\mathbf{w}^{T} \\mathbf{x}^{-} + b \\right) \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = +1 - (-1) \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = 2 \\\\ \\implies \u0026amp; \\delta = {{ 2 } \\over { \\left\\| \\mathbf{w} \\right\\|_{2}^{2} }} \\end{align*} $$ を得る。つまり、マージンを最大化することは目的関数 $\\left\\| \\mathbf{w} \\right\\|_{2}^{2} / 2$ を最小化することであり、要約するとSVMとは次のような最適化問題を解くオプティマイザーOptimizerである。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 \\end{matrix} \\\\ k = 1, \\cdots , n $$\n派生モデル 難しい定義に従えば、SVMは直線であれ超平面であれ、いずれにしても線形関数を見つける線形回帰モデルであるが、当然ながらここで満足するわけがない。\nソフトマージンSVM 例えば、次のようなデータが入ってきたとしよう。SVMはデータが混在している中央部分のために、これを完全に二値分類することができない。\nここで、サポートベクターのマージンにデータが存在できないという制約のもとで$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ という条件を満たさなければならなかったことに注目してみよう。この不等式を$1$ より小さい値に許容すれば、完全な二値分類ではないにしても、完全に諦めるよりは良い結果をもたらすだろう。そして、この許容を各データごとに$\\xi_{k} \\ge 0$ とすると、新たな制約条件$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k}$ を得る。このように条件が緩和されたマージンをソフトマージンSoft Marginという。\nもちろん、制約が少し緩和されたとはいえ、すべてを$\\xi_{l} = \\cdots = \\xi_{n} = 1$ にしてしまうとSVM自体を放棄してしまうことになる。これを防ぐためには、目的関数に$\\sum_{k} \\xi_{k}$ のような項を加えることがある。これは不可能な二値分類を可能にしたことに対する代償Penaltyである。もちろん、このような単純なペナルティはデータのスケールによっては全く意味がなかったり、逆に過敏に反応したりするため、$0 \\le \\sum_{k} \\xi_{k} \\le n$ そのままではなく、適切な正の数$\\lambda \u0026gt; 0$ を掛けて追加することにしよう。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\lambda \\sum_{k=1}^{n} \\xi_{k} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k} \\\\ \u0026amp; \\xi_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nカーネルトリック 例えば、上のようなデータが与えられた場合、ソフトマージンであろうとなかろうと、SVMでは決して二値分類することができないように見える。しかし、よく見ると$0$ に近い側には空色の点が集まっており、外側にはオレンジ色の点が現れていることが明らかである。この情報を活用するために、次のように$z$ 軸を新たに作ってみよう。 $$ \\phi (x,y) := (x,y, x^{2} + y^{2}) $$\n上の図は、下の図を適切にキャプチャしたものである。下はマウスで対話可能な3D空間なので、いろいろと回して見てみてください。\n元の$\\mathbb{R}^{2}$ ではデータを二分する直線を見つけるのが難しかったが、このようにデータを説明する次元を増やした$\\mathbb{R}^{3}$ では、適切な平面でデータを分類するSVMを使用できるようになった。ここで自然に思い浮かぶ疑問は、「それでは、このように便利な変換$\\phi$ をカーネルKernelと呼び、カーネルを使用する方法をカーネルトリックKernel Trickと呼ぶのか？」ということである。半分正しく、半分間違っている。$\\phi$ にさらに一歩進んで、内積まで含まれたものがカーネルである。\n再びマージンの最大化に戻って、我々に与えられた最適化問題を再検討してみよう。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 \\end{matrix} \\\\ k = 1, \\cdots , n $$\n制約条件$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ は見た目はすっきりしているが、実際にこの問題を解く際にはあまり役に立たない。元の学習データセットでの形に戻すと、$k = 1 , \\cdots , n$ に対して $$ \\begin{cases} f \\left( \\mathbf{x}_{k} \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = 1 \\\\ f \\left( \\mathbf{x}_{k} \\right) \\le -1 \u0026amp; , \\text{if } y_{k} = -1 \\end{cases} \\\\ \\implies \\begin{cases} y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = 1 \\\\ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = -1 \\end{cases} \\\\ \\implies y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 $$ でなければならない。このような制約条件自体を目的関数に反映させて、制約条件がないかのように扱う方法がラグランジュ乗数法である。$y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\ge 0$ に$\\alpha_{k} \\ge 0$ を掛けた項を元の目的関数から引いた$L(\\mathbf{w}, b)$ に対して、次の最適化問題を得る。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} - \\sum_{k=1}^{n} \\alpha_{k} \\left[ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\right] \\\\ \\text{subject to} \u0026amp; \\alpha_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\n再び強調するが、我々の目的はこの目的関数を最小化する$\\mathbf{w}, b$ を見つけることであった。$\\mathbf{w}, b$ に対する目的関数の偏微分が$0$ となる条件は次の通りである。 $$ \\begin{align*} {{ \\partial L } \\over { \\partial \\mathbf{w} }} = 0 \\implies \u0026amp; \\mathbf{w} = \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{x}_{k} \\\\ {{ \\partial L } \\over { \\partial b }} = 0 \\implies \u0026amp; 0 = \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\end{align*} $$\nこれをそのまま$L$ に代入してみると $$ \\begin{align*} \u0026amp; L(\\mathbf{w},b) \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} - \\sum_{k=1}^{n} \\alpha_{k} \\left[ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\right] \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\mathbf{w}^{T} \\mathbf{w} - \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) + \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\mathbf{w}^{T} \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{x}_{k} - \\sum_{k=1}^{n} \\alpha_{k} y_{k}\\mathbf{w}^{T} \\mathbf{x}_{k} - b \\sum_{k=1}^{n} \\alpha_{k} y_{k} - \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; - {{ 1 } \\over { 2 }} \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{w}^{T} \\mathbf{x}_{k} - b \\cdot 0 + \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} y_{i} a_{j} y_{j} \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} \\\\ =\u0026amp; \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} a_{j} y_{i} y_{j} \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} \\\\ =\u0026amp; L \\left( \\alpha_{1} , \\cdots , \\alpha_{n} \\right) \\end{align*} $$\nを得る。当然ながら、具体的な$\\mathbf{w}$ と$b$ を計算するためには、学習データ$\\left\\{ \\left( \\mathbf{x}_{k}, y_{k} \\right) \\right\\}_{k=1}^{n}$ が必要である。\nここで注目すべき点は、数式で$\\mathbf{x}_{i}$ と$\\mathbf{x}_{j}$ の内積が使用されていることである。結局のところ、最終的に、我々は内積を取らなければならず、$X$ が内積空間でなければ、このように順調に進む保証はない。逆に言えば、$X$ が内積空間でなくても、変換$\\phi$ が$X$ を内積空間に送ることができれば、その目的関数が $$ \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} a_{j} y_{i} y_{j} \\phi \\left( \\mathbf{x}_{i} \\right) ^{T} \\phi \\left( \\mathbf{x}_{j} \\right) $$ であるSVMを検討する価値がある。機械学習では、このように二つのベクトルに対する変換、内積まで含まれた関数 $$ K \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right) := \\left\u0026lt; \\phi \\left( \\mathbf{x}_{i} \\right) , \\phi \\left( \\mathbf{x}_{j} \\right) \\right\u0026gt; $$ をカーネルKernelと呼ぶこともある。[ 注: データサイエンスでは、これと混同される別のカーネルも存在する。元々、数学全般でのカーネルは、名前は同じでも全く異なる機能の関数である。 ]\n数式的にここまでの内容を受け入れることができれば、なぜカーネルではなく変換$\\phi$ を導入することをカーネルトリックと呼び、変換後に内積空間であることが保証されることが重要なのかを理解したことになる。\n条件を満たす限り、カーネルはいくつかの種類を考えることができる。特に元のSVMも線形カーネルLinear Kernel $$ K \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right) = \\left\u0026lt; \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right\u0026gt;^{1} = \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} $$ を使用したものと見なすことができる。\n参照 カーネルトリックの部分で数学的に簡単な内容を扱ったが、より深い理論に興味がある場合は、SVMを超えて以下の内容を学ぶことをお勧めする。\n機械学習におけるカーネルと再生カーネルヒルベルト空間 表現者定理の証明 コード 以下はカーネルトリックを実装したJuliaのコードである。\nstruct Sphere\rd::Int64\rend\rSphere(d) = Sphere(d)\rimport Base.rand\rfunction rand(Topology::Sphere, n::Int64)\rdirection = randn(Topology.d, n)\rboundary = direction ./ sqrt.(sum(abs2, direction, dims = 1))\rreturn boundary\rend\rusing Plots\rA = 0.3rand(Sphere(2), 200) + 0.1randn(2, 200)\rB = rand(Sphere(2), 200) + 0.1randn(2, 200)\rscatter(A[1,:],A[2,:], ratio = :equal, label = \u0026#34;+1\u0026#34;)\rscatter!(B[1,:],B[2,:], ratio = :equal, label = \u0026#34;-1\u0026#34;)\rpng(\u0026#34;raw.png\u0026#34;)\rPlots.plotly()\rϕ(z) = z[1]^2 + z[2]^2\rscatter(A[1,:],A[2,:],ϕ.(eachcol(A)), ms = 1, label = \u0026#34;+1\u0026#34;)\rscatter!(B[1,:],B[2,:],ϕ.(eachcol(B)), ms = 1, label = \u0026#34;-1\u0026#34;)\rsavefig(\u0026#34;kernel.html\u0026#34;) Jakkula. (2006). サポートベクターマシン（SVM）に関するチュートリアル. https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ratsgo.github.io/machine%20learning/2017/05/23/SVM/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-2%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0-SVM\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://hleecaster.com/ml-svm-concept/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2402,"permalink":"https://freshrimpsushi.github.io/jp/posts/2402/","tags":null,"title":"サポートベクターマシン"},{"categories":"위상데이터분석","contents":"概要 代数位相Algebraic Topologyにおいて、幾何学的な意味を考えずに単に定義だけを述べると、ベッチ数Betti Numberとは、単にチェインコンプレックスでのホモロジーグループのランクに過ぎない。問題は、このような説明がベッチ数の意味を知りたい人にとって全く役に立たず、その具体的な計算も難解であり、例を通して学ぶことも困難であることである。\nこの投稿では、少なくとも2つ目の質問に対する答え―ベッチ数をどのように計算するかについての整理とその詳細な証明を紹介する。以下に紹介される定理によれば、与えられたチェインコンプレックスに従ってある行列を見つけることができ、それに関する一連の計算プロセスを通じて、以下のような明示的Explicitな公式を導出することができる。 $$ \\beta_{p} = \\rank ?_{1} - \\rank ?_{2} $$\n本来、数学的な内容は数学を使わずに伝えられることが最も良い説明であるが、ベッチ数の場合は、その公式の導出過程の中でその根本的な原理を理解することができると考えられる。学部生程度では証明の難易度がかなり高く、追いかけるのが難しいかもしれないが、できるだけ省略せずに詳細に書いたので、少なくとも一度は試みることをお勧めする。\n定理 ホモロジーグループの定義:\n$n \\in \\mathbb{N}_{0}$ とする。アーベル群 $C_{n}$ と ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェイン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ これがすべての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、$\\mathcal{C} := \\left\\{ \\left( C_{n}, \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ をチェインコンプレックスChain Complexという。 商群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の**$n$番目のホモロジーグループ**$n$-th Homology Groupという。 ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界Boundaryまたは微分Differentialオペレーターという。 $Z_{n} := \\ker \\partial_{n}$ の要素を**$n$-サイクル**Cycles、$B_{n} := \\text{Im} \\partial_{n+1}$ の要素を**$n$-境界**Boundaryという。 フリーチェインコンプレックスの標準基底分解 チェインコンプレックス $\\mathcal{C} := \\left\\{ \\left( C_{p}, \\partial_{p} \\right) \\right\\}$ のすべての $C_{p}$ が有限ランクのフリーグループであるとする。するとすべての $p$ と $Z_{p} := \\ker \\partial_{p}$ に対して、次を満たす部分群 $U_{p}, V_{p}, W_{p} \\subset C_{p}$ と が存在する。 $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ =\u0026amp; U_{p} \\oplus Z_{p} \\end{align*} $$ $$ \\begin{align*} \\partial_{p} \\left( U_{p} \\right) \\subset \u0026amp; W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$ もちろん、$Z_{p}$ は $\\partial_{p}$ の核であるため、$\\partial_{p} \\left( V_{p} \\right) = 0$ であり、$\\partial_{p} \\left( W_{p} \\right) = 0$ である。さらに、$U_{p}$ での $\\partial_{p}$ の制限関数 ${\\partial_{p}}_{| U_{p}} : U_{p} \\to W_{p-1}$ は、次のような形のスミス標準形を持つ。 $$ \\begin{bmatrix} b_{1} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; \\cdots \u0026amp; b_{l} \\end{bmatrix} $$ ここで、$b_{i} \\in \\mathbb{N}$ であり、$b_{1} \\mid \\cdots \\mid b_{l}$ である。\nホモロジーグループの効率的な計算可能性 1 $H_{p} \\left( \\mathcal{C} \\right)$ のベッチ数を$\\mathcal{C}$ の$p$番目のベッチ数Betti Numberという。有限コンプレックス$K$ の$\\beta_{p}$ は次のようである。 $$ \\beta_{p} = \\rank Z_{p} - \\rank B_{p} $$ その具体的な値は、次のように$\\partial_{p}$ のスミス標準形によって計算することができる。図では、青い点線が$1$ の対角成分を、オレンジの実線が$1$ でない対角成分を示し、その他のすべての成分は$0$ である。2\nここで重要なのは、スミス標準形における$1$ の数$\\rank B_{p-1}$ と、ゼロベクトルの列の数$\\rank Z_{p}$ である。\n証明 3 Part 1. $B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$\n$$ \\begin{align*} Z_{p} :=\u0026amp; \\ker \\partial_{p} \\\\ B_{p} :=\u0026amp; \\text{Im} \\partial_{p+1} \\\\ W_{p} :=\u0026amp; \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\} \\end{align*} $$ と置く。特に、$W_{p}$ は$C_{p}$ の部分群となり、$\\lambda = 1$ のみを考えた場合に$B_{p} = W_{p}$ であるという点で、境界Boundary$B_{p}$ の条件を弱めたものと見なすことができるため、弱い境界Weak Boundariesと呼ばれる。\n$W_{p}$ の定義から、$\\lambda \\ne 1$ を考えると $$ B_{p} \\subset W_{p} $$ $Z_{p}$ の定義から、$\\forall z_{p} \\in Z_{p}$ は$\\partial_{p} z_{p} = 0$ であり、$Z_{p} = \\ker \\partial_{p}$ は$\\partial_{p} : C_{p} \\to C_{p-1}$ であるため $$ Z_{p} \\subset C_{p} $$ $C_{p}$ はフリーグループと仮定されているため、トーションフリー、すなわち$\\forall z_{p} \\in Z_{p} \\subset C_{p}$ に対して$\\lambda z_{p} = 0$ を満たす$\\lambda \\ne 0$ が存在しない。一方、すべての$c_{p+1} \\in C_{p+1}$ に対して $$ \\partial_{p+1} c_{p+1} = \\lambda z_{p} \\in W_{p} $$ の両辺に$\\partial_{p}$ を適用すると $$ 0 = \\partial_{p} \\partial_{p+1} c_{p+1} = \\partial_{p} \\lambda z_{p} = \\lambda \\partial_{p} z_{p} $$ であるため、$\\partial_{p} z_{p} = 0$ でなければならない。これは、$\\lambda z_{p} \\in W_{p}$ ならば$\\lambda z_{p} \\in Z_{p}$ であることを意味するため $$ W_{p} \\subset Z_{p} $$ このような考察から、次の包含関係を得る。 $$ B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p} $$\nPart 2. $W_{p} \\subset Z_{p}$ は$Z_{p}$ の直和群Direct Summandである\n$p$番目のホモロジーグループ$H_{p} \\left( \\mathcal{C} \\right) = Z_{p} / B_{p}$ の定義から $$ \\text{proj}_{1} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) $$ は剰余類$B_{p}$ に相当するだけのランクが下がった射影であり $H_{p} \\left( \\mathcal{C} \\right)$ のトーション部分群$T_{p} \\left( \\mathcal{C} \\right) \\subset H_{p} \\left( \\mathcal{C} \\right)$ に対して $$ \\text{proj}_{2} : H_{p} \\left( \\mathcal{C} \\right) \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ も射影である。 第1同型定理: 準同型写像$\\phi : G \\to G'$ が存在する場合 $$G / \\ker ( \\phi ) \\simeq \\phi (G)$$\nこれにより、$\\text{proj} := \\text{proj}_{1} \\circ \\text{proj}_{2}$ として定義された $$ \\text{proj} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ も射影である。$W_{p}$ の要素は$\\partial_{p+1} d_{p+1}$ のように表されるため、この射影$\\text{proj}$ の核は$W_{p}$ であり、すべての射影は全射Surjectionであるため、第1同型定理により $$ Z_{p} / W_{p} \\simeq H_{p} / T_{p} $$ が成立する。ここで、右辺の$H_{p}$ がどのようになっているかにかかわらず、トーション部分群$T_{p}$ で取り除いたため、トーションフリーであり、これにより、左辺の$Z_{p} / W_{p}$ もトーションフリーであることが保証される。したがって、$\\alpha_{1} , \\cdots , \\alpha_{k}$ が$Z_{p} / W_{p}$ の基底であり、$\\alpha'_{1} , \\cdots , \\alpha'_{l} \\in W_{p}$ が$W_{p}$ の基底であるとした場合、$\\alpha_{1} , \\cdots , \\alpha_{k}, \\alpha'_{1} , \\cdots , \\alpha'_{l}$ は$Z_{p}$ の基底となる。したがって、$Z_{p}$ は $$ Z_{p} = V_{p} \\oplus W_{p} $$ のように、$\\alpha_{1} , \\cdots , \\alpha_{k}$ を基底とする部分群$V_{p}$ と$W_{p}$ の直和として表現できる。\nPart 3. $Z_{p}, B_{p-1}, W_{p-1}$ の基底\nホモモルフィズムのスミス標準形: フリーアーベル群$G$ と$G'$ のランクがそれぞれ$n,m$ であり、$f : G \\to G'$ がホモモルフィズムである場合、次のような行列を持つホモモルフィズム$g$ が存在する。 $$ \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\in \\mathbb{Z}^{m \\times n} $$ ここで、$d_{1} , \\cdots, d_{r} \\in \\mathbb{N}$ であり、$d_{1} \\mid \\cdots \\mid d_{r}$、つまり$d_{k}$ は$d_{k+1}$ の約数Divisorである必要がある。\n$\\partial_{p} : C_{p} \\to C_{p-1}$ は、次のようなスミス標準形の$m \\times n$ 行列を持つ。\n$$ \\begin{matrix} \u0026amp; \\begin{matrix} e_{1} \u0026amp; \\cdots \u0026amp; e_{l} \u0026amp; e_{l} \u0026amp; \\cdots \u0026amp; e_{n} \\end{matrix} \\\\ \\begin{matrix} e'_{1} \\\\ \\vdots \\\\ e'_{l} \\\\ e'_{l} \\\\ \\vdots \\\\ e'_{m} \\end{matrix} \u0026amp; \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\end{matrix} $$\nこれにより、我々は直接的な計算を通じて次の3つを示すことになる:\n(1): $e_{l+1} , \\cdots , e_{n}$ は$Z_{p}$ の基底である。 (2): $b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ は$B_{p-1}$ の基底である。 (3): $e'_{1} , \\cdots , e'_{l}$ は$W_{p-1}$ の基底である。 補題\n$\\partial_{p}$ の定義により、一般的な$c_{p} \\in C_{p}$ に対して次が成立する。 $$ c_{p} = \\sum_{i=1}^{n} a_{i} e_{i} \\implies \\partial_{p} c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ (1): $b_{i} \\ne 0$ であるため、$Z_{p} = \\ker \\partial_{p}$ である必要十分条件は、$i = 1 \\cdots , l$ に対して$a_{i} = 0$ であることである。したがって、$e_{l+1} , \\cdots , e_{n}$ は$Z_{p}$ の基底である。 (2): すべての$\\partial_{p} c_{p} \\in B_{p-1}$ は$b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ の線形結合として表現され、$b_{i} \\ne 0$ であるため、$b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ は$B_{p-1}$ の基底である。 (3): $b_{i} e'_{i} = \\partial e_{i}$ であるため、まず$e'_{1}, \\cdots, e'_{l} \\in W_{p-1}$ である。逆に、$c_{p-1} \\in C_{p-1}$ を $$ c_{p-1} = \\sum_{i=1}^{m} d_{i} e'_{i} $$ と置き、$c_{p-1} \\in W_{p-1}$ と仮定すると、$W_{p-1}$ が$W_{p-1} = \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\}$ のように定義されていたため、$c_{p-1}$ はある$\\lambda \\ne 0$ に対して $$ \\lambda c_{p-1} = \\partial c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ の形で表現できる。係数を比較すると、$i \u0026gt; l$ に対して $$ \\lambda d_{i} = 0 \\implies d_{i} = 0 $$ を得る。したがって、$e'_{1} , \\cdots , e'_{l}$ は$W_{p-1}$ の基底である。 Part 4. \u0026lsquo;フリーチェインコンプレックスの標準基底分解\u0026rsquo;の証明\n$C_{p}$ と$C_{p-1}$ に対して、これまでの議論で登場する$e_{1} , \\cdots , e_{l}$ によって生成されるフリーグループを$U_{p}$ とすると、$Z_{p} = V_{p} \\oplus W_{p}$ であるため、$\\partial V_{p} = \\partial W_{p} = 0$ であり $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus Z_{p} \\\\ =\u0026amp; U_{p} \\oplus \\left( V_{p} \\oplus W_{p} \\right) \\end{align*} $$ を得る。ここで、$W_{p}$ と$Z_{p}$ は$C_{p}$ により一意であるが、$U_{p}$ と$V_{p}$ は必ずしも一意である必要はないことに注意されたい。\nPart 5. \u0026lsquo;ホモロジーグループの効率的な計算可能性\u0026rsquo;の証明\nPart 4により、コンプレックス$K$ に対して、次の分解が存在することが保証される。 $$ \\begin{align*} C_{p} \\left( K \\right) =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$\n直和の性質: $G = G_{1} \\oplus G_{2}$ としよう。もし$H_{1}$ が$G_{1}$ の部分群であり、$H_{2}$ が$G_{2}$ の部分群である場合、$H_{1}$ と$H_{2}$ も直和として表現でき、特に次が成立する。 $${{ G } \\over { H_{1} \\oplus H_{2} }} \\simeq {{ G_{1} } \\over { H_{1} }} \\oplus {{ G_{2} } \\over { H_{2} }}$$\n[1]: $H_{1} \\simeq G_{1}$ であり、$H_{2} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ G / G_{1} \\simeq G_{2} $$ [2]: $H_{1} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ {{ G } \\over { H_{2} }} \\simeq G_{1} \\oplus {{ G_{2} } \\over { H_{2} }}$$ Part 1で$B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$ であったため、直和の性質により $$ \\begin{align*} H_{p} \\left( K \\right) =\u0026amp; Z_{p} / B_{p} \\\\ =\u0026amp; \\left( {{ V_{p} \\oplus W_{p} } \\over { B_{p} }} \\right) \\\\ =\u0026amp; V_{p} \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [2] \\\\ =\u0026amp; \\left( {{ Z_{p} } \\over { W_{p} }} \\right) \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [1] \\end{align*} $$ を得る。ここで、$H_{p} \\left( K \\right) = \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right)$ の\n$Z_{p} / W_{p}$ はフリーパートであり $W_{p} / B_{p}$ はトーションパートである。 これにより、$K$ の$p$番目のベッチ数$\\beta_{p}$ は、次のように求められる。 $$ \\begin{align*} \\beta_{p} =\u0026amp; \\rank H_{p} \\left( K \\right) \\\\ =\u0026amp; \\rank \\left[ \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right) \\right] \\\\ =\u0026amp; \\rank \\left( Z_{p} / W_{p} \\right) + \\rank \\left( W_{p} / B_{p} \\right) \\\\ =\u0026amp; \\left[ \\rank Z_{p} - \\rank W_{p} \\right] + \\left[ \\rank W_{p} - \\rank B_{p} \\right] \\\\ =\u0026amp; \\rank Z_{p} - \\rank B_{p} \\end{align*} $$\n一方、$H_{p-1}(K)$ のトーションパートと$b_{1} | \\cdots | b_{l} \\in \\mathbb{N}$ に対しては、次のようなアイソモルフィズムが存在することが分かる。 $$ W_{p-1} / B_{p-1} \\simeq \\left( {{ \\mathbb{Z} } \\over { b_{1} \\mathbb{Z} }} \\right) \\oplus \\cdots \\oplus \\left( {{ \\mathbb{Z} } \\over { b_{l} \\mathbb{Z} }} \\right) $$ ここで、$i \\le l$ に対して$b_{i} = 1$ であること、つまり$B_{p-1}$ のランクが$l$ であることは $$ \\mathbb{Z} / b_{i} \\mathbb{Z} = \\mathbb{Z} / \\mathbb{Z} = \\left\\{ 0 \\right\\} $$ であるため、$W_{p-1}$ のランクが$l$ 分だけ減少することを覚えておく。\n■\n例 トーラス $$ \\begin{align*} \\beta_{0} =\u0026amp; 1 \\\\ \\beta_{1} =\u0026amp; 2 \\\\ \\beta_{2} =\u0026amp; 1 \\end{align*} $$\nトーラスのベッチ数は上記のように知られている。このトーラスのチェインコンプレックスが上の図のように定義されている場合、例として$\\beta_{1} = 2$ のみを計算してみよう。上で導出された公式を使用せずに単に数学的に考えて計算する方法もあるが、読めば分かる通り、頭が痛くなるほど難しい。これと対照的に、「ホモロジーを効率的に計算する」ということがどれほど便利かを見てみよう。\nホモモルフィズムのスミス標準形: フリーアーベルグループ$G$ と$G'$ に対して、$a_{1} , \\cdots , a_{n}$ が$G$ の基底であり、$a_{1}' , \\cdots , a_{m}'$ が$G'$ の基底であるとする。もし関数$f : G \\to G'$ がホモモルフィズムであれば、次を満たす唯一の整数の集合$\\left\\{ \\lambda_{ij} \\right\\} \\subset \\mathbb{Z}$ が存在する。 $$ f \\left( a_{j} \\right) = \\sum_{i=1}^{m} \\lambda_{ij} a_{i}' $$ この時行列$\\left( \\lambda_{ij} \\right) \\in \\mathbb{Z}^{m \\times n}$ を($G$ と$G'$ の基底に関する)$f$ の行列という。\n$\\beta_{1} = \\rank Z_{1} - \\rank B_{1}$ であるため、少なくとも境界行列$\\left( \\partial_{1} \\right)$ と$\\left( \\partial_{2} \\right)$ を求める必要がある。すべての$a , b, c \\in C_{1} (T)$ に対して $$ \\begin{align*} \\partial_{1} (a) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (b) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (c) =\u0026amp; v - v = 0 = 0v \\end{align*} $$ であるため $$ \\left( \\partial_{1} \\right) = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\implies Z_{1} = 3 , B_{0} = 0 $$ を得る。$Z_{p}$ は行列の右側のゼロベクターの数であり、$B_{p-1}$ は行列内の$1$ の数である。次に、$\\partial_{2}$ を考えると $$ \\begin{align*} \\partial_{2} (U) =\u0026amp; -a -b +c \\\\ \\partial_{2} (L) =\u0026amp; a + b - c \\end{align*} $$ であるため $$ \\left( \\partial_{2} \\right) = \\begin{bmatrix} -1 \u0026amp; 1 \\\\ -1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{bmatrix} \\sim \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} \\implies Z_{2} = 1 , B_{1} = 1 $$ を得る。これを総合すると、トーラスの$1$番目のベッチ数$\\beta_{1}$ は、次のように計算される。 $$ \\beta_{1} = \\rank Z_{1} - \\rank B_{1} = 3 - 1 = 2 $$ 当然ながら、この結果は、この投稿に紹介された定理に従って、フリーグループがどうであり、アイソモルフィズムがどうであるかといった、あらゆる数学的知識を駆使して得た値と一致することが保証されている。少し大胆に言えば、頭を使わずに指示された通りに計算すれば、ベッチ数、つまり「ホモロジー」を「計算」することができると要約できるだろう。もう少し良い言い方をすると、コンピュータを通じて位相数学を研究する道が開かれたということだ。\nMunkres. (1984). Elements of Algebraic Topology: p58.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p104.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (1984). Elements of Algebraic Topology: p58~61.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2399,"permalink":"https://freshrimpsushi.github.io/jp/posts/2399/","tags":null,"title":"ホモロジーグループのベッチ数"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 $2$キュービット $\\ket{a, b} = \\ket{a} \\otimes \\ket{b}$に対して 交換 ゲートexchange gate $\\text{ex}$を次のように定義する。\n$$ \\begin{align*} \\text{ex} : (\\mathbb{C}^{2})^{\\otimes 2} \u0026amp;\\to (\\mathbb{C}^{2})^{\\otimes 2} \\\\ \\ket{a, b} \u0026amp;\\mapsto \\ket{b, a},\\quad \\forall a,b \\in \\left\\{ 0, 1 \\right\\} \\end{align*} $$\n$$ \\text{ex} (\\ket{a} \\otimes \\ket{b}) = \\ket{b} \\otimes \\ket{a} $$\n説明 交換ゲートは二つのキュービットの状態を互いに交換する。具体的な入出力は次の通りである。\n$$ \\text{ex} (\\ket{00}) = \\ket{00} \\\\[0.5em] \\text{ex} (\\ket{01}) = \\ket{10} \\\\[0.5em] \\text{ex} (\\ket{10}) = \\ket{01} \\\\[0.5em] \\text{ex} (\\ket{11}) = \\ket{11} $$\n行列表現は次のようである。\n$$ \\text{ex} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p97\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3429,"permalink":"https://freshrimpsushi.github.io/jp/posts/3429/","tags":null,"title":"交換ゲート"},{"categories":"줄리아","contents":"概要 Juliaでは、複数のデバイスに計算タスクをスケジューリングする方法を紹介する1。正直、自分もよくわからない。\nコード using Distributed\rip_ = []\rfor last in [160,161,162,163,164,32,33,34,35,36,43,44,45,46,47]\rpush!(ip_, join([155,230,211,last],\u0026#39;.\u0026#39;))\rend\rsort!(ip_)\rfor ip in ip_\raddprocs([(\u0026#34;chaos@\u0026#34; * ip, 8)]; dir =\u0026#34;/home/chaos\u0026#34;, exename = \u0026#34;julia\u0026#34;) #add slave node\\\u0026#39;s workers\rprintln(\u0026#34;ip $ip\u0026#34; * \u0026#34; passed\u0026#34;)\rend\rnworkers()\r@everywhere function f(n)\rreturn n^2 - n end\rA = pmap(f,1:20000)\rX = []\r@async @distributed for i in 1:200\rprint(f(i))\rpush!(X, f(i))\rend pmapはうまくいくけど、@distributedはダメだ。\n環境 OS: Windows julia: v1.7.0 https://thomaswiemann.com/assets/teaching/Fall2021-Econ-31720/Econ_31720_discussion_6.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2398,"permalink":"https://freshrimpsushi.github.io/jp/posts/2398/","tags":null,"title":"ジュリアでの分散コンピューティングの方法"},{"categories":"줄리아","contents":"概要 Juliaでは、多次元配列を参照するためのインデックスタイプであるCatesianIndexを提供している1。もちろんCatesianという名前は、集合の積であるデカルト積から来ている。\nコード julia\u0026gt; M = rand(0:9, 4,4)\r4×4 Matrix{Int64}:\r9 3 7 0\r8 6 2 1\r3 8 4 9\r5 6 8 2 例えば、行列Mの3行4列目の要素、9にアクセスしたいとしよう。\njulia\u0026gt; pt = (3,4)\r(3, 4)\rjulia\u0026gt; M[pt]\rERROR: LoadError: ArgumentError: invalid index: (3, 4) of type Tuple{Int64, Int64}\rjulia\u0026gt; M[pt[1],pt[2]]\r9 直感的には、タプルpt = (3,4)をそのまま使えば良さそうだが、プログラミングに慣れている人なら、この方法に問題があることがわかるだろう。一般的に、このような二次元配列、特に行列を参照する時には、pt[1],pt[2]のように、二つの整数をはっきりと分けて入れなければならない。\njulia\u0026gt; pt = CartesianIndex(3,4)\rCartesianIndex(3, 4)\rjulia\u0026gt; M[pt]\r9 ありがたいことに、Juliaではこのインデックスを丸ごと渡すことができるCatesianIndexが提供されている。タプルをそのままCatesianIndexに変換して参照すれば、望んでいた結果を得ることができる。\n全コード M = rand(0:9, 4,4)\rpt = (3,4)\rM[pt]\rM[pt[1],pt[2]]\rpt = CartesianIndex(3,4)\rM[pt] 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/arrays/#Base.IteratorsMD.CartesianIndex\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2394,"permalink":"https://freshrimpsushi.github.io/jp/posts/2394/","tags":null,"title":"ジュリアの多次元インデックス"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 $\\mathbb{C}$ 上のベクトル空間 $\\mathbb{C}^{2}$の二つの単位ベクトル$\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$、$\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$をディラック記法で以下のように表記しよう。\n$$ \\ket{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\qquad \\ket{1} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$\n集合$\\left\\{ \\ket{0}, \\ket{1} \\right\\}$の要素を キュビットqubit、量子ビットと呼ぶ。\n$\\mathbb{C}^{2}$の$n$テンソル積 $\\left( \\mathbb{C}^{2} \\right)^{\\otimes n} = \\overbrace{\\mathbb{C}^{2} \\otimes \\cdots \\otimes \\mathbb{C}^{2}}^{n}$の標準基底\n$$ \\left\\{ \\ket{0} \\otimes \\cdots \\otimes \\ket{0}, \\dots, \\ket{1} \\otimes \\cdots \\otimes \\ket{1} \\right\\}$$\nの要素を $n$キュビット$n$qubitと呼ぶ。\n説明 キュビットはquantum bitの略語です。ビットbitが古典コンピューターで情報処理の最小単位であるならば、キュビットは量子コンピューターでその役割を果たしている。\n$n$キュビットは次のように簡単に表示される。$a = (a_{0}, a_{1}, \\dots, a_{n-1}) \\in \\left\\{ 0, 1 \\right\\}^{n}$を$n$ビットと呼ぶならば、\n$$ \\begin{align*} \\ket{a} \u0026amp;= \\ket{a_{0}, a_{1}, \\dots, a_{n-1}} \\\\ \u0026amp;= \\ket{a_{0} a_{1} \\dots a_{n-1}} \\\\ \u0026amp;= \\ket{a_{0}} \\otimes \\ket{a_{1}} \\otimes \\cdots \\otimes \\ket{a_{n-1}} \\end{align*} $$\n例：$(\\mathbb{C}^{2}) ^{\\otimes 2}$ 最も簡単な例として$(\\mathbb{C}^{2}) ^{\\otimes 2} = \\mathbb{C}^{2} \\otimes \\mathbb{C}^{2} \\cong \\mathbb{C}^{4}$の場合を具体的に見よう。$2$キュビットは次のように表記される。\n$$ \\ket{00} = \\ket{0,0} = \\ket{0} \\otimes \\ket{0},\\qquad \\ket{01} = \\ket{0,1} = \\ket{0} \\otimes \\ket{1} \\\\ \\ket{10} = \\ket{1,0} = \\ket{1} \\otimes \\ket{0},\\qquad \\ket{11} = \\ket{1,1} = \\ket{1} \\otimes \\ket{1} $$\nそれぞれの$2$キュビットを行列で表すと、クロネッカー積の定義に従って次の通りである。\n$$ \\begin{align*} \\ket{00} \u0026amp;= \\ket{0} \\otimes \\ket{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\[1em] 0 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\ \\ket{01} \u0026amp;= \\ket{0} \\otimes \\ket{1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\\\[1em] 0 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\ \\ket{10} \u0026amp;= \\ket{1} \\otimes \\ket{0} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\[1em] 1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\ \\ket{11} \u0026amp;= \\ket{1} \\otimes \\ket{1} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\\\[1em] 1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{align*} $$\nしたがって$\\braket{ik | jl} = \\delta_{ij}\\delta_{kl}$である。このとき$\\delta$はクロネッカーデルタである。任意の$(\\mathbb{C}^{2}) ^{\\otimes 2}$の要素は次のようである。\n$$ \\begin{align*} \u0026amp; (\\alpha_{0}\\ket{0} + \\alpha_{1}\\ket{1}) \\otimes (\\beta_{0}\\ket{0} + \\beta_{1}\\ket{1})\\\\ \u0026amp;= \\alpha_{0}\\beta_{0} \\ket{0} \\otimes \\ket{0} + \\alpha_{0}\\beta_{1} \\ket{0} \\otimes \\ket{1} + \\alpha_{1}\\beta_{0} \\ket{1} \\otimes \\ket{0} + \\alpha_{1}\\beta_{1} \\ket{1} \\otimes \\ket{1} \\\\ \u0026amp;= \\alpha_{0}\\beta_{0} \\ket{00} + \\alpha_{0}\\beta_{1} \\ket{01} + \\alpha_{1}\\beta_{0} \\ket{10} + \\alpha_{1}\\beta_{1} \\ket{11} \\\\ \u0026amp;= \\alpha_{00}\\ket{00} + \\alpha_{01}\\ket{01} + \\alpha_{10}\\ket{10} + \\alpha_{11}\\ket{11} \\\\ \\end{align*} $$\n特に$\\left\\{ \\ket{a} \\right\\}_{a \\in \\left\\{ 0, 1 \\right\\}^{2}}$を$(\\mathbb{C}^{2}) ^{\\otimes 2}$の基底と呼ぶならば、任意の$\\ket{\\psi} \\in (\\mathbb{C}^{2}) ^{\\otimes 2}$に対して、\n$$ \\ket{\\psi} = \\sum\\limits_{a \\in \\left\\{ 0, 1 \\right\\}^{2}} \\braket{a | \\psi} \\ket {a} = \\sum\\limits_{a \\in \\left\\{ 0, 1 \\right\\}^{2}} \\psi_{a} \\ket {a} $$\n$\\ket{\\psi}, \\ket{\\xi} \\in (\\mathbb{C}^{2}) ^{\\otimes 2}$の内積は、\n$$ \\braket{\\psi | \\xi} = \\sum\\limits_{a \\in \\left\\{ 0, 1 \\right\\}^{2}} \\overline{\\psi_{a}} \\xi_{a} $$\nこのとき$\\overline{\\psi_{a}}$は$\\psi_{a}$の共役複素数である。\n参照 ビット 量子ゲート キム・ヨンフン、ホ・ジェソン、量子情報理論 (2020)、p93-95\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3423,"permalink":"https://freshrimpsushi.github.io/jp/posts/3423/","tags":null,"title":"キュービット：量子コンピュータにおける情報の基本単位"},{"categories":"줄리아","contents":"概要 Juliaでは、\u0026amp;\u0026amp;と||は論理積、論理和だけでなく、ショートサーキット評価Short-circuit Evaluationを実行する1。例えば、A \u0026amp;\u0026amp; BはAとBが両方とも真の時に真を返すが、実際にはAが偽なら、Bが真か偽かを見る必要はなく、A \u0026amp;\u0026amp; Bは偽になる。ショートサーキット評価は、その見る必要のないBを実際に見ずに済ますことだ。Bに対する計算を省略することにより、場合によっては速度が改善される。\n見てほしい 条件文を簡潔に書く方法 速度比較 M = rand(0:2, 10^4, 10^4);\rprint(first(M))\r@time if first(M) == 2 \u0026amp;\u0026amp; sum(M) \u0026lt; 1000\rprint(\u0026#34;!!\u0026#34;)\rend\r@time if sum(M) \u0026lt; 1000 \u0026amp;\u0026amp; first(M) == 2\rprint(\u0026#34;!!\u0026#34;)\rend 二つの条件文はsum(M) \u0026lt; 1000とfirst(M) == 2の順序が変えられただけで、正確に同じ作業を行う。しかし、first(M) == 2は行列Mの最初の要素が2かどうかだけをチェックし、sum(M)は全要素を走査して加算するため、相対的に計算に時間がかかる。\njulia\u0026gt; M = rand(0:2, 10^4, 10^4);\rjulia\u0026gt; print(first(M))\r0 もしMの最初の要素が上記のように0なら、sum(M) \u0026lt; 1000かどうかを確認するためにsum(M)を計算する必要はない。その速度は、単に順序を変えるだけで有意な差が出ることがある。\njulia\u0026gt; @time if first(M) == 2 \u0026amp;\u0026amp; sum(M) \u0026lt; 1000\rprint(\u0026#34;!!\u0026#34;)\rend\r0.000009 seconds\rjulia\u0026gt; @time if sum(M) \u0026lt; 1000 \u0026amp;\u0026amp; first(M) == 2\rprint(\u0026#34;!!\u0026#34;)\rend\r0.040485 seconds (1 allocation: 16 bytes) 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/control-flow/#Short-Circuit-Evaluation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2392,"permalink":"https://freshrimpsushi.github.io/jp/posts/2392/","tags":null,"title":"ジュリアのショートサーキット"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義 集合 $\\left\\{ 0, 1 \\right\\}$ の元を ビットbitと呼ぶ。集合 $\\left\\{ 0, 1 \\right\\}^{n}$ の元を $n$ビット$n$bitと呼ぶ。\n説明 ビットは binary digitの略称である。通常、「$0$または$1$の値を取りうるもの」と説明される。古典コンピュータが処理する最小の情報単位であり、コンピュータの回路では$1$は電気信号があることを、$0$は電気信号がないことを意味する。\n量子コンピュータで処理される情報の最小単位は、ビットにクォンタムを付けて quantum bit量子ビットと呼ばれる。\n関連項目 ブール関数 量子ビット ","id":3422,"permalink":"https://freshrimpsushi.github.io/jp/posts/3422/","tags":null,"title":"ビット: 古典的なコンピュータにおける情報の基本単位"},{"categories":"줄리아","contents":"概要 ジュリアの基本組み込み関数は知れば知るほど便利だ。早速、例を見て学ぼう。\nコード x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rargmin(x)\rargmax(x)\rfindmin(x)\rfindmax(x)\rextrema(x)\rfindfirst(x .== 3)\rfindlast(x .== 3)\rfindall(x .== 3)\rfindnext(x .== 3, 5)\rfindprev(x .== 3, 5) 最適解 argmin(),argmax(),findmin(),findmax(),extrema() 最適解を見つける。\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; argmin(x)\r9\rjulia\u0026gt; argmax(x)\r7\ry = [7, 8, 9, 9, 7, 9]\rjulia\u0026gt; argmax(y)\r3\rjulia\u0026gt; findall(y.==maximum(y))\r3-element Vector{Int64}:\r3\r4\r6 argmin(),argmax() はただの最適解、つまり値が最も大きく小さい場所のインデックスを返す。そんなインデックスが複数ある場合は最も小さいものを返す。だから、実際は argmax(x) $= \\min(\\argmax(x))$だ。本当の$\\argmax$は、maximum() と下の findall() 関数と一緒に使えばいい。\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findmin(x)\r(2, 9)\rjulia\u0026gt; findmax(x)\r(12, 7) findmin(),findmax() は最適解とその値まで返す。\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; extrema(x)\r(2, 12) ちなみに extrema() はインデックスではなく、その値だけを返す。これは R の range() 関数と同じだ1。\n条件を満たした最初・最後のインデックス findfirst(), findlast() x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findfirst(x .== 3)\r1\rjulia\u0026gt; findlast(x .== 3)\r8 3が存在する最初と最後のインデックスを見つけた。配列の形が大まかに予想できる場合、これらを使ってコードの速度を向上させることができるだろう。\n条件を満たした全てのインデックス findall() x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findall(x .== 3)\r3-element Vector{Int64}:\r1\r6\r8 使いやすく、一般的なプログラミングで最も役に立つ関数だ。maximum(), minimum()と一緒に使えば、全ての $\\text{argmin}\n条件を満たした特定範囲のインデックス findnext(), findprev() julia\u0026gt; findnext(x .== 3, 5)\r6\rjulia\u0026gt; findprev(x .== 3, 5)\r1 前後である程度は例外として検索する必要がある時もある。例えば、配列の最初の要素と同じ要素を探す時にfindall()を使うと、その最初の要素も見つかってしまうので、煩わしいことになるかもしれない。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/collections/#Base.extrema\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2390,"permalink":"https://freshrimpsushi.github.io/jp/posts/2390/","tags":null,"title":"ジュリアのfind関数들"},{"categories":"줄리아","contents":"概要 1 ジュリアでは、関数名の最後に感嘆符Bang!を追加することをバンク規約と呼ぶ。これらの関数は、与えられた引数を変更する特徴がある。\nコード function add_1!(x)\rx .+= 1\rreturn x\rend\rfoo = [2,5,-1]\radd_1!(foo)\rfoo 例えば、上のコードを実行すると、以下の結果が得られる。\njulia\u0026gt; foo = [2,5,-1]\r3-element Vector{Int64}:\r2\r5\r-1\rjulia\u0026gt; add_1!(foo)\r3-element Vector{Int64}:\r3\r6\r0\rjulia\u0026gt; foo\r3-element Vector{Int64}:\r3\r6\r0 配列fooは関数の外で定義され、「add_1!()」によって要素が$1$ずつ増えて返されただけでなく、引数自体が変更された。\n説明 代表的なメソッドであるpop!()は、配列の最後の要素を削除しつつ返すが、この関数が元の配列を変更できない場合、MatlabやRのように広く認知されているデータ構造を使用するのが難しく、一般的なプログラミングに慣れているユーザーにとっては非常に不便だったかもしれない。\nPythonで関数ではなくメソッドを使用したときにクラスのデータまで変更される感覚と同様に捉えればいい。ジュリアは言語設計上クラスをサポートしていないので、正確な説明ではないが、Pythonでメソッドを使っていた時が便利だった時に役立つことができる。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/style-guide/#bang-convention\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2388,"permalink":"https://freshrimpsushi.github.io/jp/posts/2388/","tags":null,"title":"ジュリアの感嘆符の規約"},{"categories":"위상데이터분석","contents":"定義 最小包含円 $n \u0026gt; d$ としよう。$d$次元のユークリッド空間で与えられた有限な集合$P = \\left\\{ p_{k} \\right\\}_{k=1}^{n} \\subset \\mathbb{R}^{d}$に対して、以下のような最適化問題を最小包含円問題Smallest Enclosing Disk Problemという。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; r \\ge 0 \\\\ \\text{subject to} \u0026amp; \\left\\| c - p_{k} \\right\\|_{2} \\le r \\end{matrix} \\\\ c \\in \\mathbb{R}^{d} , k = 1, \\cdots , n $$\nヒント これは正確には定理と呼ぶほどのものではないが、この問題を扱う際に知っておくと良い事実である。\n$P$がアフィン独立である場合、円の境界には$P$の点が最少で$2$から最大で$d+1$個まで存在する。つまり、点が重なっていたり、3点以上が一直線上にあるような場合を除き、正確に$2 \\le m \\le d+1$個の点で最小包含円が一意に定まる。 例えば、$d = 2$次元平面では、円は正確に3つの点で一意に定まる。 一般に$n \u0026gt; d+1$個の点が与えられた場合、アルゴリズムAlgorithmではなく明示的な公式Explicit Formulaを見つけることは不可能とされている。境界上に最小包含円を一意に定める点をサポートSupportと呼ぶが、単に点を持っているだけでは誰がサポートかを知る方法がないためである。 このように境界上の点をサポートと呼ぶのは、幾何学の問題で一般的である。サポートベクターマシンでは、サポートが境界上の点を意味する。 したがって、この問題を解くアルゴリズムを開発するということは、境界上のサポートを見つけることを保証するか、それを迅速に見つける方法に関する研究と言っても過言ではない。ただし、現在使用されているアルゴリズムでは、その根本的なアイデアはほぼウェルツルのものに基づいており、その改良や変種をまとめて単にウェルツルアルゴリズムWelzl Algorithmと呼んでいる1。少なくともこの投稿で紹介されている後続の研究はすべてその系統に属している。 解法 ウェルツルアルゴリズム 2 ウェルツルアルゴリズムWelzl Algorithmは、最小包含円問題を解く再帰的なRecursive解法である。基本的には、点を一つずつ追加したり削除したりしながらサポートを見つけ、そのようにして得られた円が与えられたすべての点を包含しているかどうかを繰り返し確認する。\n点が$n \\le d+1$個の場合にそれらを包含する円を正確に見つけることは比較的簡単であるため、そのような関数が存在すると仮定する。実際の実装では、これが思ったほど単純ではないが、最小包含円問題の核心ではない。\n擬似コード 3 $(c,r)$ = welzl$\\left( P, S \\right)$\nInput: 与えられた点の集合$P = \\left\\{ p_{k} \\right\\}_{k=1}^{n} \\subset \\mathbb{R}^{d}$とサポーターの候補$S \\subset P$を受け取る。ヒントで述べたように、$\\left| S \\right| \\le d+1$である。 Output: $P$のすべての点を包含する最も小さい円の中心$c$と半径$r$のタプル$(c,r)$を得る。中心が$c$で半径が$r$の閉じた球を$D = B \\left[ c,r \\right]$と表す。 $(c,r)$ = trivial$\\left( S \\right)$\nInput: $\\left| S \\right| \\le d+1$の点の集合$S \\subset P$を受け取る。 Output: $S$のすべての点を包含する最も小さい円の中心$c$と半径$r$のタプル$(c,r)$を得る。welzlに比べて単純であるという仮定に従い、trivialの擬似コードは別途記述しない。 function welzl$\\left( P, S \\right)$\n$S := \\emptyset$\nif $P = \\emptyset$ or $\\left| S \\right| = d+1$ then\nreturn trivial$\\left( S \\right)$\nelse\nchoose $p \\in P$\n$D := $ welzl$\\left( P \\setminus \\left\\{ p \\right\\}, S \\right)$\nif $p \\in D$ then\nreturn $D$\nend if\nend if\nreturn welzl$\\left( P \\setminus \\left\\{ p \\right\\} , S \\cup \\left\\{ p \\right\\} \\right)$\nend function\nwelzlは再帰関数として記述されるため、実際のプログラムではtrivialから計算が始まることになる。welzlの擬似コードで登場するchooseは、choose$x \\in X$のように書かれ、集合$X$から要素$x$を一様ランダムに選ぶキーワードKeywordである。\n最後にwelzl$\\left( P \\setminus \\left\\{ p \\right\\} , S \\cup \\left\\{ p \\right\\} \\right)$をリターンすること自体が、$P$にある点を一つずつ取り除いて$S$に入れてみてサポートを見つけるプロセスであることが分かれば、このアルゴリズムを理解したことになる。\n説明 最小包含円の制約条件を文字通りに解釈してみよう。$\\left\\| c - p_{k} \\right\\|_{2} \\le r$を満たす$c$と$r$を見つけるということは、少なくとも与えられた点をすべて包むことができる中心Centre$c$と半径Radius$r$を見つけることを意味する。しかし、ユークリッド空間は非常に広いため、$c$がどこであっても$r$を無制限に大きくすれば、この制約条件は必ず満たせる。当然のことながら、私たちの関心事は、それをしながら$r$を最小化すること、つまり与えられた点を包む最も小さい球を見つけることである。\n単純化の問題点 包含Enclosingは、数学全般で広く使われている表現ではないが、発音そのままにインクロージングと言うにはあまりに長くて感じがこないため、任意に翻訳した。そもそもインクロージングEnclosing自体がバウンディングBoundingを圧倒しているわけではない。また、円盤Diskも内部を含む閉じた球の意味で使われたが、実際の英語表現では単にボールBallやスフィアSphereもよく使われる。単語や翻訳にこだわらないようにしよう。\n歴史 早くもライムント・ザイデルRaimund Seidelによって線形計画法に基づく解法が知られていた。\n1991年にエモ・ヴェルツルEmo Welzlが彼の論文で再帰的なRecursiveアルゴリズムを提案し、いわゆるSOTAState Of The Artを記録した。2022年現在でも、一般的な最小包含ディスク問題ではこのヴェルツルのアルゴリズムが最も優れているとされており、以降の多くの研究はこれを改良する形で行われてきた。\n1999年にはベルント・ゲルトナーBernd Gärtnerが、ヴェルツルのアルゴリズムに従いつつも、二次計画法Quadratic Programmingの応用を導入してこれを改善した。4 彼のコードはC++で書かれており、チューリッヒ連邦工科大学のウェブサイト5で実際の実装を見ることができる。\n2003年にはカスパー・フィッシャーKaspar Fischerが、ゲルトナーとの研究で線形計画法のシンプレックス法で登場するブランドのルールを導入し、高速なコードを作成した。6 2013年にはトーマス・ラーソンThomas Larssonが、近似的なものではなく、速度と堅牢性Robustnessを備えた方法を提案した。7\nここまで紹介された研究を参照すると、ヴェルツル-ゲルトナー-フィッシャーと続く大きな流れを確認することができる。\n応用 ヴェルツルアルゴリズムの代表的な応用は、チェックコンプレックスの構築である。\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p73~75.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWelzl. (1991). Smallest enclosing disks (balls and ellipsoids). https://doi.org/10.1007/BFb0038202\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Smallest-circle_problem#Welzl's_algorithm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGärtner. (1999). Fast and robust smallest enclosing balls. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.5783\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://people.inf.ethz.ch/gaertner/subdir/software/miniball.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKaspar Fischer. (2003). Fast Smallest-Enclosing-Ball Computation in High Dimensions. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.5783\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThomas Larsson. (2013). Fast and Robust Approximation of Smallest Enclosing Balls in Arbitrary Dimensions. https://doi.org/10.1111/cgf.12176\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2385,"permalink":"https://freshrimpsushi.github.io/jp/posts/2385/","tags":null,"title":"ベルツルアルゴリズム: 最小内包ディスク問題の解法"},{"categories":"줄리아","contents":"概要 ジュリアで、viewは配列のサブアレイを素早く参照させるデータ構造だ。実際に使う立場から見れば面倒で差がないように見えるけど、怠惰に参照されてもっと軽い配列を返す。だから、基本的なレベルで最適化されたジュリアのコードでは、@viewsというマクロを簡単に見つけることができる。\nコード 行列Mのサブマトリックスを参照してみよう。\n関数形式：view() view(A, inds...)\nAのinds...に従ったviewを返す。 しかし、この形式は一般的にコードを読みにくくするため、好まれない。次のマクロを使用すると、viewを使用しながらも基本的なジュリアの文法と大きな違いはない。\nマクロ：@view @viewマクロは、サブアレイを参照する文脈のコードにviewが適用されたかのように替えるマクロだ。\nブロック全体に適用：@views @viewsマクロは、続くブロック全体に@viewを適用する。これのおかげで、viewなしで快適に書いた関数の前に@views f(x) ... endと@viewsだけを付ければ、自動的にviewが適用される。\n全体のコード 速度比較 fcopy()とfview()は、まったく同じ機能を持つ関数だが、速度に違いがある。一見、速度は似ているように見えるが、ほとんどがコンパイル時間だ。これを除いて、単純な実行時間だけを比較すると、約4倍の差がある。\n環境 OS: Windows julia: v1.7.0 ","id":2384,"permalink":"https://freshrimpsushi.github.io/jp/posts/2384/","tags":null,"title":"ジュリアで部分配列を迅速に参照する方法"},{"categories":"선형대수","contents":"ビルドアップ1 便宜上、複素数空間$\\mathbb{C}$について展開するが、$\\mathbb{R}$や任意のベクター空間でも問題ない。 有限集合$\\Gamma$から複素数空間への関数の集まりを$\\mathbb{C}^{\\Gamma}$として表記しよう。\n$$ \\mathbb{C}^{\\Gamma} = \\left\\{ f : \\Gamma \\to \\mathbb{C} \\right\\} $$\n$\\Gamma$を$\\mathbf{n} = \\left\\{ 1, 2, \\dots, n \\right\\}$とする。各$1 \\le i \\le n$を複素数$z_{i} \\in \\mathbb{C}$に送る関数を$(z_{1}, \\dots, z_{n})$と表記すると、これは$\\mathbb{C}^{\\mathbf{n}}$に属する関数であり、$n$-複素数順序対集合$\\mathbb{C}^{n}$のベクターとも同じだ。\n$$ (z_{1}, \\dots, z_{n}) : i \\mapsto z_{i} $$\n$$ \\mathbb{C}^{n} := \\mathbb{C}^{\\mathbf{n}} = \\left\\{ (z_{1}, \\dots, z_{n}) \\vert z_{i} \\in \\mathbb{C} \\right\\} $$\nつまり、$v \\in \\mathbb{C}^{\\Gamma}$は$v : i \\mapsto z_{i}$と同じ関数と見ることもでき、$v = (z_{1}, \\dots, z_{\\left| \\Gamma \\right|})$と同じ順序対と見ることもできる。\n有限集合$\\Gamma_{1}$、$\\Gamma_{2}$に対して、二つのベクター空間$\\mathbb{C}^{\\Gamma_{1}}$と$\\mathbb{C}^{\\Gamma_{2}}$のテンソル積は、$\\Gamma_{1}$と$\\Gamma_{2}$の積空間$\\Gamma_{1} \\times \\Gamma_{2}$から作られる関数空間（ベクター空間）$\\mathbb{C}^{\\Gamma_{1} \\times \\Gamma_{2}}$として定義される。\n定義2 有限集合$\\Gamma_{1}$、$\\Gamma_{2}$に対し、二つのベクター空間$\\mathbb{C}^{\\Gamma_{1}}$と$\\mathbb{C}^{\\Gamma_{2}}$のテンソル積tensor productを以下のように定義する。\n$$ \\mathbb{C}^{\\Gamma_{1}} \\otimes \\mathbb{C}^{\\Gamma_{2}} := \\mathbb{C}^{\\Gamma_{1} \\times \\Gamma_{2}} $$\nここで、$\\Gamma_{1} \\times \\Gamma_{2}$は$\\Gamma_{1}$と$\\Gamma_{2}$の積空間だ。\n説明 簡単な例として、$\\Gamma_{1} = \\mathbf{2} = \\left\\{ 1, 2 \\right\\}$、$\\Gamma_{2} = \\mathbf{3} = \\left\\{ 1, 2, 3 \\right\\}$としよう。そして、$\\Gamma$をこれらの積空間としよう。\n$$ \\Gamma = \\Gamma_{1} \\times \\Gamma_{2} = \\left\\{ (1,1), (1,2), (1,3), (2,1), (2,2), (2,3) \\right\\} $$\nその要素をそれぞれ以下のように表記しよう。\n$$ e_{i} \\otimes e_{j} = (i, j) $$\nすると、概要での論理に従えば、$v \\in \\mathbb{C}^{\\Gamma}$は$(i,j) \\mapsto \\alpha_{ij}$と同じ関数であり、$\\left( \\alpha_{11}, a_{12}, a_{13}, a_{21}, a_{22}, a_{23} \\right)$と同じ順序対とみなすことができる。したがって、$\\mathbb{C}^{\\Gamma}$は$\\left\\{ e_{i} \\otimes e_{j} : 1 \\le i \\le 2, 1 \\le j \\le 3 \\right\\}$を基底として持つベクター空間だ。\n$$ \\begin{align*} \\mathbb{C}^{\\Gamma} \u0026amp;= \\left\\{ \\sum\\limits_{i,j} \\alpha_{i,j} e_{i} \\otimes e_{j} : \\alpha_{ij} \\in \\mathbb{C} \\right\\} \\\\ \u0026amp;= \\left\\{ \\left( \\alpha_{11}, a_{12}, a_{13}, a_{21}, a_{22}, a_{23} \\right) : \\alpha_{ij} \\in \\mathbb{C} \\right\\} \\end{align*} $$\nつまり、$\\mathbb{C}^{6}$と同型だ。\n$$ \\mathbb{C}^{\\Gamma} = \\mathbb{C}^{2} \\otimes \\mathbb{C}^{3} \\cong \\mathbb{C}^{6} $$\n$\\mathbb{C}$を積空間にまとめると、変数の位置が増え、テンソル積にまとめると、インデックスの位置が増えると考えるとわかりやすいだろう。\n$$ z_{1} \\in \\mathbb{C}\\qquad (z_{1},z_{2}) \\in \\mathbb{C} \\times \\mathbb{C}\\qquad (z_{1}, z_{2}, z_{3}) \\in \\mathbb{C}\\times \\mathbb{C} \\times \\mathbb{C} $$\n$$ (z_{1}, z_{2}) \\in \\mathbb{C}^{2} \\qquad (z_{11}, z_{12}, z_{21}, z_{22}) \\in \\mathbb{C}^{2} \\otimes \\mathbb{C}^{2} \\\\[1em] (z_{111}, z_{112}, z_{121}, z_{122}, z_{211}, z_{212}, z_{221}, z_{222}) \\in \\mathbb{C}^{2} \\otimes \\mathbb{C}^{2} \\otimes \\mathbb{C}^{2} $$\n各$e_{i} \\otimes e_{j}$は$\\mathbb{C}^{6}$の基準基底と以下のように対応する。\n$$ e_{1} \\otimes e_{1} = \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} \\quad e_{1} \\otimes e_{2} = \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} \\quad e_{1} \\otimes e_{3} = \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} \\\\[2em] e_{2} \\otimes e_{1} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} \\quad e_{2} \\otimes e_{2} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} \\quad e_{2} \\otimes e_{3} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} $$\n行列のクロネッカー積で表すと次のようになる。\n$$ e_{1} \\otimes e_{1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\[1.5em] 0 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\quad e_{1} \\otimes e_{2} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\[1.5em] 0 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\[2em] e_{1} \\otimes e_{3} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\\\[1.5em] 0 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\quad e_{2} \\otimes e_{1} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\[1.5em] 1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\[2em] e_{2} \\otimes e_{2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\[1.5em] 1 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\quad e_{2} \\otimes e_{3} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\\\[1.5em] 1 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} $$\nまた、これにより次のことが成立することがわかる。\n$$ \\mathbb{C} \\otimes \\mathbb{C}^{n} \\cong \\mathbb{C}^{n} \\qquad \\mathbb{C} \\otimes \\mathbb{C} \\cong \\mathbb{C} $$\n性質 $\\mathbb{C}^{n} \\otimes \\mathbb{C}^{m}$は以下の二つの演算に関してベクター空間である。 $(x_{1} \\otimes y_{1}) + (x_{2} \\otimes y_{2}) = (x_{1} + x_{2}) \\otimes (y_{1} + y_{2})$ $\\alpha (x \\otimes y) = (\\alpha x) \\otimes y = x \\otimes (\\alpha y)$ $\\mathbb{C}^{n} \\otimes \\mathbb{C}^{m} \\cong \\mathbb{C}^{nm}$ $\\dim (\\mathbb{C}^{n} \\otimes \\mathbb{C}^{m}) = \\dim(\\mathbb{C}^{n}) \\cdot \\dim(\\mathbb{C}^{m}) = nm$ 参照 積ベクター $v \\otimes w$\n${}$ 物理学におけるテンソルとは：テンソルの簡単な定義 微分多様体上で定義されるテンソル 金英勳・許在聲, 量子情報理論 (2020), p3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n金英勳・許在聲, 量子情報理論 (2020), p31\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3414,"permalink":"https://freshrimpsushi.github.io/jp/posts/3414/","tags":null,"title":"ベクトル空間のテンソル積"},{"categories":"위상데이터분석","contents":"ビルドアップ 難しい内容ですが、できるだけ理解しやすいように、すべての計算と説明を省略せずに丁寧に残しました。ホモロジーに興味がある方は、ぜひお読みください。\n実際に、私たちが興味を持っている位相空間 $X$ があり、これが特定のシンプリシャルコンプレックスに従って$\\Delta$-コンプレックス構造を通して表現されるとしましょう。小さな例として、上の図では右側のトーラスが $X$ であり、左側がシンプリシャルコンプレックスに相当します。\nシンプレックスの定義:\nアフィン独立な $v_{0}, v_{1} , \\cdots , v_{n} \\in \\mathbb{R}^{n+1}$ の凸包を**$n$-シンプレックス** $\\Delta^{n}$ と呼び、ベクトル $v_{k}$ を頂点と呼びます。数式的には以下のようになります。 $$ \\Delta^{n} := \\left\\{ \\sum_{k} t_{k} v_{k} : v_{k} \\in \\mathbb{R}^{n+1} , t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$ $\\Delta^{n}$ から一つの頂点が除かれて作られる $n-1$-シンプレックス $\\Delta^{n-1}$ を $\\Delta^{n}$ の面と呼びます。$\\Delta^{n}$ のすべての面の和集合を $\\Delta^{n}$ の境界と呼び、$\\partial \\Delta^{n}$ と表します。 シンプレックスの内部 $\\left( \\Delta^{n} \\right)^{\\circ} := \\Delta^{n} \\setminus \\partial \\Delta^{n}$ をオープンシンプレックスと呼びます。 ここで、シンプリシャルコンプレックスとはシンプレックスで構成されるコンプレックスで、具体的には以下のようなCWコンプレックスで構成されているとしましょう。\n$n$-セルの定義:\n以下のように定義された $D^{n} \\subset \\mathbb{R}^{n}$ を $n$-ユニットディスクと呼びます。 $$ D^{n} := \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{n} : \\left\\| \\mathbf{x} \\right\\| \\le 1 \\right\\} $$ $D^{n} \\setminus \\partial D^{n}$ とホモトピー同値な開集合 $e^{n}$ を $n$-セルとも呼びます。 CWコンプレックスの定義:\n離散的な集合 $X^{0} \\ne \\emptyset$ を**$0$-セル**とみなします。 $n$-スケルトン $X^{n}$ は $X^{n-1}$ に$n$-セル $e_{\\alpha}^{n}$ を $\\phi_{\\alpha} : S^{n-1} \\to X^{n-1}$ で結合することによって作られます。 $X := \\bigcup_{n \\in \\mathbb{N}} X^{n}$ が弱位相を持つ位相空間になるとき、$X$ をセルコンプレックスと呼びます。 定義 1 $\\Delta$-コンプレックス構造を持つ位相空間 $X$ が与えられているとしましょう。\n$X$ のオープン $n$-シンプレックスである$n$-セル $e_{\\alpha}^{n}$ を基底を持つ自由アーベル群 $\\Delta_{n} (X)$ と表しましょう。$\\Delta_{n} (X)$ の要素を**$n$-チェインと呼び、係数 $k_{\\alpha} \\in \\mathbb{Z}$ に対して以下のような形式的和で表します。 $$ \\sum_{\\alpha} k_{\\alpha} e_{\\alpha}^{n} $$ 一方、CWコンプレックスの定義から、各 $n$-セル $e_{\\alpha}^{n}$ にはそれに対応する特性写像** $\\sigma_{\\alpha} : \\Delta^{n} \\to X$ が存在するため、単に次のように表すこともあります。 $$ \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} $$ 次のように定義される準同型 $\\partial_{n} : \\Delta_{n} (X) \\to \\Delta_{n-1} (X)$ を境界準同型と呼びます。ここで、$\\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right]$ は、$\\sigma_{\\alpha}$ の $X$ の $n-1$-シンプレックス に対する制限関数であることを意味します。 $$ \\partial _{n} \\left( \\sigma_{\\alpha} \\right) := \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right] $$ 3. 商群 $\\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $H_{n}^{\\Delta}$ と表し、$H_{n}^{\\Delta}$ はホモロジーグループであるため、$X$ の第 $n$ シンプリシャルホモロジーグループと呼びます。\n群 $0$ は $\\left\\{ 0 \\right\\}$ で定義されたマグマです。つまり、空の代数構造です。 準同型 $\\partial^{2} = 0$ はゼロ準同型です。 $\\text{Im}$ は像です。 $\\ker$ はカーネルです。 集合でハット表記 $\\hat{v}_{i}$ は、次のように $v_{i}$ だけを除くことを意味します。 $$ \\left\\{ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right\\} := \\left\\{ v_{1} , \\cdots , v_{n} \\right\\} \\setminus \\left\\{ v_{i} \\right\\} $$ 説明 定義に文字が多いので、理解する前に目に入りにくいのは普通です。血となり肉となる説明なので、丁寧に読むようにしましょう。個人的に勉強している間に苦労した部分をできるだけわかりやすく書くように努めました。\n$\\Delta_{n} (X)$ の要素をなぜチェーンと呼ぶのか？ $$ \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} $$ のような記法で $\\sigma_{\\alpha} : \\Delta^{n} \\to X$ を考えることで、これで $e_{\\alpha}^{n}$ が $\\Delta^{n}$ の要素なのか $X$ の要素なのかといったことはあまり考える必要がなくなりました。$n=2$ で全ての係数が $k_{\\alpha} = 1$ の場合、幾何学的に想像できる例として、以下の図の右側のような図形 $\\sum_{i=1}^{7} \\sigma_{i}$ を考えてみましょう。\nここで鎖という表現が理解できれば幸いですが、そうでなくても実際にはあまり関係ありません。とにかく後で重要なのは、それぞれの $n$-チェイン $\\Delta_{n} (X)$ でチェーンコンプレックスを構築することです。\n$\\Delta_{n} (X)$ は本当にグループなのか？ 非常に重要ですが、定義でチェインを説明するときに、形式的和という表現を使いました。これは $\\Delta_{n} (X)$ の要素を説明したに過ぎず、$\\Delta_{n} (X)$ 上で定義された二項演算ではありません。形式的和という言葉が示すように、これはあくまで形式的なものです。小学校の時に使っていた記法を借りてくれば、\n2😀 + 💎 - 3🍌\rのように、とりあえずその位置を絵などで埋めたものと考えても問題ありません。上の式は数学的には意味がありません。なぜなら、笑顔 😀 の2倍が何であり、そこに宝石 💎 を加えることが何であり、バナナ 🍌 を3つ引くことが何なのか、定義されておらず、定義するのも困難だからです。これらを扱うのが難しい状況は、正確に $\\sum_{\\alpha} k_{\\alpha} e_{\\alpha} \\simeq \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ で\n(そもそも加算を定義できない)オープンシンプレックス $e_{\\alpha}^{n}$ 対応する $\\sigma_{\\alpha}$ が関数である（関数そのものなのか関数値を指しているのかがわかりにくい） それを任意の整数倍して加算した $-3 e_{1}^{n} + 7 e_{2}^{n} \\simeq -3 \\sigma_{1} + 7 \\sigma_{2}$ の意味がわからない という問題と同じです。代数的構造どころか、この集合がどのように見えるのかすらわかりにくいですが、幸いにもこれらの問題は $\\Delta_{n} (X)$ にとっては関係がありません。もし\n$\\sigma=$2😀 + 💎 - 3🍌\rが $\\Delta_{n} (X)$ の要素、つまり $n$-チェインであるとするならば、これらの要素の逆元は、すべての係数 $k_{\\alpha} \\in \\left( \\mathbb{Z} , + \\right)$ の逆元 $-k_{\\alpha} \\in \\left( \\mathbb{Z} , + \\right)$ を係数として持つ\n$-\\sigma=$ (-2)😀 + (-1)💎 + (-(-3))🍌\rで定義するだけで十分です。これにより $\\Delta_{n} (X)$ の単位元は、任意の $\\sigma \\in \\Delta_{n} (X)$ に対して $0 := \\sigma + (-\\sigma)$ で定義され、$\\mathbb{Z}$ がアーベル群であるため、$\\Delta_{n} (X)$ もアーベル群になります。ここで、群 $\\left( \\Delta_{n} (X) , + \\right)$ の演算 $+$ は $\\left( \\mathbb{Z} , + \\right)$ の $+$ から導かれたものですが、同じものではありません。$n$-チェイン $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} \\in \\Delta_{n} (X)$ で登場する $\\sum$ とも異なります。\n要約すると以下のようになります。\n最初に定義したときの $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ で加算のように見えるものは、そもそも演算ではなく記法に過ぎませんでした。 $\\left( \\Delta_{n} (X) , + \\right)$ の $+$ は $\\left( \\mathbb{Z} , + \\right)$ の $+$ から導かれましたが、同じものではありません。 $\\left( \\Delta_{n} (X) , + \\right)$ は自由アーベル群であり、これで $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ も二項演算 $+$ の関数値になります。 $\\partial$ をなぜ境界と呼ぶのか？ $$ \\partial _{n} \\left( \\sigma_{\\alpha} \\right) := \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right] $$\n定義にある数式だけを見ても理解しにくいですが、以下の図を見ればすぐに理解できるでしょう。\n例えば $\\partial_{2}$ を考えると、次のような計算を行うことができます。 $$ \\begin{align*} \u0026amp; \\partial _{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\\\ =\u0026amp; \\sum_{i=0}^{2} (-1)^{i} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\setminus \\left[ v_{i} \\right] \\\\ =\u0026amp; (-1)^{0} \\left[ v_{1}, v_{2} \\right] + (-1)^{1} \\left[ v_{0}, v_{2} \\right] + (-1)^{2} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\end{align*} $$\nホモロジーグループを学ぶレベルなら、三角形 $\\left[ v_{0} ,v_{1}, v_{2} \\right]$ の境界が $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$ で構成されること自体を受け入れられない人はほとんどいないでしょう。本当に理解しにくいのは、一体 $\\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right]$ が何なのかということです。1-シンプレックスである線分同士を引くことが意味を成すのでしょうか？それをベクトルとして扱い、2-シンプレックスである三角形同士の演算はどうなるのでしょうか？\nすべて間違っています。しっかりと頭を整理してもう一度見てみましょう。$\\partial_{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\in \\Delta_{1} (X)$ は、その幾何学的な意味を離れて、単に3つの要素 $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$ の形式的和である $$ (+1) \\left[ v_{1}, v_{2} \\right] + (-1) \\left[ v_{0}, v_{2} \\right] + (+1) \\left[ v_{0}, v_{1} \\right] $$\nに過ぎません。これを順番に $$ \\begin{align*} a := \\left[ v_{1}, v_{2} \\right] \\ b:= \\left[ v_{0}, v_{2} \\right] \\ c:= \\left[ v_{0} , v_{1} \\right] \\end{align*} $$ と置くと、$\\Delta_{1} (X)$ の正体がようやく見えてきます。例えば、$1$-チェイン $x \\in \\Delta_{1} (X)$ は、ある係数 $k_{a} , k_{b} , k_{c} \\in \\mathbb{Z}$ に対して $$ x = k_{a} a + k_{b} b + k_{c} c $$ のように表される要素です。逆に $a,b,c$ の立場から自由群 $\\Delta_{1} (X) := F[\\left\\{ a,b,c \\right\\}]$ を構築する過程を考えると、$\\Delta_{1} (X)$ とは、3つの未知数で作られる群、つまり $\\mathbb{Z}^{3} \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} \\oplus \\mathbb{Z}$ と同型な群に過ぎないことがわかります。\nこのような考え方の転換は、続く例を理解する上で必須です。幾何を置いて、代数的に考えましょう。\n例 $$ \\begin{align*} \\\\ \\partial_{n} :\u0026amp; \\Delta_{n} (X) \\to \\Delta_{n-1} (X) \\\\ H_{n}^{\\Delta} (X) =\u0026amp; \\ker \\partial_{n} / \\text{Im} \\partial_{n+1} \\end{align*} $$\n特に $n = 0$ の場合、$\\partial_{0} : \\Delta_{0} \\left( X \\right) \\to 0$ なので $\\ker \\partial_{0} = \\Delta_{0} \\left( X \\right)$ です。\n円 $S^{1}$ $1$-ユニットスフィア、つまり円 $X = S^{1}$を考えると、$0$-シンプレックスは頂点 $v$ 一つ、$1$-シンプレックスはエッジ $e$ 一つ、$n \\ge 2$ で $n$-シンプレックスは存在しないので、チェーンコンプレックス自体は以下のように構成されるでしょう。\n$$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{1}\\left( S^{1} \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( S^{1} \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\n自由群 $\\Delta_{1}\\left( S^{1} \\right)$ は $e$ 一つで生成されるので $\\Delta_{1}\\left( S^{1} \\right) \\simeq \\mathbb{Z}$ であり、$\\Delta_{0}\\left( S^{1} \\right)$ も $v$ 一つで生成されるので $\\Delta_{0}\\left( S^{1} \\right) \\simeq \\mathbb{Z}$ です。一方 $$ \\partial e = v - v = 0 $$ なので $\\partial_{1}$ はゼロ準同型です。\n$n = 0$ の場合、$\\ker \\partial_{0} = \\Delta_{0} \\left( S^{1} \\right)$ であり、$\\partial_{1}$ がゼロ準同型なのでその像は $\\left\\{ 0 \\right\\}$ となり、以下が得られます。 $$ \\begin{align*} H_{0}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n = 1$ の場合、$\\partial_{2}$ の定義域が $0$ なので $\\text{Im} \\partial_{2} = \\left\\{ 0 \\right\\}$ であり、$\\partial_{1}$ がゼロ準同型なので $\\ker \\partial_{1}$ はその定義域である $\\Delta_{1} \\left( S^{1} \\right)$ 自体となり、以下が得られます。 $$ \\begin{align*} H_{1}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{1} / \\text{Im} \\partial_{2} \\\\ \\simeq\u0026amp; \\Delta_{1} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n \\ge 2$ に対しては、$H_{n}^{\\Delta} \\left( S_{1} \\right) \\simeq 0$ なので、以下のように要約できます。 $$ H_{n}^{\\Delta} \\left( S_{1} \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0, 1 \\\\ 0 \u0026amp; , \\text{if } n \\ge 2 \\end{cases} $$\nトーラス $T^{2}$ 上の図のようなトーラス $T^{2}$を考えると、$0$-シンプレックスは頂点 $v$ 一つ、$1$-シンプレックスはエッジ $a$、$b$、$c$ 三つ、$2$-シンプレックスは $U$、$L$ 二つ、$n \\ge 3$ で $n$-シンプレックスは存在しないので、チェーンコンプレックス自体は以下のように構成されるでしょう。\n$$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{2}\\left( T \\right) \\overset{\\partial_{2}}{\\longrightarrow} \\Delta_{1}\\left( T \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( T \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\nこれにより、自由群 $\\Delta_{n} \\left( T \\right)$ は $$ \\Delta_{n} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z}^{1} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z}^{3} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z}^{2} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\nとなります。一方、エッジ $a$、$b$、$c$ の両端点は $v$ に接続されているので $$ \\begin{align*} \\partial a =\u0026amp; v - v = 0 \\\\ \\partial b =\u0026amp; v - v = 0 \\\\ \\partial c =\u0026amp; v - v = 0 \\end{align*} $$ であり、円の場合と同様に $\\partial_{1}$ はゼロ準同型です。\n$n = 0$ の場合、円の場合と同様に以下が成立します。 $$ \\begin{align*} H_{0}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( T \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n = 1$ の場合、$\\partial_{1}$ がゼロ準同型なので $\\ker \\partial_{1}$ はその定義域である $\\Delta_{1} \\left( T \\right)$ 自体です。一方で $\\partial_{2} : \\Delta_{2}\\left( T \\right) \\to \\Delta_{1}\\left( T \\right)$ について $$ \\partial_{2} U = a + b - c = \\partial_{2} L $$ であり、$\\left\\{ a, b, a + b - c \\right\\}$ は $\\Delta_{1}\\left( T \\right)$ の基底なので $H_{1}^{\\Delta}$ は$a$ と $b$ で生成される自由群と同型です。つまり、以下が成立します。 $$ H_{1}^{\\Delta} \\left( T \\right) \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} $$\n$n = 2$ の場合、$\\partial_{3}$ の定義域が $0$ なので $\\text{Im} \\partial_{3} = \\left\\{ 0 \\right\\}$ であり、$\\partial_{2} : \\Delta_{2}\\left( T \\right) \\to \\Delta_{1}\\left( T \\right)$ では $\\Delta_{2}\\left( T \\right) \\simeq \\mathbb{Z}^{2}$ で $\\Delta_{1}\\left( T \\right) \\simeq \\mathbb{Z}^{3}$ なので $\\ker \\partial_{2} \\simeq \\mathbb{Z}^{3-2}$ です。これを整理すると、以下が得られます。 $$ \\begin{align*} H_{2}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{2} / \\text{Im} \\partial_{3} \\\\ \\simeq\u0026amp; \\mathbb{Z}^{3-2} / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n \\ge 3$ に対しては、$H_{n}^{\\Delta} \\left( T \\right) \\simeq 0$ なので、以下のように要約できます。 $$ H_{n}^{\\Delta} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z} \\oplus \\mathbb{Z} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\n定理 $H_{n}^{\\Delta}$ はホモロジーグループである ホモロジーグループの定義:\n$n \\in \\mathbb{N}_{0}$ とします。アーベル群 $C_{n}$ と準同型 $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェーン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ がすべての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、$\\mathcal{C} := \\left\\{ \\left( C_{n}, \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ をチェーンコンプレックスと呼びます。 商群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の第 $n$ ホモロジーグループと呼びます。 準同型 $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界または微分オペレータと呼びます。 $$ \\cdots \\longrightarrow \\Delta_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} \\Delta_{n} \\overset{\\partial_{n}}{\\longrightarrow} \\Delta_{n-1} \\longrightarrow \\cdots $$\nチェーンコンプレックス $\\left\\{ \\left( \\Delta_{n} (X) , \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ に対して $H_{n}^{\\Delta} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ はホモロジーグループです。つまり、すべての $n \\in \\mathbb{N}$ に対して $\\partial_{n} \\circ \\partial_{n+1}$ はゼロ準同型です。\n証明 $\\sigma \\in \\Delta_{n}$ に $\\partial_{n-1} \\circ \\partial_{n}$ を適用してみると、以下が得られます。 $$ \\begin{align*} \u0026amp; \\left( \\partial_{n-1} \\circ \\partial_{n} \\right) \\left( \\sigma \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\partial_{n} \\left( \\sigma \\right) \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , v_{n} \\right] \\right) \\\\ =\u0026amp; \\sum_{j \u0026lt; i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ \u0026amp; + \\left( -1 \\right) \\sum_{j \u0026gt;i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\n実際、このような証明は、一般的に証明するよりも、帰納的な例を示すことがより役立ちます。 $$ \\begin{align*} \u0026amp; \\partial_{1} \\left( \\partial_{2} \\left[ v_{0}, v_{1} , v_{2} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left( \\left[ v_{1} , v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left[ v_{1} , v_{2} \\right] - \\partial_{1} \\left[ v_{0}, v_{2} \\right] + \\partial_{1} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{2} \\right] - \\left[ v_{1} \\right] - \\left( \\left[ v_{2} \\right] - \\left[ v_{0} \\right] \\right) + \\left[ v_{1} \\right] - \\left[ v_{0} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\n■\nHatcher. (2002). Algebraic Topology: p104~106.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2383,"permalink":"https://freshrimpsushi.github.io/jp/posts/2383/","tags":null,"title":"シンプリシアルホモロジーグループの定義"},{"categories":"줄리아","contents":"概要 ブロードキャスティングは Juliaで最も重要な概念の一つであり、ベクトル化されたコードを書く際に非常に便利な文法だ1。二項演算の前に.を置いたり、関数の後に.を置くことで使用する。これは点ごとに関数を適用するという意味であり、その目的にぴったりの表現だ。\nプログラミング的にブロードキャスティングは、マップとリデュースのマップを使いやすくしたものと見ることができる。\nコード 二項演算 二項演算には.を付けて使用する。例えば、行列$A \\in \\mathbb{Z}_{9}^{3 \\times 4}$の全要素にスカラ$a \\in \\mathbb{R}$を足すコードは以下の通りだ。\njulia\u0026gt; A = rand(0:9, 3,4)\r3×4 Matrix{Int64}:\r5 6 3 3\r7 4 8 8\r0 2 2 7\rjulia\u0026gt; a = rand()\r0.23234165065465284\rjulia\u0026gt; A .+ a\r3×4 Matrix{Float64}:\r5.23234 6.23234 3.23234 3.23234\r7.23234 4.23234 8.23234 8.23234\r0.232342 2.23234 2.23234 7.23234 一般関数 julia\u0026gt; f(x) = x^2 - 1\rf (generic function with 3 methods)\rjulia\u0026gt; f(a)\r-0.9460173573710713 例えば関数$f : \\mathbb{R} \\to \\mathbb{R}$を考えてみよう。これはスカラ関数なので、$a \\in \\mathbb{R}$に対して上記のようにうまく計算される。\njulia\u0026gt; f(A)\rERROR: LoadError: DimensionMismatch しかし、行列$A$を入れてみるとLoadErrorが発生する。考えてみれば、行列の平方、特に$A \\in \\mathbb{Z}_{9}^{3 \\times 4}$のような直方体の行列の平方とは何か、という問題から始まる。そのため、$f(x) = x^{2} - 1$のような関数に無闇に入れることはできない。しかし、行列$A$の全ての値に対してそれぞれ平方を取り、その後$1$を引いた行列を得たい場合は、f.のように点を打つことで、行列の全要素に関数$f : \\mathbb{R} \\to \\mathbb{R}$を適用できる。\njulia\u0026gt; f.(A)\r3×4 Matrix{Int64}:\r24 35 8 8\r48 15 63 63\r-1 3 3 48 速度比較 多くの場合、ブロードキャスティングは性能面でも優れている。しかし、速度を性能の基準として性能を評価する部分には、かなり難しい点があるので、必ず以下の内容を確認してほしい。\n例として、以下は1から10万までの数に平方根を取るコードだ。\njulia\u0026gt; @time for x in 1:100000\rsqrt(x)\rend\r0.000001 seconds\rjulia\u0026gt; @time sqrt.(1:100000);\r0.000583 seconds (2 allocations: 781.297 KiB) 単純な速度だけを比較すると、ブロードキャスティングはforループよりも約500倍遅い。しかし、これは単純な計算から得られたベンチマークで、保存するプロセスまで含めた場合は話が変わる。\njulia\u0026gt; z = []\rAny[]\rjulia\u0026gt; @time for x in 1:100000\rpush!(z, sqrt(x))\rend\r0.005155 seconds (100.01 k allocations: 3.353 MiB)\rjulia\u0026gt; @time y = sqrt.(1:100000);\r0.000448 seconds (2 allocations: 781.297 KiB) 保存するプロセスを含めても、ブロードキャスティングを適用したコードには変わりはないが、空の配列に値を追加しなければならない反復文の場合、ベクトル化されたコードと比べて約10倍遅いことがわかる。これはsqrt()自体よりもpush!()が動的配列を扱う際に消費するコストが大きいと言えるが、とにかく結果としてブロードキャスティング側が速い。当然、反復文をより速くする方法もあるが（例えばAny[]がFloat64[]に変わるだけで改善されるだろう）、実際に遭遇するほとんどのコーディングで、ブロードキャスティングを使用する方が扱いやすく、速度面でも優れている。\nこれは単なる概念的な部分を超え、ジュリアがインタープリターよりもコンパイラ言語に近い1こととも関連している。次のループで何が起こるかわからないfor反復文よりも、タイプとサイズが具体的に決まっているベクトルに対してコンパイルする方が、コンパイラにとって楽ではないだろうか？\n99%程度の関数では、私たちが独自に反復文を使うよりも、ジュリアを作った人たちが考案した方法をそのまま使う方が速いと断言できる。コードを無理にベクトル化する必要はないが、ベクトル化できるコードなら、ほとんどの場合、ベクトル化した方が圧倒的に\u0026hellip;本当に圧倒的に速い。これはジュリアに限らず、マトラボ、Rのようにベクトル演算に特化した言語なら誰もが持っている特徴だが、関数型プログラミングのパラダイムを最もよく受け入れている新しい言語であり、速度面で自信を示している点が異なるだけだ。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/arrays/#Broadcasting\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2382,"permalink":"https://freshrimpsushi.github.io/jp/posts/2382/","tags":null,"title":"ジュリアのブロードキャスティング文法"},{"categories":"양자정보이론","contents":"定義1 以下のようなベクトル値ブール関数をフレドキンゲートFredkin gateと呼ぶ。\n$$ F : \\left\\{ 0, 1 \\right\\}^{3} \\to \\left\\{ 0, 1 \\right\\}^{3} $$\n$$ F (a, b, c) = \\Big(a, (\\lnot a \\land b) \\lor (a \\land c), (\\lnot a \\land c) \\lor (a \\land b) \\Big) $$\n$\\text{CSWAP}$ ゲートControlled SWAP(CSWAP) gateとも呼ばれる。 説明 エドワード・フレドキンEdward Fredkinによって紹介された。フレドキンゲートは、最初の入力を変えずに、最初の入力が$1$の場合には、残りの二つの値を交換swapして出力する。その具体的な計算は次のようである。\n$$ \\begin{align*} F (0,0,0) \u0026amp;= (0, (\\lnot 0 \\land 0) \\lor (0 \\land 0), (\\lnot 0 \\land 0) \\lor (0 \\land 0)) = (0, 0 \\lor 0, 0 \\lor 0) = (0, 0, 0) \\\\ F (0,0,1) \u0026amp;= (0, (\\lnot 0 \\land 0) \\lor (0 \\land 1), (\\lnot 0 \\land 1) \\lor (0 \\land 0)) = (0, 0 \\lor 0, 1 \\lor 0) = (0, 0, 1) \\\\ F (0,1,0) \u0026amp;= (0, (\\lnot 0 \\land 1) \\lor (0 \\land 0), (\\lnot 0 \\land 0) \\lor (0 \\land 1)) = (0, 1 \\lor 0, 0 \\lor 0) = (0, 1, 0) \\\\ F (0,1,1) \u0026amp;= (0, (\\lnot 0 \\land 1) \\lor (0 \\land 1), (\\lnot 0 \\land 1) \\lor (0 \\land 1)) = (0, 1 \\lor 0, 1 \\lor 0) = (0, 1, 1) \\\\ F (1,0,0) \u0026amp;= (1, (\\lnot 1 \\land 0) \\lor (1 \\land 0), (\\lnot 1 \\land 0) \\lor (1 \\land 0)) = (1, 0 \\lor 0, 0 \\lor 0) = (1, 0, 0) \\\\ F (1,0,1) \u0026amp;= (1, (\\lnot 1 \\land 0) \\lor (1 \\land 1), (\\lnot 1 \\land 1) \\lor (1 \\land 0)) = (1, 0 \\lor 1, 0 \\lor 0) = (1, 1, 0) \\\\ F (1,1,0) \u0026amp;= (1, (\\lnot 1 \\land 1) \\lor (1 \\land 0), (\\lnot 1 \\land 0) \\lor (1 \\land 1)) = (1, 0 \\lor 0, 0 \\lor 1) = (1, 0, 1) \\\\ F (1,1,1) \u0026amp;= (1, (\\lnot 1 \\land 1) \\lor (1 \\land 1), (\\lnot 1 \\land 1) \\lor (1 \\land 1)) = (1, 0 \\lor 1, 0 \\lor 1) = (1, 1, 1) \\\\ \\end{align*} $$\n上の表を見れば、$F$が可逆関数であることと、$F$を二回合成すると恒等関数になることが容易にわかる。\n$$ \\operatorname{Id} = F \\circ F $$\nまた、$\\left\\{ F \\right\\}$が機能的に完全であるため、$F$は汎用ゲートである。\nブール関数\rシンボル\r$F$\r真理値表\r入力\r出力\r$a$\r$b$\r$c$\r$a$\r$ (\\lnot a \\land b) \\lor (a \\land c)$\r$ (\\lnot a \\land c) \\lor (a \\land b)$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r整理 (射影と注入を制約なく使えると仮定すると)フレドキンゲート$F$は汎用ゲートである。つまり、$\\left\\{ F \\right\\}$は機能的に完全である。\n証明 定理\n$\\text{NOT}$ゲートと$\\text{AND}$ゲートの集合$\\left\\{ \\lnot, \\land \\right\\}$は機能的に完全である。\n上の定理に従って、射影、注入、$F$を適切に使用して$\\text{NOT}$、$\\text{AND}$を表現できることを示せば、証明は終了する。\n$\\text{NOT}$ゲート\n$$ \\lnot = p_{0} \\circ p_{1} \\circ F \\circ \\jmath_{2} \\circ \\imath_{1} $$\nが成り立つ。まず$\\jmath_{2} \\circ \\imath_{1} (a) = (a, 0, 1)$であるため、次を得る。\n$$ \\begin{equation} F \\circ \\jmath_{2} \\circ \\imath_{1}(a) = F(a, 0, 1) = (a, a, \\lnot a) \\end{equation} $$\nここで、最初の二つの値を消去するために$p_{0} \\circ p_{1}$を取ると、\n$$ p_{0} \\circ p_{1} \\circ F \\circ \\jmath_{2} \\circ \\imath_{1} (a) = p_{0} \\circ p_{1} (a, a, \\lnot a) = \\lnot a $$\n※ さらに$(1)$に$p_{2}$を適用すると、複製関数を得る。\n$\\text{AND}$ゲート\n$$ \\land = p_{0} \\circ p_{1} \\circ F \\circ \\imath_{2} $$\nが成り立つ。まず$F \\circ \\jmath_{2} (a, b)$は次のようである。\n$$ \\begin{align*} F \\circ \\jmath_{2} (a, b) = F(a, b, 0) \u0026amp;= (a, (\\lnot a \\land b) \\lor (a \\land 0), (\\lnot a \\land 0) \\lor (a \\land b)) \\\\ \u0026amp;= (a, (\\lnot a \\land b) \\lor 0, 0 \\lor (a \\land b)) \\\\ \u0026amp;= (a, \\lnot a \\land b, a \\land b) \\end{align*} $$\nしたがって、$p_{0} \\circ p_{1}$を取ると、\n$$ p_{0} \\circ p_{1} \\circ F \\circ \\imath_{2} (a, b) = p_{0} \\circ p_{1} (a, \\lnot a \\land b, a \\land b) = a \\land b $$\n■\n関連項目 $\\text{AND}$ゲート論理積 $\\text{OR}$ゲート論理和 $\\text{NOT}$ゲート論理否定 $\\text{XOR}$ゲート排他的論理和 $\\text{NAND}$ゲート否定論理積 $\\text{NOR}$ゲート否定論理和 $\\operatorname{CNOT}$ゲート トフォリゲート$\\text{CCNOT}$ゲート 金永勳·許在成, 量子情報理論 (2020), p90-93\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3412,"permalink":"https://freshrimpsushi.github.io/jp/posts/3412/","tags":null,"title":"フレドキン・CSWAPゲート"},{"categories":"양자정보이론","contents":"定義1 以下のようなベクトル値ブール関数をトフォリゲートToffoli gateと呼ぶ。\n$$ T : \\left\\{ 0, 1 \\right\\}^{3} \\to \\left\\{ 0, 1 \\right\\}^{3} $$\n$$ T (a, b, c) = (a, b, (a \\land b) \\oplus c) $$\n$\\text{CCNOT}$ ゲートControlled Controlled NOT(CCNOT) gateとも呼ばれる。 説明 トフォリゲートでは、最初の二つの入力が両方とも$1$であれば、三番目の入力が反転する。その他の場合は、入力と出力が同じである。具体的な計算は以下の通りである。\n$$ \\begin{align*} T (0,0,0) \u0026amp;= (0, 0, (0 \\land 0) \\oplus 0) = (0, 0, 0 \\oplus 0) = (0, 0, 0) \\\\ T (0,0,1) \u0026amp;= (0, 0, (0 \\land 0) \\oplus 1) = (0, 0, 0 \\oplus 1) = (0, 0, 1) \\\\ T (0,1,0) \u0026amp;= (0, 1, (0 \\land 1) \\oplus 0) = (0, 1, 0 \\oplus 0) = (0, 1, 0) \\\\ T (0,1,1) \u0026amp;= (0, 1, (0 \\land 1) \\oplus 1) = (0, 1, 0 \\oplus 1) = (0, 1, 1) \\\\ T (1,0,0) \u0026amp;= (1, 0, (1 \\land 0) \\oplus 0) = (1, 0, 0 \\oplus 0) = (1, 0, 0) \\\\ T (1,0,1) \u0026amp;= (1, 0, (1 \\land 0) \\oplus 1) = (1, 0, 0 \\oplus 1) = (1, 0, 1) \\\\ T (1,1,0) \u0026amp;= (1, 1, (1 \\land 1) \\oplus 0) = (1, 1, 1 \\oplus 0) = (1, 1, 1) \\\\ T (1,1,1) \u0026amp;= (1, 1, (1 \\land 1) \\oplus 1) = (1, 1, 1 \\oplus 1) = (1, 1, 0) \\\\ \\end{align*} $$\n上の表を見れば$T$が可逆関数であることと$T$を二回合成すると恒等関数になることが容易に分かる。\n$$ \\operatorname{Id} = T \\circ T $$\nまた、$\\left\\{ T \\right\\}$が機能的に完全であるため、$T$は汎用ゲートである。\n부울 함수\r기호\r진리표\r$T$\r입력\r출력\r$a$\r$b$\r$c$\r$a$\r$b$\r$(a \\land b) \\oplus c$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$0$\r整理 (射影と注入を制約なく使えると仮定すると)トフォリゲート$T$は汎用ゲートである。つまり$\\left\\{ T \\right\\}$は機能的に完全である。\n証明 定理\n複製関数を許可すると、$\\left\\{ \\uparrow \\right\\}$は機能的に完全である。つまり$\\text{NAND}$ゲート$\\uparrow$は汎用ゲートである。\n上の定理に従い、射影、注入、$T$を適切に使用して複製関数$\\text{cl}$と$\\text{NAND}$ゲートを表現できることを示せば証明が完了する。\n複製関数\n$$ \\operatorname{cl} = p_{1} \\circ T \\circ \\imath_{2} \\circ \\jmath_{1} $$\nが成り立つ。まず$T \\circ \\imath_{2} \\circ \\jmath_{1} (a)$を計算すると以下のようになる。\n$$ T \\circ \\imath_{2} \\circ \\jmath_{1} (a) = T \\circ \\imath_{2} (a, 1) = T (a, 1, 0) $$\nここで$a = 1$であれば$T(1, 1, 0) = (1, 1, 1)$であり、$a = 0$であれば$T(0, 1, 0) = (0, 1, 0)$であるため、次が成り立つ。\n$$ T(a, 1, 0) = (a, 1, a) $$\nしたがって$p_{1}$を取れば、\n$$ p_{1} \\circ T \\circ \\imath_{2} \\circ \\jmath_{1} (a) = p_{1} (a, 1, a) = (a, a) = \\operatorname{cl}(a) $$\n$\\text{NAND}$ゲート\n$$ p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} = \\uparrow \\\\ p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} (a, b) = a \\uparrow b $$\nが成り立つ。順に計算すると以下のようになる。\n$$ \\begin{align*} p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} (a, b) \u0026amp;= p_{0} \\circ p_{1} \\circ T (a, b, 1) \\\\ \u0026amp;= p_{0} \\circ p_{1} (a, b, (a \\land b) \\oplus 1) \\\\ \u0026amp;= p_{0} (a, (a \\land b) \\oplus 1) \\\\ \u0026amp;= (a \\land b) \\oplus 1 \\\\ \u0026amp;= \\lnot(a \\land b) = a \\uparrow b \\end{align*} $$\n最後の行は$\\text{XOR}$ゲートの性質によって成り立つ。\n■\n木村泰宏・許在成, 量子情報理論 (2020), p89-93\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3411,"permalink":"https://freshrimpsushi.github.io/jp/posts/3411/","tags":null,"title":"トッフォリ/CCNOTゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなベクトル値ブール関数を**$\\operatorname{CNOT}$ゲート**Controlled NOT(CNOT) gateという。\n$$ \\operatorname{CNOT} : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\}^{2} $$\n$$ \\operatorname{CNOT} (a,b) = (a, a \\oplus b) $$\nファインマンゲートFeynman gateとも呼ばれる。2 説明 $\\operatorname{CNOT}$ゲートの入出力の具体的な計算は次のようになる。\n$$ \\begin{align*} \\operatorname{CNOT} (0,0) \u0026amp;= (0, 0 \\oplus 0) = (0, 0) \\\\ \\operatorname{CNOT} (0,1) \u0026amp;= (0, 0 \\oplus 1) = (0, 1) \\\\ \\operatorname{CNOT} (1,0) \u0026amp;= (1, 1 \\oplus 0) = (1, 1) \\\\ \\operatorname{CNOT} (1,1) \u0026amp;= (1, 1 \\oplus 1) = (1, 0) \\end{align*} $$\n上の表を見ると、$\\operatorname{CNOT}$が可逆関数であることと、$\\operatorname{CNOT}$を二回合成すると恒等関数になることが容易に分かる。\n$$ \\operatorname{Id} = \\operatorname{CNOT} \\circ \\operatorname{CNOT} $$\n出力の二番目の値だけを見ると、$\\text{XOR}$ゲートと同じであるため、可逆$\\text{XOR}$ゲートとも呼ばれる。\n부울 함수\r기호\r진리표\r$\\operatorname{CNOT}$\r입력\r출력\r$a$\r$b$\r$a$\r$a \\oplus b$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$0$\rキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p88-89\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Controlled_NOT_gate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3410,"permalink":"https://freshrimpsushi.github.io/jp/posts/3410/","tags":null,"title":"制御NOT(CNOT)ゲート"},{"categories":"위상데이터분석","contents":"定義 難しい定義 1 $$ \\Delta^{k} \\in K $$\n有限なシンプレックスの集合 $K$ が以下の二つの条件を満たす場合、シンプリシャル・コンプレックスSimplicial Complexと言う。\n(i): もし $\\sigma \\in K$ かつ $\\tau$ が $\\sigma$ のフェイスならば、$\\tau \\in K$ だ。 $$ \\sigma \\in K \\land \\tau \\le \\sigma \\implies \\tau \\in K $$ (ii): もし $\\sigma_{1}, \\sigma_{2} \\in K$ ならば、$\\sigma_{1} \\cap \\sigma_{2}$ は空集合か$\\sigma_{1}$ と $\\sigma_{2}$ のフェイスだ。 $$ \\sigma_{1} , \\sigma_{2} \\in K \\implies \\left( \\sigma_{1} \\cap \\sigma_{2} = \\empty \\right) \\lor \\left( \\sigma_{1} \\cap \\sigma_{2} \\le \\sigma_{1} \\land \\sigma_{1} \\cap \\sigma_{2} \\le \\sigma_{2} \\right) $$ $\\land$ は論理的に‘そして’を表す論理積記号だ。 $\\lor$ は論理的に‘または’を表す論理和記号だ。 シンプレックス $x$ のフェイスとは、$x$ から1点を取り除いて作られるシンプレックスを言う。 シンプレックス $\\tau$、$\\sigma$ に対して $\\tau \\le \\sigma$ というのは、$\\tau$ が $\\sigma$ のフェイスFaceであることを意味する。 簡単な定義 シンプレックスを連結した集合であり、すべての連結部分がシンプレックスであるコンプレックスをシンプリシャル・コンプレックスという。\n説明 シンプレックスはそれ自体で意味と役割があるが、シンプリシャル・コンプレックスを構成することで、ほぼすべての抽象的な対象の幾何学的特性の近似Approximationを得ることができる。2 例えば、以下はイルカの形状のトライアングレーションTriangulation、すなわち最大 $2$-シンプレックス(三角形)を集めて作ったシンプリシャル・コンプレックスだ。\n簡単な定義によると、集合を連結しているという表現はかなり曖昧であり、多くの文書や講演でこのようにざっくりとした定義を紹介してはそれをすんなりと超えてしまっている。これは、シンプリシャル・コンプレックスの実用的、応用的な面を説明するにあたって、厳密な定義を詳しく追求するよりも、図を1つ見せる方が理解しやすく説明もしやすいからだ。\nもちろん、一人で本を開いて勉強する時は、難しい定義を正確に理解する必要がある。シンプリシャル・コンプレックス $K$ はそもそも$k$ 個のアフィン独立な点の凸包であるシンプレックス $\\Delta^{k}$ の集合であり、集合の集合であるファミリーであるため、$\\sigma_{1} \\cap \\sigma_{2}$ といった共通集合を考えることができる。\nポリゴン 定義によれば、ポリゴンはシンプリシャル・コンプレックスのように見えるが、四角形などが含まれているため、シンプリシャル・コンプレックスではない。\n関連項目 集合 $K$ が与えられた条件をすべて満たした場合にシンプリシャル・コンプレックスであり、具体的にどのような外観であるべきかという指定はない。シンプレックスの定義方法によって、数え切れないほど多くのコンプレックスを想像することができ、同じ点（データ）を持っていても、それらの実践的なPractical特性によってシンプリシャル・コンプレックスは千差万別だ。\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p63.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Triangulation_(topology)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2379,"permalink":"https://freshrimpsushi.github.io/jp/posts/2379/","tags":null,"title":"単体複合体の定義"},{"categories":"양자정보이론","contents":"定義1 ブール関数の集合$\\left\\{ f_{k} \\right\\} = \\left\\{ f_{k} : \\left\\{ 0, 1 \\right\\}^{n_{k}} \\to \\left\\{ 0, 1 \\right\\} \\right\\}_{k\\in \\Gamma}$が与えられたとしよう。$\\Gamma$は有限集合だ。任意のブール関数\n$$ \\left\\{ 0, 1 \\right\\}^{n} \\to \\left\\{ 0, 1 \\right\\}\\quad (n \\in \\mathbb{N}) $$\nが$\\left\\{ f_{k} \\right\\}$の合成で表現できるとき、集合$\\left\\{ f_{k} \\right\\}$を機能的に完全functionally completeだという。\n定理 $\\text{NOT}$ゲートと$\\text{AND}$ゲートの集合$\\left\\{ \\lnot, \\land \\right\\}$は機能的に完全だ。\n説明 つまり、全てのブール関数は$\\text{NOT}$と$\\text{AND}$の繰り返しで作ることができる。機能的に完全な集合は一意ではなく、無数にある。例えば$a \\land b = \\lnot(\\lnot a \\lor \\lnot b)$であるため、$\\left\\{ \\lnot, \\lor \\right\\}$も機能的に完全な集合だ。この定理から、任意の集合が$\\text{AND}$ゲートと$\\text{NOT}$ゲートを作ることができるかだけ示せば、機能的に完全な集合であることを証明できる。\n機能的に完全な集合の例:\n$\\text{NAND}$ゲート $\\text{NOR}$ゲート $T$(トフォリゲート) $F$(フレドキンゲート) $\\left\\{ \\lnot, \\lor \\right\\}$ $\\left\\{ \\oplus, \\land \\right\\}$ もちろん複製関数が必要な場合もある。これらの結果をコンピュータ回路の言葉で述べれば以下のようになる。全ての論理演算は:\n$\\text{NOT}$ゲートと$\\text{AND}$ゲートのみを使用して回路に実装できる。 $\\text{NAND}$ゲートと複製関数のみを使用して回路に実装できる。 $\\text{NOR}$ゲートと複製関数のみを使用して回路に実装できる。 トフォリゲートのみを使用して回路に実装できる。 フレドキンゲートのみを使用して回路に実装できる。 ユニバーサルゲート 特に、自分自身だけで構成された集合$\\left\\{ f \\right\\}$が機能的に完全なとき、そのような$f$をユニバーサルゲートという。ユニバーサルゲートは自分自身だけで全てのブール関数を表現できる。$\\text{NAND}$、$\\text{NOR}$、$T$、$F$はユニバーサルゲートだ。\n金英勳・許在成, 量子情報理論 (2020), p88\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3408,"permalink":"https://freshrimpsushi.github.io/jp/posts/3408/","tags":null,"title":"関数的に完全な集合とは何か？"},{"categories":"위상수학","contents":"定義 上述のような写像Mapによって、$1$スフィアの二乗である四角形$S^{1} \\times S^{1} = [0,1] \\times [0,1]$と、位相同型である商空間$T$をトーラスTorusという。図にあるように、一番右のドーナツ形がトーラスの一例である。\n説明 トーラスは、数学全般において非常に貴重に扱われる空間―具体的には図形である。一般に広く知られているトポロジーのイメージ(ドーナツはコーヒーカップと位相同型であるなど)に欠かせない存在である。\nプアンカレ予想 プアンカレ予想は、フランスの偉大な数学者ポアンカレPoincaréによって提案され、グリゴリー・ペレルマンГриго́рий Перельма́нによって証明された。\nプアンカレ予想: ある閉じた$3$次元の多様体上のすべての単純閉曲線がひとつの点に縮約することができるならば、その空間は球体に変形することができる。\nトーラスがこの予想に必要不可欠というわけではないが、専門家でなくてもすぐに理解できる簡単な例がトーラスである。たとえば、上の画像のようにトーラスの面に赤い糸で輪を作ったとする。中央に穴が開いているドーナツの穴のため、これを一点に縮約させることは不可能である。プアンカレ予想は、逆にこのような閉曲線を常に一点に縮小できる時、それがドーナツの穴のない球であることを保証できるかどうかを問うものである。\n代数トポロジー 実際に、トーラスを作るには、シンプレックスのコンプレックス、つまりシンプリシャルコンプレックスまでは不要で、四角形$S^{1} \\times S^{1}$だけで十分である。しかし、$\\Delta$-コンプレックス構造を持ち、それに関する有意義な代数的探究を行うには、後で説明する$6$の写像Mapが必要である。\nこれは上から見たトーラスの投影図である。$\\sigma_{a}$、$\\sigma_{b}$、$\\sigma_{v}$は、トーラスを作る際に、一種の「骨格」となる写像である。$\\sigma_{b}$は四角形を丸めて円筒を作り、$\\sigma_{a}$はその円筒の両端を結びつけてドーナツを作る。このとき四角形の頂点は正確に一点に集まる必要があり、$\\sigma_{v}$がその役割を果たす。\nこれはトーラスを横から見た投影図である。$\\sigma_{U}$、$\\sigma_{L}$は、骨格の間を埋める「面」をマッピングしている。繰り返しになるが、$\\sigma_{c}$はトーラスを考える上で必ずしも必要ではなく、四角形を二つの三角形の合併と見たときにその境界を担う写像である。\n周期境界条件 トーラスは、周期境界条件Periodic Boundary Conditionが与えられた単位正方形$[0,1] \\times [0,1]$と見なすこともできる。次の図で、ゴルファーの球は一見$S^{1} \\times S^{1}$の境界を越えて脱出するように見えるが、これがトーラス上でのショットならば、球は背後に落ちることになる。\n当然、これは左右の境界だけでなく、上下の周期境界$b$でも起こる。四角形をトーラスと見なすことは、このように「境界に周期性があり、一方の端に到達すると反対側の端から現れる」ということを簡潔に表現している。\n無限平面 周期境界条件と同じ言葉だが、この空間をどのように見るかによって、新しい応用が可能になることもある。例えば、特定の生物に関する生態系の研究に際して、それらが位置している地形を無視し、それらの相互作用が全体空間のどこでも均等に発生すると仮定してみよう。\n上の図のように、トーラスの展開図である四角形を境界に合わせて配置することを想像してみよ。単一のトーラスは、このように無限平面の一部を代表するものと考えることができる。シミュレーションを行う場合、単一のトーラス上でのシミュレーションは、一般性を失わずに無限平面でのシミュレーションと見なすことができる。\n特性 座標区画写像 中心からチューブまでの距離が$R$で、チューブの直径が$r$である3次元のトーラスの座標区画写像は、$(u_{1}, u_{2}) \\in [0, 2\\pi) \\times [0, 2\\pi)$に関して、次のようである。\n$$ \\mathbf{x}(u_{1}, u_{2}) = \\left( (R + r\\cos u_{2})\\cos u_{1}, (R + r\\cos u_{2})\\sin u_{1}, r\\sin u_{1} \\right) $$\n単純連結性 トーラス$T^{2}$は単純連結ではない。\n","id":2377,"permalink":"https://freshrimpsushi.github.io/jp/posts/2377/","tags":null,"title":"数学でのトーラスとは?"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下のようなブール関数を $\\text{NOR}$ゲートNOR gateまたは否定論理和と呼び、次のように表記する。\n$$ \\downarrow : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\downarrow 0 = 1,\\quad 0\\downarrow 1 = 0,\\quad 1\\downarrow 0 = 0,\\quad 1\\downarrow 1 = 0 $$\n説明 $\\text{NOT}$ゲートと$\\text{OR}$ゲートの合成であり、$\\text{N(OT)}$と$\\text{OR}$を取り入れて$\\text{NOR}$と名付けた。\n$$ \\begin{equation} \\downarrow = \\lnot \\circ \\lor \\end{equation} $$\n$$ a \\downarrow b = \\lnot (a \\lor b) $$\n$\\text{OR}$ゲートとは逆に動作し、すべての入力が偽のときのみ真を出力する。また、$\\left\\{ \\downarrow \\right\\}$は機能的に完全であり、$(1)$により当然と言える。\n부울 함수\r기호\r진리표\r$\\text{NOR}$\r$a$\r$b$\r$a \\downarrow b$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r結論 (複製関数を許容するなら) $\\left\\{ \\downarrow \\right\\}$は機能的に完全である。言い換えると、$\\downarrow$は汎用ゲートである。\n証明 定理\n$\\text{NOT}$と$\\text{OR}$ゲートの集合$\\left\\{ \\lnot, \\lor \\right\\}$は機能的に完全である。\n上の定理に従い、複製関数$\\text{cl}$と$\\downarrow$だけで$\\text{NOT}$ゲートと$\\text{OR}$ゲートを作ることができることを示せばよい。\n$\\text{NOT}$ゲート\n$$ \\lnot = \\downarrow \\circ \\operatorname{cl} \\\\ \\lnot a = a \\downarrow a $$\nが成立する。\n$$ \\begin{align*} \\downarrow \\circ \\operatorname{cl}(0) = 0 \\downarrow 0 = 1 = \\lnot 0 \\\\ \\downarrow \\circ \\operatorname{cl}(1) = 1 \\downarrow 1 = 0 = \\lnot 1 \\\\ \\end{align*} $$\n$\\text{OR}$ゲート\n$$ \\lor = \\downarrow \\circ \\operatorname{cl} \\circ \\downarrow \\\\ a \\lor b = (a \\downarrow b) \\downarrow (a \\downarrow b) $$\nが成立する。\n$$ \\begin{align*} (0 \\downarrow 0) \\downarrow (0 \\downarrow 0) = (1 \\downarrow 1) = 0 = 0 \\lor 0 \\\\ (0 \\downarrow 1) \\downarrow (0 \\downarrow 1) = (0 \\downarrow 0) = 1 = 0 \\lor 1 \\\\ (1 \\downarrow 0) \\downarrow (1 \\downarrow 0) = (0 \\downarrow 0) = 1 = 1 \\lor 0 \\\\ (1 \\downarrow 1) \\downarrow (1 \\downarrow 1) = (1 \\downarrow 1) = 1 = 1 \\lor 1 \\\\ \\end{align*} $$\n■\nキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3407,"permalink":"https://freshrimpsushi.github.io/jp/posts/3407/","tags":null,"title":"否定論理和、NORゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下のブール関数を**$\\text{NAND}$ゲート**NAND gate、または否定論理積と呼び、次のように記す。\n$$ \\uparrow : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\uparrow 0 = 1,\\quad 0\\uparrow 1 = 1,\\quad 1\\uparrow 0 = 1,\\quad 1\\uparrow 1 = 0 $$\n説明 $\\text{NOT}$ゲートと$\\text{AND}$ゲートの合成であり、$\\text{N(OT)}$と$\\text{AND}$を引用して$\\text{NAND}$と命名されている。\n$$ \\begin{equation} \\uparrow = \\lnot \\circ \\land \\end{equation} $$\n$$ a \\uparrow b = \\lnot (a \\land b) $$\n$\\text{AND}$ゲートとは逆に動作し、すべての入力が真のときのみ偽を出力する。また、$\\left\\{ \\uparrow \\right\\}$は関数的に完全であるとされ、$(1)$により当然であると考えられる。\n부울 함수\r기호\r진리표\r$\\text{NAND}$\r$a$\r$b$\r$a \\uparrow b$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r定理 (複製関数を許容すると) $\\left\\{ \\uparrow \\right\\}$は関数的に完全である。つまり、$\\uparrow$はユニバーサルゲートである。\n証明 定理\n$\\text{NOT}$と$\\text{AND}$ゲートのセット$\\left\\{ \\lnot, \\land \\right\\}$は関数的に完全である。\n上記の定理に従い、複製関数$\\text{cl}$と$\\uparrow$のみで$\\text{NOT}$ゲートと$\\text{AND}$ゲートを作ることができることを示せばよい。\n$\\text{NOT}$ゲート\n$$ \\lnot = \\uparrow \\circ \\operatorname{cl} \\\\ \\lnot a = a \\uparrow a $$\nが成立する。\n$$ \\begin{align*} \\uparrow \\circ \\operatorname{cl}(0) = 0 \\uparrow 0 = 1 = \\lnot 0 \\\\ \\uparrow \\circ \\operatorname{cl}(1) = 1 \\uparrow 1 = 0 = \\lnot 1 \\\\ \\end{align*} $$\n$\\text{AND}$ゲート\n$$ \\land = \\uparrow \\circ \\operatorname{cl} \\circ \\uparrow \\\\ a \\land b = (a \\uparrow b) \\uparrow (a \\uparrow b) $$\nが成立する。\n$$ \\begin{align*} (0 \\uparrow 0) \\uparrow (0 \\uparrow 0) = (1 \\uparrow 1) = 0 = 0 \\land 0 \\\\ (0 \\uparrow 1) \\uparrow (0 \\uparrow 1) = (0 \\uparrow 0) = 0 = 0 \\land 1 \\\\ (1 \\uparrow 0) \\uparrow (1 \\uparrow 0) = (0 \\uparrow 0) = 0 = 1 \\land 0 \\\\ (1 \\uparrow 1) \\uparrow (1 \\uparrow 1) = (1 \\uparrow 1) = 1 = 1 \\land 1 \\\\ \\end{align*} $$\n■\nキム・ヨンフン, ホ・ジェソン, 量子情報理論 (2020), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3406,"permalink":"https://freshrimpsushi.github.io/jp/posts/3406/","tags":null,"title":"負論理積、NANDゲート"},{"categories":"줄리아","contents":"コード 1 julia\u0026gt; Dict([\u0026#34;a\u0026#34;, \u0026#34;bc\u0026#34;] .=\u0026gt; [2,8])\rDict{String, Int64} with 2 entries:\r\u0026#34;a\u0026#34; =\u0026gt; 2\r\u0026#34;bc\u0026#34; =\u0026gt; 8 キーKeyとバリューValueとして使いたい二つの配列が与えられた時、Dict(Key .=\u0026gt; Value)を通じて辞書を作ることができる。本質的にはペアPairを作る演算子=\u0026gt;のブロードキャスティングBroadcastingに過ぎない。\n環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/create-a-dictionary-from-arrays-of-keys-and-values/13908/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2375,"permalink":"https://freshrimpsushi.github.io/jp/posts/2375/","tags":null,"title":"ジュリアで配列から辞書を作成する方法"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなブール関数を $\\text{XOR}$ ゲートXOR gateまたは排他的論理和exclusive disjuction/orと呼び、以下のように表記する。\n$$ \\oplus : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\oplus 0 = 0,\\quad 0\\oplus 1 = 1,\\quad 1\\oplus 0 = 1,\\quad 1\\oplus 1 = 0 $$\n説明 $\\text{XOR}$ ゲートは、二つの真理値のうち一つだけが真のとき、つまり真が奇数のときに真を返す。つまり、二つの値が同じならば$0$、異なれば$1$を返すので、二つの値が同じかどうかを比較する機能を実装するのに役立つ。\n「パーセプトロンは$\\text{XOR}$問題を解くことができない」という指摘のため、AIの発展が停滞した1974年から1980年までをAIの冬AI winterと言う。\n부울 함수\r기호\r진리표\r$\\text{XOR}$\r$a$\r$b$\r$a \\oplus b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r特性 $\\text{NOT}$ ゲート、$\\text{AND}$ ゲート、$\\text{OR}$ ゲートで表現可能である。\n$$ \\begin{align*} a \\oplus b \u0026amp;= (a \\land \\lnot b) \\lor (\\lnot a \\land b) \\\\ \u0026amp;= (a \\lor b) \\land (\\lnot a \\lor \\lnot b) \\\\ \u0026amp;= (a \\lor b) \\land \\lnot (a \\land b) \\end{align*} $$\n$a \\oplus 1 = \\lnot a$が成立する。\n$a \\oplus 0 = a$が成立する。\nキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3405,"permalink":"https://freshrimpsushi.github.io/jp/posts/3405/","tags":null,"title":"排他的論理和、XORゲート"},{"categories":"위상데이터분석","contents":"概要 普通、数学ではコンプレックスComplexと言えば複素数を指すが、幾何学や位相数学ではコンプレックスと言えば、以下のような用語のことを言う。\n用語 トポロジカルにシンプルな$S$から成り立っていて、それらの交差点Intersectionは次元が低いだけで$S$と同じ種類のものを複雑なものComplexという。\n説明 この曖昧な表現から感じられるように、ちゃんとした「定義」ってわけではない。\nここでシンプルなものをシンプレックスSimplexと呼ぼうが、コンプレックスを複体Complexと呼ぼうが、小さなディテールにはこだわらなくてもいい。こちらでコンプレックスと言えば、ああ、そういうことねと思ってスルーしても構わない。コンプレックスはその方法によって種類も非常に多く、具体的な例をいくつか見ると自然と身に付くタイプの概念だ。\n","id":2374,"permalink":"https://freshrimpsushi.github.io/jp/posts/2374/","tags":null,"title":"位相数学における複体とは?"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなブール関数を $\\text{NOT}$ ゲートNOT gateまたは論理否定negationと言い、次のように表記する。\n$$ \\lnot : \\left\\{ 0, 1 \\right\\} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ \\lnot 0 = 1,\\quad \\lnot 1 = 0 $$\n説明 $\\text{NOT}$ ゲートは入力の反対を返す。\n$\\text{NOT}$ 게이트는 입력의 반대를 반환한다.\n부울 함수\r기호\r진리표\r$\\text{NOT}$\r$a$\r$\\lnot a$\r$0$\r$1$\r$1$\r$0$\rキム・ヨンフン、ホ・ジェソン, 量子情報理論 (2020), p84-85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3404,"permalink":"https://freshrimpsushi.github.io/jp/posts/3404/","tags":null,"title":"論理否定、NOTゲート"},{"categories":"줄리아","contents":"概要 Juliaは、基本的にRと同じように複素数をサポートしている。\nコード 虚数単位 im julia\u0026gt; z = 3 + 4im\r3 + 4im imは純虚数 $i = \\sqrt{-1}$ を表す。常識的に使われている四則演算は全部使える。\njulia\u0026gt; typeof(z)\rComplex{Int64}\rjulia\u0026gt; typeof(3.0 + 4.0im)\rComplexF64 (alias for Complex{Float64}) タイプをチェックすると、同じ複素数でも、どんな複素数で構成されているかが違う。まるで抽象代数で整数の場合 $\\mathbb{Z} [i]$、あるいは実数の場合 $\\mathbb{R} [i]$と区別される感じが似ている。\n実部、虚部 real(), imag() julia\u0026gt; real(z)\r3\rjulia\u0026gt; imag(z)\r4 共役複素数、モジュラス conj(), abs() julia\u0026gt; conj(z)\r3 - 4im\rjulia\u0026gt; abs(z)\r5.0 一方で、ここでのモジュラス abs()は、特に複素数に対して新たに定義されたわけではなく、絶対値そのものとして使われている点に注意。Juliaは多態性を持っているので、このような設計が自然にうまく行われている。\n一般複素関数 julia\u0026gt; cos(z)\r-27.034945603074224 - 3.851153334811777im\rjulia\u0026gt; log(z)\r1.6094379124341003 + 0.9272952180016122im 当然だが、絶対値と同様に、三角関数や対数関数も複素数 $\\mathbb{C}$ でうまく定義されており、Juliaで特別な操作なしに直接使用できる。\n全コード z = 3 + 4im\rreal(z)\rimag(z)\rconj(z)\rabs(z)\rcos(z)\rlog(z) 環境 OS: Windows julia: v1.7.0 ","id":2373,"permalink":"https://freshrimpsushi.github.io/jp/posts/2373/","tags":null,"title":"ジュリアで複素数を使用する方法"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下のようなブール関数を**$\\text{OR}$ ゲート**OR gateまたは論理和disjunctionと呼び、以下のように表記する。\n$$ \\lor : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\lor 0 = 0,\\quad 0\\lor 1 = 1,\\quad 1\\lor 0 = 1,\\quad 1\\lor 1 = 1 $$\n説明 $\\text{OR}$ ゲートは2つの真理値を1つの真理値に変換し、2つの真理値のうち一方でも真であれば真を返す。\n부울 함수\r기호\r진리표\r$\\text{OR}$\r$a$\r$b$\r$a \\lor b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$\\text{NOT}$ ゲートと$\\text{AND}$ ゲートで表現可能である。\n$$ a \\lor b = \\lnot(\\lnot a \\land \\lnot b) $$\n김영훈·허재성, 양자 정보 이론 (2020), p84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3403,"permalink":"https://freshrimpsushi.github.io/jp/posts/3403/","tags":null,"title":"論理和、ORゲート"},{"categories":"위상데이터분석","contents":"定義 1 ユークリッド空間 $\\left( \\mathbb{R}^{n} , \\left\\| \\cdot \\right\\| \\right)$では、次のような形を定義している。\n$D^{n} \\subset \\mathbb{R}^{n}$として定義されたものを、$n$-ユニットディスクと呼ぶ。 $$ D^{n} := \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{n} : \\left\\| \\mathbf{x} \\right\\| \\le 1 \\right\\} $$ $S^{n} \\subset \\mathbb{R}^{n+1}$として定義されたものを、$n$-ユニットスフィアと呼ぶ。 $$ S^{n} := \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{n+1} : \\left\\| \\mathbf{x} \\right\\| = 1 \\right\\} $$ $D^{n} \\setminus \\partial D^{n}$とホメオモーフィックな開集合 $e^{n}$も、$n$-セルとも言う。 性質 $n$-ディスクの境界は$n$-スフィアである。つまり、下記が成立する。 $$ \\partial D^{n} = S^{n-1} $$\n説明 ディスクとスフィアは、$n=2$の時点から見ると理解しやすい。表現上、$2$-ディスクは日常で接するディスク、中が全て埋まった円盤の形をしていて、$2$-スフィアはそれよりも上の$2+1$次元で、体積を持たず面積だけを持つ球自体を表す。\n定義されているように、$D^{3}$はディスクのように見えないが、ちゃんとディスクである。一方、セルは見ての通り、位相同型を通じて定義されるので、ディスクやスフィアのように集合として正確に定義される必要はない。\nこれは、セルが形やサイズ、位置などに自由であるという意味で理解してもいいし、このようにセルを考えることで、一般的に知られている位相数学の姿が明らかになる。\n$n=0$ の時 $n = 0$の場合は、$D^{0} = \\left\\{ 0 \\right\\}$であり$e^{n}$はそれにホメオモーフィックな唯一の点で構成されているが、$S^{0}$はすぐに$\\partial D^{1}$を意味するので、2つの点を持っている。\n関連項目 一般的な球の定義 一般的な球は、内積を通じてより数学的に定義することができ、実際、楕円体までも簡単に一般化できる。しかし、ディスクとスフィアが最もよく位相数学で言及されるのは、具体的な座標や幾何学的な性質があまり必要ではないからである。\nHatcher. (2002). Algebraic Topology: p xii.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2372,"permalink":"https://freshrimpsushi.github.io/jp/posts/2372/","tags":null,"title":"位相数学におけるディスクとスフィア"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなブール関数を $\\text{AND}$ ゲートAND gate、または論理積conjunctionと呼び、以下のように表記する。\n$$ \\land : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\land 0 = 0,\\quad 0\\land 1 = 0,\\quad 1\\land 0 = 0,\\quad 1\\land 1 = 1 $$\n説明 $\\text{AND}$ ゲートは二つの真理値を一つの真理値に変換し、二つの真理値が共に真の場合のみ真を返す。\n부울 함수\r기호\r진리표\r$\\text{AND}$\r$a$\r$b$\r$a \\land b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$1$\r$\\text{NOT}$ ゲート と $\\text{OR}$ ゲート で表現可能である。\n$$ a \\land b = \\lnot(\\lnot a \\lor \\lnot b) $$\nキム・ヨンフン、ホ・ジェソン、 量子情報理論 (2020)、p84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3402,"permalink":"https://freshrimpsushi.github.io/jp/posts/3402/","tags":null,"title":"論理積、ANDゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下の関数をブール関数と呼ぶ。$n \\in \\mathbb{N}$に対して,\n$$ f : \\left\\{ 0, 1 \\right\\}^{n} \\to \\left\\{ 0, 1 \\right\\} $$\nここで、$1 =$は真(True)、$0 =$は偽(False)とする。\n一般化 $n, m \\in \\mathbb{N} (m \\gt 2)$に対して,\n$$ f : \\left\\{ 0, 1 \\right\\}^{n} \\to \\left\\{ 0, 1 \\right\\}^{m} $$\nこれをベクトル値ブール関数と呼ぶ。\n説明 イギリスの数学者ジョージ・ブールの名前にちなむ。ジョージ・ブールはブール代数の創設者で、論理学を代数的に扱い、記号論理学に大きな影響を与えた。\nブール関数は、ブール演算または論理演算/接続詞とも呼ばれる。\nコンピュータ理論では、$1$と$0$はそれぞれ電気信号が「存在する」「存在しない」を意味するため、電気信号が入って出るという意味でゲートと言われる。したがって、複数のゲートの合成を回路という。\n可逆関数 ランダウアーの原理によると、情報が失われるたびにエネルギーも使用されるため、コンピューティング効率の側面から一対一対応のブール関数に関心を持つ。一対一対応のブール関数を可逆、そうでないブール関数を不可逆と呼ぶ。可逆であるためには、自明のように$m=n$でなければならない。可逆関数としては、以下がある。\n$\\operatorname{CNOT}$ゲート トフォリゲート$\\text{CCNOT}$ゲート フレドキングート$\\text{CSWAP}$ゲート 種類 $\\text{AND}$ゲート論理積 $\\text{OR}$ゲート論理和 $\\text{NOT}$ゲート論理否定 $\\text{XOR}$ゲート排他的論理和 $\\text{NAND}$ゲートナンド $\\text{NOR}$ゲートノール\n${}$ クローニング関数 $\\text{cl}$ 射影 $p_{i}$ 注入$\\imath_{i}$, $\\jmath_{i}$ キム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3401,"permalink":"https://freshrimpsushi.github.io/jp/posts/3401/","tags":null,"title":"ブール関数"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r정의1 이산확률변수 $X$에 대해서, $X=x$인 사건의 정보(량)information $I$를 다음과 같이 정의한다.\n$$ \\begin{equation} I(x) = -\\log_{2} p(x) \\end{equation} $$\n$p$는 $X$의 확률질량함수이다.\n설명 추상적 개념인 정보에 대한 정량적인 정의를 제시한 사람은 디지털 논리회로 이론과 정보이론을 창시한 클래드 섀넌Claude Shannon이다. 정보량를 '확률의 마이너스 로그'로 정의한 것을 처음 볼 때는 이해가 안되겠지만, 설명을 듣고 나면 이보다 자연스러울 수 없다는 생각이 들 것이다.\n정보의 가치는 일어나기 힘든 일일수록, 그러니까 일어날 확률이 희박할수록 크다. 가령 \u0026quot;내일 물리과 건물에 물리과 학과장님이 오신다\u0026quot;는 문장이 갖고 있는 정보량은 거의 없다고 볼 수 있다. 당연히 내일 학과장이 출근할 것이기 때문이다. 반면에 \u0026quot;내일 물리과 건물에 아이브가 온다\u0026quot;는 문장은 완전히 특급 정보이다. 아이브가 뜬금없이 물리과 건물에 등장할 확률은 거의 없다시피하므로, 이런 정보는 가치가 아주 높은 정보라고 할 수 있다. 다른 예로 \u0026quot;내일 삼성전자의 주식 상승폭이 $1 \\%$ 포인트 이내이다\u0026quot;는 거의 가치가 없는 정보이겠지만, \u0026quot;내일 삼성전자의 주식이 상한가를 친다\u0026quot;는 엄청난 정보이다. 따라서 일어날 확률이 적은 사건이 많은 정보를 갖고있다고 볼 수 있다.\n확률의 함숫값은 $0 \\le p \\le 1$이므로, $p$가 작을수록 정보의 함숫값이 커지도록 하려면 마이너스 로그를 취하면 된다. 따라서 자연스럽게 정보를 $(1)$과 같이 정의할 수 있다.\n$-\\log_{2}(x)$의 치역이 $[0, \\infty)$이므로 확률인 $1$인 사건, 그러니까 반드시 일어나는 일은 정보량이 $0$이다. 또한 일어날 확률이 낮아질수록 정보의 가치는 계속 커진다.\n확률변수 $X$ 자체에 대한 정보량은 엔트로피라 부른다.\n같이보기 확률정보이론에서 정의되는 정보 김영훈·허재성, 양자 정보 이론 (2020), p246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3398,"permalink":"https://freshrimpsushi.github.io/jp/posts/3398/","tags":null,"title":"고전정보이론에서 정보량이란?"},{"categories":"행렬대수","contents":"定義 サイズが$m\\times n$で、全ての要素が$0$である行列を零行列zero matrixと言い、$O_{m\\times n}$または簡単に$O$と表記する。\n説明 他の表記方法には$Z_{m \\times n}$、$Z$、$\\mathbf{0}_{m\\times n}$、または$\\mathbf{0}$などがある。数字$0$と間違えないように太字で書く方が良い（実際には、ただ$O$を使うのが良い）。零行列は行列の加算における単位元である。つまり、任意の$m\\times n$行列$A$に対して、以下の式が成立する。\n$$ A + O_{m\\times n} = A = O_{m\\times n} + A $$\n","id":3394,"permalink":"https://freshrimpsushi.github.io/jp/posts/3394/","tags":null,"title":"ゼロ行列"},{"categories":"줄리아","contents":"##概要1\nframestyle 属性を使って図の軸や枠線のスタイルを変更できる。可能なオプションは次の通りだ。\n:box :semi :axes :origin :zerolines :grid :none コード デフォルト設定は :axes だ。\nusing Plots\rx = rand(10)\ry = rand(10)\rp = plot(scatter(y, title=\u0026#34;dafault\u0026#34;, label=\u0026#34;\u0026#34;), scatter(y, title=\u0026#34;:axes\u0026#34;, framestyle=:axse, label=\u0026#34;\u0026#34;), size=(600,300))\rsavefig(p, \u0026#34;default.png\u0026#34;) 各属性によるスタイルは次の通りだ。\np = scatter(fill(x, 6), fill(y, 6), framestyle=[:box :semi :origin :zerolines :grid :none], title=[\u0026#34;:box\u0026#34; \u0026#34;:semi\u0026#34; \u0026#34;:origin\u0026#34; \u0026#34;:zerolines\u0026#34; \u0026#34;:grid\u0026#34; \u0026#34;:none\u0026#34;], layout=6, label=\u0026#34;\u0026#34;, size=(800,450))\rsavefig(p, \u0026#34;framestyle.png\u0026#34;) https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3376,"permalink":"https://freshrimpsushi.github.io/jp/posts/3376/","tags":null,"title":"ジュリアプロットにおける軸のスタイルの変更方法 `framestyle`e`"},{"categories":"줄리아","contents":"概要 ジュリアで、\u0026lt;condition\u0026gt; \u0026amp;\u0026amp; \u0026lt;statement\u0026gt;は\u0026lt;condition\u0026gt;が真のとき\u0026lt;statement\u0026gt;が実行される。関数としては真の場合\u0026lt;statement\u0026gt;の結果が返され、偽の場合\u0026lt;statement\u0026gt;は評価Evaluationさえされない。\n効率的かつ簡潔にコードを書くことができる一方で、可読性が落ちる可能性がある点は受け入れなければならない。また、自分が好んで使わないとしても、他人が書いたコードを読むためには理解しておく必要がある。何の文脈もなく突然出てきたとき、このような文法を知らないと全く理解できない。\n参照 ショートサーキット コード 基本使用例 julia\u0026gt; num = []\rAny[]\rjulia\u0026gt; iseven(2) \u0026amp;\u0026amp; push!(num, 2)\r1-element Vector{Any}:\r2 2は偶数なので、push!(num, 2)が評価され空の配列numに2が入った。\nリターン julia\u0026gt; check = iseven(4) \u0026amp;\u0026amp; push!(num, 4)\r2-element Vector{Any}:\r2\r4\rjulia\u0026gt; check\r2-element Vector{Any}:\r2\r4 \u0026amp;\u0026amp;も関数として何らかの値をリターンできる。この時、checkはcheck = push!(num, 4)をリターンされたと見ることができる。\njulia\u0026gt; check = iseven(5) \u0026amp;\u0026amp; push!(num, 5)\rfalse\rjulia\u0026gt; num\r2-element Vector{Any}:\r2\r4\rjulia\u0026gt; check\rfalse 一方で\u0026lt;statement\u0026gt;が偽の場合、\u0026lt;statement\u0026gt;は評価されず\u0026amp;\u0026amp;自体がfalseをリターンした。\n否定 julia\u0026gt; iseven(6) || push!(num, 6)\rtrue \u0026amp;\u0026amp;の代わりに||を使用する。\n全コード num = []\riseven(2) \u0026amp;\u0026amp; push!(num, 2)\rcheck = iseven(4) \u0026amp;\u0026amp; push!(num, 4)\rcheck\rcheck = iseven(5) \u0026amp;\u0026amp; push!(num, 5)\rnum\rcheck\riseven(6) || push!(num, 6) 環境 OS: Windows julia: v1.6.3 ","id":2341,"permalink":"https://freshrimpsushi.github.io/jp/posts/2341/","tags":null,"title":"ジュリアで条件文を簡潔に書く方法"},{"categories":"최적화이론","contents":"ビルドアップ 1 $x_{1} , x_{2} \\ge 0$ に関して、次の線形計画問題が与えられたとしよう。 $$ \\begin{matrix} \\text{Maximize} \u0026amp; \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ \\text{subject to} \u0026amp;-\u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \u0026amp; \\le \u0026amp; 1 \\\\ \u0026amp; \u0026amp; x_{1} \u0026amp; \u0026amp; \u0026amp; \\le \u0026amp; 3 \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; x_{2} \u0026amp; \\le \u0026amp; 2 \\end{matrix} $$ つまり、与えられたすべての制約条件を満たしながら $x_{1} + x_{2}$ を最大化したい。これを方程式形式に変えるためには、スラック変数 $x_{3}, x_{4}, x_{5} \\ge 0$ を導入して $$ \\begin{matrix} \\text{Maximize} \u0026amp; \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ \\text{subject to} \u0026amp;-\u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \u0026amp; + \u0026amp; x_{3} \u0026amp; \u0026amp; \u0026amp; \u0026amp; \u0026amp; = \u0026amp; 1 \\\\ \u0026amp; \u0026amp; x_{1} \u0026amp; \u0026amp; \u0026amp; \u0026amp; \u0026amp; + \u0026amp; x_{4} \u0026amp; \u0026amp; \u0026amp; = \u0026amp; 3 \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; x_{2} \u0026amp; \u0026amp; \u0026amp; \u0026amp; \u0026amp; + \u0026amp; x_{5} \u0026amp; = \u0026amp; 2 \\end{matrix} $$ のように表せばいい。これをもう一度辞書またはテーブルに表すと、元の $x_{1}$、$x_{2}$ は非基底変数となって右側に残り、$x_{3}$、$x_{4}$、$x_{5}$ は基底変数になって左側に行く。 $$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 0 \u0026amp; + \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ x_{3} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; x_{1} \u0026amp; - \u0026amp; x_{2} \\\\ x_{4} \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; - \u0026amp; x_{1} \u0026amp; \u0026amp; \\\\ x_{5} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; \u0026amp; \u0026amp; - \u0026amp; x_{2} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 1, 2 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 3, 4, 5 \\right\\} $$ ここで、$\\zeta = \\bar{\\zeta} + x_{1} + x_{2}$ の $\\bar{\\zeta}$ は、最適解に対する目的関数の関数の値、つまり現在どのくらい最適化されているかを意味すると見なせる。以下のいくつかの方程式を通して変数置換を行うと、$\\zeta$ の右側にある非基底変数の符号を $-$ に統一できるかもしれない。すると、すべての非基底変数に $0$ を代入したとき $$ \\zeta = \\zeta_{?} - x_{?_{1}} - x_{?_{2}} $$ この形が最大の値になることだ。言い換えれば、これが実行可能基底を得ることだ。非基底変数の符号の中に $+$ があるということは、まだ $\\zeta_{?}$ を増やす可能性があることを意味し、すべての符号が $-$ であるということは、これ以上 $\\zeta_{?}$ を大きくすることはできないという意味になる。これがまさにシンプレックス・メソッドのアイデアだ。問題を代数的に解いているように見えるが、スラック変数を加えて方程式形式を作った時点で解空間がシンプレックスになるため、シンプレックスという表現を使う。\n実践 $$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 0 \u0026amp; + \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ x_{3} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; x_{1} \u0026amp; - \u0026amp; x_{2} \\\\ x_{4} \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; - \u0026amp; x_{1} \u0026amp; \u0026amp; \\\\ x_{5} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; \u0026amp; \u0026amp; - \u0026amp; x_{2} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 1, 2 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 3, 4, 5 \\right\\} \\\\ \\left( x_{1} , x_{2} , x_{3}, x_{4} , x_{5} \\right) = (0,0,1,3,2) \\\\ \\bar{\\zeta} = 0 $$\n実際に上の例をシンプレックス・メソッドで解いてみよう。\n$\\zeta$ の形を見ると $x_{1}$ と $x_{2}$ の両方を増やすことはできるが、制約条件から $x_{4} = 3 - x_{1}$ と $x_{5} = 2 - x_{2}$ を知っているので、今のところ大きな意味はなさそうだ。$x_{3}$ についての条件を参考にすると $$ x_{2} = 1 + x_{1} - x_{3} $$ 新しい辞書は次のようになる。\n$$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; 2x_{1} \u0026amp; - \u0026amp; x_{3} \\\\ x_{2} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; x_{1} \u0026amp; - \u0026amp; x_{3} \\\\ x_{4} \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; - \u0026amp; x_{1} \u0026amp; \u0026amp; \\\\ x_{5} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; - \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{3} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 1, 3 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 2, 4, 5 \\right\\} \\\\ \\left( x_{1} , x_{2} , x_{3}, x_{4} , x_{5} \\right) = (0,1,0,3,2) \\\\ \\bar{\\zeta} = 1 $$\n方程式で確認できるように、目的関数の値を元の比較で $1$ だけ増やした。この辞書を修正するプロセスを ピボットステップ2と呼び、非基底から基底に入った$x_{2}$ のような変数を入口変数、基底から非基底に出た $x_{3}$ のような変数を退出変数3と呼ぶ。ここで混乱してはいけないのは、目的関数自体を変えたわけではなく、これからも変えることはないということだ。ビルドアップからずっとそのように、既存の変数は単に他の変数を代表するだけであり、以前には見えなかった定数項が今見えるようになっただけだ。\n今の辞書で私たちがやるべきことはかなり明確だ。$x_{1}$ を増やせば、その倍の量だけ目的関数が大きくなりそうだ。同じ理由で $x_{5}$ を入口変数として使うと $$ x_{1} = 1 + x_{3} - x_{5} $$ となり、\n$$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; + \u0026amp; x_{3} \u0026amp; - \u0026amp; 2x_{5} \\\\ x_{1} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; x_{3} \u0026amp; - \u0026amp; x_{5} \\\\ x_{2} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; \u0026amp; \u0026amp; - \u0026amp; x_{5} \\\\ x_{4} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; - \u0026amp; x_{3} \u0026amp; + \u0026amp; x_{5} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 3, 5 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 1, 2, 4 \\right\\} \\\\ \\left( x_{1} , x_{2} , x_{3}, x_{4} , x_{5} \\right) = (1,2,0,2,0) \\\\ \\bar{\\zeta} = 3 $$\nとなる。$\\zeta$ の右側に $x_{3}$ がまだ増える余地があるので、$x_{3}$ を入口変数として使ってみると $$ x_{3} = 2 - x_{4} + x_{5} $$ となり、\n$$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 5 \u0026amp; - \u0026amp; x_{4} \u0026amp; - \u0026amp; x_{5} \\\\ x_{1} \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; - \u0026amp; x_{4} \u0026amp; - \u0026amp; \\\\ x_{2} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; \u0026amp; \u0026amp; - \u0026amp; x_{5} \\\\ x_{3} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; - \u0026amp; x_{4} \u0026amp; + \u0026amp; x_{5} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 4, 5 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 1, 2, 3 \\right\\} \\\\ \\left( x_{1} , x_{2} , x_{3}, x_{4} , x_{5} \\right) = (3,2,2,0,0) \\\\ \\bar{\\zeta} = 5 $$\nとなる。これで、$\\zeta$ の右側のすべての変数の符号が $-$ であり、どの変数を触っても $\\zeta$ が大きくなることはなく、最適化が完了したことがわかる。実際に、最初に与えられた線形計画問題 $$ \\begin{matrix} \\text{Maximize} \u0026amp; \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ \\text{subject to} \u0026amp;-\u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \u0026amp; \\le \u0026amp; 1 \\\\ \u0026amp; \u0026amp; x_{1} \u0026amp; \u0026amp; \u0026amp; \\le \u0026amp; 3 \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; x_{2} \u0026amp; \\le \u0026amp; 2 \\end{matrix} $$ に $x_{1} = 3$ と $x_{2} = 2$ を代入して再計算してみると、すべての制約条件をよく満たし、目的関数の値も正確に $\\zeta = \\bar{\\zeta} - 0 = 5$ であることが確認できる。\nMatousek. (2007). 線形計画法を理解して使う：p57~60。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMatousek. (2007). 線形計画法を理解して使う：p59。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVanderbei. (2020). 線形計画法(5版)：p0。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2336,"permalink":"https://freshrimpsushi.github.io/jp/posts/2336/","tags":null,"title":"線形計画法のシンプレックス法"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 2 離散確率変数 $X$が $n$個の値 $x_{1}, x_{2}, \\dots, x_{n}$を取るとする。 $X$の確率質量関数を $p$とする。すると、$X$あるいは$p$のエントロピーShannon entropy$H$を次のように定義する。\n$$ \\begin{equation} H(X) = H(p) := E\\left[ I(x_{i}) \\right] = \\sum_{i=1}^{n} p(x_{i}) I(x_{i}) = -\\sum_{i=1}^{n} p(x_{i}) \\log_{2}p(x_{i}) \\end{equation} $$\nこの時、$I$は情報量、$E$は期待値である。\n$X$が連続確率変数の場合、\n$$ H(X) = H(p) = - \\int_{-\\infty}^{\\infty} p(x)\\log_{2}p(x) dx $$\n説明 簡単に言えば、エントロピーは情報の期待値(平均)です。エントロピーを通じて、符号化の効率や通信の限界について数学的に扱うことができます。\nエントロピーは一般に無秩序度と説明されますが、ここで言う秩序とは規則、傾向、パターンなどの意味で考えれば良いです。従って、エントロピーが高いとは無秩序度が高いことを意味し、確率変数$X$に対して規則やパターンを把握することが難しいという話です。\nここで、確率が操作されたコイン投げを考えてみましょう。表が出る確率を$p$とすれば、裏が出る確率は$1-p$で、エントロピーは次のようになります。\n$$ H = -p\\log_{2}p - (1-p)\\log_{2}(1-p) $$\n$p$に対する$H$をグラフにすると、次のようになります。\n表が出る確率が$\\dfrac{1}{2}$の時、エントロピーは$H = -\\dfrac{1}{2}\\log_{2}\\dfrac{1}{2}-\\dfrac{1}{2}\\log_{2}\\dfrac{1}{2} = 1$で最大値です。つまり、コイン投げのパターンや規則をよく知ることができないという意味です。実際にコイン投げの場合、私たちはコインのどの面が出るかを確信することはできません。ここで表が出る確率が少し変わると、エントロピーが下がります。例えば、表が出る確率が$\\dfrac{95}{100}$であれば、エントロピーは約$0.28$で無秩序度が低く、つまり何らかの規則やパターン(この例ではほぼ表が出るというパターン)があるという意味です。この内容を次のようにまとめることができます。\nエントロピーが高い = 無秩序度が高い = 規則性やパターンがない = 結果を予測するのが難しい エントロピーが低い = 無秩序度が低い = 規則性やパターンがある = 結果を予測するのが容易\r上の例から予想できるように、一般的に$n$個の場合があるとすると、エントロピーが最も高くなるのは全ての確率が$\\dfrac{1}{n}$で等しい時です。\n性質 確率変数$X$が$n$個の値 $x_{1}, x_{2}, \\dots, x_{n}$を取るとする。エントロピー$H$は次のような性質を持ちます。\n$H$は凹concave関数です。 ある$x_{i}$に対して$p(x_{i}) = 1$ならば、$H(X) = 0$です。 全ての確率が$p(x_{i}) = \\dfrac{1}{n}$で同じ時、エントロピーは最大で、その値は$\\log_{2}n$です。 平均が$\\mathbf{0}$で共分散行列が$K$のランダムベクトル$X \\in \\mathbb{R}^{n}$のエントロピーについて次が成立します。 $$ \\begin{equation} H(X) \\le \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{p} \\left| K \\right| \\right] \\end{equation} $$ $\\left| K \\right|$は共分散行列の行列式です。$X$が正規分布なら等号が成立します。 平均$\\mu$と分散$\\sigma^{2}$が与えられた時、エントロピーが最大の分布は正規分布です。 確率変数$X$と推定量$\\hat{X}$に対して次が成立します。 $$ E\\left[ (X - \\hat{X})^{2} \\right] \\ge \\dfrac{1}{2\\pi e} e^{2H(X)} $$ 証明 4 便宜上$\\mathbf{x} = X$と表記しましょう。$g$を$\\displaystyle \\int g(\\mathbf{x})x_{i}x_{j} d \\mathbf{x} = K_{ij}$を満たす任意の確率密度関数とします。$\\phi$を\n正規分布$N(\\mathbf{0}, K)$の確率密度関数とします。 $$ \\phi (\\mathbf{x}) = \\dfrac{1}{\\sqrt{(2\\pi)^{p} \\left| K \\right|}} \\exp \\left( -\\dfrac{1}{2}\\mathbf{x}^{T} K^{-1} \\mathbf{x} \\right) $$ まず式$\\displaystyle \\int g(\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} = \\int \\phi (\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x}$が成立することを示します。$\\ln \\phi (\\mathbf{x})$を先に計算すると、\n$$ \\begin{align*} \\ln \\phi (\\mathbf{x}) \u0026amp;= \\ln\\dfrac{1}{\\sqrt{(2\\pi)^{p} \\left| K \\right|}} - \\dfrac{1}{2}\\mathbf{x}^{T} K^{-1} \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} x_{i} x_{j} \\end{align*} $$\n第一項はある定数$C$として表せ、第二項も$K^{-1}$に依存するある定数$a_{ji}$の二次形式として表せます。従って、\n$$ \\begin{align*} \\int g(\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} \u0026amp;= C \\int g(\\mathbf{x}) d \\mathbf{x} + \\int g(\\mathbf{x})\\sum a_{ij}x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} \\int g(\\mathbf{x}) x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij}K_{ij} \\qquad \\text{by assumption for $g$} \\end{align*} $$\nまた、\n$$ \\begin{align*} \\int \\phi (\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} \u0026amp;= C \\int \\phi (\\mathbf{x}) d \\mathbf{x} + \\int \\phi (\\mathbf{x})\\sum a_{ij}x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} \\int \\phi (\\mathbf{x}) x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij}K_{ij} \\qquad \\text{by definition of covariance} \\end{align*} $$\n相対エントロピーは常に$0$以上であるため、\n$$ \\begin{align*} 0 \u0026amp;\\le D(g \\| \\phi) \\\\ \u0026amp;= \\int g \\ln \\dfrac{g}{\\phi} \\\\ \u0026amp;= \\int g \\ln g - \\int g \\ln \\phi \\\\ \u0026amp;= - H(g) - \\int \\phi \\ln \\phi \\\\ \u0026amp;= - H(g) + H(\\phi) \\end{align*} $$\n正規分布のエントロピーは$\\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right]$なので、\n$$ H(X) = H(g) \\le H(\\phi) = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right] $$\nここで$X$を1次元確率変数としましょう。\n$$ \\begin{align*} E\\left[ (X - \\hat{X})^{2} \\right] \u0026amp;\\ge \\min_{X} E\\left[ (X - \\hat{X})^{2} \\right] \\\\ \u0026amp;= E\\left[ (X - E(X))^{2} \\right] \\\\ \u0026amp;= \\Var(X) \\end{align*} $$\n$(2)$が1次元の時、次の式を得ます。\n$$ \\begin{align*} \u0026amp;\u0026amp; H(X) \u0026amp;\\le \\dfrac{1}{2} \\ln(2\\pi e \\sigma^{2}) \\\\ \\implies \u0026amp;\u0026amp; 2H(X) \u0026amp;\\le \\ln(2\\pi e \\sigma^{2}) \\\\ \\implies \u0026amp;\u0026amp; e^{2H(X)} \u0026amp;\\le 2\\pi e \\sigma^{2} \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{2\\pi e}e^{2H(X)} \u0026amp;\\le \\sigma^{2} = \\Var(X) \\\\ \\end{align*} $$\nこの式に代入すると、\n$$ E\\left[ (X - \\hat{X})^{2} \\right] \\ge \\dfrac{1}{2\\pi e} e^{2H(X)} $$\n正規分布のエントロピー 正規分布$N(\\mu, \\sigma^{2})$のエントロピーは(自然対数を用いた場合)次のようになります。\n$$ H = \\dfrac{1}{2} \\ln (2\\pi e \\sigma^{2}) = \\ln \\sqrt{2\\pi e \\sigma^{2}} $$\n多変量正規分布$N_{n}(\\boldsymbol{\\mu}, K)$のエントロピーは次のようになります。\n$$ H = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right] = \\dfrac{1}{2}\\ln (\\det (2\\pi e K)) $$\n関連項目 確率情報理論で定義されるシャノンエントロピー 熱力学で定義されるエントロピー ギブスのエントロピー表現 キム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen M. Barnett, Quantum Information (2009), p7-10\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3400,"permalink":"https://freshrimpsushi.github.io/jp/posts/3400/","tags":null,"title":"古典情報理論におけるシャノン・エントロピー"},{"categories":"줄리아","contents":"概要 特定の値に変更する方法は、列ごとに変更するので不便で、データフレーム全体でNaNを扱うときはもっといいトリックを使ってみる価値がある。\nコード julia\u0026gt; df = DataFrame(rand(1:9,3,3), :auto) ./ DataFrame(rand(0:1,3,3), :auto)\r3×3 DataFrame\rRow │ x1 x2 x3 │ Float64 Float64 Float64\r─────┼───────────────────────────\r1 │ 5.0 Inf 7.0\r2 │ Inf 8.0 Inf\r3 │ 4.0 1.0 4.0 例えば、上のデータフレームでInfを0に置換したい場合、次のようにたった一行で変更できる。\njulia\u0026gt; ifelse.(isinf.(df), 0, df)\r3×3 DataFrame\rRow │ x1 x2 x3 │ Float64 Float64 Float64\r─────┼───────────────────────────\r1 │ 5.0 0.0 7.0\r2 │ 0.0 8.0 0.0\r3 │ 4.0 1.0 4.0 もちろん、ifelse.(isinf.(df), 0, df)のisinfをisnanに変更すればNaNを扱い、0を任意の値に変更することができる。\n全コード using DataFrames, Random\rRandom.seed!(0)\rdf = DataFrame(rand(1:9,3,3), :auto) ./ DataFrame(rand(0:1,3,3), :auto)\rifelse.(isinf.(df), 0, df) 参考 データフレームの特定の値を変更する方法 環境 OS: Windows julia: v1.6.3 ","id":2330,"permalink":"https://freshrimpsushi.github.io/jp/posts/2330/","tags":null,"title":"JuliaのデータフレームでNaNを0に置き換える方法"},{"categories":"줄리아","contents":"概要 ジュリアでのA ? B : Cは、いわゆる三項演算子Ternary Operatorで、Aが真ならB、偽ならCを返す関数だ。数学的に二項演算が関数として定義されるように、三項演算もまた関数だ。条件文と似ているけれども、このような本質的な違いがあるため、慣れれば非常に便利に使える。ただし、読みやすいコードからはちょっと離れることがあるので、無理に乱用する必要はないし、逆に気に入らなくても他の人が使うかもしれないから、ある程度は慣れておく必要がある。\nコード julia\u0026gt; x = iseven(2) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; x\r\u0026#34;even\u0026#34;\rjulia\u0026gt; y = iseven(3) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; y\r\u0026#34;odd\u0026#34; 上の命令は、与えられた数が偶数か奇数かに応じて、変数x、yに\u0026quot;even\u0026quot;または\u0026quot;odd\u0026quot;という文字列を代入している。\njulia\u0026gt; x * y\r\u0026#34;evenodd\u0026#34; 条件文じゃなく関数だから、こんなに便利なコードが書ける。同じ機能を条件文だけでやろうとすると、スコープScopeなどの問題で無駄に長くなりがちだ。\n全体のコード x = iseven(2) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; x\ry = iseven(3) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; y\rx * y 環境 OS: Windows julia: v1.6.3 ","id":2328,"permalink":"https://freshrimpsushi.github.io/jp/posts/2328/","tags":null,"title":"ジュリアの三項演算子 ? :"},{"categories":"줄리아","contents":"概要 replace!() メソッドを使えばいい1。最初の引数には変更するデータフレームのカラムが入り、二番目の引数にはペア [ペア](../2201) A =\u0026gt; B` が入る。ここで、データフレームのカラムが入ることが重要だ。\nコード julia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 보스즈\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 보스즈\r8 │ 지연 95 163 보스즈\r9 │ 진숙 99 162 쪼꼬미\r10 │ 현정 94 165 보스즈 例として使用する WJSN データフレームは上記の通りだ。\njulia\u0026gt; replace!(WJSN.member, \u0026#34;진숙\u0026#34; =\u0026gt; \u0026#34;여름\u0026#34;); WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 보스즈\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 보스즈\r8 │ 지연 95 163 보스즈\r9 │ 여름 99 162 쪼꼬미\r10 │ 현정 94 165 보스즈 :member 列の \u0026quot;진숙\u0026quot; を \u0026quot;여름\u0026quot; に変えた。ここで replace() ではなく replace!() を使用した点、データフレームそのものではなくその特定の列を指定した点に注意しよう。\njulia\u0026gt; replace!(WJSN.unit, \u0026#34;보스즈\u0026#34; =\u0026gt; \u0026#34;더블랙\u0026#34;); WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 더블랙\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 더블랙\r8 │ 지연 95 163 더블랙\r9 │ 여름 99 162 쪼꼬미\r10 │ 현정 94 165 더블랙 :unit 列の \u0026quot;보스즈\u0026quot; を \u0026quot;더블랙\u0026quot; に一括変更した。\n全体のコード using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;보스즈\u0026#34;]\r)\rWJSN\rreplace!(WJSN.member, \u0026#34;진숙\u0026#34; =\u0026gt; \u0026#34;여름\u0026#34;); WJSN\rreplace!(WJSN.unit, \u0026#34;보스즈\u0026#34; =\u0026gt; \u0026#34;더블랙\u0026#34;); WJSN 併せて見る データフレーム全体を一度にNaNを0に変える方法 環境 OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Replacing-Data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2326,"permalink":"https://freshrimpsushi.github.io/jp/posts/2326/","tags":null,"title":"ジュリアでのデータフレーム特定値の変更方法"},{"categories":"줄리아","contents":"概要 1 FreqTables.jlパッケージのfreqtable()関数を使えばいい。Rのfreq()関数と似た機能を持っている。\nコード 配列 julia\u0026gt; compartment = rand([\u0026#39;S\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;], 1000);\rjulia\u0026gt; freqtable(compartment)\r3-element Named Vector{Int64}\rDim1 │\r──────┼────\r\u0026#39;I\u0026#39; │ 316\r\u0026#39;R\u0026#39; │ 342\r\u0026#39;S\u0026#39; │ 342 上記のように配列を入れると、各階級ごとにカウントしてくれる。\nデータフレーム freqtable()は特にデータフレームに便利だ。Rでの質的変数を含む回帰分析の例と同様に、組み込みデータToothGrowthを読み込んでみよう。\njulia\u0026gt; ToothGrowth = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;ToothGrowth\u0026#34;)\r60×3 DataFrame\rRow │ Len Supp Dose │ Float64 Cat… Float64\r─────┼────────────────────────\r1 │ 4.2 VC 0.5\r2 │ 11.5 VC 0.5\r3 │ 7.3 VC 0.5\r4 │ 5.8 VC 0.5\r⋮ │ ⋮ ⋮ ⋮\r58 │ 27.3 OJ 2.0\r59 │ 29.4 OJ 2.0\r60 │ 23.0 OJ 2.0\r53 rows omitted\rjulia\u0026gt; freqtable(ToothGrowth, :Len)\r43-element Named Vector{Int64}\rLen │\r─────┼──\r4.2 │ 1\r5.2 │ 1\r5.8 │ 1\r6.4 │ 1\r⋮ ⋮\r29.5 │ 1\r30.9 │ 1\r32.5 │ 1\r33.9 │ 1\rjulia\u0026gt; freqtable(ToothGrowth, :Supp)\r2-element Named Vector{Int64}\rSupp │\r──────┼───\r\u0026#34;OJ\u0026#34; │ 30\r\u0026#34;VC\u0026#34; │ 30\rjulia\u0026gt; freqtable(ToothGrowth, :Dose)\r3-element Named Vector{Int64}\rDose │\r──────┼───\r0.5 │ 20\r1.0 │ 20\r2.0 │ 20 ToothGrowthは、ビタミンCまたはオレンジジュース:Suppを異なる量:Doseで餌として与えられたモルモットの歯の長さ:Lenを記録したデータだ。各カラムごとに頻度を計算すると、上記のようにきれいに整理される。ここで、データが必ずしもカテゴリカルデータである必要はないことが確認できる。\njulia\u0026gt; freqtable(ToothGrowth, :Supp, :Dose)\r2×3 Named Matrix{Int64}\rSupp ╲ Dose │ 0.5 1.0 2.0\r────────────┼──────────────\r\u0026#34;OJ\u0026#34; │ 10 10 10\r\u0026#34;VC\u0026#34; │ 10 10 10 もちろん、このようなテーブルはカテゴリカルデータを扱う時に最も効果的だ。:Supp, :Doseに対する頻度を計算すると、自動的に2次元にカテゴリを分けて頻度を計算してくれた。\njulia\u0026gt; freqtable(ToothGrowth, :Len, :Dose, :Supp)\r43×3×2 Named Array{Int64, 3}\r[:, :, Supp=\u0026#34;OJ\u0026#34;] =\rLen ╲ Dose │ 0.5 1.0 2.0\r───────────┼──────────────\r4.2 │ 0 0 0\r⋮ ⋮ ⋮ ⋮\r33.9 │ 0 0 0\r[:, :, Supp=\u0026#34;VC\u0026#34;] =\rLen ╲ Dose │ 0.5 1.0 2.0\r───────────┼──────────────\r4.2 │ 1 0 0\r⋮ ⋮ ⋮ ⋮\r33.9 │ 0 0 1 3つ以上のカラムに対する計算は、ただの2次元テーブルを階級の数だけリターンする。この辺りになると、データを探索したり要約する意味はほとんどなくなる。\n性能比較 julia\u0026gt; @time for t in 1:10^4\rfreqtable(compartment)\rend\r@time for t in 1:10^4\rcount(compartment .== \u0026#39;S\u0026#39;)\rcount(compartment .== \u0026#39;I\u0026#39;)\rcount(compartment .== \u0026#39;R\u0026#39;)\rend\r0.068229 seconds (340.00 k allocations: 27.466 MiB)\r0.059198 seconds (180.00 k allocations: 134.125 MiB, 36.71% gc time) テーブルを離れて、頻度数の計算自体が有用だと思われる。直接カウントすることとfreqtable()を通じて一度に頻度数を計算する速度を色々と計測してみたけど、どちらかが必ずしも速かったわけではなかった。データの量や階級の数によって、その都度前後したが、全体的にfreqtable()が遅い傾向にあった。それでも大きく遅れを取るわけではないので、速度を考慮に入れずに、使うときはよく考えて使おう。\n全コード using FreqTables\rcompartment = rand([\u0026#39;S\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;], 1000);\rfreqtable(compartment)\rusing RDatasets\rToothGrowth = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;ToothGrowth\u0026#34;)\rfreqtable(ToothGrowth, :Len)\rfreqtable(ToothGrowth, :Supp)\rfreqtable(ToothGrowth, :Dose)\rfreqtable(ToothGrowth, :Supp, :Dose)\rfreqtable(ToothGrowth, :Len, :Dose, :Supp)\r@time for t in 1:10^4\rfreqtable(compartment)\rend\r@time for t in 1:10^4\rcount(compartment .== \u0026#39;S\u0026#39;)\rcount(compartment .== \u0026#39;I\u0026#39;)\rcount(compartment .== \u0026#39;R\u0026#39;)\rend 環境 OS: Windows julia: v1.6.3 FreqTables v0.4.5 https://github.com/nalimilan/FreqTables.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2324,"permalink":"https://freshrimpsushi.github.io/jp/posts/2324/","tags":null,"title":"ジュリアで周波数を計算する方法"},{"categories":"선형대수","contents":"概要 $\\beta = v_{1}, \\dots, v_{k}$を線形変換 $T : V \\to V$の固有ベクトルの集合としよう。ならば、$T$が$\\span{\\beta}$を$\\span{\\beta}$にマッピングすることがわかる。このように自分自身を自分自身にマッピングする部分空間を不変部分空間と定義する。\n定義1 $V$をベクトル空間、$T : V \\to V$を線形変換としよう。部分空間 $W$が次の条件を満たすならば、$W$を**$T$-不変部分空間**$T$-invariant subspaceという。\n$$ T(W) \\subset W $$\nつまり、\n$$ T(v) \\in W\\quad \\forall v \\in W $$\nの$W$を$T$-不変部分空間という。\n説明 線形変換$T : V \\to V$において、次のものが$T$-不変の部分空間である。\n$\\left\\{ 0 \\right\\}$ $V$ 値域 $R(T)$ 零空間 $N(T)$ 固有空間 $E_{\\lambda}$ 1と2は自明である。全ての部分集合$A \\subset V$に対して、$T(A) \\subset R(T)$なので$R(T)$は$T$-不変である。$0 \\in N(T)$なので、$T(N(T)) \\subset N(T)$である。$T(\\lambda x) = \\lambda (\\lambda x)$なので、$T(E_{\\lambda}) \\subset E_{\\lambda}$である。\n$W$が$T : V \\to V$の不変部分空間ならば、制限写像 $T|_{W} : W \\to W$を自然に定義できる。この場合、$T|_{W}$は$T$の性質を受け継ぎ、次の定理は$T$と$T|_{W}$の間の一つの関係性を示している。簡単に言えば、$T|_{W}$の特性多項式は$T$の特性多項式の因数である。この結論自体は別の定理の系としても得られる。\n定理 $V$を$n$次元のベクトル空間、$T : V \\to V$を線形変換、$W$を$T$-不変としよう。すると、$T|_{W}$の特性多項式は$T$の特性多項式を割る。\n証明 $W$の順序基底 $\\gamma = \\left\\{ v_{1} ,\\dots, v_{k} \\right\\}$を一つ選ぶ。そして、これを$V$の順序基底 $\\beta = \\left\\{ v_{1}, \\dots, v_{k}, v_{k+1}, \\dots, v_{n} \\right\\}$に拡張しよう。$A = \\begin{bmatrix} T \\end{bmatrix}_{\\beta}$、$B_{1} = \\begin{bmatrix} T|_{W} \\end{bmatrix}_{\\gamma}$とする。すると、行列 $A$を次のようなブロック行列で表せる。\n$$ A = \\begin{bmatrix} B_{1} \u0026amp; B_{2} \\\\ O \u0026amp; B_{3} \\end{bmatrix} $$\n$f(t)$を$T$の特性多項式、$g(t)$を$T|_{W}$の特性多項式とする。すると、ブロック行列の行列式の公式によって次が得られる。（$I$は行列計算が可能な適切な次元の単位行列である。）\n$$ f(t) = \\det(A-tI) = \\det \\begin{bmatrix} B_{1}-tI \u0026amp; B_{2} \\\\ O \u0026amp; B_{3}-tI \\end{bmatrix} = g(t) \\det(B_{3}-tI) $$\n従って、$g(t)$は$f(t)$を割る。\n■\n参照 循環部分空間 Stephen H. Friedberg, Linear Algebra (4th Edition, 2002), p313-315\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3353,"permalink":"https://freshrimpsushi.github.io/jp/posts/3353/","tags":null,"title":"ベクトル空間の不変部分空間"},{"categories":"줄리아","contents":"ガイド 上のようなexample.csvファイルがあるとしよう。このデータフレームに読み込むとき、データ全体ではなく、列名だけを保持し、中身が空のデータフレームを作りたい場合がある。空のデータフレームが必要な場合があるから、このような場合も間違いなくある。\nusing CSV # 行なしでデータフレームを読み込む df_empty = CSV.read(\u0026#34;example.csv\u0026#34;, DataFrame; limit = 0) 上の実行結果で作成されたデータフレームは3つの列をそのまま持っているが、その内容物は全く持っていなかった。\nCSV.read(limit = 1)\nlimit = 1オプションを通じて、データフレームを1行だけ読んだ。 CSV.read()[[false],:]\n長さ$1$のビット配列Bit Array[false]で、どの行も参照していなかった。これによって空のデータフレームが残った。 # 列名が正常に保持されていることを確認 column_names = names(df_empty) names()関数で列名を確認すると、列名が正常に保持されていることが分かる。\n# 空のデータフレームに新しいデータを挿入 push!(df_empty, [1, \u0026#34;new_data\u0026#34;, 3.14]) push!()で新しいデータを挿入すると、元々あった配列のようにきちんと動作することが確認できる。\n単にlimit = 0をすると？ # limit 0でデータフレームを読み込み、空のデータフレームを得ようとする試み df_empty_error = CSV.read(\u0026#34;example.csv\u0026#34;, DataFrame; limit = 0) StackOverflowErrorエラーが発生する。\n環境 OS: Windows julia: v1.6.3 ","id":2322,"permalink":"https://freshrimpsushi.github.io/jp/posts/2322/","tags":null,"title":"JuliaでCSVファイルから列だけを読み込む方法"},{"categories":"줄리아","contents":"ガイド 1 using RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rdescribe(iris) describe() 関数を使えばいい。iris データを要約してみよう。\njulia\u0026gt; describe(iris)\r5×7 DataFrame\rRow │ variable mean min median max nmissing eltype\r│ Symbol Union… Any Union… Any Int64 DataType\r─────┼────────────────────────────────────────────────────────────────────────────────────────────\r1 │ SepalLength 5.84333 4.3 5.8 7.9 0 Float64\r2 │ SepalWidth 3.05733 2.0 3.0 4.4 0 Float64\r3 │ PetalLength 3.758 1.0 4.35 6.9 0 Float64\r4 │ PetalWidth 1.19933 0.1 1.3 2.5 0 Float64\r5 │ Species setosa virginica 0 CategoricalValue{String, UInt8} 環境 OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Summarizing-Data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2320,"permalink":"https://freshrimpsushi.github.io/jp/posts/2320/","tags":null,"title":"ジュリアでデータフレームの要約を見る方法"},{"categories":"줄리아","contents":"概要 JuliaのCategoricalArrays.jlパッケージは、Rのfactorと似た機能を果たす。\nコード julia\u0026gt; A = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;]\r4-element Vector{String}:\r\u0026#34;red\u0026#34;\r\u0026#34;blue\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; B = categorical(A)\r4-element CategoricalArray{String,1,UInt32}:\r\u0026#34;red\u0026#34;\r\u0026#34;blue\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; levels(B)\r3-element Vector{String}:\r\u0026#34;blue\u0026#34;\r\u0026#34;green\u0026#34;\r\u0026#34;red\u0026#34; categorical() categorical()関数で通常の配列をカテゴリカル配列にキャストCastできる。\nlevels() levels()関数では、カテゴリーを確認できる。当然、カテゴリーに重複はなく、配列にそのカテゴリーに対応する要素がなくてもカテゴリー自体は維持される。\njulia\u0026gt; B[2] = \u0026#34;red\u0026#34;; B\r4-element CategoricalArray{String,1,UInt32}:\r\u0026#34;red\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; levels(B)\r3-element Vector{String}:\r\u0026#34;blue\u0026#34;\r\u0026#34;green\u0026#34;\r\u0026#34;red\u0026#34; このように配列の状態に関係なくカテゴリーが維持される特徴は、特定のコーディングで非常に便利な特性となる。特にデータ分析と関連した作業では、データセットのサブセットSubsetを多く扱うが、その時カテゴリカル配列を知っていれば大きな助けとなる。\n最適化 わざわざlevels()を使わなくても、ただの通常の配列でunique()を使えば似たような実装はできる。\njulia\u0026gt; @time for t in 1:10^6\runique(A)\rend\r0.543157 seconds (6.00 M allocations: 579.834 MiB, 17.33% gc time)\rjulia\u0026gt; @time for t in 1:10^6\rlevels(B)\rend\r0.013324 seconds しかし、その速さは約40倍の差がある。元々配列が変化するたびにカテゴリーが更新されるため、別途計算する必要なく直接参照できる。\n全コード using CategoricalArrays\rA = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;]\rB = categorical(A)\rlevels(B)\rB[2] = \u0026#34;red\u0026#34;; B\rlevels(B)\r@time for t in 1:10^6\runique(A)\rend\r@time for t in 1:10^6\rlevels(B)\rend 環境 OS: Windows julia: v1.6.3 CategoricalArrays v0.10.2 ","id":2318,"permalink":"https://freshrimpsushi.github.io/jp/posts/2318/","tags":null,"title":"ジュリアのカテゴリカル配列"},{"categories":"기하학","contents":"定義 1 アフィン独立(../2315)な $v_{0}, v_{1} , \\cdots , v_{n} \\in \\mathbb{R}^{n+1}$ の 凸包を $n$-シンプレックス$n$-simplex $\\Delta^{n}$ と言い、ベクトル $v_{k}$ を 頂点Vertexと言う。数式的には次のようになる。 $$ \\Delta^{n} := \\left\\{ \\sum_{k} t_{k} v_{k} : v_{k} \\in \\mathbb{R}^{n+1} , t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$ $\\Delta^{n}$ から一つの頂点を取り除いてできる $n-1$-シンプレックス $\\Delta^{n-1}$ を$\\Delta^{n}$ の 面Faceと言い、$\\Delta^{n}$ の全ての面の合併を $\\Delta^{n}$ の 境界Boundaryと言って、$\\partial \\Delta^{n}$で表す。 シンプレックスの内部 $\\left( \\Delta^{n} \\right)^{\\circ} := \\Delta^{n} \\setminus \\partial \\Delta^{n}$ を オープンシンプレックスOpen Simplexと言う。 アフィン独立(../2315)とは、$v_{1} - v_{0} , v_{2} - v_{0} , \\cdots , v_{n} - v_{0}$ が線形独立であることを指す。 説明 シンプレックスは線形計画法や代数トポロジーなどで出会う概念で、その名が示す通り、単純さが特徴である。韓国語では 단체と呼ばれている。\n$n$-シンプレックス 定義から、凸包と$n$-シンプレックスの違いは与えられたベクトルがアフィン独立(../2315)であることだけである。集合 $X$ の凸包とは異なり、正確に$v_{0}, v_{1} , \\cdots , v_{n}$だけで表現され、そのようにのみ表現される図形であること。\n$$ \\left\\{ \\left( t_{0} , t_{1} , \\cdots , t_{n} \\right) \\in \\mathbb{R}^{n+1} : t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$\nこの集合を 標準 $n$-シンプレックスStandard $n$-simplexと呼ぶ。ベクトル $v_{0}, v_{1} , \\cdots , v_{n}$ の長さなどすべて無視してその組み合わせだけを表すので、標準化と呼ぶに相応しい。\n例として、$\\Delta^{n} , n = 3,2,1,0$ を見てみよう。\n見る通り、$3$-シンプレックスは四面体、$2$-シンプレックスは三角形、$1$-シンプレックスは線分、$0$-シンプレックスはただの一つの点として表される。$2$-シンプレックスの三点が一直線上にあるなどの場合は、アフィン独立(../2315)の仮定から除外される。$n \\ge 4$ の場合は、幾何学的に表すことはできないが、一般化には何の問題もない。\n境界とオープンシンプレックス 本質的に、境界とオープンシンプレックスは、距離空間で語られた境界と内部と変わらないし、表記も同一だ。\n例で、$3$-シンプレックスの四面体の面は$2$-シンプレックスの三角形として現れ、面表面という表現がぴったり合うことがわかるだろう。さらに、$1$-シンプレックスの線分の面も両端の$0$-シンプレックスの点だ。この面を集めたものを境界境界と呼ぶのも非常に合理的だ。\n$\\Delta^{3}$ の境界 $\\partial \\Delta^{3}$ とオープンシンプレックス $\\left( \\Delta^{3} \\right)^{\\circ}$ は、図のように直感的に理解しても問題ない。\nHatcher. (2002). Algebraic Topology: p103.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2317,"permalink":"https://freshrimpsushi.github.io/jp/posts/2317/","tags":null,"title":"シンプレックスの定義"},{"categories":"줄리아","contents":"ガイド RDatasets.jl パッケージを使えば大丈夫。以下は最も簡単な iris データセットを読み込む例です。基本組み込みデータセットの他にも様々なデータセットが含まれているから、GitHubをチェックするといい1。\njulia\u0026gt; using RDatasets\rjulia\u0026gt; iris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\r150×5 DataFrame\rRow │ SepalLength SepalWidth PetalLength PetalWidth Species\r│ Float64 Float64 Float64 Float64 Cat…\r─────┼─────────────────────────────────────────────────────────────\r1 │ 5.1 3.5 1.4 0.2 setosa\r2 │ 4.9 3.0 1.4 0.2 setosa\r3 │ 4.7 3.2 1.3 0.2 setosa\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮\r149 │ 6.2 3.4 5.4 2.3 virginica\r150 │ 5.9 3.0 5.1 1.8 virginica\r145 rows omitted 参考 Rで組み込みデータセットを読み込む方法 環境 OS: Windows julia: v1.6.3 https://github.com/JuliaStats/RDatasets.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2316,"permalink":"https://freshrimpsushi.github.io/jp/posts/2316/","tags":null,"title":"ジュリアでRで使用されていた組み込みデータセットを読み込む方法"},{"categories":"줄리아","contents":"## ガイド 例として`Plots.jl`パッケージのバージョンを確認してみよう。REPLで`]`キーを押すとパッケージモードに入る。ここで`status foo`と入力すれば、次のように`foo`パッケージのバージョンを確認できる。 ![20211204_193048.png](20211204_193048.png#center) ## 環境 - OS: Windows - julia: v1.6.3 ","id":2313,"permalink":"https://freshrimpsushi.github.io/jp/posts/2313/","tags":null,"title":"ジュリアでパッケージバージョンを確認する方法"},{"categories":"줄리아","contents":"概要 isempty() 関数を使用すればいい。\nコード julia\u0026gt; isempty([])\rtrue\rjulia\u0026gt; isempty(Set())\rtrue\rjulia\u0026gt; isempty(\u0026#34;\u0026#34;)\rtrue タイトルでは配列とされているが、実際には集合や文字列でも良い。\n最適化 もちろん配列が空かどうかは、length()が $0$ かどうかで確認しても構わない。\njulia\u0026gt; @time for t in 1:10^6\risempty([])\rend\r0.039721 seconds (1000.00 k allocations: 76.294 MiB, 27.85% gc time)\rjulia\u0026gt; @time for t in 1:10^6\rlength([]) == 0\rend\r0.041762 seconds (1000.00 k allocations: 76.294 MiB, 19.18% gc time) 見てのとおり、空の配列の場合、二つの方法の性能差はない。\njulia\u0026gt; x = 1:10^6;\rjulia\u0026gt; @time for t in 1:10^6\risempty(x)\rend\r0.017158 seconds\rjulia\u0026gt; @time for t in 1:10^6\rlength(x) == 0\rend\r0.043243 seconds (1000.00 k allocations: 15.259 MiB) 一方で、配列が空でない場合、上記のように2倍以上の速度差が見られる。length()は具体的な長さを返さなければならないのに対し、isempty()は最初の要素が存在するかだけを確認すれば良いので、これは当然のことである。可読性の側面や条件文を使用する状況まで考えると、isempty()を使用する方がより望ましい。\n全コード isempty([])\risempty(Set())\risempty(\u0026#34;\u0026#34;)\r@time for t in 1:10^6\risempty([])\rend\r@time for t in 1:10^6\rlength([]) == 0\rend\rx = 1:10^6\r@time for t in 1:10^6\risempty(x)\rend\r@time for t in 1:10^6\rlength(x) == 0\rend 環境 OS: Windows julia: v1.6.3 ","id":2311,"permalink":"https://freshrimpsushi.github.io/jp/posts/2311/","tags":null,"title":"ジュリアで配列が空かどうかを確認する方法"},{"categories":"위상데이터분석","contents":"定義 1 2 $n \\in \\mathbb{N}_{0}$ とする。アーベル群 $C_{n}$ とホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェーン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ が全ての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、チェインコンプレックスChain Complexと呼ぶ。 剰余群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の**$n$番目のホモロジーグループ**$n$-th Homology Groupと呼ぶ。 ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界Boundaryまたは微分Differentialオペレーターと呼ぶ。 $Z_{n} := \\ker \\partial_{n}$ の要素を**$n$-サイクル**Cycles、$B_{n} := \\text{Im} \\partial_{n+1}$ の要素を**$n$-バウンダリー**Boundaryと呼ぶ。 群 $0$ は $\\left\\{ 0 \\right\\}$ で定義されたマグマだ。つまり、空の代数構造だ。 ホモモルフィズム $\\partial^{2} = 0$ はゼロモルフィズムだ。 $\\text{Im}$ は像だ。 $\\ker$ は核だ。 説明 難しいと感じるのは当然だ。紹介された定義は非常に厳格な代数的ステートメントのみを含んでいるため、直感的な理解を得るためにはすぐに単体複体ホモロジーへ移行することをお勧めする。（それも容易ではないが）境界や微分という用語を幾何学的に見なければ、代数的な表現だけで受け入れるのは難しい。\n一般化の可能性 実際、チェインコンプレックスにおけるインデックス集合は$\\mathbb{N}_{0} = \\left\\{ 0, 1, 2, \\cdots \\right\\}$以外にも負の数への拡張はもとより、実数に対しても一般化できるとされているが、$0$を基準に負へと行くと、トポロジーや幾何学的な意味は大きく薄れる。\nホモロジーグループの存在性 定理\n$U, V, W$がベクトル空間、$T_{1} : U \\to V$、$T_{2} : V \\to W$が線形変換とする。すると次が成り立つ。\n$$ T_{2}T_{1} = 0 \\iff \\operatorname{Im} (T_{1}) \\subset \\ker (T_{2}) $$\nチェインコンプレックスの条件$\\partial_{n} \\circ \\partial_{n+1} = 0$は、通常$\\partial^{2} = 0$と略されることがよくある。$\\text{Im} \\partial_{n+1}$が何であれ$\\partial_{n}$を取れば$0$に行くということは、つまり$\\ker \\partial_{n}$が$\\text{Im} \\partial_{n+1}$を完全に包含できるほど十分に大きいという意味、言い換えれば$\\text{Im} \\partial_{n+1} \\subset \\ker \\partial_{n}$である。\n$\\partial^{2} = 0$から$\\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$が出てくるのは突然かもしれないが、歴史的に見れば、$\\ker f / \\text{Im} g$のように核を像に分割する代数構造について多くの研究があり、$\\partial^{2} = 0$はその直感的な意味よりも、洗練された表現のために定義に含まれたと考えるのが適切だ。\n境界と微分 $n$-サイクル $Z_{n}$ の「Zyklus」はドイツ語からきている。\n$\\partial_{n}$は単体の境界として見ると、その命名が自然であり、微分と呼ばれることも $$ \\lim_{h \\to 0} {{ f(x + h) - f(x) } \\over { h }} $$ において $$ \\partial \\left[ v_{0} , v_{1} \\right] = \\left[ v_{1} \\right] - \\left[ v_{0} \\right] $$ と定義されるように、差分Differenceから数式的な形で直感的に理解できる。しかし、ホモロジーグループの単純な定義だけでは理解はできない。これらの説明は、$\\partial_{n}$の具体的な定義が与えられ、その普遍的な使用法が理解された後にしか妥当しない。今はその名前自体に固執せずに進もう。\n悪評 ホモロジーは意外にも一般大衆にかなり知られている概念だ。彼らがホモロジーという単語を覚えているわけではないが、Twitterで話題となり、ソウル大学生でさえも簡単に説明できない難しい何かとして広く知られている。\nHatcher. (2002). Algebraic Topology: p106.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (1984). Elements of Algebraic Topology: p41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2310,"permalink":"https://freshrimpsushi.github.io/jp/posts/2310/","tags":null,"title":"ホモロジー群の定義"},{"categories":"줄리아","contents":"概要 地の果てまで一人で居る辛さを知ってる人は、ああ、分かるんだ\nコーディング中にわからないエラーに苦労した人は、プログラミングにおいてエラーがとても大事だということを分かっている\u0026hellip;\nJuliaでは、error() 関数や @error マクロを使ってエラーを出すことができる。現在のJulia v1.63を基準に、25種類の組み込み例外が定義されている1。\nコード julia\u0026gt; log(1 + 2im)\r0.8047189562170501 + 1.1071487177940904im 例えば、プログラムで 対数関数 $\\log$ を使う時、入力は実数だけ許されるべきだと考えられる。しかし、Juliaでは基本的に複素数に拡張された $\\log_{\\mathbb{C}}$ を提供している。プログラムがエラーなしで動いていることは良いことではない。意図しない計算は予期しない問題を引き起こすから、望まない計算が行われたら、最初からエラーを出してはいけない。\n元のlogの定義域を実数 $\\mathbb{R}$ に限定するコードを作ってみよう。\nerror() 関数 julia\u0026gt; function Rlog(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\rerror(DomainError, \u0026#34;: Rlog allow real number only\u0026#34;)\rend\rend\rRlog(1 + 2im)\rERROR: LoadError: DomainError: Rlog allow real number only\rStacktrace:\r[1] error(::Type, ::String)\r@ Base .\\error.jl:42\r[2] Rlog(x::Complex{Int64})\r@ Main c:\\admin\\REPL.jl:7\r[3] top-level scope\r@ c:\\admin\\REPL.jl:11\rin expression starting at c:\\admin\\REPL.jl:11 上の Rlogでは、入力が実数でなければDomainErrorをレイズするように限定した。\n@error マクロ julia\u0026gt; function Rlog2(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\r@error \u0026#34;Rlog2 also allow Real number only\u0026#34;\rend\rend\rRlog2(1 + 2im)\r┌ Error: Rlog2 also allow Real number only\r└ @ Main c:\\admin\\REPL.jl:17 上の Rlog2では、入力が実数でなければ、とりあえずエラーをスローするように限定した。\nレイズとスローは、どちらもエラーを起こすという意味で、大きな文脈での違いはない。レイズはPythonなどで使われる表現で、スローはJavaなどで使われる表現だ。\n全体のコード log(1 + 2im)\rfunction Rlog(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\rerror(DomainError, \u0026#34;: Rlog allow real number only\u0026#34;)\rend\rend\rRlog(1 + 2im)\rfunction Rlog2(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\r@error \u0026#34;Rlog2 also allow Real number only\u0026#34;\rend\rend\rRlog2(1 + 2im) 環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/control-flow/#Built-in-Exceptions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2309,"permalink":"https://freshrimpsushi.github.io/jp/posts/2309/","tags":null,"title":"ジュリアで例外処理する方法"},{"categories":"선형대수","contents":"概要 線形変換の特性多項式を定義する。以下の定理から、式$\\det(A - \\lambda I) = 0$を解くことが固有値を見つけることと同じであることが分かる。したがって、$\\det(A - \\lambda I)$に名前をつけることは非常に自然であり、これを特性多項式という。\n定理1 $F$を任意の体、$A \\in M_{n\\times n}(F)$とする。$\\lambda \\in F$が$A$の固有値であることは、$\\det (A-\\lambda I) = 0$であることと同値である。\n証明 $\\lambda$を$A$の固有値と仮定する。すると、\n$$ \\begin{align*} \\lambda \\text{ is eigenvalue of } A \u0026amp;\\iff \\exist \\text{non-zero } v \\text{ such that } Av = \\lambda v \\\\ \u0026amp;\\iff \\exist \\text{non-zero } v \\text{ such that } (A - \\lambda I)v = 0 \\end{align*} $$\n可逆行列である同値条件\n$A$を大きさが$n\\times n$の正方行列とする。すると、以下の命題は全て同値である。\n$A$は可逆行列である。 同次線形システム$A\\mathbf{x}=\\mathbf{0}$はただ一つの自明な解を持つ。 $\\det{A} \\ne 0$ 可逆行列である同値条件によれば、$A - \\lambda I$は可逆行列ではなく、$\\det (A - \\lambda I) = 0$である。\n定義 $A \\in M_{n \\times n}(F)$とする。多項式$f(t) = \\det(A - tI)$を$A$の特性多項式characteristic polynomialとする。$f(t) = 0$を特性方程式characteristic equationとする。\n$V$を$n$次元ベクトル空間とする。$T : V \\to V$を線形変換とする。$\\beta$を$V$の順序基底とする。$T$の特性多項式$f(t)$を$T$の行列表現の特性多項式として定義する。つまり、$f(t)$は以下の通りである。\n$$ f(t) = \\det\\left( \\begin{bmatrix} T \\end{bmatrix}_{\\beta} - t I \\right) $$\n説明 定義によれば、$T : V \\to V$の特性多項式の根は固有値そのものであり、特性多項式が分解されると、$T$は$n = \\dim(V)$個の固有値を持つ(異なるとは言っていない)。\n定義で、$T$の特性多項式は順序基底$\\beta$の選び方に依存するように思えるかもしれないが、実際にはそうではない。この理由から、線形変換$T$の特性多項式を以下のように記述することもある。\n$$ \\det (T - \\lambda I) $$\n確認しよう。$\\beta$、$\\beta^{\\prime}$を$V$の順序基底、$Q$を$\\beta$-座標を$\\beta^{\\prime}$-座標に変換する座標変換行列とすると、\n$$ \\begin{align*} \\det( \\begin{bmatrix} T \\end{bmatrix}_{\\beta} - tI) \u0026amp;= \\det( \\begin{bmatrix} T \\end{bmatrix}_{\\beta} - tI ) \\det Q^{-1} \\det Q \\\\ \u0026amp;= \\det Q^{-1} \\det( \\begin{bmatrix} T \\end{bmatrix}_{\\beta} - tI ) \\det Q \\\\ \u0026amp;= \\det \\left( Q^{-1} (\\begin{bmatrix} T \\end{bmatrix}_{\\beta} - tI) Q \\right) \\\\ \u0026amp;= \\det \\left( Q^{-1}\\begin{bmatrix} T \\end{bmatrix}_{\\beta}Q - tI \\right) \\\\ \u0026amp;= \\det \\left( \\begin{bmatrix} T \\end{bmatrix}_{\\beta^{\\prime}} - tI \\right) \\end{align*} $$\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p248\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3339,"permalink":"https://freshrimpsushi.github.io/jp/posts/3339/","tags":null,"title":"線形変換の特性多項式"},{"categories":"줄리아","contents":"概要 nrow(), ncol(), size() を使用できる。Rと違って、length()はエラーになる。\nコード julia\u0026gt; df = DataFrame(rand(100000,5), :auto)\r100000×5 DataFrame\rRow │ x1 x2 x3 x4 x5 │ Float64 Float64 Float64 Float64 Float64\r────────┼─────────────────────────────────────────────────────\r1 │ 0.474921 0.942137 0.0523668 0.588696 0.0176242\r2 │ 0.842828 0.910385 0.216194 0.794668 0.664883\r3 │ 0.0350312 0.96542 0.837923 0.920311 0.748409\r4 │ 0.613249 0.731643 0.941826 0.688649 0.161736\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮\r99998 │ 0.767794 0.242687 0.965885 0.557483 0.723849\r99999 │ 0.743936 0.67815 0.529923 0.247698 0.861302\r100000 │ 0.628269 0.252583 0.985485 0.24541 0.942741\r99993 rows omitted dfは、行が10万で、列が5つのデータフレームだ。\njulia\u0026gt; nrow(df)\r100000\rjulia\u0026gt; ncol(df)\r5\rjulia\u0026gt; size(df)\r(100000, 5) nrow()とncol()はそれぞれ行と列の数を返し、size()は行と列のサイズをタプルで返す。これを行、列の順序で参照すると、行と列のサイズを別々に知ることができる。一見すると、size()の方がずっと便利に見えるが、その性能を比較してみよう。\n最適化 julia\u0026gt; @time for i in 1:10^6\rnrow(df)\rend\r0.051730 seconds (1000.00 k allocations: 15.259 MiB)\rjulia\u0026gt; @time for i in 1:10^6\rsize(df)[1]\rend\r0.536297 seconds (3.00 M allocations: 61.035 MiB, 5.44% gc time) 上はnrow()とsize()の速度比較だ。当たり前だが、一つの機能しかしていないnrow()の方が速い。データフレームが大きくなるケース―ビッグデータを扱う場合や、size()を使ってもあまり差がないと思って時間の無駄を省ける。\nまた、コードの可読性の面では大きな差がある。nrow()とncol()は他の言語でも一般的に使用される関数名で、疑問の余地なく行、列の数だが、size()は後ろに付くインデックスのために、コードの可読性を大きく下げる。可能であれば、nrow()とncol()を使用することにしよう。\n全コード using DataFrames\rdf = DataFrame(rand(100000,5), :auto)\rnrow(df)\rncol(df)\rsize(df)\r@time for i in 1:10^6\rnrow(df)\rend\r@time for i in 1:10^6\rsize(df)[1]\rend 環境 OS: Windows julia: v1.6.3 ","id":2307,"permalink":"https://freshrimpsushi.github.io/jp/posts/2307/","tags":null,"title":"ジュリアでデータフレームのサイズを確認する方法"},{"categories":"선형대수","contents":"定義1 $V$を有限次元$F$-ベクトル空間としよう。$T : V \\to V$を線形変換とする。$\\lambda \\in F$について、 $$ Tx = \\lambda x $$ これを満たすゼロベクトルではない$x \\in V$を$T$の固有ベクトルeigenvectorと呼ぶ。\nこの時のスカラー$\\lambda \\in F$を固有ベクトル$x$に対応する固有値eigenvalueと呼ぶ。\n説明 固有ベクトルをcharacteristic vectorやproper vectorで、固有値をcharacteristic valueやproper valueで置き換えれることもあるが、筆者は見たことがない。\n固有値と固有ベクトルは、線形変換の対角化と関連がある。\n定理 $n$次元ベクトル空間$V$上の線形変換$T : V \\to V$が対角化可能であるのは、$T$の固有ベクトルからなる$V$の順序基底$\\beta$が存在する場合、かつその場合に限る。つまり$T$が対角化可能であるのは、$T$の固有ベクトルが$n$個の線形独立する場合と同じである。\nさらに$T$が対角化可能であり、$\\beta = \\left\\{ v_{1}, \\dots, v_{n} \\right\\}$が$T$の固有ベクトルの順序基底であり、$D = \\begin{bmatrix} T \\end{bmatrix}_{\\beta}$である場合、$D$は対角行列で、$D_{jj}$は$v_{j}$に対応する固有値である。\n参照 行列の固有値、固有ベクトル Stephen H. Friedberg, Linear Algebra (第4版, 2002), p245~264\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3337,"permalink":"https://freshrimpsushi.github.io/jp/posts/3337/","tags":null,"title":"有限次元の線形変換の固有値と固有ベクトル"},{"categories":"위상데이터분석","contents":"定義 1 インデックス集合 $I \\ne \\emptyset$ に対して、集合 $A := \\left\\{ a_{i} : i \\in I \\right\\}$ を アルファベットAlphabetと呼び、その要素 $a_{i} \\in A$ を レターLetterとしよう。 整数 $n \\in \\mathbb{Z}$ に対して、$a_{i}^{n}$ のような形を 音節Syllableと言い、これらの有限な並べ替えJuxtapostionである文字列 $w$ を 単語Wordとする。 音節 $a_{i}^{n} a_{i}^{m}$ は、$a_{i}^{n+m}$ のように表現でき、これを 初等縮約Elementary Contractionと言う。これ以上初等縮約できない単語を 縮約語Reduced Wordとし、特に $1 := a_{i}^{0}$ を 空単語Empty Wordとする。 アルファベット $A$ のレターで作れる全ての縮約語の集合を $F [A]$ としよう。二つの単語 $w_{1} , w_{2} \\in F[A]$ に対して、$w_{1} \\cdot w_{2}$ が縮約語形で表される 二項演算 $\\cdot : F[A]^{2} \\to F[A]$ を定義する。群 $\\left( F[A], \\cdot \\right)$ を $A$ によって生成されたフリーグループFree Group Generated by $A$と呼ぶ。 $G$ が集合 $A := \\left\\{ a_{i} : i \\in I \\right\\}$ の要素を 生成元として持つ群であり、$\\phi \\left( a_{i} \\right) = a_{i}$ の 同型 $\\phi : G \\to F [A]$ が存在するなら、$G$ を $A$ 上でフリーFree on $A$であると言い、$a_{i}$ を$G$ の フリージェネレーターFree Generatorと呼ぶ。 $A \\ne \\emptyset$ 上でフリーな群を フリーグループFree Groupと定義し、$A$ の 濃度 $|A|$ をフリーグループの ランクRankと呼ぶ。 説明 定義が長くて読むのが嫌になるかもしれないけど、実際に例を考えてみると全然難しくない。\u0026lsquo;アルファベット\u0026rsquo;や\u0026rsquo;単語\u0026rsquo;のような用語が出てくる点に戸惑わないでほしい。代数学代數学という言葉自体が「数を代わりに文字を使うことについての勉強」という意味だからだ。ここまで来て考えてみれば、集合に演算を加えて考えるというアプローチがあまりにも抽象的だったかもしれない。実際、フリーグループについて定義が終わった後は、上で出てきた単語はほとんど使わない。心配せずに例を見てみよう。\nアルファベットとレター $$ A = \\left\\{ a, b \\right\\} $$\n上のような アルファベットを考えると、レターはただの $a$ と $b$ だけだ。\n音節と単語 アルファベット $A$ に対して $$ a^{2} , b^{3}, b^{-1} $$ は全て 音節だ。これらを有限に、重複を許して並べるという意味で、並べ替えJuxtapostionという表現が使われ、定義としては、ただ 単語と言った。 $$ a^{2} b \\\\ bbab \\\\ b^{-2} a a a^{-2} b a^{-24} $$\n縮約語と空単語 例として、最後の単語 $b^{-2} a a a^{-2} b a^{-24}$ が縮約される過程を見てみよう。 $$ \\begin{align*} \u0026amp; b^{-2} a a a^{-2} b a^{-24} \\\\ =\u0026amp; b^{-2} a^{2} a^{-2} b a^{-24} \\\\ =\u0026amp; b^{-2} a^{0} b a^{-24} \\\\ =\u0026amp; b^{-2} 1 b a^{-24} \\\\ =\u0026amp; b^{-2} b a^{-24} \\\\ =\u0026amp; b^{-1} a^{-24} \\end{align*} $$ ここで $a^{0} = 1$ はまるで 単位元のように機能しており、実際にも音節がないという意味でEmpty Wordと呼ばれる。群になった後は特に $1$ をアルファベットで書く必要はない。\n$A$ によって生成されたフリーグループ ここまでのビルドアップから、$\\left( F[A], \\cdot \\right)$ は自然に 群になる。単位元は空単語 $1$ であり、全ての単語 $w$ に対して、次を満たす 逆元 $w^{-1}$ が存在する。 $$ w \\cdot w^{-1} = w^{-1}\\cdot w = 1 $$ 最初から $F[A]$ は具体的な単位元と逆元を与えられていたので、当然群だ。このようにフリーグループとは他でもない「群になるために作られた群」だ。\n$A$ 上でフリーな群 $$ F[A] = \\left\\{ \\cdots , a^{-2} b^{-1} , a^{-1} b^{-1}, a^{-1}, b^{-1} , 1 , a , b , ab , a b a \\cdots \\right\\} $$ $F[A]$ の要素を具体的に並べてみると上のようだ。ここまでの定義はもちろん直感的で理解しやすいけど、実際には $G$ そのものに興味がある。例えば、整数群 $\\left( \\mathbb{Z} , + \\right)$ を考えると、$\\left\\{ 1 \\right\\}$ の要素を 生成元として持つ巡回群であり、$F \\left[ \\left\\{ a \\right\\} \\right]$ と同型なので、$\\left\\{ a \\right\\}$ 上でフリーであると言える。\nフリーグループとランク ここまでの例から、$\\mathbb{Z}$ は単元素集合 $\\left\\{ a \\right\\}$ 上でフリーなのでランク $1$ であり、$A = \\left\\{ a,b \\right\\}$ によって生成されたフリーグループ $F[A]$ はランク $2$ だ。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p341~342.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2306,"permalink":"https://freshrimpsushi.github.io/jp/posts/2306/","tags":null,"title":"抽象代数学における自由群"},{"categories":"줄리아","contents":"概要 ネームド・タプルが使える。ネームド・タプルを作る方法は、左の括弧のすぐ後ろにセミコロン;をつけることだ。例えば、DataFrame(; x, y)とすると、カラム名が:x、:yで、内容もそれぞれx、yのデータフレームが作られる。\nコード julia\u0026gt; MyCol7 = rand(5); B = 1:5;\rjulia\u0026gt; DataFrame(; MyCol7, B)\r5×2 DataFrame\rRow │ MyCol7 B │ Float64 Int64\r─────┼─────────────────\r1 │ 0.911763 1\r2 │ 0.93374 2\r3 │ 0.116779 3\r4 │ 0.467364 4\r5 │ 0.473437 5 環境 OS: Windows julia: v1.6.3 ","id":2305,"permalink":"https://freshrimpsushi.github.io/jp/posts/2305/","tags":null,"title":"ジュリアで変数名をカラム名として持つデータフレームを作成する方法"},{"categories":"줄리아","contents":"概要 名前付きタプルは、一般的なタプルとは異なり、辞書や構造体のように使用できるタプルだ。シンボルの配列をキーとして持ち、キーを使ってバリューにアクセスしつつ、タプルのようにも使用できる。\nコード x = rand(Bool, 5); y = rand(Bool, 5);\rz = (; x, y)\rtypeof(z)\rz.x 上のコードを実行して、名前付きタプルの使用方法を確認してみよう。\njulia\u0026gt; z = (; x, y)\r(x = Bool[0, 0, 1, 1, 0], y = Bool[1, 1, 0, 0, 0])\rjulia\u0026gt; typeof(z)\rNamedTuple{(:x, :y), Tuple{Vector{Bool}, Vector{Bool}}} 名前付きタプルを簡単に作る方法は、タプルを作りながら開き括弧の直後にセミコロン ; を付けることだ。例えば、(; x) は (; x=x) と同じだ。\njulia\u0026gt; z.x\r5-element Vector{Bool}:\r0\r0\r1\r1\r0\rjulia\u0026gt; z[2]\r5-element Vector{Bool}:\r1\r1\r0\r0\r0 名前付きタプルは、上のように名前のシンボルでアクセスすることも、インデックスでアクセスすることもできる。\n環境 OS: Windows julia: v1.6.3 ","id":2303,"permalink":"https://freshrimpsushi.github.io/jp/posts/2303/","tags":null,"title":"ジュリアのネームドタプル"},{"categories":"수리통계학","contents":"定義 1 仮説検定: $$ \\begin{align*} H_{0} :\u0026amp; \\theta \\in \\Theta_{0} \\\\ H_{1} :\u0026amp; \\theta \\in \\Theta_{0}^{c} \\end{align*} $$\n検定力関数 $\\beta (\\theta)$が全ての$\\theta_{0} \\in \\Theta_{0}$と$\\theta_{1} \\in \\Theta_{0}^{c}$に対して次を満たす場合、偏りがないUnbiased検定力関数という。 $$ \\beta \\left( \\theta_{0} \\right) \\le \\beta \\left( \\theta_{1} \\right) $$ $\\mathcal{C}$が上記の仮説検定を集めた集合だとする。$\\mathcal{C}$内の検定力関数$\\beta (\\theta)$を持つ仮説検定$A$が、全ての$\\theta \\in \\Theta_{0}^{c}$と$\\mathcal{C}$の全ての仮説検定の検定力関数$\\beta ' (\\theta)$に対して $$ \\beta ' (\\theta) \\le \\beta (\\theta) $$ を満たす場合、仮説検定$A \\in \\mathcal{C}$を最も強力な検定(一様に) 最も強力な検定, UMPという。 説明 偏りがない検定力関数 $$ \\beta (\\theta) := P_{\\theta} \\left( \\mathbf{X} \\in \\mathbb{R} \\right) $$ 検定力関数は、確率$P$、正確には$X$の確率分布と棄却域$R$に応じて変わるため、定義だけからは$\\beta$の形を完全に思い浮かべるのは難しい。しかし、常識的に良い検定力関数が備えるべき性質は、帰無仮説よりも対立仮説の下で検定力帰無仮説を棄却する力が高くなるべきだ。$\\theta_{0}$と$\\theta_{1}$の選び方に関わらず、これを満たす性質を検定力関数の偏りのなさといい、このような検定力関数の関数値を比較するコンセプトは、次に紹介する最も強力な検定へと続く。\n最も強力な検定 最も強い\u0026hellip; 単なるワクワクする少年の漫画ではなく、文字通り最強の仮説検定だ。\n定義のステイトメントから、検定が最も強いということは、帰無仮説が正当に棄却されるべき全ての$\\theta \\in \\Theta_{0}^{c}$に対して、どんな検定力関数$\\beta '$を考えても、最も強力な検定の検定力関数$\\beta$が最も強力な検定力を持つということだ。\nCasella. (2001). Statistical Inference(2nd Edition): p387~388.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2293,"permalink":"https://freshrimpsushi.github.io/jp/posts/2293/","tags":null,"title":"不便検定力関数と最強力検定"},{"categories":"행렬대수","contents":"定義 $A$を行列$m \\times n$とする。\n$$ A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\\\ \\end{bmatrix} $$\nこの時、行列を切る任意の垂直線、水平線を考えよう。\n$$ A = \\left[ \\begin{array}{cc|ccc|c|} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; a_{14} \u0026amp; a_{15} \u0026amp; \\cdots \u0026amp; a_{1n-1} \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \u0026amp; a_{24} \u0026amp; a_{25} \u0026amp; \\cdots \u0026amp; a_{2n-1} \u0026amp; a_{2n} \\\\ \\hline a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \u0026amp; a_{35} \u0026amp; \\cdots \u0026amp; a_{3n-1} \u0026amp; a_{3n} \\\\ a_{41} \u0026amp; a_{42} \u0026amp; a_{43} \u0026amp; a_{44} \u0026amp; a_{45} \u0026amp; \\cdots \u0026amp; a_{4n-1} \u0026amp; a_{4n} \\\\ \\hline \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\\\ \\hline a_{m1} \u0026amp; a_{m2} \u0026amp; a_{m3} \u0026amp; a_{m4} \u0026amp; a_{m5} \u0026amp; \\cdots \u0026amp; a_{m-1n} \u0026amp; a_{mn} \\end{array} \\right] $$\nそれぞれの線で切られた部分を$A$のブロックという。\n$$ A_{11} = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{21} \u0026amp; a_{22} \\end{bmatrix},\\quad A_{12} = \\begin{bmatrix} a_{13} \u0026amp; a_{14} \u0026amp; a_{15}\\\\ a_{23} \u0026amp; a_{24} \u0026amp; a_{25} \\end{bmatrix},\\quad \\cdots,\\quad A_{kl} = \\begin{bmatrix} a_{m-1n} \u0026amp; a_{mn}\\end{bmatrix} $$\n行列$A$を次のようにブロックで表したものをブロック行列という。\n$$ A = \\begin{bmatrix} A_{11} \u0026amp; A_{12} \u0026amp; \\cdots \u0026amp; A_{1l} \\\\ A_{21} \u0026amp; A_{22} \u0026amp; \\cdots \u0026amp; A_{2l} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ A_{k1} \u0026amp; A_{k2} \u0026amp; \\cdots \u0026amp; A_{kl} \\\\ \\end{bmatrix} $$\n説明 行列をブロック行列として扱うことは、行列の計算を容易にする。実際、ブロック行列の計算も行列の計算と同じように行えばいい。\n$$ A = \\begin{bmatrix} A_{11} \u0026amp; A_{12} \\\\ A_{21} \u0026amp; A_{22} \\end{bmatrix}\\quad \\text{and} \\quad B = \\begin{bmatrix} B_{11} \u0026amp; B_{12} \\\\ B_{21} \u0026amp; B_{22} \\end{bmatrix} \\\\[1em] \\implies AB = \\begin{bmatrix} A_{11}B_{11} + A_{12}B_{21} \u0026amp; A_{11}B_{12} + A_{12}B_{22} \\\\ A_{21}B_{11} + A_{22}B_{21} \u0026amp; A_{21}B_{12} + A_{22}B_{22} \\end{bmatrix} $$\n従って、行列をゼロ行列や単位行列が含まれるブロック行列形に変えれば、行列の乗算を簡単に計算できる。\n$$ A = \\begin{bmatrix} A_{11} \u0026amp; I \\\\ O \u0026amp; A_{22} \\end{bmatrix}\\quad \\text{and} \\quad B = \\begin{bmatrix} B_{11} \u0026amp; B_{12} \\\\ B_{21} \u0026amp; B_{22} \\end{bmatrix} \\\\[1em] \\implies AB = \\begin{bmatrix} A_{11}B_{11} + B_{21} \u0026amp; A_{11}B_{12} + B_{22} \\\\ A_{22}B_{21} \u0026amp; A_{22}B_{22} \\end{bmatrix} $$\n行列を行ベクトルと列ベクトルに分けたものもブロック行列である。\n$$ \\begin{align*} A =\u0026amp; \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{r}_{1} \\\\ \\mathbf{r}_{2} \\\\ \\vdots \\\\ \\mathbf{r}_{m} \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} \\mathbf{c}_{1} \u0026amp; \\mathbf{c}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{c}_{n} \\end{bmatrix} \\end{align*} $$\n従って、$A \\mathbf{x}$を次のように表せる。\n$$ \\begin{align*} A \\mathbf{x} \u0026amp;= \\begin{bmatrix} \\mathbf{c}_{1} \u0026amp; \\mathbf{c}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{c}_{n} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix} \\\\ \u0026amp;= \\sum_{i}^{n} x_{i}\\mathbf{c}_{i} \\end{align*} $$\nまた、$A$を$m \\times p$行列とし、$\\mathbf{a}_{i}$を$A$の行ベクトルとし、$B$を$p \\times n$行列とし、$\\mathbf{b}_{i}$を$B$の列ベクトルとする。そうすると、二つの行列の積は次のようになる。\n$$ AB = \\begin{bmatrix} \\mathbf{a}_{1} \\\\ \\mathbf{a}_{2} \\\\ \\vdots \\\\ \\mathbf{a}_{m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b}_{1} \u0026amp; \\mathbf{b}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{b}_{n} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a}_{1} \\mathbf{b}_{1} \u0026amp; \\mathbf{a}_{1} \\mathbf{b}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{a}_{1} \\mathbf{b}_{n} \\\\ \\mathbf{a}_{2} \\mathbf{b}_{1} \u0026amp; \\mathbf{a}_{2} \\mathbf{b}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{a}_{2} \\mathbf{b}_{n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\mathbf{a}_{m} \\mathbf{b}_{1} \u0026amp; \\mathbf{a}_{m} \\mathbf{b}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{a}_{m} \\mathbf{b}_{n} \\\\ \\end{bmatrix} $$\n要約 $A = \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ O \u0026amp; A_{3} \\end{bmatrix}$をブロック行列とする。その行列式に対して次が成り立つ。\n$$ \\det A = \\det A_{1} \\det A_{3} $$\n結論 ブロック行列$A = \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix}$の行列式は$\\det A_{1}$と同じである。 行と列に順列をとって次のような形に表せる行列$A$を簡約可能行列と呼び、その行列式$\\det A$は$\\det B \\det D$か$-\\det B \\det D$のどちらかである。 $$ \\widetilde{A} = \\begin{bmatrix} B \u0026amp; O \\\\ C \u0026amp; D \\end{bmatrix} $$ 証明 ブロック行列$A$を次のように3つのブロック行列の積に分解できる。\n$$ \\begin{align*} A \u0026amp;= \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} IA_{1} + OO \u0026amp; IA_{2} + OI \\\\ OA_{1} + A_{3}O \u0026amp; OA_{2} + A_{3}I \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\begin{bmatrix} IA_{1} + A_{2}O \u0026amp; IO + A_{2}I \\\\ OA_{1} + IO \u0026amp; OO + II \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\begin{bmatrix} I \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\begin{bmatrix} A_{1} \u0026amp; O \\\\ O \u0026amp; I \\end{bmatrix} \\end{align*} $$\n積の行列式は行列式の積と同じであるから\n$$ \\begin{align*} \\det A \u0026amp;= \\det \\left( \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\begin{bmatrix} I \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\begin{bmatrix} A_{1} \u0026amp; O \\\\ O \u0026amp; I \\end{bmatrix} \\right) \\\\ \u0026amp;= \\det \\left( \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\right) \\det \\left( \\begin{bmatrix} I \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\right) \\det \\left( \\begin{bmatrix} A_{1} \u0026amp; O \\\\ O \u0026amp; I \\end{bmatrix} \\right) \\end{align*} $$\n行列式のラプラス展開を考えると、\n$$ \\det \\left( \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\right) = \\det A_{3},\\quad \\det \\left( \\begin{bmatrix} A_{1} \u0026amp; O \\\\ O \u0026amp; I \\end{bmatrix} \\right) = \\det A_{1}, $$\n$$ \\text{and} \\quad \\det \\left( \\begin{bmatrix} I \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\right)=1 $$\nこれにより、\n$$ \\det A = \\det A_{1} \\det A_{3} $$\n■\n","id":3323,"permalink":"https://freshrimpsushi.github.io/jp/posts/3323/","tags":null,"title":"ブロック行列"},{"categories":"수리통계학","contents":"定義 1 仮説検定: $$ \\begin{align*} H_{0} :\u0026amp; \\theta \\in \\Theta_{0} \\\\ H_{1} :\u0026amp; \\theta \\in \\Theta_{0}^{c} \\end{align*} $$\nこのような仮説検定が与えられていて、$\\alpha \\in [0,1]$ とする。\nパラメータ $\\theta$ に対して、棄却域が $R$ の関数 $\\beta (\\theta) := P_{\\theta} \\left( \\mathbf{X} \\in \\mathbb{R} \\right)$ を 検定力関数Power Functionという。 $\\sup_{\\theta \\in \\Theta_{0}} \\beta (\\theta) = \\alpha$ の場合、与えられた仮説検定を サイズSize $\\alpha$ の仮説検定という。 $\\sup_{\\theta \\in \\Theta_{0}} \\beta (\\theta) \\le \\alpha$ の場合、与えられた仮説検定を レベルLevel $\\alpha$ の仮説検定という。 説明 パワー？ 数学でパワーと言えば、冪乗 powや冪を覆う、べき字を使って冪関数 $f(x) = x^{-\\alpha}$ と言っていることが多いけど、統計学の文脈では、ただ仮説検定の検定する力Powerと考えればいい。\n検定力？ $\\beta$ は確率 $P_{\\theta}$ を通じて定義されるので、当然その値域は $[0,1]$ の部分集合だ。$\\beta (\\theta)$ の値が大きいということ―検定力、検定する力が強いということは、帰無仮説を棄却する力のことだ。対立仮説を棄却しても検定は検定だろうか？という疑問が生じるかもしれないが、本来どんな仮説検定でも、帰無仮説を基準に話すことが多いし、$R$ が帰無仮説の棄却域なので、検定とは帰無仮説が棄却されるか否かだけを気にすればいいのだ。\nこの検定力関数を通じて仮説検定の良し悪しを評価する。もっといい検定方法があれば、それを より強力なMore Powerfulと言うが、ここでの表現から、一般の数学のPowerと違って、本当に力を意味していることがわかる。数理統計学の観点から、どんな仮説検定が合理的か、効率的かを考えるのはとても自然な動機だ。\nただ、単に検定力そのものを良し悪しの指標とするわけにはいかない。例えば、どんなサンプルが入ってきても帰無仮説を棄却してしまう $\\beta (\\theta) = 1 = 100 \\%$ を考えてみれば、検定力自体はとても強いが、強力すぎて第一種の過誤（帰無仮説が真の場合に棄却してしまう過誤）を全く捕らえることができない。\nサイズとレベル 通常はサイズとレベルという言葉を区別せずに使うことが多いが、定義された場合、自然にレベル $\\alpha$ テストの集合がサイズ $\\alpha$ テストの集合を含む。この違いを細かく考察して研究する際は、用語を厳密に区別して使うべきだ。\n$\\alpha$ の意味は？サイズであれレベルであれ、$\\alpha$ が高いとは、帰無仮説が真のときに棄却される確率が大きいパラメータがあるということだ。$\\alpha$ が大きければ大きいほど、寛容に帰無仮説を棄却し、もし$\\alpha$ がとても小さければ、非常に保守的な検定となる。このような違いは棄却域によって生じる。一方で レベルLevelとこの説明で 有意水準Critical Levelが頭に浮かぶのは自然だけど、結局は別の話で、無理に結びつける必要もなく、事実結びつけてはいけない。概念的に受け入れよう。\n例: 正規分布 $$ \\begin{align*} H_{0} :\u0026amp; \\theta \\le \\theta_{0} \\\\ H_{1} :\u0026amp; \\theta \u0026gt; \\theta_{0} \\end{align*} $$ 分散が既知の正規分布 $N \\left( \\theta , \\sigma^{2} \\right)$ のランダムサンプル $X_{1} , \\cdots , X_{n}$ に対する上記のような仮説検定を考えると、zスコアがある定数 $c$ よりも大きければ、帰無仮説を棄却できるだろう。検定力関数 $\\beta$ は、$\\displaystyle {{ \\bar{X} - \\theta_{0} } \\over { \\sigma / \\sqrt{n} }}$ がある確率 $P$ を$\\theta$ に関する式に変換することで求めることができる。 $$ \\begin{align*} \\beta \\left( \\theta \\right) =\u0026amp; P_{\\theta} \\left( {{ \\bar{X} - \\theta_{0} } \\over { \\sigma / \\sqrt{n} }} \u0026gt; c \\right) \\\\ =\u0026amp; P_{\\theta} \\left( {{ \\bar{X} - \\theta } \\over { \\sigma / \\sqrt{n} }} \u0026gt; c + {{ \\theta_{0} - \\theta } \\over { \\sigma / \\sqrt{n} }} \\right) \\\\ =\u0026amp; P_{\\theta} \\left( Z \u0026gt; c + {{ \\theta_{0} - \\theta } \\over { \\sigma / \\sqrt{n} }} \\right) \\end{align*} $$ ここで、$\\displaystyle Z := {{ \\bar{X} - \\theta_{0} } \\over { \\sigma / \\sqrt{n} }}$ は標準正規分布に従う確率変数だ。\nCasella. (2001). Statistical Inference(2nd Edition): p383, 385.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2291,"permalink":"https://freshrimpsushi.github.io/jp/posts/2291/","tags":null,"title":"仮説検定の検定力関数"},{"categories":"줄리아","contents":"概要 JuliaとNumPy、PyTorch（以降、便宜上Pythonと呼ぶ）の高次元配列を扱う際、各次元が意味するものが異なるため注意が必要だ。この違いはJuliaの配列が列優先であり、Pythonの配列が行優先であるために生じる。ちなみに同じ列優先のMatlabはJuliaとの違いがないため、Matlabに慣れているユーザーは特に注意する必要はないが、Pythonに慣れている人はインデックスの間違いに注意しよう。\n配列の次元とベクトルの次元を混同して使っているので、しっかり理解しよう。 説明 1次元配列 Juliaでは、サイズが$n$の配列は$n$次元の列ベクトルを意味する。\njulia\u0026gt; ones(3)\r3-element Vector{Float64}:\r1.0\r1.0\r1.0 Pythonでは、サイズが$n$の配列は$n$次元の行ベクトルを意味する。\n\u0026gt;\u0026gt;\u0026gt; import numpy as np\r\u0026gt;\u0026gt;\u0026gt; np.ones(3)\rarray([1., 1., 1.]) 列か行かの違いはあるが、1次元配列であるため、インデックスに特に注意する点はない。\n2次元配列 表面上は2次元までは違いがないように見える。しかし、その意味は異なるので注意が必要だ。まず、Juliaでは配列の次元が後方に伸びる。つまり$(m,n)$配列とは、サイズが$m$の1次元配列（列ベクトル）が$n$個あることを意味する。具体的には$(3,2)$配列は、3次元の列ベクトルが2個あるということだ。\njulia\u0026gt; ones(3,2)\r3×2 Matrix{Float64}:\r1.0 1.0\r1.0 1.0\r1.0 1.0 また、Juliaは「列優先」なので、成分のインデックスは上から下へ先に、そして左から右に大きくなる。\njulia\u0026gt; A = reshape(range(1,6), (3,2))\r3×2 reshape(::UnitRange{Int64}, 3, 2) with eltype Int64:\r1 4\r2 5\r3 6\rjulia\u0026gt; for i ∈ 1:6\rprintln(A[i])\rend\r1\r2\r3\r4\r5\r6 一方、Pythonの配列では新しい次元が前方へ伸びる。つまり$(m,n)$配列とは、サイズが$n$の1次元配列（行ベクトル）が$m$個あるということだ。以下の結果からは、配列が行単位で区分されているのがわかる。\n\u0026gt;\u0026gt;\u0026gt; np.ones([3,2])\rarray([[1., 1.],\r[1., 1.],\r[1., 1.]]) つまり、見た目だけでは、JuliaとPythonの$(m,n)$配列はどちらも$m \\times n$の行列だが、列優先/行優先の違いのためにインデックスの順序が異なる。インデックスの方向はJuliaでは上下左右、Pythonでは左右上下だ。\n# julia에서 2차원 배열의 인덱싱은 위에서 아래로, 그 다음 좌에서 우로\rjulia\u0026gt; A = reshape(range(1,6), (3,2))\r3×2 reshape(::UnitRange{Int64}, 3, 2) with eltype Int64:\r1 4\r2 5\r3 6\r# python에서 2차원 배열의 인덱싱은 좌에서 우로, 그 다음 위에서 아래로 \u0026gt;\u0026gt;\u0026gt; np.arange(6).reshape(3,2)\rarray([[0, 1],\r[2, 3],\r[4, 5]]) 3次元配列 Juliaでは、配列の新しい次元が後ろに追加されると言った。従って、$(m,n,k)$配列は、$(m,n)$配列が$k$個あることを意味する。\njulia\u0026gt; ones(3,2,4)\r3×2×4 Array{Float64, 3}:\r[:, :, 1] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 2] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 3] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 4] =\r1.0 1.0\r1.0 1.0\r1.0 1.0 一方、Pythonでは$(m,n,k)$配列は、$(n,k)$配列が$m$個あるということだ。\n\u0026gt;\u0026gt;\u0026gt; np.ones([3,2,4])\rarray([[[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.]]]) 機械学習で 画像データで、$H=\\text{hieht}$が高さ、$W=\\text{width}$が幅、$C=\\text{channel}$がチャンネル数、$B=\\text{batch size}$がバッチサイズとした場合、PyTorchでは$(B,C,H,W)$配列であり、Juliaでは$(H,W,C,B)$である。\n環境 OS: Windows11 バージョン: Julia 1.7.1, Python 3.9.2, numpy 1.19.5 ","id":3315,"permalink":"https://freshrimpsushi.github.io/jp/posts/3315/","tags":null,"title":"ジュリア、Python（NumPy、PyTorch）の配列の次元の違い"},{"categories":"수리통계학","contents":"定義 1 パラメーターに関する命題を仮説Hypothesisという。 与えられたサンプルに基づき仮説$H_{0}$を真と受け入れるか、仮説$H_{0}$を棄却して$H_{1}$を採用する問題を仮説検定Hypothesis Testという。仮説検定では補足的な仮説$H_{0}$、$H_{1}$をそれぞれ帰無仮説Null Hypothesis、対立仮説Alternative Hypothesisと呼ぶ。 帰無仮説$H_{0}$を棄却するような、サンプル空間 $\\Omega$の部分集合$R \\subset \\Omega$を棄却域Rejection Regionという。 説明 統計学科でなくても、新入生レベルの統計学に触れることで仮説検定についての説明を受ける。それだけで十分と感じる人も多いが、上の定義はできるだけ数学的に、あいまいさなく、精密に仮説検定を説明している。\n以下の説明は、読者がすでに仮説検定というコンセプトにある程度慣れているという前提で書かれている。数理統計的に概念を掴もう。\n仮設 定義によれば、仮説はただの\u0026rsquo;言葉\u0026rsquo;ではなく、命題である。パラメーターに関する命題であるという言及が重要である。例えば「正規性検定」のようにその命名自体はパラメーターではなく分布そのものに対する検定のように見える場合でも、詳しく見ると必ずパラメーターが隠されている。例として、ハーク-ベラ・テストは正規性検定であり、実は歪度と尖度を通じて仮説検定を行う。\n仮説検定 「採用する」と下線が引かれているが、注意を促す意味である。ご存知の通り、ほとんどの教科書ではRejectとAcceptという表現を両方使用するが、ほとんどの教授は「採用」という表現に注意を促す。対立仮説を採用するとは、実際に対立仮説を真と受け入れるというよりは、帰無仮説を棄却したということであり、帰無仮説を真と受け入れるということも、帰無仮説を棄却できないということであって、積極的に「採用」するという表現は避ける方が良い。\n帰無仮説と対立仮説を定義する際に、補足的という表現を使ったが、これも$H_{0}$と$H_{1}$が必ずしも論理的否定ではないことを強調する表現である。仮説検定で帰無仮説が真ではないということは、必ずしも対立仮説が真であるという結論にはつながらない。もっと実践的に説明すると、帰無仮説と共存できなければ対立仮説が十分である。例えば、 $$ H_{0} : \\theta = 0 \\\\ H_{1} : \\theta \u0026lt; 0 $$ のような仮説検定は全く問題ないが、 $$ H_{0} : \\theta \\in [-1,0] \\\\ H_{1} : \\theta \\in [0,+1] $$ のような場合は$\\theta = 0$の時、帰無仮説と対立仮説が両方真である可能性があるため、問題がある。\n棄却域 定義によれば、棄却域は事象である。仮説検定を一回の試行と見なすならば、$H_{0}$が棄却される確率は、棄却という事象が発生する確率と同じである。この確率がかなり低く、例えば$\\alpha = 0.05$よりも低いにもかかわらず、発生した場合、それは普通の出来事ではなく、注目すべき事象と見なすことができる。こうしたストーリーテリングから、有意水準（p-value）のような概念が自然に思い浮かぶかもしれない。\n参照 仮説検定の簡単な定義: 厳密さよりも、適度に受け入れやすい定義を紹介する。 帰無仮説と対立仮説の決定方法: その定義にどんな問題があるか説明する。 仮説検定の厳しい定義: 比較的厳密な数理統計的な仮説検定の定義を紹介する。 棄却域の簡単な定義 Casella. (2001)『Statistical Inference』(第2版): p373~374.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2283,"permalink":"https://freshrimpsushi.github.io/jp/posts/2283/","tags":null,"title":"数理統計的な仮説検定の定義"},{"categories":"머신러닝","contents":"概要 レファレンスと数式の番号や表記法は、論文をそのまま踏襲する。 Physics-informed neural networks (PINN[ピン]と読む)は、数値的に解くために設計された微分方程式の人工ニューラルネットワークであり、2018年Journal of Computational Physicsに発表された論文Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equationsで紹介されました。この論文の著者は、応用数学、機械工学のM. Raissi, P. Perdikaris, G.E. Karniadakisです。\nこの論文で述べられている物理情報physics informationは、壮大に見えるかもしれませんが、実際には与えられた微分方程式自体を意味すると考えても良いでしょう。つまり、\u0026lsquo;微分方程式を人工ニューラルネットワークで解く際、与えられた微分方程式を利用します\u0026rsquo;と言っているのと同じです。機械学習の論文を読むときは、このように見栄えの良い名前に惑わされないよう注意が必要です。\n微分方程式の数値的解法においてPINNが注目される理由は、損失関数に関するアイデアがシンプルで理解しやすく、実装も簡単だからでしょう。実際に論文の例では非常にシンプルなDNNが紹介されています。\n一般的に言われるPINNはSection 3.1で紹介されるモデルを指します。\n0. 抄録 著者はPINNを\u0026rsquo;与えられた非線形偏微分方程式を満たしながら、教師あり学習問題を解くために訓練された人工ニューラルネットワーク\u0026rsquo;と紹介しています。この論文で主に扱う2つの問題は、\u0026lsquo;data-driven solution and data-driven discovery of partial differential equations\u0026rsquo;です。性能評価のために、流体力学、量子力学、拡散方程式などの問題を解いてみました。\n1. 序論 最近の機械学習とデータ分析の進歩は、画像認識image recognition、認知科学cognitive science、ゲノム学genomicsなどの科学分野で革新的な結果をもたらしていますが、複雑な物理的、生物学的、工学的システムに対しては（データ収集コストが高いため）少ない情報で望ましい結果を導き出す必要がある困難があります。このような*小さなデータ領域small data regime*では、DNN、CNN、RNNなどの先進技術の収束性が保証されていません。\n[4-6]で、データ効率が良く（=少ないデータで）、物理情報を学習できる（=微分方程式を解くことができる）方法についての研\n究が進んでいます。非線形問題への拡張は、この論文の著者であるRaissiの後続研究[8,9]で提案されました。\n2. 問題設定 人工ニューラルネットワークで表される関数は、入力値（偏微分方程式でのソリューション$u$の座標$x, t$を指す）とパラメータによって関数値が決定されるが、これら2種類の変数に対して微分を行うために自動微分automatic differentiationを活用する。\nこのようなニューラルネットワークは、観測されたデータを支配する物理法則に起因する任意の対称性、不変性、または保存原理を尊重するように制約されている。これは、一般的な時間依存かつ非線形の偏微分方程式によってモデル化される。\nこの論文でこの文章が難しいと感じられるかもしれないが、私の考えでは簡単に言えば提案された人工ニューラルネットワークであるPINNが、与えられた微分方程式を満たす必要があるということだ。後述するが、微分方程式を満たす必要があるという条件を損失関数として使用するためである。\nこの論文の目的は、数理物理学におけるディープラーニングを進化させる新しいパラダイムのモデリングと計算パラダイムを提示することである。そのために、前述したように、この論文では主に2つの問題を扱う。一つは偏微分方程式のデータ駆動ソリューションdata-driven solutionであり、もう一つは偏微分方程式のデータ駆動発見data-driven discoveryである。使用された全てのコードとデータセットはhttps://github.com/maziarraissi/PINNsで確認できる。この論文では、$L1$、$L2$、ドロップアウトなどの正則化なしに、ハイパーボリックタンジェントを活性化関数として用いたシンプルなMLPが使用されている。各例では、ニューラルネットワークの構造、オプティマイザー、学習率などが具体的に紹介される。\nこの論文では、以下のようなパラメータ化された非線形偏微分方程式の一般的な形parameterized and nonlinear partial differential equations of the general formを扱う。\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u; \\lambda] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nここで、$u=u(t,x)$は(1)を満たす隠れた(=与えられていない=知られていない)関数、つまり(1)のソリューションであり、$\\mathcal{N}[\\cdot; \\lambda]$は$\\lambda$でパラメータ化された非線形演算子（NonlinearのNに由来する）であり、$\\Omega \\subset \\mathbb{R}^{D}$である。多くの数理物理学の問題problems in mathematical physicsは上記のような形で表される。例えば、1次\n元粘性バーガース方程式を見てみよう。\n$$ u_{t} + uu_{x} = \\nu u_{xx} $$\nこれは(1)で$\\mathcal{N}[u; \\lambda] = \\lambda_{1} uu_{x} - \\lambda_{2}u_{xx}$、$\\lambda = (\\lambda_{1}, \\lambda_{2})$の場合である。与えられた方程式(1)に対して、扱うべき2つの問題はそれぞれ以下の通りである。\n偏微分方程式のデータ駆動ソリューション: 固定された$\\lambda$に対して、システムのソリューション$u(t,x)$は何か？ 偏微分方程式のデータ駆動発見: 観測されたデータを最もよく表現するパラメータ$\\lambda$は何か？ 3. 偏微分方程式のデータ駆動ソリューション セクション3では、以下の形式の偏微分方程式からデータに基づいたソリューションを見つける問題について扱う。\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nつまり、$(1)$でパラメータ $\\lambda$ が固定されている状況である。セクション3.1とセクション3.2ではそれぞれ連続時間モデルと離散時間モデルを扱う。方程式を見つける問題はセクション4で扱う。ここで言う\u0026rsquo;データ\u0026rsquo;の意味は以下で詳しく説明する。\n3.1. 連続時間モデル $(t,x) \\in \\mathbb{R} \\times \\mathbb{R}$ とすると、$u : \\mathbb{R}^{2} \\to \\mathbb{R}$ である。これを人工ニューラルネットワークで近似するが、次のように実装されるシンプルなMLPを使用する。Juliaでは、\nusing Flux\ru = Chain(\rDense(2, 10, relu),\rDense(10, 10, relu),\rDense(10, 1)\r) PyTorchでは、\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rlayers = [2, 10, 10, 1]\rclass network(nn.Module):\rdef __init__(self):\rsuper(network, self).__init__()\rlayer_list = [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\rself.linears = nn.ModuleList(layer_list)\rdef forward(self, tx):\ru = tx\rfor i in range(len(layers)-2):\ru = self.linears[i](u)\ru = F.relu(u)\ru = self.linears[-1](u)\rreturn u\ru = network() これで $u$ は入力ノードが $2$ つ、出力ノードが $1$ つの人工ニューラルネットワークとなる。$(2)$ の左辺を次のような関数 $f = f(t,x; u)$ として定義しよう。\n$$ \\begin{equation} f := u_{t} + \\mathcal{N}[u] \\end{equation} $$\nここで $u$ は人工ニューラルネットワークであるため、$f$ も隠れ層のパラメータを持つ一種の人工ニューラルネットワークである。上記のような $f$ を物理情報に基づいたニューラルネットワークphysics-informed neural network, PINNと呼ぶ。言い換えれば、与えられた偏微分方程式そのものである。$f$ に含まれる微分は自動微分で実装され、$u$ と同じパラメータを共有する。人工ニューラルネットワーク $u$ が $(2)$ のソリューションを適切に近似していれば、$f$ の関数値はどこでも $0$ であるべきだ。ここから、$ f \\to 0$ になるように人工ニューラルネットワークを学習させることが推測できる。\n$(t_{u}^{i}, x_{u}^{i})$ を初期値、境界値が定義された領域の点とする。\n$$ (t_{u}^{i}, x_{u}^{i}) \\in( \\Omega \\times \\left\\{ 0 \\right\\}) \\cup (\\partial \\Omega \\times [0, T]) $$\n$u_{\\ast}$ を実際のソリューションとすると、初期条件と境界条件が与えられたということは、次のような値が与えられたということと同じである。\n$$ \\left\\{ t_{u}^{i}, x_{u}^{i}, u^{i} \\right\\}_{i=1}^{N_{u}},\\quad u^{i} = u_{\\ast}(t_{u}^{i}, x_{u}^{i}) $$\n理論上はこれらの値を無限に持つことになるが、数値的な問題では有限の点のみを扱えるので、$N_{u}$ 個を持っているとする。人工ニューラルネットワーク $u$ は $(t_{u}^{i}, x_{u}^{i})$ を入力として受け取り、$u^{i}$ を出力する必要があるので、これらがそれぞれ入力と対応するラベルとなる。\n$$ \\text{input} = (t_{u}^{i}, x_{u}^{i}),\\qquad \\text{label} = u^{i} $$\nこれがPINNで学習する\u0026lsquo;データ\u0026rsquo;である。それでは、損失関数を次のように設定できる。\n$$ MSE_{u} = \\dfrac{1}{N_{u}} \\sum\\limits_{i=1}^{N_{u}} \\left| u(t_{u}^{i},x_{u}^{i}) - u^{i} \\right|^{2} $$\nまた、$f$は適切な点集合（理論的には解 $u_{\\ast}$ が定義されるすべての点で満たされるべきだが、数値的には有限の点しか扱えない）$\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$で$(2)$を満たさなければならない。これらの適切な点を論文ではコロケーションポイントcollocation pointsと呼ぶ。コロケーションポイントに対して以下の損失関数を設定する。\n$$ MSE_{f} = \\dfrac{1}{N_{f}}\\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} $$\nつまり、$MSE_{f}$が$0$に近づくことは、物理的情報（偏微分方程式）を満たすことを意味する。したがって、人工ニューラルネットワーク $u$ を訓練するための最終的な損失関数は以下の通りである。\n$$ MSE = MSE_{u} + MSE_{f} $$\n論文では、$MSE_{f}$を使用することで物理的情報を制約として設けることは、[15, 16]で初めて研究されたが、PINN論文ではこれを現代的な計算ツールで検討し、より困難なダイナミックシステムに適用したと説明されている。\n物理情報に基づく機械学習physics-informed machine learningという用語自体は、Wangの乱流モデリングturbulence modelingに関する研究[17]で初めて使用されたとされる。しかし、PINN以前の研究では、サポートベクターマシン、ランダムフォレスト、FNNなどの機械学習アルゴリズムが単に使用されていたと説明されている。PINNがこれらと区別される点は、一般的に機械学習に使用されるパラメータに対する微分だけでなく、解の座標 $x, t$ に関する微分も考慮している点である。つまり、パラメータ $w$ を持つ人工ニューラルネットワークで近似された解を $u(t,x; w)$ とするとき、以前に提案された方法は偏微分 $u_{w}$ のみを利用したが、PINNは $u_{t}$ や $u_{x}$ などを利用して解を求める。このようなアプローチにより、少量のデータでも解をうまく見つけることができると説明されている。\nこの手続きがグローバル最小値に収束するという理論的な保証はないにもかかわらず、与えられた偏微分方程式が適切に定義されており、その解が一意であり、十分に表現力のあるニューラルネットワークアーキテクチャと十分な数のコロケーションポイント $N_{f}$ が与えられている場合、我々の方法は良好な\n予測精度good prediction accuracyを達成することが経験的に確認されていると論文には述べられている。\n3.1.1. 例（シュレーディンガー方程式） この例では、周期的な境界条件と複素数値を取る解に対して、提案された方法がうまく機能するかを重点的に確認する。例として、以下の初期条件と境界条件が与えられるシュレーディンガー方程式を扱う。\n$$ \\begin{align*} ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2}h \u0026amp;= 0,\\quad x\\in [-5, 5], t\\in[0, \\pi/2], \\\\ h(0,x) \u0026amp;= 2\\operatorname{sech} (x), \\\\ h(t,-5) \u0026amp;= h(t,5), \\\\ h_{x}(t,-5) \u0026amp;= h_{x}(t,5) \\end{align*} $$\n問題の解 $h_{\\ast}(t,x)$ は $h_{\\ast} : [0, \\pi/2] \\times [-5, 5] \\to \\mathbb{C}$ として複素関数値を持つ。しかし、関数の出力が複素数になるように人工ニューラルネットワークを定義するのではなく、実部を担当する $u(t,x)$ と虚部を担当する $v(t,x)$ の2次元ベクトルが出力されるように定義する。簡単に言えば、入力と出力のノードがそれぞれ2つのMLPとして定義することである。\n$$ h(t,x) = \\begin{bmatrix} u(t,x) \\\\[0.5em] v(t,x) \\end{bmatrix} $$\nこの問題におけるPINN $f$ は以下の通りである。\n$$ f := ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2} h $$\n$h(t,x)$ と $f(t,x)$ のパラメータは、初期値に対する損失 $MSE_{0}$、境界値に対する損失 $MSE_{b}$、物理情報に対する損失 $MSE_{f}$ を最小化するように学習される。\n$$ MSE = MSE_{0} + MSE_{b} + MSE_{f} $$\n$$ \\begin{align*} \\text{where } MSE_{0} \u0026amp;= \\dfrac{1}{N_{0}}\\sum_{i=1}^{N_{0}} \\left| h(0, x_{0}^{i}) - h_{0}^{i} \\right|^{2} \\quad (h_{0}^{i} = 2\\operatorname{sech} (x_{0}^{i})) \\\\ MSE_{b} \u0026amp;= \\dfrac{1}{N_{b}}\\sum_{i=1}^{N_{b}} \\left( \\left| h(t_{b}^{i}, -5) - h(t_{b}^{i}, 5) \\right|^{2} + \\left| h_{x}(t_{b}^{i},-5) - h_{x}(t_{b}^{i},5) \\right|^{2} \\right) \\\\ MSE_{f} \u0026amp;= \\dfrac{1}{N_{f}} \\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} \\end{align*} $$\n論文には $MSE_{b}$ の式にタイプミスがあるので注意すること。 ここで、$\\left\\{ x_{0}^{i}, h_{0}^{i} \\right\\}_{i=1}^{N_{0}}$ は初期値データ、$\\left\\{ t_{b}^{i} \\right\\}_{i=1}^{N_{b}}$ は境界でのコロケーションポイント、$\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$ は $f$ に対するコロケーションポイントである。\nデータセットを生成するために、従来のスペクトルメソッドspectral methodsを使用した。$h(0,x)$ での初期値データの数は $N_{0} = 50$、境界値データの数は $N_{b} = 50$ とし、ランダムに選んだ。また、$f$ のコロケーションポイントの数は $N_{f} = 20,000$ である。人工ニューラルネットワークは、100個のノードを持つ線形層を5層、層間の活性化関数としてハイパーボリックタンジェント $\\tanh$ を使用して構築した。\nFigure 1.\nFigure 1では、上の図は予測された解 $\\left| h(t, x) \\right|$ のヒートマップを示している。下の図は、時間がそれぞれ $t = 0.59, 0.79, 0.98$ のときの予測された解と実際の解がどれだけ一致しているかを示している。相対的 $L_{2}$ ノルムrelative $L_{2}$-norm は $0.00197 = 1.97 \\cdot 10^{-3}$ で、予測された解が正確な解と比較して約 $0.02%$ の差異があることを意味している。したがって、PINNは少ない初期値データでシュレーディンガー方程式の非線形挙動を正確に捉えることができる。\n現在扱っている連続時間モデルは、初期値が少なくてもうまく機能するが、コロケーションポイントの数 $N_{f}$ が十分に多くなければならないという潜在的な制約がある。これは空間の次元が2以下の場合はあまり問題にならないが、高次元の場合、必要なコロケーションポイントの数が指数関数的に増加する可能性があるため、問題になる可能性がある。そのため、次のセクションでは、多くのコロケーションポイントを必要としないようにするために、古典的なルンゲ・クッタ時間ステップスキームを活用した、より構造化されたニューラルネットワークを提案する。\n3.2. 離散時間モデル セクション3.1では、解を連続時間に対して近似した。この場合、人工ニューラルネットワークは全体の領域に対して同時に学習され、任意の点 $(x,t)$ に対して出力がある。このセクションでは、セクション3.1とは異なり、離散時間について扱う。つまり $t_{n}$ の値を知っているとき、$t_{n+1}$ の値を人工ニューラルネットワークで近似する方法について説明する。$(2)$ に $q$ ステージのルンゲ・クッタ法を適用すると、以下のようになる。\n$$ u(t_{n+1}, x) = u(t_{n}, x) - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u(t_{n}+c_{j} \\Delta t, x) \\right] $$\nここで $u^{n}(x) = u(t_{n}, x)$, $u^{n+c_{j}} = u(t_{n} + c_{j}\\Delta t, x)$ と表記すると、\n$$ \\begin{equation} \\begin{aligned} u^{n+1} \u0026amp;= u^{n} - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] \\\\ \\text{where } u^{n+c_{j}} \u0026amp;= u^{n} - \\Delta t \\sum_{i=1}^{q} a_{j,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] \\quad j=1,\\dots,q \\end{aligned}\\tag{7} \\end{equation} $$\n上記の $q+1$ 個の式で、右辺の $\\sum$ 項を全て左辺に移行する。そして、左辺を $u_{i}^{n}$ のように表記する。\n$$ \\begin{equation} \\begin{aligned} u_{q+1}^{n} \u0026amp;:= u^{n+1} + \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] = u^{n} \\\\ \\\\ u_{1}^{n} \u0026amp;:= u^{n+c_{1}} + \\Delta t \\sum_{i=1}^{q} a_{1,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ u_{2}^{n} \u0026amp;:= u^{n+c_{2}} + \\Delta t \\sum_{i=1}^{q} a_{2,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ \u0026amp;\\vdots \\\\ u_{q}^{n} \u0026amp;:= u^{n+c_{q}} + \\Delta t \\sum_{i=1}^{q} a_{q,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\end{aligned}\\tag{9} \\end{equation} $$\nすると、これらの全ての値が $u^{n}$ に等しくなることがわかる。\n$$ u^{n} = u_{1}^{n} = u_{2}^{n} = \\cdots = u_{q+1}^{n} \\tag{8} $$\nしたがって、セクション3.2で言及されている物理情報とは、与えられた初期条件・境界条件と $(8)$ を指す。今度は $u(t_{n+1}, x)$ を求めるために二つの人工ニューラルネットワークを定義する。セクション3.1で使用した人工ニューラルネットワークは、正確な解 $u_{\\ast}$ に収束することを期待する $u$ と、$u$ が満たすべき微分方程式 $f$ だったが、ここでは少し異なる。まず、人工ニューラルネットワーク $U$ を次の関数として定義する。\n$$ U : \\mathbb{R} \\to \\mathbb{R}^{q+1} $$\nつまり、入力層のノードが $1$ つ、出力層のノードが $q+1$ つのニューラルネットワークである。このニューラルネットワークの出力を以下の値とする。\n$$ U(x) = \\begin{bmatrix} u^{n+c_{1}}(x) \\\\[0.5em] u^{n+c_{2}}(x) \\\\ \\vdots \\\\[0.5em] u^{n+c_{q}}(x) \\\\[0.5em] u^{n+1}(x) \\end{bmatrix} \\tag{10} $$\nこのニューラルネットワークは、添付されたコード内の PhysicsInformedNN クラスで定義されている neural_net に該当する。\nつまり、以下の学習プロセスで $U$ の出力の最後の成分が $u(t_{n+1}, x)$ に収束することを期待している。二番目のニューラルネットワークは、$U$ の出力と $(7)$ の定義を利用して、次のように定義される関数である。\n3.2.1. 例（アレン・カーン方程式） 離散時間モデルにおける例として、以下の初期条件と周期的な境界条件が与えられるアレン・カーン方程式を取り上げる。\n$$ \\begin{equation} \\begin{aligned} \u0026amp;u_{t} - 0.0001u_{xx} + 5 u^{3} - 5u = 0,\\qquad x\\in [-1, 1], t\\in[0, 1], \\\\ \u0026amp;u(0,x) = x^{2} \\cos (\\pi x), \\\\ \u0026amp;u(t,-1) = u(t,1), \\\\ \u0026amp;u_{x}(t,-1) = u_{x}(t,1) \\end{aligned}\\tag{12} \\end{equation} $$\nこの例における $(9)$ に含まれる非線形演算子は以下の通りである。\n$$ \\mathcal{N}[u^{n+c_{j}}] = -0.0001u_{xx}^{n+c_{j}} + 5(u^{n+c_{j}})^{3} - 5u^{n+c_{j}} $$\nタイムステップ $t^{n}$ での $u$ の値を $u^{n,i}$ と表記する。\n$$ u^{n,i} = u^{n}(x^{n,i}) = u(t^{n}, x^{n,i}),\\qquad i=1,\\dots,N_{n} $$\n問題は $u^{n}$ が与えられたときに $u^{n+1}$ を計算することであり、$\\left\\{ x^{n,i}, u^{n,i} \\right\\}_{i=1}^{N_{n}}$ は与えられたデータセットである。$(8)$ により、このデータセットに対して以下が成り立つ。\n$$ u^{n,i} = u_{1}^{n}(x^{n,i}) = \\cdots = u_{q+1}^{n}(x^{n,i}) $$\nしたがって、以下のような二乗誤差の合計sum of squared error (SSE)を損失関数とする。\nここではなぜ $MSE$ ではなく $SSE$ を使用するのかは明確ではない。連続時間モデルでは $MSE$ を使用していたが、離散時間モデルでは $SSE$ を使用しており、何らかの実験的な理由があると思われる。 $$ SSE_{n} = \\sum\\limits_{j=1}^{q+1} \\sum\\limits_{i=1}^{N_{n}} \\left| u_{j}^{n} (x^{n,i}) - u^{n,i} \\right|^{2} $$\n各 $u_{j}^{n}$ は $(9)$ によって計算され、この計算にはニューラルネットワーク $U$ の出力が使用される。このロスは、添付されたコード内の PhysicsInformedNN クラスで定義されている net_U0 に対応する。そして $U$ の出力は $(12)$ の境界条件を満たさなければならないため、以下のような損失関数を設定する。\n$$ \\begin{align*} SSE_{b} \u0026amp;= \\sum\\limits_{i=1}^{q} \\left| u^{n+c_{i}}(-1) - u^{n+c_{i}}(1) \\right|^{2} + \\left| u^{n+1}(-1) - u^{n+1}(1) \\right|^{2} \\\\ \u0026amp;\\quad+ \\sum\\limits_{i=1}^{q} \\left| u_{x}^{n+c_{i}}(-1) - u_{x}^{n+c_{i}}(1) \\right|^{2} + \\left| u_{x}^{n+1}(-1) - u_{x}^{n+1}(1) \\right|^{2} \\ \\end{align*} $$\nこれらの合計が最終的なロスである。\n$$ SSE = SSE_{n} + SSE_{b} $$\nFigure 2.\nFig. 2では、上の図が正確な解のヒートマップを示している。下の図では、$t=0.1$ での $u$ を知っているときに $t=0.9$ での値を予測した結果を示している。下の左側の図では、青い線が正確な解であり、$\\color{red}\\mathsf{X}$ がデータとして使用された点を示している。下の右側の図では、青い線が正確な解であり、赤い線が予測された解である。\n暗黙のルンゲ・クッタ法 (IRK) では $u^{n+c_{j}}$ を計算するためにすべての $j$ に対する連立方程式を解く必要があるため、$q$ が大きくなると計算コストが大幅に増加するが、この論文で提案されている方法では $q$ が大きくなってもそれに伴う追加コストは非常に少ないと説明されている。また、$q$ が小さい場合、IRK ではタイムステップ $\\Delta t$ が大きいと正確な予測ができないが、PINN の場合は $\\Delta t$ が大きくても正確に予測できると説明されている。\n4. 偏微分方程式のデータ駆動発見 この章では、観測データがある場合に、偏微分方程式 $(1)$ のパラメータ $\\lambda$ を見つける問題について扱う。詳細は以下の例を通じて説明する。\n4.1. 連続時間モデル $f$ を以下のように $(1)$ の左辺として定義しよう。\n$$ f = u_{t} + \\mathcal{N}[u; \\lambda] $$\nセクション3の $(3)$ と異なる点は、$\\lambda$ が固定された定数ではなく、学習が必要な未知のパラメータになったことである。\n4.1.1. 例（ナヴィエ–ストークス方程式） セクション4.1.1では、非圧縮性流体の実際のデータに関する例として、ナヴィエ–ストークス方程式によって表されるケースを紹介する。以下の2次元ナヴィエ–ストークス方程式を考える。\n$$ \\begin{equation} \\begin{aligned} u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) \u0026amp;= -p_{x} + \\lambda_{2}(u_{xx} + u_{yy}) \\\\ v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) \u0026amp;= -p_{y} + \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned} \\tag{15} \\end{equation} $$\nここで、$u(t,x,y)$ は流体の速度ベクトルの $x$ 成分、$v(t,x,y)$ は $y$ 成分である。また、$p(t,x,y)$ は圧力、$\\lambda = (\\lambda_{1}, \\lambda_{2})$ は未知のパラメータである。ナヴィエ–ストークス方程式の解は発散が $0$ となる条件を満たすため、以下が成立する。\n$$ \\begin{equation} u_{x} + v_{y} = 0 \\tag{17} \\end{equation} $$\nある潜在関数 $\\psi (t, x, y)$ に対して、以下のように仮定する。\n$$ u = \\psi_{y},\\quad v = -\\psi_{x} $$\nつまり、流体の速度ベクトルを $\\begin{bmatrix} \\psi_{y} \u0026amp; -\\psi_{x}\\end{bmatrix}$ とすると、$u_{x} + v_{y} = \\psi_{yx} - \\psi_{xy} = 0$ であるため、自然に $(17)$ を満たす。$u$ と $v$ を個別に求めるのではなく、$\\psi$ を人工ニューラルネットワークで近似し、その偏微分によって $u, v$ を得る。実際の速度ベクトル場に対して、以下のように測定された情報があるとする。\n$$ \\left\\{ t^{i}, x^{i}, y^{i}, u^{i}, v^{i} \\right\\}_{i=1}^{N} $$\nこれに基づいて損失関数を以下のようにする。ここで、$u = \\phi_{y}$, $v = -\\psi_{x}$ であることを思い出す。\n$$ \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) $$\nそして、$(15)$ の右辺を左辺に定理し、それぞれを $f$ と $g$ として定義する。\n$$ \\begin{equation} \\begin{aligned} f \u0026amp;:= u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) + p_{x} - \\lambda_{2}(u_{xx} + u_{yy}) \\\\ g \u0026amp;:= v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) + p_{y} - \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned}\\tag{18} \\end{equation} $$\nすると $f, g$ の値は $\\psi$ によって以下のように表される。（$p$ もニューラルネットワークで近似する）\n$$ \\begin{align*} f \u0026amp;= \\phi_{yt} + \\lambda_{1}(\\psi_{y} \\psi_{yx} - \\psi_{x}\\psi_{yy}) + p_{x} -\\lambda_{2}(\\psi_{yxx} + \\psi_{yyy}) \\\\ g \u0026amp;= -\\phi_{xt} + \\lambda_{1}(-\\psi_{y} \\psi_{xx} + \\psi_{x}\\psi_{xy}) + p_{y} + \\lambda_{2}(\\psi_{xxx} + \\psi_{xyy}) \\\\ \\end{align*} $$\n損失関数に $f(t^{i}, x^{i}, y^{i}) = 0 = g(t^{i}, x^{i}, y^{i})$ という情報を加え、最終的に以下のようにする。\n$$ \\begin{aligned} MSE \u0026amp;:= \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) \\\\ \u0026amp;\\qquad + \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| f(t^{i}, x^{i}, y^{i}) \\right|^{2} + \\left| g(t^{i}, x^{i}, y^{i}) \\right|^{2} \\right) \\end{aligned} \\tag{19} $$\n次に、入力ノードが $3$ つ、出力ノードが $2$ つの人工ニューラルネットワークを定義する。この出力を $\\begin{bmatrix} \\psi (t, x, y) \u0026amp; p(t, x, y) \\end{bmatrix}$ とする。すると、上記の損失関数を計算することができる。\nノイズがある場合とない場合のデータについて実験を行い、どちらの場合も高い精度で $\\lambda_{1}, \\lambda_{2}$ を予測できたことが示されている。また、圧力 $p$ に関するデータが与えられていない場合でも、人工ニューラルネットワークがパラメータと共に $p$ もかなり正確に近似できることが示された。具体的な実験設定、結果、参照解の求め方については、論文に詳しく記載されている。\n5. 結論 この論文では、与えられたデータが満たす物理法則をエンコードする能力があり、偏微分方程式で説明できる新しい種類のニューラルネットワーク構造である物理情報に基づいたニューラルネットワークを紹介した。この結果から、物理モデルに対してディープラーニングが学習できることがわかった。これは多くの物理的シミュレーションに応用可能である。\nしかし、著者は提案された方法が偏微分方程式を解くための既存の方法、例えば有限要素法finite element method、スペクトル方法spectral methodsなどを置き換えるものだと考えるべきではないと述べている。実際にセクション3.2.ではルンゲ・クッタ法をPINNに適用している。\nPINNを実装するために、どれくらいの深さのニューラルネットワークが必要か、どれくらいのデータが必要かなどのハイパーパラメータに関する問題についても、著者は解決策を提案しようとした。しかし、ある方程式で効果的な設定が他の方程式ではそうではないことが観察されたと述べている。\n","id":3313,"permalink":"https://freshrimpsushi.github.io/jp/posts/3313/","tags":null,"title":"論文レビュー: 物理情報基盤ニューラルネットワーク(PINN)"},{"categories":"줄리아","contents":"日本語訳 コード println(ARGS[1] * \u0026#34; + \u0026#34; * ARGS[2] * \u0026#34; = \u0026#34; * string(parse(Float64, ARGS[1]) + parse(Float64, ARGS[2]))) 上記の通り、example.jlというファイルが1行で構成されているとしよう。Juliaでは、ARGSを通じてコマンドラインからの引数を配列で受け取ることができ、Pythonのsys.argvがコマンドライン引数を配列で受け取るのと似ている。書かれたコードは、二つの数字を受け取ってその和を出力するプログラムだ。実行結果は以下の通り。\n環境 OS: Windows julia: v1.6.3 ","id":2280,"permalink":"https://freshrimpsushi.github.io/jp/posts/2280/","tags":null,"title":"ジュリアでコマンドライン引数を挿入する方法"},{"categories":"줄리아","contents":"## 概要 Juliaでの記号演算は、`SymEngine.jl`[^1]パッケージを通じて使うことができる。 [^1]: https://symengine.org/SymEngine.jl/ ## コード ### シンボルの定義 シンボルは、以下の方法で定義することができる。 julia\u0026gt; using SymEngine\njulia\u0026gt; x = symbols(:x) x\njulia\u0026gt; x, y = symbols(\u0026ldquo;x y\u0026rdquo;) (x, y)\njulia\u0026gt; @vars x, y (x, y)\njulia\u0026gt; x = symbols(:x) x\njulia\u0026gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x)\n### ベクトルと行列 julia\u0026gt; v = [symbols(\u0026ldquo;v_▷eq1◁i$j\u0026rdquo;) for i in 1:2, j in 1:3] 2×3 Matrix{Basic}: a_11 a_12 a_13 a_21 a_22 a_23\njulia\u0026gt; Av 2-element Vector{Basic}: v_1a_11 + v_2a_12 + v_3a_13 v_1a_21 + v_2a_22 + v_3*a_23\njulia\u0026gt; @vars a, b, c, d, x, y (a, b, c, d, x, y)\njulia\u0026gt; [a b; c d] * [a x; b y] 2×2 Matrix{Basic}: a^2 + b^2 ax + by ac + bd cx + dy\n### 微分 記号微分は、[`Calculus.jl`](../3135)パッケージでも使用することができる。 julia\u0026gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x)\njulia\u0026gt; diff(f, x) 2 + 2*x - sin(x)\n## 環境 - OS: Windows10 - バージョン: Julia v1.7.1, SymEngine v0.8.7 ","id":3311,"permalink":"https://freshrimpsushi.github.io/jp/posts/3311/","tags":null,"title":"ジュリアでのシンボリック演算の方法"},{"categories":"줄리아","contents":"コード ジュリアでは、run()関数を使ってバックティックBacktickで囲まれた文字列を実行します。Pythonで言うところの[osモジュールのos.system()`](../2147)を使用したことに似ています。\njulia\u0026gt; txt = \u0026#34;helloworld\u0026#34; \u0026#34;helloworld\u0026#34; julia\u0026gt; typeof(`echo $txt`)\rCmd 위와 같이 백틱으로 감싸진 문자열은 Cmd라는 타입을 가지고, run() 함수로써 실행할 수 있다.\njulia\u0026gt; run(`cmd /C echo $txt`) helloworld Process(`cmd /C echo helloworld`, ProcessExited(0)) この例に限って言えば、Windowsではcmd内のechoを実行しなければならず、少し複雑になりますが、Linuxでは単にecho $txtを使用することができます。Windowsでこのようなコマンドを頻繁に使用する場合は、環境変数を修正することを検討してください1。\n環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/running-external-programs/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2278,"permalink":"https://freshrimpsushi.github.io/jp/posts/2278/","tags":null,"title":"Juliaで外部プログラムを実行する方法"},{"categories":"줄리아","contents":"コード parse(type, str)を使えばいいんだ。文字列strをtypeタイプの数字に変更してくれる。\njulia\u0026gt; parse(Int, \u0026#34;21\u0026#34;)\r21\rjulia\u0026gt; parse(Float64, \u0026#34;3.14\u0026#34;)\r3.14 なんでPythonみたいにInt64(\u0026quot;21\u0026quot;)みたいなのができないんだろう\u0026hellip;それは、\u0026quot;21\u0026quot;を21に変えることがタイプを変えることではなく、文字列\u0026quot;21\u0026quot;を読んで数字として解釈することだから、parseを使うのが妥当だって言われてるんだ1。\n環境 OS: Windows julia: v1.6.3 https://discourse.julialang.org/t/why-are-there-all-these-strange-stumbling-blocks-in-julia/92644/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2276,"permalink":"https://freshrimpsushi.github.io/jp/posts/2276/","tags":null,"title":"ジュリアで文字列を数値に変換する方法"},{"categories":"행렬대수","contents":"定義1 主対角線より上の要素がすべて$0$である行列$A = [a_{ij}]$を下三角行列lower triangular matrixという。\n$$ A \\text{ is lower triangluar matrix if } a_{ij} = 0 \\text{ whenever } i \\lt j $$\n主対角線より下の要素がすべて$0$である行列$A = [a_{ij}]$を上三角行列upper triangular matrixという。\n$$ A \\text{ is upper triangluar matrix if } a_{ij} = 0 \\text{ whenever } i \\gt j $$\n特に主対角要素がすべて$0$である三角行列を厳密（上/下）三角行列strictly (upper/lower) triangular matrixという。\n説明 例えば、$A$が$4 \\times 5$とする。$A$が下三角行列なら、\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ a_{21} \u0026amp; a_{22} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; 0 \u0026amp; 0 \\\\ a_{41} \u0026amp; a_{42} \u0026amp; a_{43} \u0026amp; a_{44} \u0026amp; 0 \\\\ \\end{bmatrix} $$\n上三角行列なら、以下のようになる。\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; a_{14} \u0026amp; a_{15} \\\\ 0 \u0026amp; a_{22} \u0026amp; a_{23} \u0026amp; a_{24} \u0026amp; a_{25} \\\\ 0 \u0026amp; 0 \u0026amp; a_{33} \u0026amp; a_{34} \u0026amp; a_{35} \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; a_{44} \u0026amp; a_{44} \\\\ \\end{bmatrix} $$\n定義により、対角行列は下三角行列でありつつ、上三角行列でもある。\n性質 下三角行列の転置は上三角行列であり、上三角行列の転置は下三角行列である。\n下三角行列の積は下三角行列であり、上三角行列の積は上三角行列である。\n三角行列が可逆であるための必要十分条件は、すべての主対角要素が$0$ではないことである。\n可逆な下三角行列の逆行列は下三角行列であり、可逆な上三角行列の逆行列は上三角行列である。\n正方形の厳密三角行列は冪零である。(逆は成り立たない)\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p21\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3305,"permalink":"https://freshrimpsushi.github.io/jp/posts/3305/","tags":null,"title":"三角行列"},{"categories":"줄리아","contents":"概要 1 可変引数関数とは、プログラミングで一般にVarargs Functionと呼ばれるもので、複数の引数を制限なく受け入れることができる関数のことだ。Juliaでは、変数の後ろに...を付けることで簡単に可変引数を設定できる。例のコードを見て理解しよう。\nちなみに、この...は スプラットオペレータSplat Operatorと呼ばれている。2\nコード アイザック・ニュートンは、単純に階乗の逆数を足すと$e$に収束する次の定理を発見した。 $$ e = {{ 1 } \\over { 0! }} + {{ 1 } \\over { 1! }} + {{ 1 } \\over { 2! }} + \\cdots = \\sum_{k=0}^{\\infty} {{ 1 } \\over { k! }} $$ この例では、オイラー定数$e = 2.71828182 \\cdots$に収束する数列を見ることにする。\nfunction f(x...)\rzeta = 0\rfor x_ in x\rzeta += 1/prod(1:x_)\rend\rreturn zeta\rend 上記のように、xの後ろにドットを付けてx...と書くと、与えられた引数が自動的に配列と捉えられる。関数の内容は上の数式からわかるように、階乗の逆数を順番に取り、足してリターンするだけだ。\njulia\u0026gt; f(0)\r1.0\rjulia\u0026gt; f(0,1)\r2.0\rjulia\u0026gt; f(0,1,2)\r2.5\rjulia\u0026gt; f(0,1,2,3,4,5,6,7,8,9,10)\r2.7182818011463845 実行結果は、自然数を長く与えるほど、オイラー定数に近づくことが確認できる。ここで注目すべき点は、可変的に入った引数が自動的にxという配列にまとめられて使用された点だ。例えば、次のように概念的に配列を入れるとエラーになる可能性がある。\njulia\u0026gt; f(0:10)\rERROR: MethodError: no method matching (::Colon)(::Int64, ::UnitRange{Int64}) 環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/functions/#Varargs-Functions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2266,"permalink":"https://freshrimpsushi.github.io/jp/posts/2266/","tags":null,"title":"ジュリアで可変引数関数を定義する方法"},{"categories":"줄리아","contents":"概要 eltype() 関数を使うだけだ。多分 element typeからきた名前だろう。\nコード julia\u0026gt; set_primes = Set([2,3,5,7,11,13])\rSet{Int64} with 6 elements:\r5\r13\r7\r2\r11\r3\rjulia\u0026gt; arr_primes = Array([2,3,5,7,11,13])\r6-element Vector{Int64}:\r2\r3\r5\r7\r11\r13 次のように$13$までの素数を要素とする二つのコンテナを考えてみよう。正直、同じデータを含んでいるが、上はセットで、下は配列という違いがあるだけだ。\njulia\u0026gt; typeof(set_primes)\rSet{Int64}\rjulia\u0026gt; eltype(set_primes)\rInt64\rjulia\u0026gt; typeof(arr_primes)\rVector{Int64} (alias for Array{Int64, 1})\rjulia\u0026gt; eltype(arr_primes)\rInt64 これらにtypeof()を適用すれば、セットか配列かの区別がつくが、eltype()はコンテナが何であれ、内部の要素がどのようなタイプかを返す。\njulia\u0026gt; typeof(1:10)\rUnitRange{Int64}\rjulia\u0026gt; eltype(1:10)\rInt64\rjulia\u0026gt; typeof(1:2:10)\rStepRange{Int64, Int64}\rjulia\u0026gt; eltype(1:2:10)\rInt64 上に示された1:10と1:2:10の違いは、タイプに過度にこだわるジュリアプログラミングの世界で、eltype()がどのように役立つかを示している。\n環境 OS: Windows julia: v1.6.3 ","id":2264,"permalink":"https://freshrimpsushi.github.io/jp/posts/2264/","tags":null,"title":"ジュリアのコンテナ内部の要素タイプをチェックする方法"},{"categories":"줄리아","contents":"コード default() 関数を使用すればいい。\nusing Plots\rdefault(size = (400,400), color = :red)\rdefault(:size, (400,400))\rfor key in [:size, :color], value in [(400,400), :red]\rdefault(key, value)\rend 普通の plot() 関数のように設定する方法と、キーとバリューを与えて一つずつ変更する方法がある。普通は前者の方が便利だが、設定が非常に複雑な場合は、ループを利用して下の方法を使用することもできる。\n初期化 全てのデフォルト設定を初期化したい場合は、default()を使用すればいい。\n環境 OS: Windows julia: v1.6.3 ","id":2262,"permalink":"https://freshrimpsushi.github.io/jp/posts/2262/","tags":null,"title":"ジュリアプロットの基本設定を変更する方法"},{"categories":"줄리아","contents":"概要 インデックスを取るときは、Not() 関数を使用すればいいんだ1。カラム名そのままのシンボル、またはシンボルの配列を入れると、それらのカラムのみが除外されてインデックス化されるんだ。\nコード using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r) 上の例のコードを実行して、その結果を確認しよう。\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 WJSN データフレームは、上のようになっているんだ。\njulia\u0026gt; WJSN[:,Not(:height)]\r10×3 DataFrame\rRow │ member birth unit │ String Int64 String ─────┼───────────────────────\r1 │ 다영 99 쪼꼬미\r2 │ 다원 97 메보즈\r3 │ 루다 97 쪼꼬미\r4 │ 소정 95 더블랙\r5 │ 수빈 96 쪼꼬미\r6 │ 연정 99 메보즈\r7 │ 주연 98 더블랙\r8 │ 지연 95 더블랙\r9 │ 진숙 99 쪼꼬미\r10 │ 현정 94 더블랙 :height だけが入って、:height 列が削除されたよ。\njulia\u0026gt; WJSN[:,Not([:birth, :unit])]\r10×2 DataFrame\rRow │ member height │ String Int64 ─────┼────────────────\r1 │ 다영 161\r2 │ 다원 167\r3 │ 루다 157\r4 │ 소정 166\r5 │ 수빈 159\r6 │ 연정 165\r7 │ 주연 172\r8 │ 지연 163\r9 │ 진숙 162\r10 │ 현정 165 シンボルの配列 [:birth, :unit] が入って、:birth 列と :unit 列が削除されたんだ。\n環境 OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Taking-a-Subset\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2260,"permalink":"https://freshrimpsushi.github.io/jp/posts/2260/","tags":null,"title":"ジュリアでデータフレームの特定の行を削除する方法"},{"categories":"줄리아","contents":"概要 縦線と横線を引くには、vline!() と hline!() 関数を使用すればいい。\nコード @time using Plots\rplot(rand(100))\rhline!([0.5], linewidth = 2)\rvline!([25, 75], linewidth = 2)\rpng(\u0026#34;result\u0026#34;) 線が引かれる位置は配列で渡す。配列の要素が複数あれば、一度に複数の線を引くことができる。\n環境 OS: Windows julia: v1.6.3 ","id":2258,"permalink":"https://freshrimpsushi.github.io/jp/posts/2258/","tags":null,"title":"ジュリアで図に垂直線と水平線を挿入する方法"},{"categories":"줄리아","contents":"概要 RecipesBase.jlは、ユーザーが新しい図のスタイルを自分で作れるパッケージだ。Rプログラミング言語でのggplotがそうであるように、元のジュリアとはまた別の独自の文法1がある。例を通して覚えよう。\nコード using Plots\rusing DataFrames\rdf = DataFrame(x = 1:10, y = rand(10))\rplot(df)\r@userplot TimeEvolution\r@recipe function f(te::TimeEvolution)\rdf = te.args[1]\rlinealpha --\u0026gt; 0.5\rcolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend\rend\rtimeevolution(df); png(\u0026#34;1\u0026#34;)\rtimeevolution(df, legend = :left); png(\u0026#34;2\u0026#34;) 最初に、上記のコードを実行すると、次のようなエラーが発生する。\njulia\u0026gt; df = DataFrame(x = 1:10, y = rand(10))\r10×2 DataFrame\rRow │ x y\r│ Int64 Float64 ─────┼──────────────────\r1 │ 1 0.636532\r2 │ 2 0.463764\r3 │ 3 0.604559\r4 │ 4 0.654089\r5 │ 5 0.883409\r6 │ 6 0.91667\r7 │ 7 0.0609783\r8 │ 8 0.602259\r9 │ 9 0.460372\r10 │ 10 0.479944\rjulia\u0026gt; plot(df)\rERROR: Cannot convert DataFrame to series data for plotting これは、基本的にplot()がデータフレームを受け取って図を描く方法が定義されていないためだ。\n@userplotと@recipe @userplot TimeEvolution\r@recipe function f(te::TimeEvolution)\r...\rend 例では、時系列データをそのまま折れ線チャートで描いてみる。\n@userplot：plot()関数の性質を継承する関数の名前を指定する。ここでは大文字と小文字の区別があるが、完成する関数は小文字のみ使うことができることに注意しろ。 @recipe：具体的に図のスタイルを指定する。これの後にくる関数の名前は、通常慣習的にfを使用する。 f(te::TimeEvolution) df = te.args[1]\rlinealpha --\u0026gt; 0.5\rcolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend では、上記のコードを一行一行理解していこう。\ndf = te.args[1] 受け取った引数の中で最初のものをdfとみなす。この例では、与える引数がデータフレームであると仮定されているため、その略称であるdfを使った。このように、fは私たちが使用する関数でもなく、その引数teも直接使用されないことに注意せよ。\nlinealpha --\u0026gt; 0.5 この図で描かれる線の透明度を0.5に設定する。オプションを与えるようにlinealpha = 0.5ではなく、--\u0026gt;で値を与えることに注意せよ。これは、通常のジュリアの文法と完全に異なる。\ncolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r...\rend plot()が2次元配列を直接図に描くとしても、ラベルはy1、y2などと自動的に与えられる。これを防ぐために、上記のようにデータフレームの列名を取得して別々にラベルを与える。enumerate()関数を使って、そのインデックスと列名を同時に巡回する。詳しくはenumerate()に関する説明を参照せよ。\n@series for ...\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend @seriesマクロを通じて、繰り返し描かれるデータとそのスタイルを指定する。label --\u0026gt; column_nameを通じて列名をラベルとして与え、一番下の行にdf[:,column_index]と書くことで、その列名のデータが描かれることになる。この時、描かれるデータは一番下の行になければならないことに注意しろ。\n結果 この結果によって、timeevolution()またはtimeevolution!()で、私たちが指定したスタイルで図を描くことができるようになった。\ntimeevolution(df) データフレームを入れてもエラーなく図がうまく描かれていることを確認できる。折れ線の透明度は0.5で、ラベルもデータフレームにあった列名をそのまま引き継いでいる。\ntimeevolution(df, legend = :left) 任意に凡例の位置を調整してみた。fを定義する時にlegendについて何の言及もなかったにも関わらず、うまく適用されていることを確認できる。これは、timeevolution()がplot()の残りの要素を継承しているためだ。\n環境 OS: Windows julia: v1.6.3 https://docs.juliahub.com/RecipesBase/8e2Mm/1.0.1/syntax/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2256,"permalink":"https://freshrimpsushi.github.io/jp/posts/2256/","tags":null,"title":"ジュリアでアートスタイルを作る方法"},{"categories":"줄리아","contents":"概要 groupby()を使ってグループ別に分け、combine()を使って計算すればいいんだ1。\ngroupby(df, :colname)\n:colnameを基準にしてGroupedDataFrameを返す。 combine(gdf, :colname =\u0026gt; fun)\ngdfはグループ別に分かれたGroupedDataFrameだ。 :colname =\u0026gt; funは、計算したい値が入った列の名前のシンボル:colnameと、計算する関数funのペアだ。 コード using DataFrames\rusing StatsBase\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r)\rsort!(WJSN, :birth)\runique(WJSN, :unit)\runits = groupby(WJSN, :unit)\runits[1]\runits[2]\runits[3]\rcombine(units, :height =\u0026gt; mean) 上の例のコードを実行して、その結果を確認してみよう。\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 WJSNのデータフレームは上のようだ。\nグループ別に分ける groupby() julia\u0026gt; units = groupby(WJSN, :unit)\rGroupedDataFrame with 3 groups based on key: unit\rFirst Group (4 rows): unit = \u0026#34;더블랙\u0026#34;\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 주연 98 172 더블랙\r⋮\rLast Group (2 rows): unit = \u0026#34;메보즈\u0026#34;\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다원 97 167 메보즈\r2 │ 연정 99 165 메보즈 :unit列を基準にデータフレームが三つのグループに分かれた。\njulia\u0026gt; units[1]\r4×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 주연 98 172 더블랙\rjulia\u0026gt; units[2]\r4×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 수빈 96 159 쪼꼬미\r2 │ 루다 97 157 쪼꼬미\r3 │ 다영 99 161 쪼꼬미\r4 │ 진숙 99 162 쪼꼬미\rjulia\u0026gt; units[3]\r2×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다원 97 167 메보즈\r2 │ 연정 99 165 메보즈 上のようにGroupedDataFrameにインデクシングをすることで、分かれたデータフレームにアクセスできる。\nグループ別に計算する combine() julia\u0026gt; combine(units, :height =\u0026gt; mean)\r3×2 DataFrame\rRow │ unit height_mean │ String Float64 ─────┼─────────────────────\r1 │ 더블랙 166.5\r2 │ 쪼꼬미 159.75\r3 │ 메보즈 166.0 上のコードは、:unitを基準にグループ化されたデータフレームunitsの中で、WJSNのデータフレームの:heightの平均meanを計算したものだ。概要で言及された通り、このStatBase.mean()は平均を計算する関数だ。これをsum()に変えれば合計、min()に変えればグループ別の最小値を計算する。この例では、:unit別に:heightの平均を計算し、쪼꼬미グループが159.75で一番低いことがわかる。\nhttps://stackoverflow.com/questions/64226866/groupby-with-sum-on-julia-dataframe\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2254,"permalink":"https://freshrimpsushi.github.io/jp/posts/2254/","tags":null,"title":"ジュリアでデータフレームをグループ分けして計算する方法"},{"categories":"줄리아","contents":"概要 これを実現するためには、unique()を使えばいい。正確には、重複した行を削除するというよりも、一つだけ残すことだ。\nコード using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r)\rsort!(WJSN, :birth)\runique(WJSN, :unit) 上の例のコードを実行して、その結果を確認してみよう。\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 WJSNデータフレームは上のようになっている。\n単一の列で重複した行を削除する unique() julia\u0026gt; unique(WJSN, :unit)\r3×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 수빈 96 159 쪼꼬미\r3 │ 다원 97 167 메보즈 :unitシンボルごとに、一つの行だけが残っているのを確認できる。\n環境 OS: Windows julia: v1.6.3 ","id":2252,"permalink":"https://freshrimpsushi.github.io/jp/posts/2252/","tags":null,"title":"JuliaでDataFrameの重複した行を削除する方法"},{"categories":"줄리아","contents":"概要 Juliaでは、サブプロットに関連するオプションはlayoutオプションを通して制御できる。\n整数を入力すると、その数だけのグリッドをうまく作ってくれる。 整数の2-タプルを入力すると、指定どおりのグリッドを作ってくれる。 @layoutマクロを使ってPlots.GridLayoutタイプの複雑なレイアウトを構成する。 コード using Plots\rleft = plot(randn(100), color = :red)\rright = plot(randn(100), color = :blue)\rplot(left, right)\rpng(\u0026#34;easyone\u0026#34;)\rdata = rand(10, 6)\rplot(data, layout = 6)\rpng(\u0026#34;easytwo\u0026#34;)\rplot(data, layout = (3,2))\rpng(\u0026#34;easygrid\u0026#34;)\rl = @layout [p1 ; p2 p2]\rp = plot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l)\rpng(\u0026#34;hardgrid\u0026#34;) 簡単な列挙 left = plot(randn(100), color = :red)\rright = plot(randn(100), color = :blue)\rplot(left, right) ただ、プロットを複数同時にまとめてプロットし直しても、とりあえずサブプロットにはなる。\n簡単なレイアウトlayout plot(data, layout = 6) plot(data, layout = (3,2)) 簡単に整数でやるのもいいし、具体的にタプルを渡して望んだ通りのグリッドを作ることができる。もちろん、整数を渡さなくても同じように動くから、タプルを渡す場合だけ覚えておけばいい。\n複雑なレイアウト@layout l = @layout [p1 ; p2 p2] 単純なグリッドではなく、1行目に1つの絵、2行目に2列の絵が入るように指示するPlots.GridLayoutタイプのlを定義した。これをlayoutの入力として与えると、次のようにずっと複雑なレイアウトが表現される。\np = plot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l) 空白の作成_ 上記の例で、1行目の絵を下の行と同じサイズで、中央に配置したい場合は、両側に空間を作ればいい。_で空白を示すことができる。{0.5w}で幅を全体の半分に合わせれば、\nl = @layout([_ p{0.5w} _; p p])\rplot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l) 環境 OS: Windows julia: v1.6.3 ","id":2250,"permalink":"https://freshrimpsushi.github.io/jp/posts/2250/","tags":null,"title":"ジュリアでレイアウトを使ってサブプロットを描く方法"},{"categories":"선형대수","contents":"定義1 $V$をベクトル空間としよう。$V$から$\\mathbb{C}$（または$\\mathbb{R}$）への写像$f$を汎関数functionalという。\n$$ f : V \\to \\mathbb{C} $$\n$f$が線形ならば、線形汎関数という。\nもっと詳しい定義2 $V$をフィールド$F$上のベクトル空間としよう。この時、フィールド$F$自体が$F$上の$1$次元ベクトル空間になる。線形変換 $f : V \\to F$を線形汎関数linear functionalという。\n言い換えれば、線形汎関数とはベクトル空間とその体の間の線形変換である。\n説明 よく見る定義は最初の定義だ。通常、二番目のように抽象的に定義することはない。\n汎関数を韓国語に訳すと「범함수」となり、あまり特徴がないが、英語ではfunctionalが形容詞ではなく名詞であることに注意する必要がある。また、汎関数（汎函数）という翻訳はgeneralized function一般化関数からの影響を受けている。\n線形作用素との区別は、値域が$\\mathbb{R}$または$\\mathbb{C}$と定義されるという点のみで、まさにこの違いが双対空間のような空間を考える価値があるということである。すぐにノルム$\\| \\cdot \\| = \\| \\cdot \\|_{V}$はそれ自体が汎関数になり、測度論との関連で考えると、有用であることが避けられない。\n例 トレース $V = M_{n\\times n}(\\mathbb{R})$としよう。関数$f$を次のように定義しよう。\n$$ f : M_{n\\times n}(\\mathbb{R}) \\to \\mathbb{R} \\quad \\text{ by } \\quad f(A) = \\tr(A) $$\nこの時、$\\tr$は行列のトレースである。したがって、$f$は線形汎関数である。\nフーリエ係数 $V$を$f : [0, 2\\pi] \\to \\mathbb{R}$を満たす連続関数のベクトル空間としよう。固定された$g \\in V$に対して、$h : V \\to \\mathbb{R}$を次のように定義しよう。\n$$ h(f) = \\dfrac{1}{2\\pi} \\int_{0}^{2\\pi} f(x)g(x)dx $$\nすると、$h$は線形汎関数である。$g$が$\\cos nx$または$\\sin nx$の時、$h(f)$は$f$のフーリエ係数になる。\n座標関数 $V$を有限次元ベクトル空間としよう。$\\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\right\\}$を$V$の順序基底としよう。$\\mathbf{x} \\in V$の座標ベクトルが次のようであるとしよう。\n$$ [\\mathbf{x}]_{\\beta} = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} $$\n今、$1 \\le i \\le n$に対して次のような関数を考えよう。\n$$ f_{i}(\\mathbf{x}) = a_{i} $$\nすると、$f_{i}$は$V$上で定義された線形汎関数であり、これを**$i$番目の座標関数**$i$th coordinate function with respect to the ordered basis $\\beta$という。すると、$f_{i}(\\mathbf{v}_{i}) = \\delta_{ij}$が成立する。$\\delta_{ij}$はクロネッカーのデルタである。座標関数は双対空間を語る上で重要な役割を果たす。\nKreyszig. (1989). Introductory Functional Analysis with Applications: p103~104.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p119\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3281,"permalink":"https://freshrimpsushi.github.io/jp/posts/3281/","tags":null,"title":"線形汎関数"},{"categories":"줄리아","contents":"概要 1 plot() 関数の legend オプションで、凡例の位置を自由に調整できる。$0$ から $1$ までの値で構成された2-タプルを与えると、正確にその位置に表示されるし、それ以外はシンボルで制御できる。\nシンボルの場合は、top/bottom と left/right を順に結びつけて組み合わせる。最初に outer を付けると、図の外側に凡例が表示される。組み合わせで作られるシンボルの例は以下の通り:\n:bottom :left :bottomleft :outertopright したがって、:leftbottom や :toprightouter のようなシンボルは許されない。\nコード data = randn(100, 2)\rplot(data)\rplot(data, legend = (0.5, 0.7)); png(\u0026#34;tuple\u0026#34;)\rSymbols = [:none, :bottom, :left, :bottomleft, :outertopright, :inline]\rfor symbol ∈ Symbols\rplot(data, legend = symbol)\rpng(string(symbol))\rend 正確な位置の指定 legend = (0.5, 0.7) タプル (0.5, 0.7) は、横軸の50%、縦の高さの70%ぐらいに凡例が表示される。\n凡例の除去 :none 上下左右 :bottom, :left 組み合わせ :bottomleft 外側 :outertopright 図の外側に凡例を出力した。このため、図が歪むことに注意が必要だ。\n線の終わり :inline 多くの線があり、色で区別するのが難しい、または最後の値が特に重要な場合に便利だ。\n環境 OS: Windows julia: v1.6.3 https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2248,"permalink":"https://freshrimpsushi.github.io/jp/posts/2248/","tags":null,"title":"ジュリアで図の凡例の位置を調整する方法"},{"categories":"선형대수","contents":"定義1 $V$を有限次元ベクトル空間としよう。$V$の基底に特定の順序が与えられた場合、これを順序基底という。\n$\\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\right\\}$を$V$の順序基底とする。すると、基底表示の一意性により、$\\mathbf{v} \\in V$に対して、以下のように成立するスカラー$a_{i}$が唯一に存在する。\n$$ \\mathbf{v} = a_{1}\\mathbf{v}_{1} + \\dots a_{n}\\mathbf{v}_{n} $$\n$a_{1},\\dots,a_{n}$を基底$\\beta$に関する$\\mathbf{v}$の座標と言い、$i$番目の座標を$i$番目の成分として持つ行列を基底$\\beta$に関する$\\mathbf{v}$の座標ベクトルまたは座標行列と言い、$[\\mathbf{v}]_{\\beta}$のように表記する。\n$$ [\\mathbf{v}]_{\\beta} = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} $$\nまた、順序基底$\\beta$を座標系と言う。\n説明 基底は集合として定義されるが、集合を表現する際の要素の列挙順序は関係ない、つまり$\\alpha = \\left\\{ \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3} \\right\\} = \\left\\{ \\mathbf{e}_{2}, \\mathbf{e}_{3}, \\mathbf{e}_{1} \\right\\} = \\beta$である。したがって、\u0026lsquo;座標\u0026rsquo;という概念を抽象化するためには、基底の要素に順序を与える必要がある。今、$\\alpha, \\beta$を順序基底とすると、\n$$ \\alpha = \\left\\{ \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3} \\right\\} \\ne \\left\\{ \\mathbf{e}_{2}, \\mathbf{e}_{3}, \\mathbf{e}_{1} \\right\\} = \\beta $$\n$[\\mathbf{v}_{i}]_{\\beta} = \\mathbf{e}_{i}$が成立する。$\\mathbf{e}_{i}$は標準基底である。\n関数$T : \\mathbf{v} \\mapsto [\\mathbf{v}]_{\\beta}$は$V$から$\\mathbb{R}^{n}$への線形変換となる。\nベクトル空間$\\mathbb{R}^{n}$に対して、$\\left\\{ \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{n} \\right\\}$を標準順序基底と言う。\nStephen H. Friedberg, Linear Algebra (第4版, 2002), p79-80\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3279,"permalink":"https://freshrimpsushi.github.io/jp/posts/3279/","tags":null,"title":"順序基底と座標ベクトル"},{"categories":"줄리아","contents":"概要 1 グラフの幅と高さを調整するには、オプションにratioを入れるといい。他の推奨される別名には、aspect_ratios, axis_ratioがある。\nratio = :none: デフォルト値で、グラフのサイズに合わせて比率が調整される。 ratio = :equal: グラフのサイズにかかわらず、横軸と縦軸が1対1の比率に調整される。 ratio = Number: Numberに従って比率が調整される。Numberは${{세로} \\over {가로}} = {{\\Delta y} \\over {\\Delta x}}$の比率として与えられる。 コード using Plots\rx = rand(100)\ry = randn(100)\rplot(x,y,seriestype = :scatter, ratio = :none)\rpng(\u0026#34;none\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = :equal)\rpng(\u0026#34;equal\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 0.5)\rpng(\u0026#34;0.5\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 1)\rpng(\u0026#34;1\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 2)\rpng(\u0026#34;2\u0026#34;) デフォルト :none plot(x,y,seriestype = :scatter, ratio = :none) 1対1 :equal plot(x,y,seriestype = :scatter, ratio = :equal) 特定の比率 Number Numberは${{세로} \\over {가로}}$の比率として与えられる。\nplot(x,y,seriestype = :scatter, ratio = 0.5) plot(x,y,seriestype = :scatter, ratio = 1) plot(x,y,seriestype = :scatter, ratio = 2) 環境 OS: Windows julia: v1.6.3 https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2246,"permalink":"https://freshrimpsushi.github.io/jp/posts/2246/","tags":null,"title":"ジュリア集合の絵のアスペクト比を調整する方法"},{"categories":"기하학","contents":"定義1 $n$次元の微分可能多様体 $M$に対するリーマン計量Riemannian metric, リーマン計量 $g$とは、各点$p \\in M$を$g_{p}$に対応させる関数のことだ。ここで、$g_{p}$は$p$上の接空間 $T_{p}M$で定義される内積である。\n$$ \\begin{align*} g : M \u0026amp;\\to \\left\\{ \\text{all inner products on tangent space } T_{p}M \\right\\} \\\\ p \u0026amp;\\mapsto g_{p}=\\left\\langle \\cdot, \\cdot \\right\\rangle_{p} \\end{align*} $$\n$$ \\begin{align*} g_{p} : T_{p}M \\times T_{p}M \u0026amp;\\to \\mathbb{R} \\\\ (\\mathbf{X}_{p}, \\mathbf{Y}_{p}) \u0026amp;\\mapsto g_{p}(\\mathbf{X}_{p}, \\mathbf{Y}_{p})=\\left\\langle \\mathbf{X}_{p}, \\mathbf{Y}_{p} \\right\\rangle_{p} \\end{align*} $$\nこの場合、$g_{p}$は次の意味で微分可能でなければならない。\n$\\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to M$を$p$の周りの座標系とし、$\\mathbf{x}(x_{1}, \\dots, x_{n}) = p$とする。すると、次の$g_{ij} : \\mathbb{R}^{n} \\to \\mathbb{R}$が微分可能でなければならない。\n$$ g_{ij} (x_{1}, \\dots, x_{n}) = \\left\\langle \\left. \\dfrac{\\partial }{\\partial x_{i}} \\right|_{p}, \\left. \\dfrac{\\partial }{\\partial x_{j}} \\right|_{p}\\right\\rangle_{p} $$\n$g_{ij}$をリーマン計量の局所表現と言う。リーマン計量が与えられた微分多様体$(M, g)$をリーマン多様体と呼ぶ。\n説明 リーマン多様体$(M, g)$を研究することをリーマン幾何学と呼ぶ。$g$をRiemannian structureとも呼ぶ。$g_{ij}$は微分幾何学で第1基本形式の係数と学ぶ。\n$\\mathbf{X}_{p}, \\mathbf{Y}_{p} \\in T_{p}M$としよう。すると、$T_{p}M$は基底が$\\left\\{ \\left. \\dfrac{\\partial }{\\partial x_{i}} \\right|_{p} \\right\\}$の$n$次元ベクトル空間であるため、\n$$ \\mathbf{X}_{p} = X^{i}(p)\\left. \\dfrac{\\partial }{\\partial x_{i}} \\right|_{p} \\text{ and } \\mathbf{Y}_{p} = Y^{j}(p)\\left. \\dfrac{\\partial }{\\partial x_{j}} \\right|_{p} $$\nだから、\n$$ g_{p}(\\mathbf{X}_{p}, \\mathbf{Y}_{p}) = \\left\\langle \\mathbf{X}_{p}, \\mathbf{Y}_{p} \\right\\rangle = X^{i}(p)Y^{j}(p) \\left\\langle \\left. \\dfrac{\\partial }{\\partial x_{i}} \\right|_{p}, \\left. \\dfrac{\\partial }{\\partial x_{j}} \\right|_{p} \\right\\rangle = X^{i}(p)Y^{j}(p)g_{ij}(p) $$\n$p$を一般化すると、\n$$ g(\\mathbf{X}, \\mathbf{Y}) = X^{i}Y^{j} \\left\\langle \\dfrac{\\partial }{\\partial x_{i}}, \\dfrac{\\partial }{\\partial x_{j}} \\right\\rangle = X^{i}Y^{j}g_{ij} $$\n微分可能性の条件の別の表現は次の通り。\n多様体上で定義される関数$g(\\mathbf{X}, \\mathbf{Y}) : M \\to \\mathbb{R}$が微分可能である。\n$$ g(\\mathbf{X}, \\mathbf{Y}) (p) = g_{p} (\\mathbf{X}_{p}, \\mathbf{Y}_{p}), \\quad \\mathbf{X}_{p}, \\mathbf{Y}_{p} \\in T_{p}M $$\nこの場合、$g(\\mathbf{X}, \\mathbf{Y})_{p} = g(\\mathbf{X}, \\mathbf{Y}) (p)$と表記されることもある。\n「すべての微分可能多様体はリーマン計量を持つ」という事実が知られている。したがって、研究の方向性は、「多様体$M$がリーマン計量を持つ条件」ではなく、「多様体$M$にどのような良いリーマン計量を与えることができるか」となる。\n誘導された計量 微分多様体$M, N$間のイマージョン $f : M \\to N$が与えられたとする。$(N, h)$がリーマン多様体であるとする。次のように定義される$M$上のリーマン計量$g$を$f$から誘導された計量と呼ぶ。\n$$ g_{p}(v, w) := h_{f(p)}(df_{p}(v), df_{p}(w)) $$\nこの場合、$df_{p}$は点$p$での$f$の微分である。\nユークリッド空間 微分可能多様体としてのユークリッド空間$M = \\mathbb{R}^{n}$を考えよう。すると、$T_{p}\\mathbb{R}^{n} \\approx \\mathbb{R}^{n}$であり、基底$\\left\\{ \\dfrac{\\partial }{\\partial x}_{i} \\right\\}$はユークリッド空間の標準基底$\\left\\{ e_{i} = (0, \\dots, 1, \\dots, 0) \\right\\}$と同じである。したがって、計量の係数は次の通りである。\n$$ g_{ij} = \\left\\langle e_{i}, e_{j} \\right\\rangle = \\delta_{ij} $$\nしたがって、ユークリッド空間のリーマン計量は、ユークリッド空間自体で標準的に定義される内積そのものである。$(\\mathbb{R}^{n}, g)$を研究することをユークリッド幾何学と呼ぶ。\nManfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p38-39\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3276,"permalink":"https://freshrimpsushi.github.io/jp/posts/3276/","tags":null,"title":"リーマン計量とリーマン多様体"},{"categories":"줄리아","contents":"エラー using DataFrames, CSV\rexample = DataFrame(x = 1:10, 가 = \u0026#34;나다\u0026#34;)\rCSV.write(\u0026#34;example.csv\u0026#34;, example) JuliaでCSVファイルに出力するとき、上のように韓国語が文字化けする現象が見られる。\n原因 実際には韓国語が文字化けするわけではなく、Unicodeエンコーディングの問題で、特にUTF-8エンコーディングのBOMバイトオーダーマークが原因で起こる。PythonなどでエンコーディングをUTF-8-sigとすることで解決できる。\n解決方法 1 CSV.write(\u0026#34;example.csv\u0026#34;, example, bom = true) CSV.jlでは、単にbom = trueというオプションを指定すると、以下のように文字化けせずに出力される。\n環境 OS: Windows julia: v1.6.3 https://csv.juliadata.org/stable/writing.html#CSV.write\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2244,"permalink":"https://freshrimpsushi.github.io/jp/posts/2244/","tags":null,"title":"ジュリアでCSV出力時の文字化け解決方法"},{"categories":"줄리아","contents":"概要 ジュリアでテキスト出力を飾るパッケージとしてCrayons.jlが知られている1。\n組み込み関数だけで飾りたい場合は、printstyled()を使えばいい。\nコード using Crayons\rprint(Crayon(background = :red), \u0026#34;빨강\u0026#34;)\rprint(Crayon(foreground = :blue), \u0026#34;파랑\u0026#34;)\rprint(Crayon(bold = true), \u0026#34;볼드\u0026#34;)\rprint(Crayon(italics = true), \u0026#34;이탤릭\u0026#34;)\rprint(Crayon(bold = true, italics = true), \u0026#34;볼드 이탤릭\u0026#34;) 上記のコンソールを実行すると、次のように飾られた結果が得られる。\nCrayon(...)\nforeground: テキスト自体の色を変更する。シンボルや整数のトリプル(r,g,b)、0から255の間の整数で値を与えることができる。 background: テキストの背景色を変更する。引数を指定する方法はforegroundと同じだ。 ブール値で指定するオプションは以下のものがある。上記の例では、それぞれ個別に実行した結果と同時に実行した結果が示されている。\nbold: 太字。 italics: 斜体。 underline: 下線。 環境 OS: Windows julia: v1.6.3 https://github.com/KristofferC/Crayons.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2242,"permalink":"https://freshrimpsushi.github.io/jp/posts/2242/","tags":null,"title":"- ジュリアでのテキスト出力装飾パッケージ"},{"categories":"줄리아","contents":"コード 宇宙少女のデータフレームが以下のように与えられたとしよう。\nWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\r) julia\u0026gt; WJSN\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 다원 97 167\r3 │ 루다 97 157\r4 │ 소정 95 166\r5 │ 수빈 96 159\r6 │ 연정 99 165\r7 │ 주연 98 172\r8 │ 지연 95 163\r9 │ 진숙 99 162\r10 │ 현정 94 165 データフレームに新しい列を追加するコードは以下の通りである。\ndataframe[!, :\u0026quot;column_name\u0026quot;] = values ユニットに関する列を追加してみると、\nWJSN[!, :\u0026#34;unit\u0026#34;] = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\rjulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 더블랙\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 더블랙\r8 │ 지연 95 163 더블랙\r9 │ 진숙 99 162 쪼꼬미\r10 │ 현정 94 165 더블랙 環境 OS: Windows10 Version: Julia 1.7.1, DataFrames 1.3.2 ","id":3273,"permalink":"https://freshrimpsushi.github.io/jp/posts/3273/","tags":null,"title":"ジュリアでデータフレームに新しい列を追加する方法"},{"categories":"줄리아","contents":"コード using Plots\rscatter(rand(100), randn(100))\rplot!([0,1],[0,1])\rpng(\u0026#34;example1\u0026#34;)\rplot!([.00,.25,.50],[-2,0,-2])\rpng(\u0026#34;example2\u0026#34;)\rθ = 0:0.01:2π\rplot!(.5 .+ cos.(θ)/3, 1.5sin.(θ))\rpng(\u0026#34;example3\u0026#34;) このコードを実行して、図に線分を入れる方法を見てみよう。\n線分 plot!([0,1],[0,1]) 単に一つの線分を引くか何か別のものを描くか、方法は同じだ。線分には二つの点が必要だから、x座標の配列とy座標の配列を与えればいい。\n複数の線分 plot!([.00,.25,.50],[-2,0,-2]) y3は一度に二つの線分を描いたものだ。線分間の始点と終点が繋がっている。\n楕円 plot!(.5 .+ cos.(θ)/3, 1.5sin.(θ)) 複数の線分を引く方法を応用して、楕円を描くことができる。文法的にも計算的にも、他の言語に比べて描きやすい方だ。\n環境 OS: Windows julia: v1.6.3 ","id":2240,"permalink":"https://freshrimpsushi.github.io/jp/posts/2240/","tags":null,"title":"ジュリア集合の画像に線を挿入する方法"},{"categories":"기하학","contents":"ビルドアップ1 ベクトル場の簡単な定義を考えてみよう。3次元空間でのベクトル場ベクトル関数、ベクトル場とは、3次元ベクトルを3次元ベクトルにマッピングする関数 $X : \\mathbb{R}^{3} \\to \\mathbb{R}^{3}$である。これを多様体と考えると、$X$は微分多様体 $\\mathbb{R}^{3}$の点 $p$を$\\mathbb{R}^{3}$のベクトル $\\mathbf{v}$にマッピングするが、このベクトル $\\mathbf{v}$をオペレーターとして扱い、方向微分(=接ベクトル)と考えることができる。したがって、ベクトル場とは、多様体 $\\mathbb{R}^{3}$の点 $p$を$p$の接ベクトル $\\mathbf{v}_{p} \\in T_{p}\\mathbb{R}^{3}$にマッピングする関数である。\nすると、ベクトル場の値域は全ての点の接ベクトルの集合である。したがって、ベクトル場 $X$は以下のように定義される関数である。\n$$ X : \\mathbb{R}^{3} \\to \\bigcup \\limits_{p\\in \\mathbb{R}^{3}} T_{p}\\mathbb{R}^{3} $$\nこの概念を多様体に一般化するために、微分多様体 $M$の接束tangent bundle $TM$を次のように定義しよう。\n$$ TM := \\bigsqcup \\limits_{p\\in M} T_{p}M $$\nこのとき $\\bigsqcup$は直和である。\n定義 微分多様体 $M$上のベクトル場vector field $X$とは、各点 $p \\in M$を$p$の接ベクトル $X_{p} \\in T_{p}M$にマッピングする関数である。\n$$ \\begin{align*} X : M \u0026amp;\\to TM \\\\ p \u0026amp;\\mapsto X_{p} \\end{align*} $$\n説明 ベクトル場の関数値 接束の定義を考えると、$TM$の要素は$(p, X_{p})$であるが、定義で$X_{p}$をマッピングすると書かれているので、疑問に思うことがあるかもしれない。\n$$ \\begin{equation} TM := \\bigsqcup \\limits_{p \\in M } T_{p}M = \\bigcup_{p \\in M} \\left\\{ p \\right\\} \\times T_{p}M = \\left\\{ (p, X_{p}) : p \\in M, X_{p} \\in T_{p}M \\right\\} \\end{equation} $$\nつまり、正確に言うと、直和の定義によると$TM$の要素は順序対 $(p, X_{p})$が正しいが、実際には$X_{p}$と同じものとして扱われる。\n接束の定義を再考する。接束の定義で本当にしたいのは、順序対 $(p, X_{p})$を集めることではない。各点 $p$上の接ベクトルをすべて集めたいのである。しかし、各$T_{p}M$は$\\mathbb{R}^{n}$と同型であるため、和集合をとるときに曖昧さが生じる可能性がある。\n$$ T_{p}M \\approxeq \\mathbb{R}^{n} \\approxeq T_{q}M $$\n例えば、$M$が3次元多様体であるとすると、$T_{p}M \\approxeq \\mathbb{R}^{3}$で表されるベクトル $X_{p}$と$T_{q}M \\approxeq \\mathbb{R}^{3}$で表されるベクトル $X_{q}$を同じものとして扱う曖昧さがある。したがって、$TM$を順序対の集合として定義する理由は、$X_{p}$と$X_{q}$が同じ対象ではなく、明確に異なるものとして区別するためである。ここで自然に$\\iota_{p} : (p, X_{p}) \\mapsto X_{p}$のような全単射関数を考え、$(p, X_{p}) \\approx X_{p}$として扱うことができる。\nある教科書では、このような説明を特にしたくない場合や、読者が十分に理解していると仮定する場合に、接束 $TM$を次のように定義することもある。\n$$ TM := \\bigcup\\limits_{p\\in M} T_{p}M = \\left\\{ X_{p} \\in T_{p}M : \\forall p \\in M \\right\\} $$\nもちろん、再度言うが、上の定義も$(1)$も本質的には同じである。また、上の定義によると$X$の関数値は関数 $X_{p}$であることに注意しよう。\n$$ X_{p} : \\mathcal{D} \\to \\mathbb{R} $$\nオペレーターとしてのベクトル場 $M$を$n$次元微分多様体としよう。$M$の微分可能な関数の集合を$\\mathcal{D} = \\mathcal{D}(M)$としよう。\n$$ \\mathcal{D} = \\mathcal{D}(M) := \\left\\{ \\text{all real-valued functions of class } C^{\\infty} \\text{ defined on } M \\right\\} $$\n参照 微分多様体上の微分可能な関数の集合 $\\mathcal{D}(M)$ 微分多様体上の微分可能なベクトル場の集合 $\\frak{X}(M)$ Manfredo P. Do Carmo, Riemannian Geometry (英語版, 1992), p25-27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3270,"permalink":"https://freshrimpsushi.github.io/jp/posts/3270/","tags":null,"title":"微分可能多様体上のベクトル場"},{"categories":"줄리아","contents":"コード using DataFrames\rUnit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\rUnit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\rWJSN = vcat(Unit1, Unit2)\rpush!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\rpush!(WJSN, [\u0026#34;연정\u0026#34;,99,165])\rsort(WJSN, 3)\rsort(WJSN, :birth)\rsort(WJSN, [:birth, :height])\rsort(WJSN, :birth, rev = true) 上の例のコードを実行して、その結果を確認してみましょう。\njulia\u0026gt; WJSN\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\r10 │ 연정 99 165 WJSNのデータフレームは上記の通りです。\n列番号でソート sort(df, cols::integer) cols番目の列を基準にソートします。\njulia\u0026gt; sort(WJSN, 3)\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 루다 97 157\r2 │ 수빈 96 159\r3 │ 다영 99 161\r4 │ 진숙 99 162\r5 │ 지연 95 163\r6 │ 현정 94 165\r7 │ 연정 99 165\r8 │ 소정 95 166\r9 │ 다원 97 167\r10 │ 주연 98 172 3番目の列であるheightを基準にソートされたことが確認できます。\n列名でソート sort(df, cols::Symbol) 名前のシンボルcolsの列を基準にソートします。\njulia\u0026gt; sort(WJSN, :birth)\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 현정 94 165\r2 │ 소정 95 166\r3 │ 지연 95 163\r4 │ 수빈 96 159\r5 │ 루다 97 157\r6 │ 다원 97 167\r7 │ 주연 98 172\r8 │ 다영 99 161\r9 │ 진숙 99 162\r10 │ 연정 99 165 :birthが入ってbirthを基準にソートされたことが確認できます。\nソートの優先順位 sort(df, cols::Array) colsの順番に従って優先順位をつけてソートします。\njulia\u0026gt; sort(WJSN, [:birth, :height])\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 현정 94 165\r2 │ 지연 95 163\r3 │ 소정 95 166\r4 │ 수빈 96 159\r5 │ 루다 97 157\r6 │ 다원 97 167\r7 │ 주연 98 172\r8 │ 다영 99 161\r9 │ 진숙 99 162\r10 │ 연정 99 165 birthでソートしつつ、heightもソートされました。ただbirthを基準にソートしただけと比較すると、2行目と3行目が逆転しています。\n逆順でソート sort(df, rev::Bool=false) rev = trueを指定すればOKです。デフォルトはfalseです。\njulia\u0026gt; sort(WJSN, :birth, rev = true)\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 진숙 99 162\r3 │ 연정 99 165\r4 │ 주연 98 172\r5 │ 루다 97 157\r6 │ 다원 97 167\r7 │ 수빈 96 159\r8 │ 소정 95 166\r9 │ 지연 95 163\r10 │ 현정 94 165 環境 OS: Windows julia: v1.6.3 ","id":2238,"permalink":"https://freshrimpsushi.github.io/jp/posts/2238/","tags":null,"title":"Juliaでデータフレームを並べ替える方法"},{"categories":"줄리아","contents":"コード using DataFrames\rUnit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\rUnit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\rWJSN = vcat(Unit1, Unit2)\rpush!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\rpush!(WJSN, [\u0026#34;연정\u0026#34;,99,165]) 上の例のコードを実行して、その結果を確認しよう。\n二つのデータフレームの行を結合する vcat() julia\u0026gt; Unit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\r4×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161 2 │ 루다 97 157 3 │ 수빈 96 159 4 │ 진숙 99 162 julia\u0026gt; Unit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\r4×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 소정 95 166\r2 │ 주연 98 172\r3 │ 지연 95 163\r4 │ 현정 94 165\rjulia\u0026gt; WJSN = vcat(Unit1, Unit2)\r8×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165 当然だけど、二つのデータフレームの列は同じでなければならない。\n一行を挿入する push!() julia\u0026gt; push!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\r9×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\rjulia\u0026gt; push!(WJSN, [\u0026#34;연정\u0026#34;,99,165])\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\r10 │ 연정 99 165 push!()でデータを追加する時は、列の数が一致する配列を入れてやる必要がある。\n環境 OS: Windows julia: v1.6.2 ","id":2236,"permalink":"https://freshrimpsushi.github.io/jp/posts/2236/","tags":null,"title":"ジュリアでデータフレームに新しい行を挿入する方法"},{"categories":"줄리아","contents":"概要 Infinities.jlは、Juliaで無限大記号を使用できるように支援するパッケージだ1。科学計算のコーディングにおいて、無限大は意外と便利である。\nコード julia\u0026gt; 8 \u0026lt; Inf\rtrue 序論で無限大を使用するのではなく、無限大記号を使用できるように支援すると述べた理由は、実際にパッケージなしでそれを使用できるからである。\njulia\u0026gt; using Infinities\rjulia\u0026gt; 8 \u0026lt; ∞\rtrue\rjulia\u0026gt; -∞ \u0026lt; 8\rtrue\rjulia\u0026gt; max(∞, 10, 11)\r∞\rjulia\u0026gt; sin(∞)\rERROR: MethodError: no method matching AbstractFloat(::Infinities.Infinity) 見ての通り、無限大の一般的な性質を全て備えている。\njulia\u0026gt; ℵ₀ \u0026lt; ℵ₁\rtrue\rjulia\u0026gt; ℵ₀ \u0026gt; ℵ₁\rfalse\rjulia\u0026gt; ∞ == ℵ₁\rfalse\rjulia\u0026gt; ∞ == ℵ₀\rtrue\rjulia\u0026gt; ∞ === ℵ₀\rfalse さらに、無限集合の基数である$\\aleph_{0}$と$\\aleph_{1}$を使用できるが、これにより計算上では同じ無限大でありながら、並び替えや比較などに使用できるように順序を付けることができる。\n環境 OS: Windows julia: v1.6.2 https://github.com/JuliaMath/Infinities.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2234,"permalink":"https://freshrimpsushi.github.io/jp/posts/2234/","tags":null,"title":"ジュリアで無限大を使う方法"},{"categories":"줄리아","contents":"ガイド 1 (@v1.6) pkg\u0026gt; status JuMP Status `C:\\Users\\rmsms\\.julia\\environments\\v1.6\\Project.toml` [4076af6c] JuMP v0.20.0 REPLで]キーを押すとパッケージモードに入る。例えば、v0.20.0のバージョンのパッケージをv0.21にバージョンアップしたい場合は、以下のようにパッケージの後に@x.yyを付けてインストールすればいい。\n(@v1.6) pkg\u0026gt; add JuMP@0.21 Resolving package versions... ... (@v1.6) pkg\u0026gt; status JuMP Status `C:\\Users\\rmsms\\.julia\\environments\\v1.6\\Project.toml` [4076af6c] JuMP v0.21.4 もう一度バージョンを確認すると、パッケージのバージョンが正常に変更されたことを確認できる。\n環境 OS: Windows julia: v1.6.2 https://pkgdocs.julialang.org/dev/api/#General-API-Reference\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2232,"permalink":"https://freshrimpsushi.github.io/jp/posts/2232/","tags":null,"title":"ジュリアで特定バージョンのパッケージをインストールする方法"},{"categories":"기하학","contents":"概要 微分多様体上のプルバックを定義する。微分多様体が難しい場合は、$M = \\mathbb{R}^{m}$、$N = \\mathbb{R}^{n}$と考えてもよい。\n定義1 二つの微分多様体 $M, N$と微分可能な関数 $f : M \\to N$が与えられたとする。そこで、$N$の$k$-形式を$M$の$k$-形式に送る関数$f^{\\ast}$を考えることができる。$\\omega$を多様体$N$の$k$-形式とするとき、多様体$M$の$k$-形式$f^{\\ast}\\omega$を$\\omega$のプルバックpull back, 引き戻しと呼び、以下のように定義する。\n$$ \\begin{equation} (f^{\\ast}\\omega)(p) (v_{1}, \\dots, v_{k}) := \\omega (f(p))\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M \\end{equation} $$\n説明 プルバックという名前には、($f$が$M$から$N$へのマッピングであるのに対して)$f^{\\ast}$は$N$から$M$へのマッピングであるという意味がある。定義と表記法がかなり難しいが、少しずつ理解していこう。\n$f^{\\ast}$ $f^{\\ast}$は$N$の$k$-形式を$M$の$k$-形式に送るマップである。したがって、$\\omega$を$N$の$k$-形式とすると、$f^{\\ast}\\omega = f^{\\ast}(\\omega)$は$M$の$k$-形式である。\n$f^{\\ast}\\omega (p)$ 多様体$M$上の$k$-形式は、$p \\in M$を$\\Lambda^{k}(T_{p}^{\\ast}M)$の元にマッピングする。\n$$ f^{\\ast}\\omega : M \\to \\Lambda^{k}(T_{p}^{\\ast}M) $$\n$$ \\Lambda^{k} (T_{p}^{\\ast}M) := \\left\\{ \\varphi : \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\nつまり、$f^{\\ast}\\omega (p) \\in \\Lambda^{k} (T_{p}^{\\ast}M)$もまた、一つの関数である。$\\Lambda^{k} (T_{p}^{\\ast}M)$の定義により、$f^{\\ast}\\omega (p)$は「$p$上の接ベクトル」$k$個を変数とする。これで、$(1)$はこの関数の関数値を具体的に定義した式であることがわかる。$f^{\\ast}(p)$自体が一つの関数であることをより強調するため、以下のような表記を使うことにしよう。\n$$ (f^{\\ast}\\omega)_{p} = f^{\\ast}\\omega (p) $$\n$\\omega (f(p))$ $\\omega$は$N$の$k$-形式であるため、$N$の点$f(p)$を$\\Lambda^{k}(T_{f(p)}^{\\ast}N)$の元にマッピングする。\n$$ \\Lambda^{k} (T_{f(p)}^{\\ast}N) := \\left\\{ \\varphi : \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\n$\\Lambda^{k} (T_{f(p)}^{\\ast}N)$の定義により、$\\omega (f(p))$もまた、一つの関数である。$\\omega (f(p))$は「$f(p)$上の接ベクトル」$k$個を変数とする。ここでも同様に、$\\omega (f(p))$自体が一つの関数であることを強調するために、以下のような表記を使おう。\n$$ \\omega_{f(p)} = \\omega (f(p)) $$\n$df_{p}v_{i}$ $$ df_{p} : T_{p}M \\to T_{f(p)}N $$\n$f : M \\to N$に対して、$f$の微分 $df_{p}$は上記のように定義される。したがって、$v_{i} \\in T_{p\n}M$であれば、$df_{p}v_{i} = df_{p}(v_{i})$は$T_{f(p)}N$の元である。\nこれを総合すると、$(1)$を得る。\n$$ (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) := \\omega_{f(p)}\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M $$\n上記の二つの関数の定義域を見ると、以下のような違いがある。\n$$ \\begin{align*} (f^{\\ast}\\omega)_{p} : \u0026amp;\u0026amp; \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\\\ \\omega_{f(p)} : \u0026amp;\u0026amp; \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\end{align*} $$\nこの違いを微分$df_{p} : T_{p}M \\to T_{f(p)}N$が繋いでいると考えればよい。そのため、$df_{p}$をプッシュフォワードpush forward, 押し出しとも呼ぶ。$1$-形式$\\varphi$に対して、以下が成立する。\n$$ \\begin{equation} \\varphi( dfv) = f^{\\ast}\\varphi(v) \\end{equation} $$\n$0$-形式のプルバック $f : M \\to N$を二つの微分多様体間で定義された関数とする。$g : N \\to \\mathbb{R}$を関数($N$での$0$-形式)とする。$g$のプルバック$f^{\\ast}g : M \\to \\mathbb{R}$は、以下のように定義される関数($M$での$0$-形式)である。\n$$ f^{\\ast}g := g \\circ f $$\n座標変換 関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$が与えられたとする。$\\mathbf{x} = (x_{1}, \\dots ,x_{n}) \\in \\mathbb{R}^{n}$であり、$\\mathbf{y} = (y_{1}, \\dots ,y_{m}) \\in \\mathbb{R}^{m}$である。\n$$ f(x_{1}, \\dots, x_{n}) = (f_{1}(\\mathbf{x}), \\dots, f_{m}(\\mathbf{x}) )= (y_{1}, \\dots ,y_{m}) $$\nそして、$\\omega = \\sum\\limits_{I} a_{I} dy_{I}$を$\\mathbb{R}^{m}$上の$k$-形式とする。そのとき、$\\omega$のプルバック$f^{\\ast}\\omega$は以下の特性により、次のようになる。\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= f^{\\ast} \\left( \\sum a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast} \\left( a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}dy_{I} \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}(dy_{i1} \\wedge \\cdots \\wedge dy_{ik}) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} (f^{\\ast}dy_{i1} \\wedge \\cdots \\wedge f^{\\ast}dy_{ik}) \\end{align*} $$\nこの時、$(2)$により$f^{\\ast}dy_{i1}(v) = dy_{i1}(df(v)) = d(y_{i1}\\circ f)(v) = df_{i1}(v)$であり、$f^{\\ast}a_{I} = a_{I} \\circ f$であるため、\n$$ \\begin{equation} f^{\\ast} \\omega = \\sum a_{I}(f_{1}, \\dots f_{m}) df_{i1} \\wedge \\cdots \\wedge df_{ik} \\end{equation} $$\n上記の式は座標変換を意味し、具体的にどのようになるかは以下の例で見てみよう。\n例 $\\mathbb{R}^{2} \\setminus \\left{ 0, 0 \\right}$上の$1$-形式$\\omega$が以下のようであるとする。\n$$ \\omega = - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = a_{1}dx + a_{2}dy $$\nこの直交座標上の$1$-形式を極座標に変換してみよう。$U = \\left{ (r,\\theta) : 0 \u0026lt; r, 0 \\le \\theta \u0026lt; 2\\pi \\right}$とする。そして、$f : U \\to \\mathbb{R}^{2}$を以下のようにする。\n$$ f(r,\\theta) = (r\\cos\\theta, r\\sin\\theta) = (f_{1}, f_{2}) $$\nここで、$df_{1}, df_{2}$を計算してみよう。$f_{1} = r\\cos\\theta, f_{2}=r\\sin\\theta$であるため、\n$$ \\begin{align*} df_{1} \u0026amp;= \\dfrac{\\partial f_{1}}{\\partial r}dr + \\dfrac{\\partial f_{1}}{\\partial \\theta}d\\theta = \\cos\\theta dr - r \\sin \\theta d\\theta \\\\ df_{2} \u0026amp;= \\dfrac{\\partial f_{2}}{\\partial r}dr + \\dfrac{\\partial f_{2}}{\\partial \\theta}d\\theta = \\sin\\theta dr + r \\cos \\theta d\\theta \\\\ \\end{align*} $$\nそれにより、$(3)$に従い、\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= a_{1}(f_{1}, f_{2})df_{1} + a_{2}(f_{1}, f_{2})df_{2} \\\\ \u0026amp;= - \\dfrac{f_{2}}{f_{1}^{2} + f_{2}^{2}}(\\cos\\theta dr - r \\sin \\theta d\\theta) + \\dfrac{f_{1}}{f_{1}^{2} + f_{2}^{2}}df_{2}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= - \\dfrac{r\\sin\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\cos\\theta dr - r \\sin \\theta d\\theta) \\\\ \u0026amp;\\quad + \\dfrac{r\\cos\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= -\\dfrac{\\sin\\theta \\cos\\theta}{r}dr + \\sin^{2}\\theta d\\theta + \\dfrac{\\cos\\theta \\sin\\theta}{r}dr + \\cos^{2}\\theta d\\theta \\\\ \u0026amp;= d\\theta \\end{align*} $$\nしたがって、\n$$ \\int - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = \\int d\\theta $$\n■\n特性 $M, N$をそれぞれ$m, n$次元微分多様体、$f : M \\to N$とする。$\\omega, \\varphi$を$N$上の$k$-形式とする。$g$を$N$上の$0$-形式とする。$\\varphi_{i}$たちを$N$上の$1$-形式とする。すると、以下が成立する。\n$$ \\begin{align} f^{\\ast} (\\omega + \\varphi) =\u0026amp;\\ f^{\\ast}\\omega + f^{\\ast}\\varphi \\tag{a} \\\\ f^{\\ast} (g \\omega) =\u0026amp;\\ (f^{\\ast}g) (f^{\\ast}\\omega) \\tag{b} \\\\ f^{\\ast} (\\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k}) =\u0026amp;\\ f^{\\ast}(\\varphi_{1}) \\wedge \\cdots \\wedge f^{\\ast}(\\varphi_{k}) \\tag{c} \\end{align} $$\nこのとき、$+$と$\\wedge$はそれぞれ$k$-形式の合計とくさび積である。\n$\\omega, \\varphi$を$N$上の任意の二つの形式とする。$L$を$l$次元微分多様体、$g : L \\to N$とする。\n$$ \\begin{align*} f^{\\ast}(\\omega \\wedge \\varphi) \u0026amp;= (f^{\\ast}\\omega) \\wedge (f^{\\ast}\\varphi) \\tag{d} \\\\ (f \\circ g)^{\\ast} \\omega \u0026amp;= g^{\\ast}(f^{\\ast}\\omega) \\tag{e} \\end{align*} $$\n証明 証明 $(a)$ $$ \\begin{align*} (f^{\\ast}(\\omega + \\varphi))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\omega + \\varphi)_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ \\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) + \\varphi_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ (f^{\\ast} \\omega)_{p}(v_{1}, \\dots, v_{k}) + (f^{\\ast} \\varphi)_{p}(v_{1}, \\dots, v_{k}) \\\\ =\u0026amp;\\ \\left( f^{\\ast}\\omega + f^{\\ast}\\varphi \\right)_{p}(v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\n証明 $(b)$ $0$-形式$g$と$k$-形式$\\omega$の積を以下のように定義する。\n$$ (g\\omega)(p) = g(p) \\omega (p) $$\nここで、$g(p) = g_{p}$はスカラー、$\\omega (p) = \\omega_{p}$は関数であることに注意。それにより、\n$$ \\begin{align*} (f^{\\ast} (g\\omega))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ g\\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ g_{f(p)} \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ g\\circ f(p) \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ (f^{\\ast}g)_{p} (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\n証明 $(c)$ $$ \\begin{align*} (f^{\\ast}\\left( \\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k} \\right))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\varphi_{1} \\wedge \\dots \\wedge \\varphi_{k})_{f(p)} \\left( df_{1}, \\dots, df_{k} \\right) \\\\ =\u0026amp;\\ \\det [\\varphi_{i}df(v_{j})] \\\\ =\u0026amp;\\ \\det [ f^{\\ast} \\varphi_{i}(v_{j})] \\\\ \\end{align*} $$\n■\nManfredo P. Do Carmo, Differential Forms and Applications, p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3262,"permalink":"https://freshrimpsushi.github.io/jp/posts/3262/","tags":null,"title":"微分幾何学におけるプルバック"},{"categories":"줄리아","contents":"概要 多くの言語でデータフレームがサポートされているにも関わらず、毎回新しくてイライラすることが空の配列の作成です。\nコード タイプ指定 julia\u0026gt; using DataFrames\rjulia\u0026gt; df1 = DataFrame(x = Int64[], y = String[])\r0×2 DataFrame 実際に空の配列をデータとして入れればいいです。この時、タイプが指定され、データが全くない場合は、カラム名とタイプも表示されません。\njulia\u0026gt; push!(df1, [3, \u0026#34;three\u0026#34;])\r1×2 DataFrame\r│ Row │ x │ y │\r│ │ Int64 │ String │\r├─────┼───────┼────────┤\r│ 1 │ 3 │ three │\rjulia\u0026gt; push!(df1, [3.14, \u0026#34;pi\u0026#34;])\r┌ Error: Error adding value to column :x.\r└ @ DataFrames C:\\Users\\rmsms\\.julia\\packages\\DataFrames\\GtZ1l\\src\\dataframe\\dataframe.jl:1606\rERROR: InexactError: Int64(3.14) データを入れると、正常にカラム名とタイプが出力されます。タイプが合わない場合は、データが追加されないので注意が必要です。\nタイプ未指定 julia\u0026gt; df2 = DataFrame(x = [], y = String[])\r0×2 DataFrame\rjulia\u0026gt; push!(df2, [3, \u0026#34;three\u0026#34;])\r1×2 DataFrame\r│ Row │ x │ y │\r│ │ Any │ String │\r├─────┼─────┼────────┤\r│ 1 │ 3 │ three │\rjulia\u0026gt; push!(df2, [3.14, \u0026#34;pi\u0026#34;])\r2×2 DataFrame\r│ Row │ x │ y │\r│ │ Any │ String │\r├─────┼──────┼────────┤\r│ 1 │ 3 │ three │\r│ 2 │ 3.14 │ pi │ データフレームのタイプでストレスを感じたくない場合は、上記のようにAnyの空の配列を作ればいいです。タイプ指定と違って、データがうまく入ったことが確認できます。\n環境 OS: Windows julia: v1.6.2 ","id":2230,"permalink":"https://freshrimpsushi.github.io/jp/posts/2230/","tags":null,"title":"Juliaで空のデータフレームを作成する方法"},{"categories":"줄리아","contents":"説明 Clustering.jl パッケージの hclust() 関数を使えばいい。\nhclust(d::AbstractMatrix; [linkage], [uplo], [branchorder]) 距離行列 を入力として受け取り、階層的クラスタリング の結果を返す。クラスタ間の距離のデフォルトは 単一連結 だ。\nデンドログラム を描くには、Plots.jl ではなく StatsPlots.jl を使わなければならない。\nコード using StatsPlots using Clustering using Distances using Distributions\ra = rand(Uniform(-1,1), 2, 25)\rscatt = scatter(a[1,:], a[2,:], label=false)\rsavefig(scatt, \u0026#34;julia_hclust_scatter.png\u0026#34;) D_a = pairwise(Euclidean(), a, a)\rSL = hclust(D_a, linkage=:single)\rplot_SL = plot(SL)\rp = plot(scatt, plot_SL, size=(800,400))\rsavefig(p, \u0026#34;julia_hclust.png\u0026#34;) ","id":3259,"permalink":"https://freshrimpsushi.github.io/jp/posts/3259/","tags":null,"title":"ジュリアで階層的クラスタリングを行う方法"},{"categories":"줄리아","contents":"コード julia\u0026gt; findfirst(\u0026#34;li\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r8:9\rjulia\u0026gt; findlast(\u0026#34;li\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r14:15\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 1)\r3:3\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 4)\r8:8\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 9)\r14:14\rjulia\u0026gt; findfirst(r\u0026#34;t.+t\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r4:16 findfirst(pattern, A)\n文字列Aでpatternに合致する区間をRangeでリターンする。 パターンには正規表現が入れることができる。最後の例では、最初のtから最後のtまでの区間を見つけてリターンした。 環境 OS: Windows julia: v1.6.2 ","id":2226,"permalink":"https://freshrimpsushi.github.io/jp/posts/2226/","tags":null,"title":"ジュリア文字列で特定のパターン位置を見つける方法"},{"categories":"줄리아","contents":"説明 与えられたデータをhclust()で階層的クラスタリングした後、plot()関数を使ってデンドログラムを描こうとすると、以下のようなエラーが発生する。\nusing Clustering using Distances using Plots\ra = rand(2, 10)\rD_a = pairwise(Euclidean(), a, a)\rSL = hclust(D_a, linkage=:single)\rdendrogram = plot(SL)\rERROR: LoadError: Cannot convert Hclust{Float64} to series data for plotting デンドログラムを描くにはPlots.jlではなくStatsPlots.jlを使用する必要がある。\nusing StatsPlots\rdendrogram = plot(SL)\rsavefig(dendrogram, \u0026#34;julia_dendrogram.png\u0026#34;) ","id":3257,"permalink":"https://freshrimpsushi.github.io/jp/posts/3257/","tags":null,"title":"ジュリアでデンドログラムを描く方法"},{"categories":"줄리아","contents":"コード julia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, \u0026#34;er\u0026#34;)\rtrue\rjulia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, \u0026#34;et\u0026#34;)\rfalse\rjulia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, r\u0026#34;q?\u0026#34;)\rtrue contains(haystack::AbstractString, needle)\nhaystackにneedleが含まれているかをブーリアンで返す。needleにはr\u0026quot;...\u0026quot;のような正規表現が入れられる。 ちなみに、\u0026lsquo;haystack\u0026rsquo;は干し草の山という意味で、「干し草の山から針を探す」という意味の\u0026quot;a needle in a haystack\u0026quot;という表現がある。\n環境 OS: Windows julia: v1.6.2 ","id":2224,"permalink":"https://freshrimpsushi.github.io/jp/posts/2224/","tags":null,"title":"Juliaで特定の文字列を含むかどうかを確認する方法"},{"categories":"줄리아","contents":"概要 Primes.jlは、素数関連の関数や素因数分解を取り扱うパッケージだ。解析的整数論に関する関数の実装はまだ不足している。\nパッケージの全ての機能がまとめられているわけではなく、有用なものだけを選んだので、詳細はリポジトリをチェックしてくれ1。\nタイプ 素因数分解 Primes.Factorization julia\u0026gt; factor(12)\r2^2 * 3\rjulia\u0026gt; factor(12)[1]\r0\rjulia\u0026gt; factor(12)[2]\r2\rjulia\u0026gt; factor(12)[3]\r1\rjulia\u0026gt; factor(12)[4]\r0 素因数分解は、底と指数が区別され、独自のデータ型を使用する。インデックスとして底にアクセスすると、その指数を参照できる。\n関数 素数生成 prime(), primes() julia\u0026gt; using Primes\rjulia\u0026gt; prime(4)\r7\rjulia\u0026gt; primes(10)\r4-element Vector{Int64}:\r2\r3\r5\r7 prime(::Type{\u0026lt;:Integer}=Int, i::Integer)\ni番目の素数を返す。 primes([lo,] hi)\nhiまでの素数の配列を返す。 素数判定 isprime() julia\u0026gt; isprime(7)\rtrue\rjulia\u0026gt; isprime(8)\rfalse isprime(n::Integer)\nnが素数かどうかを判断した結果をブール値で返す。この関数の実装には、ミラーラビン判定法などが使用されている。 素因数分解 factor() julia\u0026gt; factor(24)\r2^3 * 3\rjulia\u0026gt; factor(Vector, 24)\r4-element Vector{Int64}:\r2\r2\r2\r3\rjulia\u0026gt; factor(Array, 24)\r4-element Vector{Int64}:\r2\r2\r2\r3\rjulia\u0026gt; factor(Set, 24)\rSet{Int64} with 2 elements:\r2\r3 factor(n::Integer) -\u0026gt; Primes.Factorization factor(ContainerType, n::Integer) -\u0026gt; ContainerType\nnの素因数分解を返す。この関数の実装には、ポラードの$p-1$ロー素因数分解アルゴリズムなどが使用されている。 ContainerTypeを指定すると、そのコンテナに合わせて結果を返し、別に指定しない場合は、自身のデータ型Primes.Factorizationで返す。 オイラーのφ関数 julia\u0026gt; totient(12)\r4 totient(n::Integer)\nn=$n$に対して、オイラーのφ関数$\\phi$を使用して、$\\displaystyle n \\prod_{p \\mid n} \\left( 1 - {{ 1 } \\over { p }} \\right)$を返す。 https://github.com/JuliaMath/Primes.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2222,"permalink":"https://freshrimpsushi.github.io/jp/posts/2222/","tags":null,"title":"ジュリアでの因数分解および素数関数の使用方法"},{"categories":"최적화이론","contents":"定義 1 行列 $A \\in \\mathbb{R}^{m \\times n}$、$\\mathbf{b} \\in \\mathbb{R}^{m \\times 1}$、そして $\\mathbf{c} \\in \\mathbb{R}^{n}$に関して、以下の線形計画問題を標準形Standard Formまたは方程式フォームEquational Formと呼ぶ。 $$ \\begin{matrix} \\text{Maximize} \u0026amp; \\mathbf{c}^{T} \\mathbf{x} \\\\ \\text{subject to} \u0026amp; A \\mathbf{x} = \\mathbf{b} \\\\ \u0026amp; \\mathbf{x} \\ge \\mathbf{0} \\end{matrix} $$\n$\\mathbf{c}^{T}$は転置を意味する。 最適化は最大化あるいは最小化を指す。 説明 標準形への変換 原則として、どんな線形計画問題も標準形に変換できる。例えば、以下のような最大化問題が与えられたとする。\n$$ \\begin{matrix} \\text{Maximize} \u0026amp; 3 x_{1} - 2 x_{2} \\\\ \\text{subject to} \u0026amp; 2 x_{1} - x_{2} \\le 4 \\\\ \u0026amp; x_{1} + 3 x_{2} \\ge 5 \\\\ \u0026amp; x_{2} \\ge 0 \\end{matrix} $$\n変換の基本的なアイディアは「不等式をまとめること」である。最初の制約 $$ 2 x_{1} - x_{2} \\le 4 $$ は実際、何らかの $x_{3} \\ge 0$について $$ 2 x_{1} - x_{2} + x_{3} = 4 $$ と表現しても全く問題ない。$2 x_{1} - x_{2}$が$4$より小さい量を$x_{3} \u0026gt; 0$が補う役割をするからである。このように方程式を緩める役割をする変数をスラック変数Slack Variableと呼ぶ。二番目の制約 $$ x_{1} + 3 x_{2} \\ge 5 $$ は両辺に$-1$を掛け、最初の制約で行ったように新しいスラック変数$x_{4}$を導入すれば $$ - x_{1} - 3 x_{2} + x_{4} = - 5 $$ である。ここまで見ると $$ \\begin{matrix} \\text{Maximize} \u0026amp; 3 x_{1} - 2 x_{2} \\\\ \\text{subject to} \u0026amp; 2 x_{1} - x_{2} + x_{3} = 4 \\\\ \u0026amp; - x_{1} - 3 x_{2} + x_{4} = - 5 \\\\ \u0026amp; x_{2}, x_{3}, x_{4} \\ge 0 \\end{matrix} $$ は方程式フォームの条件を満たしたように見えるが、まだ$x_{1} \\ge 0$という制約がないので、これを形式に合わせなければならない。与えられた問題で$x_{1}$は実数全体で許されているので、これを二つの正数の差$x_{1} = y_{1} - z_{1}$に分解すれば$x_{1}$は式から消え、次のような方程式フォームで表現できる。 $$ \\begin{matrix} \\text{Maximize} \u0026amp; 3 \\left( y_{1} - z_{1} \\right) - 2 x_{2} \\\\ \\text{subject to} \u0026amp; 2 \\left( y_{1} - z_{1} \\right) - x_{2} + x_{3} = 4 \\\\ \u0026amp; - \\left( y_{1} - z_{1} \\right) - 3 x_{2} + x_{4} = - 5 \\\\ \u0026amp; y_{1}, z_{1}, x_{2}, x_{3}, x_{4} \\ge 0 \\end{matrix} $$\n方程式フォームの幾何 方程式$A \\mathbf{x} = \\mathbf{b}$は一般的な平面、超平面を形成し、$\\mathbf{x} \\ge \\mathbf{0}$という条件によってユークリッド空間で円錐形の部分だけを取ることができるようになる。スラック変数が導入されることで次元は増えるが、最適化のために探索しなければならない空間自体は大幅に単純化される。\n一方、この解空間は他ならぬシンプレックスであり、ここからシンプレックスメソッドのアイデアにつながる。\nMatousek. (2007). 線形プログラミングを理解して使う: p41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2221,"permalink":"https://freshrimpsushi.github.io/jp/posts/2221/","tags":null,"title":"線形計画問題の方程式フォーム"},{"categories":"줄리아","contents":"概要 Polynomials.jlは多項式関数の表現や計算を含むパッケージだ。数学的に単純な多項式だからコーディングも簡単に考えがちだが、実際に必要な機能を実装し始めると、結構面倒だ。もちろんものすごく難しいわけではないが、できればパッケージを使用しよう。\nパッケージの全ての機能をまとめたわけではなく、役に立ちそうなものだけをピックアップしたので、詳しくはリポジトリをチェックしてほしい1。\n一般的な多項式関数 係数で多項式関数を定義する Polynomial() julia\u0026gt; using Polynomials\rjulia\u0026gt; p = Polynomial([1,0,0,1])\rPolynomial(1 + x^3)\rjulia\u0026gt; q = Polynomial([1,1])\rPolynomial(1 + x)\rjulia\u0026gt; r = Polynomial([1,1], :t)\rPolynomial(1 + t)\rjulia\u0026gt; p(0)\r1\rjulia\u0026gt; p(2)\r9 Polynomial{T, X}(coeffs::AbstractVector{T}, [var = :x])\nそれ自体が多項式関数を返す。coeffsは係数の配列で、最初が定数項で、後ろに行くほど高次項になる。 varでは多項式関数の変数を与える。例のようにシンボル :tを入れるとtの多項式になる。 データで多項式関数を定義する fit() julia\u0026gt; fit([-1,0,1], [2,0,2])\rPolynomial(2.0*x^2) fit(::Type{RationalFunction}, xs::AbstractVector{S}, ys::AbstractVector{T}, m, n; var=:x)\n$x$の座標がxsで、$y$の座標がysの点を通る多項式関数を返す。 根で多項式関数を定義する roots() julia\u0026gt; fromroots([-2,2])\rPolynomial(-4 + x^2) fromroots(::AbstractVector{\u0026lt;:Number}; var=:x)\n与えられた配列の要素が根になるような多項式関数を返す。 演算 +, -, *, ÷ julia\u0026gt; p + 1\rPolynomial(2 + x^3)\rjulia\u0026gt; 2p\rPolynomial(2 + 2*x^3) スカラーを加えたり乗じたりすると、上のように直感的に演算される。\njulia\u0026gt; p + q\rPolynomial(2 + x + x^3)\rjulia\u0026gt; p - q\rPolynomial(-x + x^3)\rjulia\u0026gt; p * q\rPolynomial(1 + x + x^3 + x^4)\rjulia\u0026gt; p ÷ q\rPolynomial(1.0 - 1.0*x + 1.0*x^2) 多項式関数間の四則演算は、+, -, *, ÷でオーバーライドされる。\n根を求める roots() julia\u0026gt; roots(p)\r3-element Vector{ComplexF64}:\r-1.0 + 0.0im\r0.4999999999999998 - 0.8660254037844383im\r0.4999999999999998 + 0.8660254037844383im roots(f::AbstractPolynomial)\nfの根をベクトルとして返す。 微分 derivative() julia\u0026gt; derivative(p, 3)\rPolynomial(6) derivative(f::AbstractPolynomial, order::Int = 1)\nfのorder次の導関数を返す。 積分 integrate() julia\u0026gt; integrate(p, 7)\rPolynomial(7.0 + 1.0*x + 0.25*x^4) integrate(f::AbstractPolynomial, C = 0)\n積分定数がCのfの不定積分を返す。 特別な多項式関数 ローラン多項式関数 LaurentPolynomial() julia\u0026gt; LaurentPolynomial([4,3,2,1], -1)\rLaurentPolynomial(4*x⁻¹ + 3 + 2*x + x²) LaurentPolynomial{T,X}(coeffs::AbstractVector, [m::Integer = 0], [var = :x])\n次数が整数に拡張されたローラン多項式関数を返す。 最小項の次数はmで与えられる。 チェビシェフ多項式関数 ChebyshevT() julia\u0026gt; ChebyshevT([3,2,1])\rChebyshevT(3⋅T_0(x) + 2⋅T_1(x) + 1⋅T_2(x)) ChebyshevT{T, X}(coeffs::AbstractVector)\n第一種チェビシェフ多項式関数を返す。式のT_n(x)は$T_{n}(x) = \\cos \\left( n \\cos^{-1} x \\right)$を表す。 環境 OS: Windows julia: v1.6.2 https://juliamath.github.io/Polynomials.jl/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2220,"permalink":"https://freshrimpsushi.github.io/jp/posts/2220/","tags":null,"title":"ジュリアで多項式を使用する方法"},{"categories":"줄리아","contents":"コード 文字列の連結 * julia\u0026gt; \u0026#34;oh\u0026#34; * \u0026#34;my\u0026#34; * \u0026#34;girl\u0026#34;\r\u0026#34;ohmygirl\u0026#34; Pythonの+に相当する。\n複数の文字列を連結する string() julia\u0026gt; string(\u0026#34;oh\u0026#34;,\u0026#34;my\u0026#34;, \u0026#34;girl\u0026#34;)\r\u0026#34;ohmygirl\u0026#34; Rのpaste0()に相当する。\n文字列のリストのアイテムとして連結する join() julia\u0026gt; OMG = [\u0026#34;oh\u0026#34;,\u0026#34;my\u0026#34;, \u0026#34;girl\u0026#34;]\r3-element Vector{String}:\r\u0026#34;oh\u0026#34;\r\u0026#34;my\u0026#34;\r\u0026#34;girl\u0026#34;\rjulia\u0026gt; join(OMG)\r\u0026#34;ohmygirl\u0026#34; Pythonのjoin()に相当する。\n同じ文字列を繰り返す ^ julia\u0026gt; \u0026#34;=-\u0026#34; ^ 10\r\u0026#34;=-=-=-=-=-=-=-=-=-=-\u0026#34; Pythonの*に相当する。繰り返しをべき乗で表現することは、偶然ではなく、Pythonで文字列を連結する二項演算が+(和)で、これを繰り返すことが*(積)であるように、Juliaでは連結する演算が*(積)で、これを繰り返すことが^(べき乗)になるのだ。\nなぜ？ なぜ、他の言語と同じように直感的に理解しやすい+ではなく*を使うのか？それは代数的、数学的観点から、文字列の結合は加法よりも乗法に近く、自然だからだ1。代数学の自由群というのを理解できれば最高だけど、そういう背景知識がなくても、数学でxとyの積をx * y = xyのように表すのは納得できるだろう。 $$ x \\ast y = xy $$ 今、\u0026quot;xy\u0026quot;という文字列に\u0026quot;litol\u0026quot;という文字列を付け足して、\u0026quot;xylitol\u0026quot;キシリトールを作ると考えてみよう。 $$ xy \\ast litol = xylitol $$ 納得できるだろう。今更\u0026quot;xy\u0026quot; + \u0026quot;litol\u0026quot;を考えると、何か変に感じるかもしれない。Juliaを作った人たちは、そういう数学的直感に真剣そのものだ。\n環境 OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/why-are-there-all-these-strange-stumbling-blocks-in-julia/92644/40\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2218,"permalink":"https://freshrimpsushi.github.io/jp/posts/2218/","tags":null,"title":"ジュリアで文字列を結合する方法"},{"categories":"줄리아","contents":"コード 1 using Plots\rx = rand(30)\ry = rand(30)\rz = rand(30)\rplot(x)\rplot!(y)\rplot!(z)\rpng(\u0026#34;result1\u0026#34;) 上のように、特定のデータだけ凡例に表示させたくない場合がある。\nlabel = \u0026quot;\u0026quot; plot(x, label = \u0026#34;\u0026#34;)\rplot!(y)\rpng(\u0026#34;result2\u0026#34;) そんな時は、label = \u0026quot;\u0026quot;というオプションを使えばいい。図には最初のデータが表示されているけど、凡例には現れないのがわかる。\nprimary = false plot!(z, primary = false)\rpng(\u0026#34;result3\u0026#34;) もうひとつの方法として、primary = falseというオプションを使うこともできるらしい。見ての通り、最後のデータがオレンジ色でプロットされ、凡例からは隠されている。「primary」をオフにすることで生じる副作用なので、可能ならば「label」オプションだけ触るようにしよう。\n環境 OS: Windows julia: v1.6.2 https://github.com/JuliaPlots/Plots.jl/issues/1388#issuecomment-363940741\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2216,"permalink":"https://freshrimpsushi.github.io/jp/posts/2216/","tags":null,"title":"ジュリアプロットで特定のデータラベルを隠す方法"},{"categories":"줄리아","contents":"コード 1 annotate!()を使えばいいんだ。以下のコードはブラウン運動で最大点と最小点をマークした絵を描くコードだよ。\nusing Plots\rcd(@__DIR__)\rdata = cumsum(randn(100))\rplot(data, color = :black, legend = :none)\rannotate!(argmax(data), maximum(data), \u0026#34;max\\n\u0026#34;)\rannotate!(argmin(data), minimum(data), \u0026#34;\\nmin\u0026#34;)\rpng(\u0026#34;result\u0026#34;) 環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/plots-annotate/37784\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2214,"permalink":"https://freshrimpsushi.github.io/jp/posts/2214/","tags":null,"title":"ジュリアプロットにテキストを挿入する方法"},{"categories":"줄리아","contents":"環境 OS: Windows julia: v1.6.2 エラー julia\u0026gt; plot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rGKS: glyph missing from current font: 48652\rGKS: glyph missing from current font: 46972\rGKS: glyph missing from current font: 50868\rGKS: glyph missing from current font: 47784\rGKS: glyph missing from current font: 49496 原因 韓国語フォントが見つからないためだ。\n解決法 二つの方法は特に理想的じゃないし、他にいい方法があれば、いつでも提案してほしい。Juliaを使用する上で韓国語があまり必要ないため、韓国語のサポートが不十分であることは事実だ。\ndefault(fontfamily = \u0026quot;\u0026quot;) 1 plot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;, fontfamily = \u0026#34;\u0026#34;)\rdefault(fontfamily = \u0026#34;\u0026#34;)\rplot(data, color = :red,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;) plot()のfontfamily = \u0026quot;\u0026quot;オプションやdefault(fontfamily = \u0026quot;\u0026quot;)を通じて韓国語を表示させることはできる。しかし、具体的なフォント名を変えても、うまく認識されず、保存する際には結局フォントを見つけられず、一枚目の画像のように文字化けすることなく、テキストが空白で表示される問題があることを確認した。\nPlots.plotly() 2 Plots.plotly()\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.html\u0026#34;) plotlyバックエンドを使用すると、韓国語の文字自体は表示される。しかし、*.pngに直接保存することはできず、*.htmlに出力した後、別途保存する必要がある。\nコード using Plots\r#Plots.gr()\rdata = cumsum(randn(100))\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;, fontfamily = \u0026#34;\u0026#34;)\rdefault(fontfamily = \u0026#34;\u0026#34;)\rplot(data, color = :red,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.png\u0026#34;)\rPlots.plotly()\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.html\u0026#34;) https://discourse.julialang.org/t/nice-fonts-with-plots-gr-and-latexstrings/60037\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://besixdouze.net/16\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2212,"permalink":"https://freshrimpsushi.github.io/jp/posts/2212/","tags":null,"title":"ジュリアプロットに韓国語テキストを挿入する方法"},{"categories":"최적화이론","contents":"定義 1 目的関数Objective Functionと制約Constraintが線形な最適化問題を線形計画問題Linear Programming Problem、略してLP問題という。簡単に言うと、線形問題とは、与えられた目的関数 $f: \\mathbb{R}^{n} \\to \\mathbb{R}$ がベクトル $\\mathbf{c} \\in \\mathbb{R}^{n}$ に対して $$ f \\left( \\mathbf{x} \\right) := \\mathbf{c}^{T} \\mathbf{x} $$ であり、与えられた行列 $A \\in \\mathbb{R}^{m \\times n}$ と $\\mathbf{b} \\in \\mathbb{R}^{m \\times 1}$ に対して $$ A \\mathbf{x} \\le \\mathbf{b} $$ を満たしながら $f$ の関数値が最適化される $\\mathbf{x} \\in \\mathbb{R}^{n}$ を見つける問題である。主に以下のように表される。 $$ \\begin{matrix} \\text{Optimize} \u0026amp; \\mathbf{c}^{T} \\mathbf{x} \\\\ \\text{subject to} \u0026amp; A \\mathbf{x} \\le \\mathbf{b} \\end{matrix} $$ このような問題では、まず、制約条件を満たす解を実行可能解Feasible Solutionとし、その中で目的関数を最大化または最小化する解を最適解Optimal Solutionという。\n$\\mathbf{c}^{T}$ は転置を意味する。 最適化は、最大化または最小化を指す。 説明 語り 線形とは、与えられた目的関数と制約条件が線形であるために付けられた言葉だ。式に$\\mathbf{x} = \\left( x_{1} , \\cdots , x_{n} \\right)$の二乗項やログなどの非線形関数型が入らず、制約も行列（不等）式の形で現れなければならない点から、適切な表現である。主に行列代数、線形代数でこのように等式で表された問題を解決することと対比される。 $$ A \\mathbf{x} = \\mathbf{b} $$\n計画は、我々が一般的に理解しているコンピュータのアプリケーションではなく、スケジュールやタスクを意味する。実際の線形計画法の応用では、各変数 $x_{1} , \\cdots , x_{n}$ は時間、エネルギー、資源など様々な要素を表すことができる。\n例えば、経済学や経営学では、これらの要素を最大化することは恐らく有用性、最小化することはコストに相当するだろう。出退勤時間と効率が異なる従業員をどの仕事に割り当てるかによる最適な勤務表を作成する場合、線形計画法が最も優先される方法となるだろう。\n例 2 $$ \\begin{matrix} \\text{Optimize} \u0026amp; x_{1} + x_{2} \\\\ \\text{subject to} \u0026amp; x_{1} \\ge 0 \\\\ \u0026amp; x_{2} \\ge 0 \\\\ \u0026amp; x_{2} - x_{1} \\le 1 \\\\ \u0026amp; x_{1} + 6 x_{2} \\le 15 \\\\ \u0026amp; 4 x_{1} - x_{2} \\le 10 \\end{matrix} $$\n上記のような線形問題を考えてみよう。条件が五つもあるため、一見複雑に見えるが、実際に扱う変数は$x_{1}$と$x_{2}$の二つだけなので、これを$2$次元の平面に表示すれば、以下の図のように条件を満たす領域を確認できる。\nこの領域に属する全ての点は、少なくとも制約条件を満たしている実行可能解である。ここから、目的関数$\\Lambda \\left( x_{1} , x_{2} \\right) = x_{1} + x_{2}$が最大になる点を見つければいい。幾何学的に言うと、実行可能解の領域と$x_{1} + x_{2} = \\lambda$が交差する部分の中で$\\lambda$が最も大きくなる場所、つまり直線の$y$切片が最も大きくなる点を見つければいいのだ。\n実際の例の解答は$\\lambda = 5$になるような$\\left( x_{1} , x_{2} \\right) = (3,2)$である。しかし、この解法はどこか見覚えがあるはずだ。一般的な教育課程を経ていれば、恐らく高校一年生ぐらいの時に教科書でこのような問題を見たことがあるだろう。このように基本的なアイデアは中学校を卒業したばかりの子供たちでも理解できるほどシンプルだ。元々「線形」が付いているということは、実際の解法がどうであれ、概念自体は簡単であるということだ。\nMatousek. (2007). Understanding and Using Linear Programming: p3.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMatousek. (2007). Understanding and Using Linear Programming: p1~2.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2207,"permalink":"https://freshrimpsushi.github.io/jp/posts/2207/","tags":null,"title":"線形計画問題の定義"},{"categories":"기하학","contents":"ガウス・ボンネの定理 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$を単連結な測地線座標切片写像、$\\boldsymbol{\\gamma}(I) \\subset \\mathbf{x}(U)$である$\\boldsymbol{\\gamma}$を区間ごとに正則曲線としよう。そして、$\\boldsymbol{\\gamma}$がある領域$\\mathscr{R}$を囲むとする。すると、以下が成立する。\n$$ \\iint_{\\mathscr{R}} K dA + \\int_{\\boldsymbol{\\gamma}} \\kappa_{g} ds + \\sum \\alpha_{i} = 2\\pi $$\nここで$K$はガウス曲率、$\\kappa_{g}$は測地曲率、$\\alpha_{i}$は$\\boldsymbol{\\gamma}$の区間と区間の間の接する点juntion pointでの角度差jump anglesである。\n説明 $\\boldsymbol{\\gamma}$を区間ごとに正則な曲線と仮定したので、タンジェントの方向が突然大きく変わる点があるが、その場所での角度差を$\\alpha_{i}$とした。$\\boldsymbol{\\gamma}$が全体的に滑らかにつながる曲線なら、角度がジャンプする場所はないので、$\\alpha_{i}$は$0$である。(図(が))\n上の定理は$\\mathbf{x}$を測地線座標切片写像という強い条件を置いたときの結果である。より一般的な結果では、式にオイラー指標が登場し、次のようである。\n$$ \\iint_{\\mathscr{R}} K dA + \\int_{C_{i}}\\kappa_{g}ds + \\sum\\alpha_{i} = 2\\pi \\chi(\\mathscr{R}) $$\n証明 $\\mathbf{x}$が測地線座標切片写像であるので、第1基本形式の係数を次のようにしよう。\n$$ \\left[ g_{ij} \\right] = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; h^{2} \\end{bmatrix} $$\nそして、$\\boldsymbol{\\gamma}(t) = \\mathbf{x}\\left( \\gamma^{1}(t), \\gamma^{2}(t) \\right)$としよう。今、$\\mathbf{x}_{1}$と$\\boldsymbol{\\gamma}$のタンジェント$T = \\boldsymbol{\\gamma}^{\\prime}$間の角度を$\\alpha$としよう。\n$$ \\alpha (t) := \\angle ( \\mathbf{x}_{1}, T) $$\n私たちは、$\\boldsymbol{\\gamma}$がパスに沿って一周するとき、$\\mathbf{x}_{1}$を基準にしたときの$T$の角度変化が$2 \\pi$であることを利用して、定理を証明するだろう。まず、$\\boldsymbol{\\gamma}$を単位速度曲線と仮定しよう。そして、$P$を次を満たす$\\boldsymbol{\\gamma}$に沿った平行なベクトル場としよう。(上の図(な)参照)\n$$ P(t) = \\text{parallel vector field starting from a juction point s.t. } \\dfrac{P \\times T}{\\left\\| P \\times T \\right\\|} = \\mathbf{n} $$\nそして、$\\phi$と$\\theta$をそれぞれ$\\mathbf{x_{1}}$と$P$および$P$と$T$間の角度としよう。\n$$ \\phi (t) = \\angle(\\mathbf{x}_{1}, P),\\quad \\theta (t) = \\angle(P, T) $$\n言い換えると、$\\left\\langle \\mathbf{x}_{1}, P(t) \\right\\rangle = \\cos\\phi (t)$であり、これを微分すると、\n$$ -\\sin \\phi (t) \\dfrac{d \\phi}{d t}(t) = \\left\\langle \\dfrac{d \\mathbf{x}_{1}(\\gamma^{1}(t), \\gamma^{2}(t))}{d t}, P(t) \\right\\rangle + \\left\\langle \\mathbf{x}_{1}, \\dfrac{d P}{d t}(t) \\right\\rangle $$\nこの時、$P$が$\\gamma$に沿って平行なベクトル場であるため、$\\dfrac{dP}{dt}$は定義により$M$と垂直である。$\\mathbf{x}_{1}$は$M$と接するので、後ろの項は$0$である。さらに計算すると、\n$$ \\begin{align*} \u0026amp;\\quad -\\sin \\phi (t) \\dfrac{d \\phi}{d t}(t) \\\\ \u0026amp;= \\left\\langle \\dfrac{d \\mathbf{x}_{1}(\\gamma^{1}(t), \\gamma^{2}(t))}{d t}, P(t) \\right\\rangle \\\\ \u0026amp;= \\Big[ \\mathbf{x}_{11}(\\gamma^{1}(t), \\gamma^{2}(t)) (\\gamma^{1})^{\\prime}(t) + \\mathbf{x}_{12}(\\gamma^{1}(t), \\gamma^{2}(t)) (\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\\\ \u0026amp;= \\Big[ \\left( L_{11}\\mathbf{n} + \\Gamma_{11}^{1}\\mathbf{x}_{1} + \\Gamma_{11}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{1})^{\\prime}(t) + \\left( L_{12}\\mathbf{n} + \\Gamma_{12}^{1}\\mathbf{x}_{1} + \\Gamma_{12}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\\\ \u0026amp;= \\Big[ \\left(\\Gamma_{11}^{1}\\mathbf{x}_{1} + \\Gamma_{11}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{1})^{\\prime}(t) + \\left(\\Gamma_{12}^{1}\\mathbf{x}_{1} + \\Gamma_{12}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\end{align*} $$\n二番目の等号は連鎖律により、三番目の等号は第2基本形式とクリストッフェル記号の定義によって成立する。四番目の等号は、$P$と$\\mathbf{n}$が互いに垂直であるために成立する。\n測地線座標切片写像のクリストッフェル記号\n下のもの以外はすべて$0$である。\n$$ \\Gamma_{22}^{1} = -hh_{1},\\quad \\Gamma_{12}^{2} = \\Gamma_{21}^{2} = \\dfrac{h_{1}}{h},\\quad \\Gamma_{22}^{2} = \\dfrac{h_{2}}{h} $$\nこれで、$0$になる項をすべて整理すると、以下のようになる。\n$$ -\\sin\\phi (t) \\phi^{\\prime}(t) = \\left\\langle \\dfrac{h_{1}}{h}(\\gamma^{2})^{\\prime}(t)\\mathbf{x}_{2}, P(t) \\right\\rangle = \\dfrac{h_{1}}{h}(\\gamma^{2})^{\\prime}(t) \\left\\langle \\mathbf{x}_{2}, P(t) \\right\\rangle\\tag{1} $$\n$g_{11} = \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{1} \\right\\rangle = 1$であるため、$\\mathbf{x}_{1}$は単位ベクトルであり、$g_{12} = \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\rangle = 0$であるため、$\\mathbf{x}_{1} \\perp \\mathbf{x}_{2}$である。したがって、$\\left\\{ \\mathbf{x}_{1}, \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|} \\right\\}$はタンジェント平面の正規直交基底となる。従って、タンジェント平面の要素$P$は、以下のように表される。\n$$ P = \\left\\langle \\mathbf{x}_{1}, P \\right\\rangle\\mathbf{x}_{1} + \\left\\langle \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|}, P \\right\\rangle \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|} = \\cos\\phi \\mathbf{x}_{1} + \\sin\\phi \\dfrac{\\mathbf{x}_{2}}{h} $$\nまた、$\\left\\langle \\mathbf{x}_{2}, P \\right\\rangle = \\left\\| x_{2} \\right\\|^{2} \\dfrac{\\sin \\phi}{h} = h\\sin \\phi$を$(1)$に代入すると、\n$$ \\phi^{\\prime}(t) = -h_{1}(\\gamma^{2})^{\\prime}(t) $$\nしたがって、$\\phi$の全角変動は\n$$ \\delta \\phi = \\int_{\\boldsymbol{\\gamma}} \\phi^{\\prime} dt = - \\int_{\\boldsymbol{\\gamma}}h_{1}(\\gamma^{2})^{\\prime}(t)dt = - \\int_{\\boldsymbol{\\gamma}}h_{1} d\\gamma^{2} = - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} \\tag{2} $$\nさらに、以下の式が成立することを示す。\n$$ \\text{Claim: } \\theta^{\\prime} = k_{g} $$\n$\\theta (t) = \\angle(P, T)$としたので、$\\cos\\theta (t) = \\left\\langle P, T \\right\\rangle$であり、これを微分すると、\n$$ -\\sin\\theta (t)\\theta^{\\prime}(t) = \\left\\langle \\dfrac{d P}{d t}, T \\right\\rangle + \\left\\langle P, \\dfrac{d T}{d t} \\right\\rangle = \\left\\langle P, T^{\\prime} \\right\\rangle $$\n二番目の等号は、$dP/dt$が$\\mathbf{n}$と平行であるために成立する。測地曲率の定義により、示したいものを以下のように得る。\n$$ \\begin{align*} \\kappa_{g} = \\left\\langle \\mathbf{S}, T^{\\prime} \\right\\rangle \u0026amp;= \\left\\langle (\\mathbf{n} \\times T), T^{\\prime} \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\mathbf{n}, (T \\times T^{\\prime}) \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\dfrac{P \\times T}{\\sin \\theta}, (T\\times T^{\\prime}) \\right\\rangle \u0026amp; \\because \\dfrac{P \\times T}{\\left\\| P \\times T \\right\\|} = \\mathbf{n} \\\\ \u0026amp;= \\left\\langle \\dfrac{1}{\\sin\\theta} P, (T\\times (T\\times T^{\\prime})) \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\dfrac{1}{\\sin\\theta} P, -T^{\\prime} \\right\\rangle \\\\ \u0026amp;= \\theta^{\\prime}(t) \\end{align*} $$\n三番目、五番目の等号はスカラー三重積が交換可能であるために成立する。したがって、以下を得る。\n$$ \\delta \\theta = \\int_{\\boldsymbol{\\gamma}} \\theta^{\\prime} dt = \\int_{\\boldsymbol{\\gamma}} k_{g}dt \\tag{3} $$\n$\\alpha = \\phi + \\theta$であるため、\n$$ \\int_{\\boldsymbol{\\gamma}} \\alpha^{\\prime}dt = \\int_{\\boldsymbol{\\gamma}} \\phi^{\\prime}dt + \\int_{\\boldsymbol{\\gamma}} \\theta^{\\prime}dt $$\n$(2)$と$(3)$により、以下を得る。\n$$ \\int_{\\boldsymbol{\\gamma}} \\alpha^{\\prime}dt + \\sum_{i}\\alpha_{i} = - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} + \\int_{\\boldsymbol{\\gamma}} k_{g}dt + \\sum_{i}\\alpha_{i} $$\n$\\boldsymbol{\\gamma}$は$\\mathscr{R}$を囲むため、上の式の左辺は明らかに一周したときの角度変化、すなわち$2 \\pi$である。\n$$ {} - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} + \\int_{\\boldsymbol{\\gamma}} k_{g}dt + \\sum_{i}\\alpha_{i} = 2 \\pi $$\nグリーンの定理\n$$ \\oint_{\\partial \\mathscr{R}} Pdx = - \\iint_{\\mathscr{R}} P_{y} dy dx $$\n測地線座標切片写像のガウス曲率\n$$ K = -\\dfrac{h_{11}}{h} $$\n曲面の面積要素\n$$ dA = \\sqrt{g} du^{1} du^{2} $$\n左辺の最初の項は、グリーンの定理を利用すると、以下のように変更できる。\n$$ \\begin{align*} {} - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} \u0026amp;= - \\iint_{\\mathscr{R}}h_{11} du^{1}du^{2} \\\\ \u0026amp;= - \\iint_{\\mathscr{R}}\\dfrac{h_{11}}{h} h du^{1}du^{2} \\\\ \u0026amp;= - \\iint_{\\mathscr{R}}\\dfrac{h_{11}}{h} \\sqrt{g} du^{1}du^{2} \\\\ \u0026amp;= \\iint_{\\mathscr{R}} K dA \\end{align*} $$\n最後に、以下の結論を得る。\n$$ \\iint_{R} K dA + \\int_{\\gamma} \\kappa_{g} ds + \\sum \\alpha_{i} = 2\\pi $$\n","id":3238,"permalink":"https://freshrimpsushi.github.io/jp/posts/3238/","tags":null,"title":"ガウス・ボーネの定理"},{"categories":"줄리아","contents":"コード 1 2 3 julia\u0026gt; replace(\u0026#34;qwerty\u0026#34;, \u0026#34;q\u0026#34;=\u0026gt;\u0026#34;Q\u0026#34;)\r\u0026#34;Qwerty\u0026#34;\rjulia\u0026gt; join(\u0026#34;qwerty\u0026#34;, \u0026#34;,\u0026#34;)\r\u0026#34;q,w,e,r,t,y\u0026#34;\rjulia\u0026gt; split(\u0026#34;qwerty\u0026#34;, \u0026#34;\u0026#34;)\r6-element Vector{SubString{String}}:\r\u0026#34;q\u0026#34;\r\u0026#34;w\u0026#34;\r\u0026#34;e\u0026#34;\r\u0026#34;r\u0026#34;\r\u0026#34;t\u0026#34;\r\u0026#34;y\u0026#34; ジュリアは文字列処理に特出している言語ではないけど、そのせいか、Pythonをたくさん真似して、簡単にそして早く学べる。既に知っている機能のほとんどが実装されていて、モジュールかどうかの部分を除けば、使い方はほとんど似ている。ちなみに、replace()を使う時、\u0026quot;q\u0026quot;=\u0026gt;\u0026quot;Q\u0026quot;は何か独特の文法ではなくて、ペアを直接使ったものだ。\n## 環境 - OS: Windows - julia: v1.7.0 https://docs.julialang.org/en/v1/base/collections/#Base.replace-Tuple{Any,%20Vararg{Pair,%20N}%20where%20N}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/strings/#Base.join\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/strings/#Base.split\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2205,"permalink":"https://freshrimpsushi.github.io/jp/posts/2205/","tags":null,"title":"PythonのようにJuliaで文字列を扱う方法"},{"categories":"기하학","contents":"定義 $\\mathscr{R}$を曲面 $M$のリージョンとする。$\\mathscr{R}$内の全ての閉曲線がヌル・ホモトピックであれば、$\\mathscr{R}$は単純連結simply connectedだと言われる。\n説明 $\\mathbb{R}^{2}$、ディスク$\\left\\{ x^{2} + y^{2} = r^{2} \\right\\}$、球体$\\mathbb{S}^{2}$のような簡単な例は、すぐに単純連結であると思われるだろう。しかし、下の図を見ると、トーラス $T^{2}$が単純連結ではないことが分かる。$\\gamma$と違って、$\\alpha$や$\\beta$は一点に収縮させることができない。\n","id":3236,"permalink":"https://freshrimpsushi.github.io/jp/posts/3236/","tags":null,"title":"単純連結領域"},{"categories":"줄리아","contents":"コード 比較演算子として$\\approx$を使えば、二つの値が十分に似ている時だけ真を返す。≈は$\\TeX$でと同じように、\\approxと入力してTabを押せば使える。\njulia\u0026gt; π ≈ 3.141592653\rtrue\rjulia\u0026gt; π ≈ 3.14159265\rtrue\rjulia\u0026gt; π ≈ 3.1415926\rfalse\rjulia\u0026gt; π ≈ 3.141592\rfalse 環境 OS: Windows julia: v1.7.0 ","id":2203,"permalink":"https://freshrimpsushi.github.io/jp/posts/2203/","tags":null,"title":"ジュリアで近似値をチェックする方法"},{"categories":"줄리아","contents":"コード 1 julia\u0026gt; d = Dict(\u0026#34;A\u0026#34;=\u0026gt;1, \u0026#34;B\u0026#34;=\u0026gt;2)\rDict{String, Int64} with 2 entries:\r\u0026#34;B\u0026#34; =\u0026gt; 2\r\u0026#34;A\u0026#34; =\u0026gt; 1\rjulia\u0026gt; push!(d,(\u0026#34;C\u0026#34;,3))\rERROR: MethodError: no method matching push!(::Dict{String, Int64}, ::Tuple{String, Int64})\rjulia\u0026gt; push!(d,\u0026#34;C\u0026#34; =\u0026gt; 3)\rDict{String, Int64} with 3 entries:\r\u0026#34;B\u0026#34; =\u0026gt; 2\r\u0026#34;A\u0026#34; =\u0026gt; 1\r\u0026#34;C\u0026#34; =\u0026gt; 3\rjulia\u0026gt; typeof(\u0026#34;C\u0026#34; =\u0026gt; 3)\rPair{String, Int64} ジュリアの辞書Dictionaryは、他のプログラミング言語でよく見られる、キーKeyと値Valueがペアになったデータ型だ。ジュリアの少し違う点は、辞書を各ペアPairの集まりと見なすことだ。提供された実行例で確認できるように、ペアは辞書を構成する要素だ。キーと値は右向きの矢印 =\u0026gt; を通して繋がれ、それ自体も Pair というデータ型を持つ。\n次の例はジュリアで文字列の一部を置換する方法を示している。\njulia\u0026gt; replace(\u0026#34;qwerty\u0026#34;, \u0026#34;q\u0026#34;=\u0026gt;\u0026#34;Q\u0026#34;)\r\u0026#34;Qwerty\u0026#34; Pythonと区別される特徴としては、ペアは辞書なしでもペア自体が存在できることだ。一つのペアだけを含む辞書ではなく、ペア自体をデータとして見るからに、ジュリアのコードはこのように新しい文法のようにペアを活用することもある。直接使うかどうかは別として、読むべきものだ。\n環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/base/collections/#Base.Dict\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2201,"permalink":"https://freshrimpsushi.github.io/jp/posts/2201/","tags":null,"title":"ジュリアから：辞書とペア"},{"categories":"줄리아","contents":"概要 JLD.jlは、Juliaを使用している間に発生する一時データを保存することができるパッケージだ1。純粋なJuliaプロジェクトを進行している際、データの入出力が面倒であれば役立つ。一方で、JLD.jlのインタフェースをより直感的に改善したJLD2.jlも利用可能である。2このポストに紹介された内容は、おおよそこのような機能があるということを認識し、可能な限りJLD2.jlを使用することをお勧めする。下位互換性も問題なくサポートされる。\n一方で、matファイルのようなものではなく、正確にマットラボのmatファイルを読み書きしたい場合は、MAT.jlパッケージを参照してください。\nコード using JLD\rcd(@__DIR__); pwd()\rnumpad = reshape(1:9, 3,3)\rcube = zeros(Int64, 3,3,3)\rsave(\u0026#34;mydata.jld\u0026#34;, \u0026#34;numpad\u0026#34;,numpad, \u0026#34;cube\u0026#34;,cube)\rmydata = load(\u0026#34;mydata.jld\u0026#34;)\rmydata[\u0026#34;numpad\u0026#34;]\rmydata[\u0026#34;cube\u0026#34;] 実行結果 julia\u0026gt; numpad = reshape(1:9, 3,3)\r3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\r1 4 7\r2 5 8\r3 6 9\rjulia\u0026gt; cube = zeros(Int64, 3,3,3)\r3×3×3 Array{Int64, 3}:\r[:, :, 1] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 2] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 3] =\r0 0 0\r0 0 0\r0 0 0\rjulia\u0026gt; save(\u0026#34;mydata.jld\u0026#34;, \u0026#34;numpad\u0026#34;,numpad, \u0026#34;cube\u0026#34;,cube)\r┌ Warning: JLD incorrectly extends FileIO functions (see FileIO documentation)\r└ @ FileIO C:\\Users\\rmsms\\.julia\\packages\\FileIO\\5JdlO\\src\\loadsave.jl:215 保存されるファイルの拡張子は*.jldでなければならない。保存される各データの名前を文字列で与え、割り当てられた変数を連続してつなげると、そのデータが一つにまとめて保存される。\njulia\u0026gt; mydata = load(\u0026#34;mydata.jld\u0026#34;)\r┌ Warning: JLD incorrectly extends FileIO functions (see FileIO documentation)\r└ @ FileIO C:\\Users\\rmsms\\.julia\\packages\\FileIO\\5JdlO\\src\\loadsave.jl:215 Dict{String, Any} with 7 entries:\r\u0026#34;_creator\\\\JULIA_PATCH\u0026#34; =\u0026gt; 0x00000001\r\u0026#34;cube\u0026#34; =\u0026gt; [0 0 0; 0 0 0; 0 0 0]…\r\u0026#34;_creator\\\\WORD_SIZE\u0026#34; =\u0026gt; 64\r\u0026#34;numpad\u0026#34; =\u0026gt; [1 4 7; 2 5 8; 3 6 9]\r\u0026#34;_creator\\\\JULIA_MINOR\u0026#34; =\u0026gt; 0x00000006\r\u0026#34;_creator\\\\ENDIAN_BOM\u0026#34; =\u0026gt; 0x04030201\r\u0026#34;_creator\\\\JULIA_MAJOR\u0026#34; =\u0026gt; 0x00000001 結果を見ると、辞書が返された。保存時に文字列で与えられた名前がキーKeyとして入り、実際のデータは値Valueにある。次のように、辞書として参照すればいい。\njulia\u0026gt; mydata[\u0026#34;numpad\u0026#34;]\r3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\r1 4 7\r2 5 8\r3 6 9\rjulia\u0026gt; mydata[\u0026#34;cube\u0026#34;]\r3×3×3 Array{Int64, 3}:\r[:, :, 1] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 2] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 3] =\r0 0 0\r0 0 0\r0 0 0 JLD2 例で、文字列で辞書を作る過程が不便だったが、JLD2.jlではネームドタプルを使って、同じ機能をより便利に利用することができる。\nhttps://github.com/JuliaIO/JLD.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaIO/JLD2.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2199,"permalink":"https://freshrimpsushi.github.io/jp/posts/2199/","tags":null,"title":"ジュリアで.matのようにデータを保存する方法"},{"categories":"줄리아","contents":"コード 1 Base.Iterators.enumerate() は、Pythonのように配列のインデックスと値の両方を参照できるイテレーターIteratorを返す。\njulia\u0026gt; x = [3,5,4,1,2]\r5-element Vector{Int64}:\r3\r5\r4\r1\r2\rjulia\u0026gt; for (idx, value) in enumerate(x)\rprintln(\u0026#34;x[▷eq1◁value\u0026#34;)\rend\rx[1]: 3\rx[2]: 5\rx[3]: 4\rx[4]: 1\rx[5]: 2\rjulia\u0026gt; typeof(enumerate(x))\rBase.Iterators.Enumerate{Vector{Int64}} https://docs.julialang.org/en/v1/base/iterators/#Base.Iterators.enumerate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2197,"permalink":"https://freshrimpsushi.github.io/jp/posts/2197/","tags":null,"title":"ジュリアのループでインデックスと値の両方を参照する方法"},{"categories":"줄리아","contents":"概要 Juliaに初めて接すると、戸惑うことも少なくないのがシンボルSymbolデータタイプである。シンボルは冒頭に:を付けて使用され、内部データなしにその名前そのもので機能する。主に名前やラベル、辞書のキーなどとして使われる1。\n説明 他のプログラミング言語では、関数にオプションを付ける時に数字を使ったり、意味を正確にするために文字列を使用することが多い。例えば、次の二つの関数がそうである。\njulia\u0026gt; function foo0(x, option = 0)\rif option == 0\rreturn string(x)\relseif option == 1\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo0 (generic function with 2 methods)\rjulia\u0026gt; foo0(3.0, 0)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo0(3.0, 1)\r3\rjulia\u0026gt; function foo1(x, option = \u0026#34;string\u0026#34;)\rif option == \u0026#34;string\u0026#34;\rreturn string(x)\relseif option == \u0026#34;Int\u0026#34;\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo1 (generic function with 2 methods)\rjulia\u0026gt; foo1(3.0, \u0026#34;string\u0026#34;)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo1(3.0, \u0026#34;Int\u0026#34;)\r3 その一方で、以下はシンボルを使用した定義である。一見すると、上記の二つの関数と違いがないように見える。\njulia\u0026gt; function foo2(x, option = :string)\rif option == :string\rreturn string(x)\relseif option == :Int\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo2 (generic function with 2 methods)\rjulia\u0026gt; foo2(3.0, :string)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo2(3.0, :Int)\r3 シンボルを使用する理由は、プログラムの途中で変更されることがないから簡単に説明できる。時にはこの点が不便と感じることもあるが、整数や文字列とは異なり、予期せぬところで変化する可能性が全くない。\nまた、シンボルは真の意味での指定、命令である。インターフェース的な観点では、文字列とシンボルは似ているが、例えば\u0026quot;Int\u0026quot;という文字列を受け取り、その文字列が整数を返す意味であるのに対し、シンボルが:Intとして直接来た場合は、質問もせずに整数型で返すという程度の違いである。この違いが理解できなくても、無理に共感する必要はない。\nその他、シンボルを使用する場合には、データフレームのカラム名など、文字列で表現すると変数と区別しにくい、または区別したくない場合などがある。慣れない表記法のため難しく感じるかもしれないが、用途と違いを理解すれば、特に問題はないという点を押さえておけば良い。\nhttps://docs.julialang.org/en/v1/base/base/#Core.Symbol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2195,"permalink":"https://freshrimpsushi.github.io/jp/posts/2195/","tags":null,"title":"ジュリアでのシンボル"},{"categories":"줄리아","contents":"ガイド 1 julia\u0026gt; x = rand(\u0026#39;a\u0026#39;:\u0026#39;c\u0026#39;, 10)\r10-element Vector{Char}:\r\u0026#39;a\u0026#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\r\u0026#39;a\u0026#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase) 上に示されたような配列があるとしよう。例で、私たちの目標は'a' と 'b' の両方を選ぶことだとしよう。自然には包含演算子 $\\in$でブロードキャストすればいいと思うかもしれないが、結果は以下の通りだ。\njulia\u0026gt; x .∈ [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]\rERROR: DimensionMismatch(\u0026#34;arrays could not be broadcast to a common size; got a dimension with lengths 10 and 2\u0026#34;) DimensionMismatch エラーが発生した。これは配列 x とカテゴリー ['a', 'b'] の両方に同時にブロードキャストが行われたために起きたエラーだ。エラーメッセージを解釈すると、x の長さ10と ['a', 'b'] の長さ2が同時に入ってきて混乱しているということだ。\njulia\u0026gt; x .∈ Ref([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;])\r10-element BitVector:\r1\r1\r1\r0\r1\r0\r0\r0\r1\r1 この場合は Ref() 関数を使ってブロードキャスト問題を解決できる。これにより ['a', 'b'] 内の 'a' と 'b' がスカラとして扱われ、このように二つのキャラクターがある場所だけを見つけることができた。\n注意事項 julia\u0026gt; y = rand(\u0026#39;a\u0026#39;:\u0026#39;c\u0026#39;, 1, 10)\r1×10 Matrix{Char}:\r\u0026#39;b\u0026#39; \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026#39;b\u0026#39; \u0026#39;a\u0026#39; \u0026#39;c\u0026#39; \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026#39;b\u0026#39; \u0026#39;c\u0026#39; 上に示されたような $1 \\times 10$ 行列の場合を考えてみよう。一見すると、上のガイドで見た場合と何も変わらないように思えるかもしれないが、.∈が全く異なる方法で使われている。\njulia\u0026gt; y .∈ [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]\r2×10 BitMatrix:\r0 1 0 0 1 0 1 0 0 0\r1 0 1 1 0 0 0 1 1 0 見ての通り、最初の行は 'a' の位置を、二行目は 'b' の位置を示している。これはベクトルか行列かという違いに由来するものだ。\njulia\u0026gt; y .∈ Ref([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;])\r1×10 BitMatrix:\r1 1 1 1 1 0 1 1 1 0 Ref() を使う場合、一貫した結果を得ることができる。\n環境 OS: Windows julia: v1.7.0 https://stackoverflow.com/a/59978386/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2193,"permalink":"https://freshrimpsushi.github.io/jp/posts/2193/","tags":null,"title":"ジュリアで配列の要素がリストに属しているかを確認する方法"},{"categories":"줄리아","contents":"説明 1次元配列（ベクトル）は次のように定義される。\njulia\u0026gt; A = [1; 2; 3]\r3-element Vector{Int64}:\r1\r2\r3 ここで、;は第一次元を基準に次の要素に移る意味を持つ。これを一般化すると、;;は第二次元を基準に次の要素に移る意味を持つ。\njulia\u0026gt; A = [1; 2; 3;; 4; 5; 6]\r3×2 Matrix{Int64}:\r1 4\r2 5\r3 6 同じ方法で3次元以上の配列を定義することができる。ちなみにこのコードはジュリアバージョン1.7以降で可能である。\njulia\u0026gt; A = [1 2; 3 4;;; 5 6; 7 8]\r2×2×2 Array{Int64, 3}:\r[:, :, 1] =\r1 2\r3 4\r[:, :, 2] =\r5 6\r7 8\rjulia\u0026gt; A = [1 2; 3 4;;; 5 6; 7 8 ;;;; 9 10; 11 12;;; 13 14; 15 16]\r2×2×2×2 Array{Int64, 4}:\r[:, :, 1, 1] =\r1 2\r3 4\r[:, :, 2, 1] =\r5 6\r7 8\r[:, :, 1, 2] =\r9 10\r11 12\r[:, :, 2, 2] =\r13 14 環境 OS: Windows10 Version: Julia 1.7.1 ","id":3223,"permalink":"https://freshrimpsushi.github.io/jp/posts/3223/","tags":null,"title":"ジュリアにおいて多次元配列を直接定義する方法"},{"categories":"줄리아","contents":"ガイド while while文は他の言語と変わらない。\njulia\u0026gt; while x \u0026lt; 10\rx += 1\rprint(\u0026#34;▷eq1◁i - \u0026#34;)\rend\r1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 -\rjulia\u0026gt; for i = 1:10\rprint(\u0026#34;▷eq2◁i - \u0026#34;)\rend\r1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 - ジュリアで主に使われるループのスタイルは、上に示した3つがある。一番上はRやパイソンで使われる方法に似ているし、二番目はマトラブに似ている。最もエレガントな表現は三番目の集合の内包表記を使用した方法だ。\nネストしたループ 以下の2つのループは機能的に全く同じだ。\njulia\u0026gt; X = 1:4; Y = 8:(-1):5;\rjulia\u0026gt; for x ∈ X\rfor y ∈ Y\rprint(\u0026#34; (▷eq3◁y) = ▷eq4◁x + ▷eq5◁(x + y)\u0026#34;)\rif y == 5 println() end\rend\r(1 + 8) = 9 (1 + 7) = 8 (1 + 6) = 7 (1 + 5) = 6\r(2 + 8) = 10 (2 + 7) = 9 (2 + 6) = 8 (2 + 5) = 7\r(3 + 8) = 11 (3 + 7) = 10 (3 + 6) = 9 (3 + 5) = 8\r(4 + 8) = 12 (4 + 7) = 11 (4 + 6) = 10 (4 + 5) = 9 まるで擬似コードPseudo Codeを書くかのようにコードが書かれているのがわかる。注意事項としては、以下のように反復子Iteratorとしてタプルを与えた場合だ。\njulia\u0026gt; for (x,y) ∈ (X, Y)\rprint(\u0026#34; (▷eq3◁y) = ▷eq7◁x + ▷eq5◁(x + y)\u0026#34;)\rif y == 5 println() end\rend\r(1 + 8) = 9 (2 + 7) = 9 (3 + 6) = 9 (4 + 5) = 9 ","id":2191,"permalink":"https://freshrimpsushi.github.io/jp/posts/2191/","tags":null,"title":"ジュリアでエレガントなループを使用する方法"},{"categories":"줄리아","contents":"説明 Julia REPLで右の角括弧 ] を入力すると、パッケージ管理モードに切り替えることができる。パッケージ管理モードで利用可能なコマンドは以下の通りだ。\nコマンド 機能 add foo foo パッケージを追加する。 free foo パッケージの固定を解除する。 help, ? これらのコマンドを表示する。 pin foo foo パッケージのバージョンを固定する。 remove foo, rm foo foo パッケージを削除する。 test foo foo パッケージをテスト実行する。 status, st インストールされている全てのパッケージとそのバージョンを表示する。パッケージ名を入力すると、そのパッケージのバージョンだけが表示される。 undo 最近の実行内容を取り消す。 update, up 全てのパッケージを最新バージョンに更新する。パッケージ名を入力することで、特定のパッケージだけを更新される。 環境 OS: Windows10 Version: Julia 1.7.1 ","id":3217,"permalink":"https://freshrimpsushi.github.io/jp/posts/3217/","tags":null,"title":"ジュリアパッケージ管理モードで使用可能なコマンドのリスト"},{"categories":"줄리아","contents":"説明 この写真は、Pythonでファントム$f$のラドン変換$\\mathcal{R}f$を計算し、それを*.npyファイルとして保存する過程を撮影したものです。ジュリアでこのファイルを読み込む場合は、PyCall.jlパッケージを使用すれば良いです。\nusing PyCall\rnp = pyimport(\u0026#34;numpy\u0026#34;) このコードはPythonでimport numpy as npを実行するのと同じです。そうすることで、Pythonのnumpyで使っているコードをそのまま$f$と$\\mathcal{R}f$を読み込むことができます。\nf = np.load(\u0026#34;f.npy\u0026#34;)\rRf = np.load(\u0026#34;Rf.npy\u0026#34;) しっかり読み込めているか、ヒートマップで確認してみましょう。\np1 = heatmap(reverse(f, dims=1), color=:viridis)\rp2 = heatmap(reverse(Rf, dims=1), color=:viridis)\rplot(p1, p2, size=(728,250)) 環境 OS: Windows10 バージョン: Julia 1.6.2, PyCall 1.93.0 ","id":3215,"permalink":"https://freshrimpsushi.github.io/jp/posts/3215/","tags":null,"title":"ジュリアでnpyファイルを読み込む方法"},{"categories":"줄리아","contents":"コード 例えば、$(5,5)$の配列のヒートマップの上に、$0$から$2\\pi$までのサイン曲線を描きたいとしよう。こんなコードを書きたくなるだろうが、図に見えるように、望んだ通りに出力されない。\nusing Plots\rA = rand(Bool, 5,5)\rheatmap(A, color=:greens)\rx = range(0, 2pi, length=100)\ry = sin.(x)\rplot!(x, y, color=:red, width=3) これは、配列$A$の横と縦の範囲が$1$から$5$までと認識されるからだ。これを解決するためには、下のようなコードで$A$の横と縦の範囲をそれぞれどこからどこまでか指定してやればいい。ちなみに、配列$A$の範囲を指定せずに、ヒートマップが表示される範囲だけを指定すると、下の(な)みたいに表示される。\nxₐ = range(0,2pi, length=5)\ryₐ = range(-1.5,1.5, length=5)\rp1 = heatmap(xₐ, yₐ, A, color=:greens, xlims=(0,2pi), ylims=(-1.5,1.5)) #그림 (가)\rp2 = heatmap(A, color=:greens, xlims=(0,2pi), ylims=(-1.5,1.5)) #그림 (나) これで、p1の上にサイン曲線を再度描くと、望んだ通りに正しく描かれる。\nplot!(x, y, color=:red, width=3) 環境 OS: Windows10 Version: Julia 1.7.1, Plots 1.25.3 ","id":3213,"permalink":"https://freshrimpsushi.github.io/jp/posts/3213/","tags":null,"title":"ジュリアでヒートマップにプロットを重ねて描く方法"},{"categories":"줄리아","contents":"コード 1 LaTeXStrings ライブラリを使うには、文字列の前に L を付けて、L\u0026quot;...\u0026quot; のように書く。\n@time using Plots\r@time using LaTeXStrings\rplot(0:0.1:2π, sin.(0:0.1:2π), xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;)\rtitle!(L\u0026#34;\\mathrm{TeX\\,representation:\\,} y = \\sin x , x \\in [0, 2 \\pi]\u0026#34;) 注意するべき点は、パッケージ名が正確に LaTeXStrings であり、通常のテキストには \\text{} が効かないため、代わりに \\mathrm{} を使う必要があること、スペースは \\, で行うことである。\n環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/latex-code-for-titles-labels-with-plots-jl/1967/18\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2180,"permalink":"https://freshrimpsushi.github.io/jp/posts/2180/","tags":null,"title":"ジュリアでプロットにTeXを使用する方法"},{"categories":"줄리아","contents":"説明 julia\u0026gt; x = [1 2 3]\r1×3 Matrix{Int64}:\r1 2 3\rjulia\u0026gt; y = [1 2 3 4]\r1×4 Matrix{Int64}:\r1 2 3 4\rjulia\u0026gt; x .+ y\rERROR: DimensionMismatch サイズが異なる二つのベクトルは、基本的に要素ごとの演算を行うことができない。これを直接実装するなら、二重for文を使う必要があるが、幸いにも一つを行ベクトル、もう一つを列ベクトルとして与えることで、次のように要素ごとに演算した二次元配列を返すことができる。これは、MATLABやPython NumPyでも可能だ。\nサイズが異なってもエラーにならない点で、意図しない計算になっているか注意が必要だ。\njulia\u0026gt; x\u0026#39; .+ y\r3×4 Matrix{Int64}:\r2 3 4 5\r3 4 5 6\r4 5 6 7\rjulia\u0026gt; x\u0026#39; .* y\r3×4 Matrix{Int64}:\r1 2 3 4\r2 4 6 8\r3 6 9 12\rjulia\u0026gt; x\u0026#39; ./ y\r3×4 Matrix{Float64}:\r1.0 0.5 0.333333 0.25\r2.0 1.0 0.666667 0.5\r3.0 1.5 1.0 0.75 もちろん、x' の代わりに transpose(x) を使ってもできる。\n環境 OS: Windows10 Version: Julia 1.6.2 ","id":3207,"permalink":"https://freshrimpsushi.github.io/jp/posts/3207/","tags":null,"title":"ジュリアで異なるサイズのベクトル成分ごとに操作する方法"},{"categories":"줄리아","contents":"コード 1 ブラウザがダークモードになっていれば、背景が透明になっているのをはっきりと確認できる。\nbackground_color オプションに :transparent シンボルを入れればいいんだ。*.pngとしてはちゃんと保存されるけど、*.pdfとしてはうまく保存されないそうだ。\nusing Plots\rplot(rand(10), background_color = :transparent)\rpng(\u0026#34;example\u0026#34;) オプション名から推測できるように、カラーシンボルを入れれば、その色で出力される。たとえば、黄色の :yellow で描いた絵はこんな感じだ。\n環境 OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/save-figure-with-transparent-background-color-in-plots-jl/18808/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2173,"permalink":"https://freshrimpsushi.github.io/jp/posts/2173/","tags":null,"title":"ジュリアでグラフィックスの背景を透明にする方法"},{"categories":"줄리아","contents":"特定の値まで塗る1 plot()の属性でfillrange=a、fillalpha=b、fillcolor=:colorを使うと、プロットされた曲線から値aまで:colorの色をbの透明度で塗る。fill=(a,b,:color)と書いても同じ機能をする。つまり、以下の2つのコードは同じだ。\nplot(x,y, fillrange=a, fillalpha=b, fillcolor=:color)\rplot(x,y, fill=(a,b,:color)) バグみたいだけど、fillrangeの値を$(0,1)$から選ぶと、塗りつぶしがされない。\nusing Plots\rrandom_walk = cumsum(rand(20).-.5)\rp1 = plot(random_walk,fill=(1,0.2,:lime), lw=3, legend=:bottomright)\rp2 = plot(random_walk,fill=(2,0.2,:tomato), lw=3, legend=:bottomright)\rplot(p1, p2) ２つの曲線の間を塗る fillalphaの値に片方の曲線の関数値を入れると、2つの曲線の間が色付けされる。\nrw = random_walk\rplot([rw rw.+1],fill=(rw.+1,0.2,:lime), lw=3, legend=:bottomright) 閉曲線の内部を塗る fillalphaの値を$(0,1)$だけではなく選ぶと、内部が塗りつぶされる。\ntheta = range(0,2pi, length=40)\rx = cos.(theta)\ry = sin.(theta)\rplot(x, y, fill=(1,0.2,:lime), xlim=(-3,3), ylim=(-1.5,1.5), size=(800,400), lw=3) 環境 OS: Windows10 Version: Julia 1.6.2, Plots 1.23.6 http://docs.juliaplots.org/latest/attributes/#fill\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3203,"permalink":"https://freshrimpsushi.github.io/jp/posts/3203/","tags":null,"title":"ジュリアでの曲線から特定の値まで/二つの曲線の間/閉曲線の内部の塗り方"},{"categories":"기하학","contents":"定義1 曲面 $M$ 上の点 $p$ における主曲率を $\\kappa_{1}, \\kappa_{2}$ としよう。$L$ を ワインガルテンマップ と称する。ガウス曲率Gaussian curvature $K$ を次のように定義する。\n$$ K := \\kappa_{1} \\kappa_{2} = \\det L = \\det ([{L^{i}}_{j}]) $$\nこの時、${L^{i}}_{j} = \\sum \\limits_{k} L_{kj}g^{ki}$ が成り立つ。\n公式 主曲率の積 $$ K = \\kappa_{1} \\kappa_{2} $$\nガウス曲率はリーマン曲率テンソルで表せる。 $$\n$$\nガウス曲率はクリストッフェル記号で表せる。 $$ K = $$\n点 $p$ におけるガウス曲率はガウスマップと領域の面積で示される。 $$ K = \\lim\\limits_{\\mathscr{R} \\to p} \\dfrac{A(\\nu (\\mathscr{R}))}{A(\\mathscr{R})} $$\n定理 $H^{2} \\ge K$ が成立する。\n$\\mathbf{X}, \\mathbf{Y}$ を点 $p$ における正規直交ベクトルとする。すると、次が成り立つ。\n$$ H = \\dfrac{1}{2}\\left( II(\\mathbf{X}, \\mathbf{X}) + II(\\mathbf{Y}, \\mathbf{Y}) \\right) $$\n$\\mathbf{Y} \\in T_{p}M$ を単位接ベクトル、$\\kappa_{n}$ を法曲率とし、$\\theta$ を主方向 $\\mathbf{X}_{1}$ と $\\mathbf{Y}$ の間の角度とする。すると、次が成り立つ。 $$ H = \\dfrac{1}{2\\pi}\\int_{0}^{2\\pi}\\kappa_{n}d\\theta $$\n証明 3. $\\kappa_{n} = II(\\mathbf{Y}, \\mathbf{Y})$ かつ、$II(\\mathbf{Y}, \\mathbf{Y}) = \\kappa_{1}\\cos^{2}\\theta + \\kappa_{2}\\sin^{2}\\theta$ ので、\n$$ \\begin{align*} \\dfrac{1}{2\\pi}\\int_{0}^{2\\pi}\\kappa_{n}d\\theta \u0026amp;= \\dfrac{1}{2\\pi}\\int_{0}^{2\\pi} \\kappa_{1}\\cos^{2}\\theta + \\kappa_{2}\\sin^{2}\\theta d\\theta \\\\ \u0026amp;= \\dfrac{1}{2\\pi} \\left( \\kappa_{1} \\int_{0}^{2\\pi} \\cos^{2} d\\theta + \\kappa_{2} \\int_{0}^{2\\pi}\\sin^{2}\\theta d\\theta \\right) \\\\ \u0026amp;= \\dfrac{1}{2\\pi} \\left( \\kappa_{1} \\pi + \\kappa_{2} \\pi \\right) \\\\ \u0026amp;= \\dfrac{\\kappa_{1} + \\kappa_{2}}{2} \\\\ \u0026amp;= H \\end{align*} $$\n(三角関数の積分表 $(2), (3)$ 参照)\n■\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p130\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3200,"permalink":"https://freshrimpsushi.github.io/jp/posts/3200/","tags":null,"title":"ガウス曲率と平均曲率"},{"categories":"정수론","contents":"定義 1 $p \\ge 2$の自然数が$1$と$p$だけを約数に持つ場合、素数Prime Numberと言う。 $m \\ge 2$の自然数が素数ではない場合、合成数Composite Numberと言う。 説明 定義によると、$2$は明らかに素数だ。\n数論で扱う数は非常に広く有理数にまで及ぶが、実際その研究対象は「素数論」とも呼べるほどの関心が集まっている。合成数は他の素数の積で表すことができ、素数について何らかの性質が明らかになれば、その一般化は比較的簡単なので、整数論の多くの定理では条件として素数を求めている。\nSilverman. (2012). 数論へのやさしい入門 (第4版): p46.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2163,"permalink":"https://freshrimpsushi.github.io/jp/posts/2163/","tags":null,"title":"素数と合成数"},{"categories":"프로그래밍","contents":"コード import pandas as pd\rdata = { \u0026#39;나이\u0026#39; : [26,23,22,22,21,21,20,20,20,20,18,17], \u0026#39;키\u0026#39; : [160, 163, 163, 162, 164, 163, 164, 150, 158, 162, 172, 173], \u0026#39;별명\u0026#39; : [\u0026#39;땡모\u0026#39;, \u0026#39;김쿠라\u0026#39;, \u0026#39;광배\u0026#39;, \u0026#39;오리\u0026#39;, \u0026#39;깃털\u0026#39;, \u0026#39;쌈무\u0026#39;, \u0026#39;밍구리\u0026#39;, \u0026#39;나부키 야코\u0026#39;, \u0026#39;월클토미\u0026#39;, \u0026#39;쪼율\u0026#39;, \u0026#39;안댕댕\u0026#39;, \u0026#39;워뇨\u0026#39;], \u0026#39;국적\u0026#39; : [\u0026#39;한국\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;] }\rindexName = [\u0026#39;권은비\u0026#39;,\u0026#39;미야와키 사쿠라\u0026#39;,\u0026#39;강혜원\u0026#39;, \u0026#39;최예나\u0026#39;, \u0026#39;이채연\u0026#39;, \u0026#39;김채원\u0026#39;, \u0026#39;김민주\u0026#39;, \u0026#39;야부키 나코\u0026#39;, \u0026#39;혼다 히토미\u0026#39;, \u0026#39;조유리\u0026#39;, \u0026#39;안유진\u0026#39;, \u0026#39;장원영\u0026#39;]\rdf = pd.DataFrame(data, index = indexName) 列のラベルは .columns で取得する。\n\u0026gt;\u0026gt;\u0026gt; df.columns Index([\u0026#39;나이\u0026#39;, \u0026#39;키\u0026#39;, \u0026#39;별명\u0026#39;, \u0026#39;국적\u0026#39;], dtype=\u0026#39;object\u0026#39;) \u0026gt;\u0026gt;\u0026gt; df.columns[2] \u0026#39;별명\u0026#39; 行のラベルは .index で取得する。.rowsではないことに注意しよう。\n\u0026gt;\u0026gt;\u0026gt; df.index Index([\u0026#39;권은비\u0026#39;, \u0026#39;미야와키 사쿠라\u0026#39;, \u0026#39;강혜원\u0026#39;, \u0026#39;최예나\u0026#39;, \u0026#39;이채연\u0026#39;, \u0026#39;김채원\u0026#39;, \u0026#39;김민주\u0026#39;, \u0026#39;야부키 나코\u0026#39;, \u0026#39;혼다 히토미\u0026#39;, \u0026#39;조유리\u0026#39;, \u0026#39;안유진\u0026#39;, \u0026#39;장원영\u0026#39;], dtype=\u0026#39;object\u0026#39;) \u0026gt;\u0026gt;\u0026gt; df.index[10] \u0026#39;안유진\u0026#39; ","id":3189,"permalink":"https://freshrimpsushi.github.io/jp/posts/3189/","tags":null,"title":"Python Pandasデータフレームの列と行の名前を取得する方法"},{"categories":"줄리아","contents":"コード 1 ==は値が同じかどうかを比較し、===は比較する値が可変Mutableかどうかによって異なる動作をする。\nMutable: 二項が同じオブジェクトを参照しているか確認する。つまり、プログラム上で二つの変数が区別できるかどうかを返す。 Immutable: 二項のタイプが同じかどうかをチェックし、 二項のストラクチャーが同じかどうかをチェックし、 各要素が==で同じかどうかを再帰的にチェックする。 julia\u0026gt; X = 1; Y = 1;\rjulia\u0026gt; X == Y\rtrue\rjulia\u0026gt; X === Y\rtrue\rjulia\u0026gt; X = [1]; Y = [1];\rjulia\u0026gt; X == Y\rtrue\rjulia\u0026gt; X === Y\rfalse 例えば、Pythonでよく見られる上記の実行結果を見てみよう。整数1はImmutableでプログラム上で区別が付かないため、==と===は同じように真を返したが、1だけを含む配列と見た場合にはXに新しい要素が追加されたとしたらXとYが異なる可能性があるMutableため、単に値を比較した==は真を返し、オブジェクト自体を比較した===は偽を返した。\nオブジェクトが何かわからなくても、この用法だけ理解すれば十分だ。\n最適化 オブジェクト指向性が弱いJuliaでは、このような差はそれほど大きく感じられない。コード最適化の観点から見ると、==と===の比較では、シングルトンSingtoneの比較で以下のような性能差が出る。\nN = 10^7\rx = rand(0:9, N)\rjulia\u0026gt; @time for t ∈ 1:N\rx[t] == 0\rend\r1.292501 seconds (30.00 M allocations: 610.336 MiB, 2.63% gc time)\rjulia\u0026gt; @time for t ∈ 1:N\rx[t] === 0\rend\r1.016211 seconds (30.00 M allocations: 610.336 MiB, 2.77% gc time) 値は通常Immutableなので、==よりも===の方が早いと理解すればいい。\nこのような差はほとんどの場合、それほど大きくないかもしれない。当然ながら、イテラティブIterativeではない作業は、次のようなベクター演算の方がずっと速く、この際の速度差はほぼないか無意味である。\njulia\u0026gt; @time x .== 0;\r0.009509 seconds (6 allocations: 1.196 MiB)\rjulia\u0026gt; @time x .=== 0;\r0.009478 seconds (6 allocations: 1.196 MiB) 環境 OS: Windows julia: v1.6.1 https://stackoverflow.com/a/38638838/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2157,"permalink":"https://freshrimpsushi.github.io/jp/posts/2157/","tags":null,"title":"ジュリアにおける==と===の違い"},{"categories":"기하학","contents":"定義1 $M$を曲面、$p \\in M$を曲面上の点とする。次のように定義される写像$L : T_{p}M \\to \\mathbb{R}^{3}$をバインガルテン・マップと呼ぶ。\n$$ L (\\mathbf{X}) = - \\mathbf{X}\\mathbf{n} $$\nこの時、$\\mathbf{X} \\in T_{p}M$は接ベクトルで、$\\mathbf{n}$は単位法線、$\\mathbf{X}\\mathbf{n}$は$\\mathbf{n}$の方向微分である。\n性質 $L$は$L : T_{p}M \\to T_{p}M$の線形変換である。\n$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$が$T_{p}M$の基底であるから、$L(\\mathbf{x}_{k}) = \\sum\\limits_{l}{L^{l}}_{k}\\mathbf{x}_{l}$とすると、次が成り立つ。\n$${L^{l}}_{k} = \\sum_{i}L_{ik}g^{il} = \\sum_{i}L_{ki}g^{il}$$\nここで、$L_{ij}$は第二基本形式の係数、$[g^{kl}]$は第一基本形式係数行列の逆行列である。行列で表すと、\n$$ \\begin{bmatrix} {L^{l}}_{k} \\end{bmatrix} = \\begin{bmatrix} {L^{1}}_{1} \u0026amp; {L^{1}}_{2} \\\\ {L^{2}}_{1} \u0026amp; {L^{2}}_{2} \\end{bmatrix} = \\begin{bmatrix} g^{li} \\end{bmatrix} \\begin{bmatrix} L_{ik} \\end{bmatrix} $$\n説明 定義におけるマイナス記号は便宜のために存在する。\nバインガルテン・マップは、それぞれの点$p$において、各接ベクトル方向への$\\mathbf{n}$の変化率を測る作用素として理解できる。この理由で、形状作用素shape operatorとも呼ばれる。\n定義により$L$は$T_{p}M$を$\\mathbb{R}^{3}$へ送る写像と定めたが、実際は$T_{p}M$へ送る写像となることが確認できる。\nつまり、${L^{l}}_{k}$は$L(\\mathbf{x_{k}})$の$l$番目の基底の係数である。すなわち、基底$B = \\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$に対する座標ベクトルで表せば、以下のようになる。 $$ L(\\mathbf{x}_{k}) = {L^{1}}_{k}\\mathbf{x}_{1} + {L^{2}}_{k}\\mathbf{x}_{2} $$ $$ \\left[ L(\\mathbf{x}_{k}) \\right]_{B} = \\begin{bmatrix} {L^{1}}_{k} \\\\ {L^{2}}_{k} \\end{bmatrix} $$ したがって、$L$の行列表現は、以下のようになる。 $$ [L]_{B} = \\begin{bmatrix} {L^{1}}_{1} \u0026amp; {L^{1}}_{2} \\\\ {L^{2}}_{1} \u0026amp; {L^{2}}_{2} \\end{bmatrix} $$ また、第一基本形式の性質により、以下が成り立つ。 $$ L_{ij} = \\sum_{l}L_{il}\\delta_{lj} = \\sum\\limits_{l,k} L_{il}g^{lk}g_{kj} = \\sum\\limits_{l,k} L_{li}g^{lk}g_{kj} = \\sum\\limits_{k}{L^{k}}_{i}g_{kj} $$\n$L$が有限次元ベクトル空間間の線形変換であるため、$\\tr{L}$と$\\det(L)$は不変量であり、これをそれぞれ平均曲率、ガウス曲率と呼ぶ。\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p125\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3188,"permalink":"https://freshrimpsushi.github.io/jp/posts/3188/","tags":null,"title":"バインガルテン・マップ"},{"categories":"확률미분방정식","contents":"モデル 1 $t$ 時点で $S_{t}$ を基礎資産 $1$単位の価格とし、$S_{t}$ が幾何ブラウン運動をすると仮定しよう。すなわち、標準ブラウン運動 $W_{t}$ とトレンドDrift $\\mu \\in \\mathbb{R}$ および拡散Diffusion $\\sigma^{2} \u0026gt; 0$ に対して、$S_{t}$ は次の確率微分方程式の解である。 $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ 無リスク金利 $r \\in \\mathbb{R}$ が与えられたとき、$t$ 時点での派生商品 $1$単位の価格 $F = F \\left( t, S_{t} \\right)$ は次の偏微分方程式に従う。 $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\n変数 $F \\left( t, S_{t} \\right)$: 派生商品Derivativesは、先物、オプションなどの金融商品を指す。 $S_{t}$: 基礎資産Underlying Assetsは、通貨、債券、株式など、派生商品の取引対象となる商品を指す。 パラメータ $r \\in \\mathbb{R}$: 無リスク資産Risk-free Assetsの利率を示す。無リスク資産の代表的な例には預金がある。 $\\sigma^{2} \u0026gt; 0$: 市場のボラティリティVolatilityを示す。 説明 派生商品に対する一般的な誤解とは異なり、先物FutureやオプションOptionは、不確実な未来に対するヘッジEdgeの手段として作られた。不確実な未来に備えるために、ある程度の費用Premiumを支払ってもリスクを減らすための方法であった。問題は、その価格を設定する適切な方法がなかったことであり、取引者は経験に基づいて感覚的に派生商品を取引していた。ブラック-ショールズモデルは、そのような派生商品の価格を数学的に説明できるようにした方程式である。\n一般的に、ブラック-ショールズモデル(1973)の貢献者としては、フィッシャー・ブラックFischer Blackとマイロン・ショールズMyron Scholesのほか、本投稿の「ヘッジを用いた導出」を紹介したロバート・マートンRobert K. Mertonの3人が挙げられる。残念ながらブラックは1995年に亡くなり、ショールズとマートンは1997年にノーベル経済学賞を受賞した。ブラック-ショールズ-マートン方程式の発見以降、オプション市場は急速に発展し、学界には金融工学という新たな分野の誕生をもたらした。\nブラックの喜劇 ウィキペディア2によると、ブラックは博士課程の時に専攻を頻繁に変え、どこかに定着するのが苦手だったという。物理学から数学、コンピューター、人工知能へと変更したが、結局名を残した分野は経済学になった。\n信頼できるリファレンスは見つからなかったが、筆者がどこかで聞いた話によると、ブラックは物理学を専攻していた時に、周囲の狂った天才たちを見て「ここでは生き残れない」と思ったという。後に経済/金融を学んでみると、数学を積極的に使う先駆者がいなく、理工学の怪物たちがいない荒れ地で、数学を武器に自らが先駆者となったという。\nショールズの悲劇 ナムウィキ3によると、ショールズは1997年のノーベル経済学賞の記者会見で、賞金で株投資をすると答えてセンセーションを巻き起こしたという。当時、ショールズが運用していたヘッジファンドは過度の自信から過剰なレバレッジLeverageを使用し、1998年のロシアの債務不履行で破綻したという。危機を乗り越えた後、ショールズ は最終的に投資家に利益を返し、その後もファンドマネージャーとして活動を続けたが、サブプライムモーゲージ危機が起こる直前に引退したという。\n前提 本格的な導出に先立ち、いくつかの前提について確認しておこう。\n手数料、税金、配当などの言及されていない要素は考慮しない 物理学モデルで興味の対象でない抵抗や温度、気圧などを考慮しないのと同じ程度に受け止めればよい。そこに加えて、トレンド $\\mu$ と $\\sigma$ などは単純に定数と仮定する。\n派生商品は基礎資産と時点に依存している 派生商品の価格が基礎資産に独立していれば、派生、基礎という言葉を使う理由がない。基礎資産の価格が変わるにつれて派生商品の価格が変わるのが妥当である。また、時間の経過によって変わらない（定数である）ならば、派生商品の価格を検討する意味がない。したがって、$F$ の形を正確には言えないが、少なくとも二つの要素 $t$ と $S_{t}$ に対する関数であると仮定する。 $$ F = F \\left( t, S_{t} \\right) $$\n基礎資産は幾何ブラウン運動をする 幾何ブラウン運動 GBMの代表的な応用は、まさに株価などの基礎資産の価格変動を説明することである。人口の変動量が全体の人口に比例するように、資産の価格変動も資産の価格に比例し、上場廃止にならない限りマイナスになることはないなど、良い前提を多く持っている。 $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$\nある株式の価格 $p_{t}$ が GBMに従うと仮定しよう。$t$日目の終値を$t-1$日目の終値で割り、対数を取った $$ r_{t} = \\nabla \\log p_{t} = \\log {{ p_{t} } \\over { p_{t-1} }} $$ をリターン―リターンReturnと呼ぶが、株価の大きさに関係なく価格が上がれば正の値、下がれば負の値になり、直感と一致する。対数正規分布の項で説明したように、このリターンは正規分布に従い、単純な上下ではなく、株価の成長と逆成長その本質に関心を持つものと見ることができる。\n無リスク資産はメルサス成長をする メルサス成長モデルは、人口動態学Population Dynamicsで、資源の制限や介入などがない場合の集団の成長を説明する最も単純なモデルであり、経済/金融のセンスでは無リスク資産の増殖を説明する前提になる。無リスク収益率は$r$ 定数として仮定され、その金融収益は資産 $N_{t}$ の規模に比例するため、次のような常微分方程式で表現できる。 $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$\n無裁定価格: ポートフォリオ間には価値の差がない ポートフォリオPortfolioに関する数式的な説明は、この証明でさらに詳しく説明する。 無裁定価格Arbitrage-free Pricingの前提とは、我々が考慮するすべてのポートフォリオが同じ価値でバランスを保っているということである。例えば、ポートフォリオ $A$ の価値が $B$ よりも高い場合、合理的な取引主体はより価値のある $A$ の比重を増やして裁定利益を得ることができるため、$B$ を考える理由がない。したがって、我々が考慮するポートフォリオは、このような裁定取引によってこれ以上利益を得ることができない状態であると仮定する。\n摩擦のない市場: 分割と空売りに制限がない 株をやったことがある人ならわかるが、この\n取引というのは最小限の値段単位があり、私が望む金額で取引できないし、空売りをしたくても日本の株式市場では貸借空売りが原則であり、制限がある。その取引単位を自由に分割でき、どんな制約もなく空売りできるということは、行動を妨げる摩擦がないと見ることができる。\n導出 Part 1. ポートフォリオの構成\n我々が保有できる資産は、次の3つの種類だけとしよう。\n基礎資産: $s$ 単位保有しているとしよう。 派生商品: $f$ 単位保有しているとしよう。 無リスク資産: 基礎資産でも派生商品でもない資産で、現金と考えても構わない。 $t$ 時点で我々が保有するすべての資産の価値を $V_{t}$ とすると、$S_{t}$ が基礎資産 $1$単位の価格であり、$F \\left( t , S_{t} \\right)$ が派生商品 $1$単位の価格であったので、次のように表せる。 $$ V_{t} = f F \\left( t, S_{t} \\right) + s S_{t} $$ ポートフォリオを構成するとは、この $f$ と $s$ の量を調整すること、つまりどのように投資するかについての戦略を立てることである。このようなポートフォリオ構成によって発生する取引量が多すぎて市場に影響を与えるという前提は非合理的であるため、基礎資産と派生商品の価格は、$f$ と $s$ の選択に関係なく一定と仮定しよう。つまり、$f$ と $s$ をどのように定めても、以下の数学的議論は変わらないということである。\n注意すべきは、$V_{t}$ は全資産の合計ではないということである。株式口座の残高だけを見ると考えればわかりやすい。ポートフォリオの例としてどのようなものがあるか考えてみよう：\n貯蓄 $V_{t} = 0$：株式口座を整理してすべて貯蓄し、利息だけを受け取る。数学しか知らない士人の目にはあまりにもトリビアルTrivialに見えるかもしれないが、暴落市場や不況に対処できる立派な戦略である。 アリAnt $V_{t} = 5 S_{t}$：個人ならば、派生商品には手を出さないようにしよう。個人が空売りが禁止されている国では、ほとんどの個人投資家はこのようなポートフォリオを持っている。例として、数式で $S_{t} = 81,200$ がサムスン電子の株価であれば、このポートフォリオはサムスン電子 $5$株を保有している私の友人「キム・スヒョン」の口座である。 ヘッジHedge $\\displaystyle V_{t} = 1 \\cdot F- {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t}$：コールオプションCall Optionを $1$だけ買い、その基礎資産を ${{ \\partial F } \\over { \\partial S_{t} }}$ だけ空売りしたとしよう。オプションの満期日に基礎資産の価格が大幅に上昇した場合、コールオプションが大きな利益をもたらし、基礎資産の価格がむしろ下落した場合は、空売りで既に利益を得ている。 ヘッジについての説明で触れたオプションは、ヨーロピアンオプションEuropean Optionであり、通常、私たちが知っている「満期日にのみ権利を行使できるオプション」である。アメリカンオプションAmerican Optionは満期前でも常に権利を行使できるが、大して知る必要はなく、ヨーロピアン、アメリカンという言葉に怖がることはないようにしよう。\n我々は最後の例、現物空売りで派生商品をヘッジするポートフォリオ $$ V_{t} = 1 \\cdot F \\left( t, S_{t} \\right) - {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t} $$ からブラック-ショールズ方程式を導出する。正確にヘッジしているので、このポートフォリオは無リスク資産であり、時間 $t$ に対する増分Incrementは $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} d S_{t} $$ である。ここで、$S_{t}$ は幾何ブラウン運動をすると仮定したので、$d S_{t}$ に $\\displaystyle S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right)$ を代入すると $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ である。一方、無裁定価格の前提を考えると、このポートフォリオと無リスク資産のポートフォリオの増分は同じでなければならない。もしポートフォリオ間に価格差があると仮定すると、他方のポートフォリオを処分して異なるポートフォリオに投資することで裁定利益を得ることができるためである。無リスク資産はメルサス成長をするという前提をしたので、無リスク金利 $r$ に対して、次のような常微分方程式で表される。 $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$ これを整理して表すと $$ \\begin{align*} d V_{t} =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\\\ d V_{t} =\u0026amp; r V_{t} dt \\end{align*} $$ であるため、 $$ \\begin{equation} r V_{t} dt = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\label{1} \\end{equation} $$ を得る。これで、$dF$ を求めるために伊藤積分を通じて計算してみよう。\nPart 2. 伊藤計算\n伊藤の公式: 伊藤過程 $\\left\\{ X_{t} \\right\\}_{t \\ge 0}$ が与えられているとする。 $$ d X_{t} = u dt + v d W_{t} $$ 関数 $V \\left( t, X_{t} \\right) = V \\in C^{2} \\left( [0,\\infty) \\times \\mathbb{R} \\right)$ に対して $Y_{t} := V \\left( t, X_{t} \\right)$ と置くと、$\\left\\{ Y_{t} \\right\\}$ も伊藤過程であり、次が成立する。 $$ \\begin{align*} d Y_{t} =\u0026amp; V_{t} dt + V_{x} d X_{t} + {{ 1 } \\over { 2 }} V_{xx} \\left( d X_{t} \\right)^{2} \\\\ =\u0026amp; \\left( V_{t} + V_{x} u + {{ 1 } \\over { 2 }} V_{xx} v^{2} \\right) dt + V_{x} v d W_{t} \\end{align*} $$\n幾何ブラウン運動で $S_{t}$ を分配法則に従って展開すると $$ d S_{t} = \\mu S_{t} dt + \\sigma S_{t} d W_{t} $$ であり、伊藤の公式で $u = \\mu S_{t}$ および $v = \\sigma S_{t}$ なので $$ d F = \\left( {{ \\partial F } \\over { \\partial t }} + {{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} $$ を得る。$\\eqref{1}$ の $d F$ にこれを代入してみると $$ \\begin{align*} r V_{t} dt =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt - {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} \\\\ =\u0026amp; \\left( {{ \\partial F } \\over { \\partial t }} + {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t}} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ \u0026amp; - {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt} - {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ =\u0026amp; {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt \\end{align*} $$ である。左辺のポートフォリオの価値 $V_{t}$ が $\\displaystyle V_{t} = F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t}$ のように定義されていたので、これを代入すると $$ r \\left( F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\right) dt = {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt $$ である。$rF$ に対して式を整理すると、求めていた次の方程式を得る。 $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\n■\n崔炳善. (2012). ブラック-ショールズ式の様々な導出\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Fischer_Black\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://namu.wiki/w/%EB%B8%94%EB%9E%99-%EC%88%84%EC%A6%88%20%EB%AA%A8%ED%98%95#s-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2156,"permalink":"https://freshrimpsushi.github.io/jp/posts/2156/","tags":null,"title":"ブラック-ショールズモデルの導出"},{"categories":"줄리아","contents":"コード 1 すごく簡単なんだけど、否定演算子の ! と ~ を単項演算子じゃなくて関数として見てしまって、!. や ~. を使う間違いをよくするよ。.! や .~ と書けばいいんだ。\njulia\u0026gt; a = rand(1,10) .\u0026lt; 0.5\r1×10 BitMatrix:\r1 1 0 0 1 0 1 0 0 0\rjulia\u0026gt; .!(a)\r1×10 BitMatrix:\r0 0 1 1 0 1 0 1 1 1\rjulia\u0026gt; .~(a)\r1×10 BitMatrix:\r0 0 1 1 0 1 0 1 1 1 環境 OS: Windows julia: v1.6.0 https://discourse.julialang.org/t/negation-of-boolean-array/16159/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2149,"permalink":"https://freshrimpsushi.github.io/jp/posts/2149/","tags":null,"title":"ジュリアでビット配列を反転させる方法"},{"categories":"줄리아","contents":"コード 1 using Gtk\rfile_name = open_dialog(\u0026#34;파일 열기\u0026#34;) 最初の引数として与えられる文字列は、ダイアログのタイトルだ。実行すると、こんな感じで「ファイルを開く」ダイアログがポップアップするのが確認できる。\n環境 OS: Windows julia: v1.6.0 https://discourse.julialang.org/t/choose-a-file-interactively/10910/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2143,"permalink":"https://freshrimpsushi.github.io/jp/posts/2143/","tags":null,"title":"ジュリアでfile.choose()のようにダイアログボックスを開いてファイルを選択する方法"},{"categories":"기하학","contents":"曲線に沿ったベクトル場[^1] 定義 曲面 $M$と曲線 $\\alpha : \\left[ a, b \\right] \\to M$が与えられたとする。それぞれの$t \\in \\left[ a,b \\right]$を点$\\alpha (t)$上で曲面$M$に対する接ベクトルに対応させる関数$\\mathbf{X}$を曲線$\\alpha$に沿ったベクトル場vector field along a curve$\\alpha$という。\n$$ \\mathbf{X} : \\left[ a, b \\right] \\to \\mathbb{R}^{3} \\\\ \\mathbf{X}(t) \\in T_{\\alpha (t)}M $$\n説明 定義で言う接ベクトルは、曲線$\\alpha$の接ベクトルではなく、$T_{\\alpha (t)}M$の要素である点$\\alpha (t)$での接ベクトルであることに注意しよう。曲面上の各点$\\alpha (t)$での接ベクトルは一意ではないため、曲線$\\alpha$に沿ったベクトル場も一意ではない。接平面に無限のベクトルがあるため$\\mathbf{X}$も無限に存在する。\n簡単な例として$M$上の曲線$\\alpha (t)$が与えられた時、$\\alpha (t)$の接ベクトル場である$\\mathbf{T}(t)$は$\\alpha$に沿ったベクトル場となる。$\\mathbf{S} = \\mathbf{n} \\times \\mathbf{T}$もまた$\\alpha$ベクトル場である。\n$\\mathbf{S}$と$\\mathbf{T}$は接空間の基底となるため、全ての$\\alpha$ベクトル場$\\mathbf{X}$は次のような線形結合で表される。\n$$ \\mathbf{X}(t) = A(t)\\mathbf{T}(t) + B(t)\\mathbf{S}(t)\\quad \\text{for some } A,B:[a,b]\\to \\mathbb{R} $$\n微分可能なベクトル場 定義 $\\alpha (t)$に沿ったベクトル場$\\mathbf{X}(t)$が微分可能であるdifferentiableとは、関数$\\mathbf{X} : \\left[ a, b \\right] \\to \\mathbb{R}^{3}$が微分可能であることを意味する。\n説明 正確には'$\\mathbf{X}$が微分可能である'と言うべきだが、'$\\mathbf{X}(t)$が微分可能である'とも便宜上言う。\n平行なベクトル場 定義 微分可能な$\\alpha$ベクトル場$\\mathbf{X}(t)$が与えられたとする。$\\dfrac{d \\mathbf{X}}{dt}$が曲面$M$と垂直なら、$\\mathbf{X}(t)$が$\\alpha (t)$に沿って平行であるparallel along$\\alpha (t)$と定義する。\n説明 上で説明したように、$\\alpha$ベクトル場は本当に任意に選ぶことができるが、「微分可能な$\\alpha$ベクトル場」という条件は、「平行な線」という概念を話すために制限を設けるものである。\n曲面$M$と垂直であるということは、$\\dfrac{d \\mathbf{X}}{dt}$が接方向の成分は持たず、法線方向の成分のみを持つということと同じである。定義を見ただけではなぜこのようなベクトル場を平行であるというのか理解しにくいかもしれないので、次の例を見よう。\n例 2次元平面で $xy-$平面上の曲線$\\boldsymbol{\\gamma}(t) = \\left( a(t), b(t), 0 \\right)$を考えよう。そして$\\mathbf{X}(t) = \\left( A(t), B(t), 0 \\right)$を$\\boldsymbol{\\gamma}$に沿ったベクトル場としよう。そうすると、\n$$ \\dfrac{d \\mathbf{X}}{dt} = \\left( \\dfrac{d A}{dt}, \\dfrac{d B}{dt}, 0 \\right) $$\nこのベクトルが$xy-$平面と垂直であるためには、任意の全てのベクトル$(x,y,0)$との内積が$0$である必要があるので、次を得る。\n$$ \\dfrac{d A}{dt} = 0 = \\dfrac{d B}{dt} $$\nしたがって$A(t), B(t)$は定数である。これを図で表すと次のようになり、私たちが直感的に考える「曲線$\\boldsymbol{\\gamma}$に沿って平行なベクトルたち」によく合っている。\n球面上で $M$を単位球面としよう。${\\color{6699CC}\\boldsymbol{\\gamma}(t)}$を赤道線としよう。そして$\\boldsymbol{\\gamma}$に沿ったベクトル場${\\color{295F2E}\\mathbf{X}_{\\boldsymbol{\\gamma}}(t) = (0, 0, 1)}$を考えてみよう。そうすると$\\dfrac{d \\mathbf{X}_{\\boldsymbol{\\gamma}}}{dt} = (0,0,0)$であるため、常に▷\n","id":3174,"permalink":"https://freshrimpsushi.github.io/jp/posts/3174/","tags":null,"title":"曲面に沿った平行ベクトル場の定義"},{"categories":"정수론","contents":"定義 1 二つの整数 $n$ と $m \\ne 0$ について、次を満たす整数 $k$ が存在するなら、$n$ は $m$ で割り切れると言う。 $$ n = mk $$ この時、$n$ を $m$ の 倍数Multiple、$m$ を $n$ の 約数Divisorと言い、以下のように示す。 $$ m \\mid n $$ $m$ が $n$ を割り切れない場合は、取り消し線を引いて $m \\nmid n$ と表示する。 $0$ ではない二つの整数 $a$、$b$ が与えられているとする。両方を割る約数の中で最も大きな数を $a$ と $b$ の 最大公約数Greatest Common Divisorと言い、$\\gcd (a,b)$ のように表記する。 もし $\\gcd (a,b) = 1$ ならば、$a$ と $b$ は 互いに素Relatively Primeであると言う。 説明 最大公約数と互いに素という概念は、ほとんどの人が小学校の時から見てきた概念だ。\n互いに素の英語表現である Relatively Primeから分かるように、実際に多くの人の言葉の習慣とは異なり（このポストのタイトルでもスペース無しで互いに素と書いているように）互いに素は、相対的にRelatively素Primeという意味だ。英語で「お互いに」と表現するにはMutuallyという単語がもっと適しているかもしれないが、数学全般でMutuallyにはそれなりに重要な意味があるため、相対的に(Relatively)が使われる。ここでの相対的というのは、二つの数 $a$ と $b$ は、$1$ 以外の約数を共有していないため、お互いに対しては事実上素数と同じ扱いができるという意味だ。絶対的には素数ではないかもしれないが、お互いに対しては素数として扱うことができるという程度に受け取れば良い。\n併せて見る 代数学的一般化 一意分解整域にて一般化される。\nSilverman. (2012). A Friendly Introduction to Number Theory (4th Edition): p30.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2137,"permalink":"https://freshrimpsushi.github.io/jp/posts/2137/","tags":null,"title":"最大公約数と互いに素"},{"categories":"함수","contents":"定義1 集合 $X$に対して、以下の関数 $I_{X} : X \\to X$を恒等関数identity functionという。\n$$ I_{X}(x) = x,\\quad \\forall x \\in X $$\n説明 主に以下のような記法が使われる。\n$$ I,\\quad \\text{id},\\quad \\text{1} $$\n微分多様体上の接ベクトルは、$\\dfrac{d (f\\circ \\alpha)}{d t}$のように定義され、微分される関数を\n$$ f \\circ \\alpha = f \\circ I \\circ \\alpha = f \\circ \\mathbf{x} \\circ \\mathbf{x}^{-1} \\circ \\alpha $$\nとして分解することで、任意の座標系 $\\mathbf{x}$ に対して接ベクトルを表現しつつ、その座標系の選択に依存しないようにすることができる。\n例 恒等行列 $$ I_{n\\times n} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\end{bmatrix} $$\nYou-Feng Lin, (2011). 集合論(Set Theory: An Intuitive Approach, 李興全 訳) (2011), p165\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3167,"permalink":"https://freshrimpsushi.github.io/jp/posts/3167/","tags":null,"title":"同一関数"},{"categories":"줄리아","contents":"コード 実のところ、Juliaは文字列のフォーマットなどが特に便利な言語ではない。コンソールに出力する際に文字列自体の機能を使う方法もあるが、round()関数のデフォルトオプションであるdigitsを使用する方が便利なことが多いだろう。\njulia\u0026gt; for k in 0:8\rprintln(round(π, digits = k))\rend\r3.0\r3.1\r3.14\r3.142\r3.1416\r3.14159\r3.141593\r3.1415927\r3.14159265 環境 OS: Windows julia: v1.6.0 ","id":2133,"permalink":"https://freshrimpsushi.github.io/jp/posts/2133/","tags":null,"title":"ジュリアで小数点以下特定の桁で丸める方法"},{"categories":"기하학","contents":"ビルドアップ $\\mathbf{x} : U \\to \\mathbb{R}^{3}$を座標写像とする。微分幾何学では、幾何学的な対象の特徴や性質を微分を通して説明する。したがって、座標切れ目$\\mathbf{x}$の導関数が色々な定理と公式で登場することになる。例えば、1次の導関数$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$は接空間$T_{p}M$の基底になる。したがって、任意の接線ベクトル$\\mathbf{X} \\in T_{p}M$は次のように表現することができる。\n$$ \\mathbf{X} = X^{1}\\mathbf{x}_{1} + X^{2}\\mathbf{x}_{2} $$\nそれでは、座標写像の2次の導関数$\\mathbf{x}_{ij} = \\dfrac{\\partial^{2} \\mathbf{x}}{\\partial u_{i} \\partial u_{j}}$について考えてみよう。これは$\\mathbb{R}^{3}$のベクトルなので、$\\mathbb{R}^{3}$の基底の線形組み合わせで表現することができる。でも、すでに$\\mathbb{R}^{3}$で互いに直交する3つのベクトルを知っているけど、それは1次の導関数と単位法線だ。\n$$ \\left\\{ \\mathbf{n}, \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\} $$\nすると、$\\mathbf{x}_{ij}$は次のように表される。\n$$ \\mathbf{x}_{ij} = a_{ij} \\mathbf{n} + b^{1}_{ij} \\mathbf{x}_{1} + b^{2}_{ij} \\mathbf{x}_{2} $$\nこれらの係数$b_{ij}^{1}, b_{ij}^{2}$をクリストッフェル記号という。さて、これらの係数を具体的に求めてみよう。第1基本形式の性質によって以下が成立する。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle \u0026amp;=\\ b_{ij}^{1}\\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{l} \\right\\rangle + b_{ij}^{2}\\left\\langle \\mathbf{x}_{2}, \\mathbf{x}_{l} \\right\\rangle \\\\ \u0026amp;\u0026amp; \u0026amp;=\\ \\sum\\limits_{k^{\\prime}=1}^{2}b_{ij}^{k^{\\prime}}\\left\\langle \\mathbf{x}_{k^{\\prime}}, \\mathbf{x}_{l} \\right\\rangle \\\\ \u0026amp;\u0026amp; \u0026amp;=\\ \\sum\\limits_{k^{\\prime}=1}^{2}b_{ij}^{k^{\\prime}} g_{k^{\\prime}l} \\\\ \\implies \u0026amp;\u0026amp; \\sum\\limits_{l=1}^{2}\\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} \u0026amp;=\\ \\sum\\limits_{l=1}^{2}\\sum\\limits_{k^{\\prime}=1}^{2}b_{ij}^{k^{\\prime}} g_{k^{\\prime}l}g^{lk} \\\\ \u0026amp;\u0026amp; \u0026amp;=\\ \\sum\\limits_{k^{\\prime}=1}^{2}b_{ij}^{k^{\\prime}} \\delta_{k^{\\prime}}^{k} \\\\ \u0026amp;\u0026amp; \u0026amp;=\\ b_{ij}^{k} \\end{align*} $$\nさて、これらの$b_{ij}^{k}$を$\\Gamma_{ij}^{k}$と表記して、次のように定義しよう。\n定義 以下のように定義される$\\Gamma_{ij}^{k}(1\\le i,j,k \\le 2)$をクリストッフェル記号という。\n$$ \\Gamma_{ij}^{k} := \\sum \\limits_{l=1}^{2} \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} = \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} $$\n$\\sum$の省略された式はアインシュタインの記法を使用している。\n説明 $\\mathbf{x}_{12} = \\mathbf{x}_{21}$であるから、$\\Gamma_{12}^{k} = \\Gamma_{21}^{k}$である。\n$\\mathbf{x}_{ij}$の接成分$b_{ij}^{k}$を$\\Gamma_{ij}^{k}$と表記し、クリストッフェル記号と呼び、$\\mathbf{x}_{ij}$の法成分$a_{ij}$を$L_{ij}$と表記し、第2基本形式の係数と呼ぶ。\nここで紹介したクリストッフェル記号は具体的には第2クリストッフェル記号である。第1クリストッフェル記号は以下のように定義される。\n$$ \\Gamma_{ij \\vert l} := \\sum \\limits_{k=1}^{2} \\Gamma_{ij}^{k}g_{kl} $$\n通常、クリストッフェル記号と言えば第2記号を指す。これらの記号を初めて使ったのはG.B.クリストッフェルで、当時は第2記号を$\\begin{Bmatrix} ij \\\\ k \\end{Bmatrix}$としていたという。\n参照 第1基本形式 第2基本形式 ","id":3158,"permalink":"https://freshrimpsushi.github.io/jp/posts/3158/","tags":null,"title":"微分幾何学におけるクリストッフェル記号"},{"categories":"줄리아","contents":"コード 1 ヒートマップを描く時、数値に応じて値のスケールが固定されないと困ることがある。基本のヒートマップ関数でclimオプションを通じて色の範囲を固定することができる。\nusing Plots\rcd(@__DIR__)\rheatmap(rand(4,4)); png(\u0026#34;1.png\u0026#34;)\rheatmap(rand(4,4), clim = (0,1)); png(\u0026#34;2.png\u0026#34;) 結果は以下の通りだ。最初のヒートマップは範囲がないが、二番目のヒートマップは0と1で範囲が固定されているのを確認できる。\n一方だけを制限 heatmap(rand(4,4), clim = (0,Inf))\rheatmap(rand(4,4), clim = (-Inf,1)) 上限だけを置くか、下限だけを置きたい場合は、上に示すようにInfを通じて上下限を開けることができる。\n環境 OS: Windows julia: v1.6.0 同じく見る Pythonのmatplotlib.pyplotで https://discourse.julialang.org/t/setting-min-and-max-values-in-a-heatmap-plots-jl/36496\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2126,"permalink":"https://freshrimpsushi.github.io/jp/posts/2126/","tags":null,"title":"ジュリアでヒートマップの色範囲を指定する方法"},{"categories":"확률미분방정식","contents":"定義 1 $$ d X(t) = f \\left( t, X(t) \\right) dt + g \\left( t, X(t) \\right) d W_{t} \\qquad , t \\in \\left[ t_{0} , T \\right], T \u0026gt; 0 $$\n上記の形式の方程式は、確率微分方程式、略してSDEと呼ばれる。ここで、$f$と$g$はそれぞれドリフトDrift、ディフュージョンDiffusionの係数関数と呼ばれる。初期条件$X_{0} := X \\left( t_{0} \\right)$に対する積分形は、次のように表される。\n$$ X(t) = X_{0} + \\int_{t_{0}}^{t} f \\left( s, X (s) \\right) ds + \\int_{t_{0}}^{t} g \\left( s, X (s) \\right) d W_{s} $$\n説明 $$ d X_{t} = f \\left( t, X_{t}\\right) dt + g \\left( t, X_{t} \\right) d W_{t} $$\nこの形が気にならなければ、伊藤の微積分学をよく勉強しているか、微分方程式をほとんど知らないか、のどちらかだと思う。微分方程式には慣れているがSDEには不慣れな人にとって、自然に$g d W_{t}$が目障りになるべきだ。SDEはODEと異なり、このような確率過程が含まれており、モデルに不確実性を加えている。この項を$0$として考え、つまり$g d W_{t} = 0$の非決定論的システムとして見れば、次のようになる。 $$ \\begin{align*} d X(t) =\u0026amp; f \\left( t, X(t) \\right) dt + g \\left( t, X(t) \\right) d W_{t} \\\\ =\u0026amp; f \\left( t, X(t) \\right) dt + 0 \\\\ =\u0026amp; f \\left( t, X(t) \\right) dt \\end{align*} $$ 両辺を$dt$で割ると $$ {{ d X (t) } \\over { dt }} = f \\left( t, X(t) \\right) $$ そのため、私たちがよく知る非自律系の様子を取り戻したことを確認できる。\nドリフト この説明で、時系列解析のドリフトを思い出すと、係数関数$f$をドリフトと呼ぶのはかなり自然である。後ろの項がどうであれ、システム自体をシステムとして扱うことができる原動力は$f dt$だからだ。\nディフュージョン それでは、$g$を拡散Diffusionと呼ぶことは、その役割や性質が広がること、散らばることを自然に連想させる。確率微分方程式では、これは白色雑音の概念を指し、このようなノイズにより伊藤の公式のような独特の結果が生じる。\nPanik. (2017). Stochastic Differential Equations: An Introduction with Applications in Population Dynamics Modeling: p133.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2125,"permalink":"https://freshrimpsushi.github.io/jp/posts/2125/","tags":null,"title":"確率微分方程式とは?"},{"categories":"기하학","contents":"ビルドアップ $\\mathbf{x} : U \\to \\mathbb{R}^{3}$を座標チャートと呼ぼう。微分幾何学では、幾何学的な対象の特徴や性質を微分を通じて説明する。だから、座標チャートの微分$\\mathbf{x}$が色んな定理や公式に登場する。例えば、1次の微分$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$は接空間$T_{p}M$の基底になる。したがって、任意の接ベクトル$\\mathbf{X} \\in T_{p}M$は以下のように表される。\n$$ \\mathbf{X} = X^{1}\\mathbf{x}_{1} + X^{2}\\mathbf{x}_{2} $$\nそれでは、座標チャートの2次の微分$\\mathbf{x}_{ij} = \\dfrac{\\partial^{2} \\mathbf{x}}{\\partial u_{i} \\partial u_{j}}$について考えてみよう。これは$\\mathbb{R}^{3}$のベクトルなので、$\\mathbb{R}^{3}$の基底の線形組み合わせで表すことができる。だが、私たちは既に$\\mathbb{R}^{3}$で互いに直交する3つのベクトルを知っている。それは1次の微分と単位法線だ。\n$$ \\left\\{ \\mathbf{n}, \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\} $$\nすると$\\mathbf{x}_{ij}$は次のように表すことができる。\n$$ \\mathbf{x}_{ij} = a_{ij} \\mathbf{n} + b^{1}_{ij} \\mathbf{x}_{1} + b^{2}_{ij} \\mathbf{x}_{2} $$\n$\\mathbf{x}_{ij}$の$\\mathbf{n}$項の係数$a_{ij} = \\left\\langle \\mathbf{x}_{ij}, \\mathbf{n} \\right\\rangle$を$\\mathbf{x}$の第2基本形式の係数coefficient of the second fundamental formと呼ぶ。\n定義 $\\mathbf{x}_{ij}$と単位法線$\\mathbf{n}$の内積を$L_{ij}$と表記し、第2基本形式の係数と呼ぶ。\n$$ L_{ij} := \\left\\langle \\mathbf{x}_{ij}, \\mathbf{n} \\right\\rangle $$\n$\\mathbf{X}, \\mathbf{Y}$を曲面$\\mathbf{x}$の接空間$T_{P}M$のベクトルとしよう。接空間の基底は$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$なので、次のように表すことができる。\n$$ \\mathbf{X} = X^{1}\\mathbf{x}_{1} + X^{2}\\mathbf{x}_{2} \\quad \\text{and} \\quad \\mathbf{Y} = Y^{1}\\mathbf{x}_{1} + Y^{2}\\mathbf{x}_{2} $$\n次のような双線形形式$II$を曲面$\\mathbf{x}$の第2基本形式the second fundamental formと定義する。\n$$ II (\\mathbf{X}, \\mathbf{Y}) = \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} L_{ij}X^{i}Y^{j} = L_{ij}X^{i}Y^{j} = \\begin{bmatrix} X^{1} \u0026amp; X^{2}\\end{bmatrix} \\begin{bmatrix} L_{11} \u0026amp; L_{12} \\\\ L_{21} \u0026amp; L_{22} \\end{bmatrix} \\begin{bmatrix} Y^{1} \\\\ Y^{2}\\end{bmatrix} $$\n$\\sum$が省略された式はアインシュタインの記法を使用したものだ。\n説明 $\\mathbf{x}_{12} = \\mathbf{x}_{21}$なので、$L_{12} = L_{21}$だ。\n$\\mathbf{x}_{ij}$の法線成分normal component$a_{ij}$を$L_{ij}$と表記し、第2基本形式の係数と呼び、$\\mathbf{x}_{ij}$の接成分tangential components$b_{ij}^{k}$を$\\Gamma_{ij}^{k}$と表記し、クリストッフェル記号と呼ぶ。\n第1基本形式が曲面上の曲線の長さに関連する関数であったように、第2基本形式は曲面がどれほど曲がっているかの指標であり、ガウス曲率$\\kappa_{n}$と関連している。\n第1基本形式はリーマン計量という別の名前でも呼ばれるが、第2基本形式は単に第2基本形式と呼ばれる。\n同時参照 第1基本形式 クリストッフェル記号 ","id":3156,"permalink":"https://freshrimpsushi.github.io/jp/posts/3156/","tags":null,"title":"微分幾何学における第2基本形式"},{"categories":"줄리아","contents":"概要 1 Pythonでは、zfill()は文字列クラスのメソッドとして、左側を0で埋める機能を持っている。しかし、Juliaではもっと汎用的で使い勝手の良い組み込み関数としてlpad()を提供している。zfill()はゼロで埋めるって意味で、lpad()は左のパディングって意味だ。\nコード julia\u0026gt; lpad(\u0026#34;12\u0026#34;, 4, \u0026#34;0\u0026#34;)\r\u0026#34;0012\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;0\u0026#34;)\r\u0026#34;0012\u0026#34; 概要での説明を続けると、Juliaでのlpad()はzfill()に比べてもっとジェネリックだ。文字列のメソッドではないから、文字列を引数にしても数字を引数にしても、勝手に文字列にして返してくれる。\njulia\u0026gt; lpad(12, 4)\r\u0026#34; 12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_\u0026#34;)\r\u0026#34;__12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_!\u0026#34;)\r\u0026#34;_!12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_?!\u0026#34;)\r\u0026#34;_?12\u0026#34;\rjulia\u0026gt; lpad(12, 7, \u0026#34;_?!\u0026#34;)\r\u0026#34;_?!_?12\u0026#34; こういう関数を使う一般的な理由は、出力をきれいに整えるためで、必ずしも0が必要だからではない。埋める文字を何も与えなければ、スペースを入れて、キャラクターかストリングを与えれば、上に示したように賢く埋めてくれる。\njulia\u0026gt; rpad(\u0026#34;left\u0026#34;, 6, \u0026#39;0\u0026#39;)\r\u0026#34;left00\u0026#34; もちろん、rpad()関数もある。基本的な機能は同じで、右のパディングという点だけが異なる。\n環境 OS: Windows julia: v1.6.0 https://docs.julialang.org/en/v1/base/strings/#Base.lpad\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2124,"permalink":"https://freshrimpsushi.github.io/jp/posts/2124/","tags":null,"title":"ジュリアでzfill()を使う方法"},{"categories":"기하학","contents":"ビルドアップ1 $$ \\left\\{ T(s), N(s), B(s), \\kappa (s), \\tau (s) \\right\\} $$\n曲線を分析するとき、フレネ・セレ装置を使ったのを思い出してほしい。曲面について学ぶときも、同様のものを考えることになる。$\\boldsymbol{\\alpha}$が単位速度曲線のとき、曲線の曲率は加速度の大きさ$\\kappa = \\left| T^{\\prime} \\right| = \\left| \\boldsymbol{\\alpha}^{\\prime \\prime} \\right|$で定義されたんだ。曲面がどれくらい曲がっているかを知るためには、曲面上の曲線がどれくらい曲がっているかを見ることは自然な考え方だ。\n$\\mathbf{x} : U\\subset \\R^{2} \\to M$として与えられた曲面を考えてみよう。$\\boldsymbol{\\alpha}(s)$を単純曲面$\\mathbf{x}$上の単位速度曲線としよう。それでは$\\boldsymbol{\\alpha}$のためのフレネ・セレ装置を次のように表記しよう。\n$$ \\left\\{ \\mathbf{T}, \\mathbf{N}, \\mathbf{B}, \\kappa, \\tau \\right\\} $$\n点$p \\in M$での単位法線を$\\mathbf{n}$としよう。点$p$で$M$に垂直なすべてのベクトルの集合を$N_{p}M$としよう。\n$$ N_{p}M := \\left\\{ r \\mathbf{n} : r \\in \\R \\right\\} = \\left\\{ \\text{all vectors perpendicular to } M \\text{ at } p \\right\\} $$\nそのため、接平面の定義により、$T_{p}M$は$N_{p}M$の直交補空間だ。\n$$ N_{p}M ^{\\perp} = T_{p}M $$\nしたがって、$\\R^{3}$は次のように直交分解され、$\\boldsymbol{\\alpha}^{\\prime \\prime}$は2つの空間のベクトルの線形結合で表すことができる。\n$$ \\R^{3} = N_{p}M \\oplus T_{p}M \\quad \\text{and} \\quad \\boldsymbol{\\alpha}^{\\prime \\prime}(s) = n_{1}\\mathbf{n}(s) + n_{2}\\mathbf{n}^{\\perp}(s)\\quad (\\mathbf{n}\\in N_{p}M,\\ \\mathbf{n}^{\\perp}\\in T_{p}M) $$\n$\\mathbf{T} = \\boldsymbol{\\alpha}^{\\prime}$を接ベクトルとしよう。$\\boldsymbol{\\alpha}$は単位速度ベクトルなので、次の式が成り立つ。\n$$ \\left| \\boldsymbol{\\alpha}^{\\prime}(s) \\right|^{2} = \\left| \\mathbf{T}(s) \\right|^{2} = \\left\\langle \\mathbf{T}, \\mathbf{T} \\right\\rangle = 1 $$\n両辺を微分すると、内積の微分法により次を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\left\\langle \\mathbf{T}, \\mathbf{T} \\right\\rangle^{\\prime} =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle \\mathbf{T}^{\\prime}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle \\boldsymbol{\\alpha}^{\\prime \\prime}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\end{align*} $$\nしたがって、$\\boldsymbol{\\alpha}^{\\prime \\prime}$は$\\mathbf{T}$と垂直だ。$\\boldsymbol{\\alpha}^{\\prime \\prime}$を分けて書いてみると、$\\mathbf{n}$と$\\mathbf{T}$は互いに垂直なので、次を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\left\\langle \\boldsymbol{\\alpha}^{\\prime \\prime}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle n_{1}\\mathbf{n} + n_{2}\\mathbf{n}^{\\perp}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle n_{1}\\mathbf{n}, \\mathbf{T} \\right\\rangle + \\left\\langle n_{2}\\mathbf{n}^{\\perp}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle n_{2}\\mathbf{n}^{\\perp}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\end{align*} $$\nしたがって、$\\mathbf{n}^{\\perp}$は$\\mathbf{n}$と$\\mathbf{T}$の両方と垂直なベクトルであることがわかる。そこで、ベクトル$\\mathbf{S}$を次のように定義しよう。\n$$ \\mathbf{S} := \\mathbf{n}\\times \\mathbf{T} \\quad \\text{and} \\quad \\boldsymbol{\\alpha}^{\\prime \\prime} = n_{1}\\mathbf{n} + s\\mathbf{S} $$\n$\\mathbf{S}$を$\\boldsymbol{\\alpha}$の内在的法線と呼ぶ。\n定義 $\\mathbf{n}$の成分$n_{1}$を単位速度曲線$\\boldsymbol{\\alpha}$の法曲率と呼び、$\\kappa_{n}$と表記する。\n$$ \\kappa_{n} := \\left\\langle \\boldsymbol{\\alpha}^{\\prime \\prime}, \\mathbf{n} \\right\\rangle $$\n$\\mathbf{S}$の成分$s$を単位速度曲線$\\boldsymbol{\\alpha}$の測地曲率と呼び、$\\kappa_{g}$と表記する。\n$$ \\kappa_{g} := \\left\\langle \\boldsymbol{\\alpha}^{\\prime \\prime}, \\mathbf{S} \\right\\rangle $$\nしたがって、次の式が成り立つ。\n$$ \\kappa (s) \\mathbf{N}(s) = \\mathbf{T}^{\\prime}(s) = \\boldsymbol{\\alpha}^{\\prime \\prime}(s) = \\kappa_{n}(s)\\mathbf{n}(s) + \\kappa_{g}(s)\\mathbf{S}(s) $$\n説明 法曲率$\\kappa_{n}$は曲面$M$が$\\R^{3}$でどれくらい曲がっているかを測るのに使われる。測地曲率$\\kappa_{g}$は曲線$\\boldsymbol{\\alpha}$が曲面$M$でどれくらい曲がっているかを測るのに使われる。例えば、測地曲率$\\kappa_{g}$が$0$である曲線は、曲面上の直線、つまり測地線を意味することになる。\n$\\mathbf{n}, \\mathbf{S}$が単位ベクトルであるので、上の定義により次の式が成り立つ。\n$$ \\kappa^{2} = \\kappa_{n}^{2} + \\kappa_{g}^{2} $$\nRichard S. Millman and George D. parker, Elements of Differential Geometry (1977), p102-104\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3154,"permalink":"https://freshrimpsushi.github.io/jp/posts/3154/","tags":null,"title":"ガウス曲率と測地曲率"},{"categories":"줄리아","contents":"コード propertynames() propertynames()関数で確認するといい1。Juliaにはクラスがなく、構造体だけが存在するから2、この関数で返されるすべてのシンボルは、正確にプロパティだけの名前ということになる。\n次はGraphsパッケージでエルデシュ-レニィネットワークを生成し、ノードの数と各ノードのネイバーフッドを確認するコードである。このネットワークにpropertynames()関数を適用し、:neとfadjlistというプロパティがシンボルとして返された。\njulia\u0026gt; using Graphs\rjulia\u0026gt; G_nm = erdos_renyi(50,200)\r{50, 200} undirected simple Int64 graph\rjulia\u0026gt; propertynames(G_nm)\r(:ne, :fadjlist)\rjulia\u0026gt; G_nm.ne\r200\rjulia\u0026gt; G_nm.fadjlist\r50-element Vector{Vector{Int64}}:\r[3, 4, 11, 25, 26, 27, 33, 40, 44, 48]\r[11, 17, 20, 23, 24, 38, 45, 50]\r[1, 4, 15, 24, 29, 30, 34, 42, 46]\r[1, 3, 18, 24, 30, 32, 43, 45]\r[6, 7, 8, 24, 26, 29, 37, 39, 50]\r⋮\r[3, 13, 17, 28, 29, 32, 39, 44, 47, 49]\r[10, 14, 18, 26, 32, 36, 41, 44, 46]\r[1, 21, 23, 24, 25, 32, 41, 44, 45]\r[9, 13, 14, 17, 21, 31, 43, 46, 50]\r[2, 5, 13, 28, 31, 32, 35, 42, 44, 49] fieldnames() 次は少し難しい話だけど、具体的に知らなくてもJuliaのプログラミングには何の問題もない。\npropertynames(x)は根本的にfieldnames(typeof(x))と同じだと言われている3。実際に使う関数としては大きな意味はないが、これを通じて分かる事実は、Juliaでは構造体StructureのインスタンスInstanceをオブジェクトObjectと呼び、構造体そのものが持っている属性AttributeはフィールドField、そしてそのインスタンスとして実際に存在するオブジェクトの属性はプロパティPropertyと呼ばれることである。\n環境 OS: Windows julia: v1.6.0 https://docs.julialang.org/en/v1/base/base/#Base.propertynames\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/a/56352954\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#Base.fieldnames\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2120,"permalink":"https://freshrimpsushi.github.io/jp/posts/2120/","tags":null,"title":"ジュリアで構造体の属性を確認する方法"},{"categories":"기하학","contents":"ビルドアップ リーマン計量は、表面上の曲線の長さを計算する過程から出てくる概念であり、その過程は次の通りです。\n$\\boldsymbol{\\alpha}(t)$を単純曲面$\\mathbf{x} : U \\to \\mathbb{R}^{3}$上を動く正則曲線としよう。$(u_{1}, u_{2})$を$U$の座標としよう。すると、$\\boldsymbol{\\alpha}$は次のように表される。\n$$ \\boldsymbol{\\alpha}(t) = \\mathbf{x}(u_{1}(t), u_{2}(t)) $$\nこの時点で、$a \\le t \\le b$での$\\boldsymbol{\\alpha}$の長さは次のように定義される。\n$$ \\int_{a}^{b} \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| dt $$\n被積分関数を展開すると、次のようになる。\n$$ \\begin{align*} \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| =\u0026amp;\\ \\sqrt{\\left\\langle \\dfrac{d \\boldsymbol{\\alpha}}{d t} , \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right\\rangle} \\\\ =\u0026amp;\\ \\sqrt{\\left\\langle \\dfrac{d \\mathbf{x}(u_{1}, u_{2})}{d t} , \\dfrac{d \\mathbf{x}(u_{1}, u_{2})}{d t} \\right\\rangle} \\end{align*} $$\n連鎖律により、\n$$ \\begin{align*} \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| =\u0026amp;\\ \\sqrt{\\left\\langle \\dfrac{\\partial \\mathbf{x}}{\\partial u_{1}}\\dfrac{d u_{1}}{dt} + \\dfrac{\\partial \\mathbf{x}}{\\partial u_{2}}\\dfrac{d u_{2}}{dt}, \\dfrac{\\partial \\mathbf{x}}{\\partial u_{1}}\\dfrac{d u_{1}}{dt} + \\dfrac{\\partial \\mathbf{x}}{\\partial u_{2}}\\dfrac{d u_{2}}{dt} \\right\\rangle} \\\\ =\u0026amp;\\ \\sqrt{\\left\\langle \\mathbf{x}_{1}\\dfrac{d u_{1}}{dt} + \\mathbf{x}_{2}\\dfrac{d u_{2}}{dt}, \\mathbf{x}_{1}\\dfrac{d u_{1}}{dt} + \\mathbf{x}_{2}\\dfrac{d u_{2}}{dt} \\right\\rangle} \\end{align*} $$\nこの時点で、$\\mathbf{x}_{1} := \\dfrac{\\partial \\mathbf{x}}{\\partial u_{1}}, \\mathbf{x}_{2} := \\dfrac{\\partial \\mathbf{x}}{\\partial u_{2}}$だ。内積を展開して整理すると、\n$$ \\begin{align*} \u0026amp; \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| \\\\ =\u0026amp;\\ \\sqrt{\\left\\langle \\mathbf{x}_{1}\\dfrac{d u_{1}}{dt} + \\mathbf{x}_{2}\\dfrac{d u_{2}}{dt}, \\mathbf{x}_{1}\\dfrac{d u_{1}}{dt} + \\mathbf{x}_{2}\\dfrac{d u_{2}}{dt} \\right\\rangle} \\\\ =\u0026amp;\\ \\sqrt{\\left( \\dfrac{d u_{1}}{dt} \\right)^{2} \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{1} \\right\\rangle + \\dfrac{d u_{1}}{dt}\\dfrac{d u_{2}}{dt} \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\rangle + \\dfrac{d u_{2}}{dt}\\dfrac{d u_{1}}{dt} \\left\\langle \\mathbf{x}_{2}, \\mathbf{x}_{1} \\right\\rangle + \\left( \\dfrac{d u_{2}}{dt} \\right)^{2} \\left\\langle \\mathbf{x}_{2}, \\mathbf{x}_{2} \\right\\rangle} \\end{align*} $$\nここで、上記の内積を$g_{ij} = \\left\\langle \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right\\rangle$と表し、$\\sum$と整理すると、次のようになる。\n$$ \\begin{align*} \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| =\u0026amp;\\ \\sqrt{ \\sum \\limits_{i=1}^{2}\\sum \\limits_{j=1}^{2} g_{ij} \\dfrac{d u_{i}}{dt}\\dfrac{d u_{j}}{dt}} \\\\ =\u0026amp;\\ \\sqrt{ g_{ij} \\dfrac{d u_{i}}{dt}\\dfrac{d u_{j}}{dt}} \\end{align*} $$\n二番目の等号で、アインシュタインの記法を使用して和記号を省略した。\n定義1 $g_{ij} = \\left\\langle \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right\\rangle$は、リーマン計量の係数the coefficient of the Riemannian metricまたは第一基本形式the first fundamental formの係数と呼ばれる。\n$M$を$\\mathbb{R}^{3}$での曲面、$p \\in M$としよう。$\\mathbf{X}, \\mathbf{Y}$を$p$での接ベクトルとしよう。すると、$M$の固有切断写像$\\mathbf{x} : U \\to \\mathbb{R}^{3}$に対して次のように表される。\n$$ \\mathbf{X} = X^{1}\\mathbf{x}_{1} + X^{2}\\mathbf{x}_{2} \\quad \\text{and} \\quad \\mathbf{Y} = Y^{1}\\mathbf{x}_{1} + Y^{2}\\mathbf{x}_{2} $$\n次のような双線形形式$I$を曲面$\\mathbf{x}$のリーマン計量Riemannian metricまたは第一基本形式the first fundamental formと定義する。\n$$ I : T_{p}M \\times T_{p}M \\to \\mathbb{R} $$\n$$ I (\\mathbf{X}, \\mathbf{Y}) = \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} g_{ij}X^{i}Y^{j} = g_{ij}X^{i}Y^{j} = \\begin{bmatrix} X^{1} \u0026amp; X^{2}\\end{bmatrix} \\begin{bmatrix} g_{11} \u0026amp; g_{12} \\\\ g_{21} \u0026amp; g_{22} \\end{bmatrix} \\begin{bmatrix} Y^{1} \\\\ Y^{2}\\end{bmatrix} $$\n係数の行列$\\left[ g_{ij} \\right]$の行列式を$g$と表記する。\n$$ g := \\det (\\left[ g_{ij} \\right]) = \\begin{vmatrix} g_{11} \u0026amp; g_{12} \\\\ g_{21} \u0026amp; g_{22}\\end{vmatrix} = g_{11}g_{22} - g_{12}g_{21} $$\n行列$\\left[ g_{ij} \\right]$の逆行列の$(k,l)$成分を$g^{kl}$と表記する。\n$$ \\begin{align*} \\begin{pmatrix} g_{11} \u0026amp; g_{12} \\\\ g_{21} \u0026amp; g_{22} \\end{pmatrix} ^{-1} =\u0026amp;\\ \\dfrac{1}{\\det \\left[ g_{ij} \\right]} \\begin{pmatrix} g_{22} \u0026amp; - g_{21} \\\\ g_{12} \u0026amp; g_{22} \\end{pmatrix} = \\dfrac{1}{g} \\begin{pmatrix} g_{22} \u0026amp; - g_{21} \\\\ g_{12} \u0026amp; g_{22} \\end{pmatrix} \\\\[1em] =\u0026amp;\\ \\begin{pmatrix}\\dfrac{g_{22}}{g} \u0026amp; - \\dfrac{g_{21}}{g} \\\\[1em] -\\dfrac{g_{12}}{g} \u0026amp; \\dfrac{g_{11}}{g} \\end{pmatrix} \\\\[1em] =\u0026amp;\\ \\begin{pmatrix} g^{11} \u0026amp; g^{12} \\\\[1em] g^{21} \u0026amp; g^{22} \\end{pmatrix} \\end{align*} $$\n説明 最近は第一基本形式という言葉はほとんど使われず、リーマン計量という言葉だけが主に使われるという。計量という名前がついたのは、ビルドアップで見たように、表面上の曲線の長さを測るために使うからである。\n$E = g_{11}$、$F=g_{21}=g_{12}$、$G=g_{22}$のような表記も多く使われる。\n曲線理論では登場しなかったリーマン計量という概念が出てくる理由は、接空間の基底$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$が一般的に正規直交基底ではないからである。正規直交基底ならば$g_{ij} = \\delta_{ij}$であり、意味がない。ここで、$\\delta$はクロネッカーデルタである。リーマン計量とアインシュタインの記法を使用して、表面上の曲線$\\boldsymbol{\\alpha}$の長さを表すと、次のようになる。\n$$ \\begin{align*} L (\\boldsymbol{\\alpha}) =\u0026amp;\\ \\text{length of } \\boldsymbol{\\alpha} \\\\ =\u0026amp;\\ \\int_{a}^{b} \\sqrt{ g_{ij} \\dfrac{d u_{i}}{dt}\\dfrac{d u_{j}}{dt}} dt \\\\ =\u0026amp;\\ \\int_{a}^{b} \\sqrt{ g_{ij} \\alpha_{i}^{\\prime} \\alpha_{j}^{\\prime} } dt \\\\ =\u0026amp;\\ \\int_{a}^{b} \\sqrt{ E\\left( \\dfrac{d u_{1}}{dt} \\right)^{2} + 2F\\dfrac{d u_{1}}{dt}\\dfrac{d u_{2}}{dt} + G\\left( \\dfrac{d u_{2}}{dt} \\right)^{2}} dt \\end{align*} $$\n表面の面積も、リーマン計量の積分によって定義される。\n単純曲面$\\mathbf{x}$上のある領域$R$について、$Q = \\mathbf{x}^{-1}(R)$とする。つまり、$Q \\subset U \\subset \\R^{2}$である。すると、$R$の面積は次のようになる。 $$ \\text{area of } R = \\iint _{Q} \\sqrt{g} du_{1}du_{2} = \\iint _{Q} \\left| \\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\right| du_{1}du_{2} = \\iint _{Q} \\sqrt{EG-F^{2}} du_{1}du_{2} $$\n性質 単純曲面$\\mathbf{x} : U \\to \\mathbb{R}^{3}$に対して、\n(a) $g = \\left| \\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\right|^{2}$\n(b) $g^{11} = \\dfrac{g_{22}}{g} \\quad \\text{and} \\quad g^{12} = g^{21} = -\\dfrac{g_{12}}{g} \\quad \\text{and} \\quad g^{22} = \\dfrac{g_{11}}{g}$\n(c) $\\forall i,j$、$\\sum \\limits_{k=1}^{2} g_{ik}g^{kj} = {\\delta_{i}}^{j}$\nここで、$\\delta$はクロネッカーデルタである。\n証明 (a) 外積の性質とリーマン計量の定義により、次が成立する。\n$$ \\begin{align*} \\left| \\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\right|^{2} =\u0026amp;\\ \\left| \\mathbf{x}_{1} \\right|^{2} \\left| \\mathbf{x}_{2} \\right|^{2} \\sin ^{2} \\theta \\\\ =\u0026amp;\\ \\left| \\mathbf{x}_{1} \\right|^{2} \\left| \\mathbf{x}_{2} \\right|^{2}\\left(1- \\cos ^{2} \\theta \\right) \\\\ =\u0026amp;\\ \\left| \\mathbf{x}_{1} \\right|^{2} \\left| \\mathbf{x}_{2} \\right|^{2}\\left(1- \\dfrac{\\mathbf{x}_{1} \\cdot \\mathbf{x}_{2}}{\\left| \\mathbf{x}_{1} \\right| \\left| \\mathbf{x}_{2} \\right| } \\right) \\\\ =\u0026amp;\\ \\left| \\mathbf{x}_{1} \\right|^{2} \\left| \\mathbf{x}_{2} \\right|^{2} - \\left( \\mathbf{x}_{1} \\cdot \\mathbf{x}_{2} \\right)^{2} \\\\ =\u0026amp;\\ g_{11}g_{22} - g_{12}g_{21} \\\\ =\u0026amp;\\ \\det( [g_{ij}] ) \\\\ =\u0026amp;\\ g \\end{align*} $$\n■\n(b) 定義どおりだ。\n■\n(c) $[g^{kl}]$が$[g_{ij}]$の逆行列であるので、自然に成立する。\n$$ \\begin{align*} \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} =\u0026amp;\\ \\begin{pmatrix} g_{11} \u0026amp; g_{12} \\\\ g_{21} \u0026amp; g_{22} \\end{pmatrix} \\begin{pmatrix} g^{11} \u0026amp; g^{12} \\\\ g^{21} \u0026amp; g^{22} \\end{pmatrix} \\\\[1em] =\u0026amp;\\ \\begin{pmatrix} g_{11}g^{11}+g_{12}g^{21} \u0026amp; g_{11}g^{12} + g_{12}g^{22} \\\\[1em] g_{21}g^{11} + g_{22}g^{21} \u0026amp; g_{21}g^{12} + g_{22}g^{22} \\end{pmatrix} \\end{align*} $$\n■\n関連項目 第二基本形式 クリストッフェル記号 リーマン計量 Richard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p93-96\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3148,"permalink":"https://freshrimpsushi.github.io/jp/posts/3148/","tags":null,"title":"第1 基本形式、リーマン計量"},{"categories":"기하학","contents":"定義1 $M \\subset \\R^{3}$という$P \\in M$の全ての点に対して、イメージ$\\mathbf{x}(U)$が$P$のある$\\epsilon-$近傍$N_{p}$を含むようにする$C^{k}$ 微分同相写像$\\mathbf{x} : U \\subset \\R^{2} \\to M$が存在する場合、$M$を $\\R^{3}$の$C^{k}$ 曲面surfaceと呼ぶ。\nさらに、そのような二つの微分同相写像 $\\mathbf{x} : U \\to \\R^{3}$と$\\mathbf{y} : V \\to \\R^{3}$に対して、\n$$ \\mathbf{y}^{-1} \\circ \\mathbf{x} : \\mathbf{x}^{-1}\\left( \\mathbf{x}(U) \\cap \\mathbf{y}(V) \\right) \\to \\mathbf{y}^{-1}\\left( \\mathbf{x}(U) \\cap \\mathbf{y}(V) \\right) $$\nは$C^{k}$ 座標変換である。\n説明 $\\R^{3}$の曲面とは、端的に言えば、単純曲面のイメージをうまく合わせたものだ。\n多くの定義がそうであるように、曲面かどうかを定義だけで判断することは簡単ではない。曲面を判定するにあたって、以下のような定理がある。\n定理2 微分可能な関数$g : \\R^{3} \\to \\R$と定数$c \\in \\R$が与えられたとする。集合$M = \\left\\{ (x,y,z) : g(x,y,z) = c \\right\\}$に対して、$M$のある点で\n$$ dg = \\dfrac{\\partial g}{\\partial x}dx + \\dfrac{\\partial g}{\\partial y}dy + \\dfrac{\\partial g}{\\partial z}dz \\ne 0 $$\nが成り立つなら、$M$は曲面である。\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p89\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBarrett O\u0026rsquo;Neill, Elementary Differential Geometry (Revised 2nd Edition, 2006), p133-134\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3146,"permalink":"https://freshrimpsushi.github.io/jp/posts/3146/","tags":null,"title":"微分幾何学における曲面の定義"},{"categories":"논문작성","contents":"アルファ $\\Alpha, \\alpha$ アルファalphaと読む。TeXコードはそれぞれ\\Alpha、\\alpha\nギリシャ文字の最初の文字で、「アルファでありオメガ」は「始まりであり終わり」という意味である。\nインデックス集合のインデックス $\\alpha$ 微分幾何学での曲線 $\\alpha$ 微分多様体上の接ベクトルを定義するときに使われる曲線 $\\alpha$ ベータ $\\Beta, \\beta$ ベータbetaと読む。TeXコードはそれぞれ\\Beta、\\beta\nベータ関数 $B$ ガンマ $\\Gamma, \\gamma$ ガンマgammaと読む。TeXコードはそれぞれ\\Gamma、\\gamma\nガンマ関数 $\\Gamma$ 微分幾何学での曲線 $\\gamma$ クリストッフェル記号 $\\Gamma_{ij}^{k}$ デルタ $\\Delta, \\delta$ デルタdeltaと読む。TeXコードはそれぞれ\\Delta、\\delta\n微積分学で$x$の非常に小さな変化量 $\\Delta x$ 物理学でのラプラシアン $\\Delta$ 偏微分方程式でのラプラシアン $\\Delta$ 数学で非常に小さい正数 $\\delta$ : $\\epsilon - \\delta$ 論法 ディラックのデルタ関数 $\\delta$ クロネッカーのデルタ $\\delta_{ij}$ イプシロン $\\Epsilon, \\epsilon, \\varepsilon$ イプシロンepsilonと読む。TeXコードはそれぞれ\\Epsilon、\\epsilon、\\varepsilon\nイプシロンが正しい発音だが、エプシロンと読まれることが多い。\n数学で非常に小さい正数 $\\epsilon$ : $\\epsilon - \\delta$ 論法 電磁気学での誘電率 $\\epsilon$ レビ-チビタ記号 $\\epsilon_{ijk}$ ゼータ $\\Zeta, \\zeta$ ゼータzetaと読む。TeXコードはそれぞれ\\Zeta、\\zeta\nあまり使われない。変数が足りないときによく使われる。\nリーマンゼータ関数 $\\zeta$ エータ $\\Eta, \\eta$ エータetaと読む。TeXコードはそれぞれ\\Eta、\\eta\nゼータと同様に、適切な変数がないときに使われることがある。\n粒子物理学でバリオン $\\eta$ シータ $\\Theta, \\theta, \\vartheta$ シータthetaと読む。TeXコードはそれぞれ\\Theta、\\theta、\\vartheta\nほとんどの場合、角度を意味すると見なせる。\n角度 $\\theta$ 変数分離時の関数 $\\theta$に対する変数 $\\Theta (\\theta)$ イオタ $\\Iota, \\iota$ イオタiotaと読む。TeXコードはそれぞれ\\Iota、\\iota\nあまり使われない。\nカッパ $\\Kappa, \\kappa$ カッパkappaと読む。TeXコードはそれぞれ\\Kappa、\\kappa\n微分幾何学での曲率 $\\kappa$ :\n法曲率 $\\kappa _{n}$、測地曲率 $\\kappa_{g}$、[主曲率 $\\kappa$] 量子力学で負のエネルギーの置換定数 $\\kappa$ ラムダ $\\Lambda, \\lambda$ ラムダlambdaと読む。TeXコードはそれぞれ\\Lambda、\\lambda\n固有値 $\\lambda$ 物理学での波長 $\\lambda$ ミュー $\\Mu, \\mu$ ミューmuと読む。TeXコードはそれぞれ\\Mu、\\mu\n測度 $\\mu$ 電磁気学での透磁率 $\\mu$ 粒子物理学でミュオン $\\mu$ 統計学での平均 $\\mu$ ニュー $\\Nu, \\nu$ ニューnuと読む。TeXコードはそれぞれ\\Nu、\\nu\n物理学での振動数 $\\nu$ 粒子物理学でニュートリノ $\\nu$ クシー $\\Xi, \\xi$ クシーxiと読む。クサイ、ザイとも読む。ザイ アパートのザイがこれだ。TeXコードはそれぞれ\\Xi、\\xi\n変数が足りないときによく使われる。\n$x$に対するフーリエ変換の変数 $\\xi$ リーマンザイ関数 $\\xi$この場合はザイと読む。 オミクロン $\\Omicron, \\omicron$ オミクロンOmicronと読むが、アルファベット$o$と形がほとんど同じで、ほとんど使われない。TeXコードはそれぞれ\\Omicron、\\omicron\nパイ $\\Pi, \\pi$ パイpiと読む。円周率を意味する。TeXコードはそれぞれ\\Pi、\\pi\n積記号 $\\Pi$ 円周率 $\\pi$ 粒子物理学でパイオン中間子 $\\pi$ ロー $\\Rho, \\rho$ ローrhoと読む。TeXコードはそれぞれ\\Rho、\\rho\n円柱座標系の半径変数 $\\rho$ 物理学で密度 $\\rho$\n体積電荷密度 $\\rho$ シグマ $\\Sigma, \\sigma$ シグマsigmaと読む。TeXコードはそれぞれ\\Sigma、\\sigma\n和記号 $\\Sigma$ 統計学での分散 $\\sigma^{2}$ 電磁気学での表面電荷密度 $\\sigma$ 熱力学で[衝突断面積 $\\sigma$] タウ $\\Tau, \\tau$ 力学でのトルク $\\tau$ : $N$としてもよく使われる。 時間に対する変数として$t$の代わりに使われることがある。 円周率の2倍の数 $\\tau = 2\\pi$ ウプシロン $\\Upsilon, \\upsilon, \\varUpsilon$ ウプシロンupsilonと読む。TeXコードはそれぞれ\\Upsilon、\\upsilon、\\varUpsilon\n粒子物理学でウプシロン中間子 $\\varUpsilon$ カイ $\\Chi, \\chi$ カイchiと読む。先生が$x$を$\\chi$として使わないようにと言うのは、これが実際にはエックス$x$ではなくカイ$\\chi$だからである。是非ともこのように書かないでほしい。TeXコードはそれぞれ\\Chi、\\chi\n特性関数 $\\chi$ プサイ $\\Psi, \\psi$ プサイpsiと読む。TeXコードは\\Psi、\\psi\n$\\phi$とともに、任意の関数を表す際によく使われる。\n量子力学での波動関数 $\\psi$ ファイ $\\Phi, \\phi ,\\varphi$ ファイphiまたはパイと読む。経験上、物理学ではパイ、数学ではファイと読むことが多い。TeXコードはそれぞれ\\Phi、\\phi、\\varphi\n$\\psi$とともに、任意の関数を表す際によく使われる。\n円柱座標系の変数 $\\phi$ 球座標系の変数 $\\phi$ 量子力学での波動関数 $\\phi$ オメガ $\\Omega, \\omega$ オメガomegaと読む。TeXコードはそれぞれ\\Omega、\\omega\nギリシャ文字の最後の文字で、「アルファでありオメガ」という言葉は「始まりであり終わり」、「全て」という意味である。\n偏微分方程式、関数解析学での開集合 $\\Omega$ : $U$とともによく使われる記法である。 物理学で抵抗の単位 $\\Omega$ 球の立体角 $\\Omega$ 物理学での角振動数 $\\omega$ $t$に対するフーリエ変換の変数 $\\omega$ 多項式の複素数根 $\\omega$ ","id":3145,"permalink":"https://freshrimpsushi.github.io/jp/posts/3145/","tags":null,"title":"ギリシャ文字の読み方・書き方と数学・科学における意味"},{"categories":"기하학","contents":"定義1 座標パッチ $\\mathbf{x} : U \\to \\mathbb{R}^{3}$ 上の点 $p = \\mathbf{x}(a,b)$ を考えよう。ベクトル $\\mathbf{X}$ が $p$ を通るある曲線 $\\mathbf{x}(U)$ 上の$p$での速度ベクトルならば、$\\mathbf{X}$ を単純な曲面 $\\mathbf{x}$ に対する接ベクトルtangent vectorと定義する。\nつまり、任意の$\\epsilon \u0026gt; 0$に対して、適当に短い曲線 $\\boldsymbol{\\alpha} : (-\\epsilon, \\epsilon) \\to \\mathbf{x}(U) \\subset \\mathbb{R}^{3}$ が存在して、次の条件\n$$ \\boldsymbol{\\alpha}(0) = p \\quad \\text{and} \\quad \\boldsymbol{\\alpha}^{\\prime}(0) = \\left. \\dfrac{d \\boldsymbol{\\alpha}}{d t}\\right|_{t=0}= \\mathbf{X} \\quad \\text{and} \\quad \\boldsymbol{\\alpha} (t) = \\mathbf{x}\\left( \\alpha_{1}(t), \\alpha_{2}(t) \\right) $$\nを満たすならば、$\\mathbf{X}$ を単純な曲面 $\\mathbf{x}$ に対する接ベクトルtangent vectorという。\n説明 上述のように定義された接ベクトルの集合は、下記の定理によりベクタースペースになり、これは実際には接平面と同じである。したがって、接平面は接空間tangent spaceと呼ばれる。\n曲面 $M$ 上の点 $p \\in M$ での $M$ に対する全ての接ベクトルの集合を $T_{p}M$ と記し、接空間tangent spaceと呼ぶ。 $$ T_{p}M = \\left\\{ \\text{all vectors tangent to } M \\text{ at } p \\right\\} $$\nこの定義の方法は微分多様体上の接ベクトルを定義するときにもそのまま使用される。最初この定義を見たとき、そんな曲線 $\\boldsymbol{\\alpha}$ を考えながら定義する理由がすぐには理解しづらいかもしれないが、微分幾何を続けて学んだり、多様体への一般化に接したりすると、自然と受け入れられるようになるだろう。\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p83\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3142,"permalink":"https://freshrimpsushi.github.io/jp/posts/3142/","tags":null,"title":"単純な曲面上の接ベクトル"},{"categories":"기하학","contents":"定義1 $2$次元のユークリッド空間の部分集合$U \\subset \\mathbb{R}^{2}$が座標$u_{1}$、$u_{2}$を持っているとしよう、$\\mathbf{x}_{1}$、$\\mathbf{x}_{2}$をシンプルな曲面$\\mathbf{x} : U \\to \\mathbb{R}^{3}$での方向偏微分としよう。\n$$ \\begin{align*} \\mathbf{x}_{1} := {{ \\partial \\mathbf{x} } \\over { \\partial u_{1} }} \u0026amp; , \u0026amp; \\mathbf{x}_{2} := {{ \\partial \\mathbf{x} } \\over { \\partial u_{2} }} \\end{align*} $$\n点$p = \\mathbf{x} (a,b)$での$\\mathbf{x}_{1} \\times \\mathbf{x}_{2}$と垂直な平面を$p$での接平面Tangent Planeという。 次のように定義された$\\mathbf{n}$を$p$の単位法線Unit Normalという。 $$ \\mathbf{n}(a,b) := {{ \\mathbf{x}_{1} \\times \\mathbf{x}_{2} } \\over { \\left| \\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\right| }} $$ 説明 曲線を語る時に接線を考えたのと同じように、曲面での接平面を考えることは非常に自然なことだ。$p$での接平面は、$p$の周りで曲面を最もよく近似する平面だ。\nシンプルな曲面の定義から$\\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\ne 0$であるため、法線$\\mathbf{n}$の存在は常に保証されている。\n次の定理から、接平面は接ベクトルの集合と同じであり、ベクトル空間になることが分かる。このため、接平面は接空間tangent spaceと呼ばれる。曲面$M$の点$p$上の接空間を$T_{p}M$と表示する。\n定理2 シンプルな曲面$\\mathbf{x} : U \\to \\mathbb{R}^{3}$の点$p = \\mathbf{x}(a,b)$での全ての接ベクトルの集合は、基底が$\\left\\{ \\mathbf{x}_{1}(a,b), \\mathbf{x}_{2}(a,b) \\right\\}$である$2$次元のベクトル空間である。また$p$での接平面は、$\\mathbb{R}^{3}$の何らかの原点を通る直線と平行である。\n証明 点$p$での接ベクトル$\\mathbf{x}_{1}, \\mathbf{x}_{2}$は線形独立である。（$\\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\ne \\mathbf{0}$であるので）$p$での全ての接ベクトルの集合はベクトル空間であるため、これは少なくとも$2$次元以上のベクトル空間である。このベクトル空間が$2$次元であることを示すためには、$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$がこれを生成することを示せば良い。\n$\\mathbf{X}$を点$p$での接ベクトルとしよう。そして$\\boldsymbol{\\gamma}$を、$\\boldsymbol{\\gamma}(0) = p, \\dot{\\boldsymbol{\\gamma}}(0) = \\mathbf{X}$である$\\mathbf{x}(U)$上の曲線とする。そして、$\\boldsymbol{\\gamma}(t)$を次のように表現しよう。\n$$ \\boldsymbol{\\gamma}(t) = \\mathbf{x}\\left( \\gamma^{1}(t), \\gamma^{2}(t) \\right) $$\nそれから、連鎖規則によって、\n$$ \\dfrac{d \\boldsymbol{\\gamma}}{d t} = \\dfrac{\\partial \\mathbf{x}}{\\partial u^{1}}\\dfrac{d \\gamma^{1}}{d t} + \\dfrac{\\partial \\mathbf{x}}{\\partial u^{2}}\\dfrac{d \\gamma^{2}}{d t} = \\sum_{i}\\dfrac{d \\gamma^{i}}{d t}\\mathbf{x}_{i} $$\n$$ \\implies \\mathbf{X} = \\dfrac{d \\boldsymbol{\\gamma}}{d t}(0) = \\sum_{i}\\dfrac{d \\gamma^{i}}{d t}(0)\\mathbf{x}_{i}(a,b) $$\n任意の接ベクトル$\\mathbf{X}$が$\\left\\{ \\mathbf{x}_{i} \\right\\}$たちの線形結合で示されるので、$\\left\\{ \\mathbf{x}_{i} \\right\\}$は$p$での全ての接ベクトルの集合を生成する。したがって、$p=\\mathbf{x}(a,b)$での全ての接ベクトルの集合は、基底が$\\left\\{ \\mathbf{x}_{1}(a,b), \\mathbf{x}_{2}(a,b) \\right\\}$である$2$次元のベクトル空間である。\nRichard S. Millman and George D. Parker、Elements of Differential Geometry (1977)、p81\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRichard S. Millman and George D. Parker、Elements of Differential Geometry (1977)、p84-85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2110,"permalink":"https://freshrimpsushi.github.io/jp/posts/2110/","tags":null,"title":"平面と法線ベクトルの交点"},{"categories":"기하학","contents":"定義1 1 座標 $u_{1}$、$u_{2}$ を持つ $2$次元 ユークリッド空間の部分集合 $U \\subset \\mathbb{R}^{2}$が 開集合だとしよう。すべての $p \\in U$に対して以下を満たす $C^{k}$ 単射関数 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$ が存在するなら、それを単純曲面Simple Surfaceと呼ぶ。\n$$ {{ \\partial \\mathbf{x} } \\over { \\partial u_{1} }} (p) \\times {{ \\partial \\mathbf{x} } \\over { \\partial u_{2} }} (p) \\ne \\mathbf{0} $$\n説明 定義において、開集合 $U$ は $2$次元空間から選ばれ、それが平らであれ曲がっていれ、重なる部分なしに(単射であるため)$3$次元空間へマッピングされる。この意味で、単純曲面は $2$次元の平らな片を$3$次元空間で滑らかに接続することと想像できる。この関数としての曲面の定義を幾何学的に理解するのが最善だが、すぐには思い浮かばなくても、時間をかけて慣れること。\n曲面をこのように2次元空間から3次元空間への写像として定義する理由は、曲面は局所的に見た時、平面のように扱えるためである。実際地球は球体に近い形だが、私たちは地表を上から見た時、2次元平面のように感じる。$U$を世界地図、$\\mathbf{x}(U)$を地球儀と例えることができる。\n一方、定義における数式で与えられた条件は、正則曲線が $\\displaystyle {{ d \\mathbf{x} } \\over { d u }} (p) \\ne 0$ のような条件を満たさなければならなかった曲線理論と似ている。直感的には、とがったり、奇妙にねじれた部分はすぐに排除するということである。$\\dfrac{ \\partial \\mathbf{x} }{ \\partial u_{1} } (p) \\times \\dfrac{\\partial \\mathbf{x} }{ \\partial u_{2} } (p) \\ne \\mathbf{0}$ を満たすとは、あらゆる方向の偏微分がシンギュラーでない($0$ではない)ことを意味し、ある意味で、二つの線形独立した（曲線）軸を見て、その幾何を考える意図を読み取ることができる。\nもし単純曲面が具体的な座標とグラフで表されていれば、それはモンジュ・パッチMonge Patchとも呼ばれる。例えば、単純曲面$f$が$f(x,y) = x^{2} + y^{2}$であれば、そのグラフは $$ \\left\\{ \\left( x, y , x^{2} + y^{2} \\right) : (x,y) \\in \\mathbb{R}^{2} \\right\\} $$ であり、モンジュ・パッチと呼ぶことができる。\n定義2 2 座標 $u_{1}$、$u_{2}$を持つ $2$次元 ユークリッド空間の部分集合 $U \\subset \\mathbb{R}^{2}$が 開集合だとしよう。写像 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$が一対一で正則なら、$\\mathbf{x}$を座標片coordinate patchと言う。\n説明 3 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$が正則であるとは、$\\mathbf{x}$のヤコビ行列のランクが$2$と同じということである。$\\mathbf{x}(u,v) = (x_{1}(u,v), x_{2}(u,v), x_{3}(u,v))$とすると、$\\mathbf{x}$のヤコビ行列は次のようになる。\n$$ J = \\begin{bmatrix} \\dfrac{\\partial x_{1}}{\\partial u} \u0026amp; \\dfrac{\\partial x_{1}}{\\partial v} \\\\[1em] \\dfrac{\\partial x_{2}}{\\partial u} \u0026amp; \\dfrac{\\partial x_{2}}{\\partial v} \\\\[1em] \\dfrac{\\partial x_{3}}{\\partial u} \u0026amp; \\dfrac{\\partial x_{3}}{\\partial v} \\end{bmatrix} $$\nこの行列のランクが$2$であるとは、列空間の次元が$2$であるということであり、$\\mathbf{x}_{u} = \\left( \\dfrac{\\partial x_{1}}{\\partial u}, \\dfrac{\\partial x_{2}}{\\partial u}, \\dfrac{\\partial x_{3}}{\\partial u} \\right)$と$\\mathbf{x}_{v} = \\left( \\dfrac{\\partial x_{1}}{\\partial v}, \\dfrac{\\partial x_{2}}{\\partial v}, \\dfrac{\\partial x_{3}}{\\partial v} \\right)$が線形独立であることを意味する。したがって、この二つの外積は$\\mathbf{0}$ではない。\n$$ \\mathbf{x}_{u} \\times \\mathbf{x}_{v} \\ne \\mathbf{0} $$\nそうすると、上記の二つの定義は同値であることがわかる。\nMillman. (1977). Elements of Differential Geometry: p77.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBarrett O\u0026rsquo;Neill, Elementary Differential Geometry (Revised 2nd Edition, 2006), p130-131\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBarrett O\u0026rsquo;Neill, Elementary Differential Geometry (Revised 2nd Edition, 2006), p142\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2106,"permalink":"https://freshrimpsushi.github.io/jp/posts/2106/","tags":null,"title":"単純曲面、座標写像"},{"categories":"줄리아","contents":"概要1 名前はCalculus.jlだけど、積分はサポートしない。\n機械学習などで話される自動微分が必要ならZygote.jlパッケージを参照してほしい。\n一変数関数の微分 導関数 derivative() $f : \\R \\to \\R$の導関数を求めてくれる。\nderivative(f)またはderivative(f, :x): 導関数$f^{\\prime}$を返す。 derivative(f, a): 微分係数$f^{\\prime}(a)$を返す。 julia\u0026gt; f(x) = 1 + 2x + 3x^2\rf (generic function with 1 method)\rjulia\u0026gt; g(x) = sin(x)\rg (generic function with 1 method)\rjulia\u0026gt; derivative(f)\r#1 (generic function with 1 method)\rjulia\u0026gt; derivative(f, 1)\r7.99999999996842\rjulia\u0026gt; Df = derivative(f)\r#1 (generic function with 1 method)\rjulia\u0026gt; Dg = derivative(g)\r#1 (generic function with 1 method)\r#f\u0026#39;(x) = 2 + 6x\rjulia\u0026gt; Df(1)\r7.99999999996842\r#g\u0026#39;(x) = cos x\rjulia\u0026gt; Dg(pi)\r-0.9999999999441258 合成関数も微分することができる。\n#f∘g(x) = (2 + 6 sin x)cos x\rjulia\u0026gt; derivative(f∘g)\r#1 (generic function with 1 method)\rjulia\u0026gt; derivative(f∘g, pi/4)\r4.414213562300037\rjulia\u0026gt; (2+6sin(pi/4))cos(pi/4)\r4.414213562373095 二階導関数 second_derivative() $f : \\R \\to \\R$の二階導関数を求めてくれる。\nderivative()で返される関数は整数を入力値に使えるが、second_derivative()は整数型を使えない。無理数型も使えない。\njulia\u0026gt; derivative(f, 1)\r7.99999999996842\rjulia\u0026gt; second_derivative(f, 1)\rERROR: MethodError: no method matching eps(::Type{Int64})\rClosest candidates are:\reps() at float.jl:764\reps(::AbstractFloat) at float.jl:760\reps(::Type{Float16}) at float.jl:761\rjulia\u0026gt; second_derivative(f, 1.)\r5.9999956492003905\rjulia\u0026gt; second_derivative(g, pi)\rERROR: MethodError: no method matching eps(::Type{Irrational{:π}})\rClosest candidates are:\reps() at float.jl:764\reps(::AbstractFloat) at float.jl:760\reps(::Type{Float16}) at float.jl:761\rjulia\u0026gt; second_derivative(g, convert(Float64, pi))\r-1.3553766145945872e-7\rjulia\u0026gt; second_derivative(g, 1pi)\r-1.3553766145945872e-7 多変数関数の微分 グラディエント gradient() $f : \\mathbb{R}^{n} \\to \\mathbb{R}$のグラディエントを返す。\n注意するべき点は、多変数関数を定義するとき、実際に変数が複数ある関数として定義してはいけないということだ。ベクトルを入力として受け取る一変数関数として定義しなければならない。ベクトルを入力として受け取る関数でなければ、微分はできても、値を計算することができないということだ。例えば、$f_{1}$のように定義してはいけないが、$f_{2}$のように定義する必要があるということだ。\njulia\u0026gt; f₁(x,y,z) = x*y + z^2\rf₁ (generic function with 1 method)\rjulia\u0026gt; Calculus.gradient(f₁)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₁ = Calculus.gradient(f₁)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₁(1,1,1)\rERROR: MethodError: no method matching (::Calculus.var\u0026#34;#2#4\u0026#34;{typeof(f₁), Symbol})(::Int64, ::Int64, ::Int64)\rjulia\u0026gt; ∇f₁([1,1,1])\rERROR: MethodError: no method matching f₁(::Vector{Float64})\rjulia\u0026gt; Calculus.gradient(f₁, 1,1,1)\rERROR: MethodError: no method matching gradient(::typeof(f₁), ::Int64, ::Int64, ::Int64)\rjulia\u0026gt; Calculus.gradient(f₁, [1,1,1])\rERROR: MethodError: no method matching f₁(::Vector{Float64})\rjulia\u0026gt; f₂(x) = x[1]*x[2] + x[3]^2\rf₂ (generic function with 1 method)\rjulia\u0026gt; Calculus.gradient(f₂, [1,1,1])\r3-element Vector{Float64}:\r1.0000000000235538\r1.0000000000235538\r1.9999999999737708\rjulia\u0026gt; ∇f₂ = Calculus.gradient(f₂)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₂(1,1,1)\rERROR: MethodError: no method matching (::Calculus.var\u0026#34;#2#4\u0026#34;{typeof(f₂), Symbol})(::Int64, ::Int64, ::Int64)\rjulia\u0026gt; ∇f₂([1,1,1])\r3-element Vector{Float64}:\r1.0000000000235538\r1.0000000000235538\r1.9999999999737708 ヘッシアン hessian() $f : \\mathbb{R}^{n} \\to \\mathbb{R}$のヘッシアンを返す。\nsecond_derivative()と同様に、Floatデータ型のみ入力を受け付ける。gradient()と同様に、ベクトルを入力として受け取る関数に対してのみ値を返すことができる。\njulia\u0026gt; hessian(f₂, [1.,1.,1.])\r3×3 Matrix{Float64}:\r3.88008e-7 1.0 0.0\r1.0 3.88008e-7 0.0\r0.0 0.0 2.0\rjulia\u0026gt; H = hessian(f₂)\r#7 (generic function with 1 method)\rjulia\u0026gt; H([1.,1.,1.])\r3×3 Matrix{Float64}:\r3.88008e-7 1.0 0.0\r1.0 3.88008e-7 0.0\r0.0 0.0 2.0 ヤコビアン jacobian() $f : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$のヤコビアンを返す。\nsecond_derivative()と同様に、Floatデータ型のみ入力を受け付ける。gradient()と同様に、ベクトルを入力として受け取る関数に対してのみ値を返すことができる。\n他の関数とは異なり、jacobian(f, [x, y, z])のように使うことはできない。\njulia\u0026gt; h(x) = [x[1], x[1]*x[2], x[1]*x[3]^2]\rh (generic function with 2 methods)\rjulia\u0026gt; jacobian(h, [1.,1.,1.])\rERROR: MethodError: no method matching jacobian(::typeof(h), ::Vector{Float64})\rjulia\u0026gt; Jh = jacobian(h)\r(::Calculus.var\u0026#34;#g#5\u0026#34;{typeof(h), Symbol}) (generic function with 1 method)\rjulia\u0026gt; Jh([1.,1.,1.])\r3×3 Matrix{Float64}:\r1.0 0.0 0.0\r1.0 1.0 0.0\r1.0 0.0 2.0 記号微分 記号微分はSymEngine.jlパッケージでも使用できる。\ndifferentiate() 記号微分を実行する。\n定数項と$x$はキレイに返すが、$ax$や$x^{n}$のような場合は、積の微分法の形で返す。例えば$3x^{2}$を微分すると、それを$3$と$x^{2}$の積と見て$\\dfrac{d 3}{dx} x^{2} + 3\\dfrac{d x^{2}}{dx}$のように返すということだ。さらに$x^{2}$も$1$と$x^{2}$の積と見る。\njulia\u0026gt; differentiate(\u0026#34;1\u0026#34;, :x)\r0\rjulia\u0026gt; differentiate(\u0026#34;1 + x\u0026#34;, :x)\r1\rjulia\u0026gt; differentiate(\u0026#34;x^2\u0026#34;, :x)\r:(2 * 1 * x ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;x^3\u0026#34;, :x)\r:(3 * 1 * x ^ (3 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + x^2\u0026#34;, :x)\r:(1 + 2 * 1 * x ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + 2x + 3x^2 + 4x^3\u0026#34;, :x)\r:((0 * x + 2 * 1) + (0 * x ^ 2 + 3 * (2 * 1 * x ^ (2 - 1))) + (0 * x ^ 3 + 4 * (3 * 1 * x ^ (3 - 1))))\rjulia\u0026gt; differentiate(\u0026#34;x^2 * sin(x) + exp(x) * cos(x)\u0026#34;, :x)\r:(((2 * 1 * x ^ (2 - 1)) * sin(x) + x ^ 2 * (1 * cos(x))) + ((1 * exp(x)) * cos(x) + exp(x) * (1 * -(sin(x))))) 入力された記号でない文字は定数として扱い、二つ以上の記号を入力した場合は、それぞれの記号に対する微分を返す。ただし、\u0026quot;3yx\u0026quot;と書くとxy自体を一つの変数と見てしまうので、必ず掛け算記号を入れて3x*yのように表さなければならない。\njulia\u0026gt; differentiate(\u0026#34;1 + x + 3yx + y^2\u0026#34;, :x)\r:(1 + (0yx + 3 * 0))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3y*x + y^2\u0026#34;, :x)\r:(1 + ((0y + 3 * 0) * x + (3y) * 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3yx + y^2\u0026#34;, [:x, :y])\r2-element Vector{Any}:\r:(1 + (0yx + 3 * 0))\r:((0yx + 3 * 0) + 2 * 1 * y ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, [:x, :y])\r2-element Vector{Any}:\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\r:(((0 * x + 3 * 0) * y + (3x) * 1) + 2 * 1 * y ^ (2 - 1)) simplify() differentiate()の返り値は読みにくいが、simplify()はこれをきれいに整理してくれる。ただし、ベクトルを入力として使うときはうまく実行されない。\njulia\u0026gt; simplify(differentiate(\u0026#34;1 + 2x + 3x^2 + 4x^3\u0026#34;, :x))\r:(2 + 3 * (2x) + 4 * (3 * x ^ 2))\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :x))\r:(1 + 3y)\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :y))\r:(3x + 2y)\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, [:x, :y]))\r2-element Vector{Any}:\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\r:(((0 * x + 3 * 0) * y + (3x) * 1) + 2 * 1 * y ^ (2 - 1)) deparse() 記号微分の返り値を文字列に変換する。\njulia\u0026gt; a = differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :x)\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\rjulia\u0026gt; deparse(a)\r\u0026#34;1 + ((0 * x + 3 * 1) * y + (3 * x) * 0)\u0026#34;\rjulia\u0026gt; deparse(simplify(a))\r\u0026#34;1 + 3 * y\u0026#34; 検算 check_derivative() derivative()で得た導関数が本当の導関数とどれくらい違うかを確認することができる。jacobian()を除いた他の4つの導関数に対して実装されている。\ncheck_derivative(f, Df, a): derivative(f, a)-Df(a)の絶対値を返す。 julia\u0026gt; f(x) = 1 + x^2\rf (generic function with 1 method)\rjulia\u0026gt; Df(x) = 2x\rDf (generic function with 1 method)\rjulia\u0026gt; Calculus.check_derivative(f, Df, 1)\r2.6229241001374248e-11 応用 Polynomials.jl Polynomials.jl自体にもderivativeが実装されているが、Calculus.derivative()でも導関数を求めることができる。\njulia\u0026gt; p = Polynomial([1,2,4,1])\rPolynomial(1 + 2*x + 4*x^2 + x^3)\rjulia\u0026gt; Polynomials.derivative(p)\rPolynomial(2 + 8*x + 3*x^2)\rjulia\u0026gt; Calculus.derivative(p)\r#1 (generic function with 1 method)\rjulia\u0026gt; Polynomials.derivative(p,2)\rPolynomial(8 + 6*x)\rjulia\u0026gt; Calculus.second_derivative(p)\r#6 (generic function with 1 method) 環境 OS: Windows10 Version: Julia 1.6.2, Calculus 0.5.1 https://github.com/JuliaMath/Calculus.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3135,"permalink":"https://freshrimpsushi.github.io/jp/posts/3135/","tags":null,"title":"ジュリアでの微分の求め方"},{"categories":"다변수벡터해석","contents":"要約 二つの関数 $\\mathbf{g} : D \\subset \\mathbb{R}^{m} \\to \\mathbb{R}^{k}$、$\\mathbf{f} : \\mathbf{g}(\\mathbb{R}^{k}) \\subset \\mathbb{R}^{k} \\to \\mathbb{R}^{n}$が微分可能だとしよう。すると、これら二つの関数の合成 $\\mathbf{F} = \\mathbf{f} \\circ \\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$も微分可能であり、$\\mathbf{F}$の(全)導関数は次を満たす。\n$$ \\mathbf{F}^{\\prime}(\\mathbf{x}) = \\mathbf{f}^{\\prime}\\left( \\mathbf{g}(\\mathbf{x}) \\right) \\mathbf{g}^{\\prime}(\\mathbf{x}) $$\n解説 これを連鎖律と呼ぶ。\n$\\mathbf{x} = (x_{1}, \\dots, x_{m})$、$\\mathbf{g}(\\mathbf{x}) = (g_{1}, \\dots, g_{k})$、$\\mathbf{f}(g_{1}, \\dots, g_{k}) = (f_{1}, \\dots, f_{n})$とした場合、公式の具体的な形は全導関数の定義から次のような$n \\times m$行列である。\n$$ \\begin{align*} \\mathbf{F}^{\\prime} (\\mathbf{x}) =\u0026amp;\\ \\begin{bmatrix} \\dfrac{\\partial f_{1}(\\mathbf{g}(\\mathbf{x}))}{\\partial g_{1}} \u0026amp; \\dfrac{\\partial f_{1}}{\\partial g_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f_{1}}{\\partial g_{k}} \\\\[1em] \\dfrac{\\partial f_{2}}{\\partial g_{1}} \u0026amp; \\dfrac{\\partial f_{2}}{\\partial g_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f_{2}}{\\partial g_{k}} \\\\[1em] \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{\\partial f_{n}}{\\partial g_{1}} \u0026amp; \\dfrac{\\partial f_{n}}{\\partial g_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f_{n}}{\\partial g_{k}} \\end{bmatrix} \\begin{bmatrix} \\dfrac{\\partial g_{1}(\\mathbf{x})}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial g_{1}}{\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial g_{1}}{\\partial x_{m}} \\\\[1em] \\dfrac{\\partial g_{2}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial g_{2}}{\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial g_{2}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{\\partial g_{k}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial g_{k}}{\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial g_{k}}{\\partial x_{m}} \\end{bmatrix} \\\\[1em] =\u0026amp;\\ \\begin{bmatrix} \\dfrac{\\partial f_{1}}{\\partial g_{1}} \\dfrac{\\partial g_{1}}{\\partial x_{1}}+\\dfrac{\\partial f_{1}}{\\partial g_{2}}\\dfrac{\\partial g_{2}}{\\partial x_{1}} + \\cdots + \\dfrac{\\partial f_{1}}{\\partial g_{k}} \\dfrac{\\partial g_{k}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial f_{1}}{\\partial g_{1}} \\dfrac{\\partial g_{1}}{\\partial x_{1}}+\\dfrac{\\partial f_{1}}{\\partial g_{2}}\\dfrac{\\partial g_{2}}{\\partial x_{m}} + \\cdots + \\dfrac{\\partial f_{1}}{\\partial g_{k}} \\dfrac{\\partial g_{k}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{\\partial f_{n}}{\\partial g_{1}} \\dfrac{\\partial g_{1}}{\\partial x_{1}}+\\dfrac{\\partial f_{n}}{\\partial g_{2}}\\dfrac{\\partial g_{2}}{\\partial x_{1}} + \\cdots + \\dfrac{\\partial f_{n}}{\\partial g_{k}} \\dfrac{\\partial g_{k}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f_{n}}{\\partial g_{1}} \\dfrac{\\partial g_{1}}{\\partial x_{1}}+\\dfrac{\\partial f_{n}}{\\partial g_{2}}\\dfrac{\\partial g_{2}}{\\partial x_{m}} + \\cdots + \\dfrac{\\partial f_{n}}{\\partial g_{m}} \\dfrac{\\partial g_{k}}{\\partial x_{m}} \\end{bmatrix} \\\\[1em] =\u0026amp;\\ \\begin{bmatrix} \\displaystyle \\sum\\limits_{\\ell =1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\displaystyle \\sum\\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\displaystyle \\sum\\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\displaystyle \\sum\\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\end{bmatrix} \\end{align*} $$\nアインシュタインの記法で簡単に表すと、$1 \\le i \\le n$、$1 \\le j \\le m$に対して\n$$ \\mathbf{F}^{\\prime} = \\left[ F_{ij}^{\\prime} \\right] = \\begin{bmatrix} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{ij}^{\\prime} = \\dfrac{\\partial f_{i}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{j}} $$\nこれは最も一般化された形なので、$k, m, n$に従って、さまざまな具体的な公式を得ることができる。\n公式 ケース 1. $g : \\mathbb{R} \\to \\mathbb{R}$、$f : \\mathbb{R} \\to \\mathbb{R}$、$F = f \\circ g : \\mathbb{R} \\to \\mathbb{R}$\n$x \\in \\mathbb{R}$、$g = g(x)$、$f = f(g(x))$のとき、\n$$ F^{\\prime} = \\dfrac{d F}{d x} = \\dfrac{d f}{d g} \\dfrac{d g}{d x} $$\n証明\nケース 2. $\\mathbf{g} : \\mathbb{R} \\to \\mathbb{R}^{k}$、$f : \\mathbb{R}^{k} \\to \\mathbb{R}$、$F = f \\circ \\mathbf{g} : \\mathbb{R} \\to \\mathbb{R}$\n$x \\in \\mathbb{R}$、$\\mathbf{g}(x) = (g_{1}, \\dots, g_{k})$、$f = f(g_{1}, \\dots ,g_{k})$のとき、\n$$ F^{\\prime} = \\dfrac{d F}{d x} = \\sum \\limits_{\\ell=1}^{k}\\dfrac{\\partial f}{\\partial g_{\\ell}} \\dfrac{d g_{\\ell}}{d x} $$\nケース 3. $g : \\mathbb{R}^{m} \\to \\mathbb{R}$、$f : \\mathbb{R} \\to \\mathbb{R}$、$F = f \\circ g : \\mathbb{R}^{m} \\to \\mathbb{R}$\n$\\mathbf{x} = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$、$g = g(\\mathbf{x})$、$f = f(g(\\mathbf{x}))$のとき、\n$$ F^{\\prime} = \\dfrac{d F}{d \\mathbf{x}} = \\begin{bmatrix} \\dfrac{d f}{d g} \\dfrac{\\partial g}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{d f}{d g} \\dfrac{\\partial g}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{j}^{\\prime} = \\dfrac{d f}{d g} \\dfrac{\\partial g}{\\partial x_{j}},\\quad 1 \\le j \\le m $$\nケース 4. $\\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}^{k}$、$f : \\mathbb{R}^{k} \\to \\mathbb{R}$、$F = f \\circ \\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}$\n$\\mathbf{x} = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$、$\\mathbf{g}(x) = (g_{1}, \\dots, g_{k})$、$f = f(g_{1}, \\dots, g_{k})$のとき、\n$$ F^{\\prime} = \\dfrac{d F}{d \\mathbf{x}} = \\begin{bmatrix} \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{j}^{\\prime} = \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{j}},\\quad 1 \\le j \\le m $$\nケース 5. $g : \\mathbb{R} \\to \\mathbb{R}$、$\\mathbf{f} : \\mathbb{R} \\to \\mathbb{R}^{n}$、$\\mathbf{F} = \\mathbf{f} \\circ g : \\mathbb{R} \\to \\mathbb{R}^{n}$\n$x \\in \\mathbb{R}$、$g = g(x)$、$\\mathbf{f}(g(x)) = (f_{1}, \\dots, f_{n})$のとき、\n$$ \\mathbf{F}^{\\prime} = \\dfrac{d \\mathbf{F}}{d x} = \\begin{bmatrix} \\dfrac{d f_{1}}{d g} \\dfrac{d g}{d x} \\\\[1em] \\vdots \\\\[1em] \\dfrac{d f_{n}}{d g} \\dfrac{d g}{d x} \\end{bmatrix} $$\n$$ F_{i}^{\\prime} = \\dfrac{d f_{i}}{d g} \\dfrac{d g}{d x},\\quad 1\\le i \\le n $$\nケース 6. $\\mathbf{g} : \\mathbb{R} \\to \\mathbb{R}^{k}$、$\\mathbf{f} : \\mathbb{R}^{k} \\to \\mathbb{R}^{n}$、$\\mathbf{F} = \\mathbf{f} \\circ \\mathbf{g} : \\mathbb{R} \\to \\mathbb{R}^{n}$\n$x \\in \\mathbb{R}$、$\\mathbf{g}(x) = (g_{1}, \\dots, g_{k})$、$\\mathbf{f}(g_{1}, \\dots ,g_{k}) = (f_{1}, \\dots, f_{n})$のとき、\n$$ \\mathbf{F}^{\\prime} = \\dfrac{d \\mathbf{F}}{d x} = \\begin{bmatrix} \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{d g_{\\ell}}{d x} \\\\[1em] \\vdots \\\\[1em] \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{d g_{\\ell}}{d x} \\end{bmatrix} $$\n$$ F_{i}^{\\prime} = \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{i}}{\\partial g_{\\ell}} \\dfrac{d g_{\\ell}}{d x},\\quad 1\\le i \\le n $$\nケース 7. $g : \\mathbb{R}^{m} \\to \\mathbb{R}$、$\\mathbf{f} : \\mathbb{R} \\to \\mathbb{R}^{n}$、$\\mathbf{F} = \\mathbf{f} \\circ g : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$\n$\\mathbf{x} = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$、$g = g(\\mathbf{x})$、$\\mathbf{f}(g(\\mathbf{x})) = (f_{1}, \\dots, f_{n})$のとき、\n$$ \\mathbf{F}^{\\prime} = \\dfrac{d \\mathbf{F}}{d \\mathbf{x}} = \\begin{bmatrix} \\dfrac{d f_{1}}{d g} \\dfrac{\\partial g}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{d f_{1}}{d g} \\dfrac{\\partial g}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{d f_{n}}{d g} \\dfrac{\\partial g}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{d f_{n}}{d g} \\dfrac{\\partial g}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{ij}^{\\prime} = \\dfrac{d f_{i}}{d g} \\dfrac{\\partial g}{\\partial x_{j}},\\quad 1\\le i \\le n, 1 \\le j \\le m $$\nケース 8. $\\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}^{k}$、$\\mathbf{f} : \\mathbb{R}^{k} \\to \\mathbb{R}^{n}$、$\\mathbf{F} = \\mathbf{f} \\circ \\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$\n$\\mathbf{x} = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$、$g(\\mathbf{x}) = (g_{1}, \\dots, g_{k})$、$\\mathbf{f}(g_{1}, \\dots, g_{k}) = (f_{1}, \\dots, f_{n})$のとき、\n$$ \\mathbf{F}^{\\prime} = \\dfrac{d \\mathbf{F}}{d \\mathbf{x}} = \\begin{bmatrix} \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{ij}^{\\prime} = \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{i}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{j}},\\quad 1\\le i \\le n, 1 \\le j \\le m $$\n証明 一般化された証明を参照。\n■\n","id":3134,"permalink":"https://freshrimpsushi.github.io/jp/posts/3134/","tags":null,"title":"多変数ベクトル関数の連鎖律"},{"categories":"줄리아","contents":"コード fill() 関数を使えばいい。Rの rep() 関数と似た機能をする。\n","id":2101,"permalink":"https://freshrimpsushi.github.io/jp/posts/2101/","tags":null,"title":"ジュリアで特定の値で埋めた配列を作る方法"},{"categories":"기하학","contents":"ビルドアップ1 微分多様体 $M$ の各点で接ベクトルを定義しようとしている。微分可能な曲線 $\\alpha : (-\\epsilon , \\epsilon) \\to M$が与えられたとする。これから、微分幾何学でのように、$\\alpha$の$t=0$での微分係数$\\dfrac{d \\alpha}{dt}(0)$を接ベクトルと定義したいが、$\\alpha$の値域が$M$であるため（距離空間とは限らないため）、$\\alpha$の導関数を言及することができない。このため、多様体上の接ベクトルを関数、つまりオペレーターとして定義することになる。微分幾何学を学んだなら、ベクトルをオペレーターとして扱うことに慣れているはずだ。次の説明を見てみよう。\n方向微分\n$\\mathbf{X} \\in T_{p}M$を曲面$M$の点$p$での接ベクトル、$\\alpha (t)$を$M$上の曲線とする。この時、$\\alpha : (-\\epsilon, \\epsilon) \\to M$であり、$\\alpha (0) = p$を満たす。つまり、$\\mathbf{X} = \\dfrac{d \\alpha}{d t} (0)$である。ここで関数$f$を曲面$M$上の点$p \\in M$のある近傍で定義された微分可能な関数とする。すると$\\mathbf{X}$方向への$f$の方向微分directional derivative$\\mathbf{X}f$を次のように定義する。\n$$ \\mathbf{X} : \\mathcal{D} \\to \\mathbb{R}, \\quad \\text{where } \\mathcal{D} \\text{ is set of all differentiable functions near } p $$\n$$ \\mathbf{X} f := \\dfrac{d}{dt_{}} (f \\circ \\alpha) (0) $$\n上の定義から見て、固定された接ベクトル$\\mathbf{X}$があれば、$f$が与えられるたびに$\\mathbf{X}f$が決定される。したがって、接ベクトルはそれ自体がオペレーターとして扱われる。$\\mathbf{X}f$のような記法も、オペレーターの観点から見るために使われる。微分多様体上の接ベクトルも同様に、$M$上で微分可能な関数$f$が与えられるたびに、$f$とある曲線$\\alpha$との合成を通じて実数空間をマッピングする関数として定義される。\n定義 $M$を$n$次元の微分多様体とする。微分可能な関数 $\\alpha : (-\\epsilon , \\epsilon) \\to M$を**$M$で微分可能な曲線**とする。$\\alpha (0)=p\\in M$と仮定する。そして集合$\\mathcal{D}$を次のように$p$で微分可能な関数の集合として定義する。\n$$ \\mathcal{D} := \\left\\{ f : M \\to \\mathbb{R} | \\text{functions on } M \\text{that are differentiable at } p \\right\\} $$\nすると、$\\alpha (0) = p$での接ベクトル$\\alpha^{\\prime}(0) : \\mathcal{D} \\to \\mathbb{R}$を次のような関数として定義する。\n$$ \\alpha^{\\prime} (0) f = \\dfrac{d}{dt} (f\\circ \\alpha)(0),\\quad f\\in \\mathcal{D} $$\n点$p\\in M$でのすべての接ベクトルの集合を接空間tangent spaceと呼び、$T_{p}M$のように表す。\n説明 $f : M \\to \\mathbb{R}$と$\\alpha : (-\\epsilon, \\epsilon) \\to M$はそれぞれ定義域と値域が距離空間であることが保証されていないため、古典的な意味で微分できないが、これらの合成である$f \\circ \\alpha : (-\\epsilon, \\epsilon) \\to \\mathbb{R}$は微分可能である。\nある微分可能な曲線$\\alpha$が与えられるたびに接ベクトルが決定されるので、微分可能な曲線が存在する限り接ベクトルが存在すると考えることができる。また、二つの接ベクトル$\\mathbf{X}, \\mathbf{Y}$が異なる二つの曲線$\\alpha$、$\\beta$によって決定されたとしても、すべての$f \\in \\mathcal{D}$に対して$\\mathbf{X}f = \\mathbf{Y}f$が成立する場合、$\\mathbf{X}$と$\\mathbf{Y}$を同じ接ベクトルとして扱う。\n接ベクトルの集合$T_{p}M$を接空間と呼ぶ理由は、これが実際に$n$次元のベクトル空間であるからである。\n以下に紹介する定理から、点$p$での接ベクトルの関数値$\\alpha^{\\prime}(0)f$は、任意の座標系$\\mathbf{x} : U \\to M$を一つ選択することでこれに対して表すことができ、この値は$\\mathbf{x}$の選択に依存しないことがわかる。\n例 $T_{p}\\mathbb{R}^{3}$を考えよう。ある微分可能な曲線$\\alpha : (-\\epsilon, \\epsilon) \\to \\mathbb{R}^{3}$が決定されると、3次元ベクトル$\\alpha^{\\prime}(0) = \\mathbf{v} = (v_{1}, v_{2}, v_{3}) \\in \\mathbb{R}^{3}$が決定される。定義により、接ベクトルは次のようになる。$f : \\mathbb{R}^{3} \\to \\mathbb{R}$に対して、\n$$ \\mathbf{X}f = \\dfrac{d (f\\circ \\alpha)}{d t}(0) = \\sum \\limits_{i} \\dfrac{\\partial f}{\\partial x_{i}}\\dfrac{d \\alpha_{i}}{d t}(0) = \\sum\\limits_{i} v_{i} \\dfrac{\\partial f}{\\partial x_{i}} $$\nこれはユークリッド空間での方向微分と同じである。\n$$ \\mathbf{v}[f] = \\nabla _{\\mathbf{v}}f = \\mathbf{v} \\cdot \\nabla f = \\sum \\limits_{i} v_{i} \\dfrac{\\partial f}{\\partial v_{i}} $$\n方向微分はベクトルをオペレーターとして扱ったものであり、実質的にベクトルと同じである。したがって、$\\mathbf{X}$は$\\mathbb{R}^{3}$の要素として扱うことができ、次が成立する。\n$$ T_{p}\\mathbb{R}^{3} \\approxeq \\mathbb{R}^{3} $$\n定理 $\\alpha (0) = p$である微分可能な曲線と点$p$での座標系$\\mathbf{x} : U \\to M$が与えられたとする。$(u_{1}, \\dots, u_{n})$は$\\mathbb{R}^{n}$の座標であり、\n$$ (x_{1}(p), \\dots, x_{n}(p)) = \\mathbf{x}^{-1}(p) $$\nとする。すると、次の式が成立する。\n$$ \\begin{align*} \\alpha ^{\\prime} (0) f =\u0026amp;\\ \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(p) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{p} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(\\alpha (0)) \\left.\\dfrac{\\partial f}{\\partial x_{i}}\\right|_{t=0} \\end{align*} $$\nこの時、単純に$x_{i}^{\\prime}(0) = x_{i}^{\\prime}(\\alpha (0))$と表記する。したがって、$\\alpha^{\\prime}(0)$は次のような微分オペレーターである。\n$$ \\begin{equation} \\alpha ^{\\prime} (0) = \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} \\end{equation} $$\n基底$\\left\\{ \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} \\right\\}$に対して座標ベクトルで表記すると、次のようになる。\n$$ \\alpha ^{\\prime} (0) = \\begin{bmatrix} x_{1}^{\\prime}(0) \\\\ \\vdots \\\\ x_{n}^{\\prime}(0) \\end{bmatrix} $$\n証明 $p = \\mathbf{x}(0)$となるような$M$の座標系 $\\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to M$を一つ選ぼう。接ベクトルを座標系で表現できるように$f\\circ \\alpha = f \\circ \\mathbf{x} \\circ \\mathbf{x}^{-1} \\circ \\alpha$のように考える。すると、$\\mathbf{x} \\circ \\mathbf{x}^{-1} = I$は恒等関数であるため、どの座標系を選んでも関係ないことがわかる。これから、$f \\circ \\mathbf{x}$と$\\mathbf{x}^{-1} \\circ \\alpha$をそれぞれ全体として一つの関数とみなし、$f \\circ \\alpha$をこれら二つの合成関数と考える。\n$$ f \\circ \\alpha = (f \\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha) $$\nまず$f \\circ \\mathbf{x}$を考える。$f \\circ \\mathbf{x} : \\mathbb{R}^{n} \\to \\mathbb{R}$であるため、次のように表現され、古典的な意味で微分が可能である。\n$$ f \\circ \\mathbf{x} = f \\circ \\mathbf{x} (u) = f \\circ \\mathbf{x} (u_{1}, u_{2}, \\dots, u_{n}),\\quad u=(u_{1},\\dots,u_{n}) \\in \\mathbb{R}^{n} $$\n$\\mathbf{x}^{-1} \\circ \\alpha$もまた、$\\mathbf{x}^{-1} \\circ \\alpha : \\mathbb{R} \\to \\mathbb{R}^{n}$であるため、次のように表現され、古典的な意味で微分が可能である。\n$$ \\begin{align*} \\mathbf{x}^{-1} \\circ \\alpha (t) =\u0026amp;\\ (x_{1}(\\alpha (t)), x_{2}(\\alpha (t)), \\dots, x_{n}(\\alpha (t))) \\\\ =\u0026amp;\\ (x_{1}(t), x_{2}(t), \\dots, x_{n}(t)) \\end{align*} $$\nこの時、$x_{i}$は$x_{i} : M \\to \\mathbb{R}$である関数であり、$x_{i}(t)$は$x_{i}(\\alpha (t))$を簡単に表記したものであることに注意する。\nこのように考えると、$f \\circ \\alpha$は二つの関数の合成であり、$\\mathbb{R} \\overset{\\mathbf{x}^{-1} \\circ \\alpha}{\\longrightarrow} \\mathbb{R}^{n} \\overset{f\\circ \\mathbf{x}}{\\longrightarrow} \\mathbb{R}$のようにマッピングされる。したがって、連鎖律により、次が成立する。\n$$ \\dfrac{d}{d t}(f \\circ \\alpha) = \\dfrac{d}{dt} \\left( (f\\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha) \\right) = \\sum \\limits_{i=1}^{n}\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}} \\dfrac{d (\\mathbf{x}^{-1} \\circ \\alpha )_{i}}{d t} = \\sum \\limits_{i=1}^{n}\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}} \\dfrac{d x_{i}}{d t} $$\nしたがって、次を得る。\n$$ \\begin{align*} \\alpha^{\\prime}(0) f :=\u0026amp;\\ \\dfrac{d}{dt} (f\\circ \\alpha)(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\dfrac{d x_{i}}{d t}(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} x_{i}^{\\prime}(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\end{align*} $$\nここで、$\\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0}$を次のようなオペレーターとして定義しよう。\n$$ \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} f := \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} $$\n$\\dfrac{\\partial f}{\\partial x_{i}}$の意味をまとめると、次のようになる。\n$f$は定義域が$M$であるため微分できない。したがって、座標系$\\mathbf{x} : \\mathbb{R}^{n} \\to M$との合成を通じて$f\\circ \\mathbf{x}$を考える。これは$\\mathbb{R}^{n}$を$\\mathbb{R}$にマッピングするため、古典的な意味で微分が可能である。したがって、$\\dfrac{\\partial f}{\\partial x_{i}}$は$f$を$\\mathbf{x}$と合成した後、これをユークリッド空間$\\mathbb{R}^{n}$の$i$番目の変数$u_{i}$に対して微分したものとして定義する。\n最終的に次を得る。\n$$ \\begin{align*} \\alpha^{\\prime}(0) f =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0}f = \\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial f}{\\partial x_{i}}\\right|_{t=0} \\end{align*} $$\n$$ \\implies \\alpha^{\\prime}(0) = \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} $$\n■\n関連項目 解析学での方向微分 微分幾何学での方向微分 Manfredo P. Do Carmo, Riemannian Geometry (英語版, 1992), p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3132,"permalink":"https://freshrimpsushi.github.io/jp/posts/3132/","tags":null,"title":"微分可能多様体上の接線ベクトル"},{"categories":"기하학","contents":"定義1 $M_{1}, M_{2}$をそれぞれ$n, m$次元の微分多様体とする。マッピング$\\varphi : M_{1} \\to M_{2}$が以下の条件を満たせば、$p \\in M_{1}$で微分可能differentiable at $p$と定義される。\n$\\varphi(p)$で座標系$\\mathbf{y} : V \\subset \\mathbb{R}^{m} \\to M_{2}$が与えられた時、$p$で座標系$\\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to M_{1}$が存在し、$\\varphi\\left( \\mathbf{x}(U) \\right) \\subset \\mathbf{y}(V)$が成立する。\nマッピング$\\mathbf{y}^{-1} \\circ \\varphi \\circ \\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$が$\\mathbf{x}^{-1}(p)$で微分可能である。\n説明 微分可能な多様体を定義するときと同様に、座標系$\\mathbf{x}, \\mathbf{y}$を通じて微分を定義する。\n条件1.は一見難しそうだが、よく見ると$\\epsilon -\\delta$の方法の定義や位相数学での連続性を定義するセンスと完全に一致している。\n条件2.における$\\mathbf{y}^{-1} \\circ \\varphi \\circ \\mathbf{x}$は、ユークリッド空間からユークリッド空間への関数なので、古典的なセンスで微分可能である。このマッピングは、座標系$\\mathbf{x}$と$\\mathbf{y}$での$\\varphi$のexpressionと呼ばれる。\nManfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p5-6\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3130,"permalink":"https://freshrimpsushi.github.io/jp/posts/3130/","tags":null,"title":"微分可能多様体から微分可能多様体への微分可能関数"},{"categories":"줄리아","contents":"コード XsDB_주거인구_100M_TM.shpというshpファイルを読み込むコードは以下の通りだ。\nusing Shapefile\rcd(@__DIR__)\rpath = \u0026#34;XsDB_주거인구_100M_TM.shp\u0026#34;\rtable = Shapefile.Table(path)\rusing DataFrames\rdf = DataFrame(table) もちろん、ファイルを読み込むだけではできることが限られており、データを調べるためにはデータフレームに変換する必要がある。\n実行結果 959660×16 DataFrame\rRow │ geometry MEGA_NM MEGA_CD CTY_NM CTY_CD X_AXIS Y_AXIS HOUS POP POP_10 POP_20 POP_30 POP_40 POP_50 POP_60_O \\xb9\\xe8\\xc6\\xf7ó │ Point…? String String String String Int64 Int64 Float64 Float64 Float64 Float64 Float64 Float64 Float64 Float64 String\r────────┼─────────────────────────────────────────────────────────────────────────────────────────────────\r1 │ Point(254298.0, 4.26549e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 365950 526450 10.08 24.56 4.64 1.92 1.84 4.72 4.32 7.12 biz-gis.com\r2 │ Point(2.59622e5, 4.24405e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 371250 524250 1.42 3.47 0.81 0.29 0.52 0.52 0.59 0.74 biz-gis.com\r3 │ Point(2.61134e5, 423221.0) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 372750 523050 1.26 3.08 0.68 0.28 0.35 0.49 0.45 0.83 biz-gis.com\r4 │ Point(2.50311e5, 4.15806e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 361850 515750 10.08 25.2 3.68 2.96 1.68 4.4 6.0 6.48 biz-gis.com\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮\r959658 │ Point(2.09955e5, 2.46768e5) \\xc0\\xfc\\xb6\\xf3\\xbaϵ\\xb5 45 \\xbf\\xcf\\xc1ֱ\\xba 45710 319750 347150 1.83 4.53 1.92 0.24 0.45 0.72 0.69 0.51 biz-gis.com\r959659 │ Point(215588.0, 4.55344e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xb3\\xb2\\xbe\\xe7\\xc1ֽ\\xc3 41360 327550 555650 2.38 5.31 0.91 0.74 0.57 0.97 1.05 1.07 biz-gis.com\r959660 │ Point(2.54717e5, 4.24754e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 366350 524650 1.26 3.07 0.58 0.24 0.23 0.59 0.54 0.89 biz-gis.com\r959653 rows omitted 環境 OS: Windows julia: v1.5.0 ","id":2097,"permalink":"https://freshrimpsushi.github.io/jp/posts/2097/","tags":null,"title":"ジュリアでSHPファイルを読む方法"},{"categories":"줄리아","contents":"## 概要 `trunc`関数を使うには、第一引数に`Int`を入れるだけだ。 ## コード julia\u0026gt; @time for t in 1:10^8 Int64(ceil(t/1000)) end 0.189653 seconds\njulia\u0026gt; @time for t in 1:10^8 trunc(Int64, ceil(t/1000)) end 0.128472 seconds\n二つのループは全く同じ機能をするけど、1.5倍ほどの速度差がある。上は`ceil`で小数点以下を切り捨てて`Int64`に型キャストしたんだけど、下は`trunc`関数の内蔵機能を使ってネイティブに整数を返すから、より速いんだ。 他の言語を使ってた人には、上のループみたいなストレートな命令が自然に感じるだろうけど、Juliaにはデータ型を第一引数に入れて結果を返すような内蔵関数が多いから、下のループみたいな使い方の方が慣れると思うよ。 ## 環境 - OS: Windows - julia: v1.5.0 ","id":2095,"permalink":"https://freshrimpsushi.github.io/jp/posts/2095/","tags":null,"title":"ジュリアで小数点以下を切り捨てて整数に変換する方法"},{"categories":"줄리아","contents":"概要 rename!() 関数を使って変更するといい1。\n文字列のリストを与えて一度に変更する方法もあるし、個別に変更する方法もある。\nコード using DataFrames\rdf = DataFrame(rand(1:9, 10, 3), :auto)\rrename!(df, [\u0026#34;X\u0026#34;, \u0026#34;Y\u0026#34;, \u0026#34;Z\u0026#34;])\rrename!(df, :X =\u0026gt; :A) 実行すると、最初に次のようなデータフレームが生成される。\njulia\u0026gt; df = DataFrame(rand(1:9, 10, 3), :auto)\r10×3 DataFrame\rRow │ x1 x2 x3 │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 一度に変更する方法 julia\u0026gt; rename!(df, [\u0026#34;X\u0026#34;, \u0026#34;Y\u0026#34;, \u0026#34;Z\u0026#34;])\r10×3 DataFrame\rRow │ X Y Z │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 文字列のリストを与えればいい。\n一つずつ変更する方法 julia\u0026gt; rename!(df, :X =\u0026gt; :A)\r10×3 DataFrame\rRow │ A Y Z │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 他の言語ではあまり見られない方法だが、列名の前に : を付けて、=\u0026gt; でマッピングする。ジュリアで : で始まる変数はシンボルSymbolだ。\n環境 OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/change-column-names-of-a-dataframe-previous-methods-dont-work/48026/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2093,"permalink":"https://freshrimpsushi.github.io/jp/posts/2093/","tags":null,"title":"ジュリアでデータフレームの列名を変更する方法"},{"categories":"함수","contents":"定義1 関数 $f : X \\to Y$が与えられたとしよう。$U \\subset X \\subset V$が成立するとしよう。\n縮小写像 次を満たす$f |_{U} \\to Y$を$f$の縮小写像Uへの$f$の制限と言う。\n$$ f|_{U} : U \\to Y \\quad \\text{and} \\quad f|_{U}(x) = f (x),\\quad \\forall x \\in U $$\n拡張 次を満たす$\\tilde{f} \\to Y$を$f$の拡張Vへの$f$の拡張と言う。\n$$ \\tilde{f} : V \\to Y \\quad \\text{and} \\quad \\tilde{f}(x) = f (x),\\quad \\forall x \\in X $$\n説明 通常、縮小写像(制限とも言われる)、拡張という翻訳語より、そのまま英語読みの[リストラクション]、[エクステンション]と言うことが多い。\n簡単に言えば、関数の形をそのまま保ちながら、定義域を狭めたり広げたりすることである。\n定義によって、明らかに$f$は$\\tilde{f}$のリストラクションであり、$f|_{U}$のエクステンションである。\nErwin Kreyszig, Introductory Functional Analysis with Applications (1989), p99\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3123,"permalink":"https://freshrimpsushi.github.io/jp/posts/3123/","tags":null,"title":"関数の拡張と縮小"},{"categories":"줄리아","contents":"概要 $n$個の座標同士の距離を計算するにあたり、行列を作る必要はなく、単に距離を計算する場合、多次元検索に有利なデータ構造であるk-dツリー1を使用して速度を上げることができます。NearestNeighbors.jlに関連するアルゴリズムがすべて実装されているので、公式GitHubページを参照してください。\n速度比較 pairwise()関数で距離行列の計算に最適化された技術と比較してみましょう。\nusing Distances\rusing StatsBase\rusing Random\rusing NearestNeighbors\rRandom.seed!(0);\rε = 0.01\rN = 10^4\rcoordinate = rand(2, N);\rstate = sample([\u0026#39;S\u0026#39;, \u0026#39;I\u0026#39;], Weights([0.1, 0.9]), N);\rS = coordinate[:, state .== \u0026#39;S\u0026#39;]\rI = coordinate[:, state .== \u0026#39;I\u0026#39;]\r@time sum(pairwise(Euclidean(),S,I) .\u0026lt; ε, dims = 1)\r@time kdtree = KDTree(S); contact = length.(inrange(kdtree, I, ε, true)) 上記のコードを実行した結果は以下の通りです。一番下の二つのコマンドラインは同じ作業を行いますが、速度差は約500倍になります。実際、k-dツリーで検索する際の時間複雑度は$\\log n$で非常に効率的です。\njulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; ε, dims = 1)\r0.098394 seconds (14 allocations: 69.639 MiB, 8.23% gc Time)\r1×9004 Array{Int64,2}:\r0 0 1 0 0 0 0 … 1 0 0 0 0 0\rjulia\u0026gt; @time kdtree = KDTree(S); contact = length.(inrange(kdtree, I, ε, true))\r0.000213 seconds (22 allocations: 51.609 KiB)\r9004-element Array{Int64,1}:\r0\r0\r1\r0\r⋮\r0\r0\r0 単純な速度の問題を置いておいても、k-dツリーを使用した方法では1次元の配列を返すため、結果物を使用する側面でもさらに便利です。\n環境 OS: Windows julia: v1.6.2 https://en.wikipedia.org/wiki/K-d_tree\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2088,"permalink":"https://freshrimpsushi.github.io/jp/posts/2088/","tags":null,"title":"JuliaでNearstNeighbors.jlを使用して距離を素早く計算する方法"},{"categories":"기하학","contents":"定義1 $M$を任意の集合、$U_{\\alpha} \\subset \\mathbb{R}^{n}$を開集合とする。関数$1-1$ $\\mathbf{x}_{\\alpha} : U_{\\alpha} \\to M$に対して、以下の条件を満たす順序対$\\left( M, \\left\\{ \\mathbf{x}_{\\alpha} \\right\\}_{\\alpha\\in \\mathscr{A}} \\right)$、または簡単に$M$を**$n$次元の微分可能多様体**dimension $n$の differentiable manifoldと定義する。\n$\\bigcup \\limits_{\\alpha} \\mathbf{x}_{\\alpha} \\left( U_{\\alpha} \\right) = M$ $\\varnothing \\ne W = \\mathbf{x}_{\\alpha}\\left( U_{\\alpha} \\right) \\cap \\mathbf{x}_{\\beta}\\left( U_{\\beta} \\right)$に対して、写像$\\mathbf{x}_{\\beta}^{-1} \\circ \\mathbf{x}_{\\alpha} : \\mathbf{x}_{\\alpha}^{-1}(W) \\to \\mathbf{x}_{\\beta}^{-1}(W)$が微分可能であること。 条件1および2を満たす可能なすべての$\\alpha$に対して、指数族$\\left\\{ \\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right) \\right\\}$を構成する。 説明 単に微分多様体または滑らかな多様体とも言う。$n$次元の微分多様体は、時に$M^{n}$と表記する。\n$p \\in \\mathbf{x}_{\\alpha}(U_{\\alpha})$のとき、$\\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right)$または単に$\\mathbf{x}_{\\alpha}$を$M$の$p$における座標系$M$ at $p$の system of coordinates、局所座標系、またはパラメータ化と呼ぶ。\n$\\mathbf{x}_{\\alpha}(U_{\\alpha})$を$p \\in M$における座標近傍と呼ぶ。\n条件3.の指数族$\\left\\{ \\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right) \\right\\}$を$M$上の微分可能構造と呼ぶ。\n$p \\in M$に対して、$\\mathbf{x}_{\\alpha}^{-1}(p) = \\left( x_{1}(p), \\dots, x_{n}(p) \\right)$を満たす$x_{i}$を座標関数と呼ぶ。\n$M$は完全に任意の集合として与えられる(つまり、通常は距離空間ではない)ので、$\\mathbf{x}_{\\alpha}$が微分可能かどうかについての議論ができない。さらに、$M$は様々なイメージの合併であるため、各交差点$W = \\mathbf{x}_{\\alpha}\\left( U_{\\alpha} \\right) \\cap \\mathbf{x}_{\\beta}\\left( U_{\\beta} \\right)$で適切に良い条件が必要であり、ここではそれを微分可能であるという条件で与えられている。\n写像$\\mathbf{x}_{\\beta}^{-1} \\circ \\mathbf{x}_{\\alpha}$の条件によって、多様体が様々な名前で呼ばれることになる。例えば、微分の代わりに連続であるという条件が与えられた場合、$M$は位相多様体になる。正則という条件が与えられた場合、$M$は複素多様体になる。また、$\\mathbf{x}_{\\beta}^{-1} \\circ \\mathbf{x}_{\\alpha} \\in C^{k}$の場合、$M$は$C^{k}$多様体と呼ばれる。微分幾何学では、微分という道具を使って幾何学を説明したいため、微分可能な多様体が扱われる。\nこの内容は技術的な部分であり、二つの微分可能構造が同じか違うか等の話を避けるために存在する条件である。1および2を満たすそのようなものすべてが集められていると仮定した方が良い。「こんなのはどうだ?」「これも含まれるか?」といったタックルをかけないで欲しいという意味である。\n例 ユークリッド空間 $\\mathbb{R}^{n}$ $$ \\mathbb{R}^{n} = \\left\\{ (x_{1}, x_{2}, \\dots, x_{n}) : x_{i} \\in \\mathbb{R} \\right\\} $$\n多様体が局所的にユークリッド空間と似ているため、$\\mathbb{R}^{n}$が微分可能な多様体であることは自然なことである。${\\rm id}$を恒等作用素とする。\n微分可能構造を$\\left\\{ \\left( U_{\\alpha}, {\\rm id} \\right) | U_{\\alpha} \\subset \\mathbb{R}^{n} \\text{ is open.} \\right\\}$のようにすると成立する。\n恒等作用素は微分可能なので成立する。\nこのようなすべての順序対に対して、指数族$\\left\\{ \\left( U_{\\alpha}, {\\rm id} \\right)\\right\\}$を構成する。\nしたがって、$\\left( \\mathbb{R}^{n}, \\left\\{ {\\rm id} \\right\\} \\right)$は微分可能な多様体である。\n2次元球面 $\\mathbb{S}^{2}$ $$ \\mathbb{S}^{2} = \\left\\{ p \\in \\mathbb{R}^{3} : \\left\\| p \\right\\|=1 \\right\\} $$\n2次元球面は、以下のように6つの座標パッチで表現できる。$(u,v) \\in U = \\left\\{ (u,v) : u^{2} + v^{2} \\lt 1 \\right\\}$について、\n座標パッチ 定義 逆 $\\mathbf{x}_{1} = \\mathbf{x}_{(0,0,1)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,0,1)}(u, v) = \\left( u, v , \\sqrt{1- u^{2} -v^{2} } \\right)$ $\\mathbf{x}_{(0,0,1)}^{-1}(x, y, z) = (x,y)$ $\\mathbf{x}_{2} = \\mathbf{x}_{(0,0,-1)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,0,-1)}(u, v) = \\left( u, v , -\\sqrt{1- u^{2} -v^{2} } \\right)$ $\\mathbf{x}_{(0,0,-1)}^{-1}(x, y, z) = (x,y)$ $\\mathbf{x}_{3} = \\mathbf{x}_{(0,1,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,1,0)}(u, v) = \\left( u, \\sqrt{1- u^{2} -v^{2}}, v \\right)$ $\\mathbf{x}_{(0,1,0)}^{-1}(x, y, z) = (x,z)$ $\\mathbf{x}_{4} = \\mathbf{x}_{(0,-1,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,-1,0)}(u, v) = \\left( u, -\\sqrt{1- u^{2} -v^{2}}, v \\right)$ $\\mathbf{x}_{(0,-1,0)}^{-1}(x, y, z) = (x,z)$ $\\mathbf{x}_{5} = \\mathbf{x}_{(1,0,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(1,0,0)}(u, v) = \\left( \\sqrt{1- u^{2} -v^{2}}, u, v \\right)$ $\\mathbf{x}_{(1,0,0)}^{-1}(x, y, z) = (y,z)$ $\\mathbf{x}_{6} = \\mathbf{x}_{(-1,0,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(-1,0,0)}(u, v) = \\left( -\\sqrt{1- u^{2} -v^{2}}, u, v \\right)$ $\\mathbf{x}_{(-1,0,0)}^{-1}(x, y, z) = (y,z)$ $\\bigcup \\limits_{i=1}^6 \\mathbf{x}_{i} = \\mathbb{S}^{2}$が成立する。\n$\\mathbf{x}_{(0,0,1)}^{-1} \\circ \\mathbf{x}_{(1,0,0)}$は次のようになるため、微分可能である。\n$$\\mathbf{x}_{(0,0,1)}^{-1} \\circ \\mathbf{x}_{(1,0,0)}(u,v) = \\mathbf{x}_{(0,0,1)}^{-1} \\left( \\sqrt{1- u^{2} -v^{2}}, u, v \\right) = \\left( \\sqrt{1- u^{2} -v^{2}}, u \\right) \\in C^{\\infty}$$\nこの方法で1, 2を満たすすべての順序対を集め、指数族$\\left\\{ \\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right) \\right\\}$を構成する。 したがって、$\\left( \\mathbb{S}^{2} , \\left\\{ \\mathbf{x}_{\\alpha} \\right\\} \\right)$は微分可能な多様体である。\nManfredo P. Do Carmo, Riemannian Geometry (英語版, 1992), p2-3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3116,"permalink":"https://freshrimpsushi.github.io/jp/posts/3116/","tags":null,"title":"微分可能な多様体"},{"categories":"수리통계학","contents":"要約 random sample $X_{1} , \\cdots , X_{n}$ がパラメータ $\\theta \\in \\Theta$ に対して同じ確率質量/密度関数$f \\left( x ; \\theta \\right)$ を持つとしよう。統計量 $Y = u_{1} \\left( X_{1} , \\cdots , X_{n} \\right)$ が $\\theta$ の 十分統計量 であるのは、以下を満たす非負の関数 $k_{1} , k_{2} \\ge 0$ が存在する場合である。 $$ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) = k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) $$ ただし、$k_{2}$ は $\\theta$ に依存してはならない。\n証明 十分統計量の定義：$\\theta \\in \\Theta$ に依存しない $H \\left( x_{1} , \\cdots , x_{n} \\right)$ に対して $$ {{ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) } \\over { f_{Y_{1}} \\left( u_{1} \\left( x_{1} , \\cdots, x_{n} \\right) ; \\theta \\right) }} = H \\left( x_{1} , \\cdots , x_{n} \\right) $$ これが真である場合、$Y_{1}$ を$\\theta$ のための 十分統計量Sufficient Statistic と呼ぶ。\n連続確率分布に対してのみ証明する。離散確率分布に対する証明はCasellaを参照されたい。\n$(\\Rightarrow)$\n十分統計量の定義から $f_{Y_{1}}$ は$k_{1}$ に、$H$ は $f_{2}$ に該当するので自明である。\n$(\\Leftarrow)$\n$$ \\begin{align*} y_{1} \u0026amp;:= u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) \\\\ y_{2} \u0026amp;:= u_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\\\ \u0026amp;\\vdots \\\\ y_{n} \u0026amp;:= u_{n} \\left( x_{1} , \\cdots , x_{n} \\right) \\end{align*} $$\n便宜上、上記の関数の逆関数を以下のように置き、ヤコビアンを $J$ と表す。\n$$ \\begin{align*} x_{1} \u0026amp;:= w_{1} \\left( y_{1} , \\cdots , y_{n} \\right) \\\\ x_{2} \u0026amp;:= w_{2} \\left( y_{1} , \\cdots , y_{n} \\right) \\\\ \u0026amp;\\vdots \\\\ x_{n} \u0026amp;:= w_{n} \\left( y_{1} , \\cdots , y_{n} \\right) \\end{align*} $$\nすると、$Y_{1} , \\cdots , Y_{n}$ の 結合確率密度関数 $g$ は $w_{i} = w_{i} \\left( y_{1} , \\cdots , y_{n} \\right)$ に対して $$ g \\left( y_{1} , \\cdots , y_{n} ; \\theta \\right) = k_{1} \\left( y_{1} ; \\theta \\right) k_{2} \\left( w_{1} , \\cdots , w_{n} \\right) \\left| J \\right| $$ であり、$Y_{1}$ の 周辺確率密度関数 $f_{Y_{1}}$ は $$ \\begin{align*} f_{Y_{1}} \\left( y_{1} ; \\theta \\right) =\u0026amp; \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} g \\left( y_{1} , \\dots , y_{n} ; \\theta \\right) d y_{2} \\cdots d y_{n} \\\\ =\u0026amp; k_{1} \\left( y_{1} ; \\theta \\right) \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} \\left| J \\right| k_{2} \\left( w_{1} , \\dots , w_{n} \\right) d y_{2} \\cdots d y_{n} \\end{align*} $$ $k_{2}$ は $\\theta$ に依存しない関数であり、$J$ も $\\theta$ を含まないため、右辺の積分は $y_{1}$ のみの関数として表すことができる。これを仮に $m \\left( y_{1} \\right)$ と表すと $$ f_{Y_{1}} \\left( y_{1} ; \\theta \\right) = k_{1} \\left( y_{1} ; \\theta \\right) m \\left( y_{1} \\right) $$ ここで $m \\left( y_{1} \\right) = 0$ であれば自明に $f_{Y_{1}} \\left( y_{1} ; \\theta \\right) = 0$ である。今、$m \\left( y_{1} \\right) \u0026gt; 0$ と仮定してみると、次のように書くことができる。 $$ k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] = {{ f_{Y_{1}} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] } \\over { m \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) \\right] }} $$ 与えられた式に代入すると $$ \\begin{align*} f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) =\u0026amp; k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\\\ =\u0026amp; {{ f_{Y_{1}} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] } \\over { m \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) \\right] }} k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\\\ =\u0026amp; f_{Y_{1}} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] {{ k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) } \\over { m \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) \\right] }} \\end{align*} $$ $k_{2}$ と $m$ はどちらも $\\theta$ に依存しないので、定義により$Y_{1}$ は $\\theta$ の十分統計量である。\n■\n","id":2084,"permalink":"https://freshrimpsushi.github.io/jp/posts/2084/","tags":null,"title":"ノイマン因数分解定理の証明"},{"categories":"복소해석","contents":"定義 1 $$ g(t) := p(t) + i q(t) \\qquad , t \\in [a,b] $$\n実関数 $p, q : [a,b] \\to \\mathbb{R}$ に対し、複素関数 $g : [a,b] \\to \\mathbb{C}$ が上記のように示されるとする。区間 $[a,b]$ から $g$ までの定積分は次のように定義される。 $$ \\int_{a}^{b} g(t) dt = \\int_{a}^{b} p(t) dt + i \\int_{a}^{b} q(t) dt $$ $t \\in [a,b]$ に対して、パス $\\mathscr{C} : z(t) = x(t) + i y(t)$ に沿った複素路線積分を次のように定義する。 $$ \\int_{\\mathscr{C}} f(z) dz = \\int_{a}^{b} f \\left( z(t) \\right) z\u0026rsquo;(t) dt $$\n説明 アークArc、あるいはカーブCurve $\\mathscr{C} : z(t)$ の定義は、幾何学では重要かもしれないが、複素解析自体ではそこまで正確にする必要はないので、少し無視して進もう。以下の説明にこだわるよりは、微分幾何等でカーブをしっかり学ぶか、とりあえず目の前の $\\mathscr{C}$ については直感的な概念だけを受け入れて進んでも十分だ。\n$\\mathscr{C}$ に重なる部分がない場合、つまり次を満たす場合、シンプルSimple あるいは ジョルダンJordanという。 $$ z \\left( t_{1} \\right) = z \\left( t_{2} \\right) \\implies t_{1} = t_{2} \\qquad , \\forall t_{1}, t_{2} \\in [a,b] $$\nどこでも微分可能で、微分係数が $0$ でない場合、つまり次を満たす場合、スムースSmoothという。 $$ \\exists z\u0026rsquo;(t) \\ne 0 \\qquad , \\forall t \\in [a,b] $$\n有限個の（シンプルな）スムースなアークの端と端を繋いだものを（シンプルな）コントゥアContourという。コントゥアを翻訳すると等高線となるが、意味がうまく伝わらないこともあり、複素解析の文脈ではほとんどがコントゥアを反時計回りAnticlockwiseに沿って積分する場合に使われるため、$\\mathscr{C}$ をパスとして簡化することもできる。\n$a \\to b$ の方向に進む場合は $\\mathscr{C}$ ならば、$b \\to a$ の方向に進む場合は $-\\mathscr{C}$ と表される。パラメーターによると、$\\mathscr{C} : z(t) , a \\le t \\le b$ の時は次のようになる。 $$ -\\mathscr{C} : z(-t) , -b \\le t \\le -a $$\n正確に両端の位置が同じだけなら、つまり $z(a) = z(b)$ は 閉じたコントゥアClosed Contourという。\nもう一度強調する。この説明が優れた数学者には気に入らないかもしれないが、スキップして進もう。以下の性質を直感的に受け入れられるかが遥かに重要だ。\n基本性質 $f,g$ が $\\mathscr{C}$ で区分的に連続であるとする。\n[1]: すべての $\\alpha , \\beta \\in \\mathbb{C}$ において $$ \\int_{\\mathscr{C}} \\left( \\alpha f(z) + \\beta g(z) \\right) dz = \\alpha \\int_{\\mathscr{C}} f(z) dz + \\beta \\int_{\\mathscr{C}} g(z) dz $$ [2]: $\\mathscr{C}$ の方向が $a \\to b \\to c$ であり、$a \\to b$ の方向の $\\mathscr{C}_{1}$ と $b \\to c$ の方向の $\\mathscr{C}_{2}$ から成る場合 $$ \\int_{\\mathscr{C}} f(z) dz = \\int_{\\mathscr{C}_{1}} f(z) dz + \\int_{\\mathscr{C}_{2}} f(z) dz $$ $$ \\int_{ - \\mathscr{C}} f(z) dz = - \\int_{\\mathscr{C}} f(z) dz $$ Osborne (1999). 複素変数 その応用: p69~71.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2082,"permalink":"https://freshrimpsushi.github.io/jp/posts/2082/","tags":null,"title":"複素関数の積分"},{"categories":"복소해석","contents":"定義 1 $\\alpha \\in \\mathbb{C}$が関数$f : \\mathbb{C} \\to \\mathbb{C}$の**$n$次のゼロ**Zero of Order $n$であることは、$\\displaystyle \\lim_{z \\to \\alpha} g(z) \\ne 0$のある関数$g$に対して、$f$が次のように表されることと同等である。 $$ f(z) = (z-\\alpha)^{n} g(z) $$\n定理 ゼロは孤立している：\nゼロの周りに他のゼロが存在しないような半径を取ることができる。 $f$のゼロ$\\alpha$には、$z \\in \\mathcal{N} (\\alpha) \\setminus \\left\\{ \\alpha \\right\\}$から$f (z) \\ne 0$の間の近傍$\\mathcal{N} (\\alpha)$が存在する。 証明 一般性を失わずに、$g$が$f$の$n$次のゼロ$\\alpha$で解析的であると仮定して$g(\\alpha) = 2 \\beta \\ne 0$とする。\n$g$が$\\alpha$で連続であるため、全ての$\\beta$に対して次を満たす$\\delta \u0026gt; 0$が存在しなければならない。 $$ | z - \\alpha | \u0026lt; \\delta \\implies \\left| g(z) - g(\\alpha) \\right| \u0026lt; |\\beta| $$ 先に$g(\\alpha) = 2 \\beta$としたので、三角不等式によって $$ | z - \\alpha | \u0026lt; \\delta \\implies |g(z)| \\ge \\left| |g(\\alpha)| - \\left| g(z) - g(\\alpha) \\right| \\right| \u0026gt; |\\beta| $$ $|z-\\alpha| \u0026lt; \\delta$から$|g(z)| \u0026gt; |\\beta|$となるので、$\\alpha$は$g$のゼロになることはできない。$f(z) = (z-\\alpha)^{n} g(z)$としたので、具体的にこのオープンボール$B \\left( \\alpha , \\delta \\right)$内では、$\\alpha$だけが$f$のゼロになる。 $$ f(z) \\begin{cases} = 0 \u0026amp; , \\text{if } z = \\alpha \\\\ \\ne 0 \u0026amp; , \\text{if } z \\in B \\left( \\alpha , \\delta \\right) \\setminus \\left\\{ \\alpha \\right\\} \\end{cases} $$\n■\n参考文献 抽象代数学でのゼロ Osborne (1999). Complex variables and their applications: p66.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2080,"permalink":"https://freshrimpsushi.github.io/jp/posts/2080/","tags":null,"title":"複素解析における零点"},{"categories":"줄리아","contents":"コード using CSV, DataFrames\rA = rand(1:10, 10)\rB = zeros(10)\rAB = DataFrame(hcat(A,B), [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;])\rCSV.write(\u0026#34;AB.csv\u0026#34;, AB) CSVパッケージのwrite関数を通じて簡単に2次元配列を出力できる。A, Bは1次元配列で、hcat関数で束ねてデータフレームに変換した。\n実行結果 julia\u0026gt; using CSV, DataFrames\rjulia\u0026gt; A = rand(1:10, 10)\r10-element Array{Int64,1}:\r8\r5\r4\r3\r6\r4\r10\r6\r2\r9\rjulia\u0026gt; B = zeros(10)\r10-element Array{Float64,1}:\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\rjulia\u0026gt; AB = DataFrame(hcat(A,B), [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;])\r10×2 DataFrame\rRow │ A B │ Float64 Float64\r─────┼──────────────────\r1 │ 8.0 0.0\r2 │ 5.0 0.0\r3 │ 4.0 0.0\r⋮ │ ⋮ ⋮\r9 │ 2.0 0.0\r10 │ 9.0 0.0\r5 rows omitted\rjulia\u0026gt; CSV.write(\u0026#34;AB.csv\u0026#34;, AB)\r\u0026#34;AB.csv\u0026#34; 以下は実際に出力されたcsvファイルだ。\n一緒に見る CSV 出力時に文字化け解決法 CSV.write(bom = true) 環境 OS: Windows julia: v1.6.3 ","id":2073,"permalink":"https://freshrimpsushi.github.io/jp/posts/2073/","tags":null,"title":"ジュリアで2次元配列をCSVファイルに出力する方法"},{"categories":"동역학","contents":"定義 1 空間$X$と時点$t \\in T$におけるオペレーター$\\varphi^{t}$をフローと呼ぶ。フローの集合$F := \\left\\{ \\varphi^{t} \\right\\}_{t \\in T}$が関数合成演算$\\circ$に対して$\\left( F , \\circ \\right)$を満たす場合、三つ組$\\left( T, X, \\varphi^{t} \\right)$を動力系と呼ぶ。 $$ \\begin{align*} \\varphi^{0} =\u0026amp; \\text{id} \\\\ \\varphi^{t+s} =\u0026amp; \\varphi^{t} \\circ \\varphi^{s} \\end{align*} $$\n解説 主に$T = \\mathbb{Z}$の場合はマップ、$T = \\mathbb{R}$の場合は微分方程式で表される。これは動力系がマップと微分方程式だけで定義されるわけではないことを意味する。\n動力系はある時点のステートが過去のステートで表されるシステムだとよく説明されるが、これは厳密ではないし、それほど直感的でもない。数学的な表現なしで概念を理解する場合は、マップで表される動力系や微分方程式で表される動力系の例を学ぶ方が良いし、数学的な表現が好きなら上の定義が気に入るはずだ。\n関連項目 マップで表される動力系 微分方程式で表される動力系 動力系の厳密な定義 Kuznetsov. (1998). Elements of Applied Bifurcation Theory(2nd Edition): p27.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2071,"permalink":"https://freshrimpsushi.github.io/jp/posts/2071/","tags":null,"title":"力学系の厳密な定義"},{"categories":"기하학","contents":"定義 正則曲線 $\\alpha (t)$ が与えられたとする。\nベクトル場 $\\displaystyle T(t) := {{ d \\alpha / d t } \\over { \\left| d \\alpha / d t \\right| }}$ を接線ベクトル場Tangent Vector Fieldという。 次のように定義された直線 $l$ を$t = t_{0}$ から$\\alpha$ の接線Tangent Lineという。 $$ l := \\left\\{ \\mathbf{w} \\in \\mathbb{R}^{3} : \\mathbf{w} = \\alpha \\left( t_{0} \\right) + \\lambda T \\left( t_{0} \\right) , \\lambda \\in \\mathbb{R} \\right\\} $$ 説明 接線ベクトル場は、微分幾何学で非常に重要なベクトル関数で、正則曲線の接線の方向を考えつつ、その大きさを$1$ に統一している。実際に曲線がどれだけ急に曲がっているかとは無関係に、方向だけを表している。\n","id":2066,"permalink":"https://freshrimpsushi.github.io/jp/posts/2066/","tags":null,"title":"接線とタンジェントベクトル場"},{"categories":"줄리아","contents":"概要 ジュリアでは、変数名にユニコード(UTF-8)を許可している。だから、ギリシャ文字はもちろん、上付き文字、下付き文字、さらには韓国語や絵文字まで使える。わざわざ使う必要はないが、次のような奇妙なコードもちゃんと動く。\njulia\u0026gt; α₁ = 2\r2\rjulia\u0026gt; α₂ = 1\r1\rjulia\u0026gt; println(α₁ \\ast\\ α₂)\r2\rjulia\u0026gt; 사인(t) = sin(t)\r사인 (generic function with 1 method)\rjulia\u0026gt; 😂 = 1:20\r1:20\rjulia\u0026gt; 사인.(😂)\r20-element Array{Float64,1}:\r0.8414709848078965\r0.9092974268256817\r⋮\r0.14987720966295234\r0.9129452507276277 ギリシャ文字 texで使うギリシャ文字を全部、上のような方法で使える。\n上付き文字、下付き文字 上付き文字と下付き文字は\\_、\\^の後に数字を入力して使う。あまりに小さすぎてよく使われないが、ギリシャ文字や英語、括弧も使える。\n絵文字 絵文字は他のエディタと同じようにwindow + .(cmd + コンマ)を押して入力できる。\n","id":2065,"permalink":"https://freshrimpsushi.github.io/jp/posts/2065/","tags":null,"title":"ジュリア変数名にグリーク文字と添え字を書く方法"},{"categories":"기하학","contents":"定義 1 写像 $\\alpha : (a,b) \\to \\mathbb{R}^{3}$ を曲線Curveと呼ぶ。 $\\alpha^{\\prime} = \\dfrac{d \\alpha}{d t} = \\mathbf{0}$ の時の点 $t = t_{0}$ を特異点Singular Pointと言う。 ある $k \\in \\mathbb{N}$ に対して、全ての $t \\in (a,b)$ で $\\displaystyle {{ d \\alpha } \\over { d t }} \\ne \\mathbf{0}$ となる曲線 $\\alpha \\in C^{k}$ を正則曲線Regular Curveと呼ぶ。つまり、正則曲線とは特異点がない曲線のことだ。 曲線 $\\alpha$の $t=t_{0}$ での微分係数 $\\alpha^{\\prime}(t_{0})$を$t = t_{0}$ の時の$\\alpha$ の速度(ベクトル)velocity vectorと呼び、$\\alpha$の導関数 $\\alpha^{\\prime}$を$\\alpha$の速度ベクトル場velocity vector fieldと言う。従って、正則曲線は速度が$\\mathbf{0}$にならない曲線のことを言い、物理的に見た時、進行方向が絶対に変わらないことを意味する。 $t = t_{0}$ の時の $\\alpha$ の速度の大きさ $\\left|\\alpha^{\\prime}(t_{0}) \\right|$を速さspeedと呼ぶ。 $\\left| \\alpha^{\\prime} \\right| = 1$ の曲線を$\\alpha : (a,b) \\to \\mathbb{R}^{3}$ を単位速度曲線Unit Speed Vectorと言う。 $C^{k}$ は $k$ 回微分可能でその導関数が連続関数の集合である。 説明 $$ \\alpha (t) := \\left( \\alpha_{1} (t) , \\alpha_{2} (t) , \\alpha_{3} (t) \\right) $$\n幾何学で扱いたい対象は図形で、定義ではその図形をパラメーター $t$ に対する関数として表していることに注意しろ。これにより、多くの数学的なツールを使って図形を研究することができるようになり、特に、微分幾何学では多くの微積分が使用されるだろう。\n特異点とは、簡単に言えば曲がっているか停止している点のことだ。曲がっている点では、見方によっては2つの方向が出てくることがある。このような点は扱いにくいので、学部レベルの微分幾何学では扱わない。\u0026lsquo;停止する点\u0026rsquo;は例で説明する。\nある $k \\in \\mathbb{N}$ に対して、$\\alpha \\in C^{k}$ ということは、少なくとも一度は微分可能であることを強く意味し、実際に何回微分可能かはあまり重要ではない。通常、$k=1$ 回だけであっても、単にスムースSmoothであると言われる。\n例 直線 $$ l(t) := \\left( t, t, t \\right) $$ 定義によれば、直線 $l : \\mathbb{R} \\to \\mathbb{R}^{3}$ が曲線でない理由は全くない。韓国語では、曲がることを意味する曲曲のため、何かが曲がっているというニュアンスが混乱を招くかもしれないが、そのまま英語の発音でカーブと読んだ方が良いかもしれない。\n螺旋 $$ \\zeta (t) := \\left( \\cos t , \\sin t , t \\right) $$\n$0 \\to t \\to \\infty$ によると、螺旋は以下のように描かれる。\n不規則曲線 $$ \\beta (t) := \\left( t^{2} , t^{3} , t^{4} \\right) $$ 上記の曲線 $\\beta$ を微分すると、 $$ \\beta^{\\prime} (t) := \\left( 2t , 3t^{2} , 4t^{3} \\right) $$ その結果、$t = 0$ で $\\displaystyle {{ d \\beta } \\over { d t }} (0) = \\mathbf{0}$ になる。この特異点は曲がっていないが$t$ を沿って進んでいると $t=0$ で文字通り停止する。従って、$\\beta$ の定義域が $\\mathbb{R}$ なら正則曲線ではない。もちろん、定義域が $0$ を含まない範囲、例えば、$(0,\\infty)$ なら正則曲線だ。定義域によって正則曲線であったり、そうでなかったりすることに注意しよう。\nコード 以下は、螺旋の例で見た動画をJuliaで作成するコードだ。\nusing Plots\rζ(t) = (cos(t), sin(t), t)\ranim = @animate for T ∈ 0.1:0.1:10\rt = 0:0.1:(T*π)\rhelix = plot(ζ.(t), camera = (45,45), legend = :none)\rxlims!(-2,2); ylims!(-2,2); zlims!(0,40)\rend\rgif(anim, \u0026#34;helix.gif\u0026#34;) Millman. (1977). Elements of Differential Geometry: p15.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2062,"permalink":"https://freshrimpsushi.github.io/jp/posts/2062/","tags":["줄리아"],"title":"曲線の定義"},{"categories":"수리통계학","contents":"定義 数式的な定義 1 パラメータ$\\theta \\in \\Theta$に対するランダムサンプル$X_{1} , \\cdots , X_{n}$の確率質量/密度関数を$f(x;\\theta)$、統計量$Y_{1} := u_{1} \\left( X_{1} , \\cdots , X_{n} \\right)$の確率質量/密度関数を$f_{Y_{1}} \\left( y_{1}; \\theta \\right)$とする。\n$\\theta \\in \\Theta$に依存しない$H \\left( x_{1} , \\cdots , x_{n} \\right)$に対して $$ {{ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) } \\over { f_{Y_{1}} \\left( u_{1} \\left( x_{1} , \\cdots, x_{n} \\right) ; \\theta \\right) }} = H \\left( x_{1} , \\cdots , x_{n} \\right) $$ であれば、$Y_{1}$を$\\theta$に対する十分統計量という。\n一般的な定義 2 統計量$T(\\mathbf{X})$が、与えられたサンプルの$\\mathbf{X}$条件付き確率分布がパラメータ$\\theta$に依存しない場合、$T(\\mathbf{X})$を$\\theta$に対する十分統計量という。\n説明 定義の数式が意味するのは、直感的に見ると、分子と分母で$\\theta$がキャンセルされること―つまり十分統計量$Y_{1}$が、ランダムサンプル$X_{1} , \\cdots , X_{n}$の情報を正確に保持しているという意味になるだろう。十分統計量の「十分」とは、$\\theta$に関する情報が「十分」に与えられていると受け取れば良く、十分統計量を除いた後は、$\\theta$に関する情報が全く残ってはいけない。\n十分統計量を理解するために、以下の定理を用いよう。\nネイマン分解定理: ランダムサンプル$X_{1} , \\cdots , X_{n}$が、パラメータ$\\theta \\in \\Theta$に対して同じ確率質量/密度関数$f \\left( x ; \\theta \\right)$を持つとする。統計量$Y = u_{1} \\left( X_{1} , \\cdots , X_{n} \\right)$が$\\theta$の十分統計量であるためには、次を満たす非負の二つの関数$k_{1} , k_{2} \\ge 0$が存在することである。 $$ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) = k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) $$ ただし、$k_{2}$は$\\theta$に依存してはならない。\n響かない例 $$ X_{1} , \\cdots , X_{n} \\sim N \\left( \\mu , \\sigma^{2} \\right) $$\n経験的に、十分統計量は、なぜそんなものを計算するのか、理解することから始める必要がある。典型的に響かない例として、正規分布$N \\left( \\mu , \\sigma^{2} \\right)$の母平均$\\mu$に対する十分統計量を見ることだ。分解定理によれば、$\\mu$の十分統計量は $$ \\begin{align*} \\prod_{k=1}^{n} f \\left( x_{k} ; \\mu \\right) =\u0026amp; \\prod_{k=1}^{n} {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\exp \\left( - {{ \\left( x_{i} - \\mu \\right)^{2} } \\over { 2 \\sigma^{2} }} \\right) \\\\ =\u0026amp; \\left( {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\right)^{n} \\exp \\left( - \\sum_{k=1}^{n} {{ \\left( x_{i} - \\mu \\right)^{2} } \\over { 2 \\sigma^{2} }} \\right) \\\\ =\u0026amp; \\left( {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\right)^{n} \\exp \\left( - \\sum_{k=1}^{n} {{ x_{i}^{2} } \\over { 2 \\sigma^{2} }} \\right) \\exp \\left( - \\sum_{k=1}^{n} {{ \\left( 2 x_{i} - \\mu^{2} \\right) } \\over { 2 \\sigma^{2} }} \\right) \\\\ =\u0026amp; \\left( {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\right)^{n} \\exp \\left( - \\sum_{k=1}^{n} {{ x_{i}^{2} } \\over { 2 \\sigma^{2} }} \\right) \\cdot \\exp \\left( - {{ 1 } \\over { \\sigma^{2} }} \\sum_{k=1}^{n} x_{i} + {{ n(\\mu/\\sigma)^{2} } \\over { 2 \\ }} \\right) \\\\ =\u0026amp; k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\cdot k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\mu \\right] \\end{align*} $$ であり、サンプル和$\\sum_{k=1}^{n} X_{k}$であろうと、分子分母に$n$を掛けてサンプル平均$\\overline{X}$になろうと、関係ない。直感に従って、$\\mu$の十分統計量として、その不偏推定量であり、一致推定量であり、最尤推定量でもあるサンプル平均が出てきたのは良い。数式的には理解できる。しかし、それが一体何を意味するのか、感じることは難しいだろう。\n響く例 $$ X_{1} , \\cdots , X_{n} \\sim U (0,\\theta) \\text{ with } f \\left( x ; \\theta \\right) = \\begin{cases} 1 \u0026amp; , \\text{if } x \\in (0,\\theta) \\\\ 0 \u0026amp; , \\text{otherwise} \\end{cases} = {{ 1 } \\over { \\theta }} I_{(0,\\theta)} (x) $$\n例えば、最大値のパラメータが$\\theta$である一様分布から得られたランダムサンプルを考えてみよう。実際の実現が $$ \\begin{bmatrix}2.3 \\\\ 1.2 \\\\ 1.7 \\\\ 0.1 \\\\ 1.1\\end{bmatrix} $$ であり、これ以上のサンプルを得られない場合、一様分布$U(a,b)$の母平均が${{ b+a } \\over { 2 }}$であるため、次のような推定量を考えることができる。 $$ {{ \\hat{\\theta} + 0 } \\over { 2 }} = {{ \\sum_{k} x_{k} } \\over { n }} \\implies \\hat{\\theta} \\overset{?}{=} {{ 2 \\sum_{k} x_{k} } \\over { n }} $$ 数理統計学的にそんなに悪くない推測のようだ。実際、このデータで計算されたサンプル平均の$2$倍は$2.16$で、かなりもっともらしい。しかし、$2.3$がサンプルにあることを考えると、$\\theta = 2.16$であるはずがない。どう考えても、$\\theta$は$2.3$以上でなければならず、直感的に見て、$\\theta$に対する合理的な推定は、単純に$\\hat{\\theta} = 2.3$になる。今のサンプルを見たとき、$2.3$より大きいと考える理由が全くないからだ。さあ、実際に十分統計量を探してみよう。\n指示関数の積: $$ \\prod_{i=1}^{n} I_{(-\\infty, \\theta]} \\left( x_{i} \\right) = I_{(-\\infty, \\theta]} \\left( \\max_{i \\in [n]} x_{i} \\right) $$\nこの補題と分解定理により考えると、$\\theta$に対する十分統計量は $$ \\begin{align*} \\prod_{k=1}^{n} f \\left( x_{k} ; \\mu \\right) =\u0026amp; \\prod_{k=1}^{n} {{ 1 } \\over { \\theta }} I_{(0,\\theta)} \\left( x_{k} \\right) \\\\ = \u0026amp; {{ 1 } \\over { \\theta^{n} }} I_{(0,\\theta)} \\left( \\max x_{k} \\right) \\cdot 1 \\\\ = \u0026amp; k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\end{align*} $$ であるため、サンプルの最大値$\\max_{k} X_{k} = X_{(n)}$が十分となる。これが意味するのは、$\\theta$に関する情報を考えるとき、他のサンプルは必要なく、$\\max_{k} X_{k}$だけを考えれば「十分」であるということだ。\nこのアイデアは、データをたくさん引き出してパラメータを推定し、それをどこかに近似する考え方とは全く異なる。これは、直感的な推測に対して、数学と形式でアプローチする統計的推論であり、これを通じて、統計学のさらに深い世界に入れる。\n最小十分統計量 響く例で、$\\max_{k} X_{k}$が$\\theta$に対する十分統計量であることを直感と照らし合わせて確認した。これ以上の十分統計量はないと見えるが、最小十分統計量に関する議論がその答えとなるだろう。\nHogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p391.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCasella. (2001). Statistical Inference(2nd Edition): p272.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2061,"permalink":"https://freshrimpsushi.github.io/jp/posts/2061/","tags":null,"title":"十分統計量"},{"categories":"함수","contents":"定義 1 $n \\in \\mathbb{N}_{0}$ と $\\left\\{ a_{k} \\right\\}_{k=0}^{n} \\subset \\mathbb{C}$ に対して、次のように定義される $P: \\mathbb{C} \\to \\mathbb{C}$ を**$n$次の多項式関数**Polynomial of degree $n$という。 $$ P(z) := a_{0} + a_{1} z + \\cdots a_{n} z^{n} \\qquad , a_{n} \\ne 0 $$\n説明 多項式関数は、数学全般で最も基本的に考えられる関数であり、代数学の基本定理によって根がちょうど $n$ 個存在することが明らかにされている。\n定義により、定数関数も多項式関数である。 多項式は無限回微分可能である。 連続関数である。 抽象代数 抽象代数の記法で、このような多項式関数の集合は $\\mathbb{C}[x]$ と表される。ここで、係数の集合は必ずしも複素数体 $\\mathbb{C}$ に限定されず、体 $F$ が与えられていれば $F [ x ]$ のように表すことができる。\n多項式の次数は無限大でも構わない。$n = \\infty$ の場合、そのような多項式の集合は $F[[x]]$ のように表される。\nOsborne (1999). Complex variables and their applications: p24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2058,"permalink":"https://freshrimpsushi.github.io/jp/posts/2058/","tags":null,"title":"多項関数"},{"categories":"함수","contents":"概要 三角関数は直角三角形の底角に三角比を対応させた関数だ。\n定義 三角関数のサイン、コサイン$\\sin, \\cos : \\mathbb{R} \\to \\mathbb{R}$は以下のように定義される。\n$$ \\sin \\theta := {{ y } \\over { \\sqrt{x^{2} + y^{2}} }} \\\\ \\cos \\theta := {{ x } \\over { \\sqrt{x^{2} + y^{2}} }} $$\nこれにより、セカント、コセカント、タンジェント、コタンジェントは次のように定義される。\n$$ \\begin{align*} \\tan \\theta \u0026amp;:= {{ \\sin \\theta } \\over { \\cos \\theta }} \\qquad, \\cos \\theta \\ne 0 \\\\ \\cot \\theta \u0026amp;:= {{ \\cos \\theta } \\over { \\sin \\theta }} \\qquad, \\sin \\theta \\ne 0 \\\\ \\sec \\theta \u0026amp;:= {{ 1 } \\over { \\cos \\theta }} \\qquad, \\cos \\theta \\ne 0 \\\\ \\csc \\theta \u0026amp;:= {{ 1 } \\over { \\sin \\theta }} \\qquad, \\sin \\theta \\ne 0 \\end{align*} $$\n複素関数への拡張 1 三角関数のサイン、コサイン$\\sin, \\cos : \\mathbb{C} \\to \\mathbb{C}$は以下のように定義される。\n$$ \\sin z := {{ 1 } \\over { i2 }} \\left( e^{iz} - e^{-iz} \\right) \\\\ \\cos z := {{ 1 } \\over { 2 }} \\left( e^{iz} + e^{-iz} \\right) $$\n基本性質 [1] 三角関数は実数軸上で$2 \\pi$-周期関数だ。 [2] サイン関数は奇関数で、コサイン関数は偶関数だ。 参照 複素解析における三角関数と双曲線関数の関係 複素解析における三角関数と指数関数の関係 Osborne (1999). 複素変数及びその応用: p28.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2056,"permalink":"https://freshrimpsushi.github.io/jp/posts/2056/","tags":null,"title":"三角関数の定義"},{"categories":"다변수벡터해석","contents":"ビルドアップ1 一変数関数の導関数の定義を思い出そう。\n$$ \\lim \\limits_{h\\to 0} \\dfrac{f(x+h) - f(x)}{h} = f^{\\prime}(x) $$\n左辺の分子を$h$に対する線形関数で近似すると、次のようになる。\n$$ \\begin{equation} f(x+h) - f(x) = a h + r(h) \\label{1} \\end{equation} $$\nここで、$r(h)$を以下の条件を満たす残余remainder, 残差としよう。\n$$ \\lim \\limits_{h \\to 0} \\dfrac{r(h)}{h}=0 $$\nすると、$\\eqref{1}$の両辺を$h$で割り、$\\lim_{h\\to 0}$である極限を取ると、次のようになる。\n$$ \\lim \\limits_{h\\to 0} \\dfrac{f(x+h) - f(x)}{h} = \\lim \\limits_{h\\to 0} \\dfrac{ah+ r(h)}{h} = a + \\lim \\limits_{h\\to 0} \\dfrac{r(h)}{h} = a $$\nこの時、$a$は$h$に対する線形近似での1次項の係数であった。このセンスで、$a$を$f$の$x$での微分**\u0026lsquo;係数\u0026rsquo;**と呼ぶことにする。式を少し変形すると、$f$の$x$での微分係数は、$a$を満たす式であることがわかる。\n$$ \\lim \\limits_{h\\to 0} \\dfrac{f(x+h) - f(x) - ah}{h} = \\lim \\limits_{h\\to 0} \\dfrac{r(h)}{h} = 0 $$\nこれを基に多変数ベクトル関数の導関数を定義する。\n定義 $E\\subset \\mathbb{R}^{n}$を開集合、$\\mathbf{x}\\in E$とする。$\\mathbf{f} : E \\to \\mathbb{R}^{m}$に対して、次を満たす$\\mathbf{h} \\in \\mathbb{R}^{n}$に対する線形変換 $A\\in L(\\mathbb{R}^{n}, \\mathbb{R}^{m})$が存在する場合、$f$は$\\mathbf{x}$で微分可能とされる。また、$A$を$f$の全導関数total derivativeまたは単に導関数といい、$\\mathbf{f}^{\\prime}(\\mathbf{x})$で表記する。\n$$ \\begin{equation} \\lim \\limits_{|\\mathbf{h}| \\to 0} \\dfrac{| \\mathbf{f} ( \\mathbf{x} + \\mathbf{h}) - \\mathbf{f} (\\mathbf{x}) - A( \\mathbf{h} )|}{|\\mathbf{h}|} = 0 \\label{2} \\end{equation} $$\nもし$\\mathbf{f}$が$E$のすべての点で微分可能であれば、$\\mathbf{f}$は$E$で微分可能であるとされる。\n説明 全全は全体を意味し、偏導関数に対する言葉だ。全$\\check{}$関数ではなく、全$\\check{}$導関数である。\n注意すべき点は、$\\mathbf{f}^{\\prime}(\\mathbf{x})$は関数値ではなく、$\\mathbf{f}^{\\prime}(\\mathbf{x}) : E \\subset \\R^{n} \\to \\R^{m}$を満たす線形変換であることだ。したがって、$\\mathbf{f}^{\\prime}(\\mathbf{x}) = A$は次のように行列で表現できる。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x}) = A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} $$\nすると、$\\mathbf{f}$の全導関数$\\mathbf{f}^{\\prime}$は、$\\mathbf{x} \\in E \\subset \\R^{n}$が与えられるたびにある$m \\times n$行列$A$をマッピングする関数と見なすことができる。この行列は$\\mathbf{f}$の偏導関数から簡単に得ることができ、ヤコビ行列Jacobian matrix, ヤコビ行列とも呼ばれる。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x}) = \\begin{bmatrix} (D_{1}f_{1}) (\\mathbf{x}) \u0026amp; (D_{2}f_{1}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{1}) (\\mathbf{x}) \\\\ (D_{1}f_{2}) (\\mathbf{x}) \u0026amp; (D_{2}f_{2}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{2}) (\\mathbf{x}) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ (D_{1}f_{m}) (\\mathbf{x}) \u0026amp; (D_{2}f_{m}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{m}) (\\mathbf{x}) \\end{bmatrix} $$\n全導関数は有限次元上で定義された関数に対する微分の最終形であり、ここで$\\mathbf{f}$の定義域、値域をバナッハ空間に一般化したものをフレシェ導関数という。一変数関数のときに成り立った性質も自然と成り立つ。\n一意性 連鎖律 定理 一意性 $E, \\mathbf{x}, \\mathbf{f}$を定義での通りとする。$A_{1}, A_{2}$が$\\eqref{2}$を満たす場合、その二つの線形変換は等しい。\n$$ A_{1} = A_{2} $$\n証明 $B = A_{1} - A_{2}$とする。すると、三角不等式により、次が成り立つ。\n$$ \\begin{align*} | B( \\mathbf{h} ) | \u0026amp;= \\left| A_{1}(\\mathbf{h}) - A_{2}(\\mathbf{h}) \\right| \\\\ \u0026amp;= | A_{1}(\\mathbf{h}) - \\mathbf{f} (\\mathbf{x} + \\mathbf{h}) - \\mathbf{f} (\\mathbf{x}) + \\mathbf{f} (\\mathbf{x} + \\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{2}(\\mathbf{h}) | \\\\ \u0026amp;\\le | \\mathbf{f} (\\mathbf{x} + \\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{1}(\\mathbf{h}) | + | \\mathbf{f} (\\mathbf{x} + \\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{2}(\\mathbf{h}) | \\end{align*} $$\nすると、固定された$\\mathbf{h} \\ne \\mathbf{0}$に対して、以下の式が成り立つ。\n$$ \\lim _{t \\to 0} \\dfrac{ | B( t\\mathbf{h} ) |}{| t\\mathbf{h} |} \\le \\lim _{t \\to 0}\\dfrac{ | \\mathbf{f} (\\mathbf{x} + t\\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{1}(t\\mathbf{h}) |}{| t\\mathbf{h} |} + \\lim _{t \\to 0}\\dfrac{| \\mathbf{f} (\\mathbf{x} + t\\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{2}(t\\mathbf{h}) |}{| t\\mathbf{h} |}=0 $$\nしかし、$B$は線形変換なので、左辺は$t$に無関係であることがわかる。\n$$ \\lim _{t \\to 0} \\dfrac{ | tB( \\mathbf{h} ) |}{| t\\mathbf{h} |} = \\lim _{t \\to 0} \\dfrac{ | B( \\mathbf{h} ) |}{| \\mathbf{h} |} = \\dfrac{ | B( \\mathbf{h} ) |}{| \\mathbf{h} |} \\le 0 $$\n$\\mathbf{h} \\ne \\mathbf{0}$であるため、上記の式が成り立つには、$B=0$でなければならない。したがって、次を得る。\n$$ B=A_{1}-A_{2}=0 \\implies A_{1} = A_{2} $$\n■\n連鎖律 定義の通り、$E \\subset \\R^{n}$を開集合とし、$\\mathbf{f} : E \\to \\R^{m}$を$\\mathbf{x}_{0} \\in E$で微分可能な関数とする。$\\mathbf{g} : \\mathbf{f}(E) \\to \\R^{k}$を$\\mathbf{f}(\\mathbf{x}_{0}) \\in \\mathbf{f}(E)$で微分可能な関数とする。また、$\\mathbf{F} : E \\to \\R^{k}$を$\\mathbf{f}$と$\\mathbf{g}$の合成とする。\n$$ \\mathbf{F} (\\mathbf{x}) = \\mathbf{g} \\left( \\mathbf{f}(\\mathbf{x}) \\right) $$\nすると、$\\mathbf{F}$は$\\mathbf{x}_{0}$で微分可能であり、全導関数は以下の通りである。\n$$ \\mathbf{F}^{\\prime} (\\mathbf{x}_{0}) = \\mathbf{g}^{\\prime} \\left( \\mathbf{f}(\\mathbf{x}_{0}) \\right) \\mathbf{f}^{\\prime} (\\mathbf{x}_{0}) $$\n証明 ノルム空間に対して一般化された証明\n■\nWalter Rudin, Mathematical Analysisの原理 (第3版, 1976), p211-213\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3082,"permalink":"https://freshrimpsushi.github.io/jp/posts/3082/","tags":null,"title":"偏導関数：多変数ベクトル関数の導関数"},{"categories":"선형대수","contents":"定義1 $V, W$を有限次元のベクトル空間とする。$\\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\right\\}$と$\\gamma = \\left\\{ \\mathbf{w}_{1}, \\dots, \\mathbf{w}_{m} \\right\\}$をそれぞれ$V$と$W$の順序基底とする。$T : V \\to W$を線形変換とする。すると、基底の表現の一意性によって、次を満たすスカラー$a_{ij}$が一意に存在する。\n$$ T(\\mathbf{v}_{j}) = \\sum_{i=1}^{m}a_{ij}\\mathbf{w}_{i} = a_{1j}\\mathbf{w}_{1} + \\cdots + a_{mj}\\mathbf{w}_{m} \\quad \\text{ for } 1 \\le j \\le n $$\nここで、$A_{ij} = a_{ij}$で定義される$m \\times n$行列$A$を順序基底$\\beta$と$\\gamma$に関する$T$の行列表現matrix representation for $T$ relative to the basis $\\beta$ and $\\gamma$と呼び、$[T]_{\\gamma, \\beta}$または$[T]_{\\beta}^{\\gamma}$で表す。 したがって、次の式が成り立つ。\n$$ [T]_{\\gamma, \\beta} [\\mathbf{x}]_{\\beta} = [T(\\mathbf{x})]_{\\gamma} = [T]_{\\beta}^{\\gamma}[\\mathbf{x}]_{\\beta} $$\n直感的には、隣接する（または下付き文字で重複する）2つの$\\beta$を相殺し、$\\mathbf{x}$を$T$に代入したものと見ることができる。\nStephen H. Friedberg, Linear Algebra (第4版, 2002), 80-81ページ\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3078,"permalink":"https://freshrimpsushi.github.io/jp/posts/3078/","tags":null,"title":"線形変換の行列表現"},{"categories":"복소해석","contents":"定義 1 二次方程式$x^{2} +1 = 0$の解$x = \\sqrt{-1}$を、虚数Imaginary Numberというんだ。 ２つの実数$x,y \\in \\mathbb{R}$に対して、$z = x + iy$の形の数を、複素数Complex Numberと言って、$(x,y)$みたいに表示することもある。この時、$\\text{Re} (z) = x$と$\\text{Im} (z) = y$をそれぞれ$z$の実部Real Partと虚部Imaginary Partと言うんだ。 全ての複素数の集合を$\\mathbb{C}$で表すんだ。複素数$z_{1}, z_{2} \\in \\mathbb{C}$が等しいEqualってのは、実部と虚部がそれぞれ等しいってことだ。 $$ \\text{Re} z_{1} = \\text{Re} z_{2} \\\\ \\text{Im} z_{1} = \\text{Im} z_{2} $$ 複素数の大きさをモジュラスModulusって呼び、以下のように定義されるんだ。 $$ | z | := \\sqrt{x^{2} + y^{2}} $$ 説明 虚部$\\text{Im} z = y \\in \\mathbb{R}$には、虚数単位$i$が掛けられていないことに注意しよう。 物理学や工学では、$i$が電流を表すから、虚数単位を$j := \\sqrt{-1}$と表すこともあるんだ。 教科書では、複素数を表示するとき、一般に$1 + 2i$として$i$を数字の後に書くけど、数学に近い文献では、$1 + i2$のように$i$を数字の前に書く傾向が強くなる。これはもう$i$を文字として見なさず、他の数と同じように等価な数として扱いたいって意図があって、$i$を基準にして前が実部、後が虚部と区分されるから、実際に使ってみるとこの表示方法は実用的だと分かるんだ。 歴史 歴史的に見て、虚数は1545年、確率論の先駆者であるカルダノCardanoの著作アルス・マグナArs Magnaで初めて紹介されたんだ。数学界で完全に受け入れられたのは、19世紀頃になってからだった。ガウスGaussは$i$に想像上の数Imaginary Numberって現在の名前を付けて、代数学の基本定理の証明に使ったんだ。記号$i$自体は、オイラーEulerの1777年の回顧録で登場するんだ。\n複素平面 2 $$ \\mathbb{C} \\ni x + iy = (x,y) \\in \\mathbb{R}^{2} $$\n定義から推測できるように、複素数$\\mathbb{C}$の集合は、$2$次元平面$\\mathbb{R}^{2}$のように見ることができ、実際にも、そうと同じ議論を代数的に導出できるんだ。記号そのままで$x$は$x$軸、$y$は$y$軸を表してると見なされ、これからは実数軸、虚数軸を意味することになる。ピタゴラスの定理を考えた時、複素数の大きさモジュラスが$| z | := \\sqrt{x^{2} + y^{2}}$のように定義されることは、非常に合理的だ。\n体の公理 $$ \\begin{align*} z_{1} + z_{2} =\u0026amp; \\left( \\text{Re} z_{1} + \\text{Re} z_{2} , \\text{Im} z_{1} + \\text{Im} z_{2} \\right) \\\\ z_{1} \\cdot z_{2} =\u0026amp; \\left( \\text{Re} z_{1} \\text{Re} z_{2} - \\text{Im} z_{1} \\text{Im} z_{2} , \\text{Re} z_{1} \\text{Im} z_{2} - \\text{Im} z_{1} \\text{Re} z_{2} \\right) \\end{align*} $$\n複素数$z_{1}, z_{2} \\in \\mathbb{C}$に対する二項演算である加算Sum$+: \\mathbb{C}^{2} \\to \\mathbb{C}$と乗算Product$\\cdot: \\mathbb{C}^{2} \\to \\mathbb{C}$を上記のように定義すると、$\\mathbb{C}$は代数的に体になり、$\\mathbb{C}$を複素数体Complex Fieldって呼ぶんだ。解析学序論の実数体と同じように、体の公理が全て成立するんだ。\nOsborne (1999). 『Complex Variables and Their Applications』p1~4よ。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOsborne (1999). 『Complex Variables and Their Applications』p8~9よ。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2046,"permalink":"https://freshrimpsushi.github.io/jp/posts/2046/","tags":null,"title":"複素数の定義"},{"categories":"머신러닝","contents":"この文は逆転派アルゴリズムの原理を数学専攻者が理解しやすいように作成された。\n表記法 上図のような 人工ニューラルネットワーク が与えられたとする。$\\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n_{0}})$は入力input、 $y_{j}^{l}$は$l$番目の層の$j$ノード、$\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}})$ドルは出力outputである。\n$L \\in \\mathbb{N}$は、隠匿層hidden layerの個数であり、$\\mathbf{n}=(n_{0}、n_{1}、\\dots、n_{L}、\\hat{n}) \\in \\mathbb{N}^{N}=(n)、$の成分は順に入力層、$L$個の隠匿層と出力層のノード数を意味する。 また、便宜上、$0$番目の隠匿層は入力層を意味し、$L+1$番目の隠匿層は出力層を意味するとする。\n$w_{ji}^{l}$は、$l$の次の層の$i$のノードとその次の層の$j$のノードを連結する加重値を表す。 すると、各階から次の階への伝播は、以下のGIFのように起こる。\nここで $\\phi$ は任意の活性化関数 である。 $l$ 番目の層から次の層の $j$ 番目のノードに伝達される線形結合を $v_{i}^{l}$で表記しよう。\n$$ \\begin{align*} v_{j}^{l} \u0026amp;= \\sum _{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\\\ y_{j}^{l+1} \u0026amp;= \\phi ( v_{j}^{l} ) = \\phi \\left( \\sum \\nolimits_{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\right) \\end{align*} $$\nこれを定理すると次のようになる。\n記号 意味 $\\mathbf{x}=(x_{1}, x_{2}, \\dots, x_{n_{0}})$ 入力 $y^{l}_{j}$ $l$ 番目の層の $j$ 番目のノード $\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}} )$ 出力 $n_{l}$ $l$ 番目の層のノード数 $w_{ji}^{l}$ $l$ 番目の層の $i$ 番目のノードと その次の層の $j$ 番目のノードを接続する重み付け $\\phi$ 活性化関数 $v_{j}^{l} = \\sum \\limits _{i=1} ^{n_{l}} w_{ji}^{l}y_{i}^{l}$ 線形結合 $y^{l+1}_{j} = \\phi (v_{j}^{l})$ $l$ 番目の階から次の階への 電波 定理 $E = E(\\hat{\\mathbf{y}})$を微分可能な適切な損失関数とする。 それでは、$E$を最適化する方法は、各層での加重値$w_{ji}^{l}$を次のようにアップデートするものである。\n$$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} \\label{thm} \\end{equation} $$\nこの時、$\\alpha$は学習率で、$\\delta_{j}^{l}$ は以下の通りである。\n$l=L$の時、\n$$ -\\delta_{j}^{L} = \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\n$l \\in \\left\\{ 0,\\dots, L-1 \\right\\}$の時、\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i=1}^{n_{l}} \\delta_{i}^{l+1} w_{i j}^{l+1} $$\n説明 $(1)$を見てみよう。 $l$番目の層と$l+1$番目の層の間の加重値を更新する時、$l$番目のノードの$y_{j}^{l}$に依存するということですが、各層の出力に応じて最終的に出力$\\hat{\\mathbf{y}}$が決定されるので当然と見ることができる。 また、$y_{j}^{l}$は$l$番目から$l+1$番目の層に伝播される時の入力と見ることができるが、これは線形回帰モデルでLMSLeast Mean Squaresで学習する方法と似ている。\n$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha (\\mathbf{w}^{T}\\mathbf{x} - \\mathbf{y}) \\mathbf{x} $$\n一方、各層での出力$y_{j}^{l}$は入力層から出力層として計算される反面、最適化のための$\\delta_{j}^{l}$ は次のように出力層から入力層に逆に計算されるため、このような最適化手法を逆伝播アルゴリズムback propagation algorithmという。\n$$ \\begin{align*} \\delta_{j}^{L} \u0026amp;= - \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} \\\\ \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{j}^{L} w_{ij}^{L} \\\\ \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\\\ \\delta_{j}^{L-3} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-3}) \\sum _{i} \\delta_{i}^{L-2} w_{ij}^{L-2} \\\\ \u0026amp;\\vdots \\\\ \\delta_{j}^{1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{1}) \\sum _{i} \\delta_{i}^{2} w_{ij}^{2} \\\\ \\delta_{j}^{0} \u0026amp;= \\phi ^{\\prime} (v_{j}^{0}) \\sum _{i} \\delta_{i}^{1} w_{ij}^{1} \\end{align*} $$\n証明 入力層から出力層への計算が終わったとする。 加重値を損失関数$E$が減る方向に修正する方法は傾斜下降法を使えば次のようになる。\n$$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} - \\alpha \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l} } \\label{gradesent} \\end{equation} $$\nそれぞれの$y_{i}^{l}$は与えられた値なので、偏微分部分を計算できる形で解くことができる。 右辺の偏微分は連鎖法則によって次のようになる。\n$$ \\begin{equation} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}}) }{\\partial v_{j}^{l}} \\dfrac{\\partial v_{j}^{l}}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial v_{j}^{l}} y_{i}^{l} \\label{chainrule} \\end{equation} $$\n$(3)$の右辺の偏微分を$-\\delta_{j}^{l}$ とすると、$(2)$ から $(1)$ を得る。\n$$ w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} $$\n各層で $\\delta_{j}^{l}$ を次のように求める。\n$l=L$の場合\n$j \\in \\left\\{ 1, \\dots, \\hat{n} \\right\\}$ に対して次が成立する。\n$$ \\begin{equation} -\\delta_{j}^{L} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial \\hat{y}_{j}} \\dfrac{d \\hat{y}_{j}}{d v_{j}^{L}} \\label{deltamL} \\end{equation} $$\nこの時、$\\hat{y}_{j} =\\phi (v_{j}^{L})$ であるから次を得る。\n$$ -\\delta_{j}^{L} (t) =\\phi ^{\\prime} (v_{j}^{L}(t)) \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\n■\n$l=L-1$の場合\n$j \\in \\left\\{ 1, \\dots, n_{L-1} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{L-1} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-1}} = = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L}} \\dfrac{d y_{j}^{L}}{d v_{j}^{L-1}} $$\nこの時$y_{j}^{L} =\\phi (v_{j}^{L-1})$ であるので、次を得る。\n$$ -\\delta_{j}^{L-1} = = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\dfrac{\\partial y_{j}^{L}}{\\partial v_{j}^{L-1}} = = \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} $$\n右辺の偏微分は連鎖法則によって次のように計算される。\n$$ \\begin{align*} -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L}} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L}} \\end{align*} $$\nここで $(4)$ と ${\\color{green}v_{i}^{L}=\\sum_{j}w_{ij}^{L}y_{j}^{L}}$ により、次を得る。\n$$ \\begin{align} \u0026amp;\u0026amp; -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i=1} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial v_{i}^{L}}} {\\color{green} \\dfrac{d v_{i}^{L}}{d y_{j^{L}}} } \\nonumber \\\\ \u0026amp;\u0026amp; \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{green} w_{ij}^{L} }\\nonumber \\\\ {}\\nonumber \\\\ \\implies \u0026amp;\u0026amp; \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ij}^{L} \\label{deltajL-1} \\end{align} $$\n■\n$l=L-2$の場合\n$j \\in \\left\\{ 1, \\dots, n_{L-2} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-2}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} $$\nこの時$y_{j}^{L-1} =\\phi (v_{j}^{L-2})$ であるから次を得る。\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} = \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} $$\n右辺の偏微分は連鎖法則によって次のように計算される。\n$$ \\begin{align*} -\\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{\\partial y_{k}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}} \\dfrac{\\partial v_{k}^{L-1}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}}} {\\color{red}\\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} } {\\color{green}\\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}}} {\\color{purple}\\dfrac{d v_{k}^{L-1}}{\\partial y_{j}^{L-1}}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{red} w_{ik}^{L}} {\\color{green} \\phi^{\\prime}(v_{k}^{L-1})} {\\color{purple} w_{kj}^{L-1}} \\end{align*} $$\nしたがって、次を得る。\n$$ \\delta_{j}^{L-2} = -\\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) w_{kj}^{L-1} $$\nこのとき、$(5)$ によって次が成立する。\n$$ \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) = \\phi^{\\prime}(v_{k}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} = \\delta_{k}^{L-1} $$\nしたがって、次を得る。\n$$ \\begin{align*} \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\delta_{k}^{L-1} w_{kj}^{L-1} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\end{align*} $$\n■\n一般化: $l \\in \\left\\{1, \\dots, L-1 \\right\\}$\n上記の結果に基づき、次のように一般化することができる。$j \\in \\left\\{ 1, \\dots, n_{l} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} $$\n右辺の偏微分を連鎖法則で解くと次のようになる。\n$$ \\begin{align*} \u0026amp;\\quad \\delta_{j}^{l} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{\\partial \\hat{y}_{i_{(1)}}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{\\partial y_{i_{(2)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{\\partial y_{i_{(3)}}^{L-1} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{j}^{l}} \\\\ \u0026amp; \\quad \\vdots \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{i_{(4)}}^{L-2}} \\cdots \\frac{d y_{i_{(L-l+1)}}^{l+1} }{d v_{i_{(L-l+1)}}^{l} } \\frac{\\partial v_{i_{(L-l+1)}}^{l} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} -\\delta_{i_{(1)}}^{L} w_{i_{(1)}i_{(2)}}^{L} \\phi^{\\prime}(v_{i_{(2)}}^{L-1}) w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\delta_{i_{(2)}}^{L-1}w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\delta_{i_{(3)}}^{L-2} w_{i_{(3)} i_{(4)}}^{L-2} \\cdots w_{i_{(L-l)} j}^{L} \\\\ \u0026amp;\\quad \\vdots \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\delta_{i_{(L-l)}}^{l+1} w_{i_{(l-l)} j}^{l} \\end{align*} $$\nしたがって、定理すると次のようになる。\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i} \\delta_{i}^{l+1} w_{ij}^{l+1} $$\n■\n","id":3077,"permalink":"https://freshrimpsushi.github.io/jp/posts/3077/","tags":null,"title":"逆伝播アルゴリズム"},{"categories":"기하학","contents":"定義 1 ベクトル空間 $X$ が与えられているとする。\n次の方程式を満たす点の集まり $L \\subset X$ 又は $\\alpha (t)$ 自体を点 $\\mathbf{x}_{0} \\in X$ を通り、ベクトル $\\mathbf{v} \\ne 0$ と平行な直線と定義する。 $$ \\alpha (t) = \\mathbf{x}_{0} + t \\mathbf{v} \\qquad , t \\in \\mathbb{R} $$ 次の方程式を満たす点の集まり $P \\subset X$ を点 $\\mathbf{x}_{0} \\in X$ を通り、ベクトル $\\mathbf{n} \\ne 0$ に垂直な平面と定義する。 $$ \\left\u0026lt; \\mathbf{x} - \\mathbf{x}_{0} , \\mathbf{n} \\right\u0026gt; = \\mathbf{0} $$ 次の方程式を満たす点の集まり $S \\subset X$ を中心 $\\mathbf{x}_{0} \\in X$ 、半径 $r \u0026gt; 0$ の球体と定義する。 $$ \\left\u0026lt; \\mathbf{x} - \\mathbf{x}_{0} , \\mathbf{x} - \\mathbf{x}_{0} \\right\u0026gt; = r^{2} $$ $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ は内積だ。 線であり、平面であり、球体であるもの マジで笑 Millman. (1977). Elements of Differential Geometry: p8~10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2042,"permalink":"https://freshrimpsushi.github.io/jp/posts/2042/","tags":null,"title":"一般的な直線、平面、球の定義"},{"categories":"줄리아","contents":"概要 Juliaの便利な機能である補間Interpolationについて説明する。補間をうまく使うと、出力文を簡単できれいに書くことができるので非常に便利だ。数値解析の補間法とは関係ないが、言葉の意味は通じる。数値解析の補間法に関連する機能はInterpolations.jlの使用法を参照してほしい。\nコード 使い方は非常に単純だ。以下のように文字列の中で変数の前にドル記号$를 붙이면 변수가 알아서 문자열처럼 읽힌다. 변수 그대로가 아닌 계산이 필요하면 굳이 밖에서 계산할 필요 없이 $()を書くだけでいい。\njulia\u0026gt; x = 12\r12\rjulia\u0026gt; y = -2\r-2\rjulia\u0026gt; println(\u0026#34;value of x, y: ▷eq2◁y\u0026#34;)\rvalue of x, y: 12, -2\rjulia\u0026gt; println(\u0026#34;value of x+y: $(x+y)\u0026#34;)\rvalue of x+y: 10 環境 OS: Windows julia: v1.5.0 ","id":2041,"permalink":"https://freshrimpsushi.github.io/jp/posts/2041/","tags":null,"title":"ジュリアで変数の値を便利に出力する方法、補間"},{"categories":"선형대수","contents":"定義1 $T : V \\to W$を線形変換とする。$T$が$\\mathbf{0}$にマッピングする$V$の要素の集合をカーネルまたは零空間と言い、以下のように表記する。\n$$ \\text{ker}(T) = N(T) := \\left\\{ \\mathbf{v} \\in V : T( \\mathbf{v} ) = \\mathbf{0} \\right\\} $$\nすべての$\\mathbf{v} \\in V$の$T$による像の集合を$T$の値域またはイメージと言い、以下のように表記する。\n$$ R(T) := \\left\\{ T(\\mathbf{v}) : \\forall \\mathbf{v} \\in V \\right\\} $$\n説明 $T : V \\to W$が線形変換で、$V, W$が有限次元であれば、$T$は実質的に行列と同じで、$N(T)$は$T$を表す行列の零空間である。\n定理 $T : V \\to W$を線形変換とする。その場合、\n(a) $T$のカーネルは$V$の部分空間である。 (b) $T$の値域は$W$の部分空間である。 証明 部分空間であることを示すためには、空集合ではなく、加法とスカラー倍で閉じていることを示せばよい。\n(a) $T$が線形変換であれば、$T(\\mathbf{0})=\\mathbf{0}$によって$N(T)$は空集合ではない。今、$\\mathbf{v}_{1}, \\mathbf{v}_{2} \\in N(T)$であり、$k$を任意のスカラーとする。すると、以下が成立する。\n$$ \\begin{align*} T( \\mathbf{v}_{1} + \\mathbf{v}_{2} ) \u0026amp;= T(\\mathbf{v}_{1}) + T(\\mathbf{v}_{2}) = \\mathbf{0} + \\mathbf{0} = \\mathbf{0} \\\\ T( k\\mathbf{v}_{1}) \u0026amp;= kT(\\mathbf{v}_{1}) = k\\mathbf{0} = \\mathbf{0} \\end{align*} $$\nしたがって、$N(T)$は$V$の部分空間である。\n■\n(b) $T$が線形変換であれば、$T(\\mathbf{0})=\\mathbf{0}$によって$R(T)$は空集合ではない。今、$\\mathbf{w}_{1}, \\mathbf{w}_{2} \\in R(T)$であり、$k$を任意のスカラーとする。すると、以下を満たす$\\mathbf{a}, \\mathbf{b} \\in V$が存在することを示せば十分である。\n$$ T(\\mathbf{a}) = \\mathbf{w}_{1} + \\mathbf{w}_{2} \\quad \\text{and} \\quad T(\\mathbf{b}) = k\\mathbf{w}_{1} $$\nしかし、$\\mathbf{w}_{1}, \\mathbf{w}_{2} \\in R(T)$ということは、以下を満たす$\\mathbf{v}_{1}, \\mathbf{v}_{2} \\in V$が存在するという意味である。\n$$ T(\\mathbf{v}_{1}) = \\mathbf{w}_{1} \\quad \\text{and} \\quad T(\\mathbf{v}_{2}) = \\mathbf{w}_{2} $$\nしたがって、以下の式が成立する。\n$$ \\begin{align*} \\mathbf{w}_{1} + \\mathbf{w}_{2} \u0026amp;= T(\\mathbf{v}_{1}) + T(\\mathbf{v}_{2}) = T(\\mathbf{v}_{1} + \\mathbf{v}_{2}) = T(\\mathbf{a}) \\\\ k\\mathbf{w}_{1} \u0026amp;= kT(\\mathbf{v}_{1}) = T(k\\mathbf{v}_{1})= T(\\mathbf{b}) \\end{align*} $$\nしたがって、$R(T)$は$W$の部分空間である。\n■\nHoward Anton, Elementary Linear Algebra: アプリケーションバージョン (12版, 2019), p455-456\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3071,"permalink":"https://freshrimpsushi.github.io/jp/posts/3071/","tags":null,"title":"線形変換：カーネルと値域"},{"categories":"줄리아","contents":"ガイド ステップ0. julia 1.6 以上をインストール\nバージョン1.6からは、インストール過程で環境変数に追加できる。示されたオプションをチェックしてインストールすればいい。古いバージョンを使っている場合は、1.6以上にアップデートするか、以下の指示に従えばいい。\nステップ1. Juliaのインストールパスを確認\nJuliaのインストールパスを確認する。特にいじっていなければ、次のパスに保存されているはずだ。\nC:\\Users\\사용자명\\AppData\\Local\\Programs\\Julia x.x.x\\bin 普通、C:\\Users\\ユーザー名\\AppDataは隠しフォルダだから見えなくても焦らないで。\n該当するパスで、上のようにjulia.exeファイルがあるか確認する必要がある。ステップ3で使うためにパスをコピーしておこう。\nステップ2. 環境変数を編集\nウィンドウ+sを押すか、コントロールパネルで「システム環境変数の編集」を検索する。\n「環境変数(N)」をクリックする。\nユーザー変数ウィンドウでPathを探し、「編集(E)」をクリックする。\nステップ3. Juliaのパスを追加\n「新規作成(N)」か、最下行を押してステップ1でコピーしたパスを上のように入力し、OKを押して環境変数の編集を終了する。\nステップ4. 再起動\n再起動後、powershellなどでjuliaコマンドを実行するとJuliaが起動することを確認できる。\n環境 OS: Windows julia: v1.5.2 ","id":2036,"permalink":"https://freshrimpsushi.github.io/jp/posts/2036/","tags":null,"title":"WindowsのCMDとPowerShellでJuliaを使用する方法"},{"categories":"수리통계학","contents":"ビルドアップ パラメータ$\\theta \\in \\Theta$に対して、確率密度関数が$f \\left( x ; \\theta \\right)$である確率変数$X$について考えよう。同じ確率密度関数$f(x ; \\theta)$と実現$\\mathbf{x} := \\left( x_{1} , \\cdots , x_{n} \\right)$を持っている、$X$と同じ分布からiidに抽出されたランダムサンプル$X_{1} , \\cdots , X_{n}$がある。これに対して定義された関数$L$を、尤度関数Likelihood Functionと言う。 $$ L ( \\theta ; \\mathbf{x} ) := \\prod_{k=1}^{n} f \\left( x_{k} ; \\theta \\right) $$ 以下で説明する通り、我々はこの関数の最大値に関心があるので、掛け算$\\prod$を足し算$\\sum$に変えて、ログを取った$l$として表す方が便利である。 $$ l ( \\theta ; \\mathbf{x} ) := \\sum_{k=1}^{n} \\log f \\left( x_{k} ; \\theta \\right) $$\n定義 1 以下を満たす推定量$\\hat{\\theta} := \\hat{\\theta} \\left( \\mathbf{X} \\right)$を、最尤推定量Maximum Likelihood Estimator、略してmleと呼ぶ。 $$ \\hat{\\theta} = \\argmax L \\left( \\theta ; \\mathbf{X} \\right) $$\n$\\mathbf{X}$はランダムベクター$\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) $である。 $\\argmax g$は関数$g$の最大引数で、$g$が最大になるような値である。 説明 直感 実際、尤度は英語表現で見る方がもっと直感的で、「ありそうな」を意味する。\n例えば、通りで偶然見かけたどんな男性3人の身長を計測したところ、169cm、171cm、182cmだったとしよう。そして、韓国男性の身長は正規分布$N \\left( \\mu , \\sigma^{2} \\right)$に従っていると仮定しよう。正規分布の確率密度関数$f (x; \\mu)$は平均$x = \\mu$で最大値を取るので、その関数値の積で定義される$L \\left( \\theta ; \\mathbf{x} \\right)$は$\\theta = \\mu$の時に最も大きな値を持つ可能性が高い。\nここで、関数$L$の主な引数はデータ$\\mathbf{x}$ではなく$\\theta$に注目しよう。つまり、$L$は、確率密度関数$f(x)$に入れる$x$が動きながら値が変わらないが、$f_{\\theta}$自体が$\\theta$によって左右に動きながら変わる関数だと想像するといい。\nまだ$L$の性質についてよくわかっていないから、$L$が最も大きくなる場所が$\\theta = 171$だと確信を持って言えないが、確実に$\\theta = 182$ではない。尤度という言葉や$\\argmax$が見慣れないかもしれないが、実際には最尤推定量とは「最もありそうな値」を指しているのだ。\n数式 もし$L$が微分可能であれば、最尤推定量は次の推定方程式Estimating Equation、すなわち偏微分方程式を満たす。 $$ {{ \\partial l ( \\theta ) } \\over { \\partial \\theta }} = 0 $$ これはカリキュラムで関数の最大値を求める際に微分を使った解法の延長に過ぎない。ただし、教科書でこの部分を見ると、特に統計学の学生は大学1年生以降に微分方程式を扱うことがほとんどないので、馴染みがなく怖く感じられるかもしれない。しかし実際には微分方程式を解く必要はなく、よく知らなくても大丈夫なので、あまり心配しないでほしい。\nHogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p209, 329.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2026,"permalink":"https://freshrimpsushi.github.io/jp/posts/2026/","tags":null,"title":"最尤推定量"},{"categories":"줄리아","contents":"## コード [^1] [^1]: https://docs.julialang.org/en/v1/manual/metaprogramming/ Juliaでは[メタプログラミング](../1457)を言語レベルでサポートしている。これは文字列をそのままのコードとして読み込んで実行した結果だ。 julia\u0026gt; text = \u0026ldquo;f(x) = 2x + 1; f(2)\u0026rdquo; \u0026ldquo;f(x) = 2x + 1; f(2)\u0026rdquo;\njulia\u0026gt; code = Meta.parse(text) :($(Expr(:toplevel, :(f(x) = begin #= none:1 =# 2x + 1 end), :(f(2)))))\njulia\u0026gt; eval(code) 5\n- `Meta.Parse()`: 이 함수를 통해 입력된 문자열을 **표현식**\u0026lt;sup\u0026gt;Expression\u0026lt;/sup\u0026gt;으로 바꿔 반환한다.\r- `eval()`: 표현식을 **평가**\u0026lt;sup\u0026gt;Evaluate\u0026lt;/sup\u0026gt;한다. 위 예제코드에서는 $f(2)$ 가 실제로 평가되어 함숫값인 $5) ## 環境 - OS: Windows - julia: v1.5.0 ","id":2024,"permalink":"https://freshrimpsushi.github.io/jp/posts/2024/","tags":null,"title":"ジュリアのメタプログラミング"},{"categories":"줄리아","contents":"コード vec() 関数を使えばいい。\njulia\u0026gt; A = rand(0:9, 3,4)\r3×4 Array{Int64,2}:\r6 8 7 3\r2 9 3 2\r5 0 6 7\rjulia\u0026gt; vec(A)\r12-element Array{Int64,1}:\r6\r2\r5\r8\r9\r0\r7\r3\r6\r3\r2\r7 人間の目には、1次元配列と同じように見えるが、タイプ上では2次元配列でエラーが出るケースも、この方法で解決できる。次の2つの命令は完全に同じ配列に見えるが、$\\mathbb{N}^{10 \\times 1}$ 行列か $\\mathbb{N}^{10 }$ ベクトルかの違いがある。\njulia\u0026gt; b = rand(0:9, 10,1)\r10×1 Array{Int64,2}:\r4\r8\r0\r4\r7\r4\r4\r2\r4\r7\rjulia\u0026gt; vec(b)\r10-element Array{Int64,1}:\r4\r8\r0\r4\r7\r4\r4\r2\r4\r7 実際の flatten() 関数 実は Base.Iterators に、本当の名前がフラッテンの flatten() が実装されている。実行結果は以下の通りで、正直、使いたくないかもしれない。正確に言うと、配列を変えるというよりは、ループなどに入る時にイテレータIteratorsとして使う時に必要になるかもしれない。正直、必要ない。\njulia\u0026gt; c = rand(0:9, 3,3)\r3×3 Matrix{Int64}:\r7 7 4\r9 3 8\r4 4 5\rjulia\u0026gt; Iterators.flatten(c)\rBase.Iterators.Flatten{Matrix{Int64}}([7 7 4; 9 3 8; 4 4 5])\rjulia\u0026gt; vec(c)\r9-element Vector{Int64}:\r7\r9\r4\r7\r3\r4\r4\r8\r5 環境 OS: Windows julia: v1.5.0 ","id":2022,"permalink":"https://freshrimpsushi.github.io/jp/posts/2022/","tags":null,"title":"ジュリアで配列をフラット化する方法"},{"categories":"줄리아","contents":"結論 $n$ 個の座標間の距離を計算しようとする。\n全ての座標間を計算する必要がなければ、グループに分けて長方形の距離行列を作ればいい。 長方形の距離行列は pairwise() 関数で簡単かつ速く計算できる。 速度比較 例えば、SIRモデルに対して移動するエージェントベースのシミュレーションを行うと考えてみよう。元の時間計算量は $O \\left( n^{2} \\right)$ だが、$S$ と $I$ のグループに分けて計算すると、時間計算量は $O \\left( n_{S} n_{I} \\right)$ に大きく下がる。通常、病気の伝播は $S$ と $I$ 間の距離行列を計算して一定の半径 $\\varepsilon$ 内に入るかを判断し、どれだけ接触したかによって実装される。この作業で速度を比較してみよう。\nusing Distances\rusing StatsBase\rusing Random\rRandom.seed!(0);\rN = 10^4\rlocation = rand(2, N);\rstate = sample([\u0026#39;S\u0026#39;, \u0026#39;I\u0026#39;], Weights([0.1, 0.9]), N);\rS = location[:, state .== \u0026#39;S\u0026#39;]\rI = location[:, state .== \u0026#39;I\u0026#39;]\rfunction foo(S, I)\rcontact = Int64[]\rfor s in 1:996\rpush!(contact, sum(sum((I .- S[:,s]).^2, dims = 1) .\u0026lt; 0.1))\rend\rreturn contact\rend\r@time foo(S, I) もちろん、グループに分けて計算するという発想は、ジュリアだけでなくどんな手法を使っても性能向上をもたらす。ポイントは、無理にループを回す必要がなく、Distance パッケージの pairwise() 関数を上手く使えばいいということだ。\njulia\u0026gt; @time foo(S, I);\r0.170835 seconds (7.98 k allocations: 210.854 MiB, 12.56% gc Time)\rjulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; 0.1, dims = 1);\r0.087875 seconds (14 allocations: 69.639 MiB, 4.15% gc Time) これら二つの命令は正確に同じ機能を持つが、速度面では約2倍の差があり、allocationに関しては計算したくもないほど大きな差が出るし、コーディングの難易度もループに比べてずっと簡単だ。\n追加研究 1 ユークリッド距離が距離関数である場合、Euclidean() の代わりに SqEuclidean() を使うと、ルートを取る計算を省略できるため、速度がさらに上がる。次は正確に同じ結果を出すが、速度面では約1.5倍の差が出る。\njulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; 0.1, dims = 1);\r0.091917 seconds (14 allocations: 69.639 MiB, 7.60% gc Time)\rjulia\u0026gt; @time sum(pairwise(SqEuclidean(),S,I) .\u0026lt; 0.01, dims = 1);\r0.061776 seconds (14 allocations: 69.639 MiB, 11.37% gc Time) さらに、もっと速くなることができる。ここでは、単純なコード最適化だけでは速度を上げるのが難しく、多次元検索に有利なデータ構造であるk-d木2を使用しなければならない。NearstNeighbors.jlで速く距離を計算する方法を参照。\n環境 OS: Windows julia: v1.5.0 https://github.com/JuliaStats/Distances.jl#pairwise-benchmark\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/K-d_tree\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2020,"permalink":"https://freshrimpsushi.github.io/jp/posts/2020/","tags":null,"title":"ジュリアで距離行列計算を最適化する方法"},{"categories":"줄리아","contents":"概要 Juliaで、Rのsample()やPythonパッケージnumpyのrandom.choice()と同じ役割をするsample()関数とWeights関数の使用方法です。\nコード 1 ■コード１■\n実行結果 ■コード２■\n環境 OS: Windows julia: v1.5.0 https://stackoverflow.com/a/27560273/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2018,"permalink":"https://freshrimpsushi.github.io/jp/posts/2018/","tags":null,"title":"ジュリアで重み付けとランダムサンプリングをする方法"},{"categories":"줄리아","contents":"結論 配列の各要素をEqualオペレータ==で比較すると、整数よりもCharの方が早い。\n速度比較 julia\u0026gt; integer = rand(1:5, N); print(typeof(integer))\rArray{Int64,1}\rjulia\u0026gt; character = rand([\u0026#39;S\u0026#39;,\u0026#39;E\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;,\u0026#39;D\u0026#39;], N); print(typeof(character))\rArray{Char,1}\rjulia\u0026gt; @time integer .== 1;\r0.009222 seconds (6 allocations: 1.196 MiB)\rjulia\u0026gt; @time character .== \u0026#39;S\u0026#39;;\r0.005266 seconds (7 allocations: 1.196 MiB) 上のコードは、整数と文字で構成された配列で1とSがどこにあるかを特定するプログラムだ。整数か文字列かの違いを除いて、全て正確に同じだが、時間の消費はほぼ二倍の大きな差がある。したがって、コード最適化の過程で一般的に使用される方法なので、可能な限り文字を使用することが推奨される。\n追加研究 julia\u0026gt; string = rand([\u0026#34;S\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;I\u0026#34;,\u0026#34;R\u0026#34;,\u0026#34;D\u0026#34;], N); print(typeof(string))\rArray{String,1}\rjulia\u0026gt; @time string .== \u0026#34;S\u0026#34;;\r0.072692 seconds (7 allocations: 1.196 MiB) 当然ながら、文字Characterではなく文字列Stringを使用すると、10倍以上の速度の低下が発生する。\n環境 OS: Windows julia: v1.5.0 ","id":2016,"permalink":"https://freshrimpsushi.github.io/jp/posts/2016/","tags":null,"title":"ジュリアでの文字と整数の等価オペレータ==の速度比較"},{"categories":"프로그래밍","contents":"概要 よく使われるRGB色の商標だ。\nコード ","id":2013,"permalink":"https://freshrimpsushi.github.io/jp/posts/2013/","tags":null,"title":"RGBカラーチートシート"},{"categories":"함수","contents":"定義 関数 $f: X \\to Y$, $g: f(X) \\to Z$について次のように定義される$h: X \\to Z$を**$f$と$g$の合成**composition of $g$ with $f$と呼び、$h=g \\circ f$と表記する。\n$$ h(x) = (g\\circ f) (x) := g\\left( f(x) \\right) $$\n","id":3048,"permalink":"https://freshrimpsushi.github.io/jp/posts/3048/","tags":null,"title":"関数の合成"},{"categories":"선형대수","contents":"定義1 関数 $T : V \\to W$がベクトル空間からベクトル空間への写像である場合、つまり $V$、$W$がベクトル空間である場合、$T$を変換transformationと呼ぶ。\n変換 $T$が線形関数である場合、すなわち全ての$\\mathbf{v},\\mathbf{u} \\in V$とスカラー$k$について次の二つの条件を満たす場合、線形変換linear transformationと呼ぶ。\n$T(k \\mathbf{u}) = k T(\\mathbf{u})$ $T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$ 特に$W=\\mathbb{C}$であれば、$T$を線形汎関数と呼ぶ。\n説明 関数、写像、変換は事実上同じ言葉として考えてもいい。ただし、線形代数、関数解析学などベクトル空間からベクトル空間への写像を扱う場合は、主に変換と呼び、transformationの頭文字を取って$T$と表記する。\n有限次元から有限次元への線形変換の場合は、行列の積と同様に扱うので、以下のように表記する。\n$$ T(\\mathbf{x}) = T\\mathbf{x} $$\n$T : V \\to V$を満たす線形変換を$V$上の線形作用素linear operator on $V$と呼ぶこともある。しかし、定義域と値域が同じでなければ作用素と呼ばれるわけではない。実用的な理由から、多くの教科書では$T : V \\to V$を線形作用素として定義している。\n$$ \\text{linear transformation form } V \\text{ to } V \\to \\text{linear operator on } V $$\nベクトル空間$X$から$Y$への全ての線形変換の集合は$L(X, Y)$のように表記する。2\n$$ L(X,Y) = \\mathcal{L}(X, Y) := \\left\\{ T : X \\to Y\\enspace |\\enspace T \\text{ is linear } \\right\\} $$\n行列変換は線形変換の一種である。\n恒等変換 線形変換$I : V \\to V$が全ての$\\mathbf{v} \\in V$に対して\n$$ I(\\mathbf{v}) = \\mathbf{v} $$\nを満たす場合、恒等変換identity transformationと呼ぶ。具体的には$I_{V}$のように表記することもある。\n零変換 線形変換$T_{0} : V \\to W$が全ての$\\mathbf{v} \\in V$に対して\n$$ T_{0}(\\mathbf{v}) = \\mathbf{0}_{W} $$\nを満たす場合、零変換zero transformationと呼ぶ。この時$\\mathbf{0}_{W}$は$W$の零ベクトルである。$O$、$0$などで表記することもある。簡単に言うと、零関数である。\n性質 $T : V \\to W$が線形変換であれば、以下が成立する。\n(a) $T(\\mathbf{0}) = \\mathbf{0}$\n(b) 全ての$\\mathbf{u}, \\mathbf{v} \\in V$に対して、$T(\\mathbf{u} - \\mathbf{v}) = T(\\mathbf{u}) - T(\\mathbf{v})$\n証明 (a) ベクトル空間の性質により、$0\\mathbf{v} = \\mathbf{0}$であるので、\n$$ T(\\mathbf{0}) = T( 0\\mathbf{u}) = 0T(\\mathbf{u}) = \\mathbf{0} $$\n■\n(b) 同様に、ベクトル空間の性質により$-\\mathbf{v} = (-1)\\mathbf{v}$であるので、\n$$ \\begin{align*} T(\\mathbf{u} - \\mathbf{v}) \u0026amp;= T \\big( \\mathbf{u} + (-1)\\mathbf{v} \\big) \\\\ \u0026amp;= T(\\mathbf{u}) + T\\big( (-1)\\mathbf{v} \\big) \\\\ \u0026amp;= T(\\mathbf{u}) + (-1)T(\\mathbf{v}) \\\\ \u0026amp;= T(\\mathbf{u}) - T(\\mathbf{v}) \\end{align*} $$\n■\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p446-447\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), p207\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3026,"permalink":"https://freshrimpsushi.github.io/jp/posts/3026/","tags":null,"title":"線形変換"},{"categories":"행렬대수","contents":"定義1 拡大行列が次の条件を満たす場合、行段階形echelon formと言います。\n$0$じゃない要素がある行で、最初に現れる$0$じゃない数が$1$である。これを先導1leading 1と呼びます。\n全ての要素が$0$の行は、最も下に置きます。\n$0$じゃない要素がある行が連続している場合、上の行の先導1が下の行の先導1より左にある必要があります。\n行段階形の行列がさらに下記の条件を満たす場合、簡約行段階形reduced echelon formと言います。\n先導1がある列の他の要素が全て$0$である。 次の行列は簡約行段階形です。\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 7 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; -1 \\end{bmatrix},\\quad \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix},\\quad \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; -2 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 3 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix},\\quad \\begin{bmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} $$\n次の行列は行段階形ですが、簡約行段階形ではありません。\n$$ \\begin{bmatrix} 1 \u0026amp; 4 \u0026amp; -3 \u0026amp; 7 \\\\ 0 \u0026amp; 1 \u0026amp; 6 \u0026amp; 2 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 5 \\end{bmatrix},\\quad \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix},\\quad \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 6 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\n与えられた線形システムの拡大行列に基本行操作を行い、簡約行段階形を作るプロセスをガウス・ジョルダン消去法Gauss-Jordan eliminationと呼びます。簡約行段階形を作る過程で、先導1の下部分を全て$0$にすることを前進forward、上部を全て$0$にすることを後退backwordと言います。\n性質 すべての行列は唯一の簡約行段階形を持ちます。つまり、どんな順番で基本行操作を行っても、同じ簡約行段階形の行列が得られます。\n行段階形は一意ではありません。つまり、基本行操作の順序によって異なる行段階形が得られます。\n行段階形の全ての要素が$0$の行の数は互いに同じであり、先導1の位置も互いに同じです。これらの位置をピボットpivot位置と呼びます。\n一般解2 線形システムが無数に多くの解を持つ場合、パラメーターを代入して解を得られるパラメーター方程式の集合を線形システムの一般解general solutionと言います。\n例えば、ある線形システムの拡大行列を基本行操作で次のような簡約行段階形に変形したとします。\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 3 \u0026amp; -1 \\\\ 0 \u0026amp; 1 \u0026amp; -4 \u0026amp; 2 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nその場合、パラメーター方程式は次のようになります。\n$$ x = -1 -3t,\\quad y = 2 + 4t, \\quad z = t $$\nこの時、先導1に対応する変数 $x,y$を先導変数leading variable、その他の変数 $z$を自由変数free variableと呼びます。\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p11\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p115\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3019,"permalink":"https://freshrimpsushi.github.io/jp/posts/3019/","tags":null,"title":"ガウス-ジョルダン消去法"},{"categories":"줄리아","contents":"エラー ERROR: SystemError: opening file \u0026quot;C:\\\\Users\\\\rmsms\\\\.julia\\\\registries\\\\General\\\\Registry.toml\u0026quot;: No such file or directory\r原因 人を本当にイライラさせるエラーだけど、言葉通りこのパスにRegistry.tomlファイルがなくて発生するエラーだ。\n解決法 C:\\Users\\사용자이름\\.julia\\registries\\General フォルダを削除してもう一度試してみる。\nその後、上のようにRegistry.tomlファイルも生まれて、インストールも正常に進行することを確認できる。\n","id":2069,"permalink":"https://freshrimpsushi.github.io/jp/posts/2069/","tags":null,"title":"Juliaパッケージのインストール時に\\General\\Registry.toml: No such file or directoryというエラーを解決"},{"categories":"선형대수","contents":"定義1 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$をベクトル空間$V$の部分集合としよう。$S$が下記の二条件を満たす時、$S$を$V$の基底basisという。\n$S$が$V$を生成する。\n$$ V = \\text{span}(S) $$\n$S$が線形独立である。\n説明 基底の名前から推測できるように、「ベクトル空間を作り出すことができる最も小さいもの」の概念に当たる。生成という条件が「ベクトル空間を作る」という意味を持ち、線形独立という条件が「最も小さい」という意味を持つ。ベクトル空間を作ることは分かるが、最も小さなものでなければならない理由については直ちに理解できないかもしれない。しかし、簡単な例を一つ見ればすぐに理解できるだろう。例えば、私たちは$(2,3)$というベクトルを\n$$ (2,3)=1(1,0) + 2(0,1) + 1(1,1) $$\nのように表さない。$(1,1)$を$(1,0), (0,1)$の線形結合で表すことができるからだ。つまり、上の式は不必要に長く記載した表現にすぎないということだ。従って、線形独立という条件はそのベクトルを基底の線形結合で表す時、最もすっきりと、必要なものだけをまとめた形で表されるようにしてくれる。\nここで注意すべきことは、一つのベクトル空間に対して基底が特に一意に存在する必要はないということだ。例を挙げると、$\\left\\{ (1,0) , (0,1) \\right\\}$は$\\mathbb{R}^{2}$を生成する基底だ。しかし、定義によれば$\\left\\{ (2,0) , (0,2) \\right\\}$も$\\mathbb{R}^2$の基底になることができる。それだけか？実は$\\left\\{ (1,1) , (-1,1) \\right\\}$も$\\mathbb{R}^2$を生成する上で全く問題がない。ただ、一般的に$\\mathbb{R}^{n}$では、下記のベクトルから成る基底を扱う。\n$$ \\mathbf{e}_{1} = (1,0,0,\\dots,0), \\quad \\mathbf{e}_{2}=(0,1,0,\\dots,0),\\quad \\mathbf{e}_{n}=(0,0,0,\\dots,1) $$\nこのような基底を$\\left\\{ \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\dots, \\mathbf{e}_{n} \\right\\}$を$\\mathbb{R}^{n}$上の標準基底standard basis for $\\mathbb{R}^{n}$という。各$\\mathbf{e}_{i}$は標準単位ベクトルstandard unit vectorと呼ばれる。特に$n=3$の場合は、一般的に以下のように表記される。\n$$ \\begin{align*} \\hat{\\mathbf{x}} =\u0026amp;\\ \\mathbf{e}_{1} = \\hat{\\mathbf{x}}_{1} = \\mathbf{i}=(1,0,0) \\\\ \\hat{\\mathbf{y}} =\u0026amp;\\ \\mathbf{e}_{2} = \\hat{\\mathbf{x}}_{2} = \\mathbf{j}=(0,1,0) \\\\ \\hat{\\mathbf{z}} =\u0026amp;\\ \\mathbf{e}_{3} = \\hat{\\mathbf{x}}_{3} = \\mathbf{k}=(0,0,1) \\end{align*} $$\n以下の定理から、座標の概念を抽象化されたベクトル空間でも話すことができる。$\\mathbf{v} \\in V$が$(1)$のように表される時、$[\\mathbf{v}]_{S}$を基底$S$に対する$\\mathbf{v}$の座標ベクトルcoordinate vector $\\mathbf{x}$ of relative of $S$という。\n$$ [\\mathbf{v}]_{S} = \\begin{bmatrix} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{bmatrix} $$\n定理: 基底表現の一意性 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{n} \\right\\}$をベクトル空間$V$の基底としよう。すると、全てのベクトル$\\mathbf{v} \\in V$に対して\n$$ \\begin{equation} \\mathbf{v} = c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{n}\\mathbf{v}_{n} \\end{equation} $$\nと表現する方法は一意である。つまり、上記の式を満たす係数の組$(c_{1},c_{2},\\dots,c_{n})$が一意に存在する。\n証明 $S$が$V$を生成するため、生成の定義に従い、$V$の全てのベクトルは$S$の線形結合で表せる。あるベクトル$\\mathbf{v}$が下記の二つの線形結合で表せるとしよう。\n$$ \\begin{align*} \\mathbf{v} \u0026amp;= c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{n}\\mathbf{v}_{n} \\\\ \\mathbf{v} \u0026amp;= k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + \\cdots + k_{n}\\mathbf{v}_{n} \\end{align*} $$\n上記の式から下の式を引くと、次のようになる。\n$$ \\mathbf{0} = (c_{1} - k_{1}) \\mathbf{v}_{1} + (c_{2} - k_{2}) \\mathbf{v}_{2} + \\cdots + (c_{n} - k_{n}) \\mathbf{v}_{n} $$\nしかし$\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{n}$は線形独立であるため、上記の式を満たす解はオロジ\n$$ c_{1} - k_{1} = 0,\\quad c_{2} - k_{2} = 0,\\quad \\dots,\\quad c_{n} - k_{n} = 0 $$\nのみである。従って、次が成り立つ。\n$$ c_{1} = k_{1},\\quad c_{2} = k_{2},\\quad \\dots,\\quad c_{n} = k_{n} $$\nよって、二つの線形結合表現は同一である。\n■\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p240\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3017,"permalink":"https://freshrimpsushi.github.io/jp/posts/3017/","tags":null,"title":"ベクトル空間の基底"},{"categories":"줄리아","contents":"ガイド ステップ1. ジュリアのインストール\nジュリアのダウンロードページからインストールファイルをダウンロードして、実行する。\nステップ2. VSコードのインストール\nビジュアルスタジオコードのダウンロードページからインストールファイルをダウンロードして、実行する。\nステップ3. ジュリア拡張のインストール\n左側から5番目のアイコンをクリックするか、Ctrl + Shift + XでExtensionsを開く。\u0026lsquo;julia\u0026rsquo;と検索すると、最上段にJulia Language Supportが表示される。\nInstallをクリックしてインストールする。\nエディターで拡張子が.jlのファイルを作り、ジュリアコードを書き、Shift + Enterで全体を実行してみる。上のスクリーンショットでは、\u0026ldquo;helloworld\u0026quot;を出力するためにprintln(helloworld)という1行だけ書かれている。\n環境 OS: Windows julia: v1.5.4 ","id":2067,"permalink":"https://freshrimpsushi.github.io/jp/posts/2067/","tags":null,"title":"WindowsでJuliaの最新バージョンをインストールする方法"},{"categories":"행렬대수","contents":"定義1 $n\\times n$ 行列 $A$が与えられたとしよう。$\\mathbf{0}$でない$n\\times 1$列ベクトル$\\mathbf{x}$、そして定数$\\lambda$に対して、次の式を固有値方程式または固有値問題という。\n$$ \\begin{equation} A \\mathbf{x} = \\lambda \\mathbf{x} \\end{equation} $$\n与えられた$A$に対して、上のように固有値方程式を満たす$\\lambda$を$A$の固有値と言い、$\\mathbf{x}$を$\\lambda$に対応する$A$の固有ベクトルという。\n説明 上の定義は$\\lambda \\in \\mathbb{R}$、$\\mathbf{x} \\in \\mathbb{R}^{n}$の時だけでなく、$\\lambda \\in \\mathbb{C}$、$\\mathbf{x} \\in \\mathbb{C}^{n}$の時にもそのまま適用される。「$\\mathbf{0}$でない」という条件がついているのは、下の式から分かるように、$\\mathbf{x} = \\mathbf{0}$ならば常に成り立つからだ。\n$$ A \\mathbf{0} = \\mathbf{0} = \\lambda \\mathbf{0} $$\n幾何学的な動機 ベクトル$\\mathbf{x}$を行列$A$で変換した$A \\mathbf{x}$と$\\mathbf{x}$の方向が同じだとすると、何か実数$\\lambda$に対して\n$$ A \\mathbf{x} = \\lambda \\mathbf{x} $$\nが成り立つことになる。行列$A$は本来、どんな方向の概念も持たないが、$A$の固有ベクトルが存在するならば、$A$が何か特有の方向を指していると言えるだろう。だから、このようなベクトル$\\mathbf{x}$を固有ベクトルと呼ぶのだ。例えば、以下のような$2\\times 2$行列を考えてみよう。\n$$ A = \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix} $$\nすると、ベクトル$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$は$\\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix}$に変換された時、$\\begin{bmatrix} 14 \\\\ 7 \\end{bmatrix}$となって方向が同じである。ここでベクトル$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$に$\\lambda = 7$を掛けると、ベクトルの長さも同じになり、固有値方程式\n$$ \\begin{align*} A \\mathbf{x} \u0026amp;= \\lambda \\mathbf{x} \\\\ \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \u0026amp;= 7 \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\end{align*} $$\nの形の等式を満たす。このような理由で$\\lambda=7$を固有値と呼ぶのだ。よく見ると、固有ベクトルは$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$を伸ばしたり縮めたりして無数に見つけることができるが、固有値は変わらないことが分かる。だから、$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$を固有値$7$に対応する$A$の固有ベクトルと表現するのだ。\nこのように幾何学的に説明した議論を一般的に拡張すると、固有値は代数的に方程式$A \\mathbf{x} = \\lambda \\mathbf{x}$を満たす$\\lambda$であり、固有ベクトルは与えられた$\\lambda$に対する方程式の非自明な解である。\n固有値方程式の解法 固有値を求めることは、固有値方程式から始まる。$(1)$の式を整理すると、次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; A \\mathbf{x} \u0026amp;= \\lambda \\mathbf{x} \\\\ \\implies \u0026amp;\u0026amp; A \\mathbf{x} - \\lambda \\mathbf{x} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; A \\mathbf{x} - \\lambda I \\mathbf{x} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; \\left( A - \\lambda I \\right) \\mathbf{x} \u0026amp;= \\mathbf{0} \\end{align*} $$\nこの時、固有ベクトルは条件$\\mathbf{x} \\ne \\mathbf{0}$を満たさなければならない。上記線形システムが$\\mathbf{0}$でない解を持つ同値条件は、$\\left( A - \\lambda I \\right)$の逆行列が存在しないことであり、これは次の式が成り立つことと同値である。\n$$ \\det (A -\\lambda I) = 0 $$\nしたがって、上の式を満たす$\\lambda$が$A$の固有値になる。上記の式を$A$の特性方程式と言い、$\\det (A -\\lambda I)$は$A$が$n\\times n$行列の時、$n$次の多項式になり、これを特性多項式と呼ぶ。\nちなみに、$A+B$の固有値は$A$、$B$の固有値の和と異なる場合があり、$AB$の固有値も$A$、$B$の固有値の積と異なる場合がある。また、方程式の解として固有値を求めることから分かるように、必ずしも実数であるという保証は全くない。\n例 固有値を求める 解の例として、再び$A = \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix}$を考えてみよう。$A-\\lambda I = \\begin{bmatrix} 6 - \\lambda \u0026amp; 2 \\\\ 2 \u0026amp; 3 - \\lambda \\end{bmatrix}$なので、$A$の特性方程式を解くと次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\det (A - \\lambda I) \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; (6 - \\lambda)(3 - \\lambda) - 4 \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; \\lambda^2 - 9 \\lambda + 18 - 4 \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; (\\lambda - 2)(\\lambda - 7) \u0026amp;= 0 \\end{align*} $$\nしたがって、$A$の固有値は$\\lambda = 2$と$\\lambda = 7$である。$2$と$7$を$\\lambda$に代入してみると、それぞれの固有値に対応する固有ベクトルを求めることができる。ここでは、$\\lambda = 7$の場合のみ紹介する。\n$\\lambda = 7$に対応する固有ベクトルを求める $\\lambda = 7$を$(1)$に代入して整理すると、次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} \u0026amp;= 7\\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} \\\\ \\implies \u0026amp;\u0026amp; \\begin{bmatrix} 6x_{1} + 2x_{2} \\\\ 2x_{1} + 3x_{2} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} 7x_{1} \\\\ 7x_{2} \\end{bmatrix} \\\\ \\implies \u0026amp;\u0026amp; \\begin{bmatrix} -x_{1} + 2x_{2} \\\\ 2x_{1} - 4x_{2} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\end{align*} $$\nこれを解くと、次のようになる。\n$$ \\left\\{ \\begin{align*} -x_{1} + 2x_{2} \u0026amp;= 0 \\\\ 2x_{1} - 4x_{2} \u0026amp;= 0 \\end{align*} \\right. $$\n$$ \\implies x_{1} = 2x_{2} $$\nしたがって、$0$でないすべての$x_{2}$に対して、ベクトル$\\begin{bmatrix} 2x_{2} \\\\ x_{2} \\end{bmatrix}$が$\\lambda = 7$に対応する固有ベクトルになる。通常、最も単純な形または大きさが$1$になる単位ベクトルを選ぶ。$x_{2} = 1$を代入すると、以下の固有ベクトルを得る。\n$$ A = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} $$\n性質 正の整数$k$に対して、$\\lambda$が行列$A$の固有値であり、$\\mathbf{x}$が$\\lambda$に対応する固有ベクトルであれば、$\\lambda ^{k}$は$A^{k}$の固有値であり、$\\mathbf{x}$は$\\lambda ^{k}$に対応する固有ベクトルである。 Howard Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p291-292\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":319,"permalink":"https://freshrimpsushi.github.io/jp/posts/319/","tags":null,"title":"固有値と固有ベクトル"},{"categories":"다변수벡터해석","contents":"定義1 $E\\subset \\mathbb{R}^{n}$を開集合とし、$\\mathbf{x}\\in E$、そして$\\mathbf{f} : E \\to \\mathbb{R}^{m}$と定義しよう。$\\left\\{ \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\dots, \\mathbf{e}_{n} \\right\\}$と$\\left\\{ \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{m} \\right\\}$をそれぞれ$\\mathbb{R}^{n}$と$\\mathbb{R}^{m}$の標準基底としよう。\nそれでは、$\\mathbf{f}$の成分 $f_{i} : \\mathbb{R}^{n} \\to \\mathbb{R}$は次のように定義される。\n$$ \\mathbf{f} (\\mathbf{x}) = \\sum_{i=1}^{m} f_{i}(\\mathbf{x})\\mathbf{u}_{i}, \\quad \\mathbf{x} \\in E $$\nまたは\n$$ f_{i} (\\mathbf{x}) := \\mathbf{f} (\\mathbf{x}) \\cdot \\mathbf{u}_{i},\\quad i \\in \\left\\{ 1,\\dots, m \\right\\} $$\n次の極限が存在するならば、$f_{i}$に対する$x_{j}$の偏微分とし、$D_{j}f_{i}$または$\\dfrac{\\partial f_{i}}{\\partial x_{j}}$と記される。\n$$ \\dfrac{\\partial f_{i}}{\\partial x_{j}} = D_{j}f_{i} := \\lim _{t \\to 0} \\dfrac{f_{i}(\\mathbf{x}+ t \\mathbf{e}_{j}) -f_{i}(\\mathbf{x})}{t} $$\n説明 偏とは偏っていることを意味し、微分を全ての変数ではなく、一つの変数に対してだけ考えようという意味である。これは全微分と対比する言葉である。\n偏$\\check{}$関数ではなく、偏$\\check{}$の微分関数である。\n$\\mathbf{f}$の全微分と偏微分の間には、以下の定理と同様の関係が成立する。\n定理 $E, \\mathbf{x}, \\mathbf{f}$を定義で述べた通りとする。$\\mathbf{f}$が$\\mathbf{x}$で微分可能であるとする。それならば、各偏微分$D_{j}f_{i}(\\mathbf{x})$が存在し、次の式が成立する。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x})\\mathbf{e}_{j} = \\sum_{i=1}^{m} D_{j}f_{i}(\\mathbf{x})\\mathbf{u}_{i},\\quad j \\in \\left\\{ 1,\\dots, n \\right\\} $$\n系 $\\mathbf{f}^{\\prime}(\\mathbf{x})$は、次のような行列で表される線形変換である。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x}) = \\begin{bmatrix} (D_{1}f_{1}) (\\mathbf{x}) \u0026amp; (D_{2}f_{1}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{1}) (\\mathbf{x}) \\\\ (D_{1}f_{2}) (\\mathbf{x}) \u0026amp; (D_{2}f_{2}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{2}) (\\mathbf{x}) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ (D_{1}f_{m}) (\\mathbf{x}) \u0026amp; (D_{2}f_{m}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{m}) (\\mathbf{x}) \\end{bmatrix} $$\nこれを$\\mathbf{f}$のヤコビ行列とも言う。\n証明 $j$を固定しよう。$\\mathbf{f}$が$\\mathbf{x}$で微分可能であると仮定すると、次の式が成立する。\n$$ \\mathbf{f}( \\mathbf{x} + t \\mathbf{e}_{j}) - \\mathbf{f}(\\mathbf{x}) = \\mathbf{f}^{\\prime}(\\mathbf{x})(t\\mathbf{e}_{j}) + \\mathbf{r}(t\\mathbf{e}_{j}) $$\nここで、$\\mathbf{r}(t\\mathbf{e}_{j})$は次を満たす剰余である。\n$$ \\lim _{t \\to 0} \\dfrac{|\\mathbf{r}(t\\mathbf{e}_{j}) |}{t}=0 $$\n$\\mathbf{f}^{\\prime}(\\mathbf{x})$は線形変換であるから、次が成立する。\n$$ \\dfrac{\\mathbf{f}( \\mathbf{x} + t \\mathbf{e}_{j}) - \\mathbf{f}(\\mathbf{x})}{t} = \\dfrac{\\mathbf{f}^{\\prime}(\\mathbf{x})(t\\mathbf{e}_{j})}{t} + \\dfrac{\\mathbf{r}(t\\mathbf{e}_{j})}{t} = \\mathbf{f}^{\\prime}(\\mathbf{x})(\\mathbf{e}_{j}) + \\dfrac{\\mathbf{r}(t\\mathbf{e}_{j})}{t} $$\n両辺に$\\lim _{t \\to 0}$の極限を取ると、次のようになる。\n$$ \\lim _{t \\to 0} \\dfrac{\\mathbf{f}( \\mathbf{x} + t \\mathbf{e}_{j}) - \\mathbf{f}(\\mathbf{x})}{t} = \\mathbf{f}^{\\prime}(\\mathbf{x})\\mathbf{e}_{j} $$\n$\\mathbf{f}$を成分で表示すると、次を得る。\n$$ \\begin{align*} \\mathbf{f}^{\\prime}(\\mathbf{x})\\mathbf{e}_{j} \u0026amp;= \\lim _{t \\to 0} \\dfrac{\\mathbf{f}( \\mathbf{x} + t \\mathbf{e}_{j}) - \\mathbf{f}(\\mathbf{x})}{t} \\\\ \u0026amp;= \\lim _{t \\to 0} \\dfrac{\\sum_{i=1}^{m} f_{i}( \\mathbf{x} + t \\mathbf{e}_{j})\\mathbf{u}_{i} - \\sum_{i=1}^{m} f_{i}(\\mathbf{x})\\mathbf{u}_{i}}{t} \\\\ \u0026amp;= \\sum_{i=1}^{m} \\lim _{t \\to 0} \\dfrac{f_{i}( \\mathbf{x} + t \\mathbf{e}_{j}) - f_{i}(\\mathbf{x})}{t} \\mathbf{u}_{i} \\end{align*} $$\nそれならば、偏微分の定義により、次を得る。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x})\\mathbf{e}_{j} = \\sum_{i=1}^{m} D_{j}f_{i}(\\mathbf{x}) \\mathbf{u}_{i} $$\n■\n例 $f : \\R^{3} \\to \\R, \\gamma : \\R \\to \\R^{3}$が微分可能な関数だとしよう。また、\n$$ \\gamma (t) = \\left( x(t), y(t), z(t) \\right) $$\nそして$f$と$\\gamma$の合成を$g = f \\circ \\gamma$としよう。\n$$ g(t) = f \\circ \\gamma (t) = f \\left( \\gamma (t) \\right) $$\nすると$g^{\\prime}$は、連鎖律、偏微分の定義、上述の定理により、次のようになる。\n$$ \\begin{align*} \\dfrac{d g}{d t}(t_{0}) = g^{\\prime}(t_{0}) =\u0026amp;\\ f^{\\prime}(\\gamma (t_{0})) \\gamma^{\\prime}(t_{0}) \\\\ =\u0026amp;\\ \\begin{bmatrix} D_{1}f(\\gamma (t_{0})) \u0026amp; D_{2}f(\\gamma (t_{0})) \u0026amp; D_{3}f(\\gamma (t_{0})) \\end{bmatrix} \\begin{bmatrix} D\\gamma_{1} (t_{0}) \\\\ D\\gamma_{2} (t_{0}) \\\\ D\\gamma_{3} (t_{0}) \\end{bmatrix} \\\\ =\u0026amp;\\ \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x}(\\gamma (t_{0})) \u0026amp; \\dfrac{\\partial f}{\\partial y}(\\gamma (t_{0})) \u0026amp; \\dfrac{\\partial f}{\\partial z}(\\gamma (t_{0})) \\end{bmatrix} \\begin{bmatrix} \\dfrac{d x}{d t}(t_{0}) \\\\ \\dfrac{d y}{d t}(t_{0}) \\\\ \\dfrac{d z}{d t}(t_{0}) \\end{bmatrix} \\\\ =\u0026amp;\\ \\dfrac{\\partial f}{\\partial x}(\\gamma (t_{0}))\\dfrac{d x}{d t}(t_{0}) + \\dfrac{\\partial f}{\\partial y}(\\gamma (t_{0}))\\dfrac{d y}{d t}(t_{0}) + \\dfrac{\\partial f}{\\partial z}(\\gamma (t_{0}))\\dfrac{d z}{d t}(t_{0}) \\end{align*} $$\nしたがって、\n$$ \\implies \\dfrac{d g}{d t} = \\dfrac{\\partial f}{\\partial x}\\dfrac{d x}{d t} + \\dfrac{\\partial f}{\\partial y}\\dfrac{d y}{d t} + \\dfrac{\\partial f}{\\partial z}\\dfrac{d z}{d t} $$\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), p215\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3036,"permalink":"https://freshrimpsushi.github.io/jp/posts/3036/","tags":null,"title":"偏微分_functions"},{"categories":"행렬대수","contents":"定義1 正定値行列 二次形式$\\mathbf{x}^{\\ast} A \\mathbf{x}$が\nすべての$\\mathbf{x} \\ne \\mathbf{0}$に対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \u0026gt; 0$を満たすならば、二次形式や行列$A$を正定positive definiteという。\nすべての$\\mathbf{x} \\ne \\mathbf{0}$に対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \u0026lt; 0$を満たすならば、二次形式や行列$A$を負定negative definiteという。\n$\\mathbf{x}$に従って、正の数も負の数も成り立つ場合には、二次形式や行列$A$を不定indefiniteという。\n実行列の場合は、定義の$\\mathbf{x}^{\\ast} A \\mathbf{x}$部分を$\\mathbf{x}^{T} A \\mathbf{x}$に置き換えて考えればよい。\n準正定値行列 二次形式$\\mathbf{x}^{\\ast} A \\mathbf{x}$が\nすべての$\\mathbf{x} \\ne \\mathbf{0}$に対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \\ge 0$を満たすならば、二次形式や行列$A$を正の準定positive semidefiniteという。\nすべての$\\mathbf{x} \\ne \\mathbf{0}$に対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \\le 0$を満たすならば、二次形式や行列$A$を負の準定negative semidefiniteという。\n説明 これらの定義はクリアだが、省略されたものが多く、頭で追うのが難しい。式と説明をじっくり見ながら、概念自体を理解しよう。二次形式の定数が複素数の場合、すなわち$A$がエルミート行列である場合を考えてみよう。$A \\mathbf{x} = \\lambda \\mathbf{x}$を見ると、$\\lambda$は$A$の固有値になる。左辺に共役転置$\\mathbf{x}^{\\ast}$をかけると次のようになる。\n$$ \\mathbf{x}^{\\ast} A \\mathbf{x} = \\lambda \\mathbf{x}^{\\ast} \\mathbf{x} = \\lambda \\mathbf{x} \\cdot \\mathbf{x} = \\lambda | \\mathbf{x} |^{2} $$\nここで$\\mathbf{x} \\ne \\mathbf{0}$であるため、$|\\mathbf{x}| ^2 \u0026gt; 0$であり、エルミート行列の固有値は実数なので、$\\lambda |\\mathbf{x}| ^2$も実数だ。したがって、$\\mathbf{x}^{\\ast} A \\mathbf{x}$は実数であり、正か負かを確認できるということだ。行列とベクトルの乗算で表記すると理解しにくかったものが、$\\lambda |\\mathbf{x}| ^2$として表すとずっと分かりやすくなる。\n$\\lambda |\\mathbf{x}|^{2}$の符号を考えると、常に$|\\mathbf{x}|^{2} \u0026gt;0$なので、$\\lambda$の符号だけを考えればよい。結局、ゼロベクターでない任意のベクターに対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \u0026gt; 0$という言葉は$A$のすべての固有値が正であるという意味になる。反対に考えれば、負定値行列はすべての固有値が負である行列であるという意味だ。こうして、定義性はもともと正負の概念がない行列に正負(positive/negative)といった概念を定義(definite)することと考えることができるだろう。これを含むのが定理1である。\nさらに、可逆行列であるための同値条件により、正定値行列と負定値行列は$0$の固有値を持たないため、可逆行列である。(定理2)\n応用 数値線形代数学では、特に正定値に多くの関心が持たれる。条件として正定値を考えると、エルミート行列が基本でありながら、すべての固有値が正であるという、非常に強い条件だとわかる。 動力学では、負定値行列の性質を利用して、システムの平衡点の安定性を研究することもある。 統計学では、基本的に共分散行列が正の半定行列であるため、非常に重要だ。 定理1 二次形式 $\\mathbf{x}^{\\ast} A\\mathbf{x}$について、\n$\\mathbf{x}^{\\ast} A\\mathbf{x}$が正定であるための必要十分条件は$A$のすべての固有値が正であることである。\n$\\mathbf{x}^{\\ast} A\\mathbf{x}$が負定であるための必要十分条件は$A$のすべての固有値が負であることである。\n$\\mathbf{x}^{\\ast} A\\mathbf{x}$が不定であるための必要十分条件は$A$が少なくとも1つの負の固有値と少なくとも1つの正の固有値を持つことである。\n定理2 正の定値行列と負の定値行列は常に可逆行列である。\n定理3 対称行列$A$について、\n$A$が正定値であれば、$\\mathbf{x}^{T}A\\mathbf{x}=1$は楕円の方程式である。\n$A$が負定値であれば、$\\mathbf{x}^{T}A\\mathbf{x}=1$はグラフを持たない。\n$A$が不定であれば、$\\mathbf{x}^{T}A\\mathbf{x}=1$は双曲線の方程式である。\nHoward Anton, Chris Rorres, Anton Kaul, 「Elementary Linear Algebra: Applications Version」(第12版). 2019年, p423\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":336,"permalink":"https://freshrimpsushi.github.io/jp/posts/336/","tags":null,"title":"政府号行列"},{"categories":"행렬대수","contents":"定義 $A$を次のような$2 \\times 2$ 行列としよう。\n$$ A = \\begin{bmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{bmatrix} $$\n$A$の行列式determinantを以下のように定義し、$\\det(A)$で表す。\n$$ \\det(A) := ad - bc $$\n説明 行列式を話す上で、線形代数の目的自体を話さずにはいられない。ほとんどの数学で言う問題は基本的に「方程式を解けるか」と要約できると言っても過言ではない。例えば、簡単な方程式\n$$ ax = b $$\nを考えてみれば、$ a = 0$が0でない限り、この方程式には解があることが簡単にわかる。二次方程式\n$$ a x^2 + b x + c = 0 $$\nも解の公式を通じて簡単に解ける。こうして、数学者たちは$x$の次数を上げながら、より一層難しい問題に挑戦した。しかし、不運な天才アーベルによって「5次以上の代数方程式は一般解を持たない」と証明されてしまう。\n一方、代数の次元を上げる代わりに未知数や方程式の数そのものを増やして研究する道が残されていた。ここで行列式が登場する。韓国語で見れば、行列が出来た後行列式が出来たように見えるかもしれないが、実はそうではない。歴史的に行列式は行列が登場する前に先に登場したし1、実際に英語を見るとdeterminantとmatrixは特に関連がない。determinantという名前は次のような未知数が2つある連立一次方程式の解が存在するか、存在しないかを判別してくれる公式だから付けられた名前だ。\n$$ \\left\\{ \\begin{align*} ax + by \u0026amp;= 0 \\\\ cx + dy \u0026amp;= 0 \\end{align*} \\right. $$\n上記のような連立方程式が与えられた時$ad-bc = 0$ならば唯一の自明解しか存在せず、$ad-bc \\ne 0$ならば非自明な唯一の解を持つことになる。したがって、$ad-bc$が与えられた連立方程式の解があるかないかを判別してくれる公式となるため、判別式という名前が付けられたのだ。\nしかし、ご存知のように連立方程式は行列の形で表現できる。「簡単な」連立方程式は次のように表現できる。\n$$ A \\mathbf{x} = \\mathbf{b} $$\n$ax = b$の解が$x = \\dfrac{b}{a}$だったことをよく考えてみよう。$\\dfrac{1}{a}$は$a$の逆元なので、両辺にかけるだけで$x$だけを残すことができた。解が存在する条件と結びつけて言えば、$a= 0$は逆元が存在しないので$ax = b$の解も存在しないことになる。同様に、$A \\mathbf{x} = \\mathbf{b}$も$A$の逆行列を求めることができるかの問題に帰着される。$A$によって表される線形システムの解の存在自体が$A$の逆行列の存在であり、この逆元を求めることが解を求めることになる。この時、$A$の逆行列が存在する条件と$A$によって表される線形システムが唯一の解を持つ条件が同じであることがわかる。\n$A = \\begin{bmatrix} a \u0026amp; b \\\\ c \u0026amp; d\\end{bmatrix}$の逆行列は次のようだ。\n$$ A^{-1} = \\dfrac{1}{ad-bc} \\begin{bmatrix} d \u0026amp; -b \\\\ -c \u0026amp; a \\end{bmatrix} $$\nこれは単純に$A$と$A^{-1}$を直接掛け合わせて証明される。もし$\\det (A) = ad - bc = 0$ならば、行列の形状に関係なく、$A^{-1}$の前の定数が$\\dfrac{1}{0}$となるので、逆行列が存在できない。可逆性invertibilityをたまに非特異性nonsingularityと呼ぶのもこのためだ。singularという言葉は「特異的な」と訳されるが、数学的に言えば「0で割る」くらいの感じになる。\n一方、$n\\times n$個の実数を$1$個の実数にマッピングする関数の観点から行列式を見れば、次のように定義することができる。\n定義 関数$ \\det : \\mathbb{R}^{n \\times n } \\to \\mathbb{R} $が次の条件を満たすならば、行列式と定義する。\n単位行列$I_{n}$に対して、$\\det(I_{n}) = 1$ $1 \\le i,j \\le n$に対して、$\\det \\begin{bmatrix} \\mathbb{r_{1}} \\\\ \\vdots \\\\ \\mathbb{r_{i}} \\\\ \\vdots \\\\ \\mathbb{r_{j}} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix} = - \\det \\begin{bmatrix} \\mathbb{r_{1}} \\\\ \\vdots \\\\ \\mathbb{r_{j}} \\\\ \\vdots \\\\ \\mathbb{r_{i}} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix}$ $\\det \\begin{bmatrix} k \\mathbb{r_{1}} + l \\mathbb{r_{1}}^{\\prime} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix} = k \\det \\begin{bmatrix} \\mathbb{r_{1}} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix} + l \\det \\begin{bmatrix} \\mathbb{r_{1}}^{\\prime} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix}$ 説明 このように行列式を一般化すると、連立方程式の解が存在するか存在しないかを話すことがずっと簡単になるだろう。そして、このような議論が一行で完結したものがちょうど下の定理だ。\n$$ \\forall A \\in \\mathbb{C}^{n \\times n},\\quad \\exists A^{-1} \\iff \\det{A} \\ne 0 $$\n定理と言うよりほぼ定義のレベルで受け入れられるほど当たり前の事実だ。しかし、なぜこの定理が残るのか、本当に当たり前なのかきちんと説明できないなら、行列式を理解していないのと同じだ。特に行列式の場合は、定義よりも概念が先行するため、理解できなければ、時間をかけても知っておくべきだ。\nhttps://en.wikipedia.org/wiki/Determinant#History\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":252,"permalink":"https://freshrimpsushi.github.io/jp/posts/252/","tags":null,"title":"行列式"},{"categories":"행렬대수","contents":"定義1 定数$a_{1}$、$a_{2}$、$\\dots$、$a_{n}$、$b$に対して、変数$x_{1}$、$x_{2}$、$\\dots$、$x_{n}$の一次方程式linear equationを次のように定義する。\n$$ \\begin{equation} a_{1}x_{1} + a_{2}x_{2} + \\cdots + a_{n}x_{n} = b \\label{lineq} \\end{equation} $$\nこのとき、少なくとも一つの$a$は$0$ではない。つまり「全ての$a$が$0$」ではない。一次方程式の有限集合を連立一次方程式system of linear equationsまたは単に線形系linear systemと呼び、変数を未知数unknownsと呼ぶ。韓国語で線形と一次は同じ意味である。一般に$n$個の変数$x_{1}$、$x_{2}$、$\\dots$、$x_{n}$に対する$m$個の一次方程式で構成される線形系は次のように表される。\n$$ \\begin{equation} \\begin{aligned} a_{11}x_{1} + a_{12}x_{2} + \\cdots + a_{1n}x_{n} \u0026amp;= b_{1} \\\\ a_{21}x_{1} + a_{22}x_{2} + \\cdots + a_{2n}x_{n} \u0026amp;= b_{2} \\\\ \u0026amp;\\vdots \\\\ a_{m1}x_{1} + a_{m2}x_{2} + \\cdots + a_{mn}x_{n} \u0026amp;= b_{m} \\end{aligned} \\label{linsys} \\end{equation} $$\nこれを行列で表すと、次のようになる。\n$$ \\begin{align*} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{m} \\end{bmatrix} \\\\ A\\mathbf{x} \u0026amp;= \\mathbf{b} \\end{align*} $$\n説明 線形系を真にする$x_{1}$、$x_{2}$、$\\dots$、$x_{n}$の値を解solutionと呼ぶ。線形系が与えられると、以下の三つのうちの一つを満たさなければならない。それ以外の場合は存在しない。証明は記事の下部で紹介する。\n解が一意に存在する。 解が無数に存在する。 解が存在しない。 少なくとも一つ以上の解が存在する場合、線形系は一致するconsistentと言われる。解が存在しない場合、線形系は不一致inconsistentと言われる。\n具体的に変数が2つの場合、一次方程式は直線の方程式を意味する。変数が2つの線形系で解が一意に存在する場合、直線が一点で交わる場合を意味する。解が無数に存在する場合、直線が無数の点で交わる場合、つまり重なっている場合を意味する。解が存在しない場合、直線が交わる点が存在しない場合を意味する。\n変数が3つの一次方程式は平面の方程式を意味するので、線形系の解によって、平面がどのように重なっているかを意味することになる。\n例 次の線形系を解いてみよう。\n$$ \\begin{align*} 4x -2y \u0026amp;= 1 \\\\ 16x -8y \u0026amp;= 4 \\end{align*} $$\n上の式に$-4$を掛けて下の式に加えると、次のようになる。\n$$ \\begin{align*} 4x -2y \u0026amp;= 1 \\\\ 0 \u0026amp;= 0 \\end{align*} $$\nすると、下の式は何の情報も表さないので、上の式だけで表そう。\n$$ 4x -2y = 1 $$\nこの場合、幾何学的に二つの直線が一致することを意味する。このような場合、$x$を$y$に対して整理して$x = \\dfrac{1}{2}y + \\dfrac{1}{4}$と表記した後、$y$に任意の数$t$を代入して解を表す。\n$$ x = \\dfrac{1}{4} + \\dfrac{1}{2}t, \\quad y = t $$\nこのような$t$をパラメーターparameterと呼び、上の方程式をパラメーター方程式parametric equationsと呼ぶ。\n証明2 連立一次方程式は、解が存在しないか、一つだけか、無数に存在するかのどれかである。他の場合は存在しない。\n異なる二つの解があるとき、無数に多くの解が存在することを示せば、証明が完了する。$\\mathbf{x}_{1}$、$\\mathbf{x}_{2}$を連立一次方程式$A\\mathbf{x} =\\mathbf{b}$の異なる二つの解としよう。そして、$\\mathbf{x}_{0} = \\mathbf{x}_{1} - \\mathbf{x}_{2}$とする。$\\mathbf{x}_{1}$と$\\mathbf{x}_{2}$が異なる二つの解であるので、$\\mathbf{x}_{0} \\ne \\mathbf{0}$である。さらに、以下の式が成り立つ。\n$$ A \\mathbf{x}_{0} = A (\\mathbf{x}_{1} - \\mathbf{x}_{2}) = \\mathbf{b} - \\mathbf{b} = \\mathbf{0} $$\nこのとき、$k$を任意の定数としよう。すると、上記の結果により、以下の式も成り立つ。\n$$ \\begin{align*} A (\\mathbf{x}_{1} + k\\mathbf{x}_{0}) \u0026amp;= A\\mathbf{x}_{1} + A(k\\mathbf{x}_{0}) \\\\ \u0026amp;= A\\mathbf{x}_{1} + kA\\mathbf{x}_{0} \\\\ \u0026amp;= \\mathbf{b} + \\mathbf{0} \\\\ \u0026amp;= \\mathbf{b} \\end{align*} $$\nしたがって、$\\mathbf{x}_{1} + k\\mathbf{x}_{0}$も連立一次方程式$A\\mathbf{x} = \\mathbf{b}$の解である。これは任意の定数$k$に対して成り立つので、解が無数に存在する。\n■\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p2-6\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p62\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3013,"permalink":"https://freshrimpsushi.github.io/jp/posts/3013/","tags":null,"title":"連立一次方程式"},{"categories":"함수","contents":"定義 関数 $f : X \\to Y$が以下の二つの条件を満たす場合、線形linearという。$x,x_{1},x_{2}\\in X$、$a \\in \\mathbb{R}$に対して、\n$f(ax) = af(x)$ $f(x_{1} + x_{2}) = f(x_{1}) + f(x_{2})$ 説明 線形でない場合、非線形nonlinearという。二つの条件をまとめて次のように表すこともある\n$$ f(ax_{1} + x_{2}) = af(x_{1}) + f(x_{2}) $$\n2.で等式ではなく、以下である$\\le$が成り立つ場合、準線形という。\n双線形 二変数関数$f = f(x,y)$が各変数に対して線形である場合、双線形bilinearという。\n多重線形 多変数関数$f= f(x_{1}, \\dots, x_{n})$が各変数に対して線形である場合、多重線形multilinearという。\n","id":3037,"permalink":"https://freshrimpsushi.github.io/jp/posts/3037/","tags":null,"title":"線形関数"},{"categories":"행렬대수","contents":"定義 ユニタリ行列 $A$を正方形の複素数行列とする。$A$が以下の式を満たす時、ユニタリ行列unitaryと呼ぶ。\n$$ A^{-1}=A^{\\ast} $$\nこの時、$A^{-1}$は$A$の逆行列、$A^{\\ast}$は$A$の共役転置である。\nユニタリ対角化1 サイズが$n \\times n$の正方行列$A$が与えられたとする。$A$が対角行列$D$とユニタリ行列$P$に対して次の式を満たす場合、ユニタリ対角化可能unitarily diagonalizableと言う。\n$$ P^{\\ast} A P = D $$\nこのような条件を満たす$P$は行列$A$をユニタリ対角化するunitarily diagonalizeと言う。\n説明 ユニタリ行列は簡単に言うと、複素数行列に対して拡張された直交行列である。従って、直交行列の性質をそのまま持つ。以下のユニタリ行列の同値条件に関する証明は、直交行列の証明に置き換える。\n定理2 ユニタリ行列の同値条件: $n \\times n$サイズの複素数行列$A$に対して、以下の命題は全て同値である。\n$A$はユニタリ行列である。\n$A$の行ベクトルの集合は$\\mathbb{C}^n$の正規直交集合である。\n$A$の列ベクトルの集合は$\\mathbb{C}^n$の正規直交集合である。\n$A$は内積を保持する。つまり、全ての$\\mathbf{x},\\mathbf{y}\\in \\mathbb{C}^{n}$に対して、以下が成立する。\n$$ (A \\mathbf{x}) \\cdot (A\\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y} $$\n$A$は長さを保持する。つまり、全ての$\\mathbf{x}\\in \\mathbb{C}^{n}$に対して、以下が成立する。 $$ \\left\\| A \\mathbf{x} \\right\\| = \\left\\| \\mathbf{x} \\right\\| $$\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), 440ページ\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), 439ページ\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3008,"permalink":"https://freshrimpsushi.github.io/jp/posts/3008/","tags":null,"title":"ユニタリ行列"},{"categories":"행렬대수","contents":"定義 $n\\times n$ 行列が以下のように与えられたとする。\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{bmatrix} $$\n$A$の対角要素の合計を$A$の対角合計traceと定義し、以下のように表記する。\n$$ \\text{tr}(A)=\\text{Tr}(A)=a_{11}+a_{22}+\\cdots + a_{nn}=\\sum \\limits_{i=1}^{n} a_{ii} $$\n説明 次のように対角合計を関数として考えることもできる。$M_{n\\times n}(\\mathbb{R})$を実数を成分とする$n\\times n$行列の集合とする。すると$\\text{Tr}$は次のように定義される関数である。\n$$ \\text{Tr} : M_{n\\times n} (\\mathbb{R}) \\to \\mathbb{R},\\quad \\text{Tr}(A)=\\sum \\limits_{i=1}^{n} a_{ii} $$\n性質 $A,B,C$が$n \\times n$行列で、$k$が定数とする。\n(a) スカラー倍のトレースとトレースのスカラー倍が同じである。\n$$ \\text{Tr}(kA)= k\\text{Tr}(A) $$\n(b) 合計のトレースとトレースの合計が同じである。\n$$ \\text{Tr}(A+B)=\\text{Tr}(A)+\\text{Tr}(B) $$\n(a)+(b) トレースは線形である。\n$$ \\text{Tr}(kA+B)=k\\text{Tr}(A)+\\text{Tr}(B) $$\n(c) $AB$と$BA$のトレースが同じである。\n$$ \\text{Tr}(AB) = \\text{Tr}(BA) $$\n(c\u0026rsquo;) 循環性Cyclic Property: 上記の事実から、次の式が成り立つことがわかる。\n$$ \\text{Tr}(ABC) = \\text{Tr}(BCA) = \\text{Tr}(CAB) $$\n(d) $A$と$A^{T}$のトレースが同じである。\n$$ \\text{Tr}(A) = \\text{Tr}(A^{T}) $$\n","id":1924,"permalink":"https://freshrimpsushi.github.io/jp/posts/1924/","tags":null,"title":"トレース"},{"categories":"분포이론","contents":"定義 平均ベクトル $\\mathbf{\\mu} \\in \\mathbb{R}^{p}$ および共分散行列 $\\Sigma \\in \\mathbb{R}^{p \\times p}$ に基づく以下のような確率密度関数を持つ多変量分布 $N_{p} \\left( \\mu , \\Sigma \\right)$ を多変量正規分布と呼ぶ。\n$$ f (\\textbf{x}) = \\left( (2\\pi)^{p} \\det \\Sigma \\right)^{-1/2} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( \\textbf{x} - \\mathbf{\\mu} \\right)^{T} \\Sigma^{-1} \\left( \\textbf{x} - \\mathbf{\\mu} \\right) \\right] \\qquad , \\textbf{x} \\in \\mathbb{R}^{p} $$\n$\\mathbf{x}^{T}$ は $\\mathbf{x}$ の転置を意味する。 定理 $$ \\begin{align*} \\mathbf{X} =\u0026amp; \\begin{bmatrix} \\mathbf{X}_{1} \\\\ \\mathbf{X}_{2} \\end{bmatrix} \u0026amp; : \\Omega \\to \\mathbb{R}^{n} \\\\ \\mu =\u0026amp; \\begin{bmatrix} \\mu_{1} \\\\ \\mu_{2} \\end{bmatrix} \u0026amp; \\in \\mathbb{R}^{n} \\\\ \\Sigma =\u0026amp; \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\Sigma_{12} \\\\ \\Sigma_{21} \u0026amp; \\Sigma_{22} \\end{bmatrix} \u0026amp; \\in \\mathbb{R}^{n \\times n} \\end{align*} $$ 以下の定理の記述で、特に説明がない限り、$\\mathbf{X}$、$\\mu$、$\\Sigma$ は同じブロック行列を指す。\n多変量正規分布の線形変換 行列 $A \\in \\mathbb{R}^{m \\times n}$ とベクトル $\\mathbf{b} \\in \\mathbb{R}^{m}$ に対して多変量正規分布に従うランダムベクトル $\\mathbf{X} \\sim N_{n} \\left( \\mu , \\Sigma \\right)$ の線形変換 $\\mathbf{Y} = A \\mathbf{X} + \\mathbf{b}$ もやはり多変量正規分布 $N_{m} \\left( A \\mu + \\mathbf{b} , A \\Sigma A^{T} \\right)$ に従う。\n多変量正規分布における独立性とゼロ相関性は等価である 多変量正規分布に従うランダムベクトル $\\mathbf{X} \\sim N_{n} \\left( \\mu , \\Sigma \\right)$ があるとする。すると、以下が成立する。 $$ \\mathbf{X}_{1} \\perp \\mathbf{X}_{2} \\iff \\Sigma_{12} = \\Sigma_{21} = O $$\n多変量正規分布の条件付き平均と分散 多変量正規分布に従うランダムベクトル $\\mathbf{X} \\sim N_{n} \\left( \\mu , \\Sigma \\right)$ があるとする。その場合、条件付き確率ベクトル $\\mathbf{X}_{1} | \\mathbf{X}_{2} : \\Omega \\to \\mathbb{R}^{m}$ もやはり多変量正規分布に従い、具体的には以下のような平均ベクトルと共分散行列を持つ。 $$ \\mathbf{X}_{1} | \\mathbf{X}_{2} \\sim N_{m} \\left( \\mu_{1} + \\Sigma_{12} \\Sigma_{22}^{-1} \\left( \\mathbf{X}_{2} - \\mu_{2} \\right) , \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} \\right) $$\n回帰係数ベクトルの多変量正規性 回帰係数の推定量 $\\hat{\\beta}$ は以下のような多変量正規分布に従う。 $$ \\hat{\\beta} \\sim N_{1+p} \\left( \\beta , \\sigma^{2} \\left( X^{T} X \\right)^{-1} \\right) $$\n積率生成関数 $X \\sim N_{p} \\left( \\mu , \\Sigma \\right)$ の積率生成関数は以下のとおりである。 $$ M_{X} \\left( \\mathbf{t} \\right) = \\exp \\left( \\mathbf{t}^{T} \\mu + {{ 1 } \\over { 2 }} \\mathbf{t}^{T} \\Sigma \\mathbf{t} \\right) \\qquad , \\mathbf{t} \\in \\mathbb{R}^{p} $$\nエントロピー 多変量正規分布 $N_{p}(\\mu, \\Sigma)$ のエントロピーは以下のとおりである。\n$$ H = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{p} \\left| \\Sigma \\right| \\right] = \\dfrac{1}{2}\\ln (\\det (2\\pi e \\Sigma)) $$\n$\\left| \\Sigma \\right|$ は共分散行列の行列式である。\n参照 一変量正規分布: $p = 1$ に続いて $\\mu \\in \\mathbb{R}^{1}$ があり、そして $\\Sigma \\in \\mathbb{R}^{1 \\times 1}$ の時、上記の確率密度関数は正確に一変量正規分布の確率密度関数となる。 ","id":1954,"permalink":"https://freshrimpsushi.github.io/jp/posts/1954/","tags":null,"title":"多変量正規分布"},{"categories":"수리통계학","contents":"定義1 $p$次元のランダムベクター$\\mathbf{X} = \\left( X_{1}, \\cdots , X_{p} \\right)$に対して以下のように定義される$\\text{Cov} (\\mathbf{X})$を共分散行列Covariance Matrixという。\n$$ \\left( \\text{Cov} \\left( \\mathbf{X} \\right) \\right)_{ij} := \\text{Cov} \\left( X_{i} , X_{j} \\right) $$\n$\\text{Cov}$は共分散である。 説明 定義をもっと簡単に書いてみると以下の通り。\n$$ \\text{Cov} \\left( \\mathbf{X} \\right) := \\begin{pmatrix} \\text{Var} \\left( X_{1} \\right) \u0026amp; \\text{Cov} \\left( X_{1} , X_{2} \\right) \u0026amp; \\cdots \u0026amp; \\text{Cov} \\left( X_{1} , X_{p} \\right) \\\\ \\text{Cov} \\left( X_{2} , X_{1} \\right) \u0026amp; \\text{Var} \\left( X_{2} \\right) \u0026amp; \\cdots \u0026amp; \\text{Cov} \\left( X_{2} , X_{p} \\right) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\text{Cov} \\left( X_{p} , X_{1} \\right) \u0026amp; \\text{Cov} \\left( X_{p} , X_{2} \\right) \u0026amp; \\cdots \u0026amp; \\text{Var} \\left( X_{p} \\right) \\end{pmatrix} $$\nすべての共分散行列は正の半定値行列である。つまり、すべてのベクター$\\mathbf{x} \\in \\mathbb{R}^{p}$に対して以下が成り立つ。\n$$ 0 \\le \\textbf{x}^{T} \\text{Cov} \\left( \\mathbf{X} \\right) \\textbf{x} $$\n定理 [1]: $\\mathbf{\\mu} \\in \\mathbb{R}^{p}$が$\\mathbf{\\mu} := \\left( EX_{1} , \\cdots , EX_{p} \\right)$として与えられた場合、 $$ \\text{Cov} (\\mathbf{X}) = E \\left[ \\mathbf{X} \\mathbf{X}^{T} \\right] - \\mathbf{\\mu} \\mathbf{\\mu}^{T} $$ [2]: 定数の行列$A \\in \\mathbb{R}^{k \\times p}$が$(A)_{ij} := a_{ij}$として与えられた場合、 $$ \\text{Cov} ( A \\mathbf{X}) = A \\text{Cov} \\left( \\mathbf{X} \\right) A^{T} $$ $A^{T}$は$A$の転置行列である。 証明 [1] $$ \\begin{align*} \\text{Cov} \\left( \\mathbf{X} \\right) =\u0026amp; E \\left[ \\left( \\mathbf{X} - \\mathbf{\\mu} \\right) \\left( \\mathbf{X} - \\mathbf{\\mu} \\right)^{T} \\right] \\\\ =\u0026amp; E \\left[ \\mathbf{X} \\mathbf{X}^{T} - \\mathbf{\\mu} \\mathbf{X}^{T} - \\mathbf{X} \\mathbf{\\mu}^{T} + \\mathbf{\\mu} \\mathbf{\\mu}^{T} \\right] \\\\ =\u0026amp; E \\left[ \\mathbf{X} \\mathbf{X}^{T} \\right] - \\mathbf{\\mu} E \\left[ \\mathbf{X}^{T} \\right] - E \\left[ \\mathbf{X} \\right] \\mathbf{\\mu}^{T} + E \\left[ \\mathbf{\\mu} \\mathbf{\\mu}^{T} \\right] \\\\ =\u0026amp; E \\left[ \\mathbf{X} \\mathbf{X}^{T} \\right] - \\mathbf{\\mu} \\mathbf{\\mu}^{T} \\end{align*} $$\n■\n[2] 2 $$ \\begin{align*} \\text{Cov} \\left( A \\mathbf{X} \\right) =\u0026amp; E \\left[ \\left( A\\mathbf{X} - A\\mathbf{\\mu} \\right) \\left( A\\mathbf{X} - A\\mathbf{\\mu} \\right)^{T} \\right] \\\\ =\u0026amp; E \\left[ A\\left(\\mathbf{X} -\\mathbf{\\mu} \\right) \\left( \\mathbf{X} - \\mathbf{\\mu} \\right)^{T} A^{T} \\right] \\\\ =\u0026amp; A E \\left[ \\left(\\mathbf{X} -\\mathbf{\\mu} \\right) \\left( \\mathbf{X} - \\mathbf{\\mu} \\right)^{T}\\right] A^{T} \\\\ =\u0026amp; A \\text{Cov}\\left( \\mathbf{X} \\right) A^{T} \\end{align*} $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p126.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stats.stackexchange.com/a/106207/172321\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1950,"permalink":"https://freshrimpsushi.github.io/jp/posts/1950/","tags":null,"title":"共分散行列"},{"categories":"머신러닝","contents":"定義 強化学習とは、エージェントが環境と相互作用して累積報酬を最大化するポリシーを見つけることができるようにすることである。\n説明1 強化学習を構成する要素は次のとおりである。\nエージェントagent: 与えられた状態において、ポリシーに従って行動を決定する。 ステートstate, 状態: エージェントが置かれている状況を指す。 アクションaction, 行動: エージェントが与えられた状態で選ぶことができる選択肢を指す。 ポリシーpolicy, 方針: エージェントが与えられた状態で行動を決定する戦略を指す。 リワードreward, 報酬: エージェントが与えられた状態で選んだ行動によって得られる点数を指す。エージェントが達成すべき目標と見なすことができる。 環境environment: エージェントが与えられた状態でどのような行動を決定すれば、MDPに従って次の状態とそれに伴う報酬を決定するか。 エピソードepisode: エージェントと環境の相互作用が始まった時から終わるまでを指す。 これをさまざまな状況に例えると次のようになる。\n強化学習 試験勉強 囲碁 エージェント 学生 囲碁の棋士 ステート 試験まで残り日数 碁盤 アクション 勉強、飲酒、ゲームなど 着手 ポリシー 日付別勉強計画 戦略 リワード 試験点数 勝敗 エピソード 試験期間 一局 強化学習の問題：グリッドモデル 強化学習を説明するための代表的な例としてグリッドワールドgrid worldがある。これから次のグリッドモデルを例に各要素を具体的に説明する。一度に上下左右の4方向のうち一つに一マスずつ動けるロボットが下記のような$4 \\times 4$のグリッドで動く場合を考えてみよう。スタート地点は$\\boxed{\\ 2\\ }$から$\\boxed{15}$まで任意に決められ、ロボットが$\\fcolorbox{black}{darkgray}{\\ 1\\ }$または$\\fcolorbox{black}{darkgray}{16}$まで最短距離で行くことが目標とする。\nエージェント 強化学習におけるエージェントは学習する主体として説明されるが、実際には存在しない。後述する他の概念が確率変数などで定義されるのに対し、エージェントには明確な数学的定義がない。したがって、強化学習に関する理論的な勉強はエージェントという対象がなくても可能であり、実際にそうである。強化学習理論において本質的にエージェントを意味するのはポリシーである。しかし直感的には、学習する対象があると考える方が便利なため、「エージェントが行動する」「エージェントの状態が変わった」といった表現を用いる。エージェントは単にコンピュータシミュレーション（特にゲーム）においてキャラクターのように学習しているように見えるものに過ぎない。たとえば、グリッドモデルではエージェントが下の右側の図のように移動するのは、単純に状態の列挙で表すこともできる。 $$ \\boxed{\\ 3\\ } \\to \\boxed{\\ 2\\ } \\to \\fcolorbox{black}{darkgray}{\\ 1\\ } $$ $3, 2, 1$を順番にprintするだけでよい。強化学習の最終的に私たちが得たいのは本質的にポリシーであるため、エージェントというものを定義しなくても学習することができる。一言で言えば、エージェントはポリシーの視覚化（実現化）であると言える。\nもちろん、上記の話は理論やコンピュータシミュレーションでの話であり、自動運転のような実際の応用では、ポリシーに従って実際に動くドローンや自動車が必要である。この場合、ドローンや自動車などのロボットや機械がエージェントとなり、それがなければポリシーの学習は不可能である。\n状態 状態stateは確率変数であり、stateの頭文字をとって$S$と表記する。エピソードは時間に沿って順次進行するため、インデックスとして$t$を使用する。したがって、タイムステップが$t$のときのステート関数を$S_{t}$と表記する。初期ステートは通常$t=0$で表される。まとめると、$S_{t}$は時間が$t$のとき、各グリッドに対して次のような関数値を与える関数である。\n$$ S_{t} \\left( \\boxed{ N } \\right) = n,\\quad 1\\le n \\le 16 $$\nこのとき、可能なすべての状態値（状態関数の関数値）の集合を$\\mathcal{S}\\subset \\mathbb{R}$と表記し、その要素を$s$と表記する。\n$$ \\mathcal{S} = \\left\\{ s_{1}, s_{2},\\dots \\right\\} $$\nそれでは上記の格子モデルに対する状態関数は次のようになります。\n$$ S_{t} : \\left\\{ \\fcolorbox{black}{darkgray}{\\ 1\\ } , \\boxed{\\ 2\\ }, \\dots, \\boxed{15}, \\fcolorbox{black}{darkgray}{16} \\right\\} \\to \\mathcal{S} \\\\ S_{t} \\left( \\boxed{\\ n\\ } \\right) = s_{n} = n,\\quad 1\\le n \\le 16 $$\nそれでは時間が$t$のときの状態値が$s_{6}$から次のタイムステップで状態値が$s_{10}$に変わる確率は次のようになります。\n$$ P \\left( S_{t+1} = s_{10} | S_{t} = s_{6} \\right) $$\n到達した瞬間にエピソードが終了する状態をターミナルステートterminal stateと呼びます。上記の格子モデルではターミナルステートは$\\fcolorbox{black}{darkgray}{1}, \\fcolorbox{black}{darkgray}{16}$です。\n行動 行動actionとはエージェントが現在の状態で取ることができる選択肢のことであり、これもまた確率変数です。actionの頭文字を取って$A_{t}$と表記します。上記の格子モデルの例では、$\\boxed{2}$ ~ $\\boxed{15}$の各々で上下左右を選択することができます。可能な全ての行動値（行動関数の関数値）の集合を$\\mathcal{A}\\subset \\mathbb{R}$と表記し、その要素を$a$と表記します。\n$$ \\mathcal{A} = \\left\\{ a_{1}, a_{2}, \\dots \\right\\} $$\nそれではタイムステップ$t$での行動関数は次のようになります。\n$$ A_{t} : \\left\\{ \\uparrow, \\rightarrow, \\downarrow, \\leftarrow \\right\\} \\to \\mathcal{A} \\\\ \\begin{cases} A_{t}(\\uparrow) = a_{1} \\\\ A_{t}(\\rightarrow) = a_{2} \\\\ A_{t}(\\downarrow) = a_{3} \\\\ A_{t}(\\leftarrow) = a_{4} \\end{cases} $$\nエージェントは与えられた状態で確率に従って行動を決定します。例えばタイムステップが$t$のときの状態値が$s_{6}$で行動$a_{1}$を選択した確率は次のようになります。\n$$ P(A_{t} = a_{1} | S_{t} = s_{6}) $$\n方針 方針policyとは状態$s$で行動$a$を決定する確率を全ての$s$と$a$に対して明記したものを言い、$\\pi$で表記します。ゲームや戦争に例えると戦略です。格子モデルの例で行動を決定する確率が$\\dfrac{1}{4}$で全て同じだとすると、方針$\\pi$は次のようになります。\n$$ \\pi \\begin{cases} P(a_{1} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{2} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{2}) = \\dfrac{1}{4} \\\\ \\vdots \\\\ P(a_{2} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{4} | s_{15}) = \\dfrac{1}{4} \\end{cases} \\quad \\text{or} \\quad \\pi : \\mathcal{S} \\times \\mathcal{A} \\to [0,1] $$\nもちろんこれは最適化された方針ではありません。簡単に$\\boxed{2}$の場合だけ考えても、上に行くと格子の外に出てしまうため、上に行く確率自体が全く無い方がより良い方針です。したがって、下の図で$\\pi_{1}$よりも$\\pi_{2}$がより良い方針だと言えます。\n強化学習アルゴリズムの目標は最適な方針を見つけることです。では、最適な方針をどのように見つけるかというと、方針の良さを評価する価値関数value functionを通じて見つけることができます。\n報酬 報酬rewardとは、与えられた状態でエージェントが選択した行動に対して実数をマッピングする関数であり、rewardの頭文字を取って$R_{t}$と表記します。全ての報酬値（報酬関数の関数値）の集合を$\\mathcal{R} \\subset \\mathbb{R}$と表記し、その要素を$r$と表記します。\n$$ \\mathcal{R} = \\left\\{ r_{1}, r_{2}, \\dots \\right\\} \\\\ R_{t} = \\mathcal{S} \\times \\mathcal{A} \\to \\mathcal{R} $$\n報酬は一回のタイムステップごとに一回ずつ受け取り、一回のエピソードで受け取った総報酬、つまり蓄積された報酬が最も大きくなるような方針を見つけることが強化学習の究極的な目標です。\nでは、なぜ各タイムステップの報酬よりも蓄積された報酬が大きくなるようにするのか疑問に思うかもしれません。これは試験勉強に例えると簡単に理解できます。試験期間中に毎晩勉強する代わりにお酒を飲んだり遊んだりゲームをした場合、当面は勉強するよりも楽しいでしょう。しかし、蓄積された報酬、つまり試験の成績は散々なものになります。したがって、今は勉強することが疲れて大変だとしても、将来の大きな報酬のために勉強する方が良いと判断し、試験勉強をするわけです。\n報酬は人が設定するハイパーパラメータです。したがって、エージェントが行うべき仕事に応じて適切に設定する必要があります。例えば、格子モデルの例で格子が迷路であり、エージェントが迷路を脱出するロボットである場合、一マス移動するごとに$-1$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。格子が公園であり、エージェントがペットの散歩をするロボットである場合、一マス移動するごとに$0$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。\n環境 環境environmentとはエージェントが与えられた状態で選択した行動に応じて次の状態と報酬を決定する関数、すなわち$f : (s,a) \\mapsto (s^{\\prime},r)$です。したがって、常に現実にぴったりと当てはまる比喩を見つけるのは難しいです。\nタイムステップが$t$のときの状態を$s_{t}$、$s_{t}$で選択した行動を$a_{t}$とします。これにより、環境が決定した次の状態を$s_{t+1}$、報酬を$r_{t+1}$とすると次のように表されます。\n$$ f(s_{t}, a_{t}) = (s_{t+1}, r_{t+1}) $$\n格子モデルの例について具体的に説明すると、エージェントが$\\boxed{7}$で$\\uparrow$を選択し、環境が次の状態$\\boxed{3}$と報酬$-1$を決定した場合は、次のような数式で\n表されます。\n$$ f(s_{7}, a_{1}) = (s_{3}, -1) $$\nエージェントが行動を決定する戦略を方針と呼ぶならば、環境が次の状態と報酬を決定することをMDPmarkov decision process, マルコフ決定プロセスと言います。エージェントと環境の相互作用を図で表すと次のようになります。\nエピソード エージェントと環境が相互作用しながら決定された状態、行動、報酬の数列を経路trajectory, 軌跡または履歴historyと言います。経路が有限の場合をepisode taskと言います。上で例に挙げた試験期間、囲碁、格子モデルもこれに該当します。\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{T-1}, s_{T}, r_{T} \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{T-1}}{\\to} (s_{T}, r_{T}) $$\n経路が無限の場合をcontinuing taskと言います。ただし、非常に長い時間にわたって続くエピソードは無限の場合とみなされることもあります。\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{t-1}, s_{t}, r_{t}, a_{t}, s_{t+1}, r_{t+1},\\dots \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{t-1}}{\\to} (s_{t}, r_{t}) \\overset{a_{t}}{\\to} (s_{t+1}, r_{t+1}) \\overset{a_{t+1}}{\\to} \\cdots $$\nオ・イルソク, 機械学習(MACHINE LEARNING). 2017, p466-480\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3029,"permalink":"https://freshrimpsushi.github.io/jp/posts/3029/","tags":null,"title":"機械学習における強化学習とは？"},{"categories":"수리통계학","contents":"要約 1 $\\left\\{ X_{k} \\right\\}_{k=1}^{n}$がiid確率変数で、確率分布$\\left( \\mu, \\sigma^2 \\right) $に従うとき、$n \\to \\infty$で $$ \\sqrt{n} {{ \\overline{X}_n - \\mu } \\over {\\sigma}} \\overset{D}{\\to} N (0,1) $$\n$\\overset{D}{\\to}$は分布収束を意味する。 解説 統計学では、大数の法則と共に非常に有名な定理として挙げられる。頻繁に聞き、使われる定理だが、実際に証明するのは数理統計学を学ぶときぐらいだ。しかし、実際には利用度を超えて、証明自体が楽しいため、より価値のある定理だと言えるだろう。\n証明 戦略：モーメント生成関数とテイラーの定理を使ったトリックを使用する。\nまず、$\\displaystyle Y := \\sqrt{n} {{ \\overline{X}_{n} - \\mu } \\over { \\sigma }}$のモーメント生成関数$M(t) = E(e^{t Y}), -h\u0026lt;t\u0026lt;h$が存在すると仮定する。新しい関数$m(t) := E[e^{t(X-\\mu)}] = e^{-\\mu t} M(t)$を定義すると $$ \\begin{align*} M(t) =\u0026amp; E \\left( e^{ t \\sqrt{n} {{ \\overline{X}_n - \\mu } \\over {\\sigma}} } \\right) \\\\ =\u0026amp; E \\left( e^{ t {{ \\sum_{i=1}^{n} X_i - n \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\\\ =\u0026amp; E \\left( e^{ t {{ X_1 - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) E \\left( e^{ t {{ X_2 - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\cdots E \\left( e^{ t {{ X_n - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\\\ =\u0026amp; E \\left( e^{ t {{ X - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) E \\left( e^{ t {{ X - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\cdots E \\left( e^{ t {{ X - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\\\ =\u0026amp; { \\left\\{ E \\left( e^{ t {{ X - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\right\\} }^n \\\\ =\u0026amp; { \\left\\{ m \\left( { {t} \\over {\\sigma \\sqrt{n} } } \\right) \\right\\} } ^{n} \\qquad , -h \u0026lt; { {t} \\over {\\sigma \\sqrt{n} } } \u0026lt; h \\end{align*} $$\nテイラーの定理：関数$f(x)$が$[a,b]$で連続であり、$(a,b)$で$n$回微分可能ならば、$x_{0} \\in (a,b)$に対して$\\displaystyle f(x) = \\sum_{k=0}^{n-1} {{( x - x_{0} )^{k}\\over{ k! }}{f^{(k)}( x_{0} )}} + {(x - x_{0} )^{n}\\over{ n! }}{f^{(n)}(\\xi)}$を満たす$\\xi \\in (a,b)$が存在する。\n$n=2$にテイラーの定理を適用すると、$\\xi$が$(-t,0)$または$(0,t)$の少なくとも一方を満たすことがわかる。 したがって、$m(t)$は $$ m(t) = m(0) + m ' (0)t + { {m '' (\\xi) t^2} \\over {2} } $$ と表せる。一方、 $$ \\begin{cases} m(0)=1 \\\\ m ' (0) = E(X-\\mu) = 0 \\\\ m '' (0) = E[(X-\\mu)^2] = {\\sigma}^2 \\end{cases} $$ であるため、$\\displaystyle m(t) = 1 + { {m '' (\\xi) t^2} \\over {2} }$である。ここでトリックが登場するが、右辺に$\\displaystyle {{\\sigma^2 t^2} \\over {2}}$を加えてから引くと $$ m(t) = 1 + { { \\sigma^2 t^2} \\over {2} } + { { [ m '' (\\xi) - \\sigma^2 ] t^2} \\over {2} } $$ つまり、 $$ M(t) = { \\left\\{ m \\left( { {t} \\over {\\sigma \\sqrt{n} } } \\right) \\right\\} } ^{n} = { \\left\\{ 1 + { { t^2} \\over {2n} } + { { [ m '' (\\xi) - \\sigma^2 ] t^2} \\over {2n \\sigma^2 } } \\right\\} } ^{n} $$\nテイラーの定理により、$\\xi$は$\\displaystyle \\left( -{ {t} \\over {\\sigma \\sqrt{n} } },0 \\right)$または$\\displaystyle \\left( 0,{ {t} \\over {\\sigma \\sqrt{n} } } \\right) $の間にあるため、$n \\to \\infty$のとき$\\xi \\to 0$で、したがって$ m '' (\\xi) \\to m '' (0) = \\sigma^2$である。そのようにして収束する項を除去すると\n$$ \\lim _{n \\to \\infty} M(t) = \\lim _{n \\to \\infty} \\left( 1 + { { t^2} \\over {2n} } \\right)^{n} = e^{t^2 / 2} $$\nここで、$e^{t^2 / 2}$は標準正規分布のモーメント生成関数であるため、\n$$ \\sqrt{n} {{ \\overline{X}_n - \\mu } \\over {\\sigma}} \\overset{D}{\\to} N (0,1) $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): 313~315.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":43,"permalink":"https://freshrimpsushi.github.io/jp/posts/43/","tags":null,"title":"中心極限定理の証明"},{"categories":"행렬대수","contents":"定義 $A$を正方実数行列としよう。$A$が下の式を満たす場合、直交行列orthogonal matrixという。\n$$ A^{-1} = A^{T} $$\nこの条件を別の形で表すと次のようになる。\n$$ AA^{T} = A^{T}A =I $$\n説明 定義を言葉で解くと、直交行列とは、それぞれの行ベクトルまたは列ベクトルが互いに直交する単位ベクトルである行列だ。複素数行列に拡張した場合は、ユニタリ行列と呼ばれる。直交行列の具体的な例には、回転行列がある。2次元平面上のベクトルを反時計回りに$\\theta$だけ回転させる変換は以下のようになる。\n$$ A = \\begin{bmatrix} \\cos \\theta \u0026amp; -\\sin \\theta \\\\ \\sin \\theta \u0026amp; \\cos \\theta \\end{bmatrix} $$\n下の式により、回転変換は任意の$\\theta$に対して直交行列であることがわかる。\n$$ A^{T} A = \\begin{bmatrix} \\cos \\theta \u0026amp; -\\sin \\theta \\\\ \\sin \\theta \u0026amp; \\cos \\theta \\end{bmatrix} \\begin{bmatrix} \\cos \\theta \u0026amp; \\sin \\theta \\\\ -\\sin \\theta \u0026amp; \\cos \\theta \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} = I $$\n性質 直交行列の転置も直交行列である。\n直交行列の逆行列は直交行列である。\n二つの直交行列の積は直交行列である。\n直交行列の行列式は$1$または$-1$である。\n$$ \\det(A)=\\pm 1 $$\n直交行列の同値条件 $n \\times n$実数行列$A$について、以下の命題はすべて同値である。\n$A$は直交行列である。\n$A$の行ベクトルの集合は$\\mathbb{R}^n$の正規直交集合である。\n$A$の列ベクトルの集合は$\\mathbb{R}^n$の正規直交集合である。\n$A$は内積を保存する。つまり、すべての$\\mathbf{x},\\mathbf{y}\\in \\mathbb{R}^{n}$に対して以下が成り立つ。\n$$ (A \\mathbf{x}) \\cdot (A\\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y} $$\n$A$は長さを保存する。つまり、すべての$\\mathbf{x}\\in \\mathbb{R}^{n}$に対して以下が成り立つ。 $$ \\left\\| A \\mathbf{x} \\right\\| = \\left\\| \\mathbf{x} \\right\\| $$\n","id":3009,"permalink":"https://freshrimpsushi.github.io/jp/posts/3009/","tags":null,"title":"直交行列"},{"categories":"행렬대수","contents":"定義：二つの列ベクトルの内積1 大きさが$n \\times 1$の二つの列ベクトル$\\mathbf{u}$、$\\mathbf{v}$ $\\in \\mathbb{R}^{n}$の内積inner productを以下のように定義する。\n$$ \\begin{equation} \\mathbf{u} \\cdot \\mathbf{v} := \\mathbf{u}^{T}\\mathbf{v}=u_{1}v_{1} + u_{2}v_{2} + \\cdots + u_{n}v_{n} \\label{EuclideanIP} \\end{equation} $$\n$\\mathbf{u}$、$\\mathbf{v}$ $\\in \\mathbb{C}^{n}$の場合は、以下のようになる。\n$$ \\mathbf{u} \\cdot \\mathbf{v} := \\mathbf{u}^{\\ast}\\mathbf{v}=u^{\\ast}_{1}v_{1}^{\\ } + u_{2}^{\\ast}v_{2}^{\\ } + \\cdots + u_{n}^{\\ast}v_{n}^{\\ } $$\nここで、$\\mathbf{u}$は$\\mathbf{u}$の共役転置である。二つのベクトル$\\mathbf{u}$、$\\mathbf{v}$が次の方程式を満たす時、$\\mathbf{u}$と$\\mathbf{v}$が直交orthogonalすると言い、$\\mathbf{u} \\perp \\mathbf{v}$と表される。\n$$ \\mathbf{u} \\cdot \\mathbf{v} = 0 $$\n列ベクトル$\\mathbf{v}$のノルムnormまたは長さlengthを以下のように定義する。\n$$ \\left\\| \\mathbf{v} \\right\\| := \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} $$\nノルムが$1$のベクトルを単位ベクトルunit vecterと呼ぶ。二つのベクトル$\\mathbf{u}$、$\\mathbf{v}$の距離を$d(\\mathbf{u}. \\mathbf{v})$として表し、以下のように定義する。\n$$ d(\\mathbf{u}, \\mathbf{v}) := \\left\\| \\mathbf{u} - \\mathbf{v} \\right\\| = \\sqrt{(\\mathbf{u}-\\mathbf{v}) \\cdot (\\mathbf{u}-\\mathbf{v})} = \\sqrt{(\\mathbf{u}-\\mathbf{v})^{\\ast} (\\mathbf{u}-\\mathbf{v})} $$\n説明 座標空間で二つのベクトルの内積は行列の積として再表現されたものと、複素数まで拡張したものに過ぎない。したがって、$\\eqref{EuclideanIP}$をユークリッド内積Euclidean inner productまたは標準内積standard inner productと呼ぶ。だから、内積の記法には$\\cdot$を使うこともあるが、一般的な内積の記法は以下のようである。\n$$ \\left\\langle \\mathbf{u}, \\mathbf{v} \\right\\rangle $$\n定義により、実数行列の場合は$\\mathbf{u} \\cdot \\mathbf{v} = \\mathbf{v} \\cdot \\mathbf{u}$が成立し、複素数行列の場合は$\\mathbf{u} \\cdot \\mathbf{v} = \\overline{\\mathbf{v} \\cdot \\mathbf{u}}$が成立する。\n内積の核心的な概念は「同じ成分同士を掛け合わせて全てを足す」であり、これを$n\\times n$行列に対して一般化すると以下のようになる。\n性質 $A$を$n\\times n$実数行列、$\\mathbf{u},\\mathbf{v}$を$n\\times 1$実数行列とする。すると、次の式が成立する。\n$$ \\begin{align*} A \\mathbf{u} \\cdot \\mathbf{v} \u0026amp;= \\mathbf{u} \\cdot A^{T} \\mathbf{v} \\\\ \\mathbf{u} \\cdot A \\mathbf{v} \u0026amp;= A^{T} \\mathbf{u} \\cdot \\mathbf{v} \\end{align*} $$\n複素数行列の場合、次の式が成立する。\n$$ \\begin{align*} A \\mathbf{u} \\cdot \\mathbf{v} \u0026amp;= \\mathbf{u} \\cdot A^{\\ast} \\mathbf{v} \\\\ \\mathbf{u} \\cdot A \\mathbf{v} \u0026amp;= A^{\\ast} \\mathbf{u} \\cdot \\mathbf{v} \\end{align*} $$\n証明 四つの式の証明方法は同じだから、最初の式の証明だけ紹介する。\n転置行列の性質により、以下が成立する。\n$$ \\begin{align*} A \\mathbf{u} \\cdot \\mathbf{v} \u0026amp;= \\left( A \\mathbf{u} \\right)^{T} \\mathbf{v} \\\\ \u0026amp;= \\left( \\mathbf{u}^{T} A^{T} \\right) \\mathbf{v} \\\\ \u0026amp;= \\mathbf{u}^{T} \\left( A^{T} \\mathbf{v} \\right) \\\\ \u0026amp;= \\mathbf{u} \\cdot A^{T} \\mathbf{v} \\end{align*} $$\n■\n参照 内積の一般的な定義 ノルムの一般的な定義 距離の一般的な定義 内積とノルムと距離の関係 Howard Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p342\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3011,"permalink":"https://freshrimpsushi.github.io/jp/posts/3011/","tags":null,"title":"行列の内積"},{"categories":"행렬대수","contents":"定義 $A$をサイズが$m \\times n $の複素数行列とする。$\\overline{A}$を次のように定義して、$A$の共役行列conjugate matrixと呼ぶ。\n$$ \\overline{A} :=\\begin{bmatrix} \\overline{a_{11}} \u0026amp; \\overline{a_{12}} \u0026amp; \\cdots \u0026amp; \\overline{a_{1n}} \\\\ \\overline{a_{21}} \u0026amp; \\overline{a_{22}} \u0026amp; \\cdots \u0026amp; \\overline{a_{2n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\overline{a_{m1}} \u0026amp; \\overline{a_{m2}} \u0026amp; \\cdots \u0026amp; \\overline{a_{mn}} \\end{bmatrix} = \\left[ \\overline{a_{ij}} \\right] $$\nここで、$\\overline{a}$は$a$の共役複素数だ。つまり、各要素がある行列の要素の共役複素数である行列を共役行列という。$A$をサイズが$m\\times n$の複素数行列としよう。$A^{\\ast}$を次のように定義して、$A$の共役転置conjugate transposeと呼ぶ。\n$$ A^{\\ast} := \\overline{A^{T}} = \\left( \\overline{A} \\right) ^{T} $$\n説明 $A^{\\ast}$以外に使われる表記法としては、$A^{\\dagger}$、$A^{H}$がある。$A^{\\dagger}$は[エイダガー]と読み、$A^{H}$の$H$はエルミート行列から取られている。物理学、特に量子力学では、$A^{\\ast}$を共役行列の意味でのみ使うこともある。それゆえ、$A^{\\dagger}=(A^{\\ast})^{T}$として表記する。一方、数値線形代数などでは、逆行列ではないが逆行列のように振る舞う\u0026rsquo;擬似逆行列\u0026rsquo;の表記として$A^{\\dagger}$を使う。線形代数が非常に広く使われているため、このような記法問題は、その時に勉強している科目にしっかりと従って、注意深く対処するしかない。\n性質1 $A,B$を任意の複素数行列、$k\\in \\mathbb{C}$とする。\n(a) $\\overline{\\overline{A}}=A$\n(b) $\\overline{(AB)} = \\overline{A}\\ \\overline{B}$\n(c) $(A^{\\ast})^{\\ast}=A$\n(d) $\\left( A \\pm B\\right)^{\\ast} = A^{\\ast} \\pm B^{\\ast}$\n(e) $(kA)^{\\ast}=\\overline{k}A^{\\ast}$\n(f) $\\left( AB \\right)^{\\ast} = B^{\\ast} A^{\\ast}$\n証明 (a) (b) 共役複素数と行列乗算の定義により自明。\n■\n(c) (d) (e) (a)、転置行列の性質 $ \\left( A^{T} \\right) ^{T} = A $ 、行列の足し算の定義により成り立つ。\n■\n(f) (b)、転置行列の性質$\\left( AB \\right) ^{T} = B^{T} A^{T}$により成り立つ。\n■\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p437\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3006,"permalink":"https://freshrimpsushi.github.io/jp/posts/3006/","tags":null,"title":"共役転置行列"},{"categories":"복소해석","contents":"定義 開集合$A \\subset \\mathbb{C}$と$f: A \\to \\mathbb{C}$が定義されていて、$\\alpha \\in A$としよう。\n$\\displaystyle \\lim_{z \\to \\alpha } f(z) = f (\\alpha)$ならば、$f$は$\\alpha$で連続だといい、複素領域 $\\mathscr{R}$の全ての点で連続ならば、$f$は$\\mathscr{R}$上で連続だという。特に$f$が定義域上で連続ならば、連続関数と呼ばれる1。\n$\\alpha$での$f$の微分係数を以下のように定義し、$\\alpha$で微分係数が存在すれば、$f$は$\\alpha$で微分可能であるという。 $$ f ' (\\alpha) := \\lim_{h \\to 0} {{ f ( \\alpha + h ) - f ( \\alpha ) } \\over { h }} $$ ここで$h \\in \\mathbb{C}$とし、複素平面上のどの方向でも関係なくなければならない。\n$f$が複素領域 $\\mathscr{R}$の全ての点で微分可能ならば、$f$は$\\mathscr{R}$で解析的であるという。特に$f:\\mathbb{C} \\to \\mathbb{C}$が$\\mathbb{C}$で解析的ならば、全解析Entire関数という2。\n説明 実数集合$\\mathbb{R}$を定義域とする関数とは異なり、一般的に$\\mathbb{C}$を定義域とする関数も同じ幾何学的意味を持つわけではないが、形式的な定義上、複素解析における微分が微分と呼ばれる理由が全くないわけではない。もちろん、複素平面として$\\mathbb{C} \\simeq \\mathbb{R}^{2}$を考えるならば、やはり傾きと似た意味で見ることができる。 解析的関数は、正則関数Regular Function、ホロモーフィック関数Holomorphic Functionとも呼ばれる。しかし、解析的連続の条件として使われる点で、解析的関数という表現が最もメジャーだ。\u0026ldquo;なぜこれを微分可能な関数ではなく、わざわざ解析的関数という言葉を作って呼ぶのか？\u0026ldquo;については、複素解析が発展した当時の視点が入っていると見ることができるだろう。言及したように、複素平面での微分というのは、形式的な定義であって、我々が実数空間$\\mathbb{R}$で扱ったように考えるべきではない、という意味だったのではないかと思われる。 Osborne (1999). Complex variables and their applications: p39.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOsborne (1999). Complex variables and their applications: p50.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1929,"permalink":"https://freshrimpsushi.github.io/jp/posts/1929/","tags":null,"title":"解析関数"},{"categories":"줄리아","contents":"コード Juliaで色を取り扱うために提供されるパッケージはColors.jlだ。可視化パッケージのPlots.jlを読み込めば、Colors.jl内の機能も一緒に使える。RGB空間を表す色コードには、RGB、BGR、RGB24、RGBX、XRGBがサポートされており、これらはAbstractRGBのサブタイプだ。RGBAはRGBに透明度が加わったものだ。\njulia\u0026gt; using Plots\rjulia\u0026gt; subtypes(AbstractRGB)\r5-element Vector{Any}:\rBGR\rRGB\rRGB24\rRGBX\rXRGB\rjulia\u0026gt; subtypes(AbstractRGBA)\r2-element Vector{Any}:\rBGRA\rRGBA 文字列 plot()関数の色を指定するキーワードに文字列として\u0026quot;#FF0000\u0026quot;のように入力すると、16進RGBコードであるHEXコードを使える。下を見ればわかるが、文字列を入力しても良い理由は、plot()が文字列を自動でパースしてくれるからと見られる。\nusing Plots\rr = \u0026#34;#FF0000\u0026#34; # R빨간색 RGB(255, 0, 0)의 6자리 HEX 코드\rg = \u0026#34;#00FF0033\u0026#34; # 투명도가 0.2인 초록색 RGBA(0, 255, 0, 0.2)의 8자리 HEX 코드\rp = \u0026#34;#80F\u0026#34; # 보라색 RGB(255, 0, 136)의 3자리 HEX 코드\rplot([1 2 3; 2 3 4], ones(2, 3), fillrange = 2,\rfillcolor = [r g p],\rlabel = [r g p]) パーシング colorant\u0026quot;#FF0000\u0026quot;のような形でHEXコードをパースできる。\njulia\u0026gt; r = colorant\u0026#34;#FF0000\u0026#34; # R빨간색 RGB(255, 0, 0)의 6자리 HEX 코드\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; g = colorant\u0026#34;#00FF0033\u0026#34; # 투명도가 0.2인 초록색 RGBA(0, 255, 0, 0.2)의 8자리 HEX 코드\rRGBA{N0f8}(0.0,1.0,0.0,0.2)\rjulia\u0026gt; p = colorant\u0026#34;#80F\u0026#34; # 보라색 RGB(255, 0, 136)의 3자리 HEX 코드\rRGB{N0f8}(0.533,0.0,1.0)\rjulia\u0026gt; plot([1 2 3; 2 3 4], ones(2, 3), fillrange = 2,\rfillcolor = [r g p]) parse(RGB, \u0026quot;#FF0000\u0026quot;)のようにパースすることもできる。\njulia\u0026gt; parse(RGB, \u0026#34;#FF0000\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(RGBA, \u0026#34;#FF000080\u0026#34;)\rRGBA{N0f8}(1.0,0.0,0.0,0.502) 色名を取得 hex()関数は色のHEXコードを文字列で返す。\njulia\u0026gt; hex(colorant\u0026#34;red\u0026#34;)\r\u0026#34;FF0000\u0026#34;\rjulia\u0026gt; hex(colorant\u0026#34;rgb(0, 255, 128)\u0026#34;)\r\u0026#34;00FF80\u0026#34;\rjulia\u0026gt; hex(RGBA(1, 0.5, 0, 0.5), :RRGGBB)\r\u0026#34;FF8000\u0026#34;\rjulia\u0026gt; hex(RGBA(1, 0.5, 0, 0.5), :RRGGBBAA)\r\u0026#34;FF800080\u0026#34;\rjulia\u0026gt; hex(HSV(30,1.0,1.0), :AARRGGBB)\r\u0026#34;FFFF8000\u0026#34; 併せて見る Plotsで色を使う方法 RGB色コードを使う方法 HEX色コードを使う方法 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10, FixedPointNumbers v0.8.4 併せて見る 色を使う方法 パレットを使う方法 カラーグラデーションを使う方法 色処理のためのパッケージ Colors.jl RGBコードを使う方法 RGB(1, 0, 0) HEXコードを使う方法 \u0026quot;#000000\u0026quot; グラフ要素の色を指定する方法 サブプロットごとにグラフの色を指定する方法 軸、軸名、目盛り、目盛り値の色を指定する方法 背景色を指定する方法 ","id":1921,"permalink":"https://freshrimpsushi.github.io/jp/posts/1921/","tags":null,"title":"ジュリアで16進数RGBコード（HEX）を使用する方法"},{"categories":"줄리아","contents":"環境 OS: Windows11 バージョン: Julia 1.9.0, DataFrames v1.5.0 ","id":1930,"permalink":"https://freshrimpsushi.github.io/jp/posts/1930/","tags":null,"title":"ジュリアでのデータフレームと2次元配列間の変換方法"},{"categories":"줄리아","contents":"ガイド 旧バージョン julia v1.5.0では、*.csvファイルを以下のように読み込んだ。 実際、Juliaはまだデータ入力に特別便利な言語ではない。しかし、速さを求めるならば、PythonやR、MatlabよりもJuliaを選ばなければならない時が来るかもしれない。例えば、Eドライブ直下にある*.csvファイルを読み込みたい場合、次のように入力すればいい。\nusing CSV\rdata = CSV.read(\u0026#34;E:/example.csv\u0026#34;) 実行結果を見ると、*.csvファイルがデータフレームとしてうまく読み込まれたことが確認できる。\n新バージョン 正確な時期はわからないが、julia v1.7.0以降では、データフレームを別途読み込む必要がある。 using CSV, DataFrames\rdata = CSV.read(\u0026#34;E:/example.csv\u0026#34;, DataFrame) 環境 OS: Windows ","id":1923,"permalink":"https://freshrimpsushi.github.io/jp/posts/1923/","tags":null,"title":"ジュリアで *.csvファイルを読み込む方法"},{"categories":"줄리아","contents":"ガイド Juliaでは並列計算が日常的に使用されるため、場合によってはコンピュータの全リソースを計算に集中させる必要がある。スレッド数を変更する方法はいくつかあるが、最もスタティックで便利な方法は環境変数を編集することだ。\nステップ1. システム環境変数の編集\nWindowsキーまたはWindows+Sを押して「システム環境変数の編集」を探す。\nシステムプロパティというウィンドウが表示されたら、「環境変数」をクリックする。\nステップ2. JULIA_NUM_THREADS を探す\nユーザー変数で上記のような変数を探す。この値がスレッドの数だ。存在する場合は「編集」を、存在しない場合は「新規作成」を選択して ステップ3. に進む。\nステップ3. 変数の値を変更\n上のスクリーンショットで示された部分に、希望するスレッド数を記入する。適切なスレッド数はコンピュータのスペックによって異なるが、私たちは例を扱っているので、$5$個に変更してみよう。\nステップ4. 確認\nusing Base.Threads\rnthreads() 上のコードをJuliaコンソールで実行して確認しよう。\n反映されない場合はまず再起動を試みて、それでもだめならシステム変数で変更を試みよう。\n環境 OS: Windows julia: v1.5.0 ","id":1933,"permalink":"https://freshrimpsushi.github.io/jp/posts/1933/","tags":null,"title":"WindowsでJuliaの並列計算に使用するスレッド数を変更する方法"},{"categories":"행렬대수","contents":"定義 $A$をサイズ$n\\times n$の任意の正方行列としよう。$A$と行列の積が可能で、以下の式を満たす行列$L$を$A$の左逆行列という。\n$$ LA=I_{n} $$\nここで$I_{n}$はサイズ$n\\times n$の単位行列である。$A$と行列の積が可能で、以下の式を満たす行列$R$を$A$の右逆行列という。\n$$ AR=I_{n} $$\n$A$が左/右逆行列を両方持っていれば、これらは互いに等しく$A^{-1}$と表記され、$A$の逆行列という。\n$$ A^{-1}A=I_{n}=AA^{-1} $$\n$A$が逆行列を持つ場合、$A$を可逆行列または非特異行列という。$A$が逆行列を持たない場合、$A$を特異行列という。\n説明 定義により、$LA$のサイズが$n\\times n$でなければならないので、$L$は必ず$n \\times n$行列でなければならず、$R$も同様である。$A$を正方行列に限定した理由は、$A^{-1}$が$A$の両側から掛けられる必要があるためである。同様に行列の積は交換可能ではないため、左/右逆行列が両方存在する必要がある。実際に、任意の行列が左/右逆行列を持つ場合、これらは常に同じである。\n性質 $A$と$B$を任意の$n \\times n$正方行列としよう。すると、以下が成り立つ。\n(a) $A$が左逆行列$L$と右逆行列$R$を持つ場合、これらは同じである。\n$$ L=A^{-1}=R $$\n(b) $A$の逆行列が存在する場合、それは一意である。\n(c) $AB = I \\iff BA = I$\n(d) $A$と$B$を可逆行列としよう。すると、2つの行列の積$AB$も可逆であり、その逆行列は次の通りである。\n$$ (AB)^{-1}=B^{-1}A^{-1} $$\n(d\u0026rsquo;) 同じサイズの可逆行列の積も可逆であり、その逆行列はそれぞれの逆行列を逆順に掛けたものと同じである。つまり、$A_{1},A_{2},\\dots,A_{n}$が可逆行列ならば、次が成り立つ。\n$$ \\left( A_{1}A_{2}\\cdots A_{n} \\right)^{-1} = A_{n}^{-1}\\cdots A_{2}^{-1} A_{1}^{-1} $$\n(e) $AB$が可逆ならば、$A$と$B$も可逆である。\n(f) $A$が可逆ならば、転置も可逆であり、その逆行列は次の通りである。\n$$ \\left( A^{T} \\right)^{-1} = \\left( A^{-1} \\right)^{T} $$\n従って、(c) $\\iff$ (d) であることがわかる。\n証明 (a) $n\\times n$行列$A$が与えられたとしよう。$L$が$A$の左逆行列だとしよう。すると、下記の式が成り立つ。\n$$ LA=I_{n} $$\n$R$を$A$の右逆行列としよう。$R$を上記式の右辺に掛けると、次のようになる。\n$$ LAR = I_{n}R =R $$\nしかし、$R$は$A$の右逆行列であるため、$LAR=LI_{n}=L$が成り立つ。従って、上記の式は次のようになる。\n$$ L=R $$\n■\n(b) 任意の正方行列$A$が異なる2つの逆行列$B$と$C$を持つと仮定しよう。それから下記の計算ができる。\n$$ B=BI=B(AC)=(BA)C=IC=C $$\nしかし、この結果は$B$と$C$が異なるという仮定に矛盾する。従って、仮定は誤りであり、逆行列が存在する場合、それは一意である。\n■\n(c) 一般性を失うことなく、$BA = I \\implies AB = I$だけを証明しよう。$BA = I$と仮定しよう。これから式$A \\mathbf{x} = \\mathbf{0}$を考える。\n$$ \\begin{align*} A\\mathbf{x} = \\mathbf{0} \u0026amp;\\implies B(A\\mathbf{x}) = B \\mathbf{0} \\\\ \u0026amp;\\implies (BA)\\mathbf{x} = \\mathbf{0} \\end{align*} $$\nここで、$BA = I$と仮定したので、$\\mathbf{x} = \\mathbf{0}$である。従って、$A \\mathbf{x} = \\mathbf{0}$は自明の解だけを持つ。\n可逆行列の同値条件\n$A$をサイズ$n\\times n$の正方行列としよう。すると、以下の命題は全て同値である。\n$A$は可逆行列である。 同次線形システム$A\\mathbf{x}=\\mathbf{0}$は自明の解だけを持つ。 可逆行列の同値条件により、$A$は可逆である。従って、$A^{-1}$が存在し、\n$$ BA = I \\implies A(BA)A^{-1} = AIA^{-1} \\implies AB = I $$\n■\n(d) $A$と$B$をサイズ$n\\times n$の可逆行列としよう。その時、$A^{-1}$と$B^{-1}$が存在する。まず、$B^{-1}A^{-1}$を$AB$の右に掛けてみよう。それは次のようになる。\n$$ \\begin{align*} (AB)(B^{-1}A^{-1}) \u0026amp;= ABB^{-1}A^{-1} \\\\ \u0026amp;= AI_{n}A^{-1} = AA^{-1} \\\\ \u0026amp;= I_{n}\\end{align*} $$\n左に掛けると、次のようになる。\n$$ \\begin{align*} (B^{-1}A^{-1})(AB) \u0026amp;= B^{-1}A^{-1}AB \\\\ \u0026amp;= B^{-1}I_{n}B = B^{-1}B \\\\ \u0026amp;= I_{n}\\end{align*} $$\n従って、$AB$は可逆行列であり、その逆行列は$B^{-1}A^{-1}$である。\n■\n(d') これは**(d)**の帰結として成立する。\n■\n(e) $AB$の逆行列を$C$としよう。すると、$ABC=I_{n}$が成り立つ。従って、(c)により、$A$は可逆であり、$A^{-1}=BC$が成り立つ。また、$CAB=I_{n}$であるため、$B$も可逆であり、$B^{-1}=CA$が成り立つ。\n(f) 2つの行列を掛け合わせて単位行列が出るか確認すればよい。転置行列の性質により、次のようになる。\n$$ A^{T} \\left( A^{-1} \\right)^{T} = \\left( A^{-1} A \\right) ^{T} = I^{T} = I $$\n$$ \\left( A^{-1} \\right)^{T} A^{T} = \\left( A A^{-1} \\right)^{T} = I^{T} = I $$\n従って\n$$ \\left( A^{T} \\right)^{-1} = \\left( A^{-1} \\right)^{T} $$\n■\n","id":3003,"permalink":"https://freshrimpsushi.github.io/jp/posts/3003/","tags":null,"title":"逆行列、可逆行列"},{"categories":"줄리아","contents":"ガイド ジュリアを使っている人なら、サーバーを含む複数のオペレーティングシステムやコンピューターを使うことに慣れている可能性が高い。ファイル入出力がある場合、開発環境が変わるたびにそのパスを設定するのはとても面倒くさいだろう。これを解決してくれるのが@__DIR__マクロだ。例えば、次のようなジュリアのコードファイルがあるとしよう。\n基本的に、ターミナルから実行するとき、pwd()と@__DIR__は以下のように区別されないように見える。\nこれらの違いは、アトムなどのIDE(統合開発環境)を使う時に出る。pwd()は単に現在の作業ディレクトリを返すのに対し、@__DIR__は実際のコードファイルがある場所を教えてくれるのがその違いだ。複雑で繰り返しの多い作業をしていると、作業ディレクトリをこっちに変えたり、あっちに変えたりすることは多いが、実行されているコードファイルの場所が変わることはあまりないから、便利に使える。\n","id":1935,"permalink":"https://freshrimpsushi.github.io/jp/posts/1935/","tags":null,"title":"ジュリアで実行されるコードファイルの位置を確認する方法"},{"categories":"줄리아","contents":"ガイド Juliaでは、並列計算を日常的に使用するため、場合によってはコンピューターの全てのリソースを計算に集中させる必要がある。スレッド数を変更する方法はいくつかあるが、最もスタティックで便利な方法は、環境変数を編集することだ。\nステップ1. システム環境変数の編集\nCtrl + Alt + T を押してターミナルを開き、gedit ~/.bashrcと入力する。そうすると、以下のように環境変数を編集できるウィンドウが表示される。\nステップ2. 修正\n一番下にexport JULIA_NUM_THREADS=5を追加する。スクリーンショットで指示されている場所に希望のスレッド数を記入すると修正される。適切なスレッド数はコンピュータのスペックによって異なるが、我々は例を扱っているので、偶然に決まることがないように$5$個に修正してみよう。\nステップ3. 確認\nusing Base.Threads\rnthreads() 上記のコードをJuliaコンソールで実行して確認してみよう。\n","id":1937,"permalink":"https://freshrimpsushi.github.io/jp/posts/1937/","tags":null,"title":"Linux上のJuliaでの並列計算に使用するスレッド数の変更方法"},{"categories":"행렬대수","contents":"定義1 サイズが$m\\times n$の行列を$A$としよう。$A$の行と列を入れ替えた行列を$A$の転置行列transpose, 転置と言い、$A^{T}$または$A^{t}$と表記する。\n説明 定義に従えば、$A$が$m \\times n$行列ならば、$A^{T}$は$n \\times m$行列になる。また、$A$の$i$番目の行は$A^{T}$の$i$番目の列と同じで、その逆もまた然りだ。\n$$ A=\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} ,\\quad A^{T} = \\begin{bmatrix} 10 \u0026amp; 0 \\\\ 0 \u0026amp; 8 \\\\ 3 \u0026amp; 22 \\end{bmatrix} $$\n主対角線を基準に左右対称にしたと考えることもできる。\n性質 $r,s\\in \\mathbb{R}$且つ$A,B$がそれぞれの場合において行列の演算がうまく定義されるサイズであるとしよう。すると、以下が成り立つ。\n(a) 線形性: $$\\left( rA + sB\\right)^{T}=r A^{T} + s B^{T}$$\n(b) 積の転置は、転置を逆順に掛けたものと同じである。\n$$ (AB)^{T}=B^{T}A^{T} $$\n(b\u0026rsquo;) 複数の行列の積の転置は、それぞれの転置を逆順に掛けたものと同じである。\n$$ \\left( A_{1} A_{2}\\cdots A_{n} \\right)^{T} = A_{n}^{T} \\cdots A_{2}^{T} A_{1}^{T} $$\n証明 (b) $m\\times n$行列$A$と$n\\times k$行列$C$について\n$$ \\begin{align*} \\left[ { \\left( AC \\right) }^{ T } \\right] _{ km } \u0026amp;= \\sum _{ i=1 }^{ n }{ [A] _{ m i } { [C] } _{ i k } } \\\\ \u0026amp;= \\sum _{ i=1 }^{ n }{ { \\left[ { A }^{ T } \\right] } _{ i m } { \\left[ { C }^{ T } \\right] } _{ k i } } \\\\ \u0026amp;= \\sum _{ i=1 }^{ n }{ { \\left[ { C }^{ T } \\right] } _{ k i }{ \\left[ { A }^{ T } \\right] } _{ i m } } \\\\ \u0026amp;= { \\left[ { C }^{ T } { A }^{ T } \\right] } _{ km } \\end{align*} $$\n従って、各成分が互いに等しい場合、行列は等しく次の式が成り立つ。\n$$ \\left( AC \\right) ^{ T } = { C }^{ T } { A }^{ T } $$\n■\n(b') これは**(b)**の帰結として成り立つ。\n■\nJim Hefferon, Linear Algebra(4th Edition). 2020, p138\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3002,"permalink":"https://freshrimpsushi.github.io/jp/posts/3002/","tags":null,"title":"転置行列"},{"categories":"줄리아","contents":"コード julia\u0026gt; f(x) = 2x + 1\rf (generic function with 1 method)\rjulia\u0026gt; g(x) = x^2\rg (generic function with 1 method)\rjulia\u0026gt; (g ∘ f)(3)\r49 説明 Juliaでは、関数の合成はプログラミングでのパイプオペレーターに似ている。この合成の最大の利点は、数学者が式をコードとして表現しやすくなることだ。上の例は、以下の数式をコードに翻訳したものに過ぎない。\n$$ f(x) := 2x + 1 \\\\ g(x) := x^2 \\\\ (g \\circ f) (3) $$\n最近の多くの言語がそうであるように、関数をファーストクラスオブジェクトとして扱うことは同じだが、純粋数学で関数空間を扱うように、もう少し極端な哲学を経て文法にまで発展したと見ることができる。ちなみに、関数合成演算子はtexの文法そのままに\\circを入力することで使用できる。\n","id":1942,"permalink":"https://freshrimpsushi.github.io/jp/posts/1942/","tags":null,"title":"ジュリアで合成関数を使用する方法"},{"categories":"행렬대수","contents":"定義 大きさが$n\\times n$で、対角成分がすべて$1$の対角行列を 単位行列identity matrixあるいは 単位行列unit matrixと言い、$I_{n}$または$I_{n\\times n}$と表記する。\n$$ I_{n\\times n}= \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\end{bmatrix} $$\n説明 単位行列は行列の積における単位元である。つまり任意の$n\\times n$行列$A$に対して以下の式が成り立つ。\n$$ I_{n}A=A=AI_{n} $$\n性質 行列式 単位行列は対角行列なので、行列式は$1$である。\n$$ \\det I = 1 $$\n","id":3001,"permalink":"https://freshrimpsushi.github.io/jp/posts/3001/","tags":null,"title":"同一行列、単位行列"},{"categories":"수리통계학","contents":"定義 1 確率変数 $X$ と確率変数のシーケンス $\\left\\{ X_{n} \\right\\}$ が次を満たす場合、$n \\to \\infty$ の時、$X_{n}$ へ分布収束Convergence in Distributionすると言い、$X_{n} \\overset{D}{\\to} X$ と表示される。 $$ \\lim_{n \\to \\infty} F_{X_{n}} (x) = F_{X} (x) \\qquad, \\forall x \\in C_{F_{X}} $$\n$F_{X}$ は確率変数$X$の累積分布関数である。 $C_{F_{X}}$ は関数$F_{X}$が連続である点の集合を示している。 説明 分布収束は、分布のセンスで収束を定義した概念であり、確率収束と同様である。それぞれの$x \\in C_{F_{X}}$に対する収束は、実際には解析学で言う関数の点収束と似ており、この類似点は、確率収束するならば分布収束するという事実にもつながる。\n注意すべきは、分布収束と言っても、$X_{n} \\overset{D}{\\to} X$ で正確に表されるように、「分布収束」もまた「確率変数の収束」について議論したいということだ。分布関数が連続部分で点収束するということは、正確には確率変数が収束するわけではなく、それが持つ性質の一つである分布が収束するということである。当然ながら、これは確率変数自体の収束よりもはるかにゆるい前提となる。分布の観点から違いがないとしても、確率変数が本質的に収束するわけではない。\n実際には、$X_{n} \\overset{D}{\\to} X$ としても $Y_{n} \\overset{D}{\\to} Y$ と $X_{n} + Y_{n}$ が$X + Y$ に分布収束することが保証されるわけではない。確率収束とは異なり、分布収束は累積分布関数の点収束という軽い条件だけで充分であり、そのためにこれら常識的な性質すら持たない。\n理論 $X_{n} \\overset{D}{\\to} X$ とする。\n[1] 連続写像の定理：連続関数$g$について $$ g\\left( X_{n} \\right) \\overset{D}{\\to} g (X) $$ $$ X_{n} \\overset{P}{\\to} X \\implies X_{n} \\overset{D}{\\to} X $$ [4] スルツキーの定理2: 定数$a,b$と確率変数$A_{n}, B_{n} ,X_{n} , X$に対して$a_{n} \\overset{P}{\\to} a $、$ B_{n} \\overset{P}{\\to} b $、$ X_{n} \\overset{D}{\\to} X $であれば $$ A_{n} + B_{n} X_{n} \\overset{D}{\\to} a + b X $$ 極限分布 一方、$X_{n} \\overset{D}{\\to} X$ であれば、$X$ の分布を$\\left\\{ X_{n} \\right\\}$の漸近Asyptoticまたは極限Limiting分布とも言う。便宜上、$X$の分布をそのまま使うこともあるが、例えば$X \\sim N(0,1)$ であれば次のように表される3。 $$ X_{n} \\overset{D}{\\to} N(0,1) $$\n例 [a] 二項分布の極限分布としてのポアソン分布の導出: $X_{n} \\sim B(n,p)$とする。\n$\\mu \\approx np$であれば $$ X_{n} \\overset{D}{\\to} \\text{Poi} (\\mu) $$ [b] 二項分布の極限分布としての標準正規分布の導出: $X_i \\sim B(1,p)$であり、$Y_n = X_1 + X_2 + \\cdots + X_n$であれば、$Y_n \\sim B(n,p)$である $$ { { Y_n - np } \\over {\\sqrt{ np(1-p) } } }\\overset{D}{\\to} N(0,1) $$ [c] ポアソン分布の極限分布としての標準正規分布の導出: $X_{n} \\sim \\text{Poi} \\left( n \\right)$であり、$\\displaystyle Y_{n} := {{ X_{n} - n } \\over { \\sqrt{n} }}$であれば $$ Y_{n} \\overset{D}{\\to} N(0,1) $$ [d] スチューデントのt分布の極限分布としての標準正規分布の導出: $T_n \\sim t(n)$であれば $$ T_n \\ \\overset{D}{\\to} N(0,1) $$\n極限分布が必要な理由 これらの漸近分布から、分布収束が確率変数自体の収束と呼ぶには不十分であることがわかる。例えば、十分に大きな分布$n$が与えられ、正規分布に近似することができたとしても、その確率変数自体の本質が正規分布を模倣することはできない。$n$がどれだけ大きくても、二項分布は二項分布であり、正規分布は正規分布である。しかし、分布が似ているため、一見して区別がつかないだけである。\nそれでも分布収束を考える理由は、その区別がつかない程度で十分であり、条件でこれ以上妥協する余地がない場合があるからである。前述のように、どれだけ変わっても離散確率分布は連続確率分布になることはない。しかし、弱収束の概念を導入してすぐに離散確率分布を連続確率分布のように使えるならば、考慮しない理由はない。\n証明 [1][4] ■\n[2](../175) ■\n[3](../176) ■\n[a] ■\n[b] ■\n[c] ■\n[d] ■\n厳密な定義 測度論で定義される分布収束 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p306.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p300.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1888,"permalink":"https://freshrimpsushi.github.io/jp/posts/1888/","tags":null,"title":"数理統計学における分布収束"},{"categories":"행렬대수","contents":"対角行列1 $A$をサイズが$n\\times m$の行列としよう。行と列の番号が同じ要素、つまり$a_{ii} (1 \\le i \\le \\min(n,m))$を主対角成分main diagonal elementsという。主対角成分を結ぶ仮想の線を主対角線main diagonal, principal diagonalと言う。\n主対角成分以外の全ての成分が$0$である行列$A$を対角行列diagonal matrixという。\n$$ A = [a_{ij}] = \\delta_{ij} = \\begin{cases} 1 \u0026amp; i=j \\\\ 0 \u0026amp; i \\ne j \\end{cases} $$\nここで、$\\delta$はクロネッカーのデルタだ。\n説明 $$ A=\\begin{bmatrix} \\color{red}{a_{11}} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\color{red}{a_{22}} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\color{red}{a_{33}} \\end{bmatrix} \\quad A=\\begin{bmatrix} \\color{red}{a_{11}} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\color{red}{a_{22}} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\color{red}{a_{33}} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\color{red}{a_{44}} \u0026amp; 0 \\end{bmatrix} $$\n上の例示の通り、正方行列でなくても主対角成分、対角行列を定義できる。\n定義により、対角行列は下三角行列であり、同時に上三角行列でもある。\n性質 べき乗 $A = \\begin{bmatrix} a_{ij}\\end{bmatrix}$をサイズが$n\\times n$の対角行列としよう。すると、$A$のべき乗は次のようになる。\n$$ A^{k}=\\begin{bmatrix} (a_{11})^{k} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; (a_{22})^{k} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; (a_{nn})^{k} \\end{bmatrix} $$\n逆行列 $A$の逆行列は次の通りだ。言い換えれば、べき乗に関する性質は$k$が負の時も自然に拡張される。\n$$ A^{-1} = \\begin{bmatrix} \\dfrac{1}{a_{11}} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\dfrac{1}{a_{22}} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\dfrac{1}{a_{nn}} \\end{bmatrix} $$\n行列式 余因子展開を考えれば、対角行列の行列式は全ての対角成分の積であることが分かる。対角行列$n \\times n$の行列式は、\n$$ \\det [a_{ij}] = a_{11} \\times \\cdots \\times a_{nn} $$\nHoward Anton, Elementary Linear Algebra: Applications Version (12版, 2019), p69-71\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1958,"permalink":"https://freshrimpsushi.github.io/jp/posts/1958/","tags":null,"title":"対角行列"},{"categories":"행렬대수","contents":"定義 任意の行列 $A$の行と列の数が同じならば、行列 $A$を正方行列という。\n説明 正方行列は取り扱いやすく、多くの良い性質がある。\n例 単位行列\n可逆行列\n基本行列\n対称行列\n直交行列\nエルミート行列\nユニタリ行列\n","id":1956,"permalink":"https://freshrimpsushi.github.io/jp/posts/1956/","tags":null,"title":"正方行列"},{"categories":"행렬대수","contents":"スカラー乗算 サイズが$m \\times n$の任意の行列 $A$とスカラー $k$の積は、$A$の各成分に$k$を掛けることで定義され、以下のように表記される。\n$$ kA = k\\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} := \\begin{bmatrix} ka_{11} \u0026amp; ka_{12} \u0026amp; \\cdots \u0026amp; ka_{1n} \\\\ ka_{21} \u0026amp; ka_{22} \u0026amp; \\cdots \u0026amp; ka_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; ka_{m2} \u0026amp; \\cdots \u0026amp; ka_{mn} \\end{bmatrix} $$\n定義により、スカラーと行列の積は交換関係が成り立つ。ただし、通常はスカラーを前に書く。\n$$ kA = Ak $$\n加算 サイズが$m \\times n$の二つの行列$A$、$B$の加算は、同じ行、列にある成分同士を足すことで定義され、以下のように表記される。\n$$ \\begin{align*} A+B \u0026amp;= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} + \\begin{bmatrix} b_{11} \u0026amp; b_{12} \u0026amp; \\cdots \u0026amp; b_{1n} \\\\ b_{21} \u0026amp; b_{22} \u0026amp; \\cdots \u0026amp; b_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ b_{m1} \u0026amp; b_{m2} \u0026amp; \\cdots \u0026amp; b_{mn} \\end{bmatrix} \\\\ \u0026amp;:=\\begin{bmatrix} a_{11} + b_{11} \u0026amp; a_{12} + b_{12} \u0026amp; \\cdots \u0026amp; a_{1n} + b_{1n} \\\\ a_{21} + b_{21} \u0026amp; a_{22} + b_{22} \u0026amp; \\cdots \u0026amp; a_{2n} + b_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} + b_{m1} \u0026amp; a_{m2} + b_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} + b_{mn} \\end{bmatrix} \\end{align*} $$\n定義によれば、二つの行列の加算は同じサイズの行列間でのみ定義され、交換関係が成り立つ。\n$$ A+B=BA $$\n乗算 行列にスカラーを掛けることや二つの行列を足すことは直感的に受け入れやすいかもしれないが、乗算の場合は少し異なる。まずは行ベクトルと列ベクトルの積から見ていこう。\nサイズが$1\\times n$の行ベクトル$A=\\begin{bmatrix} a_{1} \u0026amp; a_{2} \u0026amp; \\cdots \u0026amp; a_{n} \\end{bmatrix}$とサイズが$n \\times 1$の列ベクトル$B= \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{bmatrix}$の積を以下のように定義する。\n$$ \\begin{align*} AB =\\begin{bmatrix} a_{1} \u0026amp; a_{2} \u0026amp; \\cdots \u0026amp; a_{n} \\end{bmatrix}\\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{bmatrix} \u0026amp;:= a_{1}b_{1}+a_{2}b_{2} + \\cdots +a_{n}b_{n} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{n}a_{i}b_{i} \\end{align*} $$\nこの定義を言葉で表すと「同じ順序にある成分同士を掛けたものの合計」となるが、これは高校で学んだ二つのベクトルの内積と概念的に同じである。\n$$ \\begin{align*} \\vec{a} \u0026amp;=(a_{1},a_{2},a_{3}) \\\\ \\vec{b} \u0026amp;= (b_{1},b_{2},b_{3}) \\end{align*},\\quad \\vec{a} \\cdot \\vec{b} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} $$\n二つの行列の積は、この概念の拡張と考えることができる。 $m\\times n$行列$A$と$m\\times k$行列$B$の積を以下のように定義する。\n$$ \\begin{align*} AB \u0026amp;= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} \\begin{bmatrix} b_{11} \u0026amp; b_{12} \u0026amp; \\cdots \u0026amp; b_{1k} \\\\ b_{21} \u0026amp; b_{22} \u0026amp; \\cdots \u0026amp; b_{2k} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ b_{n1} \u0026amp; b_{n2} \u0026amp; \\cdots \u0026amp; b_{nk} \\end{bmatrix} \\\\ \u0026amp;:= \\begin{bmatrix} \\sum_{i=1}^{n} a_{1i}b_{i1} \u0026amp; \\sum_{i=1}^{n} a_{1i}b_{i2} \u0026amp; \\cdots \u0026amp; \\sum_{i=1}^{n} a_{1i}b_{ik} \\\\ \\sum_{i=1}^{n} a_{2i}b_{i1} \u0026amp; \\sum_{i=1}^{n} a_{2i}b_{i2} \u0026amp; \\cdots \u0026amp; \\sum_{i=1}^{n} a_{2i}b_{ik} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\sum_{i=1}^{n} a_{mi}b_{i1} \u0026amp; \\sum_{i=1}^{n} a_{mi}b_{i2} \u0026amp; \\cdots \u0026amp; \\sum_{i=1}^{n} a_{mi}b_{ik} \\end{bmatrix} \\end{align*} $$\n数式が長く見えて難しそうだが、行ベクトルと列ベクトルの積を複数回行っただけである。$A$と$B$の積から得られる行列$AB$の$n$行、$k$列の成分は、$A$の$n$行と$B$の$k$列の内積と同じである。従って、$A$の列の数と$B$の行の数が同じでなければ、両者の乗算が定義されない。また、二つの行列の乗算は一般に交換法則が成り立たない。\n$$ AB \\ne BA $$\nこれは簡単な例でも確認できる。$A=\\begin{bmatrix} 1 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \\end{bmatrix}$、$\\begin{bmatrix} 2 \u0026amp; -1 \\\\ 1 \u0026amp; 1 \\end{bmatrix}$とすると、\n$$ \\begin{align*} AB \u0026amp;=\\begin{bmatrix} 2+1 \u0026amp; -1+1 \\\\ 0+1 \u0026amp; 0+1 \\end{bmatrix}=\\begin{bmatrix} 3 \u0026amp; 0 \\\\ 1 \u0026amp; 1 \\end{bmatrix} \\\\ BA \u0026amp;=\\begin{bmatrix} 2+0 \u0026amp; 2-1 \\\\ 1+0 \u0026amp; 1+1 \\end{bmatrix} = \\begin{bmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; 2 \\end{bmatrix} \\end{align*} $$\n従って、\n$$ AB\\ne BA $$\n行列の乗算過程を視覚的に表現すると、次のようになる。\n性質1 $A$、$B$、$C$を任意の$m \\times n$サイズの行列とする。$r$、$s$を任意のスカラーとする。行列演算について、次の性質が成り立つ。\n(a) 加算に対する交換法則: $A + B = B + A$\n(b) 加算に対する結合法則: $A + (B + C) = (A + B) + C$\n(c) $(r + s)A = rA + sA$\n(d) $r(A + B) = rA + rB $\n(e) $(rs)A = r(sA)$\n$A$、$B$、$C$を任意の$n\\times n$サイズの行列とする。\n(f) 乗算に対する結合法則: $A(BC) = (AB)C$\n(g) 乗算に対する分配法則 $A(B+C) = AB + AC \\quad \\\u0026amp; \\quad (A+B)C=AC + BC$\n行列の乗算については、交換法則が成り立たないことを再度注意しよう。\nJim Hefferon, Linear Algebra(4th Edition). 2020, p235\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1957,"permalink":"https://freshrimpsushi.github.io/jp/posts/1957/","tags":null,"title":"行列の演算: スカラー乗法、加法、乗法"},{"categories":"매트랩","contents":"Imagesc imagesc 함수를 사용하면 2차원 배열을 히트맵으로 출력할 수 있다. colorbar는 스케일을 보여주는 컬러바를 같이 출력하는 설정이다.\nN=2^8;\rp=phantom(\u0026#39;Modified Shepp-Logan\u0026#39;,N);\rfigure()\rimagesc(p)\rcolorbar 保存 方法1 saveas 함수를 사용하여 위에서 표시한 figure를 저장할 수 있다. 여기서 gcf 설정은 현재 figure를 의미한다. 그러면 아래 그림이 저장된다.\nN=2^8;\rp=phantom(\u0026#39;Modified Shepp-Logan\u0026#39;,N);\rfigure()\rimagesc(p)\rcolorbar\rsaveas(gcf,\u0026#39;phantom.png\u0026#39;) 方法2 下記の写真のように、figureウィンドウから直接保存することもできる。\n他の言語で ジュリアで ","id":1948,"permalink":"https://freshrimpsushi.github.io/jp/posts/1948/","tags":null,"title":"MATLABで2次元配列をヒートマップ画像として出力および保存する方法"},{"categories":"행렬대수","contents":"定義1 数を次のように長方形の形に並べたものを行列matrixという。\n$$ A=\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} $$\n並べた各々の数をエントリーentryまたは要素elementと呼ぶ。横の列を行row、縦の列を列columnという。また、ある行列が$m$行と$n$列を持つ場合、その行列のサイズを$m \\times n$と表す。\n上の例では、行列$A$は2行と3列を持ち、サイズは$2\\times 3$である。ここで注意すべき点は、$\\times$が乗算を意味するわけではないということである。サイズは必ず総行数と列数が明らかになるように$2\\times 3$のように表記しなければならず、絶対に$6$と書いてはいけない。ちなみに'$2 \\times 3$行列'は [ツーバイスリー行列]と読む。\n表記法 行列は主に下のように角括弧[]または丸括弧()で表記されるが、どちらの表現も一般的に見られる。ただし、手で書くときは丸括弧を使うとキレイに書きにくい。また、2次元、3次元空間の座標を表記するときとは違い、成分と成分の間にカンマ(,)を書かないのが基本である。\n$$ A=\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} \\quad A=\\begin{pmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{pmatrix} $$\n通常、行列は大文字で、成分は小文字で表記する。例えば、行列$A$の1行目3列目の成分は$3$であり、次のように表記される。\n$$ a_{13}=3 $$\n最初の下付き添え字は行の位置を、2番目の下付き添え字は列の位置を示す。同様に、$i$行目、$j$列目の成分が$a_{ij}$である行列を$\\begin{bmatrix} a_{ij} \\end{bmatrix}$のように表記する。$A$の$(i,j)$成分は$[A]_{ij}$と表記される。\n$$ A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\end{bmatrix} = \\begin{bmatrix} a_{ij} \\end{bmatrix},\\qquad [A]_{ij} = a_{ij} $$\nすべての$m\\times n$行列の集合を次のように表記する。\n$$ M_{m \\times n} $$\nサイズが$m\\times n$で成分が実数$\\mathbb{R}$、複素数$\\mathbb{C}$の行列の集合は、それぞれ次のように表記される。\n$$ M_{m\\times n}(\\mathbb{R}),\\quad M_{m \\times n}(\\mathbb{C}) $$\nもう少し抽象的に、成分が体$F$の$n \\times n$行列の集合を$M_{m \\times n}(F)$と表記する。\n列ベクトルと行ベクトル ベクトルとは、数を横または縦に並べたものをいう。この点を考えると、ある行列は列ベクトルまたは行ベクトルを並べたものと見ることができる。例として続けて使用されてきた行列$A$を見てみよう。\n$$ A= \\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} $$\n$A$の各列は列ベクトル$\\begin{bmatrix} 10 \\\\ 0 \\end{bmatrix}$、$\\begin{bmatrix} 0 \\\\ 8 \\end{bmatrix}$、$\\begin{bmatrix} 3 \\\\ 22 \\end{bmatrix}$で構成されていると考えられる。または、各行が行ベクトル$\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\end{bmatrix}$、$\\begin{bmatrix} 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix}$で構成されていると見ることができる。\nJim Hefferon, Linear Algebra(4th Edition). 2020, p15\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1955,"permalink":"https://freshrimpsushi.github.io/jp/posts/1955/","tags":null,"title":"行列の定義"},{"categories":"행렬대수","contents":"定義 数の並びをベクトルと言う。\n説明 通常の教科書では、ベクトルは「大きさと方向を持つ幾何学的なオブジェクト」として学習される。物理学で最初に接する概念だから、$3$次元以下のベクトルに慣れることが避けられない。\n$$ (3,4) = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $$ $$ (x,y,z) = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} $$\nでも実際には、ベクトルはそれより多くの座標に対して一般化が可能だ。単に数を下に更に並べるだけでいいので、例えば時間$t$を考慮した$4$次元のベクトルは、以下のように示すことができる。\n$$ (t,x,y,z) = \\begin{bmatrix} t \\\\ x \\\\ y \\\\ z \\end{bmatrix} $$\n$4$次元以上のベクトルにはどんな意味があるだろうか？例えば、各酸素分子の時間$t$での位置$(x,y,z)$と熱エネルギー$E$を表したいなら、次のように$5$次元に拡張すればいい。\n$$ (t,x,y,z,E) = \\begin{bmatrix} t \\\\ x \\\\ y \\\\ z \\\\ E \\end{bmatrix} $$\n要するに、ベクトルの長さ、つまり次元が高くなることを特に恐れる必要はないということだ。与えられた形式の下での数学の自由な世界では、このような次元の拡張は自然であり、当然のことだ。同じ方法で、$n$次元まで一般化したベクトルを考えることができ、通常ボールド体 $\\mathbf{x}$ を使って示される。\n$$ \\mathbf{x} = \\left( x_{1}, \\cdots , x_{n} \\right) = \\begin{bmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix} $$\nこの簡単な定義から、$n$次元のベクトルは**$n$-組**$n$-tupleと区別されない。物理から離れ、数学に近づくほど、$\\vec{x}$のような矢印を使った表現は少なくなり、抽象的で一般的な数学に入ると、「座標」や「並び」などの表現なしに、厳密で正確な定義が行われる。\n併せて見る ベクトルの難しい定義 ","id":1947,"permalink":"https://freshrimpsushi.github.io/jp/posts/1947/","tags":null,"title":"ベクトルの定義"},{"categories":"수리물리","contents":"公式 $f=f(x,y,z)$をスカラー関数とする。$\\mathbf{A} = A_{x}\\hat{\\mathbf{x}} + A_{y}\\hat{\\mathbf{y}} + A_{z}\\hat{\\mathbf{z}}, \\mathbf{B} = B_{x}\\hat{\\mathbf{x}} + B_{y}\\hat{\\mathbf{y}} + B_{z}\\hat{\\mathbf{z}}$をベクター関数とする。すると、次の式が成り立つ。\nグラディエント勾配\n(a) $\\nabla{(fg)}=f\\nabla{g}+g\\nabla{f}$\n(b) $\\nabla(\\mathbf{A} \\cdot \\mathbf{B}) = \\mathbf{A} \\times (\\nabla \\times \\mathbf{B}) + \\mathbf{B} \\times (\\nabla \\times \\mathbf{A})+(\\mathbf{A} \\cdot \\nabla)\\mathbf{B}+(\\mathbf{B} \\cdot \\nabla) \\mathbf{A}$\nダイバージェンス発散\n(c) $\\nabla \\cdot (f\\mathbf{A}) = f(\\nabla \\cdot \\mathbf{A}) + \\mathbf{A} \\cdot (\\nabla f)$\n(d) $\\nabla \\cdot (\\mathbf{A} \\times \\mathbf{B}) = \\mathbf{B} \\cdot (\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\cdot (\\nabla \\times \\mathbf{B})$\nカール回転\n(e) $\\nabla \\times (f\\mathbf{A}) = (\\nabla f) \\times \\mathbf{A} + f(\\nabla \\times \\mathbf{A})$\n(f) $\\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A})$\n説明 証明全体でアインシュタインの表記法を使用しているので、混乱しないよう注意。つまり、一つの式に同じインデックスが二回出る場合は次のような意味である。\n$$ x_{i}y_{i}=\\sum \\limits_{i=1}^{3} x_{i}y_{i}=x_{1}y_{1}+x_{2}y_{2}+x_{3}y_{3} $$\nまた、クロネッカーのデルタとレビ・チビタ記号を使用することに慣れ、その二つの関係を知っている必要がある。\n証明 (a) グラディエントの定義と微分の性質で簡単に示すことができる。\n$$ \\begin{align*} \\nabla(fg) =\u0026amp;\\ \\dfrac{\\partial (fg)}{\\partial x} \\hat{\\mathbf{x}}+\\dfrac{\\partial (fg)}{\\partial y} \\hat{\\mathbf{y}} +\\dfrac{\\partial (fg)}{\\partial z} \\hat{\\mathbf{z}} \\\\ =\u0026amp;\\ \\left( g\\dfrac{\\partial f}{\\partial x} + f\\dfrac{\\partial g}{\\partial x} \\right) \\hat{\\mathbf{x}} +\\left( g\\dfrac{\\partial f}{\\partial y} + f\\dfrac{\\partial g}{\\partial y} \\right) \\hat{\\mathbf{y}} + \\left( g\\dfrac{\\partial f}{\\partial z} + f\\dfrac{\\partial g}{\\partial z} \\right) \\hat{\\mathbf{z}} \\\\ =\u0026amp;\\ g\\left( \\dfrac{\\partial f}{\\partial x}\\hat{\\mathbf{x}} +\\dfrac{\\partial f}{\\partial y}\\hat{\\mathbf{y}} + \\dfrac{\\partial f}{\\partial z}\\hat{\\mathbf{z}} \\right) + f\\left( \\dfrac{\\partial g}{\\partial x}\\hat{\\mathbf{x}} +\\dfrac{\\partial g}{\\partial y}\\hat{\\mathbf{y}} + \\dfrac{\\partial g}{\\partial z}\\hat{\\mathbf{z}} \\right) \\\\ =\u0026amp;\\ g\\nabla f+ f\\nabla g \\end{align*} $$\n■\n(b) 左辺を直接計算してみると以下のようになる。\n$$ \\begin{align*} \\nabla \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right) =\u0026amp;\\ \\frac{ \\partial \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right)}{ \\partial x_{1}}\\mathbf{e}_{1}+\\frac{ \\partial \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right)}{ \\partial x_{2}}\\mathbf{e}_{2}+\\frac{ \\partial \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right)}{ \\partial x_{3}}\\mathbf{e}_{3} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{3} \\frac{ \\partial \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right)}{ \\partial x_{i}}\\mathbf{e}_{i} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{3} \\frac{ \\partial \\left( \\sum _{j=1}^{3}A_{j}B_{j} \\right)}{ \\partial x_{i}}\\mathbf{e}_{i} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{3}\\sum \\limits_{j=1}^{3} \\frac{ \\partial \\left( A_{j}B_{j} \\right)}{ \\partial x_{i}}\\mathbf{e}_{i} \\end{align*} $$\nこれをアインシュタイン表記法で簡潔に表すと以下のようになる。\n$$ \\nabla (\\mathbf{A} \\cdot \\mathbf{B}) = \\frac{\\partial(A_{j}B_{j})}{\\partial x_{i}}\\mathbf{e}_{i}=\\frac{\\partial A_{j}}{\\partial x_{i}} B_{j}\\mathbf{e}_{i}+A_{j} \\frac{\\partial B_{j}}{\\partial x_{i}} \\mathbf{e}_{i} $$\nさらにクロネッカーのデルタを使って上記式を$X_{i}Y_{i}=X_{i}Y_{j}\\delta_{ij}$のように表せば以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp;\\frac{\\partial A_{j}}{\\partial x_{i}} B_{j}\\mathbf{e}_{i}+A_{j} \\frac{\\partial B_{j}}{\\partial x_{i}} \\mathbf{e}_{i} =\u0026amp;\\ {\\color{blue}\\delta_{jm}}\\frac{ \\partial {\\color{blue}A_{j}}}{ \\partial x_{i}} {\\color{blue}B_{m}} \\mathbf{e}_{i} + {\\color{blue}\\delta_{jm} A_{m}}\\frac{ \\partial {\\color{blue}B_{j}} }{ \\partial x_{i} }\\mathbf{e}_{i} \\\\ \u0026amp;\u0026amp; =\u0026amp;{\\color{red}\\delta_{il}}{\\color{blue}\\delta_{jm}}\\frac{ {\\color{red}\\partial} {\\color{blue}A_{j}}}{ {\\color{red}\\partial x_{i}} } {\\color{blue}B_{m}} {\\color{red}\\mathbf{e}_{l}} + {\\color{red}\\delta_{il}}{\\color{blue}\\delta_{jm} A_{m}} {\\color{red}\\frac{ \\partial {\\color{blue}B_{j}} }{ \\partial x_{i} }} {\\color{red}\\mathbf{e}_{l}} \\\\ \u0026amp;\u0026amp; =\u0026amp;{\\color{red}\\delta_{jl}}{\\color{blue}\\delta_{jm}} \\left( {\\color{red}\\frac{ \\partial {\\color{blue}A_{j}}}{ \\partial x_{i} }} {\\color{blue}B_{m}}\\color{red} {\\mathbf{e}_{l}} + {\\color{blue}A_{m}}{\\color{red}\\frac{ \\partial {\\color{blue}B_{j}} }{ \\partial x_{i} } \\mathbf{e}_{l} }\\right) \\\\ \\implies \u0026amp;\u0026amp; \\nabla \\left( \\mathbf{A} \\cdot \\mathbf{B} \\right) =\u0026amp;\\ \\delta_{jl}\\delta_{jm} \\left( \\frac{ \\partial A_{j} }{ \\partial x_{i} } B_{m} \\mathbf{e}_{l} + A_{m}\\frac{ \\partial B_{j} }{ \\partial x_{i} } \\mathbf{e}_{l} \\right) \\end{align*} $$\nまた、$\\epsilon_{ijk} \\epsilon_{klm} = \\delta_{il} \\delta_{jm} - \\delta_{im} \\delta_{jl}$であるため、上記式を以下のように展開できる。\n$$ \\begin{align*} \\nabla(\\mathbf{A} \\cdot \\mathbf{B}) =\u0026amp;\\ (\\epsilon_{ijk} \\epsilon_{klm} + \\delta_{im} \\delta_{jl}) \\left(\\frac {\\partial A_{j}}{\\partial x_{i}}B_{m} \\mathbf{e}_{l} + A_{m} \\frac{\\partial B_{j}}{\\partial x_{i}}\\mathbf{e}_{l}\\right) \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\epsilon_{klm } \\frac{\\partial A_{j}}{\\partial x_{i}}B_{m} \\mathbf{e}_{l} + \\epsilon_{ijk} \\epsilon_{klm} A_{m} \\frac{\\partial B_{j}}{\\partial x_{i}} \\mathbf{e}_{l} + \\delta_{im} \\delta_{jl} \\frac{\\partial A_{j}}{\\partial x_{i}} B_{m} \\mathbf{e}_{l} + \\delta_{im} \\delta_{jl}A_{m} \\frac{\\partial B_{j}}{\\partial x_{i}} \\mathbf{e}_{l} \\end{align*} $$\nここでレビ・チビタ記号の定義により$\\epsilon_{ijk} \\dfrac{\\partial A_{j}}{\\partial x_{i}}=(\\nabla \\times \\mathbf{A})_{k}$、$\\epsilon_{ijk} \\dfrac {\\partial B_{j}}{\\partial x_{i}}=(\\nabla \\times \\mathbf{B})_{k}$であるため、次の結果を得る。\n$$ \\begin{align*} \\nabla (\\mathbf{A} \\cdot \\mathbf{B}) =\u0026amp;\\ \\epsilon _{ klm }(\\nabla \\times \\mathbf{A})_{ k }B_{ m }\\hat { \\mathbf{e}_{ l } }+\\epsilon _{ klm }A_{ m }(\\nabla \\times \\mathbf{B})_{ k }\\hat { \\mathbf{e}_{ l } }+\\frac { \\partial A_{ j } }{ \\partial x_{ i } }B_{ i }\\hat { e_{ j} }+A_{ i }\\frac { \\partial B_{ j } }{ \\partial x_{ i } }\\hat { \\mathbf{e}_{ j } } \\\\ =\u0026amp;\\ \\mathbf{B}\\times (\\nabla \\times \\mathbf{A})+\\mathbf{A} \\times (\\nabla \\times \\mathbf{B})+(\\mathbf{B} \\cdot \\nabla )\\mathbf{A}+(\\mathbf{A} \\cdot \\nabla )\\mathbf{B} \\end{align*} $$\n■\n(c)​ $$ \\begin{align*} \\nabla \\cdot (f \\mathbf{A}) =\u0026amp;\\ \\delta _{ ij }\\nabla _{ i }(fA_{ j }) \\\\ =\u0026amp;\\ \\delta _{ ij }(\\nabla _{ i }f)A_{ j }+\\delta _{ ij }f(\\nabla _{ i }A_{ j }) \\\\ =\u0026amp;\\ (\\nabla _{ i }f)A_{ i }+f(\\nabla _{ i }A_{ i }) \\\\ =\u0026amp;\\ (\\nabla f)\\cdot \\mathbf{A}+f(\\nabla \\cdot \\mathbf{A}) \\end{align*} $$\n■\n(d) $$ \\begin{align*} \\nabla \\cdot (\\mathbf{A} \\times \\mathbf{B}) \u0026amp;= \\delta _{ ij }\\nabla _{ i }(\\mathbf{A} \\times \\mathbf{B})_{ j } \\\\ \u0026amp;= \\delta _{ ij }\\nabla _{ i }(\\epsilon _{ jkl }A_{ k }B_{ l }) \\\\ \u0026amp;= \\delta _{ ij }\\epsilon _{ jkl }\\nabla _{ i }(A_{ k }B_{ l }) \\\\ \u0026amp;= \\delta _{ ij }\\epsilon _{ jkl }(\\nabla _{ i }A_{ k })B_{ l }+\\delta _{ ij }\\epsilon _{ jkl }A_{ k }(\\nabla _{ i }B_{ l }) \\\\ \u0026amp;= (\\epsilon _{ jkl }\\nabla _{ j }A_{ k })B_{ l }+A_{ k }(\\epsilon _{ jkl }\\nabla _{ j }B_{ l }) \\\\ \u0026amp;= (\\nabla \\times A)_{ l }B_{ l }-A_{ k }(\\nabla \\times B)_{ k } \\\\ \u0026amp;= (\\nabla \\times \\mathbf{A})\\cdot \\mathbf{B}-\\mathbf{A}\\cdot (\\nabla \\times \\mathbf{B}) \\end{align*} $$\n■\n(e)​ $$ \\begin{align*} \\nabla \\times (f\\mathbf{A}) \u0026amp;= \\epsilon _{ ijk }\\nabla _{ i }(fA_{ j })\\mathbf{e}_{k} \\\\ \u0026amp;= \\epsilon _{ ijk }(\\nabla _{ i }f)A_{ j }\\mathbf{e}_{k}+\\epsilon _{ ijk }f(\\nabla _{ i }A_{ j })\\mathbf{e}_{k} \\\\ \u0026amp;= (\\nabla f)\\times \\mathbf{A}+f\\epsilon _{ ijk }(\\nabla _{ i }A_{ j })\\mathbf{e}_{k} \\\\ \u0026amp;= (\\nabla f)\\times \\mathbf{A}+f(\\nabla \\times \\mathbf{A}) \\\\ \u0026amp;= f(\\nabla \\times \\mathbf{A})-\\mathbf{A} \\times (\\nabla f) \\end{align*} $$\n■\n(f) アインシュタイン表記法に慣れていなければ、証明についていくのが難しいかもしれない。\n$$ \\begin{align*} \u0026amp; \\nabla \\times (\\mathbf{A}\\times \\mathbf{B}) \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\nabla_{i} \\left(\\mathbf{A}\\times \\mathbf{B}\\right)_{j} \\mathbf{e}_{k} \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\nabla_{i} (\\epsilon_{jlm} A_{l} B_{m})\\mathbf{e}_{k} \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\epsilon_{jlm} \\nabla_{i} (A_{l} B_{m}) \\mathbf{e}_{k} \\\\ =\u0026amp;\\ \\epsilon_{jki} \\epsilon_{jlm} \\left[ B_{m}(\\nabla_{i} A_{l}) \\mathbf{e}_{k} + A_{l} (\\nabla_{i} B_{m}) \\mathbf{e}_{k} \\right] \\\\ =\u0026amp;\\ (\\delta_{kl} \\delta_{im} - \\delta_{km} \\delta_{il} ) [ B_{m} (\\nabla_{i} A_{l} ) \\mathbf{e}_{k} + A_{l} ( \\nabla_{i} B_{m} ) \\mathbf{e}_{k} ] \\\\ =\u0026amp;\\ \\delta_{kl} \\delta_{im} B_{m} ( \\nabla_{i} A_{l}) \\mathbf{e}_{k} - \\delta_{km} \\delta_{il} B_{m} (\\nabla_{ i} A_{l} ) \\mathbf{e}_{k} + \\delta_{kl} \\delta_{im} A_{l} ( \\nabla_{i} B_{m} ) \\mathbf{e}_{k} - \\delta_{km} \\delta_{il} A_{l} ( \\nabla_{i} B_{m} ) \\mathbf{e}_{k} \\\\ =\u0026amp;\\ B_{i} ( \\nabla_{i} A_{k}) \\mathbf{e}_{k} - B_{k} (\\nabla_{i} A_{i} ) \\mathbf{e}_{k} + A_{k} ( \\nabla_{i} B_{i} ) \\mathbf{e}_{k} - A_{i} ( \\nabla_{i} B_{k} ) \\mathbf{e}_{k} \\\\ =\u0026amp;\\ (\\mathbf{B}\\cdot \\nabla )\\mathbf{A}-(\\nabla \\cdot \\mathbf{A})\\mathbf{B}+\\mathbf{A}(\\nabla \\cdot \\mathbf{B})-(\\mathbf{A}\\cdot \\nabla )\\mathbf{B} \\\\ =\u0026amp;\\ (\\mathbf{B}\\cdot \\nabla )\\mathbf{A}-(\\mathbf{A}\\cdot \\nabla )\\mathbf{B}+\\mathbf{A}(\\nabla \\cdot \\mathbf{B})-\\mathbf{B}(\\nabla \\cdot \\mathbf{A}) \\end{align*} $$\n四行目は$\\epsilon_{jki} \\epsilon_{jlm} = \\delta_{kl} \\delta_{im} - \\delta_{km} \\delta_{il}$により成立する。七行目はアインシュタイン表記法により成立する。\n■\n","id":93,"permalink":"https://freshrimpsushi.github.io/jp/posts/93/","tags":null,"title":"デル演算子を含む乗法則"},{"categories":"줄리아","contents":"コード もともとさくらすし店では、もっと詳しい説明を加えることが多いが、ジュリアでアニメGIFを作るのがどれほど簡単かを強調するために、できるだけ説明を短くする。\nランダムウォークをシミュレーションすることはさておき、上のようなアニメGIFを作ることは、言語によってはとても難しく、大変なことがある。しかし、ジュリアでは@animateマクロとgif()関数を使用することで、信じられないほど簡単にアニメGIFを作ることができる。原理は単純だ。ループの前にマクロを付け、ループを回してその都度フレームを直接描くだけだ。そうして集めたフレームを変数に入れ、gif()関数に入れればそれだけである。fpsオプションでは、秒間フレーム数を指定してアニメGIFの速さを調整できる。\nusing Plots\rrandom\\_walk = cumsum(rand(100).-.5)\ranim = @animate for t in 1:100\rplot(random\\_walk[1:t], legend = :none)\rend; gif(anim, \u0026#34;example.gif\u0026#34;, fps = 10) 別のパスを指定しなければ、ドキュメントに保存されることに注意しよう。これをうまく利用すれば、下に示すような素晴らしいアニメGIFを作ることもできる。\n","id":1863,"permalink":"https://freshrimpsushi.github.io/jp/posts/1863/","tags":null,"title":"ジュリアでGIFを作る方法"},{"categories":"힐베르트공간","contents":"定義1 $\\left( X, \\left\\langle \\cdot, \\cdot \\right\\rangle \\right)$を内積空間としよう。二つの元 $\\mathbf{x}, \\mathbf{y}\\in X$が$\\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle =0$を満たすなら、$\\mathbf{y}$と$\\mathbf{x}$は互いに直交すると言い、以下のように表記する。\n$$ \\mathbf{x} \\perp \\mathbf{y} $$\n元の集合$X$、$\\left\\{ \\mathbf{x}_{k} \\right\\}_{k\\in \\mathbb{N}}$が次の式を満たすなら、直交システムあるいは直交集合と呼ぶ。\n$$ \\left\\langle \\mathbf{x}_{k}, \\mathbf{x}_{\\ell} \\right\\rangle =0\\quad \\forall k\\ne \\ell $$\n直交システム$\\left\\{ \\mathbf{x}_{k} \\right\\}_{k\\in \\mathbb{N}}$が次の式を満たす場合、正規直交システムあるいは正規直交集合と呼ぶ。\n$$ \\left\\| \\mathbf{x}_{k} \\right\\| =1\\quad \\forall k\\in \\mathbb{N} $$\n説明 内積空間で、ノルムは$\\left\\| \\cdot \\right\\|:=\\sqrt{\\left\\langle \\cdot,\\cdot \\right\\rangle }$として定義されるので、正規直交システムの定義を再記すれば以下のようになる。\n$$ \\left\\| \\mathbf{x}_{k} \\right\\| = \\left\\langle \\mathbf{x}_{k},\\mathbf{x}_{\\ell} \\right\\rangle = \\begin{cases} 1 \u0026amp; \\text{if}\\ k=\\ell \\\\ 0 \u0026amp; \\text{if}\\ k\\ne \\ell \\end{cases} $$\nまた、直交システムが可算集合に対して定義される必要は特にない。\n定義2 $A$を任意のインデックス集合、$\\alpha$、$\\beta$を$A$のインデックスとしよう。元の集合$X$、$\\left\\{ \\mathbf{x}_{\\alpha} \\right\\}_{\\alpha \\in A}$が次の式を満たすなら、直交システムあるいは直交集合と呼ぶ。\n$$ \\left\\langle \\mathbf{x}_{\\alpha}, \\mathbf{x}_{\\beta} \\right\\rangle =0\\quad \\forall \\alpha \\ne \\beta $$\n直交システム$\\left\\{ \\mathbf{x}_{\\alpha} \\right\\}_{\\alpha \\in A}$が次の式を満たす場合、正規直交システムあるいは正規直交集合と呼ぶ。\n$$ \\left\\| \\mathbf{x}_{\\alpha} \\right\\| =1\\quad \\forall \\alpha \\in A $$\n説明 従って、正規直交システム$\\left\\{ \\mathbf{x}_{\\alpha} \\right\\}_{\\alpha \\in A}$に対して、以下の式を得る。\n$$ \\left\\langle \\mathbf{x}_{\\alpha},\\mathbf{x}_{\\beta} \\right\\rangle =\\begin{cases} 1 \u0026amp; \\text{if}\\ \\alpha=\\beta \\\\ 0 \u0026amp; \\text{if}\\ \\alpha \\ne \\beta \\end{cases} $$\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p66-67\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWalter Rudin, Real and Complex Analysis (3rd Edition, 1987), p82-83\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1912,"permalink":"https://freshrimpsushi.github.io/jp/posts/1912/","tags":null,"title":"内積空間における直交性、直交集合、正規直交集合"},{"categories":"푸리에해석","contents":"概要 フーリエ変換の定義や記法は、作者のニーズや好みによってさまざまに表れる。だから、教科書や講義、論文などでフーリエ変換を扱う前に、定義や記法をしっかりと把握しておくことが多い。知っている概念だと思って定義を省略して読んでいると、式がおかしいと感じることがあるので、よく確認する必要がある。もちろん、最も重要なのは、これらの定義が本質的に全て同じであることなので、記法や定義自体について大きく心配する必要はない。この文書では、各定義の長所と短所、そして違いについて紹介する。\n説明1 フーリエ変換は、周期が実数全体である関数のフーリエ級数を考える過程から自然に導かれる。その過程で、フーリエ変換とフーリエ逆変換は次のように定義される。\nフーリエ変換 フーリエ逆変換 $\\displaystyle \\hat{f}(\\xi):=\\int _{-\\infty} ^{\\infty}f(x)e^{-i \\xi x}dx$ $\\displaystyle f(x):=\\frac{1}{2\\pi}\\int _{-\\infty} ^{\\infty}\\hat{f}(\\xi)e^{i \\xi x}d\\xi$ ここで、$\\hat{}$は「ハット」と読む。$\\hat{f}(\\xi)$は「エフハット クシ」と読む。作用素としての感じ、積分変換としての感じを強調したいときや、微分を意味する${}^{\\prime}$記号と$\\hat{}$記号を一緒に使う必要があるとき、または混乱を避けるために、以下のような記法で書くこともある。\nフーリエ変換 フーリエ逆変換 $\\mathcal{F}:L^{1} \\to L^{1}$ $\\mathcal{F}^{-1}:L^{1} \\to L^{1}$ $\\displaystyle \\mathcal{F}f(\\xi):=\\int _{-\\infty} ^{\\infty}f(x)e^{-i \\xi x}dx$ $\\displaystyle \\mathcal{F}^{-1}f(x):=\\frac{1}{2\\pi}\\int _{-\\infty} ^{\\infty}f(\\xi)e^{i \\xi x}d\\xi$ $\\mathscr{F}$も使われることがある。記法の違いはあるが、フーリエ変換自体の定義も以下のように異なる場合がある。\nフーリエ変換 フーリエ逆変換 $\\displaystyle \\hat{f}(\\xi):=\\frac{1}{\\sqrt{2\\pi}}\\int _{-\\infty} ^{\\infty}f(x)e^{-i \\xi x}dx$ $\\displaystyle f(x):=\\frac{1}{\\sqrt{2\\pi}}\\int _{-\\infty} ^{\\infty}\\hat{f}(\\xi)e^{i \\xi x}d\\xi$ $\\displaystyle \\hat{f}(\\xi):=\\int _{-\\infty} ^{\\infty}f(x)e^{-2\\pi i \\xi x}dx$ $\\displaystyle f(x):=\\int _{-\\infty} ^{\\infty}\\hat{f}(\\xi)e^{2\\pi i \\xi x}d\\xi$ 説明の便宜のため、上記の各定義を以下のように表記しよう。\n$$ \\tilde{f}(\\xi):=\\frac{1}{\\sqrt{2\\pi}}\\int _{-\\infty} ^{\\infty}f(x)e^{-i \\xi x}dx \\quad \\text{and} \\quad \\check{f}(\\xi):=\\int _{-\\infty} ^{\\infty}f(x)e^{-2\\pi i \\xi x}dx $$\nフーリエ変換の定義が多様な理由は、下記の表から見ることができる。\nプランシェレルの定理 畳み込み 導関数のフーリエ変換 $\\| \\hat{f} \\|^{2} =2\\pi\\left\\| f \\right\\|^{2}$ $(f \\ast g)\\hat{}=\\hat{f}\\hat{g}$ $(f^{\\prime})\\hat{} (\\xi)=i\\xi \\hat{f}(\\xi)$ $\\| \\tilde{f} \\|^{2}=\\left\\| f \\right\\|^{2}$ $(f \\ast g)\\tilde{}=\\sqrt{2\\pi}\\tilde{f}\\tilde{g}$ $(f^{\\prime})\\tilde{} (\\xi)=i\\xi \\tilde{f}(\\xi)$ $\\| \\check{f} \\|^{2}=\\left\\| f \\right\\|^{2}$ $(f \\ast g)\\check{}=\\check{f}\\check{g}$ $(f^{\\prime})\\check{} (\\xi)=2\\pi i\\xi \\check{f}(\\xi)$ この表から見てわかるように、定義によっては、定数$2\\pi$が出現する式が異なる場合がある。したがって、どのような式を簡単にしたいかによって、定義が異なる場合がある。経験的には、信号や画像処理分野では$\\check{f}$のような定義が多く使われる。また、フーリエ変換の定義では、指数にマイナス$(-)$がない場合もある。その場合は、逆変換側に付いているので、知っている定義と異なると混乱しないように注意しよう。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p223-224\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1898,"permalink":"https://freshrimpsushi.github.io/jp/posts/1898/","tags":null,"title":"フーリエ変換の複数の定義と記法"},{"categories":"줄리아","contents":"概要 距離行列Distance Matrixは、パーティクルダイナミクスParticle DynamicsやムービングエージェントMoving Agentベースのシミュレーションなどによく使用されるが、実際には準備された関数がなく、自分で計算するコードを書くことは大変なことが多い。Juliaでは、pairwise()やDistancesパッケージのEuclidean()関数を使用して、以下のように簡単に距離行列を計算できる1。\ndimsオプションを使用すると、行と列の方向を指定できる。見ての通り、$\\mathbb{R}^{5 \\times 3}$行列が与えられたときに、$5$次元ベクトルの$3$個の距離を計算したり、$3$次元ベクトルの$5$個の距離を計算することができる。\nコード using Distances\rcoordinate = [2 3 4; 5 1 3; 1 7 5; 1 7 6; 2 4 3]\rpairwise(Euclidean(), coordinate; dims=1)\rpairwise(Euclidean(), coordinate; dims=2) 上記のコードを実行した結果は、以下の通りである。\njulia\u0026gt; using Distances\rjulia\u0026gt; coordinate = [2 3 4; 5 1 3; 1 7 5; 1 7 6; 2 4 3]\r5×3 Array{Int64,2}:\r2 3 4\r5 1 3\r1 7 5\r1 7 6\r2 4 3\rjulia\u0026gt; pairwise(Euclidean(), coordinate; dims=1)\r5×5 Array{Float64,2}:\r0.0 3.74166 4.24264 4.58258 1.41421\r3.74166 0.0 7.48331 7.81025 4.24264\r4.24264 7.48331 0.0 1.0 3.74166\r4.58258 7.81025 1.0 0.0 4.3589\r1.41421 4.24264 3.74166 4.3589 0.0\rjulia\u0026gt; pairwise(Euclidean(), coordinate; dims=2)\r3×3 Array{Float64,2}:\r0.0 9.64365 7.07107\r9.64365 0.0 3.31662\r7.07107 3.31662 0.0 最適化 距離行列計算の最適化方法 https://discourse.julialang.org/t/pairwise-distances-from-a-single-column-or-vector/29415/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1799,"permalink":"https://freshrimpsushi.github.io/jp/posts/1799/","tags":null,"title":"ジュリアで距離行列を計算する方法"},{"categories":"줄리아","contents":"コード サイズ指定 julia\u0026gt; empty = Array{Float64, 2}(undef, 3, 4)\r3×4 Array{Float64,2}:\r3.39519e-313 3.18299e-313 4.66839e-313 1.061e-313\r4.03179e-313 5.51719e-313 1.6976e-313 4.24399e-314\r2.97079e-313 4.66839e-313 7.00259e-313 5.0e-324 上のコードを実行すると、空の配列が作成される。たまに1.76297e-315のような変な値が入っているように見えるが、これは0に非常に近い値で、初期化には大きな問題がない。\nArray{X, Y}(undef, ...)はデータ型XでY次元配列を、該当するデータ型の未定値でサイズ...だけ埋めた配列になる。ここでのポイントはundefだ。\n可変配列 一次元配列の場合、括弧の中に何も入れずに、簡単に空の配列を作ることができる。\njulia\u0026gt; empty = Array{Float64, 1}()\rFloat64[]\rjulia\u0026gt; empty = Array{Float64, 2}()\rERROR: MethodError: no method matching Array{Float64,2}()\rClosest candidates are:\rArray{Float64,2}(::UndefInitializer, ::Int64, ::Int64) where T at boot.jl:408\rArray{Float64,2}(::UndefInitializer, ::Int64...) where {T, N} at boot.jl:412\rArray{Float64,2}(::UndefInitializer, ::Tuple{Int64,Int64}) where T at boot.jl:416\r...\rStacktrace:\r[1] top-level scope at REPL[85]:1 しかし、同じ方法で二次元配列を作ろうとすると、上述のようにMethodErrorが発生する。もちろん自然な二次元配列ではないが、一次元配列の一次元配列を作るような形で空の配列を作ることは可能だが、速度の面ではネイティブな文法をそのまま使うことを推奨する。\njulia\u0026gt; empty = Array{Array{Float64, 1}, 1}()\rArray{Float64,1}[] もっと簡単な方法 下のように波括弧を使うと、もっと簡単に配列を作ることができる。\njulia\u0026gt; empty = Float64[]\rFloat64[]\rjulia\u0026gt; empty = Array{Float64, 1}[]\rArray{Float64,1}[]\rjulia\u0026gt; empty = Array{Float64, 2}[]\rArray{Float64,2}[] 環境 OS: Windows julia: v1.5.0 ","id":1797,"permalink":"https://freshrimpsushi.github.io/jp/posts/1797/","tags":null,"title":"ジュリアで空の配列を作成する方法"},{"categories":"수리물리","contents":"定義 3次元のスカラー関数 $f=f(x,y,z)$のグラディエントのダイバージェンスを $f$のラプラシアンLaplacianと言い、$\\nabla^{2}$で表される。\n$$ \\nabla ^{2} f := \\nabla \\cdot(\\nabla f)= \\frac{ \\partial^{2} f}{ \\partial x^{2} }+\\frac{ \\partial^{2} f}{ \\partial y^{2}}+\\frac{ \\partial^{2} f}{ \\partial z^{2}} $$\n説明 ラプラシアンという名称はフランスの数学者ラプラスから取られている。$\\nabla^{2}$という表記は便宜上使われているものだ。数学（偏微分方程式論）では$\\Delta$という表記をもっと多く使う。ラプラシアンを一言で言うならば、2階の微分の拡張である。グラディエントが1階の微分を3次元に拡張したものであれば、ラプラシアンは2階の微分を3次元に拡張したものだ。高校の微分の時間にこんな内容を学んだはずだ。\n1\n1階の微分は単純に関数$f$が増えているのか減っているのかの情報しか与えないが、2階の微分はどう増えたり減ったりしているのかの情報を与える。$f$のラプラシアンを求める式は上で示された通り、ダイバージェンスを求める式に微分が一回増えただけだ。\n導出 導出らしいことはない。\n$$ \\begin{align*} \\nabla \\cdot (\\nabla f) \u0026amp;= \\nabla \\cdot \\left( \\frac{ \\partial f}{ \\partial x },\\frac{ \\partial f}{ \\partial y},\\frac{ \\partial f}{ \\partial z} \\right) \\\\ \u0026amp;= \\frac{ \\partial ^{2} f }{ \\partial x^{2} }+\\frac{ \\partial ^{2} f }{ \\partial y^{2} } + \\frac{ \\partial ^{2}f }{ \\partial z^{2} } \\end{align*} $$\n■\n関連項目を見る デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ EBS 2021学年度 大学入試特講 微積分 p.70\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1879,"permalink":"https://freshrimpsushi.github.io/jp/posts/1879/","tags":null,"title":"三次元デカルト座標系におけるスカラー関数のラプラシアン"},{"categories":"동역학","contents":"정리 $2$次元の多様体 $\\mathcal{P}$ と関数 $f,g \\in C^{r} \\left( \\mathcal{P} \\right)$ に対して、次のようなベクトル場が微分方程式として与えられているとする。 $$ \\dot{x} = f(x,y) \\\\ \\dot{y} = g(x,y) $$ $\\mathcal{M}$ このベクトル場が有限個の不動点を持つ不変集合である場合、$p \\in \\mathcal{M}$ のオメガリミットセット $\\omega (p)$ は次の三つのうちの一つを満たす：\n(1): $\\omega (p)$ は単元素集合である。つまり、ただ一つの不動点のみを含む。 (2): $\\omega (p)$ は閉じた軌道である。 (3): $\\omega (p)$ は有限個の不動点 $p_{1} , \\cdots , p_{n}$ のいくつかの $i,j \\in [1,n]$ に対して次を満たす軌道 $\\gamma$ から構成される。 $$ \\alpha ( \\gamma ) = \\left\\{ p_{i} \\right\\} \\\\ \\omega ( \\gamma ) = \\left\\{ p_{j} \\right\\} $$ 説明 距離空間は当然 $T_{1}$ 空間であり、$T_{1}$ では単元素集合が閉集合であることが保証されるため、$\\omega (p) = \\left\\{ p \\right\\}$ は当然閉軌道と言えるが、ステートメントの文脈上、一つの不動点のみを含む場合は別のものとして区別しよう。\n実際、ポアンカレ・ベンディクソンの定理では、カオスというものは定義される必要もなく、カオスが起こらないというステートメント自体は系に近い。定理が言うのは単にオメガリミットセットの分類であり、それが正確に我々が知っているもので構成されているため、カオスが起こることはないという事実が導かれるのである。しかし、このような定理があることにより、カオス理論の関心は$2$次元を確実に超えることができるようになる。\n定理の直観的な理解はそれほど難しくない。$\\mathcal{M}$ がバウンドされていない場合はそもそもカオスにならず、バウンドされている場合は永遠に伸び続けることはできず、フローが狭まって回るか広がって回るかしなければならない。しかし、$3$次元とは異なり、$2$次元では線が平面を二つの領域に分けてしまうため、そのうちの一つの領域を諦めなければならない状況が常に起こる。これは、限られた空間である$\\mathcal{M}$ の残りの部分を時が経つにつれて捨てていくようなものと見ることができる。まだ通過していない領域を使用するために、すでに通過したフローを通過しようとすると、その瞬間それは閉じた軌道となり、結局閉じた軌道か不動点に収束することになり、カオスを引き起こすことはできない。\n証明 1 戦略: ポアンカレの名前が付いた定理らしく、位相数学的である。$\\mathcal{P}$ 内部で連続で連結なアーク(continuous, connected arc)を一つ$\\Sigma$としよう。\n$\\Sigma$のすべての点での法線ベクトルとベクトル場の内積が$0$でなく、符号も変わらない場合、$\\Sigma$は$\\mathcal{P}$上のベクトル場を横切るTransverseと言う。この概念は一点に対してのみ考えることもできるが、その点ではベクトル場と$\\Sigma$は接触しないだろう。フローの観点からは、一点で出会うだけでなく、$\\Sigma$を貫通することになる。\n与えられたベクトル場で作られるフローを$\\phi_{t}$、フロー$\\phi_{t}$の下で一点$p \\in \\mathcal{P}$の正の時間に対する軌道を$O_{+}(p)$として表そう。一点$p_{i}$がフロー$\\phi_{t}$の下で時間$t$の流れに従って$p_{j}$に到達するまでの軌道を$\\widehat{p_{i} p_{j}} \\subset O_{+} (p)$として表そう。また、オメガリミットセットを表す$\\omega ( \\cdot )$は元々与えられた一点に対して定義されていたが、ある集合$X$に対する$\\omega \\left( X \\right)$は次のように考えればよい。\n$$ \\omega (X) := \\bigcup_{x \\in X} \\omega (x) $$ これはアルファリミットセット$\\alpha ( \\cdot )$も同様に定義されたと考えればよい。\nその他、次のような補助定理を続けて使用することになる。\n補助定理(オメガリミットセットの性質): 全体空間がユークリッド空間$X = \\mathbb{R}^{n}$であり、フロー$\\phi_{t} ( \\cdot )$でコンパクト不変集合$\\mathcal{M}$の一点$p \\in \\mathcal{M}$が与えられているとする：\n[1]: $\\omega (p) \\ne \\emptyset$ [2]: $\\omega (p)$は閉集合である。 [3]: $\\omega (p)$はフローに不変である。つまり、$\\omega (p)$は軌道の合併である。 [4]: $\\omega (p)$は連結空間である。 まず、$2$次元で生じるオメガリミットセットは何らかの面積を持つ形状ではないため、以降言及されるオメガリミットセットは何らかの曲線の形状と考えればよい。\nPart 1.\n$\\Sigma \\subset \\mathcal{M}$がベクトル場を横切るアークである場合、$\\mathcal{M}$が$2$次元ベクトル場の不変集合であるため、$\\Sigma$がベクトル場の流れに逆らって$\\mathcal{M}$の外へ出ることはできない。したがって、任意の$p \\in \\mathcal{M}$に対して、$O_{+} (p)$と$\\Sigma$が交わる$k$番目の点を$p_{k}$とすると、$p_{k}\\subset \\widehat{p_{k-1} p_{k+1}} \\subset O_{+} (p)$でなければならない。つまり、フローが$\\mathcal{M}$内部に向かって収束していくが、その過程で$\\Sigma$と交わる交点が近づいてまた遠ざかることは起こらないということである。\nPart 2. $p \\in \\mathcal{M}$のオメガリミットセット$\\omega (p)$は$\\Sigma$と多くとも一点でしか交差しない。\n背理法で示す。$\\omega (p)$と$\\Sigma$が異なる二点$q , \\overline{q}$で交差すると仮定してみる。\nその場合、オメガリミットセットの定義により、$n \\to \\infty$のとき $$ q_{n} \\to q \\\\ \\overline{q}_{n} \\to \\overline{q} $$ を満たすシーケンス$\\left\\{ q_n \\right\\}_{n \\in \\mathbb{N}} , \\left\\{ \\overline{q}_n \\right\\}_{n \\in \\mathbb{N}} \\subset O_{+} (p)$が存在する。しかし、Part 1によれば、これらの交点はある順序$p_{1} , p_{2} , \\cdots$に並べられるため、仮定に矛盾する。したがって、$\\omega (p)$と$\\Sigma$は最初から交差しないか、交差するとしてもただ一点でのみ交差する。[ 注: トーラスの場合には、この論理をそのまま適用することはできないが、いくつかの部分に分けて$\\mathcal{M}$と同じ形状にすることで同じ結論を得ることができる。 ]\nPart 3. $\\omega (p)$が不動点を含まない場合、閉じた軌道である。\n$q \\in \\omega (p)$の軌道$O_{+}(q)$が閉じた軌道であることを示し、その後$\\omega (p) = O_{+} (q)$であることを示せばよい。\nPart 3-1. 軌道$O_{+}(q)$は閉じている。 点$x \\in \\omega (q)$を一つ選んでみると、補助定理[2]により$\\omega (p)$が閉じており、不動点を持たない軌道の合併であるため、$x$も不動点であってはならない。$p,q$が混乱しないように、仮定は$\\omega (p)$が不動点を持たないことであり、$x$は$x \\in \\omega (q)$であるため、必ずしも$x \\in \\omega (p)$である保証はないが、いずれにせよ不動点ではないと言える。この不動点でない一点$x$のベクトル場を横切る一つのアーク$\\Sigma_{x}$を選ぼう。**Part 1.**によれば、$\\Sigma_{x}$と$O_{+} (q)$の交点のシーケンス$\\left\\{ q_{n} \\right\\}_{n \\in \\mathbb{N}}$は$n \\to \\infty$のとき$q_{n} \\to x$であり、$x \\in \\mathcal{M}$であるため、**Part 2.**により$\\forall n \\in \\mathbb{N}$に対して$q_{n} = x$でなければならない。$x$は不動点ではないため、$O_{+} (q)$が$x$と交差する場合、離れた後に再び戻って交差しなければならない。ここで$x \\in \\omega (q)$としたので、$O_{+}(q)$は$x$に近づいて止まることなく、実際に$x$と交差し、したがって$O_{+}(q)$は閉じた軌道となる。 Part 3-2. $O_{+}(q) = \\omega (p)$ 点$q \\in \\omega (p)$からベクトル場を横切る一つのアーク$\\Sigma_{q}$を選んでみると、Part 2により$\\omega (p)$と$\\Sigma_{q}$はただ$q$でのみ出会う。補助定理[3]により$\\omega (p)$は軌道の合併であるため、$q \\in \\omega (p)$であれば$O_{+} (q) \\subset \\omega (p)$であるが、$\\omega (p)$は不動点を含まず連結空間であるため、正確に$O_{+}(q) = \\omega (p)$でなければならない。 Part 4. $p \\in \\mathcal{M}$に対して異なる$p_{1} , p_{2} \\in \\omega (p)$がベクトル場の不動点である場合、$\\alpha (\\gamma) = \\left\\{ p_{1} \\right\\}$と$\\omega (\\gamma) = \\left\\{ p_{2} \\right\\}$を満たす軌道$\\gamma \\subset \\omega (p)$は多くても一つしか存在しない。\n背理法で示す。二点を結ぶ異なる二つの軌道があれば、その二つの軌道の間に面積を持つ何らかの領域$\\mathcal{K}$が生じるだろう、そこから矛盾を導く。次の条件を満たす異なる二つの軌道$\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$が存在すると仮定しよう。 $$ \\alpha \\left( \\gamma_{i} \\right) = \\left\\{ p_{1} \\right\\} \\\\ \\omega \\left( \\gamma_{i} \\right) = \\left\\{ p_{2} \\right\\} $$ これらの軌道から一点ずつ$q_{1} \\in \\gamma_{1}$、$q_{2} \\in \\gamma_{2}$を選び、$q_{1}$と$q_{2}$からベクトル場を横切るアークを$\\Sigma_{1}, \\Sigma_{2}$として選ぶ。\n$\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$であるため、Part 2により、$O_{+} (p)$が$\\Sigma_{1}$と一点$a$で交差した後、$\\Sigma_{2}$は一点$b$で交差するとしよう。すると、$2$次元多様体上で次のような経路に囲まれた部分領域$\\color{red}{\\mathcal{K}}$が生じるだろう。\n$$ q_{1} \\overset{\\Sigma_{1}}{\\to} a \\overset{ O_{+} (p) }{ \\to } b \\overset{\\Sigma_{2}}{\\to} q_{2} \\overset{ \\omega (\\gamma) }{ \\to } p_{2} \\overset{ \\gamma_{1} }{ \\gets } q_{1} $$ 記法$\\displaystyle x \\overset{\\mathcal{C}}{\\to} y$は点$x,y$がカーブ$\\mathcal{C}$に繋がれたことを意味して使用された。$\\color{red}{\\mathcal{K}}$から始まったフローは$\\gamma_{1} , \\gamma_{2}$を超えることができないため、$\\color{red}{\\mathcal{K}}$は不変集合となる。しかし、$p$から始まった軌道$O_{+}(p)$が$\\color{red}{\\mathcal{K}}$に入ると、二度と出ることはできないということは、$\\gamma_{1}$や$\\gamma_{2}$が$\\omega (p)$に属することはできないということである。例えば$\\gamma_{2}$を考えると、$q_{2} \\overset{\\gamma_{2}}{\\to} p_{2}$は$\\omega (p)$に属することができるかもしれないが、その前部分である$p_{1} \\overset{\\gamma_{2}}{\\to} q_{2}$には行けない。したがって、$\\gamma_{2}$全体が$\\omega (p)$に属するという主張はできず、$\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$と矛盾する。\nPart 5.\nこのパートでは、不動点でない点を正則点Regular Pointと呼ぼう。必ずしもこのパートに限定する必要はないが、不動点の否定という文脈が頻繁に出てこないのに対し、正則Regularという表現は学問を問わず頻繁に使用されるため、注意や警告なしに使用すると大きな混乱を引き起こす可能性があるためである。\nケース1. $\\omega (p)$が不動点のみを持つ場合 $\\mathcal{M}$は有限個の不動点を持ち、$\\omega (p)$は連結空間であるため、ただ一つの不動点のみを持たなければならない。 ケース2. $\\omega (p)$が正則点のみを持つ場合 Part 3により、$\\omega (p)$は閉じた軌道である。 ケース3. $\\omega (p)$が不動点と正則点の両方を持つ場合 正則点のみからなる軌道$\\gamma \\subset \\omega (p)$を考える。 $\\gamma$は正則点のみからなっているため、Part 3により、$\\omega ( \\gamma )$と$\\alpha (\\gamma)$は閉じた軌道であるが、その一方で不動点を持たなければならない。しかし、補助定理[4]により、$\\omega ( \\gamma )$は連結空間であるため、閉じた軌道と不動点が離れていることはできず、不動点は閉じた軌道のどこかに位置していなければならないが、これはすなわち$\\omega ( \\gamma )$が不動点のみを含む単元素集合であるということである。同じ議論を$\\alpha ( \\gamma )$で繰り返すと、$\\omega (p)$のすべての正則点はそのオメガリミットポイントとアルファリミットポイントとして不動点を持つことがわかる。\n$\\omega (p)$は上記の三つのケースのいずれかに属していなければならない。これで証明は終わりである。\n■\nWiggins. (2003). Introduction to Applied Nonlinear Dynamical Systems and Chaos Second Edition(2nd Edition): 118~120.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1788,"permalink":"https://freshrimpsushi.github.io/jp/posts/1788/","tags":null,"title":"プアンカレ-ベンディクソン定理の証明"},{"categories":"힐베르트공간","contents":"定義1 $X$をベクトル空間とする。$\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in X$ 及び $\\alpha, \\beta \\in \\mathbb{C}$(または $\\mathbb{R}$)に対して、次の条件を満たす関数\n$$ \\langle \\cdot , \\cdot \\rangle : X \\times X \\to \\mathbb{C} $$\nを内積と定義し、$\\left( X, \\langle \\cdot ,\\cdot \\rangle \\right)$を内積空間と呼ぶ。\n線形性: $$\\langle \\alpha \\mathbf{x} + \\beta \\mathbf{y} ,\\mathbf{z} \\rangle =\\alpha \\langle \\mathbf{x},\\mathbf{z}\\rangle + \\beta \\langle \\mathbf{y},\\mathbf{z}\\rangle$$ 共役対称性: $$\\langle \\mathbf{x},\\mathbf{y} \\rangle = \\overline{ \\langle \\mathbf{y},\\mathbf{x} \\rangle}$$ 正定値性: $$\\langle \\mathbf{x},\\mathbf{x} \\rangle \\ge 0 \\quad \\text{and} \\quad \\langle \\mathbf{x},\\mathbf{x} \\rangle = 0\\iff \\mathbf{x}=0$$ 説明 線形性と共役対称性から、次の式が得られる。\n$$ \\begin{align*} \\langle \\mathbf{x},\\alpha \\mathbf{y}+\\beta \\mathbf{z} \\rangle =\u0026amp;\\ \\overline{\\langle \\alpha \\mathbf{y}+\\beta \\mathbf{z} ,\\mathbf{x} \\rangle} \\\\ =\u0026amp;\\ \\overline{\\alpha \\langle \\mathbf{y},\\mathbf{x} \\rangle +\\beta \\langle \\mathbf{z},\\mathbf{x} \\rangle} \\\\ =\u0026amp;\\ \\overline{\\alpha}\\overline{\\langle \\mathbf{y},\\mathbf{x} \\rangle}+\\overline{\\beta} \\overline{\\langle \\mathbf{z},\\mathbf{x} \\rangle} \\\\ =\u0026amp;\\ \\overline{\\alpha}\\langle \\mathbf{x},\\mathbf{y}\\rangle + \\overline{\\beta} \\langle \\mathbf{x},\\mathbf{z} \\rangle \\end{align*} $$\nこれは二番目の要素に対してアンチリニアであることを意味する。物理学、工学等では、内積が少しだけ異なって定義されることもある。たとえば、第一成分に対してアンチリニアで、第二成分に対してはリニアに定義されることもある。一方で内積空間では、以下のようにコーシー・シュワルツの不等式が成り立つ。\n$(X, \\langle \\cdot ,\\cdot \\rangle)$が内積空間であるとする。すると、以下の不等式が成り立ち、これをコーシー・シュワルツの不等式と呼ぶ。\n$$ \\left| \\langle \\mathbf{x},\\mathbf{y} \\rangle \\right| \\le \\langle \\mathbf{x},\\mathbf{x} \\rangle^{1/2} \\langle \\mathbf{y},\\mathbf{y} \\rangle ^{1/2},\\quad \\forall \\mathbf{x},\\mathbf{y} \\in X $$\nまた、内積から以下のようにノルムを定義できる。\n$$ \\left\\| \\mathbf{x} \\right\\| := \\sqrt{\\langle \\mathbf{x},\\mathbf{x} \\rangle},\\quad \\mathbf{x}\\in X $$\nこのように内積から自然に導出されたノルムをassociated normとも呼ぶ。また、ノルムが与えられた場合には、ノルムから距離を定義できるので、距離空間の性質である完備性についても語ることができる。完備な内積空間をヒルベルト空間と呼ぶ。\n特性 コーシー・シュワルツの不等式: 任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\left| \\langle \\mathbf{x},\\mathbf{y} \\rangle \\right| \\le \\left\\| \\mathbf{x} \\right\\| \\left\\| \\mathbf{y} \\right\\| $$\n平行四辺形の法則: 任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\left\\| \\mathbf{x} + \\mathbf{y} \\right\\|^{2} + \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|^{2} = 2 \\left( \\left\\| \\mathbf{x} \\right\\| ^{2}+ \\left\\| \\mathbf{y} \\right\\| ^{2} \\right) $$\n複素内積空間における偏極アイデンティティ: 複素内積空間 $X$及び任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\langle \\mathbf{x},\\mathbf{y} \\rangle = \\frac{1}{4} \\Big( \\left\\| \\mathbf{x} + \\mathbf{y} \\right\\|^{2} - \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|^{2} + i \\left( \\left\\| \\mathbf{x} + iy \\right\\|^{2} - \\left\\| \\mathbf{x} - iy \\right\\|^{2} \\right) \\Big) $$\n実内積空間における偏極アイデンティティ: 実内積空間 $X$及び任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\langle \\mathbf{x},\\mathbf{y}\\rangle = \\frac{1}{4} \\left( \\left\\| \\mathbf{x}+\\mathbf{y} \\right\\|^{2} - \\left\\| \\mathbf{x}-\\mathbf{y} \\right\\| ^{2} \\right) $$\nノルム対内積: 任意の $\\mathbf{x} \\in X$に対して、\n$$ \\left\\| \\mathbf{x} \\right\\| =\\sup \\left\\{ \\left| \\langle \\mathbf{x},\\mathbf{y} \\rangle \\right| : \\mathbf{y}\\in X, \\left\\| \\mathbf{y} \\right\\| =1 \\right\\} $$\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p61-65\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1842,"permalink":"https://freshrimpsushi.github.io/jp/posts/1842/","tags":null,"title":"内積空間"},{"categories":"힐베르트공간","contents":"説明 内積空間 $\\left( X, \\langle\\cdot, \\cdot\\rangle \\right)$が与えられているとする。そうすると、下記のように内積から自然にノルムを定義できる。\n$$ \\begin{equation} \\left\\| x \\right\\| := \\sqrt{ \\langle x, x\\rangle},\\quad x\\in X \\end{equation} $$\nしたがって、内積空間ならばノルム空間だ。続いてこのように定義されたノルムから距離を定義できる。\n$$ \\begin{equation} d(x,y):=\\left\\| x -y \\right\\| =\\sqrt{ \\langle x-y, x-y \\rangle},\\quad x,y\\in X \\end{equation} $$\nしたがって、内積空間ならばノルム空間であり、距離空間でもある。教科書の中には距離空間が与えられているといいながらノルムや内積の概念を使用するものがあるが、まさにこのためだ。距離空間と言っても実際には内積空間が与えられているとみなすわけだ。\n逆に、「内積空間 $X$が与えられている」という言葉は、「距離空間 $X$が与えられている」「ノルム空間 $X$が与えられている」と同じ意味だ。また、完備性は距離空間で定義される概念だが、内積やノルムから距離を定義できるため、ノルム空間や内積空間が完備であると言える理由はこれだ。証明は定義を使えば難しくないので、$(1)$についてだけ紹介する。\n定理 内積空間ならばノルム空間だ。\n証明 内積空間 $X$が与えられているとする。そして、$x,y\\in X$であり、$c\\in \\mathbb{C}$とする。すると内積の定義により、\n$$ \\left\\| x \\right\\| \\ge 0 $$\nが成り立つ。また、内積の定義により、\n$$ \\left\\| x \\right\\| =0 \\iff x=0 $$\nが成り立つ。同様に内積の定義により、\n$$ \\begin{align*} \\left\\| cx \\right\\| =\u0026amp;\\ \\sqrt{ \\langle cx,cx\\rangle } \\\\ =\u0026amp;\\ \\sqrt{ \\left| c \\right| ^{2} \\langle x,x \\rangle} \\\\ =\u0026amp;\\ \\left| c \\right| \\sqrt{\\langle x,x \\rangle} \\\\ =\u0026amp;\\ \\left| c \\right| \\left\\| x \\right\\| \\end{align*} $$\nが成り立つ。最後の条件も内積の定義により、\n$$ \\begin{align*} \\left\\| x + y \\right\\|^{2} =\u0026amp;\\ \\langle x+y,x+y \\rangle \\\\ =\u0026amp;\\ \\langle x,x+y\\rangle +\\langle y,x+y \\rangle \\\\ =\u0026amp;\\ \\langle x,x\\rangle + \\langle x,y\\rangle + \\langle y,x\\rangle + \\langle y,y\\rangle \\\\ =\u0026amp;\\ \\left\\| x \\right\\|^{2}+\\langle x,y \\rangle +\\overline{ \\langle x,y \\rangle}+ \\left\\| y \\right\\| ^{2} \\\\ \\le\u0026amp; \\left\\| x \\right\\| ^{2} + 2 \\left| \\langle x,y \\rangle \\right| + \\left\\| y \\right\\| ^{2} \\\\ \\le\u0026amp; \\left\\| x \\right\\|^{2} +2\\langle x,x \\rangle ^{1/2}\\langle y,y \\rangle^{1/2} + \\left\\| y \\right\\|^{2} \\\\ =\u0026amp;\\ \\left\\| x \\right\\|^{2}+2\\left\\| x \\right\\|\\left\\| y \\right\\| +\\left\\| y \\right\\|^{2} \\\\ =\u0026amp;\\ \\left( \\left\\| x \\right\\| + \\left\\| y \\right\\| \\right)^{2} \\end{align*} $$\n任意の複素数 $c\\in \\mathbb{C}$に対して、$c+\\overline{c} \\in \\mathbb{R}$であるため、第五行は成立する。第六行はコーシー・シュワルツの不等式によって成り立つ。したがって、\n$$ \\left\\| x \\right\\| := \\sqrt{\\langle x,x \\rangle} $$\nとして定義された $\\left\\| \\cdot \\right\\|$は、ノルムの定義を満たす。■\n","id":1840,"permalink":"https://freshrimpsushi.github.io/jp/posts/1840/","tags":null,"title":"内積空間、ノルム空間、距離空間の関係"},{"categories":"수리통계학","contents":"定義 1 $\\theta$ の推定量 $T$ が次を満たす場合、$T$ は $\\theta$ の不偏推定量Unbiased Estimatorと呼ばれる。 $$ E T = \\theta $$\n説明 特に、$\\theta$ に対する不偏推定量の中で分散が最も小さい場合、最小分散不偏推定量Minimum Variance Unbiased Estimator, MVUEと呼ばれる。\n不偏性とは、偏りを持たない性質のことを言う。例えば、$X_{i} \\sim \\left( \\mu , \\sigma^{2} \\right)$ とする時、$\\mu$ の推定量として標本平均 $\\displaystyle \\overline{X} = {{ 1 } \\over { n }} \\sum_{i} X_{i}$ を使用する場合、$\\displaystyle E \\overline{X} = \\mu$ であるため、$\\overline{X}$ は $\\mu$ の不偏推定量になる。これは一見当たり前に見えるが、推定量が母数を正確に示すことは非常に重要な性質であり、当たり前のことではない。例えば、分散と標本分散について見てみよう。\n例 $$ X_{i} \\sim \\left( \\mu , \\sigma^{2} \\right) $$ とする場合、分散の不偏推定量は次のようになる。 $$ S^{2} := {{1} \\over {n-1}} \\sum_{i=1}^{n} \\left( X_{i} - \\overline{X} \\right)^{2} $$ 知られているように、標本平均とは異なり、標本分散は偏差の二乗をすべて加算した後、$n$ ではなく $n-1$ で割る。標本分散を求める際に $n-1$ で割る理由には、聞く人のレベルに応じて様々な説明ができるが、最も正確な式で説明するならば、「標本分散が不偏推定量となるため」である。\n証明 2 $$ \\mu := E \\overline{X} \\\\ \\sigma^{2} := E X_{i} ^{2} - \\mu^{2} $$ とすると、 $$ \\begin{align*} E \\left( \\overline{X}^{2} \\right) - \\mu^{2} =\u0026amp; E \\left( \\overline{X}^{2} \\right) - \\left( E \\overline{X} \\right)^{2} \\\\ =\u0026amp; \\text{Var} \\overline{X} \\\\ =\u0026amp; \\text{Var} \\left( {{1} \\over {n}} \\sum_{i=1}^{n} X_{i} \\right) \\\\ =\u0026amp; {{1} \\over {n^{2}}} \\sum_{i=1}^{n} \\text{Var} X_{i} \\\\ =\u0026amp; {{1} \\over {n^{2}}} n \\sigma^{2} \\\\ =\u0026amp; {{\\sigma^{2}} \\over {n}} \\end{align*} $$ よって、標本分散 $S^{2}$ の期待値は $$ \\begin{align*} E S^{2} =\u0026amp; (n-1)^{-1} E \\sum_{i=1}^{n} \\left( X_{i} - \\overline{X} \\right)^{2} \\\\ =\u0026amp; (n-1)^{-1} \\left[ \\sum_{i=1}^{n} E X_{i}^{2} - \\sum_{i=1}^{n} E \\overline{X} ^{2} \\right] \\\\ =\u0026amp; (n-1)^{-1} \\left[ \\sum_{i=1}^{n} \\left( \\sigma^{2} + \\mu^{2} \\right) - n \\left( \\mu^{2} + {{\\sigma^{2}} \\over {n}} \\right) \\right] \\\\ =\u0026amp; (n-1)^{-1} \\left[ n\\sigma^{2} + n \\mu^{2} - n \\mu^{2} - \\sigma^{2} \\right] \\\\ =\u0026amp; (n-1)^{-1} (n-1) \\sigma^{2} \\\\ =\u0026amp; \\sigma^{2} \\end{align*} $$\n■\nHogg et al. (2013). 「数理統計学の概要」(第7版): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). 「数理統計学の概要」(第7版): p137.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1745,"permalink":"https://freshrimpsushi.github.io/jp/posts/1745/","tags":null,"title":"不偏推定量"},{"categories":"함수","contents":"定義 関数 $f:[a,b] \\rightarrow \\mathbb{R}$が与えられたとする。$x_{1}$、$x_{2}$、$\\in [a,b]$に対して\n$$ x_{1} \\lt x_{2} \\ \\implies f(x_{1}) \\le f(x_{2}) $$\nを満たす場合、$f$は 単調に増加monotonically increasingすると言い、$f$を 単調増加関数monotone increasing functionと呼ぶ。逆に\n$$ x_{1} \\lt x_{2} \\ \\implies f(x_{1}) \\ge f(x_{2}) $$\nを満たす場合、$f$は 単調に減少monotonically decreasingすると言い、$f$を 単調減少関数monotone decreasing functionと呼ぶ。\n$f$が単調増加関数か単調減少関数であれば、$f$を 単調関数monotoneと呼ぶ。\n説明 単調に増加するとは、変数が大きくなるにつれて、関数の値が 少なくとも減少しないということを意味する。逆に、単調に減少するとは、少なくとも増加しないということを意味する。\n定義 以下の式\n$$ x_{1} \\lt x_{2} \\implies f(x_{1}) \\lt f(x_{2}) $$\nを満たす$f$を 厳密に増加関数strictly increasing functionと呼ぶ。逆に\n$$ x_{1} \\lt x_{2} \\implies f(x_{1}) \\gt f(x_{2}) $$\nを満たす$f$を 厳密に減少関数strictly increasing functionと呼ぶ。\n","id":848,"permalink":"https://freshrimpsushi.github.io/jp/posts/848/","tags":null,"title":"単調関数、増加関数、減少関数"},{"categories":"편미분방정식","contents":"定義1 偏微分方程式 自然数$k \\in \\mathbb{N}$と、開集合$U \\subset \\mathbb{R}^{n}$に対して、次の表現を**$k$次の偏微分方程式**と呼ぶ。\n$$ \\begin{equation} F(D^{k}u(x), D^{k-1}u(x),\\cdots,Du(x),u(x),x)=0\\quad (x\\in U) \\end{equation} $$\nここで、$D^{k}u$は多重指数表記である。$F$は次のように与えられ、未知数$u$は次のようである。\n$$ F : {\\mathbb{R}}^{n^{k}}\\times{\\mathbb{R}}^{n^{k-1}}\\times \\cdots \\times \\mathbb{R}^{n}\\times \\mathbb{R}\\times U \\to \\mathbb{R} \\\\ u : U \\to \\mathbb{R} $$\n偏微分方程式の連立 与えられた$\\mathbf{F} : {\\mathbb{R}}^{mn^{k}}\\times{\\mathbb{R}}^{mn^{k-1}}\\times \\cdots \\times \\mathbb{R}^{mn}\\times \\mathbb{R}^{m}\\times U \\to \\mathbb{R}^{m}$と未知数$\\mathbf{u}:U \\to \\mathbb{R}^{m}$、$\\mathbf{u}=(u^{1},\\cdots,u^{m})$に対して、下記の表現\n$$ \\mathbf{F}(D^{k}\\mathbf{u}(x),D^{k-1}\\mathbf{u}(x),\\cdots,D\\mathbf{u}(x),\\mathbf{u}(x),x)=\\mathbf{0}\\quad (x\\in U) $$\nを**$k$次の偏微分方程式システム**と呼ぶ。\n説明 偏微分方程式は、よくPDEと略される。PDEを解くことは、$(1)$を満たす$u$を全て見つけ出すことを意味し、そのような$u$をソリューションと呼ぶ。\nソリューションを見つけることは、\n理想的には、簡単で明示的なソリューションを見つけることを意味する、 それが不可能なときは、解の存在や他の特徴を明らかにすることを意味する。 ほとんどの場合、偏微分方程式での$U, \\Omega \\subset \\mathbb{R}^{n}$は開集合を意味し、変数$t$は常に時間を意味し、$t\\ge 0$である。また、\n$$ Du=D_{x}u=(u_{x_{1}},\\cdots,u_{x_{n}}) $$\nは$u$のグラディエントを意味する。このとき、$x=(x_{1},\\cdots,x_{n})$である。\n分類 偏微分方程式は、線形性に基づいて以下のように分類できる。\n線形 偏微分方程式$(1)$が、与えられた関数$a_{\\alpha}, f$に対して、次の式を満たす場合、線形と言われる。\n$$ \\sum _{| \\alpha | \\le k} a_{\\alpha}(x) D^{\\alpha} u = f(x) $$\n$f=0$の場合、同質の線形PDEと呼ぶ。線形ではない場合、非線形と呼ぶ。2次の線形偏微分方程式はさらに以下のように分類される。\n双曲型PDE 放物型PDE 楕円型PDE 半線形 偏微分方程式$(1)$が次を満たす場合、半線形と呼ぶ。\n$$ \\sum _{| \\alpha | = k} a_{\\alpha}(x) D^{\\alpha} u + a_{0}\\left( D^{k-1}u, \\dots, Du, u, x \\right) = 0 $$\n言い換えると、半線形pdeは、オーダーが$k$最も高いオーダーの導関数の係数が$x$にのみ依存する偏微分方程式を意味する。例としては、\n反応-拡散方程式 $$ u_{t} - \\Delta u = f(u) \\qquad (\\text{e.g. } f(u) = u^{2}) $$ 準線形 偏微分方程式$(1)$が次を満たす場合、準線形と呼ぶ。\n$$ \\sum _{| \\alpha | = k} a_{\\alpha}(D^{k-1}u, \\dots, Du, u, x)D^{\\alpha} u + a_{0}\\left( D^{k-1}u, \\dots, Du, u, x \\right) = 0 $$\n例としては、\n粘性のないバーガース方程 $$ u_{t} + uu_{xx} = 0 $$ 完全非線形 準線形ではない非線形方程式を完全非線形と言う。\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p1-3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1818,"permalink":"https://freshrimpsushi.github.io/jp/posts/1818/","tags":null,"title":"偏微分方程式"},{"categories":"수리통계학","contents":"定義 12 確率変数 $X$ のサンプル $X_{1} , \\cdots , X_{n}$ の関数 $T$ を統計量Statisticと言う。 $$ T := T \\left( X_{1} , \\cdots , X_{n} \\right) $$ $X$ の分布関数が $f(x; \\theta)$ あるいは $p(x; \\theta)$ のように表される時、$T$ が $\\theta$ を把握するための統計量であれば、$T$ を$\\theta$ の推定量Estimatorと言う。 統計量の確率分布をサンプリング分布Sampling Distributionと言う。 説明 推定量(Estimator)の実現を推定値Estimateと言う。パラメータは通常スカラー $\\theta \\in \\mathbb{R}$ の場合が多く、この場合は$T$を$\\theta$の点推定量Point Estimatorとも言う。例えば、正規分布 $N \\left( \\mu, \\sigma^{2} \\right)$ に従うランダムサンプルがあるとき、母平均 $\\mu$ の推定量は次の通りである。 $$ \\overline{X} := {{ 1 } \\over { n }} \\sum_{k = 1}^{n} X_{k} $$ 実際のデータ $x_{1} , \\cdots , x_{n}$ がある場合、$\\mu$ の推定値は次の通りである。 $$ \\overline{x} := {{ 1 } \\over { n }} \\sum_{k = 1}^{n} x_{k} $$\n参考文献 基礎統計学での統計量 基礎統計学ではサンプルの関数とは言わずもっと直感的に「計算されたもの」という表現を使って定義している。本質的には同じ意味だが、数学に馴染みのない新入生や人にとってより良い定義かもしれない。\n統計量の例 平均や分散などを除外して、「統計量」と名前についている統計量には以下のような例がある:\n十分統計量: 分布内のパラメータに関するすべての情報を持つ統計量である。 最小十分統計量: 特定の条件を満たす十分統計量である。 補助統計量: 十分統計量とは反対に、パラメータに関するどんな情報も持たない統計量である。 完全統計量: 統計量としてこのような性質を持っているべきだと言われる時、実際にその性質を持つ統計量である。 推定量の例 推定量には以下のような例がある:\n不偏推定量: 偏りを持たない推定量である。 一致推定量: 極限概念でパラメータを的確に推定する推定量である。 最尤推定量: 尤度Likelihoodを最大化する推定量である。 効率的推定量: 統計量の分散と関連した推定量である。 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCasella. (2001). Statistical Inference(2nd Edition): p211.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1730,"permalink":"https://freshrimpsushi.github.io/jp/posts/1730/","tags":null,"title":"数理統計学における統計量と推定量"},{"categories":"수리통계학","contents":"定義 1 確率変数 $X$の実際に引き出された結果を実現Realizationと言い、普通、小文字の$x$で表す。 確率変数$X$と同じ確率分布からサンプルサイズSample Size$n$分の確率変数をサンプルSampleと呼び、次のように表す。 $$ X_{1} , X_{2} , \\cdots , X_{n} $$ 確率変数$X_{1} , \\cdots , X_{n}$がiidであれば、サイズ$n$のランダムサンプルと呼ぶ。 説明 これらの定義により、数理統計学は実際の統計解析との接点を持つことになる。ランダムサンプルの実現を扱うことは、統計解析に該当し、数理統計学はそのデータをどのように扱うかについての大きな灯台となる。関心を持つデータ、得たい結論、利用する方法は異なるかもしれないが、その下には数理統計学が理論的な基盤として支えていなければならない。\n実際には、数理統計学の教科書を離れると、実現という表現はあまり使われず、通常はその実現を直接言及する言葉がある。たとえばランダムサンプルの実現は、値、データ、観測値などと呼ばれる。しかし、確率変数は大文字、データは小文字という慣習は、ほとんどすべての統計学教科書で守られている。\n参照 統計入門におけるデータの定義 学部1〜2年生向けの統計学入門では、実験単位や試行などで実際に測定された結果の集まりをデータと定義している。\nHogg et al. (2013).《数理統計学入門》(第7版): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1715,"permalink":"https://freshrimpsushi.github.io/jp/posts/1715/","tags":null,"title":"数理統計学におけるランダムサンプリング"},{"categories":"머신러닝","contents":"オーバーフィッティング トレーニングロスは減っていくが、テストロス（バリデーションロス）が減らない、あるいは増えてしまう現象をオーバーフィッティングover fitting, 過剰適合と呼ぶ。\n説明 これとは反対のアンダーフィッティングという言葉もあるが、正直意味がない言葉で、経験上あまり使われない。\n機械学習で重要な点は、持っているデータで学習した関数が新しいデータに対してもよく機能しなければならないこと。だから、学習に使わなかったデータに対する性能を言う一般化性能という言葉があり、オーバーフィッティングが起きたということは一般化性能が低いということと同じ。入試で模擬試験を解くのは、結局のところ、大学入試をうまく受けるためで、模擬試験は満点を取り、大学入試をダメにする学生は模擬試験の問題にオーバーフィッティングした学生と見なせる。模擬試験で満点までではなくてもいい成績を取り、大学入試でも同様にいい成績を取る学生は、一般化性能がいいということだ。\nレギュラリゼーション （トレーニングロスではなく）テストロスを減らすためにアルゴリズムを修正するすべての方法をレギュラリゼーションと呼ぶ。1\nGoodfellowはレギュラリゼーションを「学習アルゴリズムに対して行う、訓練エラーではなく、一般化エラーを減らすことを目的としたあらゆる修正」と定義している。\nつまり、オーバーフィッティングを防ぐためのすべての方法をまとめてレギュラリゼーションと呼ぶ。機械学習やディープラーニングを勉強するときに、最初に接するのは通常ドロップアウトだ。\n種類 $\\ell_{1}$ レギュラリゼーション $\\ell_{2}$ レギュラリゼーション Weight decay Early stopping ドロップアウトdropout Batch normalization Label smoothing データオーグメンテーションdata augmentation フラッディング 一緒に見ておくべきこと 標準化：通常、統計学でデータの平均を$0$、分散を$1$に合わせるプロセスをいう。 正規化：通常、データを特定の範囲に配置するプロセスをいう。 正則化：通常、機械学習で過剰適合を防ぐプロセスをいう。 Ian Goodfellow, Yoshua Bengio, アンド Aaron Courville. (2016) Deep Learning. MIT Press\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1807,"permalink":"https://freshrimpsushi.github.io/jp/posts/1807/","tags":null,"title":"マシンラーニングにおけるオーバーフィッティングと正則化とは？"},{"categories":"수리물리","contents":"定義 多変数関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}$が与えられたとしよう。変数$\\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n})$の変化に伴う$f(\\mathbf{x})$の変化を以下のように$df$と表し、これを$f$の全微分total differentialまたは完全微分exact differentialという。\n$$ \\begin{equation} df = \\frac{ \\partial f}{ \\partial x_{1} }dx_{1} + \\frac{ \\partial f}{ \\partial x_{2} }dx_{2} + \\cdots + \\frac{ \\partial f}{ \\partial x_{n} }dx_{n} \\label{1} \\end{equation} $$\n説明 上記の定義は、変数$\\mathbf{x}$の変化に伴う$f$の値の変化を\n各成分の変化量$dx_{i}$に、各成分の変化に伴う$f$の変化率$\\dfrac{\\partial f}{\\partial x_{i}}$を掛けた$\\dfrac{ \\partial f}{ \\partial x_{i} }dx_{i}$を全て足し合わせること と考えるという意味だ。以下の式を通して、この表記が直観的で便利であることが分かる。$f=f(x,y,z)$とするとき、\n$$ \\dfrac{df}{dx} = \\frac{ \\partial f}{ \\partial x}\\dfrac{dx}{dx} + \\frac{ \\partial f}{ \\partial y}\\dfrac{dy}{dx} + \\frac{ \\partial f}{ \\partial z}\\dfrac{dz}{dx} = \\dfrac{\\partial f}{\\partial x} $$\n物理学では、次のような形でも頻繁に登場する。$\\left( x(t), y(t), z(t) \\right)$について、\n$$ \\dfrac{d f}{d t} = \\frac{ \\partial f}{ \\partial x}\\dfrac{dx}{dt} + \\frac{ \\partial f}{ \\partial y}\\dfrac{dy}{dt} + \\frac{ \\partial f}{ \\partial z}\\dfrac{dz}{dt} $$\n導出 2変数関数について、次のような方法で$\\eqref{1}$を導出できる。$z=f(x,y)$が与えられたとしよう。$z$の全微分は変数$x$、$y$が変化するときの$z$の変化量であるから、以下のように表される。\n$$ dz = f(x+dx,y+dy)-f(x,y) $$\nここで、右辺に$f(x,y+dy)$を引いて足してから式を整理すると、以下のようになる。 $$ \\begin{align*} dz \u0026amp;= f(x+dx,y+dy) {\\color{blue}-f(x,y+dy)+f(x,y+dy)}-f(x,y) \\\\ \u0026amp;= [f(x+dx,y+dy) -f(x,y+dy)]+[f(x,y+dy)-f(x,y)] \\\\ \u0026amp;= \\frac{f(x+dx,y+dy) -f(x,y+dy)}{dx}dx+\\frac{f(x,y+dy)-f(x,y)}{dy}dy \\\\ \u0026amp;\\approx \\frac{ \\partial f}{ \\partial x}dx + \\frac{ \\partial f}{ \\partial y }dy \\\\ \u0026amp;= \\frac{ \\partial z}{ \\partial x}dx+\\frac{ \\partial z}{ \\partial y}dy \\end{align*} $$\n■\n","id":1773,"permalink":"https://freshrimpsushi.github.io/jp/posts/1773/","tags":null,"title":"全微分、完全微分"},{"categories":"수리물리","contents":"定義 ベクトル関数 $\\mathbf{F}(x,y,z)=F_{x}\\hat{\\mathbf{x}}+F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$について、以下のようなスカラー関数を$\\mathbf{F}$のダイバージェンスdivergence, 発散と定義し、$\\nabla \\cdot \\mathbf{F}$と表記する。\n$$ \\begin{equation} \\nabla \\cdot \\mathbf{F} := \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} \\label{divergence} \\end{equation} $$\n説明 幾何学的に$\\nabla \\cdot \\mathbf{F}\u0026gt;0$の場合、$\\mathbf{F}$が広がり出る、外へ出る形をしていることを意味し、$\\nabla \\cdot \\mathbf{F}\u0026lt;0$の場合は$\\mathbf{F}$が集まる、内へ入る形をしていることを意味し、$\\nabla \\cdot \\mathbf{F}=0$の場合は$\\mathbf{F}$が広がりも集まりもしない、出入りの量が同じ形をしていることを意味する。\nダイバージェンスは発散と翻訳される。生エビ寿司屋では、勾配をグラディエント、回転をカールと使うため、統一感のために発散ではなくダイバージェンスと表記する。\n定義で$\\dfrac{ \\partial F_{x}}{ \\partial x} + \\dfrac{ \\partial F_{y}}{ \\partial y }+ \\dfrac{ \\partial F_{z}}{ \\partial z}$という値を$\\nabla \\cdot \\mathbf{F}$で表記することに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体に何か意味を持つと考えると$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積と外積と間違えやすい。だから$\\nabla$は単なる便利な表記法程度にしか理解しないほうがよく、グラディエント、ダイバージェンス、カールをまとめてデル演算子たちと呼んだり、むしろデル演算子=グラディエントと考えるほうがよいかもしれない。詳細は以下で続ける。\n注意点 $\\nabla \\cdot \\mathbf{F}$は$\\nabla$と$\\mathbf{F}$の内積ではない $\\nabla \\cdot \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の内積ではない。\r内積は基本的に二つのベクトル同士の演算である。$\\nabla \\cdot \\mathbf{F}$を内積と考えることは、$\\nabla$を以下のようなベクトルと見なすことである。\n$$ \\nabla \\overset{?}{=}\\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} +\\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}}+\\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{ \\partial }{ \\partial x},\\ \\dfrac{ \\partial }{ \\partial y},\\ \\dfrac{ \\partial }{ \\partial z} \\right) $$\n確かにこのように考えると、次のようにダイバージェンスの定義$(1)$通りに計算がうまくいくので便利であることは事実である。\n$$ \\nabla \\cdot \\mathbf{F} = \\left( \\dfrac{ \\partial }{ \\partial x},\\ \\dfrac{ \\partial }{ \\partial y},\\ \\dfrac{ \\partial }{ \\partial z} \\right) \\cdot \\left( F_{x}, F_{y}, F_{z} \\right) = \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} $$\nしかし、これが実際に内積であれば、内積は交換法則が成立するため、以下のような等式が成立するという奇妙な結論になる。\n$$ \\mathbf{F} \\cdot \\nabla = F_{x} \\dfrac{\\partial }{\\partial x} + F_{y} \\dfrac{\\partial }{\\partial y} + F_{z} \\dfrac{\\partial }{\\partial z} \\overset{?}{=} \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} = \\nabla \\cdot \\mathbf{F} $$\n実際には$\\nabla \\cdot$の実体はベクトル関数 $\\mathbf{F}(x,y,z)$をスカラー関数 $\\frac{ \\partial F_{x}(x,y,z)}{ \\partial x} + \\frac{ \\partial F_{y}(x,y,z)}{ \\partial y }+ \\frac{ \\partial F_{z}(x,y,z)}{ \\partial z}$に対応させる演算子である。これが何を意味するかというと、$\\operatorname{div}$という関数を次のように定義してみよう。\n$$ \\operatorname{div}(\\mathbf{F}) := \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z},\\qquad \\mathbf{F} = (F_{x}, F_{y}, F_{z}) $$\nここには内積だとかそういう言葉は一切ない。$\\operatorname{div}$という関数は単に変数$\\mathbf{F}$が代入されるたびに$\\dfrac{ \\partial F_{x}}{ \\partial x} + \\dfrac{ \\partial F_{y}}{ \\partial y }+ \\dfrac{ \\partial F_{z}}{ \\partial z}$を与える関数である。これを定義してみると、$\\nabla$というものを$\\nabla = \\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} +\\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}}+\\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}}$と同じベクトルと考えると$\\operatorname{div}$の関数値を表記するのが非常に便利で直感的であることがわかる。だから$\\operatorname{div}(\\mathbf{F})$と表記する代わりに、$\\nabla \\cdot \\mathbf{F}$と表記するのである。実際に専門の数学の教科書では、ダイバージェンスを$\\operatorname{div}$と表記することが容易に見つかるが、これは物理学部と異なり3次元ベクトルを直感的に扱わないためであると考えられる。\nでは$\\nabla \\cdot \\mathbf{F}$を交換法則が成立しない内積として考えてはいけないのか？ いけない。なぜなら$\\nabla \\cdot \\mathbf{F}$で$\\nabla \\cdot$自体が関数(演算子)であり$\\mathbf{F}$が変数である。一方で$\\mathbf{F} \\cdot \\nabla$はそのもの自体が関数(演算子)であるためである。したがって$\\nabla \\cdot \\mathbf{F}$は関数$\\nabla \\cdot$の関数値であり、$\\mathbf{F} \\cdot \\nabla$は（まだ変数が代入されていない）関数である。具体的に$\\mathbf{F} \\cdot \\nabla$という表記は次のような関数$f$を直感的\nに簡単に表記したものである。$f$はベクトル関数$\\mathbf{A}$を変数とする演算子であり、$\\mathbf{A}$の各成分に$\\left( F_{x}\\dfrac{\\partial }{\\partial x} + F_{y}\\dfrac{\\partial }{\\partial y} + F_{z}\\dfrac{\\partial }{\\partial z} \\right)$を適用する関数である。\n$$ \\begin{align*} f (\\mathbf{A}) \\overset{\\text{definition}}{=}\u0026amp; \\left( F_{x}\\dfrac{\\partial A_{x}}{\\partial x} + F_{y}\\dfrac{\\partial A_{x}}{\\partial y} + F_{z}\\dfrac{\\partial A_{x}}{\\partial z} \\right)\\hat{\\mathbf{x}} \\\\ \u0026amp;\\quad+ \\left( F_{x}\\dfrac{\\partial A_{y}}{\\partial x} + F_{y}\\dfrac{\\partial A_{y}}{\\partial y} + F_{z}\\dfrac{\\partial A_{y}}{\\partial z} \\right)\\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad+ \\left( F_{x}\\dfrac{\\partial A_{z}}{\\partial x} + F_{y}\\dfrac{\\partial A_{z}}{\\partial y} + F_{z}\\dfrac{\\partial A_{z}}{\\partial z} \\right)\\hat{\\mathbf{z}} \\\\ \\overset{\\text{notation}}{=}\u0026amp; (\\mathbf{F}\\cdot \\nabla) (\\mathbf{A}) \\end{align*} $$\n何かスカラー関数$\\phi$が変数としてある場合、次のような演算子と考える。\n$$ \\begin{align*} f (\\phi) \\overset{\\text{definition}}{=}\u0026amp; F_{x}\\dfrac{\\partial \\phi}{\\partial x} + F_{y}\\dfrac{\\partial \\phi}{\\partial y} + F_{z}\\dfrac{\\partial \\phi}{\\partial z} \\\\ \\overset{\\text{notation}}{=}\u0026amp; (\\mathbf{F}\\cdot \\nabla) (\\phi) \\end{align*} $$\nしたがって$\\nabla \\cdot \\mathbf{F}$と$\\mathbf{F}\\cdot \\nabla$は$\\nabla$と$\\mathbf{F}$の内積として理解してはいけず、$\\nabla \\cdot$と$\\mathbf{F} \\cdot \\nabla$自体を一つの関数として考えなければならない。これはもちろんダイバージェンスに限った説明ではなく、グラディエント$\\nabla f$やカール$\\nabla \\times \\mathbf{F}$も同様に理解しなければならない。\n導出 まず以下のように3次元空間で微小体積を考えてみよう。\n今、我々の目的は$\\mathbf{F}$がその微小体積内の各座標でどのように見えるかを知ることである。現実に例えるならば$\\mathbf{F}$が熱であればどの方向に、どの速度で流れているかを、$\\mathbf{F}$が水であればこれが蛇口から出ている水なのか、排水溝に入っていく水なのかを知りたいということである。まず$x$軸方向だけを計算してみよう。$\\mathbf{F}$が$d\\mathbf{a}_{1}$を通過する量は二つのベクトルの内積で求めることができる。\n$$ \\begin{align} \\mathbf{F}(x+dx) \\cdot d\\mathbf{a}_{1} \u0026amp;= \\left( F_{x}(x+dx)\\hat{\\mathbf{x}}+F_{y}(x+dx)\\hat{\\mathbf{y}}+F_{z}(x+dx)\\hat{\\mathbf{z}} \\right) \\cdot dydz\\hat{\\mathbf{x}} \\nonumber \\\\ \u0026amp;= F_{x}(x+dx)dydz \\end{align} $$\n$F_{x}(x+dx)dydz \u0026gt;0$の場合、$\\mathbf{F}$が微小体積を抜け出る量であり、$F_{x}(x+dx)dydz\u0026lt;0$の場合は$\\mathbf{F}$が微小体積に入る量である。同様に$\\mathbf{F}$が$d\\mathbf{a}_{2}$を抜け出る量は以下のようである。\n$$ \\begin{equation} \\mathbf{F}(x) \\cdot d \\mathbf{a}_{2} = F_{x}(x)\\hat{\\mathbf{x}} \\cdot(-dydz\\hat{\\mathbf{x}})=-F_{x}(x)dydz \\end{equation} $$\nしたがって$(2) + (3)$は微小体積での$\\mathbf{F}$の$x$軸方向の流入量（流出量）である。\n$$ \\begin{align*} (2) + (3) \u0026amp;=\\left[ F_{x}(x+dx) -F_{x}(x)\\right]dydz \\\\ \u0026amp;= \\frac{F_{x}(x+dx) -F_{x}(x) }{dx}dxdydz \\end{align*} $$\nしかし$dx$が微小距離であるため、$\\dfrac{F_{x}(x+dx) -F_{x}(x) }{dx}\\approx \\dfrac{ \\partial F_{x}}{ \\partial x }$と同様に近似できる。したがって$\\mathbf{F}$が$x$軸方向へ微小体積に入るまたは出る量は以下のように表される。\n$$ \\frac{ \\partial F_{x}}{ \\partial x}dxdydz $$ 同様に$y$軸方向、$z$軸方向について計算すると以下の結果を得る。\n$$ \\frac{ \\partial F_{y}}{ \\partial y}dxdydz \\quad \\text{and} \\quad \\frac{ \\partial F_{z}}{ \\partial z}dxdydz $$\nこれを全て足すと$\\mathbf{F}$が微小体積に入るまたは出る量となり、$dxdydz$で割ると単位体積あたりの流入量（流出量）となる。\n$$ \\frac{ \\partial F_{x}}{ \\partial x}+\\frac{ \\partial F_{y}}{ \\partial y}+\\frac{ \\partial F_{z}}{ \\partial z} $$\nこれからこれを$\\mathbf{F}$のダイバージェンスと呼び、$\\nabla \\cdot \\mathbf{F}$と表記しよう。\n$$ \\nabla \\cdot \\mathbf{F} := \\frac{ \\partial F_{x}}{ \\partial x}+\\frac{ \\partial F_{y}}{ \\partial y}+\\frac{ \\partial F_{z}}{ \\partial z} $$\n■\n導出される過程を見ても分かるように、上で述べた通り$\\nabla \\cdot \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の内積ではない。この点に注意しよう。\n関連する公式 線形性:\n積の規則:\n$$ \\nabla \\cdot (f\\mathbf{A}) = f(\\nabla \\cdot \\mathbf{A}) + \\mathbf{A} \\cdot (\\nabla f) $$ $$ \\nabla \\cdot (\\mathbf{A} \\times \\mathbf{B}) = \\mathbf{B} \\cdot (\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\cdot (\\nabla \\times \\mathbf{B}) $$\n二階導関数:\n$$ \\nabla \\cdot (\\nabla T) = \\dfrac{\\partial^{2} T}{\\partial x^{2}} + \\dfrac{\\partial ^{2} T} {\\partial y^{2}} + \\dfrac{\\partial ^{2} T}{\\partial z^{2}} $$ $$ \\nabla (\\nabla \\cdot \\mathbf{A} ) $$ $$ \\nabla \\cdot (\\nabla \\times \\mathbf{A})=0 $$\nガウスの定理 (発散定理)\n$$ \\int_\\mathcal{V} \\nabla \\cdot \\mathbf{ F} dV = \\oint _\\mathcal{S} \\mathbf{F} \\cdot d \\mathbf{S} $$\n積分公式\n$$ \\int_{\\mathcal{V}} \\left[ T \\nabla^{2} U + (\\nabla T) \\cdot (\\nabla U) \\right] d \\tau = \\oint_{\\mathcal{S}} (T \\nabla U) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left( T \\nabla^{2} U - U \\nabla^{2} T \\right) d \\tau = \\oint_{\\mathcal{S}} \\left( T \\nabla U - U \\nabla T \\right) \\cdot d \\mathbf{a} $$\n部分積分\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$\n参照 デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ ","id":1796,"permalink":"https://freshrimpsushi.github.io/jp/posts/1796/","tags":null,"title":"直交座標系におけるベクトル関数の発散"},{"categories":"함수","contents":"定義 $A \\subset X$ について、以下のように定義される 関数 $\\chi_{A} : X \\to \\mathbb{R}$ を 特性関数 または 指示関数 と言う。\n$$ \\chi _{A}(x) := \\begin{cases} 1, \u0026amp; x\\in A \\\\ 0 ,\u0026amp; x \\notin A \\end{cases} $$\n説明 $\\chi$ はギリシャ文字の カイ だ。学生時代、数学の先生がxを $\\chi$ で書くなって言ったのは、まさに $\\chi$ がxではないからだ。特に、こんなに強い意味を持っているから、適当に使うべきではない。\n数学科では、特に特性関数と呼ばれることはほとんどなく、そのまま [キャラクタリスティック ファンクション] と読む。特に定積分が含まれる方程式で、積分範囲を変更するトリックのためによく使われる。例えば、以下のような場合だ。 $$ \\int _{a} ^{b}f(x)g(x) dx=\\int _{-\\infty}^{\\infty}\\chi_{[a,b]}f(x)g(x)dx $$\n科目によっては、以下のように太字の1で表されることもある。どちらがより多く使われるかは、はっきりしていないが、$\\chi$ は様々な分野でそれぞれ独自の意味を持っているのに対し、$\\mathbf{1}$ は主に指示関数のみを指すので、論文では書籍よりも $\\mathbf{1}$ が $\\chi$ よりも多く使われるようだ。 $$ \\mathbf{1}_{A} = \\chi _{A}(x) $$\n","id":1790,"permalink":"https://freshrimpsushi.github.io/jp/posts/1790/","tags":null,"title":"特性関数、指示関数"},{"categories":"분포이론","contents":"定義 1 自由度$\\nu \u0026gt; 0$に対して、次の確率密度関数を持つ連続確率分布$t \\left( \\nu \\right)$をt-分布という。 $$ f(x) = {{ \\Gamma \\left( {{ \\nu + 1 } \\over { 2 }} \\right) } \\over { \\sqrt{\\nu \\pi} \\Gamma \\left( {{ \\nu } \\over { 2 }} \\right) }} \\left( 1 + {{ x^{2} } \\over { \\nu }} \\right)^{- {{ \\nu + 1 } \\over { 2 }}} \\qquad ,x \\in \\mathbb{R} $$\n$\\Gamma (\\nu)$はガンマ関数だ。 説明 t-分布は、今でもビールで有名なギネス醸造所で働いていたウィリアム・ゴセットWilliam S. Gossetによって発見され、公表された分布である。その当時、企業に所属していたため、彼は学生という筆名で投稿し、それが学生t-分布とも呼ばれるようになった。統計学の新入生は、当初、標本が正規分布に従うと仮定されるが、実際には30個に達しない小さなサンプルで使用される分布に最初に遭遇する。$\\nu \\ge 30$のときは、ほぼ正規分布に収束したとみなされる。\n一方、特に$\\nu = 1$のときの分布はコーシー分布と呼ばれる。\n基本的な性質 モーメント生成関数 平均と分散 [2]: $X \\sim t (\\nu)$であれば $$ \\begin{align*} E(X) =\u0026amp; 0 \u0026amp; \\qquad , \\nu \u0026gt;1 \\\\ \\text{Var}(X) =\u0026amp; {{ \\nu } \\over { \\nu - 2 }} \u0026amp; \\qquad , \\nu \u0026gt; 2 \\end{align*} $$ 定理 二つの確率変数$W,V$が独立であり、$W \\sim N(0,1)$、$V \\sim \\chi^{2} (r)$であるとする。\n$k$次モーメント [a]: $k \u0026lt; r$であれば$\\displaystyle T := { {W} \\over {\\sqrt{V/r} } }$は$k$次のモーメントが存在し $$ E T^{k} = E W^{k} {{ 2^{-k/2} \\Gamma \\left( {{ r } \\over { 2 }} - {{ k } \\over { 2 }} \\right) } \\over { \\Gamma \\left( {{ r } \\over { 2 }} \\right) r^{-k/2} }} $$ 標準正規分布とカイ二乗分布から導かれる [b]: $${ {W} \\over {\\sqrt{V/r} } } \\sim t(r)$$ スチューデントt分布の極限分布として標準正規分布を導く [c]: $T_n \\sim t(n)$であれば $$ T_n \\ \\overset{D}{\\to} N(0,1) $$ F分布を導く [d]: 自由度$\\nu \u0026gt; 0$のt-分布に従う確率変数$X \\sim t(\\nu)$について、次のように定義された$Y$はF分布$F (1,\\nu)$に従う。 $$ Y := X^{2} \\sim F (1,\\nu) $$ $N \\left( \\mu , \\sigma^{2} \\right)$は平均が$\\mu$で分散が$\\sigma^{2}$の正規分布だ。 $\\chi^{2} \\left( r \\right)$は自由度$r$のカイ二乗分布だ。 証明 1 確率変数のモーメント生成関数が存在するとは、すべての$k \\in \\mathbb{N}$に対して$k$次のモーメントが存在することを意味する。しかし、定理[a]によれば、t分布の$k$次モーメントは$k \u0026lt; r$のときに存在するため、モーメント生成関数は存在しない。\n■\n[2] モーメント公式[a]を使用する。\n■\n[a] カイ二乗分布のモーメント: $X \\sim \\chi^{2} (r)$とする。$k \u0026gt; - r/ 2$であれば$k$次モーメントが存在し $$ E X^{k} = {{ 2^{k} \\Gamma (r/2 + k) } \\over { \\Gamma (r/2) }} $$\n$k \u0026lt; r$の両辺に$-1/2$を掛けると$-k/2 \u0026gt; -r/2$となるので $$ \\begin{align*} E T^{k} =\u0026amp; E \\left[ W^{k} \\left( {{ V } \\over { r }} \\right)^{-k/2} \\right] \\\\ =\u0026amp; E W^{k} E \\left( {{ V } \\over { r }} \\right)^{-k/2} \\\\ =\u0026amp; E W^{k} {{ 2^{-k/2} \\Gamma \\left( {{ r } \\over { 2 }} - {{ k } \\over { 2 }} \\right) } \\over { \\Gamma \\left( {{ r } \\over { 2 }} \\right) r^{-k/2} }} \\end{align*} $$\n■\n[b] 結合密度関数から直接導く。\n■\n[c] 確率密度関数にスターリング近似を使用する。\n■\n[d] カイ二乗分布の比によって迂回する。\n■\nコード 以下はコーシー分布、t分布、コーシー分布の確率密度関数を表示するJuliaのコードだ。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = -4:0.1:4\rplot(x, pdf.(Cauchy(), x),\rcolor = :red,\rlabel = \u0026#34;Cauchy\u0026#34;, size = (400,300))\rplot!(x, pdf.(TDist(3), x),\rcolor = :orange,\rlabel = \u0026#34;t(3)\u0026#34;, size = (400,300))\rplot!(x, pdf.(TDist(30), x),\rcolor = :black, linestyle = :dash,\rlabel = \u0026#34;t(30)\u0026#34;, size = (400,300))\rplot!(x, pdf.(Normal(), x),\rcolor = :black,\rlabel = \u0026#34;Standard Normal\u0026#34;, size = (400,300))\rxlims!(-4,5); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pdf\\,of\\, t}(\\nu)\u0026#34;)\rpng(\u0026#34;pdf\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p191.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1667,"permalink":"https://freshrimpsushi.github.io/jp/posts/1667/","tags":null,"title":"t-分布"},{"categories":"초함수론","contents":"要約1 すべての$u \\in L_{\\mathrm{loc} }^1(\\Omega) $に対して、次のように定義される超関数 $T_{u} \\in D^{\\ast}(\\Omega)$が存在する。\n$$ T_{u} (\\phi) := \\int_{\\Omega} u(x)\\phi (x)dx, \\quad \\phi \\in D(\\Omega) $$\n説明 $\\mathcal{D}(\\Omega)$はテスト関数空間である。このように定義される超関数を正則超関数regular distributionと呼ぶ。また、上の式は内積空間の観点から見ると$u$と$\\phi$の内積と同じであるため、以下のように表記することもある。\n$$ T_{u}(\\phi)=\\langle u , \\phi \\rangle $$\n上の定理によれば、局所積分可能な関数を超関数として扱ってもよい。このような理由で、超関数を一般化された関数と呼ぶことがある。\n証明 上で定義した$T_{u}$が超関数であるかどうかは、$\\mathcal{D}$の連続性と線形性を持つ汎関数であることを示すことだ。線形性は積分によって定義されているため自明であり、連続性を示せば良い。このときの連続性とはテスト関数空間での収束による連続性であることを忘れないで欲しい。\n$\\phi_{j} \\rightarrow \\phi\\ \\ \\mathrm{in}\\ D(\\Omega)$と仮定する。すると、収束の定義により、次のような$K \\Subset\\Omega$が存在する。\n$$ \\mathrm{supp}(\\phi_{j}-\\phi) \\subset K\\quad \\forall\\ j $$\nすると、$u$は局所積分可能であるため、$M\u0026gt;0$に対して以下の式が成り立つ。\n$$ \\begin{align*} \\left| T_{u}(\\phi_{j})-T_{u}(\\phi) \\right| \u0026amp;= \\left| \\int_{K} u(x)\\left( \\phi_{j}(x) -\\phi (x) \\right) dx \\right| \\\\ \u0026amp; \\le \\sup \\limits_{x\\in K} \\left| \\phi_{j} (x) - \\phi (x) \\right| \\int_{K} |u(x)|dx \\\\ \u0026amp;\\le \\sup \\limits_{x\\in K} \\left| \\phi_{j} (x) - \\phi (x) \\right|M \\end{align*} $$\nこのとき、仮定により$\\phi_{j}(x) \\rightrightarrows \\phi (x)$であるため、次が成り立つ。\n$$ \\sup \\limits_{x\\in K} \\left| \\phi_{j} (x) - \\phi (x) \\right|M \\to 0 \\quad \\text{as } j \\rightarrow \\infty $$\nしたがって、次が成り立つ。\n$$ T_{u}( \\phi_{j} ) \\rightarrow T_{u}(\\phi) \\quad \\text{as } j \\rightarrow \\infty $$\nゆえに、$T_{u}$は$\\mathcal{D}$で連続である。\n■\nすべての超関数が上に述べた形であれば扱いやすいが、残念ながらそうではない。\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p20-21\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1078,"permalink":"https://freshrimpsushi.github.io/jp/posts/1078/","tags":null,"title":"すべての局所可積分関数が超関数に拡張可能であることを証明"},{"categories":"초함수론","contents":"定義1 開集合 $\\Omega \\subset \\mathbb{R}^{n}$と関数 $\\phi : \\Omega \\to \\mathbb{C}$が与えられているとする。$\\phi$が無限に微分可能で、その導関数が全部連続であり、コンパクトサポートを持っていれば、テスト関数と呼ばれる。テスト関数の関数空間は$C_{c}^{\\infty}(\\Omega)$または単純に$\\mathcal{D}(\\Omega)$と表される。\n説明 test functionまたはtesting functionとも呼ばれる。$\\phi$がテスト関数と名づけられたのは、$\\phi$自体を扱いたいわけではなく、何か他の関数を定義し、その関数の性質を研究するために使いたいからである。具体的には、テスト関数は数学的に曖昧な関数、例えばディラックのデルタ関数などを厳密に定義するために使用される。テスト関数の具体的な例にはモーリファイアがある。\n定理2 $\\phi$がテスト関数ならば、その導関数もテスト関数である。\n$$ \\phi \\in \\mathcal{D}(\\Omega) \\implies \\frac{ \\partial \\phi}{ \\partial x_{i}} \\in \\mathcal{D}(\\Omega) (i=1,\\cdots,n) $$\nこの場合、$x=(x_{1},\\cdots,x_{n})\\in \\mathbb{R}^{n}$である。\n証明 テスト関数の定義により$\\dfrac{ \\partial \\phi}{ \\partial x_{i}} \\in C^{\\infty}$は自明である。 $x_{0} \\notin \\mathrm{supp} \\phi$としよう。すると、$x_{0} \\in \\left( \\mathrm{supp} \\phi \\right)^{c}$であり、サポートは閉集合であるため$(\\mathrm{supp} \\phi)^{c}$は開集合である。したがって、開集合の定義により、$x_{0}$を含む何らかの近傍 $N_{x_{0}}$が存在する。また、サポートの定義により、$N_{x_{0}}$上で$\\phi=0$であり、当然$\\dfrac{ \\partial \\phi}{ \\partial x_{i}}=0$である。これは$x_{0} \\notin \\mathrm{supp} \\dfrac{ \\partial \\phi}{ \\partial x_{i}}$であることを意味する。したがって、以下が成り立つ。\n$$ \\mathrm{supp} \\frac{ \\partial \\phi}{ \\partial x_{i} } \\subset \\mathrm{supp} \\phi $$\nコンパクト集合の閉部分集合はコンパクトであるため、$\\mathrm{supp} \\dfrac{ \\partial \\phi}{ \\partial x_{i}}$はコンパクトである。■\n系 $\\phi,\\phi_{1},\\phi_{2} \\in \\mathcal{D}(\\mathbb{R}^{n})$、$x_{0}\\in \\mathbb{R}^{n}$、$a \\in \\mathbb{R}\\setminus \\left\\{ 0 \\right\\}$、$\\psi \\in C^{\\infty}(\\mathbb{R}^{n})$としよう。すると、以下が成り立つ：\n(a) $\\phi (x-x_{0})$、$\\phi (-x)$、$\\phi (ax)\\in \\mathcal{D}(\\mathbb{R}^{n})$\n(b) $\\psi \\phi \\in \\mathcal{D}(\\mathbb{R}^{n})$\n(c) $\\phi_{1} * \\phi_{2} \\in \\mathcal{D}$\n明らかなので、証明は省略する。\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p19-20\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDaniel Eceizabarrena perez, Distribution Theory and Fundamental Solutions of Differential Operators (2015), p1-3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1782,"permalink":"https://freshrimpsushi.github.io/jp/posts/1782/","tags":null,"title":"テスト関数とテスト関数空間"},{"categories":"초함수론","contents":"定義1 2 $\\Omega \\subset \\mathbb{R}^{n}$が開集合だとしよう。テスト関数空間の連続な線形汎関数 $T : \\mathcal{D}(\\Omega) \\to \\mathbb{C}$を超関数distributionと定義する。つまり、超関数はテスト関数空間の双対空間の要素だ。だから、\n$$ T \\in \\mathcal{D}^{\\ast} $$\nと表記し、$D^{\\ast}$を**（シュワルツ）超関数空間**(Schwartz) distribution spaceと呼ぶ。\n説明 distributionという名前は、質量が一点に集中した点質量などを表現するために考案されたディラックのデルタ関数の影響を受けているようだ。直訳すれば分布だが、数学者は超関数と呼ぶ。他の名称であるgeneralized functionは、厳密には関数ではないディラックのデルタ関数のようなものを厳密に定義した概念で、そのために付けられた。超関数の定義で重要な部分は連続性である。連続関数になる同値条件によって、超関数$T$が連続であるということは以下を意味する。\n$$ \\phi_{j} \\to \\phi \\implies T(\\phi_{j}) \\to T(\\phi) $$\nしかし、テスト関数空間での収束を少し特別に定義した。したがって、具体的に超関数の定義を再記載すると、以下の通りになる。\nテスト関数空間 $\\mathcal{D}(\\Omega)$の汎関数 $T : \\mathcal{D}(\\Omega) \\to \\mathbb{C}$が線形(a)であり、かつ連続(b)である場合、これを超関数という。\n(a) $T(a\\phi + b \\psi ) = aT(\\phi)+bT(\\psi)\\quad (\\phi,\\psi\\in \\mathcal{D},\\ a,b\\in\\mathbb{C})$\n(b) $\\phi_{j} \\to \\phi \\text{ in } \\mathcal{D} \\implies T(\\phi_{j}) \\to T(\\phi)$\n以下の条件を満たす$\\mathcal{D}(\\Omega)$の関数列 $\\left\\{ \\phi_{j} \\right\\}$に対して、$\\phi_{j} \\to \\phi \\text{ in } \\mathcal{D}$と定義する。\n(c) $\\mathrm{supp} (\\phi_{j}-\\phi) \\subset K\\quad \\forall\\ j$を満たす$K \\Subset \\Omega$が存在する。\n(d) 各多重指数 $\\alpha$について、$D^{\\alpha}\\phi_{j}$が$D^{\\alpha} \\phi$に一様収束する。\n超関数の定義に従って、ディラックのデルタ関数は以下のように定義できる。\n$$ \\begin{align*} \\delta_{a} : \\mathcal{D} \u0026amp;\\to \\mathbb{C} \\\\ \\phi \u0026amp;\\mapsto \\phi (a) \\end{align*} $$\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p19-20\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p306-307\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1009,"permalink":"https://freshrimpsushi.github.io/jp/posts/1009/","tags":null,"title":"超関数、一般化された関数"},{"categories":"바나흐공간","contents":"関数解析学は英語でfunctional analysisです。function analysisではなくfunctionalは一体何を意味しているのか、疑問に思うかもしれません。まず、functionalという単語を見ると、function+alで構成されているように見えます。つまり、functionの形容詞形のように見え、この感じで解釈すれば、functionalは「関数的な（もの）」や「関数のような（もの）」程度の意味を含んでいるように思われます。この感じは、別の名前であるgeneralized functionでも見つけることができます。なぜ関数ではなく、関数のようなものと名付けられたのか、以下のfunctionalの一般的な定義を見ながら考えてみましょう。\nベクトル空間$X$に対して以下のような関数$f$をfunctionalと呼びます。\n$$ f : X \\to \\mathbb{C} $$\nこの定義を見て「定義上は$f$はfunctionなのに、なぜfunctionalという名前を付けたのか？」と思うかもしれません。上記のような条件を満たす関数に特別な名前を付けるのは納得できるものの、なぜその名前がfunctional（関数的なもの）でなければならないのかは、納得がいかないかもしれません。\n関数の定義によれば、上の$f$は関数ですが、なぜ「関数的なもの」という名前を付けたのかを理解するためには、関数解析学が発展し始めた時代の数学について知る必要があります。現代に生まれ、数学を学ぶ人は、関数を以下のように知っています。\n全ての$x_{1}, x_{2} \\in X$に対して$x_{1} = x_{2} \\implies f(x_{1}) = f(x_{2})$を満たす$f(x_{1})$と$f(x_{2})$が$Y$に存在するならば、対応$f$を以下のように表記し、$X$から$Y$への関数と言います。\n$$ f : X \\to Y $$\n集合論で厳密に定義すると、以下のようになります。\n空集合でない二つの集合$X$、$Y$が与えられたとします。二項関係$f \\subset (X,Y)$が以下を満たすならば、関数と呼び、$f : X \\to Y$のように表されます。\n$$ (x ,y_{1}) \\in f \\land (x,y_{2}) \\in f \\implies y_{1} = y_{2} $$\n上の定義からわかるように、二つの集合$X$、$Y$には何の条件もありません。したがって、$X$が$\\mathbb{R}$であろうと、関数空間であろうと、何の問題もありません。しかし、19世紀後半の数学者にとって、関数とは上記のようではありませんでした。当時の数学者は、関数を値から値へのマッピング、つまり、$f:\\mathbb{R} \\to \\mathbb{R}$に限定して考えていました1。値を与えると、ルールに従って別の値を与える「公式」のように扱ったのです。これは、中学校で初めて関数を学ぶ時の受け入れ方と同じです。\nなぜ関数をそのように考えたのか疑問に思うかもしれませんが、ある意味で当然です。関数の厳密な定義は、上記で見たように集合論を通じて作られました。現代集合論の創始者であるカントールが1845年生まれであることを思い出せば、19世紀後半〜20世紀初頭の数学者までもが関数を数字と数字の間の公式程度に考えていた事実は全く不思議ではありません。元々、関数をなぜ機能functionと呼んだのでしょうか。\n以下のような関数を考えてみましょう。\n微分可能な関数$f$に対して、閉区間$[a,b]$での曲線$y=f(x)$の長さは以下のようになります。\n$$ L(f)=\\int_{a}^{b} \\sqrt{1+ f^{\\prime}(x)^{2}}dx $$\n当時の数学者にとって、$L$は関数ではありませんでした。値を値へ送るのではなく、関数を値へ送るためです。したがって、$L$を「関数の関数」と呼ばなければならないのですが、関数ではないため、用語に関する曖昧さが存在しました。そのため、ボルテラVolterraはfunctions of linesとも呼びました。この時、フランスの数学者アダマールHadamardがこのような「関数ではないが関数のような関数の関数」をfoncionnellesと呼ぶことを初めて提案しました。これは後に英語表現でfunctionalとなりました。\nもちろん、集合論で関数を厳密に定義した後は、functionalもfunctionになりましたが、定義域が関数空間であることが明確になるため、functionalという表現が続けられたようです。集合の集合、set of setsをcollectionやfamillyと表現するのと同様です。functionalがfunctionであることに概念的な問題はなくても、関数の関数という表現は混乱を招きやすいため、functionalという用語が生き残らなかったのではないでしょうか。この学問の名前がfunctional analysisと固まったのも影響があるでしょう。functionalは後に一般化され、ベクトル空間から複素数空間へのマッピングを意味するようになりました。\nDistribution Theory 上述のように、最初にfunctionalは関数ではないが関数のようなものを指すために作られた言葉です。最終的に関数が集合論を通じて定義された後は、functionalも関数になりましたが。面白いことに、このようなfunctionalが実際に「関数ではないが関数のようなもの」を説明するために使われるようになったのです。ディラックのデルタ関数は、ポアソンとコーシーがフーリエ解析を研究する過程で最初に考案され、理論物理学者のポール・ディラックが量子力学で広く使用されることで有名になりました。2 3 デルタ関数のナイーブnaiveな定義は、以下の条件を満たす関数です。\n$$ \\delta (x)=\\begin{cases} \\infty, \u0026amp; x=0 \\\\ 0, \u0026amp; x\\ne 0\\end{cases} \\quad \\\u0026amp; \\quad \\int_{-\\infty}^{\\infty}\\delta (x)dx=1 $$\nしかし、発散するということは値ではなく状態であるため、厳密に言えばデルタ関数は関数ではありませんでした。しかし、単に関数として扱って良い結果を得られました。1935年4にこの概念を知ったフランスの数学者ローラン-モワーズ・シュワルツLaurent-moise Schwartzが15年間の研究の末、1950年5にTheorie des distributionsという本でデルタ関数を数学的に厳密に定義しました。6 いくつかの良い条件を持つスムース関数をテスト関数と呼び、テスト関数の空間を$\\mathcal{D}$と表記します。distributionは$\\mathcal{D}$から$\\mathbb{C}$への写像であり、これはfunctionalになります。functionalという名前は、当初は関数だと思われていなかったものに付けられましたが、時間が経つにつれて、実際に関数ではないが関数のように扱うものに対する理論を立てるのに使われるようになりました。驚くべき偶然です。\nhttps://courses.mai.liu.se/GU/TATM85/FA-history.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Dirac_delta_function\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Dirac_delta_function\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n21歳でした。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n36歳でした。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://horizon.kias.re.kr/11905/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1780,"permalink":"https://freshrimpsushi.github.io/jp/posts/1780/","tags":null,"title":"ファンクショナルがファンクショナルと名付けられた理由"},{"categories":"수리물리","contents":"定義 スカラー関数 $f=f(x,y,z)$に対して、以下のようなベクトル関数を $f$のグラディエントgradient, 勾配と定義し、$\\nabla f$と表記する。\n$$ \\nabla f := \\frac{ \\partial f}{ \\partial x }\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) $$\n説明 グラディエントは勾配、坂、水勾配などと翻訳される。坂、水勾配はグラディエントの古い翻訳で、最近ではあまり使われない。また、坂は勾配の漢字語であるため、勾配と同じ意味である。グラディエントは実際にベクトルであるため、勾配という言葉はグラディエントが持つ意味をすべて含むには不十分であるように思われる。生しらす寿司店では、勾配という言葉の代わりにグラディエントと統一する。\n幾何学的には $\\nabla f$は $f$が最も急激に変化する方向を意味する。つまり点 $(x,y,z)$で $f$の増加率が最も大きい方向はベクトル $\\left( \\dfrac{\\partial f(x,y,z)}{\\partial x}, \\dfrac{\\partial f(x,y,z)}{\\partial y}, \\dfrac{\\partial f(x,y,z)}{\\partial z} \\right)$であるということである。これは微分係数を多次元に拡張したものに過ぎない。$f$が増加していれば微分係数が正、$f$が減少していれば微分係数が負であるという概念と同じである。\n一方で定義で $\\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right)$という値を $\\nabla f$と表記するとしたことに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体に何か意味を持つと考えると$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積や外積と誤解するのにちょうどよい。したがって、$\\nabla$は単なる便利な表記法としてのみ理解するべきであり、グラディエント、ダイバージェンス、カールをまとめてデル演算子と呼んだり、デル演算子=グラディエントと考える方が良いかもしれない。詳細は以下で続く。\n注意点 $\\nabla f$は $\\nabla$と $f$の積ではない グラディエントを理解する上で重要なのは、$\\nabla f$がベクトル $\\nabla = (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})$とスカラー $f$の積ではないという事実である。もちろん、そう考えると直感的で良さそうだが、実際は逆である。$\\nabla$を $(\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\\npartial y}, \\frac{\\partial }{\\partial z})$というベクトルとして説明することで、ベクトルとスカラーの積のように見えるようにするのである。もし $\\nabla f$がベクトル $\\nabla$とスカラー $f$の積であれば、ベクトルとスカラーの積は交換可能であるため、次のような奇妙な数式が成り立つことになる。\n$$ \\nabla f = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) \\overset{?}{=} \\left( f\\dfrac{\\partial }{\\partial x}, f\\dfrac{\\partial }{\\partial y}, f\\dfrac{\\partial }{\\partial z} \\right) = f\\nabla $$\nこの奇妙な数式が飛び出したのは、実際には $\\nabla$はベクトルではなく、$\\nabla f$はベクトルとスカラーの積ではないためである。$\\nabla$はベクトルではなく、$f(x,y,z)$というスカラー関数を $\\left( \\frac{\\partial f(x,y,z)}{\\partial x}, \\frac{\\partial f(x,y,z)}{\\partial y}, \\frac{\\partial f(x,y,z)}{\\partial z} \\right)$というベクトル関数に対応させる演算子である。関数自体を変数とする $\\operatorname{grad}$という関数を次のように定義してみよう。\n$$ \\begin{equation} \\operatorname{grad} (f) = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right), \\quad f=f(x,y,z) \\end{equation} $$\nこの定義から、ベクトルとスカラーの積という説明は必要ない。$\\operatorname{grad}$は単に変数として $f$が入力されると、$(1)$の規則に従って関数値を持つ関数（演算子）に過ぎない。しかし $\\operatorname{grad} (f)$の関数値をよく見ると、$\\operatorname{grad} = \\nabla$と表記し、これを $\\nabla = (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})$というベクトルとして説明すると直感的で便利な表記法になるのである。\nこれは本質的な意味を正確に説明するものではないが、計算や理解の便利さのために使われる他の表記法には微分のライプニッツ表記法がある。$\\dfrac{dy}{dx}$という表記法を採用し、分数のように扱うと、変化率という意味を理解するのに便利で、無意識に掛け算や約分などの計算をしても実際の結果とピタリと合う。しかし、皆さんは $\\dfrac{dy}{dx}$は分数ではないことを知っている。そう見えるだけで、そう扱うと計算が便利なだけである。$\\nabla f$も同様に、ベクトルとスカラーの積に見えるだけで、そう扱うと計算が便利なのであって、実際にそうであるわけではない。\nでは $f\\nabla$は何か？ 上の説明に従えば、$\\nabla$は一つの関数であるため、$\\nabla f = \\nabla(f)$は $\\nabla$という関数に $f$という変数を代入したときに得られる関数値である。一方で $f \\nabla$はそれ自体が一つの関数であり、$g$という関数を変数として代入したときに以下のように関数値を対応させる関数（演算子）である。\n$$ (f\\nabla) (g) = f\\left( \\dfrac{\\partial g}{\\partial x}, \\dfrac{\\partial g}{\\partial y}, \\dfrac{\\partial g}{\\partial z} \\right) = \\left( f\\dfrac{\\partial g}{\\partial x}, f\\dfrac{\\partial g}{\\partial y}, f\\dfrac{\\partial g}{\\partial z} \\right) $$\nもちろん、$f \\nabla g$\nという関数値を見たときには、$f \\nabla$に $g$を代入したものと考えても良いし、スカラー関数 $f$とベクトル関数 $\\nabla g$の積と見ても良い。\n導出 1次元 上の図を見よう。$f_{1}$の点 $x=2$での微分係数は $4$である。$4$という値は関数 $f_{1}$が点 $x=2$でどれほど傾いているかを教えてくれる量だけでなく、それだけではない。$4$の前にある $+$という符号が $f_{1}$のグラフは $x$が増加する方向に増加するという事実も教えてくれる。したがって、微分係数 $4$は単なるスカラーではなく、1次元ベクトル $4\\hat{\\mathbf{x}}$として理解すべきである。\n同様に、$f_{2}$の $x=2$での微分係数は $-3$であり、これは傾きの程度が $3$であることと、$x$が増加する方向に進むと $f_{2}$のグラフが減少するという意味も含んでいる。つまり、符号を方向と考えた場合、微分係数の方向は関数のグラフが大きくなる方向を向いているという話である。別の言い方をすると、微分係数が指し示す方向に進めば、グラフの頂点を見つけることができるということである。\n3次元に拡張する前に、$y$の $x$での微分係数 $\\dfrac{ d y}{ d x}=a$をまるで分数のように扱えることを思い出そう。これは微分を数学的に厳密に扱う方法ではないが、幾何学的な意味を理解する上での助けとなり、その利点がある。ライプニッツは $dy$、$dx$を $y$と $x$の非常に小さな変化量、微分素と考え、その変化量の比率を微分係数と呼んだ。1\n$$ dy=adx $$\n余談だが、このように考えるとなぜ $a$を微分 \u0026lsquo;係数\u0026rsquo;と呼ぶのか理解できる。\n3次元 ここで3次元スカラー関数 $f=f(x,y,z)$と位置ベクトル $\\mathbf{r}=x\\hat{\\mathbf{x}}+y\\hat{\\mathbf{y}}+z\\hat{\\mathbf{z}}$が与えられたとしよう。$f$の変化量は全微分で表される。\n$$ \\begin{equation} df=\\frac{ \\partial f}{ \\partial x }dx + \\frac{ \\partial f}{ \\partial y}dy+\\frac{ \\partial f}{ \\partial z}dz \\end{equation} $$\n$\\mathbf{r}$の変化量は以下のようである。\n$$ d\\mathbf{r}=dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}} $$\nこれで1次元の時と同じように、$df$と $d\\mathbf{r}$の間の比率を表す何かを探してみよう。しかし、$df$はスカラーで $d\\mathbf{r}$はベクトルであるため、その \u0026lsquo;何か\u0026rsquo;はベクトルであり、$df$はその何かと $d\\mathbf{r}$の内積として表現されることを想像できる。したがって\n、とりあえずその何かを $\\mathbf{a}=a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}}$と表記して、以下のように表現してみよう。\n$$ \\begin{align*} df=\\mathbf{a}\\cdot d\\mathbf{r}\u0026amp;=(a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}})\\cdot(dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}}) \\\\ \u0026amp;= a_{1}dx+a_{2}dy+a_{3}dz \\end{align*} $$\nこれを $(2)$と比較すると、以下の結果を得る。\n$$ \\mathbf{a}=\\frac{ \\partial f}{ \\partial x}\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} $$\nこれから、このベクトル $\\mathbf{a}$を $\\nabla f$と表記し、$f$のグラディエントと呼ぶことにしよう。グラディエントの方向は関数 $f$のグラフが最も大きく増加する方向を指し、その大きさはその程度を示す。\n関連する公式 線形性:\n$$ \\nabla (f + g) = \\nabla f + \\nabla g $$\n積の規則:\n$$ \\nabla{(fg)}=f\\nabla{g}+g\\nabla{f} $$ $$ \\nabla(\\mathbf{A} \\cdot \\mathbf{B}) = \\mathbf{A} \\times (\\nabla \\times \\mathbf{B}) + \\mathbf{B} \\times (\\nabla \\times \\mathbf{A})+(\\mathbf{A} \\cdot \\nabla)\\mathbf{B}+(\\mathbf{B} \\cdot \\nabla) \\mathbf{A} $$\n2次導関数:\n$$ \\nabla \\cdot (\\nabla T) = \\dfrac{\\partial^{2} T}{\\partial x^{2}} + \\dfrac{\\partial ^{2} T} {\\partial y^{2}} + \\dfrac{\\partial ^{2} T}{\\partial z^{2}} $$ $$ \\nabla \\times (\\nabla T)= \\mathbf{0} $$ $$\\nabla (\\nabla \\cdot \\mathbf{A} ) $$\n勾配の基本定理\n$$ T(b)-T(a) = \\int _{a}^{b} (\\nabla T) \\cdot d\\mathbf{l} $$\n積分公式\n$$ \\int_{\\mathcal{V}} (\\nabla T) d \\tau = \\oint_{\\mathcal{S}} T d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left[ T \\nabla^{2} U + (\\nabla T) \\cdot (\\nabla U) \\right] d \\tau = \\oint_{\\mathcal{S}} (T \\nabla U) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left( T \\nabla^{2} U - U \\nabla^{2} T \\right) d \\tau = \\oint_{\\mathcal{S}} \\left( T \\nabla U - U \\nabla T \\right) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\n部分積分\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$ $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n一緒に見る デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ https://pomp.tistory.com/941?category=37772\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1778,"permalink":"https://freshrimpsushi.github.io/jp/posts/1778/","tags":null,"title":"3次元デカルト座標系におけるスカラー関数の勾配"},{"categories":"매트랩","contents":"方法 clear コマンド コマンドウィンドウにclearと入力すると、作業スペースが初期化される。\n作業スペースを消去する(Alt+T+O) 作業スペースウィンドウを右クリックすると、\u0026lsquo;作業スペースを消去する(O)\u0026lsquo;を選択できる。押すと作業スペースが初期化される。これはショートカットAlt+T+Oでも実行できるが、エディターが開いている状態ではできない。\n直接選択して削除 全体をドラッグして選択するか、Ctrl+aで全選択してDeleteを押すと削除できる。\n他言語 Rで ","id":1758,"permalink":"https://freshrimpsushi.github.io/jp/posts/1758/","tags":null,"title":"MATLABで作業スペースを初期化し、すべての変数を削除する方法"},{"categories":"분포이론","contents":"定義 平均 $\\mu \\in \\mathbb{R}$ と分散 $\\sigma^{2} \u0026gt; 0$ に対し、以下のような確率密度関数を持つ連続確率分布 $N \\left( \\mu,\\sigma^{2} \\right)$ を正規分布Normal Distributionという。\n$$ f(x) = {{ 1 } \\over { \\sqrt{2 \\pi} \\sigma }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( {{ x - \\mu } \\over { \\sigma }} \\right)^{2} \\right] \\qquad, x \\in \\mathbb{R} $$\n特に、以下のような確率密度関数を持つ正規分布 $N \\left( 0,1^{2} \\right)$ を標準正規分布という。\n$$ f(z) = {{ 1 } \\over { \\sqrt{2 \\pi} }} \\exp \\left[ - {{ z^{2} } \\over { 2 }} \\right] $$\n説明 正規分布の別名はガウス分布Gaussian Distributionだ。歴史的には、ガウスが1809年に最小二乗法に関する研究で正規分布を紹介したことで広く知られるようになった。正規分布の本質を最初に理解した人がガウスであると断言することはできないが、ガウスは正規分布の異名を持つにふさわしい人物である。\n1794年、たった17歳のガウスは、日常や研究で遭遇する測定値から真値を求める方法についてのインスピレーションを得た。ガウスは頻繁に通る道で自分の歩数を数え、そのデータを収集してグラフに描き、鐘型の曲線を得た。それはヒストグラムという概念がなかった時代の発見だったが、ガウス自身はこれらの正規分布と最小二乗法の概念がすでに広く知られていて、誰もが使用している技術だと思っていた1。まさに圧倒的な天才性だ。また、正規分布に関連する多くの計算にガウス積分が使われることもある。\nその後、正規分布は広く研究され、科学全般になくてはならないツールになった。それほど馴染み深いため、一般人は統計学とは、結局のところ、データが正規分布に従うと仮定して平均分散を求めるだけではないかという誤解を持つことがある。そのような過小評価が統計学への進学につながった場合、それは残念なことだが、非専門家にはその程度の説明で十分かもしれない。それほど正規分布が重要で強力であるという意味での話だ。\n基本性質 モーメント生成関数 [1]: $$m(t) = \\exp \\left( \\mu t + {{ \\sigma^{2} t^{2} } \\over { 2 }} \\right) \\qquad , t \\in \\mathbb{R}$$ 平均と分散 [2] : $X \\sim N\\left( \\mu , \\sigma^{2} \\right)$ の場合 $$ \\begin{align*} E(X) =\u0026amp; \\mu \\\\ \\text{Var} (X) =\u0026amp; \\sigma^{2} \\end{align*} $$ 十分統計量と最尤推定量 [3] : 正規分布に従うランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim N \\left( \\mu , \\sigma^{2} \\right)$ が与えられたとする。 十分統計量 $T$ と最尤推定量 $\\left( \\hat{\\mu}, \\widehat{\\sigma^{2}} \\right)$ は以下の通りである。 $$ \\begin{align*} T =\u0026amp; \\left( \\sum_{k} X_{k}, \\sum_{k} X_{k}^{2} \\right) \\\\ \\left( \\hat{\\mu}, \\widehat{\\sigma^{2}} \\right) =\u0026amp; \\left( {{ 1 } \\over { n }} \\sum_{k} X_{k}, {{ 1 } \\over { n }} \\sum_{k} \\left( X_{k} - \\overline{X} \\right)^{2} \\right) \\end{align*} $$\nエントロピー [4] : (自然対数を選んだ場合)正規分布のエントロピーは以下の通りである。 $$ H = \\ln \\sqrt{2\\pi e \\sigma^{2}} $$ 定理 正規分布の具体的な重要性を長々と説明する必要はなく、以下のように単に定理を並べるだけで十分である。見てみよう。\n中心極限定理 [a]: $\\left\\{ X_{k} \\right\\}_{k=1}^{n}$ がiid 確率変数で、確率分布 $\\left( \\mu, \\sigma^2 \\right) $ に従うとすると、$n \\to \\infty$ の時 $$ \\sqrt{n} {{ \\overline{X}_n - \\mu } \\over {\\sigma}} \\overset{D}{\\to} N (0,1) $$ カイ二乗分布との関係 [b]: $X \\sim N(\\mu,\\sigma ^2)$ ならば $$ V=\\left( { X - \\mu \\over \\sigma} \\right) ^2 \\sim \\chi ^2 (1) $$ 二項分布の極限分布としての標準正規分布の導出 [c]: $X_i \\sim B(1,p)$ であり、$Y_n = X_1 + X_2 + \\cdots + X_n$ の場合、$Y_n \\sim B(n,p)$ である $$ { { Y_n - np } \\over {\\sqrt{ np(1-p) } } }\\overset{D}{\\to} N(0,1) $$ ポアソン分布の極限分布としての標準正規分布の導出 [d]: $X_{n} \\sim \\text{Poi} \\left( n \\right)$ であり、$\\displaystyle Y_{n} := {{ X_{n} - n } \\over { \\sqrt{n} }}$ の場合 $$ Y_{n} \\overset{D}{\\to} N(0,1) $$ スチューデントのt分布の極限分布としての標準正規分布の導出 [e]: $T_n \\sim t(n)$ の場合 $$ T_n \\ \\overset{D}{\\to} N(0,1) $$ 正規分布とカイ二乗分布からt分布の導出 [f]: 二つの確率変数 $W,V$ が独立であり、$W \\sim N(0,1)$、$V \\sim \\chi^{2} (r)$ の場合 $$ T = { {W} \\over {\\sqrt{V/r} } } \\sim t(r) $$ 証明 戦略：ガウス積分が使用できるように指数部分を完全平方形にして標準正規分布のモーメント生成関数から導き出し、置換により正規分布のモーメント生成関数を得る。\nガウス積分: $$ \\int_{-\\infty}^{\\infty} e^{-x^2} dx= \\sqrt{\\pi} $$\n[1] 2 $\\displaystyle Z := {{ X - \\mu } \\over { \\sigma }} \\sim N(0,1)$ とすると、そのモーメント生成関数は\n$$ \\begin{align*} m_{Z}(t) =\u0026amp; \\int_{-\\infty}^{\\infty} \\exp (tz) {{ 1 } \\over { \\sqrt{2 \\pi} }} \\exp \\left[ - {{ 1 } \\over { 2 }} z^{2} \\right] dz \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - {{ 1 } \\over { 2 }} z^{2} + tz \\right] dz \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( z - t \\right)^{2} + {{ t^{2} } \\over { 2 }} \\right] dz \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( z - t \\right)^{2} \\right] \\exp \\left[ {{ t^{2} } \\over { 2 }} \\right] dz \\\\ =\u0026amp; \\exp \\left[ {{ t^{2} } \\over { 2 }} \\right] {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - w^{2} \\right] \\sqrt{2} dw \\\\ =\u0026amp; \\exp \\left[ {{ t^{2} } \\over { 2 }} \\right] \\end{align*} $$\nすると、$X \\sim N \\left( \\mu , \\sigma^{2} \\right)$ のモーメント生成関数は\n$$ \\begin{align*} m_{X}(t) =\u0026amp; E \\left[ \\exp ( t X ) \\right] \\\\ =\u0026amp; E \\left[ \\exp \\left( t (\\sigma Z + \\mu) \\right) \\right] \\\\ =\u0026amp; \\exp(\\mu t) E \\left[ \\exp \\left( t \\sigma Z \\right) \\right] \\\\ =\u0026amp; \\exp(\\mu t) \\exp \\left( {{ t^{2} \\sigma^{2} } \\over { 2 }} \\right) \\\\ =\u0026amp; \\exp \\left( \\mu t + {{ \\sigma^{2} t^{2} } \\over { 2 }} \\right) \\end{align*} $$\n■\n[2] モーメント生成関数を使用して直接導く。\n■\n[3] 直接導く。\n■\n[4] 直接導く。\n■\n[a] モーメント法を応用する。\n■\n[b] 確率密度関数を直接導く。ガンマ関数とガンマ分布、カイ二乗分布との関係が使われる。\n■\n[c] 中心極限定理を使用して証明される。\n■\n[d] モーメント生成関数を使用して証明される。\n■\n[e] 難しい。スターリング近似を通じて確率密度関数が収束することを証明する。\n■\n[f] 簡単だが複雑。確率密度関数を直接導く。\n■\nコード 以下はコーシー分布、t分布、コーシー分布の確率密度関数を示すJuliaのコードである。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = -4:0.1:4\rplot(x, pdf.(Cauchy(), x),\rcolor = :red,\rlabel = \u0026#34;Cauchy\u0026#34;, size = (400,300))\rplot!(x, pdf.(TDist(3), x),\rcolor = :orange,\rlabel = \u0026#34;t(3)\u0026#34;, size = (400,300))\rplot!(x, pdf.(TDist(30), x),\rcolor = :black, linestyle = :dash,\rlabel = \u0026#34;t(30)\u0026#34;, size = (400,300))\rplot!(x, pdf.(Normal(), x),\rcolor = :black,\rlabel = \u0026#34;Standard Normal\u0026#34;, size = (400,300))\rxlims!(-4,5); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pdf\\,of\\, t}(\\nu)\u0026#34;)\rpng(\u0026#34;pdf\u0026#34;) フーベルト・マニア. (2010). 熱中すること (冷たい数字の世界で絶対的な秩序を見つけ出した、ガウスの伝記): p69~72.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nホッグ他. (2013). Introduction to Mathematical Statistics(第7版): p171~172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1645,"permalink":"https://freshrimpsushi.github.io/jp/posts/1645/","tags":null,"title":"正規分布"},{"categories":"수리물리","contents":"定義 ベクトル関数 $\\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\\hat{\\mathbf{x}} + F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$に対して、以下のようなベクトルを$\\mathbf{F}$のカールcurlと定義し、$\\nabla \\times \\mathbf{F}$と表記する。\n$$ \\begin{align} \\nabla \\times \\mathbf{F} \u0026amp;= \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} \\label{def1} \\\\ \u0026amp;=\\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z}\\end{vmatrix} \\label{def2} \\end{align} $$\n$(2)$は$\\mathbf{F}$のカールを簡単に覚えるための公式である。行列式と考えてそのまま展開すればよい。 説明 カールは回転と翻訳される。しかし、回転という言葉は日常的すぎる上に、カールではなくrotationと誤解される可能性があるため、생새우초밥집では回転の代わりにカールを使用する。\n$\\nabla \\times \\mathbf{F}$は$\\mathbf{F}$という物理量がどの方向に回転しているかを教えてくれるベクトルである。$\\nabla \\times \\mathbf{F}$の方向を軸(親指)にして右手の法則を適用すると、右手が包む方向と$\\mathbf{F}$が回転する方向が一致する。ベクトル$\\nabla \\times \\mathbf{F}$の大きさは回転の程度を示す。\nアインシュタインの表記法とレヴィ-チヴィタ記号を使用すれば、以下のように表すことができる。$\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$と表記するなら、\n$$ \\nabla \\times \\mathbf{F} = \\epsilon_{ijk}\\hat{\\mathbf{e}}_{i}\\nabla_{j}F_{k} $$\n一方、定義で$(1)$という値を$\\nabla \\times \\mathbf{F}$と表記するとしたことに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体が何かの意味を持つと考えると、$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積や外積と誤解することになりかねない。したがって、$\\nabla$は便利な表記法程度にしか理解してはならず、勾配、ダイバージェンス、カールをまとめてデル演算子と呼ぶこともあるし、むしろデル演算子=勾配と考える方が良いかもしれない。詳細は以下で続く。\n注意点 $\\nabla \\times \\mathbf{F}$は$\\nabla$と$\\mathbf{F}$の外積ではない $\\nabla \\times \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の外積ではない。\r単に$\\nabla \\times \\mathbf{F}$は$\\mathbf{F}$に関する何らかの情報を含むベクトルである。$\\nabla$を$\\nabla = \\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} + \\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}} + \\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}}$のようなベクトルと考えて計算すると、結果が$(1)$と完全に一致するため、便宜上$\\nabla \\times \\mathbf{F}$と表記しているだけである。もし$\\nabla$を実際のベクトルと仮定すると、おかしな結果になる。\n二つのベクトル$\\mathbf{A}, \\mathbf{B}$に対して次の式が成り立つ。\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\nもし$\\nabla$が本当にベクトルだったら、上の公式に代入することができ、次の結果が得られるだろう。\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=(\\mathbf{F} \\cdot \\nabla)\\nabla - (\\nabla \\cdot \\nabla)\\mathbf{F} + \\nabla (\\nabla \\cdot \\mathbf{F}) - \\mathbf{F} (\\nabla \\cdot \\nabla) $$\nしかし、正しい結果は次のようになる。\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=\\nabla(\\nabla \\cdot \\mathbf{F})-\\nabla ^{2} \\mathbf{F} $$\n他にも例がある。ベクトルの外積は反交換性を持つため、$\\nabla \\times \\mathbf{F}$が外積であるならば、次の式が成り立つはずだ。\n$$ \\nabla \\times \\mathbf{F} \\overset{?}{=} - \\mathbf{F} \\times \\nabla $$\nしたがって、$\\nabla$はベクトルではなく、$\\nabla \\times \\mathbf{F}$を$\\nabla$と$\\mathbf{F}$の外積ではないことが分かる。ベクトルではなく、$\\nabla \\times$自体を一つの関数と考えるべきだ。このように関数を変数とする関数を物理学では演算子と呼ぶ。\nでは $\\nabla \\times \\mathbf{F}$と$\\mathbf{F} \\times \\nabla$の違いは？ $\\nabla \\times$はベクトル関数を変数とする、次のように定義される演算子である。\n$$ \\nabla \\times (\\mathbf{F}) = \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} $$\nつまり $\\nabla \\times \\mathbf{F}$は $\\nabla \\times$という演算子（関数）に$\\mathbf{F}$という変数を代入したときの関数値である。もちろんこれは再び$(x,y,z)$を変数とするベクトル関数である。$\\nabla \\times \\mathbf{F}$が$\\nabla \\times$の関数値であるのに対し、$\\mathbf{F} \\times \\nabla$はそれ自体が一つの演算子である。よく使われる数式ではないが、定義するなら次のような微分演算子であると言える。\n$$ \\begin{align*} \\mathbf{F} \\times \\nabla \u0026amp;= \\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\end{vmatrix} \\\\ \u0026amp;= \\left( F_{y}\\dfrac{ \\partial }{ \\partial z} - F_{z}\\dfrac{ \\partial }{ \\partial y} \\right)\\hat{\\mathbf{x}} + \\left( F_{z}\\dfrac{ \\partial }{ \\partial x} - F_{x}\\dfrac{ \\partial }{ \\partial z} \\right)\\hat{\\mathbf{y}} + \\left( F_{x}\\dfrac{ \\partial }{ \\partial y} - F_{y}\\dfrac{ \\partial }{ \\partial x} \\right)\\hat{\\mathbf{z}} \\end{align*} $$\n導出 ここで、ベクトル関数が回転する方向（時計回りか反時計回りか）を示す関数について考えてみましょう。重要なのは、回転面内のどの方向も回転の方向を特定できないということです。下の図を見てください。\nベクトル $-\\hat{\\mathbf{x}}$は点 $A$での動きは説明できますが、$B$での動きは説明できません。 ベクトル $\\hat{\\mathbf{y}}$は点 $C$での動きは説明できますが、$D$での動きは説明できません。 ベクトル $\\hat{\\mathbf{x}} + \\hat{\\mathbf{y}}$は経路 $F$を説明できますが、$G$を説明できません。 これは時計回りの場合にも同じです。回転方向を特定するためには回転面を離れる必要があることが理解できるでしょう。実際、これを決定するための良い方法が既にあります。それは、右手の法則を使うことです。右手が巻き込む方向の回転軸を親指の方向として決定します。したがって、$xy$平面で反時計回りに回る回転の軸（方向）は$\\hat{\\mathbf{z}}$であり、時計回りに回る回転の軸（方向）は$-\\hat{\\mathbf{z}}$です。\nそれでは、$\\mathbf{F}$が$xy$平面で反時計回りに回っている場合、$\\hat{\\mathbf{z}}$方向を示す値、つまり正の値を見つけてみましょう。回転は簡単に以下のように四角形で表現しましょう。\n経路①は点 $a$から点 $b$まで動き、$\\mathbf{F}(a) = (1,0,0)$, $\\mathbf{F}(b) = (0,1,0)$としましょう。すると、点 $a$から点 $b$まで$x$は$+1$だけ変化し、$F_{y}$も$+1$だけ変化するので、次のようになります。\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 $$\n同様に、点 $b$から点 $c$までの経路で、$y$は$+1$だけ変化し、$F_{x}$は$-1$だけ変化します。4つの経路すべてを確認すると、\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 \\quad \\text{in path $\\textcircled{1}$, $\\textcircled{3}$} $$\n$$ \\dfrac{\\partial F_{x}}{\\partial y} \\lt 0 \\quad \\text{in path $\\textcircled{2}$, $\\textcircled{4}$} $$\nしたがって、上記のように反時計回りに回転するベクトル $\\mathbf{F}$に対して、以下の値は常に正です。\n$$ \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\gt 0 $$\n逆に、$\\mathbf{F}$が時計回りに回転している場合、上記の値は常に負です。それでは、ベクトル関数 $\\mathbf{F}$を代入すると、$xy$平面で回転する方向と大きさを示す演算子 $\\operatorname{curl}_{xy}$を次のように定義できます。\n$$ \\operatorname{curl}_{xy} (\\mathbf{F}) = \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right) \\hat{\\mathbf{z}} $$\nこの関数の $\\hat{\\mathbf{z}}$ 成分の符号は、$\\mathbf{F}$が$xy$平面で回転する方向を示す。 $+$の場合、$\\mathbf{F}$は$xy$平面で反時計回りに回転する。 $-$の場合、$\\mathbf{F}$は$xy$平面で時計回りに回転する。 $0$の場合、回転しない。 この関数の $\\hat{\\mathbf{z}}$ 成分の大きさは、$\\mathbf{F}$が$xy$平面でどれだけ速く回転しているかを示す。 このような議論を$yz$平面と$zx$平面にも適用することで、$\\mathbf{F}$が3次元空間で回転している方向と大きさを示すベクトル$\\nabla \\times \\mathbf{F}$を次のように定義することができます。\n$$ \\nabla \\times \\mathbf{F} := \\left( \\dfrac{\\partial F_{z}}{\\partial y} - \\dfrac{\\partial F_{y}}{\\partial z} \\right)\\hat{\\mathbf{x}} + \\left( \\dfrac{\\partial F_{x}}{\\partial z} - \\dfrac{\\partial F_{z}}{\\partial x} \\right)\\hat{\\mathbf{y}} + \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right)\\hat{\\mathbf{z}} $$\n■\n関連する公式 リニアリティ: $$ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) = \\nabla \\times \\mathbf{A} + \\nabla \\times \\mathbf{B} $$\n乗算規則:\n$$ \\nabla \\times (f\\mathbf{A}) = f(\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\times (\\nabla f) $$\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\n二次関数:\n$$ \\nabla \\times (\\nabla f) = \\mathbf{0} $$\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F}) = \\nabla (\\nabla \\cdot \\mathbf{F}) - \\nabla^{2} \\mathbf{F} $$\nストークスまとめ $$ \\int_{\\mathcal{S}} (\\nabla \\times \\mathbf{v} )\\cdot d\\mathbf{a} = \\oint_{\\mathcal{P}} \\mathbf{v} \\cdot d\\mathbf{l} $$\n積分式 $$ \\int_{\\mathcal{V}} (\\nabla \\times \\mathbf{v}) d \\tau = - \\oint_{\\mathcal{S}} \\mathbf{v} \\times d \\mathbf{a} $$\n$$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\n部分積分 $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n$$ \\int_{\\mathcal{V}} \\mathbf{B} \\cdot \\left( \\nabla \\times \\mathbf{A} \\right) d\\tau = \\int_{\\mathcal{V}} \\mathbf{A} \\cdot \\left( \\nabla \\times \\mathbf{B} \\right) d\\tau + \\oint_{\\mathcal{S}} \\left( \\mathbf{A} \\times \\mathbf{B} \\right) \\cdot d \\mathbf{a} $$\n証明 線形性 アインシュタイン表記法, レヴィ・チビタ記号を使います。$\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$ とすると、\n$$ \\begin{align*} \\left[ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) \\right]_{i} \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (\\mathbf{A} + \\mathbf{B})_{k} \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (A_{k} + B_{k}) \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j}A_{k} + \\epsilon_{ijk} \\nabla_{j}B_{k} \\\\ \u0026amp;= [\\nabla \\times \\mathbf{A}]_{i} + [\\nabla \\times \\mathbf{B}]_{i} \\\\ \\end{align*} $$\n第三の等号は、$\\dfrac{\\partial (A_{k} + B_{k})}{\\partial x_{j}} = \\dfrac{\\partial A_{k}}{\\partial x_{j}} + \\dfrac{\\partial B_{k}}{\\partial x_{j}}$であるため成立します。\n■\n参照 デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ ","id":1752,"permalink":"https://freshrimpsushi.github.io/jp/posts/1752/","tags":null,"title":"3次元デカルト座標系におけるベクトル関数のカール(回転)"},{"categories":"거리공간","contents":"定義 二つの距離空間$\\left( X , d_{X} \\right)$、$\\left( Y , d_{Y} \\right)$と部分集合$E\\subset X$に対して、関数$f : E \\to Y$を定義しよう。\n$p \\in E$としよう。ある$\\varepsilon \u0026gt; 0$に対して、\n$$ x \\in E \\quad \\text{and} \\quad d_{X}(p, x ) \u0026lt; \\delta \\implies d_{Y}(f(p) , f(x) ) \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在するならば、$f$は$p \\in E$で連続であるという。$f$が$E$の全ての点で連続ならば、$f$を$E$上での連続関数continuous functionという。\nある$ \\varepsilon \u0026gt; 0$に対して、\n$$ d_{X}(x_{1}, x_{2} ) \u0026lt; \\delta \\land x_{1}, x_{2} \\in E \\implies d_{Y}(f(x_{1}) , f(x_{2}) ) \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在するならば、$f$が$E$上で一様連続uniformly continuousであるという。\n$\\land$は論理的に「そして」を表す論理積の記号だ。 説明 連続と一様連続は、$\\mathbb{R}$を超えて距離空間に対しても定義できる。$\\mathbb{R}$の連続と異なる点は、$d_{1}$と$d_{2}$を変えての一般化が可能であることだ。\n一方で、もっと難しい表現を使って、ある$B_{d_{Y}} (f(p) , \\varepsilon )$に対して$f(B_{d_{X}} (p , \\delta)) \\subset B_{d_{Y}} (f(p) , \\varepsilon )$を満たす$B_{d_{X}} (p , \\delta)$が存在する時、$f$が$p \\in X$で連続であるとも言える。初めは抽象的すぎて避けがちだが、見ているうちにこの表現の方が便利になるかもしれない。位相空間への一般化を考えれば、早めに慣れておいた方が良いかもしれない。\n定理: 連続関数である同値条件 関数$f:X \\to Y$に対して、以下の条件は互いに同値である。\n$f : X \\to Y$は連続である。\n$\\forall x \\in X,\\ \\displaystyle \\lim_{n \\to \\infty} p_{n} = p \\implies \\lim_{n \\to \\infty} f(p_{n}) = f(p)$\n$Y$の全ての開集合$O$に対して、$f^{-1} ( O )$は$X$で開集合である。\n$Y$の全ての閉集合$C$に対して、$f^{-1} ( C )$は$X$で閉集合である。\nこれらの性質は与えられた関数が連続であることを証明するのに役立つことがある。\n上の図を見ると、一見、四番目の条件の反例に見える。閉区間$[c,d]$に対してその逆像$f^{-1} [c,d]$が$(a,b)$であり、知っての通り、$(a,b)$は開区間である。しかし、$f : (a,b) \\to \\mathbb{R}$なので、$(a,b)$は全空間になり、全空間は閉集合であるため、命題に反していない。\n","id":384,"permalink":"https://freshrimpsushi.github.io/jp/posts/384/","tags":null,"title":"距離空間における連続性と一様連続性"},{"categories":"거리공간","contents":"定義1 $\\left\\{ p_{n} \\right\\}$が距離空間 $(X,d)$の点の列であるとしよう。以下の条件を満たす点 $p \\in X$が存在するなら、列 $\\left\\{ p_{n} \\right\\}$は$p$に収束するconvergeと言い、$p_{n} \\rightarrow p$または$\\lim \\limits_{n\\to \\infty}p_{n}=p$と表される。\n$$ \\forall \\varepsilon \u0026gt;0,\\ \\exists N\\in \\mathbb{N}\\ \\mathrm{s.t}\\ n\\ge N \\implies d(p_{n},p)\u0026lt;\\varepsilon $$\n$\\left\\{ p_{n} \\right\\}$が収束しないなら発散するdivergeと言う。また、すべての$p_{n}$の集合を$\\left\\{ p_{n} \\right\\}$の値域rangeと言う。$\\left\\{ p_{n} \\right\\}$の値域が有界なら、列$\\left\\{ p_{n} \\right\\}$は有界boundedだと言われる。\n定理 $\\left\\{ p_{n} \\right\\}$を距離空間$(X,d)$の列とする。\n(a) $p_{n}\\to p$の必要十分条件は、すべての$p$の近傍が無限個を除くすべての$\\left\\{ p_{n} \\right\\}$の項を含むことである。\n(b) $p_{n} \\to p$であり、かつ$p_{n} \\to p^{\\prime}$ならば、$p=p^{\\prime}$である。\n(c) $\\left\\{ p_{n} \\right\\}$が収束すれば、有界である。\n(d) $E\\subset X$が与えられたとしよう。$p$が$E$の集積点であれば、$p=\\lim \\limits_{n \\to \\infty}p_{n}$を満たす$E$の列$\\left\\{ p_{n} \\right\\}$が存在する。また、$\\left\\{ p_{n} \\right\\}$が異なる点の集合であれば、逆も成り立つ。\n証明 (a) $(\\implies)$\n$p_{n} \\to p$と仮定しよう。任意の正の数$\\varepsilon \u0026gt;0$が与えられたとする。$V$を$p$の半径が$\\varepsilon$の近傍とする。近傍の定義により、次が成り立つ。\n$$ d(p,q)\u0026lt;\\varepsilon\\quad \\implies q\\in V $$\nしかし、仮定により、与えられた$\\varepsilon$に対して、以下の条件を満たす$N$が存在する。\n$$ \\forall n \\ge N,\\ d(p_{n},p) \u0026lt;\\varepsilon $$\n従って、有限個の点を除くすべての$p_{n}$が$V$に含まれる。\n$(\\impliedby)$\n$p$のすべての近傍が無限個を除くすべての$\\left\\{ p_{n} \\right\\}$を含むと仮定しよう。任意の正の数$\\varepsilon\u0026gt;0$が与えられたとする。$V$を$p$の半径が$\\varepsilon$の近傍とする。すると、仮定により、以下の条件を満たす$N$が存在する。\n$$ n \\ge N \\implies p_{n}\\in V $$\nなので、$V$は$p$の近傍なので、次が成り立つ。\n$$ \\forall n \\ge N,\\quad d(p_{n},p)\u0026lt;\\varepsilon $$\n従って、$p_{n}\\to p$。\n■\n(b) 任意の正の数$\\varepsilon \u0026gt;0$が与えられたとしよう。仮定により、以下の条件を満たす2つの正の数$N$、$N^{\\prime}$が存在する。\n$$ \\begin{align*} n\\ge N \u0026amp; \\implies d(p_{n},p) \u0026lt;\\frac{\\varepsilon}{2} \\\\ n\\ge N^{\\prime} \u0026amp; \\implies d(p_{n},p) \u0026lt;\\frac{\\varepsilon}{2} \\end{align*} $$\nすると、$n \\ge \\max(N,N^{\\prime})$に対して、以下の式が成り立つ。\n$$ d(p,p^{\\prime}) \\le d(p,p_{n}) + d(p_{n},p^{\\prime})\u0026lt;\\varepsilon $$\n$\\varepsilon$は任意の正の数なので、\n$$ d(p,p^{\\prime})=0 $$\nであり、距離の定義により、$p=p^{\\prime}$\n■\n(c) $\\left\\{ p_{n} \\right\\}$が$p$に収束すると仮定しよう。仮定により、以下の式が成り立つ正の数$N$が存在する。\n$$ n \\ge N \\implies d(p_{n},p)\u0026lt;1 $$\n今、\n$$ r=\\max \\left\\{ 1,\\ d(p_{1},p),\\ \\cdots,\\ d(p_{N},p) \\right\\} $$\nとしよう。すると、すべての$n$に対して、\n$$ d(p_{n},p)\\le r $$\nなので、$\\left\\{ p_{n} \\right\\}$は有界である。\n■\n(d) $(\\implies)$\n$E\\subset X$であり、かつ、$p$が$E$の集積点であるとしよう。集積点の定義により、各$n$に対して、\n$$ d(p_{n},p) \u0026lt; \\frac{1}{n} $$\nを満たす$p_{n}\\in E$が存在する。今、任意の正の数$\\varepsilon \u0026gt;0$と$N\\varepsilon\u0026gt;1$を満たす$N$が与えられたとする。すると、$ n \u0026gt;N$に対して、次が成り立つ。\n$$ d(p_{n},p)\u0026lt; \\frac{1}{n}\u0026lt;\\frac{N}{n}\\varepsilon\u0026lt;\\varepsilon $$\n従って、$\\left\\{ p_{n} \\right\\}$は$p$に収束する。\n$(\\impliedby)$\n$p=\\lim \\limits_{n\\to\\infty}p_{n}$を満たす$E$の異なる点の列$\\left\\{ p_{n} \\right\\}$が存在すると仮定しよう。すると、すべての正の数$\\varepsilon \u0026gt;0$に対して、\n$$ n \\ge N \\implies d(p_{n},p)\u0026lt; \\varepsilon $$\nを満たす$N$が存在する。この時、$V_{\\varepsilon}$を$p$の半径が$\\varepsilon$の近傍とする。すると、$V_{\\varepsilon}$は$p$ではない$p_{n} \\in E (n\\ge N)$を含むので、$p$は$E$の集積点である。\n■\nWalter Rudin, Principles of Mathmatical Analysis (第3版, 1976), p47-48, 55-58\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1713,"permalink":"https://freshrimpsushi.github.io/jp/posts/1713/","tags":null,"title":"距離空間における数列の収束"},{"categories":"거리공간","contents":"定義 $\\left\\{ p_{n} \\right\\}$を距離空間$(X,d)$の点の数列とする。全ての正の数$\\varepsilon$に対して\n$$ n\\ge N,\\ m\\ge N \\implies d(p_{n},p_{m})\u0026lt;\\varepsilon $$\nが成り立つ正の数$N$が存在するならば、$\\left\\{ p_{n} \\right\\}$をコーシー数列Cauchy sequenceという。\n距離空間$X$の全てのコーシー数列が$X$の点に収束するならば、$X$を完備空間という。\n説明 以下の定理により、全てのコンパクト距離空間とユークリッド空間は完備であることがわかる。\n定理 (a) 距離空間で、全ての収束する数列はコーシー数列である。\n(b) $X$がコンパクト距離空間で$\\left\\{ p_{n} \\right\\}$が$X$のコーシー数列であるとする。その場合、$\\left\\{ p_{n} \\right\\}$はある$p\\in X$に収束する。\n(c) $\\mathbb{R}^{k}$で、全てのコーシー数列は収束する。\n(a), (b) を一緒に言えば、「コンパクト距離空間で収束する数列とコーシー数列は同値である」となる。\n証明 (a) $p_{n} \\to p$かつ$\\varepsilon \u0026gt;0$が与えられたとする。それならば$\\forall n \\ge N,\\ d(p,p_{n})\u0026lt;\\varepsilon$を満たす$N$が存在する。したがって、次が成り立つ：\n$$ d(p_{n},p_{m}) \\le d(p_{n},p)+d(p,p_{m})\u0026lt;2\\varepsilon,\\quad \\forall m,n\\ge N $$\n従って、定義により$\\left\\{ p_{n} \\right\\}$はコーシー数列である。\n■\n(b) $\\left\\{ p_{n} \\right\\}$をコンパクト距離空間$X$のコーシー数列とする。そして、任意の自然数$N$に対して次のようであるとする：\n$$ E_{N}=\\left\\{ p_{N},p_{N+1},p_{N+2},\\cdots\\right\\} $$\nすると、次が成り立つ：\n$$ \\begin{equation} \\lim \\limits_{N\\to\\infty}\\mathrm{diam\\ }\\overline{E_{N}}=0 \\label{eq1} \\end{equation}$$\nまた、$\\overline{E_{N}}$はコンパクト空間$X$の閉じた部分集合なので、$\\overline{E_{N}}$はコンパクトである。さらに、次の式が成り立つことは自明である：\n$$ E_{N}\\supset E_{N+1} \\quad \\text{and} \\quad \\overline{E_{N}}\\supset \\overline{E_{N+1}} $$\n従って、上記の条件から、$\\forall N \\in \\mathbb{N},\\ p \\in \\overline{E_{N}}$を満たす唯一の$p \\in X$が存在する1ことがわかる。今、$\\varepsilon \u0026gt;0$が与えられたとする。それならば$\\eqref{eq1}$により、\n$$ N \\ge N_{0}\\implies \\mathrm{diam\\ }\\overline{E_{N}}\u0026lt; \\varepsilon $$\nが成り立つ$N_{0}$が存在する。しかし、$\\mathrm{diam\\ }\\overline{E}=\\mathrm{diam\\ }E$かつ$p \\in \\overline{E_{N}}$であるので、全ての$q \\in E_{N}$に対して$d(p,q)\u0026lt;\\varepsilon$が成り立つ。言い換えると、次のようになる：\n$$ n \\ge N_{0} \\implies d(p_{n},p)\u0026lt; \\varepsilon $$\nこれは$p_{n}\\to p$の定義なので、$\\lim \\limits_{n\\to\\infty} p_{n}=p$\n■\n(c) $\\left\\{ \\mathbf{x}_{n} \\right\\}$を$\\mathbb{R}^{k}$でのコーシー数列とする。$E_{N}$を証明 (b) と同じものとする。それならば、\n$$ \\mathrm{diam\\ } E_{N} \u0026lt;1 $$\nを満たす$N$を選んだとする。そして、\n$$ r=\\max \\left\\{ d(\\mathbf{x}_{N},\\mathbf{x}_{1}),\\ d(\\mathbf{x}_{N},\\mathbf{x}_{2}),\\ \\cdots,\\ d(\\mathbf{x}_{N},\\mathbf{x}_{N-1}),\\ 1 \\right\\} $$\nだとするなら、$\\forall m,n \\in \\mathbb{N},\\ d(\\mathbf{x}_{n},\\mathbf{x}_{m}) \u0026lt;r$のため、$\\left\\{ \\mathbf{x}_{n} \\right\\}$は有界である。したがって、$\\overline{ \\left\\{ \\mathbf{x}_{n} \\right\\}}$は閉じていて有界な$\\mathbb{R}^{k}$の部分集合なので、コンパクトである。従って、$\\left\\{ \\mathbf{x}_{n} \\right\\}$はコンパクト空間のコーシー数列であり、したがって**（b）**により、$\\left\\{ \\mathbf{x}_{n} \\right\\}$は収束する。\n■\n定理(b)参照\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1718,"permalink":"https://freshrimpsushi.github.io/jp/posts/1718/","tags":null,"title":"距離空間内のコーシー数列と完備性"},{"categories":"함수","contents":"定義 次のように定義される関数 $\\zeta : \\mathbb{C} \\setminus \\left\\{ 1 \\right\\} \\to \\mathbb{C}$ をリーマン ゼータ関数Riemann zeta Function\u0026lt;/supという。 $$ \\zeta (s) := \\sum_{n \\in \\mathbb{N}} n^{-s} = \\prod_{p : \\text{prime}} \\left( 1- {p^{-s}} \\right)^{-1} $$\n関連定理 [0] ラマヌジャンの和: $\\displaystyle \\sum_{n \\in \\mathbb{N}} x^{n-1} = {{ 1 } \\over { 1-x }}$ が $|x| = 1$ でも成り立つと受け入れるなら $$ \\zeta (0) = 1 + 1 + 1 + 1 + \\cdots = - {{ 1 } \\over { 2 }} $$\n[1] オーレムの証明: $\\zeta (1)$ が定義されない理由は次の通りです。 $$ \\zeta (1) = \\sum_{n \\in \\mathbb{N}} {{ 1 } \\over { n }} = \\infty $$\n[2] オイラーの証明: $$ \\zeta (2) = \\sum_{n \\in \\mathbb{N}} {{ 1 } \\over { n^{2} }} = {{ \\pi^{2} } \\over { 6 }} $$\n[a] ガンマ関数との関係: $\\text{Re} (s) \u0026gt; 1$ ならば $$ \\zeta (s) \\Gamma (s) = \\mathcal{M} \\left[ {{ 1 } \\over { e^{x} - 1 }} \\right] (s) = \\int_{0}^{\\infty} {{ x^{s-1} } \\over { e^{x} - 1 }} dx $$\n[b] ディリクレのエータ関数との関係: $$ \\eta (s) := \\sum_{n \\in \\mathbb{N}} (-1)^{n-1} n^{-s} $$\n説明 ゼータ関数は、実部が$1$より大きい複素数、すなわち$\\text{Re} (s) \u0026gt; 1$である$s$内で収束し、ガンマ関数との関係を持っている。特に整数論と複素解析での関心の対象であり、その悪名高いリーマン予想の主役でもある。\n","id":1626,"permalink":"https://freshrimpsushi.github.io/jp/posts/1626/","tags":null,"title":"リーマンゼータ関数"},{"categories":"거리공간","contents":"定義 $a_i, b_i \\in \\mathbb{R} (1 \\le i \\le k)$に対して、集合$I=[a_{1}, b_{1}] \\times [a_{2}, b_{2}] \\times \\cdots \\times [a_{k}, b_{k}]$を**$k$-セル**と言う。ここで$\\times$は集合のデカルト積である。\n定理1 $\\mathbb{R}$上の閉区間の数列$\\left\\{ I_{n} \\right\\}$が$I_{n} \\supset I_{n+1}\\ (n=1,2,\\cdots)$を満たすとする。すると以下が成立する。\n$$ \\bigcap_{i=1}^{\\infty}I_{n}\\ne \\varnothing $$\n証明 $I_{n}=[a_{n}, b_{n}]$とする。そして$E=\\left\\{ a_{n} : n=1,2,\\cdots \\right\\}$とする。すると$E\\ne \\varnothing$であり、$b_{1}$1によって上限がある。今$x=\\sup E$とする。そして任意の二つの正数$m$、$n$に対して\n$$ a_{n} \\le a_{m+n} \\le b_{m+n} \\le b_{m} $$\nが成立するので、すべての$n$に対して$x\\le b_{n}$である。また$x$が$E$の上限であるため、すべての$n$に対して$a_{n} \\le x$であることは明らかである。したがって、すべての$n$に対して$a_{n}\\le x \\le b_{n}$なので、$x\\in I_{n}\\ \\forall n$である。したがって\n$$ x\\in \\bigcap _{i=1}^{n}I_{n} $$\n■\n定理2 $\\left\\{ I_{n} \\right\\}$が$I_{n}\\supset I_{n+1}(n=1,2,\\cdots)$を満たす$k-$セルの数列であるとする。すると$\\bigcap_{i=1}^{n}I_{n}\\ne\\varnothing$である。\n定理2は定理1を$\\mathbb{R}^{k}$に拡張したものである。\n証明 $I_{n}$を以下のようにする。\n$$ I_{n}=\\left\\{ \\mathbf{x}=(x_{1},\\cdots,x_{k}) : a_{n,j} \\le x_{j} \\le b_{nj},\\quad(1\\le j \\le k;\\ n=1,2,\\cdots) \\right\\} $$\nすなわち$I_{n}=I_{n,1}\\times \\cdots\\times I_{n,k}\\ (I_{n,j}=[a_{n,j},b_{n,j}])$である。すると定理1によって、それぞれの$I_{n,j}$に対して$x_{j}^{\\ast}\\in I_{n,j} \\ (a_{n,j} \\le x_{j}^{\\ast} \\le b_{n,j})$が存在する。したがって\n$$ \\mathbf{x^{\\ast}} =(x_{1}^{\\ast},\\cdots ,x_{k}^{\\ast})\\in I_{n} ,\\quad (n=1,2,\\cdots) $$\n■\n定理3 すべての$k-$セルはコンパクトである。\n証明 $I$を以下のような任意の$k$-セルとする。\n$$ I=I^{1}\\times \\cdots \\times I^{k}=[a_{1},b_{1}]\\times \\cdots \\times [a_{k},b_{k}] $$\nそして以下のようにする。\n$$ \\mathbf{x}=(x_{1},\\cdots,x_{k}) \\quad \\text{and} \\quad a_{j} \\le x_{j} \\le b_{j}(1\\le j \\le k) $$\n今$\\delta$を以下のようにする。\n$$ \\delta =\\left( \\sum \\limits_{j=1}^{k}(b_{j})-a_{j})^{2} \\right)^{{\\textstyle \\frac{1}{2}}}=|\\mathbf{b}-\\mathbf{a}| $$\nこのとき$\\mathbf{a}=(a_{1},\\cdots,a_{n})$、$\\mathbf{b}=(b_{1},\\cdots,b_{n})$である。すると$\\delta$は$\\mathbf{b}$と$\\mathbf{a}$の間の距離と同じである。したがって\n$$ |\\mathbf{x}-\\mathbf{y}| \\le \\delta \\quad \\forall \\mathbf{x},\\mathbf{y}\\in I $$\nが成立する。今から証明が本格的に始まるが、背理法を使用する。つまり$k-$セルがコンパクトでないと仮定する。するとコンパクトの定義によって、$I$のいくつかのオープンカバー$\\left\\{ O_{\\alpha} \\right\\}$が有限部分カバーを持たないと仮定することと同じである。$c_{j}=(a_{j}+b_{j})/2$とする。すると$c_{j}$を使って各$I^{j}$を$[a_{j},c_{j}]$、$[c_{j},b_{j}]$に分けて$2^{k}$個の$1-$セルを作ることができる。これらの和集合は当然$I$になり、仮定によりこれらの中で少なくとも一つは$\\left\\{ O_{\\alpha} \\right\\}$のいくつかの有限部分カバーでカバーされなければならない。そのセルを$I_{1}$とする。すると$I$から$I_{1}$を選んだのと同じ方法で続けて区間を選ぶと、以下の三つの規則を満たす数列$\\left\\{ I_{n} \\right\\}$を得ることができる。\n$(\\mathrm{i})$ $I\\supset I_{1} \\supset I_{2}\\supset \\cdots$\n$(\\mathrm{ii})$ それぞれの$I_{n}$は$\\left\\{ O_{\\alpha} \\right\\}$のいくつかの有限部分カバーでもカバーされない。\n$(\\mathrm{iii})$ $|\\mathbf{x}-\\mathbf{y}|\\le 2^{-n}\\delta,\\quad \\forall \\mathbf{x},\\mathbf{y}\\in I_{n}$\nすると$(\\mathrm{i})$と定理2によって、すべての$n$に対して$\\mathbf{x}^{\\ast}\\in I_{n}$である$\\mathbf{x}^{\\ast}$が存在する。すると$\\left\\{ O_{\\alpha} \\right\\}$が$I$のオープンカバーであるため、いくつかの$\\alpha$に対して$\\mathbf{x}^{\\ast\n}\\in O_{\\alpha}$が成立する。$O_{\\alpha}$が開集合であるため、$|\\mathbf{x}^{\\ast}-\\mathbf{y}|\u0026lt;r \\implies \\mathbf{y}\\in O_{\\alpha}$を満たす$r\u0026gt;0$が存在する。一方で、$n$を十分大きくして$2^{-n}\\delta\u0026lt;r$を満たすようにすることができる。すると$(\\mathrm{iii})$によって$I_{n}\\subset O_{\\alpha}$である。しかし、これは$(\\mathrm{ii})$と矛盾するので、仮定が間違っていることがわかる。したがって、すべての$k-$セルはコンパクトである。\n■\n上記の事実から以下の有用な定理を証明することができる。\nユークリッド空間でコンパクトである同値条件 実数（または複素数）空間の部分集合$E\\subset \\mathbb{R}^{k}(\\mathrm{or}\\ \\mathbb{C}^{k})$に対して、以下の三つの命題は同値である。\n(a) $E$は閉じており有界である。\n(b) $E$はコンパクトである。\n(c) $E$のすべての無限部分集合は集積点 $p \\in E$を持つ。\nここで**(a)、(b)が同値であることはハイネ・ボレルの定理と呼ばれる。(c)を満たす$E$に対して\u0026rsquo;$E$は\u0026rsquo;集積点コンパクトである\u0026rsquo;または\u0026rsquo;$E$は\u0026rsquo;ボルツァーノ-ワイエルシュトラスの性質を持つ\u0026rsquo;と言う。(b)と(c)**が同値であることは距離空間では成立するが、位相空間では一般的には成立しない。\n証明 (a) $\\implies$ (b)\n**(a)**を仮定すると、$E \\subset I$を満たす$k-$セル$I$が存在する。すると$I$がコンパクトであり、コンパクト集合の閉じた部分集合はコンパクトであるため、$E$はコンパクトである。\n(b) $\\implies$ (c)\n背理法で証明する。\n$S$がコンパクト集合$E$の無限部分集合であるとする。そして$S$の集積点が存在しないと仮定する。するとすべての$p\\in E$は、せいぜい$S$の点をただ一つだけ含む$p$の近傍$N_{p}$を持つ。$p \\in S$の場合、そのただ一つの点は$p$である。そしてこれは、オープンカバー$\\left\\{ N_{p} \\right\\}$が$S$をカバーする有限部分カバーを持たないことを意味する。$S \\subset E$なので、同様に$E$をカバーする有限部分カバーも存在しない。これは$E$がコンパクトであるという仮定に矛盾するので、$S$は集積点$p \\in E$を持つ。\n(c) $\\implies$ (a)\n背理法で証明する。\npart 1. $E$は有界である\n$E$は有界ではないと仮定してみる。すると$E$は以下の不等式を満たす点$\\mathbf{x}_{n}$を含む。\n$$ |\\mathbf{x}_{n}| \u0026gt;n\\quad (n=1,2,\\cdots) $$\n今$S=\\left\\{ \\mathbf{x}_{n} : n=1,2,\\cdots\\right\\}$とする。すると$S$は無限集合であり、$\\mathbb{R}^{k}$で集積点を持たないことは明らかである。これは$(c)$に対する矛盾である。したがって$E$は有界である。\npart 2. $E$は閉じている。\n$E$は閉じていないと仮定してみる。すると定義により$E$に含まれない$E$の集積点$\\mathbf{x}_{0}$が存在する。今$n=1,2,\\cdots$に対して$\\mathbf{x}_{n} \\in E$を以下の条件を満たす点とする。\n$$ \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \u0026lt; {\\textstyle \\frac{1}{n}} $$\nそしてこのような$\\mathbf{x}_{n}$の集合を$S$とする。すると$S$は無限集合であり、$\\mathbf{x}_{0}$を集積点として持つ。今$\\mathbf{x}_{0}$が$S$の唯一の集積点であれば、$\\mathbf{x}_{0}\\notin E$であるため$(c)$に矛盾し、$E$は閉じていることがわかる。それでは$\\mathbf{y} \\ne \\mathbf{x}_{0}$である$\\mathbf{y} \\in \\mathbb{R}^{k}$を考える。すると\n$$ \\begin{align*} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \u0026amp; \\ge \\left|\\mathbf{x}_{0} - \\mathbf{y} \\right| - \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \\\\ \u0026amp; \\ge \\left| \\mathbf{x}_{0} - \\mathbf{y} \\right| -\\frac{1}{n} \\end{align*} $$\nこのとき十分に大きな$n$に対して以下の式が成立する。\n$$ \\begin{equation} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \\ge \\left| \\mathbf{x}_{0}- \\mathbf{y} \\right|-\\frac{1}{n} \\ge \\frac{1}{2}\\left|\\mathbf{x}_{0}-\\mathbf{y} \\right| \\label{eq1} \\end{equation} $$\nまた$\\mathbf{x}_{n\n}$の条件により、$n$が大きくなるにつれて$\\mathbf{x}_{n}$は$\\mathbf{x}_{0}$に近づく。この事実と$\\eqref{eq1}$により、$n$を続けて大きくすると$\\mathbf{y}$を含まない$\\mathbf{y}$の近傍を見つけることができる。したがって$\\mathbf{y}$は$S$の集積点ではなく、$\\mathbf{x}_{0}$が$S$の唯一の集積点であることから$(c)$に矛盾し、$E$は閉じている。\n■\nボルツァーノ-ワイエルシュトラスの定理 $\\mathbb{R}^{k}$のすべての有界な無限部分集合は集積点$p \\in \\mathbb{R}^{k}$を持つ。\n証明 $E$を$\\mathbb{R}^{k}$の有界な無限部分集合とする。すると$E$が有界であるため、$E \\subset I$を満たす$k-$セル$I$が存在する。$k-$セルはコンパクトであるため、$I$はコンパクトである。すると$I$がコンパクトである同値条件$(b)\\implies (c)$によって、$E$は集積点$p \\in I \\subset \\mathbb{R}^{k}$を持つ。\n■\n任意の$b_{n}$で問題ない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1711,"permalink":"https://freshrimpsushi.github.io/jp/posts/1711/","tags":null,"title":"すべてのk-cellはコンパクトである：ユークリッド空間でコンパクトである同値条件。"},{"categories":"르벡공간","contents":"定義1 $a \\in \\mathbb{R}$に対して、以下のように定義される$T_{a} : L^{2} \\to L^{2}$をトランスレーションtranslation, 平行移動と言う。 $$ \\left( T_{a} f \\right) (x) := f(x-a) $$\n$b \\in \\mathbb{R}$に対して、以下のように定義される$E_{b} : L^{2} \\to L^{2}$をモジュレーションmodulation, 変調と言う。 $$ \\left( E_{b} f \\right) (x) := e^{2 \\pi i b x} f(x) $$\n$c \u0026gt; 0$に対して、以下のように定義される$D_{c} : L^{2} \\to L^{2}$をダイレーションdilation, 膨張と言う。 $$ \\left( D_{c} f \\right) (x) := {{ 1 } \\over { \\sqrt{c} }} f \\left( {{ x } \\over { c }} \\right) $$\n説明 上記の線型演算子は$L^{2}$空間でよく使われるものだ。韓国語ではそれぞれ平行移動(translation)、変調(modulation)、膨張(dilation)と翻訳されるけれど、数式的に理解するには英語で直接読む方が楽だろう。\nモジュレーションで掛けられる$e^{2 \\pi i b x}$は文字通り抽象化された回転だ。\nダイレーションで掛けられる$\\displaystyle {{ 1 } \\over { \\sqrt{c} }}$は、ノルム$\\left\\| \\cdot \\right\\|_{2}$に合わせるためにルートが掛けられているとも見れる。特に$c = 1/2$に対して、以下のように定義される$D$は特別な役割をすることもある。\n$$ ( D f ) (x) := \\sqrt{2} f (2x) $$\n便宜上、$D$は$j \\in \\mathbb{Z}$に関して、以下のように書かれる。\n$$ ( D^{j} f ) (x) := \\sqrt{2}^{j} f \\left( 2^{j} x \\right) $$\n性質 全ての$a, b \\in \\mathbb{R}$、$c \u0026gt; 0$及び$f,g \\in L^{1}$に対して、\n$T_{a} , E_{b}, D_{c}$は有界線型演算子だ。\n逆演算子：$T_{a} , E_{b}, D_{c}$はユニタリだ。\n交換関係:\n$$ (T_{a} E_{b} f ) (x) = e^{- 2 \\pi i b a} (E_{b} T_{a} f ) (x) \\\\ (T_{a} D_{c} f ) (x) = (D_{c} T_{a/c} f ) (x) \\\\ (D_{c} E_{b} f ) (x) = (E_{b/c} D_{c} f ) (x) $$\nフーリエ変換との関係:\n$$ \\mathcal{F} T_{a} = E_{-a} \\mathcal{F} \\\\ \\mathcal{F} E_{b} = T_{b} \\mathcal{F} \\\\ \\mathcal{F} D_{c} = D_{1/c} \\mathcal{F} $$\n$D$に関しては、上の定理の系として$j, k \\in \\mathbb{Z}$について、以下を得ることができる。\n$$ T_{k} D^{j} = D^{j} T_{2^{j} k } \\\\ D^{j} T_{k} = T_{2^{-j}k} D^{j} \\\\ \\left( D^{j} \\right)^{ \\ast } = D^{-j} $$\n証明 1. Part 1. 線型\n全ての$f,g \\in L^{2}$及び$\\alpha , \\beta \\in \\mathbb{C}$に対して、\n$$ \\begin{align*} T_{a} \\left( \\alpha f + \\beta g \\right)(x) =\u0026amp; \\left( \\alpha f + \\beta g \\right)(x-a) \\\\ =\u0026amp; \\alpha f (x-a) + \\beta g (x-a) \\\\ =\u0026amp; \\alpha T_{a} f (x) + \\beta T_{a} g (x) \\end{align*} $$\nだから$T_{a}$はリニアだ。\n$$ \\begin{align*} E_{b} \\left( \\alpha f + \\beta g \\right)(x) =\u0026amp; e^{ 2 \\pi i b x } \\left( \\alpha f + \\beta g \\right)(x) \\\\ =\u0026amp; \\alpha e^{ 2 \\pi i b x } f (x) + \\beta e^{ 2 \\pi i b x } g (x) \\\\ =\u0026amp; \\alpha E_{b} f (x) + \\beta E_{b} g (x) \\end{align*} $$\nだから$E_{b}$はリニアだ。\n$$ \\begin{align*} D_{c} \\left( \\alpha f + \\beta g \\right)(x) =\u0026amp; {{ 1 } \\over { \\sqrt{c} }} \\left( \\alpha f + \\beta g \\right) \\left( {{ x } \\over { c }} \\right) \\\\ =\u0026amp; \\alpha {{ 1 } \\over { \\sqrt{c} }} f (x) + \\beta {{ 1 } \\over { \\sqrt{c} }} g (x) \\\\ =\u0026amp; \\alpha D_{c} f (x) + \\beta D_{c} g (x) \\end{align*} $$\nだから$D_{c}$はリニアだ。\nPart 2. 有界\n$t := x - a$のように置換すると、\n$$ \\begin{align*} \\left\\| T_{a} f \\right\\|_{2} =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| T_{a} f \\left( x \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| f \\left( x - a \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| f \\left( t \\right) \\right|^{2} dt \\\\ =\u0026amp; \\left\\| f \\right\\|_{2} \\end{align*} $$\nだから$T_{a}$はバウンデッドだ。$\\left| e^{2 \\pi i b x } \\right| =1$なので、\n$$ \\begin{align*} \\left\\| E_{b} f \\right\\|_{2} =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| E_{b} f \\left( x \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| e^{2 \\pi i b x } f \\left( x \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} 1 \\cdot \\left| f \\left( t \\right) \\right|^{2} dt \\\\ =\u0026amp; \\left\\| f \\right\\|_{2} \\end{align*} $$\nだから$E_{b}$はバウンデッドだ。$t := x/c$のように置換すると、\n$$ \\begin{align*} \\left\\| D_{c} f \\right\\|_{2} =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| D_{c} f \\left( x \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| {{ 1 } \\over { \\sqrt{c} }} f \\left( {{ x } \\over { c }} \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} {{ 1 } \\over { c }} \\left| f \\left( t \\right) \\right|^{2} c dt \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| f \\left( t \\right) \\right|^{2} dt \\\\ =\u0026amp; \\left\\| f \\right\\|_{2} \\end{align*} $$\nだから$D_{c}$はバウンデッドだ。\n■\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p120-122\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1616,"permalink":"https://freshrimpsushi.github.io/jp/posts/1616/","tags":null,"title":"L2空間における変換：平行移動、変調、拡大"},{"categories":"거리공간","contents":"Definition Open Cover Given a metric space $(X,d)$ and a subset $E\\subset X$, a set $\\left\\{ O_{\\alpha} \\right\\}$ of open sets that satisfies the following equation is called an open coveropen cover of $E$.\n$$ E\\subset \\bigcup _{\\alpha} O_{\\alpha} $$\nA subset of an open cover is called a subcover. In particular, a subcover with a finite number of elements is called a finite subcover.\nCompactness Let\u0026rsquo;s assume we have a subset $K$ of a metric space $X$. If every open cover of $K$ has a finite subcover, then $K$ is said to be compactcompact. In other words, if we can still have an open cover by selecting a finite number of open sets, then $K$ is called compact. Expressing this condition with an equation, if for some $\\alpha_{1},\\cdots ,\\alpha_{n}$\n$$ K\\subset O_{\\alpha_{1}}\\cup \\cdots O_{\\alpha_{n}} $$\nis satisfied, then $K$ is compact.\nExplanation The importance of compactness comes from whether a given space retains or loses the property of being compact depending on what the entire space is considered to be. That is to say, compactness is an inherent quality of the set itself. Without going too far, even when observing the concept of openness, there is no guarantee that the property of being open is preserved when the entire space is expanded, hence the term relatively open exists. As one continues to study, it becomes apparent that the condition of being compact plays an important role in various theorems. Compactness is a property bestowed upon a set regardless of the entire space, as confirmed by the theorem below. First, we will use the term compact in $X$ when $K\\subset X$ is compact with respect to the entire space $X$.\nTheorem Consider two metric spaces $X$, $Y$ and suppose $K\\subset Y \\subset X$. Then the following two propositions are equivalent.\n(a) $K$ is compact in $X$.\n(b) $K$ is compact in $Y$.\nProof Lemma\nLet two metric spaces $X$, $Y$ be given, and suppose $E \\subset Y \\subset X$. Then the two following propositions are equivalent:$(d)$ $E$ is relatively open with respect to $Y$.$(e)$ For some open set $O_{X}$ of $X$, $E=Y \\cap O_{X}$ holds.\n(a) $\\Longrightarrow$ (b)\nAssume that $K$ is compact in $X$. Let $\\left\\{ O_{\\alpha}^{Y} \\right\\}$ be a set of open sets in $Y$ that satisfies $K\\subset \\bigcup_{\\alpha} O_{\\alpha}^{Y}$. In other words, assume $\\left\\{ O_{\\alpha}^{Y} \\right\\}$ to be any open cover of $K$ with respect to $Y$. Then, by the lemma,\n$$ O_{\\alpha}^{Y}=Y\\cap O_{\\alpha}^{X},\\quad \\forall \\alpha $$\nan open set $O_{\\alpha}^{X}$ in $X$ exists that satisfies the equation. Then $\\left\\{ O_{\\alpha}^{X} \\right\\}$ forms an open cover of $K$ with respect to $X$. Thus, by assumption, for some $\\alpha_{1},\\cdots,\\alpha_{n}$, the following equation holds:\n$$ K \\subset O_{\\alpha_{1}}^{X}\\cup\\cdots \\cup O_{\\alpha_{n}}^{X} $$\nHowever, since $K\\subset Y$, the following is true:\n$$ \\begin{align*} K \u0026amp; \\subset Y \\cap (O_{\\alpha_{1}}^{X}\\cup\\cdots \\cup O_{\\alpha_{n}}^{X}) \\\\ \u0026amp;= (Y \\cap O _{\\alpha_{1}}^{X})\\cup\\cdots \\cup(Y \\cap O_{\\alpha_{n}}^{X}) \\\\ \u0026amp;= O_{\\alpha_{1}}^{Y}\\cup\\cdots \\cup O_{\\alpha_{n}}^{Y} \\end{align*} $$\nTherefore, any arbitrary open cover $\\left\\{ O_{\\alpha}^{Y} \\right\\}$ of $K$ with respect to $Y$ has a finite subcover satisfying\n$$ K \\subset O_{\\alpha_{1}}^{Y}\\cup\\cdots \\cup O_{\\alpha_{n}}^{Y} $$\nhence, $K$ is compact in $Y$.\n(a) $\\Longleftarrow$ (b)\nSuppose that $K$ is compact in $Y$. Let $\\left\\{ O_{\\alpha}^{X} \\right\\}$ be a set of open sets in $X$ that satisfies $K\\subset \\bigcup_{\\alpha} O_{\\alpha}^{X}$. In other words, take $\\left\\{ O_{\\alpha}^{X} \\right\\}$ as any open cover of $K$ with respect to $X$. Then, set $O_{\\alpha}^{Y}$ as follows:\n$$ O_{\\alpha}^{Y}=Y\\cap O_{\\alpha}^{X},\\quad \\forall \\alpha $$\nBy the lemma, $O_{\\alpha}^{Y}$ becomes an open set in $Y$. Therefore, $\\left\\{ O_{\\alpha}^{Y} \\right\\}$ forms an open cover of $K$. Then, by assumption, for some $\\alpha_{1},\\cdots,\\alpha_{n}$, the following equation holds:\n$$ K \\subset O_{\\alpha_{1}}^{Y}\\cup \\cdots \\cup O_{\\alpha_{n}}^{Y} $$\nHowever, since for each $\\alpha$, $O_{\\alpha}^{Y} \\subset O_{\\alpha}^{X}$ is true, the following holds:\n$$ K\\subset O_{\\alpha_{1}}^{X}\\cup \\cdots \\cup O_{\\alpha_{n}}^{X} $$\nTherefore, as every arbitrary open cover always has a finite subcover, $K$ is compact in $X$.\n{{qed}}\nSee Also Compactness in Topological Spaces === ```markdown ## 定義 ### オープンカバー [距離空間](../381) $(X,d)$と部分集合 $E\\subset X$が与えられたとする。以下の式を満たす[開集合](../1700)の集合 $\\left\\\\{ O\\_{\\alpha} \\right\\\\}$を $E$の**オープンカバー**\u0026lt;sup\u0026gt;open cover\u0026lt;/sup\u0026gt; と言う。 $$\rE\\subset \\bigcup \\_{\\alpha} O\\_{\\alpha}\r$$ --- オープンカバーの部分集合を部分カバーと言う。特に、要素が有限個の部分カバーを有限部分カバーと言う。 ### コンパクト 距離空間 $X$の部分集合 $K$が与えられたとする。もし $K$の全てのオープンカバーに有限部分カバーが存在すれば、$K$は**コンパクト**\u0026lt;sup\u0026gt;compact\u0026lt;/sup\u0026gt;であるという。言い換えると、有限個の集合だけ選んでもまだオープンカバーであれば、$K$をコンパクトと言う。式で表すと、何らかの $\\alpha\\_{1},\\cdots ,\\alpha\\_{n}$に対して $$\rK\\subset O\\_{\\alpha\\_{1}}\\cup \\cdots O\\_{\\alpha\\_{n}}\r$$ が満たされれば、$K$はコンパクトである。 ## 説明 コンパクトが重要である理由は、全体空間を何にするかによって、その集合がコンパクトという性質を得たり失ったりするからである。つまり、**コンパクトはその集合が持つ固有の性質**という意味である。[開放](../1700)という概念を見ても、全体空間を拡大する時に、開かれているという性質が保持される保証がないために、[相対的に開かれている](../1703)という表現がある。学び続けると、[コンパクトという条件が様々な定理で重要な役割を果たす](../1728)ことが分かる。コンパクトは全体空間と無関係に集合に与えられる性質であると、以下の定理を通じて確認できる。まず、$K\\subset X$が全体空間 $X$に対してコンパクトである時、$X$においてコンパクトという表現を使う。 ## 定理 二つの距離空間 $X$、$Y$について $K\\subset Y \\subset X$とする。すると、以下の二つの命題は同値である。 **(a)** $K$は $X$でコンパクトである。 **(b)** $K$は $Y$でコンパクトである。 ## 証明 \u0026gt; [補助定理](../1703) \u0026gt; \u0026gt; 二つの距離空間 $X$、$Y$が与えられたとする。そして $E \\subset Y \\subset X$とする。すると、以下の二つの命題は同値である。$(d)$ $E$が $Y$に対して[相対的に開かれている。](../1703)$(e)$ $X$のある開集合 $O\\_{X}$について、$E=Y \\cap O\\_{X}$が成り立つ。 - **(a)** $\\Longrightarrow$ **(b)** $K$が $X$でコンパクトであると仮定する。$\\left\\\\{ O\\_{\\alpha}^{Y} \\right\\\\}$を $Y$で開かれている集合の集合とし、$K\\subset \\bigcup\\_{\\alpha} O\\_{\\alpha}^{Y}$を満たすとする。つまり、$\\left\\\\{ O\\_{\\alpha}^{Y} \\right\\\\}$を $K$の$Y$に対する任意のオープンカバーとするわけである。すると、補助定理によって $$\rO\\_{\\alpha}^{Y}=Y\\cap O\\_{\\alpha}^{X},\\quad \\forall \\alpha\r$$ $X$で開かれている集合 $O\\_{\\alpha}^{X}$が存在する。すると、$\\left\\\\{ O\\_{\\alpha}^{X} \\right\\\\}$は $K$の$X$に対するオープンカバーとなる。すると、仮定により、何らかの $\\alpha\\_{1},\\cdots,\\alpha\\_{n}$に対して以下の式が成立する。 $$\rK \\subset O\\_{\\alpha\\_{1}}^{X}\\cup\\cdots \\cup O\\_{\\alpha\\_{n}}^{X}\r$$ しかし、$K\\subset Y$なので、次が成立する。 $$\r\\begin{align*}\rK \u0026amp; \\subset Y \\cap (O\\_{\\alpha\\_{1}}^{X}\\cup\\cdots \\cup O\\_{\\alpha\\_{n}}^{X}) \\\\\\ \u0026amp;= (Y \\cap O \\_{\\alpha\\_{1}}^{X})\\cup\\cdots \\cup(Y \\cap O\\_{\\alpha\\_{n}}^{X}) \\\\\\ \u0026amp;= O\\_{\\alpha\\_{1}}^{Y}\\cup\\cdots \\cup O\\_{\\alpha\\_{n}}^{Y}\r\\end{align*}\r$$ したがって、$K$の $Y$に対する任意のオープンカバー $\\left\\\\{ O\\_{\\alpha}^{Y} \\right\\\\}$の有限部分カバーが $$\rK \\subset O\\_{\\alpha\\_{1}}^{Y}\\cup\\cdots \\cup O\\_{\\alpha\\_{n}}^{Y}\r$$ を満たすので、$K$は $Y$でコンパクトである。 - **(a)** $\\Longleftarrow$ **(b)** $K$が $Y$でコンパクトであると仮定する。$\\left\\\\{ O\\_{\\alpha}^{X} \\right\\\\}$を $K\\subset \\bigcup\\_{\\alpha} O\\_{\\alpha}^{X}$を満たす $X$の開集合の集合とする。つまり、$\\left\\\\{ O\\_{\\alpha}^{X} \\right\\\\}$を $K$の$X$に対する任意のオープンカバーとして選ぶわけである。そして、$O\\_{\\alpha}^{Y}$を以下のように設定する。 $$\rO\\_{\\alpha}^{Y}=Y\\cap O\\_{\\alpha}^{X},\\quad \\forall \\alpha\r$$ すると、補助定理によって $O\\_{\\alpha}^{Y}$は $Y$で開かれている集合になる。したがって、$\\left\\\\{ O\\_{\\alpha}^{Y} \\right\\\\}$は $K$のオープンカバーになる。すると、仮定により、何らかの $\\alpha\\_{1},\\cdots,\\alpha\\_{n}$に対して以下の式が成立する。 $$\rK \\subset O\\_{\\alpha\\_{1}}^{Y}\\cup \\cdots \\cup O\\_{\\alpha\\_{n}}^{Y}\r$$ しかし、各 $\\alpha$に対して $O\\_{\\alpha}^{Y} \\subset O\\_{\\alpha}^{X}$なので、次が成立する。 $$\rK\\subset O\\_{\\alpha\\_{1}}^{X}\\cup \\cdots \\cup O\\_{\\alpha\\_{n}}^{X}\r$$ したがって、$K$の任意のオープンカバーが常に有限部分カバーを持つので、$K$は $X$でコンパクトである。 ■\n## 同時に見る - [位相空間でのコンパクト性](../489)[](../489) ","id":1705,"permalink":"https://freshrimpsushi.github.io/jp/posts/1705/","tags":null,"title":"距離空間におけるコンパクト性"},{"categories":"거리공간","contents":"定義 $(X,d)$が距離空間だとしよう。$p \\in X$であり、$E \\subset X$とする。\n$d(q,p)\u0026lt;r$を満たすすべての$q$を含む集合を点$p$の近傍neighborhoodと定義し、$N_{r}(p)$と表記する。このとき$r$を$N_{r}(p)$の半径と呼ぶ。距離を省略できる場合は$N_{p}$のように表記することもある。\n$p$のすべての近傍が$q\\ne p$であり、$q\\in E$の$q$を含む場合、$p$を$E$の集積点limit pointと呼ぶ。\n$p\\in E$でありながら$p$が$E$の集積点でない場合、$p$を$E$の孤立点isolated pointと呼ぶ。\n$E$のすべての集積点が$E$に含まれる場合、$E$が閉じているclosedという。\n$N\\subset E$を満たす$p$の近傍$N$が存在する場合、$p$を$E$の内点interior pointと呼ぶ。\n$E$のすべての点が$E$の内点である場合、$E$が開いているopenという。\n$p \\in X$であり$p \\notin E$のすべての$p$を含む集合を$E$の補集合complementと呼び、$E^{c}$と表記する。\n$E$が閉じており$E$のすべての点が$E$の集積点である場合、$E$が完全perfectであるという。\n$\\forall p\\in E,\\ d(p,q)\u0026lt;M$を満たす点$q\\in X$と実数$M$が存在する場合、$E$を有界boundedと呼ぶ。\n$X$のすべての点が$E$の集積点であるか$E$の点である場合、$E$は$X$で密denseであるという。\n$E$のすべての集積点の集合を$E$の導出集合derived setと呼び、$E^{\\prime}$と表記する。\n$E$と$E^{\\prime}$の和集合を閉包closureと呼び、$\\overline{E}=E\\cup E^{\\prime}$と表記する。\n説明 上で述べる開、集積点、密、内点などは他のステートメントで定義されることもあるが、本質的には同じである。それぞれの概念をなぜ上のように定義し、名前を付けたのかは、1次元、2次元で直接図を描いてみれば感覚が簡単につかめるだろう。孤立点は集積点でない点と定義されるため、孤立点でありながら同時に集積点であることはできない。これとは異なり、開集合と閉集合はそれぞれ独立した条件で定義される。したがって、名前から感じられる直感とは異なり、開いていると同時に閉じている集合や、開いても閉じてもいない集合が存在することがある。前者の例として$\\mathbb{R}^{2}$があり、後者の例として$\\left\\{ {\\textstyle \\frac{1}{n}}\\ |\\ n\\in \\mathbb{N} \\right\\}$がある。内点と近傍の定義をよく考えると、$x$が$E$の内点である条件は\n$$ d(x,p) \u0026lt;\\varepsilon \\implies x \\in E $$\nが成立するようなある正数$\\varepsilon\u0026gt;0$が存在することと同じである。上の概念と関連するいくつかの定理と証明を紹介する。上の定義での表記に従う。\n定理1 すべての近傍は開集合である。\n証明 $E=N_{r}(p)$としよう。また、任意の$q \\in E$を考える。すると、近傍の定義により、以下の式を満たす正の実数$h$が必ず存在する。\n$$ d(p,q)=r-h\u0026lt;r $$\nすると、距離の定義により、$d(q,s)\u0026lt;h$を満たすすべての$s$に対して、以下の式が成立する。\n$$ d(p,s)\\le d(p,q)+d(q,s)\u0026lt;(r-h)+h=r $$\nしたがって、近傍の定義により、$s \\in E$である。これは、▷eq68\n◁の近傍$N_{h}(q)$内の任意の点$s$も$E$の要素であることを示している。したがって、$N_{h}(q) \\subset E$であるため、$q$は$E$の内点である。最初に$q$を$E$の任意の点としたので、$E$のすべての点は内点である。よって、$E$は開集合である。■\n定理2 集合$E$が開集合であることと$E^c$が閉集合であることは同値である。\n証明 $(\\impliedby)$\n$E^c$が閉じていると仮定する。今、任意の$p\\in E$について考える。すると$p \\notin E^c$であり、閉じている定義により$p$は$E^c$の集積点ではない。したがって、$N \\cap E^c=\\varnothing$を満たす$p$の近傍$N$が存在する。これは$N \\subset E$を意味し、内点の定義により$p$は$E$の内点である。任意の$p\\in E$がすべて$E$の内点であるため、定義により$E$は開集合である。\n$(\\implies)$\n$E$が開いていると仮定する。そして、$p$を$E^{c}$の集積点とする。すると、集積点の定義により、$p$のすべての近傍は少なくとも一つの$E^{c}$の点を含む。すると、$p$のすべての近傍は$E$に含まれず、これは$p$が$E$の内点ではないことを意味する。$E$は開いていると仮定したので、$p\\notin E$である。したがって、$E^{c}$のすべての集積点$p$が$E^{c}$に含まれるので、$E^{c}$は閉じている。\n■\n定理3 $p$を$E$の集積点としよう。すると、$p$の近傍は無数に多くの$E$の点を要素として持つ。\nこれを別の言い方をすると、「有限集合は集積点を持たない」「集積点を持つ集合は無限集合である」ということである。\n証明 $p$の近傍$N$が$E$の有限個の要素のみを含むと仮定しよう。そして、$q_{1},q_{2},\\cdots,q_{n}$を$p$ではない$N\\cap E$の点としよう。そして、$p$と$q_{i}$の距離の中で最小値を$r$とする。\n$$ r= \\min \\limits _{1\\le i \\le n}d(p,q_{i}) $$\n各々の$q_{i}$は$p$と異なる点であるため、すべての距離は正であり、正の数の中で最小値を選んでも正であるため、$r\u0026gt;0$である。今、$p$の別の近傍$N_{r}(p)$を考える。すると、近傍と距離の定義により、$N_{r}(p)$にはいかなる$q_{i}$も含まれない。すると、集積点の定義により、$p$は$E$の集積点ではない。これは、$p$が$E$の集積点であるという事実に矛盾する。したがって、帰納法により仮定が間違っていることがわかる。したがって、上の定理は成立する。\n■\n系 有限個の点のみを持つ集合は集積点を持たない。\n定理4 距離空間$(X,d)$と$E \\subset X$に対して、以下の事実が成立する。$(a)$ $\\overline{E}$は閉じている。$(b)$ $E=\\overline{E}$であることと同値は$E$が閉じていることである。$(c)$ $E\\subset F$を満たすすべての閉集合$F\\subset X$に対して$\\overline{E} \\subset F$が成立する。\n$(a)$と$(c)$によって、$\\overline{E}$は$E$を含む最小の$X$の閉部分集合である。\n","id":1700,"permalink":"https://freshrimpsushi.github.io/jp/posts/1700/","tags":null,"title":"メートル空間における近傍、限界点、オープン、クローズド"},{"categories":"거리공간","contents":"定義 $(X,d)$が距離空間であるとする。$p \\in X$であり、$E \\subset X$であるとする。\n$d(q,p)\u0026lt;r$を満たす全ての$q$を含む集合を点$p$の近傍と定義し、$N_{r}(p)$と記す。この時、$r$を$N_{r}(p)$の半径と呼ぶ。距離を省略して良い場合は$N_{p}$とも記す。\n$p$の全ての近傍が$q\\ne p$であり、$q\\in E$である$q$を含んでいれば、$p$を$E$の集積点と呼ぶ。\n$E$の全ての集積点が$E$に含まれる場合、$E$が閉じていると言う。\n$N\\subset E$を満たす$p$の近傍$N$が存在すれば、$p$を$E$の内点と呼ぶ。\n$E$の全ての点が$E$の内点である場合、$E$が開いていると言う。\n$E$の全ての集積点の集合を$E$の導集合と呼び、$E^{\\prime}$と記す。\n$E$と$E^{\\prime}$の合併集合を閉包と呼び、$\\overline{E}=E\\cup E^{\\prime}$と記す。\n定理1 $A,B\\subset X$に対して以下の式が成立する。\n(1a) $A\\subset B \\implies A^{\\prime} \\subset B^{\\prime}$\n(1b) $(A\\cup B)^{\\prime}=A^{\\prime}\\cup B^{\\prime}$\n(1c) $(A \\cap B)^{\\prime} \\subset A^{\\prime}\\cap B^{\\prime}$\n証明 (1a) $A\\subset B$と仮定する。そして$p\\in A^{\\prime}$とする。すると$p$は$A$の集積点であるため、集積点の定義により以下の文が成立する。$p$の全ての近傍$N$は$q\\ne p$であり、$q\\in A$である$q$を含む。この時、$A\\subset B$と仮定したので、上記の文は以下の文を意味する。$p$の全ての近傍$N$は$q\\ne p$であり、$q\\in B$である$q$を含む。したがって、集積点の定義により$p \\in B^{\\prime}$である。\n■\n(1b) 部分 1. $A^{\\prime} \\cup B^{\\prime} \\subset (A\\cup B)^{\\prime}$\n$A\\subset A\\cup B$であり、$B \\subset A\\cup B$であるため、$(a1)$によって以下のようになる。\n$$ A^{\\prime} \\subset (A\\cup B)^{\\prime} \\quad \\text{and} \\quad B^{\\prime} \\subset (A \\cup B)^{\\prime} $$\nしたがって\n$$ A^{\\prime} \\cup B^{\\prime} \\subset (A\\cup B)^{\\prime} $$\n部分 2. $(A\\cup B)^{\\prime} \\subset A^{\\prime}\\cup B^{\\prime}$\n$p \\in (A\\cup B)^{\\prime}$とする。すると集積点の定義により$p$の全ての近傍$N$は$q\\ne p$であり、$q\\in A\\cup B$である$q$を含む。$q\\in A\\cup B$を再び書くと$q\\in A \\text{ or } q\\in B$であるため、これは$p \\in A^{\\prime} \\text{ or } p\\in B^{\\prime}$と同じである。したがって$p\\in A^{\\prime}\\cup B^{\\prime}$であるため、以下のようになる。\n$$ (A\\cup B)^{\\prime} \\subset A^{\\prime}\\cup B^{\\prime} $$\n部分 3.\n上記の結果を総合すると以下のようになる。\n$$ A^{\\prime}\\cup B^{\\prime} = (A\\cup B)^{\\prime} $$\n■\n(1c) $A\\cap B \\subset A$であり、$A\\cap B \\subset B$であるため、(1a) により以下のようになる。\n$$ (A\\cap B)^{\\prime} \\subset A^{\\prime} \\quad \\text{and} \\quad (A\\cap B)^{\\prime} \\subset B^{\\prime} $$\nしたがって\n$$ (A\\cap B)^{\\prime} \\subset A^{\\prime}\\cap B^{\\prime} $$\n■\n定理2 $A,B \\subset X$に対して以下の式が成立する。\n(2a) $A\\subset B \\implies \\overline{A} \\subset \\overline{B}$\n(2b) $\\overline{A\\cup B} = \\overline{A}\\cup \\overline{B}$\n(2c) $\\overline{A\\cap B} \\subset \\overline{A}\\cap \\overline{B}$\n証明 (2a) $A \\subset B$と仮定する。すると**(1a)** により$A^{\\prime} \\subset B^{\\prime}$である。したがって\n$$ \\overline{A} = A\\cup A^{\\prime} \\subset B \\cup B^{\\prime} = \\overline{B} $$\n■\n(2b) 部分 1. $\\overline{A\\cup B}\\subset \\overline{A}\\cup \\overline{B}$\n$p \\in \\overline{A\\cup B}$とする。すると$p\\in A\\cup B$であるか$p \\in (A\\cup B)^{\\prime}$であるという意味である。\nケース 1-1. $p \\in A\\cup B$\nこの場合$p \\in A$であるか$p \\in B$である。しかし$A \\subset \\overline{A}$であり、$B \\subset \\overline{B}$であるため\n$$ p\\in \\overline{A}\\ \\text{or} \\ p \\in \\overline{B}\\implies p \\in \\overline{A}\\cup \\overline{B} $$\nケース 1-2. $p\\in (A\\cup B)^{\\prime}$\n(1b) によって$p\\in (A\\cup B)^{\\prime}=A^{\\prime}\\cup B^{\\prime}$である。これは$p\\in A^{\\prime}$であるか$p\\in B^{\\prime}$であるという意味である。しかし$A^{\\prime} \\subset \\overline{A}$であり、$B^{\\prime} \\subset \\overline{B}$であるため、上記のケースと同様に\n$$ p\\in \\overline{A}\\ \\text{or} \\ p \\in \\overline{B}\\implies p \\in \\overline{A}\\cup\\overline{B} $$\nケース 1-1, 1-2によって以下が成立する。\n$$ \\overline{A\\cup B}\\subset \\overline{A}\\cup \\overline{B} $$\n部分 2. $\\overline{A}\\cup \\overline{B} \\subset \\overline{A\\cup B}$\n$A \\subset A\\cup B$であり、$B\\subset A\\cup B$であるため、$(b1)$によって以下が成立する。\n$$ \\overline{A} \\subset \\overline{A\\cup B}\\quad \\text{and} \\quad \\overline{B}\\subset \\overline{A\\cup B} $$\nしたがって\n$$ \\overline{A}\\cup \\overline{B} \\subset \\overline{A\\cup B} $$\n■\n(2c) $p \\in \\overline{A\\cap B}$とする。すると$p\\in A\\cap B$であるか$p\\in (A \\cap B)^{\\prime}$である。\nケース 1. $p\\in A\\cap B$\nこの場合$p \\in A$でありながら$p \\in B$である。しかし$A\\subset \\overline{A}$であり、$B\\subset \\overline{B}$であるため\n$$ p\\in A \\ \\text{and} \\ \\ p \\in B \\implies p\\in \\overline{A} \\ \\text{and} \\ p \\in \\overline{B} \\implies p\\in \\overline{A}\\cap \\overline{B} $$\nケース 2. $p \\in (A\\cap B)^{\\prime}$\n(1a) によって$(A\\cap B)^{\\prime}\\subset A^{\\prime}$であり、$(A\\cap B)^{\\prime} \\subset B^{\\prime}$である。しかし$A^{\\prime}\\subset \\overline{A}$であり、$B^{\\prime} \\subset \\overline{B}$であるため\n$$ p\\in A^{\\prime} \\ \\text{and} \\ p\\in B^{\\prime} \\implies p\\in \\overline{A}\\quad \\text{and} \\quad p\\in \\overline{B}\\implies p\\in \\overline{A}\\cap \\overline{B} $$\n■\n定理3 距離空間$(X,d)$と$E \\subset X$に対して以下の事実が成立する。\n(3a) $\\overline{E}$は閉じている。\n(3b) $E=\\overline{E}$であることと同等であるのは、$E$が閉じていることである。\n(3c) $E\\subset F$を満たす閉集合$F\\subset X$に対して、$\\overline{E} \\subset F$が成立する。\n(3a) と (3c) によって、$\\overline{E}$は$E$を含む最小の$X$の閉部分集合である。\n証明 (3a) $p \\in X$であり、$p \\notin \\overline{E}$とする。すなわち$p \\in (\\overline{E})^{c}$である。すると$p$は$E$の点でも$E^{\\prime}$の点でもない。したがって、集積点の定義により$p$は少なくとも一つの$N\\cap E=\\varnothing$である近傍$N$を持つ。したがって$N\\subset (\\overline{E})^{c}$であり、$p$は$(\\overline{E})^{c}$の任意の点であったので、内点の定義により$(\\overline{E})^{c}$の全ての点が内点であり、これは$(\\overline{E})^{c}$が開集合であることを意味する。$(\\overline{E})^{c}$が開集合であるため、$\\overline{E}$は閉集合である。1\n■\n(3b) $(\\implies)$\n$E=\\overline{E}=E \\cup E^{\\prime}$であるため、$E$の全ての集積点は$E$の要素である。これは閉集合の定義であるため、$E$は閉じている。または、閉包と閉じることの定義から直ちに成立することがわかる。\n$(\\impliedby)$\n閉集合の定義により、$E$の全ての集積点は$E$に含まれる。したがって、$\\overline{E}=E\\cup E^{\\prime}=E$である。\n■\n(3c) $F$を$E\\subset F \\subset X$である閉集合とする。すると**(3b)** によって$F^{\\prime} \\subset \\overline{F}=F$である。また、(2a) によって$E^{\\prime} \\subset F^{\\prime} \\subset F$である。したがって、以下が成立する。\n$$ E \\subset F \\quad \\text{and} \\quad E^{\\prime}\\subset F $$\nしたがって\n$$ E\\cup E^{\\prime} =\\overline{E} \\subset F $$\n■\n定理4 $E$を空集合ではない実数集合であり、上に有界とする。そして$y=\\sup E$とする。すると$y \\in \\overline{E}$である。また、$E$が閉じていれば、$y \\in E$である。\n証明 $y \\in \\overline{E}$であることが成立すれば、その後の命題は (3a) により自明であるので、$y \\in \\overline{E}$のみ証明することにする。2つの場合に分けて証明する。\nケース 1. $y \\in E$\n$$ y \\in E \\subset \\overline{E} $$\nであるため、成立する。\nケース 2. $y \\notin E$\nすると全ての正数$h\u0026gt;0$に対して、$y-h\u0026lt;x\u0026lt;y$を満たす$x\\in E$が存在する。これは$y$の全ての近傍である$N_{h}(y)$内に$E$の要素が必ず含まれることを意味する。したがって、定義により$y$は$E$の集積点である。したがって$y\\in E\\cup E^{\\prime}=\\overline{E}$である。\n■\n定理2 参照\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1701,"permalink":"https://freshrimpsushi.github.io/jp/posts/1701/","tags":null,"title":"計量空間における閉包と派生集合"},{"categories":"분포이론","contents":"定義 1 自由度 $r_{1}, r_{2} \u0026gt; 0$ に対して以下の確率密度関数を持つ連続確率分布 $F \\left( r_{1} , r_{2} \\right)$ をF分布という。 $$ f(x) = {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} x^{r_{1} / 2 - 1} \\left( 1 + {{ r_{1} } \\over { r_{2} }} x \\right)^{-(r_{1} + r_{2}) / 2} \\qquad , x \\in (0, \\infty) $$\n$B(r_{1} / 2, r_{2}/2)$ はベータ関数を意味する。 基本性質 モーメント生成関数 平均と分散 [2]: $X \\sim F ( r_{1} , r_{2})$ の場合 $$ \\begin{align*} E(X) =\u0026amp; {{ r_{2} } \\over { r_{2} - 2 }} \u0026amp; \\qquad , r_{2} \u0026gt; 2 \\\\ \\text{Var}(X) =\u0026amp; {{ 2 r_{2}^{2} (r_{1} + r_{2} - 2) } \\over { r_{1} (r_{2} -2)^{2} (r_{2} - 4) }} \u0026amp; \\qquad , r_{2} \u0026gt; 4 \\end{align*} $$ 定理 二つの確率変数 $U,V$ が独立で、$U \\sim \\chi^{2} ( r_{1})$、$V \\sim \\chi^{2} ( r_{2})$ だとする。\n$k$次のモーメント [a]: $d_{2} \u0026gt; 2k$ の場合 $\\displaystyle F := {{ U / r_{1} } \\over { V / r_{2} }}$ は$k$次のモーメントが存在し、 $$ E F^{k} = \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k} E U^{k} E V^{-k} $$ カイ二乗分布から導出 [b]: $${{ U / r_{1} } \\over { V / r_{2} }} \\sim F \\left( r_{1} , r_{2} \\right)$$ ベータ分布から導出 [c]: 自由度 $r_{1} , r_{2}$ のF分布に従う確率変数 $X \\sim F \\left( r_{1}, r_{2} \\right)$ に対して次のように定義された$Y$ は、ベータ分布 $\\text{Best} \\left( {{ r_{1} } \\over { 2 }} , {{ r_{2} } \\over { 2 }} \\right)$ に従う。 $$ Y := {{ \\left( r_{1} / r_{2} \\right) X } \\over { 1 + \\left( r_{1} / r_{2} \\right) X }} \\sim \\text{Beta} \\left( {{ r_{1} } \\over { 2 }} , {{ r_{2} } \\over { 2 }} \\right) $$ t分布から導出 [d]: 自由度 $\\nu \u0026gt; 0$ のt分布に従う確率変数 $X \\sim t(\\nu)$ に対して次のように定義された$Y$ は、F分布 $F (1,\\nu)$ に従う。 $$ Y := X^{2} \\sim F (1,\\nu) $$ 相互性Reciprocality [e]: $X \\sim F \\left( r_{1}, r_{2} \\right)$ の場合、その逆数の分布は次のようになる。 $$ {{ 1 } \\over { X }} \\sim F \\left( r_{2}, r_{1} \\right) $$ $\\chi^{2} \\left( r \\right)$ は自由度 $r$ のカイ二乗分布だ。 説明 t分布がスチューデントStudent t分布と呼ばれるように、F分布も統計学者ジョージ・スネデコーの名前を取ってスネデコーSnedecor F分布と呼ばれることがある。2\nF分布の確率密度関数は一見複雑に見えるが、実際には式を操作する必要はほとんどなく、カイ二乗分布との関係をよく理解することが最優先だ。カイ二乗分布が適合度検定に使用されたように、F分布は二つの母集団の分散を比較する際に使用できる。定理[b]で直接確認できるように、F分布はカイ二乗分布に従うデータの比として表されるため、この統計量が$1$から遠く離れている場合は、二つの分布の分散が異なると推測できるのだ。\n証明 1 確率変数のモーメント生成関数が存在するとは、すべての$k \\in \\mathbb{N}$ に対して$k$次のモーメントが存在することを意味する。しかし、定理[a]でのF分布の$k$次のモーメントは$k \u0026lt; d_{2} / 2$ のときに存在するため、モーメント生成関数は存在しえない。\n■\n[2] 定理[a]に記載されたモーメント公式を使用する。\n■\n[a] $t = {{ r_{1} } \\over { r_{2} }} x$ と置換すると$dt = {{ r_{1} } \\over { r_{2} }} dx$ となるので、 $$ \\begin{align*} E F^{k} =\u0026amp; \\int_{0}^{\\infty} x^{k} {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} x^{r_{1} / 2 - 1} \\left( 1 + {{ r_{1} } \\over { r_{2} }} x \\right)^{-(r_{1} + r_{2}) / 2} dx \\\\ =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} \\int_{0}^{\\infty} x^{k + r_{1} / 2 - 1} \\left( 1 + {{ r_{1} } \\over { r_{2} }} x \\right)^{-(r_{1} + r_{2}) / 2} dx \\\\ =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} \\int_{0}^{\\infty} \\left( {{ r_{2} } \\over { r_{1} }} t \\right)^{k + r_{1} / 2 - 1} \\left( 1 + t \\right)^{-(r_{1} + r_{2}) / 2} {{ r_{2} } \\over { r_{1} }} dt \\\\ =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k + r_{1} / 2}\\int_{0}^{\\infty} t^{k + r_{1} / 2 } \\left( 1 + t \\right)^{-r_{1}/2 - r_{2}/ 2} dt \\\\ =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k }\\int_{0}^{\\infty} t^{k + r_{1} / 2 } \\left( 1 + t \\right)^{-(r_{1}/2+k) - (r_{2}/ 2-k)} dt \\end{align*} $$\nベータ関数の定積分形式の表示: $$ B(p,q)=\\int_{0}^{\\infty}\\frac{ t^{p-1} }{ (1+t)^{p+q}}dt $$\nベータ関数とガンマ関数の関係: $$ B(p,q) = {{\\Gamma (p) \\Gamma (q)} \\over {\\Gamma (p+q) }} $$\n$$ \\begin{align*} EF^{k} =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } B \\left( {{ r_{1} } \\over { 2 }} + k, {{ r_{2} } \\over { 2 }} - k \\right) \\\\ =\u0026amp; \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } {{ \\Gamma (r_{1}/2 + r_{2}/2) } \\over { \\Gamma (r_{1}/2 ) \\Gamma ( r_{2}/2) }} {{ \\Gamma (r_{1}/2 + k) \\Gamma ( r_{2}/2 - k) } \\over { \\Gamma (r_{1}/2 +k + r_{2}/2 - k) }} \\\\ =\u0026amp; \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } {{ 1 } \\over { \\Gamma (r_{1}/2 ) \\Gamma ( r_{2}/2) }} {{ \\Gamma (r_{1}/2 + k) \\Gamma ( r_{2}/2 - k) } \\over { 1 }} \\\\ =\u0026amp; \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } {{ \\Gamma (r_{1}/2 + k) 2^{k}} \\over { \\Gamma (r_{1}/2 ) }} {{ 2^{-k} \\Gamma ( r_{2}/2 - k) } \\over { \\Gamma ( r_{2}/2) }} \\end{align*} $$\nカイ二乗分布のモーメント: $X \\sim \\chi^{2} (r)$ とする。$k \u0026gt; - r/ 2$ の場合、$k$次のモーメントが存在する $$ E X^{k} = {{ 2^{k} \\Gamma (r/2 + k) } \\over { \\Gamma (r/2) }} $$\n$$ E F^{k} = \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } E U^{k} E V^{-k} $$\n■\n[b] ジョイント密度関数から直接導く。\n■\n[c] 変数変換で直接導く。\n■\n[d] カイ二乗分布の比として遠回りする。\n■\n[e] 分子と分母が逆転しているので、定理[b]に従って自明だ。実用的な統計学者の視点からは、定理[b]に従ってF分布を定義し、それに基づいて確率密度関数を導出する方が自然だ。\n■\n参照 一般化: 非中心F分布 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p194.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCasella. (2001). Statistical Inference(2nd Edition): p222.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1606,"permalink":"https://freshrimpsushi.github.io/jp/posts/1606/","tags":null,"title":"F分布"},{"categories":"분포이론","contents":"定義 1 自由度 $r \u0026gt; 0$に対して、以下のような確率密度関数を持つ連続確率分布 $\\chi^{2} (r)$をカイ二乗分布chi-square Distributionと言う。 $$ f(x) = {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} \\qquad , x \\in (0, \\infty) $$\n$\\Gamma$はガンマ関数を示す。 基本性質 モーメント生成関数 [1]: $$m(t) = (1-2t)^{-r/2} \\qquad , t \u0026lt; {{ 1 } \\over { 2 }}$$ 平均と分散 [2] 平均と分散が$X \\sim \\chi^{2} (r)$である場合、 $$ \\begin{align*} E(X) =\u0026amp; r \\\\ \\text{Var} (X) =\u0026amp; 2r \\end{align*} $$ 十分統計量 [3]: カイ二乗分布に従うランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\chi^{2} (r)$が与えられたとする。$r$に対する十分統計量 $T$は次の通り。 $$ T = \\left( \\prod_{i} X_{i} \\right) $$ 定理 $k$次モーメント [a]: $X \\sim \\chi^{2} (r)$とする。$k \u0026gt; - r/ 2$の場合、$k$次モーメントが存在し、 $$ E X^{k} = {{ 2^{k} \\Gamma (r/2 + k) } \\over { \\Gamma (r/2) }} $$ ガンマ分布との関係 [b]: $$\\Gamma \\left( { r \\over 2 } , 2 \\right) \\iff \\chi ^2 (r)$$ F分布の導出 [c]: 2つの確率変数$U,V$が独立であり、$U \\sim \\chi^{2} ( r_{1})$、$V \\sim \\chi^{2} ( r_{2})$である場合、 $$ {{ U / r_{1} } \\over { V / r_{2} }} \\sim F \\left( r_{1} , r_{2} \\right) $$ 標準正規分布の二乗との関係 [d]: $X \\sim N(\\mu,\\sigma ^2)$の場合、 $$ V=\\left( { X - \\mu \\over \\sigma} \\right) ^2 \\sim \\chi ^2 (1) $$ 説明 カイ二乗分布は、統計学全般で広く使用される分布で、特に適合度検定や分散分析などで最初に遭遇することが多い。\n定理[d]は特に重要で、この定理の逆命題によって、標準化された残差Residualsの二乗がカイ二乗分布$\\chi^{2} (1)$に従わない場合、残差の正規性に問題があることを検出することができる。\n証明 戦略 [1], [a]: 置換積分を通じて定積分記号の中にある物を外へ出し、ガンマ関数に変えるトリックを使用する。\nガンマ関数の定義: $$ \\Gamma (x) := \\int_{0}^{\\infty} y^{x-1} e^{y} dy $$\n[1] $y=x(1/2-t)$のように置換すると、${{ 1 } \\over { 1/2 - t }}dy = dx$であるため、 $$ \\begin{align*} m(t) =\u0026amp; \\int_{0}^{\\infty} e^{tx} {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} dx \\\\ =\u0026amp; {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} x^{r/2-1} e^{x(1/2-t)} dx \\\\ =\u0026amp; {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} \\left( {{ y } \\over { 1/2 -t }} \\right)^{r/2-1} e^{y} {{ 1 } \\over { 1/2 - t }} dy \\\\ =\u0026amp; (1/2-t)^{-r/2}{{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} y^{r/2-1} e^{y} dy \\\\ =\u0026amp; (1-2t)^{-r/2}{{ 1 } \\over { \\Gamma (r/2) }} \\int_{0}^{\\infty} y^{r/2-1} e^{y} dy \\end{align*} $$ ガンマ関数の定義により、 $$ m(t) = (1-2t)^{-r/2} \\qquad , t \u0026lt; {{ 1 } \\over { 2 }} $$\n■\n[2] モーメント公式[a]に代入する。\n■\n[a] $y = x/2$のように置換すると、$2 dy = dx$であるため、 $$ \\begin{align*} EX^{k} =\u0026amp; \\int_{0}^{\\infty} x^{k} {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} dx \\\\ =\u0026amp; {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} x^{r/2+k-1} e^{-x/2} dx \\\\ =\u0026amp; {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} 2^{r/2+k-1} y^{r/2+k-1} e^{-y} 2dy \\\\ =\u0026amp; {{ 2^{k} } \\over { \\Gamma (r/2) }} \\int_{0}^{\\infty} y^{(r/2+k)-1} e^{-y} 2dy \\end{align*} $$ ガンマ関数の定義により、 $$ E X^{k} = {{ 2^{k} \\Gamma (r/2 + k) } \\over { \\Gamma (r/2) }} $$\n■\n[b] モーメント生成関数で示される。\n■\n[c] ジョイント密度関数で直接演繹する。\n■\n[d] 確率密度関数で直接演繹する。\n■\n参照 一般化: 非中心カイ二乗分布 Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p161.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1600,"permalink":"https://freshrimpsushi.github.io/jp/posts/1600/","tags":null,"title":"カイ二乗分布"},{"categories":"힐베르트공간","contents":"定義 関数空間 $\\mathbb{C}^{\\mathbb{R}}$ の関数 $f : \\mathbb{R} \\to \\mathbb{C}$ を考えてみよう。\n関数 $f$ のサポートsupportは、関数値が $0$ ではない点の集合にクロージャを取ったクローズセットとして次のように定義される。 $$ \\text{supp} f = \\overline{\\left\\{ x \\in \\mathbb{R} : f(x) \\ne 0 \\right\\}} $$\n$\\text{supp} f$が有界なら、$f$がコンパクトサポートを持つと言う。クロージャは閉集合であり、実数空間で閉じていて有界な集合はコンパクトだからである。\n$U\\Subset V$は$\\overline{U} \\subset V$であり$\\overline{U}$がコンパクトであることを意味する。つまり、$\\mathrm{supp}(f) \\Subset U$は$f$が$U$でコンパクトサポートを持つことを意味する。$\\subset \\subset$として書くこともある。\n連続関数の集合はベクトル空間になり、これを連続関数空間と呼び、次のように表記する。\n$$ C(\\mathbb{R}) := \\left\\{f \\text{ is continuous} \\right\\} $$\n$C^{1}$と混同する可能性がある場合は、$C^{0}$と書くこともある。\nコンパクトサポートを持つ連続関数のベクトル空間を次のように表記する。\n$$ C_{c} (\\mathbb{R}) := \\left\\{ f \\in C(\\mathbb{R}) : f \\text{ has compact support} \\right\\} $$\n$x \\to \\pm \\infty$の時、関数値が$0$に収束する連続関数のベクトル空間を次のように表記する。\n$$ C_{0} ( \\mathbb{R} ) := \\left\\{ f \\in C(\\mathbb{R}) : f(x) \\to 0 \\text{ as } x \\to \\pm \\infty \\right\\} $$\n$m$回まで微分可能であり、その導関数がすべて連続である連続関数のベクトル空間を次のように表記する。\n$$ C^{m}(\\mathbb{R}) :=\\left\\{ f \\in C(\\mathbb{R}) : f^{(n)} \\text{ is continuous } \\forall n \\le m \\right\\} $$\nこの場合$C^{0}(\\mathbb{R})$は$C(\\mathbb{R})$を意味する。この時の$C^{m}$の要素を$m$回 連続的に微分可能な関数continuously differentiable functionと呼ぶ。\n無限に微分可能で、その導関数がすべて連続である連続関数のベクトル空間を次のように表記する。 $$ C^{\\infty}(\\mathbb{R})=\\bigcap _{m=0}^{\\infty}C^{m}(\\mathbb{R}) $$ この時の$C^{\\infty}$の要素をスムース関数smooth functionと呼ぶ。\n※ 著者によっては$C_{0}$を$C_{c}$の意味で使う場合があるので、教科書で定義された表記をよく確認しよう。\n説明 ソボレフ空間、超関数論などでは$C_{c}^{\\infty}$を主に扱うことになる。\n当然ながら$C_{c} (\\mathbb{R})$は$C_{0} (\\mathbb{R})$の部分空間になる。二つとも単なる連続関数の空間$C (\\mathbb{R})$に比べて良い空間だが、作用素ノルム $\\left\\| \\cdot \\right\\|_{\\infty} $に対してバナッハ空間にならないことに注意する必要がある。例えば、次のような$\\left\\{ f_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset C_{c} (\\mathbb{R})$を考えてみよう\n$$ f_{k} (x) := \\begin{cases} {{ \\sin x } \\over { x }} \\chi_{[ - k \\pi , k \\pi ]} (x) \u0026amp; , x \\ne 0 \\\\ 1 \u0026amp; , x = 0 \\end{cases} $$\n$f_{k}$はすべての$k \\in \\mathbb{N}$に対してコンパクトサポート$[-k \\pi , k \\pi]$を持つが、次のようなシンク関数 $\\text{sinc} \\in C_{0} (\\mathbb{R}) \\setminus C_{c} (\\mathbb{R})$に収束する。\n$$ \\text{sinc} x = \\begin{cases} {{ \\sin x } \\over { x }} \u0026amp; , x \\ne 0 \\\\ 1 \u0026amp; , x = 0 \\end{cases} $$\n距離空間として1 区間$[0, 1]$上で連続な実数値関数の集合を$X = C[0, 1]$としよう。そして、[距離] $d$を次のように定義しよう。\n$$ d(x, y) := \\int\\limits_{0}^{1} \\left| x(t) - y(t) \\right| dt \\qquad \\forall x, y \\in X $$\nすると、距離空間$(X, d)$は完備空間ではない。以下の図(a)に示すような関数$x_{m}$を考えよう。\n$n \\gt m$とすると、任意の$\\varepsilon \\gt 0$に対して$m \\gt 1/\\varepsilon$の時はいつでも$1 \\cdot \\frac{1}{m} \\lt \\varepsilon$が成立するので、$d(x_{m}, x_{n}) \\lt \\varepsilon$によって$\\left\\{ x_{m} \\right\\}$はコーシー数列である。\nしかし、$x_{m}(t) = 0$と$(t \\in [0, 1/2])$であり、$x_{m}(t) = 1$と$(t \\in [a_{m}, 1])$なので、次のようになる。\n$$ \\begin{align*} d(x_{m}, x) \u0026amp;= \\int\\limits_{0}^{1} \\left| x_{m(t)} - x(t) \\right| dt \\\\ \u0026amp;= \\int\\limits_{0}^{\\frac{1}{2}} \\left| 0 - x(t) \\right| dt + \\int\\limits_{\\frac{1}{2}}^{a_{m}} \\left| x_{m(t)} - x(t) \\right| dt + \\int\\limits_{a_{m}}^{1} \\left| 1 - x(t) \\right| dt \\\\ \u0026amp;= \\int\\limits_{0}^{\\frac{1}{2}} \\left| x(t) \\right| dt + \\int\\limits_{\\frac{1}{2}}^{a_{m}} \\left| x_{m(t)} - x(t) \\right| dt + \\int\\limits_{a_{m}}^{1} \\left| 1 - x(t) \\right| dt \\\\ \\end{align*} $$\n各被積分関数が$0$以上であるため、$d(x_{m}, x)$が$0$に収束するためには、各被積分関数が$0$でなければならない。つまり、$x$は$t\\in[0, \\frac{1}{2})$で$x(t) = 0$であり、$t\\in (\\frac{1}{2}, 1]$では$x(t) = 1$である。これは明らかに連続関数ではないため、$x \\notin X$であり、$\\left\\{ x_{m} \\right\\}$は$X$に収束しない。\nノルム空間として2 連続関数空間$C[0, 1]$は、積分ではなく、最大値をノルムとして与えると完備空間、つまり完備ノルム空間(バナッハ空間)となる。つまり、以下のように定義された$\\left\\| \\cdot \\right\\|$に対して$(C[0, 1], \\left\\| \\cdot \\right\\|)$はバナッハ空間である。\n$$ \\left\\| f \\right\\| := \\max\\limits_{t \\in [0, 1]} \\left| f(t) \\right|,\\qquad f \\in C[0, 1] $$\nErwin Kreyszig, Introductory Functional Analysis with Applications (1978), p38\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nErwin Kreyszig, Introductory Functional Analysis with Applications (1978), p61-62\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1594,"permalink":"https://freshrimpsushi.github.io/jp/posts/1594/","tags":null,"title":"関数のサポートと連続関数空間のクラス"},{"categories":"바나흐공간","contents":"定義1 $(X, \\left\\| \\cdot \\right\\|)$をノルム空間と呼ぶことにする。$X$のすべての元$\\mathbf{x}\\in X$に対して、以下を満たすスカラーの系列$\\left\\{ a_{k} \\right\\}_{k \\in \\mathbb{N}}$が一意に存在するならば、$\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset X$を$X$のシャウダー基底Schauder basisという。\n$$ \\mathbf{x}= \\sum_{k \\in \\mathbb{N}} a_{k} \\mathbf{e}_{k} $$\n説明 ベクトル空間の基底は、特に「無限」の線形結合について議論するとき、シャウダー基底と呼ばれる。無限について語るだけあって、バナッハ空間に関する性質が多く関連しており、特にヒルベルト空間については、以下の有用な定理が知られている。\n正規直交基底の同値条件：$H$がヒルベルト空間とする。$H$の正規直交系$\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset H$に対して、以下はすべて同値である。\n(i): $\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset H$は$H$の正規直交基底である。 (ii): すべての$\\mathbf{x}\\in H$に対して、 $$ \\mathbf{x}= \\sum_{k \\in \\mathbb{N}} \\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle \\mathbf{e}_{k} $$ (iii): すべての$\\mathbf{x}, \\mathbf{y} \\in H$に対して、 $$ \\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{k \\in \\mathbb{N}} \\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle \\langle \\mathbf{e}_{k} , \\mathbf{y} \\rangle $$ (iv): すべての$\\mathbf{x}\\in H$に対して、 $$ \\sum_{k \\in \\mathbb{N}} \\left| \\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle \\right|^{2} = \\left\\| \\mathbf{x}\\right\\|^{2} $$ (v): $\\overline{\\text{span}} \\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} = H$ (vi): $\\mathbf{x}\\in H$であり、すべての$k \\in \\mathbb{N}$に対して、$\\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle = 0$ならば$\\mathbf{x}= \\mathbf{0}$ 参照 有限次元ベクトル空間の基底: ハメル基底 Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p42\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1583,"permalink":"https://freshrimpsushi.github.io/jp/posts/1583/","tags":null,"title":"無限次元ベクトル空間とシャウダー基底"},{"categories":"해석개론","contents":"정리1 この記事はリーマン・スティルチェス積分を基準に書かれています。$\\alpha=\\alpha (x)=x$とすると、リーマン積分と同じです。 $f$が$[a,b]$でリーマン（-スティルチェス）積分可能だとしましょう。すると、定数$c\\in \\mathbb{R}$に対して$cf$も$[a,b]$で積分可能であり、その値は以下の通りです。 $$ \\int_{a}^{b}cf d\\alpha = c\\int_{a}^{b}f d\\alpha $$\n二つの関数$f_{1}$、$f_{2}$が$[a,b]$でリーマン（-スティルチェス）積分可能であるとしましょう。すると、$f_{1}+f_{2}$も積分可能であり、その値は以下の通りです。 $$ \\int _{a} ^{b}(f_{1}+f_{2})d\\alpha = \\int _{a} ^{b} f_{1}d\\alpha + \\int_{a}^{b} f_{2} d\\alpha $$\n積分は線形であるということです。\n$$ \\int _{a} ^{b}(f_{1}+cf_{2})d\\alpha = \\int _{a} ^{b} f_{1}d\\alpha + c\\int_{a}^{b} f_{2} d\\alpha $$\nわざわざ加算と定数倍を別々に書いた理由は、証明を別々にするためです。\n補助定理 $[a,b]$でリーマン（-スティルチェス）積分可能な関数$f$と任意の正数$\\varepsilon\u0026gt; 0$に対して、以下の式を満たす$[a,b]$の分割$P$が存在します。\n$$ \\begin{align} U(P,f,\\alpha) \\lt \\int_{a}^{b}f d\\alpha +\\varepsilon \\tag{L1} \\\\ \\int_{a}^{b}f d\\alpha -\\varepsilon \\lt L(P,f,\\alpha) \\tag{L2} \\end{align} $$\n$U$、$L$はそれぞれリーマン（-スティルチェス）上積分、下積分です。\n証明 $\\eqref{L1}$ 任意の正数$\\varepsilon \\gt 0$が与えられたとします。すると、積分可能の必要十分条件により、以下の式を満たす分割$P$が存在します。\n$$ U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\nこの時$L(P,f,\\alpha) \\le \\displaystyle \\int_{a}^{b}fd\\alpha$なので、次が成立します。\n$$ U(P,f,\\alpha)-\\int_{a}^{b}f d\\alpha\\le U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\n従って、要約すると次のようになります。\n$$ U(P,f,\\alpha ) \\lt \\int_{a}^{b}f d\\alpha +\\varepsilon $$\n■\n$\\eqref{L2}$ 証明$\\eqref{L1}$でと同様に、次を満たす分割$P$が存在します。\n$$ U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\n$\\displaystyle \\int_{a}^{b}fd\\alpha \\le U(P,f,\\alpha)$なので、次が成立します。\n$$ \\int_{a}^{b}f d\\alpha-L(P,f,\\alpha)\\le U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\n従って、要約すると次のようになります。\n$$ \\int_{a}^{b}f d\\alpha -\\varepsilon \\lt L(P,f,\\alpha) $$\n■\n証明 $f_{1}, f_{2}, f$が積分可能の時、$f_{1}+f_{2}, cf$も積分可能であり、その値が実際に$\\displaystyle \\int f_{1} + \\int f_{2}, c\\int f$と同じであることを示します。\n1. Case 1. $c=0$\n$cf=0$が積分可能であることは自明です。また、次の等式が成立することも自明です。\n$$ \\int_{a}^{b}0fd\\alpha=0=0\\int_{a}^{b}fd\\alpha $$\nCase 2. $c\u0026gt;0$\n任意の正数$\\varepsilon \u0026gt;0$が与えられたとします。すると、積分可能の必要十分条件によって、次を満たす分割$P=\\left\\{ a=x_{0} \\lt \\cdots \\lt x_{i} \\lt \\cdots \\lt x_{n}=b\\right\\}$が存在します。\n$$ \\begin{equation} U(P,f,\\alpha) - L(P,f,\\alpha)\u0026lt;\\frac{\\varepsilon}{c} \\end{equation} $$\nそして、次のようにしましょう。\n$$ \\begin{align*} M_{i} \u0026amp;= \\sup _{[x_{i-1}, x_{i}]} f(x) \\\\ m_{i} \u0026amp;= \\inf _{[x_{i-1}, x_{i}]} f(x) \\\\ M_{i}^{c} \u0026amp;= \\sup _{[x_{i-1}, x_{i}]} cf(x) \\\\ m_{i}^{c} \u0026amp;= \\inf _{[x_{i-1}, x_{i}]} cf(x) \\end{align*} $$\nすると$c\u0026gt;0$なので$cM_{i} = M_{i}^{c}$であり、$cm_{i} = m_{i}^{c}$です。すると、リーマン（-スティルチェス）和の定義と$(1)$によって、次が成立します。\n$$ \\begin{align} U(P,cf,\\alpha)- L(P,cf,\\alpha) \u0026amp;= \\sum \\limits_{i=1}^{n}M_{i}^{c}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}m_{i}^{c}\\Delta \\alpha_{i} \\nonumber\\\\ \u0026amp;= \\sum \\limits_{i=1}^{n}cM_{i}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}cm_{i}\\Delta \\alpha_{i} \\nonumber\\\\ \u0026amp;= c\\left( \\sum \\limits_{i=1}^{n}M_{i}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}m_{i}\\Delta \\alpha_{i} \\right) \\nonumber\\\\ \u0026amp;= c\\Big[ U(P,f,\\alpha)-L(P,f,\\alpha)\\Big] \\nonumber\\\\ \u0026amp;\\lt \\varepsilon \\end{align} $$\n従って、積分可能の必要十分条件により、$cf$は積分可能です。 積分は上積分より小さいので、次が成立します。\n$$ c \\int_{a}^{b}fd \\alpha \\le cU(P,f,\\alpha) = U(P,cf,\\alpha) $$\nこれは、$(2)$と補助定理によって、次が成立します。\n$$ c\\int _{a}^{b}f d\\alpha \\le U(P,cf,\\alpha) lt \\int _{a}^{b} cf d\\alpha +\\varepsilon $$\nこの時、$\\varepsilon$は任意の正数と仮定したので、次が成立します。\n$$ \\begin{equation} c\\int_{a}^{b}fd\\alpha \\le \\int_{a}^{b}cfd\\alpha \\end{equation} $$\n反対方向の不等号を示す過程も似ています。$(1)$と補助定理によって、次が成立します。\n$$ cU(P,f,\\alpha) \\le c\\int_{a}^{b}fd\\alpha +\\varepsilon $$\nまた、次の式が成立します。\n$$ \\int_{a}^{b} cfd\\alpha \\le U(P,cf,\\alpha)=cU(P,f,\\alpha) $$\n上の二つの式から、下の式を得ます。\n$$ \\int_{a}^{b} cfd \\alpha \\le cU(P,f,\\alpha)\u0026lt; c\\int_{a}^{b}fd\\alpha +\\varepsilon $$\nこの時、$\\varepsilon$は任意の正数なので、次が成立します。\n$$ \\begin{equation} \\int_{a}^{b} cf d\\alpha \\le c\\int_{a}^{b}fd\\alpha \\end{equation} $$\n$(3)$と$(4)$によって、次が成立します。\n$$ \\int_{a}^{b}cfd\\alpha = c\\int_{a}^{b}fd\\alpha $$\nCase 3. $c=-1$\n証明の過程はCase 2. と似ています。まず、任意の正数$\\varepsilon$が与えられたとします。$f$は積分可能なので、積分可能の必要十分条件により、与えられた$\\varepsilon$に対して、次を満たす分割$P$が存在します。\n$$ U(P,f,\\alpha) - L(P,f,\\alpha) \u0026lt;\\varepsilon $$\n今、次のようにしましょう。\n$$ \\begin{align*} M_{i} \u0026amp;= \\sup _{[x_{i-1},x_{i}]}f \\\\ m_{i} \u0026amp;= \\inf_{[x_{i-1},x_{i}]}f \\\\ M_{i}^{\\ast} \u0026amp;= \\sup _{[x_{i-1},x_{i}]}(-f) \\\\ m_{i}^{\\ast} \u0026amp;= \\inf_{[x_{i-1},x_{i}]}(-f) \\end{align*} $$\nすると$M_{i}=-m_{i}^{\\ast}$であり、$m_{i}=-M_{i}^{\\ast}$です。従って$M_{i}-m_{i}=M_{i}^{\\ast}-m_{i}^{\\ast}$です。それゆえ、次が成立します。\n$$ \\begin{align*} U(P,-f,\\alpha)-L(P,-f,\\alpha) \u0026amp;= \\sum\\limits_{i=1}^{n}M_{i}^{\\ast}\\Delta \\alpha_{i}-\\sum\\limits_{i=1}^{n}m_{i}^{\\ast}\\Delta \\alpha_{i} \\\\ \u0026amp;= \\sum\\limits_{i=1}^{n}M_{i}\\Delta \\alpha_{i} - \\sum\\limits_{i=1}^{n}m_{i}\\Delta\\alpha_{i} \\\\ \u0026amp;= U(P,f,\\alpha) -L(P,f,\\alpha) \\\\ \u0026amp;\\lt \\varepsilon \\end{align*} $$\n従って、$-f$は積分可能です。\nCase 2. の証明と同様に、補助定理によって、次が成立します。\n$$ U(P,-f,\\alpha) \\lt \\int_{a}^{b}(-f)d\\alpha +\\varepsilon $$\nまた、次の式が成立します。\n$$ -\\int_{a}^{b}fd\\alpha\\le -L(P,f,\\alpha)=U(P,-f,\\alpha) \\lt \\int_{a}^{b}(-f)d\\alpha + \\varepsilon $$\nこの時、$\\varepsilon$は任意の正数なので、次が成立します。 $$ -\\int_{a}^{b}fd\\alpha \\le \\int_{a}^{b}(-f)d\\alpha $$\nそれから、補助定理によって、次の式が成立します。\n$$ \\int_{a}^{b}(-f)d\\alpha -\\varepsilon \\lt L(P,-f,\\alpha)=-U(P,f,\\alpha)\\le-\\int_{a}^{b}fd\\alpha $$\n$\\varepsilon$は任意の正数なので、次が成立します。\n$$ \\int_{a}^{b}(-f)d\\alpha \\le -\\int_{a}^{b}fd\\alpha $$\n従って、次を得ます。\n$$ \\int_{a}^{b}(-f)d\\alpha =-\\int_{a}^{b}fd\\alpha $$\nCase 4. $c \\lt 0 \\quad \\text{and} \\quad c\\ne -1$\nCase 2. と Case 3. によって成立します。\n■\n2. $f=f_{1}+f_{2}$としましょう。$P$を$[a,b]$の任意の分割とします。すると、リーマン（-スティルチェス）上積分、下積分の定義によって、次が成立します。\n$$ \\begin{equation} \\begin{aligned} L(P,f_{1},\\alpha) + L(P,f_{2},\\alpha)\u0026amp; \\le L(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f_{1},\\alpha) +U(P,f_{2},\\alpha) \\end{aligned} \\end{equation} $$\n任意の正数$\\varepsilon \u0026gt; 0$が与えられたとします。すると、積分可能の必要十分条件によって、次を満たす分割$P_{j}$が存在します。\n$$ U(P_{j},f_{j},\\alpha)-L(P_{j},f_{j},\\alpha)\u0026lt;\\varepsilon,\\quad (j=1,2) $$\n今、$P$を再び$P_{1}$と$P_{2}$の共通細分としましょう。すると、$(5)$によって、次が成立します。\n$$ \\begin{align*} U(P,f,\\alpha)-L(P,f,\\alpha) \u0026amp;\\le \\left[ U(P,f_{1},\\alpha)-L(P,f_{1},\\alpha) \\right] + \\left[ U(P,f_{2},\\alpha)-L(P,f_{2},\\alpha) \\right] \\\\ \u0026amp;\u0026lt; \\varepsilon \\end{align*} $$\n従って、積分可能の必要十分条件により、$f$は積分可能です。 それから、補助定理によって、下の式が成立します。\n$$ U(P,f_{j},\\alpha)\u0026lt;\\int _{a}^{b}f_{j}d\\alpha+\\varepsilon,\\quad (j=1,2) $$\nまた、定義によって積分より上積分が大きいため、次が成立します。\n$$ \\int_{a}^{b}fd\\alpha \\le U(P,f,\\alpha) $$\n上の式と$(5)$の三番目の不等式によって、次が成立します。\n$$ \\begin{align*} \\int_{a}^{b}fd\\alpha \u0026amp;\\le U(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f_{1},\\alpha)+U(P,f_{2},\\alpha) \\\\ \u0026amp;\u0026lt; \\int_{a}^{b}f_{1}d\\alpha +\\int_{a}^{b}f_{2}d\\alpha + 2\\varepsilon \\end{align*} $$\nこの時、$\\varepsilon$は任意の正数なので、次が成立します。\n$$ \\begin{equation} \\int_{a}^{b} fd\\alpha \\le \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha \\label{6} \\end{equation} $$\n反対方向の不等式が成立することを示せば、証明は完了です。積分可能な関数の定数倍も積分可能であることを上で示したので、$-f_{1}, -f_{2}$も積分可能であることがわかります。従って、これら二つの関数に対して上の過程を繰り返せば、下の式を得ます\n$$ \\int_{a}^{b}(-f)d\\alpha \\le \\int_{a}^{b}(-f_{1})d\\alpha + \\int_{a}^{b} (-f_{2})d\\alpha $$\nまた、$\\displaystyle \\int (-f)d\\alpha=-\\int fd\\alpha$なので、両辺に$-1$を掛けると、次を得ます。\n$$ \\begin{equation} \\int_{a}^{b}fd\\alpha \\ge \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha \\label{7} \\end{equation} $$\n従って、$(6)$と$(7)$によって、次を得ます。\n$$ \\int_{a}^{b}fd\\alpha = \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha $$\n■\nウォルター・ルーディン, 数学解析の原理 (第3版, 1976), p128-129\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1666,"permalink":"https://freshrimpsushi.github.io/jp/posts/1666/","tags":null,"title":"リーマン(-シュティールス)積分の線形性"},{"categories":"그래프이론","contents":"定義 平面グラフ グラフを平面に描いた時、エッジが重ならずに描けるなら、そのグラフを平面グラフと言う。\n説明 平面グラフが描かれると、平面上で区切られる領域をフェースFaceと呼ぶ。次の平面グラフ $K_{4}$ は四つのフェース $f_{1}, f_{2}, f_{3}, f_{4}$ を持ち、その中でも特にバウンドされていない $f_{4}$ を無限フェースInfinite Faceと呼ぶ。\n平面グラフはその名前の通り、適合する部分なしに平面に描けるグラフのことを言う。理解を深めるためには、平面グラフではないグラフが何かを見る方がいい。例えば、完全グラフ $K_{5}$ と二部グラフ $K_{3,3}$ は平面グラフではないが、どのようにしても重なるエッジがあるためだ。\n数学者なら当然、グラフが平面グラフになる条件が何かが気になるだろう。これについては、上の例で一般化された定理が知られている。\n定理 クラトフスキーの定理 1 グラフが平面グラフであることと、そのグラフのサブグラフが $K_{5}$ や $K_{3,3}$ とホメオモーフィックなものがないことは同値である。\n意義 簡単に言えば、$K_{5}$ に似ているものや$K_{3,3}$ に似ているものが含まれていれば平面グラフではなく、なければ平面グラフであることを保証してくれる定理だ。一見すると、この定理で平面グラフについて全てが解決されるように見えるが、グラフ理論は平面グラフを根に数多くの概念や問題を生み出してきた。最も簡単な一般化として、$k \\in \\mathbb{N}$ 個のエッジが重なることを許すことから始め、数学全般でよく見られるデュアル、点と線と面などを扱う証明幾何学の領域はもちろん、空間の構造自体に関心を持つ組み合わせ位相幾何学にも続く大旅行の始まりである。\nだからと言って、グラフを専攻していない人が平面グラフに関する様々な定理を覚えて証明に慣れる必要はないが、もっと広い数学の世界を見るために必ず知っておくべき概念だと言えるだろう。定義を見ればわかるが、そもそも平面グラフを理解するために広い数学的基盤が必要なわけではない。だから、少なくとも知っておこう。\n自閉症テスト 古いインターネットのミームの一つで、上のような問題を自閉症テストAutism Testと言う。問題では、水道と電力、ガスを三つの家に供給しなければならないが、その線が交差してはならないという制限をかけ、「難しいが可能だ」と言う。これを純粋な数学問題として考えれば、クラトフスキーの定理によって不可能だと反論できる。これらの切り抜きが\u0026rsquo;自閉症テスト\u0026rsquo;と呼ばれる理由は、\u0026lsquo;不可能なのに解答にこだわり、全ての可能性を検討する様子が自閉症に似ている\u0026rsquo;という種類の軽蔑的、侮辱的なジョークであるためだろう。\n数学問題でない形でアプローチすると、上のような解決策もある。実際、自閉症テストの健全な楽しみは、このように問題の条件を破る強引または斬新な反則にある。\nWilson. (1970). Introduction to Graph Theory: p2.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1565,"permalink":"https://freshrimpsushi.github.io/jp/posts/1565/","tags":null,"title":"平面グラフとクラトフスキーの定理"},{"categories":"상미분방정식","contents":"定義1 下の微分方程式を、関連ルジャンドル微分方程式という。\n$$ \\begin{equation} \\begin{aligned} \u0026amp;\u0026amp;(1-x^{2})\\frac{ d^{2}y }{ dx^{2} }-2x \\frac{dy}{dx}+\\left[ +l(l+1)-\\frac{m^{2}}{1-x^{2}} \\right]y =\u0026amp;\\ 0 \\\\ \\mathrm{or} \u0026amp;\u0026amp; \\frac{ d }{ dx } \\left[ (1-x^{2})y^{\\prime} \\right] +\\left[ l(l+1)-\\frac{m^{2}}{1-x^{2}} \\right]y =\u0026amp;\\ 0 \\end{aligned} \\label{1} \\end{equation} $$\n関連ルジャンドル微分方程式の解を$P_{l}^{m}(x)$として表記し、これを関連ルジャンドル多項式や一般化されたルジャンドル多項式という。\n$$ \\begin{align*} P_{l}^{m}(x)\u0026amp;= (1-x ^{2})^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} } P_{l}(x) \\\\ \u0026amp;=(1-x ^{2})^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} }\\left[ \\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^2-1)^{l}\\right] \\end{align*} $$\nここで、$P_{l}(x)$はルジャンドル多項式だ。 $m$の符号によって区分される場合、\n$$ P_{l}^{m}(x) = (1-x ^{2})^{\\frac{m}{2}} \\dfrac{1}{2^{l} l!} \\dfrac{d^{l+m}}{dx^{l+m}}(x^2-1)^{l} $$\n$$ P_{l}^{-m}=(-1)^{m}\\frac{(l-m)!}{(l+m)!}P_{l}^{m}(x) $$\n関連ルジャンドル多項式は、球座標系のラプラス方程式を解く際に出現する。ここで、定数$l$、$m$は、量子力学で量子数と関連している。\n解 $m=0$の場合は、ルジャンドル微分方程式だ。この場合の解を基に、$m\\ne 0$の場合の解も見つけることができる。まず、関連ルジャンドル微分方程式の解は定数$l$、$m$によって決まるので、以下のように表記しよう。\n$$ y=P_{l}^{m}(x) $$\nこれを$\\eqref{1}$に代入して整理すると以下のようになる。\n$$ \\begin{equation} \\frac{ d }{ dx }\\left[ (1-x^{2})\\frac{ d P_{l}^{m}(x)}{ dx } \\right]+\\left[ l(l+1)-\\frac{m^{2}}{1-x^{2}} \\right]P_{l}^{m}(x)=0 \\label{2} \\end{equation} $$\nそして、解が以下の形であると仮定しよう。\n$$ P_{l}^{m}(x)=(1-x^{2})^{\\frac{|m|}{2}}u(x) $$\n$x$を一度微分すると\n$$ \\frac{ d P_{l}^{m}(x)}{ d x }=-|m|x(1-x^{2})^{\\frac{|m|}{2}-1}u(x)+(1-x^{2})^{\\frac{|m|}{2}}u^{\\prime}(x) $$\nこれを$\\eqref{2}$の最初の項に代入して整理すると以下のようになる。\n$$ \\begin{align*} \\frac{ d }{ dx }\\left[ (1-x^{2})\\frac{ d P_{l}^{m}(x)}{ dx } \\right] =\u0026amp;\\ \\frac{ d }{ dx }\\left[ -|m|x(1-x^{2})^{\\frac{|m|}{2}}u(x)+(1-x^{2})^{\\frac{|m|}{2}+1}u^{\\prime}(x) \\right] \\\\ =\u0026amp;\\ -|m|(1-x^{2})^{\\frac{|m|}{2}}u(x)+|m|^{2}x^{2}(1-x^{2})^{\\frac{|m|}{2}-1}u(x) \\\\ \u0026amp; -|m|x(1-x^{2})^{\\frac{|m|}{2}}u^{\\prime}(x)-(|m|+2)x(1-x^{2})^{\\frac{|m|}{2}}u^{\\prime}(x) \\\\ \u0026amp; +(1-x^{2})^{\\frac{|m|}{2}+1}u^{\\prime \\prime}(x) \\\\ =\u0026amp;\\ (1-x^{2})^{\\frac{|m|}{2}+1}u^{\\prime \\prime}(x)-2(|m|+1)(1-x^{2})^{\\frac{|m|}{2}}u^{\\prime}(x) \\\\ \u0026amp; -[|m|(|m|+1)x^{2}-|m|] (1-x^{2})^{\\frac{|m|}{2}-1}u(x) \\end{align*} $$\n両辺に$\\dfrac{1}{(1-x^{2})^{|m|/2}}$を掛けると次のようになる。\n$$ \\begin{align*} \u0026amp;\\frac{1}{(1-x^{2})^{|m|/2}}\\frac{ d }{ dx }\\left[ (1-x^{2})\\frac{ d P_{l}^{m}(x)}{ dx } \\right] \\\\ =\u0026amp;\\ (1-x^{2})u^{\\prime \\prime}(x)-2(|m|+1)xu^{\\prime}(x) -[|m|(|m|+1)x^{2}-|m|] (1-x^{2})^{-1}u(x) \\end{align*} $$\nしたがって、$\\eqref{2}$の両辺に$\\dfrac{1}{(1-x^{2})^{|m|/2}}$を掛けると\n$$ \\begin{equation} \\begin{aligned} \u0026amp;(1-x^{2})u^{\\prime \\prime}(x)-2(|m|+1)xu^{\\prime}(x) \\\\ \u0026amp;-\\left( \\frac{|m|(|m|+1)x^{2}-|m|}{1-x^{2}}+l(l+1)-\\frac{m^{2}}{1-x^{2}}\\right)u(x)=0 \\end{aligned} \\label{1} \\end{equation} $$\n$u(x)$の係数を整理すると以下のようになる。\n$$ \\begin{align*} \u0026amp;\\frac{|m|(|m|+1)x^{2}-|m|}{1-x^{2}}+l(l+1)-\\frac{m^{2}}{1-x^{2}} \\\\ =\u0026amp;\\ \\frac{|m|(|m|+1)x^{2}-|m|+l(l+1)(1-x^{2})-m^{2}}{1-x^{2}} \\\\ =\u0026amp;\\ \\frac{-m^{2}(1-x^{2})-|m|(1-x^{2})+l(l+1)(1-x^{2})}{1-x^{2}} \\\\ =\u0026amp;\\ l(l+1)-m^{2}-|m| \\\\ =\u0026amp;\\ l(l+1)-|m|(|m|+1) \\end{align*} $$\n従って、$\\eqref{3}$は以下のような形で整理される。\n$$ \\begin{equation} (1-x^{2})\\frac{ d^{2} u }{ d x^{2} }-2(|m|+1)x\\frac{ d u}{ dx }+[l(l+1)-|m|(|m|+1)]u=0 \\label{4} \\end{equation} $$\n$m=0$であれば、実際にルジャンドル微分方程式になる。従って、$|m|=0$の場合の解は$P_{l}^{0}(x)=P_{l}(x)$である。今一度、$(4)$を$x$に関して微分してみよう。係数を整理すると以下の式を得る。\n$$ \\begin{equation} (1-x^{2}) \\frac{ d^{3} u }{ d x^{3} } -2[(|m|+1)+1]x\\frac{ d^{2} u}{ dx^{2} }+[l(l+1)-(|m|+1)(|m|+2)]\\frac{ d u}{ d x}=0 \\label{5} \\end{equation} $$\n再び、$\\eqref{5}$を$x$に関して微分して係数を整理すると以下の式を得る。\n$$ \\begin{equation} (1-x^{2}) \\frac{ d^{4} u }{ d x^{4} } -2[(|m|+2)+1]x\\frac{ d^{3} u}{ dx^{3} }+[l(l+1)-(|m|+2)(|m|+3)]\\frac{ d^{2} u}{ d x^{2}}=0 \\label{6} \\end{equation} $$\nこれらの式をよく見ると、$\\eqref{4}$を$u$に、$|m|$を$|m|+1$に置き換えたときに$\\eqref{5}$を得ることができることがわかる。$\\eqref{5}$で同様の方法で置換すると、$\\eqref{6}$を得る。これにより、$|m|=0$の場合の解は$P_{l}(x)$、$|m|=1$の場合の解は$\\dfrac{ d }{ d x }P_{l}(x)$、$|m|=2$の場合の解は$\\dfrac{d^{2}}{dx^{2}}P_{l}(x)$であることがわかる。したがって、これを一般化すると次のようになる。\n$$ u(x)=\\frac{d^{|m|}}{dx^{{|m|}}}P_{l}(x) $$\nしたがって、関連ルジャンドル多項式は以下のようになる。\n$$ \\begin{align*} P_{l}^{m}(x)\u0026amp;= (1-x ^{2})^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} } P_{l}(x) \\\\ \u0026amp;=(1-x ^{2})^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} }\\left[ \\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^2-1)^{l}\\right] \\end{align*} $$\n■\nMary L. Boas, 数理物理学(Mathematical Methods in the Physical Sciences, 最中峻輔 訳) (3rd Edition, 2008), p597-598\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1605,"permalink":"https://freshrimpsushi.github.io/jp/posts/1605/","tags":null,"title":"関連するルジャンドル微分方程式と多項式"},{"categories":"그래프이론","contents":"定義 1 グラフ $G$ が与えられたとする。\nエッジの有限 シーケンスを ウォークと呼び、以下のように表す。 $$ v_{0} v_{1} , v_{1} v_{2} , \\cdots , v_{m-1} v_{m} \\\\ v_{0} \\rightarrow v_{1} \\rightarrow v_{2} \\rightarrow \\cdots \\rightarrow v_{m-1} \\rightarrow v_{m} $$ ここで、$v_{0}$ を 始点、$v_{m}$ を 終点、$m$ を 長さと呼ぶ。 ウォークのエッジが全て異なる場合、トレイルと呼ぶ。 ウォークの頂点が全て異なる場合、パスと呼ぶ。 ウォークの始点と終点が同じ場合、閉じていると呼ぶ。 閉じたパスを サイクルと呼ぶ。 これらの概念は、有向グラフに対しても同様に定義でき、以下のように無限グラフに対しても定義できる。\n$G$が無限グラフだとする。以下の定義ではウォークはトレイル、パス、サイクルに置き換えられる。 6. (有限)ウォークは一般的なグラフのウォークと全く同じである。 7. 一方向無限ウォークは次のように定義される。 $$ v_{0} \\to v_{1} \\to v_{2} \\to \\cdots $$ 8. 双方向無限ウォークは次のように定義される。 $$ \\cdots \\to v_{-2} \\to v_{-1} \\to v_{0} \\to v_{1} \\to v_{2} \\to \\cdots $$\n説明 パスの定義において、始点と終点は例外である。つまり、$v_{0} = v_{m}$である場合にはパスとなり、したがって「閉じたパス」が存在することになる。また、パスとトレイルの定義においては、頂点が全て異なればエッジも全て異なるため、パスはトレイルでもある。\nサイクルは、簡単に言うと、グラフで見つけることができる「輪の形」で、グラフ理論全体で広く使用される概念である。例として、次の図を見ると、グラフで合計三つのサイクルを見つけることができる：\nまた、サイクルに関して次の定理が知られている。\n定理 有限グラフ $G$ の全ての頂点の 次数が $2$ 以上であれば、$G$ にはサイクルが含まれる。\n証明 $G$ がループやマルチプルエッジを持てば当然サイクルが存在するため、$G$は単純グラフと仮定する。\n任意の頂点 $v_{0} \\in V(G)$ 一つを選び、以下のようなパスを考える。 $$ v_{0} \\to v_{1} \\to v_{2} \\to \\cdots $$ 仮定により、全ての頂点の次数は $2$ 以上であるため、少なくとも二つの頂点と隣接している。したがって、$v_{i+1}$ は直前の頂点 $v_{i}$ に隣接している頂点の中から $v_{i-1}$ を除いた任意の頂点を選び続けることができる。しかし、$G$ は有限グラフであるため、いずれはすでにパスに含まれている頂点 $v_{k}$ を選ばなければならなくなる。そうすると、始点と終点が $v_{k}$ であるパス $v_{k} \\to \\cdots \\to v_{k}$ は、$G$においてサイクルとして存在することがわかる。\n■\nWilson. (1970). Introduction to Graph Theory: p27.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1528,"permalink":"https://freshrimpsushi.github.io/jp/posts/1528/","tags":null,"title":"グラフ理論における歩行、道、経路、サイクル"},{"categories":"그래프이론","contents":"定義 1 全ての頂点の次数が同じであるグラフをレギュラーグラフRegular Graphと言う。特に、全ての頂点の次数が$r$の場合、$r$-レギュラーグラフと言う。言い換えると、次を満たすグラフ$G$を$r$-レギュラーグラフと言う。 $$ \\deg (v) = r \\qquad , \\forall v \\in V(G) $$ $2$-レギュラーな連結グラフをサイクルと言う。 例 レギュラーグラフ ペーターセングラフ プラトニックグラフ：正多面体をグラフで表したもの。ペーターセングラフと同様に$3$-レギュラーグラフであり、以下の五つだけが存在する。\n完全グラフ：グラフの頂点数が$n$であれば、$(n-1)$-レギュラーグラフは完全グラフとなる。 サイクル サイクルは、最も単純な形のグラフであり、純粋なグラフ理論では大きな関心を集めている。もちろん、サイクルグラフ自体のことではなく、グラフ内でサイクルの形をした部分についての話である。[ NOTE: サイクルからたった一つのエッジを取り除いたグラフもパスと呼ばれる。] サイクルの形を直接見れば、なぜ$2$-レギュラーがサイクルと呼ばれるのかすぐに理解できる。\n一方、なぜサイクルが連結である必要があるのかについての例は次の通り。二つのコンポーネントで構成された次のグラフ$G = A \\cup B$は確かに$2$-レギュラーだが、二つのサイクルのユニオンとして表されるため、真のサイクルと呼ぶにはふさわしくないことがわかる。もちろん、$A$と$B$はそれぞれがサイクルである。\nWilson. (1970). Introduction to Graph Theory: p17.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1522,"permalink":"https://freshrimpsushi.github.io/jp/posts/1522/","tags":null,"title":"レギュラーグラフ"},{"categories":"그래프이론","contents":"定義 1 単純グラフ $G$ が与えられたとする。\n$E(G) = \\emptyset$ ならば、$G$ をヌルグラフという。 $E \\left( \\overline{G} \\right) = \\emptyset$ ならば、$G$ を完全グラフという。 説明 ヌルグラフとは、文字通り空のグラフを意味する。EmptyではなくNullという表現を使ったのは、実際に$G \\ne \\emptyset$ であっても、グラフとしての意味がないためである。例えば、次の図のように頂点だけが存在しているなら、それをグラフと呼ぶ理由はほとんどない。しかし、数学で$0$ という数が非常に重要であるように、ヌルグラフはグラフ理論全般で非常に頻繁に言及される。\n完全グラフは元のグラフ$G$ の補グラフ $\\overline{G} $ として定義される。$\\overline{G}$ がヌルグラフであることは、元のグラフ$G$ の全ての頂点が例外なく接続されていることを意味する。例えば、上のグラフの補グラフは以下の通りである。\n追加定義 特に、頂点の数が$n$ の完全グラフを$K_{n}$ と表記する場合もある。\n何らかのグラフのサブグラフとしての完全グラフは、クリークと呼ばれる。 グラフ$G$ のクリーク$K_{n}$ の中で最大の$n$ を、$G$ のクリーク数 $\\omega (G)$ と言い、 $G$ の補グラフ$\\overline{G}$ のクリーク数を、$G$ の独立数 $\\beta (G) := \\omega \\left( \\overline{G} \\right)$ と言う。 一方、向きがある完全グラフをトーナメントと呼ぶ。 Wilson. (1970). Graph TheoryのIntroduction: p17.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1520,"permalink":"https://freshrimpsushi.github.io/jp/posts/1520/","tags":null,"title":"ヌルグラフと完全グラフ"},{"categories":"분포이론","contents":"定義 1 $k, \\theta \u0026gt; 0$に対して、以下の確率密度関数を持つ連続確率分布$\\Gamma ( k , \\theta )$をガンマ分布Gamma Distributionと呼ぶ。 $$ f(x) = {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ - x / \\theta} \\qquad , x \u0026gt; 0 $$\n$\\Gamma$はガンマ関数を示す。 ガンマ分布の確率密度関数は$\\alpha , \\beta \u0026gt; 0$に対して、以下のようにも定義される。本質的には$\\theta = {{ 1 } \\over { \\beta }}$かの違いだけだ。 $$ f(x) = {{ \\beta^{\\alpha } } \\over { \\Gamma ( \\alpha ) }} x^{\\alpha - 1} e^{ - \\beta x} \\qquad , x \u0026gt; 0 $$ 基本性質 モーメント生成関数 [1]: $$m(t) = \\left( 1 - \\theta t\\right)^{-k} \\qquad , t \u0026lt; {{ 1 } \\over { \\theta }}$$ 平均と分散 [2]: $X \\sim \\Gamma ( \\alpha , \\beta )$ならば $$ \\begin{align*} E(X) =\u0026amp; k \\theta \\\\ \\text{Var} (X) =\u0026amp; k \\theta^{2} \\end{align*} $$ 十分統計量 [3]: ガンマ分布に従うランダムサンプル$\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\Gamma \\left( k, \\theta \\right)$が与えられているとする。 $\\left( k, \\theta \\right)$に対する十分統計量$T$は次の通り。 $$ T = \\left( \\prod_{i} X_{i}, \\sum_{i} X_{i} \\right) $$\n定理 スケーリング [a]: $X \\sim \\Gamma ( k , \\theta )$ならばスカラー$c \u0026gt; 0$に対して$c X \\sim \\Gamma ( k , c \\theta )$ ポアソン分布との関係 $$ \\int_{\\mu}^{\\infty} { { z^{k-1} e^{-z} } \\over { \\Gamma (k) } } dz = \\sum_{x=0}^{k-1} { { {\\mu}^{x} e^{-\\mu} } \\over {x!} } $$ 指数分布との関係 [c]: $$\\Gamma \\left(1, { 1 \\over \\lambda } \\right) \\iff \\text{exp} (\\lambda)$$ カイ二乗分布との関係 [d]: $$\\Gamma \\left( { r \\over 2 } , 2 \\right) \\iff \\chi ^2 (r)$$ ベータ分布導出 [e]: 2つの確率変数$X_{1},X_{2}$が独立であり、$X_{1} \\sim \\Gamma ( \\alpha_{1} , 1)$、$X_{2} \\sim \\Gamma ( \\alpha_{2} , 1)$とすると $$ {{ X_{1} } \\over { X_{1} + X_{2} }} \\sim \\text{beta} \\left( \\alpha_{1} , \\alpha_{2} \\right) $$ 説明 ガンマ分布は、ガンマ関数にちなんで名付けられた関数であり、その確率密度関数の積分が$1$になることはオイラー積分に由来する。直感的な意味を持つというよりは、統計学的に有用な特性が多いため人為的に導入された分布である。このような分布をサンプリング分布Sampling Distributionとも呼ばれるが、ガンマ分布は特有の形状のおかげで様々な分布へと姿を変え、多くの便利な特性を提供してくれる。\nベイズ理論 ベイジアンでは、ポアソン分布の共役事前分布として使用されることもある。\n証明 [1] $\\displaystyle t \u0026lt; {{ 1 } \\over { \\theta }}$の時、$\\displaystyle y := x {{ ( 1 - \\theta t ) } \\over { \\theta }}$と置くと$\\displaystyle dy = {{ ( 1 - \\theta t ) } \\over { \\theta }} dx$であるから $$ \\begin{align*} m(t) =\u0026amp; \\int_{0}^{\\infty} e^{tx} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ - x / \\theta} dx \\\\ =\u0026amp; \\int_{0}^{\\infty} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ x (t - 1 / \\theta) } dx \\\\ =\u0026amp; \\int_{0}^{\\infty} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ - x {{( 1 - \\theta t)} \\over {\\theta}} } dx \\\\ =\u0026amp; \\int_{0}^{\\infty} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} \\left( {{ y \\theta } \\over { 1 - \\theta t }} \\right)^{k - 1} e^{ - y } {{ \\theta } \\over { 1 - \\theta t }}dy \\\\ =\u0026amp; \\left( {{ 1 } \\over { 1 - \\theta t }} \\right)^{k } \\int_{0}^{\\infty} {{ \\theta^{k} } \\over { \\Gamma ( k ) \\theta^{k} }} y^{k-1} e^{ - y } dy \\end{align*} $$ オイラー積分により、$\\displaystyle \\int_{0}^{\\infty} {{ 1 } \\over { \\Gamma ( k ) }} y^{k-1} e^{ - y } dy = 1$である。 $$ m(t) = \\left( 1 - \\theta t\\right)^{-k} \\qquad , t \u0026lt; {{ 1 } \\over { \\theta }} $$\n■\n[2] 直接演繹する。\n■\n[3] 直接演繹する。\n[a] $X \\sim \\Gamma ( k , \\theta )$と$c \u0026gt;0$に対して$Y = c X$とすると $$ \\begin{align*} m_{X}(t) =\u0026amp; \\int_{0}^{\\infty} e^{tx} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ - x / \\theta} dx \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{tx} {{ c^{k} } \\over { \\Gamma ( k ) (c\\theta)^{k} }} x^{k - 1} e^{ - cx / c\\theta} dx \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{{{ t } \\over { c }} cx} {{ 1 } \\over { \\Gamma ( k ) (c\\theta)^{k} }} (cx)^{k - 1} e^{ - cx / c\\theta} c dx \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{{{ t } \\over { c }} y} {{ 1 } \\over { \\Gamma ( k ) (c\\theta)^{k} }} y^{k - 1} e^{ - y / c\\theta} dy \\end{align*} $$ [1]モーメント生成関数により $$ \\begin{align*} m_{Y}(t) =\u0026amp; E \\left( e^{tY} \\right) \\\\ =\u0026amp; E \\left( e^{tcX} \\right) \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{{{ tc } \\over { c }} y} {{ 1 } \\over { \\Gamma ( k ) (c\\theta)^{k} }} y^{k - 1} e^{ - y / c\\theta} dy \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{tz} {{ 1 } \\over { \\Gamma ( k ) (c\\theta)^{k} }} z^{k - 1} e^{ - z / c\\theta} dz \\\\ =\u0026amp; (1 - c \\theta)^{-k} \\end{align*} $$ それ故に$Y \\sim \\Gamma ( k , c \\theta)$である。\n■\nb 数学的帰納法で示す。\n■\n[c] モーメント生成関数で示す。\n■\n[d] モーメント生成関数で示す。\n■\nコード こちらは、ガンマ分布の確率密度関数をGIFアニメで表示するJuliaのコードです。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = 0:0.1:20\rΘ = collect(0.1:0.1:10.0); append!(Θ, reverse(Θ))\ranimation = @animate for θ ∈ Θ\rplot(x, pdf.(Gamma(1, θ), x),\rcolor = :black,\rlabel = \u0026#34;r = 1, θ = $(rpad(θ, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,} \\Gamma (1, \\theta)\u0026#34;)\rend\rgif(animation, \u0026#34;pdf1.gif\u0026#34;)\ranimation = @animate for θ ∈ Θ\rplot(x, pdf.(Gamma(2, θ), x),\rcolor = :black,\rlabel = \u0026#34;r = 2, θ = $(rpad(θ, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,} \\Gamma (2, \\theta)\u0026#34;)\rend\rgif(animation, \u0026#34;pdf2.gif\u0026#34;)\ranimation = @animate for θ ∈ Θ\rplot(x, pdf.(Gamma(4, θ), x),\rcolor = :black,\rlabel = \u0026#34;r = 4, θ = $(rpad(θ, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,} \\Gamma (4, \\theta)\u0026#34;)\rend\rgif(animation, \u0026#34;pdf4.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p158.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1517,"permalink":"https://freshrimpsushi.github.io/jp/posts/1517/","tags":null,"title":"ガンマ分布"},{"categories":"그래프이론","contents":"定義 1 二つのグラフ $G_{1}$ と $G_{2}$ に対して $V(G_{1}) \\cap V(G_{2}) = \\emptyset$ としよう。\n二つのグラフの ユニオンUnion $G = G_{1} \\cup G_{2}$ は、頂点セット $V(G_{1}) \\cup V(G_{2})$ とエッジセット $E (G_{1}) \\cup E ( G_{2} )$ を持つグラフだ。 グラフ $H$ が他のグラフのユニオンで表されない場合、$H$ を 接続されているConnectedといい、それ以外の場合は 切断されているDisconnectedという。 切断されたグラフを構成する各 接続グラフConnected Graphを コンポーネントComponentと呼ぶ。 特にエッジ $b \\in G$ の削除によりグラフが切断される場合、$b$ を ブリッジBridgeと呼ぶ。 説明 これらの定義は位相数学で接続性を定義する方法とよく似ている。\n純粋なグラフの定義からの 接続性Connectednessは重要な話のように見えるが、皮肉なことに、切断されたコンポーネントは完全に個別に扱うことができるので、接続されたグラフだけを考えれば十分だ。接続性が重要でないわけではなく、通常は研究のために切断されたケースを考える必要はないということだ。\n接続性が注目されるのは、実際のデータを反映した分析やランダムネットワークを扱う適用ネットワーク理論でよくある。ランダムネットワークが確かに接続グラフになるかどうかは、様々なシミュレーションなどでかなり重要な問題だ。ネットワークの接続性が保証されていない場合を考えよう。孤立ノードIsolated Nodeは、ほとんどの数学モデルでは影響力がなく、孤立ノードがなくても、ネットワークの一部だけを考慮して残りを捨てなければならない大惨事が起こる可能性がある。\n要約 2 シンプルグラフ $G$ が $n$ 個の頂点を持っているとする。$G$ が $k$ 個のコンポーネントを持っている場合、$G$ のエッジの数 $m$ は次を満たす。 $$ n-k \\le m \\le (n-k)(n-k+1)/2 $$\n証明 Part 1. $n-k \\le m$\n$G$ がヌルグラフの場合、$n=k$ であり、$m=0$ であるため、$ n-k = 0 \\le 0 = m$ が成立する。\n$G$ のコンポーネントが $k$ 個ある場合に $n - k \\ge m$ が成立すると仮定しよう。$G$ が $k$ 個のコンポーネントを持つための最少のエッジ数を $m_{0}$ とする。ここで一つのエッジを削除すると、コンポーネントの数は $k+1$ 、エッジの数は $m_{0}-1$ となる。したがって、$n - (k + 1) \\le m_{0} - 1$ であり、整理して $n - k \\le m_{0}$ を得る。\nこれら二つの事実から、数学的帰納法により $n-k \\le m$ が一般的に成立するという結論になる。\nPart 2. $m \\le (n-k)(n-k+1)/2$\n$G$ のすべてのコンポーネントが完全グラフであるとしよう。すると、$1 \\le j \\le i \\le n$ である二つの完全グラフ $K_{i}$、$K_{j}$ が存在するだろう。これら二つのグラフをそれぞれ $K_{i+1}$、$K_{j-1}$ に変えると、頂点の総数は変わらないが、エッジの総数は次のように変わる。 $$ \\left[ (i+1)i - i(i-1) \\right]/2 - \\left[ j(j-1) - (j-1)(j-2) \\right]/2 = i - j + 1 $$ これは正の数で、したがって、$m$ が最大化されるためには、$G$ は頂点の数が $(n-k+1)$ 個の完全グラフと、孤立頂点 $k-1$ 個を持たなければならない。この場合、$G$ のエッジの数は $(n-k)(n-(k-1))/2$ であり、求めていた結果を得る。\n■\nWilson. (1970). グラフ理論序論: p10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWilson. (1970). グラフ理論序論: p27.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1512,"permalink":"https://freshrimpsushi.github.io/jp/posts/1512/","tags":null,"title":"グラフの集合表記"},{"categories":"분포이론","contents":"定義 1 $\\lambda \u0026gt; 0$に対して、以下の確率密度関数を持つ連続確率分布$\\exp ( \\lambda)$を指数分布Exponential Distributionと呼ぶ。 $$ f(x) = \\lambda e^{-\\lambda x} \\qquad , x \\ge 0 $$\n本によっては、パラメーターがその逆数 $\\displaystyle \\theta = {{ 1 } \\over { \\lambda }}$ を使うこともある。 基本性質 モーメント生成関数 [1]: $$m(t) = {{ \\lambda } \\over { \\lambda - t }} \\qquad , t \u0026lt; \\lambda$$ 平均と分散 [2]: $X \\sim \\exp ( \\lambda)$の場合 $$ \\begin{align*} E(X) =\u0026amp; {{ 1 } \\over { \\lambda }} \\\\ \\text{Var} (X) =\u0026amp; {{ 1 } \\over { \\lambda^{2} }} \\end{align*} $$ 十分統計量と最尤推定量 [3]: ランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\exp \\left( \\lambda \\right)$が与えられたとする。 $\\lambda$に対する十分統計量$T$と最尤推定量$\\hat{\\lambda}$は以下のとおりである。 $$ \\begin{align*} T =\u0026amp; \\sum_{k=1}^{n} X_{k} \\\\ \\hat{\\lambda} =\u0026amp; {{ n } \\over { \\sum_{k=1}^{n} X_{k} }} \\end{align*} $$\n定理 無記憶性 [a]: $X \\sim \\exp ( \\lambda ) $の場合 $$ P ( X \\ge s + t \\mid X \\ge s ) = P (X \\ge t) $$ ガンマ分布との関係 [b]: $$\\Gamma \\left(1, { 1 \\over \\lambda } \\right) \\iff \\text{exp} (\\lambda)$$ ワイブル分布への一般化 $$ f(x) = {{ k } \\over { \\theta }} \\left( {{ x } \\over { \\theta }} \\right)^{k-1} e^{-(x/\\theta)^{k}} \\qquad , x \\ge 0 $$ 説明 幾何分布との関係 指数分布は、注目する事象が発生するまでの時間が従う分布で、幾何分布の連続化とも見なせる。幾何分布の発生回数に対する一般化として負の二項分布を考えることができるが、指数分布の発生回数に対する一般化はガンマ分布とも言えるだろう。\nポアソン分布との関係 一方、ポアソン分布と指数分布は似た現象に注目しているが、それぞれ単位時間あたりの事象の発生回数、事象が発生するまでの時間に関心があるという違いがある。この二つの分布の関係は、本の中にはこれら二つの分布で同じギリシャ文字$\\lambda$を使うこともある理由である。特に、ポアソン分布の平均が$\\lambda$、指数分布の平均が$\\displaystyle {{ 1 } \\over { \\lambda }}$であることを考えると、二つの分布の関係はある種の「逆」のように受け取ることができるだろう。\n証明 [1] $t \u0026lt; \\lambda$の時のみ $$ \\begin{align*} m(t) =\u0026amp; \\int_{0}^{\\infty} e^{tx} f(x) dx \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{tx} \\lambda e^{-\\lambda x} dx \\\\ =\u0026amp; \\lambda \\int_{0}^{\\infty} e^{(t - \\lambda ) x} dx \\\\ =\u0026amp; \\lambda {{ 1 } \\over { t - \\lambda }} [ 0 - 1 ] \\\\ =\u0026amp; {{ \\lambda } \\over { \\lambda - t }} \\end{align*} $$\n■\n[2] 直接導出する。\n■\n[3] 直接導出する。\n■\n[a] 条件付き確率で導出する。\n■\n[b] モーメント生成関数で示す。\n■\nc 確率密度関数から明らかである。\n■\n可視化 以下は、指数分布の確率密度関数をアニメーションGIFで示すJuliaのコードです。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = 0:0.1:10\rΛ = collect(0.1:0.1:5.0); append!(Λ, reverse(Λ))\ranimation = @animate for λ ∈ Λ\rplot(x, pdf.(Exponential(λ), x),\rcolor = :black,\rlabel = \u0026#34;λ = $(round(λ, digits = 2))\u0026#34;, size = (400,300))\rxlims!(0,10); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pdf\\,of\\,} \\exp(\\lambda)\u0026#34;)\rend\rgif(animation, \u0026#34;pdf.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p159.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1510,"permalink":"https://freshrimpsushi.github.io/jp/posts/1510/","tags":null,"title":"指数分布"},{"categories":"동역학","contents":"定義 1 フロー 空間$X$と関数$f : X \\to X$について、次のようなベクター場が微分方程式として与えられているとしよう。 $$ \\dot{x} = f(x) $$ 時間変数$t$と初期値$x_{0}$に対する自律微分方程式の解をフローと呼び、$F(t, x_{0})$のように表す。固定された単位時間$t = T$に対して、$F_{T}(x) := F(T,x)$をタイム-$T$マップと呼ぶ。\nタイムエボリューション 通常、一つの座標のみを残すプロジェクション$P : X \\to \\mathbb{R}^{1}$について、$P \\left( F \\left( t, x_{0} \\right) \\right)$を時間$t$の関数として見た場合、これをタイムエボリューションとも呼ぶ。\n説明 フローは軌跡または相空間とも呼ばれる。[ 注: 数学全般で言及される相空間とは同音異義語で、概念的には大きな関連性はない。 ]\nその定義から、フロー$F$は初期値$x_{0}$を固定して$t$に従った変化を描写することが分かる。タイム-$T$マップはもともと微分方程式で表され、連続的な動力系をマップで扱うために導入された。これにより、多次元マップでの議論を微分方程式に拡張することができるようになる。\n例 例として$\\dot{x} = x$という単純な自律システムを考えてみよう：このシステムの解は単純に$x = x_{0} e^{t}$であるため、このシステムのフローは初期値$x_{0}$に対して$F(t,x_{0}) = x_{0} e^{t}$となるだろう。一方、初期値を固定せずに、$x$から始まるシステムが時間$T$が経過したとき、タイム-$T$マップによって確認される。タイム-$T$マップは以下のように$x$を時間$T$が経過した後の$x e^{T}$にマッピングする。 $$ F_{T} : x \\mapsto x e^{T} $$ 動力学で広く使われる表現ではないかもしれないが、一般的な多次元マップのように表現したい場合、次のような式を立てることができる。 $$ F_{T+1} (x) = F_{1} \\left( F_{T}(x) \\right) $$\nYorke. (1996). CHAOS: An Introduction to Dynamical Systems: p277.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1507,"permalink":"https://freshrimpsushi.github.io/jp/posts/1507/","tags":null,"title":"自律システムのフローとタイム-Tマップ"},{"categories":"동역학","contents":"定義 1 空間 $V$ と関数 $f : V \\to V$ に対し、次のようなベクトル場が微分方程式として与えられているとしよう。 $$ \\dot{v} = f(v) $$\n変数 $t$ を含む微分方程式で、$t$ が明示的に示されていない場合、自律微分方程式Automonous Differential Equationと言う。 定数関数 $f_{0} (v)$ が自律微分方程式 $\\dot{v} = f(v)$ の解である場合、$f_{0}$ を平衡点Equilbriumと言う。 説明 自律システム 自律微分方程式で表される力学系を自律システムAutonomous Systemと言う。幾何学的にはほとんどがベクトル場で表されるため、適切な文脈では単にベクトル場とも呼ばれる。通常、変数 $t$ は時間を意味し、方程式が変数 $t$ を含みながら明示的に示されていないことは、例えば、以下のような式を指す。 $$ \\dot{y} = y $$ 上記の微分方程式の自明でない解は$y = e^t$である。なぜ自律という言葉が付くかは、非自律微分方程式を考えれば理解できる。非自律微分方程式は、名前の通り、変数 $t$ が微分方程式に明示的に示される微分方程式を言う。例えば、以下のように項 $\\sin t$ が追加されたものなどだ。 $$ \\dot{y} = y + \\sin t $$ このような微分方程式で表されるシステムは、$y$ そのものではなく、時間 $t$ に従って外部から何らかの干渉を受けると見なされる。この意味で、非自律微分方程式でない方程式を自律微分方程式と呼ぶのは適切だと思える。\n固定点 平衡点は物理学のセンスが強く、数学では単に固定点Fixed Pointという表現が好まれる。システムで固定点とは、その名の通り動かない点である。点が動かないということは、位置の変化量を示す微分係数がすべて$0$であり、固定点である限り定数関数である。厳密な表現では、関数が定義された定義域$X$ではなく、微分方程式の解が構成する関数空間$C^{1} (X)$の一元、つまり関数としての固定点であるが、教科書によっては、ゆるく$X$の一元が固定点と呼ばれることもある。\n微分方程式の記法 微分幾何学での$s$に対する微分と$t$に対する微分の記法: $$ {{ df } \\over { ds }} = f^{\\prime} \\quad \\text{and} \\quad {{ df } \\over { dt }} = \\dot{f} $$ ドット$\\dot{}$でもプライム$'$でも、微分は微分だが、微分幾何学の文脈では以上のように記号を区別できる。通常、$s$は単位速度曲線のパラメータであり、$t = t(s)$は線の長さの再パラメータ化を経た曲線のパラメータを表す。\n必ずそうである必要はないが、ダイナミクスでは、時間$t$に対する変化としてベクトル場を扱うため、プライム$y '$の代わりにドット$\\dot{y}$を多用することがある。\n例 例としてローレンツアトラクタを考えてみよう： $$ \\begin{cases} \\dot{x} = - \\sigma x + \\sigma y \\\\ \\dot{y} = - xz + \\rho x - y \\\\ \\dot{z} = xy - \\beta z \\end{cases} $$ 固定点は、ドメイン$\\mathbb{R}^3$上で動かない点を描写しているので、すべての左辺に$0$を代入することによって得られる。 $$ \\begin{cases} \\displaystyle 0 = - \\sigma x + \\sigma y \\\\ \\displaystyle 0 = - xz + \\rho x - y \\\\ \\displaystyle 0 = xy - \\beta z \\end{cases} $$ 簡単な計算を通じて、次の三つの固定点$F_{i}$を見つけ出すことができる。 $$ F_{1} = F_{1}(t) = (0,0,0) \\\\ F_{2} = F_{2}(t) = \\left( \\sqrt{\\beta (\\rho - 1)},\\sqrt{\\beta (\\rho - 1)}, (\\rho-1) \\right) \\\\ F_{3} = F_{3}(t) = \\left( -\\sqrt{\\beta (\\rho - 1)},-\\sqrt{\\beta (\\rho - 1)}, (\\rho-1) \\right) $$ ここで、$F_{i} = F_{i} (t)$のように関数として表されていることに注意せよ。一見、$F_{i}$は$\\mathbb{R}^{3}$上の点のように見えるが、定義によれば、時間$t$に従って値が変わらない定数関数であり、ローレンツ微分方程式の解として得られたものだ。概念的には、三次元空間の点と違いはない。\n参照 マップで表される力学系 微分方程式で表される力学系 力学系の厳密な定義 Yorke. (1996). CHAOS: An Introduction to Dynamical Systems: p271~277.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1505,"permalink":"https://freshrimpsushi.github.io/jp/posts/1505/","tags":null,"title":"微分方程式で表される動力学系と平衡点"},{"categories":"그래프이론","contents":"定義 1 有向グラフ $G$が与えられているとしよう。\nエッジ $vw$が存在する場合、エッジは$v$から出て$w$に入ると言われる。\n頂点 $v$に入るエッジの数を入力次数Indegreeと呼び、$\\deg^{-} (v)$として表される。 頂点 $v$から出るエッジの数を出力次数Outdegreeと呼び、$\\deg^{+}(v)$として表される。 $\\deg^{-} (v) = 0$である頂点をソースSource、$\\deg^{+} (v) = 0$である頂点をシンクSinkと言う。ソースでもシンクでもない点をインターナルInternalと呼ぶ。 エッジ $vw$が存在する場合、頂点 $v,w$はエッジ $vw$に接続されているincidentと言われる。\n頂点 $v$に接続されているエッジの数を次数Degreeと呼び、$\\deg (v)$として表される。 次数が$0$である頂点を孤立頂点Isolated Vertexと言う。 次数が$1$である頂点をエンド頂点End Vertexと言う。 グラフ$G$の最大次数を$\\Delta (G)$、最小次数を$\\delta (G)$として表現する。\n説明 ちなみに、接続されているという表現はそれほど好ましい言い回しではない。二つの頂点 $v,w$がエッジ$e$によって繋がれている場合、$v$と$w$は隣接しているAdjacentと表現され、接続されているという言葉はエッジ$e$に接続されている$v,w$を描写するために使われる。もともと形容詞のIncidentは、付随する、従うなどの意味で使われる英単語だ。\n次数の概念は、長い間多くの関心を集めてきたが、特に純粋数学ではシンプルなグラフがよく扱われるため、次数に関する研究が多い。\n有向グラフ 例えば、次のグラフでは、赤色が入力次数、青色が出力次数を示す。もちろん、入力次数と出力次数の合計は同じである。\nここで、入力次数が$0$の頂点をソースと言う。入るものがなく、出るだけである点から、ソースという名前は適切だと考えられる。これは動力学でのシンク、ソースと似ている。\n次数 例えば、次のグラフで、各頂点の次数を計算することができる。有向グラフを単純なグラフとして表した場合、入力次数と出力次数の合計と同じであることを確認しよう。[ 注: このように方向性をなくしたグラフは、元の有向グラフの基底グラフUnderlying Graphと言われる。]\n次のグラフでは、頂点$C$と$F$はエンド頂点、$H$は孤立頂点だ。\n応用数学では、$H$がなければ、つまりネットワークが完全に接続されている場合、$D$のようなノードをハブと呼ぶ。通常、ハブはネットワーク全体に大きな影響を与えるか、すべての通信を中継することができる要素を描写する。\nレギュラーグラフ 特に、すべての頂点の次数が同じであるグラフ、$\\delta (G) = \\Delta (G)$と表示される、をレギュラーグラフと呼ぶ。\nWilson. (1970). グラフ理論入門: p12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1496,"permalink":"https://freshrimpsushi.github.io/jp/posts/1496/","tags":null,"title":"グラフ理論における次数"},{"categories":"정수론","contents":"定義 1 以下のように定義された算術関数 $I$ をアイデンティティ関数と言う。 $$ I(n) := \\left[ {{ 1 } \\over { n }} \\right] $$\n[1] アイデンティティ級数：単位関数 $u$ である。つまり、 $$ \\sum_{d \\mid n}I(d) = u(n) = 1 $$ [2] 完全乗算的：すべての $n , m \\in \\mathbb{N}$ に対して $I (mn) = I(m) I(n)$ [a] 畳み込みにおける単位元：すべての算術関数 $f$ に対して $$ I \\ast\\ f = f \\ast\\ I = f $$ $\\left[ x \\right] = \\lceil x \\rceil$ は床関数Floor function と呼ばれ、$x$ より小さくまたは等しい値の中で最大の整数を表す。 説明 $$ \\begin{matrix} n \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \u0026amp; 8 \u0026amp; 9 \u0026amp; 10 \\\\ I(n) \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\sum_{d \\mid n} I(d) \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\end{matrix} $$ ほとんどの数学では、アイデンティティ関数の名前は$i(x) = x$ のように定義域の要素が自分自身にマッピングされる関数に付けられるが、少なくとも解析的整数論ではノルム $N (n) = n$ と呼ばれる。$I$ は見ての通り、畳み込み $\\ast$ に対して常に存在する単位元の役割を果たすため、アイデンティティという名前が付けられた。\n証明 [1] $\\displaystyle I(n) = \\left[ {{ 1 } \\over { n }} \\right] = \\begin{cases} 1 \u0026amp; , n=1 \\\\ 0 \u0026amp;, n\u0026gt;1 \\end{cases}$ が成り立つ。したがって、 $$ \\sum_{d \\mid n}I(d) = 1 + 0 + \\cdots = 1 $$\n■\n[2] ケース 1. $m = n = 1$ $$ I ( mn ) = I(1) = 1 = 1 \\cdot 1 = I(1) I(1) = I(m) I(n) $$ ケース 2. $m = 1 \\land n \u0026gt; 1$ $$ I(mn) = I (n) = 1 \\cdot I (n) = I(m) I(n) $$ ケース 3. $m \u0026gt; 1 \\land n = 1$ $$ I(mn) = I (m) = I(m) \\cdot 1 = I(m) I(n) $$ ケース 4. $m \u0026gt; 1 \\land n \u0026gt; 1$ $$ I(mn) = 0 = 0 \\cdot 0 = I(m) I(n) $$ ■\n[a] $d$ は $n$ の約数であるため、$d \\ne n$ の場合には$\\displaystyle \\left[ {{ d } \\over { n }} \\right] = 0$ であり、 $$ (f \\ast\\ I)(n) = \\sum_{d \\mid n} f(d) I \\left( {{ n } \\over { d }} \\right) = \\sum_{d \\mid n} f(d) \\left[ {{ d } \\over { n }} \\right] = f(n) $$ 算術関数の畳み込みの交換法則により、$f \\ast\\ I = I \\ast\\ f = f$\n■\nApostol. (1976). Introduction to Analytic Number Theory: p30.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1490,"permalink":"https://freshrimpsushi.github.io/jp/posts/1490/","tags":null,"title":"ディリクレ積に関する恒等式"},{"categories":"정수론","contents":"定義 1 二つの算術関数$f$、$g$に対し、以下を満たす算術関数$h$を$f$と$g$のディリクレ積と呼ぶ。 $$ h(n) = \\sum_{d \\mid n} f(d) g \\left( {{ n } \\over { d }} \\right) $$ ディリクレ積は$h (n) = \\left( f \\ast g \\right) (n) $や$h = f \\ast g$として表現される。\n説明 ディリクレ積は、その形から推測できるように、畳み込みとも呼ばれる。この定義で算術関数を単に$a_{d}$、$b_{n/d}$と記述することは非常に不便であると想像できる。\n畳み込み$\\ast$について、算術関数の集合は、次のような基本的な代数的性質を持つ。解析的整数論に興味があるならば、２項演算$\\ast$を算術関数の集合$A$に適用して得られるアーベル群$(A,*)$を思い浮かべることが自然であろう。残念ながら、正確な答えは「いいえ」だが、もっと適切な条件を提供することで、アーベル群を形成することができる。\n一方で畳み込みは、掛け合わされる二つの関数のうち一方が算術関数でなくてもよいように一般化される。\n基本性質 [1] 結合法則: $$ \\left( f \\ast g \\right) \\ast k = f \\ast (g \\ast k) $$ [2] 交換法則: $$ f \\ast g = g \\ast f $$ 証明 [1] $A = f \\ast g$、$B := (g \\ast k)$とすると $$ \\begin{align*} \\left( f \\ast g \\right) \\ast k =\u0026amp; A \\ast k \\\\ =\u0026amp; \\sum_{cm = n} A(m) k(c) \\\\ =\u0026amp; \\sum_{cm=n} \\left[ \\sum_{ab=m} f(a) g(b) \\right] k(c) \\\\ =\u0026amp; \\sum_{abc=n} f(a) g(b) k(c) \\\\ =\u0026amp; \\sum_{am=n} f(a) \\left[ \\sum_{bc=m} g(b) k(c) \\right] \\\\ =\u0026amp; \\sum_{am=n} f(a) B(m) \\\\ =\u0026amp; f \\ast B \\\\ =\u0026amp; f \\ast (g \\ast k) \\end{align*} $$\n■\n[2] $$ \\begin{align*} \\left( f \\ast g \\right)(n) =\u0026amp; \\sum_{d \\mid n} f(d) g \\left( {{ n } \\over { d }} \\right) \\\\ =\u0026amp; \\sum_{ab=n} f(a) g(b) \\\\ =\u0026amp; \\sum_{ab=n} g(b) f(a) \\\\ =\u0026amp; \\sum_{d \\mid n} g(d) f\\left( {{ n } \\over { d }} \\right) \\\\ =\u0026amp; (g \\ast f)(n) \\end{align*} $$\n■\n一般化 一般化されたディリクレ積 アポストル. (1976). 解析的整数論入門: p29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1488,"permalink":"https://freshrimpsushi.github.io/jp/posts/1488/","tags":null,"title":"算術関数のディリクレ積"},{"categories":"정수론","contents":"定義 1 定義域が自然数の集合$\\mathbb{N}$であり、値域が実数の集合$\\mathbb{R}$または複素数の集合$\\mathbb{C}$である関数を算術関数という。\n説明 解析的整数論では、様々な算術関数の性質や関係に関心を持ち、以下のような例がある：\n恒等関数 $I$ 約数関数 $\\sigma_{\\alpha}$ ノルム $N$ 約数関数 $\\sigma_{\\alpha}$ メビウス関数 $\\mu$ オイラーのトーティエント関数 $\\varphi$ 単位関数 $u$ マンゴルト関数 $\\Lambda$ リュービル関数 $\\lambda$ 算術関数の定義に新しさはなく、実質的には数列そのものだ。実際、数列はもともと関数であるが、数学の多くの分野ではその用語自体が関数と区別されて使われることが普通である。しかし、（解析的）整数論では扱うものが自然数であるため、定義域として$\\mathbb{N}$または$\\mathbb{Z}$があれば十分であり、数列と関数を区別する理由がほとんどなくなる。ただし、より関数に近い形で扱われるため、算術関数という用語が使われる。形式的には、定義域がベクトル場ではないが、値域が$\\mathbb{R}$または$\\mathbb{C}$である点が汎関数と似ている。\nまた、算術関数の級数にも関心がある。例えば、与えられた算術関数$f$に対して$\\displaystyle F(n) = \\sum_{d \\mid n} f(d)$を求めることだ。$\\displaystyle \\sum_{d \\mid n}$は$n$の全ての約数$d$に対して計算を行うもので、一般的な解析学で$\\displaystyle \\sum_{k=1}^{\\infty}$を計算することと似た感覚で理解できる。\nApostol. (1976). Introduction to Analytic Number Theory: p24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1487,"permalink":"https://freshrimpsushi.github.io/jp/posts/1487/","tags":null,"title":"解析的整数論における算術関数"},{"categories":"줄리아","contents":"このポストの時点でのJuliaの最新バージョンはv1.3.1です。\nガイド ステップ1. Juliaのダウンロード Generic Linux Binaries for x86から自分のCPUのビットに合ったファイルをダウンロードする。\nステップ2. 圧縮を解除して移動 圧縮を解除する。\nJuliaが保存される場所へフォルダを移動する。どこでも好きな場所で構わないが、このポストでは/home/[ユーザー名]/julia-1.3.1へ移動させた。\nステップ3. シンボリックリンク 次のコマンドを使ってシンボリックリンクを作成する。\nsudo ln -s /home/[유저이름]/julia-1.3.1/bin/julia /usr/bin/julia Juliaコマンドを使って実行すれば、1.3.1バージョンが正常にインストールされたことを確認できる。\n最新バージョンが必要でない場合 sudo apt-get install julia 上のコマンドを使って、シンボリックリンクを設定することなく素早くインストールすることもできる。ただし、この方法では最新の安定版がインストールされない。\n環境 OS: Ubuntu 18.04 ","id":1511,"permalink":"https://freshrimpsushi.github.io/jp/posts/1511/","tags":null,"title":"LinuxでJuliaの最新バージョンをインストールする方法"},{"categories":"분포이론","contents":"定義 1 $p \\in (0,1]$に対して、次のような確率質量関数を持つ離散確率分布 $\\text{Geo}(p)$を幾何分布Geometric Distributionと呼ぶ。 $$ p(x) = p (1 - p)^{x-1} \\qquad , x = 1 , 2, 3, \\cdots $$\n二つの定義が使用されているので、公式と定義域に特に注意が必要である。 基本性質 モーメント生成関数 [1]: $$m(t) = {{ p e^{t} } \\over { 1 - (1-p) e^{t} }} \\qquad , t \u0026lt; -\\log (1-p)$$ 平均と分散 [2]: $X \\sim \\text{Geo} (p)$なら $$ \\begin{align*} E(X) =\u0026amp; {{ 1 } \\over { p }} \\\\ \\text{Var}(X) =\u0026amp; {{ 1-p } \\over { p^{2} }} \\end{align*} $$ 十分統計量と最尤推定量 [3]: ランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\text{Geo} \\left( p \\right)$が与えられたとする。$p$に対する十分統計量 $T$と最尤推定量 $\\hat{p}$は以下の通りである。 $$ \\begin{align*} T =\u0026amp; \\sum_{k=1}^{n} X_{k} \\\\ \\hat{p} =\u0026amp; {{ n } \\over { \\sum_{k=1}^{n} X_{k} }} \\end{align*} $$ 定理 無記憶性 [a]: $X \\sim \\text{Geo} (p)$なら $$ P(X \\ge s+ t ,|, X \\ge s) = P(X \\ge t) $$ 幾何分布への一般化 [b]: $Y = X_{1} + \\cdots + X_{r}$であり$X_{i} \\overset{\\text{iid}}{\\sim} \\text{Geo}(p)$なら$Y \\sim \\text{NB}(r,p)$ 解説 指数分布との関係 幾何分布は、確率$0 \u0026lt; p \\le 1$で成功するまでの試行回数に関心を持っている。その確率質量関数は、確率$(1-p)$で$x-1$回失敗した後、最後に確率$p$で成功する確率を表している。この特性により、指数分布の離散化と見ることができる。\n名称 確率質量関数が幾何級数の形をしているため、この分布が幾何分布と呼ばれる。$a := p$、$r := (1-p)$と置くと、$p(x) = a r ^{x-1}$の馴染みのある式を得る。実際にモーメント生成関数を求めるときも、幾何級数の公式が登場する。\n証明 [1] $$ \\begin{align*} M(t) =\u0026amp; \\sum_{x=1}^{\\infty} e^{tx} p(x) \\\\ =\u0026amp; \\sum_{x=1}^{\\infty} e^{tx} p (1-p)^{x-1} \\\\ =\u0026amp; p e^{t} \\sum_{x=1}^{\\infty} \\left[ e^{t}(1-p) \\right]^{x-1} \\end{align*} $$ $ t \u0026lt; -\\log (1-p)$のとき、幾何級数の公式により $$ p e^{t} \\sum_{x=1}^{\\infty} \\left[ e^{t}(1-p) \\right]^{x-1} = {{ p e^{t} } \\over { 1 - (1-p) e^{t} }} $$\n■\n[2] 二つの方法がある。\n■\n[3] 直接演繹する。\n■\n[a] 条件付き確率で演繹する。\n■\n[b] モーメント生成関数で演繹する。\n■\nコード 次は、幾何分布の確率質量関数をGIFで表示するJuliaのコードだ。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = 0:20\rP = collect(0.01:0.01:0.5); append!(P, reverse(P))\ranimation = @animate for p ∈ P\rscatter(x, pdf.(Geometric(p), x),\rcolor = :black, markerstrokecolor = :black,\rlabel = \u0026#34;p = $(rpad(p, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.3); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Geo}(p)\u0026#34;)\rend\rgif(animation, \u0026#34;pmf.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p145.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1486,"permalink":"https://freshrimpsushi.github.io/jp/posts/1486/","tags":null,"title":"幾何分布"},{"categories":"분포이론","contents":"定義 1 $n \\in \\mathbb{N}$ と $p \\in [0,1]$ に対して以下の確率質量関数を有する離散確率分布 $\\text{Bin}(n,p)$ を 二項分布Binomial Distributionと呼ぶ。 $$ p(x) = \\binom{n}{x} p^{x} (1-p)^{n-x} \\qquad , x = 0 , 1, \\cdots n $$\n基本性質 積率母関数 [1]: $$m(t) = \\left[ (1-p) + pe^{t} \\right]^{n} \\qquad , t \\in \\mathbb{R}$$ 平均と分散 [2]: もし $X \\sim \\text{Bin}(n,p)$ ならば $$ \\begin{align*} E(X) =\u0026amp; np \\\\ \\text{Var}(X) =\u0026amp; np(1-p) \\end{align*} $$ 定理 二項分布の極限分布としてのポアソン分布導出 [a]: $X_{n} \\sim B(n,p)$ とする。もし $\\mu \\approx np$ ならば $$ X_{n} \\overset{D}{\\to} \\text{Poi} (\\mu) $$ 二項分布の極限分布としての標準正規分布導出 [b]: もし $X_i \\sim B(1,p)$ で $Y_n = X_1 + X_2 + \\cdots + X_n$ ならば $Y_n \\sim B(n,p)$ で $$ { { Y_n - np } \\over {\\sqrt{ np(1-p) } } }\\overset{D}{\\to} N(0,1) $$ 説明 ベルヌーイ分布 二項分布は、人が最も簡単に考えられるベルヌーイ試行Bernoulli Experimentから始まる。ベルヌーイ試行は、確率 $0 \\le p \\le 1$ で成功するか失敗するかの2つの結果しかなく、これを $n$ 回で一般化したものが二項分布である。逆に、ベルヌーイ分布は二項分布が $n=1$ の時の特別なケースである。\n多項分布 さらに、成功か失敗かの2つのケースではなく $k$ の場合に一般化することで、多変量分布 $M (n; p_{1} , \\cdots , p_{k})$ を多項分布Multinomial Distributionと呼ぶ。その確率質量関数は次のように与えられる。 $$ p(x_{1} , \\cdots , x_{k}) = {{ n! } \\over { x_{1} ! \\cdots x_{k}! }} p_{1}^{x_{1}} \\cdots p_{k}^{x_{k}} $$\n証明 [1] $$ \\begin{align*} M(t) =\u0026amp; \\sum_{x=0}^{n} e^{tx} p(x) \\\\ =\u0026amp; \\sum_{x=0}^{n} e^{tx} \\binom{n}{x} p^{x} (1-p)^{n-x} \\\\ =\u0026amp; \\sum_{x=0}^{n} \\binom{n}{x} \\left( pe^{t} \\right)^{x} (1-p)^{n-x} \\end{align*} $$ 二項定理によると $$ \\sum_{x=0}^{n} \\binom{n}{x} \\left( pe^{t} \\right)^{x} (1-p)^{n-x} = \\left[ pe^{t} + (1-p) \\right]^{n} $$\n■\n[2] 戦略: 教科課程のように数式的トリックを使って導出することもできるが、積率母関数も求めてあるので数理統計学の理論を使って簡単に導出してみよう。\n$M$ の導関数は $$ M ' (t) = n \\left[ (1-p) + pe^{t} \\right]^{n-1} \\left( pe^{t} \\right) $$ 積率母関数の定義から $ E(X) = M ' (0):$ であるため $$ \\mu := E(X) = M ' (0) = np $$ $M$ の二階導関数は $$ M '' (t) = n \\left[ (1-p) + pe^{t} \\right]^{n-1} \\left( pe^{t} \\right) + n(n-1) \\left[ (1-p) + pe^{t} \\right]^{n-2} \\left( pe^{t} \\right)^{2} $$ $M '' (0) = np + n(n-1)p^{2}$ であるため $$ \\begin{align*} \\text{Var}(X) =\u0026amp; E \\left( X^{2} \\right) - \\mu^{2} \\\\ =\u0026amp; M '' (0) - (np)^{2} \\\\ =\u0026amp; np + n(n-1)p^{2} - n^{2}p^{2} \\\\ =\u0026amp; np(1-p) \\end{align*} $$\n■\n[a] 積率生成関数で近似する。\n■\n[b] 中心極限定理のように近似する。\n■\nコード 次はJuliaのコードで、二項分布の確率質量関数をGIFで表示するものである。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = 0:20\rP = collect(0.0:0.01:1.0); append!(P, reverse(P))\ranimation = @animate for p ∈ P\rscatter(x, pdf.(Binomial(10, p), x),\rcolor = :black, markerstrokecolor = :black,\rlabel = \u0026#34;n = 10, p = $(rpad(p, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Bin}(10, p)\u0026#34;)\rend\rgif(animation, \u0026#34;pmf10.gif\u0026#34;)\ranimation = @animate for p ∈ P\rscatter(x, pdf.(Binomial(20, p), x),\rcolor = :black, markerstrokecolor = :black,\rlabel = \u0026#34;n = 20, p = $(rpad(p, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Bin}(20, p)\u0026#34;)\rend\rgif(animation, \u0026#34;pmf20.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p142.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1480,"permalink":"https://freshrimpsushi.github.io/jp/posts/1480/","tags":["줄리아"],"title":"二項分布"},{"categories":"정수론","contents":"素数 1万番目までの素数のリストである。\nダウンロード\r2 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97 101 103 107 109 113 127 131 137 139 149 151 157 163 167 173 179 181 191 193 197 199 211 223 227 229 233 239 241 251 257 263 269 271 277 281 283 293 307 311 313 317 331 337 347 349 353 359 367 373 379 383 389 397 401 409 419 421 431 433 439 443 449 457 461 463 467 479 487 491 499 503 509 521 523 541 547 557 563 569 571 577 587 593 599 601 607 613 617 619 631 641 643 647 653 659 661 673 677 683 691 701 709 719 727 733 739 743 751 757 761 769 773 787 797 809 811 821 823 827 829 839 853 857 859 863 877 881 883 887 907 911 919 929 937 941 947 953 967 971 977 983 991 997 1009 1013 1019 1021 1031 1033 1039 1049 1051 1061 1063 1069 1087 1091 1093 1097 1103 1109 1117 1123 1129 1151 1153 1163 1171 1181 1187 1193 1201 1213 1217 1223 1229 1231 1237 1249 1259 1277 1279 1283 1289 1291 1297 1301 1303 1307 1319 1321 1327 1361 1367 1373 1381 1399 1409 1423 1427 1429 1433 1439 1447 1451 1453 1459 1471 1481 1483 1487 1489 1493 1499 1511 1523 1531 1543 1549 1553 1559 1567 1571 1579 1583 1597 1601 1607 1609 1613 1619 1621 1627 1637 1657 1663 1667 1669 1693 1697 1699 1709 1721 1723 1733 1741 1747 1753 1759 1777 1783 1787 1789 1801 1811 1823 1831 1847 1861 1867 1871 1873 1877 1879 1889 1901 1907 1913 1931 1933 1949 1951 1973 1979 1987 1993 1997 1999 2003 2011 2017 2027 2029 2039 2053 2063 2069 2081 2083 2087 2089 2099 2111 2113 2129 2131 2137 2141 2143 2153 2161 2179 2203 2207 2213 2221 2237 2239 2243 2251 2267 2269 2273 2281 2287 2293 2297 2309 2311 2333 2339 2341 2347 2351 2357 2371 2377 2381 2383 2389 2393 2399 2411 2417 2423 2437 2441 2447 2459 2467 2473 2477 2503 2521 2531 2539 2543 2549 2551 2557 2579 2591 2593 2609 2617 2621 2633 2647 2657 2659 2663 2671 2677 2683 2687 2689 2693 2699 2707 2711 2713 2719 2729 2731 2741 2749 2753 2767 2777 2789 2791 2797 2801 2803 2819 2833 2837 2843 2851 2857 2861 2879 2887 2897 2903 2909 2917 2927 2939 2953 2957 2963 2969 2971 2999 3001 3011 3019 3023 3037 3041 3049 3061 3067 3079 3083 3089 3109 3119 3121 3137 3163 3167 3169 3181 3187 3191 3203 3209 3217 3221 3229 3251 3253 3257 3259 3271 3299 3301 3307 3313 3319 3323 3329 3331 3343 3347 3359 3361 3371 3373 3389 3391 3407 3413 3433 3449 3457 3461 3463 3467 3469 3491 3499 3511 3517 3527 3529 3533 3539 3541 3547 3557 3559 3571 3581 3583 3593 3607 3613 3617 3623 3631 3637 3643 3659 3671 3673 3677 3691 3697 3701 3709 3719 3727 3733 3739 3761 3767 3769 3779 3793 3797 3803 3821 3823 3833 3847 3851 3853 3863 3877 3881 3889 3907 3911 3917 3919 3923 3929 3931 3943 3947 3967 3989 4001 4003 4007 4013 4019 4021 4027 4049 4051 4057 4073 4079 4091 4093 4099 4111 4127 4129 4133 4139 4153 4157 4159 4177 4201 4211 4217 4219 4229 4231 4241 4243 4253 4259 4261 4271 4273 4283 4289 4297 4327 4337 4339 4349 4357 4363 4373 4391 4397 4409 4421 4423 4441 4447 4451 4457 4463 4481 4483 4493 4507 4513 4517 4519 4523 4547 4549 4561 4567 4583 4591 4597 4603 4621 4637 4639 4643 4649 4651 4657 4663 4673 4679 4691 4703 4721 4723 4729 4733 4751 4759 4783 4787 4789 4793 4799 4801 4813 4817 4831 4861 4871 4877 4889 4903 4909 4919 4931 4933 4937 4943 4951 4957 4967 4969 4973 4987 4993 4999 5003 5009 5011 5021 5023 5039 5051 5059 5077 5081 5087 5099 5101 5107 5113 5119 5147 5153 5167 5171 5179 5189 5197 5209 5227 5231 5233 5237 5261 5273 5279 5281 5297 5303 5309 5323 5333 5347 5351 5381 5387 5393 5399 5407 5413 5417 5419 5431 5437 5441 5443 5449 5471 5477 5479 5483 5501 5503 5507 5519 5521 5527 5531 5557 5563 5569 5573 5581 5591 5623 5639 5641 5647 5651 5653 5657 5659 5669 5683 5689 5693 5701 5711 5717 5737 5741 5743 5749 5779 5783 5791 5801 5807 5813 5821 5827 5839 5843 5849 5851 5857 5861 5867 5869 5879 5881 5897 5903 5923 5927 5939 5953 5981 5987 6007 6011 6029 6037 6043 6047 6053 6067 6073 6079 6089 6091 6101 6113 6121 6131 6133 6143 6151 6163 6173 6197 6199 6203 6211 6217 6221 6229 6247 6257 6263 6269 6271 6277 6287 6299 6301 6311 6317 6323 6329 6337 6343 6353 6359 6361 6367 6373 6379 6389 6397 6421 6427 6449 6451 6469 6473 6481 6491 6521 6529 6547 6551 6553 6563 6569 6571 6577 6581 6599 6607 6619 6637 6653 6659 6661 6673 6679 6689 6691 6701 6703 6709 6719 6733 6737 6761 6763 6779 6781 6791 6793 6803 6823 6827 6829 6833 6841 6857 6863 6869 6871 6883 6899 6907 6911 6917 6947 6949 6959 6961 6967 6971 6977 6983 6991 6997 7001 7013 7019 7027 7039 7043 7057 7069 7079 7103 7109 7121 7127 7129 7151 7159 7177 7187 7193 7207 7211 7213 7219 7229 7237 7243 7247 7253 7283 7297 7307 7309 7321 7331 7333 7349 7351 7369 7393 7411 7417 7433 7451 7457 7459 7477 7481 7487 7489 7499 7507 7517 7523 7529 7537 7541 7547 7549 7559 7561 7573 7577 7583 7589 7591 7603 7607 7621 7639 7643 7649 7669 7673 7681 7687 7691 7699 7703 7717 7723 7727 7741 7753 7757 7759 7789 7793 7817 7823 7829 7841 7853 7867 7873 7877 7879 7883 7901 7907 7919 7927 7933 7937 7949 7951 7963 7993 8009 8011 8017 8039 8053 8059 8069 8081 8087 8089 8093 8101 8111 8117 8123 8147 8161 8167 8171 8179 8191 8209 8219 8221 8231 8233 8237 8243 8263 8269 8273 8287 8291 8293 8297 8311 8317 8329 8353 8363 8369 8377 8387 8389 8419 8423 8429 8431 8443 8447 8461 8467 8501 8513 8521 8527 8537 8539 8543 8563 8573 8581 8597 8599 8609 8623 8627 8629 8641 8647 8663 8669 8677 8681 8689 8693 8699 8707 8713 8719 8731 8737 8741 8747 8753 8761 8779 8783 8803 8807 8819 8821 8831 8837 8839 8849 8861 8863 8867 8887 8893 8923 8929 8933 8941 8951 8963 8969 8971 8999 9001 9007 9011 9013 9029 9041 9043 9049 9059 9067 9091 9103 9109 9127 9133 9137 9151 9157 9161 9173 9181 9187 9199 9203 9209 9221 9227 9239 9241 9257 9277 9281 9283 9293 9311 9319 9323 9337 9341 9343 9349 9371 9377 9391 9397 9403 9413 9419 9421 9431 9433 9437 9439 9461 9463 9467 9473 9479 9491 9497 9511 9521 9533 9539 9547 9551 9587 9601 9613 9619 9623 9629 9631 9643 9649 9661 9677 9679 9689 9697 9719 9721 9733 9739 9743 9749 9767 9769 9781 9787 9791 9803 9811 9817 9829 9833 9839 9851 9857 9859 9871 9883 9887 9901 9907 9923 9929 9931 9941 9949 9967 9973 10007 10009 10037 10039 10061 10067 10069 10079 10091 10093 10099 10103 10111 10133 10139 10141 10151 10159 10163 10169 10177 10181 10193 10211 10223 10243 10247 10253 10259 10267 10271 10273 10289 10301 10303 10313 10321 10331 10333 10337 10343 10357 10369 10391 10399 10427 10429 10433 10453 10457 10459 10463 10477 10487 10499 10501 10513 10529 10531 10559 10567 10589 10597 10601 10607 10613 10627 10631 10639 10651 10657 10663 10667 10687 10691 10709 10711 10723 10729 10733 10739 10753 10771 10781 10789 10799 10831 10837 10847 10853 10859 10861 10867 10883 10889 10891 10903 10909 10937 10939 10949 10957 10973 10979 10987 10993 11003 11027 11047 11057 11059 11069 11071 11083 11087 11093 11113 11117 11119 11131 11149 11159 11161 11171 11173 11177 11197 11213 11239 11243 11251 11257 11261 11273 11279 11287 11299 11311 11317 11321 11329 11351 11353 11369 11383 11393 11399 11411 11423 11437 11443 11447 11467 11471 11483 11489 11491 11497 11503 11519 11527 11549 11551 11579 11587 11593 11597 11617 11621 11633 11657 11677 11681 11689 11699 11701 11717 11719 11731 11743 11777 11779 11783 11789 11801 11807 11813 11821 11827 11831 11833 11839 11863 11867 11887 11897 11903 11909 11923 11927 11933 11939 11941 11953 11959 11969 11971 11981 11987 12007 12011 12037 12041 12043 12049 12071 12073 12097 12101 12107 12109 12113 12119 12143 12149 12157 12161 12163 12197 12203 12211 12227 12239 12241 12251 12253 12263 12269 12277 12281 12289 12301 12323 12329 12343 12347 12373 12377 12379 12391 12401 12409 12413 12421 12433 12437 12451 12457 12473 12479 12487 12491 12497 12503 12511 12517 12527 12539 12541 12547 12553 12569 12577 12583 12589 12601 12611 12613 12619 12637 12641 12647 12653 12659 12671 12689 12697 12703 12713 12721 12739 12743 12757 12763 12781 12791 12799 12809 12821 12823 12829 12841 12853 12889 12893 12899 12907 12911 12917 12919 12923 12941 12953 12959 12967 12973 12979 12983 13001 13003 13007 13009 13033 13037 13043 13049 13063 13093 13099 13103 13109 13121 13127 13147 13151 13159 13163 13171 13177 13183 13187 13217 13219 13229 13241 13249 13259 13267 13291 13297 13309 13313 13327 13331 13337 13339 13367 13381 13397 13399 13411 13417 13421 13441 13451 13457 13463 13469 13477 13487 13499 13513 13523 13537 13553 13567 13577 13591 13597 13613 13619 13627 13633 13649 13669 13679 13681 13687 13691 13693 13697 13709 13711 13721 13723 13729 13751 13757 13759 13763 13781 13789 13799 13807 13829 13831 13841 13859 13873 13877 13879 13883 13901 13903 13907 13913 13921 13931 13933 13963 13967 13997 13999 14009 14011 14029 14033 14051 14057 14071 14081 14083 14087 14107 14143 14149 14153 14159 14173 14177 14197 14207 14221 14243 14249 14251 14281 14293 14303 14321 14323 14327 14341 14347 14369 14387 14389 14401 14407 14411 14419 14423 14431 14437 14447 14449 14461 14479 14489 14503 14519 14533 14537 14543 14549 14551 14557 14561 14563 14591 14593 14621 14627 14629 14633 14639 14653 14657 14669 14683 14699 14713 14717 14723 14731 14737 14741 14747 14753 14759 14767 14771 14779 14783 14797 14813 14821 14827 14831 14843 14851 14867 14869 14879 14887 14891 14897 14923 14929 14939 14947 14951 14957 14969 14983 15013 15017 15031 15053 15061 15073 15077 15083 15091 15101 15107 15121 15131 15137 15139 15149 15161 15173 15187 15193 15199 15217 15227 15233 15241 15259 15263 15269 15271 15277 15287 15289 15299 15307 15313 15319 15329 15331 15349 15359 15361 15373 15377 15383 15391 15401 15413 15427 15439 15443 15451 15461 15467 15473 15493 15497 15511 15527 15541 15551 15559 15569 15581 15583 15601 15607 15619 15629 15641 15643 15647 15649 15661 15667 15671 15679 15683 15727 15731 15733 15737 15739 15749 15761 15767 15773 15787 15791 15797 15803 15809 15817 15823 15859 15877 15881 15887 15889 15901 15907 15913 15919 15923 15937 15959 15971 15973 15991 16001 16007 16033 16057 16061 16063 16067 16069 16073 16087 16091 16097 16103 16111 16127 16139 16141 16183 16187 16189 16193 16217 16223 16229 16231 16249 16253 16267 16273 16301 16319 16333 16339 16349 16361 16363 16369 16381 16411 16417 16421 16427 16433 16447 16451 16453 16477 16481 16487 16493 16519 16529 16547 16553 16561 16567 16573 16603 16607 16619 16631 16633 16649 16651 16657 16661 16673 16691 16693 16699 16703 16729 16741 16747 16759 16763 16787 16811 16823 16829 16831 16843 16871 16879 16883 16889 16901 16903 16921 16927 16931 16937 16943 16963 16979 16981 16987 16993 17011 17021 17027 17029 17033 17041 17047 17053 17077 17093 17099 17107 17117 17123 17137 17159 17167 17183 17189 17191 17203 17207 17209 17231 17239 17257 17291 17293 17299 17317 17321 17327 17333 17341 17351 17359 17377 17383 17387 17389 17393 17401 17417 17419 17431 17443 17449 17467 17471 17477 17483 17489 17491 17497 17509 17519 17539 17551 17569 17573 17579 17581 17597 17599 17609 17623 17627 17657 17659 17669 17681 17683 17707 17713 17729 17737 17747 17749 17761 17783 17789 17791 17807 17827 17837 17839 17851 17863 17881 17891 17903 17909 17911 17921 17923 17929 17939 17957 17959 17971 17977 17981 17987 17989 18013 18041 18043 18047 18049 18059 18061 18077 18089 18097 18119 18121 18127 18131 18133 18143 18149 18169 18181 18191 18199 18211 18217 18223 18229 18233 18251 18253 18257 18269 18287 18289 18301 18307 18311 18313 18329 18341 18353 18367 18371 18379 18397 18401 18413 18427 18433 18439 18443 18451 18457 18461 18481 18493 18503 18517 18521 18523 18539 18541 18553 18583 18587 18593 18617 18637 18661 18671 18679 18691 18701 18713 18719 18731 18743 18749 18757 18773 18787 18793 18797 18803 18839 18859 18869 18899 18911 18913 18917 18919 18947 18959 18973 18979 19001 19009 19013 19031 19037 19051 19069 19073 19079 19081 19087 19121 19139 19141 19157 19163 19181 19183 19207 19211 19213 19219 19231 19237 19249 19259 19267 19273 19289 19301 19309 19319 19333 19373 19379 19381 19387 19391 19403 19417 19421 19423 19427 19429 19433 19441 19447 19457 19463 19469 19471 19477 19483 19489 19501 19507 19531 19541 19543 19553 19559 19571 19577 19583 19597 19603 19609 19661 19681 19687 19697 19699 19709 19717 19727 19739 19751 19753 19759 19763 19777 19793 19801 19813 19819 19841 19843 19853 19861 19867 19889 19891 19913 19919 19927 19937 19949 19961 19963 19973 19979 19991 19993 19997 20011 20021 20023 20029 20047 20051 20063 20071 20089 20101 20107 20113 20117 20123 20129 20143 20147 20149 20161 20173 20177 20183 20201 20219 20231 20233 20249 20261 20269 20287 20297 20323 20327 20333 20341 20347 20353 20357 20359 20369 20389 20393 20399 20407 20411 20431 20441 20443 20477 20479 20483 20507 20509 20521 20533 20543 20549 20551 20563 20593 20599 20611 20627 20639 20641 20663 20681 20693 20707 20717 20719 20731 20743 20747 20749 20753 20759 20771 20773 20789 20807 20809 20849 20857 20873 20879 20887 20897 20899 20903 20921 20929 20939 20947 20959 20963 20981 20983 21001 21011 21013 21017 21019 21023 21031 21059 21061 21067 21089 21101 21107 21121 21139 21143 21149 21157 21163 21169 21179 21187 21191 21193 21211 21221 21227 21247 21269 21277 21283 21313 21317 21319 21323 21341 21347 21377 21379 21383 21391 21397 21401 21407 21419 21433 21467 21481 21487 21491 21493 21499 21503 21517 21521 21523 21529 21557 21559 21563 21569 21577 21587 21589 21599 21601 21611 21613 21617 21647 21649 21661 21673 21683 21701 21713 21727 21737 21739 21751 21757 21767 21773 21787 21799 21803 21817 21821 21839 21841 21851 21859 21863 21871 21881 21893 21911 21929 21937 21943 21961 21977 21991 21997 22003 22013 22027 22031 22037 22039 22051 22063 22067 22073 22079 22091 22093 22109 22111 22123 22129 22133 22147 22153 22157 22159 22171 22189 22193 22229 22247 22259 22271 22273 22277 22279 22283 22291 22303 22307 22343 22349 22367 22369 22381 22391 22397 22409 22433 22441 22447 22453 22469 22481 22483 22501 22511 22531 22541 22543 22549 22567 22571 22573 22613 22619 22621 22637 22639 22643 22651 22669 22679 22691 22697 22699 22709 22717 22721 22727 22739 22741 22751 22769 22777 22783 22787 22807 22811 22817 22853 22859 22861 22871 22877 22901 22907 22921 22937 22943 22961 22963 22973 22993 23003 23011 23017 23021 23027 23029 23039 23041 23053 23057 23059 23063 23071 23081 23087 23099 23117 23131 23143 23159 23167 23173 23189 23197 23201 23203 23209 23227 23251 23269 23279 23291 23293 23297 23311 23321 23327 23333 23339 23357 23369 23371 23399 23417 23431 23447 23459 23473 23497 23509 23531 23537 23539 23549 23557 23561 23563 23567 23581 23593 23599 23603 23609 23623 23627 23629 23633 23663 23669 23671 23677 23687 23689 23719 23741 23743 23747 23753 23761 23767 23773 23789 23801 23813 23819 23827 23831 23833 23857 23869 23873 23879 23887 23893 23899 23909 23911 23917 23929 23957 23971 23977 23981 23993 24001 24007 24019 24023 24029 24043 24049 24061 24071 24077 24083 24091 24097 24103 24107 24109 24113 24121 24133 24137 24151 24169 24179 24181 24197 24203 24223 24229 24239 24247 24251 24281 24317 24329 24337 24359 24371 24373 24379 24391 24407 24413 24419 24421 24439 24443 24469 24473 24481 24499 24509 24517 24527 24533 24547 24551 24571 24593 24611 24623 24631 24659 24671 24677 24683 24691 24697 24709 24733 24749 24763 24767 24781 24793 24799 24809 24821 24841 24847 24851 24859 24877 24889 24907 24917 24919 24923 24943 24953 24967 24971 24977 24979 24989 25013 25031 25033 25037 25057 25073 25087 25097 25111 25117 25121 25127 25147 25153 25163 25169 25171 25183 25189 25219 25229 25237 25243 25247 25253 25261 25301 25303 25307 25309 25321 25339 25343 25349 25357 25367 25373 25391 25409 25411 25423 25439 25447 25453 25457 25463 25469 25471 25523 25537 25541 25561 25577 25579 25583 25589 25601 25603 25609 25621 25633 25639 25643 25657 25667 25673 25679 25693 25703 25717 25733 25741 25747 25759 25763 25771 25793 25799 25801 25819 25841 25847 25849 25867 25873 25889 25903 25913 25919 25931 25933 25939 25943 25951 25969 25981 25997 25999 26003 26017 26021 26029 26041 26053 26083 26099 26107 26111 26113 26119 26141 26153 26161 26171 26177 26183 26189 26203 26209 26227 26237 26249 26251 26261 26263 26267 26293 26297 26309 26317 26321 26339 26347 26357 26371 26387 26393 26399 26407 26417 26423 26431 26437 26449 26459 26479 26489 26497 26501 26513 26539 26557 26561 26573 26591 26597 26627 26633 26641 26647 26669 26681 26683 26687 26693 26699 26701 26711 26713 26717 26723 26729 26731 26737 26759 26777 26783 26801 26813 26821 26833 26839 26849 26861 26863 26879 26881 26891 26893 26903 26921 26927 26947 26951 26953 26959 26981 26987 26993 27011 27017 27031 27043 27059 27061 27067 27073 27077 27091 27103 27107 27109 27127 27143 27179 27191 27197 27211 27239 27241 27253 27259 27271 27277 27281 27283 27299 27329 27337 27361 27367 27397 27407 27409 27427 27431 27437 27449 27457 27479 27481 27487 27509 27527 27529 27539 27541 27551 27581 27583 27611 27617 27631 27647 27653 27673 27689 27691 27697 27701 27733 27737 27739 27743 27749 27751 27763 27767 27773 27779 27791 27793 27799 27803 27809 27817 27823 27827 27847 27851 27883 27893 27901 27917 27919 27941 27943 27947 27953 27961 27967 27983 27997 28001 28019 28027 28031 28051 28057 28069 28081 28087 28097 28099 28109 28111 28123 28151 28163 28181 28183 28201 28211 28219 28229 28277 28279 28283 28289 28297 28307 28309 28319 28349 28351 28387 28393 28403 28409 28411 28429 28433 28439 28447 28463 28477 28493 28499 28513 28517 28537 28541 28547 28549 28559 28571 28573 28579 28591 28597 28603 28607 28619 28621 28627 28631 28643 28649 28657 28661 28663 28669 28687 28697 28703 28711 28723 28729 28751 28753 28759 28771 28789 28793 28807 28813 28817 28837 28843 28859 28867 28871 28879 28901 28909 28921 28927 28933 28949 28961 28979 29009 29017 29021 29023 29027 29033 29059 29063 29077 29101 29123 29129 29131 29137 29147 29153 29167 29173 29179 29191 29201 29207 29209 29221 29231 29243 29251 29269 29287 29297 29303 29311 29327 29333 29339 29347 29363 29383 29387 29389 29399 29401 29411 29423 29429 29437 29443 29453 29473 29483 29501 29527 29531 29537 29567 29569 29573 29581 29587 29599 29611 29629 29633 29641 29663 29669 29671 29683 29717 29723 29741 29753 29759 29761 29789 29803 29819 29833 29837 29851 29863 29867 29873 29879 29881 29917 29921 29927 29947 29959 29983 29989 30011 30013 30029 30047 30059 30071 30089 30091 30097 30103 30109 30113 30119 30133 30137 30139 30161 30169 30181 30187 30197 30203 30211 30223 30241 30253 30259 30269 30271 30293 30307 30313 30319 30323 30341 30347 30367 30389 30391 30403 30427 30431 30449 30467 30469 30491 30493 30497 30509 30517 30529 30539 30553 30557 30559 30577 30593 30631 30637 30643 30649 30661 30671 30677 30689 30697 30703 30707 30713 30727 30757 30763 30773 30781 30803 30809 30817 30829 30839 30841 30851 30853 30859 30869 30871 30881 30893 30911 30931 30937 30941 30949 30971 30977 30983 31013 31019 31033 31039 31051 31063 31069 31079 31081 31091 31121 31123 31139 31147 31151 31153 31159 31177 31181 31183 31189 31193 31219 31223 31231 31237 31247 31249 31253 31259 31267 31271 31277 31307 31319 31321 31327 31333 31337 31357 31379 31387 31391 31393 31397 31469 31477 31481 31489 31511 31513 31517 31531 31541 31543 31547 31567 31573 31583 31601 31607 31627 31643 31649 31657 31663 31667 31687 31699 31721 31723 31727 31729 31741 31751 31769 31771 31793 31799 31817 31847 31849 31859 31873 31883 31891 31907 31957 31963 31973 31981 31991 32003 32009 32027 32029 32051 32057 32059 32063 32069 32077 32083 32089 32099 32117 32119 32141 32143 32159 32173 32183 32189 32191 32203 32213 32233 32237 32251 32257 32261 32297 32299 32303 32309 32321 32323 32327 32341 32353 32359 32363 32369 32371 32377 32381 32401 32411 32413 32423 32429 32441 32443 32467 32479 32491 32497 32503 32507 32531 32533 32537 32561 32563 32569 32573 32579 32587 32603 32609 32611 32621 32633 32647 32653 32687 32693 32707 32713 32717 32719 32749 32771 32779 32783 32789 32797 32801 32803 32831 32833 32839 32843 32869 32887 32909 32911 32917 32933 32939 32941 32957 32969 32971 32983 32987 32993 32999 33013 33023 33029 33037 33049 33053 33071 33073 33083 33091 33107 33113 33119 33149 33151 33161 33179 33181 33191 33199 33203 33211 33223 33247 33287 33289 33301 33311 33317 33329 33331 33343 33347 33349 33353 33359 33377 33391 33403 33409 33413 33427 33457 33461 33469 33479 33487 33493 33503 33521 33529 33533 33547 33563 33569 33577 33581 33587 33589 33599 33601 33613 33617 33619 33623 33629 33637 33641 33647 33679 33703 33713 33721 33739 33749 33751 33757 33767 33769 33773 33791 33797 33809 33811 33827 33829 33851 33857 33863 33871 33889 33893 33911 33923 33931 33937 33941 33961 33967 33997 34019 34031 34033 34039 34057 34061 34123 34127 34129 34141 34147 34157 34159 34171 34183 34211 34213 34217 34231 34253 34259 34261 34267 34273 34283 34297 34301 34303 34313 34319 34327 34337 34351 34361 34367 34369 34381 34403 34421 34429 34439 34457 34469 34471 34483 34487 34499 34501 34511 34513 34519 34537 34543 34549 34583 34589 34591 34603 34607 34613 34631 34649 34651 34667 34673 34679 34687 34693 34703 34721 34729 34739 34747 34757 34759 34763 34781 34807 34819 34841 34843 34847 34849 34871 34877 34883 34897 34913 34919 34939 34949 34961 34963 34981 35023 35027 35051 35053 35059 35069 35081 35083 35089 35099 35107 35111 35117 35129 35141 35149 35153 35159 35171 35201 35221 35227 35251 35257 35267 35279 35281 35291 35311 35317 35323 35327 35339 35353 35363 35381 35393 35401 35407 35419 35423 35437 35447 35449 35461 35491 35507 35509 35521 35527 35531 35533 35537 35543 35569 35573 35591 35593 35597 35603 35617 35671 35677 35729 35731 35747 35753 35759 35771 35797 35801 35803 35809 35831 35837 35839 35851 35863 35869 35879 35897 35899 35911 35923 35933 35951 35963 35969 35977 35983 35993 35999 36007 36011 36013 36017 36037 36061 36067 36073 36083 36097 36107 36109 36131 36137 36151 36161 36187 36191 36209 36217 36229 36241 36251 36263 36269 36277 36293 36299 36307 36313 36319 36341 36343 36353 36373 36383 36389 36433 36451 36457 36467 36469 36473 36479 36493 36497 36523 36527 36529 36541 36551 36559 36563 36571 36583 36587 36599 36607 36629 36637 36643 36653 36671 36677 36683 36691 36697 36709 36713 36721 36739 36749 36761 36767 36779 36781 36787 36791 36793 36809 36821 36833 36847 36857 36871 36877 36887 36899 36901 36913 36919 36923 36929 36931 36943 36947 36973 36979 36997 37003 37013 37019 37021 37039 37049 37057 37061 37087 37097 37117 37123 37139 37159 37171 37181 37189 37199 37201 37217 37223 37243 37253 37273 37277 37307 37309 37313 37321 37337 37339 37357 37361 37363 37369 37379 37397 37409 37423 37441 37447 37463 37483 37489 37493 37501 37507 37511 37517 37529 37537 37547 37549 37561 37567 37571 37573 37579 37589 37591 37607 37619 37633 37643 37649 37657 37663 37691 37693 37699 37717 37747 37781 37783 37799 37811 37813 37831 37847 37853 37861 37871 37879 37889 37897 37907 37951 37957 37963 37967 37987 37991 37993 37997 38011 38039 38047 38053 38069 38083 38113 38119 38149 38153 38167 38177 38183 38189 38197 38201 38219 38231 38237 38239 38261 38273 38281 38287 38299 38303 38317 38321 38327 38329 38333 38351 38371 38377 38393 38431 38447 38449 38453 38459 38461 38501 38543 38557 38561 38567 38569 38593 38603 38609 38611 38629 38639 38651 38653 38669 38671 38677 38693 38699 38707 38711 38713 38723 38729 38737 38747 38749 38767 38783 38791 38803 38821 38833 38839 38851 38861 38867 38873 38891 38903 38917 38921 38923 38933 38953 38959 38971 38977 38993 39019 39023 39041 39043 39047 39079 39089 39097 39103 39107 39113 39119 39133 39139 39157 39161 39163 39181 39191 39199 39209 39217 39227 39229 39233 39239 39241 39251 39293 39301 39313 39317 39323 39341 39343 39359 39367 39371 39373 39383 39397 39409 39419 39439 39443 39451 39461 39499 39503 39509 39511 39521 39541 39551 39563 39569 39581 39607 39619 39623 39631 39659 39667 39671 39679 39703 39709 39719 39727 39733 39749 39761 39769 39779 39791 39799 39821 39827 39829 39839 39841 39847 39857 39863 39869 39877 39883 39887 39901 39929 39937 39953 39971 39979 39983 39989 40009 40013 40031 40037 40039 40063 40087 40093 40099 40111 40123 40127 40129 40151 40153 40163 40169 40177 40189 40193 40213 40231 40237 40241 40253 40277 40283 40289 40343 40351 40357 40361 40387 40423 40427 40429 40433 40459 40471 40483 40487 40493 40499 40507 40519 40529 40531 40543 40559 40577 40583 40591 40597 40609 40627 40637 40639 40693 40697 40699 40709 40739 40751 40759 40763 40771 40787 40801 40813 40819 40823 40829 40841 40847 40849 40853 40867 40879 40883 40897 40903 40927 40933 40939 40949 40961 40973 40993 41011 41017 41023 41039 41047 41051 41057 41077 41081 41113 41117 41131 41141 41143 41149 41161 41177 41179 41183 41189 41201 41203 41213 41221 41227 41231 41233 41243 41257 41263 41269 41281 41299 41333 41341 41351 41357 41381 41387 41389 41399 41411 41413 41443 41453 41467 41479 41491 41507 41513 41519 41521 41539 41543 41549 41579 41593 41597 41603 41609 41611 41617 41621 41627 41641 41647 41651 41659 41669 41681 41687 41719 41729 41737 41759 41761 41771 41777 41801 41809 41813 41843 41849 41851 41863 41879 41887 41893 41897 41903 41911 41927 41941 41947 41953 41957 41959 41969 41981 41983 41999 42013 42017 42019 42023 42043 42061 42071 42073 42083 42089 42101 42131 42139 42157 42169 42179 42181 42187 42193 42197 42209 42221 42223 42227 42239 42257 42281 42283 42293 42299 42307 42323 42331 42337 42349 42359 42373 42379 42391 42397 42403 42407 42409 42433 42437 42443 42451 42457 42461 42463 42467 42473 42487 42491 42499 42509 42533 42557 42569 42571 42577 42589 42611 42641 42643 42649 42667 42677 42683 42689 42697 42701 42703 42709 42719 42727 42737 42743 42751 42767 42773 42787 42793 42797 42821 42829 42839 42841 42853 42859 42863 42899 42901 42923 42929 42937 42943 42953 42961 42967 42979 42989 43003 43013 43019 43037 43049 43051 43063 43067 43093 43103 43117 43133 43151 43159 43177 43189 43201 43207 43223 43237 43261 43271 43283 43291 43313 43319 43321 43331 43391 43397 43399 43403 43411 43427 43441 43451 43457 43481 43487 43499 43517 43541 43543 43573 43577 43579 43591 43597 43607 43609 43613 43627 43633 43649 43651 43661 43669 43691 43711 43717 43721 43753 43759 43777 43781 43783 43787 43789 43793 43801 43853 43867 43889 43891 43913 43933 43943 43951 43961 43963 43969 43973 43987 43991 43997 44017 44021 44027 44029 44041 44053 44059 44071 44087 44089 44101 44111 44119 44123 44129 44131 44159 44171 44179 44189 44201 44203 44207 44221 44249 44257 44263 44267 44269 44273 44279 44281 44293 44351 44357 44371 44381 44383 44389 44417 44449 44453 44483 44491 44497 44501 44507 44519 44531 44533 44537 44543 44549 44563 44579 44587 44617 44621 44623 44633 44641 44647 44651 44657 44683 44687 44699 44701 44711 44729 44741 44753 44771 44773 44777 44789 44797 44809 44819 44839 44843 44851 44867 44879 44887 44893 44909 44917 44927 44939 44953 44959 44963 44971 44983 44987 45007 45013 45053 45061 45077 45083 45119 45121 45127 45131 45137 45139 45161 45179 45181 45191 45197 45233 45247 45259 45263 45281 45289 45293 45307 45317 45319 45329 45337 45341 45343 45361 45377 45389 45403 45413 45427 45433 45439 45481 45491 45497 45503 45523 45533 45541 45553 45557 45569 45587 45589 45599 45613 45631 45641 45659 45667 45673 45677 45691 45697 45707 45737 45751 45757 45763 45767 45779 45817 45821 45823 45827 45833 45841 45853 45863 45869 45887 45893 45943 45949 45953 45959 45971 45979 45989 46021 46027 46049 46051 46061 46073 46091 46093 46099 46103 46133 46141 46147 46153 46171 46181 46183 46187 46199 46219 46229 46237 46261 46271 46273 46279 46301 46307 46309 46327 46337 46349 46351 46381 46399 46411 46439 46441 46447 46451 46457 46471 46477 46489 46499 46507 46511 46523 46549 46559 46567 46573 46589 46591 46601 46619 46633 46639 46643 46649 46663 46679 46681 46687 46691 46703 46723 46727 46747 46751 46757 46769 46771 46807 46811 46817 46819 46829 46831 46853 46861 46867 46877 46889 46901 46919 46933 46957 46993 46997 47017 47041 47051 47057 47059 47087 47093 47111 47119 47123 47129 47137 47143 47147 47149 47161 47189 47207 47221 47237 47251 47269 47279 47287 47293 47297 47303 47309 47317 47339 47351 47353 47363 47381 47387 47389 47407 47417 47419 47431 47441 47459 47491 47497 47501 47507 47513 47521 47527 47533 47543 47563 47569 47581 47591 47599 47609 47623 47629 47639 47653 47657 47659 47681 47699 47701 47711 47713 47717 47737 47741 47743 47777 47779 47791 47797 47807 47809 47819 47837 47843 47857 47869 47881 47903 47911 47917 47933 47939 47947 47951 47963 47969 47977 47981 48017 48023 48029 48049 48073 48079 48091 48109 48119 48121 48131 48157 48163 48179 48187 48193 48197 48221 48239 48247 48259 48271 48281 48299 48311 48313 48337 48341 48353 48371 48383 48397 48407 48409 48413 48437 48449 48463 48473 48479 48481 48487 48491 48497 48523 48527 48533 48539 48541 48563 48571 48589 48593 48611 48619 48623 48647 48649 48661 48673 48677 48679 48731 48733 48751 48757 48761 48767 48779 48781 48787 48799 48809 48817 48821 48823 48847 48857 48859 48869 48871 48883 48889 48907 48947 48953 48973 48989 48991 49003 49009 49019 49031 49033 49037 49043 49057 49069 49081 49103 49109 49117 49121 49123 49139 49157 49169 49171 49177 49193 49199 49201 49207 49211 49223 49253 49261 49277 49279 49297 49307 49331 49333 49339 49363 49367 49369 49391 49393 49409 49411 49417 49429 49433 49451 49459 49463 49477 49481 49499 49523 49529 49531 49537 49547 49549 49559 49597 49603 49613 49627 49633 49639 49663 49667 49669 49681 49697 49711 49727 49739 49741 49747 49757 49783 49787 49789 49801 49807 49811 49823 49831 49843 49853 49871 49877 49891 49919 49921 49927 49937 49939 49943 49957 49991 49993 49999 50021 50023 50033 50047 50051 50053 50069 50077 50087 50093 50101 50111 50119 50123 50129 50131 50147 50153 50159 50177 50207 50221 50227 50231 50261 50263 50273 50287 50291 50311 50321 50329 50333 50341 50359 50363 50377 50383 50387 50411 50417 50423 50441 50459 50461 50497 50503 50513 50527 50539 50543 50549 50551 50581 50587 50591 50593 50599 50627 50647 50651 50671 50683 50707 50723 50741 50753 50767 50773 50777 50789 50821 50833 50839 50849 50857 50867 50873 50891 50893 50909 50923 50929 50951 50957 50969 50971 50989 50993 51001 51031 51043 51047 51059 51061 51071 51109 51131 51133 51137 51151 51157 51169 51193 51197 51199 51203 51217 51229 51239 51241 51257 51263 51283 51287 51307 51329 51341 51343 51347 51349 51361 51383 51407 51413 51419 51421 51427 51431 51437 51439 51449 51461 51473 51479 51481 51487 51503 51511 51517 51521 51539 51551 51563 51577 51581 51593 51599 51607 51613 51631 51637 51647 51659 51673 51679 51683 51691 51713 51719 51721 51749 51767 51769 51787 51797 51803 51817 51827 51829 51839 51853 51859 51869 51871 51893 51899 51907 51913 51929 51941 51949 51971 51973 51977 51991 52009 52021 52027 52051 52057 52067 52069 52081 52103 52121 52127 52147 52153 52163 52177 52181 52183 52189 52201 52223 52237 52249 52253 52259 52267 52289 52291 52301 52313 52321 52361 52363 52369 52379 52387 52391 52433 52453 52457 52489 52501 52511 52517 52529 52541 52543 52553 52561 52567 52571 52579 52583 52609 52627 52631 52639 52667 52673 52691 52697 52709 52711 52721 52727 52733 52747 52757 52769 52783 52807 52813 52817 52837 52859 52861 52879 52883 52889 52901 52903 52919 52937 52951 52957 52963 52967 52973 52981 52999 53003 53017 53047 53051 53069 53077 53087 53089 53093 53101 53113 53117 53129 53147 53149 53161 53171 53173 53189 53197 53201 53231 53233 53239 53267 53269 53279 53281 53299 53309 53323 53327 53353 53359 53377 53381 53401 53407 53411 53419 53437 53441 53453 53479 53503 53507 53527 53549 53551 53569 53591 53593 53597 53609 53611 53617 53623 53629 53633 53639 53653 53657 53681 53693 53699 53717 53719 53731 53759 53773 53777 53783 53791 53813 53819 53831 53849 53857 53861 53881 53887 53891 53897 53899 53917 53923 53927 53939 53951 53959 53987 53993 54001 54011 54013 54037 54049 54059 54083 54091 54101 54121 54133 54139 54151 54163 54167 54181 54193 54217 54251 54269 54277 54287 54293 54311 54319 54323 54331 54347 54361 54367 54371 54377 54401 54403 54409 54413 54419 54421 54437 54443 54449 54469 54493 54497 54499 54503 54517 54521 54539 54541 54547 54559 54563 54577 54581 54583 54601 54617 54623 54629 54631 54647 54667 54673 54679 54709 54713 54721 54727 54751 54767 54773 54779 54787 54799 54829 54833 54851 54869 54877 54881 54907 54917 54919 54941 54949 54959 54973 54979 54983 55001 55009 55021 55049 55051 55057 55061 55073 55079 55103 55109 55117 55127 55147 55163 55171 55201 55207 55213 55217 55219 55229 55243 55249 55259 55291 55313 55331 55333 55337 55339 55343 55351 55373 55381 55399 55411 55439 55441 55457 55469 55487 55501 55511 55529 55541 55547 55579 55589 55603 55609 55619 55621 55631 55633 55639 55661 55663 55667 55673 55681 55691 55697 55711 55717 55721 55733 55763 55787 55793 55799 55807 55813 55817 55819 55823 55829 55837 55843 55849 55871 55889 55897 55901 55903 55921 55927 55931 55933 55949 55967 55987 55997 56003 56009 56039 56041 56053 56081 56087 56093 56099 56101 56113 56123 56131 56149 56167 56171 56179 56197 56207 56209 56237 56239 56249 56263 56267 56269 56299 56311 56333 56359 56369 56377 56383 56393 56401 56417 56431 56437 56443 56453 56467 56473 56477 56479 56489 56501 56503 56509 56519 56527 56531 56533 56543 56569 56591 56597 56599 56611 56629 56633 56659 56663 56671 56681 56687 56701 56711 56713 56731 56737 56747 56767 56773 56779 56783 56807 56809 56813 56821 56827 56843 56857 56873 56891 56893 56897 56909 56911 56921 56923 56929 56941 56951 56957 56963 56983 56989 56993 56999 57037 57041 57047 57059 57073 57077 57089 57097 57107 57119 57131 57139 57143 57149 57163 57173 57179 57191 57193 57203 57221 57223 57241 57251 57259 57269 57271 57283 57287 57301 57329 57331 57347 57349 57367 57373 57383 57389 57397 57413 57427 57457 57467 57487 57493 57503 57527 57529 57557 57559 57571 57587 57593 57601 57637 57641 57649 57653 57667 57679 57689 57697 57709 57713 57719 57727 57731 57737 57751 57773 57781 57787 57791 57793 57803 57809 57829 57839 57847 57853 57859 57881 57899 57901 57917 57923 57943 57947 57973 57977 57991 58013 58027 58031 58043 58049 58057 58061 58067 58073 58099 58109 58111 58129 58147 58151 58153 58169 58171 58189 58193 58199 58207 58211 58217 58229 58231 58237 58243 58271 58309 58313 58321 58337 58363 58367 58369 58379 58391 58393 58403 58411 58417 58427 58439 58441 58451 58453 58477 58481 58511 58537 58543 58549 58567 58573 58579 58601 58603 58613 58631 58657 58661 58679 58687 58693 58699 58711 58727 58733 58741 58757 58763 58771 58787 58789 58831 58889 58897 58901 58907 58909 58913 58921 58937 58943 58963 58967 58979 58991 58997 59009 59011 59021 59023 59029 59051 59053 59063 59069 59077 59083 59093 59107 59113 59119 59123 59141 59149 59159 59167 59183 59197 59207 59209 59219 59221 59233 59239 59243 59263 59273 59281 59333 59341 59351 59357 59359 59369 59377 59387 59393 59399 59407 59417 59419 59441 59443 59447 59453 59467 59471 59473 59497 59509 59513 59539 59557 59561 59567 59581 59611 59617 59621 59627 59629 59651 59659 59663 59669 59671 59693 59699 59707 59723 59729 59743 59747 59753 59771 59779 59791 59797 59809 59833 59863 59879 59887 59921 59929 59951 59957 59971 59981 59999 60013 60017 60029 60037 60041 60077 60083 60089 60091 60101 60103 60107 60127 60133 60139 60149 60161 60167 60169 60209 60217 60223 60251 60257 60259 60271 60289 60293 60317 60331 60337 60343 60353 60373 60383 60397 60413 60427 60443 60449 60457 60493 60497 60509 60521 60527 60539 60589 60601 60607 60611 60617 60623 60631 60637 60647 60649 60659 60661 60679 60689 60703 60719 60727 60733 60737 60757 60761 60763 60773 60779 60793 60811 60821 60859 60869 60887 60889 60899 60901 60913 60917 60919 60923 60937 60943 60953 60961 61001 61007 61027 61031 61043 61051 61057 61091 61099 61121 61129 61141 61151 61153 61169 61211 61223 61231 61253 61261 61283 61291 61297 61331 61333 61339 61343 61357 61363 61379 61381 61403 61409 61417 61441 61463 61469 61471 61483 61487 61493 61507 61511 61519 61543 61547 61553 61559 61561 61583 61603 61609 61613 61627 61631 61637 61643 61651 61657 61667 61673 61681 61687 61703 61717 61723 61729 61751 61757 61781 61813 61819 61837 61843 61861 61871 61879 61909 61927 61933 61949 61961 61967 61979 61981 61987 61991 62003 62011 62017 62039 62047 62053 62057 62071 62081 62099 62119 62129 62131 62137 62141 62143 62171 62189 62191 62201 62207 62213 62219 62233 62273 62297 62299 62303 62311 62323 62327 62347 62351 62383 62401 62417 62423 62459 62467 62473 62477 62483 62497 62501 62507 62533 62539 62549 62563 62581 62591 62597 62603 62617 62627 62633 62639 62653 62659 62683 62687 62701 62723 62731 62743 62753 62761 62773 62791 62801 62819 62827 62851 62861 62869 62873 62897 62903 62921 62927 62929 62939 62969 62971 62981 62983 62987 62989 63029 63031 63059 63067 63073 63079 63097 63103 63113 63127 63131 63149 63179 63197 63199 63211 63241 63247 63277 63281 63299 63311 63313 63317 63331 63337 63347 63353 63361 63367 63377 63389 63391 63397 63409 63419 63421 63439 63443 63463 63467 63473 63487 63493 63499 63521 63527 63533 63541 63559 63577 63587 63589 63599 63601 63607 63611 63617 63629 63647 63649 63659 63667 63671 63689 63691 63697 63703 63709 63719 63727 63737 63743 63761 63773 63781 63793 63799 63803 63809 63823 63839 63841 63853 63857 63863 63901 63907 63913 63929 63949 63977 63997 64007 64013 64019 64033 64037 64063 64067 64081 64091 64109 64123 64151 64153 64157 64171 64187 64189 64217 64223 64231 64237 64271 64279 64283 64301 64303 64319 64327 64333 64373 64381 64399 64403 64433 64439 64451 64453 64483 64489 64499 64513 64553 64567 64577 64579 64591 64601 64609 64613 64621 64627 64633 64661 64663 64667 64679 64693 64709 64717 64747 64763 64781 64783 64793 64811 64817 64849 64853 64871 64877 64879 64891 64901 64919 64921 64927 64937 64951 64969 64997 65003 65011 65027 65029 65033 65053 65063 65071 65089 65099 65101 65111 65119 65123 65129 65141 65147 65167 65171 65173 65179 65183 65203 65213 65239 65257 65267 65269 65287 65293 65309 65323 65327 65353 65357 65371 65381 65393 65407 65413 65419 65423 65437 65447 65449 65479 65497 65519 65521 65537 65539 65543 65551 65557 65563 65579 65581 65587 65599 65609 65617 65629 65633 65647 65651 65657 65677 65687 65699 65701 65707 65713 65717 65719 65729 65731 65761 65777 65789 65809 65827 65831 65837 65839 65843 65851 65867 65881 65899 65921 65927 65929 65951 65957 65963 65981 65983 65993 66029 66037 66041 66047 66067 66071 66083 66089 66103 66107 66109 66137 66161 66169 66173 66179 66191 66221 66239 66271 66293 66301 66337 66343 66347 66359 66361 66373 66377 66383 66403 66413 66431 66449 66457 66463 66467 66491 66499 66509 66523 66529 66533 66541 66553 66569 66571 66587 66593 66601 66617 66629 66643 66653 66683 66697 66701 66713 66721 66733 66739 66749 66751 66763 66791 66797 66809 66821 66841 66851 66853 66863 66877 66883 66889 66919 66923 66931 66943 66947 66949 66959 66973 66977 67003 67021 67033 67043 67049 67057 67061 67073 67079 67103 67121 67129 67139 67141 67153 67157 67169 67181 67187 67189 67211 67213 67217 67219 67231 67247 67261 67271 67273 67289 67307 67339 67343 67349 67369 67391 67399 67409 67411 67421 67427 67429 67433 67447 67453 67477 67481 67489 67493 67499 67511 67523 67531 67537 67547 67559 67567 67577 67579 67589 67601 67607 67619 67631 67651 67679 67699 67709 67723 67733 67741 67751 67757 67759 67763 67777 67783 67789 67801 67807 67819 67829 67843 67853 67867 67883 67891 67901 67927 67931 67933 67939 67943 67957 67961 67967 67979 67987 67993 68023 68041 68053 68059 68071 68087 68099 68111 68113 68141 68147 68161 68171 68207 68209 68213 68219 68227 68239 68261 68279 68281 68311 68329 68351 68371 68389 68399 68437 68443 68447 68449 68473 68477 68483 68489 68491 68501 68507 68521 68531 68539 68543 68567 68581 68597 68611 68633 68639 68659 68669 68683 68687 68699 68711 68713 68729 68737 68743 68749 68767 68771 68777 68791 68813 68819 68821 68863 68879 68881 68891 68897 68899 68903 68909 68917 68927 68947 68963 68993 69001 69011 69019 69029 69031 69061 69067 69073 69109 69119 69127 69143 69149 69151 69163 69191 69193 69197 69203 69221 69233 69239 69247 69257 69259 69263 69313 69317 69337 69341 69371 69379 69383 69389 69401 69403 69427 69431 69439 69457 69463 69467 69473 69481 69491 69493 69497 69499 69539 69557 69593 69623 69653 69661 69677 69691 69697 69709 69737 69739 69761 69763 69767 69779 69809 69821 69827 69829 69833 69847 69857 69859 69877 69899 69911 69929 69931 69941 69959 69991 69997 70001 70003 70009 70019 70039 70051 70061 70067 70079 70099 70111 70117 70121 70123 70139 70141 70157 70163 70177 70181 70183 70199 70201 70207 70223 70229 70237 70241 70249 70271 70289 70297 70309 70313 70321 70327 70351 70373 70379 70381 70393 70423 70429 70439 70451 70457 70459 70481 70487 70489 70501 70507 70529 70537 70549 70571 70573 70583 70589 70607 70619 70621 70627 70639 70657 70663 70667 70687 70709 70717 70729 70753 70769 70783 70793 70823 70841 70843 70849 70853 70867 70877 70879 70891 70901 70913 70919 70921 70937 70949 70951 70957 70969 70979 70981 70991 70997 70999 71011 71023 71039 71059 71069 71081 71089 71119 71129 71143 71147 71153 71161 71167 71171 71191 71209 71233 71237 71249 71257 71261 71263 71287 71293 71317 71327 71329 71333 71339 71341 71347 71353 71359 71363 71387 71389 71399 71411 71413 71419 71429 71437 71443 71453 71471 71473 71479 71483 71503 71527 71537 71549 71551 71563 71569 71593 71597 71633 71647 71663 71671 71693 71699 71707 71711 71713 71719 71741 71761 71777 71789 71807 71809 71821 71837 71843 71849 71861 71867 71879 71881 71887 71899 71909 71917 71933 71941 71947 71963 71971 71983 71987 71993 71999 72019 72031 72043 72047 72053 72073 72077 72089 72091 72101 72103 72109 72139 72161 72167 72169 72173 72211 72221 72223 72227 72229 72251 72253 72269 72271 72277 72287 72307 72313 72337 72341 72353 72367 72379 72383 72421 72431 72461 72467 72469 72481 72493 72497 72503 72533 72547 72551 72559 72577 72613 72617 72623 72643 72647 72649 72661 72671 72673 72679 72689 72701 72707 72719 72727 72733 72739 72763 72767 72797 72817 72823 72859 72869 72871 72883 72889 72893 72901 72907 72911 72923 72931 72937 72949 72953 72959 72973 72977 72997 73009 73013 73019 73037 73039 73043 73061 73063 73079 73091 73121 73127 73133 73141 73181 73189 73237 73243 73259 73277 73291 73303 73309 73327 73331 73351 73361 73363 73369 73379 73387 73417 73421 73433 73453 73459 73471 73477 73483 73517 73523 73529 73547 73553 73561 73571 73583 73589 73597 73607 73609 73613 73637 73643 73651 73673 73679 73681 73693 73699 73709 73721 73727 73751 73757 73771 73783 73819 73823 73847 73849 73859 73867 73877 73883 73897 73907 73939 73943 73951 73961 73973 73999 74017 74021 74027 74047 74051 74071 74077 74093 74099 74101 74131 74143 74149 74159 74161 74167 74177 74189 74197 74201 74203 74209 74219 74231 74257 74279 74287 74293 74297 74311 74317 74323 74353 74357 74363 74377 74381 74383 74411 74413 74419 74441 74449 74453 74471 74489 74507 74509 74521 74527 74531 74551 74561 74567 74573 74587 74597 74609 74611 74623 74653 74687 74699 74707 74713 74717 74719 74729 74731 74747 74759 74761 74771 74779 74797 74821 74827 74831 74843 74857 74861 74869 74873 74887 74891 74897 74903 74923 74929 74933 74941 74959 75011 75013 75017 75029 75037 75041 75079 75083 75109 75133 75149 75161 75167 75169 75181 75193 75209 75211 75217 75223 75227 75239 75253 75269 75277 75289 75307 75323 75329 75337 75347 75353 75367 75377 75389 75391 75401 75403 75407 75431 75437 75479 75503 75511 75521 75527 75533 75539 75541 75553 75557 75571 75577 75583 75611 75617 75619 75629 75641 75653 75659 75679 75683 75689 75703 75707 75709 75721 75731 75743 75767 75773 75781 75787 75793 75797 75821 75833 75853 75869 75883 75913 75931 75937 75941 75967 75979 75983 75989 75991 75997 76001 76003 76031 76039 76079 76081 76091 76099 76103 76123 76129 76147 76157 76159 76163 76207 76213 76231 76243 76249 76253 76259 76261 76283 76289 76303 76333 76343 76367 76369 76379 76387 76403 76421 76423 76441 76463 76471 76481 76487 76493 76507 76511 76519 76537 76541 76543 76561 76579 76597 76603 76607 76631 76649 76651 76667 76673 76679 76697 76717 76733 76753 76757 76771 76777 76781 76801 76819 76829 76831 76837 76847 76871 76873 76883 76907 76913 76919 76943 76949 76961 76963 76991 77003 77017 77023 77029 77041 77047 77069 77081 77093 77101 77137 77141 77153 77167 77171 77191 77201 77213 77237 77239 77243 77249 77261 77263 77267 77269 77279 77291 77317 77323 77339 77347 77351 77359 77369 77377 77383 77417 77419 77431 77447 77471 77477 77479 77489 77491 77509 77513 77521 77527 77543 77549 77551 77557 77563 77569 77573 77587 77591 77611 77617 77621 77641 77647 77659 77681 77687 77689 77699 77711 77713 77719 77723 77731 77743 77747 77761 77773 77783 77797 77801 77813 77839 77849 77863 77867 77893 77899 77929 77933 77951 77969 77977 77983 77999 78007 78017 78031 78041 78049 78059 78079 78101 78121 78137 78139 78157 78163 78167 78173 78179 78191 78193 78203 78229 78233 78241 78259 78277 78283 78301 78307 78311 78317 78341 78347 78367 78401 78427 78437 78439 78467 78479 78487 78497 78509 78511 78517 78539 78541 78553 78569 78571 78577 78583 78593 78607 78623 78643 78649 78653 78691 78697 78707 78713 78721 78737 78779 78781 78787 78791 78797 78803 78809 78823 78839 78853 78857 78877 78887 78889 78893 78901 78919 78929 78941 78977 78979 78989 79031 79039 79043 79063 79087 79103 79111 79133 79139 79147 79151 79153 79159 79181 79187 79193 79201 79229 79231 79241 79259 79273 79279 79283 79301 79309 79319 79333 79337 79349 79357 79367 79379 79393 79397 79399 79411 79423 79427 79433 79451 79481 79493 79531 79537 79549 79559 79561 79579 79589 79601 79609 79613 79621 79627 79631 79633 79657 79669 79687 79691 79693 79697 79699 79757 79769 79777 79801 79811 79813 79817 79823 79829 79841 79843 79847 79861 79867 79873 79889 79901 79903 79907 79939 79943 79967 79973 79979 79987 79997 79999 80021 80039 80051 80071 80077 80107 80111 80141 80147 80149 80153 80167 80173 80177 80191 80207 80209 80221 80231 80233 80239 80251 80263 80273 80279 80287 80309 80317 80329 80341 80347 80363 80369 80387 80407 80429 80447 80449 80471 80473 80489 80491 80513 80527 80537 80557 80567 80599 80603 80611 80621 80627 80629 80651 80657 80669 80671 80677 80681 80683 80687 80701 80713 80737 80747 80749 80761 80777 80779 80783 80789 80803 80809 80819 80831 80833 80849 80863 80897 80909 80911 80917 80923 80929 80933 80953 80963 80989 81001 81013 81017 81019 81023 81031 81041 81043 81047 81049 81071 81077 81083 81097 81101 81119 81131 81157 81163 81173 81181 81197 81199 81203 81223 81233 81239 81281 81283 81293 81299 81307 81331 81343 81349 81353 81359 81371 81373 81401 81409 81421 81439 81457 81463 81509 81517 81527 81533 81547 81551 81553 81559 81563 81569 81611 81619 81629 81637 81647 81649 81667 81671 81677 81689 81701 81703 81707 81727 81737 81749 81761 81769 81773 81799 81817 81839 81847 81853 81869 81883 81899 81901 81919 81929 81931 81937 81943 81953 81967 81971 81973 82003 82007 82009 82013 82021 82031 82037 82039 82051 82067 82073 82129 82139 82141 82153 82163 82171 82183 82189 82193 82207 82217 82219 82223 82231 82237 82241 82261 82267 82279 82301 82307 82339 82349 82351 82361 82373 82387 82393 82421 82457 82463 82469 82471 82483 82487 82493 82499 82507 82529 82531 82549 82559 82561 82567 82571 82591 82601 82609 82613 82619 82633 82651 82657 82699 82721 82723 82727 82729 82757 82759 82763 82781 82787 82793 82799 82811 82813 82837 82847 82883 82889 82891 82903 82913 82939 82963 82981 82997 83003 83009 83023 83047 83059 83063 83071 83077 83089 83093 83101 83117 83137 83177 83203 83207 83219 83221 83227 83231 83233 83243 83257 83267 83269 83273 83299 83311 83339 83341 83357 83383 83389 83399 83401 83407 83417 83423 83431 83437 83443 83449 83459 83471 83477 83497 83537 83557 83561 83563 83579 83591 83597 83609 83617 83621 83639 83641 83653 83663 83689 83701 83717 83719 83737 83761 83773 83777 83791 83813 83833 83843 83857 83869 83873 83891 83903 83911 83921 83933 83939 83969 83983 83987 84011 84017 84047 84053 84059 84061 84067 84089 84121 84127 84131 84137 84143 84163 84179 84181 84191 84199 84211 84221 84223 84229 84239 84247 84263 84299 84307 84313 84317 84319 84347 84349 84377 84389 84391 84401 84407 84421 84431 84437 84443 84449 84457 84463 84467 84481 84499 84503 84509 84521 84523 84533 84551 84559 84589 84629 84631 84649 84653 84659 84673 84691 84697 84701 84713 84719 84731 84737 84751 84761 84787 84793 84809 84811 84827 84857 84859 84869 84871 84913 84919 84947 84961 84967 84977 84979 84991 85009 85021 85027 85037 85049 85061 85081 85087 85091 85093 85103 85109 85121 85133 85147 85159 85193 85199 85201 85213 85223 85229 85237 85243 85247 85259 85297 85303 85313 85331 85333 85361 85363 85369 85381 85411 85427 85429 85439 85447 85451 85453 85469 85487 85513 85517 85523 85531 85549 85571 85577 85597 85601 85607 85619 85621 85627 85639 85643 85661 85667 85669 85691 85703 85711 85717 85733 85751 85781 85793 85817 85819 85829 85831 85837 85843 85847 85853 85889 85903 85909 85931 85933 85991 85999 86011 86017 86027 86029 86069 86077 86083 86111 86113 86117 86131 86137 86143 86161 86171 86179 86183 86197 86201 86209 86239 86243 86249 86257 86263 86269 86287 86291 86293 86297 86311 86323 86341 86351 86353 86357 86369 86371 86381 86389 86399 86413 86423 86441 86453 86461 86467 86477 86491 86501 86509 86531 86533 86539 86561 86573 86579 86587 86599 86627 86629 86677 86689 86693 86711 86719 86729 86743 86753 86767 86771 86783 86813 86837 86843 86851 86857 86861 86869 86923 86927 86929 86939 86951 86959 86969 86981 86993 87011 87013 87037 87041 87049 87071 87083 87103 87107 87119 87121 87133 87149 87151 87179 87181 87187 87211 87221 87223 87251 87253 87257 87277 87281 87293 87299 87313 87317 87323 87337 87359 87383 87403 87407 87421 87427 87433 87443 87473 87481 87491 87509 87511 87517 87523 87539 87541 87547 87553 87557 87559 87583 87587 87589 87613 87623 87629 87631 87641 87643 87649 87671 87679 87683 87691 87697 87701 87719 87721 87739 87743 87751 87767 87793 87797 87803 87811 87833 87853 87869 87877 87881 87887 87911 87917 87931 87943 87959 87961 87973 87977 87991 88001 88003 88007 88019 88037 88069 88079 88093 88117 88129 88169 88177 88211 88223 88237 88241 88259 88261 88289 88301 88321 88327 88337 88339 88379 88397 88411 88423 88427 88463 88469 88471 88493 88499 88513 88523 88547 88589 88591 88607 88609 88643 88651 88657 88661 88663 88667 88681 88721 88729 88741 88747 88771 88789 88793 88799 88801 88807 88811 88813 88817 88819 88843 88853 88861 88867 88873 88883 88897 88903 88919 88937 88951 88969 88993 88997 89003 89009 89017 89021 89041 89051 89057 89069 89071 89083 89087 89101 89107 89113 89119 89123 89137 89153 89189 89203 89209 89213 89227 89231 89237 89261 89269 89273 89293 89303 89317 89329 89363 89371 89381 89387 89393 89399 89413 89417 89431 89443 89449 89459 89477 89491 89501 89513 89519 89521 89527 89533 89561 89563 89567 89591 89597 89599 89603 89611 89627 89633 89653 89657 89659 89669 89671 89681 89689 89753 89759 89767 89779 89783 89797 89809 89819 89821 89833 89839 89849 89867 89891 89897 89899 89909 89917 89923 89939 89959 89963 89977 89983 89989 90001 90007 90011 90017 90019 90023 90031 90053 90059 90067 90071 90073 90089 90107 90121 90127 90149 90163 90173 90187 90191 90197 90199 90203 90217 90227 90239 90247 90263 90271 90281 90289 90313 90353 90359 90371 90373 90379 90397 90401 90403 90407 90437 90439 90469 90473 90481 90499 90511 90523 90527 90529 90533 90547 90583 90599 90617 90619 90631 90641 90647 90659 90677 90679 90697 90703 90709 90731 90749 90787 90793 90803 90821 90823 90833 90841 90847 90863 90887 90901 90907 90911 90917 90931 90947 90971 90977 90989 90997 91009 91019 91033 91079 91081 91097 91099 91121 91127 91129 91139 91141 91151 91153 91159 91163 91183 91193 91199 91229 91237 91243 91249 91253 91283 91291 91297 91303 91309 91331 91367 91369 91373 91381 91387 91393 91397 91411 91423 91433 91453 91457 91459 91463 91493 91499 91513 91529 91541 91571 91573 91577 91583 91591 91621 91631 91639 91673 91691 91703 91711 91733 91753 91757 91771 91781 91801 91807 91811 91813 91823 91837 91841 91867 91873 91909 91921 91939 91943 91951 91957 91961 91967 91969 91997 92003 92009 92033 92041 92051 92077 92083 92107 92111 92119 92143 92153 92173 92177 92179 92189 92203 92219 92221 92227 92233 92237 92243 92251 92269 92297 92311 92317 92333 92347 92353 92357 92363 92369 92377 92381 92383 92387 92399 92401 92413 92419 92431 92459 92461 92467 92479 92489 92503 92507 92551 92557 92567 92569 92581 92593 92623 92627 92639 92641 92647 92657 92669 92671 92681 92683 92693 92699 92707 92717 92723 92737 92753 92761 92767 92779 92789 92791 92801 92809 92821 92831 92849 92857 92861 92863 92867 92893 92899 92921 92927 92941 92951 92957 92959 92987 92993 93001 93047 93053 93059 93077 93083 93089 93097 93103 93113 93131 93133 93139 93151 93169 93179 93187 93199 93229 93239 93241 93251 93253 93257 93263 93281 93283 93287 93307 93319 93323 93329 93337 93371 93377 93383 93407 93419 93427 93463 93479 93481 93487 93491 93493 93497 93503 93523 93529 93553 93557 93559 93563 93581 93601 93607 93629 93637 93683 93701 93703 93719 93739 93761 93763 93787 93809 93811 93827 93851 93871 93887 93889 93893 93901 93911 93913 93923 93937 93941 93949 93967 93971 93979 93983 93997 94007 94009 94033 94049 94057 94063 94079 94099 94109 94111 94117 94121 94151 94153 94169 94201 94207 94219 94229 94253 94261 94273 94291 94307 94309 94321 94327 94331 94343 94349 94351 94379 94397 94399 94421 94427 94433 94439 94441 94447 94463 94477 94483 94513 94529 94531 94541 94543 94547 94559 94561 94573 94583 94597 94603 94613 94621 94649 94651 94687 94693 94709 94723 94727 94747 94771 94777 94781 94789 94793 94811 94819 94823 94837 94841 94847 94849 94873 94889 94903 94907 94933 94949 94951 94961 94993 94999 95003 95009 95021 95027 95063 95071 95083 95087 95089 95093 95101 95107 95111 95131 95143 95153 95177 95189 95191 95203 95213 95219 95231 95233 95239 95257 95261 95267 95273 95279 95287 95311 95317 95327 95339 95369 95383 95393 95401 95413 95419 95429 95441 95443 95461 95467 95471 95479 95483 95507 95527 95531 95539 95549 95561 95569 95581 95597 95603 95617 95621 95629 95633 95651 95701 95707 95713 95717 95723 95731 95737 95747 95773 95783 95789 95791 95801 95803 95813 95819 95857 95869 95873 95881 95891 95911 95917 95923 95929 95947 95957 95959 95971 95987 95989 96001 96013 96017 96043 96053 96059 96079 96097 96137 96149 96157 96167 96179 96181 96199 96211 96221 96223 96233 96259 96263 96269 96281 96289 96293 96323 96329 96331 96337 96353 96377 96401 96419 96431 96443 96451 96457 96461 96469 96479 96487 96493 96497 96517 96527 96553 96557 96581 96587 96589 96601 96643 96661 96667 96671 96697 96703 96731 96737 96739 96749 96757 96763 96769 96779 96787 96797 96799 96821 96823 96827 96847 96851 96857 96893 96907 96911 96931 96953 96959 96973 96979 96989 96997 97001 97003 97007 97021 97039 97073 97081 97103 97117 97127 97151 97157 97159 97169 97171 97177 97187 97213 97231 97241 97259 97283 97301 97303 97327 97367 97369 97373 97379 97381 97387 97397 97423 97429 97441 97453 97459 97463 97499 97501 97511 97523 97547 97549 97553 97561 97571 97577 97579 97583 97607 97609 97613 97649 97651 97673 97687 97711 97729 97771 97777 97787 97789 97813 97829 97841 97843 97847 97849 97859 97861 97871 97879 97883 97919 97927 97931 97943 97961 97967 97973 97987 98009 98011 98017 98041 98047 98057 98081 98101 98123 98129 98143 98179 98207 98213 98221 98227 98251 98257 98269 98297 98299 98317 98321 98323 98327 98347 98369 98377 98387 98389 98407 98411 98419 98429 98443 98453 98459 98467 98473 98479 98491 98507 98519 98533 98543 98561 98563 98573 98597 98621 98627 98639 98641 98663 98669 98689 98711 98713 98717 98729 98731 98737 98773 98779 98801 98807 98809 98837 98849 98867 98869 98873 98887 98893 98897 98899 98909 98911 98927 98929 98939 98947 98953 98963 98981 98993 98999 99013 99017 99023 99041 99053 99079 99083 99089 99103 99109 99119 99131 99133 99137 99139 99149 99173 99181 99191 99223 99233 99241 99251 99257 99259 99277 99289 99317 99347 99349 99367 99371 99377 99391 99397 99401 99409 99431 99439 99469 99487 99497 99523 99527 99529 99551 99559 99563 99571 99577 99581 99607 99611 99623 99643 99661 99667 99679 99689 99707 99709 99713 99719 99721 99733 99761 99767 99787 99793 99809 99817 99823 99829 99833 99839 99859 99871 99877 99881 99901 99907 99923 99929 99961 99971 99989 99991 100003 100019 100043 100049 100057 100069 100103 100109 100129 100151 100153 100169 100183 100189 100193 100207 100213 100237 100267 100271 100279 100291 100297 100313 100333 100343 100357 100361 100363 100379 100391 100393 100403 100411 100417 100447 100459 100469 100483 100493 100501 100511 100517 100519 100523 100537 100547 100549 100559 100591 100609 100613 100621 100649 100669 100673 100693 100699 100703 100733 100741 100747 100769 100787 100799 100801 100811 100823 100829 100847 100853 100907 100913 100927 100931 100937 100943 100957 100981 100987 100999 101009 101021 101027 101051 101063 101081 101089 101107 101111 101113 101117 101119 101141 101149 101159 101161 101173 101183 101197 101203 101207 101209 101221 101267 101273 101279 101281 101287 101293 101323 101333 101341 101347 101359 101363 101377 101383 101399 101411 101419 101429 101449 101467 101477 101483 101489 101501 101503 101513 101527 101531 101533 101537 101561 101573 101581 101599 101603 101611 101627 101641 101653 101663 101681 101693 101701 101719 101723 101737 101741 101747 101749 101771 101789 101797 101807 101833 101837 101839 101863 101869 101873 101879 101891 101917 101921 101929 101939 101957 101963 101977 101987 101999 102001 102013 102019 102023 102031 102043 102059 102061 102071 102077 102079 102101 102103 102107 102121 102139 102149 102161 102181 102191 102197 102199 102203 102217 102229 102233 102241 102251 102253 102259 102293 102299 102301 102317 102329 102337 102359 102367 102397 102407 102409 102433 102437 102451 102461 102481 102497 102499 102503 102523 102533 102539 102547 102551 102559 102563 102587 102593 102607 102611 102643 102647 102653 102667 102673 102677 102679 102701 102761 102763 102769 102793 102797 102811 102829 102841 102859 102871 102877 102881 102911 102913 102929 102931 102953 102967 102983 103001 103007 103043 103049 103067 103069 103079 103087 103091 103093 103099 103123 103141 103171 103177 103183 103217 103231 103237 103289 103291 103307 103319 103333 103349 103357 103387 103391 103393 103399 103409 103421 103423 103451 103457 103471 103483 103511 103529 103549 103553 103561 103567 103573 103577 103583 103591 103613 103619 103643 103651 103657 103669 103681 103687 103699 103703 103723 103769 103787 103801 103811 103813 103837 103841 103843 103867 103889 103903 103913 103919 103951 103963 103967 103969 103979 103981 103991 103993 103997 104003 104009 104021 104033 104047 104053 104059 104087 104089 104107 104113 104119 104123 104147 104149 104161 104173 104179 104183 104207 104231 104233 104239 104243 104281 104287 104297 104309 104311 104323 104327 104347 104369 104381 104383 104393 104399 104417 104459 104471 104473 104479 104491 104513 104527 104537 104543 104549 104551 104561 104579 104593 104597 104623 104639 104651 104659 104677 104681 104683 104693 104701 104707 104711 104717 104723 104729\n","id":2339,"permalink":"https://freshrimpsushi.github.io/jp/posts/2339/","tags":null,"title":"1万番目までの素数点以下のリスト"},{"categories":"줄리아","contents":"コード 最初に、生えび寿司レストランには詳しい説明が含まれているが、ジュリアは並列処理をどれだけ容易にできるかを強調するために、わざと説明を省略したいと思っている。\nusing Base.Threads\rfor i in 1:10\rprintln(i^2)\rend 上のループを並列処理したい場合は、for文の前に@threadsをつけるだけでいい。\n@threads for i in 1:10\rprintln(i^2)\rend でも、一言だけアドバイスをすると、並列処理をしても全てが速くなるわけではないということだ。並列処理をうまく使えば非常に高いパフォーマンスを出すことができるが、コードの書きやすさが向上したからといって最適化も簡単になるわけではない。時間を測ってみるなどして、実行時間に注意しよう。\n環境 OS: Windows julia: v1.5.0 ","id":1474,"permalink":"https://freshrimpsushi.github.io/jp/posts/1474/","tags":null,"title":"ジュリアでの並列処理の方法"},{"categories":"양자역학","contents":"ベクトルの一般化 線形代数学を学んでいない理科生にとって、ベクトルは大きさと方向を持つ物理量であり、3次元空間の点を意味し、一般に $\\vec{x} = (x_{1}, x_{2}, x_{3})$ のように表される。この定義で古典力学や電磁気学を学ぶ上では大きな問題はないだろう。しかし、量子力学ではフーリエ解析、関数の内積などの概念が登場するため、ベクトルの一般化された定義を知らないと学習に大きな困難を経験する可能性がある。\n線形代数学において、ベクトルとは我々が直感的に考えるそのベクトルを抽象化したものである。3次元空間のベクトルと同じ性質を持つものを全てベクトルと呼び、ベクトルを集めた集合をベクトル空間と呼ぶ。その性質とは、我々が3次元空間の点を考えた時に当然満たされるべき性質のことである。例えば\nベクトルとベクトルを加えたものもベクトルである。 ベクトルに定数を乗じたものもベクトルである。 などがそれにあたる。その結果、3次元空間の点はベクトルになり、3次元空間はベクトル空間になる。以下には量子力学で最も重要な二つの例を紹介する。行列と関数もベクトルである。\n例 行列 サイズが $m \\times n$ の行列を集めた集合を考えてみよう。これらを加えても依然として $m \\times n$ 行列であり、何らかの定数を乗じても依然として $m \\times n$ 行列なので、この集合はベクトル空間になり、各行列はベクトルになる。\n実際、$\\mathbf{x} = (x_{1}, x_{2}, x_{3})$ のように組み合わせで表記することと $\\mathbf{x} = \\begin{bmatrix} x_{1} \u0026amp; x_{2} \u0026amp; x_{3} \\end{bmatrix}$ のように $1 \\times 3$ 行列で表記することに本質的な違いがないことを思い出してみると、行列がベクトルであるということがより理解しやすくなるだろう。\n関数 連続関数の集合を考えてみよう。$f$ と $g$ が連続関数であれば、これらを加えた $f+g$ も依然として連続関数である。また、任意の定数を乗じた $cf$ も依然として連続関数である。したがって、連続関数の集合はベクトル空間になり、各連続関数はベクトルになる。\n実際、関数値が3次元ベクトルであるベクトル関数の場合、以下のように記述されることを思い出してみよう。\n$$ f(x,y,z) = (xy, yz, z^{2}) $$\n内積の一般化 内積はベクトルを扱う際に非常に便利に使われる演算である。ベクトルという概念を一般化したように、内積の概念も一般化してみよう。まず一般化された内積の表記では、点 $\\cdot$ の代わりに二重山括弧 $\\left\\langle \\ ,\\ \\right\\rangle$ を使用する。$\\mathbf{x} = \\left( x_{1}, x_{2}, x_{3} \\right)$、$\\mathbf{y}=\\left( y_{1}, y_{2}, y_{3} \\right)$ とすると、以下のように表記される。\n$$ \\mathbf{x} \\cdot \\mathbf{y} = x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} = \\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle $$\n量子力学では、中にコンマの代わりに線 $|$ を使用する。\n$$ \\mathbf{x} \\cdot \\mathbf{y} = \\braket{\\mathbf{x} \\vert \\mathbf{y} } $$\nこれをディラック記法という。ベクトルを一般化する際の核心は、「私たちがベクトルだと思うものが満たすべき性質」を満たすならば、それが何であれベクトルと呼ぶ点にある。内積の一般化でも同様に、「各成分を掛け合わせてすべて加算する」というコンセプトをそのまま保持する。どのようなベクトル空間を扱うかによって、内積の定義は以下のように異なる。\n例 行列 二つの行列 $A = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{21} \u0026amp; a_{22} \\end{pmatrix}, B = \\begin{pmatrix} b_{11} \u0026amp; b_{12} \\\\ b_{21} \u0026amp; b_{22} \\end{pmatrix}$ があるとする。これらの内積は、3次元ベクトルの内積と同じように「各成分の掛け算の和」として定義される。\n$$ \\braket{ A \\vert B } = a_{11}b_{11} + a_{12}b_{12} + a_{21}b_{21} + a_{22}b_{22} $$\n関数 上で述べたように、関数もベクトルであるので、二つの関数の内積も定義できる。関数の内積は以下のように定積分で定義される。\n$$ \\braket{\\psi \\vert \\phi} = \\int \\psi^{\\ast}(x) \\phi (x) dx $$\nここで、$\\psi^{\\ast}$ は $\\psi$ の複素共役を意味する。ただし、表記法にあいまいさがあるので注意しよう。関数の内積をなぜこのように定義するのかについては、\u0026lsquo;関数の内積を定積分で定義する理由\u0026rsquo;に詳しく説明されているので参照しよう。\n波動関数 量子力学において波動関数は、位置と時間に応じた粒子の状態を表現する関数であり、以下のように指数関数で表現される。\n$$ \\psi (x,t) = e^{i(kx + \\omega t)} $$\n主に $\\psi$ と $\\phi$ で表記され、それぞれ [プサイ]、[ファイ] と読む。$k$ は波数wave numberであり、運動量との関係 $p = \\hbar k$ を満たす。ここで $\\hbar$ は定数であるので、量子力学\nにおいては $k$ を運動量と同じと理解しても構わない。$\\omega$ は角振動数angular frequencyであり、エネルギーとの関係 $E = \\hbar \\omega$ を満たす。\nヒルベルト空間 ヒルベルト空間の厳密な定義は完備内積空間である。その数学的意味を理解しているともちろん良いが、物理学部の学生にとっては必須ではない。重要な点は、非常に良い性質を持つ集合にヒルベルト空間という名前を付けることと、波動関数を集めた集合がヒルベルト空間になるという点である。したがって、様々な良い数学的ツールを使って波動関数を扱うことができる。\n","id":1509,"permalink":"https://freshrimpsushi.github.io/jp/posts/1509/","tags":null,"title":"量子力学でベクトル、内積、波動関数, ヒルベルト空間"},{"categories":"수리통계학","contents":"定義1 確率変数$X_{1} , \\cdots , X_{n}$が次を満たすとき、$X_{1} , \\cdots , X_{n}$はペアワイズ独立と言われる。 $$ i \\ne j \\implies X_{i} \\perp X_{j} $$ 連続確率変数$X_{1} , \\cdots , X_{n}$の結合確率密度関数$f$が、それぞれの確率密度関数$f_{1} , \\cdots , f_{n}$に対して次を満たす場合、$X_{1} , \\cdots , X_{n}$は相互独立であると言う。 $$ f(x_{1} , \\cdots , x_{n} ) \\equiv f_{1} (x_{1}) \\cdots f_{n} (x_{n}) $$ 離散確率変数$X_{1} , \\cdots , X_{n}$の結合確率質量関数$p$が、それぞれの確率密度関数$p_{1} , \\cdots , p_{n}$に対して次を満たす場合、$X_{1} , \\cdots , X_{n}$は相互独立であると言う。 $$ p(x_{1} , \\cdots , x_{n} ) \\equiv p_{1} (x_{1}) \\cdots p_{n} (x_{n}) $$ 確率変数$X_{1} , \\cdots , X_{n}$が相互に独立であり、同じ分布を持つとき、iid（独立同分布）と呼ぶ。 説明 ペアワイズ独立の概念はそれ自体が重要であるというよりも、相互独立という望ましい条件を満たさない、より良くない条件というニュアンスを強く持つものである。自然と相互独立であればペアワイズにも独立であるが、その逆は成立しない。これをよく示す反例がベルンスタイン分布である。 iidは相互独立が数学的に扱いやすく、各々が同一という点で、数理統計学において重要な仮定として好まれる。例えば、その分布が$D$である場合、$X_{1} , \\cdots , X_{n}$を分布$D$に従うiid確率変数と言い、次のように表すこともできる。 $$ X_{1} , \\cdots , X_{n} \\overset{\\text{iid}}{\\sim} D $$ 定理 [1] 期待値: $X_{1} , \\cdots , X_{n}$が相互に独立である場合、それぞれに適用されるある関数$u_{1} , \\cdots , u_{n}$について $$ E \\left[ u_{1}(X_{1}) \\cdots u_{n}(X_{n}) \\right] = E \\left[ u_{1}(X_{1}) \\right] \\cdots E \\left[ u_{n}(X_{n}) \\right] $$ [2] モーメント生成関数: $X_{1} , \\cdots , X_{n}$が相互に独立であり、それぞれのモーメント生成関数が$M_{i}(t) \\qquad , -h_{i} \u0026lt; t \u0026lt; h_{i}$である場合、その線形組み合わせ$\\displaystyle T := \\sum_{i=1}^{n} a_{i} X_{i}$のモーメント生成関数は $$ M_{T} (t) = \\prod_{i=1}^{n} M_{i} \\left( a_{i} t \\right) \\qquad , -\\text{min}_{i=1, \\cdots, n} h_{i} \u0026lt; t \u0026lt; \\text{min}_{i=1, \\cdots, n} h_{i} $$ Hogg et al. (2013). Mathematical Statistics の導入 (第7版): p122~125.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1469,"permalink":"https://freshrimpsushi.github.io/jp/posts/1469/","tags":null,"title":"確率変数の独立性とiid"},{"categories":"수리통계학","contents":"定義 1 二つの確率変数 $X_{1}, X_{2}$ の結合確率密度関数 $f$ または確率質量関数 $p$ が、$X_{1}, X_{2}$ の確率密度関数 $f_{1}, f_{2}$ または確率質量関数 $p_{1}, p_{2}$ で以下を満たす場合、$X_{1}, X_{2}$ は独立であると言い、$X_{1} \\perp X_{2}$ と表記される。 $$ f(x_{1} , x_{2} ) \\equiv f_{1}(x_{1})f_{2}(x_{2}) \\\\ p(x_{1} , x_{2} ) \\equiv p_{1}(x_{1})p_{2}(x_{2}) $$\n定理 以下は、連続確率変数について言及されるが、便宜上離散確率変数についても同様である。\n以下は全て等価である。\n[1]: $X_{1} \\perp X_{2}$ [2] 確率密度関数： $$ f (x_{1} , x_{2}) \\equiv f_{1}(x_{1}) f_{2}(x_{2}) $$ [3] 累積分布関数：全ての $(x_{1} ,x_{2}) \\in \\mathbb{R}^{2}$ について $$ F (x_{1} , x_{2}) = F_{1}(x_{1}) F_{2}(x_{2}) $$ [4] 確率：全ての定数 $a\u0026lt;b$ および $c \u0026lt; d$ について $$ P(a \u0026lt; X_{1} \\le b, c \u0026lt; X_{2} \\le d) = P(a \u0026lt; X_{1} \\le b) P ( c \u0026lt; X_{2} \\le d) $$ [5] 期待値：$E \\left[ u (X_{1}) \\right]$ および $E \\left[ u (X_{2}) \\right]$ が存在する場合 $$ E \\left[ u(X_{1}) u(X_{2}) \\right] = E \\left[ u (X_{1}) \\right] E \\left[ u (X_{2}) \\right] $$ [6] モーメント生成関数：結合モーメント生成関数 $M(t_{1} , t_{2})$ が存在する場合 $$ M(t_{1} , t_{2}) = M (t_{1} , 0 ) M( 0, t_{2} ) $$ 説明 上記の等価条件の形から見て分かる通り、独立とは絡み合った（結合）関数を乗算形に分けることができる条件を言う。これは、確率を $$ P(A \\mid B) = {{ P(A B) } \\over { P(B) }} \\overset{\\text{ind}}{\\implies} P(AB) = P(A \\mid B) P(B) = P(A) P(B) $$ のように分けることができるイベントの独立を抽象化したものと見ることができる。独立を直感的に理解することも重要だが、数理統計学を学ぶ上でその数式的な形にもっと注意を払う必要がある。\n参照 測度論で定義される確率変数の独立 Hogg et al. (2013). Introduction to Mathematical Statistcs(第7版): p112.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1461,"permalink":"https://freshrimpsushi.github.io/jp/posts/1461/","tags":null,"title":"数理統計学における確率変数の独立"},{"categories":"최적화이론","contents":"定義 関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}$の関数値を最小にする$x^{ \\ast } = \\argmin_{x} f(x)$を見つける問題を最適化問題Optimization Problemと呼び、その問題を解くアルゴリズムを最適化技法と呼ぶ。最適化問題で与えられた関数$f$は特に目的関数Objective Functionと言う。 全ての$x$に対して$f(x^{ \\ast }) \\le f(x)$を満たす$x^{ \\ast }$を全域最適解Global Optimizerと呼ぶ。 全ての$x \\in \\mathcal{N}$に対して$f(x^{ \\ast }) \\le f(x)$を満たす$x^{ \\ast }$の近傍$\\mathcal{N}$が存在する場合、その$x^{ \\ast }$を局所最適解Local Optimizerと呼ぶ。 これらの定義では、不等式の向きが逆でも、つまり最大化について説明されても、それらは総じて最適化と呼ばれる。\n説明 最適化技法を使用する人々は「利益を最大化」または「コストを最小化」と言うことができるが、数学者にとっては最大か最小かは重要な問題ではない。最小化が最適化とほぼ同義とされる理由は、最小化問題で使用されるアルゴリズムが、$-1$を単に乗じることで最大化問題にも適用できるからであり、数学的に非常に重要な関数であるメトリックやノルムが$0$以上の実数の集合を値域に持つから、つまり最小値が存在するという理由からである。\n近年、ディープラーニングが流行しているが、目的関数（またはコスト関数、損失関数）は通常、スムーズであると想定されているが、必ずそうであるとは限らない。したがって、それを克服するためのアルゴリズムとメソッドも研究されてきた。目的関数の定義域が必ずしも$\\mathbb{R}^{n}$でなければならないわけではない。\n全域最適解 最適解の存在性は、数々の条件によって示すことができるかもしれないが、局所最適解が全域最適解であることを示す定理はない。理想的には誰もが最適解を見つけたいと思うが、実際に見つけた解が最適解であることを心から期待することはほとんどない。最適化問題は多くあるが、すべての問題にピッタリ合う「最適化された」最適化技法はないため、数多くの改善アルゴリズムが開発されてきた。\n最適解の厳密さと孤立性 通常は以下の定義は無意味であるが、一応言及しておく。\n全ての$x \\in \\mathcal{N}$に対して$f(x^{ \\ast }) \u0026lt; f(x)$を満たす$x^{ \\ast }$の近傍$\\mathcal{N}$が存在する場合、その$x^{ \\ast }$を厳密な局所最適解Strict Local Optimizerと呼ぶ。 全ての$x \\in \\mathcal{N}$に対して$f(x^{ \\ast }) \u0026lt; f(x)$を満たす$x^{ \\ast }$の唯一の近傍$\\mathcal{N}$が存在する場合、その$x^{ \\ast }$を孤立した局所最適解Isolated Local Optimizerと呼ぶ。 ","id":1463,"permalink":"https://freshrimpsushi.github.io/jp/posts/1463/","tags":null,"title":"数学における最適化技術"},{"categories":"수리통계학","contents":"定義 離散確率変数$X_{1}, X_{2}, \\cdots , X_{n}$に対し、次の$p_{2, \\cdots , n \\mid 1}$を$X_{1} = x_{1}$が与えられた時の$ X_{2}, \\cdots , X_{n}$の結合条件付き確率質量関数という。 $$ p_{2, \\cdots , n \\mid 1} ( x_{2} , \\cdots ,x_{n} \\mid X_{1} = x_{1} ) = {{ p_{1, \\cdots , n}(x_{1} , x_{2} , \\cdots , x_{n}) } \\over { p_{1}( X_{1} = x_{1} ) }} $$ 連続確率変数$X_{1}, X_{2}, \\cdots , X_{n}$に対し、次の$f_{2, \\cdots , n \\mid 1}$を$X_{1} = x_{1}$が与えられた時の$ X_{2}, \\cdots , X_{n}$の結合条件付き確率密度関数という。 $$ f_{2, \\cdots , n \\mid 1} ( x_{2} , \\cdots ,x_{n} \\mid X_{1} = x_{1} ) = {{ f_{1, \\cdots , n}(x_{1} , x_{2} , \\cdots , x_{n}) } \\over { f_{1}( X_{1} = x_{1} ) }} $$ $X_{2} , \\cdots , X_{n}$に対する関数$u$が与えられた時、次を$X_{1} = x_{1}$が与えられた時の$u( X_{2}, \\cdots , X_{n} )$の条件付き期待値という。 $$ \\begin{align*} \u0026amp; E \\left[ u \\left( X_{2} , \\cdots , X_{n} \\right) \\mid X_{1} = x_{1} \\right] \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} u (x_{2} , \\cdots , x_{n}) f_{2 , \\cdots , n \\mid 1} (x_{2} , \\cdots, x_{n} \\mid X_{1} = x_{1}) dx_{2} \\cdots , dx_{n} \\end{align*} $$ 定理 [1] 条件付き分散: $$ \\begin{align*} \\text{Var} (X_{2} | X_{1} = x_{1}) =\u0026amp; E \\left[ \\left( X_{2} - E (X_{2} \\mid X_{1} = x_{1}) \\right)^{2} \\mid X_{1} = x_{1} \\right] \\\\ =\u0026amp; E \\left( X_{2}^{2} \\mid X_{1} = x_{1} \\right) - \\left[ E(X_{2} \\mid X_{1} = x_{1}) \\right]^{2} \\end{align*} $$ [2]: $E \\left[ E (X_{2} | X_{1}) \\right] = E (X_{2} )$ [3]: $\\text{Var}(X_{2})$が存在すれば$\\text{Var} \\left[ E \\left( X_{2} \\mid X_{1} \\right) \\right] \\le \\text{Var} (X_{2})$ 説明 条件付き確率、条件付き期待値は、教科課程のレベルであったように、数理統計学でも最も計算が難しい部分に属している。他のことはさておき、多変量である以上、計算が多くなるのは避けられない。もちろん、条件付きという概念にはその価値がある。一方で、主として微積分学に依存している数理統計学と異なり、測度論に基づいた確率論へと発展すると、その計算ははるかに簡潔になる。要旨は、「無視することはないが、過度に執着することもない」ということだ。\n参照 測度論によって定義される条件付き確率分布 測度論によって定義される条件付き期待値 ","id":1458,"permalink":"https://freshrimpsushi.github.io/jp/posts/1458/","tags":null,"title":"数理統計学における条件付き確率分布"},{"categories":"줄리아","contents":"概要 マクロは、Juliaでコーディングする時の便利機能であり、スコープの前に置いて実行される。例えば、自分のプログラムがどれくらいの時間を消費するか知りたい場合、次のように書くといい。\n@time for t in 1:10\rfoo()\rbar()\rend 例 多くの種類があるが、以下のマクロが特に広く使われている:\n@time：後に続く関数やスコープの実行時間を測定する。どんな状況でどう最適化すべきか分からない時、まず時間を測って、良い方を選ぶのが楽になる。言語によっては、時間を測るためのコードを書くのが面倒な場合があるが、Juliaの場合は、マクロ一つで実行時間だけでなく、メモリの使用量まで教えてくれる。 @.：その後に続く式の演算にドット(.)を追加する。 @threads：並列処理を簡単に実装できるマクロだ。 @animate：GIFを簡単に焼くマクロだ。 環境 OS: Windows julia: v1.5.0 ","id":1454,"permalink":"https://freshrimpsushi.github.io/jp/posts/1454/","tags":null,"title":"ジュリアの強力な便利機能、マクロ"},{"categories":"줄리아","contents":"概要 ジュリアはデータを扱う上での強みを生かして、パイプラインオペレーターをサポートしている。\nコード julia\u0026gt; (1:5) .|\u0026gt; (x -\u0026gt; sqrt(x+2)) .|\u0026gt; sin |\u0026gt; minimum\r0.4757718381527513\rjulia\u0026gt; minimum(sin.((x -\u0026gt; sqrt(x+2)).(1:5)))\r0.4757718381527513 上のサンプルコードは、配列 $[1,2,3,4,5]$ を $\\sqrt{x + 2}$ に入れ、その結果を $\\sin$ に入れて、その中の最小値を得るコードであり、上記のコードと以下のコードは完全に同じ結果を出す。複雑なコードを書く中でパイプラインがどれだけ便利であるかは、説明するまでもないだろう。配列を入れる時は、各要素を個別に扱うために必ずドットを使用する点だけ注意すれば、他の言語のパイプラインと同じように使用できる。\n他の言語 R でのパイプラインオペレーター 環境 OS: Windows julia: v1.5.0 ","id":1450,"permalink":"https://freshrimpsushi.github.io/jp/posts/1450/","tags":null,"title":"ジュリアでパイプオペレータを使用する方法"},{"categories":"수리통계학","contents":"定義 1 標本空間 $\\Omega$で定義された$n$個の確率変数 $X_{i}$に対し$X = (X_{1} , \\cdots , X_{n})$を$n$次元ランダムベクトルRandom Vectorという。$X$の値域$X(\\Omega)$を空間とも呼ぶ。 次のを満たす関数$F_{X} : \\mathbb{R}^{n} \\to [0,1]$を$X$のジョイントJoint累積分布関数という。 $$ F_{X}\\left( x_{1}, \\cdots , x_{n} \\right) := P \\left[ X_{1} \\le x_{1} , \\cdots , X_{n} \\le x_{n} \\right] $$ ある$h_{1} , \\cdots , h_{n} \u0026gt;0$に対し、次のを満たす関数$M_{X}$が存在するなら、$X$の積率生成関数という。 $$ M_{X} (t_{1}, \\cdots , t_{n}) := E \\left[ e^{\\sum_{k=1}^{n} t_{k} X_{k} } \\right] = E \\left[ \\prod_{k=1}^{n} e^{t_{k} X_{k}} \\right] \\\\ |t_{1}| \u0026lt; h_{1} , \\cdots , |t_{n} | \u0026lt; h_{n} $$ 離散 D1: $X$の空間が可算集合なら、$X$は離散ランダムベクトルという。 D2: 次を満たす$p_{X} : \\mathbb{R}^{n} \\to [0,1]$を離散ランダムベクトル$X$のジョイント確率質量関数という。 $$ p_{X} (x_{1} , \\cdots , x_{n}) := P \\left[ X_{1} = x_{1} , \\cdots , X_{n} = x_{n} \\right] $$ D3: $1 \\le k \\le n$に対し、次のような$P_{X_{k}} (x_{k})$をマージナル確率質量関数という。 $$ P_{X_{k}} (x_{k}) := \\sum_{x_{1}} \\cdots \\sum_{x_{k-1}}\\sum_{x_{k+1}} \\cdots \\sum_{x_{n}} p_{X} (x_{1} , \\cdots , x_{n}) $$ D4: $S_{X}:= \\left\\{ \\mathbb{x} \\in \\mathbb{R}^{n} : p_{X}(\\mathbb{x}) \u0026gt; 0 \\right\\}$を$X$のサポートという。 連続 C1: 確率変数$X$の累積分布関数$F_{X} = F_{X_{1} , \\cdots , X_{n}}$が全ての$\\mathbb{x} \\in \\mathbb{R}^{n}$で連続なら、$X$は連続ランダムベクトルという。 C2: 次を満たす$f_{X} : \\mathbb{R}^{n} \\to [0,\\infty)$を、連続ランダムベクトル$X$のジョイント確率密度関数という。 $$ F_{X} (x_{1}, \\cdots, x_{n}) = \\int_{-\\infty}^{x_{1}} \\cdots \\int_{-\\infty}^{x_{n}} f_{\\mathbb{x}} (t_{1} , \\cdots , t_{n}) dt_{1} \\cdots d t_{n} $$ C3: $1 \\le k \\le n$に対し、次のような$f_{X_{k}} (t_{k})$をマージナル確率密度関数という。 $$ f_{X_{k}}(t_{k}) := \\int_{\\infty}^{x_{1}} \\cdots \\int_{\\infty}^{x_{k-1}} \\int_{\\infty}^{x_{k+1}} \\cdots \\int_{\\infty}^{x_{n}} f_{X}(t_{1} , \\cdots , t_{n}) dt_{1} \\cdots d_{k-1} d_{k+1} \\cdots d_{n} $$ C4: $S_{X} := \\left\\{ \\mathbb{t} \\in \\mathbb{R}^{n} : f_{X} ( \\mathbb{t} ) \u0026gt; 0 \\right\\}$を$X$のサポートという。 元々ランダムベクトルRandom Vectorは、確率ベクトルと訳されるが、高校卒業以上でStochasticやProbabilisticなどと混同されることを避けるため、原語をそのまま使う。 元々ジョイント累積分布関数Joint Cumulative Distribution Functionは、結合確率分布と訳されるが、独立や依存に対する誤解を招く可能性があるため、原語をそのまま使う。 元々マージナル分布Marginal Distributionは、周辺分布と訳されるが、経済学の限界Marginalのようにその意味が伝わりにくいと思われるため、原語をそのまま使う。 説明 多変量確率分布は、一変量確率分布を多次元に一般化したものであり、変数が複数ある点で根本的に大きな違いがあるが、少なくとも学部レベルの数理統計学では、微積分学的なスキルでも十分に異なることができる。どのように異なるか見てみよう：\n1: 混同してはいけないのは、ランダムベクトル$X : \\Omega^{n} \\to \\mathbb{R}^{n}$も依然として関数であることだ。そのため、その値域を考えることができ、これにより多変量に関しても離散型と連続型に分類する。 C2: 連続のジョイント密度関数は、一般的に確率が$0$の$A \\subset \\mathbb{R}^{n}$を除き、微積分学の基本定理に従って次のを満たすように定義される。 $$ {{ \\partial^{n} } \\over { \\partial x_{1} \\cdots \\partial x_{n} }} F_{X} (\\mathbb{x}) = f(\\mathbb{x}) $$ D3, C3: 式は複雑だが、簡単に言えば、ジョイント確率分布を純粋に確率変数$X_{k}$に関する分布に変えたものだ。経済学でマージナルという言葉が微分の概念と通じるのと反対に、数理統計学では関心のない変数を一掃するために積分や合計をすることだ。 Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p75~84.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1449,"permalink":"https://freshrimpsushi.github.io/jp/posts/1449/","tags":null,"title":"数理統計学における多変量確率分布"},{"categories":"줄리아","contents":"概要 Juliaでは、ラムダ式は以下のように定義される。\n(x -\u0026gt; 3x^2 - 2x + 3)(1) これは、匿名関数$\\lambda : \\mathbb{Z} \\to \\mathbb{Z}$を定義し、そこに$1$を代入して、$4$という関数値を得たものに相当する。 $$ \\lambda : x \\mapsto ( 3 x^{2} - 2 x + 3 ) \\\\ \\lambda (1) = 4 $$ 実際、ラムダ式自体はJuliaの特徴ではなく、MATLABやPythonを含む関数型言語に影響を受けて、ほぼ自然にサポートされているもので、Juliaに興味を持つ学習者ならすでにラムダ式を使った経験が多いかもしれない。しかし、これまで使ってきた「それ」がラムダ式であるかどうか知らなかったり、まだその真の価値を知らない読者のために、特にすぐに適用できるであろう例を2つ紹介する。\n例1：リストを異なる尺度でソート リストをソートするのは組み込み関数を使えば非常に簡単だが、多次元配列でカテゴリー別に優先度を設定してソーティングしたり、元のデータを異なる基準でソートしたい場合がある。そんな時は、sort()関数のbyオプションに該当する関数をラムダ式として入れることで、簡単にコードを書くことができる。\njulia\u0026gt; # Example 1\rjulia\u0026gt; example = rand(-20:20,10)\r10-element Array{Int64,1}:\r3\r8\r19\r-12\r-20\r9\r-13\r19\r13\r2\rjulia\u0026gt; sort(example, by=(x -\u0026gt; abs(x)))\r10-element Array{Int64,1}:\r2\r3\r8\r9\r-12\r-13\r13\r19\r19\r-20 上記の作業は、ランダムに選ばれた整数を絶対値が小さいものから大きいものに基づいてソートすることを示している。ラムダ式がなくても不可能ではないが、思ったより単純ではない。与えられたラムダ式(x -\u0026gt; abs(x))を上手く変えれば、コーダーが望むコードを簡単に書くことができるだろう。\n例1の応用 valueという辞書が以下のように作成されているとする。 この時、辞書の値の大きさ順でソートするコードは、ラムダ式を活用してsort(value,by=(x -\u0026gt; value[x]))のように簡単に組むことができ、その実行結果は以下の通りだ。 例2：リストの頻度計算 Rのようにデータを最優先にする言語では、最初から組み込み関数で作られているが、この頻度計算が思うほど単純ではない。アルゴリズムと呼べるほど複雑な作業ではないが、実際にやってみるとかなり手がかかる。これもまた、ラムダ関数を利用して簡単に解決できる！\njulia\u0026gt; # Example 2\rjulia\u0026gt; example = rand(1:3,10); println(example)\r[3, 1, 2, 2, 3, 2, 3, 1, 3, 3]\rjulia\u0026gt; uexample = sort(unique(example))\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; counts = map(x-\u0026gt;count(y-\u0026gt;x==y,example),uexample)\r3-element Array{Int64,1}:\r2\r3\r5 上記の作業は、ランダムに選ばれた整数の頻度を数えたものである。unique()でデータの階級を把握し、それぞれの階級に該当する要素をカウントする方式で、単純に問題を解決した。\n環境 OS: Windows julia: v1.5.0 ","id":1448,"permalink":"https://freshrimpsushi.github.io/jp/posts/1448/","tags":null,"title":"ジュリアでのラムダ式"},{"categories":"매트랩","contents":"方法 tic\rX1=rand(2^7);\rX2=rand(2^8);\rX3=rand(2^9);\rX4=rand(2^10);\rX5=rand(2^11);\rtoc\rY1=imrotate(X1,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rY2=imrotate(X2,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rY3=imrotate(X3,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rY4=imrotate(X4,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rY5=imrotate(X5,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc tic：実行時間を測定するためのストップウォッチを開始する。 toc：ストップウォッチの現在時間を返す。tocとtocの間の時間を測るわけではないことに注意。 上記の例示コードでY1〜Y6を計算する時間をそれぞれ計りたい場合、以下のようにコードを入力する必要がある。\ntic\rX1=rand(2^7);\rX2=rand(2^8);\rX3=rand(2^9);\rX4=rand(2^10);\rX5=rand(2^11);\rtoc\rtic\rY1=imrotate(X1,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rtic\rY2=imrotate(X2,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rtic\rY3=imrotate(X3,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rtic\rY4=imrotate(X4,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rtic\rY5=imrotate(X5,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc 他の言語 Rで ","id":1467,"permalink":"https://freshrimpsushi.github.io/jp/posts/1467/","tags":null,"title":"MATLABでコード実行時間を計測する方法"},{"categories":"줄리아","contents":"画像サイズの変更 Images パッケージの imresize を使えばいい。関数名はMatlabと同じだ。\nimresize(X, ratio=a): 配列Xをa倍に調整した画像を返す。Matlabとは違って、ただ比率を書くだけではなく、必ず ratio=a と書かなければならない。\nimresize(X, m, n): 配列Xをm行n列に拡大/縮小した画像を返す。以下は例示コードとその結果だ。\nusing Images\rX=load(\u0026#34;example\\_{i}mage2.jpg\u0026#34;)\rY1=imresize(X, ratio=0.5)\rY2=imresize(X,500,500)\rY3=imresize(X,1500,1500)\rY4=imresize(X,700,1000)\rY5=imresize(X,1000,1300)\rY6=imresize(X,300,300)\rsave(\u0026#34;X.png\u0026#34;,colorview(RGB,X))\rsave(\u0026#34;Y1=imresize(0.5).png\u0026#34;,colorview(RGB,Y1))\rsave(\u0026#34;Y2=imresize(500,500).png\u0026#34;,colorview(RGB,Y2))\rsave(\u0026#34;Y3=imresize(1500,1500).png\u0026#34;,colorview(RGB,Y3))\rsave(\u0026#34;Y4=imresize(700,1000).png\u0026#34;,colorview(RGB,Y4))\rsave(\u0026#34;Y5=imresize(1000,1300).png\u0026#34;,colorview(RGB,Y5))\rsave(\u0026#34;Y6=imresize(300,300).png\u0026#34;,colorview(RGB,Y6)) 参照 Matlabで画像サイズを調整する方法 環境 OS: Windows10 Version: 1.5.3 (2020-11-09) ","id":1466,"permalink":"https://freshrimpsushi.github.io/jp/posts/1466/","tags":null,"title":"ジュリアで画像サイズを変更する方法"},{"categories":"줄리아","contents":"コード using Images\rcd(\u0026#34;C:/Users/rmsms/OneDrive/examples\u0026#34;)\rpwd()\rexample = load(\u0026#34;example.jpg\u0026#34;)\rtypeof(example)\rsize(example)\rgray1 = Gray.(example)\rtypeof(gray1)\rsize(gray1)\rM = convert(Array{Float64},gray1)\rtypeof(M)\rsize(M)\rcolorview(Gray, M.^(1/2))\rsave(\u0026#34;rgb.png\u0026#34;, colorview(RGB, example))\rsave(\u0026#34;gray1.png\u0026#34;, colorview(Gray, gray1))\rsave(\u0026#34;gray2.png\u0026#34;, colorview(Gray, transpose(gray1)))\rsave(\u0026#34;gray3.png\u0026#34;, colorview(Gray, M.^(1/2))) 上から順にサンプルコードを簡単に理解してみよう:\ncd() : Change Directory, 作業ディレクトリを望む場所に変える。\npwd() : Print Working Directory, 作業ディレクトリを出力する。例をそのまま試したいなら、上のファイルを作業ディレクトリにダウンロードして、ファイル名をexample.jpgに変更しよう。\nload() : 作業ディレクトリ内の画像ファイルを読み込む。読み込まれた画像のタイプは Array{RGB{Normed{UInt8,8}},2}だ。 これは他の言語でカラー画像を3つの行列を1つのテンソルとして表現するのとは少し違う。実際、そのような方法は数学的には理解しやすいかもしれないが、色空間やライブラリによって統一された規格がなく多くの混乱を招いてきた。Juliaではタイプを1つにして幅と高さが2次元の配列として理解する。画像は読み込まれた瞬間にPlotパネルにプリントされる。 Gray() : 画像を白黒に変換するのに使われた。実際に使われるのはGray()ではなくGray.()であることに注意しよう。この関数自体は1つのピクセルを白黒に変えるもので、ドットをつけることで全てのピクセルに適用されることを意味する。\nsize() : 画像のサイズを返す。述べたように、カラーと白黒を形が違う別のテンソルとして扱わず、データタイプが異なりサイズが同じ配列として扱っている。 Convert() : Array{Float64}、すなわち行列にgray1画像を変換した。すると、0が黒色で1が白色の行列によって白黒画像が表される。\ncolorview() : この関数自体が画像や行列を出力する関数だ。わざわざ行列を画像に再変換する必要はなく、直接Plotパネルで画像を確認できる。サンプルコードでは、行列 $M$ の各成分にルートを取っている。行列の全ての成分は $[0,1]$ に属するので、この変換は画像を全体的に明るく補正することに該当する。 save() : 作業ディレクトリに画像を保存する:gray1.png : 白黒で保存された。gray2.png : 白黒であり、転置行列状態で保存された。gray3.png : 白黒であり、明るく補正された状態で保存された。rgb.png : オリジナルの色をそのまま持った状態で保存された。\n環境 OS: Windows julia: v1.5.0 ","id":1446,"permalink":"https://freshrimpsushi.github.io/jp/posts/1446/","tags":null,"title":"ジュリアで画像を読み込み、行列として保存する方法"},{"categories":"매트랩","contents":"方法 imresize(A, scale): Aのサイズをscale倍調整して新しい画像を返す。 Aが10x10の画像である場合、scaleに0.5を入力すると5x5の画像を返す。以下のように直接サイズを調整することもできる。\nimresize(A, [m n]): m行n列の画像を返す。以下は例のコードとその結果だ。 X=imread(\u0026#39;test\\_{i}mage.jpg\u0026#39;);\rfigure()\rimshow(X)\rsaveas(gcf,\u0026#39;X.png\u0026#39;)\rtitle(\u0026#39;X\u0026#39;)\rY1=imresize(X,0.5);\rY2=imresize(X,[500 500]);\rY3=imresize(X,[700 500]);\rY4=imresize(X,[500,700]);\rfigure()\rimshow(Y1)\rsaveas(gcf,\u0026#39;Y1.png\u0026#39;)\rtitle(\u0026#39;Y1=imresize(X,0.5)\u0026#39;)\rfigure()\rimshow(Y2)\rsaveas(gcf,\u0026#39;Y2.png\u0026#39;)\rtitle(\u0026#39;Y2=imresize(X,[500 500])\u0026#39;)\rfigure()\rimshow(Y3)\rsaveas(gcf,\u0026#39;Y3.png\u0026#39;)\rtitle(\u0026#39;Y3=imresize(X,[700 500])\u0026#39;)\rfigure()\rimshow(Y4)\rsaveas(gcf,\u0026#39;Y4.png\u0026#39;)\rtitle(\u0026#39;Y4=imresize(X,[500,700])\u0026#39;) 他の言語で ジュリアで ","id":1465,"permalink":"https://freshrimpsushi.github.io/jp/posts/1465/","tags":null,"title":"MATLABでの画像サイズの変更方法"},{"categories":"줄리아","contents":"画像の回転 imrotate(X, theta) : 配列Xをthetaラジアンで回転させる。ここで注意すべき点は、角度の単位が度（$^{\\circ})$のMATLABと異なり、角度の単位はラジアンであることだ。さらに、MATLABとは異なり時計回りに回転する。他の変数を入力しない場合の補間法はバイリニアであり、回転された画像は切り取られない。元の画像Xを$90^\\circ=\\pi/2$、$180^\\circ=\\pi$、$270^\\circ=\\frac{3}{2}\\pi$だけ回転させた例とその結果は下のようになる。\nusing Images\rusing Interpolations\rX=load(\u0026#34;example\\_{i}mage.png\u0026#34;)\rY1=imrotate(X,pi/2)\rY2=imrotate(X,pi)\rY3=imrotate(X,pi*3/2) 上のように回転させると、元の配列がぴったりと収まるので、画像のサイズは変わらない。しかし、$90$の倍数ではない角度で回転させると、元の形に合わなくなる。そのため、元の画像のすべての点を表現するために、以下のように画像が大きくなる。元の画像のサイズを維持したい場合は、変数axes()を追加すればいい。\nY4=imrotate(X,pi/6)\rY5=imrotate(X,pi/6,axes(X)) また、InterpolationsパッケージのConstant()を使用すると、補間法をnearest1に適用することができる。計算に最も近い点のみを使用するので、精度は低下するが、計算は速い。対照的に、bilinear2の場合は周囲の四点すべてを計算に使用するので、相対的に速度は遅いが、より精確である。ここで精確とは、回転した際に画像が損傷する度合いが低いという意味だ。以下の画像を見てみよう。画像が大きく、一見すると違いが分からないかもしれないが、拡大すると二つの補間法の違いがはっきりとわかる。\nY6=imrotate(X,pi/6,Constant()) 参照 MATLABで画像を回転させる方法 環境 OS: Windows10 Version: 1.5.3 (2020-11-09) 最近傍補間法, 最近隣補間法\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n双線形補間法, 二線形補間法\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1462,"permalink":"https://freshrimpsushi.github.io/jp/posts/1462/","tags":null,"title":"ジュリアで画像配列を回転する方法"},{"categories":"줄리아","contents":"$A = \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \\\\ 2 \u0026amp; 3 \u0026amp; 4\\end{pmatrix}$としよう。\n転置行列 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; transpose(A)\r3×3 LinearAlgebra.Transpose{Int64,Array{Int64,2}}:\r1 0 2\r2 3 3\r1 0 4\rjulia\u0026gt; A\u0026#39;\r3×3 LinearAlgebra.Adjoint{Int64,Array{Int64,2}}:\r1 0 2\r2 3 3\r1 0 4 行列の要素が実数の場合、transpose()と'は同じ行列を返すが、データ型が微妙に異なる。これは'が正確には転置ではなく共役転置であるためだ。したがって、実数の行列の場合は実質的に同じ行列を返し、複素数の行列の場合は全く異なる結果を返す。\njulia\u0026gt; A_complex=[1+im 2 1+im;\r0 3 0+im;\r2 3+im 4]\r3×3 Array{Complex{Int64},2}:\r1+1im 2+0im 1+1im\r0+0im 3+0im 0+1im\r2+0im 3+1im 4+0im\rjulia\u0026gt; transpose(A_complex)\r3×3 LinearAlgebra.Transpose{Complex{Int64},Array{Complex{Int64},2}}:\r1+1im 0+0im 2+0im\r2+0im 3+0im 3+1im\r1+1im 0+1im 4+0im\rjulia\u0026gt; A_complex\u0026#39;\r3×3 LinearAlgebra.Adjoint{Complex{Int64},Array{Complex{Int64},2}}:\r1-1im 0+0im 2+0im\r2+0im 3+0im 3-1im\r1-1im 0-1im 4+0im 繰り返し乗算 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; A^2\r3×3 Array{Int64,2}:\r3 11 5\r0 9 0\r10 25 18\rjulia\u0026gt; A*A\r3×3 Array{Int64,2}:\r3 11 5\r0 9 0\r10 25 18\rjulia\u0026gt; A^3\r3×3 Array{Int64,2}:\r13 54 23\r0 27 0\r46 149 82\rjulia\u0026gt; A*A*A\r3×3 Array{Int64,2}:\r13 54 23\r0 27 0\r46 149 82 A^2とA*Aは完全に同じ結果を返す。同様に、A^3とA*A*Aも同じである。\n要素ごとの乗算、要素ごとの除算 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; A.*A\r3×3 Array{Int64,2}:\r1 4 1\r0 9 0\r4 9 16\rjulia\u0026gt; A./A\r3×3 Array{Float64,2}:\r1.0 1.0 1.0\rNaN 1.0 NaN\r1.0 1.0 1.0 各要素を乗算または除算した結果を返す。\n左右反転、上下反転 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; reverse(A,dims=1)\r3×3 Array{Int64,2}:\r2 3 4\r0 3 0\r1 2 1\rjulia\u0026gt; reverse(A,dims=2)\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r4 3 2 reverse(A,dims=1)は行列$A$を上下に反転した行列を返し、MATLABでのflipud(A)に相当する。reverse(A,dims=2)は行列$A$を左右に反転した行列を返し、MATLABでのfliplr(A)に相当する。\n逆行列 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; inv(A)\r3×3 Array{Float64,2}:\r2.0 -0.833333 -0.5\r0.0 0.333333 0.0\r-1.0 0.166667 0.5 行列$A$の逆行列を返す。逆行列を見つけることができない場合は、エラーが発生する。\n環境 OS: Windows10 Version: 1.5.3 (2020-11-09) ","id":1460,"permalink":"https://freshrimpsushi.github.io/jp/posts/1460/","tags":null,"title":"ジュリアでの2次元配列操作の関数들"},{"categories":"줄리아","contents":"Heatmap Plots パッケージのheatmap関数を使えば、2次元配列をヒートマップ画像として出力でき、savefig関数でその画像を保存できる。@__DIR__はジュリアコードファイルの位置を教えてくれるマクロだ。\n# code1 だが、配列Aとヒートマップ画像を比較すると、配列の上下が逆さまに作成されたヒートマップ画像になっていることが分かる。出力画像がこのように作成される理由は、それぞれの成分の位置を行と列ではなく直交座標系の座標として考えるからだというのが公式のカニの話だ。つまり、例えば行列$A$では、19という値は4行4列の成分ではなく、直交座標$(4,4)$の成分と見なすわけだ。これが行列と画像が上下逆さまになっている理由を説明する。\nしたがって、配列と同じ見た目に出力されるようにしたいのであれば、yflip=trueオプションを追加すればいい1。\n# code2 また、MATLABに慣れているユーザーは、color=:bgyオプションを使用すれば、MATLABの基本色と似た出力が得られる。\n# code3 色のテーマ 使用できる色のテーマは、次のようなものがある。\n参照 マットラボから 環境 OS: Windows10 バージョン: 1.5.3 (2020-11-09) https://github.com/JuliaPlots/Makie.jl/issues/46#issuecomment-357023505\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1459,"permalink":"https://freshrimpsushi.github.io/jp/posts/1459/","tags":null,"title":"ジュリアで配列をヒートマップ画像として出力保存する方法"},{"categories":"줄리아","contents":"概要 Juliaでは、Pythonと同様にセットデータ型をサポートしています。元来のセットデータ型がそうであるように、使用する人にとっては非常に便利で、使用しない人にはまったく使われないものですが、Juliaは言語設計自体が数学に近いため、セットの概念と操作がしっかりと実装されており、必ず理解しておくべきです。 特に、他の言語、特にPythonと最も異なる点は、ユニコード記号もコードの一部として使用できることです。AtomエディタでJunoを使用している場合、上記のようにTeXコードを自動補完することができます。このコンテキストでは、$\\in$は単なる記号ではなく、実際に要素がセットに属しているかを表しています。\nコード julia\u0026gt; X = Set([1,2,3,1]); print(X)\rSet([2, 3, 1])\rjulia\u0026gt; X[1]\rERROR: MethodError: no method matching getindex(::Set{Int64}, ::Int64)\rStacktrace:\r[1] top-level scope at REPL[23]:1\rjulia\u0026gt; for i in X print(i) end\r231 上記のコードは、セット $X$ を $X := \\left\\{ 1, 2, 3, 1 \\right\\} = \\left\\{ 2,3,1 \\right\\}$ として定義することを意味します。数学でのセットと同じように、重複と順序の概念は存在しません。したがって、最初のインデックスを参照するとエラーが出ます。しかし、データ型自体はPythonと同様にイテラブルであるため、ループ内で使用することができます。\njulia\u0026gt; if 1∈X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if 0∈X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r?\rjulia\u0026gt; if 0∉X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if [1,2] ⊆ X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if [0,1,2] ⊆ X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r? これが数式を読むような親しみを感じるなら、セットデータ型を有効に使用する準備ができていることです。特に注意すべき点は、上記のような計算がセットデータ型に限定されて動作するわけではないことです。つまり、リストに対しても同様の操作を気軽に使用することができます。セットデータ型が不慣れであっても、セットにだけ慣れていれば、Juliaのセット演算子を利用することに問題はありません。なお、包含関係は \\subset $\\subset$ ではなく \\subseteq $\\subseteq$ を使用する必要があります。\njulia\u0026gt; Y = [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;,3]\r3-element Array{Any,1}:\r\u0026#34;1\u0026#34;\r\u0026#34;2\u0026#34;\r3\rjulia\u0026gt; ∪(X,Y)\rSet{Any} with 5 elements:\r\u0026#34;1\u0026#34;\r2\r3\r\u0026#34;2\u0026#34;\r1\rjulia\u0026gt; ∩(X,Y)\rSet{Int64} with 1 element:\r3\rjulia\u0026gt; ∩(Y,X)\r1-element Array{Any,1}:\r3\rjulia\u0026gt; setdiff(X,Y); X\rSet{Int64} with 3 elements:\r2\r3\r1\rjulia\u0026gt; setdiff!(X,Y); X\rSet{Int64} with 2 elements:\r2\r1 基本的にセットを扱うため、和集合と交差点も数式と同様に表現することができます。2行目と3行目の違いは操作の順序です。$X$ はJuliaでセットデータ型、$Y$はただの配列として定義されており、返される値は最初の引数のデータ型に従います。このような違いは、Juliaのような強い型付け言語では非常に重要であるため、必ず理解しておく必要があります。差集合は、setdiff()はただの差集合を返し、setdiff!()はセット自体を更新するという違いがあります。\n環境 OS: Windows julia: v1.5.0 ","id":1442,"permalink":"https://freshrimpsushi.github.io/jp/posts/1442/","tags":null,"title":"ジュリアでの集合データ型と演算子"},{"categories":"수리통계학","contents":"定義 1 確率変数$X$とある正の数$h\u0026gt;0$に対して、$E(e^{tX})$が$-h\u0026lt; t \u0026lt; h$で存在するならば、$M(t) = E( e^{tX} )$を$X$のモーメント生成関数と定義する。\n説明 モーメント生成関数(mgf)は、数理統計学で比較的早い段階に出会う概念だが、その見慣れない定義と文脈のない導入は、時としてこの分野を嫌いにさせる要因となる。mgfを理解する難しさは、教科書がその定義と応用に直行し、読者が何であるかは知っているが、その形式や目的はまったくわからないままになるからだ。基本的に、「モーメント生成関数」という語は、\u0026lsquo;モーメント\u0026rsquo;と\u0026rsquo;生成関数\u0026rsquo;を組み合わせて作られる。忙しい読者のためにポイントをまとめると以下の通り：\nモーメントが何かを理解する必要はない：基本的にモーメントは平均や分散などを包含する抽象的な概念である。モーメントは、次数に応じて適切な操作を加えることで意味のある統計量になることができるが、それ自体では統計的な意味を持たない。無理に何かの統計量と結びつけることなく、モーメントそのものとして十分に理解できる。 モーメント生成関数は単なる生成関数の一種である：生成関数は、多項式関数を一般的な形で表したものに過ぎない。モーメント生成関数がモーメントを生成する関数であるという説明も悪くないが、モーメント生成関数が生成関数の一つとして、その係数がモーメントであることを知っておけば、その性質をより正確に理解できる。 モーメント生成関数をマクローリン展開で解くと次のようになる。[ 注意: 定義で$-h\u0026lt;t\u0026lt;h$を収束半径として設定する理由がここにある。] $$ \\begin{align*} M(t) =\u0026amp; E(e^{tX}) \\\\ =\u0026amp; 1 + E(tX) + {{E(t^2 X^2)} \\over {2!}} + \\cdots \\end{align*} $$ 期待値は線形性を持つので、以下のように$t$に対する生成関数として表現できる。 $$ M(t) = 1 + E(X) t+ {{E( X^2) t^2 } \\over {2!}} + \\cdots $$ $t^k$項の係数は、$k$次モーメントの定数倍である$\\displaystyle {{E(X^{k})} \\over {k!}} $であることに注意しよう。これで、両辺を$t$に対して$n$回微分し、$t=0$を代入すると $$ M^{(n)} (0) = E(X^{n}) $$ したがって、関数$M$はモーメントを生成すると言え、このためにモーメント生成関数と呼ばれると見なしても問題ない。$M$が定義で直接与えられていなかったり、生成関数に対する言及のみがあった場合、理解するのがずっと簡単だったであろう。\n一方、確率変数$X$と$Y$に関するモーメント生成関数$M_{X}$と$M_{Y}$がそれぞれ存在するとしよう。モーメントは、統計学で我々が究極的に知りたい統計量を得るために考案された概念である。そして、すべての項のモーメントが同じであれば、その$X$と$Y$は同じ分布に従うと言えるだろう。この定理に従えば、モーメント生成関数が存在する分布同士であれば、モーメント生成関数自体を分布と同じ概念として比較することが許される。実際、分布関数は確率を表すのには便利だが、分布自体を扱うのにはそれほど良くない。その代わりに、モーメント生成関数がこのような性質を持つため、どのような確率変数がどのような分布に従うかを数式的に議論する際に最も頻繁に使用される。\n要約 $X$と$Y$を、それぞれモーメント生成関数$M_{X}$、$M_{Y}$および累積分布関数$F_{X}$、$F_{Y}$を持つ確率変数としよう。すべての$ z \\in \\mathbb{R}$に対して$F_{X} (z) = F_{Y}(z)$が真であることと、ある$h\u0026gt;0$とすべての$t \\in (-h,h)$に対して$M_{X}(t) = M_{Y}(t)$が真であることは同等である。\n$\\mathbb{R}$は実数集合を意味する。 Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p59.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":248,"permalink":"https://freshrimpsushi.github.io/jp/posts/248/","tags":null,"title":"積率母関数とは何か？"},{"categories":"줄리아","contents":"概要 ジュリアは、R、パイソン、マトラボの利点が混在する言語だ。配列はプログラミングの基本であり、その利用で複数の言語の痕跡が見られる。\nコード 行列 julia\u0026gt; M = [1. 2. ; 3. 4.]\r2×2 Array{Float64,2}:\r1.0 2.0\r3.0 4.0\rjulia\u0026gt; size(M)\r(2, 2)\rjulia\u0026gt; length(M)\r4 行列の場合、ほぼマトラボの文法で定義され、使われる。size()関数はマトラボと同じように使用され、パイソンのnumpyパッケージのプロパティ.shapeに相当する機能をする。length()はマトラボと異なり、全要素の数を返す。\n二次配列 julia\u0026gt; x = [[1,2,3,4] for _ in 1:4]; x\r4-element Array{Array{Int64,1},1}:\r[1, 2, 3, 4]\r[1, 2, 3, 4]\r[1, 2, 3, 4]\r[1, 2, 3, 4] 配列内にループを配置して使用することは、パイソンでよく見られる方法だ。これにより、Rのrep()関数などを似たように再現できる。\nスライシング julia\u0026gt; y = [3,2,5,1,4]\r5-element Array{Int64,1}:\r3\r2\r5\r1\r4\rjulia\u0026gt; y[[4,2,1,5,3]]\r5-element Array{Int64,1}:\r1\r2\r3\r4\r5\rjulia\u0026gt; y[3:end]\r3-element Array{Int64,1}:\r5\r1\r4\rjulia\u0026gt; y[3:4] .= -1; y\r5-element Array{Int64,1}:\r3\r2\r-1\r-1\r4 インデキシングは、Rと似ており、インデックスの配列を与えると、その順序で出力する。配列の最後のインデックスをendで表現することから、スライシングはマトラボからの影響があると分かる。最後に、.=を使って、3、4番目の要素に-1を直接代入することも、マトラボに似ている。\nインデキシング julia\u0026gt; x = [1 2; 3 4]\r2×2 Array{Int64,2}:\r1 2\r3 4\rjulia\u0026gt; x[1,:]\r2-element Array{Int64,1}:\r1\r2\rjulia\u0026gt; x[[1],:]\r1×2 Array{Int64,2}:\r1 2\rjulia\u0026gt; x[1,1] = -1; x\r2×2 Array{Int64,2}:\r-1 2\r3 4 特殊なのは、インデキシングの方法によって、結果が異なる可能性があることだ。概念的には、同じものを入れても、結果が同じになると思うが、入れる時に要素が入れば、結果も要素として得られ、配列が入れば、結果も配列として得ることができる。これはジュリアを使いにくくするが、同時に高度な機能を実装するのに大いに役立つ。\n環境 OS: Windows julia: v1.5.0 ","id":1437,"permalink":"https://freshrimpsushi.github.io/jp/posts/1437/","tags":null,"title":"ジュリアにおける配列のスライシングとインデックス化"},{"categories":"수리통계학","contents":"定義 1 二つの確率変数 $X, Y$に対して、次のように定義された$\\rho = \\rho (X,Y)$をピアソン相関係数Pearson Correlationと呼ぶ。 $$ \\rho = { {\\text{Cov} (X,Y)} \\over {\\sigma_X \\sigma_Y} } $$\n$\\sigma_{X}$、$\\sigma_{Y}$はそれぞれ $X$、$Y$の標準偏差だ。 説明 ピアソン相関係数(Pearson) Correlation Coefficientは、二つの変数がお互いに**（線形）相関関係**を持っているかを確認する尺度になる。$1$や$–1$に近ければ相関関係があると見なし、$0$ならばないとされる。\n相関関係と独立は同じ概念ではないことに注意が必要だ。相関関係は、二つの変数が直線形のグラフを描くかだけを確認する。相関関係がないとしても必ずしも独立とは限らない。しかし、独立であれば相関関係がないと言える。この逆が成立するのは二つの変数が正規分布に従う場合に限られる。\n性質 ピアソン相関係数は$[-1,1]$を超えない。つまり、 $$ – 1 \\le \\rho \\le 1 $$\n証明 証明は二つの方法を紹介したい。\nコーシー・シュワルツの不等式を用いた証明 $$ \\rho = { {\\text{Cov} (X,Y)} \\over {\\sigma_X \\sigma_Y} } = {1 \\over n} \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) } $$ 両辺を二乗すると $$ \\rho ^2 = {1 \\over {n^2} } \\left\\{ \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) } \\right\\} ^ 2 $$\nコーシー・シュワルツの不等式： $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\\ge { (ax+by) }^{ 2 } $$\nコーシー・シュワルツの不等式により、 $$ {1 \\over {n^2} } \\left\\{ \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) } \\right\\} ^ 2 \\le {1 \\over {n^2} } \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) ^ 2 } \\sum_{k=1}^{n} { \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) ^ 2 } $$ 右辺を整理すると、 $$ \\begin{align*} \u0026amp; {1 \\over {n^2} } \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) ^ 2 } \\sum_{k=1}^{n} { \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) ^ 2 } \\\\ =\u0026amp; {1 \\over { {\\sigma_X}^2 {\\sigma_Y}^2 } } \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over { \\sqrt{n} } } \\right) ^ 2 \\sum_{k=1}^{n} \\left( { { y_k - \\mu_{Y} } \\over {\\sqrt{n}} } \\right) ^ 2 } \\\\ =\u0026amp; {1 \\over { {\\sigma_X}^2 {\\sigma_Y}^2 } } {\\sigma_X}^2 {\\sigma_Y}^2 \\\\ =\u0026amp; 1 \\end{align*} $$ $\\rho ^2 \\le 1$であるため、 $$ -1 \\le \\rho \\le 1 $$\n■\n共分散の定義を用いた証明 $\\text{Var}(Y)={ \\sigma _ Y }^2, \\text{Var}(X)={ \\sigma _ X }^2$、$\\displaystyle Z= \\frac { Y }{ \\sigma _Y } - \\rho \\frac { X }{ \\sigma _X }$を共分散の定義とすると、 $$ \\begin{align*} \\text{Var}(Z)\u0026amp;=\\frac { 1 }{ { \\sigma _ Y }^2 }\\text{Var}(Y)+\\frac { { \\rho ^ 2 } }{ { \\sigma _ X }^2 }\\text{Var}(X)-2\\frac { \\rho }{ { \\sigma _X } { \\sigma _Y } }\\text{Cov}(X,Y) \\\\ =\u0026amp; \\frac { 1 }{ { \\sigma _ Y }^2 }{ \\sigma _ Y }^2+\\frac { { \\rho ^ 2 } }{ { \\sigma _ X }^2 }{ \\sigma _ X }^2-2\\rho \\cdot \\rho \\\\ \u0026amp;=1+{ \\rho ^ 2 }-2{ \\rho ^ 2 } \\\\ \u0026amp;=1-{ \\rho ^ 2 } \\end{align*} $$ $\\text{Var}(Z)\\ge 0$であるから、 $$ \\begin{align*} 1-{ \\rho ^ 2 }\\ge 0 \\implies\u0026amp; { \\rho ^ 2 }-1\\le 0 \\\\ \\implies\u0026amp; (\\rho +1)(\\rho –1)\\le 0 \\\\ \\implies\u0026amp; -1\\le \\rho \\le 1 \\end{align*} $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistics (7th Edition): p104.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":57,"permalink":"https://freshrimpsushi.github.io/jp/posts/57/","tags":null,"title":"ピアソン相関係数"},{"categories":"수리통계학","contents":"定義と性質 平均がそれぞれ$\\mu_{X}$、$\\mu_{Y}$である確率変数$X$と$Y$について、$\\text{Cov} (X ,Y) : = E \\left[ ( X - \\mu_{X} ) ( Y - \\mu_{Y} ) \\right]$は$X$と$Y$の共分散Covarianceと定義される。共分散は以下の性質を持っている。\n[1]: $\\text{Var} (X) = \\text{Cov} (X,X)$ [2]: $\\text{Cov} (X,Y) = \\text{Cov} (Y, X)$ [3]: $\\text{Var} (X + Y) = \\text{Var} (X) + \\text{Var} (Y) + 2 \\text{Cov} (X,Y)$ [4]: $\\text{Cov} (X + Y , Z ) = \\text{Cov}(X,Z) + \\text{Cov}(Y,Z)$ [5]: $\\text{Cov} (aX + b , cY + d ) = ac \\text{Cov}(X,Y)$ 説明 共分散は二つの変数の線形の相関関係を示し、分散と異なり$0$はもちろん負の値も取り得る。\n証明 [1] $$ \\begin{align*} \\text{Cov} (X ,X) =\u0026amp; E[ ( X - \\mu_{X} ) ( X - \\mu_{X} ) ] \\\\ =\u0026amp; E[ ( X - \\mu_{X} )^2 ] \\\\ =\u0026amp; \\text{Var} (X) \\end{align*} $$\n■\n[2] $$ \\begin{align*} \\text{Cov} (X ,Y) =\u0026amp; E[ ( X - \\mu_{X} ) ( Y - \\mu_{Y} ) ] \\\\ =\u0026amp; E[ ( Y - \\mu_{Y} ) ( X - \\mu_{X} ) ] \\\\ =\u0026amp; \\text{Cov} (X ,Y) \\end{align*} $$\n■\n[3] $$ \\begin{align*} \\text{Var} (X + Y) =\u0026amp; E [ ( X + Y - \\mu_{X} - \\mu_{Y} )^2 ] \\\\ =\u0026amp; E \\left[ \\left\\{ ( X - \\mu_{X} ) + (Y - \\mu_{Y} ) \\right\\} ^2 \\right] \\\\ =\u0026amp; E \\left[ ( X - \\mu_{X} )^2 + 2 ( X - \\mu_{X} ) (Y - \\mu_{Y} )+ (Y - \\mu_{Y} )^2 \\right] \\\\ =\u0026amp; E[ ( X - \\mu_{X} )^2] + 2 E [ ( X - \\mu_{X} ) (Y - \\mu_{Y} ) ] + E [ (Y - \\mu_{Y} )^2 ] \\\\ =\u0026amp; \\text{Var} (X) + 2 \\text{Cov} (X,Y) + \\text{Var} (Y) \\end{align*} $$\n■\n[4] $$ \\begin{align*} \\text{Cov} (X + Y , Z ) =\u0026amp; E \\left[ ( X + Y - \\mu_{X} - \\mu_{Y} ) ( Z - \\mu_{Z} ) \\right] \\\\ =\u0026amp; E \\left[ \\left\\{ ( X - \\mu_{X} ) + ( Y - \\mu_{Y} ) \\right\\} ( Z - \\mu_{Z} ) \\right] \\\\ =\u0026amp; E \\left[ ( X - \\mu_{X} ) ( Z - \\mu_{Z} ) \\right] + E \\left[ ( Y - \\mu_{Y} ) ( Z - \\mu_{Z} ) \\right] \\\\ =\u0026amp; \\text{Cov}(X,Z) + \\text{Cov}(Y,Z) \\end{align*} $$\n■\n[5] $$ \\begin{align*} \\text{Cov} (aX + b , cY + d ) =\u0026amp; E \\left[ ( aX + b - a \\mu_{X} - b ) ( cY + d - c \\mu_{Y} - d ) \\right] \\\\ =\u0026amp; E \\left[ ( aX - a \\mu_{X} ) ( cY - c \\mu_{Y} ) \\right] \\\\ =\u0026amp; E \\left[ a c ( X - \\mu_{X} ) ( Y - \\mu_{Y} ) \\right] \\\\ =\u0026amp; ac E \\left[( X - \\mu_{X} ) ( Y - \\mu_{Y} ) \\right] \\\\ =\u0026amp; ac \\text{Cov}(X,Y) \\end{align*} $$\n■\n併せて見る 共分散行列 $\\Sigma$ ","id":425,"permalink":"https://freshrimpsushi.github.io/jp/posts/425/","tags":null,"title":"共分散の様々な性質"},{"categories":"수리통계학","contents":"定義: 期待値、平均、分散 確率変数 $X$ が与えられたとする。\n連続確率変数$X$ の確率密度関数$f(x)$ が $\\displaystyle \\int_{-\\infty}^{\\infty} |x| f(x) dx \u0026lt; \\infty$ を満たす場合、以下のように定義された $E(X)$ を$X$ の期待値Expectationと言う。 $$ E(X) := \\int_{-\\infty}^{\\infty} x f(x) dx $$\n離散確率変数$X$ の確率質量関数$p(x)$ が $\\displaystyle \\sum_{x} |x| p(x) \u0026lt; \\infty$ を満たす場合、以下のように定義された $E(X)$ を$X$ の期待値Expectationと言う。 $$ E(X) := \\sum_{x} x p(x) $$\n$\\mu = E(X)$ が存在する場合、$X$ の 平均Meanと定義する。\n$\\sigma^2 = E((X - \\mu)^2)$ が存在する場合、$X$ の 分散Varianceと定義する。\n抽象化の意味 統計学を簡単な言葉で一言で言い表すなら、「だから平均的にどうなの？」を研究する学問だと言えるかもしれない。「平均」は代表値としてかなり直感的で、計算も簡単な統計量である。しかし、この世にある様々な現象を説明するには、そんな単純なレベルでは足りず、「期待値」という形で抽象化される。期待値は離散分布だけでなく区分求積法の発想を借りて連続分布にも対応する概念となる。この文章で議論するのはまさにその「抽象化」についての話である。\n数理統計学は確かに統計理論に関する内容を含んでいるが、学問の性質を見たとき、本質的には数学の一分野に近い。従って、他の数学の分野のように数学者と同じマインドセットで数理統計学を理解しようとする努力が必要である。数学者の仕事の一つは、直感や既に世に出ている理論と矛盾せず、厳格な定義を考案して、万物、対象物、現象、またそれらを超えたすべてを記号に変え、そのすべてをその場で研究できるようにすることである。\n平均と分散の定義を見ると、直感的な意味は全く残っておらず、単に確率変数に期待値を取った形で表されている。定義からこれらがどのように平均や実際につながるか、どのように分散と実際につながるかには関心がない。むしろ、平均と分散という概念を当たり前のように受け入れているほど習熟した学習者が、記号に逆らって適応することが期待される。実際、数理統計学を学ぶレベルの学習者であれば、それらの定義に大きな疑問を持つことはないだろう。\n平均の抽象化と期待値という概念が定着すると、学者たちはより多くの可能性を発見する。単純な合計や積分といった限界を超え、基本的な代数学や解析学の理論を導入してこれらを扱い始めたのである。自然と、学者たちの関心は抽象化から一般化へと移行する。\n定義: モーメント 自然数 $m$ に対して、$E( X^m )$ を$X$ の**$m$番目のモーメント**$m$th Momentと定義する。\n一般化の意味 モーメントの定義を読むだけで即座に分かる事実は、第一モーメント、つまり$E(X^1)$ が$X$ の平均になるということである。もう少し考えると、第二モーメントである$E(X^2)$ も少しの操作を通じて分散になることもわかる。\n平均は第一モーメント、分散は第二モーメントと実質的に同じ概念と見ることができる。逆に考えれば、モーメントの中で第一が平均に対応し、第二が分散に対応すると言えるだろう。\nそれでは、直感を持つ学者はもちろん、第三や第四のモーメントも何か重要な情報を与えることができるだろうと推測することになる。[ NOTE: 具体的には、これを歪度、尖度と言う。] 今では研究の方向性が逆転し、明らかになった事実を理論で説明するのではなく、理論で隠された事実を見つけ出そうとするものになっている。\nこのような方法論は、統計学だけでなく、自然科学全般で容易に見ることができる。数理統計学は統計よりも数学に近いといえるが、学問の存在意義に近づくと再び統計学の姿を取り戻す。モーメントMomentという言葉自体は物理学などでも使用されるが、統計学ではモーメントがどのような意味を持つか考える必要はない。ただ、数理統計学を支える理論を説明する際に使用する言葉として知っておけば十分である。\n一方で、期待値の存在条件である$\\displaystyle \\int_{-\\infty}^{\\infty} |x| f(x) dx \u0026lt; \\infty$、つまり$x$ でわざわざ絶対値を取る必要があるのか疑問に思うかもしれない。存在を保証することと具体的な値を計算することは別としても、$X$ の期待値$E(X)$ は別の式を使う必要がないように見えるからである。\nこの点については、次の定理を見れば少しは役立つかもしれない。最も単純な$X$ の議論ではなく、$g(X)$ への一般化まで考慮すると、むしろ$E(X)$ こそ恒等関数 $g(x) = x$ に対する特別なケースとして定義されたと見ることもできるだろう。\n定理 確率変数$X$ について、$Y$ が何らかの関数$g$ に対して$Y := g(X)$ のように表れるとする。\n[1]: $X$ が確率密度関数$f_{X}$ を持つ連続確率変数で、$\\displaystyle \\int_{-\\infty}^{\\infty} |g(x)| f_{X} (x) dx \u0026lt; \\infty$ を満たす場合、 $$ E (Y) = \\int_{-\\infty}^{\\infty} g(x) f_{X} (x) dx $$ [2]: $X$ が確率質量関数$p_{X}$ を持つ離散確率変数で、$\\displaystyle \\sum_{x} |g(x)| p_{X} (x) \u0026lt; \\infty$ を満たす場合、 $$ E (Y) = \\sum_{x} g(x) p_{X} (x) $$ 参照 代表値としての平均 測度論で定義される期待値 ","id":246,"permalink":"https://freshrimpsushi.github.io/jp/posts/246/","tags":null,"title":"数理統計学における期待値、平均、分散、モーメントの定義"},{"categories":"수리통계학","contents":"定義 1 標本空間 $\\Omega$ で 確率 $P$ が定義されているとする。\n定義域が標本空間の関数 $X : \\Omega \\to \\mathbb{R}$ を 確率変数Random Variableと呼ぶ。確率変数の値域 $X(\\Omega)$ を 空間Spaceとも呼ぶ。 以下を満たす関数 $F_{X} : \\mathbb{R} \\to [0,1]$ を $X$ の累積分布関数(Cumulative Distribution Function, cdf) とする。 $$ F_{X}(x) = P_{X}\\left( (-\\infty,x] \\right) = P \\left( \\left\\{ \\omega \\in \\Omega : X(\\omega) \\le x \\right\\} \\right) $$ 離散 D1: 確率変数 $X$ の空間が可算集合ならば $X$ を 離散確率変数Discrete Random Variableと呼び、離散確率分布に従うとする。 D2: 以下を満たす $p_{X} : \\mathbb{R} \\to [0,1]$ を離散確率変数 $X$ の確率質量関数(Probability Mass Function, pmf) と呼ぶ。 $$ p_{X}(x) := P\\left( X=x \\right) $$ D3: $\\mathcal{S}_{X} := \\left\\{ x \\in \\mathbb{R} : p_{X}(x) \u0026gt; 0 \\right\\}$ を $X$ のサポートSupportと呼ぶ。 連続 C1: 確率変数 $X$ の累積分布関数 $F_{X}$ が全ての $x \\in \\mathbb{R}$ で連続ならば $X$ を 連続確率変数Continuous Random Variableと呼び、連続確率分布に従うとする。 C2: 以下を満たす関数 $f_{X} : \\mathbb{R} \\to [0,\\infty)$ を連続確率変数 $X$ の確率密度関数(Probability Density Function, pdf) と呼び、$X$ が 絶対連続Absolutely Continuousであるとする。 $$ F_{X}(x) = \\int_{-\\infty}^{x} f_{X}(t) dt $$ C3. $\\mathcal{S}_{X} := \\left\\{ t \\in \\mathbb{R} : f_{X}(t) \u0026gt; 0 \\right\\}$ を $X$ のサポートSupportと呼ぶ。 解説 サポート 、または 支持集合 は、簡単に言えば、私たちが興味を持つ部分だけを選び出した集合である。よく使われる表現ではないが、確率論が何を表現したいのかを確かに伝える。確率は確定的な何かに関心がなく、確率が $0$ とは決して起こらないということなので、無関心で良い。だから $\\mathcal{S}$ は「本当に重要な集合」や「私たちが知るべき集合」と見なせるようになり、限られたエネルギーを $\\Omega$ 全体ではなく $\\mathcal{S}$ にだけ注ぐことができるようになる。\n高校で確率に触れたときも、教師が「確率変数は関数だ」と強調した記憶があるだろう。しかし、それとは別に、本当に確率変数を関数として考えて扱うことは、もう少し高いレベルの抽象化能力を必要とする。ここで紹介されている定義はまだ数学的に厳密ではないが、集合と関数で確率の概念を描写することは簡単ではない。わからないからといって絶望することもなければ、わかったと思って軽く見ることもない。\n定義を読めば、離散確率変数と連続確率変数には本質的な違いがあり、それが形式的な違いにもつながることがわかる。学部生レベルでは混乱することもあるかもしれないが、連続確率変数を扱うときのみヤコビアンが付くことをしっかり理解しておこう。\n要約 サポート $\\mathcal{S}_{X}$ を持つ連続確率変数 $X$ と微分可能な単射関数 $g$ に対して、確率変数 $Y$ を $Y:=g(X)$ のように定義すると、$Y$ の確率密度関数は $y \\in \\mathcal{S}_{Y}$ に関して次のように求められる。[ 注: 実際には$g$ は全単射とは仮定されていないため、逆関数 $g^{-1}$ の存在が常に保証されるわけではない。] $$ f_{Y} (y) = f_{X} \\left( g^{-1}(y) \\right) \\left| {{ d x } \\over { d y }} \\right| $$\nここで $\\mathcal{S}_{Y}$ は $Y$ のサポート、$x$ は $x = g^{-1}(y)$ を意味する。 証明 $g$ は単射で連続なので、増加関数か減少関数である。ケースを分けて考えよう。\nケース 1. $g$ が増加関数の場合 $$ \\begin{align*} F_{Y}(y) =\u0026amp; P \\left( Y \\le y \\right) \\\\ =\u0026amp; P \\left( g(X) \\le y \\right) \\\\ =\u0026amp; P \\left( X \\le g^{-1}(y) \\right) \\\\ =\u0026amp; F_{X}\\left( g^{-1}(y) \\right) \\end{align*} $$ 微積分の基本定理により、$Y$ の確率密度関数は $$ \\begin{align*} f_{Y}(y) =\u0026amp; {{ d } \\over { d y }} F_{Y}(y) \\\\ =\u0026amp; {{ d } \\over { d y }} \\int_{-\\infty}^{x} f_{X}(t) dt \\\\ =\u0026amp; {{ d } \\over { d x }} \\int_{-\\infty}^{x} f_{X}(t) dt {{ d x } \\over { d y }} \\\\ =\u0026amp; f_{X} \\left( x \\right) {{ d x } \\over { d y }} \\\\ =\u0026amp; f_{X} \\left( g^{-1} (y) \\right) {{ d x } \\over { d y }} \\end{align*} $$ $g$ が増加関数なので $\\displaystyle {{ d x } \\over { d y }} = {{ d g^{-1}(y) } \\over { d y }} \u0026gt;0$ であり、したがって $$ {{ d x } \\over { d y }} = \\left| {{ d x } \\over { d y }} \\right| $$\nケース 2. $g$ が減少関数の場合 $$ \\begin{align*} F_{Y}(y) =\u0026amp; P \\left( Y \\le y \\right) \\\\ =\u0026amp; P \\left( g(X) \\le y \\right) \\\\ =\u0026amp; P \\left( X \\le g^{-1}(y) \\right) \\\\ =\u0026amp; 1- F_{X}\\left( g^{-1}(y) \\right) \\end{align*} $$ 同様に $\\displaystyle f_{Y}(y) = - f_{X} \\left( g^{-1} (y) \\right) {{ d x } \\over { d y }}$ である。$g$ が減少関数なので $\\displaystyle {{ d x } \\over { d y }} \u0026lt; 0$ であり、したがって $$ - {{ d x } \\over { d y }} = \\left| {{ d x } \\over { d y }} \\right| $$\n■\n厳密な定義 測度論で定義される確率変数と確率分布 測度論で定義される累積分布関数 測度論で定義される離散確率分布 測度論における絶対連続 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p32~41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1433,"permalink":"https://freshrimpsushi.github.io/jp/posts/1433/","tags":null,"title":"数理統計学における確率変数と確率分布"},{"categories":"수리통계학","contents":"定義 1 同じ条件下で繰り返しできる試行を無作為試行Random Experimentと呼ぶ。 無作為試行で得られる全ての結果Outcomeを集めた集合$\\Omega$を標本空間Sample Spaceと呼ぶ。 標本空間の中で我々が興味を持っている結果の集合、即ち$B \\subset \\Omega$を事象Eventと言い、これらの集合を$\\mathcal{B}$のように表す。 次の三つの条件を満たす関数$P : \\mathcal{B} \\to \\mathbb{R}$を確率probabilityと呼ぶ： (i): 全ての$B \\in \\mathcal{B}$に対して$P(B) \\ge 0$ (ii): 全体空間$\\Omega \\in \\mathcal{B}$に対して$P(\\Omega) = 1$ (iii) 確率の加法定理Additive Law of Probability：相互に素な事象の列$\\left\\{ B_{i} \\right\\}_{i=1}^{\\infty}$、即ち$n \\ne m \\implies B_{n} \\cap B_{m} = \\emptyset$の$\\left\\{ B_{i} \\right\\}$に対して $$ P \\left( \\bigcup_{i=1}^{\\infty} B_{i} \\right) = \\sum_{i=1}^{\\infty} P \\left( B_{i} \\right) $$ 説明 数理統計学だとしても、基本的にその概念自体は教育課程内の確率、大学レベルの確率論で使用するものと変わらない。理論の基礎がどうであれ、表現や論法が異なることはあっても概念は変わらない。集合と関数に圧倒されず、ゆっくりと説明を読んでみよう：\n事象と標本空間 高校レベルの確率統計と異なる点があるとすれば、もう少し積極的に集合を使用して確率という概念を描写するということだ。実際、大学レベルの数理統計学で扱う確率の概念でさえまだ「無作為試行」とか「興味を持っている」などの曖昧な表現が残っているが、初めて接する立場からするとこれでも厳格で難しく感じることがあるかもしれない。正常だから心配するな。\n人間の身長が正規分布に従うと仮定したら、標本空間 $\\Omega$ は実数集合$\\mathbb{R}$そのものになる。確かに身長は必ず正の値でなければならないだろうが、そのような不必要な厳密さは一旦置いておこう。それではある事象$B$とは、アダムAdamという男性の身長$x$を測定した時に、それを含む集合として表される。例えば$[172,190] \\subset \\Omega$は、身長を測定した時にそれが172以上かつ190以下である事象となる。この測定は、定義で説明された無作為試行であり、そのように測定された値$x$は結果であり、そのような結果として得られる全てのケースを集めたものが標本空間である。このような抽象化を理解できなくても、数理統計学を学ぶ上で大きな問題にはならないかもしれない。しかし、それだけで基盤が不安定になることを覚悟しなければならない。\n抽象化の次のステップは形式化である。事象$B \\subset \\Omega$が$\\Omega$のべき集合$\\mathscr{P}(\\Omega)$に属する。これらを集めた$\\mathcal{B}$について、いくつかの関係をチェックしてみよう。 $$ B \\subset \\Omega \\\\ \\mathcal{B} \\not\\subset \\Omega \\\\ B \\in \\mathscr{P}(\\Omega) \\\\ B \\in \\mathcal{B} \\\\ B \\notin \\Omega \\\\ \\mathcal{B} \\subset \\mathscr{P}(\\Omega) $$\n確率 このような複雑な表現を使用する理由は、確率（関数）$P$の定義域が標本空間$\\Omega$そのものではなく、事象でなければならないためである。高校レベルで言えば、アダムの身長が正確に181である（$x=181$）確率$\\displaystyle \\int_{181}^{181} f(x) dx = 0$は関係なく、180より大きく182より小さい（$180\u0026lt;x\u0026lt;182$）確率$\\displaystyle \\int_{180}^{182}f(x) dx \u0026gt; 0$のように計算すべきだと理解してもよい。確率はある事象の可能性を$0$から$1$までの数値で量る関数である。\n全体空間、つまり$\\Omega$に対して$P(\\Omega)=1$ということは直感的に言えば「何かが起こる確率は100%だ」ということになる。数式的には「確実に起こることよりも確実なことはない」と説明できるだろう。\n相互排他的事象 事象$B \\subset \\Omega$に対して次を満たす事象$A \\subset \\Omega$を$B$の相互排他的事象Exclusive Eventと呼ぶ。 $$ P \\left( B \\cap A \\right) = 0 $$ 相互排他的事象の明白な例には$\\emptyset$や$B^{C}$などがあるが、定義が正確に$B \\cap A = \\emptyset$を言っているわけではないことを覚えておく必要がある。どこまでも相互排他的事象は確率によって定義され、具体的に集合としてこれらがどのように見えるかは関係ない。\n厳密な定義 測度論によって厳密に定義された確率 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1431,"permalink":"https://freshrimpsushi.github.io/jp/posts/1431/","tags":null,"title":"数理統計学における確率と確率の加法定理"},{"categories":"줄리아","contents":"코드 julia\u0026gt; x1=[1 2 3]\r1×3 Array{Int64,2}:\r1 2 3\rjulia\u0026gt; x2=[1, 2, 3]\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; x3=[i for i in 1:3]\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; x4=[i for i in 1:3:10]\r4-element Array{Int64,1}:\r1\r4\r7\r10\rjulia\u0026gt; x5=[i for i in 1:3:11]\r4-element Array{Int64,1}:\r1\r4\r7\r10 x1は2次元配列です。行ベクトルと同じように見えるため、成分座標を1つだけ入力すると、行ベクトルのように認識されます。x2, x3, x4, x5は1次元配列です。\nx=[i for i in n:m]と入力すると、$n$から$m$までの間隔が$1$の配列を返します。 x=[i for i in n:k:m]と入力すると、$n$から$m$までの間隔が$k$の配列を返します。 最後の要素は$m$以下で最も大きい数です。このようにリスト内にfor文を含めてリストを作ることを、Pythonなどでリスト内包List Comprehensionと言います。\njulia\u0026gt; x6=1:3\r1:3\rjulia\u0026gt; x7=1:3:10\r1:3:10\rjulia\u0026gt; x9=1:3:11\r1:3:10 上記のように書くと実際にはそうではないが、事実上x3, x4, x5のような配列が作成されたと考えても良いです。下の写真に示されるように、データタイプは異なるが上で作成されたものと同様に使用できます。\nrange(n,stop=m,length=k): これはマットラボでのlispace(n,m,k)と全く同じです。具体的な違いは以下の例のコードと結果を通じて確認しましょう。 julia\u0026gt; x9=range(1,stop=10)\r1:10\rjulia\u0026gt; x10=range(1,length=15)\r1:15\rjulia\u0026gt; x11=range(1,stop=10,length=15)\r1.0:0.6428571428571429:10.0\rjulia\u0026gt; x12=range(1,length=15,stop=10)\r1.0:0.6428571428571429:10.0 このコードも同様に、実際のデータタイプは異なるが事実上同じベクトルを生成すると考えても良いです。マットラボとは異なり、2番目、3番目の変数のうちの1つだけを入力することもでき、入力する順序を変えても構いません。\n最初の行 は、最初の要素が$1$、最後の要素が$10$のベクトルを返します。他に入力されたものがないので、要素間の間隔は$1$です。 二番目の行 は、最初の要素が$1$で、合計$15$個の要素を持つベクトルを返します。間隔は自動的に$1$となり、x=range(1,stop=15)あるいはx=1:15で作成されたベクトルと同じです。 三番目の行 は、最初の要素が$1$、最後の要素が$10$で、合計$15$個の要素を持つベクトルを返します。したがって、間隔は自動的に$9/14=0.6428571428571429$となります。これは整数ではないので、自然と実数要素を持つベクトルを返します。また、x=1.0:0.6428571428571429:10.0で作成されたものと同じです。 四番目の行 は、三番目の行で返されたベクトルと正確に同じベクトルを返します。 타언어 マットラボで等間隔の行ベクトルを生成する方法 환경 OS: Windows10 Version: 1.5.0 ","id":1452,"permalink":"https://freshrimpsushi.github.io/jp/posts/1452/","tags":null,"title":"ジュリアでベクターを生成する様々な方法"},{"categories":"줄리아","contents":"説明 circshifr(A, (n,m))を使用すると、配列Aの行を$n$カン下にシフトさせ、列を$m$カン右にシフトさせることができる。(n,m)は整数から成るタプルでなければならず、もちろん負の数も可能だ。負の数の場合は逆方向に平行移動される。\n3次元以上の配列の場合は、一番小さい2次元配列にそれぞれ適用される。\nコード 2次元配列 julia\u0026gt; A = transpose(reshape(1:25,5,5))\r5×5 LinearAlgebra.Transpose{Int64,Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}}}:\r1 2 3 4 5\r6 7 8 9 10\r11 12 13 14 15\r16 17 18 19 20\r21 22 23 24 25\rjulia\u0026gt; circshift(A, (-1,0))\r5×5 Array{Int64,2}:\r6 7 8 9 10\r11 12 13 14 15\r16 17 18 19 20\r21 22 23 24 25\r1 2 3 4 5\rjulia\u0026gt; circshift(A, (0,3))\r5×5 Array{Int64,2}:\r3 4 5 1 2\r8 9 10 6 7\r13 14 15 11 12\r18 19 20 16 17\r23 24 25 21 22\rjulia\u0026gt; circshift(A, (-1,3))\r5×5 Array{Int64,2}:\r8 9 10 6 7\r13 14 15 11 12\r18 19 20 16 17\r23 24 25 21 22\r3 4 5 1 2 高次元配列 julia\u0026gt; B = reshape(1:4*4*3,4,4,3)\r4×4×3 reshape(::UnitRange{Int64}, 4, 4, 3) with eltype Int64:\r[:, :, 1] =\r1 5 9 13\r2 6 10 14\r3 7 11 15\r4 8 12 16\r[:, :, 2] =\r17 21 25 29\r18 22 26 30\r19 23 27 31\r20 24 28 32\r[:, :, 3] =\r33 37 41 45\r34 38 42 46\r35 39 43 47\r36 40 44 48\rjulia\u0026gt; circshift(B,(-1,0))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r2 6 10 14\r3 7 11 15\r4 8 12 16\r1 5 9 13\r[:, :, 2] =\r18 22 26 30\r19 23 27 31\r20 24 28 32\r17 21 25 29\r[:, :, 3] =\r34 38 42 46\r35 39 43 47\r36 40 44 48\r33 37 41 45\rjulia\u0026gt; circshift(B,(0,2))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r9 13 1 5\r10 14 2 6\r11 15 3 7\r12 16 4 8\r[:, :, 2] =\r25 29 17 21\r26 30 18 22\r27 31 19 23\r28 32 20 24\r[:, :, 3] =\r41 45 33 37\r42 46 34 38\r43 47 35 39\r44 48 36 40\rjulia\u0026gt; circshift(B,(-1,2))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r10 14 2 6\r11 15 3 7\r12 16 4 8\r9 13 1 5\r[:, :, 2] =\r26 30 18 22\r27 31 19 23\r28 32 20 24\r25 29 17 21\r[:, :, 3] =\r42 46 34 38\r43 47 35 39\r44 48 36 40\r41 45 33 37 julia\u0026gt; C = reshape(1:3*3*2*4,3,3,2,4)\r3×3×2×4 reshape(::UnitRange{Int64}, 3, 3, 2, 4) with eltype Int64:\r[:, :, 1, 1] =\r1 4 7\r2 5 8\r3 6 9\r[:, :, 2, 1] =\r10 13 16\r11 14 17\r12 15 18\r[:, :, 1, 2] =\r19 22 25\r20 23 26\r21 24 27\r[:, :, 2, 2] =\r28 31 34\r29 32 35\r30 33 36\r[:, :, 1, 3] =\r37 40 43\r38 41 44\r39 42 45\r[:, :, 2, 3] =\r46 49 52\r47 50 53\r48 51 54\r[:, :, 1, 4] =\r55 58 61\r56 59 62\r57 60 63\r[:, :, 2, 4] =\r64 67 70\r65 68 71\r66 69 72\rjulia\u0026gt; circshift(C,(1,1))\r3×3×2×4 Array{Int64,4}:\r[:, :, 1, 1] =\r9 3 6\r7 1 4\r8 2 5\r[:, :, 2, 1] =\r18 12 15\r16 10 13\r17 11 14\r[:, :, 1, 2] =\r27 21 24\r25 19 22\r26 20 23\r[:, :, 2, 2] =\r36 30 33\r34 28 31\r35 29 32\r[:, :, 1, 3] =\r45 39 42\r43 37 40\r44 38 41\r[:, :, 2, 3] =\r54 48 51\r52 46 49\r53 47 50\r[:, :, 1, 4] =\r63 57 60\r61 55 58\r62 56 59\r[:, :, 2, 4] =\r72 66 69\r70 64 67\r71 65 68 環境 OS: Windows10 バージョン: 1.5.3 (2020-11-09) ","id":1453,"permalink":"https://freshrimpsushi.github.io/jp/posts/1453/","tags":null,"title":"ジュリアで配列を平行移動する方法"},{"categories":"줄리아","contents":"方法1 using LinearAlgebra\rusing Pkg\rPkg.add(\u0026#34;Plots\u0026#34;)\rPkg.add(\u0026#34;Distributions\u0026#34;)\rusing Plots 上のコードは、LinearAlgebraパッケージとPkgパッケージを読み込むこと、そして.add()関数を使ってPlots、Distributionパッケージをインストールするコードを示している。パッケージを読み込むキーワードusingは、数学である定理や論法を使う時に使う言葉に似ている。パッケージをインストールすること自体はPythonよりはRにもっと近いけど、使用法はPythonにもっと似ている。Rと同じようにパッケージ名をダブルクオートで囲む必要があり、一般的にパッケージ名はパスカルケース1で書き、-sをよく付けて複数形にすることが多く2混乱することがある。\n方法2 REPLで]を入力すると、上記のようにパッケージマネージャ環境に切り替わる。バックスペースを押すと、再びREPL環境に戻る。パッケージマネージャ環境でadd package_nameを入力すると、指定したパッケージがインストールされる。\n(@v1.5) pkg\u0026gt; add Plots Resolving package versions... Updating `C:\\Users\\rydbr\\.julia\\environments\\v1.5\\Project.toml` [91a5bcdd] + Plots v1.0.14 No Changes to `C:\\Users\\rydbr\\.julia\\environments\\v1.5\\Manifest.toml 単語の最初の文字を大文字で書く表示方法を言う。例のコードで確認できるように、linear algebraはLinearAlgebraのように各単語の最初の文字を大文字にして、スペースを省略する。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n例のコードで見るように、PlotとDistributionはPlotsとDistributionsとして呼ばなければならない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1416,"permalink":"https://freshrimpsushi.github.io/jp/posts/1416/","tags":null,"title":"Juliaでパッケージをインストールして使用する方法"},{"categories":"위상수학","contents":"定義 1 位相空間 $(X,\\mathscr{T})$と部分集合 $A \\subset X$が与えられたとしよう。すると以下の集合 $$ \\mathscr{T}_{A} =\\left\\{ A\\cap U\\ :\\ U\\in \\mathscr{T} \\right\\} $$ は $A$上の位相である。このとき $\\mathscr{T}_{A}$を部分空間位相Subspace Topologyまたは相対位相と呼ぶ。また、位相空間 $(A, \\mathscr{T}_{A})$を $(X,\\mathscr{T})$の部分空間Subspaceと呼ぶ。\n定理 [0]: 位相空間 $(X, \\mathscr{T}$) と部分集合 $A \\subset X$に対して $$ \\mathscr{T}_{A} = \\left\\{ A\\cap U\\ :\\ U \\in \\mathscr{T}\\right\\} $$ は $A$上の位相となる。 位相空間 $X$と部分空間 $A$が与えられたとしよう。部分空間 $A$で開集合、閉集合の同値条件は以下のようである。\n[a1]: $V\\subset A$が $A$で開集合であるための必要十分条件は、$V= A\\cap U$を満たす $X$での開集合 $U$が存在することである。 [b1]: $F\\subset A$が $A$で閉集合であるための必要十分条件は、$F=A\\cap E$を満たす $X$での閉集合 $E$が存在することである。 部分空間で開集合（閉集合）であっても、全体空間で開集合（閉集合）である保証はない。部分空間が全体空間に対して開集合（閉集合）であれば、その性質が全体空間でも保証される。位相空間 $(X,\\mathscr{T})$と部分空間 $(A,\\mathscr{T}_{A})$、部分集合 $B\\subset A\\subset X$が与えられたとしよう。\n[a2]: $B$が部分空間 $A$で開集合であり、$A$が $X$で開集合であれば、$B$は $X$で開集合である。 [b2]: $B$が部分空間 $A$で閉集合であり、$A$が $X$で閉集合であれば、$B$は $X$で閉集合である。 [3]: $\\mathscr{B}$を位相空間 $(X,\\mathscr{T})$の基底としよう。すると $$ \\mathscr{B}_{A} =\\left\\{ A\\cap B\\ :\\ B\\in \\mathscr{B} \\right\\} $$ は部分空間 $(A,\\mathscr{T}_{A})$の基底である。 説明 混乱しないように、いくつかの表記についてはっきりと確認しておこう。$(X,\\mathscr{T})$が全体空間で $$ \\mathscr{T}_{A}=\\left\\{A\\cap U\\ :\\ U \\in \\mathscr{T} \\right\\} $$ は部分集合 $A$の位相である。つまり、部分空間 $(A,\\mathscr{T}_{A})$を形成する。$\\mathscr{B}$は全体集合 $X$の基底である。$\\mathscr{B}_{A}$は全体集合の基底の各要素と $A$の交差集合のコレクションである。これが部分集合 $A$の基底となるというのが定理の内容である。 $$ \\mathscr{T}_{\\mathscr{B}_{A}}=\\left\\{U_{A}\\subset A\\ :\\ \\forall\\ x \\in U_{A},\\ \\exists\\ (A\\cap B) \\in \\mathscr{B}_{A}\\ \\ \\text{s.t.}\\ x\\in (A\\cap B) \\subset U_{A}\\right\\} $$ さらに進んで、$\\mathscr{T}_{\\mathscr{B}_{A}}$が $\\mathscr{T}_{A}$と同じであることが核心である。内容が複雑に感じられるかもしれない。整理して説明すると次のようになる。\n全体空間 $X$の基底の要素と $A$を交差させたものを集めると、$A$の基底になる。 その基底 $\\mathscr{B}_{A}$で生成した位相は $\\mathscr{T}_{\\mathscr{B}_{A}}$である。 2で生成した位相 $\\mathscr{T}_{\\mathscr{B}_{A}}$は $X$の開集合と $A$を交差させたものの集合である $\\mathscr{T}_{A}$と同じである。 証明 [0] $(T1)$: $A \\cap \\varnothing =\\varnothing$、$A \\cap X=A$であるから、空集合と全体集合が $\\mathscr{T}_{A}$に属する。 $(T2)$: $V_\\alpha \\in \\mathscr{T}_{A}( \\alpha \\in \\Lambda)$としよう。$\\mathscr{T}_{A}$の定義により、各々の $V_\\alpha$に対して、$V_\\alpha = A \\cap U_\\alpha$を満たす $U_\\alpha$が存在する。位相の定義により $U=\\cup_{\\alpha \\in \\Lambda} U_\\alpha \\in \\mathscr{T}$である。そうすると $$ \\bigcup_{\\alpha \\in \\Lambda} V_\\alpha = \\bigcup_{\\alpha \\in \\Lambda} (A \\cap U_\\alpha ) =A\\cap (\\cup_{\\alpha \\in \\Lambda} U_\\alpha ) =A\\cap U \\in \\mathscr{T}_{A} $$ であるから $\\bigcup _{\\alpha \\in \\Lambda} V_\\alpha \\in \\mathscr{T}_{A}$である。 $(T3)$: $V_{1},\\ \\cdots\\ ,V_{n} \\in \\mathscr{T}_{A}$としよう。同様に、各々の $V_{i}$に対して、$V_{i} =A \\cap U_{i}$を満たす $U_{i}$が存在する。そして $U=\\cap _{i} U_{i} \\in \\mathscr{T}$であるから $$ \\bigcap _{i=1}^n V_{i} = \\bigcap_{i=1}^n (A\\cap U_{i}) = A\\cap \\left( \\bigcap_{i=1}^n U_{i} \\right) =A\\cap U \\in \\mathscr{T}_{A} $$ である。従って $\\bigcap_{i=1}^n V_{i} \\in \\mathscr{T}_{A}$である。 位相の条件 三つを満たすので $\\mathscr{T}_{A}$は $A$の上の位相である。\n■\n[a1] 距離空間での証明を参照せよ。$\\mathscr{T}_{A}$の定義により自明である。\n■\n[b1] $(\\implies)$ $F$が $A$で閉集合であるため、$A-F$は $A$で開集合である。そこで[a1]により、$A-F=A\\cap U$を満たす $X$での開集合 $U$が存在する。$U$が開集合であるため、$E=X-U$は $X$で閉集合である。それで $$ A\\cap E=A\\cap (X-U)=A-(A\\cap U)=A-(A-F)=F $$\n$(\\Longleftarrow )$ $E$は $X$で閉集合であるため、$X-E$は $X$で開集合である。そこで[a1]により、$A \\cap (X-E)$は $A$で開集合である。$F^c=A-(A\\cap E)=A\\cap(X-E)$であるから、$F ^c$は $A$で開集合である。従って、$F$は $A$で閉集合である。\n■\n[a2] $B$が $A$で開集合である場合、[a1]により、$B=A\\cap U\\ (U\\in \\mathscr{T})$を満たす $X$での開集合 $U$が存在する。仮定により、$A$は $X$で開集合である。従って、$B$は $X$で開集合の交差であり、$X$で開集合である。\n■\n[b2] $B$が $A$で閉集合である場合、[b1]により、$B=A\\cap E$を満たす $X$での閉集合 $E$が存在する。仮定により、$A$は $X$で閉集合であり、$B$は閉集合同士の交差であるため、$B$も $X$で閉集合である。\n■\n[3] パート1. $\\mathscr{B}_{A}$は $A$の基底である。\n[b1]: 任意の $x\\in A$に対して、$A\\subset X$であるから、$x\\in X$である。$\\mathscr{B}$が $X$の基底であるため、定義により、$x \\in B \\in \\mathscr{B}$を満たす $B$が存在する。従って、$x\\in (A\\cap B ) \\in \\mathscr{B}_{A}$を満たす $A\\cap B \\in \\mathscr{B}_{A}$が存在する。[b2]: 任意の $A\\cap B_{1}$、$A\\cap B_{2}$と $x\\in \\Big( (A\\cap B_{1} ) \\cap (A \\cap B_{2}) \\Big)$に対して $$ (A\\cap B_{1})\\cap (A \\cap B_{2})=A\\cap B_{1}\\cap B_{2} $$ であるため、$x\\in (B_{1}\\cap B_{2})$である。$\\mathscr{B}$が $X$の基底であるため、定義により、$x\\in B_{3} \\subset ( B_{1}\\cap B_{2})$を満たす $B_{3}$が存在する。従って $$ x \\in (A\\cap B_{3})\\subset \\Big( A\\cap (B_{1}\\cap B_{2}) \\Big)=(A\\cap B_{1}) \\cap (A\\cap B_{2}) $$ である。基底となる二つの条件を満たしたので、$\\mathscr{B}_{A}$は部分集合 $A$の基底である。\nパート2. $\\mathscr{T}_{\\mathscr{B}_{A}}=\\mathscr{T}_{A}$である。\n$(\\subset)$ $\\mathscr{B}$が $(X,\\mathscr{T})$の基底であるため、$\\mathscr{T}_{\\mathscr{B}}=\\mathscr{T}$であり、$\\mathscr{B}\\subset \\mathscr{T_{\\mathscr{B}}}=\\mathscr{T}$である。従って、全ての $B \\in \\mathscr{B}$に対して、$B\\in \\mathscr{T}$である。$\\mathscr{T}_{A}$の定義によって、$A\\cap B \\in \\mathscr{T}_{A}$である。従って $$ \\mathscr{B}_{A} \\subset \\mathscr{T}_{A} $$ $\\mathscr{T}_{\\mathscr{B}_{A}}$は $\\mathscr{B}_{A}$を含む最も小さい位相であるため $$ \\mathscr{T}_{\\mathscr{B}_{A}} \\subset \\mathscr{T}_{A} $$\n$(\\supset )$ $V \\in \\mathscr{T}_{A}$と仮定する。[a1]によって、$V=A\\cap U$を満たす $U\\in \\mathscr{T}$が存在する。また、$V$の任意の点 $x\\in V \\subset A$に対して、$x \\in U$である。$\\mathscr{B}$は$\\mathscr{T}$を生成する基底であるため、$x\\in B \\subset U$を満たす $B\\in \\mathscr{B}$が存在する。従って、$A \\cap B \\in \\mathscr{B}_{A}$が $$ x\\in (A\\cap B) \\subset (A\\cap U) =V $$ を満たす。これは、$V$がその $\\mathscr{B}_{A}$が生成する $A$上の位相 $\\mathscr{T}_{\\mathscr{B}_{A}}$に属する条件であるため、$V \\in \\mathscr{T}_{\\mathscr{B}_{A}}$である。従って $$ \\mathscr{T}_{\\mathscr{B}_{A}} \\supset \\mathscr{T}_{A} $$ ■\nMunkres. (2000). 『Topology(2nd Edition)』: p89.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1439,"permalink":"https://freshrimpsushi.github.io/jp/posts/1439/","tags":null,"title":"部分空間トポロジー、相対トポロジー"},{"categories":"집합론","contents":"定義 1 任意の集合 $X$ に対して、次の性質を持つ $\\text{card} X$ を $X$ の濃度Cardinalityと定義する。\n(i): $X = \\emptyset \\iff \\text{card} X = 0$ (ii): $A \\sim B \\iff \\text{card} A = \\text{card} B$ (iii): 何らかの自然数 $k$ について、$X \\sim \\left\\{ 1 , 2, \\cdots , k \\right\\}$ ならば $\\text{card} X = k$ 特に、有限集合の濃度を有限濃度と言い、無限集合の濃度を超限濃度という。\n二つの集合 $A$、$B$ において、$A$ が $B$ の何らかの部分集合と等号であるが、$B$ は $A$ のどの部分集合とも等号でない場合、$\\text{card} A$ は $\\text{card} B$ より小さいと言って、以下のように表示される。 $$ \\text{card} A \u0026lt; \\text{card} B $$ 相互に素な二つの集合 $A$、$B$ がそれぞれ濃度 $a = \\text{card} A$、$b =\\text{card} B$ を持つ場合、その和集合の濃度を**$a$、$b$ の（濃度）和**と言い、以下のように表示される。 $$ \\text{card} \\left( A \\cup B \\right):= a+b $$ 二つの集合 $A$、$B$ がそれぞれ濃度 $a = \\text{card} A$、$b =\\text{card} B$ を持つ場合、そのデカルト積の濃度を**$a$、$b$ の（濃度）積**と言い、以下のように表示される。 $$ \\text{card} \\left( A \\times B \\right):= ab $$ 二つの集合 $A$、$B$ がそれぞれ濃度 $a = \\text{card} A$、$b =\\text{card} B$ を持つ場合、定義域 $A$ と値域 $B$ を持つ全ての関数の集合 $B^{A}$ の濃度を**$b$ の $a$ （濃度）乗**と言い、以下のように表示される。 $$ \\text{card} \\left( B^{A} \\right):= b^{a} $$ 説明 濃度は「集合のサイズ」を抽象化したもので、無限集合に対しても数学的に意味のある比較をするために導入されたと考えても差し支えない。集合のサイズから来た概念であるため、集合論が核でないか、便宜上 $|X| := \\text{card} X$ のように簡潔に表示されることもある。\n濃度は自然数に似た以下のような代数的性質を持つ。\n基本性質 1 $x,y,z$ を濃度と仮定する。\n[1]: $$|A| \\le |B| \\land |A| \\ge |B| \\implies |A| = |B|$$ [2]: $$x + y = y+x \\\\ (x+y) + z = x + (y + z)$$ [3]: $$xy = yx \\\\ (xy)z = x(yz) \\\\ x ( y+z) = xy + xz$$ [4]: $$z^{x} z^{y} = z^{x+y} \\\\ \\left( z^{y} \\right)^{x} = z^{yx}$$ 李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p241.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1395,"permalink":"https://freshrimpsushi.github.io/jp/posts/1395/","tags":null,"title":"集合の濃度"},{"categories":"집합론","contents":"定義 1 集合 $X$ が有限集合あるいは $X \\sim \\mathbb{N}$ の場合、可算集合と呼ぶ。 可算集合でない集合を非可算集合と呼ぶ。 $\\mathbb{N}$ は自然数の集合だ。 $X \\sim Y$ の $\\sim$ は集合の等価を言う。 説明 可算集合という概念は、韓国人を含む東洋人にとって受け入れやすいわけではない。これは英語を含むインド・ヨーロッパ語族の思考方法と私たちのマインドが根本的に異なることによる。あるのは、ヨーロッパの言語は名詞に性があり、数や格に応じて動詞と形容詞が変化するなど、「共感しにくい」文法特性を持つことが多い。特にこの数については、文法に触れる前に、そもそもなぜ区分されるのか理解しにくいものが多いが、実はこの言語的思考方法の違いが東西の数学の違いとして表れる。\n可算、つまり「一つ、二つ、\u0026hellip;」と「いくつ」か数えることができる対象を言う。例えば人間や腕時計、オレンジなどがそうだ。数えられないものとしては、水やパンのように数ではなく量があり、任意に分割できるものを言う。だから A dogは犬一匹で、Dogsは複数の犬だが、Dogは犬肉を意味することになる。もちろん言葉には文脈があり、これほど極端に解釈はされないが、可能であるという話だ。\nこの違いをなぜ区別するかを理解させるよりも、私たちも変なものを使っていると説明する方がよかった。例えば、韓国を含む東アジア国家は「単位」というまったく役に立たなさそうな言葉を使う。例えば人は人、動物は匹、長く細いものは本、薄く広いものは枚、建物は棟などだ。韓国語は必ずこのように使い、英語ではこれらの表現を全く使わないという意味ではなく、思考の基底でこれを自然に受け入れているという点が重要だ。\nつまり、鉛筆を数えるとき「鉛筆一本」と単に言うこともできるけど、「鉛筆一本貸して」と言ったとき、なぜ鉛筆を本で数えるのか変だと感じない「感覚」が実際の言語習慣を決定づける。一方で、英語を骨まで受け入れないと、コミュニケーションに問題がないほど英語を上手に話せても、a, theなどの冠詞の使い方がめっちゃくちゃで、どこかおかしくならざるを得ない。言語って本来こんなものだ。受け入れればいいし、受け入れざるを得ない。そして言語間の違いは、元々使っていたものだから、そういうものだと思ってスルーしても実は全く問題ない。\n[1]：$X \\sim \\mathbb{Z}$ ならば、$X$ は可算集合だ。 [2]：$X \\sim \\mathbb{Q}$ ならば、$X$ は可算集合だ。 [3]：$X \\sim \\mathbb{R}$ ならば、$X$ は非可算集合だ。 しかし驚くべきことに、こうした違いは実際の数学でも現れ、[3]のように具体的に非可算集合を提案できる。この事実は集合論の父、ゲオルク・カントールによって証明され、彼自身もそうした非可算集合の存在に驚いたという。しかし、可算と非可算を抜きにしても、東アジアの数学ではこれらの概念が思い浮かぶことがあったのだろうか？不可能だと断言はできないが、ピタゴラス以後、非可算集合を発見するまでにかかった時間はなんと約2500年である。東洋で純粋数学が発展していたら、東洋独自の視点で驚くべき成果を見つけ出していたかもしれないが、可算と非可算の概念は間違いなくインド・ヨーロッパ言語族が残した遺産だ。\n証明 おそらくカントールは、すべての無限集合が可算集合であることを証明しようとしたのかもしれない。直感的にもその方が簡単で、本当であればすべての集合を自然数に引き下げて考えることができるからだ。カントールの旅を追って、まず[1]と[2]の証明を見てみよう。\n[1] $\\mathbb{N}$ と $\\mathbb{Z}$ の間に全単射が存在することを示せばよい。次のような対応関係を定義すると全単射となる。 $$ (1,2,3,4,5, \\cdots ) \\mapsto (0,-1,1,-2,2, \\cdots ) $$\n■\n[2] $\\mathbb{N}$ と $\\mathbb{Q}$ の間に全単射が存在することを示せばよい。次のような対応関係を定義してみよう。 $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 6 \u0026amp; 7 \u0026amp; \\cdots \\\\ 3 \u0026amp; 5 \u0026amp; 8 \u0026amp; \\ddots \u0026amp; \\\\ 4 \u0026amp; 9 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ 10 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ \\vdots \u0026amp; \u0026amp; \\end{bmatrix} \\mapsto \\begin{bmatrix} 1/1 \u0026amp; 1/2 \u0026amp; 1/3 \u0026amp; 1/4 \u0026amp; \\cdots \\\\ 2/1 \u0026amp; 2/2 \u0026amp; 2/3 \u0026amp; \\ddots \u0026amp; \\\\ 4/1 \u0026amp; 4/2 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ 5/1 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ \\vdots \u0026amp; \u0026amp; \\end{bmatrix} $$ ここで約分を行なった後、重複する要素とそうでない要素が生じる。例えば、$2/2 = 1/1$ より重複である。これから、このような要素を重複しない要素に順番に$-1$ を掛けた要素に対応させてみよう。 $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 6 \u0026amp; 7 \u0026amp; \\cdots \\\\ 3 \u0026amp; 5 \u0026amp; 8 \u0026amp; \\ddots \u0026amp; \\\\ 4 \u0026amp; 9 \u0026amp; \\ddots \u0026amp; \\\\ 10 \u0026amp; \\ddots \u0026amp; \\\\ \\vdots \u0026amp; \u0026amp; \\end{bmatrix} \\mapsto \\begin{bmatrix} 1/1 \u0026amp; 1/2 \u0026amp; 1/3 \u0026amp; 1/4 \u0026amp; \\cdots \\\\ 2/1 \u0026amp; -(1/1) \u0026amp; 2/3 \u0026amp; \\ddots \u0026amp; \\\\ 4/1 \u0026amp; -(1/2) \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ 5/1 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ \\vdots \u0026amp; \u0026amp; \\end{bmatrix} $$ すると、この対応は全単射となる。\n■\n[3] 中学校から数を学ぶときは、自然数、整数、有理数、実数、複素数の順で学ぶが、ここまで証明したカントールがやろうとしたのは明白である。つまり、$\\mathbb{N} \\sim \\mathbb{R}$ であることを示す全単射を見つけようということだ。しかし、推測するにその試みは次々に失敗し、結局そのような全単射が存在しないことを証明しようとしたのだろう。このとき使用した方法が、その有名なカントールの対角線論法である。\n李興天 訳、林游鳳. (2011). 集合論(Set Theory: An Intuitive Approach): p219.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1383,"permalink":"https://freshrimpsushi.github.io/jp/posts/1383/","tags":null,"title":"可算集合と不可算集合"},{"categories":"집합론","contents":"## 定義 [^1] [^1]: ユウ・フォン・リン 著、イ・フンチョン 訳 (2011). 集合論(Set Theory: An Intuitive Approach): p205, 215。 1. 二つの[集合](../1316) $X,Y$ に対して[全単射](../471) $f : X \\to Y$ が存在する場合、$X$ と $Y$ が **等势**\u0026lt;sup\u0026gt;Equipotent\u0026lt;/sup\u0026gt;であるといい、$X \\sim Y$ と表示される。 2. [空集合](../1337)ではない $X$ のどの[真部分集合](../1329) $Y \\subsetneq X$ に対しても、$X \\sim Y$ が成立する場合、$X$ を **無限集合**という。 3. 無限集合でない集合を **有限集合**という。 ## 説明 1. 集合論を用いずに無限を説明しようとする時、「等势」という表現は、柵から羊を出す羊飼いに例えられることがある。十分な数の小石を用意しておき、羊が一匹柵から出るたびに小石を一つかごに入れる。そうすれば、柵から出た羊の数と小石の数は同じになるだろう。逆に、羊を再び柵に入れる際には、かごから小石を一つずつ取り出しながら数えればいい。この数が正確に一致すれば、羊飼いは羊を失くさなかったということになる。 この抽象化されたものは、かごに小石を入れたり取り出したりすることが全単射 $f$ が示す対応に相当する。例えば、自然数の集合 $\\mathbb{N}$ は全単射 $g(n) = 2n$ が存在し、偶数の集合 $2 \\mathbb{N}$ と等势である。もちろん、このような対応は閉区間 $[0,1]$ に対しても $g(x) = 2x$ を通して存在し$[0,1] \\sim [0,2]$ を示す。ここで、$2 \\mathbb{N} \\subsetneq \\mathbb{N}$ があり、$[0,1] \\subsetneq [0,2]$ であることに注目しろ。等势という概念が集合の大きさを説明するために導入されたことは確かだが、それが包含関係に繋がるわけではない。 2. 無限集合の定義が「無限」という言葉を使わずに済ませていることに注目しろ。これは、無限の本質自体が「全体と部分という概念の間に何かを許す」ということを意味している。このようにして、人間の直感に自然に浮かぶ無限は異なり、集合論が生まれた。これは **ヒルベルトのホテル** をはじめとする比喩で説明されることが多い。 ## 基本性質 以下は有限集合と無限集合が持つ基本的な性質である。等势の概念を理解すれば、それほど難しく考える必要はないだろう。 - [0] [空集合](../1337)は[有限集合](../1381)である。 - [1] 無限集合の上位集合は無限集合である。 - [2] 有限集合の部分集合は[有限集合](../1381)である。 - [3] 無限集合と等势ならば無限集合である。 - [4] [有限集合](../1381)と等势ならば有限集合である。 - [5] 無限集合から有限集合を引いた差集合は無限集合である。 ","id":1381,"permalink":"https://freshrimpsushi.github.io/jp/posts/1381/","tags":null,"title":"集合論により厳格に定義される有限集合と無限集合"},{"categories":"줄리아","contents":"julia\u0026gt; function add1(a,b)\rreturn a+b\rend\radd1 (generic function with 1 method)\rjulia\u0026gt; add1(1,2)\r3\rjulia\u0026gt; add(1,2.0)\rERROR: UndefVarError: add not defined\rStacktrace:\r[1] top-level scope at REPL[43]:1\rjulia\u0026gt; function add2(a::Int64, b::Float64)\rreturn a+b\rend\radd2 (generic function with 1 method)\rjulia\u0026gt; add2(1,2)\rERROR: MethodError: no method matching add2(::Int64, ::Int64)\rClosest candidates are:\radd2(::Int64, ::Float64) at REPL[44]:1\rStacktrace:\r[1] top-level scope at REPL[45]:1\rjulia\u0026gt; add2(1,2.0)\r3.0 上のように :: を使って変数が具体的に何であるかを知らせると、タイプが合わない時にエラーが出る。こうしてアノテーションがされていれば、タイプをチェックする必要がなくなるから、当然パフォーマンスが向上する。\nEnvironment OS: Windows julia: v1.5.0 ","id":1379,"permalink":"https://freshrimpsushi.github.io/jp/posts/1379/","tags":null,"title":"ジュリアのタイプとアノテーション"},{"categories":"집합론","contents":"定義 1 $x \\in X$ と $y \\in Y$ と $f: X \\to Y$が関数だとしよう。\nあらゆる $x_{1}, x_{2} \\in X$ に対して $x_{1} \\ne x_{2} \\implies f(x_{1}) \\ne f(x_{2})$ ならば $f$ を単射injectiveという。 $f(X) = Y$ ならば $f$ を全射surjectiveという。 $f$ が単射であり、かつ全射ならば全単射bijectiveという。 $I(x) = x$ を満たす $I : X \\to X$ を恒等関数Identity Functionという。 あらゆる $x, y$ に対して $f(x) = y$ かつ $f^{-1} (y) = x$ を満たす時、$f^{-1} : Y \\to X$ を$f$ の逆関数Inverse Functionという。 基礎的性質 [2]: $f$ が全単射であることと逆関数 $f^{-1}$ が存在することは同値だ。 説明 単射を一対一one-to-one、あるいは一対一関数ともいう。 全射をontoともいう。 全単射を一対一対応1-1 correspondingともいう。 入試数学では本当に重要でないと思われがちだが、一対一対応はとても重要な概念である。数学で苦労してる多くの学生は、この事実を聞いたことがないか、聞いても問題解決に役立たないと思うことが多い。まったく間違っているわけではないが、このレベルを知らなければ、問題解決に必要な他のこともよく知らない可能性が高い。\nそれなりにできる学生も、大学レベルの数学に触れた時に初めて全単射を本当に理解することが多い。一対一対応は集合論に限らず、広大な数学の世界で、それがどんな科目であれ、最も重要な概念である。しかし、そのほど強力で良好な条件なので、逆説的に、数学者は全単射の条件を緩和する方向で研究することになる。どのようにして他の条件で全単射であるかを導出するか、実際には全単射ではないが全単射として使うことができるかなどだ。\nどれほど重要かを簡単に説明すると、「本当に重要だから正確に知っておくべきだ」と言う必要もないほどだ。好むと好まざるとにかかわらず、全単射はあらゆる科目で出てくるので、むしろ卒業するまでに全単射を正確に知らない方が難しいだろう。\n李興天 訳, 林游峰. (2011). 集合論(Set Theory: An Intuitive Approach): p165, 181~187.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":471,"permalink":"https://freshrimpsushi.github.io/jp/posts/471/","tags":null,"title":"単射, 全射, 全単射, 逆関数"},{"categories":"줄리아","contents":"概要 ジュリアはMITで開発され、2012年に公開されたプログラミング言語で、高い生産性と速度を目指している。Cやフォートランと同等の速度を実現しながらも、PythonやRのような高レベルの構文を備えており、その他の多くの言語の利点を取り入れている。2019年11月現在、GPUの急速な発展とディープラーニングの流行により少し遅れているのは事実だが、ジュリアを超えるほど便利で速い言語は学界にはない。\nジュリアについて質問やコメントがあれば、生エビ寿司屋コミュニティ 💬に投稿してみよう。\n特長 ジュリアの特長を見てみよう：\n速い。他の言葉は必要ない。ベンチマークを見れば、Cやフォートランと肩を並べていることがわかる。 簡単だ。最適化まで気にしなければ、ジュリアのプログラミングは、マットラボ、R、パイソンと似たり寄ったりだ。ただし、これらの言語で何かプログラムを実装した後、最適化のためにCで同じコードを書き直す必要がない。ジュリアに適したコーディング技術を通じて改善されるだけだ。同じ理由で、自然科学系のユーザーがマットラボ、R、パイソンを知っていれば、ジュリアもすぐに慣れることができる。実際、これらの言語の生産性は非常に高いため、どんなプログラミング言語がベースであっても、ジュリアは簡単に感じられる。 無料だ。マットラボは強力な線形代数をサポートしているが、まず高価であり、実際には最適化によってジュリアがマットラボよりも10倍から1,000倍速いと言われている。さらに、MATLAB.jlパッケージを使用すると、マットラボと同じスタイルでコードを書くことができ、マットラボに熟練したユーザーが移行しやすい。 他言語のパッケージを簡単に取り込める。新しい言語の最大の弱点であり、フォートランやパイソンなどの言語が使用される主な理由の一つは、まさにパッケージである。パイソンの場合は、PyCall.jlパッケージを使用することで、パイソンの関数を直接呼び出すことができる。もちろん、ジュリアはすでに十分な高性能を備えているが、ccall()関数を使用することでCやフォートランの関数を呼び出すことができる。C++もCpp.jlパッケージでサポートされている。 並行処理に特化している。他の言語がその後に関連するパッケージを開発したのとは異なり、ジュリアは最初から並行処理を念頭に置いて開発された。実際、プログラムによっては、便利というよりも最適化の核心となることもある。 ","id":1374,"permalink":"https://freshrimpsushi.github.io/jp/posts/1374/","tags":null,"title":"ジュリアプログラミング言語"},{"categories":"집합론","contents":"定義 1 関数 $f: X \\to Y$ と $B \\subset Y$ について、$f^{-1}(B): = \\left\\{ x \\in X \\ | \\ f(x) \\in B \\right\\}$ を $f$ による $B$ の原像または逆像という。\n解説 表記は似ているが、定義だけで逆像と逆関数が関係しているとは言えず、これらを混同してはならない。\n韓国語で話すときは逆像が自然だが、英語では[プレイメージ]が自然と感じる人がいるだろう。これは、逆を意味する漢字が単に「どこから来たか」という逆像の概念によく合うのに対して、上で述べたように[インバース]という言葉は逆関数を連想させるため、意識的に使うのを避けるからだ。もちろん、単に[プレイメージ]が発音しやすいのでよく使われる、前置詞としての「元」が馴染みがないので使わない、などの単純な理由もある。\n基本性質 [1] 空集合: $$ f ( \\emptyset ) = \\emptyset $$ [2] 単元素集合: $$ x \\in X \\implies f \\left( \\left\\{ x \\right\\} \\right) = \\left\\{ f(x) \\right\\} $$ [3] 単調性: $$ A \\subset B \\subset X \\implies f (A) \\subset f(B) \\\\ C \\subset D \\subset Y \\implies f^{-1} (C) \\subset f^{-1} (D) \\\\ f(X) \\subset Y \\iff X \\subset f^{-1} (Y) $$ [4] 和集合: $$ f \\left( \\bigcup_{\\gamma \\in \\Gamma} A_{\\gamma} \\right)= \\bigcup_{\\gamma \\in \\Gamma } f \\left( A_{\\gamma} \\right) \\\\ f^{-1} \\left( \\bigcap_{\\gamma \\in \\Gamma} A_{\\gamma} \\right) = \\bigcap_{\\gamma \\in \\Gamma } f^{-1} \\left( A_{\\gamma} \\right) $$ [5] 交差点: $$ f^{-1} \\left( \\bigcup_{\\gamma \\in \\Gamma} A_{\\gamma} \\right)= \\bigcup_{\\gamma \\in \\Gamma } f^{-1} \\left( A_{\\gamma} \\right) \\\\ f \\left( \\bigcap_{\\gamma \\in \\Gamma} A_{\\gamma} \\right) {\\color{red}\\subset} \\bigcap_{\\gamma \\in \\Gamma } f \\left( A_{\\gamma} \\right) $$ [6] 差集合: $$ f (A) \\setminus f (B) \\subset f (A \\setminus B) \\\\ f^{-1} (C) \\setminus f^{-1}(D) = f^{-1} (C \\setminus D) $$ 特に [5]、[6]では、関数は交差点をそのまま保持できないことに注意。等号を満たすためには、$f$ が単射でなくてはならない。\n全単射と逆関数の概念は繰り返しを通じて慣れることができるが、逆像に関しては、できるだけ早く、正確に学ぶ必要がある。逆像を大まかに理解しておいても、すぐに線形代数学におけるゼロ空間に対する直感が落ち、そのまま抽象代数学まで影響を及ぼす。関数の像とは異なる性質が多いので、ただ反対だと思って見過ごすのではなく、ちゃんと勉強して、しっかり理解するようにしよう。\n李興天 訳、You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p173.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":472,"permalink":"https://freshrimpsushi.github.io/jp/posts/472/","tags":null,"title":"関数の原像"},{"categories":"집합론","contents":"定義 1 空集合じゃない二つの集合 $X$、$Y$ が与えられたとする。\n二項関係 $f \\subset (X,Y)$ が次を満たすとき、関数と呼び、$f : X \\to Y$ と表す。 $$ (x ,y_{1}) \\in f \\land (x,y_{2}) \\in f \\implies y_{1} = y_{2} $$ 関数 $f : X \\to Y$ において、$\\text{Dom} (f) = X$ を$f$ の定義域Domainといい、$Y$ を$f$ の共変域Codomainという。定義域の部分集合 $A \\subset X$ が与えられたとき、$f(A):= \\left\\{ f(x) \\in Y \\ | \\ x \\in A \\right\\}$ を$f$ に対する$A$ の像Imageという。特に $f$ に対する定義域 $X$ の像 $\\text{Im} f := f(X)$ を$f$ の値域Rangeという。 定義域が自然数の集合 $\\mathbb{N}$ の関数を数列Sequenceという。 定義域 $A$ と共変域 $B$ を持つすべての関数 $\\lambda : A \\to B$ の集合を $B^{A}$ と表す。 説明 教科書レベルでは、\u0026lsquo;全ての元 $x_{1}, x_{2} \\in X$ に対して、$x_{1} = x_{2} \\implies f(x_{1}) = f(x_{2})$ を満たす $f(x_{1})$ と $f(x_{2})$ が $Y$ に存在するなら、対応 $f : X \\to Y$ を$X$ から $Y$ への関数とする\u0026rsquo; と言われがちだ。しかし、この \u0026lsquo;対応\u0026rsquo; あるいは \u0026lsquo;写像\u0026rsquo; は、表現がやや曖昧だった。大学レベル以上の数学では、集合論を用いて、関数に関連する概念を厳格に定義する。入力があれば出力がただ一つだけ出る、という説明は、コンピュータ科学の関数にもっと適した説明だ。 なぜ値域を別に定義するのか疑問に思うかもしれない。例えば、$f(x) = x^2$ を考えると、明らかに関数の値は $\\left[ 0,\\infty \\right)$ に属していて、$f : \\mathbb{R} \\to \\mathbb{R}$ のように無駄に大きくする必要はないと見える。もともと値域は共変域の部分集合であり、実際には使われない値をなぜ別にするのか理解しにくいことだ。これは、あまりにも簡単な例だけを考えることからくる誤解で、すべての関数が定義時から値域を簡単に予測できるわけではない。関数を定義するときに保証できるのは、$x \\in X$ に対して $f(x) \\in Y$ が存在することだけで、それが何かはわからない。もし $$ f(x) = \\sin \\ln \\sqrt{x} + \\int_{1}^{3^x} {{1} \\over {7t+t^2}} dt $$ のような複雑な関数があれば、定義時からその値域を知ることは不可能であり、必要もない。値域を知ることが重要なのは、合成関数を定義するときくらいだ。 数列が単に数を並べたものだという説明よりも、もっとシンプルで一般的な定義ができたことを確認できる。共変域が「数」であることに限らず、関数であれ、いかにも珍しい集合であれ、全てをカバーできるようになった。このような抽象化は、ただちに数列の概念をきれいに表現できるだけでなく、無限を扱う数学のさまざまな分野で柔軟に使用できる。 関数の集合という概念自体が新しいかもしれないが、抽象数学では、関数空間のような集合を日常的に言及する。$B^{A}$ のような記法をなぜ使うのかは、基数を思い出せば理解しやすい。例えば、$B$ が共変域であり、$A$ が定義域であるすべての関数を考えると、 $$ e \\mapsto 1 \\text{ or } 2 \\text{ or } 3 \\\\ \\pi \\mapsto 1 \\text{ or } 2 \\text{ or } 3 $$ 全ての組み合わせは $9 = 3^2 = |B|^{|A|}$ になる。 李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p157~159.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":470,"permalink":"https://freshrimpsushi.github.io/jp/posts/470/","tags":null,"title":"集合論によって厳密に定義される関数と写像、数列"},{"categories":"집합론","contents":"定義 1 集合 $X$ の全ての部分集合 $A,B,C$ について、次の条件を満たす $\\mathscr{P} \\subset 2^{X}$ を $X$ の分割と言う。\n(i): $$A,B \\in \\mathscr{P} \\land A \\ne B \\implies A \\cap B = \\emptyset$$ (ii): $$\\bigcup_{C \\in \\mathscr{P} } C = X$$ 説明 数式で表すと複雑に見えるが、簡単に言うと、単に全体集合を欠けることなくいくつかのピースに分けることに過ぎない。数式的な定義に囚われる余裕があるなら、むしろ$X$の分割$\\mathscr{P}$が$X$の冪集合$2^{X} = \\mathscr{P} (X)$の部分集合であるというようなディテールに気を使う方がいい。\n簡単な例として、整数集合$\\mathbb{Z}$を考えてみよう。偶数の集合$2 \\mathbb{Z} = \\left\\{ \\cdots , -2 , 0 , 2 , \\cdots \\right\\}$と奇数の集合$1 + 2 \\mathbb{Z} = \\left\\{ \\cdots , -3 , -1 , 1 , 3 , \\cdots \\right\\}$を含む$\\mathscr{P} = \\left\\{ 2 \\mathbb{Z} , 1 + 2 \\mathbb{Z} \\right\\}$は$\\mathbb{Z}$の分割となる。ここで$\\mathscr{P} \\subset 2^{\\mathbb{Z}}$は$\\mathbb{Z}$の部分集合を元に持つ集合であり、元の数は$2$個である。何がどこに属しているか、元なのか集合なのかを大まかに見過ごさず、正確に定義に従って理解する練習をすることがいい。\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p147.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1049,"permalink":"https://freshrimpsushi.github.io/jp/posts/1049/","tags":null,"title":"集合の分割"},{"categories":"집합론","contents":"定義 1 反射的であり、対称的であり、推移的な２項関係を同値関係と呼ぶ。\n説明 同値関係を数学的でない言葉で言うなら、「それがそれ」ということだ。\n数学を研究するときにその理由が必ずしも必要ではないが、もし数学を研究する実用的な理由が絶対に必要であるとするならば、「もともと難しく複雑な概念を簡単でシンプルな領域に引き下げて、問題を楽に解決するため」と言えるだろう。この話をするためには「実質的に同じである」と表現することができなければならないが、これがまさに同値関係だ。\n同値関係の例としては$=$, $\\equiv$, $\\iff$ などがあるが、これらの例だけでは数学で同値関係がなぜ重要なのか理解しづらい。新しい視点を持つには自然すぎるためだ。 $$ Q : = \\left\\{ ( n, m ): n\\in \\mathbb{Z} , m \\in \\mathbb{Z} \\setminus \\left\\{ 0 \\right\\} \\right\\} $$ 例えば、次のような集合 $Q$ があるとしよう。ここで、順序ペア $(n,m)$ を $\\displaystyle (n,m) = {{n} \\over {m}}$ のように書くと、$Q$ は私たちがよく知っている有理数の集合 $\\mathbb{Q}$ と実質的に同じであると言える。しかし、詳しく調べてみると、$Q$ と $\\mathbb{Q}$ を同じ集合とは言えない。なぜなら、$Q$ では $(2,3)$ と $(4,6)$ が異なる要素であるにも関わらず、$\\mathbb{Q}$ では $\\displaystyle {{2} \\over {3}}$ と $\\displaystyle {{4} \\over {6}}$ が同じ要素であるからだ。$Q$ と $\\mathbb{Q}$ の違いは、約分があるかどうかである。\n$\\mathbb{Q}$ では、私たちは馴染みのある同値関係を発見できる。この同値関係を $\\sim$ と呼ぶと、次のように定義できるだろう。 $$ {{ a } \\over { b }} \\sim {{ c } \\over { d }} \\iff ad = bc $$ 実際、$\\displaystyle {{2} \\over {3}}$ と $\\displaystyle {{4} \\over {6}}$ を考えると、$2 \\cdot 6 = 12 = 3 \\cdot 4$ であるため、$\\displaystyle {{2} \\over {3}} \\sim {{4} \\over {6}}$ となり、$\\mathbb{Q}$ で二つの要素が同値であると言える。反面、このような同値関係が $Q$ に与えられていない場合、$(2,3)$ と $(4,6)$ が実質的に同じであることを知っていても、同じと言うことができない。\nこのように、元々同じであるべきものを同じと言う作業は、一見無用であり、過度に理論的に感じられるかもしれない。私たちの直感は既に同じであることを知っているが、難しい言葉や記号を使って、言っていたことをまた言う感じだ。しかし、数学では厳密性を最高の価値としており、この過程は避けられないと同時に、それを利用して数学の限界を破ることもできる。 例えば、図のように線分を曲げて両端をくっつけたと考えてみよう。線分だったとき、二つの点は明らかに異なる点だった。しかし、同値関係を適用し、二つの点を実質的に同じ点として扱うことで、これまでとは完全に異なる、新しい輪が作られる。位相数学という分野は、このような現象を研究し、同値関係を用いて「くっつける」という行為を数学的に表現したものである。\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p141.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1033,"permalink":"https://freshrimpsushi.github.io/jp/posts/1033/","tags":null,"title":"数学における同値関係"},{"categories":"집합론","contents":"定義 1 二つの集合$X,Y$に対して、 $$ R := \\left\\{ (x,y): x \\in X , y \\in Y \\right\\} \\subset X \\times Y $$ を (二項) 関係と定義し、次のように表す。 $$ (x,y) \\in R \\iff x R y $$ $x R y \\iff y R^{-1} x$を満たす $$ R^{-1} : \\left\\{ (y,x): (a,b) \\in R \\right\\} $$ を$R$の逆関係という。 すべての$x \\in X$に対して、次を満たす$ R \\subset X^{2}$を反射的と言う。 $$ x R x $$ すべての$x,y \\in X$に対して、次を満たす$ R \\subset X^{2}$を対称的と言う。 $$ x R y \\implies y R x $$ すべての$x,y,z \\in X$に対して、次を満たす$ R \\subset X^{2}$を推移的と言う。 $$ x R y \\land y R z \\implies x R z $$ すべての$x,y \\in X$に対して、次を満たす$ R \\subset X^{2}$を反対称的と言う。 $$ x R y \\land y R x \\implies x = y $$ 説明 二項関係は、「何かと何かが何かの関係を持つ」というような曖昧な表現ではなく、デカルト積を利用して明確に定義される。関係とは正確にデカルト積の部分集合であり、$x R y$を見て「$x$は$y$に対してどうのこうのという意味ではない」と理解しなければならない。直感的に何となく分かった気になって概念をしっかり把握せずにいると、「関係」が登場するたびに本を読むのが難しくなるだろう。\n特に反射的であり、対称的であり、推移的な二項関係を同値関係という。これらの性質は数学全般で非常に重要とされる。\n例 二項関係と逆関係 関数$f : X \\to Y$は、すべての$x$に対して$y = f(x)$を満たす$y \\in Y$が存在し、すべての$x_{1} , x_{2} \\in X$に対して $$ x_{1} = x_{2} \\implies f(x_{1}) = f(x_{2}) $$ を満たす二項関係だ。もちろん、その逆関数$f^{-1}$が存在すれば、$f^{-1}$は関係$f$の逆関係になる。\n反射関係 反射的な関係の例として、等号$=$は$x=x$が常に成り立つ。\n対称関係 対称的な関係の例として、独立$\\perp$は $$ X \\perp Y \\implies Y \\perp X $$ が常に成り立つ。\n推移関係 推移的な関係の例として、不等号$\u0026lt;$は $$ x \u0026lt; y \\land y \u0026lt; z \\implies x \u0026lt; z $$ が常に成り立つ。\n反対称関係 反対称的な関係の例として、包含関係$\\subset$は $$ A \\subset B \\land B \\subset A \\implies A = B $$ が常に成り立つ。\n李興天 訳, Lin You-Feng. (2011). 集合論(Set Theory: An Intuitive Approach): p137~141。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":960,"permalink":"https://freshrimpsushi.github.io/jp/posts/960/","tags":null,"title":"数学における二項関係"},{"categories":"집합론","contents":"定義 1 任意の二つの対象 $a$、$b$ に対して、$(a,b)$ を順序対と言う。 任意の二つの集合 $A$、$B$ について、$a \\in A$、$b \\in B$ の順序対 $(a,b)$ の集合を$A$、$B$ のデカルト積と言い、以下のように表す。 $$ A \\times B := \\left\\{ (a,b): a \\in A \\land b \\in B \\right\\} $$ 説明 デカルト積で「積」という言葉が使用される理由は、集合が持つ元の数を考えた時に$| A \\times B | = | A | \\times |B|$ になるからだ。一つの集合 $X$ について、$X \\times X$ は$X^{2}$ のように表されるが、実数の集合 $\\mathbb{R}$ を考えると、$\\mathbb{R}^2 = \\mathbb{R} \\times \\mathbb{R}$ は座標平面の全ての点を元として持つ集合と見ることができる。これは数直線の全ての点を持つ$\\mathbb{R}$ を掛けることで、座標平面$\\mathbb{R}^2$ を得ることができるということだ。デカルト積で「デカルト」という表現が使われる理由は、この座標平面を考案し、数学界に導入して解析幾何学の世界を開いた人がデカルトだったからだ。デカルトがカントールよりもずっと前の人物であり、集合論に直接的な貢献はなかったが、概念的に前進していたため、この名前の主役になる資格は十分あると言えるだろう。\nこのようなデカルト積は、もちろん一般化が可能であり、例えば、三次元空間 $\\mathbb{R}^{3}$ や一般的なユークリッド空間$\\mathbb{R}^{p}, p \\in \\mathbb{N}$ も考えられる。想像するのは難しいかもしれないが、デカルト積は自然数を越えても拡張される。\n要約 一方、デカルト積に対して、以下の分配法則が成立する。\n分配法則 任意の集合 $A$、$B$、$C$ に対して： $$ A \\times (B \\cap C) = ( A \\times B) \\cap (A \\times C) \\\\ A \\times (B \\cup C) = ( A \\times B) \\cup (A \\times C) \\\\ A \\times (B \\setminus C) = ( A \\times B) \\setminus (A \\times C) $$\n参照 集合のデカルト積 群のデカルト積 位相空間のデカルト積 翻訳：イ・フンチョン、リン・ヨウフォン。(2011)。集合論(Set Theory: An Intuitive Approach)：p129~131。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1360,"permalink":"https://freshrimpsushi.github.io/jp/posts/1360/","tags":null,"title":"集合のデカルト積"},{"categories":"집합론","contents":"定義 要素が集合である集合をファミリーFamilyという。 ファミリーの要素をメンバーMemberという。 一つの集合$\\Gamma$の各$\\gamma \\in \\Gamma$に対して集合$A_{\\gamma}$が対応する時、$\\gamma$をインデックス、$\\Gamma$をインデックス集合、$\\left\\{ A_{\\gamma} : \\gamma \\in \\Gamma \\right\\}$をインデクスファミリーという。 説明 ファミリーは本来、「集合族」という言葉で表されているが、この表現は「集合の集合」という非常に不便な言葉を避けるためだけのもので、それでもなお直感的ではなく不便である。集合が要素として集合を持つという概念を表すためにわざわざ新しい言葉を作るのは奇妙だ。つまり、ファミリーという言葉は純粋に便宜のために導入されたということだ。\n例として、次のファミリーを考えてみよう: $$ \\mathcal{F}=\\left\\{\\left\\{ 1 \\right\\} , \\mathbb{R} , \\mathbb{Q}, \\emptyset , \\mathbb{R} \\right\\} $$ $\\mathcal{F}$はその要素が全て集合であるため、ファミリーと呼べる。注目すべき点は、$\\mathbb{R}$が重複して使われていることだ。このような表記からファミリーは単なる集合の集合ではないし、正確な意味での集合の集合でもないことを知るべきだ。純粋に、純粋に便宜だ。その意味で、「集合族」の族はFamilyから来た家族や族の族しか取り入れておらず、英語の表現が言いたいことを全く生かせていない。さらにメンバーは「構成員」と純化されているが、これはどう見ても便利な表現ではないので、このブログではファミリーとメンバーをそのまま使用する。\n同様に、インデックスも「添え数」と純化されているが、これは行き過ぎだ。上で例として挙げた$\\mathcal{F}$についてインデックスファミリーを構成してみよう。幸いなことに有限集合であるため、$\\Gamma = \\left\\{ 1,2,3,4,5 \\right\\}$に対して $$ A_{1} = \\left\\{ 1 \\right\\} \\\\ A_{2} = \\mathbb{R} \\\\ A_{3} = \\mathbb{Q} \\\\ A_{4} = \\emptyset \\\\ A_{5} = \\mathbb{R} $$ とすると、$\\mathcal{F} = \\left\\{ A_{\\gamma} : \\gamma \\in \\Gamma \\right\\}$を得る。ここで、$A_{2} = \\mathbb{R}$でありながら$A_{5} = \\mathbb{R}$であることに注意しよう。重複を許したのは、集合論の基盤を揺るがすためではなく、ただ表現上便利に使うためであることを覚えておこう。同じ理由で、ファミリーは単にコレクションCollectionとも呼ばれる。英語で集合を定義する際にCollectionという表現が使われているため、意味が循環しているように見えるが、前述のようにそれは単に便利に話すためのものなので、あまり深く考えずに、使用している教材の慣習に従えばいい。\n一方、インデックスは必ずしも上記のような順序を守る必要はなく、具体的に番号が付けられている必要もない。$\\Gamma = \\mathbb{R}$として、$A_{\\gamma}$が$k \\in \\mathbb{Z}$に対して$\\gamma$を含む区間$[k , k+1)$と考えてみよう。すると、$\\gamma \\in \\mathbb{R}$であるため、$A_{\\pi} = [3,4)$、$A_{\\sqrt{10}} = [3,4)$など、全ての$\\gamma \\in \\Gamma$に対して対応する$A_{\\gamma}$を見つける理由がない。こんな変な構成がどうして必要かと思うかもしれないが、位相数学だけを見ても、このような集合を当たり前のように使う。\n任意のファミリー$\\mathcal{F}$に対して、以下の表現を使用する。\n和集合: $$ \\bigcup \\mathcal{F} = \\bigcup_{A \\in \\mathcal{F}} \\left\\{ x \\in U : \\exists A \\in \\mathcal{F} , x \\in A \\right\\} $$ 積集合: $$ \\bigcap \\mathcal{F} = \\bigcap_{A \\in \\mathcal{F}} \\left\\{ x \\in U : \\forall A \\in \\mathcal{F} , x \\in A \\right\\} $$ 基本性質 $\\left\\{ A_{\\gamma} : \\gamma \\in \\Gamma \\right\\}$について、以下が成立する。\n[1] 包含原理の集合形: 全体集合$U$に対して、 $$ \\bigcup_{\\gamma \\in \\emptyset} A_{\\gamma} = \\emptyset \\\\ \\bigcap_{\\gamma \\in \\emptyset} A_{\\gamma} = U $$ [2] ド・モルガンの定理の一般化: $$ \\left( \\bigcup_{\\gamma \\in \\Gamma} A_{\\gamma} \\right)^{c} = \\bigcap_{\\gamma \\in \\Gamma} A_{\\gamma}^{c} \\\\ \\left( \\bigcap_{\\gamma \\in \\Gamma} A_{\\gamma} \\right)^{c} = \\bigcup_{\\gamma \\in \\Gamma} A_{\\gamma}^{c} $$ [3] 分配法則: 集合$B$に対して、 $$ \\left( \\bigcup_{ \\gamma \\in \\Gamma } A_{\\gamma} \\right) \\cap B = \\bigcup_{\\gamma \\in \\Gamma} \\left( A_{\\gamma} \\cap B \\right) \\\\ \\left( \\bigcap_{ \\gamma \\in \\Gamma } A_{\\gamma} \\right) \\cup B = \\bigcap_{\\gamma \\in \\Gamma} \\left( A_{\\gamma} \\cup B \\right) $$ ","id":1358,"permalink":"https://freshrimpsushi.github.io/jp/posts/1358/","tags":null,"title":"集合族と添字"},{"categories":"확률론","contents":"定義 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられたとする。\n$\\mathcal{F}$ のサブσフィールドのシーケンス $\\left\\{ \\mathcal{F}_{n} \\right\\}_{n \\in \\mathbb{N}}$ が以下を満たす場合、フィルトレーションFiltrationと呼ぶ。 $$ \\forall n \\in \\mathbb{N}, \\mathcal{F}_{n} \\subset \\mathcal{F}_{n+1} $$ フィルトレーション $\\left\\{ \\mathcal{F}_{n} \\right\\}_{n \\in \\mathbb{N}}$ が与えられた時、ルベーグ可積分な $\\mathcal{F}_{n}$-可測確率変数 $X_{n}$ のシーケンス $\\left\\{ X_{n} \\right\\}_{n \\in \\mathbb{N}}$ が形成する順序対のシーケンス $\\left\\{ (X_{n}, \\mathcal{F}_{n}) \\right\\}$ が以下を満たす場合、マルチンゲールと言う。 $$ \\forall n \\in \\mathbb{N}, E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) = X_{n} $$ $\\mathcal{F}_{n}$ が $\\mathcal{F}$ のサブσフィールドであるとは、両方とも $\\Omega$ のσフィールドであるが、$\\mathcal{F}_{n} \\subset \\mathcal{F}$ を意味する。 $X_{n}$ が $\\mathcal{F}_{n}$-可測関数であるとは、全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $X_{n}^{-1} (B) \\in \\mathcal{F}_{n}$ であることを意味する。 説明 それぞれ サブマルチンゲール、スーパーマルチンゲールとは以下のように言う。不等式は右辺が小さくなるとサブ、大きくなるとスーパーと覚えると混乱しない。 $$ \\forall n \\in \\mathbb{N}, E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) \\ge X_{n} \\\\ \\forall n \\in \\mathbb{N}, E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) \\le X_{n} $$ もちろん、サブマルチンゲールでありスーパーマルチンゲールでもある場合は、マルチンゲールと同等である。だから、サブマルチンゲールもしくはスーパーマルチンゲールに当てはまる定理があれば、マルチンゲールにもそのまま適用できる。\nマルチンゲールを直感的に理解することは、σフィールドを事件の集合、「情報」として考えることから始まる：\nフィルトレーション：$\\forall n \\in \\mathbb{N}, \\mathcal{F}_{n} \\subset \\mathcal{F}_{n+1}$、つまりσフィールドが大きいということは、それだけ多くの情報があるという意味だ。マルチンゲールの定義では、プロセス $X_{n}$ が $\\mathcal{F}_{n}$-可測であることは、実際のデータ $x_{n}$ が観測されるにつれてσフィールド $\\mathcal{F}_{n}$ も広がり$n$ 回までの全ての情報を得たと見なしてもよいということだ。 マルチンゲール：$\\forall n \\in \\mathbb{N}, E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) = X_{n}$ とは、$n$ 回までの情報 $\\mathcal{F}_{n}$ を知っている時、次の状況である $X_{n+1}$ も$X_{n}$ と似ていると仮定することを意味する。$X_{n+1}$ の期待値がこれまでに得た $\\mathcal{F}_{n}$ とは無関係に算出されるのであれば、このような確率過程はホワイトノイズと変わらないし、統計的分析の対象にはならない。だから、マルチンゲールの直感的な定義は「私たちが何か有利な情報を持っていて、数学的、統計的により良い結果を知ることができる確率過程」と言える。 起源 「マルティグ」のフランスの村では、いわゆる「二倍返し戦略」が流行していた。一回負けたら、その損失を補うためにより大きな賭けを繰り返す方式で、心理的な面はさておき、これが本当に賢い戦略かどうか考える必要がある。数学的に見ると、このような戦略の本質は $$ E \\left( X_{n+1} | X_{1} , \\cdots , X_{n} \\right) = X_{n+1} $$ の式で要約できる。「これまでずっと負けてきたから、今回は勝つだろう」と賭博師の誤謬を指摘し、なぜマルチンゲールベッティングが意味をなさないかを説明する。\n例 (1) 自己回帰過程 $AR(1)$ $X_{n+1} = X_{n} + \\varepsilon_{n}$ を考えよう。フィルトレーションが与えられている場合、$X_{n}$ に対する情報をすべて知っているので、条件付き期待値の性質により $$ \\begin{align*} E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) =\u0026amp; E \\left( X_{n} + \\varepsilon_{n} | \\mathcal{F}_{n} \\right) \\\\ =\u0026amp; E \\left( X_{n} | \\mathcal{F}_{n} \\right) + E \\left( \\varepsilon_{n} | \\mathcal{F}_{n} \\right) \\\\ =\u0026amp; X_{n} + E \\left( \\varepsilon_{n} | \\mathcal{F}_{n} \\right) \\\\ =\u0026amp; X_{n} + E ( \\varepsilon_{n} ) \\\\ =\u0026amp; X_{n} \\end{align*} $$ となるので、$\\left\\{ (X_{n}, \\mathcal{F}_{n}) \\right\\}$ はマルチンゲールとなる。\n(2) $\\left\\{ X_{n} \\right\\}_{n \\in \\mathbb{N}}$ が互いに独立であり、$E(X_{n}) = 0$ であり、$\\displaystyle S_{n}:= \\sum_{i =1}^{n} X_{i}$ とする。その場合 $$ \\begin{align*} E(S_{n+1} | \\mathcal{F}_{n} ) =\u0026amp; S_{n} + E( X_{n+1} | \\mathcal{F}_{n} ) \\\\ =\u0026amp; S_{n} + E( X_{n+1} ) \\\\ =\u0026amp; S_{n} + 0 \\end{align*} $$ となるので、$\\left\\{ (S_{n}, \\mathcal{F}_{n}) \\right\\}$ はマルチンゲールとなる。\n一方で、マルチンゲールと凸関数 $\\phi$ が与えられた場合、上記のようにサブマルチンゲールを作り出すことができる。\n定理 マルチンゲール $\\left\\{ (X_{n}, \\mathcal{F}_{n}) \\right\\}$ と凸関数 $\\phi: \\mathbb{R} \\to \\mathbb{R}$ に対して、$( \\phi (X_{n}) , \\mathcal{F}_{n} )$ はサブマルチンゲールである。\n証明 条件付きジェンセンの不等式：確率空間 $( \\Omega , \\mathcal{F} , P)$ とサブσフィールド $\\mathcal{G} \\subset \\mathcal{F}$ が与えられ、$X$が確率変数であるとする。凸関数 $\\phi: \\mathbb{R} \\to \\mathbb{R}$ と $\\phi (X) \\in \\mathcal{L}^{1} ( \\Omega ) $に対して $$ \\phi \\left( E \\left( X | \\mathcal{G} \\right) \\right) \\le E \\left( \\phi (X) | \\mathcal{G} \\right) $$\n条件付きジェンセンの不等式により $$ E \\left( \\phi (X_{n+1}) | \\mathcal{F}_{n} \\right) \\ge \\phi \\left( E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) \\right) = \\phi ( X_{n} ) $$\n■\n結論 この定理の結論として、$p \\ge 1$ を $\\phi (x) = | x |^{p}$ として設定すると、$\\left\\{ |X_{n}|^p , \\mathcal{F}_{n} \\right\\}$ は常にサブマルチンゲールであることが分かる。\n関連項目 様々なフィルトレーション $$ A_{1} \\subset A_{2} \\subset \\cdots \\subset A_{n} \\subset \\cdots $$ 一般的に数学全般で、上記のようなネステッドシーケンスNested Sequenceを形成する構造をフィルトレーションFiltrationと使用する。\n確率過程のフィルトレーション コンプレックスのフィルトレーション ベクター空間のフラグ ","id":1349,"permalink":"https://freshrimpsushi.github.io/jp/posts/1349/","tags":null,"title":"マルチンゲールの定義"},{"categories":"집합론","contents":"公理 $$ \\exists U \\left( \\emptyset \\in U \\land \\forall X ( X \\in U \\implies S(X) \\in U) \\right) $$ 空集合と$X$を要素として持ち、$S(X)$も要素として持つ集合$U$が存在する。\n集合$X$に対して、$S(X)$は$S(X):= X \\cup \\left\\{ X \\right\\}$と同じように定義される集合である。 説明 なぜこれが無限公理と呼ばれるのかを長々と説明するより、自然数集合$\\mathbb{N}$の存在性を証明することが良いだろう。\n定理：自然数集合の存在性 $\\mathbb{N}$は存在する。\n証明 戦略：フォン・ノイマンが提案した構築法を用い、自然数自体を集合と対応させて自然数の集合$\\mathbb{N}$を直接構築する。これにより、$\\mathbb{N}$は存在し、同時に自然数の性質も即座に有する。\n空集合$\\emptyset$とその$S(n)$について、次のように定義しよう。 $$ 0 : = \\emptyset \\\\ ( n + 1 ):= S(n) = n \\cup \\left\\{ n \\right\\} $$ すると $$ 1 = 0+1 = S ( 0 ) = \\left\\{ 0 \\right\\} \\\\ 2 = 1+1 = S ( 1 ) = \\left\\{ 0, \\left\\{ 0 \\right\\} \\right\\} = \\left\\{ 0, 1 \\right\\} \\\\ 3 = 2+1 = S ( 2 ) = \\left\\{ 0, \\left\\{ 0 \\right\\}, \\left\\{ 0, \\left\\{ 0 \\right\\} \\right\\} \\right\\} = \\left\\{ 0, 1, 2 \\right\\} \\\\ \\vdots $$ 無限公理により、$\\mathbb{N} = \\left\\{ 1, 2, 3, \\cdots \\right\\}$は次の性質を満たしながら存在する。 $$ n_{1} \\in n_{2} \\iff n_{1} \u0026lt; n_{2} \\\\ n_{1} \\subset n_{2} \\iff n_{1} \\le n_{2} $$\n■\n自然数が無限に多いという主張はどうあれ真実だろうが、実際にこの宇宙の誰もが無限に多い自然数を見たことはない。いくら長く、一貫して、多くの自然数を探しても、無限集合が存在することを帰納的に証明することは不可能だ。無限公理はこのような無限を説明するために導入されたものであり、直感的にこれを拒否する理由は全くないだろう。\n","id":1348,"permalink":"https://freshrimpsushi.github.io/jp/posts/1348/","tags":null,"title":"無限公理"},{"categories":"집합론","contents":"公理 $$ \\forall X \\left( \\exists U \\left( \\forall a \\left( a \\in x \\land x \\in X \\implies a \\in U \\right) \\right) \\right) $$ 任意の集合$X$に対して、$X$の全ての要素の要素を含む集合$U$が存在する。\n和集合の定義 1 和集合公理は、以下のように定義される和集合の存在を保証する。 $$ x \\in A \\lor x \\in B \\iff x \\in A \\cup B $$ 任意の二つの集合$A$、$B$に対して、少なくともどちらか一方に属する要素の集合を$A$と$B$の和集合と呼び、$A \\cup B$のように示す。\n説明 和集合公理と和集合の定義は、明確に異なる。もちろん、定義は単にある概念を言うだけであり、公理がその存在を保証するという違いもあるが、「要素の要素を含む」という表現は異なる。要素1の要素2と言えば、要素1は当然集合であり、要素1の形状は対公理を通じて存在が保証され、$\\left\\{ A, B \\right\\}$の$A$、$B$のように見える。言い換えれば、ただの和集合は、$A$と$B$の間の操作$\\cup$で作られるものと見ることができ、和集合公理が述べようとしている和集合の概念は、$X = \\left\\{ A, B \\right\\}$のような集合の集合が与えられたとき、$U(X) := \\left\\{ a \\in x : x \\in X \\right\\}$のようなものを指す。\n実際、学部レベル以下の数学を扱う際にこの区別が大きな意味を持つことはないが、好奇心からでも公理を理解したい、または珍しく必要な場合は、正確に理解して進むべきだ。\n基本的性質 集合$X$の部分集合$A$、$B$、$C$に対して、次が成り立つ。\n[1] 同一律: $$ A \\cup \\emptyset = A \\\\ A \\cap X = A $$ [2] 冪等律: $$ A \\cup A = A \\\\ A \\cap A = A $$ [3] 交換律: $$ A \\cup B = B \\cup A \\\\ A \\cap B = B \\cap A $$ [4] 結合律: $$ A \\cup ( B \\cup C) = (A \\cup B) \\cup C \\\\ A \\cap (B \\cap C) = ( A \\cap B ) \\cap C $$ [5] 分配律: $$ A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C) \\\\ A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C) $$ [6] ド・モルガンの定理: $$ (A \\cup B)^{c} = A^{c} \\cap B^{c} \\\\ (A \\cap B)^{c} = A^{c} \\cup B^{c} $$ [7] $$ (A \\setminus B)^{c} = A^{c} \\cup B $$ 証明 [7] $$ \\begin{align*} x \\in (A \\setminus B)^{c} \u0026amp;\\iff x \\notin A \\setminus B \\\\ \u0026amp;\\iff x \\notin A \\text{ or } x \\in B \\\\ \u0026amp;\\iff x \\in A^{c} \\text{ or } x \\in B \\\\ \u0026amp;\\iff x \\in A^{c} \\cup B \\end{align*} $$\n■\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p87.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1344,"permalink":"https://freshrimpsushi.github.io/jp/posts/1344/","tags":null,"title":"和集合の公理"},{"categories":"집합론","contents":"公理 1 $$ \\forall X \\exists A \\forall a \\left( a \\in A \\iff ( a \\in X \\land p(a)) \\right) $$ 任意の集合 $X$ に対して、性質 $p$ を持つ要素で構成された部分集合 $A$ が存在する。\n$p(x)$ は $X$ 内の命題関数だ。 説明 $A$ を $X$ の部分集合として限定する理由は、ラッセルの逆理のような問題が起こるのを防ぐためだ。公理ではなく公理形である理由は、この公理が無限に多くの $p(x)$ に基づいて無限に存在するからだ。異なる二つの命題関数 $p_{1}(x)$ と $p_{2}(x)$ があるとすれば、$\\left\\{ a \\in X : p_{2}(a) \\text{ is truth} \\right\\} \\subset X$ の存在を保証するのは「$p_{1}(x)$ に関する分類公理」ではなく「$p_{2}(x)$ に関する分類公理」だ。\n交わりと差集合の定義 2 分類公理形は、以下のように定義される交わりの存在を保証する。\n$$ x \\in A \\land x \\in B \\iff x \\in A \\cap B $$ 任意の二つの集合 $A$、$B$ に対して、両方に属する要素の集合を $A$ と $B$ の交わりといい、$A \\cap B$ と表示する。\nここで、集合 $A$ に対して与えられた命題関数は $p(x): x \\in B$ で、具体的に $A \\cap B= \\left\\{ x \\in A : x \\in B \\right\\}$ のように書ける。もし $A \\cap B = \\emptyset$ ならば $A$ と $B$ は互いに素Disjointという。\n勿論、分類公理形は交わりの存在だけでなく、特定の条件を満たす全ての部分集合の存在も保証する。これは集合を表現する方法の一つである条件提示法そのものと見ることもできる。\n$$ x \\in A \\land x \\notin B \\iff x \\in A \\setminus B $$ 任意の二つの集合 $A$、$B$ に対して、$A$ には属するが $B$ には属さない要素の集合を $A$ に対する $B$ の差集合といい、$A \\setminus B$ と表示する。\n集合 $U$ に対して $U \\setminus A$ を $A$ の補集合といい、$A^{c}$ と表示する。このように補集合を考えるとき、集合 $U$ を全集合とも呼ぶ。\n集合論は無限だが、数学の全ての分野が抽象的な世界全体を探求する必要はない。通常、必要に応じてある全集合を設定し、位相数学のような分野はこれらの概念を特に多く使用する。補集合と全集合に関して、以下のいくつかの性質を紹介する。\n基本性質 集合 $A$、$B$ が全集合$U$ の任意の部分集合であるとする。\n[1] $$ \\left(A^{c} \\right)^{c} = A $$ [2] $$ \\emptyset^{c} = U \\\\ U^{c} = \\emptyset $$ [3] $$ A \\cap A^{c} = \\emptyset \\\\ A \\cup A^{c} = U $$ [4] $$ A \\subset B \\implies B^{c} \\subset A^{c} $$ [5] $$ A \\setminus B = A \\cap B^{c} $$ 証明 [5] $$ \\begin{align*} x \\in A \\setminus B \u0026amp;\\iff x \\in A \\text{ and } x \\notin B \\\\ \u0026amp;\\iff x \\in A \\text{ and } x \\in B^{c} \\\\ \u0026amp;\\iff x \\in A \\cap B^{c} \\end{align*} $$\n■\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p81.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p87, 95.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1341,"permalink":"https://freshrimpsushi.github.io/jp/posts/1341/","tags":null,"title":"分類 公理形"},{"categories":"집합론","contents":"公理 1 $$ \\exists X \\forall x \\left( \\lnot \\left( x \\in X \\right) \\right) $$ 何の要素も持たない集合 $X$ が存在し、この集合 $X$ を空集合と定義する。\n説明 空集合は一般に $\\emptyset$ のように表記される。一方で空集合は、要素の数が $0$ の集合としても見ることができ、このように要素の数で定義できる集合には以下のようなものがある：\n単元素集合：要素の数がちょうど一つの集合を単元素集合という。 有限集合：集合の要素の数が $\\mathbb{N}$ に属している場合、有限集合という。 無限集合：空集合でも有限集合でもない場合、無限集合という。 ここで有限集合、無限集合の定義は少し雑だが、後で厳密に再定義される。\n注意すべき点は、単元素集合 $\\left\\{ x \\right\\}$ はやはり集合であり、$x$ は $\\left\\{ x \\right\\}$ の要素として明らかに異なるものであるということだ。さらに言えば、実際の現代数学では$x := \\left\\{ x \\right\\}$ のような定義さえ許されない。\n空集合の公理と空集合の定義を区別することは言葉通り、それら二つが異なるからである。空集合自体は空集合の公理にかかわらず定義はできる。しかし、実際に存在しているかは別問題である。空集合が存在することは直感的に理解しているが、単なる定義ではそのことを保証できない。これは解析学での完備性公理に似ている。\n空集合の存在が当たり前でない理由は集合の定義を考えればわかるかもしれない。私たちは集合を直感または思考の対象として、互いに明確に区別されるオブジェクトの集まりとしたし、集合に属するオブジェクトを要素とした。しかし、この定義によれば空集合は「区別される個体」をまったく持たないべきなのに、集めるオブジェクトが一切ないにもかかわらず集まりがあるということは明らかに奇妙である。それにもかかわらず、私たち人間は「不存在の存在」についてあまりにもよく知っているため、空集合を扱うためにこうした公理を追加することになる。\nこのような当たり前でないことを理解したり、共感したりすることとは無関係に、空集合の存在は他の公理から導出されることがある。公理は少なければ少ないほど良いため、通常は教科書で空集合の公理は省略される傾向にある。\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p75.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1337,"permalink":"https://freshrimpsushi.github.io/jp/posts/1337/","tags":null,"title":"空集合の公理"},{"categories":"집합론","contents":"定義 1 $$ A \\subset B \\iff \\forall x (x\\in A \\implies x \\in B) $$ 任意の集合 $A$、 $B$ について、$A$ の全ての要素が $B$ の要素である場合、$A$ は $B$ の部分集合Subset、 $B$ は $A$ の上位集合Supersetと言い、$A \\subset B$ として表される。\n解説 $A \\subset B$ であり $B \\not\\subset A$ である場合、$A$ を $B$ の真部分集合Proper Subsetと言い、$A \\subsetneq B$ として表される。\n細かい注意点として、$A \\subset B$ は $A$ が $B$ に含まれると言い、$a \\in A$ は $a$ が $A$ に属すると言うことである。これが同じに見えるかもしれないが、実際の言語の習慣では混乱が生じやすいし、意味が通じていれば無理に突っ込む人はほとんどいない。しかし、包含関係は集合同士で定義されたものであり、$a \\in A$ を「集合と要素の所属関係」とは言わない点は、知っておくべき違いである。\nまとめ: 包含関係の推移性Transitivity 任意の集合 $A$、 $B$、 $C$ に対して $$A \\subset B \\land B \\subset C \\implies A \\subset C$$\n証明 仮定により、 $$ A \\subset B \\iff \\forall x (x\\in A \\implies x \\in B) \\\\ B \\subset C \\iff \\forall x (x\\in B \\implies x \\in C) $$ 三段論法によると、 $$ \\forall x (x\\in A \\implies x \\in C) \\iff A \\subset C $$\n■\n金星町 訳、You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p77.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1329,"permalink":"https://freshrimpsushi.github.io/jp/posts/1329/","tags":null,"title":"集合の包含関係"},{"categories":"매트랩","contents":"方法 linspace(a,b,n): 区間$[a,b]$を$n$個の等間隔に分けた行ベクトルを返す。 要素数を入力しなければ、$1\\times 100$ベクトルを返す。間隔の数ではなく、間隔の長さが重要な時に使われる。\na: m :b : 区間$[a,b]$を等間隔$m$で分けた行ベクトルを返す。 間隔を入力しなければ、間隔は$1$に設定される。間隔の数ではなく、間隔の長さが主要な時に使われる。$b=a+n\\cdot m$を満たす自然数$n$が存在しないこともある。この場合、終点は$b$ではなく、$a+n\\cdot m$を満たす最大の数になる。\nx1=linspace(1,10,10)\rx2=linspace(1,10)\rx3=linspace(5,55,2^5)\ry1=1:3\ry2=1:1/13:2\ry3=3:1/7:7 他の言語で Juliaで ","id":1376,"permalink":"https://freshrimpsushi.github.io/jp/posts/1376/","tags":null,"title":"MATLABで等間隔の行ベクトルを生成する方法"},{"categories":"확률론","contents":"定理 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられているとする。\n[1] 測度論での定理: 可測関数 $f$, $g$ が $\\mathcal{F}$-可測であれば、$g = h (f)$ を満たすボレル関数 $h : \\mathbb{R} \\to \\mathbb{R}$ が存在する。 [2] 確率論での応用: 確率変数 $X$, $Y$ が $\\sigma (X)$-可測であれば、$E(Y | X) = h(X)$ を満たすボレル関数 $h : \\mathbb{R} \\to \\mathbb{R}$ が存在する。 [3]: $X$ が $\\mathcal{F}$-可測であれば $$E(X|\\mathcal{F}) =X \\text{ a.s.}$$ [4]: シグマ場 $\\mathcal{G} = \\left\\{ \\emptyset , \\Omega \\right\\}$ に対して $$E(X|\\mathcal{G}) = E(X) \\text{ a.s.}$$ [5]: 定数 $c$ と全てのシグマ場 $\\mathcal{G}$ に対して $$E(c|\\mathcal{F}) = c \\text{ a.s.}$$ [6]: 定数 $c$ に対して $$E(cX | \\mathcal{G}) = c E(X | \\mathcal{G}) \\text{ a.s.}$$ [7]: $$E(X+Y | \\mathcal{G}) = E(X | \\mathcal{G}) + E(Y| \\mathcal{G}) \\text{ a.s.}$$ [8]: $X \\ge 0 \\text{ a.s.}$ であれば $$E(X | \\mathcal{G}) \\ge 0 \\text{ a.s.}$$ [9]: $X \\ge Y \\text{ a.s.}$ であれば $$E(X | \\mathcal{G}) \\ge E(Y | \\mathcal{G}) \\text{ a.s.}$$ [10]: $$\\left| E( X | \\mathcal{G} ) \\right| \\le E ( | X | | \\mathcal{G} ) \\text{ a.s.}$$ [11]: 全てのシグマ場 $\\mathcal{G}$ に対して $$E \\left[ E ( X | \\mathcal{G} ) \\right] = E(X)$$ $\\sigma (X) = \\left\\{ X^{-1} (B) : B \\in \\mathcal{B}(\\mathbb{R}) \\right\\}$ は確率変数 $X$ によって生成される $\\Omega$ の最小のシグマ場を表す。これについて $E(Y|\\sigma (X)) = E(Y|X)$ のように表記できる。 $Z$ が $\\mathcal{F}$-可測関数であることは、全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $Z^{-1} (B) \\in \\mathcal{F}$ という意味である。 ボレル関数とは、全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $f^{-1} (B)$ もボレル集合である関数 $f : \\mathbb{R} \\to \\mathbb{R}$ を指す。 説明 [1],[2]: これらの二つの定理は、$X$ に関する $Y$ の条件付き期待値が $X$ に依存するある関数として表されることを示している。特に、$X$ の値が与えられた場合は、$E(Y | X = a) = h(a)$ のように表される。[2]は[1]の系として、これにより、基本的な確率論でも日常的に使用される期待値の性質がほとんど確実に保証される。 線形性 [5]～[7]: $E(aX + b | \\mathcal{G}) = aE(X | \\mathcal{G}) + b$: 期待値の線形性は、条件付きであっても保持される。 シグマ場は情報である [3] $E(X | \\mathcal{F}) = X$: 式の意味を考えると、確率変数 $X$ が $\\mathcal{F}$-可測であることは、シグマ場 $\\mathcal{F}$ が $X$ の全ての情報を持っていることを意味する。逆に考えると、そのために可測と呼ぶのである。したがって、$E(X|\\mathcal{F})$ はいかなる妨害もなく $X$ をそのまま把握できる。$\\mathcal{F}$ 上で全ての情報が与えられた $X$ は、わざわざ $E$ で計算する必要はない。次の例を考えてみよう： 6面のサイコロを振って、目ごとに1ドルもらうゲームをするとき、もらえるお金の期待値は3.5ドルである。これを計算する理由は、実際にサイコロの目が何になるかわからないからである。しかし、サイコロを振る前に私の頭の中にシグマ場 $\\mathcal{F}$ が正確に与えられるならば、サイコロの目 $X$ を正確に測定できるため、正確に何ドルもらえるかがわかる。毎回3.5ドルを支払うとしても、勝つゲームはして、負けるゲームはしなければそれでよい。この意味で、乱数ハッキングは、シグマ場（乱数表）を盗んで、本来ランダムであるべきものを決定的にする攻撃技術に相当する。これが成功すれば、銀行のセキュリティカードやOTPなど、乱数に依存する暗号システムが破られる。 一方、$\\sigma (X)$ は $X$ の全ての情報を持ちながら最小のシグマ場として定義されているので、当\n然 $E(X| \\sigma (X)) = X$ である。これは上で紹介した表記に従って、$E(X|X) = X$ のようである。\n[4] $E(X|\\mathcal{G}) = E(X)$: 式の意味を考えると、トリビアルなシグマ場 $\\mathcal{G} = \\left\\{ \\emptyset , \\Omega \\right\\}$ は $X$ に関してどのような情報も与えないため、途方に暮れて確率空間 $\\Omega$ 全体を探して $\\displaystyle \\int_{\\Omega} X d P$ を計算するしかない。 [10] $\\left| E( X | \\mathcal{G} ) \\right| \\le E ( | X | | \\mathcal{G} )$: 絶対値の性質により $$ - E ( | X | | \\mathcal{G} ) \\le E( X | \\mathcal{G} ) \\le E ( | X | | \\mathcal{G} ) $$ [11] $E \\left[ E ( X | \\mathcal{G} ) \\right] = E(X)$: 確率論の様々な証明で有用に使用される等式で、主に $E(X)$ は直接計算が難しいが、何らかの $\\mathcal{G}$ が与えられると $E(X|\\mathcal{G})$ が計算しやすくなる場合にトリックとして使用される。 証明 [1] $h : \\mathbb{R} \\to \\mathbb{R}$ を $z \\in \\mathbb{R}$ に対して $h(z) := \\left( g \\circ f^{-1} ( \\left\\{ z \\right\\} ) \\right)$ のように定義する。\n$\\left\\{ z \\right\\} \\in \\mathcal{B}(\\mathbb{R})$ の場合、$f$ は $\\mathcal{F}$-可測なので、$f^{-1}(\\left\\{ z \\right\\}) \\in \\mathcal{F}$ であり、$g$ も $\\mathcal{F}$-可測なので、$h$ はよく定義され、$g (\\omega) = ( h \\circ f ) ( \\omega )$ を満たす。\n全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $$ h^{-1}(B) = (f \\circ g^{-1})(B) = f \\left( g^{-1} (B) \\right) $$ を考えると、$g^{-1} (B) \\in \\mathcal{F}$ なので $f(g^{-1} (B) ) \\in \\mathcal{B}(\\mathbb{R})$ である。全ての $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $h^{-1}(B) \\in \\mathcal{B}(\\mathbb{R})$ なので、$h$ はボレル関数である。\n■\n[2] $E ( Y | X ) = E ( Y | \\sigma (X) )$ は条件付き期待値の定義により $\\sigma (X)$-可測な確率変数であり、$X$ も $\\sigma (X)$ の定義に従って明らかに $\\sigma (X)$-可測な確率変数である。したがって、[1]により $\\mathcal{F} = \\sigma (X)$ とし、 $$ f = X \\\\ g = E ( Y | X ) $$ とすると、$E(Y|X) = h(X)$ を満たすボレル関数 $h : \\mathbb{R} \\to \\mathbb{R}$ が存在する。\n■\n戦略 [3]～[7]: 積分形に変換して展開し、定積分が同じであることを示した後、次の定理を適用する。元々特別な名前はないが、この投稿でのみルベーグ積分の補題と命名することにする。\nルベーグ積分の性質 $$ \\forall A \\in \\mathcal{F}, \\int_{A} f dm = 0 \\iff f = 0 \\text{ a.e.} $$\n[3] 全ての $A \\in \\mathcal{F}$ に対して $\\displaystyle \\int_{A} X dP = \\int_{A} X dP$ を満たす $X$ が一意に存在するので、条件付き期待値の定義により、$X = E(X| \\mathcal{F})$ は $\\mathcal{F}$ に対する $X$ の条件付き期待値である。したがって、全ての $A \\in \\mathcal{F}$ に対して $$ \\int_{A} E(X |\\mathcal{F}) dP = \\int_{A} X dP $$ となり、ルベーグ積分の補題により $X = E(X |\\mathcal{F}) \\text{ a.s.}$\n■\n[4] 条件付き期待値の定義により、$\\displaystyle \\int_{A} E(X |\\mathcal{G}) dP = \\int_{A} X dP$ である。\nケース 1. $A = \\emptyset$\n$$ 0 = \\int_{\\emptyset} E(X |\\mathcal{G}) dP = \\int_{\\emptyset} X dP = 0 $$\nケース 2. $A = \\Omega$\n$$ \\int_{\\Omega} E(X |\\mathcal{G}) dP = \\int_{\\Omega} X dP = E(X) = E(X) P(\\Omega) = E(X) \\int_{\\Omega} 1 dP = \\int_{\\Omega} E(X) dP $$\nしたがって、どちらの場合も、ルベーグ積分の補題により $X = E(X |\\mathcal{G}) \\text{ a.s.}$\n■\n[5] $c \\in \\mathcal{G}$ であり、$E(c | \\mathcal{G}) \\in \\mathcal{G}$ なので、条件付き期待値の定義により、全ての $A \\in \\mathcal{G}$ に対して $$ \\int_{A} E(c |\\mathcal{G}) dP = \\int_{A} X dP $$ となり、したがってルベーグ積分の補題により $c = E(c | \\mathcal{G}) \\text{ a.s.}$\n■\n[6] 条件付き期待値の定義とルベーグ積分の線形性により、全ての $A \\in \\mathcal{G}$ に対して $$ \\begin{align*} \\int_{A} E( cX |\\mathcal{G}) dP =\u0026amp; \\int_{A} cX dP \\\\ =\u0026amp; c \\int_{A} X dP \\\\ =\u0026amp; c \\int_{A} E(X|\\mathcal{G}) dP \\\\ =\u0026amp; \\int_{A} c E(X|\\mathcal{G}) dP \\end{align*} $$ となり、ルベーグ積分の補題により $E( cX |\\mathcal{G}) = c E(X|\\mathcal{G}) dP \\text{ a.s.}$\n■\n[7] 条件付き期待値の定義とルベーグ積分の線形性により、全ての $A \\in \\mathcal{G}$ に対して $$ \\begin{align*} \\int_{A} E( X+Y |\\mathcal{G}) dP =\u0026amp; \\int_{A} (X+Y) dP \\\\ =\u0026amp; \\int_{A} X dP +\\int_{A} Y dP \\\\ =\u0026amp; \\int_{A} E(X|\\mathcal{G}) dP + \\int_{A} E(Y|\\mathcal{G}) dP \\\\ =\u0026amp; \\int_{A} \\left[ E(X|\\mathcal{G}) + E(Y|\\mathcal{G}) \\right] dP \\end{align*} $$ となり、ルベーグ積分の補題により $$ E( X +Y |\\mathcal{G}) = E(X|\\mathcal{G}) + E(Y|\\mathcal{G}) dP \\text{ a.s.} $$\n■\n[8] $E( X |\\mathcal{G}) \u0026lt; 0$ と仮定すると $$ \\begin{align*} \\int_{A} E( X |\\mathcal{G}) dP =\u0026amp; \\int_{A} X dP \\\\ \\ge\u0026amp; \\int_{A} 0 dP \\\\ =\u0026amp; 0 \\end{align*} $$ となるため、矛盾が生じる。したがって、$E( X |\\mathcal{G}) \\ge 0 \\text{ a.s.}$ でなければならない。\n■\n[9] $Z := X - Y \\ge 0$ とすると、[8] により $$ E(X-Y | \\mathcal{G}) \\ge 0 $$ となり、条件付き期待値の線形性により $$ E(X| \\mathcal{G}) - E(Y | \\mathcal{G}) \\ge 0 \\text{ a.s.} $$\n■\n[10] パート 1. $X \\ge 0$\n$X \\ge 0$ の場合、$|X| = X$ となるため $$ E( |X| |\\mathcal{G}) = E(X|\\mathcal{G}) $$\n[8]により $E(X|\\mathcal{G}) \\ge 0$ となるため、同様に $E(X|\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right|$ となり $$ E( |X| |\\mathcal{G}) = E(X|\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right| $$\nパート 2. $X \u0026lt; 0$\n[6]により $$ E( |X| |\\mathcal{G}) = E( -X |\\mathcal{G}) = - E(X |\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right| $$\nパート 3. $X = X^{+} - X^{-}$\n三角不等式により $$ \\left| E(X|\\mathcal{G}) \\right| \\le \\left| E( X^{+} |\\mathcal{G}) \\right| + \\left| E( X^{-} |\\mathcal{G}) \\right| $$ $X^{+} , X^{-} \\ge 0$ であるため、パート 1により $$ \\left| E(X|\\mathcal{G}) \\right| \\le E( \\left| X^{+} \\right| |\\mathcal{G}) + E( \\left| X^{-} \\right| | \\mathcal{G}) $$\n[7]と絶対値の表示 $|f| = |f^{+}| + |f^{-}|$ により $$ \\begin{align*} \\left| E(X|\\mathcal{G}) \\right| \\le \u0026amp; E( \\left| X^{+} \\right| + \\left| X^{-} \\right| | \\mathcal{G}) \\\\ =\u0026amp; E( \\left| X \\right| | \\mathcal{G}) \\text{ a.s.} \\end{align*} $$\n■\n[11] $$ \\begin{align*} E \\left[ E( X | \\mathcal{G} ) \\right] =\u0026amp; \\int_{\\Omega} E ( X | \\mathcal{G} ) d P \\\\ =\u0026amp; \\int_{\\Omega} X d P \\\\ =\u0026amp; E(X) \\end{align*} $$\n■\n","id":1322,"permalink":"https://freshrimpsushi.github.io/jp/posts/1322/","tags":null,"title":"条件付き期待値の性質"},{"categories":"집합론","contents":"定義 1 集合: 我々の直感や思考の対象として互いに明確に区別される対象の集まりを集合という。 要素: 集合に含まれる対象を要素という。 命題関数: 集合$U$の要素$x$に対して真または偽のいずれかである命題$p(x)$を$U$の命題関数という。 説明 数学で集合はほとんど母国言語に匹敵するほど重要な概念だ。自然言語よりも優れているかもしれない。なぜなら、必然的についてくる曖昧さを排除して、その定義と形式だけで論理を展開できるからだ。 通常、要素は小文字で、集合は大文字で表される。$a$が$A$に属している場合、$a \\in A$と表され**$a$は$A$の要素**と言われる。もちろん、要素と集合を必ずアルファベットの大文字と小文字で表示する必要はない。全ての自然数の集合は通常$\\mathbb{N}$と表示され、$N \\in \\mathbb{N}$のように表示しても何の問題もない。 列挙: 自然数の集合$\\mathbb{N}$は$\\left\\{ 1 , 2, 3, \\cdots \\right\\}$のように表すことができる。こうして集合の要素を直接書き出して表示する表記法を列挙法という。 条件表示法: 列挙法とは異なり、特定の条件を満たす要素のみを集めた集合として表現することもできる。例えば、$5$より大きい自然数のみを持つ集合を表現したい場合には、命題関数$p(x): x \u0026gt; 5$に対して$\\left\\{ x \\in \\mathbb{N} : p(x) \\text{ is truth} \\right\\}$のように表現される。これをさらに簡単に、命題関数を別に定義せずに$\\left\\{ x \\in \\mathbb{N} : x \u0026gt; 5 \\right\\}$のように表現する。この表記法を条件表示法という。 命題関数は命題関数そのものとして定義される点に注意する必要がある。集合論で語る関数の定義に合致しているが、命題のみで定義できるというのが重要だ。この点が不明確な場合、関数が関数として循環定義される事態になりかねず、条件表示法の自由な使用が困難になる。一方、命題関数は論理式とも呼ばれる。 李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p47, 73, 81, 85.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1316,"permalink":"https://freshrimpsushi.github.io/jp/posts/1316/","tags":null,"title":"集合と命題関数の定義"},{"categories":"매트랩","contents":"方法 $m \\times n$ データが行列で与えられているとし、これを$A$とする。行列$A$の特定の部分のみを使用したい場合は、以下の方法を使えばいい。\nB=A(a:b, c:d) 上のようなコードを実行すると、$B$は行列$A$の$a$行目から$b$行目、$c$列目から$d$列目のデータを持つ$(b-a) \\times (d-c)$行列になる。以下は例のコードと実行結果だ。\nfor k=1:9\rfor l=1:9\rA(k,l)=10*k+l;\rend\rend\rA\ra1=A(3:7,4:9)\ra2=A(2:5,1:6) :：行や列全体を引っ張り出したいときはコロンを使えばいい。$a3$は列全体を、$a4$は行全体を引っ張り出したものだ。\na3=A(3:7,:)\ra4=A(:,4:9) 特定の行や列を引っ張り出すとき、コロンを使うと便利だ。\na5=A(3,:)\ra6=A(:,9) ","id":1362,"permalink":"https://freshrimpsushi.github.io/jp/posts/1362/","tags":null,"title":"MATLABで行列の特定の行、列を選択する方法"},{"categories":"집합론","contents":"定理 1 $$ \\left[ p(1) \\land \\left( p(n) \\implies p(n+1) \\right) \\right] \\implies \\forall n \\in \\mathbb{N} : p(n) $$ 命題 $p(n) (n=1,2,3, \\cdots )$ について、$p(1)$ が真であり、$p(n)$ を仮定した時、$p(n+1)$ が成り立てば、$p(n)$ も真である。\n説明 ある式が自然数に対して成り立つ時、特に強力な証明法として、「ペアノの第5公理」とも呼ばれるか、あるいは、単に\u0026rsquo;数学的\u0026rsquo;という言葉を取り除いて、ただの帰納法とも言われる。もともと帰納法は、現象や実体を経験的に集めて何かの結論を出すことだが、数学では、このような解説がなくとも、ただの帰納法と言われても、自然に数学的帰納法と理解される。\n数学的帰納法は、理解する前は難しく、理解するととても簡単な証明法である。ペアノの第5公理という別名からもわかるように、「公理」と呼べるほど自明な論理だからである。 数学的帰納法 数学的帰納法は、よくドミノに例えられる：\n(1) ドミノの最初のブロックが倒れる。 通常、最初のブロックが倒れるのは、自明な事実である。なぜなら、自分で倒せばいいからである。同じように、証明をする時も、普通は命題に$1$ を代入することで、簡単に示すことができる。 (2) 一つ前のブロックが倒れることで、次のブロックを倒す。 $n$ 番目のブロックが倒れるとするならば、このブロックが倒れることで、$(n+1)$ 番目のブロックも倒す。ドミノでは、ブロックを一列に並べて、前のブロックが次のブロックを倒すから、事実であろう。これを示すのが、数学的帰納法の本質であり、最も難しい部分である。 **(3) 上記の通りならば、最初のブロックを倒すと、すべてのブロックが倒れる。 最初のブロックを倒したとしよう。 (2)により、$1$ 番目のブロックが、$2$ 番目のブロックを倒す。 更に(2)により、$2$ 番目のブロックが倒れることで、$3$ 番目のブロックを倒す。 更に(2)により、$n$ 番目のブロックが倒れることで、$(n+1)$ 番目のブロックを倒す。 このように、すべてのブロックが次のブロックを倒すので、ブロックの数がどれだけ多くても、すべて倒れる。 数学的帰納法に戻ると、$p(1)$ が成り立つ時、$p(1+1)$、すなわち$p(2)$ が成り立つ。$p(2)$ が成り立つ時、$p(2+1)$ が成り立ち、$p(n)$ が成り立つ時、$p(n+1)$ が成り立つ。\nでも、$p(n)$ が成り立つことを仮定するのは、循環論法じゃないの？ 最初に数学的帰納法に出会った時に、なぜ$p(n)$ が成り立つことを仮定してもいいのか、わからなくなることがあり、私も高校時代にこの部分が紛らわしかった。しかし、実際に示すのは、$p(n)$ が自体が成り立つことではなく、$p(n)$ が成り立つ時に、$p(n+1)$ も成り立つことを示すことである。だから、数学的帰納法は間接証明法の一つとされる。\n率直に言って、全ての$n$ において$p(n)$ が成り立たなくても、どうでもいい。偽の仮定から偽の結論が導かれても、その間の論理に問題がなければいい。 もし、それが成り立つなら、$p(1)$ も成り立つので、すべての自然数に対して成り立つということになる。これが、証明の真の結論である。少なくとも一つ真の命題 $p(1)$ が存在して、すべての自然数に対する$p(n)$ を正当化できる。 李興天 訳、林有豊 (2011). 集合論(Set Theory: An Intuitive Approach): p63, 367.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":118,"permalink":"https://freshrimpsushi.github.io/jp/posts/118/","tags":null,"title":"数学的帰納法"},{"categories":"매트랩","contents":"属性 グラフの属性は以下のように指定できる。\nグラフ色 マーカー 線の形状 赤 r 点 . 実線 - 緑 g 星 * 点線 : 青 b x x 一点鎖線 -. 黒 k 円 o（アルファベットのo） 破線 -- 黄 y プラス + 紫 m 四角 s 白 w ダイア d シアン c 星 p 下向き三角 v 上向き三角 ^ 左向き三角 \u0026lt; 右向き三角 \u0026gt; 六角星 h 例 x=linspace(0,1,20);\ry=x.^3+3.*x.^2+3.*x+1;\rfigure()\rplot(x,y,\u0026#39;ro\u0026#39;)\rhold on\rplot(x,y+1,\u0026#39;g-\u0026#39;)\rplot(x,y+2,\u0026#39;c:\u0026#39;)\rplot(x,y+3,\u0026#39;k--\u0026#39;)\rlegend({\u0026#39;ro\u0026#39;, \u0026#39;g-\u0026#39;, \u0026#39;c:\u0026#39;, \u0026#39;k--\u0026#39;}) ","id":1330,"permalink":"https://freshrimpsushi.github.io/jp/posts/1330/","tags":null,"title":"MATLABグラフでの色、線の種類、マーカーの種類の指定方法"},{"categories":"매트랩","contents":"方法 imrotate(I,angle,method,bbox)\nI: 回転する画像だ。 angle: 回転角度で、単位は度だ。 method: 補間方法だ。\u0026rsquo;nearest\u0026rsquo;, \u0026lsquo;bilinear\u0026rsquo;, \u0026lsquo;bicubic\u0026rsquo;がある。何も入力しない場合は\u0026rsquo;nearet\u0026rsquo;が適用される。 X = phantom(\u0026#39;Modified Shepp-Logan\u0026#39;,64);\rfigure()\rimagesc(X)\rtitle(\u0026#39;X\u0026#39;)\rY1=imrotate(X,30,\u0026#39;nearest\u0026#39;,\u0026#39;crop\u0026#39;);\rY2=imrotate(X,30,\u0026#39;bilinear\u0026#39;,\u0026#39;crop\u0026#39;);\rY3=imrotate(X,30,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rfigure()\rsubplot(1,3,1)\rimagesc(Y1)\rtitle(\u0026#39;Y1 - nearest\u0026#39;)\rsubplot(1,3,2)\rimagesc(Y2)\rtitle(\u0026#39;Y2 - bilinear\u0026#39;)\rsubplot(1,3,3)\rimagesc(Y3)\rtitle(\u0026#39;Y3 - bicubic\u0026#39;) bbox: 出力画像のサイズを指定する。\u0026rsquo;loose\u0026rsquo;は、回転した画像で元のサイズを超える部分まで出力されるように出力画像のサイズを大きくする。\u0026lsquo;crop\u0026rsquo;は、最初の画像のサイズに合わせて回転した画像を切り取って出力する。何も入力しない場合は\u0026rsquo;loose\u0026rsquo;が適用される。 X = phantom(\u0026#39;Modified Shepp-Logan\u0026#39;,64);\rfigure()\rimagesc(X)\rtitle(\u0026#39;X - 64*64\u0026#39;)\rY1=imrotate(X,30,\u0026#39;nearest\u0026#39;,\u0026#39;loose\u0026#39;);\rY2=imrotate(X,30,\u0026#39;nearest\u0026#39;,\u0026#39;crop\u0026#39;);\rfigure()\rsubplot(1,2,1)\rimagesc(Y1)\rtitle(\u0026#39;Y1 - loose 88*88\u0026#39;)\rsubplot(1,2,2)\rimagesc(Y2)\rtitle(\u0026#39;Y2 - crop 64*64\u0026#39;) 他の言語で ジュリアで ","id":1328,"permalink":"https://freshrimpsushi.github.io/jp/posts/1328/","tags":null,"title":"MATLABで画像を回転する方法"},{"categories":"매트랩","contents":"ゼロ行列 zeros(): ゼロ行列を返す。 zeros(n): $n\\times n$ ゼロ行列を返す。 zeros(m,n): $n\\times m$ ゼロ行列を返す。 zeros(size(A)): 行列Aと同じ大きさのゼロ行列を返す。 全要素が1の行列 ones(): 全要素が1の行列を返す。ただし、二つの行列の間の演算のためには、そのまま1を使う方が便利だ。誰が見ても下のコードの方がずっとシンプルだ。 ones(n): 全要素が1の$n\\times n$ 行列を返す。 ones(m,n): 全要素が1の$n\\times m$ 行列を返す。 ones(size(A)): 行列Aと同じ大きさの全要素が1の行列を返す。 A=[1 2 3; 4 -2 3; 5 3 7]\rones(size(A))./A\r1./A 単位行列 eye(): 単位行列を返す。 eye(n): $n\\times n$ 単位行列を返す。 eye(m,n): $n\\times m$ 単位行列を返す。 eye([m,n]): 主対角線の成分が1で、その他の成分は0の$n\\times m$ 単位行列を返す。 eye(n,'like',A): 行列Aと同じデータタイプの$n\\times n$ 単位行列を返す。つまり、Aが複素行列であれば、複素単位行列を返す。サイズを指定しなければ、Aと同じ大きさの行列を返す。 eye([2,3])\reye(3,6)\rA=[1+i 3-i]\reye(3, \u0026#39;like\u0026#39;, A)\reye(3,4 \u0026#39;like\u0026#39;, A) 乱数 rand(): 0から1の間でランダムに1つの乱数を返す。各数が引かれる確率はすべて同じだ。マトラブ公式ホームページで「一様に分布した乱数」という説明は、これを意味する。 rand(n): 0から1の間の乱数で構成された$n\\times n$ 行列を返す。 rand(m,n): 0から1の間の乱数で構成された$m\\times n$ 行列を返す。 rand(n,'like',A): 行列Aと同じデータタイプの乱数で構成された$n\\times n$ 行列を返す。つまり、Aが複素行列であれば、複素行列を返す。サイズを指定しなければ、Aと同じ大きさの行列を返す。 ","id":1327,"permalink":"https://freshrimpsushi.github.io/jp/posts/1327/","tags":null,"title":"MATLABで特別な行列を作成する関数"},{"categories":"매트랩","contents":"掛け算 times(), .*: 二つの行列の各成分を掛け合わせ、その結果を返します。 二つの行列の大きさが完全に同じ、または一方がスカラー、もしくは行の大きさが同じ行ベクトル、列の大きさが同じ列ベクトルである場合のみ、演算が可能です。大きさが異なる場合、小さい行列が大きい行列と同じ大きさの行列であるかのように計算され、空いている場所は同じ値で埋められます。例えば、スカラーは全ての成分が同じ値を持つようになり、行ベクトルの場合、全ての行が同じ行列になります。よくわからない場合は、下の式を参照してください。.*は点と掛け算記号が合わさっているので、要素ごとの掛け算と理解すれば良いです。他の成分ごとの演算の記号もこのように作られています。\n$$ A=\\begin{pmatrix} a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\end{pmatrix},\\quad B=\\begin{pmatrix} b_{1} \\\\ b_2 \\\\ b_{3} \\\\ b_{4} \\end{pmatrix} \\quad \\implies \\quad \\begin{align*} A.B\u0026amp;=\\begin{pmatrix} a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\\\ a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\\\ a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\\\ a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\end{pmatrix} \\begin{pmatrix} b_{1} \u0026amp; b_{1} \u0026amp; b_{1} \\\\ b_2 \u0026amp; b_2 \u0026amp; b_2 \\\\ b_{3} \u0026amp; b_{3} \u0026amp; b_{3} \\\\ b_{4} \u0026amp; b_{4} \u0026amp; b_{4} \\end{pmatrix} \\\\ \u0026amp;= \\begin{pmatrix} a_{1}b_{1} \u0026amp; a_2b_{1} \u0026amp; a_{3}b_{1} \\\\ a_{1}b_2 \u0026amp; a_2b_2 \u0026amp; a_{3} b_2 \\\\ a_{1}b_{3} \u0026amp; a_2 b_{3} \u0026amp; a_{3} b_{3} \\\\ a_{1}b_{4} \u0026amp; a_2 b_{4} \u0026amp; a_{3} b_{4} \\end{pmatrix} \\end{align} $$\n例のコードと出力結果は以下の通りです。\nA=[2 1 -3; 4 0 3]\rB=[1 2 3]\rC=[3; 1]\ra=A.*B\rb=A.*C\rc=B.*C\rd=3.*A 割り算 rdivide(), ./: 二つの行列の各成分を割り、その結果を返します。 行列の大きさについての注意事項は.*と同じです。これを使うと、行列Aの各成分の逆数を成分とする行列を簡単に計算できます。1./Aで求めることができます。\n例のコードと出力結果は以下の通りです。\nA=[2 1 -3; 4 0 3]\rB=[1 2 3]\rC=[3; 1]\ra=rdivide(A,B)\rb=A./C\rc=B./C\rd=1./A 累乗 power(), .^: A.^Bの場合、Aの各成分を底とし、Bの各成分を指数として計算した結果を返します。 例のコードと出力結果は以下の通りです。\nA=[2 1 -3; 4 0 3]\rB=[1 2 3]\rC=[3; 1]\ra=power(A,B)\rb=A.^C\rc=B.^C\rd=3.^A ","id":1326,"permalink":"https://freshrimpsushi.github.io/jp/posts/1326/","tags":null,"title":"MATLABで二つの行列に対して要素ごとの演算を行う方法"},{"categories":"집합론","contents":"定義 1 真か偽かのどちらか一方である述語を命題という。命題は真か偽のどちらか一つの真理値Truth Valueを持つ。二つの命題 $p$、$q$ の真理値が同じならば、$p$ と $q$ は (論理的に) 同値Logically) Equivalentであり、$p \\equiv q$ のように示される。複合命題を構築する方法として、以下の記号を接続詞Connectivesという：\n否定：$\\lnot$ 論理積：$\\land$ 論理和：$\\lor$ 条件付け：$\\to$ 両条件付け：$\\leftrightarrow$ 真理表 通常、真は$T$ で、偽は$F$ で表される。上記の接続詞は定義に従った論理値を持つ。接続詞を用いて得られた命題の真理値は、以下のような真理表を使って確認すると便利である：\n否定 $p$ が真ならば$\\lnot p$ は偽で、$p$ が偽ならば$\\lnot p$ は真である。\n$\\text{NOT}$ ゲート 論理積 $p$ と $q$ が両方とも真ならば$p \\land q$ も真で、それ以外は偽である。一般的に、コンピュータ科学などの分野では$0$ を偽、$0$ 以外の数を真と見なす。$0$ 以外の二つの数$a$、$b$ を考えると、$a \\times b = ab \\ne 0$ は真だが、どちらかが$0$ ならば$a \\times b = 0$ になるため偽になる。この意味で$\\land$ は論理\u0026rsquo;積\u0026rsquo;と呼ばれる。\n$\\text{AND}$ ゲート 論理和 $p$ と $q$ のどちらか一方でも真ならば$p \\lor q$ も真で、両方とも偽の場合のみ偽である。論理積と同様に、$a + b = 0$ ならば偽であり、それ以外は真であるため、$\\lor$ は論理\u0026rsquo;和\u0026rsquo;と呼ばれる。もちろん、$b = -a \\ne 0$ ならば$a$、$b$ は両方とも真だが、$a+b = 0$ が偽になるが、ここで詳しくは語らない。だから、単なる\u0026rsquo;和\u0026rsquo;ではなく\u0026rsquo;論理和\u0026rsquo;なのだ。\n$\\text{OR}$ ゲート 条件付け $p$ が真で$q$ も真ならば$p \\to q$ も真である。実際の言語と異なり、$p$ が偽ならば$q$ が何であれ$p \\to q$ も真になることに注意。一方で、$p \\to q \\equiv \\lnot p \\lor q$ であり、これは真理表を通して簡単に証明できる。本文の下部を参照せよ。 両条件付け $p \\to q$ と $q \\to p$ が両方とも真ならば$p \\leftrightarrow q$ も真である。式で表すと$(p \\to q) \\land (q \\to p) \\equiv p \\leftrightarrow q$ になる。真理表で見る場合、$p$ と $q$ が同時に真または同時に偽、つまり同じ真理値を持つとき、$p \\leftrightarrow q$ は真になる。 条件付けの証明 否定と論理和の定義に基づき、\n■\n李興天 訳、You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p3~21.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1299,"permalink":"https://freshrimpsushi.github.io/jp/posts/1299/","tags":null,"title":"命題と論理演算子、真理値表"},{"categories":"매트랩","contents":"関数 size(): 行列の行と列の長さを要素として持つ行ベクトルを返す。\n扱っている行列と同じサイズの零行列を作る時に便利だ。\nzeros(size(A)): Aと同じサイズの零行列を返す。\nlength(): 行と列の中でより大きい数字を返す。\n行ベクトル、列ベクトルの場合には要素の数と同じなのでnumel()と同じだ。また、size()は行と列のサイズを返すので length(A)=max(size(A))となる。\nnumel(): 行列の要素の数を返す。\n例のコードと出力結果は以下の通り。\nA=[1 2 3];\rB=[1 2 3 4 ; 2 3 4 1 ; 3 4 1 2];\rC=zeros(3,8);\ra=size(A)\rb=size(B)\rc=size(C)\rd=length(A)\re=length(B)\rf=length(C)\rg=numel(A)\rh=numel(B)\ri=numel(C) ","id":1323,"permalink":"https://freshrimpsushi.github.io/jp/posts/1323/","tags":null,"title":"MATLABにおける行列のサイズと関連する関数"},{"categories":"측도론","contents":"定理1 (a) $\\nu$を可測空間 $(X, \\mathcal{E})$上で定義された符号測度とする。すると、以下を満たす $\\nu$の正集合 $P$と負集合 $N$が存在する。\n$$ P \\cup N=X \\quad \\text{and} \\quad P \\cap N =\\varnothing $$\nこのような $X=P \\cup N$を $\\nu$に対するハーン分解Hahn decompositionという。\n(b) $P^{\\prime}, N^{\\prime}$が (a) を満たす別の集合であるとする。その場合、以下の集合は $\\nu$に対する零集合である。\n$$ (P-P^{\\prime}) \\cup (P^{\\prime}-P)=(N-N^{\\prime}) \\cup (N^{\\prime}-N) $$\n対称差symmetric difference記号を使用して以下のように表記される。\n$$ P\\Delta P^{\\prime}=N\\Delta N^{\\prime} $$\n説明 (a) 任意の可測空間が与えられた時、集合 $X$を $\\nu$に対して正の集合と負の集合に分けることができるということである。\n(b) 上述のように集合 $X$を分ける方法が複数存在しても、実質的な違いはないということである。$P$と$P^{\\prime}$、$N$と$N^{\\prime}$は常に互いに零集合だけの差があるため、集合の観点では異なるかもしれないが、測度の観点では同じである。\n証明 この定理の証明自体はそれほど難しくないが、証明の流れが単純ではないため、これを事前に具体的に説明し、始める。まず、ある正の集合 $P$を定義する。そして $N$を $N:=X-P$と定義する。この時、$N$が負の集合であれば、(a) に対する証明が完了する。$N$が負の集合であることを証明する前に、上述のように定義された $N$が持つ2つの性質を確認することにする。そして、最終的な証明では背理法を使用する。$N$が負の集合でないと仮定し、2つの性質を使用して矛盾が生じることを示す。\n一般性を失わずに、$\\nu$が$+\\infty$の値を持たないと仮定する。他の場合は $-\\nu$に対して同じ方法で証明すればよい。$C$を $\\mathcal{E}$のすべてのポジティブセットのコレクションとする。すると、仮定により $\\nu$は $+\\infty$の値を持たないため、以下のように定義される $M$が存在する。\n$$ M:=\\sup \\limits_{P \\in C } \\nu (P) \u0026lt; \\infty $$\nここで、$\\nu (P)=M$を満たすマキシマイザー $P$の存在を示すことができる。以下のようなマキシマイジングシーケンス $\\left\\{ P_{j} \\right\\}$を考える。\n$$ \\lim \\limits_{j \\rightarrow \\infty} \\nu (P_{j})=M $$\nこの時、$P_{j}$同士には含まれる関係がないため、以下のような $\\tilde{P_{j}}$を考える。\n$$ \\tilde{P_{j}} :=\\bigcup \\limits_{k=1}^j P_{k} $$\nすると、$\\nu (P_{j}) \\le \\nu (\\tilde{P_{j}}) \\le M$であるため、$\\left\\{ \\tilde{P_{j}} \\right\\}$はマキシマイジングシーケンスである。また、$\\tilde{P_{1}} \\subset \\tilde{P_2}\\subset \\cdots $であることは定義によって明らかである。ここで、$P$を以下のように定義する。\n$$ P := \\bigcup \\limits_{j=1}^\\infty \\tilde{P_{j}} $$\nすると、次が成り立つ。\n$$ \\nu (P)=\\lim \\limits_{j\\rightarrow \\infty} \\nu (\\tilde{P_{j}})=M $$\nしたがって、$\\nu (P)=M$を満たすマキシマイザーが存在することを示した。また、$P$は正の集合の可算和であるため、正の集合である。実際にこのように作り出された $P$と $N:=X-P$は、定理で述べられているような一つの分解である。$N$がそのような負の集合であることを示すプロセスが残されている。ここで、$N:=X \\setminus P$とする。上述のように、$N$が負の集合であることを示せば証明が完了する。まず、このような $N$が以下の2つの性質を持つことを証明する。\n主張 1 $N$は測度値が0より大きい正の集合を含まない。つまり、0ではない正の集合を含まない。すなわち $\\nu (E)\u0026gt;0$であり、$E$が正の集合であれば、$E \\not \\subset N$である。\nこの時、注意すべき点は、正の集合でも、負の集合でもない $E \\subset N$が存在する可能性があることである。つまり、$N$の部分集合になり得るのは、1. 空集合、2. 負の集合、3. 正の集合でも負の集合でもない集合である。\n証明\n$E\\subset N$が正の集合で $\\nu (E) \u0026gt;0$であるとする。すると、$N$の定義により、$E$と $P$は互いに素な集合である。したがって、次が成り立つ。\n$$ \\nu (P \\cup E)=\\nu (P)+\\nu (E) $$\nしかし、$\\nu (P)=M$であるため、次が成り立つ。\n$$ \\nu (P \\cup E)=\\nu (P)+\\nu (E)\u0026gt;M $$\nしかし、これは $M=\\sup \\nu (F)\\ \\forall F\\in \\mathcal{E}$という仮定に矛盾する。したがって、$\\nu (E)\u0026gt;0$である正の集合 $E \\subset N$は存在しない。\n主張 2 もし $A \\subset N$で $\\nu (A)\u0026gt;0$であれば、$\\nu (B) \u0026gt; \\nu (A)$を満たす $B \\subset A$が存在する。\n証明\n$A \\subset N$で $\\nu (A)\u0026gt;0$であるとする。すると、主張 1 により、$A$は正の集合ではない。したがって、$A$は空集合でもなく、正の集合でもない。従って、次を満たす $C$が存在する2。\n$$ C \\subset A,\\ \\nu (C) \u0026lt;0 $$\nここで、$B:=A-C$とする。すると、次が成り立つ。\n$$ \\nu (A)=\\nu (B)+\\nu (C) \u0026lt; \\nu (B) $$\nここで、$N$が負の集合でないと仮定する。 上の2つの性質を利用して矛盾が生じることを示せば、$N$が負の集合であることが証明される。\nパート 1.\n$\\left\\{ A_{j} \\right\\}$を $N$の部分集合の列とし、$\\left\\{ n_{j} \\right\\}$を自然数の列とする。$N$が負の集合でないと仮定したので、$\\nu (B) \u0026gt;0$となるある $B \\subset N$が存在する。そして、$\\nu (B) \u0026gt; \\frac{1}{n_{j}}$を満たす最小の $n_{j}$を $n_{1}$とし、$n_{1}$に対してこれを満たす $B$を $A_{1}$とする。$\\nu (B)=\\nu (A_{1})\u0026gt;0$であるため、上で $N$に対して行ったプロセスを $A_{1}$に対して同じように適用することができる。\nパート 2\n再び $\\nu (B)\u0026gt;0$となるある $B\\subset A_{1}$が存在し、主張 2 により $\\nu (B) \u0026gt; \\nu (A_{1})$である。したがって、$\\nu (B) \u0026gt; \\nu (A_{1})+\\frac{1}{n}$を満たす自然数 $n$が存在する。この中で最も小さい自然数を $n_2$とし、そのような $B$を $A_2$とする。\nパート 3\n同じプロセスを繰り返すと、$n_{j}$は $\\nu (B)\u0026gt;0$となるある $B \\subset A_{j-1}$に対して $\\nu (B)\u0026gt;\\nu (A_{j-1}) + \\dfrac{1}{n_{j}}$を満たす最も小さい自然数である。また、そのような $B$を $A_{j}$とする。ここで、$A=\\bigcap \\nolimits_{1}^\\infty A_{j}$とする。$\\nu$が $+\\infty$の値を持たないと仮定した上で、符号測度の性質 $(B)$により、次が成り立つ。\n$$ \\begin{align*} +\\infty \\gt \\nu (A) \u0026amp;= \\nu \\left(\\bigcap \\nolimits_{1}^\\infty A_{j} \\right) \\\\ \u0026amp;= \\lim \\limits_{j \\rightarrow \\infty} \\nu (A_{j}) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{j-1}) +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{j-2}) + \\frac{1}{n_{j-1}} +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\vdots \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{1}) + \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\frac{1}{n_{1}}+ \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;= \\sum \\limits_{j=1}^\\infty \\frac{1}{n_{j}} \\end{align*} $$\n数列が有限であるため、極限は0である。\n$$ \\lim \\limits_{j\\rightarrow \\infty} \\frac{1}{n_{j}} =0 $$\nしたがって、次を得る。\n$$ \\begin{equation} \\lim \\limits_{j\\rightarrow \\infty} n_{j} =\\infty \\label{eq1} \\end{equation} $$\nしかし、パート 1 で見たように、主張 2 により、ある自然数 $n$に対して $\\nu (B) \u0026gt; \\nu (A) +\\dfrac{1}{n}$を満たす $B \\subset A$が存在する。すると、$A$の定義により $A \\subset A_{j-1}$であり、主張 2 により $\\left\\{ \\nu (A_{j}) \\right\\}$は増加列であることが分かる。したがって、$\\nu (A) =\\lim \\limits_{j \\rightarrow \\infty} \\nu (A_{j})$であるため、$\\nu (A) \u0026gt; \\nu (A_{j-1})$である。\nまた、$(1)$により、十分に大きな $j$に対して $n_{j} \u0026gt;n$である。したがって、次が成り立つ。\n$$ \\nu (B) \u0026gt; \\nu (A) +\\frac{1}{n}\u0026gt;\\nu (A_{j-1}) +\\frac{1}{n} \u0026gt; \\nu (A_{j-1}) +\\frac{1}{n_{j}} $$\nしかし、これは $n_{j}$と $A_{j}$の定義に対する矛盾である。したがって、$N$が負の集合でないという仮定は誤りである。すなわち、$N$は負の集合である。\n$P^{\\prime}$, $N^{\\prime}$を上記の定理を満たす別の一つの分解とする。すると、次が成り立つ。\n$$ P^{\\prime} \\cup N^{\\prime} =X \\quad \\text{and} \\quad P^{\\prime}\\cap N^{\\prime} =\\varnothing $$\nしたがって、$P-P^{\\prime} \\subset P$、$P-P^{\\prime}\\subset N^{\\prime}$であることが分かる。すると、$P-P^{\\prime}$は正の集合でありながら負の集合であるが、これを満たすのは零集合だけであるため、$P-P^{\\prime}$は$\\nu-\\mathrm{null}$である。同様に、$P^{\\prime}-P$、$N-N^{\\prime}$、$N^{\\prime}-N$に対しても同じ方法で $\\nu -\\mathrm{null}$であることを示すことができる。\n■\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (第2版, 1999), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n存在しなければ、定義により Aは空集合か、あるいは正の集合であるべきである。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1308,"permalink":"https://freshrimpsushi.github.io/jp/posts/1308/","tags":null,"title":"ハーン分解定理"},{"categories":"측도론","contents":"定義1 $\\nu$を$(X,\\mathcal{E})$上の符号測度としよう。そして$E,F \\in \\mathcal{E}$としよう。すると\n$\\nu (F) \\ge 0,\\ \\forall F\\subset E$の時、$E$を$\\nu$に対する正集合positive setまたは単にポジティブpositiveという。\n$\\nu (F) \\le 0,\\ \\forall F\\subset E$の時、$E$を$\\nu$に対する負集合negative setまたは単にネガティブnegativeという。\n$\\nu (F)=0,\\ \\forall F\\subset E$の時、$E$を$\\nu$に対する零集合null setまたは**$\\nu$-ヌル**$\\nu$-nullという。\n説明 定義によると、零集合は同時に正集合であり、負集合である集合だ。正集合、負集合の定義を誤解しやすいので、正しく理解することが重要だ。$\\mu (E)\u0026gt;0$の時、$E$を正の集合と呼ぶわけではない。$E$の全ての可測な部分集合$F\\in\\mathcal{E}$に対して$\\mu (F) \\ge 0$が成り立つ必要があって初めて、$E$を正の集合と呼ぶ。もちろん、この条件を満たすならば自然と$\\nu (E) \u0026gt;0$が成り立つ。要約すると以下の通り。\n$$ E\\ \\mathrm{is\\ positive\\ set\\ for\\ }\\nu \\implies \\nu (E)\u0026gt;0 \\\\ \\nu (E)\u0026gt;0 \\not\\implies E\\ \\mathrm{is\\ positive\\ set\\ for\\ }\\nu $$\nこれは負集合、零集合に対しても同様だ。上述の話は符号測度にのみ適用される。絶対測度については少し話が異なる。$\\mu$を絶対測度とした時、常に$0$以上の関数値を持つので、$\\mu (E)=0$が$E$が$\\nu$-ヌルであることと同値だ。正集合に関する話も同様だ。したがって、絶対測度に対しては、正集合、零集合という言葉を特に使う必要はない。下の図を見よう。\n関数$f$を区間$E_2$でリーマン積分すると、その値は確かに正だが、$E_2$を正の集合とは呼ばない。上の図の例で、関数値が0より小さい部分が一点もない区間が正の集合だ。上の図で、$E_{1}$、$E_{3}$が正の集合で、$E_{5}$が負の集合だ。$E_2$、$E_{4}$は正の集合でも、負の集合でも、零集合でもない。最も重要な点は、ある$E \\in \\mathcal{E}$が必ずしも正集合であるか、または負集合である必要がないことだ。\n要約 (a) 正の集合の可測部分集合も正の集合である。\n(b) 任意の正の集合の可算和も正の集合である。\n証明 (a) 正の集合の定義により自明だ。\n(b) $P_{1},\\ P_2,\\ \\cdots$を正の集合としよう。そして$Q_{n}$を以下のように定義しよう。\n$$ Q_{1}=P_{1},\\quad Q_{n}=P_{n}-\\left( \\bigcup \\nolimits_{j=1}^{n-1}P_{j} \\right)\\ \\forall\\ n\u0026gt;1 $$\nすると$Q_{n} \\subset P_{n}$であり、それぞれの$Q_{n}$は互いに素である。したがって$Q_{n}$は**(a)**により正の集合だ。また、以下の式が成立する。\n$$ \\bigcup \\nolimits_{1}^{\\infty} P_{j}=\\bigcup \\nolimits_{1}^{\\infty} Q_{j} $$\n今、$E$を$\\bigcup \\nolimits _{1}^\\infty P_{n}$の任意の可測な部分集合としよう。\n$$ E \\in \\left( \\bigcup \\nolimits _{1}^\\infty Q_{n} \\right)=\\left( \\bigcup \\nolimits _{1}^\\infty P_{n} \\right) $$\nそれでは、$\\nu (E) \\ge 0$を示せば証明は完了する。$E$の定義により、以下の式が成立することがわかる。 $$ E= \\bigcup \\limits_{j=1}^\\infty \\left( Q_{j} \\cap E \\right) $$ それぞれの$Q_{n}$が互いに素であるので、符号測度の可算加法性により、以下が成立する。\n$$ \\nu (E) = \\nu \\left(\\bigcup \\nolimits_{j=1}^\\infty \\left( Q_{j} \\cap E \\right) \\right) =\\sum \\limits_{j=1}^\\infty \\nu \\left( Q_{j} \\cap E \\right) $$\nこの時、$Q_{n}$が正の集合で、$(Q_{j}\\cap E ) \\subset Q_{j}$であるため、式の右辺は必ず$0$以上である。\n$$ \\nu (E) =\\sum \\limits_{j=1}^\\infty \\nu \\left( Q_{j} \\cap E \\right) \\ge 0 $$\n■\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p86\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1303,"permalink":"https://freshrimpsushi.github.io/jp/posts/1303/","tags":null,"title":"正の集合, 負の集合, 零の集合"},{"categories":"확률론","contents":"定義 1 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられたとしよう。\nすべてのボレル集合 $B \\in \\mathcal{B} (\\mathbb{R})$ に対して $X^{-1} (B) \\in \\mathcal{F}$ を満たす関数 $X : \\Omega \\to \\mathbb{R}$ を 確率変数Random Variableと呼ぶ。 以下のように定義された$\\mathcal{F}_{X}$ を $X$ によって生成されたシグマ場と呼ぶ。 $$ \\mathcal{F}_{X} := X^{-1} ( \\mathcal{B} ) = \\sigma (X) = \\left\\{ X^{-1} (B) \\in \\Omega : B \\in \\mathcal{B}( \\Omega ) \\right\\} $$ 以下のように定義された測度 $P_{X}$ を$X$ の 確率分布Probability Distributionと呼ぶ。 $$ P_{X} (B) := P ( X^{-1} (B) ) $$ 測度論についてまだ知らないなら、確率空間という言葉を無視してもいい。 説明 確率空間と同じように、確率変数も測度論で厳密に定義することができる。\n$X^{-1} (B) \\in \\mathcal{F}$ という言葉は、$X$ が $\\Omega$ の要素を実数にマッピングして大小関係 $P(a \\le X \\le b)$ のようなものを使えるようにしつつ、ボレル集合の逆像がシグマ場に属させ、理にかなった集合だけを事象として扱うように制約を加えたことを意味している。一見すると過度に抽象的に見えるかもしれないが、逆説的に、その目的は過度な抽象性を失わせることにあるとも考えられる。定義によれば、確率変数$X$ は実数関数であるだけでなく可測関数となり、もし$\\Omega = \\mathbb{R}$ なら $\\mathcal{F} = \\mathcal{B} \\left( \\mathbb{R} \\right)$ で、ただのボレル関数 $X : \\mathbb{R} \\to \\mathbb{R}$ となる。通常、数理統計学の簡単な定理はこのレベルで十分。その先、多変数確率変数への一般化は、すべてのボレル集合 $B \\in \\mathcal{B} (\\mathbb{R}^{p})$ に対して $X^{-1} (B) \\in \\mathcal{F}$ を満たす$X : \\Omega \\to \\mathbb{R}^{p}$ を定義することによって簡単に行うことができる。もちろん、$X$ は各確率変数 $X_{i} : \\Omega \\to \\mathbb{R}$ に対して$X = ( X_{1}, \\cdots , X_{p})$ のようにベクトルとして表すことができ、確率ベクターと呼ばれる。これが確率変数の数列につながれば確率過程Stochastic Process、さらに一般的には確率要素Random Elementと呼ばれる。 シグマ場$\\mathcal{G}$ に対して$Y^{-1} ( \\mathcal{B} ) \\in \\mathcal{G}$ ならば$Y$ が $\\mathcal{G}$-可測であるというけれども、$\\mathcal{F}_{X}$ の定義によれば、当然$X$ は$\\mathcal{F}_{X}$-メジャラブルである。 定義が多くて混乱するかもしれないが、一つ一つ考えてみれば全く難しいことはない。$X^{-1} (B) \\in \\mathcal{F}$ であるため、これを逆関数のように考えると$X^{-1} : \\mathcal{B} (\\mathbb{R}) \\to \\mathcal{F}$ となる。このように、$P_{X} : = ( P \\circ X^{-1} )$ は $$ P_{X} : \\mathcal{B} (\\mathbb{R}) \\to \\mathcal{F} \\to [0,1] $$ と理解でき、ボレル集合$B$ に対して$0$ から$1$ までのどんな値にもマッピングする単なる合成関数にすぎない。例えば、$[-3,-2]$ は自然に$\\mathbb{R}$ のボレル集合で、確率変数$Y$ がどのように定義されているかによって、$P_{Y} ( [-3,-2] ) = 0.7$ のような計算をすることができるようになるわけだ。 参考 数理統計学で定義された確率変数と確率分布 Capinski. (1999). Measure, Integral and Probability: p66~68.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1288,"permalink":"https://freshrimpsushi.github.io/jp/posts/1288/","tags":null,"title":"測度論で定義される確率変数と確率分布"},{"categories":"측도론","contents":"定義1 $(X, \\mathcal{E})$を可測空間だとしよう。以下の条件を満たす拡張実数値関数$\\nu : \\mathcal{E} \\to \\overline{\\mathbb{R}}$を符号付き測度signed measureという。\n$\\nu ( \\varnothing ) = 0$ $\\pm \\infty$の中で高々1つだけが$\\nu$の関数値になれる。つまり、$-\\infty \\in \\nu (\\mathcal{E})$ならば$+\\infty \\notin \\nu (\\mathcal{E})$であり、$+\\infty \\in \\nu (\\mathcal{E})$ならば$-\\infty \\notin \\nu (\\mathcal{E})$である。 $\\left\\{E_{j}\\right\\}$を$\\mathcal{E}$で互いに素な集合の数列としよう。すると$\\nu \\left( \\bigcup \\nolimits_{j=1}^\\infty E_{j} \\right) =\\sum \\limits_{j=1}^\\infty \\nu (E_{j})$を満たす。この時$\\nu (\\cup _{1}^\\infty E_{j})$が有限の場合、右辺の和は絶対収束する。 説明 簡単に言うと、負の値も取れるように一般化された測度である。従って測度だったら符号付き測度でもある。測度と符号付き測度を一緒に言及する際は強調のために測度を正の測度positive measureと呼ぶこともある。符号付き測度の具体的な例にはリーマン積分がある。\n一方で、測度は常に0以上の関数値を持たなければならないため、任意の関数のリーマン積分に絶対値を取ったものと考えることができる。また、全ての符号付き測度は2つの測度の差で表現可能である。\n$$ \\nu = \\mu_{1} -\\mu_2 $$\n性質 $\\nu$を可測空間$(X,\\mathcal{E})$で定義された符号付き測度としよう。\n下からの連続性:\n$\\left\\{ E_{j} \\right\\}_{1}^\\infty \\subset \\mathcal{E}$が単調増加数列だとしよう。つまり$E_{1} \\subset E_2 \\subset \\cdots$。すると以下が成り立つ。 $$ \\mu\\left( \\bigcup \\nolimits _{1}^\\infty E_{j} \\right)= \\lim \\limits_{j\\rightarrow \\infty} \\mu (E_{j}) $$\n上からの連続性:\n$\\left\\{ E_{j} \\right\\}_{1}^\\infty \\subset \\mathcal{E}$が単調減少数列だとしよう。つまり$E_{1} \\supset E_2 \\supset \\cdots$。そして$\\mu (E_{1})\u0026lt;\\infty$としよう。すると以下が成り立つ。 $$ \\mu\\left(\\bigcap \\nolimits _{1}^\\infty E_{j} \\right)= \\lim \\limits_{j\\rightarrow \\infty} \\mu (E_{j}) $$\n基本的には測度での証明方法1と同じである。測度の文脈での証明には可算加法性が必要だったが、符号付き測度にも可算加法性があるため、証明方法は同じである。従って省略する。\n参照 測度 複素測度 性質 (C), (D) を参照\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1301,"permalink":"https://freshrimpsushi.github.io/jp/posts/1301/","tags":null,"title":"符号付き測度"},{"categories":"측도론","contents":"定義 可測空間 $(X,\\mathcal{E})$が与えられたとしよう。以下の三条件を満たす拡張実数値を持つ関数$\\mu : \\mathcal{E} \\to \\overline{\\mathbb{R}}$を測度だという。\n(a) $\\mu ( \\varnothing ) = 0$\n(b) $\\mu (E) \\ge 0,\\quad \\forall E\\in \\mathcal{E}$\n(c) $\\left\\{E_{j}\\right\\}$を$\\mathcal{E}$の中で互いに素な集合の数列としよう。すると、次が成り立つ。\n$$ \\mu \\left( \\bigcup _{j=1}^\\infty E_{j} \\right) =\\sum \\limits_{j=1}^\\infty \\mu (E_{j}) $$\n順序対$(X,\\mathcal{E}, \\mu)$を測度空間という。\n二つの集合 $E_{1}$、$E_2$が$E_{1} \\cap E_2=\\varnothing$を満たすならば、$E_{1}$と$E_2$は互いに素の集合と言う。\n説明 $\\mu$の条件を$\\mu\\ :\\ \\mathcal{E} \\rightarrow [0,\\infty]$に変えると**(b)**を含むので、省略できる。\n条件**(c)**は、簡単に言えば可算加法性だ。注意すべき点は、互いに素な集合に対してのみ成立するということだ。\n符号付き測度と測度を一緒に言及する場合、強調のために測度を正の測度とも呼ぶ。\n性質 $(X,\\mathcal{E},\\mu)$を測度空間としよう。\n(A) 単調性: $E,F\\in \\mathcal{E}$かつ$E\\subset F$ならば、$\\mu (E) \\le \\mu (F)$である。\n(B) 可算準加法性: $\\left\\{ E_{j} \\right\\}_{1}^\\infty$が$\\mathcal{E}$の元の数列ならば、$\\mu \\left( \\bigcup_{1}^\\infty E_{j} \\right) \\le \\sum _{1}^\\infty \\mu (E_{j})$である。\n(C) 下からの連続性: $\\left\\{ E_{j} \\right\\}_{1}^\\infty \\subset \\mathcal{E}$が単調増加数列であるとする。つまり$E_{1} \\subset E_2 \\subset \\cdots$。すると、次が成り立つ。 $$ \\mu\\left( \\bigcup \\nolimits _{1}^\\infty E_{j} \\right)= \\lim \\limits_{j\\rightarrow \\infty} \\mu (E_{j}) $$\n(D) 上からの連続性: $\\left\\{ E_{j} \\right\\}_{1}^\\infty \\subset \\mathcal{E}$が単調減少数列であるとする。つまり$E_{1} \\supset E_2 \\supset \\cdots$。そして$\\mu (E_{1})\u0026lt;\\infty$とする。すると、次が成り立つ。 $$ \\mu\\left(\\bigcap \\nolimits _{1}^\\infty E_{j} \\right)= \\lim \\limits_{j\\rightarrow \\infty} \\mu (E_{j}) $$\n証明 (A) $E \\subset F$とする。すると$F=F\\setminus E+ E$が成り立つ。$E$と$F\\setminus E$は互いに素なので、測度の定義**(c)**により、次が成り立つ。\n$$ \\mu (F) = \\mu (F\\setminus E+ E) = \\mu (F\\setminus E) + \\mu (E) $$\nすると、測度の定義**(b)**によって、次が成り立つ。\n$$ \\mu (F\\setminus E) + \\mu (E) \\ge \\mu (E) $$\nだから、次が得られる。\n$$ \\mu (F) \\ge \\mu (E) $$\n■\n(B) $F_{1}=E_{1}$としよう。そして、$k\u0026gt;1$に対して$F_{k}=E_{k} \\setminus \\left( \\bigcup_{1}^{k-1} E_{j} \\right)$としよう。すると、各々の$F_{k}$は互いに素で、$\\bigcup_{1}^n F_{j}=\\bigcup_{1}^n E_{j},\\ \\forall n$である。また、各々の$j$に対して$F_{j} \\subset E_{j}$である。だから、次が成り立つ。\n$$ \\mu \\left( \\bigcup \\nolimits_{1}^\\infty E_{j}\\right)=\\mu \\left( \\bigcup \\nolimits_{1}^\\infty F_{j}\\right)=\\sum \\limits_{1}^\\infty \\mu (F_{j}) \\le \\sum \\limits_{1}^\\infty\\mu (E_{j}) $$\n二番目の等号は測度の定義**(c)によって成り立つ。最後の不等式は(A)**によって成り立つ。\n■\n(C) $E_{0}:= \\varnothing$としよう。そして、$F_{j}=E_{j}\\setminus E_{j-1}$としよう。すると、各々の$F_{j}$は互いに素である。また、$\\bigcup _{1}^\\infty F_{j} =\\bigcup_{1}^\\infty E_{j}$が成り立つ。だから、次が成り立つ。\n$$ \\begin{align*} \\mu \\left( \\bigcup \\nolimits_{1}^\\infty E_{j}\\right) \u0026amp;= \\mu \\left( \\bigcup \\nolimits_{1}^\\infty F_{j}\\right) \\\\ \u0026amp;= \\sum_{1}^\\infty \\mu (F_{j}) \\\\ \u0026amp;= \\sum \\limits_{1}^\\infty \\mu (E_{j} \\setminus E_{j-1} ) \\\\ \u0026amp;= \\lim \\limits_{n \\rightarrow \\infty} \\sum \\limits_{1} ^n \\mu (E_{j}\\setminus E_{j-1} ) \\\\ \u0026amp;= \\lim \\limits_{n \\rightarrow \\infty} \\mu (E_{n}) \\\\ \u0026amp;= \\lim \\limits_{j \\rightarrow \\infty} \\mu (E_{j}) \\end{align*} $$\n二番目の等号は測度の定義**(c)**によって成り立つ。\n■\n(D) $F_{j}=E_{1} \\setminus E_{j}$としよう。すると、$F_{1} \\subset F_2 \\subset \\cdots$が成り立つ。また、$\\mu (E_{1})=\\mu (F_{j})+\\mu (E_{j})$で、$\\bigcup_{1}^\\infty F_{j}=E_{1} \\setminus \\left( \\bigcap_{1}^\\infty E_{j} \\right)$が成り立つ。$E_{1}= \\bigcup_{1}^\\infty F_{j}+\\bigcap_{1}^\\infty E_{j}$なので、次が成り立つ。\n$$ \\begin{align*} \\mu (E_{1}) \u0026amp;= \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) + \\mu \\left( \\bigcup \\nolimits _{1}^\\infty F_{j} \\right) \\\\ \u0026amp;= \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) + \\lim \\limits_{j \\rightarrow \\infty} \\mu ( F_{j} ) \\\\ \u0026amp;= \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) + \\lim \\limits_{j \\rightarrow \\infty}\\big[ \\mu ( E_{1} )-\\mu (E_{j}) \\big] \\\\ \u0026amp;= \\mu ( E_{1} )+ \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) -\\lim \\limits_{j \\rightarrow \\infty}\\mu (E_{j}) \\end{align*} $$\n二番目の等号は**(C)**によって成り立つ。$\\mu (E_{1}) \u0026lt; \\infty$なので、次が得られる。\n$$ \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) = \\lim \\limits_{j \\rightarrow \\infty}\\mu (E_{j}) $$\n■\n参照 ルベーグ測度 ボレル測度 符号付き測度 複素測度 ","id":1302,"permalink":"https://freshrimpsushi.github.io/jp/posts/1302/","tags":null,"title":"測度の一般的な定義"},{"categories":"알고리즘","contents":"定義 与えられた問題を解く時の時間を時間複雑度Time Complexity、メモリの要求を空間複雑度Space Complexityと言う。\n例 漸近記法はこれらを表現するのに非常に便利な手段になる。時間複雑度の例を見てみよう。\n定数時間 $O(1)$ $n$ にかかわらず終わることができるアルゴリズムで、実質的に時間がかからないことだ。例えば、$\\mathbb{x} = [4,3,8,-1,-9,0,5,7,2,6]$ の三番目の要素を見つけるアルゴリズムは、$\\mathbb{x}$ がどうなっているかに関心がなく、ただ $8$ を返せばいい。\n線形時間 $O(n)$ $n$ に比例する時間がかかる。例えば、$\\mathbb{x} = [4,3,8,-1,-9,0,5,7,2,6]$ の最大値を見つけるアルゴリズムは、$8$ で見つけられるが、それが本当に最大値であることを保証するためには、他の要素もすべてチェックしなければならない。表現がこんな感じだと少し悪い気がするが、実際、線形時間くらいであればかなり速いアルゴリズムだと言われている。\n二次時間 $O(n^2)$ $n^2$ に比例する時間がかかる。例えば、$\\mathbb{x} = [4,3,8,-1,-9,0,5,7,2,6]$ を大きさ順に並べ替える場合は、最大値を一度見つけて最後に置き、その最大値を抜いた配列でまた最大値を見つけることを繰り返せばいい。最大値を見つけるたびに $n, n-1, \\cdots , 1$ 時間がかかるので、その合計は 等差数列の和の公式により $\\displaystyle \\sum_{k=1}^{n} k = {{ n (n+1)} \\over {2}} = O(n^2)$ になる。もっと賢い方法があると思うかもしれないが、これより愚かな方法は考える必要がない。\n三次時間 $O(n^3)$ $n^3$ に比例する時間がかかる。例えば、$n=2^{k}$ 時に $n \\times n$ 行列の乗算を考えてみると、$A, B \\in \\mathbb{R}^{n \\times n}$ を掛けた$C = AB$ を計算することになるが、次の8つの ${{n} \\over {2}} \\times {{n} \\over {2}}$ 行列の積を計算して解決できる。 $$ AB= \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ A_{3} \u0026amp; A_{4} \\end{bmatrix} \\begin{bmatrix} B_{1} \u0026amp; B_{2} \\\\ B_{3} \u0026amp; B_{4} \\end{bmatrix} = \\begin{bmatrix} C_{1} \u0026amp; C_{2} \\\\ C_{3} \u0026amp; C_{4} \\end{bmatrix} = C $$\n$$ C_{1} = A_{1}B_{1} + A_{2} B_{3} $$\n$$ C_{2} = A_{1}B_{2} + A_{2} B_{4} $$\n$$ C_{3} = A_{3}B_{1} + A_{4} B_{3} $$\n$$ C_{4} = A_{3}B_{2} + A_{4} B_{4} $$ この計算を続けていくわけだが、一回の計算にかかる時間が $T(n)$ で、繰り返し外の実行時間を $c$ とすると $\\displaystyle T(n) = 8 T \\left( {{n} \\over {2}} \\right) + c$ だからだ。 $$ \\begin{align*} T(n) =\u0026amp; 8 T \\left( {{n} \\over {2}} \\right) + c \\\\ =\u0026amp; 8 \\left[ 8 T \\left( {{n} \\over {4}} \\right) + c \\right] + c \\\\ =\u0026amp; 8 \\left[ 64 T \\left( {{n} \\over {8}} \\right) + 8 c + c \\right] + c \\\\ =\u0026amp; 8^3 T \\left( {{n} \\over {8}} \\right) + (1+8+8^2)c \\\\ =\u0026amp; 8^3 T \\left( {{n} \\over {8}} \\right) + {{8^3 - 1} \\over {8 - 1}}c \\\\ \u0026amp; \\vdots \u0026amp; \\\\ \\approx\u0026amp; 8^{\\log_{2} n} ( T(1) + c ) \\\\ =\u0026amp; n^{\\log_{2} 8} ( T(1) + c ) \\\\ =\u0026amp; n^{3} ( T(1) + c ) \\\\ =\u0026amp; \\Theta ( n^3 ) \\end{align*} $$ 行列の乗算は、数学が入った応用分野でほとんど例外なく、それもかなり多く行う。しかし、$n^3$ は少し大きい。$n=100$ だけになると、なんと $n^3 = 10^6$ になる。ここでさらに計算を減らすことはできるだろうか？もともと不要な計算をしていないので、これ以上どう減らせるかは見当たらない。しかし、シュトラッセンアルゴリズムという驚くべき方法を使えば、これをさらに減らすことができる。これがアルゴリズムの醍醐味だ。\n対数時間 $O \\left( \\log (n) \\right)$ 非常に速いという意味だ。例えば、ソートされた配列 $\\text{sort} (\\mathbb{x}) = [-9,-1,0,2,3,4,5,6,7,8]$ で $0$ の位置を見つける問題があるとしよう。直感的に考えられる方法は、真ん中の要素を一つ選んで $0$ より大きいか小さいかを確認し、$0$ より大きければ右側を捨て、小さい場合は左側を捨てて配列を縮めながら探すことだ。これを二分探索Binary Searchと言う。一回の計算にかかる時間が $T(n)$ で、比較にかかる時間を $c$ とすると、 $$ \\begin{align*} T(n) =\u0026amp; T \\left( {{n} \\over {2}} \\right) + c \\\\ =\u0026amp; \\left[ T \\left( {{n} \\over {4}} \\right) + c \\right] + c \\\\ =\u0026amp; T \\left( {{n} \\over {4}} \\right) + 2 c \\\\ =\u0026amp; T \\left( {{n} \\over {8}} \\right) + 3 c \\\\ \u0026amp; \\vdots \u0026amp; \\\\ \\approx\u0026amp; T \\left( 1 \\right) + c \\log_{2} n \\\\ =\u0026amp; O \\left( \\log_{2} (n) \\right) \\end{align*} $$ それだからだ。対数の底の変換公式によれば $O( \\log (n)))$ になってもいいが、元々コンピュータ科学では、特に対数の底に言及がなければ、通常は $e = 2.7182\u0026hellip;$ ではなく$2$ だから、表現に気を使う必要はない。\n指数時間 $O(2^n)$ 非常に長い時間がかかるという意味だ。再帰函数を使ってフィボナッチ数列を計算する場合、非常に非効率的で、一度の計算が $n$ 番目のフィボナッチ数 $a_{n}$ を求めるためにかかる時間が $T(n)$ で、前の二項を足すのにかかる時間を $c$ とすると、黄金比 $\\phi = 1.618\u0026hellip;$ に対して、 $$ \\begin{align*} T(n) =\u0026amp; T(n-1) + T(n-2) + c \\\\ =\u0026amp; [ T(n-2) + T(n-3) + c ] + T(n-2) + c \\\\ =\u0026amp; 2 T(n-2) + T(n-3) + (1+1) c \\\\ =\u0026amp; 3 T(n-3) + 2 T(n-4) + (1+1+2) c \\\\ =\u0026amp; 5 T(n-4) + 3 T(n-5) + (1+1+2 + 3) c \\\\ =\u0026amp; 8 T(n-6) + 5 T(n-4) + (1+1+2+3+5) c \\\\ \u0026amp; \\vdots \u0026amp; \\\\ =\u0026amp; c \\sum_{k=1}^{n} a_{n} \\\\ \\approx\u0026amp; {{\\phi^{n-1}-1} \\over {\\phi-1}} c \\\\ =\u0026amp; \\Omega ( \\phi^n ) \\end{align*} $$ こうなる理由だ。最後の部分は、$\\displaystyle \\phi \\approx {{a_{n}} \\over {a_{n-1}}}$ であるため、等比数列の和の公式を使って近似したものだ。こんなアルゴリズムは現実的には使うことができず、動的プログラミングのような解決法を探すか、または、このような時間複雑度を持つことを利用して、暗号に応用することもできる。\n","id":1283,"permalink":"https://freshrimpsushi.github.io/jp/posts/1283/","tags":null,"title":"時間計算量と空間計算量"},{"categories":"측도론","contents":"まとめ $X$を任意の集合とする。そして、空集合でない$A \\subset \\mathcal{P}(X)$が与えられたとする。そうしたら、$A$を含む最小の$\\sigma$-代数、$\\mathcal{E}_{A}$が存在する。\n証明 $\\mathcal{E}_{A}$を定義し、それが$\\sigma$-代数になることを示した後、最小である1ことを示そうと思う。\n$A$を含む全ての$\\sigma$-代数の集合を$S$とする。\n$$ S:= \\left\\{ \\mathcal{E} \\subset \\mathcal{P}(X)\\ :\\ \\mathcal{E}\\ \\mathrm{is\\ } \\sigma \\mathrm{-algebra, \\ } A \\subset \\mathcal{E} \\right\\} $$\nそれで、$\\mathcal{P}(X) \\in S$であることは自明である。従って、$S \\ne \\varnothing$である。今、$\\mathcal{E}_{A} := \\bigcap \\limits_{\\mathcal{E} \\in S} \\mathcal{E}$としよう。そうすると$A \\subset \\mathcal{E}_{A}$である。さらに、$\\mathcal{E}_{A}$が$\\sigma$-代数であることを示すことができる。\n$\\sigma$-代数\n集合$X$が与えられたとする。下記の条件を満たす$X$の部分集合のコレクション $\\mathcal{E} \\subset \\mathcal{P}(X)$を**$\\sigma$-代数**という。\n(D1) $\\varnothing, X \\in \\mathcal{E}$ (D2) $E \\in \\mathcal{E} \\implies E^c \\in \\mathcal{E}$ (D3) $E_{k} \\in \\mathcal{E}\\ (\\forall k \\in \\mathbb{N}) \\implies \\bigcup_{k=1}^\\infty E_{k} \\in \\mathcal{E}$ (D4) $E_{k} \\in \\mathcal{E}\\ (\\forall\\ k \\in \\mathbb{N}) \\implies \\bigcap_{k=1}^\\infty E_{k} \\in \\mathcal{E}$ (D1)\n各$\\mathcal{E}$が$\\sigma$-代数であるので、$\\varnothing$、$X$が含まれていることは明らかである。従って、$\\mathcal{E}_{A}$の定義によれば、$\\varnothing$、$X\\in \\mathcal{E}_{A}$は明らかである。\n(D2)\n$E \\in \\mathcal{E}_{A}$とする。すると、$\\mathcal{E}_{A}$の定義により、各$\\mathcal{E}$に対しても$E \\in \\mathcal{E}$が成立する。各$\\mathcal{E}$は$\\sigma$-代数であるので、$E^c \\in \\mathcal{E}$である。従って、$\\mathcal{E}_{A}$の定義により$E^c \\in \\mathcal{E}_{A}$である。\n(D3)\n条件**（D2）を示したように、$\\mathcal{E}_{A}$の定義と各$\\mathcal{E}$が$\\sigma$-代数である事実を利用すると、簡単に示すことができる。(D4)は（D3）**が成立すれば、デモルガンの法則により自動的に成立する。\nしたがって、$\\mathcal{E}_{A}$は条件**（D1）〜（D4）**を満たすので、$\\sigma$-代数である。今、$A$を含む別の$\\sigma$-代数を$\\mathcal{E}^{\\prime}$としよう。すると、集合$S$の定義により、$\\mathcal{E}^{\\prime} \\in S$であり、明らかに$\\mathcal{E}_{A} \\subset \\mathcal{E}^{\\prime}$である。したがって、$\\mathcal{E}_{A}$は$A$を含む最小の$\\sigma$-代数である。\n■\n定義 この時、$\\mathcal{E}_{A}$を**$A$によって生成された$\\sigma$-代数**$\\sigma$-algebra generated by Aと呼び、$\\mathcal{G}(A)$で表記する。\n対$(X,\\mathcal{T})$を位相空間と言う。位相の定義により、$\\mathcal{T} \\subset \\mathcal{P}(X)$である。従って、上記の定理により、$\\mathcal{T}$を含む最小の$\\sigma$-代数が存在する。これを$\\mathcal{B}_\\sigma (X) :=\\mathcal{G}(\\mathcal{T})$で表記し、位相空間$(X,\\mathcal{T})$上のボレル$\\sigma$-代数あるいは単にボレル代数Borel algebraという。\n$\\mathcal{B}_\\sigma (X)$の要素をボレル集合Borel setと言い、対$(X,\\mathcal{B}_\\sigma (X) )$をボレル可測空間Borel measurable spaceと言う。\n簡単に言えば、ボレル代数とは全ての開集合を要素として持つ最小の$\\sigma$-代数である。特に、ボレル代数で定義される全ての測度をボレル測度Borel measureと呼ぶ。\n参照 実数空間におけるボレル集合 無駄な部分が最小限に抑えられたシグマ場とも言える。この意味で、ボレルシグマ場は特に確率論を議論する際に便利である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1251,"permalink":"https://freshrimpsushi.github.io/jp/posts/1251/","tags":null,"title":"ボレルσ-代数、ボレル可測空間"},{"categories":"매트랩","contents":"方法 subplot() 関数を使えば、複数の図を一ページに出力することができる。第一、第二の変数はそれぞれ、画像を出力する盤面の行と列を示し、図をどんな形で配置するかを決定する。第三の変数は、その図を何番目に配置するかを決定する。\n以下はコードと実際に出力された結果である。\nX1=Phantom();\rX2=radon(X1);\rX3=fft(X2);\rX4=iradon(X2,0:179);\rsubplot(2,2,1)\rimagesc(X1)\rtitle(\u0026#34;Phantom\u0026#34;);\rsubplot(2,2,2)\rimagesc(X2)\rtitle(\u0026#34;radon\u0026#34;);\rsubplot(2,2,3)\rimagesc(abs(X3))\rtitle(\u0026#34;fft\u0026#34;);\rsubplot(2,2,4)\rimagesc(X4)\rtitle(\u0026#34;iradon\u0026#34;); ","id":1247,"permalink":"https://freshrimpsushi.github.io/jp/posts/1247/","tags":null,"title":"MATLABで1ページに複数の図を出力する方法"},{"categories":"통계적분석","contents":"定義 1 $\\left\\{ X_{t} \\right\\}_{t=1}^{n}$、$\\left\\{ Y_{t} \\right\\}_{t=1}^{n}$を確率過程としよう。\n次のように定義された$\\rho_{k}$をラグ$k$の交差相関関数Crossという。 $$ \\rho_{k} (X,Y) := \\text{cor} \\left( X_{t} , Y_{t-k} \\right) = \\text{cor} \\left( X_{t+k} , Y_{t} \\right) $$ 次のように定義された$r_{k}$をラグ$k$の標本交差相関関数という。 $$ r_{k} := {{ \\sum \\left( X_{t} - \\overline{X} \\right) \\left( Y_{t-k} - \\overline{Y} \\right) } \\over { \\sqrt{ \\sum \\left( X_{t} - \\overline{X} \\right)^2 } \\sqrt{ \\left( Y_{t-k} - \\overline{Y} \\right)^2 } }} $$ 説明 交差相関関数は、二つの時系列データ間の相関関係を理解するための関数だ。時系列に適用される点のみが異なり、式だけ見たらピアソンの相関係数そのものだ。\nsCCF $r_{k}$はCCF $\\rho_{k}$の推定値で、$\\left\\{ X_{t} \\right\\}_{t=1}^{n}$、$\\left\\{ Y_{t} \\right\\}_{t=1}^{n}$が定常性を持ちつつ互いに独立していれば、次のように正規分布に従うとされる。 $$ r_{k} \\sim N \\left( 0 , {{ 1 } \\over { n}} \\left[ 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{k} ( X , Y) \\right] \\right) $$ これを利用して回帰分析のように仮説検定を行うことができる。\nテスト $\\displaystyle Y_{t} = e_{t} + \\sum_{k=0}^{m} \\beta_{k} X_{t-k}$としよう。\n$H_{0}$: $\\beta_{k} = 0$ つまり、$X_{t}$と$Y_{t-k}$は相関関係を持たない。 $H_{1}$: $\\beta_{k} \\ne 0$ つまり、$X_{t}$と$Y_{t-k}$は相関関係を持つ。 解釈 帰無仮説の下では、$\\rho_{k} ( X , Y) = 0$と同時に$\\displaystyle N \\left( 0 , {{ 1 } \\over { n }} \\right)$を仮定し、標準誤差は$\\displaystyle {{1} \\over {\\sqrt{n}}}$となる。したがって、有意水準 $\\alpha$で仮説検定を行いたい場合は、$| r_{k} |$が信頼区間上限$\\displaystyle {{z_{1- \\alpha/2}} \\over {\\sqrt{n} }}$を超えるか確認すればいい。超えた場合は有意なラグの候補となり、超えなければ相関関係がないとみなされる。\n参照 ACF：自己相関関数 PACF：偏自己相関関数 EACF：拡張自己相関関数 Cryer. (2008). 時系列分析：Rでのアプリケーション（第2版）: p261~262.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1227,"permalink":"https://freshrimpsushi.github.io/jp/posts/1227/","tags":null,"title":"相互相関関数"},{"categories":"양자역학","contents":" 🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧\n反射係数と透過係数Reflection coefficient 波動関数の反射係数(反射率)と透過係数(透過率)は次のように表せる。 $$ R=\\left| \\frac{j_{ref}}{j_{inc}} \\right|,\\quad T=\\left| \\frac{j_{trans}}{j_{inc}}\\right| $$ ここで、$j$は確率流だ。$inc$は入射$(\\mathrm{incident})$を意味する。$R$、$ref$は反射$(\\mathrm{reflection})$を表し、$T$、$trans$は透過$(\\mathrm{transmission})$を示す。\nエネルギーが$E$の粒子がそのエネルギーよりも高いポテンシャル障壁に遭遇した時、反射と透過が起こる。古典的な観点からは、粒子は透過せずに純粋に反射するだけだ。しかし、量子の世界では、粒子の波動性のために、確率的に透過が起こる。これを量子トンネリングtunneling})$ 혹은 터널 효과$(\\mathrm{トンネル効果と呼ぶ。つまり、反射する比率と透過する比率が入射波、反射波、透過波の確率流の比率で表されるということだ。フラックス$(\\mathrm{flux}$、フラックス$)$は、単位時間に何らかの点を通過する物理量の量をフラックスという。従って、波動関数が反射および透過をする比率を知りたければ、入射波のフラックスに対する反射波、透過波のフラックスの比率を確認すればいい。すなわち、「反射率=(反射波のフラックス)/(入射波のフラックス)」、「透過率=(透過波のフラックス)/(入射波のフラックス)」と表せる。しかし、量子力学では、粒子の波動関数は、粒子が見つかる確率を意味し、確率の流れを表す物理量として確率流密度 $j(x,t)$がある。実際、$j(x,t)$が波動関数のフラックスを示すことが確認できる。ある波動関数 $\\psi=A e^{ikx}$が与えられたとする。それなら、区間 $\\Delta x$を通過する波動関数は $|\\psi|^2 \\Delta x =|\\psi \\psi^{\\ast}| \\Delta x=P\\Delta x$で表せる。$\\psi$でなく$|\\psi|^2$である理由は、私たちが扱っている物理量が実数だからだ。波動関数 $\\psi$は複素関数であるため、実数値を示すために$|\\psi \\psi^{\\ast}|$で表す。この時、$P=|\\psi \\psi^{\\ast}|=|Ae^{ikx} A^{\\ast}e^{-ikx}|=|AA^{\\ast}|=|A|^2$だ。従って、フラックスは単位時間に流れる量であるので、$\\psi$のフラックスは $$ F=|A|^2\\dfrac{\\Delta x}{\\Delta t} $$ 非常に短い時間について考えると、$\\dfrac{\\Delta x}{\\Delta t}=v$だ。量子力学では、速度ではなく運動量を扱うので、$v=\\frac{p}{m}$が成り立ち、量子力学における運動量は$p=\\hbar k$なので $$ F=|A|^2\\frac{\\hbar k}{m} $$ $\\psi$の確率流を計算すると、正確に上記の値と同じであることが分かる。 $$ \\begin{align*} j =\u0026amp;\\ \\frac{\\hbar }{2mi}\\left( \\psi* \\frac{\\partial \\psi}{\\partial x} -\\psi\\frac{\\partial \\psi^{\\ast}}{\\partial x} \\right) \\\\ =\u0026amp;\\ \\frac{\\hbar}{2mi}\\left( A^{\\ast}e^{-ikx}(ik)Ae^{ikx}-Ae^{ikx}(-ik)A^{\\ast}e^{-ikx}\\right) \\\\ =\u0026amp;\\ \\frac{\\hbar}{2mi} \\left( ikAA^{\\ast}+ikAA^{\\ast}\\right) \\\\ =\u0026amp;\\ \\frac{\\hbar k}{m}|A|^2 \\end{align*} $$ 従って、入射波の確率流と反射波の確率流の比が反射率を、入射波の確率流と透過波の確率流の比が透過率を表す。もし、波動関数が次のように実関数として与えられたとする。 $$ u=Ae^{kx} $$ それなら、波の確率流 $j$は$0$になる。 $$ \\begin{align*} j =\u0026amp;\\ \\frac{\\hbar }{2mi}\\left( u^{\\ast} \\frac{\\partial u}{\\partial x} -u\\frac{\\partial u^{\\ast}}{\\partial x} \\right) \\\\ =\u0026amp;\\ \\frac{\\hbar }{2mi}\\left( u \\frac{\\partial u}{\\partial x} -u\\frac{\\partial u}{\\partial x} \\right) \\\\ =\u0026amp;\\ 0 \\end{align*} $$\n","id":1241,"permalink":"https://freshrimpsushi.github.io/jp/posts/1241/","tags":null,"title":"波動関数の反射と透過"},{"categories":"바나흐공간","contents":"実数に関するハーン・バナッハの定理1 $X$は$\\mathbb{R}$-ベクトル空間であり、$Y \\subset X$とする。$p : X \\to \\mathbb{ R}$を$X$の準線形 線形汎関数とする。今、$y^{\\ast} : Y \\to \\mathbb{ R}$が以下の条件を満たす$Y$の$\\mathbb{R}$-線形汎関数であると仮定する。\n$$ y^{\\ast}(y) \\le p(y)\\quad \\forall y\\in Y $$\nすると、以下の条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{R}$が存在する。\n(a) $x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$\n(b) $x^{\\ast}(x) \\le p(x),\\quad \\forall x \\in X$\n説明 $\\mathbb{R}-$ベクトル空間とは、体$\\mathbb{R}$に関するベクトル空間のことである。つまり、ベクトル空間のスカラー倍に関する条件$(M1)$〜$(M5)$が実数に対して成立するという意味である。同様に、$\\mathbb{R}$-線形とは、線形の二つの性質のうちスカラー倍に関する内容が実数に対して成立するという意味である。\n$X, Y$が$\\mathbb{R}$-ベクトル空間であるため、$y^{\\ast}$、$x^{\\ast}$が線形であることと$\\mathbb{R}$-線形であることは同じ意味である。この部分が混乱する場合は、**$\\mathbb{R}$-、$\\mathbb{C}$-はこの記事では存在しない文字と考えても、証明を理解する上で問題はない。**後にハーン・バナッハの定理をノルム空間に適用する際には、関数$p$がノルムに対応する。この定理の証明は省略し、複素数に関するハーン・バナッハの定理の証明に使用する補助定理として利用する。\n複素数に関するハーン・バナッハの定理2 $X$は$\\mathbb{C}$-ベクトル空間であり、$Y \\subset X$とする。$p : X \\to \\mathbb{ R}$を以下のように定義された準線形汎関数とする。\n$$ p(\\lambda x)=|\\lambda| p(x),\\quad x\\in X, \\lambda \\in \\mathbb{C} $$\nそして、$y^{\\ast} : Y \\to \\mathbb{ C}$が以下の条件を満たす$Y$の線形汎関数であると仮定する。\n$$ \\begin{equation} \\text{Re}\\left( y^{\\ast}(y) \\right) \\le p(y),\\quad \\forall y\\in Y \\end{equation} $$\nすると、以下の条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{C}$が存在する。\n$x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$ $\\text{Re}(x^{\\ast}(x)) \\le p(x),\\quad \\forall x \\in X$ 説明 実数に関する定理と比較した場合、$p$の値域が$\\mathbb{R}$であることは変わらないが、これは上述のように$X$がノルム空間の場合、$p$がノルムに対応するからである。$X$、$Y$は$\\mathbb{C}$-ベクトル空間であり、$\\mathbb{R} \\subset \\mathbb{C}$であるため、$\\mathbb{R}$-ベクトル空間である条件も満たされる。すべての複素数に対してベクトル空間の条件$(M1)$〜$(M5)$が成立する場合、自動的にすべての実数に対しても成立するからである。同様に、$y^{\\ast}$、$x^{\\ast}$は$\\mathbb{C}$-線形であるため、$\\mathbb{R}-$線形である条件も満たされる。\n証明 関数$\\psi : Y \\to \\mathbb{ R}$を以下のように定義する。\n$$ \\psi (y) = \\text{Re} ( y^{\\ast}(y) ) $$\nすると、$\\psi$も$Y$の$\\mathbb{C}$-線形汎関数であることが示される。これは$\\mathrm{ Re}$と$y^{\\ast}$が線形であるために自明な結果であり、示す過程は非常に簡単なので省略する。$\\psi$の定義と$(1)$により、以下の式が成立する。\n$$ \\psi(y)= \\text{Re} \\left( y^{\\ast}(y) \\right) \\le |y^{\\ast}(y)| \\le p(y) $$\nすると、実数に関するハーン・バナッハの定理により、以下の条件を満たす$X$の$\\mathbb{R}$-線形汎関数$\\Psi : X \\to \\mathbb{ R}$が存在する。\n$$ \\Psi (y) = \\psi (y),\\quad \\forall y \\in Y $$\n$$ \\Psi (x) \\le p(x),\\quad \\forall x \\in X $$\nそして、新たに関数$\\Phi : X \\to \\mathbb{ C}$を以下のように定義しよう。最終的な目標は、以下のように定義された$\\Phi$が、定理で存在すると言われていた$x^{\\ast}$であることを示すことである。\n$$ \\Phi (x) := \\Psi (x) -i \\Psi(ix) $$\nすると、$\\Phi$が$X$の線形汎関数であることが確認できる。$\\Psi$が$\\mathbb{R}$-線形であるため、加法と実数乗に関しては線形性が自明であるため、$\\Phi(ix)=i\\Phi(x)$のみを確認すればよい。\n$$ \\begin{align*} \\Phi(ix) =\u0026amp;\\ \\Psi(ix) -i \\Psi( -x) \\\\ =\u0026amp;\\ \\Psi(ix)+i\\Psi(x) \\\\ =\u0026amp;\\ -i^2 \\Psi(ix)+i\\Psi(x) \\\\ =\u0026amp;\\ i \\big( \\Psi(x)-i\\Psi(ix) \\big) \\\\ =\u0026amp;\\ i\\Phi(x) \\end{align*} $$\n$\\Phi$が**(a)**を満たすことは、以下のように示すことができる。$y \\in Y$とすると、\n$$ \\begin{align*} \\Phi(y) =\u0026amp;\\ \\Psi (y) -i \\Psi(iy) \\\\ =\u0026amp;\\ \\psi(y) -i\\psi(iy) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right)-i\\text{Re} \\left( y^{\\ast}(iy) \\right) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right) +\\text{Im} \\left(-iy^{\\ast}(iy) \\right) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right) +\\text{Im} \\left( y^{\\ast}(y) \\right) \\\\ =\u0026amp;\\ y^{\\ast}(y) \\end{align*} $$\n$\\Phi$が**(b)**を満たすことを示すのはさらに簡単である。\n$$ \\mathrm{Re }\\left( \\Phi(x) \\right) = \\Psi(x) \\le p(x) $$\nしたがって、$\\Phi$が$X$の線形汎関数であり、**(a), (b)**を満たすため、$x^{\\ast}=\\Phi$が存在する。\n■\nセミノルムに関するハーン・バナッハの定理 $X$は$\\mathbb{C}$-ベクトル空間であり、$Y \\subset X$とする。$p : X \\to \\mathbb{ R}$を$X$のセミノルムとする。そして、$y^{\\ast} : Y \\to \\mathbb{ C}$が以下の条件を満たす$Y$の線形汎関数であると仮定する。\n$$ | y^{\\ast}(y) | \\le p(y),\\quad \\forall y\\in Y $$\nすると、以下の条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{C}$が存在する。\n$x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$\n$| x^{\\ast}(x) | \\le p(x),\\quad \\forall x \\in X$\n証明 セミノルムと準線形の定義から、$p$がセミノルムであれば準線形の条件も自動的に満たされる。\nまず、以下の式が成立することは自明である\n$$ \\text{Re} \\left( y^{\\ast}(y) \\right) \\le |y^{\\ast}(y) | \\le p(y) $$\nしたがって、複素数に関するハーン・バナッハの定理により、以下の二つの条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{C}$が存在する。\n$$ x^{\\ast}(y)=y^{\\ast}(y) \\quad \\forall y \\in Y $$\n$$ \\text{Re} \\left( x^{\\ast}(x) \\right) \\le p(x) \\quad \\forall x \\in X $$\n$S = \\left\\{ \\lambda \\in \\mathbb{C} : | \\lambda | =1 \\right\\}$とする。すると、\n$$ \\begin{align*} \\text{Re} \\left( \\lambda x^{\\ast}(x) \\right) =\u0026amp;\\ \\text{Re} \\left( \\lambda x^{\\ast}(\\lambda x) \\right) \\\\ \\le \u0026amp; p(\\lambda x) \\\\ =\u0026amp;\\ |\\lambda| p(x)=p(x) \\quad \\forall x \\in X \\end{align*} $$\nこの時、固定された$x \\in X$に対して$|x^{\\ast}(x)|=\\lambda x^{\\ast}(x)$を満たす$\\lambda \\in S$を常に見つけることができる。したがって、$x$とその特定の$\\lambda$に対して、以下の式が成立する。\n$$ | x^{\\ast}(x) | =\\lambda x^{\\ast}(x) = \\text{Re} \\left( \\lambda x^{\\ast}(x) \\right) \\le p(x), \\quad \\forall x \\in X $$\n$X$の線形汎関수$x^{\\ast}$が二つの条件を満たすため、証明完了。\n■\n付録 固定された$x$に対して$x^{\\ast}(x)=a+ib$とする。$\\lambda=c+id$とする。$\\lambda$の条件により$c^2+d^2 =1$であるため、$\\lambda=c+i\\sqrt{1-c^2}$である。また、$|x^{\\ast}(x)|=\\sqrt{a^2+b^2}$である。$\\lambda x^{\\ast}(x)=(ac-b\\sqrt{1-c^2})+i(a\\sqrt{1-c^2}+bc)$であり、$|x^{\\ast}(x)|$が非負の実数であるため、\n$$ \\begin{align*} \u0026amp;\u0026amp; a\\sqrt{1-c^2}+bc =\u0026amp;\\ 0 \\\\ \\implies\u0026amp;\u0026amp; a^2(1-c^2) =\u0026amp;\\ b^2c^2 \\\\ \\implies\u0026amp;\u0026amp; a^2 =\u0026amp;\\ (a^2+b^2)c^2 \\\\ \\implies\u0026amp;\u0026amp; c^2 =\u0026amp;\\ \\dfrac{a^2}{a^2+b^2} \\tag{2} \\end{align*} $$\n便宜上$c=\\dfrac{a}{\\sqrt{a^2+b^2}}$とし、$d=\\dfrac{-b}{\\sqrt{a^2+b^2}}$とする。すると、$(2)$と$c^2+d^2=1$が成立する。また、$|x^{\\ast}(x)|=ac-bd=\\sqrt{a^2+b^2}$が成立する。したがって、固定された$x$に対して$x^{\\ast}(x)=a+ib$であれば、$\\lambda=\\dfrac{a}{\\sqrt{a^2+b^2}}-i\\dfrac{b}{\\sqrt{a^2+b^2}}\\in S$に対して$|x^{\\ast}(x)|=\\lambda x^{\\ast}(x)$が成立する。\nhttp://mathonline.wikidot.com/the-hahn-banach-theorem-real-version\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://mathonline.wikidot.com/the-hahn-banach-theorem-complex-version\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1230,"permalink":"https://freshrimpsushi.github.io/jp/posts/1230/","tags":null,"title":"実数、複素数、セミノルムに対するハーン・バナッハの定理"},{"categories":"해석개론","contents":"定義1 $a$を含むある$E$で$f$が定義されていて、限界\n$$ f^{\\prime} (a) := \\lim_{h \\to 0} {{ f (a + h ) - f(a) } \\over { h }}=\\lim \\limits_{x\\rightarrow a}\\frac{f(x)-f(a)}{x-a} $$\nが存在するならば、$f$は$a$で微分可能differentiableであるといい、$f^{\\prime} (a)$を$a$での$f$の微分係数という。\n全ての点$a \\in E$に対して$f$が微分可能なら、$f$は$E$で微分可能であるという。$f$が$E$で微分可能な時、$E$上で定義された$f^{\\prime}$を$f$の導関数derivativeと呼ぶ。\n説明 解析学を学ぶ上で最も歓迎されるのが微分だ。なぜなら、数列であれ積分であれ本来の姿をそのまま持っているだけでなく、複雑になることに比べて、微分だけが比較的簡単で理解しやすいからだ。多重積分や偏微分も登場するが、他の概念に比べれば簡単で分かりやすい。このように微分の定義をあえて「実数空間」に限定し、偏微分が言及されるのは、微分が多次元に拡張されることを示唆しているためだ。\n要約 (a) 連続性: $f$が$a \\in E$で微分可能ならば、$a \\in E$で連続である。\n(b) 連鎖律: $( g \\circ f)' ( a ) = g \u0026rsquo; ( f (a) ) f '(a)$\n(c) 逆関数の定理: 開区間$E$で$f : E \\to \\mathbb{R}$が一対一の連続関数であるとする。(i) ある$a \\in E$に対して$b = f(a)$であり、(ii): $f ' (a) \\ne 0$が存在するならば、$f^{-1}$は$a$で微分可能であり、\n$$ \\left( f^{-1} \\right)' (b) = {{ 1 } \\over { f '(a) }} $$\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), p98-99\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1210,"permalink":"https://freshrimpsushi.github.io/jp/posts/1210/","tags":null,"title":"実数空間で定義された関数の微分"},{"categories":"통계적분석","contents":"定義 1 $\\left\\{ Y_{t} \\right\\}_{t=1}^{n}$ を確率過程とする。\n$\\mu_{t} := E ( Y_{t} )$ を平均関数という。 次のように定義された $\\gamma_{ t , s }$ を自己共分散関数という。 $$ \\gamma_{t , s} : = \\text{cov} ( Y_{t} , Y_{s} ) = E ( Y_{t} - \\mu_{t} ) E ( Y_{s} - \\mu_{s} ) $$ 次のように定義された $\\rho_{ t , s }$ を自己相関関数という。 $$ \\rho_{ t , s } := \\text{cor} ( Y_{t} , Y_{s} ) = {{ \\gamma_{t , s} } \\over { \\sqrt{ \\gamma_{t , t} \\gamma_{s , s} } }} $$ 次のように定義された $\\rho_{ k }$ をラグ$k$の自己相関関数という。 $$ \\rho_{ k } := \\text{cor} ( Y_{t} , Y_{t-k} ) = {{ \\gamma_{t , t - k} } \\over { \\sqrt{ \\gamma_{t , t} \\gamma_{t-k , t-k} } }} $$ 次のように定義された $r_{ k }$ をラグ$k$の標本自己相関関数という。 $$ r_{ k } := {{ \\sum_{t = k+1}^{n} \\left( Y_{t} - \\overline{Y} \\right) \\left( Y_{t-k} - \\overline{Y} \\right) } \\over { \\sum_{t=1}^{n} \\left( Y_{t} - \\overline{Y} \\right)^2 }} $$ 説明 自己相関関数とは、時系列データの自己相関性を把握するための関数で、同じ変数でもあるラグを持って自己とどの程度似ているかに関心を持つ。異なる変数の相関関係に関心を持つ回帰分析のアイデアとは異なり、自己がラグ$k$を持って$Y_{t}$と$Y_{t-k}$に分かれ、二つの変数のように扱われる。\n数式的説明 数式的には、$Y_{t}$が$MA(q)$から出てきたと考えると、$\\displaystyle Y_{t} = e_{t} - \\sum_{k=1}^{q} \\theta_{k} e_{t-k}$であるため$Y_{t}$を複数の正規分布の和と見ることができ、$\\rho_{k}$が$\\theta_{k}$であるため、$MA(q)$モデルを見つけるのに有用である。\nsACF $r_{k}$はACF $\\rho_{k}$の推定値であり、$Y_{t}$が$MA(q)$モデルから出てきた場合、$k \u0026gt; q$の時、正規分布$\\displaystyle N \\left( \\rho_{k} , {{1} \\over {n}} \\left[ 1 + 2 \\sum_{j=1}^{q} \\rho_{j}^{2} \\right]^2 \\right)$に従う。数式で表すと $$ r_{k} \\sim N \\left( \\rho_{k} , {{1} \\over {n}} \\left[ 1 + 2 \\sum_{j=1}^{q} \\rho_{j}^{2} \\right]^2 \\right) $$ であり、これを利用して仮説検定を行う。\nテスト $\\displaystyle Y_{t} = e_{t} - \\sum_{k=1}^{q} \\theta_{k} e_{t-k}$が与えられ、$k = 1 , \\cdots , q$とする。\n$H_{0}$: $MA(0) \\iff \\theta_{k} = 0$、つまり、$Y_{t}$は移動平均モデルに従わない。 $H_{1}$: $MA(k) \\iff \\theta_{k} \\ne 0$、つまり、$Y_{t}$はラグ$k$の自己相関関係を持つ。 解釈 帰無仮説の下では、すべての$k$に対して$\\rho_{k} = \\theta_{k} = 0$であるため、$q = 0$と$\\displaystyle r_{k} \\sim N \\left( 0 , {{1} \\over {N }} \\right)$を仮定し、標準誤差は$\\displaystyle {{1} \\over {\\sqrt{n} }}$となる。したがって、有意水準$\\alpha$に対して仮説検定を行いたい場合は、$| \\theta_{k} |$が信頼区間上限$\\displaystyle {{ z_{1 - \\alpha/2} } \\over { \\sqrt{n} }}$を超えるか確認すればよい。超えれば有意なラグの候補となり、超えなければ自己相関関係がないと見なされる。\n実践 ma1.2.sデータは、$MA(1)$モデルから得られたTSAパッケージのサンプルデータである。実際にARIMAモデルで分析する際も、推定値の絶対値が標準誤差の2倍を超えるかどうかを基準に、有意な係数かどうかを判断する。\nTSAパッケージのacf()関数を使用すると、上のように様々な$k$に対してコレログラムを描いてくれる。頭の中で計算することなく、線を超えれば有意と見てもよく、超えなければ有意でないと見てもよい。基本的に有意水準$5 \\%$で計算される。\n注意すべき点は、$k=6$をわずかに超えたものも統計的に有意ではあるが、実際に自己相関関係があるとは見なされないことである。時系列分析では、この程度の超出は非常に頻繁であり、精神衛生のためにも、柔軟性を持ってそのまま受け入れることをお勧めする。\n上のように実際に線を自分で引いてみることで、自己相関関数を使用した仮説検定を正しく理解したかどうかを確認する方法をお勧めする。Rではたった一行のコードだが、一度でも実行してみることで、$r_{k}$が正規分布に従い、その標準誤差が複雑な式なしで$\\displaystyle \\text{se} ( r_{k} ) = {{1} \\over {\\sqrt{n}}}$として得られることを受け入れることができる。\nコード library(TSA)\rdata(ma1.2.s); win.graph(6,4); acf(ma1.2.s)\rarima(ma1.2.s, order=c(0,0,1))\rabline(h=1.96*1/sqrt(length(ma1.2.s)),col=\u0026#39;red\u0026#39;) 参照 PACF : 部分自己相関関数 EACF : 拡張自己相関関数 CCF : 相互相関関数 Cryer. (2008). 時系列分析：Rによるアプリケーション(第2版): p11, 109。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1209,"permalink":"https://freshrimpsushi.github.io/jp/posts/1209/","tags":["R"],"title":"自己相関関数"},{"categories":"바나흐공간","contents":"定義1 $X$をベクター空間としよう。次の三条件を満たす関数$\\left\\| \\cdot \\right\\| : X \\to \\mathbb{R}$が存在すれば、$\\left\\| \\cdot \\right\\|$を$X$のノルムと呼び、$(X,\\left\\| \\cdot \\right\\| )$をノルム空間と呼ぶ。\n(a) $\\left\\| x \\right\\| \\ge 0,\\quad \\forall\\ x \\in X$かつ$\\left\\| x \\right\\|=0 \\iff x = 0$\n(b) $|cx|=|c|\\left\\| x \\right\\|,\\quad \\forall\\ x\\in X,\\ \\forall\\ c \\in\\mathbb{C}$\n(c) $\\left\\| x + y \\right\\| \\le \\left\\| x \\right\\| + \\left\\| y \\right\\|,\\quad \\forall\\ x,y\\in X$\n説明 ノルム空間$X$のノルムは以下のように表される。\n$$ \\left\\| x \\right\\|_{X},\\quad \\left\\| x, X \\right\\|, \\quad \\left\\| x ; X \\right\\| $$\n(a) $\\left\\| x \\right\\|=0 \\iff x = 0$の条件がない場合は、セミノルムになる。\n(b) は$\\left\\| x - y \\right\\| =|y -x|$が成立するという意味である。\n(c) を三角不等式と呼び、以下の不等式を逆三角不等式と呼ぶ。ノルム空間$(X, \\left\\| \\cdot \\right\\| )$と$x, y \\in X$に対して、以下の不等式が成立する。\n$$ \\left| \\left\\| x \\right\\| - \\left\\| y \\right\\|\\ \\right| \\le \\left\\| x- y \\right\\| $$\nノルムは連続写像である。\nノルム空間としての距離空間、位相空間 ノルムが与えられると、以下のように自然に距離を定義できる。したがって、ノルム空間は距離空間になる。\n$$ d(x,y) = d_{X}(x,y) = \\left\\| x - y \\right\\|_{X} $$\n距離が与えられると、以下のようにオープンボールを定義できる。\n$$ B_{d}(x,r)=B_{r}(x):=\\left\\{ y\\in X\\ :\\ \\left\\| x - y \\right\\|_{X} \u0026lt;r \\right\\} $$\n全てのオープンボールの集合は$X$上の(位相数学での)基底になる。つまり、$X$のノルムで定義されたオープンボールによって$X$上の位相を作ることができるということである。このようにして作られた位相を$X$上のノルム位相2と呼ぶ。さらに、位相ベクター空間$X$の位相がノルム位相ならば、$X$をノルマブルと呼ぶ。\n以上の内容をまとめると、$X$がノルム空間であるということは、$X$がベクター空間であり、距離空間であり、位相空間であるという意味を全て含んでいるということである。したがって、関数解析学では与えられたノルム空間を自然に距離空間、位相空間としても扱う。\n証明3 三角不等式により、\n$$ \\left\\| x \\right\\|= | (x-y) +y| \\le |x-y| + \\left\\| y \\right\\| $$\nが成立する。したがって、\n$$ \\begin{equation} \\left\\| x \\right\\| - \\left\\| y \\right\\| \\le \\left\\| x- y \\right\\| \\end{equation} $$\n同様に、\n$$ \\left\\| y \\right\\| = | (y - x) + x| \\le \\left\\| y- x \\right\\| + \\left\\| x \\right\\| $$\nなので、\n$$ \\begin{equation} \\left\\| y \\right\\| - \\left\\| x \\right\\| \\le \\left\\| y- x \\right\\|=\\left\\| x - y \\right\\| \\end{equation} $$\nが成立する。したがって、$(1), (2)$により、\n$$ \\left| \\ \\left\\| x \\right\\| -\\left\\| y \\right\\|\\ \\right| \\le \\left\\| x- y \\right\\| $$\n■\nRobert A. Adams and John J. F. Foutnier、Sobolev Space (第2版、2003)、p4-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n距離の観点では、これを距離位相と呼ぶ。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOle Christensen、Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010)、p30\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1225,"permalink":"https://freshrimpsushi.github.io/jp/posts/1225/","tags":null,"title":"ノルム空間とは何か"},{"categories":"해석개론","contents":"定義 空集合じゃない$E \\subset \\mathbb{R}$に対して$f : E \\to \\mathbb{R}$としよう。全ての$\\varepsilon \u0026gt; 0$に対して\n$$ | x - a | \u0026lt; \\delta \\implies | f(x) - f(a) | \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在するなら、$f$を$a \\in E$で連続continuousと言い、$E$の全ての点で連続なら$f$を連続関数continuous functionという。\n説明 高校で連続を定義する時、\n関数値$f(a)$が存在する。 極限$\\lim \\limits_{x \\to a}$が存在する。 $f(a) = \\lim \\limits_{x \\to a}$が成り立つ。 この三条件が成り立つ時、$f$は$x = a$で連続って言った。イプシロン-デルタ論法を受け入れたなら、この定義は実は高校のレベルと変わらないことが分かるだろう。\n$| x - a | \u0026lt; \\delta$の時に$| f(x) - f(a) | \u0026lt; \\varepsilon$っていうのは、$x$が$a$の近くでちょっとだけ動くなら、$f(x)$も$f(a)$からちょっとだけ動くって意味になるだろう。つまり、$x$を変えて$f$に入れてみても、「急激に」、つまり不連続的に関数値が変わらないってことだ。言い換えれば、連続ってグラフで考えるなら「切れてない」ことを言うんだ。\n高校生の中には、こんな直感的にだけ受け入れて「切れてない」関数を連続関数と受け入れたケースが結構ある。そうじゃない例で言うと、$f(x) := {{ 1 } \\over { x }}$は$x=0$で切れてるけど、定義域$\\mathbb{R}^{ \\ast } = \\mathbb{R} \\setminus \\left\\{ 0 \\right\\}$の全ての点で連続だから、連続関数であってる。普通は知らなくても生きていく上で支障はないけど、知らなかったら、この機会にしっかりと概念を捉え直そう。\n定理 $f$が$a \\in E$で連続っていうのは、以下と同値だ。\n$$ \\lim \\limits_{n \\to \\infty} x_{n} = a \\implies \\lim \\limits_{n \\to \\infty} f( x_{n} ) = f(a) $$\nこの定理は、関数の連続性によって$\\lim \\limits_{n \\to \\infty}$が$f$の内外を行き来できることを保証する。数学以外の多くの分野で、ちゃんとチェックせずに当たり前のように使われる場合が多いけど、これもまた、数学者の立場からすると、厳密に問いただすべき点だ。\n","id":1206,"permalink":"https://freshrimpsushi.github.io/jp/posts/1206/","tags":null,"title":"大学数学で新しく定義される連続関数"},{"categories":"수치해석","contents":"メソッド 1 $D \\subset \\mathbb{R}^2$ で定義された連続関数について、初期値問題が$\\begin{cases} y ' = f(x,y) \\\\ y( x_{0} ) = Y_{0} \\end{cases}$で与えられている。区間$(a,b)$を$a \\le x_{0} \u0026lt; x_{1} \u0026lt; \\cdots \u0026lt; x_{n} \u0026lt; \\cdots x_{N} \\le b$のようなノードポイントで分けたとする。特に十分小さい$h \u0026gt; 0$に対して$x_{j} = x_{0} + j h$としたとき、初期値$y_{0} \\simeq Y_{0}$に対して $$ y_{n+1} = y_{n} + h f ( x_{n} , y_{n} ) $$\n説明 オイラーメソッドは概念的に非常にシンプルな方法だが、数値解析の核心的なアイデアを示している。もちろん今では様々な問題が多いが、逆に言えば改善の余地も多い手法である。だからといって、オイラーメソッド自体が重要というわけではなく、導出過程をしっかりと理解することが必要だ。\n導出 テイラー展開 $Y(x_{n+1} )$を$x_{n}$に対して$2$項までテイラー展開すると $$ Y ( x_{n+1} ) = Y ( x_{n} ) + ( x_{n+1} - x_{n}) y ' ( x_{n} ) + {{( x_{n+1} - x_{n})^2} \\over {2}} Y\u0026rsquo;\u0026rsquo; ( \\xi_{n} ) $$ 整理すると $$ Y ( x_{n+1} ) = Y ( x_{n} ) + h y ' ( x_{n} ) + {{h^2} \\over {2}} Y\u0026rsquo;\u0026rsquo; ( \\xi_{n} ) $$ ここで、誤差項$\\displaystyle {{h^2} \\over {2}} Y\u0026rsquo;\u0026rsquo; ( \\xi_{n} )$を無視すると $$ y_{n+1} = y_{n} + h f ( x_{n} , y_{n} ) $$\n■\nテイラー展開を用いた導出から直ちに分かることは、$h$が小さくなると誤差も減少するということだ。また、テイラー近似がどうして$2$次でなければならないという特別な理由はないため、次数を上げれば誤差が減少するということだ。\n数値積分 $[x_{n} , x_{n+1}]$から$Y\u0026rsquo;(t) = f (t, Y(t))$の定積分は $$ \\int_{x_{n}}^{x_{n+1}} f(t,Y(t)) dt = Y(x_{n+1}) - Y(x_{n}) $$ 少しの誤差はあるが、数値積分によって$\\displaystyle \\int_{x_{n}}^{x_{n+1}} f(t,Y(t)) dt \\simeq h f(x_{n} , Y(x_{n}) ) $であるため、 $$ y_{n+1} - y_{n} = h f ( x_{n} , y_{n} ) $$\n■\n分割積分の考え方からして、$h$が小さくなればなるほど誤差も小さくなると推測できる。図からわかるように、実際の積分値と数値的に計算された値の差はかなり大きいが、台形公式やシンプソン法を使えば誤差が減少するだろう。\n実装 以下はRで書かれたコードだ。\nEuler\u0026lt;-function(f,Y\\_0,a,b,h=10^(-3))\r{\rY \u0026lt;- t(Y\\_0)\rnode \u0026lt;- seq(a,b,by=h)\rfor(x in node)\r{\rY\u0026lt;-rbind(Y,Y[length(Y[,1]),]+h*f(x,Y[length(Y[,1]),]))\r}\rreturn(Y)\r}\rf\u0026lt;-function(x,y) {y}\rout\u0026lt;-Euler(f,seq(1,1,len=100),0,2)\rout[,1]\rg\u0026lt;-function(x,y) {1/(1+x^2) + - 2*(y^(2))}\rout\u0026lt;-Euler(g,seq(0,0,len=100),0,2,h=0.2)\rout[,1] Atkinson. (1989). 数値解析入門(第2版): p341.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":687,"permalink":"https://freshrimpsushi.github.io/jp/posts/687/","tags":null,"title":"数値解析におけるオイラー法"},{"categories":"해석개론","contents":"定義1 2 $\\mathbb{N}$ は自然数の集合を、$\\mathbb{R}$ は実数の集合を意味する。\n定義域が $\\mathbb{N}$ である関数を数列と言う。\n自然数の数列 $\\left\\{ n_{k} \\right\\}_{ k \\in \\mathbb{N}}$ に対して、$\\left\\{ x_{n_{k}} \\right\\}_{ k \\in \\mathbb{N}}$ を $\\left\\{ x_{n} \\right\\}_{ n \\in \\mathbb{N}}$ の部分数列Subsequenceという。\n全ての $x \\in \\left\\{ x_{n} \\right\\}_{ n \\in \\mathbb{N}}$ に対して $x \\le M$ を満たす $M \\in \\mathbb{R}$ が存在するならば、$\\left\\{ x_{n} \\right\\}_{ n \\in \\mathbb{N}}$ は 上に有界、$m \\le x$ を満たす $m \\in \\mathbb{R}$ が存在するならば 下に有界、上にも下にも有界ならば 有界Boundedという。\n$\\left\\{ x_{n } \\right\\}_{n = 1}^{\\infty}$ が実数列だとしよう。全ての $\\varepsilon \u0026gt; 0$ に対して $$n \\ge N \\implies | x_{n} - a | \u0026lt; \\varepsilon$$ を満たす $N \\in \\mathbb{N}$ が存在するなら、$\\left\\{ x_{n } \\right\\}$ は $a \\in \\mathbb{R}$ に収束するConvergeといい、$\\lim \\limits_{n\\to \\ \\infty}x_{n}=a$と表記される。\n$\\left\\{ x_{n } \\right\\}$が収束しない場合、発散するDivergeという。\n全ての $M \\in \\mathbb{R}$ に対して $n \\ge N \\implies x_{n} \u0026gt; M$を満たす $N \\in \\mathbb{N}$ が存在するなら $$ \\lim \\limits_{n\\to \\infty} x_{n} = +\\infty \\quad \\text{ or } \\quad x_{n} \\to +\\infty $$ と表記される。\n全ての $M \\in \\mathbb{R}$ に対して $n \\ge N \\implies x_{n} \u0026lt; M$を満たす $N \\in \\mathbb{N}$ が存在するなら $$ \\lim \\limits_{n\\to \\infty} x_{n} = -\\infty \\quad \\text{ or } \\quad x_{n} \\to -\\infty $$ と表記される。\n説明 大学で初めて収束と発散の定義に触れると、一体矢印はどこへ行って、見えにくい$\\varepsilon$、$M$、$N$が現れるのだろう。正直に言って、学びたくないだろう。学生の立場では、新しい極限の定義を知らないということが、極限の概念自体を知らないということではなく、中間試験さえ何とか乗り切れば、二度と見ることがないような気がするからだ。もちろん、愚かな考えだ。\n高校時代を振り返ると、先生方も$n \\to \\infty$について述べるときには「無限に大きくなる」とか「無限大に送る」という表現を使っていたが、何となく数列を「動く何か」として扱うことに過剰に慎重だったような気がする。それは、先生方が学んだ人々だからだ。\n直感ではなく厳密な定義を使用する理由は、実は厳密な定義の方が簡単だからである。SATなどで登場する「簡単な数列」では直感が速いが、「複雑な数列」を扱うには不十分であるため、厳密な定義が導入された。歴史的にも、イギリスの数学はニュートンを軸に大陸の数学を大きく先んじていたにもかかわらず、直感主義に固執したために学界の主導権を大陸に奪われた。\n数列の収束について学ぶ際の最大の障害は、その本質的な難しさではなく、「わざわざ」「難しく」「再度」学ぶことから来る嫌悪感が大きい。たとえば、$\\displaystyle \\lim_{n \\to \\infty} {{n + 3} \\over {2n}} = {{1} \\over {2}}$のような簡単な問題を無理やり回りくどく解く時がそうである。\n学ぶことが難しいというよりも、学びたくないというのが問題で、残念ながら、理解するのに苦労する学生たちにそれだけの価値がある数列を教えることは非常に困難な仕事だ。理解と共感を助けるために、次の2つの定理を紹介する。\n定理 $\\left\\{ w_{n} \\right\\}, \\left\\{ x_{n} \\right\\}, \\left\\{ y_{n} \\right\\}$が実数列であり、$a \\in \\mathbb{R}$だとしよう。\n(a) サンドイッチ定理:\n$$ \\displaystyle \\lim_{n \\to \\infty} x_{n} = \\lim_{n \\to \\infty} y_{n} = a $$\nが成立し、\n$$ n \\ge N_{0} \\implies x_{n} \\le w_{n} \\le y_{n} $$\nを満たす$N_{0} \\in \\mathbb{N}$が存在するならば、\n$$ \\displaystyle \\lim_{n \\to \\infty} w_{n} = a $$\n(b) 比較定理:\n$$ n \\ge N_{0} \\implies x_{n} \\le y_{n} $$\nを満たす$N_{0} \\in \\mathbb{N}$が存在するならば、\n$$ \\displaystyle \\lim_{n \\to \\infty} x_{n} \\le \\lim_{n \\to \\infty} y_{n} $$\nもちろん、サンドイッチ定理や比較定理は直観的に見ても明らかに成り立つものである。別に難しい事実ではない。でも、収束の新しい定義を拒否するあなたは、一体どうやってそれらを証明するのか？\nこれらの2つの定理は、高校レベルですでに証明なしに堂々と使用されていたが、実際には論理的な推論ではなく、常識的な推測を通じて受け入れられた仮説に過ぎなかった。人間の常識がどれほど頻繁に間違っているかを考えると、厳密な証明がなぜ必要か、少なくとも理工学の学生であれば納得できるだろう。\n収束の定義に従えば、これらの定理の証明は難しくはないが、読者がこのような議論に初めて触れるという前提で、できるだけ詳細に示そうとする。証明を読むと、$N$の存在に一貫して執着していると感じられるかもしれないし、実際そうである。収束性を示す際には、$| x_{n} - a |$が$\\varepsilon$より小さくなるような不等式を立てることが重要ではなく、式を満たす$N$の存在を示すことが優先される。\n率直に言って、数列の収束性を示す際に、$\\varepsilon$をどのように導いたかは関係ない。定義に従えば、$N$が存在するだけで収束するので、まずは$N$の存在に着目するべきである。これを理解していないと、問題で明らかに与えられている$N_{1}$や$N_{2}$を使わずに、もっともらしい不等式を並べて結局は論理的に崩壊した主張を提示してしまう。\n証明 (a) 戦略: ぼんやりと無限大に送るのではなく、不等式で具体的に$n \\ge N \\implies| w_{n} - a | \u0026lt; \\varepsilon$を満たす$N$の存在を示す。\n$\\varepsilon \u0026gt; 0$とする。\n$\\displaystyle \\lim_{n \\to \\infty} x_{n} = \\lim_{n \\to \\infty} y_{n} = a$より、\n$$ n \\ge N_{1} \\implies | x_{n} - a | \u0026lt; \\varepsilon $$\n$$ n \\ge N_{2} \\implies | y_{n} - a | \u0026lt; \\varepsilon $$\nを満たす$N_{1} , N_{2} \\in \\mathbb{N}$が存在する。必要な部分を要約すると、\n$$ n \\ge N_{1} \\implies a - \\varepsilon \u0026lt; x_{n} $$\n$$ n \\ge N_{2} \\implies y_{n} \u0026lt; a + \\varepsilon $$\n一方で、▷eq59\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), Chapter 2.1-2.2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), Chapter 3.1-3.4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1184,"permalink":"https://freshrimpsushi.github.io/jp/posts/1184/","tags":null,"title":"大学数学における数列の極限を新たに定義する理由"},{"categories":"매트랩","contents":"方法 MATLABでグラフの各軸が何を意味するか示すためにラベルを付ける場合、xlabelとylabelを使用する。特殊記号やボールド体、イタリック体も使用できる。\nx=-3*pi:0.2:3* pi;\ry=sin(x-pi/6);\rplot(x,y);\rxlabel(\u0026#39;\\beta\u0026#39;), ylabel(\u0026#39;\\nabla f(x)\u0026#39;),; x=-3*pi:0.2:3* pi;\ry=sin(x-pi/6);\rplot(x,y);\rxlabel(\u0026#39;진폭{\\bf Volt}\u0026#39;), ylabel(\u0026#39;시간{\\it sec}{\\sl sec}{\\rm sec}\u0026#39;); 記号 コード 名前 記号 コード 名前 記号 コード 名前 $\\alpha$ \\alpha アルファ $\\beta$ \\beta ベータ $\\gamma$ \\gamma ガンマ $\\delta$ \\delta デルタ $\\epsilon$ \\epsilon イプシロン $\\zeta$ \\zeta ゼータ $\\eta$ \\eta エータ $\\theta$ \\theta シータ $\\vartheta$ \\vartheta バーシータ $\\iota$ \\iota イオタ $\\kappa$ \\kappa カッパ $\\lambda$ \\lambda ラムダ $\\mu$ \\mu ミュー $\\nu$ \\nu ニュー $\\xi$ \\xi クシー $\\pi$ \\pi パイ $\\rho$ \\rho ロー $\\sigma$ \\sigma シグマ $\\varsigma$ \\varsigma バーシグマ $\\tau$ \\tau タウ $\\upsilon$ \\upsilon ウプシロン $\\phi$ \\phi ファイ $\\chi$ \\chi カイ $\\psi$ \\psi プサイ $\\omega$ \\omega オメガ $\\Gamma$ \\Gamma 大文字ガンマ $\\Delta$ \\Delta 大文字デルタ $\\Theta$ \\Theta 大文字シータ $\\Lambda$ \\Lambda $ 대문자 람다 $ \\Xi $ \\Xi 대문자 크시 $ \\Pi $ \\Pi 대문자 파이 $ \\Sigma $ \\Sigma 대문자 시그마 $ \\Upsilon $ \\Upsilon 대문자 웁실론 $ \\Phi $ \\Phi 대문자 피 $ \\Psi $ \\Psi 대문자 프사이 $ \\Omega $ \\Omega 대문자 오메가 $ \\forall $ \\forall $ \\exists $ \\exsits $ \\in $ in $ \\surd $ \\surd $ \\cong $ \\cong $ \\approx $ \\approx $ \\equiv $ \\equiv $ \\Re $ \\Re $ \\Im $ \\Im $ \\otimes $ \\otimes $ \\oplus $ \\oplus $ \\cup $ \\cup $ \\cap $ \\cap $ \\subset $ \\subset $ \\supset $ \\supset $ \\subseteq $ \\subseteq $ \\supseteq $ \\supseteq $ \\lceil $ \\lceil $ \\rceil $ \\rceil $ \\lfloor $ \\lfloor $ \\rfloor $ \\rfloor $ \\displaystyle \\int $ \\int $ \\perp $ \\perp $ \\wedge $ \\wedge $ \\vee $ \\vee $ \\langle $ \\langle $ \\rangle $ \\rangle $ \\neg $ \\neg $ \\sim $ \\sim $ \\cdot $ \\cdot $ \\times $ \\times $ \\leq $ \\leq $ \\geq $ \\geq $ \\propto $ \\propto $ \\neq $ \\neq $ \\varnothing $ \\0 $ \\infty $ \\infty $ \\clubsuit $ \\clubsuit $ \\diamondsuit $ \\diamondsuit $ \\heartsuit $ \\heartsuit $ \\spadesuit $ \\spadesuit $ \\pm $ \\pm $ \\leftrightarrow $ \\leftrightarrow $ \\uparrow $ \\uparrow $ \\downarrow $ \\downarrow $ \\rightarrow $ \\rightarrow $ \\leftarrow $ \\leftarrow $ \\circ $ \\circ $ \\partial $ \\partial $ \\div $ \\div $ \\aleph $ \\aleph $ \\wp $ \\wp $ \\oslash $ \\oslash $ \\nabla $ \\nabla $ \\ldots $ \\ldots $ \\prime $ \\prime $ \\mid ","id":1191,"permalink":"https://freshrimpsushi.github.io/jp/posts/1191/","tags":null,"title":"MATLABでグラフに使用できる特殊記号一覧"},{"categories":"정수론","contents":"コード R 次に、Rコードで実装されたエラトステネスの篩である。自然数$n$が与えられると、エラトステネスの篩と同じ方法で素数かどうかを判断してくれる。$n$そのものが返されれば素数で、$n$より小さい数が返されれば、それは$n$の約数の中で最小の数を意味する。\neratosthenes\u0026lt;-function(n){\rresidue\u0026lt;-2:n\rwhile(n %in% residue){\rp\u0026lt;-residue[1]\rresidue\u0026lt;-residue[as.logical(residue%%p)]\r}\rreturn(p)\r}\reratosthenes(101)\reratosthenes(1517) 例えば、$101$は素数なので、$101$がそのまま返され、$1517=37 \\times 41$なので、$37$が返される。\nジュリア 以下はもっと効率的に実装されたジュリアコードである。\nfunction factorize(n)\rfactors = []\rwhile n \u0026gt; 1\rfor k in 2:n\rif n % k == 0\rn ÷= k\rpush!(factors, k)\rend\rend\rend\rreturn factors\rend\rfunction eratosthenes(n::Integer)\rif n == 1 return [[1]] end\rif n == 2 return [[1], [2]] end\rprimes = [2]\rfactorized = [[1], [2]]\rfor k ∈ 3:n\rm = k\rfor p ∈ primes\rif m % p == 0\rm ÷= p\rtemp = [p; factorized[m]]\rpush!(factorized, temp)\rbreak\rend\rend\rif length(factorized) != k\rpush!(primes, k)\rpush!(factorized, [k])\rend\rend\rreturn factorized, primes\rend\rF, P = eratosthenes(20)\rF\rP 一緒に見る 素因数分解 素因数分解問題の難しさを利用したセキュリティアルゴリズム RSA公開鍵暗号体系 ゴールドバサー-ミカリ確率鍵暗号体系 素因数分解問題に対する攻撃アルゴリズム ポラードのp-1素因数分解アルゴリズム 準素数の素因数分解問題が容易に解ける条件 ","id":775,"permalink":"https://freshrimpsushi.github.io/jp/posts/775/","tags":null,"title":"素因数分解"},{"categories":"수치해석","contents":"定義 1次微分方程式の存在性・一意性定理のステートメントで、リプシッツ条件Lipschitz Conditionが見られる。\n$D \\subset \\mathbb{R}^2$で定義された連続関数について、初期値問題$\\begin{cases} y ' = f(x,y) \\\\ y( x_{0} ) = Y_{0} \\end{cases}$が与えられている。$f$がすべての$(x,y_{1}) , (x , y_{2} ) \\in D$及び$K \u0026gt; 0$に対してリプシッツ条件 $$ |f(x,y_{1} ) - f(x,y_{2}) | \\le K | y_{1} - y_{2} | $$ を満たすならば、$(x_{0} , Y_{0}) \\in D^{\\circ}$に対して適切な区間$I := [ x_{0} - \\alpha , x_{0} + \\alpha ]$で一意解$Y(x)$が存在する。\n説明 リプシッツ条件を私たちに馴染みのある表現で書くならば、 $$ \\left| { f(x,y_{1} ) - f(x,y_{2}) } \\over { y_{1} - y_{2} } \\right| \\le K $$ と表すことができる。最悪の場合でも、 $$ K = \\max_{(x,y) \\in D} \\left| {{ \\partial f(x,y) } \\over { \\partial y }} \\right| $$ だから、$f$の導関数が有界であることは似たような条件になる。これは、少なくとも初期値$( x_{0} , Y_{0} )$に関しては、関数の値が急激に変化することがないということであり、中には解きやすい問題もあるという意味になる。\nこのような条件は、解の安定性Stabilityという概念を説明するために必要である。仮定で与えられた初期値問題に、少々の変動$\\delta (x)$、$\\epsilon$が追加された $$ \\begin{cases} y ' (x ; \\epsilon) = f(x, Y(x ;\\epsilon ) ) + \\delta (x) \\\\ Y( x_{0} ; \\epsilon ) = Y_{0} + \\epsilon \\end{cases} $$ を考えてみよう。この二つの問題は数学的には完全に異なるものであるが、$| \\delta |$及び$ | \\epsilon |$が十分に小さく、リプシッツ条件も満たされる場合、次のようになる。\n$D \\subset \\mathbb{R}^2$で定義された連続関数について、初期値問題$\\begin{cases} y ' (x ; \\epsilon) = f(x, Y(x ;\\epsilon ) ) + \\delta (x) \\\\ Y( x_{0} ; \\epsilon ) = Y_{0} + \\epsilon \\end{cases}$が与えられている。$f$がリプシッツ条件を満たす場合、$(x_{0} , Y_{0}) \\in D^{\\circ}$に対して適切な区間$I := [ x_{0} - \\alpha , x_{0} + \\alpha ]$及び十分に小さい$\\epsilon_{0} \u0026gt;0$において、$| \\epsilon | \\le \\epsilon_{0}$と$ | \\delta |_{\\infty}$を満たす一意解$Y(x ; \\delta, \\epsilon )$が存在する。\n微分方程式の解法において安定性が重要になるのは、数値的な近似解を気にする時だ。新しいデータが絶えず追加されており、少しの数値の変化でモデル全体を変えなければならなくなるのは厄介だ。リプシッツ条件を満たさない場合はどういう場合か例を見てみよう。初期値問題 $$ \\begin{cases} y ' = 100 y - 101 e^{-x} \\\\ y( 0 ) = 1 \\end{cases} $$ の解は単純に$y = e^{-x}$で求められる。初期値を$y(0) = 1 + \\epsilon$に変えると、その解は$y = e^{-x} + \\epsilon e^{100x}$となるが、$| \\epsilon |$がかなり小さくない限り、誤差が大きすぎる。従って、初期値を変えなかった時に得た元の解は使用が難しく、このような場合には条件が悪いill-conditionedという。逆に、増加する$x$に対して、$\\displaystyle \\int_{x_{0}}^{x} {{ \\partial f (t, Y(t) ) } \\over {\\partial y }} dt$が小さな正数で有界であれば、条件が良いwell-conditionedという。区間$I$でリプシッツ連続な関数の集合を$C^{0,1} ( I )$と表すこともある。\n参照 強いリプシッツ条件 $\\implies$ リプシッツ条件 $\\implies$ 局所リプシッツ条件\n","id":684,"permalink":"https://freshrimpsushi.github.io/jp/posts/684/","tags":null,"title":"リプシッツ条件"},{"categories":"고전역학","contents":"概要 ハミルトンの原理、汎関数、作用、変分などについて、可能な限り簡単に説明しています。他の場所で満足のいく説明を見つけられなかった場合は、最後まで読むことをお勧めします。特に、大学1〜2年生でも十分に読めるように作成しました。\nラグランジュ力学1 物体が時間 $t_{1}$ から $t_{2}$ まで運動するとき、運動経路に対するラグランジアンの積分を作用actionといい、以下のように $J$ で表します。\n$$ \\begin{equation} J=\\int_{t_{1}}^{t_{2}} L dt \\end{equation} $$\nこのとき、可能なすべての運動経路の作用の中で、実際の運動経路の作用が最小になります。ラグランジアンLagrangianは、運動エネルギーとポテンシャルエネルギーの差で定義され、一般に$L$で表されます。\n$$ L = T-V $$\nこの内容は、ハミルトンの原理Hamilton\u0026rsquo;s variational principleまたは最小作用の原理principle of least actionと呼ばれます。最小作用の原理という名前は$(1)$の積分を作用と呼ぶからです。元々最小値と極小値は異なる概念ですが、ここでは同じ意味を持つとします。正確には極値（極大または極小）が適切です。マリオンの教科書を基準にすれば、恐らく1学期、ファウルズの教科書を基準にすれば2学期に学ぶラグランジュ力学の最初の内容です。しかし、教科書に忠実であるだけでは、この内容を理解するのが非常に難しかったです。新しい概念が登場しますが、それが何であるかを親切に説明してくれません。例えば、ファウルズの教科書では以下のような式が登場します。\n$$ \\begin{equation} \\delta J =\\delta \\int_{t_{1}}^{t_{2}} L dt = 0 \\end{equation} $$\nそして、新しく登場した記号$\\delta$についての説明は以下のようです。\n\"$\\delta$は、全体の積分の變分(variation)に対する極値である。\"\nこれを読んで$\\delta$が何を意味するのかどうやって分かるでしょうか。変分が何かもちゃんと教えてくれず、その後の計算はどんどん進みます。等式がなぜ成立するのかも分からないので、一行一行読むスピードも非常に遅く、内容を理解すること自体が非常に困難でした。そこで、ラグランジュ力学を初めて学ぶ学生のために、できるだけ親切に説明しようと思います。まず、ハミルトンの原理を記述する際に使用される用語を整理する必要があります。\n汎関数 多くの資料で$(2)$の積分を汎関数と言いますが、普通に勉強してきた物理学部の学生であれば、汎関数が何であるか知らないのが普通です。皆さんは、実数を入力すると実数（または複素数）が出力されるものを関数として知っているでしょう。\n$$ f(x)=x^2,\\quad g(x)=e^{2x} $$\nしかし、関数の数学的定義を考えると、数字を入力して数字が出力される必要はありません。何かを入力してそれに対応する結果が出力されるものが関数なので、入力するものに制限はありません。このとき、ある関数に関数を入力してそれに応じてある数が出力される場合、その関数を汎関数functionalと言います。例えば、以下のように定義された関数$F$は汎関数です。\n$$ {\\color{blue}F\\big( {\\color{orange}f(x)} \\big)} := {\\color{red}\\int_{1}^{2} f(x) dx} $$\nつまり、関数$F$はある関数を$1$から$2$まで定積分した値を関数値として持ちます。実際に計算してみると、\n$$ {\\color{blue}F( {\\color{orange} e^{x} })} = \\int_{1}^2 e^x dx = {\\color{red}e^2-e},\\quad {\\color{blue}F({\\color{orange}x^2})}=\\int_{1}^2 x^{2} dx = {\\color{red}\\frac{7}{3} } $$\n上記のように、関数を入力したときに実数（または複素数）が出力される関数を汎関数と言います。続く内容ですが、最小作用の原理で作用はまさに汎関数です。\u0026lsquo;各運動経路に対するラグランジアン\u0026rsquo;という関数を入力したときにある値が出るので、汎関数です。汎関数に関する数学的な内容を含む記事がブログにありますが、リンクは紹介しません。おそらく読めばさらに混乱するでしょうから、できれば読まないことをお勧めします。本当に興味があれば、右上の検索バーで汎関数を検索して読んでみてください。よく分からなければ、忘れてしまいましょう。\n作用とラグランジアン 運動エネルギーからポテンシャルエネルギーを引いたものをラグランジアンと呼び、$L$で表します。\n$$ L=T-V $$\nラグランジアンは速度、位置、時間に影響を受けるため、位置を$y$とすると、以下のように表すこともできます。\n$$ L=L(y^{\\prime},\\ y,\\ t) $$\nラグランジアンという名前は、フランスの数学者ジョゼフ・ルイ・ラグランジュの名前から付けられました。ラグランジアンを時間に対して定積分したものを作用、またはアクションと呼び、一般に$J$で表します。\n$$ J = \\int_{t_{1}}^{t_{2}} L dt = \\int_{t_{1}}^{t_{2}} L(y^{\\prime},\\ y,\\ t) dt $$\nハミルトンの原理 1834年、イギリスの数学者ウィリアム・ローアン・ハミルトンが考案したもので、物体が実際に動く経路は作用が最小になるような原理です。これは証明可能な事実ではなく、$F=ma$のように自然界に存在する基本原理の一つと受け入れれば良いです。例えば、私たちが物体を高い場所から投げて落とすとき、物体がどのような経路で地面まで動くか知りたいとします。私たちが予想できる経路は数えきれないほど多いでしょうが、その中で実際に物体が動く経路には何か特別な点があるということです。それは、各経路に対するラグランジアンを時間に対して積分したとき、実際に動く経路に対するラグランジアンの積分値が最も小さいということです。つまり、作用が最小になる経路を見つければ、それが実際に物体が動く経路です。そのため、ハミルトンの原理は最小作用の原理とも呼ばれます。この原理を基に物体の運動を扱うことがラグランジュ力学Lagrangian mechanicsです。驚くべきことに、ラグランジュ力学はニュートン力学とは全く異なって見えますが、同じ結果を与えるということです。つまり、表現方法は異なるものの、本質は同じです。ニュートン力学はベクトル計算に基づいて物体の動きを扱い、ラグランジュ力学はスカラー（エネルギー）の計算によって力学を記述します。\n変分 簡単に言うと、上で詳しく説明した内容を数学的に整理したものです。まず、簡単な例として2次関数の最小値を見つける問題を考えてみましょう。\n上の図のような2次関数が与えられたとします。関数値の最小値は$1$で、関数値が最小になる場所は$x=3$です。最小値（極小値）を持つ点では傾きが$0$なので、微分したとき$0$であることが分かります。したがって、\n$$ \\dfrac{dy}{dx} \\bigg|_{x=3}=0 $$\nです。この内容を最小作用の原理にそのまま適用することになります。\n上の図に示されているように、物体が実際に運動する経路を$y(0, t)$としましょう。物体が運動できる任意の経路を上の図のように$y(\\alpha, t)=y(0,t)+\\alpha \\eta (t)$としましょう。参考までに$\\eta$はギリシャ文字のエタです。$\\alpha \\eta (t)$は実際の経路と比較したときの誤差と考えれば良いです。図と数式を見れば分かるように、誤差がないとき、つまり$\\alpha=0$のとき、可能な任意の経路$y(\\alpha, t)$は実際の経路になります。また、最小作用の原理は、可能なすべての経路に対する作用の中で、実際の経路に対する作用が最小の値であるという内容です。両方の内容を組み合わせて、上で挙げた例を適用すれば、作用を微分して$\\alpha=0$を代入したとき、その値が$0$であるという結果を得ます。\n$$ \\dfrac{\\partial J}{\\partial \\alpha}=\\dfrac{\\partial }{\\partial \\alpha} \\int_{t_{1}}^{t_{2}} L\\big( y^{\\prime}(\\alpha,t),\\ y(\\alpha,t),\\ t \\big) dt =0 $$\nこれを簡単に表記すると、以下のようになり、$\\delta J$を$J$の変分と呼びます。\n$$ \\delta J = 0 $$\nつまり、$\\delta=\\dfrac{\\partial }{\\partial \\alpha}$と理解すれば良いです。したがって、以下のような等式が成立します。\n$$ \\delta \\dot{y}=\\dfrac{\\partial }{\\partial \\alpha}\\frac{dy}{dt}=\\dfrac{d}{dt}\\frac{\\partial y}{\\partial \\alpha}=\\dfrac{d}{dt}\\delta y $$\n関連項目 オイラー-ラグランジュ方程式 Grant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p417-420\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1182,"permalink":"https://freshrimpsushi.github.io/jp/posts/1182/","tags":null,"title":"ラグランジュ力学とハミルトンの変分原理"},{"categories":"거리공간","contents":"定義 距離空間 $\\left( X, d \\right)$ について、$A \\subset X$ とする。\n$x \\in O \\subset A$ を満たす開集合 $O$ が存在する時、$x$ を $A$ の内点という。\n$A$ の内点の集合 $A^{\\circ}$ を $A$ の内部という。\n$A$ とその値域の和集合 $\\overline{A} : = A \\cup a '$ を $A$ の閉包という。\n$x \\in \\overline{A}$ であり、かつ $x \\in \\overline{X \\setminus A}$ の時、$x$ を $A$ の境界点という。\n$\\partial A : = \\overline{A} \\cap \\overline{X \\setminus A}$ を $A$ の境界という。\n説明 定義する必要はないかもしれないが、インテリアと対照的な $\\overline{A}$ の外側の集合をエクステリアと呼ぶ。\n開集合とこれらの概念は異なり定義されることもあるが、本質的には同じである。\n定義は慎重に読めば誰でも理解できるものであり、図を通じて早く理解しよう。\n$$ A $$\n与えられた集合が上のような時、これらの概念を考えてみよう。\n$$ A^{\\circ} $$\nインテリアは $A$ が含む $X$ の部分集合の中で最も大きな開集合である。\n$$ \\overline{A} $$\nクロージャーは $A$ を含む $X$ の部分集合の中で最も小さな閉集合である。\n$$ \\partial A $$\nバウンダリーはクロージャーからインテリアを引いた $X$ の部分集合と見ることができる。\nインテリアとクロージャーの区別はさほど難しくないが、バウンダリーは一見すると点線か実線かによって混乱するかもしれない。境界であれば、迷わずバウンダリーと考えればいい。\nこのような定義を通じて、以下の性質は事実上、開集合と閉集合の定義と見ることができる。\n性質: 開集合と閉集合 $A$ が距離空間 $X$ の部分集合とする。\n$A$ が開集合であることと $A = A^{\\circ}$ は等価である。\n$A$ が閉集合であることと $A = \\overline{A}$ は等価である。\nもちろん、これらの性質は証明可能だが、ただの事実として受け入れても問題ない。\n","id":383,"permalink":"https://freshrimpsushi.github.io/jp/posts/383/","tags":null,"title":"距離空間における内部閉包境界"},{"categories":"편미분방정식","contents":"ビルドアップ1 ハミルトニアン$H$が$Du$のみに依存するハミルトン-ヤコビ方程式の初期値問題を見てみよう。\n$$ \\begin{equation} \\left\\{ \\begin{aligned} u_{t} + H(Du)\u0026amp;=0 \u0026amp;\u0026amp; \\text{in } \\mathbb{R}^n \\times (0,\\infty) \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\mathbb{R}^n \\times \\left\\{ t=0 \\right\\} \\end{aligned} \\right. \\label{eq1} \\end{equation} $$\n一般に、ハミルトニアンは空間変数に依存して$H(Du, x)$のような形を取るが、ここでは$x$に対して影響を受けないとする。そして、ハミルトニアン$H\\in C^\\infty$に対して次のような仮定をする。\n$$ \\begin{cases} H \\mathrm{\\ is\\ convex} \\\\ \\lim \\limits_{|p|\\to \\infty} \\dfrac{H(p)}{|p|}=\\infty \\end{cases} $$\nそして$L=H^{\\ast}$とすると、ラグランジアン$L$も同様の特性を満たす。最後に、初期値$g : \\mathbb{R}^n \\to \\mathbb{R}$がリプシッツ連続であるとする。すなわち、\n$$ \\mathrm{Lip}(g):=\\sup \\limits_{x,y\\in \\mathbb{R}^n \\\\ x \\ne y} \\dfrac{ |g(x)-g(y)| }{|x-y|} \u0026lt; \\infty $$\nまた、与えられたハミルトン-ヤコビ方程式$\\eqref{eq1}$の特性方程式は次のようになる。\n$$ \\begin{align*} \\dot{\\mathbf{p}}(s) \u0026amp;= -D_{x}H \\big( \\mathbf{p}(s), \\mathbf{x}(s) \\big) \\\\ \\dot{z}(s) \u0026amp;= D_{p} H\\big( \\mathbf{p}(s),\\ \\mathbf{x}(s)\\big)\\cdot \\mathbf{p}(s) -H\\big( \\mathbf{p}(s), \\mathbf{x}(s)\\big) \\\\ \\dot{\\mathbf{x}}(s) \u0026amp;= D_{p}H\\big( \\mathbf{p}(s), \\mathbf{x}(s) \\big) \\end{align*} $$\nここで$H$は$x$に無関係であると仮定したので、再び書くと次のようになる。\n$$ \\begin{align*} \\dot{\\mathbf{p}} \u0026amp;= 0 \\\\ \\dot{z} \u0026amp;= D H( \\mathbf{p} )\\cdot \\mathbf{p} -H ( \\mathbf{p} ) \\\\ \\dot{\\mathbf{x}} \u0026amp;= DH ( \\mathbf{p}) \\end{align*} $$\nこのとき$t(s)=s, p(s)=Du(x(s), s), z(s)=u(x(s), s)$である。$p$に対する微分と$x$に対する微分を区別する必要がないため、$D$の下付き文字を省略した。オイラー-ラグランジュ方程式は固定された開始点と終了点に対して成立するため、与えられたハミルトン-ヤコビ方程式$\\eqref{eq1}$の解が存在する場合、以下のようなlocal in time solutionである。\n$$ u= u(x,t) \\in C^2\\big( \\mathbb{R}^n \\times [0,T]\\big) $$\n上記の特性方程式では、第一式と第三式はラグランジアン$L=H*$によって定義される作用の最小化問題から導かれるオイラー-ラグランジュ方程式を満たすハミルトン方程式である。\n$H$と$L$が$p$, $v\\in \\mathbb{R}^n$で微分可能であれば、以下の内容はすべて同等である。\n$$ \\begin{cases} p\\cdot v=L(v) + H(p) \\\\ p=DL(v) \\\\ v=DH(p) \\end{cases} $$\nこのとき$p=D_{v}L(v)$で定義されるため、上記の補題を使用すると次を得る。\n$$ \\begin{align*} \\dot{z}(s) \u0026amp;= DH(\\mathbf{p})\\cdot \\mathbf{p}-H(\\mathbf{p}) \\\\ \u0026amp;= \\mathbf{v} \\cdot \\mathbf{p}-H(\\mathbf{p}) \\\\ \u0026amp;= L(\\mathbf{v})+H(\\mathbf{p})-H(\\mathbf{p}) \\\\ \u0026amp;= L(\\mathbf{v}) = L\\big(\\dot{\\mathbf{x}}(s)\\big) \\end{align*} $$\nしたがって$z(t)$を求めると、次のようになる。\n$$ \\begin{align*} z(t) \u0026amp;= \\int_{0}^t \\dot{z}(s)dx +z(0) \\\\ \u0026amp;= \\int_{0}^tL \\big( \\dot{\\mathbf{x}}(s) \\big) + u\\big( \\mathbf{x}(0),\\ 0\\big) \\\\ \u0026amp;= \\int_{0}^t L\\big( \\dot{\\mathbf{x}}(s)\\big) ds +g\\big(\\mathbf{x}(0) \\big) \\end{align*} $$\nしかし、このとき上記の条件では$z(t)=u(x(t), t)$であったので、次を得る。\n$$ u(x,t)=\\int_{0}^t L\\big( \\dot{\\mathbf{x}}(s)\\big) ds +g\\big(\\mathbf{x}(0) \\big) \\quad (0 \\le t \u0026lt;T) $$\nこれはlocal in time smooth solutionであるため、global in time weak solutionを求めることができるかという問題が残る。再び作用の最小化問題に戻るが、オイラー-ラグランジュ方程式を導いた際と異なる点は終点のみを固定することである。\n固定された$x \\in \\mathbb{R}^n, t\u0026gt;0$が与えられたとする。そして、許容クラス$\\mathcal{A}$を次のようにする。\n$$ \\mathcal{A}=\\left\\{ \\mathbf{w}\\in C^1\\big( [0,t];\\mathbb{R}^n \\big)\\ :\\ \\mathbf{w}(t)=x \\right\\} $$\nそして、以下のような作用に対する最小化問題を考えてみよう。\n$$ \\mathbf{w}(\\cdot) \\in \\mathcal{A} \\mapsto \\int_{0}^t L\\big( \\dot{\\mathbf{w}}(s)\\big) ds + g(\\mathbf{w}(0)) $$\nもし上記の作用のミニマイザー$\\mathbf{x}(\\cdot)$が存在するならば、$\\mathbf{p}(s):=DL(\\dot{\\mathbf{x}}(s))$であり、オイラー-ラグランジュ方程式を満たし、したがってハミルトン方程式も満たす。したがって、上記のlocal in time solutionの場合と同様に、解は以下のように与えられるだろう。\n$$ u(x,t)=\\int_{0}^tL\\big( \\dot{\\mathbf{x}}(s)\\big)ds +g \\big( \\mathbf{x}(0) \\big) $$\n上記の内容をモチーフに、global in time weak solutionが存在する場合、次のように定義できる。\n$$ \\begin{equation} u(x,t):=\\inf \\limits_{\\mathbf{w} \\in \\mathcal{A}} \\left\\{ \\int_{0}^t L\\big( \\dot{\\mathbf{w}}(s) \\big)ds + g\\big( \\mathbf{w}(0) \\big) \\right\\} \\label{eq2} \\end{equation} $$\n定理 $x \\in \\mathbb{R}^n$であり、$t\u0026gt;0$とする。それならば、$\\eqref{eq2}$の最小化問題の解は次のように与えられる。\n$$ u(x,t) = \\min \\limits_{y \\in \\mathbb{R}^n} \\left\\{ tL\\left( \\dfrac{x-y}{t} \\right) +g(y) \\right\\} $$\nこれをホップ-ラックス公式Hopf-Lax formulaと呼ぶ。\n証明 まず$\\inf$に対して成立することを示し、その次に実際に$\\min$になることを示す順序で証明する。\nステップ 1.\n固定された任意の$y \\in \\mathbb{R}^n, t\\in \\mathbb{R}$がある。そして、$\\mathbf{w}$を次のように定義しよう。\n$$ \\mathbf{w}(s) :=y+\\frac{s}{t}(x-y) \\quad (0 \\le s \\le t) $$\nすると$\\mathbf{w}(0)=y$であり、$\\mathbf{w}(t)=x$である。それならば$\\mathbf{w}$は許容クラス$\\mathcal{A}$の要素である。\n$$ \\mathcal{A}= \\left\\{ \\mathbf{w}(\\cdot) \\ \\big| \\ \\mathbf{w}(0)=y,\\ \\mathbf{w}(t)=x\\right\\} $$\nそれならば～の定義により、次の不等式が成立する。\n$$ \\begin{align*} u(x,t) \u0026amp; \\le\u0026amp; \\int_{0}^t L \\left( \\frac{x-y}{t}\\right)ds + g(y) \\\\ \u0026amp;= tL\\left( \\frac{x-y}{t}\\right)+g(y) \\end{align*} $$\nこの不等式はすべての$y \\in \\mathbb{R}^n$に対して成立するので、次を得る。\n$$ u(x,t) \\le \\inf \\limits_{y \\in \\mathbb{R}^n} \\left(t L\\left(\\frac{x-y}{t} \\right) +g(y)\\right) $$\nステップ 2.\n▷eq41\n◁としよう。それならば$\\mathbf{w}(\\cdot) \\in C^1([0;t];\\mathbb{R}^n)$であり、$\\mathbf{w}(t)=x$である。\nイェンセンの不等式\n関数$f$が凸関数であると仮定しよう。それならば、以下の式が成立する。 $$ f \\left( -\\!\\!\\!\\!\\!\\! \\int_{U} u dx \\right) \\le -\\!\\!\\!\\!\\!\\! \\int_{U} f(u) dx $$\nそれならば、上記の補題により、次が成立する。\n$$ L \\left( \\frac{1}{t}\\int_{0}^t \\dot{\\mathbf{w}}(s) dx\\right) \\le \\dfrac{1}{t}\\int_{0}^t L \\big( \\dot{\\mathbf{w}(s)} \\big)ds $$\nそして、開始点を$y$としよう$\\mathbf{w}(0)=y$。それならば、上記の不等式は以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; L\\left( \\dfrac{1}{t} \\big( \\mathbf{w}(t)-\\mathbf{w}(0) \\big) \\right) \u0026amp;\\le \\dfrac{1}{t}\\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds\n\\\\ \\implies\u0026amp;\u0026amp; L\\left( \\dfrac{x-y}{t} \\right) \u0026amp;\\le \\dfrac{1}{t}\\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds \\end{align*} $$\n両辺に$t$を掛けて$g(y)$を足すと、次のようになる。\n$$ tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\le \\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds + g(y) $$\n右辺の$\\inf$が$u(x,t)$であるので、次のようになる。\n$$ tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\le u(x,t) $$\n最後に、両辺に$\\inf \\limits_{y\\in \\mathbb{R}^n}$を取ると、次を得る。\n$$ \\inf \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\right) \\le u(x,t) $$\nしたがって、ステップ 1. と ステップ 2. により、次が成立する。\n$$ \\begin{equation} u(x,t) = \\inf \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\right) \\label{eq3} \\end{equation} $$\nステップ 3.\n$\\left\\{y_{k} \\right\\}_{k=1}^\\infty$を$\\eqref{eq3}$の最小化シーケンスminimizing sequenceとしよう。それならば、次が成立する。\n$$ \\begin{equation} tL\\left( \\dfrac{x-y_{k}}{t} \\right) + g(y_{k}) \\to u(x,t)\\in [-\\infty, \\infty) \\quad \\mathrm{as}\\ k\\to \\infty \\label{eq4} \\end{equation} $$\nまず、$\\left\\{y_{k} \\right\\}$が有界でないと仮定しよう。これが矛盾であることを確認して、$\\left\\{ y_{k} \\right\\}$が有界であることを示す。仮定により、$|y_{k}| \\to \\infty$であり、$y_{k}=0$の$k$は多くても有限個である。したがって、$y_{k}\\ne 0$を満たすものだけを集めた部分列を再び$\\left\\{ y_{k} \\right\\}$としよう。次が成立する。\n$$ \\left| \\dfrac{x-y_{k}}{t} \\right| \\to \\infty $$\nそれならば、ラグランジアン$L$の性質により、次が成立する。\n$$ a_{k}:= \\dfrac{L\\left( \\dfrac{x-y_{k}}{t}\\right)}{\\left| \\dfrac{x-y_{k}}{t}\\right|} \\to \\infty $$\nしたがって、$L\\left( \\dfrac{x-y_{l}}{t}\\right) \\to \\infty$であり、ここに定数を掛けても同じ結果を得る。\n$$ \\begin{equation} tL\\left( \\dfrac{x-y_{k}}{t}\\right) \\to \\infty \\label{eq5} \\end{equation} $$\n$g$のリプシッツ条件を再び書くと、次のようになる。\n$$ \\dfrac{|g(x)-g(y_{k})|}{|x-y_{k}|} \\le \\mathrm{Lip}(g)=C \\quad \\forall \\ k \\in \\mathbb{N} $$\nしたがって、次を得る。\n$$ g(x) -g(y_{k}) \\le C|x-y_{k}| $$\n両辺に$\\eqref{eq5}$を足すと、次のようになる。\n$$ tL\\left( \\dfrac{x-y_{k}}{t}\\right)+ g(x) -g(y_{k}) \\le C|x-y_{k}|+ tL\\left( \\dfrac{x-y_{k}}{t}\\right) \\quad \\mathrm{for\\ large}\\ k $$\n上記の式を適切に移項すると、以下のようになる。\n$$ tL\\left( \\dfrac{x-y_{k}}{t}\\right)-C|x-y_{k}| + g(x) \\le tL\\left( \\dfrac{x-y_{k}}{t}\\right) + g(y_{k}) $$\n再び書くと、次のようになる。\n$$ a_{k}|x-y_{k}| -C|x-y_{k}| + g(x) =|x-y_{k}|(a_{k}-C)+g(x) \\le tL\\left( \\dfrac{x-y_{k}}{t}\\right) + g(y_{k}) $$\n$a_{k}\\to \\infty$であり、$|x-y_{k}| \\to \\infty$であるため、左辺が$\\infty$に発散し、右辺も発散する。したがって、$u(x,t)$の定義により、$u(x,t)\\to \\infty$である。これは$\\eqref{eq4}$に矛盾するため、$\\left\\{ y_{k} \\right\\}$は有界である。\n$\\left\\{ y_{k} \\right\\}$が有界であるため、$y_{k} \\to y_{0}$と仮定しよう。すると、次が成立する。\n$$ tL \\left( \\dfrac{x-y_{k}}{t} \\right)+g(y_{k}) \\to tL \\left( \\dfrac{x-y_{0}}{t}\\right)+g(y_{0}) =\\min\\limits_{y \\in \\mathbb{R}^n}\\left( tL \\left( \\dfrac{x-y}{t}\\right)+g(y) \\right) $$\nそれならば、$\\eqref{eq4}$により、次が成立する。\n$$ tL\\left( \\dfrac{x-y_{k}}{t} \\right) + g(y_{k}) \\to u(x,t)\\in [-\\infty, \\infty) \\quad \\mathrm{as}\\ k\\to \\infty $$\nしたがって、次を得る。\n$$ u(x,t) = \\min \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) +g(y) \\right) $$\n■\nローレンス・C・エヴァンス, 偏微分方程式 (第2版, 2010年), p122-124\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1174,"permalink":"https://freshrimpsushi.github.io/jp/posts/1174/","tags":null,"title":"ホップ・ラックス・フォーミュラ"},{"categories":"해석개론","contents":"定義 $\\mathbb{R}$ の部分集合 $E \\ne \\emptyset$、関数 $f : E \\to \\mathbb{R}$ および関数列 $\\left\\{ f_{n} : E \\to \\mathbb{R} \\right\\}_{n=1}^{\\infty}$ を定義しよう。全ての $\\varepsilon \u0026gt; 0$ に対し、$n \\ge N \\implies | f_{n} (x) - f(x) | \u0026lt; \\varepsilon$ を満たす $N \\in \\mathbb{N}$ が存在するならば、$E$ において $f_{n}$ は $f$ に一様収束uniformly convergenceすると言い、以下のように示される。\n$$ f_n \\rightrightarrows f $$\nまたは\n$$ f_{n} \\overset{\\text{unif}}{\\to} f $$\nまたは\n$$ f_{n} \\to f \\quad \\text{uniformly} $$\n説明 関数列 $f_{n}$ が実際に 関数 $f$ に収束するかまで気にする一様収束は、関数値が収束することだけを気にする点収束と違う概念である。一様収束する関数列はより強い条件がついて、それだけ多くの性質を持つ。\n逆に言えば、数学者たちが研究するために最低限「これくらいはあるべきだ」と考える常識的な性質を持たせるために強い条件を与えたのが一様収束である。点収束する関数列と違い、一様収束する関数列では次のように $f_{n}$ の性質が $f$ まで保持される。\n定理 $E$ において $f_{n}$ が $f$ に一様収束するとしよう。\n(a) 連続性: $f_{n}$ が $x_{0} \\in E$ で連続なら、$f$ も $x_{0} \\in E$ で連続である。\n(b) 微分可能性: $f_{n}$ が $E = (a,b)$ で微分可能であり、$f_{n} ' $ が $E$ で一様収束するなら、$f$ も $E$ で微分可能で、\n$$ \\lim_{n \\to \\infty} {{ d } \\over { dx }} f_{n} (x) = {{ d } \\over { dx }} \\left( \\lim_{n \\to \\infty} f_{n} (x) \\right) $$\n(c) 積分可能性: $f_{n}$ が $E = [a,b]$ で積分可能なら、$f$ も $E$ で積分可能で、\n$$ \\lim_{n \\to \\infty} \\int_{a}^{b} f_{n} (x) dx = \\int_{a}^{b} \\left( \\lim_{n \\to \\infty} f_{n} (x) \\right) dx $$\n$\\int_{a}^{b}$ と $\\displaystyle {{ d } \\over { dx }}$ が $\\displaystyle \\lim_{n \\to \\infty}$ を自由に移動できることは非常に望ましい性質だ。なぜそれが良いのかと問われれば、その問い自体が答えであるようなものだ。数学以外の分野では、関数列が現れても一様収束のような概念を考慮せず当たり前のように一様数列の性質を使うケースが結構あるが、もし一様収束性がなくなってそのような操作を使えなくなると、地獄のような状況が展開されるだろう。\n参照 関数列の点収束と一様収束の違い ","id":1154,"permalink":"https://freshrimpsushi.github.io/jp/posts/1154/","tags":null,"title":"関数列の一様収束"},{"categories":"해석개론","contents":"定義 $\\mathbb{R}$ の部分集合 $E \\ne \\emptyset$ に対して関数 $f : E \\to \\mathbb{R}$ を定義しよう。関数列 $\\left\\{ f_{n} : E \\to \\mathbb{R} \\right\\}_{n=1}^{\\infty}$ が各 $x \\in E$ について $f(x) = \\lim \\limits_{n \\to \\infty} f_{n} (X)$ を満たす場合、$E$ で $f_{n}$ に 逐点収束pointwise convergenceすると言い、以下のように表記される。\n$$ f_{n} \\to f $$\n解説 上の定義を ε-δ 論法で書き直すと、次の必要十分条件が得られる。\nすべての $\\varepsilon \u003e 0$ と $x \\in E$ に対して $n \\ge N \\implies | f_{n} (x) - f(x) | \u003c \\varepsilon$ を満たす $N \\in \\mathbb{N}$ が存在する。\r数列は単に「定義域が $\\mathbb{N}$ である関数」に過ぎないので、その値域が関数の集合であっても全く問題なく、関数列 $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ のような恐ろしい存在を思い浮かべることができる。まだ数列の概念を「$n$ が増加するにつれて数直線上で動く点」と大雑把に考えているなら、受け入れがたいだろう。\n新しい数列が現れたことで、新しい収束についても話さないわけにはいかない。もちろん、逐点で収束するという概念は実際そんなに難しくない。なぜなら、$E$ で一点以上の例外を許して収束するなら、それは$E$ での収束とは言えないからだ。しかし、このように常識的な「収束」をわざわざ「逐点収束」と呼ぶ理由は何だろう？\nその理由は明らかに、逐点収束が関数それ自体の収束を議論するにはまだ不十分であるためだ。実際、逐点収束はこれよりも良い収束と対比して「十分に良くない収束」を言うためにある言葉とも言える。率直に言って、$f_{n} (x)$ というのも具体的な $x_{0}$ を一つ固定すれば $a_{n} := f_{n} (x_{0} )$ のように現れるので、わざわざ関数列という概念を考える必要もない。\n次は、$E$ で $f_{n}$ が $f$ に逐点収束したとき、元の $f_{n}$ の性質を保持しない例である。\n定理 $E$ で $f_{n}$ が $f$ に逐点ごとに収束するとする。\n(a) $f_{n}$ が微分可能であっても、$f$ が微分可能であるわけではない。\n(b) $f_{n}$ が積分可能であっても、$f$ が積分可能であるわけではない。\n(c) $f_{n}, f$ が微分可能であっても、$\\lim \\limits_{n \\to \\infty} \\dfrac{d}{dx} f_{n} (x) = \\dfrac{d}{dx} \\left( \\lim \\limits_{n \\to \\infty} f_{n} (x) \\right)$ が成立するわけではない。\n(d) $f_{n}, f$ が積分可能であっても、$\\displaystyle \\lim \\limits_{n \\to \\infty} \\int_{a}^{b} f_{n} (x) dx = \\int_{a}^{b} \\left( \\lim \\limits_{n \\to \\infty} f_{n} (x) \\right) dx$ が成立するわけではない。\n特に (a) は連続性も保持されない例でもある。\n証明 反例(a) $E = [0,1]$ で以下のように $f_{n} , f$ を定義しよう。\n$$ \\begin{align*} f_{n} (x) \u0026amp;:= x^{n} \\\\ f(x) \u0026amp;:= \\begin{cases} 0 \u0026amp;, 0 \\le x \u0026lt; 1 \\\\ 1 \u0026amp;, x=1 \\end{cases} \\end{align*} $$\n明らかに$E$ で逐点ごとに $f_{n} \\to f$ に収束する。しかし、$f_{n}$ は $[0,1]$ で微分可能だが、$f$ は $x=1$ で連続ではないので微分可能ではない。\n■\n反例(b) $E = [0,1]$ で以下のように $f_{n} , f$ を定義しよう。\n$$ \\begin{align*} f_{n} (x) \u0026amp;:= \\begin{cases} 1 \u0026amp;, x = {{ p } \\over { m }} , p \\in \\mathbb{Z} , m \\in \\left\\{ 1 , \\cdots , n \\right\\} \\\\ 0 \u0026amp;, \\text{otherwise} \\end{cases} \\\\ f(x) \u0026amp;:= \\begin{cases} 1 \u0026amp;, x \\in \\mathbb{Q} \\\\ 0 \u0026amp;, \\text{otherwise} \\end{cases} \\end{align*} $$\n$f_{n}$ の設定は少し複雑だが、$f_{1} (x)$ は $ x \\in \\left\\{ 0 , 1 \\right\\}$ でのみ$1$ であり、$f_{2} (x)$ は $\\displaystyle x \\in \\left\\{ 0 , {{ 1 } \\over { 2 }} , 1 \\right\\}$ でのみ$1$ であり、$f_{3} (x)$ は$x \\in \\left\\{ 0 , {{ 1 } \\over { 3 }} , {{ 1 } \\over { 2 }} , {{ 2 } \\over { 3 }} , 1 \\right\\}$ でのみ$1$ である。この方法で$n$ を増やしていくと、最終的にはすべての$x \\in \\mathbb{Q}$ でのみ$1$ になるはずで、それ故に$E$ で逐点ごとに$f_{n} \\to f$ に収束することがわかる。しかし、$f_{n}$ は $[0,1]$ で積分可能だが、ディリクレ関数の$f$ は積分可能ではない。\n■\n反例(c) $E = [0,1]$ で以下のように $f_{n} , f$ を定義しよう。\n$$ \\begin{align*} f_{n} (x) \u0026amp;:= {{ x^{n} } \\over { n }} \\\\ f(x) \u0026amp;:= 0 \\end{align*} $$\n明らかに$E$ で逐点ごとに$f_{n} \\to f$ に収束し、それぞれの導関数は\n$$ \\begin{align*} f\u0026rsquo;_{n} (x) =\u0026amp; x^{n-1} \\\\ f '(x) =\u0026amp; 0 \\end{align*} $$\nのように求められる。しかし$x=1$ で\n$$ 1 = \\lim \\limits_{n \\to \\infty} \\dfrac{d}{dx} f_{n} (1) \\ne \\dfrac{d}{dx} \\left( \\lim \\limits_{n \\to \\infty} f_{n} (1) \\right) = 0 $$\n■\n反例(d) $E = [0,1]$ で以下のように $f_{n} , f$ を定義しよう。\n$$ \\begin{align*} f_{1} (x) \u0026amp;:= 1 \\\\ f_{n} (x) \u0026amp;:= \\begin{cases} n^2 x \u0026amp;, 0 \\le x \u0026lt; {{ 1 } \\over { n }} \\\\ 2n - n^2 x \u0026amp;, {{ 1 } \\over { n }} \\le x \u0026lt; {{ 2 } \\over { n }} \\\\ 0 \u0026amp;, {{ 2 } \\over { n }} \\le x \\le 1 \\end{cases} \\\\ f(x) \u0026amp;:= 0 \\end{align*} $$\n$f_{n}$ は複雑に見えるが、上の図を見ると非常にシンプルであり、$E$ で逐点ごとに $f_{n} \\to f$ に収束していることが分かる。ここで、$\\displaystyle \\int_{0}^{1} f_{n} (x) dx$ は三角形の内部の面積と同じで、高さが$n$、底辺の長さが${{ 2 } \\over { n }}$ であるため、$n$ が何であれ常に$1$ に等しい。しかし、\n$$ \\int_{0}^{1} f(x) dx = \\int_{0}^{1} 0 dx = 0 $$\nなので、\n$$ 1 = \\lim \\limits_{n \\to \\infty} \\int_{0}^{1} f_{n} (x) dx \\ne \\int_{0}^{1} \\left( \\lim \\limits_{n \\to \\infty} f_{n} (x) \\right) dx = 0 $$\n■\n参照 関数列の逐点収束と一様収束の違い ","id":1148,"permalink":"https://freshrimpsushi.github.io/jp/posts/1148/","tags":null,"title":"関数列の各点収束"},{"categories":"매트랩","contents":"方法 MatlabにはExcelのデータを読み込む機能がある。まずはホームメニューからデータの取り込みをクリックする。\n読み込みたいデータが保存されているExcelファイルを選択する。\nそうすると、読み込むデータを選択できる。最初は自動的に選択されている。確認して、「選択した項目を取り込む」を押せばいい。\n「選択した項目を取り込む」で「データを取り込む」をクリックする。\nすると、ExcelファイルのデータがExcelファイルと同じ名前を持った変数に入力される。しかし、これは配列ではなくテーブルなので、table2array()を使用して配列に変換してから使う必要がある。読み込んだExcelファイルのタイトルがXの時の例コードと実行結果は下記のとおりだ。\nX1=table2array(X) 関連項目 Matlabで計算したデータをExcelファイルに保存する方法 ","id":1163,"permalink":"https://freshrimpsushi.github.io/jp/posts/1163/","tags":null,"title":"MATLABでExcelのデータをインポートする方法"},{"categories":"매트랩","contents":"方法 MATLABで計算したデータをExcelに整理したい時、データ量が多くない場合は、直接コピー＆ペーストすることが可能だ。しかし、上の写真のように128*128行列のデータでは、その方法では無理だ。この時は、xlswriteを使ってデータをExcelファイルに保存すればいい。\n上の写真と比較して、最後の行にxlswrite('test', Y)が追加された。Yのデータがtestという名前のExcelファイルに作成される。\n該当フォルダにExcelファイルが作成された。開いてみると、128*128行列の値が自動的に整理されている。\n参考 MATLABでExcelのデータを読み込む方法 ","id":1150,"permalink":"https://freshrimpsushi.github.io/jp/posts/1150/","tags":null,"title":"MATLABで計算したデータをExcelファイルに保存する方法"},{"categories":"매트랩","contents":"方法 コメントアウトしたい部分をドラッグして選択した後、Ctrl+Rを入力すれば、ドラッグした部分全体をコメントアウトできる。元に戻したいときは同じようにドラッグして選択し、Ctrl+Tを入力すると、各行の%が消える。\n","id":1149,"permalink":"https://freshrimpsushi.github.io/jp/posts/1149/","tags":null,"title":"MATLABで一度に複数行のコメントとコメント解除をする方法"},{"categories":"측도론","contents":"定義1 $(X, \\mathcal{E})$を可測空間としよう。集合$S_{f}(\\alpha)$を次のように定義する。\n$$ S_{f}(\\alpha):=\\left\\{ x\\in X\\ |\\ f(x) \u0026gt;\\alpha \\right\\} = f^{-1}\\left( (\\alpha, \\infty) \\right),\\quad \\forall \\alpha \\in \\mathbb{R} $$\n全ての実数$\\alpha \\in \\mathbb{R}$に対して、$S_{f}(\\alpha) \\in \\mathcal{E}$が成立するならば、拡張実数値を取る関数$f : X \\to \\overline{\\mathbb{R}}$を**$\\mathcal{E}$-可測**$\\mathcal{E}$-measurableまたは単に可測measurableという。\n説明 特に$X=\\mathbb{R}$の時は、ルベーグ可測という。関数が可測かどうかを判断する時、上の定義に合っているかを確認することになるが、その時に役立つ定理がある。\n定理 関数$f : X \\to \\overline{\\mathbb{R}}$について、以下の四つの条件は互いに同値である。\n(a) 全ての$\\alpha \\in \\mathbb{R}$に対して、$A_{\\alpha} = S_{f}(\\alpha) =\\left\\{ x\\in X : f(x) \u0026gt; \\alpha \\right\\}$ $\\in$ $\\mathcal{E}$である。 (b) 全ての$\\alpha \\in \\mathbb{R}$に対して、$B_{\\alpha}=\\left\\{ x\\in X : f(x) \\le \\alpha \\right\\}$ $\\in$ $\\mathcal{E}$である。 (c) 全ての$\\alpha \\in \\mathbb{R}$に対して、$C_{\\alpha}=\\left\\{ x\\in X : f(x) \\ge \\alpha \\right\\}$ $\\in$ $\\mathcal{E}$である。 (d) 全ての$\\alpha \\in \\mathbb{R}$に対して、$D_{\\alpha}=\\left\\{ x\\in X : f(x) \u0026lt; \\alpha \\right\\}$ $\\in$ $\\mathcal{E}$である。 証明 最初に、$A_{\\alpha}$と$B_{\\alpha}$は互いに補集合なので、σ-代数の性質**(D2)によって、(a)と(b)は同値である。同様に、(c)と(d)も同値である。したがって、(a)と(c)**が同値であることを示せば証明完了である。\n$\\sigma$-代数\n集合$X$が与えられたとする。以下の条件を満たす$X$の部分集合たちのコレクション $\\mathcal{E} \\subset \\mathcal{P}(X)$を**$\\sigma$-代数**という。\n(D1) $\\varnothing, X \\in \\mathcal{E}$ (D2) $E \\in \\mathcal{E} \\implies E^c \\in \\mathcal{E}$ (D3) $E_{k} \\in \\mathcal{E}\\ (\\forall k \\in \\mathbb{N}) \\implies \\bigcup_{k=1}^\\infty E_{k} \\in \\mathcal{E}$ (D4) $E_{k} \\in \\mathcal{E}\\ (\\forall\\ k \\in \\mathbb{N}) \\implies \\bigcap_{k=1}^\\infty E_{k} \\in \\mathcal{E}$ (a) $\\implies$　(c) 条件**（a)が成立すると仮定しよう。すると全ての$n\\in \\mathbb{N}$に対して$A_{\\alpha-\\frac{1}{n}}\\in\\mathcal{E}$が成立する。そして$C_{\\alpha}=\\bigcap_{n=1}^\\infty A_{\\alpha-\\frac{1}{n}}$である。従って、$\\sigma$-代数の定義（D3)**により$C_{\\alpha} \\in \\mathcal{E}$である。\n(c) $\\implies$　(a) 条件**（c)が成立すると仮定しよう。すると全ての$n\\in \\mathbb{N}$に対して$C_{\\alpha+\\frac{1}{n}}\\in\\mathcal{E}$が成立する。そして$A_{\\alpha}=\\bigcup_{n=1}^\\infty C_{\\alpha+\\frac{1}{n}}$である。従って、$\\sigma$-代数の定義（D3)**により$A_{\\alpha} \\in \\mathcal{E}$である。\n■\nRobert G. Bartle, The Elements of Integration and Lebesgue Measure (1995), p8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1135,"permalink":"https://freshrimpsushi.github.io/jp/posts/1135/","tags":null,"title":"予測可能関数"},{"categories":"함수","contents":"関数 $f : X \\to Y$が与えられたとしよう。$a, b \\in X$、$a_{i} \\in X\\ (i=1,\\cdots)$とする。\n部分加法関数 関数$f$が下の式を満たす時、部分加法関数subadditive functionという。\n$$ f(a+b) \\le f(a)+f(b) $$\n絶対値が例として挙げられる。\n$$ |3+(-4)| \\le |3|+|-4| $$\n別の例で、$f(x)=2x+3$だとすると\n$$ 13=f(2+3) \\le f(2)+f(3)=7+9=16 $$\n加法関数 関数$f$が下の式を満たす時、加法関数additive functionという。\n$$ f(a+b) = f(a)+f(b) $$\n部分加法性から等式が成り立つ場合だ。\n例えば、$f(x)=4x$とすると\n$$ 20=f(2+3)=f(2)+f(3)=20 $$\n集合$E_{1},\\ E_2$が$E_{1} \\cap E_2 = \\emptyset$を満たし、$n(E_{i})=E_{i}$の要素の数だとする時\n$$ n(E_{1} \\cup E_2) = n(E_{1}) + n(E_2) $$\n可算部分加法関数 関数$f$が下の式を満たす時、可算部分加法関数countable subadditive/$\\sigma$-subadditive functionという。\n$$ f \\left( \\sum_{i=1}^\\infty a_{i} \\right) \\le \\sum \\limits_{i=1}^\\infty f(a_{i}) $$\n部分加法性、加法性を見ると、任意の$N$個の要素に対しても成り立つことが分かる。可算個の要素に対して成り立つなら、可算部分加法性を持つと言われる。可算部分加法性を持つ例に外測度がある。\n可算加法関数 関数$f$が下の式を満たす時、可算加法関数countable additive/$\\sigma$-additive functionという。\n$$ f \\left( \\sum_{i=1}^\\infty a_{i} \\right) = \\sum \\limits_{i=1}^\\infty f(a_{i}) $$\n可算部分加法性から等式が成り立つ場合だ。\n別々に識別される要素に対しては、外測度が可算加法性を持つ。$E_{i} \\cap E_{j} =\\emptyset \\quad \\forall\\ i,j$ならば\n$$ \\mu^{\\ast} \\left( \\bigsqcup _{i=1}^\\infty E_{i} \\right) = \\sum _{i=1}^\\infty \\mu^{\\ast}(E_{i}) $$\n部分乗法関数 関数$f$が下の式を満たす時、部分乗法関数submultiplicative functionという。\n$$ f(ab) \\le f(a)f(b) $$\n上で話した加算に関する性質を乗算に適用したものだ。\n乗法関数 関数$f$が下の式を満たす時、乗法関数multiplicative functionという。\n$$ f(ab) = f(a)f(b) $$\n部分乗法性から等式が成り立つ場合だ。\n","id":1096,"permalink":"https://freshrimpsushi.github.io/jp/posts/1096/","tags":null,"title":"加法関数と乗法関数"},{"categories":"푸리에해석","contents":"定義 関数としてのフーリエ変換 関数$f \\in$$L^{1}$のフーリエ変換Fourier transform of $f$を次のように定義する。\n$$ \\hat{f}(\\xi) := \\int_{-\\infty}^{\\infty} f(t) e^{-i \\xi t}dt $$\n演算子としてのフーリエ変換 以下のように定義される作用素$\\mathcal{F} : L^{1} \\to$$C_{0}$をフーリエ変換という。\n$$ \\mathcal{F}[f] (\\xi) = \\int_{-\\infty}^{\\infty} f(t) e^{-i \\xi t}dt $$\n説明 定義から分かるように、フーリエ変換という言葉は、演算子$\\mathcal{F}$自体を意味する言葉であり、かつ、$\\mathcal{F}$の関数値$\\hat{f} = \\mathcal{F}f = \\mathcal{F}[f]$を意味する言葉でもある。$\\mathcal{F}$の値域が$C_{0}$であることは、リーマン・ルベーグ補題により保証される。さらに、以下が成り立つことを簡単に示せる。$f \\in L^{1}$に対して、\n$$ \\left\\| \\mathcal{F}f \\right\\|_{\\infty} \\le \\left\\| f \\right\\|_{1} $$\n証明\n$$ \\begin{align*} \\left\\| \\mathcal{F}f \\right\\|_{\\infty} = \\max\\limits_{\\xi \\in \\mathbb{R}} \\left| \\mathcal{F}f(\\xi) \\right| \u0026amp;= \\max\\limits_{\\xi \\in \\mathbb{R}} \\left| \\int_{-\\infty}^{\\infty} f(t) e^{-i \\xi t}dt \\right| \\\\ \u0026amp;\\le \\max\\limits_{\\xi \\in \\mathbb{R}} \\int_{-\\infty}^{\\infty} \\left| f(t) e^{-i \\xi t} \\right| dt \\\\ \u0026amp;= \\int_{-\\infty}^{\\infty} \\left| f(t) \\right| dt = \\left\\| f \\right\\|_{1} \\end{align*} $$\nフーリエ変換は積分変換の一種であり、その逆変換は以下のようである。\n$$ f(t) = \\mathcal{F}^{-1}\\hat{f}(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{i t \\xi} d \\xi $$\n前の定数$\\dfrac{1}{2\\pi}$は、逆変換の前にも、変換の前にもつけてもどこにつけても関係ないため、または両方に$\\frac{1}{\\sqrt{2\\pi}}$をつけることもある。これは作者の便宜により異なり、本質的な違いはない。また、定義を見ると、$f$が積分可能で、つまり$f\\in L^{1}$の条件を満たさなければならないことから、フーリエ変換がうまく定義されることが分かる。$\\hat{f}$も積分可能であれば、フーリエ逆変換もまた、うまく定義される。\n多変数関数のフーリエ変換 多変数関数のフーリエ変換は、次のように定義する。多変数関数$f \\in L^{1}(\\mathbb{R}^{n})$のフーリエ変換は、\n$$ \\mathcal{F}f(\\boldsymbol{\\xi}):=\\int f(x)e^{-i \\boldsymbol{\\xi} \\cdot \\mathbf{x} }d\\mathbf{x} $$\n$$ \\mathcal{F} f(\\xi_{1},\\ \\cdots ,\\ \\xi_{n}) := \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} f(x_{1},\\ \\cdots,\\ x_{n})e^{-i(\\xi_{1} x_{1}+\\cdots+\\xi_{n} x_{n})}dx_{1}\\cdots dx_{n} $$\n表記法 $f$のフーリエ変換に一般的に使われる二つの表記法がある。\n$$ \\mathcal{F}(f),\\quad \\hat{f} $$\n教科書では、著者がどの記号を好むかによって異なるが、どちらもよく使われている。右側のハット記号を使う方が便利に見えるが、混乱の余地があるため、正確さが求められる場合は左側の表現を使う方が良い。例えば、入力関数自体の記号が長くなった場合、ハット記号を使うと混乱したり、見た目が良くない場合がある。このような場合は、$\\mathcal{F}$を使うと、式の意味を明確かつきれいに表すことができる。例えば、$W_{c}f$のフーリエ変換は、以下に示すように、$\\mathcal{F}$を使って表記する方が良い。\n$$ \\mathcal{F}(\\mathcal{W}_{c}f),\\quad \\hat{\\mathcal{W}_{c}f},\\quad \\widehat{\\mathcal{W}_{c}f} $$\nしかし、混乱の余地がない場合は、ハット記号の方が便利である。このように、同じ概念に対していくつかの表記法が存在するのは、微分にも同じことが言える。\n$$ f^{\\prime}, \\quad \\dfrac{df}{dx} $$\n$\\hat{f}$と$\\mathcal{F}$の二つの表記法の長所と短所は、微分において左側のニュートン記法が経済性と利便性に優れ、一方で右側のライプニッツ記法が連鎖律などを計算する際に、厳密さと正確さで優れているのと似ている。\n導出1 有限区間で定義された関数は、フーリエ級数を使って近似することができる。これは有用だが、周期関数にしか使えないため、非周期関数に対しても同様の役割を果たすツールが必要である。このアイデアからフーリエ変換Fourier transformが生まれた。フーリエ変換を導出する過程での核心的なアイデアは、非周期関数をまるで実数全体の区間を周期に持ち、周期が数直線全体に1回繰り返される関数として考えることである。\n$f$を区間$[-L,L)$で定義された関数とする。すると、$f$のフーリエ級数と複素フーリエ係数は次のようになる。\n$$ \\begin{equation} f(t)=\\sum \\limits_{n=-\\infty}^{\\infty} c_{n} e^{i\\frac{n\\pi t}{L}} \\end{equation} $$\n$$ c_{n} = \\dfrac{1}{2L}\\int_{-L}^{L}f(t)e^{-i\\frac{n \\pi t}{L} }dt $$\n以下の変数変換を行う。\n$$ \\Delta \\xi = \\dfrac{\\pi}{L},\\quad \\xi_{n}=n\\Delta\\xi=\\dfrac{n\\pi}{L} $$\nすると、$(1)$は次のようになる。\n$$ f(t) = \\sum \\limits_{n=-\\infty}^{\\infty} c_{n} e^{i\\xi_{n} t}, \\quad c_{n} = \\dfrac{1}{2L}\\int_{-L}^{L}f(t)e^{-i \\xi_{n} t }dt $$\n$f(t)$に適切な定数を掛け、$c_{n}$の積分項を$\\hat{f}(\\xi_{n})$とする\n$$ f(t)=\\dfrac{L}{\\pi}\\sum \\limits_{n=-\\infty}^{\\infty} c_{n} e^{i\\xi_{n} t}\\Delta \\xi , \\quad c_{n} = \\dfrac{1}{2L}\\hat{f}(\\xi_{n}) $$\n$f(t)$が$t \\rightarrow \\pm \\infty$の時に速やかに$0$に収束すると仮定する。すると、$c_{n}$に対して積分区間を$[-L,L)$から$(-\\infty,\\infty)$に拡張しても、元の$c_{n}$と大きく変わらないだろう。\n$$ c_{n} \\approx \\dfrac{1}{2L} \\int_{-\\infty}^{\\infty} f(t) e^{-i\\xi_{n} t}dt $$\nこれは$\\xi_{n}$のみの関数なので、$c_{n} = \\frac{1}{2L}\\hat{f}(\\xi_{n})$としよう。$f(t)$に代入すると\n$$ f(t) \\approx \\dfrac{1}{2 \\pi}\\sum \\limits_{n=-\\infty}^{\\infty} \\hat{f}(\\xi_{n}) e^{i\\xi_{n} t}\\Delta \\xi $$\nこれはリーマン和と非常に似ている。今、$L\\rightarrow \\infty$の極限を取ると$\\Delta\\xi \\rightarrow 0$になり、上記の式の$\\approx$は等式となり、和は積分になる。\n$$ f(t) = \\dfrac{1}{2 \\pi}\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{i\\xi t} d\\xi \\quad \\text{and} \\quad \\hat{f}(\\xi)=\\int_{-\\infty}^{\\infty} f(t) e^{-i\\xi t}dt $$\nこの時点で、$\\hat{f}$を$f$のフーリエ変換と呼び、$f$を$\\hat{f}$のフーリエ逆変換Fourier inverse transformと呼ぶ。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p204-205\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1086,"permalink":"https://freshrimpsushi.github.io/jp/posts/1086/","tags":null,"title":"フーリエ変換"},{"categories":"편미분방정식","contents":"説明1 $x$と$p$について、偏微分方程式の変数であることを強調する場合、通常のフォントで $x,p \\in \\mathbb{R}^{n}$ と表示し、$s$に関する関数であることを強調する場合、太字のフォントで $\\mathbf{x}, \\mathbf{p} \\in \\mathbb{R}^{n}$ と表示します。 特性方程式\n$$ \\begin{cases} \\dot{\\mathbf{p}} (s) = -D_{x}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)-D_{z}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)\\mathbf{p}(s) \\\\ \\dot{z}(s) = D_{p}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\cdot \\mathbf{p}(s) \\\\ \\dot{\\mathbf{x}}(s) = D_{p}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\end{cases} $$\n特性方程式を用いた非線形1階偏微分方程式の解法は、微分方程式がどのように与えられるかによって少しずつ異なります。これは与えられた微分方程式の線形性によって区別され、線形、準線形、完全非線形の場合に応じて解法が異なります。非線形性が高いほど難易度が高くなります。\n解法 同次線形 与えられた偏微分方程式が完全に線形であれば、最も簡単に解くことができます。特性方程式の $\\mathbf{p}(s)$ に関する条件は必要ないほど単純です。次の線形および同次の微分方程式を考えてみましょう。\n$$ \\begin{equation} F(Du, u, x) = \\mathbf{b}(x)\\cdot Du(x)+c(x)u(x)=0 \\quad (x\\in \\Omega \\subset \\mathbb{R}^{n}) \\label{eq1} \\end{equation} $$\nここで、各変数 $p, z, x$ を $p, z, x$とします。\n$$ \\begin{equation} F(p,\\ z,\\ x)=\\mathbf{b}(x)\\cdot p +c(x)z=b_{1}p_{1}+\\cdots +b_{n}p_{n}+cz = 0 \\label{eq2} \\end{equation} $$\n$D_{p}F$ を計算すると、次のようになります。\n$$ D_{p}F=(F_{p_{1}}, \\dots, F_{p_{n}})=(b_{1}, \\dots, b_{n})=\\mathbf{b}(x) $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s))\\cdot \\mathbf{p}(s) \\end{align*} $$\nこのとき、$(2)$により、$\\dot{z}(s)$ は次のようになります。\n$$ \\dot{z}(s) = -c(\\mathbf{x}(s))z $$\nしたがって、同次線形1階偏微分方程式の特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{\\mathbf{x}}(s)\u0026amp;=\\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= -c(\\mathbf{x}(s))z \\end{align*} \\right. $$\nこのとき、$\\mathbf{p}(s)$ に関する特性方程式は問題を解くのに必要ありませんことを例を通じて確認できます。\n例 次のような微分方程式が与えられたとします。\n$$ \\left\\{ \\begin{align*} x_{1} u_{x_{2}} - x_{2} u_{x_{1}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0, x_{2}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{ x_{1}\u0026gt;0, x_{2}=0 \\right\\}$ その場合、$(1)$ から $\\mathbf{b}=(-x_{2}, x_{1}), c=-1$ です。したがって、特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;= -x^{2} \\\\ \\dot{x}^{2} \u0026amp;=x^{1} \\\\ \\dot{z}\u0026amp;=z \\end{align*} \\right. $$\nこれは簡単な常微分方程式なので、次のように簡単に解けます。\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;=x^{0}\\cos s \\\\ x^{2}(s)\u0026amp;=x^{0} \\sin s \\\\ z(s)\u0026amp;=z^{0}e^s=g(x^{0})e^s \\end{align*} \\right. $$\nここで、 $x^{0}$ は $s=0$ のときに $x_{1}-$軸($\\Gamma$) を通過するように選ばれた定数です。その後、点 $(x_{1}, x_{2}) \\in \\Omega$ を固定しましょう。\n$$ (x_{1},\\ x_{2})=(x^{1}(s),\\ x^{2}(s)) = (x^{0} \\cos (s),\\ x^{0} \\sin (s)) $$\nすると、 $s\u0026gt;0, x^{0}\u0026gt;0$ の場合、次の結果が得られます。\n$$ x_{1}^{2} + x_{2}^{2} = (x^{0})^{2}\\cos^{2}(s) + (x^{0})^{2}\\sin^{2}(s) = (x^{0})^{2} \\implies x^{0}=({x_{1}}^{2}+{x_{2}}^{2})^{1/2} \\\\ \\dfrac{x_{2}}{x_{1}} = \\dfrac{x^{0}\\sin (s)}{x^{0} \\cos (s)} = \\tan (s) \\implies s=\\arctan \\left( \\frac{x_{2}}{x_{1}} \\right) $$\nしたがって、方程式の解は次のようになります。\n$$ \\begin{align*} u(x)\u0026amp;=u(x^{1}(s),\\ x^{2}(s)) \\\\ \u0026amp;= z(s) \\\\ \u0026amp;=g(x^{0})e^s \\\\ \u0026amp;= g(({x_{1}}^{2}+{x_{2}}^{2})^{1/2})e^{\\arctan \\left(\\frac{x_{2}}{x_{1}}\\right)} \\end{align*} $$\n■\n準線形 次に、与えられた微分方程式が最高微分項に関して線形である場合を考えます。今扱っているのは1階微分方程式なので、1階微分項に関して線形な場合です。\n$$ F(Du,\\ u,\\ x)=\\mathbf{b}(x,\\ u(x))\\cdot Du(x)+c(x,\\ u(x))=0 $$\nここで、各変数 $p, z, x$ を $p, z, x$ とします。\n$$ \\begin{equation} F(p, z, x)=\\mathbf{b}(x, z)\\cdot p + c(x, z)=b_{1}p_{1} + \\cdots + b_{n} p_{n} +c=0 \\label{eq3} \\end{equation} $$\n$D_{p}F$ を計算すると、次のようになります。\n$$ D_{p}F=(F_{p_{1}},\\ \\cdots,\\ F_{p_{n}})=(b_{1},\\ \\cdots,\\ b_{n})=\\mathbf{b}(x,\\ z) $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s)) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s))\\mathbf{p}(s)=-c(\\mathbf{x}(s),\\ z(s)) \\end{align*} $$\n$\\dot{z}$ の2つ目の等号は $(3)$ によって成立します。この場合も $\\mathbf{p}(s)$ に関する条件は問題を解くのに必要ありません。\n例 次のような微分方程式が与えられたとします。\n$$ \\left\\{ \\begin{align*} u_{x_{1}} + u_{x_{2}} \u0026amp;= u^{2} \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{2} \\gt 0 \\right\\}$ $\\Gamma=\\left\\{ x_{2} = 0 \\right\\}$ その場合、$(3)$ から $\\mathbf{b}=(1, 1)$, $c=-z^{2}$ です。したがって、特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;=1, \\dot{x}^{2}=1 \\\\ \\dot{z} \u0026amp;= z^{2} \\end{align*} \\right. $$\nこれはそれぞれ単純な常微分方程式なので、次のように解けます。\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;= x^{0}+s, x^{2}(s)=s \\\\ z(s)\u0026amp;=\\frac{z^{0}}{1-sz^{0}}=\\frac{g(x^{0})}{1-sg(x^{0})} \\end{align*} \\right. $$\nここで、$x^{0}$ は $s=0$ のときに $x_{2}-$軸($\\Gamma$) を通過するように選ばれた定数です。また、与えられた微分方程式は $u_{x_{1}}u_{x_{2}}=u$ ですから、$p_{1}^{0}p_{2}^{0}=z^{0}=(x^{0})^{2}$ となり、$p_{1}^{0}=\\frac{x^{0}}{2}$ です。これらを全て $(4)$ に代入すると、次の結果が得られます。\n$$ \\left\\{ \\begin{align*} p^{1}(s) \u0026amp;= \\frac{x^{0}}{2}e^s \\\\ p^{2}(s) \u0026amp;= 2x^{0}e^s \\\\ z(s) \u0026amp;= (x^{0})^{2}e^{2s} \\\\ x^{1}(s) \u0026amp;= 2x^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+\\frac{x^{0}}{2}(e^s-1) \\end{align*} \\right. $$\nその後、点\n$(x_{1}, x_{2}) \\in \\Omega$ を固定しましょう。\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=\\left( 2x^{0}(e^s -1), \\frac{x^{0}}{2}(e^s+1) \\right) $$\nすると、$s, x^{0}$ に関して次の結果が得られます。\n$$ x^{0}=\\frac{4x_{2}-x_{1}}{4},\\ \\ e^s=\\frac{x_{1}+4x_{2}}{4x_{2}-x_{1}} $$\nしたがって、方程式の解は次のようになります。\n$$ u(x)=u(x^{1}(s),\\ x^{2}(s))=z(s)=(x^{0})^{2}e^{2s}=\\dfrac{(x_{1}+4x_{2})^{2}}{16} $$\n■\n完全非線形 最後に、次のような微分方程式が与えられたとします。\n$$ \\begin{align*} u_{x_{1}}u_{x_{2}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u \u0026amp;= x_{2}^{2} \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{ x_{1}=0 \\right\\}$ $F$ の変数を $P, z, x$ とすると、次のようになります。\n$$ F(p, z, x)=p_{1}p_{2}-z $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{p}^{1} \u0026amp;= p^{1},\\quad \\dot{p}^{2}=p^{2} \\\\ \\dot{z} \u0026amp;= 2p^{1}p^{2} \\\\ \\dot{x}^{1} \u0026amp;= p^{2},\\quad \\dot{x}^{2}=p^{1} \\end{align*} $$\nまず、 $p$ に関する微分方程式を解くと、次のようになります。\n$$ p^{1}(s)=p_{1}^{0}e^s,\\ \\ p^{2}(s)=p_{2}^{0}e^s $$\nこのとき、 $p_{1}^{0}=p(0)$ および $p_{2}^{0}=p(0)$ です。したがって、$\\dot{z}(s)=2p_{1}^{0}p_{2}^{0}e^{2s}$ であるため、$z$ は次のようになります。\n$$ z(s)=p_{1}^{0}p_{2}^{0}e^{2s}+C $$\n$z(0)=z^{0}=p_{1}^{0}p_{2}^{0}+C$ なので、$C=z^{0}-p_{1}^{0}p_{2}^{0}$ です。したがって、次のようになります。\n$$ z(s)=z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) $$\n同様の方法で $x^{1}$ および $x^{2}$ も計算すると、次のようになります。\n$$ \\begin{equation} \\left\\{ \\begin{aligned} p^{1}(s) \u0026amp;= p_{1}^{0}e^s \\\\ p^{2}(s) \u0026amp;= p_{2}^{0}e^s \\\\ z(s) \u0026amp;= z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) \\\\ x^{1}(s) \u0026amp;= p_{2}^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+p_{1}^{0}(e^s-1) \\end{aligned} \\right. \\label{eq4} \\end{equation} $$\nこのとき、 $x^{0}$ は $s=0$ のときに $x_{1}-$軸($\\Gamma$) を通過するように選ばれた定数です。$u_{x_{2}}=p^{2}$ および境界条件により、$x_{2}^{0}=u(0, x^{0})=2x^{0}$ です。また、与えられた微分方程式は $u_{x_{1}}u_{x_{2}}=u$ であるため、$p_{1}^{0}p_{2}^{0}=z^{0}=(x^{0})^{2}$ であり、$p_{1}^{0}=\\frac{x^{0}}{2}$ です。これらを全て $(4)$ に代入すると、次の結果が得られます。\n$$ \\left\\{ \\begin{align*} p^{1}(s) \u0026amp;= \\frac{x^{0}}{2}e^s \\\\ p^{2}(s) \u0026amp;= 2x^{0}e^s \\\\ z(s) \u0026amp;= (x^{0})^{2}e^{2s} \\\\ x^{1}(s) \u0026amp;= 2x^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+\\frac{x^{0}}{2}(e^s-1) \\end{align*} \\right. $$\nその後、点 $(x_{1}, x_{2})\\in \\Omega$ を固定しましょう。\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=\\left( 2x^{0}(e^s -1), \\frac{x^{0}}{2}(e^s+1) \\right) $$\nすると、 $s, x^{0}$ に関して次の結果が得られます。\n$$ x^{0}=\\frac{4x_{2}-x_{1}}{4},\\ \\ e^s=\\frac{x_{1}+4x_{2}}{4x_{2}-x_{1}} $$\nしたがって、方程式の解は次のようになります。\n$$ u(x)=u(x^{1}(s),\\ x^{2}(s))=z(s)=(x^{0})^{2}e^{2s}=\\dfrac{(x_{1}+4x_{2})^{2}}{16} $$\n■\nLawrence C. Evans, Partial Differential Equations (第2版, 2010年), p99-102\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1074,"permalink":"https://freshrimpsushi.github.io/jp/posts/1074/","tags":null,"title":"特性方程式を利用した非線形1系偏微分方程式の解法。"},{"categories":"수치해석","contents":"定義 1 異なる$x_{0} , \\cdots , x_{n}$のデータ$(x_{0}, y_{0}) , \\cdots , (x_{n} , y_{n})$について、$p (x_{i} ) = y_{i}$と$\\deg p \\le n$を満たす多項式関数$p$を多項式補間Polynomial Interpolationという。\n定理 存在性と一意性 [1]: 与えられたデータに対し、 $p$は一意に存在する。 ラグランジュの公式 [2]: $$p_{n} (x) = \\sum_{i=0}^{n} y_{i} l_{i} (X)$$ ニュートンの差分公式 [3]: $$p_{n} (x) = f(x_{0}) + \\sum_{i=1}^{n} f [ x_{0} , \\cdots , x_{i} ] \\prod_{j=0}^{i-1} (x - x_{j} )$$ 誤差解析 [4]: $(n+1)$回微分可能な$f : \\mathbb{R} \\to \\mathbb{R}$とある$\\xi \\in \\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n} \\right\\}$に対し、$f$の多項式補間$p_{n}$はある$t \\in \\mathbb{R}$に対して次を満たす。$$\\displaystyle f(t) - p_{n} (t) = {{ (t - x_{0}) \\cdots (t - x_{n}) } \\over { (n+1)! }} f^{(n+1)} ( \\xi )$$ $\\mathscr{H} \\left\\{ a,b,c, \\cdots \\right\\}$は$a,b,c, \\cdots$を含む最小の区間を表す。 説明 多項式補間は韓国語で多項式補間法と簡化される。\n条件 $\\deg p \\le n$ $\\deg p \\le n$という条件は、$p$が$\\deg p = n$を満たす保証が常にあるわけではないことを意味する。例えば$n=2$の時、上のように3点が一直線上にある場合、$(n+1)=3$個の点を通る$p_{2} (x) = a_{2} x^{2} + a_{1} x + a_{0}$は存在しないが、それよりも低い次数の$p_{1} (x) = a_{1} x + a_{0}$は存在する。これは実際に$p_{2} (x) = a_{2} x^{2} + a_{1} x + a_{0}$を見つけたが$a_{2} = 0$の場合を意味する。\nラグランジュの公式とニュートンの差分公式は同じである 公式[2]と[3]は違うように見えても、実際には[1]によって同じであることがわかる。本質的に2つの公式の違いは$p_{n}$をどう表すかの差に過ぎず、機能的な違いは新しいデータが追加された時にニュートンの差分公式が更新しやすいという点だけである。\n実際の関数との誤差 定理[4]は、ある関数$f$を補間する$p_{n}$が$f$とどの程度違うかを示す。通常の場合には$(n+1)!$の発散速度は非常に速いため、データが多ければ多いほど補間$p_{n}$の正確性は上がる。しかし、この公式は特に収束性を論じるわけではないことに注意が必要だ。簡単な例で$t$が$\\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n} \\right\\}$から非常に遠い場所にある場合を考えることができる。また、$f$がそれほど良くなくて微分するたびに値が大きくなり、それが更に$(n+1)!$の発散速度よりも速い場合、どんなに変になるかだってある。\n証明 [1] 戦略: $(n+1)$個の連立方程式を行列で表し、逆行列の存在性と一意性を同時に持ってくる。\n全ての$i = 0, \\cdots , n$に対して$y_{i} = p_{n} (x_{i}) = a_{0} + a_{1} x_{i} + \\cdots + a_{n} x_{i}^{n}$を満たす$p_{n}$の係数$a_{0} , a_{1} , \\cdots , a_{n}$が一意であることを示せば良い。 $$ \\mathbb{y} := \\begin{bmatrix} y_{0} \\\\ y_{1} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}, X := \\begin{bmatrix} 1 \u0026amp; x_{0} \u0026amp; \\cdots \u0026amp; x_{0}^{n} \\\\ 1 \u0026amp; x_{1} \u0026amp; \\cdots \u0026amp; x_{1}^{n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{n} \u0026amp; \\cdots \u0026amp; x_{n}^{n} \\end{bmatrix} , \\mathbb{a} := \\begin{bmatrix} a_{0} \\\\ a_{1} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} $$と定義し、連立方程式を行列で表すと $$ \\begin{bmatrix} y_{0} \\\\ y_{1} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{0} \u0026amp; \\cdots \u0026amp; x_{0}^{n} \\\\ 1 \u0026amp; x_{1} \u0026amp; \\cdots \u0026amp; x_{1}^{n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{n} \u0026amp; \\cdots \u0026amp; x_{n}^{n} \\end{bmatrix} \\begin{bmatrix} a_{0} \\\\ a_{1} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} $$ 今、$\\mathbb{y} = X \\mathbb{a}$を満たす解$\\mathbb{a}$を見つける問題に変わった。\nヴァンデルモンド行列の行列式: $X$の行列式は$\\displaystyle \\det X = \\prod_{1 \\le i \u0026lt; j \\le n } (x_{j} - x_{i})$\n仮定から$x_{0} , \\cdots , x_{n}$は異なるので$\\det X \\ne 0$であり、$X^{-1}$が存在する。したがって、$\\mathbb{a} = X^{-1} \\mathbb{y}$も存在する。一方で、与えられた行列に対する逆行列は一意なので、$\\mathbb{a}$も一意である。\n■\n[2] クロネッカーデルタ関数で見る。\n■\n[3] 差分そのままを使って正直に計算する。\n■\n[4] 戦略: 新しいダミー関数を定義し、それらの微分可能性を利用して直接的な計算を回避する。設定が複雑なので、実際には後ろから理解するほうが楽である。\nClaim: $E (x) := f(x) - p_{n} (X)$に対して次が成立する。 $$ E(t) = {{ (t - x_{0}) \\cdots (t - x_{n}) } \\over { (n+1)! }} f^{(n+1)} ( \\xi ) $$\nPart 1.\nまず、$t$がノードポイント$x_{0} , \\cdots , x_{n}$と同じなら自明に成立するので、$t$がこれらのノードポイントと異なると仮定する。 $$ \\begin{align*} \\Psi (x) :=\u0026amp; (x - x_{0} ) (x - x_{1}) \\cdots (x - x_{n}) \\\\ G (x) :=\u0026amp; E(x) - {{ \\Psi (x) } \\over { \\Psi (t) }} E(t) \\end{align*} $$ 上記のように新しい関数を定義すると、$f$、$p_{n}$、$\\Psi$が$(n+1)$回微分可能であるため、$G$も$(n+1)$回微分可能である。\nPart 2.\n$x = x_{i}$を$G$に代入すると$E(x_{i}) = f(x_{i}) - p_{n} (x_{i}) = 0$であり、$\\Psi (x_{i} ) = 0$なので $$ G(x_{i}) = E(x_{i} ) - {{ \\Psi (x_{i}) } \\over { \\Psi (t) }} E(t) = 0 $$ $x = t$を$G$に代入すると$\\displaystyle {{ \\Psi (t) } \\over { \\Psi (t) }} = 1$なので $$ G(t) = E(t) - E(t) =0 $$ したがって、$G$は$(n+2)$個の異なる零点$x_{0} , \\cdots , x_{n} , t$を持つ。\nPart 3.\n便宜上$x_{n+1} :=t$としよう。\nロルの定理: 関数$f(x)$が$[a,b]$で連続であり、$(a,b)$で微分可能であり、$f(a)=f(b)$ならば、$f ' (c)=0$を満たす$c$が$(a,b)$に少なくとも一つ存在する。\n全ての$i=0, \\cdots , n$に対して $$ G(x_{i}) = 0 = G(x_{i+1}) $$ なのでロルの定理により$g ' ( x\u0026rsquo;_{i}) = 0$を満たす$x'_{i} \\in \\mathscr{H} \\left\\{ x_{i} , x_{i+1} \\right\\}$が存在し、同様に全ての$i=0, \\cdots , (n-1)$に対して $$ g ' (x\u0026rsquo;_{i}) = 0 = g ' (x\u0026rsquo;_{i+1}) $$ なのでロルの定理により$G''( x\u0026rsquo;\u0026rsquo;_{i}) = 0$を満たす$x''_{i} \\in \\mathscr{H} \\left\\{ x\u0026rsquo;_{i} , x\u0026rsquo;_{i+1} \\right\\}$が存在する。このように帰納的にロルの定理を$(n+1)$回使用して $$ G^{(n+1)} ( \\xi ) = 0 $$ を満たす$\\xi \\in \\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n+1} \\right\\}$の存在を保証することができる。\n一方で、$p_{n}$は$n$次の多項式なので $$ E^{(n+1)} (x) = f^{(n+1)} ( x) $$ $\\Psi$の最高次項は$x^{n+1}$であるため $$ \\Psi^{(n+1)} (x) = (n+1)! $$ $\\displaystyle G (x) = E(x) - {{ \\Psi (x) } \\over { \\Psi (t) }} E(t)$の両辺を$x$に対して$(n+1)$回微分すると $$ G^{(n+1)} (x) = f^{(n+1)} ( x) - {{ (n+1)! } \\over { \\Psi (t) }} E(t) $$ $x=\\xi$を$G^{(n+1)}$と$f^{(n+1)}$に代入すると $$ 0 = f^{(n+1)} ( \\xi ) - {{ (n+1)! } \\over { \\Psi (t) }} E(t) $$ $E (t) = f(t) - p_{n} (t)$なので、次を得る。 $$ f(t) - \\sum_{j=0}^{n} f( x_{j} ) l_{j} (t) = {{ (t - x_{0}) \\cdots (t - x_{n}) } \\over { (n+1)! }} f^{(n+1)} ( \\xi ) $$\n■\nAtkinson. (1989). 数値解析入門(第2版): p131.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1021,"permalink":"https://freshrimpsushi.github.io/jp/posts/1021/","tags":null,"title":"多項式補間"},{"categories":"수치해석","contents":"定義 1 与えられた$(n+1)$ペアのデータ$(x_{0}, y_{0}) , \\cdots , (x_{n} , y_{n})$に対して、$f (x_{i} ) = y_{i}$を満たしつつある特定の性質を持つ$f$を見つける方法、またはその関数自体を内挿法（インターポレーション）という。\n説明 例えば、上に示されたようなデータがあるけど、真ん中のデータが空いている状況を想像してみよう。もちろん、実際のデータがあるのが最高だけど、ない場合は予測をしてでも使わなければならない状況があるかもしれない。このように、空いている部分を埋めるという点で、インターポレーションという表現は適切である。数値解析だけでなく、このようなアプリケーションはいつも必要になるかもしれない。\nインターポレーションの最も簡単な例として、点と点を直線で結ぶ線形内挿法（リニア・インターポレーション）を考えることができる。このようなインターポレーションは直感的という利点があるが、各データが存在する地点で微分をすることはできない。従って、下に示されているように、点々を滑らかにつなげる方法が必要な場合、使用できなくなる。このように、インターポレーションは一つの方法に限らず、必要な方法、望む方法を見つけなければならない。\nAtkinson. (1989). An Introduction to Numerical Analysis(2nd Edition): p131.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1016,"permalink":"https://freshrimpsushi.github.io/jp/posts/1016/","tags":null,"title":"数値解析における補間"},{"categories":"편미분방정식","contents":"定義[^1] 要素が非負の整数の組$\\alpha=(\\alpha_{1}, \\alpha_{2}, \\cdots, \\alpha_{n})$をオーダーが$|\\alpha|$のマルチインデックスmulti-indexと言う。ここで、$| \\alpha|$は以下のように定義される。\n$$ |\\alpha| = \\sum _{i}^{n} \\alpha_{i} = \\alpha_{1} + \\cdots + \\alpha_{n} $$\n表記法 $x = (x_{1}, x_{2}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$に対して、$x^{\\alpha}$は以下のように定義される。\n$$ x^{\\alpha} := x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{n}^{\\alpha_{n}} $$\nマルチインデックスは、以下のように偏微分を表す際によく使用される。\n$$ \\begin{align*} D^\\alpha :=\u0026amp;\\ \\dfrac{\\partial ^{|\\alpha|} } {{\\partial x_{1}}^{\\alpha_{1}}\\cdots {\\partial x_{n}}^{\\alpha_{n}}} \\\\ =\u0026amp;\\ \\left( \\frac{ \\partial }{ \\partial x_{1}} \\right)^{\\alpha_{1}}\\left( \\frac{ \\partial }{ \\partial x_{2}} \\right)^{\\alpha_{2}}\\cdots \\left( \\frac{ \\partial }{ \\partial x_{n}} \\right)^{\\alpha_{n}} \\\\ =\u0026amp;\\ \\partial^{\\alpha_{1}}_{x_{1}}\\cdots\\partial^{\\alpha_{n}}_{x_{n}} \\end{align*} $$\n例えば、$\\alpha=(2,1,0)$とするならば、$D^{\\alpha} u(x)$は以下を意味する。\n$$ D^{\\alpha} u(x)=\\dfrac{ \\partial^3 u(x)} {\\partial x_{1} \\partial x_{1} \\partial x_{2}}=\\dfrac{ \\partial^3 u(x)} {\\partial x_{1} ^{2} \\partial x_{2}} $$\nまた、整数$k \\ge 0$に対して、$D^k$を以下のように定義する。\n$$ D^ku:=\\left\\{ D^{\\alpha} u : |\\alpha|=k \\right\\} $$\n$D^{k}u$はオーダーが$k$の全てのマルチインデックス$\\alpha$に対する$D^{\\alpha} u$を集めた集合である。$k$はマルチインデックスではなく、非負の整数であることに注意。$D^{k}u$の要素にそれぞれ順序を付けること、つまり、それぞれが何番目の成分かを定めると、$D^k u$を$\\mathbb{R}^{k}$の点として考えることができる。[^2]次の例を見よ。\nケース 1. $k=1$\n勾配を意味する。\n$$ D^1 u=Du:=(u_{x_{1}},\\ u_{x_{2}},\\ \\cdots,\\ u_{x_{n}})=\\nabla u \\ \\in \\ \\mathbb{R^n} $$\nケース 2. $k=2$\nヘッセ行列を意味する。\n$$ D^2u := \\begin{pmatrix} u_{x_{1}x_{1}} \u0026amp; \\cdots \u0026amp; u{x_{1}x_{n}} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\cdots \\\\ u_{x_{n}x_{1}} \u0026amp; \\cdots \u0026amp; u_{x_{n}x_{n}} \\end{pmatrix} \\in \\ \\mathbb{R^2} $$\n特に、$u$のラプラシアンの場合、$u$のヘッセ行列の対角成分をすべて足したものと同じである。\n$$ \\Delta u=\\nabla^2=\\nabla \\cdot \\nabla u=\\mathrm{div} Du = \\sum_{i=1}^nu_{x_{i}x_{i}} = \\mathrm{tr} (D^2u) $$\n","id":1062,"permalink":"https://freshrimpsushi.github.io/jp/posts/1062/","tags":null,"title":"マルチインデックス表記法"},{"categories":"물리학","contents":"定義 波を三角関数で表したものをサイン波sinusoidal waveという。\n説明 サイン波の一般的な形は以下の通りだ。式が$\\cos$である理由は下で説明するが、それは複素波動関数の実部が$\\cos$だからだ。$\\sin$は虚部だ。\n$$ f(x,t) = A \\cos \\big( k(x-vt)+\\delta \\big) $$\nここで、$A$を波の振幅amplitudeとし、コサイン関数の変数$k(z-vt)+\\delta$を位相phase、$\\delta$を位相定数phase constantという。位相定数に$2\\pi$を加えても$f(x,t)$は変わらない。だから普通は位相定数として$0\\le \\delta \\lt 2\\pi$の範囲の値を使う。$k$は波数wave numberであり、波長wavelength$\\lambda$とは次のような関係がある。\n$$ k=\\dfrac{2\\pi}{\\lambda} $$\n波が完全に1回転する時間を周期periodという。時間=距離/速度なので、波の周期$T$は\n$$ T=\\dfrac{\\lambda}{v} = \\dfrac{2 \\pi}{kv} $$\n周期は1回振動するのにかかる時間なので、単位時間あたりの振動数である振動数frequency$\\nu$は当然周期の逆数と同じだ。\n$$ \\nu=\\dfrac{1}{T}=\\dfrac{v}{\\lambda} $$\n角振動数angular frequencyは一般に$\\omega$で表され、振動を等速円運動に対応させて表現するものだ。振動数を単位時間あたりの回転角度に変えた値であり、単位はラディアンだ。\n$$ \\omega=2\\pi \\nu=2\\pi\\dfrac{1}{T}=kv $$\n$(1)$を角振動数で表したら\n$$ f(x,t)=A \\cos \\big( kx-\\omega t +\\delta \\big) $$\nこれは波数が$k$で角振動数が$\\omega$の右へ進行する波動関数だ。\n上の図のように、$\\dfrac{\\delta}{k}$を波動関数が原点から遅れた距離として定義する。だから、波の進行方向が変われば、位相定数の符号も変わる。波が左へ進むなら、右へ移動したのが遅れたということだ。つまり、波数が$k$で角振動数が$\\omega$の左へ進行する波動関数は以下の通りだ。\n$$ f(x,t)=A \\cos \\big( kx+\\omega t -\\delta \\big) $$\nでも、コサイン関数は偶関数なので、上の式は下の式と同じだ。 $$ f(x,t)=A \\cos \\big( -kx-\\omega t +\\delta \\big) $$\nこれは、波数が$k$で角振動数が$\\omega$の右へ進行する波動関数$(2)$と比較して、波数$k$の符号だけが異なる。つまり、波数$k$の符号を変えると、振幅、位相定数、振動数、波長などはすべて同じだが、進行方向だけが反対の波になることが分かる。\n複素波動関数 波動関数がコサインで表されるので、オイラーの公式を使って複素指数関数の形でも表せる。虚部を広げて複素波動関数を扱う理由は、複素関数がコサインやサインよりも多くの面で計算に便利だからだ。$e^{ix}=\\cos x +i\\sin x$を使って$(2)$を表すと\n$$ f(x,t)=\\text{Re}(Ae^{i(kx-\\omega t +\\delta)}) $$\nここで、$\\text{Re}(a+ib)=a$。すなわち実部を表す。$f$は$Ae^{i(kx-\\omega t +\\delta)}$の実部だけを表した関数なので、$\\tilde{f}=Ae^{i(kx-\\omega t +\\delta)}$としよう。つまり、$\\text{Re}(\\tilde{f})=f$だ。すると、以下のように簡単に整理できる。\n$$ \\tilde{f}(x,t)=Ae^{i(kx-\\omega t+\\delta)}=Ae^{i\\delta}e^{i(kx-\\omega t)}=\\tilde{A}e^{i(kx-\\omega t)} $$\nこの記事で扱っている波動関数$f(x,t)$は複素波動関数の実部だ。\n$$ f(x,t)=\\text{Re}\\big( \\tilde{f}(x,t) \\big) $$\n","id":1066,"permalink":"https://freshrimpsushi.github.io/jp/posts/1066/","tags":null,"title":"サイン波と複素波動関数"},{"categories":"다변수벡터해석","contents":"定義 スカラー場 $f : \\mathbb{R}^{n} \\to \\mathbb{R}$の全微分を特にグラジエントgradient, 傾きと呼び、$\\nabla f$と表記する。\n$$ \\begin{align*} \\nabla f := f^{\\prime} =\u0026amp; \\begin{bmatrix} D_{1}f \u0026amp; D_{2}f \u0026amp; \\cdots \u0026amp; D_{n}f\\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial f}{\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f}{\\partial x_{n}} \\end{bmatrix} \\\\ =\u0026amp; \\dfrac{\\partial f}{\\partial x_{1}}\\hat{x}_{1} + \\dfrac{\\partial f}{\\partial x_{2}}\\hat{x}_{2} + \\dots + \\dfrac{\\partial f}{\\partial x_{n}}\\hat{x}_{n} \\end{align*} $$\n説明 グラジエントは、簡単に言えば多変数関数の導関数だ。物理学などでよく使用される3次元スカラー関数のグラジエントは以下の通り。\n$$ \\nabla f = \\dfrac{\\partial f}{\\partial x}\\hat{\\mathbf{x}} + \\dfrac{\\partial f}{\\partial y}\\hat{\\mathbf{y}} + \\dfrac{\\partial f}{\\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) $$\n注目すべき点は、関数値がスカラーであるスカラー関数の導関数が、関数値がベクトルであるベクトル関数になることである。これは全微分の定義から当然とも言えるが、直感的にも理解することができる。\n例として、上の図を考えてみよう。この図は、$z(x,y) = x^2 - y^2$ として定義される関数 $z : \\mathbb{R}^{2} \\to \\mathbb{R}$ を視覚的に示したものである。$y = f(x)$のような一変数関数とは異なり、変数が2つ以上ある関数の変化率を考える場合、その大きさだけでなく方向も考慮する必要があることがわかる。\nこの概念を反映した方向微分は、任意の方向への微分を意味する。したがって、多変数関数は無数の方向の微分を持っているが、下の定理からグラジエントは変化率が最も大きい方向を指すことがわかる。\n証明 $\\left\\| \\mathbb{d} \\right\\| = 1$ となる方向ベクトル $\\mathbb{d} : = ( d_1 , \\cdots , d_n )$ を定義しよう。多変数関数のテイラーの定理により、\n$$ f \\left( x_{0} + h \\mathbb{d} \\right) = f ( \\mathbb{x}_{0} ) + h \\left[ {{ \\partial f ( \\mathbb{x}_{0} ) } \\over { \\partial x_{1} }} d_{1} + \\cdots + {{ \\partial f ( \\mathbb{x}_{0} ) } \\over { \\partial x_{n} }} d_{n} \\right] + O (h^2) $$\n行列の形に変換すると、\n$$ f \\left( x_{0} + h \\mathbb{d} \\right) - f ( \\mathbb{x}_{0} ) = h \\begin{bmatrix} {{ \\partial f ( \\mathbb{x}_{0} ) } \\over { \\partial x_{1} }} \\\\ \\vdots \\\\ {{ \\partial f ( \\mathbb{x}_{0} ) } \\over { \\partial x_{n} }} \\end{bmatrix} \\cdot \\begin{bmatrix} d_{1} \\\\ \\vdots \\\\ d_{n} \\end{bmatrix} + O (h^2) $$\nベクトルの形にすると、\n$$ {{ f \\left( x_{0} + h \\mathbb{d} \\right) - f ( \\mathbb{x}_{0} )} \\over {h}} = \\nabla f \\left( \\mathbb{x}_{0} \\right) \\cdot \\mathbb{d} + O (h) $$\n$h \\to 0$ の時、\n$$ \\nabla f \\left( \\mathbb{x}_{0} \\right) \\cdot \\mathbb{d} = \\lim_{h \\to 0} {{ f \\left( x_{0} + h \\mathbb{d} \\right) - f ( \\mathbb{x}_{0} )} \\over {h}} $$\n$\\mathbb{b}$ が $\\mathbb{x}_{0}$ から $f$ への傾きと同じ方向であるということは、$\\mathbb{d}$ が\n$$ \\lim_{h \\to 0} {{ f \\left( x_{0} + h \\mathbb{d} \\right) - f ( \\mathbb{x}_{0} )} \\over {h}} $$\nこれを最大化するという意味であり、これを満たす場合は $\\displaystyle \\mathbb{d} = {{\\nabla f \\left( \\mathbb{x}_{0} \\right) } \\over { \\left\\| \\nabla f \\left( \\mathbb{x}_{0} \\right) \\right\\| }}$ のみであり、\n$$ \\nabla f \\left( \\mathbb{x}_{0} \\right) = \\left\\| \\nabla f \\left( \\mathbb{x}_{0} \\right) \\right\\| \\mathbb{d} $$\nこれが $\\mathbb{x}_{0}$ から $f$ のグラジエントになる。\n■\n参照 3次元直交座標系でのグラジエント ベクトル場の微分係数：ヤコビ行列 ","id":1010,"permalink":"https://freshrimpsushi.github.io/jp/posts/1010/","tags":null,"title":"スカラーフィールドの勾配"},{"categories":"머신러닝","contents":"定義 実際の生物の閾値を模倣した非線形関数を活性化関数activation functionと言う。\n数学的定義 ディープラーニングでは非線形スカラー関数 $\\sigma : \\mathbb{R}^{n} \\to \\mathbb{R}$を活性化関数と呼ぶ。\nもちろん、この定義から外れるソフトマックスみたいなのもあるが、例外としよう。 説明 一方でベクトル関数はレイヤーlayer、層と呼ばれる。\n$\\sigma : \\mathbb{R} \\to \\mathbb{R}$で定義された活性化関数が入力としてベクトルを受け取るという表現やコードがあれば、成分ごとに適用するという意味だ。\n$$ \\sigma (\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}) = \\begin{bmatrix} \\sigma (x_{1}) \\\\ \\sigma (x_{2}) \\\\ \\vdots \\\\ \\sigma (x_{n}) \\end{bmatrix} $$\nモチーフ 閾値とは、生物が刺激に対してどのような反応を引き起こすために必要な最小限の刺激の強さのことで、ディープラーニングはこれを模倣するために、各ノードの計算結果に活性化関数を適用して次のレイヤーへと渡す。このような非線形的補正がなければ、ディープラーニングでヒドゥンレイヤーを置いて計算を何度もする意味がない。活性化関数には様々な種類があるが、どれが良いかはまさにケースバイケースだ。どの活性化関数を使えばパフォーマンスがどう変わるのかという理論はほとんどなく、ただ試してみて結果が良ければ採用するという感じだ。\n例 ステップ関数 $$u (x) := \\begin{cases} 0 \u0026amp; , x\u0026lt;0 \\\\ 1 \u0026amp; x \\ge 0 \\end{cases} $$\nステップ関数は閾値という名前に最も適した関数だが、計算結果をあまりに単純化しすぎるため、実際に使うのは難しい。他の活性化関数もステップ関数ではないが、ステップ関数のように作用するように設計されたと考えると良い。\nシグモイド関数 シグモイド関数の中で最も有名なのはおそらくロジスティック関数 $\\displaystyle \\sigma (x) := {{1} \\over { 1 + e^{-x} }}$ で、まるでステップ関数を連続関数につなげたような形状をしている。値域は異なるが、$\\tanh x$ も同じ理由で使用された。最近では、グラディエントバニッシングという問題が発見され、ほとんど使われなくなった。\nReLu(整流線形ユニット) 関数 $$\\operatorname{ReLU} (x) := \\max \\left\\{ 0 , x \\right\\}$$\nシグモイド関数の問題点を克服するために考案された関数だ。$x \u0026lt;0$ ならば関数値を完全に殺し、$0$ を超えないとそのまま伝達される、という点で活性化関数らしい。\n","id":991,"permalink":"https://freshrimpsushi.github.io/jp/posts/991/","tags":null,"title":"ディープラーニングにおける活性化関数"},{"categories":"다변수벡터해석","contents":"定義 $D \\subset \\mathbb{R}^{n}$で定義された多変数ベクトル関数 $\\mathbb{f} : D \\to \\mathbb{R}^{m}$が各スカラー関数 $f_{1} , \\cdots , f_{m} : D \\to \\mathbb{R}$に対して\n$$ \\mathbb{f} ( x_{1} , \\cdots , x_{n} ) : = \\begin{bmatrix} f_{1} ( x_{1} , \\cdots , x_{n} ) \\\\ \\vdots \\\\ f_{m} ( x_{1} , \\cdots , x_{n} ) \\end{bmatrix} $$\nと定義されているとしよう。\n$$ J := \\begin{bmatrix} {{\\partial f_{1} } \\over {\\partial x_{1} }} \u0026amp; \\cdots \u0026amp; {{\\partial f_{1} } \\over {\\partial x_{n} }} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ {{\\partial f_{m} } \\over {\\partial x_{1} }} \u0026amp; \\cdots \u0026amp; {{\\partial f_{m} } \\over {\\partial x_{n} }} \\end{bmatrix} $$\nを$\\mathbb{f}$のヤコビ行列という。\n説明 次のような記法もよく使われる。\n$$ J = \\dfrac{\\partial (f_{1}, \\dots f_{m})}{\\partial (x_{1}, \\dots, x_{n})} $$\n$\\mathbb{f}$のヤコビ行列は$D \\mathbb{f} := J$になるような演算子$D$を定義して表現されることもある。ヤコビ行列という名称は19世紀ドイツの数学者カール・グスタフ・ヤコブ・ヤコビから来ているので、ヤコビ行列と書いて読むのが正しいが、実際には$J$が\u0026rsquo;ヤコビアン\u0026rsquo;と読まれることが非常に多い。\n全微分とも呼ばれ、多変数ベクトル関数の微分を意味する。したがって、多変数関数にヤコビ行列が存在する場合は、微分可能であるとされ、逆に微分可能な関数 $f : \\mathbb{R} \\to \\mathbb{R}$が$1 \\times 1$サイズのヤコビ行列を持つと考えることもできる。簡単に言えば、ヤコビ行列はベクトル関数の微分係数行列である。\n通常は極座標とともに解析学で最初に接するもので、\n$$ \\int_{B} \\int_{A} f(x,y) dx dy $$\nで使用される直交座標を$x= r \\cos \\theta$, $y= r \\sin \\theta$のように変更すると、ご存じの通り\n$$ \\int_{B} \\int_{A} f( r \\cos \\theta , r \\sin \\theta ) r dr d \\theta $$\nとして$r$が一つ追加される。これは\n$$ \\begin{bmatrix} {{\\partial x } \\over {\\partial r }} \u0026amp; {{\\partial x } \\over {\\partial \\theta }} \\\\ {{\\partial y } \\over {\\partial r }} \u0026amp; {{\\partial y } \\over {\\partial \\theta }} \\end{bmatrix} = \\begin{bmatrix} \\cos \\theta \u0026amp; \\sin \\theta \\\\ -r \\sin \\theta \u0026amp; r \\cos \\theta \\end{bmatrix} $$\nの行列式が$r \\cos^2 \\theta + r \\sin^2 \\theta = r$のように求められるからである。同じセンスで、ヤコビ行列は高校で積分の変数置換を行うときすでに接した概念そのものである。例えば、\n$$ \\int_{0}^{1} ( 27x^3 + 9 x^2 + 3 x ) dx $$\nを計算する場合、$3x = y$のような置換を行うと考えてみる。これを$y$が$x$に対する関数$y(x) = 3x$であると見ると、そのヤコビ行列は\n$$ \\begin{bmatrix} {{\\partial 3x } \\over {\\partial x }} \\end{bmatrix} = \\begin{bmatrix} 3 \\end{bmatrix} $$\nとなる。これは$3x = y$の両辺をそれぞれの変数で微分して$3dx = dy$を得るのと同じである。\n参照 スカラー場の微分係数：デル演算子 座標変換とヤコビアン ","id":989,"permalink":"https://freshrimpsushi.github.io/jp/posts/989/","tags":null,"title":"ヤコビ行列あるいはジャコビ行列とは"},{"categories":"머신러닝","contents":"概要 損失関数の勾配を利用して損失関数の極小値を見つけるアルゴリズムの中でもっとも単純な方法として 勾配降下法Gradient Descent Algorithmがある。\n説明 ただし、このときの損失関数$L$はデータセット$X$が固定された状態での重みとバイアスに対する関数と見なされる。入力データが$\\mathbb{x} \\in \\mathbb{R}^{m}$のように見える場合、$L$は$(w_{1} , w_{2} , \\cdots , w_{m} , b) \\in \\mathbb{R}^{m+1}$に対する関数となる。同じデータであっても重みとバイアスによって損失関数の値は異なり、損失関数が小さくなるということはそれだけ良いモデルを作り出したことを意味する。\n勾配降下法は、このような関数$L$が作り出す多様体に沿って極小値となる最適な重みを見つける。この原理をもう少し厳密に理解したい場合は、数値解析学の勾配降下法について学ぶといい。\n最初に選んだ重みとバイアスのベクトル$\\mathbb{w}_{1} \\in \\mathbb{R}^{m+1}$について損失関数の値をより小さくする$\\mathbb{w}_{2}$は、ある適切な正数$\\alpha$によって $$ \\mathbb{w}_{2} := \\mathbb{w}_{1} - \\alpha \\nabla L (\\mathbb{w}_{1} ) $$ のように計算される。これを繰り返す $$ \\mathbb{w}_{n+1} := \\mathbb{w}_{n} - \\alpha \\nabla L (\\mathbb{w}_{n} ) $$ も損失関数の値を次第に小さくすることができる。これにより$\\mathbb{w}_{n}$を更新することを バックプロパゲーションと呼ぶ。機械学習では$\\alpha$を 学習率Learning Rate, ラーニングレートと呼び、この値によって勾配降下法が成功することも失敗することもある。\n成功する場合とは、上の図のように計算を繰り返しながら$L$が極小値になる重みとバイアスを正確に見つけた場合だ。特に図では極小値でありながら最小値になっているが、一般的に極小値は極小値に過ぎず、最小値であるか確信できるわけではない。\n$\\alpha$が大きすぎると、上のように値が急激に変わり学習のスピードは速くなるが、過度に大きい場合は収束しないことがある。これを オーバーシューティングと呼ぶ。\n反対に$\\alpha$が小さすぎると、数学的には収束性が保証されるが、変化が小さすぎて時間がかかりすぎ、局所最小値に引っかかるとその近くから抜け出すことができない。\nこれが勾配降下法の基本的な概念で、実際には上記のような問題を補うためにさまざまな技術を用いる。\n確率的勾配降下法 ミニバッチごとに勾配降下法を適用することを 確率的勾配降下法stochastic gradient descent, SGDという。ある文献では以下のように説明されている。\nバッチ学習で学習する: バッチ勾配降下法 ミニバッチ学習で学習する: ミニバッチ勾配降下法 オンライン学習で学習する: 確率的勾配降下法 しかし、この区分は実際には無意味だ。一般的にディープラーニングではミニバッチ学習のみが使用され、ミニバッチ学習でバッチサイズを$1$にするとそれがオンライン学習になるためだ。したがって、実際のディープラーニングでは「勾配降下法 = 確率的勾配降下法 = ミニバッチ勾配降下法」と受け入れてもよい。\n「確率的」という言葉に大きな意味を置く必要もない。全データセットを母集団と見なすと、ミニバッチで学習することは標本集団に対して繰り返し学習することと同じなので、確率的と呼んで理解しても問題ない。\n併せて見る 最適化理論の勾配降下法 最適化理論の確率的勾配降下法 ","id":987,"permalink":"https://freshrimpsushi.github.io/jp/posts/987/","tags":null,"title":"機械学習における勾配降下法と確率的勾配降下法"},{"categories":"다변수벡터해석","contents":"定義 集合 $D$ を $n$次元のユークリッド空間の部分集合 $D\\subset \\mathbb{R}^{n}$ とする。\n$D$ を定義域とする関数を多変数関数function of several variablesと呼ぶ。 $f : D \\to \\mathbb{R}$ をスカラー関数scalar functionと呼ぶ。 スカラー関数 $f_{1} , \\cdots , f_{m} : D \\to \\mathbb{R}$ に対して次のように定義された $\\mathbb{f} : D \\to \\mathbb{R}^{m}$ をベクトル値関数vector-valued functionと呼ぶ。 $$ \\mathbb{f} ( x_{1} , \\cdots , x_{n} ) : = \\begin{bmatrix} f_{1} ( x_{1} , \\cdots , x_{n} ) \\\\ \\vdots \\\\ f_{m} ( x_{1} , \\cdots , x_{n} ) \\end{bmatrix} $$ 説明 多変数関数 多変数関数を意味する英語には、function of several variables, multivariable function, multivariate function 等がある。\n多変数関数という表現は特に微分積分学を含む解析学で使われる。もともとスカラー関数であれベクトル値関数であれ、ただの関数に過ぎないが、その値域を簡単に区別するために使われる言葉だ。線型代数学の観点から見れば、ベクトル値関数が $m=1$ であればスカラー関数になると言えるので、概念的な差は全くないと言える。\nスカラー関数 スカラー関数の例として、$ F ( m , a ) := ma$ を考えることができる。$m$ が質量であろうと$a$ が加速度であろうと、数学者の目には $(m , a) \\in \\left( [0,\\infty) \\times \\mathbb{R} \\right) \\subset \\mathbb{R}^2$ のような $2$次元ベクトルに見えるべきだ。$ma$ は単に2つの実数 $m$ と $a$ の積であり、$ma \\in \\mathbb{R}$ であるため、スカラー関数の条件をよく満たしている。一方、ベクトル解析学では、与えられた空間上の全ての点に対してスカラー値が1つずつ対応している点で、スカラー場Scalar Fieldとも呼ばれる。\nベクトル値関数 ベクトル値関数の例として、 $$ \\mathbb{q} ( m , v , a ) : = \\begin{bmatrix} ma \\\\ mv \\\\ {{1} \\over {2}} m v^2 \\end{bmatrix} $$ を考えることができる。物理学者の目には、最初の成分から順に力、運動量、運動エネルギーだと思うかもしれないが、ベクトル値関数として考えれば、単に $\\mathbb{q} : D \\to \\mathbb{R}^3$ に過ぎない。一方、ベクトル解析学では、与えられた空間上の全ての点に対してベクトルが1つずつ対応している点で、ベクトル場Vector Fieldとも呼ばれる。\n","id":970,"permalink":"https://freshrimpsushi.github.io/jp/posts/970/","tags":null,"title":"スカラー関数とベクトル値関数"},{"categories":"머신러닝","contents":"定義 データ$Y = \\begin{bmatrix} y_{1} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}$の推定値が$\\widehat{Y} = \\begin{bmatrix} \\widehat{ y_{1} } \\\\ \\vdots \\\\ \\widehat{y_{n}} \\end{bmatrix}$として与えられた時、データとその推定値の乖離を表すスカラー関数$L : \\mathbb{R}^{n} \\to [ 0 , \\infty )$を損失関数と呼ぶ。\n別名 損失関数は、学習を通じて得たデータの推定値が実際のデータとどれだけ違うかを評価する指標として使用される。この値が大きいほど、より間違えている意味であり、この値が$0$であることは、「無損失」つまり完璧に推定できることを意味する。これは数学で言うメトリックと大きく異なるわけではない。\nもともと経済学で先に使われた言葉なので、$L$は時々**コスト関数Cost Function**とも呼ばれる。\n種類 次の二つは代表的な損失関数であり、使用する際に適切に知っておくだけで十分である。\nMSE（平均二乗誤差） $$ L \\left( \\widehat{Y} \\right) := {{1} \\over {n}} \\sum_{i=1}^{n} ( y_{i} - \\widehat{ y_{i} } )^2 $$ MSEは、歴史がある損失関数であり、$y_{i} \\in \\mathbb{R}$時に有意義に使用できる。\nクロスエントロピー $$ L \\left( \\widehat{Y} \\right) := - {{1} \\over {n}} \\sum_{i=1}^{n} \\left\u0026lt; y_{i} , \\log \\left( \\sigma ( \\hat{y_{i}} ) \\right) \\right\u0026gt; $$\nクロスエントロピーは、いわゆるワンホットエンコーディングを行う時に効果的な手段となり、$Y$がカテゴリカルデータである場合、$\\widehat{Y}$が推計した各カテゴリの確率を用いて計算する。主に分類問題に使用される。\nワンホットエンコーディングとは、単に標準基底へのマッピングを意味する。$m$クラスがある場合、$\\mathbb{R}^{n}$の標準基底は$\\beta = \\left\\{ e_{i} \\right\\}_{i=1}^{m}$で表記されると、各々の$y_{i}$と$\\hat{y_{i}}$は\n$$ y_{i} \\in \\mathbb{R}^{m},\\qquad \\hat{y}_{i} \\in \\beta $$\nのようなベクトルで表される。例えば、$Y$が３つのクラスを持つ場合、$y_{i}$が1番の分類に属しているなら$y_{i} = [1,0,0]^{t}$、$3$番の分類に属していれば$y_{i} = [0,0,1]^{t}$のように表される。\n$\\sigma$はソフトマックス関数であり、与えられたベクトルの値を$[0,1]$にバウンドして、確率分布の条件を満たすようにする関数だ。$\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$は内積だ。$\\sigma ( \\hat{y_{i}} ) \\in [0,1]$なので、$\\log \\left( \\sigma ( \\hat{y_{i}} ) \\right) \u0026lt; 0$であり、$y_{i}$は$0$か$1$なので $$ L \\left( \\widehat{Y} \\right) = - {{1} \\over {n}} \\sum_{i=1}^{n} \\left\u0026lt; y_{i} , \\log \\left( \\sigma ( \\hat{y_{i}} ) \\right) \\right\u0026gt; \\ge 0 $$ 容易に確認できる。\n$y_{i} = ( y_{i1} , \\cdots , y_{ij} , \\cdots , y_{im} )$の推定値$\\hat{ y_{i} } = ( \\hat{ y_{i1} } , \\cdots , \\hat{y_{ij}} , \\cdots , \\hat{y_{im}} )$の各成分は、確率が高いほど大きな値、低ければ低い値を取る。これをソフトマックス関数に入れると、確率が高いほど$1$に近く、低いほど$0$に近い値に調整される。実際の成分が$1$であるが、確率を低く計算した場合$- 1 \\cdot \\log (c)$は$c\\ll 1$となり、かなり大きな値になる。逆に、実際の成分が$0$であり、確率を高く計算しても$- 0 \\cdot \\log(c)$は大きな意味を持たない。従って、間違えれば間違えるほど、クロスエントロピーが急騰することが容易に予想できる。\n参照 数値解析における損失関数 ","id":967,"permalink":"https://freshrimpsushi.github.io/jp/posts/967/","tags":null,"title":"機械学習における損失関数"},{"categories":"그래프이론","contents":"定義1 頂点とそれらを結ぶ線から成る集合をグラフまたはネットワークと呼ぶ。頂点の集合を$V$、線の集合を$E$としよう。 $V(G) := V$の要素を$G$の ヴァーテックスVertexまたは ノードNodeと呼ぶ。 $E(G) := E$の要素を$G$の エッジEdgeまたは リンクLinkと呼ぶ。 自分自身に繋がるエッジをループLoopと呼ぶ。 二つのヴァーテックスがエッジで繋がっている場合、隣接しているAdjacentと言う。 エッジに向きがあるグラフを有向グラフDigraphと言う。 有限なヴァーテックスを持ち、二つのヴァーテックスを結ぶエッジが一つだけで、ループが存在せず、有向グラフでないグラフを単純グラフSimple Graphと言う。 説明 必ずしもそうではないが、同じ概念であっても純粋数学ではグラフという言葉を好んで用い、応用数学ではネットワークという言葉を好んで用いる傾向がある。ただし、どちらかと言えば、同義語があり、それぞれの分野ではかなりの影響力を持っているので、混在して使うことはあまりない。\n一般的に言うグラフとは、上の図のように自由な形をしている。 黒い丸はそれぞれのヴァーテックスを意味し、位置の概念は持たない。純粋なグラフ理論では、グラフのオーダーOrderは通常、このヴァーテックス集合の基数 $n = |V(G)|$を指す。上のグラフのオーダーは$5$である。 ヴァーテックスを結ぶエッジも同様に、その関係のみを表すもので、形や長さの概念はない。ちなみに、エッジは通常、韓国人が直感的に思うように[エッヂ]ではなく[エッジ]と発音されるのが正しい。純粋なグラフ理論では、グラフのサイズSizeは通常、このエッジ集合の基数$m = |E(G)|$を指す。上のグラフのサイズは$8$である。しかし、応用ネットワーク理論では、サイズは単にグラフのオーダー$|V(G)|$を呼ぶことが多い。これは分野による文脈で区別する必要がある。 左上のヴァーテックスは自分自身に繋がるエッジを持ち、このためにループと呼ばれる。 二つのヴァーテックスが隣接しているとは、エッジで繋がっているということを意味し、再び、その関係のみが重要であり、目に見える距離は重要ではない。二つのヴァーテックス$u, v$が隣接している場合、$u \\sim v$のように表され、グラフ$G$でヴァーテックス$v \\in V(G)$に隣接するヴァーテックスを集めた集合を$v$のネイバーフッド$N_{G} (v)$のように表現することもある。 有向グラフとは、上の図のようにエッジに方向性があるグラフを言う。有向グラフでは、エッジはアークArcとも呼ばれる。ヴァーテックス$u$から$v$へ入るエッジは$u \\to v$と表記され、$u$をテイルtail、$v$をヘッドheadと呼ぶ。 単純グラフという言葉は難しそうだが、簡単に言えば、ループやマルチエッジ、ディレクションなどがなく、上の図のようなきれいなグラフを指す。一般に、グラフ理論と言えば、このような単純な形に興味を持つのが普通である。 難しい定義 これらの定義は、グラフの概念を簡単に説明するが、厳密さにはいくらか問題がある。したがって、以下のより複雑な定義を紹介する。概念は文字通り上で簡単に定義されたものと同じなので、数学的な表現に慣れていれば理解するのに大きな困難はないだろう。\n集合 $V \\ne \\emptyset$ と 二項関係 $\\sim \\subset V^2$ について$G := \\left( V, \\sim \\right)$をグラフまたはネットワークと呼ぶ。 $V(G) := V$の要素を$G$のヴァーテックスまたは ノードと呼ぶ。 $E(G) := \\sim$の要素を$G$のエッジまたは リンクと呼ぶ。 $v \\in V(G)$に対して$(v,v) \\in E(G)$をループと呼ぶ。 $v_{1} , v_{2} \\in V(G)$について$(v_{1} , v_{2} ) \\in E(G)$であれば$v_{1}$と$v_{2}$が隣接していると言う。 $\\sim$が対称関係でなければ有向グラフDigraphと呼ぶ。 有限集合$V$に対し、対称関係$\\sim \\subset \\left\\{ (v_{1} , v_{2} ) \\in V^2 \\mid v_{1} \\ne v_{2} \\right\\}$をエッジとして、二つのヴァーテックスを結ぶエッジが一つだけのグラフを単純グラフと呼ぶ。 Wilson. (1970). Introduction to Graph Theory: p8~9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":966,"permalink":"https://freshrimpsushi.github.io/jp/posts/966/","tags":null,"title":"数学におけるグラフとネットワーク"},{"categories":"확률론","contents":"定義 $s\u0026lt; t \u0026lt; t+u$ とした時、以下の条件を満たす確率過程 $\\left\\{ W_{t} \\right\\}$ をウィーナー過程と呼ぶ。\n(i): $W_{0} = 0$ (ii): $\\left( W_{t+u} - W_{t} \\right) \\perp W_{s}$ (iii): $\\left( W_{t+u} - W_{t} \\right) \\sim N ( 0, u )$ (iv): $W_{t}$ のサンプルパスはほとんど至る所で連続である。 基本性質 [1]: $\\displaystyle W_{t} \\sim N ( 0 , t )$ [2]: $\\displaystyle E ( W_{t} ) = 0$ [3]: $\\displaystyle \\text{Var} ( W_{t} ) = t$ 4: $\\displaystyle \\text{cov} ( W_{t} , W_{s} ) = E (W_{t}W_{s}) = {{1} \\over {2}} (|t| + |s| - |t-s|) = \\min \\left\\{ t , s \\right\\}$ 説明 ウィーナー過程はブラウン運動Brownian Motionとも呼ばれる。\n(ii): $\\left( W_{t+u} - W_{t} \\right) \\perp W_{s}$ と言うことは\n(iii): 増分が正規分布 $N(0,t)$ に従うということは、ウィーナー過程は特定の時点には関心がなく、二つの時点を比ぼうした時、その時差が大きくなるほど不確実性が大きくなることを意味する。\n(iv): サンプルパスがほとんど至る所で連続であるということは、ウィーナー過程に従うある点があった時、その点が「瞬間移動」する確率が$0$ と見ても良いということだ。難しいなら、突然の跳躍をしないとだけ知っておけば十分だ。\n[1]: 面白い事実は$W_{t}$ の確率密度関数 $$ f_{W_{t}} (x,t) = {{1} \\over { \\sqrt{ 2 \\pi t } }} e^{ - {{x^2} \\over {2t} } } $$ が熱方程式 $$ {{\\partial u } \\over { \\partial t }} = {{1} \\over {2}} {{\\partial^2 u } \\over { \\partial x^2 }} $$ の解になるということだ。\n証明 [1] (i)と(iii)によって、$W_{t} = W_{t} - 0 = W_{t} - W_{0} \\sim N ( 0 , t )$\n■\n[2] [1]により$W_{t}$ が正規分布に従うため、$\\displaystyle E ( W_{t} ) = 0$\n■\n[3] [1]により$W_{t}$ が正規分布に従うため、$\\displaystyle \\text{Var} ( W_{t} ) = t$\n■\n4 $t \u0026gt; s$ とすると、共分散の定義と[2]により $$ \\text{cov} ( W_{t} , W_{s} ) = E \\left( \\left[ W_{t} - E ( W_{t} ) \\right] \\left[ W_{s} - E ( W_{s} ) \\right] \\right) = E \\left( W_{t} W_{s} \\right) $$\n$W_{t} = ( W_{t} - W_{s} ) + W_{s}$ だから\n$$ \\begin{align*} E \\left( W_{t} W_{s} \\right) =\u0026amp; E \\left[ \\left( ( W_{t} - W_{s} ) + W_{s} \\right) \\cdot W_{s} \\right] \\\\ =\u0026amp; E \\left[ ( W_{t} - W_{s} ) \\cdot W_{s} \\right] + E \\left( W_{s}^{2} \\right) \\end{align*} $$\n(ii)と[2]による最初の項は\n$$ E \\left[ ( W_{t} - W_{s} ) \\cdot W_{s} \\right] = E ( W_{t} ) \\cdot E ( W_{t} - W_{s} ) = 0 $$\n[3]による2番目の項は\n$$ E \\left( W_{s}^{2} \\right) - 0^2 = E \\left( W_{s}^{2} \\right) - \\left[ E ( W_{s} ) \\right]^2 = \\text{Var} ( W_{s} ) = s $$\nまとめると、$\\displaystyle \\text{cov} ( W_{t} , W_{s} ) = s$ となる。一方、$s \u0026gt; t$ の時も同じ結果が得られるため\n$$ \\text{cov} ( W_{t} , W_{s} ) = \\min \\left\\{ t , s \\right\\} $$\n■\n","id":957,"permalink":"https://freshrimpsushi.github.io/jp/posts/957/","tags":null,"title":"ウィーナープロセス"},{"categories":"푸리에해석","contents":"定義 $\\mathbb{R}$で定義された二つの関数$f$、$g$が与えられたとする。以下の積分が存在する場合、これを二つの関数$f$、$g$の畳み込みと呼び、$f \\ast g$で示す。\n$$ f \\ast g(x):=\\int _{-\\infty} ^{\\infty} f(y)g(x-y)dy $$\n$f$、$g$が離散関数の場合、以下のように定義する。\n$$ (f \\ast g)(m)=\\sum \\limits_{n}f(n)g(m-n) $$\n説明 畳み込みという翻訳があるが、コンボリューションという言葉の方がよく使われる。一般に上記の定義をコンボリューションとして学ぶが、もう少し一般的に言うと、これは積分変換の一種であるフーリエ変換に対するコンボリューションである。交換法則、分配法則など多くの良い特性を持っているため、様々な分野で使用される。\n離散畳み込みの場合、解析的数論では少し異なる定義をすることもある。\n畳み込みが定義される条件は以下の通り：\n(a)\n$f\\in L^{1}$、$|g|\u0026lt;M$である場合、\n$$ \\left| \\int f(y)g(x-y)dy \\right| \\le \\int \\left| f(y)g(x-y) \\right|dy \\le M\\int \\left| f(y) \\right|dy \\lt \\infty $$\n(b)\n$\\left| f \\right| \\le M$、$g\\in L^{1}$である場合、\n$$ \\left| \\int f(y)g(x-y)dy \\right| \\le \\int \\left| f(y) g(x-y) \\right|dy \\le M\\int \\left| g(x-y) \\right|dy \\lt \\infty $$\n(c)\n$f,g\\in L^{2}$であり、$\\tilde{g}_{x}(y)=g(x-y)$とする。すると$\\tilde{g}_{x}\\in L^{2}$、$\\left\\| g \\right\\|_{2}=\\left\\| \\tilde{g}_{x} \\right\\|_{2}$であり、コーシー・シュワルツの不等式により\n$$ \\begin{align*} \\left| \\int f(y)g(x-y)dy \\right| \u0026amp;= \\left| \\int f(y)\\tilde{g}_{x}(y)dy \\right| \\\\ \u0026amp; = \\left| \\left\\langle f,\\tilde{g}_{x} \\right\\rangle \\right| \\\\ \u0026amp;\\le \\left\\| f \\right\\|_{2} \\left\\| \\tilde{g}_{x} \\right\\|_{2} \\\\ \u0026amp;\u0026lt;\\infty \\end{align*} $$\n(d)\n$f$が閉区間$[a,b]$を除き$0$で有界であり、$g$が区分的に連続である場合、\n$$ \\int _{-\\infty} ^{\\infty} f(y)g(x-y)dy=\\int _{a}^{b}f(y)g(x-y)dy\u0026lt;\\infty $$\n","id":1000,"permalink":"https://freshrimpsushi.github.io/jp/posts/1000/","tags":null,"title":"畳み込みの定義"},{"categories":"편미분방정식","contents":"定義1 $\\ U \\in \\mathbb{R}^n$は開集合 $\\ x\\in U$ $u=u(x) : \\overline{U} \\rightarrow \\mathbb{R}^n$ ラプラス方程式 下の偏微分方程式をラプラス方程式という。\n$$ \\Delta u=0 $$\nここで、$\\Delta$はラプラシアンである。ラプラス方程式を満たす$u$を特に調和関数という。\nポアソン方程式 非同次ラプラス方程式をポアソン方程式という。\n$$ -\\Delta u = f $$\n説明 ラプラス方程式は物理学の様々な場所に現れる。通常、$u$は平衡状態でのある物理量の密度を意味する。平衡状態で、$V \\subset U$とするとき、以下の式が成り立つ。\n$$ \\int_{\\partial V}\\mathbf{F} \\cdot \\boldsymbol{\\nu}dS=0 $$\n$\\mathbf{F}$は$u$のフラックス密度、$\\boldsymbol{\\nu}$は外向き単位法線ベクトルである。\nこの式の意味は、$u$の正味のフラックスは$0$であるということである。例えば、熱平衡状態にある何か空間があるとする。その空間の外から内へ入る熱もなく、内から外へ出る熱もない。つまり、その空間の境界面で熱の流れがないということである。この話は正味のフラックスが$0$であるという話と同じである。ここでグリーン・ガウスの定理を適用すると、次の式を得る。\n$$ 0 = \\int_{\\partial V} \\mathbf{F} \\cdot \\nu dS=\\int_{V} \\nabla \\cdot \\mathbf{F} dx \\\\ \\implies \\nabla \\cdot \\mathbf{F}=0 $$\nここで、$\\mathbf{F}$が$u$の勾配 $Du$に比例する値だとしよう。多くの場合、物理的な理由から逆方向を仮定するのが都合がいい。熱力学の第二法則(熱は常に高い所から低い所へ流れる)を例に挙げることができる。\n$$ \\begin{equation} \\mathbf{F}=-aDu \\label{eq1} \\end{equation} $$\nこのとき、$a\u0026gt;0$である。\nもし、$u$が化学物質の濃度、温度、静電気ポテンシャルを意味するなら、$\\eqref{eq1}$はそれぞれフィックの拡散法則、フーリエの熱伝導法則、オームの法則を意味する。\n以上の内容からラプラス方程式が導かれる。\n$$ \\nabla \\cdot \\mathbf{F} = \\nabla \\cdot (-aDu)=-a\\Delta u=0 \\\\ \\implies \\Delta u = 0 $$\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p20-21\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":997,"permalink":"https://freshrimpsushi.github.io/jp/posts/997/","tags":null,"title":"ラプラス方程式とポアソン方程式"},{"categories":"통계적분석","contents":"モデル 1 白色雑音 $\\left\\{ e_{t} \\right\\}_{t \\in \\mathbb{N}}$ について、 $$ \\nabla^{d} Y_{t} := \\sum_{i = 1}^{p} \\phi_{i} \\nabla^{d} Y_{t-i} + e_{t} - \\sum_{i = 1}^{q} \\theta_{i} e_{t-i} $$ のように定義された $\\left\\{ Y_{t} \\right\\}_{ t \\in \\mathbb{N} }$ を $(p,d,q)$次のアリマ過程 $ARIMA (p,d,q)$ と言います。このような形の時系列分析モデルを アリマモデル と呼びます。\n説明 $ARI(p,d) \\iff ARIMA(p,d,0)$ を アリモデル、$IMA(d,q) \\iff ARIMA(0,d,q)$ を イマモデル ということもあるが、あまり使用されない。むしろ、$ARIMA(p,d,0)$ や $ ARIMA(0,d,q)$ のような表現を好んで使用する。\n式が難しそうに見えるが、思ったより難しくないんだ。ただ アルマモデル $$ Y_{t} = \\sum_{i = 1}^{p} \\phi_{i} Y_{t-i} + e^{t} - \\sum_{i = 1}^{q} \\theta_{i} e_{t-i} $$ で$Y_{t}$ が $\\nabla^{d} Y_{t}$ に変わっただけだから。ただ$d$ 回の差分を通じて定常性を得たデータをアルマモデルで分析すると見ればいい。\nCryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p992.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":941,"permalink":"https://freshrimpsushi.github.io/jp/posts/941/","tags":null,"title":"アリマモデル"},{"categories":"해석개론","contents":"定義 関数 $f$が区間 $I$で以下の条件を満たすとき、$f$は区間 $I$で片方向に連続piecewise continuousだと言われる。\n有限個の不連続点 $x_{1},\\ x_{2},\\ \\cdots ,\\ x_{n} \\in I$を持つ。\n不連続点で左極限と右極限を持つ。\n$$ \\left|\\lim \\limits_{x\\rightarrow x_{i}^{+}} f(x) \\right| \u0026lt; \\infty \\quad \\text{and} \\quad \\left|\\lim_{x \\rightarrow x_{i}^{-}}f(x)\\right|\u0026lt;\\infty \\quad (i=1,\\ \\cdots ,\\ n) $$\n関数 $f$が有限個の不連続点を除いたすべての場所で無限回微分可能なら片方向に滑らかpiecewise smoothだと言う。\n説明 条件 1.を満たしているだけでも片方向に連続と言う場合もある。簡単に言えば、不連続点を基準に関数を分割したとき、分割された各関数が全部連続なら片方向に連続である。\n片方向に連続、区間ごとに連続、分割的に連続など、様々な訳がある。\n","id":972,"permalink":"https://freshrimpsushi.github.io/jp/posts/972/","tags":null,"title":"各断片ごとの連続性、各断片ごとの滑らかさ"},{"categories":"통계적분석","contents":"定義 1 オペレーター $B$ を $B Y_{t} = Y_{t-1}$ のように定義し、バックシフトと呼ぶ。 オペレーター $\\nabla$ を $\\nabla := 1 - B$ および $\\nabla^{r+1} = \\nabla \\left( \\nabla^{r} Y_{t} \\right)$ のように定義し、差分と呼ぶ。 説明 差分の定義によると、$1$次の差分は $$ \\nabla Y_{t} = Y_{t} - Y_{t-1} $$ のように計算され、$2$次の差分は $$ \\begin{align*} \\nabla^2 Y_{t} =\u0026amp; \\nabla \\left( \\nabla Y_{t} \\right) \\\\ =\u0026amp; \\nabla \\left( Y_{t} - Y_{t-1} \\right) \\\\ =\u0026amp; \\nabla Y_{t} - \\nabla Y_{t-1} \\\\ =\u0026amp; ( Y_{t} - Y_{t-1} ) - ( Y_{t-1} - Y_{t-2} ) \\\\ =\u0026amp; Y_{t} - 2 Y_{t-1} + Y_{t-2} \\end{align*} $$ のように計算される。つまり、$Y_{t}$ に差分を2回適用したからといって、$Y_{t} - Y_{t-2}$ になるわけではない。このように連続して長くなる差分は季節性差分として別に定義される。\n時系列で差分が必要な理由は、トレンドがあるデータを扱う際に便利だからである。時系列分析におけるトレンドとは「データの値が一定期間にわたって増加または減少する傾向」を指し、この場合は定常性に問題がある。したがって、データが定常性を持つように適切に差分を取る前処理を行う。単純に増加または減少する程度であれば、一度で十分であり、複雑な形状を持つ場合はそれだけ多くの差分を取る必要があるかもしれない。\n差分を取るのが適切か、どれくらい取るべきか確信が持てない場合、通常はディッキー-フラー検定を使用し、逆になぜこれ以上差分を取る必要がないのかを正当化するのにも使える。\n実習 TSAパッケージのoil.priceデータを見てみよう。\noil.priceは1986年から2005年までの原油価格に関するデータだ。後期になるほど急騰しているので、定常性が欠けていると言える。このようなデータは分析が難しいため、差分を取ってトレンドを取り除く。\nRで差分を取る方法はとても簡単だ。diff()関数を使えば、最初の観測値を落として差分化されたデータを返す。あまり使用されないが、lag=nというオプションを与えることで、$n$回の差分も簡単に行うことができる。\n差分を取った結果、変動自体は依然として激しいが、平均的には$0$の近くで動いていることが確認できる。\nコード 以下はRの例示コードだ。\nlibrary(TSA)\rdata(oil.price); oil.price\rwin.graph(4,4); plot(oil.price,main=\u0026#39;oil.price\\\u0026#39;)\rdiff(oil.price)\rwin.graph(4,4); plot(diff(oil.price),main=\u0026#39;∇oil.price\\\u0026#39;)\rdiff(oil.price,lag=2) 参照 数値解析における差分 Cryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p90.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":916,"permalink":"https://freshrimpsushi.github.io/jp/posts/916/","tags":["R"],"title":"時系列分析における差分"},{"categories":"통계적분석","contents":"モデル 1 白色雑音 $\\left\\{ e_{t} \\right\\}_{t \\in \\mathbb{N}}$ について $$ Y_{t} := \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + \\cdots + \\phi_{p} Y_{t-p} +e_{t} - \\theta_{1} e_{t-1} - \\theta_{2} e_{t-2} - \\cdots - \\theta_{q} e_{t-q} $$ として定義される、$(p,q)$次の自己回帰移動平均過程 $ARMA(p,q)$ と呼ばれる。\n説明 アルマモデルは、単純に移動平均過程と自己回帰過程を組み合わせた形をしている。例えば $(1,1)$次であれば、 $$ ARMA(1,1) : Y_{t} = \\phi Y_{t-1} + e_{t} - \\theta e_{t-1} $$ となる式だ。しかし、アルマモデルはまだモデルとして不足している点があるため、差分を通じて改善されたアリマモデルを主に使用する。もちろん、本質的にはすべてアルマモデルとして結論づけられる。\nCryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p77.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":914,"permalink":"https://freshrimpsushi.github.io/jp/posts/914/","tags":null,"title":"自己回帰移動平均モデル"},{"categories":"상미분방정식","contents":"定義 次の微分方程式をチェビシェフChebyshev 微分方程式という。\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -x\\dfrac{dy}{dx}+n^2 y=0 $$\n説明 係数に独立変数 $x$が含まれる形式であり、解がべき級数の形であると仮定すると、解くことができる。チェビシェフ方程式の解をチェビシェフ多項式と言い、解は一般的に$T_{n}(x)$と表される。\n解法 $$ \\begin{equation} (1-x^2)y^{\\prime \\prime} -xy^{\\prime}+\\lambda^2 y=0 \\label{1} \\end{equation} $$\n上で示したチェビシェフ微分方程式の解を次のように仮定しよう。\n$$ y=a_{0}+a_{1}(x-x_{0})+a_2(x-x_{0})^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}(x-x_{0})^n $$\nこのとき$x=0$であるとき$y^{\\prime \\prime}$の係数が$(1-x^2)|_{x=0}=1\\ne 0$であるので、$x_{0}=0$としよう。すると\n$$ \\begin{equation} y=a_{0}+a_{1}x+a_2x^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}x^n \\label{2} \\end{equation} $$\nべき級数解として解法を始めるが、解法の最後に実際には$y$の項が有限であることが分かる。今$\\eqref{1}$に代入するために、$y^{\\prime}$と$y^{\\prime \\prime}$を求めよう。\n$$ y^{\\prime}=a_{1}+2a_2x+3a_{3}x^2+\\cdots=\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1} $$\n$$ y^{\\prime \\prime}=2a_2+3\\cdot 2a_{3}x+4\\cdot 3 a_{4}x^2 +\\cdots = \\sum \\limits_{n=2} n(n-1)a_{n}x^{n-2} $$\n$\\eqref{1}$に$y, y^{\\prime}, y^{\\prime \\prime}$を代入すると次のようになる。\n$$ (1-x^2)\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n一番目の項の係数$(1-x^2)$の括弧を外して整理すると\n$$ \\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x^2\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n = 0 $$\n$$ \\implies \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n = 0 $$\nここでのポイントは$x$の次数を合わせることである。残りはすべて$x^n$として表されるが、最初の級数だけが$x^{n-2}$で表されているので、$n$の代わりに$n+2$を代入すると\n$$ \\sum \\limits_{n=0} ^\\infty (n+2)(n+1)a_{n+2}x^{n} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n二番目の級数が$x^2$項から始まるので、残りの級数から$n=0,1$の項を取り除いて、定数項は定数項同士、一次項は一次項同士をまとめると\n$$ \\left[ 2\\cdot 1 a_2+\\lambda^2 a_{0} \\right]+\\left[ 3\\cdot 2 a_{3}-a_{1}+\\lambda^2a_{1} \\right]x \\\\ + \\sum \\limits_{n=2}^\\infty \\left[ (n+2)(n+1)a_{n+2}-n(n-1)a_{n}-na_{n}+\\lambda^2a_{n} \\right] x^n=0 $$\n上の式が成り立つためには、すべての係数が$0$でなければならない。\n$$ 2\\cdot 1 a_2+\\lambda^2 a_{0} = 0 $$\n$$ 3\\cdot 2 a_{3}-a_{1}+\\lambda^2a_{1} =0 $$\n$$ (n+2)(n+1)a_{n+2}-n(n-1)a_{n}-na_{n}+\\lambda^2a_{n}=0 $$\nそれぞれを整理すると\n$$ \\begin{align} a_2 \u0026amp;= -\\dfrac{\\lambda^2}{2 \\cdot 1}a_{0} \\label{3} \\\\ a_{3} \u0026amp;=-\\dfrac{\\lambda^2-1^2}{3\\cdot 2} a_{1} \\label{4} \\\\ a_{n+2} \u0026amp;= -\\dfrac{\\lambda^2-n^2}{(n+2)(n+1)}a_{n} \\label{5} \\end{align} $$\n漸化式$\\eqref{5}$を得たので、$a_{0}$と$a_{1}$の値さえ分かれば、すべての係数が分かる。$\\eqref{3}, \\eqref{5}$から偶数次の項の係数を求めると\n$$ \\begin{align*} a_{4} \u0026amp;= -\\dfrac{\\lambda^2-2^2}{4\\cdot 3}a_2=\\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}a_{0} \\\\ a_{6} \u0026amp;= -\\dfrac{\\lambda^2-4^2}{6\\cdot 5}a_{4}= -\\dfrac{\\lambda^2(\\lambda^2-2^2)(\\lambda^2-4^2)}{6!}a_{0} \\\\ \u0026amp;\\vdots \\end{align*} $$\nここで$n=2m (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m}=(-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!}a_{0} $$\n同様に$\\eqref{4}, \\eqref{5}$から奇数次の項の係数を求めると\n$$ \\begin{align*} a_{5} \u0026amp;= -\\dfrac{\\lambda^2-3^2}{5\\cdot 4}a_{3}=\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}a_{1} \\\\ a_{7} \u0026amp;= -\\dfrac{\\lambda^2-5^2}{7\\cdot 6 }a_{5}=-\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)(\\lambda^2-5^2)}{7!}a_{1} \\\\ \u0026amp;\\vdots \\end{align*} $$\nここで$n=2m+1 (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m+1}=(-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!}a_{1} $$\nこのように求めた係数を$\\eqref{2}$に代入して解を求めると\n$$ \\begin{align*} y = \u0026amp;a_{0}+a_{1}x -\\dfrac{\\lambda^2}{2!}a_{0}x^2-\\dfrac{\\lambda^2-1^2}{3!} a_{1}x^3 + \\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}a_{0}x^4 \\\\ \u0026amp;+\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}a_{1}x^5+ \\cdots +(-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!}a_{0}x^{2m} \\\\ \u0026amp;+(-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!}a_{1}x^{2m+1}+\\cdots\\quad(m=1,2,3,\\cdots) \\end{align*} $$\nこのとき偶数次の項は$a_{0}$で、奇数次の項は$a_{1}$でまとめると\n$$ \\begin{align*} y\u0026amp;=a_{0}\\left[1-\\dfrac{\\lambda^2}{2!}x^2+\\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}x^4+\\sum \\limits_{m=3}^\\infty (-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!} x^{2m} + \\cdots \\right] \\\\ \u0026amp; + a_{1}\\left[x-\\dfrac{\\lambda^2-1^2}{3!}x^3+\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}x^5+\\sum \\limits_{m=3}^\\infty (-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!} x^{2m+1} + \\cdots\\right] \\end{align*} $$\n最初の括弧を$y_{0}$、二番目の括弧を$y_{1}$とすると、チェビシェフ方程式の一般解は次のようになる。\n$$ y=a_{0}y_{0}+a_{1}y_{1} $$\n二つの級数$y_{0}$と$y_{1}$は比率判定法により$|x|\u0026lt;1$の範囲で収束することが分かる。$\\eqref{5}$により$\\dfrac{a_{n+2}}{a_{n}}=\\dfrac{n^2-\\lambda^2}{(n+2)(n+1)}=\\dfrac{n^2-\\lambda^2}{n^2+3n+2}$であるため比率判定法を使うと\n$$ \\lim \\limits_{n \\rightarrow \\infty} \\dfrac{n^2-\\lambda^2}{n^2+3n+2}x^2=x^2\u0026lt;1 $$\n$$ \\implies -1\u0026lt;x\u0026lt;1 $$\nしかし、多くの問題では$x=\\cos \\theta$、$\\lambda$は非負の整数の形で表され、すべての$\\theta$に対して収束する解を求めることが目標である。すなわち、$x=\\pm 1$でも収束する解を見つけることが目標である。幸いにも$\\lambda$が整数の場合、求める解が存在するが、このとき$\\lambda$の値によって必ず$y_{0}, y_{1}$のどちらか一方の解のみが存在する。$\\lambda$が$0$または偶数の場合は$y_{1}$が発散し、$y_{0}$は偶数次の項だけを持つ有限項の多項式となる。$\\lambda$が奇数の場合は$y_{0}$が発散し、$y_{1}$は奇数次の項だけを持つ有限項の多項式となる。表で整理すると以下のようになる。\n$\\lambda$の値 $y_{0}$ $y_{1}$ 方程式の解 $0$または偶数 有限項の多項式 発散 $y=a_{0}y_{0}$ 奇数 発散 有限項の多項式 $y=a_{1}y_{1}$ ケース 1. $\\lambda$が$0$または偶数の場合\n$\\lambda=0$の場合、2次項から$\\lambda^2$を因数として持っており、すべて$0$になるため$y_{0}=1$\n$\\lambda=2$の場合、4次項から$(\\lambda^2-2^2)$を因数として持っており、すべて$0$になるため$y_{0}=1-x^2$\n$\\lambda=4$の場合、6次項から$(\\lambda^2-4^2)$を因数として持っており、すべて$0$になるため$y_{0}=1-8x^2+8x^4$\nそして$\\lambda=0$の場合、$x=1$の$y_{1}=1+\\frac{1}{3!}+\\frac{1\\cdot3^2}{5!}+\\cdots$は発散する。他の偶数の場合も同じである。したがって、$\\lambda$が$0$または偶数の場合は、解が偶数次の項のみを持つ有限項の多項式となる。つまり、級数$y_{0}$の特定の項までのみ残る形の解を得る。$\\lambda$が奇数の場合は、反対の結果を得る。\nケース 2. $\\lambda$が奇数の場合\n$\\lambda=1$の場合、3次項から$(\\lambda^2-1^2)$を因数として持っており、すべて$0$になるため$y_{1}=x$\n$\\lambda=3$の場合、5次項から$(\\lambda^2-3^2)$を因数として持っており、すべて$0$になるため$y_{1}=-3x+4x^3$\n$\\lambda=5$の場合、7次項から$(\\lambda^2-5^2)$を因数として持っており、すべて$0$になるため$y_{1}=5x-20x^3+16x^5$\n$\\lambda=1$の場合、$x^2=1$の$y_{0}$は発散する。他の奇数の場合も同じである。したがって、$\\lambda$が奇数の場合は、解が奇数次の項のみを持つ有限項の多項式となる。つまり、級数$y_{1}$の特定の項までのみ残る形の解を得る。\nそして$\\lambda$が負の場合は、$\\lambda$が正の場合と同じであることが、$y_{0}$と$y_{1}$を見ると分かる。例えば、$\\lambda=2$の場合と$\\lambda=-2$の場合が同じであり、$\\lambda=1$の場合と$\\lambda=-1$の場合が同じである。したがって、$\\lambda$は非負の整数の範囲で考えればよい。$a_{0}$と$a_{1}$の値をうまく選択して、$x=1$のときの解が$y(x)=1$になるようにすれば、これをチェビシェフ多項式Chebyshev polynomialと言い、通常$T_{n}(x)$と表記される。初めのいくつかのチェビシェフ多項式は以下のようである。\n$$ \\begin{align*} T_{0}(x) \u0026amp;= 1 \\\\ T_{1}(x) \u0026amp;= x \\\\ T_2(x) \u0026amp;= 2x^2-1 \\\\ T_{3}(x) \u0026amp;= 4x^3-3x \\\\ T_{4}(x) \u0026amp;= 8x^4-8x^2+1 \\\\ T_{5}(x) \u0026amp;= 16x^5-20x^3+5x \\\\ \\vdots \u0026amp; \\end{align*} $$\n関連項目 チェビシェフ微分方程式とチェビシェフ多項式 ","id":955,"permalink":"https://freshrimpsushi.github.io/jp/posts/955/","tags":null,"title":"チェビシェフ微分方程式の直列解法"},{"categories":"통계적분석","contents":"定義 1 iid (同一分布による独立変数)の確率変数 $e_{t}$ の数列 $\\left\\{ e_{t} \\right\\}_{t = 1}^{\\infty}$ を ホワイトノイズWhite Noiseと呼ぶ。\niidは、independent identically distributed（同一分布による独立）の略で、互いに独立でありながら、同じ分布を共有していることを意味する。 説明 確率変数の数列であるという定義に従って、自然に確率過程となる。特に$E ( e_{t} ) = 0$ならば、$Y_{t} : = \\begin{cases} e_{1} \u0026amp; , t=1 \\\\ Y_{t-1} + e_{t} \u0026amp; , t \\ne 1 \\\\ \\end{cases}$として定義された確率過程$\\left\\{ Y_{t} \\right\\}_{t = 1}^{\\infty}$はランダムウォークになる。\n統計学では、観察された現象に対して100%完全な説明は不可能であると認識されている。問題が完全に説明できるのであれば、統計学を用いて解決する必要もなかったはずだ。どのようなモデルを立てても、避けられない誤差が発生し、統計学ではこれを「情報が不足していること」と受け取る。情報が多ければ多いほどいいが、宇宙の全てを知ることは不可能であり、実際に使用する際にはコストの問題も発生する。\nそういう意味で、ホワイトノイズは時系列分析で発生する「避けられない誤差」と見なされる。データは理想的に作られたものではなく、現実から得られたものなので、必ず存在する。初めは無視できるかもしれないが、時間が経つにつれて蓄積され、かなり大きくなっているかもしれない。そのため、時系列分析における予測は、遠い未来になるほど信頼区間が広がり、その意味を失っていく。\n参照 確率過程から見たランダムウォーク Cryer. (2008). Time Series Analysis: With Applications in R(第2版): p17.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":904,"permalink":"https://freshrimpsushi.github.io/jp/posts/904/","tags":null,"title":"時系列分析におけるホワイトノイズ"},{"categories":"양자역학","contents":"波動関数 波動関数wave functionは、量子力学で時間、位置に依存する粒子の運動状態を表す関数だ。通常、$u$、$\\psi$、$\\Psi$で表される。生エビ寿司店では、位置と時間に関する波動関数を$\\psi (x,t)$で表していて、時間に関係なく、位置についての波動関数は$u(x)$で表される。\n確率的解釈 波動関数で粒子の状態を理解する方法は、マックス・ボルンMax Bornの統計学的（確率的）解釈に基づいている。ここで、波動関数の大きさの二乗をある区間で積分した値に、その区間で粒子を発見する確率という意味を与える。\n$$ \\int _{a} ^b |\\psi (x,t)|^2dx \\\\[1em] = \\text{The probability that a particle exists in the interval } [a,b] \\text{ at time } t $$\nつまり、量子力学では、$\\left| \\psi (x,\\ t) \\right|^2$を時間が$t$の時、どこかの地点$x$で粒子が存在する確率密度関数として扱う。したがって、上の式は時間が$t$の時に区間$[a, b]$で粒子が存在する確率を意味する。そうすると、粒子はどこかには確かに存在するので、全体区間に対する積分値は$1$でなければならない。\n$$ \\int_{-\\infty}^{\\infty} |\\psi (x,\\ t)|^2 dx=1 $$\nこの条件は、波動関数を確率的に解釈する視点から出てきた。\n正規化 しかし、以下の式を見ると、$\\psi$がシュレディンガー方程式を満たす時、その定数倍である$a\\psi$もシュレディンガー方程式を満たすことが分かる。\n$$ H\\psi = E\\psi \\implies aH\\psi = aE\\psi \\implies H(a\\psi) = E(a\\psi) $$\n$a\\psi$に対して上の解釈を適用すると、${\\displaystyle \\int _{-\\infty}^{\\infty}} |a\\psi|^2 dx=a^2 \\ne 1$となってしまい、この値を確率と解釈できなくなる。したがって、波動関数の大きさを調整して、波動関数の全区間に対する積分値を$1$にすることで、確率的な意味を与えなければならない。これを正規化normalizationという。\n量子力学では、波動関数を扱う際には、必ず正規化を行う必要がある。例えば、ある波動関数$\\psi$に対して積分が以下のようだとする。\n$$ \\begin{equation} \\int_{-\\infty}^{\\infty} |\\psi|^2dx=9 \\end{equation} $$\nそのまま扱うのではない。両辺を$9$で割れば、${\\displaystyle \\int_{-\\infty}^{\\infty} }|\\frac{1}{3}\\psi|^2dx=1$となり、確率的な解釈が可能な形になる。ここで$\\psi$を正規化すると$\\frac{1}{3}\\psi$になり、$\\frac{1}{3}\\psi$を正規化された波動関数という。量子力学で扱う関数は、正規化された$\\frac{1}{3}\\psi$だ。\n二乗可積分 一方、波動関数の確率密度の積分値が$(1)$のように$1$でない場合も問題にならない。正規化を通じて大きさを調整すればいいからだ。問題になるのは、積分値が発散する場合だ。したがって、シュレディンガー方程式を満たす波動関数は、以下の数式を満たさなければならない。\n$$ \\int_{-\\infty}^{\\infty} |\\psi (x,\\ t)|^2dx \u0026lt;\\infty $$\nこの条件を満たす波動関数を二乗可積分なsquare-integrable関数という。二乗可積分な波動関数は$x \\rightarrow \\pm \\infty$の時、関数値が$0$に収束しなければならない。そうでない場合、波動関数のグラフの下の面積が収束しないということで、それは二乗可積分ではないということだ。\n","id":945,"permalink":"https://freshrimpsushi.github.io/jp/posts/945/","tags":null,"title":"量子力学における波動関数の確率的解釈と規格化"},{"categories":"통계적분석","contents":"説明 時系列Time Series とは、簡単に言うと、実際のデータから得られる確率過程と見ることができる。株価指数は時間が経つにつれて不確実性を持ち、その価値が変わるため、時系列の良い例となりうる。時系列分析とは、このように時間の流れに沿った従属変数の動きを理解し、予測することを目的とする分析法だ。\n回帰分析との最大の違いは、回帰分析が独立変数が互いに独立していること、そして変数自体も独立していることを前提としているのに対し、時系列分析は変数が自己相関性を持つことを前提としていることだ。また、回帰分析はデータの順序を全く気にしないが、時系列分析は前のデータが後のデータに影響を与えると見て分析にあたるため、その順序が重要だ。\n株も自己相関性の例としても良い。コスダック市場で株の額面が上がるか下がるかは分からないが、今日1株に10,000ウォンなら明日は最大で13,000ウォンまで上がり、7,000ウォンまで下がる。もちろん、これは現行法で30%p以上の変動があってはならないためだが、明日の額面価$Y_{t+1}$は間違いなく今日の額面価格$y_{t} = 10000$に依存している。このように無作為に変動しても、現在までのデータとある程度相関がある変化を捕捉することが時系列分析の目標だ。[ 注：明日の額面価はまだ分からないため確率変数として大文字で表しており、今日の額面価は既に知っているデータであるため小文字で表している。 ]\n","id":900,"permalink":"https://freshrimpsushi.github.io/jp/posts/900/","tags":null,"title":"時系列分析"},{"categories":"전자기학","contents":"定義1 導線のどこかの点を単位時間ごとに通過する電荷の量を電流currentと定義し、$I$と表記する。それゆえ、左に動く負の電荷と右に動く正の電荷は同じ符号の電流である。\n単位時間あたりに流れるクーロンの量をアンペアampereと言う。\n$$ 1 [A] = 1 [C/s] $$\n説明 アンペールはフランス人で、実際の発音は[アンペール]に近い。だからアンペールの法則もアンペールの法則だが、単位として使う場合はアンペアと言わなければならない。\n$I$という記号は、currentのintensityの最初の文字を取ったものである。\n線電流密度 上の図は、線電荷密度が$\\lambda$である電荷が導線を$\\mathbf{v}$の速度で移動する状況を示している。距離=速さx時間であるから、単位長さは$v\\Delta t$である。単位長さに含まれる電荷量は、単位長さと線電荷密度をかけて求める。\n$$ \\Delta q=\\lambda v \\Delta t $$\n電流は単位時間あたりに通過する電荷の量なので、$\\Delta t$間に点$P$を通過する電荷量は、\n$$ I=\\dfrac{\\Delta q}{\\Delta t}=\\dfrac{\\lambda v \\Delta t}{\\Delta t}=\\lambda v $$\n電流はベクトルなので、方向まで含めて表記すると、次のようになる。\n$$ \\mathbf{I}=\\lambda \\mathbf{v} $$\n電流が導線を通って流れる際には、その方向が明らかである（導線と平行な方向である）ため、別に言及する必要はない。しかし、表面上や体積内で流れる電流を扱う場合には、その方向をはっきりと言う必要がある。電流が流れる導線が外部磁場$\\mathbf{B}$によって受ける磁力は、\n$$ \\mathbf{F}_{\\text{mag}}=\\int (\\mathbf{v} \\times \\mathbf{B} ) dq=\\int (\\mathbf{v} \\times \\mathbf{B} ) \\lambda dl=\\int (\\mathbf{I} \\times \\mathbf{B}) dl $$\nここで、$\\mathbf{I}$と$d\\mathbf{l}$の方向が同じであるから、\n$$ \\mathbf{F}_{\\text{mag}} = \\int I (d\\mathbf{l} \\times \\mathbf{B}) $$\n導線で流れる電流の大きさが一定であるため、積分の外に出すことができる、\n$$ \\mathbf{F}_{\\text{mag}}=I \\int (d\\mathbf{l} \\times \\mathbf{B}) $$\n表面電流密度 表面で流れる電流は、表面電流密度surface current density $\\mathbf{K}$で説明される。単位長さの幅を通過する電流を表面電流密度と言い、数式では次のように表される。\n$$ \\mathbf{K}=\\dfrac{d \\mathbf{l}} {dl_\\perp} $$\nこの概念をもっと簡単に理解するための説明は、$\\mathbf{I}=\\dfrac{d\\mathbf{q} }{dt}$なので、\n$$ \\dfrac{d \\mathbf{I} }{dl_{\\perp}}=\\dfrac{d^2 \\mathbf{q}}{dl_{\\perp} dt} $$\nしたがって、表面電流密度は単位時間あたり、単位幅あたりに通過する電荷の量である。表面電荷密度が$\\sigma$、電荷の速度が$\\mathbf{v}$の時、表面電流密度は、\n$$ \\mathbf{K}=\\sigma \\mathbf{v} $$\n表面電流が外部磁場によって受ける磁力は、\n$$ \\mathbf{F}_{\\text{mag}}=\\int(\\mathbf{v}\\times \\mathbf{B})\\sigma da=\\int (\\mathbf{K} \\times \\mathbf{B})da $$\n上で見た電流の式から、電流$\\mathbf{I}$の代わりに表面電流密度$\\mathbf{K}$を入れた形である。\n体積電流密度 同様に、電流がある空間で流れる場合は、体積電流密度volume current density $\\mathbf{J}$で説明される。単位面積あたりに流れる電流を体積電流密度と言い、数式では次のように表される。\n$$ \\mathbf{J}=\\dfrac {d\\mathbf{I}} {da_{\\perp}} $$\nしたがって、逆に面$\\mathcal{S}$を通る電流$I$は、一般的に次のように表すことができる。\n$$ I = \\int_{\\mathcal{S}}J da_{\\perp} = \\int_{\\mathcal{S}}\\mathbf{J}\\cdot d\\mathbf{a} $$\nすると、発散定理によって、体積$\\mathcal{V}$から出て行った総電荷量は、次のようになる。\n$$ \\oint_{\\mathcal{S}}\\mathbf{J}\\cdot d\\mathbf{a} = \\int_{\\mathcal{V}} (\\nabla \\cdot \\mathbf{J}) d \\tau $$\n同様に、$\\dfrac {d\\mathbf{I}} {da_{\\perp}}=\\dfrac{d^2 \\mathbf{q} } {da_{\\perp}{dt}}$だから、体積電流密度は単位時間あたり、単位面積あたりに通過する電荷の量である。体積電荷密度が$\\rho$で、電荷の速度が$\\mathbf{v}$の場合、体積電流密度は、\n$$ \\mathbf{J}=\\rho \\mathbf{v} $$\n体積電流が受ける磁力は、\n$$ \\mathbf{F}_{\\text{mag}}=\\int (\\mathbf{v} \\times \\mathbf{B} )\\rho d\\tau = \\int (\\mathbf{J} \\times \\mathbf{B} ) d\\tau $$\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 金甚成 訳) (第4版, 2014), p234-241\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":898,"permalink":"https://freshrimpsushi.github.io/jp/posts/898/","tags":null,"title":"電流と電流密度"},{"categories":"푸리에해석","contents":"定義 $2L$-周期関数 $f$に対して次のような級数を $f$のフーリエ級数Fourier series of $f$と定義する。\n$$ \\begin{align*} \\lim \\limits_{N \\rightarrow \\infty} S^{f}_{N}(t) \u0026amp;= \\lim \\limits_{N \\to \\infty}\\left[ \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right] \\\\ \u0026amp;= \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\end{align*} $$\nこの時、各々の係数 $a_{0}, a_{n}, b_{n}$を フーリエ係数Fourier coefficientと言い、値は次のようになる。\n$$ \\begin{align*} \\\\ a_{0} \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin \\dfrac{n\\pi t}{L}dt \\end{align*} $$\n説明 フーリエ級数は任意の関数を三角関数の級数展開で表現するもので、フランスの数学者 ジョセフ・フーリエJoseph Fourierが熱方程式を解くために考案したことでよく知られている。任意の関数と表現した理由は、ある区間 $(a,b)$で定義された関数があれば、これをCtrl+C, Ctrl+Vして $(b-a)$-周期関数にすることができるからである。\n核心原理は互いに直交する三角関数たちの線形結合で表現されることであり、3次元ベクトルにたとえると、$(4,-1,7)$を次のように分けることに似ている。\n$$ (4,-1,7) = a_{1}\\hat{\\mathbf{e}}_{1} + a_{2}\\hat{\\mathbf{e}}_{1} + a_{3}\\hat{\\mathbf{e}}_{1} $$\n実際に、$f$のフーリエ級数は$f$との誤差が非常に小さく、条件がよく満たされれば $f$に点ごとに収束する。\n$$ f(t) = \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L}t + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) $$\n導出 回帰分析1 パート 1\n関数 $f(t)$を $1, \\cos \\dfrac{\\pi t}{L}, \\cos\\dfrac{2\\pi t}{L}, \\cdots, \\sin \\dfrac{\\pi t}{L}, \\sin \\dfrac{2\\pi t}{L}, \\cdots $たちの線形結合で表現することが目的である。したがって、$S^{f}_{N}(t)=\\dfrac{1}{2}{\\alpha_{0}}+\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right)$とした時、$f(t)$は以下のように表現できる。\n$$ f(t)=S^{f}_{N}(t)+e_{N}(t) $$\n$e_{N}(t)$は $f(t)$と近似式 $S_{N}^{f} (t)$の差である。この差が最も小さくなる$S_{N}^{f}(t)$を見つければ、それが$f(t)$との差が最も小さい級数展開になる。$e_{N}$を 平均二乗誤差mean square error2としよう。\n$$ e_{N}=\\dfrac{1}{2L}\\int_{-L}^{L} [e_{N}(t) ]^{2}dt=\\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N} (t) \\right]^{2} dt $$\nパート 2\n$$ \\begin{align*} e_{N} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N}(t) \\right]^{2} dt \\\\ \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\end{align*} $$\n平均二乗誤差 $e_{N}$が最小になる時の係数 $\\alpha_0,\\ \\alpha_{n},\\ \\beta_{n}$をそれぞれ $a_0$, $a_{n}$, $b_{n}$としよう。$e_{N}$を最小化する条件は次のようであり、正規方程式normal equationと言われる。\n$$ \\dfrac{\\partial e_{N}}{\\partial \\alpha_{0}}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\alpha_{n}}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\beta_{n}}=0\\quad (m=1,\\ 2,\\ \\cdots,\\ N) $$\nそれでは、$a_{0}$, $a_{n}$, $b_{n}$は以下のように求めることができる。\nパート 2.1 $a_{0}$\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_{0}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_{0}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{-1}{2} \\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_ {N} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac {n\\pi t}{L} \\right) \\right] dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_{0} dt +\\dfrac{1}{2L}\\int_{-L}^{L} \\sum \\limits_ {n=1}^{N}\\left( \\alpha_{n}\\cos \\dfrac{n\\pi t}{L}+\\beta_{n} \\sin \\dfrac{n \\pi t}{L} \\right) dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_{0} dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt +\\dfrac{1}{2}\\alpha_{0} \\\\ \u0026amp;= 0 \\end{align*} $$\n4番目の等号は三角関数の1周期積分が0であるため成り立つ。したがって\n$$ a_{0} = \\dfrac{1}{L} \\int_{-L}^{L}f(t)dt $$\nパート 2.2 $a_{n}$\nある$m \\in \\left\\{ 1,2,\\dots,N \\right\\}$を一つ選ぼう。\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\cos \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_{0}\\cos\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad + \\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\cos\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\alpha_{m} \\int_{-L}^{L}\\cos\\dfrac{m\\pi t}{L}\\cos\\dfrac{m\\pi t} {L} dt\\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\alpha_{m} \\\\ \u0026amp;= 0 \\end{align*} $$\n4番目、5番目の等号は三角関数の直交性によって成り立つ。したがって\n$$ a_{n}= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\quad (n=1, 2, \\cdots, N) $$\nパート 2.3 $b_{n}$\nある$m \\in \\left\\{ 1,2,\\dots,N \\right\\}$を一つ選ぼう。\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\beta_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\beta_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\sin \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_{0}\\sin\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad +\\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\sin\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\beta_{m} \\int_{-L}^{L}\\sin\\dfrac{m\\pi t}{L}\\sin\\dfrac{m\\pi t} {L} dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\beta_{m} \\\\ \u0026amp;=0 \\end{align*} $$\n4番目、5番目の等号は三角関数の直交性によって成り立つ。したがって\n$$ b_{n}=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\quad (n=1, 2, \\cdots, N) $$\nパート 3 ここで得られた$a_0$, $a_{n}$, $b_{n}$で$f(t)$を表現すると同じになる。\n$$ \\begin{align*} f(t) \u0026amp;= S^{f}_{N}(t)+e_{N}(t) \\\\[1em] \\text{where } S^{f}_{N}(t) \u0026amp;= \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t} {L} \\right) \\\\ a_{0} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\end{align*} $$\n$N$に対して極限をとれば\n$$ \\lim \\limits_{N \\rightarrow \\infty} S_{N}^{f} (t)=\\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n} \\sin\\dfrac{n\\pi t}{L} \\right) $$\n上の級数を $f$のフーリエ級数 と呼び、$a_0$, $a_{n}$, $b_{n}$を $f$のフーリエ係数 という。\n■\nチェ・ビョンソン, フーリエ解析入門 (2002), p51-53\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRSSが平均二乗誤差である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":929,"permalink":"https://freshrimpsushi.github.io/jp/posts/929/","tags":null,"title":"フーリエ級数の導出"},{"categories":"함수","contents":" 🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧\n**\n**内積inner product 区間$[a,b]$で定義された二つの複素関数$f$、$g$の内積は、以下のように定義される。 $$ \\left\\langlef, g\\right\\rangle:=\\int_{a}^b f(x) \\overline{g(x)} dx $$ したがって、同じ二つの関数の内積は $$ \\left\\langle f,f \\right\\rangle=\\int_{a}^b f(x) \\overline{f(x)} dx = \\int_{a}^b \\left| f(x) \\right| ^2 dx $$ 関数の内積を定積分で定義する理由\n直交 $(\\mathrm{orthogonal})$ 二つの複素関数$f$、$g$が下記の式を満たす時、「$f$、$g$は区間$[a,b]$で直交する**」と言われる。 $$ \\left\\langle f,g \\right\\rangle=\\int_{a}^b f(x) \\overline{g(x)} dx=0 $$ 二つの関数の内積を積分で定義したから、積分値が$0$の時直交するというのは自然である。\n直交集合と直交性 orthogonal set and orthogonality 関数$\\phi_{1}$、$\\phi_2$、$\\phi_{3}$、$\\cdots$が下記の式を満たす場合、これらの関数の集合$\\left\\{\\phi_{1},\\ \\phi_2,\\ \\phi_{3}, \\cdots \\right\\}$を直交集合と言い、これらの関数の集合が直交性**を持つと言われる。 $$ \\left\\langle \\phi_{m},\\phi_{n} \\right\\rangle = \\int_{a}^b \\phi_{m} (x) \\overline{ \\phi_{n}(x) } dx=0\\ \\ (m\\ne n) $$ 簡単に言えば、直交集合とは他の関数と直交する関数を集めた集合である。\n関数のノルム$(\\mathrm{norm})$ 内積を定義すると、ノルムを定義することができる。複素関数$f$のノルムは以下のように定義される。 $$ | f | = \\left\\langle f,f\\right\\rangle^{ \\frac{1}{2} } := \\left( \\int_{a}^b \\left| f(x) \\right| ^2 dx \\right) ^{ \\frac{1}{2} } $$\n正規化$(\\mathrm{normalization})$ 任意の関数$f$に対して、適切な定数をかけて$f$のノルムが$1$になるようにすることを正規化という。正規化された関数は正規化された$(\\mathrm{normalized})$関数または正規化関数と呼ばれる。従って、$f$の正規化関数を$f_{\\mathrm{normal}}$とすると $$ f_{\\mathrm{normal}}=\\frac{1}{ | f | }f $$\n正規直交集合orthonomal set 直交集合$\\left\\{ \\phi_{1}, \\phi_{2}, \\cdots \\right\\}$の要素が下記の条件を満たす場合、その集合を正規直交集合**と呼ぶ。 $$ \\left\\langle \\phi_{m},\\phi_{n} \\right\\rangle = \\int_{a}^b \\phi_{m} (x) \\overline{ \\phi_{n}(x) } dx=\\delta_{mn} $$ つまり、正規直交集合はすべての要素が正規化された直交集合を指す。$\\delta_{mn}$はクロネッカーデルタである。直交集合から自分自身との内積が1であるという条件が加わっている。例えば、3次元直交座標系で$\\left\\{ \\hat{\\mathbf{x}},\\ \\hat{\\mathbf{y}},\\ \\hat{\\mathbf{z}} \\right\\}$は正規直交集合である。\n**例 $f_{0}(x)=1$、$f_{1}(x)=x$、$f_2(x)=x^2+ax+b$としよう。\n$f_{0}$と$f_{1}$が$[-1,1]$で直交することを示せ解決策** $$ \\begin{align*} \\left\\langle f_{0},f_{1} \\right\\rangle \u0026amp;= \\int_{-1}^{1} x dx \\\\ \u0026amp;= \\left. \\dfrac{1}{2}x^2 \\right]_{-1}^{1} = \\dfrac{1}{2}-\\dfrac{1}{2}=0 \\end{align*} $$ ■\n2. $f_2$が$f_{0}$、$f_{1}$と同時に直交するような定数$a$, $b$を求めよ。解決策 $$ \\begin{align*} \\left\\langle f_2,f_{0} \\right\\rangle \u0026amp;= \\int_{-1}^{1} (x^2+ax+b) dx \\\\ \u0026amp;= \\left. \\frac{1}{3}x^3 +\\frac{a}{2}x^2+bx \\right]_{-1}^{1} \\\\ \u0026amp;= \\left( \\frac{1}{3}+\\frac{a}{2}+b\\right) - \\left( -\\frac{1}{3}+\\frac{a}{2}-b \\right) \\\\ \u0026amp;= \\frac{2}{3}+2b =0 \\end{align*} $$ $$ \\begin{align*} \\left\\langle f_2,f_{1} \\right\\rangle \u0026amp;= \\int_{-1}^{1}( x^3+ax^2+bx) dx \\\\ \u0026amp;= \\left. \\frac{1}{4}x^4 +\\frac{a}{3}x^3+\\frac{b}{2}x^2 \\right]_{-1}^{1} \\\\ \u0026amp;= \\left( \\frac{1}{4}+\\frac{a}{3}+\\frac{b}{2}\\right) - \\left( \\frac{1}{4}-\\frac{a}{3}+\\frac{b}{2} \\right) \\\\ \u0026amp;= \\frac{2}{3}a=0 \\end{align*} $$ 従って、$a=0$、$b=-\\dfrac{1}{3}$■\n**$f_{0}$、$f_{1}$、$f_2$の正規化関数を求めよ。解決策 $$ \\left\\langlef_{0},f_{0} \\right\\rangle=\\int_{-1}^{1} 1 dx =2 $$ $$ \\left\\langlef_{1},f_{1} \\right\\rangle=\\int_{-1}^{1} x^2 dx =\\frac{2}{3} $$\n$$ \\left\\langlef_2,f_2 \\right\\rangle=\\int_{-1}^{1} \\left( x^2-\\frac{1}{3} \\right )\\left( x^2-\\frac{1}{3} \\right) dx =\\frac{8}{45} $$ 従って、$f_{0}$、$f_{1}$、$f_2$の正規化関数はそれぞれ $$ \\frac{1}{\\sqrt{2}}f_{0},\\quad \\sqrt{\\frac{3}{2}}f_{1},\\quad \\sqrt{\\frac{45}{8}}f_2 $$ ■\n","id":926,"permalink":"https://freshrimpsushi.github.io/jp/posts/926/","tags":null,"title":"直交関数と直交集合：正規直交集合と関数のノルム"},{"categories":"추상대수","contents":"まとめ 1 素数 $p$ と 自然数 $n$ に対して、基数が $p^{n}$ の 有限 体有限体を $p^{n}$ 次のガロア体ガロア体と定義し、$\\text{GF} \\left( p^{n} \\right)$ のように表す。有限体はガロア体だけであり、与えられた $p$ と $n$ に対してガロア体は唯一に存在する。\nここで「唯一である」とは、異なる体であっても同型写像が存在し、実質的に同一の体であるという意味である。 説明 ガウスが最初に有限体の概念を思いついたときは、その実体を信じる人はいなかったが、現在では有限体が存在するだけでなく、その具体的な形まで明らかにされている。すべての有限体の形が解明されたので、無駄な研究をする必要はない。\n例えば、元が $10$ 個の体が存在するかどうかは、考える必要さえなく、$\\text{GF} \\left( p \\right) = \\mathbb{Z}_{p}$ は整数環であるため、すでに多くのことがわかっている。さらに知りたいことがあれば、抽象的な定義に固執する必要はなく、$\\mathbb{Z}_{p}$ を通じてアプローチすればよく、その逆もまた然りである。\n証明 2 パート1. すべての有限体はガロア体である。\n体 $F$ の有限拡大体を $E$ とし、$F$ 上の 次数を $n := \\left[ E : F \\right]$ とする。\n$| F | = q$ とすると、$E$ は $F$ の $n$ 次のベクトル空間であるため、$|E| = q^{n}$ である。体は単位元を持つが、標数が $0$ であれば $\\mathbb{Z}$ と同型の部分環が存在して無限体となる。したがって、有限体の標数は有限の自然数でなければならない。有限体 $E$ の標数を $p \\ne 0$ とすると、$E$ は単位元 $1$ を持つため、$p \\cdot 1 = 0$ でなければならない。体は整域であるため、 $$ p \\cdot 1 = ( p_{1} \\cdot 1 ) ( p_{2} \\cdot 1 ) = 0 $$ を満たす $p_{1}, p_{2} \\in \\mathbb{Z}$ が存在することはなく、$p$ は必ず素数である。したがって、$E$ は素体 $\\mathbb{Z}_{p}$ と同型の部分体を持ち、$\\left| \\mathbb{Z}_{p} \\right| = p$ であるため、$|E| = p^{n}$ である。\nパート2. ガロア体の存在\nパート2-1. $x^{p^{n}} - x$ のゼロ\n$\\left( x^{p^{n}} - x \\right)$ の標数が $p$ の体 $F$ の代数的閉包 $\\overline{F}$ を考える。\n$\\overline{F}$ は代数的に閉じているため、$\\left( x^{p^{n}} - x \\right) \\in \\overline{F} [ x ]$ は $1$ 次の項で因数分解される。すぐにわかる事実は $$ x^{p^{n}} - x = ( x - 0 ) \\left( x^{p^{n}-1} - 1 \\right) $$ であるため、$0$ は $\\left( x^{p^{n}} - x \\right)$ のゼロになる。$f(x) := x^{p^{n}-1} - 1$ の別のゼロ $\\alpha \\ne 0$ を考えると、 $f \\left( \\alpha \\right) = 0$ であるため、 $$ 0 = f \\left( \\alpha \\right) = \\alpha^{p^{n} - 1} - 1 \\implies \\alpha^{p^{n} - 1} = 1 $$ となり、これにより $f(x)$ を $\\left( x - \\alpha \\right)$ の積として表すと、 $$ \\begin{align*} f(x) =\u0026amp; x^{p^{n}-1} - 1 \\\\ =\u0026amp; x^{p^{n}-1} - \\alpha^{p^{n}-1} \\\\ =\u0026amp; (x - \\alpha ) \\left( x^{p^{n} - 2 } + \\alpha x^{p^{n} - 3 } + \\cdots + \\alpha^{p^{n} - 3 } x + \\alpha^{p^{n} - 2} \\right) \\end{align*} $$ である。一方、便宜上第二の因数を、 $$ g(x) := \\left( x^{p^{n} - 2 } + \\alpha x^{p^{n} - 3 } + \\cdots + \\alpha^{p^{n} - 3 } x + \\alpha^{p^{n} - 2} \\right) $$ とすると、$g(x)$ の項の数は $p^{n} - 1$ 個である。したがって、$x = \\alpha$ を代入してみると、 $$ g ( \\alpha ) = \\alpha^{p^{n} - 2} \\cdot \\left( p^{n} - 1 \\right) = {{\\alpha^{p^{n} - 1}} \\over { \\alpha }} \\left( p^{n} - 1 \\right) $$ を得る。上記で $\\alpha \\ne 0$ は $f(x)$ のゼロであるため、$\\alpha^{p^{n}-1} - 1 = 0$ としたし、標数を素数 $p$ と仮定したので、 $$ g ( \\alpha ) = {{1} \\over { \\alpha }} \\cdot (0 - 1) = - {{1} \\over { \\alpha }} \\ne 0 $$ である。したがって、$\\alpha$ は $f(x) = 0$ の重根ではなく、これは $\\alpha$ 以外の他のゼロにも当てはまる。結局、$\\left( x^{p^{n}} - x \\right)$ は正確に $p^{n}$ 個の異なるゼロを持つ。\nパート2-2. 新入生の夢\n一方で、$\\alpha , \\beta \\in F$ に対して $\\left( \\alpha + \\beta \\right)^{p}$ を計算すると、二項定理により、 $$ \\begin{align*} \\left( \\alpha + \\beta \\right)^{p} =\u0026amp; \\sum_{k=1}^{p} \\binom{p}{k} \\alpha^{k} \\beta^{p - k} \\\\ =\u0026amp; \\alpha^{p} + \\sum_{k=2}^{p-1} {{p!} \\over { ( p - k )! ( k )! }} \\alpha^{k} \\beta^{p - k} + \\beta^{p} \\\\ =\u0026amp; \\alpha^{p} + \\beta^{p} + p \\sum_{k=2}^{p-1} {{ ( p - 1 )! } \\over { ( p - k )! ( k )! }} \\alpha^{k} \\beta^{p - k} \\end{align*} $$ $F$ の標数が $p$ であるため、最後の項は $0$ となり、したがって、 $$ \\left( \\alpha + \\beta \\right)^{p} = \\alpha^{p} + \\beta^{p} $$ もう一度両辺に $p$ 乗をすると、 $$ \\left( \\left( \\alpha + \\beta \\right)^{p} \\right)^{p} = \\left( \\alpha^{p} \\right)^{p} + \\left( \\beta^{p} \\right)^{p} $$ 整理すると $\\left( \\alpha + \\beta \\right)^{p^{2}} =\\alpha^{p^2} + \\beta^{p^2}$ であり、これを $n$ 回繰り返すと、次を得る。 $$ \\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n} $$\n今度は $\\mathbb{Z}_{p}$ の代数的閉包 $\\overline{ \\mathbb{Z}_{p} }$ を考える。\n$\\left( x^{p^{n}} - x \\right) \\in \\overline{ \\mathbb{Z}_{p} } [ x ]$ のゼロをすべて集めた集合を $K \\subset \\overline{ \\mathbb{Z}_{p} } $、その元を $\\alpha , \\beta \\in K$ とする。\nパート2-3. $K$ はガロア体である。\n(i) 加算に対する閉包: $$ \\begin{cases} \\alpha^{p^{n}} - \\alpha = 0 \\\\ \\beta^{p^{n}} - \\beta = 0 \\end{cases} $$ である。両辺を加えると、パート2-2 $\\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n}$ により、 $$ \\left( \\alpha^{p^{n}} + \\beta^{p^{n}} \\right) - ( \\alpha + \\beta ) = \\left( \\alpha + \\beta \\right)^{p^{n}} - ( \\alpha + \\beta ) = 0 $$ であるため、$( \\alpha + \\beta ) \\in K$ である。 (ii) 加算に対する単位元: $0^{p^{n}} - 0 = 0$ であるため、$0 \\in K$ である。 (iii) 加算に対する逆元: $\\left( - \\alpha \\right)^{p^{n}} = \\left( - 1 \\right)^{^{p^{n}}} \\left( \\alpha \\right)^{p^{n}} = \\left( - 1 \\right)^{^{p^{n}}} \\alpha$ である。 $p=2$ の場合、$-1 = 1$ であるため、$\\left( -\\alpha \\right) = \\alpha \\in K$ である。 $p \\ne 2$ は奇数の素数であるため、$\\left( - \\alpha \\right)^{p^{n}} - ( - \\alpha ) = 0$、つまり $( - \\alpha ) \\in K$ である。 (iv) 乗算に対する閉包: $\\left( \\alpha \\beta \\right)^{p^{n}} = \\alpha^{p^{n}} \\beta^{p^{n}} = \\alpha \\beta$ であるため、$\\left( \\alpha \\beta \\right)^{p^{n}} - \\alpha \\beta = 0$、すなわち $\\alpha \\beta \\in K$ である。 (v) 乗算に対する単位元: $1^{p^{n}} - 1 = 0$ であるため、$1 \\in K$ である。 (vi) 乗算に対する逆元: $\\alpha \\ne 0$ に対して $\\displaystyle \\left( \\alpha \\right)^{p^{n}} = \\alpha$ の逆数を取ると、$\\displaystyle {{1} \\over {\\left( \\alpha \\right)^{p^{n}} }} = {{1} \\over { \\alpha }}$、すなわち $$ \\left( {{1} \\over { \\alpha }} \\right)^{p^{n}} - {{1} \\over { \\alpha }} = 0 $$ であるため、$\\alpha^{-1} \\in K$ である。 (vii): $| K | = p^{n}$ : $\\mathbb{Z}_{p}$ の標数は $p$ であるため、パート2-1により $\\left( x^{p^{n}} - x \\right)$ は正確に $p^{n}$ 個の異なるゼロを持つ。 したがって、$K$ は $p^{n}$ 次のガロア体である。\nパート3. ガロア体の一意性\nパート1では、$F$ の標数は素数 $p$ であり、パート2-1では、$F$ の代数的閉包 $\\overline{F}$ での演算が、$F$ の単位元 $1_{F}$ を $1_{\\mathbb{Z}_{p}}$ と見た場合、実際には $\\mathbb{Z}_{p}$ の代数的閉包 $\\overline{\\mathbb{Z}}_{p}$ での演算と変わらないことを指摘しておく。\nパート3-1. 基数が $p^{n}$ の体 $E \\subset \\overline{\\mathbb{Z}}_{p}$ の正体 3\nラグランジュの定理: $H$ が有限群 $G$ の部分群であれば、$|H|$ は $|G|$ の約数である。\n基数が $p^{n}$ の体 $\\left( E , + , \\times \\right)$ において、乗算 $\\times$ に対する群 $\\left( E^{\\ast} , \\times \\right)$ を考えると、$E^{\\ast}$ は $E$ で $+$ に対する単位元 $0 \\in E$ を除く $p^{n} - 1$ 個の元と単位元 $1 \\in E^{\\ast}$ を持つ。$\\alpha \\in E^{\\ast}$ のオーダーOrder、つまり $\\alpha$ によって生成される巡回群の基数である $\\left| \\alpha \\right| = \\left| \\left\u0026lt; \\alpha \\right\u0026gt; \\right|$ はラグランジュの定理により $p^{n} - 1$ の約数であり、したがって、 $$ \\alpha^{p^{n} - 1} = 1 \\implies a^{p^{n}} = \\alpha $$ を得る。つまり、$E$ のすべての元は $x^{p^{n}} - x$ のゼロであり、代数学の基本定理により、$\\mathbb{Z}_{p}$ の代数的閉包 $\\overline{\\mathbb{Z}}_{p}$ に含まれる基数が $p^{n}$ の体 $E$ の元は正確に $\\left( x^{p^{n}} - x \\right) \\in \\mathbb{Z}_{p} [x]$ のゼロである。\nパート3-2. 最小分解体\nパート2-1とパート3-1により、与えられた $p$ と $n$ に対して、すべての元が正確に $\\left( x^{p^{n}} - x \\right)$ のゼロで構成される体 $E$ が存在し、$F$ の標数が $p$ であることにより、その係数に対する演算も素体 $\\mathbb{Z}_{p}$ での演算と同じであったことに注意せよ。パート2-3とパート1により、$E$ は素体 $\\mathbb{Z}_{p}$ を素体として持ち、$|E| = p^{n}$ を満たす必要があるガロア体であり、さらにパート2-1により、$E$ は $\\left( x^{p^{n}} - x \\right)$ の最小分解体であることがわかる。\n最小分解体の性質: $f(x) \\in F [ x ]$ の最小分解体はすべて同型である。\n最小分解体の性質により、与えられた $p$ と $n$ に対して、ガロア体は一意である。\n■\n補助定理: 新入生の夢 単に面白い事実として、パート2-2で登場した等式 $$ \\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n} $$ を新入生の夢Freshman\u0026rsquo;s Dreamと呼ぶ。学校に入ったばかりの新入生の立場からすると、累乗が括弧の中に入れば、複雑な展開なしにも難しい問題を解くことができるからである。ちなみに、数論では、標数に関する言及がなくても、同様の方法で合同式 $\\left( \\alpha + \\beta \\right)^{p^{n}} \\equiv \\alpha^{p^n} + \\beta^{p^n} \\pmod{ p }$ を導くことができる。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p300.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p302~304.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p301\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":820,"permalink":"https://freshrimpsushi.github.io/jp/posts/820/","tags":null,"title":"ガロア体"},{"categories":"함수","contents":"公式 ルジャンドル多項式の明示的explicitな公式は以下の通りです。\n$$ P_{l}(x)=\\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \\tag{1} $$\n説明 $l$番目のルジャンドル多項式を得る公式であり、これをロドリゲスの公式と言います。元々はルジャンドル多項式の明示的な形を示す言葉でしたが、その後、多項式で表される特殊関数の明示的な形を示す公式の一般的な名称となりました。\n導出 ルジャンドル多項式$P_{l}$は以下のようなルジャンドルの微分方程式の解を指します。\n$$ (1 - x^{2}) \\dfrac{d^{2} y}{d x^{2}} - 2x \\dfrac{d y}{d x} + l(l+1)y = 0 $$\nしたがって、$(1)$が上記の微分方程式の解であることを示せば、証明が完了します。\nまず、$v=(x^2-1)^l$としたとき、$\\dfrac{d^lv}{dx^l}$がルジャンドル方程式の解であることを示すつもりです。その後、$P_{l}(1) = 1$を満たすように正規化して、$(1)$を得ます。\n$$ \\dfrac{dv}{dx}=l(2x)(x^2-1)^{l-1} $$\n両辺に$(x^2-1)$を掛けると、以下の式を得ます。\n$$ (x^2-1)\\dfrac{dv}{dx}=2lx(x^2-1)^l=2lxv $$\n両辺を$l+1$回微分すると、ライプニッツの法則により以下のようになります。\n$$ \\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C}_{k} \\dfrac{ d^{l+1-k}}{dx^{l+1-k} } \\left( \\dfrac{dv}{dx} \\right) \\dfrac{d^k}{dx^k} (x^2-1) = 2l\\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C} _{k} \\dfrac{d^{l+1-k} v}{dx^{l+1-k}} \\dfrac{d^k x}{dx^k} $$\nこのとき、左辺は$k \\ge 3$のとき$\\dfrac{d^k}{dx^k}(x^2-1)=0$であるため、$k=0,2,3$の項のみが残ります。右辺は$k \\ge 2$のとき$\\dfrac{d^kx}{dx^k}=0$であるため、$k=1,2$の項のみが残ります。したがって、次のように得られます。\n$$ (x^2-1)\\dfrac{d^{l+2} v}{dx^{l+2}} + (l+1)(2x)\\dfrac{d^{l+1}v}{dx^{l+1}}+\\dfrac{l(l+1)}{2!}2\\dfrac{d^l v}{dx^l}=2lx\\dfrac{d^{l+1} v}{dx^{l+1}} + 2l(l+1)\\dfrac{d^lv}{dx^l} $$\n同じ係数項をまとめて整理すると、以下のようになります。\n$$ (1-x^2)\\left( \\dfrac{d^l v}{dx^l} \\right)^{\\prime \\prime} -2x\\left( \\dfrac{d^lv}{dx^l} \\right)^{\\prime} + l(l+1)\\dfrac{d^lv}{dx^l}=0 $$\nこれはルジャンドル方程式と同じ形です。つまり、$\\dfrac{d^l v}{dx^l}$がルジャンドル方程式の解になります。\n$$ P_{l}(x)= \\dfrac{d^l}{dx^l}(x^2-1)^l $$\n$P_{l}(1) = 1$を満たす係数を求めてみましょう。$(x^2-1)^l$を$(x-1)^l(x+1)^l$で因数分解し、ライプニッツの法則で$l$回微分すると、以下のようになります。\n$$ \\begin{align*} \u0026amp;\\quad \\ P_{l}(x) \\\\ \u0026amp;= \\dfrac{d^l}{dx^l} \\left[ (x-1)^l (x+1)^l \\right] \\\\ \u0026amp;= \\sum\\limits_{k=0}^l {}_{l}\\mathrm{C}_{k} \\dfrac{d^{l-k}}{dx^{l-k}}(x-1)^l \\dfrac{d^k}{dx^k}(x+1)^l \\\\ \u0026amp;= {}_{l}\\mathrm{C}_{0} l! (x+1)^l + {}_{l}\\mathrm{C}_{1} l!(x-1) l(x+1)^{l-1}+{}_{l}\\mathrm{C}_2\\dfrac{l!}{2}(x-1)^2l(l-1)(x+1)^{l-2}+\\cdots \\end{align*} $$\n2番目の項からは因数として$(x-1)$を含むため、$x=1$のとき$0$です。したがって、$P_{l}(1)=l! 2^l$であり、この値が$1$になるためには、$\\dfrac{1}{2^l l!}$で割ればよいです。したがって、最終的に以下のようなロドリゲスの公式を得ます。\n$$ P_{l}(x)=\\dfrac{1}{2^l l!}\\dfrac{d^l}{dx^l}(x^2-1)^l $$\n■\n","id":895,"permalink":"https://freshrimpsushi.github.io/jp/posts/895/","tags":null,"title":"ルジャンドル多項式のロドリゲスの公式"},{"categories":"상미분방정식","contents":"定義1 以下の微分方程式をルジャンドルLegendre微分方程式と言う。\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -2x\\dfrac{dy}{dx}+l(l+1) y=0 $$\nルジャンドル微分方程式の解をルジャンドル多項式と言い、通常$P_{l}(x)$で示される。最初のいくつかの$l$によるルジャンドル多項式は次のようである。\n$$ \\begin{align*} P_{0}(x) =\u0026amp;\\ 1 \\\\ P_{1}(x) =\u0026amp;\\ x \\\\ P_2(x) =\u0026amp;\\ \\dfrac{1}{2}(3x^2-1) \\\\ P_{3}(x) =\u0026amp;\\ \\dfrac{1}{2}(5x^3-3x) \\\\ P_{4}(x) =\u0026amp;\\ \\dfrac{1}{8}(35x^4-30x^2+3) \\\\ P_{5}(x) =\u0026amp;\\ \\dfrac{1}{8}(63x^5-70x^3+15x) \\\\ \\vdots\u0026amp; \\end{align*} $$\n説明 ルジャンドル微分方程式は、次のような形で紹介されることもある。\n$$ \\dfrac{d}{dx}\\left[ (1-x)^2 \\dfrac{dy}{dx} \\right] +l(l+1)y=0 $$\nこれはシュツルム-リウヴィル理論Sturm-Liouville theoryで表されるものである。第一項を展開して整理すると、同じ式が得られる。ルジャンドル微分方程式を以下のように一般化したものを関連ルジャンドル微分方程式associated Legendre differential equationと言う。\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -2x\\dfrac{dy}{dx}+\\left( \\dfrac{-m^2}{1-x^2} +l(l+1) \\right) y=0 $$\nここで$m=0$の場合、ルジャンドル微分方程式となる。\nルジャンドル方程式は物理学や工学などで登場し、特に球面座標系でのラプラス方程式を解く時に見ることができる。物理学科ならば、電磁気学で球面座標系での電位を計算する時、量子力学で球面座標系でのシュレディンガー方程式を解く時に出会うことがある。解法の過程が長いため、教科書では通常、ロドリゲス公式で表される解答のみを記載することが多い。実際、物理学の学生は解法が非常に非常に気になるわけではなければ、知らなくても問題はない。\n解法 係数に独立変数$x$が含まれた形で、解が冪級数の形であると仮定すれば解くことができる。\n$$ \\begin{equation} (1-x^2)y^{\\prime \\prime} -2xy^{\\prime}+l(l+1) y=0 \\label{1} \\end{equation} $$\nルジャンドル微分方程式の解を次のように仮定しよう。\n$$ y=a_{0}+a_{1}(x-x_{0})+a_2(x-x_{0})^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}(x-x_{0})^n $$\nこの時$x=0$の時、$y^{\\prime \\prime}$の係数が$(1-x^2)|_{x=0}=1\\ne 0$であるため、$x_{0}=0$と置く。すると級数解は\n$$ \\begin{equation} y=a_{0}+a_{1}x+a_2x^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}x^n \\label{2} \\end{equation} $$\n解を級数と仮定したが、解法の最後に実際には$y$の項が有限であることがわかる。これで$\\eqref{1}$に代入するために$y^{\\prime}$と$y^{\\prime \\prime}$を求めよう。\n$$ y^{\\prime}=a_{1}+2a_2x+3a_{3}x^2+\\cdots=\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1} $$\n$$ y^{\\prime \\prime}=2a_2+3\\cdot 2a_{3}x+4\\cdot 3 a_{4}x^2 +\\cdots = \\sum \\limits_{n=2} n(n-1)a_{n}x^{n-2} $$\nこれで$\\eqref{1}$に$y, y^{\\prime}, y^{\\prime \\prime}$を代入すると\n$$ (1-x^2)\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -2x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n第一項の係数$(1-x^2)$の括弧を展開して整理すると\n$$ \\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x^2\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -2x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n$$ \\implies \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -2\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\nここでのポイントは**$x$の次数を合わせること**である。他は全て$x^n$で表されるのに対し、最初の級数だけが$x^{n-2}$で表されているため、$n$の代わりに$n+2$を代入すると\n$$ \\sum \\limits_{n=0} ^\\infty (n+2)(n+1)a_{n+2}x^{n} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -2\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n二番目の級数が$x^2$項から始まるので、他の級数から$n=0,1$の項を外して、定数項は定数項同士、1次項は1次項同士をまとめると\n$$ \\left[ 2\\cdot 1 a_2+l(l+1)a_{0} \\right]+\\left[ 3\\cdot 2 a_{3}-2a_{1}+l(l+1)a_{1} \\right]x \\\\ + \\sum \\limits_{n=2}^\\infty \\left[ (n+2)(n+1)a_{n+2}-n(n+1)a_{n}-2na_{n}+l(l+1)a_{n} \\right] x^n=0 $$\n上の式が成り立つためには全ての係数が$0$でなければならない。\n$$ 2\\cdot 1 a_2+l(l+1)a_{0} =0 $$\n$$ 3\\cdot 2 a_{3}-2a_{1}+l(l+1)a_{1} =0 $$\n$$ (n+2)(n+1)a_{n+2}-n(n+1)a_{n}-2na_{n}+l(l+1)a_{n}=0 $$\nそれぞれを整理すると\n$$ \\begin{equation} a_2=-\\dfrac{l(l+1)}{2 \\cdot 1}a_{0} \\label{3} \\end{equation} $$\n$$ \\begin{equation} a_{3}=-\\dfrac{(l+2)(l-1)}{3\\cdot 2} a_{1} \\label{4} \\end{equation} $$\n$$ \\begin{equation} a_{n+2}=-\\dfrac{(l+n+1)(l-n)}{(n+2)(n+1)}a_{n} \\label{5} \\end{equation} $$\n$\\eqref{3}, \\eqref{4}, \\eqref{5}$を利用すると、$a_{0}$と$a_{1}$の値だけを知っていれば全ての係数を知ることができる。$\\eqref{3}$と$\\eqref{5}$で偶数次項の係数を求めると\n$$ \\begin{align*} a_{4} =\u0026amp;\\ - \\dfrac{(l+3)(l-2)}{ 4 \\cdots 3}a_2 = \\dfrac{l(l-2)(l+1)(l+3)}{4!}a_{0} \\\\ a_{6} =\u0026amp;\\ -\\dfrac{(l+5)(l-4)}{6\\cdot5} a_{4} = -\\dfrac{ l(l-2)(l-4)(l+1)(l+3)(l+5)}{6!} a_{0} \\\\ \\vdots\u0026amp; \\end{align*} $$\n$n=2m\\ (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m}=(-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!}a_{0} $$\n同様に$\\eqref{4}$、$\\eqref{5}$で奇数次項の係数を求めると\n$$ \\begin{align*} a_{5} =\u0026amp;\\ -\\dfrac{(l+4)(l-3)}{5\\cdot 4}a_{3} = \\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}a_{1} \\\\ a_{7} =\u0026amp;\\ -\\dfrac{(l+6)(l-5)}{7\\cdot 6}a_{5} = -\\dfrac{(l+2)(l+4)(l+6)(l-1)(l-3)(l-5)}{7!}a_{1} \\\\ \\vdots\u0026amp; \\end{align*} $$\n$n=2m+1\\ (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m+1}=(-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!}a_{1} $$\nこれで求めた係数を$\\eqref{2}$に代入して解を求めると\n$$ \\begin{align*} y =\u0026amp;\\a_{0}+a_{1}x -\\dfrac{l(l+1)}{2!}a_{0}x^2-\\dfrac{(l+2)(l-1)}{3!}a_{1}x^3 + \\dfrac{l(l-2)(l+1)(l+3)}{4!}a_{0}x^4+\\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}a_{1}x^5 \\\\ \u0026amp;+ \\cdots +(-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!}a_{0}x^{2m} \\\\ \u0026amp;+ (-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!}a_{1}x^{2m+1} +\\cdots \\end{align*} $$\n$(m=1,2,3,\\cdots)$偶数次項は$a_{0}$で、奇数次項は$a_{1}$でまとめると\n$$ \\begin{align*} y =\u0026amp;\\a_{0}\\left[1-\\dfrac{l(l+1)}{2!}x^2+\\dfrac{l(l-2)(l+1)(l+3)}{4!}x^4 \\right. \\\\ \u0026amp;\\left.+\\sum \\limits_{m=3}^\\infty (-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!} x^{2m} \\right] \\\\ \u0026amp;+ a_{1}\\left[x- \\dfrac{(l+2)(l-1)}{3!}x^3+\\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}x^5 \\right. \\\\ \u0026amp; \\left. +\\sum \\limits_{m=3}^\\infty (-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!} x^{2m+1} \\right] \\end{align*} $$\n最初の括弧を$y_{0}$、二番目の括弧を$y_{1}$とすると、ルジャンドル方程式の一般解は次のようになる。\n$$ y=a_{0}y_{0}+a_{1}y_{1} $$\n二つの級数$y_{0}$と$y_{1}$は比率判定法により、$|x|\u0026lt;1$の範囲で収束\nすることがわかる。$\\eqref{5}$により$\\dfrac{a_{n+2}}{a_{n}}=-\\dfrac{(l+n+1)(l-n)}{(n+2)(n+1)}=\\dfrac{(n+l+1)(n-l)}{(n+2)(n+1)}$であるため、比率判定法を使うと\n$$ \\lim \\limits_{n \\rightarrow \\infty} \\dfrac{(n+l+1)(n-l)}{(n+2)(n+1)}x^2=x^2\u0026lt;1 $$\n$$ \\implies -1\u0026lt;x\u0026lt;1 $$\nしかし、多くの問題で$x=\\cos \\theta$、$l$は非負の整数の形で式が現れ、全ての$\\theta$に対して収束する解を得たい。つまり、$x=\\pm 1$でも収束する解を見つけることが目標である。幸いにも$l$が整数の時は、欲しい解が存在し、その時$l$の値によって必ず$y_{0}, y_{1}$のどちらかの解のみが存在する。$l$が$0$か偶数の時は$y_{1}$が発散し、$y_{0}$は偶数次項のみを持つ有限項の多項式となる。$l$が奇数ならば$y_{0}$が発散し、$y_{1}$は奇数次項のみを持つ有限項の多項式となる。表にまとめると以下のようになる。\n$l$の値 $y_{0}$ $y_{1}$ 方程式の解 $0$か偶数 有限項の多項式 発散 $y=a_{0}y_{0}$ 奇数 発散 有限項の多項式 $y=a_{1}y_{1}$ ケース1. $l$が$0$か偶数\n$l=0$の時、2次項から$l$を因数に持ち、全て$0$になるので、$y_{0}=1$\n$l=2$の時、4次項から$(l-2)$を因数に持ち、全て$0$になるので、$y_{0}=1-3x^2$\n$l=4$の時、6次項から$(l-4)$を因数に持ち、全て$0$になるので、$y_{0}= 1-10x^2+\\dfrac{35}{3}x^4$\nそして$l=0$の時、$x^2=1$から$y_{1}=1+\\frac{1}{3}+\\frac{1}{5}+\\cdots$であるが、これは積分判定法により発散する。他の偶数の時も同様である。したがって、$l$が$0$か偶数の時は、解が偶数次項のみを持つ有限項の多項式となる。つまり、級数$y_{0}$の特定の項までのみ残る形の解を得る。\nケース2. $l$が奇数\n偶数の時と反対の結果が現れる。\n$l=1$の時、3次項から$(l-1)$を因数に持ち、全て$0$になるので、$y_{1}=x$\n$l=3$の時、5次項から$(l-3)$を因数に持ち、全て$0$になるので、$y_{1}=x-\\dfrac{5}{3}x^3$\n$l=5$の時、7次項から$(l-5)$を因数に持ち、全て$0$になるので、$y_{1}=x-\\dfrac{14}{3}x^3+\\dfrac{21}{5}x^5$\n$l=1$の時、$x^2=1$から$y_{0}$は発散し、他の奇数の時も同様である。したがって、$l$が奇数の時は、解が奇数次項のみを持つ有限項の多項式となる。つまり、級数$y_{1}$の特定の項までのみ残る形の解を得る。\nそして、$l$が負の場合は、$l$が0ではない整数の場合と同じであることが$y_{0}$と$y_{1}$を見ればわかる。例えば、$l=2$の場合と$l=-3$の場合が同じであり、$l=1$の場合と$l=-2$の場合が同じである。したがって、$l$が非負の整数についてのみ考えれば良い。$a_{0}$と$a_{1}$の値を上手く選んで$x=1$の時の解が$y(x)=1$になるようにすると、これをルジャンドル多項式Legendre polynomialと言い、$P_{l}(x)$と書く。最初のいくつかのルジャンドル多項式は以下の通りである。\n$$ \\begin{align*} P_{0}(x) =\u0026amp;\\ 1 \\\\ P_{1}(x) =\u0026amp;\\ x \\\\ P_2(x) =\u0026amp;\\ \\dfrac{1}{2}(3x^2-1) \\\\ P_{3}(x) =\u0026amp;\\ \\dfrac{1}{2}(5x^3-3x) \\\\ P_{4}(x) =\u0026amp;\\ \\dfrac{1}{8}(35x^4-30x^2+3) \\\\ P_{5}(x) =\u0026amp;\\ \\dfrac{1}{8}(63x^5-70x^3+15x) \\end{align*} $$\nこの結果はロドリゲス公式Rodrigues\u0026rsquo; formulaで直接得ることもできる。\n■\nMary L. Boas, 数理物理学(Mathematical Methods in the Physical Sciences, 최준곤 訳) (3rd Edition, 2008), p577-580\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":889,"permalink":"https://freshrimpsushi.github.io/jp/posts/889/","tags":null,"title":"ルジャンドル微分方程式の直列解法：ルジャンドル多項式"},{"categories":"확률론","contents":"定義 確率変数 $X: \\Omega \\to E$ の値域を状態空間という。 確率変数の集合 $\\left\\{ X_{t} \\mid t \\in [ 0 , \\infty ) \\right\\}$ を連続的確率過程という。 確率変数の数列 $\\left\\{ X_{n} \\mid n = 0, 1, 2, \\cdots \\right\\}$ を離散的確率過程という。 説明 過程Processという言葉が含まれているため、確率過程を理解するのは難しい、典型的には言葉が難しいために難しい概念だ。「プロセス」とは通常、あるアルゴリズムや、言葉そのままの「過程」を意味するため、上の定義と全く合わないためである。高校を卒業すると、数列を「定義域が自然数の関数」と定義するため、「確率変数の数列」や「確率変数の集合」という説明を敢えてする。\n関数？数列？集合？ この意味で確率過程とは結局のところ、「時間的な変数 $t$ か $n$ に対して確率変数を対応させる関数」である。重要なのは結局「いつ、どのように確率が出るか？」であり、集合だの数列だの複雑に考える必要はない。人々は今日雨が降る確率も気になるし、明日の雨の確率も気になるし、明後日の雨の確率も気になる。今日をD+0、明日をD+1、明後日をD+2として、$p ( X_{n} = \\text{ rain } )$ をD+nの降水確率と表せるなら、確率過程の概念を素晴らしく理解したことになる。\n確率過程論は、その性質上、多くの学問分野で様々なレベルで学ばれるため、教科書によって言葉が異なる。少なくとも確率情報論を学び始めるときは、正確な定義よりも直観的な概念をうまく受け入れることがさらに重要だ。\n例 ギャンブル 例として、コイン投げゲームCash Processを考えてみよう。このゲームでは、コインを投げて表が出れば$1$ポイントを得て、裏が出れば$1$ポイントを失う。プレイヤーが終了を宣言した時点でゲームは終わり、最後にスコアが正ならばポイント一つにつき千円を受け取り、負ならばポイント一つにつき千円を支払わなければならないゲームである。スコアは$0$ポイントから始まる。\nまずこのゲームの状態空間はプレイヤーのスコアであり、整数の集合$\\left\\{ \\cdots, (- 2) , (-1) , 0 , 1 , 2 , \\cdots \\right\\}$になるだろう。プレイヤーが$n$回コインを投げた時のスコアが$x$ポイントである確率は$p( X_{n} = x)$のように表せる。特に、コインを一度も投げていない場合、私のスコアは必ず$0$ポイントであり、$p ( X_{0} = 0 ) = 1$を確信することができる。\nここで$X_{1}$は$(-1)$か$1$であり、$X_{2}$は$(-2) , 0 , 2$のうちの1つであることが確実だ。このように、試行回数$n$が変わると、確率変数$X_{n}$も変わっている。\n上述のゲームのシミュレーションを行うと、スコアの波は上のようにランダムに現れる。このようなランダムな波をブラウン運動Brownian Motionという。$10$回繰り返し、それでやめたなら2千円の賞金を手にしていただろうし、$400$回くらいでやめたならかなりの損失があっただろうし、$900$回くらいでやめたならかなりの大金を手にしていただろう。\n確率過程論は「では、いつやめるのがよいか」に対する答えも提供することができる。適切な目標を達成した時、次の機会はいつか、どれほどのリスクを負うかについても誰もが疑問に思う。\n株 もう一つの例は株式である。\nもちろん、株は完全にランダムではない。しかし、上のようなチャートを見て1年後の動向を予測するのは非常に難しい。確率過程論を勉強することは、チャーチストになることではなく、むしろその逆である。何が価格の変動に影響を与えるかを把握し、迅速に情報を取得し、正確なモデルを作成し、それを自分だけが知っていれば、一生懸命にならずとも生計を立てることができる（もちろん、不可能だが）。\n一方、数学的には、確率過程は非決定論的な動力学系とも見なすことができる。\nコード 以下はRを通じてキャッシュプロセスをシミュレーションした例のコードである。\nset.seed(150421)\rtoss\u0026lt;-sample(c(-1,1),10,replace=T)\rwin.graph(4,4)\rplot(cumsum(toss),type=\u0026#39;l\u0026#39;,main=\u0026#39;10회 반복\u0026#39;)\rabline(h=0)\rtoss\u0026lt;-sample(c(-1,1),1000,replace=T)\rwin.graph(4,4)\rplot(cumsum(toss),type=\u0026#39;l\u0026#39;,main=\u0026#39;1000회 반복\u0026#39;)\rabline(h=0) ","id":857,"permalink":"https://freshrimpsushi.github.io/jp/posts/857/","tags":["R"],"title":"確率過程とは何か？"},{"categories":"동역학","contents":"定義1 定義域と値域が同じ関数$f : X \\to X$をマップと言う。$f$を$k$回合成したマップを$f^{k}$と表す。 $f(p) = p$を満たす$p \\in X$を固定点と言う。 全ての$x \\in N_{ \\epsilon } ( p )$に対して$\\displaystyle \\lim_{k \\to \\infty} f^{k} (x) = p$を満たす$\\epsilon \u0026gt; 0$が存在する場合、固定点$p$をシンクとする。 $p$を除く全ての$x \\in N_{\\epsilon } (p)$に対して$f^{ \\infty } (x) \\notin N_{\\epsilon } (p)$を満たす$\\epsilon \u0026gt; 0$が存在する場合、固定点$p$をソースとする。 $N_{ \\epsilon } ( p ) = B ( p ; \\epsilon )$は、$p$の半径$\\epsilon$内にある全ての点を含むネイバーフッドを意味する。 例 $X$で定義されたマップは、各点$x_{t-1}$を$x_{t}$へマッピングすることで動力学系を形成する。例えば、時間$t$が$1$だけ変わるたびに、$60$だけ$x$方向へ移動する点がある場合、この点の位置は次のように表される。 $$ x_{t} = f(x_{t-1}) = x_{t-1} + 60 $$ 別のマップの例として$f(x) = x^3$を考えると、$$f(0) = 0 \\\\ f( \\pm 1) = \\pm 1$$であるので、$0$と$\\pm 1$は固定点である。 特に$0$を含む十分に小さい区間$( - 1, 1)$の全ての数は、二乗するたびに小さくなり、最終的には$0$に収束するので、シンクである。 $\\pm 1$を含むどんな区間を考えても、その大きさが$1$より大きい数は、三乗するたびにその大きさが大きくなるので、ソースである。 シンクは近くの点が集まる一種の「収束点」、ソースは近かった点が徐々に離れていく一種の「発散点」と見ることができる。だから、シンクを安定した固定点、ソースを不安定な固定点とも呼ぶ。\nこれはグラフ理論のシンク、ソースと似ている。\n参照 マップによって表される動力学系 微分方程式によって表される動力学系 動力学系の厳密な定義 Yorke. (1996). CHAOS: An Introduction to Dynamical Systems: p5, 9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":856,"permalink":"https://freshrimpsushi.github.io/jp/posts/856/","tags":null,"title":"地図で表される動力学系と不動点"},{"categories":"추상대수","contents":"定義 1 整域 $D$ で以下の二つの条件を満たす ユークリッドノルム $\\nu : D \\setminus \\left\\{ 0 \\right\\} \\to \\mathbb{N}_{0}$ が存在する場合、$D$ をユークリッド整域と言う。\n(i): すべての $a,b \\in D (b \\ne 0 )$ に対して $$ a = bq + r $$ を満たす $q$ と $r$ が存在する。この時、$r = 0$ または $\\nu (r) \u0026lt; \\nu (b)$ のどちらかでなければならない。 (ii): すべての $a,b \\in D (b \\ne 0 )$ に対して $\\nu ( a ) \\le \\nu ( ab )$ $\\mathbb{N}_{0}$ は自然数の集合に $0$ を含む集合を意味する。 定理 ユークリッド整域 $D$ の単位元を $0$、単位元を $1$, ユークリッドノルムを $\\nu$ としよう。\n[3]: $0$ ではないすべての $d \\in D$ に対して $\\nu (1) \\le \\nu (d)$ [4]: $u \\in D$ は単位元 $\\iff$ $\\nu ( u ) = \\nu (1)$ PIDは主理想整域を、UFDは一意分解整域を指す。 単位元は乗算の単位元 $1$ であり、単位元は乗算の逆元を持つ元である。 説明 「ユークリッド整域」という言葉はそれほど長くないが、通常EDという略称がよく使われる。\n条件 (i) と (ii) は整数環 $\\mathbb{Z}$ では自然に満たされている条件で、ユークリッドノルム $\\nu ( n ) := | n |$ が存在して $\\mathbb{Z}$ はユークリッド整域になる。もともとユークリッドノルムという言葉自体が数論のユークリッドの互除法から来ているのだ。\n一方、体 $F$ に対して $F [ x ]$ を考えると、ユークリッドノルム $\\nu ( f(x) ) : = \\deg ( f(x) )$ を定義することによってユークリッド整域になる。もともと割り算の定理がこの条件に該当する。\n上のように様々な整域を図示すると、EDがどれだけ多くの良い性質を持っているか簡単にわかる。\n証明 1 $D$ のイデアルを $N$ としよう。\n$N = \\left\\{ 0 \\right\\} = \\left\u0026lt; 0 \\right\u0026gt;$ は自然に主理想なので、$N \\ne \\left\\{ 0 \\right\\}$ を考えよう。\n$0$ ではないすべての $n \\in N$ に対して、 $$ \\nu (b) \\le \\nu (n) $$ を満たす $b \\ne 0$ を見つけることができる。これを$a \\in N$ とすると、条件 (i) により $$ a = b q + r $$ を満たす $q,r \\in D$ が存在しなければならない。$N = Nq$ はイデアルなので、$r = a - bq$ も $N$ に存在する元であることがわかる。$b$ は$\\nu (b)$ を最小にする元だったので、条件 (ii) により $r=0$ でなければならない。すべての元 $a \\in N$ が $a = bq$ として表されるということはつまり $N = \\left\u0026lt; b \\right\u0026gt;$ であり、すべてのイデアル $N$ は主理想である。\n■\n2 EDはPIDであり、PIDはUFDなので、EDもUFDである。\n■\n[3] 条件 (ii) により $$ \\nu (1) \\le \\nu ( 1 d) = \\nu (d) $$\n■\n[4] $( \\implies )$\n$u$ が単位元なので、その逆元 $u^{-1}$ が存在して $$ \\nu ( u ) \\le \\nu ( u u^{-1} ) = \\nu (1) $$ そして、定理 [3] により $\\nu (1) \\le \\nu (1)$ よって $$ \\nu ( u ) = \\nu (1) $$\n$( \\impliedby )$\n$1 = uq + r$ とすると、定理 2 により、$\\nu ( u) = \\nu (1)$ は $\\nu (0)$ を除いて最小である。定理 [3] により、$\\nu ( r) \u0026lt; \\nu (u)$ を満たす場合は $r=0$ のみで、$1 = uq$ となり、$u$ は単位元となる。\n■\n参照 ユークリッド整域 $\\implies$ 主理想整域 $\\implies$ 一意分解整域 $\\implies$ 整域 ユークリッド整域 $\\implies$ 主理想整域 $\\implies$ ネーター環 Fraleigh. (2003). 「抽象代数入門(第7版)」: p401.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":838,"permalink":"https://freshrimpsushi.github.io/jp/posts/838/","tags":null,"title":"ユークリッド幾何学"},{"categories":"추상대수","contents":"定義 1 整域 $D$ の$0$でもなく単元もない全ての要素に対して有限素因数分解が一意に存在する場合、$D$を一意素因数分解整域UFDという。 一意素因数分解整域 $D$ の $a_{1} , \\cdots , a_{n}$ に対して$d \\mid a_{i}$であり、$a_{i}$の全ての約数が$d$を割る場合、$d$を$a_{1} , \\cdots , a_{n}$の最大公約数Greatest Common Divisorといい、$\\gcd$と書く。 一意素因数分解整域 $D$ のある多項式を$f(x) := a_{0} + a_{1} x + \\cdots + a_{n} x^{n}$とする。$\\gcd ( a_{0} , a_{1} , \\cdots , a_{n} ) = 1$の場合、$f(x) \\in D [ x ]$を原始的Primitiveという。 単位元は乗算に対する恒等元$1$であり、単元は乗算に対する逆元を持つ元である。 定理 2 [2] 算術の基本定理：$\\mathbb{Z}$はUFDである。 [3] ガウスの補助定理：$D$がUFDであれば、$D [ x ]$の原始多項式たちの積も原始的である。 [4]: $D$がUFDであれば、$D [ x ]$もUFDである。 [5]: $F$が体であれば、$F[ x_{1} , \\cdots , x_{n} ]$はUFDである。 説明 「一意素因数分解整域」という言葉は通常、長いためによくUFDという略語が使われる。\nUFD 要素の有限素因数分解が存在することは、与えられた要素が有限数の既約元の積で表されることを意味する。UFDが便利である理由は、より大きなオブジェクトを分割して考えることができるようになるためである。定義上その要素が何であるかは指摘できなくても、そのような素因数分解が存在するだけで大いに役立つ。これにより、我々が考える「常識的な」計算が成り立つ整域となる。\nUFDの例は非常に多い。例えば、定理 [2] で言及されているように、整数環$\\mathbb{Z}$がそうである。しかし、整数環に$\\sqrt{-5}$を加えた単純拡大体$\\mathbb{Z} ( \\sqrt{ - 5 } )$を考えてみよう。ここで、$21 \\in \\mathbb{Z} ( \\sqrt{ - 5 } )$は素因数分解$21 = 3 \\cdot 7$を持つ一方で、$21 = ( 1 + 2 \\sqrt{-5}) ( 1 - 2 \\sqrt{-5}) $も可能であり、一意ではないため、$\\mathbb{Z} ( \\sqrt{ - 5 } )$が一意素因数分解整域でないことが容易に確認できる。\n原始的関数？ 関数が原始的であるとは、積分学における原始関数とは全く関係なく、$(3 x^2 + 6 x + 3) \\in \\mathbb{Z} [ x ]$が$3 ( x^2 + 2x + 1)$のように全体を$3$で囲むこととは異なり、係数を囲むことができない関数を指す。\n算術の基本定理 整数論におけるステートメントとは異なり、整数環$\\mathbb{Z}$がUFDであることを要約したものである。もちろん、この宣言のためには無数の概念が動員されているが、高度な整数論ではこのように代数の言葉で表現されることが多いため、代数学の学習は不可欠である。代数学を専攻しなくても、代数学の知識がなければ理解が難しい。\nガウスの補助定理 ガウスの補助定理は思っているよりも面白い定理である。例えば、$(5x + 1) , (2x^2 + 3x + 1) \\in \\mathbb{Z} [ x ]$を考えると、その積は$( 10 x^3 + 17 x^2 + 8 x + 1 )$であり、一見するといかなる最大公約数$a \\in \\mathbb{Z}$で囲むこともできない。一つくらいは反例が見つかりそうだが、ガウスの補助定理のおかげで、そうした無駄な努力をする必要はなくなる。\n証明 1 Part 1. 存在性\n$D$がPIDである場合、$d \\in D$は既約元$p_{1} , \\cdots , p_{r}$たちの有限積$a = p_{1} \\cdots p_{r}$として表現される。\nPart 2. 一意性\n別の既約元$q_{1} , \\cdots , q_{s}$に対して$a = q_{1} \\cdots q_{s}$も可能であるとしよう。\nPIDの既約元は素元であるため、ある$1 \\le j \\le s$に対して$p_{1} \\mid q_{j}$でなければならない。 $$ p_{1} p_{2} \\cdots p_{r} = p_{1} u_{1} q_{2} \\cdots q_{s} $$ 両辺から$p_{1}$を取り除くと $$ p_{2} \\cdots p_{r} = u_{1} q_{2} \\cdots q_{s} $$ 同じ方法で$i=r$まで繰り返すと $$ 1 = u_{1} \\cdots u_{r} q_{r+1} \\cdots q_{s} $$ を得る。$q_{r+1} \\cdots q_{s}$は既約元であるため、$r=s$を割る必要がある。\n■\n[2] $\\mathbb{Z}$の全てのイデアルは$\\left\u0026lt; n \\right\u0026gt; = n \\mathbb{Z}$の形であるためPIDであり、定理 1 によってUFDである。\n■\n[3] $$ \\begin{align*} f(x) \u0026amp;:= a_{0} + a_{1} x + \\cdots + a_{n} x^n \\\\ g(x) \u0026amp;:= b_{0} + b_{1} x + \\cdots + b_{m} x^m \\end{align*} $$ 原始多項式$f(x) , g(x) \\in D[x]$を上記のように表わそう。\n$p \\in D$を既約元としよう。\n$f(x)$は原始的であるため$\\gcd ( a_{0} , \\cdots , a_{n} ) = 1$であり、$p$が$a_{0} , \\cdots , a_{n}$を全て割ることはできない。それに$i = 0, 1 , \\cdots , n$に対して$p$が$a_{i}$を割ることができない最初の係数を$a_{r}$としよう。 $g(x)$も原始的であるため$\\gcd ( b_{0} , \\cdots , b_{m} ) = 1$であり、$p$が$b_{0} , \\cdots , b_{m}$を全て割ることはできない。それに$j = 0, 1 , \\cdots , m$に対して$p$が$b_{j}$を割ることができない最初の係数を$b_{s}$としよう。 すると、$f(x)g(x)$の$( r + s)$次の項の係数は $$ c_{r+s} = ( a_{0} b_{r+s} + \\cdots + a_{r-1} b_{s+1} ) + a_{r} b_{s} + ( a_{r+1} b_{s-1} + \\cdots + a_{r+s} b_{0} ) $$ となり、\n$a_{r}$の定義によれば $$ p \\mid ( a_{0} b_{r+s} + \\cdots + a_{r-1} b_{s+1} ) $$ $b_{s}$の定義によれば $$ p \\mid ( a_{r+1} b_{s-1} + \\cdots + a_{r+s} b_{0} ) $$ である。しかし、$p \\nmid a_{r} b_{s}$であるため、与えられた$p$は$f(x) g(x)$を割ることができない。これは全ての既約元についても同様であるため、$f(x) g(x)$は原始的である。\n■\n[4] $f(x) \\in D[x]$の次数を$n$としよう。\nすると、$f(x)$は $$ f (x) = g_{1} (x) \\cdots g_{r} (x)) $$ のように因数分解できる。また、$i = 1 , \\cdots , r$に対して、それぞれの因数を原始関数$h_{i} (x) \\in D[x]$と$c_{i} \\in D$の積である $$ g_{i} (x) = c_{i} h_{i} (x) $$ として表わすことができる。このような$c_{i}$を$g_{i} (x)$のコンテントContentと\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p390, 395~396.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p394~399。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":827,"permalink":"https://freshrimpsushi.github.io/jp/posts/827/","tags":null,"title":"一意因数分解整域"},{"categories":"추상대수","contents":"定義 1 整域 $D$ の $p \\ne 0$ が単元でないとする。\nPID $D$ の全てのイデアルが主イデアルである場合、$D$ を主イデアル整域PIDと呼ぶ。\n従属定義 可換環 $R$ が単位元 $1$ を持つとする。$a,b \\in R$ に対して $b=ac$ を満たす $c \\in R$ が存在する場合、$a$ が $b$ を割るDivideまたは$a$ が $b$ の因子Factorであると言い、$a \\mid b$ のように表す。 $a \\mid b$ かつ $b \\mid a$ の場合、$a,b$ が連想Associatesであると言う。 $\\forall a,b \\in D$ と $p=ab$ に対して、$a$ か $b$ のいずれかが単元である場合、$p$ を既約元Irreducible Elementと言う。 $\\forall a,b \\in D$ に対して、$p \\mid ab$ の場合、$p \\mid a$ または $p \\mid b$ の $p$ を素元Prime Elementと言う。 単位元は乗算に対する単位元 $1$、単元は乗算に対する逆元を持つ要素である。 定理 2 $D$ が主イデアル整域であるとする。\n[1]: $D$ はネーター環である。 [2]: $0$ でも単元でもない $d \\in D$ は、$D$ の既約元の積として表される。 [3]: $\\left\u0026lt; p \\right\u0026gt;$ が $D$ の極大イデアルである場合、$p$ は $D$ の既約元である。 [4]: $D$ の既約元は素元である。 説明 「主イデアル整域」という言葉は長いため、通常はPIDという略語がよく使用される。\n連想は結合則とスペルは同じだが名詞形であることに注意し、$-3,3 \\in \\mathbb{Z}$ のように互いに単元の積で表すことができる関係である。\n例 整数環 $\\mathbb{Z}$ 整数環 $\\mathbb{Z}$ は全てのイデアルが $n \\mathbb{Z} = \\left\u0026lt; n \\right\u0026gt;$ のように主イデアルとして表される。\n全ての体 $\\mathbb{F}$ ガウス整数環 $\\mathbb{Z} [i]$ とアイゼンシュタイン整数環 $\\mathbb{Z} [\\omega]$ ガウス整数環とアイゼンシュタイン整数環はそれぞれ整数環 $\\mathbb{Z}$ に純虚数 $i := \\sqrt{-1}$ または $\\omega := (-1)^{1/3}$ を加えた環である。\n証明 [1] ネーター環の定義: $N$ を環とする。\n$N$ のイデアルが $S_{1} \\le S_{2} \\le \\cdots$ を満たす場合、これを昇鎖Ascending Chainという。 昇鎖 $\\left\\{ S_{i} \\right\\}_{i \\in \\mathbb{N} }$ に対して、$S_{n} = S_{n+1} = \\cdots$ を満たす $n \\in \\mathbb{n}$ が存在する場合、定常Stationaryであるという。つまり、定常昇鎖では、ある時点からイデアルがこれ以上大きくならない。 すべての昇鎖が定常である環をネーター環という。 $D$ のイデアルの昇鎖 $N_{1} \\le N_{2} \\le \\cdots$ とその和集合 $\\displaystyle N := \\bigcup_{k=1}^{ \\infty } N_{k}$ を考える。ある $i, j \\in \\mathbb{N}$ に対して $$ a \\in N_{i} \\\\ b \\in N_{j} \\\\ N_{i} \\le N_{j} $$ とすると、$( N_{j} , + , \\cdot )$ はイデアルによって定義されるため、部分環であり、$b$ の加算に対する逆元 $(-b) \\in N_{j}$ が存在する。また、$ab \\in N_{j}$ であるため、$(a-b), ab \\in N$ であり、部分環判定法により、$N$ は $D$ の部分環である。それだけでなく、$N_{i}$ がイデアルである\nため、全ての $d \\in D$ に対して $d a = a d$ であり、$da \\in N$ であるため、$N$ は $D$ のイデアルである。\n$D$ はPIDであるため、全てのイデアルが主イデアルであり、ある $c \\in N$ に対して $N = \\left\u0026lt; c \\right\u0026gt;$ のように表せる。ここで、$\\displaystyle N = \\bigcup_{k=1}^{ \\infty } N_{k}$ であるため、$c \\in N$ であれば、$c \\in N_{r}$ を満たす自然数 $r \\in \\mathbb{N}$ が存在しなければならない。$c \\in N_{r}$ は、$N_{r}$ より小さいイデアルの中に $c$ を生成元とする主イデアルが存在することを意味する。数式で表すと $$ \\left\u0026lt; c \\right\u0026gt; \\le N_{r} \\le N_{r+1} \\le \\cdots \\le N = \\left\u0026lt; c \\right\u0026gt; $$ となり、$N_{r} = N_{r+1} = \\cdots$ である。したがって、$D$ はネーター環である。\n■\n[2] $d$ が既約元であれば証明する必要はないため、単元でない $d_{1}, c_{1} \\in D$ に対して、$d = d_{1} c_{1}$ のように表されるとする。\nすると、$\\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; d_{1} \\right\u0026gt;$ であり、$d_{i} := d_{i+1} c_{i+1}$ を続けて定義すると、昇鎖 $$ \\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; d_{1} \\right\u0026gt; \\le \\left\u0026lt; d_{2} \\right\u0026gt; \\le \\cdots $$ を得る。しかし、定理[1]により、この鎖が終わる $a_{r}$ が存在し、$a_{r}$ は同時に $a$ の因子である既約元となる。このように、$d$ を割る既約元を $p_{1}$ とし、単元でない $f_{1}$ に対して、$d = p_{1} f_{1}$ とすると、$\\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; f_{1} \\right\u0026gt;$ となり、$f_{i} := p_{i+1} f_{i+1}$ を続けて定義すると、昇鎖 $$ \\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; f_{1} \\right\u0026gt; \\le \\left\u0026lt; f_{2} \\right\u0026gt; \\le \\cdots $$ を得る。これも定理[1]により、この鎖が終わる $f_{s}$ が存在し、$f_{s}$ は同時に $f_{i}$ の因子である既約元となる。\nこのプロセスを有限回繰り返すことで、$d$ が既約元の積として表されることが確認できる。\n■\n[3] $( \\implies )$\n$D$ の極大イデアル $\\left\u0026lt; p \\right\u0026gt;$ の $p$ が、$D$ の単元でない $a,b$ に対して、$p=ab$ のように表されると仮定する。\nすると、$\\left\u0026lt; p \\right\u0026gt; \\le \\left\u0026lt; a \\right\u0026gt;$ であり、$\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ の場合、$b$ は単元でなければならないため、実際には $\\left\u0026lt; p \\right\u0026gt; \\lneq \\left\u0026lt; a \\right\u0026gt;$ を得る。しかし、$\\left\u0026lt; p \\right\u0026gt;$ が極大イデアルであるため、$\\left\u0026lt; a \\right\u0026gt; = D = \\left\u0026lt; 1 \\right\u0026gt;$ でなければならず、$a$ と $1$ は連想される。要約すると、\n$\\left\u0026lt; p \\right\u0026gt; \\ne \\left\u0026lt; a \\right\u0026gt;$ の場合、$a$ は単元であり、 $\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ の場合、$b$ は単元であるため、 $p$ は既約元である。\n$( \\impliedby )$\n既約元 $p=ab$ に対して、$\\left\u0026lt; p \\right\u0026gt; \\le \\left\u0026lt; a \\right\u0026gt;$ と仮定する。\n$a$ が単元であれば、$\\left\u0026lt; a \\right\u0026gt; = D$ で問題はないが、$a$ が単元でない場合、$b$ は必ず単元でなければならない。\n$b$ が単元であるということは、ある $u \\in D$ に対して、$bu =1$ という意味であるが、 $$ pu = abu = a $$ となるため、$\\left\u0026lt; p \\right\u0026gt; \\ge \\left\u0026lt; a \\right\u0026gt;$、つまり$\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ でなければならない。要約すると、\n$\\left\u0026lt; a \\right\u0026gt; = D$ であるか、 $\\left\u0026lt; a \\right\u0026gt; = \\left\u0026lt; p \\right\u0026gt;$ である必要があるため、 $\\left\u0026lt; p \\right\u0026gt;$ は極大イデアルとなる。\n■\n[4] $p$ が既約元であるとすると、$\\left\u0026lt; p \\right\u0026gt;$ は定理[3]により極大イデアルであり、$1 \\in D$ であるため、素イデアルである。\n$p$ が $ab$ を割るとすると、$(ab) \\in \\left\u0026lt; p \\right\u0026gt;$ であり、$\\left\u0026lt; p \\right\u0026gt;$ が素イデアルであるため、$a \\in \\left\u0026lt; p \\right\u0026gt;$ または $b \\in \\left\u0026lt; p \\right\u0026gt;$ である。これを別の形で表すと、$p \\mid ab$ の場合、$p \\mid a$ または $p \\mid b$ であるため、$p$ は素元となる。\n■\n関連項目 ユークリッド整域 $\\implies$ 主イデアル整域 $\\implies$ 一意分解整域 $\\implies$ 整域 ユークリッド整域 $\\implies$ 主イデアル整域 $\\implies$ ネーター環 Fraleigh. (2003). A First Course in Abstract Algebra(7th Edition): p389~391, 394.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A First Course in Abstract Algebra(7th Edition): p392~393.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":825,"permalink":"https://freshrimpsushi.github.io/jp/posts/825/","tags":null,"title":"主イデアル整域"},{"categories":"전자기학","contents":"説明1 電界は常にカール(回転)が$\\mathbf{0}$になる特別なベクトル関数だ。この特性から、電場$\\mathbf{E}$に関連する電位electric potentialというスカラー関数を導入する。電位は$V$と表記され、電場$\\mathbf{E}$と以下の関係が成り立つ。\n$$ \\mathbf{E} = -\\nabla V $$\n従って、電位$V$を知れば、電場$\\mathbf{E}$を知ることができる。電位はスカラー関数なので、ベクトル関数である電場を直接求めるよりも、電位を求める方が簡単である。電場と電位の関係は、重力と位置エネルギーの関係に似ている。ただし、電場の単位は力ではないため、電位は正確にはポテンシャルエネルギーではなく、単にポテンシャルである。\n以下の結果は、電場が正確に電位の勾配であり、電位を知っていればその勾配を計算して電場を知ることができるという意味である。また、この内容は電場だけでなく、カールが$\\mathbf{0}$になるすべてのベクトル関数に適用される。\n導出 $\\nabla \\times \\mathbf{E}=0$であることを証明する過程で、電場の閉路に対する線積分が$0$であることがわかった。上の図で、$1$の経路と$2$の経路を組み合わせると、点$\\mathbf{a}$から点$\\mathbf{a}$へ戻る閉路が形成される。したがって、\n$$ \\int _{1} \\mathbf{E} \\cdot d\\mathbf{l} + \\int_2 \\mathbf{E} \\cdot d\\mathbf{l} =0 $$\nとなるので、$1$の経路と$2$の経路の積分値は、大きさは同じで符号は逆である。$1$の経路を反転させ、両方とも点$\\mathbf{a}$から点$\\mathbf{b}$へ行く経路とすると、両者は同じ値を持つ。\n電場の線積分は経路に依存しないので、ある基準点$\\mathcal{O}$から位置$\\mathbf{r}$までの積分は常に同じ値を持つ。従って、スカラー関数$V$を以下のように定義しよう。\n$$ V(\\mathbf{r} ) \\equiv - \\int _\\mathcal{O} ^{\\mathbf{r}} \\mathbf{E} \\cdot d \\mathbf{l} $$\nすると、電位の定義により、次の式が成り立つ。\n$$ \\begin{align*} V(\\mathbf{b} )- V( \\mathbf{a} ) =\u0026amp;\\ -\\int _\\mathcal{O} ^{\\mathbf{b}} \\mathbf{E} \\cdot d\\mathbf{l} +\\int_\\mathcal{O} ^{\\mathbf{a}} \\mathbf{E} \\cdot d \\mathbf{l} \\\\ =\u0026amp;\\ -\\int_\\mathbf{a} ^\\mathbf{b} \\mathbf{E} \\cdot d\\mathbf{l} \\end{align*} $$\n勾配の基本定理\n$$ T(b)-T(a) = \\int _{a}^{b} (\\nabla T) \\cdot d\\mathbf{l} $$\nまた、勾配の基本定理により、次の式が成り立つ。\n$$ V( \\mathbf{b} ) - V (\\mathbf{a} ) = \\int_{\\mathbf{a}}^{\\mathbf{b}}\\left( \\nabla V \\right) \\cdot d\\mathbf{l} $$\n従って、$\\displaystyle \\int_\\mathbf{a} ^ \\mathbf{b} \\left( \\nabla V \\right) \\cdot d\\mathbf{l} = -\\int_\\mathbf{a} ^\\mathbf{b} \\mathbf{E} \\cdot d\\mathbf{l}$なので、\n$$ \\mathbf{E} = -\\nabla V $$\n■\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 金珍勝訳) (4th Edition, 2014), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":845,"permalink":"https://freshrimpsushi.github.io/jp/posts/845/","tags":null,"title":"ポテンシャル"},{"categories":"통계적분석","contents":"定義 1 多重回帰分析 $Y \\gets X_{1} , \\cdots, X_{p}$ をするとしよう。このとき、独立変数 $ X_{1} , \\cdots, X_{p}$ の中で独立変数同士が強い相関関係を持っている場合、多重共線性Multicollinearityがあるとされる。\n実践 もともと、独立変数同士が依存していること自体が回帰分析の前提に反する話であり、実際には数値的な問題を引き起こし分析結果を信頼できなくする。データによっては多重共線性があるかどうかを見つけ出すことからが仕事となる。\nデータ探索 組み込みデータから MplsDemo データを 読み込んでみよう。\nMplsDemoはアメリカ ミネアポリス地域を区別して、population(総人口)、white(白人比率)、black(黒人比率)、foreignBorn(外国生まれ)、hhIncome(世帯所得)、poverty(貧困)、collegeGrad(大学卒業者率)を推定したデータだ。\n回帰分析後の仮説検定をみると、特に問題はなさそうで説明力もまあまあ良く、残差図も良好だ。しかし、回帰係数をよく見ると、白人が多いほど大卒者が増え、外国生まれが多いほど大卒者が増えるように見える。もちろんこのデータがすべての人種について正確な情報を持っているわけではないが、これは何かおかしい。また、所得が多いほど大卒者が増えるのはそうだとしても、貧困率が全く影響を与えないのもなんとなく気持ち悪い。\n散布図を見ると、誰が見ても白人、世帯所得が正の相関関係、黒人、外国生まれが負の相関関係を持つのが正常に見える。貧困はあいまいだけれど、無理に言えば弱い負の相関関係を示す。しかし、黒人と外国生まれに対する回帰係数が正であることは、何かこのデータが正しく説明されていないことを示唆している。もちろん説明力自体は0.8を超える程度にはまあまあだが、どこか納得いかない点が明らかにある。多重共線性を考えると、白人が他の変数と強い相関関係があるのが引っかかる。\nモデル修正 白人比率を表す独立変数 whiteを削除して、再度回帰分析をやってみよう。\n新しい分析結果は、説明力が約10%近く落ちたものの、散布図から予想された回帰関係を比較的まともに説明していることが確認できる。外国生まれは少し係数が変だが、回帰係数が有意でないので気にする必要はなさそうだ。ここら辺から、より洗練された分析のためにデータをどう扱うかは完全に分析者にかかっている。\n多重共線性の検出 多重共線性がある可能性が高い状況は以下のような場合がある：\nF検定は合格したが、各々の回帰係数が t検定を通過しない場合 予想したこととは異なり、回帰係数の符号が反対でずれが大きい場合 データを追加または削除するときに、既存の回帰係数が極端に変化する場合 1の場合は、それでも多重共線性を発見できたという点では幸運なことだ。データをどう扱ってどう問題を解決するかは別として、多重共線性があるという事実自体は把握されたからである。\n2の場合は、直感と違うから発見しやすいが、「予想されること」とデータによっては多重共線性を把握するのが非常に難しくなる場合がある。例えば、植物の生長に影響を与える原因を調べている場合、独立変数として日光や水の量、土壌の質などが生長に役立つかどうか大まかな予測は可能だ。しかし、社会科学で未知の問題を解決しなければならない場合は、分析者の直感自体が信じがたくなる。多重共線性があっても分析は正しくされたように見えるため、適切なレビューがなければ実際の現象を全く説明できない分析結果を出すかもしれない。\n3の場合は、独立変数が非常に多い場合、回帰係数一つ一つに気を使うのが難しくなり、直接見ても見過ごすことがある。データとは必ずしも独立変数だけでなく、何かの異常値になることもある。\nどのケースでも、分析が誤っていて問題が発生したことから、説明力 $R^2$が低いため多重共線性を疑うことができる。しかし、説明力にはどれくらいから良いという基準もなく、間違った分析でも説明力が高く出ることがあり、それすらも大部分は主観的で、参考にならないことがある。もともと、現実の中で得られるデータはすべての変数が完全に独立である場合がより珍しい。多重共線性と言うほどではないけれど、ある程度は関係があり、あいまいな影響を与えるのが普通だ。\n例のように散布図を通じて図で把握することも万能とは言えない。もちろん散布図を見れば二つの変数間の関係はすぐにわかるが、$X_{1}+ X_{2} + X_{3} = 1$のように複数の変数が複雑な関係を持っている場合、目で見つけるのは難しい。\n数値的指標 そうなると、当然ながら数値的な指標を考えるしかない。このために最も好んで使われるのが分散膨張因子(VIF)であり、多重共線性を見つけるのに役立っている。ただしVIFはどのような確率分布に従うわけでもないため、仮説検定を行うことができない。だから経験的に定められた基準を超えると多重共線性があると主張するしかない。その経験的基準もあいまいな時があり、悩みの種だ。回帰分析をするということは実際にはこの多重共線性との戦いと言える程度である。\nVIF以外にも、主成分分析を通じて得られる条件数のような指標もあるが、あまり使われない。\nコード 以下は例示コードです。\ninstall.packages(\u0026#39;car\u0026#39;)\rlibrary(car)\rDATA=MplsDemo; head(DATA)\rwin.graph()\rplot(DATA[,-1])\rout0\u0026lt;-lm(collegeGrad~.-neighborhood,data=DATA)\rsummary(out0)\rwin.graph(4,4)\rplot(out0$residuals, main=\u0026#34;잔차\u0026#34;)\rout1\u0026lt;-lm(collegeGrad~.-neighborhood-white,data=DATA)\rsummary(out1) Hadi. (2006). Regression Analysis by Example(4th Edition): p222.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":808,"permalink":"https://freshrimpsushi.github.io/jp/posts/808/","tags":["R"],"title":"多重共線性"},{"categories":"전자기학","contents":"クーロンの法則1 固定された点電荷 $q$から距離$\\cR$だけ離れたところにある試験電荷$Q$が受ける力をクーロン力といい、その式は次の通りである。\n$$ \\mathbf{F} = \\dfrac{1}{4\\pi \\epsilon_{0}} \\dfrac{qQ}{\\cR ^2} \\crH $$\nこれをクーロンの法則Coulomb\u0026rsquo;s lawという。\n説明 クーロンの法則は繰り返しの実験から得られた実験法則である。だから数学的に証明することはできない。数学の公理みたいに考えると、理解しやすいだろう。$\\epsilon_{0}$は真空中の誘電率permittivity of free spaceで、その値は$8.85 \\times 10^{-12} \\dfrac{\\mathrm C^2}{\\mathrm N \\cdot \\mathrm m^2}$である。一方、文の上部の式は国際単位系système international, SIで表されている。ガウス単位系Gaussian systemで表すと、以下のようになる。\n$$ \\mathbf{F} = \\dfrac{qQ}{\\cR ^2} \\crH $$\nこれは、国際単位系の前に比例定数を$1$に置き換えるものである。つまり、$\\dfrac{1}{4\\pi\\epsilon_{0}} \\equiv 1$ということである。言い換えると、国際単位系をガウス単位系に簡単に変換する方法は、$\\epsilon_{0}$を$\\dfrac{1}{4\\pi}$に置き換えればいい。\n電場 点電荷分布 今、試験電荷$Q$の周りにいくつかの点電荷があるとしよう。その場合、$Q$が受ける力は単純に各点電荷から受ける力を線形に足すだけでよい。つまり$Q$と$q_{1}$の相互作用は$q_{2}, q_{3}, \\dots$に影響されないという意味である。これを重ね合わせの原理superposition principleという。\n$$ \\begin{align*} \\mathbf{F} \u0026amp;= F_{1}+F_{2}+\\cdots + F_{n} \\\\ \u0026amp;= \\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{1}Q}{{\\cR_{1}}^2}\\crH_{1} +\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{2}Q}{{\\cR_{2}}^2}\\crH_{2}+\\cdots +\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{n}Q}{{\\cR_{n}}^2}\\crH_{n} \\\\ \u0026amp;= Q\\left( \\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{1}}{{\\cR_{1}}^2}\\crH_{1} +\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{2}}{{\\cR_{2}}^2}\\crH_{2}+\\cdots +\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{n}}{{\\cR_{n}}^2}\\crH_{n} \\right) \\\\ \u0026amp;= Q\\mathbf{E} \\end{align*} $$\nここで、括弧内の部分を源電荷$q_{1},\\ q_{2},\\ \\cdots ,\\ q_{n}$たちが作る電場electric fieldと定義し、$\\mathbf{E}$と表示する。\n$$ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\sum \\limits_{i=1}^n \\dfrac{q_{i}}{{\\cR_{i}}^2}\\crH_{i} $$\n連続電荷分布 電荷が連続的に分布している場合は、合計の代わりに積分で表される。\n$$ \\sum \\rightarrow \\int \\\\ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\int \\dfrac{1}{\\cR^2}\\crH dq $$\n線電荷の場合は$dq=\\lambda dl^{\\prime}$。ここで$\\lambda$は線電荷密度である。線電荷が作る電場は以下のようである。\n$$ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\int _\\mathcal{P} \\dfrac{\\lambda (\\mathbf{r}^{\\prime})}{\\cR^2} \\crH dl^{\\prime} $$\n面電荷の場合は$dq=\\sigma da^{\\prime}$。ここで$\\sigma$は面電荷密度である。面電荷が作る電場は以下のようである。\n$$ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\int _\\mathcal{S} \\dfrac{\\sigma (\\mathbf{r}^{\\prime})}{\\cR^2} \\crH da^{\\prime} $$\n体積電荷の場合は$dq=\\rho d\\tau^{\\prime}$。ここで$\\rho$は体積電荷密度である。体積電荷が作る電場は以下のようである。\n$$ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\int _\\mathcal{V} \\dfrac{\\rho (\\mathbf{r}^{\\prime})}{\\cR^2} \\crH d\\tau^{\\prime} $$\nDavid J. Griffiths, 基礎電磁気学（Introduction to Electrodynamics, 金進世訳）(第4版). 2014, p65-70\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":836,"permalink":"https://freshrimpsushi.github.io/jp/posts/836/","tags":null,"title":"クーロンの法則と電場"},{"categories":"해석개론","contents":"この投稿はリーマン-スティルチェス積分を基準に書かれている。$\\alpha=\\alpha (x)=x$と設定すれば、リーマン積分と同じだ。\n定義 $P^{\\ast}$と$P$が$[a,b]$の分割であり、$P \\subseteq P^{\\ast}$を満たす場合、$P^{\\ast}$を$P$の細分refinementという。従って、$P$の全ての点は$P^{\\ast}$の点である。\n任意の二つの分割$P_{1}$と$P_{2}$に対して、$P_{3}=P_{1} \\cup P_{2}$を$P_{1}$と$P_{2}$の共通細分という。\n高等学校で積分を定義する時、与えられたグラフを$n$等分し、$n$が無限大になる極限を取っていたことを思い出してみると、細分の役割がすぐに理解できるだろう。\n定理 $P^{\\ast}$が$P$の細分であるとする。すると、以下の二つの式が成立する。\n$$ \\begin{align} L(P,f,\\alpha) \u0026amp;\\le L(P^{\\ast},f,\\alpha) \\label{eq1} \\\\ U(P^{\\ast},f,\\alpha) \u0026amp;\\le U(P,f,\\alpha) \\label{eq2} \\end{align} $$\nこの時、$L$と$U$はそれぞれリーマン(-スティルチェス)上和、下和である。\nつまり、分割が細分化されるほど、下和は大きくなり、上和は小さくなるということだ。\n証明 証明に先立って、以下のように与えられたとする。\n$f : [a,b] \\to \\mathbb{R}$が有界である。 $\\alpha : [a,b] \\to \\mathbb{R}$は単調増加関数である。 $P$を$[a,b]$の分割とする。 $P^{\\ast}$が$P$よりもちょうど一点多い細分であるとし、その点を$x^{\\ast}$とし、ある$i=1,\\cdots ,n$に対して$x_{i-1} \u0026lt; x^{\\ast} \u0026lt; x_{i}$とする。\n$\\eqref{eq1}$ $P$に対するリーマン(-スティルチェス)下和は次のようになる。\n$$ \\begin{align*} L(P,f,\\alpha) \u0026amp;= \\sum \\limits _{i=1} ^n m_{i} \\Delta \\alpha_{i} \\\\ \u0026amp;= m_{1}\\Delta \\alpha_{1} + \\cdots + m_{i} \\left[ \\alpha (x_{i}) - \\alpha (x_{i-1}) \\right] + \\cdots + m_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;= m_{1}\\Delta \\alpha_{1} + \\cdots + m_{i} \\left[ \\alpha (x_{i}) -\\alpha (x^{\\ast}) \\right] + m_{i} \\left[ \\alpha (x^{\\ast})- \\alpha (x_{i-1}) \\right] + \\cdots + m_{n}\\Delta \\alpha_{n} \\end{align*} $$\nそして、以下のように設定する。\n$$ \\begin{align*} w_{1} \u0026amp;= \\inf f(x) \u0026amp;(x_{i-1} \\le x \\le x^{\\ast}) \\\\ w_2\u0026amp;= \\inf f(x) \u0026amp;(x^{\\ast} \\le x \\le x_{i}) \\end{align*} $$\nすると、$m_{i}=\\inf f(x)\\ \\ (x_{i-1} \\le x \\le x_{i})$であるため、次が成り立つ。\n$$ m_{i} \\le w_{1} \\quad \\text{and} \\quad m_{i} \\le w_2 $$\n従って、次を得る。\n$$ \\begin{align*} m_{i} \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + m_{i}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] \u0026amp;\\le w_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + w_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] \\\\ \u0026amp;= w_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] + w_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] \\end{align*} $$\nしたがって、次が成り立つ。\n$$ \\begin{align*} L(P,f,\\alpha) \u0026amp;= m_{1}\\Delta \\alpha_{1} + \\cdots + m_{i} \\left[ \\alpha (x_{i}) -\\alpha (x^{\\ast}) \\right] + m_{i} \\left[ \\alpha (x^{\\ast})- \\alpha (x_{i-1}) \\right] + \\cdots + m_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;\\le w_{1}\\Delta \\alpha_{1} + \\cdots + w_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] + w_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + \\cdots + m_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;= L(P^{\\ast},f,\\alpha) \\end{align*} $$\n■\n$\\eqref{eq2}$ $\\eqref{eq1}$と同じ方法で証明する。$P$に対するリーマン(-スティルチェス)上和は次のようになる。\n$$ \\begin{align*} U(P,f,\\alpha) \u0026amp;= \\sum \\limits _{i=1} ^n M_{i} \\Delta \\alpha_{i} \\\\ \u0026amp;= M_{1}\\Delta \\alpha_{1} + \\cdots + M_{i} \\left[ \\alpha (x_{i}) - \\alpha (x_{i-1}) \\right] + \\cdots + M_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;= M_{1}\\Delta \\alpha_{1} + \\cdots + M_{i} \\left[ \\alpha (x_{i}) -\\alpha (x^{\\ast}) \\right] + M_{i} \\left[ \\alpha (x^{\\ast})- \\alpha (x_{i-1}) \\right] + \\cdots + M_{n}\\Delta \\alpha_{n} \\end{align*} $$\nそして、以下のように設定する。\n$$ \\begin{align*} W_{1} \u0026amp;= \\sup f(x)\u0026amp; (x_{i-1} \\le x \\le x^{\\ast}) \\\\ W_2\u0026amp;= \\sup f(x)\u0026amp;(x^{\\ast} \\le x \\le x_{i}) \\end{align*} $$ すると、$M_{i}=\\sup f(x)\\ \\ (x_{i-1} \\le x \\le x_{i})$であるため、次が成り立つ。\n$$ W_{1} \\le M_{i} \\quad \\text{and} \\quad W_2 \\le M_{i} $$\n従って、次を得る。\n$$ \\begin{align*} M_{i} \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + M_{i}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] \u0026amp; \\ge W_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + W_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] \\\\ \u0026amp;= W_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] + W_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] \\end{align*} $$\nしたがって、次が成り立つ。\n$$ \\begin{align*} U(P,f,\\alpha) \u0026amp;= M_{1}\\Delta \\alpha_{1} + \\cdots + M_{i} \\left[ \\alpha (x_{i}) -\\alpha (x^{\\ast}) \\right] + M_{i} \\left[ \\alpha (x^{\\ast})- \\alpha (x_{i-1}) \\right] + \\cdots + M_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;\\ge W_{1}\\Delta \\alpha_{1} + \\cdots + W_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] + W_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + \\cdots + M_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;= U(P^{\\ast},f,\\alpha) \\end{align*} $$\n■\n","id":830,"permalink":"https://freshrimpsushi.github.io/jp/posts/830/","tags":null,"title":"細分화"},{"categories":"해석개론","contents":"概要 リーマン・スティルチェス積分は、リーマン積分を一般化したもので、簡単にスティルチェス積分とも呼ばれる。リーマン積分はリーマン・スティルチェス積分の中で$\\alpha (x)=x$の特別な場合に該当する。\nリーマン・スティルチェス積分を定義するプロセスは、リーマン積分を定義するプロセスと同じなので、表記と構築に関する具体的な説明は省略する。\n定義 $\\alpha : [a,b] \\to \\mathbb{R}$を単調増加関数とし、$\\Delta \\alpha_{i}=\\alpha (x_{i})-\\alpha (x_{i-1})$とする。すると$\\alpha$が単調増加関数であるため$\\Delta \\alpha_{i} \\ge 0$が成り立つ。\n有界な関数$f : [a,b] \\to \\mathbb{R}$と$[a,b]$の分割$P$に対して$U, L$を以下のように定義する。\n$$ \\begin{align} U(P,f,\\alpha) \u0026amp;:= \\sum \\limits _{i=1} ^n M_{i} \\Delta \\alpha_{i} \\\\ L(P,f,\\alpha) \u0026amp;:= \\sum \\limits_{i=1} ^n m_{i} \\Delta \\alpha_{i} \\end{align} $$\n$(1), (2)$を**$[a,b]$での$\\alpha$に対する$f$のリーマン・スティルチェス上積分と下積分**upper and lower Riemann-Stieltjes sumとする。\n$(1), (2)$に区間$[a,b]$の全ての任意の分割$P$に対する$\\inf, \\sup$を取ったものをそれぞれ**$[a,b]$での$\\alpha$に対する$f$のリーマン・スティルチェス上積分と下積分**upper and lower Riemann-Stieltjes integralとする。\n$$ \\begin{align*} \\overline {\\int _{a} ^b} f d\\alpha \u0026amp;:= \\inf\\limits_{P} U(P,f,\\alpha) \\\\ \\underline {\\int _{a} ^b} f d\\alpha \u0026amp;:= \\sup\\limits_{P} L(P,f,\\alpha) \\end{align*} $$\n上積分と下積分が等しい場合、これを**$[a,b]$での$\\alpha$に対する$f$のリーマン・スティルチェス積分**Riemann-Stieltjes integralと呼び、以下のように表記する。\n$$ \\int _{a} ^b f d\\alpha = \\int _{a}^b f(x) d\\alpha (x) = \\overline {\\int _{a} ^b} f d\\alpha = \\underline {\\int _{a} ^b} f d\\alpha $$\n$f$のスティルチェス積分が存在する場合、$f$は$[a,b]$で$\\alpha$に対してリーマン・スティルチェス積分可能Riemann-Stieltjes integrableであり、以下のように表記する。\n$$ f \\in \\mathscr{R}(\\alpha) = \\left\\{ f : f \\text{ is Riemann-Stieltjes integrable} \\right\\} $$\n","id":829,"permalink":"https://freshrimpsushi.github.io/jp/posts/829/","tags":null,"title":"リーマン・スティルチェス積分"},{"categories":"해석개론","contents":"分割1 区間$[a,b]$が与えられたとしよう。$[a,b]$の分割partition$P$を下のように定義する。\n$$ P := \\left\\{ x_{0},\\ x_{1},\\ \\cdots, x_{n}\\right\\},\\quad a=x_{0} \u0026lt;x_{1}\u0026lt;\\cdots \u0026lt; x_{n} =b $$\nそして、$\\Delta x_{i}$を次のように定義する。\n$$ \\Delta x_{i} :=x_{i}-x_{i-1},\\quad i=1,2,\\cdots,n $$\n説明 簡単に言えば、分割とはある区間を分割した時、区間の両端と区間内のすべての境界点を要素として持つ集合のことだ。重要な点は、分割について話す場合、必ずどの区間についてのものかが必要だということだ。つまり、単に分割と言うことはできず、ある区間の分割と言うべきだ。\nリーマン和 $f$を$[a,b]$で定義された有界関数、$P$を$[a,b]$の分割としよう。そして、$M_{i}$, $m_{i}$を以下のようだとしよう。\n$$ \\begin{align*} M_{i} \u0026amp;=\\sup f(x),\u0026amp;(x_{i-1} \\le x \\le x_{i}) \\\\ m_{i}\u0026amp;=\\inf f(x), \u0026amp;(x_{i-1} \\le x \\le x_{i}) \\end{align*} $$\nすると、$U(P,f), L(P,f)$を以下のように定義し、それぞれを**$P$に対する$f$のリーマン上和、下和**upper and lower Riemann sumという。\n$$ \\begin{align*} U(P,f) \u0026amp;:=\\sum \\limits _{i=1} ^n M_{i} \\Delta x_{i} \\\\ L(P,f) \u0026amp;:= \\sum \\limits _{i=1} ^{n} m_{i}\\Delta x_{i} \\end{align*} $$\n説明 リーマン和は、関数の面積を区間を分割して近似するもので、区分求積法と同じだ。与えられた分割$P$に対して、上和は最大値を、下和は最小値を意味する。上和と下和の差がないほど近似した場合、それを$f$のグラフの下の面積と見なしてもいいだろう。\nリーマン積分 区間$[a,b]$のすべての分割$P$に対して$\\inf$を取ったものを**$[a,b]$上での$f$のリーマン上積分**upper Riemann integralという。\nそれぞれの$P$に対するリーマン上和の最小上界として定義し、以下のように示す。\n$$ \\begin{equation} \\overline{\\int _{a}^{b}} f dx := \\inf \\limits_{P} U(P,f) \\label{eq1} \\end{equation} $$\n同様に、区間$[a,b]$のすべての分割$P$に対して$\\sup$を取ったものを**$[a,b]$上での$f$のリーマン下積分**lower Riemann integralという。\n$$ \\begin{equation} \\underline {\\int _{a}^b } f dx := \\sup \\limits_{P} L(P,f) \\label{eq2} \\end{equation} $$\n$f$のリーマン上積分とリーマン下積分が同じである場合、$f$は$[a,b]$でリーマン積分可能Riemann integrableであると言い、以下のように表記する。\n$$ f \\in \\mathscr{R}= \\left\\{ f : f \\text{ is Riemann integrable} \\right\\} $$\n$\\mathscr R$はリーマン積分可能な関数の集合である。そして、$(1)$と$(2)$の共通値を以下のように表記し、これを**$[a,b]$上での$f$のリーマン積分**Riemann integralという。\n$$ \\underline {\\int _{a}^b } f dx = \\int _{a} ^b f dx = \\overline {\\int _{a}^b} f dx $$\nまたは\n$$ \\int _{a} ^b f(x) dx $$\n説明 上積分は$f$の面積を少し大きく近似したもの(上和)の中で最小のものであり、下積分は$f$の面積を少し小さく近似したもの(下和)の中で最大のものだ。だから、この二つが同じである時、$f$のグラフの下の面積を正確に近似したと言えるだろう。\nさらに、$f$が有界であるため、次を満たす二つの定数$M$、$m$が存在する。\n$$ m \\le f(x) \\le M \\ \\ \\ (a\\le x\\le b) $$\nしたがって、すべての分割$P$に対して次が成り立つ。\n$$ m(b-a) \\le L(P,f) \\le U(P,f) \\le M(b-a) $$\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), p120-121\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":828,"permalink":"https://freshrimpsushi.github.io/jp/posts/828/","tags":null,"title":"分割、リーマン和、リーマン積分"},{"categories":"힐베르트공간","contents":"定義1 完備 内積空間をヒルベルト空間Hilbert spaceと言う。ヒルベルトの名前から、主に$H$と表記される。\n説明 完備空間とは、すべてのコーシー数列が収束する空間のことだ。バナッハ空間も完備空間なので、内積が定義されたバナッハ空間としてヒルベルト空間を説明することもできる。例えば、以下のような空間がある。\nルベーグ空間 $L^{2}$ $\\ell^{2}$ 空間 実数空間 $\\mathbb{R}^{n}$ 複素数空間 $\\mathbb{C}^{n}$ 性質 ヒルベルト空間は一様に凸である 最短ベクトル定理 直交分解定理 リース表現定理 Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p65\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":776,"permalink":"https://freshrimpsushi.github.io/jp/posts/776/","tags":null,"title":"関数解析学におけるヒルベルト空間"},{"categories":"선형대수","contents":"双対空間 定義11 ベクトル空間 $X$ の全ての連続する線形汎関数の集合を $X^{ \\ast }$ と標記し、これを $X$ の双対空間dual space、簡単に $X$ のデュアルと呼び、以下のように表記する。\n$$ X^{ \\ast }:=\\left\\{ x^{ \\ast }:X\\to \\mathbb{C}\\ |\\ x^{ \\ast } \\text{ is continuous and linear} \\right\\} $$\n$$ X^{ \\ast }:=B(X,\\mathbb{C}) $$\n$B \\left( X, \\mathbb{C} \\right)$ は、定義域が $X$ で値域が $\\mathbb{C}$ の有界線形作用素の集合である。\n定義22 体 $F$ 上のベクトル空間 $X$ に対して、$X$ 上の線形汎関数の集合を $X$ の双対空間dual spaceと呼び、$X^{\\ast}$ と表記する。\n$$ X^{\\ast} = L(X, F) $$\n$L(X, F)$ は、$X$ から $F$ への全ての線形変換の集合である。\n説明 線形作用素の性質により、連続という条件は有界という条件と同値である。 双対空間の記号として $\\ast$ ではなく $^{\\prime}$ も使われることがある。 双対空間の双対空間についても話すことができる。この場合は、$X^{\\ast \\ast}=\\left( X^{ \\ast } \\right)^{ \\ast }$ のように表記し、バイデュアルbidual、ダブルデュアルdouble dual、セカンドデュアルsecond dualなどと呼ばれる。\nオペレーターのノルム $\\displaystyle \\| f \\| = \\sup_{\\substack{x \\in X \\\\ \\| x \\| =1}} | f(x) |$ について、$(X^{ \\ast } , \\| \\cdot \\| )$ は バナッハ空間となる。これに対して、次の定理が成立する。\n定理 $X$ が有限次元 ベクトル空間である場合、次が成立する。\n$$ \\dim X^{ \\ast } = \\dim X $$\n証明 方法11 戦略: $\\dim X$ の基底を使って、$\\dim X^{ \\ast }$ が有限次元になるような基底を作る。\n$\\dim X = n$ とすると、$X$ は有限次元なので基底 $\\left\\{ \\tilde{ e_{1} } , \\cdots , \\tilde{ e_{n} } \\right\\}$ を持つ.$\\displaystyle e_{j} : = {{ \\tilde{e_{j} } } \\over { \\| \\tilde{ e_{j} } \\| }} \\in X$ とすると、$\\| e_{j} \\| = 1$ であり、$\\left\\{ e_{1} , \\cdots , e_{n} \\right\\}$ は依然として $X$ の基底である。今、$e_{j}^{ \\ast } : (X , \\| \\cdot \\| ) \\to ( \\mathbb{C} , | \\cdot | )$ を以下のように定義しよう。\n$$ e_{j}^{ \\ast } (e_{i}) := \\delta_{ij} $$\n線形作用素の性質\n$T : (X , \\| \\cdot \\|_{X}) \\to ( Y , \\| \\cdot \\|_{Y} )$ が線形作用素だとしよう。$X$ が有限次元空間であれば、$T$ は連続である。\n$\\dim X = n$ と仮定したので、$e_{j}^{ \\ast }$ は連続する線形汎関数である。\n線形汎関数が線形独立組合せで表されるための必要十分条件\n$f_{1} , \\cdots , f_{n}$ が定義域が $X$ の線形汎関数だとしよう。\n$f_{1} , \\cdots , f_{n}$ が線形独立 $\\iff$ $f_{j} (x_{i} ) = \\delta_{ij}$ を満たす $x_{1} , \\cdots , x_{n}$ が存在する\n上記の定理により、$\\beta^{\\ast} = \\left\\{ e_{1}^{\\ast}, \\dots, e_{n}^{\\ast} \\right\\}$は線形独立である。$f \\in X^{ \\ast }$ を任意の $\\displaystyle x = \\sum_{i=1}^{n} t_{i} e_{i} \\in X$ に作用させると\n$$ f(x) = f\\left( \\sum_{i=1}^{n} t_{i} e_{i} \\right) = \\sum_{i=1}^{n} t_{i} f(e_{i} ) = \\sum_{i=1}^{n} f(e_{i} ) t_{i} $$\n$\\displaystyle t_{i} = e_{i}^{ \\ast } \\left( \\sum_{k=1}^{n} t_{k} e_{k} \\right) = e_{i}^{ \\ast } (x)$ であるから、\n$$ f(x) = \\sum_{i=1}^{n} f(e_{i} ) e_{i}^{ \\ast } (x) = \\left[ \\sum_{i=1}^{n} f(e_{i} ) e_{i}^{ \\ast } \\right] (x) $$\n従って、\n$$ f = \\sum_{i=1}^{n} f(e_{i} ) e_{i}^{ \\ast } \\in \\text{span} \\left\\{ e_{1}^{ \\ast } , \\cdots , e_{n}^{ \\ast } \\right\\} $$\nつまり、$\\beta^{\\ast} = \\left\\{ e_{1}^{ \\ast } , \\cdots , e_{n}^{ \\ast } \\right\\}$ は線形独立であり、$X^{\\ast}$ を生成するので、$X^{ \\ast }$ の基底である。\n$$ \\dim X^{ \\ast } = n $$\n■\n方法22 $\\dim(L(X,F)) = \\dim(X)\\dim(F)$であるから、\n$$ \\dim(X^{\\ast}) = \\dim(L(X,F)) = \\dim(X)\\dim(F) = \\dim(X) $$\n■\n定理の証明はこれで終わりだが、$X^{\\ast}$ の基底を具体的に見つけよう。$X$ の順序基底を $\\beta = \\left\\{ x_{1}, \\dots, x_{n} \\right\\}$ としよう。そして、$f_{i}$ を $i$ 番目の座標関数としよう。\n$$ f_{i}(x_{j}) = \\delta_{ij} $$\nすると、$f_{i}$ は $X$ 上で定義された線形汎関数である。今、$\\beta^{\\ast} = \\left\\{ f_{i}, \\dots, f_{n} \\right\\}$ としよう。\nClaim: $\\beta^{\\ast}$ は $X^{\\ast}$ の（順序）基底である\n$\\dim (X^{\\ast}) = n$ であることは既にわかっているので、$\\span(\\beta^{\\ast}) = X^{\\ast}$ を示せばいい。つまり、任意の $f \\in X^{\\ast}$ が $f_{i}$ の線形組合せで表されることを示さなければならない。与えられた $f$ に対して、$g = \\sum_{i=1}^{n}f(x_{i})f_{i}$ とする。すると、実際にこの $g$ がまさに $f$ であり、$f$ が $f_{i}$ の線形組合せで表されることがわかる。$1 \\le j \\le n$ について、\n$$ g(x_{j}) = \\left( \\sum_{i=1}^{n}f(x_{i})f_{i} \\right) (x_{j}) = \\sum_{i=1}^{n}f(x_{i})f_{i}(x_{j}) = \\sum_{i=1}^{n}f(x_{i})\\delta_{ij} = f(x_{j}) $$\nよって、$g=f$ であり、$f = \\sum_{i=1}^{n}f(x_{i})f_{i}$ である。したがって、$\\beta^{\\ast}$ は $X^{\\ast}$ を生成する。\n双対基底 上記の記法に従い、$X^{\\ast}$ の順序基底 $\\beta^{\\ast} = \\left\\{ f_{1}, \\dots, f_{n} \\right\\}$ を $\\beta$ の双対基底dual basis、相互基底reciprocal basisと呼ぶ。\n$$ f_{i} : X \\to \\mathbb{F} \\quad \\text{ by } \\quad f_{i}(x_{j}) = \\delta_{ij} $$\nこの場合、$\\delta_{ij}$ はクロネッカーデルタである。\nKreyszig. (1989). Introductory Functional Analysis with Applications: p106.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p119-120\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":753,"permalink":"https://freshrimpsushi.github.io/jp/posts/753/","tags":null,"title":"双対空間"},{"categories":"통계적분석","contents":"診断法 直感的パターン認識 標準化残差図を使って回帰分析が正しく行われたかを確認できる。独立性を確かめるためには、残差図にはっきりした傾向が現れていないことが条件だ。残念ながら、独立性の診断は他の回帰分析の仮定に比べて非常に主観的になるしかない。\n独立性が欠けている一番よく見られる例は、上のように特定できない直線が現れることだ。偶然の可能性もあるけれど、通常はデータを理解していないか、重要なデータが抜けている時に現れる現象だ。\n一見、何の問題もなさそうだけど、よく見ると$9$ごとに同じパターンが繰り返されている。この程度の規則性が現れているならば、自信を持って独立性が欠けていると言える。この時、残差は自己相関Autocorrelationがあるとされ、時系列を含む分析を考慮するのが良い。\n統計的検定 このような問題は、分析者自身がこれらのデータを見る目が悪いと思ったらダービン・ワトソン検定を使えば簡単に見つけられる。問題は、だからといってダービン・ワトソン検定を完全に信頼してはいけないということだ。ダービン・ワトソン検定は自己相関を見つけるだけで、独立性そのものについては保証できないからだ。下の図は理解を助けるために非常に作為的に作られたものだが、ダービン・ワトソン検定を通過しても独立性が欠けている例はいくらでもある。\n図では、残差が$0$にほぼ正確に当たる場合、次の二回も$0$に当たるが、最初の$0$がいつ現れるかはわからない。このような場合、残差の独立性には深刻な問題があると言えるが、自己相関があると言うには不規則すぎる。偶然の可能性もあるが、それを判断するのは完全に分析者の役割だ。\n併せて見る 線形性 等分散性 正規性 ","id":679,"permalink":"https://freshrimpsushi.github.io/jp/posts/679/","tags":null,"title":"モデル診断による残差の独立性の確認"},{"categories":"추상대수","contents":"定義 1 環 $(R , + , \\cdot )$ の全ての $a,b \\in R$ に対して $a I \\subset I$ と $I b \\subset I$ を満たす部分群 $(I, +)$ をアイディアルIdealと呼ぶ。\n説明 簡単な例として、$n \\mathbb{Z}$ は $\\mathbb{Z}$ のアイディアルになる。アイディアルという名前は文字通り 理想的なIdealから来たものだ。抽象代数で扱うには理想的な部分群だから、実際にそう呼ぶわけだ。\n特に$R$ が可換環である場合、$I$ が $R$ の正規部分群になる意味で、ただの $I \\triangleleft R$ とも呼ばれる。正規部分群が群論で重要だったように、アイディアルも環論のあらゆる定理で重要に登場することだろう。特に環論と呼ぶのは、アイディアルが実質的に環の概念に限定されるからだ。\nアイディアル $I$ は $R$ の部分環だ。\n定義では、条件を満たす「部分群」として群との対比を強調したが、実際には自然に部分環にもなる。ここでは証明はしないが、理解に苦しむ場合は、$a I \\subset I$ と $I b \\subset I$ という条件をよく考えればいい。感じとして、$I$ は $R$ の全ての元に対して「乗算を施した時」に依然として代数構造として存在できる「耐えうる元の集まり」だ。常識的にこのように作り出した $(I , \\cdot )$ は少なくとも $(R , + , \\cdot)$ に対して半群くらいは成り立つだろう。もちろん、この説明は数学的ではないので、疑問が残る場合は部分環の判定法を使って直接確認してみよう。実際には、教科書によっては最初から部分環として定義していることもある。\nFraleigh. (2003). 「抽象代数入門」第7版: p241.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":739,"permalink":"https://freshrimpsushi.github.io/jp/posts/739/","tags":null,"title":"抽象代数学におけるイデアル"},{"categories":"추상대수","contents":"定義 1 $$ f(x) : = \\sum_{i=0}^{n} a_{i} x^{i} = a_{0} + a_{1} x + \\cdots + a_{k} x^{k} $$ 多項式関数 $f \\in F [x]$と体 $F \\le E$において、$\\alpha \\in E$の評価関数Evaluation $\\phi_{\\alpha} : F [ x ] \\to E$を以下のように定義する。 $$ \\phi_{\\alpha} ( f(x) ) : = a_{0} + a_{1} \\alpha + \\cdots + a_{n} \\alpha^n = f (\\alpha) $$ $f( \\alpha ) = 0$を満たす$\\alpha \\in E$を$f(x)$のゼロZeroという。\n説明 評価関数 事実として、$\\phi_{\\alpha}$は準同型写像になる。\n定義があいまいに感じる場合、簡単な例$\\phi_{i} : \\mathbb{R} [ x ] \\to \\mathbb{C}$を考えてみよう。例えば$\\phi_{i} ( x^2 - 3 x + 2)$だとすると、単純に $$ f(x) = x^2 - 3 x + 2 $$ に$i$を代入した $$ i^2 - 3 i + 2= 1 -i2 \\in \\mathbb{C} $$ になる。もちろん、$\\mathbb{R} \\le \\mathbb{C}$であることは問題ない。\nゼロのモチーフ カーネルを考えると、$\\ker ( \\phi_{\\alpha} )$は$f(\\alpha) = 0$を満たす関数の集まりになる。上の例を続けると、$\\ker ( \\phi_{i} ) \\subset \\mathbb{R} [ x ]$の要素は$(x-i)$を因数に持つ多項式関数である。\nこのように$f( \\alpha ) = 0$を満たすとき、$\\alpha$を$f(x)$のゼロと呼ぶことには疑いの余地がない。同様に、$\\phi_{\\alpha} ( f(x) ) = 0$ならば$\\alpha$を$f(x)$のゼロと呼んでいた。\n「方程式の解」という概念を厳密に定義するためわざわざ関数まで話を持ち込む理由はまさにそこにある。 $$ f(x) = g(x) $$ 例えば上のような方程式を考える。こんな方程式の集合を考えない理由はないが、関係を集めるよりも$f(x)$と$g(x)$自体を別々に扱う方がずっと簡単で明瞭だ。もし方程式の集合$X$が上のような方程式の集まりだった場合 $$ \\left( f(x) = g(x) \\right) \\in X $$ のように表せるはずだが、どうせ化学反応をして $$ f(x) = g(x) \\iff f(x) - g(x) = 0 $$ になるから、右辺をわざわざ汚く自由にする必要がない。集合 $X$が方程式ではなく関数を持つ構造に従った場合、右辺が$0$である方程式を集めて、それらがいつ成立するかに関心を持つのと同じことである。\nこのような考えの拡張により、「実数系係数を持つ多項式関数でも虚根が出ることがある」などの事実が抽象化・一般化されるだろう。\nフレーリー。(2003)。初等代数学入門(第7版)：p201、204。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":723,"permalink":"https://freshrimpsushi.github.io/jp/posts/723/","tags":null,"title":"多項式の零点"},{"categories":"바나흐공간","contents":"定義1 $(X, \\left\\| \\cdot \\right\\|_{X}), (Y, \\left\\| \\cdot \\right\\|_{Y})$をノルム空間と呼ぶ。\nノルム空間からノルム空間への写像を作用素と呼ぶ。\n$x,x_{1},x_{2}\\in X$に対して、$T : X \\to Y$が $$ T( x_{1} + x_{2} ) = T( x_{1} ) + T( x_{2} ) \\quad \\text{and} \\quad T( a x ) = a T( x ) $$ を満たす場合、線形作用素と呼ぶ。\nすべての$x \\in X$に対して、$\\left\\| T(x) \\right\\|_{Y} \\le C \\left\\| x \\right\\|_{X}$を満たす$C \\ge 0$が存在する場合、$T$は有界であるという。\n3.を満たす$C$の中で最も小さい$C$を$T$の作用素ノルムと定義し、以下のように記す。 $$ \\left\\| T \\right\\| :=\\min \\left\\{ C : \\left\\| T(x) \\right\\|_{Y} \\le C \\left\\| x \\right\\|_{X} \\right\\} $$\n有界線形作用素$T : X \\to Y$をすべて集めた集合を$B(X,Y)$のように表す。\n説明 ベクトル空間からベクトル空間への写像を特に変換と呼ぶように、ノルム空間からノルム空間への関数を特に作用素と呼ぶ。\n便宜上、多くの教科書では、ベクトル空間間の任意の関数$X \\to Y$を変換と呼び、$X \\to X$のような変換を作用素と呼ぶ。\n4.の定義から次のことが得られる。\n$$ \\left\\| T \\right\\| = \\sup \\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\left\\| T(x) \\right\\|_{Y} $$\nこれは$T$のノルムとも定義される。$\\left\\| x \\right\\|_{X}=1$という条件がなぜ存在するのか理解できなければ、3.を考えればいい。\n作用素は代数的には演算を保持するホモモルフィズムであり、当然、これに関する定理もすべて使うことができる。\n「演算」という表現の代わりに「作用素」という表現を使うのは、過去のように「演算」に焦点を当てるのではなく、ある空間での「作用」に興味を持ち、数学的に抽象化して扱うためである。回転変換のようなものを考えると、空間上である点を回転させて移動させると見ることができる。座標をベクトルとしてとり、行列を乗算して「計算」した結果を得るという説明も正しいが、点の位置を「移動させる」という行動として考えれば、作用素という表現も十分適切である。\nこのように、与えられた空間内でベクトルとして表される数学的な対象に対して、ある「作用$T$を加える」という表現を使うことができるようになった。その中でも特に私たちが関心を持つのは線形作用素であり、例として次のようなものがある。\n恒等作用素 $I : X \\to X, Ix = x$ その名の通り、作用を加えても変わらない、あるいは作用を加えないのと同じである作用である。$1$や${\\rm id}$とも記される。\n零作用素 $\\mathbb{0} (x) : = 0$\nどんな元も$0$にする作用で、作用素のベクトルスペースでゼロベクトルの役割を果たす。\n微分作用素 $D : C^{1} \\to C^{1}, Df = \\dfrac{d f}{d x}=f^{\\prime}$\n微分を行う作用素であり、実際には高校から誰もが知らず知らずのうちに使ってきた事実である。\n積分作用素 $T : C[0, 1] \\to C[0, 1]$, $\\displaystyle y(t) = Tx(t) = \\int_{0}^{1}K(t, s)x(s) ds$ 積分もまた一つの作用素であり、このとき$K$をカーネルという。積分変換とも言う。\n行列 $T_{A} ( \\mathbb{x} ) := A \\mathbb{x}$ $m \\times n$行列$A$は、$\\mathbb{C}^{n}$から$\\mathbb{C}^{m}$への関数と考えることができる。\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p37\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":728,"permalink":"https://freshrimpsushi.github.io/jp/posts/728/","tags":null,"title":"関数解析学における作用素"},{"categories":"추상대수","contents":"定義 1 環$R$において、$ab = 0$を満たす$0$ではない$a,b \\in R$を零因子Zero Divisorと呼ぶ。 単位元$1 \\ne 0$を持つ$D$が零因子を持たない場合、その環を整域Integral Domainという。 説明 零因子 $0$ではない要素同士の積が$0$になる例には $$ \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} $$ や $2 \\cdot 3 \\equiv 0 \\pmod{6}$がある。このように環では、常に計算が便利に行えるわけではないため、注意が必要である。これは、$xy = 0$で、$x \\ne 0$である場合、$y = 0$とは言えないということである。\n整域 整域は、IDともよく略される。\n整域の例としては、単純に整数の集合$\\mathbb{Z}$を挙げることができる。もともと整域のIntegralという言葉は、整数Integerから来ているので当然である。整域の良い点は、$0$以外のもので割り算をする際に心配する必要がない点である。整域では、$x y = 0$ならば$x = 0$または$y = 0$であることが保証され、代数構造として大変便利である。\n$R$が整域であることは、$R$において乗法に関する消去法Cancellation lawが成立することを保証し、零因子を持たない環であることから、体と密接な関係がある。以下の有用な定理を見てみよう。\n定理 [3] $p$が素数ならば$\\mathbb{Z}_{p}$は体である。 証明 1 体$F$に対して、$a \\ne 0$かつ$ab = 0$ならば $$ \\left( {{1} \\over {a}} \\right) (ab) = \\left( {{1} \\over {a}} \\right) 0 = 0 $$ かつ、同時に $$ \\left[ \\left( {{1} \\over {a}} \\right) a \\right] b =1 b = b $$ が成り立つ。これは、$ab= 0$ならば、どちらかが必ず$0$でなくてはならないということであり、従って体の元は零因子になることができず、$F$は整域である。\n■\n2 有限整域$D$の元で$0$を除く残りの元を$1, a_{1} , \\cdots , a_{n}$としよう。それらに$a \\ne 0$を乗じた $$ a, aa_{1} , \\cdots , aa_{n} $$ を考えると、$D$が整域であるため、これらの中に$0$は存在しない。\nまた、整域では消去法が成立するため、$aa_{i} = aa_{j}$ならば$a_{i} = a_{j}$であることがわかる。つまり $$ a_{i} \\ne a_{j} \\implies aa_{i} \\ne aa_{j} $$ となり、 $$ \\left\\{ 1, a_{1} , \\cdots , a_{n} \\right\\} = \\left\\{ a, aa_{1} , \\cdots , aa_{n} \\right\\} $$ を得る。従って、$a \\ne 0$に対しては常に$ab=1$を満たす$b \\in \\left\\{ 1, a_{1} , \\cdots , a_{n} \\right\\}$が存在することがわかる。$b$は$a$の乗法に関する逆元であるため、$D$は体である。\n■\n[3] 自明のように、$\\mathbb{Z}_{p} = \\left\\{ 0 , 1, \\cdots , p-1 \\right\\}$は有限集合である。しかし、$p$が素数であるため、 $$ ab \\equiv 0 \\pmod{p} $$ を満たす$0$ではない$a,b \\in \\mathbb{Z}_{p}$は存在しないため、$\\mathbb{Z}_{p}$は整域であり、定理2により体である。\n■\n4 体$F$において、$0^2 = 0$かつ$1^2 = 1$であるため、$0$と$1$は$F$の冪等元である。$0$でも$1$でもない冪等元$a \\in F$が存在すると仮定すると、$a^2 = a$であるため、$a( a-1) = 0$でなければならない。しかし、定理1により、$F$は整域であり、零因子を持たないため、この仮定は矛盾している。\n■\n参照 ユークリッド整域 $\\implies$主イデアル整域 $\\implies$一意分解整域 $\\implies$ 整域 Fraleigh. (2003). A first course in abstract algebra(7th Edition): p178~179.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":719,"permalink":"https://freshrimpsushi.github.io/jp/posts/719/","tags":null,"title":"反射と屈折"},{"categories":"추상대수","contents":"定義 1 環 $(R , + , \\cdot)$が乗算$\\cdot$に対する単位元$1 \\in R$を持っている時、$1$を単位元Unityと言う。 単位元を持つ環$R$で、乗算に対する逆元が存在する元素$r \\ne 0$を単元Unitと言う。 単位元を持つ環$R$で、$0$以外の全ての元が単元であれば、それを除算環Division Ringと言う。 除算環$R$が乗算に関して可換ならば、その環を体Fieldと言う。 説明 簡単に言えば、体$(F , + , \\cdot )$とは、加算に対する単位元$0 \\in F$を除くすべての元が逆元を持つ可換環である。抽象代数の観点から考えると難しそうだが、解析学で学んだ$\\mathbb{R}$を思い出してみれば、実際にはこれが「代数構造」らしいと見ることができるだろう。\nなぜ逆元を持つ元がユニットと呼ばれるのか 単位元の英語表現であるUnityは簡単に受け入れられるが、なぜ逆元を持つ元素をUnitと呼ぶのか理解し難い人も多いのではないだろうか。通常、Unitは「単位」と訳され、「ある量を測るときの基準」として良く使われるからだ。逆元が存在することと単位は関係ないように見えるが、なぜUnitと定義したのか？ここで面白い脳内提案をしてみたい。\n代数学の発展初期には、当然ながら整数に関する研究が活発だった。実際、私たちが整数集合を$\\mathbb{Z}$と書くのも、ドイツ語のZahlringからきており、\u0026lsquo;Zahl-\u0026lsquo;が「数」を意味し、\u0026rsquo;-ring\u0026rsquo;はご存知の通り環に翻訳されている。代数学で使われる多くの概念が数論のセンスから出てきたと受け入れるのはそう難しくないだろう。\nここで整数体$\\mathbb{Z}$を考えてみよう。\n$\\mathbb{Z}$は無限に多くの整数を元として持つ。ここで、乗算に対して単位元となるのは$1$のみで、逆元を持つ元素は$-1$と$1$のみである。抽象代数を学ぶだけの数学に親しんでいれば、$-1$と$1$が「ユニット」と呼ばれるのに違和感を感じないはずだ。この背景から、整数を超えて様々な代数構造を見ていくうちに、これらをユニットと呼ぶのが適切だったのではないかと思われる。\n$\\mathbb{R}$に至って、$0$を除く全ての$r \\in \\mathbb{R}$に対して乗算に対する逆元$\\displaystyle {{1} \\over {r}} \\in \\mathbb{R}$が存在するので、$0$を除く全ての元がユニットである。考えてみれば、ある数$a$を$r$に掛けて欲しい数である$x$を作ることができるので、$r \\ne 1$も単位としての役割を果たす理由が全くない。そして、そのある数$a$は当然$a = r^{-1}x$であり、$r^{-1}$の存在無しには確信できないことだ。\n参照 解析学での体の公理 Fraleigh. (2003). 「抽象代数入門」(第7版): p173。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":715,"permalink":"https://freshrimpsushi.github.io/jp/posts/715/","tags":null,"title":"抽象代数学における体"},{"categories":"상미분방정식","contents":"公式1 これはラプラス変換の表です。\n$f(t)=\\mathcal{L^{-1}}$ $F(s)=\\mathcal{L} \\left\\{ f(t) \\right\\}$ 導く정 $1$ $\\dfrac{1}{s}$ link $e^{at}$ $\\dfrac{1}{s-a}$ link $t^n$ $\\dfrac{n!}{s^{n+1}}$ link $t^{p}$ $\\dfrac{ \\Gamma (p+1) }{ s^{p+1}}$ link $t^{p}e^{at}$ $\\dfrac{ \\Gamma (p+1) }{ (s-a)^{p+1}}$ link $\\sin (at)$ $\\dfrac{a}{s^2+a^2}$ link $\\cos (at)$ $\\dfrac{s}{s^2+a^2}$ link $e^{at}\\sin(bt)$ $\\dfrac{b}{(s-a)^2 +b^2}$ link $e^{at}\\cos(bt)$ $\\dfrac{s-a}{(s-a)^2+b^2}$ link $\\sinh (at)$ $\\dfrac{a}{s^2-a^2}$ link $\\cosh (at)$ $\\dfrac{s}{s^2-a^2}$ link $e^{at} \\sinh (bt)$ $\\dfrac{b}{(s-a)^2-b^2}$ link $e^{at} \\cosh (bt)$ $\\dfrac{s-a}{(s-a)^2-b^2}$ link $u_{c}(t)= \\begin{cases} 0 \u0026amp; t\u0026lt;c \\\\ 1 \u0026amp; t\\ge c\\end{cases}$ $\\dfrac{e^{-cs}}{s}$ link $u_{c}(t)f(t-c)$ $e^{-cs}F(s)$ link $f^{\\prime}(t)$ $s\\mathcal{L} \\left\\{ f(t) \\right\\} -f(0)$ link $f^{(n)}$ ${s^n\\mathcal {L}\\left\\{ f(t) \\right\\} -s^{n-1}f(0) - \\cdots -f^{(n-1)}(0) }$ link $f(t)=f(t+T)$ $\\dfrac{\\displaystyle \\int_{0}^T e^{-st}f(t)dt}{1-e^{-st}}$ link $\\delta (t-t_{0})$ $e^{-st_{0}}$ link $f(ct)$ $\\frac{1}{c}F \\left( \\frac{s}{c} \\right)$ link $\\frac{1}{k}f (\\frac{t}{k} )$ $F(ks)$ link $\\frac{1}{a} e^{-\\frac{b} {a}t}f\\left(\\frac{t}{a}\\right)$ $F(as+b)$ link $t^{n}f(t)$ $(-1)^{n}F^{(n)}(s)$ link William E. Boyce , Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), Chapter6 The Laplace Transform\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":743,"permalink":"https://freshrimpsushi.github.io/jp/posts/743/","tags":null,"title":"ラプラス変換の表"},{"categories":"바나흐공간","contents":"定義1 完備なノルム空間をバナッハ空間Banach spaceと言う。\n説明 完備空間とは、すべてのコーシー列が収束する空間のことを言う。\nバナッハ空間は以下の各項をすべて満たす空間として、非常に便利な空間である。距離関数が定義されている上に完備性を備えている。\nベクトル空間である。 ノルム空間である。$\\implies$ 距離空間である。 完備空間である。 また、バナッハ空間の例としては、定義域が閉区間である連続関数の集合が考えられる。これは非常に簡単な例でありながら、様々な重要な定理を支えているため、非常に重要な事実でもある。バナッハ空間の例には以下のものがある。\n$C[a,b]$ $\\R^{n}$ $\\mathbb{C}^{n}$ $C[a,b]$に関する証明を紹介する。\n証明 1 パート 1. ベクトル空間\n閉区間で定義された連続関数は、定数関数 $f(x) = 0$ を単位元として、$f(x) = - f(x)$ を逆元として持つ。また、$C[a,b]$ はスカラーフィールド $\\mathbb{R}$上でベクトル空間の条件をよく満たす。\nパート 2. ノルム空間\n$f \\in C [a,b]$ に対して $\\| \\cdot \\|$ を $\\displaystyle \\| f \\| := \\sup_{ a \\le t \\le b } | f (t) |$ として定義すると、ノルムの条件をよく満たす。\nパート 3. 完備性\n$\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ を $C [a,b]$ のコーシー列としよう。つまり、すべての $\\varepsilon / 3 \u0026gt; 0$ に対して、$n,m \u0026gt; N_{1}$ の時いつでも $\\| f_{n} (t) - f_{m} (t) \\| \u0026lt; \\varepsilon / 3$ を満たす $N_{1} \\in \\mathbb{N}$ が存在する。\n$\\mathbb{R}$ は完備空間なので、固定された $t_{0} \\in [a,b]$ が与えられるたびに、$\\displaystyle \\lim_{n \\to \\infty} f_{n} (t_{0})$ はある $f : [a,b] \\to \\mathbb{R}$ に関して\n$$ \\lim_{n \\to \\infty} f_{n} ( t_{0} ) = f ( t_{0} ) $$\nと表せる。すると、$f_{n}$ はコーシー列なので、任意の $t \\in [a,b]$ に対して、$m \\ge N_{2}$ の時に\n$$ \\begin{align*} | f(t) - f_{m} (t) | =\u0026amp; \\left| \\lim_{n \\to \\infty} f_{n} (t) - f_{m} (t) \\right| \\\\ =\u0026amp; \\lim_{n \\to \\infty} | f_{n} (t) - f_{m} (t) | \\\\ \\le \u0026amp; \\lim_{n \\to \\infty} \\sup_{t \\in [a,b] } | f_{n} (t) - f_{m} (t) | \\\\ =\u0026amp; \\lim_{n \\to \\infty} \\| f_{n} - f_{m} \\| \\\\ \u0026lt;\u0026amp; \\varepsilon / 3 \\end{align*} $$\nを満たす $N_{2}$ が存在する。もちろん、関数 $f$ が連続である保証はまだなく、単にすべての $t \\in [a,b]$ に対して最終的に $f_{n} (t)$ に収束する値を関数値として定義されただけである。しかし、この定義から、$f_{n}$ が $f$ に一様収束する、つまりすべての $x,y \\in [a,b]$ および $\\varepsilon / 3 \u0026gt; 0$ に対して $n \\ge N_{3}$ の時に同時に満たされる $N_{3} \\in \\mathbb{N}$ が存在することを保証することができる。\n$$ \\left| f_{n} (x) - f(x) \\right| \u0026lt; \\varepsilon / 3 \\\\ \\left| f_{n} (y) - f(y) \\right| \u0026lt; \\varepsilon / 3 $$\nこれで、$f$ が連続関数であることを示すだけである。\n空集合でない $E \\subset \\mathbb{R}$ について、$f : E \\to \\mathbb{R}$ としよう。\nコンパクト距離空間\n$f$ が連続で、$E$が有界閉区間であれば、$f$ は一様連続である。\n$f_{n} : [a,b] \\to \\mathbb{R}$ は連続であり、$[a,b] \\subset \\mathbb{R}$ はコンパクトなので、$f_{n}$ は $[a,b]$ で一様連続である。つまり、すべての $x,y \\in [a,b]$ および $\\varepsilon / 3 \u0026gt; 0$ に対して、$|x-y| \u0026lt; \\delta$ の時に満たされる $\\delta \u0026gt; 0$ が存在する。\n$$ \\left| f_{n}(x) - f_{n}(y) \\right| \u0026lt; \\varepsilon / 3 $$\n以上の結果を組み合わせると、$|x-y| \u0026lt; \\delta$ および $n \\ge N_{3}$ の場合に\n$$ \\begin{align*} |f(x) - f(y)| \\le \u0026amp; \\left| f (x) - f_{n} (x) \\right| + \\left| f_{n}(x) - f_{n}(y) \\right| + \\left| f_{n} (y) - f(y) \\right| \\\\ =\u0026amp; \\varepsilon / 3 + \\varepsilon / 3 + \\varepsilon / 3 \\\\ =\u0026amp; \\varepsilon \\end{align*} $$\nを満たす $\\delta \u0026gt; 0$ および $N_{3} \\in \\mathbb{N}$ が存在するため、$f$ は $[a,b]$ で一様連続であり、$f \\in C[a,b]$ に収束する。したがって、任意の連続関数のコーシー列 $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ が一様に何らかの $f \\in C[a,b]$ に収束するため、$C[a,b]$ は完備性を有する。\n■\nKreyszig. (1989). Introductory Functional Analysis with Applications: p36.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":703,"permalink":"https://freshrimpsushi.github.io/jp/posts/703/","tags":null,"title":"バナッハ空間"},{"categories":"선형대수","contents":"定義 $V$上のベクトル空間として$\\mathbb{F}$を定義しよう。\n$\\left\\| \\cdot \\right\\| : V \\to \\mathbb{F}$が$\\mathbf{u}, \\mathbf{v} \\in V$と$k \\in \\mathbb{F}$に対して次の三つの条件を満たす場合、$\\left\\| \\cdot \\right\\|$を**$V$上のノルム**と定義する。\n(i) 正定性: $\\left\\| \\mathbf{u} \\right\\| \\ge 0$であり$\\mathbf{u} = \\mathbb{0} \\iff \\left\\| \\mathbf{u} \\right\\| = 0$ (ii) 斉次性: $\\left\\|k \\mathbf{u} \\right\\| = | k | \\left\\| \\mathbf{u} \\right\\| $ (iii) 三角不等式: $\\left\\| \\mathbf{u} + \\mathbf{v}\\right\\| \\le \\left\\|\\mathbf{v} \\right\\| + \\left\\| \\mathbf{u} \\right\\|$ 説明 ノルムは絶対値から始まって抽象化された概念だ。韓国語にはそのまま当てはまる言葉がなく、そのまま読む方式で翻訳された。個人的にはちょっと変だと思うので、可能な限り[nɔ:m]に近い発音で読むことにしている。\n線形代数学ではノルムの定義は上記の通りだ。(つまり、他の分野ではノルムが別の方法で定義されることを意味する。)読んでみれば分かるが、ノルムの定義に必要な条件は基本的に「測定」または「比較」を可能にする要素だ。「測定」と「比較」を可能にする​。単に三次元空間$\\mathbb{R}^3$を考えると、これらの概念は直観的な定義で十分だが、複素数を考えるだけでも抽象化が必要になる。世の中には思っているよりも多くのノルムの種類があり、ベクトル空間においてのノルムも必ずしも一つである必要はない。これらの定義を満たしてくれさえすれば、ノルムについて無限に考え出すことができるだろう。\nベクトル空間$\\mathbb{C}^n$とそのベクトル$\\mathbf{u} = ( u_1 , u_2 , \\cdots , u_n ) \\in \\mathbb{C}^n$に対して次のノルムを紹介する：\nマンハッタンノルム $$ \\left\\| \\mathbf{u} \\right\\|_1 = \\sum_{k=1}^{n} |u_k| $$\n$\\mathcal{l}^1$ノルムとも呼ばれ、タクシー幾何学で距離を定義する際に使われるノルムだ。実際の都市マンハッタンから名前が来ており、単純な直線距離ではなく実際の移動距離を表すために考案された。正確には同じ概念ではないが、理解を助けるための図を見ると、なぜこのノルムにマンハッタンという名前が付けられたのか納得できるはずだ。上の図では青い線に相当し、緑色の正方形の一辺を$1$とすると、AとBの距離は$6+2 = 8$になる。\nユークリッドノルム $$ \\left\\| \\mathbf{u} \\right\\|_2 = \\sqrt{\\sum_{k=1}^{n} |u_k|^2} $$\n我々がよく知っている距離、大きさの概念で、寸法に関係なく絶対値の二乗の和の平方根で求められる。上の図では赤い線に相当し、よく知っている通り、AとBの距離は$\\sqrt{6^2 + 2^2} =6.32\u0026hellip;$になる。\n$\\infty$-ノルム、最大ノルム $$ \\left\\| \\mathbf{u} \\right\\|_\\infty = \\max_{1\\le k \\le n} |u_k| $$\nスプレマムノルムsupremum normとも呼ばれ、単純に最大値だけを取るノルムだ。\n$p$-ノルム $$ \\left\\| \\mathbf{u} \\right\\|_p = \\left( \\sum_{k=1}^{n} |u_k|^p \\right) ^ {{1} \\over {p} } $$\n$p$は$1$以上で、必ずしも自然数である必要はない。上でマンハッタンノルムとユークリッドノルムは$p$-ノルムの特別な例で、それぞれ$1$-ノルム、$2$-ノルムに該当する。特に$p = \\infty$の場合は最大ノルムになり、上述の記法をすべてカバーする。\n注目に値するかどうかは分からないが、一つ興味深い点は、$p$-ノルムの形が統計学の$p$-次モーメントの形に似ているということだ。絶対値があるとか、中央値が0に固定されているなどの違いはあるが、形だけを見た場合、$1$-ノルムは平均を、$2$-ノルムは分散を連想させる。\n","id":257,"permalink":"https://freshrimpsushi.github.io/jp/posts/257/","tags":null,"title":"線形代数学においてノルムとは何か？"},{"categories":"추상대수","contents":"定義 1 二つの二項演算、足し算$+$と掛け算$\\cdot$に関して以下のルールを満たす集合$R$を環と定義する。\n$a$、$b$、$c$が$R$の元の時、\n足し算に対して交換法則が成り立つ。$$a+b=b+a$$ 足し算に対して結合法則が成り立つ。$$(a+b)+c=a+(b+c)$$ 足し算に対する単位元が存在する。$$\\forall a \\ \\exists 0\\ \\ \\mathrm{s.t} \\ a+0=a$$ すべての元の足し算に対する逆元が存在する。$$\\forall a \\ \\exists -a\\ \\ \\mathrm{s.t}\\ a+(-a)=0$$ 掛け算に対して結合法則が成り立つ。$$(ab)c=a(bc)$$ 足し算と掛け算に対して分配法則が成り立つ。$$a(b+c)=ab+ac\\ \\mathrm{and} \\ (b+c)a=ba+ca$$ 説明 要するに、集合$R$が足し算に対して可換群であり、掛け算に対して半群であり、二つの演算に対して分配法則が成り立つ時、$R$を環という。\n特に、掛け算に対しても交換法則が成り立つ場合、可換環またはアーベル環と呼ばれる。また、環の定義からわかる通り、掛け算に対する単位元や逆元が存在する必要はない。単位元が存在しても、逆元が存在する必要もない。上記の6つの条件を満たせば、環と言える。\n群を扱う時、演算に対する単位元を$e$と表わす。環では演算が二つあるため、どちらの演算に対する単位元か簡単に分かるように異なる記号を使う。足し算に対する単位元は$0$とし、単位元と呼ぶ。掛け算に対する単位元が存在すれば$1$とし、単位と呼ぶ。ある元$a$の掛け算に対する逆元が存在する時、$a$を環$R$の単位と呼ぶ。\n群と同じく、環の掛け算に対する単位元も存在するならば、その存在は一意である。各元の逆元も存在すれば、それも一意である。この証明は群で行った方法と同じなので、ここでは書かないが、詳細はこちらを参照。\n例 整数の集合$\\mathbb{Z}$を考える。上記の6つの条件を満たすため、足し算、掛け算に対する環だ。また、掛け算に対して交換法則も満たすため、可換環だ。単位$1$が存在し、その元は整数の1であり、単位は1と-1だ。（それぞれの逆元として1と-1を持つ）\n注意 環では、掛け算に対する単位元や逆元が存在する「必要」がない。だから、群でのように安易に消去することができない。つまり、$a,\\ b,\\ c$が環$R$の元の時、$ab=ac$として$b=c$と結論づけることはできないのだ。$a$の逆元が必ず存在するわけではないからだ。\n同様に、$a^2=a$としても、安易に$a=0$や$a=1$という結論を出すことはできない。環を扱う際には、この点に注意しよう。\nFraleigh. (2003). 「抽象代数入門(第7版)」: p167.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":587,"permalink":"https://freshrimpsushi.github.io/jp/posts/587/","tags":null,"title":"抽象代数学における環"},{"categories":"통계적분석","contents":"診断法 1 標準化残差グラフを使って回帰分析が正しく行われているか確認できる。等分散性を確認するためには、残差の散らばりが全体的に均一かを確認すればいい。等分散性がない一般的な例として、以下の二つのケースが代表的だ。\n後ろに行くほど分散が大きくなる形で、このような場合には変換や重みを導入することで解決しなければならない。本当に簡単に解決されるかは別として、モデル診断で見つかった問題の中で最も典型的で簡単な解決策がある。\n中央部分のみ分散が信じられないほど小さいが、データ収集の段階から問題があると疑われる状況だ。極端な差を正確に説明できる別の変数がある可能性が高いので、データセットをもう一度見直すことをお勧めする。\n参照 線形性 独立性 正規性 Hadi. (2006). Regression Analysis by Example(4th Edition): p98.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":681,"permalink":"https://freshrimpsushi.github.io/jp/posts/681/","tags":null,"title":"モデル診断により確認される残差の等分散性"},{"categories":"통계적분석","contents":"必要性 単純回帰分析の場合は、独立変数と従属変数を考慮しても$2$の次元であるため、分析が適切に行われたか一目で確認することができる。しかし、多重回帰分析の場合$3$の次元を超えると図で描きにくくなり、分析が実際に適切であるかを確認するのが難しくなる。回帰分析の仮定を満たさないが仮説検定は通過する場合があるが、この場合分析は間違っているとするしかない。\n分析が間違っているのは主に(1)データが線形モデルに合わない場合や(2)分析結果と実際のデータに対する理解との乖離が激しい場合だ。モデル診断はデータが線形モデルに適合しているかどうかを確認するために行われる。\n診断法 1 データが線形モデルに合わないとは、簡単に言えばデータが直線状にないということだ。データが線形モデルに合っているかどうかは、標準化された残差グラフを見てモデル診断を行うことで判断する。この残差分析は、高次元で直線を描くのが難しいために考案されたかなり独創的な方法である。残差を計算する理由がそもそもこれであるとも言える。\n残差グラフで下記の四つの条件が満たされた場合、モデル診断はパスしたと見なされる。\n(i) 線形性Linearity: 残差が$0$を中心に対称的に分布していればよい。 これは回帰分析の本質とも言える前提であり、回帰分析の目的が直線を求めることであるため、線形性を満たさないと意味がない。もともと回帰分析を使用する理由自体が線形性があると推測することにあるので、実際の分析では非常に簡単に満たされる。 (ii) 等分散性Homoscedasticity: 残差の分布が均等であればよい。 特定の区間で変動が急に小さくなったりすると、データが同じプロセスで得られたとは言い難くなる。調査員の違い、ミスなどの問題を考慮せざるを得ない。データが後になるほど分散が大きくなったり小さくなったりする場合、変数の変換によって部分的に解決することができる。 (iii) 独立性Independecy: 残差同士にどんな傾向もなければよい。 残差に何らかの傾向があるということは、誤差が完全に偶然であるという回帰分析の仮定に反する。独立性が欠けているということは、逆に言えば、まだ私たちが知らない規則、たとえば自己相関がある可能性があるという意味になる。この場合、遠回りして解決しようとするよりも、時系列分析など、より適したツールを探すほうが良い。程度がひどい場合は一目で分かるほど明らかになるが、そうでなければ大きな問題ではないとも言える。独立性をチェックするためにダービン-ワトソン検定を安易に使わないこと。ダービン-ワトソン検定は、厳密に言えば、一定の間隔で離れた残差の自己相関を見つけるものであり、独立性を確認するものではない。明らかな傾向があるのにダービン-ワトソン検定を通過したからと言って独立と信じてはいけない。 (iv) 正規性Normality: 残差が標準正規分布に従っているように見えればよい。 他の前提と異なり、正規性はシャピロ-ウィルク検定やハルケ-ベラ検定のように客観的な診断が可能である。しかし、問題が単純であるとは限らず、主に正規性に大きな影響を与えるのが異常値であることが多い。分析者が異常値に該当するデータを直接見て、その現象を直接説明できれば大きな問題にはならない。異常値異常値といっても安易に除外してはいけないこと。例えば、標本が$300$個で、上下にシックスシグマ($\\pm 3 \\sigma$)を超える異常値が$3$個程度あれば、それは正常であるとされる。異常値が多すぎるのも問題だが、分布理論と異なり異常値が過剰に少なすぎるのも正規分布に適切に従っているわけではない。 これら四つの条件は無作為に並べられたのではなく、重要な順に配置されており、回帰係数に対する仮説検証の理論的導出過程を見れば、この順序を理解することができる。2実際の統計分析に臨むと、すべてのデータがきれいに現れるわけではなく、時にはいくつかの条件と妥協しなければならない場面もある。そのような場合、異常値が多かったり、わずかに偏っていても、ある程度正規性から逸脱しても許容できる場合がある。\nこのようなモデル診断はかなりの部分が目視に依存しており、データに対する理解が不可欠である。間違った部分を見つけることが第一の問題であり、どのように解決するかが第二の問題である。この能力を養う方法は、可能な限り実際の分析に取り組み、多くのタイプを見ることが最善である。\nコード 以下は、残差グラフを出力するRコードである。\nout\u0026lt;-lm(rating~.,data=attitude); summary(out)\rwin.graph(5,5); plot(rstudent(out),main=\u0026#34;표준화된 잔차\u0026#34;) Hadi. (2006). Regression Analysis by Example(4th Edition): p86~88.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n残念ながら、学部レベルで理解するのはかなり難しい。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":675,"permalink":"https://freshrimpsushi.github.io/jp/posts/675/","tags":null,"title":"回帰分析のモデル診断"},{"categories":"통계적분석","contents":"概要 回帰分析とは、変数間の関係を解明する方法であり、特に線形関係を明らかにするのに役立つ。多重線形回帰分析Multiple Linear Regressionは、一つの従属変数（応答変数）に複数の独立変数（説明変数）がどのように影響を及ぼすかを把握する回帰分析を指す。\nモデル 1 $$Y = \\beta_{0} + \\beta_{1} X_{1} + \\cdots + \\beta_{p} X_{p} + \\varepsilon $$\n私たちは、変数が上記のような線形関係を持っているかに興味がある。各変数は互いに独立していると仮定され、同様に、回帰係数は他の変数が固定されたときのその変数の単位変化率を意味する。設計行列で表すと $$ \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdots \u0026amp; x_{p1} \\\\ 1 \u0026amp; x_{12} \u0026amp; \\cdots \u0026amp; x_{p2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{1n} \u0026amp; \\cdots \u0026amp; x_{pn} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{bmatrix} $$ であり、まとめると$Y = X \\beta + \\varepsilon$である。\n計算自体は、最小二乗法を使用するが、幸い最小二乗法は次元$p$に特にこだわらない。しかし、単純回帰分析と異なり$p$次元に対して一般化されるため、$p \\ge 3$ではグラフで確認するのも難しい。\nただ見るだけでは分析が適切に行われたか分からないので、分析者は様々な診断を通じて結果を正当化する必要がある。そのような診断を通過したとしても、交互作用や多重共線性などの問題が残り、どの変数を選ぶかも重要な問題だ。\n関連項目 Rでの多重回帰分析結果 回帰係数のF検定 多重回帰係数ベクトルの推定量導出 Hadi. (2006). Regression Analysis by Example(4th Edition): p53.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":666,"permalink":"https://freshrimpsushi.github.io/jp/posts/666/","tags":null,"title":"多重回帰分析"},{"categories":"통계적분석","contents":"定義 1 回帰分析で得られた回帰式を$Y \\gets X_{1} + X_{2} + \\cdots + X_{n}$とし、$y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\cdots + \\beta_{n} x_{n}$で示そう。n番目のデータを$(y_{i} , x_{i1} , x_{i2} , \\cdots , x_{in})$と表す。\n平均Mean: $$ \\displaystyle \\overline{y} := {{1} \\over {n}} \\sum_{i=1}^{n} y_{i} $$ 適合値Fitted Value: n番目のデータ $y_{i}$ に対して $$ \\hat{y}_{i} := \\beta_{0} + \\beta_{1} x_{i1} + \\beta_{2} x_{i2} + \\cdots + \\beta_{n} x_{in} $$ 予測値Predicted Value: 新しいデータ $y_{0}$ に対して $$ \\hat{y}_{0} := \\beta_{0} + \\beta_{1} x_{01} + \\beta_{2} x_{02} + \\cdots + \\beta_{n} x_{0n} $$ 適合による偏差Deviation due to Fit: $$ \\hat{y}_{i} - \\overline{y} $$ 残差Residual: $$ y_{i} - \\hat{y}_{i} $$ TSS(総平方和)またはSST(全体の平方和): $$ \\text{TSS} =\\text{SST} := \\sum_{i=1}^{n} ( y_{i} - \\overline{y} )^2 $$ ESS(説明される平方和)またはSSR(回帰による平方和): $$ \\text{ESS} = \\text{SSR} := \\sum_{i=1}^{n} ( \\hat{y}_{i} - \\overline{y} )^2 $$ RSS(残差平方和)またはSSE(平方誤差和): $$ \\text{RSS} = \\text{SSE} := \\sum_{i=1}^{n} ( y_{i} - \\hat{y}_{i} )^2 $$ R二乗R-squaredまたは説明率: $$ R^2 := {{ \\text{ SSR } } \\over { \\text{ SST} }} $$ 説明 適合値と予測値は数学的には全く同じだが、回帰式に代入するデータが実データかどうかの違いがある。ここで$\\hat{y_{i}}$を求めるということは、与えられた情報を反映した値を計算することを意味する。この場合、5番の残差は、我々がどうしようもない―当然存在するべき、自然にあるべきエラーだ。回帰分析は、それらの平方和を最小化して回帰線を求め、その後残差を見て回帰分析の仮定が満たされているかを確認することをモデル診断と呼ぶ。 「説明される平方和ESS」は、「説明できない平方和RSS」と対照的な表現に過ぎない。困ったことに、EとRはそれぞれExplainedとRegression, ErrorとResidualで、似ているように書かれている。 $$ \\text{TSS} = \\text{SST} \\\\ \\text{ESS} = \\text{SSR} \\\\ \\text{RSS} = \\text{SSE} $$ 先に付くか後に付くかによって$E$と$R$が変わるような暗記はお勧めしない。ただ自分にとって快適な記号を一つ選んで、数式で覚えておき、知っていることと反対に書かれていたら、略語も反対にされ得るという事実だけを覚えておけば十分だ。 R二乗は説明率とも呼ばれ、分析がデータをどれ程よく説明しているかを示す尺度になる。一方で、$\\text{SST} = \\text{SSR} + \\text{SSE}$は容易に示され、これによると、$\\text{ESS}$が高まるにつれて、$\\text{RSS}$は小さくなり、$0 \\le R^{2} \\le 1$が真となる。それでは直感的に見た場合、 $$ R^2 = {{ \\text{ SSR } } \\over { \\text{ SST} }} = {{ \\text{ ESS } } \\over { \\text{ TSS } }} = {{\\text{설명할 수 있는 에러}} \\over {\\text{전체 에러}}} $$ となるので、分析でのデータの説明比率として理解することができる。 Hadi. (2006). 回帰分析の例（第4版）: p40~42.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":650,"permalink":"https://freshrimpsushi.github.io/jp/posts/650/","tags":null,"title":"適合値、予測値、残差、誤差"},{"categories":"위상수학","contents":"定義 1 位相空間 $X$ と $Y$ に対して次のように定義された積空間 $Y^{X}$を関数空間という。 $$ Y^{X} : = \\prod_{x \\in X} Y = \\left\\{ f \\ | \\ f : X \\to Y \\text{ is a function} \\right\\} $$\n関数空間の位相として次がある:\n$x \\in X$ と $Y$ の開集合 $U$ に対して $$ S (x , U) = \\left\\{ f \\in Y^{X} \\ | \\ f(x) \\in U \\right\\} $$ としよう。部分基底 $\\left\\{ S(x,U) \\ | \\ x \\in X , U \\subset Y \\right\\}$ によって生成される $Y^{X}$ の位相をポイント-オープン位相という。 コンパクト集合 $K \\subset X$ と $Y$ の開集合 $U$ に対して $$ S (K , U) = \\left\\{ f \\in Y^{X} \\ | \\ f(K) \\subset U \\right\\} $$ としよう。部分基底 $\\left\\{ S(K,U) \\ | \\ K \\subset X , U \\subset Y \\right\\}$ によって生成される $Y^{X}$ の位相をコンパクト-オープン位相という。 $(Y,d)$ を距離空間としよう。\nコンパクト集合 $K \\subset X$ と $\\varepsilon \u0026gt; 0$ に対して $$ B_{K} (f, \\epsilon) = \\left\\{ g \\in Y^{X} \\ \\left| \\ \\sup_{x \\in K} \\left\\{ d(f(x),g(x)) \\right\\} \u0026lt; \\varepsilon \\right. \\right\\} $$ としよう。基底 $\\left\\{ B_{K} (f, \\varepsilon ) \\ | \\ K \\subset X , \\varepsilon \u0026gt; 0 \\right\\}$ によって生成される $Y^{X}$ の位相をコンパクト収束位相という。 一様距離 $$ \\overline{ \\rho } (f,g) : = \\sup_{x \\in X} \\left\\{ \\min \\left\\{ d(f(x) , g(x) ) , 1 \\right\\} \\right\\} $$ によって生成される距離空間 $(Y^{X} , \\overline{ \\rho } )$ の位相を一様位相という。 定理 [5]: $X$ が離散空間ならば$Y^{X}$ のコンパクト収束位相はポイント-オープン位相と同じである。 [6]: $X$ がコンパクト空間ならば$Y^{X}$ のコンパクト収束位相はティコノフ位相と同じである。 $\\left\\{ f_{n} : X \\to Y \\right\\}$ を$Y^{X}$ での数列とし、定義域を$K \\subset X$ に限定した関数を$f_{n} |_{K} : K \\to Y$ と表すことにしよう。\n[7]: $\\left\\{ f_{n} \\right\\}$ が$Y^{X}$ のポイント-オープン位相に属する$f$ に収束する場合、全ての$x \\in X$ に対して$ f_{n} (x) $ は$f(x)$ に収束する。 [8]: $\\left\\{ f_{n} \\right\\}$ が$Y^{X}$ のコンパクト収束位相に属する$f$ に収束する場合、全てのコンパクト$K \\subset X$ に対して$f_{n} |_{K}$ は$f |_{K}$ に一様に収束する。 定義域が位相空間$X$ で値域が距離空間$Y$ の連続関数の集合を $$ C(X,Y) := \\left\\{ f \\in Y^{X} \\ | \\ f \\text{ is continuous} \\right\\} $$ とし、$C(X,Y)$ を$Y^{X}$ の部分空間としよう。\n[9]: $C(X,Y)$ のコンパクト-オープン位相とコンパクト収束位相は同じである。 [10]: $C(X,Y)$ のコンパクト収束位相は$Y$ の距離関数に依存しない。 [11]: $C(X,Y)$ の数列 $\\left\\{ f_{n} \\right\\}$ が$f \\in Y^{X}$ に収束すれば$f : X \\to Y$ は連続関数である。 説明 特に$C(X, \\mathbb{R})$ を$C(X)$ のように示し、特に$X$ が区間の時、すなわち$X=(a,b)$、$X=[a,b]$ の時はそれぞれ$C(a,b)$、$C[a,b]$ とも示される。\n1~4 要約すると、ポイント-オープン位相は小さく、一様位相は大きいと言える。\n[7], [8] 関数が一様連続であることを示すのに役立てられる。\n[10], [11] 一般位相を解析学の一般化と見た時、非常に重要な事実である。\nMunkres. (2000). 『トポロジー(第2版)』: p267.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":646,"permalink":"https://freshrimpsushi.github.io/jp/posts/646/","tags":null,"title":"位相数学における関数空間"},{"categories":"추상대수","contents":"定義 $G, G'$の単位元$e, e'$と準同型写像$\\phi : G \\to G'$に対して$\\left\\{ e' \\right\\}$の原像$ \\phi^{-1} [ \\left\\{ e' \\right\\} ]$を$\\phi$の核Kernelといい、$\\ker \\phi $と書く。\n定義 [1]: $g \\in G$に対して$g ( \\ker \\phi ) = ( \\ker \\phi ) g$ [2]: $\\ker \\phi \\triangleleft G$ [3]: $\\ker \\phi = \\left\\{ e \\right\\}$ $\\iff$ $\\phi$は単射だ。 [4]: $\\phi$が全射で$\\ker \\phi = \\left\\{ e \\right\\}$ならば、$\\phi$は同型写像である。 説明 定理[3]は必要十分条件だが、特に準同型写像が単射であることを示すのに便利に使われる。線形代数学では、零空間は与えられた方程式に対する解集合としてのアイデンティティが強かった。\n一方、抽象代数学では、少なくとも群論では、$G$が何であれ、正規部分群として「中心を持つこと」の性質が強い。面白いことに、定理[1]では$\\phi$が実際にどのように定義されたか、また$G'$がどのような群であるかさえ気にせず、$G'$は$G$から$\\phi$を受け取るだけで、それ以外は無意味だとされている。\n証明 [3] $( \\implies )$ $\\ker \\phi = \\left\\{ e \\right\\}$ならば、すべての$g \\in G$に対して$\\phi ( \\left\\{ g \\right\\} )$は正確に$\\left\\{ g \\right\\} = g \\left\\{ e \\right\\}$にのみ対応するので、$\\phi$は単射だ。\n$( \\impliedby )$ $\\phi$が単射であり、$\\phi (e) = e'$により、$\\ker \\phi = \\left\\{ e \\right\\}$でなければならない。\n■\n一緒に見る 線形代数学での零空間 ","id":622,"permalink":"https://freshrimpsushi.github.io/jp/posts/622/","tags":null,"title":"抽象代数学における核、カーネル"},{"categories":"위상수학","contents":"定義 1 インデックス集合 $\\mathscr{A}$ に対して、$\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ を位相空間の集合とし、$O_{\\alpha}$ を $X_{\\alpha}$ での開集合とする。\nデカルト積 $\\displaystyle X := \\prod_{\\alpha \\in \\mathscr{A}} X_{ \\alpha}$ において $p_{\\alpha} : X \\to X_{\\alpha}$ を射影と言う。 部分基底 $\\mathscr{S} : = \\left\\{ p_{\\alpha}^{-1} ( O_{\\alpha} ) \\ | \\ O_{\\alpha} \\subset X_{\\alpha} , \\alpha \\in \\mathscr{A} \\right\\}$ によって生成される$X$ の位相を積位相と言う。 基底 $\\displaystyle \\mathscr{B} : = \\left\\{ \\prod_{\\alpha \\in \\mathscr{A}} O_{\\alpha} \\left. \\ \\right| \\ O_{\\alpha} \\subset X_{\\alpha} , \\alpha \\in \\mathscr{A} \\right\\}$ によって生成される$X$ の位相を箱位相と言う。 定理 [1]: $p_{\\alpha}$ は連続関数だ。 [2]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ がハウスドルフ空間の集合なら、$X$ はハウスドルフだ。 [3]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ が連結空間の集合なら、$X$ は連結だ。 [4]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ がコンパクト空間の集合なら、$X$ はコンパクトだ。 $\\mathscr{A} = \\mathbb{N}$ とする。\n[5]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ が可分空間の集合なら、$X$ は可分だ。 [6]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ が第一加算空間の集合なら、$X$ は第一加算だ。 [7]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ が第二加算空間の集合なら、$X$ は第二加算だ。 [8]: $\\mathscr{A}$ が有限集合なら、$X$ の積位相と箱位相は同じだ。 説明 定義で難解な部分基底が登場する理由は、主に交差を取るためであり、基底の定義によっては、合併以外は出てこないからだ。\n部分基底の定義に従って、部分基底$\\mathscr{S}$ によって生成される積位相の基底は $$ \\left\\{ \\left. \\bigcap_{i=1}^{n} p_{\\alpha_{i} }^{-1} ( O_{ \\alpha_{i} } ) \\ \\right| \\ p_{\\alpha_{i} }^{-1} ( O_{ \\alpha_{i} } ) \\in \\mathscr{S} \\right\\} $$ である。自然に、箱位相の基底$\\mathscr{B}$ について $$ \\left\\{ \\left. \\bigcap_{i=1}^{n} p_{\\alpha_{i} }^{-1} ( O_{ \\alpha_{i} } ) \\ \\right| \\ p_{\\alpha_{i} }^{-1} ( O_{ \\alpha_{i} } ) \\in \\mathscr{S} \\right\\} \\subset \\mathscr{B} $$ が成り立つ。箱位相の要素が積位相の部分基底から生成される基底を含むということは、箱位相の要素が積位相の要素よりも同じか多いという意味であり、このために積位相は小さい、粗い、弱いと表現される。\n定理[8]が成り立つことは、意外と稀なケースであるということだ。\u0026lsquo;箱の中に積が入っている\u0026rsquo;と覚えれば混乱しない。有限次元でも可算無限次元でもなく、任意の次元にまで触れるというのは、多少衝撃的だ。\n非専門家が見る位相数学 しかし、位相数学でこのようなデカルト積を考えることは、他のどの分野よりも興味深い。次元に対する一般化であれ、多変量解析であれ、何でもよいが、ようやく一般に知られた位相数学に近づいてきたと感じる。\n$I := [0,1]$ と $S^{1} = \\left\\{ (x,y) \\in \\mathbb{R}^2 \\ | \\ x^2 + y^2 =1 \\right\\}$ について以下の空間を考えよう。\n左から順に、正方形$I \\times I$、円筒$I \\times S^{1}$、トーラス$S^{1} \\times S^{1}$ だ。\n一点コンパクト化から始まり、今や空間がねじれたり、折り曲げられたりする数学になった。\n参照 集合のデカルト積 群のデカルト積 位相空間のデカルト積 Munkres. (2000). Topology(2nd Edition): p113~114.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":620,"permalink":"https://freshrimpsushi.github.io/jp/posts/620/","tags":null,"title":"位相空間のデカルト積"},{"categories":"추상대수","contents":"定義 1 2 群 $G_{1} , \\cdots , G_{n}$ の直積とその要素 $\\displaystyle (a_{1},\\cdots , a_{n}), (b_{1} , \\cdots , b_{n} ) \\in \\prod_{i=1}^{n} G_{i}$ について $$ (a_{1},\\cdots , a_{n}) (b_{1} , \\cdots , b_{n} ) = (a_{1} b_{1},\\cdots , a_{n} b_{n}) $$ これを$G_{1} , \\cdots , G_{n}$の直積Direct Productと言う。 特に $G_{1}, \\cdots , G_{n}$ が可換群の場合、$\\displaystyle \\bigoplus_{i=1}^{n} G_{i}$ と書き、直和Direct Sumとも呼ぶ。 $G_{1}$ が $G$ の部分群だとする時、次を満たす$G$の別の部分群$G_{2}$が存在すれば、$G_{1}$を直和因子Direct Summandと呼ぶ。 $$ G = G_{1} \\oplus G_{2} $$ 性質 $G = G_{1} \\oplus G_{2}$ とする。もし$H_{1}$ が $G_{1}$ の部分群で、$H_{2}$ が $G_{2}$ の部分群なら、$H_{1}$ と $H_{2}$ も直和として表せ、特に以下が成り立つ。 $$ {{ G } \\over { H_{1} \\oplus H_{2} }} \\simeq {{ G_{1} } \\over { H_{1} }} \\oplus {{ G_{2} } \\over { H_{2} }} $$\n[1]: $H_{1} \\simeq G_{1}$ で $H_{2} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ G / G_{1} \\simeq G_{2} $$ [2]: $H_{1} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ {{ G } \\over { H_{2} }} \\simeq G_{1} \\oplus {{ G_{2} } \\over { H_{2} }} $$ 説明 ベクトル空間は加法に関して群だが、群はベクトル空間ではないため、線形代数学の直和と完全に一致するわけではないが、比較が何らかの意味を持つためには少なくとも環くらいにはなっている必要がある。\n例えば、クラインの四元群は$V \\simeq \\mathbb{Z}_{2} \\times \\mathbb{Z}_{2}$を満たし、$\\gcd (m , n) = 1$ の場合、$\\mathbb{Z}_{m} \\times \\mathbb{Z}_{n} \\simeq \\mathbb{Z}_{mn}$が巡回群であるという定理が知られている。\n自由群 記法上は、自由アーベル群については、単に整数環 $\\mathbb{Z}$の直和と同型であると表現する方が便利だ。例えば $G$ がランク $3$ の自由アーベル群なら、$G$ は次のようにも表せる。 $$ G \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} \\oplus \\mathbb{Z} $$\n参照 線形代数学における直和 集合の直積 群の直積 位相空間の直積 Fraleigh. (2003). 「抽象代数入門」(第7版): p104~105.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (1984). 「代数的トポロジーの要素」: p23~24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":619,"permalink":"https://freshrimpsushi.github.io/jp/posts/619/","tags":null,"title":"群のデカルト積"},{"categories":"열물리학","contents":"定義1 2 エネルギーが$E$の系があるとしよう。$E$に関する微視状態の数を$\\Omega (E) = \\Omega$とするとき、\n$$ \\dfrac{1}{k_{B} T} := \\dfrac{d \\ln ( \\Omega )}{d E } $$\n$T$を系の温度temperatureと定義する。（ただし、$k_{B}$はボルツマン定数）\n微視状態と巨視状態 統計力学で、ある系の巨視状態Macrostateと微視状態Microstateは、例えば次のような概念だ。箱の中にコイン四枚が入っているとする。この箱を強く振って開けると、表と裏がランダムに決まる。この時のコインの状態を表を白色、裏を濃灰色で表現すると、次のようになる。\n表の数だけを見るなら$0$個から$4$個までの合計$5$通りがあり、これを巨視状態の数 $S$という。一方、各コインが表か裏かまで数えると、知られているように$2^4=16$通りあり、これを微視状態の数 $\\Omega$という。\n当然ながら、微視状態の数$\\Omega$が大きければ、そのに対応する巨視状態が観測される確率が高い。上の状況で、コイン$n$個の中で$k$個が表である微視状態の数を$\\Omega (k, n-k)$とすると、微視状態の数が最も多いのは$\\Omega (2,2) = 6$で、したがって、表と裏がそれぞれ二枚ずつの場合が観測されやすい。\n導出 温度の定義は、相互作用する二つの系の巨視状態を探す過程で自然に導かれる。以下のような閉じた系$X$を考えよう。\n$X$は$A$と$B$に分かれている。$A$、$B$内部のエネルギーをそれぞれ$E_{A}$、$E_{B}$としよう。上のコインの例で言うと、$A$と$B$は特定のコインの集まりで、$E_{A}$と$E_{B}$はそれぞれ表のコインの数だ。\nすべての微視状態が起こりうる確率が同じで、$A$と$B$が十分に相互作用した（または時間が十分に流れた）と仮定して、二つの系が熱平衡状態にあるとする。全系のエネルギーは$E_{X} = E_{A} + E_{B}$と同じである。全系$X$の微視状態の数は、$A$が可能な微視状態の数$\\Omega (E_{A})$と$B$が可能な微視状態の数$\\Omega (E_{B})$の積で表される。\n$$ \\begin{equation} \\Omega_{X} (E_{X}) = \\Omega_{A} (E_{A}) \\Omega_{B} (E_{B}) \\end{equation} $$\nすると、熱平衡状態での巨視状態は、上の式の値が最も大きい時と自然に受け入れられる。実際、熱平衡時の巨視状態で可能な微視状態の数は、他の場合より圧倒的に多いと言われている。微視状態の数$\\Omega$を正規分布と考えれば、$(1)$を微分して$0$になる点が最大値であることを自然に受け入れられるだろう。\nしかし、実際には、粒子のエネルギーは連続した値ではなく量子化されている。したがって、系全体のエネルギー$E_{X}$も離散的な値を持つ。しかし、熱物理学の場合、扱う系の粒子の数が非常に多いので、可能な$E_{X}$の値も非常に多い。したがって、$E_{X}$、$E_{A}$、$E_{B}$を連続した値を持つ変数と考えよう。\n再び巨視状態を探すことに戻って、熱平衡時の巨視状態（エネルギー）を$\\overline{E} = \\overline{E}_{A} + \\overline{E}_{B}$としよう。すると、$(1)$を$E_{A}$で微分して$E_{A}=\\overline{E}_{A}$を代入すると、$0$になるということだ。\n$$ \\left. \\dfrac{d( \\Omega_{A} (E_{A} ) \\Omega_{B} (E_{B}) )}{dE_{A}} \\right|_{E_{A}=\\overline{E}_{A}} = 0 $$\n上の式を計算すると、積の微分法によって次のようになる。\n$$ \\Omega_{B} (E_{B}) \\left. \\dfrac{d \\Omega_{A} (E_{A} )}{d E_{A}} \\right|_{E_{A}=\\overline{E}_{A}} + \\Omega_{A} (E_{A}) \\left. \\dfrac{d \\Omega_{B} (E_{B} )}{d E_{B}} {{d E_{B} } \\over {d E_{A} }} \\right|_{E_{A}=\\overline{E}_{A}} = 0 $$\nここで、$A$と$B$の間でどのようにエネルギーが移動しても、全体のエネルギー$E_{X} = E_{A} + E_{B}$は変わらない定数であるため、次が成立する。\n$$ d E_{A} = - d E_{B} \\implies \\dfrac{d E_{B}}{d E_{A} } = -1 $$\nこれを上の式に代入すると、次の式を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\Omega_{B} \\left. \\dfrac{ d \\Omega_{A} }{d E_{A}}\\right|_{E_{A}=\\overline{E}_{A}} - \\left. \\Omega_{A} \\dfrac{ d \\Omega_{B} }{d E_{B}}\\right|_{E_{B}=\\overline{E}_{B}} =\u0026amp; 0 \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{ \\Omega_{A} } \\left. \\dfrac{ d \\Omega_{A} }{d E_{A}}\\right|_{E_{A}=\\overline{E}_{A}} - \\dfrac{1}{\\Omega_{B} } \\left. \\dfrac{ d \\Omega_{B} }{d E_{B}} \\right|_{E_{B}=\\overline{E}_{B}} =\u0026amp; 0 \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{ \\Omega_{A} } \\left. \\dfrac{ d \\Omega_{A} }{d E_{A}} \\right|_{E_{A}=\\overline{E}_{A}} =\u0026amp; \\dfrac{1}{\\Omega_{B} } \\left. \\dfrac{ d \\Omega_{B} }{d E_{B}} \\right|_{E_{B}=\\overline{E}_{B}} \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{ d \\ln \\Omega_{A} }{d E_{A}} \\left(\\overline{E}_{A}\\right) =\u0026amp; \\dfrac{ d \\ln \\Omega_{B} }{d E_{B}}\\left(\\overline{E}_{B}\\right) \\end{align*} $$\n最後の行は、対数関数の微分法と連鎖律により成立する。ここで、上の式は熱平衡の条件で、左辺は系$A$の変数だけで構成された値で、右辺は系$B$の変数だけで構成された値だ。熱平衡状態で両側がそれぞれの状態だけで同じ値を持つので、この値で温度を定義すれば妥当だろう。すると、$A$と$B$の温度をそれぞれ$T_{A}$と$T_{B}$と定義できる。\n$$ \\begin{align*} \\dfrac{1}{k_{B} T_{A} } \u0026amp;:= \\dfrac{ d \\ln \\Omega _{A} }{d E_{A}} \\left(\\overline{E}_{A}\\right) \\\\ \\dfrac{1}{k_{B} T_{B} } \u0026amp;:= \\dfrac{ d \\ln \\Omega _{B} }{d E_{B}} \\left(\\overline{E}_{B}\\right) \\end{align*} $$\n■\nStephen J. Blundell and Katherine M. Blundell, 熱物理学 (Concepts in Thermal Physics、이재우 による翻訳) (2版, 2014), p45-49\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. K. Pathria and Paul D. Beale, 統計力学 (3版, 2011), p1-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":613,"permalink":"https://freshrimpsushi.github.io/jp/posts/613/","tags":null,"title":"物理学における温度の定義"},{"categories":"르벡공간","contents":"要約1 $\\Omega \\subset \\mathbb{R}^{n}$を開集合としよう。次の式を満たす二つの定数$1 \\lt p \\lt \\infty, 1 \\lt p^{\\prime} \\lt \\infty$が与えられたとする。\n$$ \\dfrac{1}{p}+\\dfrac{1}{p^{\\prime}} = 1 \\left(\\text{or } p^{\\prime} = \\frac{p}{p-1} \\right) $$\nもし$u \\in L^p(\\Omega)$と$v\\in L^{p^{\\prime}}(\\Omega)$ならば$uv \\in L^1(\\Omega)$であり、下記の不等式が成り立つ。\n$$ \\| uv \\|_{1} = \\int_{\\Omega} |u(x)v(x)| dx \\le \\| u \\|_{p} \\| v \\|_{p^{\\prime}} $$\nこの不等式をヘルダーの不等式ヘルダーの不等式という。\n説明 $p^{\\prime}$は$p$のヘルダー共役ヘルダー共役または共役指数共役指数と呼ばれる。$q$と表記することが多い。\n$| u(x) |^{p}$と$| v(x) |^{p^{\\prime}}$が$\\Omega$においてほとんど至る所で比例関係にある場合、等式が成り立つ。\n本質的にユークリッド空間におけるヘルダーの不等式と同じで、$p=p^{\\prime}=2$の時にコーシー・シュワルツの不等式になるのも同様だ。証明自体はコーシー・シュワルツの不等式の証明と同じで、ヤングの不等式が追加されただけである。\n次のような形での一般化も可能である。\n$$ \\| uv \\|_{r} = \\left( \\int_{\\Omega} |u(x)v(x)|^{r} dx \\right)^{1/r} \\le \\| u \\|_{p} \\| v\\|_{p^{\\prime}} $$\n$$ \\| u \\|_{r} = \\left( \\int_{\\Omega} |u(x)|^{r} dx \\right)^{1/r} \\le \\prod_{j=1}^{N} \\| u_{j} \\|_{{p}_j} = \\| u_{1} \\|_{{p}_1} \\cdots \\| u_{N} \\|_{p_{N}} $$\n証明 ヤングの不等式\n$\\dfrac{1}{p} + \\dfrac{1}{p^{\\prime}} = 1$を満たし、1より大きい二つの定数$p, p^{\\prime}$と二つの正の数$a,b$に対して\n$$ ab \\le { {a^{p}} \\over {p} } + {{b^{p^{\\prime}}} \\over {p^{\\prime}}} $$\nケース1. $\\| u \\|_{p} = 0$または$\\| v \\|_{p^{\\prime}} = 0$\n$\\Omega$のほとんど至る所で$u(x) = 0$であるか、$\\Omega$のほとんど至る所で$v(x) = 0$であるため、$\\Omega$のほとんど至る所で$u(x)v(x) = 0$である。したがって\n$$ \\left| \\int_{\\Omega} u(x) v(x) dx \\right| = \\| uv \\|_{1} = 0 $$\nそして\n$$ \\| u \\|_{p} \\| v \\|_{p^{\\prime}} = 0 $$\nとなり、不等式が成り立つ。\nケース2. その他の場合\nヤングの不等式に$a = \\dfrac{\\left| u(x) \\right|}{\\| u \\|_{p}}$と$b = \\dfrac{\\left| v(x) \\right|}{\\| v \\|_{p^{\\prime}}}$を代入する。すると\n$$ \\dfrac{\\left| u(x) \\right|}{\\| u \\|_{p}} \\dfrac{\\left| v(x) \\right|}{\\| v \\|_{p^{\\prime}}} \\le \\dfrac{ \\left| u(x) \\right|^{p}}{ p \\| u \\|_{p}^{p}} + \\dfrac{\\left| v(x) \\right|^{p^{\\prime}}}{ p^{\\prime} \\| v \\|_{p^{\\prime}}^{p^{\\prime}}} $$\n両辺を積分すると以下のようになる。\n$$ \\begin{align*} \\dfrac{1}{\\| u \\|_{p} \\| v \\|_{p^{\\prime}}} \\int_{\\Omega}\\left| u(x)v(x) \\right| dx \\le \u0026amp; \\dfrac{1}{p \\| u \\|_{p}^{p}} \\int_{\\Omega} \\left| u(x) \\right|^{p} dx + \\dfrac{1}{ p^{\\prime} \\| v \\|_{p^{\\prime}}^{p^{\\prime}}} \\int_{\\Omega} \\left| v(x) \\right|^{p^{\\prime}} dx \\\\ \\le \u0026amp; \\dfrac{1}{p \\| u \\|_{p}^{p}} \\| u \\|_{p}^{p} + \\dfrac{1}{ p^{\\prime} \\| v \\|_{p^{\\prime}}^{p^{\\prime}}} \\| v \\|_{p^{\\prime}}^{p^{\\prime}} \\\\ \\le \u0026amp; \\dfrac{1}{p} + \\dfrac{1}{ p^{\\prime} } \\\\ =\u0026amp; 1 \\end{align*} $$\n左辺の定数を移行すると\n$$ \\| uv \\|_{1} = \\int_{\\Omega} |u(x)v(x)| dx \\le \\| u \\|_{p} \\| v \\|_{p^{\\prime}} $$\nよって$uv \\in L^{1}(\\Omega)$であり、不等式が成り立つ。\n■\n関連項目 ユークリッド空間におけるヘルダーの不等式 一般化されたヘルダーの不等式 Robert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p24-25\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":609,"permalink":"https://freshrimpsushi.github.io/jp/posts/609/","tags":null,"title":"ルベーグ空間におけるヘルダーの不等式の証明"},{"categories":"르벡공간","contents":"定義1 2 3 $\\Omega \\subset \\mathbb{R}^{n}$を開集合、$p$を正の実数としよう。\n$\\Omega$上で定義された全ての可測関数 $f$に対して、集合 $L^{p}(\\Omega)$を以下のように定義する。\n$$ L^{p}(\\Omega) := \\left\\{ f : \\int_{\\Omega} \\left| f(x) \\right|^{p} dx \u0026lt; \\infty \\right\\} $$\nこれをLp空間あるいはルベーグ空間と呼び、簡単に$L^{p}$などと表記することもある。通常、関数解析の教科書では上記のように記述され、測度論、実解析の教科書では次のように記述される。\n測度空間 $(X, \\mathcal{E}, \\mu)$が与えられたとする。$X$上で定義された可測関数 $f$に対して、集合 $L^{p}(X, \\mathcal{E},\\mu)$を次のように定義する。\n$$ L^{p}(X, \\mathcal{E}, \\mu) := \\left\\{ f : \\int \\left| f \\right|^{p} d \\mu \u0026lt; \\infty \\right\\} $$\nここで$\\mu$は測度である。簡単に$L^{p}(\\mu), L^{p}(X)$などと表記される。\n性質 $L^{p}$はベクトル空間である。 $1 \\le p \\le \\infty$に対して$L^{p}$はノルム空間である。 $L^{p}$は完備空間である。 $E\\subset X$に対して、$1 \\le p \\le q \\le \\infty$かつ$\\mu (E) \u0026lt; \\infty \\implies L^{q} (E) \\subset L^{p} (E)$である。 説明 2. $p \\lt 1$の場合、$\\left\\| \\cdot \\right\\|_{p}$は三角不等式を満たさず、ノルムにならない。しかし、$p = \\infty$の場合、$L^{p}$空間はノルム空間になる。\n完備なノルムベクトル空間を特にバナッハ空間と呼ぶ。したがって、$L^{p}$空間はバナッハ空間である。$L^{p}$は、ヘルダーの不等式およびミンコフスキーの不等式が成立する空間として特に重要である。\n内積が定義されたベクトル空間を内積空間と言う。完備な内積空間を特にヒルベルト空間と言う。$L^{2}$空間の場合は、次のように内積を定義することができる。\n$$ \\left( \\int |f(x)|^2 dx\\right)^{\\frac{1}{2}} = \\left( \\int f(x)\\overline{f(x)}dx \\right) ^{\\frac{1}{2}} = \\langle f,f \\rangle ^{\\frac{1}{2}} $$\nしたがって、$L^{2}$空間はヒルベルト空間である。\n4. $\\mu (E) \u0026lt; \\infty$という条件に注目しよう。もし積分範囲が有界でない場合、$L^{1} (E)$と$L^{2} (E)$はいかなる包含関係も持たなくなる。$1 \\le p \\lt q \\lt r$が特定の条件を満たす場合、${u \\in L^{p} \\cap L^{r} \\implies u \\in L^{q}}$が成り立つこともある。\n証明 2. $1\\le p \u0026lt;\\infty$に対して$\\| \\cdot \\|_{p}$を次のように定義する。\n$$ \\left\\| f \\right\\|_{p} := \\left( \\int_{\\Omega} \\left| f(x) \\right|^{p} dx \\right)^{1/p},\\quad f\\in L^{p}(\\Omega) $$\nすると、$\\| \\cdot \\|_{p}$は$L^{p}$空間のノルムになる（$0\u0026lt;p\u0026lt;1$の時はノルムにならない）。$\\| f \\|_{p} \\ge 0$であることは自明であり、$\\| f \\|_{p}=0 \\iff f=0$であることも自明だ。また、$c \\in \\mathbb{C}$に対して$\\| cf \\|_{p} = \\left| c \\right| \\left\\| f \\right\\|_{p}$が成立することも以下のように示すことができる。\n$$ \\begin{align*} \\left\\| cf \\right\\|_{p} =\u0026amp; \\left( \\int_{\\Omega} \\left| cf(x) \\right|^{p} dx \\right)^{1/p} \\\\ =\u0026amp; \\left( \\left| c \\right|^{p} \\int_{\\Omega} \\left| f(x) \\right|^{p} dx \\right)^{1/p} \\\\ =\u0026amp; \\left| c \\right| \\left( \\int_{\\Omega} \\left| f(x) \\right|^{p} dx \\right)^{1/p} \\\\ =\u0026amp; \\left| c \\right| \\left\\| f \\right\\|_{p} \\end{align*} $$\n$f,g \\in L^{p}$に対して、$\\left\\| f + g \\right\\|_{p} \\le \\| f \\|_{p} + \\| g \\|_{p}$も同様に成立し、これはミンコフスキーの不等式と呼ばれている。\n■\n3. 戦略：ほとんど全てがファトゥの補題によって解決される。\n与えられたコーシー数列 $f_{n}$ に対して、$\\left\\| f_{n} - f_{n_{k}} \\right\\|_{p} \u0026lt; \\dfrac{1}{2^{k}}$を満たす部分数列 $f_{n_{k}}$ を見つけることができる。全ての $k \\in \\mathbb{N}$ に対して\n$$ \\begin{align*} g_{k} :=\u0026amp; \\sum_{i=1}^{k} \\left| f_{n_{i+1}} - f_{n_{i}} \\right| \\\\ g :=\u0026amp; \\lim_{k \\to \\infty} g_{k} = \\sum_{i=1}^{\\infty} \\left| f_{n_{i+1}} - f_{n_{i}} \\right| \\end{align*} $$\nを定義すると、三角不等式によって\n$$ \\left\\| g_{k} \\right\\|_{p} \\le \\sum_{i}^{k} \\dfrac{1}{2^{i}} \u0026lt; 1 $$\nファトゥの補題\n関数値が非負の可測関数の数列 $\\left\\{ f_{n} \\right\\}$ に対して\n$$ \\int \\left( \\liminf_{n \\to \\infty} f_{n} \\right) d \\mu \\le \\liminf_{n \\to \\infty} \\int f_{n} d \\mu $$\nファトゥの補題により\n$$ \\left\\| g \\right\\|_{p}^{p} \\le \\int \\lim_{n \\to \\infty} g_{k}^{p} d \\mu \\le \\liminf_{k \\to \\infty} \\int g_{k}^{p} d \\mu \\le 1 $$\n$g$がほとんど至る所で有限であるため、\n$$ f_{n_{k}} = f_{n_{1}}(x) + \\sum_{i=1}^{ k } \\left[ f_{n_{i}} (x) - f_{n_{i-1}} (x) \\right] $$\nはほとんど至る所で収束する。$f := \\lim\\limits_{k \\to \\infty} f_{n_{k}}$と定義すると、ファトゥの補題により\n$$ \\left\\| f - f_{m} \\right\\|_{p} = \\int |f - f_{m}|^{p} d \\mu \\le \\liminf_{k \\to \\infty} \\int | f_{n_{k}} - f_{m}|^{p} d \\mu \\le \\varepsilon^{p} $$\nしたがって$f - f_{m} \\in L^{p}$であり、$f = f_{m} + (f - f_{m} ) \\in L^{p}$である。$L^{p}$の全てのコーシー数列が$L^{p}$の元に収束するため、$L^{p}$は完備空間である。\n■\n4. 戦略：$|f(x)|^{p} \\le 1 + |f(x)|^{q}$という不等式を示せば、残りはルベーグ積分の性質によって証明が終わる。\n$f \\in L^{q}$としよう。すると、次の式が成り立つ。\n$$ \\begin{align*} | f(x) | \\le 1 \\implies\u0026amp; |f(x) |^{p} \\le 1 \\\\ 1 \\le |f(x)| \\implies\u0026amp; |f(x)|^{p} \\le |f(x)|^{q} \\end{align*} $$\nしたがって、$| f(x) |$が$1$より大きいか小さいかにかかわらず、次が成り立つ。\n$$ |f(x)|^{p} \\le 1 + |f(x)|^{q} $$\nルベーグ積分 $\\displaystyle \\int_{E} d \\mu$ をとると、以下のようになる。\n$$ \\int_{E} |f|^{p} d \\mu \\le \\int_{E} 1 d \\mu + \\int_{E} |f|^{q} d \\mu = m(E) + \\int_{E} |f|^{q} d \\mu \u0026lt; \\infty $$\n$m(E) \u0026lt; \\infty$であり$\\displaystyle \\int_{E} |f|^{q} d \\mu \u0026lt; \\infty$であるため、次が成り立つ。\n$$ \\int_{E} |f|^{p} d \\mu \u0026lt; \\infty $$\n言い換えれば、$f \\in L^{q} \\implies f \\in L^{p}$であるため、\n$$ L^{q} (E) \\subset L^{p} (E) $$\n■\n参照 $L^{1}$空間 $L^{2}$空間 ヒルベルト空間 Capinski, Measure, Integral and Probability (1999), p140\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p23\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p181\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":605,"permalink":"https://freshrimpsushi.github.io/jp/posts/605/","tags":null,"title":"Lp空間、ルベーグ空間"},{"categories":"르벡공간","contents":"定義 1 関数空間 $L^{2}$ を次のように定義する。\n$$ L^{2} (E) := \\left\\{ f : \\left( \\int_{E} | f |^2 dm \\right)^{{1} \\over {2}} \u0026lt; \\infty \\right\\} $$\n性質 $L^{2}$は距離空間である。距離は次のように定義される。 $$ d(f,g) := \\left( \\int \\left| f(x) - g(x) \\right|^{2}dx \\right)^{\\frac{1}{2}} = \\left\\| f-g \\right\\|_{2} = \\sqrt{\\braket{f-g, f-g}} $$ $L^{2}$はベクトル空間である。 $L^{2}$はノルム空間である。ノルムは次のように定義される。 $$ \\left\\| f \\right\\|_{2} := \\left( \\int \\left| f(x) \\right|^{2}dx \\right)^{\\frac{1}{2}} = \\sqrt{\\braket{f,f}} $$ $L^{2}$は完備空間である。 $L^{2}$は内積空間である。内積は次のように定義される。 $$ \\braket{f, g} := \\int \\overline{f(x)}g(x)dx $$ 説明 $L^{2}$空間は、$p=2$の時の$L^{p}$空間の特別な場合であり、$L^{p}$空間の中で唯一内積が定義される空間である。完備内積空間は特にヒルベルト空間Hilbert spaceと呼ばれる。したがって、$L^{2}$はヒルベルト空間である。ヒルベルト空間は、偏微分方程式や量子力学を含む様々な分野で登場する重要な空間である。\n$L^{p}$空間についての一般化された証明は、こちらを参照してください。\n証明 3. ノルムの定義\n$V$を$\\mathbb{F}$上のベクトル空間としよう。関数$\\left\\| \\cdot \\right\\| : V \\to \\mathbb{F}$が$\\mathbf{u}, \\mathbf{v} \\in V$と$k \\in \\mathbb{F}$に対して次の三つの条件を満たすならば、それを**$V$上のノルム**と定義する。\n正定性: $\\left\\| \\mathbf{u} \\right\\| \\ge 0$そして$\\mathbf{u} = \\mathbb{0} \\iff \\left\\| \\mathbf{u} \\right\\| = 0$ 斉次性: $\\left\\|k \\mathbf{u} \\right\\| = | k | \\left\\| \\mathbf{u} \\right\\| $ 三角不等式: $\\left\\| \\mathbf{u} + \\mathbf{v}\\right\\| \\le \\left\\|\\mathbf{v} \\right\\| + \\left\\| \\mathbf{u} \\right\\|$ $L ^{2}$のノルムを$\\displaystyle \\left\\| f \\right\\|_{2} := \\left( \\int_{E} | f |^2 dm \\right)^{{1} \\over {2}}$のように定義しよう。\nPart 1. 正定性\n$| f | \\ge 0$だから、$\\left\\| f \\right\\|_{2} \\ge 0$ほとんど至る所で$f = 0$ならば、$\\left\\| f \\right\\|_{2} = 0$である。反対に、$\\left\\| f \\right\\|_{2} = 0$ならば、ほとんど至る所で$f = 0$でなければならない。\nPart 2. 斉次性\n$$ \\left\\| c f \\right\\|_{2} = \\left( \\int_{E} | c f |^2 dm \\right)^{{1}\\over {2}} =\\left( |c|^2 \\int_{E} | f |^2 dm \\right)^{{1}\\over {2}} = |c| \\left( \\int_{E} | f |^2 dm \\right)^{{1}\\over {2}} = |c| \\left\\| f\\right\\|_{2} $$\nPart 3. 三角不等式\n$$ \\begin{align*} \\left\\| f + g \\right\\|_{2}^{2} =\u0026amp; \\int_{E} | f + g |^2 dm \\\\ =\u0026amp; \\int_{E} ( f + g ) \\overline{( f + g )} dm \\\\ =\u0026amp; \\int_{E} | f |^2 dm + \\int_{E} ( f \\overline{g} + \\overline{f} g ) dm +\\int_{E} | g |^2 dm \\end{align*} $$\nコーシー-シュワルツの不等式により、次を得る。\n$$ \\begin{align*} \\int_{E} ( f \\overline{g} + \\overline{f} g ) dm \\le \u0026amp; 2 \\int_{E} | fg | dm \\le 2 | f |_{2} | g |_{2} \\\\ =\u0026amp; | f + g | _{2}^{2} \\le | f | _{2} + 2 | f | _{2} | g | _{2} + | g | _{2} \\\\ =\u0026amp; \\left( | f |_{2} + | g |_{2} \\right)^{2} \\end{align*} $$\nまとめると\n$$ \\left\\| f + g \\right\\|_{2} \\le \\left\\| f \\right\\|_{2} + | g |_{2} $$\n■\n5. 内積の定義\n$H$をベクトル空間としよう。$x,y,z \\in H$と$\\alpha, \\beta \\in \\mathbb{C}$に対して、次の条件を満たす関数\n$$ \\langle \\cdot , \\cdot \\rangle : H \\times H \\to \\mathbb{C} $$\nを内積と定義し、$\\left( H, \\langle \\cdot ,\\cdot \\rangle \\right)$を内積空間と言う。\n線形性: $\\langle \\alpha x + \\beta y ,z \\rangle =\\alpha \\langle x,z\\rangle + \\beta \\langle y,z\\rangle$ 共役対称性: $\\langle x,y \\rangle = \\overline{ \\langle y,x \\rangle}$ 正定性: $\\langle x,x \\rangle \\ge 0 \\quad \\text{and} \\quad \\langle x,x \\rangle = 0\\iff x=0$ $L ^{2}$の内積を$\\displaystyle \\langle f , g \\rangle := \\int_{E} f \\overline{g} dm$のように定義しよう。\nPart 1. 線形性\n$$ \\langle f + g , h \\rangle = \\int_{E} ( f + g ) \\overline{g} dm = \\int_{E} f \\overline{g} dm + \\int_{E} g \\overline{g} dm = \\langle f , h \\rangle + \\langle g , h \\rangle $$\nそして\n$$ \\langle c f , g \\rangle = \\int_{E} c f \\overline{g} dm = c \\int_{E} f \\overline{g} dm = c \\langle f , g \\rangle $$\nPart 2. 共役対称性\n$$ \\langle f , g \\rangle = \\int_{E} f \\overline{g} dm = \\overline{ \\int_{E} \\overline{f} g dm} = \\overline{ \\int_{E} g \\overline{f} dm} = \\overline{ \\langle f , g \\rangle } $$\nPart 3. 正定性\n$$ \\langle f, f \\rangle = \\int_{E} f \\overline{f} dm = \\int_{E} | f |^2 dm = \\sqrt{ | f |_{2} } $$\n性質 3. のPart 1により証明終了だ。\n■\n参照 $L^{p}$空間 $L^{1}$空間 ヒルベルト空間 Capinski. (1999). Measure, Integral and Probability: p131.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":594,"permalink":"https://freshrimpsushi.github.io/jp/posts/594/","tags":null,"title":"L2空間"},{"categories":"르벡공간","contents":"定義1 次のように関数空間$L^{1}$を定義する。\n$$ L^{1} (E) := \\left\\{ f : E \\to \\mathbb{R} \\Big \\vert \\int_{E} | f | dm \\lt \\infty \\right\\} $$\n性質 $L^{1}$はベクトル空間だ。 $L^{1}$はノルム空間だ。ノルムは以下のように定義される。 $$ \\left\\| f \\right\\|_{1} := \\int \\left| f(x) \\right| dx $$ $L^{1}$は完備空間だ。 説明 $L^{1}$空間は$L^{p}$空間の$p=1$の時の特別なケースであり、ルベーグ可積分について話す時に、可積分な関数の集まりとして定義された。\n$L^{p}$空間に関する一般化された証明はここを参照。\n証明 2. ノルムの定義\n$V$を$\\mathbb{F}$上のベクトル空間としよう。関数$\\left\\| \\cdot \\right\\| : V \\to \\mathbb{F}$が$\\mathbf{u}, \\mathbf{v} \\in V$と$k \\in \\mathbb{F}$に対して以下の三つの条件を満たすなら、$\\left\\| \\cdot \\right\\|$を**$V$上のノルム**と定義する。\n正定値性: $\\left\\| \\mathbf{u} \\right\\| \\ge 0$かつ$\\mathbf{u} = \\mathbb{0} \\iff \\left\\| \\mathbf{u} \\right\\| = 0$ 斉次性: $\\left\\|k \\mathbf{u} \\right\\| = | k | \\left\\| \\mathbf{u} \\right\\| $ 三角不等式: $\\left\\| \\mathbf{u} + \\mathbf{v}\\right\\| \\le \\left\\|\\mathbf{v} \\right\\| + \\left\\| \\mathbf{u} \\right\\|$ $L ^{1}$のノルムを$\\displaystyle \\left\\| f \\right\\|_{1} := \\int_{E} |f| dm$として定義しよう。\nPart 1. 正定値性\n$| f | \\ge 0$より、ほとんど至る所で$f = 0$ならば$\\left\\| f \\right\\|_{1} = 0$である。逆に、$\\left\\| f \\right\\|_{1} = 0$ならば、ほとんど至る所で$f = 0$でなければならない。\nPart 2. 斉次性\n$$\\left\\| c f \\right\\| _{1} = \\int_{E} | c f | dm = |c| \\int_{E} | f | dm = |c| \\left\\| f \\right\\| _{1}$$\nPart 3. 三角不等式\n$$ \\left\\| f + g \\right\\|_{1} = \\int_{E} | f + g | dm \\le \\int_{E} | f | dm + \\int_{E} | g | dm = \\left\\| f\\right\\|_{1} + \\left\\| g\\right\\|_{1} $$\n■\n3. 完備性\nベクトル空間$X$に対して、ノルム$\\left\\| \\cdot \\right\\|_{X}$が定義されているとしよう。すべての$\\varepsilon \u0026gt; 0$に対して $$n, m \\ge N \\implies \\left\\| f_{n} - f_{m} \\right\\|_{X} \\lt \\varepsilon$$ $N \\in \\mathbb{N}$が存在するなら、数列$f_{n} \\in X$をコーシー数列と言う。もしすべてのコーシー数列が$X$の要素に収束するなら、$X$を完備と言う。\n$f_{n} \\in L^{1}$がコーシー数列なら\n$$ \\left\\| f_{n} - f_{N_{1}} \\right\\|_{1} \\lt {{1} \\over {2}} $$\n$N_{1}$が存在し、同様に\n$$ \\left\\| f_{n} - f_{N_{2}} \\right\\|_{1} \\lt {{1} \\over {2^2}} $$\n$N_{2} \u0026gt; N_{1}$が存在する。この方法で $$ \\left\\| f_{n} - f_{N_{n}} \\right\\|_{1} \\lt {{1} \\over {2^n}} $$ $N_{n} \u0026gt; N_{n-1}$が存在する。三角不等式により $$ \\left\\| f_{N_{n}} - f_{N_{n-1}} \\right\\|_{1} \\lt \\left\\| f_{N_{n}} - f_{n} \\right\\|_{1} + \\left\\| f_{n} - f_{N_{n-1}} \\right\\|_{1} \\lt {{1} \\over {2^n}} + {{1} \\over {2^{n-1}}} \\lt {{3} \\over {2^{n}}} $$\nレヴィの定理\n$\\displaystyle \\sum_{k=1}^{\\infty} \\int |f_{k}| dm \\lt \\infty$ならば$\\displaystyle \\sum_{k=1}^{\\infty} f_{k} (x)$はほとんど至る所では収束し、以下が成立する。\n$$ \\int \\sum_{k=1}^{\\infty} f_{k} dm = \\sum_{k=1}^{\\infty} \\int f_{k} dm $$\nレヴィの定理により、$\\displaystyle \\sum_{n=1}^{\\infty} | f_{N_{n}} - f_{N_{n-1}} |_{1}$は収束する。したがって、以下はほとんど至る所で収束する。\n$$ f_{N_{1}}(x) + \\sum_{n=2}^{ k } \\left[ f_{N_{n}} (x) - f_{N_{n-1}} (x) \\right] = f_{N_{k}} $$\nここで、右辺が$f(x)$に収束するとすれば、右辺の$f_{N_{k}} (x)$も$f(x)$に収束する。\nファトゥの補助定理\n関数値が非負の可測関数の数列$\\left\\{ f_{n} \\right\\}$に対して\n$$ \\displaystyle \\int_{E} \\left( \\liminf_{n \\to \\infty} f_{n} \\right) dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$\nファトゥの補助定理により、\n$$ \\begin{align*} \\left\\| f - f_{n} \\right\\|_{1} =\u0026amp; \\int |f - f_{n}| dm \\\\ \\le \u0026amp; \\liminf_{k \\to \\infty} \\int | f_{N_{k}} - f_{n}| dm \\\\ =\u0026amp; \\liminf_{k \\to \\infty} \\left\\| f_{N_{k}} - f_{n} \\right\\| \\\\ \\lt\u0026amp; \\varepsilon \\end{align*} $$\n$f_{n}$がコーシー数列であるため、任意の$\\varepsilon \u0026gt; 0$に対して上記の不等式が成立し、したがって$\\left\\| f_{n} - f \\right\\|_{1} \\to 0$である。要約すると、$f_{n}$がコーシーで、その部分数列が$f$に収束するので、$f_{n}$は$f$に収束する。ここで、$f - f_{n} \\in L^{1}$であり、$L^{1}$がベクトル空間なので、\n$$ ( f - f_{n} ) + f_{n} = f \\in L^{1} $$\n$L^{1}$のすべてのコーシー数列が$L^{1}$の要素に収束するので、$L^{1}$は完備空間である。\n■\n参照 $L^{p}$空間 $L^{2}$空間 Capinski. (1999). Measure, Integral and Probability: p127.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":592,"permalink":"https://freshrimpsushi.github.io/jp/posts/592/","tags":null,"title":"L1空間"},{"categories":"추상대수","contents":"定義 1 群 $G$ の部分集合 $H$ が群 $G$ の演算で群である時、$H$は群 $G$の部分群subgroupと呼ばれる。\n定理 部分群の判定法: 群 $G$ の空でない部分集合 $H$ において、$a,\\ b$ が $H$ の要素の時に $ab^{-1}$ も $H$ の要素であれば、$H$は $G$ の部分群である。つまり、$a,\\ b$ が $H$ の要素の時 $a-b$ も $H$ の要素であれば、$H$ は部分群である。\n証明 $a,\\ b$ が $H$ の要素の時に $ab^{-1}$ も $H$ の要素であると仮定しよう。すると、$H$ が群になるための3つの条件を満たすか確認すればいい。\n$H$ の演算は群 $G$ の演算と同じなので、結合法則は自明である。 $a=x,\\ b=x$ としよう。すると、$ab^{-1}=xx^{-1}=e$ であり、仮定により $H$ の要素になるので、$H$ は単位元を持つ。 $a=e,\\ b=x$ としよう。すると、$ex^{-1}=x^{-1}$ であり、仮定により $H$の要素になるので、$H$ の任意の要素 $b$ は逆元を持つ。 3により、どの要素も逆元を持つことが確認できたので、$a=x,\\ b=-y$ としよう。すると、$x(y^{-1})^{-1}=xy$ であり、仮定により $H$ の要素になるので、$H$ は演算に対して閉じている。 1~4により、$H$ は群 $G$ の演算に対して閉じており、結合法則が成立し、単位元と逆元を持つため、群である。したがって、部分集合 $H$ は群 $G$ の部分群である。\n■\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p50.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":589,"permalink":"https://freshrimpsushi.github.io/jp/posts/589/","tags":null,"title":"部分群の定義と部分群の判定法"},{"categories":"통계적분석","contents":"ビルドアップ Rで内蔵データ faithful を読み込み、head()関数で確認してみよう。\nたった六つだけど、一見すると、eruptionsとwaitingは正の相関関係を持っているように見える。これらの関係を何らかの二つの定数$\\beta_{0}, \\beta_{1}$について $$\\text{(eruptions)} = \\beta_{0} + \\beta_{1} \\cdot \\text{(waiting) }$$ と表すことができればいい。上の式は二変数の線形関係を直線の方程式として表したもので、$\\beta_{0}$は定数項、$\\beta_{1}$は傾きを意味する。\nしかし、実際のデータでは、理論と異なり誤差が生じるため、何らかの誤差項$\\varepsilon$が必要だ。式を簡単にするために$y:=\\text{(eruptions)}$及び$x:=\\text{(waiting) }$とすると、次を得る。 $$y = \\beta_{0} + \\beta_{1} x + \\varepsilon$$\n上のスクリーンショットでは、合計$6$組の順序対が表示されているが、これらを連立方程式で表すと次のようになる。\n$$ \\begin{cases} 3.600 = \\beta_{0} + \\beta_{1} 79 + \\varepsilon_{1} \\\\ 1.800 = \\beta_{0} + \\beta_{1} 54 + \\varepsilon_{2} \\\\ 3.333 = \\beta_{0} + \\beta_{1} 74 + \\varepsilon_{3} \\\\ 2.283 = \\beta_{0} + \\beta_{1} 62 + \\varepsilon_{4} \\\\ 4.533 = \\beta_{0} + \\beta_{1} 85 + \\varepsilon_{5} \\\\ 2.883 = \\beta_{0} + \\beta_{1} 55 + \\varepsilon_{6} \\end{cases} $$\n実際にfaithfulは、272組の順序対を含んでいるので、このように全てを表現するのは非現実的だが、再び記号を通して表現してみよう。\n$$ \\begin{cases} y_{1} \u0026amp;= \\beta_{0} + \\beta_{1} x_{1} + \\varepsilon_{1} \\\\ y_{2} \u0026amp;= \\beta_{0} + \\beta_{1} x_{2} + \\varepsilon_{2} \\\\ \u0026amp;\\vdots\u0026amp; \\\\ y_{272} \u0026amp;= \\beta_{0} + \\beta_{1} x_{272} + \\varepsilon_{272} \\end{cases} $$\n一方、このような連立方程式は行列方程式としてよりシンプルに表現できる。 $$ \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{272} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{1} \\\\ 1 \u0026amp; x_{2} \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{272} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{272} \\end{bmatrix} $$ 通常通り、行列まで大文字で表記すると、ついに$Y = X \\beta + \\varepsilon$を得ることができる。\n定義 ここでは、$X$のように独立変数をまとめた行列を設計行列Design Matrixと呼ぶ。\n先修科目 このようにデータを行列で表現できることは、線形代数の様々なツールを統計学に適用できることを意味する。ここで$\\beta$を見つけることがまさに回帰分析であり、これを正確に理解するには、線形代数の知識が不可欠だ。\n統計学に接近する多くの学習者が線形代数の必要性を感じずに軽視してしまい、行列が出てくると苦労する。先輩たちの轍を踏まないためにも、解析学や線形代数のような低学年科目を徹底的に磨き上げることが重要だ。\n","id":550,"permalink":"https://freshrimpsushi.github.io/jp/posts/550/","tags":null,"title":"デザイン行列"},{"categories":"수리물리","contents":"まとめ1 3次元ベクトル関数$\\mathbf{F}$について、以下が成り立つ。\n$$ \\begin{equation} \\int_{\\mathcal{V}} \\nabla \\cdot \\mathbf{F} dV = \\oint_{\\mathcal{S}} \\mathbf{F} \\cdot d \\mathbf{S} \\label{1} \\end{equation} $$\nここで、$\\nabla \\cdot \\mathbf{F}$はダイバージェンス、$\\int_{\\mathcal{V}}$は体積積分、$\\oint_{\\mathcal{S}}$は閉曲面積分である。\n説明 これをガウスの定理Gauss\u0026rsquo;s theorem、グリーンの定理Green\u0026rsquo;s theorem、または発散定理divergence theoremと呼ぶ。特に電磁気学でよく使用される。\n数式的意味 数式的には、面積分を体積積分に、体積積分を面積分に変換できるという意味である。つまり、三重積分と二重積分を互いに変換できるということである。\n物理的意味 物理的には、各点（小さい体積）で入ってくる量と出て行く量の総和$\\big( \\eqref{1}$の左辺は、全体の体積の表面で入ってくる量と出て行く量の総和$\\big( \\eqref{1}$の右辺と同じという意味である。\n簡単な例として、ある部屋に人々がいると考えてみよう。人々はドアを通して部屋に入ったり、部屋を出たりする。部屋の中とドアを見ている2名の観察者がいるとする。合計で2名が部屋に入り、3名が部屋から出たとしよう。この場合、**部屋の中の観察者が見た人の変化2は$|2-3|=1$**であり、**ドア番が見た人の変化3は$|3-2|=1$**である。（$1$名がドアを開けて出た時、$+1$と数えるとする）この2つは常に同じである。\n証明 それぞれの面の体積積分を全て足した時に、発散の体積積分と同じになるか確認しよう。まずは、下の図のように各点の座標と各面の名前を設定しよう。\n全ての面について面積分を足すと、以下のようになる。\n$$ \\int _{S_{1}} \\mathbf{F} \\cdot d \\mathbf{S}_{1} + \\int_ {S_2} \\mathbf{F} \\cdot d \\mathbf{S}_2 + \\int_ {S_{3}} \\mathbf{F} \\cdot d \\mathbf{S}_ + \\int_ {S_{4}} \\mathbf{F} \\cdot d\\mathbf{S}_{4} + \\int _{S_{5}} \\mathbf{F} \\cdot d\\mathbf{S}_{5}+\\int _{S_{6}} \\mathbf{F} \\cdot d\\mathbf{S}_{6} $$\nまず、$x$軸に垂直な$S_{1}$と$S_2$の面について計算してみよう。$\\mathbf{F}= F_{x} \\hat{\\mathbf{x}} + F_{y} \\hat{\\mathbf{y}} + F_{z} \\hat{\\mathbf{z}}$であり、各面の方向は外向きである。$\\mathbf{F}$の方向が$S_{1}$の方向と同じだとしよう。すると、二つの面積分は以下のようになる。\n$$ \\begin{align*} \\int _{S_{1}} \\mathbf{F} \\cdot d \\mathbf{S}_{1} + \\int _{S_2} \\mathbf{F} \\cdot d \\mathbf{S}_2 \u0026amp;= \\int_ {S_{1}} F_{x} dS_{1} - \\int _{S_2} F_{x} dS_2 \\\\ \u0026amp;= \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} F_{x} (x+\\Delta x,y,z) dydz - \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} F_{x} (x,y,z) dydz \\\\ \u0026amp;= \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} \\bigg[ F_{x} (x+\\Delta x,y,z) - F_{x} (x,y,z) \\bigg] dydz \\end{align*} $$\nこの時点で、微積分学の基本定理によれば$\\displaystyle \\int _{a} ^b \\dfrac{ dF(x)}{dx}dx=F(b) - F(a)$であるので、以下のようにまとめることができる。\n$$ \\begin{align*} \u0026amp; \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} \\left[ F_{x} (x+\\Delta x,y,z) - F_{x} (x,y,z) \\right] dydz \\\\ =\u0026amp;\\ \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} \\left[ \\int_{x} ^{x +\\Delta x} \\dfrac{ \\partial F_{x}(x,y,z) }{\\partial x} dx \\right] dydz \\\\ =\u0026amp;\\ \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} \\int_{x} ^{x +\\Delta x} \\dfrac{ \\partial F_{x}(x,y,z) }{\\partial x} dx dydz \\\\ =\u0026amp;\\ \\iiint \\dfrac{ \\partial F_{x} }{\\partial x} dV \\end{align*} $$\nよって、以下の結果を得る。\n$$ \\int_ {S_{1}} \\mathbf{F} \\cdot d \\mathbf{S}_{1} + \\int_ {S_2} \\mathbf{F} \\cdot d \\mathbf{S}_2 =\\iiint \\dfrac{ \\partial F_{x} }{\\partial x} dV $$\n同様に、$S_{3}$と$S_{4}$に対する面積分と、$S_{5}$と$S_{6}$に対する面積分を計算すると、以下のようになる。\n$$ \\int _{S_{3}} \\mathbf{F} \\cdot d \\mathbf{S}_{1} + \\int _{S_{4}} \\mathbf{F} \\cdot d \\mathbf{S}_2 =\\iiint \\dfrac{ \\partial F_{y} }{\\partial y} dV $$\n$$ \\int_ {S_{5}} \\mathbf{F} \\cdot d \\mathbf{S}_{5} + \\int_ {S_{6}} \\mathbf{F} \\cdot d \\mathbf{S}_2 =\\iiint \\dfrac{ \\partial F_{z} }{\\partial z} dV $$\n最後に、6面すべてに対する面積分を全部足すと、以下のようになる。\n$$ \\begin{align*} \\oint _\\mathcal{S} \\mathbf{F} \\cdot d \\mathbf{S} \u0026amp;= \\iiint \\dfrac{ \\partial F_{x} }{\\partial x} dV + \\iiint \\dfrac{ \\partial F_{y} }{\\partial y} dV +\\iiint \\dfrac{ \\partial F_{z} }{\\partial z} dV \\\\ \u0026amp;= \\iiint \\left[ \\dfrac{ \\partial F_{x} }{\\partial x} + \\dfrac{ \\partial F_{y} }{\\partial y} + \\dfrac{ \\partial F_{z} }{\\partial z} \\right] dV \\\\ \u0026amp;= \\iiint \\nabla \\cdot \\mathbf{F} dV \\\\ \u0026amp;= \\int_\\mathcal{V} \\nabla \\cdot \\mathbf{F} dV \\end{align*} $$\n■\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 金 仁成 訳) (第4版, 2014), p35\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n中での変化、つまり、体積に関する変化を意味する。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n出入り口での変化、つまり、表面における変化を意味する。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":565,"permalink":"https://freshrimpsushi.github.io/jp/posts/565/","tags":null,"title":"ガウスの定理, 発散定理"},{"categories":"통계적분석","contents":"説明 回帰分析は、ほぼすべての統計的手法の基礎となっているため、一般的すぎるか特殊すぎる説明が多い。回帰分析が何かを一言で説明するなら、変数間の関係を見つける方法と言えるだろう。\nこの便利で驚くべき分析方法は、優生学の父フランシス・ゴルトンFrancis Galtonのアイディアから生まれた。\nゴルトンは遺伝学を研究しているうちに、父とその息子の身長に関するデータに遭遇し、一般的に父が背が高ければ息子も高く、父が背が低ければ息子も低い傾向があることに気付いた。この関係自体は以前から皆が知っていたが、ゴルトンは世代が経つにつれて平均へ回帰Regressする現象に注目した。\n背の高い父の息子も背が高いが、父よりは低くなる傾向があり、背の低い父の息子も背が低いが、父よりは高くなる傾向があった。論理的に考えれば当然のことで、そうでなければ世代を重ねるごとに身長が無限に発散したり$0$に収束してしまうだろう。\n一方で、必ず平均へ回帰するわけではない。成長環境や突然変異のように避けられない誤差が生じるためだ。それにもかかわらず、明らかに現れる線形関係は、ゴルトンに「身長は遺伝する」という確信を与えたに違いない。\nでは、正確ではなくとも、ある程度の誤差はあるが、父の身長だけを見て息子の身長をだいたい当てることはできないだろうか？父の身長$x$と息子の身長$y$が$y = a + b x$のような関係にあるなら、$x$に父の身長を代入することで、息子の身長を推測することになる。もちろん、完全に一致するわけではないが、平均的にはだいたい合うだろう。\nこれが回帰分析の起こりだ。もちろん、今では回帰分析は非常に多岐にわたる分野に応用されており、世代が変わるといった話はもはや不要であるため、「回帰」という言葉はその意味を失った。語源を理解して、それでいい。\n","id":548,"permalink":"https://freshrimpsushi.github.io/jp/posts/548/","tags":null,"title":"回帰分析とは?"},{"categories":"측도론","contents":"ビルドアップ リーマン積分の一般化を考える前に、簡単な関数Simple Functionを定義する必要がある。\n関数値が非負の$\\phi : \\mathbb{R} \\to \\mathbb{R}$の値域が有限集合$\\left\\{ a_{1} , a_{2}, \\cdots , a_{n} \\right\\}$であるとする。$A_{i} = \\phi^{-1} \\left( \\left\\{ a_{i} \\right\\} \\right) \\in \\mathcal{M}$を満たすなら、$\\phi$を簡単な関数と呼ぶ。簡単な関数には以下の特性がある。\n(i): $i \\ne j$なら$A_{i } \\cap A_{j} = \\emptyset$ (ii): $\\displaystyle \\bigsqcup_{k=1}^{n} A_{k} = \\mathbb{R}$ (iii): $\\displaystyle \\phi (x) = \\sum_{k=1}^{n} a_{k} \\mathbb{1}_{A_{k}}(x)$は可測関数だ。 簡単な関数は定義から、取り扱いが非常に簡単な3つの要素で構成されている。まず第一に、関数値が非負であるため、符号を考える必要がなく、第二に有限であるために、加算と減算が自由であり、第三に可測だ。数学のさまざまな分野で簡単Simpleという言葉はさまざまな意味で使用されるが、少なくとも実解析では「複雑」の反対と考えてもよいだろう。このように扱いやすく便利な簡単な関数を定義した後、すぐにリーマン積分をカバーする新しい積分を考えることができる。\n簡単な関数のルベーグ積分 $\\phi$が簡単な関数で、$E \\in \\mathcal{M}$とするとき、$\\displaystyle \\int_{E} \\phi dm := \\sum_{k=1}^{n} a_{k} m (A_{k} \\cap E)$を簡単な関数$\\phi$のルベーグ積分と呼ぶ。ルベーグ積分には以下の特性がある。\n[1]: すべての$r\u0026gt;0$に対して$\\displaystyle \\int_{E} a \\phi dm = a \\int_{E} \\phi dm $ [2]: 二つの簡単な関数$\\phi , \\psi$に対して$\\phi \\le \\psi$ならば$\\displaystyle \\int_{E} \\phi dm \\le \\int_{E} \\psi dm$ [3]: $A, B \\in \\mathcal{M}$に対して$A \\cap B = \\emptyset$ならば$\\displaystyle \\int_{A \\cup B} \\phi dm = \\int_{A} \\phi dm + \\int_{B} \\phi dm$ $m$はルベーグ測度だ。 しかし、簡単な関数という条件はあまりにも強力で特殊であるため、多くの場所で使うことができない。分割求積法のアイデアのようなものを加えると、ある程度満足できる「ルベーグ積分」が完成する。\n定義 1 $\\phi$が簡単な関数であるとき、関数値が非負の可測関数$f$と$E \\in \\mathcal{M}$に対して $$\\displaystyle \\int_{E} f dm := \\sup \\left\\{ \\left. \\int_{E} \\phi dm \\ \\right| \\ 0 \\le \\phi \\le f \\right\\}$$ を可測関数$f$のルベーグ積分Lebesgue Integralと呼ぶ。\n基本性質 ルベーグ積分には以下の性質がある。\n[1]\u0026rsquo;: すべての$r \\ge 0$に対して$\\displaystyle \\int_{E} r f dm = r \\int_{E} f dm $ [2]\u0026rsquo;: 二つの簡単な関数$f, g$に対して$f \\le g$ならば$\\displaystyle \\int_{E} f dm \\le \\int_{E} g dm$ [3]\u0026rsquo;: $A, B \\in \\mathcal{M}$に対して$A \\cap B = \\emptyset$ならば$\\displaystyle \\int_{A \\cup B} f dm = \\int_{A} f dm + \\int_{B} f dm$ [4]\u0026rsquo;: $A, B \\in \\mathcal{M}$に対して$A \\subset B$ならば$\\displaystyle \\int_{A} f dm \\le \\int_{B} f dm$ [5]\u0026rsquo;: $N \\in \\mathcal{N}$ならば$\\displaystyle \\int_{N} f dm = 0$ [6]\u0026rsquo;: $\\displaystyle m(E) \\inf_{E} f \\le \\int_{E} f dm \\le m(E) \\sup_{E} f $ 説明 これらの基本的な性質に加えて、以下のような定理を考えることができる。この定理を使用すれば、$\\displaystyle \\int_{\\mathbb{R}} \\mathbb{1}_{\\mathbb{Q}} dm = 0$として新鮮な計算も一切れで終わらせることができる。見た目ほど証明は簡単ではないが、一度は見ておく価値があるだろう。\n定理 可測空間$( X , \\mathcal{E} )$の可測関数$f \\ge 0$とすべての可測集合$A \\in \\mathcal{E}$に対して $$ \\int_{A} f dm = 0 \\iff f = 0 \\text{ a.e.} $$\n$\\text{a.e.}$はほとんど至る所を意味する。 証明 $( \\implies )$\n$E := f^{-1} ( 0 , \\infty)$に対して$m(E) = 0$ならば、$f$はほとんど至る所$f=0$だ。$\\displaystyle E_{n} := f^{-1} \\left[ {{1} \\over {n}} , \\infty \\right)$と仮定して、$\\displaystyle E = \\bigcup_{n=1}^{\\infty} E_{n}$でありながら$\\displaystyle \\lim_{n \\to \\infty} E_{n} = E$が成り立つ場合を考える。簡単な関数$\\displaystyle \\phi_{n} := {{1}\\over {n}} \\mathbb{1}_{E_{n}} \\le f$を考えると $$ {{1}\\over {n}} m( E_{n} ) = \\int_{A} \\phi_{n} dm \\le \\int_{A} f dm = 0 $$ 従って $$ {{1} \\over {n}} m(E_{n}) \\le 0 $$ つまり、すべての$n \\in \\mathbb{N}$に対して$m(E_{n}) = 0$である。\n[7]: $E_{n} \\in \\mathcal{M}$, $\\displaystyle E_{n} \\subset E_{n+1} \\implies m \\left( \\bigcup_{n=1}^{\\infty} E_{n} \\right) = \\lim_{n \\to \\infty} m (E_{n})$\n一方で$E_{n} \\subset E_{n+1}$であるため、次のことが成り立つ。 $$ m \\left( \\bigcup_{n=1}^{\\infty} E_{n} \\right) = \\lim_{n \\to \\infty} m (E_{n}) = m(E) = 0 $$\n$( \\impliedby )$\n$f$がほとんど至る所$f=0$であり、簡単な関数$\\phi$が$0 \\le \\phi \\le f$を満たすため、$\\phi$もほとんど至る所$\\phi = 0$である。従って$\\displaystyle \\int_{A} f dm = 0$が真である。\n■\nCapinski. (1999). Measure, Integral and Probability: p77。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":527,"permalink":"https://freshrimpsushi.github.io/jp/posts/527/","tags":null,"title":"ルベーグ積分"},{"categories":"고전역학","contents":"単純調和運動1 バネに吊り下げられた物体の運動を考えてみよう。バネの復元力によって前後に振動する。このような運動を調和振動と呼ぶ。調和振動を表す関数である$\\sin$と$\\cos$が、昔は調和関数と呼ばれたからだ。調和振動の中でも、摩擦力や他の外力が全くなく、バネの復元力のみによって運動する場合を単純調和振動という。まず、バネによる復元力がどのように表されるかを見てみよう。$V(x)$を一次元単純調和振動子のポテンシャルエネルギーとしよう。そして、これが多項式の無限和、級数で表されると仮定しよう。すると、以下のように表せる。\n$$ V(x)=a_{0} + a_{1}x + a_2x^2+ a_{3}x^3 + \\cdots $$\nだけど、二つのポテンシャルの差だけが物理的意味を持つから、定数項は$0$としてもよい。平衡点を$0$とするのと同じだ。また、$-\\dfrac{dV}{dx}=F(x)$で復元力は$F(0)=0$だから、$V^\\prime(0)=0$となる。つまり、一次項の係数は$0$でなければならない。だから、復元力のポテンシャルは次のようになる。\n$$ V(x)=a_2x^2+a_{3}x^3+\\cdots $$\n$x$が十分に小さいとき、三次項以上の項は無視できる。最終的に復元力を求めると、以下のようになる。\n$$ F(x)=-\\dfrac{dV}{dx}=-2a_2x=-kx \\ \\ (k=2a_2) $$\nこの時、$k$を弾性係数あるいはバネ定数という。そして、$F(x)=-kx$をフックの法則Hooke’s law2という。今、復元力に関する運動方程式を解こう。$F=ma=m\\ddot{x}$で復元力は$F=-kx$なので、次の式が得られる。\n$$ \\begin{align*} \u0026amp;\u0026amp; m \\ddot{x} \u0026amp; =-kx \\\\ \\implies \u0026amp;\u0026amp; m \\ddot{x} + kx \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; \\ddot{x} + \\dfrac{k}{m}x \u0026amp;= 0 \\end{align*} $$\nこの時、${\\omega_{0}}^2 \\equiv \\dfrac{k}{m}$と置き換えよう。二乗に置き換える理由は、最終式の形を単純にするためだ。$\\omega_{0}$を系の角振動数と呼ぶ。減衰振動システム、強制振動システムと区別するために固有角振動数あるいは固有振動数とも呼ぶ。今、運動方程式は以下のようになる。\n$$ \\ddot{x} + {\\omega_{0}}^2x=0 $$\n係数が負の2次微分方程式の解法\n以下のような2次微分方程式 $$ \\dfrac{d^{2}X}{dx^{2}} = -\\alpha^{2}X $$ の解は、次のようになる。\n$$ X(x) = Ae^{i\\alpha x} + B e^{-i \\alpha x} $$\nそして、以下のような解を得る。\n$$ \\begin{align*} x(t) \u0026amp;=A_{1}e^{i\\omega_{0} t}+A_2e^{-i\\omega_{0} t} \\\\ \u0026amp;=A_{3}\\cos \\omega_{0} t+ A_{4}\\sin \\omega_{0} t \\\\ \u0026amp;=A \\cos (\\omega_{0} t + \\phi) \\end{align*} $$\nここで、$A_{1}$、$A_{2}$、$A_{3}$、$A_{4}$、$A$は、それぞれ任意の複素数あるいは実数定数だ。普通、三番目の式のように、コサインやサイン関数の形で表されることが多い。\n一緒に見る 減衰振動 強制振動 多重バネ振動 結合振動 Grant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p84-86\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nフックの法則とも呼ばれる。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":543,"permalink":"https://freshrimpsushi.github.io/jp/posts/543/","tags":null,"title":"復元力と一次元単純調和振動子"},{"categories":"측도론","contents":"定義 1 関数 function $f : E \\to \\overline{\\mathbb{R}}$ が、$E_{0} \\subset E$ の集合(ここで $m(E_{0}) = 0$)を除いて、ある性質 $P$ を持つ場合、$f$ は $E$ のほとんど至る所で $P$ の性質を持つと言われる。\n表記 確率を話す時に、ほとんど至る所ではほとんど確実にと表現され、短く書くために $$ f = g \\text{ a.e.} \\\\ P(E) = 0 \\text{ a.s.} $$ という略語を使うことがある。\n説明 簡単に言えば、零集合を除いた全ての点を\u0026rsquo;ほとんど至る所\u0026rsquo;と見ることである。この概念は正式に定義されただけで、高校で定積分を学んだ時に既に知っていたことだ。そのため、上限と下限が同じなら、その定積分は必ず $0$ であり、端点が含まれるかどうかを確率を計算する時には無視した。\n基本的な性質 [1]: $f : E \\to \\mathbb{R}$ が計測可能で、$E$ のほとんど至る所で $f = g$ ならば、$g$ は $E$ で計測可能である。 [2]: $f,g$ が $E$ で計測可能で、$E$ のほとんど至る所で $|f| , |g| \u0026lt; \\infty$ ならば、$\\alpha f + \\beta g$ は $E$ で計測可能である。 [3]: $f,g$ が $E$ で計測可能で、$E$ のほとんど至る所で $|f| , |g| \u0026lt; \\infty$ ならば、$f g$ は計測可能である。 証明 これらの性質は、一度は手で直接証明してみることが良いが、[3]を除いてはそれほど面白くなさそうだ。\n[1] $E_{0} = \\left\\{ x \\in E \\ | \\ f(x) \\ne g(x) \\right\\}$ とすると、$E_{0} \\subset E$ かつ $m(E_{0}) = 0$。任意の $c$ に対して $$ \\left\\{ x \\in E \\ | \\ g(x) \u0026gt; c \\right\\} = \\left\\{ x \\in E_{0} \\ | \\ g(x) \u0026gt; c \\right\\} \\cup \\left[ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; c \\right\\} \\cap ( E \\setminus E_{0} ) \\right] $$, 右辺の項を一つずつ見ると、$\\left\\{ x \\in E_{0} \\ | \\ g(x) \u0026gt; c \\right\\} \\subset E_{0}$ のため、 $$ \\left\\{ x \\in E_{0} \\ | \\ g(x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$, $f$ は $E$ で計測可能なので、 $$ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$, 最後に、 $$ E \\cap (\\mathbb{R} \\setminus E_{0}) = ( E \\setminus E_{0} ) \\in \\mathcal{M} $$, よって $\\left\\{ x \\in E \\ | \\ g(x) \u0026gt; c \\right\\} \\in \\mathcal{M}$ であり、$g$ は $E$ で計測可能である。\n■\n[2] $\\alpha = 0$ なら、$\\alpha f$ は計測可能で、$\\beta = 0$ なら、$\\beta g $ は計測可能である。\n$\\alpha \\ne 0$ なら、$f$ が計測可能なので、任意の $\\displaystyle {{c} \\over {\\alpha}}$ に対して $$ \\left\\{ x \\in E \\ \\left| \\ f(x) \u0026gt; {{c} \\over {\\alpha}} \\right. \\right\\} \\in \\mathcal{M} $$, ここで $\\alpha\u0026gt; 0$ なら、 $$ \\left\\{ x \\in E \\ | \\ \\alpha f(x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$, そして $\\alpha \u0026lt;0$ なら、 $$ \\left\\{ x \\in E \\ | \\ \\alpha f(x) \u0026lt; c \\right\\} \\in \\mathcal{M} $$, 従って、$\\alpha f$ は計測可能であり、同じ方法で $\\beta \\ne 0$ の時に $\\beta g$ も計測可能であるとも示せる。\n今、$(f + g)$ が計測可能、つまり $\\left\\{ x \\in E \\ | \\ f(x) + g(x) \u0026lt; c \\right\\} \\in \\mathcal{M}$ を示せば、証明は終了する。両関数は有限の値を持つため、全ての $x \\in E$ に対して $f(x) + g(x) \u0026lt; c$ を満たす $c \\in \\mathbb{R}$ が存在するであろう。再び表示すると、$f(x) \u0026lt; c - g(x)$ で、有理数の密集性により、$f(x) \u0026lt; q \u0026lt; c - g(x)$ を満たす $q \\in \\mathbb{Q}$ が存在する。そうすると、 $$ \\bigcup_{q \\in \\mathbb{Q}} \\left\\{ x \\in E \\ | \\ g(x) \u0026lt; c - q \\right\\} \\cap \\left\\{ x \\in \\ | \\ E f(x) \u0026lt; q \\right\\} = \\left\\{ x \\in E \\ | \\ f(x) + g(x) \u0026lt; c \\right\\} \\in \\mathcal{M} $$\n■\nStrategy[3]**: $fg$ が計測可能であることを示すアイデアは、$\\displaystyle fg = {{1} \\over {2}} \\left[ (f+ g)^2 - f^2 - g^2\\right]$ の等式一つに要約される。\n[3] すでに [2]で、発散しない計測可能関数の和が計測可能であることを示したので、$f^2$ が計測可能であることを示せば十分である。$f$ が計測可能なので、全ての $c$ に対して $$ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; \\sqrt{c} \\right\\} \\in \\mathcal{M} \\\\ \\left\\{ x \\in E \\ | \\ f(x) \u0026lt; - \\sqrt{c} \\right\\} \\in \\mathcal{M} $$, 従って、 $$ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; \\sqrt{c} \\right\\} \\cup \\left\\{ x \\in E \\ | \\ f(x) \u0026lt; - \\sqrt{c} \\right\\} = \\left\\{ x \\in E \\ | \\ f^2 (x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$\n■\n参照 ほとんど至る所での収束 $\\implies$ 測度による収束 ほとんど確実に収束 $\\implies$ 確率による収束 Capinski. (1999). Measure, Integral and Probability: p55.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":524,"permalink":"https://freshrimpsushi.github.io/jp/posts/524/","tags":null,"title":"測度論でのほとんど至る所とほとんど確実に"},{"categories":"측도론","contents":"定義 1 関数 $f: E \\in \\overline{ \\mathbb{R} }$ が全ての区間 $I \\subset \\overline{ \\mathbb{R} }$ に対して $$ f^{-1} (I) = \\left\\{ x \\in \\mathbb{R} \\ | \\ f(x) \\in I \\right\\} \\in \\mathcal{M} $$ であれば、$f$ を (ルベーグ) 可測(Lesbegue) Measurableと言う。\n$\\overline{ \\mathbb{R} } = \\mathbb{R} \\cup \\left\\{ - \\infty , + \\infty \\right\\}$ は、正負の無限大を含む $1$次元ユークリッド空間 の拡張実数空間を指す。 同値条件 以下の命題は互いに同値である。\n(1): $f$ はルベーグ可測関数である。 (2): すべての $r \\in \\mathbb{R}$ に対して $f^{-1} ( - \\infty , r ] \\in \\mathcal{M}$ (3): すべての $r \\in \\mathbb{R}$ に対して $f^{-1} (r, \\infty ) \\in \\mathcal{M}$ (4): すべての $r \\in \\mathbb{R}$ に対して $f^{-1} ( - \\infty , r ) \\in \\mathcal{M}$ (5): すべての $r \\in \\mathbb{R}$ に対して $f^{-1} [r, \\infty ) \\in \\mathcal{M}$ 定理 [1]: $f$ が可測であるための必要十分条件は、すべての開集合 $O$ に対して $f^{-1} ( O ) \\in \\mathcal{M}$ であることである。 [2]: $D \\subset E$、$D \\in \\mathcal{M}$ の時、$f |_{E}$ が可測であるための必要十分条件は、$f |_{D}$、$f |_{E \\setminus D}$ が可測であることである。 $f |_{X}$ は、定義域を $X$ に制限し、$f = f |_{X}$ を満たす縮小写像を意味する。 指示関数Indicator Functionとは、ある集合に属すれば $1$、そうでなければ $0$ を返す関数 $$\\displaystyle \\mathbb{1}_{E} (x) = \\chi _{E} (x) = \\begin{cases} 1 \u0026amp; , x \\in E \\\\ 0 \u0026amp; , x \\notin E \\end{cases}$$ である。この定義は $E \\in \\mathcal{M}$ という条件を省略しているため、注意が必要である。 説明 より容易な操作のために、原像の定義である $f^{-1} (-\\infty , r) = \\left\\{ x \\in E \\ | \\ f(x) \u0026lt; r \\right\\}$ をそのまま使用する方が便利である。\nルベーグ可測関数の条件の下で全ての区間 $I \\subset \\mathbb{R}$ が $f^{-1} (I) = \\left\\{ x \\in \\mathbb{R} \\ | \\ f(x) \\in I \\right\\} \\in \\mathcal{B}$ を満たす場合、それは ボレル可測Borel Measurableと呼ばれ、ボレル関数Borel Functionと呼ばれる。\n拡張実数 $\\overline{\\mathbb{R}} : = [ - \\infty, \\infty]$ は、実数全体と無限大も一点として含むものである。これまでの解析学で無限は非常に難しく恐ろしい概念であったが、今は単に征服すべき対象に過ぎない。あまり怖がらずに、高校時代の柔軟な思考を取り戻そう。\n一般的な可測空間を考えるとき、[1] は可測関数の定義にもなり得る。\n証明 [1] 閉区間の場合、開区間の両端に2点を加えるだけで十分であるため、開区間だけを考えれば十分である。\n$(\\Rightarrow)$\n開区間 $A_{k} := (a_{k}, \\infty)$、$B_{k} := (b_{k}, \\infty)$ を定義すると、$f$ が可測関数であるため、 $$f^{-1} (A_{k}), f^{-1} (B_{k}) \\in \\mathcal{M}$$ 任意の開集合 $O \\subset \\overline{ \\mathbb{R} }$ は $\\displaystyle O = \\bigcup_{k=1}^{\\infty} A_{k} \\cap B_{k}$ として表すことができるため、 $$\\displaystyle f^{-1} ( O ) = f^{-1} \\left[ \\bigcup_{k=1}^{\\infty} A_{k} \\cap B_{k} \\right] = \\bigcup_{k=1}^{\\infty} \\left[ f^{-1} (B_{k}) \\cap f^{-1} (B_{k}) \\right]$$ σ-フィールドの性質により $f^{-1} ( O ) \\in \\mathcal{M}$ である。\n$(\\Leftarrow)$ すべての開集合 $O \\subset \\overline{ \\mathbb{R} }$ に対して $f^{-1} ( O ) \\in \\mathcal{M}$ であるから、すべての開区間 $(a,b) \\subset \\overline{ \\mathbb{R} }$ に対しても $f^{-1} (a,b) \\in \\mathcal{M}$ である。\n可測関数の定義により、$f$ は可測関数である。\n■\nCapinski. (1999). Measure, Integral and Probability: p57.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":518,"permalink":"https://freshrimpsushi.github.io/jp/posts/518/","tags":null,"title":"ルベーグ可測関数"},{"categories":"통계적검정","contents":"定義 1 帰無仮説が真だにも関わらず、検定で帰無仮説を棄却する誤りを第一種の過誤と言う。 対立仮説が真だにも関わらず、検定で帰無仮説を棄却できない誤りを第二種の過誤と言う。 第一種の過誤を犯す確率の最大値を有意水準Significance Levelと言う。 仮説検定を行うために使用する統計量を検定統計量Test Statisticと言う。 帰無仮説を棄却する検定統計量の観測値の領域を棄却域Rejection Regionと言う。 説明 いくらデータが山のように積もっていて、洗練された数学的手法を適用したとしても、使えなければ意味がない。ここで「使う」とは、何らかのデータに対して統計を取り、その統計を根拠に何らかの「主張をする」ことである。そのためには、当然その統計が信頼できる必要があるし、誰が何の基準で判断するのかという問いの答えが仮説検定である。\n例 上のデータを江北高校 理科3年生の1組から15組までの中間試験平均点としよう。一目で、15組の平均が格段に高いことがわかり、標準化をするとさらにはっきりする。しかし、順位をつけたり全体の平均より高かったり低かったりするのは簡単だが、どれだけよくなったり悪くなったりするかは判断しづらい。「平均のちょっとの差」で誰が上かを言っても、どんぐりの背比べに過ぎないというものだ。明らかにある程度以上から「クラスが違う」と言えるはずだが、その程度が曖昧である。\nここでZ-scoreが自由度$14$ のt-分布に従うと考えよう。\n$t_{14}$ の確率密度関数と平均の分布を一緒に表示すると、上の図のようになる。Z-scoreの平均は$0$で、Z-scoreが$0$に近いことは、それだけ元のデータが「平均から離れていない」という意味である。一方で、$0$から遠く離れたZ-scoreの元のデータは、高いか低いかにかかわらず、平均と似ているとは言い難いだろう。\n黄色で塗られた面積は両方を合わせて$\\color{red}{0.05}$で、これはデータがその区間に現れる確率が$\\color{red}{0.05}$であることを示している。ここに属するデータは理論的に$\\color{red}{5 \\%}$の確率の非常に珍しいケースであり、たまたま出てきたとは言い難いほどの大きな差がある。こうして平均と異なりつつも、点数が高い場合、たまたまではなく、実力自体が優れていると言えるのではないだろうか。\n再び例に戻ると、15組は単にたまたま試験でよくできたとは見られないほど平均が異常に高い。ここで帰無仮説$H_{0}$が「15組の平均は3年生全体の平均と大きな違いはない」とすれば、帰無仮説を棄却することができるだろう。このとき、黄色く塗られた領域が「棄却する領域」であるため、棄却域と呼ばれる。そして、その領域を定めるときの面積が「どの程度から意味があるかの度合い」であるため、有意確率と呼ばれる。一言で仮説検定は「ただの偶然とは言い難い」という言葉を統計学的に裏付けることであり、その判断は棄却域に入ったか入らなかったかであり、その基準は有意水準である。\n少なくとも棄却域と有意水準については、その正確な定義そのものよりも、その概念をしっかりと身につけることが重要である。実際に見ることもあまりなく、使わないからと軽視すると、本当に必要で、基本としてすぐに思い出さなければならないときに思い出せなくなる。\nR コード 以下は、本投稿に使用されたRのコードである。\nset.seed(150421);\ravg\u0026lt;-signif(6*rnorm(15)+60,3); names(avg)\u0026lt;-paste0(\u0026#39;(\u0026#39;,(1:15),\u0026#39;)\u0026#39;); avg\rZ = scale(avg)[,1]; Z\rwin.graph()\rplot(0,0,type=\u0026#39;n\u0026#39;,xlim=c(-4,4),ylim=c(-0.08,0.4),xlab=\u0026#39;Z-score\\\u0026#39;,ylab=\u0026#39;t\u0026#39;,main=\u0026#39;중간고사 결과\u0026#39;)\rabline(h=0)\rlines(seq(-5,5,0.01),dt(seq(-5,5,0.01),df=14))\rpoints(x=Z,y=rep(0,15),pch=16)\rtext(x=Z,-0.05,labels=paste0(\u0026#39;(\u0026#39;,(1:15),\u0026#39;)\u0026#39;))\rarrows(Z,-0.04,Z,-0.005,length=0.1)\rwin.graph()\rplot(0,0,type=\u0026#39;n\u0026#39;,xlim=c(-4,4),ylim=c(-0.08,0.4),xlab=\u0026#39;Z-score\\\u0026#39;,ylab=\u0026#39;t\u0026#39;,main=\u0026#39;중간고사 결과\u0026#39;)\rpolygon(c(seq(qt(0.975,14),5,0.01),qt(0.975,14)),\rc(dt(seq(qt(0.975,14),5,0.01),df=14),0),\rcol=\u0026#39;yellow\u0026#39;,lty=0)\rpolygon(c(seq(-5,qt(0.025,14),0.01),qt(0.025,14)),\rc(dt(seq(-5,qt(0.025,14),0.01),df=14),0),\rcol=\u0026#39;yellow\u0026#39;,lty=0)\rabline(h=0)\rlines(seq(-5,5,0.01),dt(seq(-5,5,0.01),df=14))\rwin.graph()\rplot(0,0,type=\u0026#39;n\u0026#39;,xlim=c(-4,4),ylim=c(-0.08,0.4),xlab=\u0026#39;Z-score\\\u0026#39;,ylab=\u0026#39;t\u0026#39;,main=\u0026#39;중간고사 결과\u0026#39;)\rpolygon(c(seq(qt(0.975,14),5,0.01),qt(0.975,14)),\rc(dt(seq(qt(0.975,14),5,0.01),df=14),0),\rcol=\u0026#39;yellow\u0026#39;,lty=0)\rpolygon(c(seq(-5,qt(0.025,14),0.01),qt(0.025,14)),\rc(dt(seq(-5,qt(0.025,14),0.01),df=14),0),\rcol=\u0026#39;yellow\u0026#39;,lty=0)\rabline(h=0)\rlines(seq(-5,5,0.01),dt(seq(-5,5,0.01),df=14))\rpoints(x=Z,y=rep(0,15),pch=16)\rtext(x=Z,-0.05,labels=paste0(\u0026#39;(\u0026#39;,(1:15),\u0026#39;)\u0026#39;))\rarrows(Z,-0.04,Z,-0.005,length=0.1) 一緒に見る 棄却域の複雑な定義 第一種の過誤と第二種の過誤の違い 慶北大学校 統計学科. (2008). エクセルを利用した統計学: p200~201.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":509,"permalink":"https://freshrimpsushi.github.io/jp/posts/509/","tags":null,"title":"棄却域と有意水準"},{"categories":"확률론","contents":"定義 1 $\\mathcal{F}$が集合$\\Omega$のシグマ場だとしよう。\n可測集合 $E \\in \\mathcal{F}$を事象と呼ぶ。 $\\mathcal{F}$上の測度 $P : \\mathcal{F} \\to \\mathbb{R}$が$P(\\Omega) = 1$を満たすなら、$P$を確率と呼ぶ。 $( \\Omega, \\mathcal{F} , P )$を確率空間と呼ぶ。 説明 測度論の力を借りれば、確率論のさまざまな概念に対する数学的基盤を提供し、あいまいさを取り除くことができる。\n高校の課程や確率論、または数理統計学で、事象とは任意の試行で起こり得るケースだった。数理統計学では確率が全ての事象を集めた集合を定義域とする関数として定義されていたのと異なり、今では$\\mathcal{F}$の元を事象とし、標本空間という言葉はもはや使わない。シグマ場$\\mathcal{F}$は、試行が正確に何であるかについて心配することなく、全体集合$\\Omega$とそれに関する形式的な代数体系としてのみ定義される。したがって、誰が、何を、どう話すかによって生じうるあいまいさは存在しない。 確率は標本空間が定義域であり、$[0,1]$が値域であり、確率の加法則を満たす関数であった。測度論で再定義された確率の概念は、「任意の試行」や「ケースの数」などの言葉さえ許容しない。測度の定義を考えれば、このような確率の定義は、既に馴染み深い確率の概念を完全にカバーしつつ、厳密に一般化したものである。 「確率空間」という新しい言葉をわざわざ定義したのは、今や空間$\\Omega$自体を$P$として捉えようという意図があるからだ。基礎的な数理統計学でのように、$\\Omega = \\mathbb{R}$ならば$\\mathcal{F}$はボレルシグマ場 $\\mathcal{B}$となり、$(\\Omega , \\mathcal{F})$を論じる意味はない。あまりにも簡単すぎるということであり、言い換えれば、応用できる範囲が限られているということだ。測度論の導入によって、確率の世界は、広大な一般化の段階に入る。しっかり勉強するつもりなら、この$\\Omega$がどれだけ驚異的に与えられるかに注意が必要だ。 参照 数理統計学で定義された確率 Capinski. (1999). Measure, Integral and Probability: p46.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":498,"permalink":"https://freshrimpsushi.github.io/jp/posts/498/","tags":null,"title":"測度論で定義される確率"},{"categories":"상미분방정식","contents":"定義1 1次微分方程式が下記の条件を満たす時、分離可能と言う。\n$$ f(x)+g(y)\\dfrac{dy}{dx}=0 \\quad \\text{or} \\quad f(x)dx = -g(y)dy $$\n説明 色々な形で表現できるが、大事な点は両辺に各変数が分離されなければならないということだ。このように二つの変数を分けて解を見つける方法を変数分離法と呼ぶ。\n分離可能とは非常に良い条件で、与えられた微分方程式が分離可能なら、解を簡単に見つけることができる。一方で変数分離がされない場合は、色々な方法を通じて分離可能な形にする。つまり、1次微分方程式を解く方法は色々あるが、その本質は結局変数分離であるということだ。\n解答 $$ \\begin{align*} \u0026amp;\u0026amp; g(y)\\dfrac{dy}{dx} + f(x)\u0026amp;=0 \\\\ \\implies \u0026amp;\u0026amp; g(y)\\dfrac{dy}{dx} \u0026amp;=-f(x) \\\\ \\implies \u0026amp;\u0026amp; g(y)dy \u0026amp;=-f(x)dx \\\\ \\implies \u0026amp;\u0026amp; \\int g(y)dy\u0026amp; =-\\int f(x)dx+C \\end{align*} $$\nこの時$C$は積分定数だ。積分後、左辺を$y$に対して整理すれば良い。\n■\n例 $\\dfrac{dy}{dx}+y=0$の一般解を求めよ。\n$$ \\begin{align*} \u0026amp;\u0026amp;\\dfrac{dy}{dx}\u0026amp; =-y \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{y}dy\u0026amp;=-dx \\\\ \\implies \u0026amp;\u0026amp; \\int \\dfrac{1}{y} dy \u0026amp;=-\\int dx \\\\ \\implies \u0026amp;\u0026amp;\\ln y \u0026amp;=-x+C \\\\ \\implies \u0026amp;\u0026amp; y\u0026amp;=e^{-x+C}=e^{-x}e^C=Ce^{-x} \\end{align*} $$ 初期値が$y(0)=y_{0}$ならば$y(0)=C=y_{0}$を得るので\n$$ y(x)=y_{0}e^{-x} $$\n■\n核の放射性崩壊 放射性核が単位時間当たりに崩壊する個数は核の個数$N$に比例する。\n$$ \\dfrac{dN}{dt}=-\\lambda N $$\nここで$\\lambda$は崩壊定数といる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\dfrac{dN}{dt} \u0026amp;=-\\lambda N \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{N}dN\u0026amp;=-\\lambda dt \\\\ \\implies \u0026amp;\u0026amp; \\int \\dfrac{1}{N}dN \u0026amp;=-\\int \\lambda dt \\\\ \\implies \u0026amp;\u0026amp; \\ln N \u0026amp;=-\\lambda t+C \\\\ \\implies \u0026amp;\u0026amp; N\u0026amp;=Ce^{-\\lambda t} \\end{align*} $$\n初期値が$N(0)=N_{0}$ならば$N(0)=C=N_{0}$を得るので\n$$ N(t)=N_{0}e^{-\\lambda t} $$\n■\nWilliam E. Boyce, Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), p33-37\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":503,"permalink":"https://freshrimpsushi.github.io/jp/posts/503/","tags":null,"title":"分離可能な一階微分方程式"},{"categories":"측도론","contents":"定義 1 $E \\in \\mathcal{M}$に関して、関数$m : \\mathcal{M} \\to [0,\\infty]$を$m(E) := m^{ \\ast } (E)$のように定義しよう。$m$を**(ルベーグ)測度**という。\n$\\mathcal{M}$は$X = \\mathbb{R}$の可測集合の集合であるシグマ代数だ。 $m^{\\ast}$は外測度だ。 説明 外測度は$m^{ \\ast } : \\mathscr{P}( \\mathbb{R} ) \\to [0, \\infty]$によってきれいに定義されたが、長さの一般化としては物足りなかった。その代わり実数のシグマ-フィールドに定義域を制限することで、理想的な\u0026rsquo;長さの一般化\u0026rsquo;を完成させた。これはカラテオドリ条件を満たすために条件的に一歩下がると見ることができる。\nもちろん、一般的な測度と比較してみると、$X = \\mathbb{R}$での特別な例である。\n基本的な性質 $A, B, E \\in \\mathcal{M}$とすべての$n \\in \\mathbb{N}$に対して$A_{n}, B_{n}, \\in \\mathcal{M}$とする。測度は以下の性質を持つ。\n[1]: $$A \\subset B \\implies m(A) \\le m(B)$$ [2]: $A \\subset B$の場合、$$m(A) \u0026lt; \\infty \\implies m(B \\setminus A) = m(B) - m(A)$$ [3]: $$t \\in \\mathbb{R} \\implies m(E) = m(E + t)$$ [4]: $$m(A \\triangle B) = 0 \\implies B \\in \\mathcal{M} \\\\ m(A) = m(B)$$ [5]: すべての$\\varepsilon \u0026gt; 0, A \\subset \\mathbb{R}$に対して、以下を満たす開集合の$O$が存在する。 $$ A \\subset O \\\\ m(O) \\le m^{ \\ast }(A) + \\varepsilon $$ [6]: すべての$A \\subset \\mathbb{R}$に対して、以下を満たす開集合の数列$\\left\\{ O_{n} \\right\\}$が存在する。 $$ A \\subset \\bigcap_{n} O_{n} \\\\ m \\left( \\bigcap_{n} O_{n} \\right) = m^{ \\ast }(A) $$ [7]: $$\\displaystyle A_{n} \\subset A_{n+1} \\implies m \\left( \\bigcup_{n=1}^{\\infty} A_{n} \\right) = \\lim_{n \\to \\infty} m (A_{n})$$ [8]: $A_{n+1} \\subset A_{n}$の場合、$$\\displaystyle m(A_{1}) \u0026lt; \\infty \\implies m \\left( \\bigcap_{n=1}^{\\infty} A_{n} \\right) = \\lim_{n \\to \\infty} m (A_{n})$$ [9]: $$\\displaystyle m \\left( \\bigsqcup_{i=1}^{n} A_{i} \\right) = \\sum_{i = 1}^{n} m (A_{i})$$ [11]: $$B_{n} \\to \\emptyset \\implies m(B_{n}) \\to 0$$ $A \\triangle B = ( A \\setminus B ) \\cup ( B \\setminus A )$が真である。 証明 [1] $m = m^{ \\ast } |_{\\mathcal{M}}$より、外測度の性質から自然に導かれる。\n■\n[2] まず、$(B \\setminus A) \\in \\mathcal{M}$を証明する必要がある。$(B \\setminus A) = B \\cap (\\mathbb{R} \\setminus A) = B \\cap A^{c}$であり、$A \\in \\mathcal{M}$であるから$A^{c} \\in \\mathcal{M}$だ。したがって、$(B \\setminus A) \\in \\mathcal{M}$であり、$(B \\setminus A ) \\cap A = \\emptyset$かつ$(B \\setminus A ) \\cup A = B$であるから$m(B \\setminus A ) + m(A) = m(B)$となる。仮定から$m(A) \u0026lt; \\infty$であったので、両辺すると$m(B \\setminus A) = m(B) - m(A)$を得る。\n■\n[3] $m = m^{ \\ast } |_{\\mathcal{M}}$より、外測度の性質から自然に導かれる。\n■\n[4] $B = (A \\cap B) \\cup (B \\setminus A) = A \\setminus (A \\setminus B) \\cup (B \\setminus A)$より、$B \\in \\mathcal{M}$である。一方で$m(A \\triangle B) = 0$より、$m(A \\setminus B) = 0$かつ$m(B \\setminus A) = 0$だ。したがって、 $$ m(B) = m( B \\setminus A) + m(B \\cap A) = m( A \\setminus B) + m(A \\cap B) = m(A) $$\n■\n[7] $B_{n} :=A_{n} \\setminus A_{n-1}$とすると、$i \\ne j$に対して$B_{i} \\cap B_{j} = \\emptyset$かつ$\\displaystyle \\bigcup_{n=1}^{\\infty} A_{n} = \\bigsqcup_{n=1}^{\\infty} B_{n}$である。したがって、 $$ m \\left( \\bigcup_{n=1}^{\\infty} A_{n} \\right) = m \\left( \\bigsqcup_{n=1}^{\\infty} B_{n} \\right) = \\sum_{n=1}^{\\infty} m(B_{n}) = \\lim_{n \\to \\infty} \\sum_{k=1}^{n} m(B_{k}) = \\lim_{n \\to \\infty} m \\left( \\bigsqcup_{k=1}^{n} B_{k} \\right) = \\lim_{n \\to \\infty} m \\left( A_{n} \\right) $$\n■\n[8] $(A_{1} \\setminus A_{n} ) \\subset (A_{1} \\setminus A_{n+1} )$より、**[7]**によって $$ m \\left( \\bigcup_{n=1}^{\\infty} ( A_{1} \\setminus A_{n} ) \\right) = \\lim_{ n \\to \\infty} m (A_{1} \\setminus A_{n}) $$ $m(A_{n}) \u0026lt; \\infty$より、[3]によって $$ m (A_{1} \\setminus A_{n}) = m(A_{1}) - m(A_{n}) $$ 一方、$\\displaystyle \\bigcup_{n=1}^{\\infty} (A_{1} \\setminus A_{n}) = A_{1} \\setminus \\bigcap_{n=1}^{\\infty} A_{n}$より、 $$ m \\left( \\bigcup_{n=1}^{\\infty} (A_{1} \\setminus A_{n}) \\right) = m \\left( A_{1} \\right) - m \\left( \\bigcap_{n=1}^{\\infty} A_{n} \\right) $$ 整理すると、 $$ m \\left( \\bigcup_{n=1}^{\\infty} (A_{1} \\setminus A_{n}) \\right) = m(A_{1}) - \\lim_{n \\to \\infty} m(A_{n}) = m \\left( A_{1} \\right) - m \\left( \\bigcap_{n=1}^{\\infty} A_{n} \\right) $$ したがって$\\displaystyle \\lim_{n \\to \\infty} m(A_{n}) = m \\left( \\bigcap_{n=1}^{\\infty} A_{n} \\right)$\n■\n一般化 測度の一般化 Capinski. (1999)。Measure, Integral and Probability: p35.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":494,"permalink":"https://freshrimpsushi.github.io/jp/posts/494/","tags":null,"title":"ルベーグ測度"},{"categories":"측도론","contents":"定義 集合$X \\ne \\emptyset$に対して、以下の条件を満たす$\\mathcal{E} \\subset \\mathscr{P} (X)$を$X$上のシグマ代数またはシグマ場という。集合$X$とシグマ場$\\mathcal{E}$の順序対$(X , \\mathcal{E})$を可測空間と呼ぶ。\n(i): $\\emptyset \\in \\mathcal{E}$ (ii): $E \\in \\mathcal{E} \\implies E^{c} \\in \\mathcal{E}$ (iii): $\\displaystyle \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{E} \\implies \\bigcup_{n=1}^{\\infty} E_{n} \\in \\mathcal{E}$ (iv): $\\displaystyle \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{E} \\implies \\bigcap_{n=1}^{\\infty} E_{n} \\in \\mathcal{E}$ 説明 ある空間$X$に対してシグマ場$\\mathcal{E}$が与えられた場合、$(X , \\mathcal{E})$を可測空間と呼ぶ。測度$\\mu$が与えられた場合は測度空間と呼び、特に測度$\\mu$が確率である場合は確率空間と呼ぶ。\n同じ概念だが、数学ではシグマ代数、統計学ではシグマ場と呼ばれることに注意。\nカラテオドリの条件: $E \\subset \\mathbb{R}$が$A \\subset \\mathbb{R}$に対して$m^{ \\ast }(A) = m^{ \\ast } ( A \\cap E ) + m^{ \\ast } ( A \\cap E^{c} )$を満たす場合、$E$を可測集合と呼び、$E \\in \\mathcal{M}$と表記する。\n\u0026lsquo;可測集合\u0026rsquo;は、文字通り測ることができる集合を意味する。外測度の単調性から $$m^{ \\ast }(A) \\le m^{ \\ast } ( A \\cap E ) + m^{ \\ast } ( A \\cap E^{c} )$$ は自明であるので、ある集合が可測かどうかを確認することは、 $$m^{ \\ast }(A) \\ge m^{ \\ast } ( A \\cap E ) + m^{ \\ast } ( A \\cap E^{c} )$$ であるかを確認することと同じである。\n可測集合の集合のシグマ代数 上記の定義から、$X = \\mathbb{R}$の可測集合の集合である$\\mathcal{M}$は、以下の性質を持つシグマ代数となる。\n$\\mathcal{M}$は、以下の性質を持つシグマ代数である。\n[1]: $$ \\emptyset \\in \\mathcal{M} $$ [2]: $$ E \\in \\mathcal{M} \\implies E^{c} \\in \\mathcal{M} $$ [3]: $$ \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{M} \\implies \\bigcup_{n=1}^{\\infty} E_{n} \\in \\mathcal{M} $$ [4]: $$ \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{M} \\implies \\bigcap_{n=1}^{\\infty} E_{n} \\in \\mathcal{M} $$ [5]: $$ \\mathcal{N} \\subset \\mathcal{M} $$ [6]: $$ \\mathcal{I} \\subset \\mathcal{M} $$ [7]: $E_{i} , E_{j} \\in \\mathcal{M}$とすると、以下が成立する。 $$ E_{i} \\cap E_{j} = \\emptyset , \\forall i \\ne j \\implies m^{ \\ast } \\left( \\bigcup_{n=1}^{\\infty} E_{n} \\right) = \\sum_{n = 1} ^{\\infty} m^{ \\ast } ( E_{n}) $$ $\\mathcal{I}$はすべての区間の集合、$\\mathcal{N}$はすべての零集合の集合である。 特に[7]は、ルベーグが夢見た「長さの一般化」に絶対に必要な性質であることに注目してください。\n","id":490,"permalink":"https://freshrimpsushi.github.io/jp/posts/490/","tags":null,"title":"シグマ代数と可測空間"},{"categories":"위상수학","contents":"定義 1 位相空間 $\\left( X, \\mathscr{T} \\right)$ に対して $A \\subset X$ としよう。\n$X$ の開集合からなる集合 $\\mathscr{O} \\subset \\mathscr{T}$ が次を満たす場合、$\\mathscr{O}$ を $A$ の開被覆Open Coveringという。 $$ A \\subset \\bigcup_{O \\in \\mathscr{O}} O $$ $\\mathscr{O} ' \\subset \\mathscr{O}$ である $\\mathscr{O} ' $ を $\\mathscr{O}$ の部分被覆Subcoverという。特に $\\mathscr{O} ' $ の基数が自然数である場合は、有限部分被覆Finite Subcoverという。 $X$ の全ての開被覆が有限部分被覆を持つ場合、$X$ はコンパクトであるという。言い換えると、全ての開被覆 $\\mathscr{O}$ に対して、次を満たす有限集合 $\\mathscr{O} ' = \\left\\{ O_{1} , \\cdots , O_{n} \\right\\} \\subset \\mathscr{O}$ が存在する場合、$X$ はコンパクトである。 $$ X = \\bigcup_{i=1}^{n} O_{i} $$ $A$ が $X$ の部分空間としてコンパクトである場合、$A$ をコンパクトであるという。 位相空間 $X$ とする。部分集合 $K \\subset X$ の閉包 $\\overline{K}$ がコンパクトである場合、$K$ はプリコンパクトである、あるいは相対的にコンパクトrelatively compactであると言う。 説明 コンパクト 解析概論でコンパクトという条件がどれほど有用であったかを考えれば、その一般化を追求することは当然と言えるだろう。一般化されると、言葉は少し難しくなるが、本質的な部分は変わらない。\n実際に、コンパクトはさまざまな理論で非常に重要な応用を持つ。ある集合がコンパクトであるということは、有限の部分に分割して考えることができるということであり、厳密性が要求される証明では良い条件となる。逆に言えば、ある定理を証明する際に現れる集合 $A$ が本当にコンパクトであるかを示すことが鍵となる場合が多い。\nプリコンパクト プリコンパクトは、$K$ 自体はコンパクトではないが、$K$ に閉包を取るとコンパクトになるという点で、「まだコンパクトではないが、すぐにコンパクトになり得る」という概念をよく表している。距離空間では完全有界空間とも呼ばれ、別名相対的コンパクトは、閉じられた性質が相対的なものから来ていることを表す表現である。$K$ を $X$ の部分空間ではなく、それ自体で全体空間とした場合、$K$ は $K$ で閉じているため、$K = \\overline{K}$ であり、したがって$\\overline{K}$ がコンパクトであるということは、$K$ が（相対的に）コンパクトであるということになる。\n一方、数列によるプリコンパクトの定義も可能である。その定義は次のようである：\n$K \\subset X$ がプリコンパクトであるとは、$K$ で定義された全ての数列 $\\left\\{ x_{n} \\right\\} \\subset K$ に対して、$x \\in X$ に収束する部分数列 $\\left\\{ x_{n '} \\right\\} \\subset \\left\\{ x_{n} \\right\\}$ が存在することである。\n数式で再表現すると次のようになる：\n$$ K : \\text{precompact} \\iff \\forall \\left\\{ x_{n} \\right\\} \\subset K, \\exists \\left\\{ x_{n '} \\right\\} \\subset \\left\\{ x_{n} \\right\\} : x_{n '} \\to x \\in X \\text{ as } n \\to \\infty $$\n特に条件で $x \\in X$ ではなく $x \\in K$ の場合、$K$ を点列コンパクトSequentially Compactと呼ぶ。\n定理 [1]: $A$ がコンパクトであることは、$A$ の全ての開被覆が有限部分被覆を持つことと同値である。 [2]: コンパクト集合 $K$ の部分集合 $F$ が閉集合である場合、$F$ はコンパクト集合である。 [3]: $X$ がコンパクトであることは、$X$ の閉集合のみを含む全ての集合族が有限交差性を持ち、それを単に交差させても空集合にならないことと同値である。 証明 [1] $\\Gamma$ は指標集合である。\n$( \\implies )$\n$A \\subset X$ がコンパクトであり、$\\mathscr{U} := \\left\\{ U_{\\alpha} : \\alpha \\in \\Gamma \\right\\}$ が $A$ の開被覆であるとする。すると、$U_{\\alpha} \\cap A$ は $X$ の部分空間 $A$ で開集合であり、$\\mathscr{O} := \\left\\{ U_{\\alpha} \\cap A : U_{\\alpha} \\in \\mathscr{U} \\right\\}$ は $A$ の開被覆となる。$A$ はコンパクトであるため、$\\displaystyle A \\subset \\bigcup_{i=1}^{n} \\left( U_{\\alpha_{i}} \\cap A \\right)$ を満たす $\\alpha_{1} , \\cdots , \\alpha_{n} \\in \\Gamma$ が存在する。したがって、$\\left\\{ U_{\\alpha_{1}} , \\cdots , U_{\\alpha_{n}} \\right\\}$ が $\\mathscr{U}$ の有限部分被覆として存在することが確認できる。\n$( \\impliedby )$\n$A$ で開集合からなる開被覆 $\\mathscr{O} := \\left\\{ O_{\\alpha} : O_{\\alpha} \\text{ is open in } A, \\alpha \\in \\Gamma \\right\\}$ を考える。$O_{\\alpha}$ が $A$ で開集合であるため、各 $\\alpha \\in \\Gamma$ に対して $U_{\\alpha} \\cap A = O_{\\alpha}$ を満たす開集合 $U_{\\alpha}$ が存在する。これらの集合 $\\mathscr{U} := \\left\\{ U_{\\alpha} : \\alpha \\in \\Gamma \\right\\}$ は $A$ の開被覆である。全ての開被覆が有限部分被覆 $\\left\\{ U_{\\alpha_{1}} , \\cdots , U_{\\alpha_{n}} \\right\\}$ を持つという仮定から、$\\left\\{ O_{\\alpha_{1}} , \\cdots , O_{\\alpha_{n}} \\right\\}$ は $\\mathscr{O}$ の有限部分被覆となる。\n■\n[2] $$ F \\subset K \\subset X $$ $F$ が $X$ で閉集合であり、$K$ がコンパクトであるとする。$F$ は閉集合であるため、$F^{c}$ は $X$ で開集合であり、$K \\subset F^{c}$ であるため、$F^{c} \\cup \\left\\{ U_{\\alpha} \\right\\}$ は $K$ の開被覆の一つとなり、$K$ がコンパクトであるため、$F \\subset K \\subset \\Phi$ を満たす $F^{c}\\cup \\left\\{ U_{\\alpha} \\right\\}$ の有限部分被覆 $\\Phi$ が存在する。\nもし $F^{c}\\notin \\Phi$ であれば、$\\Phi$ は $\\left\\{ U_{\\alpha} \\right\\}$ の有限部分被覆となるため、$F$ はコンパクトである。 もし $F^{c}\\in \\Phi$ であれば、$\\Phi \\setminus \\left\\{ F^{c} \\right\\}$ が $\\left\\{ U_{\\alpha} \\right\\}$ の有限部分被覆となるため、$F$ はコンパクトである。 どちらの場合も、$F$ はコンパクトであるため、$F$ はコンパクトである。\n■\n[3] 戦略：言葉が非常に複雑なので、言葉を理解することが鍵である。$\\mathscr{C}$ が有限交差性を持つとしても、$\\displaystyle \\bigcap_{C \\in \\mathscr{C}} C \\ne \\emptyset$ が保証されるわけではなく、コンパクトという条件が必要である。一方で、コンパクトの定義では開集合の和集合を考慮しており、この定理では閉集合の交差を考慮している点に注意する必要がある。これらの考察から、有限交差性がどのようにコンパクトと関連しているのかを感じ取り、証明に入る必要がある。\n$\\Gamma$ は指標集合である。\n有限交差性: $X$ の部分集合からなる集合族 $\\mathscr{A} \\subset \\mathscr{P}(X)$ が有限交差性(f.i.p, finite intersection property)を持つとは、$\\mathscr{A}$ の全ての有限部分集合 $A \\subset \\mathscr{A}$ が交差を取ったときに空集合でないことである。数式で表すと、次のようになる。 $$ \\forall A \\subset \\mathscr{A}, \\bigcap_{a \\in A} a \\ne \\emptyset $$\n$( \\implies )$\n$X$ がコンパクトであり、$\\mathscr{C} := \\left\\{ C_{\\alpha} : C_{\\alpha} \\text{ is closed in } X, \\alpha \\in \\Gamma \\right\\}$ が有限交差性を持つとする。ここで、$\\displaystyle \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} = \\emptyset$ と仮定し、$\\mathscr{O} := \\left\\{ X \\setminus C_{\\alpha} : C_{\\alpha} \\in \\mathscr{C} \\right\\}$ を選ぶ。すると、 $$ \\begin{align*} \\bigcup_{\\alpha \\in \\Gamma} ( X \\setminus C_{\\alpha}) =\u0026amp; X \\setminus \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} \\\\ =\u0026amp; X \\setminus \\emptyset \\\\ =\u0026amp; X \\end{align*} $$ となるため、$\\mathscr{O}$ は $X$ の開被覆となる。$X$ がコンパクトであるため、$\\mathscr{O}$ は有限部分被覆 $\\displaystyle \\left\\{ (X \\setminus C_{\\alpha_{1}}) , \\cdots ,(X \\setminus C_{\\alpha_{n}}) \\right\\}$ を持つ。これは、つまり $$ X = \\bigcup_{i=1}^{n} ( X \\setminus C_{\\alpha_{i}}) = X \\setminus \\bigcap_{i=1}^{n} C_{\\alpha_{i}} $$ となり、$\\displaystyle \\bigcap_{i=1}^{n} C_{\\alpha_{i}} = \\emptyset$ であることを意味する。これは、$\\mathscr{C}$ が有限交差性を持つという仮定に矛盾する。したがって、$\\displaystyle \\bigcap_{C \\in \\mathscr{C}} C \\ne \\emptyset$ でなければならない。\n$( \\impliedby )$\n$X$ の開被覆 $\\mathscr{O} := \\left\\{ O_{\\alpha} : O_{\\alpha} \\text{ is open in } A, \\alpha \\in \\Gamma \\right\\}$ と $\\mathscr{C} := \\left\\{ X \\setminus O_{\\alpha} : O_{\\alpha} \\in \\mathscr{O} \\right\\}$ を考える。 $$ \\begin{align*} \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} \u0026amp;= \\bigcap_{\\alpha \\in \\Gamma} ( X \\setminus O_{\\alpha}) \\\\ =\u0026amp; X \\setminus \\bigcup_{\\alpha \\in \\Gamma} O_{\\alpha} \\\\ =\u0026amp; X \\setminus X \\\\ =\u0026amp; \\emptyset \\end{align*} $$ となるため、対偶により、$\\mathscr{C}$ は有限交差性を持たない。これは、言い換えると、$\\displaystyle \\bigcap_{i=1}^{n} C_{\\alpha_{i}} = \\emptyset$ を満たす $C_{\\alpha_{1}} , \\cdots , C_{\\alpha_{n}} \\in \\mathscr{C}$ が存在することを意味する。すると、\n$$ \\begin{align*} X \\setminus \\bigcup_{i=1}^{n} O_{i} =\u0026amp; X \\setminus \\bigcup_{i=1}^{n} (X \\setminus C_{i}) \\\\ =\u0026amp; X \\setminus \\left( X \\setminus \\bigcap_{i=1}^{n} C_{i} \\right) \\\\ =\u0026amp; \\bigcap_{i=1}^{n} C_{i} \\\\ =\u0026amp; \\emptyset \\end{align*} $$ となるため、$\\displaystyle X = \\bigcup_{i=1}^{n} O_{i}$ である。つまり、開被覆 $\\mathscr{O}$ に対して有限部分被覆が存在するため、コンパクトである。\n■\n関連項目 距離空間におけるコンパクト Munkres. (2000). Topology(2nd Edition): p164.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":489,"permalink":"https://freshrimpsushi.github.io/jp/posts/489/","tags":null,"title":"位相空間におけるコンパクトとプレコンパクトとは？"},{"categories":"상미분방정식","contents":"説明 微分方程式を分類する基準はいくつかある。大きく常微分方程式か偏微分方程式かで分けられる。その次に係数と次数、線形/非線形でさらに細かく分類することができる。微分方程式を分類する理由は明らかに微分方程式を解くためである。微分方程式の分類によって解法も異なる。\n常微分方程式と偏微分方程式 常微分方程式は一つかそれ以上の従属変数を一つの独立変数で微分した導関数のみを含む微分方程式を指す。通常ODEOrdinary Differential Equationと略される。\n$$ \\begin{align*} \\dfrac{dy}{dx}\u0026amp;=2y-1 \\\\ \\dfrac{d^2y}{dx^2}+3\\dfrac{dy}{dx}-2y \u0026amp;=0 \\\\ \\dfrac{dy}{dt}+\\dfrac{dx}{dt}\u0026amp;=2c \\end{align*} $$\n偏微分方程式は一つかそれ以上の従属変数を二つ以上の独立変数で微分した導関数を含む微分方程式である。簡単に言うと偏導関数を含む微分方程式だ。PDEPartial Differential Equationと略され、$u=u(x,t)$とすると、\n$$ \\begin{align*} \\dfrac{\\partial u}{\\partial x}-\\dfrac{\\partial u }{\\partial t} =0 \\\\ \\dfrac{\\partial^2 u }{\\partial x^2}=\\dfrac{1}{c^2} \\dfrac{\\partial^2 u}{\\partial t^2} \\end{align*} $$\n係数と次数 係数orderと次数degreeを区別せずに次数と呼ぶことが多いが、明確に異なる。意味がまったく変わってくるので、用語を正しく使う必要がある。2階導関数を2次導関数と言わないことを覚えておこう。英語で言うと最も確実で、実際にオーダーという言葉をよく使う。\n微分方程式で係数は最大の微分回数を指す。赤色で表示された項が微分方程式の係数を決める。\n$$ \\begin{align} x^2 {\\color{red} \\dfrac{dy}{dx} }+y\u0026amp;=0 \\label{eq1} \\\\ {\\color{red}\\dfrac{d^2u}{dx^2}}+2 \\left( \\dfrac{dy}{dx} \\right) ^3\u0026amp;=5x \\label{eq2} \\end{align} $$\n$(1)$は1階微分方程式、$(2)$は2階微分方程式である。微分方程式で次数は最高係数項の累乗回数を指す。赤色で表示された項が微分方程式の次数を決める。\n$$ \\begin{align} x^2 \\left(\\dfrac{d^{\\color{blue}1}y}{dx^{\\color{blue}1}} \\right)^{\\color{red}1} + y \u0026amp; =0 \\label{eq3} \\\\ \\left( \\dfrac{d^{\\color{blue}3}y}{dx^{\\color{blue}3}} \\right)^{\\color{red}2} + x^2\\dfrac{dy}{dx}\u0026amp;=0 \\label{eq4} \\\\ \\left( \\dfrac{d^{\\color{blue}2}y}{dx^{\\color{blue}2}} \\right)^{\\color{red}3} + \\left( \\dfrac{dy}{dx} \\right)^5+x^2y\u0026amp;=0 \\label{eq5} \\end{align} $$\n$(3)$は$\\color{blue}1$階$\\color{red}1$次、$(4)$は$\\color{blue}3$階$\\color{red}2$次、$(5)$は$\\color{blue}2$階$\\color{red}3$次の微分式だ。\n線形と非線形 以下のような形の微分方程式があるとき、$\\mathrm{n}$階線形微分方程式と言う。\n$$ a_{n}(x)\\dfrac{d^ny}{dx^n}+a_{n-1}(x)\\dfrac{d^{n-1}y}{dx^{n-1}}+ \\cdots + a_{1}(x)\\dfrac{dy}{dx}+a_{0}(x)y=f(x) $$\n各項の係数が独立変数$x$にのみ依存していれば線形である。つまり、上のような微分方程式を$L(y)$という関数で表現でき、$L$が線形関数であれば、$L$で表される微分方程式を線形と言う。\n$$ x \\dfrac{dy}{dx} $$\n係数が従属変数$y$に依存する項が一つでもあれば非線形である。\n$$ L(y) = y\\dfrac{dy}{dx}\\\\ \\implies L(y+Y) = (y+Y)\\left( \\dfrac{dy}{dx} + \\dfrac{dY}{dx} \\right) \\ne y\\dfrac{dy}{dx} + Y\\dfrac{dY}{dx}=L(y) + L(Y) $$\n同次と非同次 同次（非同次）と言うこともあるが、同次（非同次）という言葉をより多く使う。次のような微分方程式が与えられたとする。 $$ a_{n}(x)\\dfrac{d^ny}{dx^n}+a_{n-1}(x)\\dfrac{d^{n-1}y}{dx^{n-1}}+ \\cdots + a_{1}(x)\\dfrac{dy}{dx}+a_{0}(x)y=f(x) $$\n$f(x)=0$であれば同次homogeneous、$f(x) \\ne 0$であれば非同次nonhomogenous, inhomogenousと言う。当然のことながら、同次微分方程式の方がずっと解きやすい。\n","id":483,"permalink":"https://freshrimpsushi.github.io/jp/posts/483/","tags":null,"title":"微分方程式の分類"},{"categories":"위상수학","contents":"定義 1 $X$ を位相空間と呼び、$C \\subset \\mathbb{R}^{n}$ だとしよう。\n連続関数 $p : [0,1] \\to X$ を 始点 $p(0)$ から 終点 $p(1)$ への 経路 とする。$\\overline{p}(t) = p(1-t)$ を $p$ の 逆経路 という。 すべての $a,b \\in X$ に対して、$p(0) = a$ と $p(1) = b$ を満たす経路 $p$ が存在する場合、$X$を 経路連結 空間という。 すべての $a,b \\in C$ と $t \\in [0,1]$ に対して、$(1-t) a + t b \\in C$ ならば 凸 であるという。 $p(0) = p(1)$ ならば 閉経路 という。 説明 簡単に言うと、ある空間の任意の二点を結ぶ経路が常に存在する場合、それを経路連結と呼ぶ。\n$X$ は非連結空間である $\\iff$ 連続関数 $f : X \\to \\left\\{ a, b \\right\\}$ が存在する。 $X$ は経路連結空間である $\\iff$ 連続関数 $p : [0,1] \\to X$ が存在する。 非連結空間と経路連結空間は、ある連続関数が特定の条件を満たしているかによって区別すると考えると理解しやすい。もちろん、上記の命題には多くの詳細が省略されているので、そのまま受け取らない方が良い。\n凸性 凸性の概念は、ユークリッド空間の部分集合でのみ定義される必要はなく、ベクトル空間の部分空間でも定義される。幾何学的に言えば、凸であるとは$C$ の任意の二点を結ぶ直線が常に$C$ 内に存在することである。\n例えば、上記の二つの図形を見ると、青の円は任意の二点を直線で結ぶことが可能なので凸である。オレンジの図形は内部に$a$ と $b$ を結ぶ直線が存在しないため、凸ではない。\n連結性 一方で、経路連結空間の定義をよく見ると、実質的に連結空間と変わらないように見える。実際に、次の定理はそれほど難しくなく証明でき、これら二つを区別することは無意味に思えるかもしれない。しかし、連結と経路連結は確かに異なる概念であり、定理の逆が成り立たない反例が存在するためだ。逆が成り立つケースには、$\\mathbb{R}$ の凸部分空間や開連結部分空間がある。\n定理: 経路連結空間であれば、連結空間である。 証明 経路連結空間 $X$ について $X = \\emptyset$ であれば、$X$ は連結空間である。$X \\ne \\emptyset$ であれば、ある点 $a \\in X$ を選べる。すると、任意の $x \\in X$ に対して、$p_{x} (0) = a$、$p_{x} (1) = x$ を満たす連続関数 $p_{x} : [0,1] \\to X$ が存在する。\n連結空間の連続像は連結空間 連結空間 $X$ に対して、$f : X \\to Y$ が連続関数であれば、$f(X)$ は連結空間である。\n$[0,1]$ が連結であるので、$p_{x} ( [0,1] )$ は連結であり、$\\displaystyle a \\in \\bigcap_{x \\in X} p_{x} ( [0, 1] )$ なので $\\displaystyle \\bigcap_{x \\in X} p_{x} ( [0, 1] ) \\ne \\emptyset$\n(3) $X$ の連結部分空間の集合 $\\left\\{ A_{\\alpha} \\ | \\ \\alpha \\in \\forall \\right\\}$ に対して、$\\displaystyle \\bigcap_{\\alpha \\in \\forall} A_{\\alpha} \\ne \\emptyset$ であれば、$\\displaystyle \\bigcup_{\\alpha \\in \\forall} A_{\\alpha}$ は連結空間である。\nしたがって、$\\displaystyle X = \\bigcup_{x \\in X} p_{x} ( [0, 1] )$ は連結空間である。\n■\n参照 ベクトル空間で一般的に定義された凸セット Munkres. (2000). Topology(2nd Edition): p155.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":478,"permalink":"https://freshrimpsushi.github.io/jp/posts/478/","tags":null,"title":"位相数学におけるパス連結성"},{"categories":"추상대수","contents":"定義 1 集合$G$とその部分群$H$で$aH = \\left\\{ ah \\ | \\ h \\in H \\right\\}$が左余剰類Left Coset、$Ha = \\left\\{ ha \\ | \\ h \\in H \\right\\}$が右余剰類Right Cosetと言われる。ここで$a \\in G$、$aH, Ha \\subset G$だ。 $H \\leqslant G$の左(右)余剰類の数を$(G : H)$と書き、$G$における$H$の指数Indexと言う。 $H$が$G$の部分群であり、全ての$g \\in G$に対して$gH = Hg$が成り立つなら、$H$を$G$の正規部分群Normal Subgroupと言い、$H \\triangleleft G$で書く。 $H = \\left\\{ e \\right\\}$や$H = G$以外の$H \\triangleleft G$を持たない$G$を単純Simpleと言う。つまり、$G$が単純だとは、$\\left\\{ e \\right\\}$と$G$自体だけを正規部分群として持つことを意味する。 説明 余剰類 余剰類のアイデアは、必然的に代数学をより高い次元に導く。\n例えば、$3$の倍数だけを集めた集合$3 \\mathbb{Z} = \\left\\{ \\cdots, -6, -3, 0 , 3, 6 , \\cdots\\right\\}$は群であり、特に$\\mathbb{Z}$が可換群であるため、$3 \\mathbb{Z} \\triangleleft \\mathbb{Z}$が成立する。\nここに整数を足すと考えてみると $$ \\begin{align*} 1 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -5, -2, 1 , 4, 7 , \\cdots\\right\\} \\\\ 2 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -4, -1, 2 , 5, 8 , \\cdots\\right\\} \\\\ 3 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -3, 0 , 3, 6 , 9 , \\cdots\\right\\} = 3 \\mathbb{Z} \\\\ 4 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -2, 1 , 4, 7 , 10 , \\cdots\\right\\} = 1 + 3 \\mathbb{Z} \\\\ 5 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -1, 2 , 5, 8 , 11 , \\cdots\\right\\} = 2 + 3 \\mathbb{Z} \\end{align*} $$ まるで$\\pmod{3}$で整数を足すのと似た形になる。\nつまり、$\\mathbb{Z}_{3} : = \\left\\{ 3 \\mathbb{Z} , 1 + 3 \\mathbb{Z} , 2 + 3 \\mathbb{Z}\\right\\}$のように集合を要素として持つ新しい群を考えることができるということだ。このような新たに作られる群を商群と言う。初めて勉強するときはかなり理解しづらい概念だが、通常は余剰類に対する誤解がその原因だ。取るに足らないように見えて使われないように見えるが、実際には手で書きながら余剰類をしっかり理解することが後の部分を楽にする。\n指数 左と書かれているのも右と書かれているのも別に区別する必要がないからだる。本来、指数は左余剰類の数として定義されるが、実際には右余剰類と一対一の対応が存在するので、右余剰類の数として定義しても良い。2\n正規性 $gH$と$Hg$が群になるか、$gH = Hg$が成り立つかを確認するのは、一見教科書で学んだ連続の定義を思い出させる。正規Normalという言葉がついているだけに、かなり強力な条件であり、多くの便利な性質が推測できるだろう。\n定義からすぐにわかる事実としては、$G$の単位元$e$に対して$\\left\\{ e \\right\\} \\triangleleft G$がある。少し考えればわかるのは、可換群$G$に対して$H \\leqslant G$ならば、$H \\triangleleft G$程度があることだ。\n単純性 例えば、素数$p$について、$\\mathbb{Z}_{p}$は自明群や自分自身以外に部分群を持たないため、単純群となる。\n参考文献 余剰類の性質 Fraleigh. (2003). 『現代の抽象代数学』(第7版): p97, 101, 132, 149.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). 『現代の抽象代数学』(第7版): p103\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":469,"permalink":"https://freshrimpsushi.github.io/jp/posts/469/","tags":null,"title":"抽象代数学における剰余類と正規部分群"},{"categories":"위상수학","contents":"定義 1 位相空間$X$で、$A \\cap B = \\emptyset$と$A \\cup B = X$を満たす開集合$A \\ne \\emptyset$、$B \\ne \\emptyset$が存在する場合、$X$を非連結Disconnected空間という。非連結でない場合を連結Connected空間という。\n定理 説明 非連結であることの定義はかなり直感的で、その否定、つまり連結もまた容易に受け入れられるだろう。グラフ理論でも同様に連結を定義する。\n例えば、ユークリッド空間$( \\mathbb{R} , d )$を考えると、どのような開区間を考えても非連結の条件を満たさないため、連結空間である。一方、その部分空間$( \\mathbb{Q}, d )$を考えると、$( \\mathbb{Q} , d ) = ( \\mathbb{Q} , \\mathscr{P} ( \\mathbb{Q} ) )$が離散空間であるため、簡単に非連結空間であることが示せる。\n証明 1 位相同型写像$f : X \\to Y$が存在し、$X$が連結空間であるとする。$Y$が連結空間であることを示せば証明は完了である。\n$Y$が非連結空間であると仮定すると、 $$ A \\cap B = \\emptyset \\\\ A \\cup B = Y $$ を満たす開集合$A, B \\subset Y$が存在する。\n$f$が連続関数ならば、すべての開集合$V \\subset Y$に対して、$f^{-1} (V)$が$X$で開集合である。\n$Y$は連続関数なので、$f^{-1} (A)$と$f^{-1} (B)$は$X$で開集合である。しかし、 $$ f^{-1} (A) \\cap f^{-1} (B) = f^{-1} (A \\cap B) = f^{-1} ( \\emptyset ) = \\emptyset \\\\ f^{-1} (A) \\cup f^{-1} (B) = f^{-1} (A \\cup B) = f^{-1} ( Y ) = X $$ これは、$X$が非連結空間であるという前提と矛盾する。\n■\n2 自明空間$X$の位相$\\mathscr{T} = \\left\\{ \\emptyset , X \\right\\}$では、空でない2つの開集合が存在しないため、$X$は連結空間である。\n■\n3 $X$の元が1つだけの場合、離散空間より自明空間であるが、$X$が2つ以上の元を持つと仮定しなければならない。離散空間$X$で、空でないすべての開集合$U$について$V = X \\setminus U$が$X$で開集合であるため、非連結空間である。\n■\n4 $A , B \\subset \\left\\{ x \\right\\}$が$A \\cap B = \\emptyset$を満たすためには、$A$または$B$が必ず空でなければならず、非連結空間にはなり得ない。\n■\nMunkres. (2000). 『トポロジー(第2版)』: p148.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":457,"permalink":"https://freshrimpsushi.github.io/jp/posts/457/","tags":null,"title":"位相数学における連結性"},{"categories":"분포이론","contents":"定義 1 連続型 $[a,b] \\subset \\mathbb{R}$に対して、次のような確率密度関数を持つ連続確率分布 $U(a,b)$を一様分布という。 $$ f(x) = {{ 1 } \\over { b - a }} \\qquad , x \\in [a,b] $$\n離散型 有限集合 $\\left\\{ x_{k} \\right\\}_{k=1}^{n}$に対して、次のような確率質量関数を持つ離散確率分布を一様分布という。 $$ p \\left( x_{k} \\right) = P \\left( X = x_{k} \\right) = {{ 1 } \\over { n }} \\qquad , k = 1, \\cdots , n $$\n説明 一般に一様分布は均一分布とも呼ばれる。離散一様分布の代表的な例として$x_{k} = k$のようなサイコロがあり、この場合、数理的な性質にはあまり関心を持つ必要がないことが多い。特に断りがない限り、一様分布は連続離散分布を指す。\n一様分布が重要な理由は、特別な理由があるというより、我々が考えうる最もシンプルな分布だからである。分布理論に慣れ親しんだ統計学の学生にはあまりにも素朴に見えるかもしれないが、まだ数学や人工知能などの分野で思ったよりも広く使われている。\n情報理論 情報理論の視点では非常に重要な分布で、離散型でも連続型でもシャノンエントロピーが最大化される分布だからである。考えてみれば、他の分布は確率関数で高低があっても、一様分布はサンプルがどうなるかのヒントすらないので、当然のことである。\n離散型でエントロピーが最大化されることは、ラグランジュ乗数法にとっても良い例である。\n基本性質 モーメント生成関数 [1]: $$m(t) = {{ e^{tb} - e^{ta} } \\over { t(b-a) }}$$ 平均と分散 [2]: $X \\sim U(a,b)$なら $$ E(X) = {{ a+b } \\over { 2 }} \\\\ \\text{Var}(X) = {{ (b-a)^{2} } \\over { 12 }} $$ 十分統計量と最尤推定量 [3]: ランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim U \\left( 0 , \\theta \\right)$が与えられたとする。 $\\theta$の十分統計量 $T$と最尤推定量 $\\hat{\\theta}$は次の通りである。 $$ \\begin{align*} T =\u0026amp; \\max_{k=1 , \\cdots , n} X_{k} \\\\ \\hat{\\theta} =\u0026amp; \\max_{k=1 , \\cdots , n} X_{k} \\end{align*} $$\n証明 [1] $$ \\begin{align*} m(t) = \\int_{a}^{b} e^{tx} {{ 1 } \\over { b-a }} dx =\u0026amp; {{ 1 } \\over { b-a }} \\left[ {{ 1 } \\over { t }} e^{tx} \\right]_{a}^{b} \\\\ =\u0026amp; {{ e^{tb} - e^{ta} } \\over { t(b-a) }} \\end{align*} $$\n■\n[2] 直接演繹する。\n■\n[3] 直接演繹する。\n■\nHogg et al. (2013). Introduction to Mathematical Statistics (7th Edition): p45.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":443,"permalink":"https://freshrimpsushi.github.io/jp/posts/443/","tags":null,"title":"二項分布"},{"categories":"추상대수","contents":"定義 群 $\\left\u0026lt; G , \\ast\\ \\right\u0026gt; , \\left\u0026lt; G' , *' \\right\u0026gt;$ を $\\phi : G \\to G'$ としよう。\n$\\forall x ,y \\in G $、$\\phi (x \\ast\\ y) = \\phi (x ) *' \\phi ( y)$ のとき、$\\phi$ を準同型写像Homomorphismという。 準同型写像 $\\phi$ が単射の場合、$\\phi$ を単射写像Monomorphismといい、$G \\hookrightarrow G'$ と表す。 準同型写像 $\\phi$ が全射の場合、$\\phi$ を全射写像Epimorphismといい、$G \\twoheadrightarrow G'$ と表す。 準同型写像 $\\phi$ が全単射の場合、$\\phi$ を同型写像Isomorphismといい、$G \\simeq G'$ と表す。 準同型写像 $\\phi$ において $G = G'$ の場合、$\\phi$ を準自己同型写像Endomorphismという。 同型写像 $\\phi$ において $G = G'$ の場合、$\\phi$ を自己同型写像Automorphismという。 説明 急に出てくる定義に頭が痛くなるかもしれないけど、すぐに慣れるから難しく考えないで堂々と対面しよう。\n単射写像と全射写像は任意に翻訳されたもので、日本の数学界では単にモノ射かエピ射と使用されている。抽象代数学以外では、これらがそれぞれ単射と全射そのものとして使われるが、抽象代数学では通常、準同型写像が含まれる。\n同型写像はその性質が直ちに有益であるが、それが求められる条件が難点である。そのような条件を減らすことができれば、つまり単射写像や全射写像だけで十分であれば、より良いだろう。\n","id":439,"permalink":"https://freshrimpsushi.github.io/jp/posts/439/","tags":null,"title":"In Japanese: 抽象代数学における様々な写像"},{"categories":"위상수학","contents":"定義 1 二つの位相空間 $X,Y$について、全単射 $f : X \\to Y$が存在し、$f$とその逆関数 $f^{-1}$が共に連続関数ならば、$f$を位相同型写像Homeomorphismと呼び、二つの位相空間は位相同型Homeomorphicであるという。\n定理 以下の命題は互いに等価だ。\n(1): $f : X \\to Y$は位相同型写像だ。 (2): $f^{-1} : Y \\to X$は位相同型写像だ。 (3): $f : X \\to Y$は閉関数でありながら連続の全単射だ。 (4): $f : X \\to Y$は開関数でありながら連続の全単射だ。 説明 距離空間で定義されたものと同様に、位相同型の概念も簡単に拡張できる。連続関数を学ぶ理由そのものと見ても良い。\n特に(3)、そして特に(4)が良い理由は、逆関数に対するチェックが不要なためだ。開関数と閉関数の性質から簡単に推論されて、逆関数が連続でなければならない条件を代わりに満たしてくれる。\n特に、$f,f^{-1}$が微分可能ならば、微分同型写像Diffeomorphism, ディフィオモルフィズムと呼ぶ。\n参照 グラフ理論における位相同型 Munkres. (2000). Topology(2nd Edition): p105.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":438,"permalink":"https://freshrimpsushi.github.io/jp/posts/438/","tags":null,"title":"位相空間におけるホモトピー"},{"categories":"정수론","contents":"要約 1 素数 $p$ とそれと互いに素な整数 $a$ について、$a^{p-1} \\equiv 1 \\pmod{p}$\n説明 フェルマーの小定理は、シンプルでありながら非常に多くの場所で利用される定理の一つである。オイラーによって一般化された定理もあるが、フェルマーの小定理だけで十分な場合も多い。特に有限体でのべき乗を多く扱う暗号理論などでは、不可欠な定理である。\n証明 戦略: 証明はシンプルだが、それほど簡単ではない。もし $$ a \\cdot 2a \\cdot \\cdots \\cdot (p-1)a \\equiv (p-1)! \\pmod{p} $$ が成立するなら、両辺から$(p-1)!$を消去してフェルマーの小定理を証明することができるだろう。つまり、二つの集合$\\left\\{ a,2a,\\cdots , (p-1)a \\right\\}$ と $\\left\\{1,2,\\cdots ,(p-1) \\right\\}$ が$\\pmod{p}$で同じであることを示せばよい。\n集合 $\\left\\{ a,2a,\\cdots , (p-1)a \\right\\}$ は、有限集合であるとして、正確に$(p-1)$個の要素を持つ。$a$が$p$と互いに素であるため、これらの要素を$p$で割った余りは$1$から$(p-1)$までの整数のいずれかになるだろう。そして、その余りに重複がなければ、 $$ \\left\\{ a,2a,\\cdots , (p-1)a \\right\\} = \\left\\{1,2,\\cdots ,(p-1) \\right\\} $$ が成立するだろう。$\\left\\{ a,2a,\\cdots , (p-1)a \\right\\}$から異なる任意の二つの要素$ia$と$ja$を取り出して考えてみる。これらを$p$で割った余りが同じと仮定すると、$ia \\equiv ja \\pmod{p}$が成立する。$a$が$p$と互いに素であるため、両辺から$a$を消去すると、$i \\equiv j \\pmod{p}$も成立する。しかし、上記の集合で$i$と$j$は$0$より大きく$p$より小さい整数だったため、$ia$と$ja$も同じである。これは仮定に反するので、異なる任意の二つの要素を$p$で割った余りは常に異なると言える。余りに重複がないため$\\pmod{p}$で、 $$ \\left\\{ a,2a,\\cdots , (p-1)a \\right\\} = \\left\\{1,2,\\cdots ,(p-1) \\right\\} $$ が成立する。一方で$p$は素数であるため、$(p-1)!$と互いに素である。 $$ (p-1)! a^{p-1} \\equiv (p-1)! \\pmod{p} $$ 両辺から$(p-1)!$を消去すると、合同式$a^{p-1} \\equiv 1 \\pmod{p}$を得る。\n■\n以下の系も覚えておくと良い。\n系 [1] 逆元: $\\pmod{p}$で、乗算に対する$a$の逆元は必ずこのように与えられる。 $$ a^{-1} \\equiv a^{p-2} \\pmod{p} $$ [2] フェルマーのテスト: $a^n \\equiv a \\pmod{n}$が成立しない場合、$n$は合成数である。 ある数が素数かどうかを判定することは難しいが、合成数であることは比較的判定しやすい。注意すべきは、フェルマーのテストの逆は成立しないということである。特に逆が成立しないことを示す反例としては、カーマイケル数がある。カーマイケル数の例として$561=3 \\cdot 11 \\cdot 17$は合成数だが、$a^{561} \\equiv a \\pmod{561}$は常に成立する。\nコード 以下は、Rでフェルマーテストを実装したもので、計算には連続べき乗法が使用された。\nFPM\u0026lt;-function(base,power,mod) #It is equal to (base^power)%%mod\r{\ri\u0026lt;-0\rif (power\u0026lt;0) {\rwhile((base*i)%%mod != 1) {i=i+1}\rbase\u0026lt;-i\rpower\u0026lt;-(-power)}\rif (power==0) {return(1)}\rif (power==1) {return(base%%mod)}\rn\u0026lt;-0\rwhile(power\u0026gt;=2^n) {n=n+1}\rA\u0026lt;-rep(1,n)\rA[1]=base\rfor(i in 1:(n-1)) {A[i+1]=(A[i]^2)%%mod}\rfor(i in n:1) {\rif(power\u0026gt;=2^(i-1)) {power=power-2^(i-1)}\relse {A[i]=1} }\rfor(i in 2:n) {A[1]=(A[1]*A[i])%%mod}\rreturn(A[1])\r}\rfermat.test\u0026lt;-function(n)\r{\rfor(i in 2:(n-1)) {if( i!=FPM(i,n,n) ) {return(paste(i,\u0026#34;is a Fermat witness!\u0026#34;))}}\rpaste(n,\u0026#34;passes the Fermat test!\u0026#34;)\r}\rfermat.test(121)\rfermat.test(341) #Almost composite yields fermat witness 2, but 341=11*31 doesn\u0026#39;t.\rfermat.test(561) #Carmicheal number 561 = 3*11*17\rfermat.test(1031) #1031 is a prime\rfermat.test(1105) #Carmicheal number 1105 = 5*13*17\rfermat.test(1729) #Carmicheal number 1729 = 7*13*19\rfermat.test(41041) #Carmicheal number 41041 = 7*11*13*41 以下はコードを実行した結果である。\nフェルマーテストは$121$や$341$のような合成数を確実に捕捉でき、$1031$のような素数を正しく通過させた。しかし、$561$、$1105$、$1729$、$41041$のようなカーマイケル数は捕捉できなかった。カーマイケル数を捕捉するには、ミラー-ラビンテストのような方法を使う必要がある。\nSilverman. (2012). A Friendly Introduction to Number Theory (4th Edition): p66.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":121,"permalink":"https://freshrimpsushi.github.io/jp/posts/121/","tags":null,"title":"フェルマーの小定理の証明"},{"categories":"위상수학","contents":"定義 日本語 位相空間 $(X, \\mathscr{T}_{X} )$ と $(Y, \\mathscr{T}_{Y} )$ に対して、$f: X \\to Y$ としよう。$f(a)$ を含む全ての $V \\in \\mathscr{T}_{Y}$ に対して、$f(U) \\subset V$ を満たしながら $a$ を含む $U \\in \\mathscr{T}_{X}$ が存在する場合、$f$ を $a$ で 連続Continuousという。$f$ が $X$ の全ての点で連続であれば 連続関数といい、$f \\in C(X,Y)$ で表せる。\n英語 $f$ is continuous at $a$ $\\iff$ For all neighborhood $V \\in \\mathscr{T}_{Y}$ of $f(a)$, there exists a neighborhood $ U \\in \\mathscr{T}_{X}$ of $a$ such that $a \\in U \\implies f(a) \\in f(U) \\subset V$\n説明 この定義を初めて見ると、何を意味しているのか理解するのが難しいかもしれないが、よく考えてみると、解析学での連続を定義する時に、$\\epsilon \u0026gt; 0$ が与えられるたびに $\\left| x - a \\right| \\lt \\delta \\implies \\left| f(x) - f(a) \\right| \\lt \\epsilon$ の $\\delta$ が存在することを話しているのと全く同じ感覚だと分かる。$\\left\\{ x : \\left| x - a \\right| \\lt \\delta \\right\\}$ と $\\left\\{ f(x) : \\left| f(x) - f(a) \\right| \\lt \\epsilon \\right\\}$ がオープンセットであることに注目すれば、$\\epsilon$ が与えられるたびに $\\delta$ を見つけられるということは、$Y$ でオープンセットが与えられるたびに、$X$ で条件を満たすオープンセットを見つけられるということと同じであると理解できるだろう。\nちなみに、$C(X,Y)$ は定義域が $X$ で値域が $Y$ の連続関数の集合である。位相数学を学ぶくらいなら、通常はイプシロン-デルタ論法を見飽きるほど見ているだろうから、文章よりも数式や記号の方が扱いやすいはずだ。\n連続性はユークリッド空間を超えて距離空間へ、そして今や距離空間を超えて位相空間へと一般化された。解析学で連続を議論する理由が微分のためであるならば、位相数学では位相同型を議論するために連続の概念が必要である。\n以下は連続点と連続関数に関するいくつかの有用な同値条件である。同値条件であるため、教科書によってはこれらの同値条件を定義として設定することもある。\n連続点の同値条件 $a \\in X$ とすると、以下の命題は互いに同値である。\n(1): $f : X \\to Y$ は $a$ で連続である。 (2): $f(a)$ を含む全ての $V \\in \\mathscr{T}_{Y}$ に対して、$a \\in U \\subset f^{-1} (V)$ を満たす $ U \\in \\mathscr{T}_{X}$ が存在する。 (3): 全ての $\\mathcal{N} ( f(a) )$ に対して、$f^{-1} ( \\mathcal{N} ( f(a) ) )$ は $a$ の近傍である。 (4): $f(a) \\in V^{\\circ}$ を満たす全ての $V \\subset Y$ に対して、$a \\in (f^{-1} (V))^{\\circ} $ ちなみに、$\\mathcal{N} (a)$ は $a$ を含む $X$ の開集合であり、$a$ の近傍Neighborhoodと呼ばれる。\n連続関数の同値条件 以下の命題は互いに同値である。\n[1]: $f : X \\to Y$ は連続関数である。 [2]: $f(a)$ を含む全ての $V \\in \\mathscr{T}_{Y}$ と全ての点 $a \\in f^{-1} (V)$ に対して、$a \\in U_{a} \\subset f^{-1} (V)$ を満たす $ U_{a} \\in \\mathscr{T}_{X}$ が存在する。 [3]: 全ての開集合 $V \\subset Y$ に対して、$f^{-1} (V)$ が $X$ で開集合である1。 [4]: 全ての閉集合 $C \\subset Y$ に対して、$f^{-1} (C)$ が $X$ で閉集合である。 [5]: 全ての $A \\subset X$ に対して、$f( \\overline{A} ) \\subset \\overline{ f(A) } $ [6]: 全ての $B \\in \\mathscr{B}$ に対して $f^{-1} (B) \\in \\mathscr{T}_{X}$ を満たす $\\mathscr{T}_{Y}$ の基底 $\\mathscr{B}$ が存在する。 [7]: 全ての $S \\in \\mathscr{S}$ に対して $f^{-1} (S) \\in \\mathscr{T}_{X}$ を満たす $\\mathscr{T}_{Y}$ の部分基底 $\\mathscr{S}$ が存在する。 [8] 連続関数の合成関数: $f : X \\to Y$ と $g : Y \\to Z$ が連続関数であれば、合成関数 $g \\circ f : X \\to Z$ も連続である。\n連続関数の別の定義 特に \u0026lsquo;定理 [3]: 全ての開集合 $V \\subset Y$ に対して、$f^{-1} (V)$ が $X$ で開集合である\u0026rsquo;は非常に頻繁に使用され、[3]をもって連続関数を定義する場合も多い。上で挙げた全ての条件を覚える必要はないが、[3]だけは必ず覚えて、いつでも取り出せるようにしておこう。\nMunkres. (2000). Topology(2nd Edition): p102.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":432,"permalink":"https://freshrimpsushi.github.io/jp/posts/432/","tags":null,"title":"位相数学における連続とは"},{"categories":"위상수학","contents":"定義 位相空間 $\\left( X , \\mathscr{T} \\right)$ において $\\mathscr{B} , \\mathscr{B}_{x} \\subset \\mathscr{T}$ とする。\n$B_{\\lambda} \\in \\mathscr{B}$ とするとき、全ての $U \\in \\mathscr{T}$ に対して $$ U = \\bigcup_{\\lambda \\in \\Lambda} B_{ \\lambda } $$ を満たす近傍 $\\Lambda$ が存在すれば、$\\mathscr{B}$ を $\\mathscr{T}$ に対する基底Basisという。このとき位相 $\\mathscr{T}$ は $\\mathscr{B}$ によって生成されるGeneratedという。 $x \\in X$ とするとき、全ての $B \\in \\mathscr{B}_{x}$ に対して $x \\in B$ であり、$x$ を含む全ての $U \\in \\mathscr{T}$ に対して $$ x \\in B \\subset U $$ を満たす $B \\in \\mathscr{B}_{x}$ が存在すれば、$\\mathscr{B}_{x}$ を $x$ での局所基底Local Basisという。 説明 定義がかなり分かりづらく書かれているため、練習問題を解く前に概念的に受け入れる方がずっと楽だろう。線形代数学での基底と感じが似ているが、定義上で似ていることはほとんどないので、その関連性を探りすぎないようにしよう。\n一言で言えば、基底は与えられた位相を合集として作ることができる集合だ。交差点を考える必要はないので、位相で「小さい」開集合を集めて構成すればいい。\n例えば、距離空間を例に挙げると、全ての開球の集合は距離空間の基底になる。\n必要性 本を読んで学ぶ立場からすると、線形代数でそうだったように、位相 $\\mathscr{T}$ で基底 $\\mathscr{B}$ を探すと考えると難しくて理解しづらい概念だ。反対に、生成、つまり基底から位相を作る立場になれば、基底というものがどれほど便利かが分かる。\n例えば、自然数の数列で位相空間を作るとしたら、先頭を基準に位相を作りたい場合、$B_{1}$ は先頭が $1$ の数列の集合で、$B_{2}$ は先頭が $2$ の数列の集合、$B_{k}$ は先頭が $k$ の数列の集合\u0026hellip; といった方法でアプローチすることができる。問題は、$\\mathscr{T}$ に合集 $B_{1} \\cup B_{2}$ が存在しないことだ。なぜなら、先頭が $1$ または $2$ の数列が存在しないからだが、このとき$\\mathscr{B} = \\left\\{ B_{k} \\right\\}_{k \\in \\mathbb{N}}$ で可能な全ての合集があるとすれば、仕事はずっと簡単になる。これがまさに基底で生成された位相を上手く使ったことになる。\n判定法 1 基底の判定: 全集合 $X$ に対して、$\\mathscr{B} \\subset \\mathscr{P} (X)$ が以下の二つの条件を満たすとき、$X$ の基底だ。\n(i): $\\displaystyle X = \\bigcup_{B \\in \\mathscr{B}} B$ (ii): $x \\in B_{1} \\cap B_{2}$ である全ての $ B_{1} , B_{2} \\in \\mathscr{B}$ に対して、以下を満たす $B_{x} \\in \\mathscr{B}$ が存在する。 $$ x \\in B_{x} \\subset B_{1} \\cap B_{2} $$ この判定法は、実際の問題解決などで有効に使うことができる定理なので、必ず覚えておくべきだ。教科書によっては、この判定法が定義となることもある。\n局所基底は、基底とは違い、位相全体ではなく与えられた一点だけを扱う概念だ。言葉が長く難しいが、要約すると最終的には、$x$ を含む全ての開空間の中で最も「小さい」ものだけを集めても局所基底の条件を満たす。\n例えば、距離空間を例に挙げると、$x$ を中心とする全ての開球の集合は、$x$ での局所基底になる。\n基底と局所基底の関係 $X$ を位相空間としよう。\n$\\mathscr{B}$ が $X$ の基底であれば、$\\mathscr{B}_{x} := \\left\\{ B \\in \\mathscr{B} \\ | \\ x \\in B \\right\\}$ は $x \\in X$ の局所基底だ。逆に、全ての $x \\in X$ に対して $\\mathscr{B}_{x}$ が局所基底であれば、$\\displaystyle \\mathscr{B} := \\bigcup_{x \\in X} \\mathscr{B}_{x}$ は $X$ の基底だ。\n注意事項 必要十分条件ではないので、逆が成立するためには、全ての点での局所基底を考える必要がある点に注意しよう。\nMunkres. (2000). Topology(2nd Edition): p78.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":412,"permalink":"https://freshrimpsushi.github.io/jp/posts/412/","tags":null,"title":"位相数学における基底と局所基底"},{"categories":"복소해석","contents":"定義 1 関数$f: A \\subset \\mathbb{C} \\to \\mathbb{C}$が$\\mathscr{R} \\subset A$で解析的であり、すべての$z \\in \\mathscr{R}$に対して$f ' (z) \\ne 0$を満たす場合、$f$とすると等角写像Conformal Mappingまたは等角変換Conformal Transformという。一方で、$f ' (\\alpha) = 0$を満たす点$\\alpha$が存在する場合、$\\alpha$を$f$の臨界点Critical Pointという。\n説明 等角等角という漢字そのまま、等角変換を取ると形状が作る角が保持される。\nその名の通り、等角写像同士の合成は等角写像であるという事実を覚えておこう。証明は、以下の対偶を確認することで十分だ。 $$ (f \\circ g) \u0026rsquo; = f '(g) g' = 0 \\iff g' = 0 \\lor f ' = 0 $$\nこのような等角変換は、単純閉路を多く扱う複素解析において非常に重要で、積分経路を扱うときに便利に使われる。幾何学的には、臨界点を考えると、つまり完全に停止するために方向を変えなければならない、つまり折れる点と言える。一方、解析的で単射である関数は、以下の二つの重要な性質を持つ。\n基本性質 1 [1]: もし関数$f$が$\\mathscr{R}$で解析的であり単射ならば、すべての$z \\in \\mathscr{R}$で$f ' (z) \\ne 0$が成り立つ。言い換えると、$f$は等角写像である。 [2]: もし関数$f$が$\\mathscr{R}$で解析的であり単射であり、単純閉路$\\mathscr{C}$を$\\mathscr{C} ' $に対応させる場合、$f$は$\\mathscr{C}$内部の点を$\\mathscr{C} ' $の内部か外部のみに対応させる。 [1]で必要十分条件ではないことに注意しよう。[2] は特に重要な性質であって、$\\mathscr{C}$内部の一点だけチェックすれば、他の点が$\\mathscr{C} ' $の内部に行くか外部に行くかがわかる。 Osborne (1999). Complex variables and their applications: p193, 196.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":409,"permalink":"https://freshrimpsushi.github.io/jp/posts/409/","tags":null,"title":"複素解析における等角写像とは？"},{"categories":"위상수학","contents":"定理 一般に、位相空間での数列の極限は一意ではない。\n説明 何のことかと思うかもしれないが、驚くべきことにこれは事実だ。これまで私たちは解析学などで数列が含まれる区間が徐々に狭まりながら一点に収束するイメージを持っていた。しかし、位相数学で定義される収束の概念によると、位相空間によっては一点に収束する理由が全くない。\n極限の一意性を保証するためには、主にハウスドルフ空間が仮定される。\n反証 極限が複数存在する反例を示せば十分である。\n余裕のある空間 $\\left( \\mathbb{R} , \\mathscr{T}_{f} \\right)$ で、異なる点からなる数列 $\\left\\{ x_{n} \\right\\}$ を考えてみよう。まず、$\\left\\{ x_{n} \\right\\}$ が収束する点を任意に $x \\in \\mathbb{R}$ としよう。ここで、$x$ を含む開集合 $U \\in \\mathscr{T}_{f}$ が存在し、$\\mathbb{R} \\setminus U$ は有限集合である。$\\left\\{ x_{n} \\right\\}$ は異なる点から成っているので、全ての $n \u0026gt; n_{0}$ に対して $x_{n} \\notin \\mathbb{R} \\setminus U$ を満たす $n_{0} \\in \\mathbb{N}$ が存在することはない。しかし、$\\left\\{ x_{n} \\right\\}$ が収束するので、全ての $n \u0026gt; n_{0}$ に対して $x_{n} \\in U$ を満たす $n_{0} \\in \\mathbb{N}$ が存在しなければならない。収束の定義により、$\\left\\{ x_{n} \\right\\}$ は $x$ に収束するが、ここでの $x$ は何であっても特に問題はない。\n■\n理解が難しい場合は、収束の定義がどのように変わったのか、余裕のある空間の開集合が何かを考えてみると良い。\n従来の距離空間で開集合と言えば、ある点を中心に与えられた距離以内の点の集合を指すものだった。したがって、「全ての開集合」と言っても実際はその点の近傍で条件を満たすことが重要だった。どんなに小さくしても条件を満たし続ければ、「全ての開集合」に対するチェックは終わったも同然だった。\nしかし、余裕のある空間で$U$ と別の開空間を考えると、$U \\setminus \\left\\{ a \\right\\}$ のようなものも開集合になる。全体空間が実数であれ何であれ、全ての点を一つずつ取り除くだけで開集合は開集合となり、「距離」を考えることは無意味である。したがって、全ての開集合に対してチェックすると言っても、距離空間のように徐々に小さくなる理由はなく、極限が特定されないのである。\n","id":407,"permalink":"https://freshrimpsushi.github.io/jp/posts/407/","tags":null,"title":"一般的な位相空間における数列の極限は唯一ではない。"},{"categories":"위상수학","contents":"定義 1 位相空間 $X$ について $A \\subset X$ とする。\n$x \\in O \\subset A$ を満たす開集合 $O$ が存在する時、$x$ を $A$ の内点Interior Pointという。 $A$ の内点の集合 $A^{\\circ}$ を $A$ の内部Interiorという。 $A$ とその値域の合集 $\\overline{A} : = A \\cup a '$ を $A$ の閉包Closureという。 $x \\in \\overline{A}$ であり、かつ $x \\in \\overline{X \\setminus A}$ の時、$x$ を $A$ の境界点Boundary Pointという。 $\\partial A : = \\overline{A} \\cap \\overline{X \\setminus A}$ を $A$ の境界Boundaryという。 $\\overline{A} = X$ の時、$A$ は $X$ で稠密であるDenseという。 $\\left( \\overline{A} \\right) ^{\\circ} = \\emptyset$ の時、$A$ は $X$ でどこにも稠密でないNowhere Denseという。 $X$ が稠密な可算部分集合を持つ時、$X$ は可分であるSeparableという。 $\\overline{A}$ と $A^{\\circ}$ はそれぞれ $A$ のクロージャー、インテリアだ。 説明 まず、距離空間で定義されていた様々な定義を持ち込んでも全く問題がないことを確認しよう。\n典型的な可分空間の例は、$\\overline{ \\mathbb{Q} } = \\mathbb{R}$ だ。\n可算部分集合という言葉が難しいなら、まず整数の集合 $\\mathbb{Z}$ で実数空間 $\\mathbb{R}$ を分割すると考えてみよう。このイメージは想像しやすいが、どんな集合でもその部分集合で分割するのは簡単なので、何の意味もない。逆に、不可算集合で分割すると、扱いづらく、結局意味がない。一方、定義されたように、稠密性と可算性を満たす概念があるなら、あまりにも簡単でも難しくもないとみなせるだろう。その場合、部分集合 $A$ とはいえ、実際には全体集合を支える大きな「骨格」のような感じでなければならない。\n骨格の比喩をもう一歩進めてみるなら、ある空間が可分空間であるということは、どんなに $x \\in X$ が与えられたとしても、$x$ に収束する数列 $\\left\\{ x_{n} \\right\\}_{n \\in \\mathbb{N} }$ の存在を保証することと同じだ。例えば、$x \\in \\mathbb{R}$ が与えられたとすると、$x$ が何であれ、$x$ に収束する有理数の数列 $\\left\\{ q_{n} \\right\\}_{n \\in \\mathbb{N}}$ を見つけることができるということだ。\n可分性が重要なのは、それによって、自分が望む元に収束（稠密）する数列（可算）を作ることができるからだ。実用的な側面から見れば、この性質の有用性がさらに際立つ。応用数学では、複雑な関数をよく知られた簡単な関数たちで近似することが非常に重要だ。\nたとえば、連続関数の空間 $C[a,b]$ は可分空間であり、それに応じて、どんなに $f$ が与えられても、$f$ に収束する連続関数の数列 $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ の存在が保証されるということがわかる。この $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ が具体的に何であるかを明らかにするのは、数値解析などの応用数学の役割だが、存在を明らかにすることは純粋数学の役割だ。\n要約 稠密性の判定法 稠密性を判定する方法として、以下の有用な同値条件を必ず覚えておこう。\n$A$ が $X$ で稠密であることと、$X$ のすべての開部分集合 $U$ に対して、$U \\cap A \\ne \\emptyset$ が交差することは同値だ。\n基本性質: 部分空間の境界 [1]: $\\partial A \\subset A \\iff A = \\overline{A}$ [2]: $\\partial A \\subset X \\setminus A \\iff A = A^{\\circ}$ [3]: $\\partial A = \\emptyset \\iff A = A^{\\circ}= \\overline{A}$ 可算性とは関係ないが、これらの概念を新たに定義しただけに、これらの性質を知っておくことが重要だ。\n境界を利用して、空間が開いているか閉じているかを判断できる便利な性質だ。位相数学に慣れるにつれ、空間という空間がますます抽象的になるので、単語だけでも定義を推測し、考えることができることに感謝しよう。\nMunkres. (2000). 『位相空間』(第2版): p95, 97.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":405,"permalink":"https://freshrimpsushi.github.io/jp/posts/405/","tags":null,"title":"位相空間における可分性と閉包"},{"categories":"추상대수","contents":"定義 1 二つの二項演算構造 $\\left\u0026lt; S , * \\right\u0026gt;$ と $\\left\u0026lt; S' , *' \\right\u0026gt;$ に対し、全ての $x , y \\in S$ について $$ \\phi (x \\ast\\ y) = \\phi ( x ) *' \\phi ( y ) $$ を満たす全単射関数 $\\phi : S \\to S'$ が存在する場合、$\\phi$ を同型写像と呼び、$S$ と $S'$ は同型Isomorphicであると言い、$S \\simeq S'$ と書く。\n説明 定義を要約すると、演算を保持する全単射が存在する場合、実質的に同じとみなすということです。抽象代数に限らず、同型Isomorphismとして知られるこの写像は、数学全般でとても重要です。\nもし $\\phi$ が演算を保持するが全単射ではない場合、これを準同型写像Homomorphismという。このように、同型写像ではないが多くの重要な写像が存在し、その研究も無数にあります。\n次は、同型写像によって、恒等元の同一性も保持されるという意味の定理です。\n定理 同型写像 $\\phi$ により $S \\simeq S'$ であり、$e$ が $S$ の恒等元であれば、$\\phi (e)$ は $S'$ の恒等元です。\n証明 $e$ が $S$ の恒等元であるため、$s \\in S$ に対して $$ e \\ast\\ s = s \\ast\\ e = s $$\n$$ \\phi ( e \\ast\\ s ) = \\phi ( s \\ast\\ e ) = \\phi ( s ) $$ $\\phi$ が同型写像であるため、$s' : = \\phi (s) \\in S'$ に対して $$ \\phi ( e ) *' \\phi ( s ) = \\phi ( s ) *' \\phi ( e ) = \\phi ( s ) $$ したがって、$\\phi (e)$ は $S'$ の恒等元です。\n■\n参照 グラフ理論における同型 Fraleigh. (2003). A first course in abstract algebra(7th Edition): p29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":403,"permalink":"https://freshrimpsushi.github.io/jp/posts/403/","tags":null,"title":"抽象代数学における同型"},{"categories":"위상수학","contents":"定義 位相空間 1 集合 $X$ が与えられた時、$\\mathscr{T} \\subset \\mathscr{P} (X)$ が $T \\in \\mathscr{T}$ に対して次の三つの条件を満たすなら、$\\mathscr{T}$ を $X$ の位相と呼び、$\\left( X , \\mathscr{T} \\right)$ を位相空間と呼ぶ。\n(i): $$\\emptyset , X \\in \\mathscr{T}$$ (ii): $$\\displaystyle \\bigcup_{ \\alpha \\in \\forall } T_{\\alpha} \\in \\mathscr{T}$$ (iii): $$\\displaystyle \\bigcap_{ i= 1}^{n} T_{i} \\in \\mathscr{T}$$ 条件 (i)~(iii) を言葉で説明すると以下の通り:\n(i): $\\mathscr{T}$ は空集合 $\\emptyset$ と全集合 $X$ を含む。 (ii): $\\mathscr{T}$ の元の和集合は $\\mathscr{T}$ に属する。 (iii): $\\mathscr{T}$ の元の有限交わりは $\\mathscr{T}$ に属する。 開集合と閉集合 2 $O \\in \\mathscr{T}$ を開集合と定義する。 $C \\subset X$ に対して $ X \\setminus C \\in \\mathscr{T}$ ならば $C$ を閉集合と定義する。 開集合であり、かつ、閉集合であるならば開かつ閉な集合と言う。 説明 位相空間 定義上、$\\mathscr{T}$ は $\\cup$ と $\\cap$ に対して閉じていて、代数的な考えが思い浮かぶかもしれないが、この定義だけでは代数的な性質を見つけるのが難しい。\n高校で習ったように「力と方向を持つ量」ではなく、条件を満たせばベクトルになるように、位相空間の位相も単に条件を満たす部分集合の集合として一般化されたものである。\n開集合と閉集合 位相を定義すると同時に、開けることと閉じることも新たに定義される。従来の距離空間では、開区間と閉区間の概念から続くものとして直感的に定義されていたが、一般的な位相では集合を使うため、抽象的で奇妙な空間を生み出すことができる。\n定義を見ると、開けることは完全に位相の概念を借りて新たに定義されているが、閉じることは距離空間でほとんど同じであることがわかる。\n位相と開けること、閉じることの定義に従って、以下の性質を簡単に確認できる。\n定理 [1-1]: $\\displaystyle \\bigcup_{ \\alpha \\in \\forall } O_{\\alpha} \\in \\mathscr{T}$ は開集合である。 [1-2]: $\\displaystyle \\bigcap_{ i= 1}^{n} O_{i} \\in \\mathscr{T}$ は開集合である。 [2-1]: $\\displaystyle \\bigcap_{ \\alpha \\in \\forall } C_{\\alpha} \\in \\mathscr{T}$ は閉集合である。 [2-2]: $\\displaystyle \\bigcup_{ i= 1}^{n} C_{i} \\in \\mathscr{T}$ は閉集合である。 [3]: $\\emptyset$ と $X$ は開集合であり、かつ、閉集合である。 例 以下の例を通して位相についての感覚を掴んでみよう。\n$X:=\\left\\{ a,b,c,d \\right\\}$ に対して $\\mathscr{T} : = \\left\\{ \\emptyset , \\left\\{ b \\right\\} , \\left\\{ a, b \\right\\} , \\left\\{ b,c \\right\\} , \\left\\{ a,b,c \\right\\} , \\left\\{ a,b,c,d \\right\\} \\right\\}$ が $X$ の位相であることを示せ。\n(i): $\\emptyset \\in \\mathscr{T}$ であり、$\\left\\{ a,b,c,d \\right\\} =X \\in \\mathscr{T}$ である。 (ii): 全集合 $X$ を除いては、$d$ は使われず、$\\left\\{ a,b,c \\right\\} \\in \\mathscr{T}$ である。 (iii): 空集合 $\\emptyset$ を除いては、すべて $b$ を共有し、$\\left\\{ b \\right\\} \\in \\mathscr{T}$ である。 ■\nMunkres. (2000). Topology(2nd Edition): p76.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (2000). Topology(2nd Edition): p93.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":398,"permalink":"https://freshrimpsushi.github.io/jp/posts/398/","tags":null,"title":"位相空間とは？"},{"categories":"거리공간","contents":"定義 1 距離空間 $\\left( X , d \\right)$について、$A \\subset X$としよう。\n$X$ の 数列 $\\left\\{ x_{n} \\right\\}$ の全てが、$\\varepsilon \u0026gt; 0$ に対して $n,m \u0026gt; n_{0}$ の時いつでも $d(x_{n} , x_{m}) \u0026lt; \\varepsilon$ を満たす 自然数 $n_{0}$ が存在すれば、コーシー数列Cauchy sequenceという。 $\\left( X , d \\right)$ 上のコーシー数列が収束する点が $X$ に属していれば、$\\left( X , d \\right)$を 完備Completeといい、そうでなければ 不完備Incompleteという。 $\\overline{A} = X$ の時、$A$が $X$ で 稠密Denseであるという。 $\\left( \\overline{A} \\right) ^{\\circ} = \\emptyset$ の時、$A$が $X$ で 至る所で稠密でないNowhere Denseであるという。 $X$ が $X$ の 加算Countablyな数の至る所で稠密でない 部分集合たちの 和集合で表せるなら、$\\left( X , d \\right)$ を 第1カテゴリーfirst category、そうではないなら 第2カテゴリーsecond categoryと呼ぶ。 $\\overline{A}$ と $A^{\\circ}$ は、それぞれ$A$ の クロージャー、インテリア である。 説明 完備性 完備性とは概念は閉じていることと非常に似ており、位相数学に触れる前には、むしろそれを閉集合と理解されがちだ。明確な違いは、完備では全体空間の概念が不要であり、閉じていることは全体空間が与えられて初めて知ることができるということである。\n例えば、$[a,b)$ の全体空間を $[a,b)$ とすると、$[a,b)$は全体空間なので閉集合になる。しかし、$[a,b)$上には$b$に収束するコーシー数列が存在するので、全体空間が何であれ$[a,b)$ は不完備である。\nしかし、完備性はこの直感的な理解よりも、より重要な概念を内包している。つまり、我々が関心を持っている空間の我々が関心を持っている数列が我々が関心を持っている要素に収束することを保証しているのである。空間が完備性を持たないということは、解答が実数でなければならない方程式の解が虚数で出てくることに例えられるかもしれない。その答えが何であれ、我々が望む形でなければ無用であり、完備性とはそれを保証しているのである。\n解析学での完備性公理に初めて触れた時、なぜそれがCompletenessと呼ばれるのか理解するのが難しかった。日常生活で、英単語のCompleteは完全に備えるという表現としてはあまり使われず、\u0026lsquo;完成\u0026rsquo;や\u0026rsquo;完結\u0026rsquo;など、何かが続いているもののその終わりと一緒に使われることが多いからである。完備性の一般化された定義を見ると、数列のその終わり、つまり収束点が（その空間の中に）存在することを保証している点で、completeという表現が適切であることがわかる。\n稠密性 稠密性は、新しく学ぶというよりは、直感的に受け入れてきた概念を位相数学的に考え直すことで十分である。そのような表現はクリーンだが、実際にはより詳しく説明する方が良い場合もある。別の表現では、すべての開集合 $O \\subset X$ に対して、$A \\cap O \\ne \\emptyset$ であれば、$A$ が $X$ で稠密であるという。後に、稠密性という概念は分離可能空間という概念に発展し、どのような数列を選べるかどうかという重要な問題に直面する。\n距離空間の研究は、数学的な重要性だけでなく、概念的にも我々人類にとって最も直感的なものである。当然、距離空間に関する研究も多く、完備性に関する議論も上述のように多くの、非常に多くの性質がある。\nカテゴリー バイアのカテゴリー定理: すべての完備距離空間は、それ自体を全体集合とみなした場合、第2カテゴリーである。\nカテゴリーについては、実際に理論が展開されるのを見るまでは、定義すらもなぜ存在するのか理解するのが難しい。とりあえずは、学びながら理解するしかない。バイアのカテゴリー定理の文脈では、それが自分自身を全体集合として考えるのか部分集合として考えるかよりも、「完備性」に焦点を当てるようにしよう。\nCroom. (1989). Principles of Topology: p87~89.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":396,"permalink":"https://freshrimpsushi.github.io/jp/posts/396/","tags":null,"title":"距離空間における完備性と密度性"},{"categories":"추상대수","contents":"定義 1 群 $G$ において、ある元 $a$ と任意の $x \\in G$ に対して $x = a^{n}$ を満たす整数 $n \\in \\mathbb{Z}$ が存在する場合、 $G$ を巡回群Cyclic Groupと言い、 $a$ を生成元Generatorという。\n説明 簡単に言えば、群のすべての元を生成元のべき乗で表現できる場合、それは巡回群です。「巡回」という表現が非常に適切であることがわかります。これは、すべての元を生成元のべき乗で表現する形式を続けるからです。\n定義からすぐにはわかりませんが、すべての巡回群はアーベル群であり、生成元が必ずしも一意ではない。定理[1]がその例である。\nさらに、定義によると巡回群が必ずしも有限群である必要はない。注意すべき点は、$n$ が存在するが、自然数ではなく整数であることであり、これは生成元の逆元を加えても構わないという意味です。定理[2]がその例である。\n定理 [1]: $\\mathbb{Z}_{4} = \\left\\{ 0,1,2,3 \\right\\}$ の生成元は一意ではない。 [2]: $\\mathbb{Z}$ は巡回群である。 証明 [1] $1$ だけでもすべての元を表現できるが、 $3 \\equiv -1 \\pmod{4}$ ゆえに、$3$ でもすべての元を表現できる。\nしたがって、$\\left\u0026lt; 1 \\right\u0026gt; = \\left\u0026lt; 3 \\right\u0026gt; = \\mathbb{Z}_{4}$ であり、生成元が一意である必要はないことがわかる。\n■\n[2] $\\left\u0026lt; \\mathbb{Z} , + \\right\u0026gt;$ のすべての元は$1 \\cdot n = (-1) \\cdot (-n) = n$ として表現できるから、$\\mathbb{Z} = \\left\u0026lt; 1 \\right\u0026gt; = \\left\u0026lt; -1 \\right\u0026gt;$\n■\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p59.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":392,"permalink":"https://freshrimpsushi.github.io/jp/posts/392/","tags":null,"title":"抽象代数学における巡回群"},{"categories":"거리공간","contents":"定義 距離空間 $\\left( X, d \\right)$ とするとき、$a \\in X$ であり、$r \u0026gt; 0$ とする。\n中心が $a$ で、半径が $r$ の開いた球Open Ballを $B_{d} (a,r) = \\left\\{ x \\in X \\ | \\ d(a,x) \u0026lt; r \\right\\}$ という。 中心が $a$ で、半径が $r$ の閉じた球Closed Ballを $B_{d} [a,r] = \\left\\{ x \\in X \\ | \\ d(a,x) \\le r \\right\\}$ という。 $O \\subset X$ が開いた球の和集合なら、$O$ を $X$ での開集合Open Setという。 $C \\subset X$ に対し、$X \\setminus C$ が開集合なら、$C$ を $X$ での閉集合Closed Setという。 説明 開集合と閉集合は別に定義できるが、本質的には同じだ。\n球とは区間、開区間、閉区間を一般化した概念であり、区間も $1$次元の球だと考えると、これは自然な話だ。もちろん、ユークリッド空間 $\\mathbb{R}$ の次元に対する一般化に止まらず、距離がきちんと与えられればどこでもしっかり定義される。\n開集合と閉集合は一般的に以下の性質を満たす。\n性質 全空間 $X$ 上の開集合を $O_{\\alpha}$ 、閉集合を $C_{\\alpha}$ とする。\n[1]: $X$ と $\\emptyset$ は開いていると同時に閉じている。 [2]: 開集合の和集合 $\\displaystyle \\bigcup_{\\alpha \\in \\forall} O_{\\alpha}$ は $X$ で開集合である。 [3]: 開集合の有限交差 $\\displaystyle \\bigcap_{i = 1}^{n} O_{i} $ は $X$ で開集合である。 [4]: 閉集合の交差 $\\displaystyle \\bigcap_{\\alpha \\in \\forall} C_{\\alpha}$ は $X$ で閉集合である。 [5]: 閉集合の有限和集合 $\\displaystyle \\bigcup_{i = 1}^{n} C_{i}$ は $X$ で閉集合である。 [3]で有限という条件がなければ、$\\displaystyle \\bigcap_{n = 1}^{ \\infty } \\left( -{{1} \\over {n}} , {{1} \\over {n}} \\right) = \\left\\{ 0 \\right\\}$ という反例が出せる。**[5]**で有限という条件がなければ、$\\displaystyle \\bigcup_{n = 1}^{ \\infty } \\left[ 0 , 1-{{1} \\over {n}} \\right] = [ 0 , 1 )$ という反例が出せる。\n証明 [1] このポストで紹介されている。\n[2]~[5] このポストで紹介されている。\n","id":382,"permalink":"https://freshrimpsushi.github.io/jp/posts/382/","tags":null,"title":"距離空間での球と開集合閉集合"},{"categories":"거리공간","contents":"定義 集合 set $X$ に対して、関数 $d : X \\times X \\to [0, \\infty)$が $x,y,z \\in X$ について以下の条件を満たす場合、$d$ を距離metricと呼び、$\\left( X, d\\right)$を距離空間metric spaceという。距離が自明の場合、簡単に $X$ と記されることもある。\n$d(x,y)=0 \\iff x = y$\n$d(x,y) = d(y,x)$\n$d(x,y) + d(y,z) \\ge d(x,z)$\n説明 線型代数学でノルムの概念を理解したなら、大きさや距離が直感的にのみ定義される必要はないことがわかるだろう。以下の3つの例は特に $\\mathbb{R}^{n}$上で定義され、前述のように、線型代数学で見たノルムと大きく変わらない。これは、ノルム $\\left\\| \\cdot \\right\\|$がどのように定義されても常に距離 $d ( \\mathbf{x} , \\mathbf{y} ) := \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|$ を定義できるためで、ある種のノルムがあればそれに対応する距離も存在する。\n例 $\\mathbf{x} = (x_{1} , x_{2} , \\cdots , x_{n} )$ そして $\\mathbf{y} = (y_{1} , y_{2} , \\cdots , y_{n} ) $ とする。\nユークリッド距離: $d(\\mathbf{x} , \\mathbf{y}) = \\sqrt{ \\sum \\limits_{i = 1}^{n} (x_{i} - y_{i} )^2 }$\nタクシーキャブ距離: $d^{\\prime}(\\mathbf{x} , \\mathbf{y}) = \\sum \\limits_{i = 1}^{n} | x_{i} - y_{i} |$\nマックス距離: $d^{\\prime \\prime}(\\mathbf{x} , \\mathbf{y}) = \\max \\left\\{ | x_{i} - y_{i} | \\right\\}_{i=1}^{n}$\n基本的な解析学では、主に $\\mathbb{R}^{1}$ を扱い、ユークリッド距離だけが使われると考えても良い。解析学に限って言えば、距離空間について詳細に学ぶ必要はなく、実数集合 $\\mathbb{R}$ を距離空間 $\\left( \\mathbb{R} , d \\right)$として受け入れるだけで十分だ。以下の二つの例は、ユークリッド空間を超えた距離の概念だ。\n離散距離:\n$$ d_{0} (x,y) = \\delta_{xy} = \\begin{cases}1, \u0026amp; \\ x \\ne y \\\\ 0, \u0026amp; \\ x = y \\end{cases} $$\n離散距離はクロネッカーのデルタを使用し、二つの要素が同じかどうかだけで判断する。三角不等式を満たしているかは、場合分けをして簡単に証明できる。\n積分距離:\n$$ \\rho (f,g) = \\int_{a}^{b} | f(x) - g(x) | dx $$\n積分距離は連続関数の集合 $C[a,b]$ で定義できる距離だ。二つの関数のグラフが完全に同じであれば、その値は $0$ になる。\n図で示すと、実線で囲まれた部分がちょうど $\\rho (f,g)$ になる。\nこれらの定義から、metric は従来の意味での‘距離’よりも、二つの間の‘距離感’として理解する方が適していることがわかる。完全に同じものは必ず $0$ であるため、‘無限大にどれだけ近いか’ではなく、‘$0$ からどれだけ遠いか’が重要だ。より抽象的な思考のために、距離が大きくなるほど‘場所’が遠くなるという直感的な考えから離れよう。\n","id":381,"permalink":"https://freshrimpsushi.github.io/jp/posts/381/","tags":null,"title":"距離空間の定義"},{"categories":"행렬대수","contents":"定義1 行列 $A \\in \\mathbb{C}^{m \\times n}$ とベクトル $\\mathbf{b} \\in \\mathbb{C}^{m}$ に関する線形システム $A\\mathbf{x} = \\mathbf{b}$ が過剰決定 あるいは 過少決定 であるとしよう。この場合、システムは解を持たないまたは無数に持つ。ここで、\n$$ \\left\\| A \\mathbf{x} - \\mathbf{b} \\right\\|_{2} $$\nの値を最小化する問題について考えてみよう。これを**最小二乗問題(LSP, Least Square Problem)と呼ぶ。この問題の解$\\mathbf{x}_{\\ast}$を最小二乗解(least square solution)**と呼ぶ。\n$$ \\mathbf{x}_{\\ast} = \\argmin \\left\\| A \\mathbf{x} - \\mathbf{b} \\right\\|_{2} $$\n$A \\mathbf{x} - \\mathbf{b}$を最小二乗誤差ベクトル(least square error vector), $\\left\\| A \\mathbf{x} - \\mathbf{b} \\right\\|$ を**最小二乗誤差(least square error)**と呼ぶ。\n説明 方程式の解が存在しないのは残念だが、だからといって解を求める努力を諦めるわけにはいかない。現実には、解決が難しい、学術の最前線で数学者たちの解法を待っている方程式が多く、そういう問題を、できるだけ近似的に解く方法を研究することは、間違いなく価値がある。その中でも、最小二乗法は、最も代表的な方法の一つだ。実用科学を問わず、活発に使用されており、特に、統計学においては、回帰分析を支える理論の根幹だ。 $\\left\\| \\mathbf{b} - A \\mathbf{x} \\right\\|_{2}$ の大きさが最小になるということは、$A \\mathbf{x}$ と$\\mathbf{b}$ の間の距離、つまり誤差が小さくなるということだ。直交影 $P : \\mathbb{C}^{m} \\to \\mathcal{C} (A)$ について\n$$ \\mathbf{b} = P \\mathbf{b} + (I -P) \\mathbf{b} $$\n$$ P \\mathbf{b} \\in \\mathcal{C} (A) $$\nであり、あるベクトル$\\mathbf{x}_{\\ast}$に対して$A \\mathbf{x}_{\\ast} = P \\mathbf{b}$ である。これについて\n$$ \\left\\| A \\mathbf{x} - \\mathbf{b} \\right\\|_{2} = \\left\\| A \\mathbf{x} - P \\mathbf{b} + P \\mathbf{b} - \\mathbf{b} \\right\\|_{2} $$\nと表示してみると、$( A \\mathbf{x} - P \\mathbf{b} ) \\in \\mathcal{C} (A)$ と $(I -P )\\mathbf{b} \\in \\mathcal{N}(A)$ が直交していることが分かる。ピタゴラスの定理によって\n$$ \\left\\| \\mathbf{b} - A \\mathbf{x} \\right\\|_{2}^{2} = \\left\\| A \\mathbf{x} - P \\mathbf{b} \\right\\|_{2}^{2} + \\left\\| (I -P )\\mathbf{b} \\right\\|_{2}^{2} $$\nであり、$\\left\\| \\mathbf{b} - A \\mathbf{x} \\right\\|_{2}$ が最も小さくなるのは、$\\mathbf{x} = \\mathbf{x}_{\\ast}$ の時だ。\nまた、影の性質から $A \\in \\mathcal{C} (A)$ であり、$(I - P) \\mathbf{b} \\in \\mathcal{C} (A)^{\\perp}$ ので、\n$$ A^{\\ast} (I - P) \\mathbf{b} = A^{\\ast} ( \\mathbf{b} - A \\mathbf{x}_{\\ast} ) = 0 $$\n結論として$A^{\\ast} A \\mathbf{x}_{\\ast} = A^{\\ast} \\mathbf{b}$なので、最小二乗法とは、基本的に標準方程式 $A^{\\ast} A \\mathbf{x}_{\\ast} = A^{\\ast} \\mathbf{b}$ を満たす解 $\\mathbf{x}_{\\ast}$ を見つけることである。\n数式ではなく、図を通して直感的に理解するためには、次の例をみると役に立つだろう。\n上のように平面上に置かれた点をすべて通る直線を引く問題を考えてみよう。当然ながら、この問題の解となる直線(解)は存在せず、できる限り近く通る直線(近似解)を探さなければならない。\n緑と赤の線を比較すると、左の方が右よりも正確であることが一目でわかるだろう。青で描かれた線の長さは、各点が直線に射影されたときに離れた距離を示している。この問題での最小二乗解は、これらの距離の二乗の和が最小となるある直線である。\nHoward Anton, Elementary Linear Algebra: Aplications Version (12番目版, 2019), p417-418\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":356,"permalink":"https://freshrimpsushi.github.io/jp/posts/356/","tags":null,"title":"最小二乗法"},{"categories":"선형대수","contents":"定義 ベクトル空間 $V$ の二つの部分空間 $W_{1}$と$W_{2}$に対して、次のことを満たせば、$V$を$W_{1}$と$W_{2}$の直和direct sumと呼び、$V = W_{1} \\oplus W_{2}$と表記する。\n(i) 存在性: 任意の$\\mathbf{v} \\in V$に対して、$\\mathbf{v} = \\mathbf{v}_{1} + \\mathbf{v}_{2}$を満たす$\\mathbf{v}_{1} \\in W_{1}$と$\\mathbf{v}_{2} \\in W_{2}$が存在する。\n(ii) 排他性: $W_{1} \\cap W_{2} = \\left\\{ \\mathbf{0} \\right\\}$\n(iii) 一意性: 与えられた$\\mathbf{v}$に対して、$\\mathbf{v} = \\mathbf{v}_{1} + \\mathbf{v}_{2}$を満たす$\\mathbf{v}_{1} \\in W_{1}$と$\\mathbf{v}_{2} \\in W_{2}$は一意である。\n一般化1 $W_{1}, W_{2}, \\dots, W_{k}$をベクトル空間 $V$の部分空間としよう。これらの部分空間が次の条件を満たすとき、$V$を$W_{1}, \\dots, W_{k}$たちの直和と呼び、$V = W_{1} \\oplus \\cdots \\oplus W_{k}$と表記する。\n$\\displaystyle V = \\sum\\limits_{i=1}^{k}W_{i}$\n$\\displaystyle W_{j} \\bigcap \\sum\\limits_{i \\ne j}W_{i} = \\left\\{ \\mathbf{0} \\right\\} \\text{ for each } j(1\\le j \\le k)$\nこのとき、$\\sum\\limits_{i=1}^{k}W_{i}$は$W_{i}$たちの和である。\n解説 (i) 存在性: この条件は$V = W_{1} + W_{2}$、つまり「$V$は$W_{1}$と$W_{2}$の和である」と書き換えることができる。\n(iii) 一意性: 実際、この条件は必要ない。条件**(ii)**により$\\mathbf{v}_{1} \\in W_{1}$であれば、$\\pm \\mathbf{v}_{1} \\notin W_{2}$であり、$W$のゼロベクトルに対して次の表現だけが存在する。\n$$ \\mathbf{0} = \\mathbf{0} + \\mathbf{0},\\quad \\mathbf{0}\\in W_{1}, W_{2} $$\nしたがって、$\\mathbf{v}$に対して、二つの表現$\\mathbf{v}_{1} + \\mathbf{v}_{2}$と$\\mathbf{v}_{1}^{\\prime} + \\mathbf{v}_{2}^{\\prime}$が存在するならば、\n$$ \\mathbf{0} = \\mathbf{v} - \\mathbf{v} = (\\mathbf{v}_{1} - \\mathbf{v}_{1}^{\\prime}) + (\\mathbf{v}_{2} - \\mathbf{v}_{2}^{\\prime}) = \\mathbf{0} + \\mathbf{0} \\implies \\mathbf{v}_{1}=\\mathbf{v}_{1}^{\\prime},\\ \\mathbf{v}_{2}=\\mathbf{v}_{2}^{\\prime} $$\nさらに、(i), (ii) $\\iff$ (iii) が成り立つ。\n定義を見ただけでは把握しづらいが、ユークリッド空間での例を見れば、これが非常に理にかなった便利な概念であることがわかる。例えば、$\\mathbb{R}^{3} = \\mathbb{R} \\times \\mathbb{R} \\times \\mathbb{R}$を考えると、$\\mathbb{R}^{3}$の要素は$3$次元ベクトル$(x,y,z)$であるが、これを$(x,y)$と$(z)$に分けてみよう。\n一方で、分けたものを再結合する過程を考えれば、$(x,y) \\in \\mathbb{R}^2$となり、それによって$(z) \\in \\mathbb{R}$となるため、これらの単純な和集合$\\mathbb{R}^2 \\cup \\mathbb{R}$は、スカラーと$2$次元ベクトルを要素に含むことになる。これらの記号だけでは、実際に私たちが望む空間の拡張と分離を表現するのが難しいことがわかる。だから、直和という概念を導入すると、部分空間がベクトル空間をきれいに分割するときに、多くの面で説明しやすくなるだろう。\n参照 抽象代数学での直和 ベクトル空間の和 $+$ Stephen H. Friedberg, Linear Algebra (第4版, 2002), p275\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":353,"permalink":"https://freshrimpsushi.github.io/jp/posts/353/","tags":null,"title":"ベクトル空間における直和"},{"categories":"R","contents":"概要 Rは代表的な統計プログラミング言語で、便利なメソッドだけでなく、例示に適したデータセットも提供しています。このようなデータセットがなければ、講義のたびに新しいデータをダウンロードして読み込む手間をかけなければならないでしょう。\nガイド データセットを読み込む方法は非常に簡単です。読み込みたいデータセットの名前を使用する変数に割り当てるだけです。統計学を勉強するうちに何度も見ることになるアイリス（花）データを見てみましょう。\n各列は順に、がくの長さ、幅、花びらの長さ、幅、種類を意味します。列ごとに名前は書かれていますが、これだけでデータを把握するのが難しい場合は、?iris を入力してヘルプを読んでみましょう。\nもちろん、アイリスだけがデータセットにあるわけではありません。コンソール窓に library(help=datasets) を入力すると、以下のように読み込むことができるデータセットのリストと簡単な説明を見ることができます。\n大まかな分類 回帰分析 attitude LifeCycleSavings Loblolly attenu faithful iris quakes wiss trees 時系列 AirPassengers BJsales EuStockMarkets WorldPhones JohnsonJohnson LakeHuron Nile UKDriverDeaths UKgas USAccDeaths USPersonalExpenditure WWWusage airmiles airquality austres co2 discoveries freeny lh longley lynx nhtemp nottem presidents sunspot.month sunspot.year sunspots treering uspop 多変量 Harman23.cor Harman74.cor USJudgeRatings カテゴリー HairEyeColor Titanic UCBAdmissions ability.cov 実験 CO2 ChickWeight DNase Indometh InsectSprays Orange OrchardSprays PlantGrowth Puromycin Theoph cars chickwts morley mtcars npk pressure warpbreaks 小標本 BOD Formaldehyde VADeaths anscombe euro sleep stackloss women その他 crimtab esoph eurodist islands occupationalStatus precip randu rivers rock volcano ","id":331,"permalink":"https://freshrimpsushi.github.io/jp/posts/331/","tags":null,"title":"Rで組み込みデータセットを読み込む方法"},{"categories":"추상대수","contents":"定義 1 群 $\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$の二つの元$a, b$に対して$a \\ast\\ b = b \\ast\\ a$が成り立つ場合、$\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$を可換群Abelian Groupと定義する。\n説明 可換とは「交換法則が成り立つ」という意味で捉えてもいい。英語ではCommutativeの代わりにAbelianという言葉が使われており、これは天才数学者アーベルにちなんで名付けられた。もちろん、韓国語でアーベル群と呼んでも、意味の伝達上全く問題はない。\n可換群となると、もうかなり多くの条件を満たしたため、想像しにくい構造ではない。群でありながら可換群になれない例を見てみよう。\n逆行列が存在する正方行列の集合$\\text{GL}_{n} (\\mathbb{R}) = \\left\\{ A \\in \\mathbb{R}^{n \\times n} \\ | \\ \\det A \\ne 0 \\right\\}$において、群$\\left\u0026lt; \\text{GL}_{n} (\\mathbb{R}) , \\cdot \\right\u0026gt;$は可換群ではない。\n行列の乗算は交換法則が成り立たない。 行列の操作を扱う上で乗算では交換法則が成り立たないことが重要だと認識されることが多い。それだけ交換法則は我々が日常で扱う数において当然の性質であるため、注意すべきことを意味している。逆に言えば、交換法則を満たす例はかなり多く、それらの例は通常、我々にとって親しいものであることを意味している。\n群$\\left\u0026lt; \\mathbb{R} , + \\right\u0026gt;$は可換群である。\n実数の加算は交換法則が成り立つ。 我々にとって最も身近な実数だけ考えてもそうだし、複素数や有理数、整数も同様である。通常、群でありながら可換群でない例を見つける方がずっと難しい。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p39.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":309,"permalink":"https://freshrimpsushi.github.io/jp/posts/309/","tags":null,"title":"抽象代数学における可換群"},{"categories":"선형대수","contents":"定義1 ベクトル空間 $V$ の 基底 の要素（ベクトルの数）を $V$ の 次元dimension と定義し、以下のように表記する。\n$$ \\dim (V) $$\n説明 このような次元の一般化は、単にベクトル空間に対する探求を超えて、この社会を支える様々な技術に応用されている。世界が $3$ 次元で、描けもしない $4$ 次元が何の役に立つのかと思うかもしれないが、ユークリッド空間だけがベクトル空間ではないからである。例えば、統計学で使われる データセットを考えると、それをベクトルとして見ることができる。例えば、「アダム」という人が身長が175、体重が62、年齢が22、IQが103、視力が1.2であるとすると、「アダム=(175,62,22,103,1.2)」と表せるのである。こんな単純なデータでさえ、既に $5$ 次元を使用しており、些細な制限があると役に立たなくなる。\n一方で、ベクトル空間の基底が一意ではないことを考慮すると、上記の定義が妥当な定義であるためには、すべての基底が同じ数の要素を持つ必要があるという条件が必要である。以下の二つの定理から、有限次元ベクトル空間のすべての基底が同じ数のベクトルを持たなければならないことがわかる。\n定理 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots \\mathbf{v}_{n} \\right\\}$ をベクトル空間 $V$ の任意の基底とする。\n(a) 基底よりもベクトルの数が多い $V$ の部分集合は 線形従属である。\n(b) 基底よりもベクトルの数が少ない $V$ の部分集合は、$V$ を 生成できない。\n証明2 (a) $W=\\left\\{ \\mathbf{w}_{1},\\ \\mathbf{w}_{2},\\ \\cdots ,\\ \\mathbf{w}_{m} \\right\\} \\subset V$ とすると、$m \u0026gt; n$ である。$S$ が $V$ の基底であるので、$W$ の要素は $S$ のベクトルの 線形結合 で表すことができる。\n$$ \\begin{equation} \\begin{aligned} \\mathbf{w}_{1} \u0026amp;= a_{11}\\mathbf{v}_{1}+a_{21}\\mathbf{v}_{2} + \\cdots + a_{n1}\\mathbf{v}_{n}=\\sum \\limits _{i}^{n} a_{i1}\\mathbf{v}_{i} \\\\ \\mathbf{w}_{2} \u0026amp;= a_{12}\\mathbf{v}_{1}+a_{22}\\mathbf{v}_{2} + \\cdots + a_{n2}\\mathbf{v}_{n}=\\sum \\limits _{i}^{n} a_{i2}\\mathbf{v}_{i} \\\\ \u0026amp; \\vdots \\\\ \\mathbf{w}_{m} \u0026amp;= a_{1m}\\mathbf{v}_{1}+a_{2m}\\mathbf{v}_{2} + \\cdots + a_{nm}\\mathbf{v}_{n}=\\sum \\limits _{i}^{n} a_{im}\\mathbf{v}_{i} \\end{aligned} \\label{wlincom1} \\end{equation} $$\n$W$ が線形従属であることを示すために、\n$$ \\begin{equation} k_{1}\\mathbf{w}_{1} + k_2\\mathbf{w}_{2} + \\cdots + k_{m}\\mathbf{w}_{m}= \\mathbf{0} \\label{wlincom2} \\end{equation} $$\n$(k_{1},k_{2},\\dots,k_{m}) \\ne (0,0,\\dots,0)$ が存在することを示せばよい。$(1)$ を $(2)$ に代入すると、次のようになる。\n$$ \\begin{align*} \u0026amp;k_{1}(a_{11}\\mathbf{v}_{1} + a_{21}\\mathbf{v}_{2} + \\cdots + a_{n1}\\mathbf{v}_{n}) \\\\ + \u0026amp;k_2(a_{12}\\mathbf{v}_{1} + a_{22}\\mathbf{v}_{2} + \\cdots + a_{n2}\\mathbf{v}_{n}) \\\\ + \u0026amp;\\cdots \\\\ + \u0026amp;k_{m}(a_{1m}\\mathbf{v}_{1} + a_{2m}\\mathbf{v}_{2} + \\cdots + a_{nm}\\mathbf{v}_{n}) = \\mathbf{0} \\end{align*} $$\nこれを $\\mathbf{v}_{i}$ に対して整理すると、次のようになる。\n$$ \\left( \\sum \\limits _{j} ^{m} k_{j}a_{1j} \\right)\\mathbf{v}_{1} + \\left( \\sum \\limits _{j} ^{m} k_{j}a_{2j} \\right)\\mathbf{v}_{2} + \\cdots + \\left( \\sum \\limits _{j} ^{m} k_{j}a_{nj} \\right)\\mathbf{v}_{n} = \\mathbf{0} $$\nこの時、$S$ が $V$ の基底であり、線形独立であるため、上記の方程式を満たす解は、係数がすべて $0$ の場合のみである。したがって、次の方程式が成り立つ。\n$$ \\begin{align*} a_{11}k_{1} + a_{12}k_{2} + \\cdots + a_{1m}k_{m} = 0 \\\\ a_{21}k_{1} + a_{22}k_{2} + \\cdots + a_{2m}k_{m} = 0 \\\\ \\vdots \\\\ a_{n1}k_{1} + a_{n2}k_{2} + \\cdots + a_{nm}k_{m} = 0 \\end{align*} $$\n連立方程式を見ると、方程式の数は $n$ 個、未知数 $k$ の数は $m$ 個である。方程式の数よりも未知数の数が多いため、この連立方程式は無数の非自明な解を持つ。したがって、$(2)$ を満たすすべてが $0$ ではない $k_{1},\\dots,k_{m}$ が存在する。したがって、$W$ は線形従属である。また、この証明は基底よりも要素の数が多い任意の集合にも適用される。\n■\n(b) 背理法で証明する。\n$W=\\left\\{ \\mathbf{w}_{1},\\ \\mathbf{w}_{2},\\ \\cdots ,\\ \\mathbf{w}_{m} \\right\\} \\subset V$ とすると、$m \u0026lt; n$ である。そして、$W$ が $V$ を生成すると仮定してみる。すると、$V$ のすべてのベクトルを $W$ の線形結合で表すことができる。\n$$ \\begin{equation} \\begin{aligned} \\mathbf{v}_{1} \u0026amp;= a_{11}\\mathbf{w}_{1}+a_{21}\\mathbf{w}_{2} + \\cdots + a_{m1}\\mathbf{w}_{m} \\\\ \\mathbf{v}_{2} \u0026amp;= a_{12}\\mathbf{w}_{1}+a_{22}\\mathbf{w}_{2} + \\cdots + a_{m2}\\mathbf{w}_{m} \\\\ \u0026amp; \\vdots \\\\ \\mathbf{v}_{n} \u0026amp;= a_{1n}\\mathbf{w}_{1}+a_{2n}\\mathbf{w}_{2} + \\cdots + a_{mn}\\mathbf{w}_{m} \\end{aligned} \\label{vlincom1} \\end{equation} $$\nすると、$\\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots \\mathbf{v}_{n} \\right\\}$ が線形従属であるという矛盾が生じる。次の同次方程式を見てみよう。\n$$ k_{1}\\mathbf{v}_{1} + k_2\\mathbf{v}_{2} + \\cdots + k_{n}\\mathbf{v}_{n}= \\mathbf{0} $$\nここに $(1)$ を代入すると、次のようになる。\n$$ \\begin{align*} \u0026amp;k_{1}(a_{11}\\mathbf{w}_{1} + a_{21}\\mathbf{w}_{2} + \\cdots + a_{m1}\\mathbf{w}_{m}) \\\\ + \u0026amp;k_2(a_{12}\\mathbf{w}_{1} + a_{22}\\mathbf{w}_{2} + \\cdots + a_{m2}\\mathbf{w}_{m}) \\\\ + \u0026amp;\\cdots \\\\ + \u0026amp;k_{n}(a_{1n}\\mathbf{w}_{1} + a_{2n}\\mathbf{w}_{2} + \\cdots + a_{mn}\\mathbf{w}_{m}) = \\mathbf{0} \\end{align*} $$\nこれを $\\mathbf{w}_{i}$ に対して整理すると、次のようになる。\n$$ \\left( \\sum \\limits _{j} ^{n} k_{j}a_{1j} \\right)\\mathbf{w}_{1} + \\left( \\sum \\limits _{j} ^{n} k_{j}a_{2j} \\right)\\mathbf{w}_{2} + \\cdots + \\left( \\sum \\limits _{j} ^{n} k_{j}a_{mj} \\right)\\mathbf{w}_{m} = \\mathbf{0} $$\nすると、未知数 $k$ に関する次の同次線形システムが得られる。\n$$ \\begin{align*} a_{11}k_{1} + a_{12}k_{2} + \\cdots + a_{1n}k_{n} = 0 \\\\ a_{21}k_{1} + a_{22}k_{2} + \\cdots + a_{2n}k_{n} = 0 \\\\ \\vdots \\\\ a_{m1}k_{1} + a_{m2}k_{2} + \\cdots + a_{mn}k_{n} = 0 \\end{align*} $$\n未知数の数が $n$ で、方程式の数が $m$ であり、$m \u0026lt; n$ であるので、この線形システムは無数の非自明な解を持つ。したがって、$S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots \\mathbf{v}_{n} \\right\\}$ が線形従属であるという結果が得られ、これは $S$ が線形独立であるという事実と矛盾する。したがって、仮定は間違っていることがわかる。よって、$W$ は $V$ を生成することができない。\n■\nHoward Anton, Elementary Linear Algebra: Applications Version (第12版, 2019), p248\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Applications Version (第12版, 2019), p252-253\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3018,"permalink":"https://freshrimpsushi.github.io/jp/posts/3018/","tags":null,"title":"ベクトル空間の次元"},{"categories":"수리물리","contents":"解説 物理学で、デル演算子$\\nabla$を含む4つの演算、勾配、発散、回転、ラプラシアンは非常に重要である。そのため、3つの座標系における上記の演算を必ず知っておく必要がある。もちろん、これが暗記しなければならないという意味ではない。物理学の勉強は公式を暗記するのではなく、勉強しているうちに自然と覚えることになるので、わざわざ暗記しようとせず、公式を印刷して持ち歩くか、このページをブックマークして必要な時にすぐに取り出せるようにしよう。\n公式 $f$をスカラー関数、ベクトル関数$\\mathbf A$を$\\mathbf A= A_{1}\\mathbf{\\hat e_{1}}+A_2\\mathbf{\\hat e_2}+A_{3}\\mathbf{\\hat e_{3}}$とする。\n勾配:\n$$ \\begin{align*} \\nabla f \u0026amp;= \\mathbf{\\hat e_{1}}\\frac{1}{h_{1}}\\frac{\\partial f}{\\partial e_{1}}+ \\mathbf{\\hat e_2}\\frac{1}{h_2}\\frac{\\partial f}{\\partial e_2}+\\mathbf{\\hat e_{3}}\\frac{1}{h_{3}}\\frac{\\partial f}{\\partial e_{3}} \\\\ \u0026amp;= \\sum \\limits_{i=1}^3 \\mathbf{\\hat e_{i}}\\frac{1}{h_{i}}\\frac{\\partial f}{\\partial e_{i}} \\end{align*} $$\n発散:\n$$ \\nabla \\cdot \\mathbf A=\\frac{1}{h_{1}h_2h_{3}} \\left[ \\frac{\\partial}{\\partial e_{1}} (h_2h_{3}A_{1}) + \\frac{\\partial}{\\partial e_2} (h_{1}h_{3}A_2) + \\frac{\\partial}{\\partial e_{3}} (h_{1}h_2A_{3}) \\right] $$\n回転:\n$$ \\nabla \\times \\mathbf A =\\frac{1}{h_{1}h_2h_{3}} \\begin{vmatrix} h_{1} \\mathbf{\\hat e_{1}} \u0026amp; h_2 \\mathbf{\\hat e_2} \u0026amp; h_{3} \\mathbf{\\hat e_{3}} \\\\[0.5em] \\dfrac{\\partial}{\\partial e_{1}} \u0026amp; \\dfrac{\\partial }{\\partial e_2} \u0026amp; \\dfrac{\\partial}{\\partial e_{3}} \\\\[1em] h_{1}A_{1} \u0026amp; h_2A_2 \u0026amp; h_{3}A_{3} \\end{vmatrix} $$\nラプラシアン:\n$$ \\begin{align*} \u0026amp; \\nabla \\cdot (\\nabla f) \\\\ =\u0026amp;\\ \\nabla ^2 f \\\\ =\u0026amp;\\ \\frac{1}{h_{1}h_2h_{3}} \\left[ \\frac{\\partial }{\\partial e_{1}} \\left( \\frac{h_2h_{3}}{h_{1}} \\frac{\\partial f}{\\partial e_{1}} \\right) +\\frac{\\partial }{\\partial e_2} \\left( \\frac{h_{1}h_{3}}{h_2} \\frac{\\partial f}{\\partial e_2} \\right) + \\frac{\\partial }{\\partial e_{3}} \\left( \\frac{h_{1}h_2}{h_{3}} \\frac{\\partial f}{\\partial e_{3}} \\right) \\right] \\end{align*} $$\nこのとき、各座標系ごとの単位ベクトル、スケールファクターは以下の通りである。\n直交座標系:\n$$ \\mathbf{\\hat{e_{1}}}=\\mathbf{\\hat{\\mathbf{x}}},\\quad\\mathbf{\\hat{e_{2}}}=\\mathbf{\\hat{\\mathbf{y}}},\\quad\\mathbf{\\hat{e_{3}}}=\\mathbf{\\hat{\\mathbf{z}}},\\quad h_{1}=1,\\quad h_{2}=1,\\quad h_{3}=1 $$\n円筒座標系:\n$$ \\mathbf{\\hat{e_{1}}}=\\boldsymbol{\\hat \\rho},\\quad\\mathbf{\\hat{e_{2}}}=\\boldsymbol{\\hat \\phi},\\quad\\mathbf{\\hat{e_{3}}}=\\mathbf{\\hat{\\mathbf{z}}},\\quad h_{1}=1,\\quad h_{2}=\\rho,\\quad h_{3}=1 $$\n球座標系\n$$ \\mathbf{\\hat{e_{1}}}=\\mathbf{\\hat r},\\quad\\mathbf{\\hat{e_{2}}}=\\boldsymbol{\\hat \\theta},\\quad\\mathbf{\\hat{e_{3}}}=\\boldsymbol{\\hat \\phi},\\quad h_{1}=1,\\quad h_{2}=r,\\quad h_{3}=r\\sin\\theta $$\n直交座標系 勾配 $$ \\nabla f = \\frac{\\partial f}{\\partial x}\\mathbf{\\hat{\\mathbf{x}} }+ \\frac{\\partial f}{\\partial y}\\mathbf{\\hat{\\mathbf{y}}} + \\frac{\\partial f}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} $$\n発散 $$ \\nabla \\cdot \\mathbf A=\\frac{\\partial A_{x}}{\\partial x}+\\frac{\\partial A_{y}}{\\partial y}+\\frac{\\partial A_{z}}{\\partial z} $$\n回転 $$ \\begin{align*} \\nabla \\times \\mathbf A\u0026amp;=\\left(\\frac{\\partial A_{z}}{\\partial y}-\\frac{\\partial A_{y}}{\\partial z} \\right) \\mathbf{\\hat{\\mathbf{x}}}+\\left(\\frac{\\partial A_{x}}{\\partial z}-\\frac{\\partial A_{z}}{\\partial x} \\right) \\mathbf{\\hat{\\mathbf{y}}}+\\left(\\frac{\\partial A_{y}}{\\partial x}-\\frac{\\partial A_{x}}{\\partial y} \\right) \\mathbf{\\hat{\\mathbf{z}}} \\\\ \u0026amp;= \\begin{vmatrix} \\mathbf{\\hat{\\mathbf{x}}} \u0026amp; \\mathbf{\\hat{\\mathbf{y}}} \u0026amp; \\mathbf{\\hat{\\mathbf{z}}} \\\\ \\dfrac{\\partial}{\\partial x} \u0026amp; \\dfrac{\\partial }{\\partial y} \u0026amp; \\dfrac{\\partial}{\\partial z} \\\\ A_{x} \u0026amp; A_{y} \u0026amp; A_{z} \\end{vmatrix} \\end{align*} $$\nラプラシアン $$ \\begin{align*} \\nabla \\cdot (\\nabla f) = \\nabla ^2 f \u0026amp;= \\left( \\frac{\\partial}{\\partial x}\\mathbf{\\hat{\\mathbf{x}}}+\\frac{\\partial}{\\partial y}\\mathbf{\\hat{\\mathbf{y}}}+\\frac{\\partial}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} \\right) \\cdot \\left( \\frac{\\partial f}{\\partial x}\\mathbf{\\hat{\\mathbf{x}}}+\\frac{\\partial f}{\\partial y}\\mathbf{\\hat{\\mathbf{y}}}+\\frac{\\partial f}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} \\right) \\\\ \u0026amp;= \\frac{\\partial^2 f}{\\partial x^2}+\\frac{\\partial^2 f}{\\partial y^2}+\\frac{\\partial^2 f}{\\partial z^2} \\end{align*} $$\n円筒座標系 勾配 $$ \\nabla f = \\frac{\\partial f}{\\partial \\rho}\\boldsymbol{\\hat \\rho} + \\frac{1}{\\rho}\\frac{\\partial f}{\\partial \\phi}\\boldsymbol{\\hat \\phi} + \\frac{\\partial f}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} $$\n発散 $$ \\nabla \\cdot \\mathbf A=\\frac{1}{\\rho}\\frac{\\partial (\\rho A_\\rho)}{\\partial \\rho}+\\frac{1}{\\rho}\\frac{\\partial A_\\phi}{\\partial \\phi}+\\frac{\\partial A_{z}}{\\partial z} $$\n回転 $$ \\begin{align*} \\nabla \\times \\mathbf A\u0026amp;=\\left[\\frac{1}{\\rho}\\frac{\\partial A_{z}}{\\partial \\phi}-\\frac{\\partial A_\\phi}{\\partial z} \\right] \\boldsymbol{\\hat \\rho}+\\left[\\frac{\\partial A_\\rho}{\\partial z}-\\frac{\\partial A_{z}}{\\partial \\rho} \\right] \\boldsymbol{\\hat \\phi}+\\frac{1}{\\rho}\\left[\\frac{\\partial (\\rho A_\\phi)}{\\partial \\rho}-\\frac{\\partial A_\\rho}{\\partial \\phi} \\right] \\mathrm{\\hat{\\mathbf{z}}} \\\\ \u0026amp;= \\frac{1}{\\rho}\\begin{vmatrix} \\boldsymbol{\\hat \\rho} \u0026amp; \\rho\\boldsymbol{ \\hat \\phi} \u0026amp; \\mathbf{\\hat{\\mathbf{z}}} \\\\ \\dfrac{\\partial}{\\partial \\rho} \u0026amp; \\dfrac{\\partial }{\\partial \\phi} \u0026amp; \\dfrac{\\partial}{\\partial z} \\\\ A_\\rho \u0026amp; \\rho A_\\phi \u0026amp; A_{z} \\end{vmatrix} \\end{align*} $$\nラプラシアン $$ \\nabla \\cdot (\\nabla f) = \\nabla ^2 f = \\frac{1}{\\rho}\\frac{\\partial}{\\partial \\rho}\\left( \\rho \\frac{\\partial f}{\\partial \\rho} \\right) + \\frac{1}{\\rho^2}\\frac{\\partial^2 f}{\\partial \\phi^2} + \\frac{\\partial^2 f}{\\partial z^2} $$\n球座標系 勾配 $$ \\nabla f = \\frac{\\partial f}{\\partial r} \\mathbf{\\hat{\\mathbf{r}}} + \\frac{1}{r}\\frac{\\partial f}{\\partial \\theta} \\boldsymbol{\\hat{\\boldsymbol{\\theta}}} + \\frac{1}{r\\sin\\theta}\\frac{\\partial f}{\\partial \\phi}\\boldsymbol{\\hat \\phi} $$\n発散 $$ \\nabla \\cdot \\mathbf A=\\frac{1}{r^2}\\frac{\\partial (r^2 A_{r})}{\\partial r}+\\frac{1}{r\\sin\\theta}\\frac{\\partial (\\sin\\theta A_\\theta)}{\\partial \\theta}+\\frac{1}{r\\sin\\theta}\\frac{\\partial A_\\phi}{\\partial \\phi} $$\n回転 $$ \\begin{align*} \\nabla \\times \\mathbf A \u0026amp;=\\frac{1}{r\\sin\\theta} \\left[\\frac{\\partial (\\sin\\theta A_\\phi)}{\\partial \\theta}-\\frac{\\partial A_\\theta}{\\partial \\phi} \\right]\\mathbf{\\hat{\\mathbf{r}}}+\\frac{1}{r}\\left[\\frac{1}{\\sin\\theta} \\frac{\\partial A_{r}}{\\partial \\phi}-\\frac{\\partial (rA_\\phi)}{\\partial r} \\right] \\boldsymbol{\\hat{\\boldsymbol{\\theta}}} \\\\ \u0026amp; \\quad+ \\frac{1}{r} \\left[\\frac{\\partial (rA_\\theta)}{\\partial r}-\\frac{\\partial A_{r}}{\\partial \\theta} \\right]\\boldsymbol{\\hat \\phi} \\\\ \u0026amp;= \\frac{1}{r^2\\sin\\theta}\\begin{vmatrix} \\mathbf{\\hat{\\mathbf{r}}} \u0026amp; r\\boldsymbol{\\hat{\\boldsymbol{\\theta}}} \u0026amp; r\\sin\\theta\\boldsymbol{\\hat \\phi} \\\\ \\dfrac{\\partial}{\\partial r} \u0026amp; \\dfrac{\\partial }{\\partial \\theta} \u0026amp; \\dfrac{\\partial}{\\partial \\phi} \\\\ A_{r} \u0026amp; r A_\\theta \u0026amp; r\\sin\\theta A_\\phi \\end{vmatrix} \\end{align*} $$\nラプラシアン $$ \\nabla \\cdot (\\nabla f) = \\nabla ^2 f = \\frac{1}{r^2}\\frac{\\partial}{\\partial r} \\left( r^2\\frac{\\partial f}{\\partial r} \\right) + \\frac{1}{r^2\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left( \\sin\\theta \\frac{\\partial f}{\\partial \\theta} \\right) + \\frac{1}{r^2\\sin^2\\theta}\\frac{\\partial^2 f}{\\partial^2 \\phi} $$\n","id":299,"permalink":"https://freshrimpsushi.github.io/jp/posts/299/","tags":null,"title":"曲線座標系における勾配、発散、回転、ラプラシアン"},{"categories":"복소해석","contents":"定義 特異点 1 関数$f$が$\\alpha$で$\\mathcal{N}(\\alpha)$の全ての点で微分可能なら、$\\alpha$で解析的Analyticだという。 関数$f$が$\\alpha \\in \\mathbb{C}$では解析的ではないが、$\\mathcal{N}(\\alpha)$のいくつかの点で解析的な時、$\\alpha$を$f$の特異点Singular Pointと呼ぶ。 特異点$\\alpha$が$\\alpha$を除く全ての点で解析的な$\\mathcal{N}(\\alpha)$が存在するなら、$\\alpha$は孤立Isolatedしているという。 $\\mathcal{N}$は近傍を意味し、$\\alpha$を含む開集合を指す。 種類 $\\alpha \\in \\mathbb{C}$が$f$の特異点だとしよう。\n$\\displaystyle \\exists \\lim_{z \\to \\alpha} f(z) \\iff$$\\alpha$は取り除けるRemovable特異点だ。 $\\displaystyle \\lim_{z \\to \\alpha} (z - \\alpha)^n f(z) = k \\ne 0 \\iff$$\\alpha$は**$n$次の極**Pole of Order $n$だ。 $\\alpha$が極ではない、または分岐に関連している。$\\iff$$\\alpha$は本質的特異点essential singular pointだ。 説明 特に、極が$n=1$の時、単純極Simple Poleと言う。\n実際、非常に変態的なケースでなければ、普通は$f$が定義されていない点がそのまま特異点になる。\n例えば、$\\displaystyle f(z) = {{z - i} \\over {(z^2+1)(z+i)}}$という場合、特異点は$z= \\pm i$になるだろう。$\\csc z$のケースでは、特に有限である必要はなく、$z = n \\pi ( n \\in \\mathbb{Z} )$全てが特異点だ。一方で$\\text{Log} z$は$z= 0$で特異点を持っており、上で挙げた例とは少し違う感じがするだろう。\n$\\displaystyle f(z) = {{z - i} \\over {(z^2+1)(z+i)}}$で、$z = i$は取り除け、$z = -i$は$2$次の極だ。\n$\\displaystyle \\lim_{z \\to n \\pi} {{ z - n \\pi } \\over {\\sin z }} = 1$なので、$\\csc z$の特異点は全て$1$次の極、即ち単純極だ。\n最後に、$\\text{Log} z$では、$z = 0$は**分岐点**であるため、本質的特異点だ。\nこのような特異点の分類は一見何の意味もない定義の遊びのように思えるが、後に続く積分に関する議論では非常に重要な概念になる。\nOsborne (1999). Complex variables and their applications: p63.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":281,"permalink":"https://freshrimpsushi.github.io/jp/posts/281/","tags":null,"title":"複素解析での特異点の種類"},{"categories":"선형대수","contents":"定義1 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$をベクター空間$V$の空集合ではない部分集合としよう。定数$k_{1}, k_{2}, \\dots, k_{r}$に対して、次の方程式\n$$ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0} $$\nは少なくとも一つの解\n$$ k_{1} = 0,\\ k_{2} = 0,\\ \\dots,\\ k_{r} = 0 $$\nを持つ。これを自明解という。自明解だけが唯一の解である場合、ベクター$\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r}$は線形独立と呼ばれる。自明解ではない解が少なくとも一つ存在する場合は、線形従属と言う。\n説明 自明解とは、一見して分かる解で、そのためあまり価値がないとされる。なぜなら上記の定義の内容と同様に、$0$の場合が多いからだ。\nこの定義から次の定理がすぐに導き出される。\n$S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$をベクター空間$V$の空集合ではない部分集合としよう。$S$のどのベクターも他のベクターの線形組み合わせで表すことができない場合、線形独立だとされる。逆に、他のベクターの線形組み合わせで表せるベクターが少なくとも一つ存在する場合は、線形従属だとされる。\nこの定理の内容を考えると、「独立」と「従属」の命名がピンとくるだろう。教科書によっては、定義と定理が反対になっているものもある。\n興味深いことに、脚注の参考文献「Elementary Linear Algebra」は、翻訳版がこの文章と同じように定義されていて、原著は反対に定義されている。個人的には、この文章のように定義する方がクリーンだと思う。それは、反対に定義する場合、要素が一つの集合に対して独立/従属を別途定義する必要があるからだ。定理の証明は下で紹介する。\nもう少しかんたんに説明すると、異なる二つのベクターがある時、一つのベクターを増やしたり減らしたりしても、もう一つのベクターと同じにすることができない場合、それは独立だとされる。例えば、$(1,0)$と$(0,1)$は、どんな定数を乗じても、つまり増やしたり減らしたりしても、互いに同じにすることができない。定義に合わせて書き直すと、\n$$ k_{1} (1,0) + k_{2} (0,1) = \\mathbf{0} $$\n二番目の項を移項すると、\n$$ k_{1}(1,0) = - k_{2}(0,1) $$\n再整理すると、\n$$ (k_{1}, 0) = ( 0 , - k_{2}) $$\nとなるため、上記の式を満たす解は$k_{1} = k_{2} = 0$だけであるため、$(1,0)$、$(0,1)$は線形独立である。これは定理として証明することができる内容である。\n定理 (a) 零ベクターを含む有限集合は線形従属である。\n(b) 一つのベクター$\\mathbf{v}$が線形独立であるための必要十分条件は$\\mathbf{v} \\ne \\mathbf{0}$である。\n(c) 異なる二つのベクターが線形独立であるための必要十分条件は、一つのベクターが他のベクターの定数倍で表すことができないことである。\n(d) $S=\\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$を二つ以上のベクターを持つ集合としよう。$S$が線形独立であるための必要十分条件は、$S$のどのベクターも他のベクターの線形組み合わせで表すことができないことである。\n(e) $T \\subset S$としよう。$S$が線形独立であれば、$T$も線形独立である。\n(e') $T \\subset S$としよう。$T$が線形従属であれば、$S$も線形従属である。\n証明 (a) $S=\\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r}, \\mathbf{0} \\right\\}$としよう。すると次の式が成り立つ。\n$$ 0 \\mathbf{v}_{1} + 0 \\mathbf{v}_{2} + \\dots + 0 \\mathbf{v}_{r} + 1 \\mathbf{0} = \\mathbf{0} $$\nしたがって、定義により$S$は線形従属である。\n■\n(b) **(a)**を要素が一つの集合に適用すると成立する。\n■\n(c) $(\\Longrightarrow)$\n$\\mathbf{v}_{1}, \\mathbf{v}_{2}$が線形独立と仮定しよう。すると、\n$$ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} = \\mathbf{0} $$\nこの式を満たす解は$k_{1}=k_{2}=0$だけであるため、$\\mathbf{v}_{1} = -\\frac{k_{2}}{k_{1}}\\mathbf{v}_{2} = -k\\mathbf{v}_{2}$を満たす定数$k$は存在しない。\n$(\\Longleftarrow)$\n$\\mathbf{v}_{1}$が$\\mathbf{v}_{2}$の定数倍で表されないと仮定しよう。つまり、次の方程式\n$$ \\mathbf{v}_{1} = k_{2}\\mathbf{v} $$\nを満たす$k_{2}$が存在しないとしよう。すると、\n$$ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} = \\mathbf{0} $$\nこの式を満たす解は自明解だけであるため、$\\mathbf{v}_{1}, \\mathbf{v}_{2}$は線形独立である。\n■\n(d) $(\\Longrightarrow)$\n$S$が線形独立と仮定しよう。\n$$ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0} $$\nこの式を満たす解は$k_{1}=k_{2}=\\cdots=k_{r}=0$だけであり、\n$$ \\mathbf{v}_{1} = -\\frac{k_{2}}{k_{1}}\\mathbf{v}_{2} - \\cdots - \\frac{k_{r}}{k_{1}}\\mathbf{v}_{r} $$\nこの式を満たす定数$\\frac{k_{2}}{k_{1}}, \\dots, \\frac{k_{r}}{k_{1}}$は存在しない。これはすべての$\\mathbf{v}_{i}$に当てはまるため、どのベクターも他のベクターの線形組み合わせで表すことができない。\n$(\\Longleftarrow)$\nどのベクターも他のベクターの線形組み合わせで表すことができないと仮定しよう。つまり、次の方程式\n$$ \\mathbf{v}_{1} = k_{2}\\mathbf{v}_{2} + \\cdots + k_{r}\\mathbf{v}_{r} $$\nを満たす$k_{2}, \\dots, k_{r}$が存在しないとしよう。すると、\n$$ k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + \\cdots + k_{r}\\mathbf{v}_{r} = \\mathbf{0} $$\nこの式を満たす解は自明解だけであるため、$S$は線形独立である。\n■\n(e) 二つの集合$T$、$S$が次のようであるとしよう。\n$$ T = \\left\\{ \\mathbf{v}_{1},\\ \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\},\\quad S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r}, \\mathbf{v}_{r+1}, \\dots, \\mathbf{v}_{n} \\right\\} $$\n$T$は$S$の部分集合である。現在、$S$が線形独立と仮定しよう。すると、\n$$ c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} + c_{r+1} \\mathbf{v}_{r+1} + \\cdots + c_{n} \\mathbf{v}_{n} = \\mathbf{0} $$\nこの式を満たす解は自明解$c_{1}=c_{2} = \\cdots = c_{r} = c_{r+1} = \\cdots = c_{n} = 0$だけである。したがって、$c_{r+1} = \\cdots = c_{n} = 0$であるため、次の式が成り立つ。\n$$ \\begin{align*} \u0026amp;\u0026amp; c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} + c_{r+1} \\mathbf{v}_{r+1} + \\cdots + c_{n} \\mathbf{v}_{n} =\u0026amp;\\ \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} + \\left( 0\\mathbf{v}_{r+1} + \\cdots + 0 \\mathbf{v}_{n} \\right) =\u0026amp;\\ \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} + \\mathbf{0} =\u0026amp;\\ \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} =\u0026amp;\\ \\mathbf{0} \\end{align*} $$\nしかし、この式は$c_{1} = c_{2} = \\cdots = c_{r} = 0$の時にのみ成り立つため、$T$は線形独立である。\n■\n(e') **(e)**の対偶として成立する。\n■\nHoward Anton, Chris Rorres, Anton Kaul, Elementary Linear Algebra: Applications Version(12th Edition). 2019, p228-229\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":253,"permalink":"https://freshrimpsushi.github.io/jp/posts/253/","tags":null,"title":"線形独立と線形従属"},{"categories":"추상대수","contents":"定義 1 モノイド $\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$ の元 $a$ と単位元 $e$ に対し、$a \\ast\\ a\u0026rsquo; = a\u0026rsquo; \\ast\\ a = e$ を満たす $a '$ が存在すれば、$\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$を群Groupと定義する。すなわち、群は以下の性質を満たす二項演算構造だ。\n(i): 演算に対して結合法則が成立する。 (ii): すべての元に対して単位元が存在する。 (iii): すべての元に対して逆元が存在する。 説明 マグマから始まり半群、モノイドを経て、ついに群に到達した。大したことないように見えるかもしれないが、マグマと比較すると、条件はかなり増えている。演算で閉じており、結合法則が成立し、単位元と逆元の存在が必要なので、何でもかんでも群とは言えなくなっている。\n群を研究する理由は、他の代数的構造よりもずっとシンプルで簡単だからだ。群より条件が少なければ有用な性質も少なくなり、群より条件が多ければ使い道が減る。\n代数学で興味を持つ代数的構造のほとんどは基本的に群に基づいており、代数学は純粋数学の数論から、暗号理論のような日常生活に溶け込んだ応用数学にまで応用されている。数学以外では、驚くべきことに物理学でも群論が使われている。\nモノイドになりながら、群にならない例を見てみよう。\n正方行列の集合 $\\mathbb{R}^{n \\times n}$ に対して、モノイド $\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$ は群ではない。\n行列 $A \\in \\mathbb{R}^{n \\times n}$ に対して、$\\det A = 0$ ならば、$A^{-1}$ が存在しない。 もちろん、集合に制限を加えれば、これも群になりえる。\n逆行列が存在する正方行列の集合 $\\text{GL}_{n} (\\mathbb{R}) = \\left\\{ A \\in \\mathbb{R}^{n \\times n} \\ | \\ \\det A \\ne 0 \\right\\}$ に対して、モノイド $\\left\u0026lt; \\text{GL}_{n} (\\mathbb{R}) , \\cdot \\right\u0026gt;$ は群である。\n$\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$ の部分モノイド $\\left\u0026lt; \\text{GL}_{n} (\\mathbb{R}) , \\cdot \\right\u0026gt;$ は、乗算の逆元を持つので、定義 $\\text{GL}_{n} (\\mathbb{R})$ により群である。 対称性？ 群の概念を掴むとき、よく対称性についての話から始まったり、完全に数学的な定義だけで築き上げられたりする。\n対称性の例としては、ルービックキューブを回したりそのままにしたり(単位元)、元に戻したり(逆元)することがよく言及される。しかし、このような説明は構造が対称性を持つものが群の構造を持つことを理解しやすいが、群の構造が対称性を持つことを把握するのは難しい。群の形で数学的な定義に従うと、対称とは逆元の概念を思い浮かべるのが良い。\n$a$ が存在するなら、群の定義により、それに対応する $a '$ が必ず存在する。\n一方で、$a '$ もそれに対応する $a\u0026rsquo;\u0026rsquo;=a$ が存在するので、このような関係から対称を考えるのは自然だと言える。モノイドと群の違いは逆元だけなので、概念と定義がより妥当に一致していることが確認される。\n対称の話が出たからには、対称にピッタリの群の例を見てみよう。\nモノイド $\\left\u0026lt; \\mathbb{Z} , + \\right\u0026gt;$ は群である。\n整数 $a$ に対して、$-a$ は常に $a + (-a) = 0$ を満たす逆元になる。 $1$ が存在すれば、単位元 $0$ を中心に対称の $-1$ が存在し、$-2$ に対しては $2$ が存在し\u0026hellip;$n$ に対しては $-n$ が存在する。対称という意味で考えれば、かなり自然な例だ。\n一般的に、群だけを扱う場合、群 $\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$ はただ $G$ と表し、操作は特に言及がなければ $\\cdot$ と書く。ただし$\\left\u0026lt; \\mathbb{Z} , + \\right\u0026gt;$ のように加算が明確なコンテキストでは、$+$ のように場合に応じて適切な操作を使う。\nFraleigh. (2003). \u0026ldquo;A first course in abstract algebra(7th Edition)\u0026rdquo;: p37.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":278,"permalink":"https://freshrimpsushi.github.io/jp/posts/278/","tags":null,"title":"抽象代数学における群"},{"categories":"추상대수","contents":"定義 1 半群$\\left\u0026lt; M , \\ast\\ \\right\u0026gt;$において、全ての元$a$に対して、$a \\ast\\ e = e \\ast\\ a = a$を満たす$e$が存在するなら、$\\left\u0026lt; M , \\ast\\ \\right\u0026gt;$をモノイドMonoidと定義する。\n説明 モノイドは恒等元が存在する半群だ。恒等元という概念を導入することで、話せることが格段に増える。半群でありながらモノイドにならない典型的な例を見てみよう。\n半群$\\left\u0026lt; \\mathbb{N} , +\\right\u0026gt;$はモノイドではない。\n任意の自然数$a$に対して恒等元$ e$が存在して$a + e = a$を満たすと仮定する。 $e$は$1$以上の自然数だから、$a + e \\ge a + 1$が成り立つ。一方で$a + 1 \u0026gt; a$であるから、$a + e \u0026gt; a$となり、これは仮定に矛盾する。\nこのように自然に反証できる例があるということは、恒等元の存在が必ずしも自明ではないとも言えるだろう。\n定方行列の集合$\\mathbb{R}^{n \\times n}$について、$\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$はモノイドだ。\n行列の積の定義に従って、$\\left\u0026lt; \\mathbb{R}^{n \\times n} , + \\right\u0026gt;$が半群になることは容易に示せる。一方、単位行列$I_{n}$と任意の行列$( a_{ij} )$を考えると、$a_{ij} \\cdot 1 = a_{ij}$および$a_{ij} \\cdot 0 = 0$だから$(a_{ij}) I = I (a_{ij}) = (a_{ij})$となる。したがって、$I$は$\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$の恒等元になる。 ■\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p42.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":277,"permalink":"https://freshrimpsushi.github.io/jp/posts/277/","tags":null,"title":"抽象代数学におけるモノイド"},{"categories":"선형대수","contents":"定義: 線形組み合わせ1 $\\mathbf{w}$をベクトル空間$V$のベクトルとする。もし$\\mathbf{w}$が$V$のベクトル$\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots ,\\mathbf{v}_{r}$と任意の定数$k_{1}, k_{2}, \\cdots, k_{r}$によって以下のように表されるなら、$\\mathbf{w}$は$\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots ,\\mathbf{v}_{r}$の線形組み合わせlinear combination、または一次組み合わせという。\n$$ \\mathbf{w} = k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + \\cdots + k_{r}\\mathbf{v}_{r} $$\nこの場合、定数$k_{1}, k_{2}, \\cdots, k_{r}$は線形組み合わせ$\\mathbf{w}$の係数coefficientsと呼ばれる。\n解説 式で示されると見慣れないかもしれないが、難しい概念ではない。2次元直交座標系でのベクトル表示は、まさに二つの単位ベクトル$\\hat{\\mathbf{x}} = (0,1)$と$\\hat{\\mathbf{y}} = (0,1)$の線形組み合わせである。\n$$ \\mathbf{v} = (v_{1}, v_{2}) = (v_{1},0)+(0,v_{2}) = v_{1}(1,0) + v_{2}(0,1) = v_{1}\\hat{\\mathbf{x}} + v_{2} \\hat{\\mathbf{y}} $$\n定理 $S = \\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\}$をベクトル空間$V$の空でない部分集合とする。すると、以下が成り立つ。\n(a) $S$の要素の全ての可能な線形組み合わせの集合を$W$としよう。$W$は$V$の部分空間である。\n(b) **(a)**の$W$は$S$を含む$V$の部分空間の中で最小の部分空間である。つまり、$W^{\\prime}$を$S$を含む$V$の部分空間とすると、次の式が成立する。\n$$ S \\subset W \\le W^{\\prime} $$\n証明 (a) $W$が加算とスカラー倍に対して閉じているかを確認するためには、部分空間判定法を適用する。\n$$ \\mathbf{u} = c_{1} \\mathbf{w}_{1} + c_{2} \\mathbf{w}_{2} + \\cdots + c_{r} \\mathbf{w}_{r}, \\quad \\mathbf{v} = k_{1} \\mathbf{w}_{1} + k_{2} \\mathbf{w}_{2} + \\cdots + k_{r} \\mathbf{w}_{r} $$\n(A1)\n$\\mathbf{u}+\\mathbf{v}$は以下のようになる。\n$$ \\mathbf{u} +\\mathbf{v} = ( c_{1} + k_{1} ) \\mathbf{w}_{1} + ( c_{2} + k_{2} ) \\mathbf{w}_{2} + \\cdots + ( c_{r} + k_{r} ) \\mathbf{w}_{r} $$\nこれは$\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r}$の線形組み合わせなので、$\\mathbf{u} + \\mathbf{v} \\in W$が成り立つ。\n(M1)\n任意の定数$k$に対して、$k\\mathbf{u}$は以下のようになる。\n$$ k\\mathbf{u} = ( k c_{1} ) \\mathbf{w}_{1} + ( k c_{2} ) \\mathbf{w}_{2} + \\cdots + ( k c_{r} ) \\mathbf{w}_{r} $$\nこれは$\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r}$の線形組み合わせなので、$k\\mathbf{u} \\in W$が成り立つ。\n結論\n$W$が加算とスカラー倍に対して閉じているので、部分空間判定法により、$W$は$V$の部分空間である。\n$$ W \\le V $$\n■\n(b) $W^{\\prime}$を$S$を含む$V$の部分空間とする。すると、$W^{\\prime}$は加算とスカラー倍に対して閉じているので、$S$の要素の全ての線形組み合わせは$W^{\\prime}$の要素である。従って、\n$$ W \\le W^{\\prime} $$\n■\n定義: 生成 定理の$W$は$S$によって生成されたspanned$V$の部分空間という。また、ベクトル$\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r}$が$W$を生成するspanと言い、以下のように表記される。\n$$ W = \\text{span}\\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\} \\quad \\text{or} \\quad W = \\text{span}(S) $$\n解説 生成という概念が必要な理由は、ある要素を含む最小の集合を考えるためである。実際、上の定理でこの点を確認することができる。さらに、$S$自体から重複する要素をすべて除くと、これはベクトル空間の基底になる。\n定理 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$と$S^{\\prime} = \\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\}$をベクトル空間$V$の空でない部分集合とする。すると、\n$$ \\text{span} \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\} = \\text{span} \\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\} $$\n必要十分条件は、$S$の全てのベクトルが$S^{\\prime}$のベクトルの線形組み合わせとして表され、$S^{\\prime}$の全てのベクトルが$S$のベクトルの線形組み合わせとして表されることである。\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p220-222\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":512,"permalink":"https://freshrimpsushi.github.io/jp/posts/512/","tags":null,"title":"線形結合、生成"},{"categories":"추상대수","contents":"ビルドアップ 数学を大きく三つに分けるなら、幾何学、解析学、代数学と言えるだろう。その中で、代数学は教育課程で学ぶ二項、約分などを扱う数学の一分野だった。代数学とは基本的に「数」の代わりに文字を使ってどんな方程式でも解くことを目標とする学問だ。特定の数に限らず、一般的で強力な解法を探求するため、当時は最先端技術と言えた。しかし、このような数学的テクニックは教育が発達した現代では誰にでも当たり前の常識のようになった。\n一方、数学界はこれらの概念をさらに発展させ、「数」を超えて抽象的な「構造」に関心を持ち始めた。我々がもともと「数」と「計算」と呼んでいたものを「元素」と「演算」に抽象化したのだ。だから、現代代数学は代数的テクニックを使える条件やあるいは構造自体を研究する学問になった。上の説明から想像できるように、現代代数学は特に抽象的な面が強く、一般に「抽象代数学」と呼ばれる。\n抽象代数学で関心を持つのは主にある集合とその集合で定義される演算の性質だ。集合$S$と演算$\\ast$が与えられているなら、$S$は$\\ast$に対して閉じているか、単位元は存在するか、逆元は存在するかなどを研究する。その中でも抽象代数学で関心を持つ演算は、$a \\ast\\ b = c$のように二つの元が一つの元に対応する二項演算二項演算だ。\n定義1 二項演算は$\\ast : S \\times S \\to S$で定義される関数と見なせ、そのような二項演算が定義される集合を二項演算構造と言う。 集合$M \\ne \\emptyset$の元素$a,b$と二項演算$\\ast$に対して、$a * b \\in M$ならば$\\left\u0026lt; M , \\ast\\ \\right\u0026gt;$をマグママグマと定義する。 説明 マグマは抽象代数学が関心を持つ二項演算構造の中でも最も単純な概念だ。単に閉じている閉じているだけで良い。\nマグマになれない例 奇数の集合を$O$、無理数の集合を$I$としたら、$\\left\u0026lt; O , + \\right\u0026gt;$と$\\left\u0026lt; I , \\cdot \\right\u0026gt;$はマグマではない。\nマグマになれない例としては、奇数の集合や無理数の集合などがある。これらは乗算や加算などに対して閉じていないため、二項演算構造であってもマグマになれない。\n奇数の集合を$O$とした時、加算を考えると二つの奇数の和は必ず偶数なので$O$は閉じておらず、マグマになれない。 無理数の集合を$I$とした時、$I$で乗算を考えると$\\sqrt{2} , 2\\sqrt{2} \\in I$であり$\\sqrt{2} \\cdot 2 \\sqrt{2 } = 4 $だが、$ 4 \\notin I$なので$I$はマグマではない。 マグマになる例 任意の集合$S$のべき集合$\\mathscr{P}(S)$と差集合$\\setminus$について$\\left\u0026lt; \\mathscr{P}(S) , \\setminus \\right\u0026gt;$はマグマだ。\n$S$の部分集合$A$と$B$について、$( A \\setminus B ) \\subset S$なので、$( A \\setminus B ) \\in \\mathscr{P}(S)$であり$\\left\u0026lt; \\mathscr{P}(S) , \\setminus \\right\u0026gt;$はマグマだ。 演算も重要だ 大切なことは、代数的構造を探求する時は集合そのものだけでなく、演算も一緒に考えなければならない点だ。マグマにならない例をもう一度見よう。\n奇数の集合を$O$、無理数の集合を$I$としたら、$\\left\u0026lt; O , \\cdot \\right\u0026gt;$はマグマだが、$\\left\u0026lt; I , + \\right\u0026gt;$はマグマではない。\n奇数の集合を$O$とした時、乗算を考えると二つの奇数の乗算は必ず奇数なので、$O$は閉じておりマグマになる。 無理数の集合を$I$とした時、$I$で加算を考えると$\\sqrt{2} , -\\sqrt{2} \\in I$であり$\\sqrt{2} + ( - \\sqrt{2 } ) = 0$だが、$ 0 \\notin I$なので$I$はマグマではない。 奇数の集合は演算を変えることでマグマになったが、無理数の集合は依然としてマグマになれなかった。つまり、今は意味がないと思われる集合も、与えられた演算によっては意味のある代数的構造を持つ可能性があるということだ。\n一方で、マグマの定義は非常にシンプルで一般的なため、マグマ自体が有用な性質を提供するわけではない。マグマという名前自体が我々が知る「溶岩」に同じルーツを持ち、フランス語で「ごちゃごちゃしたもの」という意味を持つ。それだけ多くの代数的構造がマグマから始まるが、その概念自体はそれほど重要ではないと言えるだろう。\nFraleigh. (2003). アブストラクトアルジェブラ序論(第7版): p20, 29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":275,"permalink":"https://freshrimpsushi.github.io/jp/posts/275/","tags":null,"title":"抽象代数における二項演算"},{"categories":"선형대수","contents":"定義1 $W$がベクトル空間$V$の空集合でない部分集合とする。$W$が$V$で定義された加算とスカラー乗算に対してベクトル空間の定義を満たす時、$W$をベクトル空間$V$の部分空間subspaceと呼び、以下のように表記する。\n$$ W \\le V $$\n説明 ベクトル空間$V$の部分集合$W$が$V$の部分空間かどうかを判断するためには、ベクトル空間になるための10の規則を全て満たさなければならない。ベクトル空間の部分集合を取るたびに10の規則を全て確認するのはかなり面倒で困難なことになるだろう。しかし幸いなことに、あるベクトル空間の部分集合である理由だけで、いくつかの規則は自明に成立する。\n例えば$\\mathbf{u},\\mathbf{v}$が$W$の要素であれば、同時に$V$の要素であるため、(A2), (A3), (M2)-(M5) は自然に成立する。したがって、加算に対する閉包性**(A1)、ゼロベクトルの存在(A4)、逆の存在(A5)、スカラー乗算に対する閉包性(M1)だけを確認すれば、$W$は部分空間であることがわかる。しかし実際にはもっと単純である。条件(A1)、(M1)**を満たすことが部分空間であるための同値条件となる。\n例 ベクトル空間$V$の部分空間の例には、以下のものがある。\n自分自身$V$ 剰余類$v + W$ 線形変換$T : V \\to W$に対して、\n$T$の零空間$N(T) \\le V$ $T$の値域$R(T) \\le W$ 線形変換$T : V \\to V$に対して、\n固有空間$E_{\\lambda}$ $T$-不変空間 $T$-循環空間 定理: 部分空間判定法 $W$をベクトル空間$V$の空集合でない部分集合とする。$W$が$V$の部分空間であることと$W$が以下の二つの条件を満たすことは必要十分条件である。\n(A1) 部分集合$W$が$V$で定義された加算に対して閉じている。\n(M1) 部分集合$W$が$V$で定義されたスカラー乗算に対して閉じている。\n証明 $(\\implies)$\n$W$が$V$の部分空間であると仮定する。$W$が部分空間であれば、ベクトル空間の定義により$W$が**(A1)、(M1)**を満たすことは自明である。\n$(\\impliedby)$\n$W$が$(A1)$、$(M1)$を満たすと仮定する。そして$\\mathbf{u} \\in W$とする。そうすると$W$はスカラー乗算に対して閉じていて、$0\\mathbf{u}=\\mathbf{0}$であるため、以下が成立する。\n$$ 0 \\mathbf{u} = \\mathbf{0} \\in W $$\n同じ理由で$(-1)\\mathbf{u}=-\\mathbf{u}$によって以下が成立する。\n$$ (-1)\\mathbf{u} = -\\mathbf{u} \\in W $$\nしたがって$W$は**(A1)-(M5)**を全て満たすので$V$の部分空間である。\n■\n定理: 部分空間の交差も部分空間である2 $W_{1}, W_2$をベクトル空間$V$の部分空間とする。すると$W_{1} \\cap W_2$も$V$の部分空間である。\n証明 部分空間判定法により、$W_{1} \\cap W_2$が**(A1)、(M1)**を満たしているか確認すればよい。$W= W_{1} \\cap W_2$とする。\n(A1)\n$W = W_{1} \\cap W_2$であるため、$W$内の任意の二つのベクトル$\\mathbf u,\\mathbf v$はそれぞれ$W_{1}$、$W_2$にも含まれている。$W_{1}, W_2$は部分空間であるため、加算に対して閉じている。したがって\n、以下が成立する。\n$$ \\mathbf u + \\mathbf v \\in W_{1}, \\quad \\mathbf u + \\mathbf v \\in W_2 $$\nしたがって、交差の定義により以下が成立する。\n$$ \\mathbf u + \\mathbf v \\in W $$\n$W$内の任意の二つのベクトル$\\mathbf u,\\ \\mathbf v$に対して、$\\mathbf u + \\mathbf v$も$W$の要素であるため、$W$は加算に対して閉じており、**(A1)**を満たす。\n(M1)\n上記の場合と同様に証明する。\n$W = W_{1} \\cap W_2$であるため、$W$内の任意のベクトル$\\mathbf u$は$W_{1}$、$W_2$にも含まれている。$W_{1},\\ W_2$は部分空間であるため、スカラー乗算に対して閉じている。したがって、あるスカラー$k$に対して以下が成立する。\n$$ k\\mathbf{u} \\in W_{1} \\quad k \\mathbf{u} \\in W_2 $$\nしたがって、交差の定義により以下が成立する。\n$$ k\\mathbf u \\in W $$\n$W$内の任意のベクトル$\\mathbf u$に対して、$k\\mathbf u$も$W$の要素であるため、$W$はスカラー乗算に対して閉じており、**(M1)**を満たす。\n結論\n$W_{1}, W_{2}$が部分空間の時、$W = W_{1} \\cap W_2$が**(A1)、(M1)**を満たすので、$W$も部分空間である。\n■\n従来の定理 $W_{1}, W_{2}, \\dots W_{n}$がベクトル空間$V$の部分空間とする。すると$W = W_{1} \\cap \\cdots \\cap \\dots W_{n}$も$V$の部分空間である。\nHoward Anton, 基礎線形代数: アプリケーションバージョン (第12版, 2019), p211-212\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, 基礎線形代数: アプリケーションバージョン (第12版, 2019), p216\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":285,"permalink":"https://freshrimpsushi.github.io/jp/posts/285/","tags":null,"title":"ベクトル空間の部分空間"},{"categories":"선형대수","contents":"定義1 空集合ではない集合 $V$ の要素が二つの演算 加算additionと スカラー乗算scalar multiplicationに対して下記の10個の規則を満たす時、$V$を体2 $\\mathbb{F}$に対するベクトル空間vector spaceまたは$\\mathbb{F}$-ベクトル空間と呼び、$V$の要素をベクトルvectorという。\n$\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V$と$k, l \\in \\mathbb{F}$に対して、\n(A1) $\\mathbf{u}, \\mathbf{v}$が$V$の要素であれば$\\mathbf{u}+\\mathbf{v}$も$V$の要素である。\n(A2) $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$\n(A3) $(\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})$\n(A4) $V$内の全ての$\\mathbf{u}$に対して、$\\mathbf{u} + \\mathbf{0} = \\mathbf{0} + \\mathbf{u} = \\mathbf{u}$を満たす$\\mathbf{0}$が$V$内に存在する。この時$\\mathbf{0}$を零ベクトルzero vectorと呼ぶ。\n(A5) $V$内の全ての$\\mathbf{u}$に対して$\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u} = \\mathbf{0}$を満たす$\\mathbf{v}$が$V$内に存在する。この時$\\mathbf{v}$を**$\\mathbf{u}$の負**negative of $\\mathbf{u}$と呼び、$\\mathbf{v} = -\\mathbf{u}$と表記する。\n(M1) $\\mathbf{u}$が$V$の要素であれば$k \\mathbf{u}$も$V$の要素である。\n(M2) $k(\\mathbf{u} + \\mathbf{v})=k\\mathbf{u} + k\\mathbf{v}$\n(M3) $(k+l)\\mathbf{u}=k\\mathbf{u}+ l\\mathbf{u}$\n(M4) $k(l\\mathbf{u})=(kl)(\\mathbf{u})$\n(M5) $1\\in \\mathbb{F}$に対して、$1\\mathbf{u} = \\mathbf{u}$\n説明 線形空間linear spaceという言葉も使われる。 当然ながらスカラー(体)が実数である必要はない。特に$\\mathbb{F} = \\mathbb{R}$の場合を実ベクトル空間real vector spaceと呼び、$\\mathbb{F} = \\mathbb{C}$の場合を複素ベクトル空間complex vector spaceと呼ぶ。\n数学部の線形代数学では主に$\\mathbb{R}^{n}$や$\\mathbb{C}^{n}$を扱う。$\\mathbb{R}^{n}$は実数$n$個の順序対を要素とするベクトル空間を意味し、即ち$n$次元ユークリッド空間を意味し、具体的に$\\mathbb{R}^{3}$は高校数学、微分積分学でよく見た3次元空間を意味する。\nベクトル空間となる集合は様々ある。関数の集合もベクトル空間となり得て、これを関数空間と呼ぶ。\n物理学では大きさと方向があるものをベクトルと呼ぶ。その概念を一般化したものが線形代数学のベクトルである。例えば大きさが$m\\times n$の実数行列を集めた集合$M_{m\\times n}(\\mathbb{R})$を考えると、$M_{m\\times n}(\\mathbb{R})$は上記の10個の規則を全て満たすことがわかる。したがって同じ大きさの行列を集めた集合はベクトル空間となり、各々の行列はその中でのベクトルとなる。このような抽象的なベクトル空間に初めて接したならば、行列もベクトルだという事実に驚くかもしれないが、これまでに座標空間のベクトルをどのように表記していたかを考えれば驚くこともない。\nある集合がベクトル空間であるかどうかを判断するには、上記の定義を満たしているか一つ一つ確かめればよい。一見ベクトル空間に思えるけれどもそうでない場合もあり、また一見ベクトル空間でなさそうに思えるけれども実はベクトル空間である場合もある。直感とは全く異なる場合があるので、問題を解く時は一つ一つしっかりと確認することが良い。また零ベクトル$\\mathbf{0}$とスカラー$0$は全く異なるものであるので、しっかり区別するようにしよう。通常、教科書ではベクトルは太字で表される。\n定理1 $V$をベクトル空間、$\\mathbf{u}$を$V$の要素とする。\n(1a) $V$の零ベクトルは唯一である。\n(1b) $\\mathbf{u}$の負は唯一である。\n証明 ベクトル空間の定義を利用した証\n明である。\n(1a) $\\mathbf{0},\\mathbf{0}^{\\prime}$が$V$の零ベクトルであるとする。するとベクトル空間の定義により次が成立する。\n$$ \\begin{align*} \\mathbf{0} \u0026amp;= \\mathbf{0} + \\mathbf{0}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\\\ \u0026amp;= \\mathbf{0}^{\\prime} + \\mathbf{0} \u0026amp;\u0026amp; \\text{by (A2)} \\\\ \u0026amp;= \\mathbf{0}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\end{align*} $$\nしたがって、二つの零ベクトルは互いに等しい。\n■\n(1b) $\\mathbf{v}, \\mathbf{v}^{\\prime}$が$\\mathbf{u}$の負であるとする。するとベクトル空間の定義により次が成立する。\n$$ \\begin{align*} \\mathbf{v} \u0026amp;= \\mathbf{v} + \\mathbf{0} \u0026amp;\u0026amp; \\text{by (A4)} \\\\ \u0026amp;= \\mathbf{v} + \\left( \\mathbf{u} + \\mathbf{v}^{\\prime} \\right) \u0026amp;\u0026amp; \\text{by (A5)} \\\\ \u0026amp;= \\left( \\mathbf{v} + \\mathbf{u} \\right) + \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A3)} \\\\ \u0026amp;= \\mathbf{0} + \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A5)} \\\\ \u0026amp;= \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\end{align*} $$\nしたがって、$\\mathbf{u}$の二つの負は互いに等しい。\n■\n定理2 $V$をベクトル空間、$\\mathbf{u}$を$V$の要素、$k$をスカラーとする。\n(2a) $0 \\mathbf{u} = \\mathbf{0}$\n(2b) $k \\mathbf{0} = \\mathbf{0}$\n(2c) $(-1) \\mathbf{u} = -\\mathbf{u}$\n(2d) もし$k \\mathbf{u} = \\mathbf{0}$であれば、$k = 0$か$\\mathbf{u} = \\mathbf{0}$である。\n証明 ベクトル空間の定義を利用した証明である。\n(2a) $$ \\begin{align*} \u0026amp;\u0026amp; 0\\mathbf{u} \u0026amp;= (0 + 0)\\mathbf{u} \\\\ \u0026amp;\u0026amp; \u0026amp;= 0\\mathbf{u} + 0\\mathbf{u} \u0026amp;\u0026amp;\\text{by (M3)} \\\\ \u0026amp; \u0026amp; \\\\ \\implies \u0026amp;\u0026amp; 0\\mathbf{u}+(-0\\mathbf{u}) \u0026amp;= 0\\mathbf{u} + 0\\mathbf{u} +(-0\\mathbf{u}) \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{0} \u0026amp;= 0\\mathbf{u} \u0026amp;\u0026amp;\\text{by (A5)} \\end{align*} $$\n■\n(2b) $$ \\begin{align*} \u0026amp;\u0026amp; k\\mathbf{0} \u0026amp;= k(\\mathbf{0} + \\mathbf{0}) \u0026amp;\u0026amp;\\text{by (A4)} \\\\ \u0026amp;\u0026amp; \u0026amp;= k\\mathbf{0} + k\\mathbf{0} \u0026amp;\u0026amp;\\text{by (M2)} \\\\ \u0026amp; \u0026amp; \\\\ \\implies \u0026amp;\u0026amp; k\\mathbf{0}+(-k\\mathbf{0}) \u0026amp;= k\\mathbf{0} + k\\mathbf{0} +(-k\\mathbf{0}) \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{0} \u0026amp;= k\\mathbf{0} \u0026amp;\u0026amp;\\text{by (A5)} \\end{align*} $$\n■\n(2c) $$ \\begin{align*} \\mathbf{u} + (-1)\\mathbf{u} \u0026amp;= 1 \\mathbf{u} + (-1)\\mathbf{u} \u0026amp;\u0026amp;\\text{by (M5)} \\\\ \u0026amp;= \\big( 1 + (-1) \\big) \\mathbf{u} \u0026amp;\u0026amp;\\text{by (M3)} \\\\ \u0026amp;= 0 \\mathbf{u} \\\\ \u0026amp;= \\mathbf{0} \u0026amp;\u0026amp;\\text{by (a2)} \\end{align*} $$\nすると**（A5）により$(-1)\\mathbf{u}$は$\\mathbf{u}$の負であり、（1b）**により$\\mathbf{u}$の負は唯一であるため、\n$$ (-1)\\mathbf{u} = -\\mathbf{u} $$\n■\n(2d) $k$は必ず$0$か$0$のどちらか一方の場合にのみ該当するので、二つの場合に分けて考える。\n$k=0$の場合\n結論を満たす。\n$k\\ne 0$の場合\n$k$が$0$でないため、$k$で割ることができる。したがって\n$$ \\begin{align*} \u0026amp;\u0026amp; k \\mathbf{u} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{u} \u0026amp;= \\frac{1}{k}\\mathbf{0} \\\\ \u0026amp;\u0026amp; \u0026amp;= \\mathbf{0} \u0026amp;\u0026amp; \\text{by (2b)} \\end{align*} $$\n■\n一緒に見る ベクトルの簡単な定義 抽象代数 線形代数学でのベクトル空間 抽象代数学でのベクトル空間 下記の文書で述べられている$F$-ベクトル空間は、実際に上記の文書のベクトル空間と何の差異もない。ただ視点が少し異なるだけで、線形代数学でのベクトル空間が直感的なユークリッド空間の抽象化であり、抽象代数学でのベクトル空間はそれを真の意味での\u0026rsquo;代数\u0026rsquo;として扱うことである。\n逆に$R$-モジュールは$F$-ベクトル空間のスカラー体$F$をスカラー環$R$に一般化することに意義があり、したがって$F$-ベクトルフィールドの歴史と意味に関心がないネーミングでそのアイデンティティを示している。群$G$の立場から見れば、環$R$と新しい演算$\\mu$が加えられたことであるため、その逆も加群加群である。\n抽象代数学でのR-モジュール 抽象代数学での$F$-ベクトル空間 Howard Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p202-203\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n体をよくわからなければ、簡単に$\\mathbb{F}=\\mathbb{R}$または$\\mathbb{F}=\\mathbb{C}$と考えればよい。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":282,"permalink":"https://freshrimpsushi.github.io/jp/posts/282/","tags":null,"title":"ベクトル空間の定義"},{"categories":"함수","contents":"定義 区間 $I \\subset \\mathbb{R}$ の二つの要素 $x_{1} , x_{2}$ と関数 $f : I \\to \\mathbb{R}$ および $0 \\le t \\le 1$ について、\n$f( t x_{1} + (1-t) x_{2}) \\le t f(x_{1}) + (1-t) f(x_{2})$ のとき、$f$ は $I$での凸関数と定義される。 $f( t x_{1} + (1-t) x_{2}) \\ge t f(x_{1}) + (1-t) f(x_{2})$ のとき、$f$ は $I$での凹関数と定義される。 説明 凸や凹には、上向きの凸や下向きの凹など、混乱しやすい表現が多いため、グラフの形状に対応させて**凸(convex)と凹(concave)**を英語のまま使用して記憶することを強く推奨する。式を見ただけでは一見馴染みのない定義に思えるが、内分の概念を考えれば、非常に直感的な定義として受け入れられるだろう。直感的に難しくない概念なので、式的な展開や説明が必要ない場合は、わざわざ定義を覚える必要もない。通常、中学校の二次関数から始まり、二階導関数の符号などを延々と見てきたため、その性質も親しみやすいはずだ。\n正直に言って 正直に言って、凹はあまり使われず、凸だけで考えればいいと思う。\n二階導関数 凸関数の二階導関数: $f$ が $I$ で二回微分可能とする。$f$ が $I$ で凸であることと $f '' (x) \\ge 0$ は必要十分条件である。\nここで、二回微分可能という条件が加わっていることに注目しよう。通常、例として $y = x^2$ や $y = \\ln {x}$ のような曲線が使用されるが、見逃しやすい点だが、私たちが再定義した凸関数では「連続」であることは言及されていないことに気づくだろう。\n一緒に見る 一般的な凸ベクトル関数 集合の凸 凸関数の様々な性質 ","id":262,"permalink":"https://freshrimpsushi.github.io/jp/posts/262/","tags":null,"title":"凸関数、凹関数"},{"categories":"수리물리","contents":"定義 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^3$ に対して**$\\mathbf{x}$と$\\mathbf{y}$の外積**cross product を定義する。\n$$ \\begin{align*} \\mathbf{x} \\times \\mathbf{y} =\u0026amp; (x_{2}y_{3} - x_{3}y_{2}, x_{3}y_{1} - x_{1}y_{3}, x_{1}y_{2} - x_{2}y_{1}) \\\\ =\u0026amp; \\det \\begin{bmatrix} \\mathbf{i} \u0026amp; \\mathbf{j} \u0026amp; \\mathbf{k} \\\\ x_{1} \u0026amp; x_{2} \u0026amp; x_{3} \\\\ y_{1} \u0026amp; y_{2} \u0026amp; y_{3} \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} 0 \u0026amp; -x_{3} \u0026amp; x_{2} \\\\ x_{3} \u0026amp; 0 \u0026amp; -x_{1} \\\\ -x_{2} \u0026amp; x_{1} \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ y_{3} \\end{bmatrix} \\end{align*} $$\n説明 ちなみに$\\mathbf{i} = (1,0,0)$、$ \\mathbf{j} = (0,1,0)$、$\\mathbf{k} = (0,0,1)$ だ。内積と同様に外積ももっと一般的な定義が可能だけど、実用的な面では通常三次元に限って考えることが多い。この三次元空間での定義はベクトル積という名前もあるけど、厳格に区別する時だけ使われる。最も使われるのは物理学で、トルクやローレンツ力などを表すときに頻繁に登場する。幾何学的な形も右ねじの法則を思い出せば簡単に想像できる。外積の性質をいくつか証明なしで紹介する。\n性質 $\\mathbf{x}, \\mathbf{y}, \\mathbb{z} \\in \\mathbb{R}^3$ と $k \\in \\mathbb{R}$ に対して以下が成り立つ。\n(1) $\\mathbf{x} \\times \\mathbf{x} = 0$\n(2) 反交換性anti commutativity: $\\mathbf{x} \\times \\mathbf{y} = -\\mathbf{y} \\times \\mathbf{x} $\n(3) $(k \\mathbf{x}) \\times \\mathbf{y} = k (\\mathbf{x} \\times \\mathbf{y}) = \\mathbf{x} \\times (k \\mathbf{y})$\n(4) $\\mathbf{x} \\times ( \\mathbf{y}+ \\mathbb{z} )= (\\mathbf{x} \\times \\mathbf{y}) + (\\mathbf{x} \\times \\mathbb{z})$\n(5) スカラー三重積: $(\\mathbf{x} \\times \\mathbf{y}) \\cdot \\mathbb{z} = \\mathbf{x} \\cdot ( \\mathbf{y} \\times \\mathbb{z})$\n(6) ベクトル三重積(bac-cab公式): $\\mathbf{x} \\times ( \\mathbf{y} \\times \\mathbb{z} ) = (\\mathbf{x} \\cdot \\mathbb{z}) \\mathbf{y} - (\\mathbf{x} \\cdot \\mathbf{y}) \\mathbb{z} $\n(7) $|| \\mathbf{x} \\cdot \\mathbf{y} ||^2 = (\\mathbf{x} \\cdot \\mathbf{x} ) ( \\mathbf{y} \\cdot \\mathbf{y} ) - ( \\mathbf{x} \\cdot \\mathbf{y} )^2$\n(8) $|| \\mathbf{x} \\times \\mathbf{y} || = || \\mathbf{x} || || \\mathbf{y} || \\sin{\\theta} $\n(9) $\\mathbf{x} \\times \\mathbf{y} \\ne \\mathbb{0}$ の場合、$\\mathbf{x} \\times \\mathbf{y} $は$\\mathbf{x}$と$\\mathbf{y}$に垂直である。\n交換法則が成り立たないため、直感的に理解しにくい性質が多い。問題を解いたり、紙に書きながら慣れるようにしよう。\n","id":256,"permalink":"https://freshrimpsushi.github.io/jp/posts/256/","tags":null,"title":"三次元ユークリッド空間における外積"},{"categories":"행렬대수","contents":"要旨1 行列$A$の行空間と列空間の次元は同じである。\n証明 $R$を$A$の行階段形行列とする。基本的な行操作は$A$の行空間と列空間の次元を変えないため、次の式が成立する。\n$$ \\begin{align*} \\dim \\big( \\mathcal{R}(A) \\big) \u0026amp;= \\dim \\big( \\mathcal{R}(R) \\big) \\\\ \\dim \\big( \\mathcal{C}(A) \\big) \u0026amp;= \\dim \\big( \\mathcal{C}(R) \\big) \\end{align*} $$\n従って、$R$の行空間と列空間の次元が同じであることを示せば十分である。しかし$R$の行空間は先頭1がある行、$R$の列空間は先頭1がある列で生成されるため、$R$の行空間と列空間の次元は同じである。\n■\n定義 行列$A$の行空間(列空間)の次元を ランクrankといい、次のように表示する。\n$$ \\text{rank}(A) = \\dim \\mathcal{R}(A) = \\dim \\mathcal{C}(A) $$\n行列$A$の零空間の次元を 無効次元nullityといい、次のように表示する。\n$$ \\text{nullity}(A) = \\dim \\mathcal{N}(A) $$\n説明 ランクは係数、無効次元は劣化次元と訳されることもある。\n一方で$\\text{rank}(A)$は、$A$を行階段形にしたときのピボットの数としても定義することができる。\n正方行列でない$m \\times n$の行列$A$を考えると、行空間は最大で$n$次元、列空間は最大で$m$次元になる。しかし、これら2つの値が同じで、それがランクであるため、次の式が成立する。\n$$ \\rank(A) \\le \\min(m,n) $$\n$\\rank(A) = \\min(m,n)$の場合、$A$がフルランクfull rankを持っていると言われる。フルランクを持たない場合はランク不足rank deficientと言われる。\n直感的に理解が難しい場合は、連立方程式の未知数を数えることから導き出された概念と考えると良い。定義自体は全く難しくないが、$ m \\ne n$の場合、特に零空間や係数、劣化次元などの概念が原書で学んだ人には意味を推測するのが非常に難しいレベルである。これらの概念を学ぶ理由は、後に続く線形代数学の応用を数学の言葉で簡単に表現するためである。複雑な理論が展開されるとき、列空間や零空間などの定義は相当な面積を節約し、より複雑な現象をカバーしてくれる。\nちなみに列空間は$\\text{Im} (A)$、つまり像imageとも呼ばれる。行列$A$を関数の概念として考える場合、それは$A \\in \\mathbb{R}^{m \\times n}$に対応する関数$T_{A}$を$T_{A} : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$としても見ることができる理由である。\n以下のランク-無効次元定理も、関数の概念として考えると理解が容易である。$\\text{rank} A = \\text{rank} A^{T}$であることを忘れないでください。\nランク-無効次元定理 行列$A \\in M_{ m \\times n }(\\mathbb{R})$に対して、次の式が成立する。\n$$ \\begin{align*} \\text{rank} (A) + \\text{nullity} (A) \u0026amp;= \\dim \\mathbb{R}^{n} = n \\\\ \\text{rank} (A^{T}) + \\text{nullity} (A^{T}) \u0026amp;= \\dim \\mathbb{R}^{m} = m \\end{align*} $$\n行列の次元定理とも呼ばれる。線形変換について一般化すると、次のようになる。\nベクター空間$V, W$と線形変換$T : V \\to W$に対して、次の式が成立する。\n$$ \\text{rank} (T) + \\text{nullity} (T) = \\dim (V) $$\n証明 $A$を$m \\times n$行列とする。すると、$A$の列が$n$個であるため、斉次線形システム$A \\mathbf{x} = \\mathbf{0}$は$n$個の未知数を持つ。従って、「先導変数の数 + 自由変数の数 = $n$」が成立する。先導変数の数は先導1の数と同じであり、これは行空間の次元と同じである。また、自由変数の数はパラメータの数と同じであり、これは零空間の次元と同じである。したがって、定理が成立する。\n■\n関連する話題 抽象代数学における核 零空間は$\\ker A$と表され、核Kernelとも呼ばれる。これは抽象代数学で扱われる一般的な核の概念を線形代数で特殊化した表現であり、これもまた$A$を関数として見ることに由来する。\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p278\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3021,"permalink":"https://freshrimpsushi.github.io/jp/posts/3021/","tags":null,"title":"行列のランク、零化次元"},{"categories":"행렬대수","contents":"定義1 $$ A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} $$\n行列 $A$に対して、$A$の行から作られる$m$個の$\\mathbb{R}^{n}$ベクターは\n$$ \\begin{align*} \\mathbf{r}_{1} =\u0026amp; \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\end{bmatrix} \\\\ \\mathbf{r}_{2} =\u0026amp; \\begin{bmatrix} a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\end{bmatrix} \\\\ \u0026amp;\\vdots \\\\ \\mathbf{r}_{m} =\u0026amp; \\begin{bmatrix} a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} \\end{align*} $$\n$A$の行ベクトルrow vectorsと呼ばれる。$A$の列から作られる$n$個の$\\mathbb{R}^{m}$ベクターは\n$$ \\mathbf{c}_{1} = \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix},\\quad \\mathbf{c}_{2} = \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\end{bmatrix},\\quad \\dots,\\quad \\mathbf{c}_{n} = \\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} $$\n$A$の列ベクトルcolumn vectorsと呼ばれる。\n$$ \\begin{align*} A =\u0026amp; \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{r}_{1} \\\\ \\mathbf{r}_{2} \\\\ \\vdots \\\\ \\mathbf{r}_{m} \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} \\mathbf{c}_{1} \u0026amp; \\mathbf{c}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{c}_{n} \\end{bmatrix} \\end{align*} $$\n$A$の行ベクトル$\\mathbf{r}_{1}, \\mathbf{r}_{2},\\dots,\\mathbf{r}_{m}$によって生成される$\\mathbb{R}^{n}$の部分空間を$A$の行空間row spaceと言い、以下のように表記する。\n$$ \\mathcal{R} (A) \\quad \\text{or} \\quad \\text{row}(A) $$\n$A$の列ベクトル$\\mathbf{c}_{1}, \\mathbf{c}_{2},\\dots,\\mathbf{c}_{n}$によって生成される$\\mathbb{R}^{m}$の部分空間を$A$の列空間column spaceと言い、以下のように表記する。\n$$ \\mathcal{C} (A) \\quad \\text{or} \\quad \\text{col}(A) $$\n同次連立一次方程式 $A \\mathbf{x} =\\mathbf{0}$の解集合を$A$の零空間null spaceと言い、以下のように表記する。\n$$ \\mathcal{N}(A) \\quad \\text{or} \\quad \\text{null}(A) $$\n説明 上記の概念は\n$$ \\begin{equation} A\\mathbf{x} = \\mathbf{b} \\end{equation} $$\n連立一次方程式を解くために考案された。つまり、線形代数学では$(1)$の解と$A$の行空間、列空間、零空間の関係に興味があるのだ。具体的には行空間の基底を求めることが線形システムを解くことに関連している。特に、行空間と列空間の次元をランクと言い、零空間の次元を無効次元と言う。\nなお、列空間は$\\text{Im} (A)$、すなわち像imageとも呼ばれる。行列$A$を関数の概念として考えるならば、$A \\in \\mathbb{R}^{m \\times n}$に対応する関数$T_{A}$は$T_{A} : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$としても見ることができるからだ。\n定理1 線形システム$A \\mathbf{x} = \\mathbf{b}$が解を持つための必要十分条件は$\\mathbf{b} \\in \\mathcal{C}(A)$である。\n定理2 $\\mathbf{x}_{0}$が$A\\mathbf{x} = \\mathbf{b}$のある解だとしよう。$S= \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{k} \\right\\}$を$\\mathcal{N}(A)$の基底としよう。そうすると、$A\\mathbf{x} = \\mathbf{b}$の全ての解は下記のように表現できる。\n$$ \\begin{equation} \\mathbf{x} = \\mathbf{x}_{0} + c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{k}\\mathbf{v}_{k} \\end{equation} $$\n逆に、全ての定数$c_{1}, c_{2}, \\dots, c_{k}$に対して、上記の$\\mathbf{x}$は$A\\mathbf{x} = \\mathbf{b}$の解である。\n$(2)$を$A \\mathbf{x} = \\mathbf{b}$の一般解general solutionという。$\\mathbf{x}_{0}$を$A \\mathbf{x} = \\mathbf{b}$の特殊解particular solutionという。そして、$c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{k}\\mathbf{v}_{k}$を$A \\mathbf{x} = \\mathbf{0}$の一般解という。\nこれらの定理から、非同次線形システムの一般解はある特殊解と同次線形システムの一般解の和として表されることが分かる。\n参照 抽象代数学における核 零空間は$\\ker A$と書き、核kernelとも呼ばれる。これは抽象代数学で扱われる一般的な核の概念を線形代数学で特殊化した表現であり、これも$A$を関数と見なしたものだ。\nHoward Anton, Chris Rorres, Anton Kaul, Elementary Linear Algebra: Applications Version(12th Edition). 2019, p263-267\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":254,"permalink":"https://freshrimpsushi.github.io/jp/posts/254/","tags":null,"title":"行空間、列空間、零空間"},{"categories":"복소해석","contents":"定義 $z$ を $z=a+ib(a,b\\in \\mathbb{R})$ の複素数としよう。\n$\\overline{z}$ を以下のように定義し、$z$ の共役複素数と呼ぶ。 $$ \\overline{z}:=\\overline{a+ib}=a-ib $$\n説明 元の複素数に $i$ ではなく $-i$ を代入したもの、複素平面で実数軸に対する反射として説明できる。共役という言葉は、足したときに実数を作り出す一対であるという点から名付けられたようだ。共役複素数は複素解析を学ぶ際に最初に出会う概念の一つだが、実際にはすぐに使うことはないため、学習を怠ることが多い。しかし、これらの性質は単に学ぶだけではなく、反復を通じて習得することが重要である。通常、[5] 以降は本でも言及されないので、生さえずし屋で学んでおき、利益を得るようにしよう。\n性質 $z_{1}$, $z_{2}$, $z \\in \\mathbb{C}$ とする。すると以下の等式が成り立つ。\n[1]: $(z+\\overline{z}) = 2 \\Re(z) \\in \\mathbb{R}$ [2]: $\\overline{z_{1} + z_{2}} = \\overline{z_{1}} + \\overline{z_{2}}$ [3]: $\\overline{z_{1} z_{2}} = \\overline{z_{1}} \\cdot \\overline{z_{2}}$ [4]: $\\overline{ \\overline{z} } = z$ [5]: $z \\overline{z} = |z|^2$ [6]: $\\overline{ \\left( { \\dfrac{1}{z} } \\right) } = \\dfrac{1}{\\overline{z}}$ [7]: $\\overline{ \\left( \\dfrac{z_{1}}{z_{2}} \\right) } = \\dfrac{\\overline{z_{1}}}{\\overline{z_{2}}}$ [8]: $\\overline{ \\sin{ z } } = \\sin{\\overline{z}}$ [9]: $\\overline{ \\cos{ z } } = \\cos{\\overline{z}}$ [10]: $\\overline{ e^{ z } } = e^{\\overline{z}}$ [11]: $\\overline{ \\cosh{z} } = e^{\\overline{z}}$ [12]: $\\overline{ \\tan{ z } } = \\tan{\\overline{z}}$ 証明 証明に移る前に変数を $z_{1} = x_{1} + i y_{1}$, $z_{2} = x_{2} + i y_{2}$, $z = x + i y$ としよう。\n[1] $$ z +\\overline{z} = (x+iy)+(x-iy) = 2x $$\nよって、$(z+\\overline{z})=2x\\in \\mathbb{R}$である。\n■\n[2] $$ \\begin{align*} \\overline{z_{1} + z_{2}} =\u0026amp; \\overline{ ( x_{1} + i y_{2} ) + ( x_{2} + i y_{2} ) } \\\\ =\u0026amp; \\overline{ ( x_{1} + x_{2} ) + i ( y_{1} + y_{2} ) } \\\\ =\u0026amp; ( x_{1} + x_{2} ) - i ( y_{1} + y_{2} ) \\\\ \u0026amp;=(x_{1} - i y_{1}) + (x_{2} - i y_{2}) \\\\ =\u0026amp; \\overline{z_{1}} + \\overline{z_{2}} \\end{align*} $$\n■\n[3] $$ \\begin{align*} \\overline{z_{1} z_{2}} =\u0026amp; \\overline{ ( x_{1} + i y_{1} ) ( x_{2} + i y_{2} ) } \\\\ =\u0026amp; \\overline{( x_{1} x_{2} - y_{1} y_{2} ) + i ( x_{1} y_{2} + y_{1} x_{2} )} \\\\ =\u0026amp; ( x_{1} x_{2} - y_{1} y_{2} ) - i ( x_{1} y_{2} + y_{1} x_{2} ) \\\\ =\u0026amp; ( x_{1} - i y_{1} ) ( x_{2} - i y_{2} ) \\\\ =\u0026amp; \\overline{( x_{1} + i y_{1} )} \\ \\overline{( x_{2} + i y_{2} )} \\\\ =\u0026amp; \\overline{z_{1}} \\cdot \\overline{z_{2}} \\end{align*} $$\n■\n[4] $$ \\overline{ \\overline{ z } } = \\overline{ x - i y } = x + i y =z $$\n■\n[5] $$ z \\overline{z} = (x + iy ) ( x - i y )= x^2 + y^2 =|z|^2 $$\n■\n[6] $$ \\overline{ \\left( { {1} \\over {z} } \\right) } = \\overline{ \\left( {1} \\over { x + iy } \\right) } = \\overline{{x - i y} \\over {x ^ 2 + y^2 }} = {{x + i y} \\over {x ^ 2 + y^2 }} = {{1} \\over {x - i y }} = {{1} \\over { \\overline{z} }} $$\n■\n[7] [3], [6]によると $$ \\overline{ \\left( { {z_{1}} \\over { z_{2} } } \\right) } = \\overline{ z_{1} { {1} \\over { z_{2} } }}=\\overline{z_{1}}\\cdot \\overline{ \\left( {1} \\over { z_{2} } \\right) } = \\overline{z_{1}}\\cdot { 1 \\over \\overline{z_{2}} } = {{\\overline{z_{1}}} \\over { \\overline{z_{2}} }} $$ ■\n[8] [9] [8]と[9]の証明は本質的に同じなので、[9]の証明は省略する。\n$$ \\begin{align*} \\overline{ \\sin{ z } } =\u0026amp; \\overline{\\sin{(x+ i y)}} \\\\ =\u0026amp; \\overline{ \\sin{x} \\cosh{y} - i \\cos{x} \\sinh{y} } \\\\ =\u0026amp; { \\sin{x} \\cosh{y} + i \\cos{x} \\sinh{y} } \\\\ =\u0026amp; \\sin{(x-iy)} \\\\ =\u0026amp; \\sin{ \\overline{z} } \\end{align*} $$\n■\n[10] オイラーの公式によると、以下が成り立つ。\n$$ \\overline{e^z} = \\overline{\\cos{x} + i \\sin{y}} = \\cos{x} - i \\sin{y} =e^{\\overline{z}} $$\n■\n[11] [7], [10]によると、以下が成り立つ。\n$$ \\overline{\\cosh{z}} = \\overline{\\left( \\dfrac{e^{z} + e^{-z}}{2} \\right)} = \\dfrac{ e^{\\overline{z}} + e^{-\\overline{z}} } {2} = \\cosh\\overline{z} $$\n■\n[12] [7], [8], [9]によると、以下が成り立つ。\n$$ \\overline{ \\tan{z} } = \\overline{ \\left( { \\sin{z} } \\over { \\cos{z} }\\right) } = {{ \\overline{\\sin{z}} } \\over { \\overline{\\cos{z}} }} = {{ \\sin{ \\overline{z} } } \\over { \\cos { \\overline{z} } }} = \\tan{ \\overline{z} } $$\n■\n補足 証明の過程から分かるように、[11]は他の双曲関数に対しても適用できる。特定の関数だけが重要で別に証明が必要なわけではなく、このような良い性質を容易に導出できることを示すために証明を残した。三角関数も同様に、これらの良い性質を自ら簡単に導き出せるので、自分で試してみよう。\n[13]: $\\dfrac{1 + i}{1 - i } = i$ [14]: $\\dfrac{1 - i}{1 + i } = -i$ [15]: $\\dfrac{1}{i } = -i$ [16]: $i \\cdot (- i ) = 1$ 公式ではないが、これらの虚数の計算は非常に頻繁に使用されるため、マスターすると計算量を劇的に減らすことができる。特に[15]は、約分や両辺に虚数を掛ける状況で非常に便利だ。\n","id":245,"permalink":"https://freshrimpsushi.github.io/jp/posts/245/","tags":null,"title":"共役複素数"},{"categories":"교과과정","contents":"概要 $$ (x+y)^{n} = \\sum_{r=0}^{n} {_n C _r} x^{r} y^{n-r} $$ ここで、${_n C _r}$ を 二項係数Binomial Coefficientと定義する。 $$ {_n C _r} = \\binom{n}{r} = {{ n! } \\over { r ! (n-r)! }} $$\n説明 高校で学ぶにはとても役に立つもので、学んだ直後から多くの場面で使用される定理だ。その柔軟性から、多くの公式を一気に導出でき、分野を問わず広く使用される。\n証明 $(x+y)^{n}$ を展開するとき、$x^{r} y^{n-r}$ の係数は $$ (x+y)^{n} = (x+y)(x+y)(x+y) \\cdots (x+y) $$ $(x+y)$ の各 $x$ を $n$ 回、$y$ を $n-r$ 回選択することと同じである。したがって、組合せ $_n C _r$ が $x^{r} y^{n-r}$ の係数になるので、 $$ (x+y)^{n} = \\sum_{r=0}^{n} {_n C _r} x^{r} y^{n-r} $$\n■\n","id":218,"permalink":"https://freshrimpsushi.github.io/jp/posts/218/","tags":null,"title":"二項定理の証明"},{"categories":"해석개론","contents":"まとめ1 関数 $f$ が閉区間 $[a,b]$ で連続だとしよう。\n(1) 関数 $\\displaystyle F(x) = \\int_{a}^{x} f(t) dt$ は $[a,b]$ で連続で、$(a,b)$ で微分可能で、$\\displaystyle {{dF(x)} \\over {dx}} = f(x)$ を満たす。\n(2) $f$ の任意の不定積分 $F$ について、$\\displaystyle \\int_{a}^{b} f(x) dx = F(b) - F(a)$\n説明 もちろん、微分、積分という言葉を使うからには、これらの関係を簡単に推測できる。しかし、英語では differential と integral で全く関係ない上に、概念も特に似ていない。\n微積分学の基本定理は、この微分と積分が、実際には互いに逆の演算であることを示している。\n証明 (1) 積分の平均値定理により、$\\displaystyle f(c) = {{1} \\over {h}} \\int_{x}^{x+h} f(t) dt$ を満たす $c$ が $x, x+h$ の間に存在する。\n$h \\to 0$ のとき、$c \\to x$ になるので、\n$$ \\lim_{h \\to 0} {{1} \\over {h}} \\int_{x}^{x+h} f(t) dt = \\lim_{h \\to 0} f(c) = f(x) $$\n一方、$\\displaystyle F(x+h) - F(x) = \\int_{a}^{x+h} f(t) dt - \\int_{a}^{x} f(t) dt = \\int_{x}^{x+h} f(t) dt$ ので、\n$$ {{1} \\over {h}} \\int_{x}^{x+h} f(t) dt = { {F(x+h) - F(x)} \\over {h} } $$\n従って、\n$$ \\lim_{h \\to 0} { {F(x+h) - F(x)} \\over {h} } = F ' (x) = f(x) $$\n■\n(2) $F$ は $f$ の不定積分なので $\\displaystyle \\int_{a}^{b} f(t) dt = F(b) + C$ となり、\n$$ \\int_{a}^{a} f(t) dt = F(a) + C $$\n両辺を引くと、\n$$ \\int_{a}^{b} f(x) dx = F(b) - F(a) $$\n■\n参照 解析学における微積分学の基本定理 (1) 解析学における微積分学の基本定理 (2) 慶北国立大学基礎教育院, 理工学生のための大学数学 (2012), p108-109\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":213,"permalink":"https://freshrimpsushi.github.io/jp/posts/213/","tags":null,"title":"積分学の基本定理の証明"},{"categories":"수리물리","contents":"定義 自然数$n \\in \\mathbb{N}$に対し、実数の集合$\\mathbb{R}$のデカルト積$\\mathbb{R}^{n}$をユークリッド空間と呼ぶ。\n$$ \\mathbb{R}^{n} = \\mathbb{R} \\times \\cdots \\times \\mathbb{R} $$\n$\\mathbb{R}^{1}$は実数空間または数直線と呼ばれる。 $\\mathbb{R}^{2}$は平面と呼ばれる。 $\\mathbb{R}^{3}$は**$3$次元空間**と呼ばれる。 ここで、$\\mathbb{N} := \\left\\{ 1, 2, 3, \\cdots \\right\\}$は自然数を全部集めた集合を意味する。$\\mathbb{R}$は実数を全部集めた集合を意味する。\n説明 ユークリッド空間は、幾何学原論の著者であるユークリッドの名を冠した空間で、私たちが生きている$3$次元空間を含め、平面、数直線はもちろん、それ以上の多次元空間まで表現する空間だ。\n私たちの生活と密接に関連しているため、多くの理論で全体の空間としてユークリッド空間が仮定されることが多い。もちろん、ユークリッド空間だけでは、深く複雑な科学技術の理論を全て説明することはできず、YouTubeに生息する疑似数学者、疑似科学者たちのおもちゃになることもある。\n多次元への拡張は、時空間が$\\mathbb{R}^{1+3}$であるからとか、弦理論で$\\mathbb{R}^{11}$となるといった派手な目的がなくとも必ず必要なものだ。代表的には統計学での応用もあり、空間自体が$3$次元であっても、速度や加速度を導入するために$9$次元が必要になることもある。 もしこの文章を読んでいる読者が主に工学、または実用的な物理学までの学びを希望しているなら、実際には通常のユークリッド空間を抜け出すことはないかもしれない。式や証明なしにロマンを歌う教養書を見て夢を育てても仕方がない。しかし、それを越えた数学や物理の世界に興呀があれば、ユークリッド空間を足場と見なして早く慣れるべきだ。\n","id":205,"permalink":"https://freshrimpsushi.github.io/jp/posts/205/","tags":null,"title":"ユークリッド空間"},{"categories":"해석개론","contents":"公理1 集合 $E \\subset \\mathbb{R}$ が空集合ではなく、もし $E$ が上界を持つならば、上限 $\\sup(E) \u0026lt; \\infty$ が存在する。\n説明 体の公理や順序の公理は、知っていることを複雑に書き直した感じだが、完備性の公理は一見そうではない。まずはここで登場する単語に対する定義が必要だろう。\n定義 $E$ の全ての元 $a$ に対して $a \\le M$ が成り立つならば、$E$ を上に有界bounded aboveと呼ぶ。このような条件を満たす $M$ を全て、$E$ の上界upper boundと呼ぶ。$\\sup(E)$ は$E$ の最小の上界であり、全ての$E$ の上界 $M$ に対して $\\sup (E) \\le M$ を満たす数である。これを$E$ の最小上界supremum, 上限と呼ぶ。\n反対の不等号の場合は、以下のようになる。\n$E$ の全ての元 $a$ に対して $a \\ge m$ が成り立つならば、$E$ を下に有界bounded belowと呼ぶ。このような条件を満たす $m$ を全て、$E$ の下界lower boundと呼ぶ。$\\inf(E)$ は$E$ の最大の下界であり、全ての$E$ の下界 $m$ に対して $\\inf (E) \\ge m$ を満たす数である。これを$E$ の最大下界infimum, 下限と呼ぶ。\n突然定義がたくさん出てきて混乱するかもしれないが、根本的に我々の概念を揺るがすわけではない。ただどんな集合がいつ限界を持つか、その時限界を何と呼ぶかを定義するだけだ。\n完備性公理に戻ってみると、上で紹介された定義を繰り返す感じがする。違いは簡単だ。定義では存在する時に何と呼ぶかだけを言っていて、本当に存在するかについては話していない。完備性公理では、その「存在」について話している。\nしかし、これが公理であるべきかという疑問が生じるだろう。公理として必要なほど基本的な事実なのか？証明できないのか？定義を読むと、定義によって当然のようにこのような上限が存在し、証明できるように思えるが、そうではない。\n反証 $E$ が上に有界としたならば、条件を満たす上界 $M$ があり、その中で最も小さい値が存在して、上限 $\\sup(E)$ が存在するだろう。しかし、逆に考えてみると、$\\sup(E)$ は$-M$ の中で最も大きい値、つまり上限である。そもそも最も小さい値が存在するという主張自体が上限の存在性を根拠にしている。これにより、循環論法に陥るしかない。\n■\n上でも下でも、大きさに関係なく、議論の方向は逆になる。結局、我々はこのような上限や下限の存在性を明らかにできない。だから、新しい公理として完備性公理を作り出すしかなかった。\n定理 整数の集合 $\\mathbb{Z}$ の部分集合 $E$ が上限を持つならば $\\sup(E) \\in E$\n完備性公理がなければ、こんなにも自明な事実でさえ、その仮定が疑わしいために信じられない。\n完備？ 完備とは、Completeの和訳で、実数空間 $\\mathbb{R}$ を超えて距離空間で一般化されるとき、コーシー列の収束点を含む空間を完備空間と定義する。ただし、日常の中でCompleteという英語は完全に(完)備える(備)という意味で使われることは少なく、「完成」や「完結」など、何か続いているもののその終わりと共に使われることが多い。これは、述べられたように、収束点の存在(その空間内に)を保証する点から、completeという表現が適切であることが分かる。\nもちろん、コーシー列を捉えることと$E \\subset \\mathbb{R}$ を捉えることは異なるが、$\\mathbb{R}$ が可分性を持つといった説明はまだ早い。後でそんなことをまた学ぶんだと思って、先に進んでもいい。\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), p16-18\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":180,"permalink":"https://freshrimpsushi.github.io/jp/posts/180/","tags":null,"title":"解析学の三つの公理：完備性公理"},{"categories":"교과과정","contents":"数式 初項が$a$で、公比が$r$の等比数列$a_{n} = a r^{n-1}$について、 $$ \\sum_{k=1}^{n} a_{k}= {{a (1- r^{n} ) } \\over {1-r}} $$\n証明 $\\displaystyle S= \\sum_{k=1}^{n} a_{k}$としよう。すると、 $$ S= a + ar + \\cdots + ar^{n-2} + ar^{n-1} $$ 両辺に$r$をかけると、 $$ rS= ar + a r^2 + \\cdots + ar^{n-1} + ar^{n} $$ ここで、上の二つの式から両辺を引くと、 $$ S - rS = (1-r)S = a- a r^n $$ 右辺の二つの式を$1-r$で割ると、 $$ S=\\sum_{k=1}^{n} a_{k}= {{a (1- r^{n} ) } \\over {1-r}} $$\n■\n説明 等差数列の和とは異なり、この公式自体が非常によく使われる。証明法も少し異なるが、別にもっと勉強するほど難しいわけではない。\n等比級数は幾何級数Geometric Seriesとも言われる。人々がよく「幾何級数的に」と言うが、それがこの言葉だ。つまり、ほとんどの数学に詳しくない人々が「幾何級数的に」という言葉を間違って使っているのだ。\n等比級数で$n$が無限大になるとどうなるか？$|r|\u0026lt;1$ならば収束し、$|r|\u0026gt;1$ならば発散するだろう。等比級数でこのように$n \\to \\infty$を考えることを「無限等比級数」という。\n無限等比級数 $|r|\u0026lt;1$の時、 $$ \\sum_{n=1}^{\\infty} a r^{n-1} = { a \\over {1-r}} $$\n$n \\to \\infty$から$ar^n \\to 0$になるので、等比級数から自然に導かれる。\n","id":170,"permalink":"https://freshrimpsushi.github.io/jp/posts/170/","tags":null,"title":"等比数列の和を求める"},{"categories":"수리물리","contents":"定義1 原点から観察点までのベクトルを分離ベクトルseparation vectorという。\n$$ \\bcR = \\mathbf{r} - \\mathbf{r}^{\\prime} $$\n説明 原点ベクトルsource vector $\\mathbf{r}^{\\prime}$: 電荷や電流が存在する場所。つまり、電磁場を生成する起源の座標を表すベクトル。 位置ベクトルposition vector $\\mathbf{r}$: 電場 $\\mathbf{E}$や磁場 $\\mathbf{B}$を測定する場所の座標を表すベクトル。 分離ベクトル $\\bcR$: 位置ベクトルと原点ベクトル（起源ベクトル）の差。 分離ベクトルの表示には標準がなく、ばらばらである。記号を特に定めずに$\\mathbf{r} - \\mathbf{r}^{\\prime}$と書く場合もある。エビ寿司屋では、グリフィスの電磁気学と同様に、筆記体$r$(Kaufmannフォント) $\\bcR$で表示する。その他に使用される文字には、ギリシャ文字のエータ$\\eta$などがある。分離ベクトルの大きさと単位ベクトルは次のとおりである。\n$$ \\left| \\bcR \\right| = \\cR = \\left| \\mathbf{r} - \\mathbf{r}^{\\prime} \\right| $$\n$$ \\crH = \\dfrac{\\bcR}{\\cR} = \\dfrac{\\mathbf{r} - \\mathbf{r}^{\\prime}}{ \\left| \\mathbf{r} - \\mathbf{r}^{\\prime} \\right|} $$\n直交座標系では、以下のようになる。\n$$ \\bcR = (x-x^{\\prime})\\hat {\\mathbf{x}} + (y-y^{\\prime})\\hat{\\mathbf{y}} + (z-z^{\\prime})\\hat{\\mathbf{z}} $$ $$ \\cR = \\sqrt{ (x-x^{\\prime})^2 + (y-y^{\\prime})^2 + (z-z^{\\prime})^2 } $$ $$ \\crH = \\dfrac{ (x-x^{\\prime})\\hat {\\mathbf{x}} + (y-y^{\\prime})\\hat{\\mathbf{y}} + (z-z^{\\prime})\\hat{\\mathbf{z}}}{\\sqrt{ (x-x^{\\prime})^2 + (y-y^{\\prime})^2 + (z-z^{\\prime})^2 }} $$\n例 原点(2,8,7)から観察点(4,6,8)までの分離ベクトル$\\bcR$を求めなさい。また、その大きさと単位ベクトルを求めなさい。\n$$ \\bcR=(4,6,8)-(2,8,7)=(2,-2,1)=2\\hat{\\mathbf{x}} -2\\hat{\\mathbf{y}}+\\hat{\\mathbf{z}} $$\n$$ \\cR=\\sqrt{ 2^2+ (-2)^2+1^2}=\\sqrt{4+4+1}=\\sqrt{9}=3 $$\n$$ \\crH=\\left( \\dfrac{2}{3}, - \\dfrac{2}{3},\\dfrac{1}{3} \\right) = \\dfrac{2}{3}\\hat{\\mathbf{x}} -\\dfrac{2}{3}\\hat{\\mathbf{y}}+\\dfrac{1}{3}\\hat{\\mathbf{z}} $$\nデイビッド・J・グリフィス, 『基礎電磁気学』（金進丞 訳）(第4版). 2014, p9-10\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":141,"permalink":"https://freshrimpsushi.github.io/jp/posts/141/","tags":null,"title":"分離ベクトル"},{"categories":"해석개론","contents":"まとめ オイラーの公式: $$ { e }^{ ix }= \\cos x + i \\sin x $$\nオイラーの等式: $$ { e }^{ i\\pi }+1=0 $$\n説明 オイラーの公式Euler\u0026rsquo;s Formulaは、それ自体の形がすごく奇妙で、オイラー自身もどこで使われるか分からなかったけど、今では多くの分野で使われていて、その有用性を要約するのが難しい程だ。虚数が学界でまだうまく受け入れられていなかった当時の発見だと考えれば、さらに驚くべきだ。導出自体は指数関数、サイン関数、コサイン関数のテイラー展開を通じて簡単にできる。\n$$ { { e ^ x } }=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ n } }{ n! } } $$\n$$ \\sin x=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n+1 } }{ (2n+1)! }{ { (-1) }^{ n } } } $$\n$$ \\cos x=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n } }{ (2n)! }{ { (-1) }^{ n } } } $$\n導出（オイラーの公式） $$ \\begin{align*} { e }^{ ix } =\u0026amp; \\sum _{ n=0 }^{ \\infty }{ \\frac { { (ix) } ^{ n } }{ n! } } \\\\ =\u0026amp;\\frac { { (ix) } ^{ 0 } }{ 0! }+\\frac { { (ix) } ^{ 1 } }{ 1! }+\\frac { { (ix) } ^{ 2 } }{ 2! }+\\frac { { (ix) } ^{ 3 } }{ 3! }+\\frac { { (ix) } ^{ 4 } }{ 4! }+ \\cdots \\\\ =\u0026amp;\\frac { 1 }{ 0! }+\\frac { ix }{ 1! }-\\frac { { x }^{ 2 } }{ 2! }-\\frac { i { x }^{ 3 } }{ 3! }+\\frac { { x } ^{ 4 } }{ 4! }+ \\cdots \\\\ =\u0026amp; \\left( \\frac { 1 }{ 0! } - \\frac { { x } ^{ 2 } }{ 2! }+\\frac { { x } ^{ 4 } }{ 4! }-\\frac { { x } ^{ 6 } }{ 6! }+\\cdots \\right) + i\\left( \\frac { x }{ 1! } - \\frac { { x } ^{ 3 } }{ 3! }+\\frac { { x } ^{ 5 } }{ 5! }-\\frac { { x } ^{ 7 } }{ 7! }+\\cdots \\right) \\\\ =\u0026amp; \\cos x + i \\sin x \\end{align*} $$\nしたがって、\n$$ { e }^{ ix }= \\cos x + i \\sin x $$\n■\n特に$x=\\pi$を代入すると、いわゆる\u0026rsquo;世界で最も美しい等式\u0026rsquo;であるオイラーの等式を得る。また、オイラーの等式をうまくいじると、虚数単位$i$の$i$乗、つまり$i^i$の値も求めることができる。驚くべきことに、その値は実数で、証明は次の通り。\n証明 $$ \\begin{align*} \u0026amp;\u0026amp; { e }^{ i\\pi }+1 =\u0026amp; 0 \\\\ \\implies \u0026amp;\u0026amp; { e }^{ i\\pi }=\u0026amp;-1 \\\\ \\implies \u0026amp;\u0026amp; { e }^{ \\frac { i\\pi }{ 2 } } =\u0026amp; \\sqrt { -1 } \\\\ \\implies \u0026amp;\u0026amp; { \\left( { e } ^{ \\frac { i\\pi }{ 2 } } \\right) }^{ i } =\u0026amp; { \\sqrt { -1 } }^{ i } \\\\ \\implies \u0026amp;\u0026amp; { e }^{ \\frac { i\\pi }{ 2 }i } =\u0026amp; { i } ^{ i } \\\\ \\implies \u0026amp;\u0026amp; { i }^{ i } =\u0026amp; { e }^{ -\\frac { \\pi }{ 2 } } \\\\ \\implies \u0026amp;\u0026amp; { i }^{ i } =\u0026amp; \\frac { 1 }{ \\sqrt { { e }^{ \\pi } } } \\end{align*} $$\n■\n","id":112,"permalink":"https://freshrimpsushi.github.io/jp/posts/112/","tags":null,"title":"微積分学におけるオイラーの公式"},{"categories":"함수","contents":"定義 以下の二つの条件を満たす関数をディラックデルタ関数という。\n$$ \\delta (x) = \\begin{cases} 0, \u0026amp; x\\neq 0 \\\\ \\infty , \u0026amp; x=0 \\end{cases} $$\n$$ \\int_{-\\infty}^{\\infty}{\\delta (x) dx}=1 $$\n説明 ※クロネッカーデルタと間違えないように注意が必要だ。\n工学では、単位インパルス関数unit impulse functionと呼ばれる。正確に言うと、数学的にディラックデルタ関数は関数ではない。それは0で無限大に発散するからだが、グリフィスの教科書ではこのように説明されている。「デルタ関数は$x=0$で値が無限大になるため、技術的には関数ではない。数学文献では、一般化した関数generalized functionまたは分布distributionと呼ばれる」上の式だけを見てもデルタ関数が何かを一度に理解するのは難しい。以下の図を見れば、その幾何学的な意味を把握するのに役立つだろう。 もう少し直感的に説明すると以下のようだ。高さが$n$、幅が$\\displaystyle \\frac{1}{n}$の長方形$R_{n}(x)$、または高さが$n$、底辺が$\\displaystyle \\frac{2}{n}$の二等辺三角形$T_{n}(x)$のような関数列の極限\nデルタ関数が何か理解できたら、デルタ関数の特性を見てみよう。関数$f(x)$がデルタ関数ではない一般的な関数だとすると、$f(x)\\delta (x)$の値は$x=0$を除くすべての場所で$0$である。（$\\because$ $\\delta (x)$が$x=0$を除くすべての場で$0$だから）つまり、$x=0$でのみ値が存在する。したがって、下記の式が成り立つ。\n$$ f(x)\\delta (x) = f(0) \\delta (x) $$\n積分形で表すと\n$$ \\displaystyle{ \\int_{-\\infty}^{\\infty} f(x) \\delta (x) dx = f(0) \\int_{-\\infty}^{\\infty} \\delta (x) dx = f(0)} $$\n一般的な場合を表すために、デルタ関数の峰を$x=0$から$x=a$に移すと、以下のようになる。\n$$ \\delta (x-a) = \\begin{cases} 0, \u0026amp; x\\neq a \\\\ \\infty , \u0026amp; x=a \\end{cases} $$\n$$ \\displaystyle{ \\int_{-\\infty}^{\\infty}{\\delta (x-a) dx}=1 } $$\n$$ f(x)\\delta (x-a) = f(a) \\delta (x-a) $$\n$$ \\displaystyle{ \\int_{-\\infty}^{\\infty} f(x) \\delta (x-a) dx = f(a)} $$\n3次元では、デルタ関数は以下のようになる。\n$$ \\int f( \\mathbf{r} ) \\delta ^3 (\\mathbf{r}-\\mathbf{a}) d\\tau = f(\\mathbf{a}) $$\nこの時、\n$$ \\int \\delta ^3 (\\mathbf{r} ) d\\tau =1 $$\n$$ \\delta ^3 (\\mathbf{r})=\\delta (\\mathbf{x}) \\delta (\\mathbf{y}) \\delta (\\mathbf{z}) $$\n","id":103,"permalink":"https://freshrimpsushi.github.io/jp/posts/103/","tags":null,"title":"ディラックのデルタ関数"},{"categories":"함수","contents":"定義 次のように定義された関数 $\\Gamma : (0, \\infty) \\to \\mathbb{R}$ をガンマ関数と言う。 $$ \\Gamma (x) := \\int_{0}^{\\infty} t^{x-1} e^{-t} dt $$\n説明 上の式において積分に焦点を置くと、オイラー積分とも呼ばれる。ガンマ関数は、純粋数学だけでなく物理学、統計学などでも非常に重要な関数として有名である。非常に多くの興味深い性質を持っているが、最も代表的なのは階乗を実数に対して一般化する概念である点である。\n定理 階乗の一般化としてのガンマ関数 自然数 $n \\in \\mathbb{N}$ に対して $\\Gamma (n) = (n-1)!$ が成り立つ。\n証明 戦略：ガンマ関数が階乗の形で表されることだけを示せば、一般化に関しては十分である。\nガンマ関数の定義により $$ \\Gamma (n) = \\int_{0}^{\\infty} t^{n-1} e^{-t} dt $$\nCase 1. $n=1$ $$ \\Gamma (1) = \\int_{0}^{\\infty} e^{-t} dt = 1 $$ これは、$0! = 1$ と同じ意味で受け取ることができる。\nCase 2. $n\u0026gt;1$\n部分積分法により $$ \\begin{align*} \\Gamma (n) =\u0026amp; \\int_{0}^{\\infty} t^{n-1} e^{-t} dt \\\\ =\u0026amp; \\left[ -t^{n-1} e^{-t} \\right] _{0} ^{\\infty} - \\int_{0}^{\\infty} -(n-1)t^{n-2} e^{-t} dt \\\\ =\u0026amp; (n-1) \\int_{0}^{\\infty} t^{n-2} e^{-t} dt \\\\ =\u0026amp; (n-1) \\Gamma (n-1) \\end{align*} $$\n両方のケースをまとめると $$ \\begin{align*} \\Gamma (n) =\u0026amp; (n-1) \\cdot (n-2) \\cdots 2\\cdot\\Gamma (2) \\\\ =\u0026amp; (n-1) \\cdot (n-2) \\cdots 2\\cdot 1\\cdot \\Gamma (1) \\\\ =\u0026amp; (n-1)! \\end{align*} $$\n■\n参考 ガンマ関数の導出 物理学でのガンマ関数 多項式のラプラス変換の結果としてのガンマ関数 ガンマ関数に対するオイラーの反射公式 ガンマ関数に対するワイエルシュトラスの無限積 ","id":95,"permalink":"https://freshrimpsushi.github.io/jp/posts/95/","tags":null,"title":"ガンマ関数"},{"categories":"수리물리","contents":"ノーテーション ２回以上繰り返される添字については、合計記号$\\sum$を省略する。\n説明 アインシュタインの合計規約Einstein summation conventionとも呼ばれる。公式のようなものではなく、文字通りの規則である。ベクトル計算をしていくと、一つの式の中で$\\sum$を何重にも書かなければならない場面がよくあるが、これでは式が汚くなり、手で書く時も非常に面倒である。したがって、添字が２回以上繰り返される場合は、合計記号を省略するという約束である。もちろん、意味を混同しないように注意が必要である。\n混乱する場合は、左辺にどのようなインデックスがあるかを確認すればよい。左辺に明らかにインデックス$i$がなければ、右辺ではアインシュタインのノーテーションにより$\\sum \\limits_{i}$が省略されていることになる。逆に、左辺にインデックス$j$がある場合、右辺では$j$に対する$\\sum$が省略されているわけではなく、単にないのである。\n例 $1,2,3$がそれぞれ$x,y,z$を表すとする。ベクトル$\\mathbf{A} = (A_{1}, A_{2}, A_{3})$と$\\mathbf{B} = (B_{1}, B_{2}, B_{3})$が与えられたとする。\nベクトル $$ \\begin{align*} \\mathbf{A} \u0026amp;= \\hat{\\mathbf{e}}_{1}A_{1} + \\hat{\\mathbf{e}}_{2}A_{2} + \\hat{\\mathbf{e}}_{3}A_{3} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{3} \\hat{\\mathbf{e}}_{i}A_{i} \\\\ \u0026amp;= \\hat{\\mathbf{e}}_{i}A_{i} \\end{align*} $$\n２つのベクトルの内積 $$ \\begin{align*} \\mathbf{A} \\cdot \\mathbf{B} \u0026amp;= A_{1}B_{1} + A_{2}B_{2} + A_{3}B_{3} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{3} A_{i}B_{i} \\\\ \u0026amp;= A_{i}B_{i} \\end{align*} $$\nクロネッカーのデルタを使って、次のように表せる。\n$$ \\mathbf{A} \\cdot \\mathbf{B} = A_{i}B_{i} = \\delta_{ij}A_{i}B_{j} $$\nベクトル関数の発散 $\\dfrac{\\partial }{\\partial x_{i}} = \\nabla_{i}$とする。そうすると、２つのベクトルの内積から得られる結果と似た結果が得られる。\n$$ \\begin{align*} \\nabla \\cdot \\mathbf{A} \u0026amp;= \\dfrac{\\partial A_{1}}{\\partial x_{1}} + \\dfrac{\\partial A_{2}}{\\partial x_{2}} + \\dfrac{\\partial A_{3}}{\\partial x_{3}} \\\\ \u0026amp;= \\nabla_{1} A_{1} + \\nabla_{2} A_{2} + \\nabla_{3} A_{3} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{3} \\nabla_{i} A_{i} \\\\ \u0026amp;= \\nabla_{i}A_{i} \\\\ \u0026amp;= \\delta_{ij}\\nabla_{i}A_{j} \\end{align*} $$\n２つのベクトルの外積 $$ \\begin{align*} \u0026amp; \\mathbf{A} \\times \\mathbf{B} \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1} \\left( A_{2} B_{3} - A_{3} B_{2} \\right) + \\hat{\\mathbf{e}}_{2} \\left( A_{3} BA_{1} - A_{1} B_{3} \\right) + \\hat{\\mathbf{e}}_{3} \\left( A_{1} B_{2} - A_{2} B_{1} \\right) \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1} A_{2} B_{3} - \\hat{\\mathbf{e}}_{1} A_{3} B_{2} + \\hat{\\mathbf{e}}_{2} A_{3} B_{1} - \\hat{\\mathbf{e}}_{2} A_{1} B_{3} + \\hat{\\mathbf{e}}_{3} A_{1} B_{2} - \\hat{\\mathbf{e}}_{1} A_{2} B_{1} \\\\ =\u0026amp;\\ \\epsilon_{123} \\hat{\\mathbf{e}}_{1} A_{2} B_{3} + \\epsilon_{132} \\hat{\\mathbf{e}}_{1} A_{3} B_{2} + \\epsilon_{231} \\hat{\\mathbf{e}}_{2} A_{3} B_{1} + \\epsilon_{213} \\hat{\\mathbf{e}}_{2} A_{1} B_{3} + \\epsilon_{312} \\hat{\\mathbf{e}}_{3} A_{1} B_{2} + \\epsilon_{321} \\hat{\\mathbf{e}}_{3} A_{2} B_{1} \\\\ =\u0026amp;\\ \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} \\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} A_{j} B_{k} \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} A_{j}B_{k} \\end{align*} $$\nこの時、$\\epsilon_{ijk}$はレヴィ=チヴィタ記号である。上記の結果により、次の式が成り立つ。\n$$ (\\mathbf{A} \\times \\mathbf{B} )_{i} = \\epsilon_{ijk} A_{j}B_{k} $$\nベクトル関数の回転 再び$\\dfrac{\\partial }{\\partial x_{i}} = \\nabla_{i}$とする。そうすると、２つのベクトルの外積から得られる結果と似た結果が得られる。\n$$ \\begin{align*} \u0026amp; \\nabla \\times \\mathbf{A} \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1} \\left( \\nabla_{2} A_{3} - \\nabla_{3} A_{2} \\right) + \\hat{\\mathbf{e}}_{2} \\left( \\nabla_{3} A_{1} - \\nabla_{1} A_{3} \\right) + \\hat{\\mathbf{e}}_{3} \\left( \\nabla_{1} A_{2} - \\nabla_{2} A_{1} \\right) \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1} \\nabla_{2} A_{3} - \\hat{\\mathbf{e}}_{1} \\nabla_{3} A_{2} + \\hat{\\mathbf{e}}_{2} \\nabla_{3} A_{1} - \\hat{\\mathbf{e}}_{2} \\nabla_{1} A_{3} + \\hat{\\mathbf{e}}_{3} \\nabla_{1} A_{2} - \\hat{\\mathbf{e}}_{1} \\nabla_{2} A_{1} \\\\ =\u0026amp;\\ \\epsilon_{123} \\hat{\\mathbf{e}}_{1} \\nabla_{2} A_{3} + \\epsilon_{132} \\hat{\\mathbf{e}}_{1} \\nabla_{3} A_{2} + \\epsilon_{231} \\hat{\\mathbf{e}}_{2} \\nabla_{3} A_{1} + \\epsilon_{213} \\hat{\\mathbf{e}}_{2} \\nabla_{1} A_{3} + \\epsilon_{312} \\hat{\\mathbf{e}}_{3} \\nabla_{1} A_{2} + \\epsilon_{321} \\hat{\\mathbf{e}}_{3} \\nabla_{2} A_{1} \\\\ =\u0026amp;\\ \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} \\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} \\nabla_{j} A_{k} \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} \\nabla_{j} A_{k} \\end{align*} $$\nここで、$\\nabla_{i}$が微分であることを常に意識しなければならない。通常のベクトル成分は順番を変えても大きな問題はない。\n$$ A_{1}A_{2}A_{3} = A_{2}A_{1}A_{3} $$\nしかし、$\\nabla_{i}$は微分であるため、ベクトルの成分と順序を絶対に入れ替えてはいけない。\n$$ A_{1}\\nabla_{2}A_{3} \\ne \\nabla_{2}A_{1}A_{3} $$\n例えば、$\\mathbf{A} = (y,xy,xyz)$とすると、次の結果が得られる。\n$$ A_{1}\\nabla_{2}A_{3} = y \\dfrac{\\partial (xyz)}{\\partial y} = xyz \\ne 2xyz = \\dfrac{\\partial (xy^{2}z)}{\\partial y} = \\nabla_{2}A_{1}A_{3} $$\nもちろん、$\\dfrac{\\partial^{2} }{\\partial x\\partial y} = \\dfrac{\\partial^{2} }{\\partial y\\partial x}$であるため、$\\nabla_{1}\\nabla_{2}=\\nabla_{2}\\nabla_{1}$が成立する。\n","id":90,"permalink":"https://freshrimpsushi.github.io/jp/posts/90/","tags":null,"title":"アインシュタインの記法"},{"categories":"수리물리","contents":"まとめ 次のように定義される $\\epsilon_{ijk}$ を レビ-チビタ記号 と呼ぶ。\n$$ \\epsilon_{ijk} = \\begin{cases} +1 \u0026amp; \\text{if} \\ \\epsilon_{123}, \\epsilon_{231}, \\epsilon_{312} \\\\ -1 \u0026amp; \\text{if} \\ \\epsilon_{132}, \\epsilon_{213}, \\epsilon_{321} \\\\ 0 \u0026amp; \\text{if} \\ i=j \\ \\text{or} \\ j=k \\ \\text{or} \\ k=i \\end{cases} $$\n次のように定義される $\\delta_{ij}$ を クロネッカーのデルタ と呼ぶ。\n$$ \\delta_{ij} := \\begin{cases} 1,\u0026amp;i=j \\\\ 0, \u0026amp; i\\ne j \\end{cases} $$\n二つのレビ-チビタ記号の積とクロネッカーのデルタとの間には、次の関係が成り立つ。\n(a) 一つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ilm} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl}$\n(b) 二つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijm}=2\\delta_{km}$\n(c) 三つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijk}=6$\n説明 文章全体で $\\sum$ を省略する アインシュタインの記法 を使用していることに注意してください。これは上記の式においても同じです。 (a)は使用頻度が高いため、覚えておくと便利です。簡単に覚える方法は次のとおりです。\n証明 (a) $\\mathbf{e}_{i}$ $(i=1,2,3)$ を3次元における 標準単位ベクトル としよう。\n$$ \\mathbf{e}_{1} = (1, 0, 0),\\quad \\mathbf{e}_{2} = (0, 1, 0),\\quad \\mathbf{e}_{3} = (0, 0, 1) $$\n$P_{ijk}$ を1行目が $\\mathbf{e}_{i}$、2行目が $\\mathbf{e}_{j}$、3行目が $\\mathbf{e}_{k}$ の $3 \\times 3$ 行列 とする。\n$$ P_{ijk} = \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} $$\nすると、行列式の性質により $\\det P_{ijk} = \\epsilon_{ijk}$ と簡単にわかる。まず $P_{123}$ は 単位行列 であるため、行列式は $1$ である。また、異なる行の順序を偶数回変えると行列式の値は変わらないため、\n$$ \\det P_{123} = \\det P_{231} = \\det P_{312} = 1 $$\n異なる行の順序を奇数回変えると行列式の符号が逆になるため、\n$$ \\det P_{132} = \\det P_{213} = \\det P_{321} = -1 $$\n同じ行を二つ以上含む行列の行列式は $0$ であるため、残りの場合は全て $0$ となる。したがって $\\det P_{ijk} = \\epsilon_{ijk}$ が成立する。一つのインデックスが同じ二つのレビ-チビタ記号の積は、 行列式の性質 をうまく使うと、次のようになる。\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ilm} \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{l} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{m} \\text{ \u0026mdash;} \\end{bmatrix} \\\\ \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \u0026amp; (\\because \\det A = \\det A^{T}) \\\\ \u0026amp;= \\det \\left( \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \\right) \u0026amp; \\Big(\\because (\\det A) (\\det B) = \\det (AB) \\Big) \\\\ \u0026amp;= \\det \\begin{bmatrix} \\mathbf{e}_{i} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{j} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{k} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{m} \\end{bmatrix} \\end{align*} $$\n$\\mathbf{e}_{i}$ は標準単位ベクトルであるため、$\\mathbf{e}_{i} \\cdot \\mathbf{e}_{j} = \\delta_{ij}$ が成立する。\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} \\delta_{ii} \u0026amp; \\delta_{il} \u0026amp; \\delta_{im} \\\\ \\delta_{ji} \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ \\delta_{ki} \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} $$\nこのとき、$i$ が $j, k, l, m$ と全て異なる場合のみを考えていることに注意しよう。なぜなら $j, k, l, m$ のいずれかが $i$ と同じであれば、$\\epsilon_{ijk}\\epsilon_{ilm} = 0$ となり、意味のない結果だからである。したがって結果\nは次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ `` 0 \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl} $$\n■\n(b) (a) で $l=j$ の場合である。したがって、次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} $$\nこのとき $\\delta_{jj}=3$ が成立し、また $\\delta_{jm}\\delta_{kj}=\\delta_{mk}$ も成立するため、結果は次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} = 3\\delta_{km} - \\delta_{mk} = 2\\delta_{km} $$\n■\n(c) (b) で $m=k$ の場合であるため、\n$$ \\epsilon_{ijk}\\epsilon_{ijk} = \\sum_{k=1}^{3}2\\delta_{kk} = 2\\delta_{11} + 2\\delta_{22} + 2\\delta_{33} = 2 + 2 + 2 = 6 $$\nまたは、0でない全ての項を展開すると、次を得る。\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ijk} \u0026amp;=\\sum \\limits _{i=1} ^{3}\\sum \\limits _{j=1} ^{3}\\sum \\limits _{k=1} ^{1} \\epsilon_{ijk}\\epsilon_{ijk} \\\\ \u0026amp;=\\epsilon_{123}\\epsilon_{123}+\\epsilon_{231}\\epsilon_{231}+\\epsilon_{312}\\epsilon_{312}+\\epsilon_{132}\\epsilon_{132}+\\epsilon_{213}\\epsilon_{213}+\\epsilon_{321}\\epsilon_{321} \\\\ \u0026amp;=6 \\end{align*} $$\n■\n","id":88,"permalink":"https://freshrimpsushi.github.io/jp/posts/88/","tags":null,"title":"二つのレビ-チビタ記号の積"},{"categories":"수리물리","contents":"定義 以下のように定義される$\\delta_{ij}$をクロネッカーのデルタKronecker deltaと呼ぶ。\n$$ \\delta_{ij} := \\begin{cases} 1,\u0026amp;i=j \\\\ 0, \u0026amp; i\\ne j \\end{cases} $$\n説明 クロネッカーのデルタは非常に多くの場所で使用され、すべての成分（要素、可能性など）の中で欲しいものだけを示すのが主な役割だ。物理学の学生なら、内積に関する表現で主に接することになる。これが何を意味するのかすぐにはわからないかもしれないから、以下の例を見て理解しよう。\n例 まず、2つのベクトル$\\mathbf{A}=(A_{1}, A_{2}, A_{3})$、$\\mathbf{B}=(B_{1}, B_{2}, B_{3})$が与えられたとしよう。すると、二つのベクトルの内積は次のようになる。\n$$ \\mathbf{A} \\cdot \\mathbf{B} = A_{1}B_{1} + A_{2}B_{2} + A_{3}B_{3} $$\nこれを合計記号$\\sum$を使って表現すると、次のようになる。\n$$ \\mathbf{A} \\cdot \\mathbf{B} = A_{1}B_{1} + A_{2}B_{2} + A_{3}B_{3} = \\sum \\limits_{i=1}^{3}A_{i}B_{i} $$\nそれでは、上の式と$\\sum \\limits_{i=1}^{3}\\sum \\limits_{j=1}^{3}\\delta_{ij}A_{i}B_{j}$が同じ式であることを次でわかる。\n$$ \\begin{align*} \\sum _{i=1}^{3}\\sum _{j=1}^{3}\\delta_{ij}A_{i}B_{j} \u0026amp;= \\delta_{11}A_{1}B_{1} + \\delta_{12}A_{1}B_{2} + \\delta_{13}A_{1}B_{3} \\\\ \u0026amp; \\quad+ \\delta_{21}A_{2}B_{1} + \\delta_{22}A_{2}B_{2} + \\delta_{23}A_{2}B_{3} \\\\ \u0026amp; \\quad+ \\delta_{31}A_{3}B_{1} + \\delta_{32}A_{3}B_{2} + \\delta_{33}A_{3}B_{3} \\\\ \u0026amp;= 1\\cdot A_{1}B_{1} + 0 \\cdot A_{1}B_{2} + 0\\cdot A_{1}B_{3} \\\\ \u0026amp; \\quad+ 0\\cdot A_{2}B_{1} + 1\\cdot A_{2}B_{2} + 0\\cdot A_{2}B_{3} \\\\ \u0026amp; \\quad+ 0\\cdot A_{3}B_{1} + 0\\cdot A_{3}B_{2} + 1\\cdot A_{3}B_{3} \\\\ \u0026amp;= A_{1}B_{1} + A_{2}B_{2} + A_{3}B_{3} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{3}A_{i}B_{i} \\\\ \u0026amp;= \\mathbf{A} \\cdot \\mathbf{B} \\end{align*} $$\n一方の項に同じインデックスが2回以上出現する場合、$\\sum$を省略するアインシュタイン記法を適用すると、次のようになる。\n$$ \\delta_{ij}A_{i}B_{j} = \\mathbf{A} \\cdot \\mathbf{B} $$\nそれで$\\delta_{ij}A_{i}B_{j}$と$\\mathbf{A} \\cdot \\mathbf{B}$が同じであることはわかるが、なぜこのような表現を使うのかは理解できないかもしれない。上の例は非常に単純な式であるため、その有用性が目立たないかもしれないが、電磁気学などで多数のベクトルの内積や外積、勾配、発散、回転、ラプラシアンなどを計算すると、その便利さがわかるだろう。学部2年生なら、その便利さを自然に知ることになるので、今すぐ無理に納得する必要はない。\nまた、下添字が両方とも同じ場合のみ値があるため、複数のクロネッカーのデルタが掛けられている場合は、すべての添字が同じ場合のみ値がある。\n$$ \\delta_{ij}\\delta_{jk} $$\nこのような場合、$i=j=k$の場合のみ、$0$ではない値が存在する。また、クロネッカーのデルタは$2$次テンソルの一例である。\n公式 (a) $\\delta_{ii} = 3$\n(b) $\\delta_{ij}\\delta_{jl} = \\delta_{il}$\n(c) $\\delta_{ii}\\delta_{jj} = 9$\n(d) $\\delta_{ii}\\delta_{jj} = 6 \\quad (i \\ne j)$\n同じインデックスが項に2回以上出現する場合は$\\sum$が省略されていることを忘れないでほしい。\n証明 (a) アインシュタイン記法により、以下が成立する。\n$$ \\delta_{ii} = \\sum \\limits_{i=1}^{3} \\delta_{ii} = \\delta_{11}+\\delta_{22}+\\delta_{33}=3 $$\n■\n(b) アインシュタイン記法により、以下が成立する。\n$$ \\delta_{ij}\\delta_{jl}=\\sum\\limits_{j=1}^{3}\\delta_{ij}\\delta_{jl}=\\delta_{i1}\\delta_{1l}+\\delta_{i2}\\delta_{2l}+\\delta_{i3}\\delta_{3l} $$\nでは、上記の値が$0$でない場合について考えよう。\n$$ i=l=1 \\quad \\text{and} \\quad i=l=2 \\quad \\text{and} \\quad i=l=3 $$\n最初の場合、以下が成立する。\n$$ \\delta_{i1}\\delta_{1l} = 1 \\quad \\text{and} \\quad \\delta_{i2}\\delta_{2l}=\\delta_{i3}\\delta_{3l} = 0 \\\\ \\implies \\delta_{ij}\\delta_{jl} = \\delta_{i1}\\delta_{1l}+\\delta_{i2}\\delta_{2l}+\\delta_{i3}\\delta_{3l} = 1 $$\n二番目の場合、以下が成立する。\n$$ \\delta_{i2}\\delta_{2l} = 1 \\quad \\text{and} \\quad \\delta_{i1}\\delta_{1l}=\\delta_{i3}\\delta_{3l} = 0 \\\\ \\implies \\delta_{ij}\\delta_{jl} = \\delta_{i1}\\delta_{1l}+\\delta_{i2}\\delta_{2l}+\\delta_{i3}\\delta_{3l} = 1 $$\n三番目の場合、以下が成立する。\n$$ \\delta_{i3}\\delta_{3l} = 1 \\quad \\text{and} \\quad \\delta_{i1}\\delta_{1l}=\\delta_{i2}\\delta_{2l} = 0 \\\\ \\implies \\delta_{ij}\\delta_{jl} = \\delta_{i1}\\delta_{1l}+\\delta_{i2}\\delta_{2l}+\\delta_{i3}\\delta_{3l} = 1 $$\nしたがって、$\\delta_{ij}\\delta_{jl}$は$i=l$のときのみ$1$の値を持ち、それ以外の場合はすべて値が$0$であるので、以下の結果が得られる。\n$$ \\delta_{ij}\\delta_{jl} = \\delta_{il} $$\n■\n(c) アインシュタイン記法により、$\\sum$が省略されているので、以下のようになる。\n$$ \\begin{align*} \\delta_{ii}\\delta_{jj} \u0026amp;= \\sum\\limits_{i=1}^{3}\\sum\\limits_{j=1}^3{\\delta_{ii}\\delta_{jj}} \\\\ \u0026amp;= \\sum\\limits_{i=1}^{3}{\\delta_{ii} \\sum\\limits_{j=1}^3\\delta_{jj}} \\\\ \u0026amp;= 3\\cdot 3 \\\\ \u0026amp;= 9 \\end{align*} $$\n三番目の等号は**（a）**により成立する。\n■\n(d) アインシュタイン記法により、$\\sum$が省略されているので、以下のようになる。\n$$ \\begin{align*} \\delta_{ii}\\delta_{jj} \u0026amp;= \\sum\\limits_{i=1}^{3}\\sum\\limits_{\\substack{j=1 \\\\ j\\ne i}}^{3}{\\delta_{ii}\\delta_{jj}} \\\\ \u0026amp;= \\delta_{11}\\delta_{22} +\\delta_{11}\\delta_{33} +\\delta_{22}\\delta_{11} +\\delta_{22}\\delta_{33}+\\delta_{33}\\delta_{11}+\\delta_{33}\\delta_{22} \\\\ \u0026amp;= 6 \\end{align*} $$\n■\n","id":84,"permalink":"https://freshrimpsushi.github.io/jp/posts/84/","tags":null,"title":"クロネッカーのデルタ"},{"categories":"수리물리","contents":"定義 以下のように定義される$\\epsilon_{ijk}$を レビ-チビタ記号Levi-Civita-symbol という。\n$$ \\epsilon_{ijk} = \\begin{cases} +1 \u0026amp; \\text{if} \\ \\epsilon_{123}, \\epsilon_{231}, \\epsilon_{312} \\\\ -1 \u0026amp; \\text{if} \\ \\epsilon_{132}, \\epsilon_{213}, \\epsilon_{321} \\\\ 0 \u0026amp; \\text{if} \\ i=j \\ \\text{or} \\ j=k \\ \\text{or} \\ k=i \\end{cases} $$\n説明 クロネッカーのデルタがインデックス同士が同じかだけを考えたとするなら、レビ-チビタ記号は定義から見えるようにインデックスの順番も値に影響を与える。$\\epsilon_{ijk}$は$i$、$j$、$k$が重複なしで昇順$(1\\to 2\\to 3\\to 1)$であれば$+1$、重複なしで降順$(3\\to 2\\to 1\\to 3)$であれば$-1$、一つでも重複があれば$0$を値とする。単純に述べれば上のようで、全部の値を列挙すれば合計$3\\times 3\\times 3=27$個で、その中で$0$ではないのは$6$個である。\n$$ \\begin{array}{|c|c|c|c|}\\hline i=1 \u0026amp; k=1 \u0026amp; k=2 \u0026amp; k=3 \\\\ \\hline j=1 \u0026amp; \\epsilon_{111}=0 \u0026amp; \\epsilon_{112}=0 \u0026amp; \\epsilon_{113}=0 \\\\ j=2 \u0026amp; \\epsilon_{121}=0 \u0026amp; \\epsilon_{122}=0 \u0026amp; \\epsilon_{123}=1 \\\\ j=3 \u0026amp; \\epsilon_{131}=0 \u0026amp; \\epsilon_{132}=-1 \u0026amp; \\epsilon_{133}=0 \\\\ \\hline \\end{array}\\quad \\begin{array}{|c|c|c|c|}\\hline i=2 \u0026amp; k=1 \u0026amp; k=2 \u0026amp; k=3 \\\\ \\hline j=1 \u0026amp; \\epsilon_{211}=0 \u0026amp; \\epsilon_{212}=0 \u0026amp; \\epsilon_{213}=-1 \\\\ j=2 \u0026amp; \\epsilon_{221}=0 \u0026amp; \\epsilon_{222}=0 \u0026amp; \\epsilon_{223}=0 \\\\ j=3 \u0026amp; \\epsilon_{231}=1 \u0026amp; \\epsilon_{232}=0 \u0026amp; \\epsilon_{233}=0 \\\\ \\hline \\end{array} \\\\ {} \\\\ \\begin{array}{|c|c|c|c|}\\hline i=3 \u0026amp; k=1 \u0026amp; k=2 \u0026amp; k=3 \\\\ \\hline j=1 \u0026amp; \\epsilon_{311}=0 \u0026amp; \\epsilon_{312}=1 \u0026amp; \\epsilon_{313}=0 \\\\ j=2 \u0026amp; \\epsilon_{321}=-1 \u0026amp; \\epsilon_{322}=0 \u0026amp; \\epsilon_{323}=0 \\\\ j=3 \u0026amp; \\epsilon_{331}=0 \u0026amp; \\epsilon_{332}=0 \u0026amp; \\epsilon_{333}=0 \\\\ \\hline \\end{array} $$\n$3$次テンソルの一例である。\n例 外積 レビ-チビタ記号を使うと、二つのベクトルの外積を非常に簡単に表現できる。3次元直交座標系で二つのベクトルの外積は以下の通りである。\n$$ \\begin{align*} \\mathbf{A} \\times \\mathbf{B} =\u0026amp;\\ \\hat{\\mathbf{e}}_{x} (A_{y}B_{z}-A_{z}B_{y}) + \\hat{\\mathbf{e}}_{y} (A_{z}B_{x}-A_{x}B_{z}) + \\hat{\\mathbf{e}}_{z} (A_{x}B_{y}-A_{y}B_{x}) \\\\ =\u0026amp;\\ \\begin{vmatrix} \\hat{\\mathbf{e}}_{x} \\quad \\hat{\\mathbf{e}}_{y} \\quad \\hat{\\mathbf{e}}_{z} \\\\ A_{x} \\quad A_{y} \\quad A_{z} \\\\ B_{x} \\quad B_{y} \\quad B_{z} \\end{vmatrix} \\end{align*} $$\nここで$x=1$、$y=2$、$z=3$とすると、二つのベクトルの外積はレビ-チビタ記号を使用して以下のように表現できる。\n$$ \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} {\\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} A_{j}B_{k}} $$\n$0$ではない項だけを展開すると、以下のようになる。\n$$ \\begin{align*} \u0026amp; \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} {\\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} A_{j}B_{k}} \\\\ =\u0026amp;\\ \\epsilon_{123} \\hat{\\mathbf{e}}_{1}A_{2}B_{3} + \\epsilon_{132} \\hat{\\mathbf{e}}_{1}A_{3}B_{2} + \\epsilon_{ 231 }\\hat{\\mathbf{e}}_{2}A_{3}B_{1} + \\epsilon_{213}\\hat{\\mathbf{e}}_{2}A_{1}B_{3} + \\epsilon_{312}\\hat{\\mathbf{e}}_{3}A_{1}B_{2} + \\epsilon_{321}\\hat{\\mathbf{e}}_{3}A_{2}B_{1} \\end{align*} $$\nここで$\\epsilon_{123}=\\epsilon_{231}=\\epsilon_{312}=1$、$\\epsilon_{132}=\\epsilon_{213}=\\epsilon_{321}=-1$を代入して整理すると、以下のようになる。\n$$ \\begin{align*} \u0026amp; \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} {\\epsilon_{ijk} A_{i}B_{j}}\\hat{\\mathbf{e}}_{k} \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1}\\left( A_{2}B_{3} - A_{3}B_{2} \\right) + \\hat{\\mathbf{e}}_{2}\\left( A_{3}B_{1} - A_{1}B_{3} \\right) + \\hat{\\mathbf{e}}_{3}\\left( A_{1}B_{2} - A_{2}B_{1} \\right) \\end{align*} $$\n最後に$1$、$2$、$3$にそれぞれ$x$、$y$、$z$を代入すると、以下を得る。\n$$ \\hat{\\mathbf{e}}_{x}\\left( A_{y}B_{z} - A_{z}B_{y} \\right) + \\hat{\\mathbf{e}}_{y}\\left( A_{z}B_{x} - A_{x}B_{z} \\right) + \\hat{\\mathbf{e}}_{z}\\left( A_{x}B_{y} - A_{y}B_{x} \\right) $$\nしたがって、以下の結果を得る。\n$$ \\mathbf{A} \\times \\mathbf{ B } = \\sum \\limits_{i=1}^{3} \\sum \\limits_{j=1}^{3} \\sum \\limits_{k=1}^{3} \\epsilon_{ijk}\\hat{\\mathbf{e}}_{i}A_{j}B_{k} $$\nアインシュタインの記法を使えば、以下のようになる。\n$$ \\mathbf{A} \\times \\mathbf{ B } = \\epsilon_{ijk}\\hat{\\mathbf{e}}_{i}A_{j}B_{k} $$\n外積の各成分も容易に表現できるが、上の式から分かるように、$(\\mathbf{A} \\times \\mathbf{B})$の$i$成分は以下の通りである。\n$$ (\\mathbf{A} \\times \\mathbf {B})_{i}=\\epsilon_{ijk}A_{j}B_{k} $$\n行列式 $3 \\times 3$行列$A = [a_{ij}]$の行列式は、以下のように表される。\n$$ \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\\\ \\end{vmatrix} = \\sum\\limits_{i,j,k=1}^{3} \\epsilon_{ijk}a_{1i}a_{2j}a_{3k} $$\n示す方法は簡単だ。行列式を展開し、行列の各成分の二番目のインデックスをよく見ると、インデックスのレビ-チビタ記号が各項の符号と一致することがわかる。\n$$ \\begin{align*} \u0026amp; \\det A \\\\ \u0026amp;= a_{11}(a_{22}a_{33} - a_{23}a_{32}) + a_{12}(a_{23}a_{31} - a_{21}a_{33}) + a_{13}(a_{21}a_{32} - a_{22}a_{31}) \\\\ \u0026amp;= a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} + a_{12}a_{23}a_{31} - a_{12}a_{21}a_{33} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} \\\\ \u0026amp;= a_{1\\textcolor{red}{1}}a_{2\\textcolor{red}{2}}a_{3\\textcolor{red}{3}} - a_{1\\textcolor{red}{1}}a_{2\\textcolor{red}{3}}a_{3\\textcolor{red}{2}} + a_{1\\textcolor{red}{2}}a_{2\\textcolor{red}{3}}a_{3\\textcolor{red}{1}} - a_{1\\textcolor{red}{2}}a_{2\\textcolor{red}{1}}a_{3\\textcolor{red}{3}} + a_{1\\textcolor{red}{3}}a_{2\\textcolor{red}{1}}a_{3\\textcolor{red}{2}} - a_{1\\textcolor{red}{3}}a_{2\\textcolor{red}{2}}a_{3\\textcolor{red}{1}} \\\\ \u0026amp;= \\epsilon_{\\textcolor{red}{123}}a_{1\\textcolor{red}{1}}a_{2\\textcolor{red}{2}}a_{3\\textcolor{red}{3}} + \\epsilon_{\\textcolor{red}{132}}a_{1\\textcolor{red}{1}}a_{2\\textcolor{red}{3}}a_{3\\textcolor{red}{2}} + \\epsilon_{\\textcolor{red}{231}}a_{1\\textcolor{red}{2}}a_{2\\textcolor{red}{3}}a_{3\\textcolor{red}{1}} + \\epsilon_{\\textcolor{red}{213}}a_{1\\textcolor{red}{2}}a_{2\\textcolor{red}{1}}a_{3\\textcolor{red}{3}} + \\epsilon_{\\textcolor{red}{312}}a_{1\\textcolor{red}{3}}a_{2\\textcolor{red}{1}}a_{3\\textcolor{red}{2}} + \\epsilon_{\\textcolor{red}{321}}a_{1\\textcolor{red}{3}}a_{2\\textcolor{red}{2}}a_{3\\textcolor{red}{1}} \\\\ \u0026amp;= \\sum\\limits_{i,j,k=1}^{3} \\epsilon_{ijk}a_{1i}a_{2j}a_{3k} \\end{align*} $$\n公式 (a) 一つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{lmk}=\\delta_{il}\\delta_{jm}-\\delta_{im}\\delta_{jl}$\n(b) 二つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ljk}=2\\delta_{il}$\n(c) 三つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijk}=6$\n","id":83,"permalink":"https://freshrimpsushi.github.io/jp/posts/83/","tags":null,"title":"レヴィ-チヴィタ記号"},{"categories":"해석개론","contents":"要約1 関数 $f(x)$ が $[a,b]$ で連続であり、$(a,b)$ で $n$ 回微分可能であれば、\n$$ \\begin{align*} f(b) =\u0026amp; \\sum_{k=0}^{n-1} {{(b-a)^{k}\\over{k!}}{f^{(k)}( a )}} + {(b-a)^{n}\\over{n!}}{f^{(n)}(\\xi)} \\\\ =\u0026amp; {f(a)} + {(b-a)f ' (a)} + \\cdots + {(b-a)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(a)} + {(b-a)^{n}\\over{(n)!}}{f^{(n)}(\\xi)} \\end{align*} $$\nを満たす $\\xi \\in (a,b)$ が存在する。\n説明 数学全般で非常に重要な定理で、この定理にちなんでテイラー級数がある。$n$回微分するという意味においては、平均値の定理を一般化したものと言える。\n通常、テイラーの定理を使用する時は、$c$ではなく、$\\xi$を使用する。\n証明 $$ \\begin{align*} f(b) :=\u0026amp; {(b-a)^0\\over{0!}}{f(a)} + {(b-a)^1\\over{1!}}{f ' (a)} + {(b-a)^2\\over{2!}}{f '' (a)} \\\\ \u0026amp;+ \\cdots + {(b-a)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(a)} + {(b-a)^{n}\\over{(n)!}}c \\end{align*} $$\nとしよう。$c={f^{(n)}(\\xi)}$ を示せば証明は終わる。関数 $g$ を次のように定義する。\n$$ \\begin{align*} g(x):=\u0026amp; -f(b) + f(x) + {(b-x)^1\\over{1!}}{f ' (x)} + {(b-x)^2\\over{2!}}{f '' (x)} \\\\ \u0026amp; + \\cdots + {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(x)} + {(b-x)^{n}\\over{(n)!}}c \\\\ =\u0026amp; -f(b) + \\sum_{k=0}^{n-1}{(b-x)^{k}\\over{(k)!}}{f^{(k)}(x)} + {(b-x)^{n}\\over{(n)!}}c \\end{align*} $$\n$g$ は $[a,b]$ で連続で、$(a,b)$ で微分可能であり、$c$ の定義により $g(b)=g(a)=0$である。\nロルの定理: 関数 $f(x)$が $[a,b]$で連続で、$(a,b)$で微分可能で、$f(a)=f(b)$ ならば、$(a,b)$ で少なくとも一つの $\\xi$ が $f ' (\\xi)=0$ を満たす。\n$h(x)$ を $$ \\begin{align*} h(x):=\u0026amp; \\left[ \\sum_{k=0}^{n-1}{(b-x)^{k}\\over{(k)!}}{f^{(k)}(x)} \\right] ' \\\\ =\u0026amp; \\left[ {(b-x)^{0}\\over{(0)!}}{f^{(0)}(x)} + {(b-x)^{1}\\over{(1)!}}{f^{(1)}(x)} + \\cdots + {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(x)} \\right] ' \\\\ =\u0026amp; \\left[ f (x) + {(b-x)^{1}\\over{(1)!}}{f^{(1)}(x)} + \\cdots + {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(x)} \\right] ' \\\\ =\u0026amp; f^{(1)} (x) - \\left[ f^{(1)} (x) + {(b-x)^{1}\\over{(1)!}}{f^{(2)}(x)} \\right] \\\\ \u0026amp; + \\left[ - {(b-x)^{1}\\over{(1)!}} f^{(2)} (x) + {(b-x)^{2}\\over{(2)!}}{f^{(3)}(x)} \\right] \\\\ \u0026amp; \\vdots \\\\ \u0026amp; + \\left[ - {(b-x)^{n-3}\\over{(n-3)!}} f^{(n-2)} (x) + {(b-x)^{n-2}\\over{(n-2)!}}{f^{(n-1)}(x)} \\right] \\\\ \u0026amp; + \\left[ - {(b-x)^{n-2}\\over{(n-2)!}} f^{(n-1)} (x) + {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n)}(x)} \\right] \\\\ =\u0026amp; {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n)}(x)} \\end{align*} $$\nとし、$\\displaystyle g ' (x) = 0 + h(x) + {(b-x)^{n-1}\\over{(n-1)!}}c$ であるため、ロルの定理により、\n$$ \\begin{align*} g ' (\\xi) =\u0026amp; h(\\xi) - {(b-\\xi)^{n-1}\\over{(n-1)!}}c \\\\ =\u0026amp; {(b-\\xi)^{n-1}\\over{(n-1)!}}{f^{(n)}(\\xi)} - {(b-\\xi)^{n-1}\\over{(n-1)!}}c \\\\ =\u0026amp; 0 \\end{align*} $$\nを満たす $\\xi$ が $(a,b)$ に少なくとも一つ存在する。したがって、\n$$ \\begin{align*} \u0026amp;\u0026amp; {(b-\\xi)^{n-1}\\over{(n-1)!}}{f^{(n)}(\\xi)} - {(b-\\xi)^{n-1}\\over{(n-1)!}}c =\u0026amp; 0 \\\\ \\implies \u0026amp;\u0026amp; {(b-\\xi)^{n-1}\\over{(n-1)!}}{f^{(n)}(\\xi)} =\u0026amp; {(b-\\xi)^{n-1}\\over{(n-1)!}}c \\\\ \\implies \u0026amp;\u0026amp; {f^{(n)}(\\xi)} =\u0026amp; c \\end{align*} $$\n$c={f^{(n)}(\\xi)}$ を示したので、証明が終了した。\n■\n上記のように証明したが、より一般的に使用される形は以下の通りである。もちろん$x \\in [a,b]$で、$x_{0} \\in (a,b)$として、実質的に$[x_{0} , x] \\subset [a,b]$となる。\nテイラーの定理\n関数 $f(x)$ が $[a,b]$ で連続であり、$(a,b)$ で $n$ 回微分可能であれば、$x_{0} \\in (a,b)$に対して、\n$$ f(x) = \\sum_{k=0}^{n-1} {{( x - x_{0} )^{k}\\over{ k! }}{f^{(k)}( x_{0} )}} + {(x - x_{0} )^{n}\\over{ n! }}{f^{(n)}(\\xi)} $$\nを満たす $\\xi \\in (a,b)$ が存在する。\n参照 多変数関数のテイラーの定理 慶北大学校基礎教育院, 理工学生のための大学数学 (2012), p67-68\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":41,"permalink":"https://freshrimpsushi.github.io/jp/posts/41/","tags":null,"title":"テイラーの定理の証明"},{"categories":"해석개론","contents":"要約1 関数$f$が点$a$の近くで無限に微分可能で、$\\displaystyle f(x) = \\sum_{n=0}^{\\infty} {{f^{(n)} (a)}\\over{n!}} {(x-a)}^n$の必要十分条件はある$\\xi \\in \\mathscr{H} \\left\\{ x , a \\right\\}$に対して\n$$ \\lim_{n \\to \\infty} {{f^{(n)} (\\xi)}\\over{n!}} {(x-a)}^n = 0 $$\n$\\xi \\in \\mathscr{H} \\left\\{ x , a \\right\\}$とは、$\\xi$が$(x,a)$または$(a,x)$にあるという表現だ。\n説明 テイラー定理は関数が無限に微分可能な場合、一般に無限級数の形で表される。これをテイラー級数と呼び、特に$a=0$の場合、マクローリン級数と呼ばれる。テイラー級数は テイラー公式, テイラー展開ともよく呼ばれる。\n証明 テイラー定理\n関数$f(x)$が$[a,b]$で連続であり、$(a,b)$で$n$回微分可能なら$x_{0} \\in (a,b)$に対して\n$$ f(x) = \\sum_{k=0}^{n-1} {{( x - x_{0} )^{k}\\over{ k! }}{f^{(k)}( x_{0} )}} + {(x - x_{0} )^{n}\\over{ n! }}{f^{(n)}(\\xi)} $$\nを満たす$\\xi \\in (a,b)$が存在する。\nテイラー定理により、\n$$ f(x) = \\sum_{k=0}^{n-1} {{( x - a )^{k}\\over{ k! }}{f^{(k)}( a )}} + {(x - a )^{n}\\over{ n! }}{f^{(n)}(\\xi)} $$\n$\\xi$が$x$と$a$の間に少なくとも1つ存在する。関数$f$は無限に微分可能なので、\n$$ f(x) =\\lim_{n \\to \\infty} \\left[ \\sum_{k=0}^{n-1} {{f^{(k)} (a)}\\over{k!}} {(x-a)}^k + {{f^{(n)} (a)}\\over{n!}} {(x-a)}^n \\right] $$\nもし$\\displaystyle \\lim_{n \\to \\infty} {{f^{(n)} (a)}\\over{n!}} {(x-a)}^n = 0$なら、\n$$ f(x) =\\lim_{n \\to \\infty} \\sum_{k=0}^{n-1} {{f^{(k)} (a)}\\over{k!}} {(x-a)}^k = \\sum_{n=0}^{\\infty} {{f^{(n)} (a)}\\over{n!}} {(x-a)}^n $$\n■\n慶北大学校基礎教育院、理工学生のための大学数学 (2012)、p220-221\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":42,"permalink":"https://freshrimpsushi.github.io/jp/posts/42/","tags":null,"title":"テイラー級数とマクローリン級数"},{"categories":"해석개론","contents":"要約1 関数 $f(x)$ が $[a,b]$ で連続であり、$(a,b)$ で微分可能ならば、$\\displaystyle f '(c)={{f(b)-f(a)}\\over{b-a}}$ を満たす$c$ が $(a,b)$ 内に少なくとも一つ存在する。\n説明 ただよく使われるというだけではなく、MVTという略語を使用するほど有名な定理だ。平均値という言葉は、微分係数が全区間の平均変化率と同じになる点があるという意味から来ている。平均という概念が有用であるため、さまざまな分野に適用するための異なる変形形式が存在する。\n証明 $\\displaystyle m:= {{f(b)-f(a)}\\over{b-a}}$ とし、$g(x):=f(x)-mx$ を定義すると、$g(b)=g(a)$ であり、$g(x)$ は微分可能である。\nロルの定理\n関数 $f(x)$ が $[a,b]$ で連続であり、$(a,b)$ で微分可能で、$f(a)=f(b)$ ならば、$c$ が $(a,b)$ 内に少なくとも一つ存在して、$f ' (c)=0$ を満たす。\nロルの定理により、$g ' (c)=0$ を満たす$c$ が $(a,b)$ 内に少なくとも一つ存在し、$g ' (x)=f ' (x) - m$ なので、$g ' (c) = f '(c) - m = 0$ である。$f ' (c) -m = 0$ から$(-m)$ に項を移動すると、$\\displaystyle f '(c) = m = {{f(b)-f(a)}\\over{b-a}}$ を満たす $c$ が $(a,b)$ 内に少なくとも一つ存在することがわかる。\n■\n参照 コーシーの平均値の定理 積分の平均値の定理 ガウスの平均値の定理 慶北国立大学基礎教育院, 理工学生のための大学数学 (2012), p65-66\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":37,"permalink":"https://freshrimpsushi.github.io/jp/posts/37/","tags":null,"title":"微分積分学における平均値定理の証明"},{"categories":"복소해석","contents":"定理 1 $\\left\\{ a_{i} \\right\\}_{i=0}^{n} \\subset \\mathbb{R}$ で $a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$ としましょう。すると、多項関数 $$ P(z) := a_0 + a_1 z + \\cdots + a_{n-1} z^{n-1} + a_n z^n $$ において、あらゆる根 $z \\in \\mathbb{C}$ は $|z| \\ge 1$ を満たします。\n証明 もし $P(z) = 0$ の根が $z=1$ である場合、$\\displaystyle 0 = P(1) = \\sum_{i=0}^{n} a_{i} \u0026gt; 0$ より、根は $z \\ne 1$ でなければなりません。式 $P(z) = 0$ の両辺に $z$ を乗じて元の式から引き、$a_0$ を以下のように表せます。 $$ a_0 = (1-z)P(z) + (a_0 - a_1) z + \\cdots + (a_{n-1} - a_n) z^n + a_n z^{n+1} $$ ここで、$P(z) = 0$ の根 $z \\ne 1$ で $|z| \u0026lt; 1$ を仮定してみると、$a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$ より $$ \\begin{align*} \u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + (a_0 - a_1) + \\cdots + (a_{n-1} - a_n) + a_n \\\\ \\implies\u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + a_0 + (- a_1 + a_1) + \\cdots + (- a_{n-1} + a_{n-1} )+ (- a_n + a_n ) \\\\ \\implies\u0026amp; a_0 = |a_0| \u0026lt; |(1-z)P(z)| + a_0 \\\\ \\implies\u0026amp; 0 \u0026lt; |(1-z)P(z)| \\end{align*} $$ ですが、$z \\ne 1$ が $P(z) = 0$ の根であることを仮定しているので、以下の矛盾が生じます。 $$ 0 \u0026lt; |(1-z)P(z)| = 0 $$ これは、$| z | \u0026lt; 1$ という仮定が誤りであることを意味し、結果として $|z | \\ge 1$ でなければなりません。\n■\nOsborne. (1999). 複素変数とその応用: p. 6.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":5,"permalink":"https://freshrimpsushi.github.io/jp/posts/5/","tags":null,"title":"エーネストローム-カケヤ定理の証明"},{"categories":"교과과정","contents":"式 $$ d=\\frac {|2k|}{\\sqrt{m^2+1}} $$\n説明 双曲線の接線の問題を解いていると、二つの接線の間の距離を求めることがよくあります。点から直線までの距離を求める公式があるため、それ自体を解くことは難しくありません。しかし、その距離を簡単かつ迅速に計算できる公式を知っていれば、少しでも計算量を減らすことができるでしょう。\n導出 二つの平行な直線の方程式を $y=mx\\pm k$ とします。ある点 $(x,y)$ から直線 $y=mx+k$ までの距離は $$ \\frac {|mx-y+k|}{\\sqrt{m^2+1}} $$ 直線 $y=mx-k$ 上の点 $(x_1,y_1)$ に対しては $$ k=mx_1-y_1 $$ これを距離の公式に代入すると $$ \\frac {|mx_1-y_1+k|}{\\sqrt{m^2+1}} = \\frac {|k+k|}{\\sqrt{m^2+1}} $$ したがって、二つの平行な直線 $y=mx\\pm k$ の間の距離は $$ \\frac {|2k|}{\\sqrt{m^2+1}} $$\n■\n","id":4,"permalink":"https://freshrimpsushi.github.io/jp/posts/4/","tags":null,"title":"二本の平行な直線の間の距離を求める公式の導出"},{"categories":"보조정리","contents":"定義 $n$ 個の正数 ${x}_1,{x}_2,\\cdots,{x}_n$ に対して算術平均、幾何平均、調和平均は以下のように定義される。\n算術平均 : $$ \\sum_{ k=1 }^{ n }{ \\frac { {x}_k }{ n } }=\\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n } $$ 幾何平均 : $$ \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } }=\\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n } $$ 調和平均 : $$ \\left( \\frac { \\sum_{ k=1 }^{ n }{ \\frac { 1 }{ {x}_k } } }{ n } \\right)^{-1}=\\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ n{x}_2 }+\\cdots+\\frac { 1 }{ n{x}_n } } $$ 定理 これらの平均に対して、次の不等式が成り立つ。\n$$ \\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n }\\ge \\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ n{x}_2 }+\\cdots+\\frac { 1 }{ n{x}_n } } $$\n説明 高校生であれば、算術・幾何平均について一度は耳にするかもしれないが、特定の名称で定義されることはあまりなく、通常は「算術幾何」という略称で口伝えにされることが一般的である。$n=2$ の場合には証明も簡単で、高校レベルの問題解決にも役立つ。高校生レベルで一般的な証明には複雑な式を使った数学的帰納法を用いる必要があるが、より洗練されたが難しい証明を紹介する。\n証明 戦略：次の補助定理を利用する。\nジェンセンの不等式： $f$ が 凸関数 で、$E(X) \u0026lt; \\infty$ の場合、以下の不等式が成り立つ。 $$ E{f(X)}\\ge f{E(X)} $$\n算術-幾何 $f(x)=-\\ln x$ とすると、$f$ は区間 $(0,\\infty )$ で凸関数である。確率変数 $X$ が確率質量関数\n$$ p(X=x)=\\begin{cases}{1 \\over n} \u0026amp; , x={x}_1,{x}_2, \\cdots ,{x}_n \\\\ 0 \u0026amp; , その他の場合\\end{cases} $$\nを持つとする。すると $E(X)$ は\n$$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\u0026lt;\\infty $$\nであり有限である。これはジェンセンの不等式に必要な全ての条件を満たすため、次を得る。\n$$ E(-\\ln X)\\ge –\\ln E(X) $$\n左辺は\n$$ \\begin{align*} E(-\\ln X)\u0026amp;=-E(\\ln X) \\\\ \u0026amp;=-\\frac { 1 }{ n } \\sum_{ k=1 }^{ n }{ \\ln{x}_k } \\\\ \u0026amp;=-\\frac { 1 }{ n }\\ln \\prod_{ k=1 }^{ n }{ {x}_k } \\\\ \u0026amp;=-\\ln { \\left( \\prod_{ k=1 }^{ n }{ {x}_k } \\right) }^{ \\frac { 1 }{ n } } \\\\ \u0026amp;=-\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\end{align*} $$\n右辺は\n$$ \\begin{align*} -\\ln E(X)=-\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\end{align*} $$\nこの両者を定理すると\n$$ \\begin{align*} -\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\ge\u0026amp; -\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\\\ \\implies \\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n } \\ge\u0026amp; \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } \\end{align*} $$\n■\nこれにより、算術平均と幾何平均の間の不等式が証明された。これを用いて、幾何平均と調和平均の間の不等式を証明しよう。\n幾何-調和 $$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } $$\n$\\displaystyle {x}_k=\\frac { 1 }{ n{y}_k }$ と置くと、\n$$ \\begin{align*} \\frac { \\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } }{ n }\\ge \\sqrt [ n ]{ \\frac { 1 }{ n{y}_1 }\\frac { 1 }{ n{y}_2 }\u0026hellip;\\frac { 1 }{ n{y}_n } } \\\\ \\implies \\frac { 1 }{ n\\sqrt [ n ]{ \\frac { 1 }{ n{y}_1 }\\frac { 1 }{ n{y}_2 }\u0026hellip;\\frac { 1 }{ n{y}_n } } }\\ge \\frac { n }{ n\\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\\\ \\implies \\sqrt [ n ]{ {y}_1{y}_2\u0026hellip;{y}_n }\\ge \\frac { n }{ n\\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\end{align*} $$ ■\n","id":3,"permalink":"https://freshrimpsushi.github.io/jp/posts/3/","tags":null,"title":"算術平均と幾何平均、調和平均の間の不等式"},{"categories":"머신러닝","contents":"説明 PyTorchには、多くのニューラルネットワーク関連の関数がtorch.nnとtorch.nn.functionalに同じ名前で含まれています。 nn の関数はニューラルネットワークを関数として返し、 nn.functional の関数はニューラルネットワークそのものです。\n例えば nn.MaxPool2d はカーネルサイズを入力として受け取り、プーリング層を返す。\nimport torch import torch.nn as nn pool = nn.MaxPool2d(kernel_size = 2) # MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) A = torch.arange(16.).reshape(1, 4, 4) # tensor([[[ 0., 1., 2., 3.], # [ 4., 5., 6., 7.], # [ 8., 9., 10., 11.], # [12., 13., 14., 15.]]]) pool(A) # tensor([[[ 5., 7.], # [13., 15.]]]) 一方で nn.functional.MaxPool2d はそのものが2次元マックスプーリングレイヤーです。そのため、この関数はプーリングを適用するテンソルとプーリングの条件をすべて入力として受け取り、実際に入力テンソルをプーリングした結果を返す。\nimport torch import torch.nn.functional as F A = torch.arange(16.).reshape(1, 4, 4) F.max_pool2d(A, kernel_size=2) #tensor([[[ 5., 7.], # [13., 15.]]]) つまり nn.MaxPool2d(kernel_size=(n,m)) が返す関数の forward が max_pool2d( ,kernel_size(n,m)) として定義されているわけです。コードを見ると、実際には次のようになっています。\nclass MaxPool2d(_MaxPoolNd): kernel_size: _size_2_t stride: _size_2_t padding: _size_2_t dilation: _size_2_t def forward(self, input: Tensor): return F.max_pool2d(input, self.kernel_size, self.stride, self.padding, self.dilation, ceil_mode=self.ceil_mode, return_indices=self.return_indices) パラメータが含まれるレイヤー、例えば線形層であれば、F.linear(input, weight, bias) のようにパラメータも入力として受け取ります。\n環境 OS: Windows11 バージョン: Python 3.11.5, torch==2.0.1+cu118 ","id":3626,"permalink":"https://freshrimpsushi.github.io/jp/posts/3626/","tags":null,"title":"パイトーチでtorch.nnとtorch.nn.functionalの違い"},{"categories":"편미분방정식","contents":"整理 次のような波動方程式が与えられたとする。 この時、$\\Delta_{\\mathbf{x}}$は変数$\\mathbf{x}$に対するラプラシアンである。\n$$ \\begin{align} \\partial_{t}^{2} p(\\mathbf{x}, t) \u0026amp;= \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;\\text{on } \\mathbb{R} \\times [0, \\infty) \\\\ p(\\mathbf{x}, 0) \u0026amp;= f(\\mathbf{x}) \u0026amp;\\text{on } \\mathbb{R} \\\\ \\partial_{t} p(\\mathbf{x}, 0) \u0026amp;= 0 \u0026amp;\\text{on } \\mathbb{R} \\end{align} $$\n上辺微分方程式の解は次の通りである。\n$$ \\begin{equation} p(\\mathbf{x}, t) = \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\end{equation} $$\nこのとき$\\hat{f}$は$f$の「フーリエ変換」（../1086）である。 今回は初期条件が以下のように与えられた波動方程式を考えてみよう。\n$$ \\begin{align*} \\partial_{t}^{2} p(\\mathbf{x}, t) \u0026amp;= \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;\\text{on } \\mathbb{R} \\times [0, \\infty) \\\\ p(\\mathbf{x}, 0) \u0026amp;= 0 \u0026amp;\\text{on } \\mathbb{R} \\\\ \\partial_{t} p(\\mathbf{x}, 0) \u0026amp;= g(\\mathbf{x}) \u0026amp;\\text{on } \\mathbb{R} \\end{align*} $$\n上辺微分方程式の解は次の通りである。\n$$ p(\\mathbf{x}, t) = \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{g} (\\boldsymbol{\\xi}) \\dfrac{\\sin (t \\left| \\boldsymbol{\\xi} \\right|)}{\\left| \\boldsymbol{\\xi} \\right|} e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\n説明 「フーリエ変換」(../1086)と「逆変換」(../1112)の定義を以下のようにしておこう。\n$$ \\hat{f}(\\boldsymbol{\\xi}) = \\int\\limits_{\\mathbb{R}^{n}} f(\\mathbf{x}) e^{\\mathrm{i} \\boldsymbol{\\xi} \\cdot \\mathbf{x}} \\mathrm{d} \\mathbf{x}, \\qquad f(\\mathbf{x}) = \\dfrac{1}{(2\\pi)^{n}}\\int\\limits_{\\mathbb{R}^{n}} f(\\mathbf{x}) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\n後者の証明法は前者と大同小異なので省略する。\n証明 $(4)$が$(1)$、$(2)$、$(3)$を満足させるか確認するだけだ。 まず、時間に対する2階導関数を計算してみると、\n$$ \\partial_{t}^{2} p(\\mathbf{x}, t) = -\\left| \\boldsymbol{\\xi} \\right|^{2} \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\nラプラシアンを計算してみると次のようになる。\n$$ \\begin{align*} \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) (\\Delta_{\\mathbf{x}} e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}}) \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= (- \\left| \\boldsymbol{\\xi} \\right|^{2}) \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \\end{align*} $$\nしたがって$(1)$が成立する。 $p(\\mathbf{x}, 0)$を計算してみると次のようになるので$(2)$が成立する。\n$$ \\begin{align*} p(\\mathbf{x}, 0) \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos ( 0 \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= f(\\mathbf{x}) \\end{align*} $$\n$(3)$が成立することも容易に確認できる。\n$$ \\begin{align*} \\partial_{t}p(\\mathbf{x}, 0) \u0026amp;= - \\left| \\boldsymbol{\\xi} \\right| \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\sin ( 0 \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= 0 \\end{align*} $$\n■\n","id":3623,"permalink":"https://freshrimpsushi.github.io/jp/posts/3623/","tags":null,"title":"初期条件が0の波動方程式の解。"},{"categories":"머신러닝","contents":"説明 AdaBeliefは2020年にJ. Zhuangらによって紹介されたオプティマイザで、Adamの変形の一つです1。PyTorchではこのオプティマイザをデフォルトで提供していないため、別途インストールして使用する必要があります。\nコード2 インストール cmdで以下のコードでインストールできます。\npip install adabelief-pytorch==0.2.0 使用方法 以下のコードで読み込んで使用できます。\nfrom adabelief_pytorch import AdaBelief\roptimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False) 環境 OS: Windows11 Version: Python 3.11.5, torch==2.0.1+cu118, adabelief-pytorch==0.2.0 https://arxiv.org/abs/2010.07468\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/juntang-zhuang/Adabelief-Optimizer?tab=readme-ov-file#installation-and-usage\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3620,"permalink":"https://freshrimpsushi.github.io/jp/posts/3620/","tags":null,"title":"파이토치에서 AdaBelief 옵티마이저 사용하는 방법"},{"categories":"줄리아","contents":"説明 カラーグラディエントは、Juliaの視覚化パッケージ Plots.jlがサポートする2つのカラースキームのうちの1つ（もう1つはパレット）で、私たちが一般的にグラデーションと呼んでいるものと同じだ。つまり、簡単に言うとグラデーションが実装されているタイプが ColorGradientである。\nグラディエントは、heatmap(), surface(), contour()などの図表を描くのに使われる。複数のグラフの色をそれぞれ変えたい場合は、グラディエントではなくパレットを使用する。\nコード シンボル cgrad(シンボル)で使用できる。デフォルトのグラディエントは cgrad(:inferno)で、色は以下の通りである。\nusing Plots\rcgrad(:inferno) heatmap(reshape(1:25, (5, 5))) Plots.jlに事前定義されているパレットとグラディエントは、公式ドキュメントで確認できる。(パッケージ ColorSchemes.jlの公式ドキュメントでさらに多様なパレットとグラディエントを探すことができる。)\nPythonのmatplotlibで imshowのデフォルトカラーマップでグラディエントに似たものは :viridisである。\nheatmap(reshape(1:25, (5, 5)), fillcolor = cgrad(:viridis)) 直接定義 cgrad([開始色, 終了色])で直接パレットを定義することができる。色が変わるポイントを設定するには、オプション引数として$0$と$1$の間の値を要素に持つベクトルを入力する。\ncgrad([:blue, :orange]) cgrad([:blue, :orange], [0.1, 0.9]) cgrad([:blue, :orange], [0.5, 0.50001]) キーワード rev キーワード引数として rev = trueを入力すると、順序が逆になる。\ncgrad(:darktest) cgrad(:darktest, rev = true) scale キーワードscaleは、グラディエントのスケールを指定する。:logまたは:expを入力できる。\ncgrad(:rainbow) cgrad(:rainbow, scale = :log) cgrad(:rainbow, scale = :exp) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参照 色の使い方 パレットの使い方 カラーグラディエント（グラデーション）の使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1,0,0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色指定の仕方 サブプロット毎にグラフの色を指定する方法 軸、軸の名前、目盛、目盛の値の色指定の仕方 背景色の指定の仕方 ","id":3608,"permalink":"https://freshrimpsushi.github.io/jp/posts/3608/","tags":null,"title":"ジュリアプロットでカラーグラデーションを使用する方法"},{"categories":"줄리아","contents":"説明 パレットとは、予め絞り出された絵の具がおかれている板のことを指します。数学的に説明すると、「色の集合」や「色の数列」と言えるでしょう。1つの絵に複数のグラフを描く際、最も一般的な方法は異なる色を使って区別することですが、その目的のためにJuliaでは、様々な色を集めたColorPaletteというタイプが実装されています。色のベクトルとして理解すると便利です。実際に、デフォルトのパレット：defaultを読み込んでみると、とても複雑に見えますが、中を見れば、ただの色のベクトルです。\nヒートマップを描くときは、パレットではなくグラデーションを使用します。\njulia\u0026gt; using Plots\rjulia\u0026gt; palette(:default)\rColorPalette(ColorSchemes.ColorScheme{Vector{RGB{Float64}}, String, String}(RGB{Float64}[RG\rB{Float64}(0.0,0.6056031611752245,0.9786801175696073), RGB{Float64}(0.8888735002725198,0.43\r564919034818994,0.2781229361419438), RGB{Float64}(0.2422242978521988,0.6432750931576305,0.3\r044486515341153), RGB{Float64}(0.7644401754934356,0.4441117794687767,0.8242975359232758), R\rGB{Float64}(0.6755439572114057,0.5556623322045815,0.09423433626639477), RGB{Float64}(4.8211\r81644776295e-7,0.6657589812923561,0.6809969518707945), RGB{Float64}(0.930767491919665,0.367\r4771896571412,0.5757699667547829), RGB{Float64}(0.7769816661712932,0.5097431319944513,0.146\r4252569555497), RGB{Float64}(3.8077343661790943e-7,0.6642678029460116,0.5529508754522481), RGB{Float64}(0.558464964115081,0.5934846564332882,0.11748125233232104), RGB{Float64}(5.9476\r23898072685e-7,0.6608785231434254,0.7981787608414297), RGB{Float64}(0.6096707676128648,0.49\r918492100827777,0.9117812665042642), RGB{Float64}(0.3800016049820351,0.5510532724353506,0.9\r665056985227146), RGB{Float64}(0.942181647954218,0.37516423354097583,0.4518168202944593), R\rGB{Float64}(0.8684020893043971,0.3959893639954845,0.7135147524811879), RGB{Float64}(0.42314\r674364630817,0.6224954944199981,0.19877060252130468)], \u0026#34;\u0026#34;, \u0026#34;\u0026#34;))\rjulia\u0026gt; palette(:default).colors.colors\r16-element Array{RGB{Float64},1} with eltype RGB{Float64}:\rRGB{Float64}(0.0,0.6056031611752245,0.9786801175696073)\rRGB{Float64}(0.8888735002725198,0.43564919034818994,0.2781229361419438)\rRGB{Float64}(0.2422242978521988,0.6432750931576305,0.3044486515341153)\rRGB{Float64}(0.7644401754934356,0.4441117794687767,0.8242975359232758)\rRGB{Float64}(0.6755439572114057,0.5556623322045815,0.09423433626639477)\rRGB{Float64}(4.821181644776295e-7,0.6657589812923561,0.6809969518707945)\rRGB{Float64}(0.930767491919665,0.3674771896571412,0.5757699667547829)\rRGB{Float64}(0.7769816661712932,0.5097431319944513,0.1464252569555497)\rRGB{Float64}(3.8077343661790943e-7,0.6642678029460116,0.5529508754522481)\rRGB{Float64}(0.558464964115081,0.5934846564332882,0.11748125233232104)\rRGB{Float64}(5.947623898072685e-7,0.6608785231434254,0.7981787608414297)\rRGB{Float64}(0.6096707676128648,0.49918492100827777,0.9117812665042642)\rRGB{Float64}(0.3800016049820351,0.5510532724353506,0.9665056985227146)\rRGB{Float64}(0.942181647954218,0.37516423354097583,0.4518168202944593)\rRGB{Float64}(0.8684020893043971,0.3959893639954845,0.7135147524811879)\rRGB{Float64}(0.42314674364630817,0.6224954944199981,0.19877060252130468) 既に定義されたパレットのシンボルを入力するか、色と長さを入力してpalette()関数でパレットを読み込んだり作ったりすることができます。図表を描くときは、plot()関数のpaletteキーワードに代入すればOKです。\nコード シンボル palette(シンボル)のように使用します。デフォルトのパレットのシンボルは:defaultで、色は以下の通りです。\n1つの絵に複数のグラフを描くと、上記の色が順番に適用されます。色を使い切った後は、再び最初から循環します。\nusing Plots x = 0:0.01:2π plot([x -\u0026gt; sin(x - a) for a in range(0, π, length = 5)], 0, 2π) Plots.jlに予め定義されたパレットやグラデーションは、公式ドキュメントで確認できます。(パッケージColorSchemes.jlの公式ドキュメントでも、より多様なパレットやグラデーションを見つけることができます。)\n:rainbowで描いてみると、\npalette(:rainbow) plot([x -\u0026gt; sin(x - a) for a in range(0, π, length = 5)], 0, 2π,\rpalette = palette(:rainbow)) 直接定義 palette([開始色、終了色], 長さ)で、直接パレットを定義することができます。また、range()を使用して色を補間することもできます。\npalette([:blue, :orange], 10) palette([RGB(0.5, 0.6, 0.2), RGB(1.0, 0.2, 0.9)], 10) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 併せて見る 色の使用方法 パレットの使用方法 カラーグラデーション(グラデーション)の使用方法 色処理のためのパッケージ Colors.jl RGBコードの使用方法 RGB(1, 0, 0) HEXコードの使用方法 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフの色を指定する方法 軸、軸名、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3607,"permalink":"https://freshrimpsushi.github.io/jp/posts/3607/","tags":null,"title":"ジュリアプロットでパレットを使用する方法"},{"categories":"줄리아","contents":"コード 大きくスケールの異なる2つのデータを同じプロットに描いた場合、下の図のようにスケールが小さい方が完全に無視されてしまう。\nusing Plots\rx = 0:0.01:2π\rplot(x, sin.(x))\rplot!(x, exp.(x)) 2つ目のデータをプロットするとき、twinx()を最初の引数に入力すれば、$x$軸を共有し、新しい$y$軸に対してグラフが描かれる。\nplot(x, sin.(x), ylabel = \u0026#34;sin x\u0026#34;)\rplot!(twinx(), x, exp.(x), ylabel = \u0026#34;exp x\u0026#34;) 逆に、$y$軸を共有して描くときは、twiny()を最初の引数に入力すればいい。\n環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3606,"permalink":"https://freshrimpsushi.github.io/jp/posts/3606/","tags":null,"title":"ジュリアプロットで異なるスケールの2つのデータ軸を共有して描く方法"},{"categories":"줄리아","contents":"説明 JuliaのPlots.jlでは、プロットも一つのオブジェクトだ。空のプロットを描いてタイプを確認すると、以下のようになる。\njulia\u0026gt; using Plots\rjulia\u0026gt; p = plot()\rjulia\u0026gt; p |\u0026gt; typeof\rPlots.Plot{Plots.GRBackend} Plots.を外してみると、Plot{GRBackend}となり、要素のデータタイプがFloat64のベクターがVector{Float64}と表示されるのと同様に、バックエンドがGRのプロットだという意味だ。Plotのプロパティを確認してみると、以下のようになる。\njulia\u0026gt; p |\u0026gt; propertynames\r(:backend, :n, :attr, :series_list, :o, :subplots, :spmap, :layout, :inset_subplots, :init) 各プロパティは、図の属性を含むベクターか、辞書か、そういうものだ。\np.backend プロットのバックエンドだ。\njulia\u0026gt; p.backend\rPlots.GRBackend() p.attr 図の属性に関する辞書だ。以下のような30個のキー・バリューを含んでいる。\njulia\u0026gt; plot(rand(10, 4), layout = 4).attr\rRecipesPipeline.DefaultsDict with 30 entries:\r:dpi =\u0026gt; 100\r:background_color_outside =\u0026gt; :match\r:plot_titlefontvalign =\u0026gt; :vcenter\r:warn_on_unsupported =\u0026gt; true\r:background_color =\u0026gt; RGBA{Float64}(1.0,1.0,1.0,1.0)\r:inset_subplots =\u0026gt; nothing\r:size =\u0026gt; (600, 400)\r:display_type =\u0026gt; :auto\r:overwrite_figure =\u0026gt; true\r:html_output_format =\u0026gt; :auto\r:plot_titlefontfamily =\u0026gt; :match\r:plot_titleindex =\u0026gt; 0\r:foreground_color =\u0026gt; RGB{N0f8}(0.0,0.0,0.0)\r:window_title =\u0026gt; \u0026#34;Plots.jl\u0026#34;\r:plot_titlefontrotation =\u0026gt; 0.0\r:extra_plot_kwargs =\u0026gt; Dict{Any, Any}()\r:pos =\u0026gt; (0, 0)\r:plot_titlefonthalign =\u0026gt; :hcenter\r:tex_output_standalone =\u0026gt; false\r:extra_kwargs =\u0026gt; :series\r:thickness_scaling =\u0026gt; 1\r:layout =\u0026gt; 4\r:plot_titlelocation =\u0026gt; :center\r:plot_titlefontsize =\u0026gt; 16\r:plot_title =\u0026gt; \u0026#34;\u0026#34;\r:show =\u0026gt; false\r:link =\u0026gt; :none\r:plot_titlefontcolor =\u0026gt; :match\r:plot_titlevspan =\u0026gt; 0.05\r:fontfamily =\u0026gt; \u0026#34;sans-serif\u0026#34;\rjulia\u0026gt; plot(rand(10, 4), layout = 4).attr[:size]\r(600, 400) p.series_list 各データのグラフに関する属性の辞書を要素とするベクターだ。\njulia\u0026gt; plot(rand(10,5)).series_list\r5-element Vector{Plots.Series}:\rjulia\u0026gt; plot(plot(rand(10, 4)), plot(rand(10, 3))).series_list\r7-element Vector{Plots.Series}: 各辞書に含まれるキー・バリューは、以下の通りだ。\njulia\u0026gt; plot(rand(10, 2)).series_list[1].plotattributes\rRecipesPipeline.DefaultsDict with 62 entries:\r:plot_object =\u0026gt; Plot{Plots.GRBackend() n=2}\r:subplot =\u0026gt; Subplot{1}\r:label =\u0026gt; \u0026#34;y1\u0026#34;\r:fillalpha =\u0026gt; nothing\r:linealpha =\u0026gt; nothing\r:linecolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:x_extrema =\u0026gt; (NaN, NaN)\r:series_index =\u0026gt; 1\r:markerstrokealpha =\u0026gt; nothing\r:markeralpha =\u0026gt; nothing\r:seriestype =\u0026gt; :path\r:z_extrema =\u0026gt; (NaN, NaN)\r:x =\u0026gt; Base.OneTo(10)\r:markerstrokecolor =\u0026gt; RGBA{Float64}(0.0,0.0,0.0,1.0)\r:fillcolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:clims_calculated =\u0026gt; (NaN, NaN)\r:seriescolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:extra_kwargs =\u0026gt; Dict{Symbol, Any}()\r:z =\u0026gt; nothing\r:series_plotindex =\u0026gt; 1\r:y =\u0026gt; [0.477103, 0.00362131, 0.864524, 0.391488, 0.663659, 0.89787, 0.157973, 0.964416, 0.806635, 0.243531]\r:markercolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:y_extrema =\u0026gt; (0.00362131, 0.964416)\r:linewidth =\u0026gt; 1\r:group =\u0026gt; nothing\r:stride =\u0026gt; (1, 1)\r:permute =\u0026gt; :none\r:marker_z =\u0026gt; nothing\r:show_empty_bins =\u0026gt; false\r:seriesalpha =\u0026gt; nothing\r:smooth =\u0026gt; false\r:zerror =\u0026gt; nothing\r:arrow =\u0026gt; nothing\r:normalize =\u0026gt; false\r:linestyle =\u0026gt; :solid\r:contours =\u0026gt; false\r:bar_width =\u0026gt; nothing\r:bins =\u0026gt; :auto\r:markerstrokestyle =\u0026gt; :solid\r:weights =\u0026gt; nothing\r:z_order =\u0026gt; :front\r:fill_z =\u0026gt; nothing\r:markershape =\u0026gt; :none\r:markerstrokewidth =\u0026gt; 1\r:xerror =\u0026gt; nothing\r:bar_position =\u0026gt; :overlay\r:contour_labels =\u0026gt; false\r:hover =\u0026gt; nothing\r:primary =\u0026gt; true\r:yerror =\u0026gt; nothing\r:ribbon =\u0026gt; nothing\r:fillstyle =\u0026gt; nothing\r:line_z =\u0026gt; nothing\r:orientation =\u0026gt; :vertical\r:markersize =\u0026gt; 4\r:bar_edges =\u0026gt; false\r:quiver =\u0026gt; nothing\r:fillrange =\u0026gt; nothing\r:colorbar_entry =\u0026gt; true\r:series_annotations =\u0026gt; nothing\r:levels =\u0026gt; 15\r:connections =\u0026gt; nothing\rjulia\u0026gt; plot(rand(10, 2)).series_list[1][:label]\r\u0026#34;y1\u0026#34; 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3605,"permalink":"https://freshrimpsushi.github.io/jp/posts/3605/","tags":null,"title":"ジュリアプロッツでのプロットのプロパティリスト"},{"categories":"줄리아","contents":"概要 Plots.jlでの図の背景の格子に関連するキーワードは次の通りだ。\nキーワード名 機能 grid 格子表示 gridalpha, ga, gα 格子の透明度指定 foreground_color_grid, fgcolor_grid 格子の色指定 gridlinewidth, grid_lw 格子の太さ指定 gridstyle, grid_ls 格子の線スタイル指定 minorgrid 補助格子表示 minorgridalpha 補助格子の透明度指定 foreground_color_minor_grid, fgcolor_minorgrid 補助格子の色指定 minorgridlinewidth, minorgrid_lw 補助格子の太さ指定 minorgridstyle, minorgrid_ls 補助格子の線スタイル指定 コード 格子表示 格子を表示するキーワードはgridだ。:xや:yを入力するとそれぞれ$x$軸の目盛り補助線、$y$軸の目盛り補助線のみを表示する。falseを入力すると格子を表示しない。\nplot(plot(rand(10)), plot(rand(10), grid = :x), plot(rand(10), grid = :y), plot(rand(10), grid = false)) 透明度 背景の格子は基本的に0.1の透明度で描かれる。格子の透明度を調節するキーワードはgridalpha(=ga)(=gα)だ。\nplot(rand(10, 3), layout = (3, 1), gridalpha = [0.1 0.5 1]) 色 格子の基本色は黒で、キーワードforeground_color_grid(=fgcolor_grid)で他の色を指定できる。\nplot(rand(10, 3), layout = (3, 1), gridalpha = 1, fgcolor_grid = [:red :green :orange]) 太さ 格子の太さを指定するキーワードはgridlinewidth(=grid_lw)で、基本値は0.5だ。\nplot(rand(10, 3), layout = (3, 1), grid_lw = [0.5 5 10]) 格子スタイル キーワードgridstyle(=grid_ls)で格子の線スタイルを指定できる。可能なシンボルは:auto, :solid, :dash, :dot, :dashdot, :dashdotdot。\nplot(rand(10, 2), layout = 2, ga = 1, gridstyle = [:solid :dash]) 補助格子 キーワード引数としてminorgrid = trueを入力すると補助格子を描く。補助格子の透明度、色、線の太さ、線スタイルを指定するキーワードはそれぞれminorgridalpha, foreground_color_minor_grid minorgrid_lw, minorgrid_lsだ。\nplot(plot(rand(10)), plot(rand(10), minorgrid = true), gridalpha = 0.8, minorgridalpha = 0.2) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3604,"permalink":"https://freshrimpsushi.github.io/jp/posts/3604/","tags":null,"title":"ジュリアプロットで背景のグリッドを飾る方法"},{"categories":"줄리아","contents":"概要 Plots.jlで図の背景色に関連するキーワードは次の通りです。\nキーワード名 機能 background_color, bg_color 全体の背景の色を指定 background_color_outside, bg_color_outside グラフが描かれた外側の領域の色を指定 background_subplot, bg_subplot グラフが描かれた領域の色を指定 background_inside, bg_inside 凡例を除いたグラフが描かれた領域の色を指定 コード 背景色を指定するキーワードはbackground_color(=bg_color)です。凡例、グラフが描かれた場所とその他の全ての背景色を入力した値で指定します。\nplot(rand(10), bg_color = :tomato) グラフが描かれた領域外側の色を指定するキーワードはbackground_color_outside(=bg_color_outside)です。\nplot(rand(10), bg_color_outside = :palegreen) グラフが描かれた領域の色を指定するキーワードはbackground_subplot(=bg_subplot)です。\nplot(rand(10), bg_subplot = :violet) 凡例を除いてグラフが描かれた領域の色を指定するキーワードはbackground_inside(=bg_inside)です。\nplot(rand(10), bg_inside = :brown4) サブプロット 複数のサブプロットがある場合、bg_subplotやbg_insideで色を指定する必要があり、全体のプロットにまとめたときそれぞれの背景色を維持します。\np₁ = plot(rand(10), bg_subplot = :tomato)\rp₂ = scatter(rand(10), bg_inside = :yellow)\rp = plot(p₁, p₂) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参照 色の使用方法 パレットの使用方法 カラーグラデーションの使用方法 色処理のためのパッケージ Colors.jl RGBコードの使用方法 RGB(1, 0, 0) HEXコードの使用方法 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフ色を指定する方法 軸、軸名、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3603,"permalink":"https://freshrimpsushi.github.io/jp/posts/3603/","tags":null,"title":"ジュリア・プロットで背景色を指定する方法"},{"categories":"줄리아","contents":"概要 サブプロットごとにグラフの色を指定する3つの方法を紹介する。グラフ要素に色を指定する方法はここを参照してください。\n方法 1 サブプロットのグラフの色を指定する最初の方法は、各サブプロットを定義するときにあらかじめ色を指定することです。Juliaでは、1枚の絵が各オブジェクトであるため、属性を異にする絵を複数定義してから、それらを1つのプロットにまとめればよいです。\np₁ = plot(rand(10), lc = :red)\rp₂ = scatter(rand(10), mc = :blue)\rp₃ = bar(rand(10), fc = :green)\rplot(p₁, p₂, p₃,\rlayout = (3, 1),\rtitle = [\u0026#34;p₁\u0026#34; \u0026#34;p₂\u0026#34; \u0026#34;p₃\u0026#34;],\r) 方法 2 2つ目の方法は、全体のプロットを定義するときに、キーワード引数として色の行ベクトルを入力することです。列ベクトルではなく行ベクトルでなければならないことに注意してください。\np₄ = plot(rand(10))\rp₅ = plot(rand(10))\rp₆ = plot(rand(10))\rplot(p₄, p₅, p₆,\rlayout = (3, 1),\rlinecolor = [:brown :purple :orange],\rtitle = [\u0026#34;p₄\u0026#34; \u0026#34;p₅\u0026#34; \u0026#34;p₆\u0026#34;],\r) 方法 3 3つ目の方法は、全体のプロットを定義した後に、各サブプロットのプロパティ値を直接変更することです。プロパティ.series_listは、各サブプロットのシリーズ属性情報を含む辞書のベクトルです。すなわち、p.series_list[1]は、最初のサブプロットのシリーズ属性辞書を返します。この辞書に:linecolorキーを入力してその値を変更することで、最初のサブプロットの線の色が変わります。\np₇ = plot(rand(10))\rp₈ = scatter(rand(10))\rp₉ = bar(rand(10))\rp = plot(p₇, p₈, p₉,\rlayout = (3, 1),\rtitle = [\u0026#34;p₇\u0026#34; \u0026#34;p₈\u0026#34; \u0026#34;p₉\u0026#34;],\r)\rp.series_list[1][:linecolor] = :goldenrod1\rp.series_list[2][:markercolor] = :olivedrab3\rp.series_list[3][:fillcolor] = :hotpink3 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参照 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとのグラフの色の指定方法 軸、軸の名前、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3602,"permalink":"https://freshrimpsushi.github.io/jp/posts/3602/","tags":null,"title":"Julia Plotsで各サブプロットごとにグラフの色を指定する方法"},{"categories":"줄리아","contents":"要約 Plots.jlでは、グラフの各構成要素の色を指定するキーワードは以下の通りだ。\nキーワード 機能 markercolor, mc マーカー内部の色を指定 markerstrokecolor, msc マーカーの縁の色を指定 linecolor, lc 線の色を指定 fillcolor, fc 面積の色を指定 seriescolor, c すべての要素の色を指定 キーワード 機能 markeralpha, ma, mα マーカー内部の透明度を指定 markerstrokealpha, msa, msα マーカーの縁の透明度を指定 linealpha, la, lα 線の透明度を指定 fillalpha, fa, fα 面積の透明度を指定 seriesalpha, a, α すべての要素の透明度を指定 色 Plots.jlでは、変更可能な対象は点、線、面の三つだ。それぞれの色を指定するキーワード引数は、markercolor(=mc)、linecolor(=lc)、そしてfillcolor(=fc)である。これらのキーワードで指定された属性は互いに影響を与えないので、線グラフを描いてmc = :redと入力しても、線の色が赤になることはない。実際にp = plot(rand(10), mc = :red)のプロパティを確認すると、以下のようになる。\njulia\u0026gt; p = plot(rand(10), mc = :red)\rjulia\u0026gt; p.series_list[1][:linecolor]\rRGBA{Float64}(0.0,0.6056031611752245,0.9786801175696073,1.0)\rjulia\u0026gt; p.series_list[1][:markercolor]\rRGBA{Float64}(1.0,0.0,0.0,1.0) プロットされた線グラフの線色は依然としてデフォルトの色であり、赤ではない。\nだから、複数のサブプロットを描き、上記の三つのキーワードで色を指定すると、それぞれが適用される。点（マーカー）を紫色の:purple、線をダークグリーンの:darkgreen、面をスカイブルーの:skyblueで色付けすると、以下のようになる。\nst = [:line :scatter :barhist :steppre :scatterhist :bar]\rx = rand(20)\ry = repeat(x, outer = (1, length(st)))\rplot(y, seriestype = st, layout = 6, mc = :purple,\rlc = :darkgreen,\rfc = :skyblue\r) 透明度 色の透明度を決定するキーワードは、色を指定するキーワード名でcolorをalphaに置き換えたものだ。また、ギリシャ文字のαを直接使用してもよい。\nまたは、RGBAのように、透明度が含まれる色コードを、色を指定するキーワードに入力してもよい。\nplot(rand(20, 3), layout = (3,1), seriestype = [:scatter :line :bar],\rmc = :red,\rlc = :green,\rfc = :blue\r) plot(rand(20, 3), layout = (3,1), seriestype = [:scatter :line :bar],\rmarkeralpha = 0.5, mc = :red,\rla = 0.5, lc = :green,\rfα = 0.5, fc = :blue\r) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参考 色の使い方 パレットの使い方 カラーグラデーション（グラデーション）の使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフの色の指定方法 軸、軸の名前、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3601,"permalink":"https://freshrimpsushi.github.io/jp/posts/3601/","tags":null,"title":"ジュリアプロットでグラフ要素の色を指定する方法"},{"categories":"줄리아","contents":"コード Juliaで色を扱うために提供されるパッケージはColors.jlだ。視覚化パッケージのPlots.jlを読み込むと、Colors.jlの中の機能も一緒に使用できる。RGB空間を表す色コードにはRGB, BGR, RGB24, RGBX, XRGBがサポートされており、これらはAbstractRGBのサブタイプだ。RGBAはRGBに透明度が加わったものだ。\njulia\u0026gt; using Plots\rjulia\u0026gt; subtypes(AbstractRGB)\r5-element Vector{Any}:\rBGR\rRGB\rRGB24\rRGBX\rXRGB\rjulia\u0026gt; subtypes(AbstractRGBA)\r2-element Vector{Any}:\rBGRA\rRGBA 文字列 関数plot()の色を指定するキーワードに文字列で\u0026quot;rgb(255, 0, 0)\u0026quot;のように入力すれば、RGBコードが(255, 0, 0)の色を使用できる。下を見ればわかるが、文字列を入力してもいい理由は、plot()が文字列を自動でパースしてくれるからのようだ。名前がある色の場合は、\u0026quot;red\u0026quot;や:redのように文字列やシンボルで使用できる。\nusing Plots\rr = \u0026#34;rgb(255, 0, 0)\u0026#34; # 정수로 표현된 RGB 빨간색\rg = \u0026#34;rgba(0, 255, 0, 0.2)\u0026#34; # 투명도가 0.2인 RGB 초록색\rp = \u0026#34;rgb(50%, 0%, 100%)\u0026#34; # 퍼센테이지로 표현된 RGB 보라색\rplot(\rplot(rand(15), lc = r),\rbar(rand(15), fc = g),\rscatter(rand(15), mc = p),\rlayout = (3, 1)\r) パース colorant\u0026quot;rgb(0, 0, 0)\u0026quot;のようにRGB色コードをパースできる。\njulia\u0026gt; r = colorant\u0026#34;rgb(255, 0, 0)\u0026#34; # 정수로 표현된 RGB 빨간색\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; g = colorant\u0026#34;rgba(0, 255, 0, 0.2)\u0026#34; # 투명도가 0.2인 초록색\rRGBA{N0f8}(0.0,1.0,0.0,0.502)\rjulia\u0026gt; p = colorant\u0026#34;rgb(50%, 0%, 100%)\u0026#34; # 퍼센테이지로 표현된 RGB 보라색\rRGB{N0f8}(0.502,0.0,1.0)\rjulia\u0026gt; plot(\rplot(rand(15), lc = r),\rbar(rand(15), fc = g),\rscatter(rand(15), mc = p),\rlayout = (3, 1)) parse(RGB, \u0026quot;rgb(0, 255, 255)\u0026quot;)と同様にパースすることができる。\njulia\u0026gt; parse(RGB, \u0026#34;rgb(0, 255, 255)\u0026#34;)\rRGB{N0f8}(0.0,1.0,1.0)\rjulia\u0026gt; parse(RGBA, \u0026#34;rgba(0, 255, 0, 0.5)\u0026#34;)\rRGBA{N0f8}(0.0,1.0,0.0,0.502) 直接定義 関数RGB(), RGBA()などを使えば、色を直接定義できる。\njulia\u0026gt; RGB(1, 0, 0)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0, 0.0, 0.0)\rRGB{Float64}(1.0,0.0,0.0)\rjulia\u0026gt; RGBA(1, 0, 0.5, 0.5)\rRGBA{Float64}(1.0,0.0,0.5,0.5) colorantでパースされた色と正確に同じタイプを得るには、入力としてN0f8タイプの数を入力する必要がある。これを使用するためにはFixedPointNumbers.jlが必要だ。または、直接1.0N0f8のように定義してもいい。以下は、colorant\u0026quot;rgb(255, 0, 0)\u0026quot;と同じ、赤色の色を返すコードだ。\njulia\u0026gt; using FixedPointNumbers\r# RGB 빨간색 RGB\rjulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0, 0.0, 0.0)\rRGB{Float64}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0N0f8, 0N0f8, 0N0f8)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(reinterpret(N0f8, UInt8(255)), reinterpret(N0f8, UInt8(0)), reinterpret(N0f8, UInt8(0)))\rRGB{N0f8}(1.0,0.0,0.0) 他の色空間からの変換 関数convert()は、他の色空間の色コードをRGBコードに変換する。\njulia\u0026gt; using Colors julia\u0026gt; convert(RGB{N0f8}, HSL(270, 0.5, 0.5)) ERROR: UndefVarError: `N0f8` not defined julia\u0026gt; using FixedPointNumbers julia\u0026gt; convert(RGB{N0f8}, HSL(270, 0.5, 0.5)) RGB{N0f8}(0.502,0.251,0.749) 色名を得る 関数rgb_string()とrgba_string()はそれぞれ色のRGB、RGBAコードを文字列で返す。\njulia\u0026gt; rgb_string(colorant\u0026#34;rgba(255, 0, 0, 0.5)\u0026#34;)\r\u0026#34;rgb(255, 0, 0)\u0026#34;\rjulia\u0026gt; rgba_string(colorant\u0026#34;rgba(255, 0, 0, 0.5)\u0026#34;)\r\u0026#34;rgba(255, 0, 0, 0.502)\u0026#34;\rjulia\u0026gt; rgb_string(colorant\u0026#34;red\u0026#34;)\r\u0026#34;rgb(255, 0, 0)\u0026#34;\rjulia\u0026gt; rgb_string(parse(RGB, :blue))\r\u0026#34;rgb(0, 0, 255)\u0026#34;\rjulia\u0026gt; rgb_string(colorant\u0026#34;#00FF00\u0026#34;)\r\u0026#34;rgb(0, 255, 0)\u0026#34; 併せて見る Plotsで色を使う方法 RGB色コードの使い方 HEX色コードの使い方 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10, FixedPointNumbers v0.8.4 併せて見る 色を使う方法 パレットの使い方 カラーグラデーションの使い方 色の処理のためのパッケージColors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフ色を指定する方法 軸、軸名、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3600,"permalink":"https://freshrimpsushi.github.io/jp/posts/3600/","tags":null,"title":"ジュリアでRGBカラーコードを使用する方法"},{"categories":"줄리아","contents":"概要1 Juliaで色処理のためのパッケージであるColors.jlの機能について紹介する。視覚化パッケージであるPlots.jlを使う場合はColors.jlを別に読み込む必要はない。以下の機能を提供する。\n色のパースと変換 色マップ 色スケール パースと変換 strを色情報を表した文字列とすると、@colorant_strまたはparse(Colorant, str)を通じて文字列を特定の色空間の色コードにパースできる。なお、colorantは染料、色素などの意味を持つ英単語である。\nRGBコードの使い方 HEXコードの使い方 julia\u0026gt; using Colors\rjulia\u0026gt; colorant\u0026#34;red\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(Colorant, \u0026#34;rgba(0, 255, 0, 0.5)\u0026#34;)\rRGBA{N0f8}(0.0,1.0,0.0,0.502)\rjulia\u0026gt; parse(Colorant, \u0026#34;#FF0000\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(Colorant, \u0026#34;hsl(120, 100%, 25%)\u0026#34;)\rHSL{Float32}(120.0f0,1.0f0,0.25f0) convert()関数で他の色空間の色コードに変換できる。\njulia\u0026gt; convert(RGB, HSL(270, 0.5, 0.5))\rRGB{Float64}(0.5,0.25,0.75) 色の補間 range()関数を通じて色を補間interpolationできる。この動作は非常に論理的で直感的である。例えば、RGBコードを考えてみよう。赤はRGBコードで$(255, 0, 0)$または$(1, 0, 0)$で表され、これは実質的に3次元ベクトルと同じだ。したがって、二つのベクトル間を補間するrange()関数の引数として色コードを使う理由はない。\njulia\u0026gt; v1 = [1.0, 0.0, 0.0];\rjulia\u0026gt; v2 = [0.0, 0.5, 0.0];\rjulia\u0026gt; collect(range(v1, v2, length = 15))\r15-element Vector{Vector{Float64}}:\r[1.0, 0.0, 0.0]\r[0.9285714285714286, 0.03571428571428571, 0.0]\r[0.8571428571428572, 0.07142857142857142, 0.0]\r[0.7857142857142857, 0.10714285714285714, 0.0]\r[0.7142857142857143, 0.14285714285714285, 0.0]\r[0.6428571428571428, 0.17857142857142858, 0.0]\r[0.5714285714285714, 0.21428571428571427, 0.0]\r[0.5, 0.25, 0.0]\r[0.4285714285714286, 0.2857142857142857, 0.0]\r[0.3571428571428571, 0.32142857142857145, 0.0]\r[0.2857142857142857, 0.35714285714285715, 0.0]\r[0.2142857142857143, 0.39285714285714285, 0.0]\r[0.1428571428571429, 0.42857142857142855, 0.0]\r[0.0714285714285714, 0.4642857142857143, 0.0]\r[0.0, 0.5, 0.0]\rjulia\u0026gt; c1 = colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; c2 = colorant\u0026#34;rgb(0, 128, 0)\u0026#34;\rRGB{N0f8}(0.0,0.502,0.0)\rjulia\u0026gt; range(colorant\u0026#34;rgb(255,0,0)\u0026#34;, colorant\u0026#34;rgb(0,128,0)\u0026#34;, length=15)\r15-element Array{RGB{N0f8},1} with eltype RGB{FixedPointNumbers.N0f8}:\rRGB{N0f8}(1.0,0.0,0.0)\rRGB{N0f8}(0.929,0.035,0.0)\rRGB{N0f8}(0.859,0.071,0.0)\rRGB{N0f8}(0.784,0.11,0.0)\rRGB{N0f8}(0.714,0.145,0.0)\rRGB{N0f8}(0.643,0.18,0.0)\rRGB{N0f8}(0.573,0.216,0.0)\rRGB{N0f8}(0.502,0.251,0.0)\rRGB{N0f8}(0.427,0.286,0.0)\rRGB{N0f8}(0.357,0.322,0.0)\rRGB{N0f8}(0.286,0.357,0.0)\rRGB{N0f8}(0.216,0.392,0.0)\rRGB{N0f8}(0.141,0.431,0.0)\rRGB{N0f8}(0.071,0.467,0.0)\rRGB{N0f8}(0.0,0.502,0.0) 上記の色レンジの視覚化は、VS CodeでJulia拡張機能をインストールして実行すると以下のように得られる。\nまた、range()の戻り値はパレットとして使用できる。\nmy_palette = range(colorant\u0026#34;rgb(255,0,0)\u0026#34;, colorant\u0026#34;rgb(0,128,0)\u0026#34;, length=15)\rplot(rand(10, 15), palette = my_palette) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10 関連項目 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理のためのパッケージColors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロット毎にグラフの色を指定する方法 軸、軸ラベル、目盛り、目盛り値の色の指定方法 背景色の設定方法 https://juliagraphics.github.io/Colors.jl/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3599,"permalink":"https://freshrimpsushi.github.io/jp/posts/3599/","tags":null,"title":"ジュリアのカラー処理のためのパッケージ"},{"categories":"줄리아","contents":"概要 Juliaで色を便利に使うためのパッケージにはColors.jlがある。「Plots.jl」という視覚化パッケージを読み込めば一緒に使うことができる。\nシンボルと文字列 名前がついた色のリストを確認する方法は、コンソールにColors.color_namesを入力するか、公式ドキュメントを確認することだ。\njulia\u0026gt; using Plots\rjulia\u0026gt; Colors.color_names\rDict{String, Tuple{Int64, Int64, Int64}} with 666 entries:\r\u0026#34;darkorchid\u0026#34; =\u0026gt; (153, 50, 204)\r\u0026#34;chocolate\u0026#34; =\u0026gt; (210, 105, 30)\r\u0026#34;chocolate2\u0026#34; =\u0026gt; (238, 118, 33)\r\u0026#34;grey69\u0026#34; =\u0026gt; (176, 176, 176)\r\u0026#34;grey97\u0026#34; =\u0026gt; (247, 247, 247)\r\u0026#34;olivedrab3\u0026#34; =\u0026gt; (154, 205, 50)\r\u0026#34;deeppink2\u0026#34; =\u0026gt; (238, 18, 137)\r\u0026#34;mediumpurple2\u0026#34; =\u0026gt; (159, 121, 238)\r\u0026#34;ivory1\u0026#34; =\u0026gt; (255, 255, 240)\r⋮ =\u0026gt; ⋮ 色を指定できるキーワード引数には基本的にシンボルと文字列が使用できる。色名をシンボル、文字列で入力すれば、その色が反映される。入力するものが何であれ、Colors.parse(Colorant, 色名)に渡されるため、シンボルでも文字列でも結果は同じである。\njulia\u0026gt; Colors.parse(Colorant, :red)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; Colors.parse(Colorant, \u0026#34;red\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0) 様々なグラフに色を指定してみると、その結果は次のようになる。\nplot(randn(50, 6),\rseriescolor = [:red :hotpink1 :purple3 \u0026#34;blue\u0026#34; \u0026#34;lime\u0026#34; \u0026#34;brown4\u0026#34;],\rseriestype = [:line :scatter :histogram :shape :sticks :steppre],\rlayout = (3,2)\r) RGB RGB色コードはcolorant\u0026quot;rgb(255, 0, 0)\u0026quot;で使用できる。rgb()には$[0, 255]$内の整数のみ入力できる。\njulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34; # rgb() notation with integers in [0, 255]\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;rgba(0, 0, 255, 0.5)\u0026#34; # with alpha in [0, 1]\rRGBA{N0f8}(0.0,0.0,1.0,0.502)\rplot(rand(20, 2),\rseriescolor = [colorant\u0026#34;rgb(255, 0, 0)\u0026#34; colorant\u0026#34;rgba(0, 0, 255, 0.5)\u0026#34;],\rlayout = 2\r) RGB色コードを扱う詳細はこちらを参照。\nHEX 6桁のHEXコードはcolorant\u0026quot;#FF0000\u0026quot;、3桁のHEXコードはcolorant\u0026quot;#00f\u0026quot;のように使用できる。\njulia\u0026gt; colorant\u0026#34;#FF0000\u0026#34; # 6-digit hex notation\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;#00f\u0026#34; # 3-digit hex notation\rRGB{N0f8}(0.0,0.0,1.0)\rjulia\u0026gt; plot(rand(20, 2),\rseriescolor = [colorant\u0026#34;#FF0000\u0026#34; colorant\u0026#34;#00f\u0026#34;],\rlayout = 2\r) HEX色コードを扱う詳細はこちらを参照。\n環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10 一緒に見る 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定の仕方 サブプロットごとのグラフ色の指定の仕方 軸、軸名、目盛り、目盛り値の色の指定の仕方 背景色の指定の仕方 ","id":3598,"permalink":"https://freshrimpsushi.github.io/jp/posts/3598/","tags":null,"title":"ジュリアプロットでの色の使用方法"},{"categories":"줄리아","contents":"コード 関数 printstyled(文字列; color = 色)を使用すると、出力される関数を装飾できる。キーワード引数 colorの入力としては、シンボル、自然数$(0 \\le n \\le 255)$が可能である。文字列は不可能であることに注意。\n利用可能なシンボルには、:blink、:reverse 等の色ではないものも含まれている。これらはキーワード引数として blink = true、bold = true等と入力して適用することもできる。\n:normal :default :blink :bold :hidden :nothing :reverse :underline :white :light_white :black :light_black :blue :light_blue :cyan :light_cyan :green :light_green :magenta :light_magenta :red :light_red :yellow :light_yellow symbols = [:normal :default :blink :bold :hidden :nothing :reverse :underline :white :light_white :black :light_black :blue :light_blue :cyan :light_cyan :green :light_green :magenta :light_magena :red :light_red :yellow :light_yellow]\rfor i ∈ 1:length(symbols)\rprintstyled(\u0026#34;Hello ($(symbols[i]))\\n\u0026#34;, color = symbols[i])\rend Base.text_colorsを入力すると、キーワード引数（シンボルを含む）で可能な全ての値を返す。\njulia\u0026gt; Base.text_colors\rDict{Union{Int64, Symbol}, String} with 280 entries:\r56 =\u0026gt; \u0026#34;\\e[38;5;56m\u0026#34;\r35 =\u0026gt; \u0026#34;\\e[38;5;35m\u0026#34;\r60 =\u0026gt; \u0026#34;\\e[38;5;60m\u0026#34;\r220 =\u0026gt; \u0026#34;\\e[38;5;220m\u0026#34;\r:blink =\u0026gt; \u0026#34;\\e[5m\u0026#34;\r67 =\u0026gt; \u0026#34;\\e[38;5;67m\u0026#34;\r215 =\u0026gt; \u0026#34;\\e[38;5;215m\u0026#34;\r73 =\u0026gt; \u0026#34;\\e[38;5;73m\u0026#34;\r251 =\u0026gt; \u0026#34;\\e[38;5;251m\u0026#34;\r115 =\u0026gt; \u0026#34;\\e[38;5;115m\u0026#34;\r⋮ =\u0026gt; ⋮ 参照 パッケージ Crayons.jlも使用できる。\n環境 OS: Windows11 Version: Julia 1.9.4 ","id":3597,"permalink":"https://freshrimpsushi.github.io/jp/posts/3597/","tags":null,"title":"ジュリアでテキスト出力を装飾する組み込み関数"},{"categories":"데이터과학","contents":"定義 データセット$X \\subset \\mathbb{R}^{n}$が与えられたとする。$m \\lt n$に対して、次のようなマッピングを次元削減dimension reductionという。\n$$ r : X \\to \\mathbb{R}^{m} $$\nまたは、主に機械学習で、パフォーマンスを可能な限り維持しながら入力変数を減らす方法の全体を次元削減技術という。\n説明 次元削減とは、その名の通りベクトルの次元を減らすことを言う。データをより簡単に、より直感的に理解するために使用されることが多い。次元を減らす方法はアルゴリズムによって異なる。特定の成分をそのまま削除することもあれば、既存のデータを用いて定められたルールに従って次元が小さい新しいデータを生成することもある。以下のような技術がある。\n主成分分析PCA 数理統計学での主成分分析 目的 可視化 我々は4次元以上のデータを効率的に可視化することが事実上不可能である。さらに3次元のデータでさえ、その形状によっては可視化に困難を感じることがある。可視化に困難を感じるとは、データの特徴をよく表す図を描くことが難しいということである。3次元データであれば、視点によってその形状が異なって見えるだろう。このような時に次元を減らして描いてみると、データの特徴を把握しやすくなるかもしれない。以下の図は、同じデータだが見る方向によってその形状が著しく異なる例を示している。右の図は左のデータを$xy$-平面に射影したものである。\n4次元データであるアイリスデータセットを以下のように複数の2次元図に分けて可視化することが、多くのデータサイエンスの教科書で紹介されている。\n選択と集中 あまり重要でない情報を排除して、より重要な情報に集中するために次元削減を使用することができる。ここで言う「あまり重要でない情報」とは、ノイズとして扱われるか、重複した情報を指す。例えば、下の左の表を見ると、最初の列がすべてのデータに対して同じ値であることがわかる。また、2番目の列と3番目の列は異なる値だが、実質的には同じ値であることがわかる。したがって、最初の列と2番目（または3番目）の列を削除することで次元削減を行うことができる。また、右の表は大邱の天気情報をまとめたものである。一見すると、ここで不要な情報はないように見えるが、「日差 = 最高気温 - 最低気温」であるため、この3つは線形独立ではなく、実際には回帰分析時にエラーを引き起こす可能性がある。したがって、この場合には多重共線性を除去するために4番目の列を削除することが次元削減である。\n学校\r学年\r所属\r名前\rハイブ高等学校\r3学年\rプロミスナイン\nfromis_9\rイ・ナギョン\n이나경\rハイブ高等学校\r3学年\rプロミスナイン\nfromis_9\rペク・ジホン\n백지헌\rハイブ高等学校\r2学年\rル・セラフィム\nLE SSERAFIM\rキム・チェウォン\n김채원\rハイブ高等学校\r2学年\rル・セラフィム\nLE SSERAFIM\rホ・ユンジン\n허윤진\rハイブ高等学校\r1学年\rニュージンス\nNewJeans\rヘリン\n해린\rハイブ高等学校\r1学年\rニュージンス\nNewJeans\rミンジ\n민지\r日付\r最高気温\r最低気温\r日差\r降水確率\r19일\r32º\r24º\r8º\r60%\r20일\r33º\r22º\r11º\r0%\r21일\r32º\r23º\r9º\r30%\r22일\r30º\r21º\r9º\r60%\r23일\r31º\r24º\r7º\r60%\r24일\r33º\r25º\r8º\r60%\r軽量化 データの次元が減ると、それだけ保存しなければならない数字が少なくなるため、データ自体の容量が減る。人工神経網の場合、MLPは線形層で構成されており、入力データの次元がモデルのパラメータ数に影響を与える。この場合、次元削減を使用してモデルのパラメータを減らすことができる。CNNのように、入力データの次元がモデルのパラメータ数に影響を与えない場合でも、計算速度での利点をもたらすことができる。\n過学習防止 適切な次元削減は、オーバーフィッティングをある程度防ぐことができるとされている。\n","id":3563,"permalink":"https://freshrimpsushi.github.io/jp/posts/3563/","tags":null,"title":"データサイエンスにおける次元削減"},{"categories":"머신러닝","contents":"概要 TensorFlowでは、Kerasを使用して簡単にニューラルネットワークを定義することができます。以下では、Sequential()と関数型APIを使用してシンプルなMLPを定義し、訓練する方法を紹介します。ただし、Sequential()はモデルの定義自体は簡単ですが、それを使用して複雑な構造を設計するには適していません。同様に、関数型APIを使用して複雑な構造を設計する場合は、keras.Modelクラスの使用が適しており、より複雑で自由なカスタマイズを求める場合は、Kerasを使用せずに低レベルで実装する方が良いでしょう。どのような作業にディープラーニングを使用するかによって異なりますが、もし自分が理工学の研究者であり、専門分野にディープラーニングを応用したい場合は、以下の方法を主に使用する可能性は低いでしょう。ディープラーニングを初めて学び、実践する際は、「これが使用法だ」と感じ取る程度だと考えられます。\nシーケンシャルモデル モデル定義 サイン関数 $\\sin : \\mathbb{R} \\to \\mathbb{R}$ の近似のために、入力と出力の次元が1のMLPを次のように定義しましょう。\nimport tensorflow as tf\rfrom tensorflow.keras import Sequential\rfrom tensorflow.keras.layers import Dense\r# モデル定義\rmodel = Sequential([Dense(10, input_dim = 1, activation = \u0026#34;relu\u0026#34;),\rDense(10, input_dim = 10, activation = \u0026#34;relu\u0026#34;),\rDense(1, input_dim = 10)])\rmodel.summary() # output↓\r# Model: \u0026#34;sequential_3\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param # # =================================================================\r# dense_9 (Dense) (None, 10) 20 # # dense_10 (Dense) (None, 10) 110 # # dense_11 (Dense) (None, 1) 11 # # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ keras.layers.Dense()の特徴の一つに、入力の次元を記述する必要がないという点があります。なぜこのような許容がされているのかは分かりませんが、コードの可読性のためには（特に他の人が見る可能性があるコードであれば）入力の次元を明示的に記述することが良いでしょう。このために、出力の次元が左、入力の次元が右に記述されるという特徴があります。したがって、モデルの構造を読むためには、アラビア語ではなく、右から左に読む必要があります。もし線形層を線形変換としての行列と考えた場合、$\\mathbf{y} = A\\mathbf{x}$ なので、入力が右、出力が左に来るのが自然です。しかし、TensorFlowはこのような数学的な厳密さを考慮して設計された言語ではないので、この理由だけでそう設計されたとは考えにくいです。数学的な厳密さを非常に重視するJuliaでも、線形層は Dense(in, out) のように実装されています。これは、左から右へ読む方が便利で分かりやすいためです。元々、$X$ から $Y$ への関数 $f$ の記述自体が $f : X \\to Y$ であり、（Kerasを除いて）世界のどこにも右から左へのマッピングで記述される関数はありません。\nデータ生成 サイン関数を訓練するため、データをサイン関数の関数値とし、モデルの出力とサイン関数のグラフを比較すると以下のようになります。\n# データ生成\rfrom math import pi\rx = tf.linspace(0., 2*pi, num=1000) # 入力データ\ry = tf.sin(x) # 出力データ(label)\r# モデルの出力確認\rimport matplotlib.pyplot as plt\rplt.plot(x, model(x), label=\u0026#34;model\u0026#34;)\rplt.plot(x, y, label=\u0026#34;sin\u0026#34;)\rplt.legend()\rplt.show() 訓練及び結果 from tensorflow.keras.optimizers import Adam\rmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\u0026#39;mse\u0026#39;) model.compile(optimizer, loss, metric) .compile() メソッドでオプティマイザと損失関数を指定します。他の主要なオプションには metric があり、これはモデルを評価する関数を意味します。これは loss と同じになることもありますし、異なることもあります。例えば、MLPで MNISTデータセット を学習する場合、lossは出力とラベルのMSEであり、metricは全データの中で予測に成功した割合になるでしょう。\n\u0026gt; model.fit(x, y, epochs=10000, batch_size=1000, verbose=\u0026#39;auto\u0026#39;)\r.\r.\r.\rEpoch 9998/10000\r1/1 [==============================] - 0s 8ms/step - loss: 6.2260e-06\rEpoch 9999/10000\r1/1 [==============================] - 0s 4ms/step - loss: 6.2394e-06\rEpoch 10000/10000\r1/1 [==============================] - 0s 3ms/step - loss: 6.2385e-06 .fit() メソッドに入力とラベル、エポック数、バッチサイズなどを入力すると訓練が実行されます。verboseは訓練の進行状況をどのように表示するかを決めるオプションで、0、1、2の中から選択でき、0は何も表示しません。他のオプションは以下のフォーマットで表示されます。 # verbose=1\rEpoch (現在のエポック)/(全エポック)\r(現在のバッチ)/(全体のバッチ) [==============================] - 0s 8ms/step - loss: 0.7884\r# verbose=2\rEpoch (現在のエポック)/(全エポック)\r(現在のバッチ)/(全体のバッチ) - 0s - loss: 0.7335 - 16ms/epoch - 8ms/step 訓練が終わり、サイン関数とモデルの関数値を比較すると、学習がうまく行われたことがわかります。\n関数型API Input() 関数と Model() 関数でレイヤーを直接連結する方法です。MLPのようなシンプルなモデルであれば、上記のシーケンシャルモデルで定義する方がはるかに簡単です。上のシーケンシャルモデルで定義したニューラルネットワークと同じ構造のモデルを定義する方法は次のようになります。\nfrom tensorflow.keras import Model\rfrom tensorflow.keras.layers import Input, Dense\rinput = Input(shape=(10)) # 変数は \u0026#34;出力の次元 = 最初の層の入力の次元\u0026#34;\rdense1 = Dense(10, activation = \u0026#34;relu\u0026#34;)(input)\rdense2 = Dense(10, activation = \u0026#34;relu\u0026#34;)(dense1)\routput = Dense(1)(dense2)\rmodel = Model(inputs=input, outputs=output)\rmodel.summary() # output↓\r# Model: \u0026#34;model_10\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param #\r# =================================================================\r# input_13 (InputLayer) [(None, 1)] 0\r# # dense_19 (Dense) (None, 10) 20\r# # dense_20 (Dense) (None, 10) 110\r# # dense_21 (Dense) (None, 1) 11\r# # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ Inputはインプットレイヤーを定義する関数です。正確にはレイヤーではなくテンソルですが、重要な点ではないので、ただの入力層として受け入れても良いでしょう。混乱する点は、出力の次元を入力する必要があるという点です。つまり、最初の層の入力の次元を入力する必要があります。これを定義した後、Dense関数の入力として入力し、明示的に直接各層を連結します。最後に、Model関数で入力と出力を引数に入れると、モデルを定義することができます。\nその後、モデルを .compile() メソッドでコンパイルし、.fit() メソッドで訓練するプロセスは、上で紹介した通りです。\n環境 OS: Windows11 Version: Python 3.9.13, tensorflow==2.12.0, keras==2.12.0 ","id":3562,"permalink":"https://freshrimpsushi.github.io/jp/posts/3562/","tags":null,"title":"TensorFlow-Kerasでシーケンスモデル、関数型APIでMLPを定義してトレーニングする方法"},{"categories":"줄리아","contents":"コード サイズ plot(x, y, size=(600,400)) Juliaでは、図のサイズは size オプションで設定する。Tuple{Integer, Integer} 型で入力する必要があり、各整数はそれぞれ横ピクセルと縦ピクセルを意味する。デフォルト値は (600,400) だ。\nusing Plots\rx = rand(10)\rplot(x)\rsavefig(\u0026#34;size_default.png\u0026#34;)\rplot(x, size=(1200,800))\rsavefig(\u0026#34;size_(1200,800).png\u0026#34;) 1800x1200の画像 (左)、600x400の画像 (右) 解像度 plot(x, y, dpi=100) 画像の解像度は dpi オプションで設定し、デフォルト値は 100 だ。論文やレポート、PowerPointなどに添付する場合は、300くらいにするのがいい。\nusing Plots\rx = rand(10)\rplot(x)\rsavefig(\u0026#34;dpi_default.png\u0026#34;)\rplot(x, dpi=300) savefig(\u0026#34;dpi_300.png\u0026#34;) dpi=100の画像 (上)、dpi=300の画像 (下) また、サイズを増やす時は、解像度も一緒に増やさないと、画像が綺麗さを保てないので注意。dpiを300にして保存すると、画像のサイズが1800x1200に増えるが、サイズだけを増やしてdpiをデフォルト値の100のままにしておくと、画像が醜くなるから気をつけて。\ndpi=300, size=1800x1200の画像 (左)、dpi=100, size=1800x1200の画像 (右) 環境 OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12 ","id":3559,"permalink":"https://freshrimpsushi.github.io/jp/posts/3559/","tags":null,"title":"ジュリアで画像のサイズと解像度を調整する方法"},{"categories":"줄리아","contents":"コード plot!([x1, x2], [y1, y2], arrow=:true) このコードは、プロット上に点$(x1, y1)$から点$(x2, y2)$までの矢印を描く。当然ながら、矢印の先端は終点$(x2, y2)$にある。サイン関数の最大値は以下のように示すことができる。\nusing Plots\rx = range(0, 2π, 100)\rplot(x, sin.(x), label=\u0026#34;\u0026#34;, ylims=(-1.3,1.3))\rplot!([π/2, 3], [1, 1.1], arrow=:true, color=:black, label=\u0026#34;\u0026#34;)\rannotate!(3.7, 1.1, \u0026#34;maximum\u0026#34;) 矢印の先 矢印の先のスタイルは:openか:closedで選ぶことができる。\n指定しないか:trueの場合：折れ線$\\to$ plot!([3π/2, 3], [-1, -1.1], arrow=:open, color=:red, label=\u0026#34;\u0026#34;)\rannotate!(2.3, -1.1, \u0026#34;minimum\u0026#34;) 矢印の方向 矢印の先の方向は:head、:tail、:bothで設定でき、:headがデフォルトである。\nplot!([π/2, π/2], [0, 1], arrow=(:closed, :both), color=:purple, label=\u0026#34;\u0026#34;)\rannotate!(0.75π, 0.5, \u0026#34;amplitude\u0026#34;) 公式ドキュメントで1、headlengthとheadwidthのオプションについての説明があるが、使ってみるとエラーしか出なくてどう使うかよくわからない。\n環境 OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12 https://docs.juliaplots.org/v1.38/api/#Plots.arrow-Tuple\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3558,"permalink":"https://freshrimpsushi.github.io/jp/posts/3558/","tags":null,"title":"ジュリアでグラフィックスに矢印を描く方法"},{"categories":"줄리아","contents":"説明1 Juliaでは、ランダムシードは以下のように固定する。\nseed!([rng=default_rng()], seed) -\u0026gt; rng seed!([rng=default_rng()]) -\u0026gt; rng 入力変数rngはランダムナンバージェネレータの略で、乱数を抽出するアルゴリズムを意味する。Randomパッケージでは、以下のオプションを提供している。\nTaskLocalRNG：デフォルトの設定値だ。 Xoshiro RandomDevice MersenneTwister コード シードを0に固定して三回抽出した後、再び0に固定して三回抽出すると、同じ値が得られることが確認できる。\njulia\u0026gt; using Random\rjulia\u0026gt; Random.seed!(0)\rTaskLocalRNG()\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.4056994708920292\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.06854582438651502\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.8621408571954849\rjulia\u0026gt; Random.seed!(0)\rTaskLocalRNG()\rjulia\u0026gt; rand(3)\r3-element Vector{Float64}:\r0.4056994708920292\r0.06854582438651502\r0.8621408571954849 https://docs.julialang.org/ja/v1/stdlib/Random/index.html#Random.seed!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3555,"permalink":"https://freshrimpsushi.github.io/jp/posts/3555/","tags":null,"title":"ジュリアでランダムシードを固定する方法"},{"categories":"줄리아","contents":"日本語訳 説明 ボックスプロットを描くには、統計的可視化パッケージであるStatsPlots.jlを使用する必要がある。\nboxplot([data], labels=[label]) コード using StatsPlots\rx = rand(0:100, 100)\ry = rand(50:100, 100)\rz = cat(x,y, dims=1)\rboxplot(x, label=\u0026#34;x\u0026#34;)\rboxplot!(y, label=\u0026#34;y\u0026#34;)\rboxplot!(z, label=\u0026#34;z\u0026#34;) または、boxplot([x,y,z], label=[\u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot;])も同じ図を描く。lableにはコンマがないことに注意しよう。つまり、$3 \\times 1$ベクターではなく、$1 \\times 3$配列である必要がある。\nx軸の目盛り 文字列でx軸の目盛りを表したい場合、\nboxplot([x, y, z], xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;]) または、以下のコードも同じ図を描く。違いは、実際の座標がどうなっているかだ。上のコードでは、実際に各ボックスが描かれるx座標が1, 2, 3だが、グラフの目盛り値だけx, y, zと見えるように変えたものだ。下のコードは、実際に\u0026quot;x\u0026quot;, \u0026ldquo;y\u0026rdquo;, \u0026ldquo;z\u0026quot;の座標上にボックスを描く。\n2次元配列で描く a = rand(100, 3)\rboxplot(a, xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;]) データフレームで描く データフレームそのもので描くことはできず、配列に変換する必要がある。\nusing MLDatasets\rusing DataFrames\rdf = Iris().features\rboxplot(Array(df), xticks=(1:4, names(df)), label=reshape(names(df), (1,4))) 平均 平均を表示するオプションは別にない。scatterで打ってみよう。\nusing Statistics\rboxplot(fill(\u0026#34;x\u0026#34;, length(x)), x, labels=\u0026#34;x\u0026#34;)\rboxplot!(fill(\u0026#34;y\u0026#34;, length(y)), y, labels=\u0026#34;y\u0026#34;)\rboxplot!(fill(\u0026#34;z\u0026#34;, length(z)), z, labels=\u0026#34;z\u0026#34;)\rscatter!([\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;], [mean(x), mean(y), mean(z)], color=palette(:default)[1:3], label=\u0026#34;\u0026#34;) または、次のコードも同じ図を描く。\nboxplot([x, y, z], xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;])\rscatter!([1, 2, 3], [mean(x), mean(y), mean(z)], color=palette(:default)[1:3], label=\u0026#34;\u0026#34;) 環境 OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12, StatsPlots v0.15.5, DataFrames v1.5.0, MLDatasets v0.7.11 併せて見る Python matplotlibで描く方法 ","id":3553,"permalink":"https://freshrimpsushi.github.io/jp/posts/3553/","tags":null,"title":"ジュリアでボックスプロットを描く方法"},{"categories":"줄리아","contents":"概要 ジュリアで決定木Decision Treeを実装したDecisionTree.jlパッケージを紹介する1。\nコード 例としては、Rの組み込みデータであるirisデータセットを使う。目標は、四つの変数SepalLength, SepalWidth, PetalLength, PetalWidthを用いてSpeciesを予測する決定木を作り、そのパフォーマンスを評価することである。\njulia\u0026gt; iris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\r150×5 DataFrame\rRow │ SepalLength SepalWidth PetalLength PetalWidth Speci ⋯\r│ Float64 Float64 Float64 Float64 Cat… ⋯\r─────┼──────────────────────────────────────────────────────────\r1 │ 5.1 3.5 1.4 0.2 setos ⋯\r2 │ 4.9 3.0 1.4 0.2 setos 3 │ 4.7 3.2 1.3 0.2 setos 4 │ 4.6 3.1 1.5 0.2 setos 5 │ 5.0 3.6 1.4 0.2 setos ⋯\r6 │ 5.4 3.9 1.7 0.4 setos 7 │ 4.6 3.4 1.4 0.3 setos 8 │ 5.0 3.4 1.5 0.2 setos 9 │ 4.4 2.9 1.4 0.2 setos ⋯\r10 │ 4.9 3.1 1.5 0.1 setos 11 │ 5.4 3.7 1.5 0.2 setos 12 │ 4.8 3.4 1.6 0.2 setos 13 │ 4.8 3.0 1.4 0.1 setos ⋯\r14 │ 4.3 3.0 1.1 0.1 setos 15 │ 5.8 4.0 1.2 0.2 setos ⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋱\r136 │ 7.7 3.0 6.1 2.3 virgi 137 │ 6.3 3.4 5.6 2.4 virgi ⋯\r138 │ 6.4 3.1 5.5 1.8 virgi 139 │ 6.0 3.0 4.8 1.8 virgi 140 │ 6.9 3.1 5.4 2.1 virgi 141 │ 6.7 3.1 5.6 2.4 virgi ⋯\r142 │ 6.9 3.1 5.1 2.3 virgi 143 │ 5.8 2.7 5.1 1.9 virgi 144 │ 6.8 3.2 5.9 2.3 virgi 145 │ 6.7 3.3 5.7 2.5 virgi ⋯\r146 │ 6.7 3.0 5.2 2.3 virgi 147 │ 6.3 2.5 5.0 1.9 virgi 148 │ 6.5 3.0 5.2 2.0 virgi 149 │ 6.2 3.4 5.4 2.3 virgi ⋯\r150 │ 5.9 3.0 5.1 1.8 virgi 1 column and 120 rows omitted モデル作成 julia\u0026gt; using DecisionTree\rjulia\u0026gt; model = DecisionTreeClassifier(max_depth=2)\rDecisionTreeClassifier\rmax_depth: 2\rmin_samples_leaf: 1\rmin_samples_split: 2\rmin_purity_increase: 0.0\rpruning_purity_threshold: 1.0\rn_subfeatures: 0\rclasses: nothing\rroot: nothing モデルを作る。DecisionTreeClassifier()を通じて、決定木で使われるパラメーターを指定できる。\nモデルフィッティング julia\u0026gt; features = Matrix(iris[:, Not(:Species)]);\rjulia\u0026gt; labels = iris.Species;\rjulia\u0026gt; fit!(model, features, labels) DecisionTreeClassifier\rmax_depth: 2\rmin_samples_leaf: 1\rmin_samples_split: 2\rmin_purity_increase: 0.0\rpruning_purity_threshold: 1.0\rn_subfeatures: 0\rclasses: [\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;]\rroot: Decision Tree Leaves: 3\rDepth: 2 データを独立変数と従属変数に分けて、fit!()関数でモデルを学習させる。\nパフォーマンス確認 julia\u0026gt; print_tree(model)\rFeature 3 \u0026lt; 2.45 ?\r├─ setosa : 50/50\r└─ Feature 4 \u0026lt; 1.75 ?\r├─ versicolor : 49/54\r└─ virginica : 45/46 学習が終わったモデルは、print_tree()関数を通じてどんな構造をしているか確認できる。\njulia\u0026gt; sum(labels .== predict(model, features)) / length(labels)\r0.96 簡単に正解率を確認した結果、96%程度でかなり良いことがわかった。\n全コード using RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rusing DecisionTree\rmodel = DecisionTreeClassifier(max_depth=2)\rfeatures = Matrix(iris[:, Not(:Species)]);\rlabels = iris.Species;\rfit!(model, features, labels)\rprint_tree(model)\rsum(labels .== predict(model, features)) / length(labels) 環境 OS: Windows julia: v1.9.0 https://github.com/JuliaAI/DecisionTree.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2618,"permalink":"https://freshrimpsushi.github.io/jp/posts/2618/","tags":null,"title":"ジュリアで決定木を使う方法"},{"categories":"줄리아","contents":"概要 ジュリアでコレクションの重複をなくし、チェックする方法を紹介する。重複をなくすunique()関数は、アルゴリズム的に見て難しくないが、自分で実装しようとすると面倒で、効率的でないかもしれない。重複要素がないかをチェックするallunique()関数は、実装も簡単なほどに見つけることがない関数だから、この機会にしっかり覚えておくべきだ。\nコード unique() julia\u0026gt; x = [3, 1, 4, 1, 5, 9, 2, 6, 5];\rjulia\u0026gt; y = unique(x)\r7-element Vector{Int64}:\r3\r1\r4\r5\r9\r2\r6 allunique() 1 実はこのポストは、allunique()を紹介することが目的だ。コレクションが重複要素を持っているかをチェックする常識的な方法の一つは、length(unique(x)) == length(x)としてunique()を適用し、要素が減ったかをチェックすることだ。\nこの方法はあまりにも簡単で、効率に対して過信しやすいが、一旦unique()関数は長さ$n$の配列を少なくとも一度は全ての要素を見る必要があるので、時間複雑度は$O (n)$だ。これは、コード内で要素の重複チェックを頻繁に行う場合、確かに負担になるレベルのコストであり、allunique()は配列の長さによって実装が異なり、途中で重複要素が見つかれば計算を中断し、チェックに成功するなど、性能面で明らかな利点がある。\njulia\u0026gt; allunique(x)\rfalse\rjulia\u0026gt; allunique(y)\rtrue 完全なコード x = [3, 1, 4, 1, 5, 9, 2, 6, 5];\ry = unique(x)\rallunique(x)\rallunique(y) 環境 OS: Windows julia: v1.9.0 https://docs.julialang.org/en/v1/base/collections/#Base.allunique\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2616,"permalink":"https://freshrimpsushi.github.io/jp/posts/2616/","tags":null,"title":"ジュリアでコレクションの重複を削除する方法"},{"categories":"줄리아","contents":"概要 Juliaでは、クラスタリング用のパッケージとしてClustering.jlが提供されている1。実装されているアルゴリズムは次の通りです:\nK-means K-medoids Affinity Propagation Density-based spatial clustering of applications with noise (DBSCAN) Markov Clustering Algorithm (MCL) Fuzzy C-Means Clustering 階層クラスタリング Single Linkage Average Linkage Complete Linkage Ward\u0026rsquo;s Linkage コード DBSCAN DBSCAN (Density-based spatial clustering of applications with noise)はdbscan()関数で実装されています。$p$次元のデータが$n$個ある場合、$p \\times n$サイズの行列と半径Radiusが引数として与えられなければなりません。\njulia\u0026gt; points = [iris.PetalLength iris.PetalWidth]\u0026#39;\r2×150 adjoint(::Matrix{Float64}) with eltype Float64:\r1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 … 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.0 5.2 5.4 5.1 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 0.2 2.1 2.4 2.3 1.9 2.3 2.5 2.3 1.9 2.0 2.3 1.8 julia\u0026gt; dbscaned = dbscan(points, 0.5)\rDbscanResult(DbscanCluster[DbscanCluster(50, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], Int64[]), DbscanCluster(100, [51, 52, 53, 54, 55, 56, 57, 58, 59, 60 … 141, 142, 143, 144, 145, 146, 147, 148, 149, 150], Int64[])], [1, 51], [50, 100], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1 … 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\rjulia\u0026gt; dbscaned |\u0026gt; propertynames\r(:clusters, :seeds, :counts, :assignments) DBSCANの結果はDbscanResultという構造体で返されます。.assignmentsと.clusterが重要です。\n各クラスタにどのデータポイントが属しているかは、次のようにgetproperty()関数を通じて得ることができます。\njulia\u0026gt; getproperty.(dbscaned.clusters, :core_indices)\r2-element Vector{Vector{Int64}}:\r[1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\r[51, 52, 53, 54, 55, 56, 57, 58, 59, 60 … 141, 142, 143, 144, 145, 146, 147, 148, 149, 150] 各データポイントがどのクラスタに属しているかは、次のように.assignmentsプロパティを通じて知ることができます。\njulia\u0026gt; dbscaned.assignments\r150-element Vector{Int64}:\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r⋮\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2 視覚化のコツとして、クラスタは任意の整数に割り当てられるので、散布図を描く際に*.assignmentsをそのままcolorオプションに入れると、次のように各クラスタに対応する色が指定されます。\nscatter(iris.PetalLength, iris.PetalWidth,\rxlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;,\rcolor = dbscaned.assignments) クラスタリングがうまく実行されたことが確認できます。\n全コード using Clustering\rusing RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rscatter(iris.PetalLength, iris.PetalWidth, xlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;)\rpng(\u0026#34;iris\u0026#34;)\rpoints = [iris.PetalLength iris.PetalWidth]\u0026#39;\rdbscaned = dbscan(points, 0.5)\rdbscaned |\u0026gt; propertynames\rgetproperty.(dbscaned.clusters, :core_indices)\rdbscaned.assignments\rscatter(iris.PetalLength, iris.PetalWidth,\rxlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;,\rcolor = dbscaned.assignments) 環境 OS: Windows julia: v1.9.0 Clustering v0.15.4 https://github.com/JuliaStats/Clustering.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2613,"permalink":"https://freshrimpsushi.github.io/jp/posts/2613/","tags":null,"title":"ジュリアでクラスタリングパッケージを使用する方法"},{"categories":"줄리아","contents":"概要 ジュリアでは、マシンラーニング、特にディープラーニングに関連した自動微分Automatic DifferentiationのためにZygote.jlというパッケージを使っている1。開発者たちは、このパッケージは次世代の自動微分システムとして、ジュリアで微分可能プログラミングDifferentiable Programmingができるようにすると宣伝していて、実際に使ってみると驚くほど直感的だと分かる。\n自動微分ではなく、導関数に関連したパッケージ自体が気になるなら、Calculus.jl パッケージを参照してほしい。\nコード 単変数関数 信じられないくらい簡単だ。普段私たちが微分するのと同じように、関数名の後ろにプライム'をつけると、まるで本当に導関数を使って計算しているように、微分係数が計算される。\njulia\u0026gt; using Zygote\rjulia\u0026gt; p(x) = 2x^2 + 3x + 1\rp (generic function with 1 method)\rjulia\u0026gt; p(2)\r15\rjulia\u0026gt; p\u0026#39;(2)\r11.0\rjulia\u0026gt; p\u0026#39;\u0026#39;(2)\r4.0 多変数関数 gradient() 関数を使う。\njulia\u0026gt; g(x,y) = 3x^2 + 2y + x*y\rg (generic function with 1 method)\rjulia\u0026gt; gradient(g, 2,-1)\r(11.0, 4.0) もう少し直感的にコードを書きたいなら、次のように\\nabla、すなわち∇で再び関数を定義して試してみるのも良い。\njulia\u0026gt; ∇(f, v...) = gradient(f, v...)\r∇ (generic function with 1 method)\rjulia\u0026gt; ∇(g, 2, -1)\r(11.0, 4.0) 全体のコード using Zygote\rp(x) = 2x^2 + 3x + 1\rp(2)\rp\u0026#39;(2)\rp\u0026#39;\u0026#39;(2)\rg(x,y) = 3x^2 + 2y + x*y\rgradient(g, 2,-1)\r∇(f, v...) = gradient(f, v...)\r∇(g, 2, -1) 環境 OS: Windows julia: v1.9.0 Zygote: v0.6.62 https://github.com/FluxML/Zygote.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2609,"permalink":"https://freshrimpsushi.github.io/jp/posts/2609/","tags":null,"title":"ジュリアの自動微分パッケージZygote.jl"},{"categories":"줄리아","contents":"概要 Juliaで構造体のプロパティを参照する方法は主に二つある。文法的な便宜または実際の用途に応じて適切に使用するべきだ。\nコード 例として、Juliaで//演算子は以下のように有理数(Rational)タイプの数を作る。有理数が持つプロパティの名前には、分子Numeratorを意味する:numと分母Denominatorを意味する:denがある。\njulia\u0026gt; q = 7 // 12\r7//12\rjulia\u0026gt; q |\u0026gt; typeof\rRational{Int64}\rjulia\u0026gt; q |\u0026gt; propertynames\r(:num, :den) getproperty(x, :y)とx.y julia\u0026gt; getproperty(q, :den)\r12\rjulia\u0026gt; q.den\r12 基本的には、getproperty()関数の二番目の引数にそのプロパティの名前をシンボルで与えればいい。あるいは、普通のプログラミング言語でのフィールド、プロパティへの参照のように、オブジェクト変数名の後ろに点を付けてそのプロパティにアクセスできる。\n配列に対するプロパティの参照 一方、上記の方法は一回だけ必要な時に使用できる方法だが、配列にある複数の要素にアクセスする必要があれば、以下のようにブロードキャストを使用しなければならない。あるいは、性能が重要でなくてただ早くコーディングが必要なら、Pythonみたいにリストコンプリヘンションを使用することも一つの方法だ。\njulia\u0026gt; Q = [k // 12 for k in 1:12]\r12-element Vector{Rational{Int64}}:\r1//12\r1//6\r1//4\r1//3\r5//12\r1//2\r7//12\r2//3\r3//4\r5//6\r11//12\r1//1\rjulia\u0026gt; getproperty.(Q, :num)\r12-element Vector{Int64}:\r1\r1\r1\r1\r5\r1\r7\r2\r3\r5\r11\r1\rjulia\u0026gt; [q.num for q in Q]\r12-element Vector{Int64}:\r1\r1\r1\r1\r5\r1\r7\r2\r3\r5\r11\r1 全コード q = 7 // 12 q |\u0026gt; typeof q |\u0026gt; propertynames getproperty(q, :den) q.den Q = [k // 12 for k in 1:12] getproperty.(Q, :num) [q.num for q in Q] 環境 OS: Windows julia: v1.9.0 ","id":2607,"permalink":"https://freshrimpsushi.github.io/jp/posts/2607/","tags":null,"title":"ジュリアで関数として構造体のプロパティを参照する方法"},{"categories":"줄리아","contents":"コード quiver(, quiver=) Juliaでは、quiver()関数を使ってベクトルフィールドを視覚化することができる。\nθ = 0:0.2:2π\rquiver(cos.(θ),sin.(θ), quiver = (-sin.(θ), cos.(θ)), size = (600,600), lims = (-2,2)); png(\u0026#34;1\u0026#34;) 矢印の長さの変更 矢印の大きさを変えるもっといい方法があるかもしれないけど、基本的にはquiver=オプションで提供されているベクトルの長さを伸ばしたり縮めたりして、もっといい図を描くことができる。\nquiver(cos.(θ),sin.(θ), quiver = (-sin.(θ), cos.(θ)) ./ 2, size = (600,600), lims = (-2,2)); png(\u0026#34;2\u0026#34;) 環境 OS: Windows julia: v1.9.0 ","id":2605,"permalink":"https://freshrimpsushi.github.io/jp/posts/2605/","tags":null,"title":"ジュリアでベクトル場を描く方法"},{"categories":"줄리아","contents":"概要 複数の配列が与えられた時、例えば、それぞれの配列の3番目の要素にアクセスしたいという状況は意外と多い。Juliaでは、getindex()関数のブロードキャストを通じてこれを実装できる。\nコード getindex.() julia\u0026gt; seq_ = [collect(1:k:100) for k in 1:10]\r10-element Vector{Vector{Int64}}:\r[1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\r[1, 3, 5, 7, 9, 11, 13, 15, 17, 19 … 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\r[1, 4, 7, 10, 13, 16, 19, 22, 25, 28 … 73, 76, 79, 82, 85, 88, 91, 94, 97, 100]\r[1, 5, 9, 13, 17, 21, 25, 29, 33, 37 … 61, 65, 69, 73, 77, 81, 85, 89, 93, 97]\r[1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81, 86, 91, 96]\r[1, 7, 13, 19, 25, 31, 37, 43, 49, 55, 61, 67, 73, 79, 85, 91, 97]\r[1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99]\r[1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97]\r[1, 10, 19, 28, 37, 46, 55, 64, 73, 82, 91, 100]\r[1, 11, 21, 31, 41, 51, 61, 71, 81, 91]\rjulia\u0026gt; getindex.(seq_, 3)\r10-element Vector{Int64}:\r3\r5\r7\r9\r11\r13\r15\r17\r19\r21 first(), last() first()はgetindex(, 1)と同じだが、last()はgetindex(, end)と同じ表現が存在しないため、特別な関数と言える。プログラムが繰り返される中で最後にある結果が必要な場合が多く、その最後の要素のインデックスは様々な場合が多いため、last()関数は知っておくべきだ。\njulia\u0026gt; first.(seq_)\r10-element Vector{Int64}:\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\rjulia\u0026gt; last.(seq_)\r10-element Vector{Int64}:\r100\r99\r100\r97\r96\r97\r99\r97\r100\r91 環境 OS: Windows julia: v1.9.0 ","id":2603,"permalink":"https://freshrimpsushi.github.io/jp/posts/2603/","tags":null,"title":"ジュリアで配列の特定の位置を関数で参照する方法"},{"categories":"줄리아","contents":"概要 ジュリアでパッケージを読み込む方法はusingを使うことだけど、プログラムが大きくなるとそれを一つ一つ書くのも大変だ。ループを通してパッケージを読み込む方法を紹介する1。\nコード メタプログラミング packages = [:CSV, :DataFrames, :LinearAlgebra, :Plots]\rfor package in packages\r@eval using ▷eq1◁(package)\rend 実際の使い方としては、プログレスバーだけ別に読み込んで、その他のパッケージの読み込みは目で確認する方がいい。\n環境 OS: Windows julia: v1.9.0 https://discourse.julialang.org/t/programmatically-load-packages/52435/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2601,"permalink":"https://freshrimpsushi.github.io/jp/posts/2601/","tags":null,"title":"ジュリアからRへのパッケージのインポート方法"},{"categories":"줄리아","contents":"概要 Juliaで行列の正規化を簡単にするヒントを紹介する1。基本的には行列を行ごと、列ごとにスカラー倍する方法とeachcol()関数、LinearAlgebraモジュールのnorm()関数を混ぜて使っただけだが、一行で終わり、使う機会が多いので覚えておくと役に立つ。\nコード julia\u0026gt; using LinearAlgebra\rjulia\u0026gt; X = reshape(1:15, 5, :)\r5×3 reshape(::UnitRange{Int64}, 5, 3) with eltype Int64:\r1 6 11\r2 7 12\r3 8 13\r4 9 14\r5 10 15 与えられた行列Xを列ごとに正規化Normalizeするのは、X ./ norm.(eachcol(X))'の一行で可能だ。実行自体と実際にうまく正規化されたかを確認した結果は以下の通りだ。\njulia\u0026gt; Z = X ./ norm.(eachcol(X))\u0026#39;\r5×3 Matrix{Float64}:\r0.13484 0.330289 0.376192\r0.26968 0.385337 0.410391\r0.40452 0.440386 0.444591\r0.53936 0.495434 0.47879\r0.6742 0.550482 0.512989\rjulia\u0026gt; norm.(eachcol(Z))\r3-element Vector{Float64}:\r1.0\r1.0\r1.0 全コード using LinearAlgebra\rX = reshape(1:15, 5, :)\rZ = X ./ norm.(eachcol(X))\u0026#39;\rnorm.(eachcol(Z)) 環境 OS: Windows julia: v1.9.0 https://stackoverflow.com/a/72627341/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2599,"permalink":"https://freshrimpsushi.github.io/jp/posts/2599/","tags":null,"title":"ジュリアで列ごとに行列を正規化する方法"},{"categories":"행렬대수","contents":"定義 1 2 置換行列 $P^{T}$ と 可逆行列 $A \\in \\mathbb{R}^{n \\times n}$ に対し、その 行列の積 $P^{T} A$ は $LU$ を与える。この分解を $A$ の PLU分解Permutation LU Decomposition と言う。$P$ は置換行列であるため、直交行列 となり、すなわち $P^{-1} = P^{T}$ であり、次のように表すことができる。 $$ P^{T} A = LU \\iff A = PLU $$\n説明 LU分解のアルゴリズム：$(a_{ij}) \\in \\mathbb{R}^{n \\times n}$ を可逆行列とする。\nStep 1. $k = 1$\n$u_{1j} = a_{1j}$ とし、$\\displaystyle l_{i1} = {{1} \\over {u_{11}}} a_{i1}$ を計算する。\nStep 2. $k = 2, 3, \\cdots , n-1$\nStep 2-1. 以下を計算する。 $$ u_{kk} = a_{kk} - \\sum_{s = 1}^{k-1} l_{ks} u_{sk} $$ Step 2-2. $j = k+1 , k+2 , \\cdots , n-1$ に対し以下を計算する。 $$ u_{kj} = a_{kj} - \\sum_{s = 1}^{k-1} l_{ks} u_{sj} $$ Step 2-3. $i = k+1 , k+2 , \\cdots , n-1$ に対し以下を計算する。 $$ l_{ik} = {{1} \\over {u_{kk}}} \\left{ a_{ik} - \\sum_{s = 1}^{k-1} l_{is} u_{sk} \\right} $$ Step 3. $k = n$ に対し以下を計算する。 $$ u_{nn} = a_{nn} - \\sum_{s = 1}^{n-1} l_{ns} u_{sn} $$\n行列のLU分解を行うには $u_{11} = a_{11}$ や $u_{kk}$ の逆数をとることが可能でなければならないが、 $$ A = \\begin{bmatrix} 0 \u0026amp; 3\\\\ 2 \u0026amp; 1 \\end{bmatrix} $$ のような 行列でもこのアルゴリズムを適用することはできない。LU分解を可能にするためにある置換行列 $P^{T}$ を乗じて $A$ を $PLU$ として表すことを PLU分解 と呼ぶ。もちろん、左または右、行または列が重要というわけではないため、 $$ A P^{T} = LU \\iff A = LUP $$ と書き LUP分解 と呼んでも差し支えない。\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://math.unm.edu/~loring/links/linear_s08/LU\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2,"permalink":"https://freshrimpsushi.github.io/jp/posts/2/","tags":null,"title":"PLU分解"},{"categories":"행렬대수","contents":"定義 1 各行で成分が一つだけ$1$で、残りがすべて$0$である正方行列$P \\in \\mathbb{R}^{n \\times n}$を順列行列と呼ぶ。\n基本的性質 直交性 すべての順列行列は直交行列である: $$P^{-1} = P^{T}$$\nスパース性 十分に大きな$n$に対して、$P \\in \\mathbb{R}^{n \\times n}$はスパース行列となる。\n説明 順列行列はその名前が示す通り、行列の乗算によって行と列の順列を与える。次の例では、左側に乗算すると行の順列となり、右側に乗算すると列の順列となることがわかる。 $$ \\begin{align*} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\\\ \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{12} \u0026amp; a_{11} \u0026amp; a_{13} \\\\ a_{22} \u0026amp; a_{21} \u0026amp; a_{23} \\\\ a_{32} \u0026amp; a_{31} \u0026amp; a_{33} \\end{bmatrix} \\end{align*} $$\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1,"permalink":"https://freshrimpsushi.github.io/jp/posts/1/","tags":null,"title":"順列行列"},{"categories":"줄리아","contents":"エラー Juliaでデータフレームを使っていると、文字列データがString7やString15、String31などと読み込まれて様々なエラーを発生させることがある。具体的にどんなエラーが発生するというより、普段からよく使っている関数がここでは通用しなくて色々な問題を起こす。\n原因 パフォーマンス上の理由で、Stringをより早く処理できるString7などに変更したのだ。意図通りに設計されているので、仕方がない。\n解決法 CSV.read()にオプションとしてstringtype = Stringを渡せばいい。\n実際に使ってみるとかなり不便だけど、我慢して使うしかない。\n環境 OS: Windows julia: v1.8.5 ","id":2574,"permalink":"https://freshrimpsushi.github.io/jp/posts/2574/","tags":null,"title":"ジュリアでString7, String15なしでデータフレームを呼び出す方法"},{"categories":"다변수벡터해석","contents":"質問 偏微分では、通常の微分と異なり、$\\displaystyle {{ d f } \\over { d t }}$ の代わりに $\\displaystyle {{ \\partial f } \\over { \\partial t }}$ のような表現を使用します。$\\partial$ は[ラウンドディー]Round Deeまたは[パーシャル]Partialと読み、歴史的にも$d$ を丸めて書いた[カーリーディー]Curly Deeから由来しています。1 $\\TeX$ のコードでは \\partial であり、韓国では[ラウンドディー]さえも長いと考えるのか、単に[ラウンド]と読む人も多いです。\nなぜ $d$ を $\\partial$ で書くのか？ 問題は、偏微分が単に他の変数に関して微分するだけなのに、なぜ記号を異なるものにする必要があるのかということが納得できないということです。学部の授業レベルでは、偏微分が初めて登場するたびに必ず出てくる質問ですが、実際の答えは、数学科でなければ「そんなことは数学科で考えることだ」または数学科であっても「ただの表記の違いとして受け入れても問題ない」という程度で返ってくることがあります。これが決して間違っているわけではないのは、$d$ で書こうが $\\partial$ で書こうが、数学科でなければそれが特に重要なわけではなく、数学科であっても式の意味自体が変わるわけではないからです。 例えば、熱方程式を学ぶ場合、 $$ {{ \\partial u } \\over { \\partial t }} = {{ \\partial u } \\over { \\partial x^{2} }} $$ の $\\partial$ を通常の微分表記 $d$ に変えて $$ {{ d u } \\over { d t }} = {{ d u } \\over { d x^{2} }} $$ と書いた場合、2つの方程式が同じかどうかを尋ねることができます。非常に混乱することに、その答えは「実際には同じ」なので、この時点で多くの学生が $d$ と $\\partial$ の区別に意味がないと感じたり、定義レベルで受け入れてしまったりすることになります。\n回答 ニュートンとライプニッツ 本格的な偏微分の話に入る前に、微分の2人の父、ニュートンNewtonとライプニッツLeibnizの話を面白い読み物として取り上げたいと思います。現代において、両者は独自に微分の概念および記法を考案したと認められていますが、関数$y = f(x)$ の導関数を表す際、ニュートンは $$ y ' = f ' (x) $$ のような表記を使用し、ライプニッツは $$ {{ dy } \\over { dx }} = {{ d f(x) } \\over { dx }} $$ のような表記を使用しました。同じ微分であっても、このように表現の違いが生じるのは、両者の思考方法や微積分に対する見方自体が異なっていたためです。現在では、同時代に独自に微分を考案した人がもう一人いても良かったと思えるほど、幸運なことです。ニュートンは古典力学の巨匠として、「位置を一度微分すると速度、二度微分すると加速度」といった話をしなければならず、この時 $$ \\begin{align*} v =\u0026amp; x ' \\\\ a =\u0026amp; v ' = x '' \\end{align*} $$ のような表現は非常にすっきりして効率的です。ライプニッツは幾何学的Geometricな観点から見るとより理にかなっており、直線の傾きが横と縦の変化量の比として定義されるため、曲線では非常に小さな単位を与えて $$ {{ \\Delta y } \\over { \\Delta x }} \\approx {{ d y } \\over { d x }} $$ のように接線の傾きに自然に近づくことができます。興味深いことに、ここまで述べたのは全て通常の微分に関するものであり、分野によっては以下のような表記の分化が起こり、ニュートンとライプニッツの表記が共存することができるという事実です。\n微分幾何学における$s$ に対する微分と$t$ に対する微分の表記： $$ {{ df } \\over { ds }} = f^{\\prime} \\quad \\text{and} \\quad {{ df } \\over { dt }} = \\dot{f} $$ ドット$\\dot{}$ やプライム$'$ はどちらも微分を表していますが、微分幾何学の文脈では上記のように記号を区別することができます。通常、$s$ は単位速度曲線のパラメータであり、$t = t(s)$ は曲線の長さの再パラメータ化によって表されます。\nこの表記は、微分という概念が変形されて出てきたわけではありません。微分幾何学では、単に$s$ で多くの微分を行い、$t$ でも多くの微分を行う必要があるため、ニュートンの表記では何に対して微分しているのか区別できず、ライプニッツの表記では数式が複雑すぎるため、両者の長所を取り入れるために新たな表記を作成したものです。\n本当に興味深いのは、このように幾何学的な観点から$s$ や $t$ は単なるパラメータに過ぎないにもかかわらず、常微分方程式の中でも特に時間timeによる変化を表す場合には、その頭文字を取って$t$ に対する$v$ の導関数を$v '$ ではなく$\\dot{v}$ と書くようになりました。これにより、ダイナミクスなどのほとんどのシステムで時間による変化を記述する際には、$v '$ の代わりに $$ \\dot{v} = f(v) $$ という表現を好んで使用するようになりました。ポイントは、「何によって微分するか」を明確かつすっきりと表現するための検討自体が、偏微分という枠組みに縛られなくても自然に浮かぶことができるということです。\n多変数関数の暗示 前節では、$f '$ と $\\dot{f}$ が単に表現の違いだけで、どの変数によって微分されたかを区別できること、特にダイナミクスシステムでは、時間$\\dot{v} = f(v)$ が現れなくても、一般的な規約とコンテキストからそれが時間による微分であることを暗示できることを指摘しました。このように表現によって暗黙的Implicitにわかる情報についてもう少し話してみたいと思います。\n再び偏微分に戻ると、$d$ と $\\partial$ の表記がどのように異なるかを実感するのが難しいのは、その式自体が示す偏導関数に違いがないからです。例えば、$f$ を$t$ で微分した導関数が$g$ である場合、その$g$ は $$ g = {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} $$ のように$d$ で表されても$\\partial$ で表されてもあまり関係がない。記号がどうであれ、$t$ で微分された「結果」である$g$ が同じだからです。しかし、$\\partial$ が暗黙的に与える情報は$g$ ではなく$f$ に関するものです。ある関数$h$ が$H$ に関して$x$ で微分された結果だとすると、次のように2つの表現を比較してみましょう：\n偏微分表現を使用しない場合：$\\displaystyle h = {{ d H } \\over { d x }} \\implies$ $H$ を微分すると$h$ になるらしい。\n偏微分表現を使用する場合：$\\displaystyle h = {{ \\partial H } \\over { \\partial x }} \\implies$ なぜこれだけ？何か$y$ があって$H = H (x , y)$ になるんだろう？\rつまり、$\\partial$ という記号は、与えられた関数が多変数関数であることを暗示しているのです。多くの場合、偏微分に初めて本格的に触れるのは通常偏微分方程式であり、 $$ {{ \\partial u } \\over { \\partial t }} = {{ \\partial u } \\over { \\partial x^{2} }} $$ のような方程式があれば、私たちは$u$ を$t$ で微分した偏導関数$u_{t}$ が気になるわけではなく、$u$ を$x$ で2回微分した2階偏導関数$u_{xx}$ が気になるわけでもなく、その両方が等しいときの$t$ と$x$ の関数$u = u (t,x)$ が何であるかが気になるのです。この観点から、偏微分に使用される$\\partial$ が偏微分方程式の記述に使用されるのは妥当で自然だと主張することができます。\n一方で、このような慣習が広く受け入れられることにより、$d$ 自体の意味も変わります。多変数関数ではない関数をわざわざ$\\partial$ で微分することは意味がないため、導関数の表現に$d$ が使用されていれば、それは多変数関数ではないことを暗示することになります。例えば、2変数関数$u = u (t,x)$ に対して位置を一点に固定して$u = u \\left( t , x_{0} \\right)$ とすると、 $$ \\left. {{ \\partial u } \\over { \\partial t }} \\right|_{x = x_{0} } = {{ d u } \\over { d t }} = \\dot{u} $$ のような式は、$\\partial$ と$d$ の暗黙的な情報伝達を非常にうまく活用しています。これは、単なる表現の違いにとどまらず、実際に式を扱う思考方法にも影響を与え、偏微分方程式の問題を比較的簡単な常微分方程式に変換して解くといったアイデアにつながることもあります。\n✅ 全微分における混乱を避けるために $$ df = \\frac{ \\partial f}{ \\partial x_{1} }dx_{1} + \\frac{ \\partial f}{ \\partial x_{2} }dx_{2} + \\cdots + \\frac{ \\partial f}{ \\partial x_{n} }dx_{n} $$ 多変数関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}$ に対する数理物理学などで使用される全微分は、通常上記のような形で表され、もう少し直感的に書くために$n = 3$ のとき次のように$t,x,y,z$ のみを書き、$x,y,z$ は互いに独立であるとしましょう。 $$ df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz $$ 一見すると、$d$ と$\\partial$ が混在していて複雑に見えますが、ライプニッツの遺産に従って「両辺を$dt$ や$dx$ で割る」ような操作を行うと、 $$ \\begin{align*} df =\u0026amp; {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz \\\\ {{ d f } \\over { d t }} =\u0026amp; {{ \\partial f } \\over { \\partial x }} {{ d x } \\over { d t }} + {{ \\partial f } \\over { \\partial y }} {{ d y } \\over { d t }} + {{ \\partial f } \\over { \\partial z }} {{ d z } \\over { d t }} \\\\ {{ d f } \\over { d x }} =\u0026amp; {{ \\partial f } \\over { \\partial x }} {{ d x } \\over { d x }} + {{ \\partial f } \\over { \\partial y }} {{ d y } \\over { d x }} + {{ \\partial f } \\over { \\partial z }} {{ d z } \\over { d x }} = {{ \\partial f } \\over { \\partial x }} \\end{align*} $$ のように$f$ を$t$ で微分する意味と$x$ で偏微分する意味が同時によく表現されていることがわかります。これは全微分の形が数式的に扱う上で非常に便利であることを示していますが、全微分で$\\partial$ をすべて取り除いて$d$ で統一して再度書き直すと次のようになります。 $$ df = {{ d f } \\over { d x }} dx + {{ d f } \\over { d y }} dy + {{ d f } \\over { d z }} dz $$ もちろん、ライプニッツの微分記法が分数の分子と分母を扱うときのように非常に直感的であることは事実ですが、この記事を読んでいる皆さんであれば、$dx$ や$dy$、$dz$ を本当にそのように扱ってはいけないことを知っているでしょう。それにもかかわらず、皆さんの内なる本能はこのように約分するように叫ぶでしょう。 $$ \\begin{align*} df =\u0026amp; {{ d f } \\over { dx }} dx + {{ d f } \\over { dy }} dy + {{ d f } \\over { dz }} dz \\\\ \\overset{?}{=} \u0026amp; {{ d f } \\over { \\cancel{dx} }} \\cancel{dx} + {{ d f } \\over { \\cancel{dy} }} \\cancel{dy} + {{ d f } \\over { \\cancel{dz} }} \\cancel{dz} \\\\ =\u0026amp; df + df + df \\\\ \\overset{???}{=}\u0026amp; 3 df \\end{align*} $$ このような惨事は、$d$ が$\\partial$ と同じになる条件を見落としたために起こった循環論法と見ることができます。\u0026rsquo;$\\partial$ をすべて取り除いて$d$ で統一して再度書き直す\u0026rsquo;という展開を無造作に行うことがあまりにも大胆であるため、何らかの方法で$\\partial$ を$d$ で置き換えてもよいと考えること自体が、$x,y,z$ が独立である場合 $$ df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz \\implies {{ d f } \\over { d x }} = {{ \\partial f } \\over { \\partial x }} \\implies d \\equiv \\partial $$ から出てきたものです。その一方で、$d \\equiv \\partial$ の根拠となる$df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz$ を無闇にいじると、どのような方法でも必ず問題が発生します。$d$ と$\\partial$ が等しくなるには、例で仮定したように多変数関数の変数が互いに独立である場合や、何らかの特別な条件の下での何らかの驚くべき定理を通じて、$d$ と$\\partial$ が本当に同じである必要があります。\nこれまでの考察から、偏微分で$d$ の代わりに$\\partial$ を使用する理由は、実際にそれらが異なるためであると要約することができます。これまで見てきた、$d$ と$\\partial$ が同じだったすべての例は、必ずそのための仮定を暗黙的に含んでいます。その良い仮定の中で、$\\partial$ が実質的に$d$ と同じになるかもしれませんが、だからといってわざわざ$\\partial$ を$d$ で書き直す必要もないのです。\n❌ 微分する変数以外は定数とみなすために？ 結論から言うと、間違った答えです。\nもっと正確に言うと、現象を説明する因果関係が逆転しています。例えば、$f(t,x) = \\left( t^{2} + x^{2} \\right)$ であれば、形式的にFormally$\\partial t$ 以外の変数を定数として $$ {{ \\partial f } \\over { \\partial t }} = 2t + 0 = 2t = {{ d f } \\over { d t }} $$ ではないのは、前節で見たように、$t$ と$t$ が独立であるという仮定$x$ の下で $$ \\begin{align*} \u0026amp; df = {{ \\partial f } \\over { \\partial t }} dt + {{ \\partial f } \\over { \\partial x }} dx \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} {{ dt } \\over { dt }} + {{ \\partial f } \\over { \\partial x }} {{ dx } \\over { dt }} \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} \\cdot 1 + {{ \\partial f } \\over { \\partial x }} \\cdot 0 \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} \\end{align*} $$ が成立するからです。偏微分$\\displaystyle {{ dx } \\over { dt }} = 0$ 自体が$\\partial$ という結果をもたらしたのではなく、$\\displaystyle {{ dx } \\over { dt }} = 0$ という原因が$\\displaystyle {{ dx } \\over { dt }} = 0$ という結果をもたらしたのです。このように「偏微分は微分する変数以外を定数として扱う」という説明は、まるで通常の微分$\\partial \\equiv d$ と異なり、偏微分$d$ がより強力なオペレーターであるかのような印象と誤解を与えます。また、$\\partial$ を定数として扱った場合、$x$ で微分した後には消えるはずですが、単純に$t$ のような例を考えると、$f(t,x) = t^{2} + x^{2} + 2tx$ は依然として変数が$\\dfrac{\\partial f}{\\partial t}$ である2変数関数です。\nこのような誤解がなくならない理由は、これがかなりもっともらしいからです。実際には、変数間に$(t,x)$ のような関係があると仮定する場合、そもそも$x = x(t)$ で偏微分するという表現自体を使用する必要がありません。チェーンルールに従えば、 $$ \\begin{align*} {{ d f } \\over { d t }} =\u0026amp; {{ d } \\over { d t }} \\left( t^{2} + x^{2} \\right) \\\\ =\u0026amp; 2t + {{ d x^{2} } \\over { d x }} {{ dx } \\over { dt }} \\\\ =\u0026amp; 2t + 2x \\dot{x} \\end{align*} $$ のように最初から誤解の余地なく式を展開することができます。少なくともこの例では、$t$ は実質的に$f = f(t,x)$ と同じか、むしろ難しいですし、結局のところ教科書ではこのような無意味なケースをすべて排除して、変数間が独立でありながらも依然として多変数関数である形だけが残ります。通常はきれいな例だけを見ながら学び、時間が経ち、偏微分に慣れ、誤った直感が定着し、他の人もそうです。しかし、違うものは違うものです。単に微分の記号を変えるだけで与えられた関数の従属関係を勝手に変えることはできません。\nhttps://math.stackexchange.com/a/2000353/459895\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2573,"permalink":"https://freshrimpsushi.github.io/jp/posts/2573/","tags":null,"title":"偏微分の記号を使い分ける理由"},{"categories":"줄리아","contents":"概要 ジュリアで図を描く時、titleでタイトルを入れるとサブプロット全てに適用されるので、plot_titleを使うべきだ1。これはプロットがサブプロットを含む場合、つまり\nplot(\rplot1, plot2, ...\r) のような形で構成された時、最も外側にあるplot()関数の引数が内部のサブプロットに継承Inheritanceみたいなことが起こるからだ。これを明確に区分するためにtitleとplot_titleが別々にある。\nコード plot(p1, p2, title = \u0026#34;Two Plots\u0026#34;) 見ての通り、title = \u0026quot;Two Plots\u0026quot;をすると全てのサブプロットにそのタイトルが適用される。\nplot(p1, p2, plot_title = \u0026#34;Two Plots\u0026#34;) plot_title = \u0026quot;Two Plots\u0026quot;では全体の図にただ一つのタイトルが適用されているのが確認できる。\n全体コード using Plots\rp1 = scatter(rand(100))\rp2 = histogram(rand(100))\rplot(p1, p2, title = \u0026#34;Two Plots\u0026#34;)\rplot(p1, p2, plot_title = \u0026#34;Two Plots\u0026#34;) 環境 OS: Windows julia: v1.8.5 https://stackoverflow.com/a/69713616/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2572,"permalink":"https://freshrimpsushi.github.io/jp/posts/2572/","tags":null,"title":"Juliaのサブプロットにメインタイトルを追加する方法"},{"categories":"줄리아","contents":"概要 Juliaで、カラーバー、軸、目盛り、グリッドなどの図のグラフィック要素を消す方法があるけれども、グラフィカルな要素をいじるから数字だけをきれいに消すことができず、formatterというオプションを使わないといけない。\nformatter = (_...) -\u0026gt; \u0026quot;\u0026quot; plot()関数のオプションにformatter = (_...) -\u0026gt; \u0026quot;\u0026quot;を与えるといい。\nusing Plots\rx = rand(10)\ry = rand(10)\rplot(\rplot(x,y)\r,plot(x,y, formatter = (_...) -\u0026gt; \u0026#34;\u0026#34;)\r) 上の画像では、左が普通の画像で、右が値を全部消した画像だ。元々formatterはこう使うだけじゃなくて、もっと多機能を持っている。原理を簡単に説明すると、元の画像に表示されるべきだった値に与えられた関数を適用する方式だ。上の例では、(_...) -\u0026gt; \u0026quot;\u0026quot;というラムダ式を受け取って、どんな数値が入っても空白の文字列を返して、軸の値を消した1。\nxformatter, yformatter 当然、xformatter、yformatterがあり、軸別に指定もできる。x軸だけを消したいなら、yformatterに、y軸だけを消したいなら、xformatterに(_...) -\u0026gt; \u0026quot;\u0026quot;を伝えればいい。\n環境 OS: Windows julia: v1.8.5 foreground_color_text = false plot()関数のキーワードにforeground_color_text = falseを入力すればいい。目盛り値(名前)の色を指定するキーワードだけど、falseを入力すると値が全然表示されなくなる。\nx_foreground_color_textとy_foreground_color_textでも軸別に指定できる。\nusing Plots\rplot(\rplot(rand(10)),\rplot(rand(10), foreground_color_text = false)\r) 環境 OS: Windows11 Version: Julia 1.9.3, Plots v1.39.0 https://stackoverflow.com/questions/74842089/remove-only-axis-values-in-plot-julia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2570,"permalink":"https://freshrimpsushi.github.io/jp/posts/2570/","tags":null,"title":"ジュリアプロットで軸の値を削除する方法"},{"categories":"줄리아","contents":"概要 Juliaで有限差分法を使うには、特に有限差分の係数を求めるためには、FiniteDifferences.jlを使うのがいいだろう1。ノイズに弱い場合は、TVDiffとして知られるTotal Variation Regularized Numerical Differentiationを使ってみる価値がある。これはNoiseRobustDifferentiation.jlに実装されている。\nコード FiniteDifferenceMethod() julia\u0026gt; f′ = FiniteDifferenceMethod([-2, 0, 5], 1)\rFiniteDifferenceMethod:\rorder of method: 3\rorder of derivative: 1\rgrid: [-2, 0, 5]\rcoefficients: [-0.35714285714285715, 0.3, 0.05714285714285714]\rjulia\u0026gt; typeof(f′)\rFiniteDifferences.UnadaptedFiniteDifferenceMethod{3, 1} 上記の例では、関数の中央の点、左から2番目の点、右から5番目の点を使って1次微分を計算して、これら3点の重みを求める。基本的にFiniteDifferenceMethod()を通じて得られるのは、これらの係数である。\njulia\u0026gt; propertynames(f′)\r(:grid, :coefs, :coefs_neighbourhood, :condition, :factor, :max_range, :∇f_magnitude_mult, :f_error_mult)\rjulia\u0026gt; f′.grid\r3-element StaticArraysCore.SVector{3, Int64} with indices SOneTo(3):\r-2\r0\r5\rjulia\u0026gt; f′.coefs\r3-element StaticArraysCore.SVector{3, Float64} with indices SOneTo(3):\r-0.35714285714285715\r0.3\r0.05714285714285714 構造体で使用するプロパティには、主に.gridと.coefsの2つがある。\njulia\u0026gt; f′(sin, π/2)\r-1.2376571459669071e-11\rjulia\u0026gt; f′(cos, π/2)\r-1.0000000000076525 データではなく関数自体が与えられた場合は、上記のように直接関数形式で書いても問題ない。\n_fdm() central_fdm(2, 1)\rcentral_fdm(3, 2)\rbackward_fdm(4, 1)\rforward_fdm(5, 1) 特にカスタマイズせずに一般的に知られているFDMが必要な場合は、上記のようにすでに用意されている関数を使う方が便利だ。第1引数は、関数の種類に応じて何点を使用するかに依存し、第2引数$n$は$n$次微分を計算することを決定する。central_fdm()の第1引数は奇数が与えられた場合、中心にある点の係数が確実に0であることは言うまでもない。\n全コード using FiniteDifferences\rf′ = FiniteDifferenceMethod([-2, 0, 5], 1)\rtypeof(f′)\rpropertynames(f′)\rf′.grid\rf′.coefs\rf′(sin, π/2)\rf′(cos, π/2)\rcentral_fdm(2, 1)\rcentral_fdm(3, 2)\rbackward_fdm(4, 1)\rforward_fdm(5, 1) 環境 OS: Windows julia: v1.8.5 https://github.com/JuliaDiff/FiniteDifferences.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2568,"permalink":"https://freshrimpsushi.github.io/jp/posts/2568/","tags":null,"title":"ジュリアで有限差分を使用する方法"},{"categories":"줄리아","contents":"概要 ジュリアでは、数値解析的な補間のためにInterpolations.jlパッケージを使用する1。ジュリアで変数の値を出力する際に使用する補間法と混同しないように注意しよう。\nコード Interpolate() julia\u0026gt; y = rand(10)\r10-element Vector{Float64}:\r0.8993801321974316\r0.12988982511901515\r0.49781160399025925\r0.22555299914088356\r0.4848674643768577\r0.6089318286915111\r0.10444895196527337\r0.5921775799940143\r0.2149546302906653\r0.32749334953170317\rjulia\u0026gt; f = interpolate(y, BSpline(Linear()));\rjulia\u0026gt; f(1.2)\r0.7454820707817483\rjulia\u0026gt; f(0.1)\rERROR: BoundsError: attempt to access 10-element interpolate(::Vector{Float64}, BSpline(Linear())) with element type Float64 at index [0.1] 基本的には上記のようにデータを渡して、補間関数 f=$f$ 自体をリターンしてもらって使用することができる。例では、10個の点が与えられ、1番目(0.899)と2番目(0.129)の間の1.2あたりにある値(0.745)がどのように補間されるかを示している。具体的にどの方法を使用するかは、公式ドキュメントのAPIセクションを参照しよう2。\nlinear_interpolation() x = 1:10\rf_lin = linear_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_lin.(1:0.01:10), label = \u0026#34;Linear\u0026#34;) cubic_spline_interpolation() f_cub = cubic_spline_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_cub.(1:0.01:10), label = \u0026#34;Cubic\u0026#34;) constant_interpolation() f_con = constant_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_con.(1:0.01:10), label = \u0026#34;Constant\u0026#34;) 全体のコード using Interpolations, Plots\ry = rand(10)\rf = interpolate(y, BSpline(Linear()));\rf(1.2)\rf(0.1)\rx = 1:10\rf_lin = linear_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_lin.(1:0.01:10), label = \u0026#34;Linear\u0026#34;)\rf_cub = cubic_spline_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_cub.(1:0.01:10), label = \u0026#34;Cubic\u0026#34;)\rf_con = constant_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_con.(1:0.01:10), label = \u0026#34;Constant\u0026#34;) 環境 OS: Windows julia: v1.8.5 https://github.com/JuliaMath/Interpolations.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://juliamath.github.io/Interpolations.jl/stable/api/#Public-API\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2566,"permalink":"https://freshrimpsushi.github.io/jp/posts/2566/","tags":null,"title":"ジュリアでの数値解析的補間"},{"categories":"줄리아","contents":"概要 Juliaでは、差分を計算するためにdiff()関数が提供されている1。circshift()関数も使って簡単に書けるけど、端点の処理などがちょっと面倒に感じるから、知ってるとずっと楽になる。RやMatlabのdiff()関数とほぼ同じ方法で使えるが、これらと違い2次差分（差分を二回とること）などは特に実装されていない。\nコード 基本的な使い方 julia\u0026gt; x = rand(0:9, 12)\r12-element Vector{Int64}:\r3\r1\r9\r7\r1\r0\r6\r5\r3\r2\r9\r9 例えば上記のような配列がある場合、ただdiff()を適用して、前の要素と後ろの要素の差を計算した結果を得ることができる。結果から配列のサイズが正確に1だけ縮小されたのを確認できる。\njulia\u0026gt; diff(x)\r11-element Vector{Int64}:\r-2\r8\r-2\r-6\r-1\r6\r-1\r-2\r-1\r7\r0 多次元配列 julia\u0026gt; X = reshape(x, 3, :)\r3×4 Matrix{Int64}:\r3 7 6 2\r1 1 5 9\r9 0 3 9\rjulia\u0026gt; diff(X)\rERROR: UndefKeywordError: keyword argument dims not assigned\rStacktrace:\r[1] diff(a::Matrix{Int64})\r@ Base .\\multidimensional.jl:997\r[2] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\DataDrivenModel\\REPL.jl:7 例えば、上記のような多次元配列がある場合、ただdiff()を適用するとエラーになる。これは、配列を押す方向が与えられていないためであり、次のようにdimsを引数にしてどの次元で差分を計算するかを決める必要がある。1次元の場合と同じように、差分をとった方向で長さが1ずつ短くなることに注意。\njulia\u0026gt; diff(X, dims = 1)\r2×4 Matrix{Int64}:\r-2 -6 -1 7\r8 -1 -2 0\rjulia\u0026gt; diff(X, dims = 2)\r3×3 Matrix{Int64}:\r4 -1 -4\r0 4 4\r-9 3 6 全コード x = rand(0:9, 12)\rdiff(x)\rX = reshape(x, 3, :)\rdiff(X)\rdiff(X, dims = 1)\rdiff(X, dims = 2) 環境 OS: Windows julia: v1.8.5 https://docs.julialang.org/en/v1/base/arrays/#Base.diff\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2564,"permalink":"https://freshrimpsushi.github.io/jp/posts/2564/","tags":null,"title":"ジュリアで配列の差分を計算する方法"},{"categories":"줄리아","contents":"概要 実は、Juliaではネイティブに円形配列Circular Arrayをサポートしていないが、要素を円形にCircularlyシフトしてくれるcircshift()関数を提供していて、事実上それを使うことができる1。自分で書くのはそれほど難しくないが、知っていればわざわざ書かなくても良い。この関数はマットラボのcircshift()とほぼ同じ方法で使える。\nコード この関数は、配列を平行移動する方法のポストでも紹介された。\n基本的な使い方 julia\u0026gt; circshift(1:4, 1)\r4-element Vector{Int64}:\r4\r1\r2\r3\rjulia\u0026gt; circshift(1:4, -1)\r4-element Vector{Int64}:\r2\r3\r4\r1 circshift()は基本的に二番目の引数に整数を入れて、要素をシフトする。上の例では、正の整数で後ろ(下)にシフトし、負の整数で前(上)にシフトすることが確認できる。\n多次元配列 julia\u0026gt; ca = reshape(1:20, 5, :)\r5×4 reshape(::UnitRange{Int64}, 5, 4) with eltype Int64:\r1 6 11 16\r2 7 12 17\r3 8 13 18\r4 9 14 19\r5 10 15 20 上のような多次元配列が与えられた場合、次のように同じ次元のタプルを与えて、各次元をどれだけシフトするかを指定する。\njulia\u0026gt; circshift(ca, (0,1))\r5×4 Matrix{Int64}:\r16 1 6 11\r17 2 7 12\r18 3 8 13\r19 4 9 14\r20 5 10 15\rjulia\u0026gt; circshift(ca, (-1,0))\r5×4 Matrix{Int64}:\r2 7 12 17\r3 8 13 18\r4 9 14 19\r5 10 15 20\r1 6 11 16\rjulia\u0026gt; circshift(ca, (-1,1))\r5×4 Matrix{Int64}:\r17 2 7 12\r18 3 8 13\r19 4 9 14\r20 5 10 15\r16 1 6 11 全体のコード circshift(1:4, 1)\rcircshift(1:4, -1)\rca = reshape(1:20, 5, :)\rcircshift(ca, (0,1))\rcircshift(ca, (-1,0))\rcircshift(ca, (-1,1)) 環境 OS: Windows julia: v1.8.5 https://docs.julialang.org/en/v1/base/arrays/#Base.circshift\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2562,"permalink":"https://freshrimpsushi.github.io/jp/posts/2562/","tags":null,"title":"ジュリアで円形配列を使う方法"},{"categories":"줄리아","contents":"コード 1 長々と説明する必要はなく、文字通りマーカースタイルとラインスタイルが実際にどう見えるかを示す。\nlinesytle [:auto, :solid, :dash, :dot, :dashdot, :dashdotdot] の中から一つ選ぶ。\nshape [:none, :auto, :circle, :rect, :star5, :diamond, :hexagon, :cross, :xcross, :utriangle, :dtriangle, :rtriangle, :ltriangle, :pentagon, :heptagon, :octagon, :star4, :star6, :star7, :star8, :vline, :hline, :+, :x] の中から一つ選ぶ。\n全体のコード using Plots\rlines = [:auto, :solid, :dash, :dot, :dashdot, :dashdotdot]\rplt = plot(grid = :none, showaxis = :false, legend = :outerright)\rfor k in 1:6\rplot!(plt, [0, 1], [-k, -k], lw = 2, color = :black, linestyle = lines[k], label = string(lines[k]))\rend\rplt\rshapes = [:none, :auto, :circle, :rect, :star5, :diamond, :hexagon, :cross, :xcross, :utriangle, :dtriangle, :rtriangle, :ltriangle, :pentagon, :heptagon, :octagon, :star4, :star6, :star7, :star8, :vline, :hline, :+, :x]\rplot(grid = :none, showaxis = :false, xlims = (-0.1,4), size = (600, 600))\rscatter!((0:23) .% 4, -(0:23) .÷ 4, shape = shapes, color = :black, legend = :none, markersize = 10)\rannotate!(.15 .+ ((0:23) .% 4), -(0:23) .÷ 4, text.(shapes, :left)) 環境 OS: Windows julia: v1.8.5 Plots v1.38.5 https://docs.juliaplots.org/latest/generated/attributes_series/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2560,"permalink":"https://freshrimpsushi.github.io/jp/posts/2560/","tags":null,"title":"ジュリアでのマーカーとラインスタイルのリスト"},{"categories":"줄리아","contents":"コード Juliaの散布図に回帰直線を入れる方法は、オプションでsmooth = trueを使うことだ。\nusing Plots\rx = rand(100)\rscatter(x, 2x .+ 0.1randn(100), smooth = true)\rsavefig(\u0026#34;plot.svg\u0026#34;) 環境 OS: Windows julia: v1.8.3 Plots v1.38.5 ","id":2558,"permalink":"https://freshrimpsushi.github.io/jp/posts/2558/","tags":null,"title":"ジュリアプロットで回帰直線を描く方法"},{"categories":"줄리아","contents":"概要 Juliaで頻繁に使われるsplatの...の用途について、オプショナル引数を伝える方法を説明する。基本的に、どんなオプションにどんな引数を入れるかを事前に名前付きタプルの形で決めた後、そのタプルにsplatオペレーターを適用する方式で使う。\nコード 複数の関数に伝える args1 = (; dims = 1)\n上の名前付きタプルargs1はdimsというオプショナル引数がある全ての関数に共通して使うことができる。次の例でsum()とminimum()は全く異なる関数だが、共にdimsを持っているため、関数の種類に関係なく適用された。\njulia\u0026gt; sum(rand(100,100); args1...)\r1×100 Matrix{Float64}:\r47.0704 45.7637 44.4513 48.2325 50.5745 51.9176 … 49.9548 47.6825 50.7284 50.0861 50.0168 50.5116\rjulia\u0026gt; minimum(rand(100,100); args1...)\r1×100 Matrix{Float64}:\r0.00702003 0.0163299 0.00665818 0.0174564 0.00589048 … 0.002967 0.00460205 0.0116248 0.0114521 0.0698425 複数の引数を伝える args2 = (; dims = 2, rev = true)\n上の名前付きタプルargs2はdimsに加えてrevというオプショナル引数を含んでいる。次の例でsort()関数は入力データに関係なく二つのオプションをよく反映して計算結果を返した。\njulia\u0026gt; sort(rand(0:9, 3,3); args2...)\r3×3 Matrix{Int64}:\r9 4 4\r6 5 2\r8 0 0\rjulia\u0026gt; sort(rand(3,3); args2...)\r3×3 Matrix{Float64}:\r0.438682 0.211154 0.108741\r0.72113 0.445214 0.00910109\r0.971441 0.666732 0.0227372 全体コード args1 = (; dims = 1)\rsum(rand(100,100); args1...)\rminimum(rand(100,100); args1...)\rargs2 = (; dims = 2, rev = true)\rsort(rand(0:9, 3,3); args2...)\rsort(rand(3,3); args2...) 環境 OS: Windows julia: v1.8.3 ","id":2554,"permalink":"https://freshrimpsushi.github.io/jp/posts/2554/","tags":null,"title":"Juliaスプラットオペレーターを通じたオプション引数の渡し方のヒント"},{"categories":"줄리아","contents":"概要 Juliaで...はスプラット・オペレーターと呼ばれ、関数を使用したり、配列を定義する際に便利に使われる1。このオペレーターはJuliaに限定されているわけではないが、他の言語に比べて直感的に定義されており、学びやすく覚えやすいところが突出している。個人的な経験だと、...を使うようになるとJuliaプログラミングに関する何かしらの気づきを得る気がする。\nコード 関数入力 基本的に...は配列やジェネレータの後に付けて使い、そのまま前にあるコンテナ/イテレータの要素を全部展開して出す。\njulia\u0026gt; min([1,2,3,4,5,6,7,8,9,10])\rERROR: MethodError: no method matching min(::Vector{Int64})\rjulia\u0026gt; min(1,2,3,4,5,6,7,8,9,10)\r1 例えばJuliaのmin()関数はリデュースとして働くから、そのまま配列を渡すのではなく、複数の数を直接引数として渡さなければならない。もちろん、配列が大きくなるほど手で全部解いて書くのは難しくなり、配列の要素を展開して入れることができるように...を使うことができる。\njulia\u0026gt; min(1:10)\rERROR: MethodError: no method matching min(::UnitRange{Int64})\rjulia\u0026gt; min((1:10)...)\r1 もちろん、実際は配列に使えるminimum()関数があるから、わざわざこうする必要はない。\n配列定義 julia\u0026gt; [(1:10)]\r1-element Vector{UnitRange{Int64}}:\r1:10 上で定義された配列はユニットレンジの配列なので、直接要素にアクセスするのが少し面倒になる。時間があれば、ただ単に1から10までの数字を書き込んだらいいけど、スプラットオペレータを使えば、次のように簡単に定義できる。\njulia\u0026gt; [(1:10)...]\r10-element Vector{Int64}:\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10 もちろん、collect()関数のような代替手段もあるが、知っておくときれいでセンスがある表現だという点が魅力的だ。ただし、速度の面ではあまり推奨されていないので、必要以上に...を使わないようにしよう。\njulia\u0026gt; [eachrow(rand(3,3))...]\r3-element Vector{SubArray{Float64, 1, Matrix{Float64}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}}:\r[0.6695368164913422, 0.69695509795356, 0.12084083239135612]\r[0.6833475867141307, 0.5368141950494666, 0.7877252857572066]\r[0.2810163716135018, 0.04317485597011517, 0.44214186775440534] ...が面白くなるのは、上のようにジェネレータを一度通るときだ。eachrow()は行列を行単位に切ったベクトルのジェネレータをリターンし、スプラットオペレータを通じて、その各ベクトルを配列表記[]に入れて、ベクトルのベクトルを作ったものだ。\n全コード min([1,2,3,4,5,6,7,8,9,10])\rmin(1,2,3,4,5,6,7,8,9,10)\rmin(1:10)\rmin((1:10)...)\r[(1:10)]\r[(1:10)...]\r[eachrow(rand(3,3))...] 環境 OS: Windows julia: v1.8.3 https://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2552,"permalink":"https://freshrimpsushi.github.io/jp/posts/2552/","tags":null,"title":"ジュリアのスプラットオペレータ"},{"categories":"줄리아","contents":"概要 他のプログラム言語がそうであるように、ジュリアでは英語をASCIIコードASCII Codeで書き、漢字、韓国語などをユニコードUnicodeで書く。問題は、他の言語たちと違って、この文字列たちを扱うことがかなり厄介であることだけど、これは性能上の理由から意図されたものであるため1、不便でも我慢して使うしかない。\nコード julia\u0026gt; str1 = \u0026#34;English\u0026#34;\r\u0026#34;English\u0026#34;\rjulia\u0026gt; str2 = \u0026#34;日本語\u0026#34;\r\u0026#34;日本語\u0026#34;\rjulia\u0026gt; str3 = \u0026#34;한국어\u0026#34;\r\u0026#34;한국어\u0026#34; 例えば、上のような文字列たちが与えられているとしよう。\njulia\u0026gt; str1[2:end]\r\u0026#34;nglish\u0026#34; str1の場合は何も特別なことがない英語の文字列で、ASCIIコードであるため、上のように普通の配列にアクセスするようにスライシングが可能である。\njulia\u0026gt; str2[2:end]\rERROR: StringIndexError: invalid index [2], valid nearby indices [1]=\u0026gt;\u0026#39;日\u0026#39;, [4]=\u0026gt;\u0026#39;本\u0026#39;\rStacktrace:\r[1] string_index_err(s::String, i::Int64)\r@ Base .\\strings\\string.jl:12\r[2] getindex(s::String, r::UnitRange{Int64})\r@ Base .\\strings\\string.jl:266\r[3] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\population_dynamics\\REPL.jl:6 しかし、str2は漢字があるためにユニコードで書かれており、上のようにインデックスエラーを発生させる。エラーメッセージを見ると、2番目の文字のインデックスは2ではなく4であることが推測でき、実際に4からインデキシングをすると、意図された通りにスライシングされる。\njulia\u0026gt; str2[4:end]\r\u0026#34;本語\u0026#34; これは以下のように韓国語にも同様に適用される。同じユニコードであるため、違う理由がない。\njulia\u0026gt; str3[4:end]\r\u0026#34;국어\u0026#34;\rjulia\u0026gt; str3[6]\rERROR: StringIndexError: invalid index [6], valid nearby indices [4]=\u0026gt;\u0026#39;국\u0026#39;, [7]=\u0026gt;\u0026#39;어\u0026#39;\rStacktrace:\r[1] string_index_err(s::String, i::Int64)\r@ Base .\\strings\\string.jl:12\r[2] getindex_continued(s::String, i::Int64, u::UInt32)\r@ Base .\\strings\\string.jl:237\r[3] getindex(s::String, i::Int64)\r@ Base .\\strings\\string.jl:230\r[4] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\population_dynamics\\REPL.jl:9\rjulia\u0026gt; str3[7]\r\u0026#39;어\u0026#39;: Unicode U+C5B4 (category Lo: Letter, other) トリック julia\u0026gt; String(collect(str3)[2:3])\r\u0026#34;국어\u0026#34; まあまあ楽に使う方法としては、上のようにcollect()で文字の配列に解いてスライシングし、また文字列にまとめる方法がある。\n環境 OS: Windows julia: v1.8.3 https://discourse.julialang.org/t/weird-string-slicing-in-korean/92252/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2550,"permalink":"https://freshrimpsushi.github.io/jp/posts/2550/","tags":null,"title":"ジュリアでユニコード文字列の一部だけをスライスする方法"},{"categories":"줄리아","contents":"概要 ジュリアのStatsPlotsパッケージでは、図を描く時に@dfマクロを通して、繰り返されるデータフレーム名を省略することができる1。マクロを使う文法は、データフレームXのa列を使う場合、@df X というようにどのデータフレームを使うか指定した後、直ぐに続くスコープでaをシンボルの:aとして引数に渡し、plot (:a)という風に書くことだ。コードで要約すると、@df X plot(:a)と書く。\nコード 以下は、アイリスデータのSepalLengthとSepalWidthで描いた散布図だ。\n次のコードscatter(iris.SepalLength, iris.SepalWidth)と@df iris scatter(:SepalLength, :SepalWidth)は同じである。\nusing RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rusing StatsPlots\rscatter(iris.SepalLength, iris.SepalWidth)\r@df iris scatter(:SepalLength, :SepalWidth) 環境 OS: Windows julia: v1.8.3 StatsPlots v0.15.4 https://github.com/JuliaPlots/StatsPlots.jl#original-author-thomas-breloff-tbreloff-maintained-by-the-juliaplots-members\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2548,"permalink":"https://freshrimpsushi.github.io/jp/posts/2548/","tags":null,"title":"Julia StatsPlotsでデータフレーム名を省略するマクロ@df"},{"categories":"줄리아","contents":"概要 他のファイルにある関数を使えるようにするために、ジュリアコード自体を実行するinclude()関数を紹介する。マットラボでは、同じディレクトリ内にあれば自動的に関数を見つけてくれるため、このプロセスを難しく考える人もいる。ちなみに、ちゃんとモジュール化してエクスポートする方法があるが1、難しくて複雑なので、機能が急に必要な初心者にはお勧めしない。パッケージを自分で作るか、プログラムの規模が扱えないほど大きくなった後にモジュール化を学んでも遅くはない。\nガイド 上記のように、foo/bar.jlファイルにあるbaz()関数をmain.jlから実行したいとしよう。スクリーンショットで確認できるように、モジュールのようなものを別に使用せず、ただ普通にジュリアコードを書けばいい。\nそして、include()でパスを指定して実行した結果は、次のようになる。\ninclude()の実行結果で23が表示された理由は、bar.jlファイルの最下部にy = 23という値の割り当てがあったからだ。見ての通り、関数だけでなく変数も移せるし、ファイル自体を実行する方式なので、データのロードやログ出力も全部できる。\n環境 OS: Windows julia: v1.8.3 https://docs.julialang.org/en/v1/manual/modules/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2544,"permalink":"https://freshrimpsushi.github.io/jp/posts/2544/","tags":null,"title":"ジュリアで他のファイルに定義された関数の使用方法"},{"categories":"줄리아","contents":"コード using Plots\rx, y = rand(100), rand(100) 上記のようなデータが与えられたとしよう。データが連続かカテゴリカルかによって、図の形や描く方法が異なる。\n連続型 scatter(marker_z=) z = x + y\rscatter(x, y, marker_z = z) カテゴリカル scatter(group=) 1 team = rand(\u0026#39;A\u0026#39;:\u0026#39;C\u0026#39;, 100)\rscatter(x, y, group = team) 環境 OS: Windows julia: v1.8.3 https://stackoverflow.com/a/60846501/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2537,"permalink":"https://freshrimpsushi.github.io/jp/posts/2537/","tags":null,"title":"ジュリア集合でマーカーに色をつける方法"},{"categories":"통계적분석","contents":"モデル オーディナリークリギング 空間データ分析で、ランダムフィールド $\\mathbf{Y} = \\left( Y \\left( s_{1} \\right) , \\cdots , Y \\left( s_{n} \\right) \\right)$ の平均 $\\mu \\in \\mathbb{R}$ と共分散行列 $\\Sigma \\in \\mathbb{R}^{n \\times n}$ が多変量正規分布に従う $\\varepsilon \\sim N_{n} \\left( \\mathbf{0} , \\Sigma \\right)$ ことについて、次のモデル $$ \\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon $$ を通じて、新しいサイトSite $s_{0}$ の $Y \\left( s_{0} \\right)$ を推定した値をオーディナリークリギング推定値Ordinary Kriging Estimateと呼ぶ。このようにモデルを立てて推定する行為自体をクリギングとも呼ぶ。\n$\\mathbf{1} = (1 , \\cdots , 1)$ は全ての成分が $1$ の1ベクトルだ。 $N_{n} \\left( \\mathbf{0} , \\Sigma \\right)$ は多変量正規分布を意味する。 説明 語源 クリギングKrigingは、ダニエル・G・クリージDaine G. Krigeという巨匠の名前がそのまま動詞化されたものである。通常の統計学で予測、予想、適合、推定などと使う表現はもちろん、「空白の空間」の値を埋めるという意味で補間技術のように説明する場合もかなりあるが、これら全ての説明を短くしてクリギングするという一般動詞になったと考えられる。\n狭義の応用数学、コンピュータアルゴリズム、機械学習技術などと区別されるクリギングの特徴は、とにかく統計学らしくその平均（点推定量）だけでなくその分散まで考慮することである。想像してみてほしいが、各地点で分散が高い場所のクリギング推定値は同様に分散が大きく、分散が低い場所同士の地点では分散が低いだろう。これはあるデータの観測所の位置を選定するのにも使われるが、例えば微粒子の濃度を測定するとしたら、微粒子をどのように測定するかが気になるのではなく、その測定がどれだけ正確か―つまり、測定値の分散が最も高い場所を選定するようなアプローチがある。\n依存性 $$ \\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon \\qquad , \\text{where } \\varepsilon \\sim N_{n} \\left( 0, \\Sigma \\right) $$ モデルの数式を見ると、回帰分析や時系列分析と異なり $\\varepsilon$ こそが我々の関心事である。$\\Sigma$ が対角行列、つまり観測値ごとの依存性がなければ、そもそも空間的な構造がないという意味であり、わざわざクリギングをする理由がない。実際の分析ではこの $\\Sigma$ はセミバリオグラムのモデルを通じて次のように決定される。 $$ \\Sigma = \\sigma^{2} H \\left( \\phi \\right) + \\tau^{2} I $$ ここで $\\tau^{2}$ はナゲット効果分散（理論と異なり実際のデータで距離に関係なく基本的に見られる共分散性）であり、$I$ は単位行列である。\n一般化 次のように他の独立変数に対して一般化モデルを使用するクリギングをユニバーサルクリギングと呼ぶ。 $$ \\mathbf{Y} = \\mathbf{X} \\beta + \\varepsilon $$\n公式 ランダムフィールド $\\left\\{ Y (s_{k}) \\right\\}_{k=1}^{n}$ が固有的定常空間過程であるとし、新たに予測したい地点を $s_{0}$ とする。バリオグラム $2 \\gamma$ に対して行列 $\\Gamma \\in \\mathbb{R}^{n \\times n}$ を $\\left( \\Gamma \\right)_{ij} := \\gamma \\left( s_{i} - s_{j} \\right)$ のように定義し、ベクトル $\\gamma_{0} \\in \\mathbb{R}^{n}$ を $\\left( \\gamma_{0} \\right)_{i} := \\left( \\gamma \\left( s_{0} - s_{i} \\right) \\right)$ のように定義する。あるベクトル $l = \\left( l_{1} , \\cdots , l_{n} \\right)$ に対して $Y \\left( s_{0} \\right)$ の最良線形不偏予測量BLUP, Best Linear Unbiased Predictorは $l$ と $\\mathbf{Y}$ の内積であり、 $$ l^{T} \\mathbf{Y} = \\begin{bmatrix} l_{1} \u0026amp; \\cdots \u0026amp; l_{n} \\end{bmatrix} \\begin{bmatrix} Y \\left( s_{1} \\right)\\\\ Y \\left( s_{2} \\right)\\\\ \\vdots\\\\ Y \\left( s_{n} \\right) \\end{bmatrix} = \\sum_{k=1}^{n} l_{k} Y \\left( s_{k} \\right) $$ ベクトル $l$ は具体的に次のように求められる。 $$ l = \\Gamma^{-1} \\left( \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\right) $$\n導出 1 具体的にクリギングがどのように行われるか、数式で調べるプロセスである。この公式の仮定にガウス過程という仮定まで追加されれば、オーディナリークリギングになる。\nパート1. 最適化問題\nある定数 $l_{1} , \\cdots , l_{n} , \\delta_{0} \\in \\mathbb{R}$ に対して新しい $Y \\left( s_{0} \\right)$ を既存データの線形結合 $$ \\hat{y} \\left( s_{0} \\right) = l_{1} y_{1} + \\cdots + l_{n} y_{n} + \\delta_{0} $$ で予測したいとする。これはつまり、目的関数 $$ E \\left[ Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) + \\delta_{0} \\right) \\right]^{2} $$ を最小化する最適解 $l_{1} , \\cdots , l_{n} , \\delta_{0}$ を見つけることになる。\n固有的定常性の定義: ユークリッド空間の固定された部分集合 $D \\subset \\mathbb{R}^{r}$ で、確率変数 $Y(s) : \\Omega \\to \\mathbb{R}^{1}$ の集合である空間過程 $\\left\\{ Y(s) \\right\\}_{s \\in D}$ と方向ベクトル $\\mathbf{h} \\in \\mathbb{R}^{r}$ を考える。具体的に $n \\in \\mathbb{N}$ 個のサイトSiteを $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$ のように表し、$Y(s)$ はすべての $s \\in D$ に対して分散が存在すると仮定する。$\\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]$ の平均が $0$ でありながら分散が唯一 $\\mathbf{h}$ にのみ依存する場合、$\\left\\{ Y \\left( s_{k} \\right) \\right\\}$ が固有的定常性Intrinsic Stationarityを持つと言われる。 $$ \\begin{align*} E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 0 \\\\ \\text{Var} \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 2 \\gamma ( \\mathbf{h} ) \\end{align*} $$\nここで $\\left\\{ Y \\left( s_{k} \\right) \\right\\}_{k=1}^{n}$ が固有的定常性を持つ場合、$\\sum_{k} l_{k} = 1$ という制約条件を置くことにより、 $$ \\begin{align*} \u0026amp; E \\left[ Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right) \\right] \\\\ =\u0026amp; E \\left[ \\sum_{k} l_{k} Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right) \\right] \\\\ =\u0026amp; \\sum_{k} l_{k} E \\left[ Y \\left( s_{0} \\right) - Y \\left( s_{k} \\right) \\right] \\\\ =\u0026amp; 0 \\end{align*} $$ となるようにできる。これにより、我々が最小化するべき目的関数は $\\delta_{0}$ が外れた次の形になる。 $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right]^{2} + \\delta_{0}^{2} $$ ここで $\\delta_{0}^{2}$ は予測と関係ない。実際にもモデルが $\\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon$ であれば、$\\delta_{0}$ は $\\mu$ に該当し、$\\delta_{0} = 0$ として $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right]^{2} $$ としても問題ない。今、$a_{0} = 1$ とし、$a_{k} = - l_{k}$ とすると、 $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k=1}^{n} l_{k} Y \\left( s_{k} \\right) \\right]^{2} = E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2} $$ となるため、我々は次の最適化問題をラグランジュ乗数法で解くことになる。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; \\displaystyle E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2} \\\\ \\text{subject to} \u0026amp; \\displaystyle \\sum_{k=0}^{n} a_{k} = 0 \\end{matrix} $$\nパート2. セミバリオグラム $\\gamma$\n今、$E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2}$ を「我々がデータから計算できると仮定できる」セミバリオグラムに依存した形で表してみよう。\nセミバリオグラムの定義: ユークリッド空間の固定された部分集合 $D \\subset \\mathbb{R}^{r}$ で、確率変数 $Y(s) : \\Omega \\to \\mathbb{R}^{1}$ の集合である空間過程 $\\left\\{ Y(s) \\right\\}_{s \\in D}$ と方向ベクトル $\\mathbf{h} \\in \\mathbb{R}^{r}$ を考える。具体的に $n \\in \\mathbb{N}$ 個のサイトを $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$ のように表し、$Y(s)$ はすべての $s \\in D$ に対して分散が存在すると仮定する。次のように定義される $2 \\gamma ( \\mathbf{h} )$ をバリオグラムVariogramと呼ぶ。 $$ 2 \\gamma ( \\mathbf{h} ) := E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]^{2} $$ 特にバリオグラムの半分 $\\gamma ( \\mathbf{h} )$ をセミバリオグラムSemivariogramと呼ぶ。\n$\\gamma \\left( s_{i} - s_{j} \\right)$ を2つのサイト $s_{i}, s_{j}$ の間の方向ベクトルによるセミバリオグラムとしよう。$\\sum_{0=1}^{n} a_{k} = 0$ を満たす任意の集合 $\\left\\{ a_{k} : k = 1 , \\cdots , n \\right\\} \\subset \\mathbb{R}$ に対して次が成立する。 $$ \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) = - E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} $$ これは次の展開 $$ \\begin{align*} \u0026amp; \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} \\text{Var} \\left[ Y \\left( s_{i} \\right) - Y \\left( s_{j} \\right) \\right] \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left[ Y \\left( s_{i} \\right) - Y \\left( s_{j} \\right) \\right]^{2} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left( \\left[ Y \\left( s_{i} \\right) \\right]^{2} - 2 Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) + \\left[ Y \\left( s_{j} \\right) \\right]^{2} \\right) \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left( Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) \\right) \u0026amp; \\because \\text{cases of } i = j \\\\ =\u0026amp; - E \\sum_{i} \\sum_{j} a_{i} a_{j} Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) \\\\ =\u0026amp; - E \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\sum_{j} a_{j} Y \\left( s_{j} \\right) \\\\ =\u0026amp; - E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} \\end{align*} $$ で確認できる。今、$\\gamma_{ij} = \\gamma \\left( s_{i} - s_{j} \\right)$ とし、$\\gamma_{0j} = \\gamma \\left( s_{0} - s_{j} \\right)$ とすれば、我々の目的関数は $$ \\begin{align*} \u0026amp; E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} l_{i} l_{j} \\gamma_{ij} + 2 \\sum_{i} l_{i} \\gamma_{0i} \\end{align*} $$ のように表され、ラグランジュ乗数法によって制約条件 $\\sum_{i} l_{i} = 1$ にラグランジュ乗数Lagrange Multiplierを掛けて引くことで、 $$ \\ - \\sum_{i} \\sum_{j} l_{i} l_{j} \\gamma_{ij} + 2 \\sum_{i} l_{i} \\gamma_{0i} - \\lambda \\sum_{i} l_{i} $$ を得る。したがって、$l_{i}$ に対して偏微分して $$ \\ - \\sum_{j} l_{j} \\gamma_{ij} + \\gamma_{0i} - \\lambda = 0 $$ となる時、目的関数を最小化できることがわかる。\nパート3. 最適解\nここで、具体的な最適解の公式を導出する。行列 $\\Gamma \\in \\mathbb{R}^{n \\times n}$ の $(i,j)$ 成分を ▷\neq74◁ とし、つまり $\\left( \\Gamma \\right)_{ij} := \\gamma_{ij}$ とし、ベクトル $\\gamma_{0} \\in \\mathbb{R}^{n}$ を $\\left( \\gamma_{0} \\right)_{i} := \\gamma_{0i}$ のように定義しよう。係数のベクトルも同様に $l := \\left( l_{1} , \\cdots , l_{n} \\right) \\in \\mathbb{R}^{n}$ とすると、パート2で得た式は次のような行列/ベクトル形式で表すことができる。 $$ \\begin{align*} \u0026amp; - \\sum_{j} l_{j} \\gamma_{ij} + \\gamma_{0i} - \\lambda = 0 \\\\ \\implies \u0026amp; - \\Gamma l + \\gamma_{0} - \\lambda \\mathbf{1} = 0 \\\\ \\implies \u0026amp; \\Gamma l + \\lambda \\mathbf{1} = \\gamma_{0} \\end{align*} $$ 一方、制約条件で $$ \\sum_{i} l_{i} = 1 \\iff \\begin{bmatrix} 1 \u0026amp; \\cdots \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} l_{1} \\\\ \\vdots \\\\ l_{n} \\end{bmatrix} = 1 \\iff \\mathbf{1}^{T} l = 1 $$ となるので、$\\mathbf{1}^{T} l = 1$ も得られる。ここで、$\\mathbf{x}^{T}$ は $\\mathbf{x}$ の転置を表す。まず単独で $\\lambda$ は $$ \\begin{align*} 1 =\u0026amp; \\mathbf{1}^T l \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\Gamma l \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\left( \\gamma_{0} - \\lambda \\mathbf{1} \\right) \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} - \\lambda \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} \\end{align*} $$ となり、整理すると $$ \\ - \\lambda = {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} $$ のように表すことができる。今、$l$ はほぼ求めたも同然である。 $$ \\begin{align*} \u0026amp; \\Gamma l + \\lambda \\mathbf{1} = \\gamma_{0} \\\\ \\implies \u0026amp; \\Gamma l = \\gamma_{0} - \\lambda \\mathbf{1} \\\\ \\implies \u0026amp; \\Gamma l = \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\\\ \\implies \u0026amp; l = \\Gamma^{-1} \\left( \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\right) \\end{align*} $$\n■\nBanerjee. (2015). Hierarchical Modeling and Analysis for Spatial Data(2nd Edition): p25, 40~41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2521,"permalink":"https://freshrimpsushi.github.io/jp/posts/2521/","tags":null,"title":"空間データ分析におけるクリギングとは？"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","id":null,"permalink":"https://freshrimpsushi.github.io/jp/search/","tags":null,"title":"Search Results"}]